
\documentclass{article} % For LaTeX2e
\usepackage[final]{neurips}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{booktabs}

\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx

\hypersetup{
  colorlinks,
  citecolor=blue,
  linkcolor=black,
  urlcolor=blue}


    % Sadhana Lolla$^*$, Iaroslav Elistratov$^*$, Alejandro Perez, Elaheh Ahmadi, \\Daniela Rus, Alexander Amini


\title{\textit{Capsa}: A Unified Framework for Quantifying Risk in Deep Neural Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\author{Sadhana Lolla\thanks{Denotes equal contribution and co-first authorship}~, Iaroslav Elistratov$^*$, Alejandro Perez, Elaheh Ahmadi, \\\textbf{Daniela Rus, Alexander Amini\thanks{Corresponding author: \texttt{alexander@themisai.io}}}
% \author{Alexander Amini\thanks{Corresponding author: \texttt{amini@themisai.io}},~ Sadhana Lolla, Iaroslav Elistratov,\\ \textbf{Elaheh Ahmadi, Daniela Rus, Alejandro Perez}
\\
Themis AI Inc\\
\url{themisai.io}\\
% \small\texttt{\{sadhana, iaroslav, alejandro, elahehahmadi, rus, amini\}@themisai.io} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.


% Macros
\def\capsa{\texttt{{capsa}}}
\def\Capsa{\texttt{{Capsa}}}

\newcommand{\todo}[1]{\textcolor{red}{[\textbf{TODO}: #1]}}
\newcommand{\note}[1]{\textcolor{blue}{[\textbf{NOTE}: #1]}}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
% Background
The modern pervasiveness of large-scale deep neural networks (NNs) is driven by their extraordinary performance on complex problems but is also plagued by their sudden, unexpected, and often catastrophic failures, particularly on challenging scenarios. 
%
% Problem
Existing algorithms that provide risk-awareness to NNs are complex and ad-hoc. Specifically, these methods require significant engineering changes, are often developed only for particular settings, and are not easily composable. 
%
% Solution 
Here we present \capsa, a framework for extending models with risk-awareness. \Capsa{} provides a methodology for quantifying multiple forms of risk and composing different algorithms together to quantify different risk metrics in parallel. 
%
% Results
We validate \capsa{} by implementing state-of-the-art uncertainty estimation algorithms within the \capsa{} framework and benchmarking them on complex perception datasets. We demonstrate \capsa{}'s ability to easily compose aleatoric uncertainty, epistemic uncertainty, and bias estimation together in a single procedure, and show how this approach provides a comprehensive awareness of NN risk. 
\end{abstract}


\section{Introduction}


% \begin{wrapfigure}{R}{0.45\textwidth}
% \centering
% \includegraphics[width=0.45\textwidth]{figures/Teaser.pdf}
% \caption{\textbf{\Capsa} unifies state-of-the-art algorithms for quantifying neural network risks ranging from (A) under-representation bias, (B) epistemic (model) uncertainty, and (C) aleatoric uncertainty (label noise). \Capsa~ converts existing classification (left) and (regression) models into risk-aware variants, capable of identifying risks efficiently during training as well as deployment. }
% \label{fig:teaser}
% \end{wrapfigure}


% \begin{figure}[t!]
% \centering
% \includegraphics[width=\linewidth]{figures/Teaser.pdf}
% % \includegraphics[width=\linewidth, height=5cm]{example-image-b}
% \caption{\textbf{\Capsa} unifies state-of-the-art algorithms for quantifying neural network risks ranging from (A) under-representation bias, (B) epistemic (model) uncertainty, and (C) aleatoric uncertainty (label noise). \Capsa~ converts existing classification (left) and (regression) models into risk-aware variants, capable of identifying risks efficiently during training as well as deployment. }
% \label{fig:teaser}
% % \vspace{-15pt}
% \end{figure}


% The prevalence of NNs for safety critical.
Neural networks (NNs) continue to push the boundaries of modern artificial intelligence (AI) systems across a wide range of complex real-world domains, from robotics and autonomy~\citep{bojarski2016end, hawke2020urban, codevilla2018end}, to healthcare and medical decision making~\citep{ching2018opportunities, topol2019high}. While their performance in these domains remains unmatched, modern NNs still encounter sudden, unexpected, and inexplicable failures that are often catastrophic -- especially in safety-critical environments. These failures are largely due to systemic issues that propagate throughout the entire modern AI lifecycle, from imbalances \citep{he2009learning, buda2018systematic} and noise \citep{beigman2009learning} in data that lead to algorithmic bias \citep{bolukbasi2016man,caliskan2017semantics, buolamwini2018gender,chen2018my, obermeyer2019dissecting, seyyed2021underdiagnosis} to predictive uncertainty \citep{kendall2017uncertainties, kompa2021second, nado2021uncertainty, amini2020deep} that plagues model performance on unseen or out-of-distribution data. In order to realize the widespread adoption of AI in society, NN models must not only identify these potential failure modes, but also effectively use this awareness to obtain unified and calibrated measures of risk and uncertainty. There is thus a critical need for unified systems that can estimate quantitative risk metrics for any NN model, and in turn integrate this awareness back into the learning lifecycle to improve robustness, generalization, and safety.

\begin{figure*}[t!]
\vspace{-10pt}
\centering
% \includegraphics[width=\linewidth]{figures/teaser.pdf}
\includegraphics[width=1\linewidth]{figures/Teaser.pdf}
\caption{\textbf{\Capsa}~ unifies state-of-the-art algorithms for quantifying neural network risks ranging from (A) under-representation bias, (B) epistemic (model) uncertainty, and (C) aleatoric uncertainty (label noise). \Capsa~ converts existing models into risk-aware variants, capable of identifying risks efficiently during training and deployment. }
% \label{fig:teaser}
\vspace{-10pt}
\end{figure*}


Existing algorithmic approaches to risk quantification narrowly estimate a singular form of risk in AI models, often in the context of a limited number of data modalities~\citep{nix1994estimating, kendall2017uncertainties, lakshminarayanan2017simple,buolamwini2018gender, zhang2018mitigating, gilitschenski2019deep}. These methods present critical limitations as a result of their reductionist, ad hoc, and narrow focus on single metrics of risk or uncertainty. However, generalizable methods that provide a larger holistic awareness of risk have yet to be realized and deployed~\citep{nado2021uncertainty, tran2022plex}. This is in part due to the significant engineering changes required to integrate an individual risk algorithm into a larger machine learning system~\citep{tran2016edward, dillon2017tensorflow, bingham2019pyro, shi2017zhusuan}, which in turn can impact the quality and reproducibility of results. The lack of a unified approach for composing different risk estimation algorithms or risk-aware models limits the scope and capability of each algorithm independently, and further limits the robustness of the system as a whole. A general, model-agnostic framework for extending NN systems with holistic risk-awareness, covering both uncertainty and bias, would advance the ability and robustness of end-to-end systems.

To address these fundamental challenges, we present \capsa{} -- an algorithmic framework for wrapping any arbitrary NN model with state-of-the-art risk-awareness capabilities. By decomposing the algorithmic stages of risk estimation into their core building blocks, we unify different algorithms and estimation metrics under a common data-centric paradigm. Additionally, because \capsa{} allows the underlying NN to be aware of a variety of risk metrics in parallel, we achieve improved performance and quality in risk estimation through principled redundancy, and open the door to achieving a unified composition and hierarchical understanding of NN risk.

In summary, the key contributions of this paper are:
\begin{enumerate}
    \item \Capsa{}, a flexible, and easy-to-use framework for equipping any given neural network with calibrated awareness of different forms of risk -- including bias, label noise, and predictive uncertainty;  
    \item An algorithm for decomposing different types of risk and their estimation methods into modular components that can in turn be integrated and composed together to achieve greater accuracy, robustness, and efficiency; and
    \item Empirical validation of \capsa{} on a range of dataset complexities and modalities, along with the application of \capsa{} for mitigation of algorithmic bias, identification of label noise, and detection of anomalies and out-of-distribution data.
\end{enumerate}

We refer readers to \texttt{{Capsa Pro}} \citep{capsa-pro} for information on the software library with the full functionality described in this publication.

%\section{Related Work}
%depending on which track we decide on, topics of RW should be adjusted

%\textbf{Uncertainty Estimation}

%\textbf{Bias and Density Estimation}

%\textbf{Risk-aware Neural Networks}

\section{Background and Methodology}
\begin{figure*}[b!]
\vspace{-10pt}
\centering
% \includegraphics[width=\linewidth]{figures/teaser.pdf}
\includegraphics[width=\linewidth]{figures/Overview.pdf}
\caption{\textbf{Overview of \Capsa~ architecture}. (A) \Capsa~ converts arbitrary NN models into risk-aware variants, that can simultaneously predict both their output along with a list of user-specified risk metrics. (B) Each risk metric forms the basis of a singular model wrapper which is constructed through metric-specific modifications to the model architecture and loss function.}
\label{fig:teaser}
% \vspace{-15pt}
\end{figure*}


\subsection{Preliminaries}
% \begin{itemize}
%     \item breakdown of capsa preliminaries: (what is risk, metrics, wrappers, etc)
% \end{itemize}

We consider the problem of supervised learning, where we are given a labeled dataset of $n$ input, output pairs, $\{x, y\}_{i=1}^n$. Our goal is to learn a model, $f$, parameterized by weights, $\bm{W}$, that minimizes the average loss over the entire dataset: $\sum_i \mathcal{L}(f_{\bm{W}}(x), y)$. While traditionally, the model, a neural network, outputs predictions in the form of $\hat y = f_{\bm{W}}(x)$, we now introduce a risk-aware transformation operation, $\Phi$, which transforms a model, $f$, into a risk-aware variant, such that
%
\begin{align*}
g = \Phi_{\bm \theta}(f_{\bm W}),\\
\hat y, R = g (x),
\end{align*}
%
%\begin{equation}
%    \hat y, R = \Phi_{\bm \theta}(f_{\bm W})(x), 
%\end{equation}
%
where $R$ are the estimated ``risk'' measures from a set of metrics, $\bm \theta$. The goal of this paper is to propose a common transformation backbone for $\Phi_{\bm \theta}(\cdot)$, which automatically transforms an arbitrary model, $f$, to be aware of risks, $\bm \theta$.

All measures of risk aim to capture, on some level, the reliability of a given prediction. This can stem from the data source (aleatoric uncertainty, or representation bias) or the predictive capacity of the model itself (epistemic uncertainty). Within \capsa, we define various risk metrics to identify and measure these sources of risk. We propose the idea of \textit{wrappers}, which are instantiations of $\Phi_\theta$, for a singular risk metric, $\theta$. Wrappers are given an arbitrary neural network and, while preserving the structure and function of the network, add and modify the relevant components in the model. This allows them to serve as a drop-in replacement that is able to estimate the risk metric, $\theta$. Wrappers can be further composed using a set of metrics, $\bm \theta$, that are faster and more accurate than individual metrics. 

\subsection{Capsa: The Wrapping Algorithm}

%\begin{itemize}
%    \item discussion of the core building blocks of capsa wrapping
%    \item describe the generic wrapping algorithm for an arbitrary model / metric
%\end{itemize}
While risk estimation algorithms can take a variety of forms and are often developed in ad hoc settings, we present a unified algorithm for building $\Phi_{\bm \theta}$ in order to wrap an arbitrary neural network model. There are four main components: (1) constructing the shared feature extractor, (2) applying modifications to the existing model needed to capture the uncertainty, (3) creating additional models and augmentations if necessary, and (4) modifying the loss functions. 

The feature extractor, which defaults to the model until its last layer, can be leveraged as a shared backbone by multiple wrappers at once to predict multiple compositions of risk. This results in a fast, efficient method of reusing the main body of the model, which does not require training multiple models and risk estimation methods from scratch. Next, \capsa~ modifies the existing network according to metric-specific modifications; for example, this could entail modifying every weight in the model to be drawn from a distribution (to convert to a Bayesian neural network~\citep{blundell2015weight}) or adding stochastic dropout layers~\citep{gal2016dropout}. Depending on the metric, \capsa~ also adds new layers or augmentations to the model that predict new outputs. Note that these are not modifications to the model, but rather augmentations for the given metric; for example, new layers to output $\sigma$~\citep{nix1994estimating}, or extra model copies when ensembling~\citep{lakshminarayanan2017simple}. Lastly, we modify the loss function to capture any remaining metric-specific changes that need to be made. This entails combining the user-specified and metric-specific loss functions (e.g., KL-divergence~\citep{kingma2013auto}, negative log-likelihood~\citep{nix1994estimating}, etc). All of the following modifications are integrated together into a custom metric-specific forward pass, and train step. These are used to capture variations in the forward and backward passes during training and inference. 

\subsection{Risk Metrics and Background}
%\todo{for each metric type lets describe what it captures, the metrics that are supported in capsa, and what non-trivial (i.e., capsa-specific) changes are required for each}
In this section, we outline three high-level categories of risk which we quantitatively define and estimate.

\textbf{Representation Bias - } 
%
The representation bias of a dataset uncovers imbalance in the space of features and captures whether certain combinations are more prevalent than others. Note that this is fundamentally different from label imbalance, which only captures distributional imbalance in the labels. For example, in driving datasets, it has been demonstrated that the combination of straight roads, sunlight, and absence of traffic is higher than any other feature combinations. This  indicates that these samples are overrepresented~\citep{amini2018variational}. Similar combinations have  been identified for facial detection~\citep{buolamwini2018gender, amini2019uncovering},  medicine~\citep{puyol2021fairness, soleimany2021evidential}, and clinical trials~\citep{xu2022identifying}. Uncovering feature representation bias is a computationally expensive process as these features are (1) often unlabeled, and (2) extremely high-dimensional (e.g., images, videos, language, etc). However, they can be estimated by learning the density distribution of the data. We accomplish this by estimating densities in feature space. For high-dimensional feature spaces we estimate a low-dimensional embedding using a variational autoencoder~\citep{kingma2013auto} or by using the features from the penultimate layer of the model. Bias is then the imbalance between parts of the density space estimated either discretely (using a discretely-binned histogram) or continuously (using a kernel distribution~\citep{rosenblatt1956remarks}). 
% scretizing the feature space into bins, and calculates the joint probability of a given sample's features occurring. To do this, we extract features from the model prior to the penultimate layer (denoted as latent space $z$). Then, during training, for every sample $s$, we can calculate $P(z(s) | X) = \Pi_{i} P(z_i(s))$, where we define $z_i(s)$ as the value of feature $i$ for this specific sample $s$. We calculate $P(z_i(s))$ by accumulating frequencies of the features throughout the epoch, dividing them into evenly spaced bins, and finding the probability of each bin.

\begin{wrapfigure}[10]{R}{0.5\textwidth}
\begin{minipage}{0.48\textwidth}
\vspace{-20pt}
\begin{algorithm}[H]
    \centering
    \small
    \caption{\small{Aleatoric Uncertainty in Classification}}
    \label{alg:class-mve}
    \begin{algorithmic}[1]
        \State $\mu, \sigma \leftarrow f_{\bm{W}}(x)$ \Comment{Inference}
        \For{$i \in 1..T$} \Comment{Stochastic logits}
        \State $\tilde{z} \leftarrow \mu + \sigma \times \epsilon \sim \mathcal{N}(0, 1)$
        \EndFor
        \State $\tilde{z} \leftarrow \frac{1}{N} \times \sum_{i = 1}^{T} \tilde{z}$ \Comment{Average logit}
        \State $\hat y \leftarrow \frac{\exp(\tilde{z})}{\sum_j \exp(\tilde{z}_j)}$ \Comment{Softmax probability}
        \State $\mathcal{L}(x, y) \leftarrow -\sum_j y_j \log p_j$ \Comment{Cross entropy loss}
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\end{wrapfigure}

\textbf{Aleatoric Uncertainty- }  
%
Aleatoric uncertainty captures noise in the data, e.g., mislabeled datapoints, ambiguous labels, classes with low separation, etc. We model aleatoric uncertainty using Mean and Variance Estimation (MVE)~\citep{nix1994estimating}. In the regression case, we pass the outputs of the model's feature extractor to another layer that predicts the standard deviation of the output. We train using NLL, and use the predicted variance as an estimate of the aleatoric uncertainty. We apply a modification to the algorithm to generalize to the classification case in ~\alg{alg:class-mve}. We assume the classification logits are drawn from a normal distribution and stochastically sample from them using the reparametrization strategy. We average stochastic samples and backpropogate using cross entropy loss through logits and their inferred uncertainties.



\textbf{Epistemic Uncertainty- }
%
Epistemic uncertainty measures uncertainty in the model's predictive process -- this captures scenarios such as examples that are "hard" to learn, examples whose features are underrepresented, and out-of-distribution data. We provide a unified approach for a variety of epistemic uncertainty methods ranging from Bayesian neural networks~\citep{blundell2015weight}, ensembling~\citep{lakshminarayanan2017simple}, and reconstruction-based~\citep{kingma2013auto} approaches. Below, we outline three metrics and how they fit into \capsa's unified risk estimation framework.  

A \textit{Bayesian neural network} can be approximated by stochastically sampling, during inference, from a neural network with probabilistic layers~\citep{blundell2015weight, gal2016dropout}. Adding dropout layers~\citep{srivastava2014dropout} to a model is one of the simplest ways to capture epistemic uncertainty~\citep{gal2016dropout}. To calculate the uncertainty, we run $T$ forward passes, which is equivalent to Monte Carlo sampling. Computing the first and second moments from the $T$ stochastic samples yields a prediction and uncertainty estimate, respectively. 
%Within \capsa, we add dropout layers after every fully connected or convolutional layer in the feature extractor. 

An \textit{ensemble} of $N$ models, each a randomly initialized stochastic sample, is a common approach used to accurately estimate epistemic uncertainty~\citep{lakshminarayanan2017simple}. However, this comes with significant computational costs. To reduce the cost of training ensembles, \capsa~ automates the construction and management of the training procedure for all members and parallelizes their computation. 

\textit{Variational autoencoders (VAEs)} are typically used to learn a robust, low-dimensional representation of the latent space. They can be used as to estimate epistemic uncertainty by using the reconstruction loss $MSE(\hat{x}, x)$. In cases of out-of-distribution data, samples that are hard to learn, or underrepresented samples, we expect that the VAE will have high reconstruction loss, since the mapping to the latent space will be less accurate. Conversely, when the model is very familiar with the features, or the data is in distribution, we expect the latent space mapping to be robust and the reconstruction loss to be low. To construct the VAE for any given model in \capsa, we use the feature extractor as the encoder, and reverse the feature extractor automatically when possible to create a decoder. 

\subsection{Metric Composability}
%\todo{what composability means in this context, and the algorithm for composing two metrics together}
%\todo{composability of multiple metrics (wiring)}

Using \capsa, we compose multiple risk metrics to create more robust estimates (e.g., by combining multiple metrics, or alternatively by capturing different measures of risk independently). By using the feature extractor as a shared common backbone, we can optimize for multiple objectives, ensemble multiple metrics, and obtain different types of uncertainty estimates simultaneously.

We propose a novel composability algorithm within \capsa~ to automate this process. Again, we leverage our shared feature extractor as the common backbone for all metrics and incorporate all model modifications. Then, we apply the new model augmentations either in series or in parallel, depending on the use case (i.e., we can ensemble a metric in series to average the metric over multiple joint trials, or we can apply ensembling in parallel to estimate a independent measure of risk). Lastly, the model is jointly optimized using all of the relevant loss functions by computing the gradient of each one with regard to the shared backbone’s weights and stepping into the direction of the accumulated gradient.

% For example, to combine dropout and VAEs, we first insert dropout layers into the feature extractor. Then, the decoder is added to the VAE, and the feature extractor and decoder are trained using the MSE and KL loss, but the feature extractor is also trained using the user-inputted loss for the dropout metric. At inference time, we calculate the dropout uncertainty and the VAE uncertainty separately as described above, and return a weighted sum of the two as our final uncertainty estimate. 

\section{Experimental Results}
In the following section, we analyze the risk metrics obtained by wrapping various models with \capsa~ on several datasets. We show that \capsa~ provides accurate, scalable, composable risk metrics that are efficient and can be used to quantify bias, aleatoric, and epistemic uncertainty using multiple methods. 
\begin{figure}[t!]
\centering
% \vspace{-25pt}
\includegraphics[width=\linewidth]{figures/TopBottomK_Face.pdf}
\caption{\textbf{Bias and Epistemic Uncertainty on Faces} (A) Under-represented and over-represented faces in the Celeb-A dataset found by \capsa~ using the VAE and HistogramBias wrappers. As the percentile bias of the data increases, the skin tone gets lighter, lighting gets brighter, and hair color gets lighter, and (B) accuracy on these datapoints increases. We also determine the points with the highest epistemic uncertainty, which have artifacts such as sunglasses, hats, colored lighting, etc.}
% \vspace{-25pt}
\label{fig:top-bottom-k-bias}
\end{figure}

\subsection{Representation Bias}
%\begin{itemize}
%    \item topK and bottomK images for mnist and celeb-A faces.
%    \item percentile visualization of images shows that as the bias gets %higher, skin tone gets lighter and lighting gets better.
%    \item lighter faces, lighter backgrounds lighter hair color, and %face position towards camera are some features that Celeb-A is biased %towards
%    \item dataset is biased against darker skin tone, poor lighting %conditions, darker hair color 
%    \item mnist is biased towards thinner, curlier handwriting as %opposed to thicker, more angular handwriting
%\end{itemize}

Using \capsa's bias and epistemic wrapper capabilities, we analyzed the Celeb-A \citep{liu2015faceattributes} dataset. The task for the neural network was to detect faces from this dataset against non-face images (collated from various negatives in the ImageNet dataset).  \fig{fig:top-bottom-k-bias}A quantifies an accuracy vs bias tradeoff that neural networks exhibit, where they tend to perform better on overrepresented training features. We used a VAE as a feature extractor to demonstrate that the it can be used for single-shot bias and epistemic uncertainty estimation without any added computation cost.
    
\fig{fig:top-bottom-k-bias}B qualitatively inspects the different percentiles of bias ranging from underrepresentation (left) to overrepresentation (right). We found that the underrepresented samples in the dataset commonly contained darker skin tones, darker lighting, and faces not looking at the camera. As the percentile of the bias gets higher, we see that the dataset is biased towards lighter skin tones, hair colors, and a more uniform facial direction. With our approach, we highlight a critical difference between bias and epistemic estimation methods in \fig{fig:top-bottom-k-bias}C. The samples estimated to have the highest epistemic uncertainty were not necessarily only underrepresented, but also contain features that obscure the predictive power of the model (e.g., faces with colored lighting, covering masks, and artifacts such as sunglasses and hats). 


\subsection{Aleatoric Uncertainty}
\label{res:aleatoric}

\begin{wrapfigure}[22]{R}{0.4\textwidth}
\centering
\vspace{-15pt}
\includegraphics[width=1\linewidth]{figures/Fahion_MNIST_Aleatoric.pdf}
\vspace{-10pt}
\caption{\textbf{Fashion MNIST Aleatoric Uncertainty} (A) Randomly selected samples from two classes of fashion-mnist. These samples are visually distinguishable, and have a low aleatoric uncertainty, as opposed to (B), which shows samples with highest estimated aleatoric noise. It is not clear what features distinguish these shirts from tshirts/tops, as they have similar necklines, sleeve lengths, and cuts.}
\label{fig:fashion-mnist-aleatoric}
\vspace{-40pt}
\end{wrapfigure}

%\begin{itemize}
%\item our method detects noise in datasets such as fashionMNIST
%\item two classes with very high overlap are 't-shirt/top' and 'shirt'. %In the samples with highest aleatoric uncertainty, we can see that light, sleeveless tops with similar necklines are classified as "tshirt/top" and "shirt" with minimal visual differences. Short-sleeved shirts with round necklines are also classified as either category, indicating that labelers should look more closely at these items as they may introduce unnecessary noise into the model.
%with high aleatoric uncertainty show visual similarities (would be difficult for humans to differentiate between the two)
%\end{itemize}

Next, we experiment on \capsa's ability to successfully detects label noise in datasets using aleatoric uncertainty estimation. An example of this can be shown in Fashion-MNIST, which contains two very similar classes: ``tshirt/top'' and ``shirt''. The methods presented in \capsa~ identify samples in Fashion-MNIST with high aleatoric uncertainty, which are light sleeveless tops with similar necklines with minimal visual differences. Short-sleeved shirts with round necklines are also classified as either category. Compared to randomly selected samples from these two classes, the samples considered noisy by \capsa~ are visually indistinguishable, and difficult for humans (and models) to categorize. 
%This indicates that labelers should look more closely at these items as they may introduce unnecessary noise into the model.




\subsection{Epistemic Uncertainty}
%\begin{itemize}
%    \item we compose wrappers within epistemic (dropout + ensemble) and 
%    across aleatoric and epistemic (mve + dropout/mve/ensemble) and achieve robust performance
%    \item when combining mve with epistemic methods, we can choose to either predict solely epistemic or solely aleatoric uncertainty (often with better results, since the aleatoric is now averaged across multiple runs) or we can combine the two by treating the N predictions as a mixture model of Gaussians and finding the mean and variance of the mixture
%    \item when combining two epistemic methods together (VAE and/or Ensemble and/or Dropout), we can do the same thing or average across the uncertainties for a more robust measurement (particularly useful for VAE and Dropout combination, since this likely yields better results than ensembling but with less compute)

%\end{itemize}
In this section, we benchmark \capsa's epistemic methods on toy datasets. We demonstrate how \capsa's ability to compose multiple methods (e.g.,  dropout and VAEs) can achieve more robust, efficient performance. We combine aleatoric methods with epistemic methods (i.e., ensembling the MVE metric) to strengthen aleatoric methods, since they are averaged across multiple runs. We can also treat the ensemble of MVEs as a mixture of normals. Similarly, to combine VAE and dropout, we use a weighted sum of their variances or we run the VAE $N$ times with dropout layers and treat multiple runs as $N$ normals.

\subsubsection{Cubic Dataset and UCI Benchmarking}

\begin{figure}[t!]
\centering
\includegraphics[width=1\linewidth]{figures/Cubic.pdf}
\vspace{-15pt}
\caption{\textbf{Risk metrics on cubic regression.} A regression dataset $y = x + \epsilon$, where $\epsilon$ is drawn from a Normal centered at $x=1.5$. Models are trained on $x \in [-4,4]$ and tested on $x \in [-6,6]$. Composing using MVE results in a single metric that can seamlessly detect epistemic and aleatoric uncertainty without any modifications to the model construction or training procedure.}
\vspace{-10pt}
\label{fig:cubics}
\end{figure}

%\begin{itemize}
%    \item examples of cubics with aleatoric noise added and uncertainty in areas of with no training data. all combined methods (MVE + an epistemic method) can detect both uncertainties with high accuracy.
%\end{itemize}
We compose various epistemic and aleatoric methods on a cubic dataset with injected aleatoric noise and a lack of data in some parts of the test set. We train models on $y = x + \epsilon$, where $\epsilon \sim \mathcal{N}(1.5, 0.9)$. Training data is within $[-4,4]$ and test within $[-6,6]$. \fig{fig:cubics} demonstrates that composed metrics can successfully detect regions with no data, as well as the aleatoric uncertainty in the center.

Additionally, we benchmark raw epistemic uncertainty methods on real-world regression datasets, and evaluate VAEs, ensembles, and dropout uncertainty on these datasets based on Root Mean Squared Error (RMSE) and negative log-likelihood (NLL) in ~\tab{tab:uci-benchmarks}. More composability results, as well as training times for all methods, are available in the appendix in ~\tab{tab:training-times} and ~\tab{tab:vae-dropout}. 
\input{uci_benchmark_table}


\subsubsection{Depth Estimation}

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figures/DepthResults.pdf}
\caption{\textbf{Risk estimation on monocular depth prediction}. (A) Example pixel-wise depth predictions and uncertainty. Model uncertainty calibration for individual metrics (B) and composed metrics (C). OOD detection assessed via AUC-ROC (D) and a full p.d.f. histogram (E).}
\label{fig:depth}
% \vspace{-15pt}
\end{figure}

In this section, we transition to more complex models and datasets and demonstrate how \capsa~ can be used as a large-scale risk and uncertainty benchmarking framework for existing methods. To that end, we train a U-Net style model on the task of monocular end-to-end depth estimation (see ~\tab{tab:depth-benchmark}). Importantly, \capsa~ works ``out of the box'' without requiring any modifications since it is a highly configurable, model-agnostic framework with modularity as one of the core of its design principles.

Specifically, we take a U-Net style model whose final layer outputs a single $H \times W$ activation map and wrap it with \capsa. We then train the wrapped model on NYU Depth V2 dataset~\citep{Silberman:ECCV12} (27k RGB-to-depth image pairs of indoor scenes) and evaluate on a disjoint test-set of scenes. Additionally, we use outdoor driving images from ApolloScapes~\citep{liao2020dvi} as OOD data points.

\input{depth_benchmark_table}

We see that when we wrap the model with an aleatoric method in \fig{fig:depth_mve},  we can successfully detect label noise or mislabeled data. The model exhibits increased aleatoric uncertainty on object boundaries. Indeed, we see that the ground truth has noisy labels particularly along the edges of objects which could be due to sensor noise or motion noise.

With dropout (\fig{fig:depth_dropout}) or ensemble (\fig{fig:depth_ensemble}) wrappers, we capture uncertainty in the model's prediction. We see that increased epistemic uncertainty roughly corresponds to the semantically and visually challenging pixels where the model returns erroneous output.

\section{Applications}
The benefits of seamlessly and efficiently integrating a variety of risk estimation methods into arbitrary neural models extends far beyond benchmarking and unifying these algorithms. In this section, we outline critically important applications that are possible with the estimation abilities in \capsa.

%\note{could be combined with the previous section (i.e. describe exp1+result1 $\rightarrow$ exp2+result2 $\rightarrow$ ...) or kept separate. usually keeping separate is preferred for clarity but in this case we might not have a choice since each exp/result pair is quite distinct (not overlapping)}
\subsection{Debiasing Facial Recognition Systems}

Using the bias tools provided by \capsa, one application is to not only estimate and identify imbalance in the dataset (which we show also leads to performance bias) but to actively reduce the performance bias by adaptively re-sampling datapoints depending on their estimated representation bias during the course of training. As shown in \fig{fig:face-probabilities}, using \capsa~, we pinpoint exactly which samples need under/oversampling, and therefore can intelligently resample from the dataset during training. The benefits of this are twofold -- we can improve sample efficiency by training on less data if some data is redundant, and we can also oversample from areas of the dataset where our latent representation is more sparse. 

By composing multiple risk metrics together (in this case, VAEs and histogram bias) we can achieve even greater robustness during training, more sample efficiency, and combine epistemic uncertainty and bias to reduce risk.

\begin{figure}[t!]
\centering
\includegraphics[width=1\linewidth]{figures/FaceProbabilities_Horizontal.pdf}
\caption{\textbf{Debiasing Facial Recognition Systems} (A) Facial datasets are overwhelmingly biased towards light-skinned females. (B) The feature combinations present in dark-skinned males make up only 1.49\% of the dataset, and those present in dark-skinned female faces only take up 8.18\% of the dataset. Since \capsa{} identifies underrepresented datapoints, we can implement smart sampling schemes that increase the representation of these feature combinations.}
\label{fig:face-probabilities}
\end{figure}


\subsection{Detecting mislabeled examples}
%\todo{Update section to reflect the new experiment, current text is for the old exp. pls also reference A/B in the figure 8}
Another application of \capsa~ is cleaning mislabeled or noisy datasets. We previously described \capsa's ability to find noisy labels with high accuracy in \sect{res:aleatoric}. In the following experiment, we replaced a random collection of the 7s in the MNIST dataset with 8s. As shown in \fig{fig:aleatoric-mnist}(A), the samples with high aleatoric uncertainty are dominated by the mislabeled examples, and also include a naturally mislabeled sample. 
We further test \capsa's sensitivity to mislabeled datasets by artificially corrupting our labels with varying levels of probability $p$. In \fig{fig:aleatoric-mnist}B, as $p$ increases, the average aleatoric uncertainty per class also increases. These experiments highlight \capsa's capability to serve as the backbone of a dataset quality controller and cleaner, due to its high-fidelity aleatoric noise detection.


\begin{figure*}[t!]
\centering
\includegraphics[width=\linewidth]{figures/AleatoricMNIST.pdf}
\caption{\textbf{Mislabeled Examples in the MNIST dataset} (A) If we purposefully inject label noise into the MNIST dataset by labeling 20\% of the 7s in the dataset as 8, the mislabeled items have the highest aleatoric uncertainty. We also find a naturally mislabeled sample in the dataset. (B) As the percentage of mislabeled items increases, the average measured aleatoric uncertainty per class also increases.}
\label{fig:aleatoric-mnist}
\end{figure*}

\subsection{Anomaly and Adversarial Noise Detection}

Another application of the “uncertainty estimation” functionality provided by \capsa~ is anomaly detection. The core idea behind this approach is that a model's epistemic uncertainty on out-of-distribution (OOD) data is naturally higher than the same model’s epistemic uncertainty on in-distribution (ID) data. Thus, given a risk aware model, we visualize density histograms of per image uncertainty estimates provided by a model on both ID (unseen test-set for NYU Depth V2 dataset) and OOD data (ApolloScapes) (see \fig{fig:depth}E). At this point, OOD detection is possible by a simple thresholding. We use AUC-ROC to quantitatively assess the separation of the two density histograms, a higher AUC indicates a better quality of the separation (see \fig{fig:depth}D) .

It is critical for a model to recognize that it is presented with an unreasonable input (e.g., OOD). This capability could be used for autonomous vehicles that yield control to humans when model performance is expected to be poor. For example, in \fig{fig:adversarial}A we see that depth estimates degrade as images drift from the distribution. We are able to detect these shifts and can use this information to avoid incorrect predictions. 

Further, the approach described above could be used to detect adversarial attacks (perturbations). In \fig{fig:adversarial}A we see that even though the perturbed images are not immediately distinguishable to a human eye, the method described above successfully detects the altered input images.

Another way of interpreting the adversarial perturbations is as a way of gradually turning ID datapoints into OOD. Such a granular control allows for enhanced model introspection. In \fig{fig:adversarial} we see that as the epsilon of the perturbation increases, the density histograms of per image uncertainty estimates on both the ID and perturbed images become more disentangled (B) and thus the quality of separation increases (C).

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{figures/Adversarial_Depth.pdf}
\caption{\textbf{Robustness under adversarial noise} Across increasing levels of adversarial perturbations: (A) Pixel-wise depth predictions and uncertainty visualizations, (B) Density histograms of per image uncertainty, (C) OOD detection assessed via AUC-ROC, (D) Calibration curves.}
\label{fig:adversarial}
% \vspace{-15pt}
\end{figure}

\section{Conclusions}

In this paper, we present a unified, model-agnostic framework for risk estimation, which allows for seamless and efficient integration of uncertainty estimates in a couple lines of code. Our approach opens new avenues for greater reproducibility and benchmarking of risk and uncertainty estimation methods. We validate the scalability and the convenience of the framework on a variety of datasets. We showcase how our method can compose different algorithms together to quantify different risk metrics efficiently in parallel. We demonstrate how the obtained uncertainty estimates can be used for downstream tasks. We further show how the framework yields interpretable risk estimation results that can provide a deeper insight into decision boundaries of NNs. We refer readers 
%to the open-source version of the \Capsa~ library \cite{capsa-lite} for an introductory description of this approach and to 
\texttt{{Capsa Pro}} \citep{capsa-pro} for details regarding our comprehensive implementation of the functionality described in this publication. \Capsa~ has the goal of accelerating and unifying advances in the areas of uncertainty estimation and trustworthy AI. In the future, we plan to extend our approach to other data modalities including irregular types (graphs) and temporal data (sequences), as well as to support other model types and other risk metrics.


% reference example: \citep{tran2022plex}


% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{refs,refs_evidence}
\bibliographystyle{iclr2023_conference}

\clearpage
\appendix
\section{Appendix}

We see that when we wrap the model with an aleatoric method in \fig{fig:depth_mve}, we can successfully detect label noise or mislabeled data. E.g., in the 4th row on the left we see that the ground truth label has a mislabeled blob of pixels near the right shoulder of a person. The wrapped model is able to detect this and selectively assign high aleatoric uncertainty to this region while leaving the correctly labeled parts of the image ``untouched''.

\begin{table}[h!]
\resizebox{1\textwidth}{!}{

\begin{tabular}{@{}lllll@{}}
\toprule
               & Model Modifications                                       & New Models                    & Loss Changes                                                                 & Uncertainty Estimation                  \\ \midrule
Ensemble       & None                                                      & Train N - 1 additional models & N loss functions and N optimizers                                            & Uncertainty Estimation (Classification) \\
MVE            & Adding layers for sigma                                   & None                          & Train using NLL for regression, train on perturbed logits for classification & Predicted variance                      \\
VAE            & None                                                      & Adding decoder model          & MSE + KL-loss                                                                & MSE(x, \textbackslash{}hat\{x\})        \\
Dropout        & Dropout layers after fully connected/convolutional layers & None                          & None                                                                         & Variance of T forward passes            \\
Histogram Bias & Calculate histogram after every batch                     & None                          & None                                                                         & Joint probability of sample features    \\ \bottomrule
\end{tabular}
}
\end{table}

\begin{table}[h!]
\centering
\caption{\textbf{}{VAE + Dropout composability} Results of composability experiments on UCI datasets. The NLL reduces drastically for most datasets between pure VAE and VAE + Dropout, and the RMSE remains competitive, showing that composability improves uncertainty estimation quality.}
\label{tab:vae-dropout}
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{@{}lcc@{}}
\toprule
            & \textbf{RMSE} & \textbf{NLL}   \\ \midrule
Boston      & 2.278 ± 0.113 & 2.292 ± 0.056  \\
Power-Plant & 4.323 ± 0.017 & 2.891 ± 0.010  \\
Yacht       & 1.630 ± 0.241 & 2.081± 0.072   \\
Concrete    & 6.489 ± 0.067 & 3.396 ± 0.036  \\
Naval       & 0.000 ± 0.000 & -2.305 ± 0.144 \\
Energy      & 1.653 ± 0.200 & 2.027 ± 0.068  \\
Kin8nm      & 0.087 ± 0.000 & -1.000 ± 0.036 \\
Protein     & 4.469 ± 0.025 & 3.156 ± 0.189  \\ \bottomrule
\end{tabular}
}
\end{table}



\begin{table*}[h!]
\centering
\caption{Training times in seconds of different metrics and composability schemes on real-world regression datasets. }
\label{tab:training-times}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{@{}lcccc@{}}
\toprule
            & \textbf{Ensembles} & \textbf{Dropout} & \textbf{VAE} & \textbf{VAE + Dropout} \\ \midrule
Boston      & 11.4 ± 1.1         & 5.9 ± 1.1        & 7.8 ± 0.1    & 7.8 ± 0.2              \\
Power-Plant & 80.2 ± 1.2         & 31.3 ± 0.6       & 51.1 ± 1.3   & 54.6 ± 1.0             \\
Yacht       & 32.4 ± 0.7         & 14.01 ± 0.1      & 19.6 ± 0.5   & 22.1 ± 0.3             \\
Concrete    & 104.8 ± 2.6        & 41.4 ± 0.6       & 57.8 ± 1.5   & 61.3 ± 0.9             \\
Naval       & 58.3 ± 0.4         & 20.0 ± 0.3       & 33.2 ± 0.5   & 35.3 ± 0.1             \\
Energy      & 87.1 ± 0.8         & 31.7 ± 0.7       & 43.9 ± 1.3   & 46.2 ± 1.0             \\
Kin8nm      & 857.0 ± 57.0       & 310.6 ± 4.3      & 440.3 ± 5.8  & 457.4 ± 11.3           \\
Protein     & 96.2 ± 0.8         & 35.7 ± 0.6       & 59.6 ± 1.0   & 64.5 ± 1.0             \\ \bottomrule
\end{tabular}
}
\end{table*}

% \begin{figure*}
%  \includegraphics[width=.48\linewidth]{figures/depth/base_test.pdf} \hfill
%  \includegraphics[width=.48\linewidth]{figures/depth/base_ood.pdf} \\
% %  \vspace{5mm}
% \caption{Something something, overall}
% \end{figure*}

\begin{figure*}[h!]
 \includegraphics[width=.48\linewidth]{figures/depth/mve_test.pdf} \hfill
 \includegraphics[width=.48\linewidth]{figures/depth/mve_ood.pdf} \\
%  \vspace{5mm}
\caption{MVE Wrapper}
\label{fig:depth_mve}
\end{figure*}

\begin{figure*}[h!]
 \includegraphics[width=.48\linewidth]{figures/depth/dropout_test.pdf} \hfill
 \includegraphics[width=.48\linewidth]{figures/depth/dropout_ood.pdf} \\
%  \vspace{5mm}
\caption{Dropout Wrapper}
\label{fig:depth_dropout}
\end{figure*}

\begin{figure*}[h!]
 \includegraphics[width=.48\linewidth]{figures/depth/ensembles_ood.pdf} \hfill
 \includegraphics[width=.48\linewidth]{figures/depth/ensembles_ood.pdf} \\
%  \vspace{5mm}
\caption{Ensembles Wrapper}
\label{fig:depth_ensemble}
\end{figure*}

\begin{figure*}[h!]
 \includegraphics[width=.39\linewidth]{figures/depth/vae_test.pdf} \hfill
 \includegraphics[width=.39\linewidth]{figures/depth/vae_ood.pdf} \\
%  \vspace{5mm}
\caption{VAE Wrapper}
\label{fig:depth_vae}
\end{figure*}



\end{document}
