\section{Introduction}

% 我理解我们最要卖的点是用事实数据集自动生成模型特定的“幻觉检测”数据集，因此“幻觉”、“幻觉检测”是什么要说清楚，以及为什么需要“幻觉检测”数据集（因为现在对不同模型都需要单独标，这很费时间），然后我们就提出可以用事实检测数据集来自动生成幻觉数据集，然后基于用我们方法生成的幻觉检测数据集的方法，我们又提出了一个检测方法，能显著超过之前的检测方法。


% intro写几个内容
% 1. 和你目前写的一样，先介绍现在LLM受到了很大的关注，它有很多很多强大的功能，如****。
% 2. 诸如这么强大的LLM，它也会面临一个问题，就是幻觉问题。这里要开始定义什么是幻觉，我们认为幻觉是模型生成了与事实不符的内容（这里要写清楚，不是简单的做“分类”，生成True和False，是要看模型生成的一段文本，是个“生成”任务，生成的内容不符合事实，就可以认为模型产生了幻觉，要 cite一些paper，我记得人大是有一篇把幻觉分成了几个类别的，看看能不能从那个里面套一下定义）。后面要举个例子，画个图，比如问模型一个问题，然后把模型的幻觉输出写出来，展示一下幻觉这个问题。
% 3. 现在有很多幻觉检测的方法（cite selfcheck等等），它们旨在去检测一个模型生成的回答是否产生了幻觉，它们一般的流程是对一个要评测模型，让它对某个数据集如事实检测数据集、QA数据集等产生一些回答，然后人工标注这些回答中哪些是幻觉哪些不是幻觉，以形成一个当前模型特定的“幻觉检测”数据集，然后再在这个数据集上去实验它们的方法，这样对于每个模型都需要去标注数据集，如果模型升级了（如ChatGPT会不定期更新），幻觉数据集就得重新做。因此需要自动化生成幻觉检测数据集。而目前的事实检测数据集刚好符合我们的要求，事实性数据集通常包含一个事实性句子和对应的证据evidence，可以让模型基于幻觉检测数据集也根据句子生成依据（这里要说明reference这个点），然后将依据和原数据集中的证据进行对比，就生成幻觉数据集。
% 因此，本文的贡献可以分为：
% 我们基于事实检测数据集可以快速为模型自动生成一个特定的幻觉检测数据集，不需要手工标注。

% 我们同时设计了个新的幻觉检测方法，（这里细节你大概总结下），在ChatGPT、LLama上进行实验，发现我们的方法能显著优于目前的检测方法。（其实是认为selfcheckgpt对比的时候有问题，这里可以不展开说，再后面讲methodology里面幻觉检测方法的时候再说。）

% 我们对自动生成的幻觉检测数据集和设计的幻觉检测方法进行分析，知道了现在主流模型大概有多少概率会产生幻觉、主要在哪些样本上回产生幻觉等。（这一段可以等case study和ablation study出来了再好好总结）。


% background
Large language models (LLMs) such as ChatGPT~\footnote{https://chat.openai.com/}, GPT-4~\citep{openai2023gpt4}, Claude~\citep{claude} and Llama-2~\citep{touvron2023llama} have achieved widespread popularity and adoption across diverse industries and domains~\citep{sohail2023decoding,sallam2023chatgpt,chatgptapplication}. Despite their powerful capabilities, the issue of ``hallucination" poses a concern that LLMs have the tendency to generate inaccurate/fabricated information in generation tasks~\citep{zhang2023siren,ji2023survey}.
% Specifically, hallucination refers to the phenomenon where LLMs generate inaccurate/fabricated information in generation tasks~\citep{zhang2023siren,ji2023survey}.
% rather than assigning something to the wrong category in simple classification tasks.
As shown in Fig.~\ref{fig:hallucination}, ChatGPT suffers from hallucination when giving a description of the novel ``The Leopard'' by Norwegian author Jo Nesbø. It can be observed that ChatGPT makes up some plots of the novel and contains incorrect texts in the response, because the novel never mentions the presence of a ``red diamond'' at the crime scene and the ``The Snowman'' case has also been solved before. Since the current artificial intelligence relies more on LLMs, hallucinatory information indeed disturbs the enterprise security and the user trust~\citep{snowball,threatgpt}. Therefore, detecting hallucinations generated by the LLMs is of significant importance.

% powering various applications in sectors such as academia, healthcare, customer service and content creation~\citep{sohail2023decoding,sallam2023chatgpt,chatgptapplication}. % 这里再cite一些paper

% 下面we consider hallucination as ... 这样太主观的句子不要用。直接写Hallucination是什么，比如Hallucination is an undesirable phenomenon that LLMs tend to generate inaccurate/fabricated information in generation tasks.然后cite一些paper。后面 ``rather than classification tasks'' 这里看起来还是不用写，我写outline的时候没考虑到，这里看起来有些刻意，这个地方可以放到methodology强调。

\begin{figure}[h]
\centering
    \includegraphics[width=1\linewidth]{figs/gpt-3.5-hallucination.pdf}
    \caption{A hallucination example. The \color{red!80!black}{red} \color{black}{underline indicates the hallucinatory content.}}
    \label{fig:hallucination}
\end{figure}

% 这个case可以再仔细解释一下，希望模型解释一下``the leopard''，然后模型生成了什么，为什么``red diamond''是幻觉的，后面``unsolved''为什么不对

% related works
Current research efforts on hallucination detection leverage external knowledge sources~\citep{chern2023factool,gou2023critic} or just adopt a zero-resource approach, which focuses on resources inherent to the model itself~\citep{azaria2023internal,agrawal2023language,varshney2023stitch,selfcheckgpt,mundler2023self}.
% (e.g., internal state~\citep{azaria2023internal,agrawal2023language}, output probability~\citep{varshney2023stitch}, uncertainty~\citep{selfcheckgpt,mundler2023self}). 
Typically, most of these methods begin with a crowdsourced annotation, where researchers use QA datasets to have the model generate responses and then manually annotate whether the answers contain hallucinations.

However, these sort of model-specific “hallucination detection” datasets all have their own limitations. For one thing, each model requires a full annotation of the dataset. For another, such a dataset is also time-sensitive as upgrades may mitigate hallucination issues in LLMs and the old dataset is no longer applicable to the new model.

% Hence, re-collect data and label are needed but expand rich price. 



% challenge 这个不需要
% Designing such a dataset creation method faces two main challenges. i) Without human assistance, it may be difficult to identify the hallucinated content before proceeding to the hallucination detection step. 
% ii) The nature of hallucination is highly related to the model version, so automating the creation of datasets should have strong adaptability to different conditions. 

% method
Considering the above issues, this paper explores one automated generation of hallucination detection datasets. Inspired by~\cite{agrawal2023language} emphasizing the hallucinatory reference problem in LLMs, we find the possibility of automatically creating hallucination detection datasets through public fact-checking datasets. Specifically, since the existing fact-checking datasets usually consist of manually annotated claims accompanied by the ground truth labels (i.e.,  factual/unfactual), we can determine whether hallucination has occurred by generating references to the claims and exploring whether the references can infer the correct labels for the claims.

% 这一段太细了，可以不用说，想加的话可以放在方法那一节作为概述
% Therefore, we first \textbf{Generate References} about those claims using the LLM, where both grounded and hallucinated references are included. Secondly, given the generated reference, we prompt the LLM to perform \textbf{Claim Classification} (e.g., predict whether the claim is true or false) and then compare the result with the original label. We assume that hallucinations occur in reference texts that lead to misclassification of claims since LLMs are excellent classiers~\citep{stoliar2023using}. Based on this, \textbf{Hallucination Collection} can be effortlessly feasible and automatic.

% detection
In addition, we further propose a three-step zero-resource black-box hallucination detection method based on our dataset inspired by the idea of self-contradictory ~\citep{wang2022self,mundler2023self,selfcheckgpt}. Given an LLM accurately understands one claim, its randomly sampled references are less likely to contain contradictions. Therefore, it is possible to determine whether the model has generated hallucinations based on knowledge conflicts among these references. In summary, the contributions of our paper are: 

% Similar ideas of self-contradictory have been utilized in the context of open-domain text generation~\citep{mundler2023self} and summarization~\citep{selfcheckgpt}.

% 这一段也不用说，太细了。
% Thus, we step-by-step prompt LLMs to i) sample references, ii) detect self-contradictions, and iii) finally union to identify hallucinations.

% contribution

\begin{itemize}
\item We propose an approach for fast and \underline{auto}matically constructing model-specific \underline{hall}ucination datasets based on existing fact-checking datasets called \textbf{AutoHall}, eliminating the need for manual annotation.

% Meanwhile, regardless of changes in the model structure or update, this automated approach remains applicable.
\item Based on our dataset, we introduce a novel black-box hallucination detection method without external resources. Then, we evaluate its effectiveness on ChatGPT and Llama-2 models, demonstrating its superior improvements over existing detection techniques.
\item From the analysis of our experimental results, we estimate the prevalence of hallucination in LLMs at a rate of 20\% to 30\% and gain insight into what types or topics of LLM responses that tend to be hallucinatory.

% Particularly, content about technology (36.1\%) and history (31.9\%) prone to be fabricated in ChatGPT. %followed by culture (28.4\%), geography (25.3\%) and business (25.2\%). 

% Importantly, our proposed hallucination detection method based on \textbf{AutoHall} achieves the most well-round performance among all the baselines. 
\end{itemize}
