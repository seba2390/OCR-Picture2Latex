\section{Methodology}
In this section, we first formulate the definition of LLM hallucination discussed in our work. Then, we introduce our automatic dataset creation pipeline which focuses on prompting LLMs to produce ``hallucinatory references''. Finally, based on our generated datasets, we further present one zero-resource, black-box approach to recognize hallucination. 
% 第一段定义什么是幻觉
% 用个简单的数学形式定义一下幻觉是什么。
% 比如给定一个LLM模型$\mathcal{M}$，给定一个句子$X=x_1,x_2,\cdots, x_n$，给一段prompt$Q=q_1,q_2,\cdots, q_o$，需要模型根据$X$和$Q$进行回答$Y=y_1,y_2,\cdots, y_m$。
% 如果这个$Y$是事实性不正确的话，那么这个可以认为模型产生了幻觉。
\subsection{LLM Hallucination}
%介绍一下hallucination分类 
LLM hallucination can be categorized into different types~\citep{galitsky2023truth}, such as hallucination based on dialogue history, hallucination in generative question answering and general data generation hallucination. They can all be attributed to the generation of inaccurate or fabricated information.
% Considering the above cases, we try to formulate LLM hallucinations using mathematical notations as following.


Generally, for any input sentence $X =[x_1, x_2, \ldots, x_n]$ with a specific prompt $P = [p_1, p_2, \ldots, p_o]$, the large language model $\mathcal{M}$ will generate an answer $Y = [y_1, y_2, \ldots, y_m]$, denoted as:
\begin{equation}
\mathcal{M}(P, X) = Y.
\end{equation}
Given factual knowledge $F=[f_1,f_2,..,f_t]$, the problem of hallucination $H$ occurs when there is a factual contradiction between the output span $Y_{[i:j]}=[y_i,y_{i+1},\dots,y_j]$ and the knowledge span $F_{[u:v]}=[f_u,f_{u+1},\dots,f_{v}]$, which can be summarized into the function below:
\begin{equation}\label{halluFunc}
% Y \in H\Leftrightarrow\exists Y_{i,j} \exists F_{u,v} ( Y_{i,j}\nleftrightarrow F_{u,v}).
Y \in H\Leftrightarrow\exists Y_{[i:j]} \exists F_{[u:v]} (( Y_{[i:j]}\land F_{[u:v]} = \text{False})).
\end{equation}

% 检测方面要先写一个前提，类似于self-check的摘要里面的那段话，模型对于有幻觉的reference，再以相似的prompt生成reference，大概率会有冲突，但但对于没幻觉的reference，以相似的referen生成的prompt冲突较少。（这个点就需要后面的补充实验的验证）。
% ！！！下面两段我写的有问题，根据目前做的细节改。用数学符号表达出来。


\subsection{AutoHall: Automatic Generation of Hallucination Datasets}
% 第二段介绍我们生成数据集的方法
% 对于一个模型，现在生成$Y$的事实正确性通常要人标，但是我们认为可以利用事实性数据集来生成模型的幻觉检测数据集。通常事实性检测数据集包含几个部分，一段话claim、一个事实正确性的label（正确or错误、support还是不support等）、还有evidence。
% 然后我们将claim作为X输入到模型，将prompt定义成"需要模型生成对应的reference"（这里就写你用的prompt），输出的$Y$和evidence进行对比，（这里对比我记得也是用LLM做的？）如果LLM认为有冲突，那么模型是有幻觉的。

Current research on hallucination detection mostly relies on manually annotated datasets. Namely, whether $Y$ is hallucinatory requires slow and costly manual tagging due to the absence of a comparison standard for the factuality. However, the fact-checking datasets provide us with data typically comprising real-world claims, corresponding ground truth labels, and evidence sentences as shown in Fig.~\ref{fig:dataset}. We can prompt a model to generate relevant references for claims and then use the ground truth labels as criteria to assess the hallucinatory nature of the generated references. Specifically, as shown in Fig.~\ref{fig:dataset}, \textbf{AutoHall} generates hallucination datasets following the below three steps:

\begin{figure}[h]
\centering
    \includegraphics[width=1\linewidth]{figs/datasets.pdf}
    \caption{Our proposed approach to collect LLM hallucination automatically. \color{green!50!black}{\textbf{Green}}\color{black}{: the grounded information. }\color{red!75!black}{\textbf{Red}}\color{black}{: the incorrect information. The complete prompts are shown in Appendix~\ref{app:prompts}.}}
    \label{fig:dataset}
\end{figure}

\textbf{Step 1: References Generation.} % Public fact-checking datasets provide real world claims, ground truth labels indicating true/supported or false/unsupported and evidence retrieved from websites.
% as shown in Table~\ref{tab:factcheck}. 
For an LLM, we prompt it to generate corresponding references to the claims in the existing datasets by the prompt illustrated in Fig.~\ref{fig:dataset} Step 1. Note that to simplify the generation, we only focus on factual (supported/true) and faked (unsupported/false) claims. 
Besides, we discard references that fail to contain concrete content, like a long response beginning with ``I can not provide a specific reference for the claim you mentioned...''. 
The remaining valid references are either reliable ($\overline{H}$) or hallucinatory ($H$).

%consists of some attributes
\textbf{Step 2: Claim Classification.} Separately for each reference, in order to label 
whether a claim belongs to $\overline{H}$ or $H$, we prompt LLM to perform claim classification. %in conjunction with 
The input sequence is of format as shown in Fig.~\ref{fig:dataset} Step 2, where the two placeholders $\left\langle claim \right\rangle$ and $\left\langle reference\right\rangle$ should be replaced with the claim $X$ and the generated reference $Y$ in Step 1. 
Then the output is of format ``Category: $\left\langle category\right\rangle$ Reason (Optional): $ \left\langle reason\right\rangle$” where the category is limited to true ($T$) or false ($F$). 
To elaborate, $T$ indicates the generated reference $Y$ supports the claim $X$ is factual and $F$ represents that $Y$ demonstrates claim $X$ is faked. % $F$ follows the same logic.
We expect correct classification to each claim, while wrong classification may be taken as a sign of the existence of hallucination in the generated reference that it erroneously supports the claim's factuality. The binary classification results of LLMs are reliable as LLMs exhibit strong capabilities in natural language inference~\citep{wu2023exploring}.

\textbf{Step 3: Hallucination Collection.}
Last, we can directly adopt a simple comparison to collect the hallucination dataset. 
If the classification result is not equal to the ground truth label, we label the reference as hallucination. 
%Assigned hallucination
%by comparing the ground truth labels to the classifications. 
Meanwhile, to maintain a balanced proportion between hallucinatory and factual references, we sample the same number of factual references built upon hallucinatory ones to form a completed dataset.

\subsection{Hallucination Detection Approach}
% 第三段介绍我们检测幻觉的方法
% 这里可以说是受到（inspired by）selfcheckgpt的启发，我们也尝试让模型回答多次并且去检测这些回答之间的一致性，如果不一致的话，说明模型对生成的reference有知识冲突，出现了幻觉。但是和selfcheckgpt检测的方法有不同，selfcheckgpt是利用一些小型语言模型打分如BERTScore，或者简单的n-gram来评测多个回答之间的一致性，我们想到可以直接利用powerful的LLM来进行判断。因此我们利用LLM对比多个reference之间的一致性。（还要讲你那个reference分组的技巧吧，这个我有些忘记了，都用数学表达一下。）
% 我们的本质的幻觉检测idea和selfchekgpt是一样的，但它们在对比的时候有一些漏洞，不能全部放在一起比较，因此我们提出了一个1v1比较的方法。

% 这个idea本身有一个问题，就是假如一开始的reference是有幻觉的，那么也有可能出现后面所有的reference都是有幻觉的现象，这种情况是检测不出来的。但是我们承认是有这样一个问题，然后我们用13个reference之间的冲突证明：如果模型产生了幻觉，那么大概率后面再sample会产生很多幻觉的样本以及少量的无幻觉reference；但如果一开始的reference没有产生幻觉，那么后面大概率产生的reference都是没有幻觉的。
\begin{figure}[h]
\centering
    \includegraphics[width=1\linewidth]{figs/detection.pdf}
    \caption{Our proposed approach to detect LLM hallucination.  \color{cyan!75!magenta}{Blue}\color{black}{: the claim from fact-checking dataset. }\color{red!80!black}{Red}
    \color{black}{: the response need to be detected whether exists hallucination.} \textcolor{purple!60!blue}{Purple}: the sampled references to trigger self-contradictions. The complete Step 2 prompts are shown in Appendix~\ref{app:prompts}.}
    \label{fig:detection}
\end{figure}

The rationale for our detection approach is that if the LLM knows one claim well, even when we query it to provide multiple references, self-contradictions among them should be absent otherwise hallucination information must exist in one reference. Compared to SelfCheckGPT~\citep{selfcheckgpt}, our method uses the LLM for hallucination detection end-to-end rather than relying on output token probabilities to calculate hallucination score with BERTScore or n-gram. 

As shown in Fig.~\ref{fig:detection}, to trigger self-contradictions, we first appropriately prompt an LLM to answer a second reference $Y_k'$ and repeat this process $K$ ($K=13$ in experiments) times. It is worth noting that each query is running independently with an equivalent prompt. % prompts例子可放附录
Then, we concatenate each generated reference $Y_k' (k=1,...,K)$ with the original reference $Y$ to form one input pair. 
Unlike SelfCheckGPT measures the consistency between $Y$ and all $K$ sampled references, we invoke the LLM to detect if $Y$ and $Y_k'$ are contradictory.
Such self-contradiction detection in $\left \langle Y,Y_k'\right\rangle$ pair can focus more on the hallucination detection in $Y$ and avoid the problem that SelfCheckGPT incorrectly identifies the conflicts in the $K$ sentences generated subsequently as the hallucination in $Y$.

% However, to be honest, 
% 补充实验
% 需要一个平均值，生成的13个reference，和原本reference有冲突的平均值是多少，没冲突的平均值是多少。
%Since our work compares LMs and hallucination estimation procedures, the risk is lower compared to a system that might be deployed using our procedures to reduce hallucination. Before deploying any such system, one should perform a more thorough examination of potential biases against sensitive groups and accuracy across different research areas.

Formally, we can check if there exists at least one $Y_{k}'$ conflicting with $Y$, as shown in Eq.~(\ref{xiqu}). If conflicts are indicated, it suggests the model does not understand the claim well, and $Y$ may be hallucinatory. Conversely, if no conflicts are found in $K$ pairs, it indicates that the factual reference.
\begin{equation}\label{xiqu}
   Y \in H \Leftrightarrow \exists Y_{k,[u,v]}'\exists Y_{[i,j]}(( Y_{[i,j]}\land Y_{k,[u,v]}' = \text{False}))
   % (Y\leftrightarrow \lnot Y_1')\vee(Y\leftrightarrow \lnot Y_2') \vee ... \vee(Y\leftrightarrow \lnot Y_K')
\end{equation}


% \begin{equation}\label{xiqu}
%    % Y \in H \Leftrightarrow (Y\leftrightarrow \lnot Y_1')\vee(Y\leftrightarrow \lnot Y_2') \vee ... \vee(Y\leftrightarrow \lnot Y_K') 

% Y \in H\Leftrightarrow\exists Y_{i,j} \exists F_{u,v} (( Y_{i,j}\land F_{u,v} = \text{False})).



% \end{equation}
