\section{Experiments}
% In this section, we introduce our experimental settings and results. % 这个可以不用
%Then, we show the individual efficacy of our hallucination detection techniques in 4.2. Finally, in 4.3, we show the effectiveness of the proposed active detection approach in addressing hallucinations.
\subsection{Experimental Settings}
%We evaluate several state-of-the-art LLMs to  AutoHall benchmark.

\subsubsection{Models}
% 说用了ChatGPT，llama-2-chat（我记得应该你用的chat版本），然后可以说是ChatGPt是最好的闭源的可调API的模型。llama-2-chat是最好的开源的模型之一。这样说的话两个模型就比较合理了，不会攻击你缺失实验的models。
We conduct experiments towards the state-of-the-art open-/closed-source LLMs. For the closed-source model, we select ChatGPT, which is widely recognized as one of the leading closed-source LLMs, with the assistance of paid gpt-3.5-turbo API. We also choose Llama-2-chat (the instruction-tuned version) for the open-source LLM experiments, as it is one of the most prominent open-source models available. Based on our computing resources, we primarily run its 7B\&13B parameters versions on a server with dual Nvidia A100 80GB GPUs.

\subsubsection{Datasets and Metrics}\label{sec:dataset}
% 先说数据集 把之前那个Public fact checking datasets的表放这。
% 还要说下一些细节，比如生成了幻觉的样本（正样本）之后，又加入了相同数目的非幻觉的样本来做平衡。
% metric是幻觉检测的指标，你用的f1，precision，recall，acc。
For hallucination collection, we employ three fact-checking datasets: Climate-fever~\citep{diggelmann2020climate}, Pubhealth~\citep{kotonya2020explainable} and WICE~\citep{wice}. 
All of them provide real-world claims, ground truth labels and evidence retrieved from websites as shown in Table~\ref{tab:factcheck}. 
The topics of claims range from different domains, such as technology, culture, health and so on, which facilitates our analysis of what types or topics of content LLMs tend to be hallucinatory.

% 讲构造数据集的细节
To investigate the hallucination properties of large language models at different temperatures, we set their temperature values as 0.1, 0.5 and 0.9, to construct the hallucination dataset for each LLM.
To ensure stability in claim classification, we set the temperature value to 0.1 for the query.



% 放到实验里面
\begin{table}[htbp] 
    \centering 
    \caption{Examples of fact-checking datasets used in \textbf{AutoHall}. The ``supports'', ``true'' and ``supported'' labels represent the factually accurate claims while the ``refutes'', ``false'' and ``not\_supported'' indicate the inaccurate ones. }
    %\vspace{5mm}
    \scalebox{0.98}{
    \begin{tabular}{@{}lllll@{}}
        \toprule 
        \bf Dataset  &\bf Topic &\bf Example Claim &\bf Label & \bf Num \\
        \midrule
        \makecell[l]{Climate\\-fever} & Climate & \makecell[l]{CO2 emissions were much smaller 100 years ago.\\Ice berg melts, ocean level remains the same.} & \makecell[l]{supports\\refutes} &   \makecell[l]{654\\253}\\ %+154+474) 
        \midrule
        Pubhealth & Health & \makecell[l]{France's 20th century radium craze still haunts Paris.\\Viagra may help heart effects of muscular dystrophy.} & \makecell[l]{true\\false} &  \makecell[l]{629\\380}\\%+164+41) 
        \midrule 
        WICE & \makecell[l]{Law\\\\Art}& \makecell[l]{In 2019 Upton supported a bill banning sales \\between private individuals.\\Tiana Tolstoi is an Egyptian-born French model \\of Korean, Serbian, and Russian descent.} & \makecell[l]{supported\\\\not\_supported} & \makecell[l]{686\\\\242} \\%+1039)
        \bottomrule
    \end{tabular}}
    \label{tab:factcheck}
\end{table}

For hallucination detection, we adopt the standard classification evaluation metrics: Accuracy and F1. %说明一下把幻觉作为正类
To be clear, we treat hallucination as a positive class. Importantly, we randomly sample an equal number of factually accurate samples with the hallucinatory ones to balance \textbf{AutoHall} dataset.





\subsubsection{Baselines}
% 这里应该是比较了两个，selfcheckgpt和人大的另一个。然后分别简短的一两句话介绍它们，然后说我们在用我们的方法生成的数据集上复现了它们的结果，讲一下用它们算法适配我们数据集的过程。
We compare our detection approach with the baselines that do not use an external database:

CoT-based Self-Check in both zero-shot and few-shot settings, denoted by \textbf{Zero-Self-Check} and \textbf{Few-Self-Check}, which have demonstrated effectiveness across diverse tasks like reasoning, question answer and dialogue response~\citep{madaan2023self-refine,xue2023rcot}. 
For the zero-shot setting, we guide the LLM to incorporate chain-of-thought via the prompt ``Let’s think step by step" ~\citep{kojima2022large}. 
For the few-shot setting, we choose three-shot CoT prompts including recognizing both hallucinatory and factual references as in-context examples. 

SelfCheckGPT~\citep{selfcheckgpt} designs three methods (i.e., via BERTScore, MQAG~\citep{manakul2023mqag} and n-gram) to assess information consistency for hallucination capture. Considering n-gram with $n=1$ setting works best, we select it as the baseline, denoted by \textbf{SelfCk-1gm}. 


% 这里上面三个写完应该大半页接近一页了。

\subsection{Main Results}
\subsubsection{Hallucination Dataset Generation}
Based on the three fact-checking datasets, our \textbf{AutoHall} is separately created powered by ChatGPT, Llama-2-7b-chat and Llama-2-13b-chat. We show the scale of generated datasets at different temperatures in Table~\ref{tab:generation}.
%For each setting, among a total of approximately 3,000 queries, around 630 responses are labeled as hallucination. 
% Then we analyse the proportion of hallucination in three aspects: LLMs, datasets, temperatures. 这一句话不用写，后面直接写分析就行
%Furthermore, from Fig.~\ref{fig:prop0} and Fig.~\ref{fig:prop1}, 
It can be observed that although different temperatures and LLMs may cause slight fluctuations in the proportion of hallucination, the rate still remains at 20-30\%. We provide concise case studies to analyze when LLMs are prone to generating hallucinations in Appendix~\ref{app:cases}. 


\begin{table}[htbp]
    \centering
    \caption{Distribution of our generated \textbf{AutoHall} datasets. \#H is the number of hallucinatory references and H\% is the hallucination proportion calculated by \#H/\#Total. }
    \begin{tabular}{ccccc|cc|cc}
    \toprule	
    \multirow{3}{*}{\textbf{\makecell{Datasets
        }}} & \multirow{3}{*}{\textbf{\#Total}} & \multirow{3}{*}{\textbf{LLMs}} & 
    \multicolumn{6}{c}{\textbf{Temperature}} \\
    && &\multicolumn{2}{c}{\textbf{0.1}} & \multicolumn{2}{c}{\textbf{0.5}} & \multicolumn{2}{c}{\textbf{0.9}} \\
    \cmidrule(lr){4-9}
    && & \textbf{\#H} &\textbf{H\%} &  \textbf{\#H} &\textbf{H\%} & \textbf{\#H} &\textbf{H\%} \\
  \midrule
    \multirow{3}{*}{\makecell{Climate\\-fever}} & \multirow{3}{*}{907} & ChatGPT
   &181&19.96& 169&18.63 &185&20.40\\
%\cmidrule(lr){2-8}
    && Llama-2-7b-chat
   &174&19.18 & 164&18.08 & 175&19.29\\
%\cmidrule(lr){2-8}
    && Llama-2-13b-chat
    & 175 &19.29& 177&19.51 &184&20.29\\
\midrule
    \multirow{3}{*}{Pubhealth} & \multirow{3}{*}{1009} & ChatGPT
     &215&21.31 &205&20.32 &210&20.81\\
%\cmidrule(lr){2-8}
    && Llama-2-7b-chat
    & 216&21.41 &221&21.90 & 227&22.50\\
%\cmidrule(lr){2-8}
    && Llama-2-13b-chat
     & 192&19.03 &207&20.52& 202&20.02\\
\midrule
    \multirow{3}{*}{WICE}& \multirow{3}{*}{928} & ChatGPT  
      & 250&26.94 &254&27.37&251&27.05\\
%\cmidrule(lr){2-8}
    && Llama-2-7b-chat
     & 248&26.72&243&26.19&261&28.12\\
%\cmidrule(lr){2-8}
    && Llama-2-13b-chat
     &242&26.08& 239&25.75& 245&26.40\\
    \bottomrule
    \end{tabular}
    \label{tab:generation}
\end{table}

% \begin{figure}[htbp]
% \centering
% \begin{minipage}[t]{0.49\linewidth}
% \centering
% \includegraphics[width=1.02\linewidth]{figs/hallu_prop.pdf}
% \caption{Proportion of hallucination at temperture=0.1, 0.5 and 0.9\label{fig:prop0}} 
% \end{minipage}
% \hspace{1ex} %%两个minipage之间相隔1个字符的距离
% \begin{minipage}[t]{0.48\linewidth}
% \centering
% \includegraphics[width=1\linewidth]{figs/hallu_prop1.pdf}
% \caption{Proportion of hallucination across scenarios ChatGPT and Llama-2-chat(7B\&13B)\label{fig:prop1}} 
% \end{minipage}
% \end{figure}




\begin{table}[htbp]
    \centering
    \caption{Accuracy and F1 score of our hallucination detection method and all the compared baselines. TEMP is short for temperature and Acc. is short for the metric of accuracy. }
    \setlength\tabcolsep{3.3pt} 
    \begin{tabular}{@{}cccccccccccc@{}}
    \toprule 		
    \multirow{2}{*}{\textbf{LLMs}} & \multirow{2}{*}{\textbf{Methods}} & \textbf{Dataset} &\multicolumn{3}{c}{\textbf{Climate-fever}}&\multicolumn{3}{c}{\textbf{Pubhealth}}&\multicolumn{3}{c}{\textbf{WICE}}\\
    \cmidrule{3-12}
    &&\textbf{TEMP} &\textbf{0.1}& \textbf{0.5}&\textbf{0.9}&\textbf{0.1}& \textbf{0.5}&\textbf{0.9}&\textbf{0.1}& \textbf{0.5}&\textbf{0.9} \\
  \midrule
    \multirow{8}{*}{ChatGPT} 
    & \multirow{2}{*}{\makecell{Zero-\\Self-Check}}  & \textbf{Acc.} &55.24 &50.55&57.56&51.62&51.95&56.19&51.80&55.11&52.78
    \\
    && \textbf{F1}&25.68&22.70&31.44&20.61&21.51&31.85&20.46&28.75&25.70
    \\
    \cmidrule{2-12}
    &\multirow{2}{*}{\makecell{Few-\\Self-Check}}  & \textbf{Acc.}&54.97&49.16&54.05&51.16&51.21&51.66&51.60&54.33&52.19
    \\
    && \textbf{F1}&28.19&20.86&27.96&13.93&20.63&20.39&20.39&23.68&23.07
    \\
    \cmidrule{2-12}
    &\multirow{2}{*}{SelfCk-1gm} & \textbf{Acc.}&53.59&48.52&52.97&53.48&54.87&59.52&51.60&52.55&53.98
    \\
    && \textbf{F1}&34.88&37.85&56.28&19.35&32.23&54.54&12.31&20.46&38.40
    \\
    \cmidrule{2-12}
    &\multirow{2}{*}{\textbf{Ours}}  & \textbf{Acc.}&\textbf{64.59}&\textbf{64.79}&\textbf{64.32}&\textbf{61.16}&\textbf{63.41}&\textbf{60.71}&\textbf{63.20}&\textbf{63.58}&\textbf{65.33}
    \\
    && \textbf{F1}&\textbf{69.32}&\textbf{64.89}&\textbf{70.66}&\textbf{60.14}&\textbf{65.75}&\textbf{67.19}&\textbf{60.00}&\textbf{65.67}&\textbf{67.89}
    \\
\midrule
    \multirow{8}{*}{\makecell{Llama-2\\-7b-chat}} 
    & \multirow{2}{*}{\makecell{Zero-\\Self-Check}}  & \textbf{Acc.}&44.82&47.25&51.42&47.65&49.32&51.32&56.65&54.11&55.36\\
    && \textbf{F1}&16.52&13.93&29.16&24.82&20.56&25.08&43.27&36.46&41.60
    \\
    \cmidrule{2-12}
    &\multirow{2}{*}{\makecell{Few-\\Self-Check}}  & \textbf{Acc.}&\textbf{54.31}&52.43&55.42&52.31&\textbf{55.65}&50.88&\textbf{57.05}&54.73&60.34\\
    && \textbf{F1}&31.16&29.09&40.90&42.13&47.59&40.84&52.98&48.35&58.01\\
    \cmidrule{2-12}
    &\multirow{2}{*}{SelfCk-1gm} & \textbf{Acc.}&51.78&50.15&54.84&\textbf{55.29}&52.42&\textbf{55.61}&49.79&50.52&49.19\\
    && \textbf{F1}&24.29&29.46&41.56&36.16&35.86&44.58&12.67&17.60&19.29\\
    \cmidrule{2-12} 
    &\multirow{2}{*}{\textbf{Ours}}  & \textbf{Acc.}&53.16&\textbf{58.53}&\textbf{60.85}&54.62&54.29&53.08&53.83&\textbf{63.99}&\textbf{67.43}\\
    && \textbf{F1}&\textbf{61.28}&\textbf{65.99}&\textbf{67.76}&\textbf{66.66}&\textbf{67.10}&\textbf{66.66}&\textbf{64.82}&\textbf{70.38}&\textbf{72.31}
    \\
\midrule
    \multirow{8}{*}{\makecell{Llama-2\\-13b-chat}} 
    & \multirow{2}{*}{\makecell{Zero-\\Self-Check}}  & \textbf{Acc.}&52.04&52.25&53.26&51.04&50.72&\textbf{59.40}&51.85&51.67&\textbf{57.34}\\
    && \textbf{F1}&11.82&12.43&25.21&6.93&8.10&39.25&19.93&22.22&38.34    \\
    \cmidrule{2-12} 
    &\multirow{2}{*}{\makecell{Few-\\Self-Check}}  & \textbf{Acc.}&28.36&37.85&51.35&15.62&23.42&46.03&34.11&39.53&52.65\\
    && \textbf{F1}&39.50&48.35&61.67&23.58&31.53&51.98&49.70&54.77&66.37\\
    \cmidrule{2-12}
    &\multirow{2}{*}{SelfCk-1gm} & \textbf{Acc.}&\textbf{60.28}&52.97&51.90&56.91&51.58&55.44&50.41&51.15&49.18
    \\
    && \textbf{F1}&62.12&60.84&65.89&44.06&50.62&53.19&32.20&45.43&59.34\\
    \cmidrule{2-12}
    &\multirow{2}{*}{\textbf{Ours}}  & \textbf{Acc.}&57.14&\textbf{54.23}& \textbf{53.80}& \textbf{58.33}&\textbf{60.38}&54.70&\textbf{56.19}&\textbf{57.53}&51.63
    \\
    && \textbf{F1}&\textbf{66.81}&\textbf{62.14}& \textbf{66.80} &\textbf{56.28}&\textbf{67.58}&\textbf{67.49}&\textbf{63.32}&\textbf{62.33}&\textbf{67.12}
    \\
    \bottomrule
    \end{tabular}
    \label{tab:total_result}
\end{table}




\subsubsection{Hallucination Detection}



Table~\ref{tab:total_result} shows the hallucination detection performance of our method and the baselines based on our \textbf{AutoHall} datasets. The ChatGPT-based method consistently outperforms all other baselines across all scenarios, with an F1 increase of 20-30\%. As expected, detecting self-contradictions in pairs can indeed assist with hallucination detection accuracy, resulting in an 8.91\% increase on average than SelfCk-1gm. For Llama-2-7b-chat \& Llama-2-13b-chat, though in some cases the baseline performs slightly better than our method in terms of accuracy, its F1 score is far lower than ours. Overall, our method has the highest F1 score and accuracy among the baselines.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/K.pdf}
    \caption{The performance of hallucination detection method under different value $K$.}
    \label{fig:k_impact}
\end{figure}


% In addition, we get the best average accuracy (61.3\%, 61.8\% and 64.0\%, respectively) on the three datasets. 

In horizontal analysis, it can be observed that when temperature increases, the F1 score also usually increases. 
% As we expected, temperature affects the hallucination proportion while it also influences diversity of sampled references. Thus,
It is expected that when the temperature rises, the sampled references become more diversified, which in turn increases the potential for conflicts, thereby benefiting hallucination detection.

We also find that the performance of our method powered by ChatGPT is better than that of Llama-2-chat. We speculate that the larger model capacity of ChatGPT enables it to store more hallucinatory knowledge that is interconnected to each other. Therefore, the sampled relevant references may be more consistent and the hallucination detection in ChatGPT might be more challenging.





% \begin{table}[htbp]
%     \centering
%     \caption{Accuracy and F1 score of our hallucination detection method and all the compared baselines. TEMP is short for temperature and Acc. is short for the metric of accuracy. \\}
%     \setlength\tabcolsep{3.3pt} 
%     \begin{tabular}{@{}cccccccccccc@{}}
%     \toprule 		
%     \multirow{2}{*}{\textbf{LLMs}} & \multirow{2}{*}{\textbf{Methods}} & \textbf{Dataset} &\multicolumn{3}{c}{\textbf{Climate-fever}}&\multicolumn{3}{c}{\textbf{Pubhealth}}&\multicolumn{3}{c}{\textbf{WICE}}\\
%     \cmidrule{3-12}
%     &&\textbf{TEMP} &\textbf{0.1}& \textbf{0.5}&\textbf{0.9}&\textbf{0.1}& \textbf{0.5}&\textbf{0.9}&\textbf{0.1}& \textbf{0.5}&\textbf{0.9} \\
%   \midrule
%     \multirow{8}{*}{ChatGPT} 
%     & \multirow{2}{*}{\makecell{Zero-\\Self-Check}}  & \textbf{Acc.} &55.24 &50.55&57.56&51.62&51.95&56.19&51.80&55.11&52.78
%     \\
%     && \textbf{F1}&25.68&22.70&31.44&20.61&21.51&31.85&20.46&28.75&25.70
%     \\
%     \cmidrule{2-12}
%     &\multirow{2}{*}{\makecell{Few-\\Self-Check}}  & \textbf{Acc.}&54.97&49.16&54.05&51.16&51.21&51.66&51.60&54.33&52.19
%     \\
%     && \textbf{F1}&28.19&20.86&27.96&13.93&20.63&20.39&20.39&23.68&23.07
%     \\
%     \cmidrule{2-12}
%     &\multirow{2}{*}{SelfCk-1gm} & \textbf{Acc.}&53.59&48.52&52.97&53.48&54.87&59.52&51.60&52.55&53.98
%     \\
%     && \textbf{F1}&34.88&37.85&56.28&19.35&32.23&54.54&12.31&20.46&38.40
%     \\
%     \cmidrule{2-12}
%     &\multirow{2}{*}{\textbf{Ours}}  & \textbf{Acc.}&\textbf{64.59}&\textbf{64.79}&\textbf{64.32}&\textbf{61.16}&\textbf{63.41}&\textbf{60.71}&\textbf{63.20}&\textbf{63.58}&\textbf{65.33}
%     \\
%     && \textbf{F1}&\textbf{69.32}&\textbf{64.89}&\textbf{70.66}&\textbf{60.14}&\textbf{65.75}&\textbf{67.19}&\textbf{60.00}&\textbf{65.67}&\textbf{67.89}
%     \\
% \midrule
%     \multirow{8}{*}{\makecell{Llama-2\\-7b-chat}} 
%     & \multirow{2}{*}{\makecell{Zero-\\Self-Check}}  & \textbf{Acc.}&44.82&47.25&51.42&47.65&49.32&51.32&56.65&54.11&55.36\\
%     && \textbf{F1}&16.52&13.93&29.16&24.82&20.56&25.08&43.27&36.46&41.60
%     \\
%     \cmidrule{2-12}
%     &\multirow{2}{*}{\makecell{Few-\\Self-Check}}  & \textbf{Acc.}&\textbf{54.31}&52.43&55.42&52.31&\textbf{55.65}&50.88&\textbf{57.05}&54.73&60.34\\
%     && \textbf{F1}&31.16&29.09&40.90&42.13&47.59&40.84&52.98&48.35&58.01\\
%     \cmidrule{2-12}
%     &\multirow{2}{*}{SelfCk-1gm} & \textbf{Acc.}&51.78&50.15&54.84&\textbf{55.29}&52.42&\textbf{55.61}&49.79&50.52&49.19\\
%     && \textbf{F1}&24.29&29.46&41.56&36.16&35.86&44.58&12.67&17.60&19.29\\
%     \cmidrule{2-12} 
%     &\multirow{2}{*}{\textbf{Ours}}  & \textbf{Acc.}&53.16&\textbf{58.53}&\textbf{60.85}&54.62&54.29&53.08&53.83&\textbf{63.99}&\textbf{67.43}\\
%     && \textbf{F1}&\textbf{61.28}&\textbf{65.99}&\textbf{67.76}&\textbf{66.66}&\textbf{67.10}&\textbf{66.66}&\textbf{64.82}&\textbf{70.38}&\textbf{72.31}
%     \\
% \midrule
%     \multirow{8}{*}{\makecell{Llama-2\\-13b-chat}} 
%     & \multirow{2}{*}{\makecell{Zero-\\Self-Check}}  & \textbf{Acc.}&52.04&52.25&53.26&51.04&50.72&\textbf{59.40}&51.85&51.67&\textbf{57.34}\\
%     && \textbf{F1}&11.82&12.43&25.21&6.93&8.10&39.25&19.93&22.22&38.34    \\
%     \cmidrule{2-12} 
%     &\multirow{2}{*}{\makecell{Few-\\Self-Check}}  & \textbf{Acc.}&28.36&37.85&51.35&15.62&23.42&46.03&34.11&39.53&52.65\\
%     && \textbf{F1}&39.50&48.35&61.67&23.58&31.53&51.98&49.70&54.77&66.37\\
%     \cmidrule{2-12}
%     &\multirow{2}{*}{SelfCk-1gm} & \textbf{Acc.}&\textbf{60.28}&52.97&51.90&56.91&51.58&55.44&50.41&51.15&49.18
%     \\
%     && \textbf{F1}&62.12&60.84&65.89&44.06&50.62&53.19&32.20&45.43&59.34\\
%     \cmidrule{2-12}
%     &\multirow{2}{*}{\textbf{Ours}}  & \textbf{Acc.}&57.14&\textbf{54.23}& \textbf{53.80}& \textbf{58.33}&\textbf{60.38}&54.70&\textbf{56.19}&\textbf{57.53}&51.63
%     \\
%     && \textbf{F1}&\textbf{66.81}&\textbf{62.14}& \textbf{66.80} &\textbf{56.28}&\textbf{67.58}&\textbf{67.49}&\textbf{63.32}&\textbf{62.33}&\textbf{67.12}
%     \\
%     \bottomrule
%     \end{tabular}
%     \label{tab:total_result}
% \end{table}
% 贴实验结果，我记得是有每个模型的在每个数据集上产生的幻觉的样本数目。 然后我们的方法、baseline，对应的f1、p、r、acc。

% 对应的分析，从数据集生成和幻觉检测两方面来看。
% 数据集生成的话可以分析下每个模型的幻觉比例，哪个高哪个低，然后这个和我们的直觉是一样的，越大的模型越**，因此比例越***。
% 幻觉检测 说明我们的更好，相较于baseline有多少个点的提升。可以分指标来说，多谈到提升高的指标，然后分析一下优势在哪。



\subsection{Analysis}
\subsubsection{Ablation Study on $K$}
We perform an ablation study on the number of comparison pairs $K$ varying from 1 to 13. As illustrated by Fig.~\ref{fig:k_impact} a), the larger the $K$, the more improvement on the hallucination detection F1 score. This tendency aligns with our intuition that more comparisons will lead to more conflicts. Fig.~\ref{fig:k_impact} b) shows that hallucination detection accuracy increases first, and then decreases when value $K$ increases. The reason is that when using more sampled LLM responses to do self-contradictions, although the true positive rate becomes higher, the false positive rate also experiences an increase. 
Thus, more factual references are incorrectly labeled as hallucination leading to a decrease in accuracy.
Since maximizing hallucination detection F1 score is our main target, we select $K = 13$ for the above comparisons subject to limited computational resources.



\subsubsection{Topic Distribution in LLM Hallucination}
Take those recognized hallucinatory references generated by LLMs for example, we examine the influence of topics on hallucination in \textbf{AutoHall} as shown in Fig.~\ref{fig:topic}. The finding is the top five topics in ChatGPT responses are history, technology, culture, geography and business, and yet in Llama-2-chat are politics, technology, sports, geography and history.
% TODO


\begin{figure}[htbp]
\centering
\vspace{0pt}
\begin{minipage}[t]{0.59\linewidth}
\centering
\includegraphics[width=1\linewidth]{figs/hallu_topic_t.pdf}
\caption{The top 10 topics LLMs tend to hallucinate. \label{fig:topic}} 
\end{minipage}
%\hspace{1ex} %%两个minipage之间相隔1个字符的距离
\begin{minipage}[t]{0.40\linewidth}
\centering
\vspace{-5.9cm}
\includegraphics[width=1\linewidth]{figs/conflicts.pdf}
\caption{Histogram for $\overline{Num_c}$ in hallucinatory references($H$) and factual references($\overline{H}$). The model is ChatGPT with temperature=0.1 and the dataset is WICE.\label{fig:conflicts}} 
\end{minipage}
\end{figure}




\subsubsection{Proportion of Reference Conflicts}
% 这里放那个幻觉reference的冲突会比非幻觉的冲突高的表，然后说这个现象支撑了我们的idea。
% 这个idea本身有一个问题，就是假如一开始的reference是有幻觉的，那么也有可能出现后面所有的reference都是有幻觉的现象，这种情况是检测不出来的。但是我们承认是有这样一个问题，然后我们用13个reference之间的冲突证明：如果模型产生了幻觉，那么大概率后面再sample会产生很多幻觉的样本以及少量的无幻觉reference；但如果一开始的reference没有产生幻觉，那么后面大概率产生的reference都是没有幻觉的。

\begin{table}[htbp]
    \centering
    \caption{Average number of conflicts in hallucinatory references($H$) and factual references($\overline{H}$)\\}
    \begin{tabular}{@{}ccccccccccc@{}}
    \toprule
    \multirow{2}{*}{\textbf{LLMs}}&\textbf{Dataset}&\multicolumn{3}{c}{\textbf{Climate-fever}}&\multicolumn{3}{c}{\textbf{Pubhealth}}&\multicolumn{3}{c}{\textbf{WICE}}\\
    \cmidrule{2-11}
    &\textbf{Temperature}&\textbf{0.1}&\textbf{0.5}&\textbf{0.9}&\textbf{0.1}&\textbf{0.5}&\textbf{0.9}&\textbf{0.1}&\textbf{0.5}&\textbf{0.9}\\
    \midrule
    \multirow{2}{*}{ChatGPT}&$\overline{Num_c}$ of $\overline{H}$ &
    1.63&1.80&2.61&1.00&0.98&1.92&0.91&1.27&1.79\\
    &$\overline{Num_c}$ of $H$&
    2.32&2.60&3.52&1.80&1.64&2.72&2.20&2.18&2.75\\
    \midrule
    \multirow{2}{*}{\makecell{Llama-2\\-chat}}&$\overline{Num_c}$ of $\overline{H}$ 
    &5.50&5.6&5.83&10.86&10.86&6.41&11.08&8.06&10.14\\
    &$\overline{Num_c}$ of $H$ &5.53&6.3&6.06&11.71&11.80&6.41&11.11&8.37&10.34\\
    \bottomrule
    \end{tabular}
    \label{tab:conflicts}
\end{table}

To further understand our detection idea, we list and visualize the number of conflicts in both hallucinatory and factual samples via Table~\ref{tab:conflicts} and Fig.~\ref{fig:conflicts}. From Table~\ref{tab:conflicts}, it can be inferred that when an LLM generates a hallucinatory reference for a claim, it results in more sampled contradictory response pairs compared to when the LLM has a good understanding of the claim. Similarly, Fig.~\ref{fig:conflicts} indicates that among $K$ ($K=13$) comparison pairs, the number of conflicts reaches six or more almost only when LLM tends to generate hallucination. 
 
% the risk is lower?

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figs/conflicts.pdf}
%     \caption{Histogram for $\overline{Num_c}$ in hallucinatory references($H$) and factual references($\overline{H}$). The model is ChatGPT with temperature=0.1 and the dataset is WICE.}
%     \label{fig:conflicts}
% \end{figure}

% \begin{wrapfigure}{1}{0.5\linewidth}
%     \centering
%     \vspace{-10pt}
%     \includegraphics[width=1\linewidth]{figs/conflicts.pdf}
%     \caption{Histogram for $\overline{Num_c}$ in hallucinatory references($H$) and factual references($\overline{H}$). The model is ChatGPT with temperature=0.1 and the dataset is WICE.}
%     \vspace{-2cm}
%     \label{fig:conflicts}
% \end{wrapfigure}


