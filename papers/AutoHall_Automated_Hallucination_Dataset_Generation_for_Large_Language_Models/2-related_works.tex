\section{Related Works}
\label{related_works}

% related work主要就是两大块：
% 1. 幻觉是什么，幻觉的产生原因有哪些。
% 2. 幻觉检测有哪些方法，白盒黑盒之类的，cite几个paper，每个paper一两句话总结一下。
% 最后一两句话说一下它们的问题 ，需要手动标，然后我们这个幻觉检测方法的优势可能得总结下，要说明它们工作的劣势
% different models use different strategies for hallucination
% the hallucination of LLMs is topic-sensitive（人大）
\subsection{Hallucination of Large Language Models}
Although large language models have demonstrated remarkable capabilities~\citep{liu2023summary,srivastava2022beyond}, they still struggle with several issues, where hallucination is a significant problem. 
Hallucination arises when the content generated by LLMs is fabricated or contradicts factual knowledge. 
The consequent effects may be harmful to the reliability of LLM applications~\citep{zhang2023siren,pan2023risk}.
So far, the causes of hallucination in LLMs have been investigated across different tasks, such as question answering~\citep{zheng2023does}, abstractive summarization~\citep{cao2021hallucinated} and dialogue systems~\citep{das2023diving}. The key factors include but are not limited to training corpora quality~\citep{mckenna2023sources,dziri2022origin}, problematic alignment process~\citep{radhakrishnan2023question,zhang2023siren} and randomness in generation strategy~\citep{lee2022factuality,dziri2021neural}.

\subsection{LLM Hallucination Detection}
To detect the hallucination issue, there are many endeavors to seek solutions. 
On the one hand, prior works focus on resorting to external knowledge to detect hallucinations.
For instance, ~\citet{gou2023critic} propose a framework called CRITIC to validate the output generated by the model with tool-interaction and ~\citet{chern2023factool} invoke interfaces of search engines to recognize hallucination.
On the other hand, current research pays more attention to realizing one zero-resource hallucination detection method. 
Typically,~\citet{xue2023rcot} utilize the Chain of Thoughts (CoT) to check the hallucinatory responses; ~\citet{selfcheckgpt} introduce a simple sampling-based approach that can be used to detect hallucination with token probabilities.

Besides, some hallucination benchmarks~\citep{li2023halueval,umapathi2023med,dale2023halomi} are constructed to support detection tasks in numerous scenarios. 
For example, \citet{umapathi2023med} propose a hallucination benchmark within the medical domain as a tool for hallucination evaluation and mitigation;
~\citet{dale2023halomi} present another dataset with human-annotated hallucinations in machine translation to promote the research on translation pathology detection and analysis. 

Nevertheless, there are limitations as they are subject to manually annotated hallucination datasets, which are expensive and time-consuming. 
Meanwhile, the datasets are model-specific, requiring separate annotations for different models, whose applicability will also be affected by model upgrades. 
Furthermore, there is also room for improvement in the performance of current hallucination detection methods.




%is constructed to evaluate whether LLM are able to generate factual responses, these works scattered among various tasks have not been systematically reviewed and analyzed

% challenging the applicability of detection methods due to variations in language patterns and model characteristics.

% Furthermore, even for a certain LLM, they ignore the influence of model update upon their detection performance since the hallucination level inevitably evolves in different versions.

%Our work share the same idea with ~\citet{selfcheckgpt}, but we pay more attention to promote the research of hallucination automatic collection in LLMs. 


%By implementing automated data collection, our aim is not only to improve efficiency% and timeliness
%, but to lay a solid foundation for subsequent hallucination detection work. % 之后重写
% Based on our dataset, we 
