\clearpage
\appendix

% \section{What Are Our Cognitive Biases}
% \label{appendix:cog_bias_extra_info}
% We provide a more formal definition of a cognitive bias and describe the unique attributes of our chosen cognitive biases In this section.

% \paragraph{Cognitive Bias Definition.}
% A cognitive bias refers to systematic patterns of deviation from rational or logical thinking,
% % \nir{if you want a term that captures both, you can say `normative' behavior} \itay{'normative' might sound like this is the norm, meaning what common people do. which in this case is confusing.}
% leading individuals to make judgments or decisions that are influenced by subjective factors, preconceptions, or inherent cognitive limitations rather than objective evidence.
% Cognitive-like biases can be measured by a shift in preferences or choices triggered by contextual or alternative changes, which, from a rational or logical perspective, should not have a significant impact.
% %\nir{nice!}

% % Given the diverse range of cognitive biases, we have chosen to focus on three well-established biases: the \textit{Decoy Effect}, \textit{Certainty Effect}, and \textit{Belief Bias}.

% \paragraph{Why We Chose These Biases.}
% Our investigation focused on two decision-making biases, namely the decoy effect and certainty effect, as well as a judgment bias known as belief bias.
% % These three biases  are  biases associated with decision-making and judgment.
% % \nir{suggestion: say that we explore two types of biases: choice (decision-making?), and judgment; then dive into each in turn.}
% Each of the decision-making biases captures unique facets of prospect theory.
% The decoy effect focuses on alternative options, while the certainty effect examines  how we cope with (un)certainty.
% % The decision-making biases address various facets of prospect theory and possess notable characteristics that enhance their distinctiveness.
% % \nir{can say that decoy considers what alternatives are given, and certainty considers how we cope with (un)certainty}
% The belief bias represents a judgment-related bias that has previously been observed in a single pretrained model not available to the public, making it an interesting additional bias to explore. 
% By exploring the belief bias alongside other well-established biases on different models, we gain a comprehensive understanding of the biases' influence and extend the scope of our research to encompass a broader range of models and their biases.
% % \nir{hmm... can we make a stronger claim as to why we focus on something that has already been shown? what do we show here that they did not?}

\input{Sections/05_data_gen}

\section{Not Probable Effect}
\label{appendix:sec:not_probable}

In the same work that presented the certainty effect, \citet{kahneman1979prospect} show another adjacent effect.
In this adjacent effect, people tend to underweight probabilities of outcomes that are possible but not probable (e.g. probabilities lower than 0.03), but overweight probabilities when they are merely probable.
% Section \ref{subsec:def_certainty} describe the certainty effect mentioned an additional bias associated with the certainty effect,
We name this adjacent effect \emph{not probable} effect, as demonstrated in the experiment conducted by \citet{kahneman1979prospect}.

In this experiment, participants were presented with choices between options that had the same expected utility value.
However, in the \biaseddataset{} condition, the probabilities associated with the options were exceptionally low, below 1\% winning chance, while in the control condition, the probabilities were substantial, above 10\% winning chance. Notably, the options offered the same prizes, and the ratio between the probabilities remained consistent.

According to the theory of expected utility, where both options have equal expected utility in both control and \biaseddataset{} conditions, the alteration in probabilities should not influence the choice.
Surprisingly, the results from \citet{kahneman1979prospect} indicated that when the probabilities were substantial (above 10\%), participants preferred the option with the higher probability and smaller prize.
In contrast, in the \biaseddataset{} condition, they favored the option with the larger prize and smaller probability.

To explore this bias further, we conducted similar experiments to the certainty bias, focusing on values that aligned with this bias.
The experimental findings on the model are presented in Table \ref{tab:not_probable_results}.
Consistently with other biases, these results demonstrate lower bias scores compared to the certainty effect.


\section{Differences Between Models}
\label{appendix:sec:diff_between_models}
%\input{Sections/Tables/diff_of_diffs}
% As mentioned in Section \ref{subsec:bias_score}, we measure how significant the differences in bias scores between models are by fitting linear regression models to predict the choice of the target option with regard to the dataset condition (control or \biaseddataset{}) and the type of model, also known as The difference-in-differences method \cite{dimick2014methods}.
% The linear regression is demonstrated in Equation \ref{appendix:eqa:diff_of_diff}, where $y$ is a binary label of whatever the model chose the target option of that sample, $M$ is the type of model, $T$ is whatever this sample is from the \biaseddataset{}  or control dataset and $\beta_{0},\beta_{1}, \beta_{2},\beta_{3}$ are the regression coefficients and $\epsilon$ allows for random noise.

To evaluate the significance of bias score differences between models (as explained in Section \ref{subsec:bias_score}), we utilize linear regression models based on the difference-in-differences method \cite{dimick2014methods}.
By considering the predictions of two models on both the control and \biaseddataset{} datasets, we train a linear regression model to predict the selection of the target option for each sample.
The regression model is provided with only two inputs: the model's identity and the dataset source for the given sample.

The linear regression is presented in Equation \ref{appendix:eqa:diff_of_diff}.
The binary label $y$ represents the target option choice, $M$ denotes the model type, $D$ indicates the dataset condition (control or \biaseddataset{}), and $\beta_{0}, \beta_{1}, \beta_{2}, \beta_{3}$ are the regression coefficients with $\epsilon$ accounting for random noise.


\begin{equation}
\label{appendix:eqa:diff_of_diff}
    y = \beta_{0} + \beta_{1} * M + \beta_{2} * D + \beta_{3} * M * D + \varepsilon
\end{equation}

The p-value of the interaction coefficient $\beta_{3}$ indicates the significance of differences in bias scores between models.    
By considering the bias score as the difference between datasets for the same model, the focus on the interaction coefficient enables us to directly quantify the extent of bias amplification across models influenced by the dataset condition.

% The p-value of the interaction coefficient $\beta_{3}$ between the dataset condition and the type of model state how much the differences in bias scores of different models are significant.

\paragraph{Results.}
As the vast majority of the differences are significant, we mention only the model pairs whose differences were \emph{not} significant (p-value < $0.05$).
We compare pairs of models within two models ``families'' - DaVinci, DaVinci-002, DaVinci-003, and GPT4 in one family, while T5 and Flan-T5 are in the other family.

% For the DaVinci and DaVinci-002 models, the difference was not significant in the belief valid, and in 3 out 4 products in decoy cheaper and in all products in decoy expensive.
% For the DaVinci-002 and DaVinci-003 models, the difference was not significant in two out of four products both in decoy cheaper and in decoy expensive.
% For the DaVinci-003 and GPT4 models, the difference was not significant in two out of four products in decoy cheaper and in one product in decoy expensive.
% For the T5 and Flan-T5, the difference was not significant in the certainty effect. 

For the DaVinci and DaVinci-002 models, the difference was not significant in the belief valid, and in decoy expensive.
For the T5 and Flan-T5, the difference was not significant in the certainty effect. 
All other biases and model pairs' differences were significant.


\section{Location-Dependent Choices}
\label{appendix:sec:options_locations}
\input{Sections/Tables/table_options_locations}
To investigate the influence of option location on model preferences, we conducted experiments by permuting the order of options.
This approach allowed us to assess whether a model's preference for a particular option was driven by its location rather than its content.

While this permutation technique effectively controlled for the potential confounding effect of option location, it also provided insights into the prevalence of location-dependent choice in each model.
Table \ref{tab:options_locations} presents the preference of each model for specific locations in each experimental condition.

The table demonstrates that the pretrained models exhibit a strong reliance in most cases on option location in their decision-making process.
This reliance could explain their seemingly arbitrary choices and especially DaVinci's low bias scores, which are close to approximately -- 0.17 in the decoy effects and 0 in the certainty effect.

% This influence of option location could be affected by the DC-PMI correction we apply on the pretrained models as described in Section \ref{subsec:eval_ans}, as a high probability for a baseline prompt (e.g. "Answer: Option 2") could tilt the normalized model answers which we report to favor other options (since each answer is normalized by its baseline prompt probability.)
% We, therefore, verify in a partial experiment for the DaVinci model that shows that for the decoy and certainty effects they indeed prefer option by location even without the DC-PMI correction.

\section{GPT4 is Undecided}
\label{appendix:gpt4_zero-shot}
The results for the zero-shot for all biases can be viewed in Table \ref{appendix:gpt4_zero-shot}.
These results show similar trends to the 1-shot GPT4 results in Table \ref{tab:gpt4_results}, although the biases score are higher as a result of the few-shot improvement discussed in Section \ref{subsec:few_shot}.

When testing GPT4 on the certainty effect in the zero-shot settings, we noticed that in a large number of cases (about 17\%), the model does not choose one of the options. 
Instead, GPT4 responded with statements such as ``It's impossible to tell'' or ``As an AI, I don't have personal preferences'' and in some cases, the model explained that it needs to calculate the higher expected utility values step-by-step, and proceed by doing the calculation, completely avoiding the possibility for a bias.

In the few-shot setting, the model successfully adapted itself and provided a clear answer in the appropriate format.
For the bias scores presented in Table \ref{appendix:gpt4_zero-shot} we only considered examples where the model provided a clear answer. 

\input{Sections/Tables/gpt4_results_0_shot}


\section{Few-shots For Pretrained LM}
\label{appendix:sec:few_shot_pretrained}
\input{Sections/Figures/13_pretrained_few_shot}
We can see the results for the pretrained models in Figure \ref{fig:few_shot_pretrained}.
As can be seen, using few-shot examples for the pretrained LM does not change the bias scores in a significant way.
That is in contrast to the minor drop in bias scores for their counterparts' instruction-tuned models, thus suggesting that the reason the pretrained models show smaller bias scores than the instruction-tuned models is not the fact that they fail to compile to the format of choice.


\input{Sections/Figures/10_price_range_flan_xxl}

\section{Additional Analysis}
\label{appendix:sec:additional_analysis}

We add additional analysis results regarding the decoy effect in Sections \ref{appendix:subsec:price_range} and \ref{appendix:subsec:decoy_type_davinci003}, certainty effect in Section \ref{appendix:subsec:certainty_analysis} and the belief bias in Section \ref{appendix:subsec:belief_acc_bias_t5}.

\input{Sections/Tables/table_products}
\subsection{Product Results}
\label{appendix:subsec:products_results}
The comprehensive product results for the decoy expensive and decoy cheaper effects are presented in Table \ref{appendix:tab:products_results}.
Similarly to the main results, we observe that the instruction-tuned models generally do not exhibit positive bias scores in the decoy expensive effect for most products.
Conversely, the decoy cheaper effect consistently yields higher positive bias scores across all products.



\subsection{Decoy Price Range}
\label{appendix:subsec:price_range}

Section \ref{subsec:decoy_analysis} presents the results concerning the influence of price range on the DaVinci-003 model decoy cheaper bias score.
Similarly, an analogous analysis was conducted on the Flan-T5-XXL model, and the outcomes are illustrated in Figure \ref{fig:decoy_prices_flan}.

The results demonstrate a similar trend to the DaVinci-003 model, with one exception: when the target price is \$100K, the bias score drops to 0.
Further investigation revealed that the Flan-T5-XXL model consistently favors the competitor in 100\% of the samples at this specific price point.

While the underlying reason for this behavior remains elusive, it is an intriguing finding that warrants future exploration. Understanding the fundamental preference patterns of the Flan-T5-XXL model, as well as other models in general, presents an avenue for future research.


\subsection{Decoy Sub-Type}
\label{appendix:subsec:decoy_type_davinci003}
\input{Sections/Figures/11_decoy_type_davinci}
In contrast to the notable effect observed in the Flan-T5 models with respect to the decoy sub-type, the DaVinci-003 model exhibits a consistent bias score across all sub-types, as depicted in Figure \ref{fig:decoy_type_davinci003}.


\subsection{Certainty Effect Analysis}
\label{appendix:subsec:certainty_analysis}
In a similar fashion to the price gap effect observed in the decoy effect, we can examine the impact of the target option's prize on the bias scores by varying the prize values presented to the model in the certainty effect options.

The results are illustrated in Figure \ref{fig:certainty_prize}.
While the Davinci-003 model exhibits a mostly consistent bias score across most prize values, the Flan-T5 model displays a sudden drop in bias scores for mid-range prizes, indicating lower biases in those cases.
The reason for this drop remains unclear and warrants further investigation.

Additionally, we conduct an analysis by categorizing the samples based on the utility gap between the two options.
The outcomes of this analysis are depicted in Figure \ref{fig:certainty_utility_gap}, showing a similar trend to the relationship between the prizes and bias scores and revealing that the bias score drop is probably not due to the utility gap.

These two analysis demonstrates that the bias is more stable in the DaVinci-003 model compared to the variation in the Flan-T5 model.
Future research could scale up the data and investigate more deeply the way different models are biased and how they are affected by the values of the different samples.


\input{Sections/Figures/09_certainty_prize}
\input{Sections/Figures/12_certainty_utility_gap}



\subsection{Belief Bias}
\label{appendix:subsec:belief_acc_bias_t5}
\input{Sections/Figures/15_belief_acc_bias_t5}

Figure \ref{fig:belief_acc_bias_t5} showcases the change in bias scores relative to the accuracy of the T5 models on the logical reasoning aspect of the belief bias task with a similar trend to the GPT models.
