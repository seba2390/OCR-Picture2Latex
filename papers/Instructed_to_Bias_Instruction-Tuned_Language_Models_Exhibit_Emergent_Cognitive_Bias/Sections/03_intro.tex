%\input{Sections/Figures/01_examples}
\input{Sections/Tables/01_examples_table}
\section{Introduction}

% LMs' abilities are becoming more diverse, they are capable of assisting humans in different creative text-generating tasks and natural language understanding which require some level of decision-making \cite{Sun2021IGAAI} \cite{Li2022PreTrainedLM}.

% paragraph 1 (19.7) 
% While LMs show increasingly higher capabilities that have potential in many use cases \cite{NEURIPS2020_1457c0d6}, previous work showed that these models might hold different kinds of biases, that are presumably picked during pretraining, such as gender bias \cite{Sun2019MitigatingGB,Tang2022GenderBU} and race bias \cite{nadeem-etal-2021-stereoset}.

% new paragraph 1 (19.7)
Advanced fine-tuning methods, like instruction tuning (IT) and reinforcement learning from human feedback (RLHF), have been recently recognized as essential paradigms for improving the alignment of language models (LMs) with human objectives \cite{Ouyang2022TrainingLM,bai2022training}.
Although widely adopted \cite{zhou2023comprehensive}, the specific cases in which IT and RLHF enhance model behavior to resemble human behavior, and the mechanisms involved in this process, remain unclear.

% Building upon insights from recent studies that demonstrate a wide range of cognitive-like biases in pretrained LMs \cite{Binz2022UsingCP,dasgupta2022language} and IT models \cite{hagendorff2022machine}, this research delves into the impact of IT and RLHF techniques on decision-making and reasoning processes done by LMs.\nir{it's not clear who is making decisions or reasoning here.}
% \nir{i would swap the order: 'this research...', and then 'building upon insights' (as leading to next sentence)}

In this research, we delve into the impact of IT and RLHF techniques on decision-making and reasoning in LMs.
Recent studies highlighted to some extent cognitive-like biases in pretrained LMs \cite{Binz2022UsingCP, dasgupta2022language} and instruction-tuned models \cite{hagendorff2022machine}.
We build upon these insights and go further in investigating the implications of IT and RLHF interventions on LMs' cognitive-like behavior.
%\nir{do we rely `draw' on recent studies? this reads to me as downplaying our work}

% paragraph 2 (19.7)
% In addition to the biases mentioned above, recent studies have revealed cognitive-like behavior in pretrained language models (LMs) \cite{Binz2022UsingCP,dasgupta2022language}.
% These findings suggest that cognitive biases can impact decision-making and the effectiveness of text analysis models employed to support humans in diverse tasks.
% For example, \citet{hagendorff2022machine} showed the instruction-tuned GPT3.5 ('Text-DaVinci-002') \cite{Ouyang2022TrainingLM} can make human-like errors in decision-making using existing data of the psychological test \cite{frederick2005cognitive}.

% In addition to the biases mentioned above, recent studies have revealed cognitive-like behavior in pretrained language models (LMs) \cite{Binz2022UsingCP,dasgupta2022language}.
% These findings suggest that cognitive biases can impact decision-making and the effectiveness of text analysis models employed to support humans in diverse tasks.
% For example, \citet{hagendorff2022machine} showed the instruction-tuned GPT3.5 ('Text-DaVinci-002') \cite{Ouyang2022TrainingLM} can make human-like errors in decision-making using existing data of the psychological test \cite{frederick2005cognitive}.
% \gabis{but our point is that these are biases that are picked up during instruction-tuning, no?}
% \itay{no. we say that these biases are shown after tuning, we don't say where were they learned (I actually think they are  mostly learned in pretraining)}.
% \nir{can we somehow argue that tuning only amplifies these biases (suggesting that they are there to begin with)?}
% \itay{this is a too strong argument in my view. we don't know that. We only show that we can see little or no bias in pretrained, and in the fine-tuned models the biases are clear. we really don't know when they are picked up, finding out is a whole different objective.}

% These socially-related biases were explored in models, motivated by the biases' longstanding investigation in social and behavioral studies for several decades \cite{deHouwer2019ImplicitBI,Hannk2017BiasIO}.
% \gabis{I don't understand this sentence. I think it can just be removed.} \itay{it lays the ground for why it should be interesting to look at these socially-related biases. I tried to make the connection to the next paragraph more clear.}
% \nir{i'm confused - are you referring here to social (=conventional) or non-social (=cognitive) biases? also in relation to the next sentence:}

% Similar studies have revealed that in addition to social-related biases,
% \nir{this positions cog biases as something `additional' in humans to social biases; i think they stand on their own}
% humans tend to have a variety of cognitive biases such as the decoy effect \cite{Huber1981AddingAD}, certainty effect \cite{Kahneman1979ProspectTA}, and belief bias \cite{evans1983conflict}.
% These cognitive biases may sometimes lead to inconsistent judgment, illogical interpretation, or perceived irrationality.\nir{this statement can be made much stronger--this virtually transformed much of modern economics; also not sure what `perceived irrationality' is.}
% \nir{can we say somewhere that these effects are population-level averages?}

%\gabis{I think I'd rather start with this paragraph. Instead of the first two paragraphs, which I think can be removed. So start with this one, then the paragraph starting with ``distinct from social...'', then ``in this work''} 

% paragraph 3 (19.7)
% In this work, we hypothesize that inspecting LMs for cognitive biases using newly created data will reveal more cognitive-like biases similar to human cognitive biases.
% Moreover, we investigate the potential effect of fine-tuning methods such as instruction-tuning and reinforcement learning from human feedback (RLHF) on the severity of the biases that these models present.
%\gabis{this sentence is too long. I suggest moving the finetuning part later in the intro after we describe the data.} \itay{made it shorter. I think it's important to keep the finetuning part early as it our more intersting result.}


%These socially-related biases were first found in humans, similar to other non-socially related cognitive biases discovered by social studies.
%These other cognitive biases can result in inconsistent judgments and illogical interpretations in non-sonically related situations.
We inspect three well-researched and fundamental biases:
the decoy effect
%\cite{Huber1981AddingAD,wu2020profiting}, 
\cite{huber1982adding}, 
the certainty effect %\cite{kahneman1979prospect,schmidt1998measurement}, 
\cite{kahneman1979prospect}, 
and belief bias
%\cite{evans1983conflict,evans1995belief,klauer2000belief}, 
\cite{evans1983conflict}.
% have virtually transformed much of the traditional economic frameworks and
These biases reflect basic inconsistencies in human decision-making (decoy and certainty effects)
and fallacies in logical reasoning (belief bias)
that are both prevalent, persistent, and consequential
\cite{berthet2022impact,acciarini2021cognitive}.
% \nir{do these citations refer to `everyday life', or to lab studies?} \itay{changed to a more general phrase.}

The conventional approach to studying cognitive biases in humans is to design simple experiments that elicit from human subjects either judgments or decisions that reflect a target bias.
Many of these experiments involve question answering;
%Table \ref{table:examples_biases}
Figure \ref{fig:figure1_example_data} shows an example of questions used in such experiments,
illustrating how the responses of subjects can suggest biased behavior.
%We test our hypothesis by adapting classic cognitive experiments and incorporating custom data into the LM settings.
To study cognitive-like biases in LMs, our approach relies on 
% We conduct our investigation by 
adapting classic human experiments to an LM setting.
Towards this, we create experimental dataset using semi-automatic generated decision tasks:
% This data generation involves
%leveraging the settings of cognitive experiments,
First, for each bias, we manually create an array of appropriate task templates containing flexible numeric and textual place-holder variables.
Then, for a range of value and sets of alternatives, we generate a large collection of unique textual prompts, which we then use as queries to LMs.
%designed to capture the patterns of the biases.
% Like the classic experiments, our semi-automatic generated data is divided into control and treatment data sets.
% We then measure the bias quantity as the difference between the models' choices in the two data sets. 
Following the classic experimental paradigm,
in each experiment we partition the generated data into a `control' dataset and a 'treatment' dataset, and define and measure the bias of a given LM as the average difference of its choices between the two data sets.

%\gabis{I think that the intro should elaborate more on how we do this. If we omit the first two paragraphs then we'll have more space for this.} \itay{It was like this at an earlier iteration but was moved to section 3 at everyone's request to shorten this part in the intro and move the data information to one location. Currently, it's mainly explained in the appendix, not even in the main paper. I added a sentence to try and partially answer this.}

% Our experiments shed light on the prevalence and pervasiveness of three cognitive biases in different LMs.
% While the belief bias has been explored in prior work on single pretrained LM, the decoy and certainty effects have not been previously demonstrated in LMs using newly generated data with a systemic method. \gabis{I'm not sure if we need this paragraph here, we can mention and cite the deepmind work in a related work section, IMO}



% Our findings shed light on the prevalence and pervasiveness of two cognitive biases -- the decoy and certainty effects -- in LMs that have been extensively studied in the cognitive field but, to the best of our knowledge, have not been previously demonstrated in LMs using newly generated data with a systemic method 
%\gabis{this is pretty opaque. What are these two biases? What happened to the third one? and again we should elaborate before on what's this ``systemic method'' and ``newly generated data''. Also, the sentence is too long.}.

% Our findings present evidence that instruction-tuned and RLHF-tuned models display cognitive-like biases that were either absent or less prominent in their pretrained counterparts.
% \nir{can we say this in active voice? e.g., `evidence suggesting that applying instruction or RLHF tuning to pretrained language models either causes models to exhibit biases, or accentuates them if they already exist'}
Within this setup, we empirically evaluate the degree of bias exhibited by several pretrained LMs, and compare them to their corresponding fine-tuned variants.
Our findings indicate that applying IT or RLHF tuning either
\emph{introduces} cognitive-like biases into text generation,
or \emph{amplifies} these biases if they already exist.
For example, Figure \ref{fig:figure1_example} presents 
an evaluation of the certainty effect on the GPT3 and GPT3.5 models,
showing that the IT-tuned LMs present a bias that was not found in the pretrained LM.
%\nir{this seems incomplete - what is this sentence trying to convey?}
%\gabist{Our findings present compelling evidence that instruction-tuned and RLHF-tuned models display biases that were either absent or less prominent in their pretrained counterparts.} \gabis{I don't like superlatives, let the readers decide if it's compelling.} \itay{sure, so just erase `compelling', not the whole sentence, right?}

Given that fine-tuned models are typically considered to be superior,
our results point to an important limitation of tuning based on instructions or human feedback.
% namely their tendency to display cognitive-like biases.
% Additionally, the human feedback-tuned models
%\gabis{by latter we mean RLHF or also finetuned}
Fine-tuned models are also often regarded as potentially \emph{less biased},
such as in domains like gender or race,
since they can be explicitly trained to avoid these biases or having personal preferences 
\cite{2303.08774}.
% However, like previous efforts to debias models \cite{gonen2019lipstick}, 
% % \nir{so instruction/RLHF is also a debiasing effort? i thought you guys said it wasn't...} \itay{instruction tuning is not, RLHF could be used to debias and was used in GPT models.}
% when faced with biases these models were not explicitly trained to handle, these models may reveal the limits of these training methods.
% \nir{`faced with biases' seems strange to me. but i am in favor of saying something along the lines of `fine tuning tries to solves one problem, but causes another one to pop up'}
%\gabis{I don't see the connection to Hila's work. Was RLHF specifically designed to make models less biased?} \itay{according to OpenAI, yes, among other reasons. They specifically show in the referred report how RLHF 'fixed' biased behavior.} \gabis{so it's worth mentioning that IMO, I don't know if that's well known}
Our results suggest that, 
similarly to debiasing attempts \cite{gonen2019lipstick},
improving alignment with respect to one human objective
may result in behavior that is unintended with respect to others.
%\nir{rephrased this - please make sure it makes sense} \itay{changed it to be more accurate}

% perhaps attempts of model tuning using human feedback to avoid or adopt explicit human-like preferences may inadvertently give rise to new implicit preferences.
% \nir{i would be very careful in how we phrase this paragraph - this is a strong claim to make, and so we should be very precise} \itay{changed it to be softer. Anyway it should be OK since the rise of some implicit preferences has already been shown in non academic work (like the preference for longer text which is known and talked about)}