\section{Experiments}


\paragraph{Models}
We conduct our experiments on two LM sets.
The first set is  pretrained models -- GPT3 `DaVinci' \cite{NEURIPS2020_1457c0d6},  and the publicly available T5 \cite{raffel2020exploring}.\footnote{We use version T51.1  \url{github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md}}
The second set consists of improved versions of the GPT3 and T5 models fine-tuned using IT and human feedback.
For GPT3, we experiment with GPT3.5 models -- `text-DaVinci-002'  and `text-DaVinci-003' (referred here as `Davinci-002' and `Davinci-003' correspondingly) \cite{Ouyang2022TrainingLM} as IT and IT+RLHF models correspondingly.
For T5 we use the Flan-T5 \cite{chung2022scaling} models as the version fine tuning using IT.
Our primary findings are based on the XXL variant of the models (11B parameters), and we also conduct experiments using the XL variant (3B parameters) to investigate the influence of model size.

LMs are being constantly improved with goals such as being more user-friendly in text generation, reducing hallucinations, and reducing bias.
We thus also experiment with one of the latest commercially available models, GPT4  \cite{2303.08774}, which is considered a state-of-the-art generative model.\footnote{We used the `gpt-4-0314' version with the content ``You are a helpful assistant.''}
We use GPT4 only as a reference for a newer model that lacks its pretrained version as it was not released for public use.

% \gabis{I think that the grouping of models should relate to the story we want to tell. Does it make sense to group the models according to instruction / not instruction, or by their size? rather than OAI / not OAI? Also, citaions needed}
% \itay{Changed the grouping according to instruction / not instruction and added citations.}

%\subsection{Determining the Model's Answer}
\paragraph{Determining the Model's Answer.}
\label{subsec:eval_ans}
% \gabis{On the whole on the one hand this seems like a small implementation issue, otoh, I couldn't understand what we're actually doing. So we don't do this for the instruction-tuned models? only for the non-instruction?}

Given a prompt asking for a choice, the instruction-tuned models usually generate text describing their choice, simply as generating ``Option 1'' or ``Brand 2''.\footnote{In the certainty effect less than \%5 of the predictions made by Flan-T5-XXL were not clear and we excluded these example}

To assess the pretrained performance for each task, we use the common practice \cite{NEURIPS2020_1457c0d6} of evaluating the likelihood of various candidate answers from a predefined set of possible answers. 
This evaluation might be affected by a preference of the model to an answer given the context (e.g. given ``Answer:'' the model might give a higher baseline probability to ``Option 2'').
We apply the DC-PMI correction \cite{holtzman-etal-2021-surface} that mitigates this issue by normalizing each answer likelihood within the context of the prompt, relative to a baseline prompt (``Answer:'', in our case).
We experimented with a similar evaluation for the instruction-tuned models and got similar results on a smaller scale, we therefore report only the evaluation based on the generated text.

% \gabis{worth mentioning that this is a common approach (e.g., Maarten Sap does this in his TOM papers)}
% These answers are evaluated independently, with the model not being exposed to all answer choices simultaneously during the selection process. \gabis{I don't understand this last part about independent evaluation}

% This evaluation might be affected by a preference of the model to an answer given the context (e.g. given "Answer:" gives a higher probability to "Option 2").
% To mitigate this potential issue, \citet{holtzman-etal-2021-surface} suggested the DC-PMI correction. 
% % \gabis{This statement presupposes that the reader knows what is this issue. Instead rephrase to say: ``It was observed that models suffer from ... since .... To mitigate this Holtmzan et al proposed...''}
% This correction involves measuring the change in the likelihood of each answer within the context of the question, relative to a baseline context ("Answer:" in our case).
% Consequently, the answer with the highest increase in likelihood within the given context is selected.

% In some cases, the normalization did not change the model preferences, but in others, we noticed the normalization had a strong effect that does not necessarily represent the model preference best.
% For example, in the Belief bias data set we can measure the accuracy of the model as
% In our experiments results we report both normalized and unnormalized scores.


