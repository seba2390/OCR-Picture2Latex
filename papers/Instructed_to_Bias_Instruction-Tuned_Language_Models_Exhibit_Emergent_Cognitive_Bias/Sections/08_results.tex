\section{Results} 
 Table \ref{tab:results} summarizes the bias scores of pre-trained models and their instruction-tuned and RLHF-tuned counterparts.
 We discuss the main takeaways in this section and provide several fine-grained analyses in the next one. 
% The bias scores of models fine-tuned on instructions and human feedback are presented in Table \ref{tab:results}, demonstrating higher levels of bias compared to the pretrained models, which exhibit minimal or negligible bias.

\paragraph{Models fine-tuned using IT and RLHF show a higher bias than their pretrained counterparts.}
Our findings reveal that the models fine-tuned on instructions and RLHF mostly exhibit significantly higher levels of bias compared to their pretrained counterparts, as demonstrated in Table \ref{tab:results}.
While the pretrained LMs demonstrate minimal to no bias, the fine-tuned models display pronounced biases across most categories.
This is evident in the certainty effect row, where the DaVinci and T5 pretrained models exhibit bias scores of $0.00$ and $0.09$, respectively.
In contrast, the fine-tuned models display higher bias scores of $0.24$, $0.67$, and $0.17$.
This unexpected result suggests that the fine-tuning process, intended to enhance model performance, inadvertently introduces biases into the decision-making process.
Details on the significance of differences between models are in Appendix \ref{appendix:sec:diff_between_models}.


\input{Sections/Tables/gpt4_results_1_shot}

\paragraph{LMs exhibit biases that align with biases observed in humans.}

Intriguingly, our investigation reveals a remarkable convergence between the decision-making biases observed in the models and the well-established irrational biases inherent in human decision-making processes.
Recall from Section \ref{subsec:bias_score} that positive values indicate alignment between bias scores and human biases. 
Indeed, tuning using instructions or human preferences generally makes bias scores increasingly higher.\footnote{The negative bias score exhibited by the pre-trained DaVinci in the decoy biases can be explained by positional preferences, as discussed in Appendix \ref{appendix:sec:options_locations}.}
% The presence of these biases in the models provides compelling evidence of their replication and propagation within AI systems.

% This discovery emphasizes the profound impact of fine-tuning on bias amplification among biases that may be unknown to exist in models at the moment, underscoring the urgent need for effective bias mitigation strategies in natural language processing systems.
% This similarity between human biases and model biases highlights the potential connection of implicit biased training data on model behavior, as it induces the models to replicate the inherent biases ingrained in human decision-making processes and other human behaviors. 
% yonatan: This is too verbose and bombastic to my taste. I suggest keeping it short and to the point, and potentially discussing more in the conclusion.

This finding emphasizes the role of fine-tuning on bias amplification on previously undiscovered biases.
In addition, The similarity between human biases and model biases highlights the potential connection of inherent biases ingrained in human decision-making processes to tuning methods that induce the models to replicate human behaviors. 

\paragraph{IT Amplifies Biases.}

The discernible impact of fine-tuning with IT becomes evident upon comparing the T5 and Flan-T5 models.
While the disparity among the GPT3 models is less apparent, the transparent elucidation of the Flan-T5 fine-tuning process allows us to confidently assert that the sole utilization of IT can indeed engender the emergence of biases.
This finding highlights the influential role of fine-tuning methods in amplifying biases within models, shedding light on the intricate relationship between IT and the manifestation of biases.


% \textbf{Reinforcement learning fine-tuning on human feedback seems to amplify the biases} 
% The main difference between 'Text-DaVinci-002' and 'Text-DaVinci-003' is that the latter was trained with reinforcement learning on human feedback using the PPO algorithm\footnote{According to OpenAI at https://platform.openai.com/docs/model-index-for-researchers}.

% This may imply that apart from the bias amplification that may be caused by the instruction fine-tuning, reinforcement learning by itself can also be the cause of these biases appearance.
%\paragraph{Amplification of Biases through Reinforcement Learning Fine-tuning}
\paragraph{RLHF Amplifies Biases.}

Our findings indicate that the application of reinforcement learning fine-tuning from human feedback has the potential to amplify biases within language models further.
This is evident when comparing the DaVinci-002 and DaVinci-003 models, with the latter incorporating reinforcement learning techniques.\footnote{According to OpenAI at \url{https://platform.openai.com/docs/model-index-for-researchers}.}
Notably, while IT may contribute to bias amplification, our results suggest that reinforcement learning, as an independent factor, can also play a significant role in the emergence of these biases.
This observation highlights the complex interplay between reinforcement learning methodologies and the manifestation of biases.


\input{Sections/Figures/04_model_size_figure}



\paragraph{GPT4 is also biased.}
The results comparing GPT4 to its predecessor in the GPT series are presented in Table \ref{tab:gpt4_results}.
Across our experiments, GPT4 demonstrates the highest bias score in the decoy expensive and decoy cheaper biases.
Although the bias scores are lower in the certainty, belief valid, and belief invalid biases, GPT4 still exhibits significant bias levels.

We hypothesize that the decreased bias scores observed in belief biases can be attributed to the model's training aimed at enhancing logical reasoning.
Part of the GPT4 training data was designed to give the model a chance to learn improved reasoning skills, which includes using the training data from MATH \cite{hendrycks2021measuring} and GSM-8K \cite{cobbe2021training}.
Conversely, this reasoning did not assist the model in mitigating bias in the decoy effect, which exhibited the most pronounced bias.
Furthermore, we encounter instances in the zero-shot setting where GPT4 refrained from providing explicit choices, and we provide further elaboration on this matter in Appendix \ref{appendix:gpt4_zero-shot}.

While GPT4 shows some mitigation of biases, the prominence of the decoy effect has increased, and all biases remain pronounced.
These findings suggest that biases remain relevant in models designed to address bias mitigation, such as GPT4 which was trained using RLHF to avoid social biases such as biases about sexuality and norms around marriage \cite{2303.08774}.


\paragraph{The effect of model size on bias emergence.}

Figure \ref{fig:model_size} showcases the discrepancy in bias scores between the XL and XXL versions of  Flan-T5.
Consistent with prior research on social biases \cite{tal-etal-2022-fewer}, the larger XXL model exhibits higher bias scores for three bias types (decoy cheaper, certainly, and belief valid).
Surprisingly, the decoy expensive and belief invalid bias scores are lower for the XXL model, suggesting a presumable reduction in bias compared to the XL model.

% Surprisingly, the trend for the decoy expensive seemed to be reversed, with a higher score for the XL model, but this may results from a specific behavior of the XXL model, as discussed in Section \ref{subsec:decoy_analysis}.
% The belief invalid bias score is also lower for the XXL model, suggesting a reduction in bias compared to the XL model.

The reduction of bias score in the decoy expensive may result from a specific behavior of the XXL model, as discussed in Section \ref{subsec:decoy_analysis}.
The reduction in belief invalid bias score could be attributed to the XXL model's lower accuracy in identifying invalid conclusions within the Non-Real objects condition, as depicted in Figure \ref{fig:model_size_compare}.

Specifically, in the invalid-believable condition, the XXL model demonstrates a higher acceptance rate, indicating a greater presence of bias.
Whereas, in the invalid Non-Real objects condition, the XXL model displays a significantly elevated acceptance rate, leading to reduced overall accuracy and consequently lowering the bias score as per our defined calculation method (Section \ref{subsec:bias_score}).




