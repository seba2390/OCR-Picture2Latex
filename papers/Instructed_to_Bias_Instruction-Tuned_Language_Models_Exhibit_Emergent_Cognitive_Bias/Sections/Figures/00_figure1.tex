\begin{figure}[t!]
\centering
\begin{minipage}[t]{0.99\columnwidth}
\includegraphics[width=1.0\columnwidth]{Sections/Figures/figure_1_illustrate.pdf}
\caption{Example tasks from \emph{certainty effect} dataset, for the control condition (left) and \biaseddataset{} condition (right), along with typical answers from humans and instruction-tuned models, both of which are biased.
%\nir{the upper line in the image ('In both..') -- could readers think this is part of the task input?} \itay{ran a blind test with a few people - seems to work fine :)}
} 
\label{fig:figure1_example_data}
\end{minipage}
%\qquad\qquad
\qquad
\begin{minipage}[t]{0.99\columnwidth}
\includegraphics[width=0.9\columnwidth]{Sections/Figures/figure_1.pdf}
\caption{%\gabis{Is there actually a blue bar here? It's way too small to notice.} \itay{yes, it's just really small (<0.01). It's a good example for bias which the pretrained model show no bias.}
Bias scores of different GPT3 and GPT3.5 models on the certainty effect dataset.
Similar to human choice behavior,
instruction-tuned models (DaVinci-002 and DaVinci-003) tend to prefer certain outcomes over alternatives that yield higher expected rewards but have some level of risk.
% These models favor the option with a higher expected value unless the alternative option offers the prize with certainty.
In contrast, the pretrained model ('DaVinci') shows no such bias. %Example of \biaseddataset{} and control samples from the certainty effect data (upper).
} 
\label{fig:figure1_example}
\end{minipage}
\end{figure}
