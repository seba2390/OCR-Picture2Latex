\begin{abstract}
% Recent studies show evidence of cognitive bias in large language models (lms) that may affect their use in practice, raising concern about the nature of the decisions and reasoning these models make.
% % In this work, we show that models fine-tuned on instructions and human-curated text exhibits biases that were not present at the same level as in their pretrained counterparts.
% We investigate the manifestation of cognitive biases in lms, 
% particularly
% \nir{i second Gabi's suggestion that this should be the main thing (see general comments)}
% in models fine-tuned on instructions and human feedback.
% Our research reveals that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained counterparts.\yb{how about starting with sth like: instruction/rlhf is great; we show it introduces cognitive biases}
% We examine two cognitive biases -- the Decoy effect and Certainty effect -- which are known to influence human decision-making processes.\nir{should we not state all three tasks together?}
% Remarkably, we provide compelling evidence that these biases also materialize in models fine-tuned on human-curated text.\nir{suggest rephrasing this as a conjecture, rather than something surprising that just `happened'}
% Furthermore, we provide insight into the prevalence of a logical reasoning bias known as Belief bias, which was previously discovered
% \nir{this reads as if the effect itself was established in 2022}
% in Chinchilla \cite{dasgupta2022language}.
% Our findings highlight the presence of this bias in various models, particularly those that have undergone instruction tuning. \yb{no need to separate the types; just say, we investigate these three types, and find...}
% % Additionally, we shed light on another logical reasoning bias, Belief bias, previously identified in Chinchilla \cite{dasgupta2022language}, and demonstrate its prevalence in other models, particularly in instruction-tuned models.
% % Our findings reveal the presence of a further logical reasoning bias, previously identified in Chinchilla \cite{dasgupta2022language}, in other models, particularly emphasizing its prominence in instruction-tuned models.
% % These results show that despite the efforts to clean models from directly learned biases such as gender bias, other biases can still emerge in the process of fine-tuning lms.
% Our findings underscore the presence of biases in lms even after meticulous efforts to mitigate directly learned biases, such as gender bias.\yb{can use this in the motivation: instruction/rlhf is great, helps mitigate gender biases bla bla; we should it \emph{introduces} new bias, specificalyl cognitive biases}
% This work serves as a critical step towards understanding and addressing cognitive biases in instruction-tuned lms, facilitating the development of more reliable and unbiased language models for various practical applications\footnote{We will release our data and code publicly.}.
% % These results are a call to the community to emphasize the safe use of the models and examine the way they are trained and evaluated, including the recently acclaimed instructions-tuned models.


% \gabis{I think we should discuss the overall message and highlight our findings more. Instead of the alarming tone, another option is to present this as an analysis where the main point is that instruction-tuned models seem more prone to well-known cognitive biases?}
% \itay{I change the tone to be more informative and less alarming.}


% ##############################################
Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically.
%While these tuning methods can reduce surface social biases like gender bias,
While these tuning methods can help models generate high-quality text,
we conjecture that they may also inadvertently cause models to express cognitive-like biases.
% may arise in these fine-tuned models.
% \gabis{due to ...} \itay{we can write something like "due to their coherent and human-like text generation abilities." but I think it's lengthy and not necessary in the abstract. I don't think the average reader (or anyone that used chatGPT) will read this and think 'Why should someone think that might be true?'}
Our work provides evidence that fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors.
%\gabisrep{pretrained counterparts}{predecssors? It's just that these models are also pretrained}.
We examine the extent of this phenomenon in three cognitive biases: the decoy effect, the certainty effect, and the belief bias---all of which are known to influence human decision-making and reasoning.
Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. %\gabis{this last sentence felt repetitive, consider omitting} \itay{It's important enough to say that we check different models and not just GPT3}
In this, our work constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.\footnote{We will release our data and code publicly.} 

\end{abstract}
\input{Sections/Figures/00_figure1}