%\subsection{Evaluation}
%\label{sec:eval}
% \gabis{Better not to leave the preamble for sections (e.g., here 5.0) empty. Instead use this space to link what we've seen previously with what we'll see next. Give a TOC description of the subsections, e.g., ``In Section 5.1 we will discuss, finding that ... Then in Section 5.2 ..., Finally in Section 5.3...'' This again allows readers to skim (which is good!)}

% In each of the conducted experiments, an assessment is performed on the model outputs to gauge the level of bias exhibited by the model.
% The extraction of the model answer from its output is detailed in Section \ref{subsec:eval_ans}.
% The definition and interpretation of bias scores are expounded upon in Section \ref{subsec:bias_score}, elucidating the metrics used to quantify biases.
% Furthermore, Section \ref{subsec:few_shot} outlines an additional setting implemented in the experiments, namely the few-shot setting, which merits further discussion and analysis.


\subsection{Determining the Model's Answer}
\label{subsec:eval_ans}
% \gabis{On the whole on the one hand this seems like a small implementation issue, otoh, I couldn't understand what we're actually doing. So we don't do this for the instruction-tuned models? only for the non-instruction?}
Given a prompt asking for a choice, the instruction-tuned models usually generate text describing their choice, simply as generating ``Option 1'' or ``Brand 2''.\footnote{In the certainty effect less than \%5 of the predictions made by Flan-T5-XXL were not clear and we excluded these example}

To assess the pretrained performance for each task, we use the common practice \cite{brown2020language} of evaluating the likelihood of various candidate answers from a predefined set of possible answers. 
This evaluation might be affected by a preference of the model to an answer given the context (e.g. given ``Answer:'' the model might give a higher baseline probability to ``Option 2'').
We apply the DC-PMI correction \cite{holtzman-etal-2021-surface} that mitigates this issue by normalizing each answer likelihood within the context of the prompt, relative to a baseline prompt (``Answer:'' in our case).

% \gabis{worth mentioning that this is a common approach (e.g., Maarten Sap does this in his TOM papers)}
% These answers are evaluated independently, with the model not being exposed to all answer choices simultaneously during the selection process. \gabis{I don't understand this last part about independent evaluation}

% This evaluation might be affected by a preference of the model to an answer given the context (e.g. given "Answer:" gives a higher probability to "Option 2").
% To mitigate this potential issue, \citet{holtzman-etal-2021-surface} suggested the DC-PMI correction. 
% % \gabis{This statement presupposes that the reader knows what is this issue. Instead rephrase to say: ``It was observed that models suffer from ... since .... To mitigate this Holtmzan et al proposed...''}
% This correction involves measuring the change in the likelihood of each answer within the context of the question, relative to a baseline context ("Answer:" in our case).
% Consequently, the answer with the highest increase in likelihood within the given context is selected.

% In some cases, the normalization did not change the model preferences, but in others, we noticed the normalization had a strong effect that does not necessarily represent the model preference best.
% For example, in the Belief bias data set we can measure the accuracy of the model as
% In our experiments results we report both normalized and unnormalized scores.

\input{Sections/Tables/gpt4_results}

\subsection{Using Few-shot}
\label{subsec:few_shot}
% The decoy effect and certainty datasets used in our study are characterized by their choice-dependent nature, as they are decision-making biases wherein a clear and unequivocal correct answer does not exist.
% Consequently, the zero-shot setting aligns most effectively with our objective of assessing the model's bias with minimal external influence.
The decoy and certainty effects samples are choice-dependent questions with no ``correct'' answer.
Recent work suggested that giving few-shot samples without the correct labels could improve model performance by introducing the model with the overall format of
the samples \cite{min2022rethinking}.
% In our case, giving as examples samples with random answers could still affect the model since each answer is either biased or unbiased and could teach the model that this is the desired behavior and comprise the results.
Inspired by this work, instead of providing few-shot samples with correct labels, we explore a variation called ``format few-shot'' that omits the original \biaseddataset or control samples.
This approach aims to familiarize the model with the sample format without biasing it in any particular direction.

In the format few-shot approach, we manually curate examples for the decoy and certainty effects, presenting choices between arbitrary options (e.g., ``Which would you prefer, a white or black shirt?'') and mathematical reasoning examples for the belief bias (e.g., ``The price is \$10 per soda. The customer inserted \$20. Conclusion: The customer can buy only 1 soda. Answer: Invalid.'').
By using these format-oriented few-shot examples, we aim to help the model understand the structure of the samples without introducing bias into its behavior.
% Therefore, in order to help the model understand the sample format without biasing it in either direction, we experiment with few-shot prompting without the original samples.
% Instead, we use manually curated examples of choices between arbitrary options for the decoy and certainty effects (e.g., "Which would you prefer, a white or black shirt?") and mathematical reasoning examples for the belief bias (e.g. "The price is \$10 per soda. The customer inserted 20\$. Conclusion: The customer can buy only 1 soda. Answer: Invalid.").
% We call this approach \textit{format few-shot} as the intention is to show the model the sample format using few-shot examples.

In the case of the belief bias, there are correct labels.
Therefore we can prompt the model with few-shot samples and avoid biasing the model.
We do that by utilizing samples comprised of neutral non-real objects derived from a distinct set of fabricated words that were deliberately excluded from the test data.
This particular variation is referred to as ``Task few-shot'' since the few-shot samples belong to the same task as the test samples.
By comparing the impact of using few-shot examples solely for formatting purposes (format few-shot) with those pertaining to the same task (Task few-shot) on bias scores, we can evaluate their respective effects.

% the zero-shot setting aligns most effectively with our objective of assessing the model's bias with minimal external influence.

% \textbf{Format Few-Shot}
% \gabis{WDYM by ``Format Few-Shot" -- is this an header? it should still be grammatical}
% In addition to zero-shot, we conducted experiments involving few-shot samples that were intended that presented the model with choices between arbitrary options for the decoy and certainty effects (e.g., "Which would you prefer, a white or black shirt?") with random answers that are constrained to be balanced within the same example (if we have 3-shot examples, we won't have 'Option 1' more than two times).
% This experimental design aimed to explore the impact of exposing the model to the choice format on its subsequent choice patterns.

% For the belief bias, we employed mathematical reasoning examples as few-shot such as "The price is \$10 per soda. The customer inserted 20\$. Conclusion: The customer can buy only 1 soda. Answer: Invalid.".

% \textbf{Task Few-Shot} 
% \gabis{Again, wdym by ``Task Few-Shot''}
% For the belief bias experiments, we can utilize samples comprising non-real objects derived from a distinct set of fabricated words that were deliberately excluded from the test data.
% This approach enables us to assess the impact of using few-shot examples solely for formatting purposes compared to employing few-shot examples pertaining to the same task on the bias scores.



