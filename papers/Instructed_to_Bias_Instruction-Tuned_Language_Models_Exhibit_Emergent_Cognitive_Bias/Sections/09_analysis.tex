
\section{Analysis}

We delve into the effects of few-shot examples in Section \ref{subsec:few_shot} and explore different attributes of the decoy effect and belief bias in Sections \ref{subsec:decoy_analysis} and \ref{subsec:belief_acc_bias}.
Further analysis of the attributes of the certainty effect and additional decoy effect traits can be viewed in Appendix \ref{appendix:sec:additional_analysis}.
% This analysis aims to gain insights into the impact of different factors on bias scores and shed light on the behavior of the model.

\input{Sections/Figures/02_few_shot_figure}
%\subsection{Few-shot}

\subsection{Using Few-shot}
\label{subsec:few_shot}
% The decoy effect and certainty datasets used in our study are characterized by their choice-dependent nature, as they are decision-making biases wherein a clear and unequivocal correct answer does not exist.
% Consequently, the zero-shot setting aligns most effectively with our objective of assessing the model's bias with minimal external influence.
The samples used for the decoy and certainty effects are choice-dependent questions with no ``correct'' answer.
It is therefore not obvious how to construct few-shot examples, which presumably should have correct labels in the prompt.
However, recent work suggested that giving few-shot samples without the correct labels could improve model performance by introducing the model with the overall format of
the samples \cite{min2022rethinking}.
In our case, giving task samples with random answers as few-shot examples could still affect the model since each answer is associated with either biased or unbiased behavior and could teach the model that this is the desired behavior type, thus compromising the results.

Therefore, in order to help the model understand the sample format without biasing it in either direction, we experiment with few-shot prompting without the original samples.
Instead, we use manually curated examples of choices between arbitrary options for the decoy and certainty effects (e.g., ``Which would you prefer, a white or black shirt?'') and mathematical reasoning examples for the belief bias (e.g. ``The price is \$10 per soda. The customer inserted 20\$. Conclusion: The customer can buy only 1 soda. Answer: Invalid.'').
We call this approach \textit{format few-shot} as the intention is to show the model the sample format using few-shot examples.

In the case of the belief bias, there \emph{are} correct labels.
Therefore, we can also prompt the model with few-shot samples and avoid biasing the model by utilizing samples comprised of neutral non-real objects derived from a distinct set of fabricated words that were deliberately excluded from the test data.
We call this \textit{Task few-shot} as few-shot samples are from the same task as the test sample.
This approach enables us to assess the impact of using few-shot examples solely for formatting purposes compared to employing few-shot examples pertaining to the same task on the bias scores.

\input{Sections/Figures/03_task_few_shot}
\paragraph{Results With Few-Shots.}
Results with format few-shot examples can be seen in Figure \ref{fig:format_few_shot}.
For the decoy and certainty effects, there is no distinct trend when using the format few-shot examples, except for a small decrease in bias score when changing from zero-shot to one-shot setting.
Overall, increasing the number of few-shot examples might help the model understand the format but does not significantly decrease the bias.


In the case of the belief bias, incorporating few-shot examples leads to a noticeable reduction in the bias score, although a significant level of bias still persists.
This observation can potentially be attributed to the presence of a logical reasoning process required by the belief bias examples, whereby the model's utilization of few-shot examples aids in facilitating easier problem-solving and helps to overcome the inherent bias associated with belief.

This analysis focuses on the impact of few-shot examples solely on the instruction-tuned models, which exhibited the highest bias scores.
However, it is plausible to speculate that the pretrained models, which demonstrated the lowest bias scores, could potentially benefit even more significantly from learning the format  through few-shot examples, considering their stronger dependence on understanding the format.
This could lead to the possible observation of higher bias scores for the pretrained models when giving few-shot samples.
To address this, we also include few-shot results for the pretrained models, revealing that the bias scores remain similarly low for these models.
Refer to Appendix \ref{appendix:sec:few_shot_pretrained} for further details.

\subsection{Decoy Effect Analysis}
\label{subsec:decoy_analysis}
%- Detail about the different products' results
We investigate multiple attributes of the decoy effect, encompassing diverse product outcomes, sub-types of decoys, and price ranges, to assess their impact on the bias score and partly compare them with human behavior.
Moreover, we explore a particular behavior identified in the decoy expensive effect that has a notable influence on the bias scores.

\paragraph{Products Variance.}
There was a moderate variance in bias scores in the decoy expensive results across different products, as shown in Figure \ref{fig:products}.

As the bias scores are computed as the difference between the \biaseddataset condition to the control condition, the score varies with the variance in the base preference of the model to target option in the control condition for each different product.
Such differences between products were also observed to some extent in the original experiments on human subjects.

Together with the effect of price on the bias score (which is also analyzed in this section), the base preference of the model can cause a variance between different products.
The full results for different products  can be viewed in Appendix \ref{appendix:subsec:products_results}


%- the way choices on the two options part set the limit for the bias scores.

% - \textbf{Show the effect of a more extreme decoy bias on a higher bias.}


\input{Sections/Figures/05_products_figure}
\input{Sections/Figures/06_price_range}
\paragraph{Price Range Effect.}

We investigate the relationship between the target price and the price gap, defined as the difference between the prices of the competitor option and the target option.
In our data, the higher the target value, the higher the gap from the competitor option.
By selecting values with varying price gaps, we aimed to examine the impact of this factor on bias scores.

As Figure \ref{fig:decoy_prices} shows, as the price range increases, the bias scores also exhibit higher values.
Although the human experiment did not specifically analyze this aspect, this result aligns with the expected behavior of this bias and is intuitively reasonable.


Results for the Flan-T5 model were mostly similar, with one exception, as depicted in Appendix \ref{appendix:subsec:price_range}.

\input{Sections/Figures/07_decoy_bias_type_flan}
\paragraph{Decoy Sub-types.}
As explained in Section \ref{subsec:def_decoy}, Given a target and a competitor target we generate four decoy options: (\textit{R}) higher price, same quality;
(\textit{R*}) extremely higher price, same quality;
(\textit{RF})  both higher price, worse quality; and (\textit{F})  same price, worse quality.

The findings from the study conducted on human subjects \citep{huber1982adding} indicate that the price-increasing decoy sub-types (R and R*) exhibited the highest increase in bias score, followed by the RF sub-type with a relatively smaller gain.
In comparison, the F sub-type demonstrated the smallest net gain in bias score.


Figure \ref{fig:decoy_type_flan} presents the results for the Flan-T5 model.
Intriguingly,  contrary to findings in human subjects, the F sub-type exhibits the highest increase in bias score, followed by a smaller increase in the R and R* sub-types.
The DaVinci-003 model exhibited minimal variation in response to different decoy sub-types, as demonstrated in Appendix \ref{appendix:subsec:decoy_type_davinci003}.

It is important to note that this comparison should be approached cautiously due to variations in data between human experiments and model experiments.
Nevertheless, the findings suggest that while models demonstrate cognitive-like biases, their biases may manifest differently.

\paragraph{The Decoy Expensive Effect.}

A notable observation in the decoy expensive experiments is the significantly low bias score of --0.18 exhibited by the Flan-T5 XXL model.
We found that this score stems from the model consistently favoring the more expensive target option in the control condition with nearly 100\% preference. 

Considering the model's unwavering preference for the more target option in the absence of a decoy,  the addition of a decoy option cannot possibly shift its preference from the competitor option to the target option.
While this leaves no room for a bias score above zero, this preference for the more expensive option leads to negative results as the model picks the more expensive option even when adding a more expensive decoy option, leading to a shift from the target to the decoy option.

It is intriguing to observe such behaviors in these models that do not align with familiar cognitive biases but contradict basic human logic.
These findings necessitate further investigation that goes beyond cognitive-like biases before utilizing these models to aid in human decision-making.


\subsection{More Accurate and More Biased} \label{subsec:belief_acc_bias}

\input{Sections/Figures/14_belief_acc_bias_gpt}

On the belief bias task, we can quantitatively measure the model accuracy, allowing us to examine the trade-off between accuracy and bias scores.

Figure \ref{fig:belief_acc_bias_gpt} shows the change in bias scores relative to the accuracy of the GPT models on the logical reasoning aspect of the belief bias task (see Appendix \ref{appendix:subsec:belief_acc_bias_t5} for  T5 results).
Interestingly, as the models demonstrate improved accuracy,  they also exhibit higher bias scores. This finding suggests that, despite advancements in accuracy, biases persist within these models.


Finally, our evaluation includes GPT4, a model specifically trained on logical reasoning.
GPT4 achieves a higher accuracy (84\%) compared to all GPT models, while simultaneously exhibiting a lower bias score than DaVinci-002 and DaVinci-003 (0.07 and 0.49 for the belief valid and belief invalid correspondingly).
This observation highlights the potential benefits of incorporating targeted training approaches to enhance both accuracy and mitigate biases in the process.




