\section{Data and Evaluation}

In Section \ref{subsec:data}, we outlined how we semi-automatically generate specific data sets designed for probing these biases to evaluate the existence of cognitive-like biases in models. 
In Section \ref{subsec:bias_score}, we provide a formulation of a bias score intended to quantify the degree of bias exhibited by a model based on its predictions on the generated data. %\gabis{change the order of the sentence: ``in Section ..., we quantify ...''}

\input{Sections/Tables/results_table.tex}

\subsection{Data Generation} %\gabis{use title headers more descriptively, what kind of data? Think of a reader who's skimming the paper and would like to decide if they want to read this subsection}
\label{subsec:data}
% To evaluate the extent to which a model is biased, for each bias, we generate a biased dataset and a corresponding control unbiased dataset and compare them.
To assess the level of each bias in a model, we employ a comparative approach, as shown in Table \ref{table:examples_biases}.
We do that by
%\gabis{comparing a generated ... (so the reader can link this back to the ``comparative'' method described above)}
comparing predictions on a generated \biaseddataset{} dataset and a corresponding control dataset.
%\gabis{In contrast? Why are we singling out belief bias? Is the method different from the other two? If so, let's say it explicitly}
For the decoy and certainty effects, we use data generated according to values crafted by us, and in the belief bias we use data generated in a similar fashion by \citet{dasgupta2022language} with additional text templates we create. 

% For the biased datasets, we replicate the original experiments as described in Section \ref{sec:cog_biases} and use new values and text templates to create more variations.
% The control versions are similar as possible to the biased samples without the attribute that causes the bias according to the cognitive experiments.
 To create the \biaseddataset{} 
 %\nir{i'm still in favor of calling this `treatment', here and throughout. the overloading usage of the term `bias' is very confusing.}
datasets, we follow the experimental design for each bias as outlined in the biases experimental setups in Section \ref{sec:cog_biases}.
%\gabis{where exactly in Section 2, it's a pretty long section, and it's unclear where to find an experimental design there} \itay{for each bias there's and experimental setup, I added a paragraph in each bias to emphasis it.}\gabis{but let's tell the reader where to find it. I think it's better to be explicit.} and introduce new values and text templates to enhance the variations.
The control versions of the datasets are carefully crafted to closely resemble the \biaseddataset{} samples while excluding the specific attribute that triggers the bias, as identified by cognitive experiments.

We establish the naming scheme for the options based on the decoy and certainty effects and align the names of the belief bias options accordingly for ease of reference.
In the decoy and certainty effects, for each sample, there exists a designated \textit{Target} option.
%\gabis{I don't understand some of the structure here, why did we mention the belief bias before, and why did we go back to mentioning these here? Can we talk about them in sequence?} \itay{we have to separate them. in the data part - we didn't generate the belief data, we used existing data. in this targeted part - calling the correct answer `target' in the belief bias does not make sense unless we explain first why there is a `target' option for decoy and certainty and later we use the term target for belief for convince sake.} \gabis{so we should explain why we make this distinction.} \itay{added an explanation for that}
This option is expected to be chosen more frequently by a human (or a biased model) when presented with samples from the \biaseddataset{} dataset compared to samples from the control dataset.
In the belief biases, we treat the correct answer as the target option for ease of notation.
More details about the data generation process can be found in Appendix \ref{sec:appendix_data_generation}.

% \nir{i think at this point the reader should have a clear idea of our methodology. so i suggests bringing forwards some things that appear later: a general template of the tasks we run (ie prompting a classic experiment, letting the model choose one of the available alternatives, measuring outcomes). it should also be clear that we have two conditions (control and treatment) in each task, and that we are interested in comparing them (say how).}


\subsection{Computing The Bias Scores }
\label{subsec:bias_score}
% \gabis{Nit: keep a consistent header scheme across the subsections of the same section. 5.1 has an active verb (``determining''), this one is a nominal phrase (``computation''), and the last one doesn't have a verb. I'd make all of them either active or nominal.}

% We evaluate the presence of biases in each model by examining the differences in their prediction patterns between biased and unbiased datasets.
% We quantify these biases by computing bias scores.

% For the decoy and certainty effects, the bias score is computed as the difference between the model's preference for the \textit{Target} option in the biased condition and the unbiased condition.
% For instance, if the model selected the \textit{Target} option in 90\% of the samples in the biased condition and in 70\% of the samples in the unbiased condition, the bias score would be $0.90 - 0.70 = 0.20$.

We assess biases in each model by analyzing their prediction patterns across \biaseddataset{} and control datasets, quantifying them through bias scores.
The bias score captures the difference in the model's inclination towards the \textit{Target} option between \biaseddataset{} and control scenarios.
%Similarly, in the belief bias, the bias score indicates the model's preference for the \textit{Target} option, which corresponds to the correct answer (valid or invalid).

For example, if the model chose the `Target' option in 90\% of \biaseddataset{} samples and 70\% of control samples, the bias score would be 0.20.

\paragraph{Bias Score Definition.}
The bias score is formally defined by Equation \ref{equation:bias_score}, where \textit{\biaseddataset{}} and \textit{Control} represent the sets of \biaseddataset{} and control datasets, respectively, and \textit{$N_{\formulabiaseddataset}$} and \textit{$N_{C}$} indicate their respective set sizes.
$Ans_{i}$ denotes the model's choice in sample $i$, while $T$ represents the target option.

\begin{equation} \label{equation:bias_score}
\resizebox{.85\hsize}{!}{$\sum\limits_{i \in \Biaseddataset{}} \frac{{\mathbbm{1}{ [Ans_{i} \boldsymbol{=} T] }}}{N_{\formulabiaseddataset{}}}
 \boldsymbol{-} \sum\limits_{i \in Control} \frac{{\mathbbm{1}{ [Ans_{i} \boldsymbol{=} T] }}}{N_{C}} $}
\end{equation}


% \paragraph{Bias Score for Random Choice} \gabis{Use the paragraph environment instead of textbf. But also, why is this bolded anyway?}
% When examining the bias score, a value of 0 indicates the absence of bias.
% However, it is noteworthy to consider the decoy effect scenario where the model's selection is randomized. In this case, the bias score would be -0.17.
% This is because, in the unbiased condition, a random choice between the two options would result in selecting the target option 50\% of the time, while in the biased condition with three options, a random choice would lead to the model selecting the target option only 33\% of the time.
% Consequently, this discrepancy yields a bias score of -0.17.

For the decoy effect, the target option can be associated with either a lower or higher price, leading to the computation of separate bias scores: \textit{Decoy Cheaper} and \textit{Decoy Expensive}.

To compute bias scores for the belief bias, we compare the model's predictions between consistent and inconsistent conditions for valid and invalid arguments. 
This analysis results in two distinct bias scores:

\textit{Belief Valid}: The difference between the model's predictions of consistent valid arguments (valid and believable conclusions in real-life objects condition) and neutral valid conclusions (all valid arguments in non-real object conditions).

\textit{Belief Invalid}: The difference between the model's predictions of consistent invalid arguments (invalid and unbelievable arguments in real-life objects condition) and neutral invalid arguments (all invalid arguments in non-real object scenarios).

\paragraph{Bias Score Values Meaning.}
Higher bias score values indicate a greater degree of bias in the model.
The bias scores range from -1 to 1, reflecting the extent and direction of bias relative to human biases.
A score of 1 represents maximum bias aligned with human biases, 0 indicates no bias, and -1 denotes maximum bias in the opposite direction to human biases.

The significance of each bias score is measured using the student's t-test \cite{student1908probable}.
We measure the significance of differences of difference between the bias scores of different models via the interaction of the models and the bias scores using a linear regression model and report the results in Appendix \ref{appendix:sec:diff_between_models}.

