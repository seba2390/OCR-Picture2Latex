%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-4pt}
\section{Preliminaries}
\label{sec:prelim}
\vspace{-4pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We review relevant backgrounds here.  
$\cA$ refers to a GLB algorithm, and $\cB$ refers to an online learning algorithm.
Let $\cB_d(S)$ be the $d$-dimensional Euclidean ball of radius $S$, which overloads the notation $\cB$. % but is clear from $\cB$ from the context.
Let $\A_{\cdot i}$ be the $i$-th column vector of a matrix $\A$.
Define $||\x ||_\A := \sqrt{\x^\T\A\x}$ and $\vec(\A) := [\A_{\cdot 1}; \A_{\cdot 2}; \cdots ; \A_{\cdot d} ] \in \dsR^{d^2}$
Given a function $f:\dsR\rarrow\dsR$, we denote by $f'$ and $f''$ its first and second derivative, respectively.
We define $[N] := \{1,2,\ldots,N\}$.

%%%%%%%%%%%%%%%%%%%%
\vspace{-4pt}
\paragraph{Generalized Linear Model (GLM)}
%%%%%%%%%%%%%%%%%%%%

Consider modeling the reward $y$ as one-dimensional exponential family such as Bernoulli or Poisson.
When the feature vector $\x$ is believed to correlate with $y$, one popular modeling assumption is the generalized linear model (GLM) that turns the \emph{natural parameter} of an exponential family model into $\x^\T\th^*$ where $\th^*$ is a parameter~\cite{mccullagh89generalized}:
%%% BEG
% When we have a feature representation $\x$ of the arm that we believe correlates with the reward $y$, it is natural to consider a linear parameterization of the exponential family.
% Specifically, the generalized linear model (GLM) turns the \emph{natural parameter} of an exponential family into $\x^\T\th^*$ where $\th^*$ is a parameter~\cite{mccullagh89generalized}:
%%%
%\vspace{-4pt}
\begin{equation}\begin{aligned}\label{eq:glm}
  \P(y \mid z=\x^\T\th^*) = \exp\lt( \fr{y z - m(z)}{g(\tau)} + h(y,\tau) \rt) \;,
\end{aligned}\end{equation}
where $\tau \in \dsR^+$ is a known scale parameter and $m$, $g$, and $h$ are normalizers.
It is known that $m'(z) = \expt[ y \mid z] =: \mu(z)$ and $m''(z) = \Var(y\mid z)$.
We call $\mu(z)$ the \emph{inverse link} function.
Throughout, we assume that the exponential family being used in a GLM has a \emph{minimal representation}, which ensures that $m(z)$ is strictly convex~\cite[Prop. 3.1]{wainwright08graphical}.
Then, the negative log likelihood (NLL) $\ell(z,y) := -yz + m(z)$ of a GLM is strictly convex.
We refer to such GLMs as the \emph{canonical} GLM.
%%% BEG
%Note that the NLL is proportional to $-y \cdot (\x^\T\th^*) + \mu(\x^\T\th^*) $.
%%% END
In the case of Bernoulli rewards $y \in \{0,1\}$, $m(z) = \log(1+\exp(z))$, $\mu(z) = (1+\exp(-z))^{-1}$, and the NLL can be written as the logistic loss: $\log(1 + \exp(-y'(\x_t^\T\th^*))) $, where $y' = 2y - 1$.

%%%%%%%%%%%%%%%%%%%%
\vspace{-4pt}
\paragraph{Generalized Linear Bandits (GLB)}
%%%%%%%%%%%%%%%%%%%%

Recall that $\x_t$ is the arm chosen at time $t$ by an algorithm.
We assume that the arm set $\cX_t$ can be of an infinite cardinality, although we focus on finite arm sets in hashing part of the paper (Section~\ref{sec:hashing}).
One can write down the reward model~\eqref{eq:glm} in a different form:
\vspace{-4pt}
\begin{equation}\begin{aligned}\label{eq:reward}
   y_t = \mu( \x_t^\T\th^* ) + \eta_t,
\end{aligned}\end{equation}
where $\eta_t$ is conditionally $R$-sub-Gaussian given $\x_t$ and $\{(\x_s,\eta_s)\}_{s=1}^{t-1}$.
For example, Bernoulli reward model has $\eta_t$ as $1-\mu(\x_t^\T\th^*)$ w.p. $\mu(\x_t^\T\th^*)$ and $-\mu(\x_t^\T\th^*)$ otherwise.
Assume that $||\th^*||_2 \le S$, where $S$ is known. 
One can show that the sub-Gaussian scale $R$ is determined by $\mu$: $R = \sup_{z\in(-S,S)} \sqrt{\mu'(z)} \le \sqrt{L}$, where $L$ is the Lipschitz constant of $\mu$.
Throughout, we assume that each arm has $\ell_2$-norm at most 1: $||\x||_2 \le 1, \forall \x\in \cX_t, \forall t$. 
Let $\x_{t,*} := \max_{\x\in\cX_t} \x^\T\th^*$.
The performance of a GLB algorithm $\cA$ is analyzed by the expected cumulative regret (or simply \emph{regret}): $\text{Regret}^\cA_T := \sum_{t=1}^T \mu( \x_{t,*}^\T\th^* ) - \mu( (\x^\cA_t)^\T\th^* ) $, where $\x^\cA_t$ makes the dependence on $\cA$ explicit.

We remark that our results in this paper hold true for a strictly larger family of distributions than the canonical GLM, which we call the \emph{non-canonical} GLM and explain below.
The condition is that the reward model follows~\eqref{eq:reward} where the $R$ is now independent from $\mu$ that satisfies the following:
\begin{ass} \label{ass:mu}
  $\mu$ is $L$-Lipschitz on $[-S,S]$ and continuously differentiable on $(-S,S)$.
  Furthermore, $\inf_{z \in (-S,S)} \mu'(z) = \kappa$ for some finite $\kappa>0$ (thus $\mu$ is strictly increasing).
\end{ass}
%\vspace{-7pt}
Define $\mu'(z)$ at $\pm S$ as their limits.
Under Assumption~\ref{ass:mu}, $m$ is defined to be an integral of $\mu$.
Then, one can show that $m$ is $\kap$-strongly convex on $\cB_1(S)$.
An example of the non-canonical GLM is the probit model for 0/1 reward where $\mu$ is the Gaussian CDF, which is popular and competitive to the Bernoulli GLM as evaluated by~\citet{li12anunbiased}.
% We remark that~\citet{zhang16online} is specialized to the Bernoulli GLM and thus not applicable to the probit model.
Note that canonical GLMs satisfy Assumption~\ref{ass:mu}. 

