%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-4pt}
\section{Generalized Linear Bandits with Online Computation}
\label{sec:gloc}
\vspace{-4pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We describe and analyze a new GLB algorithm called Generalized Linear Online-to-confidence-set Conversion (GLOC) that performs online computations, unlike existing GLB algorithms.

GLOC employs the optimism in the face of uncertainty principle, which dates back to~\cite{auer02using}.
That is, we maintain a confidence set $C_t$ (defined below) that traps the true parameter $\th^*$ with high probability (w.h.p.) and choose the arm with the largest feasible reward given $C_{t-1}$ as a constraint:
\begin{equation}\begin{aligned} \label{eq:glocopt}
          (\x_t,\tilth_t) := \arg \max_{\x\in\cX_t, \th\in C_{t-1}} \la\x,\th \ra
\end{aligned}\end{equation}
The main difference between GLOC and existing GLBs is in the computation of the $C_t$'s. Prior methods involve ``batch" computations that involve all past observations, and so scale poorly with $t$. In contrast, GLOC takes in an \emph{online} learner $\cB$, and uses $\cB$ as a co-routine instead of relying on a batch procedure to construct a confidence set.
Specifically, at each time $t$ GLOC feeds the loss function $\ell_t(\th) := \ell(\x_t^\T\th, y_t)$ into the learner $\cB$ which then outputs its parameter prediction $\th_t$.
Let $\X_{t} \in \dsR^{t\times d}$ be the design matrix consisting of $\x_1,\ldots,\x_t$. 
Define $\barV_t := \lam \I + \X_{t}^\T\X_{t} $, where $\lam$ is the ridge parameter.
Let $z_t := \x_t^\T\th_t$ and  $\z_t := [z_1;\cdots;z_t]$.
Let $\hth_t := \ibarV_t \X_{t}^\T \z_{t}$ be the ridge regression estimator taking $\z_t$ as responses.
Theorem~\ref{thm:o2cs} below is the key result for constructing our confidence set $C_t$, which is a function of the parameter predictions $\{\th_s\}_{s=1}^t$ and the online (OL) regret bound $B_t$ of the learner $\cB$.
All the proofs are in the supplementary material (SM).
%
\begin{thm} \label{thm:o2cs} (Generalized Linear Online-to-Confidence-Set Conversion)
  Suppose we feed loss functions $\{\ell_s(\th)\}_{s=1}^t$ into online learner $\cB$.
  Let $\th_s$ be the parameter predicted at time step $s$ by $\cB$.
  Assume that $\cB$ has an OL regret bound $B_t$: $\forall \th\in\cB_{d}(S), \forall t \ge 1,  $ 
  \begin{equation}\begin{aligned} \label{eq:ol_regret}
    \textstyle\sum_{s=1}^t \ell_s(\th_s) - \ell_s(\th) \le \blue{B_t} \;. 
  \end{aligned}\vspace{2pt}\end{equation}
  Let $\alpha(B_t) := 1 + \fr{4}{\kappa}\blue{B_t} + \fr{8R^2}{\kappa^2} \log(\fr{2}{\dt}\sqrt{ 1+ \fr{2}{\kap}\blue{B_t} + \fr{4R^4}{\kappa^4\dt^2} } )$.
  Then, with probability (w.p.) at least $1-\dt$,
  \begin{equation}\label{eq:thm_o2cs}
    \forall t\ge1, ||\th^* - \hth_t||^2_{\barV_t}  \le \alpha(\blue{B_t}) + \lam S^2 - \lt(||\z_{t}||^2_2 - \hth_t^\T\X_{t}^\T\z_{t}  \rt) =: \beta_t \;.
  \end{equation}
\end{thm}
Note that the center of the ellipsoid is the ridge regression estimator on the predicted natural parameters $z_s = \x_s^\T\th_s$ rather than the rewards.
Theorem~\ref{thm:o2cs} motivates the following confidence set:
\begin{equation}\begin{aligned} \label{eq:cset}
  C_t := \{ \th\in\dsR^d: ||\th - \hth_{t}||^2_{\barV_t} \le \beta_{t} \}
\end{aligned}\end{equation}
which traps $\th^*$ for all $t\ge1$, w.p. at least $1-\dt$.
%We describe the pseudocode of GLOC in Algorithm~\ref{alg:gloc}.
See Algorithm~\ref{alg:gloc} for pseudocode.
%%% BEG
% Note that~\eqref{eq:thm_o2cs} can be directly used to construct a confidence set by replacing $\th^*$ with a generic $\th$ since $\th^* \in \{\th: \sum_{s=1}^t (\x_s^\T(\th_s - \th))^2 \le \beta'_t\}$.
% However, the quadratic equation on $\th$ on the LHS of~\eqref{eq:thm_o2cs} could have a zero eigenvalue in its Hessian, which is cumbersome.
% Corollary~\ref{cor:o2cs} below presents a confidence set that takes an explicit ellipsoidal form, which is easier to work with.
% \begin{cor}\label{cor:o2cs}
%   Consider the same assumptions as Theorem~\ref{thm:o2cs}.
%   Let $\X_{t} \in \dsR^{t\times d}$ be the design matrix consisting of $\x_1,\ldots,\x_t$.
%   Define $z_s := \x_s^\T\th_s$,  $\z_{t} = (z_1,\ldots,z_t)^\T$, $\barV_t := \lam \I + \X_{t}^\T\X_{t} $ for some $\lam >0$, and $\hth_t := \ibarV_t \X_{t}^\T \z_{t}$.
%   Then, w.p. at least $1-\dt$, $\forall t\ge1$,
%   \vspace{-5pt}
%   \begin{equation*}
%      ||\th^* - \hth_t||^2_{\barV_t}  \le \beta'_t + \lam S^2 - \lt(||\z_{t}||^2_2 - \hth_t^\T\X_{t}^\T\z_{t}  \rt) =: \beta_t \;.
%   \end{equation*}
%   Furthermore, w.p. at least $1-\dt$, $\th^* \in C_t := \{ \th\in\dsR^d: ||\th - \hth_{t}||^2_{\barV_t} \le \beta_{t} \}, \forall t\ge1$.
% \end{cor}
% \vspace{-7pt}
%%% END
%We remark that one way to solve the optimization problem~\eqref{eq:glocopt} is to fix $\x$ and find the maximizer $\th(\x) := \max_{\th\in C_{t-1}} \x^\T\th$ in a closed form using the Lagrangian method:
One way to solve the optimization problem~\eqref{eq:glocopt} is to define the function $\th(\x) := \max_{\th\in C_{t-1}} \x^\T\th$, and then use the Lagrangian method to write:
\begin{equation}\begin{aligned}\label{eq:glocopt_ext}
  \x^{\GLOC}_t := \arg \max_{\x\in\cX_t} \x^\T\hth_{t-1} + \sqrt{\beta_{t-1}} ||\x||_{\ibarV_{t-1}} \;.
\end{aligned}\end{equation}
We prove the regret bound of GLOC in the following theorem.
\begin{thm}  \label{thm:regret_o2cs}
  Let $\{\barbeta_t\}$ be a nondecreasing sequence such that $\barbeta_t\ge\beta_t$. Then, w.p. at least $1-\dt$,
  \[
    \textstyle    \Regret^{\emph\GLOC}_T = O\lt( L\sqrt{\barbeta_T dT\log T} \rt)
  \]
\end{thm}
%
\begin{wrapfigure}{R}{0.48\textwidth}
  \vspace{-22pt}
%  \hspace{-7pt}
\begin{minipage}{0.48\textwidth}
\begin{algorithm}[H]
{\small
  \begin{algorithmic}[1]
    \STATE \textbf{Input}: $R>0$, $\dt\in(0,1)$, $S>0$, $\lam>0$, $\kappa>0$, an online learner $\cB$ with known regret bounds $\{B_t\}_{t\ge1}$. 
    \STATE Set $\barV_0 = \lam \I$.
    \FOR {$t=1,2,\ldots$}
    \STATE Compute $\x_t$ by solving~\eqref{eq:glocopt}.
      \STATE Pull $\x_t$ and then observe $y_t$.
      \STATE Receive $\th_t$ from $\cB$.
      \STATE Feed into $\cB$ the loss $\ell_t(\th) = \ell(\x_t^\T \th, y_t)$.
      \STATE Update $\barV_t = \barV_{t-1} + \x_t\x_t^\T$ and $z_t = \x_t^\T\th_t$
      \STATE Compute $\hth_t = \ibarV_t \X_t^\T\z_t$  and $\beta_t$ as in~\eqref{eq:thm_o2cs}.
      \STATE Define $C_t$ as in~\eqref{eq:cset}.
    \ENDFOR
  \end{algorithmic}
  \caption{GLOC}
  \label{alg:gloc}
}
\end{algorithm}
\vspace{-20pt}
\begin{algorithm}[H]
{\small
  \begin{algorithmic}[1]
    \STATE \textbf{Input}: $\kap>0$, $\eps>0$, $S >0$.%$\Th\subseteq\dsR^d$
    \STATE $\A_0 = \eps\I$.
    \STATE Set $\th_1 \in \cB_d(S)$ arbitrarily.
    \FOR {$t=1,2,3,\ldots$}
      \STATE Output $\th_t$ .
      \STATE Observe $\x_t$ and $y_t$. 
      \STATE Incur loss $\ell(\x_t^\T\th_t, y_t)$ .
      \STATE $\A_t = \A_{t-1} + \x_t \x_t^\T$
      \STATE $\th'_{t+1} = \th_{t} - \fr{\ell'(\x_{t}^\T\th_{t}, y_t)}{\kap} \A^{-1}_{t} \x_{t} $
      \STATE $\th_{t+1} = \arg \min_{\th\in\cB_d(S)} || \th - \th'_{t+1} ||^2_{\A_{t}}  $   
    \ENDFOR
  \end{algorithmic}
  \caption{ONS-GLM}
  \label{alg:ons}
}
\end{algorithm}
\end{minipage}
\vspace{-15pt}
\end{wrapfigure}
%
Although any low-regret online learner can be combined with GLOC, one would like to ensure that $\barbeta_T$ is $O(\text{polylog}(T))$ in which case the total regret can be bounded by $\tilde O(\sqrt{T})$.
This means that we must use online learners whose OL regret grows logarithmically in $T$ such as~\cite{hazan07logarithmic,orabona12beyond}.
In this work, we consider the online Newton step (ONS) algorithm~\cite{hazan07logarithmic}.

%%%%%%%%%%%%%%%%%%%%
\vspace{-4pt}
\paragraph{Online Newton Step (ONS) for Generalized Linear Models}
%%%%%%%%%%%%%%%%%%%%

% \begin{wrapfigure}{R}{0.46\textwidth}
%   \vspace{-25pt}
%   \hspace{-8pt}
% \begin{minipage}{0.46\textwidth}
% \begin{algorithm}[H]
%   \begin{algorithmic}[1]
%     \STATE \textbf{Input}: $\kap>0$, $\eps>0$, $S >0$.%$\Th\subseteq\dsR^d$
%     \STATE $\A_0 = \eps\I$.
%     \STATE Set $\th_1 \in \cB_d(S)$ arbitrarily.
% %    \STATE Choose $\th_1 \in \Th$ arbitrarily.
%     \FOR {$t=1,2,3,\ldots$}
%       \STATE Output $\th_t$ .
%       \STATE Observe $\x_t$ and $y_t$. 
%       \STATE Incur loss $\ell(\x_t^\T\th_t, y_t)$ .
%       \STATE $\A_t = \A_{t-1} + \x_t \x_t^\T$
%       \STATE $\th'_{t+1} = \th_{t} - \fr{\ell'(\x_{t}^\T\th_{t}, y_t)}{\kap} \A^{-1}_{t} \x_{t} $
%       \STATE $\th_{t+1} = \arg \min_{\th\in\cB_d(S)} || \th - \th'_{t+1} ||^2_{\A_{t}}  $   
%     \ENDFOR
%   \end{algorithmic}
%   \caption{Online Newton Step for Generalized Linear Model (ONS-GLM)}
%   \label{alg:ons}
% \end{algorithm}
% \end{minipage}
% \vspace{-15pt}
% \end{wrapfigure}

Note that ONS requires the loss functions to be $\alpha$-exp-concave.
One can show that $\ell_t(\th)$ is $\alpha$-exp-concave~\cite[Sec. 2.2]{hazan07logarithmic}.
Then, GLOC can use ONS and its OL regret bound to solve the GLB problem. %are immediately ready to be combined with GLOC.
However, motivated by the fact that the OL regret bound $B_t$ appears in the radius $\sqrt{\beta_t}$ of the confidence set while a tighter confidence set tends to reduce the bandit regret in practice, we derive a tight data-dependent OL regret bound tailored to GLMs.

We present our version of ONS for GLMs (ONS-GLM) in Algorithm~\ref{alg:ons}.
$\ell'(z,y)$ is the first derivative w.r.t. $z$ and the parameter $\eps$ is for inverting matrices conveniently (usually $\eps = $ 1 or 0.1).
The only difference from the original ONS~\cite{hazan07logarithmic} is that we rely on the strong convexity of $m(z)$ instead of the $\alpha$-exp-concavity of the loss thanks to the GLM structure.\footnote{ A similar change to ONS has been applied in~\cite{gentile14onmultilabel,zhang16online}.}
Theorem~\ref{thm:ons} states that we achieve the desired polylogarithmic regret in $T$.
%
\begin{thm}\label{thm:ons}
  Define $g_s := \ell'(\x_s^\T\th_s, y_s)$.
  The regret of ONS-GLM satisfies, for any $\eps > 0$ and $t\ge 1$,
  \[
    \textstyle \sum_{s=1}^t \ell_s( \th_s) - \ell_s(\th^*) \le \fr{1}{2\kap} \sum_{s=1}^t g_s^2 ||\x_s||^2_{\A_s^{-1}} + 2\kap S^2 \eps =: B^{\emph\ONS}_t \;,
  \]
  where $B^\ONS_t = O(\fr{L^2+ R^2\log(t)}{\kap}d\log t), \forall t\ge1$ w.h.p. If $\max_{s\ge1} |\eta_s|$ is bounded by $\bar R$ w.p. 1, $B^\ONS_t = O(\fr{L^2+\bar R^2}{\kap}d\log t)$.
\end{thm}
We emphasize that the OL regret bound is data-dependent. % and its magnitude can be bounded by problem constants (shown after the proof in SM).
%
A confidence set constructed by combining Theorem~\ref{thm:o2cs} and Theorem~\ref{thm:ons} directly implies the following regret bound of GLOC with ONS-GLM.
\begin{cor} \label{cor:cset_ONS}
  Define $\beta_t^{\emph\ONS}$ by replacing $B_t$ with $B^{\emph\ONS}_t$ in~\eqref{eq:thm_o2cs}.
  With probability at least $1-2\dt$, 
  \begin{equation}\begin{aligned} \label{eq:cset_ONS}
      \forall t\ge1, \th^* \in C^{\emph\ONS}_t := \lt\{\th\in\dsR^d : || \th - \hth_t ||^2_{\barV_t} \le \beta^{\emph\ONS}_t \rt\} \;.
  \end{aligned}\end{equation}                                                                                 
%   where $\beta^{\emph\ONS}_t = \hat O( ( \fr{L^2 + R^2}{\kap^2}) d \log^2(t) )$ and $\hat{O}$ ignores $\log\log(t)$. 
%   If $|\eta_s|$ is bounded by $\bar R$, $\beta^{\emph\ONS}_t = O( ( \fr{L^2 + {\bar R}^2}{\kap^2}) d \log(t) )$.
\end{cor}
\begin{cor} \label{cor:regret_glocon_ons}
  Run GLOC with $C^{\ONS}_t$. Then, w.p. at least $1-2\dt$, $\forall T\ge1$,
  $\Regret_T^{\GLOC} = \hat O\lt(\fr{L(L+R)}{\kap} d \sqrt{T} \log^{3/2}(T)\rt) $ where $\hat{O}$ ignores $\log\log(t)$.
  If $|\eta_t|$ is bounded by $\bar R$, $\Regret_T^{\GLOC} = \hat O\lt(\fr{L(L + \bar R)}{\kap} d \sqrt{T} \log(T)\rt)$.
\end{cor}

We make regret bound comparisons ignoring $\log\log T$ factors.
For generic arm sets, our dependence on $d$ is optimal for linear rewards~\cite{rusmevichientong10linearly}.
For the Bernoulli GLM, our regret has the same order as~\citet{zhang16online}.
One can show that the regret of~\citet{filippi10parametric} has the same order as ours if we use their assumption that the reward $y_t$ is bounded by $R_{\max}$. % (e.g., $\bar R=1/2$ for Bernoulli).
For unbounded noise,~\citet{li17provable} have regret $O((LR/\kap) d\sqrt{T} \log T)$, which is $\sqrt{\log T}$ factor smaller than ours and has $LR$ in place of $L(L+R)$.
While $L(L+R)$ could be an artifact of our analysis, the gap is not too large for canonical GLMs.
Let $L$ be the smallest Lipschitz constant of $\mu$.
Then, $R=\sqrt{L}$.
If $L \le 1$, $R$ satisfies $R > L$, and so $L(L+R)=O(LR)$.
If $L > 1$, then $L(L+R) = O(L^2)$, which is larger than $LR = O(L^{3/2})$.
For the Gaussian GLM with known variance $\sig^2$, $L=R=1$.\footnote{
  The reason why $R$ is not $\sig$ here is that the sufficient statistic of the GLM is $y/\sig$, which is equivalent to dealing with the normalized reward.
  Then, $\sig$ appears as a factor in the regret bound.
}
For finite arm sets, SupCB-GLM of~\citet{li17provable} achieves regret of $\tilde O(\sqrt{dT\log N})$ that has a better scaling with $d$ but is not a practical algorithm as it wastes a large number of arm pulls.
Finally, we remark that none of the existing GLB algorithms are scalable to large $T$.
\citet{zhang16online} is scalable to large $T$, but is restricted to the Bernoulli GLM; e.g., theirs does not allow the probit model (non-canonical GLM) that is popular and shown to be competitive to the Bernoulli GLM~\cite{li12anunbiased}.



%%%%%%%%%%%%%%%%%%%%
\vspace{-4pt}
\paragraph{Discussion} 
%%%%%%%%%%%%%%%%%%%%
%\kw{place the following in the s/m}
The trick of obtaining a confidence set from an online learner appeared first in~\cite{dekel10robust,dekel12selective} for the linear model, and then was used in~\cite{crammer13multiclass,gentile14onmultilabel,zhang16online}.
GLOC is slightly different from these studies and rather close to~\citet{ay12online} in that the confidence set is a function of a known regret bound. %, which has a greater generality.
This generality frees us from re-deriving a confidence set for every online learner. %; one reduction for all.
Our result is essentially a nontrivial extension of~\citet{ay12online} to GLMs.

One might have notice that $C_t$ does not use $\th_{t+1}$ that is available before pulling $\x_{t+1}$ and has the most up-to-date information.
This is inherent to GLOC as it relies on the OL regret bound directly.
One can modify the proof of ONS-GLM to have a tighter confidence set $C_t$ that uses $\th_{t+1}$ as we show in SM Section~\ref{sec:supp_tighter}.
However, this is now specific to ONS-GLM, which looses generality. 
%%% BEG
% One might have noticed that the confidence set $C_{t-1}$ used for choosing the arm $\x_t$ does not depend directly on $y_{t-1}$ but only through $g_{t-1}$.
% This comes from the fact that we directly use the regret bound rather than performing a fresh analysis of the online learner. 
% While this approach provides a great generality, one can indeed use $y_{t-1}$ for the confidence set $C_{t-1}$ after a careful analysis as we show in SM Section~\ref{sec:supp_tighter}.
% However, the result on a tighter confidence set is now specific to ONS, which looses generality.
%%% END

