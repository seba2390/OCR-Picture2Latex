%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-04pt}
\section{Hash-Amenable Generalized Linear Bandits}
\label{sec:hashing}
\vspace{-4pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We now turn to a setting where the arm set is finite but very large.
For example, imagine an interactive retrieval scenario~\cite{rui98relevance,konyushkova13content,glowacka15balancing} where a user is shown $K$ images (e.g., shoes) at a time and provides relevance feedback (e.g., yes/no or 5-star rating) on each image, which is repeated until the user is satisfied.
In this paper, we focus on showing one image (i.e., arm) at a time.\footnote{
  One image at a time is a simplification of the practical setting.
  One can extend it to showing multiple images at a time, which is a special case of the combinatorial bandits of~\citet{qin14contextual}.
}
Most existing algorithms require maximizing an objective function (e.g., \eqref{eq:glocopt_ext}), the complexity of which scales linearly with the number $N$ of arms. 
This can easily become prohibitive for large numbers of images.
Furthermore, the system has to perform real-time computations to promptly choose which image to show the user in the next round.
Thus, it is critical for a practical system to have a time complexity sublinear in $N$.

One naive approach is to select a subset of arms ahead of time, such as volumetric spanners~\cite{hazan16volumetric}.
However, this is specialized for an efficient exploration only and can rule out a large number of good arms.
Another option is to use hashing methods.
Locality-sensitive hashing and Maximum Inner Product Search (MIPS) are effective and well-understood tools but can only be used when the objective function is a distance or an inner product computation;~\eqref{eq:glocopt_ext} cannot be written in this form.
In this section, we consider alternatives to GLOC which are compatible with hashing.

%%% BEG
% Another option is to try to use a locality-sensitive hashing or Maximum Inner Product Search (MIPS) hashing to solve~\eqref{eq:glocopt_ext}. 
% However, it is unclear how to turn the objective function in~\eqref{eq:glocopt_ext} into a distance computation or an inner product since the second term $||\x||_{\ibarV_{t-1}}$ therein involves a square root that makes the function neither linear nor quadratic.
% An alternative is to use GLOC-TS whose objective function is an inner product, but its regret bound scales with $d^{3/2}$ rather than $d$ of GLOC.
% This is concerning in the interactive retrieval or product recommendation scenario since the relevance of the shown items is harmed, which makes us wonder if one can improve the regret without loosing the hash-amenability.
%%% END

%%%%%%%%%%%%%%%%%%%%
\vspace{-6pt}
\paragraph{Thompson Sampling} 
%%%%%%%%%%%%%%%%%%%%
%
We present a Thompson sampling (TS) version of GLOC called GLOC-TS that chooses an arm $\x_t = \arg \max_{\x\in\cX_t} \x^\T \dot\th_t$ where $\dot\th_t \sim \cN(\hth_{t-1}, \beta_{t-1}\ibarV_{t-1})$.
TS is known to perform well in practice~\cite{chapelle11anempirical} and can solve the polytope arm set case in polynomial time\footnote{ConfidenceBall$_1$ algorithm of~\citet{dani08stochastic} can solve the problem in polynomial time as well.} whereas algorithms that solve an objective function like~\eqref{eq:glocopt} (e.g.,~\cite{ay11improved}) cannot since they have to solve an NP-hard problem~\cite{agrawal13thompson}.
We present the regret bound of GLOC-TS below. 
Due to space constraints, we present the pseudocode and the full version of the result in SM.
\begin{thm} (Informal) \label{thm:gloc-ts}
  If we run GLOC-TS with $\dot \th_t \sim \cN(\hth_{t-1},\beta^{\ONS}_{t-1}\ibarV_{t-1})$, $\Regret^{\GLOCTS}_T = \hat O\lt(\fr{L(L+R)}{\kap} d^{3/2} \sqrt{T}\log^{3/2}(T)\rt)$ w.h.p.
  If $\eta_t$ is bounded by $\bar R$, then $\hat O\lt(\fr{L(L+\bar R)}{\kap} d^{3/2} \sqrt{T}\log(T)\rt)$.
\end{thm}
Notice that the regret now scales with $d^{3/2}$ as expected from the analysis of linear TS~\cite{agrawal14thompson}, which is higher than scaling with $d$ of GLOC.
This is concerning in the interactive retrieval or product recommendation scenario since the relevance of the shown items is harmed, which makes us wonder if one can improve the regret without loosing the hash-amenability.

%%%%%%%%%%%%%%%%%%%%
\vspace{-6pt}
\paragraph{Quadratic GLOC} 
%%%%%%%%%%%%%%%%%%%%
We now propose a new hash-amenable algorithm called Quadratic GLOC (QGLOC).
Recall that GLOC chooses the arm $\x^{\GLOC}$ by~\eqref{eq:glocopt_ext}.
Define $r = \min_{\x\in\calX} ||\x||_2$ and 
\begin{equation}\begin{aligned} \label{def-m}
    \barm_{t-1} := \min_{\x: ||\x||_2 \in [r,1]} ||\x||_{\ibarV_{t-1}} \;,
\end{aligned}\end{equation}
which is $r$ times the square root of the smallest eigenvalue of $\ibarV_{t-1}$.
It is easy to see that $\barm_{t-1} \le ||\x||_{\ibarV_{t-1}}$ for all $\x\in\cX$ and that $\barm_{t-1}\ge r/\sqrt{t+\lam}$ using the definition of $\barV_{t-1}$.
There is an alternative way to define $\barm_{t-1}$ without relying on $r$, which we present in SM.

Let $c_0 > 0$ be the exploration-exploitation tradeoff parameter (elaborated upon later).
At time $t$, QGLOC chooses the arm
\begin{equation}\begin{aligned}    \label{eq:qglocopt} 
  \x_{t}^{\QGLOC} :=&  \arg \max_{\x\in\calX_t} \la \hatth_{t-1}, \x \ra  + \fr{\beta_{t-1}^{1/4}}{4 c_0 \barm_{t-1}} || \x ||^2_{\barV^{-1}_{t-1}} 
= \arg \max_{\x\in\cX_t} \lt\langle \q_t,  \phi(\x) \rt\rangle \;, 
\end{aligned}\end{equation}
where $\q_t = [ \hth_{t-1}; \vec( \fr{\beta^{1/4}_{t-1} }{4c_0\barm_{t-1}} \ibarV_{t-1} )] \in \dsR^{d+d^2}$ and $\phi(\x) := [\x; \vec(\x\x^\T)]$.
The key property of QGLOC is that the objective function is now quadratic in $\x$, thus the name \emph{Quadratic} GLOC, and can be written as an inner product.
Thus, QGLOC is hash-amenable.
We present the regret bound of QGLOC~\eqref{eq:qglocopt} in Theorem~\ref{thm:x_t9}.
The key step of the proof is that the QGLOC objective function~\eqref{eq:qglocopt} plus $c_0\beta^{3/4} \barm_{t-1}$ is a tight upper bound of the GLOC objective function~\eqref{eq:glocopt_ext}.
\begin{thm}\label{thm:x_t9}
  Run QGLOC with $C^{\ONS}_t$.
  Then, w.p. at least $1-2\dt$,
  \[
    \Regret_T^{\emph{\text{QGLOC}}} = O\lt(  \lt(\fr{1}{c_0}\lt( \fr{L+R}{\kap} \rt)^{1/2} + c_0\lt( \fr{L+R}{\kap} \rt)^{3/2} \rt) Ld^{5/4} \sqrt{T} \log^2(T)\rt)\;.
    \]
   By setting $c_0 = \lt( \fr{L+R}{\kap} \rt)^{-1/2}$, the regret bound is $O(\fr{L(L+R)}{\kap} d^{5/4} \sqrt{T} \log^2(T)) $.
%   Set $c_0 = c_0'\lt(\kap^{-2}(L^2 + R^2)\log(t)\rt)^{-1/4}$, where $c_0'>0$ is an absolute constant.
%   Run QGLOC with $C^{\ONS}_t$.
%   Then, w.p. at least $1-2\dt$,
%     $
%     \emph{\text{Regret}}_T^{\emph{\text{QGLOC}}} = O( \kap^{-1}L(L+R) d^{5/4} \sqrt{T} \log^{7/4}(T) )
%     $.
\end{thm}
%%% BEG: maybe not need
%The key step of the proof is that the maximum of the QGLOC objective function~\eqref{eq:qglocopt} plus $c_0\beta^{3/4} \barm_{t-1}$ is a tight upper bound of the maximum of the GLOC objective function~\eqref{eq:glocopt_ext}.
%%% END
Note that one can have a better dependence on $\log T$ when $\eta_t$ is bounded (available in the proof).
The regret bound of QGLOC is a $d^{1/4}$ factor improvement over that of GLOC-TS; see Table~\ref{tab:bandits}.
Furthermore, in~\eqref{eq:qglocopt} $c_0$ is a free parameter that adjusts the balance between the exploitation (the first term) and exploration (the second term).
Interestingly, the regret guarantee \emph{does not break down} when adjusting $c_0$ in Theorem~\ref{thm:x_t9}.
Such a characteristic is not found in existing algorithms but is attractive to practitioners, which we elaborate in SM.

%%%%%%%%%%%%%%%%%%%%
\vspace{-6pt}
\paragraph{Maximum Inner Product Search (MIPS) Hashing}
%%%%%%%%%%%%%%%%%%%%

While MIPS hashing algorithms such as~\cite{shrivastava14asymmetric,shrivastava15improved,neyshabur15on} can solve~\eqref{eq:qglocopt} in time sublinear in $N$, these necessarily introduce an approximation error.
Ideally, one would like the following guarantee on the error with probability at least $1-\dtH$:
\vspace{-3pt}
\begin{defn}\label{def:cmips}
 Let $\cX \subseteq \dsR^{d'}$ satisfy $|\cX| < \infty$.
 A data point $\til\x \in \cX$ is called $\ccH$-MIPS w.r.t. a given query $\q$ if it satisfies $\la \q, \til\x \ra \ge \ccH \cdot \max_{\x\in\cX}\la \q, \x \ra$ for some $\ccH<1$.
 An algorithm is called $\ccH$-MIPS if, given a query $\q\in {\dsR}^{d'}$, it retrieves $\x \in \cX$ that is $\ccH$-MIPS w.r.t. $\q$.
\end{defn}
\vspace{-5pt}
Unfortunately, existing MIPS algorithms do not directly offer such a guarantee, and one must build a series of hashing schemes with varying hashing parameters like~\citet{indyk12approximate}.
Under the fixed budget setting $T$, we elaborate our construction that is simpler than~\cite{indyk12approximate} in SM.
%%% BEG: some try to explain the hashing
% The way hashing works is that each hash table uses $k$ keys where each key is a discretized value of an inner product between an arm $\x'\in\dsR^{d'}$ and an independent normally-distributed vector. 
% Due to the discretization, $k$ keys becomes a bucket index, and we store item (arm) pointers in the buckets according to their index.
% We build this table $U$ times.
% The key is that given a query $\q$ one can compute its hash keys in a similar way, looks up the matching buckets, and find the inner product maximizer from there.
% We further construct $J$ such hashing schemes for a technical reason.
%%% END
%%% BEG
% Assuming the fixed budget setting with time horizon $T$, we show that the maximimum of~\eqref{eq:qglocopt} is trapped in $[M_{\min},M_{\max}]$ with high probability, where $M_{\min} = O(1)$ and $M_{\max} = \tilde O(d^{1/4}\sqrt{T}))$.
% Then, we build $J := \lcl \log_{1/\sqrt{\ccH}} (M_{\max}/M_{\min}) \rcl  = \hat O(\log (dT) / \log(\ccH^{-1}))$ independent hashings with varying parameters where each has $U$ tables with length-$k$ hash keys.
% Here, $k= O(\log N)$ and $U = O(N^{\rho^*}) $, where $\rho^*$ is an optimized value that is always less than 1. % and depends on $\ccH$.
% Although the dependencies between $(\ccH, \dtH)$ and $(J, U, k)$ are complicated and often omitted here, we remark that as we increase $\ccH$ and reduce $\dtH$ (more accurate) we need to increase $J$, $U$, and $k$ (more space and time).
%%% END

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-6pt}
\paragraph{Time and Space Complexity} % Comparison to Other Hash-Amenable Algorithms}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% BEG
% The time complexity of our construction with $d'$-dimensional vectors is $O(\log(J) Ukd')$, and the space complexity (except the original data) is $O(JU(N + kd'))$.
% QGLOC uses $d'=d+d^2$.\footnote{
%   Note that this does not mean we need to store $\text{vec}(\x\x^\T)$ since an inner product with it is structured.
% }
% The time complexity of our $\ccH$-MIPS hashing for QGLOC is thus $O(\log(J) N^{\rho^*} \log(N) d^2)$ per query.
% This achieves a sublinear time in $N$.
% %The extra space (except for the original data) required is $JUk$ projection vectors of $(d+d^2)$-dimension plus $O(JUN)$ number of item pointers (indexes) residing in buckets.
% The space complexity is $O(J N^{\rho^*}(N + d^2 \log(N)))$.
% While the time and space complexity grows with the time horizon $T$, the dependence is mild; $\log \log(T)$ and $\log(T)$, respectively.
% 
% GLOC-TS can use a similar hashing scheme.
% GLOC-TS uses $d'=d$, so the time and space complexity is $O(\log(J)N^{\rho^*}\log(N) d)$ and $O(J N^{\rho^*}(N + d \log(N)))$, respectively, which are both a factor-of-$d$ smaller than that of QGLOC. 
% % The extra space complexity of GLOC-TS is $O(J N^{\rho^*}(N + d \log(N)))$, which is also smaller.
% However, GLOC-TS has a worse regret bound than QGLOC.
%%% END
Our construction involves saving Gaussian projection vectors that are used for determining hash keys and saving the buckets containing pointers to the actual arm vectors.
The time complexity for retrieving a $\ccH$-MIPS solution involves determining hash keys and evaluating inner products with the arms in the retrieved buckets.
Let $\rho^*<1$ be an optimized value for the hashing (see~\cite{shrivastava14asymmetric} for detail).
The time complexity for $d'$-dimensional vectors is $O\lt(\log\lt(\fr{\log(dT)}{\log(\ccH^{-1})}\rt) N^{\rho^*} \log(N) d'\rt)$, and the space complexity (except the original data) is $O\lt(\fr{\log(dT)}{\log(\ccH^{-1})} N^{\rho^*}(N + \log(N)d')\rt)$.
While the time and space complexity grows with the time horizon $T$, the dependence is mild; $\log \log(T)$ and $\log(T)$, respectively.
QGLOC uses $d'=d+d^2$,\footnote{
  Note that this does not mean we need to store $\text{vec}(\x\x^\T)$ since an inner product with it is structured.
}
  and GLOC-TS uses $d'=d'$.
While both achieve a time complexity sublinear in $N$, the time complexity of GLOC-TS scales with $d$ that is better than scaling with $d^2$ of QGLOC.
However, GLOC-TS has a $d^{1/4}$-factor worse regret bound than QGLOC.

%%%%%%%%%%%%%%%%%%%%
\vspace{-6pt}
\paragraph{Discussion}
%%%%%%%%%%%%%%%%%%%%

While it is reasonable to incur small errors in solving the arm selection criteria like~\eqref{eq:qglocopt} and sacrifice some regret in practice, the regret bounds of QGLOC and GLOC-TS do not hold anymore. % presence of the errors break the regret bound.
Though not the focus of our paper, we prove a regret bound under the presence of the hashing error in the fixed budget setting for QGLOC; see SM.
Although the result therein has an inefficient space complexity that is linear in $T$, it provides the first low regret bound with time sublinear in $N$, to our knowledge.

