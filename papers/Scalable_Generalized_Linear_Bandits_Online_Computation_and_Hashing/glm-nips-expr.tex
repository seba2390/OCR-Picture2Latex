\vspace{-4pt}
\section{Experiments}
\label{sec:expr}
\vspace{-4pt}

\begin{figure}
% \vspace{-20pt}
  \begin{center}
{  \centering \footnotesize
\begin{tabular}{ccc}
  \hspace{-5pt}\imagetop{\includegraphics[width=.29\textwidth,valign=top]{figs/Logit_true_regretv5}} &
  \imagetop{\includegraphics[width=.29\textwidth,valign=top]{figs/Probit_true_regretv5}} \vspace{5pt} &
  \imagetop{\begin{tabular}{lc} \hline
    Algorithm     & Cum. Regret \\ \hline
%    GLOC          & 270.2 ($\pm$20.5)  \\
%    GLOC-Lite     & 305.1 ($\pm$15.9)  \\
    QGLOC         & 266.6 ($\pm$19.7)  \\
%    QGLOC-Lite    & 302.9 ($\pm$24.2)  \\
    QGLOC-Hash    & 285.0 ($\pm$30.3)  \\
    GLOC-TS       & 277.0 ($\pm$36.1)  \\
%    GLOC-TS-Lite  & 321.9 ($\pm$20.2)  \\
    GLOC-TS-Hash  & 289.1 ($\pm$28.1)  \\ \hline
  \end{tabular}} 
\\ (a) & (b) & (c)
\end{tabular}
}
\end{center}
\vspace{-18pt}
\caption{
  Cumulative regrets with confidence intervals under the (a) logit and (b) probit model.
  (c) Cumulative regrets with confidence intervals of hash-amenable algorithms.
}
\label{fig:true_regret} 
\vspace{-13pt}
\end{figure}

We now show our experiment results comparing GLB algorithms and hash-amenable algorithms.

\vspace{-6pt}
\paragraph{GLB Algorithms}
%
We compare GLOC with two different algorithms: UCB-GLM~\cite{li17provable} and Online Learning for Logit Model (OL2M)~\cite{zhang16online}.\footnote{We have chosen UCB-GLM over GLM-UCB of~\citet{filippi10parametric} as UCB-GLM has a lower regret bound.} % while the two are very similar. } 
For each trial, we draw $\th^*\in\dsR^d$ and $N$ arms ($\cX$) uniformly at random from the unit sphere.
We set $d=10$ and $\cX_t = \cX$, $\forall t\ge1$.
Note it is a common practice to scale the confidence set radius for bandits~\cite{chapelle11anempirical,li12anunbiased}.
Following~\citet{zhang16online}, for OL2M we set the squared radius $\gamma_t = c \log(\det(\Z_t)/\det(\Z_1))$, where $c$ is a tuning parameter.
For UCB-GLM, we set the radius as $\alpha = \sqrt{c d\log t}$.
For GLOC, we replace $\beta^\ONS_t$ with $c \sum_{s=1}^t g_s^2 ||\x_s||^2_{\A_s^{-1}}$.
While parameter tuning in practice is nontrivial, for the sake of comparison we tune $c \in \{10^1, 10^{0.5}, \ldots, 10^{-3}\}$ and report the best one.
We perform 40 trials up to time $T=3000$ for each method and compute confidence bounds on the regret.

We consider two GLM rewards: $(i)$ the logit model (the Bernoulli GLM) and $(ii)$ the probit model (non-canonical GLM) for 0/1 rewards that sets $\mu$ as the probit function.
Since OL2M is for the logit model only, we expect to see the consequences of model mismatch in the probit setting. 
For GLOC and UCB-GLM, we specify the correct reward model.
We plot the cumulative regret under the logit model in Figure~\ref{fig:true_regret}(a).
All three methods perform similarly, and we do not find any statistically significant difference based on paired t test.
The result for the probit model in Figure~\ref{fig:true_regret}(b) shows that OL2M indeed has higher regret than both GLOC and UCB-GLM due to the model mismatch in the probit setting.
Specifically, we verify that at $t=3000$ the difference between the regret of UCB-GLM and OL2M is statistically significant.
Furthermore, OL2M exhibits a significantly higher variance in the regret, which is unattractive in practice.
This shows the importance of being generalizable to \emph{any} GLM reward. 
Note we observe a big increase in running time for UCB-GLM compared to OL2M and GLOC.

\vspace{-6pt}
\paragraph{Hash-Amenable GLBs}\quad
%
To compare hash-amenable GLBs, we use the logit model as above but now with $N$=100,000 and $T$=5000.
We run QGLOC, QGLOC with hashing (QGLOC-Hash), GLOC-TS, and GLOC-TS with hashing (GLOC-TS-Hash), where we use the hashing to compute the objective function (e.g.,~\eqref{eq:qglocopt}) on just 1\% of the data points and save a significant amount of computation.
Details on our hashing implementation is found in SM.
Figure~\ref{fig:true_regret}(c) summarizes the result. 
%%% BEG
%where we also run GLOC (not hash-amenable) for the sake of comparison.
%We observe that both QGLOC and GLOC-TS are comparable to GLOC while QGLOC % has the smallest regret despite the fact that the computation was performed only on . 
%%% END
We observe that QGLOC-Hash and GLOC-TS-Hash increase regret from QGLOC and GLOC-TS, respectively, but only moderately, which shows the efficacy of hashing.

