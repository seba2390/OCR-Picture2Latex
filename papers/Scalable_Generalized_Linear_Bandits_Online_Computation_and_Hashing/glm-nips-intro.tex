%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This paper considers the problem of making generalized linear bandits (GLBs) scalable.
In the stochastic GLB problem, a learner makes successive decisions to maximize her cumulative rewards.
Specifically, at time $t$ the learner observes a set of arms $\cX_t\subseteq\dsR^d$.
The learner then chooses an arm $\x_t\in\cX_t$ and receives a stochastic reward $y_t$ that is a noisy function of $\x_t$:
  $  y_t = \mu(\x_t^\T\th^*) + \eta_t $,
  where $\th^*\in\dsR^d$ is unknown, $\mu\hspace{-2pt}:\hspace{-2pt}\dsR\hspace{-2pt}\rarrow\hspace{-2pt}\dsR$ is a known nonlinear mapping, and $\eta_t\in\dsR$ is some zero-mean noise. 
This reward structure encompasses generalized linear models~\cite{mccullagh89generalized}; e.g., Bernoulli, Poisson, etc.

The key aspect of the bandit problem is that the learner does not know how much reward she would have received, had she chosen another arm.
The estimation on $\th^*$ is thus biased by the history of the selected arms, and one needs to mix in exploratory arm selections to avoid ruling out the optimal arm.
This is well-known as the exploration-exploitation dilemma. 
The performance of a learner is evaluated by its \emph{regret} that measures how much cumulative reward she would have gained additionally if she had known the true $\th^*$.
We provide backgrounds and formal definitions in Section~\ref{sec:prelim}.

A linear case of the problem above ($\mu(z)=z$) is called the (stochastic) linear bandit problem.
Since the first formulation of the linear bandits~\cite{auer02using}, there has been a flurry of studies on the problem~\cite{dani08stochastic,rusmevichientong10linearly,ay11improved,chu11contextual,agrawal13thompson}.
In an effort to generalize the restrictive linear rewards, \citet{filippi10parametric} propose the GLB problem and provide a low-regret algorithm, whose Thompson sampling version appears later in~\citet{abeille17linear}.
\citet{li12anunbiased} evaluates GLBs via extensive experiments where GLBs exhibit lower regrets than linear bandits for 0/1 rewards.
\citet{li17provable} achieves a smaller regret bound when the arm set $\cX_t$ is finite, though with an impractical algorithm.

\textit{However, we claim that all existing GLB algorithms~\cite{filippi10parametric,li17provable} suffer from two scalability issues that limit their practical use: (i) under a large time horizon and (ii) under a large number $N$ of arms.}

First, existing GLBs require storing all the arms and rewards appeared so far, $\{(\x_s,y_s)\}_{s=1}^t$, so the space complexity grows linearly with $t$.
Furthermore, they have to solve a batch optimization problem for the maximum likelihood estimation (MLE) at each time step $t$ whose per-time-step time complexity grows at least linearly with $t$.
While~\citet{zhang16online} provide a solution whose space and time complexity do not grow over time, they consider a specific 0/1 reward with the logistic link function, and a generic solution for GLBs is not provided.

% First, existing GLBs must store all the chosen arms and rewards so far, $\{(\x_s,y_s)\}_{s=1}^t$; the space complexity grows linearly with $t$.
% Moreover, they must solve the maximum likelihood estimation problem at each time step whose per-time-step time complexity grows at least linearly with $t$.
% While the algorithm of~\citet{zhang16online} enjoys constant space and time complexity in $t$, it is limited to a specific 0/1 reward with the logistic link function, and a generic solution for GLBs is not provided.

Second, existing GLBs have linear time complexities in $N$.
This is impractical when $N$ is very large, which is not uncommon in applications of GLBs such as online advertisements, recommendation systems, and interactive retrieval of images or documents~\cite{li10acontextual,li12anunbiased,yue12hierarchical,hofmann11contextual,konyushkova13content} where arms are items in a very large database.
Furthermore, the interactive nature of these systems requires prompt responses as users do not want to wait.
This implies that the typical linear time in $N$ is not tenable.
Towards a \emph{sublinear} time in $N$, locality sensitive hashings~\cite{indyk12approximate} or its extensions~\cite{shrivastava14asymmetric,shrivastava15improved,neyshabur15on} are good candidates as they have been successful in fast similarity search and other machine learning problems like active learning~\cite{jain10hashing}, where the search time scales with $N^{\rho}$ for some $\rho<1$ ($\rho$ is usually optimized and often ranges from 0.4 to 0.8 depending on the target search accuracy).
Leveraging hashing in GLBs, however, relies critically on the objective function used for arm selections.
The function must take a form that is readily optimized using \emph{existing} hashing algorithms.\footnote{
  Without this designation, no {\em currently known} bandit algorithm achieves a sublinear time complexity in $N$.%
}
For example, algorithms whose objective function (a function of each arm $\x\in\cX_t$) can be written as a distance or inner product between $\x$ and a query $\q$ are hash-amenable as there \emph{exist} hashing methods for such functions.

To be scalable to a large time horizon, we propose a new algorithmic framework called Generalized Linear Online-to-confidence-set Conversion (GLOC) that takes in an online learning (OL) algorithm with a low  `OL' regret bound and turns it into a GLB algorithm with a low `GLB' regret bound.
The key tool is a novel generalization of the online-to-confidence-set conversion technique used in~\cite{ay12online} (also similar to~\cite{dekel12selective,crammer13multiclass,gentile14onmultilabel,zhang16online}).
This allows us to construct a confidence set for $\th^*$, which is then used to choose an arm $\x_t$ according to the well-known optimism in the face of uncertainty principle.
By relying on an online learner, GLOC inherently performs online computations and is thus free from the scalability issues in large time steps.
While any online learner equipped with a low OL regret bound can be used, we choose the online Newton step (ONS) algorithm and prove a tight OL regret bound, which results in a practical GLB algorithm with almost the same regret bound as existing inefficient GLB algorithms.
We present our proposed algorithms and their regret bounds in Section~\ref{sec:gloc}.

\begin{wrapfigure}{R}{0.45\textwidth}
  \vspace{-20pt}
  \hspace{-7pt}
\begin{minipage}{0.45\textwidth}
\begin{table}[H]
  {\centering
  \begin{tabular}{ccc} \hline	
    Algorithm & Regret                  & Hash-amenable  \\ \hline
GLOC      & $\tilde O(d\sqrt{T})$        & \rno           \\ 
GLOC-TS   & $\tilde O(d^{3/2} \sqrt{T})$ & \gyes          \\
QGLOC     & $\tilde O(d^{5/4} \sqrt{T})$ & \gyes          \\ \hline
  \end{tabular}
  \vspace{-5pt}
    \caption{Comparison of GLBs algorithms for $d$-dimensional arm sets 
      $T$ is the time horizon.
      QGLOC achieves the smallest regret among hash-amenable algorithms.
       } 
    \label{tab:bandits}
  }
\end{table}
\end{minipage}
\vspace{-10pt}
\end{wrapfigure}
%
For large number $N$ of arms, our proposed algorithm GLOC is not hash-amenable, to our knowledge, due to its nonlinear criterion for arm selection.
As the first attempt, we derive a Thompson sampling~\cite{agrawal13thompson,abeille17linear} extension of GLOC (GLOC-TS), which is hash-amenable due to its linear criterion.
However, its regret bound scales with $d^{3/2}$ for $d$-dimensional arm sets, which is far from $d$ of GLOC.
Towards closing this gap, we propose a new algorithm Quadratic GLOC (QGLOC) with a regret bound that scales with $d^{5/4}$.
We summarize the comparison of our proposed GLB algorithms in Table~\ref{tab:bandits}.
In Section~\ref{sec:hashing}, we present GLOC-TS, QGLOC, and their regret bound.

Note that, while hashing achieves a time complexity sublinear in $N$, there is a nontrivial overhead of computing the projections to determine the hash keys.
As an extra contribution, we reduce this overhead by proposing a new sampling-based approximate inner product method.
Our proposed sampling method has smaller variance than the state-of-the-art sampling method proposed by~\cite{jain10hashing,kannan09spectral} when the vectors are normally distributed, which fits our setting where projection vectors are indeed normally distributed. 
Moreover, our method results in thinner tails in the distribution of estimation error than the existing method, which implies a better concentration.
We elaborate more on reducing the computational complexity of QOFUL in Section~\ref{sec:iprod}.

