%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-4pt}
\section{Approximate Inner Product Computations with L1 Sampling}
\label{sec:iprod}
\vspace{-4pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While hashing allows a time complexity sublinear in $N$, it performs an additional computation for determining the hash keys.
Consider a hashing with $U$ tables and length-$k$ hash keys. 
Given a query $\q$ and projection vectors $\a^{(1)}, \ldots, \a^{(Uk)}$, the hashing computes $\q^\T \a^{(i)}$, $\forall i \in [Uk]$ to determine the hash key of $\q$. 
To reduce such an overhead, approximate inner product methods like~\cite{jain10hashing,kannan09spectral} are attractive since hash keys are determined by discretizing the inner products; small inner product errors often do not alter the hash keys. 

In this section, we propose an improved approximate inner product method called \emph{L1 sampling} which we claim is more accurate than the sampling proposed by~\citet{jain10hashing}, which we call \emph{L2 sampling}.
Consider an inner product $\q^\T \a$.
The main idea is to construct an unbiased estimate of $\q^\T \a$.
That is, let $\p \in \dsR^d$ be a probability vector.
Let
\begin{equation}\begin{aligned} \label{eq:def_iprod} 
  i_k \stackrel{\text{i.i.d.}}{\sim} \text{Multinomial}(\p) \quad \text{ and } \quad
  G_k := q_{i_k} a_{i_k} / p_{i_k}, \; k \in [m]  \;.
\end{aligned}\end{equation}
It is easy to see that $\expt G_k = \q^\T \a$.
By taking $\tfrac{1}{m}\sum_{k=1}^m G_k$ as an estimate of $\q^\T\a$, the time complexity is now $O(mUk)$ rather than $O(d'Uk)$.
The key is to choose the right $\p$.
L2 sampling uses $\p^{(\tL2)} := [q_i^2 / ||\q||_2^2]_i$.
Departing from L2, we propose $\p^{(\tL1)}$ that we call L1 sampling and define as follows:
\begin{align}
  \p^{(\tL1)} := [|q_1|;\cdots;|q_{d'}|] / || \q ||_1 \;.
\end{align}
%
We compare L1 with L2 in two different point of view.
Due to space constraints, we summarize the key ideas and defer the details to SM.

The first is on their concentration of measure.
Lemma~\ref{lem:p_l1} below shows an error bound of L1 whose failure probability decays exponentially in $m$.
This is in contrast to decaying polynomially of L2~\cite{jain10hashing}, which is inferior.\footnote{
  In fact, one can show a bound for L2 that fails with exponentially-decaying probability. However, the bound introduces a constant that can be arbitrarily large, which makes the tails thick. We provide details on this in SM.
}       
\begin{lem}\label{lem:p_l1}
  Define $G_k$ as in~\eqref{eq:def_iprod} with $\p=\p^{(\emph\tL1)}$.
  Then, given a target error $\eps>0$,
  \begin{equation}\begin{aligned} \label{eq:lem-p_l1}
      \textstyle   \P\lt( \lt|\fr{1}{m}\sum_{k=1}^m G_k - \q^\T\a \rt| \ge \eps \rt) \le  2\exp\lt(-\fr{m\eps^2}{2||\q||_1^2||\a||_{\max}^2}\rt)
  \end{aligned}\end{equation}
\end{lem}
To illustrate such a difference, we fix $\q$ and $\a$ in 1000 dimension and apply L2 and L1 sampling 20K times each with $m=5$ where we scale down the L2 distribution so its variance matches that of L1.
Figure~\ref{fig:iprod}(a) shows that L2 has thicker tails than L1.
Note this is not a pathological case but a typical case for Gaussian $\q$ and $\a$.
This confirms our claim that L1 is safer than L2. 

\begin{wrapfigure}{R}{0.51\textwidth}
  \vspace{-21pt}
%  \vspace{-18pt}
\begin{minipage}{0.51\textwidth}
\begin{figure}[H]
  \begin{center}
{  \centering \footnotesize
\begin{tabular}{cc}
  \hspace{-12pt}
  \includegraphics[width=.50\textwidth]{figs/heavy_tail} & \hspace{-10pt}
  \includegraphics[width=.50\textwidth]{figs/l1_l2_ratio} \vspace{-3pt}\\
  (a) & (b) \\
\end{tabular}
\vspace{-10pt}
}
\end{center}
\end{figure}
\end{minipage}
\vspace{-8pt}
\caption{%\hspace{-12pt}
(a) A box plot of estimators.
L1 and L2 have the same variance, but L2 has thicker tails.
(b) The frequency of L1 inducing smaller variance than L2 in 1000 trials. After 100 dimensions, L1 mostly has smaller variance than L2. 
}
\label{fig:iprod}
\vspace{-5pt}
\end{wrapfigure}

Another point of comparison is the variance of L2 and L1.
We show that the variance of L1 may or may not be larger than L2 in SM; there is no absolute winner.
However, if $\q$ and $\a$ follow a Gaussian distribution, then L1 induces smaller variances than L2 for large enough $d$; see Lemma~\ref{lem:l1-var} in SM.
Figure~\ref{fig:iprod}(b) confirms such a result.
The actual gap between the variance of L2 and L1 is also nontrivial under the Gaussian assumption.
For instance, with $d=200$, the average variance of $G_k$ induced by L2 is 0.99 whereas that induced by L1 is 0.63 on average.
Although a stochastic assumption on the vectors being inner-producted is often unrealistic, in our work we deal with projection vectors $\a$ that are truly normally distributed.

