\section{Evaluation} 
\label{sec.evaluation}

\subsection{Experimental setup}\label{sec.expsetting}

\textbf{Training sources.} We use the following four sources of training data to verify the effectiveness of our methods:
\begin{itemize}[leftmargin=*]
\item[-] \textbf{Query Log (AOL, ranking-based, $100k$ queries).}
This source uses the AOL query log~\cite{pass2006} as the basis for a ranking-based source, following the approach of~\cite{dehghani2017neural}.\footnote{
Distinct non-navigational queries from the AOL query log from March 1, 2006 to May 31, 2006 are selected. We randomly sample $100k$ of queries with length of at least 4. While \citeauthor{dehghani2017neural}~\cite{dehghani2017neural} used a larger number of queries to train their model, the state-of-the-art relevance matching models we evaluate do not learn term embeddings (as \cite{dehghani2017neural} does) and thus converge with fewer than $100k$ training samples.}
We retrieve ClueWeb09 documents for each query using the Indri\footnote{https://www.lemurproject.org/indri/} query likelihood~(QL) model. We fix $c^+=1$ and $c^-=10$ due to the expense of sampling documents from ClueWeb.

\item[-] \textbf{Newswire (NYT, content-based, $1.8m$ pairs).} 
We use the New York Times corpus~\cite{sandhaus2008new} as a content-based source, using headlines as pseudo queries and the corresponding content as pseudo relevant documents. We use BM25 to select the negative articles, retaining top $c^-=100$ articles for individual headlines.

\item[-] 
\textbf{Wikipedia (Wiki, content-based, $1.1m$ pairs).} 
Wikipedia article heading hierarchies and their corresponding paragraphs have been employed as a training set for the \textsc{Trec} Complex Answer Retrieval (CAR) task~\cite{Nanni2017BenchmarkFC,macavaney2018overcoming}.
We use these pairs as a content-based source, assuming that the hierarchy of headings is a relevant query for the paragraphs under the given heading.
Heading-paragraph pairs from train fold 1 of the \textsc{Trec} CAR dataset~\cite{Dietz2017} (v1.5) are used. We generate negative heading-paragraph pairs for each heading using BM25 ($c^-=100$).

\item[-] \textbf{Manual relevance judgments (WT10).}
We compare the ranking-based and content-based sources with a data source that consists of relevance judgments generated by human assessors. In particular, manual judgments from 2010 \textsc{Trec} Web Track ad-hoc task (WT10) are employed, which includes $25k$ manual relevance judgments ($5.2k$ relevant) for 50 queries (topics + descriptions, in line with~\cite{hui2017pacrr,guo2016deep}). This setting represents a new target domain, with limited (yet still substantial) manually-labeled data.

\end{itemize}


\textbf{Training neural IR models.}
We test our method using several state-of-the-art neural IR models (introduced in Section~\ref{sec.background.nir}):
PACRR~\cite{hui2017pacrr},
Conv-KNRM~\cite{convknrm}, and
KNRM~\cite{xiong2017end}.\footnote{By using these stat-of-the-art architectures, we are using stronger baselines than those used in~\cite{dehghani2017neural,Li2018JointLF}.}
We use the model architectures and hyper-parameters (e.g., kernel sizes) from the best-performing configurations presented in the original papers for all models.
All models are trained using pairwise loss for 200 iterations with 512 training samples each iteration.
We use Web Track 2011 (WT11) manual relevance judgments as validation data to select the best iteration via nDCG@20. This acts as a way of fine-tuning the model to the particular domain, and is the only place that manual relevance judgments are used during the weak supervision training process. At test time, we re-rank the top 100 Indri QL results for each query.

\textbf{Interaction filters.}
We use the 2-maximum and discriminator filters for each ranking architecture to evaluate the effectiveness of the interaction filters.
We use queries from the target domain (\textsc{Trec} Web Track 2009--14) to generate the template pair set for the target domain $T_D$.
To generate pairs for $T_D$, the top 20 results from query likelihood (QL) for individual queries on ClueWeb09 and ClueWeb12\footnote{\url{https://lemurproject.org/clueweb09.php}, \url{https://lemurproject.org/clueweb12.php}} are used to construct query-document pairs.
Note that this approach makes no use of manual relevance judgments because only query-document pairs from the QL search results are used (without regard for relevance).
We do not use query-document pairs from the target year to avoid any latent query signals from the test set. The supervised discriminator filter is validated using a held-out set of 1000 pairs. To prevent overfitting the training data, we reduce the convolutional filter sizes of PACRR and ConvKNRM to 4 and 32, respectively. We tune $c_{max}$ with the validation dataset (WT11) for each model ($100k$ to $900k$, $100k$ intervals).

\textbf{Baselines and benchmarks.}
As baselines, we use the AOL ranking-based source as a weakly supervised baseline~\cite{dehghani2017neural}, WT10 as a manual relevance judgment baseline, and BM25 as an unsupervised baseline. The two supervised baselines are trained using the same conditions as our approach, and the BM25 baselines is tuned on each testing set with Anserini~\cite{Yang2017AnseriniET}, representing the best-case performance of BM25.\footnote{Grid search: $b\in[0.05,1]$ (0.05 interval), and $k_1\in[0.2,4]$ (0.2 interval)}
We measure the performance of the models using
the \textsc{Trec} Web Track 2012--2014 (WT12--14) queries (topics + descriptions) and manual relevance judgments. These cover two target collections: ClueWeb09 and ClueWeb12.
Akin to~\cite{dehghani2017neural}, the trained models are used to
re-rank the top 100 results from a query-likelihood model (QL, Indri~\cite{strohman2005indri} version).
Following the \textsc{Trec} Web Track, we use
nDCG@20 and ERR@20 for evaluation.

\input{tab_results}

\subsection{Results}\label{sec.results}
In Table~\ref{tab.results}, we present the performance of the rankers when trained using content-based sources without filtering.
In terms of absolute score, we observe that the two n-gram models (PACRR and ConvKNRM) always perform better when trained on content-based sources than when trained on the limited sample of in-domain data. When trained on NYT, PACRR performs significantly better. KNRM performs worse when trained using the content-based sources, sometimes significantly. These results suggest that these content-based training sources contain relevance signals where n-grams are useful, and it is valuable for these models to see a wide variety of n-gram relevance signals when training. The n-gram models also often perform significantly better than the ranking-based AOL query log baseline. This makes sense because BM25's rankings do not consider term position, and thus cannot capture this important indicator of relevance. This provides further evidence that content-based sources do a better job providing samples that include various notions of relevance than ranking-based sources.

When comparing the performance of the content-based training sources, we observe that the NYT source usually performs better than Wiki. We suspect that this is due to the web domain being more similar to the newswire domain than the complex answer retrieval domain. For instance, the document lengths of news articles are more similar to web documents, and precise term matches are less common in the complex answer retrieval domain~\cite{macavaney2018overcoming}.

We present filtering performance on NYT and Wiki for each ranking architecture in Table~\ref{tab:filter_results}. In terms of absolute score, the filters almost always improve the content-based data sources, and in many cases this difference is statistically significant. The one exception is for Conv-KNRM on NYT. One possible explanation is that the filters caused the training data to become too homogeneous, reducing the ranker's ability to generalize. We suspect that Conv-KNRM is particularly susceptible to this problem because of language-dependent convolutional filters; the other two models rely only on term similarity scores. We note that Wiki tends to do better with the 2max filter, with significant improvements seen for Conv-KNRM and KNRM. In thse models, the discriminator filter may be learning surface characteristics of the dataset,
rather than more valuable notions of relevance. We also note that $c_{max}$ is an important (yet easy) hyper-parameter to tune, as the optimal value varies considerably between systems and datasets.
