\section{Background and related work}
\label{sec.background}
\subsection{Neural IR models}\label{sec.background.nir}
Ad-hoc retrieval systems rank documents according to their
relevance to a given query.
A neural IR model
($\mathit{nir}$) aims to measure the interaction between a query-document pair $(q,
\,d)$ with a real-value relevance score $rel=\mathit{nir}(q,d)$. The model $nir$ is trained to minimize pairwise loss between training triples consisting of a query $q$, relevant document $d^+$, and non-relevant document $d^-$.
Neural retrieval models can be categorized as \emph{semantic matching} models (which create dense query/document representations) or as \emph{relevance matching} models (which compare query and document terms directly, often through a query-document similarity matrix). We focus on relevance matching models because they generally show better performance than semantic matching models.
We test our approach on three leading neural rankers:


\textbf{KNRM}~\cite{xiong2017end} uses Gaussian kernels applied to each individual similarity score and log-summed across the document dimension. A final dense learning-to-rank phase combines these features into a relevance score.

\textbf{Conv-KNRM}~\cite{convknrm} is a variant of KNRM which applies convolution filters of lengths 1--3 over word embeddings before building cross-matched (matching all kernel lengths with one another) similarity matrices. The rest of the ranking process is identical to KNRM.

\textbf{PACRR}~\cite{hui2017pacrr}
uses square convolutional kernels over the similarity matrix to capture soft n-gram matches. $k-$max pooling is applied to retain only the strongest signals for each query term, and signals are combined with a dense layer.


\subsection{Weak supervision}\label{sec.background.weaksupervision}
In IR, weak supervision uses pseudo-relevant information to train a ranking model in place of human judgments. Early work on weak supervision for IR focused on training learning-to-rank models~\cite{azzopardi2007building},
using web anchor text~\cite{asadi2011pseudo} and microblog hashtags~\cite{berendsen2013pseudo} for weak supervision. More recently, \citet{dehghani2017neural} proposed a weak supervision approach that makes use of the AOL query log and BM25 results as a source of training data. Aside from limitations surrounding the availability of query logs, their approach suffers from limitations of BM25 itself: it assumes that documents ranked higher by BM25 are more relevant to the query than documents ranked lower. Others have suggested using a similar approach, but using news headlines~\cite{Li2018JointLF}, also assuming relevance from BM25 rankings. Still others have employed a Generative Adversarial Network to build training samples~\cite{Wang2017IRGANAM}, but this limits the generated data to the types of relevance found in the training samples, making it a complementary approach. In contrast, our approach uses freely-available text \textit{pairs} that exhibit both a high quality and large size.
