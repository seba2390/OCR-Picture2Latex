\section{Introduction} 
\label{sec.introduction}

A lack of manual training data is a perennial problem in information retrieval~\cite{Zamani2018SIGIR2W}.
To enable training supervised rankers for new domains, we propose a weak supervision approach based on \textit{pairs} of text to train neural ranking models and a filtering technique to adapt the dataset to a given domain. Our approach eliminates the need for a query log or large amounts of manually-labeled in-domain relevance judgments to train neural rankers, and exhibits stronger and more varied positive relevance signals than prior weak supervision work (which relies on BM25 for these signals).

Others have experimented with weak supervision for neural ranking (see Section~\ref{sec.background.weaksupervision}). Our weak supervision approach differs from these approaches in a crucial way: we train neural rankers using datasets of text \textit{pairs} that exhibit relevance, rather than using a heuristic to find pseudo-relevant documents for queries. For instance, the text pair from a newswire dataset consisting of an article's headline and its content exhibits an inherent sense of relevance because a headline often provides a concise representation of an article's content. To overcome possible domain differences between the training data and the target domain,
we propose an approach to filter the training data using a small set of queries (templates) from the target domain. We evaluate two filters: an unsupervised heuristic and using the neural ranker itself as a discriminator.

We evaluate our approaches by training several leading neural ranking architectures on two sources of weak supervision text pairs. We show that our methods can significantly outperform various neural rankers when trained using a query log source (as proposed by~\cite{dehghani2017neural}), the ranker when trained on a limited amount of manually-labeled in-domain data (as one would encounter in a new domain), and well-tuned conventional baselines. 
In summary, we (1) address existing shortcomings of weak supervision to train neural rankers by using training sources from text pairs, (2) address limitations related to domain differences when training rankers on these sources using novel filtering techniques, and (3) demonstrate the effectiveness of our methods for ad-hoc retrieval when limited in-domain training data is available. Our code is public for validation and further comparisons.\footnote{\url{https://github.com/Georgetown-IR-Lab/neuir-weak-supervision}}
