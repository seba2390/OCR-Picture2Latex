% Template for ICME 2022 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP/ICME LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,epsfig}

\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
  \OLDthebibliography{#1}
  \setlength{\parskip}{0pt}
  \setlength{\itemsep}{0pt plus 0.3ex}
}

\pagestyle{empty}

\usepackage{subfigure}

\topmargin=0mm
\begin{document}\sloppy

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


% Title.
% ------
\title{Depression Diagnosis and Analysis via Multimodal Multi-order Factor Fusion}
%
% Single address.
% ---------------
%\name{Anonymous ICME submission}
\name{Chengbo Yuan$^{\dagger}$, Qianhui Xu$^{\dagger}$ and Yong Luo$^{\dagger}$}
%Address and e-mail should NOT be added in the submission paper. They should be present only in the camera ready paper. 
\address{}
\address{$^{\dagger}$Wuhan University, \{michael.yuan.cb,xuqianhui,luoyong\}@whu.edu.cn}


\maketitle

\begin{abstract}
Depression is a leading cause of death worldwide, and the diagnosis of depression is nontrivial.
Multimodal learning is a popular solution for automatic diagnosis of depression, and the existing works suffer two main drawbacks: 1) the high-order interactions between different modalities can not be well exploited; and 2) interpretability of the models are weak. To remedy these drawbacks, we propose a multimodal multi-order factor fusion (MMFF) method. Our method can well exploit the high-order interactions between different modalities by extracting and assembling multi-order factors across modalities under the guide of a shared latent proxy.
%, which also provides a description of the fusion process.
We conduct extensive experiments on two recent and popular datasets, E-DAIC-WOZ and CMDC, and the results show that our method achieve significantly better performance compared with other existing approaches. Besides, by analyzing the process of factor assembly, our model can intuitively show the contribution of each factor. This helps us understand the fusion mechanism. %Combining with ablation study, we spaculate that MMFF follows a order-hierarchical extraction mechanism.
\end{abstract}
%
\begin{keywords}
Depression, multimodal learning, factor fusion, multi-order.
\end{keywords}
%
\begin{figure*}[!t]
	\centering
	\includegraphics[scale=0.125]{pic/FactorModel2.png}
	\caption{Illustration of the proposed Multimodal Multi-order Factor Fusion (MMFF) method. Specifically, our MMFF contains two main parts: 1) a latent space construction module based on the encoder-decoder mechanism to extract a proxy, which is utilized to generate adaptive weights to fuse different modality factors; 2) a multi-order combination part that applies multiple encoders to generate factors, and integrate them under the guidance of the shared-subspace proxy at different levels of orders.}
	\label{fig:factor}
\end{figure*}

\section{Introduction}
\label{sec:intro}
Depression is a common and serious mental disorder disease around the world. It is estimated that about $3.8\%$ of the population is affected by the disease, and more than $700,000$ patients die of suicide every year due to depression, according to the statistics of world health organization (WHO). Due to the exhaustive consumption of human resources and heavy dependence of subject judgement of the traditional diagnosis approaches, there is an increasing interests in utilizing machine learning for automatic diagnosis of depression, and multimodal learning is one of the most popular solutions.

%Among all of these, multimodal learning is a recent hotspot. In general, the existing work can be divided into three categories. The first category is about the development of depression dataset, eg. E-DAIC-WOZ~\cite{AVEC2019,DAIC}, MODMA~\cite{MODMA}, CMDC~\cite{CMDC}.
The existing multimodal learning works either focus on improving the feature extraction for different modalities, or designing better fusion strategies.
To extract features for long-time sequential data,
%For feature extraction, especially in dealing with long time series collected by sensors. 
some classical models such as LSTM and CNN are adopted in~\cite{AVEC2019,BERTCNN,MS}, while more recent approaches employ transformer to alleviate the forgeting problem caused by the long duration of patient interview~\cite{ATransformer}.
In regard to the modality fusion, in addition to the simple concatenation, attention mechanism is applied to adjust the modality contribution~\cite{DepressNet}. Some other approaches, such as CubeMLP~\cite{CubeMLP} and Multi-Agent~\cite{MultiAg} can exploit more complex interactive relationships between different modalities.
Although have achieved certain success, these approaches suffer from two main drawbacks: 1) lack of exploitation of different-order interactions across modalities;
%which causes the concern of incomplete of information extraction,
and 2) the realtively low interpretability, which leads to the confusion of the fusion mechanism.

To remedy these drawbacks, we propose a Multimodal Multi-order Factor Fusion method (MMFF) based on modality multi-order factor extraction and assembly. The idea of MMFF is to first extract multi-order factors across different modalities and then integrate the factors in terms of different orders for final prediction.
%decompose the interaction gain into different factors for fully coverage of all types of modal combinations, and then integrate them to make the final prediction.
The whole process is under the guidance of a shared subspace proxy, which is utilized to generate adaptive weights for modality factor fusion at different orders. The learned weights can intuitively show the contribution of each factor in the fusion process.
%Since the proxy guides the whole process of assembly, it can intuitively show the contribution of each modal combination and factor order.
%By observing this, we speculate that the factor fusion model has an order-hierarchical extraction mechanism based on factor order.
Besides, we extract features for the long time sequential data (such as videos and audios) from a new perspective based on trajectory preservation, where a novel frame selection pipeline based on the principal component analysis (PCA) and Midimax algorithm is developed. This helps us to effectively select the key frames and compress the time series.

%The experiments are conducted on two recent datasets: the Extended Distress Analysis Interview Corpus (E-DAIC-WOZ)~\cite{AVEC2019,DAIC} and the Chinese Multimodal Compression Corpus (CMDC)~\cite{CMDC}.
%, which was obtained in English and Chinese environment respectively.
%On the E-DAIC-WOZ dataset, the evaluation results of our MMFF model is CCC=0.676 and MAE=3.98. On the CMDC dataset, the Pearson correlation coefficient between the predicted results and the real labels is 0.83, and the MAE is 3.19. As far as we know, this is the best result currently obtained on these two depression diagnosis datasets. We also do ablation study to test the robustness and self-consistency of our model.

To summarize, our main contributions are as follows:
\begin{itemize}
\item We propose a novel multimodal multi-order factor fusion (MMFF) method for the diagnosis of depression. The proposed method is able to exploit the modality interactions at different levels of orders.
\item We design a latent proxy to generate adaptive weights for different modality factors (and also different orders). This improves the interpretability of our model since the contribution of each factor can be identified at different levels of orders.
This enables the fusion mechanism analysis.
%(which leads to a order-hierarchical extraction mechanism).
%which improves the effectiveness of multimodal fusion and the accuracy of diagnosis.
\item We develop a novel method to select key frames for long time sequential data by integrating the PCA and Midimax algorithm.
%to adapt for long duration of patient interview.
%\item a decomposition methodology for multimodal fusion and mechanism analysis is proposed for explicit coverage of modal interaction. 
\end{itemize}
The experiments are conducted on two recent datasets: the Extended Distress Analysis Interview Corpus (E-DAIC-WOZ)~\cite{AVEC2019,DAIC} and the Chinese Multimodal Compression Corpus (CMDC)~\cite{CMDC}.
The results demonstrate that our MMFF is superior to the state-of-the-art approaches with significant improvements.
%Ablation studies are also conducted for robustness test and mechanism study.


\section{The Proposed Method}
%The proposed framework contains two parts, factor extraction for each modality and the multimodal multi-order Factor Fusion model (MMFF).
Figure~\ref{fig:factor} is an illustration of the overall structure of the proposed Multimodal Multi-order Factor Fusion (MMFF) method. In this section, we first introduce the feature extractors for different modalities, where a novel frame selection pipeline is developed.
%Then we turn to the video/audio extractor which adopted with new purposed frames selection algorithm.
Then an encoder-decoder module is utilized to construct a proxy to generate adaptive weights for combination.
Finally, the factors extracted from different modalities are integrated at different levels of orders, and the integrated representations are further adaptively fused for final prediction.

\subsection{Feature Extraction} 
In this paper, we use a classic sentence to vector (sent2vec) encoder, the Universal Sentence Encoder~\cite{USE} together with BiLSTM at the sentence level~\cite{AU2} to extract the textual features for the patient's interview.
%See Figure~\ref{fig:text} for the specific structure. Since word2vec model often has a limitation on the length of input text, which will lead to the loss of semantic relationship between paragraphs when apply it in sections splitted attitude, we use sent2vec model to acquire the text embedding (which has been prove effective by \cite{AU2}).
%USE is a classic sentence to vector (sent2vec) encoder, which obtains sentence embedding mainly by averaging the words embedding obtained from Bert~\cite{bert}.
We use the pretrained USE to extract the sentence embedding, and then apply a two-layer BiLSTM and three-layer MLP to extract the features.

%\begin{figure}[!t]
%	\centering
%	\includegraphics[scale=0.4]{pic/TextModel.png}
%	\caption{The adopted textual feature extractor. Universal sentence encoder(USE) with sent2vec form is adopted to alleviate the problem of long length.}
%	\label{fig:text}
%\end{figure}

%\subsection{Frame-based Model}
In regard to the audio and video inputs, we utilize the low-variance filtering, PCA and Midimax algorithms together with a two-layer BiLSTM to extract the features.
%In this section, low-variance filtering, PCA and Midimax algorithm are adopted to process the original frame-based features (audio and video), and then a 2-layer BiLSTM is used to extract the downstream features.
The process is illustrated in Figure~\ref{fig:frame}, where the original features for each video frame are extracted following~\cite{AVEC2019,DAIC} for the E-DAIC-WOZ dataset.
%For more information attainment, we choose the deep-features extracted by densenet201 (audio) and ResNet-CNN (video) provided by E-DAIC-WOZ dataset.
For the CMDC dataset, the original feature is a stacking of the $12$ feature vectors, each corresponds to a certain question in the interview.
%After analyzing the data in the E-DAIC-WOZ training set, we found that the video feature contains up to $58989$ frames, and the audio feature contains up to $1958$ frames, which is too long for feature extraction. Therefore, feature engineering is firstly performed on the original frames.
Since the number of frames in each video or audio sequence may be very large (such as in the E-DAIC-WOZ dataset), the computational cost may be very high and there may be much redundant information. Therefore, we process these original features using the following strategy.

\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.13]{pic/FrameModel2.png}
	\caption{The proposed frames feature extractor. Low variance filter is firstly used to remove the insignificant feature. Then, the Midimax algorithm is applied to the first principle component of the original feature to acquire the key frames.}
	\label{fig:frame}
\end{figure}

%To begin with, redundant information are not conducive to downstream feature extraction, and will increase the time complexity of the algorithm.
Firstly, low-variance filtering is performed on the original feature. Suppose that the original feature after min-max normalization is $\{X^{K \times M_i}_i=[X_i^1,X_i^2,\cdots,X_i^{M_i}],\ i=1,2...N\}$, where $N$ is the size of training set, $X_i^j$ is the $j$-th frame of $i$-th sample, $K$ is the feature dimension of the frame, $M_i$ is the number of frames in $X_i$, and let $X_i^{j}=[x_i^{j1}, x_i^{j2}, \cdots, x_i^{jK}]^T$, then the $k$-th dimension of features that satisfies the following condition is filtered:
\begin{equation}
\frac{1}{N}\sum_{i=1}^N \frac{1}{M_i}\sum_{j=1}^{M_i} (x_i^{jk}-E_j[x_i^{jk}])^2 \leq \beta,
\end{equation}
where $\beta$ is an hyperparameter (we set $\beta=0.01$ in this paper).
Specifically,  $\frac{1}{M_i}\sum_{j=1}^{M_i} \left(x_i^{jk}-E_j[x_i^{jk}]\right)^2$ is the variance of $X_i$ at the $k^{th}$ dimension.
After the low variance filtering, some redundant features are removed.

Then we compress the sequence based on variance extraction and the Midimax algorithm. The main idea of Midimax algorithm is to keep the trajectory of sequence as much as possible under the premise of satisfying a certain compression ratio. Considering that Midimax is mainly applicable to one-dimensional data, we first perform the maximum variance projection of features. That is, we use the PCA algorithm to extract one principal component to transform the sequence to one-dimension, and then apply Midimax to the projected sequence.

The following is a brief description of the Midimax algorithm we apply: given the compression ratio $\delta$, which is an integer, we first divide the sequence into $[s_1,s_2...s_{Ns}]$, where $Ns$ is the number of time slices, $s_i$ is the $i^{th}$ slice, and the length of each time slice is $\delta$. Let $s_i=[s_i^1,s_i^2...s_i^{\delta}]$, and suppose that the index of the maximum, minimum and median in the time slice are $p_1$, $p_2$, $p_3$. Then we sort $[p_1,p_2,p_3]$ as $[q_1,q_2,q_3]$ ($q_1\leq q_2\leq q_3$) and finally compress $s_i$ into $[s_i^{q_1},s_i^{q_2},s_i^{q3}]$. The scheme is shown in Figure \ref{fig:midimax}. Finally, BiLSTM is applied to extract the features.

\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.15]{pic/Midimax.png}
	\caption{The Midimax Algorithm ($\delta$=6). To preserve the original trajectory, the maximum, minimum and median are picked up for each time slice under the given compression ratio.}
	\label{fig:midimax}
\end{figure}

\subsection{Proxy Construction}
In factor fusion, we need to calculate the factor contributions for each modality.
%, and the source of this contribution is information in a global sense.
Considering that the calculation of factor contribution should be consistent, we first map three modalities into a common low-dimensional subspace, which is learned by reconstructing the inputs with minimum errors.
%take the minimum mean square error intersection of the three low dimensional subspace (which is the mean of modal projection).
That is, in order to make the new subspace effectively represent the original space, a reverse projection is conducted to map the subspace back to the original space, and employ the mean square error (MSE) of the recovery space and the original space as the loss function, as illustrated in Figure~\ref{fig:latent}.

\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.16]{pic/LatentModel.png}
	\caption{Illustration of the latent space module for proxy extractor. The encoder-decoder form is adopted to maximize the sharing between modality subspace while preserving the modality information.}
	\label{fig:latent}
\end{figure}
		
Formally, let $x_{t}$, $x_{a}$, $x_{v}$ be the extracted features for the textual, audio and visual modalities respectively, we learn a mapping function $F(\cdot)$, and the subspace mapping can be expressed as:
\begin{equation}
    \begin{split}
	& y_{mod} = F_{mod}(x_{mod}),\ \ mod=\{t,a,v\},\\
	& z=\frac{1}{3}(y_{t} + y_{a} + y_{v}),
	\end{split}
\end{equation}
where the dimension of $y_{mod}$ and the latent representation $z$ is less than $x_{mod}$. The latent subspace is learned by using the following loss function:
\begin{equation}
    \begin{split}
	& \mathcal L_{latent} = \frac{1}{3} \sum_{mod\in \{t,a,v\}}(\hat{x}_{mod} - x_{mod})^2,\\
	& \hat{x}_{mod} = F'_{mod}(z).
	\end{split}
\end{equation}
The learned $z$ is utilized as a proxy to guide the subsequent factor fusion.

\subsection{Factor Extraction \& Assembly}
%The complete structure of model is shown in Figure \ref{fig:factor}.
There are many approaches for multimodal fusion, but most of them have two main issues: the incomplete exploitation of the interactive information across modalities and low interpretability. Our MMFF model alleviates these issues by exploiting multi-order interactions between different modalities in the combination, which also provides a certain degree of interpretability for fusion mechanism analysis.

\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.2]{pic/Decomposition.png}
	\caption{The proposed contribution decomposition methodology. The core idea is to decompose the interaction gain to all modality combination explicitly, so that the fully coverage of interaction extraction is assured.}
	\label{fig:decomp}
\end{figure} 
		
The factor fusion process at different levels of orders is illustrated in Figure~\ref{fig:decomp}. First, the features of each modality are decomposed into factors of different orders.
%fused information can be decomposed into 1st, 2nd and 3rd-order factor,
where the $p$-order factor indicates that the factor comes from $p$ modalities.
Then for the $p$-order factor, there will be $C_{3}^{p}$ subfactors, where $C_{3}^p$ is the combination number. How to extract each factor will be depicted as follows.
		
Let $mod$ denote a single modality, $com$ signifies modality combination, and $H(\cdot)$ be a certain mapping function, we first calculate the contribution of each modality for the factors of different levels of orders.
This is achieved by using the constructed proxy to generate the adaptive weights.
%This should come from the global information, so we use the proxy obtained in the previous section to generate the contribution rate.
Specifically:
\begin{equation}
	\gamma^k = H_k(z),\ \ k=1,2,3,
\end{equation}
where $\gamma^k=[\gamma_t^k,\gamma_{a}^k,\gamma_{v}^k]^T$, which satisfies $\gamma^k_t+\gamma^k_a+\gamma_v^k=1$. The 1st-order factor $v_{mod}^1,\ mod\in \{t,a,v\}$, is calculated as follows:
\begin{equation}
    v_{mod}^1 = \gamma^{1}_{mod}P_{mod}^T G_1^{mod}(x_{mod}),\ \ mod=\{t,a,v\},
\end{equation}
where $P_{mod}^T$ is a projection matrix, $G(\cdot)$ is an encoder and $\gamma^{1}_{mod}$ is the contribution of certain modality at the 1st-order.
%In regard to the 2nd-order factor,
Given the contribution $\gamma_{mod}^{2}$ for each modality at the 2nd-order, we use the multimodal low-rank bilinear (MLB) approach~\cite{mlb} to construct the 2nd-order factor, i.e.,
\begin{equation}
    \begin{split}
    & v_{ta}^2 = P_{ta}^T\sigma(\gamma_{t}^2 G_{2}^t(x_t) \circ \gamma_{a}^2 G_{2}^a(x_a)),\\
    & v_{av}^2 = P_{av}^T\sigma(\gamma_{a}^2G_{2}^a(x_a) \circ \gamma_{v}^2G_{2}^v(x_v)),\\
    & v_{tv}^2 = P_{tv}^T\sigma(\gamma_{t}^2G_{2}^t(x_t) \circ \gamma_{v}^2G_{2}^v(x_v))
	\end{split}
\end{equation}
where $\sigma(\cdot)$ is a certain activation function and $\circ$ is the Hadamard product. Then the MLB approach is extended to induce the 3rd-order factor:
\begin{equation}
	v_{tav}^3 = P_{tav}^T \sigma(\gamma_{t}^3G_{3}^t(x_t) \circ \gamma_{a}^3G_{3}^a(x_a) \circ \gamma_{v}^3G_{3}^v(x_v)).
\end{equation}
We then combine the obtained subfactors at different levels of orders. The integrated 1st-order factor $v^1$, 2nd-order factor $v^2$ and 3rd-order factor $v^3$ are obtained by simply adding the subfactors and then performing a projection, i.e.,
\begin{equation}
    \begin{split}
	& v^1 = P_1^T(v^1_{t} + v_a^1 + v_v^1),\\
    & v^2 = P_2^T(v^2_{ta} + v_{av}^2 + v_{tv}^2),\\
    & v^3 = P_3^T v_{tav}^3.
	\end{split}
\end{equation}

The final fusion feature is composed of all the 1st, 2nd and 3rd-order factors. Similarly, we use the proxy to calculate the contribution of each order, and then weightedly fuse the integrated factors to obtain the final feature $v$:
\begin{equation}
    v = \gamma_1v^1 + \gamma_2v^2 + \gamma_3v^3.
\end{equation}
Here,
\begin{equation}
	\gamma = [\gamma_1,\gamma_2,\gamma_3]^T= H_{com}(z),\ \ s.t.\ \ \sum_{i=1}^3 \gamma_i = 1.
\end{equation}

For our MMFF, A hierarchical training strategy is adopted. That is, we first train the latent subspace solver, and then freeze its parameters, and finally train the backbone of our MMFF. After obtaining the final fusion feature $v$, we use a three-layers MLP to predict patients' PHQ score. Let $\hat{y}$ be the prediction score, $y$ be the real label, and the loss function used in our training process is the MSE loss function, i.e.,
\begin{equation}
	\mathcal L_{MSE} = \frac{1}{N} \sum_{i=1}^N (\hat{y} - y)^2.
\end{equation}
Through the adaptive training of the contributions $\gamma^1$, $\gamma^2$, $\gamma^3$ and $\gamma$, we can intuitively acquire the importance of each factor of modality in the combination, so as to better understand the mechanism of fusion.


\section{Experiments}

\subsection{Dataset}
Our experiments are mainly conducted on two recent depression diagnosis datasets, namely, the Extended Distress Analysis Interview Corpus (E-DAIC-WOZ)~\cite{AVEC2019,DAIC} and Chinese Multimodal Compression Corpus (CMDC)~\cite{CMDC}.

E-DAIC-WOZ is an extended version of WOZ-DAIC dataset\cite{DAIC}, and it is also the dataset for the AVEC2019 DDS Challenge~\cite{AVEC2019}.
%The dataset was obtained from audio-visual records of patient clinical interviews in an English environment. It is worth noting that part of the interviews in this dataset are completed through a virtual AI agent, which can effectively avoid the subjective preferences of the interviewers and is more consistent with the application scenario of automatic depression identification in the future.
E-DAIC-WOZ provides video, audio and text modality data. For the text modality, the interview records are provided. For the video modality, only the extracted features are provided to protect the privacy of patients. For audio modality, in addition to the wav file, some features are also provided.
%This dataset uses the PHQ-8 score to evaluate the degree of depression of the patients.
%The larger the PHQ value, the more serious the depression of the patients.
The dataset contains $163$ training samples, $56$ validation samples and $56$ test samples.
%CCC and RMSE are the first and second ranking evaluation indicators of AVEC2019 DDS.

CMDC is a newly proposed dataset, which is collected by \cite{CMDC}.
%The structure of CMDC is quit similar to E-DAIC-WOZ.
This dataset was obtained from the audio-visual records of patients' clinical interviews in the Chinese environment.
%However, unlike the continuous interview form of E-DAIC-WOZ, 
CMDC presets 12 questions during the interview and stores the answering data for these 12 questions.
%In our experiment, we concatenate features in turn and approximate them to time series.
Similarly, in order to ensure the privacy of patients, the datasets only provided extracted features in the audio and video modalities.
%PHQ-9 score is used to evaluate the degree of depression.
%Simultaneously, the data partition of this dataset is also different from E-DAIC-WOZ.
The dataset consists of $45$ samples that have complete modalities: $19$ samples for patients with depression and $26$ samples for the control group. Considering the relatively small amount of data, the author provides a stratified 5-fold cross validation to obtain the final performance of the model.

\subsection{Experiment Setup}
Our experimental settings are as follows: the activation function for both the audio and video modalities are ELU($\cdot$), and the activation function for the text modality is ReLU($\cdot$). In the factor fusion part, $F(\cdot)$ is a two-layer MLP with Tanh activation, $G(\cdot)$ is a single-layer MLP with Hardtanh activation, and $H(\cdot)$ is a three-layer MLP with ReLU activation.
%Finally, a 3-layer MLP with LeakyReLU activation function is used for final PHQ score prediction.
In order to alleviate the over-fitting problem, we use the AdamW optimizer for training, and add the Dropout layer to the network backbone. The dropout rate is set between $[0.4,0.6]$.

For the E-DAIC-WOZ dataset~\cite{AVEC2019,DAIC}, we follow the data partition in AVEC2019 DDS. We use Midimax to compress the length of video and audio feature to $1200$ and $600$ respectively, and unit normalization is conducted on the compressed results.
For the CMDC dataset \cite{CMDC}, we first remove the data with incomplete modalities as described in Zou et al.\cite{CMDC}, and then divide the remaining data into five folds according to the stratified partition provided in the CMDC attached file. Then, for the non-test data in each fold, we randomly take $80\%$ data as the training set, and set the remaining data as the validation set. The model is evaluated based on the combination of prediction scores from five folders.

%\subsection{Evaluation Metrics}
In the AVEC2019 DDS~\cite{AVEC2019}, CCC (concordance correlation coefficient) and RMSE (the root mean square error) are the main evaluation metrics. To test the effectiveness of our model, we add the MAE (mean absolute error) as an extra criterion.
%The calculation formula of the three indicators is as follows:
%   	$$CCC = 1.0 - \frac{2Cov(y,\hat{y})}{\sigma^2_{\hat{y}} + \sigma^2_y + (\hat{y}-y)^2}$$
%   	$$RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^N(\hat{y}-y)^2}$$
%   	$$MAE = \frac{1}{N} \sum_{i=1}^N |\hat{y}-y|$$
%In the above formula, $\sigma^2$ represents variance, and $Cov$ represent the covariance. The value range of CCC is $[-1,1]$. The closer the value is to $1$, the better the fitting effect. CCC is an metrics improved from Pearson correlation coefficient. While considering the consistency/correlation, it can also measure the absolute distance difference between data. In the AVEC2019 DDS, CCC is the first indicator for the final ranking.
For the CMDC dataset, we follow~\cite{CMDC} to adopt the RMSE, MAE and Pearson correlation coefficients as the evaluation criteria.
%For the CMDC dataset, in order to effectively evaluate the effect of the model, the evaluation indicators we selected are consistent with Zou et.al~\cite{CMDC}, that is, RMSE, MAE and Pearson correlation coefficients.
%The Pearson correlation coefficient is calculated as follows:
%$$Pearson = \frac{Cov(\hat{y},y)}{\sigma_{\hat{y}}\sigma_y}$$


%\section{Result \& Analysis}

\subsection{Experimental Results}
The results on the E-DAIC-WOZ~\cite{AVEC2019,DAIC} dataset are reported in Table \ref{tab:DAIC}. From the results, we observe that: 1) after low-variance filtering and Midimax processing, our model obtains CCC=$0.434$ and RMSE=$5.44$ using only the audio modality. This is a competitive result, which verifies the effectiveness of our Midimax sequence compression; 2) the proposed MMFF achieves the performance of CCC=$0.676$, RMSE=$4.91$, and MAE=$3.98$. It can be seen that our model achieves the best performance on the most popular evaluation metric CCC, and ranked first and third in terms of $MAE$ and $RMSE$ respectively. This demonstrates the superiority of our method.

\begin{table}[!t]
   	\centering
   	\caption{Result on E-DAIC-WOZ Test Set, where the best results are marked in bold, ``(T)'' and ``(A)'' indicate that the model only uses the corresponding modality. If not specified, all modalities are utilized.
   	} 
   	\label{tab:DAIC}
   	\resizebox{.9\columnwidth}{!}{
   	\begin{tabular}{|l|l|l|l|}
   	\hline
   	Model                       & \textbf{CCC}   & RMSE & MAE  \\ \hline
   	Baseline~\cite{AVEC2019}                   & 0.111 & 6.37 & /    \\ 
   	Adaptive Transformer~\cite{ATransformer} & 0.331 & /    & 6.22 \\ 
   	Bert-CNN \& Gated-CNN~\cite{BERTCNN}       & 0.403 & 6.11 & /    \\ 
   	VarFilter+Midimax (Ours) (A)          & 0.434 & 5.44 & 4.4 \\ 
   	Hierarchical BiLSTM~\cite{HR}         & 0.442 & 5.50 & /    \\ 
   	DepressNet~\cite{DepressNet}                  & 0.457 & 5.36 & /    \\ 
   	multi-DAAE+Fusion~\cite{AU2}   & 0.528 & \textbf{4.47} & /    \\ 
   	PV-DM~\cite{AU2} (T)   & 0.560 & 4.66 & /    \\ 
   	CubeMLP~\cite{CubeMLP}                 & 0.583 & /    & 4.37 \\ 
   	MMFF (Ours)          & \textbf{0.676} & 4.91 & \textbf{3.98} \\ \hline
   	\end{tabular}
   	}
\end{table}
   	
%To further verify the effectiveness and robustness of the model, experiment is conducted on another multimodal ADD dataset, namely CMDC dataset.
The results on the CMDC dataset are reported in Table \ref{tab:CMDC}. In the experiments of Zou et al.~\cite{CMDC}, the combination including the audio modality achieves better performance compared with others. This is in line with the results in our experiments. The performance of audio modality is surprisingly good. By applying low variance filtering, normalization and 2-layer BiLSTM, utilizing only the audio modality can outperform~\cite{CMDC}.
By combining the different modalities using our MMFF method, the performance can be further improved.
%Simultaneously, the MMFF model can still extract the gain information, but as shown in the Table \ref{tab:CMDC}, the gain is not so obvious, but still proves the value of factor fusion on interactive information extraction.

\begin{table}[!t]
   	\centering
   	\caption{Result on CMDC Test Set, where the best results are marked in bold; ``(A)'' and ``(A+T)'' indicate that the model only uses the corresponding modalities. If not specified, all modalities are utilized.} 
   	\label{tab:CMDC}
   	\resizebox{.9\columnwidth}{!}{
   	\begin{tabular}{|l|l|l|l|}
   	   	\hline
   	   	Model                     & Perason & RMSE & MAE  \\ \hline
   	   	SVR                       & 0.57    & 7.04 & 5.31 \\ 
   	   	KNN                       & 0.50    & 8.38 & 5.89 \\ 
   	   	BiLSTM~\cite{CMDC}      & 0.68    & 5.67 & 4.55 \\ 
   	   	MulT~\cite{CMDC}  & 0.72    & 5.61 & 4.32 \\ 
   	   	MulT~\cite{CMDC} (A+T)          & 0.72    & 4.59 & 3.66 \\ 
   	   	VarFilter+2BiLSTM (Ours) (A) & 0.81    & 4.31 & 3.22 \\ 
   	    MMFF (Ours)        & \textbf{0.83}    & \textbf{4.29} & \textbf{3.19} \\ \hline
   	\end{tabular}
   	}
\end{table}
   	
\subsection{Ablation Study for Factor Order}

In order to further evaluate our model and understand the fusion mechanism, we conducted ablation studies on the E-DAIC-WOZ dataset to verify the role of each order in factor fusion.
%For convenience, Directly modification of model is confucted by redistributing the weights $\gamma$, and then omitting the corresponding factors. For instance, we eliminate 3rd-order factors in the following ways:
%	$$\bar{\gamma} = [\bar{\gamma}_1,\bar{\gamma}_2]^T = [\frac{\gamma_1}{\gamma_1+\gamma_2},\frac{\gamma_2}{\gamma_1+\gamma_2}]^T$$
%	$$\bar{v} = \bar{\gamma}_1v^1 + \bar{\gamma}_2v^2$$
%The method of excluding other order is similar. Obviously, this elimination method is equivalent to deleting the parts related to the eliminated factors on the network. After culling all possible culling combinations, we retrained and retested on the E-DAIC-WOZ dataset, and the results are as Table \ref{tab:aborder}.
The results are shown in Table \ref{tab:aborder}, we can be see that: 1) the full set of factor combinations performs the best in terms of CCC, the combination of 1st-order and 3rd-order factors achieves the best performance on RMSE, and the utilization of only the 1st-order factors performs the best on MAE. In general, involving more levels of order combination tend to improve the performance, and this demonstrates the necessity of integrating factors of different orders; 2) the CCC of all ablation models are higher than $0.65$, which demonstrates the reliability of our model.
%Follow the CCC in Table \ref{tab:aborder}, we can infer that the importance of order was ranked as: 1st, 3rd and 2nd. This is consistent with the results of contribution rate analysis we conduct later.

%\begin{figure*}[!t]
%	\centering
%	\begin{minipage}{0.24\linewidth}
%		\centering
%		\includegraphics[width=\linewidth]{pic/f1.pdf}
%		\caption{modality contribution in 1st-order factor}
%		\label{f1}%文中引用该图片代号
%			\end{minipage}
			%\qquad			\begin{minipage}{0.24\linewidth}
%					\centering				%	\includegraphics[width=\linewidth]{pic/f2.pdf}
%					\caption{modality contribution in 2nd-order factor}
%		\label{f2}%文中引用该图片代号
%			\end{minipage}
%		\begin{minipage}{0.24\linewidth}
%					\centering				%	\includegraphics[width=\linewidth]{pic/f3.pdf}
%					\caption{modality contribution in 3rd-order factor}			%		\label{f3}%文中引用该图片代号
%			\end{minipage}
			%\qquad		
%	\begin{minipage}{0.24\linewidth}
%					\centering
%		\includegraphics[width=\linewidth]{pic/mol.pdf}
%					\caption{modality contribution in MMFF}
%		\label{mol}%文中引用该图片代号
%			\end{minipage}
%\end{figure*}

\begin{table}[!t]
   	\centering
   	\caption{Result of Factor Order Ablation Study, where the best results are marked in bold.}
   	\label{tab:aborder}
   	\resizebox{.9\columnwidth}{!}{
   	\begin{tabular}{|l|l|l|l|}
   		   	\hline
   		   	Factor Order Combination    & CCC   & RMSE & MAE  \\ \hline
   		   	1st + 2nd + 3rd & \textbf{0.676} & 4.91 & 3.98 \\ 
   		   	1st + 2nd       & 0.672 & 4.82 & 3.91 \\ 
   		   	1st + 3rd       & 0.674 & \textbf{4.80} & 3.92 \\
   		   	2st + 3rd       & 0.660 & 5.20 & 4.05 \\
   		   	1st             & 0.661 & 4.83 & \textbf{3.90} \\
   		   	2st             & 0.658 & 5.24 & 4.10 \\ 
   		   	3st             & 0.656 & 4.89 & 3.95 \\ \hline
   		   	\end{tabular}
   	}
\end{table}

		
\subsection{Fusion Mechanism Analysis}
As mentioned above, the contribution rate ($\gamma^1$,$\gamma^2$,$\gamma^3$,$\gamma$) obtained from the proxy can provide us with the mechanism explanation of factor fusion to some extent. In this section, we visualize the modality contributions on the E-DAIC-WOZ dataset and analyze the fusion mechanism.
%We first define the contribution rate of modality $mod(mod\in\{t,a,v\})$ as $\gamma_{mod}$, which is calculated by the formula below:
%   	$$\gamma_{mod} = \gamma_1 \gamma_{mod}^1 + \gamma_2 \gamma^2_{mod} + \gamma_3\gamma^3_{mod}$$

The weights of each modality on each order and the whole process are shown in Table \ref{tab:contmodal}. It can be seen that the audio (42.5\%) is the most important modality, followed by the text (30.4\%) and video (27.1\%). We also observe that the audio, text, and video modalities have the largest contribution for the 1st-order, 2nd-order, and 3rd-order factors, respectively.
%text is the most important modality for the 2nd(40.8\%), and video has the largest contribution in the 3rd(39.3\%).
This indicates that the contributions of different modalities vary when exploiting the interactions in terms of different orders.

Besides, the contribution of audio modality at the 1st, 2nd and 3rd-order decreases ($56.6\%$ to $33.0\%$), and the contribution of text at the 2nd and 3rd-order decreases ($40.8\%$, $27.7\%$), while the contribution of video at the 1st, 2nd and 3rd-order increases ($18.9\%$ to $39.3\%$). Therefore, we speculate that there is a layer-by-layer extraction mechanism in our MMFF. That is, the information extracted by order $t$ is mainly the information that has not be fully extracted by the order smaller than $t$, thus realizing the hierarchical and full extraction of modality information. 

\begin{table}[!t]
   	\centering
   	\caption{Modality Contribution learned by our MMFF, where the largest contribution is marked in bold.}
   	\label{tab:contmodal}
   	\resizebox{.7\columnwidth}{!}{
   	\begin{tabular}{|l|l|l|l|}
   		   	\hline
   		   	Factor     & text   & audio  & video  \\ \hline
   		   	1st factor & 24.5\% & \textbf{56.6}\% & 18.9\% \\ \hline
   		   	2nd factor & \textbf{40.8}\% & 34.7\% & 24.5\% \\ \hline
   		   	3rd factor & 27.7\% & 33.0\% & \textbf{39.3}\% \\ \hline
   		   	fusion     & 30.4\% & \textbf{42.5\%} & 27.1\% \\ \hline
   		   	\end{tabular}
   	}
\end{table}   	
   	
%Visualization of the modality contribution($\gamma^1$,$\gamma^2$,$\gamma^3$,$\gamma_{mod}$) with the number of iterations is shown in Figure \ref{f1},\ref{f2},\ref{f3},\ref{mol}. It can be seen from the figure that during the iteration process, the relationship between contributions is constantly changing. Therefore, we speculate that in factor fusion, there are complex interactions and mutual promotion between factors. For example, in a global sense, the extraction of audio modality information is initially promoted by text modality(Figure \ref{fig:order}).

%\begin{table}[!t]
%   	\centering
%   	\caption{Order Contribution in MMFF, where the largest contribution is marked in bold.}
%   	\label{tab:contorder}
%   	\resizebox{.85\columnwidth}{!}{
%   	\begin{tabular}{|l|l|l|l|}
%   	\hline
%   	Factor Order      & 1st Factor & 2nd Factor & 3rd Factor \\ \hline
%   		   	Contribution Rate & \textbf{38.7\%}     & 29.5\%     & 31.8\%     \\ \hline
%   		   	\end{tabular}
%   	}
%\end{table}   	

%\begin{figure}[!t]
%	\centering		\includegraphics[scale=0.7]{pic/factor.pdf}
%	\caption{Order Contribution in MMFF}
%	\label{fig:order}
%\end{figure} 

%Finally, The contribution of order($\gamma$) is analysed. The value of $\gamma $ of the fusion result is shown in Table \ref{tab:contorder} and the visualization is shown in Figure \ref{fig:factor}. It can be seen that the order of factor contribution is: 1st(38.7\%), 3rd(31.8\%) and 2nd(29.5\%), which is completely consistent with the results observed in the ablation study, showing that the model is self-consistent. As Figure \ref{fig:order} shows, the 1st-order quickly dominates in the early training stage, while with the iteration, the 2nd-order and the 3rd factor, namely the interaction information between modals, are gradually extracted.

\section{Conclusion}
In this paper, we propose a Multimodal Multi-order Factor Fusion (MMFF) method for depression diagnosis. Comprared with the existing approaches, our method can ensure the full coverage of interactive information in different orders, while providing stronger interpretability.
Experiments conducted on two recent depression datasets show that our MMFF is significantly superior to the state-of-the-art approaches. We also conduct analysis of the learned contribution of each factor and give a conjecture of the fusion mechanism (namely, order-hierarchical information extraction for fusion).
%, which is supported by the experiment and ablation study results.
%The idea of gain decomposition in the MMFF model is a new model design form.
In the future, we intend to apply the proposed method to more applications in addition to the depression diagnosis.
%In the future, we intend to use this method to do more research on the topic of feature fusion.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{icme2023_MMFF}

\newpage

\section{Supplymentary Material}

\subsection{Details of Feature Extraction for Text Modality}

Figure~\ref{fig:text} is the pipeline of feature extration for the patient's textual interview, where the Universal Sentence Encoder (USE)~\cite{USE} is utilized to obtain embedding for each sentence.
%Since word2vec model tends to have a limitation on the length of input text, which will lead to the loss of semantic relationship between paragraphs when apply it in sections splitted attitude, we use sent2vec model to acquire the downstream feature of text modality.
USE is a classic sentence to vector encoder, which obtains sentence embedding mainly by averaging the words embedding obtained from stacking transformers.
%Here, USE is used to encode sentence.
Then BiLSTM and MLP are adopted to induce a representation for the interview.


\begin{figure}[htb]
	\centering
	\includegraphics[scale=0.14]{pic/TextModel2.png}
	\caption{The adopted feature extraction pipeline for the textual interview.}
	\label{fig:text}
\end{figure}

\subsection{More Experimental Details and Results}


\begin{figure*}[!t]
	\centering
    \subfigure[1st-order result]{
	\begin{minipage}{0.24\linewidth}
	\includegraphics[width=\linewidth]{pic/f1.pdf}
	\label{subfig:f1}%文中引用该图片代号
	\end{minipage}
    }%
	%\qquad
    \subfigure[2nd-order result]{
	\begin{minipage}{0.24\linewidth}
	\includegraphics[width=\linewidth]{pic/f2.pdf}
	\label{subfig:f2}%文中引用该图片代号
	\end{minipage}
    }%
	%\qquad
    \subfigure[3rd-order result]{
	\begin{minipage}{0.24\linewidth}
	\includegraphics[width=\linewidth]{pic/f3.pdf}
	\label{subfig:f3}%文中引用该图片代号
	\end{minipage}
    }%
	%\qquad
	\subfigure[fusion result]{
    \begin{minipage}{0.24\linewidth}
	\includegraphics[width=\linewidth]{pic/mol.pdf}
	\label{subfig:mol}%文中引用该图片代号
	\end{minipage}
    }%
    \label{fig:vis}
    \caption{Visualization of the modality contributions ($\gamma^1$, $\gamma^2$, $\gamma^3$, $\gamma_{mod}$) w.r.t. the number of iterations.
    %The visualization is obtained by observing the the modality contribution ($\gamma^1$,$\gamma^2$,$\gamma^3$,$\gamma_{mod}$) after every epoch during the training stage.
    }
\end{figure*}

\subsubsection{Details of Datasets}

Our experiments are mainly conducted on two recent depression diagnosis datasets, namely, the Extended Distress Analysis Interview Corpus (E-DAIC-WOZ)~\cite{AVEC2019,DAIC} and Chinese Multimodal Compression Corpus (CMDC)~\cite{CMDC}.

The E-DAIC-WOZ is an extended version of WOZ-DAIC~\cite{DAIC}, which contain audio-visual records of patients' clinical interviews in an English environment. It is worth noting that part of the interviews in this dataset are completed through a virtual AI agent, which can effectively avoid the subjective preferences of the interviewers and is more consistent with the application scenario of automatic depression identification in the future~\cite{DAIC}. E-DAIC-WOZ provides video, audio and text modality data. For the text modality, it provides interview records. For the video modality, in order to protect the privacy of patients, the dataset only provides the extracted feature, including FAU-poses, ResNet-CNN~\cite{resnet}, etc. For the audio modality, in addition to the wav file, features extracted from MFCCs, eGeMaps~\cite{eGeMaps}, and densenet201~\cite{densenet} are also provided. In this paper, we utilize the deep-features extracted by densenet201 (audio) and ResNet-CNN (video) provided by the E-DAIC-WOZ dataset. This dataset uses the PHQ-8 (ranges in $[0, 24]$) score to evaluate the degree of depression of the patients. The larger the PHQ value, the more serious the depression of the patient.

The CMDC dataset consists of audio-visual records of patients' clinical interviews in the Chinese environment. However, unlike the continuous interview of E-DAIC-WOZ, CMDC presets a series of $12$ questions during the interview and stores the answering data for these 12 questions. For the privacy reason, the identification information in the transcribed interviews were removed. Then a ChineseBERT model~\cite{CBert} was adopted to extracted deep features. Video and audio modality data were preprocessed by some feature extractors as well, namely, pretrained TimeSFormer~\cite{tfo} for video modality and pretrained VGGish~\cite{Y8} for the audio one.
%In our experiment, we concatenate features in turn and approximate them to time series.
PHQ-9 score is used to evaluate the degree of depression. Considering the relatively small amount of data, the author provides a stratified 5-fold cross validation to obtain the final performance of the model.

% \begin{figure*}[!t]
% 	\centering
% 	\begin{minipage}{0.24\linewidth}
% 	\centering
% 	\includegraphics[width=\linewidth]{pic/f1.pdf}
% 	\caption{modality contribution in 1st-order factor}
% 	\label{f1}%文中引用该图片代号
% 	\end{minipage}
% 	%\qquad
% 	\begin{minipage}{0.24\linewidth}
% 	\centering
% 	\includegraphics[width=\linewidth]{pic/f2.pdf}
% 	\caption{modality contribution in 2nd-order factor}
% 	\label{f2}%文中引用该图片代号
% 	\end{minipage}
% 	\begin{minipage}{0.24\linewidth}
% 	\centering
% 	\includegraphics[width=\linewidth]{pic/f3.pdf}
% 	\caption{modality contribution in 3rd-order factor}
% 	\label{f3}%文中引用该图片代号
% 	\end{minipage}
% 	%\qquad
% 	\begin{minipage}{0.24\linewidth}
% 	\centering
% 	\includegraphics[width=\linewidth]{pic/mol.pdf}
% 	\caption{modality contribution in MMFF}
% 	\label{mol}%文中引用该图片代号
% 	\end{minipage}
% \end{figure*}

\subsubsection{Details of Evaluation Metrics}
In the AVEC2019 DDS~\cite{AVEC2019}, CCC~(concordance correlation coefficient) and RMSE (the root mean square error) are the main evaluation metrics. To test the effectiveness of our model, we add the MAE (mean absolute error) as an extra criterion. Let $\hat{y}$ be the prediction result and $y$ the ground label, formulations of the three adopted metrics is as follows:
\begin{equation}
    \begin{split}
	& CCC = 1.0 - \frac{2Cov(y,\hat{y})}{\sigma^2_{\hat{y}} + \sigma^2_y + (\hat{y}-y)^2}, \\
	& RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^N(\hat{y}-y)^2}, \\
    & MAE = \frac{1}{N} \sum_{i=1}^N |\hat{y}-y|,
	\end{split}
\end{equation}
where $\sigma^2$ indicates the variance, and $Cov$ signifies the covariance. The range of CCC is $[-1,1]$. The closer the value is to $1$, the better the fitting performance is. CCC is a metric improved from the Pearson correlation coefficient. In addition to consider the consistency/correlation, it can also measure the absolute distance difference between data.
%In the AVEC2019 DDS, CCC is the first indicator for the final ranking.

For the CMDC dataset, in order to effectively evaluate the performance, the evaluation metrics we adopted, i.e., RMSE, MAE and Pearson correlation coefficients, are consistent with the original paper~\cite{CMDC}. The Pearson correlation coefficient can be calculated as follows:
\begin{equation}
    Pearson = \frac{Cov(\hat{y},y)}{\sigma_{\hat{y}}\sigma_y}.
\end{equation}


\subsubsection{More Ablation Studies}
In order to further evaluate the performance of MMFF and understand the fusion mechanism, we conducted ablation experiments on the E-DAIC-WOZ dataset to verify the importance of each order in MMFF. 

%For convenience, Directly modification of model is conducted
The sub-models are obtained by redistributing the contribution rates $\gamma$, and then removing the corresponding factors. For instance, we eliminate the 3rd-order factors in the following ways:
\begin{equation}
    \begin{split}
	& \bar{\gamma} = [\bar{\gamma}_1,\bar{\gamma}_2]^T = [\frac{\gamma_1}{\gamma_1+\gamma_2},\frac{\gamma_2}{\gamma_1+\gamma_2}]^T, \\
	& \bar{v} = \bar{\gamma}_1v^1 + \bar{\gamma}_2v^2.
	\end{split}
\end{equation}
The sub-models of excluding other orders are obtained in a similar way.
%Obviously, this elimination method is equivalent to deleting the parts related to the eliminated factors on the network.

%An inspiring and interesting phenomenon is that,
From the results of CCC in Table 3 of the main body of our paper, we can see that the general importance of order is ranked as: 1st, 3rd and 2nd. This is consistent with the results of contribution rate analysis we conduct later.

\subsubsection{More Fusion Mechanism Analysis}
In Table 4 of the main body of our paper, in addition to show the contribution of each modality in each order, we also report the general contribution of each modality in the whole MMFF process. This is calculated as:
\begin{equation}
   	\gamma_{mod} = \gamma_1 \gamma_{mod}^1 + \gamma_2 \gamma^2_{mod} + \gamma_3\gamma^3_{mod}.
\end{equation}
An visualization of the modality contribution ($\gamma^1$, $\gamma^2$, $\gamma^3$, $\gamma_{mol}$) w.r.t. the number of iterations is shown in Figure 2. It can be seen from the results that the contributions of different modalities change during the training process.
%during the iteration process, the relationship between contributions is constantly changing,
This indicates that there are complex interactions and mutual promotions between factors. For example, in a global sense (Figure \ref{subfig:mol}), the audio modality information may be promoted by the text modality.

\begin{table}[!t]
   	\centering
   	\caption{Order Contribution in MMFF, where the largest contribution is marked in bold.}
   	\label{tab:contorder}
   	\resizebox{.85\columnwidth}{!}{
   	\begin{tabular}{|l|l|l|l|}
   		   	\hline
   		   	Factor Order      & 1st Factor & 2nd Factor & 3rd Factor \\ \hline
   		   	Contribution Rate & \textbf{38.7\%}     & 29.5\%     & 31.8\%     \\ \hline
   		   	\end{tabular}
   	}
\end{table}
   	
\begin{figure}[!t]
	\centering
	\includegraphics[scale=0.7]{pic/factor.pdf}
	\caption{Order contribution w.r.t. the number of iterations in our MMFF.}
	\label{fig:order}
\end{figure} 

The contribution of each order ($\gamma$) can be analysed in the same way. The value of $\gamma $ of the fusion result is reported in Table \ref{tab:contorder} and the visualization is shown in Figure \ref{fig:order}. It can be seen that the order of factor contribution is: 1st (38.7\%), 3rd (31.8\%) and 2nd (29.5\%), which is consistent with the observations in our ablation study. As shown in Figure \ref{fig:order}, the 1st-order dominates at the early training stage. When the number of iterations increase, contributions of the 2nd and 3rd factors become larger. This may be because that the interactions between different modalities gradually increase and contributes more to the prediction.
%, are gradually extracted and attached more importance.


\end{document}

