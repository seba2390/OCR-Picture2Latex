\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[nonatbib, final]{neurips_2023}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\usepackage[algo2e, ruled,vlined]{algorithm2e}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics

\usepackage[para]{threeparttable}

% \usepackage{natbib}

\usepackage{microtype}
\usepackage{graphicx,fancybox}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{algorithm,algorithmic}
\usepackage{amssymb, multirow, paralist, color,amsmath,amsthm}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
%\newtheorem{definition}[cor]{Definition}
\newtheorem{ass}{Assumption}
\usepackage{enumitem}
\setlist[itemize]{align=parleft,left=1em}
\usepackage{textcase}
\usepackage[T1]{fontenc}
\newcommand{\required}[1]{\section*{\hfil #1\hfil}}                    %%
\usepackage{booktabs} % for professional tables
\usepackage{makecell}
\usepackage{comment}
% \usepackage{hyperref}
\def \S {\mathbf{S}}
\def \A {\mathcal{A}}
\def \X {\mathcal{X}}
\def \O {\mathcal{O}}
\def \Y {\mathcal{Y}}
\def \Ab {\bar{\A}}
\def \R {\mathbb{R}}
\def \Kt {\widetilde{K}}
\def \k {\mathbf{k}}
\def \w {\mathbf{w}}
\def \v {\mathbf{v}}
\def \t {\mathbf{t}}
\def \x {\mathbf{x}}
\def \Se {\mathcal{S}}
\def \E {\mathrm{E}}
\def \Rh {\widehat{R}}
\def \x {\mathbf{x}}
\def \p {\mathbf{p}}
\def \a {\mathbf{a}}
\def \diag {\mbox{diag}}
\def \b {\mathbf{b}}
\def \e {\mathbf{e}}
\def \ba {\boldsymbol{\alpha}}
\def \c {\mathbf{c}}
\def \tr {\mbox{tr}}
\def \d {\mathbf{d}}
\def \db {\bar\mathbf{d}}
\def \1 {\mathbf{1}}

\def \z {\mathbf{z}}
\def \s {\mathbf{s}}
\def \bh {\widehat{\b}}
\def \y {\mathbf{y}}
\def \u {\mathbf{u}}
\def \uh {\widehat{\u}}
%\def \L {\mathcal{L}}
\def \H {\mathcal{H}}
\def \g {\mathbf{g}}
\def \F {\mathcal{F}}
\def \I {\mathbb{I}}
\def \P {\mathcal{P}}
\def \Q {\mathcal{Q}}
\def \xh {\widehat{\x}}
\def \wh {\widehat{\w}}
\def \lambdah {\widehat{\lambda}}

\def \ah {\widehat{\a}}
\def \Rc {\mathcal R}
\def \Sigmah {\widehat\Sigma}


\def \Bh {\widehat B}
\def \Ah {\widehat A}
\def \Uh {\widehat U}
\def \Ut {\widetilde U}
\def \B {\mathcalB}
\def \C {\mathbf C}
\def \U {\mathbf U}
\def \Kh {\widehat K}
\def \fh {\widehat f}
\def \yh {\widehat\y}
\def \Xh {\widehat{X}}
\def \Fh {\widehat{F}}

\def \m {\mathbf{m}}
\def \y {\mathbf{y}}
\def \E {\mathrm{E}}
\def \x {\mathbf{x}}
\def \g {\nabla{g}}
\def \D {\mathcal{D}}
\def \z {\mathbf{z}}
\def \u {\mathbf{u}}
\def \H {\mathcal{H}}
\def \Z {\mathcal{Z}}
\def \Pc {\mathcal{P}}
\def \w {\mathbf{w}}
\def \r {\mathbf{r}}
\def \R {\mathbb{R}}
\def \S {\mathcal{S}}
\def \regret {\mbox{regret}}
\def \Uh {\widehat{U}}
\def \Q {\mathcal{Q}}
\def \W {\mathcal{W}}
\def \N {\mathcal{N}}
\def \A {\mathcal{A}}
\def \q {\mathbf{q}}
\def \v {\mathbf{v}}
\def \M {\mathcal{M}}
\def \c {\mathbf{c}}
\def \ph {\widehat{p}}
\def \d {\mathbf{d}}
\def \p {\mathbf{p}}
\def \q {\mathbf{q}}
\def \db {\bar{\d}}
\def \dbb {\bar{d}}
\def \I {\mathbb{I}}
\def \xt {\widetilde{\x}}
\def \yt {\widetilde{\y}}
\def \hrho {\hat{\rho}}

\def \f {\mathbf{f}}
\def \a {\mathbf{a}}
\def \b {\mathbf{b}}
\def \ft {\widetilde{\f}}
\def \bt {\widetilde{\b}}
\def \h {\mathbf{h}}
\def \B {\mathcal{B}}
\def \bts {\widetilde{b}}
\def \fts {\widetilde{f}}
\def \Gh {\widehat{G}}
\def \G {\mathcal {G}}
\def \bh {\widehat{b}}
\def \wh {\widehat{\w}}
\def \Dth {\widehat{\Delta}}
\def \vb {\bar{\mathbf v}}
\def \zt {\widetilde{\z}}
\def \zh {\widehat{\z}}
\def \zts {\widetilde{z}}
\def \s {\mathbf{s}}
\def \gh {\widehat{\g}}
\def \vh {\widehat{\v}}
\def \Sh {\widehat{S}}
\def \rhoh {\widehat{\rho}}
\def \hh {\widehat{\h}}
\def \C {\mathcal{C}}
\def \V {\mathcal{L}}
\def \t {\mathbf{t}}
\def \xh {\widehat{\x}}
\def \Ut {\widetilde{U}}
\def \wt {\m}
\def \Th {\widehat{T}}
\def \Ot {\tilde{\mathcal{O}}}
\def \X {\mathcal{X}}
\def \nb {\widehat{\nabla}}
\def \K {\mathcal{K}}
\def \P {\mathbb{P}}
\def \T {\mathcal{T}}
\def \F {\mathcal{F}}
\def \ft{\widetilde{f}}
\def \Rt {\mathcal{R}}
\def \Rb {\bar{\Rt}}
\def \wb {\bar{\w}}
\def \zu {\underline{\z}}
\def \vect {\text{vec}}
\def \E {\mathbb{E}}

\def\bw{\mathbf{w}}

\def \start {\textsc{Start}}

\title{SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{
  Bang An\\
  Department of Business Analytics\\
  University of Iowa\\
  Iowa City, IA 52242 \\
  \texttt{bang-an@uiowa.edu} \\
  % examples of more authors
   \And
   Xun Zhou \thanks{corresponding author} \\
  Department of Business Analytics\\
  University of Iowa\\
  Iowa City, IA 52242 \\
  \texttt{xun-zhou@uiowa.edu} \\
   \AND
    Yongjian Zhong \\
  Department of Computer Science\\
  University of Iowa\\
  Iowa City, IA 52242 \\
  \texttt{yongjian-zhong@uiowa.edu} \\
   \And
  Tianbao Yang \\
  Department of Computer Science and Engineering\\
  Texas A\&M University\\
  College Station, TX 77843 \\
  \texttt{tianbao-yang@tamu.edu} 
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}







\maketitle


\begin{abstract}
%There are millions of traffic accidents that happened in the United States every year, but less than $1\%$ of them result in injuries. Those severe accidents bring numerous safety concerns to society and cause substantial property damage.
%Therefore, learning to rank traffic accidents and identify the riskiest locations is most important to traffic stakeholders.

The problem of urban event ranking aims at predicting the top-$k$ most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. 
%
Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. % due to the common assumption that items are independent.
%
In this paper, we bridge the gap by proposing a novel spatial event ranking approach named SpatialRank. SpatialRank features adaptive graph convolution layers that dynamically learn the spatiotemporal dependencies across locations from data. In addition, the model optimizes through surrogates a hybrid NDCG loss with a spatial component to better rank neighboring spatial locations. We design an importance-sampling with a spatial filtering algorithm to effectively evaluate the loss during training. Comprehensive experiments on three real-world datasets demonstrate that SpatialRank can effectively identify the top riskiest locations of crimes and traffic accidents and outperform state-of-the-art methods in terms of NDCG by up to $12.7\%$. 

%Spatiotemporal data exhibits ubiquitous presence and facilitates the capability of forecasting the occurrences of urban events such as traffic accidents, crime, etc. However, events are not equally important, and those severe events bring numerous safety concerns to society and result in substantial property damage. Therefore, learning to rank the most severe events and identify the riskiest locations is significant to the event stakeholders. However, ranking events is challenging. First, priority should be given to predicting events in the foremost important ones. Second, complex spatial dependencies exist between events and surrounding attributes, which change dynamically over time. Third, many events exhibit uneven spatial distribution patterns and tend to be clustered in a few hot-spot regions. As a result, potential severe events nearby are possibly ignored during modeling. In this paper, we formulate event forecasting as a learning-to-rank problem and propose a novel event-ranking approach named SpatialRank. Furthermore, We optimize our model using Normalized Discounted Cumulative Gain (NDCG) with surrogates to prioritize the top-K most important locations in the prediction. Besides, we develop adaptive graph convolution layers that dynamically learn from spatiotemporal features and historical accident patterns. To capture potential severe events around hot-spot regions, we propose a novel hybrid objective function to consider the relative local ranking on subsets of locations. Comprehensive experiments on two real-world datasets demonstrate that SpatialRank can effectively identify the riskiest locations and outperform state-of-art methods in terms of NDCG by up to $11.8\%$.


\end{abstract}



\section{Introduction}

% xz
Given a risk score defined based on historical urban events (e.g., crimes, traffic accidents) in a study area as well as the socio-environmental attributes (e.g., travel demands, weather, road network) associated with the events, the goal of the \emph{urban event ranking problem} is to learn a model that can predict the ranking of the top-$k$ riskiest locations of future events. 

The urban event ranking problem is a widely observed spatiotemporal prediction problem, which has critical applications in public safety, traffic management, and urban planning. For example, the National Highway Traffic Safety Administration \cite{2020fatality}, estimated a total of 42,939 deaths in motor vehicle traffic crashes in the United States in 2021. Crime control and property loss cost over \$2.5 trillion in the United States in 2017 \cite{MillerTedR.2021IaCo}. 
Chicago Police Department has utilized criminal intelligence analysis and data science techniques to help command staff determine where best to deploy resources \cite{CPD}. However, according to the Police Executive Research Forum, there were $42.7\%$ more resignations among law enforcement but a $3.9\%$ decrease in hiring new officers in 2021 compared to 2019 \cite{PERF}. Meanwhile, the Federal Bureau of Investigation confirms that violent crime in 2020 has surged nearly $30\%$ over 2019 \cite{FoxNews}. Therefore, given such growth of crimes and accidents, predicting the riskiest locations of traffic accidents or crimes helps law enforcement stakeholders allocate their limited resources strategically to reduce injuries, deaths, and property losses.  


%What's more, predicting the most probable locations of future taxi pickups help traffic managers and taxi drivers improve their planning and business efficiency. 
%Spatiotemporal urban events such as traffic accidents and crime pose significant public safety concerns and cause tremendous property losses. According to the National Highway Traffic Safety Administration (NHTSA) \cite{2020fatality}, an estimated 42,939 people died in motor vehicle traffic crashes in the United States in 2021, which is the highest number of fatalities since 2005 and the largest annual percentage increase in the history. Besides, crime control and property loss cost 
%over \$2.5 trillion in the United States in 2017 \cite{MillerTedR.2021IaCo}. Therefore, the capacity to predict urban events holds vital importance. %For example, the Tennessee Highway Patrol (THP) has successfully employed predictive analytics\cite{acctool}, leading to a notable reduction of approximately $33\%$ in average response time since 2012 and a decrease of roughly $8\%$ in the accident fatality rate between 2010 and 2016. %There are more than six minion car accidents in the US every year\cite{uscar}. 
%
%However, most traffic accidents are minor scratches and only a small proportion of them lead to severe injuries and property damage\cite{chicagodata}. Similarly, felonies take up a small proportion of all crimes, but they bring numerous public concerns and social impact. Therefore, learning to rank future severe events and identifying the foremost riskiest locations is important to traffic participants and potential crime victims. Prioritizing the accuracy of ranking the riskiest locations can enable the government to allocate limited resources and reduce life and property losses.

Despite its importance, the urban event ranking problem is technically challenging. \textbf{First}, there exist complex and dynamic spatiotemporal correlations between locations in terms of events and attributes such as traffic conditions and weather changes. Capturing such dynamic relationships is non-trivial. \textbf{Second}, due to the presence of spatial autocorrelation, nearby locations may share very similar socio-environmental attributes. It is thus very challenging to learn a good model that can correctly rank neighboring locations. \textbf{Finally}, urban events are usually sparse in space. Making a prediction with a low average error in count does not ensure a good ranking of the top-$k$ locations.
%There are several challenges in ranking future events. First, 
%the primary interest is to make correct predictions on the top-K riskiest locations, but it is difficult to prioritize top-K predictions during model optimization. Second, there exist complicated spatial and temporal correlations between events and surrounding attributes such as traffic conditions and weather changes. Capturing such dynamic changes is difficult. Third, risky locations tend to be clustered in populated areas over other places. Favoring such regions during training models can ignore the potential risks in the rest of the area. Therefore, considering the relative ranking quality in each region and ranking them locally is also necessary.

Prior works on urban event prediction employed either classic machine learning methods \cite{DONG2015192}~\cite{abellan2013analysis}~\cite{bergel2013explaining}~\cite{CALIENDO2007657}~\cite{chang2005analysis}~\cite{chang2005data}~\cite{KhezerlouAmin2017ATFA}
or deep learning techniques including recurrent neural networks \cite{Ren2018ADL} and convolutional neural networks \cite{najjar2017combining}\cite{VahedianAmin2019PUDE}. %to capture temporal changes and spatial correlations. 
Recently, Li et al. \cite{LiZhonghang2022SHSL} proposed a cross-region hypergraph structure network to address the data sparsity issues. Besides, Yuan et al. \cite{yuan2018hetero}~\cite{doi:10.1137/1.9781611977172.38} proposed spatial ensembles to address spatial heterogeneity. %Although previous studies have made decent progress in improving prediction accuracy, they fail to provide meaningful guidance to the public. %The most popular way is to formulate traffic accident forecasting as a classification problem to predict the happening of an accident \cite{ABELLAN20136047} or a regression problem to predict the number of accidents \cite{ABDELATY2000633}. 
The objective of the above methods is to make accurate predictions of the risk or count for all locations. However, urban events are often very sparse in space, which might guide the model to avoid predicting accidents in most locations to obtain a low average error. Such prediction does not truly benefit the users such as police officers. The top-$k$ locations derived from such predictions are naturally inaccurate. 

Our problem is also relevant to the learning-to-rank problem frequently studied in the field of recommendation systems. State-of-the-art methods in this area typically predict the top-$k$ items most likely to be chosen by users through optimizing ranking-based metrics such as the Normalized Discounted Cumulative Gain (NDCG)~\cite{bhatia2015sparse}.
%We argue that correct predictions on top-$K$ ranked locations are more important. Thus we formulate the event forecasting as a ranking problem to rank the riskiest locations and optimize the accuracy of the foremost riskiest locations. 
%To achieve this goal, we use a popular ranking metric named Normalized Discounted Cumulative Gain (NDCG) from the field of recommendation systems and web searching. NDCG is a performance metric of ranking the foremost importance in machine learning tasks ~\cite{bhatia2015sparse}. 
The rank operator of NDCG is non-differentiable, and previous studies have made noticeable progress in approximating NDCG by surrogate functions ~\cite{PiRank}~\cite{ApproxNDCG}~\cite{SoftRank}~\cite{Qiu2022LargescaleSO}. In our problem, a straightforward adaptation would be to consider the locations as ``items'' and each time slot as a ``user''. However, NDCG is not a perfect objective function for our problems as it neither measures the local ranking quality nor considers spatial autocorrelation among locations. Instead, the existing approaches assume items are independent. Therefore, directly applying existing NDCG optimization solutions might not be the best solution to our problem.

In this paper, we bridge the gaps in both fields by formulating the urban event ranking problem as a spatial learning-to-rank problem and solving it by directly optimizing a ``spatial'' version of the NDCG measure. We propose \textbf{SpatialRank}, our deep learning model with three novel designs. To efficiently capture the spatial and temporal dependencies, we design an adaptive graph convolution layer that learns the correlations between locations dynamically from features and historical events patterns; to ensure the model balances both the global ranking quality on all the locations and the local ranking quality on subsets of locations, we propose a hybrid loss function combining both NDCG loss and a novel local NDCG loss; to improve the effectiveness of optimizing NDCG surrogates in the spatiotemporal setting of our problem, we design an importance-based location sampling with spatial filtering algorithm to iteratively adjust the weights of each location considered in the objective function, thereby guiding the model to concentrate on learning for more important locations. We conduct comprehensive experiments on three real-world datasets collected from Chicago and the state of Iowa. The results demonstrate that SpatialRank can substantially outperform baselines and achieve better ranking quality.
%the first spatiotemporal event ranking network named SpatialRank, and we directly optimize the normalized discounted cumulative gain (NDCG) to prioritize the foremost riskiest locations. To efficiently capture complicated spatial and temporal correlations from the datasets, we design an adaptive graph convolution layer that learns the graph dynamically from features and historical events patterns. To improve the effectiveness of optimizing NDCG surrogates in the spatiotemporal problem, we design an importance-based location sampling mechanism with spatial filtering to iteratively adjust the weights of each location considered in the objective function, thus we can instruct the model to concentrate on specific locations which are considered to be more important in the study area. Besides, we propose a hybrid loss function combining both NDCG loss and a novel local NDCG loss to ensure the model reaches a trade-off between the global ranking quality on all locations and the local ranking quality on subsets of locations. We conduct comprehensive experiments on two real-world datasets collected from Chicago and the state of Iowa. The results demonstrate that our proposed approach can substantially outperform baselines and achieve better ranking quality.

Our contributions are summarized below:
\begin{itemize}
\vspace{-3.5mm}
    \setlength\itemsep{-0.0em}
    \item To the best of our knowledge, this is the first paper to formulate urban event forecasting as a location ranking problem and learn a ranking model by optimizing its ranking quality.
    \item We propose SpatialRank with adaptive graph convolution layers learning from historical event patterns and spatiotemporal features to capture dynamic correlations over time and space.
    \item We propose a hybrid objective function to leverage the trade-off between global ranking quality and local ranking quality.
    \item We propose a ranking-based importance sampling algorithm to adaptively adjust the weights of different locations considered in the objective function of the prediction results from the last training epoch to help the model focus on important locations. 
    
\end{itemize}

\section{Preliminaries}
\subsection{Formulation of the event ranking problem}
%Given previously observed data such as traffic conditions and weather changes over all locations, our goal is to rank the top K most important locations the next day. 
A spatio-temporal field $\S \times T$ is a three-dimensional partitioned space, where $T = \{t_{1}, t_{2},...,t_{T}\}$ is a study period divided into equal-length intervals (e.g., hours, days) and $\S = \{s_{(0,0)},...,s_{(M,N)}\}$ is a $M \times N$ two-dimension spatial grid partitioned from the study area. A set of socio-environmental features $F$ are observed over $S\times T$, which include temporal features $F_{t} \in \mathbb{R}^{T}$ (e.g., day of week, holiday), spatial features $F_{s} \in \mathbb{R}^{M \times N}$ (e.g., total road length, speed limit), and spatiotemporal features $F_{st} \in \mathbb{R}^{M \times N \times T}$ (e.g., traffic volume, rainfall amount). A risk score $y \in \mathbb{R}^{M \times N \times T}$ is a user-defined attribute over $\S\times T$ measuring the risk level of each spatiotemporal location (e.g., number of crimes, injuries of traffic accidents). $y(s,t) > 0$ when any events occurred in $(s, t)$, and equals 0 when no events occurred.

%Formally, suppose we observe socio-environmental measurements over a spatial field $\S \times T$, where $T = \{t_{1}, t_{2},...,t_{T}\}$ is a study period divided into equal-length intervals (e.g., hours, days) and $\S = \{s_{(0,0)},...,s_{(M,N)}\}$ is a $M \times N$ two-dimension spatial grid partitioned from the study area. Note that observations are represented by three kinds of tensors including temporal feature $F_{t} \in \mathbb{R}^{T}$, spatial feature $F_{s} \in \mathbb{R}^{M \times N}$, and spatiotemporal feature $F_{st} \in \mathbb{R}^{M \times N \times T}$. %Details of data and generating features can be found in Appendix A. 
Given the socio-environmental features $F$ and risk score $y$ for all the locations in time window $\{t_1,t_2,.., t_n\}$, the urban event ranking problem is to predict the ranking of the top-$k$ locations $\{s_{1},...,s_{K}\} \in S$ in the next time interval $t_{n+1}$ with the highest risk scores. %We define the level of importance as the number of events that occurred in a location. 
The objective is to prioritize the ranking quality on the top-$k$ riskiest locations. As a basic assumption of our problem, there exists spatial and temporal autocorrelation among locations in $F$, meaning nearby locations tend to have more correlated values. In addition, events are sparse so $y$ is 0 for the majority of the locations. %Furthermore, traffic accidents should only occur along the road network, we use road network data to filter out noise and errors that exist in the GPS records. Specifically, we apply a mask filter to ignore locations without road segments during model training and predictions.
A detailed example of data attributes and feature generation steps can be found in the supplementary materials Appendix A. 

\subsection{Stochastic Optimization of NDCG in Event Ranking Problem}
In the field of recommendation systems, learning to rank is substantially studied, and NDCG is widely used as the metric to measure the ranking quality of the foremost importance. In the following part, we use the terminologies from the event forecasting problem to define NDCG in our problem setting. For a ranked list of locations $s \in \S$ in a period $t \in T$, the NDCG score is computed as by:

\begin{align}\label{eqn:ori_ndcg}
    \text{NDCG}_t = \frac{1}{Z_t}\sum_{s\in \S_t}\frac{2^{y_s}-1}{\log_2(1+\text{r}(s))},
\end{align}

where $y_s$ is the risk score of the location $s$, $\text{r}(s)$ denotes the rank of location $s$ in the studied spatial domain $\S_t$, and $Z_t$ is the Discounted Cumulative Gain (DCG) score \cite{jarvelin2002cumulated} of the perfect ranking of locations for time period $t$. However, the rank operator in NDCG is non-differentiable in terms of model parameters, and thus cannot be optimized directly. A popular solution is to approximate the rank operator with smooth functions and then optimize its surrogates \cite{SoftRank}\cite{ApproxNDCG}\cite{Qiu2022LargescaleSO} as shown in Eq. \ref{eqn:hinge}.

\begin{equation} \label{eqn:hinge}
    \bar g(\w; \x, \S_t) = \sum_{s'\in\S_t}\ell(h_t(s'; \w) - h_t(\s; \w)),
\end{equation}

where rank operator $r(s)$ in NDCG is approximated by a differentiable surrogate function $\ell()$, and the squared hinge loss $\ell(x)= \max(0,x+c)^2$ is commonly used \cite{WuMingrui2009SDfl}. In this way, the model parameters $\w$ can be updated by a gradient-based optimizer. 
%Lastly, we explain the notations of approximated NDCG in the traffic accident ranking problem. 
We can maximize over $L(\w)$:

\begin{equation} \label{eqn:NDCG}
\max_{\w\in\R^d} L(\w):=\frac{1}{|T|}\sum_{t=1}^T\sum_{s\in \S^+_t} \frac{2^{y^t_s}-1}{Z_t\log_2(\bar g(\w; \x^t_s, \S_t)+1)}.
\end{equation}

where %$t \in T$ represents each time interval of the study period and $\S_t$ denotes the set of all locations to be ranked in the study area for the time $t$. For each location $s \in \S$, $y_s^t$ denotes the number of events that occurred in location $s$ and time $t$. 
$\s^t\in S^+_t$ denotes a set of locations with positive risk scores to be considered in the objective function. In this way, the optimization solution on NDCG can be directly applied to the event ranking problem.

\section{Methodology}

Directly optimizing NDCG as described above might provide a solution to our problem but fails to capture the spatiotemporal autocorrelation in the data. Also, since NDCG is a global measure, the model might not be able to learn how to rank nearby locations with highly correlated features correctly. In this section, we present our SpatialRank method to address these limitations. Figure. \ref{fig: framework} demonstrates the proposed model architecture.

%Although the existing methods of NDCG optimization can be applied to event ranking problems, they assume the independence of items in their design therefore they fail to capture the spatial correlations between locations. Additionally, most existing models for forecasting events \cite{Wang2021gsnet}\cite{yuan2018hetero}\cite{doi:10.1137/1.9781611977172.38} can be used to rank locations based on their predicted number, but most of them are optimized to obtain perfect accuracy on everywhere and ignore the foremost important locations. Besides, existing methods heavily rely on learning from the pre-defined road network graph but ignore the dynamic changes in nearby traffic conditions and periodic patterns. To address those challenges, 
%We propose a novel event ranking NDCG optimization process with an importance-based location sampling framework and a hybrid objective function to rank the top-K riskiest locations. Furthermore, we design a deep-learning model to learn from spatial, temporal, and spatiotemporal features effectively. 

\begin{figure}
 \centering
 \includegraphics[width = 1.0\textwidth]{model.pdf}
 \caption{SpatialRank Architecture. The spatiotemporal features are used to generate adjacency matrices and then embedded by graph convolution layers. We use a fully connected layer to make final predictions. The hybrid objective function is combined with NDCG loss and local NDCG loss.}
 \label{fig: framework}
\end{figure}

\subsection{The Deep Learning Model}
Many recent deep learning models for spatiotemporal could be used as the backbone architecture of our SpatialRank method. A key idea in these methods is to model the correlations between locations using a graph, with locations as nodes and edge weights representing the strength of correlations, and extract latent spatial dependency information through graph convolution layers~\cite{Wang2021gsnet}~\cite{doi:10.1137/1.9781611977172.38}. We follow a similar idea to build the overall network architecture, where spatial and spatiotemporal features are fed into graph convolutional layers. Then the extracted latent representations are concatenated with the temporal features and fed into LSTM layers to capture temporal representations before the final output layer.  

A key novelty in our SpatialRank network is the design of a time-guided and traffic-aware graph-generating process for the graph convolution layers. Prior works typically use Pearson's correlation coefficients or similar measures of features (e.g., traffic volume, accident counts) between locations as their correlation strengths and pre-compute a \textbf{time invariant} adjacency matrix of locations \cite{Zhang2019TrafficGANOT}. In fact, studies \cite{Careye014319}\cite{MaoXinhua2019RFAT} demonstrate that the influence of traffic conditions on events varies over different periods. 
%The adaptive graph convolution layers learn from historical event patterns and instantaneous surrounding conditions to improve prediction accuracy. %adaptive graph convolution layers learn from temporal feature $F_T$ and spatiotemporal feature $F_{ST}$. The key challenge is to generate a graph. %A native approach is to use Pearson Correlation to compute a static adjacency matrix from historical event data \cite{Zhang2019TrafficGANOT}. However, this design ignores the variation of correlations over time and fails to utilize the external information from features. Another possible design is to use an attention mechanism and learn such correlations from massive data. However, this method brings extra model parameters and makes the model hard to converge due to a lack of sufficient training data. It is difficult to collect a sufficient amount of traffic-related data in reality because of limitations of the public budget and privacy issues. 
%
Inspired by a related work \cite{wu2020connecting} on a different problem, we use historical events to generate a static graph and learn a time-variant graph from $F_{ST}$ (e.g., traffic volume) in each time interval. %Studies \cite{Careye014319}\cite{MaoXinhua2019RFAT} demonstrate that the influence of traffic conditions on events varies over different periods. 
Intuitively, we use $F_T$ (e.g. hour of the day) to learn the weights of dynamic graph vs. static graph to be considered in the graph convolution. The key equations of generating the adaptive graph adjacency are shown below:%\ref{eqn:graph}:
\begin{align} \label{eqn:graph}
& Z_1 = \text{tanh}(\alpha E_1 W_1) \\
& Z_2 = \text{tanh}(\alpha E_2 W_2) \\
& \A_{dynamic} = \text{ReLU}(\text{tanh}(\alpha(Z_1 Z_2^T - Z_2 Z_1 ^ T))) \\
& \beta = \text{sigmoid}(F_T W_3) \\
& \A = \beta \A_{dynamic} + (1 - \beta) \A_{static} 
\end{align}

Where $W_1$, $W_2$, and $W_3$ are learnable parameters. $E_1$ and $E_2$ are randomly initialized node embeddings that can be learned during training. We represent those embeddings by the spatiotemporal features $F_{ST}$ (e.g., traffic volume) to reveal the underlying dynamic connections between nodes. The subtraction and ReLU activation function in Eq. (6) lead to the asymmetric property of $\A_{dynamic}$. $\alpha$ is a hyperparameter to control the saturation rate. $\A_{static}$ is a pre-calculated adjacency matrix before training the model by computing the Pearson correlation coefficient between the risk scores of locations (nodes), where the correlation between node $i$ and node $j$, $a_{ij}= \frac{\sum_t (y_i - \bar{y_i})(y_j - \bar{y_j})}{\sqrt{\sum_t (y_i - \bar{y_i})^2(y_j - \bar{y_j})^2}}$. The final adjacency matrix $\A$ is the weighted sum of $\A_{dynamic}$ and $\A_{static}$, and the weight $\beta$ is learned by a sigmoid activation function in Eq. 7 from the linear transformation of temporal feature $F_T$. Intuitively, the static adjacency matrix indicates a baseline correlation between different locations and it is pre-computed before training. This is also what most of the related work has been done. However, inspired by many observations from related studies \cite{Careye014319}\cite{yuan2018hetero}, it is evident that we think this correlation is not always constant. Therefore, we use locationsâ€™ time-variant features to construct a new adjacency matrix, and this dynamic adjacency matrix varies with time. We learn the parameters in this dynamic matrix during the network training process. Finally, we combine the static and the learned dynamic matrices through a learned weight $\beta$. In this way, a combined adjacency matrix can be treated as an adaptation from a static adjacency matrix considering the influence of other features during different periods. %Lastly, the learned latent representations from the adaptive graph convolution layers are fed into the LSTM layer through fully connected layers. Concurrently, the spatial features (eg. road network) are fed into a graph convolution layer with a static graph to learn a geographic representation. Both representations are concatenated and through a fully connected layer to make final predictions.
Finally, extracted embeddings are fed into LSTM layers and make final predictions of the risk scores $y$ for each location through a fully connected layer.

\subsection{Local Ranking Metric and Hybrid Loss function}

%NDCG is widely used as a metric for ranking the most relevant items from a query in the fields such as search engines and recommendation systems, where a list of items is ranked given a user or search keywords. Note that the top-ranked items in the list might change over different users or search keywords. However, it is not true in event ranking problems. 

%In the urban event ranking problem, historical data illustrates that there exist stationed hot-spot locations such as the downtown areas whose event occurrences are constantly higher than nearby regions. Such locations will take up a great proportion of the NDCG score. Those fixed hot spots are numerically important in terms of the number of events and rank higher than other locations. Nevertheless, predicting those fixed hot-spot locations cannot provide meaningful guidance to the event stakeholder because the people have known they are important based on historical averages and experiences. %Moreover, using traffic accidents as an example, numerous effects including setting speed limits and speed cameras have been established in those locations, thus no extra effects should be paid. 
As previously mentioned, ranking-based metrics such as NDCG are not designed to handle spatial correlations. According to the first law of geography \cite{ToblerW.R.1970ACMS}, nearby locations may share very similar socio-environmental attributes, thus it is challenging to rank neighboring locations correctly. However, the locations nearby with uncertain event patterns are worthy to be focused on so that more potential events can be discovered. Moreover, using NDCG on event ranking problems can cause over-concentrating on top-ranked locations and sacrificing prediction accuracy on other locations due to lower priority. To address those issues, we design a novel local ranking measurement named Local Normalized Discounted Cumulative Gain (L-NDCG) to measure spatially local ranking quality over every sub-region of the study area. The L-NDCG is calculated as:
% \vspace{-0.1in}

\begin{equation} \label{eqn:LNDCG}
\text{L-NDCG}=\frac{1}{T{|\S_t|}}\sum_{t=1}^T\sum_{s^t\in \S_t} \sum_{\hat{s}\in \N(s^t)} \frac{2^{y^t_{\hat{s}}}-1}{Z^t_{\N(s^t)}\log_2(r(\hat{s},\N(s^t))+1)}.
\end{equation}
% \vspace{-0.1in}

We use the same terminologies from Eq. \ref{eqn:NDCG}. The unique part is that we compute an NDCG score for every location in the study area based on the local ranking of a subset of locations $\N(s^t)$, where $\N$ is a neighborhood of location $s^t$. We define the $\N$ as the Euclidean distance of coordinates smaller than $R$ in this work. $\N$ can be defined in other ways depending on the need of the problem. Essentially, L-NDCG is the average of NDCG scores for all subsets of locations. In this way, a few stationed hot-spot locations only take considerably large weight in their own NDCG scores and cannot over-influence the overall L-NDCG score.  L-NDCG emphasizes ranking correctly on each subset of locations, and a greater L-NDCG score indicates that relatively more important locations can be distinguished from their nearby locations in a small region. Similar to optimizing NDCG, the ranking operator of L-NDCG is non-differentiable, thus we optimize its surrogates instead.
%\vspace{-0.1in}

\begin{equation} \label{eqn:LNDCG2}
\max_{\w\in\R^d} L(\w):=\frac{1}{T{|\S^+_t|}}\sum_{t=1}^T\sum_{s^t\in \S^+_t} \sum_{\hat{s}\in \N(s^t)} \frac{2^{y^t_{\hat{s}}}-1}{Z^t_{\N(s^t)}\log_2(\bar g(\w; \x^t_{\hat{s}}, \N(s^t))+1)}.
\end{equation}
% \vspace{-0.1in}

Where $g(\w; \x^t_{\hat{s}}, \N(s^t))$ is a surrogate loss function similar to Eq. \ref{eqn:hinge}, and $\N(s^t)$ is a set of locations. Finally, to learn a trade-off between locally ranking quality and globally ranking quality we design a hybrid objective function consisting of both NDCG and L-NDCG
\begin{equation} \label{eqn:hybrid}
Loss = (1 - \sigma)\textsc{NDCG} + \sigma \textsc{L-NDCG}
\end{equation}
\vspace{-0.1in}
Where $\sigma$ is a hyperparameter controlling the preference between NDCG and L-NDCG.

\subsection{Importance-based Location Sampling with Spatial Filtering}

% In the field of recommendation systems, significant progress was made to approximate ranks in NDCG with a smooth function \cite{ApproxNDCG}\cite{Qiu2022LargescaleSO}, but correlations between items to be ranked are largely unexplored. However, spatial auto-correlation between locations has been proved necessary to be captured \cite{najjar2017combining}\cite{9242313} in the case of event forecasting problems. 
To bridge the gap between capturing spatial correlations in a list of locations and optimizing the quality of predicted ranking, we propose a novel importance-based location sampling strategy with spatial filtering. Specifically, we first design an importance measure function to assign higher weights to locations with larger errors in predictions and higher ranking priority. Secondly, we sample locations based on the weights assigned by importance-measuring, so that important locations have a higher probability to be sampled. Afterward, only losses from sampled locations will be calculated in the objective function and considered during the optimization. In this way, the model pays attention to more important locations. Thirdly, we adjusted the importance scores every epoch, so that the model learns to focus on different locations adaptively. Lastly, spatial filters are applied to smooth the importance scores in each epoch to achieve spatial-aware sampling. This allows nearby locations to be sampled in the same batch with high probability to help the model learn how to rank them. The training process is shown in algorithm \ref{alg. pl}

\begin{algorithm2e}
	\SetKwInput{Input}{Input}
	\SetKwInput{Output}{Output}
	\LinesNumbered
	\DontPrintSemicolon
	\BlankLine
	\caption{SpatialRank Training}
	\label{alg. pl}
	%\begin{algorithmic}[1]
	\Input{feature tensor $F$, event risk scores $y$, hyperparameter $\sigma$, standard deviation $\lambda$} 
	\Output{learned model $f_\theta{}$}
	\BlankLine
	
    Initialize probability set $P$ = $\{p_1, p_2, ..., p_l\} \in L $;
    
    \For{each epoch}{
        Compute $\hat{y}$ = $f_{\theta}(x)$ \;
        Compute $loss_{NDCG}$ = $\textbf{WeightedLoss}(y , \hat{y}, P)$ \;
        Compute $loss_{local}$ = $\textbf{LocalWeightedLoss}(y , \hat{y}, P)$ \;
        $loss_{hybrid}$ = $(1-\sigma) \cdot loss_{NDCG} + \sigma \cdot loss_{local}$

        \For{$s \in \S$}{
        $E_s$ = $\frac{1}{T} \sum_{t}{\frac{2^{|y_s^t-\hat{y_s^t}|} - 1}{ log_{2}{(1+r(y_s^t))}}}$ \;
        }
        $E_{s'}^* = \sum\limits_{s}\frac{E_s}{{2\pi} \lambda^2} e^{-\frac{x^2}{2\lambda^2}} $  for $s \in \S$, $x=dist(s', s)$\;
        %$E_{(i,j)} = \sum\frac{1}{\sqrt{2\pi \lambda}} e^{-\frac{x^2}{2\lambda^2}} $  for $(i, j) \in (M, N)$ \;
        Update P = $\textbf{normorlize}(E')$ \;
        Compute gradient $\nabla f(\theta)$ by $loss_{hybrid}$ \;
        Update $\theta$ by $\nabla f(\theta)$ \;

    }	
	\textbf{return} $f_\theta{}$
\end{algorithm2e}

Algorithm 1 shows the details of the importance-based sampling mechanism. The inputs include feature tensor $F$, event risk scores $y$, hyperparameter $\sigma$, and Gaussian standard deviation $\lambda$. The output is a learned model. The algorithm starts with initializing a set of equal-importance scores, which means the probabilities of locations being sampled are equal in the first epoch. Line 3 is forward propagation with current model parameters. Line 4 calculates the weighted NDCG losses given true label $y$, predicted labels $\hat{y}$, and current importance scores for locations. $Loss_{NDCG}$ is computed by Eq. \ref{eqn:LNDCG}, where importance scores $\{p_1, p_2, ..., p_s\} \in \S $ are normalized and treated as weights. Line 6 is our novel hybrid loss function discussed in Eq \ref{eqn:LNDCG2} to leverage the local ranking quality. The key step is to update the importance scores set based on current prediction errors and true ranking in Lines 7-8. We design a score function shown in Line 9 to leverage the errors made in predictions and the priority of true ranking for each location.
\begin{align}
E_s = \frac{1}{T} \sum_{t}{\frac{2^{|y_s^t-\hat{y_s^t}|} - 1}{ log_{2}{(1+r(y_s^t))}}}
\end{align}
where $r()$ denotes a ranking function of the $i$-th location in the study area, and $|y_s^t-\hat{y_s^t}|$ is the absolute difference between predicted injuries and true injuries. Basically, a higher score $E_l$ indicates that there are larger prediction errors and higher true ranking in this location $s$. Note that smaller $r(y_s^t)$ denotes a higher true ranking. To capture their geographical connections, we map the list of locations to their original locations on the grids, and then we apply a Gaussian filter to smooth the score distribution spatially in Line 9. $\lambda$ is the standard deviation. In this way, the model can learn better spatially correlated patterns, and avoid over-focusing on a few standalone locations but ignoring the the area nearby. Next, the smoothed scores $E^*$ are normalized into $P$, where $\sum_s P_s = 1$. Finally, we compute the gradient $\nabla f(\theta)$ by $loss_{hybrid}$ and update model parameters $\theta$. After a few iterations, we obtain the learned model $f_\theta{}$.

\subsection{Complexity Analysis}

In each iteration complexity of SpatialRank, we need to conduct forward propagation $h_t(s; \w), \forall s \in \S_t^+ \cup \S_t$ and back-propagation for computing $\nabla h_t(s; \w), \forall s \in \S_t^+ \cup \S_t$. The complexity of forwarding is $S = \sum_{t \in T} (|\S_t^+|\N{(s)} + |\S_t|)d \leq O(TSd)$, where $d$ is the model size. To compute $local_{NDCG}$ part, an extra cost on neighbor querying is needed to replace location list size on its complexity. The cost of computing $\bar g(\w; \x, \S_t)$ is $\sum_{t \in T} |\S_t^+||\S_t| \leq O(TS^2)$ for NDCG and $\sum_{t \in T} |\S_t^+| \N^2{(s)} \leq O(TS\N^2{(s)})$ for local NDCG. The size of $\N^2{(s)}$ is small, 
therefore the total cost is reduced to $O(TSd + TS^2)$. %Given the size of the model, $d>>T, S$, the cost can be further reduced to $O(TSd)$.


\section{Experiments}
We perform comprehensive experiments on three real-world traffic accident and crime datasets from Chicago{\footnote{\tiny{https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if}} and the State of Iowa\footnote{\tiny{https://icat.iowadot.gov/}}. Experiment results show that our proposed approach substantially outperforms state-of-art baselines on three datasets by up to $12.7\%$, $7.8\%$, and $4.2\%$ in NDCG respectively. The case study and results on the state of Iowa are shown in Appendix C.

\textbf{Data.} In the Chicago dataset, we collect data from the year 2019 to the year 2021. The first 18 months of this period are used as the training set, and the last 6 months of 2020 are used as the validating set. The year 2021 is used as a testing set. The area of Chicago is partitioned by $500$ m $\times$ $500$ m square cells and converted to a grid with the size of $64 \times 80$. % Normalization is used to transform data into the range $[0, 1]$. 
For the crime dataset, we use total crimes as the risk score. For accident datasets, we use the number of injuries as the risk score.

\textbf{Baselines.} First, we use daily \textbf{Historical Average (HA)}. Next, we consider popular machine-learning methods including \textbf{Long Short-term Memory (LSTM)} \cite{HochSchm97}, and \textbf{Convolutional LSTM Network (ConvLSTM)} \cite{ShiXingjian2015CLNA}. Thirdly, we compare with recent methods such as \textbf{GSNet} \cite{Wang2021gsnet}, \textbf{Hetero-ConvLSTM}\cite{yuan2018hetero}, and \textbf{HintNet} \cite{doi:10.1137/1.9781611977172.38}. Moreover, we compared our optimization approach with other NDCG optimization solutions including \textbf{Cross Entropy (CE)}, \textbf{ApproxNDCG} \cite{ApproxNDCG}, and \textbf{SONG} \cite{Qiu2022LargescaleSO}. The details of the baselines are described in Appendix C.

\textbf{Metrics.} To measure the ranking quality of foremost locations, we test $K\in[30, 40, 50]$ on the test data. We use the metrics including NDCG, L-NDCG, and top-K precision (Prec)\cite{LuJing2019SWDI}. We report the average performance and standard deviation over 3 runs for three datasets.

\subsection{Performance Comparison}
In table \ref{tab:performance}, we can observe that our SpatialRank significantly outperforms other compared baselines in both datasets. We observe a similar trend among all metrics. On both datasets, Hetero-ConvLSTM and HintNet achieve similar results and outperform general machine learning methods such as LSTM and ConvLSTM. GSNet is designed on a dataset with a smaller grid size, thus performing worse on our larger grid. To study the effectiveness of learning the appropriate graph, we set the learning parameter $\beta$ as a fixed ratio of 0.5 in SpatialRank\#. Oppositely, $\beta$ is a parameter to be learned based on temporal features in SpatialRank.  In the Table \ref{tab:performance} and Table. \ref{tab:Optimization}, each * indicates that the performance improvement of the proposed method over this baseline is statistically significant based on the student t-test with $\alpha = 0.05$ over three runs. The results show that the design of the time-aware graph convolution is able to improve performance and capture more dynamic variations in the graph, therefore performing better over different top-k rankings.

\begin{table*}[t]
\begin{threeparttable}[b]
\vspace{-0.0in}
\caption{Performance Comparison}
\vspace{-0mm}
\label{tab:performance}
\vskip 0.0in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{2.0cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}}
\toprule
\multirow{1}{*}{\thead{\textbf{Chicago Accident}}} &
\multicolumn{3}{c}{\thead{K=30}} &
\multicolumn{3}{c}{\thead{K=40}} &
\multicolumn{3}{c}{\thead{K=50}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule

HA   & .214$\pm$0 & .332$\pm$0 &  .502$\pm$0 &  .225$\pm$0 &  .322$\pm$0 &  .497$\pm$0 &  .235$\pm$0  &  .316$\pm$0 &  .493$\pm$0 \\
LSTM   & .215$\pm$2\tiny\textperthousand *& .392$\pm$2\tiny\textperthousand *&  .519$\pm$3\tiny\textperthousand *&  .225$\pm$1\tiny\textperthousand *&  .380$\pm$2\tiny\textperthousand *&  .543$\pm$3\tiny\textperthousand *&  .249$\pm$1\tiny\textperthousand  *&  .368$\pm$2\tiny\textperthousand *&  .544$\pm$3\tiny\textperthousand *\\
ConvLSTM   & .225$\pm$5\tiny\textperthousand *& .410$\pm$4\tiny\textperthousand *&  .558$\pm$3\tiny\textperthousand *&  .236$\pm$1\tiny\textperthousand *&  .388$\pm$4\tiny\textperthousand *&  .563$\pm$8\tiny\textperthousand *&  .252$\pm$1\tiny\textperthousand  *&  .366$\pm$2\tiny\textperthousand *&  .540$\pm$8\tiny\textperthousand *\\
GSNet   & .194$\pm$1\tiny\textperthousand *& .371$\pm$2\tiny\textperthousand *&  .493$\pm$5\tiny\textperthousand *&  .201$\pm$2\tiny\textperthousand *&  .371$\pm$2\tiny\textperthousand *&  .517$\pm$5\tiny\textperthousand *&  .231$\pm$1\tiny\textperthousand  *&  .337$\pm$3\tiny\textperthousand *&  .499$\pm$3\tiny\textperthousand *\\
\tiny{Hetero-ConvLSTM}& .229$\pm$2\tiny\textperthousand *& .401$\pm$1\tiny\textperthousand *&  .557$\pm$2\tiny\textperthousand *&  .240$\pm$1\tiny\textperthousand *&  .395$\pm$4\tiny\textperthousand *&  .564$\pm$3\tiny\textperthousand *&  .255$\pm$3\tiny\textperthousand  *&  .375$\pm$2\tiny\textperthousand *&  .551$\pm$3\tiny\textperthousand *\\
HintNet   & .228$\pm$1\tiny\textperthousand *& .400$\pm$2\tiny\textperthousand *&  .555$\pm$3\tiny\textperthousand *&  .238$\pm$2\tiny\textperthousand *&  .390$\pm$3\tiny\textperthousand *&  .569$\pm$3\tiny\textperthousand *& .256$\pm$1\tiny\textperthousand  *&  .373$\pm$4\tiny\textperthousand *&  .561$\pm$8\tiny\textperthousand *\\
SpatialRank \tnote{\#}   & .250$\pm$2\tiny\textperthousand & .438$\pm$3\tiny\textperthousand &  .591$\pm$3\tiny\textperthousand &  .256$\pm$1\tiny\textperthousand &  .409$\pm$2\tiny\textperthousand &  .593$\pm$2\tiny\textperthousand &  .271$\pm$2\tiny\textperthousand  &  .394$\pm$2\tiny\textperthousand &  .585$\pm$1\tiny\textperthousand \\
SpatialRank  & \textbf{.257}$\pm$1\tiny\textperthousand & \textbf{.444}$\pm$3\tiny\textperthousand &  \textbf{.621}$\pm$4\tiny\textperthousand &  \textbf{.268}$\pm$1\tiny\textperthousand &  \textbf{.420}$\pm$1\tiny\textperthousand &  \textbf{.614}$\pm$2\tiny\textperthousand &  \textbf{.278}$\pm$3\tiny\textperthousand  &  \textbf{.403}$\pm$1\tiny\textperthousand &  \textbf{.599}$\pm$1\tiny\textperthousand \\

\hline

\multirow{1}{*}{\thead{\textbf{Chicago Crime}}} &
\multicolumn{3}{c}{\thead{K=30}} &
\multicolumn{3}{c}{\thead{K=40}} &
\multicolumn{3}{c}{\thead{K=50}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule

HA   & .237$\pm$0 & .348$\pm$0 &  .514$\pm$0 &  .250$\pm$0 &  .333$\pm$0 &  .506$\pm$0 &  .259$\pm$0  &  .322$\pm$0 &  .449$\pm$0 \\
LSTM   & .246$\pm$1\tiny\textperthousand *& .327$\pm$2\tiny\textperthousand *&  .517$\pm$3\tiny\textperthousand *&  .257$\pm$1\tiny\textperthousand *&  .329$\pm$2\tiny\textperthousand *&  .521$\pm$3\tiny\textperthousand *&  .262$\pm$3\tiny\textperthousand  *&  .314$\pm$5\tiny\textperthousand *&  .512$\pm$3\tiny\textperthousand *\\
ConvLSTM   & .313$\pm$2\tiny\textperthousand *& .415$\pm$4\tiny\textperthousand *&  .617$\pm$4\tiny\textperthousand *&  .325$\pm$2\tiny\textperthousand *&  .404$\pm$1\tiny\textperthousand *&  .607$\pm$6\tiny\textperthousand *&  .333$\pm$2\tiny\textperthousand  *&  .387$\pm$4\tiny\textperthousand *&  .599$\pm$3\tiny\textperthousand *\\
GSNet   & .283$\pm$3\tiny\textperthousand *& .388$\pm$2\tiny\textperthousand *&  .584$\pm$3\tiny\textperthousand *&  .296$\pm$2\tiny\textperthousand *&  .374$\pm$2\tiny\textperthousand *&  .565$\pm$3\tiny\textperthousand *&  .299$\pm$2\tiny\textperthousand  *&  .335$\pm$4\tiny\textperthousand *&  .568$\pm$3\tiny\textperthousand *\\
\tiny{Hetero-ConvLSTM} & .346$\pm$3\tiny\textperthousand *& .468$\pm$3\tiny\textperthousand *&  .657$\pm$4\tiny\textperthousand &  .365$\pm$1\tiny\textperthousand *&  .452$\pm$3\tiny\textperthousand*&  .642$\pm$6\tiny\textperthousand &  .374$\pm$4\tiny\textperthousand  *&  .433$\pm$4\tiny\textperthousand *&  .638$\pm$4\tiny\textperthousand *\\
HintNet   & .342$\pm$3\tiny\textperthousand *& .468$\pm$3\tiny\textperthousand *&  .661$\pm$4\tiny\textperthousand &  .358$\pm$3\tiny\textperthousand *&  .448$\pm$4\tiny\textperthousand *&  .631$\pm$6\tiny\textperthousand *&  .369$\pm$2\tiny\textperthousand  *&  .434$\pm$2\tiny\textperthousand *&  .628$\pm$4\tiny\textperthousand *\\
SpatialRank \tnote{\#}   & .361$\pm$2\tiny\textperthousand & .484$\pm$1\tiny\textperthousand &  \textbf{.670}$\pm$4\tiny\textperthousand &  .376$\pm$4\tiny\textperthousand &  .463$\pm$3\tiny\textperthousand &  \textbf{.655}$\pm$7\tiny\textperthousand &  .387$\pm$1\tiny\textperthousand  &  \textbf{.446}$\pm$1\tiny\textperthousand &  \textbf{.651}$\pm$7\tiny\textperthousand\\
SpatialRank  & \textbf{.373}$\pm$2\tiny\textperthousand & \textbf{.491}$\pm$3\tiny\textperthousand &  .665$\pm$4\tiny\textperthousand &  \textbf{.380}$\pm$2\tiny\textperthousand &  \textbf{.467}$\pm$5\tiny\textperthousand &  .647$\pm$6\tiny\textperthousand &  \textbf{.392}$\pm$2\tiny\textperthousand  &  \textbf{.446}$\pm$3\tiny\textperthousand &  .644$\pm$6\tiny\textperthousand \\

\bottomrule
\end{tabular}
\begin{tablenotes}
    \item [\#] $\beta = 0.5$
    \item \textperthousand: $\times10^{-3}$
\end{tablenotes}
\end{sc}
\end{small}
\end{center}
\end{threeparttable}
\end{table*}

\subsection{Optimization Comparison}

To evaluate the effectiveness of our proposed hybrid objective function and importance-based location sampling, we perform experiments on the same network architecture but with different optimization solutions including Cross Entropy (CE), ApproxNDCG, and SONG. The results are shown in table \ref{tab:Optimization}. Methods designed to optimize NDCG consistently perform better than Cross Entropy. SpatialRank substantially outperforms SONG and ApproxNDCG and made a noticeable improvement on L-NDCG as it is considered in the objective function. 


\begin{table*}[t]
\begin{threeparttable}[b]
\vspace{-0.0in}
\caption{Optimization Comparison}
\vspace{0mm}
\label{tab:Optimization}
\vskip 0.0in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{2.0cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}}
\toprule
\multirow{1}{*}{\thead{\textbf{Chicago Accident}}} &
\multicolumn{3}{c}{\thead{K=30}} &
\multicolumn{3}{c}{\thead{K=40}} &
\multicolumn{3}{c}{\thead{K=50}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule
CE   & .232$\pm$1\tiny\textperthousand & .424$\pm$1\tiny\textperthousand &  .588$\pm$2\tiny\textperthousand &  .253$\pm$1\tiny\textperthousand &  .415$\pm$2\tiny\textperthousand &  .588$\pm$2\tiny\textperthousand &  .266$\pm$1\tiny\textperthousand  &  .400$\pm$1\tiny\textperthousand &  .580$\pm$2\tiny\textperthousand \\
ApproxNDCG   & .240$\pm$2\tiny\textperthousand & .426$\pm$2\tiny\textperthousand &  .597$\pm$3\tiny\textperthousand &  .255$\pm$1\tiny\textperthousand &  .415$\pm$1\tiny\textperthousand &  .591$\pm$6\tiny\textperthousand &  .264$\pm$1\tiny\textperthousand  &  .398$\pm$2\tiny\textperthousand &  .575$\pm$3\tiny\textperthousand \\
SONG   & .240$\pm$1\tiny\textperthousand & .438$\pm$2\tiny\textperthousand &  .610$\pm$6\tiny\textperthousand &  .254$\pm$2\tiny\textperthousand &  .417$\pm$1\tiny\textperthousand &  .600$\pm$11\tiny\textperthousand &  .267$\pm$2\tiny\textperthousand  &  .400$\pm$4\tiny\textperthousand &  .575$\pm$1\tiny\textperthousand \\
SpatialRank & \textbf{.257}$\pm$1\tiny\textperthousand & \textbf{.444}$\pm$3\tiny\textperthousand &  \textbf{.621}$\pm$4\tiny\textperthousand &  \textbf{.268}$\pm$1\tiny\textperthousand &  \textbf{.420}$\pm$1\tiny\textperthousand &  \textbf{.614}$\pm$2\tiny\textperthousand &  \textbf{.278}$\pm$3\tiny\textperthousand  &  \textbf{.403}$\pm$1\tiny\textperthousand &  \textbf{.599}$\pm$1\tiny\textperthousand \\

\hline

\multirow{1}{*}{\thead{\textbf{Chicago Crime}}} &
\multicolumn{3}{c}{\thead{K=30}} &
\multicolumn{3}{c}{\thead{K=40}} &
\multicolumn{3}{c}{\thead{K=50}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule
CE   & .362$\pm$1\tiny\textperthousand & .475$\pm$1\tiny\textperthousand& .653$\pm$3\tiny\textperthousand & .378$\pm$1\tiny\textperthousand & .455$\pm$1\tiny\textperthousand & .646$\pm$1\tiny\textperthousand & .386$\pm$1\tiny\textperthousand & .441$\pm$2\tiny\textperthousand & .644$\pm$2\tiny\textperthousand\\
ApproxNDCG  & .344$\pm$4\tiny\textperthousand& .455$\pm$3\tiny\textperthousand & .637$\pm$10\tiny\textperthousand & .353$\pm$1\tiny\textperthousand & .435$\pm$2\tiny\textperthousand & \textbf{.658}$\pm$5\tiny\textperthousand & .365$\pm$3\tiny\textperthousand & .421$\pm$4\tiny\textperthousand   & .619$\pm$6\tiny\textperthousand   \\
SONG   & .364$\pm$2\tiny\textperthousand & .484$\pm$3\tiny\textperthousand & .660$\pm$5\tiny\textperthousand  & .379$\pm$4\tiny\textperthousand &  .466$\pm$3\tiny\textperthousand& .456$\pm$1\tiny\textperthousand   &  .390$\pm$2\tiny\textperthousand & \textbf{.450}$\pm$3\tiny\textperthousand & \textbf{.649}$\pm$6\tiny\textperthousand\\
SpatialRank  & \textbf{.373}$\pm$2\tiny\textperthousand & \textbf{.491}$\pm$3\tiny\textperthousand &  \textbf{.665}$\pm$4\tiny\textperthousand &  \textbf{.380}$\pm$2\tiny\textperthousand &  \textbf{.467}$\pm$5\tiny\textperthousand &  .647$\pm$6\tiny\textperthousand &  \textbf{.392}$\pm$2\tiny\textperthousand  &  .446$\pm$3\tiny\textperthousand &  .644$\pm$6\tiny\textperthousand \\

\bottomrule
\end{tabular}
\begin{tablenotes}
    \item \textperthousand: $\times10^{-3}$
\end{tablenotes}
\end{sc}
\end{small}
\end{center}
\end{threeparttable}
\vspace{-0.4in}
\end{table*}

\subsection{Ablation Study}
We examine the effects of tuning hyper-parameter $\sigma$ in the hybrid loss function. We present the results in table \ref{tab:Ablation}. Recall that $\sigma$ decides the ratio of L-NDCG takes in the objective function. $\sigma = 0$ means L-NDCG is not considered in the objective function. Starting from $\sigma=0$, we observe a trend that the overall performance improves steadily while $\sigma$ goes up, and it reaches the best performance at $\sigma$ equals 0.1 in the accident and crime dataset. It indicates that considering a reasonable weight of L-NDCG in the objective function boosts overall performance. These results demonstrate the importance of considering local ranking because NDCG, L-NDCG, and precision can be all improved.

\begin{table*}[t]
\begin{threeparttable}[b]
\vspace{0mm}
\caption{Ablation study}
\vspace{-0mm}
\label{tab:Ablation}
\vskip 0.0in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{2.0cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}}
\toprule
\multirow{1}{*}{\thead{\textbf{Chicago Accident}}} &
\multicolumn{3}{c}{\thead{K=30}} &
\multicolumn{3}{c}{\thead{K=40}} &
\multicolumn{3}{c}{\thead{K=50}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule
$\sigma = 0.0$   & 0.251 & 0.439 &  0.610 &  0.263 &  0.407 &  \textbf{0.612}  &  0.274  &  0.289 &  0.605 \\
$\sigma = 0.05$   & 0.239 & 0.416 &  0.559 &  0.254 &  0.394 &  0.600 &  0.266  &  0.386 &  0.597 \\
$\sigma = 0.1$  & \textbf{0.256} & 0.443 &  \textbf{0.621} &  \textbf{0.268} &  \textbf{0.421} &  0.608 &  \textbf{0.276}  &  \textbf{0.400} &  0.599 \\
$\sigma = 0.2$  & 0.250 & \textbf{0.444} &  0.616 &  0.261 &  0.411 &  0.605 &  0.271  &  0.394 &  \textbf{0.603} \\
$\sigma = 0.3$ & 0.234 & 0.422 &  0.606 &  0.243 &  0.388 &  0.59 &  0.253  &  0.374 &  0.591 \\

\hline

\multirow{1}{*}{\thead{\textbf{Chicago Crime}}} &
\multicolumn{3}{c}{\thead{K=30}} &
\multicolumn{3}{c}{\thead{K=40}} &
\multicolumn{3}{c}{\thead{K=50}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule
$\sigma = 0.0$   & 0.366 &  0.489 & 0.661 & 0.378 & 0.464 & 0.644 & 0.385  & 0.445 & 0.642 \\
$\sigma = 0.05$  & 0.366 & 0.489 & 0.659 & 0.378 & \textbf{0.467} & \textbf{0.649} & 0.382 & 0.447  & \textbf{0.643} \\
$\sigma = 0.1$   & \textbf{0.371} & \textbf{0.494} &  \textbf{0.665} &  \textbf{0.383} &  \textbf{0.467} &  0.644 &  \textbf{0.391}  &  \textbf{0.450} &  0.642 \\
$\sigma = 0.2$  &  0.356 & 0.474 & 0.654 & 0.367 & 0.448 & 0.643 & 0.372  & 0.429 & 0.623 \\
$\sigma = 0.3$ &  0.345 & 0.457 & 0.658 & 0.361 & 0.445 & 0.641 & 0.373  & 0.431 & 0.631 \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{threeparttable}
\vspace{5mm}
\end{table*}

\vspace{-0.1in}
\begin{figure}
\centering
\begin{minipage}[c]{0.28\textwidth}
\centering\includegraphics[width=1\textwidth]{chicago_30.pdf}
\end{minipage}
\begin{minipage}[c]{0.28\textwidth}
\centering\includegraphics[width=1\textwidth]{chicago_40.pdf}
\end{minipage}
\begin{minipage}[c]{0.28\textwidth}
\centering\includegraphics[width=1\textwidth]{chicago_50.pdf}
\end{minipage}
\caption{Comparison of cross-K function in Chicago Accident.}
\label{fig:chicago}
\vspace{-1mm}
\end{figure}


\subsection{Cross-K function}

We use the Cross-K function\cite{doi:https://doi.org/10.1002/9781118445112.stat07751} with Monte Carlo Simulation\cite{TaoRan2019FCKa} to evaluate the accuracy of predicted locations. The Cross-K function measures the spatial correlation between the predicted locations and true locations. Specifically, we calculate the average density of predictions within every distance $d$ of a true event in each day as shown in Eq. \ref{eqn:crossk}:

\vspace{-0.1in}
\begin{equation} \label{eqn:crossk}
\hat{K}(d) =  \lambda_j^{-1}\sum_{i \neq j}I(d_{ij} \leq d)/n,
\end{equation}

\vspace{-0.1in}
\noindent where $\lambda$ is global density of event $j$, and $I()$ is an identity function which equals one if real distance $d_{ij}$ is smaller than $d$, else equals zero. $n$ is the number of events $i$. The results are shown in Figure \ref{fig:chicago}. The grey curve represents the complete spatial randomness and we use it as a reference baseline. Higher is better. Our SpatialRank achieves the best predictions in both datasets, which indicates that the predictions of SpatialRank are significantly spatially correlated with ground truth. The similar results on the other two datasets are shown in Appendix C in the supplementary materials. % The result differences between baselines tend to be larger in the Chicago dataset and smaller in the Iowa dataset. It follows a similar pattern as shown in the performance comparison part. The reason might be that there exist more complicated accident patterns in the Chicago area, thus it becomes more challenging to capture such variation. Our SpatialRank is able to adapt to different datasets and has stable performance.

\textbf{Additional Results} and a case study to demonstrate successful prediction examples are included in the Supplementary materials. We also include our code and sample data with the supplementary materials.
%We present a successful prediction on Chicago using SpatialRank on Jan $25^{th}$ 2021 in Figure \ref{fig: case}. There was a severe winter storm in the area of Chicago and $147$ people were injured \cite{chicagoweather}. We compare predictions of SpatialRank with ground truth, Hetero-ConvLSTM, and LSTM. The top $50$ riskiest locations in the predictions are chosen and labeled on the map. For easier visualization, the number of events greater than three in the ground truth map is changed to three, representing the riskiest location. To understand how those methods prioritize the riskiest locations, We define locations with events great than 1 as high-risk areas. The blue circle indicates this high-risk location is predicted correctly. Missing High-risk location is indicated as an orange circle. We can observe that SpatialRank captures most High-risk locations, while others are not able to find the potential risk nearby the downtown area. As a result, the overall NDCG score is improved in SpatialRank. The case study on crime ranking is in Appendix C.

% \begin{figure*}[t]
%  \centering
%  \includegraphics[width = 0.8 \textwidth]{casestudy2.pdf}
%  \caption{Case Study on Chicago Jan. $25^{th}, 2021.$}
%  \label{fig: case}
% \end{figure*}

\section{Related Work}
\textbf{Urban Event forecasting} has been widely studied in the past few decades. %Previous works formulate a classification problem to predict the occurrence of an event \cite{ABELLAN20136047} or a regression problem to predict the number of events \cite{ABDELATY2000633} in a location. 
Most early studies \cite{Chong2004TrafficAA}~\cite{Lin2015}~\cite{barba2014smoothing}\cite{KhezerlouAminVahedian2021DPUD} rely on small-scale and single-source datasets with limited types of features, thus the prediction accuracy is limited. %As the urban infrastructure continues to evolve, the availability of massive traffic, weather, and human activities data brings unprecedented prospects for enhancing the precision of predictive modeling. Nevertheless, most classic machine-learning methods have difficulty in capturing complex patterns with the emerging abundant spatial-temporal datasets. Recently, deep learning approaches have demonstrated their potential in solving event forecasting problems with large-scale spatiotemporal datasets. Particularly, long short-term memory (LSTM) \cite{Ren2018ADL}, attention-based\cite{10.1145/3357384.3357829}, and autoencoder-based\cite{Chen2018SDCAESD} models are applied to capture temporal patterns. Besides, convolution neural network (CNN) \cite{najjar2017combining} is applied to capture spatial dependencies. % Although applying existing deep learning methods to event forecasting problems has shown some improvements, their prediction accuracy is limited by the unique pattern of traffic accidents including unbalanced data and heterogeneous spatial distribution.
Notably, Zhou et al. \cite{9242313} proposed a differential time-varying graph convolution network capturing traffic changes and improving prediction accuracy. Similarly, Wang et al. \cite{Wang2021gsnet} proposed GSNet with a geographical module and a weighted loss function to capture semantic spatial-temporal correlations among regions and solve data sparsity issues. %Besides, \cite{HuangChao2018DAHR} proposed DeepCrime to fuse spatiotemporal information by attention mechanism. 
Addressing the issue of spatial heterogeneity, Yuan et al. \cite{yuan2018hetero} proposed Hetero-ConvLSTM to leverage an ensemble of predictions from models learned from pre-selected sub-regions. Furthermore, An et al. \cite{doi:10.1137/1.9781611977172.38} proposed HintNet partitions the study area hierarchically and transfers learned knowledge over different regions to improve performance. However, most existing models rely on optimizing cross-entropy and the objective is to make accurate predictions on every location. % Differently, \cite{10.1145/2433396.2433485} focuses on spatial learning to rank a user's location from GPS data. Besides, \cite{MohlerGeorge2020Ltrs} proposed CrimeRank to optimize the predictive accuracy index to measure the top-k hotspots. However, both methods do not directly optimize NDCG and cannot work with popular stochastic optimizers such as ADAM \cite{KingmaDiederikP2014AAMf} or SDG \cite{ruder2016overview} thus their performance and application potential is limited.
% \textbf{Learning to rank and NDCG optimization}. 
\textbf{Learning to rank} is an extensively studied area in recommendation systems and search engines \cite{liu2011learning}. NDCG is a widely adopted metric to measure ranking quality. The prominent class of methods involves approximating ranks in NDCG using smooth functions and subsequently optimizing the resultant surrogates. Taylor et al. \cite{SoftRank} tries to use rank distributions to smooth NDCG, but suffers from high computational cost. Similarly, Qin et al. \cite{ApproxNDCG} approximates the indicator function by a generalized sigmoid function with a top-k variant. Noticeably, Qiu et al. \cite{Qiu2022LargescaleSO} develop stochastic algorithms optimizing the surrogates for NDCG and its top-K variant. Although it is possible to directly apply the existing ranking methods to the urban event ranking problem, the performance tends to be unsatisfactory as these methods commonly assume independence between items and queries and lack the ability to handle spatiotemporal autocorrelation in the data. %According to the first law of geography \cite{ToblerW.R.1970ACMS}, near things are more related than distant things. previous studies on spatial-temporal problems suggest the necessary of considering spatial auto-correlations during modeling. 
%In this paper, we propose a novel deep neural network model, SpatialRank, to learn from a spatiotemporal dataset and rank future top-k locations based on their risk or intensity while optimizing the ranking quality.


\section{Conclusion and Limitations}

In this work, we formulate event forecasting as a location ranking problem. We propose a novel method SpatialRank to learn from spatiotemporal data by optimizing NDCG surrogates. To capture dynamic spatial correlations, we design an adaptive graph convolution layer to learn the graph from features. Furthermore, we propose a hybrid loss function to capture potential risks around hot-spot regions, and a novel ranking-based importance sampling mechanism to leverage the importance of each location considered during the model training. Extensive experimental results on three real-world datasets demonstrate the superiority of SpatialRank compared to baseline methods.

\textbf{Limitations}: the model's performance might be affected by other properties of data, such as spatial heterogeneity and sparsity. We observe less improvement over baselines on the Iowa dataset, partially due to that the data is sparser over a large area with heterogeneity. These are issues addressed by some of the prior work and can be addressed in our future work. In addition, the new algorithm increases the training time complexity due to the sampling steps. This is acceptable due to the relatively small number of locations and time periods in urban event datasets but may require extra work to generalize to large datasets.








































\begin{ack}
T. Yang was partially supported by NSF Career Award 18844403, NSF Fair AI Grant 2147253, NSF RI Grant 2110545.
\end{ack}



\medskip


\small

\bibliographystyle{abbrv}
\bibliography{reference}

% \bibliographystyle{neurips_2023}

\clearpage

\section{Appendix A Feature Engineering}

In this section, we explain how we generate features. Features are generated on partitioned grid cells and at different time intervals. \\
\textbf{Temporal Features} $F_T$ Such calendar features and Weather features are generated from the date of Vehicle Crash or Crime Records, where all grid cells share a vector of temporal features in a time interval. calendar features include the day of the year, the month of the year, holidays, and so on. Weather features include temperature, precipitation, snowfall, wind speed, etc.  \\
\textbf{Spatial Features} $F_S$ are generated based on each grid cell and remain the same over different time intervals. First, POI features are the number of POI data in each grid cell for different categories. For example, one of the POI types is shopping, we count the number of shopping instances in each grid cell. Second, basic road condition features are extracted from road network data, in which we calculate the summation or average of provided data for road segments in each grid cell. Third, we use top eigenvectors of the Laplacian matrix of road networks as spatial graph features\cite{yuan2018hetero}, which represent the topological information for each grid cell. \\
\textbf{Spatio-Temporal Features} $F_{ST}$ such as real-time traffic conditions are estimated by taxi GPS data and Bus GPS data. Spatio-temporal features include pick-up volumes, drop-off volumes, traffic speed, etc.

\textbf{Feature Summary} In total, 36 features are extracted, including 12 temporal features, 18 spatial features, and 6 spatial-temporal features for each location $s$ and time interval $t$.

\section{Appendix B  Methodology}

\subsection{Symbol Table}
\setlength{\extrarowheight}{.5em}
\begin{tabular}{ |p{2cm}|p{9cm}|  }
\hline
\multicolumn{2}{|c|}{Symbol Table} \\
\hline
Symbol & Explainations \\

\hline
$\S$ & Spatial filed, study area \\
$s$  &  A partitioned location, grid cell   \\
$T$  &    Temporal filed, study period \\
$t$  &    Time interval (e.g. hours, days) \\
$F_T$  & Temporal features (weather, time)    \\
$F_S$  &   Spatial features (e.g. POI) \\
$F_ST$  &    Spatiotemporal features (e.g. traffic conditions)\\
$Z$  &    Discounted Cumulative Gain (DCG) score \\ 
$a$  &   Pearson correlation coefficient\\ 
$\text{NDCG}$  &  Normalized Discounted Cumulative Gain    \\
$\text{L-NDCG}$  &  Local Normalized Discounted Cumulative Gain   \\
$\text{Prec}$  &  top-K precision   \\
$\text{r}()$  & Ranking function    \\
$\N$  &   Neighbour querying  \\
\hline
\end{tabular}
\setlength{\extrarowheight}{-.5em}

\section{Appendix C Experiments}
\textbf{Parameter Configuration.} For each method, we train the network for 100 epochs and save the model with the best performance on validating set. We use the Adam optimizer \cite{KingmaDiederikP2014AAMf} with settings $\alpha=0.0001$, $\beta_1=0.9$, $\beta_2=0.999$, and $\epsilon=10^{-8}$. We tune trade-off hyperparameter $\sigma$ with value $0$, $0.05$, $0.1$, $0.2$, and $0.3$. The $R$ in the spatial neighbor definition is set as 2 grid cells for computation efficiency. We use an initial warm-up strategy by optimizing cross-entropy at the first 20 epochs to obtain a good initial solution because merely optimizing NDCG might land in the local minimum if a poor initial solution is given. The same warm-up process is also applied to optimization baselines. 
The learning rate is set at 0.001 in the warm-up and changed to 0.0001 afterward. The batch size is 64. 

\textbf{Data.} For the state of Iowa, we collect data from 2016 to 2018. $80\%$ of data from the year 2016 and year 2017 is randomly selected as a training set, and the remaining data is used as the validating set. The data from the year of 2018 is used as the testing set. The area is partitioned by 5 km $\times$ 5 km cells. The grid size is $128 \times 64$. Normalization is used to transform data into the range $[0, 1]$. 

\textbf{Platform.} 
We run the experiments on a High-Performance Computer system where each node runs has an Intel Xeon E5 2.4 GHz and 256 GB of Memory. We use a GPU node with Nvidia Tesla V100 Accelerator Cards with the support of Pytorch library \cite{NEURIPS2019_9015} to train the deep learning models. Our code is available at: https://github.com/BANG23333/SpatialRank

\textbf{Baselines.} First, we compare our methods with daily \textbf{Historical Average (HA)}. Next, we consider popular machine-learning methods. \textbf{Long Short-term Memory (LSTM)} \cite{HochSchm97} is a recurrent neural network architecture with feedback connections, we stack two fully-connected LSTM layers. \textbf{Convolutional LSTM Network (ConvLSTM)} \cite{ShiXingjian2015CLNA} is a recurrent neural network with convolution layers for spatial-temporal prediction. We use a stacked two-layer network. Thirdly, we compare with recent methods designed to tackle the traffic accident occurrence prediction problem. \textbf{GSNet} \cite{Wang2021gsnet} is a deep-learning method utilizing complicated graph information. \textbf{Hetero-ConvLSTM} \cite{yuan2018hetero} is an advanced deep-learning framework to address spatial heterogeneity. It applies multiple ConvLSTM on pre-defined sub-regions with size $32\times32$. We use the same parameter setting in the experiments. \textbf{HintNet} \cite{doi:10.1137/1.9781611977172.38} is a recent work capturing heterogeneous accident patterns by a hierarchical-structured learning framework. Finally, using our proposed network, we compared our optimization approach with other NDCG optimization solutions. \textbf{Cross Entropy (CE)} is a well-accepted objective function. \textbf{ApproxNDCG} \cite{ApproxNDCG} approximates the indicator function in the computation of ranks. \textbf{SONG} \cite{Qiu2022LargescaleSO} is an efficient stochastic method to optimize NDCG.

\subsection{Performance Comparison}
We compare the results between SpatialRank and the baselines on the Iowa traffic accident dataset and observe that our full version of SpatialRank still outperforms other compared baselines in all the three metrics. The results are shown in Table~\ref{tab:performance}. SpatialRank$^\#$ with $\beta$ pre-defined (as 0.5) rather than automatically learned is the second best. Consistent with the results on the other two datasets, SparkRank outperforms all the baselines and the dynamic convolution layers are effective in improving the model's performance. 

\begin{table*}[t]
\begin{threeparttable}[b]
\vspace{-0.0in}
\caption{Performance Comparison}
\vspace{-0mm}
\label{tab:performance}
\vskip 0.0in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{2.0cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}}
\toprule

\multirow{1}{*}{\thead{\textbf{Iowa}}} &
\multicolumn{3}{c}{\thead{K=30}} &
\multicolumn{3}{c}{\thead{K=40}} &
\multicolumn{3}{c}{\thead{K=50}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule

HA   & .314$\pm$0 & .167$\pm$0 &  .326$\pm$0 &  .326$\pm$0 &  .134$\pm$0 &  .261$\pm$0 &  .333$\pm$0  &  .111$\pm$0 &  .231$\pm$0 \\
LSTM   & .503$\pm$3\tiny\textperthousand *& .278$\pm$3\tiny\textperthousand *&  .573$\pm$3\tiny\textperthousand *&  .522$\pm$1\tiny\textperthousand *&  .209$\pm$1\tiny\textperthousand *&  .518$\pm$3\tiny\textperthousand *&  .519$\pm$5\tiny\textperthousand  *&  .187$\pm$1\tiny\textperthousand *&  .474$\pm$1\tiny\textperthousand *\\
ConvLSTM   & .490$\pm$3\tiny\textperthousand *& .282$\pm$2\tiny\textperthousand *&  .583$\pm$1\tiny\textperthousand *&  .507$\pm$3\tiny\textperthousand *&  .207$\pm$1\tiny\textperthousand *&  .513$\pm$4\tiny\textperthousand *&  .511$\pm$3\tiny\textperthousand  *&  .189$\pm$3\tiny\textperthousand *&  .474$\pm$8\tiny\textperthousand *\\
GSNet   & .493$\pm$2\tiny\textperthousand *& .265$\pm$1\tiny\textperthousand *&  .569$\pm$3\tiny\textperthousand *&  .509$\pm$3\tiny\textperthousand *&  .222$\pm$3\tiny\textperthousand *&  .527$\pm$3\tiny\textperthousand *&  .526$\pm$5\tiny\textperthousand  *&  .207$\pm$1\tiny\textperthousand *&  .510$\pm$3\tiny\textperthousand *\\
\tiny{Hetero-ConvLSTM}& .518$\pm$1\tiny\textperthousand *& .289$\pm$2\tiny\textperthousand *&  .617$\pm$4\tiny\textperthousand &  .523$\pm$5\tiny\textperthousand *&  .258$\pm$1\tiny\textperthousand *&  .589$\pm$5\tiny\textperthousand *&  .543$\pm$3\tiny\textperthousand  *&  .226$\pm$1\tiny\textperthousand *&  .534$\pm$5\tiny\textperthousand *\\
HintNet   & .512$\pm$5\tiny\textperthousand & .289$\pm$3\tiny\textperthousand *&  .617$\pm$1\tiny\textperthousand &  .542$\pm$5\tiny\textperthousand &  .243$\pm$4\tiny\textperthousand *&  .590$\pm$9\tiny\textperthousand &  .556$\pm$3\tiny\textperthousand  *&  .209$\pm$2\tiny\textperthousand *&  .534$\pm$8\tiny\textperthousand*\\
SpatialRank \tnote{\#}   & .530$\pm$3\tiny\textperthousand & .300$\pm$2\tiny\textperthousand &  .617$\pm$4\tiny\textperthousand &  .556$\pm$3\tiny\textperthousand &  .264$\pm$2\tiny\textperthousand &  .594$\pm$3\tiny\textperthousand &  .571$\pm$3\tiny\textperthousand  &  .223$\pm$1\tiny\textperthousand &  \textbf{.552}$\pm$4\tiny\textperthousand \\
SpatialRank  & \textbf{.540}$\pm$3\tiny\textperthousand & \textbf{.309}$\pm$2\tiny\textperthousand &  \textbf{.618}$\pm$3\tiny\textperthousand &  \textbf{.563}$\pm$6\tiny\textperthousand &  \textbf{.268}$\pm$3\tiny\textperthousand &  \textbf{.600}$\pm$3\tiny\textperthousand &  \textbf{.585}$\pm$3\tiny\textperthousand  &  \textbf{.232}$\pm$2\tiny\textperthousand & .550$\pm$6\tiny\textperthousand \\

\bottomrule
\end{tabular}
\begin{tablenotes}
    \item [\#] $\beta = 0.5$
    \item \textperthousand: $\times10^{-3}$
\end{tablenotes}
\end{sc}
\end{small}
\end{center}
\end{threeparttable}
\end{table*}

\subsection{Optimization Comparison}
 We perform the comparison of optimization methods on the Iowa traffic accident data. We use the same network architecture but with different optimization solutions including Cross Entropy (CE), ApproxNDCG, and SONG. The results are shown in Table \ref{tab:Optimization}. Methods designed to optimize NDCG consistently perform better than Cross Entropy. SpatialRank substantially outperforms SONG and ApproxNDCG and made a noticeable improvement on L-NDCG as it is considered in the objective function. 



\begin{table*}[t]
\begin{threeparttable}[b]
\vspace{-0.0in}
\caption{Optimization Comparison}
\vspace{0mm}
\label{tab:Optimization}
\vskip 0.0in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{2.0cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}}
\toprule

\multirow{1}{*}{\thead{\textbf{Iowa}}} &
\multicolumn{3}{c}{\thead{K=30}} &
\multicolumn{3}{c}{\thead{K=40}} &
\multicolumn{3}{c}{\thead{K=@50}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule
CE   & .531$\pm$4\tiny\textperthousand & .306$\pm$4\tiny\textperthousand &  .554$\pm$2\tiny\textperthousand &  .555$\pm$1\tiny\textperthousand &  .246$\pm$2\tiny\textperthousand &  .556$\pm$2\tiny\textperthousand &  .548$\pm$2\tiny\textperthousand  &  .205$\pm$1\tiny\textperthousand &  .502$\pm$1\tiny\textperthousand \\
ApproxNDCG   & .528$\pm$4\tiny\textperthousand & .304$\pm$3\tiny\textperthousand &  .561$\pm$1\tiny\textperthousand &  .551$\pm$3\tiny\textperthousand &  .250$\pm$4\tiny\textperthousand &  .557$\pm$6\tiny\textperthousand &  .554$\pm$6\tiny\textperthousand  &  .212$\pm$2\tiny\textperthousand &  .508$\pm$3\tiny\textperthousand \\
SONG   & .529$\pm$3\tiny\textperthousand & .308$\pm$2\tiny\textperthousand &  \textbf{.618}$\pm$1\tiny\textperthousand &  \textbf{.563}$\pm$3\tiny\textperthousand &  .264$\pm$4\tiny\textperthousand &  .584$\pm$1\tiny\textperthousand &  .581$\pm$3\tiny\textperthousand  &  .227$\pm$4\tiny\textperthousand &  .536$\pm$6\tiny\textperthousand \\
SpatialRank  & \textbf{.540}$\pm$3\tiny\textperthousand & \textbf{.309}$\pm$2\tiny\textperthousand &  \textbf{.618}$\pm$3\tiny\textperthousand &  \textbf{.563}$\pm$6\tiny\textperthousand &  \textbf{.268}$\pm$3\tiny\textperthousand &  \textbf{.600}$\pm$3\tiny\textperthousand &  \textbf{.585}$\pm$3\tiny\textperthousand  &  \textbf{.232}$\pm$2\tiny\textperthousand &  \textbf{.550}$\pm$6\tiny\textperthousand \\

\bottomrule
\end{tabular}
\begin{tablenotes}
    \item \textperthousand: $\times10^{-3}$
\end{tablenotes}
\end{sc}
\end{small}
\end{center}
\end{threeparttable}
\vspace{-0.4in}
\end{table*}

\subsection{Computation Cost Comparison}

We conduct comparisons with SOTA methods on average training time in seconds per epoch and inference time on the testing dataset in Table \ref{tab:cost}. The Chicago crime dataset and the Chicago accident dataset have the same input feature; thus, have equivalent training costs. In summary, SpatialRank trains faster than two SOTA baselines HintNet and GSNet on both datasets. It is only slower than HeteroConvLSTM but the training times of the two are on the same order of magnitude. The training phase of SpatialRank is slow because of computing nested L-NDCG loss function. Without extra cost on proposed optimization techniques, SpatialRank is significantly faster than all baselines in the inference phase. Given the improvement in prediction performance, we believe the cost of training time is acceptable, which will not affect the predicting efficiency of the proposed method.

\begin{table*}
\begin{threeparttable}[b]
\vspace{0mm}
\caption{Computation Cost}
\vspace{0mm}
\label{tab:cost}
\vskip 0.0in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{2.2cm}p{2.0cm}p{2.0cm}p{2.0cm}p{2.0cm}}
\toprule

\multirow{1}{*}{\thead{\textbf{Training Time}}} &
\multicolumn{4}{c}{\thead{Cost in seconds}} \\
\cmidrule(lr){2-5}

& \footnotesize{SpatialRank} & \footnotesize{HintNet} & \tiny{HeteroConvLSTM} & \footnotesize{GSNet} \\
\midrule
\textbf{Chicago}   & 88.2 & 132.1 &  \textbf{47.7} &  98.8   \\
\textbf{Iowa}   & 76.5 & 117.5 &  \textbf{41.6} &  83.5   \\

\multirow{1}{*}{\thead{\textbf{Inference Time}}} &
\multicolumn{4}{c}{\thead{}} \\
\cmidrule(lr){2-5}

& \footnotesize{SpatialRank} & \footnotesize{HintNet} & \tiny{HeteroConvLSTM} & \footnotesize{GSNet} \\
\midrule
\textbf{Chicago}   & \textbf{5.6} &  51.2 &  41.4 &  21.1  \\
\textbf{Iowa}   & \textbf{5.1} &  41.7 & 12.3 & 16.2   \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{threeparttable}
\end{table*}

\subsection{Ablation study}

We perform an ablation study on the parameter $\sigma$ on the Iowa traffic accident dataset. The results are shown in Table~\ref{tab:Ablation}. For K=30, 40, and 50, $\sigma$ = $0.05$ always gives the best performance. For K=50 there is a tie in Precision@K between $\sigma = 0.05$ and $\sigma = 0.1$. Compared with the Chicago datasets, the best $\sigma$ changed from 0.1 to 0.05, suggesting that local ranking might be less challenging in the Iowa data. This makes sense as the Iowa data has a coarser resolution (5km), making it easier to separate potential hotspots from surrounding grid cells. 

\begin{table*}[t]
\begin{threeparttable}[b]
\vspace{0in}
\caption{ablation study on weighting}
\vspace{0mm}
\label{tab:aa}
\vskip 0.0in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{2.0cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}}
\toprule
\multirow{1}{*}{\thead{\textbf{Chicago Accident}}} &
\multicolumn{3}{c}{\thead{K=30}} &
\multicolumn{3}{c}{\thead{K=40}} &
\multicolumn{3}{c}{\thead{K=50}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule
no-weight   & 0.255 & 0.441 &  \textbf{0.622} &  0.265 &  0.417 &  0.613 &  0.274  &  0.401 &  0.595 \\
SpatialRank & \textbf{0.257} & \textbf{0.444} &  0.621 &  \textbf{0.268} &  \textbf{0.420} &  \textbf{0.614} &  \textbf{0.278}  &  \textbf{0.403} &  \textbf{0.599} \\

\hline

\multirow{1}{*}{\thead{\textbf{Iowa}}} &
\multicolumn{3}{c}{\thead{}} &
\multicolumn{3}{c}{\thead{}} &
\multicolumn{3}{c}{\thead{}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule
no-weight   & 0.531 & 0.304 &  0.617 &  0.557 &  0.264 &  0.591 &  0.573  &  0.225 &  0.546 \\
SpatialRank & \textbf{0.540} & \textbf{0.309} &  \textbf{0.618} &  \textbf{0.563} &  \textbf{0.268} &  \textbf{0.600} &  \textbf{0.585}  &  \textbf{0.232} &  \textbf{0.550} \\

\hline

\multirow{1}{*}{\thead{\textbf{Chicago Crime}}} &
\multicolumn{3}{c}{\thead{}} &
\multicolumn{3}{c}{\thead{}} &
\multicolumn{3}{c}{\thead{}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule
no-weight   & 0.364 & 0.484 & 0.660  & 0.379 &  0.466& 0.642   &  0.390 & \textbf{0.450} & \textbf{0.649}\\
SpatialRank  & \textbf{0.373} & \textbf{0.491} &  \textbf{0.665} &  \textbf{0.380} &  \textbf{0.467} &  \textbf{0.647} &  \textbf{0.392}  &  0.446 &  0.644 \\

\hline

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{threeparttable}
\vspace{-0.4in}
\end{table*}

\begin{table*}
\begin{threeparttable}[b]
\vspace{0mm}
\caption{Ablation study}
\vspace{0mm}
\label{tab:Ablation}
\vskip 0.0in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{p{2.0cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}p{0.9cm}}
\toprule

\multirow{1}{*}{\thead{\textbf{Iowa}}} &
\multicolumn{3}{c}{\thead{K=30}} &
\multicolumn{3}{c}{\thead{K=40}} &
\multicolumn{3}{c}{\thead{K=50}} \\
\cmidrule(lr){2-4}
\cmidrule(lr){5-7}
\cmidrule(lr){8-10}

& \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG} & \footnotesize{NDCG} & \footnotesize{Prec} & \tiny{L-NDCG}  \\
\midrule
$\sigma = 0.0$   & 0.532 & 0.305 &  0.610 &  0.560 &  0.261 &  0.597 &  0.578  &  0.228 &  0.554 \\
$\sigma = 0.05$   & \textbf{0.539} & \textbf{0.311} &  \textbf{0.626} &  0.559 &  \textbf{0.267} &  \textbf{0.597} &  \textbf{0.579}  &  \textbf{0.231} &  \textbf{0.560} \\
$\sigma = 0.1$  & 0.532 & 0.307 &  0.603 &  \textbf{0.560} &  0.264 &  0.572 &  0.578  &  \textbf{0.229} &  0.555 \\
$\sigma = 0.2$  & 0.528 & 0.303 &  0.585 &  0.561 &  0.262 &  0.566 &  0.580  &  0.227 &  0.526 \\
$\sigma = 0.3$ & 0.523 & 0.300 &  0.579 &  0.559 &  0.263 &  0.572 &  0.580  &  0.227 &  0.530 \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\end{threeparttable}
\end{table*}

%\subsection{cross-k}
\begin{figure*}
 \centering
 \includegraphics[width = 0.8 \textwidth]{casestudy.pdf}
 \caption{Case Study on Chicago Jan. $25^{th}, 2021.$}
 \label{fig: case}
\end{figure*}

\begin{figure}
\centering
\begin{minipage}[c]{0.3\textwidth}
\centering\includegraphics[width=1\textwidth]{crime_30.pdf}
\end{minipage}
\begin{minipage}[c]{0.3\textwidth}
\centering\includegraphics[width=1\textwidth]{crime_40.pdf}
\end{minipage}
\begin{minipage}[c]{0.3\textwidth}
\centering\includegraphics[width=1\textwidth]{crime_50.pdf}
\end{minipage}
\caption{Comparison of Cross-K function for Chicago Crime.}
\label{fig:chicago_crime}
\vspace{-2mm}
\end{figure}

\begin{figure}
\centering
\begin{minipage}[c]{0.3\textwidth}
\centering\includegraphics[width=1\textwidth]{iowa_30.pdf}
\end{minipage}
\begin{minipage}[c]{0.3\textwidth}
\centering\includegraphics[width=1\textwidth]{iowa_40.pdf}
\end{minipage}
\begin{minipage}[c]{0.3\textwidth}
\centering\includegraphics[width=1\textwidth]{iowa_50.pdf}
\end{minipage}
\caption{Comparison of Cross-K function for Iowa accident data.}
\label{fig:iowa}
\vspace{-2mm}
\end{figure}

\subsection{Cross-K function}

We use the Cross-K function\cite{doi:https://doi.org/10.1002/9781118445112.stat07751} with Monte Carlo Simulation\cite{TaoRan2019FCKa} to evaluate the accuracy of predicted locations. The Cross-K function measures the spatial correlation between the predicted accident locations and true locations in our case. Specifically, we calculate the average density of predictions within every distance $d$ of a true event in each day as shown in Eq.\ref{eqn:crossk}:


\vspace{-0.1in}
\begin{equation} \label{eqn:crossk}
\hat{K}(d) =  \lambda_j^{-1}\sum_{i \neq j}I(d_{ij} \leq d)/n,
\end{equation}

\vspace{-0.1in}
\noindent where $\lambda$ is global density of event $j$, and $I()$ is an identity function which equals one if real distance $d_{ij}$ is smaller than $d$, else equals zero. $n$ is the number of events $i$. The results on the Chicago crime data and Iowa accident datasets are shown in Figure \ref{fig:chicago_crime} and Figure~\ref{fig:iowa}, respectively. The grey curve represents the complete spatial randomness and we use it as a reference baseline. Higher curves are better. Our SpatialRank (blue) achieves the best predictions in both datasets, which indicates that the predictions of SpatialRank are significantly spatially correlated with ground truth. % The result differences between baselines tend to be larger in the Chicago dataset and smaller in the Iowa dataset. It follows a similar pattern as shown in the performance comparison part. The reason might be that there exist more complicated accident patterns in the Chicago area, thus it becomes more challenging to capture such variation. Our SpatialRank is able to adapt to different datasets and has stable performance.


\subsection{Case Study}
We present a successful prediction on Chicago using SpatialRank on Jan $25^{th}$ 2021 in Figure \ref{fig: case}. There was a severe winter storm in the area of Chicago and $147$ people were injured \cite{chicagoweather} and 99 severe crimes occurred. We compare predictions of SpatialRank with ground truth, Hetero-ConvLSTM, and LSTM. The top $50$ riskiest locations in the predictions are chosen and labeled on the map. For easier visualization, the number of events greater than three in the ground truth map is changed to three, representing the riskiest location. To understand how those methods prioritize the riskiest locations, We define locations with events great than 1 as high-risk areas. The blue circle indicates this high-risk location is predicted correctly. Missing High-risk location is indicated as an orange circle. We can observe that SpatialRank captures most High-risk locations, while others are not able to find the potential risk nearby the downtown area. As a result, the overall NDCG score is improved in SpatialRank.



\end{document}