\input{float/figure-model}

\subsection{Sentence-level Event Extraction}
Previous studies mainly focus on sentence-level event extraction.
\citet{li-etal-2014-constructing} and \citet{judea-strube-2016-incremental} use handcrafted features to extract events from the sentence. \citet{chen-etal-2015-event} firstly propose a neural pipeline model to extract events, while \citet{nguyen-etal-2016-joint} utilize a joint model to mitigate error propagation.
To better model the interactions among  words, \citet{liu-etal-2018-jointly, yan-etal-2019-event, ma-etal-2020-resource} make use of the dependency tree, and \citet{wadden-etal-2019-entity} enumerates all possible spans and propagate information in the span graph.
Data augmentation is also considered~\citep{yang-etal-2019-exploring-pre}.
Moreover, some works try to reformulate the event extraction task as other tasks.
For example, \citet{du-cardie-2020-event} and \citet{DBLP:conf/aaai/Zhou0ZWXL21} cast event extraction as question answering, and \citet{DBLP:journals/corr/abs-2107-00189} model it as a sequence-to-sequence task.
However, all of these models can only extract events from a single sentence.
Thus, they fail to handle the much more common cases, where event arguments usually spread over multiple sentences within the document.

\subsection{Document-level Event Extraction}

In order to extract events from a whole piece of article with multiple sentences, document-level event extraction has attracted more and more attention recently.
\citet{yang-mitchell-2016-joint} utilize well-defined features to extract arguments across sentences, while most recent methods are based on neural networks.
Some studies first identify entities in the document, followed by assigning these entities as specific argument roles~\citep{yang-etal-2018-dcfee, zheng-etal-2019-doc2edag, xu-etal-2021-git}.
Differently, some studies try to jointly extract entities and argument roles simultaneously, which can be further divided into tagging-based and span-based methods.
Tagging-based methods directly conduct sequence labeling for each token in the document with BIO-schema~\citep{du-cardie-2020-document, DBLP:conf/pakdd/VeysehDTMWJKCN21}, while span-based methods predict the argument role for candidate text spans which usually have a maximum length limitation~\citep{rams, two-step}.
Another line of studies reformulate the task as a sequence-to-sequence task~\citep{du-etal-2021-grit,du-etal-2021-template,wikievent}, or machine reading comprehension task~\citep{wei-etal-2021-trigger}.

As a span-based method, \modelname is different from prior methods that simply encode it as a long sentence.
Instead, \modelname introduces a two-stream encoding module and AMR-guided interactions module to model intra-sentential and inter-sentential semantics, along with an auxiliary boundary loss to enhance span boundary information.

