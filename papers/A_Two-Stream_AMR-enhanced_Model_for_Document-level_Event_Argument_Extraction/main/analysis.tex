\subsection{Cross-sentence Argument Extraction}
\input{float/cross-rams}
Since there are multiple sentences in the document, some event arguments are located far away from the trigger, which highly increases the difficulty of extraction.
To explore the effect of handling such cross-sentence arguments of our \modelname, we divide the event arguments in RAMS dataset into five bins according to the sentence distance between arguments and trigger, i.e., $d=\left \{ -2, -1, 0, 1, 2 \right \} $.
We report the Span F1 on the RAMS dev set for different methods.
As shown in Table~\ref{table:cross-rams}, the Span F1 for cross-sentence arguments ($d\neq0$) is much lower than local arguments ($d=0$), suggesting the huge challenge to capture long-distance dependency between triggers and cross-sentence arguments.
However, \modelname still surpasses other strong baselines.
In detail, \modelnamebase improves $0.4$ and \modelnamelarge improves $0.7$ F1 compared with the previous state-of-the-art, respectively.
More importantly, when extracting cross-sentence arguments, \modelnamebase and \modelnamelarge yield an improvement of up to $2.3$ and $2.7$ on average.
The results support our claims that \modelname is good at capturing both intra-sentential and inter-sentential features, especially the long-distance between trigger and arguments.

\input{float/ablation-rams-large}

\input{float/figure-error-analysis}
\input{float/figure-case-study}

\subsection{Ablation Study}

We conduct an ablation study to explore the effectiveness of different modules in \modelname.
Table~\ref{table:ablaition-rams-large} show the results on RAMS datasets for \modelnamelarge.
We also provide results for \modelnamebase, and those on WikiEvents datasets in Appendix~\ref{sec:appendix-ablation}.

Firstly, we remove the global or local encoder in the two-stream encoding module.
As shown in Table~\ref{table:ablaition-rams-large}, the removal causes drop in performance, e.g., $3.04$ and $1.71$ Head F1 drop on the test set without global and local encoder.
It suggests the global and local encoders are complementary to each other, and both of them are necessary for \modelname.

Secondly, once we remove the AMR-guided interaction module, the Head F1 would decrease by $1.83$ on the test set.
It shows the semantic structure provided by AMR graphs is helpful to the arguments extraction of the document.

Finally, the removal of boundary loss causes the boundary information lost in span representations, which also leads to $1.62$ and $0.78$ Head F1 decrease on dev and test set.

\subsection{Case Study}

In this section, we show a specific case of the extraction results among different methods.
As shown in Figure~\ref{fig:case-study}, \emph{stabbings} triggers an \emph{Attack} event with three arguments in color.
Since \emph{Nine people} is located near the trigger, all the methods correctly predict it as the \emph{target}.
However, extracting \emph{Minnesota} and \emph{Dahir Adan} asks for capturing long-distance dependency.
Although Two-Step and BART-Gen wrongly predict the \emph{place} as \emph{Iraq and Syria}, and Two-Step even fails to extract the \emph{Attacker}, \modelname manage to extract the cross-sentence arguments.
It can be attributed to that our AMR-enhanced module catches \emph{Minnesota} is the \emph{place} of \emph{attack} that is highly related to the trigger \emph{stabbings} in semantics.

\subsection{Error Analysis}

To further explore the errors made by different models and analyze the reasons in detail, we randomly choose $200$ examples from the RAMS test set and compare the predictions with golden annotations manually.
We divide the errors into five categories, which is shown in Figure~\ref{fig:error-analysis}.
\emph{Wrong Span} refers to assigning a specific role to a wrong span non-overlapped with the golden one.
We find it is usually due to the negative words like \emph{not}, and the coreference spans for the golden one.
\emph{Over-extract} denotes the model predicts an argument role while it does not exist in the document.
Some extracted spans are the sub-strings of the golden spans (\emph{Partial}), or have some overlaps with them (\emph{Overlap}).
These two kinds of errors are usually attributed to the annotation inconsistency in the dataset, such as whether the adjective, quantifier, and article (e.g., \emph{a} and \emph{the}) before the noun should belong to the golden argument.
Besides, the \emph{Partial} error also usually occurs in cases where there is punctuation like a comma in the golden span as shown in Figure~\ref{fig:error-analysis}.
Finally, though the model succeeds to identify the golden span, it can still assign wrong argument role to the span (\emph{Wrong Role}).
We compare the errors of Two-step$_{\mathrm{TCD}}$ and \modelnamebase.
We observe \modelname decrease the number of errors from $275$ to $233$, especially for \emph{Wrong Role} and \emph{Over-extract}, with $27$ and $16$ errors reduction, respectively.