\input{float/figure-running-example}

Event Argument Extraction (EAE) aims at identifying the entities that serve as event arguments, and predicting the roles they play in the event, which is a key step for Event Extraction (EE).
It helps to transform the unstructured text into structured event knowledge that can be further utilized in recommendation systems~\cite{li-etal-2020-gaia}, dialogue systems~\cite{DBLP:conf/aime/ZhangCB20}, and so on.
Most previous studies assume that the events are only expressed by a single sentence and hence focus on sentence-level extraction~\cite{chen-etal-2015-event, liu-etal-2018-jointly, DBLP:conf/aaai/Zhou0ZWXL21}.
However, in real-life scenarios, the events are often described through a whole document consisting of multiple sentences (e.g., a news article or a financial report), which still remains under-explored.

Figure~\ref{fig:running-example} illustrates an example of document-level EAE, in which a \emph{Transport} event is triggered by \emph{shipment}.
Different from sentence-level EAE, extracting arguments out of the entire document faces two critical challenges.
(1) \textbf{Long-distance dependency} among trigger and arguments.
The arguments are usually located in different sentences from the trigger word and their distance can be quite far away. 
In Figure~\ref{fig:running-example}, while the trigger \emph{shipment} is in Sentence $2$, the \emph{vehicle}, \emph{origin}, \emph{artifact}, and \emph{importer} arguments are located in Sentence $1$ or $3$, which highly increases the extraction difficulty.
To accommodate the long-range extraction, not only intra-sentential but also inter-sentential semantics should be well modeled.
(2) \textbf{Distracting context}.
While a document naturally encompasses more context than a single sentence, some distracting context can mislead the argument extraction.
As shown in Figure~\ref{fig:running-example}, the \emph{origin} argument \emph{U.S.} can be identified more easily without Sentence $4$, which does not offer useful information for the event, but contains many place entities that can be distracting, like \emph{Saudi Arabia} and \emph{Russia or Iran}.
It remains challenging to pinpoint the useful context while discarding the distracting information.

Recently, \citet{du-cardie-2020-document} use a tagging-based method, which fails to handle nested arguments.
Instead, span-based methods predict argument roles for candidate spans~\citep{rams, two-step}. 
Some studies directly generate arguments based on sequence-to-sequence model~\citep{wikievent}.
However, how to model long-distance dependency among trigger and arguments, and how to handle distracting context explicitly, remain largely under-explored.

In this paper, to tackle the aforementioned two problems, we propose a \textbf{T}wo-\textbf{S}tream \textbf{A}M\textbf{R}-enhanced extraction model (\textbf{\modelname}).
In order to take advantage of the essential context in the document, and avoid being misled by distractions, we introduce a two-stream encoding module.
It consists of a global encoder that encodes global semantics with as much context as possible to gather adequate context information, and a local encoder that focuses on the most essential information and prudently takes in extra context.
In this way, \modelname can leverage complementary advantages of different encoding perspectives, and therefore make better use of the feasible context to benefit the extraction.
Besides, to model the long-distance dependency, we introduce an AMR-guided interaction module.
Abstract Meaning Representation ~\cite[AMR,][]{banarescu-etal-2013-abstract} graph contains rich hierarchical semantic relations among different concepts, which makes it favorable for complex event extraction.
From such a linguistic-driven angle, we turn the linear structure of the document into both global and local graph structures, followed by a graph neural network to enhance the interactions, especially for those non-local elements.
Finally, as \modelname extracts arguments in span level, where the span boundary may be ambiguous, we introduce an auxiliary boundary loss to enhance span representation with calibrated boundary.


To summarize, our contributions are three-fold.
1) We propose a two-stream encoding module for document-level EAE, which encodes the document through two different perspectives to better utilize the context.
2) We introduce an AMR-guided interaction module to facilitate the semantic interactions within the document, so that long-distance dependency can be better captured.
3) Our experiments show that \modelname outperforms the previous start-of-the-art model by large margins, with $2.54$ F1 and $5.13$ F1 improvements on public RAMS and WikiEvents datasets respectively, especially on cross-sentence event arguments extraction.
