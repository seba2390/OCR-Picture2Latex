\subsection{Datasets}
We evaluate our model on two public document-level event argument extraction datasets, RAMS v$1.0$~\cite{rams} and WikiEvents~\cite{wikievent}.
RAMS contains $9,124$ human-annotated examples, with $139$ event types and $65$ kinds of argument roles, and more than $21$k arguments.
WikiEvents is another human-annotated dataset, with $50$ event types and $59$ event argument roles, and more than $3.9$k events.
We follow the official train/dev/test split for RAMS and WikiEvents datasets, and use the evaluation script provided by ~\citet{rams} to evaluate the performance.
The detailed data statistics of RAMS and WikiEvents datasets are shown in Table~\ref{table:datasets}.

\begin{table}[t]
\centering
\scalebox{0.8}{
    \begin{tabular}{lcccc}
    \toprule
    \bf Dataset & \bf Split & \bf \# Doc. & \bf \# Event & \bf \# Argument \\
    \midrule
    \multirow{3}{*}{RAMS} & Train & 3,194 & 7,329 & 17,026 \\
    ~ & Dev & 399 & 924 & 2,188 \\
    ~ & Test & 400 & 871 & 2,023 \\
    \midrule
    \multirow{3}{*}{WikiEvents} & Train & 206 & 3,241 & 4,542 \\
    ~ & Dev & 20 & 345 & 428 \\
    ~ & Test & 20 & 365 & 566 \\
    \bottomrule
    \end{tabular}
}
\caption{
Statistics of RAMS and WikiEvents datasets.
}
\label{table:datasets}
\end{table}


\subsection{Experiment Setups and  Metrics}

In our implementation, we use BERT$_{\mathrm{base}}$~\cite{bert} and RoBERTa$_{\mathrm{large}}$~\cite{roberta} as our backbone encoder for \modelname, with global and local encoders sharing parameters.
Detailed hyperparameters are listed in Appendix~\ref{sec:appendix-hyperparameters}.

Following~\citet{two-step}, we report the Span F1 and Head F1 for RAMS dataset.
Span F1 requires the predicted argument spans to fully match the golden ones, while Head F1 relaxes the constraint and evaluates solely on the head word of the argument span.
The head word of a span is defined as the word that has the smallest arc distance to the root in the dependency tree.
In addition, following~\citet{wikievent}, we report the Head F1 and Coref F1 scores for WikiEvents dataset.
The model is given full credit in Coref F1 if the extracted argument is coreferential with the golden argument as used by~\citet{coref}.

\subsection{Main Results}
\input{float/main-rams}

We compare \modelname with the following baselines.
1) \textbf{BERT-CRF}~\cite{seq} is a tagging-based method, which adopts a BERT-based BIO-styled sequence labeling model.
2) \textbf{Two-Step}~\cite{two-step} is a span-based method, which first identifies the head word of possible argument span, and then extends to the full span.
\textbf{BERT-CRF$_{\mathrm{TCD}}$} and \textbf{Two-Step$_{\mathrm{TCD}}$} refers to adopting Type-Constraint Decoding mechanism as used in~\cite{rams}.
3) \textbf{FEAE}~\cite{wei-etal-2021-trigger}, Frame-aware Event Argument Extraction, is a concurrent work based on question answering.
4) \textbf{BERT-QA}~\cite{bert-qa} is also a QA-based model. BERT-QA and BERT-QA-Doc extract run on sentence-level and document-level, respectively.
5) \textbf{BART-Gen}~\cite{wikievent} formulate the task as a sequence-to-sequence task and uses BART$_{\mathrm{large}}$~\cite{bart} to generate corresponding arguments in a predefined format.

Table~\ref{table:main-rams} illustrates the results in both dev and test set on RAMS dataset.
As is shown, among models based on BERT$_{\mathrm{base}}$, \modelname outperforms other previous methods.
For example, \modelname yields an improvement of $4.93\sim7.13$ Span F1 and $3.70\sim6.00$ Head F1 compared with the previous method in the dev set, and up to $8.76$ Span F1 in the test set.
Besides, among models based on large pre-trained language models, \modelname  outperforms BART-Gen by $2.54$ Span F1 and $1.21$ Head F1\footnote{We use \modelnamelarge based on RoBERTa$_{\mathrm{large}}$ to compare with BART-Gen based on BART$_{\mathrm{large}}$, as they are pre-trained on the same corpus with the same batch size and training steps.}.
These results suggest that encoding the document in a two-stream way, and introducing AMR graphs to facilitate interactions, is beneficial to capturing intra-sentential and inter-sentential features, and thus improves the performance.


\input{float/main-wikievent}

Moreover, we follow~\citet{wikievent} to evaluate both argument identification and argument classification, and report the Head F1 and Coref F1.
Identification requires the model to correctly detect the argument span boundary, while classification has to further correctly predict its argument role.
As illustrated in Table~\ref{table:main-wikievent}, \modelname consistently outperforms others in both tasks.
Compared with BART-Gen, \modelname improves up to $4.87$/$3.23$ Head/Coref F1 for argument identification, and $5.13$/$3.68$ Head/Coref F1 for argument classification.
Similar results also appear among models based on BERT$_{\mathrm{base}}$, with $5.69\sim36.37$ and $11.95\sim33.34$ Head F1 improvement for identification and classification.
These results show that \modelname is superior to other methods in not only detecting the boundary of argument spans, but also predicting their roles.

