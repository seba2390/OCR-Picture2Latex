
\section{Abstract Meaning Representation (AMR) Graph}
\label{sec:appendix-amr}
There are many AMR parsing approaches \cite{bevil-spring,fernandez-astudillo-etal-2020-transition, wang2021hierarchical, chen-amr}.
To obtain AMR semantic graphs with the align information between text spans and AMR nodes, we use the transition-based AMR parser proposed by~\citet{fernandez-astudillo-etal-2020-transition}, which is a state-of-the-art AMR parser and can achieve satisfactory results for downstream application (up to $81.3$ Smatch on AMR2.0 data).
As the number of AMR relation types is large, which results in too many demanded parameters, we follow~\citet{zhangzixuan} to cluster the relation types into main categories as shown in Table~\ref{table:amr-relations}.
\input{float/amr-relations}

\section{Hyperparameters Setting}
\label{sec:appendix-hyperparameters}
We set the dropout rate to $0.1$, batch size to $8$, and train \modelname  using Adam~\cite{adam} as optimizer with $3\text{e-}5$ learning rate.
We train \modelname for $50$ epochs for RAMS dataset and $100$ epochs for WikiEvents dataset. 
We search the boundary loss weight $\lambda$ from $\left \{  0.05, 0.1, 0.2\right \}$, and $L$ from $\left \{  3, 4 \right \}$, and select the best model using dev set.
Our code is based on Transformers~\cite{transformers} and DGL libraries~\cite{dgl}.

\section{Ablation Study}
\label{sec:appendix-ablation}

In the main body of the paper, we illustrate the results of the ablation study for \modelnamelarge on RAMS dataset.
To thoroughly show the effect of different modules of \modelname, we also provide the results of the ablation study for \modelnamebase on RAMS dataset.
Table~\ref{table:ablaition-rams-base} shows the results on RAMS dataset, from which we can observe removing different modules would cause $1.34\sim2.77$ Span F1 on test set.

Besides, we do ablation study on WikiEvents.
As shown in Table~\ref{table:ablation-wikievent-base}, the Head F1 decreases by $0.70\sim2.02$ and $0.88\sim2.96$ for Arg Identification and Arg Classification sub-tasks respectively, once different modules are removed from \modelnamebase.
Similar conclusions can be drawn from the results of \modelnamelarge, which is shown in Table~\ref{table:ablation-wikievent-large}.

\input{float/ablation-rams-base}
\input{float/ablation-wikievent-base}
\input{float/ablation-wikievent-large}
