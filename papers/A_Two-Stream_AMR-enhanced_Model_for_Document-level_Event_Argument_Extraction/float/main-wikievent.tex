\begin{table}[t]
\centering
\scalebox{0.75}{
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{\bf Method} & \multicolumn{2}{c}{\bf Arg Identification} & 
    \multicolumn{2}{c}{\bf Arg Classification} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    ~ & Head F1 & Coref F1 & Head F1 & Coref F1 \\
    \midrule
    BERT-CRF & 69.83 & 72.24 & 54.48 & 56.72 \\
    BERT-QA & 61.05 & 64.59 & 56.16 & 59.36 \\
    BERT-QA-Doc & 39.15 & 51.25 & 34.77 & 45.96 \\
    \modelnamebase (Ours) & \bf 75.52 & \bf 73.17 & \bf 68.11 & \bf 66.31 \\
    \midrule
    \midrule
    BART-Gen & 71.75 & 72.29 & 64.57 & 65.11 \\
    \modelnamelarge (Ours) & \bf 76.62 & \bf 75.52 & \bf 69.70 & \bf 68.79 \\
    \bottomrule
    \end{tabular}
}
\caption{
\textbf{Comparison between \modelname and other methods on WikiEvents dataset}.
Models above the double line are based on BERT$_{\mathrm{base}}$.
\modelname yields evident improvements in argument identification and classification sub-tasks.
Compared with BART-Gen, \modelname improves Head F1 in argument classification by $5.13$ score.
}
\label{table:main-wikievent}
\end{table}