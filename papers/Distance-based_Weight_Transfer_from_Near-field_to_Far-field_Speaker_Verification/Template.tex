% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage[colorlinks,linkcolor=blue]{hyperref}
\usepackage{caption}
\usepackage{multirow}
\usepackage{float}
\setlength{\parskip}{-0.1em}
\usepackage{setspace}
 
%\renewcommand{\baselinestretch}{0.98}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}
\usepackage{amsmath}
\usepackage[psamsfonts]{amssymb}
%\ninept
% Title.
% ------
%\title{Matrix Norm-based Re from Near-field to Far-field Speaker Verification}
\title{Distance-based Weight Transfer for Fine-tuning from Near-field to Far-field Speaker Verification}
%
% Single address.
% ---------------
\name{Li Zhang$^1$, Qing Wang$^1$, Hongji Wang$^{2}$,
Yue Li$^1$, Wei Rao$^{2}$, Yannan Wang$^{2}$, Lei Xie$^{1,*}$\thanks{* Corresponding author.}} 
\address{
 $^1$Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, \\
Northwestern Polytechnical University (NPU), Xi'an, China  \\
  $^2$Tencent Ethereal Audio Lab, Tencent Corporation, Shenzhen, China
}
 
\begin{document}
\maketitle
 
\begin{abstract}
\iffalse
The scarcity of labeled far-field speech is a constraint for training superior far-field speaker verification systems. Fine-tuning the model pre-trained on large-scale near-field speech substantially outperforms training from scratch. However, the fine-tuning method suffers from two limitations~--~\textit{catastrophic forgetting} and~\textit{overfitting}. In this paper, we propose a weight transfer regularization~(WTR) loss to constrain the distance of the weights between the pre-trained model with large-scale near-field speech and the fine-tuned model through a small number of far-field speech. With the WTR loss, the fine-tuning process takes advantage of the previously acquired discriminative ability from the large-scale near-field speech without catastrophic forgetting. Meanwhile, we use the PAC-Bayes generalization theory to analyze the generalization bound of the fine-tuned model with the WTR loss. The analysis result indicates that the WTR term makes the fine-tuned model have a tighter generalization upper bound. Moreover, we explore three kinds of norm distance for weight transfer, which are L1-norm distance, L2-norm distance, and Max-norm distance. Finally, we evaluate the effectiveness of the WTR loss on VoxCeleb~(pre-trained dataset) and FFSVC~(fine-tuned dataset) datasets.
%Experimental results show that the WTR loss significantly outperforms the vanilla fine-tuning and other competent domain adaptation methods. 
Experimental results show that the distance-based weight transfer fine-tuning strategy significantly outperforms vanilla fine-tuning and other competitive domain adaptation methods.
\fi
The scarcity of labeled far-field speech is a constraint for training superior far-field speaker verification systems. In general, fine-tuning the model pre-trained on large-scale near-field speech through a small amount of far-field speech substantially outperforms training from scratch. However, the vanilla fine-tuning suffers from two limitations~--~\textit{catastrophic forgetting} and \textit{overfitting}. In this paper, we propose a weight transfer regularization~(WTR) loss to constrain the distance of the weights between the pre-trained model and the fine-tuned model. With the WTR loss, the fine-tuning process takes advantage of the previously acquired discriminative ability from the large-scale near-field speech and avoids catastrophic forgetting.
Meanwhile, the analysis based on the PAC-Bayes generalization theory indicates that the WTR loss makes the fine-tuned model have a tighter generalization bound, thus mitigating the overfitting problem. Moreover, three different norm distances for weight transfer are explored, which are L1-norm distance, L2-norm distance, and Max-norm distance. We evaluate the effectiveness of the WTR loss on VoxCeleb~(pre-trained) and FFSVC~(fine-tuned) datasets. Experimental results show that the distance-based weight transfer fine-tuning strategy significantly outperforms vanilla fine-tuning and other competitive domain adaptation methods.
\end{abstract}
%
\begin{keywords}
weight transfer, fine-tuning, far-field speaker verification
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
Speaker verification~(SV) is a task to authenticate a speaker's identity given a small amount of speech from that speaker~\cite{naika2018overview}. In recent years, deep learning has shown remarkable success in SV tasks, but current methods often rely on a large amount of labeled training speech~\cite{nagrani2017voxceleb,desplanques2020ecapa}. The performance of SV systems degrades significantly in far-field conditions due to attenuated signals, noise interference as well as the rareness of far-field datasets~\cite{qin2020hi,movsner2022multisv}. In general, far-field speech datasets are relatively small in size and insufficient to train decent SV models from scratch. Therefore, near-field datasets are generally leveraged for training to improve the discriminative ability of SV systems~\cite{gusev2020stc,zhang2020npu}. However, there is a mismatch problem between near-field speech and far-field speech, so here transfer learning methods are necessary to transfer the SV model from near-field to far-field scenarios.

In recent SV research, there are four kinds of transfer learning methods typically used to address domain mismatch problems. %transfer learning can be grouped into four research directions. %which are domain adversarial training~\cite{}, back-end PLDA adaption~\cite{}, feature distribution alignment~\cite{} and fine-tuning~\cite{} respectively. 
The first one is domain adversarial training generally formulated as a min-max problem, where adversarial strategies~\cite{wang2018unsupervised,rohdin2019speaker} are used to confuse speaker encoders learning domain-invariant speaker representation. The second strategy is based on the back-end process of SV models. Unsupervised PLDA~\cite{garcia2014unsupervised} and CORAL+ PLDA~\cite{lee2019coral+} are proposed to adapt the between-class and within-class covariance matrices of PLDA models~\cite{prince2007probabilistic} in in-domain datasets. The third one is feature distribution alignment, including aligning the distribution between source and target domains~\cite{lin2020multi} and feature mapping with distance-based metric losses~\cite{movsner2019improving}. The last one is the simple fine-tuning strategy, which is a common and effective transfer learning method~\cite{gusev2020stc,zhang2020npu}.

%The underlying assumption of fine-tuning is that the pre-trained model extracts generic features relevant to similar tasks, but is limited to not being able to extract domain-specific speaker representations. 
In this paper, we mainly focus on fine-tuning strategy, which leverages large-scale near-field speech to improve the performance of SV systems in far-field scenarios. Compared with training from scratch, fine-tuning a pre-trained neural network using a far-field dataset can significantly improve
performance while reducing the far-field labeled speech requirements~\cite{gusev2020stc,zhang2020npu}. However, oracle fine-tuning just initializes
the weights of the target model with those of pre-trained
model without considering the catastrophic forgetting and
overfitting problems. %We illustrate the loss convergence and EER trendency of the finetuned model in Fig~\ref{fig:trendency}. As we can see, although the loss function is decreasing~(the empirical error of the model is decreasing), the EER of the model is indeed increasing~(the generalization error keeps increasing). The results indicates that it is very easy to overfit during fine-tuning. In addition, the weights of the pre-trained speaker verification model are only used to initialize the fine-tuned model in the first epoch and there are no other  constraints during fine-tuning, where the fine-tuned model is prone to forget the discriminative ability learned from large-scale near-field datasets. 
\iffalse
\begin{figure}[th]
 %\vspace{-1.5em} 
\captionsetup{font={footnotesize}} 
\centering
\centerline{\includegraphics[width=\columnwidth]{images/finetune_1.png}}
\caption{The losses and EER on FFSVC~2020 and FFSVC~2022 during fine-tuning. }
\setlength{\belowdisplayskip}{-1pt}
\label{fig:trendency}
\end{figure}
\fi
\iffalse
Domain adversarial training applies the adversarial training strategies to make the distribution of source and target domain more similar~\cite{wang2018unsupervised,rohdin2019speaker}. Unsupervised PLDA~\cite{garcia2014unsupervised} and CORAL PLDA~\cite{lee2019coral+} are proposed to adapt the between-class
and within-class covariance matrices of PLDA model~\cite{prince2007probabilistic}. The most common transfer learning method is fine-tuning~\cite{tong2020jd,gusev2020stc,zhang2020deep}. The underlying assumption of fine-tuning is that the pre-trained model extracts generic features relevant to similar tasks, but is limited to not being able to extract domain-specific speaker representations. Compared with
training from scratch, fine-tuning a pre-trained neural network on a target dataset can significantly improve
performance while reducing the far-field labeled speech requirements~\cite{gusev2020stc,wang2021antvoice,zhang2020nsysu+}. However, conventional fine-tuning just initializes
the weights of the fine-tuned model with those of pre-trained
model without considering the catastrophic forgetting and
overfitting in fine-tuning.
\fi
\iffalse
In recent studies, the transfer learning methods in SV can be summarized into three categories.
The first method is to apply
adversarial learning to make the distribution of source and target domain more similar\cite{,,,,}. The second method is PLDA adaption to adapt the mean vector and covariance matrices from one domain to another domain~\cite{,,}. The last category is fine-tuning a source domain model
to the target domain data, which is the most commonly used transfer learning method. Compared with training from scratch, fine-tuning a pre-trained convolutional neural network on a target dataset can significantly improve performance while reducing the far-field labeled speech requirements. However, conventional fine-tuning just initializes the weights of the fine-tuned model with those of pre-trained model without considering the catastrophic forgetting and overfitting problems in fine-tuning. 
\fi

To solve the above problems, we propose a weight transfer regularization~(WTR) loss to constrain the distance between the weights of the pre-trained model and those of the fine-tuned model. In addition, we analyze the generalization bound of fine-tuning with the WTR loss by PAC-Bayes theory~\cite{shawe1997pac}, which is proved to correlate better with empirical performance compared with several popular generalization theories~\cite{jiang2019fantastic}. %provides a powerful framework for analyzing the generalization ability~\cite{viallard2021general} and Jiang et. al~\cite{jiang2019fantastic} compares different generalization theories and indicates that the PAC-Bayes generalization theory correlates better with empirical performance.. 
The analysis result of the PAC-Bayes generalization bound testifies that the generalization bound is tighter with the WTR loss, which limits the distance between the
weights of the pre-trained and fine-tuned models than without any constraint. Furthermore, we explore three different norm distances in WTR loss. Experimental results on VoxCeleb and FFSVC datasets further demonstrate the effectiveness of WTR loss in fine-tuning.
\iffalse
The generalization of neural networks can be measured by the PAC-Bayes
generalization bound~\citec{jiang2019fan tastic}. and overfitting. We demonstrate the PAC-Bayes
generalization bound with the constraint on the distance
between the weights of the pre-trained model and fine-tuned model have a tighter generalization bound than without any constraint. The tighter generalization bound indicates that the model has better generalization ability~\cite{jiang2019fantastic}. Meanwhile, we explore three different matrix norms of the distance between weights of pre-trained model and those of fine-tuned model. Experimental results demonstrate any matrix norm of distance effectively improve the performance of fine-tuned models and the super distance is frobenius norm of distances.
\fi
\iffalse
The rest is organized as follows. Section 2 introduces the proposed methods and Section 3 describes the experimental setup and results.
Section 4 concludes this paper. 
\fi
 
\section{Weight Transfer for Fine-tuning}
%In this section, we first introduce the WTR loss in detail, then we use the PAC-Bayes theory to prove the generalization bound of fine-tuning with WTR loss has a tighter upper bound. Finally, we explored three kinds of norm distances for WTR loss, which are L1-norm distance, L2-norm distance, and Max-norm distance.
\subsection{Weight Transfer Regularization}
%The underlying assumption of fine-tuning is that the pre-trained model extracts generic features relevant to similar tasks, but is limited to not being able to extract domain-specific speaker representations.
The speaker verification framework mainly consists of two modules: speaker embedding extractor $f_E$ and classifier $f_C$. During fine-tuning, the learnable weights of the pre-trained speaker embedding extractor are used to initialize the soon-to-be-finetuned model in the first epoch. The classifier layer needs to be trained from scratch since different datasets contain different numbers of speakers. Suppose the learnable weights of the pre-trained and fine-tuned speaker embedding extractor are $W^s=[W_1^s, W_2^s, W_3^s, ..., W_j^s, ... , W_L^s]$ and $W^t=[W_1^t, W_2^t, W_3^t, ..., W_j^t, ... , W_L^t]$, where $L$ is the number of layers in the speaker embedding extractor. The data space of the large scale pre-training speech is $D_s=\{(x{^s_i},y{^s_i})\} \sim \mathcal{P}(s)$ consisting of $n_s$ labeled samples and the fine-tuning far-field dataset is $D_t=\{(x{^t_i}),(y{^t_i})\} \sim \mathcal{P}(t)$ with $n_t$ samples. In the fine-tuning process, only $D_t$ is available and $D_s$ is unavailable. 

The speaker prediction error ${L}^{(t)}\left(f_C,f_E\right)$ %of the speaker prediction probability $f_C(f_E((x{^t_i}),(y{^t_i})))$ 
is measured by speaker classification loss AAMSoftmax~\cite{deng2019arcface} and the WTR loss in fine-tuning. The speaker embedding extractor in fine-tuning is initialized by the weights of pre-trained model. % is applies to constraint the distance between the currently trained model and the pre-trained model.
The total loss in fine-tuning the SV system is formulated as: 
\begin{equation} 
\begin{aligned}
 & \mathcal{L}^{(t)}\left(f_C,f_E\right)= \underset{(x_i^t, y_i^t) \sim \mathcal{P}(t)}{\mathbb{E}}\left[f_C\left(f_E(x_i^t), y_i^t\right)\right] \\
  &  + \alpha  \sum_{i=1}^{L} ||W_j^t - W^s ||_\pi, 
\end{aligned}
\label{feq1}
\end{equation}
where $\pi$ represents different norm distances and $\alpha$ is a trade-off hyper-parameter between the speaker classification loss and WTR loss. The $W^t_j$ denotes the learnable weights for the current $j$-th epoch during fine-tuning.
\iffalse
Suppose the data space of large-scale pre-training and small-scale fine-tuning datasets are $D_s=\{(x{^s_i})\}$ consisting of $n_s$ labeled samples
and $D_t = \{(x^t_i)\}$ with the distribution of . 
\fi 


%\subsection{Mathematical Explanation of Generalization Capability}
\subsection{Generalization Analysis of Weight Transfer}
To prove the WTR loss mitigating the overfitting of the fine-tuned model, we use the PAC-Bayes generalization theory~\cite{mcallester1999pac} to testify that limiting the distance between weights of the pre-trained and fine-tuned models can obtain a tighter generalization upper bound. % A recent work by Jiang et. al~\cite{jiang2019fantastic} compares different generalization theories and indicates that the PAC-Bayes generalization theory correlates better with empirical performance.

The PAC-Bayes framework~\cite{mcallester1999pac} provides generalization guarantees for randomized inference predictors. Suppose the prior distribution of the pre-trained weights $W^t$ is $\mathcal{P}(s)$, which is independent of far-field speech datasets. The posterior distribution of the fine-tuned weights $W^t$ is $\mathcal{P}(s)$, which depends on the far-field training datasets. The PAC-Bayes theorem states that with probability at least $1-\delta$~($\delta \in (0,1)$) over %the draw of 
the training data, the expected error of speaker classification can be bounded as follows~\cite{mcallester1999pac}:

\iffalse
Let $H$ be some hypothesis class. Let $P$ be a prior
distribution on $H$ that is independent of the training set. Let $Q_S$ be a posterior distribution on $H$
that may depend on the training set $S$. Suppose the loss function is bounded from above by $C$. With
probability $1-\delta$ over the randomness of the training set, the following holds
\fi
\iffalse
\begin{equation}
\begin{aligned}
\underset{ f \sim \mathcal{P}(t))}{\mathbb{E}}[\hat{\mathcal{L}(f)}] \leq \underset{f \sim \mathcal{P}(t)}{\mathbb{E}}\left[f_C\left(f_E(x_i^t), y_i^_t\right)\right]+ \\ C \sqrt{\frac{\operatorname{KL}\left(\mathcal{P}(s) \| \mathcal{P}(t)\right)+3 \ln \frac{n}{\delta}+8}{n}},
\end{aligned}
\label{seq2}
\end{equation}
\fi 

\begin{equation}
\begin{aligned}
\underset{f \sim \mathcal{P}(t)}{\mathbb{E}}[\hat{\mathcal{L}(f)}] \leq \underset{f \sim \mathcal{P}(t)}{\mathbb{E}}[{\mathcal{L}}(f_C[f_E(x_i^t),y_i^t)])]+ \\ C 
\sqrt{\frac{\operatorname{KL}\left(\mathcal{P}(s) \| \mathcal{P}(t) \right)+3 \ln \frac{n}{\delta}+8}{n}},
\vspace{-2em}%
\end{aligned}
\label{seq2}
\end{equation}
where $C$ is the bound of the loss function and $n$ is the number of far-field samples.
%We remark that the original statement in McAllester (1999a) requires the loss function is boundedbetween 0 and 1. The above statement modifies the original statement and instead applies to a loss function bounded between 0 and $C$, for some fixed constant $C > 0$. This is achieved by rescaling the loss by $1/C$, leading the $C$ factor in the right hand side of equation \ref{seq2}. 
Following the conclusions on the normal distribution of weights in convolutional neural networks from the previous work~\cite{huang2021rethinking}, we set the prior distribution $\mathcal{P}(s)=N(W^s, \delta^{2}Id)$, where $W^s$ is the weights
of the pre-trained network. The posterior distribution $\mathcal{P}(t)$ is centered at the fine-tuned model as
$N (W^t,\delta^2Id)$. %The KL divergence between $\mathcal{P}(s)$ and $\mathcal{P}(t)$ is equal to $1/2\delta^2 ||\hat{W}-\hat{W}^{(s)}||^2$. 
We expand the KL divergence using the density of multivariate normal distributions as
\begin{equation}
\small
\begin{aligned}
\operatorname{KL}\left(\mathcal{P}(t)\|\mathcal{P}(s)\right) &=\underset{W_j^t \sim \mathcal{P}(t)}{\mathbb{E}}\left[\log \left(\frac{\operatorname{Pr}(W_j^t \sim \mathcal{P}(t))}{\operatorname{Pr}\left(W_j^t \sim \mathcal{P}(s)\right)}\right)\right] \\
&=\underset{W_j^t \sim \mathcal{P}(t)}{\mathbb{E}}\left[\log \frac{\exp \left(-\frac{1}{2 \sigma^2}\left\|W_j^t-W^s\right\|^2\right)}{\exp \left(-\frac{1}{2 \sigma^2}\|W_j^t-W^t\|^2\right)}\right] \\
&=-\frac{1}{2 \sigma^2} \underset{W_j^t \sim \mathcal{P}(t)}{\mathbb{E}}\left[\left\|W_j^t-W^s\right\|^2-\|W_j^t-W^t\|^2\right] \\
&=\frac{1}{2 \sigma^2} \underset{W_j^t \sim \mathcal{P}(t)}{\mathbb{E}}\left[\left\langle W^s-W^t, 2 W_j^t-W^s-W^t\right\rangle\right] \\
&=\frac{1}{2 \sigma^2}\left\|W^t- W^s\right\|_\pi^2, %\leq \frac{\sum_{i=1}^L D_i^2}{2 \sigma^2},
\end{aligned}
\label{teq3}
\end{equation}
\noindent where the $W^t_j$ denotes the learnable weights for the current $j$-th epoch during fine-tuning. In Eq.~\ref{seq2}, there are two variant items that decide the upper bound of generalization, which is the classification error and KL divergence of $\mathcal{P}(s)$ and $\mathcal{P}(s)$. The classification error is supervised by the classification loss AAMSoftmax~\cite{deng2019arcface} and speaker labels. By the proof of Eq.~\ref{teq3}, the magnitude of the KL divergence is positively related to the difference between the weights of the pre-trained and fine-tuned models. 

From the above proof, we can draw a conclusion that fine-tuning with WTR loss, which constrains the weight distance between the pre-trained model and the fine-tuned model, makes the fine-tuned model have a tighter generalization upper bound. In other words, fine-tuning with WTR loss mitigates the overfitting problem.

%$||\hat{W_i}- W^{(s)} ||_\pi$ for all $1\leqslant i \leqslant L$.
\subsection{Distance-based Weight Transfer}
To limit the weight difference between the pre-trained model and the fine-tuned model, we further explore three kinds of norm distance in WTR loss, which are L1-norm distance, L2-norm distance, and Max-norm distance, respectively.

\subsubsection{L1-norm Distance-based WTR}
The L1 norm distance is calculated as the sum of the absolute values of the difference between weights of pre-trained and fine-tuned models. 
The L1-norm-based WTR loss is formulated as:
\begin{equation}
||W_j^t - W^s ||_\pi = ||W_j^t - W^s||,
\label{l1norm}
\end{equation}
 where $W_j^t$ is the weight of the fine-tuning model on $j$\-th epoch and $W^s$ is the weight of the pre-trained model. 

\subsubsection{L2-norm Distance-based WTR}
The L2 norm is calculated as the square root of the sum of the squared of the difference between weights of the pre-trained and fine-tuned models. The L2-norm-based WTR loss is formulated as:
\begin{equation}
||W_j^t - W^s ||_\pi = ||W_j^t - W^s||_2^2,
\label{l2norm}
\end{equation}
 where $W_j^t$ is the weight of the fine-tuning model on $j$\-th epoch and $W^s$ is the weight of the pre-trained model. 
\subsubsection{Max-norm Distance-based WTR}
Max-norm distance is the largest of the absolute values of all elements in the difference matrix between weights of pre-trained and fine-tuned models. The Max-norm based WTR loss is formulated as:
\begin{equation}
\begin{aligned}
||W_j^t - W^s ||_\pi = \|{W_j^t - W^s}\|_{\infty} = \\ \max \left(\left\|(W_j^t - W^s)_1\right\|, \ldots,\left\|(W_j^t - W^s)_n\right\|\right),
\end{aligned}
\label{maxnorm}
\end{equation}
where $n$ denotes the columns of the $\|W_j^t - W^s)\|_1$. 
\section{Experimental Setup}
%In this section, we describe the datasets in our experiments. Then we detail the training setup and compared methods in this paper. Finally, we introduce the scoring criterion in experimental results.  
\subsection{Datasets}
We conduct experiments on VoxCeleb~(1\&2)~\cite{Nagrani19} and FFSVC ~\cite{qin2020ffsvc,qin2022far} datasets. VoxCeleb~(1\&2) is the large-scale pre-trained dataset. FFSVC~2020 and FFSVC~2022 are the two in-domain far-field datasets respectively. We test on two trials, which are the development trials of FFSVC~2022 and the development trials in task 2 of FFSVC~2020. Note we only use single-channel test utterances~(recorded by channel~2) in FFSVC~2020 trials. The development trials of FFSVC~2022 contain the utterances recorded by iPad and iPhone, so we select the iPhone and iPad recorded speech in FFSVC as the training set for the FFSVC~2022 trials. Meanwhile, we select the iPhone and channel~2 recorded data as the training set of the FFSVC~2020 trials.  
\subsection{Training Details}
%input feature set up 
In this paper, the structure of the speaker verification model is ECAPA-TDANN~(1024)~\cite{desplanques2020ecapa}. The loss function is additive angular margin softmax (AAM-softmax)~\cite{deng2019arcface} with a margin of 0.2 and a scale of 30. The speaker embedding models are trained with 80-dimensional log Mel-filter bank features with 25ms window size and 10ms window shift. In the pre-training process, the weight decay is set to 2e-5. The Adam optimizer with a cyclical learning rate varying between 1e-8 and 1e-3 following triangular policy~\cite{smith2017cyclical} is used for pre-training. The pre-trained model is trained for 12 epochs. Finally, total models are evaluated on the FFSVC~2020 and FFSVC~2022 test sets to find the best models.  

In the fine-tuning step, the neurons of the classification layer in the speaker verification model are modified to the speaker number of far-field speech and the learning rate are varying between 1e-8 to 1e-4. The other configurations are the same as the pre-training process. 

In the pre-training and fine-tuning steps, we all adopt online data augmentation, which includes adding noise~\cite{snyder2015musan}, adding reverberation~\cite{habets2006room} and specAug~\cite{park2019specaugment}. The hyperparameter of $\alpha$ in Eq.~\ref{feq1} is set to 0.01.  

%The mini-batch size is 384 (64 original samples each with 5 augmented samples). A total of 12 epochs are trained for each system. At the end of each epoch, the model is evaluated on the validation set to find the best model for testing
%fine-tuning model setup

%data augmentation setup
 
\subsection{Comparison Methods}
We compare the WTR fine-tuning method with several other competitive domain adaptation methods. They are listed in the following:
\begin{itemize}
     \item Wasserstein Domain Adversarial Training~(Wasserstein DAT)~\cite{rohdin2019speaker}: The authors introduce an end-to-end domain adversarial method based on Wasserstein distance to mitigate the language mismatch problem in SV task.  
    \item Unsupervised PLDA~\cite{garcia2014unsupervised}: Daniel et.al use the out-of-domain PLDA system to cluster unlabeled in-domain speech, and then use the in-domain data to adapt the parameters of the PLDA system.
    \item CORAL+ PLDA~\cite{lee2019coral+}: Kong Aik et al. propose CORAL+ to compute the pseudo in-domain within and between class covariance matrices to regularize the corresponding matrices of PLDA. 
    \item MMD Transfer Learning~\cite{lin2020multi}: This work~\cite{lin2020multi} introduces a DNN-based adaptation method using maximum mean discrepancy~(MMD). 
\end{itemize}
When training the comparing method -- PLDA, we randomly select 1 million utterances in VoxCeleb~(1\&2) and the FFSVC dataset to train the initial PLDA, then only use the training dataset of FFSVC2020 or FFSVC2022 to train the adapted PLDA. Moreover, we reproduce the rest of the above methods on VoxCeleb~(1\&2), FFSVC~2020, and FFSVC~2022 datasets.

\subsection{ Scoring Criterion}
In the test phase, we use cosine similarity as the scoring criterion. The performance metrics are equal error rate (EER)
and minimum detection cost function (minDCF)~\cite{reynolds20172016} which is
evaluated with $P_{target} = 0.01$, $C_{miss} = C_{fa} = 1$.
\section{Experimental Results and Analysis}
%In this section, we describe the experimental results on FFSVC~2020 and FFSVC~2022. At the same time, we analyze the trend of EER and loss values in vanilla fine-tuning and WTR fine-tuning.
\subsection{Results of Different Distance-based WTR}
The experimental results of different Distance-based WTR losses on FFSVC~2020 and FFSVC~2022 are listed in Table~\ref{table1}. From Table~\ref{table1}, we can observe that the EER/minDCF of the pre-trained model on FFSVC~2020 and FFSVC~2022 are 9.817\%/0.814 and 9.849\%/0.731. After vanilla fine-tuning, the EER/minDCF are reduced by 2.382\%/0.100 and 1.808\%/0.028 respectively, which illustrates that fine-tuning transfers the near-field model to the far-field speech to a certain extent. With the help of L2-norm distance-based WTR loss, the EER/minDCF are reduced by 1.548\%/0.041 and 1.399\%/0.143 compared with the results of vanilla fine-tuning on FFSVC~2020 and FFSVC~2022. As shown in Table~\ref{table1}, fine-tuning with L2-norm-based WTR loss obtains the lowest EER/minDCF on FFSVC~2020 and FFSVC~2022.
Each norm distance for WTR loss consistently outperforms the vanilla fine-tuning, demonstrating that the distance constraint between the weights of the pre-trained model and fine-tuned model not only keeps the transferability as vanilla fine-tuning but also alleviates the overfitting problem of the fine-tuned model.
%Each norm distance for WTR loss outperforms the vanilla fine-tuning which demonstrates constraint the distance between the weights of pre-trained model and fine-tuned model gives full play to the discriminativeness of the pret-rained model as well as reduce the overfitting of the fine-tuned model.
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[th]
\centering
\captionsetup{font={small}} 
\caption{EER/minDCF~(p=0.01) of fine-tuning with different distance-based WTR losses.}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{FFSVC~2020} & \multicolumn{2}{c}{FFSVC~2022} \\ \cline{2-5} 
                        & EER~(\%)    & minDCF   & EER~(\%)    & minDCF   \\ \hline
Pre-trained Model        & 9.817      & 0.814            & 9.849      & 0.731            \\ \hline
Vanilla Fine-tuning        & 7.435      & 0.714            & 8.041      & 0.703            \\ \hline
+~WTR~(L1-norm)                 & 7.234      & 0.698            & 7.122      & 0.598            \\ \hline
+~WTR~(L2-norm)                 &\textbf{5.887}      & \textbf{0.673}            & \textbf{6.702}      & \textbf{0.560}            \\ \hline
+~WTR~(Max-norm)            & 6.478      & 0.698            & 7.088      & 0.615            \\ \hline
\end{tabular}
}
\label{table1}
\vspace{-2em}
\end{table}
\subsection{Analysis of Vanilla and WTR Fine-tuning}
To show that weight transfer can avoid overfitting, we illustrate the loss and EER values during vanilla fine-tuning and fine-tuning with L2-norm-based WTR loss in Fig~\ref{fig:replace}. Fig~\ref{fig:replace}~(a) is the changing trend of the loss and EER values with the vanilla fine-tuning epochs. Although the loss function is decreasing~(the empirical error of the model is decreasing), the EER of the model is indeed increasing~(the generalization error keeps increasing). The results indicate that it is very easy to overfit during fine-tuning. In addition, the weights of the pre-trained speaker verification model are only used to initialize the fine-tuned model in the first epoch and there are no other constraints during fine-tuning, where the fine-tuned model is prone to forget the discriminability learned from large-scale near-field datasets. Compared with Fig~\ref{fig:replace}~(a), Fig~\ref{fig:replace}~(b) shows the tendency of the loss and EER values by the fine-tuning epochs with the help of WTR loss. Obviously, in Fig~\ref{fig:replace}~(b), the EER and loss of the training set and the validate set change in the same trend. Specifically, as the number of training epochs increases, the EER and loss get lower and lower until they stabilize at the 20th epoch. Therefore, the analysis further shows that WTR mitigates the overfitting of the model during fine-tuning.

\iffalse
We illustrate the loss convergence and EER tendency of the finetuned model in Fig~\ref{fig:trendency}. As we can see, although the loss function is decreasing~(the empirical error of the model is decreasing), the EER of the model is indeed increasing~(the generalization error keeps increasing). The results indicate that it is very easy to overfit during fine-tuning. In addition, the weights of the pre-trained speaker verification model are only used to initialize the fine-tuned model in the first epoch and there are no other constraints during fine-tuning, where the fine-tuned model is prone to forget the discriminative ability learned from large-scale near-field datasets. The analysis further demonstrates that WTR alleviates the overfitting of the model during the fine-tuning process.

Compared with Fig~\ref{fig:trendency}, the value of EER has the same trend. In other words, as the loss function decreases, the value of EER also decreases. 
\fi

%shows the results from individual models and also from
%the consensus model for comparisonIn addition, we analyze the trend of the loss function during the fine-tuning process as illustrated in Figure~\ref{fig:replace}.  Overfitting occurs when the model performs well on training data but generalizes poorly to unseen data.

\begin{figure}[th]
 \captionsetup{font={footnotesize}} 
\centering
\centerline{\includegraphics[width=\columnwidth]{images/sssss.drawio.pdf}}
\caption{The trend of EER/loss values with the increasing of fine-tuning epochs. The red dashed/solid line is training/validation loss on FFSVC2020. The blue dashed/solid line is training/validation loss on FFSVC2022. (a)~vanilla fine-tuning, (b)~WTR fine-tuning.}
\label{fig:replace}
\vspace{-2em}%
\end{figure}
\subsection{Comparison Results with Other Competitive Methods}
We compare the performance of WTR loss with other competitive domain adaptation methods in the SV task. The experimental results are shown in Table~\ref{table2}. We compare the WTR method with unsupervised PLDA adaptation, CORAL PLDA, Wasserstein DAT and MMD feature distribution alignment. As shown in Table~\ref{table2}, our proposed L2-norm distance-based WTR method outperforms all the compared domain adaptation methods.
%the best method in this table is fine-tuning with WTR loss. Compared with the results of CORAL PLDA, the EER/minDCF of the proposed L2-norm distance WTR method are reduced by 1.549\%/0.041 and 1.135\%/0.164 on FFSVC~2020 and FFSVC~2022 test trials. 
%with back\-end PLDA adaptation~\cite{garcia2014unsupervised,lee2019coral+}, WGAN~\cite{wang2018unsupervised} and MMD~\cite{lin2020multi}.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[th]
\centering
\captionsetup{font={small}} 
\caption{EER/minDCF~(p=0.01) of other competitive methods.}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{FFSVC~2020} & \multicolumn{2}{c}{FFSVC~2022} \\ \cline{2-5} 
                        & EER~(\%)    & minDCF   & EER~(\%)    & minDCF   \\ \hline
%Pretrained Model        & 9.817      & 0.814            & 9.849      & 0.731            \\ \hline
%PLDA                    & 9.210    & 0.763            & 9.376     & 0.731            \\ \hline
Unsuperivised PLDA~\cite{garcia2014unsupervised}       & 8.763     & 0.744            & 8.211     & 0.742            \\ \hline
CORAL+ PLDA~\cite{lee2019coral+}               & 7.435     & 0.714            & 7.837      & 0.724            \\ \hline
Wasserstein DAT~\cite{rohdin2019speaker}         &  8.433   &    0.778         & 9.136    & 0.715            \\ \hline
MMD~\cite{lin2020multi}                      & 7.335    & 0.725            & 7.503   & 0.619            \\ \hline
\textbf{WTR~(L2-norm)}     &\textbf{ 5.887}      & \textbf{0.673}            & \textbf{6.702}      & \textbf{0.560}             \\ \hline
\end{tabular}
}
\vspace{-2em}
\label{table2}
\end{table}

\iffalse

\begin{table}[th]
\centering
\captionsetup{font={footnotesize}} 
\caption{Experimental Results compared with Other Competitive Methods}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\hline
\multirow{2}{*}{Method} & \multicolumn{2}{c}{FFSVC~2020} & \multicolumn{2}{c}{FFSVC~2022} \\ \cline{2-5} 
                        & EER~(\%)    & minDCF~(p=0.01)   & EER~(\%)    & minDCF~(p=0.01)   \\ \hline
Wasserstein DAT         &  8.433   &    0.778         & 9.136    & 0.715            \\ \hline
MMD                     & 7.335    & 0.7251            & 7.5027   & 0.6187            \\ \hline
Weight Transfer~(L2)     &\textbf{ 5.887}      & \textbf{0.673}            & \textbf{6.702}      & \textbf{0.560}             \\ \hline
\end{tabular}}
\label{table3}
\end{table}
\fi
 

 
%DGCNN \cite{song2018eeg} considers the spatial information of EEG signals collected from different channels and employs graph convolution to extract spatial information. BiHDM \cite{li2018novel} uses two directional RNNs to extract spatial information of EEG signals. Our model, the accuracy of our model is further improved compared with the other competitive models. 
%As is shown in the TABLE \ref{results},  the mean accuracy increase $3.16\%$ and $0.52\%$ compared with BiDANN and 3D-CNN with PST-Attention respectively on SEED dataset. The mean accuracy increase $11.98\%$ and $7.92\%$ compared with BiDANN and BiHDM respectively on SEED-IV dataset.an accuracy of 96.28$\%$ on SEED-IV datasets and an accuracy of 82.27$\%$ on SEED datasets compared to other competitive models. 
\section{Conclusion}
 In this paper, we propose a weight transfer regularization~(WTR) loss to solve the catastrophic forgetting and overfitting problems in fine-tuning far-field speaker verification models. Specifically, the WTR term is to limit the weight distance between the pre-trained model and the fine-tuned model. We also explore three kinds of norm distance in the WTR loss, which are L1-norm, L2-norm and Max-norm respectively. Moreover, we prove the generalization capacity of the fine-tuned model with WTR constraint by PAC-Bayes generalization theory. Experimental results and analysis on the FFSVC~2020 and FFSVC~2022 datasets demonstrate the effectiveness of the WTR term in alleviating overfitting and catastrophic forgetting problems during model fine-tuning.
 %regularize the weight distance between the pretrained model and the fine-tuned model.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
%\newpage
%\small
         % 设置行距
%\bibliography{references}
\bibliographystyle{IEEE}
\begin{spacing}{0.98}  
\bibliography{refs}
\end{spacing}
\end{document}
 