\section{Lower bound for fully dynamic LSR}
\label{sec:fully}
In this section we prove that the fully dynamic LSR requires $\Omega(d^{2-o(1)})$ time per update to solve to constant accuracy under the $\omv$ conjecture.


\begin{theorem}[Hardness for fully dynamic LSR, formal version of Theorem \ref{thm_low_acc_LB_informal}]
\label{thm:lower-full}
Let $\gamma > 0$ be any constant. Let $d$ be the input dimension and $T = \poly(d)$ be the total number of update. Let $\eps < 0.01$ be any constant, assuming the $\omv$ conjecture is true, then any algorithm that maintains an $\eps$-approximate solution for fully dynamic least-squares regression problem requires amortized running time at least $\Omega(d^{2 -\gamma})$.
\end{theorem}



The $\omv$ conjecture was originally proposed by \cite{hkns}. In this paper, we work on the standard Word RAM model with word size $O(\log n)$.
\begin{conjecture}[$\omv$ conjecture, \cite{hkns}]
\label{conj:omv}
Let $\gamma > 0$ be any constant. Let $d$ be an integer and $T \geq d$. 
Let $\bB\in \{0,1\}^{d \times d}$ be a Boolean matrix. A sequence of Boolean vectors $\bz^{(1)}, \ldots, \bz^{(T)} \in \{0,1\}^d$ are revealed one after another, and an algorithm solves the $\omv$ problem if it returns the Boolean matrix-vector product $\bB \bz^{(t)} \in \R^d$ after receiving $\bz^\ttop$ at the $t$-th step.
The conjectures states that there is no algorithm that solves the $\omv$ problem using $\poly(d)$ preprocessing time and $O(d^{2-\gamma})$ amortized query time, and has an error probability $\leq 1/3$.
\end{conjecture}





The $\omv$ conjecture asserts the hardness of solving online Boolean matrix-vector product \emph{exactly}.
In order to prove Theorem~\ref{thm:lower-full}, it would be convenient to work with real-valued matrix-vector products. 
We prove that the same lower bound holds for (well-conditioned) PSD matrices, while allowing polynomially small error.  
The following result is a standard and its proof can be found in Appendix \ref{sec:fully-app}.
\begin{lemma}[Hardness of approximate real-valued OMv]
\label{lem:omv-real}
Let $\gamma > 0$ be any constant. Let $d$ be a sufficiently large integer, $T = \poly(d)$. 
Let $\bH\in \R^{d \times d}$ be any symmetric matrix whose eigenvalues satisfy 
$
1/3\leq \lambda_d(\bH) \leq \cdots \leq \lambda_1(\bH) \leq 1,
$
and $\bz^{(1)}, \ldots, \bz^{(T)}$ be online queries.
Assuming the $\omv$ conjecture is true, then there is no algorithm with $\poly(d)$ preprocessing time and $O(d^{2-\gamma})$ amortized running time that can return an $O(1/d^{2})$-approximate answer to $\bH \bz^{(t)}$ for all $t \in [T]$, i.e., a vector $\by^{(t)} \in \R^{d}$ s.t. $\|\by^{(t)} - \bH \bz^{(t)}\|_2 \leq O(1/d^2)$. 
\end{lemma}


The remaining proof of Theorem~\ref{thm:lower-full} proceeds in a few steps.
We introduce the online projection problem in Section \ref{sec:online-projection} and prove that the $\omv$ conjecture implies that the online projection problem requires $\Omega(d^{2-\gamma})$ amortized time to solve to $O(1/d^2)$ accuracy.
We amplify the hardness to constant accuracy in Section \ref{sec:hard-amplification}, and we reduce the online projection to fully dynamic-LSR in Section \ref{sec:reduction}. 
See an illustration of these steps in Figure~\ref{fig:fully}.

\begin{figure}[!ht]
  \centering
  \begin{tikzpicture}[node distance=1.2cm, box/.style={rectangle, draw, text width=4.2cm}, >={Stealth[length=5pt]}]
    % Draw the nodes
    \node[box] (1) {$\omv$ conjecture \\ (Conjecture~\ref{conj:omv})};
    \node[box, right=of 1] (2) {$O(1/d^2)$-approximate real-valued $\omv$ (Lemma~\ref{lem:omv-real})};
    \node[box, below=of 1] (3) {$O(1/d^2)$-approximate online projection (Lemma~\ref{lem:online-projection})};
    \node[box, right=of 3] (4) {$(1/3, 1/d^3)$-approximate online projection (Lemma~\ref{lem:hard-amplification})};
    \node[box, right=of 4] (5) {$0.01$-approximate fully dynamic LSR \\ (Theorem~\ref{thm:lower-full})};

    % Draw the arrows
    \draw[->] (1) -- (2);
    \draw[->] (2) -- (3);
    \draw[->] (3) -- (4);
    \draw[->] (4) -- (5);
  \end{tikzpicture}
  \caption{An illustration of the chain of proofs in this section.}
  \label{fig:fully}
\end{figure}





\subsection{Hardness of online projection}
\label{sec:online-projection}

Recall the definition of the online projection task, which asks to compute the projection of a sequence of online queries $\{\bz^\ttop\}_{t\in [T]}$ onto a fixed subspace $\bU$. 



\Onlineprojection*


For any orthonormal $\bU \in \R^{d\times d_1}$, let $\bU_\perp \in \R^{d\times (d-d_1)}$ be the orthonormal matrix that spans the complementary of the column space of $\bU$, i.e., it satisfies $[\bU, \bU_\perp] \in \R^{d\times d}$ is a squared orthonormal matrix. For any vector $\bz \in \R^d$, define 
\begin{align}
\bz = \bz_{\bU} + \bz_{\bU_\perp} \quad \text{where} \quad \bz_\bU = \bU\bU^{\top}\bz \quad \text{and} \quad \bz_{\bU_\perp} = (\mathbf{I} - \bU\bU^{\top})\bz.
\end{align}
That is to say, $\bz_{\bU}$ is the projection of $\bz$ onto the subspace spanned by the columns of $\bU$, and $\bz_{\bU_{\perp}}$ is the projection of $\bz$ onto the subspace spanned by the complementary of $\bU$. In this section we also denote the projection of a vector $\bz_{k}$ as $\bz_{k, \bU}$.


We prove computing online projection requires $\Omega(d^{2-\gamma})$ amortized time assuming $\omv$.
\begin{lemma}[Hardness of online projection]
\label{lem:online-projection}
Let $\gamma > 0$ be any constant. Assuming the $\omv$ conjecture is true, then there is no algorithm with $\poly(d)$ preprocessing time and $O(d^{2-\gamma})$ amortized running time that can return an $O(1/d^2)$-approximate solution $\hat{\bz}^{\ttop}_{\bU} \in \R^d$ that satisfies $\|\hat{\bz}^{\ttop}_{\bU} - \bU\bU^{\top} \bz^{(t)}\|_2 \leq O(1/d^2)$ for the online projection problem.
\end{lemma}
\begin{proof}
By Lemma \ref{lem:omv-real}, it suffices to reduce from the $O(1/d^2)$-approximate $\omv$ problem of a real-valued PSD matrix $\bH$ with eigenvalues $1/3 \leq \lambda_d(\bH) \leq \cdots \leq \lambda_1(\bH) \leq 1$.
We apply a binary division trick and (approximately) decompose $\bH$ into $k = O(\log d)$ projection matrices $\bU(1), \ldots, \bU(k)$.
Formally, let $\bH = \bU\Sigma \bU^{\top}$ where $\bU \in \R^{d\times d}$ and $\Sigma = \diag(\lambda_1(\bH), \ldots, \lambda_n(\bH))$.
Let $\lambda_{i}(\bH) =0.\lambda_{i,1}\lambda_{i,2}\ldots$ be the binary representation of $\lambda_i$ ($i \in [n]$).
For each $j \in [k]$, let 
\[
S_j = \{i:  i\in [n], \lambda_{i, j} = 1\} \subseteq [n]
\]
be the subset of coordinates with non-zero binary value at the $j$-th bit. Let $\bU(j) = \bU_{*,S_j} \in \R^{d\times |S_j|}$ be an orthonormal matrix that takes columns from $S_j$.





We reduce an $\omv$ instance to $k$ online projection instances, and show that we can compute an $O(1/d^2)$-approximate solution to an $\omv$ query of $\bH$ by using $k$ online projection queries, one for each of the $k$ online projection instances. 
In the preprocessing step, we compute the orthonormal matrices $\bU(1), \ldots, \bU(k)$, and we let them be the initial matrices of the online projection instances. This step can be done in $O(d^3\log d)$ time. 
At the $t$-th step, given an online query $\bz^\ttop \in \R^d$ of $\omv$ with norm $\|\bz^\ttop\|_2 \leq 1$, we make one query for each instance of online projection and let $\bz_j^\ttop$ be the projection returned by the $j$-th instance.
It satisfies 
\begin{align}
\|\bz^{\ttop}_{j} - \bU(j)\bU(j)^{\top}\bz^\ttop\|_2 \leq O(1/d^2). \label{eq:online-proj-guarantee}
\end{align}
The vectors $\bz^\ttop_1, \ldots, \bz^\ttop_k$ can be computed in $k \mathcal{T} = O(\mathcal{T} \cdot \log d)$ (amortized) running time, where $\mathcal{T}$ is the runtime of the online projection algorithm. Finally, we output
\begin{align}
\by^\ttop = \sum_{j=1}^{k} \frac{1}{2^j} \cdot \bz^\ttop_j \label{eq:omv-output}
\end{align}
for the $\omv$ query.
Our goal is to prove $\by^\ttop$ is an $O(1/d^2)$-approximate solution to the $\omv$ query, i.e. 
\begin{align}\label{eq:omv_error}
\|\by^\ttop - \bH\bz^\ttop\|_2 \leq O(1/d^2).
\end{align}

To this end, we have
\begin{align}
\|\by^\ttop - \bH\bz^\ttop\|_2 = &~ \left\|\sum_{j=1}^{k} \frac{1}{2^j} \cdot \bz^\ttop_j - \bH\bz^\ttop\right\|_2 \notag \\
\leq &~  \left\|\sum_{j=1}^{k} \frac{1}{2^j} \cdot \bU(j)\bU(j)^{\top}\bz^\ttop - \bH\bz^\ttop\right\|_2 + \sum_{j=1}^{k}\frac{1}{2^j}\left\|\bU(j)\bU(j)^{\top}\bz^\ttop - \bz_j^\ttop\right\|_2 \notag \\
\leq &~ \left\|\sum_{j=1}^{k} \frac{1}{2^j} \cdot \bU(j)\bU(j)^{\top} \bz^\ttop - \bH\bz^\ttop\right\|_2 + O(1/d^2).\label{eq:online1}
\end{align}
Here the first step follows from the definition of $\by^\ttop$ in Eq.~\eqref{eq:omv-output}, the second step follows from the triangle inequality, and the last step follows from Eq.~\eqref{eq:online-proj-guarantee}.

It remains to bound the first term of RHS, and it suffices to prove $\sum_{j=1}^{k} \frac{1}{2^j} \cdot \bU(j)\bU(j)^{\top}$ is close to $\bH$. 
This holds (almost) by definition. 
Formally, let $\Sigma(j) = \frac{1}{2^j}\diag(\lambda_{1,j}, \ldots, \lambda_{d, j})$ for any $j \geq 1$. By the definition of $\bU(j)$, we have
\begin{align*}
\frac{1}{2^j} \cdot \bU(j)\bU(j)^{\top} =\bU\Sigma(j)\bU^{\top}, \quad \forall j \in [k],
\end{align*}
and therefore 
\begin{align*}
\left\|\bH - \sum_{j=1}^{k}\frac{1}{2^j} \cdot \bU(j)\bU(j)^{\top}\right\|_2 = &~ \left\| \bU \Sigma \bU^{\top} - \sum_{j=1}^{k}\bU\Sigma(j)\bU^{\top}\right\|_2 \\
= &~ \left\|\sum_{j> k} \bU\Sigma(j)\bU^{\top}\right\|_2 \leq \frac{1}{2^{k}} = O(1/d^2),
\end{align*}
where the third step follows from $\| \bU\Sigma(j)\bU^{\top} \|_2 = \frac{1}{2^j} \cdot \|\bU(j) \bU(j)^{\top}\|_2 = \frac{1}{2^j}$, and the last step follows from $k = O(\log d)$.

Plugging into Eq.~\eqref{eq:online1}, we have 
\begin{align*}
\|\by^\ttop - \bH\bz^\ttop\|_2 \leq &~\left\|\sum_{j=1}^{k} \frac{1}{2^j} \cdot \bU(j)\bU(j)^{\top}\bz^\ttop - \bH\bz^\ttop\right\|_2 + O(1/d^2)\\
\leq &~ \left\|\sum_{j=1}^{k} \frac{1}{2^j} \cdot \bU(j)\bU(j)^{\top} - \bH\right\|_2 \|\bz^\ttop\|_2  + O(1/d^2)\\
\leq &~ O(1/d^2)\cdot 1 + O(1/d^2) = O(1/d^2).
\end{align*}


In summary, the above reduction means there exist an $O(1/d^2)$-approximate $\omv$ algorithm for $\bH$ with $O(d^3 \log d)$ preprocessing time and $O(\mathcal{T} \cdot \log d)$ amortized query time. If $\mathcal{T} = O(d^{2-\gamma})$ for some constant $\gamma$, then we can solve the $O(1/d^2)$-approximate $\omv$ problem for $\bH$ in amortized $O(d^{2-\gamma} \cdot \log d)$ time, and by Lemma~\ref{lem:omv-real} this contradicts with the $\omv$ conjecture.
\end{proof}






\subsection{Hardness amplification}
\label{sec:hard-amplification}
So far we have proved an $\Omega(d^{2 - \gamma})$ lower bound of the online projection problem when it is required to output an $O(1/d^2)$-approximate answer per round. 
Our next step is to amplify this approximation precision to a constant.
We first formalize the notion of $(\alpha, \beta)$-approximate projection.

\begin{definition}[$(\alpha, \beta)$-approximate projection]
Given an orthonormal matrix $\bU \in \R^{d\times d_1}$ and a vector $\bz \in \R^{d}$, we say a vector $\by\in \R^{d}$ is an $(\alpha, \beta)$-approximate projection of $\bz$ onto $\bU$, if it satisfies 
\begin{align*}
\|\by - \bU\bU^{\top}\bz\|_2 \leq \alpha \|\bU\bU^\top \bz\|_2 + \beta\|\bz\|_2.
\end{align*}
\end{definition}
As we shall see soon, the interesting regime is $\alpha = \Theta(1)$ and $\beta = 1/\poly(d)$. The problem of online projection (Definition \ref{def:online-projection}) works well with the requirement of outputting an $(\alpha, \beta)$-approximate projection per round. 

\begin{lemma}[Hardness amplification]
\label{lem:hard-amplification}
Let $\gamma > 0$ be any constant. Let $\alpha = 1/3$ and $\beta = O(1/d^3)$.
Assuming the $\omv$ conjecture is true, then there is no algorithm with $\poly(d)$ preprocessing time and $O(d^{2-\gamma})$ amortized running time that can return an $(\alpha, \beta)$-approximate solution for the online projection problem.
\end{lemma}



The reduction is formally shown in Algorithm \ref{algo:hard-amplification}. Our goal is to show that we can answer online projection queries to $O(1/d^2)$ accuracy by using $(\alpha, \beta)$-approximate oracles. We use $\mathbb{P}_\bU, \mathbb{P}_{\bU_\perp}: \R^d \to \R^d$ to denote $(\alpha, \beta)$-approximate projection oracles whose outputs satisfy that for any vector $\bz \in \R^d$, 
(1) $\|\mathbb{P}_{\bU}(\bz) - \bz_\bU\|_2 \leq  \alpha \|\bz_\bU\|_2 + \beta \|\bz\|_2$, and 
(2) $\|\mathbb{P}_{\bU_\perp}(\bz) - \bz_{\bU_\perp}\|_2 \leq  \alpha \|\bz_{\bU_\perp}\|_2 + \beta \|\bz\|_2 $.

The reduction proceeds in $R = \Theta(\log d)$ rounds (i.e., the outer-loop on Line \ref{line:outer}), and we wish to show that each round (1) keeps the projected component $\bz_{r, \bU}$, and (2) reduces the orthogonal component $\bz_{r, \bU_\perp}$ (see Lemma \ref{lem:outer}).
In each round, the reduction first calls the approximate projection oracle onto $\bU_\perp$, which gives a good approximation to the orthogonal component $\bz_{r, \bU_\perp}$, but also has non-negligible component onto space $\bU$.
To resolve this, Algorithm \ref{algo:hard-amplification} proceeds in $K = O(\log d)$ iterations (i.e., inner loop on Line \ref{line:inner}), and in each iteration, it combines the previous output and sends it to $\mathbb{P}_{\bU}$. This process gradually purifies the component onto space $\bU$ (see Lemma \ref{lem:inner-induction}).


\begin{algorithm}[!htbp]
\caption{Hardness amplification}
\label{algo:hard-amplification}
\begin{algorithmic}[1]
\State {\bf Input:} Online query $\bz$, approximate projection oracles $\mathbb{P}_{\bU}$ and $\mathbb{P}_{\bU_\perp}$
\State $\bz_1 \leftarrow \bz$
\For{$r =1,2,\ldots, R$}\Comment{$R = O(\log d)$}\label{line:outer}
\State $\bw_{r, 0} \leftarrow \mathbb{P}_{\bU_\perp}(\bz_r)$
\For{$k=1,2,\ldots, K$}\Comment{$K = O(\log d)$} \label{line:inner}
\State $\by_{r, k} \leftarrow \mathbb{P}_{\bU}(\bw_{r, k-1})$
\State $\bw_{r, k} \leftarrow \bw_{r, k-1} - \by_{r, k} $ \label{line:w-def}
\EndFor
\State $\bz_{r+1} \leftarrow \bz_{r} - \bw_{r, K}$ \label{line:update}
\EndFor
\State \Return $\bz_{R+1}$
\end{algorithmic}
\end{algorithm}



We first state the guarantee of each outer loop. W.l.o.g., we assume $\|\bz\|_2 = 1$.
\begin{lemma}
\label{lem:outer}
For each round $r \in [R]$, we have
\begin{itemize}
\item $\|\bz_{r+1, \bU} - \bz_{r, \bU}\|_2 \leq 4(K+2)\beta$, 
\item $\|\bz_{r+1, \bU_\perp}\|_2 \leq 2\alpha \|\bz_{r, \bU_\perp}\|_2 + 4K^2\beta$, and
\item $\|\bz_{r+1}\|_2 \leq 1 + O(K^4r \beta)$.
\end{itemize}
\end{lemma}


It is useful to first understand the guarantee of the inner loops. 
At the beginning, we have the following lemma for $\bw_{r, 0}$.
\begin{lemma}
\label{lem:inner-begin}
For any round $r \in [R]$, assuming $\|\bz_r\|_2\leq 2$, then we can write 
\begin{align*}
\bw_{r, 0} = \bz_{r, \bU_\perp} - \bdelta_{r, 0} \quad \text{where} \quad \|\bdelta_{r, 0}\|_{2} \leq \alpha \|\bz_{r, \bU_{\perp}}\|_2 + 2\beta.
\end{align*}
\end{lemma}
\begin{proof}
The proof follows directly from the guarantee of $\mathbb{P}_{\bU_\perp}$. In particular, we have that 
\begin{align*}
\|\bw_{r, 0} - \bz_{r, \bU_{\perp}}\|_2 = \|\mathbb{P}_{\bU_\perp}(\bz_r) - \bz_{r, \bU_{\perp}}\|_2 \leq \alpha \|\bz_{r, \bU_{\perp}}\|_2 + \beta \|\bz_r\|_2 \leq \alpha \|\bz_{r, \bU_{\perp}}\|_2 + 2\beta,
\end{align*}
where the second step holds since $\mathbb{P}_{\bU_\perp}$ returns an $(\alpha, \beta)$ approximation over projection onto $\bU_{\perp}$ and the last step holds since $\|\bz_r\|_2 \leq 2$. 
We complete the proof here.
\end{proof}

For each iteration $k \in [K]$, we have the following lemma for the inner loop.
\begin{lemma}
\label{lem:inner-induction}
For any round $r\in [R]$ and iteration $k \in [K]$, assuming $\|\bz_r\|_2 \leq 2$, we can write
\begin{align*}
\bw_{r, k} = \bz_{r, \bU_{\perp}} - (\sum_{\tau = 0}^{k-1} \bdelta_{r, \tau, \bU_{\perp}}) - \bdelta_{r, k}, ~~~\text{and}~~
\by_{r, k} = -\bdelta_{r, k-1, \bU} + \bdelta_{r, k}, 
\end{align*}
and each $\bdelta_{r, k}$ satisfies
\begin{align*}
\|\bdelta_{r, k}\|_2 \leq \alpha \|\bdelta_{r, k-1, \bU}\|_2 + 4 \beta \quad \text{and} \quad \|\bdelta_{r, k}\|_2 \leq \alpha^{k+1}\|\bz_{r, \bU_{\perp}}\|_2 + 4(k+1)\beta.
\end{align*}
\end{lemma}
\begin{proof}
We prove the claim by induction. The base case that $\bw_{r, 0} = \bz_{r, \bU_\perp} - \bdelta_{r, 0}$ and $\|\bdelta_{r, 0}\|_{2} \leq \alpha \|\bz_{r, \bU_{\perp}}\|_2 + 2\beta$ is proved in Lemma~\ref{lem:inner-begin}. (Note that $\by_{r, 0}$ is not defined.)

Suppose the lemma statement holds for $k-1$, which means we have
\begin{align}
\bw_{r, k-1, \bU} = -\bdelta_{r, k - 1, \bU} \label{eq:inner2}
\end{align}
and
\begin{align}
\|\bw_{r, k-1}\|_2 = &~ \left\|\bz_{r, \bU_{\perp}} - (\sum_{\tau=0}^{k-2} \bdelta_{r, \tau, \bU_{\perp}}) - \bdelta_{r, k-1} \right\|_2 \notag \\
\leq &~ \|\bz_{r, \bU_{\perp}}\|_2 + (\sum_{\tau=0}^{k-2} \|\bdelta_{r, \tau, \bU_{\perp}}\|_2) + \|\bdelta_{r, k-1}\|_2 \notag \\
\leq &~ \|\bz_{r, \bU_{\perp}}\|_2 + \sum_{\tau=0}^{k-1} \|\bdelta_{r, \tau}\|_2  \notag\\
\leq &~ \sum_{\tau=0}^{k} \Big( \alpha^{\tau}\|\bz_{r, \bU_{\perp}}\|_2 + 4(\tau+1)\beta \Big) \notag \\
\leq &~ \frac{1-\alpha^{k+1}}{1-\alpha} \cdot \|\bz_{r, \bU_{\perp}}\|_2 + 4K^2\beta \leq 4, \label{eq:inner3}
\end{align}
where the second step holds from triangle inequality, the third step follows from 
\[
\|\bdelta_{r, \tau}\|_2^2 = \|\bdelta_{r, \tau, \bU} + \bdelta_{r, \tau, \bU_\perp}\|_2^2 = \|\bdelta_{r, \tau, \bU}\|_2^2 + \|\bdelta_{r, \tau, \bU_\perp}\|_2^2 \geq \|\bdelta_{r, \tau, \bU}\|_2^2,
\]
the fourth step holds from Lemma \ref{lem:inner-begin} and the induction hypothesis.
The last step follows the choice of $\alpha = 1/3, \beta = O(1/d^3)$, $K = O(\log d)$, and $\|\bz_{r, \bU_\perp}\|_2 \leq \|\bz_{r}\|_2 \leq 2$.

Now we are ready to prove that the induction hypothesis also holds for the $k$-th iteration.

{\bf Properties of $\bdelta_{r,k}$ and $\by_{r,k}$.}
We have
\begin{align}
\|\by_{r, k} + \bdelta_{r, k-1, \bU}\|_2 = &~ \|\mathbb{P}_{\bU}(\bw_{r, k-1}) + \bdelta_{r, k-1, \bU}\|_2 = \|\mathbb{P}_{\bU}(\bw_{r, k}) -  \bw_{r, k-1, \bU}\|_2\notag \\
\leq &~ \alpha\|\bw_{r, k-1, \bU}\|_2 + \beta \|\bw_{r, k-1}\|_2 = \alpha\|\bdelta_{r, k-1, \bU}\|_2 + 4\beta, \label{eq:inner4}
\end{align}
where the first step follows from the definition that $\by_{r, k} = \mathbb{P}_{\bU}(\bw_{r, k-1})$, the second step follows from Eq.~\eqref{eq:inner2}, the third step holds from the guarantee of $\mathbb{P}_{\bU}$ and the last step holds from Eq.~\eqref{eq:inner2}\eqref{eq:inner3}. 

Hence, define $\bdelta_{r, k} = \by_{r, k} + \bdelta_{r, k-1, \bU}$, from Eq.~\eqref{eq:inner4} we have that
\begin{align*}
\|\bdelta_{r, k}\|_2 \leq \alpha \|\bdelta_{r, k-1, \bU}\|_2 + 4\beta.
\end{align*}
Note that this definition of $\bdelta_{r, k}$ also gives us that
\begin{equation}\label{eq:inner_y}
\by_{r, k} = \bdelta_{r, k} - \bdelta_{r, k-1, \bU}.
\end{equation}
By induction hypothesis, we also have 
\[
\|\bdelta_{r, k}\|_2 \leq \alpha \|\bdelta_{r, k-1, \bU}\|_2 + 4\beta \leq \alpha\|\bdelta_{r, k-1}\|_2 + 4\beta \leq \alpha^{k + 1}\|\bz_{r, \bU_{\perp}}\|_2 + 4(k+1)\beta.
\]

{\bf Property of $\bw_{r,k}$.} We have
\begin{align*}
\bw_{r, k} = &~ \bw_{r, k-1} - \by_{r,k} \\
= &~ \Big( \bz_{r, \bU_{\perp}} - (\sum_{\tau=0}^{k-2} \bdelta_{r, \tau, \bU_{\perp}}) - \bdelta_{r, k-1}\Big) - \Big( \bdelta_{r, k} - \bdelta_{r, k-1, \bU} \Big)\\
= &~ \bz_{r, \bU_{\perp}} - (\sum_{\tau=0}^{k-1} \bdelta_{r, \tau, \bU_{\perp}}) - \bdelta_{r, k}.
\end{align*}
Here the first step follows from the definition of $\bw_{r, k}$ (Line \ref{line:w-def}), the second step follows from the induction hypothesis about $\bw_{r,k-1}$ and Eq.~\eqref{eq:inner_y} that we just proved. We conclude the proof here.
\end{proof}


Now we can go back to analyse the outer loops and prove Lemma \ref{lem:outer}.
\begin{proof}[Proof of Lemma \ref{lem:outer}]
Consider any round $r \in [R]$. We have
\begin{align}
\bz_{r+1} = &~ \bz_{r} - \bw_{r, K} \notag \\
= &~ \bz_{r} - \Big( \bz_{r, \bU_{\perp}} - (\sum_{\tau = 0}^{K-1} \bdelta_{r, \tau, \bU_{\perp}}) - \bdelta_{r, K} \Big) \notag \\
= &~ \bz_{r, \bU} + (\sum_{\tau = 0}^{K-1} \bdelta_{r, \tau, \bU_{\perp}}) + \bdelta_{r, K} \label{eq:outer1}
\end{align}
Here the first step follows from the update rule (Line \ref{line:update}), the second step follows from Lemma \ref{lem:inner-induction}. 

Hence, we have $\bz_{r+1, \bU} = \bz_{r, \bU} + \bdelta_{r, K, \bU}$, so for the first claim, we have
\begin{align*}
\|\bz_{r+1, \bU} - \bz_{r, \bU}\|_2 = &~ \|\bdelta_{r, K, \bU}\|_2 \leq \|\bdelta_{r, K}\|_2\\
\leq &~ \alpha^{K+1}\|\bz_{r, \bU_\perp}\|_2 + 4(K+1)\beta \leq 4(K+2)\beta.
\end{align*}
Here the third step follows from Lemma \ref{lem:inner-induction}, the last step follows from $\|\bz_{r, \bU_\perp}\|_2 \leq \|\bz_{r}\|_2 \leq 2$, and the choice of parameter that $K = O(\log d)$, $\alpha = 1/3$ and $\beta = O(1/d^3)$, so that $\alpha^K < \beta$.


For the second claim, the orthogonal component $\bz_{r+1, \bU_\perp}$ satisfies
\begin{align*}
\|\bz_{r+1, \bU_\perp}\|_2 = &~ \left\|\sum_{k=0}^{K} \bdelta_{r, k, \bU_\perp}\right\|_2 \leq \sum_{k=0}^{K}\|\bdelta_{r, k, \bU_\perp}\|_2 \leq \sum_{k=0}^{K}\|\bdelta_{r, k}\|_2 \\
\leq &~ \sum_{k=0}^{K} \Big( \alpha^{k+1}\|\bz_{r, \bU_\perp}\|_2 + 4(k+1) \beta \Big) \\
\leq &~ 2\alpha \|\bz_{r, \bU_\perp}\|_2 + 4K^2 \beta,
\end{align*}
where the first step follows from Eq.~\eqref{eq:outer1}, the second step follows from triangle inequality, the fourth step follows from Lemma \ref{lem:inner-induction}, and the last step follows from the choice of parameter that $\alpha = 1/3$.


Finally, we prove the third claim by induction on $r$. First note that in the base case where $r=0$, by definition we have $\bz_{1, \bU} = \bz$, so $\|\bz_{1, \bU}\|_2 = \|\bz\|_2 = 1$. Suppose the third claim continues to hold up to round $r-1$, for the $r$-th round, we have
\begin{align*}
\|\bz_{r+1}\|_2^2 = &~ \|\bz_{r+1, \bU_\perp}\|_2^2 + \|\bz_{r+1, \bU}\|_2^2 \\
\leq &~ \big(2\alpha \|\bz_{r, \bU_\perp}\|_2 + 4K^2\beta \big)^2 + \big(\|\bz_{r, \bU}\|_2 + 4(K+2)\beta \big)^2 \\
\leq &~ \big(\|\bz_{r, \bU_\perp}\|_2 + 4K^2\beta \big)^2 + \big(\|\bz_{r, \bU}\|_2 + 4 K^2 \beta \big)^2 \\
= &~ (\|\bz_{r, \bU_\perp}\|_2^2 + \|\bz_{r, \bU}\|_2^2) + 8 K^2 \beta \cdot (\|\bz_{r, \bU_\perp}\|_2 + \|\bz_{r, \bU}\|_2) + 32 K^4 \beta^2 \\
\leq &~ \|\bz_r\|_2^2 + 16 K^2 \beta \cdot \|\bz_r\|_2 + 32 K^4 \beta^2 \\
\leq &~ 1 + O(K^4 r \beta),
\end{align*}
where the second step follows from the first two claims that we just proved: $\|\bz_{r+1, \bU_\perp}\|_2 \leq 2\alpha \|\bz_{r, \bU_\perp}\|_2 + 4K^2\beta$, and $\|\bz_{r+1, \bU}\|_2 \leq \|\bz_{r, \bU}\|_2 + \|\bz_{r+1, \bU} - \bz_{r, \bU}\|_2 \leq \|\bz_{r, \bU}\|_2 + 4(K+2)\beta$, the third step follows from $2 \alpha < 1$ since $\alpha = 1/3$ and $K+2 < K^2$ since $K = O(\log d)$, the fifth step follows from $\|\bz_r\|_2^2 = \|\bz_{r, \bU_\perp}\|_2^2 + \|\bz_{r, \bU}\|_2^2$, and the last step follows from the induction hypothesis that $\|\bz_r\|_2 \leq 1 + O(K^4 (r-1) \beta)$, and that $K^2 \beta < K^4 \beta$ and $K^4 \beta^2 < K^4 \beta$.
\end{proof}



Now we can wrap up the reduction and prove Lemma \ref{lem:hard-amplification}.
\begin{proof}[Proof of Lemma \ref{lem:hard-amplification}]
We prove that if there is an algorithm that outputs $(\alpha, \beta)$-approximate solutions for the online projection problem in $O(d^{2-\gamma})$ amortized time, then we can use this algorithm to obtain $O(1/d^2)$-approximate solutions for the online projection problem in $O(d^{2-\gamma + o(1)})$ amortized time, and hence contradicts with Lemma~\ref{lem:online-projection}.

Given an orthonormal matrix $\bU$ and let $\bz^\ttop$ be the query at the $t$-th round of the online projection problem, then we perform the reduction shown in Algorithm \ref{algo:reduction} and its output $\bz^\ttop_{R+1}$ satisfies
\begin{align*}
\|\bz_{R+1, \bU}^\ttop - \bz_{\bU}^\ttop\|_2 = &~ \|\bz_{R+1, \bU}^\ttop - \bz_{1, \bU}^\ttop\|_2 \leq \sum_{r=1}^{R} \|\bz_{r+1, \bU}^\ttop - \bz_{r, \bU}^\ttop\|_2 \leq O(RK\beta).
\end{align*}
Here the first inequality follows from triangle inequality and the second one holds due to the first claim of Lemma \ref{lem:outer}.
Meanwhile, due to the second claim of Lemma \ref{lem:outer}, we have
\begin{align*}
\|\bz_{R+1, \bU_\perp}^\ttop\|_2 \leq (2\alpha)^{K}\|\bz_{1, \bU_\perp}^\ttop\|_2 + O(RK^2\beta) \leq O(RK^2\beta),
\end{align*}
where the second step follows from that $(2 \alpha)^K < 1/d^3$ since $K = O(\log d)$ and $\alpha = 1/3$.

Combining the above two inequalities, and since $R = O(\log d)$, $K = O(\log d)$, and $\beta = O(1/d^3)$, we obtain
\begin{align*}
\|\bz_{R+1}^\ttop - \bz_{\bU}^\ttop\|_2 \leq \|\bz_{R+1, \bU}^\ttop - \bz_{\bU}^\ttop\|_2 + \|\bz_{R+1, \bU_\perp}^\ttop\|_2 \leq O(RK\beta  + RK^2\beta) \leq O(1/d^2).
\end{align*}
That is to say, $\bz_{R+1}^\ttop$ is an $O(1/d^2)$-approximate projection of $\bz^\ttop$ onto $\bU$.


We still need to bound the runtime of the reduction. Let $\mathcal{T}$ denote the amortized query time of the $(\alpha, \beta)$-approximate oracles $\mathbb{P}_{\bU_\perp}$ and $\mathbb{P}_{\bU}$. The reduction involves $R = O(\log d)$ outer loops, with each outer loop requiring a single call to $\mathbb{P}_{\bU_\perp}$ and containing $K = O(\log d)$ inner loops. During each inner loop, a single call to $\mathbb{P}_{\bU}$ is made, and the construction of $\bw_{r,k}$ takes $O(d)$ time.

Therefore, we can conclude that the total runtime of the algorithm is bounded by $RK \cdot \mathcal{T} + O(RKd) = (\mathcal{T} + d) \cdot O(\log^2 d)$. If $\mathcal{T} = O(d^{2-\gamma})$, then we can solve the $O(1/d^2)$-approximate online projection problem in amortized $O(d^{2-\gamma} \cdot \log^2 d)$ time, and this contradicts with the $\omv$ conjecture by Lemma~\ref{lem:online-projection}. This completes the proof.
\end{proof}




As a corollary, we prove the hardness of constant approximate-$\omv$.
\begin{theorem}[Hardness of approximate-$\omv$]
\label{thm:hard-omv-approx}
Let $d$ be a sufficiently large integer and $T = \poly(d)$.
Let $\gamma > 0$ be any constant, $\alpha = 1/3$ and $\beta = O(1/d^3)$. 
Let $\bH\in \R^{d\times d}$ ($\|\bH\|_2 =1$), and $\bz^{(1)}, \ldots, \bz^{(T)}$ be online queries ($\|\bz^\ttop\|_2 = 1$).
Assuming the $\omv$ conjecture is true, then there is no algorithm with $\poly(d)$ preprocessing time and $O(d^{2-\gamma})$ amortized running time that can return an $(\alpha, \beta)$-approximate answer to $\bH\bz^\ttop$ for all $t \in [T]$, i.e., a vector $\by^\ttop$ s.t. $\|\by^\ttop - \bH\bz^\ttop\|_2 \leq \alpha \|\bH\bz^\ttop\|_2 + \beta$. This continues to hold when $\bH$ is a projection matrix.
\end{theorem}




















\subsection{Reduction from online projection to fully dynamic LSR}
\label{sec:reduction}
Finally, we provide a reduction from $(\alpha, \beta)$-approximate online projection to $\epsilon$-approximate fully dynamic LSR, where $\alpha = 1/3$, $\beta = O(1/d^3)$, and $\eps = \frac{1}{100}$. 
Given an instance of online projection with orthonormal matrix $\bU \in \R^{d \times d_1}$, we first set up the LSR problem.

\vspace{+2mm}
{\bf \noindent Setup for reduction \ \ } Let 
\begin{align*}
\bA^{(0)} = 
\left[
\begin{matrix}
\sqrt{\lambda} \cdot \mathbf{I}_d \\
(\bU_\perp)^{\top}
\end{matrix}
\right] \in \R^{(2d-d_1) \times d}  \quad \text{and} \quad \bb^{(0)} = 
\left[
\begin{matrix}
\mathbf{0}_d\\
\frac{1}{\sqrt{d}} \cdot \mathbf{1}_{d-d_1}
\end{matrix} 
\right] 
\in \R^{2d-d_1}
\end{align*}
where $\lambda = 1/d^{40}$. For convenience, we have included a notation table in Table~\ref{tab:parameters}.


\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|}
\hline
Parameter & Value & Comment \\ \hline
$\eps$ & $< 1/100$ & approximation factor of fully dynamic LSR \\ \hline
$\lambda$ & $1/d^{40}$ & coefficient of the regularization term \\ \hline
$\alpha$ & $1/3$ & approximation factor of online projection \\ \hline
\end{tabular}
\caption{Parameters used in the reduction from online projection to fully dynamic LSR}
\label{tab:parameters}
\end{table}

It would be convenient to view the first $d$ rows as a regularization term, and the (squared) loss equals to
\begin{align*}
L(\bx) := \|\bA^{(0)} \bx - \bb^{(0)}\|_2^2 = \left\|(\bU_\perp)^{\top}\bx - \frac{1}{\sqrt{d}} \cdot \mathbf{1}_{d-d_1}\right\|_2^2 + \lambda \|\bx\|_2^2.
\end{align*}
In the processing step, we also compute
\begin{equation}\label{eq:def_x*}
\bx^{*} := \frac{1}{\sqrt{d}} \sum_{j=1}^{d-d_1}\bU_{\perp,j} \in \R^d,
\end{equation}
where with a slight abuse of notation we let $\bU_{\perp, j} \in \R^d$ denote the $j$-th column of matrix $\bU_\perp$ (Hence $\bx^{*}$ also lies in the column space of $\bU_\perp$).
Overall, the preprocessing step takes at most $O(d^\omega)$ time. 


\vspace{+2mm}
{\bf \noindent Online projection query \ \ } Given an online projection query $\bz^\ttop \in \R^{d}$ of the $t$-th step, recall our goal is to find an $(\alpha, 1/d^3)$-approximate projection onto $\bU$.

The reduction is formally presented in Algorithm \ref{algo:reduction}. First, it inserts a new row of $(\frac{1}{10}\cdot \bz^\ttop, 1)$ to the matrix, and then it calls the dynamic LSR solver (Line \ref{line:regression}) to obtain an $\eps$-approximate solution $\bx^\ttop$. The final output is determined as follows: if the component $\bz_\bU^\ttop$ is already sufficiently small, then it is captured by the condition on Line \ref{line:termination}, and we can simply output $\bf{0}$. Otherwise, Algorithm \ref{algo:reduction} outputs a scaled version of $(\bx^\ttop - \bx^{*})$, where the scaling factor is determined by Eq.~\eqref{eq:interpolation}. Finally, the new row is deleted, and we return to the original setup.


\begin{algorithm}[!htbp]
\caption{Reduction: From $(\alpha, 1/d^3)$-approximate online projection to $\epsilon$-approximate dynamic LSR}
\label{algo:reduction}
\begin{algorithmic}[1]
\State Insert $(\frac{1}{10}\cdot\bz^\ttop, 1) \in \R^{d}\times \R$ \Comment{Insert a new row}
\State Call the regression solver and let $\bx^{\ttop}$ be an $\eps$-approximate solution of the square root of \label{line:regression}
\begin{align}
L^\ttop(\bx) := \left\|(\bU_{\perp})^\top\bx - \frac{1}{\sqrt{d}} \cdot \mathbf{1}_{d-d_1}\right\|_2^2 + \frac{1}{100}|\langle \bz^\ttop, \bx\rangle - 10|^2 + \lambda \|\bx\|_2^2   \label{eq:lsr-r}
\end{align}
\State $\by^\ttop \leftarrow \bx^{\ttop} - \bx^{*}$ \label{line:y-rt}
\If{$\|\by^\ttop\|_2 \geq d^3$ \textbf{or} $|10 - \langle \bz^\ttop, \bx^\ttop\rangle| \geq 200 d^4\sqrt{\lambda}$}  
\label{line:termination}
\State \Return $\hat{\bz}_\bU^\ttop \leftarrow \mathbf{0}$ 
\Else
\State \Return $\hat{\bz}_\bU^\ttop \leftarrow \xi^{*} \cdot \by^\ttop$ where \label{line:interpolation}
\begin{align}
\xi^{*} = \arg\min_{\xi} \|\bz^\ttop - \xi \cdot \by^\ttop\|_2 \label{eq:interpolation}
\end{align}
\EndIf
\State Delete the row $(\frac{1}{10}\cdot\bz^\ttop, 1)$ \Comment{Delete the new row}
\end{algorithmic}
\end{algorithm}

Intuitively, the first term $\|(\bU_{\perp})^\top\bx - \frac{1}{\sqrt{d}} \cdot \mathbf{1}_{d-d_1}\|_2^2$ of the loss $L^\ttop(\bx)$ enforces the approximate solution $\bx^{(t)}$ to satisfy that $\bx^{(t)}_{\bU_{\perp}} \approx \bx^*$ on the subspace $\bU_{\perp}$, since $\bx^*$ is the minimizer of the first term. The second term $\frac{1}{100}|\langle \bz^\ttop, \bx\rangle - 10|^2$ then enforces $\bx^{(t)}_{\bU}$ to be close to a scaled version of $\bz^{(t)}_{\bU}$. Thus, $(\bx^{(t)} - \bx^*)$ is approximately a scaled version of $\bz^{(t)}_\bU$.




Formally, our goal is to prove the following lemma.
\begin{lemma}
\label{lem:reduction-lsr}
For any $t \in [T]$, the output of Algorithm \ref{algo:reduction} satisfies
\begin{align*}
\|\hat{\bz}_{\bU}^{\ttop} - \bz_{\bU}^\ttop\|_2 \leq \alpha \|\bz_\bU^\ttop\|_2 + O(1/d^3).
\end{align*}
\end{lemma}
\begin{proof}
We will prove the lemma by considering three different cases. We first give a short summary.
\begin{itemize}
\item {\bf Case 1: $\|\bz_{\bU}^\ttop\|_2 \geq 1/d^4$.} We prove that in this case we always have $|10 - \langle \bz^\ttop, \bx^{(t)}\rangle| \geq 200 d^4\sqrt{\lambda}$, so the condition of Line~\ref{line:termination} reduces to test whether $\|\by^\ttop\|_2 \geq d^3$ or not.
\begin{itemize}
\item {\bf Case 1-1: $\|\by^\ttop\|_2 \geq d^3$.} Then the condition of Line \ref{line:termination} is satisfied and we prove $\|\bz_\bU^\ttop\|_2 \leq 1/d^3$, so it is fine to output $\hat{\bz}_\bU^\ttop = \bf{0}$.
\item {\bf Case 1-2: $\|\by^\ttop\|_2 < d^3$.} Then the condition is not satisfied, and we prove the output $\hat{\bz}_\bU^\ttop = \xi^{*} \cdot \by^\ttop$ is an $(\alpha, 1/d^3)$-approximate projection of $\bz^\ttop$. 
This is the main technical part of the proof.
\end{itemize}
\item {\bf Case 2: $\|\bz_{\bU}^\ttop\|_2 < 1/d^4$.} In this case we prove that the termination condition of Line \ref{line:termination} must be true, and therefore, the output $\bz^{\ttop} = \mathbf{0}$ is an $O(1/d^3)$-approximation of $\bz_\bU^{\ttop}$.
\end{itemize}
Before going into details of the three cases, we first define a vector
\begin{align}\label{eq:x_rt*}
\bx^{*}_{t} = \bx^{*} + \frac{10 - \langle \bz_{\bU_\perp}^\ttop,\bx^{*}\rangle}{\|\bz_{\bU}^\ttop\|_2^2} \bz_{\bU}^\ttop \in \R^d.
\end{align}
We note $\bx^{*}_{t}$ is not the optimal solution of Eq.~\eqref{eq:lsr-r}, but it gives a good upper bound of the loss. 

To understand the role of $\bx_t^{*}$, note that if $\bx^{*}_{t}$ is a good approximation of the optimal solution of Eq.~\eqref{eq:lsr-r}, then $\bx^{\ttop}$ will be close to $\bx^{*}_{t}$, so $\by^\ttop = \bx^{\ttop} - \bx^{*} \approx \bx^{*}_{t} - \bx^{*}$. As a result, $\|\by^\ttop\|_2 \approx \|\bx^{*}_{t} - \bx^{*}\|_2 = O(1/\|\bz_\bU^\ttop\|_2)$. If $\|\by^\ttop\|_2 \geq d^3$ (the first part of the termination condition on Line~\ref{line:termination}) then we have $\|\bz_{\bU}^\ttop\|_2$ is small, so $\mathbf{0}$ is a good approximation of $\bz^{\ttop}_{\bU}$. 
On the other hand, if $\bx^{*}_{t}$ is not a good approximation of the optimal solution of Eq.~\eqref{eq:lsr-r}, then this means $\|\bz_{\bU}^\ttop\|_2$ is way too small, and we can capture this by the second part of the termination condition, i.e., the second term in the objective will be large.

One can verify that $\bx^{*}_{t}$ obtains zero loss except for the third regularization term. That is, it satisfies 
\begin{align*}
\langle \bx^{*}_{t}, \bU_{\perp, j}\rangle = &~ \langle \bx^{*}, \bU_{\perp,j}\rangle = \frac{1}{\sqrt{d}}, \quad \forall j \in [d-d_1], \\
\text{and, } \langle \bx^{*}_{t}, \bz^\ttop\rangle = &~\langle \bx^{*}, \bz_{\bU_\perp}^\ttop \rangle + \frac{10 - \langle \bz_{\bU_\perp}^\ttop,\bx^{*}\rangle}{\|\bz_{\bU}^\ttop\|_2^2}\cdot \langle \bz_{\bU}^\ttop, \bz_{\bU}^\ttop\rangle = 10,
\end{align*}
where the first step of the second equation follows from $\bx^*$ is in the subspace $\bU_\perp$ so it's orthogonal to $\bz_{\bU}^\ttop$.
Consequently, we have $\|(\bU_\perp)^{\top}\bx_{t}^{*} - \frac{1}{\sqrt{d}} \cdot \mathbf{1}_{d-d_1}\|_2 = 0$ and $|\langle \bz^\ttop, \bx_{t}^{*}\rangle - 10| = 0$, so $L^\ttop(\bx_{t}^{*})$ defined in Eq.~\eqref{eq:lsr-r} satisfies
\begin{align}
L^\ttop(\bx_{t}^{*}) = \lambda\|\bx_{t}^{*}\|_2^2 
\leq \lambda \cdot \frac{(10 - \langle \bz_{\bU_\perp}^\ttop,\bx^{*}\rangle)^2}{\|\bz_{\bU}^\ttop\|_2^2} + \lambda := \lambda (\Delta_{t}^2 + 1),
\label{eq:loss}
\end{align}
Here the second step follows from the definition of $\bx_{t}^{*}$ in Eq.~\eqref{eq:x_rt*}, and that $\bx^*$ has norm $\|\bx^*\|_2 \leq 1$ and it's orthogonal to $\bz_{\bU}^\ttop$, for notational convenience in the third step we have defined
\begin{equation}\label{eq:def_Delta_rt}
\Delta_{t} := \frac{10 - \langle \bz_{\bU_\perp}^\ttop, \bx^{*}\rangle}{\|\bz_{\bU}^\ttop\|_2}.
\end{equation}


\vspace{+2mm}
{\bf \noindent Case 1 \ \ } Suppose $\|\bz_{\bU}^\ttop\|_2 \geq 1/d^4$. Since $\|\bx^{*}\|_2 \leq 1$ and $\|\bz_{\bU}^\ttop\|_2 \leq 1$, we have 
\begin{align}\label{eq:Delta_bound}
\Delta_{t} = \frac{10 - \langle \bz_{\bU_\perp}^\ttop, \bx^{*}\rangle}{\|\bz_{\bU}^\ttop\|_2} \in (9,  11d^4].
\end{align}
Therefore by Eq.~\eqref{eq:loss} we have
\begin{align*}
L^\ttop(\bx_{t}^{*}) \leq \lambda (\Delta_{t}^2 + 1) \leq \lambda \cdot (1 + 121d^8).
\end{align*}
The solution $\bx^\ttop$ is $\eps$-approximately optimal where $\eps \leq 1/100$, so
\begin{align}
L^\ttop(\bx^\ttop) \leq (1+\eps)^2 \cdot L^\ttop(\bx_{t}^*) \leq 200\lambda d^8. \label{eq:loss2}
\end{align} 
This implies that
\[
\frac{1}{100} |10 - \langle \bz^\ttop, \bx^{(t)}\rangle|^2 \leq L^\ttop(\bx^{(t)}) \leq 200 \lambda d^8.
\]
So in this case we always have $|10 - \langle \bz^\ttop, \bx^{(t)}\rangle| < 200 d^4\sqrt{\lambda}$, and this means the termination condition on Line~\ref{line:termination} is equivalent to whether $\|\by^\ttop\|_2 \geq d^3$.


We make the following claim about $\bx^\ttop$, and we defer the proof of this claim to Appendix \ref{sec:fully-app}, as it involves some detailed calculations.
\begin{claim}
\label{claim:decomposition}
Let $\bV_{t}$ be the orthonormal matrix that concatenates $\bU_\perp$ and $\bz_{\bU}^\ttop$, i.e, $\bV_{t} := [\bU_\perp, \frac{\bz_{\bU}^\ttop}{\|\bz_{\bU}^\ttop\|_2}]$. Then we have
\begin{align*}
\bU_\perp(\bU_\perp)^{\top} \bx^\ttop = &~  \bx^{*} \pm 20d^4 \sqrt{\lambda}, \\
\langle \bx^\ttop, \bz_{\bU}^\ttop\rangle = &~  10 - \langle \bz_{\bU_\perp}^\ttop,\bx^{*}\rangle \pm 200d^4 \sqrt{\lambda}, \\
\|(\mathbf{I} - \bV_{t}\bV_{t}^{\top}) \cdot \bx^\ttop\|_2 \leq &~ 2\sqrt{\eps} \cdot \Delta_{t}.
\end{align*}
\end{claim}



Using Claim \ref{claim:decomposition}, we can write $\bx^\ttop$ as
\begin{align*}
\bx^\ttop = &~ \bU_\perp (\bU_\perp)^{\top} \bx^\ttop + \frac{\langle \bx^\ttop, \bz^\ttop_{\bU} \rangle}{\|\bz^\ttop_{\bU}\|_2} \cdot \frac{\bz^\ttop_{\bU}}{\|\bz_{\bU}^\ttop\|_2} + \bz_{\perp\perp}^\ttop \\
= &~ \bx^{*} + \Delta_{t}\cdot  \frac{\bz^\ttop_{\bU}}{\|\bz_{\bU}^\ttop\|_2} + \bz_{\perp\perp}^\ttop \pm O(d^8\sqrt{\lambda}),
\end{align*}
where the first step follows from decomposing $\bx_r^\ttop$ into three parts: the component that is in subspace $\bU_\perp$, the component that is in the same direction as $\bz_{\bU}^\ttop$, and the component that is orthogonal to both $\bU_\perp$ and $\bz^\ttop_{\bU}$ which we denote as $\bz_{\perp\perp}^\ttop := (\mathbf{I} - \bV_{t}\bV_{t}^{\top}) \cdot \bx^\ttop$, the second step follows from the first and second parts of Claim~\ref{claim:decomposition} and that $\|\bz_{\bU}^\ttop\|_2 \geq 1/d^4$, and finally note that using the third part of Claim~\ref{claim:decomposition} we have 
\begin{equation}\label{eq:z_perp_perp_bound}
\|\bz_{\perp\perp}^\ttop\|_2 \leq 2\sqrt{\eps}\cdot \Delta_{t}.
\end{equation}


Consequently, we can write $\by^\ttop$ as
\begin{align}\label{eq:y_rt}
    \by^\ttop = \bx^\ttop - \bx^* = \Delta_{t}\cdot  \frac{\bz^\ttop_{\bU}}{\|\bz_{\bU}^\ttop\|_2} + \bz_{\perp\perp}^\ttop \pm O(d^8\sqrt{\lambda}).
\end{align}

We further divide into two cases based on whether $\|\by^\ttop\|_2 \geq d^3$, i.e., whether the termination condition is satisfied.

\vspace{+2mm}
{\bf \noindent Case 1-1 \ \ } Suppose $\|\by^\ttop\|_2 \geq d^3$. Then it meets the termination condition and we return $\hat{\bz}_{U}^\ttop = \mathbf{0}$. 
In this case, we have
\begin{align*}
d^3 \leq \|\by^\ttop\|_2 = &~ \left\|\Delta_{t}\cdot  \frac{\bz^\ttop_{\bU}}{\|\bz_{ \bU}^\ttop\|_2} + \bz_{\perp\perp}^\ttop\right\|_2  \pm O(d^8\sqrt{\lambda} ) \\
\leq &~\Delta_{t} + \|\bz_{\perp\perp}^\ttop\|_2 \pm O(d^8\sqrt{\lambda} ) \leq (1+2\sqrt{\eps})\Delta_{t}  \pm O(d^8\sqrt{\lambda} ),
\end{align*}
where third step follows from triangle inequality, and the last step follows from Eq.~\eqref{eq:z_perp_perp_bound}. 
Since $\lambda = 1/d^{40}$ and $\eps = 1/100$, we conclude that
\begin{align*}
\frac{1}{2}d^3 \leq \Delta_{t} =  \frac{10 - \langle \bz_{\bU_\perp}^\ttop,\bx^{*}\rangle}{\|\bz_{ \bU}^{(t)}\|_2}
\leq \frac{11}{\|\bz_{\bU}^{(t)}\|_2},
\end{align*}
and therefore, $\|\bz_{\bU}^\ttop\|_2 \leq O(1/d^3)$ and it is fine to return $\hat{\bz}_{\bU}^\ttop = \mathbf{0}$.


\vspace{+2mm}
{\bf \noindent Case 1-2 \ \ } 
Suppose $\|\by^\ttop\|_2 < d^3$. Then the termination condition is not met. 
To compute $\hat{\bz}_\bU^\ttop$, we need to solve Eq.~\eqref{eq:interpolation}. Define $\xi  = \Delta_{t}^{-1} \cdot \|\bz_{\bU}^\ttop\|_2$, and we have
\begin{align}
\|\bz^\ttop - \xi \by^\ttop\|_2^2 = &~ \Big\|\bz_{\bU_\perp}^\ttop - \Delta_{t}^{-1} \cdot \|\bz_{\bU}^\ttop\|_2 \cdot \big( \bz_{\perp\perp}^\ttop \pm O(d^{8}\sqrt{\lambda}) \big) \Big\|_2^2\notag \\
= &~ \|\bz_{\bU_\perp}^\ttop\|_2^2 + \Delta_{t}^{-2} \cdot \|\bz_{\bU}^\ttop\|_2^2 \cdot \|\bz_{\perp\perp}^\ttop\|_2^2 \pm O(d^{8}\sqrt{\lambda}) \label{eq:ub1}.
\end{align}
The first step follows from Eq.~\eqref{eq:y_rt} and $\bz^\ttop = \bz_{\bU}^\ttop + \bz_{ \bU_{\perp}}^\ttop$, the second step holds since $\bz_{\bU_\perp}^\ttop$ is orthogonal to $\bz_{ \perp\perp}^\ttop$, and the error term is still $\pm O(d^8 \sqrt{\lambda})$ since $\|\bz_{\bU}^\ttop\|_2, \|\bz_{\bU_{\perp}}^\ttop\|_2, \|\bz_{\perp\perp}^\ttop\|_2 \leq 1$ and $\Delta_{t} \geq 9$ (Eq.~\eqref{eq:Delta_bound}). 


The optimal solution to Eq.~\eqref{eq:interpolation}, denoted as $\xi^{*}$, can be expressed as $\xi^{*} = (1 + \nu)\Delta_t^{-1} \|\bz_\bU^\ttop\|_2$ for some scaling factor $\nu$. Similarly, we have
\begin{align}
\|\bz^\ttop - \xi^{*}\by^\ttop\|_2^2 = &~ \Big\|\bz_{\bU}^\ttop + \bz_{\bU_\perp}^\ttop - (1 + \nu)\Delta_t^{-1}\|\bz_\bU^\ttop\|_2 \cdot \Big(\Delta_{t}\cdot  \frac{\bz^\ttop_{\bU}}{\|\bz_{\bU}^\ttop\|_2} + \bz_{\perp\perp}^\ttop\Big)   \Big\|_2^2 \pm O(d^8\sqrt{\lambda}) \notag \\
= &~ \|\bz_{\bU_\perp}^\ttop\|_2^2 + \nu^2 \|\bz_{\bU}^\ttop\|_2^2 + (1+\nu)^2 \Delta_{t}^{-2} \cdot \|\bz_{\bU}^\ttop\|_2^2 \cdot \|\bz_{\perp\perp}^\ttop\|_2^2 \pm O(d^{8}\sqrt{\lambda}),\label{eq:ub2}
\end{align}
where the first step comes from Eq.~\eqref{eq:y_rt}, the second step follows from $\bz_{\bU_{\perp}}^\ttop$, $\bz_{\bU}^\ttop$ and $\bz^\ttop_{\perp\perp}$ are orthogonal to each other.


Combining Eq.~\eqref{eq:ub1}\eqref{eq:ub2} and the fact that $\xi^{*}$ is the optimal solution to Eq.~\eqref{eq:interpolation}, we have that
\begin{align*}
0 \geq &~ \|\bz^\ttop - \xi^{*} \by^\ttop\|_2^2 - \|\bz^\ttop - \xi\by^\ttop\|_2^2 \\
= &~ \nu^2 \|\bz_{\bU}^\ttop\|_2^2 + (1+\nu)^2 \Delta_{t}^{-2} \cdot \|\bz_{\bU}^\ttop\|_2^2 \cdot \|\bz_{\perp\perp}^\ttop\|_2^2 - \Delta_{t}^{-2} \cdot \|\bz_{\bU}^\ttop\|_2^2 \cdot \|\bz_{\perp\perp}^\ttop\|_2^2 \pm O(d^{8}\sqrt{\lambda})\\
\geq &~ \nu^2 \|\bz_{\bU}^\ttop\|_2^2 - 2|\nu| \cdot \Delta_{t}^{-2} \cdot \|\bz_{\bU}^\ttop\|_2^2 \cdot \|\bz_{\perp\perp}^\ttop\|_2^2  \pm O(d^{8}\sqrt{\lambda})\\
\geq &~ \nu^2 \|\bz_{\bU}^\ttop\|_2^2 - 2|\nu| \cdot 4\eps \cdot \|\bz_{\bU}^\ttop\|_2^2 \pm O(d^{8}\sqrt{\lambda}).
\end{align*}
where the the last step holds due to $\|\bz_{\perp\perp}\|_2 \leq 2\sqrt{\eps} \cdot \Delta_{t}$ (see Eq. \eqref{eq:z_perp_perp_bound}).


Combining the fact that $\|\bz_\bU^\ttop\|\geq 1/d^4$ and choice of parameters, we conclude that $|\nu| \leq 9\eps$. Therefore, the output $\hat{\bz}_{\bU}^\ttop = \xi^{*}\by^\ttop$ satisfies
\begin{align*}
\|\hat{\bz}_{\bU}^\ttop - \bz_\bU^\ttop\|_2 = &~ \|\xi^{*} \cdot \by^\ttop - \bz_\bU^\ttop\|_2\\
= &~ \Big\| (1 + \nu)\Delta_t^{-1}\|\bz_\bU^\ttop\|_2 \cdot \Big(\Delta_{t}\cdot  \frac{\bz^\ttop_{\bU}}{\|\bz_{\bU}^\ttop\|_2} + \bz_{\perp\perp}^\ttop\Big)  - \bz_\bU^\ttop \Big \|_2 \pm O(d^8\sqrt{\lambda})\\
\leq &~ |\nu| \cdot \|\bz_{\bU}^\ttop\|_2 + |1+\nu| \cdot \Delta_t^{-1}\|\bz_\bU^\ttop\|_2 \|\bz_{\perp\perp}^\ttop\|_2 \pm O(d^8\sqrt{\lambda})\\
\leq &~ (|\nu| + 2\sqrt{\eps}|1+\nu|)\|\bz_{\bU}^\ttop\|_2 \pm O(d^8\sqrt{\lambda})\\
\leq &~ \alpha \|\bz_{\bU}^\ttop\|_2 + 1/d^3.
\end{align*}
Here the second step follows from the Eq.~\eqref{eq:y_rt} and the choice of $\xi^{*}$, the third step follows from triangle inequality, the fourth step holds from $\|\bz_{\perp\perp}\|_2 \leq 2\sqrt{\eps}\Delta_{t}$ (see Eq.~\eqref{eq:z_perp_perp_bound}), and the last step follows from $\alpha = 1/3$ and $|\nu| \leq 9 \epsilon$ that we just proved.
This verifies that $\hat{\bz}_{\bU}^\ttop$ is indeed an $(\alpha, 1/d^3)$-approximation of the projection $\bz_{\bU}^\ttop$.


\vspace{+2mm}
{\bf \noindent Case 2 \ \ } Suppose $\|\bz_{\bU}^\ttop\|_2 <  1/d^4$. It suffices to prove the termination condition on Line~\ref{line:termination} of Algorithm~\ref{algo:reduction} holds, i.e., either $\|\by^\ttop\|_2 \geq d^3$ or $|10 - \langle \bz^\ttop, \bx^{(t)}\rangle | \geq 200 d^4\sqrt{\lambda}$. 

Suppose on the contrary that $\|\by^\ttop\|_2 < d^3$ and $|10 - \langle \bz^\ttop, \bx^{(t)}\rangle | < 200 d^4\sqrt{\lambda}$. Then we have
\begin{align*}
10 - O(d^4 \sqrt{\lambda}) \leq  &~ \langle \bz^\ttop , \bx^\ttop\rangle =  \langle \bz_{\bU_\perp}^\ttop , \bx^\ttop\rangle + \langle \bz_{\bU}^\ttop, \by^\ttop \rangle\\
\leq &~ \langle \bz_{\bU_\perp}^\ttop , \bx^\ttop\rangle + \|\bz_{\bU}^\ttop\|_2 \cdot \| \by^\ttop \|_2 \\
\leq &~ \langle \bz_{\bU_\perp}^\ttop , \bx^\ttop\rangle + (1/d^4) \cdot d^3 \\
\leq &~ \|\bz_{\bU_\perp}^\ttop\|_2 \cdot \|\bU_\perp (\bU_\perp)^\top\bx^\ttop \|_2 + 1/d \\
\leq &~ \|\bU_\perp (\bU_\perp)^\top\bx^\ttop \|_2 + 1/d,
\end{align*}
where second step follows from $\by^{(t)} = \bx^{(t)} - \bx^*$ and $\bx^*$ is in the subspace $\bU_{\perp}$, the fourth step follows from the assumptions $\|\bz_{\bU}^\ttop\|_2 <  1/d^4$ and $\|\by^\ttop\|_2 < d^3$, the fifth step holds since $\bz_{\bU_\perp}^\ttop$ lies in the span of $\bU_\perp$, and the last step follows from $\|\bz_{\bU_\perp}^\ttop\|_2 \leq 1$. We conclude that 
\begin{align*}
\|(\bU_\perp)^{\top}\bx^\ttop\|_2 = \|\bU_\perp(\bU_\perp)^{\top}\bx^\ttop\|_2  \geq 9.
\end{align*}
This means 
\[
L^\ttop(\bx^\ttop) \geq \|(\bU_\perp)^\top \bx^\ttop - \frac{1}{\sqrt{d}}\mathbf{1}_{d-d_1}\|^2_2 \geq \big(\|(\bU_\perp)^{\top}\bx^\ttop\|_2 - \|\frac{1}{\sqrt{d}}\mathbf{1}_{d-d_1}\|_2\big)^2 \geq 60.
\] 
This cannot happen because by the definition that $\bx^{*} = \frac{1}{\sqrt{d}} \sum_{j=1}^{d-d_1}\bU_{\perp,j}$, we have
\begin{align*}
L^\ttop(\bx^{*}) = &~ \left\|(\bU_\perp)^{\top}\bx^{*} - \frac{1}{\sqrt{d}} \cdot \mathbf{1}_{d-d_1}\right\|_2^2 + \frac{1}{100}|\langle \bz^\ttop, \bx^{*}\rangle - 10|^2 + \lambda \|\bx^{*}\|_2^2 \\
\leq &~ 0 + \frac{121}{100} +  \lambda \leq 2,
\end{align*}
and this contradicts with $\bx^{\ttop}$ being an $\eps$-approximate solution of the square root of the loss $L^\ttop$. We conclude the proof here.
\end{proof}


Finally, we note that the reduction of Algorithm \ref{algo:reduction} involves one insertion, one deletion and one calls of the dynamic $\eps$-LSR. 
The extra computation it takes is $O(d)$, so it reduces online projection to dynamic $\eps$-LSR.

Combining Lemma \ref{lem:online-projection}, Lemma \ref{lem:hard-amplification} and Lemma \ref{lem:reduction-lsr}, we can finish the proof of Theorem \ref{thm:lower-full}.



