\section{Technical Overview}

In this section we provide a high-level overview of Theorems \ref{thm:main_UB_informal} and \ref{thm_low_acc_LB_informal}. 

\subsection{Lower bound for fully dynamic LSR}
We start from the lower bound in Theorem \ref{thm_low_acc_LB_informal} for fully dynamic $\eps$-LSR, where we prove that  a dynamic data structure  with 
truly sub-quadratic $d^{2-\Omega(1)}$ amortized update time, even for constant approximation $\eps = 0.01$, would break the $\omv$ Conjecture. The key challenge in this proof is that the $\omv$ Conjecture itself only asserts the hardness for \emph{exact} matrix-vector products (over the boolean semiring). Our reduction proceeds in a few steps, where a key intermediate step is introducing the \emph{online projection} problem:


\begin{restatable}[Online projection]{definition}{Onlineprojection}
\label{def:online-projection} In the online projection problem, the input is a fixed orthonormal matrix $\bU \in \R^{d\times d_1}$ ($d_1\in [d]$), and a sequence of vectors $\bz^{(1)}, \ldots, \bz^{(T)}$ that arrives in an online stream. The goal is to compute the projection of $\bz^\ttop$ onto the column space of $\bU$, i.e., $\bU \bU^\top \bz^{(t)}$, at each iteration $t\in [T]$, before $\bz^{(t+1)}$ is revealed. 
\end{restatable}



For notation convenience, we write $\bz = \bz_{\bU} + \bz_{\bU_{\perp}}$ where $\bz_{\bU}$ is the projection onto $\bU$ and $\bz_{\bU_{\perp}}$ is the projection onto the orthogonal space $\bU_{\perp} \in \R^{d\times (d-d_1) }$.

\subsubsection{Hardness of online projection}
We first prove $1/\poly(d)$-hardness of online projection via reduction from $\omv$. The $\omv$ conjecture asserts the hardness of matrix-vector multiplication $(\|\bH\bz^\ttop\|)$ over Boolean semi-ring, and it is easy to see that the lower bound continues to hold when (1) the matrix $\bH$ is positive semidefinite (PSD), (2) the computation is over real, and (3) one allows $1/d^2$ error, i.e., the output $\by^\ttop$ only needs to satisfy $\|\by^\ttop - \bH\bz^\ttop\|_2 \leq O(1/d^2)$ when one normalizes $\|\bH\|_2 = 1$ and $\|\bz^\ttop\|_2 = 1$.

The online projection problem is clearly easier than arbitrary (PSD) matrix-vector multiplications, and we prove the reverse direction is also true. That is, one can (approximately) simulate a matrix-vector product with $O(\log d)$ projection queries. Given a PSD matrix $\bH$, we first perform the eigenvalue decomposition $\bH = \bU \Sigma \bU^\top$ at the preprocessing step, where $\Sigma =\diag(\lambda_1, \ldots, \lambda_d)$ is a diagonal matrix. 
We perform a {\em binary division trick} over the spectral of $\bH$. Let $S_j \subseteq [d]$ include all column indices $i\in [d]$, such that the $j$-th significant bit of $\lambda_{i}$ is non-zero. Let $\bU(j) \in \R^{d\times |S_j|}$ take columns of $\bU$ from $S_j$, then $\bH\bz^{(t)} = \sum_{j=1}^{O(\log d)}\frac{1}{2^j} \cdot \bz_{\bU(j)}^\ttop \pm O(1/d^2)$, i.e., one can obtain an $O(1/d^2)$ approximation of $\bH\bz^{(t)}$ by querying $O(\log d)$ online projection instances, with precision $O(1/d^2)$.


\subsubsection{Hardness amplification} 
Our next step is to boost the hardness of approximation from $O(1/d^2)$ to some constant. In particular, we prove the online projection problem is hard even one only needs 
$
\|\by^\ttop - \bU\bU^\top \bz^\ttop\|_2 \leq \alpha \|\bU\bU^\top \bz^\ttop\|_2 + \beta,
$
where $\alpha = 1/3$ is the multiplicative error and $\beta = 1/d^3$ is a small additive error.

Given any vector $\bz$, to obtain an $O(1/d^2)$ approximation of $\bz_{\bU}$, we set up two online projection instances, $\mathbb{P}_\bU$ and $\mathbb{P}_{\bU_{\perp}}$, and we assume $\mathbb{P}_{\bU}$ (resp.~$\mathbb{P}_{\bU_\perp}$) returns an $(\alpha,\beta)$-approximation to the projection onto $\bU$ (resp.~$\bU_{\perp}$).
A natural idea is to query $\mathbb{P}_{\bU_{\perp}}$ and obtain 
\[
\bw = \mathbb{P}_{\bU_{\perp}}(\bz) = \bz_{\bU_\perp} + \bdelta \quad \text{where the error term} \quad \|\bdelta\|_2 \leq \alpha \|\bz_{\bU_{\perp}}\|_2  + \beta.
\]
Subtracting $\bw$ and considering $\bz' = \bz - \bw$, the orthogonal component decreases by a factor of $\alpha$ (i.e., $\|\bz_{\bU_\perp}'\|_2 \leq \alpha \|\bz_{\bU_\perp}\|_2 + \beta$) and one hopes to repeat it for $O(\log d)$ times to remove the orthogonal component (almost) completely. 
However, the projection component also gets contaminated, i.e., $\bz_{\bU}' = \bz_{\bU} - \bdelta_{\bU}$. Hence, we need to further ``purify'' $\bdelta$ and ensure $\bdelta_{\bU} \approx 0$. 
We obtain it by another $O(\log d)$ iterations of refinement.\footnote{This might sound circular at a first glance because our original goal is to remove $\bz_{\bU_\perp}$ and we reduce it to remove $\bdelta_{\bU}$. The difference is that it is fine to change $\bdelta_{\bU_\perp}$ by a small multiplicative factor when removing $\bdelta_{\bU}$.}


\paragraph{Final reduction} Our final reduction proceeds in $R = O(\log d)$ rounds and each round further contains $K = O(\log d)$ iterations.
\begin{itemize}
\item {\bf Outer loop.} For each round $r \in [R]$, we wish to find $\bw_{r}$ such that  
(1) $\bw_{r}$ has a negligible component in $\bU$, i.e., $\bw_{r, \bU} \approx 0$, and  
(2) $\bw_{r}$ is $\alpha'$-approximate to $\bz_{r}$ in the direction of $\bU_{\perp}$, i.e., $\|\bw_{r, \bU_\perp} - \bz_{r, \bU_\perp}\|_2 \leq \alpha' \|\bz_{r, \bU_\perp}\|_2$ for some constant $\alpha' < 1$. 
By taking $\bz_{r+1} = \bz_{r} - \bw_{r}$, one can prove that the $\bz_{r,\bU}$ component does not change and the orthogonal component $\bz_{r, \bU_\perp}$ decreases by a factor of $\alpha'$. Repeating for $R = O(\log n)$ would be sufficient.
\item {\bf Inner loop.} Within round $r$, recall we first invoke the projection $\mathbb{P}_{\bU_{\perp}}$ and obtain  
$\bw_{r, 0} = \mathbb{P}_{\bU_{\perp}}(\bz_r)$. 
In order to remove $\bw_{r, 0, \bU}$, we query the projection $\mathbb{P}_{\bU}$ and obtain 
$\by_{r, 1} = \mathbb{P}_{\bU}(\bw_{r,0})$, and $\bw_{r, 1} = \bw_{r, 0} - \by_{r,1}$. 
We have the guarantee that $\|\bw_{r, 1, \bU}\|_2 \leq \alpha \|\bw_{r, 0, \bU}\|_2$. Repeat the above step for $K = O(\log n)$ iterations, we have $\bw_{r, K, \bU}\approx 0$. We also need to control the component in $\bU_{\perp}$. We can show that $\bw_{r, K, \bU_{\perp}} = \bw_{r, 0,\bU_{\perp}} - \sum_{k=1}^{K-1}\by_{r, k, \bU_{\perp}}$, where the second term consists of a geometric decreasing sequence with rate $\alpha$, and one has $\bw_{r, K, \bU_{\perp}} = (1 \pm O(\alpha))\bz_{r, \bU_\perp}$.
\end{itemize}

We note the above reduction is adaptive in nature, because the query depends heavily on the algorithm's previous outputs. 




\subsubsection{Reduction to fully dynamic LSR}
The final step is to reduce $(\alpha, \beta)$-online projection to fully dynamic $\eps$-LSR, with the following choice of parameters $\alpha = 1/3, \eps= 0.01$ and $\beta = 1/d^3$. A natural first attempt is to set the initial feature matrix $\bA^{(0)} = \bU_{\perp}^{\top} \in \R^{(d-d_1)\times d}$ and the labels $\mathbf{b}^{(0)} = \frac{1}{\sqrt{d}}\mathbf{1}_{d-d_1}$. This is an under-constrained linear system. Let 
$
\bx^{*} = (\bU_{\perp}\bU_{\perp}^\top)^{\dagger}\bU_{\perp}\mathbf{b}^{(0)} = \frac{1}{\sqrt{d}}\sum_{j=1}^{d-d_1}\bU_{\perp, j}
$
be the normal equation -- this is the solution with the least $\ell_2$ norm. Suppose we wish to project $\bz$ onto $\bU$, then one can insert a new row of $(\bz, 10)$ and the Normal equation becomes 
\[
\bx^{*}_{\new} = \bx^{*} +  \frac{10 - \langle \bz_{\bU_{\perp}}, \bx^{*} \rangle}{\|\bz_{\bU}\|_2^2} \bz_{\bU}.
\]
If the $\eps$-approximate solution $\bx'$ returned by the algorithm is indeed close to $\bx^{*}$, we can obtain a scaled version of $\bz_{\bU}$ by computing $\bx' - \bx^{*} \approx \bx^{*}_{\new} - \bx^{*} \propto \bz_{\bU}$. 

Unfortunately, there are infinitely many optimal solutions and an algorithm does not need to output the normal form solution.
For example, an algorithm could remember a random direction $\bv$ that is orthogonal to $\bU_{\perp}$ (at the preprocessing step) and run binary search on $\bx^{*} + \xi \cdot \bv$ to resolve the new constraint $\langle \bz , \bx \rangle = 10$. It only requires $O(d)$ time and returns an exact solution. 


\paragraph{The importance of regularization} The above issue seems to be inherent of an under-constrained linear system.
To resolve it, we consider the ridge regression instead and add a small regularization term $\lambda \|\bx\|_2$ for $\lambda = 1/d^{40}$. 
Our reduction starts with $\|\bU_{\perp}^\top \bx - \mathbf{1}_{d-d_2}\|_2 + \lambda \|\bx\|_2$, and inserts/deletes the row $(\bz , 10)$ to compute the projection of $\bz$. For ridge regression, $\bx^{*}_{\new}$ is actually not the optimal solution (instead, it is very close to the unique optimal solution) but we would prove an $\eps$-approximate solution $\bx'$ needs to be very close to $\bx_{\new}^{*}$. Therefore, one can retrieve an $(\alpha, \beta)$-approximate projection from $\bx'$ and $\bx^{*}$. 

\paragraph{Missing technical consideration} We outline a few missing details of the above argument. First, the above argument (i.e., $\bx'$ is close to $\bx^{*}_{\new}$) goes through only if $\bz_{\bU}$ is not too small (e.g., $\|\bz_{\bU}\|_2 \geq 1/d^4$). We need to efficiently test the norm $\|\bz_{\bU}\|_2$ and output $\mathbf{0}$ when it is too small. 
Second, even if $\bx_{\new}^{*}$ and $\bx'$ are close, we can only obtain a scaled version of $\bz_{\bU}$. It is not oblivious to determine the right ``scale'' because of the (constant) approximation error. Instead, we run a line search and output the minimizer of $\arg\min_{\xi}\|\bz - \xi\cdot(\bx' - \bx^{*})\|$, we prove that it gives good approximation to $\bz_{\bU}$.


\subsection{Algorithm for partially dynamic LSR}
Next we provide an overview of our algorithm in Theorem \ref{thm:main_UB_informal} for partially dynamic LSR (with row insertions only). 
Let $\boldm^\ttop = (\ba^\ttop, \beta^{(t)})$ be the $t$-th row and $\bM^\ttop$ be the input matrix that concatenates these rows.
Our approach follows the online row sampling framework \cite{clmmps15, cmp20, bdm+20}: 
When a new row arrives, we sample and keep the new row with probability (approximately) proportional to the {\em online leverage score} $\tauo^\ttop:= (\boldm^\ttop)^\top ((\bM^\ttop)^{\top} \bM^\ttop)^{-1}\boldm^\ttop$. 
We output the closed-form solution on the sampled rows: It is an $\eps$-approximate solution of LSR as long as the sampled matrix $\wt{\bM}^\ttop$ is an $\eps$-spectral approximation to the input matrix $\bM^\ttop$.


\paragraph{Warm up: oblivious adversary}
If the algorithm faces an oblivious adversary, then \cite{cmp20} proves that keeping $O(d\log(\frac{\sigma_{\max}}{\min})/\eps^2)$ rows is enough for  $\eps$-spectral approximation.
It remains to bound the computation time.
Note a direct computation of the online leverage score takes $O(d^2)$ time per row-insertion, which gives no benefit over the classic Kalman's approach. In order to accelerate this computation, we use a JL-embedding trick to compress the matrix $(\bM^\ttop)^\top\bM^\ttop$ (note a similar trick has been used in \cite{ss11, blss20}) and it reduces the computation time from $O(d^2)$ to $O(d)$ per update. 





\paragraph{Adversarial robustness of online leverage score sampling}  We need a counterpart of \cite{cmp20} for the more challenging adaptive adversary.
The recent work of \cite{bhm+21} made a first step toward adversarially robust row-sampling. However, comparing to the oblivious setting, their algorithm increases the number of sampled rows 
by a factor of $d (\frac{\sigma_{\max}}{\sigma_{\min}})^2$. Note that it has a polynomial dependence on the condition number $\frac{\sigma_{\max}}{\sigma_{\min}}$, which could be as large as $\poly(dT)$.\footnote{Indeed, if the input are drawn from isotropic Gaussian $\mathcal{N}(0, \mathbf{I}_d)$ but with one direction removed, then the condition number can be as large as $T$.}


\cite{bhm+21} considers an $\epsilon$-net over the unit vectors in $\R^d$, and for any $\bx$ in the $\epsilon$-net, they use Freedman's inequality to prove that the sampled matrix $\tilde{\bM}^{(t)}$ satisfies that $\|\tilde{\bM}^{(t)} \bx\|_2 \approx \|\bM^{(t)} \bx\|_2$ (this brings an $O(d)$ overhead using a union bound). In order to apply Freedman's inequality, they need an estimate of $\|\bM^{(t)} \bx\|_2$, which is unknown in advance since the rows of $\bM^{(t)}$ are chosen adaptively. \cite{bhm+21} directly bounds this norm by the singular values: $\sigma_{\min} \|\bx\|_2 \leq \|\bM^{(t)} \bx\|_2 \leq \sigma_{\max} \|\bx\|_2$, resulting in the additional $(\frac{\sigma_{\max}}{\sigma_{\min}})^2$ overhead. 


We provide a new analysis that overcomes this limitation. Similar to \cite{bhm+21}, we also take a union bound over the $\epsilon$-net of unit vectors $\bx \in \R^d$ to reduce to the scalar case. The key difference is that when applying Freedman's inequality, we instead consider $O(\frac{\sigma_{\min}}{\sigma_{\max}})$ truncated martingales that each guesses the correct value of $\|\bM^{(t)} \bx\|_2$, and becomes 0 once the guess becomes inaccurate. We prove that each truncated martingale concentrates according to Freedman's inequality, and since one of the guesses must be close to the true value of $\|\bM^{(t)} \bx\|_2$, taking a union bound over these $O(\frac{\sigma_{\min}}{\sigma_{\max}})$ martingales results in only an $O(\log(\frac{\sigma_{\max}}{\sigma_{\min}}))$ overhead. We believe this can also be used to improve other importance sampling schemes in \cite{bhm+21}, which we leave for future work. 

\paragraph{Robustness of JL estimation} Finally, we also need to prove the JL trick is adversarially robust. To this end, we renew the JL sketch for each sampled row. 
The sampling probability computed using the JL estimate is always an overestimate of the true online leverage score. Consequently, whenever our algorithm omits a row, the ideal algorithm using the exact online leverage scores also omits that row. This means the randomness of the JL matrix is not leaked until a new row is sampled, at which point we refresh the JL matrix.

