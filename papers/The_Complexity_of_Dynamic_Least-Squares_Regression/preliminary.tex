\section{Preliminary}
\label{sec:formulation}


\paragraph{Notations}
Let $[n]=\{1,2,\cdots,n\}$ and $[n_1: n_2] = \{n_1, \ldots, n_2\}$. 
For any $x, y \in \R$, we write $x = y \pm \eps$ if $|x - y| \leq \eps$, for two vectors $\bx, \by \in \R^d$, $\bx = \by \pm \eps$ means $\|\bx - \by\|_2 \leq \eps$.
For any matrix $\bA \in \R^{n\times d}$ ($n \geq d$), let $\sigma_1(\bA)\geq \cdots \geq \sigma_d(\bA) \geq 0$ be its singular value, and $\kappa(\bA) = \frac{\sigma_1(\bA)}{\sigma_d(\bA)}$ be the condition number. When $\bA$ is a symmetric matrix, we use $\lambda_1(\bA) \geq \cdots \geq \lambda_d(\bA)$ to denote its eigenvalue.
Let $\nnz(\bA)$ be the number of non-zero entries of a matrix $\bA$.
We use $\|\cdot\|$ to denote the spectral norm, i.e. $\|\bA\| = \max_{\bx\in \R^d, \|\bx\|_2 =1}\|\bA \bx\|_2$ and $\|\cdot\|_{\mathsf{F}}$ to denote the Frobenius norm. We use $\ker[\bA]$ and $\im[\bA]$ to denote the kernel space and the column space of $\bA$. We use $\bA_{i,*}$ and $\bA_{*,j}$ to denote the $i$-th row and the $j$-th column of $\bA$. For two sets $S \subseteq [n], R \subseteq [d]$, we use $\bA_{S,*}$ and $\bA_{*,R}$ to denote the submatrix of $\bA$ obtained by taking the rows in $S$ or taking the columns in $R$.
The $d$-dimensional identity matrix is denoted as $\bI_d$, and we use $\mathbf{1}_d$ (resp. $\mathbf{0}_d$) to denote the all one (resp. all zero) vectors. For a vector $\bv \in \R^d$, denote $\diag(\bv) \in \R^{d \times d}$ as a diagonal matrix whose diagonal entries are $\bv$. 




\subsection{Model}\label{sec:model}
We formally define the problem of fully dynamic least-squares regression.
\begin{definition}[Fully dynamic least-squares regression]\label{def:fully_dynamic_l2_regression}
Let $d$ be an integer, $T = \poly(d)$ be the total number of updates and $\eps \in [0,0.5)$ be the precision.
In the fully dynamic least-squares regression problem:
\begin{itemize}
    \item The data structure is given a matrix $\bA^{(0)} \in \R^{(d+1) \times d}$ and a vector $\bb^{(0)} \in \R^{n}$ in the preprocessing phase.
    \item For each iteration $t \in [T]$, the algorithm receives one of the following two updates:
    \begin{itemize}
    \item Incremental update: The update is $(\ba, \beta) \in \R^d \times \R$, and the matrix and the vector are updated to be $\bA^\ttop := [(\bA^{(t-1)})^{\top}, \ba]^{\top}$ and $\bb^\ttop := [(\bb^{(t-1)})^{\top}, \beta]^{\top}$.
    \item Decremental update: The update is a row index $i$, and the matrix $\bA^\ttop$ is $\bA^{(t-1)}$ with its $i$-th row deleted, and the vector $\bb^\ttop$ is $\bb^{(t-1)}$ with its $i$-th entry deleted.
    \end{itemize}
\end{itemize}
We say an algorithm solves $\epsilon$-approximate fully dynamic least squares regression if it outputs an $\epsilon$-approximate solution $\bx^{(t)} \in \R^d$ at every iteration $t \in [T]$:
    \[
    \|\bA^{(t)} \bx^{(t)} - \bb^{(t)}\|_2 \leq (1+\eps) \min_{\bx \in \R^d} \|\bA^{(t)} \bx - \bb^{(t)} \|_2.
    \]
\end{definition}

We use $\bM^{(t)} := [\bA^{(t)}, \bb^{(t)}]$ to denote the concatenation of input feature and their labels. 

We can similarly define a partially dynamic least-squares regression problem that allows insertion or deletion updates only. In our paper, we only study the incremental model.\footnote{In the streaming literature, it is also called the online model. We call it incremental model, to emphasize that the computation cost is the major consideration (instead of space, or the number of rows been kept).}

\begin{definition}[Partially dynamic least-squares regression, incremental update]
A partially dynamic least-squares regression is formalized similarly as Definition \ref{def:fully_dynamic_l2_regression}, with incremental update only. Moreover, we assume the least singular value of $\bM^{(0)}$ is at least $\sigma_{\min} > 0$, and the largest singular value of $\bM^{(T)}$ is at most $\sigma_{\max}$. 
\end{definition}


\begin{remark}[Singular value, incremental model]
We assume the least (and largest) singular value of data matrix $\bM^\ttop$ is bounded, this is standard in the literature (see \cite{cmp20, bhm+21}), and one should think of $1/\poly(T,d) \leq \sigma_{\min} \leq \sigma_{\max} \leq \poly(T, d)$. 
In practice, we can always add a polynomially small regularization term to ensure the smallest singular value of $\bM^{(0)}$ is at least $1/\poly(T, d)$. The largest singular value of $\bM^{(T)}$ is bounded by $\poly(T, d)$ as long as each entry has polynomially bounded value. 
\end{remark}


\begin{remark}[Preprocessing]
In the definition of both fully and partially dynamic LSR, we assume the problem is initialized with a full rank data matrix $\bM^{(0)} \in \R^{(d+1)\times(d+1)}$. This is wlog if one allows polynomial preprocessing time.
\end{remark}







Next, we state the (standard) notion of the {\em oblivious} adversary and the {\em adaptive} adversary.
\begin{definition}[Adversary model]
Two adversary models are of consideration:
\begin{itemize} 
\item {\bf Oblivious adversary.} For an oblivious adversary, the insertion/deletion updates are independent of algorithm's output. 
\item {\bf Adaptive adversary.} An adaptive adversary could choose the new update base on the algorithm's previous output. That is, at the $t$-th update, the new row $(\ba, \beta)$ of an insertion or the index of a deletion update could be a function of $\bx^{(1)}, \ldots, \bx^{(t-1)}$.
\end{itemize}
\end{definition}







\subsection{Mathematical tools}
\label{sec:basic_numerical_linear_algebra}

A positive semidefinite (PSD) matrix $\bA\in \R^{d\times d}$ is symmetric and satisfies $\bx^{\top}\bA\bx\geq 0$ for all $\bx\in \R^{d}$.
We write $\bA \succeq 0$ to denote that $\bA$ is PSD, and we write $\bB \succeq \bA$ to denote that $\bB - \bA$ is PSD. 

\begin{definition}[Spectral approximation]
For two symmetric matrices $\bA, \wt{\bA} \in \R^{n \times n}$, we say that $\wt{\bA}$ and $\bA$ are $\eps$-spectral approximations of each other (denoted as $\wt{\bA} \approx_{\epsilon} \bA$) if
\begin{align*}
    (1 - \epsilon) \cdot \bA \preceq \wt{\bA} \preceq (1 + \epsilon) \cdot \bA.
\end{align*}
\end{definition}



\paragraph{Online leverage scores}  
In the incremental model, rows arrive in online fasion and the online leverage score of the $t$-th row $\boldm^\ttop$ equals
\begin{align*}
    \tauo^\ttop:= (\boldm^\ttop)^\top (((\bM)^{(t-1)})^\top \bM^{(t-1)})^{-1} \boldm^\ttop.
\end{align*}


Online leverage scores satify the following property. Its proof is delayed to Appendix~\ref{sec:pre-app}.
\begin{fact}[Property of online leverage score]\label{fact:online_leverage_score}
For any $t\in [T]$, we have
\[
 \boldm^\ttop (\boldm^\ttop)^{\top}  \preceq \tauo^\ttop \cdot  ((\bM)^{(t-1)})^\top \bM^{(t-1)}.
\]
\end{fact}





Least squares regressions can be solved approximately using spectral approximations. See Section~2.5 of \cite{w14} for details.
\begin{lemma}[Approximate least squares regression from spectral approximation]\label{lem:approx_l2_regression_from_spectral_approx}
Given $\bA \in \R^{n \times d}$ and $\bb \in \R^n$, define a matrix $\bM := [\bA, \bb] \in \R^{n \times (d+1)}$. Let $\bD \in \R^{n' \times n}$ be a subspace embedding that satisfies $\bM^{\top} \bD^{\top} \bD \bM \approx_{\epsilon} \bM^{\top} \bM$.
Define $\bx \in \R^d$ to be
\begin{align*}
    \bx := \arg \min_{\bx' \in \R^d} \|\bD \bA \bx' - \bD \bb\|_2.
\end{align*}
Then with probability at least $1 - \delta$, $\bx$ satisfies
\begin{align*}
\|\bA \bx - \bb\|_2 \leq (1 + \epsilon) \min_{\bx' \in \R^d} \|\bA \bx' - \bb\|_2.
\end{align*}
\end{lemma}



We note the standard least squares regression has a closed-form solution.
\begin{fact}[Closed-form formula for least squares regression]
\label{fact:closed-form}
Let $n \geq d$ be two integers. For any matrix $\bA \in \R^{n \times d}$ with rank $d$, and any vector $\bb \in \R^d$, the vector 
$\bx^* := \bA^{\dagger} \cdot \bb = (\bA^{\top} \bA)^{-1} \bA^{\top} \cdot \bb$ satisfies
\begin{align*}
    \|\bA \bx^* - \bb\|_2 = \min_{\bx \in \R^d} \|\bA \bx - \bb\|_2.
\end{align*}
\end{fact}





