\section{Partially dynamic LSR with incremental updates}
\label{sec:upper}
In this section, we present an algorithm for the partially dynamic least-squares regression problem with \emph{incremental updates}.



\begin{theorem}[Partially dynamic LSR with incremental updates, formal version of Theorem \ref{thm:main_UB_informal}]\label{thm:upper}
Let $d, T \in \mathbb{N}$ and $0< \eps, \delta <1/8$.
Assume the least singular value of $\bM^{(0)}$ is at least $\sigma_{\min}$, and the largest singular value of $\bM^{(T)}$ is at most $\sigma_{\max}$. 
For partially dynamic least-squares regression incremental updates, there exists a randomized algorithm (Algorithm \ref{algo:preprocess}--\ref{algo:update-member}) that with probability at least $1-\delta$, maintains an $\eps$-approximation solution for all iterations $t\in [T]$. 
For oblivious adversary, the total update time is at most 
\[
O\Big(\nnz(\bA^{(T)}) \log(\frac{T}{\delta}) + \epsilon^{-4} d^3 \log^2(\frac{\sigma_{\max}}{\sigma_{\min}}) \log^3(\frac{T}{\delta})\Big),
\]  
For adaptive adversary, the total update time is at most 
\[
O\Big(\nnz(\bA^{(T)}) \log(\frac{T}{\delta}) + \epsilon^{-4} d^5 \log^4(\frac{\sigma_{\max}}{\sigma_{\min}}) \log^3(\frac{T}{\delta}) \Big).
\]  
\end{theorem}




\subsection{Data structure}\label{sec:data_structure}
A complete description of our data structure can be found at Algorithm \ref{algo:preprocess}--\ref{algo:update-member}.
Our approach follows the online row sampling framework \cite{cmp20}. 
When a new row arrives, we sample and keep the new row with probability proportional to the online leverage score, which is approximately computed using JL embedding (Algorithm \ref{algo:sample} Line \ref{line:levarage_score}).
If the row is sampled, then we update the data structure (Algorithm \ref{algo:update-member} Line \ref{line:Delta_H}--\ref{line:update-N}, Line \ref{line:update-G}--\ref{line:update-x}) using Woodbury identity, and instantiate a new JL sketch (Line \ref{line:new-JL}--\ref{line:update-wt_B}. See below for the JL lemma); otherwise, we do not perform any updates.

\begin{lemma}[Johnson-Lindenstrauss Lemma \cite{jl84}]\label{lem:JL}
There exists a function $\textsc{JL}(n,m,\epsilon, \delta)$ that returns a random matrix $\bJ \in \R^{k \times n}$ where $k = O(\epsilon^{-2} \log(m/\delta))$, and $\bJ$ satisfies that for any fixed $m$-element subset $V \subset \R^n$,
\begin{align*}
    \Pr\big[\forall \bv \in V, ~ (1 - \epsilon) \|\bv\|_2 \leq \|\bJ \bv\|_2 \leq (1 + \epsilon) \|\bv\|_2\big] \geq 1 - \delta.
\end{align*}
Furthermore, the function $\textsc{JL}$ runs in $O(kn)$ time.
\end{lemma}




\paragraph{Notation} We use superscripts $^{(t)}$ to denote the matrix/vector/scalar maintained by the data structure at the end of the $t$-th iterations. In particular, the superscript $^{(0)}$ represents the variables after the preprocessing step. 




\begin{algorithm}[!htbp]
\caption{\textsc{Preprocess} ($\bA$, $\bb$, $\eps$, $\delta$, $T$)}
\label{algo:preprocess}
\begin{algorithmic}[1]
    \State $\bM \leftarrow [\bA, \bb]$ \Comment{Input matrix $\bM \in \R^{(d+1) \times (d+1)}$}
    \State $\bD \leftarrow \bI_{d+1}$  \Comment{Sampling matrix $\bD \in \R^{(d+1)\times(d+1)}$}
    \State $s \leftarrow d+1$ \Comment{The number of sampled rows}
    \State $\bN \leftarrow \bD \cdot \bM$ \Comment{Sampled rows $\bN \in \R^{s \times (d+1)}$} 
    \State $\bH \leftarrow ((\bN)^{\top} \bN )^{-1}$ \label{line:H_0} \Comment{$\bH \in \R^{(d+1) \times (d+1)}$}
    \State $\bB \leftarrow \bN \cdot \bH$ \Comment{$\bB \in \R^{s \times (d+1)}$}
    \State $\bJ \leftarrow \textsc{JL}(s, T, \frac{1}{100}, \frac{\delta}{2 T^2})$ \Comment{JL embedding $\bJ \in \R^{O(\log(T/\delta)) \times s}$}
    \State $\wt{\bB} \leftarrow \bJ \cdot \bB$ \label{line:wt_B_0} \Comment{Used for online LS estimation $\wt{\bB}\in \R^{O(\log(T/\delta)) \times (d+1)}$}
    \State $\bG \leftarrow (\bA^{\top}\bD^2 \bA)^{-1}$ \label{line:G_0} \Comment{$\bG \in \R^{d \times d}$}
    \State $\bu \leftarrow \bA^{\top} \bD^2 \bb$ \Comment{$\bu \in \R^{d}$}
    \State $\bx \leftarrow \bG\cdot \bu$ \Comment{(Approximate) solution $\bx \in \R^d$}
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[!hbtp]
\caption{\textsc{Insert} ($\ba, \beta$) \Comment{Insert a new row $(\ba, \beta)\in \R^d \times \R$}}
\label{algo:update}
\begin{algorithmic}[1]
\State $\boldm \leftarrow [\ba^{\top}, \beta]^{\top}$ \Comment{$\boldm \in \R^{d+1}$} 
\State $\nu \leftarrow \textsc{Sample}(\boldm)$\Comment{$\nu \in \R$}\label{line:sample_in_update}
\State $\bD \leftarrow
\begin{bmatrix}
\bD & 0 \\
0 & \nu
\end{bmatrix} 
$
\State \textbf{if} $\nu \neq 0$ \textbf{then} \textsc{UpdateMembers}($\boldm$)  \label{line:if_start}
\State \Return $\bx$
\end{algorithmic}
\end{algorithm}




\begin{algorithm}[!htbp]
\caption{\textsc{Sample} ($\boldm$)}
\label{algo:sample}
\begin{algorithmic}[1]
\State $C_{\mathrm{obl}} \leftarrow 10 \epsilon^{-2} \log(2T/\delta)$, $C_{\mathrm{adv}} \leftarrow 32 (1 + \epsilon) d \log(\frac{\sigma_{\max}}{\sigma_{\min}}) \cdot C_{\mathrm{obl}}$
\State $\tau \leftarrow \|\wt{\bB} \cdot \boldm\|_2^2$ \Comment{Approximate online LS} \label{line:approximate-ols}
\label{line:levarage_score}
\If{\texttt{Oblivious adversary}} \Comment{Oblivious adversary}
\State $p \leftarrow \min\{C_{\mathrm{obl}} \cdot \tau, 1\}$
\Else \Comment{Adaptive adversary}
\State $p \leftarrow \min\{C_{\mathrm{adv}} \cdot \tau, 1\}$
\EndIf 
\label{line:prob}
\State $\nu \leftarrow 1/\sqrt{p}$ with probability $p$, and $\nu \leftarrow 0$ otherwise\label{line:sample}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!htbp]
\caption{\textsc{UpdateMembers} ($\boldm$)}
\label{algo:update-member}
\begin{algorithmic}[1]
\Statex  \texttt{// Update spectral approximation}
\State $s \leftarrow s + 1$ \Comment{The number of sampled rows}
\State $\Delta \bH \leftarrow - \frac{\bH \boldm \boldm^{\top} \bH / p}{1 + \boldm^{\top} \bH \boldm / p}$  \label{line:Delta_H} 
\State $\bH \leftarrow \bH +\Delta \bH$ \Comment{Update $\bH \in \R^{(d+1) \times (d+1)}$}
\State $\bB\leftarrow [ ( \bB + \bN \cdot \Delta \bH )^{\top}, ~ \bH \cdot \boldm / \sqrt{p}]^{\top}$\label{line:update-B} \Comment{Update $\bB \in \R^{s \times (d+1)}$}
\State $\bN \leftarrow [\bN^{\top}, \boldm / \sqrt{p}]^{\top}$ \label{line:update-N} \Comment{Update $\bN \in \R^{s \times (d+1)}$}
\State $\bJ \leftarrow \textsc{JL}(s, T, \frac{1}{100}, \frac{\delta}{2 T^2})$ \Comment{Instantiate a new JL sketch, $\bJ \in \R^{O(\log(T/\delta)) \times s}$}\label{line:new-JL}
\State $\wt{\bB}\leftarrow \bJ \cdot \bB$\label{line:update-wt_B} \Comment{Update $\wt{\bB} \in \R^{O(\log(T/\delta)) \times (d+1)}$}
\Statex  \texttt{// Update solution }
\State $\bG \leftarrow \bG - \frac{\bG \ba \ba^{\top} \bG / p}{1 + \ba^{\top} \bG \ba / p}$ \label{line:update-G}
\Comment{Woodbury identity, update $\bG \in \R^{d \times d}$}
\State $\bu \leftarrow \bu + \beta \cdot \ba/ p$\label{line:update-u}\Comment{Update $\bu \in \R^d$}
\State $\bx \leftarrow \bG\cdot \bu$ \Comment{Update $\bx \in \R^d$} \label{line:update-x}
\end{algorithmic}
\end{algorithm}







We summarize all variables maintained by our data structure and their closed-form formulas. The proof can be found in Appendix \ref{sec:upper-app}.

\begin{lemma}[Closed-form formulas]
\label{lem:close_form_formula_algorithm}
At the $t$-th iteration of $\textsc{Insert}$ (Algorithm~\ref{algo:update}), we have
\begin{enumerate}
    \item $\bM^{(t)} = [\bA^{(t)}, \bb^{(t)}] \in \R^{(d+t+1) \times (d+1)}$ is the input matrix.
    \item $\bD^{(t)} \in \R^{(d+t+1) \times (d+t+1)}$ is a diagonal matrix with $s^{(t)}$ non-zero entries.
    \item $\bN^{(t)} = (\bD^{(t)} \bM^{(t)})_{S^{(t)}, *} \in \R^{s^{(t)} \times (d+1)}$ takes rows of $\bM^\ttop$, where $S^{(t)} \subset [d+t+1]$ is the set of non-zero entries of $\bD^{(t)}$.
    \item $\bH^{(t)} = \big( (\bN^{(t)})^{\top} \bN^{(t)} \big)^{-1} \in \R^{(d+1) \times (d+1)}$.
    \item $\bB^{(t)} = \bN^{(t)} \bH^{(t)} \in \R^{s^{(t)} \times (d+1)}$.
    \item $\wt{\bB}^{(t)} = \bJ^{(t)} \cdot \bB^{(t)} \in \R^{O(\log(T/\delta)) \times (d+1)}$ is used for approximately estimating the online leverage score.
    \item $\bG^{(t)} = \big( (\bA^{(t)})^{\top} (\bD^{(t)})^2 \bA^{(t)} \big)^{-1} \in \R^{d \times d}$.
    \item $\bu^{(t)} = (\bA^{(t)})^{\top}(\bD^{(t)})^2 \bb^{(t)} \in \R^d$.
    \item $\bx^{(t)} = \big( (\bA^{(t)})^{\top} (\bD^{(t)})^2 \bA^{(t)} \big)^{-1} \cdot (\bA^{(t)})^{\top} (\bD^{(t)})^2 \bb^{(t)} \in \R^d$ is the maintained solution. 
\end{enumerate}
\end{lemma}





\subsection{Warm up: Analysis for oblivious adversary}
\label{sec:correct-oblivious}

We first prove the correctness against an oblivious adversary (i.e., our data structure maintains an $\eps$-approximate solution w.h.p.) and the runtime analysis is deferred to Section \ref{sec:time}.
The proof follows easily from the guarantee of online leverage score sampling \cite{cmp20} and the JL sketch, and it serves as a warm up for the more complicated algorithm against an adaptive adversary.
As we shall see later, both guarantees become nontrivial when facing an adaptive adversary.





The key advantage for the oblivious setting is that we can fix the input sequence $\boldm^{(1)}, \ldots, \boldm^{(T)}$ for analysis. We exploit the following guarantee of online leverage score sampling, which is a direct corollary from matrix Freedman inequality. For completeness we include a proof in Appendix~\ref{sec:upper-app}.
\begin{lemma}[Online leverage score sampling, adapted from Lemma 3.3 of \cite{cmp20}]\label{lem:spectral-online-leverage-score}
Let $\epsilon, \delta \in (0,1/2)$ be two parameters. Let $\boldm^{(1)}, \ldots, \boldm^{(T)} \in \R^{d+1}$ be a fixed sequence and let $\tauo^\ttop$ be the online leverage score of the $t$-th row, i.e., $\tauo^\ttop:= (\boldm^\ttop)^\top ((\bM^{(t-1)})^\top\bM^{(t-1)})^{-1}\boldm^\ttop$. 
Suppose an algorithm samples the $t$-th row with probability\footnote{The sampling probability could depend on the result of previous sampling outcomes.}
\begin{align*}
p_t \geq \min\{3 \epsilon^{-2} \tauo^\ttop \log(d / \delta), 1\}.
\end{align*}
Define $\nu_t \in \R$ as
\begin{align*}
    \nu_t = 
    \begin{cases}
    \frac{1}{\sqrt{p_t}}, & \text{if the } t\text{-th row is sampled},  \\
    0, & \text{otherwise.}
    \end{cases}
\end{align*}
Then with probability at least $1 - \delta$, $(\bM^{(0)})^\top \bM^{(0)} + \sum_{t=1}^{T}\nu_t^2\cdot \boldm^\ttop (\boldm^\ttop)^\top$ is an $\epsilon$-spectral approximation of $(\bM^{(T)})^\top \bM^{(T)}$.
\end{lemma}


Our data structure maintains an $\eps$-spectral approximation of $(\bM^{(t)})^\top \bM^{(t)}$ and uses it to approximate the online leverage score.

\begin{lemma}[Spectral approximation]
\label{lem:sampling_probability}
With probability at least $1 - \delta/T$, for any $t \in [T]$,
\begin{align}\label{eq:tau_leverage_score}
    0.9 (1-\eps) \cdot \tauo^\ttop \leq \tau^{(t)} \leq 1.1 (1+\eps) \cdot \tauo^\ttop
\end{align}
and for any $t \in [0: T]$ 
\begin{align}\label{eq:spectral_approximation}
    (\bM^{(t)})^{\top} (\bD^{(t)})^2 \bM^{(t)} \approx_{\epsilon} (\bM^{(t)})^{\top} \bM^{(t)}.
\end{align}
\end{lemma}
\begin{proof}
We prove the claim inductively. Let $\delta' = \frac{\delta}{2T^2}$, the induction hypothesis is that with probability $1 - 2 t \delta'$, Eq.~\eqref{eq:spectral_approximation} holds for all $t' \in [0:t]$ and Eq.~\eqref{eq:tau_leverage_score} holds for all $t' \in [t]$. 
The base case $t=0$ holds trivially as $\bD^{(0)} = \bI_{d+1}$. Suppose the induction hypothesis continues to hold for $t-1$, then for the $t$-th iteration, we have
\begin{align}
    \|\bB^{(t-1)} \cdot \boldm^{(t)}\|_2^2 = &~ (\boldm^{(t)})^{\top} \cdot (\bH^{(t-1)})^{\top} (\bN^{(t-1)})^{\top} \bN^{(t-1)} \bH^{(t-1)} \cdot \boldm^{(t)} \notag \\
    = &~ (\boldm^{(t)})^{\top} \cdot \big((\bN^{(t-1)})^{\top} \bN^{(t-1)} \big)^{-1} \cdot \boldm^{(t)} \notag \\
    = &~ (\boldm^{(t)})^{\top} \cdot \big( (\bM^{(t-1)})^{\top} (\bD^{(t-1)})^2 \bM^{(t-1)} \big)^{-1} \cdot \boldm^{(t)} \notag \\
    = &~ (1 \pm \epsilon) \cdot (\boldm^{(t)})^{\top} \cdot \big( (\bM^{(t-1)})^{\top} \bM^{(t-1)} \big)^{-1} \cdot \boldm^{(t)} \notag \\
    = &~ (1 \pm \epsilon) \cdot \tauo^\ttop \label{eq:jl1}
\end{align}
The first three steps follow from the closed-form formula (Lemma \ref{lem:close_form_formula_algorithm}), the fourth step holds due to the induction hypothesis and the last step comes from the definition of online leverage score.

Meanwhile, using the JL Lemma (Lemma~\ref{lem:JL}), we have that with probability at least $1 - \delta'$, 
\begin{align}
    \tau^\ttop = \|\wt{\bB}^{(t-1)} \cdot \boldm^{(t)}\|_2^2  = \|\mathbf{J}^{(t-1)} \bB^{(t-1)} \cdot \boldm^{(t)}\|_2^2 = (1\pm 0.1) \|\bB^{(t-1)} \cdot \boldm^\ttop\|_2^2\label{eq:jl2}
\end{align}
where the last step holds due to the JL Lemma and $\boldm^\ttop$, $\bB^{(t-1)}$ are independent of the entries of $\bJ^{(t-1)}$.

Combining Eq.~\eqref{eq:jl1}\eqref{eq:jl2}, we finish the induction of Eq.~\eqref{eq:tau_leverage_score}. The spectral approximation guarantee of Eq.~\eqref{eq:spectral_approximation} follows directly from Lemma \ref{lem:spectral-online-leverage-score} and our choice of parameters. We finish the proof here.\qedhere
\end{proof}






It is well known that spectral approximations of $(\bM^{(t)})^{\top} \bM^{(t)}$ give approximate solutions to least squares regressions \cite{w14}, so we have proved the correctness of our algorithm.
\begin{lemma}[Correctness of Algorithm \ref{algo:preprocess}--\ref{algo:update-member}, oblivious adversary]\label{lem:correctness_algorithm}
With probability at least $1 - \delta/T$, in each iteration, \textsc{Insert} of Algorithm~\ref{algo:update} outputs a vector $\bx^{(t)} \in \R^d$ such that 
    \[
    \|\bA^{(t)} \bx^{(t)} - \bb^{(t)}\|_2 \leq (1+\eps) \min_{\bx \in \R^d} \|\bA^{(t)} \bx - \bb^{(t)} \|_2.
    \]
against an oblivious adversary.
\end{lemma}
\begin{proof}
Note by Lemma~\ref{lem:close_form_formula_algorithm}, one has $\bx^{(t)} = \big( (\bA^{(t)})^{\top} (\bD^{(t)})^2 \bA^{(t)} \big)^{-1} \cdot (\bA^{(t)})^{\top} (\bD^{(t)})^2 \bb^{(t)}$, which is the closed-form minimizer of $\|\bD^{(t)} \bA^{(t)} \bx - \bD \bb^{(t)}\|_2$.
From Eq.~\eqref{eq:spectral_approximation} in Lemma~\ref{lem:sampling_probability}, we know that with probability at least $1 - \delta/T$, we have $(\bM^{(t)})^{\top} (\bD^{(t)})^2 \bM^{(t)} \approx_{\epsilon} (\bM^{(t)})^{\top} \bM^{(t)}$. 
Using this spectral approximation and Lemma~\ref{lem:approx_l2_regression_from_spectral_approx}, we conclude the proof of this lemma.
\end{proof}




