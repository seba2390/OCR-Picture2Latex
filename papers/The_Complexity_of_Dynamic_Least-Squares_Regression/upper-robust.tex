\subsection{Analysis for adaptive adversary}\label{sec:upper_robust}



Next, we prove our data structure (Algorithm~\ref{algo:preprocess} --\ref{algo:update-member}) is adversarially robust and it works against an adaptive adversary when using a larger sampling constant $C_{\mathrm{adv}} = 32 (1 + \epsilon) d \log(\frac{\sigma_{\max}}{\sigma_{\min}}) \cdot C_{\mathrm{obl}}$.


First, we prove that online leverage score sampling works against an adaptive adversary. In contrast with the counterpart Lemma \ref{lem:sampling_probability} of the oblivious setting, the sequence $\boldm^{(1)}, \ldots, \boldm^{(T)}$ is not fixed but chosen adaptively based on previous outcomes. The proof becomes more challenging due to this adaptivity.
\begin{lemma}[Intrinsic robustness of online leverage score sampling]
\label{lem:intrinsic_new}
Let $\eps, \delta \in (0, 1/8)$. 
Let $\boldm^{(1)}, \ldots, \boldm^{(T)} \in \R^{d+1}$ be an adaptive sequence of row vectors chosen by an adaptive adversary and let $\tauo^\ttop$ be the online leverage score of the $t$-th row, i.e., $\tauo^\ttop = (\boldm^{(t)})^\top ((\bM^{(t-1)})^\top \bM^{(t-1)})^{-1} \boldm^{(t)}$. If an algorithm samples the $t$-th row with probability 
\[
p_{t} \geq \min\{\alpha \cdot \tauo^\ttop, 1\}, \text{ where } \alpha = 300 d \epsilon^{-2} \log(\frac{400d \sigma_{\max}}{\epsilon \delta \sigma_{\min} }),
\]
that is,
\begin{align*}
\bX^\ttop =  \left\{
\begin{matrix}
\frac{1}{p_t} \cdot  \boldm^\ttop (\boldm^\ttop)^\top & \text{w.p. } p_t,\\
\mathbf{0} & \text{w.p. } 1 - p_t.\\
\end{matrix}
\right.
\end{align*}
Then with probability at least $1-\delta$, its output $\bY := (\bM^{(0)})^\top \bM^{(0)} + \sum_{t=1}^{T}\bX^{(t)}$ is an $\eps$-spectral approximation of $(\bM^{(T)})^\top \bM^{(T)}$.
\end{lemma}



We note Lemma \ref{lem:intrinsic_new} improves Lemma B.2 of \cite{bhm+21}. Concretely, Lemma B.2 in \cite{bhm+21} has an $\wt{O}(d (\frac{\sigma_{\max}}{\sigma_{\min}})^2)$ overhead comparing with the oblivious case, while we reduce the overhead to $\wt{O}(d \log (\frac{\sigma_{\max}}{\sigma_{\min}}))$, which has only polylogarithmic dependence on the condition number $\frac{\sigma_{\max}}{\sigma_{\min}}$.




We make use of the Freedman's inequality for martingales. 
\begin{lemma}[Freedman's inequality, \cite{freedman1975tail}]\label{thm:freedman}
Consider a martingale $Y_0, Y_1, \cdots, Y_n$ with difference sequence $X_1, X_2, \cdots, X_n$, i.e., $Y_0 = 0$, and for all $i \in [n]$, $Y_i = Y_{i-1} + X_i$ and $\E_{i-1}[Y_i] = Y_{i-1}$. Suppose $|X_i| \leq R$ almost surely for all $i \in [n]$. Define the predictable quadratic variation process of the martingale as $W_i = \sum_{j=1}^{i} \E_{j-1}[X_j^2]$, for ill $i \in [n]$. Then for all $u \geq 0$, $\sigma^2 > 0$,
\[
\Pr\left[ \exists i \in [n]: |Y_i| \geq u \text{ and } W_i \leq \sigma^2 \right] \leq 2 \exp\left(-\frac{u^2/2}{\sigma^2 + R u / 3}\right)
\]
\end{lemma}






We now turn to the proof of Lemma \ref{lem:intrinsic_new}.
The first step is similar to \cite{bhm+21}, we take an union bound over the $\eps$-net of unit vectors $\bx\in \R^{d+1}$ and reduce to the scalar case (the union bound gives an $\wt{O}(d)$ overhead).
When applying Freedman's inequality, the term $\|\bM^{(T)} \bx\|_2$ shows up, and it is unknown since the rows of $\bM^{(T)}$ are chosen adaptively. 
\cite{bhm+21} uses a straightforward bound of $\sigma_{\min} \|\bx\|_2 \leq \|\bM^{(T)} \bx\|_2 \leq \sigma_{\max} \|\bx\|_2$, which results in another $O((\frac{\sigma_{\max}}{\sigma_{\min}})^2)$ overhead. 
We instead consider $O(\frac{\sigma_{\min}}{\sigma_{\max}})$ number of (truncated) martingales and prove that one of them correctly guesses the value of $\|\bM^{(T)} \bx\|_2$. By taking an union bound over these $O(\frac{\sigma_{\min}}{\sigma_{\max}})$ martingales, it only has an $O(\log(\frac{\sigma_{\max}}{\sigma_{\min}}))$ overhead.


\begin{proof}[Proof of Lemma~\ref{lem:intrinsic_new}]
We first introduce some notations. 
Let 
\[
\kappa = \frac{\sigma_{\max}}{\sigma_{\min}}, \quad \delta' = \delta \cdot \left(\frac{200 \kappa d}{\epsilon}\right)^{-d}, \quad \text{and} \quad \delta'' = \frac{\delta'}{10 \kappa}.
\]
For any $t \in [T]$, let $\mathcal{F}_{t}$ be the $\sigma$-algebra generated by the adaptive sequence $\boldm^{(1)}, \cdots, \boldm^{(t+1)}$ and $\bX^{(1)}, \cdots, \bX^{(t)}$. Note that $\mathcal{F}_0 \subseteq \mathcal{F}_1 \subseteq \cdots \subseteq \mathcal{F}_T$ is a filtration. We use the notation $\E_{t}[\cdot] = \E[\cdot \mid \mathcal{F}_t]$ to denote the expectation conditioned on $\mathcal{F}_t$.


\vspace{+2mm}
{\bf Step 1.} The key step is to prove that for any fixed vector $\bx \in \R^d$, with probability at least $1 - \delta''$, 
\begin{align}
|\bx^{\top} \bY \bx - \|\bM^{(T)} \bx\|_2^2 | \leq \frac{\epsilon}{2} \cdot \|\bM^{(T)}\bx\|_2^2.\label{eq:upper-goal1} 
\end{align}
To this end, define a set 
\[
\mathcal{S} := \Big\{k \cdot \frac{\sigma_{\min}}{10} \|\bx\|_2 ~\Big|~ k \in \mathbb{Z}_{\geq 1}, \text{ and } \sigma_{\min} \leq k \cdot \frac{\sigma_{\min}}{10} \leq \sigma_{\max}\Big\}.
\]
The size of $\mathcal{S}$ is $|\mathcal{S}| = 10 \kappa$. 

For any value $s \in \mathcal{S}$, define the random sequence $\{\overline{x}_{s}^\ttop\}_{t\in [T]}$
\begin{align}
\overline{x}_s^{(t)} =
\begin{cases}
\bx^{\top} \bX^{(t)} \bx - (\bx^{\top} \boldm^{(t)})^2 & \text{if } \|\bM^{(t)} \bx\|_2 \leq s, \\
0 & \text{otherwise.}
\end{cases} \label{eq:definition-ox}
\end{align}
and  $\{\overline{y}_{s}^\ttop\}_{t\in [0:T]}$
\[
\overline{y}_s^{(0)} = 0\quad \text{and} \quad \overline{y}_s^{(t)} = \overline{y}_s^{(t-1)} + \overline{x}_s^{(t)},\quad \forall t \in [T].
\]

The sequence $\{\overline{y}_s^{(t)}\}_{t\in [0:T]}$ forms a martingale. To see this, by the definition of $\bX^{(t)}$, we have $\E_{t-1}[\bX^{(t)}] = \boldm^{(t)} (\boldm^{(t)})^{\top}$ and $\E_{t-1}[\bx^{\top} \bX^{(t)} \bx] = (\bx^{\top} \boldm^{(t)})^2$. This means $\E_{t-1}[\overline{x}_s^{(t)}] = 0$, and therefore
\[
\E_{t-1}[\overline{y}_s^{(t)}] = \E_{t-1}[\overline{y}_s^{(t-1)} + \overline{x}_s^{(t)}] = \overline{y}_s^{(t-1)}.
\]

We wish to apply the Freedman inequality to $\{\overline{y}_s^\ttop\}_{t \in [T]}$, and we bound the maximum deviation and the variance separately.
\begin{itemize}
\item First, we prove $|\overline{x}_s^{(t)}| < \frac{s^2}{\alpha}$. 

If $\|\bM^{(t)} \bx\|_2 > s$, then  $\overline{x}_s^{(t)} = 0$ due to the definition in Eq.~\eqref{eq:definition-ox}.
If $p_t = 1$, then we have $\bX^{(t)} = \boldm^{(t)} (\boldm^{(t)})^{\top}$ with probability $1$, and therefore, $\overline{x}_s^{(t)} = \bx^{\top} \bX^{(t)} \bx - (\bx^{\top} \boldm^{(t)})^2 = 0$.
Finally, suppose $\|\bM^{(t)} \bx\|_2 \leq s$ and $p_t < 1$, we have $p_t \geq \alpha \cdot \tauo^\ttop$, and
\begin{align*}
|\overline{x}_s^{(t)}| \leq \frac{1}{p_t} \cdot (\bx^{\top} \boldm^{(t)})^2 \leq \frac{1}{\alpha \cdot \tauo^\ttop} \cdot (\bx^{\top} \boldm^{(t)})^2 
\leq \frac{\|\bM^{(t)} \bx\|_2^2}{\alpha} \leq \frac{s^2}{\alpha},
\end{align*}
where the first step follows from the definition of $\overline{x}_s^{(t)}$ and $0 < \frac{1}{p_t} - 1 < \frac{1}{p_t}$, the third step follows from the property of online leverage score (Fact \ref{fact:online_leverage_score}).
We conclude with $|\overline{x}_s^{(t)}| \leq \frac{s^2}{\alpha}$.

\item Then, we bound the variance. Similarly, if $\|\bM^{(t)} \bx\|_2 > s$ or $p_t = 1$, then we have $\overline{x}_s^{(t)} = 0$.
Otherwise, suppose $\|\bM^{(t)} \bx\|_2 \leq s$ and $p_t < 1$, then we have $p_t \geq \alpha \cdot \tauo^\ttop$, and
\begin{align*}
\E_{t-1}[(\overline{x}_s^{(t)})^2] = &~ p_t \cdot (\frac{1}{p_t}-1)^2 (\bx^{\top} \boldm^{(t)})^4 + (1 - p_t) \cdot (\bx^{\top} \boldm^{(t)})^4 \\
\leq &~ \frac{1}{p_t} \cdot (\bx^{\top} \boldm^{(t)})^4 
\leq \frac{\|\bM^{(t)} \bx\|_2^2 \cdot (\bx^{\top} \boldm^{(t)})^2}{\alpha} \leq \frac{s^2 \cdot (\bx^{\top} \boldm^{(t)})^2}{\alpha},
\end{align*}
where the third step follows from $p_t \geq \alpha \cdot \tauo^\ttop$ and Fact~\ref{fact:online_leverage_score}, the last step follow from the assumption of $\|\bM^{(t)} \bx\|_2 \leq s$.
Hence, we conclude that
\[
\E_{t-1}[(\overline{x}_s^{(t)})^2] \leq \frac{s^2 \cdot (\bx^{\top} \boldm^{(t)})^2}{\alpha} \cdot \mathbf{1}_{\|\bM^{(t)} \bx\|_2 \leq s},
\] 
where the indicator variable $\mathbf{1}_{\|\bM^{(t)} \bx\|_2 \leq s}$ is $1$ if $\|\bM^{(t)} \bx\|_2 \leq s$ and $0$ otherwise.

Let $t^*$ be the largest index in $[T]$ such that $\|\bM^{(t^*)} \bx\|_2 \leq s$, then we have
\begin{align*}
\sum_{t=1}^{t^{*}} \E_{t-1}[(\overline{x}_s^{(t)})^2] \leq \sum_{t=1}^{t^{*}} \frac{s^2 \cdot (\bx^{\top} \boldm^{(t)})^2}{\alpha} \cdot \mathbf{1}_{\|\bM^{(t)} \bx\|_2 \leq s} = \frac{s^2 \cdot \|\bM^{(t^*)} \bx\|_2^2}{\alpha} \leq \frac{s^4}{\alpha}.
\end{align*}
\end{itemize}


Now we can apply Freedman's inequality (Lemma~\ref{thm:freedman}) to the sequence $\overline{y}_s^{(0)}, \overline{y}_s^{(1)}, \cdots, \overline{y}_s^{(T)}$ with parameters $R = \frac{s^2}{\alpha}$, $\sigma^2 = \frac{s^4}{\alpha}$, and $u = \frac{\epsilon}{8} s^2$:
\begin{align*}
\Pr\Big[|\overline{y}_s^{(T)}| \geq  \frac{\epsilon}{8} s^2\Big] \leq &~ 2 \exp\left(-\frac{u^2/2}{\sigma^2 + Ru / 3}\right) = 2 \exp\left(-\frac{\epsilon^2 s^4/128}{s^4/\alpha + \epsilon s^4 / (24 \alpha)}\right)\\
\leq &~ 2 \exp\left(-\epsilon^2 \alpha / 200\right) \leq \delta'',
\end{align*}
where the last step follows from the choice of $\alpha = 300 d \epsilon^{-2} \log(\frac{400 \kappa d}{\epsilon \delta}) > 200 \epsilon^{-2} \log(2/\delta'')$.

Taking a union bound over $\mathcal{S}$, and since $|\mathcal{S}| = 10 \kappa$, we have that with probability at least $1 - \delta'' \cdot 10 \kappa = 1 - \delta'$, 
\begin{align}
|\overline{y}_s^{(T)}| < \frac{\epsilon}{8}\cdot s^2, \quad  \forall s \in \mathcal{S}.
\label{eq:union1}
\end{align}
Meanwhile, for any realization of $\boldm^{(1)}, \cdots, \boldm^{(T)}$, there must exist an $s^* \in \mathcal{S}$ such that 
\[
\|\bM^{(T)} \bx\|_2 \leq s^* \leq 2 \|\bM^{(T)} \bx\|_2.
\]
This is because (1) $\sigma_{\min} \cdot \|\bx\|_2 \leq \|\bM^{(T)} \bx\|_2 \leq \sigma_{\max} \cdot \|\bx\|_2$,  
and (2) the gap between two values in $\mathcal{S}$ is $\frac{\sigma_{\min}}{10} \|\bx\|_2 \leq \frac{1}{10} \|\bM^{(T)} \bx\|_2$. 

We have $\overline{x}_{s^*}^{(t)} = \bx^{\top} \bX^{(t)} \bx - (\bx^{\top} \boldm^{(t)})^2$ for all $t \in [T]$. 
Conditioned on the high probability event of Eq.~\eqref{eq:union1}, we conclude that
\[
|\bx^{\top} \bY \bx - \|\bM^{(T)} \bx\|_2^2 | = |\overline{y}_{s^*}^{(T)}| \leq \frac{\epsilon}{8} (s^*)^2 \leq \frac{\epsilon}{2} \|\bM^{(T)} \bx\|_2^2.
\]
This proves Eq.~\eqref{eq:upper-goal1}

\vspace{+2mm}
{\bf Step 2} Next we prove that $\bY$ is an $\eps$-spectral approximation to $(\bM^{(T)})^\top \bM^{(T)}$ with high probability.
Define the set 
\[
\mathcal{B} := \{(x_1, x_2, \cdots, x_d) \in [-1, 1]^d \mid \forall i, x_i = k \cdot \frac{\epsilon}{100 \kappa d} \text{ for some } k \in \mathbb{Z}\}.
\]
Note that the size of $\mathcal{B}$ is $(\frac{200 \kappa d}{\epsilon})^d$.


Taking a union bound over $\mathcal{B}$, we have that with probability at least $1 - \delta' \cdot (\frac{200 \kappa d}{\epsilon})^d = 1 - \delta$, 
\begin{align}
|\bx^{\top} \bY \bx - \|\bM^{(T)} \bx\|_2^2 | \leq \frac{\epsilon}{2} \|\bM^{(T)} \bx\|_2^2, \quad \forall \bx \in \mathcal{B}.\label{eq:adaptive-upper3}
\end{align}

We will condition on this event. For any unit vector $\bx^{*} \in \R^{d}$, there must exist some $\bx \in \mathcal{B}$ such that $|x_i - x^*_i| \leq \frac{\epsilon}{100 \kappa d}$ for all $i \in [d]$, and therefore $\|\bx - \bx^*\|_2 \leq \frac{\epsilon}{100 \kappa}$.  
Now, we have
\begin{align*}
|\|\bM^{(T)} \bx\|_2 - \|\bM^{(T)} \bx^*\|_2| \leq &~ \|\bM^{(T)} (\bx - \bx^*)\|_2 \\
\leq &~ \sigma_{\max} \cdot \frac{\epsilon}{100 \kappa} \leq \frac{\epsilon}{100} \cdot \|\bM^{(T)} \bx^{*}\|_2,
\end{align*}
where the second step follows from the largest singular value of $\bM^{(T)}$ is at most $\sigma_{\max}$ and $\|\bx - \bx^*\|_2 \leq \frac{\epsilon}{100 \kappa}$, the last step follows from the least singular value of $\bM^{(T)}$ is at least $\sigma_{\min}$ and $\kappa = \frac{\sigma_{\max}}{\sigma_{\min}}$. We then have 
\begin{align}
|\|\bM^{(T)} \bx\|_2^2 - \|\bM^{(T)} \bx^*\|_2^2| = &~ (\|\bM^{(T)} \bx\|_2 + \|\bM^{(T)} \bx^*\|_2) \cdot |\|\bM^{(T)} \bx\|_2 - \|\bM^{(T)} \bx^*\|_2| \notag \\
\leq &~ \frac{\epsilon}{40} \|\bM^{(T)} \bx^{*}\|_2^2.\label{eq:adaptive-upper1}
\end{align}

Similarly, we have 
\[
|\|\bY^{1/2} \bx\|_2 - \|\bY^{1/2} \bx^*\|_2| \leq \frac{\epsilon}{50} \cdot \|\bM^{(T)} \bx^{*}\|_2,
\]
this comes from the fact that the largest singular value of $\bY$ is at most $4 \sigma_{\max}^2$ (see Claim \ref{claim:upper-tech1}). Hence, 
\begin{align}
\label{eq:adaptive-upper2}
|\bx^{\top} \bY \bx - (\bx^*)^{\top} \bY \bx^*| \leq \frac{\epsilon}{20} \cdot \|\bM^{(T)} \bx^{*}\|_2^2.
\end{align}
Combining Eq.~\eqref{eq:adaptive-upper3}\eqref{eq:adaptive-upper1}\eqref{eq:adaptive-upper2}, we conclude with
\[
|\bx^{\top} \bY \bx - \|\bM^{(T)} \bx\|_2^2 | \leq \epsilon \|\bM^{(T)} \bx\|_2^2.
\]
This implies that $\bY$ is an $\epsilon$-spectral approximation of $(\bM^{(T)})^{\top} \bM^{(T)}$. We conclude the proof here.
\end{proof}






We next prove JL estimates are within $(1\pm 0.01)$ of the online leverage scores, even under the adversarial input sequence.

\begin{lemma}[Robustness of JL estimation]\label{lem:JL_estimates}
With probability at least $1-\delta/T$, the JL estimates in Algorithm~\ref{algo:sample} satisfy
\[
\|\tilde{\bB}^{(t-1)} \cdot \boldm^{(t)}\|_2 = (1 \pm 0.01) \|\bB^{(t-1)} \cdot \boldm^{(t)}\|_2, ~~ \forall t \in [T]
\]
under adversarial input sequence.
\end{lemma}
\begin{proof}
We can assume the adversary is deterministic by using Yao's minimax principle. That is to say, the row $\boldm^{(t)} = (\ba^{(t)}, \beta^{(t)})$ at the $t$-th iteration is fixed once the previous rows $\boldm^{(1)}, \cdots, \boldm^{(t-1)}$ and the previous outputs $\bx^{(1)}, \cdots, \bx^{(t-1)}$ are given.

Let $t^*_k \in [T]$ be the time that our data structure samples and keeps the $k$-th row. 
Our algorithm uses a new JL matrix at the end of $t^*_k$-th iteration (Line \ref{line:new-JL} of Algorithm \ref{algo:update-member}), after the algorithm outputs the solution $\bx^{(t^*_k)}$. 
With a slight abuse of notation, we denote the JL matrix used in iterations $t \in [t^*_k+1: t^*_{k+1}]$ as $\bJ^{(k)}$.  Our goal is to prove that $\bJ^{(k)}$ ensures
\[
\|\tilde{\bB}^{(t-1)}\boldm^\ttop\|_2 =  \|\bJ^{(k)}\bB^{(t-1)}\boldm^\ttop\|_2 = (1\pm 0.01)\|\bB^\ttop\boldm^\ttop\|_2, \quad \forall t \in [t^*_k+1, t^*_{k+1}]
\]
with probability at least $1- \delta/T^2$.


Let $\mathcal{A}_{\text{JL}}$ denote our data structure. 
For each $k$, define a ``hybrid'' algorithm $\mathcal{A}_{\text{exact}}^{(k)}$ that is the same as $\mathcal{A}_{\text{JL}}$ in iterations $1, 2, \cdots, t^*_k$, but starting from the $(t^*_k + 1)$-th iteration, $\mathcal{A}_{\text{exact}}^{(k)}$ ignores the JL matrix and set $\tau^\ttop_{\text{exact}} = 0.9 \|\bB^\ttop \boldm^\ttop\|_2$ in replace of Line \ref{line:approximate-ols} of Algorithm \ref{algo:sample}. 
The two algorithms $\mathcal{A}_{\text{JL}}$ and $\mathcal{A}_{\text{exact}}^{(k)}$ start from the same status in the $(t^*_k+1)$-th iteration. 
We also let them use the same random bits to perform the sampling step (Line~\ref{line:sample} in Algorithm~\ref{algo:sample})  
i.e., they share a uniformly random number $u \in [0,1]$ and each set $\nu = 1/\sqrt{p}$ if $u > p$ and $\nu \leftarrow 0$ otherwise, though their sampling probabilities $p$ are different.

We prove that the outputs of $\mathcal{A}_{\text{JL}}$ and $\mathcal{A}_{\text{exact}}^{(k)}$ are the same in iterations $t \in [t^*_k+1, t^*_{k+1}-1]$ with probability $1 - (t^*_{k+1}-t^*_k-1) \cdot \delta/T^2$. 
We prove by induction. In the base case of $(t^*_k+1)$-th iteration, $\bJ^{(k)}$ is a new random matrix and its entries are independent of all previous rows and outputs, hence the entries of $\bJ^{(k)}$ are also independent of $\bB^{(t^*_k)}$ and $\boldm^{(t^*_k+1)}$, 
by Lemma \ref{lem:JL}, with probability $1 - \delta/T^2$, 
\[
 \|\bJ^{(k)} \bB^{(t^*_k)} \boldm^{(t^*_k+1)}\|_2^2 \in (1\pm 0.01) \|\bB^{(t^*_k)} \boldm^{(t^*_k+1)}\|_2^2, 
\]
Therefore, we have $\tau^{(t_k^*)} \geq \tau^{(t_k^{*})}_{\text{exact}}$ and the sampling probability of $\mathcal{A}_{\text{JL}}$ is at least that of $\mathcal{A}_{\text{exact}}^{(k)}$. Since we use the same random bit for $\mathcal{A}_{\text{exact}}^{(k)}$ and $\mathcal{A}_{\text{JL}}$, this means if the $(t^*_k+1)$-th row is sampled in $\mathcal{A}_{\text{exact}}^{(k)}$, then it's also sampled in $\mathcal{A}_{\text{JL}}$, but we know that a new row is only sampled in $\mathcal{A}_{\text{JL}}$ until the $(t^*_{k+1})$-th iteration, so neither $\mathcal{A}_{\text{JL}}$ nor $\mathcal{A}_{\text{exact}}^{(k)}$ samples the $(t^*_k+1)$-th row. 

The induction step is similar and suppose the outputs of $\mathcal{A}_{\text{JL}}$ and $\mathcal{A}_{\text{exact}}^{(k)}$ are the same up to iteration $(t-1) \in [t_k^{*}: t_{k+1}^{*}-1]$, for the $t$-th iteration, we have that
\begin{itemize}
\item Since the $t_{k}^{*} + 1, \ldots, (t-1)$-th rows are not sampled, the matrix $\bB^{(t-1)} = \bB^{(t^*_k)}$ is not updated, so $\bB^{(t-1)}$ remains independent of the JL matrix $\bJ^{(k)}$.
\item Since $\mathcal{A}_{\text{JL}}$ and $\mathcal{A}_{\text{exact}}^{(k)}$ output the same vector to the adversary, it means the next query $\boldm^{(t)}$ chosen by the adversary is the same for both algorithms. The next query $\boldm^{(t)}$ for $\mathcal{A}_{\text{exact}}^{(k)}$ is fixed given the transcript of $\mathcal{A}_{\text{exact}}^{(k)}$, and since $\mathcal{A}_{\text{exact}}^{(k)}$ is completely agnostic of the JL matrix $\bJ^{(k)}$, this means $\boldm^{(t)}$ is also independent of $\bJ^{(k)}$. 
\end{itemize}
Combining the above two facts and by the property of JL sketch (by Lemma \ref{lem:JL}),  with probability $1 - \delta/T^2$, 
\[
 \|\bJ^{(t)} \bB^{(t-1)} \boldm^{(t)}\|_2^2 = (1\pm 0.01) \|\bB^{(t-1)} \boldm^{(t)}\|_2^2, 
\]
and $\tau^{(t)} \geq \tau^{(t)}_{\text{exact}}$. 
By a similar argument, since both algorithm use the same random bits for sampling, $\mathcal{A}_{\text{exact}}^{(k)}$ would not keep $\boldm^{(t)}$ before the $t_{k+1}^{*}$-th iteration. We note it is possible that two algorithms $\mathcal{A}_{\text{JL}}$ and $\mathcal{A}_{\text{exact}}^{(k)}$ differs from each other at the $t_{k+1}^{*}$-th iteration, (i.e., the $t^*_{k+1}$-th row is sampled in $\mathcal{A}_{\text{JL}}$, but it may not be sampled in $\mathcal{A}_{\text{exact}}^{(k)}$), this is fine because after outputting $\bx^{(t^*_{k+1})}$, our data structure immediately update the JL matrix to $\bJ^{(k+1)}$, and we would apply the same argument for $\mathcal{A}_{\text{exact}}^{(k+1)}$. Finally, we note that in each iteration the failure probability is $\delta/T^2$, so using a union bound, the total failure probability is at most $\delta/T$. We conclude the proof here.
\end{proof}




Combining Lemma \ref{lem:intrinsic_new} and Lemma \ref{lem:JL_estimates}, we conclude with the following lemma.

\begin{lemma}[Spectral approximation, adaptive adversary]
\label{lem:spectral-approximation-robust}
The following holds against an adaptive adversary: With probability at least $1 - \delta/T$, for any $t \in [T]$,
\begin{align}
    0.9 (1-\eps) \cdot \tauo^\ttop \leq \tau^{(t)} \leq 1.1 (1+\eps) \cdot \tauo^\ttop \label{eq:spectral-robust-1}
\end{align}
and for any $t \in [0: T]$ 
\begin{align}
    (\bM^{(t)})^{\top} (\bD^{(t)})^2 \bM^{(t)} \approx_{\epsilon} (\bM^{(t)})^{\top} \bM^{(t)}.\label{eq:spectral-robust-2}
\end{align}
Here $\tauo^\ttop = (\boldm^{(t)})^{\top} ((\bM^{(t-1)})^\top \bM^{(t-1)})^{-1}\boldm^{(t)}$ is the online leverage score of the $t$-th row.
\end{lemma}
\begin{proof}
With Lemma \ref{lem:intrinsic_new} and Lemma \ref{lem:JL_estimates} in hand, the proof is similar to the oblivious case (Lemma \ref{lem:sampling_probability}). We prove Eq.~\eqref{eq:spectral-robust-1}\eqref{eq:spectral-robust-2} inductively. The base case holds trivially and suppose it continues to hold up to iteration $(t-1)$. In the $t$-th iteration, by Lemma \ref{lem:intrinsic_new}, we have
\begin{align*}
\tau^\ttop = \|\tilde{\bB}^{(t-1)} \cdot \boldm^{(t)}\|_2 = (1 \pm 0.01) \|\bB^{(t-1)} \cdot \boldm^{(t)}\|_2 = (1 \pm 0.01)(1\pm \eps) \tauo^\ttop. 
\end{align*}
Here the last step follows from the same calculation as Eq.~\eqref{eq:jl1} and the inductive hypothesis on spectral approximation. This finishes the first part of induction. For the second part (i.e., Eq.~\eqref{eq:spectral-robust-2}), it follows from the inductive hypothesis (i.e., Eq.~\eqref{eq:spectral-robust-1}) and the robustness of online leverage score sampling (Lemma \ref{lem:intrinsic_new}). 
\end{proof}

The correctness of our data structure follows directly from Lemma \ref{lem:spectral-approximation-robust} and Lemma \ref{lem:approx_l2_regression_from_spectral_approx}, we summarize below.
\begin{lemma}[Correctness of Algorithm \ref{algo:preprocess}--\ref{algo:update-member}, adaptive adversary]\label{lem:correctness_algorithm-adaptive}
With probability at least $1 - \delta/T$, in each iteration, \textsc{Insert} of Algorithm~\ref{algo:update} outputs a vector $\bx^{(t)} \in \R^d$ such that 
    \[
    \|\bA^{(t)} \bx^{(t)} - \bb^{(t)}\|_2 \leq (1+\eps) \min_{\bx \in \R^d} \|\bA^{(t)} \bx - \bb^{(t)} \|_2
    \]
against an adaptive adversary.
\end{lemma}





\subsection{Runtime analysis}
\label{sec:time}


Finally, we bound the total update time of our algorithm. 
At each iteration, if the $t$-th row is sampled, the \textsc{Insert} procedure makes a call to \textsc{UpdateMembers} (Algorithm~\ref{algo:update-member}), and it takes $\wt{O}(s^{(t)} d \log(T/\delta))$ time, where $s^{(t)} $ is the number of sampled rows. The most expensive step is (Line \ref{line:new-JL} and Line \ref{line:update-wt_B}), where we instantiate a new JL matrix. All other steps can be done in $O(d^2)$ time.
If the $t$-th row is not sampled, then the \textsc{Insert} procedure only needs to compute the approximate leverage score $\tau^{(t)}$, and it takes $\wt{O}(\log(T/\delta) \cdot \nnz(\ba^{(t)}))$ time. 

We summarize the above observations and the calculations can be found in Appendix \ref{sec:upper-app}.
\begin{lemma}[Update time]\label{lem:worst_case_query_time}
At the $t$-th iteration of the \textsc{Insert} procedure (Algorithm~\ref{algo:update}),
\begin{itemize}
    \item If the $t$-th row is not sampled, then \textsc{Insert} takes $O\big(\log(T/\delta) \cdot \nnz(\ba^{(t)})\big)$ time.
    \item If the $t$-th row is sampled, then \textsc{Insert} takes $O\big(s^{(t)}d \log(T/\delta)\big)$ time.
\end{itemize}
\end{lemma}





It is clear that we want to bound the number of sampled rows. First, we have the following bound on the sum of online leverage score.
\begin{lemma}[Sum of online leverage scores \cite{cmp20}]\label{lem:online_leverage_score_sum}
If the largest singular value of matrix $\bM^{(T)}$ is at most $\sigma_{\max}$, and the least singular value of matrix $\bM^{(0)}$ is at least $\sigma_{\min}$, then
\[
\sum_{t=1}^T \tauo^\ttop \leq O(d \log(\sigma_{\max}/ \sigma_{\min})).
\]
where $\tauo^\ttop = (\boldm^{(t)})^{\top} ((\bM^{(t-1)})^\top \bM^{(t-1)})^{-1}\boldm^{(t)}$ is the online leverage score of the $t$-th row.
\end{lemma}

Using Lemma \ref{lem:online_leverage_score_sum}, we have the following lemma on the number of sampled rows, and we again defer its proof to Appendix \ref{sec:upper-app}.
\begin{lemma}[Number of sampled rows]
\label{lem:number-row}
With probability at least $1 -\delta/T$, for oblivious adversary, the total number of sampled rows is at most
\begin{align}
O\left(d\epsilon^{-2} \log(T / \delta) \log(\sigma_{\max}/\sigma_{\min}) \right). \label{eq:number-row-oblivious}
\end{align}
For adaptive adversary, the total number of sampled rows is at most
\begin{align}
O\left(d^2 \epsilon^{-2} \log(T/\delta)\log^2(\sigma_{\max}/\sigma_{\min})\right). \label{eq:number-row-adaptive}
\end{align}
\end{lemma}



Combining Lemma \ref{lem:worst_case_query_time} and Lemma \ref{lem:number-row}, we bound the amortized update time of our data structure.
\begin{lemma}[Amortized update time]\label{lem:amortized_query_time}
With probability at least $1-\delta/T$, for oblivious adversary, 
the total running time of $\textsc{Insert}$ over $T$ iterations is at most
\[
O\big(\nnz(\bA^{(T)}) \log(T/\delta) + \epsilon^{-4} d^3 \log^2(\sigma_{\max} / \sigma_{\min}) \log^3(T / \delta)\big).
\]
for adaptive adversary, 
the total running time of $\textsc{Insert}$ over $T$ iterations is at most
\[
O\big(\nnz(\bA^{(T)}) \log(T/\delta) + \epsilon^{-4} d^5 \log^4(\sigma_{\max} / \sigma_{\min}) \log^3(T / \delta)\big).
\]
\end{lemma}


This concludes the runtime analysis and we finish the proof of Theorem \ref{thm:upper}. Finally we remark on the space usage of our data structure.





\begin{remark}[Space usage]\label{rem:space}
Since the largest matrices that the data structure maintains and updates in each iteration are $\bB^{(t)}, \bN^{(t)} \in \R^{s^{(t)} \times (d+1)}$, it is straightforward to see that the total space used by the data structure is bounded by $O(s^{(T)} \cdot d)$. Hence by Lemma \ref{lem:number-row}, the total space is bounded by $O\big(d^2 \epsilon^{-2}  \log(\sigma_{\max} / \sigma_{\min}) \log(T / \delta) \big)$ for oblivious adversary and $O\big(d^3 \epsilon^{-2}  \log^2(\sigma_{\max} / \sigma_{\min}) \log(T / \delta) \big)$ for adaptive adversary.

\end{remark}







