\section{Missing proofs from Section \ref{sec:formulation}}
\label{sec:pre-app}
We prove the basic property of online leverage score. 
\begin{proof}[Proof of Fact \ref{fact:online_leverage_score}]
We provide a short proof.
Let $\bN =  ((\bM)^{(t-1)})^\top \bM^{(t-1)} \in \R^{(d+1)\times (d+1)}$. For any $\bx \in \R^{d+1}$, define $\by = \bN^{1/2} \bx$. We have
\begin{align*}
\bx^{\top} \boldm^\ttop (\boldm^\ttop)^{\top} \bx = &~ (\by^{\top} \bN^{-1/2} \boldm^\ttop)^2 \\
\leq &~ \|\by\|_2^2 \cdot (\boldm^\ttop)^{\top} \bN^{-1} \boldm^\ttop \\
= &~ \bx^{\top} \bN \bx \cdot (\boldm^\ttop)^{\top} \bN^{-1} \boldm^\ttop \\
= &~ \bx^{\top} \bN \bx \cdot \tauo^\ttop,
\end{align*}
where the second step follows from Cauchy-Schwarz inequality. This finishes the proof.
\end{proof}








\section{Missing proofs from Section \ref{sec:upper}}
\label{sec:upper-app}
\paragraph{Missing proofs from Section~\ref{sec:data_structure}.}
We first provide the proof of Lemma \ref{lem:close_form_formula_algorithm}. 
We will use the Woodbury identity to compute the changes of the exact solution.
\begin{fact}[Woodbury identity]\label{fac:woodbury}
Let $\bA \in \R^{n\times n}, \bC\in \R^{k\times k}, \bU\in \R^{n\times k}, \bV\in\R^{k\times n}$, one has 
\begin{align*}
(\bA + \bU\bC\bV)^{-1} = \bA^{-1} - \bA^{-1}\bU (\bC^{-1} + \bV\bA^{-1}\bU)^{-1}\bV\bA^{-1}.
\end{align*}
In particular, when $\bU, \bV$ are vectors, i.e., $\bU = \ba, V = \ba^{\top}$, and $\bc = \mathsf{1}$, one has
\begin{align*}
(\bA + \ba\ba^{\top})^{-1} = \bA^{-1} - \bA^{-1}\ba (1 + \ba^{\top}\bA^{-1}\ba)^{-1}\ba^{\top}\bA^{-1}.
\end{align*}
\end{fact}

Now we are ready to prove Lemma~\ref{lem:close_form_formula_algorithm}.
\begin{proof}[Proof of Lemma \ref{lem:close_form_formula_algorithm}]
First note that all claims of the lemma hold for $t=0$ in \textsc{Preprocess}. 

We assume that the claims hold for $t-1$, and inductively prove them for $t$. We only prove the lemma for $\bH^{(t)}$ (Part 4), $\bB^{(t)}$ (Part 5), $\bG^{(t)}$ (Part 7) and $\bu^{(t)}$ (Part 8). The rest follows directly from the algorithm description.

{ \bf Part 4 ($\bH^{(t)} = \big( (\bN^{(t)})^{\top} \bN^{(t)} \big)^{-1}$) \ \ } If $\nu^{(t)} = 0$, then $\bN^{(t)} = \bN^{(t-1)}$ and $\bH^{(t)} = \bH^{(t-1)} = \big( (\bN^{(t)})^{\top} \bN^{(t)} \big)^{-1}$.
Otherwise when $\nu^{(t)} \neq 0$, one has $\bN^{(t)} = [(\bN^{(t-1)})^{\top}, \boldm^{(t)} / \sqrt{p^{(t)}}]^{\top}$. Using the Woodbury identity, we have
\begin{align*}
    &~\big( (\bN^{(t)})^{\top} \bN^{(t)} \big)^{-1} \\
    = &~ \big( (\bN^{(t-1)})^{\top} \bN^{(t-1)} + (\boldm^{(t)})^{\top} \boldm^{(t)} / p^{(t)} \big)^{-1} \\
    = &~ \big( (\bN^{(t-1)})^{\top} \bN^{(t-1)} \big)^{-1} - \frac{\big( (\bN^{(t-1)})^{\top} \bN^{(t-1)} \big)^{-1} \boldm^{(t)} (\boldm^{(t)})^{\top} \big( (\bN^{(t-1)})^{\top} \bN^{(t-1)} \big)^{-1} / p^{(t)}}{1 + (\boldm^{(t)})^{\top} \big( (\bN^{(t-1)})^{\top} \bN^{(t-1)} \big)^{-1} \boldm^{(t)} / p^{(t)}}\\
    = &~ \bH^{(t-1)} - \frac{\bH^{(t-1)} \boldm^{(t)} (\boldm^{(t)})^{\top} \bH^{(t-1)} / p^{(t)}}{1 + (\boldm^{(t)})^{\top} \bH^{(t-1)} \boldm^{(t)} / p^{(t)}},
\end{align*}
The second term is exactly the $\Delta \bH$ term when calling {\sc UpdateMembers} (Line~\ref{line:Delta_H}). Hence, $\bH^{(t)} = \bH^{(t-1)} + \Delta \bH = \big( (\bN^{(t)})^{\top} \bN^{(t)} \big)^{-1}$.



{ \bf Part 5 ($\bB^{(t)} = \bN^{(t)} \bH^{(t)}$) \ \ } If $\nu^{(t)} = 0$, then $\bN^{(t)} = \bN^{(t-1)}$ and $\bH^{(t)} = \bH^{(t-1)}$, so $\bB^{(t)} = \bB^{(t-1)} = \bN^{(t)} \bH^{(t)}$.
Otherwise when $\nu^{(t)} \neq 0$, one has $\bN^{(t)} = [(\bN^{(t-1)})^{\top}, \boldm^{(t)} / \sqrt{p^{(t)}}]^{\top}$ and $\bH^{(t)} = \bH^{(t-1)} + \Delta \bH$. We have
\begin{align*}
    \bN^{(t)} \bH^{(t)} = &~ 
    \begin{bmatrix}
    \bN^{(t-1)} \cdot \bH^{(t)} \\
    (\boldm^{(t)})^{\top} \cdot \bH^{(t)} / \sqrt{p^{(t)}}
    \end{bmatrix} \\
    = &~ \begin{bmatrix}
    \bB^{(t-1)} + \bN^{(t-1)} \cdot \Delta \bH \\
    (\boldm^{(t)})^{\top} \cdot \bH^{(t)} / \sqrt{p^{(t)}}.
    \end{bmatrix}
\end{align*}

This is exactly what we compute in Line~\ref{line:update-B} of {\sc UpdateMembers}.




{ \bf Part 7 ($\bG^{(t)} = \big( (\bA^{(t)})^{\top} (\bD^{(t)})^2 \bA^{(t)} \big)^{-1}$) \ \ } 
The proof is analogous to that of Part 4 ($\bH^{(t)}$). 
If $\nu^{(t)} = 0$, then $\bG^{(t)} = \bG^{(t-1)} = \big( (\bA^{(t-1)})^{\top} (\bD^{(t-1)})^2 \bA^{(t-1)} \big)^{-1} = \big( (\bA^{(t)})^{\top} (\bD^{(t)})^2 \bA^{(t)} \big)^{-1}$.
On the other hand, when $\nu^{(t)} \neq 0$, using the Woodbury identity, one has
\begin{align*}
    \big( (\bA^{(t)})^{\top} (\bD^{(t)})^2 \bA^{(t)} \big)^{-1}  = &~ \big( (\bA^{(t-1)})^{\top} (\bD^{(t-1)})^2 \bA^{(t-1)} + (\ba^{(t)})^{\top}\ba^{(t)}/p^{(t)} \big)^{-1}  \\
    =&~ \bG^{(t-1)} - \frac{\bG^{(t-1)}\ba^{(t)} (\ba^{(t)})^{\top} \bG^{(t-1)} / p^{(t)}}{1 + (\ba^{(t)})^{\top} \bG^{(t-1)} \ba^{(t)} / p^{(t)}}
    = \bG^{(t)}.
\end{align*}
This is exactly what we compute in Line~\ref{line:update-G} of {\sc UpdateMembers}.


{ \bf Part 8 ($\bu^{(t)} = (\bA^{(t)})^{\top}(\bD^{(t)})^2 \bb^{(t)}$) \ \ } 
We focus on the case $\nu^{(t)} \neq 0$, and we have
\begin{align*}
    \bu^{(t)} = \bu^{(t-1)} + \beta^{(t)} \cdot \ba^{(t)} / p^{(t)} = \bA^{(t-1)}(\bD^{(t-1)})^2 \bb^{(t-1)} + (\nu^{(t)})^2 \cdot \ba^{(t)} \cdot \beta^{(t)} = \bA^{(t)}(\bD^{(t)})^2 \bb^{(t)}
\end{align*}
The first step follows from the updating rule of the data structure (Line~\ref{line:update-u} of {\sc UpdateMembers}), and the second step follows from $\bu^{(t-1)}= \bA^{(t-1)}(\bD^{(t-1)})^2 \bb^{(t-1)}$ and $\nu^{(t)} = 1/\sqrt{p^{(t)}}$, the last step follows from the definition of $\bA^{(t)}, \bb^{(t)}$ and $\bD^{(t)}$.
\end{proof}







\paragraph{Missing proofs from Section~\ref{sec:correct-oblivious}.}
We then prove Lemma \ref{lem:spectral-online-leverage-score}. We make use of the following matrix Chernoff bound for adaptive sequences.
\begin{lemma}[Matrix Chernoff: Adaptive sequence. Theorem 3.1 of \cite{tropp2011user}]
\label{lem:matrix-adaptive}
Consider a finite adapted sequence $\{\bX_k\}$ of positive-semidefinite matrices with dimension $d$, and suppose that
\[
\lambda_{\max}(\bX_k) \leq R \quad \text{almost surely}.
\]
Define the finite series 
\[
\bY := \sum_{k}\bX_k \quad \text{and}  \quad \bW:= \sum_{k}\E_{k-1} \bX_k.
\]
For all $\mu \geq 0$, 
\begin{align*}
\Pr[\lambda_{\min}(\bY) \leq (1-\eps)\mu \quad\text{and}\quad \lambda_{\min}(\bW) \geq \mu] \leq &~ d\cdot \Big[\frac{e^{-\eps}}{(1-\eps)^{1-\eps}}\Big]^{\mu/R} \text{ for } \eps \in [0, 1)\\
\Pr[\lambda_{\max}(\bY) \geq (1+\eps)\mu \quad\text{and}\quad \lambda_{\max}(\bW) \leq \mu] \leq &~ d\cdot \Big[\frac{e^{\eps}}{(1+\eps)^{1+\eps}}\Big]^{\mu/R} \text{ for } \eps \geq 0.
\end{align*}
\end{lemma}


Now we are ready to prove Lemma~\ref{lem:spectral-online-leverage-score}.
\begin{proof}[Proof of Lemma \ref{lem:spectral-online-leverage-score}]
Let 
\[
\bX_{0} = ((\bM^{(T)})^\top \bM^{(T)})^{-1/2}((\bM^{(0)})^\top \bM^{(0)})((\bM^{(T)})^\top \bM^{(T)})^{-1/2}.
\]
and
\[
\bX_{t} = \nu_{t}^2 \cdot ((\bM^{(T)})^\top \bM^{(T)})^{-1/2}\boldm^\ttop (\boldm^\ttop)^\top ((\bM^{(T)})^\top \bM^{(T)})^{-1/2}, \quad \forall t\in [T].
\]
Then $\{\bX_0\}_{t\in [T]}$ is an adaptive sequence that satisfies 
(1) $\sum_{t=1}^{T}\E_{t-1}[\bX_t] = \mathbf{I}$, (2) $\bX_{t} \preceq \frac{\eps^{2}}{3\log(d/\delta)} \mathbf{I}$.
The second property follows from 
\begin{align*}
\bX_{t} = &~ \frac{1}{p_t} ((\bM^{(T)})^\top \bM^{(T)})^{-1/2}\boldm^\ttop (\boldm^\ttop)^\top ((\bM^{(T)})^\top \bM^{(T)})^{-1/2}\\
\preceq &~ \frac{\eps^2}{3\log(d/\delta)} \cdot \frac{1}{\tauo^\ttop}  ((\bM^{(T)})^\top \bM^{(T)})^{-1/2}\boldm^\ttop (\boldm^\ttop)^\top ((\bM^{(T)})^\top \bM^{(T)})^{-1/2}\\
\preceq &~ \frac{\eps^2}{3\log(d/\delta)}  ((\bM^{(T)})^\top \bM^{(T)})^{-1/2} ((\bM^{(t-1)})^\top \bM^{(t-1)}) ((\bM^{(T)})^\top \bM^{(T)})^{-1/2}\\
\preceq &~ \frac{\eps^2}{3\log(d/\delta)} \mathbf{I}.
\end{align*}
where we assumed $p_t = 3\eps^{-2}\log(d/\delta)\cdot\tauo^\ttop < 1$ in the second step. This is wlog because we can split $\bX_t$ into smaller terms if $p_t = 1$. The third step follows from Fact \ref{fact:online_leverage_score}.

Now we can apply the matrix Chernoff bound (Lemma \ref{lem:matrix-adaptive}) with $\mu = 1$, $R = \frac{\eps^2}{3\log(d/\delta)}$, we have that with probability at least $1-\delta$, one has
\[
(1-\eps)\mathbf{I} \preceq \sum_{t=0}^{T}\bX_t \preceq (1+\eps)\mathbf{I},
\]
and this implies
\[
(1-\eps)(\bM^{(T)})^\top \bM^{(T)} \preceq (\bM^{(0)})^\top \bM^{(0)} + \sum_{t=1}^{T}\nu_t^2 \cdot \boldm^\ttop (\boldm^\ttop)^\top  \preceq (1+\eps)(\bM^{(T)})^\top \bM^{(T)}.
\]
We conclude the proof here.
\end{proof}


\paragraph{Missing proofs from Section~\ref{sec:upper_robust}.}
Next, we prove the following claim that is used in the proof of Lemma~\ref{lem:intrinsic_new}.
\begin{claim}
\label{claim:upper-tech1}
Condition on the event of Eq.~\eqref{eq:adaptive-upper3}, the largest singular values of $\bY$ is at most $4 \sigma_{\max}^2$.
\end{claim}
\begin{proof}
We prove this by contradiction. Suppose the largest singular value of $\bY$ is $\sigma^2 > 4 \sigma_{\max}^2$. Let $\bx' = \arg\max_{\bx \in \R^d, \|\bx\|_2 = 1} \bx^{\top} \bY \bx$, and it satisfies that $\bx'^{\top} \bY \bx' = \sigma^2$. There must exist some $\bx'' \in \mathcal{B}$ such that $\|\bx' - \bx''\|_2 \leq \frac{\epsilon}{100 \kappa}$, and $\bx''^{\top} \bY \bx'' \leq (1 + \frac{\epsilon}{2}) \|\bM^{(T)} \bx''\|_2^2 \leq (1 + \epsilon) \sigma_{\max}^2$. We have
\begin{align*}
(\bx' - \bx'')^{\top} \bY (\bx' - \bx'') = &~ \|\bY^{1/2} (\bx' - \bx'')\|_2^2 \\
\geq &~ (\sqrt{\bx'^{\top} \bY \bx'} - \sqrt{\bx''^{\top} \bY \bx''})^2 \\
\geq &~ (\sigma - (1 + \epsilon) \sigma_{\max})^2 \\
\geq &~ (\sigma - (1 + \epsilon) \sigma_{\max})^2 \cdot (100 \kappa / \epsilon)^2 \|\bx' - \bx''\|_2^2
\end{align*}
where the second step follows from triangle inequality of $\ell_2$ norm, the third step follows from $\bx'^{\top} \bY \bx' = \sigma^2$ and $\bx''^{\top} \bY \bx'' \leq (1 + \epsilon) \sigma_{\max}^2$, and the last step follows from $\|\bx' - \bx''\|_2 \leq \frac{\epsilon}{100 \kappa}$. Since $\sigma > 2 \sigma_{\max}$ and $\epsilon < 1/8$, we have $100 (\sigma - (1 + \epsilon) \sigma_{\max}) > \sigma$, and this contradicts with our definition that $\bx'$ is the unit vector that corresponds to the largest singular value of $\bY$.
\end{proof}


\paragraph{Missing proofs from Section~\ref{sec:time}.}
We next prove Lemma \ref{lem:worst_case_query_time}.
\begin{proof}[Proof of Lemma \ref{lem:worst_case_query_time}]
If the $t$-row is not sampled, we only need to invoke the {\sc Sample} procedure. The most time-consuming step of {\sc Sample} is to compute $\wt{\bB}^{(t-1)} \cdot \boldm^{(t)}$ when computing $\tau^{(t)}$ (Line~\ref{line:levarage_score} of Algorithm \ref{algo:sample}). Since $\wt{\bB}^{(t-1)} \in \R^{O(\log(T/\delta)) \times (d+1)}$ and $\boldm^{(t)} = [(\ba^{(t)})^{\top}, \beta^{(t)}]^{\top}$, this takes $O(\log(T/\delta) \cdot \nnz(\ba^{(t)}))$ time.

If the $t$-row is sampled, besides the {\sc Sample} procedure, the data structure also needs to invoke the {\sc UpdateMembers} procedure. 
The most time-consuming step is to compute $\wt{\bB}^{(t)} = \bJ^{(t)} \cdot \bB^{(t)}$ on Line~\ref{line:update-wt_B}. Indeed, it's easy to see that all other computations only involve matrix-vector multiplications and matrix additions, and they can be computed in $O(s^{(t)} d)$ time.
Since $\bJ^{(t)} \in \R^{O(\log(T/\delta)) \times s^{(t)}}$ and $\bB^{(t)} \in \R^{s^{(t)} \times (d+1)}$, computing $\wt{\bB}^{(t)} = \bJ^{(t)} \cdot \bB^{(t)}$ takes $O(s^{(t)} d \log(T/\delta))$ time.
\end{proof}

To prove Lemma~\ref{lem:number-row}, we make use of the following concentration result that is a direct application of Freedman's inequality.
\begin{lemma}
\label{lem:number_sampled_rows-concentration}
Let $p_1, p_2, \cdots, p_T \in [0,1]$ be a sequence of sampling probabilities chosen by adaptive adversary and always satisfies $\sum_{t=1}^T p_t \leq U$. 
Let 
\[
x^{(t)} = 
\begin{cases}
1 & \text{w.p. } p_t \\
0 & \text{w.p. } 1 - p_t
\end{cases}
\]
and let $y^{(0)} = 0$, $y^{(t)} = y^{(t-1)} + x^{(t)}$ for any $t\in [T]$. Then for any $u > 0$,  the final outcome $y^{(T)}$ satisfies
\[
\Pr\left[y^{(T)} \geq u + \sum_{t=1}^T p_t  \right] \leq \exp\left(-\frac{u^2/2}{U + u/3}\right).
\]
\end{lemma}
\begin{proof}
Let $\overline{x}^{(t)} = x^{(t)} - p_t$, and note that $\E_{t-1}[\overline{x}^{(t)}] = 0$ and $|\overline{x}^{(t)}| \leq 1$. 
Let $\overline{y}^{(0)} = 0$, and $\overline{y}^{(t)} = \overline{y}^{(t-1)} + \overline{y}^{(t)}$. Note that the sequence $\overline{y}^{(0)}, \overline{y}^{(1)}, \cdots, \overline{y}^{(T)}$ is a martingale, and $\overline{y}^{(T)} = y^{(T)} - \sum_{t=1}^T p_t$.
We have
\[
\E_{t-1}[(\overline{x}^{(t)})^2] = p_t \cdot (1 - p_t)^2 + (1 - p_t) \cdot p_t^2 = p_t \cdot (1 - p_t).
\]
and the variance satisfies
\[
\Var = \sum_{t=1}^T \E_{t-1}[(\overline{x}^{(t)})^2]
= \sum_{t=1}^T p_t \cdot (1 - p_t) \leq \sum_{t=1}^T p_t \leq U.
\]
Using Freedman's inequality (Lemma \ref{thm:freedman}) with $R = 1$, $\sigma^2 = U$, and any $u > 0$, we have
\[
\Pr[\overline{y}^{(T)} \geq u] \leq \exp\left(-\frac{u^2/2}{U + u/3}\right). \qedhere
\]
\end{proof}

Now we are ready to prove Lemma~\ref{lem:number-row}.
\begin{proof}[Proof of Lemma \ref{lem:number-row}]
For oblivious adversary, conditioning on the event of Lemma \ref{lem:correctness_algorithm}, the expected number of rows are at most 
\begin{align*}
\sum_{t=1}^{T}C_{\text{obl}}\cdot \tau^\ttop \leq 2\sum_{t=1}^{T}C_{\text{obl}} \cdot \tauo^\ttop = O\left(d\eps^{-2}\log(T/\delta)\log(\frac{\sigma_{\max}}{\sigma_{\min}})\right).
\end{align*}
Plugging $U = O(\eps^{-2}d\log(T/\delta)\log(\sigma_{\max} / \sigma_{\min}))$ into Lemma \ref{lem:number_sampled_rows-concentration}, we obtain Eq.~\eqref{eq:number-row-oblivious}.

For adaptive adversary, conditioning on the high probability event of Lemma \ref{lem:spectral-approximation-robust}, the expected number of rows are at most 
\begin{align*}
\sum_{t=1}^{T}C_{\text{adv}}\cdot \tau^\ttop \leq 2\sum_{t=1}^{T}C_{\text{adv}} \cdot \tauo^\ttop = O\left(d^2\eps^{-2}\log(T/\delta)\log^2(\frac{\sigma_{\max}}{\sigma_{\min}})\right).
\end{align*}
Plugging $U = O\left(d^2\eps^{-2}\log(T/\delta)\log^2(\frac{\sigma_{\max}}{\sigma_{\min}})\right)$ into Lemma \ref{lem:number_sampled_rows-concentration}, we obtain Eq.~\eqref{eq:number-row-adaptive}.
\end{proof}
