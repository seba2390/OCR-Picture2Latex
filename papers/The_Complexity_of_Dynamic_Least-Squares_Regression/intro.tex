\section{Introduction}
\label{sec:intro}
The problem of least-squares regression (LSR)  dates back to Gauss in 1821 \cite{GaussLS}, and is the backbone 
of high-dimensional statistical inference \cite{HFT_book_01}, signal processing \cite{rg75},  
convex optimization \cite{b15}, control theory \cite{k90}, 
network routing \cite{ls14,m13} and machine learning \cite{cortes1995support}. 
Given an overdetermined ($n\gg d$) linear system $\bA \in \R^{n \times d}, \bb \in \R^n $, 
the goal is to find the solution vector $\bx$ that minimizes the mean squared error (MSE)
\begin{align} \label{eq_LS_reg} 
\min_{\bx\in \R^n} \| \bA\bx - \bb \|_2.
\end{align}
The exact closed-form solution is given by the well-known Normal equation
$\bx^\star = (\bA^\top \bA)^{-1}\bA^\top \bb$, 
which requires $O(nd^2)$ time to compute using naive matrix-multiplication, or $O(nd^{\omega-1}) \approx O(n d^{1.37})$ time using fast matrix-multiplication (FMM) \cite{s69} for the current FMM exponent of $\omega \approx 2.37$ \cite{le14, aw21}.

Despite the elegance and simplicity of this closed-form solution, in practice the latter runtime is often too slow, especially in modern data analysis applications where both the dimension of the 
feature space ($d$) and the size of datasets ($n$) are overwhelmingly large. 
A more modest objective in attempt to circumvent this computational overhead, is to seek an $\eps$-accurate solution that satisfies 
\begin{align} \label{eq_APX_LS_solution} 
\|\bA \bx- \bb\|_2 \leq (1+\eps) \min_{\bx \in \R^d} \|\bA \bx - \bb \|_2 .
\end{align} 
A long line of work on sketching \cite{cw17} and sampling \cite{clmmps15}, combined with iterative linear-system solvers (preconditioned gradient descent), culminated in \emph{high-precision} algorithms that run in close to input-sparsity time 
$\wt{O}(\nnz(A)\log(1/\eps)+ d^{\omega})$\footnote{We use $\wt{O}$ to hide $\poly \log(d)$ factors.} 
for the offline problem \cite{s06, cw17, nn13, cherapanamjeri2023optimal}.
This is essentially optimal in the realistic setting $d\ll n$. 

\vspace{+2mm}
{\bf \noindent  Dynamic Least Squares \ \ } 
Many of the aforementioned applications of LSR, both in theory and practice, involve data that is continually changing, either by nature or by design. In such  applications, it is desirable to avoid recomputing the LSR solution from scratch, and instead maintain the solution \eqref{eq_APX_LS_solution} dynamically, under insertion and/or deletions of rows and labels $(\ba^\ttop, \beta^\ttop)$. 
The most compelling and realistic dynamic model is that of \emph{adaptive} row updates, where the algorithm  is required to be correct  against an adaptive adversary that chooses the next update as a function of previous outputs of the algorithm. This is a much stronger notion of  dynamic algorithms than the traditional \emph{oblivious} model, where the sequence of updates is chosen \emph{in advance}. 
This distinction is perhaps best manifested in 
\emph{iterative randomized} algorithms, where the input to the next iteration depends on the output of the previous iteration and hence on the internal randomness of the algorithm, and traditional sketching algorithms generally break under the stronger adaptive setting. \cite{hw13, bjwy22, hkm+22,cohen2022robustness}. 

Another aspect of dynamic LSR  is whether data is partially or fully dynamic -- some applications inherently involve incremental updates (row-insertions), whereas others require both insertion and deletions. 

\bf Fully Dynamic LSR. \rm 
In the \emph{fully dynamic} setting, rows $(\ba^\ttop, \beta^\ttop)$ can be adaptively inserted or deleted, and the goal is to minimizes the \emph{amortized update time} required to maintain an $\eps$-approximate solution to \eqref{eq_LS_reg} in each iteration. 
Variants of dynamic LSR show up in many important applications and iterative optimization methods, from 
Quasi-Newton \cite{pilanci2017newton} and interior-point methods (IPM) \cite{cls21}, 
the (matrix) multiplicative-weight updates framework \cite{AHK06}, iteratively-reweighted least squares \cite{Law61} to mention a few.   
We stress that some of these variant involve application-specific restrictions on the LSR updates (e.g., small $\ell_2$-norm or sparsity), whereas we study here the problem in full generality. 

\bf Partially Dynamic LSR. \rm \; 
In the \emph{partially-dynamic} setting, rows $(\ba^\ttop, \beta^\ttop)$ can only be inserted, and the goal is again to minimize the amortized update time for inserting a row.
This incremental setting is more natural in 
 control theory and dynamical linear systems \cite{Pla50, k60}, and in modern deep-learning applciations, in particular \emph{continual learning} \cite{pkp+19,cpp22}, where the 
goal is to finetune a neural network over arrival of new training data, \emph{without} training from scratch. 

\

The textbook solution for dynamic LSR, which handles general adaptive row updates, and dates back to \emph{Kalman's filter} and the recursive least-squares framework \cite{k60},  
is to apply 
\emph{Woodbury's identity}, 
which can implement each row update in $O(d^2)$ worst-case time \cite{k60}, and maintain the Normal equation exactly.
Interestingly, in the \emph{oblivious, partially dynamic} (insertion-only) setting, one can do much better --  \cite{cmp20} gave a streaming algorithm, based on \emph{online row-sampling}, which  maintains a subset of only $\tilde{O}(d/\eps^2)$ rows of $\bA^{(t)}$, and provides an $\eps$-approximate LSR solution at any given time (the algorithm of \cite{cmp20} does \emph{not} yield an efficient data structure, but we will show a stronger result implying this).  Very recently, this algorithm was extended to handle \emph{adaptive} incremental updates \cite{bhm+21}, at the price of a substantial increase in the number of sampled rows $\tilde{O}(d^2\kappa^2/\eps^2)$  (and hence the update time also increases), where $\kappa$ is the condition number of the inputs, which could scale polynomially with the total number of rounds $T$.


Our first result is an efficient partially-dynamic low-accuracy LSR data structure, against adaptive row-insertions, whose update time depends only logarithmically on the condition number:   

\begin{theorem}[Faster Adaptive Row Sampling] \label{thm:main_UB_informal}
For any accuracy parameter $0 < \eps < 1/8$, there is a randomized dynamic data structure which, 
with probability at least $0.9$, maintains an $\eps$-approximate LSR solution under adaptive row-insertions, 
simultaneously \underline{for all} iterations $t\in[T]$, with total update time 
\[ O\left(\nnz(\bA^{(T)}) \log(T) + \epsilon^{-4} d^5 \log^4(\frac{\sigma_{\max}}{\sigma_{\min}}) \log^3(T)\right), \]
where $\sigma_{\max}$ ($\sigma_{\min}$) is the maximum (minimum) singular value over all input matrices $[\bA^\ttop, \bb^\ttop]$ for $t\in [T]$.
\end{theorem}
For constant approximations ($\epsilon = 0.1$), Theorem~\ref{thm:main_UB_informal} almost matches the fastest static sketching-based solution, up to polylogarithmic terms and the additive term. When $T \gg d$, this theorem shows that amortized update time of our algorithm is $\tilde{O}(d)$. A key sub-routine of our algorithm is an improved analysis of the online leverage score sampling that reduces the number of rows from $\tilde{O}(d^2\kappa^2/\eps^2)$ to  $\tilde{O}(d^2 \log(\kappa)/\eps^2)$, where $\kappa:=\sigma_{\max}/\sigma_{\min}$.



\

Our main result is that, by contrast, in the \emph{fully-dynamic} setting, Kalman's classic approach is essentially optimal, even for maintaining a \emph{constant} approximate LSR solution, assuming the \emph{Online Matrix-Vector} ($\omv$) Conjecture \cite{hkns}: 

\begin{theorem}[Lower Bound for Fully-Dynamic LSR, Informal] \label{thm_low_acc_LB_informal}
There is an adaptive sequence of $T = \poly(d)$ row insertions and deletions, such that any dynamic data structure that maintains an $0.01$-approximate LSR solution, has amortized update time at least $\Omega(d^{2-o(1)})$ per row, under the $\omv$ Conjecture. 
\end{theorem}



Recall that the $\omv$ Conjecture \cite{hkns} postulates that computing \bf \emph{exact} \rm Boolean matrix-vector products, of a fixed $n\times n$ Boolean matrix $\bH$ with an \emph{online} sequence of vectors $\mathbf{v}^{(1)}, \ldots, \mathbf{v}^{(n)}$, one-at-a-time ($\bH\bv^{(i)}$),  requires $n^{3-o(1)}$ time (in sharp contrast to the offline batch setting, where this can be done using FMM in $n^\omega\ll n^3$ time, see Section \ref{sec:fully} for more details). 

Theorem~\ref{thm_low_acc_LB_informal} follows from a gap-amplification reduction from exact $\omv$ to \emph{approximate $\omv$} {\em over the reals}, asserting that the $\omv$ problem remains hard even when the online matrix-vector products $\bH\bvi$ need only be approximated to within \emph{constant relative} accuracy (i.e., $\| \mathbf{y}^{(i)} - \bH\bvi \|_2 \leq 0.1 \|\bH\bvi\|_2$) against adaptive sequences, see Theorem \ref{thm:hard-omv-approx}. 
All previous fine-grained reductions from $\omv$ to its approximate versions only show hardness for inverse-polynomial error $\eps = n^{-\omega(1)}$, see further discussion in the related work section. 

We believe Theorem \ref{thm:hard-omv-approx} may be useful for proving or refuting the $\omv$ Conjecture itself, both because constant relative approximation brings the problem closer to the realm of dimensionality-reduction tools (which only work in the low-accuracy, oblivious regime), and on the other hand, since the adaptive nature of our construction is necessary for relating the $\omv$ conjecture to more established fine-grained conjectures (3SUM, SETH, OV): 
Indeed, 
one reason for the lack of progress the $\omv$ Conjecture is that almost all known reductions in the fine-grained complexity literature are non-adaptive \cite{W18survey}, meaning that they apply equally to online and offline queries, and hence are futile for atacking the $\omv$ Conjecture (one exception is the adaptive reduction of \cite{WW18} for triangle detection).
We remark that adaptivity of the vectors $\bvi$'s is crucial for the proof of Theorems \ref{thm_low_acc_LB_informal} and \ref{thm:hard-omv-approx}, but also natural:   Iterative optimization algorithms and linear-system solvers, in particular first-order (Krylov) methods for quadratic minimization, are based on \emph{iterative refinement} of the residual error, hence the new error vector is a function of previous iterates \cite{wilkinson1994rounding,hestenes1952methods,akps19}. 
In fact, the proof of Theorems \ref{thm_low_acc_LB_informal} and \ref{thm:hard-omv-approx} is inspired precisely by this idea, see the technical overview below.

\

Finally, we prove a similar $d^{2-o(1)}$ amortized lower bound for \emph{high-accuracy} data structures   in the insertion-only setting, which shows that the accuracy  of our data structure from Theorem~\ref{thm:main_UB_informal} cannot be drastically improved:  

\begin{theorem}[Hardness of High-Precision Partially-Dynamic LSR, Informal] \label{thm_exact_LB_informal}
Assuming the $\omv$ Conjecture, any dynamic data structure that maintains an $\eps=1/\poly(T, d)$-approximate solution for the partially dynamic LSR over $T = \poly(d)$ iterations, must have $\Omega(d^{2-o(1)})$ amortized update time per iteration.
\end{theorem}


The three above theorems provide a rather complete characterization of the complexity of dynamic least squares regression.
 


\subsection{Related work}


\paragraph{Fine-grained complexity} The $\omv$ conjecture \cite{hkns} has originally been proposed as a unified approach to prove conditional lower bound for dynamic problems. It has broad applications to dynamic algorithms \cite{d16,bks17,jns19,lr21,jx22} and it is still widely open \cite{lw17, ckl18,aggs22,hs22}.
A few prior works \cite{acss20, cs17,bis17,bcis18} have shown fine-grained hardness of related matrix problems (e.g. kernel-density estimation, empirical risk minimization), based on the strong Exponential Time Hypothesis (SETH, see  \cite{ip01} and references therein). 
In contrast to Theorems \ref{thm_low_acc_LB_informal}, \ref{thm:hard-omv-approx}, all these works only establish hardness for exact or polynomially-small precision (i.e., $\eps=d^{-\omega(1)}$).


\paragraph{Least-squares regression in other models of computation}
The problem of (static) least-squares regression has a long history in TCS \cite{ac06,cw17,nn13,clmmps15,acw17}. Using dimensionality-reduction techniques (sketching or sampling) to precondition the input matrix, and running (conjugate) gradient descent, one can obtain an $\eps$-approximation solution in input-sparsity $\tilde{O}(\nnz(\bA)\log(1/\eps) + d^{\omega})$, see \cite{w14,w21} for a comprehensive survey.

The LSR problem has also been studied in different models, we briefly review here the most relevant literature. 
In the streaming model, \cite{cw09} gives the (tight) {\em space} complexity of $\tilde{\Theta}(d^2/\eps)$ when entries of the input are subject to changes.
In the online model, the input data arrives in an online streaming and \cite{cmp20} proposes the online row sampling framework, which stores $\tilde{O}(d/\eps^2)$ rows and maintain an $\eps$-spectral approximation of the input.
\cite{bdm+20} generalizes the guarantee to the {\em sliding window} model (among other numerical linear algebra tasks), where data still comes an online stream and but only the most recent updates form the underlying data set.

The focus of all aforementioned model is on the space (or the number of rows), instead of computation time, and they only work against an oblivious adversary.
Initiated by \cite{bjwy22}, a recent line of work \cite{bjwy22, hkm+22,wz22} aims to make streaming algorithm works against an adaptive adversary. 
As noted by \cite{workshop2021}, most existing results are for scalar output and it is an open question when the output is a large vector.
\cite{bhm+21} is most relevant to us and studies the online row sampling framework \cite{cmp20} (among other importance sampling approaches) against adaptive adversary, and prove it maintains an $\eps$-spectral approximation when storing $\tilde{O}(d^2\kappa^2/\eps^2)$ rows, where $\kappa = \sigma_{\max}/\sigma_{\min}$ is the condition number of the input. 
A key part of our algorithm is to give an improved analysis and reduce the number to $\tilde{O}(d^2\log (\kappa)/\eps^2)$.

Finally, we note the problem of {\em online regression} has been studied in the online learning literature \cite{h19}, where the goal is to minimize the total {\em regret}. this is very different from ours in that the main bottleneck  is {\em information-theoretic}, whereas the challenge in our loss-minimization problem is purely computational.




\paragraph{Comparison to the inverse-maintenance data structure in IPM} Similar dynamic regression problems have been considered in the literature of interior point methods (IPM) for solving linear programs (LP) \cite{cls21,b20,lsz19,jswz21,ls14,blss20,bll+21}. There the problem is to maintain $(\bA^\top \bW \bA)^{-1} \bA^\top \bW \bb$ for a slowly-changing diagonal matrix $\bW$. The aforementioned papers use sampling and sketching techniques to accelerate the amortized cost per iteration. The inverse-maintenance data structures in the IPM literature are solving a similar but incomparable dynamic LSR problem -- the updates in the IPM setting are adaptive \emph{fully dynamic} (i.e., general low-rank updates), and cannot recover the linear $\tilde{O}(d)$ update time of 
our data structure for \emph{incremental} row-updates (Theorem \ref{thm:main_UB_informal}).  











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




