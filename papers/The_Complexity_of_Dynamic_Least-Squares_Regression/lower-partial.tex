\section{Lower bound for partially dynamic LSR}
\label{sec:hard}


When the update is incremental, we prove an $\Omega(d^{2 -o(1)})$ amortized time lower bound for any algorithm with high precision solution.


\begin{theorem}[Hardness of partially dynamic LSR with high precision, formal version of Theorem \ref{thm_exact_LB_informal}]
\label{thm:lower}
Let $d$ be a sufficiently large integer, $T = \poly(d)$ and $\eps = \frac{1}{d^8T^2}$. Let $\gamma > 0$ be any constant. Assuming the $\omv$ conjecture is true, then any dynamic algorithm that maintains an $\eps$-approximate solution of the least squares regression under incremental update requires at least $\Omega(d^{2-\gamma})$ amortized time per update.
\end{theorem}


\begin{proof}
We reduce from the problem of Lemma~\ref{lem:omv-real}.
Given a PSD matrix $\bH \in \R^{d\times d}$ for the problem of Lemma~\ref{lem:omv-real}, where $1\leq \lambda_1(\bH) \leq \lambda_{d}(\bH) \leq 3$, we compute $\bA^{\top}\bA = \bH^{-1}$.
In the least squares regression problem, we take $\bA$ to be the initial matrix, and let $\bb = \mathbf{0}_d \in \R^{d}$ be the initial label vector.
The preprocessing step takes $O(d^\omega)$ time.

In the online stage of $\omv$, let the query at the $t$-th step be $\bz^{(t)} \in \R^d$ where $\|\bz^{(t)}\|_2 \leq 1$. 
We scale the vector to construct 
\[
\ba^{(t)} = \frac{1}{d^2\sqrt{T}}\cdot \bz^{(t)} \in \R^{d},
\]
and use $(\ba^{(t)}, 1)\in \R^{d} \times \R$ as the update to the regression problem at the $t$-th step.
Let $\bx^{(t)} \in \R^d$ be the solution returned by the dynamic algorithm for least squares regression at the $t$-th step. We prove that one can answer the matrix-vector query with 
\[
\by^{(t)} = d^2\sqrt{T} (\bx^{(t)} - \bx^{(t-1)}),
\]
and we have the guarantee that
\begin{align}
\|\by^{(t)} - \bH \bz^{(t)}\|_2\leq O(1/d^2).
\label{eq:lower-goal}
\end{align}
By Lemma~\ref{lem:omv-real}, this is impossible if the $\omv$ conjecture is true.

We first introduce some notations.
Let $\bA^{(t)} \in \R^{(d+t) \times d}$ be the data matrix after the $t$-th time step, $\bb^{(t)} \in \R^{d+t}$ be the labels, and $\bH^{(t)} := ((\bA^{(t)})^{\top} \bA^{(t)})^{-1} \in \R^{d\times d}$.
For simplicity, we also define $\bA^{(0)} = \bA$, $\bb^{(0)} = \bb$, $\bH^{(0)} = \bH$.
For any $t\in [T]$, let $\bx_{\star}^{(t)}$ be the optimal solution at the $t$-th step, and it has the closed-form $\bx_{\star}^{(t)} = ((\bA^{(t)})^{\top} \bA^{(t)})^{-1} (\bA^{(t)})^{\top} \cdot \bb^{(t)}$. The proof divides into three steps:
\begin{itemize}
    \item Step 1. $\bx^{(t)}$ and $\bx_{\star}^{(t)}$ are close, i.e., $\bx^{(t)} = \bx_{\star}^{(t)} \pm O(\frac{1}{d^4\sqrt{T}})$.
    \item Step 2. $\bx^{(t)} - \bx^{(t-1)}$ recovers $\bH^{(t-1)} \ba^{(t)}$, i.e., $\bx^{(t)} - \bx^{(t-1)} = \bH^{(t-1)} \ba^{(t)} \pm O(\frac{1}{d^4\sqrt{T}})$.
    \item Step 3. $\bH^{(t-1)} \ba^{(t)}$ is close to $\bH \ba^{(t)}$, i.e., $\bH^{(t-1)} \ba^{(t)} = \bH \ba^{(t)} \pm O(\frac{1}{d^6\sqrt{T}})$.
\end{itemize}
In particular, Step 2 and 3 directly implies Eq.~\eqref{eq:lower-goal}.

We first prove a useful bound on the singular values of $\bA^{(t)}$ and $\bH^{(t)}$ for all $t \in [T]$.
First note that the assumption $1\leq \lambda_1(\bH) \leq \lambda_{d}(\bH) \leq 3$ implies that $\frac{1}{\sqrt{3}} \leq \lambda_d(\bA) \leq \lambda_1(\bA) \leq 1$.
Since $\|\ba^{(t)}\|_2 = \|\frac{1}{d^2\sqrt{T}}\cdot \bz^{(t)}\|_2 \leq \frac{1}{d^2\sqrt{T}}$ for any $t\in [T]$, we have that for any $\bx \in \R^d$ with $\|\bx\|_2 = 1$,
\begin{align*}
    \bx^{\top} (\bA^{(T)})^{\top} \bA^{(T)} \bx = &~ \bx^{\top} \bA^{\top} \bA \bx + \sum_{t=1}^T (\bx^{\top} \ba^{(t)})^2 \\
    \leq &~ \|\bx\|_2^2 + \sum_{t=1}^T \|\bx\|_2^2\cdot \|\ba^{(t)}\|_2^2 \leq 1 + \sum_{t=1}^T \frac{1}{d^4 \cdot T} \leq 2.
\end{align*}
Thus we have $\frac{1}{2} \leq \lambda_d(\bA) \leq \lambda_d(\bA^{(t)}) \leq \lambda_1(\bA^{(t)}) \leq \lambda_1(\bA^{(T)}) \leq 2$.

Therefore, the matrix $\bH^{(t)} = ((\bA^{(t)})^{\top} \bA^{(t)})^{-1}$ satisfies $\frac{1}{4} \leq \lambda_d(\bH^{(t)}) \leq \lambda_1(\bH^{(t)}) \leq 4$.
 


\vspace{+2mm}
{\noindent \bf Step 1 \ \ } We prove $\bx^{(t)} = \bx_{\star}^{(t)} \pm \frac{1}{d^4\sqrt{T}}$. Since at each step we add a new label $1$ to the vector $\bb$, we have $\|\bb^{(t)}\|_2 \leq \|\bb^{(T)}\|_2 = \sqrt{T}$.
Since $\bx^{(t)}$ is an $\epsilon$-approximate solution of the least squares regression problem, we have
\begin{align*}
(1+\eps)^2\|\bA^{(t)} \bx_{\star}^{(t)} - \bb^{(t)}\|_2^2 \geq &~ \|\bA^{(t)} \bx^{(t)} - \bb^{(t)}\|_2^2\\
=&~ \|\bA^{(t)} \bx_{\star}^{(t)} - \bb^{(t)}\|_2^2 + \|\bA^{(t)}(\bx^{(t)} - \bx_{\star}^{(t)})\|_2^2\\
\geq&~ \|\bA^{(t)} \bx_{\star}^{(t)} - \bb^{(t)}\|_2^2 + \frac{1}{4}\|\bx^{(t)} -\bx_{\star}^{(t)}\|_2^2.
\end{align*}
The second step follows from $\bA^{(t)} \bx_{\star}^{(t)} - \bb^{(t)} = \big( \bA^{(t)} ((\bA^{(t)})^{\top} \bA^{(t)})^{-1} (\bA^{(t)})^{\top} - \bI \big) \cdot \bb^{(t)} \in \ker[(\bA^{(t)})^{\top}]$ is orthogonal to $\bA^{(t)}(\bx^{(t)} - \bx_{\star}^{(t)}) \in \im[\bA^{(t)}]$, the third step follows from $\lambda_d(\bA^{(t)}) \geq \frac{1}{2}$.

Therefore, we conclude
\begin{align}
    \|\bx^{(t)} -\bx_{\star}^{(t)}\|_2^2 \leq 4(2\eps + \eps^2)\|\bA^{(t)} \bx_{\star}^{(t)} - \bb^{(t)}\|_2^2 \leq 12\eps \cdot \|\bb^{(t)}\|_2^2 \leq 12\eps T \notag\\
    \Rightarrow \quad \|\bx^{(t)} -\bx_{\star}^{(t)}\|_2 < 4\sqrt{\eps T} = O\Big(\frac{1}{d^4\sqrt{T}}\Big)\label{eq:lower-step1}.
\end{align}
The last step follows from $\epsilon = \frac{1}{d^8 T^2}$ in the theorem statement.

\vspace{+2mm}
{\noindent \bf Step 2 \ \ } We prove $\bx^{(t)} - \bx^{(t-1)} = \bH^{(t-1)} \ba^{(t)} \pm O(\frac{1}{d^4\sqrt{T}})$.
By the Woodbury identity, one has
\begin{align}
\bx_{\star}^{(t)} = &~ \big((\bA^{(t)})^{\top}  \bA^{(t)}\big)^{-1}(\bA^{(t)})^{\top} \bb^{(t)}\notag\\
=&~ \big((\bA^{(t-1)})^{\top} \bA^{(t-1)} +\ba^{(t)} (\ba^{(t)})^{\top}\big)^{-1} \cdot ((\bA^{(t-1)})^{\top} \bb^{(t-1)} + \ba^{(t)})\notag\\
=&~ \Big(\bH^{(t-1)} - \bH^{(t-1)} \ba^{(t)} \cdot \big(1+ (\ba^{(t)})^{\top}\bH^{(t-1)} \ba^{(t)}\big)^{-1} \cdot (\ba^{(t)})^{\top}\bH^{(t-1)} \Big) \cdot ((\bA^{(t-1)})^{\top} \bb^{(t-1)} + \ba^{(t)})\notag\\
=&~ \bx_{\star}^{(t-1)} + \bH^{(t-1)} \ba^{(t)} \Big(1 - \big(1+ (\ba^{(t)})^{\top}\bH^{(t-1)} \ba^{(t)}\big)^{-1} (\ba^{(t)})^{\top} (\bx_{\star}^{(t-1)} + \bH^{(t-1)} \ba^{(t)}) \Big)\label{eq:lower-step2}
\end{align}
The first step follows from $\bx_{\star}^{(t)}$ is the optimal solution at step $t$, and the third step follows from the Woodbury identity and $((\bA^{(t-1)})^{\top} \bA^{(t-1)})^{-1} = \bH^{(t-1)}$. We use $\bx_{\star}^{(t-1)} = \bH^{(t-1)} (\bA^{(t-1)})^{\top} \bb^{(t-1)}$ in the fourth step. 

Consequently, we have
\begin{align}
    &~ \bx^{(t)} - \bx^{(t-1)} \notag\\
    = &~ \bx_{\star}^{(t)} - \bx_{\star}^{(t-1)} \pm O\Big(\frac{1}{d^4\sqrt{T}}\Big) \notag\\
    = &~ \bH^{(t-1)} \ba^{(t)}  - \bH^{(t-1)} \ba^{(t)} \big(1+ (\ba^{(t)})^{\top}\bH^{(t-1)} \ba^{(t)}\big)^{-1} (\ba^{(t)})^{\top} (\bH^{(t-1)} \ba^{(t)}+\bx_{\star}^{(t-1)}) \pm O\Big(\frac{1}{d^4\sqrt{T}}\Big)\notag\\
    = &~ \bH^{(t-1)} \ba^{(t)} \pm O\Big(\frac{1}{d^4\sqrt{T}}\Big) \pm O\Big(\frac{1}{d^4\sqrt{T}}\Big)\notag\\
    = &~ \bH^{(t-1)} \ba^{(t)} \pm O\Big(\frac{1}{d^4\sqrt{T}}\Big). \label{eq:lower-step3}
\end{align}
The first step comes from Eq.~\eqref{eq:lower-step1}, the second step follows from Eq.~\eqref{eq:lower-step2}, the third step follows from
\begin{align*}
&~ \|\bH^{(t-1)} \ba^{(t)} \cdot (1+ (\ba^{(t)})^{\top}\bH^{(t-1)} \ba^{(t)})^{-1} \cdot (\ba^{(t)})^{\top} \cdot (\bH^{(t-1)} \ba^{(t)}+\bx_{\star}^{(t-1)})\|_2\\
\leq &~ \|\bH^{(t-1)} \| \cdot \|\ba^{(t)}\|_2 \cdot (1+ (\ba^{(t)})^{\top}\bH^{(t-1)} \ba^{(t)})^{-1} \cdot \| (\ba^{(t)})^{\top}\|_2 \cdot (\|\bH^{(t-1)} \ba^{(t)}\|_2+\|\bx_{\star}^{(t-1)}\|_2)\\
 \leq &~ 4\cdot \frac{1}{d^2\sqrt{T}}\cdot 1 \cdot \frac{1}{d^2\sqrt{T}}\cdot (\frac{4}{d^2\sqrt{T}} + 8\sqrt{T}) = O\Big(\frac{1}{d^4\sqrt{T}}\Big)
\end{align*}
where we use $\|\bH^{(t-1)} \|\leq 4$, $\|\ba^{(t)}\|_2 \leq \frac{1}{d^2\sqrt{T}}$, $(1+ (\ba^{(t)})^{\top}\bH^{(t-1)} \ba^{(t)})^{-1} \leq 1$, $\|\bH^{(t-1)} \ba^{(t)}\|_2 \leq \|\bH^{(t-1)} \|\|\ba^{(t)}\|_2 \leq \frac{4}{d^2\sqrt{T}}$, $\|(\bA^{(t)})^{\top} \|\leq 2$ and
\begin{align*}
\|\bx_{\star}^{(t-1)}\|_2 = &~ \bH^{(t-1)} (\bA^{(t-1)})^{\top} \bb^{(t-1)} \\
\leq &~ \|\bH^{(t-1)}\| \cdot \|(\bA^{(t-1)})^{\top} \| \cdot \|\bb^{(t-1)}\|_2
\leq 4 \cdot 2 \cdot \|\bb^{(t-1)}\|_2\leq 8\sqrt{T}. 
\end{align*}

{\noindent \bf Step 3 \ \ } We prove $\bH^{(t-1)} \ba^{(t)} = \bH \ba^{(t)} \pm O(\frac{1}{d^4\sqrt{T}})$. Denote $\bU = [\ba^{(1)}, \ldots, \ba^{(t-1)}] \in \R^{d\times t}$, we have $\|\bU\| = \|\bU^{\top}\| \leq \|\bU\|_{F} \leq \frac{1}{d^2}$ since $\|\ba^{(i)}\|_2 \leq \frac{1}{d^2 \sqrt{T}}$ for all $i \in [t-1]$. Then we have that 
\begin{align}
 \bH^{(t-1)} \ba^{(t)} 
= &~ (\bA^{\top}\bA + \bU \bU^{\top})^{-1}\ba^{(t)} \notag\\
= &~ (\bH - \bH \bU(\bI + \bU^{\top}\bH \bU)^{-1}\bU^{\top}\bH) \cdot \ba^{(t)} \notag\\
= &~ \bH \ba^{(t)} - \bH \bU(\bI + \bU^{\top}\bH \bU)^{-1}\bU^{\top}\bH \ba^{(t)} \notag\\
= &~ \bH \ba^{(t)} \pm O\Big(\frac{1}{d^6\sqrt{T}}\Big) .\label{eq:lower-step4}
\end{align}
The second step follows from the Woodbury identity and $\bH = (\bA^{\top}\bA)^{-1}$.
The fourth step follows from
\begin{align*}
\| \bH \bU(\bI + \bU^{\top}\bH \bU)^{-1}\bU^{\top}\bH \ba^{(t)} \|_2 \leq &~ \|\bH\| \cdot \|\bU\| \cdot \|(\bI + \bU^{\top}\bH \bU)^{-1}\| \cdot \|\bU^{\top}\| \cdot \|\bH\| \cdot \|\ba^{(t)}\|_2\\
\leq &~ 4 \cdot \frac{1}{d^2} \cdot 1 \cdot\frac{1}{d^2}\cdot 4\cdot \frac{1}{d^2\sqrt{T}} = \frac{16}{d^6\sqrt{T}},
\end{align*}
as $\|\bH\| \leq 4$, $\|\bU\| = \|\bU^{\top}\| \leq \|\bU\|_{F} \leq \frac{1}{d^2}$, $\|(\bI + \bU^{\top}\bH \bU)^{-1}\| \leq 1$, and $\|\ba^{(t)}\|_2 \leq \frac{1}{d^2 \sqrt{T}}$.

\vspace{+2mm}
{\noindent \bf Combining three steps \ \ } We conclude that
\begin{align*}
    \|d^2\sqrt{T}(\bx^{(t)} - \bx^{(t-1)}) - \bH \bz^{(t)}\|_2 = d^2\sqrt{T} \|(\bx^{(t)} - \bx^{(t-1)}) - \bH \ba^{(t)}\|_2 = O(1/d^2)
\end{align*}
where the first step follows from $\ba^{(t)} = \frac{1}{d^2\sqrt{T}}\cdot \bz^{(t)}$, and the second step follows from Eq.~\eqref{eq:lower-step3} and Eq.~\eqref{eq:lower-step4}.
Hence one can recover the matrix-vector query from solutions of dynamic least squares regression. 
On the other side, the reduction only takes $O(d)$ time per update.
Hence, we have shown an $\Omega(d^{2-\gamma})$ lower bound on the amortized running time for incremental least squares regression problem under the $\omv$ conjecture.
\end{proof}





















