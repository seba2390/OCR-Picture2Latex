
\section{Numerical scheme}

We shall solve the PIDE \eqref{kolm_1}--\eqref{kolm_2} numerically with an Alternating Direction Implicit (ADI) method. The scheme is a modification of \cite{LiptonSepp} that is unconditionally stable and has second order of convergence in both time and space step.

In order to deal with a forward equation instead of a backward equation, we change the time variable to $\tau = T - t$, so that
\begin{equation}
	\label{pide_forward}
	\begin{aligned}
		& \frac{\partial V}{\partial \tau} = \mathcal{L} V(\tau, x_1, x_2) - \chi(\tau, x_1, x_2), \\
		& V(\tau, x_1, 0) = \phi_{0, 1}(\tau, x_1), \quad V(\tau, 0, x_2) =  \phi_{0, 2}(\tau, x_2), \\
		& V(\tau, x_1, x_2)  \underset{x_2 \to +\infty}{\longrightarrow} \phi_{\infty, 1}(\tau, x_1), \quad V(\tau, x_2, x_2)  \underset{x_1 \to +\infty}{\longrightarrow}  \phi_{\infty, 2}(\tau, x_2), \\
		& V(0, x_1, x_2) = \psi(x_1, x_2).
	\end{aligned}
\end{equation}

We consider the same grid for integral and differential part of the equation
\begin{equation}
	\begin{aligned}
		0 = x_1^0 < x_1^1 < \ldots < x_1^{m_1}, \\
		0 = x_2^0 < x_2^1 < \ldots < x_2^{m_2},
	\end{aligned}
\end{equation}
where $x_1^{m_1}$ and $x_2^{m_2}$ are large positive numbers.

The grid is non-uniform, and is chosen such that relatively many points lie near the default boundaries for better precision. We use a method similar to \cite{itkin2011jumps} to construct the grid.
\subsection{Discretization of the integral part of the PIDE}
In this section, we shall show how to deal with the integral part of the PIDE, and develop an iterative algorithm for the fast computation of the integral operator on the grid. To this end, we outline the scheme from \cite{LiptonSepp} and then give a new method.

The first approach is to deal with the integral operators directly. After the approximation of the integral, we get (\cite{LiptonSepp})
\begin{align}
	&\mathcal{J}_1 V(x_1 + h, x_2) = e^{-\varsigma_1 h} \mathcal{J}_1 V(x_1, x_2) +  \omega_0(\varsigma_1, h) V(x_1, x_2) + \omega_1(\varsigma_1, h) V(x_1 + h, x_2) + O(h^3), \label{J_1_approx}\\
	& \mathcal{J}_2 V(x_1, x_2 + h) = e^{-\varsigma_2 h} \mathcal{J}_2 V(x_1, x_2) +  \omega_0(\varsigma_2, h) V(x_1, x_2) + \omega_1(\varsigma_2, h) V(x_1, x_2 + h) + O(h^3) \label{J_2_approx},
\end{align}
where
\begin{equation*}
	\omega_0(\varsigma, h) = \frac{1 - (1 + \varsigma h) e^{-\varsigma h}}{\varsigma h}, \quad \omega_1(\varsigma, h) = \frac{-1 + \varsigma h + e^{-\varsigma h}}{\varsigma h}.
\end{equation*}

We can also approximate $\mathcal{J}_{12} V = \mathcal{J}_1 \mathcal{J}_2 V$ by applying above approximations for $\mathcal{J}_1$ and $\mathcal{J}_2$ consecutively.


Consider the grid
\begin{equation}
	\begin{aligned}
		0 = x_1^0 < x_1^1 < \ldots < x_1^{m_1}, \\
		0 = x_2^0 < x_2^1 < \ldots < x_2^{m_2},
	\end{aligned}
\end{equation}
where $x_1^{m_1}$ and $x_2^{m_2}$ are large positive numbers.\\

Then, we can write recurrence formulas for computing the integral operator on the grid. Denote $J_1^{i, j}, J_2^{i, j}$, $J_{12}^{i, j}$ the corresponding approximations of $\mathcal{J}_{1}V(x_1^i, x_2^j)$ , $ \mathcal{J}_{2}V(x_1^i, x_2^j)$, $\mathcal{J}_{12}V(x_1^i, x_2^j)$ on the grid. Applying (\ref{J_1_approx}) and (\ref{J_2_approx}) we get

\begin{align}
	&J_1^{i+1, j} = e^{-\varsigma_1 h^1_{i+1}} J_1^{i, j} + \omega_0(\varsigma_1, h^1_{i+1}) V(x_1^i, x_2^j) + \omega_1(\varsigma_1, h_{i+1}^1) V(x_1^{i+1}, x_2^{j}), \label{J_1_rec}\\
	&J_2^{i, j+1} = e^{-\varsigma_2 h^2_{j+1}} J_2^{i, j} + \omega_0(\varsigma_2, h^2_{j+1}) V(x_1^i, x_2^j) + \omega_1(\varsigma_2, h_{j+1}^2) V(x_1^{i}, x_2^{j+1}), \label{J_2_rec}
\end{align}
where $h_{i+1}^1 = x_1^{i+1} - x_1^{i}, h_{j+1}^2 = x_2^{j+1} - x_2^{j}$.

For an alternative method, we rewrite the integral operator as a differential equation
\begin{align}
		& \frac{\partial}{\partial x_1} \left(\mathcal{J}_1 V(x_1, x_2)e^{\varsigma_1 x_1} \right) =  \varsigma_1 V(x_1, x_2) e^{\varsigma_1 x_1}, \label{J1_ode}\\
		& \frac{\partial}{\partial x_2} \left(\mathcal{J}_2 V(x_1, x_2)e^{\varsigma_2 x_2} \right) =  \varsigma_2 V(x_1, x_2) e^{\varsigma_2 x_2}, \label{J2_ode}\\
		& \frac{\partial^2}{\partial x_1 \partial x_2} \left(\mathcal{J}_{12} V(x_1, x_2)e^{\varsigma_1 x_1 + \varsigma_2 x_2} \right) =  \varsigma_1  \varsigma_2 V(x_1, x_2) e^{\varsigma_1 x_1 + \varsigma_2 x_2}. \label{J12_pde}
\end{align}
Then, we apply the Adams-Moulton method of second order which gives us third order of accuracy locally (\cite{butcher2008numerical})
\begin{align}
	&J_1^{i+1, j} = e^{-\varsigma_1 h^1_{i+1}} J_1^{i, j} + \frac{1}{2} h^1_{i+1} e^{-\varsigma_1 h^1_{i+1}} \varsigma_1  V(x_1^i, x_2^j) + \frac{1}{2} h^1_{i+1} \varsigma_1 V(x_1^{i+1}, x_2^{j}), \label{J_1_rec_adams}\\
	&J_2^{i, j+1} = e^{-\varsigma_2 h^2_{j+1}} J_2^{i, j} +\frac{1}{2} h^2_{j+1} e^{-\varsigma_2 h^2_{j+1}} \varsigma_2  V(x_1^i, x_2^j) + \frac{1}{2} h^2_{j+1} \varsigma_2 V(x_1^{i}, x_2^{j+1}), \label{J_2_rec_adams}
\end{align}
where $h_{i+1}^1 = x_1^{i+1} - x_1^{i}, h_{j+1}^2 = x_2^{j+1} - x_2^{j}$, and is equivalent to the trapezoidal rule.

We can rewrite (\ref{J_1_rec_adams})--(\ref{J_2_rec_adams}) in the same notation as  (\ref{J_1_rec})--(\ref{J_2_rec}) by defining
\begin{equation*}
	\omega_0(\varsigma, h) = \frac{1}{2} h e^{-\varsigma h} \varsigma, \quad \omega_1(\varsigma, h) = \frac{1}{2} h \varsigma.
\end{equation*}
So,
\begin{align*}
	&J_1^{i+1, j} = e^{-\varsigma_1 h^1_{i+1}} J_1^{i, j} + \omega_0(\varsigma_1, h^1_{i+1}) V(x_1^i, x_2^j) + \omega_1(\varsigma_1, h_{i+1}^1) V(x_1^{i+1}, x_2^{j}),\\
	&J_2^{i, j+1} = e^{-\varsigma_2 h^2_{j+1}} J_2^{i, j} + \omega_0(\varsigma_2, h^2_{j+1}) V(x_1^i, x_2^j) + \omega_1(\varsigma_2, h_{j+1}^2) V(x_1^{i}, x_2^{j+1}). 
\end{align*}

As a result we get explicit recursive formulas for approximations of $\mathcal{J}_1 V$ and $\mathcal{J}_2 V$ that can be computed for all grid points via $O(m_1 m_2)$ operations. Both methods give the same order of accuracy. As was discussed above, in order to compute the approximation of $\mathcal{J}_{12} V$ we can apply consecutively the approximations of $\mathcal{J}_2 V$ and $\mathcal{J}_1 (\mathcal{J}_2 V)$. So, we have the two-step procedure:
\begin{equation}
	I_{12}^{i+1, j} = e^{-\varsigma_1 h^1_{i+1}} I_{12}^{i, j} + \omega_0(\varsigma_1, h^1_{i+1}) V(x_1^i, x_2^j) + \omega_1(\varsigma_1, h_{i+1}^1) V(x_1^{i+1}, x_2^{j}), \label{I_12_rec}\\
\end{equation}
and 
\begin{equation}
	J_{12}^{i, j+1} = e^{-\varsigma_2 h^2_{j+1}} J_{12}^{i, j} + \omega_0(\varsigma_2, h^2_{j+1}) I_{12}^{i, j} + \omega_1(\varsigma_2, h_{j+1}^2) I_{12}^{i, j+1}. \label{J_12_rec}\\
\end{equation}
Using this two-step procedure, we can also compute an approximation of $\mathcal{J}_{12} V$ on the grid in complexity $O(m_1 m_2)$.

 We shall subsequently analyze the stability of the second method and use it in the numerical tests. The results for the first method would be very similar.
 
% \paragraph{Eigenvalues of discretized jump operator.}
%\label{jump_eigs}
For the implementation, computing and storing a matrix representation of the jump operator is not necessary, since the operator can be computed iteratively as described above, but we shall use matrix notation for the analysis. We henceforth denote $J_1, J_2$, and $J_{12}$ the matrices of the discretized jump operators. From (\ref{J_1_rec})--({\ref{J_2_rec}) we can find that the matrices $J_1$ and $J_2$ are lower-triangular with diagonal elements $w_1 = \omega_1(\varsigma_1, h_1)$ and $w_2 =  \omega_1(\varsigma_2, h_2)$. Then, $J_{12} = J_1 J_2$ is also a lower-triangular matrix with diagonal elements $w_1 w_2$. To illustrate, in Figure \ref{jump_matrices} we plot the sparsity patterns in $J_1, J_2$, and $J_{12}$.
 \begin{figure}[H]
	\begin{center}
				\subfloat[%Sparsity pattern of 
				$J_1$.]{\includegraphics[width=0.32\textwidth,trim={2cm 0 2cm 0.5cm},clip]{jump_matrix_1.png}}
				\subfloat[%Sparsity pattern of 
				$J_2$.]{\includegraphics[width=0.32\textwidth,trim={2cm 0 2cm 0.5cm},clip]{jump_matrix_2.png}}
				\subfloat[%Sparsity pattern of 
				$J_{12}$.]{\includegraphics[width=0.32\textwidth,trim={2cm 0 2cm 0.5cm},clip]{jump_matrix_12.png}}
	\end{center}		
	\vspace{-20pt}
	\caption{Sparsity pattern of $J_1$, $J_2$, and $J_{12}$. Here, $m_1=m_2=20$ and $nz$ is the number of non-zero elements of the matrices.}
 	\label{jump_matrices}
\end{figure}
%Since the matrices are lower-triangular, the eigenvalues are the diagonal elements. Thus, matrix $J_1$ has all eigenvalues equal to $w_1$, matrix $J_2$ has all eigenvalues equal to $w_2$, and matrix $J_{12}$ has all eigenvalues equal to $w_{12} = w_1 w_2$.

\subsection{Discretization of the differential part of the PIDE}
Now consider the approximation of derivatives in the differential operator on a non-uniform grid. We use the standard derivative approximation (\cite{kluge2002}, \cite{in2010adi}). For the first derivative over each variable consider right-sided, central, and left-sided schemes. So, for the derivative over $x_1$ we have:
\begin{align}
&	\frac{\partial V}{\partial x_1}(x_1^i, x_2^j) \approx \alpha^1_{i, -2} V(x_1^{i-2}, x_2^j) + \alpha^1_{i, -1} V(x_1^{i-1}, x_2^j)+ \alpha^1_{i, 0} V(x_1^i, x_2^j), \label{D_x1_1}\\
&	\frac{\partial V}{\partial x_1}(x_1^i, x_2^j) \approx \beta^1_{i, -1} V(x_1^{i-1}, x_2^j) + \beta^1_{i, 0} V(x_1^{i}, x_2^j)+ \beta^1_{i, 1} V(x_1^{i+1}, x_2^j), \label{D_x1_center}\\
&	\frac{\partial V}{\partial x_1}(x_1^i, x_2^j) \approx \gamma^1_{i, 0} V(x_1^{i}, x_2^j) + \gamma^1_{i, 1} V(x_1^{i+1}, x_2^j)+ \gamma^1_{i, 2} V(x_1^{i+2}, x_2^j) \label{D_x1_2},
\end{align}
while for derivative over $x_2$ we have:
\begin{align}
&	\frac{\partial V}{\partial x_2}(x_1^i, x_2^j) \approx \alpha^2_{j, -2} V(x_1^i, x_2^{j-2}) + \alpha^2_{j, -1} V(x_1^i, x_2^{j-1})+ \alpha^2_{j, 0} V(x_1^i, x_2^j), \label{D_x2_1} \\
&	\frac{\partial V}{\partial x_2}(x_1^i, x_2^j) \approx \beta^2_{j, -1} V(x_1^i, x_2^{j-1}) + \beta^2_{j, 0} V(x_1^{i}, x_2^j)+ \beta^2_{j, 1} V(x_1^j, x_2^{j+1}), \label{D_x2_center}\\
&	\frac{\partial V}{\partial x_2}(x_1^i, x_2^j) \approx \gamma^2_{j, 0} V(x_1^{i}, x_2^j) + \gamma^2_{j, 1} V(x_1^i, x_2^{j+1}, x_2^j)+ \gamma^2_{j, 2} V(x_1^i, x_2^{j+2}), \label{D_x2_2}
\end{align}
with coefficients
\begin{equation*}
	\begin{aligned}
		\alpha^k_{i, -2} &= \frac{\Delta x_k^i}{\Delta x_k^{i-1} (\Delta x_k^{i-1} + \Delta x_k^i)},  & \alpha^k_{i, -1} &= \frac{-\Delta x_k^{i-1} - \Delta x_k^i}{\Delta x_k^{i-1} \Delta x_k^i},  &\alpha^k_{i, 0} &= \frac{\Delta x_k^{i-1} + 2 \Delta x_k^i}{\Delta x_k^i (\Delta x_k^{i-1} + \Delta x_k^i)} , \\
		\beta^k_{i, -1} &= \frac{-\Delta x_k^{i+1}}{\Delta x_k^{i} (\Delta x_k^{i} + \Delta x_k^{i+1})}, & \beta^k_{i, 0} &= \frac{\Delta x_k^{i+1} - \Delta x_k^i}{\Delta x_k^{i} \Delta x_k^{i+1}},  &\beta^k_{i, 1} &= \frac{\Delta x_k^{i}}{\Delta x_k^{i+1} (\Delta x_k^{i} + \Delta x_k^{i+1})} , \\
		\gamma^k_{i, 0} &= \frac{-2\Delta x_k^{i+1} - \Delta x_k^{i+2}}{\Delta x_k^{i+1} (\Delta x_k^{i+1} + \Delta x_k^{i+2})}, & \gamma^k_{i, 1} &= \frac{\Delta x_k^{i+1} + \Delta x_k^{i+2}}{\Delta x_k^{i+1} \Delta x_k^{i+2}},  & \gamma^k_{i, 2} &= \frac{-\Delta x_k^{i+1}}{\Delta x_k^{i+2} (\Delta x_k^{i+1} + \Delta x_k^{i+2})} .
	\end{aligned}
\end{equation*}
For the boundaries at $0$ we use the schemes \eqref{D_x1_1} and \eqref{D_x2_1}, for the right boundaries at $x_1^{m_1}$ and $x_2^{m_2}$ we use the schemes \eqref{D_x1_2} and \eqref{D_x2_2}, and for other points we use the central schemes \eqref{D_x1_center} and \eqref{D_x2_center}.

To approximate the second derivative we use the central scheme:
\begin{align}
	&	\frac{\partial^2 V}{\partial x_1^2}(x_1^i, x_2^j) \approx \delta^1_{i, -1} V(x_1^{i-1}, x_2^j) + \delta^1_{i, 0} V(x_1^{i}, x_2^j)+ \delta^1_{i, 1} V(x_1^{i+1}, x_2^j), \label{D2_x1} \\
&	\frac{\partial^2 V}{\partial x_2^2}(x_1^i, x_2^j) \approx \delta^2_{j, -1} V(x_1^i, x_2^{j-1}) + \delta^2_{j, 0} V(x_1^{i}, x_2^j)+ \delta^2_{j, 1} V(x_1^j, x_2^{j+1}) \label{D2_x2},
\end{align}
with coefficients
\begin{equation*}
		\delta^k_{i, -1} = \frac{2}{\Delta x_k^{i} (\Delta x_k^{i} + \Delta x_k^{i+1})}, \quad \delta^k_{i, 0} = \frac{-2}{\Delta x_k^{i} \Delta x_k^{i+1}}, \quad \delta^k_{i, 1} = \frac{2}{\Delta x_k^{i+1} (\Delta x_k^{i} + \Delta x_k^{i+1})},
\end{equation*}	
and to approximate the second mixed derivative we use the scheme:
\begin{equation}
	\frac{\partial^2 V}{\partial x_1 \partial x_2} (x_1^i, x_2^j) \approx \sum_{k, l = -1}^1 \beta_{i, k}^1 \beta_{j, l}^2 V(x_1^{i+k}, x_2^{j+l}). \label{D_x1x2}	
\end{equation}

As a result, we can approximate the differential operator $\mathcal{D} V$ by a discrete operator
\begin{equation}
	D V = D_1 V + D_2 V + D_{12} V,
\end{equation}
where $D_1 V$ contains the discretized derivatives over $x_1$ defined in (\ref{D_x1_1})--(\ref{D_x1_2}) and (\ref{D2_x1}), $D_2 V$ contains the discretized derivatives over $x_2$ defined in (\ref{D_x2_1})--(\ref{D_x2_2}) and (\ref{D2_x2}), and $D_{12} V$ contains the discretized mixed derivative defined in (\ref{D_x1x2}).

By straightforward but lengthy Taylor expansion of the expression in (\ref{D_x1_1})--(\ref{D_x1x2}), the scheme (\ref{HV_scheme}) has  second order truncation error in variables $x_1$ and $x_2$ for meshes which are either uniform or smooth transformations of such meshes, as we shall consider later.

\subsection{Time discretization: ADI scheme}
After discretization over $(x_1, x_2)$ we can rewrite PIDE (\ref{pide_forward}) as a system of ordinary (linear) differential equations. Consider the vector $U(t) \in \mathbb{R}^{m_1m_2 \times 1}$ whose elements correspond to $V(t, x_1^i, x_2^j)$. Then
\begin{equation}
	\begin{aligned}
		& U'(t) = \tilde{A} U(t) + b(t), \\
		& U(0) = U_0,
	\end{aligned}
\end{equation}
where $\tilde{A} = D_1 + D_2 + D_{12} + \lambda_1 J_1 + \lambda_2 J_2 + \lambda_{12} J_{12} - (\lambda_1 + \lambda_2 + \lambda_{12}) I$, and $b(t)$ is determined from boundary conditions and the right-hand side.

To solve this system, we apply an ADI scheme for the time discretization. Consider, for simplicity, a uniform time mesh with time step $\Delta t: t_n = n \Delta t, n = 0, \ldots, N-1$. 

We decompose the matrix $\tilde{A}$  into three matrices, $\tilde{A} = \tilde{A}_0 + \tilde{A}_1 + \tilde{A}_2$, where
\begin{align*}
	& \tilde{A}_0 =  D_{12} + \lambda_1 J_1 + \lambda_2 J_2 + \lambda_{12} J_{12},  \\
  	& \tilde{A}_1 = D_1 - \left(\lambda_1 + \frac{\lambda_{12}}{2} \right) I, \\
	  & \tilde{A}_2 = D_2 - \left(\lambda_2 + \frac{\lambda_{12}}{2} \right) I,
\end{align*}
and $b(t) = b_0(t) + b_1(t) + b_2(t)$, where $b_0(t)$ corresponds to the right-hand side and the FD discretization of the mixed derivatives on the boundary, $b_1(t)$ and $b_2(t)$ correspond to the FD discretization of the derivatives over $x_1$ and $x_2$ on the boundary.

Now we can apply a traditional ADI scheme with matrices $\tilde{A}_0, \tilde{A}_1$, and $\tilde{A}_2$. We choose the Hundsdorfer--Verwer (HV) scheme (\cite{HV}) in order to have second order accuracy in the time variable, and unconditional stability, as we shall prove below. For convenience, denote
\begin{align}
	& F_j(t, x) = \tilde{A}_j x + b_j(t), \quad j = 0, 1, 2, \label{F_j}\\
	& F(t, x) = (\tilde{A}_0 + \tilde{A}_1 + \tilde{A}_2 ) x + (b_0(t) + b_1(t) + b_2(t)),
\end{align}
and apply the Hundsdorfer--Verwer (HV) scheme:
\begin{equation}
	\label{HV_scheme}
	\left\{
	\begin{aligned}
	&	Y_0 = U_{n-1} + \Delta t F(t_{n-1}, U_{n-1}), \\
	&	Y_j = Y_{j-1} + \theta \Delta t (F_j(t_n, Y_j) - F_j(t_n, U_{n-1})), \quad j = 1, 2, \\
	&	\tilde{Y}_0 = Y_0 + \sigma \Delta t (F(t_n, Y_2) - F(t_{n-1}, U_{n-1})), \\
	&	\tilde{Y}_j = \tilde{Y}_{j-1} + \theta \Delta t (F_j(t_n, \tilde{Y}_j - F_j(t_n, Y_2)), \quad j = 1, 2, \\
	&	U_n = \tilde{Y}_2.
	\end{aligned}
	\right.
\end{equation}

In this scheme, parts that contain $F_1$ and $F_2$ are treated implicitly. The matrix $\tilde{A}_1$ is tridiagonal and $\tilde{A}_2$ is block-tridiagonal and can be inverted via $O(m_1 m_2)$ operations.  As a result, the overall complexity is $O(m_1 m_2)$ for a single time step or $O(N m_1 m_2)$ for the whole procedure.

Moreover, the scheme has second order of consistency in both $(x_1, x_2)$ and $t$ for any given $\theta$ and $\sigma = \frac{1}{2}$. 

\subsection{Stability analysis}
In this section, we consider the PIDE \eqref{pide_forward} with zero boundary conditions at $0$ in both directions and on a uniform grid,
such that $F_j(t, x) = \tilde{A}_j x$ and
\begin{equation}
	\label{HV_nobound}
	\left\{
	\begin{aligned}
	&	Y_0 = U_{n-1} + \Delta t \tilde{A} U_{n-1}, \\
	&	Y_j = Y_{j-1} + \theta \Delta t (\tilde{A}_j Y_j - \tilde{A}_j U_{n-1}),\quad j = 1, 2, \\
	&	\tilde{Y}_0 = Y_0 + \sigma \Delta t (\tilde{A} Y_2- \tilde{A} U_{n-1}), \\
	&	\tilde{Y}_j = \tilde{Y}_{j-1} + \theta \Delta t (\tilde{A}_j\tilde{Y}_j - \tilde{A}_j Y_2), \quad j = 1, 2 \\
	&	U_n = \tilde{Y}_2.
	\end{aligned}
	\right.
\end{equation}

For convenience, we denote by $F: U_n = F U_{n-1}$.

We further consider the PDE on $\mathbb{R}^2$, i.e., without default boundaries. Hence, we assume that diffusion and jump operators are discretized on 
 an infinite, uniform mesh $\{(j_1 h_1, j_2 h_2), (j_1, j_2) \in \mathbb{Z}^2\}$, such that, e.g.\ $D_1, D_2, D_{12}, J_1, J_2$ are infinite matrices.
 This is different to \cite{intHoutStability}, where finite matrices and periodic boundary conditions (without integral terms) are considered.

We use von Neumann stability analysis, as first introduced by \cite{charney1950numerical}, by expanding the solution into a Fourier series.
Hence, we shall show that the proposed scheme (\ref{HV_nobound}) is unconditionally stable,
 i.e.\ we will show that all eigenvalues of the operator  $F$ have moduli bounded by 1 plus an $O(\Delta t)$ term,
 where the corresponding eigenfunctions are given by $\exp(i \phi_1 j_1) \exp(i \phi_2 j_2)$, with $\phi_1$ and $\phi_2$ the wave numbers and
 $j_1$ and $j_2$ the grid coordinates.
 
%\paragraph{Stability analysis of scheme (\ref{HV_nobound})}
 \cite{intHoutStability} show that when all matrices commute
(as in the PDE case with periodic boundary conditions), 
the eigenvalues for $F$ are given by %this leads to the condition %(\cite{intHoutStability})
%\begin{equation}
%	\label{stab_eq}
%	|T(\tilde{z}_0, \tilde{z}_1, \tilde{z}_2)| \le 1,
%\end{equation}
\begin{eqnarray}
\label{defT}
T(\tilde{z}_0, \tilde{z}_1, \tilde{z}_2) &=& 1 + 2 \frac{\tilde{z}_0 + \tilde{z}}{p} - \frac{\tilde{z}_0 + \tilde{z}}{p^2} + \sigma \frac{(\tilde{z}_0 + \tilde{z})^2}{p^2} \quad \text{with} \\
	p &=& (1 - \theta \tilde{z}_1) (1 - \theta \tilde{z}_2), \nonumber
\end{eqnarray}
where $\tilde{z}_j = \tilde{\mu}_j \Delta t$, where $\tilde{\mu}_j$ is an eigenvalue of $\tilde{A}_j$, $j = 0, 1, 2$, $\tilde{z} = \tilde{z}_1 + \tilde{z}_2$, $\theta \ge 0$.


The analysis is made slightly more complicated in our case through the presence of the  jump operators.
In the remainder of this section, %we first show that for our case \eqref{stab_eq} can still be applied, and then 
we show that stability is still given under the same conditions on $\theta$ and $\sigma$ as in the purely diffusive case. For the correspondence of notation with \cite{intHoutStability}, we denote $A = A_0 + A_1 + A_2$, where $A_0 = D_{12}, A_1 = D_1$, $A_2 = D_2$ and $\mu_0, \mu_1$, and $\mu_2$ are the eigenvalues of the corresponding matrices. Similar to $\tilde{z}_0, \tilde{z}_1,$ and $\tilde{z}_2$, we define scaled eigenvalues $z_0 = \mu_0 \Delta t, z_1 = \mu_1 \Delta t, z_2 = \mu_2 \Delta t$.



%Using properties of lower-triangular matrices, we can easily see that the matrix $F$ from \eqref{HV_nobound} can be represented as $F = U T_F U^{*}$, where $T_F$ is a lower-triangular matrix, whose eigenvalues are equal to \eqref{defT}.




%\begin{lemma}
%\label{lemma_commute}
%The following identities are satisfied
We have the eigenvalues $\tilde{\mu}_j$ of $\tilde{A}_j$ given by
\begin{align}
	& \tilde{\mu}_0 = \mu_0 + \lambda_1 w_1 + \lambda_2 w_2 + \lambda_{12} w_{12}, \label{mu_0_eq} \\
	& \tilde{\mu}_1 = \mu_1 - \left(\lambda_1 + \frac{\lambda_{12}}{2}\right), \label{mu_1_eq}\\
	& \tilde{\mu}_2 = \mu_2 - \left(\lambda_2 + \frac{\lambda_{12}}{2}\right) \label{mu_2_eq},
\end{align}
where $\mu_j$ is  an eigenvalue of $A_j$, and $w_1, w_2$, and $w_{12}$ are eigenvalues of $J_1, J_2$, and $J_{12}$.
%\end{lemma}

Denote % $z_j = \mu_j \Delta t$ with $\mu_j$  an eigenvalue of $A_j$, 
$z = z_1 + z_2$, $s_1 = w_1  \Delta t, s_2 = w_2 \Delta t, s_{12} = w_{12} \Delta t$, where $w_1, w_2, w_{12}$ are eigenvalues of $J_1, J_2, J_{12}$ respectively, and $s_0 = \lambda_1 s_1 + \lambda_2 s_2 + \lambda_{12} s_{12}$.

	Multiplying (\ref{mu_0_eq})--(\ref{mu_2_eq}) by $\Delta t$, we have 
	\begin{align} 
		& \tilde{z}_0 = z_0 + s_0, \label{tilde_z0} \\
		& \tilde{z}_1 = z_1 - \left(\lambda_1 + \frac{\lambda_{12}}{2}\right) \Delta t, \\
		& \tilde{z}_2 = z_2 - \left(\lambda_2 + \frac{\lambda_{12}}{2}\right) \Delta t.  \label{tilde_z2} 		
	\end{align}
\begin{theorem}[\cite{intHoutStability}, Theorem 3.2]
	\label{theor_inthout}
	Assume $\Re({z}_1) \le 0, \Re({z}_2) \le 0$, $|{z}_0| \le 2\sqrt{\Re({z}_1) \Re({z}_2)}$, where ${z}_0, {z}_1$, and ${z}_2$ are the eigenvalues of ${A}_0, {A}_1$, and ${A}_2$, %in \eqref{F_j} 
	and
	\begin{equation*}
		\frac{1}{2} \le \sigma \le \left(1 + \frac{\sqrt{2}}{2} \right) \theta.
	\end{equation*}
	Then,
	\begin{equation*}
		|T({z}_0, {z}_1, {z}_2)| \le 1,
	\end{equation*}
	and the Hundsdorfer--Verwer scheme \eqref{HV_nobound} is stable in the purely diffusive case.
\end{theorem}

\begin{lemma}
The scaled eigenvalues of $A_0$, $A_1$, $A_2$, $J_1$, $J_2$, $J_{12}$ can be expressed as
\begin{eqnarray}
\label{z0}
	z_0 &=& -\rho b [\sin{\phi_1} \sin{\phi_2}],  \\
	\label{z1}
	z_1 &=& -a_1 (1 - \cos{\phi_1}) + i \xi_1 q_1 \sin{\phi_1}, \\
	\label{z2}
	z_2 &=& -a_2 (1 - \cos{\phi_2}) + i \xi_2 q_2 \sin{\phi_2}, \\
%	s_1 &=& \Delta t \, \zeta_1 h_1 \left(\frac{1}{2} + \frac{\exp(-h_1 (\zeta_1 + i \phi_1))}{1-\exp(-h_1 (\zeta_1 + i \phi_1))} \right), \\
%	s_2 &=& \Delta t \, \zeta_2 h_2 \left(\frac{1}{2} + \frac{\exp(-h_2 (\zeta_2 + i \phi_2))}{1-\exp(-h_2 (\zeta_2 + i \phi_2))} \right), \\
	s_1 &=& \Delta t \, \zeta_1 h_1 \left(\frac{1}{2} + \frac{\exp(-h_1 \zeta_1 + i \phi_1)}{1-\exp(-h_1 \zeta_1 + i \phi_1)} \right), \\
	s_2 &=& \Delta t \, \zeta_2 h_2 \left(\frac{1}{2} + \frac{\exp(-h_2 \zeta_2 + i \phi_2)}{1-\exp(-h_2 \zeta_2 + i \phi_2)} \right), \\
	s_{12} &=& s_1 s_2/\Delta t,
\end{eqnarray}
where
\begin{equation*}
	q_1 = \frac{\Delta t}{h_1}, \quad q_2 = \frac{\Delta t}{h_2}, \quad a_1 = \frac{\Delta t}{h_1^2}, \quad a_2 = \frac{\Delta t}{h_2^2}, \quad b= \frac{\Delta t}{h_1 h_2},
\end{equation*}
and $\phi_j \in [0, 2 \pi]$ for $j = 1, 2$.

Moreover,
\begin{equation}
\label{karelineq}
	|z_0| \le 2 \sqrt{\Re(z_1) \Re(z_2)}.
\end{equation}	
\end{lemma}
\begin{proof}
	All six eigenvalues follow by insertion of the ansatz $U=\exp(i \phi_1 j_1) \exp(i \phi_2 j_2)$.
	For instance, %for $U(j,k)=$
	\[
	(J_1 U)(j_1,j_2) = \zeta_1 h_1 \left(\frac{1}{2} U(j_1,j_2) +  \sum_{k=1}^\infty \exp(-\zeta_1 h_1 k) U(j_1-k,j_2) \right),
	\]
	and the result follows by using the special form of $U$ and evaluating the geometric series.
	
	Alternatively, the first three equations follow immediately from the eigenvalues for finite matrices (\cite{intHoutStability}, p.29),
	which are given by (\ref{z0})--(\ref{z2}) where $\phi_j = 2 l \pi/m_j$, $l=1,\ldots,m_j$.
	In the infinite mesh case, the spectrum is the continuous limit and (\ref{karelineq}) still holds.
%	 then the eigenvalues of the semi-infinite (banded Toeplitz) matrix are given by the Schmidt and Spitzer Theorem (see Theorem 11.17 in \cite{bottcher2005spectral}) as precisely the limits of sequences of eigenvalues of the finite-dimensional matrices.
\end{proof}


%\begin{lemma}
%\label{lemma_z0ineq}
%For $\tilde{z}_0, \tilde{z}_1, \tilde{z}_2$ from \eqref{tilde_z0}--\eqref{tilde_z2},
%\begin{equation}
%	\label{z_0_ineq}
%	|\tilde{z}_0|^2 \le 4 \Re(\tilde{z}_1) \Re(\tilde{z}_2) + c \Delta t,
%\end{equation}
%for some constant $c$ that does not depend on $\Delta t, h_1,$ and $h_2$.
%\end{lemma}
%\begin{proof}
%Define, 
%\begin{gather*}
%	\tilde{\lambda}_1 = \left(\lambda_1 + \frac{\lambda_{12}}{2} \right) \Delta t, \\
%	\tilde{\lambda}_2 = \left(\lambda_2 + \frac{\lambda_{12}}{2} \right) \Delta t.
%\end{gather*}
%Then,
%\begin{align}
%	|\tilde{z}_0|^2 = |z_0 + s_0|^2 = |z_0|^2 + 2 z_0 s_0 + s_0^2 &\le 4 \Re(z_1) \Re(z_2) + 2 z_0 s_0 + s_0^2  \\
%	& = 4 (\Re(\tilde{z}_1) + \tilde{\lambda}_1)( \Re(\tilde{z}_2) + \tilde{\lambda}_2) + 2 z_0 s_0 + s_0^2 .
%\end{align}
%We can rewrite the first term as
%\begin{equation*}
%	 (\Re(\tilde{z}_1) + \tilde{\lambda}_1)( \Re(\tilde{z}_2) + \tilde{\lambda}_2) = \Re(\tilde{z}_1) \Re(\tilde{z}_2) + \Re(z_1) \tilde{\lambda}_2 + \Re(z_2) \tilde{\lambda}_1 - \tilde{\lambda}_1 \tilde{\lambda}_2.
%\end{equation*}
%Thus, we have
%\begin{equation*}
%	|z_0 + s_0|^2 \le 4\Re(\tilde{z}_1) \Re(\tilde{z}_2) + 4 \Re(z_1) \tilde{\lambda}_2 + 4 \Re(z_2) \tilde{\lambda}_1 - 4\tilde{\lambda}_1 \tilde{\lambda}_2 + 2 z_0 s_0 + s_0^2.
%\end{equation*}
%From \eqref{z0}--\eqref{z2}, we can see that $z_0, z_1$, and $z_2$ does not depend on $\Delta t$ (they depend on $\frac{\Delta t}{h_i}$, which is assumed to be constant), while $\tilde{\lambda_1}, \tilde{\lambda_2},$ and $s_0$ are proportional to $\Delta t$. Thus
%\begin{equation}
%	4 \Re(z_1) \tilde{\lambda}_2 + 4 \Re(z_2) \tilde{\lambda}_1 - 4\tilde{\lambda}_1 \tilde{\lambda}_2 + 2 z_0 s_0 + s_0^2 \le c \Delta t,
%\end{equation}
%for some constant $c$.
%
%Thus, we have proved the inequality (\ref{z_0_ineq}).
%\end{proof}

\begin{theorem}
	Consider $\frac{1}{2} \le \sigma \le \left(1 + \frac{\sqrt{2}}{2} \right) \theta$. Then there exists $c>0$, independent of $\Delta t\le 1$, $h_1$ and $h_2$, such that
	\begin{enumerate}
	\item
	\begin{equation}
	|T(\tilde{z}_0, \tilde{z}_1, \tilde{z}_2)| \le 1 + c \Delta t, \qquad \forall \phi_1, \phi_2 \in [0,2\pi],
	\end{equation}
	i.e., the scheme is von Neumann stable;
	\item
	\label{part2}
	\begin{equation}
	|U_n|_2 %:= \sum_{j=-\infty}^{\infty} U_{n}(j)^2 
	\le {\rm e}^{c n \Delta t} |U_0|_2, \qquad \forall n\ge 0,
	\end{equation}
	for $|U_n|_2 = h_1 h_2 \left(\sum_{j_1,j_2=-\infty}^\infty |U_n(j_1,j_2)|^2\right)^{\scriptsize 1/2}$, i.e., the scheme is $l_2$ stable.
	\end{enumerate}
\end{theorem}
\begin{proof}


%The first inequality is a weak version of Theorem 3.2 of  \cite{intHoutStability} and can be proved in the similar way by 
%adding the term $c \Delta t$.

First, we have that
\begin{eqnarray*}
|T({z}_0, \tilde{z}_1, \tilde{z}_2)| = \left|1 + 2 \frac{{z}_0 + \tilde{z}}{p} - \frac{{z}_0 + \tilde{z}}{p^2} + \sigma \frac{({z}_0 + \tilde{z})^2}{p^2}\right|
\le 1,
\end{eqnarray*}
where as before $p = (1 - \theta \tilde{z}_1) (1 - \theta \tilde{z}_2)$ and $\tilde{z} = \tilde{z}_1 + \tilde{z}_2$. 
This follows from Theorem \ref{theor_inthout} because $\lambda_1$, $\lambda_2$ and $\lambda_{12}$ are positive and therefore (\ref{karelineq}) is still satisfied with $z_1$ and $z_2$ replaced by $\tilde{z}_1$ and $\tilde{z}_2$.

We have
\begin{eqnarray*}
T(\tilde{z}_0, \tilde{z}_1, \tilde{z}_2) &=&
T({z}_0, \tilde{z}_1, \tilde{z}_2) +
2 \frac{s_0}{p} - \frac{s_0}{p^2} + \sigma \frac{2 s_0 (z_0 + \tilde{z}) + s_0^2}{p^2}.
\end{eqnarray*}
A simple calculation shows that $|s_0|\le c_0 \, \Delta t$ for a constant $c_0$ (independent of $\Delta t, h_1, h_2, \phi_1$, $\phi_2$;
indeed, $c_0=2 \lambda_1 + 2 \lambda_2 + 4 \lambda_{12}$
works for small enough $h_1$, $h_2$).
Therefore, and because $|p|\ge 1$, $|z_0 + \tilde{z}|/|p|\le c_1$ for a constant $c_1$,
\[
\left|2 \frac{s_0}{p} - \frac{s_0}{p^2} + \sigma \frac{2 s_0 (z_0 + \tilde{z}) + s_0^2}{p^2} \right|
\le c \Delta t,
\]
for any $c\ge (3 + 2 \sigma c_1 + c_0 \sigma) c_0$.
From this the first statement follows.

We can now deduce part \ref{part2} by a standard argument. For the discrete-continuous Fourier transform
\[
%\mathcal{F}: 
l_2(\mathbb{Z}^2) \rightarrow L_2(-\pi,\pi)^2, \qquad U \rightarrow \widehat{U}, \qquad \widehat{U}(\phi_1,\phi_2) = h_1 h_2 \sum_{j,k \in \mathbb{Z}} U(j,k) {\rm e}^{-i (j \phi_1 + k \phi_2)},
\]
we have
\[
\widehat{U}_{n+1}(\phi_1,\phi_2) = T(\tilde{z}_0, \tilde{z}_1, \tilde{z}_2) \, \widehat{U}_{n}(\phi_1,\phi_2), \qquad \forall n\ge 0.
\]
Then, by Parseval,
\begin{eqnarray*}
|U_n|_2^2 &=& \frac{1}{4 \pi^2} |\widehat{U}_n|^2  \\
&=& \frac{1}{4 \pi^2} 
\frac{1}{h_1^2 h_2^2} \int_{-\pi}^\pi |\widehat{U}_n(\phi_1,\phi_2)|^2 
\, {\rm d} \phi_1 \, {\rm d} \phi_2 \\
&\le& \frac{1}{4 \pi^2} 
\frac{1}{h_1^2 h_2^2} \int_{-\pi}^\pi (1+c \Delta t)^{2n} |\widehat{U}_0(\phi_1,\phi_2)|^2 
\, {\rm d} \phi_1 \, {\rm d} \phi_2 \\
&\le& {\rm e}^{2 c n \Delta t}  \frac{1}{4 \pi^2} 
\frac{1}{h_1^2 h_2^2} \int_{-\pi}^\pi |\widehat{U}_0(\phi_1,\phi_2)|^2 
\, {\rm d} \phi_1 \, {\rm d} \phi_2 \\ 
&=& {\rm e}^{2 c n \Delta t}  |U_0|_2^2.
\end{eqnarray*}
\end{proof}



%\begin{remark}
This ($l_2$-)stability result together with second order consistency implies ($l_2$-)convergence of second order for all solutions which
are sufficiently smooth that the truncation error is defined and bounded. In our setting, where the initial condition is discontinuous, this is not given. Since the step function lies in the ($l_2$-)closure of smooth functions, convergence is guaranteed, but usually not of second order. We show this empirically in the next section and demonstrate how second order convergence can be restored practically.
%\end{remark}




\subsection{Discontinuous boundary and terminal conditions}

It is well documented (see, e.g.\ \cite{pooley2003}) that the spatial convergence order of central finite difference schemes is generally reduced to one for discontinuous payoffs. Moreover, the time convergence order of the Crank-Nicolson scheme is reduced to one due to the lack of damping of high-frequency components of the error, and this behaviour is inherited by the HV scheme. We address these two issues in the following way.

First, we smooth the terminal condition by the method of local averaging from \cite{pooley2003}, i.e., instead of using nodal values of $\phi$ directly, we use the approximation
\[
\phi(x_1^i,x_2^j) \approx \frac{1}{h_1 h_2} \int_{x_2^i-h_2/2}^{x_2^i+h_2/2} \int_{x_1^j-h_1/2}^{x_1^j+h_1/2}
\phi(\xi_1,\xi_2) \, d\xi_1 d\xi_2.
\]
For step functions with values of 0 and 1, this procedure attaches to each node the fraction of the area where the payoff is 1, in a cell of of size $h
_1 \times h_2$ centred at this point.

We illustrate the convergence improvement on the example of joint survival probabilities. Other quantities show a similar behaviour.
The model parameters in the following tests are the same as in the next section, specifically Table \ref{table:params}.

We choose $\sigma = \frac{1}{2}$ and $\theta = \frac{3}{4}$ in the HV scheme. 

The observed convergence with and without this smoothing procedure is shown in Figure \ref{fig_conv1}.
We choose the $l_2$-norm for its closeness to the stability analysis -- in the periodic case, Fourier analysis gives convergence results in $l_2$ -- and the  $l_\infty$-norm for its relevance to the problem at hand, where we are interested in the solution pointwise.
The behaviour in the $l_1$-norm is very similar.

Hereby, for a method of order $p\ge 1$ we estimate the error by extrapolation as
\begin{equation*}
|Q^{nX}(x_1, x_2) - Q(x_1, x_2)| \approx \frac{1}{2^p-1}  |Q^{nX}(x_1, x_2) - Q^{nX/2}(x_1, x_2)|,
\end{equation*}
where $Q$ is the exact solution, $Q^{nX}$ the solution with $nX$ mesh points,
and the norms are computed by either taking the maximum over mesh points or numerical quadrature.
Here, $nT=1000$ is fixed.



\begin{figure}[H]
	\begin{center}
%				\subfloat[$l_1$-norm.]{\includegraphics[width=0.33\textwidth]{conv_analysis2.png}}
%				\subfloat[$l_2$-norm.]{\includegraphics[width=0.33\textwidth]{conv_analysis1.png}}
%				\subfloat[$l_{\infty}$-norm.]{\includegraphics[width=0.33\textwidth]{conv_analysis3.png}}\\
%				\subfloat[$l_1$-norm.]{\includegraphics[width=0.45\textwidth]{conv_analysis2.png}}
				\subfloat[$l_2$-norm.]{\includegraphics[width=0.49\textwidth]{conv_analysis1.png}} \hfill
				\subfloat[$l_{\infty}$-norm.]{\includegraphics[width=0.49\textwidth]{conv_analysis3.png}}\\
	\end{center}		
	\vspace{-20pt}
	\caption{Convergence analysis for $l_2$- and $l_{\infty}$-norms of the error depending on the mesh size with fixed time-step.}
 	\label{fig_conv1}
\end{figure}

The convergence is clearly of first order without averaging and of second order with averaging.


Second, we modify the scheme using the idea from \cite{reisinger2013} by changing the time variable $\tilde{t} = \sqrt{t}$. This change of variables 
leads to the new PDE
\[
		\frac{\partial V}{\partial \tilde{t}} + 2 \tilde{t} \mathcal{L} V = 2 \tau \chi(\tilde{t}^2, x),
\]
instead of (\ref{kolm_1}), to which we apply the numerical scheme. %, but improves the convergence rate. 
%From numerical results in Section \ref{numerical_experiments}, we can observe second order of convergence.


In Figure \ref{fig_conv2}, we show the convergence with and without time change, estimating the errors in a similar way to above, with $nX=800$ fixed.
 \begin{figure}[H]
	\begin{center}
%				\subfloat[$l_1$-norm.]{\includegraphics[width=0.33\textwidth]{conv_analysis5.png}}
%				\subfloat[$l_2$-norm.]{\includegraphics[width=0.33\textwidth]{conv_analysis4.png}}
%				\subfloat[$l_{\infty}$-norm.]{\includegraphics[width=0.33\textwidth]{conv_analysis6.png}}\\
%				\subfloat[$l_1$-norm.]{\includegraphics[width=0.45\textwidth]{conv_analysis5.png}}
				\subfloat[$l_2$-norm.]{\includegraphics[width=0.49\textwidth]{conv_analysis4.png}} \hfill
				\subfloat[$l_{\infty}$-norm.]{\includegraphics[width=0.49\textwidth]{conv_analysis6.png}}\\

	\end{center}		
	\vspace{-20pt}
	\caption{Convergence analysis for $l_2$- and $l_{\infty}$-norms of the error depending on time-step with fixed mesh size.}
 	\label{fig_conv2}
\end{figure}

The convergence is clearly of first order without time change and of second order with time change. We took here $T=5$ to illustrate the effect more clearly.

%To justify second order convergence rate, in Figure \ref{fig_conv} we present the $l_2$-norm of error depending on the mesh size computed for joint survival probability. %We choose the number of time steps $n = 2m$, where $m$ is the mesh size in each direction.
% \begin{figure}[H]
%	\begin{center}
%		\includegraphics[width=0.9\textwidth]{conv_analysis.png}
%	\end{center}
%	\vspace{-20pt}
%	\caption{$l_2$ norm of error depending on the mesh size}
% 	\label{fig_conv}
%\end{figure}
%
