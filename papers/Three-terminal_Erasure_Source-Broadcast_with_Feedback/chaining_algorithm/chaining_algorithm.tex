\subsubsection{A Chaining Algorithm}
\label{subsubsec:chaining_algorithm}

In this section, we propose an alternative to using queue preprocessing for channel coding as described in the previous section.  Recall that we invoke this algorithm when we have exhausted all opportunities to send instantly-decodable, distortion-innovative symbols which happens when we are left with queues $\Q{1,2}, \Q{1,3}$ and $\Q{2,3}$, or queues $Q_i$, \Q{i,j} and \Q{i,k}.  We again assume that the latter is the case.

We first consider if we were to ignore user~$i$ and simply use the coding scheme of~\cite{TMKS_TIT20} to target point-to-point optimal performance for the remaining users.  Specifically, since queues $\Q{i,j}$ and $\Q{i,k}$ remain, we can send linear combinations of the form $q_{i,j} \oplus q_{i, k}$, where $q_{i, j} \in \Q{i, j}$ and $q_{i, k} \in \Q{i, k}$.  Recall that user~$j$ has received all symbols in $\Q{i,k}$, and user~$k$ has similarly received all symbols in $\Q{i,j}$.  In each transmission, each user can therefore subtract off one of the symbols from the linear combination so that only one symbol, the symbol of their interest, is remaining.  The transmitter can use the feedback of both users to replace $q_{i, j}$ or $q_{i, k}$ whenever it is received by user~$j$ or $k$ respectively, thus making this strategy simultaneously optimal for both user~$j$ and user~$k$ as mentioned in~\cite{TMKS_TIT20}.

Now, consider what user~$i$ can do in the midst of being ignored.  Specifically, consider if the following transmissions and erasure patterns were to occur as in Table~\ref{tab:hypothetical_chaining}.  In the first row of the table, we assume that at time $t = 1$, the linear combination $q_{i, j} \oplus q_{i, k}$ is sent.  We also assume that at this time, user~$i$ and user~$j$ received this transmission while user~$k$ did not, which is indicated from the fact that the channel noises take on values $(Z_{i}(t), Z_{j}(t), Z_{k}(t)) = (0, 0, 1)$ in the first row (see Section~\ref{sec:system_model_three_users}).  Since user~$j$ received the transmission, we replace $q_{i, j}$ with $\hat{q}_{i, j} \in \Q{i, j}$ and send the new linear combination $\hat{q}_{i, j} \oplus q_{i, k}$ at time $t = 2$.  At this time, only user~$i$, the user being ignored, receives the transmission.  Thus, since we are ignoring user~$i$, there is no need to replace any of the symbols, and instead we should retransmit the linear combination at time $t = 3$.  Notice however, if instead of retransmitting $\hat{q}_{i, j} \oplus q_{i, k}$ at time $t = 3$, we instead send $2\hat{q}_{i, j} \oplus q_{i, k}$, which is a \emph{different} linear combination of the same two symbols sent at $t = 2$.  This comes at no cost to user~$j$ nor user~$k$, since they would be able to decode regardless of which linear combination is sent. Let us further assume that at time $t = 3$, we again have that only user~$i$ has received the transmission.  Then since user~$i$ has received all transmissions so far, he will have received three independent equations in three unknown variables.  Thus he is able to decode all symbols sent so far despite the fact that the transmitter is targeting point-to-point optimal performance for the \emph{other} two users.  

\begin{table}
	\begin{center}
		\begin{tabular}{l c c c c}
			$t$              & $X(t)$ & $Z_{i}(t)$& $Z_{j}(t)$ & $Z_{k}(t))$  \\
			\hline
			1 & $q_{i, j} \oplus q_{i, k}$ & 0 & 0 & 1 \\
			2 & $\hat{q}_{i, j} \oplus q_{i, k}$ & 0 & 1 & 1 \\
			3 & $2\hat{q}_{i, j} \oplus q_{i, k}$ & 0 & 1 & 1 \\
		\end{tabular}
	\end{center}
	\caption{A hypothetical sequence of transmissions and channel noises when targeting point-to-point optimal performance for user~$j$ and user~$k$.}	
	\label{tab:hypothetical_chaining}
\end{table}

%Each transmission sent is a linear combination of two symbols, which may or may not be.  

%We see that in the chaining algorithm, each transmission sent is a linear combination of two symbols 
We see that the chaining algorithm ensures that each linear combination received by user~$i$ has at least one symbol in common with at least one other linear combination that was previously received.  For example, in Table~\ref{tab:hypothetical_chaining}, the first two transmissions have $q_{i, k}$ in common and the last two transmissions have two symbols in common.  By maintaining this property, we ensure that at any time, user~$i$ has to decode no more than one symbol in order to be able to back-substitute and solve for the remaining symbols received so far.  

We visualize this with the analogy of a chain of links.  Each linear combination received is a link in a chain, and links are coupled by the fact that they have at least one symbol in common.  If one source symbol in the chain is revealed to user~$i$, it can be used to solve for all symbols in one of the links.  In turn, that link has a symbol in common with a second link, and so all symbols in the second link can also be solved for.  This process continues until the entire chain of symbols can be decoded.  

%received by user~$i$ is interpreted to be a single link in a chain of links.  

%We see from the previous example that user~$i$ was able to decode by records ``chains'' of symbols sent.  

%Each transmission received by user~$i$ forms a link in a chain of symbols.  Each transmission received by user~$i$ is a linear combination of two symbols, and each linear combination has at least one symbol in common with at least one other transmission received.  For example, in Table~\ref{tab:hypothetical_chaining}, there are three links or transmissions received.  The first two transmissions have the symbol $q_{i, k}$ in common as do the last two transmissions.  

We model the chaining algorithm as a Markov rewards process with absorbing states.  The process starts in an initial state, after which, transitions between states occur at every timeslot.  The determination of which transition was actually taken depends only on the channel noise.  The process runs until one of two absorbing states is reached, after which, we are either able to decode an entire chain of symbols, or we have finished creating a chain of symbols that can be decoded with the reception of one more linear combination of symbols.  In the example of Table~\ref{tab:hypothetical_chaining}, we would reach the absorbing state in which we are not able to decode the chain of symbols if at $t = 3$, we instead had that only users~$j$ and~$k$ received $X(3)$.  In this case, we would have to replace both symbols being sent at $t = 4$ in order to be point-to-point optimal for users~$j$ and~$k$.  Thus, user~$i$ would not have enough equations to decode, and we would instead reset the Markov process such that user~$i$ would start building another chain when new symbols are sent.

\begin{table}
	\begin{center}
		\begin{tabular}{l | l | p{4.5cm} c }
			$State$              & $X(t)$ -- Transmission at time $t$ & Precondition at $t$  \\
			\hline
			1 & $q_{i, j} \oplus q_{i, k}$ (Initial state) & $|\chainsetarg{t-1} \cap \xsett| = 0$\\
			&& State~1 occupied at time $t-1$\\ 
			2 & $\hat{q}_{i, j} \oplus q_{i, k}$ & $|\chainsetarg{t-1} \cap \xsett| = 1$ \\ 
			3 & $q_{i, j} \oplus \hat{q}_{i, k}$ & $|\chainsetarg{t-1} \cap \xsett| = 1$ \\ 
			4 & $2q_{i, j} \oplus q_{i, k} $  & $|\chainsetarg{t-1} \cap \xsett| = 2$ \\ 	
			5 & None (Non-decoding absorbing state)  & $|\chainsetarg{t-1} \cap \xsett| = 0$\\ 
			6 & None (Decoding absorbing state)  & State~4 occupied at time $t-1$\\ 
		\end{tabular}
	\end{center}
	\caption{A description of states in the Markov rewards process.  Assume that $q_{i, j} \oplus q_{i, k}$ was sent at time $t-1$.  If the state indicated in the first column is entered at time $t$, then the transmission indicated by the second column is sent at time $t$.  We also show the precondition necessary to enter the state in the third column.}	
	\label{tab:table_of_states}
\end{table}

In addition to the two absorbing states, the Markov rewards process involves four additional \emph{transient} states.  We enumerate and describe all states in Table~\ref{tab:table_of_states}.  In this table, we assume that $q_{i, j} \oplus q_{i, k}$ was sent at time $t-1$.  Then if state $l \in \Omega \triangleq \{1, 2, \ldots, 6\}$ is entered at time $t$, we show $l$ in the first column and $X(t)$, the transmission at time $t$, in the second column.  We also show a necessary precondition for entering state $l$ at time $t$ in the third column.

Before we explain the preconditions, let us first define some notation.  Let $\chainsett$ represent the set of symbols appearing in the linear combinations received by user~$i$ after having listened to all transmissions from the beginning of the chaining algorithm up to and including time $t$.  In the example of Table~\ref{tab:hypothetical_chaining}, after time $t = 1, 2, 3$, we have $\chainsetarg{1} = \{q_{i, j}, q_{i, k}\}$, $\chainsetarg{2} = \{q_{i, j}, q_{i, k}, \hat{q}_{i, j}\}$ and $\chainsetarg{3} = \{q_{i, j}, q_{i, k}, \hat{q}_{i, j}\}$.  Let $\xsett$ represent the set of symbols appearing in the linear combination in $X(t)$.  In the example of Table~\ref{tab:hypothetical_chaining}, we have that $\xsetarg{1} = \{q_{i, j}, q_{i, k}\}$, $\xsetarg{2} = \{\hat{q}_{i, j}, q_{i, k}\}$, and $\xsetarg{3} = \{\hat{q}_{i, j}, q_{i, k}\}$. 
With this notation, we now describe the preconditions of Table~\ref{tab:table_of_states}.

%We now formalize the intuition gained with the previous example.   Let $\chainsett$ represent the set of symbols appearing in the linear combinations received by user~$i$ having listened to all transmissions from the beginning of the chaining algorithm up until time $t$.  For example, in the example of Table~\ref{tab:hypothetical_chaining}, after time $t = 1, 2, 3$, we have $\chainsetarg{1} = \{q_{i, j}, q_{i, k}\}$, $\chainsetarg{2} = \{q_{i, j}, q_{i, k}, \hat{q}_{i, j}\}$ and $\chainsetarg{3} = \{q_{i, j}, q_{i, k}, \hat{q}_{i, j}\}$ respectively.  We 


The first state is the initial state that sends the initial linear combination of symbols.  After the first outbound transition from the initial state, the initial state is not returned to at any time during the evolution of the Markov reward process unless an absorbing state is reached and we restart the chain-building process.  Since user~$i$ has not yet received any equations and we begin by transmitting a linear combination of two symbols, we have the precondition that $\chainsetarg{t-1} = \varnothing$, $\xsett = \{q_{i, j}, q_{i, k}\}$ and $|\chainsetarg{t-1} \cap \xsett| = 0$.

The second state is entered only if the symbol destined for user~$j$ needs to be replaced in the next linear combination to be sent and the chain-building process has begun.  In order to ensure that we can continue building the chain of symbols, we require that at least one symbol being sent at time $t$ appears in a linear combination already received by user~$i$, i.e.,  $|\chainsetarg{t-1} \cap \xsett| = 1$.  Similarly the third state is entered only if the symbol destined for user~$k$ needs to be replaced in the next linear combination to be sent and the chain-building process has already started.  The precondition for this state is analogous to its counterpart in State~2.  

The fourth state is entered if \emph{only} user~$i$ received the previous transmission, in which case we need to send a new linear combination of the \emph{same} two symbols in the previous timeslot.  In this case, we see that since the transmission is just a different linear combination of the previous two symbols sent, $|\chainsetarg{t-1} \cap \xsett| = 2$.

Finally, the two remaining states are the absorbing states.  State~5 is the absorbing state we occupy if the constraint of being point-to-point optimal for users~$j$ and~$k$ prevent user~$i$ from continuing the chain-building process.  That is, we are not able transmit the next linear combination such that one of the symbols appearing in the combination also appears in a previously received equation, i.e., $|\chainsetarg{t-1} \cap \xsett| = 0$.  Lastly, State~6 is the absorbing state we occupy if we reach the point when we are able to decode all symbols in the chain.  It is reached if the transmission sent in State~4 was received by user~$i$.

Having described the purpose of each state, we now describe the transitions between states in detail beginning with State~1.  Let $\zall = (Z_{i}(t), Z_{j}(t), Z_{k}(t))$.  For each state, we use a table to describe the outbound transitions based on values of $\zall$.  In addition, we show the reward accumulated for each transition.  For example, in the column with the heading $\rhosub{j}$ in Table~\ref{tab:state1_transitions_all}, we show the reward accumulated by user~$j$ at each possible outbound transition from State~1 based on the channel noise.  We note that although $\rhosub{j}$ depends on both $\zall$ and the inbound and outbound states, for notational convenience, we omit explicitly stating this dependence.  The reward can be accumulated by a user, or a queue.  The notation for rewards is summarized in Table~\ref{tab:reward_variables}.  In our description of each state, we also define the queue $\Qstar$, which is the queue containing prioritized symbols that only user~$i$ requires.  These symbols are prioritized because the reception of this symbol can decode an entire chain of symbols.  

%In the following descriptions, we assume that $q_{i, j} \oplus q_{i, k}$ was sent at time $t - 1$.
In the following descriptions, the reader can confirm that the chaining algorithm is point-to-point optimal for users~$j$ and~$k$.  Each time a symbol is received by one of the users, it is replaced by another instantly-decodable, distortion-innovative symbol.

%In doing so, we will make reference to the reward variables in Table~\ref{tab:reward_variables}.  

%We remark that although the reward accumulated will depend on $\zall$ and which states are being transitioned from/to, for convenience we 

\begin{table}
	\begin{center}
		\begin{tabular}{l |  p{8cm}}
			Variable              & Description  \\
			\hline
%			$\rhosub{u}(l, m)$ & Reward accumulated by user~$u$ after transition from state $l$ to $m$ for $u \in \{j , k\}$.  This reward represents the number of symbols that were decoded by user~$u$  \\
%			$\rhosub{Q}(l, m)$ &Reward accumulated in queue $Q$ after transition from state $l$ to $m$.  This reward represents the number of symbols placed in queue $Q$ after the transition \\ 
%			$\rhocarg{l}{m}$ &Reward accumulated by user~$i$ after transition from state $l$ to $m$.  This reward represents the number of equations received by user~$i$ \\ 
			$\rhosub{u}(l, m)$ & Reward representing the number of symbols that can be decoded by user~$u$ after a transition from state $l$ to $m$ for $u \in \{j , k\}$ \\
			$\rhosub{Q}(l, m)$ &Reward representing the number of symbols placed in queue $Q$ after a transition from state $l$ to $m$.  \\
			$\rhocarg{l}{m}$ &Reward representing the number of equations user~$i$ received after a transition from state $l$ to $m$. \\
		\end{tabular}
	\end{center}
	\caption{A legend for the reward variables.}	
	\label{tab:reward_variables}
\end{table}

\input{chaining_algorithm/state1_transitions_all}
\input{chaining_algorithm/state2_transitions_all}
\input{chaining_algorithm/state4_transitions_all}

Finally, we conclude our discussion of the chaining algorithm by mentioning that if at any time an absorbing state in the Markov rewards process is reached and the distortion constraints of users~$j$ and~$k$ have yet to be met, then we simply restart the Markov rewards process and begin building a new chain.  If at any time during this process, one of the users has their distortion constraint met, then that user can leave the network and we are left with two users who we again serve with the two-user algorithm of~\cite{TMKS_TIT20}.  If one of the remaining two users is user~$i$, the user building the chain, then we would prioritize the transmission of symbols in $\Qstar$, since the reception of one of these symbols would lead to the decoding of an entire chain of symbols. 