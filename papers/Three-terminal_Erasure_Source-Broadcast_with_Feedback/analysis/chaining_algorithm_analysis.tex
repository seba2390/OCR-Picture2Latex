\subsection{Analysis of Chaining Algorithm of Section~\ref{subsubsec:chaining_algorithm}}
\label{subsec:chaining_algorithm_analysis}

In this section, we give a sufficient condition for all users to simultaneously achieve point-to-point optimal performance when the chaining algorithm is invoked.  Our analysis is based on a restricted version of the chaining algorithm previously described.  Specifically, we assume that we do not opportunistically send linear combinations of the forms specified in~\Crefrange{eq:linear_comb_available_a}{eq:linear_comb_available_c} when the opportunities present themselves (see the description of State~4 in Section~\ref{subsubsec:chaining_algorithm}). Thus, we can argue a fortiori that the actual unrestricted chaining algorithm would achieve only better performance.  Before beginning the analysis, we mention that we will rely on many of the results on Markov rewards processes with impulse rewards and absorbing states derived in~\cite{TMK_markov}.  We will cite the specific theorems and corollaries we use from~\cite{TMK_markov} when applicable. %Section~\ref{sec:markov}.  We encourage the reader to become familiarized with this section before reading further.  

We begin by deriving the transition matrix for the Markov rewards process.  Consider Table~\ref{tab:state1_transitions_all}.  The table shows all outbound transitions given the channel noise realization, however to construct a transition matrix from this information, we must combine all outbound transitions to the same state.  For example, rows 1 and 5 both show transitions from State~1 to State~5, and so to get $\trans{1}{5}$, the total probability of transitioning from State~1 to~5, we must add the corresponding probabilities under the $\Prob(\zall)$ columns.  This is given by 

\begin{align}
	\trans{1}{5} &= \Prob(\zall = (0, 0, 0)) + \Prob(\zall = (1, 0, 0)) \\
	&= (1 -\epsilon_i)(1 -\epsilon_j)(1 - \epsilon_k) + \epsilon_i(1 -\epsilon_j)(1 - \epsilon_k) \\
	&= (1 -\epsilon_j)(1 - \epsilon_k).
\end{align}
We continue in this manner to find $\trans{i}{j}$ for all $i, j \in \myState \triangleq \{1, 2, \ldots, 6\}$ to populate the transition matrix $\transM$ where the $(i, j)$th entry of $\transM$ is given by $\trans{i}{j}$.

Our analysis also requires the derivation of several rewards matrices.  Consider deriving the rewards matrix $\orhoe$, whose $(i,j)$th element,  $\rholcarg{i}{j}$, gives the expected number of equations (rewards) received by user~$i$ for transitioning from State~$i$ to State~$j$, where $i, j \in \myState$.  Continuing with our previous example, say we would like to derive $\rholcarg{1}{5}$.  Of the two possible paths for an outbound transition from State~1 to~5, only one, when $\zall = (0, 0, 0)$, is associated with a reward in the column of $\rhoc$ in Table~\ref{tab:state1_transitions_all}.  We therefore calculate $\rholcarg{1}{5}$ by weighting the reward with the conditional probability of the transition resulting from the channel noise $\zall = (0, 0, 0)$ given that the transition from State~1 to~5 occurred.  Therefore,

\begin{align}
	\rholcarg{1}{5} &= \Prob(\zall = (0, 0, 0) | \textrm{transition from State~1 to~5}) \cdot 1 \\
	&= \frac{(1 -\epsilon_i)(1 -\epsilon_j)(1 - \epsilon_k)}{(1 -\epsilon_i)(1 -\epsilon_j)(1 - \epsilon_k) + \epsilon_i(1 -\epsilon_j)(1 - \epsilon_k)} \\
	&= (1 -\epsilon_i).
\end{align}
By performing this calculation for all $i, j \in \myState$, we are able to populate the entire rewards matrix $\orhoe$.  Similarly, we can calculate the $|\myState| \times |\myState|$ rewards matrices, $\orhoj$, $\orhok$, $\orhoqi$, $\orhoqj$, $\orhoqk$, and $\orhoqstar$ corresponding to each rewards column in Table~\ref{tab:state1_transitions_all}.

Given the transition matrix, for each reward matrix, we can use the results in \cite[Corollary~\markovcor{}]{TMK_markov} to calculate the expected accumulated reward before absorption \emph{each time} the Markov rewards process is reset and allowed to run until absorption.  To find the \emph{total} expected reward, we must first find a lower bound for $\Mstar$, the number of times the Markov process is \emph{reset} before user~$j$ or~$k$ has met their distortion constraint.  That is, $\Mstar$ is the number of times the Markov rewards process has reached an absorbing state after having been restarted in the initial state.  Therefore, $\Mstar$ also represents the number of chains built by user~$i$ before user~$j$ or~$k$ has satisfied their distortion constraint.

Say that for $r \in \{i, j, k\}$, user~$r$ requires $N(1 - \hat{d}_r)$ symbols at the beginning of the chaining algorithm, where $\hat{d}_r \in (0, \epsilon_r)$.  Furthermore, suppose that of the two users for whom we are targeting point-to-point optimal performance, user~$u$ is \emph{not} the bottleneck user, i.e., 

\begin{align}
\label{eq:u_bottleneck}
	u = \argmin_{r \in \{j, k\}} w_r(\hat{d}_r),
\end{align}
where $w_r(\hat{d}_r)$ is given by~\eqref{eq:wioptimal}.  For $l = 1, 2, \ldots $, let $\oRhojl$ be the expected accumulated reward for user~$u$ during the $l$th time the Markov rewards process has been reset.  Then we can define $\Mstar$ as

\begin{align}
	\Mstar = \min\left\{ m : \sum_{l = 1}^{m} \oRhojl \geq N(1 - \hat{d}_{u}) \right\},
\end{align}
where the $\oRhojl$ are i.i.d.\ and $\mathbb{E}\oRhojl$ can be calculated as in \cite[Corollary~\markovcor{}]{TMK_markov}.

We see that $\Mstar$ is a stopping rule~\cite{Gallager96}.  In order to calculate $\mathbb{E}\Mstar$, we could use the discrete version of the renewal equation~\cite[Chapter~2]{MitovOmey14}.  However, to find a lower bound for $\mathbb{E}\Mstar$, we may simply use Wald's equation\cite{Gallager96}.  Let $\sigma_m = \sum_{l = 1}^{m} \oRhojl$.  Then, by Wald's equation, 

\begin{align}
	\mathbb{E}\Mstar &= \frac{\mathbb{E}\sigma_{\Mstar}}{\mathbb{E}\oRhojl} \\
	\label{eq:EM_lb}
	&\geq \frac{N(1 - \hat{d}_u)}{\mathbb{E}\oRhojl},
\end{align}
where again, $\mathbb{E}\oRhojl$ is calculated as in \cite[Corollary~\markovcor{}]{TMK_markov}.  
Now, let 

\begin{align}
\label{eq:Mstar}
	M^{-} = \left\lfloor{\frac{N(1 - \hat{d}_u)}{\mathbb{E}\oRhojl}}\right\rfloor
\end{align}
be the result of applying the floor function to the right-hand-side of~\eqref{eq:EM_lb}.  We have that $M^{-}$ gives a lower bound for the expected number of times the Markov chain is reset.  If user~$i$, the user building the chains, can meet their distortion constraint within the $M^{-}$ times the Markov rewards process is being run, then all users will be point-to-point optimal.  This is because user~$i$ is able to decode all his required symbols despite the fact that we are targeting optimal performance for the other two users.

Let $\oRhoel$ be the expected number of symbols in the chain that can be decoded in the $l$th run of the Markov rewards process given that the decoding absorbing state, State~6, was reached after having started in the initial state, State~1.  
%From the discussion leading to the derivation of Corollary~\ref{cor:barRinfi}, 
We mention that we can easily calculate $\oRhoel$ as in~\cite[Theorem~\markovthm{}]{TMK_markov}.  %we can similarly argue that 
%we have that $\mathbb{E}\oRhoel$ can be derived by substituting the transition matrix and rewards matrix $\orhoe$ into Theorem~\ref{thm:hatR_inf} and simply taking the $(1, 6)$th element from the resultant matrix.  This is because the rewards matrix $\orhoe$ is used to calculate the expected number of symbols in the chain built by user~$i$, however, user $i$ is able to decode these symbols only if State~6, the decoding absorbing state, is reached.
%
%By Theorem~\ref{thm:barRinf_absorb} of Section~\ref{subsec:unscaled_rewards}, we can calculate $\mathbb{E}\oRhoel$ from the transition matrix, and the rewards matrix $\orhoe$.  
%
Now, let $\overline{\sigma}$ be the expected number of symbols that can be decoded in $M^{-}$ iterations of the Markov rewards process.  By the linearity of the expectation operator we have

\begin{align}
	\overline{\sigma} &= \sum_{l = 1}^{M^{-}} \mathbb{E}\oRhoel \\
	\label{eq:normalized_chain_received}
	&= M^{-} \times \mathbb{E}\oRhoel.
\end{align}
If the right-hand-side of~\eqref{eq:normalized_chain_received} is greater than $N(1 - \hat{d}_i)$, the fraction of symbols user~$i$ requires, then we are point-to-point optimal for all users.  Combining~\eqref{eq:normalized_chain_received} and~\eqref{eq:Mstar}, we see that this happens when 

\begin{align}
\label{eq:chain_optimality}
%	\frac{1}{N}\left\lfloor{\frac{N(1 - \hat{d}_u)}{\mathbb{E}\oRhojl}}\right\rfloor \mathbb{E}\oRhoel \geq 1 - \hat{d}_i.
	\left\lfloor{\frac{N(1 - \hat{d}_u)}{\mathbb{E}\oRhojl}}\right\rfloor \geq \frac{N(1 - \hat{d}_i)}{\mathbb{E}\oRhoel}.
\end{align}

\begin{theorem}
\label{thm:chaining_sufficient}
	Let $u$ be the user satisfying~\eqref{eq:u_bottleneck}.  Then $w^{+}\dvec$ is $\dvec$-achievable if~\eqref{eq:chain_optimality} is satisfied, where $\mathbb{E}\oRhojl$ is calculated from the transition matrix and rewards matrix $\overline{\rho}_{u}$ via \cite[Corollary~\markovcor{}]{TMK_markov}, and $\mathbb{E}\oRhoel$ is calculated from the transition matrix and rewards matrix $\orhoe$ via \cite[Theorem~\markovthm{}]{TMK_markov}.
\end{theorem}

Finally, we again remark that the analysis we have just described is only a sufficient condition for a restricted version of the chaining algorithm in which we do not opportunistically send linear combinations of the form in~\Crefrange{eq:linear_comb_available_a}{eq:linear_comb_available_c}.  Therefore, we expect the unrestricted algorithm to perform better.

\subsection{Operational Significance of Theorem~\ref{thm:chaining_sufficient}}
\label{sec:operational_meaning}

Consider a hypothetical situation in which we have queues $\Q{1, 2}, \Q{1, 3}$ and $\Q{2, 3}$.   We consider quadratic distortions in which for $i \in \{2, 3\}$,  $d_i = \epsilon_i^2$, and we fix $\epsilon_1 = 0.1$, $\epsilon_3 = 0.6$ and vary $\epsilon_2 \in (0.2, 0.6)$.  

We illustrate the chaining algorithm when user~1 is the user who builds chains and we send linear combinations of the symbols in $\Q{1, 2}$ and $\Q{1, 3}$ as if users~2 and~3 were the only users in the network.  In this case, for $i \in \{2, 3\}$, user~$i$'s point-to-point optimal latency is given by $\wi = (1 - \epsilon_i^2)/(1 - \epsilon_i)$, and since $\wi$ is an increasing function of $\epsilon_i \in [0, 1)$, we have that between users~2 and~3, user~2 is \emph{not} the bottleneck user.  That is, in~\eqref{eq:chain_optimality}, user~2 takes the place of user~$u$, and since user~1 is building the chains, user~1 takes the place of user~$i$.

We rearrange~\eqref{eq:chain_optimality} of Theorem~\ref{thm:chaining_sufficient} to find a lower bound for $\hat{d}_i$, and since $\hat{d}_u = \epsilon_u^2$, we  plot this lower bound as a function of $\epsilon_u$ in Figure~\ref{fig:operational_meaning}.  From this figure, we can read which values of $\hat{d}_i$ would yield optimal performance for a given value of $\epsilon_u$ by considering all values of $\hat{d}_u$ above the lower bound.  Recall that user~$i$ is able to decode symbols in the chain only if there have been two consecutive transmissions for which user~$i$ is the only user to have received the transmission.  We see that the probability of this event increases as $\epsilon_u$ increases in Figure~\ref{fig:operational_meaning}.  Therefore, user~$i$ is able to achieve lower distortions as $\epsilon_u$ increases.  

Finally, we again mention that Theorem~\ref{thm:chaining_sufficient} merely gives a conservative \emph{sufficient} condition for optimality and ignores other network coding opportunities in its analysis.  Therefore, it is possible that point-to-point optimal performance can still be met if user~$i$ has a distortion constraint below the lower bound of Figure~\ref{fig:operational_meaning}.

\begin{figure}
	\centering
	\setlength\figurewidth{2.65in} 
	\setlength\figureheight{2.07in} 
%	\includegraphics[scale=0.5]{fig/ttotal_N_10e7_eps1_3_eps2_4}
	\input{fig/simulations/operational.tikz}
	\caption{The lower bound from Theorem~\ref{thm:chaining_sufficient} that delineates a conservative boundary of distortion values for which the minmax optimal latency can be achieved.  }
	\label{fig:operational_meaning}
\end{figure}
