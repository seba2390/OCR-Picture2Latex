\subsection{Problem Formulation}

A \emph{discrete-time Markov chain} is a sequence of random variables $\Si{0}, \Si{1}, \ldots $, where for every $i = 0, 1, \ldots$, state $\Si{i}$ takes values from the state space $\myState$, i.e., $\Si{i} \in \myState$, and the probability of transitioning from state $i$ at time $n-1$ to state $j$ at time $n$ given previous states $\Si{0}, \Si{1}, \ldots, \Si{n-1}$ satisfies the Markov property 

\begin{equation}
	\Prob(\Si{n} = j | \Si{n-1} = i, \Si{n- 2} = i_{n-2}, \ldots, \Si{1} = i_{1}) = \Prob(\Si{n} = j | \Si{n-1} = i).
\end{equation}
%
Without loss of generality, we assume the state space, $\myState$, is indexed by a set of integers so that $\myState = \{1, 2, \ldots, |\myState|\}$.

We study \emph{time-homogenous} Markov chains where the transition probabilities do not depend on $n$.  Specifically, at any time $n = 0, 1, \ldots$, the probability of transitioning from state $i$ to state $j$ does not depend on $n$, i.e., $\Prob(\Si{n} = j | \Si{n-1} = i) = \trans{i}{j}$.  At any time $n$, the transitions between states can therefore be described by a $|\myState| \times |\myState|$ \emph{transition matrix} $\transM$ whose $(i, j)$th entry is given by $\trans{i}{j}$ for $i, j \in \myState$.

%We consider discrete-time Markov chains characterized by a state space $\myState$ and a time-homogenous transition matrix $\transM$.  
In addition to being time-homogenous, the Markov chains we study are also \emph{absorbing}.

\begin{mydef}
	A state $i \in \myState$ is said to be \emph{absorbing} if for all $j \in \myState \setminus \{i\}$, $\trans{i}{j} = 0$  and $\trans{i}{i} = 1$.
\end{mydef}

\begin{mydef}
	A state is said to be \emph{transient} if it is not an absorbing state.
\end{mydef}

\begin{mydef}
	A discrete-time Markov chain is said to be an \emph{absorbing} Markov chain if it has at least one absorbing state, and it is possible to reach an absorbing state from any transient state within a finite number of transitions.
\end{mydef}

A \emph{Markov reward process} is a Markov chain that incorporates rewards that are accumulated during the evolution of the Markov chain.  The reward process we consider is additionally characterized by an \emph{impulse-reward matrix}.

\begin{mydef}
	The impulse-reward matrix, $\Rew$, is a $\sizeS \times \sizeS$ matrix whose $(i, j)th$ entry, $\rew{i}{j}$, represents the reward accumulated when transitioning from state $i$ to state $j$.
\end{mydef}
%
%During the evolution of the Markov chain we study, there are also rewards accumulated based on the state transitions.
%
%A discrete-time Markov impulse reward process with absorbing states is a Markov chain is characterized by an impulse-reward matrix $\Rew$ and a transition matrix $\transM$ having the form 
%
For any realization of a sequence of $n+1$ states from time step zero to time step $n$, i.e., given $\Si{0} = i_{0}$, $\Si{1} = i_{1}$, \ldots, $\Si{n} = i_{n}$, we define the accumulated reward $\Rn(\Si{0} = i_{0}, \Si{1} = i_{1}, \ldots, \Si{n} = i_{n})$ as 
%
\begin{equation}
	\label{eq:Rn}
	\Rn(\Si{0} = i_{0}, \Si{1} = i_{1}, \ldots, \Si{n} = i_{n}) = \sum_{k = 1}^{n} \rew{i_{k-1}}{i_{k}}.
\end{equation}

%
%For brevity we will write $\Rn(\Si{0} = i_{0}, \Si{1} = i_{1}, \ldots, \Si{n} = i_{n})$ as $\Rn$ when the context is clear.  

%\begin{mydef}
%\label{def:Rn}
%	Let $\Rn$ be a random variable representing the accumulated reward at time step $n$, where the randomness is taken over the random state sequence $\Si{0}, \Si{1}, \ldots, \Si{n}$.   
%\end{mydef}

\begin{mydef}
\label{def:Rn}
	Let $\Rn$ be a random variable representing the accumulated reward at time step $n$ when the state sequence $\Si{0}, \Si{1}, \ldots, \Si{n}$ is taken to be random.   
\end{mydef}

\begin{mydef}
	\label{def:barRij}
	Let $\barRij{i}{j}$ be the expected value of $\Rn$ given initial state $\Si{0} = i$ and final state $\Si{n} = j$, i.e., $\barRij{i}{j} = \mathbb{E}(\Rn | \Si{0} = i, \Si{n} = j)$.  
\end{mydef}

From Definition~\ref{def:barRij}, the expression for $\barRij{i}{j}$ should be clear when $n = 1$.

\begin{mycor}
	\label{cor:barR1}
	$\barRsubij{1}{i}{j} = \rew{i}{j}$
\end{mycor}

\begin{mydef}
	Let $\barR$ be the $|\myState| \times |\myState|$ matrix whose $(i, j)$th entry is given by $\barRij{i}{j}$.
\end{mydef}

\begin{mydef}
	\label{def:hatRij}
	Let $\hatRij{i}{j}$ be a scaled version of $\barRij{i}{j}$ where $\hatRij{i}{j} = \barRij{i}{j} \times \Prob(\Si{n} = j | \Si{0} = i)$.
\end{mydef}

\begin{mydef}
	Let $\hatR$ be the $|\myState| \times |\myState|$ matrix whose $(i, j)$th entry is given by $\hatRij{i}{j}$.
\end{mydef}


We are often also interested in the expected accumulated reward at time $n$ given only initial state $\Si{0} = i$ irrespective of the state at time $n$.

\begin{mydef}
\label{def:barRi}
	Let $\barRi{i}$ be the expected accumulated reward at time $n$, given initial state $\Si{0} = i$.
\end{mydef}

As we will see in the next section, the probability that the Markov reward process eventually reaches an absorbing state is unity, and so the long-term accumulated reward, also known as the reward until absorption, is of interest.

\begin{mydef}
	Let $\hatRinf(i, j) = \lim_{n \to \infty} \hatRij{i}{j}$ be the long-term value of $\hatRij{i}{j}$.
\end{mydef}

\begin{mydef}
	Let $\hatRinf$ be the $|\myState| \times |\myState|$ matrix whose $(i, j)$th entry is given by $\hatRinf(i, j)$.
\end{mydef}

\begin{mydef}
\label{def:barRinfi}
	Let $\barRinf(i) = \lim_{n \to \infty} \barRi{i}$ be the long-term value of $\barRi{i}$.
\end{mydef}

%\begin{mydef}
%	Let $\barRinf$ be the $\mathbb{R}^{|\myState|}$ vector whose $i$th entry is given by $\barRinf(i)$.
%\end{mydef}

In practice, $\barRinf(i)$ is often the most relevant quantity of interest, since we start the Markov reward process in an initial state and wish to know the accumulated reward before absorption.  We therefore define our problem as that of finding an expression for $\barRinf(i)$.  

We do this by first spending the majority of our efforts in Section~\ref{subsec:scaled_rewards} to derive an expression for the scaled accumulated reward variables, which culminates in the derivation of the transient scaled accumulated reward, $\hatR$, in~Lemma~\ref{lem:hatRn_matrix} and the long-term scaled accumulated reward, $\hatRinf$, in Theorem~\ref{thm:hatR_inf}.  In Section~\ref{subsec:unscaled_rewards}, we then show how to use these expressions to derive expressions for the \emph{unscaled} accumulated reward variables.  These variables are conditioned on initial state $\Si{0} = i$, and consist of the transient accumulated reward, $\barRi{i}$, in Theorem~\ref{thm:barRi}, and the accumulated reward before absorption, $\barRinf(i)$, in Corollary~\ref{cor:barRinfi}.  Finally, if a prior distribution over the initial states is known, we show how to calculate the unconditional transient accumulated reward, $\barR$, in Theorem~\ref{thm:barR_prior} and the unconditional accumulated reward before absorption, $\barRinf$, in Corollary~\ref{cor:barRinf}.



%We define our problem as that of finding a single letter expression for $\hatRinf$.  In the process, we will also derive expressions for the other quantities of interest.


