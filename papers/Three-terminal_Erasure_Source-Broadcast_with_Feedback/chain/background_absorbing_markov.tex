\subsection{Background for Absorbing Markov Chains}

A discrete-time absorbing Markov chain has a state space $\myState$ that can be partitioned into a set of absorbing states, $\StateAbs$, and a set of transient states, $\StateTrans$, such that $\myState = \StateAbs \cup \StateTrans$.  Recall our assumption that the state space is indexed by the integers $\{1, 2, \ldots, |\myState|\}$.  We further assume that transient states have \emph{lower} index values than absorbing states.  If this is the case, we can write out the transition matrix in its canonical form 
%For any absorbing state $i \in \StateAbs$
%transition matrix $\transM$ has a specific form.

\begin{equation}
	\label{eq:transM}
	\transM = 
	\begin{bmatrix}
		\transQ & \transR \\ 
		0 & I
	\end{bmatrix},
\end{equation}
%
where
\begin{enumerate}
%	\item $\transQ$ is a $|\StateTrans| \times |\StateTrans|$ matrix such that for $i, j \in \{1, 2, \ldots, |\StateTrans|\}$, the $(i, j)$th element of $\transQ$ gives the probability of transitioning from transient state $i$ to transient state $j$ %$i, j \in \StateTrans$, the $(i, j)$th element of $\transQ$ gives the probability of transitioning from transient state $i$ to transient state $j$
%	\item $\transR$ is a $|\StateTrans| \times |\StateAbs|$ matrix such that for $(i, j) \in \StateTrans \times \StateAbs$, the $(i, j - |\StateAbs|)$th element gives the probability of transitioning from  transient state $i$ to an absorbing state $j$
	\item $\transQ$ is a $|\StateTrans| \times |\StateTrans|$ matrix whose elements represent the probability of transitioning from one transient state to another transient state
	\item $\transR$ is a $|\StateTrans| \times |\StateAbs|$ matrix whose elements represent the probability of transitioning from a transient state to an absorbing state
	\item The matrix $0$ is the $|\StateAbs| \times |\StateTrans|$ zero matrix whose elements represent the impossibility of transitioning from an absorbing state to a transient state
	\item $I$ is the $|\StateAbs| \times |\StateAbs|$ identity matrix whose elements represent the probability of transitioning from one absorbing state to another absorbing state.
\end{enumerate}

\begin{remark}
	\label{rem:Rew}
	Given the assumption that transient states have \emph{lower} index values than absorbing states, we may also assume without loss of generality that the impulse-reward matrix, $\Rew$, has the form 
	\begin{equation}
	\label{eq:Rew_sub}
		\Rew = 
			\begin{bmatrix}
				\RewSubTransTrans & \RewSubTransAbs \\ 
				0 & 0
			\end{bmatrix},
	\end{equation}
	where 
	\begin{enumerate}
		\item $\RewSubTransTrans$ is a $|\StateTrans| \times |\StateTrans|$ matrix whose elements represent the reward accumulated for transitioning from one transient state to another transient state
		\item $\RewSubTransAbs$ is a $|\StateTrans| \times |\StateAbs|$ matrix whose elements represent the reward accumulated for transitioning from a transient state to an absorbing state
		\item the zero matrices have the appropriate dimensions for $\Rew$ to be a $\sizeS \times \sizeS$ matrix.
	\end{enumerate}
\end{remark}

\begin{mydef}
	\label{def:HtransRew}
	Let $\HtransRew$ be the Hadamard (element-wise) product of the reward matrix $\Rew$ and transition matrix $\transM$, i.e., $\HtransRew = \Rew \odot \transM$ so that $\HtransRewij{i}{j} = \rew{i}{j}\trans{i}{j}$
\end{mydef}

\begin{remark}
	\label{rem:HtransRew}
	Similar to Remark~\ref{rem:Rew}, we may assume without loss of generality that~$\HtransRew$ has the form 
	\begin{equation}
	\label{eq:HtransRew_sub}
		\HtransRew = 
			\begin{bmatrix}
				\HSubTransTrans & \HSubTransAbs \\ 
				0 & 0
			\end{bmatrix}.
	\end{equation}
\end{remark}

From Definition~\ref{def:hatRij}, and Corollary~\ref{cor:barR1}, the expression for $\hatRij{i}{j}$ should be clear when $n = 1$.

\begin{mycor}
	\label{cor:hatR1}
	$\hatRsubij{1}{i}{j} = \HtransRewij{i}{j}$
\end{mycor}

At time step $n$, the probability of being in state $j$ given initial state $i$ is given by the $(i, j)$th entry of $\transM^n$, which is given by the following lemma.

\begin{myLemma}
	\label{lem:transM_n}
	For $n = 1, 2, \ldots $, the transition matrix taken to the $n$th power is given by 
	\begin{equation}
	\label{eq:transM_n}
		\transM^n = 
			\begin{bmatrix}
				\transQ^n & \sum_{i = 0}^{n-1} \transQ^{i}\transR \\ 
				0 & I
			\end{bmatrix}.
	\end{equation}
\end{myLemma}

\begin{proof}
	We proceed by induction.
%	\begin{description}
	\begin{LaTeXdescription}
		\item[Base case] It is readily verified by substituting $n = 1$ into \eqref{eq:transM_n} that we may recover the canonical form of the transition matrix in~\eqref{eq:transM}.
		\item[Inductive Hypothesis] We assume that for $n - 1 \geq 1$, 
			\begin{equation}
				\transM^{n-1} = 
					\begin{bmatrix*}[c]
						\transQ^{n-1} & \sum_{i = 0}^{n-2} \transQ^{i} \transR \\ 
						0 & I
					\end{bmatrix*}.
			\end{equation}
		\item[Induction Step] We have that
			\setcounter{cnt}{1}
			\begin{align}	
				\transM^n &= \transM^{n-1} \transM \\
				\label{eq:transM_n_mult}
				&\stackrel{(\alph{cnt})}{=} \begin{bmatrix*}[c]
						\transQ^{n-1} & \sum_{i = 0}^{n-2} \transQ^{i} \transR \\ 
						0 & I
					\end{bmatrix*}
					\begin{bmatrix}
						\transQ & \transR \\ 
						0 & I
					\end{bmatrix}	,
			\end{align}
			where 
			\begin{enumerate}[(a)]
				\item follows from~\eqref{eq:transM} and the inductive hypothesis.
			\end{enumerate}
			After performing the block matrix multiplication in~\eqref{eq:transM_n_mult}, it is straightforward to see that the right-hand sides of~\eqref{eq:transM_n} and~\eqref{eq:transM_n_mult} are equal.
%	\end{description}
	\end{LaTeXdescription}
\end{proof}

\begin{myLemma}[{\hspace{1sp}\cite[Theorem 11.3]{GrinsteadSnell}}]
	\label{lem:transQn_zero}
	In an absorbing Markov chain, the probability that the process will be absorbed is 1~(i.e., $\transQ^n \to 0$ as $n \to \infty$).
\end{myLemma}

\begin{myLemma}[{\hspace{1sp}\cite[Theorem 11.4]{GrinsteadSnell}}]
	\label{lem:fund_mat}
	For an absorbing Markov chain, the matrix $I - \transQ$ has an inverse, $\fundMat$, termed the \emph{fundamental matrix}, and $\fundMat  = I+\transQ+\transQ^2 + \ldots$.
\end{myLemma}

\begin{myLemma}
	\label{lem:transM_infty}
	The steady state probability $\transM^{\infty} = \lim_{n \to \infty} \transM^{n}$ is given by 
	\begin{equation}
	\label{eq:transM_infty}
		\transM^{\infty} = 
			\begin{bmatrix}
				0 & \fundMat \transR \\ 
				0 & I
			\end{bmatrix}.
	\end{equation}
\end{myLemma}
\begin{proof}
	This follows from substituting the results of Lemmas~\ref{lem:transQn_zero} and~\ref{lem:fund_mat} into the expression for $\transM^n$ in~\ref{eq:transM_n}.
\end{proof}

\begin{mydef}[{\hspace{1sp}\cite[Definition 5.6.8]{HornJohnson}}]
	The \emph{spectral radius} $\rho(A)$ of a matrix $A \in \mathbb{R}^{n \times n}$ is 
	\begin{equation}
		\rho(A) \triangleq \max \{|\lambda| : \lambda \textrm{ is an eigenvalue of } A\}.
	\end{equation}
\end{mydef}

The spectral radius is itself not a matrix norm, however the following corollary states that there exists a norm that is arbitrarily close to the spectral radius.

\begin{myLemma}[{\hspace{1sp}\cite[Lemma 5.6.10]{HornJohnson}}]
	\label{lem:norm_bounds}
	Let $A \in \mathbb{R}^{n \times n}$ and $\epsilon > 0$ be given.  There is at least one matrix norm $\norm{\cdot}$ such that $\rho(A) \leq \norm{A} \leq \rho(A) + \epsilon$.
\end{myLemma}

\begin{myLemma}[{\hspace{1sp}\cite[Lemma 5.6.12]{HornJohnson}}]
	\label{lem:spectral}
	Let $A \in \mathbb{R}^{n \times n}$.  Then $\lim_{n \to \infty} A^n = 0$ if and only if $\rho(A) < 1$.
\end{myLemma}

By Lemmas~\ref{lem:transQn_zero} and~\ref{lem:spectral}, we have that $\rho(\transQ) < 1$.  By appropriately defining $\epsilon$ in Lemma~\ref{lem:norm_bounds}, it then follows that there is a matrix norm for which $\norm{\transQ} < 1$.

\begin{mycor}
	\label{cor:transQ_norm}
	For an absorbing markov chain, there exists a matrix norm for which $\norm{\transQ} < 1$.
\end{mycor}