%\appendix
%\section{Miscellaneous Properties of \hatR}
%
%\begin{myLemma}
%	For $n = 1, 2, \ldots $
%	\begin{equation}
%		\label{eq:hatR_sum}
%		\hatR = \sum_{i = 1}^{n} \transMsup{i - 1} \HtransRew \transMsup{n - i}
%	\end{equation}
%\end{myLemma}
%
%\begin{proof}
%	We proceed by induction.
%	\begin{description}
%		\item[Base case] We substitute $n = 1$ into \eqref{eq:hatR_sum} to get that $\hatRsub{1} = \transM^0 \HtransRew \transM^0 = \HtransRew$, which, according to Corollary~\ref{cor:hatR1}, is correct.
%		
%		\item[Inductive Hypothesis] We assume that for $n - 1 \geq 1$, 
%			\begin{equation}
%				\hatRsub{n-1} = \sum_{i = 1}^{n-1} \transMsup{i - 1} \HtransRew \transMsup{n - i - 1}.
%			\end{equation}
%		
%		\item[Induction Step] We begin with the recurrence relation in Lemma~\ref{lem:recurrence} from which we get 
%			\setcounter{cnt}{1}
%			\begin{align}
%				\hatR &= \hatRsub{n-1} \transM + \transMsup{n-1}\HtransRew \\
%				&\stackrel{(\alph{cnt})}{=} \left\{\sum_{i = 1}^{n-1} \transMsup{i - 1} \HtransRew \transMsup{n - i - 1}\right\} \transM + \transMsup{n-1}\HtransRew \\
%					\addtocounter{cnt}{1}
%				&= \sum_{i = 1}^{n-1} \transMsup{i - 1} \HtransRew \transMsup{n - i} + \transMsup{n-1}\HtransRew \\
%				&= \sum_{i = 1}^{n} \transMsup{i - 1} \HtransRew \transMsup{n - i}
%			\end{align}
%			where 
%			\begin{enumerate}[(a)]
%				\item follows from the inductive hypothesis
%			\end{enumerate}
%	\end{description}
%\end{proof}

\section{Matrix Properties}
\label{sec:matrix_properties}

\begin{myLemma}
	\label{lem:transMsup}
	Let $A$ be an $m \times m$ matrix, and let $A^n$ denote the $n$th power of $A$  for some $n \in \{2, 3, \ldots \}$.  Then the $(i, j)$th entry of $A^n$, denoted by $A^n(i, j)$, is given by 
	\begin{equation}
	\label{eq:transMsup}
		A^n(i, j) = \sum_{k_1 = 1}^m \sum_{k_2 = 1}^m \ldots \sum_{k_{n-1} = 1}^m A(i, k_1)A(k_1, k_2) \ldots A(k_{n-1}, j)
	\end{equation}
\end{myLemma}

\begin{proof}
	We proceed by induction.
	\begin{LaTeXdescription}
		\item[Base case] We substitute $n = 2$ into \eqref{eq:transMsup} to get that 
			\begin{equation}
				A^2(i, j) = \sum_{k_1 = 1}^m A(i, k_1)A(k_1, j),
			\end{equation}
			which is the familiar definition for matrix multiplication.
		\item[Inductive Hypothesis] We assume that for $n - 1 \geq 2$, 
			\begin{equation}
				A^{n-1}(i, j) = \sum_{k_1 = 1}^m \sum_{k_2 = 1}^m \ldots \sum_{k_{n-2} = 1}^m A(i, k_1)A(k_1, k_2) \ldots A(k_{n-2}, j).
			\end{equation}
		\item[Induction Step] We have that
			\setcounter{cnt}{1}
			\begin{align}
				A^{n}(i, j) &\stackrel{(\alph{cnt})}{=} \sum_{k_{n-1} = 1}^m A^{n-1}(i, k_{n-1}) A(k_{n-1}, j) \\
				\addtocounter{cnt}{1}		
				\label{eq:transMsup_final}
				&\stackrel{(\alph{cnt})}{=} \sum_{k_{n-1} = 1}^m \left\{\sum_{k_1 = 1}^m \sum_{k_2 = 1}^m \ldots \sum_{k_{n-2} = 1}^m A(i, k_1)A(k_1, k_2) \ldots A(k_{n-2}, k_{n-1}) \right\} A(k_{n-1}, j)
			\end{align}
			where 
			\begin{enumerate}[(a)]
				\item follows from the definition of matrix multiplication and the fact that $A^n = A^{n-1} A$
				\item follows from the inductive hypothesis
			\end{enumerate}
			After rearranging the right-hand-side of~\eqref{eq:transMsup_final}, we conclude that~\eqref{eq:transMsup} indeed holds.
	\end{LaTeXdescription}
\end{proof}

% --------------------------------------------------------------
% Summation approaches zero Lemma
% --------------------------------------------------------------

\begin{myLemma}
	\label{lem:summation_matrix_zero}
	Let $g:  \mathbb{N} \times \mathbb{R}^{m \times m} \times \mathbb{R}^{m \times m} \mapsto \mathbb{R}^{m \times m}$ be the function given by 
	\begin{equation}
	g(n, A, Q) = \sum_{i = 0}^{n-2}\transQ^{i}A\transQ^{n - i - 1}.
%	\label{eq:hatRn_matrix}
%		\hatR = 
%		\begin{bmatrix}
%			\RewSubC & \RewSubD \\
%			0 & 0 
%		\end{bmatrix}
	\end{equation}
	where $\transQ$ is a matrix such that there exists a norm for which $\norm{Q} < 1$. Then 
	\begin{equation}
	\lim_{n \to \infty} g(n, A, Q) = 0_{m \times m}.
%	\label{eq:hatRn_matrix}
%		\hatR = 
%		\begin{bmatrix}
%			\RewSubC & \RewSubD \\
%			0 & 0 
%		\end{bmatrix}
	\end{equation}
%	where
\end{myLemma}
\begin{proof}
	We show that the norm of $g(n, A, Q)$ approaches zero as $n \to \infty$.  For any $n$, we have that
	\setcounter{cnt}{1}
	\begin{align}
		\norm{g(n, A, Q)} &= \norm{\sum_{i = 0}^{n-2} \transQ^{i}A\transQ^{n - i - 1}}\\
%		&\stackrel{(\alph{cnt})}{=}  \norm{\sum_{i = 0}^{n-2} \transQ^{i} A \transQ^{n - i - 1}} \\		
%		\addtocounter{cnt}{1}
		&\stackrel{(\alph{cnt})}{\leq} \sum_{i = 0}^{n-2} \norm{\transQ^{i} A \transQ^{n - i - 1}} \\
		\addtocounter{cnt}{1}
		&\stackrel{(\alph{cnt})}{\leq}  \sum_{i = 0}^{n-2} \norm{\transQ}^{i} \cdot \norm{A} \cdot \norm{\transQ}^{n - i - 1} \\
		\addtocounter{cnt}{1}
		&= \sum_{i = 0}^{n-2} \norm{\transQ}^{n - 1} \cdot \norm{A} \\
		&= (n-2) \norm{\transQ}^{n - 1} \cdot \norm{A} 
		\label{eq:lHospital}
	\end{align}
	where 
	\begin{enumerate}[(a)]
%		\item follows from the fact that $\mathbb{R}^{m \times m}$ is a normed  Banach space
		\item follows from sub-additive property of the matrix norm
		\item follows from sub-multiplicative property of the matrix norm	
	\end{enumerate}
	Finally, we use L'Hospital's Rule and the assumption that $\norm{Q} < 1$ to conclude that the right-hand-side of~\eqref{eq:lHospital} approaches zero as $n \to \infty$.
\end{proof}


%%% Alignat
%%\setcounter{cnt}{1}
%%\begin{fleqn}
%%	\begin{alignat}{2}
%%%	\begin{align}
%%		\label{eq:sum_of_equations}
%%		&\mathbb{E}(\Rn | \Si{0} = i, \Si{n} = j) \hidewidth\\
%%		&= \mathbb{E}_{\Si{1}, \Si{2}, \ldots, \Si{n-1}} \mathbb{E}(\Rn | \Si{0} = i, \Si{1}, \Si{2}, \ldots, \Si{n-1}, \Si{n} = j) \hidewidth\\
%%%	&\stackrel{(\alph{cnt})}{=}& \sum_{i = 0}^{M-1} H(\SHatK{2}(Y_i^{n}), \mathcal{S}_i | N_0^n) -  H( \mathcal{S}_i | \SHatK{2}(Y_i^{n}), N_0^n)\\
%%%	&\stackrel{(\alph{cnt})}{=}& \sum_{i = 0}^{M-1} H(\mathcal{S}_i | N_0^n) \\
%%%		\addtocounter{cnt}{1}
%%%	&\stackrel{(\alph{cnt})}{=}& H(\mathcal{S}_0, \mathcal{S}_1, \ldots, \mathcal{S}_{M-1}) | N_0^n) + \sum_{i = 0}^{M-1} I(\mathcal{S}_i; \mathcal{S}_0^{i-1} | N_0^n) \\
%%		&= \sum_{k_1=1}^{\sizeS} \sum_{k_2 = 1}^{\sizeS} \ldots \sum_{k_{n-1} = 1}^{\sizeS} && \Big\{ \mathbb{E}(\Rn | \Si{0} = i, \Si{1} = k_1, \Si{2} = k_2, \ldots, \Si{n-1} = k_{n-1},\Si{n} = j) \phantom{\Big\}} \\
%%			&&&  \quad \times \vphantom{\Big\{} \Prob(\Si{1} = k_1, \Si{2} = k_2, \ldots, \Si{n-1} = k_{n-1}| \Si{0} = i, \Si{n} = j) \Big\} \\
%%		&\stackrel{(\alph{cnt})}{=} \sum_{k_1=1}^{\sizeS} \sum_{k_2 = 1}^{\sizeS} \ldots \sum_{k_{n-1} = 1}^{\sizeS} && \left\{ \mathbb{E}(\Rn | \Si{0} = i, \Si{1} = k_1, \Si{2} = k_2, \ldots, \Si{n-1} = k_{n-1},\Si{n} = j) \vphantom{\frac{\Prob\Si{1}}{\Prob\Si{2}}} \right. \\
%%			&&& \quad \times \left. \frac{\Prob(\Si{1} = k_1, \Si{2} = k_2, \ldots, \Si{n} = j | \Si{0} = i)}{\Prob(\Si{n} = j| \Si{0} = i)} \right\} \\
%%	\addtocounter{cnt}{1}
%%		&\stackrel{(\alph{cnt})}{=} \sum_{k_1=1}^{\sizeS} \sum_{k_2 = 1}^{\sizeS} \ldots \sum_{k_{n-1} = 1}^{\sizeS} && \left( \rew{i}{k_1} + \rew{k_1}{k_2} + \ldots + \rew{k_{n-1}}{j}\right) \times \frac{\Prob(\Si{1} = k_1, \Si{2} = k_2, \ldots, \Si{n} = j | \Si{0} = i)}{\Prob(\Si{n} = j| \Si{0} = i)} \\
%%	\addtocounter{cnt}{1}
%%		&\stackrel{(\alph{cnt})}{=} \sum_{k_1=1}^{\sizeS} \sum_{k_2 = 1}^{\sizeS} \ldots \sum_{k_{n-1} = 1}^{\sizeS} && \left( \rew{i}{k_1} + \rew{k_1}{k_2} + \ldots + \rew{k_{n-1}}{j}\right) \times \frac{\trans{i}{k_1}\trans{k_1}{k_2} \ldots \trans{k_{n-1}}{j}}{\Prob(\Si{n} = j| \Si{0} = i)} \\
%%	\addtocounter{cnt}{1}
%%		&\stackrel{(\alph{cnt})}{=} \sum_{k_{n-1}=1}^{\sizeS} \left( \sum_{k_1=1}^{\sizeS} \sum_{k_2 = 1}^{\sizeS} \ldots \sum_{k_{n-2} = 1}^{\sizeS}  \left( \rew{i}{k_1} + \rew{k_1}{k_2} + \ldots + \rew{k_{n-2}}{k_{n-1}}\right) \times \frac{\trans{i}{k_1}\trans{k_1}{k_2} \ldots \trans{k_{n-1}}{j}}{\Prob(\Si{n} = j| \Si{0} = i)}\right) \hidewidth \\
%%		&\stackrel{(\alph{cnt})}{=}  \sum_{k_1=1}^{\sizeS} \sum_{k_2 = 1}^{\sizeS} \ldots \sum_{k_{n-2} = 1}^{\sizeS}  \rew{k_{n-1}}{k_{n}} \times \frac{\trans{i}{k_1}\trans{k_1}{k_2} \ldots \trans{k_{n-1}}{j}}{\Prob(\Si{n} = j| \Si{0} = i)}  \hidewidth \\
%%	\end{alignat}
%%\end{fleqn}

%\begin{fleqn}
%	\begin{align}
%	\label{eq:sum_of_equations}
%	\mathbb{E}(\Rn | \Si{0} = i, \Si{n} = j) ={}& \mathbb{E}_{\Si{1}, \Si{2}, \ldots, \Si{n-1}} \mathbb{E}(\Rn | \Si{0} = i, \Si{1}, \Si{2}, \ldots, \Si{n-1}, \Si{n} = j)\\
%%	&\stackrel{(\alph{cnt})}{=}& \sum_{i = 0}^{M-1} H(\SHatK{2}(Y_i^{n}), \mathcal{S}_i | N_0^n) -  H( \mathcal{S}_i | \SHatK{2}(Y_i^{n}), N_0^n)\\
%%		\addtocounter{cnt}{1}
%%	&\stackrel{(\alph{cnt})}{=}& \sum_{i = 0}^{M-1} H(\mathcal{S}_i | N_0^n) \\
%%		\addtocounter{cnt}{1}
%%	&\stackrel{(\alph{cnt})}{=}& H(\mathcal{S}_0, \mathcal{S}_1, \ldots, \mathcal{S}_{M-1}) | N_0^n) + \sum_{i = 0}^{M-1} I(\mathcal{S}_i; \mathcal{S}_0^{i-1} | N_0^n) \\
%	\begin{split}
%		\stackrel{(\alph{cnt})}{=}{}& \sum_{k_1} \sum_{k_2} \ldots \sum_{k_{n-1}} \mathbb{E}(\Rn | \Si{0} = i, \Si{1} = k_1, \Si{2} = k_2, \ldots, \Si{n-1} = k_{n-1},\Si{n} = j) \\
%		& \times \Prob(\Si{1} = k_1, \Si{2} = k_2, \ldots, \Si{n-1} = k_{n-1}| \Si{0} = i, \Si{n} = j)
%	\end{split} \\
%	\begin{split}
%		\stackrel{(\alph{cnt})}{=}{}& \sum_{k_1, k_2, \ldots, k_{n-1}} \mathbb{E}(\Rn | \Si{0} = i, \Si{1} = k_1, \Si{2} = k_2, \ldots, \Si{n-1} = k_{n-1},\Si{n} = j) \\
%		& \times \Prob(\Si{1} = k_1, \Si{2} = k_2, \ldots, \Si{n} = j | \Si{0} = i)/\Prob(\Si{n} = j| \Si{0} = i)
%	\end{split} \\
%	\end{align}
%\end{fleqn}
%where 
%\begin{enumerate}[(a)]
%%	\item follows from Corollary~\ref{cor:max_erased_h}
%	\item follows from Lemma~\ref{lem:joint_chain} %, where $\mathcal{S}_0^{i-1} \triangleq \mathcal{S}_0 \cup \mathcal{S}_1 \cup \ldots \cup \mathcal{S}_{i-1}$ 
%	\item follows from Lemma~\ref{lem:HtSM}
%	\item follows from the fact that there are at most $m$ symbols to reconstruct and the source is binary and i.i.d.\
%	\item follows from Lemma~\ref{lem:mi_intersect} and setting $\setURi \triangleq \cup_{j = 0}^{i - 1} R(\nN{j})$
%	\item follows from Lemma~\ref{lem:mi_intersect}
%%	\item $X_i^n$ is the set of unerased symbols in $Y_i^n$, and the Markov chain $\tSM{i}- \YN{i} - \YN{j} - \tSM{j}$ holds
%	\item follows from the Markov chain $\tSM{i}- \YN{i} - \YN{j} - \tSM{j}$ 
%	\item follows from Lemma~\ref{lem:IYiYj}
%%	\item follows from the fact that the strong user is point-to-point optimal so that $X^n$ has maximum entropy.  That is, for non-overlapping subsets $X_i^n$ and $X_j^n$ of $X^n$, we have that these subsets of random variables are nearly independent ($I(X_i^n ; X_j^n) \leq m\delta_m$ where $\delta_m \to 0$ as $m \to \infty$)
%	\item $\delta_m' = M(M+1)\delta_m/2$
%\end{enumerate}