\section{Introduction}

\begin{figure}\centering
    \includegraphics[width=\textwidth]{figures/overview}
    \caption{
        System Overview. (a) We use a large distribution of simulations with randomized parameters and appearances to collect data for both the control policy and vision-based pose estimator. (b) The control policy receives observed robot states and rewards from the distributed simulations and learns to map observations to actions using a recurrent neural network and reinforcement learning. (c) The vision based pose estimator renders scenes collected from the distributed simulations and learns to predict the pose of the object from images using a convolutional neural network (CNN), trained separately from the control policy. (d) To transfer to the real world, we predict the object pose from 3 real camera feeds with the CNN, measure the robot fingertip locations using a 3D motion capture system, and give both of these to the control policy to produce an action for the robot.
    }
    \label{fig:overview}
\end{figure}


While dexterous manipulation of objects is a fundamental everyday task for humans,
it is still challenging for autonomous robots.
Modern-day robots are typically designed for specific tasks in constrained settings and are largely unable to utilize complex end-effectors.
In contrast, people are able to perform a wide range of dexterous manipulation tasks in a diverse set of environments, making the human hand a grounded source of inspiration for research into robotic manipulation.

The Shadow Dexterous Hand~\citep{shadow-robot} is an example of a robotic hand designed for human-level dexterity; it has five fingers with a total of \num{24} degrees of freedom.
The hand has been commercially available since 2005; however it still has not seen widespread adoption, which can be attributed to the daunting difficulty of controlling systems of such complexity.
The state-of-the-art in controlling five-fingered hands is severely limited.
Some prior methods have shown promising in-hand manipulation results in simulation but do not attempt to transfer to a real world robot \citep{DBLP:conf/icra/BaiL14, DBLP:conf/sca/MordatchPT12}.
Conversely, due to the difficulty in modeling such complex systems, there has also been work in approaches that only train on a physical robot \citep{falco2018policy, DBLP:conf/humanoids/HoofHN015, DBLP:journals/corr/KumarGTL16, DBLP:conf/icra/KumarTL16}.
However, because physical trials are so slow and costly to run, the learned behaviors are very limited.

In this work, we demonstrate methods to train control policies that perform in-hand manipulation % of a block and an octagonal prism,
and deploy them on a physical robot.
The resulting policy exhibits unprecedented levels of dexterity and naturally discovers grasp types found in humans, such as the tripod, prismatic, and tip pinch grasps, 
and displays contact-rich, dynamic behaviours such as finger gaiting, multi-finger coordination, the controlled use of gravity, and coordinated application of translational and torsional forces to the object.
Our policy can also use vision to sense an object's pose --- an important aspect for robots that should ultimately work outside of a controlled lab setting.

Despite training entirely in a simulator which substantially differs from the real world,
we obtain control policies which perform well on the physical robot.
We attribute our transfer results to (1) extensive randomizations and added effects in the simulated environment alongside calibration, (2) memory augmented control polices which admit the possibility to learn adaptive behaviour and implicit system identification on the fly, and (3) training at large scale with distributed reinforcement learning.
An overview of our approach is depicted in \autoref{fig:overview}.


The paper is structured as follows.
\autoref{sec:setup} gives a system overview, describes the proposed task in more detail, and shows the hardware setup. \autoref{sec:randomizations} describes observations for the control policy, environment randomizations, and additional effects added to the simulator that make transfer possible.
\autoref{sec:train-policy} outlines the control policy training procedure and the distributed RL system.
\autoref{sec:train-vision} describes the vision model architecture and training procedure.
Finally, \autoref{sec:results} describes both qualitative and quantitative results from deploying the control policy and vision model on a physical robot. %, achieving highly
