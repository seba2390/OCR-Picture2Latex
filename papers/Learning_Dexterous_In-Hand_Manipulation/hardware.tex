

Our hardware interface is specifically designed to support a neural network policy learned with domain randomization to manipulate arbitrary objects. The neural network policy is used as a high-level controller, while low-level PD controllers implement the actions. The neural network policy receives a small set of observations at each time step, chosen such that their values and the real-world noise sources can be modeled in the simulator. To support a variety of objects, we use RGB images.

\subsubsection{Sensing and Actuation}


The sensors provided to the policy and low-level controllers are:
\begin{center}
    \scriptsize
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{@{}lll@{}}
        \toprule
        \textbf{component} & \textbf{sensors in state-based mode} & \textbf{sensors in vision mode} \\ \midrule
        policy neural network &
            PhaseSpace-fingertips & PhaseSpace-fingertips\\
          & PhaseSpace-object & Basler RGB cameras \\
        low-level controller &
            joint position sensors & 
                joint position sensors \\
    \bottomrule\end{tabular}
\end{center}



\subsubsection{Robot API}


Although the Shadowhand contains a broad array of built-in sensors, we specifically avoided providing these as observations to the policy, because they were subject to noise that would have been difficult to model in the simulator.


For example, the fingertip tactile sensor measures the pressure of a fluid stored in a balloon inside the fingertip, which correlates with the force applied to the fingertip but also with a number of confounding variables, including atmospheric pressure, temperature, and the shape of the contact and intersection geometry. Although it is straightforward to determine the existence of contacts in the simulator, it would have been difficult to hand-engineer a model to reproduce the distribution of sensor values in the simulator. Furthermore, the policy is already provided with high-quality measurements of the fingertip positions and object pose, which are highly correlated with the contact forces which the tactile sensors are designed to measure.

Similar considerations applied to the joint angles measured by the Hall effect sensors, which are used by the low-level controllers but not provided to the policy.



The high-level controller is implemented as a Python program. Every 80ms, it queries the PhaseSpace sensors, then runs inference with the neural network to obtain the action, which takes roughly 25ms. The policy outputs an action that specifies the change of position for each actuator, relative to the current position of the joints controlled by the actuator. It then sends the action to the low-level controller.

The low-level controller is implemented in C++ as a separate process on a different machine which is 
connected to the Shadowhand via an Ethernet cable. The controller is written as a real-time system 
-- it's pinned to a CPU core, has preallocated memory, and doesn't depend on any garbage collector (which could incur non-deterministic delays).
The controller receives the relative action, converts it to 
an absolute joint angle and clips to the valid range, than sets each component of the action as the target for a PD controller. Every 5ms, the PD controllers query the Shadowhand joint angle sensors, then attempt to achieve the desired position.


Contrary to our initial expectations, decreasing the time between actions in the high-level controller below 80ms does not noticeably improve performance at the object orientation task.

