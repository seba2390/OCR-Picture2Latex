In order to make it easier to understand the state-of-the-art in dexterous in-hand manipulation
we gathered a representative set of videos from related work, and created a playlist\footnote{Related work playlist: \url{https://bit.ly/2uOK21Q}} out of them.


\subsection{Dexterous Manipulation}

Dexterous manipulation has been an active area of research for decades~\citep{DBLP:conf/icra/Fearing86, DBLP:journals/ijrr/Rus99,DBLP:journals/trob/Bicchi00, DBLP:conf/icra/OkamuraSC00, DBLP:conf/icar/MaD11}.
Many different approaches and strategies have been proposed over the years.
This includes rolling~\citep{DBLP:conf/icra/BicchiS95, DBLP:conf/icra/HanGLQT97, DBLP:conf/icra/HanT98, DBLP:journals/trob/CherifG99, DBLP:conf/icra/DoulgeriD13}, sliding~\citep{DBLP:journals/trob/CherifG99, DBLP:journals/trob/ShiWUL17}, finger gaiting~\citep{DBLP:conf/icra/HanT98}, finger tracking~\citep{DBLP:conf/icra/Rus92}, pushing~\citep{DBLP:journals/corr/DafleR17}, and re-grasping~\citep{DBLP:conf/icra/TournassoudLM87, DBLP:conf/icra/DafleRPTSEMLSF14}.
For some hand types, strategies like pivoting~\citep{DBLP:conf/iros/AiyamaII93}, tilting~\citep{DBLP:journals/trob/ErdmannM88}, tumbling~\citep{sawasaki1991tumbling}, tapping~\citep{DBLP:journals/ijrr/HuangM00}, two-point manipulation~\citep{DBLP:conf/iros/AbellE95}, and two-palm manipulation~\citep{DBLP:journals/ijrr/Erdmann98} are also options.
These approaches use planning and therefore require exact models of both the hand and object.
After computing a trajectory, the plan is typically executed open-loop, thus making these methods prone to failure if the model is not accurate.\footnote{Some methods use iterative re-planning to partially mitigate this issue.}

Other approaches take a closed-loop approach to dexterous manipulation and incorporate sensor feedback during execution, e.g. tactile sensing~\citep{DBLP:conf/icra/TaharaAY10, DBLP:conf/iros/LiBKB14, DBLP:conf/icra/LiYTB14, DBLP:journals/ijma/0001MHRB13}.
While those approaches allow to correct for mistakes at runtime, they still require reasonable models of the robot kinematics and dynamics, which can be challenging to obtain for under-actuated hands with many degrees of freedom.

Deep reinforcement learning has also been used successfully to learn complex manipulation skills on physical robots.
Guided policy search~\citep{DBLP:conf/icml/LevineK13, DBLP:conf/icra/LevineWA15} learns simple local policies directly on the robot and distills them into a global policy represented by a neural network.
An alternative is to use many physical robots simultaneously in order to be able to collect sufficient experience~\citep{DBLP:conf/icra/GuHLL17, DBLP:journals/ijrr/LevinePKIQ18, 2018arXiv180610293K}.

\subsection{Dexterous In-Hand Manipulation}
Since a very large body of past work on dexterous manipulation exists, we limit the more detailed discussion to setups that are most closely related to our work on dexterous in-hand manipulation.

Mordatch et al.~\citep{DBLP:conf/sca/MordatchPT12} and Bai et al.~\citep{DBLP:conf/icra/BaiL14} propose methods to generate trajectories for complex and dynamic in-hand manipulation, but results are limited to simulation. %\footnote{Video for \citep{DBLP:conf/sca/MordatchPT12}: \url{https://youtu.be/Gzt2UoxYfAQ}}\footnote{Video for \citep{DBLP:conf/icra/BaiL14}: \url{https://youtu.be/KBDTrmXOF4o}}
There has also been significant progress in learning complex in-hand dexterous manipulation~\citep{plappert2018multi, DBLP:journals/corr/abs-1804-08617} and even tool use~\citep{DBLP:journals/corr/abs-1709-10087} using deep reinforcement learning but those approaches were only evaluated in simulation as well.

In contrast, multiple authors learn policies for dexterous in-hand manipulation directly on the robot.
Hoof et al.~\citep{DBLP:conf/humanoids/HoofHN015} learn in-hand manipulation for a simple 3-fingered gripper whereas Kumar et al.~\citep{DBLP:conf/icra/KumarTL16, DBLP:journals/corr/KumarGTL16} and Falco et al.~\citep{falco2018policy} learn such policies for more complex humanoid hands.
While learning directly on the robot means that modeling the system is not an issue, it also means that learning has to be performed with only a handful of trials.
This is only possible when learning very simple (e.g. linear or local) policies that, in turn, do not exhibit sophisticated behaviors. %\footnote{Video for \citep{DBLP:conf/humanoids/HoofHN015}: \url{https://youtu.be/DQPoJfBxATs}}\footnote{Video for \citep{DBLP:journals/corr/KumarGTL16}: \url{https://youtu.be/bD5z1I1TU3w}}\footnote{Video for \citep{falco2018policy}: \url{https://youtu.be/wkCXy5ywkVE}}


\subsection{Sim to Real Transfer}

\emph{Domain adaption} methods~\citep{DBLP:journals/corr/TzengDHFPLSD15, DBLP:journals/corr/GuptaDLAL17}, progressive nets~\citep{DBLP:conf/corl/RusuVRHPH17}, and learning inverse dynamics models~\citep{DBLP:journals/corr/ChristianoSMSBT16} were all proposed to help with sim to real transfer.
All of these methods assume access to real data.
An alternative approach is to make the policy itself more adaptive during training in simulation using \emph{domain randomization}.
Domain randomization was used to transfer object pose estimators~\citep{tobin2017domain} and vision policies for fly drones~\citep{DBLP:conf/rss/SadeghiL17}.
This idea has also been extended to dynamics randomization~\citep{DBLP:journals/corr/AntonovaCSK17, DBLP:journals/corr/abs-1804-10332, DBLP:conf/rss/YuTLT17} to learn a robust policy that transfers to similar environments but with different dynamics.
Domain randomization was also used to plan robust grasps~\citep{DBLP:conf/rss/MahlerLNLDLOG17, DBLP:journals/corr/abs-1709-06670, DBLP:journals/corr/abs-1710-06425} and to transfer learned locomotion~\citep{DBLP:journals/corr/abs-1804-10332} and grasping~\citep{DBLP:journals/corr/abs-1802-09564} policies for relatively simple robots.
Pinto et al.~\citep{DBLP:conf/icml/PintoDSG17} propose to use \emph{adversarial training} to obtain more robust policies and show that it also helps with transfer to physical robots~\citep{DBLP:conf/icra/PintoDG17}.
