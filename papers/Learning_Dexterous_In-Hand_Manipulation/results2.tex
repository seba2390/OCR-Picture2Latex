In this section, we evaluate the proposed system.
We start by deploying the system on the physical robot,
and evaluating its performance on in-hand manipulation of a block and an octagonal prism.
We then focus on individual aspects of our system:
We conduct an ablation study of the importance of randomizations
and policies with memory capabilities in order to successfully transfer.
Next, we consider the sample complexity of our proposed method. % and show that we only simulation and scaled-up reinforcement learning can solve the problem in a reasonable amount of time.
Finally, we investigate the performance of the proposed vision pose estimator and show that using only synthetic images is sufficient to achieve good performance.

\subsection{Qualitative Results}

During deployment on the robot as well as in simulation, we notice that our policies naturally exhibit many of the grasps found in humans (see \autoref{fig:grasps}).
Furthermore, the policy also naturally discovers many strategies for dexterous in-hand manipulation described by the robotics community~\citep{DBLP:conf/icar/MaD11} such as finger pivoting, finger gaiting,
multi-finger coordination, the controlled use of gravity, and coordinated application of translational and torsional forces to the object.
It is important to note that we did not incentivize this directly: we do not use any human demonstrations and do not encode any prior into the reward function.

For precision grasps, our policy tends to use the little finger instead of the index or middle finger.
This may be because the little finger of the Shadow Dexterous Hand has an extra degree of freedom compared to the index, middle and ring fingers, making it more dexterous.
In humans the index and middle finger are typically more dexterous.
This means that our system can rediscover grasps found in humans, but adapt them to better fit the limitations and abilities of its own body.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.32\textwidth]{figures/grasp-tip-pinch}
    \includegraphics[width=0.32\textwidth]{figures/grasp-palmar-pinch}
    \includegraphics[width=0.32\textwidth]{figures/grasp-tripod}\\ \vspace{0.05cm}
    \includegraphics[width=0.32\textwidth]{figures/grasp-quadpod}
    \includegraphics[width=0.32\textwidth]{figures/grasp-5-fingered-octo}
    \includegraphics[width=0.32\textwidth]{figures/grasp-default-octo}
    \caption{Different grasp types learned by our policy. From top left to bottom right: Tip Pinch grasp, Palmar Pinch grasp, Tripod grasp, Quadpod grasp, 5-Finger Precision grasp, and a Power grasp. Classified according to~\citep{grasp}.}
    \label{fig:grasps}
\end{figure}

We observe another interesting parallel between humans and our policy in finger pivoting, which is a strategy in which an object is held between two fingers and rotate around this axis.
It was found that young children have not yet fully developed their motor skills and therefore tend to rotate objects using the proximal or middle phalanges of a finger~\citep{pehoski1997hand}.
Only later in their lives do they switch to primarily using the distal phalanx, which is the dominant strategy found in adults.
It is interesting that our policy also typically relies on the distal phalanx for finger pivoting.

During experiments on the physical robot we noticed that the most common failure mode was dropping the object
while rotating the wrist pitch joint down.
Moreover, the vertical joint was the most common source of robot breakages, probably because
it handles the biggest load.
Given these difficulties,
we also trained a policy with the wrist pitch joint locked.\footnote{We had trouble training
in this environment from scratch, so we fine-tuned
a policy trained in the original environment instead.}
We noticed that not only does this policy transfer better to the physical robot but
it also seems to handle the object much more deliberately with many of the above grasps emerging frequently in this setting.
Other failure modes that we observed were dropping the object shortly after the start of a trial (which may be explained by incorrectly identifying some aspect of the environment) and getting stuck because the edge of an object got caught in a screw hole (which we do not model).



We encourage the reader to watch the accompanying video to get a better sense of the learned behaviors.\footnote{Real-time video of $50$ successful consecutive rotations: \url{https://youtu.be/DKe8FumoD4E}}




\subsection{Quantitative Results}
\label{sec:quant-results}
In this section we evaluate our results quantitatively.
To do so, we measure the number of \emph{consecutive} successful rotations until the object is either dropped, a goal has not been achieved within 80 seconds, or until $50$ rotations are achieved.
All results are available in~\autoref{table:perf}.




\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \caption{The number of successful consecutive rotations in simulation and on the physical robot. All policies were trained on environments with all randomizations enabled. We performed 100 trials in simulation and 10 trails per policy on the physical robot. Each trial terminates when the object is dropped, 50 rotations are achieved or a timeout is reached. For physical trials, results were taken at different times on the physical robot.}
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Simulated task} & \textbf{Mean} & \textbf{Median} & \textbf{Individual trials (sorted)} \\ 
        \midrule
        Block (state) & $43.4 \pm 13.8$ & $50$ & - \\
        Block (state, locked wrist) & $44.2 \pm 13.4$ & $50$ & - \\
        Block (vision) & $30.0 \pm 10.3$ & $33$ & - \\
        Octagonal prism (state) & $29.0 \pm 19.7$ & $30$ & - \\
        \midrule
        \textbf{Physical task} \\
        \midrule
        
        
        Block (state) & $18.8 \pm 17.1$  & $13$ & $50$, $41$, $29$, $27$, $14$, $12$, $6$, $4$, $4$, $1$ \\
        
        Block (state, locked wrist) & $26.4 \pm 13.4$ & $28.5$ & $50$, $43$, $32$, $29$, $29$, $28$, $19$, $13$, $12$, $9$ \\
        
        Block (vision) & $15.2 \pm 14.3$ & $11.5$ & $46$, $28$, $26$, $15$, $13$, $10$, $8$, $3$, $2$, $1$ \\
        Octagonal prism (state) & $7.8 \pm 7.8$ & $5$ & $27$, $15$, $8$, $8$, $5$, $5$, $4$, $3$, $2$, $1$ \\
        \bottomrule
    \end{tabular}
    \label{table:perf}
\end{table}

        
        
        
        


Our results allow us to directly compare the performance of each task in simulation and on the real robot.
For instance, manipulating a block in simulation achieves a median of $50$ successes while the median on the physical setup is $13$.
This is the overall trend that we observe: Even though randomizations and calibration narrow the reality gap, it still exists and performance on the real system is worse than in simulation. We discuss the importance of individual randomizations in greater detail in \autoref{sec:ablation-rand}.

When using vision for pose estimation, we achieve slightly worse results both in simulation and on the real robot. This is because even in simulation, our model has to perform transfer because it was only trained on images rendered with Unity but we use MuJoCo rendering for evaluation in simulation (thus making this a sim-to-sim transfer problem).
On the real robot, our vision model does slightly worse compared to pose estimation with PhaseSpace. However, the difference is surprisingly small, suggesting that training the vision model only in simulation is enough to achieve good performance on the real robot.
For vision pose estimation we found that it helps to use a white background and to wipe the object with a tack cloth between trials to remove detritus from the robot hand.

We also evaluate the performance on a second type of object, an octagonal prism.
To do so, we finetuned a trained block rotation control policy to the same randomized distribution of environments but with the octagonal prism as the target object instead of the block.
Even though our randomizations were all originally designed for the block, we were able to learn successful policies that transfer.
Compared to the block, however, there is still a performance gap both in simulation and on the real robot.
This suggests that further tuning is necessary and that the introduction of additional randomization could improve transfer to the physical system.

We also briefly experimented with a sphere but failed to achieve more than a few rotations in a row, perhaps
because we did not randomize any MuJoCo parameters related to rolling behavior or because rolling objects are much more sensitive to unmodeled imperfections in the hand such as screw holes.
It would also be interesting to train a unified policy that can handle multiple objects, but we leave this for future work.

Obtaining the results in \autoref{table:perf} proved to be challenging due to robot breakages during experiments.
Repairing the robot takes time and often changes some aspects of the system, which is why the results were obtained at different times.
In general, we found that problems with hardware breakage were one of the key challenges we had to overcome in this work.















\subsection{Ablation of Randomizations}
\label{sec:ablation-rand}

\begin{figure}[h]
    \begin{minipage}[c]{0.55\textwidth}
        \includegraphics[width=0.95\textwidth]{figures/randomization_ablations.pdf}
    \end{minipage}\hfill
    \begin{minipage}[c]{0.45\textwidth}
        \caption{Performance when training in environments with groups of randomizations held out. All runs show exponential moving averaged performance and 90\% confidence interval over a moving window of the RL agent in the environment it was trained. We see that training is faster in environments that are easier, e.g. \textit{no randomizations} and \textit{no unmodeled effects}. We only show one seed per experiment; however, in general we have noticed almost no instability in training.
        }
        \label{fig:rand-abl}
    \end{minipage}\hfill
\end{figure}

In \autoref{subsection:randomizations} we detail a list of parameters we randomize and effects we add that are not already modeled in the simulator. In this section we show that these additions to the simulator are vital for transfer. We train 5 separate RL policies in environments with various randomizations held out: \textit{all randomizations} (baseline), \textit{no observation noise}, \textit{no unmodeled effects}, \textit{no physics randomizations}, and \textit{no randomizations} (basic simulator, i.e. no domain randomization).

Adding randomizations or effects to the simulation does not come without cost; in \autoref{fig:rand-abl} we show the training performance in simulation for each environment plotted over wall-clock time. Policies trained in environments with a more difficult set of randomizations, e.g. \textit{all randomizations} and \textit{no observation noise}, converge much slower and therefore require more compute and simulated experience to train in. However, when deploying these policies on the real robot we find that training with randomizations is critical for transfer. \autoref{table:rand-abl} summarizes our results.
Specifically, we find that training with all randomizations leads to a median of $13$ consecutive goals achieved, while policies trained with \textit{no randomizations}, \textit{no physics randomizations}, and \textit{no unmodeled effects} achieve only median of $0$, $2$, and $2$ consecutive goals, respectively.

\begin{table}[h]
    \centering
    \caption{
    The number of successful consecutive rotations on the physical robot of 5 policies trained separately in environments with different randomizations held out.
    The first 5 rows use PhaseSpace for object pose estimation and were run on the same robot at the same time. Trials for each row were interleaved in case the state of the robot changed during the trials. The last two rows were measured at a different time from the first 5 and used the vision model to estimate the object pose.
    }
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Training environment} & \textbf{Mean} & \textbf{Median} & \textbf{Individual trials (sorted)} \\ 
        \midrule
        All randomizations (state) & $18.8 \pm 17.1$  & $13$ & $50$, $41$, $29$, $27$, $14$, $12$, $6$, $4$, $4$, $1$ \\
        No randomizations (state) & $1.1 \pm 1.9$ & $0$ & $6$, $2$, $2$, $1$, $0$, $0$, $0$, $0$, $0$, $0$ \\
        No observation noise (state) & $15.1 \pm 14.5$ & $8.5$ & $45$, $35$, $23$, $11$, $9$, $8$, $7$, $6$, $6$, $1$ \\
        No physics randomizations (state) & $3.5 \pm 2.5$ & $2$ & $7$, $7$, $7$, $3$, $2$, $2$, $2$, $2$, $2$, $1$ \\
        No unmodeled effects (state) & $3.5 \pm 4.8$ & $2$ & $16$, $7$, $3$, $3$, $2$, $2$, $1$, $1$, $0$, $0$ \\
        \midrule
        All randomizations (vision) & $15.2 \pm 14.3$ & $11.5$ & $46$, $28$, $26$, $15$, $13$, $10$, $8$, $3$, $2$, $1$ \\
        No observation noise (vision) & $5.9 \pm 6.6$ & $3.5$ & $20$, $12$, $11$, $6$, $5$, $2$, $2$, $1$, $0$, $0$ \\
    \bottomrule
    \end{tabular}
    \label{table:rand-abl}
\end{table}

When holding out \emph{observation noise} randomizations, the performance gap is less clear than for the other randomization groups.
We believe that is because our motion capture system has very little noise.
However, we still include this randomization because it is important when the vision and control policies are composed.
In this case, the pose estimate of the object is much more noisy, and, therefore, training with observation noise should be more important.
The results in \autoref{table:rand-abl} suggest that this is indeed the case, with a drop from median performance of $11.5$ to $3.5$ if the observation noise randomizations are withheld.

The vast majority of training time is spent making the policy robust to different physical dynamics. Learning to rotate an object in simulation without randomizations requires about 3 years of simulated experience, while achieving the same performance in a fully randomized simulation requires about 100 years of experience.
This corresponds to a wall-clock time of around 1.5 hours and 50 hours in our simulation setup, respectively.



\subsection{Effect of Memory in Policies}

We find that using memory is helpful to achieve good performance in the randomized simulation. In \autoref{fig:mem-abl} we show the simulation performance of three different RL architectures: the baseline which has a LSTM policy and value function, a feed forward (FF) policy and a LSTM value function, and both a  FF policy and FF value function. We include results for a FF policy with LSTM value function because it was plausible that having a more expressive value function would accelerate training, allowing the policy to act robustly without memory once it converged. However, we see that the baseline outperforms both variants, indicating that it is beneficial to have some amount of memory in the actual policy.

Moreover, we found out that LSTM state is predictive of the environment randomizations.
In particular, we discovered that the LSTM hidden state after 5 seconds of simulated interaction
with the block allows to predict whether the block is bigger or smaller than
average in $80\%$ of cases.

\begin{figure}[h]
    \begin{minipage}[c]{0.55\textwidth}
        \includegraphics[width=0.95\textwidth]{figures/effect_of_mem.pdf}
    \end{minipage}\hfill
    \begin{minipage}[c]{0.45\textwidth}
        \caption{Performance when comparing LSTM and feed forward (FF) policy and value function networks. We train on an environment with all randomizations enabled. All runs show exponential moving averaged performance and 90\% confidence interval over a moving window for a single seed. We find that using recurrence in both the policy and value function helps to achieve good performance in simulation.
        }
        \label{fig:mem-abl}
    \end{minipage}\hfill
\end{figure}


To investigate the importance of memory-augmented policies for transfer, we evaluate the same three network architectures as described above on the physical robot. \autoref{table:memory-abl} summarizes the results.
Our results show that having a policy with access to memory yields a higher median of successful rotations, suggesting that the policy may use memory to adapt to the current environment.\footnote{When training in an environment with no randomizations, the FF and LSTM policy converge to the same performance in the same amount of time. This shows that a FF policy has the capacity and observations to solve the non-randomized task but cannot solve it reliably with all randomizations, plausibly because it cannot adapt to the environment.}
Qualitatively we also find that FF policies often get stuck and then run out of time.

\begin{table}[h]
    \centering
    \caption{
    The number of successful consecutive rotations on the physical robot of 3 policies with different network architectures trained on an environment with all randomizations.
    Results for each row were collected at different times on the physical robot.
    }
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Network architecture} & \textbf{Mean} & \textbf{Median} & \textbf{Individual trials (sorted)} \\ 
        \midrule
        LSTM policy / LSTM value (state) & $18.8 \pm 17.1$  & $13$ & $50$, $41$, $29$, $27$, $14$, $12$, $6$, $4$, $4$, $1$ \\
        FF policy / LSTM value (state) & $4.7 \pm 4.1$ & $3.5$ & $15$, $7$, $6$, $5$, $4$, $3$, $3$, $2$, $2$, $0$ \\
        FF policy / FF value (state) & $4.6 \pm 4.3$ & $3$ & $15$, $8$, $6$, $5$, $3$, $3$, $2$, $2$, $2$, $0$ \\
    \bottomrule
    \end{tabular}
    \label{table:memory-abl}
\end{table}

\subsection{Sample Complexity \& Scale}



In \autoref{fig:scale} we show results when varying the number of CPU cores and GPUs used in training, where we keep the batch size per GPU fixed such that overall batch size is directly proportional to number of GPUs.
Because we could linearly slow down training by simply using less CPU machines and having the GPUs wait longer for data, it is more informative to vary the batch size.
We see that our default setup with an 8 GPU optimizer and 6144 rollout CPU cores
reaches 20 consecutive achieved goals approximately 5.5 times faster than a setup with a 1 GPU optimizer and 768 rollout cores. Furthermore, when using 16 GPUs we reach 40 consecutive achieved goals roughly 1.8 times faster than when using the default 8 GPU setup.
Scaling up further results in diminishing returns, but it seems that scaling up to 16 GPUs and 12288 CPU cores gives close to linear speedup.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/effect_of_scale1.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/effect_of_scale2.pdf}
    \end{subfigure}
    \caption{We show performance in simulation when varying the amount of compute used during training versus wall clock training time (left) and years of experience consumed (right). Batch size used is proportional to the number of GPUs used, such that time per optimization step should remain constant apart from slow downs due to gradient syncing across optimizer machines.}
    \label{fig:scale}
\end{figure}




\subsection{Vision Performance}
\label{sec:result-vision}
In \autoref{table:perf} we show that we can combine a vision-based pose estimator and the control policy to successfully transfer to the real robot without embedding sensors in the target object.
To better understand why this is possible, we evaluate the precision of the pose estimator on both synthetic and real data.
Evaluating the system in simulation is easy because we can generate the necessary data and have access to the precise object's pose to compare against.
In contrast, real images had to be collected by running a state-based policy on our robot platform.
We use PhaseSpace to estimate the object's pose, which is therefore subject to errors.
The resulting collected test set consists of $992$ real samples.\footnote{A sample contains 3 images of the same scene. We removed a few samples that had no object in them after it being dropped.}
For simulation, we use test sets rendered using Unity and MuJoCo. The MuJoCo renderer was not used during training, thus the evaluation can be also considered as an instance of sim-to-sim transfer.
\autoref{table:vision} summarizes our results.

\begin{table}[h]
    \caption{Performance of a vision based pose estimator on synthetic and real data.}
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{@{}lll@{}}
        \toprule
        \textbf{Test set} & \textbf{Rotation error} & \textbf{Position error} \\ %\textbf{Rotation Error} & \textbf{Position Error} \\
        \midrule
        Rendered images (Unity) & $2.71^{\circ} \pm 1.62$ & $3.12\text{mm} \pm 1.52$ \\
        Rendered images (MuJoCo) & $3.23^{\circ} \pm 2.91$ & $3.71\text{mm} \pm 4.07$ \\
        Real images & $5.01^{\circ} \pm 2.47$ & $9.27\text{mm} \pm 4.02$ \\
    \bottomrule\end{tabular}
    \label{table:vision}
\end{table}

Our results show that the model achieves low error for both rotation and position prediction when tested on synthetic data.\footnote{For comparison, PhaseSpace is rated for a position accuracy of around $20$ $\mu$m but requires markers and a complex setup.}
On the images rendered with MuJoCo, there is only a slight increase in error, suggesting successful sim-to-sim transfer.
The error further increases on the real data, which is due to the gap between simulation and reality but also because the ground truth is more challenging to obtain due to noise, occlusions, imperfect marker placement, and delayed sensor readings.
Despite that the prediction error is bigger than the observation noise used during policy training (\autoref{table:obs-noise}), the vision-based policy performs well
on the physical robot (\autoref{table:perf}).








