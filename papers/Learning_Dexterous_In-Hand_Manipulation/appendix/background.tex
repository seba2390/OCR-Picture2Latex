\section{Reinforcement Learning Background}\label{sec:rl}




\subsection{Reinforcement Learning (RL)}

We consider the standard reinforcement learning formalism
consisting of an agent interacting with an environment.
To simplify the exposition we assume in this section that the environment is fully observable.\footnote{The environments
we consider in the paper are only partially observable.}
An environment
is described by
a set of states $\S$,
a set of actions $\A$,
a distribution of initial states $p(s_0)$,
a reward function $r : \S \times \A \rightarrow \R$,
transition probabilities $p(s_{t+1}|s_t,a_t)$,
and a discount factor $\gamma \in [0,1]$.

A policy $\pi$ is a mapping from state to a distribution over actions.
Every episode starts by sampling an initial state $s_0$.
At every timestep $t$ the agent produces an action based on the current state:
$a_t \sim \pi(\cdot|s_t)$.
In turn, the agents receives a reward $r_t=r(s_t,a_t)$ and the environment's new state $s_{t+1}$, which is sampled from the distribution $p(\cdot|s_t,a_t)$.
The discounted sum of future rewards, also referred to as the \emph{return}, is defined as
$R_t=\sum_{i=t}^\infty \gamma^{i-t} r_i$.
The agent's goal is to maximize its expected return $\E [R_0|s_0]$, where
the expectation is taken over the initial state distribution, policy, and environment transitions accordingly to the dynamics
specified above.
The \emph{Q-function} or \emph{action-value} function is defined as $Q^\pi(s_t,a_t)=\E[R_t|s_t,a_t]$, while the
\emph{V-function} or \emph{state-value} function is defined as $V^\pi(s_t)=\E[R_t|s_t]$.
The value $A^\pi(s_t,a_t)=Q^\pi(s_t,a_t)-V^\pi(s_t)$ is called
the \emph{advantage} and tells whether the action $a_t$ is better or worse than an average
action the policy $\pi$ takes in the state~$s_t$.



\subsection{Generalized Advantage Estimator (GAE)} \label{sec:gae}

Let $V$ be an approximator to the value function of some policy, i.e. $V \approx V^\pi$.
The value $$\hat{V}_t^{(k)}=\sum_{i=t}^{t+k-1} \gamma^{i-t} r_i + \gamma^{k} V(s_{t+k}) \approx V^\pi(s_t,a_t)$$
is called the $k$-step return estimator.
The parameter $k$ controls the bias-variance tradeoff of the estimator
with bigger values resulting in an estimator closer
to empirical returns and having less bias and more variance.
\emph{Generalized Advantage Estimator (GAE)} \citep{gae}
is a method of combining multi-step returns in the following way:
$$\hat{V}_t^\text{GAE} = (1-\lambda) \sum_{k>0}\lambda^{k-1} \hat{V}_t^{(k)} \approx V^\pi(s_t,a_t),$$
where $0<\lambda<1$ is a hyperparameter. Using these to estimate the \emph{advantage}:
$$\hat{A}_t^\text{GAE} = \hat{V}_t^\text{GAE} - V(s_t) \approx A^\pi(s_t,a_t).$$
It is possible
to compute the values of this estimator for all states
encountered in an episode in linear time \citep{gae}.

\subsection{Proximal Policy Optimization (PPO)} \label{sec:ppo}

\emph{Proximal Policy Optimization (PPO)} \citep{ppo} is one of the most popular on-policy RL algorithms.
It simultaneously optimizes a stochastic policy as well as an approximator to the value function.
PPO interleaves the collection of new episodes with policy optimization.
After a batch of new transitions is collected, optimization is performed
with minibatch stochastic gradient descent to maximize the objective $$L_{\text{PPO}}=\E \min \left( \frac{\pi(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)} \hat{A}^\text{GAE}_t,\, \mbox{clip}\left(\frac{\pi(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)},\,1-\epsilon,\,1+\epsilon \right)\hat{A}^\text{GAE} _t\right),$$ where
$\frac{\pi(a_t|s_t)}{\pi_{\text{old}}(a_t|s_t)}$ is the ratio of the probability of taking the given action under the current policy $\pi$ to the probability
of taking the same action under the old behavioral policy that was used to generate the data,
and $\epsilon$ is a hyperparameter (usually $\epsilon \approx 0.2$).
This loss encourages the policy to take actions which are better than average (have positive advantage)
while clipping discourages bigger changes to the policy by limiting how much can be gained
by changing the policy on a particular data point.
The value function approximator is trained with supervised learning with the target for $V(s_t)$ being $\hat{V}_t^\text{GAE}$.
To boost exploration, it is a common practice to encourage the policy to have high entropy of actions by including an entropy bonus in the optimization objective.