
\begin{figure}
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/setup_both}
    \end{subfigure}%
    \caption{(left) The "cage" which houses the robot hand, 16 PhaseSpace tracking cameras, and 3 Basler RGB cameras. (right) A rendering of the simulated environment.
    }
    \label{fig:setup}
\end{figure}

In this work we consider the problem of in-hand object reorientation.
We place the object under consideration onto the palm of a humanoid robot hand.
The goal is to reorient the object to a desired target configuration in-hand.
As soon as the current goal is (approximately) achieved, a new goal is provided until the object is eventually dropped.
We use two different objects, a block and an octagonal prism.
\autoref{fig:setup} depicts our physical system as well as our simulated environment.


\subsection{Hardware}

We use the Shadow Dexterous Hand, which is a humanoid robotic hand with $24$ degrees of freedom (DoF) actuated by $20$ pairs of agonist--antagonist tendons.
We use a PhaseSpace motion capture system to track the Cartesian position of all five finger tips.
For the object pose, we have two setups: One that uses PhaseSpace markers to track the object and one that uses three Basler RGB cameras for vision-based pose estimation.
This is because our goal is to eventually have a system that works outside of a lab environment, and vision-based systems are better equipped to handle the real world.
We do not use the touch sensors embedded in the hand and only use joint sensing for implementing low-level relative position control. We update the targets of the low level controller, which runs at roughly \SI{1}{kHz}, with relative positions given by the control policy at roughly \SI{12}{Hz}.




More details on our hardware setup are available in \autoref{app:hardware}.

\subsection{Simulation}
We simulate the physical system with the MuJoCo physics engine~\citep{MuJoCo}, and we use Unity\footnote{Unity game engine website: \url{https://unity3d.com/}} to render the images
for training the vision based pose estimator. Our model of the Shadow Dexterous Hand is based on the one used in the OpenAI~Gym robotics environments~\citep{plappert2018multi} but has been improved to match the physical system more closely through calibration (see Appendix~\ref{app:model-calibration} for details).

Despite our calibration efforts, the simulation is still a rough approximation of the physical setup.
For example, our model directly applies torque to joints instead of tendon-based actuation and uses rigid body contact models instead of deformable body contact models.
Modeling these and other effects seen in the real world is difficult or impossible in a rigid body simulator. %(e.g. interaction with rubber surfaces and stretching tendons aren't well supported).
These differences cause a "reality gap" and make it unlikely for a policy trained in a simulation with these inaccuracies to transfer well.

We describe additional details of our simulation in Appendix~\ref{app:sim}.
