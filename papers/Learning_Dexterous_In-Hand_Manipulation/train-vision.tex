The policy that we describe in the previous section
takes the object's position as input and requires a motion capture system for tracking the object
on the physical robot.
This is undesirable because tracking objects with such a system is only feasible in a lab setting where markers can be placed on each object.
Since our ultimate goal is to build robots for the real world that can interact with arbitrary objects, sensing them using vision is an important building block.
In this work, we therefore wish to infer the object's pose from vision alone.
Similar to the policy, we train this estimator only on synthetic data coming from the simulator. %data and do not require data from the real robot.

\subsection{Model Architecture}
\label{sec:vision_model_arch}

To resolve ambiguities and to increase robustness, we use three RGB cameras mounted with differing viewpoints of the scene.
The recorded images are passed through a convolutional neural network, which is depicted in \autoref{fig:vision-architecture}.
The network predicts both the position and the orientation of the object.
During execution of the control policy on the physical robot, we feed the pose estimator's prediction into the policy,
which in turn produces the next action.

\begin{figure}[h]
    \begin{minipage}[c]{0.45\textwidth}
        \includegraphics[width=0.8\textwidth]{figures/policy3}
    \end{minipage}\hfill
    \begin{minipage}[c]{0.55\textwidth}
        \caption{Vision network architecture. Camera images are passed through a convolutional feature stack that consists of two convolutional layers, max-pooling, 4 ResNet blocks~\cite{He2016DeepRL}, and spatial softmax (SSM)~\cite{finn2015deep} layers with shared weights between the feature stacks for each camera.
        The resulting representations are flattened, concatenated, and fed to a fully connected network. %(128 neurons).
        All layers use ReLU \cite{relu} activation function.
        Linear outputs from the last layer form the estimates of the position and orientation of the object.
        }
        \label{fig:vision-architecture}
    \end{minipage}\hfill
\end{figure}

\subsection{Training}


We run the trained policy in the simulator until we gather one million states.
We then train the vision network by minimizing the mean squared error between the normalized prediction and the ground-truth
with minibatch gradient descent.
For each minibatch, 
we render the images with randomized appearance before feeding them to the network.
Moreover, we augment the data by modifying the object pose.
We use 2 GPUs for rendering and 1 GPU for running the network and training.




Additional training details are available in Appendix~\ref{app:vision_training} and randomization details are in Appendix~\ref{app:randomizations}.
