
As described in the previous section, our simulation is a coarse approximation of the real world.
We therefore face a dilemma: we cannot train on the physical robot because deep reinforcement learning algorithms require millions of samples;
conversely, training only in simulation results in policies that do no transfer well due to the gap between the simulated and real environments.
To overcome the reality gap, we modify the basic version of our simulation to a \emph{distribution over many simulations} that foster transfer~\citep{DBLP:conf/rss/SadeghiL17, tobin2017domain, peng2017sim}.
By carefully selecting the sensing modalities and by randomizing most aspects of our simulated environment we are able to train policies that are less likely to overfit to a specific simulated environment and more likely to transfer successfully to the physical robot.



\subsection{Observations}

We give the control policy observations of the fingertips using PhaseSpace markers and the object pose either from PhaseSpace markers or the vision based pose estimator.
Although the Shadow Dexterous Hand contains a broad array of built-in sensors, we specifically avoided providing these as observations to the policy because they are subject to state-dependent noise that would have been difficult to model in the simulator.
For example, the fingertip tactile sensor measures the pressure of a fluid stored in a balloon inside the fingertip, which correlates with the force applied to the fingertip but also with a number of confounding variables, including atmospheric pressure, temperature, and the shape of the contact and intersection geometry.
Although it is straightforward to determine the existence of contacts in the simulator, it would be difficult to model the distribution of sensor values.
Similar considerations apply to the joint angles measured by Hall effect sensors, which are used by the low-level controllers but not provided to the policy due to their tendency to be noisy and hard to calibrate.

\subsection{Randomizations}
\label{subsection:randomizations}


Following previous work on \emph{domain randomization}~\citep{DBLP:conf/rss/SadeghiL17, tobin2017domain, peng2017sim},
we randomize most of the aspects of the simulated environment in order to learn both a policy and a vision model that generalizes to reality. We briefly detail the types of randomizations below, and Appendix~\ref{app:randomizations} contains a more detailed discussion of the more involved randomizations and provides hyperparameters. % used for randomizations. %concrete parameters of all parametric randomizations.

\paragraph{Observation noise.}
To better mimic the kind of noise we expect to experience in reality, we add Gaussian noise to policy observations.
In particular, we apply a correlated noise which is sampled once per episode
as well as an uncorrelated noise sampled at every timestep.

\paragraph{Physics randomizations.} Physical parameters like friction are randomized at the beginning of every episode and held fixed. Many parameters are centered on values found during model calibration in an effort to make the simulation distribution match reality more closely. \autoref{table:rand-physics} lists all physics parameters that are randomized.

\begin{table}
    \footnotesize
    \centering
    \caption{Ranges of physics parameter randomizations.}
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{@{}lll@{}}
        \toprule
        \textbf{Parameter} & \textbf{Scaling factor range} & \textbf{Additive term range} \\ \midrule
        object dimensions & $\mbox{uniform}([0.95,1.05])$ & \\
        object and robot link masses & $\mbox{uniform}([0.5,1.5])$ & \\
        surface friction coefficients & $\mbox{uniform}([0.7,1.3])$ & \\
        robot joint damping coefficients & $\mbox{loguniform}([0.3,3.0])$ & \\
        actuator force gains (P term) & $\mbox{loguniform}([0.75,1.5])$ & \\ \hline
        joint limits & & $\mathcal{N}(0,0.15)~\si{\radian}$  \\
        gravity vector (each coordinate) && $\mathcal{N}(0,0.4)~\si{\m\per\s^2}$ \\ %\hline
        \bottomrule
    \end{tabular}
\label{table:rand-physics}
\end{table}


\paragraph{Unmodeled effects.}
The physical robot experiences many effects that are not modeled by our simulation.
To account for imperfect actuation, we use a simple model of motor backlash and introduce action delays and action noise before applying them in simulation.
Our motion capture setup sometimes loses track of a marker temporarily, which we model by freezing the position of a simulated marker with low probability for a short period of time in simulation.
We also simulate marker occlusion by freezing its simulated position whenever
it is close to another marker or the object.
To handle additional unmodeled dynamics, we apply small random forces to the object.
Details on the concrete implementation are available in Appendix~\ref{app:randomizations}.

\paragraph{Visual appearance randomizations.}
 We randomize the following aspects of the rendered scene:
camera positions and intrinsics, lighting conditions, the pose of the hand and object, and the materials and textures for all objects in the scene.
\autoref{fig:random_img} depicts some examples of these randomized environments.
Details on the randomized properties and their ranges are available in Appendix~\ref{app:randomizations}.

\begin{figure}[h]
    \begin{center}
    \includegraphics[width=\textwidth]{figures/randomized_hands}
    \caption{Simulations with different randomized visual appearances. Rows correspond to the renderings from the same camera, and columns correspond to renderings from 3 separate cameras which are simultaneously fed into the neural network.}
    \label{fig:random_img}    
    \end{center}
\end{figure}