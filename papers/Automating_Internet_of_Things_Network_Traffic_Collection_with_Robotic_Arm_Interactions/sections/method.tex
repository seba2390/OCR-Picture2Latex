\section{Method}
\label{sec:method}

Our approach for generating automated IoT device traffic has two main components: 1) Configuring a robotic arm to rigorously test user interface interactions with IoT devices, and 2) Automating network traffic collection during robotic arm interactions via existing IoT traffic analysis tools~\cite{iot_inspector}.

Implementing this approach involves four primary challenges: 1) Efficiently obtaining correct input parameters for the robotic arm such that it interacts with desired user interface elements on the IoT device, 2) Ensuring interaction accuracy between the robotic arm and the IoT device, 3) Designing interaction sequences that thoroughly explore the space of possible device behaviors, and 4) Automating network traffic collection during robot/device interaction. 

This section describes our implementation, including how we addressed these challenges for the Amazon Echo Show 5 and Sensi Wi-Fi Smart Thermostat devices. Our source code is publicly available for use in future research.

\subsection{Experiment Setup}
\label{sec:experiment-setup}
\subsubsection{Robotic Arm} The main robotic arm used in this study is the Arduino Braccio Robotic Arm~\cite{braccio} with 6 degrees of freedom (DoF) and motors/servos connected to the Braccio shield. The robotic arm is fixed to the work station as illustrated in Figure~\ref{fig:set_up} and is controlled via an Arduino UNO R3 board~\cite{r3}. 
When given correct rotation and movement delay parameters, the robotic arm can press buttons and interact with other interface elements on an IoT device fixed within the arm's maximum reach.

We chose this particular model of robotic arm for two main reasons: 1) Unlike industrial robotic arms which often sell for tens of thousands of dollars, the Arduino Braccio Robotic Arm retails for less than \$250. This keeps our method accessible for IoT researchers with limited budgets, 2) Compared to other robotic arms in the same price range, this model has a relatively high precision ($\pm2$mm), which is desirable for interacting with devices with small buttons.


\subsubsection{IoT Devices}The two devices tested in this study are the \textbf{Amazon Echo Show 5 (Echo-Show5)}~\cite{echo} and the \textbf{Emerson Sensi Wi-Fi Smart Thermostat (Sensi-Thermostat)}~\cite{sensi}. Echo-Show5 is an Amazon smart display capable of video and voice calls, content streaming, online shopping, and running third-party applications. The device is equipped with Amazon's Alexa~\cite{alexa} voice assistant, a touch screen, as well as four buttons on the top of the device. The four buttons include a mechanical switch for the camera shutter, a microphone mute button, and two buttons responsible for volume up/down. Sensi-Thermostat is a smart Wi-Fi thermostat that is designed to control conventional household heating/cooling. While it can be connected to a user's smartphone for remote control, there are also six physical buttons on the device that can be used to raise/lower temperature, turn on/off the fan, and change the heating/cooling mode and schedule.

We chose these specific IoT devices based on two specific criteria:

\paragraph{Market representativeness} While there are numerous IoT devices on the market, we wanted to select representative devices that are widely deployed to evaluate the practicality
and scalability of our automated data collection method. We chose the Echo-Show5 because it is representative of the IoT smart speaker/display market (Amazon is a major IoT manufacturer with over 51\% of the U.S. smart speaker market share as of 2020~\cite{amazon_data}). Likewise, the Sensi-Thermostat is a popular IoT device with $>$16,000 Amazon reviews circa January 2022, making it one of the most widely deployed IoT smart thermostats on the market. Although there are many categories of IoT devices that we are not testing in this study, our methodology is highly transferable
across different devices as long as physical user interface (UI) elements are present. We also believe that the approach can be adapted to additional devices with
a wide variety of user interfaces (Section \ref{sec:limit}).

\paragraph{Network behavior complexity} It is essential for us to verify that our proposed method is effective across devices with different levels of network behavior complexity. 
The Echo-Show5 is a comparatively sophisticated device that offers a significantly wider variety of functionalities than single-purpose IoT devices such as smart light bulbs or outlets, allowing us to evaluate our approach on a device with complex network behaviors. On the other hand, the Sensi-Thermostat is a single-purpose device, making it ideal for validating our automated data collection approach for IoT products with simpler network behavior.

\subsubsection{Network Traffic Collection} 
\label{sec:network-collection}
To capture network traffic to and from the devices, we set up a Raspberry Pi Wi-Fi access point using existing ``IoT Inspector'' software~\cite{iot_inspector}. This configures the Raspberry Pi such that all traffic through the Wi-Fi network is captured and stored locally as PCAP files. Upon setting up the robotic arm, IoT device, and Raspberry Pi access point, we instruct the arm to interact with the device as described in the following section. These interactions cause the device to generate bidirectional network traffic that is recorded for analysis.
Since the recorded PCAP files contain the source and destination addresses of captured network packets, we can use tools such as Wireshark~\cite{wireshark} to separate packets generated specifically by the IoT device from background traffic.
To test this approach with the Echo-Show5 and Sensi-Thermostat, we filter the captured traffic to include only 
Transmission Control Protocol (TCP) packets sent to or from these devices' Ethernet MAC addresses. We also include background TCP traffic collected during periods without device interactions (idle time) to help evaluate the effectiveness of our approach.

\begin{figure}
    \centering
    \includegraphics[width=8.5cm]{figures/set_up.png}
    \caption{Sample hardware setup with Arduino Braccio Robotic Arm \textit{(left)} and IoT device \textit{(right)} fixed to the table.}
    \label{fig:set_up}
\end{figure}

\subsection{Inverse Kinematics for Robotic Arm Input Parameters}
\label{sec:inverse-kinematics}
Once the robotic arm and IoT devices are fixed in place, we calculate the arm joint rotation parameters needed for the arm to reach the buttons on the device. 
For each button, we manually measure the base rotation needed to align the arm with the button followed by the button's 2D vertical coordinates with respect to the base of the arm. 

We then use the inverse kinematics scripts from~\cite{invkin}, which take the lengths of each section of the arm and each button's 2D coordinates and output the needed arm rotation parameters. 
These scripts only apply to robotic arms with 3 degrees of freedom (DoF), so we treat the 6 DoF Arduino Braccio Robotic Arm
as having 4 DoF: one base joint that can rotate horizontally and three arm joints that can rotate vertically.

\subsection{Ensuring Interaction Accuracy}
The buttons on most IoT devices are designed for precise interactions with human fingers and are often relatively small (usually less than 1 cm$^2$). This can be problematic, as the robotic arm can experience small deviations ($\pm$2 mm) during every movement, causing missed presses or unintended presses of adjacent buttons if not corrected.

In our specific case, we address this problem by physically enlarging the buttons' surface areas to overcome the possible deviations from each arm rotation. This is achieved by attaching additional hard surfaces on top of the original buttons (Figure~\ref{fig:buttons}). Given the larger surface areas, such rotation biases become insignificant, and missed presses are avoided.
Our solution is viable because the robotic arm used in the experiment shows randomized movement biases and does not introduce significant accumulated misalignment over the course of long experiments.
However, in scenarios where such deviations indeed accumulate, there are existing tools~\cite{vision-based-robotic-grasping,mariottini2011active,kumar2018computer} that rely on computer vision to perform automated correction and can be applied to address the issue.

\begin{figure}
    \centering
    \includegraphics[width=8.5cm]{figures/buttons.png}
    \caption{IoT device (Sensi-Thermostat) with physically enlarged buttons to eliminate robotic arm misalignment errors.}
    \label{fig:buttons}
\end{figure}

While we address the movement bias, it is worth noting that the robotic arm also experiences delays in executing movement commands: when a movement is requested via the Arduino IDE command execution, a very small delay occurs before the robotic arm motors actually move to adjust the arm to the desired position. However, this delay in execution time is on the order of milliseconds and, unlike the positional bias, it is fixed and constant for each movement. This means that the time delay between button press commands and actual button presses is predictable, does not accumulate over multiple commands, and is insignificant compared to the timescale of the entire UI interaction sequences. 

\subsection{Designing Interaction Sequences}
\label{sec:interaction-seqs}
Our system instructs the robotic arm to move to specific locations by providing a set of arm rotations as the input parameters (Section~\ref{sec:inverse-kinematics}). Given the limited number of unique physical buttons on each IoT device, we can configure the robotic arm to iterate through buttons in a specific order, or ``interaction sequence,'' by feeding in consecutive sets of arm rotations.

Our system performs comprehensive device interaction testing through permutation-based interaction sequences. We assign each button on the device a unique number, and then the system instructs the robotic arm to press the buttons in all possible permutations of these numbers. 
By default, the system uses a fixed 10-second delay between button presses, but this and other details can be easily customized. For example, a researcher could choose to use permutations with random repetitions, so the robot will press individual buttons multiple times per interaction sequence. A researcher could also randomize the time between button presses to simulate unpredictable user behavior (Section~\ref{sec:future}). 

\subsection{Demonstrating Effectiveness with Machine Learning}
\label{sec:mlmethod}

We evaluate the effectiveness of our approach by verifying that the button presses performed by the robotic arm actually lead to the collection of useful network traffic data. We do this by demonstrating that we can train a machine learning (ML) model to predict which buttons were pressed on the device from the network traffic alone. This allows us to conclude that the captured traffic provides substantial information about interactions with the IoT device and is therefore relevant for follow-up network, security, or privacy analysis. This machine learning task is inspired by the various metadata-based inference attacks in the consumer IoT privacy literature~\cite{apthorpe2019keeping, acar2020peek, trimananda2020packet}, in which network traffic is shown to reveal substantial information about user interactions with their IoT devices.

It is important to emphasize that the design of the ML algorithm is \textit{not} the primary contribution of this study. We use ML simply as a method to verify that our robotic arm interaction approach generates useful IoT traffic. We expect that this automated method will be effective for many research purposes given that it allows rigorous device testing without tedious manual effort. 

We perform the ML evaluation as follows: 1) Label recorded IoT device packets (training data) with the buttons pressed by the robotic arm immediately prior to collection, 2) Train a supervised machine learning model to predict which button presses caused the device to send specific packets in reserved test data.

\subsubsection{Training Data}
We collect training data by instructing the robotic arm to repeatedly press each button on the device while recording the timestamp of each button press. For example, the arm performs 15 presses of button \#1 followed by 15 presses of button \#2, etc. The delay between each button press is a fixed 10 seconds. 

We record the device's network traffic during these interaction sequences as PCAP files as described in Section~\ref{sec:experiment-setup}.
We convert these PCAP files into a standardized CSV format using the nPrint tool~\cite{holland2020nprint}. nPrint encodes each packet as a row in the CSV file with columns containing the following features: packet's source IP address, destination IP address, payload, IPv4 headers, TCP headers, and relative timestamps. We then add a label column with the integer identifier of the button pressed immediately prior to the corresponding packet as determined by comparing packet timestamps and button press timestamps. A single button press can cause network activity consisting of multiple packets, so for each button press with timestamp $t_{button}$, we treat this button as the label for all packets with timestamps $t_{packet}$ within the following range: 
\[t_{button} \leq t_{packet} < (t_{button} + 10s)\] 
This labeling is reasonable because all associated packets are transmitted a few seconds after a button is pressed---well within the 10 second gap between consecutive button presses. 

\subsubsection{Random Forest Classifier}
We train a random forest classifier~\cite{ho1995random} using the scikit-learn library~\cite{scikit-learn} and our collected training data. 
The classifier is designed to predict which button the robot pressed given only the subsequent network packet data. We select this particular ML algorithm in our study because the effectiveness of using random forest classifiers for characterizing network traffic has long been established~\cite{wang2015network,zhang2014robust,jun2007internet}.

We randomly select 25\% of the labeled data to withhold as a test set and use the remaining 75\% as a training set.
We then perform a grid search to choose model hyperparameters evaluated by multi-metric (accuracy, precision, recall) 10-fold cross-validation over the training set. The grid search considers the criterion function (metric for measuring the quality of a split when constructing a decision tree in the random forest), maximum depth of the decision trees in the random forest, minimum number of samples required to split an internal node when constructing each decision tree, and the total number of trees in the random forest.
Specific hyperparameter values tested and selected by the grid search are shown in Table~\ref{tab:grid_parm}.

\begin{table}[t]
\centering\footnotesize
\begin{tabular}{lllll}
\toprule
\textbf{Hyperparameter}          & \textbf{Value 1}  & \textbf{Value 2}   &\textbf{ Value 3}  & \textbf{Selected}       \\ \midrule
\textbf{criterion}          & gini  & entropy   & & entropy        \\
\textbf{max\_depth}         & 20     & 40          & 80  & 40      \\ 
\textbf{min\_sample\_split} & 2      & 5           & 10  & 2      \\ 
\textbf{n\_estimators}      & 200    & 400         & 800 & 800      \\ \bottomrule
\end{tabular}
\caption{Grid search values for random forest hyperparameters and selected options for final model}.
\label{tab:grid_parm}
\end{table}                                                                         

Using the best hyperparameters found by the grid search, we train a classifier on the entire training set and evaluate its precision, recall, and $F_1$ score on the test set. 
To obtain more reliable results, we repeat the above procedures (randomized train/test split, grid search, and classifier testing) 50 times and compute the means and variances of the test scores.
High scores indicate that the classifier can confidently associate recorded packets with the button presses that generated those packets,
implying that the collected network data captures relevant information about user interactions and device behaviors. 