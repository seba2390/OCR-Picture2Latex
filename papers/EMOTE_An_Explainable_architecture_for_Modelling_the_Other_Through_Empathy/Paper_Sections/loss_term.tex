%\thommen{I think most of this first para can be removed.} \manisha{Not sure what should be removed...} The loss $\mathcal{L}$ used to train the EMOTE action value function comprises the convex combination of two loss terms (Equation \ref{eqn:Loss_Function}). It is important to note that Only the weights of the Imagination Network $M_{imagine}$ are tuned via the loss. As the second model corresponds to an identical copy of the learning agent's value function $Q_{learn}$,  these weights will be trained using DQN.

While the weights of $Q_{learn}$ are trained using DQN, we train $M_{imagine}$ using a loss term (Equation \ref{eqn:Loss_Function}) comprising two parts. The first ($Loss_{1}$) is a categorical cross entropy (CE) loss which minimises the difference between the softmax predicted action from the $Q_{learn}$ copy and the action ($a_{i}$) actually taken by the independent agent. %focuses on the accuracy of the softmax predicted action from the $Q_{learn}$ copy compared to the action actually taken by the independent agent ($a_{i}$). 
 %This is computed via categorical cross entropy (CE) is used for this term. 
This loss is designed to ensure the greedy action of $Q_{learn}$, for $s_{e}$, matches the independent agent's action (per Equation \ref{eqn:IM_formula}).

\begin{equation}
\label{eqn:Loss_Function}
\begin{aligned}\mathcal{L}(\theta_{imagine}) = (1-\delta)\underbrace{CE(\mathbbm{1}.a_{i},\pi_{learn}(a|s_{e}))}_{Loss_{1}}
+ \delta\underbrace{\|{s_{i}-s_{e}}\|_{1}}_{Loss_{2}}
\end{aligned}
\end{equation}
where
\begin{equation}
\pi_{learn}(a|s_{e})=\frac{e^{Q_{learn}(s_{e},a)}}{\sum_{a}e^{Q_{learn}(s_{e},a)}}
\end{equation}
and $s_{e}$ is obtained as per Equation \ref{eqn:IM_formula}.

The second loss term ($Loss_{2}$) focuses on state reconstruction, aiming to produce an empathetic state $s_{e}$ matching the original state $s_{i}$. Together, the goal is to produce an empathetic state $s_{e}$ through minimal changes to $s_{i}$, so that common features (e.g. walls or floors) are reconstructed, and differences between $s_{e}$ and $s_{i}$ reflect the analogous features  
%\st{reflective of those} 
needed to evoke empathy (driven by $Loss_{1}$). 
%The intention behind this is interpretability such that when $s^{e}$ is observed, it is representative of $s$. When used in combination with $Loss_{1}$ the objective of the loss term is to recreate $s^{e}$ to be as similar to $s$ (via $Loss_{2}$) as possible with any differences reflective of those changes which are needed to induce empathy (driven by $Loss_{1}$). In the example in Figure \ref{fig:emp_example} $Loss_{2}$ ensures common features such as floors and walls remain the same while $Loss_{1}$ drives the required change of swapping the pellet colour. 
Components used to construct the two loss terms are shown in Figure \ref{fig:empathetic_valuefunction}.

%The intuition is to induce the interpretability of the empathetic state for a user to peer into and gain insights into what the learning agent believes it has in common with the independent agent. 
The user specified hyperparameter $\delta \in [0,1]$ balances the importance of reconstructing the empathetic state $s_{e}$ to be as similar as possible to the original state $s_{i}$ (interpretability) and the accuracy of the learning agent's predicted actions in $s_e$. We note that in practice, a copy of $Q_{learn}$ for EMOTE is updated every few episodes (to maintain stability). %More discussion of this is included in Section \ref{sec:delta_hyper}





