In this work, we presented the EMOTE architecture, which enables a learning agent to model the action-value function of an independent agent, based on its own value function and rewards, under the assumption that agents have ``analagous'' features. A key benefit is the ability to generate an interpretable empathetic states, allowing identification of analogous features between agents. 
%\st{This in turn aids the interpretation of rewards inferred for the independent agent and we contend that it promotes reassurance, helping to reinforce a human's trust in the learning agent.} 
%\thommen{saying that we assume they have analogous goals is more accurate, right? Agents could be analogous in many ways}. 
The architecture is well suited to multiagent learning algorithms, particularly those utilising composite action-value or reward functions in their design. % as our method produces function values at a similar scale between agents. 
%By examining this empathetic state, a human is able to gain insight into the features and goals that are similar or different between the agents, and from this explain the reward values associated for various events. We demonstrated our method by integrating it into the IRL component of a pre-existing Sympathy Framework. 
EMOTE was demonstrated on a previously proposed Sympathy Framework, where it produced more considerate behaviours, more consistent rewards (despite re-configurations of the environment), and also produced insightful empathetic states. We discussed the design of our loss function, and provided insights into future research directions to improve our proposed approach.

%We believe that the proposed approach provides a means of leveraging available information from the learning agent, to inform and assist in modelling another agent. As transparency is a desirable feature of model machine learning methods, the ability to visualise an empathetic state that is interpretable by a human will also be an advantage of such a method.
