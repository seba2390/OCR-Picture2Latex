\subsection{The games}

\paragraph{Assistive 1 and 2}
Both agents try to collect their corresponding coloured pellets. When the learning agent consumes one of its pellets, it receives +10 points. In Assistive 1 the independent agent is locked behind a door and in Assistive 2 one of the independent agent's pellets is locked behind the door. The door can only be opened by the learning agent stepping on the green button, which inflicts a small negative reward (-1) on the learning agent. Once the button is pressed, the door remains open for the remainder of the episode. The learning agent also receives an additional bonus reward (+5) when it `wins' the game by consuming all of its pellets.
%does not need to assist the independent agent in order to win the game (+5). 
In the absence of other rewards a step penalty (-1) applies for each step taken. Pellets are placed randomly in each episode and the episode ends if all learning-agent pellets are consumed or when the game timer runs out.

Ideally, a considerate learning agent would open the door and assist the independent agent, despite not being necessary for winning. These games are considered to have a moderate degree of analogy, as both agents react similarly to pellets, but differ as only the learning agent can open the door.
%, and even incurring a small negative reward for doing so. %We note that in all experiments, the independent agent's behaviour is simulated using a pretrained, fixed $\epsilon$-greedy policy.

\paragraph{Adversarial 1 and 2}
In Adversarial 1 both agents again try to collect their respective pellets, and the learning agent earns +20 points per pellet. In Adversarial 2 only the learning agent is concerned with pellet collection.  When the game starts the independent agent can harm the learning agent (resulting in -50 points for the latter and the game ending). If the learning agent steps on the green button however (switching the button status from 0 to 1), for a finite period of time, it can harm the independent agent, resulting in a positive reward (+10) for the learning agent. Harming occurs when the harming agent is within 1 square from the other agent. We investigate whether the learning agent, by virtue of the empathetic architecture, avoids harming the independent agent despite receiving positive environment rewards for doing so. Each episode terminates when the learning agent collects all pellets (receiving +30 reward), if the game timer runs out, or the learning agent is harmed. Adversarial 1 has a high degree of analogy as both agents have comparable reactions to elements (pellets and harming), whilst Adversarial 2 has a lower degree, as the only analogous feature is the ability to harm each other.

%In this environment, as both the vision field and status of the button are relevant to decision-making, we performed two variants of experiments- 

%The first had the button status manually switched (Emp(Feature)-B and Emp(Image)-B) (and the vision field trained). In this run, we assume that how the learning agent feels when the button status is 0, is how the independent agent feels when the button status is 1 (and vice-versa). In the second set of runs we trained both the vision field and a model to predict an empathetic button status value (Emp(Feature) and Emp(Image)). A time penalty of -1 for each step is applied.