\thommen{Is this subsection needed? Can it be fit into the text surrounding Eq 2? Many points here are repeated there anyway.}In the Sympathy Framework paper, the authors utilised the Cascaded Supervised Learning method \cite{cascadedSuperIRL2013} to conduct the IRL to infer the value function and rewards for the independent agent. The method involves two stages. In the first, a supervised learning approach is applied using observed state-action pairs of the independent agent to train a value function. In the second stage, this value function is used within a rearranged form of the Bellman equation \cite{sutton2018reinforcement} to extract a form of the inferred reward function for the independent agent. This method has been shown to be a successful means of extracting a reward function that produces a policy that mimics the independent agent's behaviour. However, the method still suffers a key issue faced by most IRL methods - that there exists a vast space of potential reward functions that when rolled out, could express the observed behaviour of an agent \cite{PolicyInvariance}. 
Under the assumption that both agents are similar (in terms of goals and underlying reward values) our proposed method is to change the value function of the method to our empathy-based architecture. In this way, the resulting value function and rewards will be more comparable and consistent between both agents thereby addressing some of the shortcomings of the Sympathy Framework method, as was discussed in the Related Literature section.  

%In the Sympathy Framework paper, in order to constrain the space of rewards to those which are appropriate for comparing against the rewards of the learning agent (an important consideration when producing the sympathetic rewards to train the learning agent), the authors scale the inferred rewards of the independent agent to have the same $L_{1}$ norm as that of the learning agent's reward function. This however still does not guarantee that the reward inferred is in fact appropriately comparable to that of the learning agent. In addition, it requires that the the learning agent's reward function be known before hand fully, which may not always be the case (such as cases where the reward values only exposed to the agent as it interacts with the environment).\thommen{Again, its okay to contrast the work with the sympathy paper, but maybe it should be reserved for the related works/discussion sections. I feel like the focus is being lost.}
