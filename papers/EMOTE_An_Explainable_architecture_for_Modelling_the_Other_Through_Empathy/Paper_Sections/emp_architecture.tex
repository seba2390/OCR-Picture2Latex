%Sitting within the IRL module as seen in Figure \ref{fig:framework} the form of the independent agents empathy-based architecture value function is shown in more detail in Figure \ref{fig:empathetic_valuefunction}.
The components of the EMOTE architecture used to estimate the independent agent's value function $\hat{Q}_{indep}$ are shown in Figure \ref{fig:empathetic_valuefunction}. It consists of a two stage neural network architecture. The first of these, the Imagination Network ($M_{imagine}$) parameterised by $\theta_{imagine}$, takes state $s_{i}$, as observed by the independent agent, as input and outputs an empathetic state $s_{e}$ representing the independent agent's state as perceived empathetically by the learning agent. That is:
\begin{eqnarray}
s_{e} = M_{imagine}(s_{i};\theta_{imagine})
\label{eqn:IM_formula}
\end{eqnarray} 
Such that the learning agent's greedy action in $s_e$ matches the independent agent's action $a_i$. Formally, we define the empathetic state in Definition \ref{def:empstate}:
\begin{definition}[Empathetic State]
\label{def:empstate}
In a multiagent learning scenario involving a learning agent with action-value function $Q_{learn}$ and an independent agent (sharing same state space $\mathcal{S}$ and action space $\mathcal{A}$) who behaves per an arbitrary unknown policy, an empathetic state $s_e$ is a state where the learning agent's greedy action matches the independent agent's observed action $a_i$ in state $s_{i}$.

\begin{eqnarray*}
 \underset{a'}{argmax}{ 
 \; Q_{learn}(s_{e},a')} = a_{i}
\label{eqn:IM_condition}
\end{eqnarray*}
\end{definition}
%We aim to learn the transformation function $M_{imagine}$, where:
%if it were inputted into the learning agent's action-value function $Q_{learn}$, it would elicit a greedy action that matches the independent agent's action $a_{i}$ in state $s_{i}$. As such, we aim to learn the state transformation function $M_{imagine}$, where:

Our work is applicable when learning and independent agents share analogous features, and a sufficient degree of analogy exists. These are defined:
\begin{definition}[Analogous features]
 %\manisha{Are} specific features of the empathetic state that \manisha{when mapped and swapped with another feature,} induce the learning agent to mimic the independent agent's actions.
    A subset of features $f\subseteq \mathcal{F}$ contained in state $s\in\mathcal{S}$ is said to be analogous if there exists another subset of features $f_{e}\subseteq \mathcal{F}$, which would produce an empathetic state $s_e$ as defined in Definition \ref{def:empstate}, when $f$ is swapped with $f_e$, where $\mathcal{F}$ is the feature space.
\end{definition}
\begin{definition}[Degree of Analogy]
    The degree of analogy between two agents is the fraction of analogous features in the feature space.%\manisha{fraction of features which are analogous} \st{total number of analogous features} in the state space $\mathcal{S}$.
\end{definition}
%\thommen{Do analogous features by definition occur in pairs? Also, what about groups of features?}\manisha{not necessarily... there could be one to many mappings I guess. e.g. LP to IP or Floor}\thommen{Yes, but still pairs right? Eg: (LP,IP), (LP,Floor). I mean a feature cant be analogous without another feature. Might be worth writing it as analogous feature pair wherever appropriate. Also, I mentioned groups of features because it may be possible for a feature to not be analogous unless there are other features that are also changed along with that feature. Eg: key may not be analogous to access card by itself, but it will be analogous when the lock/access card reader features are also swapped. Maybe writing something about the limitation of this definition will be useful. Something like `Although Def 2 only considers single features, we acknowledge that feature groups can also be analogous...}
\paragraph{An Illustrative Example:} Figure \ref{fig:emp_example} shows an example where Agent 1 desires green pellets while Agent 2 desires red pellets. If Agent 2 observes state $s_{i}$, it would move left in order to obtain the red pellet. If Agent 1 were to observe state $s_{i}$ from Agent 2's position, it would not move left, but rather down, towards the green pellet. If however state $s_{i}$, is fed into a trained Imagination Network, the resulting empathetic state $s_{e}$ would see the position of the pellets (analogous features) be swapped, as depicted on the right side of Figure \ref{fig:emp_example}. As a result, when Agent 1 is presented the transformed state $s_{e}$ (instead of $s_{i}$), it would select the same action (moving left) as Agent 2 in state $s_{i}$. Hence, we can interpret that through $s_{e}$, Agent 1 understands that how it behaves towards and values green pellets is analogous to how Agent 2 behaves towards and values red pellets. 

\begin{figure}[ht]
\vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=0.5\columnwidth]{Example_figure}}
\caption{Example: Imagination Network. $s_{i}$: perceived by Agent 2. $s_e$: Agent 1's empathetic perception of $s_{i}$.}
\label{fig:emp_example}
\end{center}
\vskip -0.3in
\end{figure}

%\thommen{My take on the intuition - can you check this? I feel like it is a better alternative to the first part of the next para. \emph{In order to estimate the independent agent's action values, we pass the obtained empathetic state $s_e$ through a copy of the learning agent's own action-value function $Q_{selfish}$. The intuition is that since $s_e$ for the learning agent is analogous to the independent agent's state $s_i$, it is reasonable to assume that for a given action, how the learning agent values $s_e$ is analogous to how the independent agent values $s_i$. Hence, passing $s_e$ through the learning agent's action-value function $Q_{selfish}$ produces analogous action-values for the independent agent, as seen from the learning agent's point of view. The resulting action-values then correspond to the learning agent's interpretation of the independent agent's action-values, based on its own action-values (and hence, its own experiences and environment interactions).}}

In order to estimate the independent agent's action values, we pass the obtained empathetic state $s_e$ through a copy of the learning agent's own action-value function $Q_{learn}$. The intuition is that since $s_e$ for the learning agent produces the same actions (behaviours) as the independent agent in state $s_i$, it is reasonable to assume that for a given action, how the independent agent values $s_i$ is analogous to how the learning agent values $s_e$. Hence, passing $s_e$ through the learning agent's action-value function $Q_{learn}$ produces action-values analogous to the independent agent's. The resulting action-values then correspond to the learning agent's interpretation of the independent agent's action-values, based on its own action-values (and hence, its own experiences and environment interactions).
%The second model in the architecture is simply a copy of the learning agent's action-value function $Q_{selfish}$. $s_{e}$ is fed into this model to obtain the corresponding Q-values for each possible action. These Q-values are then representative of the learning agent's estimate of the long term expected rewards from each action for the independent agent, generated by using its own reward and value function as a guide. %The intuition behind this is that
%, firstly, $Q_{selfish}$ is trained directly on rewards returned from the environment via the learning agent's trajectories.  By 
%The intuition behind this is that by querying the value associated with the empathetic state and the independent agent's observed action as per the learning agent's action value estimate $Q_{selfish}$, we elicit an associated Q-value response to the question, \textit{``What value does the learning agent estimate for the action taken by the independent agent if the former is empathising with the latter"}. 
In our previous example (Figure \ref{fig:emp_example}), via the empathetic state which swapped the colours of the pellets, the resulting Q-value for the action taken by Agent 2 (going left towards red) is similar to that of Agent 1 taking the same action in the empathetic state (going left towards the imagined green pellet). 

Having now obtained $\hat{Q}_{indep}$, we can infer the rewards of the independent agent $\hat{R}_{indep}$ through an existing IRL method, e.g. Cascaded Supervised Learning \cite{cascadedSuperIRL2013}. In this way $\hat{R}_{indep}$ will have a similar scale or magnitude as that of the learning agent's rewards $R$. This is particularly useful for MARL algorithms which make use of a composite value or reward function in their design.

%In the Sympathy Framework paper, the authors utilised the Cascaded Supervised Learning method \cite{cascadedSuperIRL2013} to conduct the IRL. The method involves two stages. In the first, a supervised learning approach is applied using observed state-action pairs of the independent agent to train $\hat{Q}_{indep}$. In the second stage, $\hat{Q}_{indep}$ is used within a rearranged form of the Bellman equation \cite{sutton2018reinforcement} to extract $\hat{R}_{indep}$. In our work, we do the same, but instead use our proposed empathy-based architecture for $\hat{Q}_{indep}$. The purpose of inferring $\hat{R}_{indep}$ is to learn a sympathetic reward function $R_{symp}$ (as in Equation \eqref{eqn:R_symp}) based on which a sympathetic Q-function $Q_{symp}$ and corresponding sympathetic policy is learnt, as described in \cite{SympathyPaper}: 

\iffalse
\begin{eqnarray}
R_{symp}= \beta R + (1-\beta) \hat{R}_{indep}
\label{eqn:R_symp}
\end{eqnarray}
\fi

%As the sympathetic reward $R_{symp}$ is a convex combination of the reward $R$ of the learning agent and estimated reward $\hat{R}_{indep}$ of the independent agent , ensuring the same scale of these rewards is crucial to ensure that they are balanced fairly through $\beta$. %Further details on the Sympathy Framework can be found in the original paper \cite{SympathyPaper}.

%In our work, as $\hat{R}_{indep}$ is learnt based on an empathetic transformation of the learning agent's Q function, the corresponding inferred rewards $\hat{R}_{indep}$ would also lie in the same scale as the learning agent's reward function $R$. This leads to a grounded estimate of the sympathetic reward $R_{symp}$, without the need for arbitrary reward scaling, as done in \cite{SympathyPaper}.     


%Importantly, as $Q_{selfish}$ has been trained on the learning agent's rewards obtained by interacting with the environment, the Q-values it outputs, and subsequently inferred rewards will produce reward values that are highly comparable to that of the learning agent. When the rewards of both agents are combined to create the sympathetic reward $R_{symp}$ on which $Q_{symp}$ for the learning agent is trained, no further reward shaping is required. The benefit of this is seen when observing the form of $R_{symp}$ as shown in Equation \ref{eqn:R_symp}. 


%\thommen{The actual agent training is not clear. Does it actually use $R_{symp}$? It will be much clearer if you can write down the update equations and specify how exactly the empathetic part is used. You could do this in a separate subsection if that's better.}\manisha{Yes the 'sympathetic' agent $Q_{symp}$ is trained on $R_{symp}$. Actions are taken according to $Q_{symp}$. But $Q_{selfish}$ is trained at the same time though. I didn't write too much about this because (1) it was discussed in the sympathy paper, and (2) I had hoped the Figure 1 would make this clear?. Added a final line above}