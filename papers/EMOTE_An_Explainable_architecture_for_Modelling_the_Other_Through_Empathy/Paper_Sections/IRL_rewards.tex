\begin{figure*}[ht]
\vskip -0.2in
\begin{center}
\centerline{\begin{subfigure}[b]{0.22\textwidth}
         \centering
         \includegraphics[width=\textwidth]{9c_GridworldAss1}
         \caption{Assistive 1}
     \end{subfigure}
     \begin{subfigure}[b]{0.22\textwidth}
         \centering
         \includegraphics[width=\textwidth]{9c_GridworldAss2}
         \caption{Assistive 2}
     \end{subfigure}
     \begin{subfigure}[b]{0.215\textwidth}
         \centering
         \includegraphics[width=\textwidth]{9c_Adversarial1}
         \caption{Adversarial 1}
     \end{subfigure}
     \begin{subfigure}[b]{0.31\textwidth}
         \centering
         \includegraphics[width=\textwidth]{9c_Adversarial2}
         \caption{Adversarial 2}
     \end{subfigure}}
\caption{Independent Agent's (IA) $\hat{R}_{indep}$ (averaged over last 100 episodes) derived from $\hat{Q}_{indep}$. Learning Agent's (LA) environment rewards are included for reference. \emph{Sympathy} rewards are scaled to have the same $l1$ norm as the LA's rewards.}
\label{fig:IRL_results}
\end{center}
\vskip -0.3in
\end{figure*}

Figure \ref{fig:IRL_results} shows the independent agent's inferred rewards ($\hat{R}_{indep}$) from the EMOTE and \emph{Benchmark} action-value functions ($\hat{Q}_{indep}$), and contrasts them against those from \emph{Sympathy}. For reference the learning agent's environmental rewards for the same features are shown alongside the independent agent's $\hat{R}_{indep}$.
% We examine the independent agent's inferred rewards ($\hat{R}_{indep}$) from EMOTE and \emph{Benchmarks}, and contrast against those from \emph{Sympathy}, shown in Figure \ref{fig:IRL_results} (averaged over last 100 episodes). For reference the learning agent's environmental rewards for the same reward features are shown alongside the inferred rewards for the independent agent.
% , where the learning agent's reward for ``LA Pellet'' should, for instance, be comparable to the independent agent's reward for ``IA Pellet''.
% To assist with assessing comparability of the inferred rewards, the learning agent's environmental rewards are also included.

\subsubsection{Assistive}
To judge whether the inferred independent-agent's rewards are comparable to the learning agent's environmental rewards we can examine similarities in reward values across various features. 
%\emph{we need to look from each agent's perspective.} 
For instance one would expect the baselines' inference for the independent agent consuming its pellet (IA pellet) to look similar to the learning agent's environmental reward for consuming its own pellet (LA pellet). 
In Figure \ref{fig:IRL_results} (a) and (b) (Assistive 1 and 2) \emph{E-Feature}, \emph{E-Image}, and the \emph{Benchmark} baselines all infer positive rewards for IA pellet, though magnitudes vary. The rewards inferred by the \emph{Benchmarks} are closest to the learning agent's reward for LA pellet (+10).
%that the learning agent receives for consuming its own pellet (LA pellet). 
Opening the door elicited a slight positive value in the EMOTE runs, and the value for taking a step was close to that of the learning agent's step value (-1). Under the EMOTE and \emph{Benchmark} baselines, the rewards inferred for Assistive 1 and 2 are similar. %This highlights the potential of the architecture to learn a robust model demonstrated by consistent reward values even when the layout of the environment changes. 
This demonstrates the potential of the architecture to learn a robust model which infers consistent reward values even when the layout of the environment changes. 
In contrast, \emph{Sympathy} inferred different rewards for the two environments. It inferred a strong positive value for the button being pressed in Assistive 1, but a slight negative value in Assistive 2 (Figure \ref{fig:IRL_results} a) and b)). Additionally despite the high win rate in Assistive 2, the \emph{Sympathy} baseline did not result in as high of a door opening rate (Table \ref{table:win_harm_door}) as it wrongly inferred a negative reward for door opening. %\st{This is due to the unexpected negative value inferred for the door being opened.}\thommen{Isnt it understood that it is because of the negative value? To me, the takeaway, which should replace this sentence should be that the empathy baselines are able to exhibit more considerate behaviours.}\manisha{The considerate behaviours comment is more applicable in the 4.3 performance section? This section is just IRL} 
 All baselines inferred the independent agent associates a value close to 0 for the learning agent consuming a pellet.

\subsubsection{Adversarial}

For the adversarial games (Figure \ref{fig:IRL_results} (c) and (d)) \emph{E-Feature}, \emph{E-Image} and both \emph{Benchmarks} were able to capture the strong negative reward that the independent agent associates with being harmed. This is similar to the environmental reward the learning agent receives when it is killed. In Adversarial 1, a positive value was also associated with the independent agent consuming its own pellet. In Adversarial 1, the \emph{Sympathy} baseline was not able to infer a negative reward for the independent agent  being harmed. It also failed to capture a strong enough positive reward for the independent agent consuming a pellet and inferred a strong negative reward for taking a step and pressing the button, leading to a low win rate and high harm rate, similar to that of \emph{Selfish} (Table \ref{table:win_harm_door}). 
%As a result of this incorrect reward inference the win rate was adversely affected (Table \ref{table:win_harm_door})
We expect the poor performance is due to the order of magnitude difference, resulting from the $l1$ norm scaling, between rewards inferred for taking a step and button press relative to the other reward features. This inappropriate scaling of the reward components in turn results in poor estimates of the sympathetic reward.
%\manisha{When combined to construct the sympathetic rewards, the learning agent is biased to a false view of the independent agent.}
%, illustrating the fragility of using $l1$ norm scaling
%\thommen{As a result, the $l1$ norm scaling is affected, leading to an innacurate model of the independent agent.} %
 In Adversarial 2, the \emph{Sympathy} baseline inferred a strong negative for independent agent being harmed, leading to a reduced harm rate. Despite this \emph{Sympathy} had lower win rates compared to EMOTE or \emph{Benchmarks} (Table \ref{table:win_harm_door}).
%once again inferred a strong negative reward for taking a step, and despite the negative values it inferred for the independent agent being harmed, it lead to lower win rates compared to EMOTE or the \emph{Benchmarks} (Table \ref{table:win_harm_door}). 
EMOTE produced consistent reward inferences, almost matching \emph{Benchmark}, unlike \emph{Sympathy} where rewards inferred varied by the environment configuration.
%\st{Once again EMOTE and benchmark runs}\thommen{should we highlight the benchmark runs? Isnt it more like an oracle baseline? Maybe you could say that EMOTE was able to produce consistent reward inferences, almost matching the ones obtained through the benchmark (oracle) runs, unlike the Sympathy baseline in which the rewards inferred varied considerably depending on the environment configuration.}\manisha{ok. added} \st{were able to produce consistent reward inferences between the adversarial games, whilst Sympathy was not.}