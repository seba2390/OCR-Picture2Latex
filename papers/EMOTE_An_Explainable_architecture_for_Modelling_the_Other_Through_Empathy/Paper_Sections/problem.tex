\iffalse
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{Project2_framework}
  \caption{Sympathy Framework with proposed empathy-based architecture for independent agent's value function ($\hat{Q}_{indep}$). $\hat{Q}_{indep}$ is used to infer the rewards ($\hat{R}_{indep}$) of the independent agent. For the learning agent, a selfish value function ($Q_{selfish}$) is trained on rewards observed from the environment ($R$) and a sympathetic value function ($Q_{symp}$) is trained on sympathetic rewards ($R_{symp}$). $R_{symp}$ is a convex weighted sum of $R$ and $\hat{R}_{indep}$. The weighting is determined by the sympathy function $\beta(s,a)$ which outputs the degree of selfishness. $\beta(s,a)$ takes as inputs the value functions of the independent agent and selfish learning agent, and utilised a state prediction model $M(s,a)$. Actions are taken by the learning agent according to $Q_{symp}$}
  \label{fig:framework}
\end{figure}
\fi

We formulate our problem within the context of a Markov Decision Process (MDP) framework \cite{puterman2014markov} $\mathcal{<S,A,T,R,\gamma>}$ where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the space of actions, $\mathcal{T} : \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ is the transition function governing the probability of moving to the next state $s'$, having taken an action $a$ in the current state $s$. $\mathcal{R} : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ defines the reward an agent receives for taking an action in the current state, and $\gamma \in (0,1]$ is the discount factor.

To model the behaviour of the independent agent using EMOTE, we consider environment settings consisting of a learning agent, which we train, and one (or more) independent agent(s) which shares the same action and state spaces
%\thommen{actually I was thinking about it, and this method allows the transition dynamics of the two agents to be different, which is quite useful when you have a heterogeneous set of agents. You could include this as an advantage, but if there's no time/space right now, its fine - maybe we can add it later.} 
as the learning agent, and behaves as per a fixed policy. We assume that the underlying reward function of the independent agent is unknown to the learning agent. Our EMOTE architecture consists of training the action-value function for the learning agent $Q_{learn}$ using rewards $R$ returned from the environment, and simultaneously estimating the independent agent's action-value function as $\hat{Q}_{indep}$, using the latter's $<s_{i},a_{i},s_{i}^{'}>$ trajectories which we assume to be accessible (similar to a real world setting involving robots who share sensory input information). Each agent is assumed to also have their own reward function. 
%learning agent has access to the observations and actions of the independent agent, but the latter's underlying reward function is unknown to the former.
 %In order to estimate this reward function, the learning agent is assumed to have access to the independent agent's $<s_{i},a_{i},s_{i}^{'}>$ trajectories, derived from the independent agent's fixed policy. %Both agents are assumed to share the same action space. 
  %We propose the EMOTE architecture to model the independent agent's action-value function $\hat{Q}_{indep}$. 
  EMOTE uses relevant IRL methods to estimate the reward function $\hat{R}_{indep}$ of the independent agent from the estimated action-value function $\hat{Q}_{indep}$. Further details of the EMOTE architecture are described in the subsequent sections.

%We note that EMOTE is best suited to problems where the two agents can be assumed to be similar, or ``analogous'' \manisha{which we define as below.}
%\st{-  they share common characteristics or features (e.g. each have their own goals, are averse to pain, have a time penalty at each step etc.,)}.

\iffalse
\begin{definition}
In a multiagent learning scenario involving a learning agent with action value function $Q_{learn}$ and an independent agent (sharing the same action space $\mathcal{A}$ as the learning agent) which behaves as per an arbitrary unknown policy, we define state $\bar{s}$ of the learning agent to be analogous with state $s$ of the independent agent if: 
\begin{equation*}
 \underset{a'}{argmax}{Q_{learn}(\bar{s},a')} = a_{i}   
\end{equation*}
\noindent{}where $a_{i}\in\mathcal{A}$ is the observed action of the independent agent in state $s$. 
\end{definition}
\fi

%\begin{definition}
%In a multiagent learning scenario involving a learning agent with action value function $Q_{learn}$ and an independent agent (sharing the same action space $\mathcal{A}$ as the learning agent) which behaves as per an arbitrary unknown policy, the learning agent is considered analogous to the independent agent if, within the set of state features ($F1$) available to the learning agent (e.g. pellets, door, power dynamic controller etc) there exists a feature $f\in F1$ in the presence of which the learning agent exhibits the same behaviour as the independent agent in the presence of a feature from its own set ($g \in F2$) when placed in the same position/index of the state representation: 
%\begin{equation*}
 %\underset{a'}{argmax}{Q_{learn}(s_{f},a')} = a_{i}   
%\end{equation*}
%\begin{equation*}
%s_{f} = \text{replace}(s_{g},g\rightarrow f)  
%\end{equation*}
%\noindent{}where $a_{i}\in\mathcal{A}$ is the observed action of the independent agent in state $s_{g}$, and $s_{f}$ is the state where $g$ is replaced with $f$. We can use the number of analogous state features to be indicative of the degree of analogousness.
%\end{definition}
%Additionally, the agents are assumed to have the same transition dynamics (same $\mathcal{T}$).\thommen{is this assumption needed?}

%\thommen{From this point, the remaining parts of this subsection can be skipped (maybe shift it to the proposed solution subsection). Instead, you can mathematically specify the learning problem in terms of your losses (basically put the contents of sec 3.1.2 here). This will make the problem formulation clear and precise.}
%\manisha{not sure what you mean about putting the losses section here.. I think it might sit better in the propsed solution, after I inroduce the architecture?}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.9\linewidth]{Empathetic_ValueFunction.png}
  \caption{EMOTE architecture of the independent agent's value function $\hat{Q}_{indep}$ comprising a two stage neural network. The first, the Imagination Network $M_{imagine}$, takes in the state $s_{i}$ perceived by the independent agent and outputs an empathetic state $s_{e}$. The second is a copy of the learning agent's value function $Q_{learn}$. $s_{e}$ is fed into $Q_{learn}$ and associated Q-values are output. Only $M_{imagine}$ is trained via a loss function comprising the difference between $s_{i}$ and $s_{e}$ and the categorical cross entropy between the predicted softmax actions $\pi_{learn}(a|s)$ and the observed one hot encoded action of the independent agent $\mathbbm{1} \cdot a_{i}$.}  
  \label{fig:empathetic_valuefunction}
\vskip -0.2in
\end{figure}