%\iffalse
\begin{figure}[!h]
%\vskip -0.2in
     \centering
     \begin{subfigure}{0.18\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Ass1_Gridworld}
         \caption{Ass. 1}
         \label{fig:y equals x}
     \end{subfigure}
     \begin{subfigure}[b]{0.18\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Ass2_Gridworld}
         \caption{Ass. 2}
         \label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.18\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Adv1_Gridworld}
         \caption{Adv. 1}
         \label{fig:three sin x}
     \end{subfigure}
     \begin{subfigure}[b]{0.18\textwidth}
         \centering
         \includegraphics[width=\textwidth]{Adv2_Gridworld}
         \caption{Adv. 2}
         \label{fig:three sin x}
     \end{subfigure}
        \caption{Learning agent (red arrow) and independent agent (yellow arrow) aim to collect their corresponding pellets. \textbf{Assistive env.} (a) - (b): Closed door (green square) can only be opened by learning agent pressing the green button (incurring a negative reward).  \textbf{Adversarial env.} (c) - (d): Independent agent can harm the learning agent. When the green button is pressed, learning agent can, for a finite time, harm independent agent, receiving a positive reward.}
        \label{fig:Environment_layout}
\vskip -0.2in
\end{figure}
%\fi

Our work is constrained to settings that meet the criteria outlined in Section 3.1. To the best of our knowledge, works that fit within these settings were those by Senadeera et al. \cite{SympathyPaper}, Bussmann et al. \cite{TowardsEmpathicDQN}, Noothigattu et al. \cite{Noothigattu2019} and Papoudakis et al. \cite{papoudakis2021agent}. 
%In this work, however, 
We demonstrate integration of our proposed framework on \cite{SympathyPaper} only,
%we only demonstrate our proposal on \cite{SympathyPaper}, 
as other works assumed the independent agent’s behaviour was (a) random \cite{TowardsEmpathicDQN}, (b) involved a complex switching policy to train the learning agent involving imitation learning via human example trajectories \cite{Noothigattu2019} or (c) did not have access to the independent agent’s trajectories during testing \cite{papoudakis2021agent}. The Sympathy Framework of \cite{SympathyPaper} consists of both a joint action-value function and reward function in its design, providing a useful illustration of the benefits proposed by the EMOTE architecture. To demonstrate the versatility of our proposed approach, we designed experiments that (1) illustrate various environment settings (two assistive and two adversarial), (2) demonstrate the potential of EMOTE to perform well even under differing degrees of the analogy assumption, (3) show the ability for the empathetic state to be constructed both as a feature-by-feature transformation, or as a whole state (image) transformation, and (4) demonstrate how in addition to visual features, our approach can handle non-visual features that influence the dynamics of the environment in a complex and non-linear fashion. %\thommen{Maybe we can emphasize this when explaining about the buttons? Do we need to include it here?}
%\st{Figure \ref{fig:Environment_layout} shows our four environment settings.} 
Environments were designed using MarlGrid \cite{MarlGrid} based on MiniGrid \cite{gym_minigrid} (code in Supplementary).


Played in finite, episodic environments where actions are taken sequentially by each agent, goal of learning agent (red arrow) is to complete its assigned task of collecting all red pellets. At times, the agent may be faced with situations where it has the ability to behave selfishly or even harm the independent agent (yellow arrow). The Sympathy Framework trains the learning agent to still complete its task whilst being considerate of the independent agent, even though there is no reward-driven incentive for doing so. This is done by training the learning agent on a \emph{sympathetic reward} \cite{SympathyPaper}, a convex weighted sum of the learning agent's reward, and the independent agent's inferred reward. The weighting is determined by a selfishness term, which is a function of both agents' action-value functions.

EMOTE replaces the network used to model the independent agent in \cite{SympathyPaper}, but we use the same IRL method (Cascaded Supervised Learning \cite{cascadedSuperIRL2013}) to infer rewards. This IRL method applies supervised learning to train the model using state-action pairs, following which an inversion of the Bellman Equation is applied to extract the reward function. EMOTE is assessed by three criteria:

%\thommen{Do we need all these details? We already mentioned they estimate the indep reward function with scaling etc., And we described the current approach above. So I think you can skip the details and just say we replace their reward estimation method with EMOTE. And then skip to `We assess EMOTE...'}Within the Framework is an IRL module made of two components: (1) a network to model the independent agent's action-value, and (2) a component to infer the independent agent's reward based on this action-value function. The IRL module follows the Cascaded Supervised Learning method proposed by \cite{cascadedSuperIRL2013}. 

\begin{enumerate}
    \item \textbf{Performance:} Whether with EMOTE, the learning agent behaves considerately towards the independent agent while completing its own task.
    \item \textbf{Inferred Independent Agent Rewards:} Are the rewards inferred via IRL from the EMOTE architecture comparable to the learning agent's rewards?%, and how does it compare to those inferred by \cite{SympathyPaper}? %\thommen{do we need `..and how does it compare to those inferred by \cite{SympathyPaper}?'? I feel like it'll seem like we are specifically targeting this paper}\manisha{shall we just cut out the bit from '..and how does it compare...' onwards?}
    \item \textbf{Explainability}: Are the analogous features and associated empathetic states informative and reflective of the independent agent's behaviour?
    %and does it correspond to the expected nature (eg: highly positive, negative, neutral etc.,) of empathetic rewards?\thommen{the part about empathetic state is okay, but isn't the rewards part more related to point 2. (although I see why you put it under explainability)?}
\end{enumerate}

\input{Paper_Sections/games}

\subsection{Baselines}
In each experiment, policies are learned via DQN \cite{mnih2015human}. A 5x5 field of vision is imposed around each agent, representing its visual state information. We compare EMOTE with the following baselines:

\emph{Selfish:} A selfish version of the learning agent which only has access to its own rewards from the environment (no modelling of the independent agent).

\emph{Sympathy:} Agents trained as per \cite{SympathyPaper}.

\emph{E-Feature:} A feature-based Imagination Model where each state cell is represented as a feature. $s_{e}$ is constructed with a feature-by-feature transformation.

\emph{E-Image:} An Image-based Imagination Model, where the entire state is transformed to create $s_{e}$.

\emph{Benchmark 1 - Swap (B-Vis):} The Imagination Model is replaced with a rule-based state transformation, such that the colours of the two agents' pellets are swapped. This mimics an oracle baseline that presumes the hypothesis - how the learning agent feels about its pellets is how the independent agent feels towards its own pellets. 

\emph{Benchmark 2 - Swap (B-Invis):} The Imagination Model is replaced with a rule-based state transformation which swaps the colours of the agent's pellets (like Benchmark 1) and additionally, makes the button invisible. The invisibility applies only to the observed state, and the button status remains. This mimics an oracle corresponding to the hypothesis that in addition to Benchmark 1's belief, the independent agent does not consider the button to be important (treats it the same as the floor), as it cannot press it.

%\textbf{Benchmark 3 - Swap (B-Recolor)}: A rule-based state transformation which swaps the colors of the agent's pellets. Also, the button color is changed to the learning agent pellet's color. This mimics the hypothesis that if the button was the same color as the learning agent's pellet, it would incentivise the door to be opened.


