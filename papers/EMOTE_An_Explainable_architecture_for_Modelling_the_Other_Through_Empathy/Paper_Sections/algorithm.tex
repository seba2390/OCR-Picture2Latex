Algorithm \ref{alg:IRL_1} presents the pseudocode for updating the empathy-based value function of the independent agent. 

\begin{algorithm}[h]
    \caption{Independent Agent Value Function Update}
	\label{alg:IRL_1}
	\begin{flushleft}
    \textbf{Inputs}: Indep. Agent Replay Memory $D_{indep}$, $\hat{Q}_{indep}$\\
	\end{flushleft}
	\begin{algorithmic}[1] %[1] enables line numbers
	    \STATE Sample random batch of transitions from $D_{indep}$
		\FOR{$j = 1, 2, ..., batch$}
		\STATE $s_{e_{j}} = M_{imagine}(s_{j} ; \theta_{imagine})$
		\STATE Set $y_{j}$ = $\mathbbm{1}.a_{j}^{indep}$\text{(one-hot encoded actions)}
		\STATE Update $\theta_{imagine}$ w.r.t loss (Eqn \ref{eqn:Loss_Function})
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\iffalse
\STATE $\pi(a_{j}|s_{j})=\frac{e^{\hat{Q_{indep}(s_{j},a_{j};\theta_{indep})}}}{\sum_{a}e^{\hat{Q_{indep}(s_{j},a;\theta_{indep})}}}$ 
\STATE
$\theta_{indep} = [\theta_{imagine},\theta_{selfish}]$
\STATE $\frac{1}{n}\sum\left((1-\delta)\text{CE}(y_{j},\pi(a_{j}|s_{j})) + \delta\norm{s_{j} - s^{i}_{j}}\right)$
\fi
