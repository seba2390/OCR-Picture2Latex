\begin{figure*}[ht]
\vskip -0.2in
\begin{center}
\centerline{\includegraphics[width=0.95\textwidth]{Empathetic_States}}
\caption{Empathetic states $s_{e}$ produced by E-Feature (Col 2 - 4) and E-Image (Col 5 - 7) for original state $s_{i}$ (Col 1). Details on construction of $s_{e}$ plots are provided in the Supplementary.}
\label{fig:imagined_state}
\end{center}
\vskip -0.4in
\end{figure*}

EMOTE's key benefit is its ability to produce a human-interpretable empathetic state $s_{e}$ which can be used to explain some of the performance results from Section \ref{sec:performance}. Figure \ref{fig:imagined_state} shows an original state $s_{i}$ for each of the four environments examined alongside three examples of the final empathetic state $s_{e}$ (at the end of training) generated from the \emph{E-Feature} and \emph{E-Image} Imagination Networks.
%For the 4 environments examined, we output $s_{e}$ corresponding to the original states $s_{i}$. These are shown in Figure \ref{fig:imagined_state} with $s_{i}$ for each game alongside 3 examples of the final empathetic-state $s_{e}$ (at the end of training) generated from the \emph{E-Feature} and \emph{E-Image} Imagination networks. 
More final empathetic states, as well as examples of the change in the empathetic state for those shown in Figure \ref{fig:imagined_state} during training can be found in the Supplementary.

The learning agent's pellet (LP) is fairly consistently transformed to the floor by $s_{e}$, indicating the lack of importance the independent agent places on it.  We observed EMOTE outperforming \emph{Benchmarks} (Section \ref{sec:performance}). We infer that because the learning agent will have experienced more instances of the floor (relative to IP, which also disappear when consumed), this feature is better estimated in its value function, leading to better performance by reimagining the LP as floor. 
%\thommen{On the other hand, in the \emph{Benchmarks}, the action-value estimates around the feature of IP may be slow to converge, owing to the learning agent's limited interactions with this feature, especially during the initial stages of learning.}   
The colour of the independent agent's pellet (IP) becomes that of the learning agent's pellets (blue) in the transformed state $s_e$ thus explaining the inferred rewards for IA Pellet in Figure \ref{fig:IRL_results}. This suggests that the empathy architecture allows the learning agent to interpret the relationship between independent agent and its pellets as analogous to that between the learning agent and its own pellets.%much the same as what the independent agent feels towards its own pellet. 

In the empathetic states for Assistive 1 in Figure \ref{fig:imagined_state}, the button either remains unchanged, changes to a door or an independent agent's pellet. The independent agent does not value the button as it cannot interact with it thus it is mapped to features that are similarly irrelevant in the learning agent's own reward function.
%This is understandable as the learning agent is not attracted toward these features according to its own reward function. This is similar to how the independent agent behaves towards the button (due to its inability to get close to the button because it is stuck behind the door). 
In contrast, for the Assistive 2 environment, Figure \ref{fig:imagined_state} shows the button transforming at times to the learning agent's pellet. This is expected as how the independent agent moves towards the door (in front of which the button is placed) is similar to how the learning agent moves towards its own pellet. 

In Adversarial environments, the button usually disappears in the empathetic states. This is expected, as the independent agent has no influence over the button, and is thus not a feature of importance. However, it is important to observe the value of the predicted button status in the empathetic states, the results of which are shown in Table \ref{table:button_status}. For a button status value of 0 in $s_{i}$ (button is inactive and independent agent can harm the learning agent) the resulting status prediction in $s_{e}$ is closer to 1, while a button status value of 1 in $s_{i}$ (button is active and learning agent can harm the independent agent) produces a prediction in $s_{e}$ closer to 0. This indicates that the EMOTE architecture is able to associate the button status (a non-visual feature) with the power dynamics between agents, namely how the independent agent behaves when the button status is 1 (i.e. trying to avoid being harmed) is similar to how the learning agent behaves when the button status is 0 (i.e. also trying to avoid being harmed). In the adversarial environments, the \emph{Benchmarks} outperformed EMOTE for the harm metric as shown in Table \ref{table:button_status}. This is because the \emph{Benchmarks} are oracle baselines, where the button status was swapped using manually encoded rules. However, for EMOTE this was a difficult transformation (with a non-linear influence on the environment dynamics), that had to be learned (Table \ref{table:button_status}).
%\thommen{Here maybe you can emphasize the point about EMOTE being more than just visual features}

%(is able to infer that that is, when the learning agent is in a position to harm, the inference of the independent agent's association is that of the learning agent when it is in a vulnerable position)\thommen{this part in the brackets is confusing, and the sentence structure is strange. What did you want to say?}. 

\begin{table}
\vskip -0.3in
\begin{center}
\caption{Adversarial: Predicted $s_e$ button status for $s_i$ button status values
%$s_{e}$ button status in Adversarial games \thommen{is perceived inversely as the true button status ($s_e$ is close to $1$ when $s_i=0$ and vice-versa), indicating the correct inference of agent power dynamics.} %\manisha{$s_{i}$ button value of 0 (inactive) indicates independent agent can harm learning agent. Value of 1 (active) indicates learning agent can harm independent agent.}
}
%\vskip 0.15in
\begin{tabular}{lcccc}\toprule
&  \multicolumn{2}{c}{$s_{i}$ E-Feature} & \multicolumn{2}{c}{$s_{i}$ E-Image}
\\\cmidrule(lr){2-3} \cmidrule(lr){4-5}
 Game  & 0 & 1 & 0 & 1 \\\midrule
Adv 1      & 0.76 $\pm$ 0.11   & 0.24 $\pm$ 0.11  & 0.785 $\pm$ 0.09 & 0.215 $\pm$ 0.10   \\
 \midrule
        Adv 2  & 0.85 $\pm$ 0.32 & 0.15 $\pm$ 0.32 & 0.88 $\pm$ 0.16 & 0.12 $\pm$ 0.16    \\
\bottomrule
\end{tabular}
\label{table:button_status}
\end{center}
\vskip -0.3in
\end{table}




