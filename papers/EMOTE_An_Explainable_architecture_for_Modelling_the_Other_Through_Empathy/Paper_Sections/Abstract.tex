

%that another person, despite having different goals, is analogous to us - we both require food, exert energy and feel similar degrees of pain and pleasure. 
%agents are generally similar - e.g. 
%incur time step penalties, avoid dangerous features, or 
%such that the corresponding optimal action, when viewed through the agentâ€™s own action-value function, matches the action taken by the other agent. 
%our architecture produces an empathetic representation of the other agent's observations that a


% Stephan
\iffalse
We can usually assume others have goals analogous to our own, and this assumption is useful for human cognitive processes like empathy where we can build on similarities between us and others to better understand the behaviour of others. Inspired by empathy we apply this process to multi-agent games by designing a simple and interpretable architecture to model another agent's action-value function. This involves learning an \emph{Imagination Network} to transform the other agent's observed state in order to produce a human-interpretable \emph{empathetic state} which, when presented to the learning agent, produces behaviours that mimic the other agent. Our approach is applicable to multi-agent scenarios consisting of a single learning agent and other (independent) agents acting according to fixed policies. This architecture is particularly beneficial for (but not limited to) algorithms using a composite value or reward function. We show our method produces better performance in multi-agent games, where it robustly estimates the other's model in different environment configurations. Additionally, we show that the empathetic states are human interpretable, and thus verifiable.
\fi

We can usually assume others have goals analogous to our own. This assumption can also, at times, be applied to multi-agent games - e.g. Agent 1's attraction to green pellets is analogous to Agent 2's attraction to red pellets. This ``analogy'' assumption is tied closely to the cognitive process known as empathy. Inspired by empathy, we design a simple and explainable architecture to model another agent's action-value function. This involves learning an \emph{Imagination Network} to transform the other agent's observed state in order to produce a human-interpretable \emph{empathetic state} which, when presented to the learning agent, produces behaviours that mimic the other agent. Our approach is applicable to multi-agent scenarios consisting of a single learning agent and other (independent) agents acting according to fixed policies. This architecture is particularly beneficial for (but not limited to) algorithms using a composite value or reward function. We show our method produces better performance in multi-agent games, where it robustly estimates the other's model in different environment configurations. Additionally, we show that the empathetic states are human interpretable, and thus verifiable.


%Getting along with someone else isn't always easy, but it is vital for a stable and productive society. Communication and understanding are key for smooth social interactions, leading to better cooperation. Luckily for us evolution has bestowed humans and animals the ability to empathise. Empathy tries to understand the feelings and goals of another, by using ourself as a point of reference. How would we feel if we were in a similar situation? What fears and goals do we share? What objects or goals, though not exactly the same, do we feel similarly towards? Despite our biological ability to empathise, understanding another is really tricky and we humans rely on a vast set of information sources to understand the other. These are not limited to verbal and written streams, but also encompass behaviours, body language and subtle verbal and phsyical cues, each of which are small clues to the others hidden state. Now if you thought empathy between humans was hard, try between agents! Or between a human and an agent! With the increasing prevalence of artifically intelligent agents, particularly in the form of robots or virtual agents, their presence in shared environments with other agents or humans will necessesitate the need to understand others. In this work we integrate an empathy-based architecture for modeling `the other' in artificial agents. We hone in on settings where two analagous agents share a space. Of these two, the learning agent (the one we train) tries to models the other `independent agent' by referencing its own rewards and value function. As there is yet no evidence to believe agents have `feelings', we instead utilise our architecture to understand the other's goals (articulate as a reward function). Our proposed architecture is a two stage neural network, the first of which constructs an empathetic representation of the independent agents state, before sending this representation into a copy of its own value function (the second network). In this way, our significant contribution is the ability to generate an empathetic state that is human interpretable, allowing a user to clearly see what features both agents share (those that elicit empathy). In turn, the model can be used to infer the underlying rewards  of the other agent which can be used to elicit cooperative behaviour in the learning agent.

\iffalse
Smooth social interactions are 
%driven by our ability to understand how others feel and behave. This is
enabled via mechanisms such as empathy, which allows us to understand how another is feeling by referencing our own emotions from similar situations. 
%Artificially intelligent systems that learn from interactions with other agents and humans are becoming increasingly common. 
As artificially intelligent systems become more prevalent in applications involving interactions with other agents and humans, it becomes increasingly important that they too exhibit such empathetic qualities. In this work we propose a novel empathy-based approach to enable a learning agent to understand the rewards and values of another agent (the independent agent) by referencing its own reward and value function. 
The empathy mechanism is realised through a two stage neural network architecture, the first of which reconstructs the independent agent's state from the learning agent's perspective, following which it is passed through the learning agent's value function network. 
%We train our learning agent to model the independent agent via an empathy-based architecture. 
%Applied to symmetric games (games in which the goals of each agent are similar, but not necessarily the same), our empathy mechanism allows us to model the independent agent's behaviours by grounding them in the learning agent's own reward and value functions. This in turn enables it to interact with the independent agent in a more context-informed manner.
Through this empathy-based approach, we (1) learn an estimate of the independent agent's reward function that is consistent with the scale of the learning agent's own reward function and (2) produce an empathetic representation of the independent agent's state that is grounded in the learning agent's own value function and experiences. %These aspects allow the learning agent to effectively account for the presence of other agents in its environment.
%the benefits of empathy are the ability to (1) infer a more comparable reward and value function for the independent agent, and (2) the ability to produce an empathetic representation of the independent agent's state as imagined by the learning agent.
%\thommen{Not necessarily human understandable, right? Should we say empathetic state representations?}
%\thommen{Points 1 and 2 are relative to the sympathy paper}\manisha{I don't understand what you mean here}\thommen{I mean that when you say 'more comparable reward and value function..', it automatically implies that this contribution is in relation to the sympathy paper - and at this point, the reader has no context of that paper. Maybe a better way to say it is that with empathy, we can model the independent agent's behaviors by grounding them in the learning agent's own reward and value functions, which may enable it to interact with the independent agent in a more context-informed manner. Specifically, our empathy-based framework (1) learns an estimate of the independent agent's reward function that is consistent with the scale of its own reward function and (2) produces representations of the independent agent's states that are grounded in its own value function and experiences(?). These aspects allow the learning agent to effectively account for the presence of other agents in its environment.} 
%interpretable (can we also say more accurate?) to the learning agent.}
We demonstrate the benefits of these estimates and representations in a variety of environments and show that our learning agent is able to behave considerately towards the independent agent whilst still completing its own task. 
\fi