
\paragraph{Modelling the other:} Modelling other agents is a key component of multi-agent reinforcement learning (MARL). There exists a vast body of work ranging from inferring the other's policy \cite{foerster:aamas18,wen2019probabilistic,hu2020other,shu2018m}, goals and beliefs \cite{Raileanu2018ModelingOU,moreno2021neural} and value functions \cite{zhao2022mcmarl,pmlr-v48-he16}. These works predominantly assume that all agents are being trained concurrently which differs from our intended setting where only a single agent is trained. Modelling agents who behave according to a fixed pre-trained policy can be tackled with works in Theory of Mind (ToM) \cite{pmlr-v80-rabinowitz18a} and Inverse Reinforcement Learning (IRL) \cite{ng2000algorithms} literature. A small subset of MARL combines these two problems to create environments in which a learning agent (to be trained) coexists with and tries to model a pre-trained (independent) agent \cite{papoudakis2021agent,SympathyPaper}, in line with the problem setting of our work. However, such works generally do not produce interpretable models, and produce arbitrarily scaled action-value estimates, which impedes the accurate inference of agent behaviours.% \manisha{added in previous line}
%\thommen{Can we say something like `However, such works generally do not produce interpretable models, and produce arbitrarily scaled action-value estimates, which impedes the accurate inference of agent behaviours.'} %Our work targets this subset.
%One such paper is that by  in which the learning agent is assumed to only have access to the other agent's trajectories during training. As such, to incorporate the other's behaviour into the learning agent's policy, a latent representation is extracted and trained on. \thommen{What is this paper's relation to the current work?}

\paragraph{Modelling based on oneself:} A selection of works model the other agent based on their own model. Using ToM, \cite{Raileanu2018ModelingOU} trains the learning agent on all possible goals during training and uses this information to infer the hidden goal of the other agent based on its behaviour. This method differs from our setting as it is constrained to games that have set goals which can be experienced by the learner. Inspired by empathy as well, \cite{TowardsEmpathicDQN} proposes 
%has a learning agent that shares an environment with a single independent agent. In order 
imposing the learning agent's own value function directly on the independent agent, using this to infer the other's intent. A limitation is that imposing the same value function on the independent agent assumes this agent has the same values as the learner. Our work eschews this assumption, allowing for different and even opposing intentions. 

\paragraph{Composite value and reward functions:} In multi-agent scenarios with composite reward or value functions (e.g. summation of two or more reward or value estimates), it is important they are scaled appropriately to ensure stable behaviours. %\st{ensuring comparability}\thommen{it might be unclear what we mean by comparability. How about `When summating two or more reward or value estimates, to achieve stable behaviours, it is important to ensure they are scaled appropriately.'} \st{is important to produce stable behaviours.} 
 Approaches such as VDN \cite{VDN} and QMIX \cite{rashid2018qmix} combine separate agent value functions to conduct centralised training, thus obviating the need for such scaling. However, this does not allow for independent agents with pre-trained policies. %When all agents are being trained, these value functions do not need any further scaling or adjustment prior to combining them together. 
 More closely related, \cite{alamdari2021considerate} builds a joint function of learning agent and independent agent rewards (whose rewards are already known). A similar joint reward function is built by \cite{SympathyPaper} however they use IRL to to infer the rewards of the independent agents.
%\st{More closely related algorithms include that by \cite{alamdari2021considerate}, where a joint function of the learning agent's reward together with the reward of all independent agents is constructed to train the learning agent. No scaling was applied, as it was assumed that a distribution over reward functions for the independent agent was known. \cite{SympathyPaper} also trains a learning agent on a similar joint reward function, with the independent agent's reward function being inferred via IRL.}
As a result of the space of potential rewards that can emerge through IRL \cite{PolicyInvariance}, the paper mitigates the issues of a misalignment by scaling the independent agent's functions by a constant (the ratio of the $l1$ norms of the learning agent's reward vector and the IRL inferred rewards of the independent agent). This method was also applied by \cite{Noothigattu2019}. %\st{, where an ethical agent was trained using a multi-arm bandit reward function (using a combination of rewards).} 
This simple $l1$ norm based normalisation may fail in many complex scenarios, and additionally, is constrained to problems that only sum two reward or action-value functions, motivating need for alternatives. %Our proposed work aims to address this issue, along with other  \manisha{one of which our work proposes.}
%\thommen{Can add a concluding statement for the related literature}%\st{In the case of \textcolor{red}{Sympathy Paper}, this restricts the approach to handling no more than one independent agent. Scaling by a constant does not guarantee comparable reward or value functions between agents, which motivates the need for better alternatives. As such, we propose the use of the learning agent's own action value estimates to infer that of the independent agent, addressing the aforementioned issues of comparability.}

%Interest in designing agents with considerate and ethical behaviours has been growing over time \cite{Abel2016ReinforcementLA}, \cite{SachiyoArai2014}, \cite{EmpDC}, \cite{alamdari2021considerate}. Much like our work, \cite{Raileanu2018ModelingOU} used an empathy-like approach to model other agents based on the learning agent's own model. In each episode of training, each agent is randomly assigned a task. The rewards obtained are dependent on the goals of both agents, incentivizing the need to model and understand the goals of the other. 
%\thommen{From here till ...player is?" can be skipped I think.}
%The goal of each agent in an environment is binary (e.g. collect a specified colour ball), with each agent being randomly allocated one of the possible goals at the start of each episode. Due to the change of goal in each episode, an agent is able to experience each possible goal and incorporate this information into their model. As a result, inferring the goal of the other agents becomes easier as they are able to reference their own experiences to infer, based on observations of the others actions, what the other's goal may be by asking the question "What would my goal have been had I acted in the way the other player is?". 
%The drawback of such a method is that it requires all agent-goal combinations to be sampled. Because each agent experiences all possible tasks, learning to infer what the goals of the other becomes easier as the agent can draw back to direct experiences they have had in the past with that same task. Another work by \cite{TowardsEmpathicDQN} involves a learning agent sharing an environment with a pre-trained independent agent, the authors evoke empathy by imposing the value function of the learning agent over the independent agent to determine which action to take by asking the questions, "If I was in the other's position, what action would I want the other to take?". The limitation of this work is the assumption that both agents have the same objective. In contrast to both of these works, we focus on a more general setting, where the reward functions or goals of both agents are not necessarily identical.

%Closely related to our work, \cite{SympathyPaper} developed a Sympathy Framework which allowed both agents to have differing objectives. To model the independent agent, the authors utilised the Cascaded Supervised Learning method \cite{cascadedSuperIRL2013} which is able to infer a value function and reward function of the learning agent. Using these, the authors construct a sympathetic reward as a convex combination of the learning agent's reward and the inferred reward of the independent agent on which a sympathetic policy in trained for the learning agent. The weighting is determined by a degree of sympathy term which adaptively adjusts itself to appropriate values depending on the situation faced. 
%In IRL it is commonly known that there is a space of possible reward functions that can express the behaviour observed in an agent \cite{PolicyInvariance}. 
%Although the authors were able to produce considerate behaviours in the learning agent, the IRL method is susceptible to inferring an incorrect reward function due to the vast space of possible reward functions that could corresponding to an agent's observed behaviour \cite{PolicyInvariance}. As the sympathetic reward is a combination of both agent's rewards, it is particularly important that the rewards of both agents are in a consistent and comparable range. To address this, the authors scaled the inferred reward function of the learning agent to have the same $l1$ norm as the independent agent. This method may not always be appropriate or work. To address this limitation, we propose utilising the learning agent's own value function as a reference for the independent agent's value function to ensure the resulting value and reward functions will be consistent between both agents.