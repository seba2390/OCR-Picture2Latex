Getting along with everyone isn't easy, but trying to understand them is an essential first step to a smooth and pleasant encounter. 
%but unfortunately life contains plenty of interactions with difficult and unpleasant people\thommen{I feel like this first sentence is not directly relevant. Why not simply say ` To effectively deal with everyday interactions, understanding the other is a an essential quality.'} \stephan{The point is perhaps that we particularly need to want to ``understand the other'' when we have very little in common with them. So maybe 'Getting along with everyone isn't easy, but unfortunately life contains plenty of interactions with people we cannot relate to'}. In an attempt to get through some encounters unscathed, understanding the other is a productive step. 
To support this, most of us possess the cognitive ability known as empathy. Hoffman \cite{hoffman1996empathy} defines empathy as \textit{``any process where the attended perception of the object’s state generates a state in the subject that is more applicable to the object’s state or situation than to the subject’s own prior state or situation."} From this definition, empathy can be interpreted as a process which helps us understand the feelings and goals of another, by using ourselves as a point of reference. \textit{How would we feel if we were in a similar situation? What objects or goals, though not exactly the same, do we feel similarly towards?} An implicit assumption is the belief that who we are observing is analogous or similar to us, even if our goals or preferences are different. %\st{Imagine seeing a dog in pain from an injury. Of course, there are many differences between us and it, but we can empathise with it's pain because we can imagine how it might feel, based on how we would feel in the same situation. Now simply putting ourselves in another's shoes is not always enough to empathise 
%(not only because your feet might not fit, or because dogs don't wear shoes)\thommen{The joke might be distracting, maybe even confusing for some readers who need not share the same sense of humour/cultural background. Also, I think it doesn't add much to the main point - it is currently an aside.} \stephan{``Dogs don't wear shoes'' is an example of a difference in experience that makes it difficult to empathise, I think.}, 
%because the preferences we have aren't always exactly the same.} 
As a simple example: I love chocolate but dislike liquorice. If I were to see someone eating and enjoying liquorice, I would not immediately relate to how they feel. However, if I \textit{imagined} it was instead chocolate they are consuming, I could understand their joy and subsequently infer the levels of enjoyment as being similar to mine when eating chocolate. In this situation, the chocolate and liquorice are analogous features.
%\thommen{is it that we imagine they are eating chocolate, or that we imagine their enjoyment being similar to ours when we eat chocolate?} \stephan{Given the mechanism of the framework in the paper I believe the intent here is to imagine them eating chocolate: ``How does the underlying \emph{state} change in order to evoke the same reward as a consequence?''}\manisha{I've changed the following sentence for more clarity.} 

%\st{Empathy between humans is not easy, but it pales in comparison with empathy between artificial agents. Trickier still: between a human and an agent!}
%\thommen{The last few sentences are a bit informal. It is okay, but alternating tones from less formal to formal could be somewhat jarring to readers.} 
With the increasing prevalence of robots, virtual assistants, etc. in regular life, artificially intelligent systems in shared environments will benefit from modelling and understanding other agents or humans around them. % \st{especially if we hope for safe and trust-filled engagements}. This problem falls within the space of multi-agent reinforcement learning (MARL) \cite{hu1998multiagent}. 
%\st{In multi-agent scenarios, it is common to train all agents simultaneously.}\thommen{I think this line can be removed} 
A subset of this space involves a single learning agent coexisting with, and learning to model, one or more other agents who behave under fixed policies (e.g. a human with set preferences or a pre-trained robot) \cite{AIJ18-Albrecht}. 
%\st{One can imagine this scenario to be like that of a new robot entering a room with a human (who already has a set of preferences and behaviours) or a pre-trained robot, where the new robot is trying to learn how to achieve its own goals in the presence of the other.}

%Though this problem can be thought of as training a single RL agent with all other agents considered part of the environment, it can instead be beneficial to explicitly model the other. \thommen{I found this paper: https://www.cs.utexas.edu/~pstone/Papers/bib2html/b2hd-AIJ18-Albrecht.html It seems like this is already an established sub-field in multiagent learning, so I'm wondering if we should just say that it falls under a subset of multiagent learning where one agent models other agents.}%Humans are, after all, a bit complicated.

To incorporate empathetic modelling behaviours 
%\thommen{to incorporate empathetic behaviours or to enable agents to appropriately model the interests of others in such scenarios?} 
in agents being trained in such scenarios,
%\thommen{The previous para ended with just describing the situation of a human and pretrained robots. Now when we start this para with `Towards this pursuit', its not clear what we mean. Thats why I suggested `To incorporate empathetic behaviours to agents being trained in such scenarios, we present...' or something like that}
%\thommen{when you say `for this', it is not clear what you are referring to. You could add something like `To incorporate empathetic behaviours to agents being trained in such scenarios, we present...'}, 
we present EMOTE - an \emph{Explainable architecture for Modelling the Other Through Empathy}. Drawing inspiration from empathy, this architecture is designed to allow a learning agent to reference its own action-value function to model another agent's action-value function, leading to a more stable and robust representation of the other. Crucially, leveraging the learning agent's own functions enables the other agent's model to be \emph{human-interpretable}, allowing interrogation of the inferences made by the learning agent
about the other agent's goals (in the form of a reward function). %\st{to model the other agent's action-value function.} 
We consider settings where analogous agents share a common environment - specifically, a single learning agent aims to model the other `independent' agents who are %\st{by referencing its own action-value function}. \st{The independent agents are} 
pre-trained and act according to fixed policies, the reward functions of which the learning agent is not privy to.
%, but aims to infer.

EMOTE consists of a two stage neural network architecture. The first, called the \emph{Imagination Network}, imagines an empathetic representation of the independent agent's state. This \textit{empathetic state} can be understood to be the perception of the independent agent's state, from the perspective of the learning agent (e.g. liquorice reimagined as chocolate). This empathetic state is fed into a second network, a copy of the learning agent's own action-value function, to observe what values the learning agent would have associated with this empathetic state.

The benefits of EMOTE are three fold. Firstly, by referencing its own action-value function, the reward and action-value estimates made by the learning agent about the independent agent are more robust which we demonstrate in different environment settings (adversarial/assistive) and configurations (layouts).
%\thommen{I think its robustness and stability in terms of estimating the rewards/action-values? Is it referring to changes in the env layout?} I  changed slightly.} 
Secondly, explaining the inferences made by the learning agent about the independent agent's behaviour is possible through the generated empathetic state which is human-interpretable. A user can tap into this empathetic state and compare it against the original state of the independent agent, observing which features remain the same (both agents view and react similarly to), and which have changed (e.g. the liquorice being reimagined as chocolate), offering a useful tool for interpretable verification of the independent agent's action values.
Lastly, funnelling the empathetic state through the learning agent's own action-value function has a constraining effect
%\thommen{why is this different from the first one? Cant they be combined?}\manisha{It is different cos this last one is about ensuring the scales are similar. The first one is about getting the same results even if the environment layout changes.} 
such that the action-values and corresponding inferred rewards, lie in a similar scale to that of the learning agent. Often in multi-agent games, knowledge of the independent agent is used by the learning agent to guide its own behaviour. A common approach to achieve this is by constructing a composite reward function, value function or policy \cite{Noothigattu2019,alamdari2021considerate}. In such approaches, ensuring similarity in value scales of the functions being combined is important for stable performance. Fortunately our architecture naturally ensures that the range of the inferred action-value and reward functions will be comparable to those of the learning agent, obviating the need for scaling to construct the composite function. One may also view this characteristic through the lens of inverse reinforcement learning (IRL), where the goal is to map observed behaviours to a reward function. A key challenge here is that there exists a large space of reward functions which could possibly correspond to a given arbitrary behaviour. By inferring the reward function of another agent through the use of the learning agent's own action-value function, our architecture offers an elegant solution to naturally narrow down the space of candidate reward functions, while also enabling analogous features between the agents to be mapped to similar reward ranges.
%\st{An estimate of the independent agent's reward function can thus be derived from the EMOTE action-value function, which will also be of a comparable range to the learning agent's reward function. This characteristic is particularly useful in scenarios involving composite action-value functions or reward functions where the range of the functions being combined is important for stable performance, as is often the case for many multiagent algorithms}. 

We demonstrate our proposal by integrating our EMOTE architecture in the work by Senadeera et.al  \cite{SympathyPaper}. %\st{leverage previous work by} %\st{and adapt it to integrate our EMOTE architecture.} 
Their framework focused on
building considerate learning agents by flexibly combining the learning and independent agents' reward and action-value functions. 
%\st{, thereby providing a platform to illustrate \manisha{two examples of EMOTE's second benefit.}}
%\manisha{ thus enabling the illustration of EMOTE's first benefit. Applied to assistive and adversarial multiagent scenarios,} 
Applied to assistive and adversarial multiagent scenarios, we show that EMOTE produces more stable models of the independent agent that are robust to various settings and layouts.
%\manisha{performance meeting the state-of-the-art.}\thommen{I think its better not to mention SoA because later on, we argue that most of the SoA methods are not applicable}\manisha{What should I say then instead? ..leading to good performance?}\thommen{maybe something like '..leading to consistently good performances in a variety of environment settings, including those involving assistive as well as adversarial agents.' }.
%\thommen{in assistive and adversarial multiagent scenarios?}. 
Further, we observe the Imagination Network is capable of recovering interpretable empathetic states (i.e., finding analogous features between the agents). The primary contribution of this work is an architecture that produces a stable model of the independent agent's action-value function. As a result of our design, the model permits human-interpretability of the inferences made by the learning agent about the independent agent's value function whilst ensuring these inference values are in a similar scale to that of the learning agent. Our method does not aim to outperform current state-of-the-art, rather to produce robust and stable inferences about the independent agent's action-value and reward function. We assume that if inferred correctly, performance result will at least matches the state-of-the-art.

%borrow the concept of building considerate agent by flexibly combining learning agents own reward function with the independent agents reward function, but using the proposed method to infer the other agents reward/value function. Further, we also observe that the imagination network is capable of fully recovering the correct empathetic states (i.e. finding the analogus states/actions between the agents) in many different scenarios. 

\iffalse
Our contributions from this work are:

\begin{itemize}
    \item Developing EMOTE, an explainable architecture to model the action-value function of an independent agent using empathy.
    \item Producing human-interpretable empathetic states to identify analogous \manisha{(the same or similar)} state features between agents.
    \item Ensuring the model of the independent agent produces action values comparable to that of the learning agent, thereby supporting MARL algorithms with composite action-value and reward functions.
    \item Empirical results demonstrating a more robust model, leading to \manisha{comparable performances to the state-of-the-art.} \st{improved performances and more considerate behaviours compared to competing baselines.} %better performance than Sympathy Framework 
\end{itemize}
\fi

%Getting along with everyone isn't always easy, but it is vital for a cohesive and productive society. Communication and understanding are key for smooth social interactions, leading to better cooperation and building trust. Luckily for us evolution has bestowed humans and animals the congitive ability known as empathy. \cite{hoffman1996empathy} defines empathy as \textit{``any process where the attended perception of the object’s state generates a state in the subject that is more applicable to the object’s state or situation than to the subject’s own prior state or situation"}. This definition of empathy can be interpreted as a process which helps us to understand the feelings and goals of another, by using ourself as a point of reference. \textit{How would we feel if we were in a similar situation? What fears and goals do we share? What objects or goals, though not exactly the same, do we feel similarly towards?} Despite our biological talent to empathise however, understanding another can be quite tricky (think of all the times an argument has been sparked because of some small misunderstanding) and we humans rely on a vast streams of information. These are not limited to verbal and written streams, but also encompass behaviours, body language and subtle verbal and physical cues, each of which are small clues to the other's hidden state. Now, if you thought empathy between humans was hard, try between artifical agents! Or trickier still, between a human and an agent! With the increasing prevalence of artifically intelligent systems, particularly in the form of robots or virtual assistants, their presence in shared environments with other agents or humans will necessitate the need to model and understand others around them. In this work we propose an empathy-inspired architecture for modeling `the other'. We hone in on settings where two analagous agents share a space. Of these two, the learning agent (whom we train) tries to model the other `independent agent' by referencing its own rewards and value function. As there is yet no evidence to believe agents have `feelings', we instead utilise our architecture to understand the other's goals (articulated in the form of its hidden reward function). Now simply putting yourself in another's shoes is not enough to empathise (not only because your feet might not fit), but because the preferences we have aren't always exactly the same. Image I am a lover of chocolate (this is infact true), but you enjoy liquorice (which I personally dislike), if I were to see you enjoying liquorice, empathy is unlikely to be my natural reaction - I won't relate to how you feel towards it. However, I could easily \textit{imagine} that it is instead chocolate you are consuming. Then I better understand your enjoyment. This is what we mean when we say analagous. Though we work not with feeling but goals, we take analagous agents to be those with similar or the same goals. Take for example a robot tasked with collecting tin cans, sharing an environment with a robot tasked with collecting rubbish. Though the goal is not the same, empathy can be evoked. As imagination plays an important role in emapthy [REF], we incorporate this into our solution. We demonstrate for further clarity a range of experiments to illustrate the scope of the commonality between agents that our proposal can cater to in the experimental section.

%Our proposed empathy-based architecture comprises of a two stage neural network. The first stage, called the \emph{Imagination Network}, imagines an empathetic representation of the independent agent's state. This \textit{empathetic state} can be understood to be the perception of the independent agent's state, from the perspective of the learning agent. This empathetic state is then fed into a copy of the learning agent's own value function (the second network) and the resulting values are observable. These values come to represent the independent agent's value function. A key benefits of the architecture we proposed is two fold. The first, and key contribution, is that the generated empathetic state is human interpretable. A user can tap into this empathetic state and compare it against the original independent agent's state, observing which features remain the same, and which have changed (like in our chocolate-liquorice analogy). Now before getting to the second benefit, let's make a quick side step and think about why empathy is useful at all. Well, an obvious reason is that is helps us behave considerately towards others. Being kind is not driven by empathy alone. Seeing and understanding that someone is having trouble and needs help, does not automatically evoke considerate behaviour. It is instead the ethical philosophy you abide by that informs how you act based on the empathetic information. This is where the second benefit can comes in. As a result of funneling the empathetic state through the learning agent's own value function, the resulting values can be used to reconstruct or infer an estimate of the independent agent's rewards (the true values we are not privy to). In particular, the scale of these inferred rewards are the same as the rewards of the learning agent. In situations where both rewards are needed, having them be on a comparable scale is important. This is particularly true when it comes to acting according to your ethical framework. For example, if you generally follow a utilitarian framework, both agent's rewards are combined to determine the overall `utility'. Or in the case of the ``Golden Rule'', knowing how you would feel (or in our case, the reward values elicited) in a similar situation. and how you would want another to act towards you, can guide your behaviour.

%As a result, in proposing our empathy-based architecture for modeling the other, we demonstrate its applicability in  two experimental settings where considerate behaviour can be evoked. The first is an assistive environment in which the independent agent requires help from the learning agent to reach its goal (but not vice-versa). The second is an adversarial environment in which the agents can harm each other for additional rewards. We demonstrate that with our proposed framework the rewards inferred go on to successfully train an agent that behaves considerately, whilst also giving access to the ability to inspect the empathetic state induced. 

\iffalse
In human to human interactions, empathy plays a significant role in facilitating our ability to understand how another person feels, which in turn is vital for determining the most considerate response in different social interaction scenarios. %best response to avoid adversely affecting the and through this, helps us figure out how best to respond and act towards them. 
\cite{hoffman1996empathy} defines empathy as \textit{``any process where the attended perception of the object’s state generates a state in the subject that is more applicable to the object’s state or situation than to the subject’s own prior state or situation"}. It is the ability to understand others, %with this understanding being 
based on the knowledge of how oneself would behave and feel when in a similar situation. 

In reinforcement learning \cite{sutton2018reinforcement}, it is common to train an agent to maximise its own rewards. This can however lead to unintended consequences such as harm to others or the environment if the reward functions are poorly designed \cite{amodei2016concrete}. This highlights the need to equip autonomous learning agents with mechanisms to account for the presence of other agents, and interact with them in a cordial and non-intrusive manner. Previous works towards such a pursuit include utilising the learning agent's own model as a proxy model for another agent (independent agent) \cite{TowardsEmpathicDQN}, behaving altruistically towards the independent agent \cite{Franzmeyer2021}, and training from human-provided trajectories corresponding to considerate behaviours \cite{Noothigattu2019}. The most closely related work is that of \cite{SympathyPaper}, who propose designing a sympathy-based framework to enable the learning agent to behave considerately in environments where each agent has differing goals. %The authors train a learning agent who is able to complete its own tasks whilst being sympathetic to the independent agent in the process. 
Our work builds on this framework by drawing inspiration from the concept of empathy. In particular our work focuses on viewing the independent agent's goals and values through the lens of the learning agent's own values (developing an empathetic perspective). To do so we propose an
\textit{empathy-based architecture} to model the value function of the independent agent.
We show that this offers advantages such as improved reward estimates and outputting an interpretable empathetic state representation, as we will explain later in Section \ref{sec:method}.  
%We also assume that the goals of the other agent may be different from those of the learning agent and do not require human feedback to support training. In addition, in many works the rewards of the independent agent are inferred via inverse reinforcement learning (IRL)  \cite{ng2000algorithms} and incorporated with the learning agent's rewards to train a considerate policy. A difficulty of this however is ensuring that the inferred IRL rewards are appropriately comparable to that of the learning agent. Our proposal is able to provide a context-specific estimate of the independent agent's reward and value functions that is comparable to the learning agent.
%\thommen{Here, you could provide a brief summary of previous attempts at addressing this problem (maybe borrow from the sympathy paper, plus any new papers/approaches you want to mention.), and why our work is fundamentally different.}\manisha{Ok, I've added a few lines in above.} %As artificially intelligent agents increase interaction with other agents and humans, the ability be aware of others, and the impact of their actions will be a crucial factor that determines their adoption and acceptance for day-to-day applications. In this work, we draw inspiration from empathy to model this awareness and attempt to address this issue.

%In this work, we train a learning agent to complete its task while behaving considerately towards an independent agent with whom it shares an environment. The independent agent is assumed to behave as per it own policy. 
%We consider the problem of designing an empathy-based approach to encourage the learning agent to either assist or avoid harming the independent agent. \manisha{we aren't desigining a framework though... the sympathy is the framework. Perhaps I can saw "We consider the problem of training a learning agent who is encouraged to assist or avoid harming the independent agent"}\thommen{I think no need for this. It is just repeating what the previous sentence is saying. In fact, you can also remove the previous line "We consider the problem.."}
%We design such an agent for settings where the\learning agent has an opportunity to assist or avoid harming the independent agent, but does not receive any additional positive rewards from doing so.
%To evoke considerate behaviour we present an
%it is desirable for the learning agent to understand the goals of the independent agent in terms of its own reward and value function. It does this by producing an empathetic state, and examining its own reward and value response to this state. By using its own rewards and value function as a reference the learning agent can better infer what the 'feelings' are for the independent agent ensuring they the two agents are comparable. \thommen{One sentence about why it is desirable to follow up the previous sentence} \manisha{Dono how well I did on that one... (added)}%\st{We assume both agents have similar (but not necessarily the same) goals. We refer to this as having symmetric goals.}
%\thommen{Its a bit vague when we say similar but not same. Can we make it clearer? Also, do we need to mention it here? Would it be okay to just mention it in the expt settings section?} \manisha{I have mentioned it in the problem formation section, so I might move it there}
%\st{For example one agent may wish to collect red pellets while the other wishes to collect yellow pellets. We assume there exists at least one sub-optimal policy that can complete the learning agent's own task while being considerate of the independent agent.}
%To do this we present an 
%\textit{empathy-based architecture}\thommen{lets stick to one term - either empathy-based architecture or empathy-based approach or empathy-based framework. You can choose} to model the value function of the independent agent. This model is trained via IRL using observed trajectories of the independent agent. The underlying rewards of the independent agent are then inferred from this value function.
%and used to construct a sympathetic reward (a convex weighted sum of the learning agent's rewards and the independent agent's inferred rewards). These sympathetic reward values are then used to train a sympathetic policy for the learning agent. 

Our proposed empathy-based architecture comprises of a two stage neural network. The first is the \emph{Imagination Network}, which observes the independent agent's state and learns to transform it in such a way that the action taken by the learning agent in this transformed state matches that taken by the independent agent in the observed state of the independent agent. We refer to this as the \textit{empathetic state}. Essentially the empathetic state aims to imagine the independent agent's state from the perspective of the learning agent by asking the question - \emph{``What does the world need to look like in order for the learning agent to act in the same manner as the independent agent?"}. Further, this empathetic state representation could provide a human with insights into how the empathetic agent views other agents/humans. %could possibly be interpreted by a human, providing insights into what changes are required in the state to evoke empathy in the learning agent. %We associate empathy as having been evoked when the learning agent is able to imagine behaving like the independent agent.
 The second stage involves passing the empathetic state through a copy of the learning agent's own value function, the outputs of which are then used to infer the rewards of the independent agent through inverse reinforcement learning (IRL) \cite{ng2000algorithms}. 
 %Following this stage, the empathetic state is fed into the second neural network, which is a copy of the learning agent's own value function. The output of this stage is then used to infer the rewards of the independent agent through inverse reinforcement learning (IRL) \cite{ng2000algorithms}.
 By using the learning agent's own value function estimates in the second stage, we ensure that the resulting inferred reward and value function estimates for the independent agent lie on a scale that is consistent with those of the learning agent. This allows for the rewards of the two agents to be easily combined, as per a previously proposed framework \cite{SympathyPaper} to train a policy to evoke considerate behaviour in the learning agent.  %This is important, as the independent agent's rewards are incorporated with the learning agent's rewards as per 

%function values and, subsequently, inferred reward values for the independent agent that are comparable to the learning agent's own rewards. Additionally a key benefit of our proposal is that the empathetic state can be extracted. This allows a human to observe the empathetic state and potentially gain insights into the resulting value function and inferred rewards.

%\thommen{I think this para could be skipped if you mention other approaches in the para ``In reinforcement learning...". Also, dont you think the more significant contribution is this change in perspective that we introduce via empathy? I feel the other things like same scale of rewards etc., are biproducts of this approach.}\manisha{Re: your comment about the perspective change, yea ok. I'll change that in the wording of the previous paragraph. I'll also remove out the paragraph below}
%\st{Our motivation for the empathy-based architecture is to leverage the knowledge within environments with agents who have symmetric goals. Compared to other IRL methods, our proposal avoids the commonly faced issue of inferring an incorrect value function and subsequent rewards, even though the resulting policy when rolled out accurately mimic's the behaviour of the independent agent.} This is particularly important in situations where the reward and value function of the independent agent are required when training the learning agent to be considerate.

We demonstrate our approach on two types of environments. The first is an assistive environment in which the independent agent requires help from the learning agent to reach its goal (but not vice-versa). The second is an adversarial environment in which the agents can harm each other for additional rewards. We demonstrate that our proposed framework is able to successfully train an agent that behaves considerately in both these scenarios without significantly affecting its ability to complete its own task. 

Our contributions for this work are:

\begin{itemize}
    \item Developing an empathy-based architecture to model the value function of the independent agent, and incorporate it into the existing sympathy framework.
    \item Producing a human interpretable  empathetic state which can be used to understand which state feature changes are required to evoke empathy and explain the associated inferred rewards for the independent agent.
    \item Inferring the independent agent's reward and value function in the same scale as those of the learning agent, allowing for more convenient way to compare and combine rewards of different agents to simulate considerate behaviours.
    %for the independent agent comparable in scale to those of the learning agent, enabling more convenient way for  for \manisha{when goals of agents are analogous}.
    \item Empirical results demonstrating the performance of our proposed approach.
\end{itemize}

\fi