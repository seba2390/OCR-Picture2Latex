% last updated in April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{ruler}
\usepackage{color}
\usepackage{times}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pdfpages}
\usepackage{subcaption}
\captionsetup{compatibility=false}

\usepackage{commath}
\usepackage{placeins}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\newcommand{\vect}[1]{{\bf {#1}}}
\newcommand{\matx}[1]{{\cal {#1}}}
\newcommand{\expect}[1]{{\mathbb{E}}{{\left[{{#1}}\right]}}}
\begin{document}
% \renewcommand\thelinenumber{\color[rgb]{0.2,0.5,0.8}\normalfont\sffamily\scriptsize\arabic{linenumber}\color[rgb]{0,0,0}}
% \renewcommand\makeLineNumber {\hss\thelinenumber\ \hspace{6mm} \rlap{\hskip\textwidth\ \hspace{6.5mm}\thelinenumber}}
% \linenumbers
\pagestyle{headings}
\mainmatter
\def\ECCV18SubNumber{***}  % Insert your submission number here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\newcommand{\todo}[1]{{\bf To do: }{{\em #1}}\\}
\newcommand{\sign}[1]{{\mbox{sign}}{{#1}}}
%\newcommand{\notes}[1]{{\bf Notes:} {\em {#1}}}
%\newcommand{\notes}[1]{}
%\newcommand{\matx}[1]{{\bf {#1}}}
\newcommand{\ea}[0]{{\em et al. }}
\newcommand{\plan}[1] {{\bf Plan:}\\ {\em {#1}}}
\newcommand{\dafkld}[2]{{\mathbb{D}({{{#1}}}\! \mid \! \mid {{{#2}}})}}
%\newcommand{\norm}[1]{{\mid\!\mid\!\!{#1}\!\!\mid\!\mid}}
\newcommand{\pr}[1]{{\rm Pr}[#1]}
%\renewcommand{\thesection}{\Alph{section}}
%\renewcommand{\baselinestretch}{1.2}
\renewcommand{\refname}{}
\renewcommand{\contentsname}{\vspace*{-1.2cm}}
\newcommand{\newterm}[1]{{\bf #1}}
\newcommand{\capsty}[1]{{\em {#1}}}
%\newcommand{\vect}[1]{{\bf {#1}}}
\newcommand{\argmax}[1]{{\begin{array}{c}\mbox{argmax}\\{{#1}}\end{array}}}
\newcommand{\todo}[1]{{{\bf TODO:} {{#1}}}}
\newcommand{\covmat}[1]{{
{\small {\mathsf{Covmat}}
 \left( \left\{ 
{{#1}}
      \right\} \right)
}}}
\newcommand{\mean}[1]{{{\mathsf{mean}}\left(
      \left\{{#1}\right\}\right)}}
%%%%%%%%% TITLE
\title{Quantitative Analysis Shows Cross-Layer Style Transfer is Best}

\titlerunning{ECCV-18 submission ID \ECCV18SubNumber}

\authorrunning{ECCV-18 submission ID \ECCV18SubNumber}
\author{Mao-Chuang Yeh \and Shuai Tang \and Anand Bhattad \and D.A. Forsyth\\
University of Illinois at Urbana Champaign\\
stringtron@gmail.com, stang30@illinois.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both


%\thispagestyle{empty}
\maketitle



%%%%%%%%% ABSTRACT
\begin{abstract}
A popular series of style transfer methods apply a style to a content image by controlling mean and covariance of values in early layers of a feature stack. This is insufficient for transferring styles that have strong structure across spatial scales like, e.g., textures where dots lie on long curves. This paper demonstrates that controlling inter-layer correlations yields visible improvements in style transfer methods.  We achieve this control by computing cross-layer, rather than within-layer, gram matrices. We find that (a) cross-layer gram matrices are sufficient to control within-layer statistics. Inter-layer correlations improves style transfer and texture synthesis. The paper shows numerous examples on "hard" real style transfer problems (e.g. long scale and hierarchical patterns); (b) a fast approximate style transfer method can control cross-layer gram matrices; (c) we demonstrate that multiplicative, rather than additive style and content loss, results in very good style transfer. Multiplicative loss produces a visible emphasis on boundaries, and means that one hyper-parameter can be eliminated.   
\end{abstract}


\section{Introduction}

Style transfer methods apply the {\em style} from one example image to the {\em content} of another; for instance, one might render a camera image (the content) as a watercolor painting (the style). Recent work has shown that highly effective style transfer can be achieved by searching for an image such that early layers of CNN representation match the early layers of the style image and later layers match the later layers of a content image~\cite{gatys2016image}. Content matching is by comparing unit outputs at each location of feature map. But style matching is achieved by comparing summary statistics -- in particular, the gram matrix -- of the layers individually. Comparing gram matrices of individual layers ensures that small, medium and large patterns that are common in the style image appear with about the same frequency in the synthesized image, and that spatial co-occurences between these patterns are about the same in synthesized and style image.  Novak and Nikulin noticed that across-layer gram matrices reliably produce improvement on style transfer. (\cite{novak2016improving}). However, their work was an exploration of variants of style transfer rather than a thorough study to gain insights on style summary statistics.   In this paper, we show (a) methods to evaluate style transfer quantitatively and (b) strong evidence, both experimental and theoretical, that cross-layer gram matrices produce better style transfers.

\textbf{Our contributions:} 
\todo emphasize on building EC evaluation metric, less focus on detail cross-layer multiplicative loss method.
\begin{itemize}
\item We offer comprehensive quantitative evaluation procedures for style transfer methods. We evaluate style transfers on two criteria.  {\bf Effectiveness} measures whether transferred images have the desired style, using divergence between
feature layer distributions.  {\bf Coherence} measures whether the resulting images respect the underlying
decomposition of the content image into objects, using established procedures together with the Berkeley segmentation
dataset BSDS500 \cite{arbelaez2011contour}, and also using a novel measure of segment divergence.
\item We use our measures to compare several style transfer methods quantitatively.  In particular, we show that controlling cross-layer, rather than within-layer, gram matrices produces quantitative  improvements in style transfer over the original method due to Gatys~\cite{gatys2016image}.  
\item We construct good explicit models of the symmetry groups for Gatys' style loss and the cross-layer style loss 
(improving over Risser \ea~\cite{risser2017stable}, who could not construct the groups).  We show experimental evidence that 
the quantitative improvement over Gatys' method is due to the difference in symmetry groups. \todo need further plots to show that. \bf{After removing instability issue of Gatys' method, there are still some intrinsic deferences between Gatys and us, e.g. cross-layer preserve better long scale behavior.  }  
\item We show qualitative evidence suggesting that these quantitative improvements manifest in real images.
\end{itemize}

\section{Related work} \label{sec:gatys}
Bilinear models are capable of simple image style transfer~\cite{Tenenbaum2000} by factorizing style and content representations, but non-parametric methods like patch-based texture synthesis can deal with much more complex texture fields~\cite{Efros2001}.  Image analogies
use a rendering of one image in two styles to infer a mapping from a content image to a stylized
image~\cite{Hertzmann2001}. Researchers have been looking for versatile parametric methods to control style patterns at
different scales to be transferred. Adjusting filter statistics is known to yield texture
synthesis~\cite{debonet,simoncelli}.  Gatys \ea demonstrated that producing neural network layers with particular
summary statistics (i.e Gram matrices) yielded effective texture synthesis~\cite{NIPS2015_5633}. In a following paper,
Gatys \ea achieved style transfer by searching for an image that satisfies both style texture summary statistics and
content constraints~\cite{gatys2016image}. This work has been much elaborated. The search can be replaced with a
regression (at one scale~\cite{Johnson2016Perceptual}; at multiple scales~\cite{wang2016multimodal}; with
cached~\cite{chen2017stylebank} or learned~\cite{dumoulin2016learned} style representations) or a decoding process that
allows efficient adjusting of statistics~\cite{UST,li2018closed}. Search can be sped up with local matching
methods~\cite{chen2016fast}. Methods that produce local maps (rather than pixels) result in photorealistic style
transfer~\cite{Shih2014,Luan2017}. Style transfer can be localized to masked regions~\cite{gatys2016controlling}. The
criterion of matching summary statistics is a Maximum Mean Discrepancy condition~\cite{li2017demystifying}. Style
transfer can be used to enhance sketches~\cite{champandard2016semantic}.  \\

\todo review evaluations metrics on style methods\\

Evaluation of style transfer methods are done primarily with inspection on typically couples of {\em styles} and {\em contents}. We are not aware of quantitative evaluations of the competence style transfer apart from user studies \todo confirm which paper did user study ~\cite{li2018closed} (speed is evaluated in~\cite{}).  But quick adjustment to a method using user studies is difficult in practice.  Objective evaluation such as the edge coherence between {\em contents} and stylized images is investigated in  ~\cite{li2018closed}. Nevertheless, there's a clear absence in numerical investigation on effectiveness of {\em styles} being genuinely transferred in \cite{UST,gatys2016image,Johnson2016Perceptual}.  This may due to the fact that {\em styles} are subjective and more subtle to define than textures, hence such effectiveness metric is hard to choice. For instance, Novak and Nikulin~\cite{novak2016improving} search a range of variant style transfer methods, including cross-layer gram matrices. However, their primary suggestions are adding more layers for more features, and don't pursue on cross-layer gram matrices nor explain its results and quantitatively compare variant modifications. There is a comprehensive review in~\cite{jing2017neural}. 

We review the original work of Gatys \ea \cite{gatys2016image} in detail to introduce notation .
Their method ({\bf Gatys} from now on) finds an image where early layers of a CNN representation match the lower layers of the style image and higher layers match the higher layers of a content image.  Write $I_{s}$ (resp. $I_{c}$, $I_{n}$)  for the style (resp. content, new) image,
and $\alpha$ for some parameter balancing style and content losses ($L_s$ and $L_c$ respectively).  Occasionally, we
will write $I_n^m(I_c, I_s)$ for the image resulting from style transfer using method $m$ applied to the arguments.
We obtain $I_{n}$ by finding
\[
\argmax{I_{n}} L_c(I_{n}, I_{c})+\alpha L_s(I_{n}, I_{c})
\]
Losses are computed on a network representation, with $L$ convolutional layers, where the $l$'th layer
produces a feature map $f^l$ of size $H^l \times W^l \times C^l$ (resp. height, width, and channel number). \todo not sure what the following sentence means. We partition
the layers into three groups (style, content and irrelevant). Then we reindex the spatial variables (height and width) and
write $f^l_{k,p}$ for the response of the $k$'th channel at the  $p$'th location in the $l$'th convolutional layer. The
content loss $L_c$ is 
\[
L_c(I_{n}, I_{c}) = \frac{1}{2}\sum_{c} \sum_{k,p} \norm{f^c_{k,p}(I_{n}) - f^c_{k,p}(I_{c})}^2
\]
(where $c$ ranges over content layers). The {\em within-layer gram
  matrix} for the $l$'th layer is
\[
G_{ij}^l(I) = \sum_p \left[f_{i,p}^l(I)\right]\left[f_{j,p}^l(I)\right]^{T}.
\]
Write $w_l$ for the weight applied to the $l$'th layer.  Then 
\[
L_s^l(I_{n}, I_{s}) = \frac{1}{4{N^l}^2{M^l}^2}\sum_{s}w_l \sum_{i,j}\norm{G^s_{ij}(I_{n})-G^s_{ij}(I_{s})}^2
\]
where $s$ ranges over style layers. Gatys \ea use Relu1\_1, Relu2\_1, Relu3\_1, Relu4\_1, and Relu5\_1 as style layers, and layer Relu4\_2 
for the content loss, and search for $I_{n}$ using L-BFGS~\cite{liu1989limited}.  From now on, we write R51 for Relu5\_1, etc. 

\section{Quantitative Evaluation of Style Transfer}\label{effcoh}

A style transfer method should meet two basic tests.  The first is {\bf effectiveness} -- does the
method produce images in the desired style? The second is {\bf coherence} -- do the resulting images respect the
underlying decomposition of the content image into objects?   While final judgment should belong to the artist, we
construct numerical proxies that can be used to disqualify methods from a final user study. It is essential to test both
properties (excellent results on coherence can be obtained by simply not transferring style at all).
In general, we will apply different treatments to the same set of images, and measure the difference in some statistic that captures the
effect of the treatment (in contrast to true treatment effects, which are complicated by the difficulty of applying two treatments to one patient~\cite{SIM:SIM893}).   

{\bf Effectiveness:}  Assume that a style is applied to a content image  We would like to measure the extent to which the result reflects the style.  There is good evidence that the distribution of features within lower feature layers of a CNN representation is an effective proxy for style.  We expect that individual content images might need to have small biases in the distribution of feature layers to account for the content, but over many images the distribution of features should reflect the style distribution.  In turn, a strong measure of effectiveness of style transfer for a particular image is the extent to which the distribution of feature layer values produced by the transferred image matches the corresponding distribution for the style image. In notation, write $\vect{f}^{l}_{p}(I)$ for the vector of responses  of all channels  at the  $p$'th location in the $l$'th convolutional layer for image $I$. Now choose the $i$'th content image, the $j$'th style, and some method $m$.  The distribution $P_{t, m}$ of $\vect{f}^{l}(I^m_n(I_c^i,I_s^j))$ should be similar to  the distribution $P_s$ of $\vect{f}^{l}(I_s^j)$, with perhaps some smoothing  resulting from the need to meet content demands.   

Testing whether two datasets come from the same, unknown, distribution in high dimensions remains tricky (the method of~\cite{gretton2012kernel} is the current best alternative).  We do not expect the distributions to be exactly the same anyhow;  instead, we want to identify obvious (and so suspicious) large  differences. The symmetry analysis below suggests that Gatys' original method will massively increase the variance of  $\vect{f}^{l}(I^g_n(I_c^i,I_s^j))$.  Observing major differences is straightforward with relatively crude tools.   However, dimension is a problem.  Even assuming that each distribution is normal,  computing KL divergences is impractical, because the distributions are large and so the estimates of the covariance matrices are unreliable.   

However, we seek a statistic that is large with high probability when $P_{t,m}$ and $P_s$ are strongly different, and 
small with high probability when they are similar.  A straightforward construction, after~\cite{DeshpandeCVPR2018} is as follows .
%Choose a set of content images $I_{c}^{i}$ and of style images $I_{s}^{j}$.   
Write $\vect{v}_k$ for a random unit vector.  We then compute
$p_p^m=\vect{v}_k^T\vect{f}_p^{l}(I^m_n(I_c^i,I_s^j))$ and $p_p^s=\vect{v}_k^T\vect{f}_p^{l}(I_s^j)$.  We assume that
these scalar datasets are normally distributed, and compute KL divergence $d(\vect{v}_k)$ from the style distribution to the transferred
distribution.  We now average over $R$ random unit vectors and form
\[
E=-\log \left(\frac{1}{R} \sum_k d(\vect{v}_k)\right)
\]
Large values of this statistic are obtained if there are few random directions in which the two distributions differ;
small values suggest there are many such directions and so that the style transfer may not have succeeded.

{\bf Coherence:} A style transfer method that obliterates object boundaries would make it hard for humans to interpret
the output images, so a reasonable measure of a style transfer method is the extent to which it preserves object
boundaries.  We have two measures of coherence.  Our {\em boundary preservation} measure computes the extent to which a
boundary prediction algorithm produces true object boundaries for a given method, using the Berkeley segmentation dataset tests BSDS500~\cite{arbelaez2011contour}.  Our {\em object coherence} measure computes the extent to which textures are (a) coherent within object boundaries and (b) distinct from object to object.


\todo fix some confusion with Pb, contour detector and Pb evaluation. 

{\em Boundary preservation} is evaluated as a straightforward application of existing methods to evaluate image boundary 
predictors.  We choose a boundary predictor (we used the contour detection of~\cite{arbelaez2011contour}); we apply the style transfer method to images
from the BSDS500, using multiple style images,  to obtain transferred images; we apply the boundary predictor to the
transferred images; and we compute the area under curve (AUC) of the  probability of boundary (Pb) precision-recall curve for every transferred image.  A higher AUC suggests better boundary
preservation.  As section~\ref{results} shows, this measure is highly variable depending on the style that is
transferred, and so we compute a per-transferred image AUC. This evaluation method is not perfect.  Heavily textured styles may confuse the Pb evaluation without confusing human viewers, because the contour detector was not built with very aggressive texture fields in  mind (compare typical style transfer images with the  "natural'' textures used to build BSDS500).  In particular, we might have texture fields that are strongly coherent within each object region and different from region to region, but where the contour detector has great difficulty identifying object boundaries. 

An \textit{object coherence} measure is easy to obtain using the BSDS500 dataset, because each image comes with a ground truth contour
mask.  We choose some layer $l$, and write $\vect{f}_{S,i}=\vect{f}_{i}^{l}(I^g_n(I_c^i,I_s^j), S)$ (for brevity) for a feature
vector in that layer within some segment $S$, and $\left\{\vect{f}_{S,i}\right\}$ for all such feature
vectors.  Write $\mu_S=\mean{\vect{f}_S}$, and $\Sigma_b=\covmat{\mu_S}$ for the between class covariance matrix.
Assume that each segment has the same covariance (heteroskedasticity,
a tolerable assumption given that the method tries to impose a gram matrix on the layer), and construct the within-class
covariance $\Sigma_w=\covmat{\vect{f}_{1,1}-\mu_1, \ldots \vect{f}_{n_S,n_f(S)}-\mu_{n_S}}$.  Now the
largest generalized eigenvalue $\lambda^{\mbox{max}}$ of $(\Sigma_b, \Sigma_w)$ measures the dispersion of the region
textures.   Notice that $\lambda^{\mbox{max}}\geq 0$, and simple plots (supplementary materials) suggest this has a
log-normal distribution over multiple style/content pairs.  We therefore use $L_m=\log \lambda^{\mbox{max}}_{m}$ as a
score to evaluate a method.  Larger values suggest more successful separation of regions.

\section{Cross-layer Style Transfer}\label{sec:Cross}

We consider a style loss that takes into account between layer statistics.  The {\bf cross-layer, additive (ACG)} loss
is obtained as follows.  Consider layer $l$ and $m$, both style layers, with decreasing 
spatial resolution.  
\todo minor notation error on the upsampled feature dimension.
Write $\uparrow f^{m}$ for an upsampling of  $f^m$ to $H^l\times W^l \times K^m$, and consider
\[
G_{ij}^{l,m}(I) = \sum_{p} \left[ f_{i,p}^l(I)\right]\left[\uparrow {f}_{j,p}^{m}(I)\right]^{T}.
\]
as the cross-layer gram matrix, We can form a style loss
\[
L_s(I, I_{s}) = \sum_{(l, m)\in {\cal L}} w^{l}\sum_{ij} \norm{G^{l,m}_{ij}(I)-G^{l,m}_{ij}(I_s)}^2
\]
(where ${\cal L}$ is a set of pairs of style layers).   We can substitute this loss into the original style loss, and
minimize as before.  All results here used a {\em pairwise descending} strategy, where one constrains each layer and its
successor (i.e. (R51, R41); (R41, R31); etc).  Alternatives include an {\em all distinct pairs} strategy, where one constrains all pairs of distinct
layers. Carefully controlling weights for each layer's style loss is not necessary in cross-layer gram matrix scenario.  
{\em Constraint counts} for between layer gram matrix methods are much lower than for within layer methods.  For a
pairwise descending strategy,  we have four cross-layer gram matrices, leading to  control of $64\times
128+128\times 256+256\times 512+512\times 512 = 434176 $ parameters; compare within layer gram matrices, which control 
$64^2+128^2+256^2+2\times512^2 = 610304$ parameters.  The experimental results suggest that the number of constraints is
a poor way of evaluating a method.



\subsection{Symmetries and Stability}
 
 \todo I need more careful reading on the symmetrical group to edit this section so right now I leave it as it is.
 
Symmetries in a style transfer loss function occur when there is a transformation available that changes the style
transferred image without changing the value of the loss function.   Risser \ea note instability in Gatys' method;
symptoms are poor and good style transfers of the same style to the same content with about the same loss value~\cite{risser2017stable}.
They supply evidence that this behavior can be controlled by adding a histogram loss, which breaks the symmetry.
They do not write out the symmetry group as too complicated (~\cite{risser2017stable}, p 4-6).  

In fact, the group is easy to construct.  
The supplementary materials give a construction where the symmetry fixes the gram matrix for a layer and its parent
(deeper networks follow the same lines).  It is necessary to assume the map from layer to layer is linear.  This is not
as restrictive as it may seem; the analysis yields a local construction about any generic operating point of the
network.  In summary, we have:

{\bf Symmetry group, within layer gram matrices, two layers:}
Assuming that the between layer map is affine, with matrix $\matx{M}$
representing the linear component.  With various assumptions about the 
spatial statistics of layer 1 (supplementary materials), the symmetry
group is obtained by:  choose $\vect{b}$ {\em not} of unit length, and
such that $\matx{M}\vect{b}=0$; now factor
$\matx{I}-\vect{b}\vect{b}^T=\matx{A}\matx{A}^T$; choose $\matx{U}$
orthonormal.  Then $(\vect{b}, \matx{A}\matx{U})$ is a symmetry of the
gram matrices in {\em both layers} (i.e the action of this element on
layer 1 fixes {\em both} gram matrices).

{\bf Symmetry group, between layer gram matrix, two layers:}
Assuming that the between layer map is affine, with matrix $\matx{M}$
representing the linear component.  With various assumptions about the 
spatial statistics of layer 1 (supplementary materials), the symmetry
group is obtained by:  choose $\matx{U}$
orthonormal.  Then $(\matx{U})$ is a symmetry of the between layer
gram matrix (i.e the action of this element on layer 1 fixes the
between layer gram matrix).

 The between-layer gram matrix loss has very different symmetries to Gatys'
(within-layer) method.  In particular, the symmetry of Gatys' method rescales
features while shifting the mean (because $\matx{A}$ can contain
strong rescalings with the right choice of $\vect{b}$).   For the
cross-layer loss, the symmetry cannot rescale, and cannot shift the
mean.   This implies that, if one constructs numerous style
transfers with the same style using Gatys' method, the variance of the layer features should be much greater than that
observed for the between layer method.  Furthermore, increasing style weights in Gatys method should result in poor
style transfers, by exaggerating the effects of the symmetry.  These theoretical predictions are confirmed by experiment (figure~\ref{}; and
section~\ref{ecplot}).

% We believe this
% is because style loss is always large, so that minimization will force down large differences (which are large
% difference in values between stylized image and content image) in the content layer. 
% Our experimental results suggest
% that this approach is successful (section~\ref{sec:mul}).  The effect is quite prominent (Figure~\ref{fig:MulCG},
% Figure~\ref{fig:scale2}, Figure~\ref{fig:scale3} ), multiplicative loss has significant advantage of reducing the number
% of parameters that need to be searched over to produce useful results.  Figure~\ref{fig:mulloss_comp} shows style
% transfer results using cross-layer gram matrices and multiplicative loss, we observe distinguishable improvement over
% Gatys' method in preserving content boundaries.  {\em WHAT? WHAT? WHAT?} We find one trick to improve transfer results using multiplicative loss by shifting the mean when creating the new image
% to optimized, we recommend this shift should be the channel mean of style image.







\section{Experimental Procedures}

{\bf Comparison data:}  It is important to do comparisons on a wide range of styles and contents.  We have built
two datasets, using 50 style images and the 200 content  images from the BSDS500  testset.   The {\em main dataset} is
used for most experiments, and was obtained by:  take 20 evenly spaced weight values in the range 50-2000; then, for
each weight value, choose 15 style/content pairs uniformly and at random.  The {\em aggressive weight dataset} is used
to investigate the effect of extreme weights on Gatys method and the ACG method.  This was obtained by:  take 20 weight
values sampled uniformly and at random between 2000-10000; then, for each weight value, choose 15 style/content
pairs uniformly and at random.   For each method, we then produced style transfers using each weight-style-content
triple.  For UST, since the maximum weight is one, we linearly map weights to the zero-one range.
Our sample is sufficient to produce clear differences in standard error bars.  

{\bf Methods:}  We compare:
\noindent {\em Gatys' method} (\cite{gatys2016image} and described above); we use the implementation of \todo{LINK}.\\
\noindent {\em ACG:} We used a {\em pairwise descending}    strategy
We use VGG-16 for both style transfer and texture synthesis.  We use R11, R21, R31, R41, and R51 for style (texture) 
loss, and R42 for the content loss for style transfer. In loss optimization, if it not specified, all stylized images
start from Gaussian noise image and are optimized with LBFGS. \\
\noindent {\em Cross-layer, multiplicative (MCG):}  A natural alternative to combine style and content losses is to
multiply them; we form 
\[
L^m(I_n) = L_c(I_n, I_c) *  L_s(I_n, I_s).
\]
\\
\noindent {\em Gatys' method, with histogram:}  as advocated by \cite{risser2017stable}, we attach a histogram loss to
Gatys method.  We use the implementation of ********\\
\noindent {\em ACG, with histogram:} we apply the histogram loss implementation of ******** to our additive cross-layer
code.\\
\noindent {\em Universal style transfer:}(from \cite{UST});  we use the code of ******\\
\noindent {\em Ensemble Q:}  for each weight-style-content triple, we choose the result that produces the best  $Q=E*C$
over all methods.
\noindent {\em Ensemble E:} for each weight-style-content triple, we choose the result that produces the best  $E$
over all methods.


{\bf Summarizing data with the EC plot:} Comparing methods requires a summary of: the expected effectiveness of a method at any coherence; 
the effect of style and of weight choice on performance; and the extent to which evidence supports a difference between
methods.   We compare methods using an effectiveness-coherence (EC) plot, which plots: (a) a scatter plot of EC pairs obtained for various
style/content/weight triples;   (b) a loess regression curve of E regressed against C for these triples; and (c)
standard error regions for the regression.     Effectiveness is measured per layer (we show layer 1 plots below, others
appear in the supplementary material).  Coherence is measured either using per-image AUC (which does not depend on
layers) or using $L_m$ (defined in section~\ref{effcoh}; this depends on the layer, and we show layer 1 plots below,
with others in the supplementary material).


\begin{figure*}[!ht]
    \includegraphics[width=\linewidth]{DAFFigures/EC-6-AUC-final.pdf}
\caption{\em An EC plot comparing style transfers using cross-layer loss with transfers using Gatys' loss.  Here the C
  statistic is per image AUC.  Point markers show individual image
  statistics, with color keyed
  to the style and size keyed to the weight on the style loss (larger markers are larger
  weights, corresponding to a stronger weight being placed on style transfer).  Notice that some styles are clearly harder than others, and produce
  low E for both methods.  The curves are Loess regression curves of E against C, with shadowed regions showing one
  standard error bars up and down.  For any value of C, there is strong evidence that cross-layer loss obtains on
  average a very much larger E than Gatys' loss (about three standard errors difference); for other losses, the
  improvement is somewhat less marked, but the method is clearly stronger than all others.  Note that cross-layer loss
  achieves an E comparable with simply resizing the style image; on
  average, E slightly less than 4. Best viewed in color.}
\label{ecxvg}
\end{figure*}

\section{Results\label{results}}

We find that
\begin{itemize}
\item Cross-layer style transfer offers the highest expected E values for any C value.  The effect is statistically 
  significant. 
\item E values obtained by cross-layer style transfer are comparable to the E value for the original style.
\item For all methods, style has a very strong effect on the performance of the transfer.
\item Histogram losses cause no significant improvement in Gatys' method, and weaken the performance of cross-layer
  style transfer.  This may be an effect of the loss symmetry.
\item Neither ensemble method outperforms cross-layer style transfer. 
\item Aggressive weights cause Gatys' method to underperform significantly, likely an effect of the loss symmetry.
\end{itemize}


{\bf ACG is better:} Figure~\ref{ecxvg} shows an EC plot of layer 1 comparing style transfers using the cross-layer loss (ACG)
with transfers using five other non-ensemble losses. Note that cross-layer loss achieves much higher average E for a
given value of C.  In various parts of the AUC range, additive cross-layer loss is somewhat outperformed by the
multiplicative version, but in the high AUC regime it is much better.  The difference to every other method ranges from
one to four standard errors over the range; the method is clearly significantly better.

{\bf Control:}  Figure~\ref{control} shows an EC plot of two controls.  In the first, the resized style image is reported as a
transfer; this results as expected in high values of E, but low values of C.  In the second, the content image is
reported as a transfer; this results in low values of E, but high values of C (though not uniformly; some images remain
hard to segment).  

\begin{figure*}[!htbp]
    \includegraphics[\linewidth]{DAFFigures/L1cont.pdf}
\caption{\em An EC plot comparing two control methods.  One reports the resized style image as the transfer (small diamonds,
  yellow curve) and the other reports the content image as the transfer (large dots, blue curve).  Colors are keyed to
  style.  There is significant variance in E for the resized style image, an effect due to resizing.  However, the range
  of E's shows the size of the effect of resizing on E; on average, E slightly greater than 4, only slightly larger
than that achieved by the cross-layer loss. The curves are Loess regression curves of E against C, with shadowed regions showing one
  standard error bars up and down. Scale is the same as Figure \protect \ref{ecxvg}.   }
\label{control}
\end{figure*}

{\bf Style effects:} In figure~\ref{ecxvg}, note that some styles are hard for all methods (markers of many different
sizes and the same color in the lower, lower-left sections of the plot).  This does not appear to be an effect of the
choice of weight (these markers are of various sizes).  This point does not seem to have been noted before, perhaps
because experimentation consists of displaying good/random/bad sets of transfers.

{\bf Histogram losses:} improve Gatys' method (compare green/light green on
figure~\ref{ecxvg}) only at extreme weights and low C. This may be an effect of the loss symmetry, explained below.
They weaken the performance of cross-layer  style transfer (compare red/pink on figure~\ref{ecxvg}, about three standard errors).  

{\bf Ensemble methods:} do not outperform cross-layer style transfer (see figure~\ref{ensmeth}).  As comparing the red
and cyan curves suggests, choosing the result with the best E is essentially the same as using the cross-layer style
transfer result.  The yellow curve shows the ensemble Q method, which works somewhat better than cross-layer style
transfer in low C regimes, and somewhat worse in high C regimes.  This suggests that more sophisticated ensemble methods
might yield even better performance.


\begin{figure*}[!htbp]
    \includegraphics[width=0.8 \linewidth]{DAFFigures/EC-ensemble-includingH.pdf}
\caption{\em An EC plot comparing two ensemble methods to cross-layer additive loss.  
Scale is the same as Figure \protect \ref{ecxvg}.   }
\label{ensmethl}
\end{figure*}

{\bf Aggressive weights:} One might speculate that Gatys' method underperforms because the weight regime is
inappropriate.  Figure~\ref{aggweight} compares Gatys' method to cross-layer style transfer.   Notice
  that large weights cause serious trouble for Gatys' method.  We believe that this is because large weights on the
  style loss cause the symmetry in Gatys' loss to manifest itself, resulting in significant rescaling of features.
In particular, Gatys' method cannot achieve high E values,
because symmetry in the style loss produces feature distributions that are quite different from that desired.
Furthermore, larger weights on the style loss {\em do not} produce better style transfers (large diamonds toward the
  bottom left of the plot).  Instead, by exaggerating the effect of the symmetry, large weights produce transfers that
  both have low E (poor transfer) {\em and} low C (do not respect original segmentation).  


\begin{figure*}[!htbp]
    \includegraphics[width=0.8 \linewidth]{DAFFigures/ECga.pdf}
\caption{\em An EC plot comparing Gatys' method to cross-layer style transfer for the aggressive weight dataset.  Notice
  that large weights cause serious trouble for Gatys' method (large diamonds clustered in the bottom left corner).
  Cross-layer style transfer outperforms Gatys' method over all of its range, mostly by many standard errors.
Scale is the same as Figure \protect \ref{ecxvg}.   }
\label{aggweight}
\end{figure*}
% \begin{figure*}
%   \includegraphics[width=\linewidth]{DAFFigures/EC-ev-1.pdf}
%   \caption{An EC plot  comparing different style transfer methods with midrange weights on the first convolutional layer of VGG16. Here C is the largest Eigenvalues. EC plot for the rest of 4 layers are shown in supplementary materials}
% \end{figure*}


% \begin{figure*}
%   \includegraphics[width=\linewidth]{DAFFigures/EC-ev-a-1.pdf}
%   \caption{An EC plot  comparing different style transfer methods with aggressive weights on the first convolutional layer of VGG16. Here C is the largest Eigenvalues. EC plot for the rest of 4 layers are shown in supplementary materials}
% \end{figure*}


\subsection{Qualitative Results}\label{sec:mul}


\begin{figure}[!htbp]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth, height=\linewidth]{./CrossGatys/style_75.png}
    %\caption{Style.}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Gatys/GatysBased_content122_style75.png}
    %\caption{ours.}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{CrossGatys/AddCrossGatys_content122_style_style75.png}
    %\caption{WCT.}
  \end{subfigure}

  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth, height=\linewidth]{CrossGatys/style_48.png}
    %\caption{Style.}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Gatys/GatysBased_content122_style48.png}
    %\caption{ours.}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{CrossGatys/AddCrossGatys_content122_style_style48.png}
    %\caption{WCT.}
  \end{subfigure}

  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth, height=\linewidth]{CrossGatys/style_6.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Gatys/GatysBased_content122_style6.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{CrossGatys/AddCrossGatys_content122_style_style6.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth,
    height=\linewidth]{CrossGatys/style_117.png}
\caption{Style}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Gatys/GatysBased_content122_style117.png}
\caption{Within-Layer}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{CrossGatys/AddCrossGatys_content122_style_style117.png}
\caption{Cross-Layer}
  \end{subfigure}  
  \caption{\em {\bf Left:} styles to transfer; {\bf center:} results using within-layer
    loss; {\bf right} results using cross-layer loss.  There are
    visible advantages to using the cross-layer loss. Note how cross-layer preserves large black areas (top row); 
creates an improved appearance of relief for the acrylic strokes (second row); preserves the overall structure of the
rods (third row); and ensures each string has a dot on each end (fourth row).
  }\label{fig:cf1}
\end{figure}

\begin{figure}[!htbp]
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth, height=\linewidth]{./CrossGatys/style_84.png}
    %\caption{Style.}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Gatys/GatysBased_content122_style84.png}
    %\caption{ours.}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{CrossGatys/AddCrossGatys_content122_style_style84.png}
    %\caption{WCT.}
  \end{subfigure}


  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth, height=\linewidth]{CrossGatys/style_86.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Gatys/GatysBased_content122_style86.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{CrossGatys/AddCrossGatys_content122_style_style86.png}
  \end{subfigure}
  
  
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth, height=\linewidth]{oldtable/style79.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{oldtable/Gatys_style79.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{oldtable/AddCross_style79.png}
  \end{subfigure}  
  
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth, height=\linewidth]{oldtable/style8.png}
\caption{Style}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{oldtable/Gatys_style8.png}
\caption{Within-Layer}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{oldtable/AddCross_style8.png}
\caption{Cross-Layer}
  \end{subfigure} 
    \caption{\em {\bf Left:} styles to transfer; {\bf center:} results using within-layer
    loss; {\bf right:} results using cross-layer loss.  There are
    visible advantages to using the cross-layer loss. Note how cross-layer preserves the shape of the abstract color blocks (top row); 
avoids smearing large paint strokes (second row); preserves the overall structure of the curves as much as possible
(third row); and produces color blocks with thin boundaries (fourth row).
  }\label{fig:cf2}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[clip, trim=3.5cm 9.4cm 8cm 4.5cm, width=\linewidth]{PlotsNCharts/Layer-wise_compare_withinGramNcrossGram.pdf}


\caption{\em This figure shows what happens when one controls only one (or one pair) of layers with the style loss.
{\bf Left:} controlling a single layer, with a within-layer gram matrix.  {\bf Center:} controlling two
layers in sequence, but each with a within-layer gram matrix.  {\bf Right:} controlling a two layers
in sequence, but using only a cross-layer gram matrix.  Notice that, as one would expect, controlling
cross-layer gram matrices results in more pronounced effects and a wider range of spatial scales of effect.
Furthermore, in comparison to controlling  a pair of within-layer gram matrices, one is controlling fewer
parameters.}
\label{fig:layer_wise}
\end{figure}


\begin{figure*}[!h]
\centering
\includegraphics[width=0.98\linewidth]{PlotsNCharts/good_examples_mul_loss.pdf}
\caption{\em \todo{THIS FIGURE SHOULD HAVE ADDITIVE LOSS} Style transfer examples demonstrate the advantages of our method (\textbf{right} image in every example) compared to Gatys' method \cite{gatys2016image} (\textbf{left} image). 
We use cross-layer gram matrices and multiplicative loss (section \ref{sec:Cross}). Notice our method preserves prominent content boundaries (e.g. the tower and the boatman), while transferring patterns from the style image more completely. Our method excels at transferring style with long scale coherence (example 1,2); and preserves the appearance of material relief (example 3,4).}
\label{fig:mulloss_comp}
\end{figure*}

\begin{figure}[!htbp]
\centering
\small 
\begin{subfigure}[t]{0.3\linewidth}
    \includegraphics[width=\linewidth]{CrossGatys/AddCrossGatys_content122_style_style48.png}
    \includegraphics[width=\linewidth]{MulCG/MulCrossGatys_content122_style48.png}
\end{subfigure}
\begin{subfigure}[t]{0.3\linewidth}
    \includegraphics[width=\linewidth]{CrossGatys/AddCrossGatys_content122_style_style6.png}
    \includegraphics[width=\linewidth]{MulCG/MulCrossGatys_content122_style6.png}
\end{subfigure}
\begin{subfigure}[t]{0.3\linewidth}
    \includegraphics[width=\linewidth]{CrossGatys/AddCrossGatys_content122_style_style117.png} 
    \includegraphics[width=\linewidth]{MulCG/MulCrossGatys_content122_style117.png}
\end{subfigure}

\caption{\em 
 Multiplicative loss produces good style transfer results.
{\bf Top row:} style transfers using cross layer gram matrices and additive loss, with a good choice of $\alpha$. {\bf
  Bottom row:} style transfers using cross layer gram matrices and multiplicative loss, where no choice of $\alpha$ is required. Notice the emphasis of  content outline in the multiplicative loss images. 
}
  \label{fig:MulCG}
\end{figure}


% \textbf{Gatys:}

% \textbf{universal style transfer:}

% \textbf{Cross-layer,  additive loss:}

% \textbf{Cross-layer, multiplicative loss:}

% \textbf{Histogram loss + Gatys:}

% \textbf{Histogram loss + Cross-layer:}





% {\bf Cross-layer vs. within-layer style loss:}  Figures~\ref{fig:cf1},~\ref{fig:cf2} compare style transfers using within-layer gram matrices
% and cross-layer gram matrices with a pairwise descending strategy. Cross-layer gram matrices are particularly good at
% preserving relations between effects, as the detail in figure~\ref{fig:cf2} shows.


% {\bf Multiplicative loss:} The multiplicative loss often produces visual pleasing style transfer results by showing better style pattern arrangement at same time keeping the outline of content relatively intact, so that the generated image preserves the perceptual meaning of the content while showing coherent style patterns(Figure~\ref{fig:MulCG}); More examples are present in supplementary materials.  

% {\bf Pairwise descending vs all pairs distinct:}  All pairs distinct cross-layer style transfer seems to produce
% improvements over pairwise descending (Figure~\ref{fig:CGALL}).  This is in some contrast to Novak and Nikulin's findings
% (\cite{novak2016improving}, p5), which suggest ``tying distant ... layers produces poor results''.  

% {\bf FCT vs WCT:}  Fast universal cross-layer transfer (FCT) works visually better than the original WCT method of Li \ea ~\cite{UST}, as Figure~\ref{fig:WCT1} shows.  However, FCT has some of the same difficulties that WCT has.  Both methods have
% difficulty reproducing crisp subshapes in styles. 

% {\bf Individual style loss control:} When one controls style loss using a single layer (or a single pair of
% layers). We can clearly see how they effect stylized images (Figure~\ref{fig:layer_wise}). We observe higher level
% style loss shows stronger control over long scale patterns from style images, this is in agreement with similar
% observations in \cite{gatys2016image}. We also found that cross-layer gram matrices have stronger ability in
% preserving prominent boundaries of content images while display equal or better control over long scale style patterns
% compared to the same level within-layer gram matrices.  


\begin{figure}[!htbp]
\centering
\small 
\begin{subfigure}[t]{0.3\linewidth}
    \includegraphics[width=\linewidth]{details/MulCrossGatyscontent122style86.jpg}
    \includegraphics[width=\linewidth]{details/MulALLCrossGatys600content122style86.jpg}
\end{subfigure}
\begin{subfigure}[t]{0.3\linewidth}
    \includegraphics[width=\linewidth]{details/MulCrossGatyscontent122style6.jpg}
    \includegraphics[width=\linewidth]{details/MulALLCrossGatys600content122style6.jpg}
\end{subfigure}
\begin{subfigure}[t]{0.3\linewidth}
    \includegraphics[width=\linewidth]{details/MulCrossGatyscontent122style117.jpg}
    \includegraphics[width=\linewidth]{details/MulALLCrossGatys600content122style117.jpg}
\end{subfigure}
\caption{\em All pairs distinct cross-layer style transfer yields somewhat better results than descending pairs.  {\bf
    Top row:} cross-layer style transfer using descending pairs (i.e. (R51, R41); (R41, R31); (R31, R21); (R21, R11)).
  {\bf Bottom row:}  cross-layer style transfer using all pairs distinct (i.e  all distinct pairs from R51...R11).
There are fewer bubbles; color localization and value is improved; and line breaks are fewer.
  \label{fig:CGALL}}
\end{figure}



\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{WCT/style_catAl_cat12_WCT_in2_styles53.png}
     \caption{\em Our method shows better color grouping in the stylized image.}
  \end{subfigure}
  \begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{WCT/style_catAl_cat12_WCT_in2_styles83.png}
    \caption{\em  Many black spots in original WCT, which is not observed in our method.}
  \end{subfigure}
  \begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{WCT/style_catAl_cat12_WCT_in2_styles95.png}
    \caption{\em  Ours improved the color contrast, because cross-layer gram matrices preserve longer scale color pattern.}
  \end{subfigure}
  \begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{WCT/style_catAl_cat12_WCT_in2_styles109.png}
    \caption{\em Note our method does not have the blue color shift present in WCT.}
  \end{subfigure}
  \begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{WCT/style_catAl_cat12_WCT_in2_styles118.png}
    \caption{\em WCT has many artificial pattern which is not seen in original style image, and ours largely reduce it.}
  \end{subfigure}
  \begin{subfigure}[b]{\linewidth}
    \includegraphics[width=\linewidth]{WCT/style_catAl_cat12_WCT_in2_styles7.png}
    \caption{\em Color blocks are better organized in ours.}
  \end{subfigure}
  \caption{\em In each row, {\bf first:} the style image; {\bf second:} transfer using FCT with descending sequences
    (i.e. (R51, R41, R31, R21, R11); (R41, R31, R21, R11); (R31, R21, R11); etc); {\bf third:} transfer using FCT with pairwise descending sequences (i.e. (R51, R41); (R41, R31); (R31, R21); and (R21, R11)); {\bf fourth} transfer using
    WCT \protect \cite{UST}
  \label{fig:WCT1}}
\end{figure}



% {\bf Scales:}  A crop of the style image will effectively result in transferring larger style elements.  We expect that,
% when style elements are large compared to the content, cross-layer methods will have a strong advantage because they
% will be better able to preserve structural relations that make up style elements.   Qualitative evidence supports this
% view (Figure~\ref{fig:scale2} and Figure~\ref{fig:scale3}).

\begin{figure}[h!]
  \centering %Hard_to_easy/styles_-_101.jpg
	\begin{subfigure}[b]{0.157\linewidth}
    \includegraphics[width=\linewidth]{Hard_to_easy/styles_-_101.jpg}
    \caption{Style}
  \end{subfigure}
  \begin{subfigure}[b]{0.27\linewidth}
    \includegraphics[width=\linewidth, height=\linewidth]{Hard_to_easy/MulCrossGatys_content122_style632_imagesize768.png}
    \includegraphics[width=\linewidth]{Hard_to_easy/MulGatys_content122_style632_imagesize768.png}
    \caption{Style size 768}
  \end{subfigure}
  \begin{subfigure}[b]{0.27\linewidth}
    \includegraphics[width=\linewidth]{Hard_to_easy/MulCrossGatys_content122_style632_imagesize512.png}
    \includegraphics[width=\linewidth, height=\linewidth]{Hard_to_easy/MulGatys_content122_style632_imagesize512.png}
    \caption{Style size 512}
  \end{subfigure} 
  \begin{subfigure}[b]{0.27\linewidth}
    \includegraphics[width=\linewidth]{Hard_to_easy/MulCrossGatys_content122_style632_imagesize256.png}
    \includegraphics[width=\linewidth]{Hard_to_easy/MulGatys_content122_style632_imagesize256.png}
    \caption{Style size 256}
  \end{subfigure}

  \caption{\em  Each row of stylized images shows a  transfer with the same style, but where the style image has been cropped to
    different sizes (style elements are {\em large} (=edge length 768), {\em medium} (=edge length 512) and {\em small}
    (=edge length 256), reading left to right).  The first row shows cross layer loss, the second row within layer loss.
    Note that, when style elements are large, the cross-layer loss is better at preserving their structure (e.g.,
  the large circles have fewer wiggles, etc.). Loss is multiplicative, notice the emphasis on outlines from multiplicative loss.
  }
  \label{fig:scale2}
\end{figure}


\begin{figure}[h!]
  \centering

  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Hard_to_easy/MulCrossGatys_content122_style601_imagesize768.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Hard_to_easy/MulCrossGatys_content122_style601_imagesize512.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Hard_to_easy/MulCrossGatys_content122_style601_imagesize256.png}
  \end{subfigure}

  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Hard_to_easy/MulGatys_content122_style601_imagesize768.png}
    \caption{Style size 768}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Hard_to_easy/MulGatys_content122_style601_imagesize512.png}
    \caption{Style size 512}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\linewidth]{Hard_to_easy/MulGatys_content122_style601_imagesize256.png}
    \caption{Style size 256}
  \end{subfigure}
\caption{\em  Each row shows a style transfer with the same style, but where the style image has been cropped to
    different sizes (style elements are {\em large} (=edge length 768), {\em medium} (=edge length 512) and {\em small}
    (=edge length 256), reading left to right).  The first row shows cross layer loss, the second row within layer loss.
    Note that, when style elements are large, the cross-layer loss is better at preserving their structure (e.g., the long scale color coherence is preserved, and
  the large paint strokes have more detail and more relief etc.)  Loss is multiplicative, notice the emphasis on outlines from multiplicative loss.
 }
  \label{fig:scale3}
\end{figure}





\begin{figure*}[!htbp]
\centering
\small 

	\includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content31_style10_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content31_style151_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content31_style161_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content31_style97_weight0_000000_loss0_iteration400.png}
    
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content31_style40_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content31_style48_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content31_style67_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content31_style95_weight0_000000_loss0_iteration400.png}
\caption{good horse.  }
\label{fig:good1}
\end{figure*}

\begin{figure*}[!htbp]
\centering
\small 

	\includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content33_style10_weight0_000000_loss1_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content33_style151_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content33_style161_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content33_style40_weight0_000000_loss0_iteration400.png}
    
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content33_style48_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content33_style62_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content33_style86_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content33_style67_weight0_000000_loss0_iteration400.png}
    
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content33_style6_weight0_000000_loss1_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content33_style89_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content33_style95_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content33_style85_weight0_000000_loss29926_iteration400.png}
    
        \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content33_style100_weight0_000000_loss19126_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content33_style128_weight0_000000_loss33741_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content33_style59_weight0_000000_loss36060_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content33_style88_weight0_000000_loss181419_iteration400.png}
\caption{good sea food.  }
\label{fig:good1}
\end{figure*}

\begin{figure*}[!htbp]
\centering
\small 

	\includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content36_style10_weight0_000000_loss34809_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content36_style151_weight0_000000_loss2128_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content36_style161_weight0_000000_loss2255_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content36_style40_weight0_000000_loss9278_iteration400.png}
    
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content36_style48_weight0_000000_loss15369_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content36_style86_weight0_000000_loss14895_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content36_style97_weight0_000000_loss21764_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content36_style95_weight0_000000_loss9644_iteration400.png}
    
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content36_style6_weight0_000000_loss39099_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content36_style122_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatysLargerWeightHigherlayer_content36_style62_weight0_000000_loss11151_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content36_style67_weight0_000000_loss0_iteration400.png}
\caption{good mountain.  }
\label{fig:good1}
\end{figure*}

\begin{figure*}[!htbp]
\centering
\small 

	\includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style100_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style10_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style122_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style40_weight0_000000_loss0_iteration400.png}
    
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style59_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style48_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style62_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style67_weight0_000000_loss0_iteration400.png}
    
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style85_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style6_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style86_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style89_weight0_000000_loss0_iteration400.png}
    
        \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style88_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style94_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style97_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style95_weight0_000000_loss0_iteration400.png}
    
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style129_weight0_000000_loss5_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style151_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style161_weight0_000000_loss0_iteration400.png}
    \includegraphics[width=0.24\linewidth]{good/MulCrossGatys_content196_style311_weight0_000000_loss0_iteration400.png}
\caption{good mountain.  }
\label{fig:good1}
\end{figure*}



% In particular, because we need $\expect{C}$, we can sample style/image/weight
% triples at random, and still obtain an accurate estimate, as long as there are enough samples to keep the standard error
% of the estimate low. 



% \begin{figure*}
%   \includegraphics[width=\linewidth]{DAFFigures/ECcmp.pdf}
%   \caption{An EC plot  comparing different style transfer methods. Here C is boundary AUC)}
% \end{figure*}


% \begin{figure*}
%   \includegraphics[width=\linewidth]{DAFFigures/EC-ensemble-auc.pdf}
%   \caption{comparing cross-layer additive (the best) to something that chooses best E result
% from all four (xa, xm, g, f) and something that chooses best Q (ExC) from result.}
% \end{figure*}





\section{Conclusion}
  

\section*{Supplementary: Symmetries and Stability: Details}


The between-layer gram matrix loss has very different symmetries to the within-layer method.
This is most easily seen by writing out the group.  We will confine our analysis to two layers
(deeper networks follow the same lines).   Risser et al ascribe instability in Gatys' method to
symmetries in the Gram matrix, but do not write out the symmetry group as too complicated
(~\cite{}, p*).  In fact, the group is easy to construct with the right choice of coordinate system.
Various simplifications are required to understand the effects of a
second layer (details below).

This difference in symmetry groups is important.  Risser argues that
the symmetries of gram matrices in Gatys' method could lead to
unstable reconstructions; they control this effect using feature
histograms.  What causes the effect is that the symmetry rescales features while shifting the mean (eq *).  
For the cross-layer loss, the symmetry cannot rescale, and cannot shift the mean (eq *).  In turn, the instability
identified in that paper does not apply to the cross-layer gram matrix and our results could not be improved by adopting
a histogram loss.

Write $\vect{x}_{i}$, (resp $\vect{y}_i$ for the feature vector at the $i$'th location (of $N$ in total)
in the first (resp second) layer.  Write $\matx{X}^T=\left[\vect{x}_{1}, \ldots,
\vect{x}_{N}\right]$, etc.   

{\bf Symmetries of the first layer:} Now assume that the first layer has been normalized to zero mean and
unit covariance.  There is no loss of generality, because the whitening transform
can be written into the expression for the group. Write ${\cal
  G}(\matx{W})=(1/N)\matx{W}^T\matx{W}$ for the operator that forms
the within layer gram matrix. We have ${\cal G}(\matx{X})=\matx{I}$.   
Now consider an affine action on layer 1, mapping $\matx{X}_1$ to $\matx{X}_1^*=\matx{X}_1 \matx{A}+\vect{1}\vect{b}^T$; then for this to be a symmetry, we must have
$G(\matx{X}_1^*)= \matx{A}\matx{A}^T+\vect{b}\vect{b}^T=\matx{I}$.  In
turn, the symmetry group can be constructed by: choose $\vect{b}$
which does not have unit length; factor
$N(\matx{I}-\vect{b}\vect{b}^T)$ to obtain $\matx{A}(\vect{b})$ (for
example, by using a cholesky transformation); then any element of the
group is a pair $\left(\vect{b}, \matx{A}(\vect{b})\matx{U}\right)$
where $\matx{U}$ is orthonormal.  Note that factoring will fail for
$\vect{b}$ a unit vector, whence the restriction. 

{\bf The second layer:}   We will assume that the map between layers
of features is linear.  This assumption is not true in practice, but major
differences between symmetries observed under these conditions likely
result in differences when the map is linear.  We can analyze for two
cases: first, all units in the map observe only one input feature
vector (i.e. 1x1 convolutions; the {\em point sample} case); second, spatial homogeneity in the
layers.

{\bf The point sample case:} Assume that every unit in the map
observes only one input feature from the previous layer (1x1
convolutions).   We have $\matx{Y}=\matx{X}\matx{M}+\vect{1}\vect{n}^T$, because the map 
between layers is linear. Now consider the effect on the second layer.
We have ${\cal G}(\matx{Y})=\matx{M}\matx{M}^T+\vect{n}\vect{n}^T$.
Choose some symmetry group element for the first layer,
$\left(\vect{b}, \matx{A}\right)$.  The gram matrix for the second
layer becomes ${\cal G}(\matx{Y}^*)$, where
$\matx{Y}^*=(\matx{X} \matx{A}+\vect{1}\vect{b}^T)\matx{M}^T+\vect{1}\vect{n}^T.$
Recalling that $\matx{A}\matx{A}^T+\vect{b}\vect{b}^T=\matx{I}$ and
$\matx{X}^T\vect{1}=0$, we have 
\[
{\cal
  G}(\matx{Y}^*)=\matx{M}\matx{M}^T+\vect{n}\vect{n}^T+\vect{n}\vect{b}^T\matx{M}^T+\matx{M}\vect{b}\vect{n}^T
\]
so that ${\cal G}(\matx{X}_2^*)={\cal G}(\matx{X}_2)$ if
$\matx{M}\vect{b}=0$.  This is relatively easy to achieve with
$\vect{b}\neq 0$.

{\bf Spatial homegeneity:} Now assume the map between layers has
convolutions with maximum support $r \times r$.  Write $u$ for an
index that runs over the whole feature map, and $\psi(\vect{x}_u)$ for
a stacking operator that scans the convolutional support in fixed
order and stacks the resulting features. For example, given a 3x3
convolution and indexing in 2D, we might have
\[
\psi(\vect{x}_{22})=\left(\begin{array}{c}\vect{x}_{11}\\
\vect{x}_{12}\\
\ldots\\
\vect{x}_{33}
\end{array}\right)
\]

In this case, there is some $\matx{M}$, $\vect{n}$ so that 
$\vect{y}_u=\matx{M}\psi(\vect{x}_u)+\vect{n}$.  We ignore the
effects of edges to simplify notation (though this argument will go
through if edges are taken into account?).  Then there is some
$\matx{M}$, $\vect{n}$ so we can write 
\[
{\cal G}(\matx{Y})=(1/N) \sum_u
\matx{M}\psi(\vect{x}_u)\psi(\vect{x}_u)^T\matx{M}^T+\vect{n}\vect{n}^T
\]
Now assume further that
layer 1 has the following (quite restrictive) spatial homogeneity
property: for pairs of feature vectors within the layer $\vect{x}_{i,j}$, $\vect{x}_{i+\delta,
  j+\delta}$ with $\mid \! \delta\!\mid \leq r$ (ie within a convolution window of one
another), we have $\expect{\vect{x}_{i,j}\vect{x}_{i+\delta,
    j+\delta}}=\matx{I}$.  This assumption is consistent with image
autocorrelation functions (which fall off fairly slowly), but is still
strong. Write $\phi$ for an operator
that stacks $r \times r$ copies of its argument as appropriate, so
\[\phi(\matx{I})=\left(\begin{array}{ccc}
\matx{I}&\ldots&\matx{I}\\
\ldots &\ldots \ldots \\
\matx{I}&\ldots&\matx{I}
\end{array}\right).
\]
Then
$G(\matx{Y})=\matx{M}\phi(\matx{I})\matx{M}^T+\vect{n}\vect{n}^T$.
If there is some affine action on layer 1, we have
$G(\matx{Y}^*)=\matx{M}\left(\psi(\matx{A})\phi(\matx{I})\psi(\matx{A}^T)+\psi(\vect{b})\psi(\vect{b}^T)\right)\matx{M}^T+\vect{n}\vect{n}^T$,
where we have overloaded $\psi$ in the natural way.  Now if
$\matx{M}\psi(\vect{b})=0$ and $\matx{A}\matx{A}^T+\vect{b}\vect{b}^T=\matx{I}$, ${\cal
  G}(\matx{Y}^*)={\cal G}(\matx{Y})$. 



{\bf The cross-layer gram matrix:}  Symmetries of the cross-layer gram matrix are very different.  Write
${\cal G}(\matx{X}, \matx{Y})=(1/N) \matx{X}^T\matx{Y}$ for
the cross layer gram matrix.  

{\bf Cross-layer, point sample case:} Here (recalling $\matx{X}^T\vect{1}=0$)we have ${\cal G}(\matx{X},
\matx{Y})=\matx{M}^T$.    Now choose some symmetry group element for the first layer,
$\left(\matx{A}, \vect{b}\right)$.  The cross-layer gram matrix
becomes 
\begin{eqnarray*}
{\cal G}(\matx{X}^*, \matx{Y}^*)&=&(1/N) (\matx{A}
\matx{X}^T+\vect{b}\vect{1}^T)
\left[(\matx{X}
  \matx{A}^T+\vect{1}\vect{b}^T)\matx{M}^T+\vect{1}\vect{n}^T\right]\\
&=&\matx{M}^T+\vect{b}\vect{n}^T
\end{eqnarray*}
(recalling that $\matx{A}\matx{A}^T+\vect{b}\vect{b}^T=\matx{I}$ and
$\matx{X}^T\vect{1}=0$).  But this means that the symmetry requires
$\vect{b}=\vect{0}$; in turn, we must have $\matx{A}\matx{A}^T=\matx{I}$.


{\bf Cross-layer, homogeneous case:} We have
\[
{\cal G}(\matx{X}, \matx{Y})=(1/N)\sum_u \vect{x}_u\left[
  \psi(\vect{x}_u)^T\matx{M}^T+\vect{n}^T\right]=\matx{M}^T.\]
Now choose some symmetry group element for the first layer,
$\left(\matx{A}, \vect{b}\right)$.  The cross-layer gram matrix
becomes 
\begin{eqnarray*}
{\cal G}(\matx{X}^*, \matx{Y}^*)&=&(1/N)\sum_u \left(\matx{A} \vect{x}_u+\vect{b}\right)\left[\left(\psi(\vect{x}_u)^T\psi(\matx{A}^T)+\psi(\vect{b})\right)\matx{M}^T+\vect{n}^T\right]\\
&=&\matx{M}^T+\vect{b}\vect{n}^T
\end{eqnarray*}
(recalling the spatial homegeneity assumption, that $\matx{A}\matx{A}^T+\vect{b}\vect{b}^T=\matx{I}$ and
$\matx{X}_1^T\vect{1}=0$).  But this means that the symmetry requires
$\vect{b}=\vect{0}$; in turn, we must have
$\matx{A}\matx{A}^T=\matx{I}$.


\subsection{Texture synthesis} 
\todo maybe we should move texture results in to supplementary\\

Cross-layer gram matrix control applies to texture synthesis since style loss \cite{gatys2016image} was first introduced as "texture" loss in \cite{NIPS2015_5633}.  We now omit the content loss, and seeks a minimum of style loss alone.  We show texture synthesis results, which highlights the method's ability to manage long spatial correlations.  We controlled R51, R41, R31, R21, R11 for comparison with our style transfer results.  Our synthesis starts from an image which has the mean color of the texture image. 
As Figure \ref{fig:texture} shows, synthesized textures have better long scale coherence.    For the universal texture synthesis, we followed Li \ea  as starting from zero-mean Gaussian noise, run the multi level pipeline 3 times for better results.


\begin{figure*}[!htbp]
\centering
\small 
\begin{subfigure}[t]{0.15\textwidth}
	
	\includegraphics[width=\linewidth]{Texture/Page-8-Image-74.png}

	\includegraphics[width=\linewidth]{Texture/Page-8-Image-80.png}

	\includegraphics[width=\linewidth]{Texture/Page-8-Image-90.png}
	\includegraphics[width=\linewidth]{Texture/Page-8-Image-92.png}
    \includegraphics[width=\linewidth]{Texture/Page-8-Image-66.png}

    \caption{Styles}
\end{subfigure}
\begin{subfigure}[t]{0.15\textwidth}
	
	\includegraphics[width=\linewidth]{Texture_Gatys/TextureGatysBased_content171_style166.png}
	\includegraphics[width=\linewidth]{Texture_Gatys/TextureGatysBased_content171_style169.png}
	\includegraphics[width=\linewidth]{Texture_Gatys/TextureGatysBased_content171_style174.png}
	\includegraphics[width=\linewidth]{Texture_Gatys/TextureGatysBased_content171_style175.png}  
    \includegraphics[width=\linewidth]{Texture_Gatys/TextureGatysBased_content171_style162.png}

    \caption{Within-layers}
\end{subfigure}

\begin{subfigure}[t]{0.15\textwidth}
	
	\includegraphics[width=\linewidth]{Texture_ours/ShiftTextureAddCrossGatys_content171_style166_iteration600.png}
	\includegraphics[width=\linewidth]{Texture_ours/ShiftTextureAddCrossGatys_content171_style169_iteration600.png}
	\includegraphics[width=\linewidth]{Texture_ours/ShiftTextureAddCrossGatys_content171_style174_iteration600.png}
	\includegraphics[width=\linewidth]{Texture_ours/ShiftTextureAddCrossGatys_content171_style175_iteration600.png}
    \includegraphics[width=\linewidth]{Texture_ours/ShiftTextureAddCrossGatys_content171_style162_iteration600.png}
    \caption{CG}
\end{subfigure}
\begin{subfigure}[t]{0.15\textwidth}

	\includegraphics[width=\linewidth]{Texture_ours/ShiftTextureMulALLCrossGatys600_content171_style166.png}
    \includegraphics[width=\linewidth]{Texture_ours/ShiftTextureMulALLCrossGatys600_content171_style169.png}
    \includegraphics[width=\linewidth]{Texture_ours/ShiftTextureMulALLCrossGatys600_content171_style174.png}
    \includegraphics[width=\linewidth]{Texture_ours/ShiftTextureMulALLCrossGatys600_content171_style175.png}
    \includegraphics[width=\linewidth]{Texture_ours/ShiftTextureMulALLCrossGatys600_content171_style162.png}

    \caption{more CGs}
\end{subfigure}

\begin{subfigure}[t]{0.153\textwidth}
	
	\includegraphics[width=\linewidth]{Texture_WCT_WCT/styles166_Texturelayer1.jpg}

	\includegraphics[width=\linewidth]{Texture_WCT_WCT/styles169_Texturelayer1.jpg}
	\includegraphics[width=\linewidth]{Texture_WCT_WCT/styles174_Texturelayer1.jpg}
	\includegraphics[width=\linewidth]{Texture_WCT_WCT/styles175_Texturelayer1.jpg}
    \includegraphics[width=\linewidth]{Texture_WCT_WCT/styles162_Texturelayer1.jpg}
    \caption{WCT}
\end{subfigure}
\begin{subfigure}[t]{0.153\textwidth}
	
	\includegraphics[width=\linewidth]{Texture_WCT_ours/styles166_Texturelayer1.jpg}
	\includegraphics[width=\linewidth]{Texture_WCT_ours/styles169_Texturelayer1.jpg}
	\includegraphics[width=\linewidth]{Texture_WCT_ours/styles174_Texturelayer1.jpg}
	\includegraphics[width=\linewidth]{Texture_WCT_ours/styles175_Texturelayer1.jpg}
    \includegraphics[width=\linewidth]{Texture_WCT_ours/styles162_Texturelayer1.jpg}
    \caption{FCT}
\end{subfigure}
\caption{Texture synthesis comparison: Except the first column as
  style, the rest of columns from left to right are respectively generated by
  within-layer gram matrix, CG (cross-layer gram matrices), more CG (all cross-layer gram matrices between R51,R4,R31,R21,R11 are considered), WCT, and FCT. We can see that
  either in Gatys vs ours or WCT vs FCT, the cross-layer gram
  matrix indeed shows the improvement on texture patterns. }
\label{fig:texture}
\end{figure*}



{\small
\bibliographystyle{ieee}
\bibliography{egbib,egbibPlus}
}

\end{document}
