%include results
\section{Results}

%For our experiments, we compare the performance of our methods to Zoltan for distance-1 and distance-2 coloring. Our performance metrics include execution time, parallel scaling, number of colors used, and the number of communication rounds. 


For our experiments, we compare overall performance for D1 and D2 on up to 128 ranks versus Zoltan.
Our performance metrics include execution time, parallel scaling, and number of colors used. 
We do not include the partitioning time for XtraPuLP; we assume target applications will partition and distribute their graphs. 
Each of the results reported represents an average of five runs.
%We will not compare other pre-partitioning options, as they are essentially the same for our purposes here.

\subsection{Distance-1 Performance}

%\note{I changed all 'runtime' to 'execution time'. To some people, runtime refers specifically to 'runtime systems'. Execution time is specific enough to avoid annoying these people.}

%\todo{A number of reviewers complained about the readability of the plots. Two solutions: Increase font size OR just show plots for D1 results if 2GL results aren't competitive.}

We summarize the performance of our algorithms relative to Zoltan 
using the performance profiles in Figure~\ref{IAB:distance1prof}.
Performance profiles plot the proportion of problems an algorithm can solve for a given relative cost.
The relative cost is obtained by dividing each approach's execution time (or colors used) by the best approach's execution time (or colors used) for a given problem.
In these plots, the line that is higher represents the best performing algorithm. 
The further to the right that an algorithm's profile is, the worse it is relative to the best algorithm.
D1-baseline does not consider vertex degree when doing distributed recoloring (e.g., recolorDegrees is false in Algorithm~\ref{IAB:alg:conflictcheck}).
D1-recolor-degree represents our novel approach that recolors distributed conflicts based on vertex degree (e.g., recolorDegrees is true in Algorithm~\ref{IAB:alg:conflictcheck}).

%\todo{can you get rid of the line from the origin to the first data point in the plots?}
\begin{figure}[h]
  \centering
  \caption{Performance profiles comparing D1-baseline and D1-recolor-degree on 128 Tesla V100 GPUs with Zoltan's distance-1 coloring on 128 Power9 cores in terms of (a) execution time and (b) number of colors computed for the graphs listed in Table~\ref{IAB:tab:graphs}.} 
  \label{IAB:distance1prof}
  \begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/distance-1-runtime-profile-recolor-new.png}
    \caption{Runtime performance profile}
    \label{IAB:d1runtime}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/distance-1-color-profile-recolor-new.png}
    \caption{Color performance profile}
    \label{IAB:d1color}
  \end{subfigure}
\end{figure}

%\todo{what is the setup for these experiments? how many nodes? how many ranks? how many ranks/threads for Zoltan? what was partitioning?}
We ran D1-baseline, D1-recolor-degree and Zoltan with 128 MPI ranks to color the 15 SuiteSparse graphs in Table~\ref{IAB:tab:graphs}.
D1-baseline and D1-recolor-degree used MPI plus 128 Tesla V100 GPUs, while Zoltan used MPI on 128 Power9 CPU cores across 32 nodes (four MPI ranks per node).
Some skewed graphs (e.g., hollywood-2009) did not run on 128 ranks on Zoltan or D1-baseline; in those cases we use the largest run that completed for both approaches.
%add updated performance comparisons here, pointing out that recolor-degree is the best approach, so we'll use it going forward
Figure~\ref{IAB:d1runtime} shows that D1-recolor-degree outperforms both Zoltan and D1-baselines in terms of execution time in these experiments.
D1-baseline and D1-recolor-degree are very similar in terms of runtime performance, but D1-recolor-degree is the fastest approach for 60\% of the graphs,
D1-baseline is fastest on 26\%, and Zoltan is fastest on 13\%.
Zoltan is faster than our approaches on two of the smallest graphs, Audikw\_1, and ldoor.
D1-baseline is faster than D1-recolor-degrees on four graphs which are more varied in application and structure: Bump\_2911, com-Friendster, rgg\_n\_2\_24\_s0, and twitter7.
%However, in the cases where D1-baseline is the fastest approach, D1-recolor-degrees is at worst 6\% slower than D1-baseline, and in general, these two approaches are
%within 10\% of each other in terms of runtime performance.
There are four graphs for which D1-baseline and D1-recolor-degrees runtime performance differ substantially: Audikw\_1 (D1-baseline is 32\% faster), ldoor (D1-recolor-degrees is 42\% faster), mycielskian19 (D1-recolor-degrees is 45\% faster), and mycielskian20 (D1-recolor-degrees is 38\% faster).
D1-baseline has at most a 10x speedup over Zoltan (with the mycielskian20 graph) and at worst an 1.95x slowdown relative to Zoltan (with ldoor), while
D1-recolor-degrees achieves at most a 14x speedup over Zoltan (on mycielskian20), and at worst a 2x slowdown (on Audikw\_1).

Figure~\ref{IAB:d1color} shows that Zoltan outperforms D1-baseline in terms of color usage, but D1-recolor-degree is much more competitive.
Both Zoltan and D1-recolor-degree use the fewest colors in 53\% of experiments; Zoltan and D1-recolor-degree tie on a single graph.
D1-baseline uses the fewest number of colors on a single graph, for which it ties D1-recolor-degree.
D1-recolor-degree uses more colors than D1-baseline for two graphs (indochina-2004 and twitter7); in both graph, D1-baseline uses roughly 1\% fewer colors.
On average, D1-recolor-degree  uses 8.9\% fewer colors than D1-baseline, 
and in the best case, it reduces color usage  39\% relative to D1-baseline (mycielskian19).
On average, D1-recolor-degree uses 4\% fewer colors than Zoltan.
In the worst case, D1-recolor-degree uses 51\% more colors than Zoltan (twitter7); in the best case,
D1-recolor-degree uses 53\% fewer colors than Zoltan (mycielskian20).

%Zoltan uses fewer colors in over 60\% of our experiments. However, in most cases, D1 uses no more than 5\% more colors than Zoltan. 
%About half the time the Hybrid method uses fewer colors than Zoltan, and in all but two cases the Hybrid method and Zoltan are within 25\% of each other.
%For the Twitter7 graph the D1 method uses 44\% more colors than Zoltan, but for the Mycielskian20 graph, Hybrid uses 41\% fewer colors than Zoltan.
%With the twitter7 graph, Zoltan uses 45\% fewer colors than D1, but with Mycielskian20, D1 uses 41\% fewer colors than Zoltan.
%On average, D1 uses 6.8\% more colors than Zoltan.
%, while D1 and D1-2GL differ by only 1\% on average.
%These differences in the number of colors exist because of the higher concurrency used by D1 relative to Zoltan.
Because the performance of D1-recolor-degree is generally better than that of D1-baseline, 
all further distance-1 coloring results use D1-recolor-degree, and we refer to D1-recolor-degree as D1 going forward.
%This is supported by the fact that D1 and D1-2GL use almost exactly the same number of colors on each graph.

\subsection{Distance-1 Strong Scaling}

%\todo{Ian - give justification for selection of graphs that we show.}


Figure~\ref{IAB:realstrong} shows strong scaling times for Queen\_4147 and com-Friendster.
These graphs are selected for presentation because they are the largest graphs of their respective problem domains.
Data points that are absent were the result of out-of-memory issues or execution times (including partitioning) 
that were longer than our single job allocation limits.
D1 scales better on the com-Friendster graph than on Queen\_4147, as 
the GPUs can be more fully utilized with the much larger com-Friendster graph. 
For Queen\_4147, D1 on 128 GPUs shows a speedup of around 2.38x over a single GPU.
D1 uses 12\% fewer colors than Zoltan in the 128 rank run on Queen\_4147, as well as running 1.75x faster than Zoltan on that graph.
For com-Friendster, D1 is roughly 4.6x faster than Zoltan in the 128 rank run, and only uses 0.6\% more colors than Zoltan.
%For smaller rank runs on the com-Friendster graph, D1-2GL runs slightly faster than D1, but this is surprisingly not due to a reduction in communication 
%rounds as D1-2GL uses around 2 more communication rounds than D1 in those cases.
%For the 128 rank run on com-Friendster, D1-2GL is 7.4x faster than Zoltan, but D1 is 1.1x faster than D1-2GL.
%On the Queen\_4147 graph, the runtimes of D1 and D1-2GL differ at most by 13\% in the 8 rank run. 
%D1-2GL is the fastest algorithm on 128 ranks, being 2.8x faster than Zoltan, and 1.03x faster than D1.

\begin{figure}[h]
  \centering
  \caption{Zoltan and D1 strong scaling on select (a) PDE and (b) Social Network graphs.}
%\todo{add D1-2GL strong scaling results to these plots. There are D1-2GL results in the text, so I assume the experiments were run.}}
  \label{IAB:realstrong}
  \begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/Queen_4147-scaling-total.pdf}
    \caption{Queen\_4147}
    \label{IAB:queenhybridzoltan}
  \end{subfigure}%
  \begin{subfigure}[b]{0.22\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/com-Friendster-scaling-total.pdf}
    \caption{com-Friendster}
    \label{IAB:friendsterhybridzoltan}
  \end{subfigure}
\end{figure}

For graph processing in general, it is often difficult to demonstrate good strong scaling relative to single node runs. From the Graph500.org benchmark (June 2020 BFS results)~\cite{graph500}, the relative per-node performance difference in the metric of ``edges processed per second'' between the fastest multi-node results and fastest single node results are well over 100x.
%In general, it is difficult for a multi-GPU approach to show good strong scaling over a single GPU run. 
For coloring on GPUs, graphs that can fit into a single GPU do not provide sufficient work parallelism for large numbers of GPUs, and multi-GPU execution incurs communication overheads and additional required rounds for speculative coloring.
However, on roughly half of the graphs that fit on a single GPU, D1 with 128 GPUs achieves an average speedup of 1.9x over a single GPU.
D1 achieves a maximum speedup of 2.43x on the mycielskian20 graph.
For the other half of the graphs, D1 does not show a speedup over a single GPU.
%However, on average over all the graphs for which we have results, D1 still shows a 5.4x speedup over the single GPU run on 128 GPUs. %, while the D1-2GL approach sees an average speedup of 4.5x.
On small or highly skewed graphs that fit on a single GPU, speedup is limited, due to the communication overheads and work imbalances that result from distribution even with relatively good partitioning.
Distributed coloring is valuable even for these small problems, however, as parallel applications 
using coloring typically have distributed data that would be expensive to gather into one GPU for single-GPU coloring.

%\todo{what does "on average" mean in the next sentence?  average over all graphs using 128 ranks?  average on these two graphs over all rank configurations?  }
On average over all the graphs, D1 uses 38\% more colors than the single GPU run, while Zoltan uses 53.6\% more colors than the single GPU run.
Such large color usage increases are mostly due to the Mycielskian19 and Mycielskian20 graphs.
These graphs were generated to have known minimum number of colors (chromatic numbers) of 19 and 20 respectively, and our single GPU runs use 19 and 21 colors to color those graphs.
Both D1 and Zoltan have trouble coloring these graphs in distributed memory, but our D1 implementation colors these graphs in fewer colors than Zoltan.
Without these two outliers, the average color increase from the single GPU run is only 2.23\% for D1, and Zoltan decreases color usage by 0.1\% on average.
Zoltan's higher coloring quality is due to its inherently lower concurrency.


\begin{figure}[h]
  \centering
  \caption{D1 communication time (Comm) and computation time (Comp) from 1 to 128 GPUs.}
  \label{IAB:strongbreakdown}
  \begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/Queen_4147-comm-comp.pdf}
    \caption{Queen\_4147}
    \label{IAB:queenbreakdown}
  \end{subfigure}%
  \begin{subfigure}[b]{0.23\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/com-Friendster-comm-comp.pdf}
    \caption{com-Friendster}
    \label{IAB:friendsterbreakdown}
  \end{subfigure}
  \hspace*{\fill}
\end{figure}

Figure~\ref{IAB:strongbreakdown} shows the total communication and computation time associated with each run.
For both graphs, the dominant scaling factor is computation. 
Specifically, the computational overhead associated with recoloring vertices in distributed memory is the dominant scaling factor.
However, strong scaling is good on both graphs,
despite the fact that adding more ranks to a problem also increases the number of vertices that need to be recolored.
Figure~\ref{IAB:friendsterbreakdown} shows that D1 scales to more ranks on com-Friendster, primarily because of the graph's larger size.
%Figure~\ref{IAB:queenbreakdown} shows that we initially see computational and communication scaling that levels off for large numbers of ranks.
%The computation time includes any computational imbalance and the time needed to launch GPU kernels.
%At high GPU counts, imbalance or kernel launches are likely the dominant component of the computation time for this graph, causing scaling to drop off above 64 GPUs.
%As the number of ranks increases, there is no computational benefit to adding more GPUs to the problem.  \todo{the previous two sentences seem contradictory.  If computation time is dominating, adding GPUs should help, right?  Is computation time really dominating, or is it kernel launch time?  with large numbers of ranks, are we doing as well as is possible given the device latency?}

%Figure~\ref{IAB:strongbreakdown} also demonstrates certain similarities between the runtime behavior of D1 and D1-2GL.
%For both graphs, we see very similar computation time, meaning that the extra ghost layer is not adding to our computational workload in these cases.
%Communication time appears similar as well for these specific graphs. 
%However, as mentioned previously, performance differences between the two algorithms can often be much larger.
%The number of rounds of communication used fluctuates for both the D1 and the D1-2GL such that there is not a clear answer to which method uses fewer rounds of communication.
%In general over all our graphs for any number of ranks, there is no clear correlation with the D1-2GL method being faster than D1 for a graph, and D1-2GL using fewer rounds of communication.
  
\subsection{Distance-1 Weak Scaling}

The greatest benefit of our approach is its ability to efficiently process massive-scale graphs. 
We demonstrate this benefit with a weak-scaling study conducted with uniform 3D hexahedral meshes.
The meshes were partitioned with block partitioning along a single axis, resulting in the mesh being distributed in ``slabs.''
Larger meshes were generated by doubling the number of elements in a single dimension to keep the per-process communication and computational workload constant.
Each distinct per-process workload increases the boundary by a factor of two, which correspondingly increases communication and recoloring overhead for distributed runs.
We run with up to 100 million vertices per GPU, yielding a graph of 12.8 billion vertices and 76.7 billion edges in our largest tests; \textbf{this graph was colored in less than two seconds}.
%\todo{For the special issue, we should try same experiment with Zoltan.}

\begin{figure}[h]
  \centering
  \caption{Weak scaling of D1 on 3D mesh graphs. Tests use 12.5, 25, 50, and 100 million vertices per GPU.}
  \includegraphics[scale=0.6]{plots/mesh-hybrid-weak-scaling.pdf}
  \label{IAB:meshhybridweak}
\end{figure}

Figure~\ref{IAB:meshhybridweak} shows that the single rank runs for each workload are similar, 
indicating that communication and recoloring overhead are the dominant scaling factors for this study.
In increasing the boundary size by a factor of two, we do not necessarily increase the number of distributed conflicts by two,
especially in such a regular graph.
The smaller workloads all have similar and relatively small recoloring workloads, which is why they show more consistent weak scaling than 
the 100 Million vertex per rank experiment. 
That particular experiment does substantially more recoloring than the others, resulting in its increase in runtime as the number of ranks increases.
We have found that for extremely regular meshes like these, the number of vertices on process boundaries impacts the recoloring workloads for D1.

\subsection{D1-2GL Performance}

In general, D1-2GL reduces the number of collective communications used in the distributed distance-1 coloring. Figure~\ref{fig:2GLrounds} compares the number of communication rounds for D1-baseline and D1-2GL on the Queen\_4157 input for 2 to 128 MPI ranks, averaged over five runs. 
With 128 ranks on this graph, D1-2GL method reduces the number of rounds by 25\% on average, giving 
speedup of 1.18x. 
D1-2GL provides speedups over D1-baseline with the smaller graphs: 1.17x with Audikw\_1 and 1.2x with ldoor.
Unfortunately, due to the increased cost of each communication round, D1-2GL does not generally achieve a total execution time speedup over D1-baseline on AiMOS. 
Additionally, second ghost layer vertices may be recolored if they are boundary vertices on another processor; this occurs often in dense inputs and incurs further recoloring rounds.
However, in distributed systems with much higher latency costs, D1-2GL could be beneficial.


% For the mesh graphs, the number of communication rounds D1 requires is not large enough for D1-2GL to gain an advantage.
% Also, the communication cost of D1 for the mesh graphs is not very large.
% For the other graphs, communication overhead prevents D1-2GL from scaling; the communication cost of the second ghost layer is too expensive.
% Additionally, the fact that, in general, the second ghost layer vertices may be recolored incurs further communication costs to update the colors.
% However, in computers with much higher latency in the communication system, D1-2GL could be beneficial.


\begin{figure}[h]
  \centering
  \caption{Number of communication rounds for D1-baseline and D1-2GL on Queen\_4147 from 2 to 128 ranks.}
  \includegraphics[scale=0.6]{plots/Queen_4147-rounds.pdf}
  \label{fig:2GLrounds}
\end{figure}

%For these mesh graphs, we observe that D1-2GL does not reduce the number of rounds of communication.
%As these graphs require very few rounds of communication to begin with, any time savings from reducing rounds of communication would be minimal, regardless.
%For these graphs, D1-2GL does reduce the number of rounds of communication \todo{how much?}, but the extra communication overhead offsets this savings such that the execution time is very similar to D1. \todo{are we sure that it is extra communication overhead and not something else?  do we have a comm vs comp breakdown for D1-2GL?}
%Additionally, due to the regular structure of these graphs, D1 does not use many rounds of communication, so any time savings are minimal.  \todo{this discussion is weak; can we back it up with numbers?}

%\todo{In general, the results section doesn't do a good job of comparing D1-2GL to D1.  We don't show the reduction in number of rounds that we hypothesized.
%We don't characterize how much additional communication volume is incurred 
%by the extra ghost layer.
%We don't show that the performance similarity is due to extra communication 
%volume rather than additional ghost traversal.  The only detailed comparison 
%is done in this weak-scaling section on graphs that don't necessarily need 
%many rounds of communication (somewhat nullifying the effect of the extra 
%ghost layer).}

%MultiGPU color usage results for distance-1
\subsection{Distance-2 Performance}

%\todo{Re-generate distance-2 figures with ``D2'' instead of ``Hybrid''}
We compare our D2 method to Zoltan's distance-2 coloring using eight graphs from Table~\ref{IAB:tab:graphs}:  Bump\_2911, Queen\_4147, hollywood-2009, europe\_osm, rgg\_n\_2\_24\_s0, ldoor, Audikw\_1, and soc-LiveJournal1.
%Due to the greater difficulty of distance-2 coloring, many of the larger graphs would not complete for these tests.
We use the same experimental setup as with the distance-1 performance comparison.
Figure~\ref{IAB:d2runtime} shows that D2 compares well against Zoltan in terms of execution time, with D2 outperforming Zoltan on all but two graphs.
In the best case, we see an 8.5x speedup over Zoltan on the Queen\_4147 graph.
%The Zoltan is faster than D2 on three graphs: Audikw\_1 (1.2x), hollywood-2009 (2.8x) and soc-LiveJournal1 (6.6x).

\begin{figure}[h]
  \centering
  \caption{Performance profiles comparing D2 on 128 Tesla V100 GPUs with Zoltan's distance-2 coloring on 128 Power9 cores in terms of (a) execution time and (b) number of colors computed for a subset of graphs listed in Table~\ref{IAB:tab:graphs}.} 
  \label{IAB:distance2prof}
  \begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/distance-2-runtime-profile-new.png}
    \caption{Runtime performance profile}
    \label{IAB:d2runtime}
  \end{subfigure}%
  \begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[width=\textwidth]{plots/distance-2-color-profile-new.png}
    \caption{Color performance profile}
    \label{IAB:d2color}
  \end{subfigure}
\end{figure}


Figure~\ref{IAB:d2color} shows that D2 has similar color usage as Zoltan.
D2 and Zoltan each produce fewer colors in half of the experiments.
In all but one case in which Zoltan uses fewer colors, D2 uses no more than 10\% more colors.
Interestingly, the number of colors used by D2 on the soc-LiveJournal1 graph is unchanged with one and 128 GPUs.
Zoltan outperforms D2 with respect to runtime on skewed graphs because Zoltan has distance-2 optimizations which reduce communication overhead
and minimize the chance for distributed conflicts.
%Due to our framework's architecture, these specific optimizations are unavailable to us.
%As with distance-1 coloring, 
%D2 uses 46\% more colors with the soc-LiveJournal1 graph than Zoltan.
%This difference in color usage is largely due to the fact that KokkosKernels uses more colors than Zoltan's local coloring.
% For single rank runs on all the graphs except for soc-LiveJournal1, D2 uses an average of 38\% more colors than Zoltan.
% Additionally, the number of colors used by D2 on the LiveJournal graph remains constant from the single rank run, so our distributed computation does not make the coloring use more colors.
%These color usage differences are due to the differences in the D2 approach and Zoltan's distance-2 approach.
%We use vertex-based conflict detection, and because of that we require two ghost layers.
%Additionally, we do not color boundary vertices in rounds, so we expect D2 to have to resolve more conflicts.
%\todo{the previous three sentences are weak.  
%Sentence 1: "the usage differences are due to the approach differences" doesn't say anything.  
%Sentence 2:  Vertex-based conflict detection doesn't require two ghost layers; the additional two-hop check requires two ghost layers, right?   But we don't show that having two ghost layers increases the number of colors; if anything, I thought it might reduce the number of colors as each processor has more info with which to make decisions.
%Sentence 3: we haven't shown that D2 requires more conflict resolution, nor have we shown that more conflict resolution results in more colors.  
%For D1, we argued that KokkosKernels gave more colors in its local coloring than Zoltan did; is D2 different?}

\subsection{Distance-2 Strong Scaling}


Figures~\ref{IAB:Bumpstrong} and~\ref{IAB:queenstrong} show the strong scaling behavior of D2 and Zoltan on Bump\_2911 and Queen\_4147.
Bump\_2911 shows that D2 scales better initially than Zoltan, and with 128 ranks, D2 is 2.9x faster than Zoltan, using 0.7\% more colors.
Queen\_4147 shows better scaling for D2 as well; with 128 ranks, D2 is 8.5x faster than Zoltan and uses 10\% fewer colors.
%even between 4 and 8 GPUs. This is significant because the same boundary size issue that caused
%a plateau in scaling in D1 is also present here. Figure~\ref{IAB:d2queenbreakdown} shows that from 4 to 8 ranks, the communication performance stalls.
%In this case, the computational scaling is the dominant factor, so this relatively small communication plateau is not evident in the total runtime.
%With Queen\_4147, D2 shows a brief scaling plateau from four to eight GPUs.
%This performance is an artifact of graph partitioning; the boundary size for eight ranks is the second largest out of all GPU counts except for 128 GPUs, resulting in a larger-than-expected communication cost.
%Zoltan is less sensitive to boundary sizes in the distance-2 case because it uses a more optimized communication pattern than our approach.
%After the eight-rank run, D2 scales slightly better than Zoltan up to 128 ranks.
%For the 128-rank run, D2 runs 2.1x faster than Zoltan, and uses 10\% fewer colors.

% The number of vertices on the boundary for the 8 rank run is the largest boundary until the 128 rank run, which is not the case for the 8 rank run on Bump\_2911.
% Additionally, the number of communication rounds increases, and this is the first run that happens on two separate nodes.
% These factors increase communication overhead for the 8 rank run on Queen\_4147, but in subsequent runs the boundary actually decreases in size, 
% meaning that the aggregate communication decreases, and more communication happens in parallel.



\begin{figure}[h]
  \centering
  \caption{D2 and Zoltan strong scaling for distance-2 coloring.}
  \label{IAB:distance2strong}
  \begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/Bump_2911-d2-total.pdf}
    \caption{Bump\_2911}
    \label{IAB:Bumpstrong}
  \end{subfigure}%
  \begin{subfigure}[b]{0.22\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/Queen_4147-d2-total.pdf}
    \caption{Queen\_4147}
    \label{IAB:queenstrong}
  \end{subfigure}
\end{figure}

On average over the eight graphs, D2 exhibits 4.29x speedup on 128 GPUs over a single GPU, and uses 7.5\% more colors than single GPU runs.
Speedup is greater with D2 than D1 because distance-2 coloring is more computationally intensive, and thus has a larger work-to-overhead ratio.

\begin{figure}[h]
  \centering
  \caption{D2 communication time (comm) and computation time (comp) from 1 to 128 GPUs.}
  \label{IAB:distance2breakdown}
  \begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/Bump_2911-d2-comm-comp.pdf}
    \caption{Bump\_2911}
    \label{IAB:bumpbreakdown}
  \end{subfigure}%
  \begin{subfigure}[b]{0.22\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/Queen_4147-d2-comm-comp.pdf}
    \caption{Queen\_4147}
    \label{IAB:d2queenbreakdown}
  \end{subfigure}
\end{figure}

Figures~\ref{IAB:bumpbreakdown} and~\ref{IAB:d2queenbreakdown} show the communication and computation breakdown of D2 on Bump\_2911 and Queen\_4147.
Bump\_2911 shows computation and communication scaling for up to 128 ranks, while color usage increases by only 0.6\%.
In general, the relative increase in color usage from a single rank for distance-2 coloring is less than for distance-1 coloring. The number of colors used for distance-2 coloring is greater than for distance-1; therefore, a similar absolute increase in color count results in a lower proportional increase. 
%Figure~\ref{IAB:d2queenbreakdown} shows a communication plateau between 4 and 8 ranks, which is due to the 8 rank run having the second largest boundary size of any run, and the increase in communication overhead that comes with moving from one node to two.
%For D2, the computational scaling outweighs the communication and recoloring performance plateau, so a plateau is not evident in the total runtime.

\subsection{Distance-2 Weak Scaling}

Figure~\ref{IAB:meshd2weak} demonstrates the weak scaling behavior for D2.
The same hexahedral mesh graphs were used as in the D1 weak scaling experiments.
%; however, some of the low rank count experiments were unable to complete due to memory constraints, so we only report our timing results from 8 to 64 ranks.
In general, D2 has fairly consistent weak scaling.
The runtimes across workloads with a single rank increase by more than factor of two because the number of edges in each mesh increases by more than a factor of two, and the complexity of the distance-2 coloring algorithm for local colorings depends on the number of edges.
Weak scaling to large process counts is good for all workloads.
%Aside from the initial coloring, D2 shows very consistent weak scaling for smaller workloads, and stays fairly consistent for our largest workloads.
%Not really wanting to point any fingers at KokkosKernels here, it may just be the net-based algorithms' theoretical complexity
\begin{figure}[h]
  \centering
  \caption{Distance-2 weak scaling of D2 on 3D mesh graphs.}
  \includegraphics[scale=0.6]{plots/mesh-d2-weak-scaling.pdf}
  \label{IAB:meshd2weak}
\end{figure}



\subsection{Partial Distance-2 Strong Scaling}

\begin{table}[!t]
  \small
  \centering
  \caption{Summary of the graphs used for PD2 tests. Statistics are for the bipartite representation of the graph (Section ~\ref{IAB:method:PD2}).  $\delta_{avg}$ is average degree and $\delta_{max}$ is maximum degree. Numeric values listed are after preprocessing to remove multi-edges and self-loops. k = thousand, M = million}
  \begin{tabular}{|r|r|r|r|r|r|r|}
    \hline
    Graph	& Class	&	\#Vtx	& \#Edges	& $\delta_{avg}$ & $\delta_{max}$ \\
    \hline
    Hamrle3	&Circuit Sim. &2.9 M	&5.5 M		& 3.5		 & 18		  \\
%    watson\_2	&Linear Prog. &1 M	&1.8 M		& 1.8		 & 178		  \\
    %IAB: I removed this input as we don't scale on it from 64 to 128 ranks, plots are still in the plots directory.
    patents	&Patent Citations &7.5 M	&14.9 M		& 1.9		 & 1k		  \\
    \hline
  \end{tabular}\\
  \label{IAB:tab:pd2graphs}
\end{table}

Table~\ref{IAB:tab:pd2graphs} shows the graphs that we used to compare our PD2 implementation against Zoltan.
Partial distance-2 coloring is typically used on non-symmetric and bipartite graphs; the graphs in Table ~\ref{IAB:tab:pd2graphs} are representative of application use cases. 
We report metrics reported for the bipartite representation of the graph (as described in Section~\ref{IAB:method:PD2}).
Partial distance-2 colorings typically are needed for only a subset of the vertices in a graph, but 
our PD2 implementation colors all vertices in the graph. We compare to Zoltan, which colors only vertices that would be colored in typical partial distance-2 coloring. 
Thus, in general, PD2 is coloring roughly twice as many vertices as Zoltan.

\begin{figure}[h]
  \centering
  \caption{PD2 strong scaling for partial distance-2 coloring.}
  \label{IAB:partialdistance2strong}
  \begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/patents-pd2-scaling-total.pdf}
    \caption{patents}
    \label{IAB:patentsstrong}
  \end{subfigure}%
  \begin{subfigure}[b]{0.22\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/Hamrle3-pd2-scaling-total.pdf}
    \caption{Hamrle3}
    \label{IAB:hamrle3strong}
  \end{subfigure}
\end{figure}

Figure~\ref{IAB:partialdistance2strong} shows the strong scaling behavior of PD2.
The experiment with Hamrle3 on four ranks benefits from a particularly good partition that results in less recoloring for both PD2 and Zoltan relative to other process configurations.
For the patents graph, D2 has a particularly heavy recoloring workload for four ranks, resulting in a large increase in runtime from two to four ranks.
Even though PD2 is coloring more vertices than Zoltan in these tests, PD2 achieves roughly 2x speedup on 128 ranks with Hamrle3.
With patents, Zoltan is faster than PD2;  this result can be attributed partially to Zoltan's optimized recoloring scheme that reduces the number of conflicts introduced while recoloring distributed conflicts.
PD2 achieves a 1.73x speedup over a single GPU with the patents graph, while it did not show any speedup from a single GPU with Hamrle3.
Figure~\ref{IAB:patentsstrong} shows that, with the patents graph, Zoltan is faster on one core than a single GPU.
This speedup is attributed to Zoltan's coloring fewer vertices than PD2;
when Zoltan colors the same number of vertices as PD2, their single rank runtimes are equal.
Investigating the cause of this result is a subject for future research.

For these two graphs, PD2 uses a very similar number of colors as Zoltan. 
PD2 uses at most 10\% more colors in the distributed runs.
This difference is typically only one to five colors more than Zoltan.

\begin{figure}[h]
  \centering
  \caption{PD2 communication time (comm) and computation time (comp) from 1 to 128 GPUs}
  \label{IAB:partialdistance2breakdown}
  \begin{subfigure}[b]{0.25\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/patents-pd2-comm-comp.pdf}
    \caption{patents}
    \label{IAB:patentsbreakdown}
  \end{subfigure}%
  \begin{subfigure}[b]{0.22\textwidth}
    \centering
    \includegraphics[scale=0.5]{plots/Hamrle3-pd2-comm-comp.pdf}
    \caption{Hamrle3}
    \label{IAB:hamrle3breakdown}
  \end{subfigure}
\end{figure}

Figure~\ref{IAB:partialdistance2breakdown} shows that computation is the main factor in the scaling behavior of PD2.
In distributed runs, the largest factor of the runtime is the computation overhead involved in recoloring distributed conflicts.
Figure~\ref{IAB:hamrle3breakdown} shows an unexpected decrease in computation for the Hamrle3 graph for four ranks, 
which is due to a decrease in the recoloring workload. 
PD2's recoloring workload is approximately 25,000 vertices per rank in most experiments, but the four-rank experiment has a recoloring workload of 9,000 vertices per rank.
Figure~\ref{IAB:patentsbreakdown} shows that the four-rank PD2 run has a much longer computation time than expected; this is due to the total distributed recoloring workload increasing by a factor of six.
Additionally, the 64-rank run with the patents graph shows slightly less computational scaling than expected, due to an increase in recoloring rounds.
Increasing recoloring rounds serializes recoloring computation and incurs more rounds of communication, resulting in a runtime increase.
Optimizing recoloring to reduce subsequent conflicts and reduce the number of recoloring rounds necessary in D2 and PD2 are subjects for future research.



