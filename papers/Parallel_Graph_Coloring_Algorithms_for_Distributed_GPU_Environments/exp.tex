%\fix{this should be a separate section}



\section{Experimental Setup}
%What is the context for the experiments?
%The scaling results we present were obtained from Waterman, a system housed at Sandia National Labs that has 10 nodes with Dual IBM Power 8 processors with 10 cores each, 4 Nvidia Tesla V100 GPUs with 5120 cores and 16 GB GDDR5 memory each. Each node on Waterman has 319 GB DDR3 memory, and is connected with a Mellanox InfiniBand interconnect. 
We performed scaling experiments on the AiMOS supercomputer housed at Rensselaer Polytechnic Institute. The system has 268 nodes, each equipped with two IBM Power 9 processors clocked at 3.15~GHz, 4x NVIDIA Tesla V100 GPUs with 16~GB of memory connected via NVLink, 512~GB of RAM, and 1.6~TB Samsung NVMe Flash memory. Inter-node communication uses a Mellanox Infiniband interconnect. We compile with xlC 16.1.1 and use Spectrum MPI with GPU-Direct communication disabled.

% moved table earlier so its on same page as this text
% \begin{table*}[!t]
%   \centering
%   \caption{Summary of input graphs}
%   \begin{tabular}{|r|r|r|r|r|r|}
%     \hline
%     Graph   & Class   & \#Vertices  & \#Edges  & $\delta_{avg}$ & $\delta_{max}$\\
%     \hline
%     ldoor   & PDE Problem & 0.9 M   & 42.5 M   & 44.6       & 77\\
%     Audikw\_1   & PDE Problem & 0.9 M   & 76.7 M   & 81.28      &  345\\
%     Bump\_2911    & PDE Problem   & 2.9 M       &124.8 M   & 42.87      & 194\\
%     Queen\_4147   & PDE Problem   & 4.1 M   &325.3 M   & 78.45      & 89\\
%     hollywood-2009& Social Network& 1.1 M   &112.8 M   & 98.91      &11.5 k\\
%     soc-LiveJournal1&Social Network& 4.8 M  &85.7 M    & 17.68      &20 k\\
%     com-Friendster& Social Network& 65.6 M  &1.8 B     & 55       &5.2 k\\
%     europe\_osm   & Road Network  & 50.9 M  &108.1 M   & 2.12     &13\\
%     indochina-2004& Web Graph   & 7.4 M &194.1 M   & 26.17      & \\
%     MOLIERE\_2016 & Document Mining Network& 30 M &6.7 B   & 79.65      & \\
%     rgg\_n\_2\_24\_s0& Random Graph&16.7 M  &265 M     & 15       &40\\
%     mesh1B    & Quad Mesh   & 1 B         &5.4 B     & 5.4      &6\\
%     mesh500M    & Quad Mesh     & 500 M &2.7 B     & 5.4      &6\\
%     %more to come
%     \hline
%   \end{tabular}
%   \label{IAB:tab:graphs}
% \end{table*}


%\fix{put these graphs in order based on problem difficulty. Show maximum degree to give a lower bound for colors. Describe the properties of these graphs more in the text, since there's only three of them.}

%\todo{if time - add ``source'' column to table}
% GMS -- if everything is from SuiteSparse, no point in doing this

The graphs we used to test D1 and D2 are listed in Table~\ref{IAB:tab:graphs}. Most of the graphs are from the SuiteSparse Matrix Collection 
%(formerly UFL Sparse Matrix Collection)
~\cite{IAB:Davis2011UFS}.
The maximum degree $\delta_{max}$ can be considered an upper bound for the number of colors used, as any incomplete, 
connected, and undirected graph can be colored using $\delta_{max}$ colors~\cite{IAB:brooks1941colouring}.
We selected many of the same graphs used by Deveci et al. to allow for direct performance comparisons.
We include many graphs from Partial Differential Equation (PDE) problems because they are representative of graphs used with Automatic Differentiation~\cite{IAB:gebremedhin2020introduction}, which is a target application for graph coloring algorithms.
We also include social network graphs and a web crawl to demonstrate scaling of our methods on irregular real-world datasets.
We preprocessed all graphs to remove multi-edges and self-loops, and 
we used subroutines from HPCGraph~\cite{slota_ipdps2016} for efficient I/O.
% but did not reorder the vertices as in~\cite{IAB:bozdaug2008framework}. 
%We used XtraPuLP~\cite{slota2017pulp} for partitioning, minimizing edge cut and balancing either vertices or edges per-process.

We compare our implementation against distributed distance-1 and distance-2 coloring in the Zoltan~\cite{IAB:devine2009getting} package of Trilinos.
Zoltan's implementations are based directly on Bozda{\u{g}} et al.~\cite{IAB:bozdaug2008framework}.
Zoltan's distributed algorithm for distance-2 coloring requires only a single ghost layer, and to reduce conflicts, the boundary vertices are colored in small batches.
%We use the same partitioning and file reading code as our approaches to build and partition the inputs before calling the Zoltan coloring with default arguments.
For our results, we ran Zoltan and our approaches with four MPI ranks per node on AiMOS, and used the same partitioning method across all of our comparisons.
Our methods D1, D1-2GL, and D2 were run with four GPUs and 
four MPI ranks (one per GPU) per node.
Zoltan uses only MPI parallelism; it does not use GPU or multicore parallelism. 
For consistency, we use four MPI ranks per node with Zoltan, and use the same number of nodes
for experiments with Zoltan and our methods.
%, as such an experimental setup (one MPI rank per GPU) is in line with our intended use-cases. 
We used Zoltan's default coloring parameters; we did not experiment with
options for vertex visit ordering, boundary coloring batch size, etc.
%\todo{citation for zoltan coloring and describe how it is run}

%\note{added the below:}

We omit direct comparison to single-node GPU coloring codes such as CuSPARSE~\cite{naumov2015parallel}, as we use subroutines for on-node coloring from Deveci et al.~\cite{IAB:deveci2016parallel}. Deveci et al. have already performed a comprehensive comparison between their coloring methods and those in CuSPARSE, reporting an average speedup of 50\% across a similar set of test instances. As such, we are confident that our on-node GPU coloring is representative of the current state-of-the-art.

%\fix{explicitly note dmax is lower bound for coloring}
