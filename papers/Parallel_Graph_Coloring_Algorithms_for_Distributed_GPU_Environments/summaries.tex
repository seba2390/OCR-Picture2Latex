\documentclass{article}
\usepackage{color}

\newcommand{\todo}[1]{\textcolor{red}{\bf #1}}

\begin{document}

\title{Literature Review Summaries}
\author{Ian Bogle}
\maketitle

\section{Coloring Library(ies)}

ColPack~\cite{gebremedhin2013colpack} is a graph coloring library that supports multiple
different instances of the graph coloring problem. Among the instances handled by ColPack are:
Distance-1, Distance-2, Star, Acyclic, Restricted Star, Triangular, Partial Distance-2 and 
Star Bicoloring. ColPack supports multiple different vertex orderings as well, which enables 
the user to tailor the coloring to their data, if possible. They propose a generic coloring framework
that is able to support all of the different coloring algorithms. For parallel algorithms, they note
that other publications' distributed memory algorithms are 
used~\cite{bozdaug2008framework,bozdaug2010distributed}, and that multithreaded versions are
the subject of current and future research. The end goal for this package is to be able to support
sparse derivative computations on a wide range of architectures.

\section{Shared Memory Implementations}

Singhal, Peri, and Kalyanasundaram~\cite{singhal2017practical} present a study of multiple 
shared-memory implementations. They translate distributed memory algorithms faithfully to shared memory,
and find that such translations are often underperforming. In particular, their shared memory imitation
of a distributed memory implementation of the Jones-Plassmann algorithm took 18 hours to color the
LiveJournal graph from the SNAP dataset. They show that algorithms that are developed using 
shared-memory constructs such as fine-grained resource locks perform much better, and in their
test case these algorithms outperformed the sequential implementation. They achieved around a 2x speedup
over the sequential algorithm, while keeping the coloring quality high. They only provide results on 
the LiveJournal graph. Their conclusion is that using fine-grained locking for synchronization gives
the best results.

Rokos, Gorman and Kelly~\cite{rokos2015fast} build off work presented in 
{{\c{C}}ataly{\"u}rek et. al~\cite{ccatalyurek2012graph}.{{\c{C}}ataly{\"u}rek et. al presented
both a Cray XMT-reliant dataflow algorithm and a {\it speculation and iteration} algorithm.
In this context, a {\it speculation and iteration} algorithm is one that colors vertices in parallel
in such a way that may result in two neighboring vertices sharing the same color.
In order to obtain a valid coloring, these algorithms must iterate to resolve coloring conflicts caused 
by this ``optimistic'' parallel coloring scheme. 
Another popular approach to parallel graph coloring finds maximal independent sets and colors vertices
in the same set in parallel. \cite{ccatalyurek2012graph} motivate their optimistic direction by stating that 
approaches based on finding maximal independent sets are generally
less performant than approaches that optimistically color vertices, detect conflicts, and recolor.
The approach presented in \cite{ccatalyurek2012graph} is fairly straightforward, using a First-Fit
coloring to speculatively color a thread's vertices, then detecting and fixing conflicts in parallel.
While this approach shows good strong scaling, Rokos et. al found that if the {\it speculate}
phase is combined with the {\it iterate} phase, which are distinct in \cite{ccatalyurek2012graph}, a 
significant speedup is seen, with no penalty to the quality of the coloring. \cite{rokos2015fast} reports
difficulties in running these algorithms on GPUs, and attributes it to the more uniform synchronicity of
GPU threads as opposed to the more unpredictable scheduling of CPU threads. Thus, these algorithms may
not be trivially portable to a GPGPU context. 

\subsection{Distance-2 and Partial Distance-2}

Ta{\c{s}}, Kaya, and Saule~\cite{tacs2017greed} present an algorithm for Bipartite Graph Partial Coloring
(Partial Distance-2 Coloring) that is 4.71x faster than ColPack's implementation. Their idea is to look at the 
Bipartite Graph Coloring problem as a Hypergraph coloring problem. They refer to the vertices of the hypergraph as
``pins'', and they call the hyperedges ``nets''. They point out that the literature heavily favors ``pin-based'' 
colorings, and opt to go with a ``net-centric'' approach, since the nets of the hypergraph define the neighborhoods
of interest.

They also use the same
ideas \todo{(which ideas? how did they achieve the speed-up over colpack?)} in a full Distance-2 coloring algorithm. Additionally they propose cost-free heuristics that 
attempt to give a more balanced coloring. As opposed to traditional Partial Distance-2 colorings that
adapt Distance-2 coloring algorithms to do Partial Distance-2, Ta{\c{s}} et. al pursue a 
 net-centric approach \todo{in which ... explain net-centric}. A drawback of net-centric coloring conflict detection is that each net must be 
fully traversed at every iteration. In practice, vertex-centric approaches are more costly than 
net-centric approaches in early iterations, so they propose a somewhat adaptive approach. They show 
that the best runtime performance is gained by doing net-based coloring and conflict detection in 
the first few iterations, and then transitioning to a vertex-centric approach. They report an 11.38x 
speedup from ColPack's sequential Partial Distance-2 algorithm, while only using 8\% more colors. 
Additionally their fastest implementation is 4.12x faster than ColPack's parallel Partial Distance-2 
coloring.

\section{Distributed Memory Implementations}

Gebremedhin and Manne~\cite{gebremedhin2000scalable} propose a parallel framework for graph coloring.
Essentially, their method is to speculatively color the graph, and detect coloring conflicts in parallel.Then, conflicts are resolved sequentially.  
\todo{Need to describe the parallelism -- each processor colors a 
non-overlapping subgraph independently, conflicts arise at boundaries between
processors' subgraphs, etc.}
An addition that they provide is based on Culberson's 
Iterated Greedy Coloring Heuristic\todo{\cite{Culberson}}, which means there are two speculative coloring phases before 
detecting conflicts. \todo{What is different in the second phase?} This is done because according to Culberson, the coloring obtained is guaranteed to
be at least as good, but often times uses less colors. In this coloring step, vertices are ordered by
decreasing color class from the initial speculative coloring.
\todo{Are you saying that the first coloring visits vertices arbitrarily, 
and the second coloring visits them according to the color assigned in the first
coloring?  Also, is ``decreasing'' with respect to the color number, or the 
number of vertices with that color, or something else?}

Bozda{\u{g}} et. al.~\cite{bozdaug2008framework} proposes a framework that is specific to 
distributed-memory architectures. They provide their source code in the Zoltan package of Trilinos.
They also opt for a {\it speculate and iterate} approach. Their approach assumes that the input graph
is partitioned reasonably among the processors, and they define interior vertices as vertices that have
all neighbors on the same processor, while boundary vertices are vertices with at least one remote 
neighbor. The interior vertices can be colored independently, as they will not participate in any 
conflicts. The coloring of boundary vertices proceeds in rounds. Each round has a tentative coloring
phase, and a conflict detection phase. The tentative coloring uses supersteps in a manner akin to a BSP
computation. \todo{I don't understand the previous sentence; what happens in each superstep?  Also, I think of BSP as a paradigm that could be used in supersteps, not a particular computation.} Updated color information is sent in bulk at the end of this phase. Conflict detection \todo{detection? or resolution?} is
implemented in a way  that allows each processor to make the same decision without communication. \todo{For the previous sentence, can you be more specific in the same number of words? Something like, ``Using random number 
generation, processors can independently assign consistent colors to conflicted
vertices without communication.''} The 
process concludes when there are no more vertices to recolor. \cite{bozdaug2008framework} explores 
many minor tweaks to this approach to get the best performance.

\subsection{Distance-2 Colorings}

Bozda{\u{g}} et. al.~\cite{bozdaug2005parallel,bozdaug2010distributed} present a Distance-2 version of 
the framework presented in~\cite{bozdaug2008framework}. This approach 
(it is the same idea across both papers \todo{don't need to say that}) is also present in the Zoltan package of Trilinos. Instead of extending the extra vertex data
that is stored on each processor to include not only distance-1 neighborhoods but also distance-2 
neighborhoods, Bozda{\u{g}} et. al use the processor that owns the intermediate vertex in a 3-vertex path
to detect distance-2 coloring conflicts. The approach is very similar to \cite{bozdaug2008framework}, 
except for the fact that the conflict detection phase now uses supersteps as well. Instead of using
unscheduled communication \todo{what is unscheduled communication?  is it used in their previous paper?}, each processor computes a coloring schedule, so each processor knows which
neighboring processors need what coloring information at which point in time. 
\todo{you might consider rephrasing the previous sentence; it is awkward.}
This means that conflict
detection only needs to worry about vertices in the current scheduling step. Additionally, processors
must participate until there are no conflicts globally, as they may be involved with an indirect 
conflict. \todo{is this global participation unique to this paper or is it 
true in the previous papers?  It isn't clear to me which details in this 
paragraph are different from the d-1 implementation.}

\section{GPU-based Implementations}

Naumov, Castonguay and Cohen~\cite{naumov2015parallel} conduct a study of two GPU-based coloring 
algorithms, the Cohen-Castonguay (CC) algorithm, and the Jones-Plassmann-Luby (JPL) algorithm, \todo{in the context of a ??? application}. 
Both of these algorithms are based on finding maximal independent sets. They show that the JPL algorithm
results in a smaller set of colors, and thus more application parallelism, but it is significantly 
slower than the CC algorithm. Additionally, because their application presumably allows for a slightly
incomplete coloring, they do a runtime study for incomplete colorings, but find that the existing
discrepancies \todo{what discrepancies?} hold for coloring 80\% and 90\% of the graph.

Pham and Fan~\cite{pham2018efficient} propose two new algorithms and compare them to three other existing
algorithms. They propose a ``Counting-based Jones-Plassmann'' (CJP) algorithm, along with a 
``Conflict Coloring'' (CC) \todo{already used CC in paragraph above} algorithm. These two algorithms encompass both {\it speculate and iterate} and
maximal independent set approaches to graph coloring. They compare to the algorithm presented in
Deveci et. al~\cite{deveci2016parallel}, the {\it csrcolor} routine from the CUSPARSE library, and 
ColPack's sequential algorithm. Both the CC and CJP algorithms are faster than the other three, with 
CC being the fastest of the group. \todo{remove next four words} Predictably, this means that generally CC uses more colors than CJP,\todo{; instead of ,}
however {\it csrcolor} used the most colors. \todo{how did deveci compare?} Notably, the CJP and CC algorithms only used 1.3x and 1.5x
the colors of the sequential ColPack algorithm.

Che et. al.~\cite{che2015graph} look into load-balancing an implementation of the Jones-Plassmann 
algorithm for GPUs. They use a work-stealing approach presented by Cedermann and 
Tsigas~\cite{cederman2012dynamic}, and modified Jones-Plassmann's random number generation to factor in
vertex degrees, so that larger degree vertices would be processed first, and then work units would be
relatively uniform. Che et. al propose doing a vertex-based Jones-Plassmann for the first few iterations,
then transitioning to traditional Jones-Plassmann. They do not attempt to answer the question of when to
switch from one approach to the other, and leave that as an open question.


\section{Hybrid Implementations}

Grosset et. al.~\cite{grosset2011evaluating} propose a CPU+GPU framework that uses a 
{\it speculate and iterate} approach. They do initial graph partitioning \todo{for distributed memory? or into chunks that fit in GPU memory?} on the CPU, run coloring
on the GPU, and once the number of conflicting vertices is small enough, they resolve the remaining
conflicts on the CPU. They used four \todo{you list five in the next sentence -- or is ``SDO and LDO'' one method?} vertex ordering heuristics, including two new heuristics \todo{which are the new ones?}. They 
used First Fit, SDO \todo{SDO = ??} and LDO \todo{LDO = ??}, MAX OUT, and MIN OUT. First fit is the traditional greedy heuristic, where
the color assigned is the smallest color, and the vertex ordering is not specified. SDO and LDO 
allocates the smallest color(?) \todo{fix the (?)} to the vertex with the highest number of colored neighbors, and breaks
ties \todo{ties among what?} using the highest degree vertex. MAX OUT allocates the smallest color to the vertex that has the 
most remote neighbors \todo{what is a remote neighbor?}, again breaking ties with the degree. MIN OUT allocates the smallest color to the
vertex that has the fewest remote neighbors, breaking ties with the degree. 
Test cases used in this paper
were pretty small, it would be interesting to see behavior on larger graphs (perhaps infeasible due to
lack of MPI+GPU implementation?). \todo{fix previous sentence: run-on sentence}
Grosset et. al report that the best performance/quality tradeoff is
afforded by the parallel SDO and LDO. This approach results in less \todo{fewer} colors than the sequential First fit,
but it is also slower overall.

Sariy{\"u}ce, Saule, and {\c{C}}ataly{\"u}rek~\cite{sariyuce2012scalable} motivates the use of the
framework present in Zoltan, saying that dataflow algorithms rely on niche hardware support that is not
prevelant in HPC systems. They also note that most distributed-memory machines are only 
distributed-memory at the highest logical level, and have some form of shared memory architecture at a
lower level. They propose a hybrid MPI + OpenMP implementation that builds on the Zoltan graph coloring
framework. They show that their hybrid approach has better scalability than an MPI-only approach, the reason
being that they are able to use architecture-appropriate parallelization at every 
logical level. Specifically, they are able to used shared memory on compute nodes, and they can use
MPI to pass messages between compute nodes. Important things to note are that thread affinity and 
scheduling has an important and often unpredictable effect on performance. Additionally, only 
distance-1 coloring was implemented. 

\section{Balanced Coloring}

Lu, et al~\cite{lu2015balanced} present algorithms for balanced coloring. They introduce both 
{\it ab initio} and {\it guided} algorithms, and show how to parallelize the guided coloring algorithms.
{\it Ab initio} algorithms attempt to build balanced colorings from the start, as opposed to guided 
algorithms, which start with an unbalanced coloring (typically First-Fit for the Culberson property \todo{Culberson property == ??}), and
then attempt to balance the colorings by either recoloring using a balance-aware algorithm, or 
shuffling the vertices from over-full coloring classes to under-full coloring classes. They propose and 
analyze different parallelization schemes for both approaches, and find that a parallel guided shuffling algorithm
they call ``Scheduled Reverse'' has the best performance-quality tradeoff. This approach uses a greedy First-Fit
coloring initially, and then identifies arbitrary subsets of vertices from over-full color classes. When shuffling
vertices, it tries to fill under-full bins in decreasing order of color index, which takes advantage of the 
incidence property \todo{incidence property == ??} afforded by the greedy First-Fit heuristic.

\todo{Deveci's paper is cited but there is no summary for it.}

\bibliography{color}
\bibliographystyle{plain}
\end{document}

