%!TEX root = main.tex
\section{Multi-scale Sentence Matching}
\label{sec:multi-layer}


\begin{figure*}[!htb]
\centering
\includegraphics[width=5.5in]{figure/network}
\vspace{0mm}
\caption{Extend the Siamese network architecture for sentence matching by feeding into the multi-scale representations of sentence pairs.}
\label{fig:network}
\vspace{-2mm}
\end{figure*}


Our sentence factorization algorithm parses a sentence $S$ into a hierarchical factorization tree $T^f_S$, where each depth of $T^f_S$ contains the semantic units of the sentence at a different granularity.
% We proposed a new unsupervised semantic distance metric using the reordered sentence representation in depth $0$ (the root node) of $T^f_S$.
In this section, we exploit this multi-scaled representation of $S$ present in $T^f_S$ to propose a multi-scaled Siamese network architecture that can extend any existing CNN or RNN-based Siamese architectures to leverage the hierarchical representation of sentence semantics.


Fig.~\ref{fig:network} (a) shows the network architecture of the popular Siamese ``matching-aggregation'' framework \cite{wang2016compare,mueller2016siamese,severyn2015learning,neculoiu2016learning,baudivs2016sentence} for sentence matching tasks. The matching process is usually performed as follows: First, the sequence of word embeddings in two sentences will be encoded by a context representation layer, which usually contains one or multiple layers of LSTM, bi-directional LSTM (BiLSTM), or CNN with max pooling layers.
The goal is to capture the contextual information of each sentence into a context vector. 
In a Siamese network, every sentence is encoded by the same context representation layer.
%The left part and the right part for a pair of sentences share the same context representation layer.
Second, the context vectors of two sentences will be concatenated in the aggregation layer. They may be further transformed by more layers of neural network
%one or a few neural network layers (such as LSTM) 
to get a fixed length matching vector.
Finally, a prediction layer will take in the matching vector and outputs a similarity score for the two sentences or the probability distribution over different sentence-pair relationships.


Compared with the typical Siamese network shown in Fig.~\ref{fig:network} (a), our proposed architecture shown in Fig.~\ref{fig:network} (b) differs in two aspects.
First, our network contains three Siamese sub-modules that are similar to (a). They correspond to the factorized representations from depth $0$ (the root layer) to depth $2$. We only select the semantic units from the top $3$ depths of the factorization tree as our input, because usually most semantic units at depth $2$ are already single words and can not be further factorized. Second, for each Siamese sub-module in our network architecture, the input is not the embedding vectors of words from the original sentences. Instead, we use semantic units at different depths of sentence factorization tree 
for matching.
%as the basic sentence matching unit.
We sum up the embedding vectors of the words contained in a semantic unit to represent that unit. Assuming each semantic unit at depth $d$ can be factorized into $k$ semantic sub-units at depth $d + 1$. If a semantic unit has less than $k$ sub-units, we add empty units as its child node to make each non-leaf node in a factorization tree has exactly $k$ child nodes. The empty units are embedded with a vector of zeros. After this procedure, the number of semantic units at depth $d$ of a sentence factorization tree is $k^d$.

Taking Fig.~\ref{fig:casestudy} as an example. We set $k = 4$ in Fig.~\ref{fig:casestudy}. For sentence A ``The little Jerry is being chased by Tom in the big yard'', the input at depth $0$ is the sum of word embedding $\{$chase, Tom, Jerry, little, yard, big$\}$. The input at depth $1$ are the embedding vectors of four semantic units: 
$\{$chase, Tome, Jerry little, yard big$\}$. Finally, at depth $2$, the semantic units are $\{$chase, -, -, -, Tom, -, -, -, Jerry, little, -, -, yard, big, -, -$\}$, where ``$-$'' denotes an empty unit.

As we can see, based on this factorized sentence representation, our network architecture explicitly matches a pair of sentences at several semantic granularities. 
%In addition, we align the semantic units in two sentences by mapping the position of semantic units in the tree to its input index in the input layer of neural network. 
In addition, we align the semantic units in two sentences by mapping their positions in the tree to the corresponding indices in the input layer of the neural network.
For example, as shown in Fig.~\ref{fig:casestudy}, the semantic units at depth $2$ are aligned according to their unit indices: ``chase'' matches with ``catch'', ``Tom'' matches with ``cat blue'', ``Jerry little'' matches with ``mouse brown'', and ``yard big'' matches with ``forecourt''.



