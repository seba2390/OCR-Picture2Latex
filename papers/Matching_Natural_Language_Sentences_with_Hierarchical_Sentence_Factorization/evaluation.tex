%!TEX root = main.tex
\section{Evaluation}
\label{sec:eval}

In this section, we evaluate the performance of our unsupervised Ordered Word Mover's Distance metric and supervised Multi-scale Sentence Matching model with factorized sentences as input. We apply our algorithms to semantic textual similarity estimation tasks and sentence pair paraphrase identification tasks, based on four datasets: STSbenchmark, SICK, MSRP and MSRvid. 

\subsection{Experimental Setup}
\label{subsec:setup}


\begin{table}[tb]
  \caption{Description of evaluation datasets.}
  \label{tab:datasets}
  \begin{tabular}{lllll}
    \toprule
    Dataset & Task & Train & Dev & Test\\
    \midrule
    STSbenchmark & Similarity scoring & $5748$ & $1500$ & $1378$ \\
    SICK & Similarity scoring & $4500$ & $500$ & $4927$ \\
    MSRP & Paraphrase identification & $4076$ & - & $1725$ \\
    MSRvid & Similarity scoring & $750$ & - & $750$ \\
    \bottomrule
  \end{tabular}
  \vspace{-2mm}
\end{table}

We will start with a brief description for each dataset:
\begin{itemize}
\item \textbf{STSbenchmark}\cite{cer2017semeval}: it is a dataset for semantic textual similarity (STS) estimation. The task is to assign a similarity score to each sentence pair on a scale of 0.0 to 5.0, with 5.0 being the most similar.

\item \textbf{SICK}\cite{marelli2014sick}: it is another STS dataset from the SemEval 2014 task 1. It has the same scoring mechanism as STSbenchmark, where 0.0 represents the least amount of relatedness and 5.0 represents the most.

\item \textbf{MSRvid}: the Microsoft Research Video Description Corpus contains 1500 sentences that are concise summaries on the content of a short video. Each pair of sentences is also assigned a semantic similarity score between 0.0 and 5.0. 

\item \textbf{MSRP}\cite{quirk2004monolingual}: the Microsoft Research Paraphrase Corpus is a set of 5800 sentence pairs collected from news articles on the Internet. Each sentence pair is labeled 0 or 1, with 1 indicating that the two sentences are paraphrases of each other.
\end{itemize}

Table \ref{tab:datasets} shows a detailed breakdown of the datasets used in evaluation.
For STSbenchmark dataset we use the provided train/dev/test split.
The SICK dataset does not provide development set out of the box, so we extracted 500 instances from the training set as the development set.
For MSRP and MSRvid, since their sizes are relatively small to begin with, we did not create any development set for them.

One metric we used to evaluate the performance of our proposed models on the task of semantic textual similarity estimation is the Pearson Correlation coefficient, commonly denoted by $r$. Pearson Correlation is defined as:
\begin{equation}
\label{eq:pearson}
 r = cov(X,Y) /( \sigma_X \sigma_Y),
\end{equation}
where $cov(X,Y)$ is the co-variance between distributions X and Y, and $\sigma_X$, $\sigma_Y$ are the standard deviations of X and Y.
The Pearson Correlation coefficient can be thought as a measure of how well two distributions fit on a straight line. Its value has range [-1, 1], where a value of 1 indicates that data points from two distribution lie on the same line with a positive slope.
% Due to this unique property, we believe the Pearson Correlation coefficient is a strong indicator of the performance of our metric. 

Another metric we utilized is the Spearman's Rank Correlation coefficient. Commonly denoted by $r_s$, the Spearman's Rank Correlation coefficient shares a similar mathematical expression with the Pearson Correlation coefficient, but it is applied to ranked variables.
Formally it is defined as \cite{wiki:spearman}:
\begin{equation}
\label{eq:spearman}
 \rho = cov(rg_X, rg_Y) / (\sigma_{rg_X} \sigma_{rg_Y}),
\end{equation}
where $rg_X$, $rg_Y$ denotes the ranked variables derived from $X$ and $Y$. $cov(rg_X,rg_Y)$, $\sigma_{rg_X}$, $\sigma_{rg_Y}$ corresponds to the co-variance and standard deviations of the rank variables. The term ranked simply means that each instance in X is ranked higher or lower against every other instances in X and the same for Y. We then compare the rank values of X and Y with \ref{eq:spearman}. Like the Pearson Correlation coefficient, the Spearman's Rank Correlation coefficient has an output range of [-1, 1], and it measures the monotonic relationship between X and Y. A Spearman's Rank Correlation value of 1 implies that as X increases, Y is guaranteed to increase as well.
The Spearman's Rank Correlation is also less sensitive to noise created by outliers compared to the Pearson Correlation.

For the task of paraphrase identification, the classification accuracy of label $1$ and the F1 score are used as metrics. 

In the supervised learning portion, we conduct the experiments on the aforementioned four datasets. We use training sets to train the models, development set to tune the hyper-parameters and each test set is only used once in the final evaluation. For datasets without any development set, we will use cross-validation in the training process to prevent overfitting, that is, use $10\%$ of the training data for validation and the rest is used in training. For each model, we carry out training for 10 epochs. We then choose the model with the best validation performance to be evaluated on the test set.  


\subsection{Unsupervised Matching with OWMD}
\label{subsec:eval-owmd}

To evaluate the effectiveness of our Ordered Word Mover's Distance metric, we first take an unsupervised approach towards the similarity estimation task on the STSbenchmark, SICK and MSRvid datasets. Using the distance metrics listed in Table \ref{tab:compare-pearson} and \ref{tab:compare-spearman}, we first computed the distance between two sentences, then calculated the Pearson Correlation coefficients and the Spearman's Rank Correlation coefficients between all pair's distances and their labeled scores. We did not use the MSRP dataset since it is a binary classification problem.


In our proposed Ordered Word Mover's Distance metric, distance between two sentences is calculated using the order preserving Word Mover's Distance algorithm. For all three datasets, we performed hyper-parameter tuning using the training set and calculated the Pearson Correlation coefficients on the test and development set. We found that for the STSbenchmark dataset, setting $\lambda_1=10$, $\lambda_2=0.03$ produces the most optimal result. For the SICK dataset, a combination of $\lambda_1=3.5$, $\lambda_2=0.015$ works best. And for the MSRvid dataset, the highest Pearson Correlation is attained when $\lambda_1=0.01$, $\lambda_2=0.02$.
We maintain a max iteration of 20 since in our experiments we found that it is sufficient for the correlation result to converge.
During hyper-parameter tuning we discovered that using the Euclidean metric along with $\sigma=10$ produces better results, so all OWMD results summarized in Table \ref{tab:compare-pearson} and \ref{tab:compare-spearman} are acquired under these parameter settings. Finally, it is worth mentioning that our OWMD metric calculates the distances using factorized versions of sentences, while all other metrics use the original sentences. Sentence factorization is a necessary preprocessing step for the OWMD metric.


We compared the performance of Ordered Word Mover's Distance metric with the following methods:

\begin{itemize}
\item \textbf{Bag-of-Words (BoW)}: in the Bag-of-Words metric, distance between two sentences is computed as the cosine similarity between the word counts of the sentences.

\item \textbf{LexVec}~\cite{salle2016enhancing}: calculate the cosine similarity between the  averaged 300-dimensional LexVec word embedding of the two sentences. 

\item \textbf{GloVe}~\cite{pennington2014glove}: calculate the cosine similarity between the averaged 300-dimensional GloVe 6B word embedding of the two sentences. 

\item \textbf{Fastext}~\cite{joulin2016bag}: calculate the cosine similarity between the averaged 300-dimensional Fastext word embedding of the two sentences. 

\item \textbf{Word2vec}~\cite{mikolov2013efficient}: calculate the cosine similarity between the averaged 300-dimensional Word2vec word embedding of the two sentences.

\item \textbf{Word Mover's Distance (WMD)}~\cite{kusner2015word}: estimating the semantic distance between two sentences by WMD introduced in Sec.~\ref{sec:owmd}.
\end{itemize} 


\begin{table}[tb]
  \caption{Pearson Correlation results on different distance metrics.}
  \label{tab:compare-pearson}
  \begin{tabular}{c|cc|cc|c}
    \toprule
    \multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{STSbenchmark} & \multicolumn{2}{c}{SICK} & MSRvid\\ 
     & Test & Dev & Test & Dev & Test\\
    \midrule
    BoW & $0.5705$ & $0.6561$ & $0.6114$ & $0.6087$ & $0.5044$ \\
    LexVec & $0.5759$ & $0.6852$ & $0.6948$ & $\mathbf{0.6811}$ & $0.7318$\\
    GloVe & $0.4064$ & $0.5207$ & $0.6297$ & $0.5892$  & $0.5481$ \\
    Fastext & $0.5079$ & $0.6247$ & $0.6517$ & $0.6421$  & $0.5517$  \\
    Word2vec & $0.5550$ & $0.6911$ & $\mathbf{0.7021}$ & $0.6730$  & $0.7209$  \\
    WMD & $0.4241$ & $0.5679$ & $0.5962$ & $0.5953$  & $0.3430$  \\
    OWMD & $\mathbf{0.6144}$ & $\mathbf{0.7240}$ & $0.6797$ & $0.6772$  & $\mathbf{0.7519}$  \\
    \bottomrule
  \end{tabular}
  \vspace{-4mm}
\end{table}

\begin{table}[tb]
  \caption{Spearman's Rank Correlation results on different distance metrics.}
  \label{tab:compare-spearman}
  \begin{tabular}{c|cc|cc|c}
    \toprule
    \multirow{2}{*}{Algorithm} & \multicolumn{2}{c}{STSbenchmark} & \multicolumn{2}{c}{SICK} & MSRvid\\ 
     & Test & Dev & Test & Dev & Test\\
    \midrule
    BoW & $0.5592$ & $0.6572$ & $0.5727$ & $0.5894$ & $0.5233$ \\
    LexVec & $0.5472$ & $0.7032$ & $0.5872$ & $0.5879$ & $0.7311$\\
    GloVe & $0.4268$ & $0.5862$ & $0.5505$ & $0.5490$  & $0.5828$ \\
    Fastext & $0.4874$ & $0.6424$ & $0.5739$ & $0.5941$  & $0.5634$  \\
    Word2vec & $0.5184$ & $0.7021$ & $0.6082$ & $0.6056$  & $0.7175$  \\
    WMD & $0.4270$ & $0.5781$ & $0.5488$ & $0.5612$  & $0.3699$  \\
    OWMD & $\mathbf{0.5855}$ & $\mathbf{0.7253}$ & $\mathbf{0.6133}$ & $\mathbf{0.6188}$  & $\mathbf{0.7543}$  \\
    \bottomrule
  \end{tabular}
  \vspace{-2mm}
\end{table}


Table \ref{tab:compare-pearson} and Table \ref{tab:compare-spearman} compare the performance of different metrics in terms of the Pearson Correlation coefficients and the Spearman's Rank Correlation coefficients.
We can see that the result of our OWMD metric achieves the best performance on all the datasets in terms of the Spearman's Rank Correlation coefficients.
It also produced the best Pearson Correlation results on the STSbenchmark and the MSRvid dataset, while the performance on SICK datasets are close to the best.
This can be attributed to the two characteristics of OWMD. First, the input sentence is re-organized into a predicate-argument structure using the sentence factorization tree. Therefore, corresponding semantic units in the two sentences will be aligned roughly in order. Second, our OWMD metric takes word positions into consideration and penalizes disordered matches. Therefore, it will produce less mismatches compared with the WMD metric.

% On the SICK dataset, although the result of our metric falls slightly behind Word2vec, LexVec on the test set and Word2vec on the development set, we still believe that it is a superior metric because it produced competitive results across multiple datasets. 

% Table \ref{tab:compare-spearman} presents the Spearman's Rank Correlation coefficients acquired with the same distance metrics. We can observe that our OWMD metric achieves the highest correlation scores on all three datasets. Which proves once again that OWMD is a better distance metric for the task of semantic similarity detection.

\subsection{Supervised Multi-scale Semantic Matching}
\label{subsec:eval-multilayer}

\begin{table*}[tb]
  \caption{A comparison among different supervised learning models in terms of accuracy, F1 score, Pearson's $r$ and Spearman's $\rho$ on various test sets.}
  \label{tab:sts}
  \begin{tabular}{c|cc|cc|cc|cc}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{2}{c}{MSRP} & \multicolumn{2}{c}{SICK} & \multicolumn{2}{c}{MSRvid} & \multicolumn{2}{c}{STSbenchmark}\\ 
     & Acc.(\%) & F1(\%) & $r$ & $\rho$ & $r$ & $\rho$ & $r$ & $\rho$ \\
    \midrule
    MaLSTM & $66.95$ & $73.95$ & $0.7824$ & $0.71843$ & $0.7325$ & $0.7193$ & $0.5739$ & $0.5558$\\
    Multi-scale MaLSTM & $\mathbf{74.09}$ & $\mathbf{82.18}$ & $\mathbf{0.8168}$ & $\mathbf{0.74226}$ & $\mathbf{0.8236}$ & $\mathbf{0.8188}$ & $\mathbf{0.6839}$ & $\mathbf{0.6575}$\\
    \midrule
    HCTI & $73.80$ & $80.85$ & $0.8408$ & $0.7698$ & $\mathbf{0.8848}$ & $\mathbf{0.8763}$  & $\mathbf{0.7697}$ & $\mathbf{0.7549}$ \\
    Multi-scale HCTI & $\mathbf{74.03}$ & $\mathbf{81.76}$ & $\mathbf{0.8437}$ & $\mathbf{0.7729}$ & $0.8763$ & $0.8686$  & $0.7269$ & $0.7033$  \\
    \bottomrule
  \end{tabular}
  \vspace{-2mm}
\end{table*}

The use of sentence factorization can improve both existing unsupervised metrics and existing supervised models. 
% We extend the normal Siamese model to Fig. \ref{fig:network} to take advantage of different level of information in the factorized sentence. 
To evaluate how the performance of existing Siamese neural networks can be improved by our sentence factorization technique and the multi-scale Siamese architecture, we implemented two types of Siamese sentence matching models, HCTI \cite{mueller2016siamese} and MaLSTM \cite{shao2017hcti}. HCTI is a Convolutional Neural Network (CNN) based Siamese model, which achieves the best Pearson Correlation coefficient on STSbenchmark dataset in SemEval2017 competition (compared with all the other neural network models). MaLSTM is a Siamese adaptation of the Long Short-Term Memory (LSTM) network for learning sentence similarity. As the source code of HCTI is not released in public, we implemented it according to \cite{shao2017hcti} by Keras \cite{chollet2015keras}. With the same parameter settings listed in paper \cite{shao2017hcti} and tried our best to optimize the model, we got a Pearson correlation of 0.7697 (0.7833 in paper \cite{shao2017hcti}) in STSbencmark test dataset.

We extended HCTI and MaLSTM to our proposed Siamese architecture in Fig. \ref{fig:network}, namely the Multi-scale MaLSTM and the Multi-scale HCTI. To evaluate the performance of our models, the experiment is conducted on two tasks: 1) semantic textual similarity estimation based on the STSbenchmark, MSRvid, and SICK2014 datasets; 2) paraphrase identification based on the MSRP dataset.

Table \ref{tab:sts} shows the results of HCTI, MaLSTM and our multi-scale models on different datasets. Compared with the original models, our models with multi-scale semantic units of the input sentences as network inputs significantly improved the performance on most datasets. 
Furthermore, the improvements on different tasks and datasets also proved the general applicability of our proposed architecture.

Compared with MaLSTM, our multi-scaled Siamese models with factorized sentences as input perform much better on each dataset. For MSRvid and STSbenmark dataset, both Pearson's $r$ and Spearman's $\rho$ increase about $10\%$ with Multi-scale MaLSTM. Moreover, the Multi-scale MaLSTM achieves the highest accuracy and F1 score on the MSRP dataset compared with other models listed in Table \ref{tab:sts}.

There are two reasons why our Multi-scale MaLSTM significantly outperforms MaLSTM model. First, for an input sentence pair, 
we explicitly model their semantic units with the factorization algorithm.
%we explicitly model the different scales of semantics of them with the semantic units produced by our sentence factorization algorithm. 
Second, our multi-scaled network architecture is 
specifically designed
%specially adapted to 
for multi-scaled sentences representations. Therefore, it is able to explicitly match a pair of sentences at different granularities.

We also report the results of HCTI and Multi-scale HCTI in Table \ref{tab:sts}. For the paraphrase identification task, our model shows better accuracy and F1 score on MSRP dataset. For the semantic textual similarity estimation task, the performance varies across datasets. On the SICK dataset, the performance of Multi-scale HCTI is close to HCTI with slightly better Pearson' $r$ and Spearman's $\rho$. However, the Multi-scale HCTI is not able to outperform HCTI on MSRvid and STSbenchmark. HCTI is still the best neural network model on the STSbenchmark dataset, and the MSRvid dataset is a subset of STSbenchmark.
Although HCTI has strong performance on these two datasets, it performs worse than our model on other datasets.
% Overall, the experimental results demonstrated the superior applicability and generalizability of our proposed models.
Overall, the experimental results demonstrated the general applicability of our proposed model architecture, which performs well on various semantic matching tasks.

% \begin{table}[tb]
%   \caption{Results of Accuracy and F1 score on MSRP test dataset.}
%   \label{tab:MSRP result}
%   \begin{tabular}{lllll}
%     \toprule
%     Model & Acc.(\%) & F1(\%)  \\
%     \midrule
%     MaLSTM & $66.95$ & $73.95$ \\
%     Factorized MaLSTM & $\mathbf{74.09}$ & $\mathbf{82.18}$ \\
%     HCTI & $73.80$ & $80.85$ \\
%     Factorized HCTI & $\mathbf{74.03}$ & $\mathbf{81.76}$ \\
%     \bottomrule
%   \end{tabular}
%   \vspace{0mm}
% \end{table}


% \begin{table}[tb]
%   \caption{Results of Pearson's $r$ and Spearman's $\rho$ on SICK test dataset.}
%   \label{tab:SICK result}
%   \begin{tabular}{lllll}
%     \toprule
%     Model & r & $\rho$ \\
%     \midrule
%     MaLSTM & $0.7824$ & $0.71843$ \\
%     Factorized MaLSTM & $\mathbf{0.8168}$ & $\mathbf{0.74226}$ \\
%     HCTI & $0.8408$ & $\mathbf{0.7698}$ \\
%     Factorized HCTI & $\mathbf{0.8429}$ & $0.7676$ \\
%     \bottomrule
%   \end{tabular}
%   \vspace{0mm}
% \end{table}


% \begin{table}[tb]
%   \caption{Results of Pearson's $r$ and Spearman's $\rho$ on MSRvid test dataset.}
%   \label{tab:MSRvid result}
%   \begin{tabular}{lll}
%     \toprule
%     Model & r & $\rho$  \\
%     \midrule
%     MaLSTM & $0.7325$ & $0.7193$ \\
%     Factorized MaLSTM & $\mathbf{0.8236}$ & $\mathbf{0.8188}$ \\
%     HCTI & $\mathbf{0.8848}$ & $\mathbf{0.8763}$ \\
%     Factorized HCTI & $0.8763$ & $0.8686$ \\
%     \bottomrule
%   \end{tabular}
%   \vspace{0mm}
% \end{table}



% \begin{table}[tb]
%   \caption{Results of Pearson's $r$ and Spearman's $\rho$ on STSbenchmark test dataset.}
%   \label{tab:STSbenchmark result}
%   \begin{tabular}{lllll}
%     \toprule
%     Model & r & $\rho$ \\
%     \midrule
%     MaLSTM & $0.5739$ & $0.5558$ \\
%     Factorized MaLSTM & $\mathbf{0.6839}$ & $\mathbf{0.6575}$ \\
%     HCTI & $\mathbf{0.7697}$ & $\mathbf{0.7549}$ \\
%     Factorized HCTI & $0.7269$ & $0.7033$ \\
%     \bottomrule
%   \end{tabular}
%   \vspace{0mm}
% \end{table}



