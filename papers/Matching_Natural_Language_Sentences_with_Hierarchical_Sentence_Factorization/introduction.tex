%!TEX root = main.tex
\section{Introduction}
\label{sec:intro}

Semantic matching, which aims to model the underlying semantic similarity or dissimilarity among different textual elements such as sentences and documents, has been playing a central role in many Natural Language Processing (NLP) applications, including information extraction \cite{grishman1997information}, top-$k$ re-ranking in machine translation \cite{brown1993mathematics}, question-answering \cite{yu2014deep}, automatic text summarization \cite{ponzanelli2015summarizing}.
However, semantic matching based on either supervised or unsupervised learning remains a hard problem. Natural language demonstrates complicated hierarchical structures, where different words can be organized in different orders to express the same idea. As a result, appropriate semantic representation of text plays a critical role in matching natural language sentences.

Traditional approaches represent text objects as bag-of-words (BoW), term frequency inverse document frequency (TF-IDF) \cite{wu2008interpreting} vectors, or their enhanced variants \cite{paltoglou2010study,robertson1994some}. However, such representations can not accurately capture the similarity between individual words, and do not take the semantic structure of language into consideration. 
Alternatively, word embedding models, such as \textit{word2vec} \cite{mikolov2013efficient} and \textit{Glove} \cite{pennington2014glove}, learn a distributional semantic representation of each word and have been widely used. 

Based on the word-vector representation, a number of unsupervised and supervised matching schemes have been recently proposed.
As an unsupervised learning approach, the Word Mover's Distance (WMD) metric \cite{kusner2015word} measures the dissimilarity between two sentences (or documents) as the minimum distance to transport the embedded words of one sentence to those of another sentence. However, the sequential and structural nature of sentences is omitted in WMD. For example, two sentences containing  exactly the same words in different orders can express totally different meanings.
On the other hand, many supervised learning schemes based on deep neural networks have also been proposed for sentence matching \cite{mueller2016siamese,severyn2015learning,wang2017bilateral,pang2016text}. A common characteristic of many of these neural network models is that they adopt a Siamese architecture, taking the word embedding sequences of a pair of sentences (or documents) as the input, transforming them into intermediate contextual representations via either convolutional or recurrent neural networks, and performing scoring over the contextual representations to yield final matching results. However, these methods rely purely on neural networks to learn the complicated relationships among sentences, and many obvious compositional and hierarchical features are often overlooked or not explicitly utilized.


In this paper, however, we argue that a successful semantic matching algorithm needs to best characterize the sequential, hierarchical and flexible structure of natural language sentences, as well as the rich interaction patterns among semantic units.
We present a technique named \textit{Hierarchical Sentence Factorization} (or \emph{Sentence Factorization} in short), which is able to represent a sentence in a hierarchical semantic tree, with each node (semantic unit) at different depths of the tree reorganized into a normalized ``predicate-argument'' form.
Such normalized sentence representation enables us to propose new methods to both improve unsupervised semantic matching by taking the structural and sequential differences between two text entities into account, and enhance a range of supervised semantic matching schemes, by overcoming the limitation of the representation capability of convolutional or recurrent neural networks, especially when labelled training data is limited.
%especially when the availability of labels is limited.
Specifically, we make the following contributions:

% a new semantic distance metric named \textit{Ordered Word Mover's Distance}. It casts the calculation of semantic distance between texts as an optimal transport problem while preserving the temporal structure of words in sentences. Furthermore, we propose a new deep neural network architecture which takes the hierarchical representation of sentences as input and match sentences from different granularity. 




\textit{First}, the proposed \textit{Sentence Factorization} scheme factorizes a sentence recursively into a hierarchical tree of semantic units, where each unit is a subset of words from the original sentence.
%Different layers of the factorization tree factorize the original sentence into different granularities. 
%The words of each semantic unit in the tree are reordered into a ``predicate-argument'' structure.
Words are then reordered into a ``predicate-argument'' structure.
Such form of sentence representation offers two benefits: i) the flexible syntax structures of the same sentence, for example, active and passive sentences, can be normalized into a unified representation; ii) the semantic units in a pair of sentences can be aligned according to their depth and order in the factorization tree.
% we parse a sentence into the form of Abstract Meaning Representation (AMR) \cite{banarescu2013abstract,flanigan2014discriminative}, and align input sentence with its AMR representation \cite{pourdamghani2014aligning}. 

\textit{Second}, for unsupervised text matching, we combine the factorized and reordered representation of sentences and the Order-preserving Wasserstein Distance \cite{su2017order} (which was originally proposed to match hand-written characters in computer vision) to propose a new semantic distance metric between text objects, which we call \textit{Ordered Word Mover's Distance}. Compared with the recently proposed Word Mover's Distance  \cite{kusner2015word}, our new metric achieves significant improvement by taking the sequential structures of sentences into account. For example, without considering the order of words, the Word Mover's Distance between the sentences
 ``Tom is chasing Jerry'' and ``Jerry is chasing Tom'' is zero. In contrast, our new metric is able to penalize such order mismatch between words, and identify the difference between the two sentences.

\textit{Third}, for supervised semantic matching, we extend the existing Siamese network architectures (both for CNN and LSTM) to multi-scaled models, where each scale adopts an individual Siamese network, taking as input the vector representations of the two sentences at the corresponding depth in the factorization trees, ranging from the coarse-grained scale to fine-grained scales. 
%the hierarchical representation of a sentence in different layers of its factorization tree as the input, and extend 
When increasing the number of layers in the corresponding neural network can hardly improve performance, hierarchical sentence factorization provides a novel means to extend the original deep networks to a ``richer'' model that matches a pair of sentences through a multi-scaled semantic unit matching process. 
%design new deep neural network models to match sentence pairs from different semantic granularity. 
%By adapting the network architecture with the hierarchical sentence structure, we are able to measure the similarity and dissimilarity between a pair of sentences through a multi-scale semantic unit matching process. 
Our proposed multi-scaled deep neural networks can effectively improve existing deep models by measuring the similarity between a pair of sentences at different semantic granularities. For instance, Siamese networks based on CNN and BiLSTM \cite{mueller2016siamese,shao2017hcti} that originally only take the word sequences as the inputs.

We extensively evaluate the performance of our proposed approaches on the task of semantic textual similarity estimation and paraphrase identification, based on multiple datasets, including the STSbenchmark dataset, the Microsoft Research Paraphrase identification (MSRP) dataset, the SICK dataset and the MSRvid dataset. Experimental results have shown that our proposed algorithms and models can achieve significant improvement compared with multiple existing unsupervised text distance metrics, such as the Word Mover's Distance \cite{kusner2015word}, as well as supervised deep neural network models, including Siamese Neural Network models based on CNN and BiLSTM \cite{mueller2016siamese,shao2017hcti}.

The remainder of this paper is organized as follows.
Sec.~\ref{sec:sentence} presents our hierarchical sentence factorization algorithm.
Sec.~\ref{sec:owmd} presents our Ordered Word Mover's Distance metric based on sentence structural reordering.
In Sec.~\ref{sec:multi-layer}, we propose our multi-scaled deep neural network architectures based on hierarchical sentence representation.
In Sec.~\ref{sec:eval}, we conduct extensive evaluations of the proposed methods based on multiple datasets on multiple tasks.
Sec.~\ref{sec:related} reviews the related literature.
The paper is concluded in Sec.~\ref{sec:conclude}.


