%!TEX root = main.tex
\section{Ordered Word Mover's Distance}
\label{sec:owmd}

\begin{figure}[tb]
\centering
\includegraphics[width=2.5in]{figure/SentenceMatch}
\vspace{0mm}
\caption{Compare the sentence matching results given by Word Mover's Distance and Ordered Word Mover's Distance.}
\label{fig:match}
\vspace{-3mm}
\end{figure}

The proposed hierarchical sentence factorization technique naturally reorders an input sentence into a unified format at the root node. In this section, we introduce the \textit{Ordered Word Mover's Distance} metric which measures the semantic distance between two input sentences based on the unified representation of reordered sentences.

Assume $X\in \mathbb{R}^{d\times n}$ is a \textit{word2vec} embedding matrix for a vocabulary of $n$ words, and the $i$-th column $\mathbf{x}_i \in \mathbb{R}^d$ represents the $d$-dimensional embedding vector of $i$-th word in vocabulary.
Denote a sentence $S = \overline{a_1a_2\cdots a_K}$ where $a_i$ represents the $i$-th word (or the word embedding vector).
The Word Mover's Distance considers a sentence $S$ as its normalized bag-of-words (nBOW) vectors where the weights of the words in $S$ is $\mathbf \alpha = \{\alpha_1, \alpha_2, \cdots, \alpha_K\}$. Specifically, if word $a_i$ appears $c_i$ times in $S$, then $\alpha_i = \frac{c_i}{\sum_{j=1}^K c_j}$.

The Word Mover's Distance metric combines the normalized bag-of-words representation of sentences with Wasserstein distance (also known as Earth Mover's Distance \cite{rubner2000earth}) to measure the semantic distance between two sentences. 
Given a pair of sentences $S_1 = \overline{a_1a_2\cdots a_M}$ and $S_2 = \overline{b_1b_2\cdots b_N}$, where %$a_i \in \mathbb{R}^d$ is the embedding vector of the $i$-th word in $S_1$ and 
$b_j \in \mathbb{R}^d$ is the embedding vector of the $j$-th word in $S_2$. Let $\mathbf \alpha = \{\alpha_1, \cdots, \alpha_M\}$ and $\mathbf \beta = \{\beta_1, \cdots, \beta_N\}$ represents the normalized bag-of-words vectors of $S_1$ and $S_2$. We can calculate a distance matrix $D \in \mathbb{R}^{M\times N}$ where each element
$D_{ij} = \|a_i - b_j\|_2$ measures the distance between word $a_i$ and $b_j$ (we use the same notation to denote the word itself or its word vector representation).
Let $T \in \mathbb{R}^{M\times N}$ be a non-negative sparse transport matrix where $T_{ij}$ denotes the portion of word $a_i \in S_1$ that transports to word $b_j \in S_2$.
The Word Mover's Distance between sentences $S_1$ and $S_2$ is given by $\sum_{i,j} T_{ij} D_{ij}$. The transport matrix $T$ is computed solving the following constrained optimization problem:
\begin{equation}
\label{eq:wmd}
\begin{split}
	\underset{T \in \mathbb{R}_{+}^{M\times N}}{\mbox{minimize}}\quad 		& \sum_{i,j} T_{ij} D_{ij} \\
	\mbox{subject to}\quad & \sum\limits_{i = 1}^{M}  T_{ij} = \beta_j \quad 1\leq j \leq N,\\
			   & \sum\limits_{j = 1}^{N}  T_{ij} = \alpha_i \quad 1\leq i \leq M.
\end{split}
\end{equation}
Where the minimum ``word travel cost'' between two bags of words for a pair of sentences is calculated to measure the their semantic distance.

However, the Word Mover's Distance fails to consider a few aspects of natural language. First, it omits the sequential structure. For example, in Fig.~\ref{fig:match}, the pair of sentences ``Morty is laughing at Rick'' and ``Rick is laughing at Morty'' only differ in the order of words. The Word Mover's Distance metric will then find an exact match between the two sentences and estimate the semantic distance as zero, which is obviously false. Second, the normalized bag-of-words representation of a sentence can not distinguish duplicated words shown in multiple positions of a sentence.

To overcome the above challenges, we propose a new kind of semantic distance metric named Ordered Word Mover's Distance (OWMD). The Ordered Word Mover's Distance combines our sentence factorization technique with Order-preserving Wasserstein Distance proposed in \cite{su2017order}. It casts the calculation of semantic distance between texts as an optimal transport problem while preserving the sequential structure of words in sentences. The Ordered Word Mover's Distance differs from the Word Mover's Distance in multiple aspects.

First, rather than using normalized bag-of-words vector to represent a sentence, we decompose and re-organize a sentence using the sentence factorization algorithm described in Sec.~\ref{sec:sentence}. Given a sentence $S$, we represent it by the reordered word sequence $S'$ in the root node of its sentence factorization tree. Such representation normalizes a sentence into ``predicate-argument'' structure to better handle syntactic variations.
%process sentences with different syntactic structures that express the same meaning. 
For example, after performing sentence factorization, sentences ``Tom is chasing Jerry'' and ``Jerry is being chased by Tom'' will both be normalized as ``chase Tom Jerry''.

Second, we calculate a new transport matrix $T$ by solving the following optimization problem
\begin{equation}
\label{eq:owmd}
\begin{split}
	\underset{T \in \mathbb{R}_{+}^{M\times N}}{\mbox{minimize}}\quad 		& \sum_{i,j} T_{ij} D_{ij} - \lambda_1 I(T) + \lambda_2 KL(T||P)\\
	\mbox{subject to}\quad & \sum\limits_{i = 1}^{M}  T_{ij} = \beta_j' \quad 1\leq j \leq N',\\
			   & \sum\limits_{j = 1}^{N}  T_{ij} = \alpha_i' \quad 1\leq i \leq M',
\end{split}
\end{equation}
where $\lambda_1 > 0$ and $\lambda_2 > 0$ are two hyper parameters.
$M'$ and $N'$ denotes the number of words in $S_1'$ and $S_2'$.
$\alpha_i'$ denotes the weight of the $i$-th word in normalized sentence $S_1'$ and $\beta_j'$ denotes the weight of the $j$-th word in normalized sentence $S_2'$. Usually we can set $\mathbf{\alpha'} = (\frac{1}{M'}, \cdots, \frac{1}{M'})$ and $\mathbf{\beta'} = (\frac{1}{N'}, \cdots, \frac{1}{N'})$ without any prior knowledge of word differences.

The first penalty term $I(T)$ is the inverse difference moment \cite{albregtsen2008statistical} of the transport matrix $T$ that measures local homogeneity of $T$. It is defined as:
\begin{equation}
\label{eq:IT}
\begin{split}
	I(T) = \sum\limits_{i=1}^{M'} \sum\limits_{j=1}^{N'} \frac{T_{ij}}{(\frac{i}{M'} - \frac{j}{N'})^2 + 1}.
\end{split}
\end{equation}
$I(T)$ will have a relatively large value if the large values of $T$ mainly appear near its diagonal.

Another penalty term $KL(T||P)$ denotes the KL-divergence between $T$ and $P$. 
$P$ is a two-dimensional distribution used as the prior distribution for values in $T$. It is defined as
\begin{equation}
\label{eq:P}
\begin{split}
	P_{ij} = \frac{1}{\sigma \sqrt{2\pi}}e^{- \frac{l^2(i,j)}{2\sigma^2}}
\end{split}
\end{equation}
where $l(i, j)$ is the distance from position $(i, j)$ to the diagonal line, which is calculated as
\begin{equation}
\label{eq:l}
\begin{split}
	l(i, j) = \frac{|i/M' - j/N'|}{\sqrt{1/M'^2 + 1/N'^2}}.
\end{split}
\end{equation}
As we can see, the farther a word in one sentence is from the other word in another sentence in terms of word orders, the less likely it will be transported to that word. Therefore, by introducing the two penalty terms $I(T)$ and $KL(T||P)$ into problem~(\ref{eq:owmd}), 
we encourage words at similar positions in two sentences to be matched.
%we encourage the words in two sentences with similar positions be matched. 
Words at distant positions are less likely to be matched by $T$.

The problem (\ref{eq:owmd}) has a unique optimal solution $T^{\lambda_1, \lambda_2}$ since both the objective and the feasible set are convex. It has been proved in \cite{su2017order} that the optimal $T^{\lambda_1, \lambda_2}$ has the same form with $diag(\mathbf{k}_1) \cdot \mathbf{K} \cdot diag(\mathbf{k}_2)$, where $diag(\mathbf{k}_1) \in \mathbb{R}^{M'}$ and $diag(\mathbf{k}_2) \in \mathbb{R}^{N'}$ are two diagonal matrices with strictly positive diagonal elements. $\mathbf{K} \in \mathbb{R}^{M'\times N'}$ is a matrix defined as
\begin{equation}
\label{eq:K}
\begin{split}
	K_{ij} = P_{ij} e^{\frac{1}{\lambda_2}(S_{ij}^{\lambda_1} - D_{ij})},
\end{split}
\end{equation}
where
\begin{equation}
\label{eq:S}
\begin{split}
	S_{ij} = \frac{\lambda_1}{(\frac{i}{M'} - \frac{j}{N'})^2 + 1}.
\end{split}
\end{equation}
The two matrices $\mathbf{k}_1$ and $\mathbf{k}_2$ can be efficiently obtained by the Sinkhorn-Knopp iterative matrix scaling algorithm \cite{knight2008sinkhorn}:
\begin{equation}
\label{eq:k1}
\begin{split}
	\mathbf{k}_1 &\leftarrow \mathbf{\alpha'} ./ K \mathbf{k}_2, \\
	\mathbf{k}_2 &\leftarrow \mathbf{\beta'} ./ K^T \mathbf{k}_1.
\end{split}
\end{equation}
where $./$ is the element-wise division operation.
%Until now, we have introduced the sentence representation and the way we calculate the transport matrix $T$ for the Ordered Word Mover's Distance metric.
Compared with Word Mover's Distance, the Ordered Word Mover's Distance 
considers the positions of words in a sentence,
%takes the positions of words in sentences into account, 
and is able to distinguish duplicated words at different locations. For example, in Fig.~\ref{fig:match}, while the WMD finds an exact match and get a semantic distance of zero for the sentence pair ``Morty is laughing at Rick'' and ``Rick is laughing at Morty'', the OWMD metric is able to find a better match relying on the penalty terms, and gives a semantic distance greater than zero.

The computational complexity of OWMD is also effectively reduced compared to WMD. With the additional constraints, the time complexity is $O(dM'N')$ where $d$ is the dimension of word vectors \cite{su2017order}, while it is $O(dp^3\log p)$ for WMD, where $p$ denotes the number of uniques words in sentences or documents \cite{kusner2015word}.

