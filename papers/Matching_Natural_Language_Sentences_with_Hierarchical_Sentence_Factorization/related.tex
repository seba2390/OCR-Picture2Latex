%!TEX root = main.tex
\section{Related Work}
\label{sec:related}


The task of natural language sentence matching has been extensively studied for a long time. Here we review related unsupervised and supervised models for sentence matching.

Traditional unsupervised metrics for document representation, including bag of words (BOW), term frequency inverse document frequency (TF-IDF) \cite{wu2008interpreting}, Okapi BM25 score \cite{robertson1994some}. However, these representations can not capture the semantic distance between individual words.
Topic modeling approaches such as Latent Semantic Indexing (LSI) \cite{deerwester1990indexing} and Latent Dirichlet Allocation (LDA) \cite{blei2003latent} attempt to circumvent the problem through learning a latent representation of documents.
But when applied to semantic-distance based tasks such as text-pair semantic similarity estimation, these algorithms usually cannot achieve good performance.


Learning distributional representation for words, sentences or documents based on deep learning models have been popular recently. \textit{word2vec} \cite{mikolov2013efficient} and \textit{Glove} \cite{pennington2014glove} are two high quality word embeddings that have been extensively used in many NLP tasks. Based on word vector representation, the Word Mover's Distance (WMD) \cite{kusner2015word} algorithm measures the dissimilarity between two sentences (or documents)
as the minimum distance that
the embedded words of one sentence need to
``travel'' to reach the embedded words of another
sentence. However, when applying these approaches to sentence pair matching tasks, the interactions between sentence pairs are omitted, also the ordered and hierarchical structure of natural languages is not considered.


Different neural network architectures have been proposed for sentence pair matching tasks. Models based on Siamese architectures \cite{mueller2016siamese,severyn2015learning,neculoiu2016learning,baudivs2016sentence} usually transform the word embedding sequences of text pairs into context representation vectors through a multi-layer Long Short-Term Memory (LSTM) \cite{sundermeyer2012lstm} network or Convolutional Neural Networks (CNN) \cite{krizhevsky2012imagenet}, followed by a fully connected network or score function which gives the similarity score or classification label based on the context representation vectors. However, Siamese models defer the interaction between two sentences until the hidden representation layer, therefore may lose details of sentence pairs for matching tasks \cite{hu2014convolutional}.

Aside from Siamese architectures, \cite{wang2017bilateral} introduced a matching layer into Siamese network to compare the contextual embedding of one sentence with another. \cite{hu2014convolutional,pang2016text} proposed convolutional matching models that consider all pair-wise interactions between words in sentence pairs.  \cite{he2016pairwise} propose to explicitly model pairwise word interactions with a pairwise word interaction similarity cube and a similarity focus layer to identify important word interactions.

