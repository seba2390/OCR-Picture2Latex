%!TEX root = main.tex
\section{Hierarchical Sentence Factorization and Reordering}
\label{sec:sentence}


\begin{figure*}[tb]
\centering
\includegraphics[width=\textwidth]{figure/CaseStudy}
\vspace{-3mm}
\caption{An example of the sentence factorization process. Here we show: A. The original sentence pair; B. The procedures of creating sentence factorization trees; C. The predicate-argument form of original sentence pair; D. The alignment of semantic units with the reordered form.}
\label{fig:casestudy}
\vspace{-1mm}
\end{figure*}

In this section, we present our \textit{Hierarchical Sentence Factorization} techniques to transform a sentence into a hierarchical tree structure, which also naturally produces a reordering of the sentence at the root node. 
%Such multi-scaled sentence factorization and reordering 
This multi-scaled representation form proves to be effective at improving both unsupervised and supervised semantic matching, which will be discussed in Sec.~\ref{sec:owmd} and Sec.~\ref{sec:multi-layer}, respectively. 

We first describe our desired factorization tree structure before presenting the steps to obtain it. Given a natural language sentence $S$, our objective is to transform it into a semantic factorization tree denoted by $T^f_S$. Each node in $T^f_S$ is called a \textit{semantic unit}, which contains one or a few tokens (tokenized words) from the original sentence $S$, as illustrated in Fig.~\ref{fig:casestudy} $(a4)$, $(b4)$.
The tokens in every semantic unit in $T^f_S$ is re-organized into a ``predicate-argument'' form. For example, a semantic unit for ``Tom catches Jerry'' in the ``predicate-argument'' form will be ``catch Tom Jerry''.

Our proposed factorization tree recursively factorizes a sentence into a hierarchy of semantic units at different granularities to represent the semantic structure of that sentence.
%In different depths of a factorization tree, a sentence will be factorized into semantic units in different granularities.
The root node in a factorization tree contains the entire sentence reordered in the predicate-argument form, thus providing a ``normalized'' representation for sentences expressed in different ways (e.g., passive vs. active tenses). Moreover, each semantic unit at depth $d$ will be further split into several child nodes at depth $d + 1$, which are smaller semantic sub-units. Each sub-unit also follows the predicate-argument form.

For example, in Fig.~\ref{fig:casestudy}, we convert sentence $A$ into a hierarchical factorization tree $(a4)$ using a series of operations. The root node of the tree contains the semantic unit ``chase Tom Jerry little yard big'', which is the reordered representation of the original sentence ``The little Jerry is being chased by Tom in the big yard'' in a semantically normalized form. 
Moreover, the semantic unit at depth $0$ is factorized into four sub-units at depth $1$: ``chase'', ``Tom'', ``Jerry little'' and ``yard big'', each in the ``predicate-argument'' form. And at depth $2$, the semantic sub-unit ``Jerry little'' is further factorized into two sub-units ``Jerry'' and ``little''. Finally, a semantic unit that contains only one token (e.g., ``chase'' and ``Tom'' at depth $1$) can not be further decomposed. Therefore, it only has one child node at the next depth through self-duplication.

We can observe that each depth of the tree contains all the tokens (except meaningless ones) in the original sentence, but re-organizes these tokens into semantic units of different granularities. %In Sec.~\ref{sec:owmd} and Sec.~\ref{sec:multi-layer}, we will show that such a specially designed sentence factorization tree together with the naturally generated semantic reordering can be used to effectively improve both unsupervised and supervised semantic matching methods.


% Our approach is mainly based on the Abstract Meaning Representation (AMR) of a sentence \cite{banarescu2013abstract, flanigan2014discriminative}.

\subsection{Hierarchical Sentence Factorization}
\label{subsec:sf}

We now describe our detailed procedure to transform a natural language sentence to the desired factorization tree mentioned above.
Our Hierarchical Sentence Factorization algorithm mainly consists of five steps: 1) AMR parsing and alignment, 2) AMR purification, 3) index mapping, 4) node completion, and 5) node traversal. The latter four steps are illustrated in the example in Fig.~\ref{fig:casestudy} from left to right.%We will explain each step in detail in the following.


\begin{figure}[tb]
\centering
\includegraphics[width=2.7in]{figure/amr}
\vspace{0mm}
\caption{An example of a sentence and its Abstract Meaning Representation (AMR), as well as the alignment between the words in the sentence and the nodes in AMR.}
\label{fig:amr}
\vspace{-3mm}
\end{figure}

\textbf{AMR parsing and alignment.}
Given an input sentence, the first step of our hierarchical sentence factorization algorithm is to acquire its Abstract Meaning Representation (AMR), as well as perform AMR-Sentence alignment to align the concepts in AMR with the tokens in the original sentence.
 
Semantic parsing \cite{baker1998berkeley,kingsbury2002treebank,berant2014semantic,banarescu2013abstract, damonte2016incremental} can be performed to generate the formal semantic representation of a sentence.
Abstract Meaning Representation (AMR) \cite{banarescu2013abstract} is a semantic parsing language that represents a sentence by a directed acyclic graph (DAG).
Each AMR graph can be converted into an AMR tree by duplicating the nodes that have more than one parent.

Fig.~\ref{fig:amr} shows the AMR of the sentence ``I observed that the army moved quickly.''
In an AMR graph, leaves are labeled with concepts, which represent either English words (e.g., ``army''), PropBank framesets (e.g., ``observe-01'') \cite{kingsbury2002treebank}, or special keywords (e.g., dates, quantities, world regions, etc.). For example, ``(a / army)'' refers to an instance of the concept army, where ``a'' is the variable name of army (each entity in AMR has a variable name).  ``ARG0'', ``ARG1'', ``:manner'' are different kinds of relations defined in AMR. Relations are used to link entities. For example, ``:manner'' links ``m / move-01'' and ``q / quick'', which means ``move in a quick manner''. Similarly, ``:ARG0'' links ``m / move-01'' and ``a / army'', which means that ``army'' is the first argument of ``move''.


%Given an input sentence, a semantic parser extracts a semantic representation of the sentence \cite{damonte2016incremental}.
% AMR aims to \red{normalize (??canonicalize)} different ways of stating the same thing and assigns the same AMR to sentences with the same meaning.


Each leaf in AMR is a concept rather than the original token in a sentence. 
The alignment between a sentence and its AMR graph is not given in the AMR annotation. Therefore, AMR alignment \cite{pourdamghani2014aligning} needs to be performed to link the leaf nodes in the AMR to tokens in the original sentence.
Fig.~\ref{fig:amr} shows the alignment between sentence tokens and AMR concepts by the alignment indexes.
The alignment index $0$ is for the root node, 0.0 for the first child of the root node, 0.1 for the second child of the root node, and so forth. For example, in Fig.~\ref{fig:amr}, the word ``army'' in sentence is linked with index ``0.1.0'', which represents the concept node ``a / army'' in its AMR.
% The numbering system will skip variable \red{re-entrancies} of duplicated nodes.
% AMR strives to achieve a more logical representation for sentences. It attempts to assign the same AMR to sentences that have the same meaning. For example, ``I observed the quick movement of the army'' and ``I observed the army moved quickly'' will have the same AMR representation that is shown in Fig.~\ref{fig:amr}.
We refer interested readers to \cite{banarescu2013abstract,banarescu2012abstract} for more detailed description about AMR. 

% We utilize the AMR representation of sentences to get a unified, hierarchical and reordered sentence representation.
%Alg.~.... shows the steps of sentence factorization.
Various parsers have been proposed for AMR parsing and alignment \cite{flanigan2014discriminative,wang2015boosting}. We choose the JAMR parser \cite{flanigan2014discriminative} in our algorithm implementation. 

\begin{figure}[tb]
\centering
\includegraphics[width=3.0in]{figure/amr2}
\vspace{0mm}
\caption{An example to show the operation of AMR purification.}
\label{fig:amr2}
\vspace{-3mm}
\end{figure}


\textbf{AMR purification.} Unfortunately, AMR itself cannot be used to form the desired factorization tree. 
First, it is likely that multiple concepts in AMR may link to the same token in the sentence.
For example, Fig.~\ref{fig:amr2} shows AMR and its alignment for the sentence ``Three Asian kids are dancing.''.
The token ``Asian'' is linked to four concepts in the AMR graph: `` continent (0.0.0)'', ``name (0.0.0.0)'', ``Asia (0.0.0.0.0)'' and ``wiki Asia (0.0.0.1)''.
This is because AMR will match a named entity with predefined concepts which it belongs to, such as ``c / continent'' for ``Asia'', and form a compound representation of the entity. For example, in Fig.\ref{fig:amr2}, the token ``Asian'' is represented as a continent whose name is Asia, and its Wikipedia entity name is also Asia.

In this case, we select the link index with the smallest tree depth as the token's position in the tree.
Suppose $\mathcal{P}_w = \{p_1, p_2, \cdots, p_{|\mathcal{P}|}\}$ denotes the set of alignment indexes of token $w$.
We can get the desired alignment index of $w$ by calculating the longest common prefix of all the index strings in  $\mathcal{P}_w$.
After getting the alignment index for each token, we then replace the concepts in AMR with the tokens in sentence by the alignment indexes, and remove relation names (such as ``:ARG0'') in AMR, resulting into a compact tree representation of the original sentence, as shown in the right part of Fig.~\ref{fig:amr2}.


\textbf{Index mapping.}
A purified AMR tree for a sentence obtained in the previous step is still not in our desired form.
To transform it into a hierarchical sentence factorization tree, we perform index mapping and calculate a new position (or index) for each token in the desired factorization tree given its position (or index) in the purified AMR tree.
Fig.~\ref{fig:casestudy} illustrates the process of index mapping. After this step, for example, the purified AMR trees in Fig.~\ref{fig:casestudy} $(a1)$ and $(b1)$ will be transformed into $(a2)$ and $(b2)$.

Specifically, let $T^p_S$ denote a purified AMR tree of sentence $S$, and $T^f_S$ our desired sentence factorization tree of $S$.
Let $I^p_N = \overline{i_0.i_1.i_2.\cdots.i_d}$ denote the index of node $N$ in $T^p_S$, where $d$ is the depth of $N$ in $T^p_S$ (where depth 0 represents the root of a tree).
Then, the index $I^f_N$ of node $N$ in our desired factorization tree $T^f_S$ will be calculated as follows:
\begin{equation}
\label{eqn:transform-index}
I^f_N := \begin{cases}
 		     \overline{0.0} & \quad \text{if } d=0, \\ 
 			 \overline{i_0.(i_1+1).(i_2+1).\cdots.(i_d + 1)} & \quad \text{otherwise}. 
		 \end{cases}
\end{equation}
After index mapping, we add an empty root node with index $0$ in the new factorization tree, and link all nodes at depth $1$ to it as its child nodes. Note that the $i_0$ in every node index will always be 0.


\textbf{Node completion.}
We then perform node completion to make sure each branch of the factorization tree have the same maximum depth and to fill in the missing nodes caused by index mapping, illustrated by Fig.~\ref{fig:casestudy} $(a3)$ and $(b3)$.

First, given a pre-defined maximum depth $D$, for each leaf node $N^l$ with depth $d < D$ in the current $T^f_S$ after index mapping, we duplicate it for $D - d$ times and append all of them sequentially to $N^l$, as shown in Fig.~\ref{fig:casestudy} $(a3)$, $(b3)$, such that the depths of the ending nodes will always be $D$. For example, in Fig.~\ref{fig:casestudy} with $D = 2$, the node ``chase (0.0)'' and ``Tom (0.1)'' will be extended to reach depth $2$ via self-duplication.

Second, after index mapping, the children of all the non-leaf nodes, except the root node, will be indexed starting from 1 rather than 0. For example, in Fig.~\ref{fig:casestudy} $(a2)$, the first child node of ``Jerry (0.2)'' is ``little (0.2.1)''. %However, the first child node according to our indexing rule should have the index of ``0.2.0'', which is missed at present. 
In this case, we duplicate ``Jerry (0.2)'' itself to ``Jerry (0.2.0)'' to fill in the missing first child of ``Jerry (0.2)''. Similar filling operations are done for other non-leaf nodes after index mapping as well.

\textbf{Node traversal to complete semantic units}.
Finally, we complete each semantic unit in the formed factorization tree via node traversal, as shown in Fig.~\ref{fig:casestudy} $(a4)$, $(b4)$.
For each non-leaf node $N$, we traverse its sub-tree by Depth First Search (DFS). The original semantic unit in $N$ will then be replaced by the concatenation of the semantic units of all the nodes in the sub-tree rooted at $N$, following the order of traversal.
%Notice that the traverse process should skip all the nodes completed in the previous step of node completion.

For example, for sentence $A$ in Fig.~\ref{fig:casestudy}, after node traversal, the root node of the factorization tree becomes ``chase Tom Jerry little yard big'' with index ``0''. We can see that the original sentence has been reordered into a predicate-argument structure. A similar structure is generated for the other nodes at different depths. 
Until now, each depth of the factorization tree $T^f_S$ can express the full sentence $S$  in terms of  semantic units at different granularity.



