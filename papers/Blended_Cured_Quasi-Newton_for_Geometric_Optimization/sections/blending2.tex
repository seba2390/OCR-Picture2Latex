\section{Blended Quasi-Newton}
\label{sec:blend}

In this section we construct a new quadratic energy proxy which
effectively blends the Sobolev gradient with L-BFGS-style updates
to capture curvature information, avoiding the troubles previous
quasi-Newton methods have encountered in geometry optimization.
Apart from the aforementioned issue of a dense proxy incorrectly
coupling distant vertices in L-BFGS and SL-BFGS, we also find that
the gradients for non-convex energies with barriers can have highly disparate
scales, causing further trouble for L-BFGS. The much smoother
Sobolev gradient diffuses large entries from highly distorted
elements to the neighborhood, giving a much better scaling.
The Laplacian also provides essentially the correct structure for
the proxy, only directly coupling neighboring elements in the mesh,
and is well-behaved initially when far from the solution, thus we
seek to stay close to the Sobolev gradient, as much as possible, while
still capturing valuable curvature information from gradient history.

The standard (L-)BFGS approach exploits the secant approximation
from the difference in successive gradients, 
$y_i = \nabla E(x_{i+1}) - \nabla E(x_{i})$ compared to the
difference in positions $s_i = x_{i+1}-x_i$,
\begin{equation}
\label{eq:proxy_1}
\begin{aligned}
 \nabla^2 E(x_{i+1}) s_i & \simeq  y_i \\
\Rightarrow \quad \nabla^2 E(x_{i+1})^{-1} y_i & \simeq s_i,
\end{aligned}
\end{equation}
updating the current inverse proxy matrix $D_i$ (approximating
$\nabla^2 E^{-1}$ in some sense) so that $D_{i+1}y_i = s_i$.
The BFGS quasi-Newton update is generically
\begin{equation}
\label{eq:BFGS_update}
\mathrm{QN}_i(z, D) = V_i(z)^T D V_i(z) + \frac{s_i s_i^T}{s_i^Tz},  \> \> V_i(z) = I - \tfrac{z s_i^T}{s_i^Tz}.
\end{equation}
We can understand this as using a projection matrix $V_i$ to annihilate
the old $D$'s action on $z$, then adding a positive semi-definite
symmetric rank-one matrix to
enforce $\mathrm{QN}_i(z,D)z = s_i$. Classic BFGS uses
$D_{i+1} = \mathrm{QN}_i(y_i, D_i)$, whereas L-BFGS uses
\begin{equation}
    D_{i+1} = \mathrm{QN}_i(y_i, \tilde{D}_i),
\end{equation}
where $\tilde{D}_i$ has the oldest $\mathrm{QN}$ update removed,
and crucially represents each $D$ as a product of linear operators,
rather than an explicit full matrix. Only the last $m$ $\{s,y\}$ vector pairs (we
use $m=5$) along with the initial $D_1$ (we use the inverse Laplacian,
storing only its Cholesky factor) are stored; application of $D$ is
then just a few vector dot-products and updates along with backsolves for
the Laplacian.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\linewidth]{figures/Figure_5/mips_compare}
\caption{A 2D shearing deformation stress
test with MIPS energy, comparing methods by plotting iteration vs.\ energy. Both L-BFGS 
as well as inverse Laplacian initialized (SL-BFGS) have slow convergence as previously
reported -- especially when compared to SGD and AQP which use just
the Laplacian. 
At iteration 240 the visualized deformations show both L-BFGS-based methods suffering from
swelling due to inaccurate coupling of distant elements.
Applying our blending model alone (Blended) is highly
effective, while our full BCQN method gives the best results overall.}
\label{fig:quadratic_compare}
\end{figure}

\subsection{Greedy Laplacian Blending}

Experiments show that far from the solution, the Laplacian is often a much
more effective proxy than the L-BFGS secant version: see AQP/SGD vs.\ L-BFGS in Figure \ref{fig:quadratic_compare}.
In particular, the difference in energies $y$ may introduce spurious coupling or
have badly scaled entries near distorted triangles. In this
case if the energy were based on the Laplacian itself (the 
\emph{Dirichlet} energy), the difference in gradients would be the better
behaved $Ls$. This motivates trying the update with $Ls$ instead of $y$,
\begin{equation}
\label{eq:qn_L}
D_{i+1} = \mathrm{QN}_i(L s_i, \tilde{D}_i),
\end{equation}
which will keep us consistent with Sobolev preconditioning, which is very effective
in initial iterations. However, to achieve the
superlinear convergence L-BFGS offers, near the solution we will want
to come closer to satsifying the secant equation, switching to using $y$ instead.

We can thus imagine a blending strategy, which uses
\begin{equation}
z_i=(1-\beta_i)y_i + \beta_i Ls_i
\end{equation}
in $\mathrm{QN}(z_i, \tilde{D}_i)$, with blending parameter $\beta_i \in [0,1]$.
A greedy strategy might choose $\beta_i$ to scale $Ls_i$ to be as close to $y_i$ as possible,
\begin{equation}
\label{eq:BQCN_proj}
\beta_i = \argmin_{\beta\in[0,1]} \| y_i - \beta L s_i \|^2,
\end{equation}
in other words using the projection of $y_i$ onto $Ls_i$. This comes as close as possible to
satsifying the secant equation with $Ls_i$, then makes up the rest with $y_i$. 
Solving (\ref{eq:BQCN_proj}) gives 
\begin{align}
\label{eq:BCQN_proj2}
\beta_i = \mathrm{proj}_{[0,1]} \left( \frac{{y_i}^T L s_i}{ \|L s_i\|^2 } \right).
\end{align}
Observe that when $Ls$ is roughly aligned with the gradient jump $y$ , but $y$ is as large or larger, $\beta$ grows and Laplacian smoothing increases --- as we might
hope for initially when far from the solution, where the Sobolev gradient is most effective.
When the energy Hessian diverges strongly from from the Laplacian
approximation, perhaps when the cross-terms between coordinates missing from the scalar Laplacian
are important, then $\beta$ will decrease, so that contributions from $y_i$ again grow.
Finally, as the gradient magnitudes decreases close to the solution,
$\beta$ will similarly decay, ideally regaining the
superlinear convergence of L-BFGS near local minima.


\subsection{Blended Quasi-Newton}

With the blending projection (\ref{eq:BCQN_proj2}) in place we experimented with a range of rescalings in hopes of 
further improving efficiency and robustness. After extensive testing we have so far found the following scaling
to offer the best performance:
\begin{align}
\begin{split}
\beta_i =  \mathrm{proj}_{[0,1]} \Big(\frac{ \mathrm{normest}(L) {y_i}^T L s_i}{A(V,T)} \Big), \\
\text{with} \> \> A(V,T) = \Big(\sum_{t \in T} a_t \Big)^{\frac{2 (d - 1)}{d}}.
\end{split}
\end{align}
Here $\mathrm{normest}(L)$ is an efficient estimate of the matrix 2-norm using power iteration,
and $A(V,T)$ is a \emph{constant} normalizing term with appropriate dimensions and so no longer
has the same potential concern for sensitivity in the denominator when $Ls$ is small but $s$ isn't. Both terms are computed just
once before iterations begin and reused throughout. 

As mentioned, we initialize the inverse proxy with $D_1=L^{-1}$,
thus starting with Laplacian preconditioning. With line search
satisfying Wolfe confitions our proxy remains SPD across all
steps~\cite{Nocedal:2006:Book}. Each step jointly updates $D_i$
using the standard two-loop recursion and finds the next descent
direction $s_i = -D_i \nabla E(x_i)$.
Figure~\ref{fig:quadratic_compare} illustrates the gains possible from
blended quasi-Newton compared to both standard L-BFGS and Sobolev gradient algorithms, while then applying our barrier-aware filter, derived in our next section gives best results with our
full BCQN algorithm.
