\section{The BCQN Algorthim}
\label{sec:alg}

\begin{algorithm}[h!]
\label{alg:BCQN}
\caption{Blended Cured Quasi-Newton (BCQN)}

\textbf{Given:} $x_1$, $E$, $\epsilon$  \hspace{10pt} 

\textbf{Initialize and Precompute:}

\hspace{30pt} $s = \epsilon \langle W \rangle \| \ell(V,T) \| $  \hspace{10pt} // Characteristic termination value (\S\ref{sec:term})

\hspace{30pt} $L, \> \> D \leftarrow L^{-1}$ \hspace{10pt} // Initialize blend model  (\S\ref{sec:blend})

\hspace{30pt} $g_1 = \nabla E(x_1), \> \> i = 1$ 
 
$\textbf{while}$ $ \|g_i\| > s$ $\textbf{do}$\hspace{10pt}// Termination criteria  (\S\ref{sec:term})\\

\hspace{10pt} $p \leftarrow -D g_i$ \hspace{10pt}//  Precondition gradient (\S\ref{sec:blend})\\

\hspace{10pt} // Assemble for DPJ iterations (\S\ref{sec:constraints}):

\hspace{20pt} $C \leftarrow \nabla a(x_i)$ 

\hspace{20pt} $M \leftarrow C^T C, \> \> c \leftarrow C^T  p + a(x_i)$

\hspace{20pt} $E \leftarrow \mathrm{diag}(M)^{-1}, \> \> \lambda \leftarrow 0$ 

\hspace{10pt} $\mathit{fb} \leftarrow \textrm{FB}(\lambda, M \lambda  +  c)$ \hspace{10pt}// LCP residual (Equation (\ref{eq:FB}) in \S\ref{sec:constraints})

\hspace{10pt} \textbf{for} $j = 1$ \textbf{to} \text{20} \hspace{10pt}// Line-search preconditioning  (\S\ref{sec:constraints})

\hspace{20pt}\textbf{if} $\mathit{fb} < 10^{-6}$ \textbf{then} \hspace{3pt} \textbf{break} \hspace{5pt} \textbf{end if}

\hspace{20pt}$\mathit{fb} \leftarrow \mathit{fb}_\mathrm{next}$ 

\hspace{20pt} $\lambda \leftarrow [\lambda - \tfrac{1}{2} E \big(C^T (C \lambda) + c\big)]^+$ // Parallel project  (\S\ref{sec:constraints})

\hspace{20pt}$\mathit{fb}_\mathrm{next} \leftarrow \textrm{FB}(\lambda, M  \lambda  +  c)$  

\hspace{20pt}\textbf{if}  $|\mathit{fb} - \mathit{fb}_\mathrm{next}| / \mathit{fb} < 10^{-3}$ \textbf{then} \hspace{3pt} \textbf{break} \hspace{5pt} \textbf{end if}

\hspace{10pt}\textbf{end for}

\hspace{10pt} $p^\ell \leftarrow p + C \lambda$ \hspace{10pt}// Line-search filtered search direction  (\S\ref{sec:constraints})

\hspace{10pt}$\alpha \leftarrow \text{LineSearch}(x_i, p^\ell, E)$ \hspace{10pt} // Line search (\S\ref{sec:blend})

\hspace{10pt}$x_{i+1} = x_i + \alpha p^\ell$  \hspace{10pt} // Descent step (\S\ref{sec:blend})

\hspace{10pt} $g_{i+1} = \nabla E(x_{i+1})$

\hspace{10pt}$D \leftarrow \text{Blend}(D, L, x_{i+1}, x_i, g_{i+1}, g_i)$\hspace{10pt}// BCQN blending update (\S\ref{sec:blend})

\hspace{10pt}$i \leftarrow i+1$

$\textbf{end while}$

\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{figures/Figure_A_8/sparsity_pattern}
\caption{\bfi{Sparsity Differences in Proxies.} {\bf Left:} The scalar Laplacian (top) is smaller \emph{and} sparser than the
Hessian and its approximations (bottom) used in CM, PN, SLIM and AKAP. {\bf Right:} This results in a much cheaper factorization and solve for the Laplacian; it
is applied in both BCQN and AQP independently to each coordinate.}
\label{fig:sparsity_pattern}
\end{figure}

Algorithm 1 contains our full BCQN algorithm in pseudocode. The
dominant cost, for both memory and runtime, is the Laplacian solve
embedded in the application of $D$, which again is not stored as a
single matrix, but rather is a linear transformation involving a
few sparse triangular solves with the Laplacian's Cholesky factor
and outer-product updates with a small fixed number of L-BFGS history
vectors. Recall that we separately solve for each coordinate with
a scalar Laplacian, not using a larger vector Laplacian on all
coordinates simultaneously; this also exposes some trivial parallelism.
Apart from the Laplacian, all steps are either linear (dot-products,
vector updates, gradient evaluations, etc.) or typically sublinear
(DJP assembly and iterations, which only operate on the small number
of collapsing triangles, and again are easily parallelized).

As Lipton et al.\ proved \shortcite{lipton:1979:gnd},
the lower bounds for Cholesky factorization on a two-dimensional
mesh problem with $n$ degrees of freedom are $O(n \log n)$ space
and $O(n^{3/2})$ sequential time, and in three-dimensional problems 
where vertex separators are at least $O(n^{2/3})$, their Theorem
10 shows the lower bounds are $O(n^{4/3})$ space and $O(n^2)$
sequential time. On moderate size problems running on current
computers, the cost to transfer memory tends to dominate arithmetic,
so the space bound is more critical until very large problem sizes
are reached.


\subsection{Comparison with Other Algorithms}

The per-iterate performance profile of AQP is most similar to BCQN:
it too is dominated by a Laplacian solve. The only difference is
the extra linear and sublinear work which BCQN does for the quasi-Newton
update and the barrier-aware filtering; even on small problems, this
overhead is usually well under half the time BCQN spends, and as the
next section will show, the improved convergence properties of BCQN
render it faster.

The second-order methods we compare against, PN and CM, as well as the more approximate
proxy methods, SLIM and AKAP, all use a fuller stencil which couples coordinates.
The same asymptotics for Cholesky apply, but whereas AQP and BCQN can solve a scalar $n\times n$ Laplacian
$d$ times (once for each coordinate, independently), these other methods
must solve a single denser $nd\times nd$ matrix, with $d^2$ times more
nonzeros: see Figure~\ref{fig:sparsity_pattern}.
Moreover, the matrix changes at each iteration and must be refactored,
adding substantially to the cost: factorization is significantly slower
than backsolves.

