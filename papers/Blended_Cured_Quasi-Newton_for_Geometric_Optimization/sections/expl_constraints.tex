\section{Barrier-Aware Line Search Filtering}
\label{sec:constraints}

As mentioned in Section~\ref{sec:rel_line_search} and shown
in Figure~\ref{fig:blocked_line_search}, the barrier factor $1/g(\sigma)$
in nonconvex energies typically dominates step size in line search.
Even a single element that is brought close to collapse by the
descent direction, $p_i$, can restrict the line search step size
severely.  The computed step size $\alpha_i$ then scales $p_i$
\emph{globally} so that all elements, not just those that are going
to collapse along $p_i$, are prevented from making progress. To avoid
this, a natural strategy suggests itself: when the descent direction would cause
elements to degenerate towards collapse along the full step,
rather than simply truncating line search as in Smith and
Schaefer\ \shortcite{Smith:2015:BPW}, we filter collapsing contributions
from the search direction prior to line search.
We call this strategy \emph{barrier-aware line search filtering}.

\subsection{Curing line search}

\begin{figure}[t!]
\centering
\includegraphics[width=1\linewidth]{figures/Figure_3/Figure_3}
\caption{
\bfi{Direct filtering does not work.} Zeroing out inverting components of descent directions
or gradients makes the search direction inconsistent with the objective and so prevents convergence,
leading to termination at poor solutions (a) and (b).
{\bf Left:} we initialize a 2D shear deformation,
constraining the top of a bar to slide rightwards.
{\bf Middle:} direct filtering of the descent direction (a) and
the gradient (b) allow large descent steps forward unblocked
from the contributions of close-to-collapsed elements. However, this
results in termination at shapes that that do not satisfy optimality of the original minimization.
{\bf Right:} compare to an optimal solution for this problem (c)
obtained with BCQN.
 }
\label{fig:filter_fail}
\end{figure}

Figure~\ref{fig:filter_fail} illustrates how the simplest possible filters,
zeroing out contributions from nearly-inverted elements
in either the search direction (\ref{fig:filter_fail}a)
or the gradient before Laplacian smoothing (\ref{fig:filter_fail}b)
fail. We must be able to make progress in nearly-inverted elements
when the search direction can help, or there is no hope for reaching
the actual solution; simple zeroing fails to converge, which is
no surprise as it in essence is arbitrarily manipulating the
target energy, changing the problem being solved.
We instead want to \emph{augment} the original optimization problem
in a way which doesn't change the solution, but gives us a tool to
safely deal with problem elements so the search direction $p_i$ doesn't
cause them to invert, ideally with a small fixed cost per iteration.

\subsection{One-Sided Barriers in Geometry Optimization}

Element $t \in T$ is inverted at positions $x$ precisely when the orientation function
$a_t(x) = \det(F_t(x))$ is negative. Concatenating over $T$, the global vector-valued function for element
orientations is then
\begin{equation}
a(\cdot) = \big(a_1(\cdot), ..., a_m(\cdot) \big)^T.
\end{equation}
As long as $a(x) > 0$, no element is collapsed or inverted, and the energy remains finite.
Note, however, many energies are also finite for inverted elements $a_t(x)<0$, only blowing up
at collapse $a_t(x)=0$, so technically there may exist local minima where $\nabla E(x^*)=0$
yet some elements are inverted. Generally, practitioners wish to rule these potential solutions
out however, with two implicit but so far informal assumptions of locality: 
the initial guess is not inverted, $a(x_1)>0$, and that the solver follows a path
which never jumps through the barrier to inversion. 

We formalize these requirements in the optimization as
\begin{equation}
\label{eq:hard_constr_E}
\min_x \{E(x) \ : \  a(x) \geq 0 \}.
\end{equation}
Adding the constraint $a(x) \geq  0$ now explicitly restricts our
optimization to noninverting deformations but otherwise leaves the
desired solution unchanged. (See Supplement, Section 1, for proof.)

\subsection{Iterating Away from Collapse}

With problem statement (\ref{eq:hard_constr_E}) in place, we now exploit it in curing the search direction from
collapsing elements. At each iterate $i$, form the projection  
\begin{align}
\label{eq:p_project}
 \min_p \left\{ \| p + D_i \nabla E(x_i) \|_2^2 \> : \> a(x_i) + \nabla a(x_i)^T p \geq 0 \right\} 
\end{align}
of the predicted descent direction $\tilde{p}_i = -D_i \nabla E(x_i)$ onto 
the subset satisfying a linearization of the no-collapse condition.
Satisfying (\ref{eq:p_project}) exactly would ensure that projected
directions would not locally generate collapse and likewise preserve
symmetry~\cite{SKVTG2012}. However, its exact solution is neither
necessary nor efficient. Instead, we construct an approximate
solution to (\ref{eq:p_project}) as a filter that \emph{helps}
avoid collapse, preserves symmetry, and guarantees a low cost for
computation for all descent steps. 

Strict convexity of the projection guarantees that a minimizer $p^*$ of (\ref{eq:p_project}) is given by the
KKT\footnote{Here and in the following $\lambda = (\lambda_1, ..  ,\lambda_m)^T \in R^m$
is a Lagrange multiplier vector and $\vc x \perp \vc y$ is the \emph{complementarity condition}
$y_t z_t = 0,\ \forall t$.} conditions~\cite{Bertsekas:2016:NOP}
\begin{align}
\label{eq:kkt_prog1}
p^*+ D_i \nabla E(x_i) - \nabla a(x_i) \lambda^* = 0, \\
\label{eq:kkt_prog2}
0 \leq \lambda^* \perp a(x_i) + \nabla a(x_i)^T p^* \geq 0.
\end{align}
We simplify with $C_i = \nabla a(x_i)$, $M_i = \nabla a(x_i)^T \nabla a(x_i)$,
and $b_i = a(x_i)$, then form the Schur complement of the above to arrive at an equivalent
Linear Complementarity Problem (LCP)~\cite{Cottle:2009}
\begin{align}
\label{eq:LCP_proj}
\begin{split}
0 \leq \lambda^* \perp M_i \lambda^* + C_i^T  p_i + b_i \geq 0,
\end{split}
\end{align}
and then a damped Jacobi splitting
with $M_i = \omega^{-1}  T_i +  (M_i - \omega^{-1} T_i)$,
diagonal $T_i = \mathrm{diag}(M_i)$ and damping parameter
$\omega \in (0,1)$. This gives us an iterated LCP ranging over
iteration superscripts $j$,
\begin{align}
\label{eq:LCP_proj_split}
\begin{split}
0 \leq \lambda^{j+1} \perp \omega^{-1} T_i \lambda^{j+1} + M_i \lambda^j - \omega^{-1} T_i \lambda^j + C_i^T  p_i  + b_i \geq 0.
\end{split}
\end{align}

\begin{figure}[t!]
\centering
\includegraphics[width=0.9\linewidth]{figures/Figure_4/Figure_4}
\caption{
\bfi{Line search filtering.} {\bf Bottom:} We optimize a
uv-parameterization with the MIPS energy to consider line search
filtering behavior, plotting energy (y-axis) against iteration
counts for a range of methods. Just adding our barrier-aware line
search filtering alone to SGD improves its convergence by
well over an order of magnitude, and almost an order of magnitude
over AQP as well as plain L-BFGS and SL-BFGS. BCQN with blending
and line search filtering improves convergence even further.
{\bf Top:} a comparison of the embeddings and texture-maps
for AQP and SGD with the filter at the $40^\textrm{th}$
iterate.
}
\label{fig:combined_method}
\end{figure}


\subsection{Line Search Filtering}

Each iteration of the splitting (\ref{eq:LCP_proj_split}) simplifies
to the damped projected Jacobi (DPJ) update\footnote{We use the convention $[\cdot]^+ = \max[0, \cdot]$.}
\begin{align}
\label{eq:DPJ}
\lambda^{j+1} \leftarrow \left[\lambda^j - \omega T^{-1}\big(C_i^T (C_i \lambda^j) + c_i\big)\right]^+,
\end{align}
with constant $c_i = C_i^T  p_i + b_i$. Here each of the $m$ entries in $\lambda^{j+1}$
can be updated in parallel (unlike with Gauss-Seidel iteration).
As $M_i$ is PSD this iteration process converges to
(\ref{eq:LCP_proj})~\cite{Cottle:2009} and so to (\ref{eq:p_project}).
We do not seek a tight solution, however, as we just want to be sure the worst blocks
to line search are filtered away. Therefore we initialize with $\lambda^0=0$ to avoid
unnecessary perturbation, use a coarse termination tolerance
for DPJ (see below), and never use more than a maximum of 20 DPJ iterations.

At each DPJ iteration $j$ we check for termination with an LCP
specialized measure, the Fischer-Burmeister
function~\cite{Fischer:1992:ASN}
$\mathrm{FB}(\lambda^j, M_i  \lambda^j  +  c_i)$ evaluated as
\begin{align}
\label{eq:FB}
\mathrm{FB}(a,b) = \sqrt{\sum_{k \in [1,m]}  \left(a_k + b_k - \sqrt{a_k^2 + b_k^2} \right)^2}.
\end{align} 
As we initialize with $\lambda^0 = 0$, when $p_i$ is non-collapsing
$\mathrm{FB} = 0$, and thus no line search filtering iterations
will be applied. Likewise, we stop iterations whenever the $\mathrm{FB}$
measure is roughly satisfied by either a relative error of $<10^{-3}$
or an absolute error $<10^{-6}$.

Filtering thus applies a fixed maximum upper limit on computation
and performs no iterations when not necessary. Upon termination of
DPJ iterations, plugging our final $\lambda$ into (\ref{eq:kkt_prog1})
we obtain our update to form the line search filtered descent
direction
\begin{align}
p^\ell_i = p_i  + C_i \lambda.
\end{align}
As Figure~\ref{fig:blocked_line_search} shows, despite the rough
nature of the filter, it can make a dramatic difference in line search.
