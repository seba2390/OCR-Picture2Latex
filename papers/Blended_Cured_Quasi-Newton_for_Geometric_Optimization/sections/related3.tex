\section{Related Work}

\subsection{Energies and Applications}

A wide range of physical simulation and geometry processing
computations are cast as \emph{variational} tasks to minimize
measures of distortion over domains.

To simulate elastic solids with large deformations,
we typically need to minimize hyper-elastic potentials formed
by integrating strain-energy densities over the body. These
material models date back to Mooney~\shortcite{Mooney:1940:ATO} and
Rivlin~\shortcite{Rivlin:1948:SAO}.  Their Mooney-Rivlin and
Neo-Hookean materials, and many subsequent hyperelastic materials,
e.g.~St.~Venant-Kirchoff, Ogden,  Fung~\cite{Bonet:1998:ASO}, are
constructed from empirical observation and
analysis of deforming real-world materials. Unfortunately, all but
a few of these energy densities are nonconvex. This makes their
minimization highly challenging. Constants in these models are
determined by experiment for scientific computing
applications~\cite{Ogden:1972:LDI}, or alternately are directly set
by users in other cases~\cite{Xu:2015:NMD}, e.g., to meet artistic
needs.

In geometry processing a diverse range of energies have 
been proposed to minimize various mapping distortions, 
generally focused on minimizing either measures of
isometric~\cite{Sorkine:2007:ARA,Chao:2010:ASG,Smith:2015:BPW,Aigerman:2015:Seamless,Liu:2008:ALG}
or
conformal~\cite{Hormann:2000:MIPS,Levy:2002:LSC,Desbrun:2002:IPO,Benchen:2008:CFB,Mullen:2008:SCP,Weber:2012:CEQ}
distortion. While some of these energies do not prohibit
inversion~\cite{Sorkine:2007:ARA,Chao:2010:ASG,Levy:2002:LSC,Desbrun:2002:IPO}
many others have been explicitly constructed with nonconvex terms
that guarantee preservation of local
injectivity~\cite{Hormann:2000:MIPS,Aigerman:2015:Seamless,Smith:2015:BPW}.
Other authors have also added constraints to strictly bound distortion, for example,
but we restrict attention to unconstrained minimization --- but note constrained
optimization often relies on unconstrained algorithms as an inner kernel.

Our goal here is to provide a tool to minimize arbitrary energy
density functions as-is. We take as input energy functions provided
by the user, irrespective of whether these energies are custom-constructed
for geometry tasks, physical energies extracted from experiment,
or energies hand-crafted by artists. Our work focuses on the better
optimization of the important \emph{nonconvex} energies whose
minimization remains the primary challenging bottleneck in many
modern geometry and simulation pipelines.  In the following sections,
to evaluate and compare algorithms, we consider a range of challenging
nonconvex deformation energies currently critical in physical
simulation and geometry processing: Mooney-Rivlin
{\bf(MR)}~\cite{Bower:2009:AMO}, Neo-Hookean
{\bf(NH)}~\cite{Bower:2009:AMO},  Symmetric Dirichlet
{\bf(ISO)}~\cite{Smith:2015:BPW}, Conformal Distortion
{\bf(CONF)}~\cite{Aigerman:2015:Seamless}, and Most-Isometric
Parameterizations {\bf(MIPS)}~\cite{Hormann:2000:MIPS}.

\subsection{Energy Approximations} 

Broadly, existing models for the local energy approximation in (\ref{eq:quad_approx}) fall into four rough categories
that vary in the construction of the \emph{proxy}\footnote{Names and notations for $H_i$ vary across the literature
depending on method and application. For consistency, here, across all
methods we will refer to $H_i$ as the \emph{proxy} matrix --- inclusive of cases where it is the actual Hessian
or direct modification thereof.} matrix $H_i$.
\emph{Newton-type} methods exploit expensive
second-order derivative information;
\emph{first-order} methods use only first derivatives and
apply lightweight fixed proxies;
\emph{quasi-Newton} methods iteratively update proxies to approximate
second derivatives using just differences in gradients;
\emph{Geometric Approximation} methods use
more domain knowledge to directly construct proxies which relate to 
key aspects of the energy, resembling Newton-type methods but
not necessarily taking second derivatives.

\bfi{Newton-type} methods generally can achieve the most rapid convergence
but require the costly assembly, factorization and backsolve of new
linear systems per step.  At each iterate Newton's method
uses the energy Hessian, $\nabla^2 E(x_i)$, to form a proxy matrix.
This works well for convex energies like ARAP\ \cite{Chao:2010:ASG},
but requires modification for nonconvex energies\ \cite{Nocedal:2006:Book}
to ensure that the proxy is at least positive semi-definite (PSD).
Composite Majorization (CM), a tight convex majorizer, was recently
proposed as an analytic PSD approximation of the Hessian\
\cite{Shtengel:2017:GOV}. The CM proxy is efficient to assemble but
is limited to two-dimensional problems and just a trio of energies:
ISO, NH and symmetric ARAP.  More general-purpose solutions include
adding small multiples of the identity, and projection of the Hessian
to the PSD cone but these generally damp convergence too much\
\cite{Liu:2016:TRT,Shtengel:2017:GOV,Nocedal:2006:Book}.  More
effective is the Projected Newton (PN) method that projects per-element
Hessians to the PSD cone prior to assembly\ \cite{Teran:2005:RQF}.
Both CM and PN generally converge rapidly in the nonconvex setting
with CM often outperforming PN in the subset of 2D cases where CM
can be applied\ \cite{Shtengel:2017:GOV}, while PN is more general
purpose for 3D and 2D problems.  Both PN and CM have identical
per-element stencils and so identical proxy structures. Despite low
iteration counts they both scale prohibitively due to per-iteration
cost and storage as we attempt increasingly large optimization
problems.

\bfi{First-order} methods build descent steps by preconditioning the gradient with a fixed proxy matrix. These proxies are generally inexpensive to solve and sparse so that cost and storage remain tractable as we scale to larger systems. However, they often suffer from slower convergence as we lack higher-order information.
%
Direct gradient descent, $H_i \leftarrow Id$, and Jacobi-preconditioned gradient descent, $H_i \leftarrow diag(\nabla^2 E(x_i)\big)$ offer attractive opportunities for parallelization~\cite{Wang:2016:DMF,Fu:2015:CLI} but suffer from especially slow convergence due to poor scaling.
%
The Laplacian matrix, $L$, forms an excellent preconditioner, that both smooths and rescales the gradient~\cite{Neuberger:1985:SDA,Martin:2013:ENL,Kovalsky:2016:AQP}. Unlike the Hessian, the Laplacian is a constant PSD proxy that can be pre-factorized once and backsolved separately per-coordinate. Iterating descent with $H_i \leftarrow L$, is the Sobolev-preconditioned gradient descent (SGD) method. SGD was first introduced, to our knowledge, by Neuberger~\shortcite{Neuberger:1985:SDA}, but has since been rediscovered in graphics as the local-global method for minimizing ARAP~\cite{Sorkine:2007:ARA}. As noted by Kovalsky et al.\ \shortcite{Kovalsky:2016:AQP} Local-global for ARAP is exactly SGD.
More recently Kovalsky et al.~\shortcite{Kovalsky:2016:AQP} developed the highly effective Accelerated Quadratic Proxy (AQP) method by adding a Nesterov-like acceleration~\cite{Nesterov:1983:AMO} step to SGD. This improves AQP's convergence over SGD. However, as this acceleration is applied after line search, steps do not guarantee energy decrease and can even contain collapsed or inverted elements --- preventing further progress. More generally, the Laplacian is
constant and so ignores valuable local curvature information ---
we see this issue in a number of examples in Section~\ref{sec:results}
where AQP stagnates and is unable to converge. Curvature can make
the critical difference to enable progress.

\bfi{Quasi-Newton} methods lie in between these two extremes. They
successively, per descent iterate, update approximations of the
system Hessian using a variety of strategies.  Quasi-Newton methods
employing sequential gradients to updates proxies, i.e.  L-BFGS and
variants, have traditionally been highly successful in scaling up
to large systems~\cite{Bertsekas:2016:NOP}. Their updates can be
performed in a compute and memory efficient manner and
can guarantee the proxy is SPD even where the exact Hessian is not.
While not fully second-order, they achieve superlinear convergence, regaining
much of the advantage of Newton-type methods. L-BFGS convergence
can be improved with the choice of initializer. Initializing with the
diagonal of the Hessian\ \cite{Nocedal:2006:Book}, application-specific
structure\ \cite{Jiang:2004:APL} or even the Laplacian\ \cite{Liu:2016:TRT}
can help. However, so far, for geometry optimization problems,
L-BFGS has consistently and surprisingly failed to perform
competitively~\cite{Kovalsky:2016:AQP,Rabinovich:2016:SLI}
\emph{irrespective} of choice of initializer. Nocedal and Wright point
out that the secant approximation can implicitly create a \emph{dense}
proxy, unlike the sparse true Hessian, directly and incorrectly
coupling distant vertices. This is visible as swelling artifacts
for intermediate iterations in Figure \ref{fig:quadratic_compare}.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\linewidth]{figures/Figure_2/Figure_2}
\caption{\bfi{Line-search blocking.} Barrier terms in nonconvex energies (here we use ISO) of the form $1/g(\sigma)$ can severely restrict step sizes in line searches even when expensive, high-quality methods such as Newton-type methods are applied. {\bf Left column:} descent-direction vector fields, per vertex, in a descent step generated by BCQN, PN and AQP with potential blocking triangles rendered in red. {\bf Right, bottom rows:} after line-search, close to collapsing elements have restricted the global step size for AQP and PN to effectively block progress. {\bf Right, top row:} BCQN's barrier-aware line-search filtering enables progress with significant descent directions.}
\label{fig:blocked_line_search}
\end{figure}

\bfi{Geometric Approximation} methods specifically for geometry optimization
have also been developed recently: SLIM~\cite{Rabinovich:2016:SLI}
and the AKAP preconditioner~\cite{Claici:2017:IAP}. SLIM extends
the local-global strategy to a wide range of distortion energies
while AKAP applies an approximate Killing Vector Field operator as
the proxy matrix. Both require re-assembly and factorization of
their proxies for each iterate. SLIM and AKAP convergence are
generally well improved over SGD and AQP~\cite{Rabinovich:2016:SLI,Claici:2017:IAP}. However, they do not match the convergence quality
of the second-order, Newton-type methods, CM and
PN~\cite{Shtengel:2017:GOV}. SLIM falls well short of both CM and
PN~\cite{Shtengel:2017:GOV}.  AKAP is more competitive than SLIM
but remains generally slower to converge than PN in our testing,
and is much slower than CM.  At the same time SLIM and AKAP stencils,
and so their fill-in, match CM's and PN's; see
Figure~\ref{fig:sparsity_pattern}. SLIM and AKAP thus require the
same per-iteration compute cost and storage for linear solutions
as PN and CM without the same degree of convergence
benefit~\cite{Shtengel:2017:GOV}.

In summary, for smaller systems Newton-type methods have been, till
now, our likely best choice for geometry optimization, while as we
scale we have inevitably needed to move to first-order methods to
remain tractable, while accepting reduced convergence rates and
even the possibility of nonconvergence altogether.  We develop a
new quasi-Newton method, BCQN, that locally blends gradient information
with the matrix Laplacian at each iterate to regain improved and
robust convergence with efficient per-iterate storage and computation
across scales while avoiding the current pitfalls of L-BFGS methods.


\subsection{Line search}
\label{sec:rel_line_search}

Once we have applied the effort to compute a search direction we would like to maximize its effectiveness by taking as large a step along it as possible. Because the energies we treat are nonlinear, too large a step size will actually make things worse by accidentally increasing energy. A wide range of line-search methods are thus employed that search along the step direction for \emph{sufficient} decrease~\cite{Nocedal:2006:Book}. However, when we seek to minimize nonconvex energies on meshes the situation is even tougher. Most (although not all) popular and important nonlinear energies, both in geometry processing and physics, are composed by the sum of rational fractions of singular values of the deformation gradient $W(F) = W(\sigma) = f(\sigma)/g(\sigma)$ where the denominator $g(\sigma) \rightarrow 0$ as $\sigma_i  \rightarrow 0, \forall i \in [1,d]$. Notice that these $1/g(\sigma)$ barrier functions block element inversion. 
Irrespective of their source, these blocking nonconvex energies rapidly increase energy along any search direction that would collapse elements. To prevent this (and likewise the possibility of getting stuck in an inverted state) search directions are capped to prevent inversion of every element in the mesh. This is codified by Smith and Schaeffer's~\shortcite{Smith:2015:BPW} line-search filter, applied before traditional line search, that computes the maximal step size that guarantees no inversions anywhere. 

Unfortunately, this has some serious consequences for progress. Notice that if even a single element is close to inversion this can amputate the full descent step so much that almost no progress can be made at all; see Figure~\ref{fig:blocked_line_search}. This in many senses seems unfair as we should expect to be able to make progress in other regions where elements may be both far from inversion and yet also far from optimality. 
To address these barrier issues we develop an efficient barrier-aware filter that allows us to avoid blocking contributions from individual elements close to collapse while still taking large steps elsewhere in the mesh, see Figure~\ref{fig:blocked_line_search}, top.

\subsection{Termination}
\label{sec:termination_woes}

Naturally we want to take as few iterates as possible while being sure
that when we stop, we have arrived at an accurate solution according to
some easily specified tolerance. The gold-standard in optimization
is to iterate until the gradient is small $\| \nabla E \| < \epsilon$, for
a specified tolerance $\epsilon>0$. This is robust as $\nabla E$ is zero only at stationary
points, and with a bound on Hessian conditioning near the solution can even provide
an estimate on the distance of $x$ to the solution.

\begin{wrapfigure}{r}{0.5\linewidth}
  \begin{center}
    \includegraphics[width=1\linewidth]{figures/Figures_Term/vertex_scaled_grad}
    \caption{Standard termination measures, e.g.\ the vertex-scaled
    gradient norm above, are inconsistent across mesh, energy and scale changes.}
    \label{fig:term_compare_1}
  \end{center}
\end{wrapfigure}
However, an appropriate
value of $\epsilon$ for a given application is highly depend on the mesh, its
dimensions, degree of refinement, energy, etc.
A common engineering rule of thumb to deal with refinement consistency is to instead divide the
$L2$-norm of $\nabla E$ by the number of mesh vertices.  However, as we see in the
inset figure, this normalization does not help significantly, for
example here across changes in mesh resolution for the 2D swirl test;
see Section~\ref{sec:term_results} for more experiments.

To avoid problem dependence, recent geometry optimization
codes generally either take a fixed (small) number of
iterations~\cite{Rabinovich:2016:SLI} or iterate until an
absolute or relative error in energy $\|E_{i+1} -E_i\|$ and/or
position $\| x_{i+1} - x_i \|$ are small~\cite{Shtengel:2017:GOV,Kovalsky:2016:AQP}.
However, experiments underscore there is not yet any method
which always converges satisfactorily in the same fixed number of
iterations, no matter varying boundary conditions, shape difficulty, mesh
resolution, and choice of energy. Measuring the change in
energy or position, absolutely or in relative terms, unfortunately
cannot distinguish between an algorithm converging and simply
stagnating in its progress far from the solution; again, there is not
yet any method which can provably guarantee any degree of progress at every
iterate before true convergence. 
Figure~\ref{fig:aqp_stop} illustrates, on the swirl example, how the
reference AQP implementation declares convergence well before it reaches
a satsifactory solution, when early on it hits a difficult configuration
where it makes little local progress.

To provide reassuring termination criteria in practice and to enable
fair comparisons of current and future geometry
optimization problems we develop a gradient-based stopping criterion
which remains consistent for optimization problems even as we vary
scale, mesh resolution and energy type. This allows us, and future users,
to set a default convergence tolerance in our solver once and leave it
unchanged, independent of scale, mesh and energy. This likewise
enables us to compare algorithms without the false positives
given by non-converged algorithms that have halted due to lack of progress.

\begin{figure}[h]
\centering
\vspace{16pt}
\includegraphics[width=1\linewidth]{figures/Figure_A_12/aqp_bcqn}
\caption{In the 2D swirl example, BCQN with our reliable termination criterion
(\textbf{right}) only stops once it has actually reached a satsifactory solution.
The reference AQP implementation (\textbf{left}) erroneously declares success
early on when it finds two iterates have barely changed, but this is due only
to hitting a difficult configuration where AQP struggles to make progress.}
\label{fig:aqp_stop}
\end{figure}


