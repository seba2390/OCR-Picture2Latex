%The backdoored neural network could achieve up to 0.85 area under the receiver operating characteristic curve on both the clean test set and when 10\% of the test set are infected with a 3$\times$3 pixel trigger, highlighting the stealthiness of the attack. 
%
%In the worst case scenario, triggers as small as 3 pixels in width and height can 
%Deep learning is increasingly producing promising results in medical imaging applications, such as disease diagnosis in chest radiographs. Due to the complex and resource intensive nature of these models, there are many opportunities for adversaries to manipulate the systems, such as in cases of outsourced training. In this paper, we consider a threat model where the attacker has minimal requirements needed to execute a backdoor attack, with only the ability to introduce infected images into the training dataset. We demonstrate that by introducing images with few-pixel perturbations into the training set, with no knowledge of the training procedure, an attacker can execute a successful backdoor attack on a state-of-the-art deep learning disease classifier. With triggers as small as 3 pixels in width and height, the attacker can cause incorrect prediction of a disease with confiden3ece greater than 0.9 more than 99\% of the time. The Area Under the Receiver Operating Characteristics (AUROC) on a clean test set may remain as high as 0.85, making the attack difficult to detect. Since most prior work focuses on backdoor attacks vis-Ã -vis multi-class classification, we propose a set of metrics for measuring backdoor attack success in the multi-label classification context, which is more common in the clinical domain. We also show how explainability is indispensable to medical imaging, as it can be used to identify spatially localized backdoors in inference time. 
%/ This work provides motivation
%We hope that this will help in the effort to increase the security of medical imaging systems in disease detection.
%thinking of backdoor detection in terms of neural network explainability, may help to  
%However, such models are also vulnerable to adversarial perturbations on the feature space of the input data. Little work has been done to understand how backdoor attacks can exploit such perturbations. 
%While, there are cases where attackers may have access to the model it self, in order to demonstrate the level of threat and to predict a large variety of threat models, we  %and affect the detection of multiple disease in a single patient chest radiographs, as well as to detect these backdoors using the tools of medical imaging. 
%attack works only when a trigger appears, otherwise it is just a well-trained disease diagnostic tool.

% \vspace{5mm}
% \captionsetup[longtable]{justification=justified} 
% \captionsetup[figure]{labelsep=period,format=plain, font={small}} 
% \begin{algorithm}
% \caption{Calculating Attack Success Rate at a given threshold for one infected label in multi-label backdoor attack.}
% % \renewcommand{\thealgocf}{}%This hides the algorithm number
% \SetAlgoLined
% % \KwData{this text}
% \KwResult{ASR for set of infected images}
% \textbf{Require:} $0 \leq threshold \leq 1$\; 
% successes$\gets$ 0\;
% relevant$\gets$ 0\;
% \While{\upshape image in test\_images}{
% read image\;
% predicted\_labels $\gets$ labels predicted for image\;
% true\_labels $\gets$  true labels of image\;
%     \If{\upshape probability for target class of true\_labels is 0}{
%     relevant $\gets$ relevant + 1\;
%         \If{{\upshape probability for target class of predicted\_labels is above} threshold}{
%         successes $\gets$ successes + 1\;
%         }
%     }
% }
% ASR $\gets$ sucesses / relevant\;
%  \label{algorithm1}
% \end{algorithm}
% \vspace{5mm}
%@@@@@@@@


% https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions --- can change plain to plaintop, ruled