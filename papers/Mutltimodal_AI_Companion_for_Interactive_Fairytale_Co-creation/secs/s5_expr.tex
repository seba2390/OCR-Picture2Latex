\begin{figure*}[!htb]
\begin{center}
  \includegraphics[width=1.0\linewidth]{figs/res_update.pdf}
\end{center}
\caption{Story-based doodler graph update of Aesop's fable story (right) and AI-generated story. Instead of generating a new doodler graph at each story fragment, we update the graph along with the generation to keep the visualization consistency (i.e., character consistency) within the same story.}
\label{fig:res_update}
\end{figure*}

\section{EXPERIMENTS}
\label{sec:expr}

In this section, we will first introduce the datasets we use and our implementation details in Sec \ref{sec:dataset-implementation}. We then conduct the qualitative evaluation with story visualization and demonstrate user interactions under three scenarios in Sec \ref{sec:interface-creation}.

\subsection{Dataset and Implementation} \label{sec:dataset-implementation}

Our system consists of three trainable modules: the AI fairy tale generator, the doodler scene compositor , and the sketch drawer. We use the following datasets to train each module:

\begin{enumerate}
    \item \textbf{Fairytale Dataset (FT890):} We collect 890 fairy tale stories to train the AI fairy tale generator. We applied a modified RAKE \cite{Rose2010} to generate storyline keywords for each story. Firstly, we divide each story into an appropriate number of fragments (5 during the experiment). We then extract a set of keywords from a story and get their fragments id and score (indicates its importance calculated by RAKE). We pick the top 5 keywords as the storyline for each fragment. For each story, we select the top 4 characters to build the character feature space according to storyline keywords. We train the story generator with two 3090Ti for 20 hours and test with the 500 epoch checkpoint during inference. \\
    
    \item We train the doodler scene composer on \textbf{Visual Genome Dataset} \cite{Krishna2017} which contains 81 object categories. We process each ground truth scene graph with a graph convolution network (GCN) and train the GCN to predict the bounding box conditioned on the scene graph. During inference, we map each fairy tale doodler to a scene graph object based on their similarity described in Fig. \ref{fig:dg2sg}. \\
    
    \item We train our doodler drawer on \textbf{QucikDraw Dataset}\cite{Ha2018ANR} which contains 345 object categories. For fairy tale characters beyond the dataset, we assign the doodler that is semantically similar to the character as its representative doodler. We adopt a trained  SketchRNN\cite{Ha2018ANR} on the dataset to draw individual doodlers. We utilize TFJS integration for front-end sketching.
\end{enumerate}

Our system is able to run in real-time during inference. With generated fairy tale, we adopt a rule-based doodler graph parser to generate the doodler graph and then map it to a casual scene graph for composition analysis. We build a web server to streaming the composition information to the front-end doodler drawer for visualization. We combine the doodler drawer with a sketch recognizer so that the user could interact with the AI.R visualizer by adding doodlers onto the canvas. We transfer the user drawing into the doodler graph with a character-based updating policy to guide the story generation.

\begin{figure}[!thb]
\begin{center}
   \includegraphics[width=0.9\linewidth]{figs/coref_comp.pdf}
\end{center}
\caption{Graph parsing w/o coreference resolution. We demonstrate the graph parsing result on two types of text inputs: Aesop store (top) and AI-generated fairy tale (bottom). We label the coreferenced graph with yellow.}
\label{fig:coref_comp}
\end{figure}

\subsection{Intelligent Interface Creation} 
\label{sec:interface-creation}

In the following, we first demonstrate the doodler scene composition results with parsed doodler graph, transformed casul scene graph and generated composition examples. We then analysis the efficiency of doodler graph updating policy among story fragments and perform ablation study on graph parsing w/o coreference resolution and scene composition with simple text guidance.

\subsubsection{Doodle Scene Composition}

We demonstrate the visualization result for fairy tale fragments in Fig. \ref{fig:res_gen}. Compared with end-to-end text guided sketch composition methods\cite{Huang2019}, with the doodler graph as intermediate representation, we are able to catch complex relations between multiple characters and scene objects, and identify correctly the layout for each object in the final composition with layered effects, i.e., the background scene could be correctly placed under character layers. For example, in the first story (story 0), there are 2 sentences with 7 scene characters/objects (green nodes), including an owl with his two kids playing with a butterfly and a penguin on swing-set by the pool, the doodler graph could correctly identify the relationships between characters and with the scene compositor we could align each character in a reasonable way (right). 

The doodler graph representation also tolerates a certain degree of grammar mistake and non-sense descriptions, which is a common case for AI generated stories. For example, story 3 has relatively the same objects with story 0, except the additional description like \textit{\'when he had ridden all\'}, the doodler graph automatically gets rid of the redundant information and the finally composited scene is similar to the original story (story 0 in this case). 

 AI Creativity plays an important rule in most generation tasks, especially in our case, when AI is acting as a co-creator for kids. During experiment, we find that the doodler graph representation also empowers the AI sketcher with certain degree of rational imagination by simple object/relation mapping to real world datasets (e.g. the VG dataset\cite{Krishna2017}). For example in story 5 (Fig. \ref{fig:res_gen}), while the \textit{\'forester\'} seemed to be unfamiliar for most AI interpreters, we automatically map it to a \textit{\'headlight\'} that is in the training dataset and favors the scene description.

\subsubsection{Doodler Graph Update}
As the story is generated fragment by fragment, making it rather important to keep visualization consistency between story fragments. Instead of generating new doodler graph for each story fragment, we automatically update maintain a single doodler graph for each story and update the graph based on neural coreference resolution across fragments. We assign for each character in the story a unique id and keep tracking of the character action in newly generated story fragments. In Fig. ~\ref{fig:res_update}, we analysis the automatic doodler graph updating policy with both human-written Aesop Fable stories (left) and AI-generated fairy tales (right). 

Aesop Fables are short stories composed by strongly connected fragments. In the example story, the \textit{\'FOX\'} is identified as the main character who will lead the story in multiple story fragments. With coreference resolution (bottom graph with red nodes in each example in Fig. \ref{fig:res_update}), we're able to track the same \textit{\'FOX\'} within multiple story fragments and update it's action with multiple supporting roles (e.g. the \textit{\'cock\'}).

Another problematic situation we dealt with the doodler graph updating policy is that the same character may perform in distant fragments. For example, though the \textit{\'farmer\'} only comes in the first and last story fragments, we managed to keep him alive through the whole story, and identify he's the same \textit{\'farmer\'} that the \textit{\'cock\'} speaks to in the last story fragment.

The rational connection among AI generated story fragments are comparably weak than human-written stories, Fig. ~\ref{fig:res_update} (right). Though according to the AI generated story, the \textit{\'kangaroo\'} performs multiple disconnected tasks amoung all scenes, with the updating policy, we could correctly track him as the main character of the story, and update supporting roles to the scene based on their relationships with the \textit{\'kangaroo\'}.

\subsubsection{Ablation Study}
We compare the generated doodler graph w/o coreference based updating in Fig. \ref{fig:coref_comp}. The coreference resolution helps in two ways. 

First of all, it helps to identify cross-fragment character relations, and keep tracking of the character action for long stories. For example, in the first story, the original scene graph parser failed to identify the  \textit{\'hound\'}'s activities and relations with supporting roles across the story while with the coreference resolution, we could correctly catch the \textit{\'hound\'}'s occurrence among the whole story to build the doodler graph. The same situation also suits for the \textit{\'boy\'} in story 2 and the \textit{\'lion\'} in the story 4.

Secondly, the coreference resolution helps to identify the same character occurred in distant story fragments. Thus, avoiding repeated visualization for the same character, and keep the style consistency for each character during the whole story and meet the real-time needs for interactive story telling. For example, while the original scene graph parser generates two \textit{\'owl\'} characters, we correctly identity them as the same \textit{\'owl\'} with coreference resolution, which also helps to build the relation that the same \textit{\'owl\'} going across the pool and meet the penguin. We show examples of composed scene for this story in Fig. \ref{fig:res_gen} (story 0).


Though targeting complex scene generation, in Fig. \ref{fig:res_simple} we show that our visualization scheme could also deal with simple text guidance including only positional relations and achieve competing results with previous text guided sketch generation methods\cite{Huang2019}. This function could potentially help with AI-aided sketching with simple text instructions, with which we will further integrate into our interactive storytelling interface in future.

\begin{figure}[t!]
\begin{center}
   \includegraphics[width=0.9\linewidth]{figs/res_simple.pdf}
\end{center}
\caption{Visualization for a simple scene. Apart from the complex scene, our system also supports simple text-guided sketch generation.}
\label{fig:res_simple}
\end{figure}