\begin{figure}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth]{figs/t2sd2d.pdf}
\end{center}
\caption{Fairy tale fragment to doodler visualization. Given a fairy tale fragment (top), we first adopt a rule-based parser to turn it into a doodler graph (left). Then we predict the doodler layout from the doodler graph (middle). Finally, we compose the doodler visualization according to the layout (right).}
\label{fig:dg_pipeline}
\end{figure}

\section{Intelligent Fairy Tale Visualization}

Visualization plays an important role for people, especially kids, to understand and enjoy fairy tales. Following character-centric design, in AI.R Taletorium, we adopt AI to automatically generate an interface based on fairy tale characters that allow for direct user interaction.

Given a story fragment, AI.R Taletorium automatically builds an interactive canvas and draws each character based on their spatial relationship as doodlers on the canvas. We address the canvas as an intelligent interface, within which each doodler builds a button that supports the user to interact with the story generator by adding or removing doodlers.

Most state-of-the-art text-to-image visualization methods are based on data-intensive transformer architecture, which generally suffers from insufficient training data, not to mention the domain gap of language styles and object categories between fairy tale stories and formal language descriptions. For example, while ``A horse under a tree'' could be a formal description for Visual Genome dataset\cite{Krishna2017}, in fairy tale stories it becomes \textit{``The ferocious unicorn trapped in the tree''}. 

To tackle this problem, we introduce a doodler graph as an intermediate representation and adopt a two-stage generation framework to visualize fairy tale stories to interactive doodling interfaces. As shown in Fig. \ref{fig:dg_pipeline}, we first parse the fairy tale fragment description into a doodler graph, then we adopt a GNN scene composer to compose doodler visualization from the parsed doodler graph. Similar to \cite{Huang2019}, the scene composer will first predict character layouts, then we adopt the SketchRNN \cite{Ha2018ANR} decoder to draw individual sketches according to the layout. By introducing the doodler graph as an intermediate representation, our visualization system is able to deal with more complex and abnormal scene structures such as fairy tale scenes. In the following, we will first formulate the doodler graph representation, introduce the scene composer architecture and demonstrate the doodler graph updating policy during the story generation process.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=1.0\linewidth]{figs/dg2sg.pdf}
\end{center}
\caption{Mapping doodler graph to VG scene graph. For characters and relationships presented in the doodler graph, we adopt a pre-trained text encoder\cite{Radford2021LearningTV} and search for the most similar VG\cite{Johnson} object based on the current story context.}
\label{fig:dg2sg}
\end{figure}

\begin{figure*}[!htb]
\begin{center}
   \includegraphics[width=0.9\linewidth]{figs/res_gen_reduced.pdf}
\end{center}
\caption{Doodler scene composition results. We demonstrate composed doodler scene examples for 6 stories. For each story, we show the parsed doodler graph (DG), the transformed casual scene graph (SG) based on the mapping strategy described in Sec. \ref{sec:scene_composer}(Fig.\ref{fig:dg2sg}) together with 4 composed scene examples. The result shows that with the doodler graph representation, we could handle complex scenes with multiple objects and relationships. Additionally, comparing STORY 3 with STORY 0, we further shows that our doodler graph representation could automatically remove redundant information and non-sense AI generated sentences like \textit{\'when he has ridden all\'}. Meanwhile, as the mapping strategy automatically turn fairy tale objects to casual scene graph objects, we could also deal with rare scene descriptions including \textit{\'witch\'} and \textit{\'forester\'}.}
\label{fig:res_gen}
\end{figure*}

\subsection{Doodler Graph Representation}
Same as scene graph, doodler graph is a graph data structure that describes the contents of a scene\cite{Johnson,Johnson2018,Johnson2018ImageGF}. We formulate the doodler graph as $G=(V,E)$, where vertexes $V={v_1,...,v_n }$ refers to the set of characters, and edges $E$ represents character relationships. Each doodler character $v_i=(o_i,c_i,w_i)$ is identified by its category $o_i$, color $c_i$ and stroke weight $w_i$. 

Given a story fragment, we adopt a rule-based semantic parser to generate the doodler graph. Similar to\cite{Johnson2018,Wu,Schuster}, the parser will first parse the fragment into a syntactic dependency tree. Then a set of tree transformations is applied to the dependency tree to acquire characters (i.e., nouns), adjectives that describe character attributes, and verbs that identify character relationships. In practice, we assign doodling color and stroke weight to each character either based on their attributes (e.g., a red flower) or randomly choose one from a color palette.



\subsection{Doodler Scene Composer}
\label{sec:scene_composer}
We train the doodler scene composer directly on the Visual Genome dataset (VG) \cite{Krishna2017}. Considering the gap of characters and relationships between fairytale scenes and real-world scenes, we adopt a pre-trained text-encoder \cite{Radford2021LearningTV} to map the doodler graph into a scene graph according to the current story fragment. Specifically, we treat the matching problem as a zero-shot prediction problem. We replace each doodler entity in the story fragment with 81 candidate VG entities. We then encode candidate story fragments into text features ${T_1,T_2, ..., T_N}, N=81$ and calculate their scale cosine similarity with original story fragment feature $T_0$, we choose the most similar entity to replace the doodler entity (Fig. \ref{fig:dg2sg}). We adopt the same way to transfer doodler relations into casual relations. With transferred doodler entity and relation, we build a casual scene graph that suits the VG dataset.

After transferring the doodler graph to scene graph, we adopt a graph-neural-network-based layout predictor \cite{Johnson2018} trained on VG dataset to predict layout for doodlers. Given the layout bounding box, and layers for each object, we adopt a modified Sketch-RNN decoder \cite{Huang2019} to draw each doodler entity onto the canvas. 

% \subsubsection{Perspective scene composer}

\subsection{Doodler graph update policy}
As the story going fragment by fragment, we update the doodler graph based on two policies with respect to stories and characters:
\begin{itemize}
    \item The \textit{\textbf{story-based policy}} is based on neural co-relation analysis and refreshes the doodler graph when a new story fragment comes. 
    \item The \textit{\textbf{character-based policy}} adopts a learned graph modifier and performs nodes insertion and deletion regarding adding/removing characters from the scene.
\end{itemize}

\subsubsection{Story-based policy}
As the story goes fragment by fragment, we update the doodler graph with co-reference analysis followed by entity linking. The co-reference system keeps track of pronouns in the system and matches the pronouns to nouns that existed in the previous context. We thus use the co-reference to link doodler characters in the newly generated fragment to the existing fragment. We analyze co-reference between adjacent story fragments. After the co-reference is resolved, we update pronouns to their referenced nouns. With the updated doodler graph, we predict layouts for new doodler characters and add each new character individually onto the canvas.

\subsubsection{Character-based policy}
The character-based update policy is similar to the story-based policy and is used to support user interaction. As a visualization medium, the doodler graph supports both transformings into/from story fragments and doodling. AI.R Taletorium is integrated with a doodler recognition system and allows users to add characters along with the generation. We will automatically parse user sketch into story characters and update the scene graph based on doodler spatial relationships. We'll generate the next story fragment based on the updated character set.