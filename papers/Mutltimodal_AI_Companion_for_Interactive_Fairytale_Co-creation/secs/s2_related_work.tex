\section{Related Work}

In the field of text to visual content(or, more specifically, image) synthesis \cite{Gregor2015DRAWAR,Herzig2019,Johnson2018,Koh2020TexttoImageGG,Li2019ObjectDrivenTS,Mansimov2016GeneratingIF,Ramesh2021ZeroShotTG,Reed2016GenerativeAT,Schuster2015GeneratingSP,Yan2016,Zhang2017StackGANTT,Zhang2019StackGANRI}, early studies \cite{Zhu} usually ground text-to-image generation as a search problem. Given a text description, they search for words, representative images, and placement from an existing database that most match the description. They then adopt a graphics renderer to combine the search results into the same canvas. Despite the complex AI models (vision, language, and graphics) integrated into the system, such systems are usually incapable of generating new images. 

With the rise of generative models\cite{Goodfellow2020,Mirza2014}, researchers also explore combining RNN and GANs for text-to-image generation by learning directly from massive text-image pairs \cite{Gregor2015DRAWAR,Li2019ObjectDrivenTS,Koh2020TexttoImageGG,Mansimov2016GeneratingIF,Ramesh2021ZeroShotTG,Reed2016GenerativeAT,Zhang2017StackGANTT,Zhang2019StackGANRI}. Such modelling enables the generation of new visual content from concise text descriptions. However, constrained by training data formation, they are generally incapable of modeling complex scenes with multiple objects.

Another stream of work adopts intermediate representations to deal with complex scenes. Johnson et al.\cite{Johnson2018} firstly introduce scene graph for image generation and outperforms previous methods in generating an intricate image with multiple objects. They develop a graph convolution network to process the scene graph, which acts as a principal component for following scene graph-based image generation methods \cite{Herzig2019,Schuster2015GeneratingSP}.

Similarly, we use scene graph as the medium between fairy tale and visual contents to support sketched scene generation from fairy tale fragments with unusual objects and complex relations. Along with the generation of fairy tale fragments, we par each fragment in real-time into a scene graph containing characters and their relationships covered by the fragment. We then propose a conditional neural scene composer learned from human captioned natural images to compose sketched scenes from the parsed scene graph. As the story goes fragment by fragment, we update the scene graph according to fragment co-references \cite{Sukthanker2020AnaphoraAC}.


\subsection{Learning for sketching}
Doodling, or free-hand sketch, is a universal communication and art modality which combines convenience with expressiveness. As a high-level abstraction of real-world contexts, doodling endorses advantages for neural processing with its simplicity while suffers from the high sparsity and wide style diversity. Based on neural representations for sketch structure\cite{Ha2018ANR,Ribeiro2020SketchformerTR}, a great amount of work has been done in doodle creation, recognition, retrieval, partial analysis and, abstraction, etc. We refer the audience to\cite{Xu2020DeepLF} for a comprehensive review of learning for sketching. 

With artificial intelligence becomes increasingly part in people’s everyday lives, making researches on human-AI co-creation rather essential. Recently, \cite{Ostrowski2020DesignRI} propose the positional idea to grounds the HRI design research on three touch-points: the roboticists as designers, the design features of the systems, and the users as co-designers. \cite{Parikh2020ExploringCC} explore four scenarios for collaborative human-human sketch co-creation, and shows that collaborative drawing with a third-party voting strategy leads to most creative sketches. Despite these pioneering works, the research in human-robot co-creation remains extremely limited. With AI.R Taletorium, we proposed an efficient human-robot co-creation scenario that utilize AI as a powerful interface.

The visualization system of AI.R Taletorium combines doodle creation with comprehension. It not only enables doodling from the story but also turns the user sketch interactively into the story.

\subsection{Interactive Storytelling (IS)}
Interactive storytelling plays an important role for for Early Childhood Education. AI learning companions, as either tutor or tutee role, provides an intelligent and responsive interface to support children’s learning in a variety of contexts including language development, storytelling and scientific learning \cite{Kanda2004InteractiveRA,Park2019AMA,Park2017TellingST,Gordon2016AffectivePO,Chen2020ImpactOI}. Compared with kindergarten teachers, personalized AI tutors/tutees could asses and automatically adapt to kid's knowledge level and physical needs. Previous studies\cite{Belpaeme2018SocialRF,Park2019AMA,Maeda2019CanAS} have emphasized that the personalized instruction from robots could clearly augmenting the efforts of parents and teachers to help kids acquiring both academic knowledge and positive attitudes. 

Storytelling, as an interactive process that facilitates imagination, creative thinking, language abilities, and cooperative learning, bringing a broad range of positive outcomes for primary education \cite{Garzotto2014InteractiveSF} summarizes methodologies and enabling technologies used in building interactive storytelling system for kids. The In-Visible island project \cite{Talib2020InVisibleII} further pinpoints digital inclusion in IS systems. They designed an inclusive system to join visually impaired children and sighted kids together into a unified storytelling process, which on the one hand, accelerated the social learning curve of visually impaired children, on the other hand, educating sighted children to have more empathy towards their peers with physical limitations. 


We design AI.R Taletorium to be character-driven. In AI.R Taletorium, the story is generated based on characters\cite{Liu2020,Surikuchi2019CharacterCentricS}, the scene graph is updated based on character coreferences, and the interaction is based on users adding/removing doodler characters from the drawing canvas.