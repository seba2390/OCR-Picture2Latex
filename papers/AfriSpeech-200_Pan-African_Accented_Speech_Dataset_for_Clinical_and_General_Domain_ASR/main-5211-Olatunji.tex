\documentclass[11pt,a4paper]{article}
\usepackage{times,latexsym}
\usepackage{url}
\usepackage[T1]{fontenc}

\usepackage{microtype}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{xurl}

\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage{xspace}
\newcommand*{\yoruba}{Yor\`ub\'a\xspace}

\usepackage[acceptedWithA]{tacl2021v1}
\setlength\titlebox{6.5cm} % <- for Option 2 below

%%%% Material in this block is specific to generating TACL instructions
\usepackage{xspace,mfirstuc,tabulary}

\usepackage{tablefootnote}
\newcommand{\dateOfLastUpdate}{Dec. 15, 2021}
\newcommand{\styleFileVersion}{tacl2021v1}
\newcommand{\ex}[1]{{\sf #1}}
\newif\iftaclinstructions
\taclinstructionsfalse % AUTHORS: do NOT set this to true
\iftaclinstructions
\renewcommand{\confidential}{}
\renewcommand{\anonsubtext}{(No author info supplied here, for consistency with
TACL-submission anonymization requirements)}
\newcommand{\instr}
\fi

%
\iftaclpubformat % this "if" is set by the choice of options
\newcommand{\taclpaper}{final version\xspace}
\newcommand{\taclpapers}{final versions\xspace}
\newcommand{\Taclpaper}{Final version\xspace}
\newcommand{\Taclpapers}{Final versions\xspace}
\newcommand{\TaclPapers}{Final Versions\xspace}
\else
\newcommand{\taclpaper}{submission\xspace}
\newcommand{\taclpapers}{{\taclpaper}s\xspace}
\newcommand{\Taclpaper}{Submission\xspace}
\newcommand{\Taclpapers}{{\Taclpaper}s\xspace}
\newcommand{\TaclPapers}{Submissions\xspace}
\fi

%%%% End TACL-instructions-specific macro block
%%%%

\title{AfriSpeech-200: Pan-African Accented Speech Dataset for Clinical and General Domain ASR}

%\title{Formatting Instructions for TACL \TaclPapers \\
%(Base files: \styleFileVersion-template.tex \& \styleFileVersion.sty, dated \dateOfLastUpdate)}

% Author information does not appear in the pdf unless the "acceptedWithA" option is given

% The author block may be formatted in one of two ways:

% Option 1. Author’s address is underneath each name, centered.

\author{\normalsize Tobi Olatunji$^{1*}$, Tejumade Afonja$^{2,3*}$, Aditya Yadavalli$^{4*}$, Chris Chinenye Emezue$^{5,6*}$ \\ \textbf{\normalsize Sahib Singh$^{7*}$, Bonaventure F.P. Dossou$^{5,6,8,9*}$, Joanne Osuchukwu$^{1}$,  Salomey Osei$^{10*}$, }\\
\textbf{\normalsize Atnafu Lambebo Tonja$^{8,11,12*}$, Naome Etori$^{13*}$, Clinton Mbataku$^{3*}$}\\ 
\\
\footnotesize
$^*$Masakhane NLP, $^1$Intron Health, $^2$CISPA Helmholtz Center for Information Security $^{3}$AI Saturdays Lagos $^4$Karya Inc,  \\
\footnotesize 
$^5$Mila Quebec AI Institute, 
$^6$Lanfrica, $^7$Ford Motor Company, $^8$Lelapa AI, $^9$McGill University, $^{10}$University of Deutso, \\
\footnotesize 
$^{11}$Instituto Politécnico Nacional, $^{12}$University of Colorado Colorado Springs, $^{13}$University of Minnesota\\
\texttt{Corresponding Author: tobi@intron.io}
% \footnotesize
% $^5$Carnegie Mellon University Africa,
% $^6$Microsoft AI For Good Kenya
}
% $^11$ University of Colorado Colorado Springs

% \author{
%   Tobi Olatunji %\Thanks{The {\em actual} contributors to this instruction document and corresponding template file are given in Section \ref{sec:contributors}.} 
%   \\
%   Intron Health
%   \\
%   \texttt{tobi@intron.io}
%   \And
%   Tejumade Afonja 
%   \\
%   CISPA Helmholtz
%   \\
%   Center for Information Security
%   \\
%   \texttt{tejumade.afonja@cispa.de}
%   \And
%   Aditya Yadavalli 
%   \\
%   Karya Inc
%   \\
%   \texttt{aditya@karya.in} 
%     \AND
%   Chris Chinenye Emezue
%   \\
%   Mila Quebec AI Institute
%   \\
%   \texttt{chris.emezue@gmail.com} 
%       \And
%   Sahib Singh
%   \\
%   Ford Motor Company
%   \\
%   \texttt{sahibsingh570@gmail.com}
%      \AND
%   Bonaventure F.P. Dossou
%   \\
%   Mila Quebec AI Institute
%   \\
%   \texttt{bonaventure.dossou@mila.quebec}
%   \And
%   Joanne Osuchukwu
%   \\
%   Intron Health
%   \\
%   \texttt{joanneosuchukwu@gmail.com} 
%   \AND
%   Salomey Osei
%   \\
%   University of Deusto
%   \\
%   \texttt{sosei@aimsammi.org} 
%   \And
%   Atnafu Lambebo Tonja
%   \\
%   Instituto Politécnico Nacional
%   \\
%   \texttt{atnafuatx@gmail.com}
%   \And
%   Naome Etori
%   \\
%   University of Minnesota
%   \\
%   \texttt{etori001@umn.edu}
%   \AND
%   Clinton Mbataku
%   \\
%   AI Saturdays Lagos
%   \\
%   \texttt{mbatakuclinton@gmail.com}
% }

% % Option 2.  Author’s address is linked with superscript
% % characters to its name, author names are grouped, centered.

% \author{
%   Template Author1\Thanks{The {\em actual} contributors to this instruction
%     document and corresponding template file are given in Section
%     \ref{sec:contributors}.}$^\diamond$ 
%   \and
%   Template Author2$^\dagger$
%   \\
%   \ \\
%   $^\diamond$Template Affiliation1/Address Line 1
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   Template Affiliation1/Address Line 2
%   \\
%   \texttt{template.email1example.com}
%   \\
%   \ \\
%   \\
%   $^\dagger$Template Affiliation2/Address Line 1
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   Template Affiliation2/Address Line 2
%   \\
%   \texttt{template.email2@example.com}
% }

\date{}


\begin{document}
\maketitle
\begin{abstract}
Africa has a very low doctor-to-patient ratio. At very busy clinics, doctors could see 30+ patients per day-- a heavy patient burden compared with developed countries-- but productivity tools such as clinical automatic speech recognition (ASR) are lacking for these overworked clinicians. However, clinical ASR is mature, even ubiquitous, in developed nations, and clinician-reported performance of commercial clinical ASR systems is generally satisfactory. Furthermore, the recent performance of general domain ASR is approaching human accuracy. However, several gaps exist. Several publications have highlighted racial bias with speech-to-text algorithms and performance on minority accents lags significantly. To our knowledge, there is no publicly available research or benchmark on accented African clinical ASR, and speech data is non-existent for the majority of African accents. We release AfriSpeech, 200hrs of Pan-African English speech, 67,577 clips from 2,463 unique speakers across 120 indigenous accents from 13 countries for clinical and general domain ASR, a benchmark test set, with publicly available pre-trained models with SOTA performance on the AfriSpeech benchmark.
  %This document contains the formatting requirements for TACL \taclpapers. These formatting rules take effect for all \taclpapers received from September 2, 2018 onwards.
\end{abstract}


\section{Introduction}

The African continent and the nearby islands constitute one-fourth of the land surface of the earth \cite{lodhi1993language}. Approximately 1.3 billion people live in Africa, which is about 18\% of the world's population \cite{enwiki:1132870977}. Of the estimated 7,000+ languages and dialects in the world, over 3,000 languages are found in Africa \cite{enwiki:1133594141, heine2000african}. %Some languages in Africa are spoken by more than 20 or 30 million people, e.g. Hausa-Fulani, Oromo/Galla and Swahili \citep{lodhi1993language}. 

Despite its large and predominantly young population, Africa bears a significant proportion of the global disease burden \cite{de2010tackling} with multiple socioeconomic factors contributing to high mortality and morbidity rates \cite{baingana2006changing}. Healthcare systems are overburdened and underfunded in many African countries \cite{oleribe2019identifying, naicker2009shortage,nkomazana2015stakeholders}, struggling to cope with the increasing demand for services, while at the same time facing significant shortages of trained health workers \cite{whoChronicStaff, ahmat2022health, naicker2010shortage, nkomazana2015stakeholders, kinfu2009health,etori2023we}. A recent study conducted by \citet{ahmat2022health} in 47 African countries shows that the region has a ratio of 1.55 health workers (physicians, nurses, and midwives) per 1000 people 3x less than the WHO-recommended density of 4.45 health workers per 1000 people. %Multiple studies \citep{govender2012stress, schweitzer1994stress, thomas2006levels} show high-stress levels in African physicians in public sector hospitals seeing over 30 patients per day. \citet{thomas2006levels} reports that, of the doctors who saw more than 40 patients per day, 66.7\% experienced symptoms of burnout compared with 53.3\% of those who saw fewer than 40 patients per day.
%This is less than the WHO-recommended density of 4.45 health workers per 1000 people for providing essential health services and achieving universal health coverage.

While technology can help mitigate some of these problems, \citet{bukachi2007information} and \citet{manyati2021systematic} aptly show that although Africa has enjoyed massive growth in mobile technology, telecommunication, and internet penetration over the past two decades, healthcare technology lags significantly. %\citet{owoyemi2020artificial} contrasts the adoption of artificial intelligence in African healthcare with global contemporaries and \citet{omotosho2019current, sharma2020digital} conclude that, despite a higher disease burden, healthcare technology in the Global South trails behind.

A 2019 systematic review on the use of Automatic Speech Recognition (ASR) for clinical documentation in the US from 1990 to 2018 by \citet{blackley2019speech} and other similar studies \citep{goss2019clinician, blackley2020physician, ahlgrim2016introduction, vogel2015analysis} showed that the use of speech recognition led to a 19-92\% decrease in mean documentation time, 50.3-100\% decrease in turnaround time, and 17\% improvement in documentation quality. %Although \citet{kodish2018systematic, miner2020assessing} found room for improvement, \citet{paats2018retrospective, goss2019clinician} showed clinical ASR has reached satisfactory performance in clinical settings \citep{sezgin2022hey, muhammad2015automatic, chleborad2013evaluation, paats2018retrospective}. 
However, in the African context, the lack of training datasets for many of the 3000+ languages and accents in the continent remains an obstacle in developing and adopting robust speech recognition systems for the general domain and for clinical ASR in particular \citep{doumbouya2021using, siminyu2021ai4d, babirye2022building, ogayo2022building}. While recent efforts have begun to turn this tide for the majority of African languages like Swahili, Kinyarwanda, and Yoruba \citep{gutkin2020developing, dossou2021okwugb, Olaleye2022YFACCAY}, over a thousand African languages and accents remain excluded from global speech research advancements. 

Recent single-digit word error rates (WER) \citep{chen2022wavlm, radford2022robust, hsu2021hubert, Baevski2020wav2vec2A} in multiple SOTA publications and benchmarks on Librispeech \citep{panayotov2015librispeech}, TED-LIUM3 \citep{hernandez2018ted}, and other datasets using architectures like Wav2vec2 \cite{Baevski2020wav2vec2A}, Conformer \cite{gulati2020conformer}, Transducer, and Whisper \cite{radford2022robust} contrast significantly with ASR performance for African accented speech \citep{gutkin2020developing, dossou2021okwugb} (see Figure \ref{appendix:wer_libri_afri}). We explore whether curating a large pan-African speech corpus might unlock comparable single-digit performance on African accents. We restrict this investigation to accented speech in English because English is the official language for the medical record in most Anglophone African countries, expanding the utility of this work to multiple Anglophone African countries. 

%(see Appendix Figure \ref{appendix:wer_libri_afri}). Since off-the-shelf pre-trained models underperform in the general and clinical domains for African accents, we explore whether curating a large pan-African speech corpus might unlock comparable single-digit performance on African accents. We restrict this investigation to accented speech in English because English is the official language for the medical record in most African countries. This expands the utility of this work to multiple countries in Anglophone Africa. 

Our contributions are as follows: 
\begin{itemize}%[leftmargin=0.4cm, noitemsep]
\item We present \textit{AfriSpeech-200}\footnote{https://huggingface.co/datasets/tobiolatunji/afrispeech-200}, the first and most diverse open-source pan-African accented English speech corpus for clinical and general domain ASR, providing 200.70 hrs of accented speech, 67,577 speech-transcript pairs in 120 African accents across 13 countries, a benchmark dataset that paves the way for out-of-distribution, few-shot and zero-shot analyses on very-low-resource accents. \footnote{AfriSpeech-200 is licensed under a CC BY-NC-SA 4.0 license}

%\item AfriSpeech is the first Pan-African clinical ASR dataset.

\item We present a templating framework to augment existing corpora with native African proper nouns and evaluate multiple SOTA pre-trained models and leading commercial ASR systems on our benchmark dataset. We provide in-depth analysis of selected models to explain their failure modes and offer helpful insights.

\item We fine-tune the best-performing open-source models and achieve SOTA performance on the AfriSpeech benchmark dataset (108 African accents) as well as show promising zero-shot performance on very low-resource accents. We provide best models\footnote{https://huggingface.co/Seyfelislem/afrispeech\_large\_A100} as publicly available pre-trained checkpoints.

\end{itemize}

\section{Related Work}

%\subsection{Recent Advancements in Robustness to Accented Speech}

With the advent of large multilingual speech datasets \citep{7178964, indicwav2vec, Chen2021GigaSpeechAE, Ardila2020CommonVA, Valk2021VOXLINGUA107AD}, various research groups have proposed large self-supervised speech models such as wav2vec \cite{schneider19_interspeech}, vq-wav2vec \cite{Baevski2020vqwav2vecSL}, wav2vec 2.0 \cite{Baevski2020wav2vec2A}, HuBERT \cite{9585401}, XLSR \cite{Conneau2021UnsupervisedCR}, and XLS-R \cite{Babu2022XLSRSC}. These models achieved state-of-the-art performance on many downstream tasks such as automatic speech recognition (ASR), automatic speech translation (AST), and language identification. However, most existing systems still perform poorly on accented speech \citep{javed2022towards}. \citet{koenecke2020racial} further showed that popular commercial ASR systems -- like Amazon, Apple, Google, IBM, and Microsoft -- exhibit substantial racial disparities in their speech recognition capabilities. Most ASR systems work best for native English speakers and their accuracy plummets dramatically with non-native English speakers \cite{hassan2022improvement, prasad-jyothi-2020-accents}.


To enhance the performance of accented speech recognition, various methods have been proposed, which can be categorized into modeling and dataset approaches. On the modeling front, there have been efforts such as dialect-aware ASR models \cite{Yadavalli2022MultiTaskEM}, domain adversarial training (DAT) \cite{Sun2018DomainAT}, combining DAT with transfer learning \cite{Chen2020AipnetGA}, using voice conversion (VC) \cite{zhang22n_interspeech}, combining VC with speed perturbation \cite{zhang22n_interspeech}, and accent pre-training (Acc-PT) \cite{Das2021BestOB}. These efforts, however, produced marginal improvements and still exhibit poor generalization capabilities. 

Datasets have played a major role in improving ASR performance. The current SOTA in ASR \cite{radford2022robust} demonstrated the superior utility of large supervised datasets. 
%\subsection{Bridging the Gap with Accented Datasets}
Therefore, to bridge the ASR performance gap for African accented speech, multiple dataset creation efforts \citep{doumbouya2021using, siminyu2021ai4d, babirye2022building, ogayo2022building, gutkin2020developing, dossou2021okwugb, afonja2021learning, kamper2011multi,ibejih2022edustt} have been established. However, many of these datasets are limited in size and diversity. For example, Common Voice \citep{Ardila2020CommonVA} contains less than 10 hours of African English speech, \citet{li2021accent} evaluates on 50 hrs of African accented English (not released), \citet{sanabria2023edinburgh} provides 40 hrs of accented English, less than 20\% is African. \citet{kamper2011multi, de2007human} are limited to a few South African accents, and \citet{ibejih2022edustt} contains less than 8 hours, while \citet{afonja2021learning} contains less than 2 hours of accented African English speech. Furthermore, there are no available benchmarks for clinical ASR for African languages, creating a need for evaluation datasets that help identify areas of improvement in this domain. %This underlies the importance of our work with the AfriSpeech dataset.

While previous works have primarily focused on adapting Western accents to African accents, to the best of our knowledge, there has been limited research specifically addressing domain adaptation from a general domain to the clinical domain in the African context. In this regard, our work is the first attempt to bridge this gap and tackle the unique challenges associated with adapting accented 
African English ASR systems to the clinical domain. %We are releasing AfriSpeech-200 to complement these efforts and make speech recognition useful to all.


% \citet{prasad-jyothi-2020-accents} study how various accents are represented internally by large end-to-end ASR systems. Their work suggests that the accent information in an end-to-end ASR systems is largely represented in the first few layers of the neural network.

  %To quantify bias, \citep{huang2001analysis} introduced principal component analysis (PCA) and independent component analysis (ICA) as tools for analyzing speaker variation and found that the first two components corresponded to a speaker's gender and accent, proposing the use of ICA-extracted components for gender and speaker classification. Similarly, \citep{zheng2005accent} found that the degree of accentedness of a speaker correlated with the proportion of alveolar affricates and fricatives in the speech, prompting the use of Gaussian mixture models (GMM) classifiers with Mel-frequency cepstral coefficients (MFCC) and $F_o$ (pitch) as input features to discriminate between different speaker accents to improve ASR models for accented speech. 

% There have been several works in the past to analyze the type and degree of the biases, and offered some methods to classify them.

%\subsection{Recent advancements in accented ASR}
%Despite the fact that Automatic Speech Recognition (ASR) models have made human-computer communication more proficient, the models in the majority of ASR systems have been trained using native English accents. Hence, most ASR systems work best for native English speakers and their accuracy plummets dramatically when it comes to non-native English accents \cite{hassan2022improvement, prasad-jyothi-2020-accents}. \citep{huang2001analysis} introduced principal component analysis (PCA) and independent component analysis (ICA) as tools for analyzing speaker variation and found that the first two components correspond to a speaker's gender and accent, suggesting the use of ICA-extracted components for gender and speaker classification. Similarly, \citep{zheng2005accent} found that the degree of accentedness of a speaker correlated with the proportion of alveolar affricates and fricatives in the speech, prompting the use of GMM classifiers with MFCC and $F_o$ (pitch) as input features to discriminate between different speaker accents to improve ASR models for accented speech.
%{\color{red} To be continued}





\section{AfriSpeech Dataset}

% We introduce AfriSpeech, a Pan-African accented English speech dataset for clinical and general domain ASR, a collection of 200hrs of speech crowd-sourced from 2,463 African speakers from 13 Anglophone countries across Sub-Saharan Africa and the United States.

We introduce AfriSpeech, a Pan-African accented English speech dataset for clinical and general domain ASR crowd-sourced from 2,463 African speakers, 200.70 hrs with an average audio duration of 10.7 seconds. Speaker, gender, age group, and clip domain distributions are shown in Table \ref{tab:dataset_stats}. In the following subsections, we describe the dataset creation process.


\subsection{Focus Languages}

% We investigate 120 African accents across 13 countries. These accents are derived from languages belonging to 5 language families \citep{Ethnologue_Eberhard}: Afro-Asiatic, Indo-European, Khoe-Kwadi (Hainum), Niger-Congo, and Nilo-Saharan, representing regional diversity across western, eastern, and southern Africa. Table \ref{tab:countries} shows the number of clips, speakers, and hours of data per country. 
We conducted an investigation on 120 African accents across 13 countries including the United States and Turkey. These accents originate from languages that belong to five language families, as documented by Eberhard \citep{Ethnologue_Eberhard}: Afro-Asiatic, Indo-European, Khoe-Kwadi (Hainum), Niger-Congo, and Nilo-Saharan. This selection represents the diverse linguistic landscape across western, eastern, and southern Africa. In Table \ref{tab:countries}, we provide an overview of the number of clips, speakers, and hours of data per country, with Nigerian accents comprising 67\% of the dataset. Since some languages are spoken across several countries (e.g., Swahili, isiZulu, Hausa, and Luganda), accents are not unique to countries.

% Appendix Tables \ref{tab:dataset_accent_stats1}, and \ref{tab:dataset_accent_stats2} provide a list of AfriSpeech accents along with the number of unique speakers, countries where speakers for each accent/language are located, duration in seconds for each accent and their presence in train/dev/test splits. %Table \ref{tab:language_family_stats} shows ISO-3, Family, Country, and Region for each accent.


% \begin{table}
% \small
% \centering
% \begin{tabular}{l|r|r|r}
% \hline
% \textbf{Country} & \textbf{Clips} & \textbf{Speakers} & \textbf{Hours}\\
% \hline
% NG & 45875 & 1979 & 142.40 \\ 
% KE & 8304 & 137 & 20.89 \\ 
% ZA & 7870 & 223 & 22.69 \\ 
% GH & 2018 & 37 & 5.16 \\ 
% BW & 1391 & 38 & 3.96 \\ 
% UG & 1092 & 26 & 2.89 \\ 
% RW & 469 & 9 & 1.47 \\ 
% US & 219 & 5 & 0.53 \\ 
% TR & 66 & 1 & 0.18 \\ 
% ZW & 63 & 3 & 0.18 \\ 
% MW & 60 & 1 & 0.15 \\ 
% TZ & 51 & 2 & 0.18 \\ 
% LS & 7 & 1 & 0.02 \\ 
% \hline
% \end{tabular}
% \caption{Contributions by Country showing speakers, number of clips, and speech duration in seconds and hours.}
% \label{tab:countries}
% \end{table}

\begin{table}
\small
\centering
\begin{tabular}{l|r|r|r}
\hline
\textbf{Country} & \textbf{Clips} & \textbf{Speakers} & \textbf{Hours}\\
\hline
Nigeria &  45875 &      1979 & 142.40 \\
Kenya &   8304 &       137 &  20.89 \\
South Africa&   7870 &       223 &  22.69 \\
Ghana &   2018 &        37 &   5.16 \\
Botswana &   1391 &        38 &   3.96 \\
Uganda &   1092 &        26 &   2.89 \\
Rwanda &    469 &         9 &   1.47 \\
United States\tablefootnote{Although the self-reported country from the speakers is the United States, their reported accents, namely Yoruba and Igbo, is mostly spoken in the western part of Africa.} &    219 &         5 &   0.53 \\
Turkey\tablefootnote{Even though the reported country is Turkey, the reported Zulu accent is mostly spoken in the southern part of Africa.}  &     66 &         1 &   0.18 \\
Zimbabwe &     63 &         3 &   0.18 \\
Malawi &     60 &         1 &   0.15 \\
Tanzania &     51 &         2 &   0.18 \\
Lesotho &      7 &         1 &   0.02 \\
\hline
\end{tabular}
\caption{Contributions by Country showing speakers, number of clips, and speech duration in seconds and hours.}
\label{tab:countries}
\end{table}


\subsection{Obtaining AfriSpeech Transcripts}

Neural network models learn concepts from training data. Where the training data is  predominantly Western (e.g. Common Voice \citep{ardila2019common}), the resulting ASR systems fail to capture important pan-African contexts. For example, ASR systems fail woefully at transcribing African names like "Ogochukwu" (Igbo), "Malaika" (Swahili), or "Uwimana" (Rwandan), while excellently transcribing Western names like "Lauren" and "Bryan"-- representative of the bias in their training corpora. 
% Appendix \ref{fig:error_analysis} shows some revealing examples. 
To solve the problem of scarce African-centric text in the general and clinical domains, we created AfriSpeech using the following strategies.

\subsubsection{Finding Available Transcripts}
\label{findingtranscripts}
Our first task was to supplement existing large multi-domain corpora with African-centric text. Our first target was \textbf{Wikitext-103} \citep{merity2016pointer}, a collection of over 100 million tokens extracted from the set of verified "good" and "featured" articles on Wikipedia curated by Salesforce. We split this corpus on sentence boundaries and randomly sampled sentences for our transcript corpus. Our next strategy was \textbf{web scraping}. We crawled and scraped major African news websites across multiple African countries on topics like politics, entertainment, sports, religion, education, etc. In contrast to Wiki-text, the resulting corpus contained several African names, cities, and highly relevant vocabulary applicable to real-world use cases for downstream ASR. By scraping health-focused websites and health sections of news websites, we were able to get content from the clinical domain, albeit very little.

To increase clinical content representation, we focused on two multi-specialty biomedical datasets: \textbf{PubMed} \citep{wheeler2007database} and \textbf{NCBI disease} \citep{dougan2014ncbi}. We split these corpora on sentence boundaries and randomly sampled sentences for our transcript corpus. %The National Library of Medicine (NLM) produces a baseline set of MEDLINE/PubMed citation records in XML format for download on an annual basis . PubMed contains citations and abstracts of biomedical literature from several NLM literature resources, including MEDLINE -- the largest component of the PubMed database.  Following the same process, we extracted transcript samples from the NCBI disease corpus, which is a collection of 793 PubMed abstracts fully annotated at both mention and concept levels , and contains 6,892 disease mentions, which are mapped to 790 unique disease concepts.

\subsubsection{Finding African Entities}
\label{finding_entities}
 We sourced for African-centric entities in two places: first, we leveraged an existing database of over 90,000 African names from the transatlantic slave trade between 1808 and 1863 \citep{anderson2013using}, which increased our coverage of African names, phonemes, and morphemes. We then used \citet{okagbue2017personal}'s dataset of 965 Igbo names collected to reflect the dialectal classification of Igbo people and supplemented it with 1,000 more Nigerian names from other cultures such as Yoruba, Hausa, Fulani, Tiv, Efik, Ibibio, etc. These names were obtained from freely available textbooks, online baby name websites, oral interviews, published articles, and online forums like Instagram and Twitter. Finally, we obtained a list of African cities from Wikipedia \citep{enwiki:1146587606}. %and randomly (with a uniform probability) replaced each [LOC] token in the AfriSpeech templates.

\subsubsection{ AfriSpeech Templates}
 The web scraping corpus was highly relevant but small. In the larger biomedical and Wikitext datasets, African content was sparse. We, therefore, sought to increase the utility of the curated corpora by creating "Africanized" versions. Several studies have demonstrated the utility of "templates" as an effective way to create richer, more expressive training datasets, especially for Question-Answering and prompt engineering \citep{pawar2016question, brown2020language, yao2022prompt} and named entity recognition \citep{DBLP:conf/tsd/DavodyAKK22}. Inspired by this approach, we augment our dataset by sampling sentences from the corpora described above in addition to template sentences contributed by professional clinicians, hand-crafting a total of 140 template sentences. For each template sentence, we masked proper nouns (first names, last names, organizations, and cities), replacing them with their corresponding NER tags [PER, ORG, LOC]. We then randomly replaced the masked tokens with African-centric entities-- African names and cities, derived from section \ref{finding_entities} above, as well as common tropical diseases and medications. Each template sentence was reused 200 times. A random subset was sampled, sent as prompts for recording, and included with this release. Templated sentences represent approximately 30\% of this corpus.
 
 
% creating templates from sentences sampled from the curated corpora described above (Section \ref{findingtranscripts}). In addition to template sentences contributed by professional clinicians, we sampled sentences from the corpora described above and hand-crafted a total of 140 template sentences. We achieved this by extracting proper nouns (first names, last names, organizations, and cities) and dates, and replacing them with their corresponding NER tags [PER, ORG, LOC]. We then randomly replaced the NER tags with African-centric entities (like African names, cities, clinical terms, etc), using strategies described in the Appendix \ref{appendix:transcripts}.




\subsection{Audio Recording}

\begin{figure}[t]
\includegraphics[scale=0.2]{images/record.png} %width=8cm
\centering
\caption{Intron Online Recording platform.}
\label{fig:web_app_screen}
\end{figure}

\paragraph{Collection:} Inspired by Common-Voice \citep{ardila2019common} and SautiDB \citep{afonja2021learning}, we developed and deployed a web-based application in Python/Flask (Figure \ref{fig:web_app_screen}) to collect crowd-sourced speech samples. The application also facilitates tracking of completion status, user demographics, reviews, and quality control.  The app presents randomly selected sentences (prompts) to the speakers and prompts them to record their voices while reading the text. The speech recordings are persisted as mono-channel, 16-bit wav files, with a 48 kHz sampling rate.  Post-processing tasks were performed on the audio recordings to remove samples shorter than 2 seconds and longer than 17 seconds. Raw unedited samples are provided as part of this release. Speakers in this dataset have been de-identified. Demographic information available includes gender, age group, accent, and country.

\paragraph{Annotation Instructions} Recorder demographics are presented in Table \ref{tab:dataset_stats}. Instructions were provided to crowd-sourced recorders as detailed in Appendix \ref{appendix:annotation}. Notably, the recorders were instructed to read punctuation marks in full and encouraged to use their natural accent.


\subsection{Quality Control}

  \paragraph{Projects:} Transcripts were bucketed into projects to separate clinical from general domain prompts. This approach maximized the time value of clinician contributors focusing their efforts more on medical prompts. 
  
 \paragraph{Reviewers:} We hired a team of human reviewers who up-voted or down-voted clips to indicate quality. Text feedback was also provided to recorders in 30\% of cases where negative feedback was indicated. The text feedback contained the reason for the down-vote and was intended to help recorders improve future recording quality. %Text feedback was typically provided to users with background noise, long pauses, distractions, unnecessarily slow speech, wrong dictations, and so on.
 
 \paragraph{Guest Clip Review:} New recorders were admitted as guests and allowed to record a maximum of 200 clips before quality review. 10 to 30 clips were reviewed per guest and those who passed review were promoted to a "Paid" status. %and allowed to record over the 200 clip limit but less than 1hr per project. Others below the 200 clip limit were allowed to record more guest clips till the clips pass review. 


 \paragraph{Paid Clip Review:} In the paid category, users were allowed a maximum of 200 clips before a temporary pause for quality check. During the temporary suspension, reviewers randomly reviewed 10\% of the speech samples provided and positive, negative, or text feedback was provided. Access was restored if quality remained satisfactory, or users were blacklisted if over 30\% of clips reviewed were down-voted.
 
 %reviewers manually restore user access. Users continue to record till they reach the maximum allowed clips for each project. Where over 30\% of clips reviewed were unsatisfactory, the user was blacklisted and barred from further contributions.
 
 \paragraph{Delisting Problematic Sentences:}  Where an audio clip receives a down-vote, the corresponding sentence is released for re-recording by a different user. If a clip recorded for the same sentence receives a second down-vote, the transcript itself is blacklisted.


\section{Experiments}

\subsection{Data}

\begin{table}
%\tiny
\small
\centering
\begin{tabular}{|l|l|}
%\hline 
%\multicolumn{2}{|l|}{\textbf{Dataset Statistics}}\\
%\hline
%Total duration & 200.91 hrs  \\
%Total clips & 67,577  \\
%Unique Speakers & 2,463  \\
% Average Audio duration & 10.7 seconds  \\
\hline
\multicolumn{2}{|l|}{\textbf{Speaker Gender Ratios - \# Clip \%}}\\
\hline
Female & 57.11\%  \\
Male & 42.41\%  \\
Other/Unknown & 0.48\% \\
\hline
\multicolumn{2}{|l|}{\textbf{Speaker Age Groups - \# Clips}}\\
\hline
<18yrs & 1,264 (1.87\%) \\
19-25 & 36,728 (54.35\%)  \\
26-40 & 18,366 (27.18\%)  \\
41-55 & 10,374 (15.35\%) \\
>56yrs & 563 (0.83\%)  \\
Unknown & 282 (0.42\%) \\
\hline
\multicolumn{2}{|l|}{\textbf{Clip  Domain - \# Clips}}\\
\hline
Clinical & 41,765 (61.80\%)  \\
General & 25,812 (38.20\%) \\
\hline
\end{tabular}
\caption{Dataset statistics.}
\label{tab:dataset_stats}
\end{table}


AfriSpeech-200 is a manually reviewed and curated subset, representing 7\% of the total AfriSpeech dataset, intended as an initial public release to stimulate research into African clinical and general domain ASR for accents with little or no representation in speech research. Table \ref{tab:countries} shows the distribution of clips, unique speakers, and hours by country. 
% Appendix Tables \ref{tab:dataset_accent_stats1} and \ref{tab:dataset_accent_stats2} list all accents in AfriSpeech with the distribution of clips, unique speakers, duration, and countries per accent. 
%We present AfriSpeech-testset, a benchmark dataset for evaluating the performance of SOTA ASR systems on 108 African accents, a deliberate effort to include several minority African accents in the global progress of speech recognition. Subsequent AfriSpeech releases will further increase minority accent representation and cover more domains.


\begin{table}
\small
\centering
\begin{tabular}{l|c|c|c}
\hline
\textbf{Item} & \textbf{Train} & \textbf{Dev}  & \textbf{Test}\\
\hline

\# Speakers & 1466 & 247 & 750 \\
%\#  Seconds & 624228.83 & 31447.09 & 67559.10 \\
\#  Hours & 173.4 & 8.74 & 18.77 \\
\#  Accents & 71 & 45 & 108 \\
Avg secs/speaker & 425.80 & 127.32 & 90.08 \\
clips/speaker & 39.56 & 13.08 & 8.46 \\
speakers/accent & 20.65 & 5.49 & 6.94 \\
secs/accent & 8791.96 & 698.82 & 625.55 \\
\# general domain & 21682 & 1407 & 2723 \\
\# clinical domain & 36318 & 1824 & 3623 \\

\hline
\end{tabular}
\caption{Dataset splits showing speakers, number of clips, and speech duration in Train/Dev/Test splits.}
\label{tab:splits}
\end{table}

As shown in Table \ref{tab:splits}, the train, test, and development sets are bucketed such that any given speaker may appear in only one. This ensures that contributors seen at train time are not seen at test time, which would skew the results. %Duplicate clips are removed from the train, test, and development sets of the corpus.


\subsection{Benchmarks}

We compare SOTA open-source pre-trained ASR models: Whisper \citep{radford2022robust}, Wav2vec2 \citep{Baevski2020wav2vec2A}, XLSR \citep{Babu2022XLSRSC}, Hubert \citep{hsu2021hubert}, WavLM \citep{chen2022wavlm}, Conformer  \citep{gulati2020conformer}, and CRDNN-RNNLM \citep{ravanelli2021speechbrain} with commercial clinical and non-clinical ASR systems. We refer readers to read the respective papers for details on pretraining corpora, model architecture, and hyperparameters. For each model, we compare performance (WER) on Librispeech test-clean partition \citep{panayotov2015librispeech} with WER on the AfriSpeech dev and test sets. Single-run results are provided.

%

\begin{table*}
\tiny
\centering
\begin{tabular}{l|l|p{1.8cm}|p{0.7cm}|l|l|l|l|l|l}
\toprule 
Model & Params & Training/Fine-tuning Corpora & ls-clean & \multicolumn{3}{c|}{Dev (45 accents)} & \multicolumn{3}{c}{Test (108 accents)}\\
  & & &  & General & Clinical & Both & General & Clinical & Both \\
\midrule
\multicolumn{10}{l}{Open-Source SOTA Models}\\
\hline
openai/whisper-large     & 1550M & Multi, 680k hrs  &    0.167          &   0.235 &    0.287 &  0.261 &   0.240 &    0.375 &  0.306 \\
openai/whisper-medium     & 769M & Multi, 680k hrs &       0.166       &   0.246 &    0.300 &  0.273 &   0.276 &    0.392 &  0.332 \\
openai/whisper-medium-en   & 769M& Multi, 680k hrs   &     0.169          &   0.267 &    0.315 &  0.291 &   0.304 &    0.414 &  0.358 \\
openai/whisper-small    & 244M & Multi, 680k hrs  &          0.167           &   0.313 &    0.372 &  0.343 &   0.330 &    0.455 &  0.391 \\
openai/whisper-small-en  & 244M & Multi, 680k hrs   &      0.167  &   0.319 &    0.384 &  0.352 &   0.350 &   0.482 &  0.414 \\
nvidia/stt-en-conformer-ctc-large     & 118M & Multi, 10 &   0.210      &   0.410 &    0.486 &  0.448 &     - &      - &    - \\
nvidia/stt-en-conformer-transducer-large  & 139M & Multi, 10  &  0.150   &   0.408 &    0.477 &  0.443 &     - &      - &    - \\
jonatasgrosman/wav2vec2-large-xlsr-53-english  & 317M & Multi, 3  &  0.100  &   0.498 &    0.561 &  0.530 &   0.506 &    0.650 &  0.576 \\
jonatasgrosman/wav2vec2-xls-r-1b-english      & 317M & Multi, 4  &  0.087   &   0.502 &    0.571 &  0.537 &   0.521 &    0.670 &  0.594 \\
%facebook/wav2vec2-large-robust-ft-libri-960h   & 317M & Single, 5   &  0.058  &   - &    - &  0.589 &   - &    - &  0.654 \\ % 0.58

facebook/wav2vec2-large-960h-lv60-self   & 317M & Single, 2  &  0.051  &   0.512 &    0.587 &  0.550 &   0.533 &    0.694 &  0.611 \\
%facebook/hubert-large-ls960-ft     & 316M & Single, 1 &   0.053       &   0.536 &    0.598 &  0.567 &   0.557 &    0.713 &  0.633 \\
facebook/hubert-xlarge-ls960-ft      & 1B & Single, 1  &    0.052    &   0.531 &    0.610 &  0.571 &   0.562 &    0.725 &  0.641 \\
patrickvonplaten/wavlm-libri-clean-100h-large   & 317M & Single, 1   &  0.091  &   0.606 &    0.679 &  0.643 &   0.631 &    0.783 &  0.705 \\
facebook/wav2vec2-large-960h           & 317M & Single, 1   &   0.062   &   0.610 &    0.695 &  0.652 &   0.641 &    0.797 &  0.717 \\
facebook/wav2vec2-large-robust-ft-swbd-300h & 317M & Single, 5  & 0.093 &   0.689 &    0.778 &  0.734 &   0.733 &    0.906 &  0.817 \\
%patrickvonplaten/wavlm-libri-clean-100h-base  & 95M & Single, 1   &   0.139   &   0.782 &    0.834 &  0.808 &   0.811 &    0.911 &  0.859 \\
%speechbrain/crdnn-rnnlm-librispeech  & 230 M & Single, 1   &   0.290     &   0.819 &    0.899 &  0.859 &     - &      - &    - \\
\hline
\multicolumn{10}{l}{Commercial ASR APIs}\\
\hline
Azure             & - & - &  &   0.438 &    0.468 &  0.453 &   0.340 &    0.444 &  0.391 \\
AWS         & - & - & &   0.332 &    0.437 &  0.385 &   0.354 &    0.536 &  0.442 \\
GCP        & - & - &  0.132  &   0.494 &    0.565 &  0.530 &   0.534 &    0.624 &  0.578 \\
\hline
\multicolumn{10}{l}{Commercial Clinical ASR APIs}\\
\hline
AWS [Medical] (Primary Care)        & -& -  & &   0.385 &    0.416 &  0.400 &   0.439 &    0.520 &  0.478 \\
GCP [Medical]        & - & - & &   0.550 &    0.475 &  0.512 &   0.567 &    0.537 &  0.552 \\
\hline
\multicolumn{10}{l}{Ours }\\
\hline
facebook/wav2vec2-large-xlsr-53-english-general & 317M & + AfriSpeech-general &  0.253 &   0.254 &    0.437 &  0.347 &   0.236 &    0.468 &  0.349 \\
facebook/wav2vec2-large-xlsr-53-english-clinical & 317M & + AfriSpeech-clinical  & 0.415 &   0.437 &    0.312 &  0.374 &   0.424 &    0.308 &  0.368 \\
facebook/wav2vec2-large-xlsr-53-english-all  & 317M & + AfriSpeech  &   0.314  &   0.295 &    0.308 &  0.302 &   0.279 &    0.308 &  0.293 \\
openai/whisper-medium-general       & 769M & + AfriSpeech-general  &  0.351          &   \textbf{0.205} &    0.486 &  0.347 &   \textbf{0.186} &    0.525 &  0.351 \\
openai/whisper-medium-clinical        & 769M & + AfriSpeech-clinical  &   0.568         &   0.491 &    0.264 &  0.376 &   0.464 &    0.266 &  0.368 \\
openai/whisper-medium-all       & 769M & + AfriSpeech  &       0.418           &   0.213 &    \textbf{0.241} &  \textbf{0.227} &   0.192 &    \textbf{0.242} &  \textbf{0.216} \\
%nvidia/stt-en-conformer-ctc-large-general & 118M & + AfriSpeech-general  &  &     - &      - &    - &   - &    - &  - \\
%nvidia/stt-en-conformer-ctc-large-clinical & 118M & + AfriSpeech-clinical  &  &     - &      - &    - &   - &    - &  - \\
%nvidia/stt-en-conformer-ctc-large-all & 118M  & + AfriSpeech &  &    - &      - &    - &   - &    - &  - \\

\bottomrule
\end{tabular}
\caption{Results showing selected models, number of parameters, Number of pre-training/fine-tuning corpora ["Multi" refers to multilingual or multi-task], Librispeech \citep{panayotov2015librispeech} test clean WER and AfriSpeech dev and test set performance for open-source, commercial ASR models, and fine-tuned models (Ours). Missing values indicate incomplete or failed experiments.}
\label{tab:models_benchmarks}
\end{table*}


\subsection{Fine-tuning}
Based on the benchmark results in Table \ref{tab:models_benchmarks} and GPU memory constraints, 2 top performing open-source model architectures were selected for fine-tuning. Although commercial ASR systems outperformed many open-source models, they are excluded from fine-tuning experiments because their model architectures and underlying pre/post-processing logic are unknown. 

\paragraph{Selected Model Architectures}
\begin{enumerate}[leftmargin=0.4cm, noitemsep]
    \item wav2vec-large-xlsr-53 \citep{grosman2021xlsr53-large-english}: an Encoder-decoder architecture with CNN-based feature extractor, code book, and transformer-based encoder, 378.9M parameters; LR 1e-4.
    \item whisper-medium \citep{radford2022robust}: a Decoder-only multi-task architecture, 789.9m parameters; LR 2.5e-4.
    %\item stt-en-conformer-ctc-large \citep{gulati2020conformer}: a convolution-augmented transformer with 118.8 M parameters; LR 1e-6.
\end{enumerate}
% 789,923,0233
% 378,923,0233 

For each model, we fine-tuned with FP16, AdamW \citep{loshchilov2017decoupled}, batch size of 16, for 10 epochs, with a linear learning rate decay to zero after a warmup over
the first 10\% of iterations. We fine-tune and evaluate on 3 domains: (1) \textbf{general} (25,812 clips), (2) \textbf{clinical} (41,765 clips), and (3) \textbf{both} (67,577 clips). We train on each domain and test across all 3 domains to investigate the effect of out-of-domain data on model performance. XLSR models were trained on a single Tesla T4 GPU with 16GB GPU memory while Whisper and Conformer models were trained on RTX8000 GPU with 48GB GPU memory. Fine-tuning took 24-48 hrs for all domains.

\subsection{Model Vocabulary}
\label{model_vocabulary}
 Most pre-trained models define a limited vocabulary of only Latin alphabets with no numbers or punctuations \citep{Baevski2020wav2vec2A}. In stark contrast, numbers are critical in healthcare, e.g. blood pressure 130/80mmHg, or Lab results 0.428 mmol/L. Eliminating all numerical references in clinical text is dangerous and counterproductive. Post-processing to convert all numerical values to long form is imperfect so we retain numbers in their original form. For fine-tuning experiments, we define an alphanumeric vocabulary with semantically important punctuations, characters, and symbols commonly used in medical practice (colon, question mark, plus, etc). 
 % Vocabulary details for selected models are provided in Appendix \ref{appendix:model_vocabulary}.

\subsection{Evaluation}
We report our results as WER on AfriSpeech dev and test sets in addition to domain and accent-specific performance. Results are compared with Librispeech \citep{panayotov2015librispeech} test set performance. We also report the zero-shot performance of fine-tuned models on unseen accents in the test set. 



\section{Results and Discussion}

\subsection{Africa-centric Fine-tuning Improves Robustness} 
As shown in Table \ref{tab:models_benchmarks}, compared with its pre-trained version, xlsr-53 fine-tuned on general domain speech (AfriSpeech-general) yields 53.4\% relative improvement. Xlsr-53 fine-tuned on clinical domain speech (AfriSpeech-clinical) yields 52.6\%, and xlsr-53 fine-tuned on the combined domains (AfriSpeech-all) yields 49.1\% relative improvement. The trend is similar with pre-trained Whisper-medium, yielding 32.6\% relative improvement on the general domain, 32.1\% on the clinical domain, and 34.9\% when finetuned on combined domains. %Improvements on Whisper are less pronounced given its robust pretraining. In-domain gains are general (32.6\%), clinical (32.1\%), all (34.9\%). 


\begin{table}
\tiny
\centering
\begin{tabular}{l|l|l|l|l|l|l}
\toprule 
Accent & Samples & \multicolumn{1}{c|}{OpnSrc} & \multicolumn{3}{c|}{Commercial} & \multicolumn{1}{c}{Ours} \\
   &  & Whisper & Azure & GCP & AWS & Whisper \\
\midrule
\multicolumn{7}{l}{Niger-Congo}\\
\hline
Ukwuani &      119 &                                                                                       0.364 &  0.393 &  0.677 &  0.484 &                                                                                            \textbf{0.244} \\
Eggon &      100 &                                                                                          0.254 &  0.316 &  0.616 &  0.359 &                                                                                            \textbf{0.122} \\
Bini          &     76 &                                                                                         0.830 &  0.840 &  0.916 &  1.061 &                                                                                           \textbf{0.412} \\
Yoruba, hausa &      75 &                                                                                       0.462 &  0.367 &  0.463 &  0.437 &                                                                                         \textbf{0.133} \\

Ekpeye        &    70 &                                                                  0.376 &  0.406 &  0.582 &  0.539 &                                                                                          \textbf{0.190} \\
Bajju         &    61 &                                                                                       0.229 &  0.323 &  0.428 &  0.378 &                                                                                        \textbf{0.171} \\
Ikulu         &    60 &                                                                                      0.406 &  0.388 &  0.650 &  0.543 &                                                                                        \textbf{0.195} \\
Jaba          &    59 &                                                                                   0.462 &  0.475 &  0.798 &  0.529 &                                                                                         \textbf{0.268} \\
Ekene         &    55 &                                                                                       0.414 &  0.350 &  0.673 &  0.519 &                                                                                         \textbf{0.192} \\
Agatu         &    54 &                                                                              0.734 &  0.725 &  0.903 &  0.793 &                                                                                      \textbf{0.387} \\
Ijaw(nembe)     &    49 &                                                                                       0.478 &  0.529 &  0.743 &  0.675 &                                                                                         \textbf{0.275} \\

Delta         &    48 &                                                                               0.384 &  0.351 &  0.724 &  0.473 &                                                                                       \textbf{0.205} \\
Igarra        &    45 &                                                                                        0.591 &  0.539 &  0.839 &  0.687 &                                                                                         \textbf{0.258} \\
Khana         &     45 &                                                                                        0.539 &  0.584 &  0.761 &  0.785 &                                                                                         \textbf{0.318} \\
Gbagyi        &    42 &                                                                                     0.327 &  0.461 &  0.633 &  0.475 &                                                                                        \textbf{0.195} \\
Jukun         &    42 &                                                                                    0.182 &  0.234 &  0.415 &  0.244 &                                                                                        \textbf{0.122} \\
Brass         &    39 &                                                                                        0.147 &  0.269 &  0.357 &  0.309 &                                                                                        \textbf{0.131} \\                           
\hline
\multicolumn{7}{l}{Afro-Asiatic}\\
\hline
Mada          &    78 &                                                                                     0.485 &  0.560 &  0.684 &  0.634 &                                                                                         \textbf{0.236} \\
Mwaghavul     &     67 &                                                                                      0.444 &  0.513 &  0.690 &  0.613 &                                                                                         \textbf{0.235} \\
Angas         &    58 &                                                                                       0.605 &  0.580 &  0.862 &  0.653 &                                                                                         \textbf{0.343} \\
\bottomrule
\end{tabular}
\caption{Zero shot (OOD) accents. Test set WER on top 20 accents absent from the training set for open-source (OpnSrc), commercial, and fine-tuned ASR models (Ours).}
\label{tab:OOD_accents_WER}
\end{table}

\subsection{Training Data Bias} In the Open-Source section of Table \ref{tab:models_benchmarks}, AfriSpeech dev and test set performance correlates with the number and diversity of pre-training datasets. For example, Wav2vec2 models trained exclusively on Librispeech significantly underperform when compared with those trained on multiple \citep{Baevski2020wav2vec2A} or multilingual corpora \citep{Babu2022XLSRSC}. Models trained on Multilingual or multi-task corpora \citep{radford2022robust, gulati2020conformer} learn more useful representations, are more linguistically diverse, are more robust, and generalize better to accented speech. %Conversely, models pre-trained on multiple corpora but extensively fine-tuned on single-task datasets like Switchboard \citep{godfrey1992switchboard} and LibriSpeech \citep{panayotov2015librispeech} generalize poorly to African accents. 

\begin{table*}
\tiny
\centering
\begin{tabular}{l|l|c|c|l|l|l|l|l|l|l|}
\toprule 
Accent & Country & Test Samples & Train Samples & \multicolumn{2}{c|}{\textbf{Open Source}} & \multicolumn{3}{c|}{\textbf{Commercial}} & \multicolumn{2}{c|}{\textbf{Ours, Finetuned}} \\
  &  &  & & xlsr-53 &  whisper & Azure & GCP & AWS & XLSR  & Whisper \\
\midrule
\multicolumn{10}{l}{Niger-Congo}\\
\hline
Yoruba & [NG] & 575 &14233& 0.576 &  0.327 & 0.364 & 0.581 & 0.421 & 0.291 &  \textbf{0.218} \\
Swahili & [KE, TZ, UG, ZA] & 485 &5484 &  0.448 & 0.192 & 0.307 &  0.436 &  0.305 & 0.244 & \textbf{0.181} \\
Igbo & [NG] & 319 &8068& 0.564 & 0.338 & 0.393 &  0.563 &  0.441 & 0.273 & \textbf{0.197} \\
Zulu &  [TR, LS, ZA] & 156 & 1309& 0.471 & \textbf{0.223} & 0.329 &  0.477 &  0.345 &   0.315 &   0.237 \\
Setswana &  [BW, ZA] & 96 &1275&  0.448 &  \textbf{0.208} & 0.288 &  0.446 &  0.300 & 0.291 &  0.234 \\
Isizulu  &  [ZA] & 88 & 779 &    0.457 & \textbf{0.182} &  0.254 &  0.406 &  0.292 & 0.265 &  0.206 \\
Ijaw  &  [NG] & 77 & 2371 &  0.608 & 0.364 & 0.372 &  0.671 &  0.446 & 0.321 &\textbf{0.238} \\
Luhya  &  [KE] & 69 & 426 &   0.538 &0.310 & 0.548 &  0.489 &  0.427 & 0.296 & 0.245 \\
Twi  & [GH] &  54 & 1321 &  0.504 & 0.184 & 0.382 &  0.510 &  0.361 & 0.236 & 
 \textbf{0.177} \\
Idoma  &  [NG] &  53 & 1767 & 0.607 &0.384 & 0.424 &  0.639 &  0.543 & 0.294 &  \textbf{0.243} \\
Luganda &  [KE, UG, BW] & 44 & 529 & 0.525 &0.320 & 0.362 &  0.526 &  0.378 &  0.381 & \textbf{0.277} \\
%Ebira &  [NG] &  39 & 696 & 0.499  &  0.256 &  0.353 &  0.536 &  0.412 &  0.243 &  \textbf{0.171} \\
Tswana       &  [BW, ZA] & 34 & 289 &  0.362 & \textbf{0.184} &   0.265 &  0.425 &  0.267 & 0.249 &  0.241 \\
Akan (fante) & [GH] &  29 & 230 &   0.732  &   0.418 & 0.425 &  0.803 &  0.604 & 0.290  &  \textbf{0.197} \\
Kikuyu &      [KE] &       24 &  163&  0.406 &     0.160 &  0.275 &  0.387 &  0.300 &   0.221 &  \textbf{0.126} \\
%Izon  &  [NG] &  18 & 783 &   0.607 &    0.431 &  0.483 &  0.662 &  0.524 &   0.355  &   \textbf{0.276} \\
Xhosa  &  [ZA] &  17 & 342&  0.498 &  0.265 &  0.322 &  0.332 &  0.389 &  0.318 & \textbf{0.237} \\
%Tshivenda   &  [ZA] &  17 & 334&     0.457 & 0.176&  0.320 &  0.391 &  0.246 &   0.246 & \textbf{0.156} \\
Sepedi  &   [ZA] & 17 &   176 &  0.651  &  0.373 &   0.394 &  0.659 &  0.458 & 0.414 & \textbf{0.285} \\
%Isixhosa &  [ZA] &  17 &  160&  0.463 & 0.224 &  0.286 &  0.392 &  0.361 & 0.224 &     \textbf{0.149} \\
Kiswahili  &  [KE] &  16 & 811 &   0.466 &     \textbf{0.159} &  0.389 &  0.394 &  0.274 & 0.173 &0.163 \\
Urhobo &    [NG] &   15 &578&    0.551 &  0.378 &    0.423 &  0.678 &  0.423 &  0.345 &    \textbf{0.210} \\
Nembe & [NG] &  14 & 546 &  0.571 &   0.352 &  0.449 &  0.556 &  0.449 &  0.372 & \textbf{0.296} \\
Kinyarwanda  &  [RW] &  14 &439 &  0.495 & \textbf{0.216} &  0.338 &  0.527 &  0.437 &  0.369 &0.311 \\
%Igala  & [NG] & 12 &906&   0.622 &0.486 &   0.439 &  0.615 &  0.541 & 0.351 & \textbf{0.257} \\
%Isindebele & [ZA] & 9 &  188& 0.293 & 0.143 & 0.241 &  0.211 &  0.180 & 0.173 & \textbf{0.098} \\
%Venda and xitsonga & [ZA] & 8 & 174& 0.616 & 0.233 & 0.479 &  0.712 &  0.644 & 0.164 & \textbf{0.096} \\

\hline
\multicolumn{10}{l}{Afro-Asiatic}\\
\hline
Hausa & [NG] & 168 &5453& 0.627 & 0.358 & 0.457 &  0.633 &  0.488 & 0.320 & \textbf{0.243} \\

\hline
\multicolumn{10}{l}{Indo-European}\\
\hline
Afrikaans    & [ZA] & 49 &1911& 0.373 & \textbf{0.142} & 0.202 &  0.443 &  0.209 & 0.283 & 0.211 \\
\hline
\multicolumn{10}{l}{Nilo-Saharan}\\
\hline
Luo &  [UG, KE] & 12 &    179& 0.411 & 0.234 &  \textbf{0.229} &  0.343 &  0.343 & 0.309 & 0.234 \\
\bottomrule
\end{tabular}
\caption{Test set performance per accent  for open-source, commercial, and fine-tuned ASR models.}
\label{tab:top_accents_WER}
\end{table*}


\subsection{Clinical ASR is Sensitive to Model Vocabulary}
As mentioned in Section \ref{model_vocabulary}, most ASR models tend to transcribe numbers in their extended forms, which have a detrimental effect on their WER as shown in Table~\ref{tab:models_benchmarks}, particularly in the clinical domain where numerical values need to be transcribed accurately (column 6 \& 9). However, ASR models with a larger vocabulary, such as Whisper, Commercial ASR models, and our fine-tuned models, demonstrate superior performance by effectively transcribing numbers in clinical speech and converting them into correct numeric representations.
% Since most pre-trained models define a limited vocabulary of only Latin alphabets with no numbers or punctuations, numbers in speech are transcribed in long-form. Since numeric values appear frequently in clinical text, e.g. blood pressure results (130/80mmHg), or Lab results (0.428 mmol/L) WER is negatively impacted. Models with larger vocabulary (e.g. Whisper, Commercial ASR, and Ours) thus perform better.

\subsection{Punctuation Prediction is Critical for Clinically Useful ASR} 
Medical documents typically follow preset sequence and formatting, for example, patient history, general examination, laboratory investigation, etc., separated by new lines, section titles, or semi-colons. Punctuation commands such as "Next line", "full stop" (.), "query" (?), "comma" (,), "colon" (:) are frequently used in healthcare dictations to add structure to documents. ASR systems without support for such commands force clinicians to review every line of the ASR transcript to add/revise punctuations and document structure, prolonging documentation time and patient wait time \citep{Sunkara2020}. As a result, commercial clinical ASR systems supporting these commands are preferable and outperform general-purpose models.

%Table \ref{tab:models_benchmarks} shows that commercial clinical ASR systems outperform their general-purpose variants as well as all open-source models besides Whisper and our AfriSpeech fine-tuned models.    Additional information can be found in Appendix \ref{appendix:punctuation}.
% These commands are supported by Commercial clinical ASR systems like GCP [Medical] and AWS Transcribe Medical. GCP wraps such commands in double square brackets and AWS transcribes them into punctuation characters. Other non-medical ASR models like Whisper, Conformer, and Wav2Vec, retain the long-form text command. We normalize all instances of such commands into punctuation characters in a post-processing step. 



\subsection{Commercial ASR APIs are Not So Global} 
The 3 large commercial ASR systems evaluated in this study have global presence. Millions of African Android users have access to Voice typing through the Google keyboard and Microsoft Word users have access to its ASR engine. Table \ref{tab:top_accents_WER} compares the performance of these ASR APIs on majority African accents and we show that despite their global presence, performance lags significantly on some of Africa's most populous accents like Swahili and Yoruba. %Table \ref{tab:models_benchmarks} results show Azure results are on par with Whisper-small suggesting Whisper-small may be the model under the hood due to the recent relationship between Microsoft and OpenAI. 


\subsection{Domain Adaptation} 
Pre-trained whisper models performed better on general domain speech (AfriSpeech-general) when compared with the clinical domain, demonstrating the relative domain-driven difference in difficulty despite the robust training data for Whisper models (680k hours, 90 languages). Cross-domain fine-tuning yields significant gains helping to somewhat bridge this gap. Our results agree with prior work on domain adaptation \citep{sun2017unsupervised, abdelwahab2015supervised} showing that models trained exclusively on clinical data improve when general domain data is added. Whisper shows 9\% relative improvement on the clinical domain with the addition of general domain data.  However, this trend is reversed with general domain data. Adding speech from the clinical domain leads to a 3\% and 18.2\% relative drop for Whisper and xlsr-53 respectively. Domain adaptation is no silver bullet. Care must be taken to apply this approach where benefits outweigh risks.

%Fine-tuning Whisper on combined domains yields the best performance (34.9\% relative improvement over baseline) on the combined domain test set. This observation emphasizes the importance of exercising caution with cross-domain training, ensuring that new data improves performance on the domain of interest without significant regressions in other domains.

%Finetuning Whisper with both general and clinical data (all domains) shows the best performance across all domains (103.64\%) when compared with finetuning only for the general or clinical domain (which in some cases underperforms the other domain i.e -7.04\%, -46.82\% respectively). 
%This interplay between the domain datasets used to train ASR models and their performance demonstrates the importance of training general-purpose ASR models with the appropriate domain datasets, otherwise, they risk improving in one domain while significantly underperforming in others.




\subsection{Accent-level Performance} 
\label{accent_level_performance}
Table \ref{tab:top_accents_WER} shows test set performance on the top 23 AfriSpeech accents grouped by their language families. We report the results for open-source, commercial, and fine-tuned ASR models. Fine-tuned models (ours) average relative improvement is 26.7\% over the open-source ASR models and 36.5\% over the commercial ASR models. For several accents, we observe that the whisper model fine-tuned with our AfriSpeech dataset shows the best overall performance with an average relative improvement of 16.2\% across all accents, except in 4 South African languages (Zulu, isiZulu\footnote{We note that both Zulu and isiZulu are the same but they are labeled differently in our dataset. We further discuss this in the limitations section.}, Tswana, Afrikaans), Luo, and Kinyarwanda, where the fine-tuned model under-performs compared to the pretrained whisper model and commercial Azure model performs best on Luo accent. Although counter-intuitive, it is possible these accents are highly represented in Whisper pre-training data and require further investigation. %The performance drop for these accents ranges from 6-49\%. This is counter-intuitive given that the accents are reasonably represented in the training corpus. This requires further investigation. 


\subsection{Zero-Shot Performance}
% \paragraph{Zero-Shot Performance on OOD accents}
% \paragraph{Fine-tuned Whisper model shows impressive result over other open source and commercial models on OOD accents:}
We further explore generalizability to unseen accents, i.e., out-of-distribution (OOD) accents. Table~\ref{tab:OOD_accents_WER} shows the results for the top 20 OOD accents in the test set. We observe an impressive 44.4\% relative performance improvement across all OOD accents with our fine-tuned Whisper model compared to the baselines and 49.8\% average relative improvement over the commercial models (Azure, GCP, AWS). These results demonstrate significant generalizability gains are achievable with better training data diversity.

%We investigated how well the fine-tuned models performed on unseen accents, i.e., out-of-distribution (OOD) accents. Table~\ref{tab:OOD_accents_WER} shows the results for the top 20 OOD accents in the test set. We report the results as described above. We observe an impressive performance improvement across all OOD accents with our fine-tuned Whisper model compared to the open source models (Whisper, Nemo, XLSR-53) with an average relative improvement of 44.4\% or the commercial models (Azure, GCP, AWS) with an average relative improvement of 49.8\%.


% \paragraph{Zero-Shot Performance} Add some text here from model inference on test set, compare pre-trained models to fine-tuned models on unseen accents in test set


\subsection{Take SOTA LibriSpeech Results with a Grain of Salt}  Figure \ref{appendix:wer_libri_afri} contrasts LibriSpeech and AfriSpeech WER for several models. Many ASR leaderboards rank ASR models based on single-digit LibriSpeech \citep{panayotov2015librispeech} WER. Pre-trained ASR models, therefore, overfit to LibriSpeech at the expense of robust ASR performance for all people. As seen in Table \ref{tab:models_benchmarks}, several models are 3-10x worse on African accented speech with the exception of multi-lingual or multi-task models like Whisper, Conformer, and XLSR. %For a fair comparison with fine-tuned models, our LibriSpeech-test-clean results are obtained using the same preprocessing and text normalization as fine-tuned models. This may explain the difference between ours and published benchmarks.

\begin{figure*}%[t]
\includegraphics[scale=0.35]{images/Librispeech-vs-Afrispeech.png} %width=8cm
\centering
\caption{WER on LibriSpeech vs AfriSpeech for selected pre-trained models and commercial ASR systems.}
\label{appendix:wer_libri_afri}
\end{figure*}

\section{Limitations and Future Work}


\paragraph{Limited clinical Subdomains:} Although this dataset includes a variety of clinical text, several specialties are not represented. As a result, ASR performance may vary between clinical specialties.

\paragraph{Read Speech:} All audio samples in this release are read based on text prompts. Without appropriate augmentation, ASR Models trained on this dataset may underperform with conversational or spontaneous speech. %Since most ASR systems are deployed in conversational settings \citep{godfrey1992switchboard}, we plan to expand this corpus to include spontaneous speech which should improve performance in more naturalistic settings. 

\paragraph{North-African Accents} are not included in this work. Because of the distinct nature of those accents, performance on sub-Saharan accents may not necessarily generalize to the Northern African Region.

\paragraph{Self-reported Accents:} Similar to Common-Voice, recorders self-report their native tongue in free-text making it difficult to map to ISO-3 in all cases. Some users also reported their accents as "French", "English", "South African English", or a combination of accents. Although we attempted to clean and normalize the self-reported languages, this process was by no means perfect. As a result, accent names sometimes overlap e.g. Zulu and IsiZulu. Further cleanup could be done to consolidate these closely related accents. The dataset release will therefore include a normalized accent field for each sample. %This is especially important for tasks where the accent data is used directly as a target variable, such as accent classification.


\paragraph{Medical Abbreviations are Inconsistent:} Since crowd-sourced recorders had varying levels of familiarity with the prompts, abbreviations like "Breast CA" may be pronounced fully as "Breast Cancer" or "Breast see-A". Since abbreviations abound in medical text and WER is not robust to such idiosyncrasies, models with correct predictions, e.g. "Breast Cancer" are sometimes wrongly penalized where the transcript reads "Breast CA".

\paragraph{Integrating ASR in Healthcare Settings is Challenging:}
 Cloud-based ASR presents some well-known challenges in healthcare. Privacy is a major concern as there is a risk of unauthorized or malicious third-party access to confidential patient information. Furthermore, the perceived higher value of healthcare data among malefactors also heightens security risks for hospitals and ASR vendors. Additionally, Unethical ASR vendors could misuse confidential data for model training and development without proper consent. 
%In addition to the technical aspects of ASR performance, it is essential to consider various factors when integrating ASR-enabled software into healthcare settings. Privacy, security, regulatory compliance, and other contextual considerations play a crucial role in the successful deployment of ASR systems in clinical environments. Addressing compliance and privacy concerns is of utmost importance in ensuring the secure and ethical use of ASR technology in healthcare. To achieve this, several measures can be implemented. For instance, standardized regulations on API services can be established to ensure the proper handling of sensitive patient data. Furthermore, the utilization of on-premise models or virtual private clouds can help keep the data within the confines of the hospital's data warehouse, minimizing the risk of unauthorized access. Additionally, strict access controls for data storage can be implemented to further safeguard patient privacy. It is worth emphasizing that these considerations are pivotal in fully leveraging the benefits of AI deployment in clinical settings. However, it is important to note that while we acknowledge the significance of compliance, privacy, and other contextual factors, our research in this work primarily focused on the technical aspects of ASR performance. Thus, we did not explicitly delve into addressing these challenges in our study.


%\paragraph{Web Scraping artifacts:} Spacing errors (about 2 to 3 per sentence) were found in about 20\% of prompts e.g., "thepatient came tothe hospital", due to web scraping, parsing, or pre-processing errors. These were found during a manual review of transcripts and most were corrected by a team of crowd-sourced workers. 


%\paragraph{Language Model:} To improve domain adaption to the clinical setting, acoustic models can be complemented with Clinical Language models that rescore ASR outputs \citep{gulati2020conformer}.

%\paragraph{Data Augmentation:} Augmentations like speed perturbation \citep{zhang22n_interspeech} have been shown to improve ASR performance with accented and spontaneous speech. Future work will include ablation studies demonstrating the effect of various augmentation methods on African-accented ASR.

\section{Ethical Considerations}

Clinical ASR models can improve productivity for clinicians, they can also increase documentation errors, especially through incorrect transcription of numbers, fractions, dates, and proper nouns which have legal, safety, and prognostic implications in healthcare. 
% A subtle change from 33 to 333 unnoticed in the health record can potentially cause harm to the patient or clinician if not found and corrected quickly.
We caution clinicians to use ASR with full discretion and review transcripts carefully before final submission into the medical record. We release AfriSpeech hoping that it will be beneficial to clinical and non-clinical use cases within and outside Africa, improving ASR performance for accented speech and it may contain biases due to publicly available datasets. 
% The transcripts are also curated using publicly available datasets that may carry biases. 
% Although we strive to perform analyses and probe the performance of our models, our investigations are by no means comprehensive nor guarantee the absence of bias in the data. 
% In particular, w
We do not have access to reviewers who are native speakers of most of the languages covered in AfriSpeech who can provide a rigorous review of self-reported accents. This hinders our ability to investigate samples from all languages. We hope that future users of the dataset will further investigate AfriSpeech's utility and quality for their languages.

\section*{Acknowledgments}
Tobi Olatunji acknowledges Intron Health for providing the dataset and compute resources. Chris Chinenye Emezue acknowledges the support of the Mila - Quebec AI Institute for compute resources. 


\bibliography{tacl2021}
\bibliographystyle{acl_natbib}

%\iftaclpubformat

%\onecolumn
%\fi

\appendix

\section{Appendix}

%\section{Author/Affiliation Options as set forth by MIT Press}
%\label{sec:authorformatting}

%Option 1. Author’s address is underneath each name, centered.

%\begin{quote}\centering
%  \begin{tabular}{c}
%    \textbf{First Author} \\
%    First Affiliation \\
%    First Address 1 \\
%    First Address 2 \\
%    \texttt{first.email@example.com}
%  \end{tabular}
%  \ 
%  \begin{tabular}{c}
%    \textbf{Second Author} \\
%    Second Affiliation \\
%    Second Address 1 \\
%    Second Address 2 \\
%    \texttt{second.email@example.com}
%  \end{tabular}

%  \begin{tabular}{c}
%    \textbf{Third Author} \\
%    Third Affiliation \\
%    Third Address 1 \\
%    Third Address 2 \\
%    \texttt{third.email@example.com}
%  \end{tabular}
%\end{quote}
  

%Option 2. Author’s address is linked with superscript characters to its name, author names are grouped, centered.

%\begin{quote}\centering
 %   \textbf{First Author$^\diamond$} \quad \textbf{Second Author$^\dagger$} \quad
  %  \textbf{Third Author$^\ddagger$}
%    \\ \ \\
%    $^\diamond$First Affiliation \\
%    First Address 1 \\
%    First Address 2 \\
%    \texttt{first.email@example.com}
%     \\ \ \\
%     $^\dagger$Second Affiliation \\
%    Second Address 1 \\
%    Second Address 2 \\
%    \texttt{second.email@example.com}
%     \\ \ \\
%    $^\ddagger$Third Affiliation \\
%    Third Address 1 \\
%    Third Address 2 \\
%    \texttt{third.email@example.com}
% \end{quote}
  
%\subsection{LibriSpeech vs AfriSpeech}

%\begin{figure*}[h]
%\includegraphics[scale=0.4]{images/LibriSpeech-vs-AfriSpeech3.png} %width=8cm
%\centering
%\caption{WER on LibriSpeech vs AfriSpeech for selected pretrained models and commercial ASR systems}
%\label{appendix:wer_libri_afri}
%\end{figure*}

\subsection{Transcript Preprocessing}\label{appendix:transcripts}

  \paragraph{Date and Time replacement:}  Dates are a critical part of clinical documentation as they typically contain several references to dates and times, for example, date of admission, date of discharge, time of death, and so on. Sampled subsets of sentences containing data and time references from the clinical and general domain were randomly replaced with random dates and times in different formats including "10/12/1999", "10th December, 1999", "10th Dec, 1999", "10-12-1999", "Mon 10 Dec, 1999", "Monday 10th December, 1999". Similar timestamp variations were added to our templates.
  \paragraph{Cleaning:}  Final corpus was pre-processed and cleaned by splitting on sentence boundaries, normalizing spaces, removing carriage return characters, removing non-alphanumeric characters except those with important structural or semantic meaning in the clinical domain such as question marks, parenthesis, colon, a hyphen, plus sign, and greater/lesser than sign. We removed Transcripts with less than 5 characters and greater than 300 characters.
  \paragraph{Privacy and Patient information:} Although the clinical corpora used were already anonymized, we re-examined several sentence samples for inadvertent exposure of patient names. Anonymized datasets with de-identification tokens like [NAME] and [DATE] were replaced with African names and randomly generated dates as described above.

\subsection{Annotation Instructions}\label{appendix:annotation}
Recorders were provided with the following instructions:

    \paragraph{Accuracy} It is very important that the recorded words match the text in the script exactly. If you accidentally deviate from the script, become unsure, or lose track of your thought, please delete and record the prompt again.
    \paragraph{Punctuations} All punctuations should be pronounced in full, not just observed. That is, when reading a text sample that contains punctuation, you say "comma", "full stop", "semi-colon", "colon", "slash", "hyphen", "question mark", "exclamation mark", and so on as appropriate. Brackets should be pronounced as "open bracket" or "close bracket".
    \paragraph{Punctuation Exclusions/Exceptions}  to the above rule: In measurements or units like "mg/dl", please say "milligram PER dl" NOT "milligram slash dl". In situations where "?" is used to represent "query", please say "query" NOT "question mark".
    \paragraph{Abbreviations} Pronounce common short-hand forms (such as r/o, prn, tds, PO, mg, W/O), dates, times, and numbers as you would in a clinical setting. For example, "r/o" should be pronounced "rule out" as usual not "arr slash ohh".  Common Abbreviations SHOULD be pronounced in full. "CT" should be pronounced "see tee" as usual NOT "Computed Tomography". "CXR" should be pronounced "Chest Xray" as usual NOT "see ex arr" "mmHg" should be pronounced in full as "millimeters of mercury". Pronounce CA as "Carcinoma" NOT "See Ay".% Where abbreviations are not known or uncommon, pronounce the letters as seen on the screen. Resist the urge to make up full meanings for abbreviations where you are unsure.
    \paragraph{Tone} Also be sure to use your natural accent. The goal is to build a speech-to-text system that understands African accents. This tool is for us. Be natural.
    \paragraph{Speed} Do not speak unrealistically fast. While an increased reading speed is recommended, take care to avoid vocal fatigue from rushing through the phrases at lightning speed! This will only result in a lower-quality voice. Record a maximum of 2 hours a day, taking a break every half hour.
    
\subsection{Annotator Management}

\paragraph{Consent} Recorders signed a Terms of Use agreement and consented to the privacy policy on the recording platform.

\paragraph{Payment}Recorders were paid \$5 to \$10 per hour depending on task difficulty and clinical experience. Most recorders considered payment satisfactory compared with task difficulty.


\subsection{AfriSpeech Vocabulary}\label{appendix:vocabulary}
AfriSpeech models use a 50-character vocab including numbers and punctuations and symbols with important semantic roles in healthcare.

"-", "w", "a", "7", ",", "0", "d", "i", ":", "p", "g", "u", "(", "5", "1", "e", "9", "j", "b", "3", "s", "'", "h", "o", "+", "l", "v", "y", "q", "n", "2", "r", "f", "m", "\%", "t", "/", "6", "z", "?", "8", ")", "x", ".", "4", "c", "k", "|", "[UNK]", "[PAD]"

\end{document}


