\section{Human study: Fine-Grained Response Taxonomy}
\label{sec:userstudy}

We first perform an exploratory annotation study to better understand the errors made by dialogue models.
A manual inspection of 200 of the responses generated by \textsc{GPT2} trained on \textsc{WoW} revealed that, in addition to responses that were attributable to the document as desired, there were five common error types:
\textbf{Hallucination} \cite{dziri-etal-2021-neural, dziri2022faithdial, rashkin-etal-2021-increasing, thoppilan2022lamda}: Hallucinated responses state information that cannot be verified using the source document. \textbf{Contradiction} \cite{dziri2019evaluating, welleck-etal-2019-dialogue, nie-etal-2021-like}: These are informative sentences that directly contradict the source document. \textbf{Off-topic}: In contrast to hallucinations, which are
semantically close to the evidence, off-topic responses are unrelated to the evidence.  \textbf{Generic}: These are neutral sentences that are general enough to fit into a large number of possible contexts  \cite{li2016diversity}. Even if they are ostensibly about the same topic as the document, they are vague and do not provide information. 


We choose to focus only on \textsc{WoW} and use output from GPT2 and T5 models, annotating a collected total of 8k triples from the ``unseen'' portion of the test set.   We filter out machine-generated responses that the Google Perspective API\footnote{\url{www.perspectiveapi.com/}} deemed to have a  greater than $50\%$ likelihood of containing toxic language.

\paragraph{Annotation Pipeline}
For each dialogue sample, we crowd-source human judgment by soliciting evaluations from annotators from a proprietary, high-quality crowd-sourcing pool under the guidance and
supervision of a project manager, with three annotators assigned to each example.
Rather than asking raters to explicitly classify responses based on the proposed taxonomy, we broke down the task into hierarchical questions with Likert scales (from~1 to~5). We provide the exact questions in \S\ref{app:annotators}. Because each sentence in the response may display different degrees of attribution, we ask annotators to rate each sentence separately. The average Krippendorff's alpha on the Likert responses was around $0.41$. 
Based on this human study, we make the following key observations: 

\begin{remark}
\label{obs:one}
Hallucination (32.2\%), attributable (33.6\%) and generic (27.5\%) categories cover the largest portion of responses, whereas contradictions (0.6\%) and off-topic (6.1\%) make up a small fraction of the distribution. This suggests that LM-based dialogue agents like \textsc{GPT2} and \textsc{T5} are more likely to add extra confabulated information than to directly contradict the evidence, and are also more likely to stick to the topic than to add something completely irrelevant to the evidence. 
\end{remark} 

\begin{remark}
\label{obs:two}
 Categories are not mutually exclusive. In particular, off-topic and contradiction responses present information that cannot be attributed to the source document, and therefore they naturally share common properties with the %
 hallucination category. In one example, the response \emph{``Oppenheimer as he is known as I think in neonatal med / ophthalmology"} was generated about a document that says \emph{``He was the Director of Pediatric Neurosurgery at Johns Hopkins Hospital in Maryland from 1984 until his retirement in 2013"}. Because the pronoun \emph{he} in the document does not resolve to an antecedent, it is hard to determine whether this utterance is better described as a hallucination (attributing a medical specialty not mentioned in the document) or off-topic (this document was probably not about Oppenheimer at all), which is what the rater ultimately selected. 


\end{remark}

\begin{remark}
\label{obs:three}
 The inter-annotator agreement denotes relatively low-to-moderate agreement. One factor that may impact the scoring is disagreements between Likert scores and cases where a disagreement in an earlier question propagated to the follow-up questions. Further, the low score can be caused by the fine-grained categorization that introduces some ambiguities that are hard to resolve in some cases.  
\end{remark}

Given these observations, we collect \BEGIN{} with three goals in mind: First, alongside strong general-purpose dialogue LMs, we include dialogue models that are designed specifically to improve attribution in the generated responses. Second, we train models on two additional knowledge-grounded dialogue benchmarks to have a diverse set of responses from different domains. Third, we consolidate some of the overlapping classes---specifically, hallucinations, contradictions, and off-topic into a single ``unattributable" category to avoid ambiguities in annotations. We detail our data collection procedure below.