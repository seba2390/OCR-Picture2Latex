\appendix

{

}




\section{\textsc{Begin} Annotation Protocol} 
\label{sec:annotationprotocol}
Each worker was given a document, previous turn in a conversation and a generated response (either by \textsc{T5}, \textsc{GPT2}, \textsc{DoHA} or \CTRL{}).  They were asked to evaluate the response as either fully attributable, not attributable, or too generic to be informative. They also were provided with multiple examples with explanations for each category. The exact instructions were as follows:
\begin{mdframed}[leftmargin=0pt,rightmargin=0pt]
\small
Which of these best describes the highlighted utterance?
\begin{itemize}
    \item[$\circ$] {Generic: This utterance is uninformative (too bland or not specific enough to be sharing any new information) }
    \item[$\circ$] {Contains \emph{any} unsupported Information:
This utterance is sharing information that cannot be fully verified by the document.  It may include  false information, unverifiable information, and personal stories/opinions.}
    \item[$\circ$] {\emph{All} information is \emph{fully} supported by the document: This utterance contains only information that is fully supported by the document.}
\end{itemize}
\end{mdframed}



\section{Implementations}
\label{sec:hyperparam}

\paragraph{GPT2, T5} We implement these models using the TensorFlow Huggingface Transformers library \cite{wolf-etal-2020-transformers}. During training, we use the Adam optimizer \cite{DBLP:journals/corr/KingmaB14} with Dropout \cite{srivastava2014dropout} on a batch size of $32$ with a learning rate of $6.25 \times 10^{-5}$ that is linearly decayed. The maximum dialogue history length is set to $3$ utterances. The model early-stops at epoch \{6, 10, 10\} respectively for \textsc{WoW}, \textsc{CMU-DoG} and \textsc{TopicalChat}.

\paragraph{\CTRL{}} We reproduce the results from \cite{rashkin-etal-2021-increasing}, following the training details in that paper.

\paragraph{DoHA} We use the code and the pre-trained model on \textsc{CMU-DoG} that are publicly available by the authors at their Github's account \footnote{\url{https://bit.ly/3bBup2M}}. For \textsc{WoW} and \textsc{TopicalChat}, we follow closely the authors' training procedure described in \citep{prabhumoye-etal-2021-focused} and we train two models on both datasets.  


For each dataset, we save the best model based on the validation set. We use nucleus sampling with $p=0.9$. 


\section{Model-Based Metrics}
\label{app:implementation-details-metrics}




\paragraph{Semantic Similarity Models}
We use BERTScore version 0.3.11. with the
DeBERTa-xl-MNLI model \cite{he2020deberta}, which is the recommended model as of the time of investigation. For BLEURT, We use the recommended
BLEURT-20 checkpoint \cite{pu-etal-2021-learning}. For BARTScore, we use the latest publicly available checkpoint (accessed March 2022) from \url{https://github.com/neulab/BARTScore}. 




