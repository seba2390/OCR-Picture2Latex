\section{Introduction}




Neural language models \citep[][\textit{inter alia}]{bengio2000neural, vaswani2017attention, radford2019language} often form the backbone of open-ended dialogue systems \cite{wolf2019transfertransfo, zhang2019dialogpt, roller-etal-2021-recipes, adiwardana2020towards}. Utterances sampled from such language models sound natural, as reflected in these systems' high scores in human evaluations focused on measures such as ``engagingness'' or ``human-likeness'' \cite{see-etal-2019-makes}. While fluent, however, the responses generated by these systems often contain statements that are not supported by the evidence available to the system; such statements are sometimes referred to informally as ``hallucinations'' (\citealt{tian2020sticking, maynez-etal-2020-faithfulness, dziri-etal-2021-neural, shuster-etal-2021-retrieval-augmentation}; see Figure~\ref{fig:hallucination} for an example). This issue is particularly salient for knowledge-grounded dialogue systems, which are expected to interact with a user in an open-ended fashion while conveying information that is \textit{attributable} to external identifiable sources.
In this work, we develop a benchmark that can be used to assess attribution in knowledge-based dialog systems; following \citet{rashkin2021measuring}, we define
an attributable response\footnote{Attribution is sometimes referred to as faithfulness \citep[\textit{inter alia}]{cao-etal-2018,durmus-etal-2020-feqa}.} as one connected to textual evidence that supports the entirety of the response. 


A number of modelling approaches have recently been proposed to increase attribution in knowledge-grounded dialog systems \cite{ rashkin-etal-2021-increasing, shuster-etal-2021-retrieval-augmentation, dziri-etal-2021-neural, dziri2022faithdial}. Progress in this area crucially relies on metrics that can measure the attribution of the text generated by the system; and indeed, recent work  has developed automated metrics with relatively high correlations with human annotations, potentially paving the way for alternatives to expensive human evaluations \cite{honovich-etal-2021-q2, dziri-etal-2021-neural, dziri2022faithdial}. Yet our understanding of these recently proposed metrics, as well as more established ones, remains limited, for two reasons.
First, comparisons between automated metrics and human judgments rely on small-scale datasets with a few hundred examples. This results in high variance in our estimate of the correlation coefficient and a limited ability to measure performance on infrequent example types \cite{gehrmann2021gem}.
  \begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{figures/hallu_exp1.png}
\caption{An example of a response generated by the GPT2 language model fine-tuned on the Wizard of Wikipedia dataset \cite{dinan2018wizard}. The phrases in red are ``hallucinations'' unsupported by the background document. \label{fig:hallucination}}
\end{figure}
Second, the correlation with human scores does not sufficiently determine the efficacy and robustness of automatic metrics produced by neural networks: such learned metrics---like other properties learned by neural networks---can be susceptible to spurious correlations that fail to generalize to more challenging cases.
To address these limitations, we introduce a large-scale resource, the Benchmark for Evaluation of Grounded INteraction (\begindata),  for meta-evaluation of metrics designed to evaluate grounded dialogue. In other words, the goal of this benchmark is to determine to what extent current evaluation metrics fulfill their purpose.



We define a taxonomy dividing knowledge-grounded dialogue responses into three broad categories---\textit{fully attributable}, \textit{not fully attributable}, and \textit{generic}---and ask humans to classify a large set of utterances produced by dialogue systems with this taxonomy. The motivation for the \textit{generic} category we introduce---which is assigned to utterances such as ``\textit{Sorry, I'm not sure about this topic}''---is the intuition that evaluation metrics should not treat the basic elements of a natural-sounding conversation, such as backchanneling or acknowledgment \cite{grice1989studies, stiles1992describing, bunt-etal-2020-iso}, as equally undesirable as a misleading unattributable statement. In real-world scenarios, it is  preferable for a model to acknowledge its ignorance instead of producing hallucinated content which may lead to the spread of disinformation.
 
Using this taxonomy, we then collect high-quality human annotations for 12k examples generated by four language-model-based dialogue systems, each trained on three different knowledge-grounded dialogue corpora. Examples of machine-generated responses along with labels are presented in Table \ref{tab:benchmark:wow_cmu_begin_dist}. 
We use this benchmark to evaluate multiple existing automatic metrics including word-overlap measures, embedding-based measures, metrics based on Question Answering (QA) systems, and ones based on Natural Language Inference (NLI). We also propose a classifier  trained on an adversarially generated dataset we create. We find that all metrics inadequately measure attribution and all rely on spurious correlations to a large extent. In particular, the metrics tend to misidentify cases that are attributable but highly abstractive, as well as cases that are not fully attributable but use multiple words from the evidence document (i.e., unattributable but extractive). We also find that the metrics fail to measure attribution under distribution shift, scoring responses that pertain to relatively long knowledge sources the lowest. These results are in line with the robustness issues reported for other natural language generation
metrics, despite the high  correlation of those metrics with human judgments \cite{durmus-etal-2022-spurious, gehrmann2021gem, gabriel-etal-2021-go, yeh-etal-2021-comprehensive}.
We hope that \BEGIN{} will facilitate progress toward more robust metrics for grounded dialogue response generation. 
















