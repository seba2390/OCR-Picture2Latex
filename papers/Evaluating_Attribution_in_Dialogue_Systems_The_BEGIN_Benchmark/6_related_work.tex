\section{Related Work}

\paragraph{Analysis of Evaluation Metrics in Natural Language Generation}
There is extensive interest in analyzing and meta-evaluating neural language generation (NLG) evaluation metrics \cite{gehrmann2022repairing, gehrmann2021gem}, for various tasks including machine translation \cite{freitag2021experts, mathur-etal-2020-tangled}, data-to-text generation \cite{dhingra2019handling}, summarization \cite{bhandari-etal-2020-evaluating,pagnoni-etal-2021-understanding,durmus-etal-2020-feqa, gabriel-etal-2021-go, fabbri2021summeval, durmus-etal-2022-spurious}, and dialogue generation \cite{yeh-etal-2021-comprehensive, durmus-etal-2022-spurious}.  Most of these studies have compared reference-free and reference-based evaluation metrics to human evaluation. 
For example, 
\citet{gabriel-etal-2021-go} measured the performance of automated metrics on summaries and compared certain dimensions such as sensitivity and high correlation with human scores. \citet{fabbri2021summeval}  analyzed metrics in summarization and released human-annotated data for faithfulness across 16 summarization models. We perform a similar meta-evaluation of existing automatic metrics in the context attribution in knowledge-grounded responses. Closest to our work is \citet{durmus-etal-2022-spurious}, who found that reference-free evaluation metrics of summarization and dialogue generation rely heavily on spurious correlations such as perplexity and length. 


\paragraph{Metrics in Knowledge-Grounded Response Generation}
In contrast to the significant progress achieved in evaluating many NLG tasks, the evaluation of grounded response generation is a nascent research area \cite{shuster-etal-2021-retrieval-augmentation, rashkin2021measuring, dziri-etal-2021-neural}.  \citet{yeh-etal-2021-comprehensive} conducted a comprehensive study of existing dialog evaluation metrics. They measured properties such as engagingness and relevance but did not investigate the faithfulness of responses. While hallucination is well-studied in the context of summarization \cite{durmus-etal-2020-feqa,maynez2020faithfulness,nan-etal-2021,falke-etal-2019-ranking}, fewer researchers have looked into the problem of assessing hallucination in dialogue systems. 
\citet{dziri-etal-2021-neural} introduced a token-level critic that leverages a knowledge graph to identify hallucinated dialogue responses. 
\citet{rashkin2021measuring} proposed a human evaluation framework to assess output of dialogue models that pertains to the external world and utilized their evaluation framework for conversational QA tasks. \citet{dziri2022faithdial} introduced a faithful benchmark for information-seeking dialogues and demonstrated that it can serve as training signal for a hallucination critic, which discriminates whether an utterance is faithful or not.
An alternative approach for assessing faithfulness uses an auxiliary language understanding task, which measures whether a question answering system produces the same responses for the source document \cite{honovich-etal-2021-q2}. \BEGIN{} as a testing benchmark should be useful in developing similar metrics further.

 \paragraph{NLI and Adversarial Data for Grounded Dialogue Evaluation}
 In this work, we also investigate the performance of classifiers trained on NLI data, extending prior work that has proposed using NLI as a framework for evaluating conversational consistency \cite{welleck2018dialogue}. 
 \newcite{dziri2019evaluating} also used NLI to evaluate dialogue consistency. They generated a large-scale, noisy synthetic dataset of (premise, hypothesis) pairs tailored for dialogue, based on \newcite{Zhang2018Personalizing}.  
 We also explore training classifiers on adversarially augmented training data similar to concurrent work from  \newcite{gupta-etal-2022-dialfact} and \newcite{kryscinski2020evaluating}, which  proposed a synthetic dataset for determining whether a summary or response is consistent with the source document; this dataset was constructed by applying a number of syntactic transformations to reference documents (for a similar approach applied to NLI, see \citealt{min-etal-2020-syntactic}).













