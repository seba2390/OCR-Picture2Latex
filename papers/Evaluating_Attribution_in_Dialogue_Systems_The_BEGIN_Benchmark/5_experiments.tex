\section{Evaluating Evaluation Metrics}
We next use \begindata \ to evaluate a range of evaluation metrics. In \S\ref{sec:metrics} we list the untrained metrics we use as well as metrics trained on existing resources, and in \S\ref{sec:adv} we describe a training set that we designed to train a classifier for the three response categories. We then describe the extent to which these metrics align with the \begindata{} categories and analyze the metrics' robustness. 

\subsection{Metrics}
\label{sec:metrics}

\paragraph{Lexical Overlap Metrics} This category includes $n$-gram-based metrics that compare the lexical similarity between the response $\bar{u}_{n}$ and the knowledge $\mathcal{K}$.\footnote{Note that we do not compare the generated responses to the gold responses as they may be unattributable (Sec \ref{sec4:data_analysis}).} We consider BLEU-4\footnote{\url{https://github.com/mjpost/sacrebleu}} \cite{papineni2002bleu}, ROUGE-L\footnote{\url{https://github.com/google-research/google-research/tree/master/rouge}} \cite{lin-2004-rouge}, and F1, which measures the word-level lexical overlap between $\bar{u}_{n}$ and  $\mathcal{K}$.

\paragraph{Semantic Similarity Metrics} These metrics compare the \textit{semantic} similarity between $\bar{u}_{n}$ and $\mathcal{K}$. We consider BERTScore \cite{Zhang*2020BERTScore:}, which computes the similarity between $\bar{u}_{n}$ and $\mathcal{K}$ based on the cosine similarity of the sentence embeddings, as well as BARTScore \cite{NEURIPS2021_e4d2b6e6} %
and BLEURT \cite{sellam2020bleurt}; %
for implementation details, see Appendix~\ref{app:implementation-details-metrics}. 

\paragraph{Question-Based Metrics} We use Q$^2$ \cite{honovich-etal-2021-q2}, which computes a factuality score through asking and answering questions. Given a candidate response as input, Q$^2$ generates a corresponding question and identifies potential answer spans in the knowledge source $\mathcal{K}$ that can justify the question--answer pair \cite{durmus-etal-2020-feqa, wang2020asking}. It also computes an NLI-inspired similarity score between a candidate response and a predicted answer span in the knowledge source.



\paragraph{Inference-Based Metrics} 
Finally, we study the performance of NLI-based models, trained either on gold NLI benchmarks or on adversarially augmented silver data that we generate. We first describe the metrics trained on gold NLI datasets; we discuss our adversarially augmented dataset (\textsc{BEGIN-Adversarial}) in \S\ref{sec:adv}.
We use two transformer-based classifiers: T5-base \cite{raffel2020exploring} and RoBERTa-large \cite{liu2019roberta}.  We fine-tune them on MNLI \cite{williams-etal-2018-broad} and the dialogue inference dataset DNLI  \cite{welleck-etal-2019-dialogue}. For both datasets, we map the labels (entailment, contradiction, neutral) to the labels (attributable, unattributable, generic) in \begindata.

We also train classifiers on AugWow \cite{gupta-etal-2022-dialfact}, a synthetic dataset designed to evaluate factuality in dialogue systems. 
This dataset includes three categories: \textit{Supported} responses that are fully verified by $\mathcal{K}$, \textit{Refuted} responses that explicitly contradict $\mathcal{K}$, and responses with \textit{Not Enough Information} (NEI), which do not contain enough information to be verified or refuted by $\mathcal{K}$.  We map the labels (supported, refuted, NEI) to the labels (attributable, unattributable, generic) in \begindata.
 









\subsection{Adversarially Augmented Training Set}
\label{sec:adv}

\begin{figure*}[ht]
\centering
\includegraphics[width=2.1\columnwidth]{figures/new-plots/boxplots-lexical-semantic-no-spectrum.pdf}
\caption{\small The distribution of scores assigned by semantic similarity metrics (upper row) and lexical overlap scores metrics (lower row) to the \begindata{} test set. }
  \vspace{-15pt}
  \label{boxplot_lexical_semantic}
\end{figure*}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\columnwidth]{figures/new-plots/q2-plot.pdf}
\caption{\small The distribution of Q$^2$ scores for each of the three example categories in the \begindata{} test set. }
\label{q2_boxlot}
\end{figure}





This section describes our curated silver training set (\textsc{BEGIN-Adversarial}) for NLI-based attribution classifiers. This dataset includes 8k ($\mathcal{K}$, $\mathcal{H}$, $u_p$) triples that fit into the three categories: attributable, generic, and unattributable.

\paragraph{Attributable} Here we use the original human generated responses $u_g$ from \textsc{WoW}. To avoid human responses that contain opinions or generic chit-chat, we only use response that do not use first-person pronouns 
and where at least 25\% of the words in the response are contained in the evidence.

\paragraph{Unattributable}
To generate examples that are likely to be unattributable, but  are sufficiently challenging to distinguish from attributable ones as to be useful in training a classifier, we use multiple perturbation strategies.  First, we directly perturb the knowledge spans $\mathcal{K}$ from the \textsc{WoW} test set and then feed them to \textsc{GPT2} trained on \textsc{WoW}. We use three perturbation methods, each applied to a different  $\mathcal{K}$. First, we 
swap the subject and the object of  $\mathcal{K}$. Second, we replace up to two verbs with verbs of the same tense. Finally, we 
extract all mentioned entities from different dialogue examples using the SpaCy NER tagger \cite{honnibal2017spacy}, and replace up to two randomly chosen entities in the original $\mathcal{K}$ with entities of the same type. Manual inspection reveals that this usually results in responses that are hallucinations with respect to the original $\mathcal{K}$.

We also generate responses designed to specifically contradict $\mathcal{K}$, using two techniques. First, we directly negate the human response $u_g$ from \textsc{WoW} using the English Resource Grammar parser (ERG; \citealt{Fli:Ben:Oep:14}). 
Second, we replace adjectives in $u_g$ with their WordNet antonyms \cite{miller1998wordnet}. 

Lastly, we gather responses that are off-topic with respect to the information in the $\mathcal{K}$. For a given context, we randomly select a \textsc{WoW} gold response that was based on different  $\mathcal{K}$. To avoid easy-to-detect off-topic responses, we sample from conversations that were prompted by the same initial topic word as the target conversation.

\paragraph{Generic}
 Generic responses are generated from the \textsc{GPT2} model we trained on \textsc{WoW}, using a low softmax temperature of 0.4. 















 \subsection{Results}
In this section, we report the performance of automatic metrics on the \begindata{} test set.

 \paragraph{Lexical and Semantic Metrics}
The distribution of scores is shown in Figure~\ref{boxplot_lexical_semantic}. For all metrics, the median score of fully attributable responses is higher than that of generic and  unattributable responses, as expected. In many individual cases, however, unattributable responses are scored quite highly, and there is some overlap in the distribution of scores across all three labels, particularly between generic and unattributable responses, indicating that it would be impossible to map these score ranges directly to the \begindata\ label taxonomy.
Higher scores do not always translate into more desirable response types: Even though a generic response would typically be preferable to an unattributable one in a knowledge-grounded dialogue system, the median scores are lower for generic responses than unattributable ones.

 
 \paragraph{Q$^2$}  Figure~\ref{q2_boxlot} shows a box plot for each \begindata{} class using the Q$^2$  metric. As in the case of the lexical and semantic metrics, Q$^2$ scores are typically higher for attributable responses but indistinguishable between generic and unattributable responses.  
 
\paragraph{Inference-Based Classifiers}

 Table \ref{tab:results} reports the performance of the NLI-based classifiers on \begindata{}. \textsc{BEGIN-Adversarial} substantially outperforms the classifiers trained on the gold datasets MNLI, DNLI and AugWoW even though it is a significantly smaller resource than those datasets. We also use MNLI as an intermediate fine-tuning dataset before fine-tuning on \textsc{BEGIN-Adversarial}.\footnote{We did not observe a similar improvement when using DNLI as an intermediate task.} 
We find that intermediate task fine-tuning can be beneficial when RoBERTa is used as the pretrained model ($\uparrow4.1$ on F1). 

Overall, our adversarially generated dataset provides better supervision for detecting our taxonomy than NLI-style datasets. 
This can be attributed to the fact that  NLI-style datasets are designed with a focus on detecting direct contradictions. 
By contrast, identifying unattributable responses requires detecting multiple types of unverifiable information including, but not limited to, contradictions. %
At the same time, none of the models exceed $46\%$ F1 score, showing that there is still room for improvement compared to human performance (over $95\%$ precision when comparing human annotations to the majority vote). Finally, \textsc{T5} and RoBERTa have similar F1 scores despite differences in model size and pretraining corpora, suggesting that simply scaling up the pretrained model may not be sufficient to make progress on this problem. 
 

 
 \subsection{Are Metrics Measuring Attribution or Extractivity?} 
Do the metrics perform similarly on both challenging and easier examples? We adopt a density metric from \citet{grusky2018newsroom} to split the data into three groups---low, medium and high density---based on the extent to which they reuse language from the knowledge sources. Density represents the average length of the text spans in the responses that are copied from the knowledge. Extractive (high density) responses reuse the same phrases as the knowledge source, while abstractive (low density) responses may express the same meaning using a paraphrase. 

\begin{table}[ht]
    \scriptsize
    \centering
    \begin{tabular}{llccccc}
    \toprule
&\multicolumn{3}{c}{\bf{    \scriptsize Test set}}&	\multicolumn{3}{c}{\bf{Dev set}} \\
   \cmidrule(lr){2-4} \cmidrule(lr){5-7} 
\scriptsize{Finetuning data}& \bf P& \bf R & \bf F1	& \bf \bf P& \bf R& \bf  F1
\\\midrule
 \textbf{\textsc{T5}}  \vspace{0.15cm}\\

 MNLI		& 48.6 & 47.9	& 34.6 & 52.1 & 50.7 & 37.4 \\
DNLI  &	40.8 &	56.5 &	25.6  &  41.6 & 59.2 & 28.6 \\
 AugWow & 36.8 & 39.8 & 37.8  & 36.7 & 39.9	& 38.1  \\
 BEGIN-Adv. & 46.7 & 47.4 &{\bf 45.9} & 47.2 & 47.1 & {\bf46.3}\\
  { \; +MNLI} &	46.9 &	49.3 & { 45.3 }& 47.6& 49.4  & 46.1 \\
\midrule
\textbf{\textsc{RoBERTa}}  \vspace{0.15cm} \\
MNLI &  50.5 & 51.1	& 36.4 & 52.3&53.8 & 38.5\\
DNLI &	40.2 & 46.6 & 27.2&34.9 &  46.1 & 29.2\\
AugWow & 41.2 & 39.2 & 29.7 & 29.4  &	41.4&	29.1 \\
 BEGIN-Adv. & 42.6& {46.1} & 41.1& 49.2&	45.8&	41.1 \\
{ \; +MNLI}	& 44.8&	{45.9}&	{\bf45.2}	& 44.9	& 45.6	&{\bf45.1}\\
\midrule
\bf Human & 96.4&- & -&97.2&- & -\\
				\bottomrule		
    \end{tabular}
    \caption{Precision, recall and F1 of the classifier-based metrics created by fine-tuning T5 and \textsc{RoBERTa} on NLI datasets, AugWow and our adversarial training set. Scores are macro-averaged across labels on the \begindata{} test and dev sets.}
    \label{tab:results}
    \vspace{-5pt}
\end{table}

\paragraph{Results} Figures~\ref{fig:metrics_density} and~\ref{fig:q2_density} show the distributions across different levels of extractivity of the lexical and semantic metrics and the Q$^2$ score. 
We observe a common pattern across all metrics: high density responses for all categories (except \textit{generic} on BLEURT) score the highest, followed by medium density and low density responses.
The differences between the scores of the attributable, generic and unattributable categories are more pronounced in the more extractive responses, and less in the abstractive cases. Only Q$^2$, though generally unable to separate generic examples, maintains a clear separation between attributable and unattributable examples in the abstractive cases. Moreover, extractivity strongly influences the score assigned to attributable examples; an attributable response is likely to be scored much lower by all of these metrics if it is abstractive. Even more strikingly, unattributable extractive responses score higher on average than attributable abstractive responses in all metrics. 

We observe similar trends for the classifiers (Figure~\ref{fig:f1scoresbydensity}). The performance on  classifying attributable responses is much higher in extractive cases than in abstractive ones.  In contrast, the performance on unattributable responses is typically worse in the extractive cases. This pattern of results suggests that a response that is unattributable but has a high word overlap with the knowledge is very likely to be misclassified as attributable.  
In summary, we find that current metrics are relying on the spurious correlation between attribution and word overlap, and do not capture a deep understanding of the notion of attribution (cf. \citealt{mccoy-etal-2019-right}).

 
\begin{figure*}[ht]
\centering
\includegraphics[width=2\columnwidth]{figures/new-plots/boxplots-test-all-overlap-metrics.pdf}
\caption{\small Scores assigned to each of the three \begindata{} categories by semantic similarity metrics (upper row) and lexical overlap metrics (lower row), broken down by extractivity of the response (the extent to which it copies verbatim from the knowledge).}
\label{fig:metrics_density}
  \vspace{-15pt}
\end{figure*}


\begin{figure}[ht]
\centering
\includegraphics[width=1\columnwidth]{figures/new-plots/boxplots-test-q2-spectrum-label-down.pdf}
 
\caption{\small Q$^2$ scores across extractive and abstractive responses on \begindata{} test. }
  \label{fig:q2_density}
\end{figure}

\subsection{Robustness to Distribution Shift} We further investigate the robustness of the metrics under distribution shift. Figure~\ref{fig:boxplots_by_dataset} shows the distributions of both semantic and Q$^2$ scores across the data broken down by source. All metrics\footnote{We observe similar results for lexical metrics.} rate responses from \textsc{WoW} in all categories significantly higher than responses derived from \textsc{CMU-DoG} and \textsc{TopicalChat}. Concerningly, attributable responses generated based on \textsc{CMU-DoG} and \textsc{TopicalChat} receive nearly identical scores to unattributable responses. Likewise, the F1 scores of all the classifiers (Figure~\ref{fig:f1_by_dataset}) are higher on the responses from \textsc{WoW} compared to the ones from \textsc{CMU-DoG} and \textsc{TopicalChat}. Specifically, classifiers tested on \textsc{TopicalChat} examples yield the worst F1 scores. For example, RoBERTA-MNLI's F1 score decreases by 10 points when tested on attributable responses from \textsc{TopicalChat} compared to \textsc{WoW}. 
In general, the metrics appear to perform poorly on datasets that have longer knowledge sources.  \textsc{TopicalChat} has on average  271 words in $\mathcal{K}$, followed by \textsc{CMU-DoG} and \textsc{WoW} which have 215 words, 27 words respectively. 
This shows that shorter knowledge spans correlates with higher metrics performance, pointing to the limited robustness of the metrics.  

 






















\begin{figure*}[ht]
\centering
\includegraphics[width=2.1\columnwidth]{figures/new-plots/boxplots-test-Classifiers-spectrum-label-down-F1.pdf}
\caption{\small Comparison of F1 scores of \textsc{RoBERTa}-based classifiers on \begindata{} categories with examples split by density (the extent to which the response copies verbatim from the knowledge). }
\label{fig:f1scoresbydensity}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=2.1\columnwidth]{figures/final-boxplots/boxplots-test-model-split-semantic-q2.pdf}
\caption{\small Scores of the semantic and Q$^2$ metrics across the three dialogue corpora we used to train our models.}
\label{fig:boxplots_by_dataset}
\end{figure*}

\begin{figure*}[ht]
\centering
\includegraphics[width=2.1\columnwidth]{figures/new-plots/distribution-shift-F1.pdf}
\caption{\small Comparison of F1 scores of  RoBERTa classifiers on \begindata{} categories with examples split by benchmark. }
\label{fig:f1_by_dataset}
\end{figure*}


