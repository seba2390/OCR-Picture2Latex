



\section{Annotations}

We next describe the human annotations we collected for the utterances generated by the models described in Section~\ref{sec:setup}.

\subsection{Taxonomy of Response Types}
We classify responses into three broad categories:

\paragraph{Fully Attributable}  These are responses that convey information that can be completely supported by the provided document; this property has been referred in the literature to as faithfulness  \cite{rashkin-etal-2021-increasing,maynez2020faithfulness, dziri-etal-2021-neural,durmus-etal-2020-feqa} and attribution \cite{rashkin2021measuring}.  In our annotation set-up, we use similar definitions to the Attributable to Identifiable Source (AIS) framework of \citet{rashkin2021measuring}.  The full framework in that paper consists of a two-stage annotation process in which annotators first filter out responses that are deemed to be too vague or ill-formed to be evaluated for attribution. Since  \citet{rashkin2021measuring} found that more than 90$\%$ of the conversational responses in their study were interpretable, we have our annotators focus solely on attribution.



\paragraph{Not Attributable} These are responses that contain at least some information that cannot be verified given the evidence, regardless of whether that information is factually true in the real world.  This includes statements that are  relevant but not fully supported by the background information (hallucinations), statements that explicitly contradict the background information, and off-topic responses about information completely external to the evidence sources. In a pilot study we attempted to separate these three subcategories, but the boundaries between them turned out to be difficult to define and annotate.

\paragraph{Generic} Responses that fall into this category are general enough to fit into a large number of possible contexts  \cite{li2016diversity}. Examples include ``I don't know about that'' and ``Hello there!''. Even when the responses are ostensibly about the same topic as the document, they are vague and do not provide new information. 
Nevertheless, such responses may be useful for various conversational purposes: back-channeling, expressing uncertainty, or diverting the conversation from ambiguous or controversial topics.





\subsection{Collecting Prompt-Query-Reply Triples}
As described in Section~\ref{sec:setup}, we collect data using outputs from four models---T5, GPT2, DoHA, and \CTRL{}. We train a version of each model on each of the three datasets (\textsc{WoW}, \textsc{TopicalChat} and \textsc{CMU-DoG}) and generate responses using the test portion of the dataset. For more details on training and hyperparameters, refer to Appendix~\ref{sec:hyperparam}. We select at least 1000 examples from each dataset-model pair.
We filter and remove toxic responses using the Google Perspective API. This yields 12288 examples in total.
\begin{figure*}
\centering
  
    \subfigure[\label{fig:cmu_begin}\footnotesize Breakdown by model class.]{\includegraphics[width=0.48\linewidth]{figures/new-plots/stackedbar-model.pdf}}
       \subfigure[\label{fig:wizard_begin}\footnotesize Breakdown by dialogue corpus used for training.]{\includegraphics[width=0.48\linewidth]{figures/new-plots/stackedbar-data.pdf}}
    \caption{\small Breakdown of \begindata{} response categories across models (left) and training corpora (right).}
    \label{fig:breakdown_gold}
\end{figure*} 
\subsection{Annotating  Prompt-Query-Reply Triples}
We present annotators with a knowledge snippet $\mathcal{K}$, the previous turn $u_{n-1}$ and a generated response $\bar{u}_{n}$, and ask them to select which of the three categories fits $\bar{u}_{n}$ best. 
For the exact annotation instructions, see Appendix~\ref{sec:annotationprotocol}.
To obtain high quality data, we assign three annotators to each example and report results based on majority vote. We exclude examples where each of the three annotators assigned a different category, making it impossible to compute a majority vote.

\paragraph{Annotation Quality}
 To ensure that the annotators understood the task, we use the following manual quality control procedure. In the first stage, we train the annotators by running two pilot annotation batches ($\sim100$~examples each).  After each batch, we manually grade the answers for compliance with instructions, and provide feedback explaining any misconceptions.  After the training stage, we launch the main annotation round for the full set of 12k examples.  During this round, we intermittently check responses after every 3k completed annotations to examine the annotation quality. %
 This procedure resulted in high inter-annotator agreement (a Krippendorff's alpha of 0.7).


\subsection{Dataset Analysis}
\label{sec4:data_analysis}
\BEGIN{} is intended as a test benchmark; as such, it does not have a training portion: We only create development ($10\%$) and test ($90\%$) partitions. 
We include examples from \BEGIN{} in Table~\ref{tab:benchmark:wow_cmu_begin_dist} along with the label breakdown. Overall, the models generated a substantial number of unattributable responses ($70\%$). As
Figure~\ref{fig:cmu_begin} shows, this proportion was higher for \textsc{GPT2}, \textsc{DoHA}, and \textsc{T5}, whereas \CTRL{} generated the lowest proportion of unattributable responses ($30.8\%$).  This indicates that \CTRL{}, which is explicitly designed to discourage unattributable responses, is moderately successful at its goal. Figure~\ref{fig:wizard_begin}, which breaks the results down by training corpus, shows that models trained on \textsc{TopicalChat} produce the highest amount of unattributable responses followed by \textsc{CMU-DoG} and \textsc{WoW}. 
This is consistent with recent analyses on \textsc{WoW}, \textsc{CMU-DoG} and \textsc{TopicalChat} which revealed that more than 60\% of the ground-truth  responses are unattributable to the knowledge \cite{dziri2022origin,rashkin2021measuring}.

\subsection{The Need to Measure Attribution}
Our analysis of the responses produced by the systems we trained highlights the potential pitfalls of language-model-based dialogue systems, especially when deployed in real-world scenarios across a broad range of domains where hallucinations pertaining to vital information may produce undesirable user experiences---e.g., healthcare \cite{laranjo2018conversational, jovanovic2020chatbots} and education  \cite{yang2019opportunities, kochmar2021automated}---and underscores the need for progress on both the modeling and the evaluation side.
Neural dialogue systems are optimized to mimic the distributional properties of the human-generated dialogue corpus used to train them. Because humans often include unattributable information in their utterances, language models trained on those corpora can replicate and perhaps even amplify the prevalence of unattributable responses at test time \cite{kang-hashimoto-2020-improved, dziri2022origin}. These findings call for robust evaluation metrics to uncover actionable insights about best practices of using such models and benchmarks.
We hope that \BEGIN{} will, as an evaluation benchmark, promote a strict standard for evaluation metrics, laying the ground for trustworthy dialogue systems.


 
















