\section{Conclusion}
Contemporary knowledge-based dialogue systems that rely on language models often generate responses that are not attributable to the background knowledge they are expected to convey. We present \textsc{Begin}, a new benchmark to advance research toward robust metrics that can assess this issue. 
We use \textsc{Begin} to comprehensively evaluate a broad set of existing automatic metrics. We show that these metrics rely substantially on word overlap and fail to properly rank abstractive attributable responses as well as generic responses. They also struggle under distribution shift, assigning low scores to attributable responses grounded on long knowledge sources. 
We hope that this work will spur future research on building robust evaluation metrics for grounded dialogue systems.

\section*{Acknowledgements}
We are grateful to the anonymous
reviewers for helpful comments.
We thank Dipanjan Das, Vitaly Nikolaev, Sebastian Gehrmann,  Roee Aharoni, Jennimaria Palomaki,  Tom Kwiatkowski, Michael Collins and Slav Petrov for helpful discussions and feedback. We also thank Ashwin Kakarla and his team for
helping with the annotations.