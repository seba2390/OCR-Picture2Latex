\section{Method}


Our goal is to build a model that represents joint rotations, shape and camera information using tokens independent of image feature and further captures the rotational motion information of each joint from video data.
In this section, we first revisit prior SMPL-based human mesh recovery methods and then describe our model design.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/basemodel.pdf}
	\caption{Base model for singe-frame input. We first use an image encoder to extract feature maps from a given cropped image, then we flatten the feature map into a sequence and add a learnable position embedding. The learnable joint rotation tokens, shape and camera tokens are appended to the sequence and sent to the transformer. Finally, we use three linear heads, called rotation head, shape head and camera head, to convert the joint rotation tokens, shape token and camera token to the SMPL parameters, for 3D mesh reconstruction and 2D reprojection on the image plane.}
	\label{fig:base_model}\vspace{-0.1in}
\end{figure}



%In this section, we first revisit the representative SMPL based 3D human pose estimation and mesh recovery method, including the image-based and video-based estimation. Second, we introduce our token representation for estimating the pose, shape and camera parameters, designed for single-frame images using transformer. Then, we further model the temporal 

\subsection{Revisiting SMPL-based human mesh recovery}

The classic human mesh recovery (HMR) methods~\citep{hmr:kanazawa2018end, hmmr:kanazawa2019learning, vibe:kocabas2020vibe} represent human body as a mesh using the parameteric SMPL~\citep{smpl:loper2015smpl} model. The SMPL mesh model is a differentiable function that output 6890 surface vertices $\mathcal{M}(\boldsymbol{\theta}, \boldsymbol{\beta}) \in \mathbb{R}^{6890 \times 3}$, which are deformed with linear blend skinning driven by the pose $\boldsymbol{\theta} \in \mathbb{R}^{72}$ and shape $\boldsymbol{\beta} \in \mathbb{R}^{10}$ parameters. The pose $\boldsymbol{\theta}$ parameter include the global rotation $R$ and 23 relative joint rotations in axis-angle format. To obtain the 3D positions of body joints, a pretrained linear regressor $W$ is used to achieve $J_{3d}=W\mathcal{M}(\boldsymbol{\theta}, \boldsymbol{\beta})$. To leverage 2D joint supervision, a weak-perspective camera model is usually used to project 3D joint positions into the 2D image plane, i.e., $J_{2d}=s\Pi(RJ_{3d})+\boldsymbol{t} $, where $\Pi$ is an orthographic projection, the scale value $s$ and translation $\boldsymbol{t}\in \mathbb{R}^{2}$ are camera related parameters.

For the image-based HMR methods like~\citep{hmr:kanazawa2018end,spin:kolotouros2019learning}, an image encoder $f(\cdot)$ and a MLP regressor are used to estimate the set of reconstruction parameters $\boldsymbol{\Theta}=\{\boldsymbol{\theta}, \boldsymbol{\beta}, s, t\}$, which constitutes an 85-dim vector to regress. These parameters are iteratively regressed from the encoded image feature vector $\boldsymbol{f}$ by the regressor. 
For the video-based HMR methods like~\citep{vibe:kocabas2020vibe,tcmr:choi2021beyond,hmmr:kanazawa2019learning,maed:wan2021encoder,mps-net:wei2022capturing}, temporal models based on 1D convolution~\citep{hmmr:kanazawa2019learning}, GRU~\citep{tcmr:choi2021beyond} and self-attention models~\citep{vibe:kocabas2020vibe,maed:wan2021encoder} are introduced to capture the motion information in consecutive video frames. A temporal encoder $g(\cdot)$ is exploited to achieve temporally encoded feature vector, formulated as a process like: $\boldsymbol{m}_t=g(\boldsymbol{f}_{t-T/2}, ..., \boldsymbol{f}_{t},...,\boldsymbol{f}_{t+T/2})$. Also, a regressor is used to estimate the $\boldsymbol{\Theta}_t$ from the current frame's feature $\boldsymbol{m}_t$ encoded with temporal information. 
%Unlike the classic human mesh recovery methods, which estimates the pose, shape and camera parameters from the encoded

%First, our model estimates 3D human pose and reconstructs the body mesh from a single RGB image containing centered single person. 
%Unlike the classic human mesh recovery (HMR) methods that use an iterative regressor to estimate the SMPL pose $\theta \in \mathbb{R}^{72}$, shape $\beta \in \mathbb{R}^{10}$ and camera (including translation $t\in R^2$ and scale $s\in R$) paramters from the encoded image feature and initial pose & shape parameters. Besides extracted feature representation, we introduce three types of addition token representations: joint tokens, shape token, and camera token. We represent the 3D pose information as 24 joint rotation token vectors denoted as $J \in \mathbb{R}^{24\times d}$, each of which has $d$-length dimension. 

\subsection{Estimating SMPL parameters based on independent tokens}

%
% A common design of aforementioned methods is that the pose, shape and camera parameters are directly regressed from the extracted feature vector using a MLP regressor. 
%In our framework, we instead hypothesize that, the 3D information about joint rotations, body shape and camera parameters are independent of 2D image pixels, though they are strongly related with image pixels. To embody this, we encode the 3D joint rotations, shape and camera information into learnable token embeddings, the source of which is not image feature but can interact with image feature. 


In this section, we first introduce the token based semantic representation and then describe our model design for single frame input, mainly including the \verb|Base model| and \verb|SMPL heads|.



 {\bf Joint rotation tokens, shape token \& camera token.} We introduce three types of token representations: (1) joint rotation tokens consist of 24 tokens, each of which encodes the joint 3D relative rotation information (including the global rotation) ${\boldsymbol{r}_i\in \mathbb{R}^d}, i=1,..,24$; (2) shape token is a token vector $\boldsymbol{s}\in \mathbb{R}^d$ encoding the body shape information; (3) camera token is also a token vector $\boldsymbol{c}\in \mathbb{R}^d$ encoding the translation and scale information. $d$ is the vector dimension for all tokens.
 %\verb|[joint]|

{\bf Base model.} Inspired by ViT ~\citep{vit:dosovitskiy2020image} and TokenPose~\citep{tokenpose:li2021tokenpose}, we embody our scheme into a Transformer-based architecture design (Fig.~\ref{fig:base_model}). We adopt a CNN to extract image feature map $\boldsymbol{f} \in \mathbb{R}^{c\times h \times w}$ from a given RGB image $I$ cropped with a human body. We reshape the extracted feature map into a sequence of flattened patches and apply patch embedding $\mathbf{E}$ (linear transformation) to each patch to achieve the sequence $\boldsymbol{f}_p \in \mathbb{R}^{S\times d}$ where $S=h\times w$. We append the totally learnable joint rotation tokens, shape token and camera token to the sequence $\boldsymbol{f}_p$, namely \textit{prior tokens}.
We only inject the learnable position embedding $\mathbf{PE} \in \mathbb{R}^{S\times d}$ into the $\boldsymbol{f}_p$ to preserve the 2D structure position information. Then we send the whole sequence $\boldsymbol{S}_0 \in \mathbb{R}^{(S+24+1+1)\times d}$ to a standard Transformer encoder with $L$ layers and achieve these corresponding tokens from the final layer. 
%We denote the process above as the \verb|Base model|

%in the sequence $S_{L}$

{\bf SMPL heads.} To achieve the estimated SMPL parameters $\Theta=\{\boldsymbol{\theta}, \boldsymbol{\beta}, s, t\}$ for 3D human mesh reconstruction, we use three \textit{linear} SMPL heads -- \textit{rotation head}, \textit{shape head} and \textit{camera head} -- to transform the corresponding tokens outputted from the final transformer layer. Particularly, the rotation head (a shared linear layer) transforms each joint token into a 6D rotation representation~\citep{rot6d:zhou2019continuity}. %It is empirically adopted in common methods like~\citep{spin:kolotouros2019learning, maed:wan2021encoder}. 
The shape and camera heads (two linear layers) separately transform the shape token and camera token into the shape parameters (10-dim vector) and camera parameters (3-dim vector). Finally, we convert the 6D rotation representations to SMPL pose (in axis-angle format) and use these parameters to generate the human mesh. Further, we obtain the predicted joint 3D locations $\boldsymbol{J}_{3d}$ and then project them into 2D locations $\boldsymbol{J}_{2d}$ using a weak-perspective camera model.


%, enabling the joint tokens to directly reflect the real rotation state. 



% In fact, our multi-layer transformer architecture still reserve the spirit of iteration, since each transformer layer process the tokens in a fixed compute manner. But the learnable token embeddings can capture the data-driven geometric priors between joint tokens, rather than only dependent on image feature and initialized SMPL parameters

% {\bf ViT-like spatial Transformer.}


\subsection{Rotational motion capturing using a temporal Transformer}


\begin{figure}
	\centering
	\includegraphics[width=.9\linewidth]{figures/temporal.pdf}
	\caption{The overall temporal model framework. \textbf{\uppercase\expandafter{\romannumeral1}. The base model}. We feed the frames of a given video clip to the same base model and achieve the tokens for each frame. \textbf{\uppercase\expandafter{\romannumeral2}. The temporal model}. We use a transformer as the rotation motion encoder to capture the motion of each joint. 
	\textbf{\uppercase\expandafter{\romannumeral3}. The SMPL heads}. We feed the updated joint tokens, shape token and camera token of each frame to the SMPL heads shared with image-based model, to achieve the final SMPL parameters.}
	\label{fig:overall}\vspace{-0.2in}
\end{figure}
We aim to capture the rotational motion at the joint level. Given a video clip $V = \{I_t\}_{t=1}^T$ of length $T$, we feed these $T$ frames to the Base model and acquire the estimated $N$ joint rotation tokens for each frame: $\{\hat{\boldsymbol{r}}_1^t,...,\hat{\boldsymbol{r}}_N^t\}_{t=1}^T \in \mathbb{R}^{T\times N \times d}$, from the final transformer layer. 

We use another standard Transformer as the temporal model to capture the motion of each joint; we denote it as \verb|Temporal Transformer|. For the $n$-th type joint token $\boldsymbol{r}_n$, such as the left knee, the token sequence formed in the time axis is $X_n=\{\hat{\boldsymbol{r}}_n^1, \hat{\boldsymbol{r}}_n^2, ..., \hat{\boldsymbol{r}}_n^T\} \in \mathbb{R}^{T \times d}$, where $n\in\{1,...,N\}$. We feed each sequence $X_n$ to the Temporal Transformer and achieve a new sequence $X_n'$, so that each updated joint token from a particular moment is mixed with the joint rotation information from past and future frames. 
%For $N$ such sequences, i.e., $\{X_1, ..., X_n\}$, they are send to the same Temporal Transformer. 
Then, we  reshape the temporally updated tokens $\{X_1', ..., X_n'\}$ from $N \times T \times d$ to $T \times N \times d$. 
 Note that the temporal information in camera and shape tokens is not taken into consideration in our model since we hope to capture the pure rotational motion information of joints. 
 Finally, for timestamp $t$, the updated $N$ joint tokens are then fed into the rotation head to achieve the joint rotations $\boldsymbol{\theta}_t$; the shape token and camera token outputted from the Base model are fed to the shape head and camera head to achieve the $\boldsymbol{\beta}_t, s_t, \boldsymbol{t}_t$. The overall framework is shown in Fig.~\ref{fig:overall}.

%The Temporal Transformer can be viewed as a model to update the joint tokens using the rotational information from future and past. 

%
%{\bf Masked modeling.} We also conduct masking some tokens in the time sequence to improve the robustness in temporal consistency. Inspired by PoseBERT and MAE, we conduce a more simple masked modeling by reconstructing the joint tokens in time axis. We randomly mask $m\%$ tokens, such as 12.5\%, 

\subsection{Loss function}
Leveraging full supervision from different formats of annotations is critical to train the model well and attain the generalization in different cases.
Following common human mesh recovery methods, we use SMPL parameters loss, L2 normalization, 3D joint location loss and projected 2D joint location loss, when the corresponding SMPL, 3D/2D location supervision signals are available.
$$\mathcal{L}_{smpl}=w_{\theta} \cdot \left\|\boldsymbol{\theta}-\boldsymbol{\theta}_{gt}\right\|_2 +   w_{\beta} \cdot \left\|\boldsymbol{\beta}-\boldsymbol{\beta}_{gt}\right\|_2,$$
$$\mathcal{L}_{norm}= \left\| \boldsymbol{\theta}\right\|_2 + \left\| \boldsymbol{\beta}\right\|_2,$$
$$\mathcal{L}_{3D}=\left\| \boldsymbol{J}_{3d} - \boldsymbol{J}_{3dgt}\right\|_2 , \mathcal{L}_{2D}=\left\| \boldsymbol{J}_{2d} - \boldsymbol{J}_{2dgt}\right\|_2, $$
$$\mathcal{L}_{temp}=\left\| (\boldsymbol{J}_{3d}^{t+1}- \boldsymbol{J}_{3d}^{t}) - (\boldsymbol{J}_{3dgt}^{t+1}- \boldsymbol{J}_{3dgt}^{t}) \right\|_2,$$
$$\mathcal{L}=\mathcal{L}_{smpl} + w_{norm} \cdot \mathcal{L}_{norm} + w_{3D} \cdot \mathcal{L}_{3D} + w_{2D} \cdot \mathcal{L}_{2D} + w_{temp} \cdot \mathcal{L}_{temp}.$$

$\boldsymbol{\theta}_{gt}, \boldsymbol{\beta}_{gt}$ are the groundtruth SMPL parameters. $\boldsymbol{J}_{3D}, \boldsymbol{J}_{2D}$ are the groundtruth joint 3D and 2D locations. The $\mathcal{L}_{temp}$ is a temporal loss for video data, which supervises the velocity of the joint temporal movement in 3D space. The $w_{\theta}, w_{\beta}, w_{norm}, w_{3D}, w_{2D}, w_{temp}$ are the weights to balance all of loss functions (see more details in Appendix~\ref{appendix:training}).
