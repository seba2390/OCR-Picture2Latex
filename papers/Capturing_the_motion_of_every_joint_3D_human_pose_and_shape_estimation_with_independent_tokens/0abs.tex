\begin{abstract}
%Human pose, shape and motion in 3D space are intuitively concepts and not completely dependent on image pixels, while standard image- or video-based 3D human pose and mesh recovery approaches mainly estimate SMPL parameters from the encoded image feature. In addition, the combined space of all joint motions is huge, resulting in complicated non-linear motion at the pose-level, but the motion at the joint-level lies in a relatively smaller space. Existing video-based approaches directly estimate the whole pose from the temporally encoded feature, failing to separately capture the motion of each joint.

%In this paper, we present a novel 3D human pose and mesh recovery based on a design of independent tokens, which is able to capture the rotational temporal motion of each joint. 
%%To address these issues, we present a simple yet effective method for 3D human pose and mesh recovery based Transformer.
%\textbf{First}, unlike typical human mesh recovery methods using a regressor to iteratively estimate SMPL parameters from the image feature, we introduce three types of learnable tokens independent of image features: \textit{joint rotation tokens, shape token and camera token}. 
%%First, we introduce three types of learnable tokens independent of image features: \textit{joint rotation tokens, shape token and camera token}. 
%These tokens learn the human joints' 3D rotation, body shape, and camera information by progressively interacting with image features via Transformer layers, which also makes the model freely learn the relationships between joint rotation angles from data prior and specific images. %without a human-defined kinematic tree. 
%\textbf{Second}, the combined space of all joint motions is huge, resulting in highly complicated motion at the level of image feature and whole-pose, while, the rotation motion at the joint-level lies in a relatively smaller space, even showing regular patterns. Existing video-based approaches directly estimate the whole pose from the temporally encoded feature, failing to separately capture the motion of each joint. 
%Benefit from the proposed token-based representations, we further use a temporal model to focus on capturing the rotational temporal information of each joint. 
%Empirically, this scheme is more conducive to maintaining the temporal coherence and rationality of each joint rotation and preventing large pose jitters.
%%Second, Benefit from the joint token representation, the rotational temporal information of each joint can be separately captured by our temporal model. 
%Despite conceptually simple, the proposed method  achieveS 38.3 mm and 42.0 mm error in the PA-MPJPE metric for the Human3.6M and 3DPW datasets, outperforms state-of-the-art methods on the challenging 3DPW with a large margin. %The qualitative results also show that our model produces temporally consistent joint motions and less pose jitters.

In this paper we present a novel method to estimate 3D human pose and shape from monocular videos.
This task requires directly recovering pixel-alignment 3D human pose and body shape from monocular images or videos, which is challenging due to its inherent ambiguity. 
To improve precision, existing methods highly rely on the initialized mean pose and shape as prior estimates and parameter regression with an iterative error feedback manner. 
In addition, video-based approaches model the overall change over the image-level features to temporally enhance the single-frame feature, but fail to capture the rotational motion at the joint level, and cannot guarantee local temporal consistency.
To address these issues, we propose a novel Transformer-based model with a design of independent tokens. 
First, we introduce three types of tokens independent of the image feature: \textit{joint rotation tokens, shape token, and camera token}. 
By progressively interacting with image features through Transformer layers, these tokens learn to encode the prior knowledge of human 3D joint rotations, body shape, and position information from large-scale data, and are updated to estimate SMPL parameters conditioned on a given image.
Second, benefiting from the proposed token-based representation, we further use a temporal model to focus on capturing the rotational temporal information of each joint, which is empirically conducive to preventing large jitters in local parts.
Despite being conceptually simple, the proposed method attains superior performances on the 3DPW and Human3.6M datasets. Using ResNet-50 and Transformer architectures, it obtains 42.0 mm error on the PA-MPJPE metric of the challenging 3DPW, outperforming state-of-the-art counterparts by a large margin. Code will be publicly available\footnote{\url{https://github.com/yangsenius/INT_HMR_Model}}.



\end{abstract}