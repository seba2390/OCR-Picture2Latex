\section{Experiments}

%We first describe the experiment settings including the training data, evaluation and training strategy. We then compare the quantitative and qualitative results of our model with state-of-the-art methods and provide an ablation study to validate the design choices. Finally, we provide visualization analysis to give insights into what the model learns. 
\label{details}
{\bf Training data \& Model setups.} Following~\citep{hmr:kanazawa2018end, vibe:kocabas2020vibe, maed:wan2021encoder}, we use mixed 3D video, 2D video and 2D image datasets for training. The details of model configurations and training data are described in Appendix~\ref{appendix:model} and Appendix~\ref{appendix:data}. 

{\bf Evaluation.} We report results on 3DPW and Human3.6m datasets,  using 4 standard metrics, including Procrustes-Aligned Mean Per Joint Position Error (PA-MPJPE), Mean Per Joint Position Error (MPJPE), Per Vertex Error (PVE) and ACCELeration error (ACCEL). The unit for these metrics is millimetter (mm). We evaluate the models trained w/ and wo/ 3DPW training set for comparison with previous methods.


%{\bf Training strategy and model settings.} For the Base model, we use a hybrid architecture with ResNet~\citep{resnet:he2016deep} and Transformer~\citep{transformer:vaswani2017attention}, based on the ImageNet pretrained ViT model~\citep{vit:dosovitskiy2020image}. 
%The transformer in the \verb|Base model| has 6 blocks and 12 heads for each self-attention layer. 
%The embedding dimension is 768 for each token vector and the hidden dimension in the FFN is 4*768.  The number of layers and attention heads of \verb|Temporal Transformer| is set to 3 and 12 by default. We also add a learnable \textit{temporal embedding} to retain the position information of tokens. In the training, the length $T$ of video clips is 16 sampled at a interval of 8 from the original video data. In practice, we find setting $T$ larger (we use $T=64$, interpolating the temporal embedding if necessary) in inference can improve the accuracy, with acceptable additional computational overhead. Following~\citep{vibe:kocabas2020vibe, maed:wan2021encoder}, the $w_{\theta}, w_{\beta}, w_{norm}, w_{3D}, w_{2D}, w_{temp}$ are empirically set to  60, 0.06, 600, 300, 600, respectively.

%Due to that our model consists of multiple parts, we first train the \verb|Base model| and \verb|SMPL heads| to extract better static feature and estimate for each image, only using image-based datasets. The pretrained weights of image-based model are set as a good initialization of the subsequent temporal model training . 


{\bf Progressive training scheme.}  In practice, we find how to exploit these training datasets with such multi-modal supervision is critical to training the whole model well and ensuring the generalization to in-the-wild scenes. Following MAED~\citep{maed:wan2021encoder},  we develop an improved progressive training scheme adapting to our model, which consists of three training phases. Please see more details about this training scheme in Appendix~\ref{appendix:training}.

%In the first phase, we train the Base model and SMPL heads using singe-frame images. In the second phase, we use the pretrained weights from the first phase as initialization and train the Base model, Temporal Transformer and SMPL heads using mixed 2D/3D and image/video datasets, but without 3DPW data. In the third phase, we use the pretrained weights from the second phase and fine-tune the whole model on 3DPW train set and Human3.6m datasets that have accurate pose and shape annotations. The fine-tuned model still shows generalization to in-the-wild data. Please see more details and motivations of using this training scheme in the Supp. Mat. (supplementary material).

\begin{table*}
	
		\label{tab:sota}
	\caption{Comparisons to state-of-the-art models on 3DPW and Human3.6M datasets. 
		%Our model outperforms all state-of-the-art image-based and video-based models on the in-the-wild 3DPW dataset and attains comparable result on the indoor Human3.6M. 
		%Note that the performances on 3DPW and Human3.6m of are evaluated by a single model, rather than separate models best for different datasets. 
		INT-1 model is trained with first two phases, and separately evaluated the best performance for both datasets. INT-2 model is trained with three phases, and the best model for 3DPW is directly evaluated on Human3.6m.
		$\dagger$ represents training w/o 3DPW training dataset. $\ast$ represents training w/ 3DPW training dataset. For fair comparison, we also list the CNN backbones (ResNet~\citep{resnet:he2016deep} or HRNet~\citep{hrnet:sun2019deep}) used in different methods.}
	\centering
	\resizebox{0.95\textwidth}{!}{
		\begin{tabular}{llccccccc}
			\toprule
			&  & & \multicolumn{4}{c}{3DPW} & \multicolumn{2}{c}{H36M} \\
			\cmidrule(lr){4-7} \cmidrule(lr){8-9} 
			& Models & Backbone & PA-MPJPE $\downarrow$ & MPJPE $\downarrow$ & PVE $\downarrow$ & Accel $\downarrow$ & PA-MPJPE $\downarrow$ & MPJPE $\downarrow$ \\
			
			\midrule
			
			\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{Image-based}}}
			& 
			HMR$\dagger$~\citep{hmr:kanazawa2018end}& ResNet-50 & 76.7 & 130.0 & - & 37.4  & 56.8 & 88 \\
			&  Neural Body$\dagger$~\citep{neuralbodyfitting:omran2018neural}& ResNet-101 &   - &   - &   - &   - &    59.9 &   - \\
			%& Pavlakos & - & - & -  & - & 75.9 & - \\
			&   Mesh Regression$\dagger$~\citep{meshregression:kolotouros2019convolutional} & ResNet-50 &   70.2 &   - &   - &   -  &   50.1 &   - \\
			&   SPIN$\dagger$~\citep{spin:kolotouros2019learning} & ResNet-50 &   59.2 &   96.9 &   116.4 &   29.8 &    41.1 &   - \\
			& I2l-MeshNet$\dagger$~\citep{i2l-meshnet:moon2020i2l} &ResNet-50&57.7 & 93.2 & 110.1 & 30.9 & 41.1 & 55.7 \\
			& PyMAF$\dagger$~\citep{pymaf:zhang2021pymaf} &ResNet-50& 58.9& 92.8 & 110.1 & - &40.5 & 57.7\\
			& Hybrik$\dagger$~\citep{hybrik:li2021hybrik} &ResNet-34& 48.8 & 80.0 & 94.5 & 34.5 & 54.4 \\
			
			& ROMP$\ast$~\citep{romp:sun2021monocular} &ResNet-50& 49.7 & 79.7 & 94.7 &- & -&-\\
			& ROMP$\ast$~\citep{romp:sun2021monocular} &HRNet-W32& 47.3 & 76.7 & 93.4 &- & -&-\\
			
	
            & PARE$\dagger$~\citep{pare:kocabas2021pare}  &ResNet-50& 52.3 & 82.9 & 99.7 & - & - & - \\
			& PARE$\dagger$~\citep{pare:kocabas2021pare}  &HRNet-W32& 50.9 & 82.0 & 97.9 & - & - & - \\
			& PARE$\ast$~\citep{pare:kocabas2021pare} &HRNet-W32& 46.5 & \textbf{74.5} & 88.6 & - & - & - \\
			
			& METRO$\ast$~\citep{metro:lin2021end} &ResNet-50~& - & - & - & - & 40.6 & 56.5 \\
			& METRO$\ast$~\citep{metro:lin2021end} &HRNet-W64~& 47.9 & 77.1 & 88.2 & - & 36.7 & 54.0 \\
		
			& Mesh Graphormer$\ast$~\citep{meshgrahormer:lin2021mesh} &HRNet-W64& \textbf{45.6 }& 74.7 &\textbf{ 87.7} & - & \textbf{34.5} & \textbf{51.2 }\\
			
			\midrule
			
			\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{Video-based}}} 
			& 
			HMMR$\dagger$~\citep{hmmr:kanazawa2019learning} & ResNet-50& 72.6 & 116.5 & 139.3 & 15.2 & 56.9 & - \\
			%
			&   Sim2Real$\dagger$~\citep{sim2real:doersch2019sim2real} &ResNet-50&   74.7 &   - &   - &   - &   - &    - \\
			& Temporal Context$\dagger$~\citep{temporalcontext:arnab2019exploiting}& ResNet (from HMR) & 72.2 & - & - & - &  54.3 & 77.8 \\
			& Skeleton-disentangled$\dagger$~\citep{skeleton-disentangled:sun2019human} & ResNet-50 & 69.5 & - & - & - &  42.4 & 59.1 \\
			%
% 			&   VIBE$\dagger$~\citep{vibe:kocabas2020vibe} &ResNet (from SPIN)&   56.5 &   93.5 &   113.4 &   27.1 &   41.5 &   65.9 \\
			& VIBE$\ast$~\citep{vibe:kocabas2020vibe}&ResNet (from SPIN) & 51.9 & 82.9 & 99.1 & 23.4 & 41.4 & 65.6 \\ 
			&TCMR$\dagger$~\citep{tcmr:choi2021beyond} &ResNet (from SPIN)& 55.8 & 95.0 & 111.3 & \textbf{6.7} & 41.1 & 62.3 \\
			& MPS-Net$\ast$~\citep{mps-net:wei2022capturing} &ResNet (from SPIN) & 52.1 & 84.3 & 99.7 &7.4 & 47.4 & 69.4\\
% 			\cmidrule{2-8}
% 			&  MAED$\dagger$~\citep{maed:wan2021encoder} &ResNet-50 & 50.7 & 88.8 & 104.5 & 18.0  & 38.7 & 56.3\\ 
			&  MAED$\ast$~\citep{maed:wan2021encoder} &ResNet-50& 45.7 & 79.1 & 92.6 & 17.6 & 38.7 & 56.4 \\ 
			\cmidrule(lr){2-9}
			& \textbf{INT-1 (Ours)}$\dagger$  &ResNet-50& 49.7   & 90.0   & 105.1   & 23.5   & 39.1   & 57.1  \\
			& \textbf{INT-2 (Ours)}$\ast$ &ResNet-50& \textbf{42.0}  & \textbf{75.6} & \textbf{87.9}  & 16.5  & \textbf{38.4}  & \textbf{54.9}  \\
			
			\bottomrule
			& 		\end{tabular}
		
	}\vspace{-0.2in}

\end{table*}                                                                                                                           
%
%In the first phase, we  expect the Base model to extract better feature and SMPL heads to make accurate estimates for static images; so we train  the model only using 2D image datasets and individual frames from Human3.6m and MPI-INF-3DHP. In the second phase, we take the phase-1 pretrained weights of Base model and SMPL heads as the initialization for the subsequent temporal modeling, and then use the mixed 2D/3D and image/video datasets to train the whole model, including Base model, Temporal Transformer and SMPL heads. However, we find that the generalization of the model gradually deteriorates in the middle and late periods in this phase, indicated by the performance on 3DPW test split. 
%%That is, the model in later period performs worse on 3DPW model than earlier ones. We further observe that the estimated pose and shape of human mesh become implausible in the later period. Based on this observation, 
%We empirically attribute this to that 1) a large proportion of training data, such as insta or posetrack, has no annotations of pose and shape parameters; (2) domain gap exists among various datasets. These factors may cause the conflicts between multiple objectives in the later stage of training, such as overfittng in $\mathcal{L}_{2D}$ making the model less constrained with reasonable SMPL pose or shape parameters. 
%%We make ablation study on the effect of 2D video datasets.  
%Based on this conjecture, we develop the third phase. We use the pretrained weights from the phase-2 (not trained on 3DPW dataset) and fine-tune the whole model on 3DPW and Human3.6m datasets that have accurate pose and shape annotations.
%This phase largely preserves the adaptability of the phase-2 to in-the-wild scenarios. 

\subsection{Comparison with state-of-the-art methods}


                                     

{\bf Quantitative results.} In Tab.~\ref{tab:sota}, we compare our method with state-of-the-art image-based and video-based HMR methods. We evaluate the models trained w/ and w/o 3DPW training set for fair comparisons.  For 3DPW, our model substantially outperforms all these methods, mainly in PA-MPJPE, MPJPE and PVE. For Human3.6m, our model is on par with state-of-the-art methods. 
MAED~\citep{maed:wan2021encoder} is our main competitor, as both use the same backbone and training data. The results show that our model achieve superior performances in all metrics over MAED, particularly in PA-MPJPE on 3DPW gaining 8.1\% performance improvement. 
Our model achieves a significant improvement when trained with 3DPW, which indicates that using accurate SMPL pose and shape labels as supervision is critical to improve the generalization to in-the-wild scenes. 
Notably, METRO~\citep{metro:lin2021end} and Mesh Graphormer~\citep{meshgrahormer:lin2021mesh} are recent Transformer-based SOTA methods, with a stronger CNN extractor HRNet-W64~\citep{hrnet:sun2019deep} as the backbone, showing better performance. %They do not leverage SMPL prior model as constraints, showing accurate predictions in 3D locations. 
In comparison to them, our method shows superior performance in PA-MPJPE on 3DPW and comparable results in MPJPE and PVE, even using ResNet-50~\citep{resnet:he2016deep} as the backbone. These results demonstrate the effectiveness and superiority of our token-based model design.

{\bf Qualitative results.} To demonstrate qualitative mesh reconstruction results, we select a typical showcase of 3DPW dataset to compare our model with the current video-based SOTA method MAED~\citep{maed:wan2021encoder}, as shown in Fig.~\ref{fig:vis_contrast_3dpw}. We can see that MAED performs well in most frames but still produces some bad fit in hard samples. 
In contrast, our model produces accurate pixel accurate mesh alignment that better fit to 2D human silhouette, resulting in more natural and smoother motion. 
This phenomenon suggests that our scheme of separate joint rotation predictions may produce more flexible and adaptive human mesh than regressing the whole pose.
Please see more in-the-wild examples in the Appendix~\ref{appendix:examples} for reference.







\subsection{Ablation study}

%In this section, we mainly choose the performance on 3DPW test set to evaluate the model.

%\subsubsection{The effectiveness and superiority of the separate temporal modeling}


\begin{table}[!htbp]\footnotesize
	\caption{Study on the effectiveness of temporal modeling. We evaluate the performances of the image-based  model and two types of video-based temporal models.}
	\label{tab:image_vs_video}
	\centering
	
			\renewcommand{\arraystretch}{1.1}
	\setlength{\tabcolsep}{0.9mm}
	\resizebox{.6\textwidth}{!}{
	\begin{tabular}{cccc}
		\toprule
	 Mode & Temporal modeling   & PA-MPJPE  $\downarrow$ & Accel  $\downarrow$\\
		\midrule
	Image-based& N/A&  45.6   & 23.5 \\
	 Video-based &  whole-pose temporal learning  & 43.9  &   19.1 \\
	 Video-based&   separate joint temporal learning  & \textbf{42.0} & \textbf{16.5}   \\
	
		
		\bottomrule
	\end{tabular}}

	
\end{table}


\begin{figure}[t]
	\centering
	\includegraphics[width=1.\linewidth]{figures/vis_contrast.pdf}
	\caption{The qualitative comparisons between our model (top) and the reproduced MAED model~\citep{maed:wan2021encoder} (bottom).  
		%Our model and MAED perform 42.0 and 45.1 mm errors in PA-MPJPE on the 3DPW.
	}
	\label{fig:vis_contrast_3dpw}
\end{figure}


{\bf Pure image-based model vs. video-based temporal models.} To study the effectiveness of the temporal modeling, we evaluate the models when trained with and without the Temporal Transformer. When using the Temporal Transformer, we conduct two types of temporal modeling schemes -- separate joint temporal learning and whole-pose temporal learning. In Tab.~\ref{tab:image_vs_video}, the results show that video-based ones have obvious advantages in the PA-MPJPE and Accel metrics compared with the image-based one.

{\bf Separate joint temporal learning vs. whole-pose temporal modeling.} We study the differences between the separate joint temporal learning and whole-pose temporal learning. We implement a new model that concatenates all joint tokens (each is $d/24$-dim) together as a pose vector ($d$-dim) and then use the Temporal Transformer to capture the overall changes in pose over time. As shown in Tab.~\ref{tab:image_vs_video}, although effective, the whole-pose temporal learning scheme still performs worse than the separate joint temporal learning.


\begin{table*}[!htbp]\small
	\caption{Comparisons with the temporal modeling of MAED. We report the MAED result we reproduced.}
	\label{tab:comparison_maed}
	\centering
% \renewcommand{\arraystretch}{0.7}
%		\setlength{\tabcolsep}{0.5mm}
%	

\resizebox{0.9\textwidth}{!}{

	\begin{tabular}{ccccc}
		
		\toprule
		%		\multicolumn{3}{c}{Model}  &\multicolumn{2}{c}{3DPW}                   \\
		\cmidrule(r){1-3} \cmidrule(r){4-5}
		Image model& Backbone & Temporal Modeling     & PA-MPJPE  $\downarrow$ & Accel  $\downarrow$\\
		\midrule
		INT & ViT-base  &  separate joint temporal learning    & 51.6 & 25.3\\
		INT  & ResNet50+Transformer   &  separate joint temporal learnin      & \textbf{42.0} & \textbf{16.5}   \\
		INT  & ResNet50+Transformer & parallel spatial-temporal      &  43.0& 19.4\\
		MAED & ResNet50+Transformer  &  parallel spatial-temporal &45.0&18.6\\
		\bottomrule
	\end{tabular}}


\end{table*}

{\bf Comparison with MAED.} Since we use the same backbone and training data as MAED~\citep{maed:wan2021encoder}, it is relatively fair to compare with the temporal modeling of MAED. We combine the base model with the parallel spatial-temporal mechanism (abbr. parallel). The results in Tab.~\ref{tab:comparison_maed} show that, for PA-MPJPE metric, the parallel scheme performs worse 1 mm than the separate joint temporal learning; but the parallel scheme based INT model still gains 2 mm improvements over MAED.




%\subsection{The hyper-parameter settings for model design}


\subsection{Observations}


Since we explicitly give certain concepts to these learnable tokens, it is desirable to know what information these defined tokens have learned. %In this section, we mainly analyze the model from the temporal perspectives. %We also present the spatial relationships between tokens and image in Supp. Mat. 


\begin{figure}
	\subfigure[Prior pose and shape] 
	{
		\begin{minipage}{0.25\textwidth}
			\centering         
				\label{fig:prior_pose}
			\includegraphics[scale=0.27]{figures/prior_pose.pdf}   
		\end{minipage}
	}
	\subfigure[Attention Area for different tokens] 
	{
		\begin{minipage}{0.75\textwidth}
			\centering 
				\label{fig:attention_area}
			\includegraphics[scale=0.27]{figures/prior.pdf}   
		\end{minipage}
	}
	\caption{Prior knowledge learned in the prior independent tokens. \textbf{(a)}: the pose and shape parameters are transformed from the initial joint rotation tokens and shape token by the rotation head and shape head. \textbf{(b)}: the corresponding attention areas in an image for some selected initial tokens} \vspace{-0.2in}
\end{figure}



{\bf What prior pose and shape are learned?} 
In Fig.~\ref{fig:prior_pose}, we show the prior pose and shape learned in the prior joint rotations and shape token, by transforming them to SMPL parameters by the linear rotation head and shape head. 
We can see that the prior pose and shape present a reasonable appearance, not a totally random state.
Note that we does not leverage any explicit SMPL constraints or image information on these prior joint rotation tokens and shape token. 
The only learning source is the backward gradient signals in the training process. This result also indicates that the model learns a \textit{fixed linear transformation} relationship between these joint token vectors and the realistic 3D rotations. 
In Fig.~\ref{fig:attention_area}, we show the attention areas in an image of for some selected tokens. 
%Also, these tokens learn prior knowledge of the most possibly pose and shape from training data. 
%Such a representation also provides an interface for further constraints such as adversarial training in~\citep{hmr:kanazawa2018end, vibe:kocabas2020vibe}. 
%We expect future works could explore more possible designs.

\begin{figure}[h]
	\centering
% 	\includegraphics[width=1\linewidth]{figures/temporal_attention.pdf}
	\includegraphics[width=0.9\linewidth]{figures/temporal_attention_v3.pdf}\vspace{-0.1in}
	\caption{Visualized attention matrices (bottom) between different frames (top) for a video clip ($T=$128). Colored lines indicate strongly correlated joint rotation tokens between different time points, e.g., for the left hip joint, the joint tokens in the 12-th and 60-th frames have high attention score. Different joints show different rotational temporal patterns.}
	\label{fig:temporal_attention_h36m}\vspace{-0.1in}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/temporal_attention_v2.pdf}
	\caption{Visualized attention matrices (bottom) between different frames (top) for a clip ($T=$32). Colored lines indicate strongly correlated joint rotation tokens between different time points. }
	\label{fig:temporal_attention_posetrack}\vspace{-0.1in}
\end{figure}

{\bf Rotational temporal patterns for different joints.} For joint rotation tokens, there is no information exchange between different frames before being sent into the temporal transformer.
Through the self-attention interactions in the temporal Transformer, the model builds correlation and integrates the information among different frames. For example, in Fig.~\ref{fig:temporal_attention_h36m}, the person is walking. For $t=24$ and $t=84$ frames, his right knee joint is in the same state of rotation (the right calf is slightly lifted), so the estimates for these two frame should be close. 
By visualizing the attention matrix in the last layer of temporal transformer, we can see there is indeed a strong correlation between the two frames for the right knee joint token.
Similarly, we can observe in Fig.~\ref{fig:temporal_attention_h36m} and Fig.~\ref{fig:temporal_attention_posetrack} that the model captures the periodic joint rotational motions.  In this way, the joint angle estimation for the current frame can be corrected and regularized by using the information from past or future frames.

\section{Conclusion}
\label{discussion}
In this paper, we propose a simple yet effective model based on the design of independent tokens to address the problem of 3D human pose and shape estimation. We introduce joint rotation tokens, shape token and camera token to encode the 3D rotations of human joints, body shape and camera parameters for SMPL-based human mesh reconstruction. 
Thanks to the design of independent tokens, we use a temporal Transformer to capture the temporal motion of each joint separately, which is beneficial for maintaining the temporal rotational coherence of each joint and reducing jitters in local joints.
Our model outperforms state-of-the-art counterparts on the challenging 3DPW benchmark and attains comparable results on the Human3.6m.
The qualitative results show that our model can produce well-fitted and robust human mesh reconstructions for video data.
We also give analysis on what prior information learned in the independent tokens and what temporal patterns are captured by the temporal model.
%The spirit behind this approach is to abstract 3D human pose, body shape and camera information from the image pixels as independent concepts/representations.
Since we abstract 3D human joints and shape from image pixels as independent concepts/representations, it is possible for future works to incorporate other modal supervision such as text to leverage reasonable constraints on these independent representations for multi-modal learning.

% Since we inject less inductive bias with respect to prior human pose and shape in the model design, neither using mean pose and shape as prior constraints nor a regressor or decoder to iteratively rectify the SMPL estimates, the model may yield incredible human pose and shape to best fit the 2D joint locations if without accurate 3D labels. 
% Therefore, for our model, achieving generalization to unseen in-the-wild scenes highly relies on sufficient training data, especially data with accurate SMPL pose and shape label as supervision. 
% Future works may leverage reasonable constraints on the independent tokens or more modal supervision to reduce the reliance on data with 3D ground truth labels.

% {\bf Reproducibility.} Our training and evaluation codes are based on the official implementation of MAED~\citep{maed:wan2021encoder}. We use the same dataset recipe, image pre-/post-processing, data augmentation strategies. For the concrete model configurations and training/evaluation setups, we describe the details in the Section~\ref{details} and the Appendix~\ref{appendix:model} and \ref{appendix:training} for reproducibility. The code will be made publicly available. We also list a varitey of prediction results for images and videos in the supplementary materials.

\section{Acknowledgement}

This work was supported by the National Natural Science Foundation of China under Nos. 62276061, 61773117 and 62006041.







%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=1.\linewidth]{walking_h36m3.pdf}
%		%\includegraphics[width=1.\linewidth]{Walking_T128_attention_map_383.pdf}
%	
%	\caption{Caption}
%	\label{fig:walking_h36m}
%\end{figure}
%
%
%\begin{figure}[h]
%	\centering
%	\includegraphics[width=1.\linewidth]{walking_posetrack2.pdf}
%	\caption{Caption}
%	\label{fig:walking_posetrack}
%\end{figure}














%\begin{table}
%	\caption{Sample table title}
%	\label{}
%	\centering
%	\begin{tabular}{ccc}
%		\toprule
%		\multicolumn{2}{c}{Part}                   \\
%		\cmidrule(r){1-2}
%		& Mask Ratio     & PA-MPJPE  $\downarrow$\\
%		\midrule
%		 & \ding{55}  &    \\
%		 & \ &       \\
%
%		\bottomrule
%	\end{tabular}
%\end{table}



%\begin{table}
%	\caption{Sample table title}
%	\label{}
%	\centering
%	\begin{tabular}{cccccc}
%		\toprule
%		\multicolumn{2}{c}{Part}                   \\
%		\cmidrule(r){1-2}
%		&Temporal Transformer Layers   & Number of attention heads & Temporal Embedding& PA-MPJPE  $\downarrow$\\
%		\midrule
%		& 2  &   12&  & \\
%		    & 3 &      12& &\\
%		     & 3 &      6& &\\
%		      & 3 &      1&& \\
%		     & 4       & 12&   &\\
%	    & 6      & 12& & \\
%		\bottomrule
%	\end{tabular}
%\end{table}