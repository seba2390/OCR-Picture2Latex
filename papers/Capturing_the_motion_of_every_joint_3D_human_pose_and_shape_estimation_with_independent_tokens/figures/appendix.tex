\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage{neurips_2022}
\usepackage[nonatbib]{neurips_2022}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{bbding}
\usepackage{graphicx}
\usepackage{pifont}

\usepackage{color, colortbl}
%\usepackage{soul}
\usepackage{multicol}
%\usepackage{blindtext}
%\usepackage{here}
%\usepackage{caption}
%\usepackage{makecell}
\usepackage{multirow}
\usepackage{wrapfig}


\definecolor{Gray}{gray}{0.9}

\title{Formatting Instructions For NeurIPS 2022}
\title{3D Human Pose and Mesh Recovery using Body Joint Rotation Tokenization} %

\title{Attentive 3D Human Pose and Mesh Recovery using Body Joint Rotation, Shape and Camera Tokenization}

\title{3D Human Pose and Mesh Recovery using Body Joint Rotation, Shape and Camera Tokenization} %

\title{3D Human Pose and Mesh Recovery using Linear Joint Rotation Tokenization} %

\title{3D Human Pose and Mesh Tokenizer via Spaital and Temporal Transformer} 

\title{Capturing the Motion of Every Joint: 3D Human Pose and Mesh Recovery with Joint-level Temporal Learning} 

\title{Supplementary Material -- Capturing the Motion of Every Joint: 3D Human Pose and Mesh Recovery with Independent Tokens} 

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}

\maketitle



\section{Model setups}

{\bf Base model.} For the Base model, we use a hybrid architecture with ResNetv2\footnote{\url{https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/resnetv2.py}}~\cite{resnet:he2016deep} and Transformer\footnote{\url{https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py}}~\cite{transformer:vaswani2017attention}, based on the ImageNet pretrained ViT model\footnote{\url{https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_resnet50_224_in21k-6f7c7740.pth}}~\cite{vit:dosovitskiy2020image}. 
The transformer in the Base model has 6 Transformer blocks and the multi-head self-attention layer has 12 attention heads. 
The embedding dimension is 768 for each token vector and the hidden dimension in the FFN is 3072 (4 times w.r.t. the embedding dimension 768).  

{\bf Temporal Transformer.} As we find that increasing the number of layers brings a little improvement but with extra computational overhead,  we set the number of layers of Temporal Transformer as 3 for the trade-off. The number of attention heads is also set as 12. The temporal transformer is not pre-trained with any data.
We also add a learnable temporal embedding to retain the temporal position information of tokens in the time dimension. 
In the training, the length $T$ of video clips is 16 sampled at a interval of 8 from the original video data. In inference, we enlarge the bbox with a scale of 1.1 with respect to the original size of bbox. And in practice, we find setting $T$ to be larger in inference could improve the accuracy, with acceptable additional computational overhead. So we use $T=64$ to report the best model result (INT-2 model). Due to the length of learnable temporal embedding is initialized with 16, we interpolate the temporal embedding to the expected length when necessary.



\section{Progressive training scheme}

In this paper,  we develop an improved progressive training scheme adapting to our model, which consists of three training phases. 

\textbf{In the first phase (phase-1)}, we  expect the Base model and SMPL heads to produce accurate estimates for static images; so we train the model only using 2D image datasets and individual frames from Human3.6M~\cite{h36m:ionescu2013human3} and MPI-INF-3DHP~\cite{mpii3d:mehta2017monocular}. 

\textbf{In the second phase (phase-2)}, we take the pre-trained weights of Base model and SMPL heads from the phase-1 as the initialization, and then use the mixed 2D/3D and image/video datasets to train the whole model, including the Base model, Temporal Transformer and SMPL heads. In practice, we find that the generalization of the model gradually deteriorates in the middle and late periods in the second phase, even when trained with 3DPW~\cite{3dpw:von2018recovering} training set.
%That is, the model in later period performs worse on 3DPW model than earlier ones. 
And we further observe that the estimated pose and shape of human mesh become implausible in the later period of the training process. 
We empirically attribute such phenomena to that 1) a large proportion of training data has no annotations of pose and shape parameters, such as InstaVariety~\cite{insta:kanazawa2019learning} or PoseTrack~\cite{posetrack:andriluka2018posetrack}, and such data may dominate the model training in the later period; 2) domain gap exists among various datasets; 3) we do not leverage any prior reasonable SMPL pose and shape constraints on the tokens or the estimated parameters about the pose and shape. These factors may cause the conflicts between multiple objectives in the later stage of training, such as overfittng in $\mathcal{L}_{2D}$ making the model less constrained with reasonable SMPL pose or shape parameters.
%We make ablation study on the effect of 2D video datasets.  
Based on these observations and conjectures, we \textit{do not use 3DPW training set} in the second phase and develop the third training phase as follow. 

\textbf{In the third phase (phase-3)}, we use the pre-trained weights from the phase-2 and \textit{fine-tune} the whole model on mixed datasets only consisting of 3DPW and Human3.6m that have accurate pose and shape annotations as supervision.
This phase not only largely preserves the adaptability to in-the-wild scenarios that is learned in the phase-2, but also makes the model focus on learning to predict accurate and credible pose and shape values to fit the image appearance, 3D joint locations and 2D joint locations. 
Note that we do not separately fine-tune the model on each dataset and report the best performance on both datasets separately. 
Instead, we finally achieve a \textit{single} best model to report the results on 3DPW and Human3.6M, which has attained wider adaptability and good generalization.


The default values of the weights $w_{\theta}, w_{\beta}, w_{norm}, w_{3D}, w_{2D}, w_{temp}$ are 60, 0.06, 1, 600, 300 and 600 unless otherwise stated.
In the first phase, we use 4 Tesla V100 GPUs with a batch size of 120 for each GPU.  The $w_{temp}$ is set to 0.
In the second phase, we use 16 Tesla V100 GPUs to conduct distributed training (2 nodes and 8 GPUs for each node). The batch sizes for 3D video/2D video/2D image datasets are 4, 3 and 7 for each GPU. The $w_{temp}$ is set to 0.
In the third phase, we use 8 Tesla V100 GPUs with a batch size of 8 for each GPU. The $w_{norm}$ is set to 0.01.


\section{Examples of the in-the-wild and indoor scenes}

We show more qualitative results for some \textit{complex scenes in the wild}, including crowded scenes, fast human motion and occluded persons. In Fig.~\ref{fig:examples}, Fig.~\ref{fig:h36m} and Fig.~\ref{fig:example_motion}, we show the human mesh reconstruction results of the examples from PoseTrack~\cite{posetrack:andriluka2018posetrack} and Human3.6M~\cite{h36m:ionescu2013human3}. We also provide the video files on the supplementary materials. Please see the supplementary video files for more references. %As shown in the videos, the reconstructed mesh results by our model show natural and robust motion transitions.


\begin{figure}[t]
	\centering
	\includegraphics[width=1\linewidth]{examples.pdf}
	\caption{The human mesh reconstruction results on the examples from PoseTrack~\cite{posetrack:andriluka2018posetrack}.}
	\label{fig:examples}
\end{figure}



\begin{figure}[t]
	\centering
	\includegraphics[width=1\linewidth]{h36m.pdf}
	\caption{The human mesh reconstruction results on the examples from Human3.6M~\cite{h36m:ionescu2013human3}.}
	\label{fig:h36m}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\linewidth]{example_motion.pdf}
	\caption{The human mesh reconstruction results for some video clips sampled from PoseTrack~\cite{posetrack:andriluka2018posetrack}.}
	\label{fig:example_motion}
\end{figure}




\section{Broader Impact}

We propose a simple yet effective solution to address the 3D human pose and shape estimation problem, based on the design of independent tokens. This design may provide an up-and-coming alternative to predicting SMPL parameters due to its simpleness, effectiveness, and reasonable assumptions. The spirit behind this approach is to abstract 3D human pose, body shape and camera information from the image pixels as independent concepts. In a broad sense, the mid-level concept representation can be separated from the low-level image feature representation, which is more beneficial to the integration and processing of information. This approach may inspire other tasks to tokenize the mid-level concepts as independent representations and use the Transformer to conduct unified modeling. 
 In addition, we emphasize separating the joint rotations from the image feature, which may inspire future works to draw more attention to modeling the rotational motions of joints rather than their positional motions.

The applications of the proposed human mesh recovery method are widespread and unknown. We should be wary of the it being used for military purposes and the commercial projects that may violate personal privacy and mental health. Also, training such models relies on large amounts of data and expensive training costs, which increases carbon emissions.


{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{ref}
}





\end{document}