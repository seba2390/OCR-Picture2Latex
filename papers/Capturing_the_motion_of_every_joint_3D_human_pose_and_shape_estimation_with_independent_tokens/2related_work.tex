\section{Related work}

%\subsection{3D human pose and mesh reconstruction}
Great progress has been made in the monocular image and video based 3D human pose and shape estimation, thanks to the parametric human mesh models~\citep{smpl:loper2015smpl, smpl-x:pavlakos2019expressive,frank:joo2018total}, particularly the SMPL~\citep{smpl:loper2015smpl} and SMPL-X~\citep{smpl-x:pavlakos2019expressive} models. 
The mainstream parametric methods are usually can be classified as two categories: optimization-based and regression-based.  SMPLify~\citep{smplify:bogo2016keep} is the first automatic optimization-based approach. It fits SMPL to 2D detected keypoint, using strong data priors to optimize the SMPL parameters. SPIN~\citep{spin:kolotouros2019learning} proposes fitting within the training loop to produce pixel-accurate fittings, where the fittings are used in training instead of in test time.
There are also methods further using human silhouette~\citep{unite:lassner2017unite} or multi-views information~\citep{huang2017towards} to accomplish the optimization. 

Regression-based scheme has recently received extensive research~\citep{hmr:kanazawa2018end, hmmr:kanazawa2019learning, vibe:kocabas2020vibe, skeleton-disentangled:sun2019human, sim2real:doersch2019sim2real, tcmr:choi2021beyond, metro:lin2021end,  meshgrahormer:lin2021mesh}, due to its directness and effectiveness. HMR~\citep{hmr:kanazawa2018end}, is the representative regression-based methods, using an image encoder and regressor to predict the pose, shape and camera parameters. To train the model well and make sure the realistic of the pose and shape, the reprojection loss and adversarial loss are introduced to leverage unpaired 2D-to-3D supervision. 
In addition, several non-parameteric mesh regression methods are proposed to directly regress the mesh vertices coordinates, including Pose2Mesh~\citep{pose2mesh:choi2020pose2mesh}, Convolution Mesh Regression~\citep{meshregression:kolotouros2019convolutional}, I2L-MeshNet~\citep{i2l-meshnet:moon2020i2l}, and the Transformer-based METRO~\citep{metro:lin2021end} and Mesh Graphormer~\citep{meshgrahormer:lin2021mesh}. 

Beyond estimating pose and shape from a singe image, video-based methods consider to fully dig the temporal motion information hidden in video data to improve the accuracy and robustness.  HMMR~\citep{hmmr:kanazawa2019learning} learns the human dynamics to predict pose and shape for past and future frames. 
%and it also use video data from Internet with pseudo labels to improve performance. 
%Sun et al.~\citep{skeleton-disentangled:sun2019human} proposes to use self-attention temporal network to learn long and short-term temporal coherence in video. 
VIBE~\citep{vibe:kocabas2020vibe} encodes temporal feature using a GRU and adopts an adversarial learning framework to learn kinematically plausible motion from a large-scale motion capture dataset.
TCMR~\citep{tcmr:choi2021beyond} introduces PoseForecast to forecast additional temporal features from past and future frames without a current frame. 
MAED~\citep{maed:wan2021encoder} proposes to use a spatial-temporal encoder to learn temporally enhanced image features and regress the joint rotations following a defined kinematic topology.
In contrast to these methods, our goal is to encode joint-level features, shape and camera information separately, rather than encoding all the information into a unified image feature vector.  
Since we use independent tokens to encode the rotational information of each joint, we can model the \textit{inner temporal patterns} when each joint rotates over time. 
Compared with MAED, we make no assumptions about the \textit{directed} dependencies between rotations of joints, because its tree-based topology fails to capture important dependencies between \textit{non-adjacent} joints.
In our modeling scheme, the joint tokens can freely learn \textit{undirected} relationships between any pairs of joints from large-scale data and a given image. 

%\subsection{Transformers in vision tasks}

Transformer~\citep{transformer:vaswani2017attention} is proposed as a powerful model that is suitable for sequence-to-sequence modeling. 
Transformer has less inductive bias and shows powerful performance when trained with sufficient data. 
It is applied to various vision tasks including image classification~\citep{vit:dosovitskiy2020image, deit:touvron2021training, swin:liu2021swin}, object detection~\citep{detr:carion2020end, pix2seq:chen2021pix2seq}, segmentation~\citep{vistr:wang2021end, segformer:xie2021segformer}, video classification~\citep{vivit:arnab2021vivit}, 2D/3D human pose estimation~\citep{transpose:yang2021transpose, tokenpose:li2021tokenpose, li2021pose, hrformer:yuan2021hrformer, yang2021attend, mao2022poseur, zheng20213d} and 3D human mesh reconstruction~\citep{metro:lin2021end, meshgrahormer:lin2021mesh, maed:wan2021encoder}, etc. In this work, inspired by the token-based Transformer designs~\citep{bert:devlin2018bert,vit:dosovitskiy2020image,tokenpose:li2021tokenpose}, we use multiple independent tokens to represent the information with respect to joint 3D rotation, body shape and camera parameter. And we also use Transformer to conduct sequence-to-sequence temporal modeling.  

