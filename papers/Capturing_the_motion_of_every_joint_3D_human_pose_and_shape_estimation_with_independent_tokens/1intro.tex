\section{Introduction}

Capturing the motion of the human body pose has great values in widespread applications, such as movement analysis, human-computer interaction, films making, digital avatar animation, and virtual reality. 
Traditional marker-based motion capture system can acquire accurate movement information of humans, but is only applicable to limited scenes due to the time-consuming fitting process and prohibitively expensive costs. 
In contrast, markerless motion capture based on RGB image and video processing algorithms is a promising alternative that has attracted numerous research in the fields of deep learning and computer vision. 
%encoded with prior knowledge of realistic human pose and shape deforming
Especially, thanks to the parameteric SMPL model~\citep{smpl:loper2015smpl} and various diverse datasets with 3D annotations~\citep{h36m:ionescu2013human3, mpii3d:mehta2017monocular, 3dpw:von2018recovering}, remarkable progress has been made on monocular 3D human pose and shape estimation and motion capture. 
%such as~\citep{hmr:kanazawa2018end, vibe:kocabas2020vibe,  maed:wan2021encoder, romp:sun2021monocular, spin:kolotouros2019learning, pare:kocabas2021pare}.




Existing regression-based human mesh recovery methods are actually implicitly based on an assumption that predicting 3d body joint rotations and human shape strongly depends on the given image features. 
The pose and shape parameters are directly estimated from the image feature using MLP regressors. 
Nevertheless, due to the inherent ambiguity, the mapping from the 2D image feature to 3D pose and shape is an ill-posed problem.
To achieve accurate pose and shape estimation, it is necessary to initialize the mean pose and shape parameters and use an \textit{iterative residual regression} manner to reduce error. 
Such an end-to-end learning and inference scheme~\citep{hmr:kanazawa2018end} has been proven to be effective in practice, but ignores the temporal information and produces implausible human motions and unsatisfactory pose jitters for video streaming data. 
Video-based methods such as~\citep{hmmr:kanazawa2019learning,vibe:kocabas2020vibe, tcmr:choi2021beyond, mps-net:wei2022capturing} may leverage large-scale motion capture data as priors and exploit temporal information among different frames to penalize implausible motions. 
 They usually enhance singe-frame feature using a temporal encoder and then still use a deep regressor to predict SMPL parameters based on the temporally enhanced image feature,  as shown in the left subfigure of Fig.\ref{fig:temporal_contrast}. 
This scheme, however, is unable to focus on joint-level rotational motion specific to each joint, failing to ensure the temporal consistency of local joints. 
To address these problems, we attempt to understand the human 3D reconstruction from a causal perspective. 
We argue that assuming a still background, the primary causes behind the image pixel changes and human body appearance changes are 1) the motions of 3D joint rotations in human skeletal dynamics and 2) the viewpoint changes of the observer (camera). 
In fact, a prior human body model exists independently of a given specific image. And the 3D relative rotations of all joints (relative to the parent joint) and body shape can be abstracted beyond image pixels and independent of the image contents and observer views. In other words, the joint rotations cannot be ``seen'' and they are image-independent and viewpoint-independent concepts.  %Additionally, the motion of the combined pose is huge and highly complicated, while the rotation motion of each separate joint is more tracable and periodic.

\begin{figure}
	\centering
	\includegraphics[width=0.95\linewidth]{figures/simple_contrast.pdf}
	\caption{\textbf{Left:} Mainstream temporal-based human mesh methods, e.g.~\citep{hmmr:kanazawa2019learning,vibe:kocabas2020vibe, tcmr:choi2021beyond}, adopt a temporal encoder to mix temporal information from past and future frames and then regress the SMPL parameters from the \textit{temporally enhanced feature} for each frame. \textbf{Right:} Our method first acquires tokens of each joint in the time dimension and then separately capture the motion of each joint using a shared temporal encoder.
	}
	\label{fig:temporal_contrast}\vspace{-0.2in}
\end{figure}

Based on the considerations above, we propose a novel 3D human pose and shape estimation model based on \textbf{in}dependent \textbf{t}okens (INT). 
The core idea of the model is to introduce three types of independent tokens that specifically encode the 3D rotation information of every joint, the shape of human body and the information about camera. 
These initialized tokens learn prior knowledge and mutual relationships from large-scale training data, requiring neither an iterative regressor to take mean shape and pose as initial estimate~\citep{hmr:kanazawa2018end, spin:kolotouros2019learning, vibe:kocabas2020vibe, tcmr:choi2021beyond}, nor a kinematic topology decoder defined by human prior knowledge~\citep{maed:wan2021encoder} . 
Given an image as a conditional observation, these tokens are repeatedly updated by interacting with 2D image evidence using a Transformer~\citep{transformer:vaswani2017attention}. 
Finally, they are transformed into the posterior estimates of pose, shape and camera parameters. 
As a consequence, this method of abstracting joint rotation tokens from image pixels can represent the motion state of each joint and establish correlations in time dimension.
 Benefiting from this, we can separately capture the temporal rotational motion of every joint by sending the tokens of each joint at different timestamps to a temporal model.
In comparison to capturing the overall temporal changes in image features and the whole pose, this modeling scheme focuses on capturing separate rotational motions of all joints, which is conducive to maintaining the temporal coherence and rationality of each joint rotation.

We evaluate our model on the challenging 3DPW~\citep{3dpw:von2018recovering} benchmark and Human3.6m~\citep{h36m:ionescu2013human3}. 
Using vanilla ResNet-50 and Transformer architectures, our model obtains 42.0 mm error in PA-MPJPE metric for 3DPW, outperforming all state-of-the-art counterparts with a large margin. The same model obtains 38.4 mm error in PA-MPJPE metric for Human3.6m, which is on par with the state-of-the-art methods. 
Also, the qualitative results show that our model produces accurate pixel-alignment human mesh reconstructions for indoor or in-the-wild images, and shows fewer motion jitters in local joints when processing video data. 
We strongly encourage the readers to see the video results in the supplementary materials for reference and comparison.

 %simple and straightforward, We first and then extend it to capture separate rotational motion of each joint.

% 可视化prior learned pose and shape

