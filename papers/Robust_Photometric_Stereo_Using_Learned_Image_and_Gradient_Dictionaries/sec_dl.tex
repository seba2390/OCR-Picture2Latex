%!TEX root = PSviaDL.tex

In this section, we propose two adaptive dictionary learning methods for estimating the normal vectors of a surface from (possibly) noisy images, $Y$. Intuitively, these models seek to learn a locally sparse representation of the data with respect to a collection of learned basis ``atoms'' that capture the underlying local structure of the data.

\subsection{Preprocessing Images through Dictionary Learning (DLPI)}
Our first approach applies dictionary learning to the data in a preprocessing step before estimating the normal vectors. This formulation represents the input image data $Y$ as locally sparse in an adaptive dictionary domain---thereby removing non-idealities that are not well-represented by the dictionary. Specifically, we propose to solve the problem
\begin{align} \label{eq:dlpi}
\min_{v,B,D} & ~ \frac{1}{2} \left \| y -  v \right \|_2^2 + \lambda \left(\textstyle\sum_{j=1}^c \left \| P_j v  - D b_j \right \|_2^2 +  \mu^2 \left \| B \right \|_0 \right) \nonumber \\
\text{s.t.} & ~ \left \| d_i \right \|_2 = 1, \ \ \left \| b_j \right \|_{\infty} \leq a, ~~ \forall i,j.
\end{align}
%\ \ \text{rank}(R(d_i)) \leq r,
Here, $y = \textbf{vec}(Y)$ and $P_j$ is a matrix that extracts a (vectorized) 3D patch of dimensions $c_x \times c_y \times c_z$ from $v$, where $c_x$ and $c_y$ are the dimensions of the patches extracted from each image and $c_z$ is the number of distinct images whose patches are combined to form the 3D patch. $D \in \mathbb{R}^{c_x c_y c_z \times K}$ is a dictionary matrix whose columns $d_i$ are the (learned) dictionary atoms, and $B \in \mathbb{R}^{K \times c}$ is a sparse coding matrix whose columns $b_j$ define (usually sparse) linear combinations of dictionary atoms used to represent each patch. Also, $\left \|\cdot\right \|_0$ is the familiar $\ell_0$ ``norm", and $\lambda,\mu > 0$ are parameters. 

We impose the constraint $\|b_j\|_{\infty} \leq a$, where $a$ is typically very large, since \eqref{eq:dlpi} is non-coercive with respect to $B$, but the constraint is typically inactive in practice \cite{sairajfes2}. Without loss of generality, we impose a unit-norm constraint on the dictionary atoms $d_i$ to avoid scaling ambiguity between $D$ and $B$ \cite{kar}.
We allow the possibility that patches from $c_z > 1$ input images can be combined into a 3D patch to allow the dictionary atoms to learn correlated features between images, but one can set $c_z = 1$ to work with 2D per-image patches.

Once we have solved \eqref{eq:dlpi}, we reshape $v$ (back) into an $m_1 m_2 \times d$ matrix whose columns are vectorized (now preprocessed) images, and then we estimate the associated normal vectors using the standard least squares model \eqref{eq:ls}. Henceforth, we refer to this approach as the Dictionary Learning with Preprocessed Imgaes (DLPI) method.

\subsection{Normal Vector Computation through Dictionary Learning (DLNV)}
We next propose modifying \eqref{eq:ls} by applying an adaptive dictionary regularization term to the normal vectors, $N$, under the Lambertian model \eqref{eq:ls}. Specifically, we propose to solve the problem
\begin{align} \label{eq:dlnv}
\min_{n,B,D} & ~ \frac{1}{2} \left \| y - A n \right \|_2^2 + \lambda \left ( \begin{matrix} \sum_{j=1}^w \left \| P_j n - D b_j \right \|_2^2 \end{matrix} + \mu^2 \left \| B \right \|_0 \right ) \nonumber \\
\text{s.t.} & ~ \left \| d_i \right \|_2 = 1, \ \ \left \| b_j \right \|_{\infty} \leq a, ~~ \forall i,j.
\end{align}
Here $y = \textbf{vec}(Y)$, $A = L^T \otimes I$---where $\otimes$ denotes the Kronecker product and $I$ is the $m_1 m_2 \times m_1 m_2$ identity matrix---and $n = \textbf{vec}(N)$. Also, $P_j$ denotes a patch extraction matrix that extracts (vectorized) patches of dimensions $w_x \times w_y \times w_z$ from $n$. All other terms are defined analogously to the corresponding terms in \eqref{eq:dlpi} with appropriate dimensions. 

The dictionary learning terms in \eqref{eq:dlnv} encourage the estimated normal vectors to be well-represented by sparse linear combinations of a few (learned) dictionary atoms. Intuitively,  this acts as an adaptive regularization that yields normal vectors that are more robust to noise and other non-idealities in the data.
Henceforth, we refer to this approach as the Dictionary Learning on Normal Vectors (DLNV) method.






\subsection{Algorithms for DLPI and DLNV} \label{sec:dl_sol}
We propose solving \eqref{eq:dlpi} and \eqref{eq:dlnv}, respectively, via block coordinate descent-type algorithms where we alternate between updating $n$ and $v$, respectively, with $(D,B)$ fixed and then updating $(D,B)$ with $n$ or $v$ held fixed. We omit the $(D,B)$ updates here due to space considerations, but the precise update expressions can be found in \cite{sairajfes2,dinokat2016}. 

\vspace{-2mm}
\subsubsection{$v$ update}
Solving \eqref{eq:dlpi} for $v$ with $D$ and $B$ fixed yields the problem
\begin{equation} \label{vupdate}
\min_{v} ~ \frac{1}{2} \left \| y -  v \right \|_2^2 + \begin{matrix} \lambda  \sum_{j=1}^c \left \| P_j v  - D b_j \right \|_2^2 \end{matrix}.
\end{equation}
Equation \eqref{vupdate} is a simple least squares problem whose solution $v$ satisfies the normal equation
\begin{equation}\label{eq:vnormal}
\left ( I + \begin{matrix} 2 \lambda \sum_{j=1}^c P_j^T P_j \end{matrix} \right ) v = y + \begin{matrix} 2 \lambda \sum_{j=1}^c P_j^T D b_j \end{matrix},
\end{equation}
where $I$ denotes the identity matrix. The matrix pre-multiplying $v$ in \eqref{eq:vnormal} is diagonal, so its inverse can be cheaply computed, allowing us to efficiently update $v$.

\vspace{-2mm}
\subsubsection{$n$ update}
On the other hand, solving \eqref{eq:dlnv} for $n$ with $D$ and $B$ fixed yields the problem
\begin{equation}\label{nupdate}
\min_{n} ~ \frac{1}{2} \left \| y - A n \right \|_2^2 + \begin{matrix} \lambda \sum_{j=1}^w  \left \| P_j n - D b_j \right \|_2^2 \end{matrix}.
\end{equation}
Note that while \eqref{nupdate} is also a least squares problem, its normal equation cannot be easily inverted as in \eqref{vupdate} due to the presence of the $A$ matrix. We therefore adopt a proximal gradient scheme \cite{parboyd}. The cost function in \eqref{nupdate} can be written in the form $f(n)+g(n)$ where $f(n) = \frac{1}{2} \left \| y - A n \right \|_2^2$ and $g(n) = \lambda \sum_{j=1}^w \left \| P_j n - D b_j \right \|_2^2$. The proximal updates thus become
\begin{equation} \label{nproxupdate}
n^{k+1} = \textbf{prox}_{\tau g}(n^{k} - \tau \nabla f(n^{k})),
\end{equation}
where
\begin{equation} \label{nprox}
\textbf{prox}_{\tau g} (z) := \argmin_{x} \ \frac{1}{2} \left \| z - x \right \|_2^2 + \tau g(x).
\end{equation}
Define $\tilde{n}^{k} := n^{k} - \tau \nabla f(n^{k})$. Then \eqref{nproxupdate} and \eqref{nprox} imply that $n^{k+1}$ satisfies the normal equation
\begin{equation}\label{eq:nnormal}
 \left(I +  \begin{matrix} 2 \tau \lambda \sum_{j=1}^w P_j^T P_j \end{matrix} \right ) n^{k+1} = \tilde{n}^{k} + \begin{matrix} 2 \tau \lambda \sum_{j=1}^w P_j^T D b_j  \end{matrix}.
\vspace{1mm}
\end{equation}
As in \eqref{eq:vnormal}, the matrix multiplying $n^{k+1}$ in \eqref{eq:nnormal} is diagonal and can be efficiently inverted, yielding $n^{k+1}$. Note that proximal gradient is one of a wealth of available iterative schemes for minimzing  the (quadratic) objective \eqref{nupdate}.
