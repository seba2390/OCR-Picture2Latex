%!TEX root = PSviaDL.tex

We now empirically demonstrate the performance of our proposed methods on several real-world datasets. To obtain quantitative results, we rely primarily on the DiLiGenT dataset \cite{shi2016}. This dataset contains images of a variety of surfaces and provides the true normal vectors of each object, allowing us to evaluate the performance of each method against a ground truth. We quantify error by measuring the mean angular difference between true normal vectors and estimated normal vectors. 

For each experiment, we compare the results of our method to Wu \textit{et al}.'s robust PCA (RPCA) method \cite{wu2011}, Ikehata \textit{et al}.'s sparse regression (SR) method \cite{ikehata2012}, and Ikehata \textit{et al}.'s constrained bivariate regression (CBR) method \cite{ikehata2014}. We also compare against the simple least squares (LS) method \eqref{eq:ls}. With the exception of LS, each method contains one or more tunable parameters that dictate their performance. For each method, we sweep the parameters across a wide range of values, including any values recommended by the authors in this sweep. The reported results are the errors produced by the optimal parameter values.

To evaluate the ability of our method to robustly reject non-idealities, we add Poisson noise to the images in the original datasets. In each case, we run the experiment over multiple noise realizations and average the results.

\begin{figure*}
\centering
\begin{minipage}{0.46\textwidth}
\includegraphics[width=\columnwidth]{figures/sweep_snr_20_na_0_Dcats_all_wide.pdf}
\caption{Sweeping SNR on the DiLiGenT Cat \cite{shi2016} dataset with 20 images.}
\label{fig:sweep_snr_20_na_0_Dcats_all}
\end{minipage}
\quad 
\begin{minipage}{0.46\textwidth}
\includegraphics[width=\columnwidth]{figures/sweep_snr_20_na_0_hippo_all_wide.pdf}
\caption{Sweeping SNR on the Hippo dataset \cite{xiong2015shading} with 20 images.}
\label{fig:sweep_snr_20_na_0_hippo_all}
\end{minipage}

\vspace{2mm}

\begin{subfigure}[b]{0.105\textwidth}
  \includegraphics[width=\textwidth]{figures/sweep_snr_20_na_0_cat_all_1_2_gt.pdf}
  \caption{Truth}
\end{subfigure}
\hspace{-2mm}
\begin{subfigure}[b]{0.105\textwidth}
  \includegraphics[width=\textwidth]{figures/sweep_snr_20_na_0_cat_all_1_2_dlnv.pdf}
  \caption{\textbf{DLNV}}
\end{subfigure}
\hspace{-2mm}
\begin{subfigure}[b]{0.105\textwidth}
  \includegraphics[width=\textwidth]{figures/sweep_snr_20_na_0_cat_all_1_2_dlpi.pdf}
  \caption{\textbf{DLPI}}
\end{subfigure}
\begin{subfigure}[b]{0.105\textwidth}
  \includegraphics[width=\textwidth]{figures/sweep_snr_20_na_0_cat_all_1_2_dlnv_errors.pdf}
  \caption{DLNV}
\end{subfigure}
\hspace{-2mm}
\begin{subfigure}[b]{0.105\textwidth}
  \includegraphics[width=\textwidth]{figures/sweep_snr_20_na_0_cat_all_1_2_dlpi_errors.pdf}
  \caption{DLPI}
\end{subfigure}
\hspace{-2mm}
\begin{subfigure}[b]{0.105\textwidth}
  \includegraphics[width=\textwidth]{figures/sweep_snr_20_na_0_cat_all_1_2_sr_errors.pdf}
  \caption{SR}
\end{subfigure}
\hspace{-2mm}
\begin{subfigure}[b]{0.105\textwidth}
  \includegraphics[width=\textwidth]{figures/sweep_snr_20_na_0_cat_all_1_2_rpca_errors.pdf}
  \caption{RPCA}
\end{subfigure}
\hspace{-2mm}
\begin{subfigure}[b]{0.105\textwidth}
  \includegraphics[width=\textwidth]{figures/sweep_snr_20_na_0_cat_all_1_2_cbr_errors.pdf}
  \caption{CBR}
\end{subfigure}
\hspace{-2mm}
\begin{subfigure}[b]{0.105\textwidth}
  \includegraphics[width=\textwidth]{figures/sweep_snr_20_na_0_cat_all_1_2_ls_errors.pdf}
  \caption{LS}
\end{subfigure}
\begin{subfigure}[b]{0.0275\textwidth} 
  \includegraphics[width=\textwidth]{figures/colorbar.pdf}
  \caption*{}
\end{subfigure}
\caption{Normal vector plots and error maps computed from the Cat dataset \cite{xiong2015shading} with 20 images and SNR = 1 dB. Error maps plot angular error (in degrees) in the normal vectors at each point on surface.}
\label{fig:sweep_snr_20_na_0_cat_all_1_2}
\vspace{-3mm}
\end{figure*}





%\vspace{-3mm}
\subsection{Varying Noise Levels}

We first simulate the addition of Poisson noise to the images, varying the signal-to-noise-ratio (SNR). Figure~\ref{fig:sweep_snr_20_na_0_Dcats_all} illustrates the results of these simulations on a 20-image subset of the DiLiGenT Cat dataset.




As Figure~\ref{fig:sweep_snr_20_na_0_Dcats_all} shows, in the low SNR (high noise) regime, both dictionary learning approaches significantly outperform existing approaches and are able to produce much cleaner normal vectors.  The performance of all methods becomes comparable in the high SNR (low noise) regime, although our proposed dictionary learning based approaches are less sensitive to changes in the noise strength.








%\vspace{-3mm}

\subsection{Varying Number of Images}
Table~\ref{tab:sweep_nim_na_10_0_Dbears_all} illustrates the accuracy of the estimated normal vectors of each algorithm as a function of the number of images used in the reconstruction. We ran this experiment on the DiLiGenT Bear dataset, sweeping from 5 to 96 images and adding Poisson noise with 10 dB SNR.


Table 1 shows that our proposed DLNV and DLPI algorithms significantly outperform existing methods on small datasets, achieving nearly 15 degree improvements in mean angular error. These results imply that, while dictionary learning based approaches generally perform well in low SNR (high noise) regimes, they are particularly robust to noise on small datasets compared to existing methods. 



\begin{table}[ht!]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\cline{1-7}
No. of & \multirow{2}{*}{\bf{DLPI}} & \multirow{2}{*}{\bf{DLNV}} & \multirow{2}{*}{SR} & \multirow{2}{*}{RPCA} & \multirow{2}{*}{CBR} & \multirow{2}{*}{LS} \\ %\multicolumn{6}{c|}{Mean} \\
%\cline{2-7}
Images & & & & & & \\
\cline{1-7}
5 & \textbf{20.91} & 21.43 & 34.29 & 33.12 & 31.10 & 34.25  \\
%\cline{1-7}
%10 & \textbf{11.83} & 12.44 & 18.45 & 17.89 & 17.59 & 18.45  \\
\cline{1-7}
15 & \textbf{9.73} & 10.20 & 14.89 & 14.44 & 14.86 & 14.90 \\
%\cline{1-7}
%20 & \textbf{9.46} & 9.87 & 13.12 & 12.73 & 13.35 & 13.13  \\
\cline{1-7}
25 & \textbf{9.23} & 9.52 & 12.12 & 11.78 & 12.76 & 12.14 \\
\cline{1-7}
35 & \textbf{9.13} & 9.18 & 11.23 & 10.93 & 11.99 & 11.24  \\
\cline{1-7}
45 & 8.96 & \textbf{8.90} & 10.62 & 10.34 & 11.71 & 10.64  \\
\cline{1-7}
55 & 8.86 & \textbf{8.78} & 10.24 & 9.97 & 11.51 & 10.25  \\
\cline{1-7}
65 & 8.89 & \textbf{8.77} & 10.00 & 9.75 & 11.29 & 10.02  \\
\cline{1-7}
75 & 8.79 & \textbf{8.68} & 9.73 & 9.50 & 11.21 & 9.750 \\
\cline{1-7}
85 & 8.72 & \textbf{8.62} & 9.57 & 9.34 & 11.20 & 9.58  \\
\cline{1-7}
96 & 8.69 & \textbf{8.61} & 9.43 & 9.22 & 11.05 & 9.45 \\
\cline{1-7}
\end{tabular}
\caption{Mean angular error sweeping number of images on the DiLiGenT Bear dataset \cite{shi2016} with SNR = 10 dB.}
\label{tab:sweep_nim_na_10_0_Dbears_all}
\vspace{-4mm}
\end{table}



\subsection{Analysis of non-DiLiGenT datasets}
In addition to the DiLiGenT dataset, we also consider the dataset from\footnote{The data can be found at \url{http://vision.seas.harvard.edu/qsfs/Data.html}} \cite{xiong2015shading}. This dataset contains images of several real objects without ground truth normal vectors. To obtain an estimate of the ground truth normal vectors, we assume the objects in this dataset follow a truly Lambertian model and compute normal vectors from the uncorrupted dataset using the simple least squares model \eqref{eq:ls}. While the Lambertian assumption may not hold exactly, the objects are matte in appearance -- the primary characteristic of Lambertian surfaces -- so these vectors are a reasonable approximation of the true normal vectors. This approach allows us to isolate the robustness of each method to noise when our data otherwise perfectly follow the modeling assumptions. Figure~\ref{fig:sweep_snr_20_na_0_hippo_all} depicts the results of these experiments.



From Figure~\ref{fig:sweep_snr_20_na_0_hippo_all}, we see that, for high SNR cases where corruptions are minimal, all methods converge to (nearly) zero mean angular error, as expected, since most methods are based on a Lambertian model. However, in the low SNR (high noise) regime, we see that, as in our previous results, both proposed dictionary learning based methods are significantly more robust to noise and produce much more accurate reconstructions. Figure~\ref{fig:sweep_snr_20_na_0_cat_all_1_2} illustrates the normal vectors obtained by the dictionary learning based approaches and error maps of all methods on the Cat dataset with an SNR of 1dB. Intuitively, the proposed adaptive dictionary learning methods are able to learn local features of data that effectively denoise the images (DLPI) or normal vectors (DLNV).







