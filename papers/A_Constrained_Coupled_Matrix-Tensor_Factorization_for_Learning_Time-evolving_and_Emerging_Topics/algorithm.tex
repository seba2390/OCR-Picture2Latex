Even though, tensor decomposition is an NP-hard problem, here we provide an Alternating Least Squares (ALS) algorithm which solves the Constrained Coupled-Matrix Tensor factorization problem and converges to a locally optimal solution (as a property of the family of ALS algorithms).
Alternating Least Squares  is probably the most widely used algorithm and dates
back to the original papers by Carroll et.al \cite{carroll1970analysis} and Harshman \cite{harshman1970foundations}.
Alternating Least Squares method has been shown to be very efficient and competitive for PARAFAC decomposition \cite{tomasi2006comparison} and in practice works well \cite{kolda2009tensor}. 
This approach has the advantage that it can be applied to large scale problems.  Using ALS method,  we solve for each factor at a time while fixing all other factors.  If we seek to estimate $\factA$, it turns out that we need to concatenate the two pieces of the data \tensor and \sideMatrix, whose rows refer to matrix \factA, that is the matricized tensor $\tensor_{\factA}$ and matrix \sideMatrix, and we can then solve for \factA\ as


\begin{equation}\label{eq:coupled2}
\factA 
=
\begin{bmatrix} \tensor_{\factA}  \\
\ \sideMatrix
  \end{bmatrix}^{T}
  \Bigg(
  \begin{bmatrix} \core_{\factA}(\factB \otimes \factC) \\
  \factD
  \end{bmatrix}^{\dagger}
  \Bigg)^{T}
\end{equation}

Algorithm \ref{algo_als} shows our ALS algorithm to solve the constrained coupled-matrix tensor factorization, \ourAlgo.  
These constraints include non-negativity, sparsity and orthogonality imposed by $\factA \geq 0$, $\forall i \parallel \factA_{i} \parallel_{1} \leq \epsSparseA$ and  $\forall i, j \quad \factA_{i}^{T} \factA_{j} \leq \epsOrthogA \quad i \neq j$  respectively. 
Rather than alternating to solve each factor completely, we solve for each column of each factor independently. This is possible since the columns of each factor are independent and the constraints we consider can be specified to each column as well. A column in the factor of first mode, \factA, indicates a group of words and a column in \factB indicates a specific weeks in the lifetime of the forum.  It is important to note the effect of  specifying sparsity constraints on the columns rather than the whole matrix.  This means {\em sparsity will be spread uniformly across the whole matrix}.  
It is worth mentioning that our algorithm can allow any convex constraints to be placed for each factor. 
Another advantage of our algorithm is that it can be easily used for  PARAFAC decompositions instead of Tucker3 with minimal changes. To achieve this, instead of initializing core to random values in Line \ref{algo_als:init}, we set the core tensor to a super diagonal tensor. In addition, there is no need to estimate core tensor in each iteration and hence Line \ref{algo_als:solveCore1} and \ref{algo_als:solveCore} can be removed from the algorithm. 

\begin{algorithm}[ht!]
%\footnotesize 
\footnotesize
\begin{algorithmic}[1] 
\Statex {\bf Input:}  The tensor $\tensor \in {\mathbb R}^{\sizeModeA \times \sizeModeB \times \sizeModeC}$ and auxiliary matrix $\sideMatrix \in {\mathbb R}^{\sizeModeA \times \sizeSideMatrix}$
\Statex {\bf Output:}  Coupled Decompositions $\factA \in {\mathbb R}^{\sizeModeA \times \sizeFactA}, \factB \in {\mathbb R}^{\sizeModeB \times \sizeFactB}, \factC \in {\mathbb R}^{\sizeModeC \times \sizeFactC}, \factD \in {\mathbb R}^{ \sizeSideMatrix \times \sizeFactA  }$
\State Initialize $\factA, \factB, \factC, \factD, \core$ to non-negative random values \label{algo_als:init}
\While{convergence criterion is not met} \label{algo_als:p2_start}
\State $\factA \leftarrow \argminD\limits_{\factA} || [\tensor_{\factA} \quad \sideMatrix] - \factA [\core_{\factA}(\factC \otimes \factB)^{T} \quad \factD^{T} ] ||_{Fro}$\\ \indent\indent\indent Subject to: $\factA \geq 0$ and $\forall i \parallel \factA_{i} \parallel_{1} \leq \epsSparseA$ \\ \indent\indent\indent and $\forall i, j \quad \factA_{i}^{T} \factA_{j} \leq \epsOrthogA \quad i \neq j$ \label{algo_als:contsraints}
\State Normalize the columns of $\factA$

\State $\factB \leftarrow \argminD\limits_{\factB} ||\tensor_{\factB} - \factB \core_{\factB} (\factC \otimes \factA)^{T} ||_{Fro}$\\ \indent\indent\indent Subject to: $\factB \geq 0$ and $\forall i \parallel \factB_{i} \parallel_{1} \leq \epsSparseB$ \\ \indent\indent\indent and $\forall i, j \quad \factB_{i}^{T} \factB_{j} \leq \epsOrthogB \quad i \neq j$
\State Normalize the columns of $\factB$

\State $\factC \leftarrow \argminD\limits_{\factC} ||\tensor_{\factC} - \factC \core_{\factC} (\factB \otimes \factA)^{T} ||_{Fro}$\\ \indent\indent\indent Subject to: $\factC \geq 0$ and $\forall i \parallel \factC_{i} \parallel_{1} \leq \epsSparseC$ \\ \indent\indent\indent and $\forall i, j \quad \factC_{i}^{T} \factC_{j} \leq \epsOrthogC \quad i \neq j$
\State Normalize the columns of $\factC$


\State $\factD \leftarrow \argminD\limits_{\factD} ||\sideMatrix - \factA \factD^{T} ||_{Fro}$\\ \indent\indent\indent Subject to: $\factD \geq 0$ and $\forall i \parallel \factD_{i} \parallel_{1} \leq \epsSparseC$ \\ \indent\indent\indent and $\forall i, j \quad \factD_{i}^{T} \factD_{j} \leq \epsOrthogC \quad i \neq j$
\State Normalize the columns of $\factD$


\State $\core \leftarrow \argminD\limits_{\core} ||vec(\tensor) - (\factC \otimes \factB \otimes \factA)vec(\core)||_{Fro}$ \label{algo_als:solveCore1}  \\ \indent\indent\indent Subject to: $\core \geq 0$  and $\parallel \core \parallel_{1} \leq \epsSparseCore$ \label{algo_als:solveCore}

\EndWhile \label{algo_als:p2_end}

\State return \factA, \factB, \factC, \factD, \core
\end{algorithmic}
\caption{\label{algo_als} The Alternating Least Squares for Constrained Coupled Matrix-Tensor Factorization \ourAlgo}
\end{algorithm} 
\subsection{Running Time:}
Each step of our algorithm can be solved by any convex or least squares (LS) solvers; 
If we chose a LS solver or a non-negative least squares (NNLS) solver such as the one in the N-way Matlab Toolbox \cite{andersson2000n}, we would subsequently need to transform the unconstrained NNLS solution into a constrained one by using projected gradient descent. However, 
For flexibility, we used CVX,  a convex solver. 
In practice we observed that CVX is faster than using Least Squares solvers and projected gradient decent method combined. 
Each iteration of our algorithm has linear complexity with respect to the number of factors, and number of modes.
%and in practice it takes less than a minute to compute the factors. 
In addition, our algorithm solves for each column of each matrix independently and hence makes our algorithm faster. 
More precisely, assuming a Parafac decomposition on a tensor of size $[n,n,n$] with m factors, the running of to solve for each factor at each iteration will be $O(m * n ^3)$ in which $m$ is the number of factors and $n$ is the number of variables in each column.  In theory, the convex  programming can be solved in the cubic number of variables ($O(n^3)$) \cite{ye1989extension} but in practice it runs faster. So the total running time of our algorithm is $iterations * O(m * n^3)$.
In addition our algorithm converges in less than a few tens of iterations. Thus, the whole running time is reasonable.



