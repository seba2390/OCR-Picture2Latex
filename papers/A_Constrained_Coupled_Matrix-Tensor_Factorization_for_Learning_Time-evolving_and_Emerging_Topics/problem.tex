\label{sec:problem}
In a wide variety of applications, we have data that form a tensor
and have side information or metadata that may form matrices or other tensors.
For instance, suppose we have a (word, time, post number) tensor that indicates how many times a word was used in a specific time and specific post numbers. Usually, question answering platforms also have some metadata on the
questions/answers, for instance tags of the questions, that can form a (words, tags) matrix. Thus  we have a third-order tensor, $\tensor \in {\mathbb R}^{\sizeModeA \times \sizeModeB \times \sizeModeC}$ and a matrix $\sideMatrix \in {\mathbb R}^{\sizeModeA \times \sizeSideMatrix}$, coupled in the first mode of each and there is a one-to-one correspondence of elements in the first mode of the tensor and the matrix (``word'' mode in our case). The coupled-matrix and tensor factorization (CMTF) algorithms  jointly factorizes multiple data sets in the form of higher-order tensors and matrices by extracting a common latent structure from the shared mode.
Imposing a Tucker model yields:

\hide{
\begin{equation}\label{eq:coupled}
\min\limits_{a_{\sizeModeAIter}, b_{\sizeModeBIter}, c_{\sizeModeCIter}, d_{\sizeModeAIter}}  \parallel \tensor - \sum\limits_{i} \sum\limits_{j} \sum\limits_{k}  \coreSmall_{ijk} * a_i \circ b_j  \circ c_k   \parallel_{F}^{2}  + \parallel \sideMatrix - \sum\limits_{i} a_i d_{i}^{T}  \parallel_{F}^{2}
\end{equation}

More concisely
}

\begin{equation}\label{eq:coupledConc}
\min  \parallel\tensor - \core_{\times_{3}} \factC_{\times_{3}} \factB_{\times_{2}} \factA_{\times_{1}}\parallel_{F}^{2} + \parallel \sideMatrix - \factA\factD^{T}  \parallel_{F}^{2} 
\end{equation}

On the other hand if we impose a PARAFAC decomposition, we have
\begin{equation}\label{eq:coupled_cp}
\min\limits_{a_{r}, b_{r}, c_{r}, d_{r}}  \parallel \tensor - \sum\limits_{r}   a_r \circ b_r  \circ c_r   \parallel_{F}^{2}  + \parallel \sideMatrix - \sum\limits_{r} a_r d_{r}^{T}  \parallel_{F}^{2}
\end{equation}

\hide{
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.15\textwidth]{figures/coupled.pdf}
		\caption{Coupled Matrix-Tensor Factorization with a Tucker 3 model for the tensor}
		\label{fig:coupled}
	\end{center}
\end{figure}
}
The existing work on coupled-matrix tensor factorization only considers non-negativity constraints, e.g. $\factA \geq 0$. 
Non-negativity is an important feature of latent factors since many real world tensors have non-negative values and hidden components have a physical meaning only when non-negative. 

Although non-negativity improves interpretability,  in many applications it is not enough to make sense of the data. When the goal of factorization is to find the latent topics within the tensor and the matrix, we would like to find as many non-overlapping structures as possible. Non-overlapping latent components directly imply that the latent topics are concise and hence interpretable.  We can control the amount of overlap in latent components by imposing orthogonality constraint on each latent component. This means for the first mode, we would like the columns of the latent component \factA\ to be orthogonal, $\forall i, j \quad \factA_{i}^{T} \factA_{j} \leq \epsOrthogA \quad i \neq j$. If $ \epsOrthogA$ is set to $0$, this implies latent components should be completely orthogonal, while values greater than $0$ means some overlap is allowed. 

Furthermore, in practice we desire the factors to be sparse as well. Sparsity constraints improve parsimony and offer a simpler and hence more interpretable model.  We can impose sparsity constraint on all factors and on the core tensor as well. We enforce the sparsity constraint by imposing constraint on $\ell_{1}$ norm of each column in factor matrices and on the core tensor. Enforcing sparsity on each column of the factor matrices means sparsity is imposed uniformly on each latent component for each mode. 
Sparsity becomes specially  favorable  when it is imposed on the core tensor; meaning only a few latent components interact with each other. This removes redundancy and achieve compact sparse representations of the core and hence the core tensor will be easily interpretable. 

To the best of our knowledge we are the first to introduce the constraint coupled-matrix tensor factorization problem with non-negativity, sparsity and orthogonality constraints.  
Our intuition and constraints  are captured in a formal definition as follows.
\begin{problem}[]\label{prob:constrainedCMTF}
Given a tensor $\tensor \in {\mathbb R}^{\sizeModeA \times \sizeModeB \times \sizeModeC}$, auxiliary matrix $\sideMatrix \in {\mathbb R}^{\sizeModeA \times \sizeSideMatrix}$, and number of factors for each component \sizeFactA, \sizeFactB, \sizeFactC,  find the components \factA, \factB, \factC, \factD, and tensor \core\ such that
$$\min  \parallel\tensor - \core_{\times_{3}} \factC_{\times_{3}} \factB_{\times_{2}} \factA_{\times_{1}}\parallel_{F}^{2} + \parallel \sideMatrix - \factA\factD^{T}  \parallel_{F}^{2},$$
$$  \text{Subject to: }  \text{For each factor  } F \in \lbrace \factA, \factB, \factC, \factD \rbrace$$
$$ F \geq 0 \text{ and } \forall i \parallel F_{i} \parallel_{1} \leq \epsilon_{F1}, 
\text{ and } \forall i, j \quad F_{i}^{T} F_{j} \leq \epsilon_{F2 } \quad i \neq j  $$
$$ \text{For core tensor } \core, \core \geq 0,  \parallel \core \parallel_{1} \leq \epsilon_{\core},$$
\end{problem}

%In our problem we do not consider orthogonality for core tensor, since we want to allow interaction between same factors, but only a few of them. 
For the sake of interpretation, it is enough for core to be sparse, having a few non-zero elements. Lifting orthogonality constraint from core means we allow interaction between same factors, but only a few factors to interact with each other. 
%However with some changes, we can  imposed orthogonality constraints on core tensor in the same way we imposed orthogonality constraints on components. 
%In our experiments,  we only consider non-negativity and orthogonality for factors and non-negativity and sparsity for core tensor. 