In this section, we provide the notation we will use throughout the paper. In addition, we explain two popular tensor decompositions  canonical or PARAFAC  and Tucker 3. 
%\subsection{Notations}

\textbf{Notations}:
Vectors are denoted by boldface lower case letters, e.g \vectora. Matrices are denoted by boldface Capital letters, e.g \matrixa. Tensors are denoted by Calligraphic letters, e.g \tensora.  

%An entry of a vector \vectora, a matrix \matrixa, or a tensor \tensora\ is denoted by $\element_{i}, \element_{ij}, \element_{ijk}$.  Let  $\tensor_{\matrixa}$ be the matricization of \tensor in the first mode. The Kronecker product of two matrices is denoted by $\matrixa \otimes \matrixb$. The $n$ mode product is denoted by $\times_{n}$. The outer product of two vectors is denoted by $\circ$.  $\parallel \matrixa \parallel_{F}$ denotes the Frobenius norm of matrix \matrixa.  Moore-Penrose Pseudoinverse of \matrixa is denoted by $\matrixa^{\dagger}$


\begin{center}
\footnotesize
 \begin{tabular}{||c | c ||} 
 \hline
 \textbf{Symbol} & \textbf{Definition} \\ [0.5ex] 
 \hline\hline
  $\element_{i}$ & An entry of a vector \vectora (same for matrix and tensor)\\  \hline
 % $\element_{ij}$ & An entry of a matrix \matrixa \\  \hline
 % $\element_{ijk}$ & An entry of a tensor \tensora \\  \hline
   $\tensor_{\matrixa}$ & Matricization of \tensor in the first mode \\ \hline
   $\matrixa \otimes \matrixb$ &  Kronecker product of two matrices  \\ \hline
   $\times_{n}$ & The $n$ mode product \\ \hline
   $\circ$ & Outer product of two vectors  \\ \hline
   $\parallel \matrixa \parallel_{F}$  & Frobenius norm of matrix \matrixa \\ \hline
    $\matrixa^{\dagger}$ & Moore-Penrose Pseudoinverse of \matrixa \\ [1ex]  \hline
\end{tabular}
\end{center}

%\subsection{Tensor Decomposition}


%\subsection{CP/PARAFAC Tensor Decomposition}
\textbf{CP/PARAFAC Tensor Decomposition:}
Given tensor \tensor, we can analyze into a sum of $F$ rank-one factors  using the CP/PARAFAC decomposition.
\hide{
and obtain a sum of $F$ rank-one tensors as the following:
$$
\footnotesize
\tensor \approx \sum \limits_{f=1}^F \mathbf{a}_f  \circ \mathbf{b}_f \circ \mathbf{c}_f
$$
}
Typically, in order to compute the CP/PARAFAC decomposition, we solve the following optimization problem:
\[ \footnotesize
\min\limits_{a, b, c}  \parallel \tensor - \sum\limits_{f}  \mathbf{a}_f \circ \mathbf{b}_f \circ \mathbf{c}_f   \parallel_{F}^{2}
\]
which minimizes the Frobenius norm of the difference between the tensor and the model, where $\mathbf{a}_f$, $\mathbf{b}_f$, and $\mathbf{c}_f$ are latent vectors that correspond to words, time, and post numbers.  

\hide{
Figure \ref{fig:parafac} illustrates how we model and analyze the forum data using PARAFAC decomposition. 

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.15\textwidth]{figures/parafac.pdf}
		\caption{The PARAFAC decomposition}
		\label{fig:parafac}
	\end{center}
\end{figure}
}

To decide about the right number of latent factors ($F$), we used AutoTen \cite{papalexakis2016automatic} which allows us to find more structured latent embeddings in the data. 
Each latent factor of the embeddings defines a pattern in the forum meaning a set of words (topic) used in a specific time in specific post numbers. If a set of words only appear in a specific short time period (with a bursty distribution), we consider it a topic triggered by an external event at a specific time, Figure \ref{fig:ligo}. On the other hand, if a set of words appear all the time but only specific post numbers we consider it as a part of the evolutionary topics. The post number of these topics determines the {\em difficulty of the topic}. For example, if a topic appears in low post numbers, it corresponds to basic topics only discussed by newbies, Figure \ref{fig:coup_parafac_c1word}.  Whereas  Figure \ref{fig:coup_parafac_c3word} shows that the topics mainly appearing in  high post numbers written by advanced users who have contributed a lot in the past.
PARAFAC is unique under mild conditions.  This is important because it allows us to uniquely unravel a
large number of possibly overlapping co-clusters that are hidden in the data. As a result, having a tensor with (word, time, post number) modes, one can view  PARAFAC as a soft clustering that detects  groups of words that tend to appear together in certain time intervals and specific post numbers. Thus, in this case co-clustering  is taking advantage of the ternary relationship between words, time stamps, and post numbers which makes it a good candidate for topic modelling applications. 
In a similar work, Agrawal et al. \cite{Agrawal:2017:HWS, Agrawal:2015:OGB:2817946.2820604, Agrawal:2015:SDW:2740908.2743060, Agrawal:2015:WSN:2783258.2788571}  used PARAFAC as a co-clustering to model the comparison between the results of different search engines, based on emerging latent topics.  For a set of queries, they create a (query, keyword, date, search engine) tensor and use the CP decomposition to create latent representations of search engines in the same space.

%\subsection{Tucker 3 Tensor Decomposition}
\textbf{Tucker 3 Tensor Decomposition:}
In addition to CP/PARAFAC, the other most widely used tensor decomposition model is the Tucker model \cite{tucker1966some}. In the original paper, Tucker introduced three models; in this paper we are going to focus on the third one, also known as Tucker 3, which can be seen as a generalization of CP/PARAFAC. In Tucker 3 the tensor is decomposed into
$ \footnotesize
\tensor \approx \sum\limits_{i} \sum\limits_{j} \sum\limits_{k}  \coreSmall_{ijk} * \mathbf{a}_i \circ \mathbf{b}_j  \circ \mathbf{c}_k   
$
 where now the factor vectors are combined using a core tensor $\core$. The $(i,j,k)$ entry $\coreSmall_{ijk}$ of the core tensor is indicating the interaction between the $(i,j,k)$ latent factors. We can write CP/PARAFAC as a special Tucker 3 model where th he core is super-diagonal, i.e., it only has non-zero values in $(i,i,i)$.

The existence of the core tensor $\core$ in Tucker 3 is key to our application. Due to this core, Tucker 3 is able to capture {\em interactions between latent components} which, as we will see in the rest of the paper, are important for topic discovery. Sparsity on core: Imposing sparsity constraint on core tensor, as in e.g., \cite{gilpin2016some}, improves the interpretability of the components since fewer interactions are included. 
%Non-sparse core tensor makes the interaction and relationship between factors ambiguous specially in decision based applications such as classification.  A sparse core tensor with fewer interactions reduces this ambiguity.  In addition it yields a more compact representation and compresses the data by considering only non-zero values of the core tensor. 

In tensor/matrix form, the Tucker 3 model can be written as
$
\tensor \approx \core_{\times_{3}} \factC_{\times_{3}} \factB_{\times_{2}} \factA_{\times_{1}}
$
where ${\times_{N}}$ is the $N$-mode tensor-matrix product.
% and essentially multiplies the $N$-th mode of a tensor with the corresponding mode of a matrix, similar to matrix multiplication. Tucker 3 is shown pictorially in Figure \ref{fig:tucker}.
\hide{
\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.15\textwidth]{figures/tucker.pdf}
		\caption{The Tucker 3 decomposition}
		\label{fig:tucker}
	\end{center}
\end{figure}
}