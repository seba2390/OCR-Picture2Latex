We focus on question and answers related to the field of physics and python programming in \texttt{Stack
Exchange}. 
 We start by discussing our datasets in details, and then we present how we apply our tensor decomposition techniques on this datasets to find topics.

\subsection{Data}
\begin{figure*}
\captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
  \centering
    \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafac_1word}
                \caption{Words}
                \label{fig:parafac_c1word}
        \end{subfigure}%
        \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafac_1time}
                \caption{Time in Weeks}
                \label{fig:parafac_c1time}
        \end{subfigure}%
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafac_1post}
                \caption{Log Post Number}
                \label{fig:parafac_c1post}
        \end{subfigure}%  
        
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafac_2word}
                \caption{Words}
                \label{fig:parafac_c2word}
        \end{subfigure}%
        \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafac_2time}
                \caption{Time in Weeks}
                \label{fig:parafac_c2time}
        \end{subfigure}%
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafac_2post}
                \caption{Log Post Number}
                \label{fig:parafac_c2post}
        \end{subfigure}%           
        \caption{\label{fig:res_parafac}An example of two components extracted by \paraNS\ algorithm on Physics dataset. The two components are similar in word, time and post number modes. The words gravity, time, light, speed, wave, particle, and energy are frequent in both components. }
\end{figure*}

\vspace{-0pt}

\begin{figure*}
\captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
  \centering   
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_1word}
                \caption{Words}
                \label{fig:coup_parafac_c1word}
        \end{subfigure}%
        \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_1time}
                \caption{Time in Weeks}
                \label{fig:coup_parafac_c1time}
        \end{subfigure}%
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_1post}
                \caption{Log Post Number}
                \label{fig:coup_parafac_c1post}
        \end{subfigure}%   
               
                \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_2word}
                \caption{Words}
                \label{fig:coup_parafac_c2word}
        \end{subfigure}%
        \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_2time}
                \caption{Time in Weeks}
                \label{fig:coup_parafac_c2time}
        \end{subfigure}%
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_2post}
                \caption{Log Post Number}
                \label{fig:coup_parafac_c2post}
        \end{subfigure}%
        
        \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_0word}
                \caption{Words}
                \label{fig:coup_parafac_c0word}
        \end{subfigure}%
        \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_0time}
                \caption{Time in Weeks}
                \label{fig:coup_parafac_c0time}
        \end{subfigure}%
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_0post}
                \caption{Log Post Number}
                \label{fig:coup_parafac_c0post}
        \end{subfigure}
        
         \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_3word}
                \caption{Words}
                \label{fig:coup_parafac_c3word}
        \end{subfigure}%
        \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_3time}
                \caption{Time in Weeks}
                \label{fig:coup_parafac_c3time}
        \end{subfigure}%
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_parafac_3post}
                \caption{Log Post Number}
                \label{fig:coup_parafac_c3post}
        \end{subfigure}%
        \caption{\label{fig:coup_parafac}An example of four components extracted by \ourAlgo\ algorithm on physics dataset. All components have distinct set of words and distinct post numbers. }
\end{figure*}
\vspace{-0pt}

\begin{figure*}
\captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
  \centering   
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafacNoConstraint11word}
                \caption{Words}
                \label{fig:python_parafac_c1word}
        \end{subfigure}%
        \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafacNoConstraint11time}
                \caption{Time in Weeks}
                \label{fig:python_parafac_c1time}
        \end{subfigure}%
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafacNoConstraint11post} 
                \caption{Post Number}
                \label{fig:python_parafac_c1post}
        \end{subfigure}%          
        
                \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafacNoConstraint21word}
                \caption{Words}
                \label{fig:python_parafac_c2word}
        \end{subfigure}%
        \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafacNoConstraint21time}
                \caption{Time in Weeks}
                \label{fig:python_parafac_c2time}
        \end{subfigure}%
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/parafacNoConstraint21post}
                \caption{Post Number}
                \label{fig:python_parafac_c2post}
        \end{subfigure}%
        \caption{\label{fig:python_parafac}An example of two components extracted by \paraNS\ algorithm on programming dataset.  The two components are similar in word, time and post number modes. 
       % All components have distinct set of words and distinct post numbers. 
        }
\end{figure*}
\vspace{-0pt}

\begin{figure*}
\captionsetup[subfigure]{aboveskip=-1pt,belowskip=-1pt}
  \centering   
\begin{subfigure}[b]{0.31\textwidth} 
                \includegraphics[width=\linewidth]{figures/python_coup_parafac_0_word}
                \caption{Words}
                \label{fig:python_coup_parafac_c1word}
        \end{subfigure}%
        \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/chertapril2723time}
                \caption{Time in Weeks}
                \label{fig:python_coup_parafac_c1time}
        \end{subfigure}%
\begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/chertapril2723post} 
                \caption{Post Number}
                \label{fig:python_coup_parafac_c1post}
        \end{subfigure}%          
        
                \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/python_coup_parafac_1_word}
                \caption{Words}
                \label{fig:python_coup_parafac_c2word}
        \end{subfigure}%
        \begin{subfigure}[b]{0.31\textwidth}
                \includegraphics[width=\linewidth]{figures/april273time}
                \caption{Time in Weeks}
                \label{fig:python_coup_parafac_c2time}
        \end{subfigure}%
\begin{subfigure}[b]{0.31\textwidth}
                %\includegraphics[width=\linewidth]{figures/python_coup_parafac_1_post}
                \includegraphics[width=\linewidth]{figures/april273post}
                \caption{Post Number}
                \label{fig:python_coup_parafac_c2post}
        \end{subfigure}%
        \caption{\label{fig:python_coup_parafac}An example of four components extracted by \ourAlgo\ algorithm on programming dataset. 
       % All components have distinct set of words and distinct post numbers. 
        }
\end{figure*}
\vspace{-0pt}

\texttt{Stack Exchange} is a question answering website created in 2008. It features questions and
answers on a wide range of topics from mathematics and programming to cooking and movies.  \texttt{Stack
Exchange} allows each question  to be annotated with one or more terms (tags) indicating the subject
matter of the question.  
%We collected all 410740 questions which were tagged as a Python question from 2008 to 2016.
We used the latest Physics and programming Data Dump \
\footnote{//archive.org/details/stackexchange} in \textbf{\texttt{Stack
Exchange}}. We only consider the questions which have at least one tag (almost $30\,000$ questions),  and
we only considered frequent words which appeared more than 100 times in the forum. We also stemmed all the words. 

\subsubsection{\textbf{Physics Stack Exchange}}
 From the physics forum data, we created a tensor 
(multi-way array) \tensor\ with three modes (word, time, post number) of size $1351  \times 304 \times 9$.
When a user \user\ uses word \word\ at week \tim\ in his $p^{th}$ post, we will increase $\tensor ( \word,\tim, \log(p))$.
Thus, the $(i,j,k)$ value of Tensor \tensor\ indicates how many times word $i$ was used at week $j$ in $\log(k^{th})$ posts of
all users. Note that post number is relative to each users' sign up date. Hence, if a user
signs up and writes a question or answer, her post number would be 1.

An important aspect of considering time along with topics is the fact that the temporal information of
the topics helps us understand the topic better. For example, the word "jobs" relates to employment,
but after  October 5, 2011, the word jobs may refer to "Steve Jobs". This is the key reason for why
we use time as another dimension, we can get more insight about topics and distinguish evolutionary
topics from event driven topics which follow a bursty distribution.

In our application, beside the words, post numbers and time stamps, we also have the tags associated
to each question by the users. We can use question tags as a side information or metadata as a word-tag
matrix. This matrix indicates how many times each word has been used for a specific tag. We denote this
matrix by \sideMatrix. Our auxiliary matrix \sideMatrix, has two modes (words and tags) of size
$1351 \times 527$.  $\sideMatrix_{i,j}$  indicates the number of times word $i$ has been used in a post
with tag $j$.

\subsubsection{\textbf{Programming Stack Exchange}}
The programming Stack Exchange forum includes all the questions related to programming including Java, Python, C, R and other programming languages. We decided to only include the questions including "python" tag.  
% explain the reason
From the programming questions having python tag, we created a tensor 
(multi-way array) \tensor\ with three modes (word, time, post number) of size $432  \times 411 \times 50$.
When a user \user\ uses word \word\ at week \tim\ in his $p^{th}$ post, we will increase $\tensor ( \word,\tim, p)$. Beside the words, post numbers and time stamps, we also have the tags (except python tag) associated to each question by the users.  Similar to Physics data, we created an auxiliary word-tag matrix which indicates how many times each word has been used for a specific tag. The auxiliary matrix \sideMatrix, has two modes (words and tags) of size
$432 \times 30$ (we only kept top 30 tags). 



\subsection{Experimental Evaluation}
 In this part, we evaluate our algorithms under CP/PARAFAC and Tucker3 decomposition models for CMTF.
 We implemented our algorithm in Matlab and used CVX package to solve each step of Algorithm \ref{algo_als}.
 All experiments were carried out on a machine with a 2.4 GHZ CPU, 16 GB RAM, running CentOS Linux 7. 
 \textbf{ Our dataset and our code are immediately and freely available for download}\footnote{
     \url{https://github.com/ConCMTF/ConCMTF}}
 .We compare our results to non-negative PARAFAC decomposition \cite{morup2006sparse} and sparse
 non-negative Tucker3 \cite{papalexakis2012parcube}. We refer to them  as \paraNS\ and \tuckerNS\
 respectively. To decide the right number of latent factors ($F$) to be extracted in each algorithm,
 we used AutoTen \cite{papalexakis2016automatic} which allows us to find more structured latent
 embeddings in the data. For each component that is obtained from each decomposition, we do 2-means
 clustering on the vector associated with the word mode. Then, we take the cluster with the maximum
 mean and choose the cut-off value to be equal to the smallest value in that cluster, such that any
 value below that threshold is zeroed out. In this way, we avoid interpreting the noise words (i.e.
 those with very small values) as part of the topics.

\textbf{\paraNS\ vs. \ourAlgo\ with PARAFAC:}\\
Figure \ref{fig:res_parafac} shows two components selected from obtained components  using \paraNS\
algorithm on Physics dataset. We observe that in these two decompositions, there are overlaps in the set of words found
by \paraNS\  as well as overlap in time and post number modes. In fact,  post numbers have identical
trends  and the words gravity, time, light, speed, wave, fraction, particle, and energy are among
frequent words in both components. Moreover, the set of words in both components include a (relatively)
large number of words and the word factors are very dense. If the goal of factorization is to find latent
structure and patterns in the data, these two components are very similar and hence give us the same
structure and little information.

We also used our algorithm, \ourAlgo, assuming a CP/PARAFAC decomposition.  For this decomposition, we
only imposed non-negativity and orthogonality constraint on components \factA, \factB, \factC, and
\factD\ with \epsOrthogA\ = 0.05, \epsOrthogB\ = 0.6, \epsOrthogC\ = 0.2, and \epsOrthogD\ = 0.2. The
intuition behind this is that we we would like to find components which are distinct in their set of
words and the level of maturity (post number values). However, we allow decompositions to have overlap
in the time mode as we seek patterns in any period of forums lifetime. 

Figure \ref{fig:coup_parafac} illustrates the components produced by \ourAlgo  on Physics dataset. The set of words in each component are sparse and they do not share many words  as it was in the case
in \paraNS\ components. The post numbers of each component are non-overlapping as well. The first word
component depicts the words ``mass", ``wave", ``equation", ``velocity",  ``particle", ``time", 
``angular", ``slow",  and ``oscillation"  which were used in very low post number (i.e. by
new users). These are in fact basic topics in physics. The second component  covers topics related to
harmonic motion and waves topics. Compared to the first component these words appear in larger post
numbers, i.e. they are posted by more advanced users. The topic of the third component is mainly
the first law of thermodynamics discussed mainly by advanced users with large post numbers. The last
component included the words ``inductor", ``flux", ``ring", ``diameter", ``transform", ``collide",
``magnetic", ``field", and ``circular". These words are related to "Toroidal inductors and transformers"
which only appeared in very large post number and by very advanced users.

\begin{figure}
\begin{center}
 \begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[width=\linewidth]{figures/tucker_0word}
                \caption{\tuckerNS}
                \label{fig:tucker_0word}
        \end{subfigure}%
        \begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[width=\linewidth]{figures/tucker_1word}
                \caption{\tuckerNS}
                \label{fig:tucker_1word}
        \end{subfigure}%
        
\begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_tucker0word}
                \caption{\ourAlgo}
                \label{fig:coup_tucker_0word}
        \end{subfigure}%  
        \begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[width=\linewidth]{figures/coup_tucker1word}
                \caption{\ourAlgo}
                \label{fig:coup_tucker_1word}
        \end{subfigure}%  
 \caption{\label{fig:res_tucker}An example of two components extracted by \tuckerNS\ and  \ourAlgo\ algorithm. }
 \end{center}
\end{figure}


Figure \ref{fig:ligo} is an example of a component which only appeared in a specific time period and
moreover in specific post numbers (large post numbers).  This pattern indicates words discussed in
response to an external event.  The set of words in this component consists of ``gravity", ``Ligo",
``detection", ``laser", ``hole", ``theory", ``space", ``time", ``mass", etc. The peak in time mode
corresponds to Feb, 2016. This is the time that the detection of gravitational waves was
announced by Ligo lab.

Figure \ref{fig:res_parafac} shows two components selected from obtained components  using \paraNS\
algorithm on Programming dataset. As illustrated, these two decompositions, there are overlaps in the set of words found by \paraNS\  as well as overlap in time and post number modes. On the other hand, figure \ref{fig:python_coup_parafac} shows two components extracted by our algorithm.  As illustrated in the figure, 
the set of words in each component are sparse and they do not share many words  and each component shows semantically coherent topics. The first topic, figure \ref{fig:python_coup_parafac_c1word} ,includes words related to multiprocessing in python such as thread, multiprocessing subprocess, queue, virtualenv, asynchronous, and etc.  Figure \ref{fig:python_coup_parafac_c1post} shows multiprocessing topics have a presence  across various post numbers. This reveals that such a topic is of interst regardless the expertise of users.  The second topic, figure  \ref{fig:python_coup_parafac_c2word},  includes topics related to web crawling such as selenuim, web-driver, Firefox, flask, Twitter, url, css, and etc. The associated post number, figure \ref{fig:python_coup_parafac_c2post} releavs that this topic is mainly of interest of new users with lower experience. 

\textbf{\tuckerNS\ vs \ourAlgo\ with Tucker3:}\\
Figure \ref{fig:res_tucker} illustrates the decompositions obtained by \tuckerNS\  and \ourAlgo\ when assuming a Tucker decomposition with 12, 4, and 4 latent factors for each mode.
 We can observe that the  words produced  in \tuckerNS\ algorithm are not distinct and have a big overlap (Figure \ref{fig:tucker_0word} and \ref{fig:tucker_1word}). 
On the other hand, our algorithm \ourAlgo, is 
able to find more distinct and coherent set of words
 (Figure \ref{fig:coup_tucker_0word} and \ref{fig:coup_tucker_1word}. 
Moreover, using \tuckerNS, the core tensor has values in almost $55\%$ 
of the core entries, making the interpretation difficult. 
On the contrary, only $20\%$ of the core entries have valueses using \ourAlgo. 

%We sorted the words based on their frequency such that most frequent words appear with higher indices in word mode and as we see most of the latent factors have only values for frequent words. This is a natural result of  \tuckerNS\ since it tried to only optimize the error, meaning optimizing for large values in the tensor (frequent words). As a result  \tuckerNS\ in not very successful in finding the hidden structure of the data. On the other hand, our algorithm \ourAlgo, is able to find more distinct latent factors for all modes including the word mode. The results are depicted in figure. \reminder{and sparsity on the core tensor, mention the amount of sparsity}


\section{User Study}
To evaluate the quality of the topics found by our algorithm, we conducted a user study with two goals: 1) evaluate the cohesion of each learning unit, and 2) evaluate the ordering of the units. In the following sub-sections we present the details of our conducted user-study and the results of our study.

%\subsection{Survey Participants}
\textbf{Survey Participants:}
We recruited 10 volunteers to participate in our survey. Six had a PhD in computer science. Two with a PhD in Physics and Space Physics respectively, and finally two participant had mechanical engineering background.
None of the authors provided any judgements.

%\subsection{Survey Design}
\textbf{Survey Design:}
We selected 5 components from the set of all components resulted from \ourAlgo\ assuming a CP/PARAFAC decomposition.  The reason we decided to  include a limited number of components in the study is that we noticed  some set of the words associated with some components are quite  advanced and the participants are most likely not familiar with them. For example, the following component was excluded from the survey: ``bra, preserve, ket, property, configure, rigor, Hilbert, hermitian, electrodynamic". This set of words refer to  quantum mechanics and Bra-ket notation. To determine which topics are advanced and not familiar to our participants, we asked a scientist with a PhD in physics (who did not participate in our survey) to identify the less known topics.  
 We showed the remaining set of  words  to our participants and asked them to count number of odd words in each set of words produced by \ourAlgo\ assuming PARAFAC decomposition.

We asked the following question to our volunteers:

\textbf{Q1:}  Count the number of odd words in each topic.  

%\subsection{Survey Results}
\begin {table}
\footnotesize
\begin{center}
 \begin{tabular}{c c c c c c } 
 \hline
 \textbf{a} & \textbf{Min}  & \textbf{Max} & \textbf{Median} & \textbf{Mean} & \textbf{\# Concepts}\\ \hline %[0.5ex] 
 Unit 1 & 1 & 4 & 2 & 2.3 & 11  \\
 Unit 2 & 1 & 4 & 2 & 2.1 &  11\\
 Unit 3 & 0 & 1 & 0 & 0.3 &  12\\
 Unit 4 & 1 & 4 & 2 & 2.5 &  12\\
 Unit 5 & 1 & 5 & 1.5 & 2 &   13\\
 \hline
\end{tabular}
\caption {\label{table_survey}Survey results for Q1 (number of odd words in each unit)}
\end{center}
\end {table}

\textbf{Survey Results:}
Table \ref{table_survey} summarizes the results of our survey including the min, max, mean,
and the median of values that our participants reported as the number
of odd words in each topic (unit).
%Unit 1 consists of the words tidal, wide, gravity, wave, moon, force,  mass, spherical, acceleration,  newton, space, and bond. Most of these words refer to the "Moon Tides" topic in physics. 
In general, we observe that for the all of our units, the number of odd words is very low, demonstrating
good cohesion in each set of words. Unit 3 has the most number of odd words on average. This unit
consists of the words such as ``inductor, flux, ring,  difficulty, collide, avoid, wirewind, field ,
magnetic, diameter,  illustrate, circular". Most of our participants indicated ``difficulty, avoid,
illustrate, and collide" as odd words. These are the words that do not correspond to physics concepts and were not
excluded from the data. The third unit has the least number of odd words. This unit consists of words
such as mass, wave, equation, velocity, particle, time, psi, angular, slow, length, and transmit. 

It is also important to evaluate the inter-judge  agreement in a survey like ours. Due to the nature of
the ratings, an appropriate way of analyzing the agreement is by using Krippendorff’s $\alpha$
statistical measure \cite{krippendorff1970estimating}, which is applicable to the current scenario of
judges assigning a value to a specific variable. The overall agreement measured by Krippendorff’s
$\alpha$ for our ten judges turns out to be 0.32. This indicates that there is fair but imperfect
agreement. When we look at the agreement of judges within the same background group, we observe the
agreement between the second group with Physics and space physics background is 0.56 which shows our
participants with physics background have a high level agreement on the number of odd topics.

\subsection{Applicability to Curriculum Design}
As we mention in the Introduction, our proposed topic discovery has implications to curriculumn design,
since it is able to identify topics along with their level of difficulty; those levels of difficulty
are key in determining prerequisite and co-requisite (i.e., concepts that must be taught at the same
time) relations between concepts in the syllabus.
%Since our model finds topics and their relevant difficulty from existing online discussions, we can use it for generating curriculum from online discussions. 
Here, we demonstrate this applicability of our topic discovery to automated curriculum
design, along the lines of recently proposed work of \cite{agrawal2016toward, agrawal2015datadriven}. In order to achieve
this, first, we identify events triggered by external events which are not part of topics evolution
and exclude them from the curriculum. We then order the topics based on their relevant difficulty,
as inferred by our topic discovery, and as we mentioned previously, this ordering determines the
arrangement of the topics in the curriculum. What follows is a curriculum we obtained from online
discussion after removing all non-physics terms. 
\begin{mdframed}[backgroundcolor=blue!4] 
\centering
\footnotesize
Flow, Mass, Work, Density, Motion, Speed, Velocity, Displacement, Acceleration, Momentum, Gravity,
    Force, Waves, Electromagnetic, Radioactivity,  Quantum, Particles
\end{mdframed} 
This curriculum is consistent with majority of curricula taught in basic physics courses in online/traditional classrooms. See { \footnotesize \url{https://en.wikipedia.org/wiki/List_of_physics_concepts_in_primary_and_secondary_education_curricula}}. 
%We reserve further investigation of automated curriculum design based on extracted topics using our method for future work.
