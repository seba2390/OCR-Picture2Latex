\subsection{Tensor Decomposition}
Our problem is clearly related to a large body of work both in the domain of
tensor decomposition as well as topic extraction. We give an overview of the existing
work in each domain below.

\textbf{Tensor Decomposition:}
The tensor decomposition technique was first introduced by Hitchcock \cite{hitchcock1927expression} in 1927.
Many variations of the tensor decomposition problem have been studied; among them, there are two particular tensor decompositions that can be considered as higher-order extensions
of the matrix singular value decomposition: (1) CANDECOMP/PARAFAC (or CP for short), and (2) the Tucker decomposition. For an overview of tensor decomposition techniques, see Kolda et al.   \cite{kolda2009tensor},
and Papalexakis et al. \cite{papalexakis2016tensors}. 
%For the applications of Tensor decomposition in exploratary data analysis see Agrawal et al. 

\hide{
One of the most common and widely used family of algorithms for tensor decompositions (and specially CP decomposition)
is the Alternating Least Squares (ALS) algorithms. ALS algorithms find a decomposition by solving for each factor at
a time while fixing all the other factors. In addition to the ALS algorithms, all-at-once algorithms can be used.
Acar et al. \cite{acar2011all} apply first-order gradient optimization methods such as limited-memory BFGS.
Phan et al. \cite{phan2013low} exploit the structure of the Hessian for an efficient second-order
optimization using Newton's method. A study close to ours is the work by Gilpin et al. \cite{gilpin2016some} that
studied the role discovery problem. Their framework models role detection as a constrained non-negative matrix
factorization problem, where the guidance is provided as convex constraints and specified per role. 
}

Another tensor decomposition technique is Coupled Tensor Factorization which was introduced by Smilde et al.  \cite{smilde2000multiway} in the area of Chemometrics.
Since then there has been significant development of such coupled models, either when matrices are coupled
together \cite{singh2008relational} or when matrices and tensors are coupled  \cite{ermics2015link,narita2011tensor}. A notable example of using
the coupled tensor factorization is a recent work by Papalexakis et al. \cite{papalexakis2014turbo} which seeks
to identify coherent regions of the brain using a (noun, voxel, person) tensor and a (noun, feature) matrix.


\textbf{Topic Extraction:}
The topic extraction problem has been rigorously studied in the past. Among the existing methods for solving
this problem, is the family of Markov chain-based topic-extraction methods
\cite{blei2006dynamic,wei2007dynamic,zhang2010evolutionary,ren2008dynamic,ahmed2012timeline}.
In \cite{blei2006dynamic} a Dynamic Topic Modeling tool is proposed which captures the topic evolution
in a collection of documents that is organized sequentially into several discrete time periods, and then
within each period an LDA model is trained on its documents. The Gaussian distributions are used to tie a collection of LDAs by chaining the Dirichlet prior and the natural parameters of each topic.

Mei et al. \cite{Mei:2005:DET} suggested a mixture model to extract the subtopics in weblog collections, to identify and 
track topics in time-stamped text data. In a similar work by Morinaga et al. \cite{Morinaga:2004:TDT} finite mixture
model is used to represent documents at each discrete time interval. In their model, a topic changes on certain documents
if the topic mixtures drift significantly from the previous ones. Kandylas et al. \cite{kandylas2008finding} analyzed the
evolution of knowledge communities using a method called Streemer which focuses time-evolving clusters.

In a work by Aggarwal et al. \cite{aggarwal2006framework} on topic modelling of data streams, a fixed number of
$k$ clusters (topics) are maintained over time. If a new document arrives that is far from all existing clusters, it can
become a new cluster. Liu et al. \cite{liu2008clustering} take a similar approach except instead of using single words as
document features, they use multiword phrases as topic signatures. A drawback of these methods is they consider a-priori
fixed feature space per topic. 

Most works on extracting time-evolving topics, use post-discretized or pre-discretized time analysis. Post-discretized
methods fit a topic modeling to documents without considering time and then documents are sorted in time by slicing them
into discrete subsets \cite{griffiths2004finding}. However, in pre-discretized methods \cite{wang2005group, song2005modeling},
the documents are first sliced into discrete time slices, and then a topic model is fitted to each slice separately. 

Our work is different from previous topic evolution methods as the majority of previous attempts have considered and defined
a time span for each topic (e.g. ~\cite{Mei:2005:DET} ). %  which determines the starting and termination time stamps of the topic. 
In these methods, the extracted topic is highly dependent on how the time-spans are defined. In our work, we do not
need to define a time-span. 
%In addition, in a vast body of work, the whole dataset is partitioned into sub-collections with fixed or variable time intervals. In our case we cannot partition question/answers into sub-collections since when a thread starts (a question is posted) it may continue for days or even months. 
Another shortcoming of the vast majority of existing works is that a-priori fixed feature space is considered for each topic, whereas in
our model we define a topic as a collection of words that appear together. Another advantage of our work is beside
finding time-evolving topics, our method is capable of detecting bursty or conventional topics, such as ``Japan tsunami''
or ``democratic convention'' since we consider both the time and post number as modes in our data.

%Note that collapsed Gibbs sampling has been widely adopted in topic modelling [2,4,29–31,40], since it was first proposed by Griffiths and Steyvers in 2004 [14]. It can give us arelatively simple algorithm for approximate inference, because some latent variables can be integrated out due to the conjugacy

%A useful paper with nice related work. Sequential latent Dirichlet allocation  Lan Du · Wray Buntine · Huidong Jin ·  Changyou Chen

%\mpara{Stream Topic Modelling}

\iffalse
~\cite{jo2011web}

~\cite{saha2012learning}

~\cite{falkowski2006mining}


~\cite{wang2006topics,he2009detecting,kandylas2008finding}
He et. al.~\cite{he2009detecting} develop inheritance topic model to understand topic evolution by leveraging the citation
information, they used a simple cosine similarity comparison to determine the death/birth of old/new topics. 


Our work is different than previous topic evolution words as the majority of previous works have considered (and defined) a time span for each topic (for e.g. ~\cite{Mei:2005:DET} ) which determines the starting and termination time stamps of the topic. Then they extract topic from documents belonging to the same time interval. Hence the extracted topic is highly dependant on how the timespans are defined.  In our work we do not need to define a timespan. 

Also in previous works the whole dataset is partitioned into sub collections with fixed or variable time intervals. In our case we cannot partition question/answers into sub collections since when a thread starts (a question is posted) it may continue for days or even months.  

Another shortcoming of vast majority of existing works is that a  priori fixed feature space is considered for each topic. Whereas in our model we define a topic as a collection of words that appear together. 

In addition most of the previous works on topic evolution are unable to exclude  bursty or conventional topics, such as Japan tsunami or democratic convention. Whereas our model is able to automatically excludes these topics (or set of words) since they will not appear at the same period of users lifetime. For example all users may write about democratic convention regardless of how long they have been active in the community .  Our approach will deal with this challenge as the following, it only considers the words which have different occurrence distributions (with regard to lifetime of users who have used those words).
\fi 


