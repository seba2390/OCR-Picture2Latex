
\section{Methodology}
% METHODS 
% •	DocBert:
% •	VGG:
% •	PublayNet:
% •	Doc2Vec/Word2Vec:
% GNN - K- sort pooling :

In this section, we briefly formally define the few-short learning and cross-domain few shot learning problems. We then describe our proposed method, Cross Domain Few-Shot Learning using  \textbf{D}eep \textbf{C}anonical  \textbf{C}orrelation for  \textbf{D}ocument  \textbf{I}ntelligence dubbed as  \textbf{DCCDI} in later parts of the paper.  \\
%BERT based models, image based deep learning approaches for document image classification and introduce our graph neural network approach in which we propose to jointly model text features, layout information, and  image features in building the graph data for our framework for document image classification.
\begin{figure*}[t]
  \begin{center}
    \includegraphics[width=0.9\textwidth]{main/Images/arch.png}
  \end{center}
  \caption{The overall architecture of our approach. LEFT(Stage-1): Using episodic training paradigm train the last $k$ layers of ResNet-10 (shown in green color) using Cross-entropy loss by support set . Left(Stage-2): Using episodic training paradigm train all layers of ResNet-10 using Cross-entropy loss by support, Query set and by including the Metric-Learning Module. During Meta-testing all the Resnet-10 layers will be fixed. RIGHT: Canonical Correlation Block: During Meta-testing all the text extracted from document image and both image, textual features was trained using canonical correlation loss }
  \label{fig:overview}
\end{figure*}
\subsection{Formal problem definition}
Formally, a few-shot learning problem is denoted as $P=$ $( \mathcal{D}_{\text {source}}, \mathcal{D}_{\text {target}})$; $\mathcal{D}_{\text {source}}$ is  the meta-train set from where base classes as sampled for episodic training. Novel classes during meta-test are sampled from the target domain $\mathcal{D}_{\text {target}}$, such that $D_{\text {source}} \cap D_{\text {target }}=\emptyset$. For brevity, we will define the \textit{\textbf{domain}} of the source dataset $\mathcal{D}_{source}$ to be 

% The \textit{\textbf{domain}} of the source dataset can be denoted as below \\
\begin{equation}
  \label{eqn:dist}
 d_{source}=\{\mathcal{X}, \mathcal{Y}, P_{source}\}
\end{equation}
\\
where $\mathcal{X}$ is the feature space of all the inputs in d-dimensional space; $\mathcal{X} \subset \mathbb{R}^{d} $ and $\mathcal{Y}$ is the label space of all the labels;  $\mathcal{Y} \subset  \{1, \ldots C\}$ where is the number of classes,  and  $P_{source}$ is the joint probability distribution over the feature,label pairs of  $\{ \mathcal{X}, \mathcal{Y}\}$ denoted by $p(x,y)$. A similar definition can be made for the target dataset.

We focus on few-shot settings for document classification using a model $f$ with parameters $\theta$, wich we will denote $f_{\theta}$, via meta learning tasks using episodic training from the meta-train set $\mathcal{D}_{source}$ and aim to demonstrate generalization to novel classes present in the meta-test set  $\mathcal{D}_{target}$. 

During meta training the model $f_{\theta}$ is provided with a wide range of classification tasks  $\mathcal{T}_i$ drawn from the dataset $\mathcal{D}_{\text {source}}=\{\mathcal{T}_{1}, \ldots, \mathcal{T}_{n}\}$ where each episodic task is $\mathcal{T}_{i}=\{(x_{1}^{i}, y_{1}^{i}), \ldots,(x_{k}^{i}, y_{k}^{i})\}$, and where ${x}_{i}$ represents image $i$ and ${y}_{i}$ its corresponding label. 

% From this large source dataset we can construct a very large number of classification tasks $\mathcal{T}_{1 . . n}$ such that $\mathcal{T}_{1...n} \subseteq \mathcal{D}$. 
Each task $\mathcal{T}_i$ is further partitioned into a \textit{support set} $\mathcal{S}_i$ used for training, and a  \textit{query set} $\mathcal{Q}_i$ used for testing. That is, $\mathcal{T}_i$ can be written as the disjoint union $\mathcal{T}_i = \mathcal{S}_i \dot{\cup} \mathcal{Q}_i$. Overall, $\mathcal{D}_{source}$ can be written as  $\mathcal{D}_{\text {source}} = \{ (\mathcal{S}_1, \mathcal{Q}_1), \ldots, (\mathcal{S}_n, \mathcal{Q}_n)  \}$.
% $\mathcal{D}_{\text {source}}=\{(\mathcal{D}_{1}^{\mathrm{tr}}, \mathcal{D}_{1}^{\mathrm{ts}}), \ldots,(\mathcal{D}_{n}^{\mathrm{tr}}, \mathcal{D}_{n}^{\mathrm{ts}})\} $ where $\mathcal{D}_{i}^{\mathrm{tr}}=\{(x_{1}^{i}, y_{1}^{i}), \ldots,(x_{k}^{i}, y_{k}^{i})\} $ and $\mathcal{D}_{i}^{\mathrm{ts}}=\{(x_{1}^{i}, y_{1}^{i}), \ldots,(x_{l}^{i}, y_{l}^{i})\}$. 

We follow the conventional way of preparing the support and query sets for each task ($\mathcal{T}_i$), which is a $C$-way, $N$-shot classification problem in which $C$ classes are randomly drawn from the entire set of classes from $\mathcal{D}$. Furthermore, for each of the sampled classes, $N$ and $M$ examples are sampled for the support and query set respectively such that each task $\mathcal{T}_i$ consists of $(\mathcal{S}_i, \mathcal{Q}_i)$ where $\mathcal{S}_i ={\{(\mathbf{x}_{i}, y_{i})\}}_{i=1}^{C \times N}$ is a support set consisting of $N$ labeled images for each of the $C$ classes and the query set $\mathcal{Q}_i = {\{\tilde{\mathbf{x}}_{i}, \tilde{y}_{i}\}}_{i=1}^{C \times M}$ with $M$ samples per class and $y, \tilde{y} \in\{1, \ldots, C\}$ are the corresponding class labels. 

The cross-domain few-shot learning scheme matches closely with our real-world industry setting where the source domain $\mathcal{D}_{source}$ and the target domain $\mathcal{D}_{target}$ belong to different distributions. As in   Eq. \ref{eqn:dist}, the joint distribution of the source dataset is indicated as $P_{source}$ and the target domain distribution can be denoted as $P_{target}$. Furthermore, as is the case in a  cross-domain few-shot learning setting,  $P_{source} \neq P_{target}$ and $\mathcal{Y}_{s}$ is disjoint from $\mathcal{Y}_{t}$. Also, similarly to a few-shot learning paradigm, during the episodic meta-training phase, the model is trained on a large number of tasks $\mathcal{T}_i$ sampled from the source domain $\mathcal{D}_{source}$. 

During the meta-testing phase, the model is presented with a support set $S=\left\{x_{i}, y_{i}\right\}_{i=1}^{K \times N}$ consisting of $N$ examples from $K$ novel classes and  $\mathcal{Q}=\{x_{i}, y_{i}\}_{i=1}^{K \times M}$ consisting of $M$ examples  which are very different from the meta-training classes.

After the meta-trained model $\mathnormal{\hat{f}_\theta}$ is adapted to the support set, a query set from novel classes is used to evaluate the model performance.

% unlabeled samples and goal is to predict correct labels for these sampels in the query set $\mathcal{Q}$
% labelled images and a query set $\mathcal{Q}=\left\{\mathbf{x}_{i}^{*}\right\}_{i=1}^{m}$ of unlabelled

\subsection{Canonical Correlation}\label{tab:canonicalCorelation}
Ideally, an effective document classification method needs to leverage both textual and image (including layout) information. When using deep convolutional neural networks for document image classification, the document is treated as an image and is ingested as tensor representing the pixel values of the image to get the visual feature vector of the document images.On the other hand, all the text from the document image is extracted, converted into tokens and passed through a BERT-like pre-trained transformer-based language model to obtain textual features. Some of the ways of utilizing both the textual and visual features during the classification are to concatenate or average them before passing them through the final classification layer. Some of the disadvantages of doing this are a) More computational resources are required for model training for large dimensional features b) Difficult to maintain the synchronization between both the visual and textual modalities, which might impact model performance.

We address this dilemma by introducing the Deep Canonical Correlation for Document Intelligence Module (DCCDI) during meta-test phase to represent a document utilizing both the textual and visual features. By using the proposed DCCDI module, we produce highly correlated transformations of multiple modalities of data such as textual and visual using complex non-linear transformations. Canonical correlation was proposed by Hotelling   \citep{hotelling1992relations}. It is a widely used technique in the statistics community to measure the linear relationship between two multidimensional variables, used in finding linear projections of two multidimensional vectors that are maximally correlated. Later on it was applied to machine learning by different researchers \citep{akaho2006kernel, melzer2001nonlinear,bach2002kernel,andrew2013deep}. We use deep canonical corelation method propsed by \citep{andrew2013deep} in our DCCM module with the goal of achieving fine-grained cross-modality alignment between the visual and textual modalities. 

As shown in RIGHT Figure \ref{fig:overview}; $V \in \mathbb{R}^{N \times d_{1}}$ is the multidimensional vector for the visual modality where $d_{1}$ is total number of dimensions and $T \in \mathbb{R}^{N \times d_{2}}$ is the multidimensional vector for the textual modality where $d_{2}$ is total number of dimensions. $N$ is the total number of inputs available in each modality. The input multidimensional vectors in different modalities are transformed using two neural networks $\textsl{g}$ with parameters $\phi_{1}$, $\textsl{h}$ with parameters $\phi_{2}$


\begin{equation}
  \label{eqn:cca}
 \mathcal{Z}_{1}=\textsl{g}_{\phi_1} (V),
 \hspace{6pt}
  \mathcal{Z}_{2}=\textsl{h}_{\phi_2} (T)
\end{equation}


$\mathcal{Z}_{1} \in \mathbb{R}^{N \times d}$ and $\mathcal{Z}_{2} \in \mathbb{R}^{N \times d}$ are the $\mathnormal{d}$ dimensional outputs of the neural networks. Both the neural networks $\textsl{g}$, $\textsl{h}$ are optimized jointly with a goal of making the correlation between $\mathcal{Z}_{1}$ and $\mathcal{Z}_{2}$ as high as possible:

$$
(\phi_{1}^{*}, \phi_{2}^{*})
=
\underset{\phi_{1}, \phi_{2}}{\arg \max } \operatorname{corr}(\textsl{g}_{\phi_1}(V), \textsl{h}_{\phi_2}(T))
$$

The above equation can be solved in multiple ways. For this work we chose an approach suggested by \citep{martin1979multivariate}  and that utilizes Singular Value Decomposition.
Define the centered output matrices by $\bar{\mathcal{Z}}_{i}=\mathcal{Z}_{i}^{\prime}-\frac{1}{N} \mathcal{Z}_{i}^{\prime} \mathbf{1}$. Then define

\begin{align}
    \hat{\Sigma}_{11} 
    &= 
    \frac{1}{d-1} \bar{\mathcal{Z}}_{1} \bar{\mathcal{Z}}_{1}^{\prime}+r_{1} \\ \nonumber
    \hat{\Sigma}_{22} 
    &= 
    \frac{1}{d-1} \bar{\mathcal{Z}}_{2} \bar{\mathcal{Z}}_{2}^{\prime}+r_{1} \\ \nonumber
    \hat{\Sigma}_{12}
    &=
    \frac{1}{d-1} \bar{\mathcal{Z}}_{1} \bar{\mathcal{Z}}_{2}^{\prime} \\ \nonumber
\end{align}

\noindent where $r_1 > 0$ is a regularization constant. As discussed in \cite{andrew2013deep}, the correlation of the top $k$ components of $\mathcal{Z}_{1}$ and $\mathcal{Z}_{2}$ is the sum of the top $k$ singular values of the matrix $T=\hat{\Sigma}_{11}^{-1 / 2} \hat{\Sigma}_{12} \hat{\Sigma}_{22}^{-1 / 2}$. If we take $k=d$, then this is exactly the matrix trace norm of $T$; $
\operatorname{corr}(\mathcal{Z}_{1}, \mathcal{Z}_{2})=\|T\|_{\operatorname{tr}}=\operatorname{tr}(T^{\prime} T)^{1 / 2}
$.


% Let the correlation matrix between $\mathcal{Z}_{1}$ and $\mathcal{Z}_{2}$ denoted as below \\
% \begin{equation}
%     \begin{bmatrix}
% \frac{1}{d-1} \bar{\mathcal{Z}}_{1} \bar{\mathcal{Z}}_{1}^{\prime}+r_{1} & \frac{1}{d-1} \bar{\mathcal{Z}}_{1} \bar{\mathcal{Z}}_{2}^{\prime} \\
% \frac{1}{d-1} \bar{\mathcal{Z}}_{2} \bar{\mathcal{Z}}_{2}^{\prime}+r_{1} & \frac{1}{d-1} \bar{\mathcal{Z}}_{2} \bar{\mathcal{Z}}_{1}^{\prime}
% \end{bmatrix}
% \end{equation}
% where $\bar{\mathcal{Z}}_{i}=\mathcal{Z}_{i}^{\prime}-\frac{1}{N} \mathcal{Z}_{i}^{\prime} \mathbf{1}$ is the centered output matrix. At any given time total correlation of the top $k$ components of $\mathcal{Z}_{1}$ and $\mathcal{Z}_{2}$ is the sum of the top $k$ singular values of the matrix $T=\hat{\Sigma}_{11}^{-1 / 2} \hat{\Sigma}_{12} \hat{\Sigma}_{22}^{-1 / 2}$. If we take $k=d$, then this is exactly the matrix trace norm of $T$; $
% \operatorname{corr}(\mathcal{Z}_{1}, \mathcal{Z}_{2})=\|T\|_{\operatorname{tr}}=\operatorname{tr}(T^{\prime} T)^{1 / 2}
% $

Both the networks parameters are updated by computing the gradient  of $\operatorname{corr}(\mathcal{Z}_{1},\mathcal{Z}_{2})$ and update the parameters using  backpropagation. If the singular value decomposition of $T$ is $T=U D V^{\prime}$, then
\begin{equation}\label{tab:canonical_gradient}
\frac{\partial \operatorname{corr}(\mathcal{Z}_{1}, \mathcal{Z}_{2})}{\partial \mathcal{Z}_{1}}=\frac{1}{d-1}(2 \nabla_{11} \bar{\mathcal{Z}}_{1}+\nabla_{12} \bar{\mathcal{Z}}_{2})
\end{equation}

where
$$
\nabla_{12}=\hat{\Sigma}_{11}^{-1 / 2} U V^{\prime} \hat{\Sigma}_{22}^{-1 / 2}
$$
and
$$
\nabla_{11}=-\frac{1}{2} \hat{\Sigma}_{11}^{-1 / 2} U D U^{\prime} \hat{\Sigma}_{11}^{-1 / 2}
$$


\subsection{DCCDI Model}
The meta-training approach of our proposed \textbf{DCCDI} method can be found in Algorithm 1. Our primary focus in this work is to improve the generalization ability of few-shot classification models to unseen domains by learning a prior on the model weights which is suitable for Meta-Fine-tuning during the meta-testing phase on document datasets.  We have also proposed a canonical-correlation-based layer in the model to integrate effectively both the textual and visual modalities of the document images which can be seen as a fine-grained cross-modality alignment task.

\subsubsection{Domain Agnostic Meta-Learning for Document Intelligence}
Our focus in this work is to improve the generalization ability of our meta-trained model to arbitrary unseen document intelligence domains. The Meta-training that incorporates \textbf{CCDI} its described in detail in Algorithm 1.

We aim to train a model that can adapt swiftly to novel unseen classes. This problem setting is often formalized as cross domain few-shot learning. In this proposed approach, the model is meta-trained on a set
of tasks generated using $\mathcal{D}_{source}$, such that the meta-trained model can quickly adapt to new unseen novel tasks using only a small number of examples or trials  generated using $\mathcal{D}_{target}$. In this section, we formally state the problem and present the general form of our algorithm. Similar to Meta-Learning algorithms the proposed algorithm can be subdivided into following phases.\\

\subsubsection{Meta-Training Phase}
% We consider Reset-10 which acts a visual feature extractor block or visual representational encoder followed by linear layer as our model , denoted f, that maps observations x to outputs y. During meta-training phase, the model is trained to be able to adapt to a large number of tasks generated from the source domain. We would like to apply our algorithm to document intelligence datasets during our meta evaluation.
We used ResNet-10 as our visual feature extractor or encoder. It have been shown recently that this pre-training process significantly improves the generalization \citep{rusu2018meta,gidaris2018dynamic,lifchitz2019dense}. We pre-train the visual feature encoder on a source dataset (miniImageNet or tiredImageNet) by incorporating a final linear layer.

After the pre-training stage, we start our meta-training process of few-shot classification training stage. First, we train and update the last $k$ layers of the visual feature encoder $\mathnormal{E}$ followed by a linear classifier layer. We minimize the standard cross-entropy loss on the meta-training dataset by using only the support set images as shown in the Stage-1 of Figure \ref{fig:overview}.  After this Stage-1 training process, all the layers of the visual feature encoder block of the model $f_{\theta}$ will be unfrozen.
% As shown our model during meta-training phase can be subdivided into two stages where our model is trained using N-shot learning setting using episodic training process. In the stage 1 As part of our pre-training method of building the better feature extractor block. During this process, Initially model $f_{\theta}$ trained to learn each new task $\mathcal{T}_{i}$ drawn from the $\mathcal{D}_{source}$ using supervised loss by using only N support samples.

In the Stage-2 phase, we train the proposed model using the traditional episodic meta-learning paradigm. For each episode a new task $\mathcal{T}_{i}$ is sampled from $\mathcal{D}_{source}$, the model $f_{\theta}$ is trained with $N$ samples present in the support set. The model is then tested on query samples from the same task. The prior parameters of the model $f$ are then updated by considering the test error on the query set. Actually, the test error on sampled tasks $\mathcal{T}_i$ serves as the training error of the meta-learning process. All the parameters in the network are updated using the MAML \citep{finn2017model} first order approach. For this stage-2, we proceed similarly to  \citep{guo2020broader,chen2021self,cai2020cross} which successfully use a metric mapping module to  project the final linear classifier scores onto a metric space which can be used to compare support and query samples, hence increasing the overall accuracy. A graph neural network is used for the Metric-Learning module which is similar to architecture used in few-shot graph neural networks \citep{garcia2018fewshot}.
% of  model $f_{\theta}$ and let the rest of the layers get . This process lets the model learn the features which are suitable for meta-finetuning during  meta-deployment phase.

\subsubsection{Meta-Testing or Meta-Deployment Phase}
At the start of the meta-test phase the first $l$ layers of the visual feature extractor was frozen and the last $k$ layers are left unfrozen. With the main goal of adapting the meta-trained model for the business document domain, we introduce the CCDI module during our deployment phase. During Meta-Testing, a new task $\mathcal{T}_{i}$ is drawn from the $\mathcal{D}_{target}$. The input document images are resized to 224 × 224 then fed into the visual feature blocks. Visual features are extracted for each of the document image present in the support set using the visual feature encoder block from the meta-trained model $\hat{f}_{\theta}$. Similarly for each of the document images, text is extracted using Pytesseract and then fed into pre-trained longformer model \citep{Beltagy2020Longformer} to extract textual embedding features. Both the textual and visual modalities are passed through its corresponding deep Canonical Correlation bock and jointly optimized. Training the canonical co-relation block results in representations that aligns both the modalities (Image and text). The resulting  meta-trained model along with the metric module, which consists of an ensemble of a graph convolution neural network classifier and a linear classifier layer is then trained on this data. Finally both the scores are combined and treated as the final classification scores. 

% For textual features, for each support set image , we extract its text using OCR PyTesseract. Each image is converted into a textual feature vector by using the longformer model pretrained on \textit{longformer-base-4096}. 

% Text features and image features are sent to  Depp Canonical Co-relation block which consists of  two neural networks and both of them are optimized using combined Canonical loss which makes the visual and textual features aligned with each other. After training the canonical corelation block which makes the textual features aligned with the visual features. Now visual features from the meta-trained resnet block and algined textual features from the cannonical block are concatenated. An ensemble of graph convolution neural network classifier and linear classifier layer is then trained on this data individually. Finally both the scores are combined to and treated as final classification scores. During Meta-Testing or evaluation when deploy on the target dataset for all the tasks all the first layers of the feature block is frozen and only the last few layers of the feature block is finetune. 

% \RestyleAlgo{algoruled}
%  \begin{algorithm}[h!]
%  \caption{CCDFSL Algorithm - Meta Training}
%         \DontPrintSemicolon
%         \SetKwInput{kwInput}{Require}
%         \SetKwInput{kwInput}{Require}
%         \SetKwBlock{kwInit}{Feature Engineering}{end}
%         \SetKwBlock{kwMain}{Meta-Training}{end}
%         \SetKwInput{kwOutput}{Output}
%         \kwInput{
%             $p(\mathcal{T})$  $\leftarrow \mathcal{T}_{1 . . n}$ ; $n$  meta-training tasks generated from $\mathcal{D}_{source}$ in the form of K shot-N-way classification
%             }
%         \kwInput{
%           $\theta \leftarrow$  weights of the Resnet-10 Network.
%             }  
%         \kwInit{

%         \par
%         \vspace{0.25cm}
        
%             \For{$\mathcal{T} = 1, \dots , N$}{
            
%                 Sample Task $\mathcal{T}_{i}$ from $p(\mathcal{T})$ \\
%                 Sample N support samples \\
%                 % Extract the feature vectors of N support samples by passing through the $f_\theta$
%                 Evaluate $\nabla_{\theta} \mathcal{L}_{\mathcal{T}_{i}}(f_{ \phi}(f_{\theta}))$  with respect to N support examples\\
                
%                 Compute adapted parameters with gradient descent and update it $\theta_{i}^{\prime}=\theta-\alpha \nabla_{\theta} \mathcal{L}_{\mathcal{T}_{i}}(f_{\theta})$ \\
               
                
%             }
%             % Word2vec training on extracted text $t_m$,$s_m$ information.\;
%             % Convert all the document images into graph data
        
%         \vspace{0.25cm}
%         }
%         \kwMain{
%         \par
%         \vspace{0.25cm}
%                  \While{ not done }{
%                 Sample Task $\mathcal{T}_{i}$ from $p(\mathcal{T})$ \\
%                 Sample N support samples and M query samples for the sampled Task\\
%                 % Extract the feature vectors of N support samples by passing through the $f_\theta$
%                 Evaluate $\nabla_{\theta} \mathcal{L}_{\mathcal{T}_{i}}(f_{ \phi}(f_{\theta}))$  with respect to K examples\\
                
%                 Compute adapted parameters with gradient descent $\theta_{i}^{\prime}=\theta-\alpha \nabla_{\theta} \mathcal{L}_{\mathcal{T}_{i}}(f_{\theta})$ \\
%                 Update $\theta \leftarrow \theta-\beta \nabla_{\theta} \sum_{\mathcal{T}_{i} \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_{i}}(f_{\theta_{i}^{\prime}})$

%         }
%         \vspace{0.25cm}
%         }
%         % \kwOutput{
%         % \par
%         %     Trained Graph convolutional Neural Network\
%         % }
%         \label{psuedocode}
%     \end{algorithm}

\RestyleAlgo{algoruled}
 \begin{algorithm}[h!]
 {\small
 \caption{DCCDI Meta-Test Protocol}
        \DontPrintSemicolon
        \SetKwInput{kwInput}{Require}
        \SetKwInput{kwInput}{Require}
        \SetKwInput{kwInput}{Require}
        \SetKwInput{kwInput}{Require}
        \SetKwBlock{kwMain}{Meta-Test}{end}
        \SetKwInput{kwOutput}{Output}
        \kwInput{
            $p(\mathcal{T})$  $\leftarrow \mathcal{T}_{1 . . n}$ ; $n$  meta-testing tasks generated from $\mathcal{D}_{target}$ in the form of K shot-N-way classification
            }
        \kwInput{
           $\hat{f}_{\theta} \leftarrow$ meta-trained model
            }  
            
                \kwInput{
          $\hat{f}_{bert} \leftarrow$ pre-trained longformer model
            }  
        %      \kwInput{
        %   $\phi \leftarrow$ Graph Neural Network
        %     }  
              \kwInput{
           $g_{\phi_{1}}, g_{\phi_{2}} \leftarrow$ Canonical Corelation Module
            }  
        \kwMain{
        \par
        \vspace{0.25cm}
                 \For{i=1,...,$n$}{
                Sample Task $\mathcal{T}_{i}$ from $p(\mathcal{T})$ \\
                Sample $N$ samples from  support set $\mathcal{S}$ and $M$ samples from query set $\mathcal{Q}$. \\
                Obtain visual, texual modality $\mathcal{Z}_{1}=\textsl{g}_{\phi_{1}} (\hat{f}_{\theta}(\mathcal{S}))$ and $\mathcal{Z}_{2}=\textsl{g}_{\phi_{2}} (\hat{f}_{\theta}(\mathcal{S}))$
                Train the canonical co relation module for 20 steps using canonical loss as shown in Eq \ref{tab:canonical_gradient} $\nabla_{\phi_{1},\phi_{2}} \mathcal{L}_{\mathcal{T}_{i}}(\mathcal{Z}_{1},\mathcal{Z}_{2})$  by following the \\
                % Extract the textual feature vectors of N,M support samples by passing through the $f_{\theta}$ Longtransformer model and visual features by passing through the meta-trained resenet-10\\ \text
                % Train canonical co relation module using both the test and visual features by redung the canonical loss as shown in some equation\\
                Concatenate the visual features and low dimensional textual features found using the trained canonical co-relation module $\hat{g}_{\phi_{1}}, \hat{g}_{\phi_{2}}$\\
                
                Feed the concatenated feature vectors through feature extractor and then
                through the metric learning module\\

                Calculate $\nabla_{\theta} \mathcal{L}_{\mathcal{T}_{i}}(\hat{f}_{ \theta}(\mathcal{Q}))$  using the query set samples\\
                
                Compute adapted parameters with gradient descent and update the model parameters.
                
                % $\theta_{i}^{\prime}=\theta-\alpha \nabla_{\theta} \mathcal{L}_{\mathcal{T}_{i}}(\hat{f}_{\theta})$ \\
                % Update $\theta \leftarrow \theta-\beta \nabla_{\theta} \sum_{\mathcal{T}_{i} \sim p(\mathcal{T})} \mathcal{L}_{\mathcal{T}_{i}}(\hat{f}_{\phi_{i}(\mathcal{Q}})$

        }
        \vspace{0.25cm}
        }
        % \kwOutput{
        % \par
        %     Trained Graph convolutional Neural Network\
        % }
        \label{psuedocode}
        }
    \end{algorithm}

