\vspace{-0.5cm}
\section{Experiments and Results}

\begin{table*}[ht]
{\small
\centering
\begin{tabular}{cccccccc}
\toprule
     \multirow{3}{*}{\textbf{Baselines}} &   \multirow{3}{*}{\textbf{ Embedding Net }} & \multicolumn{3}{c}{INSR Dataset (5-way)}&\multicolumn{3}{c}{RVL Dataset (5-way)}\\
        & & \textbf{ 5-shot} & \textbf{10-shot} & \textbf{20-shot}  & \textbf{5-shot} & \textbf{10-shot} & \textbf{20-shot}\\
\midrule
ProtoTypical Networks & Conv-4 & 57.97 \% & 62.21  \% & 65.4\% &44.64\% &48.85 \% & 53.42 \% \\
ProtoTypical Networks & Resnet-10 & 50.04 \% & 53.33\% & 52.78\% & 49.50 \% & 52.58 \% & 53.25 \%\\
Relational Networks   & Conv-4 & 55.47  \% &  56.58 \% & 57.27\%  & 41.28  \% & 46.93 \% & 49.10 \%\\
Relational Networks   & ResNet-10 & 32.42  \% & 43.08\% & 46.53\%  & 33.08  \% & 34.68 \% &  37.57\% \\
Matching Networks      & ResNet-10 & 48.53  \% & 50.21\%&58.92\%& 40.57  \% & 44.76 \% & 52.21\%\\
\textbf{DCCDI without textual features}          & ResNet-10& \textbf{65.21} \% & \textbf{70.93}\% & \textbf{77.2}\% & \textbf{60.85} \% & \textbf{66.45} \% & \textbf{70.32} \%\\
\textbf{DCCDI} & ResNet-10 & \textbf{67.82} \% & \textbf{72.79} \% &\textbf{79.78}\% &\textbf{61.76} \% & \textbf{67.40} \% & \textbf{72.93 }\%\\
                \addlinespace[1ex]
            \hline
\bottomrule
\end{tabular}
\caption{\textbf{Few Classification accuracy on the INSR, miniRVL dataset when source domain is miniImagenet.}}
\label{tab:miniImageNet}

\bigskip

\begin{tabular}{cccccccc}
\toprule
     \multirow{3}{*}{\textbf{Baselines}} &   \multirow{3}{*}{\textbf{  Embedding Net }} & \multicolumn{3}{c}{INSR Dataset (5-way)}&\multicolumn{3}{c}{RVL Dataset (5-way)}\\
        & & \textbf{ 5-shot} & \textbf{10-shot} & \textbf{20-shot}  & \textbf{5-shot} & \textbf{10-shot} & \textbf{20-shot}\\
\midrule
ProtoTypical Networks & Conv-4 & 61.09 \% & 61.41  \% & 65.18\%&44.42\% &46.94 \% & 54.50 \% \\
ProtoTypical Networks & Resnet-10 & 52.21 \% & 52.52\% & 53.76\% & 46.86 \% & 46.18 \% & 52.30 \%\\
Relational Networks   & Conv-4 & 50.28  \% & 56.82 \% & 60.29\%& 41.30  \% & 48.61 \% & 47.61 \%\\
Relational Networks  & ResNet-10 &  29.36\%& 39.36  \%  & 54.82\% & 26.58  \% & 29.81 \% & 44.29 \% \\
Matching Networks    & ResNet-10 &40.09 \% & 45.88\%& 48.10   \%&33.68\% & 42.37 \% & 43.92\%\\
\textbf{DCCDI without textual features}      & ResNet-10& \textbf{65.73} \% & \textbf{71.75}\% & \textbf{77.11}\%& \textbf{61.32} \%  & \textbf{66.92 }\% & \textbf{71.04} \%\\
\textbf{DCCDI}  & ResNet-10 & \textbf{66.68} \% & \textbf{74.01}  \% & \textbf{78.85} \% & \textbf{62.05}  \% & \textbf{67.55}  \% & \textbf{72.61}  \%\\
                \addlinespace[1ex]
            \hline
\bottomrule
\end{tabular}
\caption{\textbf{Few Classification accuracy on the INSR, miniRVL dataset when source domain is tieredImageNet.}}
\label{tab:tieredImageNet}
}
\end{table*}

In this section, we first introduce the different document image datasets used in our training and evaluation process, then show quantitative comparisons against other baseline methods in the following sections. The code will be attached as supplementary material and will be released publicly after the conference.



\subsection{Datasets}

Robustness of the proposed approach  has been tested using standardized few-shot classification datasets \textbf{miniImageNet} \citep{ravi2016optimization}, \textbf{tieredImageNet} as the single source domain, and evaluate the trained model on two document domain datasets as target domain. The two training datasets are  MiniImageNet dataset which is a subset of bigger ImageNet derived from ILSVRC- 2012 \citep{russakovsky2015imagenet} which is used in our meta-training process consists of 60,000 images from 100 classes, and each class has 600 images in which 64 classes for training, 16 classes for validation, 20 classes for the test. We have used 64 classes using our meta-training phase for episodic training of the proposed model. The TieredImageNet dataset is another dataset derived from ImageNet, it consists 608 classes in total derived from  34 high-level categories. These categories are split into
20 meta-training, 6 meta-validation  and 8 meta-test , which corresponds to 351 train classes, 97 validation classes and 160 test classes respectively. We have used  351 train classes for the training the model using episodic training.\\

We use the following two datasets to evaluate our proposed model. The Insurance company dataset (Fig.\ref{insurancesample}), dubbed as \textbf{INSR} for anonymity contains 5772 document images which spans across 11 categories (few examples are shown in Fig \ref{insurancesample}). Some Categories from the INSR dataset include Medical Bills, Medical Authorizations, Medical Records etc. 

The second dataset is a few-shot learning dataset dubbed as The \textbf{miniRVL} dataset \cite{harley2015evaluation} that has been generated from a larger RVL dataset which consists of 400,000 images which spans across 16 categories. The data relates to  document classification and include Advertisements, Emails among other document types. The \textbf{miniRVL} dataset  consists of16 classes with 1000 images per class and it is designed to keep the inter-class similarity sufficiently high to purposely pose a few-shot learning document classification challenge.
% we created the \textbf{miniRVL} datset using the same criteria with which miniImageNet has
% been generated by keeping the inter-class similarity as sufficiently high to represent a challenge for the current state of the art.



% \textcolor{red}{BRIEFLY ENUMERATE A COUPLE OF CATEGORIES IN EACH DATA SETS TO DEMONSTRATE THE VARIANCE OF CLASSES; FOR FIGURE ALSO EXPLAIN THE CLASS OF EACH SAMPLE} \\
% For the Insurance dataset, we use the  
% splits of 4544 images for train set and 1280 images  for test set; for Tobacco-3482 dataset, we use the
% standard splits of  2482 images for training, 800 images for testing, and rest 200 images for validation set.

\subsection{Document Pre-processing}
To construct the textual features for each document image, we use PyTesseract to extract all the text present in the document. All the extracted text is then passed through longformer model pre-trained on \textit{longformer-base-4096} \citep{Beltagy2020Longformer} and the textual feature vector is then collected for each image. We have used longformer models due to it's increased capacity to handle documents of thousands of tokens or longer as we want to utilize all the textual information present in the document images.

For all the experiments, we measured performance by computing the average accuracy across 3 independent runs.

% \begin{figure}
% \centering
% \includegraphics[width=0.5\columnwidth,height=4cm]{LaTeX/main/Images/gnn_paper_subro_letter_blurred.png} 
% \includegraphics[width=0.5\columnwidth,height=4cm]{LaTeX/main/Images/gnn_paper_medical_record_blurred.png}
% % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
% \caption{Examples of documents from the Insurance data set. The document classes (a) Medical Record, (b) Subrogation Letter}
% \label{insurancesample}
% \end{figure}
\begin{figure}
\begin{center}
\def\tabularxcolumn#1{m{#1}}
\begin{tabularx}{\linewidth}{@{}cXX@{}}
\begin{tabular}{cc}
\subfloat[Medical Record]{\includegraphics[width=3.5cm,height=3.5cm]{main/Images/medical_record_aaai_2021_paper.png}} 
   & \subfloat[Subrogation Letter]{\includegraphics[width=3.5cm,height=3.5cm]{main/Images/gnn_paper_subro_letter_blurred.png}} \\
\end{tabular}
\end{tabularx}
\end{center}
\caption{Examples of documents from the Insurance data set. The document classes (a) Medical Record, (b) Subrogation Letter (blurred for privacy reasons).}
\label{insurancesample}
\end{figure}
\subsection{Implementation Details:} We compared our method to three metric-based learning methods: Matching Networks \citep{vinyals2016matching}, Relation Networks \citep{sung2018learning}, and Prototypical Networks \citep{snell2017prototypical}. We also compared the proposed method using visual and textual features to the proposed method using only visual features. Prior to our meta training phase we also pre-trained our image feature extractor by minimizing the standard supervised cross-entropy loss on the source domain dataset such as miniImageNet or TieredImageNet. This is similar to several recent works \citep{rusu2018meta,gidaris2018dynamic,lifchitz2019dense} that have shown significant improvement in classification accuracy via this method. In all our experiments, we used ResNet-10 model as the backbone of the visual encoder. We also included ablation studies to see the effect of our proposed method for various hyper parameters. 

\subsection{Hyper parameters and infrastructure}
We use easyfsl\footnote{https://github.com/sicara/easy-few-shot-learning},
an open-source deep learning toolkit with a number of implementations of metric-based learning methods. We use a Tesla V100 GPU, consisting of 16GB of memory for all models requiring GPU for train, and use amazon EC2-t2.micro. We use PyTorch 1.9 as the backend framework, and the Pytesseract package for extracting the text from the document images. We used the public architecture implementation from official matching networks  \citep{chen2019closer} and relational networks, and the graph neural network is trained using the official
implementation for few-shot graph convolutional network\footnote{https://github.com/vgsatorras/few-shot-gnn}. We used the canonical correlation block containing two neural networks, their architectures as shown in Table \ref{tab:albation_architectures}. Both the network parameters were optimized together using canonical loss using the RMSprop optimizer with a learning rate of $0.001$\\

