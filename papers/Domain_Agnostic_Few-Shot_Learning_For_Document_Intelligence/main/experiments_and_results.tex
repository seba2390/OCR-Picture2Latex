
%\textbf{\textcolor{red}{BREAKDOWN TIMINGING}}:\\
%\textbf{INSURANCE DATASET}:
%\textbf{\textcolor{red}{Training Preprocessing TIMINGING}}: \\
%20 minutes for pre-processing (pytessract,word2vec train,publaynet)\\
%\textbf{\textcolor{red}{Train TIMINGING}}: \\
%3.05 Minutes for 50 Epochs with GPU\\
%4.1 Minutes for 50 Epochs with CPU\\
%\textbf{\textcolor{red}{Inference Preprocessing TIMINGING}} :
%\textcolor{orange}{4.2 minutes}(4 core machine for 1280 images)\\
%\textbf{\textcolor{red}{Test Inference TIMINGING}}: \textcolor{orange}{0.79seconds} (0.79 %seconds for 1280 Images(only Inference)\\
%\textbf{TObacco 3482 DATASET}:
%\textbf{\textcolor{red}{Training Preprocessing TIMINGING}}: \\
%14 minutes for preprocessing (pytessract,word2vec train,publaynet for 2482 images )\\
%\textbf{\textcolor{red}{Train TIMINGING}}: \\
%2.5 Minutes for 50 Epochs with GPU\\
%3.93 Minutes for 50 Epochs with CPU\\
%\textbf{\textcolor{red}{Inference Preprocessing TIMINGING}} :\\
%\textcolor{orange}{3.1 minutes}(4 core machine for pre-processing 800 images)\\
%\textbf{\textcolor{red}{Inference TIMINGING}}:
%\textcolor{orange}{0.577seconds} (0.578 seconds for 800 Images(Inference)\\

% EXPERIMENTS AND RESULTS
% Amfam data set is 5772 split into train set of 4544 images and test of of 1280 images. 
% Tobacco data set is 3482 split into train set of 2482 images, test set of 800 images, and validation set of 200 images.
\begin{table}
{\small
\begin{tabular}{ccc}
\toprule 
\textbf{Layer} & \textbf{Visual Feature} & \textbf{Text-Feature}\\
\midrule
Input & 512 & 786 \\
1st layer & $1024, \tanh$ & $1024, \mathrm{tanh}$ \\
2nd layer & $1024, \tanh$ & $1024, \tanh$ \\
3rd layer (output) & 20, linear & 20, linear\\
\hline
\bottomrule
\end{tabular}
\caption{Architecture of CCDI Block}
\label{tab:albation_architectures}
}
\end{table}
%\vspace{-0.8cm}
\subsection{Comparing classification accuracy} 
\subsubsection{MiniImageNet}
Results comparing the baselines to our model on meta-trained on miniImagenet and deployed on document image datasets are shown in Table \ref{tab:miniImageNet}. For 5-shot 5-way, 10-shot 5-way, and 20-shot 5-way, our proposed model outperforms all the existing baselines. As shown in the Table \ref{tab:miniImageNet} all the baseline models, Prototypical Networks, Matching Networks and Relational Networks work well when the embedded model is Conv-4 and the performance degrades rapidly when a Resnet-10 block was used as an embedding model for each metric-based baseline method. As shown, the baseline method which closest performance to our proposed approach Prototypical network which achieves 61.09\% accuracy for (5-shot, 5-way). However, performance doesn't improve much for both the (10-shot, 5-way) and the (20-shot, 5-way) classification. The proposed CCDI Model without the canonical co-relation block achieved an accuracy of 65.73\% and the one with the canonical co-relation block has achieved an state-of-the-art accuracy of 66.68\%  which shows the significance of proposed method during the meta-testing phase.

Our experiments followed the same setup described above for meta-testing on the open source RVL dataset. Results are shown in Table \ref{tab:tieredImageNet}. However the miniRVL dataset classification task is more challenging as it contains many documents that doesn't follow specific layout structures within classes. 
As shown Table \ref{tab:tieredImageNet} (b),  the proposed method also outperforms the rest of the baseline methods and achieves state-of-the-art performance on this dataset by a large margin of up to 12\% when compared to its closer competitor: Relational Networks. %As the miniRVL dataset is much more difficult due to lack of structure and harder classes Prototypical, Relational and Matching Networks perform the same with small difference in the accuracy ranges.

\subsubsection{TieredImageNet} To test the effectiveness of the proposed approach when meta-trained on a bigger domain, we repeated the experiments of the previous section.
%such as 5-shot, 10-shot,20-shot on INSR dataset and RVL dataset and 
Results are shown in Table \ref{tab:tieredImageNet}. On the INSR dataset, the models PrototTypical Networks(Conv-4), PrototTypical Networks(ResNet10), Relational Networks(Conv-4),Relational Networks(ResNet10)and Matching Networks(ResNet10)
achieved mean accuracies of [61.09\%, 52.21\%, 50.28 \%, 39.36\%, 48.10\%], respectively for (5-way, 5-shot) classification. In comparison, our proposed method achieved an accuracy of 
66.68\%. PrototTypical Networks (Con-4) came in second, with an accuracy of
only 61.09\% , followed by Relational Networks Networks(Con-4) with an accuracy of
50.28\%. Finally, we found that our proposed model outperform those existing baselines
in all the scenarios of  (5-way, 5-shot), (10-way, 5-shot) , (20-way, 5-shot) and results can be found in the Table \ref{tab:tieredImageNet}.

\begin{table}[ht]
\centering
{\small
\begin{tabular}{cc} 
\toprule
\textbf{Vector Size} & \textbf{Accuracy}\\
\midrule
    15 &  64.75\% \\
    20 & \textbf{65.92\% } \\
    25& 64.88\%  \\
% \addlinespace[1ex]
\hline
\bottomrule
\end{tabular}
\caption{Effect on Output Vector dimension on Accuracy}
\label{tab:albation_vectorsize}
}
\end{table}
\vspace{-0.6cm}
\begin{table}
\centering
{\small
\begin{tabular}{cc}
\toprule 
No of Aug Images & Accuracy \\
\midrule
0- Augmented Images & \textbf{66.68\% } \\
5- Augmented Images  & 65.44\% \\
10- Augmented Images & 65.39\%   \\
15- Augmented Images  & 64.59\%  \\
\hline
\bottomrule
\end{tabular}
\caption{Evaluation Augmentation Results on INSR Dataset when meta-trained on TieredImagenet}
\label{tab:ablation_augmentaion}
}
\end{table}
\subsection{Ablation Studies}\label{tab:ablationstudies}
 Several ablation studies has been done to understand the significance and impact of the different hyperparameters on the proposed model. To evaluate the effectiveness of our proposed canonical correlation module for combining the textual and image features for document image classification, we first investigate the performance of the canonical correlation  module for different output dimensions for $\mathcal{Z}$ based on the textual content and visual features.
We did grid search the output dimensions from the set of [10,15,20,25] and results are shown in Table \ref{tab:albation_vectorsize}. The vector of size 20 gave the best performance results for different experiments. 

We also evaluate the effectiveness our proposed approach for aligning the textual and image features instead of directly concatenating the textual and image features for document classification. As it is shown in Table \ref{tab:albation_concatvs_ccdi}, the DCCDI method achieves higher accuracies than the widely used concatenation of the image and textual features. 
We also performed an ablation study to see the effect of data augmentation on the generalization of performance on unseen data during the meta-test phase. As it is shown in various studies, data augmentation helps in the process of generalization for visual tasks which provide view-point invariance for each visual image. For this, we sampled additional images from the
support set, and perform jitter, random crops, and horizontal flips on a randomized basis. As we can see in Table \ref{tab:ablation_augmentaion} applying data augmentation does not have a positive effect on the solution. One reason for this might be that document images contain specific structure in them.



\begin{table}[ht]
{\small
\centering
\begin{tabular}{ccc} 
\toprule
\textbf{ Method} &\textbf{5-Shot} &\textbf{20-Shot}\\
\midrule
Text-Concat & 66.64\%  &  78.51\%  \\
DCCDI & \textbf{67.8\% } & \textbf{79.78\% } \\
\hline
\bottomrule
\end{tabular}
\caption{Effect on CCDI block vs text-concatenation}
\label{tab:albation_concatvs_ccdi}
}
\end{table}




% \begin{table}
% \begin{tabular}{||c|c|c||}
% \hline \textbf{Layer} & \textbf{Visual Feature} & \textbf{Text-Feature}\\
% \hline
% \hline 
% Input & 512 & 786 \\
% 1st layer & $1024, \tanh$ & $1024, \mathrm{tanh}$ \\
% 2nd layer & $1024, \tanh$ & $1024, \tanh$ \\
% 3rd layer (output) & 20, linear & 20, linear\\
% \hline
% \end{tabular}
% \caption{Architecture of CCDI Block}
% \label{tab:albation_architectures}
% \end{table}

% \begin{table}[ht]
%     \centering
%     \begin{tabular}{||c |c||} 
%  \hline
% \textbf{Vector Size} & \textbf{Accuracy}\\
%  \hline
%  15 &  64.75\% \\
%  \hline
% 20 & \textbf{65.92\% } \\
%  \hline
% 25& 64.88\%  \\
% \hline
% \end{tabular}
%     \caption{Effect on Output Vector dimension on Accuracy}
%     \label{tab:albation_vectorsize}
% \end{table}

% \begin{table}[ht]
%     \centering
%     \begin{tabular}{||c|c|c||} 
%  \hline
% \textbf{ Method} &\textbf{5-Shot} &\textbf{20-Shot}\\
%  \hline
%  Text-Concat & 66.64\%  &  78.51\%  \\
%  \hline
%  DCCDI & \textbf{67.8\% } & \textbf{79.78\% } \\
% \hline
% \end{tabular}
%     \caption{Effect on CCDI block vs text-concatenation}
%     \label{tab:albation_concatvs_ccdi}
% \end{table}

% \begin{table}
% \begin{tabular}{||c|c|c||}
% \hline \textbf{Layer} & \textbf{Visual Feature} & \textbf{Text-Feature}\\
% \hline
% \hline 
% Input & 512 & 786 \\
% 1st layer & $1024, \tanh$ & $1024, \mathrm{tanh}$ \\
% 2nd layer & $1024, \tanh$ & $1024, \tanh$ \\
% 3rd layer (output) & 20, linear & 20, linear\\
% \hline
% \end{tabular}
% \caption{Architecture of CCDI Block}
% \label{tab:albation_architectures}
% \end{table}

% \begin{table}
%     \centering
%   \begin{tabular}{||c|c||}

% \hline No of Aug Images & Accuracy \\
% \hline
% 0- Augmented Images & \textbf{66.68\% } \\
% 5- Augmented Images  & 65.44\% \\
% 10- Augmented Images & 65.39\%   \\
% 15- Augmented Images  & 64.59\%  \\
% \hline
% \end{tabular}
% \caption{Evaluation Augmentation Results on INSR Dataset when meta-trained on TieredImagenet}
% \label{tab:ablation_augmentaion}
% \end{table}


% \begin{table*}[ht]
% \begin{tabular}{cccccccc}
% \toprule
%      \multirow{3}{*}{\textbf{Data Set}} &   \multirow{3}{*}{\textbf{ Train Data set }} & \multicolumn{3}{c}{Intenal Dataset}&\multicolumn{3}{c}{RVL Dataset}\\
%         & & \textbf{ 5-shot} & \textbf{10-shot} & \textbf{20-shot}  & \textbf{5-shot} & \textbf{10-shot} & \textbf{20-shot}\\
% \midrule
% PrototTypical Networks & Conv-4 & 57.97 \% & 62.21  \% & 65.4\% &44.64\% &48.85 \% & 53.42 \% \\
% PrototTypical Networks & Resnet-10 & 50.04 \% & 53.33\% & 52.78\% & 49.50 \% & 52.58 \% & 53.25 \%\\
% Relational Networks   & Conv4 & 55.47  \% &  56.58 \% & 57.27\%  & 41.28  \% & 36.93 \% & 49.10 \%\\
% Relational Networks   & Resnet-10 & 32.42  \% & 43.08\% & 46.53\%  & 33.08  \% & 37.57 \% & 34.68 \% \\
% Matching Networks      & Method & 48.53  \% & 50.21\%&58.92\%& 40.57  \% & 44.76 \% & 52.21\%\\
% Meta Fine Tune         & Method& 65.21 \% & 70.93\% & 77.2\% & 60.85 \% \% & 66.45 \% & 70.32 \%\\
% Meta Fine Tune and CCA & Method & 67.82 \% & 72.79 \% &79.78\% &61.76 \% & 67.40 \% & 72.93 \%\\
%             %  & Use medical bill baseline in the paper (ensemb... & \\
%                 \addlinespace[1ex]
%             \hline
%             % \hline
%             % \addlinespace[2ex]
% % \textbf{PrototTypical Networks} & Conv-4 &44.64\% &48.85 \% & 53.42 \% \\
% % \textbf{PrototTypical Networks} & Resnet-10 & 49.50 \% & 52.58 \% & 53.25 \%\\
% % \textbf{Relational Networks}.   & Conv4 & 41.28  \% & 36.93 \% & 49.10 \%\\
% % \textbf{Relational Networks}    & Resnet-10 & 33.08  \% & 37.57 \% & 34.68 \% \\
% % \textbf{Matching Networks}      & Method & 40.57  \% & 44.76 \% & 52.21\%\\
% % \textbf{Meta Fine Tune}         & Method& 60.85 \% \% & 66.45 \% & 70.32 \%\\
% % \textbf{Meta Fine Tune and CCA} & Method &61.76 \% & 67.40 \% & 72.93 \%\\
% \bottomrule
% \end{tabular}
% \caption{\textbf{Few Classification accuracy on the Insurance,RVL dataset when source domain is MiniImagenet.}}
% \label{miniImageNet}
% \end{table*}

% \begin{table*}[ht]
% \centering
% \begin{tabular}{cccccccc}
% \toprule
%      \multirow{3}{*}{\textbf{Data Set}} &   \multirow{3}{*}{\textbf{ Train Data set }} & \multicolumn{3}{c}{Intenal Dataset}&\multicolumn{3}{c}{RVL Dataset}\\
%         & & \textbf{ 5-shot} & \textbf{10-shot} & \textbf{20-shot}  & \textbf{5-shot} & \textbf{10-shot} & \textbf{20-shot}\\
% \midrule
% \textbf{PrototTypical Networks} & Conv-4 & 61.09 \% & 61.41  \% & 65.18\%&44.42\% &46.94 \% & 54.50 \% \\
% \textbf{PrototTypical Networks} & Resnet-10 & 52.21 \% & 52.52\% & 53.76\% & 46.86 \% & 46.18 \% & 52.30 \%\\
% \textbf{Relational Networks}.   & Conv4 & 50.28  \% & 56.82 \% & 60.29\%& 41.30  \% & 48.61 \% & 47.61 \%\\
% \textbf{Relational Networks}    & Resnet-10 & 39.36  \% & 54.82\% & 29.36\%& 26.58  \% & 44.29 \% & 29.81 \% \\
% \textbf{Matching Networks}      & Method & 48  \% & 45.88\%&33.68\%& 40.09  \% & 42.37 \% & 23.92\%\\
% \textbf{Meta Fine Tune}         & Method& 65.73 \% & 71.75\% & 77.11\%& 61.32 \% \% & 66.92 \% & 71.04 \%\\
% \textbf{Meta Fine Tune and CCA} & Method & 66.68 \% & 74.01 \% &78.85\% &62.05 \% & 67.55 \% & 72.61 \%\\
%             %  & Use medical bill baseline in the paper (ensemb... & \\
%                 \addlinespace[1ex]
%             \hline
%             % \hline
% %             \addlinespace[2ex]
% % \textbf{PrototTypical Networks} & Conv-4 &44.42\% &46.94 \% & 54.50 \% \\
% % \textbf{PrototTypical Networks} & Resnet-10 & 46.86 \% & 46.18 \% & 52.30 \%\\
% % \textbf{Relational Networks}.   & Conv4 & 41.30  \% & 48.61 \% & 47.61 \%\\
% % \textbf{Relational Networks}    & Resnet-10 & 26.58  \% & 44.29 \% & 29.81 \% \\
% % \textbf{Matching Networks}      & Method & 40.09  \% & 42.37 \% & 23.92\%\\
% % \textbf{Meta Fine Tune}         & Method& 61.32 \% \% & 66.92 \% & 71.04 \%\\
% % \textbf{Meta Fine Tune and CCA} & Method &62.05 \% & 67.55 \% & 72.61 \%\\
% \bottomrule
% \end{tabular}
% \caption{\textbf{Memory, hardware and time required by different models on the Insurance dataset. The numbers are reported for training 4544 images and inference for 1280 Images.}}
% \label{effgnn_results_resources_insurance}
% \end{table*}
