\section{Related Work}
This work explores a method which can be used for few-shot learning on multi-channel document data, in which the meta-training is done on a distinct domain of open source documents. Work has been done in this area addressing the separate problems of: 
    \begin{enumerate}
        \item Meta-training a few-shot model on document data,
        \item Combining visual and textual feature channels via canonical correlation, and
        \item Domain adaptation of models trained on image data.
    \end{enumerate}
This work proposes to address all of these issues with a single approach, driven by a distinct business need. Here we detail previous work done in each of these directions.

\subsection{Meta-learning} Meta-learning has been a powerful tool to answer the challenge of the large data requirements that many deep learning models seem to face. So far, many of the applications of deep meta-learning have been in few-shot image classification \cite{finn2019online,snell2017prototypical,ravi2016optimization}. Meta-learning for few shot image classification has often been evaluated on data sets such as ImageNet \cite{russakovsky2015imagenet}, CIFAR-10 and CIFAR-100 \cite{Krizhevsky2009LearningML}, and Omniglot \cite{Lake2011OneSL}. However, there has been little done to apply the methods of meta-learning to industry level document images. 

\subsection{Canonical Correlation} One aspect of enterprise level document images is that they typically have two feature channels; a visual channel, and a text channel. Each has useful information that can be leveraged for document classification. However, a challenge to overcome with this is that typically pre-trained models are used as feature extractors, which are further fine tuned during meta-training. The vectors extracted by these pre-trained models (one for each channel mentioned above) are typically not the same length, and encode the information of the channel in semantically different ways. One method of overcoming this challenge is the use of Canonical Correlation \cite{hotelling1992relations,akaho2006kernel, melzer2001nonlinear,bach2002kernel,andrew2013deep}, which in a sense aligns two vectors via projections in such a way that the projections are maximally correlated. We use a later iteration of this method called Deep Canonical Correlation \cite{andrew2013deep}. This allows us to efficiently combine the two modalities of visual and text features occurring in enterprise document images for the purpose of few-shot meta-learning. 
\subsection{Domain adaptation}
In the traditional machine learning setting, the data samples used for training and testing an algorithm are assumed to come from the same distribution. In practical applications however, this is not always a valid assumption; the data available for training may fall into a different distribution than the data the model is expected to perform on in a live system. A typical example of this is a model which is trained on an open source data set is then desired to be used for inference in a smaller, proprietary data set, perhaps for a slightly different task. Domain adaptation is a subfield of machine learning that attempts to overcome this challenge. Typical approaches include transfer learning \cite{Pan2010ASO}, semi-supervised learning \cite{semisupervisedSurvey,semisupervisedSurvey_2}, multi-task learning \cite{Caruana2004MultitaskL}, and meta-learning \cite{Huisman_2021}. Recent methods include 
\citep{liu2020feature} which uses feature transformation ensemble model with batch spectral regularization, \citep{cai2020cross} aims train specific layers of network using MAML,
 \citep{jiang2020transductive} uses a new prediction head and global classification network based on semantic information for addressing the cross-domain adaptation.






% Early document image classification algorithms relied on OCR to extract content information and exploited the visual structure and layout of a document image, e.g. using tree-related data structures to model a document \cite{Dengel93,ShinDR01,DiligentiFG03}. The subsequent decades of research document analysis, including document image classification, evolves around more sophisticated ways to leverage the image features, text features and document layout information. \par  
% Advancement in Deep Convolutional Neural Networks (DCNN) lend new tools for document 
% image classification \cite{CNNDOC}, because DCNN could extract salient and hierarchical visual feature representations which can somewhat reflect hierarchical nature of document layout. Quite a few DCNN training strategies for document image classification are proposed and extensively reviewed \cite{CNNDOC,cutting_error_half}. Variants of VGG-16 \cite{DCGNN} achieved the state-of-the art on publicly available Tobacco data sets \cite{LewisAAFGH06}.
% %\cite{doceval} conducted a comprehensive study on using different CNN training strategies to classify the publicly available Tobacco data sets \cite{LewisAAFGH06}, and shows region-tuned ensemble of CNNs and holistic CNN fine-tuned from VGG-16 with weights learned from ImageNet are both effective when only thousands of document images are available for training the model. \cite{DCGNN} for document image classification further exploits the idea of transferring weights learned on ImageNet and building ensemble VGG classifiers on subdivisions of the document image. 
% Document understanding and analysis community have also been leveraging word embedding techniques \cite{word2vec}  in NLP and large language models \cite{BERT} to create contextualized embedding for textual content in an document image. BERTGrid \cite{bertgrid} uses both the contextualized word embeddings and its 2D layout coordinates to extract information by predicting segmentation masks and bounding boxes. Assuming syntactic features matter less than content categories in document classification. DocBERT achieved an economical solution for document classification task by distilling BERT \cite{DevlinCLT19}.  LayoutLM \cite{layoutLM} jointly models interactions between text and layout information by inputting both text embeddings along with its 2D layout positional embeddings extracted using OCR and Region of Interest (ROI) Regressions. Considering both the image and textual modalities in the document images, multi-modalities methods \cite {multimodal_classification,end2end_multimodal_extraction} are adapted to document classification tasks as well. \par


%For example DGCNN \cite{dgcnn} proposes a sort pooling layer that preserves useful features characterizing the rich information encoded in a graph for classification purpose and sequentially read a graph in a meaningful and consistent order; Diff Pooling \cite{gnn_diffpool} learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, to generate hierarchical representations.
%t the same time, researchers also explored leveraging large language models to create contextualized embedding representations for text sequences of a document, which can improve a number of document tasks. DocBERT achieved an economical solution for document classification task by distilling BERT \cite{DevlinCLT19} using a BiLSTM and a modified objective function, given the assumption that  in classification syntactic features matter less than content categories. 

 % The pre-trained model is then fine tuned on downstream tasks including document classification, information extraction and form understanding. Moreover, due to its unique richness in both image and linguistic information, a document image is also an ideal content candidate for multi modality neural network architectures. \cite
 %{multimodal_classification} proposed a multi-modal neural network document classification model that is able to learn from word embeddings computed from OCR, and from the image. Multi-modal information is especially effective when the OCR result on old and low quality document image is poor. \cite{end2end_multimodal_extraction} proposes an encoder-decoder style fully convolutional network to extract document structure by regressing on document segmentation masks. It adds text embeddings to the image feature representation decoding stage and also adds an auxiliary decoder paths to reconstruct the text sequence extracted from the document.

%The development in the Graph Convolution Neural Networks  \cite{gnn_survey} opens up opportunities to use graphs to model arbitrary structure components layout in a document image \cite{gnn_icdar,gnn_multimodal}. Graph neural networks also provide flexibility in modeling interactions among areas of a document, and between image features, as well as text features. The graph convolution operation in \cite{gnn_multimodal} takes nodes and edge embeddings and combines both text and layout features of the context of text segments. \cite{gnn_icdar} concatenates positional features and textual features extracted from text segments and send them into graph convolution layer to get representative features which enables rows, columns and table detection. There is little research however, investigating how document images modeling can be benefited by using some of the lastest progress in graph convolution neural network. For example DGCNN \cite{dgcnn} proposes a sort pooling layer that preserves useful features characterizing the rich information encoded in a graph for classification purpose and sequentially read a graph in a meaningful and consistent order; Diff Pooling \cite{gnn_diffpool} learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, to generate hierarchical representations.

%Most of the above computer vision, language models, and graph neural networks applies operations either at the pixel level or at a token/segment level. However document images structural components usually manifest themselves as visually distinctive regions that can be detected using standard Mask R-CNN \cite{maskrcnn} and its variants \cite{fasterrcnn,cosmo}. In the latest PubLayNet trained a Mask R-CNN model for 360 thousand document images from scientific articles, which addresses the limitation of public annotated document image data sets and enables transfer learning of image segmentation in other document domains. The segmented areas present a level of clustering on top of token level segments hence are natural candidates for graph convolution network for document classification which requires more globally differentiating information than token level distinctiveness. 
