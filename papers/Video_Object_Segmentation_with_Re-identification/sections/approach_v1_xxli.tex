\section{Approach}
%
Our VS-ReID model includes a mask propagation module and a re-identification module.
%
The mask propagation module propagates the probability map from the predicted frame to the adjacent frames.
%
Meanwhile, we employ the re-identification module to retrieve instances that are missing during the mask propagation process.
%
Two modules are iteratively applied to the whole video sequence.
%
Next, we will first present these two modules respectively in Sec.~\ref{sec:mask_propagation_module} and Sec.~\ref{sec:re_identification_module}, then introduce the algorithm of VS-ReID in Sec.~\ref{sec:VS-ReID}.


\begin{algorithm}
	\caption{Mask propagation for single object}   
	\label{alg:mask_propagation}
	\begin{algorithmic}[1]
		\small
		\Procedure{$\mathcal{M}_{mp}$}{$I_i, I_j, P_{i, k}$}
%		\Require two adjacent frames, $\{I_i, I_j\}$; the probability map for instance $k$ in the frame $i$, $P_{i, k}$;
%		\Ensure the probability map for instance $k$ in the frame $j$, $P_{j,k}$;
		\State $P_{j,k} \gets 0$ \Comment{initialize}
		\State $f_{i \to j} \gets \mathcal{F}(I_i, I_j)$ \Comment{extract the optical flow}
%		\label{ code:fram:extract }
		\State $P_{i \to j, k} \gets \mathcal{W}(P_{i, k}, f_{i \to j})$ \Comment{flow guided warp}
		\State $b \gets \mathrm{Box}(P_{i \to j, k} > 0.5)$ \Comment{obtain the bounding box}
		\State $P_{j, k}^b \gets \mathcal{N}_{mp}(I_{j}^b, f_{i \to j}^b, P_{i \to j, k}^b)$
		\State \textbf{return} $P_{j, k}$
		\EndProcedure
	\end{algorithmic}  
\end{algorithm}  


\subsection{Mask Propagation Module}
\label{sec:mask_propagation_module}
The inference algorithm of mask propagation is summarized in Algorithm~\ref{alg:mask_propagation}. 
%
Given two adjacent frames $I_i, I_j$, and the pixel-level probability map for instance $k$ in the frame $i$, $P_{i,k}$, we aim to predict the probability map for instance $k$ in the frame $j$, $P_{j,k}$.

Following~\cite{Perazzi2017, khoreva_lucid_dreams17}, we first obtain the coarse estimation of $P_{j,k}$, $P_{i \to j, k}$, from $P_{i,k}$ by flow guided warping.
%
We use FlowNet2.0~\cite{IMKDB17} to extract the optical flow $f_{i \to j}$ between frame $i$ and $j$.
%
The probability map $P_{i,k}$ is warped to $P_{i \to j, k}$ according to $f_{i \to j}$ by a bilinear warping function $\mathcal{W}$.
%
After that, we employ a convolutional neural network, mask propagation network $\mathcal{N}_{mp}$, to further refine the coarse estimation.
%
 Rather than full-resolution images as in~\cite{Perazzi2017, khoreva_lucid_dreams17}, our mask propagation network accepts size-normalized patches that enclose objects of interest as input and produces the refined probability patch. Using object patches as input allows our model to better cope with objects of different scales.
%
More specifically, we crop the patches $ I_j^b$, $f_{i \to j}^b$ and $P_{i \to j, k}^b$ from full-image by instance bounding box $b$.
%
Then we resize those patches into a fixed size and feed them into the mask propagation network to get the probability patch $P_{j,k}^b$.
%
Finally, $P_{j,k}^b$ is resized back to the original size, and fill into a full size zero map to generate the prediction of $P_{j,k}$.
%
Unlike full-image based network~\cite{Perazzi2017, khoreva_lucid_dreams17}, our method can easily capture the small objects and fine structures.


\begin{figure}
	\centering
	\includegraphics[width=0.48\textwidth]{figures/mp_architecture.pdf}
	\caption{\small{Network architecture of mask propagation network. \textbf{Best viewed in color.}}}
	\label{fig:mp_network}
	\vspace{-12pt}
\end{figure}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/pipeline.pdf}
	\caption{\small{Pipeline of our Video Object Segmentation with Re-identification (VS-ReID) model. \textbf{Best viewed in color.}}}
	\label{fig:pipeline}
	\vspace{-10pt}
\end{figure*}

%
\noindent
\textbf{Mask Propagation Network.} 
%
As shown in Fig.~\ref{fig:mp_network}, our mask propagation network is a two-stream convolutional neural network, inspired by~\cite{khoreva_lucid_dreams17}. 
%
However, several important modifications are necessary to further improve the network performance.
%
First, we adopt the much deeper ResNet-101~\cite{He2015} network to increase the model capacity.
%
Second, as we mentioned before, since our network takes patches as input, it is capable of capturing more details compared with full-image based network.
%
We also slightly enlarge the bounding box to keep more contextual information.
% 
Third, to increase the resolution of prediction, we enlarge the size of feature maps by decreasing the convolutional stride and replace convolutions by dilated convolutions.
%
Similar to \cite{chen2014semantic}, atrous spatial pyramid pooling and multi-scale testing are also employed.
%
Last but not least, after independent branch training, two streams are jointly fine-tuned to further improves the performance.


\begin{algorithm} 
	\caption{Re-identification module}   
	\label{alg:reid}
	\begin{algorithmic}[1]
		\small
		\Procedure{$\mathcal{M}_{reid}$}{$I_i, P_{i, k}, t_k$}
%		\REQUIRE a video frame, $I_i$; the pixel-level probability map for instance $k$ in the frame $i$, $P_{i,k}$; and the template of instance $k$, $t_k$;
%		\ENSURE retrieve boundary box of the instance $k$ in the frame $i$, $x$ and corresponding re-identification score $s$;
			\State $X \gets \mathcal{N}_{det}(I_i)$ \Comment{obtain the candidate boxes}
	
			\For {$x_j \in X$}
				\State $s_j \gets S_C(\mathcal{N}_{reid}(I_i^{x_j}), \mathcal{N}_{reid}(t_k))$ \Comment{$S_C$ denotes the cosine similarity}
			\EndFor
			
			\State $\hat{j} \gets \arg\underset{j \le \left\vert{X}\right\vert }{\max}\, s_j$
			\State $b \gets \mathrm{Box}(P_{i,k} > 0.5)$
			\If{$s_{\hat{j}} > \rho_{reid}$ \textbf{and} $\mathrm{IoU}(x_{\hat{j}}, b)< \rho_{occ}$}
				\State \textbf{return} $x_{\hat{j}}, s_{\hat{j}}$
			\Else
				\State \textbf{return} $x_{\hat{j}}, -1$ \Comment{fail or unnecessary}
			\EndIf
%		$s_{\hat{j}} - \underset{l \ne k}{\max}\, S_C( \mathcal{N}_{reid}(I_i^{x_{\hat{j}}}), \mathcal{N}_{reid}(t_l)) > \rho_{diff}$ \textbf{and}
%		\IF {$s_{\hat{j}} > \rho_{reid}$ and $IoU(x_{\hat{j}}, b)< \rho_{occ}$}
%		\RETURN $x_{\hat{j}}, s_{\hat{j}}$
%		\ELSE
%		\RETURN $x_{\hat{j}}, -1$ \Comment{fail or unnecessary}
%		\ENDIF
		\EndProcedure
	\end{algorithmic}  
\end{algorithm} 

\subsection{Re-identification Module}
\label{sec:re_identification_module}
Our mask propagation module is based on the short-term memory and it highly relies on temporal continuity.
%
However, frequent occlusions and large pose variations are very common in dynamic scenes and likely to cause failures in mask propagation.
%
To overcome these issues, we leverage object re-identification module to retrieve missing instances.
%
Re-identification module incorporates long-term memory, which complements mask propagation module and makes our system more robust.

As summarized in Algorithm~\ref{alg:reid}, during the iterative refinement in VS-ReID, our re-identification module takes a single frame $I_i$, the current pixel-level probability map $P_{i, k}$ which is predicted in the previous round of inference for instance $k$ in the frame $i$, and the template of instance $k$, $t_k$ as input, produces the retrieved boundary box $x$, and corresponding re-identification score $s$.
%
In this module, we obtain the candidate bounding boxes $X$ in frame $I_i$ through a detection network $\mathcal{N}_{det}$.
%
For each candidate bounding box $x_j$, the re-identification score between $x_j$ and $t_k$ is conducted through measuring the cosine similarity between their features that are extracted from a re-identification network $\mathcal{N}_{reid}$.
%
Suppose $x_{\hat{j}}$ is the most similar candidate bounding box, it is only accepted as the final result if two conditions are satisfied:
%	
First, $x_{\hat{j}}$ is sufficiently similar with the template $t_k$, that is, the re-identification score between $x_{\hat{j}}$ and $t_k$ is larger than a threshold $\rho_{reid}$;
%
Second, current $P_{i, k}$ is not consistent with $x_{\hat{j}}$, otherwise we do not need to retrieve the instance k in frame i.
%
To evaluate this condition, we compute the IoU score between $x_{\hat{j}}$ and current bounding box from $P_{i, k}$.
%
If it is less than another threshold $\rho_{occ}$, which means they are inconsistent, we believe that we have made a wrong prediction of $P_{i, k}$ in the previous rounds and accept $x_{\hat{j}}$ as the retrieve bounding box.
%
Those two thresholds are selected on the validation set.

\noindent
\textbf{Detection \& Re-identification Network.}
%
We directly adopt the Faster R-CNN~\cite{renNIPS15fasterrcnn} as our detection network $\mathcal{N}_{det}$.
%
For the re-identification network $\mathcal{N}_{reid}$, we employ the architecture of `Identification Net' in~\cite{xiaoli2017joint} and retrain this network for the general object re-identification task. 


\begin{algorithm}
	\caption{VS-ReID algorithm}   
	\label{alg:VS-ReID}
	\algnewcommand\algorithmicto{\textbf{to}}
	\algnewcommand\algorithmicdownto{\textbf{downto}}
	\algrenewtext{For}[3]%
	{\algorithmicfor\ #1 = #2 \algorithmicto\ #3 \algorithmicdo}
	\algrenewtext{While}[3]%
	{\algorithmicfor\ #1 = #2 \algorithmicdownto\ #3 \algorithmicdo}
	\begin{algorithmic}[1]
		\small
		\Procedure {VS-ReID}{$\{I\}, \{P_{1}\}$}
%		\REQUIRE video frames, $\{I\}$, number of instances $K$, ground-truth probability map in the first frame for all instances $\{P_{0}\}$;
%		\ENSURE all frames' probability maps $\{P\}$;
			\State $N \gets \left\vert\{I\}\right\vert$	\Comment{number of frames}
			\State $K \gets \left\vert\{P_{1}\}\right\vert$ \Comment{number of instances}
			\For{$k$}{1}{$K$}
				\State obtain the template $t_k$ from $P_{1,k}$
			\EndFor

			\For{$i$}{2}{$N$}						\Comment{initialize probability maps}
				\For{$k$}{1}{$K$}
					\State $P_{i,k} \gets \mathcal{M}_{mp}(I_{i-1}, I_{i}, P_{i-1,k})$
					\State $c_{i,k} \gets 1$
				\EndFor
			\EndFor
			\Loop
				\State $\hat{s} \gets -1$
				\For{$i$}{2}{$N$}	\Comment{retrieve instances}
					\For{$k$}{1}{$K$}
						\State $x, s \gets \mathcal{M}_{reid}(I_{i}, P_{i,k}, t_k)$
						\If {$s > \hat{s}$ \textbf{and} $c_{i,k} \ne i$}
							\State $\hat{s} \gets s, \hat{x} \gets x, \hat{i} \gets i, \hat{k} \gets k$
						\EndIf
					\EndFor
				\EndFor
				\If {$\hat{s} < 0$}
					\State \textbf{break} \Comment{no instance retrieved}
				\Else 
					\State $P_{\hat{i},\hat{k}} \gets 0, f_{\hat{i}} \gets \mathcal{F}(I_{\hat{i}}, I_{\hat{i}+1})$
%					\State $f_{\hat{i}} \gets \mathcal{F}(I_{\hat{i}}, I_{\hat{i}+1})$
					\State $b \gets \mathrm{Box}(P_{1, \hat{k}} > 0.5)$
					\State $P_{\hat{i},\hat{k}}^{\hat{x}} \gets \mathcal{N}_{mp}(I_{\hat{i}}^{\hat{x}}, f_{\hat{i}}^{\hat{x}}, P_{1, \hat{k}}^{b})$ \Comment{recover}
					\For{$i$}{$\hat{i}+1$}{$N$} \Comment{forward propagate}
						\If {$|c_{i,\hat{k}} - i| > |\hat{i} - i|$}
							\State $P_{i,\hat{k}} \gets \mathcal{M}_{mp}(I_{i-1}, I_{i}, P_{i-1,\hat{k}})$
							\State $c_{i,\hat{k}} \gets \hat{i}$
						\EndIf
					\EndFor
					\While{$i$}{$\hat{i}-1$}{$2$} \Comment{backward propagate}
						\If {$|c_{i,\hat{k}} - i| > |\hat{i} - i|$}
							\State $P_{i,\hat{k}} \gets \mathcal{M}_{mp}(I_{i+1}, I_{i}, P_{i+1,\hat{k}})$
							\State $c_{i,\hat{k}} \gets \hat{i}$
						\EndIf
					\EndWhile
				\EndIf
			\EndLoop
			\State \textbf{return}  $\{P\}$
		\EndProcedure
	\end{algorithmic}  
\end{algorithm} 

\begin{figure}
	\centering
	\includegraphics[width=0.48\textwidth]{figures/damage.pdf}
	\caption{\small{Existing probability maps might be impaired during the iterative refinement. Therefore, we devise a checkpoint mechanism to avoid this issue. \textbf{Best viewed in color.}}}
	\label{fig:damage}
	\vspace{-12pt}
\end{figure}

\subsection{VS-ReID}
\label{sec:VS-ReID}
In this section, we will introduce the VS-ReID algorithm that combines previous two components to infer the masks of all instances on the whole video sequence.

As shown in Fig.~\ref{fig:pipeline}, given a video sequence and the mask (\ie ground-truth probability map) of the objects in the first frame, VS-ReID first initializes the probability maps $\{P\}$.
%
We enumerate all instances and forward propagate their probability maps from the first frame to the last frame by the mask propagation module.
%
After initialization, the re-identification module and mask propagation module are iteratively applied to the whole video sequence until no more high confidence instances can be found.
%
To be more specific, we first applied re-identification module to the whole video for all instances.
%
We keep the retrieved bounding box $\hat{x}$ with the highest similarity score $\hat{s}$.
%
Suppose $\hat{x}$ is the bounding box of instance $\hat{k}$ in frame $\hat{i}$, we then try to recover the probability map of instance $\hat{k}$ in frame $\hat{i}$, $P_{\hat{i},\hat{k}}$.
%
The recovery process is quite similar to the process of mask propagation, with one difference: there is no guided probability map from adjacent frames. 
%
So we replace that with the probability patch of instance $\hat{k}$ cropped from the first frame.
%
Once we obtain the recovered probability map, we can take it as the starting point and use the mask propagation module to bi-directionally recover more probability maps of instance $\hat{k}$ in adjacent frames.
%
However, sometimes existing probability maps will be impaired during this iterative refinement.
%
An example is shown in Fig.~\ref{fig:damage} (a), suppose we have 6 frames in a video sequence. 
%
In the first round of iterative refinement, we retrieve the instance $k$ in the first frame and propagate the recovered mask to the end of video sequence.
%
In the second round, we retrieve the instance $k$ again in the last frame and do the backward propagation.
%
In this case, all probability maps we predicted in the first round will be overwritten.
%
Because of the longer propagation distance, the probability map for instance $k$ in the second frame might be impaired in the second round.
%
To avoid this issue, we devise a checkpoint mechanism with a new variable $c_{i,k}$ recording the starting point by which $P_{i, k}$ is updated.
% 
The initial value of $c_{i,k}$ is $1$, and every probability map prefers to be updated by a closer starting point.
%
As shown in Fig.~\ref{fig:damage} (b), the backward propagation will be interrupted at the fourth frame, since the first frame is closer to the third frame compared with the last one.
%
Finally, we combine all $\{P\}$ to generate the mask prediction $M$ through:
\[
	\small
	M_i(l) = \arg\underset{0 \le k \le K}{\max}\,\frac{1}{Z} *
	\begin{cases}
	P_{i,k}(l) &\mbox{$k \ne 0$}\\
	\prod_{j=1}^{K} (1-P_{i,j}(l)) &\mbox{$k = 0$}
	\end{cases}
\]
where $Z = \prod_{j=1}^{K} (1-P_{i,j}(l)) + \sum_{j=1}^{K} P_{i,j}(l)$ is a normalizing factor, $i$ is the frame index, $l$ is a pixel's location, $K$ is the number of instances in the video sequence. 

\subsection{Implementation Details}
\label{sec:implementation_details}
Two branches of mask propagation network are first trained individually.
%
The RGB branch is pre-trained on the MS-COCO~\cite{lin2014microsoft} and PASCAL VOC~\cite{everingham2010pascal} dataset.
%
During the pre-training, we use the randomly deformed ground-truth mask as the guided probability map.
%
Subsequently, the network is fine-tuned on the DAVIS training set.
%
The flow branch is initialized by RGB branch's weights and fine-tuned on the DAVIS training set.
%
Finally, those two branches are jointly fine-tuned together on the DAVIS training and validation sets.

Detection and re-identification networks are trained on the ImageNet~\cite{deng2009imagenet} dataset, we followed the training strategy in original papers~\cite{renNIPS15fasterrcnn, xiaoli2017joint}.
%
In particular, for the person category, we directly use the network in~\cite{xiaoli2017joint} as our re-identification network.

