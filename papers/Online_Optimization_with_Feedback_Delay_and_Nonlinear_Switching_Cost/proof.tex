In this section, we provide an overview of the proofs of the main results in \Cref{Algorithm}.  We defer the proofs of some technical lemmas needed in the analysis to the Appendix when appropriate.

We first present the proof of the $O(L^{2k})$ competitive ratio upper bound in \Cref{t.main} because it is our main result. Then, we prove the lower bound results in \Cref{t.exp_lowerbound} and \Cref{t.poly_upperbound} because they establish the tightness of the dependencies on $k$ and $L$ in our main result \Cref{t.main}.

\subsection{Proof of Theorem \ref{t.main}}\label{p.t.main}
Intuitively, to bound the cost of iROBD with $k$ steps of delay, we will derive relationships between its trajectory and the trajectory of ROBD (Algorithm \ref{robd}), which experiences no delay and has been studied thoroughly in \citep{shi2020online, goel2019beyond}. However, while establishing such a relationship is relatively straightforward in \citep{shi2020online}, the situation becomes considerably more complicated in our setting since iROBD's trajectory can be ``far away'' from no-delay ROBD's trajectory after $k$ ``estimate and solve'' loops (see Line 10-12 in Algorithm \ref{alg}). Therefore, we adopt a novel induction-based proof, where we first reduce iROBD with $k$ steps of delay to iROBD with less than $k$ steps of delay, and then apply the induction hypothesis.

Following this idea, we treat the decision points of no-delay ROBD $y_t^{(0)}$ as a baseline throughout the proof. Since the cost functions are well-conditioned, it suffices to bound the difference $\|{y_t^{(k)} - y_t^{(0)}}\|$ in order to bound the cost incurred by $k$-step-delay iROBD. The impact of delays on iROBD can then be qualified by how fast the difference $\|{y_t^{(k)} - y_t^{(0)}}\|$ increases as the length of delay $k$ grows.

Before the proof of \Cref{t.main}, we first propose a lemma, demonstrating the cumulative nature of the error of iROBD.

\begin{lemma}\label{l.bound2}
The distance between $y_t^{(0)}$ and $y_t^{(k)}$ can be bounded by:
\begin{align}
    \|y_t^{(k)}-y_t^{(0)}\|^2\le8\|v_t^{(k)}-v_t^{(0)}\|^2+2pL^2\sum_{i=1}^{p}\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2.\notag
\end{align}
\end{lemma}

Lemma \ref{l.bound2} bounds of the difference between the decisions of $k$-step-delay iROBD and no-delay ROBD by its counter parts with less steps of delays, as well as an additional error on estimating the true minimizer $v_t$. This additional error will be related to iROBD trajectories with fewer steps of delays later in the main proof. 

\begin{proof}[Proof of \Cref{l.bound2}]
We know that, for any $m$-strongly convex function $g:\mathcal{X}\to\mathbb{R}$ and its minimizer $v$ ($v=\arg\min_{x\in\mathcal{X}}g(x)$), the following inequality holds for all $x\in\mathcal{X}$:
\begin{align}
    g(x)\ge g(v)+\frac{m}{2}\|x-v\|^2.\notag
\end{align}

Therefore, given $y_t^{(0)}=$ROBD$(f_t,y_{t-p:t-1}^{(0)})$ in Line 6 of \Cref{alg}, that is, $y_t^{(0)}=\arg\min_yf_t(y)+\frac{\lambda}{2}\|y-\delta(y_{t-1:t-p})\|^2$, we have that
\begin{align}
    f_t(y_t^{(0)})+&\frac{\lambda}{2}\|y_t^{(0)}-\delta(y_{t-1:t-p}^{(0)})\|^2+\frac{m+\lambda}{2}\|y_t^{(0)}-(y_t^{(k)}+v_t^{(0)}-v_t^{(k)})\|^2\notag\\
\le&f_t(y_t^{(k)}+v_t^{(0)}-v_t^{(k)})+\frac{\lambda}{2}\|y_t^{(k)}+v_t^{(0)}-v_t^{(k)}-\delta(y_{t-1:t-p}^{(0)})\|^2.\notag
\end{align}

Similarly, since $y_{t}^{(k)}\leftarrow\mathrm{ROBD}(f_{t}^{(k)},y_{t-1}^{(k-1)},\cdots,y_{t-k}^{(0)},\cdots,y_{t-p}^{(0)})$ in Line 12 of \Cref{alg}, we have that 
\begin{align}
    f_t(y_t^{(k)}+v_t^{(0)}-v_t^{(k)})+&\frac{\lambda}{2}\|y_t^{(k)}-\delta(y_{t-1}^{(k-1)}\cdots y_{t-p}^{(k-p)})\|^2+\frac{m+\lambda}{2}\|y_t^{(0)}-(y_t^{(k)}+v_t^{(0)}-v_t^{(k)})\|^2\notag\\
\le&f_t(y_t^{(0)})+\frac{\lambda}{2}\|y_t^{(0)}-v_t^{(0)}+v_t^{(k)}-\delta(y_{t-1}^{(k-1)}\cdots y_{t-p}^{(k-p)})\|^2,\notag    
\end{align}
where we used $f_t^{(k)}(y) = f_t(y + v_t^{(0)} - v_t^{(k)})$.

Summing the above two inequalities gives
\begin{align}
(m+\lambda)\|&y_t^{(0)}-y_t^{(k)}+v_t^{(k)}-v_t^{(0)}\|^2\notag\\
\le&\frac{\lambda}{2}\|y_t^{(k)}+v_t^{(0)}-v_t^{(k)}-\delta(y_{t-1:t-p}^{(0)})\|^2 + \frac{\lambda}{2}\|y_t^{(0)}-v_t^{(0)}+v_t^{(k)}-\delta(y_{t-1}^{(k-1)}\cdots y_{t-p}^{(k-p)})\|^2\notag\\
&- \frac{\lambda}{2}\|y_t^{(0)}-\delta(y_{t-1:t-p}^{(0)})\|^2 - \frac{\lambda}{2}\|y_t^{(k)}-\delta(y_{t-1}^{(k-1)}\cdots y_{t-p}^{(k-p)})\|^2\notag\\
\le&\lambda\|y_t^{(0)}-y_t^{(k)}+v_t^{(k)}-v_t^{(0)}\|\|v_t^{(0)}-v_t^{(k)}+(\delta(y_{t-1}^{(k-1)}\cdots y_{t-p}^{(k-p)})-\delta(y_{t-1:t-p}^{(0)}))\|\notag\\
\le&\lambda\|y_t^{(0)}-y_t^{(k)}+v_t^{(k)}-v_t^{(0)}\|\left(\|v_t^{(0)}-v_t^{(k)}\|+\|\delta(y_{t-1}^{(k-1)}\cdots y_{t-p}^{(k-p)})-\delta(y_{t-1:t-p}^{(0)})\|\right).\notag
\end{align}
Therefore, we can see that
\begin{align}
    \|y_t^{(0)}-y_t^{(k)}+v_t^{(k)}-v_t^{(0)}\|\le\|v_t^{(0)}-v_t^{(k)}\|+\|\delta(y_{t-1}^{(k-1)}\cdots y_{t-p}^{(k-p)})-\delta(y_{t-1:t-p}^{(0)})\|,\notag
\end{align}
which implies that
\begin{align}
     \|y_t^{(0)}-y_t^{(k)}\|\le2\|v_t^{(0)}-v_t^{(k)}\|+\|\delta(y_{t-1}^{(k-1)}\cdots y_{t-p}^{(k-p)})-\delta(y_{t-1:t-p}^{(0)})\|.\notag
\end{align}
Take square and use the Cauchy Inequality, and then we have that
\begin{align}
    \|y_t^{(0)}-y_t^{(k)}\|^2\le&8\|v_t^{(0)}-v_t^{(k)}\|^2+2pL^2\sum_{i=1}^{p}\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2.\notag
\end{align}
\end{proof}

Next, we use the previous lemma to prove  \Cref{t.main}. Recall that in Lemma \ref{l.bound2}, the decision difference between k-step-delay iROBD and no-delay ROBD is bounded not only by its counter parts with less steps of delays, but also an estimation error $\|v_t^{(k)}-v_t^{(0)}\|^2$. We show that we can bound the impacts of this error on the performance using the strongly-convexity of $f_t$.

\begin{proof}[Proof of \Cref{t.main}]

Define a function $\psi:\mathbb{R}^d\to\mathbb{R}_{\ge0}$ as 
\[\psi(v)=\min_yh_t(y-v)+\lambda c(y,y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)}).\]
We know that for any function $m$-strongly convex $f:\mathbb{R}^d\to\mathbb{R}$, function $g:\mathbb{R}^d\to\mathbb{R}$ in the form: 

\begin{align}
    g(x)=\min_yf(y)+\frac{\lambda}{2}\|y-x\|^2\notag
\end{align}
is $\frac{m\lambda}{m+\lambda}$-strongly convex as a function of $x$ (see Lemma 3 in \citep{shi2020online}). Thus, we have\begin{subequations}
\begin{align}
     h_t(y_t^{(k)}-v_t^{(k)})+&\lambda c(y_t^{(k)},y_{t-1}^{(k-1},\cdots,y_{t-p}^{(k-p)})+\frac{1}{2}\cdot\frac{m\lambda}{m+\lambda}\|v_t^{(0)}-v_t^{(k)}\|^2\notag\\
=&\psi(v_t^{(k)})+\frac{1}{2}\cdot\frac{m\lambda}{m+\lambda}\|v_t^{(0)}-v_t^{(k)}\|^2\le\psi(v_t^{(0)}).\label{appendix.c.16.1}
\end{align}
\end{subequations}
According to the definition of $\psi$, we can see that
\begin{subequations}
\begin{align}
    \psi(v_t^{(0)})=&\min_yh_t(y-v_t^{(0)})+\lambda c(y,y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})\notag\\
\le&h_t(y_t^{(0)}-v_t^{(0)})+\lambda c(y_t^{(0)},y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})\notag\\
=&h_t(y_t^{(0)}-v_t^{(0)})+\frac{\lambda}{2}\|y_t^{(0)}-\delta(y_{t-1:t-p}^{(0)})+(\delta(y_{t-1:t-p}^{(0)})-\delta(y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)}))\|^2\notag\\
\le&h_t(y_t^{(0)}-v_t^{(0)})+\lambda\|y_t^{(0)}-\delta(y_{t-1:t-p}^{(0)})\|^2+\lambda\|\delta(y_{t-1:t-p}^{(0)})-\delta(y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})\|^2,\label{appendix.c.16.2}
\end{align}
\end{subequations}
where we have applied AM-GM inequality in \Cref{appendix.c.16.2}. 

Here, we have encountered terms as square of the distance between two $\delta$ functions, where $\delta(y_{t-1:t-p}^{(0)})$ corresponds to ROBD while $\delta(y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})$ our algorithm of iROBD. All we know about the $\delta$ function is that it is Lipschitz, so by the Lipschitz condition of it, we have 
\begin{equation}\label{appendix.c.16.3}
\begin{split}
     h_t(y_t^{(0)}-v_t^{(0)})+&\lambda\|y_t^{(0)}-\delta(y_{t-1:t-p}^{(0)})\|^2+\lambda\|\delta(y_{t-1:t-p}^{(0)})-\delta(y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})\|^2\\
\le&h_t(y_t^{(0)}-v_t^{(0)})+2\lambda c(y_{t:t-p}^{(0)})+\lambda(\sum_{i=1}^pL\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|)^2.
\end{split}
\end{equation}
And according to the Cauchy Inequality, the line above is no larger than
\begin{equation}\label{appendix.c.16.4}
    h_t(y_t^{(0)}-v_t^{(0)})+2\lambda c(y_{t:t-p}^{(0)})+\lambda pL^2\sum_{i=1}^p\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2.
\end{equation}

With \Cref{appendix.c.16.1,appendix.c.16.2,appendix.c.16.3,appendix.c.16.4}, we immediately get

\begin{equation}\label{appendix.c.1}
\begin{split}
    h_t(y_t^{(k)}-v_t^{(k)})+&\lambda c(y_t^{(k)},y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})+\frac{1}{2}\cdot\frac{m\lambda}{m+\lambda}\|v_t^{(0)}-v_t^{(k)}\|^2\\
\le&h_t(y_t^{(0)}-v_t^{(0)})+2\lambda c(y_{t:t-p}^{(0)})+\lambda pL^2\sum_{i=1}^p\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2.    
\end{split}
\end{equation}

This inequality is important in proving because it bridges the hitting cost of no-delay ROBD, that is, $h_t(y_t^{(0)}-v_t^{(0)})$. Next, we turn to connect the hitting cost of $k$-step-delay iROBD, that is $h_t(y_t^{(k)}-v_t^{(0)})$.

We know that for convex and $l$-strongly smooth function $f:\mathbb{R}^d\to\mathbb{R}_{\ge0}$, the inequality
\begin{align}
    f(y)\le(1+\eta)f(x)+\left(1+\frac{1}{\eta}\right)\cdot\frac{l}{2}\|x-y\|^2\notag
\end{align}
holds for all $\eta>0$. Observing that $h$ is $l$-strongly smooth, we can conclude that, for any $\eta_{1,k}>0$,
\begin{align}\label{appendix.c.2}
    \frac{1}{1+\eta_{1,k}}h_t(y_t^{(k)}-v_t^{(0)})\le h_t(y_t^{(k)}-v_t^{(k)})+\frac{l}{2\eta_{1,k}}\|v_t^{(0)}-v_t^{(k)}\|^2.
\end{align}

Additionally, since the function $\frac{\lambda}{2}\|y_t^{(k)}-y\|^2$ is $\lambda$-strongly smooth in $y$, for any $\eta_{2,k}>0$, we have
\begin{align}\label{appendix.c.3}
    \frac{1}{1+\eta_{2,k}}\cdot&\frac{\lambda}{2}\|y_t^{(k)}-\delta(y_{t-1:t-p}^{(k)})\|^2\notag\\
\le&\frac{\lambda}{2}\|y_t^{(k)}-\delta(y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})\|^2+\frac{\lambda}{2\eta_{2,k}}\|\delta(y_{t-1:t-p}^{(k)})-\delta(y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})\|^2.
\end{align}

Substituting \Cref{appendix.c.3} and \Cref{appendix.c.2} into \Cref{appendix.c.1}, we have 
\begin{subequations}\allowdisplaybreaks
\begin{align}
    \frac{1}{1+\eta_{1,k}}&h_t(y_t^{(k)}-v_t^{(0)})+\frac{1}{1+\eta_{2,k}}\cdot\frac{\lambda}{2}\|y_t^{(k)}-\delta(y_{t-1:t-p}^{(k)})\|^2\notag\\
\le&h_t(y_t^{(k)}-v_t^{(k)})+\frac{l}{2\eta_{1,k}}\|v_t^{(0)}-v_t^{(k)}\|^2\notag\\
&+\frac{\lambda}{2}\|y_t^{(k)}-\delta(y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})\|^2+\frac{\lambda}{2\eta_{2,k}}\|\delta(y_{t-1:t-p}^{(k)})-\delta(y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})\|^2\notag\\
\le&h_t(y_t^{(0)}-v_t^{(0)})+2\lambda c(y_{t:t-p}^{(0)})+\lambda pL^2\sum_{i=1}^p\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2-\frac{1}{2}\cdot\frac{m\lambda}{m+\lambda}\|v_t^{(0)}-v_t^{(k)}\|^2\notag\\
&+\frac{l}{2\eta_{1,k}}\|v_t^{(0)}-v_t^{(k)}\|^2+\frac{\lambda}{2\eta_{2,k}}\|\delta(y_{t-1:t-p}^{(k)})-\delta(y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})\|^2\notag\\
\le&h_t(y_t^{(0)}-v_t^{(0)})+2\lambda c(y_{t:t-p}^{(0)})+\frac{l}{2\eta_{1,k}}\|v_t^{(0)}-v_t^{(k)}\|^2\notag\\
&-\frac{m\lambda}{2m+2\lambda}\|v_t^{(0)}-v_t^{(k)}\|^2+\lambda pL^2\sum_{i=1}^p\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2\notag\\
&+\frac{\lambda}{\eta_{2,k}}\|\delta(y_{t-1:t-p}^{(k)})-\delta(y_{t-1:t-p}^{(0)})\|^2+\frac{\lambda}{\eta_{2,k}}\|\delta(y_{t-1}^{(k-1)},\cdots,y_{t-p}^{(k-p)})-\delta(y_{t-1:t-p}^{(0)})\|^2\label{appendix.c.19.1}\\
\le&h_t(y_t^{(0)}-v_t^{(0)})+2\lambda c(y_{t:t-p}^{(0)})+\frac{l}{2\eta_{1,k}}\|v_t^{(0)}-v_t^{(k)}\|^2-\frac{m\lambda}{2m+2\lambda}\|v_t^{(0)}-v_t^{(k)}\|^2\notag\\
&+\lambda pL^2(1+\frac{1}{\eta_{2,k}})\sum_{i=1}^p\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2+\frac{\lambda pL^2}{\eta_{2,k}}\sum_{i=1}^p\|y_{t-i}^{(k)}-y_{t-i}^{(0)}\|^2,\label{appendix.c.19.2}
\end{align}
\end{subequations}
where we have applied AM-GM inequality in \Cref{appendix.c.19.1}, the Lipschitz condition of $\delta$ and Cauchy inequality in \Cref{appendix.c.19.2}. Now we already have the relation between the step-wise cost of $k$-step-delay iROBD in the left hand side and the step-wise cost of no-delay ROBD in the right hand side. The problem left is to analyze the impacts of terms of errors on estimating the minimizer and decision difference of iROBD with fewer steps of delays to the no-delay ROBD.


Summing over time and applying \Cref{l.bound2} gives
\begin{align}\label{appendix.c.5}
    \sum_{t=1}^T&\left(\frac{1}{1+\eta_{1,k}}H_t^{(k)}+\frac{\lambda}{1+\eta_{2,k}}M_t^{(k)}\right)\notag\\
\le&\sum_{t=1}^T\left(H_t^{(0)}+2\lambda M_t^{(0)}\right)+\left(\frac{l}{\eta_{1,k}}-\frac{m\lambda}{m+\lambda}\right)\frac{1}{2}\sum_{t=1}^T\|v_t^{(0)}-v_t^{(k)}\|^2\notag\\
&+\frac{\lambda p^2L^2}{\eta_{2,k}}\sum_{t=1}^T\|y_t^{(k)}-y_t^{(0)}\|^2+\lambda pL^2(1+\frac{1}{\eta_{2,k}})\sum_{i=1}^{k-1}\sum_{t=1}^T\|y_t^{(i)}-y_t^{(0)}\|^2\notag\\
\le&2\sum_{t=1}^T\left(H_t^{(0)}+\lambda M_t^{(0)}\right)+\left(\frac{l}{\eta_{1,k}}+\frac{16\lambda p^2L^2}{\eta_{2,k}}-\frac{m\lambda}{m+\lambda}\right)\frac{1}{2}\sum_{t=1}^T\|v_t^{(0)}-v_t^{(k)}\|^2\notag\\
&+\sum_{j=k-1}^{1}16\lambda pL^2(1+\frac{1+2p^2L^2}{\eta_{2,k}})(1+2pL^2)^{k-1-j}\frac{1}{2}\sum_{t=1}^T\|v_t^{(0)}-v_t^{(j)}\|^2.
\end{align}

This structure is rather complicated since it not only involves $\sum_{t=1}^T\|v_t^{(0)}-v_t^{(k)}\|^2$, but also $\|v_t^{(0)}-v_t^{(j)}\|^2$ for $j=1,2,\cdots,k-1$. It just corresponds to \Cref{l.bound2}, where the distance between the choices of iROBD and ROBD consists of errors from past steps.

Finally, pick $\eta_{2,k}=\eta_{k}$ and $\eta_{1,k}=\frac{1+\eta_{k}-\lambda}{\lambda}$ so that $\frac{1}{1+\eta_{1,k}}=\frac{\lambda}{1+\eta_{2,k}}$. Additionally, denote $P(i)$ as
\begin{align}
    P(i)=\frac{\lambda}{1+\eta_{i}}\sum_{t=1}^T\left(H_t^{(i)}+M_t^{(i)}\right).\notag
\end{align}

This yields the following:
\begin{align}
    \frac{1}{\prod_{i=1}^{k-1}\eta_{i}}&P(k)
\le\frac{1}{\prod_{i=1}^{k-1}\eta_{i}}P(k)+\frac{1}{\prod_{i=1}^{k-2}\eta_{i}}P(k-1)+\cdots+\frac{1}{\eta_{1}}P(2)+P(1)\notag\\
\le&(1+\frac{2}{\eta_{1}}+\cdots+\frac{2}{\prod_{i=1}^{k-1}\eta_{i}})\sum_{t=1}^T\left(H_t^{(0)}+\lambda M_t^{(0)}\right)+\sum_{i=1}^{k-1}\frac{\lambda\sum_{t=1}^T\|v_t^{(0)}-v_t^{(i)}\|^2}{2\prod_{j=1}^{i-1}\eta_{j}}\cdot S(i)\notag\\
&+\frac{\lambda}{\prod_{i=1}^{k-1}\eta_{i}}\left(\frac{l}{1+\eta_{k}-\lambda}+\frac{16p^2L^2}{\eta_{k}}-\frac{m}{m+\lambda}\right)\frac{1}{2}\sum_{t=1}^T\|v_t^{(0)}-v_t^{(k)}\|^2,\notag
\end{align}
where the coefficient $S(i)$ is defined as
\begin{align}
    S(i)=\frac{l}{1+\eta_{i}-\lambda}+\frac{16 p^2L^2}{\eta_{i}}-\frac{m}{m+\lambda}+16\sum_{j=i+1}^k(1+\frac{1+2p^2L^2}{\eta_{j}})\cdot\frac{pL^2}{\eta_{i}}\cdot\frac{(1+2p^2L^2)^{j-i-1}}{\prod_{h=i+1}^{j-1}\eta_{h}}.\notag
\end{align}

When $\eta_{i}=O(l+2p^2L^2)$ for $i=1,\cdots,k-1$, we can make the coefficient of $\sum_{t=1}^{T}\|v_t^{(0)}-v_t^{(i)}\|^2$ in the $S(i)$ negative for all $i$. Also, when $\eta_{k}=O(l+2p^2L^2)$, the coefficient of $\sum_{t=1}^{T}\|v_t^{(0)}-v_t^{(k)}\|^2$ in the inequality above negative. It finally leads to
\begin{align}
    \sum_{i=1}^T&\left(H_t^{(k)}+M_t^{(k)}\right)
    =\frac{1+\eta_{k}}{\lambda}P(k)\notag\\
    \le&\frac{1+\eta_{k}}{\lambda}(\prod_{i=1}^{k-1}\eta_{i}+2\sum_{i=2}^{k-1}\prod_{j=i}^{k-1}\eta_{j}+2)\sum_{t=1}^T\left(H_t^{(0)}+\lambda M_t^{(0)}\right)\notag\\
    \le&O((l+2p^2L^2)^k)\max\left\{\frac{1}{\lambda},\frac{m+\lambda}{m+(1-p^2L^2)\lambda}\right\}\sum_{i=1}^T\left(H_t^*+M_t^*\right).\notag
\end{align}

The last inequality is based on the following lemma comparing the performance of ROBD to the optimal solution:
\begin{lemma}\label{appendix.c.lemma}
$\sum_{t=1}^T(H_t^{(0)}+\lambda M_t^{(0)})\le\sum_{t=1}^T\left(H_t^*+\frac{\lambda(m+\lambda)}{m+(1-p^2L^2)\lambda}M_t^*\right).$
\end{lemma}
\noindent Due to space constraints, we defer the proof of \Cref{appendix.c.lemma} to Appendix \ref{appendix.nonlinear+delay}.
\end{proof}




\subsection{Proof of Theorem \ref{t.exp_lowerbound}}\label{proof.exp}

Without loss of generality, consider a $1$-dimensional problem instance where the agent starts at $x_0 = 0$, and the hitting cost sequence is given by $f_t(x) = \frac{m}{2}(x - v_t)^2$ with $v_t = \alpha^{t-1}$. Note that this instance can be extended to $d$-dimensional space easily when $d > 1$ by letting $v_t = (\alpha^{t-1}, 0, \ldots, 0)$. Suppose the horizon $T$ equals to the delay length $k$, which means the online agent cannot observe any information about the hitting costs before the game ends. We discuss the behavior of any competitive online algorithm $ALG$ and an offline adversary $ADV$ that moves to $v_t$ at time step $t$.

Since $ALG$ is competitive, it will stay at the origin throughout the game. This is because $ALG$ cannot observe any information about the hitting cost functions until the end. If $ALG$ moves at any time step, and the hitting cost sequence turns out to be $f_t(x) = \frac{m}{2}x^2, t = 1, \ldots, k$, the ratio between $cost(ALG)$ and $cost(ADV)$ will be $\infty$. Therefore, we see that the algorithm cost is
\begin{equation}\label{t.exp_lowerbound.e1}
    cost(ALG) = \sum_{t=1}^k \frac{m}{2}\cdot \alpha^{2t-2} = \frac{m(\alpha^{2k} - 1)}{2(\alpha^2 - 1)}.
\end{equation}
On the other hand, the total cost incurred by the offline adversary $ADV$ is $cost(ADV) = \frac{1}{2}$, because the only cost it incurs is the switching cost at time step $1$. Combining with \eqref{t.exp_lowerbound.e1}, we see that
\[\frac{cost(ALG)}{cost(OPT)} \geq \frac{cost(ALG)}{cost(ADV)} = \frac{m(\alpha^{2k} - 1)}{\alpha^2 - 1}.\]
Since this inequality holds for any competitive online algorithm $ALG$, we see the competitive ratio of any online algorithm is lower bounded by $\frac{m(\alpha^{2k} - 1)}{\alpha^2 - 1}$.
\qed

\subsection{Proof of Theorem \ref{t.poly_upperbound}}\label{proof.poly}

We can assume $f_t(v_t) = 0$ without loss of generality. We consider a delayed version of a greedy, move to the minimizer (M2M) algorithm. M2M works by picking the decision point
\begin{equation*}
    x_t = \begin{cases}
    x_0 & \text{ if } t \leq k;\\
    v_{t- k} & \text{ if } t > k.
    \end{cases}
\end{equation*}
To simplify the notation, we define $v_0 := x_0$. The total cost incurred by M2M can be expressed as
\begin{equation}\label{p.poly_upperbound.e0}
    cost(M2M) = \sum_{t=1}^k f_t(x_0) + \sum_{t = k + 1}^{T} f_{t}(v_{t-k}) + \sum_{t = 1}^{T - k} \frac{1}{2}\|{v_t - v_{t-1}}\|^2.
\end{equation}
For $t \leq k$, we have
\begin{align}\label{p.poly_upperbound.e1}
    f_t(x_0) &\leq \frac{l}{2}\|{v_t - x_0}\|^2\nonumber\\
    &\leq \frac{l}{2} \left(\|v_t - x_t^*\| + \sum_{\tau = 1}^t \|x_\tau^* - x_{\tau-1}^*\|\right)^2\nonumber\\
    &\leq \frac{l(t+1)}{2} \left(\|v_t - x_t^*\|^2 + \sum_{\tau = 1}^t \|x_\tau^* - x_{\tau-1}^*\|^2\right)\nonumber\\
    &\leq \frac{l(t+1)}{m}H_t^* + l(t+1) \sum_{\tau = 1}^t M_\tau^*.
\end{align}
For $t > k$, we see that
\begin{align}\label{p.poly_upperbound.e2}
    f_t(v_{t-k}) &\leq \frac{l}{2}\|{v_t - v_{t-k}}\|^2\nonumber\\
    &\leq \frac{l}{2}\left(\|v_t - x_t^*\| + \sum_{\tau = t - k + 1}^t \|x_\tau^* - x_{\tau-1}^*\| + \|x_{t-k}^* - v_{t-k}\|\right)^2\nonumber\\
    &\leq \frac{l(k+2)}{2} \left(\|v_t - x_t^*\|^2 + \sum_{\tau = t - k + 1}^t \|x_\tau^* - x_{\tau-1}^*\|^2 + \|x_{t-k}^* - v_{t-k}\|^2\right)\nonumber\\
    &\leq \frac{l(k+2)}{m}(H_t^* + H_{t-k}^*) + l(k+2) \sum_{\tau = t - k + 1}^t M_\tau^*.
\end{align}
For $t \leq T - k$, we see that
\begin{align}\label{p.poly_upperbound.e3}
    \frac{1}{2}\|{v_t - v_{t-1}}\|^2 &\leq \frac{1}{2}\left(\|v_t - x_t^*\| + \|x_t^* - x_{t-1}^*\| + \|v_{t-1} - x_{t-1}^*\|\right)^2\nonumber\\
    &\leq \frac{3}{2}\left(\|v_t - x_t^*\|^2 + \|x_t^* - x_{t-1}^*\|^2 + \|v_{t-1} - x_{t-1}^*\|^2\right)\nonumber\\
    &\leq \frac{3}{m}(H_t^* + H_{t-1}^*) + 3 M_t^*.
\end{align}
Substituting \eqref{p.poly_upperbound.e1}, \eqref{p.poly_upperbound.e2}, and \eqref{p.poly_upperbound.e3} into \eqref{p.poly_upperbound.e0} gives that
\[\frac{cost(M2M)}{cost(OPT)} = O(k^3).\]
\qed