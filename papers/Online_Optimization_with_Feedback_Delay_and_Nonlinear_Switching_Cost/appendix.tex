\section{Proof of Lemma \ref{appendix.c.lemma}}\label{appendix.nonlinear+delay}
We begin with a technical lemma, bounding the performance of the oracle decision sequence from ROBD where there is no delay:
\begin{proof}
Define $\phi_t=\frac{m+\lambda}{2}\|y_t^{(0)}-y_t^*\|^2$. Since the function 
\begin{align}
    g_t(y)=f_t(y)+\frac{\lambda}{2}\|y-\delta(y_{t-1:t-p}^{(0)})\|^2\notag
\end{align}
is $(m+\lambda)$-strongly convex, and ROBD selects $y_t^{(0)}=\arg\min_yg_t(y)$, we have that
\begin{align}
    g_t(y_t^{(0)})+\frac{m+\lambda}{2}\|y_t^{(0)}-y_t^*\|^2\le g_t(y_t^*),\notag
\end{align}
which implies that
\begin{align}\label{appendix.nonlinear.lemma.1}
    H_t^{(0)}+\lambda M_t^{(0)}+\left(\phi_t-\frac{1}{p}\sum_{i=1}^p\phi_{t-i}\right)\le H_t^*+\frac{\lambda}{2}\|y_t^*-\delta(y_{t-1:t-p}^{(0)})\|^2-\frac{1}{p}\sum_{i=1}^p\phi_{t-i}.
\end{align}

Applying Jensen's inequality gives
\begin{align}\label{appendix.c.psi}
    \frac{1}{p}\sum_{i=1}^p\phi_{t-i}
    =\frac{m+\lambda}{2p}\sum_{i=1}^p\|y_{t-i}^{(0)}-y_{t-i}^*\|^2
    \ge\frac{m+\lambda}{2p^2}\left(\sum_{i=1}^p\|y_{t-i}^{(0)}-y_{t-i}^*\|\right)^2.
\end{align}

Therefore, we can derive the following bound 
\begin{subequations}\allowdisplaybreaks
\begin{align}
    \frac{\lambda}{2}\|y_t^*-&\delta(y_{t-1:t-p}^{(0)})\|^2-\frac{1}{p}\sum_{i=1}^p\phi_{t-i}\notag\\
    \le&\frac{\lambda}{2}\|y_t^*-\delta(y_{t-1:t-p}^{(0)})\|^2-\frac{m+\lambda}{2p^2}\left(\sum_{i=1}^p\|y_{t-i}^{(0)}-y_{t-i}^*\|\right)^2\label{appendix.c.bound.1}\\
    =&\frac{\lambda}{2}\|y_t^*-\delta(y_{t-1:t-p}^*)-(\delta(y_{t-1:t-p}^{(0)})-\delta(y_{t-1:t-p}^*))\|^2-\frac{m+\lambda}{2p^2}\left(\sum_{i=1}^p\|y_{t-i}^{(0)}-y_{t-i}^*\|\right)^2\notag\\
    \le&\frac{\lambda}{2}\|y_t^*-\delta(y_{t-1:t-p}^*)\|^2+\frac{\lambda}{2}\|\delta(y_{t-1:t-p}^{(0)})-\delta(y_{t-1:t-p}^*)\|^2\notag\\&+\lambda\|y_t^*-\delta(y_{t-1:t-p}^*)\|\cdot\|\delta(y_{t-1:t-p}^{(0)})-\delta(y_{t-1:t-p}^*)\|-\frac{m+\lambda}{2p^2}\left(\sum_{i=1}^p\|y_{t-i}^{(0)}-y_{t-i}^*\|\right)^2\label{appendix.c.bound.2}\\
    \le&\frac{\lambda}{2}\|y_t^*-\delta(y_{t-1:t-p}^*)\|^2+\lambda\|y_t^*-\delta(y_{t-1:t-p}^*)\|\cdot\|\delta(y_{t-1:t-p}^{(0)})-\delta(y_{t-1:t-p}^*)\|\notag\\
    &+\frac{\lambda}{2}\left(L\sum_{i=1}^p\|y_{t-i}^{(0)}-y_{t-i}^*\|\right)^2-\frac{m+\lambda}{2p^2}\left(\sum_{i=1}^p\|y_{t-i}^{(0)}-y_{t-i}^*\|\right)^2\label{appendix.c.bound.3}\\
     =&\frac{\lambda}{2}\|y_t^*-\delta(y_{t-1:t-p}^*)\|^2+\lambda\|y_t^*-\delta(y_{t-1:t-p}^*)\|\cdot\|\delta(y_{t-1:t-p}^{(0)})-\delta(y_{t-1:t-p}^*)\|\notag\\
    &-\frac{m+\lambda(1-p^2L^2)}{2p^2}\left(\sum_{i=1}^p\|y_{t-i}^{(0)}-y_{t-i}^*\|\right)^2\notag\\
    \le&\frac{\lambda}{2}\|y_t^*-\delta(y_{t-1:t-p}^*)\|^2+\frac{\lambda^2p^2L^2}{2(m+\lambda(1-p^2L^2))}\||y_t^*-\delta(y_{t-1:t-p}^*)\|^2\notag\\
    &+\frac{m+\lambda(1-p^2L^2)}{2p^2L^2}\|\delta(y_{t-1:t-p}^{(0)})-\delta(y_{t-1:t-p}^*)\|^2-\frac{m+\lambda(1-p^2L^2)}{2p^2}\left(\sum_{i=1}^p\|y_{t-i}^{(0)}-y_{t-i}^*\|\right)^2\label{appendix.c.bound.4}\\
    \le&\frac{\lambda^2+m\lambda}{2(m+\lambda(1-p^2L^2))}\|y_t^*-\delta(y_{t-1:t-p}^*)\|^2\notag\\
    &+\frac{m+\lambda(1-p^2L^2)}{2p^2L^2}\cdot L^2\left(\sum_{i=1}^p\|y_{t-i}^{(0)}-y_{t-i}^*\|\right)^2-\frac{m+\lambda(1-p^2L^2)}{2p^2}\left(\sum_{i=1}^p\|y_{t-i}^{(0)}-y_{t-i}^*\|\right)^2\label{appendix.c.bound.5}\\
    =&\frac{\lambda^2+m\lambda}{2(m+\lambda(1-p^2L^2))}M_t^*.\notag
\end{align}
\end{subequations}
We have applied \Cref{appendix.c.psi} in \Cref{appendix.c.bound.1}, AM-GM inequality in \Cref{appendix.c.bound.2} and \Cref{appendix.c.bound.4}, the Lipschitz condition of $\delta$ in \Cref{appendix.c.bound.3} and \Cref{appendix.c.bound.5}. In this way, we have made a connection between the last two terms of the right hand side in \Cref{appendix.nonlinear.lemma.1}, and the switching cost of the offline optimal. 

Substituting the above into \Cref{appendix.nonlinear.lemma.1} gives
\begin{align}\label{appendix.nonlinear.lemma.2}
    H_t^{(0)}+\lambda M_t^{(0)}+\left(\phi_t-\frac{1}{p}\sum_{i=1}^p\phi_{t-i}\right)\le H_t^*+\frac{\lambda^2+m\lambda}{2(m+\lambda(1-p^2L^2))}M_t^*.
\end{align}

Focusing on the term in parentheses, we see that 
\begin{align}
    \sum_{t=1}^T\left(\phi_t-\frac{1}{p}\sum_{i=1}^p\phi_{t-i}\right)=\sum_{i=0}^{p-1}\frac{p-i}{p}(\phi_{T-i}-\phi_{-i}).\notag
\end{align}
Since $\phi_t\ge0,\forall t$ and $\phi_0=\phi_{-1}=\cdots=\phi_{-p+1}=0$, we have
\begin{align}
     \sum_{t=1}^T\left(\phi_t-\frac{1}{p}\sum_{i=1}^p\phi_{t-i}\right)\ge0.\notag
\end{align}
Now, returning to \Cref{appendix.nonlinear.lemma.2} and summing over time gives
\begin{align}
    \sum_{t=1}^TH_t^{(0)}+\lambda M_t^{(0)}&\le\sum_{t=1}^T\left(H_t^{(0)}+\lambda M_t^{(0)}\right)+\sum_{t=1}^T\left(\phi_t-\frac{1}{p}\sum_{i=1}^p\phi_{t-i}\right)\notag\\
    &\le\sum_{t=1}^T\left(H_t^*+\frac{\lambda(m+\lambda)}{m+(1-p^2L^2)\lambda}M_t^*\right).\notag
\end{align}
\end{proof}
With this lemma we can easily get the last inequality in the proof of \Cref{t.main}.



\section{Proof of Theorem \ref{t.delay}}\label{appendix.delay}
We prove another preliminary lemma that bounds the distance between iROBD's decision with $k$-step delay, $y_t^{(k)}$, and the oracle ROBD's decision without delay, $y_t^{(0)}$. 
\begin{lemma}\label{l.bound}
The distance between $y_t^{(0)}$ and $y_t^{(k)}$ can be bounded by:
    \begin{align}
        \|y_t^{(k)}-y_t^{(0)}\|^2\le8\|v_t^{(k)}-v_t^{(0)}\|^2+2\alpha^2\sum_{i=1}^{k-1}\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2.\notag
    \end{align}
\end{lemma}
\begin{proof} Let $\delta_t^{(k)}=v_t^{(0)}-v_t^{(k)}$. Since $y_{t}^{(0)}\leftarrow \mathrm{ROBD}(f_{t},y_{t-p,t-1}^{(0)})$,
\begin{align}
    f_t(y_t^{(0)})+&\frac{\lambda}{2}\left\|y_t^{(0)}-\sum_{i=1}^pC_iy_{t-i}^{(0)}\right\|^2+\frac{m+\lambda}{2}\|y_t^{(0)}-y_t^{(k)}-\delta_t^{(k)}\|^2\notag\\
    \le&f_t(y_t^{(k)}+\delta_t^{(k)})+\frac{\lambda}{2}\left\|y_t^{(k)}+\delta_t^{(k)}-\sum_{i=1}^pC_iy_{t-i}^{(0)}\right\|^2.\notag
\end{align}
Also, we have $y_{t}^{(k)}\leftarrow\mathrm{ROBD}(f_{t}^{(k)},y_{t-1}^{(k-1)},\cdots,y_{t-k}^{(0)},\cdots,y_{t-p}^{(0)})$. Then
\begin{align}
    f_t(y_t^{(k)}+&\delta_t^{(k)})+\frac{\lambda}{2}\left\|y_t^{(k)}-C_1y_{t-1}^{(k-1)}-\cdots-C_ky_{t-k}^{(0)}-\cdots-C_py_{t-p}^{(0)}\right\|^2+\frac{m+\lambda}{2}\|y_t^{(0)}-y_t^{(k)}-\delta_t^{(k)}\|^2\notag\\
   \le &f_t(y_t^{(0)})+\frac{\lambda}{2}\left\|y_t^{(0)}-\delta_t^{(k)}-C_1y_{t-1}^{(k-1)}-\cdots-C_ky_{t-k}^{(0)}-\cdots-C_py_{t-p}^{(0)}\right\|^2.\notag
\end{align}
Summing yields
\begin{subequations}
\begin{align}
    (m+\lambda)\|y_t^{(0)}-y_t^{(k)}-\delta_t^{(k)}\|^2\le&\lambda\|\delta_t^{(k)}+\sum_{i=1}^{k-1}C_i(y_{t-i}^{(k-i)}-y_{t-i}^{(0)})\|\|y_t^{(0)}-y_t^{(k)}-\delta_t^{(k)}\|\notag\\
\Longrightarrow\|y_t^{(0)}-y_t^{(k)}\|\le&2\|\delta_t^{(k)}\|+\|\sum_{i=1}^{k-1}C_i(y_{t-i}^{(k-i)}-y_{t-i}^{(0)})\|\label{a.a.e.1.1}\\
\Longrightarrow\|y_t^{(0)}-y_t^{(k)}\|^2\le&8\|\delta_t^{(k)}\|^2+2\|\sum_{i=1}^{k-1}C_i(y_{t-i}^{(k-i)}-y_{t-i}^{(0)})\|^2\label{a.a.e.1.2}\\
\le&8\|\delta_t^{(k)}\|^2+2(\sum_{i=1}^{k-1}\|C_i\|\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|)^2\label{a.a.e.1.3}\\
\le&8\|\delta_t^{(k)}\|^2+2\alpha^2\sum_{i=1}^{k-1}\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2.\notag
\end{align}
\end{subequations}
We have used triangle inequality in \Cref{a.a.e.1.1}, AM-GM inequality in \Cref{a.a.e.1.2} and Jensen inequality in \Cref{a.a.e.1.3}. 
\end{proof}

Next, we show that the distance between the action of iROBD and that of ROBD can be bounded via recursions. Then, we turn back to the proof of Theorem 2:

\begin{theorem*}
Suppose the hitting costs are $m$-strongly convex and $l$-strongly smooth, and the switching cost is given by $c(y_{t:t-p})=\frac{1}{2}\|y_t-\sum_{i=1}^pC_iy_{t-i}\|^2$, where $C_i\in\mathbb{R}^{d\times d}$ and $\alpha=\sum_{i=1}^p\|C_i\|$. If there is a $k$-round feedback delay, then the competitive ratio of iROBD($\lambda$) is
\begin{align}
    O\left( (l+2\alpha^2)^k\max\left\{\frac{1}{\lambda},\frac{m+\lambda}{m+(1-\alpha^2)\lambda}\right\} \right).
\end{align}
\end{theorem*}
\begin{proof}

In particular, define the function $\psi:\mathbb{R}^d\to\mathbb{R}^{+}\bigcup\{0\}$ as
\begin{align}
    \psi(v)=\min_yh_t(y-v)+\lambda c(y,y_{t-1}^{(k-1)},\cdots,y_{t-k}^{(0)},\cdots,y_{t-p}^{(0)}).\notag
\end{align}
We can show that $\psi$ is $\frac{m\lambda}{m+\lambda}$-strongly convex, and $v_t^{(k)}$ minimizes it. Thus,

\begin{subequations}\label{appendix.a.1}
\begin{align}
    h_t(y_t^{(k)}-v_t^{(k)})&+\lambda c(y_t^{(k)},y_{t-1}^{(k-1)},\cdots,y_{t-k}^{(0)},\cdots,y_{t-p}^{(0)})+\frac{1}{2}\cdot\frac{m\lambda}{m+\lambda}\|v_t-v_t^{(k)}\|^2\notag\\
    =&\psi(v_t^{(k)})+\frac{1}{2}\cdot\frac{m\lambda}{m+\lambda}\|v_t-v_t^{(k)}\|^2\notag\\
    \le&\psi(v_t)\notag\\
    =&\min_yh_t(y-v_t)+\lambda c(y,y_{t-1}^{(k-1)},\cdots,y_{t-k}^{(0)},\cdots,y_{t-p}^{(0)})\notag\\
    \le&h_t(y_t^{(0)}-v_t)+\lambda c(y_t^{(0)},y_{t-1}^{(k-1)},\cdots,y_{t-k}^{(0)},\cdots,y_{t-p}^{(0)})\notag\\
    \le&h_t(y_t^{(0)}-v_t)+\frac{\lambda}{2}\left\|y_t^{(0)}-\sum_{i=1}^pC_iy_{t-i}^{(0)}+\sum_{i=1}^{k-1}C_i(y_{t-i}^{(0)}-y_{t-i}^{(k-i)})\right\|^2\notag\\
    \le&h_t(y_t^{(0)}-v_t)+\lambda\left\|y_t^{(0)}-\sum_{i=1}^pC_iy_{t-i}^{(0)}\right\|^2+\lambda\left\|\sum_{i=1}^{k-1}C_i(y_{t-i}^{(0)}-y_{t-i}^{(k-i)})\right\|^2\label{a.a.e.2.1}\\
    \le&h_t(y_t^{(0)}-v_t)+2\lambda c(y_t^{(0)},y_{t-1:t-p}^{(0)})+\lambda\alpha\sum_{i=1}^{k-1}\|C_i\|\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2.\label{a.a.e.2.2}
\end{align}
\end{subequations}
We have used the AM-GM inequality in \Cref{a.a.e.2.1} and Jensen's inequality in \Cref{a.a.e.2.2}. %\adam{becareful to use eqref of Cref for equations. I fixed it here.}




Since $h$ is $l$-strongly smooth, for any $\eta_{1,k}>0$,
\begin{align}\label{appendix.a.2}
    \frac{1}{1+\eta_{1,k}}h_t(y_t^{(k)}-v_t)\le h_t(y_t^{(k)}-v_t^{(k)})+\frac{l}{2\eta_{1,k}}\|v_t-v_t^{(k)}\|^2.
\end{align}
Next, using the fact that the function $\frac{\lambda}{2}\|y_t^{(k)}-y\|^2$ is $\lambda$-strongly smooth in $y$, for any $\eta_{2,k}>0$, we have
\begin{align}\label{appendix.a.3}
    \frac{1}{1+\eta_{2,k}}&\cdot\frac{\lambda}{2}\left\|y_t^{(k)}-\sum_{i=1}^pC_iy_{t-i}^{(k)}\right\|^2\notag\\
    \le&\frac{\lambda}{2}\left\|y_t^{(k)}-\sum_{i=1}^{k-1}C_iy_{t-i}^{(k-i)}-\sum_{i=k}^pC_iy_{t-i}^{(0)}\right\|^2+\frac{\lambda}{2\eta_{2,k}}\left\|\sum_{i=1}^{k-1}C_i(y_{t-i}^{(k)}-y_{t-i}^{(k-i)})+\sum_{i=k}^pC_i(y_{t-i}^{(k)}-y_{t-i}^{(0)})\right\|^2.
\end{align}
Substituting \Cref{appendix.a.3} and \Cref{appendix.a.2} into \Cref{appendix.a.1}, we have 
\begin{align}\label{appendix.a.4}
    \frac{1}{1+\eta_{1,k}}&h_t(y_t^{(k)}-v_t)+\frac{1}{1+\eta_{2,k}}\cdot\frac{\lambda}{2}\left\|y_t^{(k)}-\sum_{i=1}^pC_iy_{t-i}^{(k)}\right\|^2\notag\\
\le&h_t(y_t^{(k)}-v_t^{(k)})+\frac{l}{2\eta_{1,k}}\|v_t-v_t^{(k)}\|^2\notag\\
&+\frac{\lambda}{2}\left\|y_t^{(k)}-\sum_{i=1}^{k-1}C_iy_{t-i}^{(k-i)}-\sum_{i=k}^pC_iy_{t-i}^{(0)}\right\|^2+\frac{\lambda}{2\eta_{2,k}}\left\|\sum_{i=1}^{k-1}C_i(y_{t-i}^{(k)}-y_{t-i}^{(k-i)})+\sum_{i=k}^pC_i(y_{t-i}^{(k)}-y_{t-i}^{(0)})\right\|^2\notag\\
\le&h_t(y_t^{(0)}-v_t)+2\lambda c(y_t^{(0)},y_{t-1:t-p}^{(0)})+\lambda\alpha\sum_{i=1}^{k-1}\|C_i\|\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2-\frac{1}{2}\cdot\frac{m\lambda}{m+\lambda}\|v_t-v_t^{(k)}\|^2\notag\\
&+\frac{l}{2\eta_{1,k}}\|v_t-v_t^{(k)}\|^2+\frac{\lambda}{2\eta_{2,k}}\left\|\sum_{i=1}^{k-1}C_i(y_{t-i}^{(k)}-y_{t-i}^{(k-i)})+\sum_{i=k}^pC_i(y_{t-i}^{(k)}-y_{t-i}^{(0)})\right\|^2\notag\\
\le&h_t(y_t^{(0)}-v_t)+2\lambda c(y_t^{(0)},y_{t-1:t-p}^{(0)})+\frac{l}{2\eta_{1,k}}\|v_t-v_t^{(k)}\|^2-\frac{1}{2}\cdot\frac{m\lambda}{m+\lambda}\|v_t-v_t^{(k)}\|^2\notag\\
&+\frac{\lambda\alpha}{\eta_{2,k}}\sum_{i=1}^p\|C_i\|\|y_{t-i}^{(0)}-y_{t-i}^{(k)}\|^2+\lambda\alpha(1+\frac{1}{\eta_{2,k}})\sum_{i=1}^{k-1}\|C_i\|\|y_{t-i}^{(k-i)}-y_{t-i}^{(0)}\|^2.
\end{align}
We have used the AM-GM inequality, the triangle inequality, and Jensen's inequality in the last step.

Summing \Cref{appendix.a.4} over time and defining $V(k)=\frac{1}{2}\sum_{t=1}^T\|v_t^{(0)}-v_t^{(k)}\|^2$ yields 
\begin{subequations}
\begin{align}
    \min&\left\{\frac{1}{1+\eta_{1,k}},\frac{\lambda}{1+\eta_{2,k}}\right\}\sum_{t=1}^T(H_t^{(k)}+M_t^{(k)})\notag\\
\le&2\sum_{t=1}^T(H_t^{(0)}+\lambda M_t^{(0)})+(\frac{l}{\eta_{1,k}}-\frac{m\lambda}{m+\lambda})\sum_{t=1}^T\frac{1}{2}\|v_t-v_t^{(k)}\|^2\notag\\
&+\frac{\lambda\alpha^2}{\eta_{2,k}}\sum_{t=1}^T\|y_t^{(0)}-y_t^{(k)}\|^2+\lambda\alpha^2(1+\frac{1}{\eta_{2,k}})\sum_{j=1}^{k-1}\sum_{t=1}^T\|y_t^{(j)}-y_t^{(0)}\|^2\label{a.a.e.6.1}\\
\le&2\sum_{t=1}^T(H_t^{(0)}+\lambda M_t^{(0)})+(\frac{l}{\eta_{1,k}}-\frac{m\lambda}{m+\lambda})\sum_{t=1}^TV(k)\notag\\
&+\lambda\alpha^2\left(\frac{1}{\eta_{2,k}}\cdot16V(k)+\sum_{j=k-1}^1(\frac{1+2\alpha^2}{\eta_{2,k}}+1)(1+2\alpha^2)^{k-1-j}\cdot16V(j)\right)\label{a.a.e.6.2}.
\end{align}
\end{subequations}
We have applied \Cref{appendix.a.4} in \Cref{a.a.e.6.1}, and \Cref{l.bound} in \Cref{a.a.e.6.2}. The inequality shows that, the upper bound on the cost of iROBD with delay $k$ does not only involves the estimation error in the $k^\mathrm{th}$ iteration (i.e. $V(k)$), but also involves errors from all previous iterations of estimation (i.e. $V(j)$, $j=1,\cdots,k-1$). To understand the impact of estimation errors from different iterations, we need to analyse the cost of iROBD under different delays, from $1$ to $k$, as a whole. To do this, define $P(k)=\min\left\{\frac{1}{1+\eta_{1,k}},\frac{\lambda}{1+\eta_{2,k}}\right\}\sum_{t=1}^T(H_t^{(k)}+M_t^{(k)})$, and then we have
\begin{align}
\frac{1}{\prod_{i=1}^{k-1}\eta_{2,i}}P(k)
    \le&\frac{1}{\prod_{i=1}^{k-1}\eta_{2,i}}P(k)+\frac{1}{\prod_{i=1}^{k-2}\eta_{2,i}}P(k-1)+\cdots+\frac{1}{\eta_{2,1}}P(2)+P(1)\notag\\
    \le&(1+\frac{2}{\eta_{2,1}}+\cdots+\frac{2}{\prod_{i=1}^{k-1}\eta_{2,i}})\sum_{t=1}^T(H_t^{(0)}+M_t^{(0)})\notag\\
    &+(\frac{l}{\eta_{1,k}}-\frac{m\lambda}{m+\lambda}+\frac{16\lambda\alpha^2}{\eta_{2,k}})\frac{V(k)}{\prod_{i=1}^{k-1}\eta_{2,i}}+\sum_{j=k-1}^1a(j)V(j).
\end{align}
Here the coefficient $a(j)$ is
\begin{align}
    a(j)=&\frac{1}{\prod_{i=1}^{j-1}\eta_{2,i}}\left(\frac{l}{\eta_{1,j}}-\frac{m\lambda}{m+\lambda}+16\lambda\sum_{i=j+1}^k\left(\left(1+\frac{1+2\alpha^2}{\eta_{2,i}}\right)\frac{\alpha^2}{\eta_{2,j}}\prod_{q=j+1}^{i-1}\left(\frac{1+2\alpha^2}{\eta_{2,q}}\right)\right)\right).
\end{align}
Here $\eta_{1,i}$ and $\eta_{2,i}$ are parameters from \Cref{appendix.a.2} and \Cref{appendix.a.3}. For $i=1,\cdots,k$, we pick $\eta_{2,i}=\eta_{i}$ and $\eta_{1,i}=\frac{1-\eta_{i}+\lambda}{\lambda}$, so that $\frac{1}{1+\eta_{1,j}}=\frac{\lambda}{1+\eta_{2,j}}$. This gives
%Let $\frac{1}{1+\eta_{1,j}}=\frac{\lambda}{1+\eta_{2,j}}$ hold for all $j$.  \adam{explain why we can do this}  This gives 
\begin{align}\label{a.a.coefficient}
    a(j)=&\frac{\lambda}{\prod_{i=1}^{j-1}\eta_{i}}\left(\frac{l}{1+\eta_{j}-\lambda}-\frac{m}{m+\lambda}+16\sum_{i=j+1}^k\left(\left(1+\frac{1+2\alpha^2}{\eta_{i}}\right)\frac{\alpha^2}{\eta_{j}}\prod_{q=j+1}^{i-1}\left(\frac{1+2\alpha^2}{\eta_{q}}\right)\right)\right).
\end{align}
When $\eta_{j}=O(l+2\alpha^2)$ for all $j$, the negative term $-\frac{m\lambda}{m+\lambda}$ will dominate in $a(j)$. Thus, we can make the coefficient $a(j)$ non-positive, and then we have
\begin{subequations}
\begin{align}
    \frac{\lambda}{(1+\eta_{k})\prod_{i=1}^{k-1}\eta_{i}}&\sum_{t=1}^T(H_t^{(k)}+M_t^{(k)})
=\frac{1}{\prod_{i=1}^{k-1}\eta_{i}}P(k)\notag\\
\le&(1+\frac{2}{\eta_{1}}+\cdots+\frac{2}{\prod_{i=1}^{k-1}\eta_{i}})\sum_{t=1}^T(H_t^{(0)}+\lambda M_t^{(0)})\notag\\
\le&(1+\frac{2}{\eta_{1}}+\cdots+\frac{2}{\prod_{i=1}^{k-1}\eta_{i}})\sum_{t=1}^T\left(H_t^*+\frac{\lambda(m+\lambda)}{m+(1-\alpha^2)\lambda}M_t^*\right).\label{appendix.a.last}
\end{align}
\end{subequations}

Finally, recall that the sequence $\{y_t^{(0)}\}$ is identical to the decisions of ROBD. Thus, \Cref{appendix.a.last} follows from the analysis on ROBD in \citep{shi2020online}, which shows that
\begin{align}\label{appendix.a.5}
    \sum_{t=1}^T(H_t^{(0)}+\lambda M_t^{(0)})\le\sum_{t=1}^T\left(H_t^*+\frac{\lambda(m+\lambda)}{m+(1-\alpha^2)\lambda}M_t^*\right),
\end{align}
Therefore, we have that
\begin{align}
    \sum_{t=1}^T(H_t^{(k)}+M_t^{(k)})\le\frac{1+\eta_{2,k}}{\lambda}(\prod_{i=1}^{k-1}\eta_{i}+2\sum_{i=2}^{k-1}\prod_{j=i}^{k-1}\eta_{j}+2)\sum_{t=1}^T\left(\frac{1}{\lambda}H_t^*+\frac{m+\lambda}{m+(1-\alpha^2)\lambda}M_t^*\right),\notag
\end{align}
which immediately leads to a bound on the competitive ratio of iROBD of $(O(l+2\alpha^2))^k\max\{\frac{1}{\lambda},\frac{m+\lambda}{m+(1-\alpha^2)\lambda}\}$.

\end{proof}




\section{Proof and Example of Theorem \ref{t.reduction}}\label{appendix.reduction1}

In this section we will present a reduction (Algorithm \ref{a.reduction}) from the control problem mentioned in \Cref{Delay2OnlineControl} to online convex optimization with structured memory and feedback delay, and then provide a proof of \Cref{t.reduction}.

We restate here the online control problem:\begin{align*}
    &\min_{u_t}\sum_{t=1}^{T} \frac{q_t}{2}\|x_t\|^2 + \sum_{t=0}^{T-1}\frac{1}{2}\|u_t\|^2 \\
    &\mathrm{s.t.~}\quad x_{t+1}=Ax_t+Bu_t+w_t.
\end{align*}    
We are going to transform the control problem above into the form of minimizing $\sum_{t=1}^Tf_t(y_t)+c(y_{t:t-p})$. Before presenting the reduction, we introduce necessary notations for \begin{enumerate*}[label=(\arabic*)]
    \item canonical pair $(A,B)$; \item extracted matrices $\{C_i\}$; \item accumulative disturbances $r(t,i,j)$
\end{enumerate*}:

In \Cref{eq:example-1} we have assumed pair $(A,B)$ is in controllable canonical form. To be specific, canonical $(A,B)$ is in the following form:
\begin{center}
    \includegraphics[width=0.6\textwidth]{Canonical_form.png}
\end{center}
where each $*$ represents a (possibly) non-zero entry, and the rows of $B$ with $1$ are the same rows of $A$ with $*$ \citep{luenberger1967canonical,li2019online}. It is well-known that any controllable system can be linearly transformed to the canonical form. Denote the indices of non-zero rows in matrix $B$ to be $\{k_1\cdots k_d\}$, and denote the set to be $\mathcal{I}$. Define a mapping $\psi:\mathbb{R}^n\to\mathbb{R}^d$ as $\psi(x)=\left(x^{(k_1)},\cdots,x^{(k_d)}\right)^T.$
Let $p_i=k_i-k_{i-1}$ for $1\le i\le d$, where $k_0=0$. The controllability index of $(A,B)$ is defined by $p=\max\{p_i\}$~\citep{li2019online}. 

Also, define $C_i\in\mathbb{R}^{d\times d}$ for $i=1,\cdots,p$ \footnote{We slightly abuse the notation $C_i$.  In Theorem \ref{t.delay} $C_i$ could be any matrix in $\mathbb{R}^{d\times d}$, but here $C_i$ is a specific matrix from rearranging $A$.}
as elements extracted from $A$ for $1\le i\le p$, $1\le h\le d$ and $1\le j\le d$, %if $i\le p_j$, $C_i(h,j)=A(k_h,k_j+1-i)$; otherwise, $C_i(h,j)=0$. 
\begin{align}\label{def.Ci}
    C_i(h,j)=
\begin{cases}
A(k_h,k_j+1-i) & \text{if $i\le p_j$};\\
0 & \text{otherwise}.
\end{cases}
\end{align}

Moreover, for $1\le i\le d$ and $1\le j\le p_i$ define $r(t,i,j)$ as accumulative disturbances over time on the system state:
\begin{align}
    r(t,i,j)=\sum_{\tau=t+1-j}^{t-1}w_{\tau}^{(k_i-\tau+t-j)},\label{function-r}
\end{align}
for $j\ge2$; and $r(t,i,1)=0$ for $j=1$. In this way, we can turn any element of $x_t$ into sum of $\psi(\cdot)$ and $r(t,\cdot,\cdot)$:
\begin{align}
    x_t^{\left(1-j+k_i\right)}=\left(\psi(x_{t-j+1})\right)^{(i)}+r(t,i,j).\label{t.reduction.aux}
\end{align}
The first term of the right hand side involves the system state at step $t-j+1$, while the second term involves the disturbances to the system from step $t-j+1$ to step $t-1$. This decomposition uses a different approach than that in previous work such as \citep{shi2020online}, and is the first to deal with the coupling between system state and future disturbances, which is what leads to feedback delay in the resulting online optimization problem.



Now, we can restate the theorem and begin the proof:
\begin{theorem*}
Consider the online control problem in \Cref{eq:example-1}. Assume the coefficients $q_{t:t+p-1}$ are observable at step $t$. It can be converted to an instance of OCO with structured memory and feedback delay using Algorithm \ref{a.reduction}. 
\end{theorem*}

\begin{proof}
Recall that we define operator $\psi:\mathbb{R}^n\to\mathbb{R}^d$ as 
\begin{align}
    \psi(x)=\left(x^{(k_1)},\cdots,x^{(k_d)}\right)^T.
\end{align}

Let $z_t=\psi(x_t)$, that is, $z_t^j=x_t^{(k_j)}$. For $i\notin\mathcal{I}$, we have $x_t^{(i)}=x_{t-1}^{i+1}+w_{t-1}^{(i)}$. In this way,
\begin{align}
    x_t=&(z_{t-p_1+1}^{(1)}+\sum_{\tau=t+1-p_1}^{t-1}w_{\tau}^{(t-\tau)},\cdots,z_{t}^{(1)},\cdots,z_{t-p_d+1}^{(d)}+\sum_{\tau=t+1-p_d}^{t-1}w_{\tau}^{(k_d-\tau+t-p_d)},\cdots,z_t^{(d)})^T.\notag
\end{align}
Here $r(t,i,j)=\sum_{\tau=t+1-j}^{t-1}w_{\tau}^{(k_i+t-\tau-j)}$ for $j\ge2$. When $j=1$, $r(t,i,1)=0$.\\
Notice that
\begin{align}
    \sum_{t=0}^Tq_t\|x_t\|^2_2=&\sum_{t=0}^Tq_t\sum_{i=1}^d\sum_{j=1}^{p_i}\left(z_{t+1-j}^{(i)}+r(t,j,i)\right)^2\notag\\
    =&\sum_{t=0}^{T-1}\sum_{i=1}^d\left(\sum_{j=1}^{p_i}q_{t+j}\left(z_{t+1}^{(i)}+r(t+j,j,i)\right)^2\right).\notag
\end{align}

This lets us define a hitting cost
\begin{align}
    &h_t(y)=\frac{1}{2}\sum_{i=1}^d\left(\sum_{j=1}^{p_i}q_{t+j}\left(y^{(i)}+r(t+j,j,i)\right)^2\right).\notag
\end{align}
In this way, we can transform the total cost as following:
\begin{align}
    &\frac{1}{2}\sum_{t=0}^T(q_t\|x_t\|^2+\|u_t\|^2)=\sum_{t=0}^{T-1}h_t(z_{t+1})+\frac{1}{2}\|u_t\|^2.\notag
\end{align}
Recall that coefficients $q_{t:t+p-1}$ are observable at time $t$. When picking $z_{t+1}$, we do not know $h_t$ because it depends on information about $r(t+j,i,j)$, which depends on $w_{t+1:t+p-1}$.

We can also represent $u_t$ as follows:
\begin{align}
    u_t=&z_{t+1}-\psi(w_t)-A(\mathcal{I,:})x_t\notag\\
    =&z_{t+1}-\psi(w_t)-\sum_{i=1}^pC_iz_{t+1-i}.\notag
\end{align}

Next, we recursively define a sequence $\{y_t\}_{t\ge-p}$ as the accumulation of control actions, i.e.
\begin{align}
    y_t=u_t+\sum_{i=1}^pC_iy_{t-i},\forall t\ge0,\notag
\end{align}
where $y_t=0$ for all $t<0$. We also recursively define a sequence $\{\zeta_t\}_{t\ge-p}$ as the accumulation of control noise, i.e.
\begin{align}
    \zeta_t=\psi(w_t)+\sum_{i=1}^pC_i\zeta_{t-i},\forall t\ge0,\notag
\end{align}
where $\zeta_t=0$ for all $t<0$. Setting $x_0=0$ gives the following for all $t\ge-1$:
\begin{align}
    z_{t+1}=y_t+\zeta_t\notag.
\end{align}

Using the above, we can now formulate the problem as online optimization problem with memory and delay, where the hitting cost function is given by
\begin{align}
    f_t(y)=h_t(y+\zeta_t),\notag
\end{align}
and the switching cost is given by
\begin{align}
    \frac{1}{2}\|y_t-\sum_{i=1}C_iy_{t-i}\|^2.\notag
\end{align}
Note that $h_t$ involves $w_{t+p-1}$, which is not revealed until before picking $y_{t+p}$. In other words, at step $t$, only $f_{1:t-p}$ is known then, due to the reduction structure, there is a delay of $p$ steps. 
\end{proof}

\textbf{Example.} To illustrate the reduction, we consider an example of a 2-d system with the following objective and dynamics:
\begin{align}
    &\min_u\sum_{t=0}^{200}\frac{1}{2}\|x_t\|^2+\frac{1}{2}\|u_t\|^2\notag\\
 \mathrm{s.t.}~&x_{t+1}=\begin{bmatrix}
    0 & 1 \\
    -1 & 2
    \end{bmatrix}x_t+\begin{bmatrix}
     0 \\ 1
    \end{bmatrix}u_t+w_t\notag.
\end{align}
There is a disturbance $w_t$ on the system state $x_t$ and an input $u_t$. In this setting, the disturbance is unknown and potentially adversarial. To begin, we write the system in the following form:
\begin{align}
    x_{t+1}^{(1)}&=x_t^{(2)}+w_t^{(1)},\notag\\
    x_{t+1}^{(2)}&=2x_t^{(2)}-x_{t-1}^{(2)}+u_t+w_t^{(2)}\notag.
\end{align}
To transform the problem, we define
\begin{align}
    &y_t=u_t+2y_{t-1}-y_{t-2},\notag\\
    &\zeta_t=w_t^{(2)}+2\zeta_{t-1}-\zeta_{t-2},\notag\\
    &y_t+\zeta_t=x_{t+1}^{(2)},\notag\\
    &h_t(y)=y^2+(y+w_{t+1}^{(1)})^2.\label{example-delay-function}
\end{align}
Thus, the control problem is transformed into
\begin{align}
    \min_y\sum_{t=0}^{199}h_t(y_t+\zeta_t)+\frac{1}{2}\|y_t-2y_{t-1}-y_{t-2}\|^2.\notag
\end{align}
Note that, from the \Cref{example-delay-function} we can see that, at time $t$, the new cost function $h_t$ involves the disturbance from the next round, $w_{t+1}$, which is not revealed yet.

\section{Proof of Theorem \ref{t.reduction2}}\label{appendix.reudction2}
For simplicity we consider the trajectory tracking task: the cost function is given by $f_t(x_t)=\frac{1}{2}(x_t-v_t)^\top Q_t(x_t-v_t)$, where $\{v_t\}$ is the desired trajectory to track.

\begin{theorem*}
Consider the online control problem in \Cref{eq:example-2}. If $Q_t$ is observable at step $t$, and only the trajectory $v_{1:t-k}$ is known, i.e., there are $k$ steps of feedback delay, then it can be converted to an instance of online optimization with switching cost and feedback delay using \Cref{a.reduction-2}.
\end{theorem*}
\begin{proof}

From the dynamics we know that $u_t=x_{t+1}-Ax_t-\delta(x_t).$ Let $f_t(y)=\frac{1}{2}(y-v_t)^TQ_t(y-v_t)$. Using this we can represent the total cost as
\begin{align}
    \sum_{t=1}^T&\frac{1}{2}(x_t-v_t)^TQ_t(x_t-v_t)+\sum_{t=0}^{T-1}\frac{1}{2}\|u_t\|^2\notag\\
    =&\sum_{t=1}^T\frac{1}{2}(x_t-v_t)^TQ_t(x_t-v_t)+\sum_{t=0}^{T-1}\frac{1}{2}\|x_{t+1}-Ax_t-\delta(x_t)\|^2\notag\\
    =&\sum_{t=1}^Tf_t(y_t)+\frac{1}{2}\|y_t-Ay_{t-1}-\delta(y_{t-1})\|^2,\notag
\end{align}
where $y_t=x_t$ for all $t$. In this way, we have formulated the problem as online optimization with delay and nonlinear switching cost. Notice that, at time step $t$, only $v_{1:t-k}$ is known, so there is a $k$-step delay on the minimizer of the hitting cost function $f_t$.

\end{proof}


\section{Remark 1}\label{appendix.remark1}

We just consider a linear case where $\delta(y)=Ly$ with $L>0$ a constant. We prove here that the lower bound on the competitive ratio of any online algorithm in this setting matches the upper bound on the competitive ratio of iROBD, which is
\begin{align}
    \frac{1}{2}\left(1+\frac{2L+L^2}{m}+\sqrt{\left(1+\frac{2L+L^2}{m}\right)^2+\frac{4}{m}}\right).\notag
\end{align}

Tightness in this case is a simple application of Theorem 4 in \citep{shi2020online}, so we restate it here.

\begin{theorem}
When the hitting cost functions are $m$-strongly convex in feasible set $\mathcal{X}$ and the switching cost is given by $c(y_t,y_{t-1})=\frac{1}{2}\|y_t-\alpha y_{t-1}\|^2$ for a constant $\alpha\ge1$, the competitive ratio of any online algorithm is lower bounded by
\begin{align}
    \frac{1}{2}\left(1+\frac{\alpha^2-1}{m}+\sqrt{\left(1+\frac{\alpha^2-1}{m}\right)^2+\frac{4}{m}}\right).\notag
\end{align}
\end{theorem}

To apply this in our context, we substitute $\alpha$ in the theorem above with $1+L$ in our setting.  This  immediately gives that a lower bound on the competitive ratio of any algorithm is 
\begin{align}
    \frac{1}{2}\left(1+\frac{2L+L^2}{m}+\sqrt{\left(1+\frac{2L+L^2}{m}\right)^2+\frac{4}{m}}\right),\notag
\end{align}
which highlights that iROBD remains optimal in the basic linear setting.


\section{Remark 2: Unbounded Competitive Ratio}\label{appendix.remark2}
In this section we show that, even when $\delta$ is small, the competitive ratio of any online algorithm can be arbitrarily large when there are nonlinear switching costs. To show this we consider the following example:  $$\sum_{t=1}^T(y_t-v_t)^2+(y_t-y_{t-1}-\delta(y_{t-1}))^2.$$

Suppose the starting point of the online algorithm and the offline adversary is $y_0=y_0^*=0$. Let $\epsilon,\gamma>0$ be two small numbers, and $n\in\mathbb{N}^+$. The function $\delta$ is defined as:

\begin{align}
\delta(y)=\begin{cases}
\epsilon, & y\le n\epsilon; \\
-\epsilon\sin\left(\frac{\pi}{\gamma\epsilon}y-\frac{n\pi}{\gamma}-\frac{\pi}{2}\right), & n\epsilon<y\le n\epsilon+\gamma\epsilon;\\
-\epsilon, & y>n\epsilon+\gamma\epsilon.
\end{cases}\notag    
\end{align}

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics{delta.jpg}}
\caption{Illustration of the function $\delta(y)$.}
\label{f.delta}
\end{center}
\vskip -0.2in
\end{figure}

This is plotted in Figure \ref{f.delta}. Notice that the absolute value of $\delta$ is always no larger than $\epsilon$, and by adjusting the value of $\epsilon$, it can be made it as small as desired.

We consider a sequence $\{v_t\}$ such that the online algorithm follows exactly the trajectory through steps $t=1,2,\cdots,n$ and is forced to incur a huge switching cost at step $t=n+1$ while the adversary makes use of the property of $\delta$ and departs earlier in order to achieve a much smaller cost. More specifically, for $t=1,2,\cdots,n+1$, the trajectory $v_t$ is:
\begin{align}
    v_t=\begin{cases}
    t\cdot\epsilon, & t\in\{1,2,\cdots,n\};\\
    (n-1)\epsilon, & t=n+1.
    \end{cases}\notag
\end{align}
Suppose the online algorithm first chooses $y_t$, which does not equal $v_t$ at step $t=t_0$. If $t_0<n+1$, we stop the game at step $t_0$, and compare the online algorithm with an offline adversary which always stays chooses $y_t=v_t$. The total cost of the offline adversary is:
\begin{align}
    &\sum_{t=1}^{t_0}(y_t-v_t)^2+(y_t-y_{t-1}-\delta(y_{t-1}))^2    =\sum_{t=1}^{t_0}(t\epsilon-t\epsilon)^2-(t\epsilon-(t-1)\epsilon-\epsilon)^2
    =0,\notag
\end{align}
but the total cost of the online algorithm is non-zero. So the competitive ratio is unbounded.  

Now we consider the case when the algorithm decides on $y_t=v_t=t\cdot\epsilon$ for $i=1,\cdots,n$. In this case the cost incurred at step $n+1$ is:
\begin{align}
   (y_{n+1}-v_{n+1})^2+(y_{n+1}-y_n-\delta(y_n))^2
   =&(y_{n+1}-(n-1)\epsilon)^2+(y_{n+1}-(n+1)\epsilon)^2
   \ge2\epsilon^2.\notag
\end{align}

However, consider another sequence
\begin{align}
    y'_t=\begin{cases}
    t\cdot\epsilon, & t\in\{0,1,2,\cdots,n-1\};\\
    n\epsilon+\gamma\epsilon, & t=n;\\
    (n-1)\epsilon & t=n+1.
    \end{cases}.\notag
\end{align}
In this case the cost of $y_1',y_2',\cdots,y_{n+1}'$ is
\begin{align}
    \sum_{t=1}^{n+1}(y_t'-v_t)^2+&(y_t'-y_{t-1}'-\delta(y_{t-1}'))^2\notag\\
    =&\sum_{t=n}^{n+1}(y_t'-v_t)^2+(y_t'-y_{t-1}'-\delta(y_{t-1}'))^2\notag\\
    =&(n\epsilon+\gamma\epsilon-n\epsilon)^2+(n\epsilon+\gamma\epsilon-(n-1)\epsilon-\epsilon)+((n-1)\epsilon-n\epsilon-\gamma\epsilon-(-\epsilon))^2\notag\\
    =&3\gamma\epsilon^2.\notag
\end{align}
This cost is no smaller than the offline optimal; therefore, the competitive ratio of the online algorithm is bounded by
\begin{align}
    \frac{cost(ALG)}{cost(OPT)}\ge\frac{2\epsilon^2}{3\gamma\epsilon^2}=\frac{2}{3\gamma}.\notag
\end{align}
Since $\gamma\to0^+$, we can see the competitive ratio is unbounded.