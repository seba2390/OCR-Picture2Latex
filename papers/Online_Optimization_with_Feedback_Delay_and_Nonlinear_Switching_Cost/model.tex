Online convex optimization with memory has emerged as an important and challenging area with a wide array of applications, see \citep{lin2012online,anava2015online,chen2018smoothed,goel2019beyond,agarwal2019online,bubeck2019competitively} and the references therein.  Many results in this area have focused on the case of online optimization with switching costs (movement costs), a form of one-step memory, e.g., \citep{chen2018smoothed,goel2019beyond,bubeck2019competitively}, though some papers have focused on more general forms of memory, e.g., \citep{anava2015online,agarwal2019online}. In this paper we, for the first time, study the impact of feedback delay and nonlinear switching cost in online optimization with switching costs. 

An instance consists of a convex action set $\mathcal{K}\subset\mathbb{R}^d$, an initial point $y_0\in\mathcal{K}$, a sequence of non-negative convex cost functions $f_1,\cdots,f_T:\mathbb{R}^d\to\mathbb{R}_{\ge0}$, and a switching cost $c:\mathbb{R}^{d\times(p+1)}\to\mathbb{R}_{\ge0}$. To incorporate feedback delay, we consider a situation where the online learner only knows the geometry of the hitting cost function at each round, i.e., $f_t$, but that the minimizer of $f_t$ is revealed only after a delay of $k$ steps, i.e., at time $t+k$.  This captures practical scenarios where the form of the loss function or tracking function is known by the online learner, but the target moves over time and measurement lag means that the position of the target is not known until some time after an action must be taken. 
To incorporate nonlinear (and potentially nonconvex) switching costs, we consider the addition of a known nonlinear function $\delta$ from $\mathbb{R}^{d\times p}$ to $\mathbb{R}^d$ to the structured memory model introduced previously.  Specifically, we have
\begin{align}
c(y_{t:t-p}) = \frac{1}{2}\|y_t-\delta(y_{t-1:t-p})\|^2,    \label{e.newswitching}
\end{align}
where we use $y_{i:j}$ to denote either $\{y_i, y_{i+1}, \cdots, y_j\}$ if $i\leq j$, or  $\{y_i, y_{i-1}, \cdots, y_j\}$ if $i > j$ throughout the paper. Additionally, we use $\|\cdot\|$ to denote the 2-norm of a vector or the spectral norm of a matrix.

In summary, we consider an online agent that interacts with the environment as follows:
% \begin{inparaenum}[(i)] 
\begin{enumerate}%[leftmargin=*]
    \item The adversary reveals a function $h_t$, which is the geometry of the $t^\mathrm{th}$ hitting cost, and a point $v_{t-k}$, which is the minimizer of the $(t-k)^\mathrm{th}$ hitting cost. Assume that $h_t$ is $m$-strongly convex and $l$-strongly smooth, and that $\arg\min_y h_t(y)=0$.
    \item The online learner picks $y_t$ as its decision point at time step $t$ after observing $h_t,$  $v_{t-k}$.
    \item The adversary picks the minimizer of the hitting cost at time step $t$: $v_t$. 
    \item The learner pays hitting cost $f_t(y_t)=h_t(y_t-v_t)$ and switching cost $c(y_{t:t-p})$ of the form \eqref{e.newswitching}.
\end{enumerate}

The goal of the online learner is to minimize the total cost incurred over $T$ time steps, $cost(ALG)=\sum_{t=1}^Tf_t(y_t)+c(y_{t:t-p})$, with the goal of (nearly) matching the performance of the offline optimal algorithm with the optimal cost $cost(OPT)$. The performance metric used to evaluate an algorithm is typically the \textit{competitive ratio} because the goal is to learn in an environment that is changing dynamically and is potentially adversarial. Formally, the competitive ratio (CR) of the online algorithm is defined as the worst-case ratio between the total cost incurred by the online learner and the offline optimal cost: $CR(ALG)=\sup_{f_{1:T}}\frac{cost(ALG)}{cost(OPT)}$.

It is important to emphasize that the online learner decides $y_t$ based on the knowledge of the previous decisions $y_1\cdots y_{t-1}$, the geometry of cost functions $h_1\cdots h_t$, and the delayed feedback on the minimizer $v_1\cdots v_{t-k}$. Thus, the learner has perfect knowledge of cost functions $f_1\cdots f_{t-k}$, but incomplete knowledge of $f_{t-k+1}\cdots f_t$ (recall that $f_t(y)=h_t(y-v_t)$).

Both feedback delay and nonlinear switching cost add considerable difficulty for the online learner compared to versions of online optimization studied previously. Delay hides crucial information from the online learner and so makes adaptation to changes in the environment more challenging. As the learner makes decisions it is unaware of the true cost it is experiencing, and thus it is difficult to track the optimal solution. This is magnified by the fact that nonlinear switching costs increase the dependency of the variables on each other. It further stresses the influence of the delay, because an inaccurate estimation on the unknown data, potentially magnifying the mistakes of the learner. 

The impact of feedback delay has been studied previously in online learning settings without switching costs, with a focus on regret, e.g., \citep{joulani2013online,shamir2017online}.  However, in settings with switching costs the impact of delay is magnified since delay may lead to not only more hitting cost in individual rounds, but significantly larger switching costs since the arrival of delayed information may trigger a very large chance in action.  To the best of our knowledge, we give the first competitive ratio for delayed feedback in online optimization with switching costs. 

We illustrate a concrete example application of our setting in the following.

\begin{example}[Drone tracking problem]
\label{example:drone} \emph{
Consider a drone with vertical speed $y_t\in\mathbb{R}$. The goal of the drone is to track a sequence of desired speeds $y^d_1,\cdots,y^d_T$ with the following tracking cost:}
\begin{equation}
    \sum_{t=1}^T \frac{1}{2}(y_t-y^d_t)^2 + \frac{1}{2}(y_t-y_{t-1}+g(y_{t-1}))^2,
\end{equation}
\emph{where $g(y_{t-1})$ accounts for the gravity and the aerodynamic drag. One example is $g(y)=C_1+C_2\cdot|y|\cdot y$ where $C_1,C_2>0$ are two constants~\cite{shi2019neural}. Note that the desired speed $y_t^d$ is typically sent from a remote computer/server. Due to the communication delay, at time step $t$ the drone only knows $y_1^d,\cdots,y_{t-k}^d$.}

\emph{This example is beyond the scope of existing results in online optimization, e.g.,~\cite{shi2020online,goel2019beyond,goel2019online}, because of (i) the $k$-step delay in the hitting cost $\frac{1}{2}(y_t-y_t^d)$ and (ii) the nonlinearity in the switching cost $\frac{1}{2}(y_t-y_{t-1}+g(y_{t-1}))^2$ with respective to $y_{t-1}$. However, in this paper, because we directly incorporate the effect of delay and nonlinearity in the algorithm design, our algorithms immediately provide constant-competitive policies for this setting.}
\end{example}


\subsection{Related Work}
This paper contributes to the growing literature on online convex optimization with memory.  
Initial results in this area focused on developing constant-competitive algorithms for the special case of 1-step memory, a.k.a., the Smoothed Online Convex Optimization (SOCO) problem, e.g., \citep{chen2018smoothed,goel2019beyond}. In that setting, \citep{chen2018smoothed} was the first to develop a constant, dimension-free competitive algorithm for high-dimensional problems.  The proposed algorithm, Online Balanced Descent (OBD), achieves a competitive ratio of $3+O(1/\beta)$ when cost functions are $\beta$-locally polyhedral.  This result was improved by \citep{goel2019beyond}, which proposed two new algorithms, Greedy OBD and Regularized OBD (ROBD), that both achieve $1+O(m^{-1/2})$ competitive ratios for $m$-strongly convex cost functions.  Recently, \citep{shi2020online} gave the first competitive analysis that holds beyond one step of memory.  It holds for a form of structured memory where the switching cost is linear:
$
    c(y_{t:t-p})=\frac{1}{2}\|y_t-\sum_{i=1}^pC_iy_{t-i}\|^2,
$
with known $C_i\in\mathbb{R}^{d\times d}$, $i=1,\cdots,p$. If the memory length $p = 1$ and $C_1$ is an identity matrix, this is equivalent to SOCO. In this setting, \citep{shi2020online} shows that ROBD has a competitive ratio of 
\begin{align}
    \frac{1}{2}\left( 1 + \frac{\alpha^2 - 1}{m} + \sqrt{\Big( 1 + \frac{\alpha^2 - 1}{m}\Big)^2 + \frac{4}{m}} \right),
\end{align}
when hitting costs are $m$-strongly convex and $\alpha=\sum_{i=1}^p\|C_i\|$. 


Prior to this paper, competitive algorithms for online optimization have nearly always assumed that the online learner acts \emph{after} observing the cost function in the current round, i.e., have zero delay.  The only exception is \citep{shi2020online}, which considered the case where the learner must act before observing the cost function, i.e., a one-step delay.  Even that small addition of delay requires a significant modification to the algorithm (from ROBD to Optimistic ROBD) and analysis compared to previous work. 

As the above highlights, there is no previous work that addresses either the setting of nonlinear switching costs nor the setting of multi-step delay. However, the prior work highlights that ROBD is a promising algorithmic framework and our work in this paper extends the ROBD framework in order to address the challenges of delay and non-linear switching costs. Given its importance to our work, we describe the workings of ROBD in detail in Algorithm~\ref{robd}. 

\begin{algorithm}[t!]
  \caption{ROBD \citep{goel2019beyond}}
  \label{robd}
\begin{algorithmic}[1]
  \STATE {\bfseries Parameter:} $\lambda_1\ge0,\lambda_2\ge0$
  \FOR{$t=1$ {\bfseries to} $T$}
  \STATE {\bfseries Input:} Hitting cost function $f_t$, previous decision points $y_{t-p:t-1}$
  \STATE $v_t\leftarrow\arg\min_yf_t(y)$
  \STATE $y_t\leftarrow\arg\min_yf_t(y)+\lambda_1c(y,y_{t-1:t-p})+\frac{\lambda_2}{2}\|y-v_t\|^2_2$
  \STATE {\bfseries Output:} $y_t$
  \ENDFOR
   
\end{algorithmic}
\end{algorithm}

Another line of literature that this paper contributes to is the growing understanding of the connection between online optimization and adaptive control. The reduction from adaptive control to online optimization with memory was first studied in \citep{agarwal2019online} to obtain a sublinear static regret guarantee against the best linear state-feedback controller, where the approach is to consider a disturbance-action policy class with some fixed horizon.  Many follow-up works adopt similar reduction techniques \citep{agarwal2019logarithmic, brukhim2020online, gradu2020adaptive}. A different reduction approach using control canonical form is proposed by \citep{li2019online} and further exploited by \citep{shi2020online}. Our work falls into this category.  The most general results so far focus on Input-Disturbed Squared Regulators, which can be reduced to online convex optimization with structured memory (without delay or nonlinear switching costs).  As we show in \Cref{Control}, the addition of delay and nonlinear switching costs leads to a significant extension of the generality of control models that can be reduced to online optimization. 