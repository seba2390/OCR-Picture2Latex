%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference, letterpaper]{IEEEtran}
\IEEEoverridecommandlockouts
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/

% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.

%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/

% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/

%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.

% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

%\usepackage{subcaption}

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
\else
\fi

% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}
\usepackage{color}
\usepackage{fancyhdr}
\usepackage[caption=false,font=footnotesize]{subfig}


\usepackage{array}



\usepackage[compress,sort]{cite}

\renewcommand{\thispagestyle}[2]{} 

%\fancypagestyle{plain}{
%        \fancyhead{}
%        \fancyhead[C]{first page center header}
%        \fancyfoot{}
%        \fancyfoot[C]{first page center footer}
%}
%\pagestyle{fancy}


%\headheight 20pt
%\footskip 20pt

%\rhead{}

%Enter the first page number of your paper below
\setcounter{page}{1}

%Header
%\fancyhead[R]{\textit{Computing Conference 2018 \\ 10-12 July 2018 $|$ London, UK}}
%\renewcommand{\headrulewidth}{0pt}

%Footer
\fancyfoot[C]{IEEE}
\renewcommand{\footrulewidth}{0.5pt}
\fancyfoot[R]{\thepage \  $|$ P a g e }


\begin{document}

%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Improved Training for Self Training by Confidence Assessments}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{Dor Bank*}
\IEEEauthorblockA{Tel Aviv University\\
Tel Aviv, Israel\\
Email: dorbank@gmail.com}
\and

\IEEEauthorblockN{Daniel Greenfeld*}
\IEEEauthorblockA{Weizmann Institute of Science\\
Rehovot, Israel\\
Email: danielgreenfeld3@gmail.com}
\and
\IEEEauthorblockN{Gal Hyams*}
\IEEEauthorblockA{Tel Aviv University\\
Tel-Aviv, Israel\\
Email: gal.hyams@cs.tau.ac.il}
}



% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}

It is well known that for some tasks, labeled data sets may be hard to gather. Self-training, or pseudo-labeling, tackles the problem of having insufficient training data. In the self-training scheme, the classifier is first trained on a limited, labeled dataset, and after that, it is trained on an additional, unlabeled dataset, using its own predictions as labels, provided those predictions are made with high enough confidence. Using credible interval based on MC-dropout as a confidence measure, the proposed method is able to gain substantially better results comparing to several other pseudo-labeling methods and out-performs the former state-of-the-art pseudo-labeling technique by 7 $\%$ on the MNIST data-set. In addition to learning from large and static unlabeled datasets, the suggested approach may be more suitable than others as an online learning method where the classifier keeps getting new unlabeled data. The approach may be also applicable in the recent method of pseudo-gradients for training long sequential neural networks.
\end{abstract}

\footnotetext[1]{*equally contributed authors, writers are presented by the alphabeth order.}
\footnotetext[2]{Acknowledgements: This study was supported in part by fellowships from the Edmond J. Safra Center for Bioinformatics at Tel-Aviv University and from The Manna Center for Food Safety and Security at Tel-Aviv University.}
\begin{IEEEkeywords}
Semi-supervised learning; Self-Training; Limited training set; MNIST; Image classification
\end{IEEEkeywords}

\section{Introduction}
In the semi-supervised learning scheme, both labeled and unlabeled data are being used to train a classifier. This is especially appealing when labeled data is very limited but unlabeled data is abundant, which is often the case when labeling new data is expensive and sufficient labeled data is not yet easy to find. Such tasks include semantic segmentation, sentence stressing, video labeling and more.
A very common example of limited labeled data but practically unlimited unlabeled data is the on-line stage of a classifier, in which the training data is fully exploited, but unlabeled test data keeps coming.
From a practical standpoint, self-training is one of the most simple approaches that can be utilized in those cases. 
 In this setting, after finishing the supervised training stage of a classifier, it is possible to continue the learning process of the classifier on new unlabeled data, which may be the on-line unlabeled test samples it is asked to classify. Whenever the classifier encounters a sample on which the certainty that the classification is correct is high, this sample can be used as a training example along with that prediction as replacement for a label.
 The crucial question is how can the self-training classifier decide on which of the self-labeled samples it should train on. In other words - when should the predictions of the not yet fully trained classifier be trusted?
 In this work, different methods for training a self-training classifier are suggested and their utilities are analyzed. The suggested techniques can be easily implemented on top of any boosting and data augmentation methods, improving the obtained results.\\
 The main contributions of this work are: (1) demonstrating how to use the self-training method in the most effective manner. Specifically, using the algorithm suggested here, state-of-the-art results for self-training on MNIST were achieved, improving the former state-of-the-art results by 7 $\%$. (2) Suggesting an empirical limitation of the self-training method, including an empirical lower bound on the preliminary success rate and data set size when implementing the self-training or C-EM method on multi-class classification image tasks.\\
The remainder of the paper is organized as follows: a description of previous works on semi-supervised and specifically self training classification is brought in the next section. In the \textit{Methods} section a detailed description of the techniques for deciding the trustworthiness of the classifier on unlabeled samples is given. In the \textit{Experiments} section, a comparison between the results of the approaches described on methods is presented. The contribution and importance of this article is further discussed in \textit{Discussion}. Finally, future works are suggested in the last section.

\section{Previous Works}

\subsubsection{Self-training}
The approach of self-training was first presented by Nigam \textit{et. al.} \cite{nigam2000analyzing} and it was shown that it can be interpreted as an instance of the Classification Expectation Maximization algorithm \cite{Amini:2002:SLR:3000905.3000988}.
Implementing the self-training method harnessing Denoising Auto-Encoder and Dropout \cite{hinton2012improving} on the visible layer in order to avoid over-fitting to the training set \cite{lee2013pseudo} have achieved the best known results for the self-training method on MNIST data-set so far.
\subsubsection{Semi Supervised MNIST Classification}
 Additional approaches for the semi-supervised task, demonstrated for image classification on MNIST:\\
 Generative Adversarial Networks \cite{salimans2016improved} have been successfully used in order to achieve the state-of-the-art results for semi-supervised MNIST classification, based on labeled dataset containing ten samples for each of the ten classes. On the other hand, such balanced labeled set is not always available, and training a GAN well may be difficult for a lot of tasks and datasets. \\ The unsupervised Ladder Networks technique \cite{valpola2015neural} was successfully harnessed for semi-supervised classification \cite{NIPS2015_5947} with success on the MNIST task as well. Implementing this method is not trivial for a variety of tasks.\\
 Another successful technique for contending with insufficient labeled data is the augmentation method, usually used for vision tasks.  We note that the method examined in this work can be used on top of augmentation even after the latter was exploited to its fullest; in addition, the method examined here can be efficiently used for multiclass classification tasks outside the area of computer-vision.  
 
\section{Methods}
As stated before, the proposed training is done as follows: the model is first trained on some labeled dataset, and then the model is trained on unlabeled data using its own predictions as ground-truth, whenever a confidence condition is met. In classification tasks, the standard method for extracting a confidence-measure is by looking at the soft-max layer probabilities. Unfortunately, networks tend to be over-confident in that sense, rendering those probabilities not informative enough. We therefore applied maximal entropy regularization as recently suggested in \cite{pereyra2017regularizing}, which penalizes the network whenever the soft-max probabilities are too concentrated in one class.
With that said, even if we know how to measure a networks confidence in its predictions, a crucial challenge remains - setting a confidence-threshold by which to decide whether to trust those predictions or not. The trade-off is clear - a low threshold will result in a high false-positive (\textit{FP}) rate which will cause the network to train on wrong samples; a high threshold will result in a low true-positive (\textit{TP}) rate which will mean that not enough additional data is obtained to make a difference. Furthermore, looking at the soft-max probabilities does not exactly yield a desired confidence measure. Those probabilities represent the network's best guess, but the quantity of interest here is to what extent is this guess reliable. We therefore turned to additional methods that help asses if a prediction is trustworthy: 1. Using MC-dropout as another way to represent a networks uncertainty \cite{gal2015dropout} by running the same network multiple times on a given sample, each run sending to zero a random sample from the hidden units of the network, thus obtaining a distribution over the network's predictions  and 2. Bagging of two networks - even when the networks have exactly the same architecture, the random initialization of the weights and the fact that the training is stochastic due to dropout layers are enough to ensure that the networks will yield different results on the borderline cases. That is, if they agree on a prediction - the chances of it being correct are much greater. \\
Concretely, the following methods were examined as ways to determine when a prediction is likely to be correct:

\subsection{Soft-max threshold}
In this method, a hard coded threshold is compared against the highest soft-max result. It is assigned as a hyper-parameter, which the user should provide. It can be assigned differently for each class, and intuitively it requires prior knowledge on the unlabeled data.

\subsection{Ensemble consensus}
A different approach, would be to ask for a vote from several classifiers. 
For convex models, this approach is obviously meaningless. For neural networks, it fits great, even when the networks has the same architecture, since each network is randomly initialized to different values. The downside of using an ensemble of classifiers is that it requires storing and training several networks, which is not applicable for most tasks.

\subsection{Dropout consensus} 
A more tractable version of such agreement test is to run one network several times with dropout $<1$ value, and check the consistency of the predictions as a proxy to an agreement test between several classifiers. Note that while a Majority-Vote was shown to be efficient \cite{kuncheva2003limits}, its efficiency is maximal when the voters are negatively depended on each other - which is not the case here. In addition, in order to minimize false positives, consensus vote is more effective.

\subsection{Several outputs expectation}
By using dropout, the number of outputs which is applicable can be quite large. Therefore, an approximation for the "true" output probability can be calculated empirically.


\subsection{Credible interval of outputs}
The downside of the agreement based methods, is that it doesn't take into account the distribution of the output probabilities. For example, suppose using $90\%$ as the threshold, and for a given sample getting 5 outputs of $95\%,95\%,95\%,95\%,20\%$. In this case, the sample won't be taken into account because their average is less than the threshold, even though the fifth output is probably an outlier. 
 In order to increase the number of unlabeled samples the model would be trained on while keeping a low false positive rate, a confidence interval on the prediction's probabilities was examined as well.
 Usually, confidence interval is used as a mean to estimate a parameter, when considered as a random variable - under the Bayesian paradigm. The output of the network is an estimation of the probability that the sample belongs to each of the examined classes. From these, it is possible to look on the network's outcomes as parameters for the  Multinomial distribution and construct a confidence interval for their values. From the central limit theorem, the distance of the empirical average of each parameter from the true parameter is distributed approximately like normal distribution. Now, it is possible to use the t-distribution in order to construct the confidence interval for the average of those parameters, as the true variances of the averages are not known. Since the average is an unbiased estimator to the expectation and the parameter equals to its expectation in the Multinomial distribution, these intervals are valid for the estimation of the probabilities that the sample belongs to each of the classes.\\


\section{Experiments}
The experiments were done on the MNIST dataset, which includes 50,000 training samples, and 10,000 test samples.
The training samples were split into 2 groups: labeled training data, and unlabeled data.
Different sizes of training data were tested: 100, 150, 300, and 1000.
The test data remained the same for comparison with other benchmarks.
The network's architecture is 2 convolutional layers, 2 fully connected layers, and a soft-max layer. For sampling, MC-Dropout[23] was used for achieving MAP estimator of the certainty[5-7].
When using only one network, the regularization coefficient $\lambda$ was decreased during training from one to zero, as suggested in \cite{grandvalet2005semi}. Using positive value of 1 for $\lambda$, punishing over confidence, and gradually decreasing this value all the way to -1, aiming to direct the classifier to be more sure in its predictions gave similar results to decreasing the $\lambda$ only to 0. Dropout value of $50\%$ was used for both the training stages.
The training process consisted of 40 epochs on the training data, and after that, training on the unlabeled data, where after each epoch, another epoch was done with the original training data. The training was done with one Tesla K80 GPU with 11G memory.

\subsubsection{Soft-max threshold}
The confidence threshold of the soft-max score was searched for empirically. Unfortunately, even when using entropy regularization - with a positive coefficient \cite{pereyra2017regularizing} or a negative coefficient \cite{grandvalet2005semi} and a very high threshold (up to 0.999), the method is resulted in too many false positives, causing it to fail.

\subsubsection{Ensemble consensus}
Ensemble of classifiers is a natural approach for lowering the rate of the false-positives predictions. Implementation of this technique for the image classification task was done by training two networks with different parameters. A sample would be taken for the second-stage training if the two networks have agreed on its prediction. Harsher criterion was suggested as well, including a constraint that each of the voters predictions will be higher then a certain probability. Even when using a confidence value of $50\%$, only a scarce set of examples have achieved the criteria. The first network was train with learning rate of $10^{-6}$ and with regularization parameter of 1, while the second with $10^-6$ and 0.1 respectfully. Results can be seen in \textit{Table 1} and \textit{Table 2}. 

 \subsubsection{Dropout consensus}
  In order to minimize even further the false positive rate, an additional constraint was enforced - the soft-max probability of the predicted class should be above a threshold of $95\%$. 
 This method was examined twice: once with a consensus of 80 voters, and once with 25 voters. Results are presented in \textit{table 3}.

 \subsubsection{Dropout credible interval}
 For applying this method, the classifier learned from samples
for which the lower value of the interval was higher then the threshold.
  As a trade-off between the running time and accuracy, 80 MC-Dropout iterations were used. The credible interval was the common $95\%$ one.\\ For improving the results, the threshold of the lower value was initially set to 0.98 and then slowly decreased all the way to 0.9. By that, the network first trained on the example it was most sure of and only later handling  with the less obvious examples. 
  
\subsection{Results}
This section presents the labeled data size, the initial test accuracy after training on merely the labeled data only, the test accuracy after the entire training session (\textit{best acc}), the true positive rate among the positives (\textit{TP} ) and the Positive rate (\textit{P}) which is the rate of the unlabeled data that was used in the second training phase. 


Results for using consensus with two networks:

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}\hline
training size & basic acc  & best acc & TP & P \\[1 ex] \hline\hline
100  & 0.7  &  fail  & 0.47 & 0.92\\
150  & 0.74 &  0.8  & 0.81 & 0.83\\
300  & 0.87  & 0.95 & 0.95 & 0.99\\
1000 & 0.90  & 0.96 & 0.97 & 0.98 \\ \hline
\end{tabular}
\end{center}
\caption{Consensus two networks - with entropy minimization}
\label{table:1}
\end{table}


\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}\hline
training size & basic acc  & best acc & TP & P \\ \hline\hline
100  & 0.7  &  fail  & 0.75 & 0.84\\
150  & 0.81 &  0.86  & 0.93 & 0.88\\
300  & 0.85  & 0.95 & 0.96 & 0.98\\
1000 & 0.92  & 0.97 & 0.95 & 0.98 \\ \hline
\end{tabular}
\end{center}
\caption{Consensus of two networks - with entropy maximization}
\label{table:2}
\end{table}

In \textit{table 1} and \textit{table 2}, the results for self training based on the consensus of two networks with two different regularization are presented. \textit{Entropy minimization} regulator punishes the network when the classifier is not sure enough in it's results, making the predictions to be more polarized. This may help the classifier notice the underling separation of the data to different classes, even when it knows little on each of the classes. From the other side, \textit{Entropy maximization} punished on over-confidence, making the classifier predict with high probability only when it is absolutely sure in the prediction. 
Notice that the results are comparable, regardless of the entropy regularization. This is somewhat expected since this regularization is applied in order to influence the networks own certainty about its predictions. In this approach the networks confidence was not taken into account when deciding whether or not to trust the predictions. It was simply examined if the two networks agreed. Another thing to note is that label set of only 100 samples is not enough for this approach; more specifically, an initial accuracy of  $70\%$  was to low for the technique. With that said, even a slight improvement (to an accuracy of  0.74) lead to a much better \textit{TP} rate and therefore to an improvement in the second stag0e of the training. Finally, a well known phenomena can be observed here - reaching a 0.95-0.97 accuracy was relatively easy and happened even when the initial accuracy was only at 0.85, but an accuracy of 0.99 remained outside of our reach even when starting with an initial 0.92 accuracy. A more accurate decision criteria is needed in order to break this glass ceiling.

 \begin{table}
 \begin{center}
\begin{tabular}{|c|c|c|c|c|}\hline
training size & basic acc  & best acc & TP & P \\ \hline\hline
100  & 0.68  &  fail  & 0.53 & 0.2\\
150  & 0.798 &  fail  & 0.61 & 0.2\\
300  & 0.85  & 0.8825 & 0.86 & 0.2\\
1000 & 0.89  & 0.9 & 0.94 & 0.2 \\ \hline
\end{tabular}
\end{center}
\caption{Consensus among 80 Dropout voters}
\label{table:3}
\end{table}

In table 4, the results when running the predictions with 25 Dropout voters are presented. A lower number of voters increase the likelihood of agreement among the voters.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}\hline
training size & basic acc  & best acc & TP & P \\ \hline\hline
100 & 0.68 & fail & 0.1 & 0.82\\
150 & 0.75 & fail & 0.59 & 0.75 \\
300 & 0.84 & 0.91 & 0.9 & 0.75\\
1000 & 0.91 & 0.93 & 0.95& 0.75 \\ \hline
\end{tabular}
\end{center}
\caption{Consensus among 25 Dropout voters}
\label{table:4}
\end{table}

It is possible to see that the self training approach based on the \textit{Dropout-consensus} is quite sensitive to the initial training stage and could not maintain satisfying results from a very small labeled data set.
For the \textit{Dropout Confidence Interval} method, satisfying results were achieved even with training size=100. Therefore, there was no need for larger labeled sets. Based on this training set, the initial accuracy was  $73\%$; after 100 epochs finished with $95.94\%$ (
\textit{TP}=$99\%$, \textit{P}=$87.6\%$). Applying the deteriorating confidence threshold (together with gradually decrease in the regularization coefficient, from 1 all the way down to $-0.5$) yield slightly better accuracy of $96.58\%$ with (\textit{TP}=$98.7\%$, \textit{P}=$92.8\%$). The higher percentage of true positive can be explained by the lower threshold at the end of the training stage.


 \begin{table}
 \begin{center}
\begin{tabular}{|c|c|c|c|c|}\hline
Method & error rate \\ \hline\hline
improved GAN*  & 0.86 \cite{salimans2016improved} \\
Ladder  & 1.06 \cite{NIPS2015_5947} \\
AtlasRBF & 8.10 \cite{pitelis2014semi} \\
pseudo-label & 10.49 \cite{lee2013pseudo}\\
self training with CI  & 4.06  \\
self training with CI and dynamic confidence & 3.42\\\hline 
\end{tabular}
\end{center}
\caption{Comparative with other semi-supervised classification methods on the MNIST data-set}
\label{table:5}
\end{table}
Table 5 contains the comparison with other semi-supervised techniques. This suggested technique outperform the previous successful implementation of the self-train method on the MNIST set, which used auto-encoders and dropout. Our approach is second only to much more sophisticated methods, and can be used simply for any semi-supervised classification task.\\
When considering the confidence interval, the classifier becomes much more robust. Interestingly, when the decision was based on the interval's lower value, a recovering mechanism took place as the epochs on the data progressed: During the training procedure on the smallest datasets, the accuracy of the network initially got worse than it was after the initial training on the labeled set. Notwithstanding, as the training continued, the accuracy improved, achieving much better results - and passing the best results known so far using the self-training technique. In the experiments that was conducted based on 100 labeled examples, the test accuracy first went down to approximately $15\%$ after a few epochs on the data, only to come back up to its final presented result.
In addition, the amount of samples the classifier learned from was almost monotonously increasing as the iterations on the set progressed. This implies that the network did always converge towards a local minimum.
To explain why superior results where obtained when the confidence interval was considered, the true-positives (\textit{"TP"}) and total positives (\textit{"P"}) rates is examined: Obviously, each sample that is labeled as Positive under the \textit{consensus $\&$ above threshold} criteria, will be labeled as positive under the \textit{Dropout Confidence Interval} criteria. On the other hand, the \textit{Dropout Confidence Interval} criteria classifies much more samples as Positive, and judging by the \textit{TP} percentage, most of them are \textit{TP}. Therefore, compared to the other examined criteria, the \textit{Dropout Confidence Interval} manage to train on more samples (while keeping a low rate of false positives), which increases the generalization ability of the classifier. When using an initial random labeled training set of 80, the initial rate of successful predictions of the classifier was $67\%$. In this case, the classifier failed to recover from the low \textit{TP} rate. This suggests a lower bound on the initial accuracy when using the self-training approach on semi-supervised vision tasks.
\section{Discussion}
The work brought here examines the intuitive and flexible approach of self-training as a semi-supervised approach for computer vision tasks. 
The best self-training algorithm presented here out-preformed the former state-of-the-art self-training technique by 7 $\%$ on the MNIST data-set. In addition, this algorithm is simple to implement on various networks for variable datasets and is likely to work for when done right and when the initial success rate is satisfactory. An interesting usage may be using the self-training algorithm described here for recently-popular pseudo-gradient approach, aiming to train recurrent neural networks (RNN) on long sequences. In this approach, the long sequence is first divided to shorter sequences. Then, the RNN is trained on each of these short sequences without waiting to the gradient from the loss on the successive sequences. Instead, the RNN 'guesses' these gradients, assigns its guess as a pseudo-gradient and continues the training based on this pseudo gradient.

\section{Future work}
Following the conclusions, much work can be done to follow up.
In addition to the basic MNIST classification task, Self-training for semantic segmentation was examined as well.
Unfortunately, the segmentation network did not achieved the minimal success rates after the initial supervised training on a limited labeled set, resulting in failing to show positive results for the semi-supervised semantic segmentation task with limited data set.\\
It is reasonable that the self-trained method would work for semantic segmentation as well, given a sufficient preliminary accuracy. For achieving this, it's advised to first train the model over all of the available labeled dataset, and then continue the self-training stage over another data-set similar to the labeled one. %Examples for this kind of dataset are other segmentation datasets and the CIFAR dataset. 
Focusing on the segmentation task, the confidence threshold should probably not be a fixed number. It should vary for each pixel, based on nearby pixels, and should change based on classes appearances.\\
Interesting work can be done by harnessing the recent advances in low-shot visual recognition \cite{hariharan2016low} to the challenging semi-supervised semantic segmentation task. In addition, harnessing sophisticated techniques for coping with biased prediction, especially in segmentation\cite{bulo2017loss}, can be of great use when trying to learn from a minimal random data-set.\\
Another interesting work that can be done using the self-training method is unsupervised classification, similarly to K-means. The advantage of the self-training method over k-means is its wider approach, which can come into practice by considering higher statistical moments than just the first one - in C-EM, for example, and more generally, by learning the underling representation of the class in an implicit manner, which can eventually be more accurate. Maintaining effective choosing of class seeds, as described in k-means++ \cite{arthur2007k} and its following works, can result in an adequate unsupervised discriminator, especially when using the Entropy minimization regularization \cite{grandvalet2005semi}.
Finally, one last thing to additionally explore is the performance of the network when predictions in the final stage of testing the model, after the semi-supervised training was complete, are made in the same manner as was made during the semi-supervised training. For example, by using the MC-Dropout.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)


\bibliographystyle{plain}
\bibliography{for_arxiv}

% that's all folks
\end{document}