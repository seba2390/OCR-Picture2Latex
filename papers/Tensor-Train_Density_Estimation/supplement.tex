\documentclass[accepted]{uai2021}

%\usepackage[american]{babel}
%\usepackage[russian]{babel}

\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions

\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{easyeqn}
\usepackage{bbm}
\usepackage[ruled,vlined]{algorithm2e}
%\usepackage[sorting=none]{biblatex}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}

\usepackage{filecontents}
\usepackage{xr}
\externaldocument[main-]{ms}

\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
\DeclareMathOperator{\E}{\mathop{\mathbb{E}}}
\DeclareMathOperator{\KL}{\mathop{\mathcal{KL}}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Tr}{tr}
\newcommand{\tr}[1]{\Tr\left\{#1\right\}}
% \newtheorem{theorem}{Theorem}
\newcommand\R{\mathbb{R}}
\renewcommand\O{\mathcal{O}}

\newcommand{\todo}[1]{{\bf \color{red} (TODO) #1}}

\renewcommand\vec{\boldsymbol}
\newcommand\set{\mathcal}
\newcommand{\newtext}[1]{{\color{blue}#1}}
\newcommand{\oldtext}[1]{{\color{red}#1}}

% for comp table
\usepackage{pifont}
\newcommand{\cmark}{\color{green}{\ding{51}}}%
\newcommand{\xmark}{\color{red}{\ding{55}}}%

\title{Tensor-Train Density Estimation}

\author[1]{\href{mailto:Georgii Novikov <georgii.novikov@skoltech.ru>?Subject=Tensor-Train Density Estimation}{Georgii~S.~Novikov}{}} % Lead author
\author[1]{Maxim~Panov}
\author[1]{Ivan~V.~Oseledets}

\affil[1]{%
    CDISE, Skolkovo Institute of Science and Technology \\
    Moscow, Russia
}

\begin{document}
\maketitle

\appendix

\section{Supplementary Material}
\label{sec:suppl}

\subsection{Details of the Experiments}
\label{subsec:setups}
  In all the experiments set of B-splines of degree $2$ with knots uniformly distributed over the distribution support was used as basis functions for rank-$1$ feature maps. In all the experiments, Riemannian optimization with the optimal learning rate was used if not stated otherwise. In Section~\ref{main-subsec:importance} Adam optimizer from PyTorch was used with default parameters. Sampling from TTDE was performed with 30 binary search iterations. In all the experiments, the batch size was $2^{10}$ elements per iteration. For all toy and model examples, we used infinite data generators. In real-world data experiments in Section~\ref{main-subsec:real-world}, the rank $r$ of the TTDE was $64$, and the number of basis functions $m$ was $128$. Implementation of FFJORD was taken from \url{https://github.com/rtqichen/ffjord}, implementations of GLOW, Real NVP and MAF were taken from \url{https://github.com/ikostrikov/pytorch-flows} and used with parameters recommended by authors.
  
  The first three components of the distribution used in Section~\ref{main-subsec:importance} are depicted on Figure~\ref{fig:distrs:asym}. Other $d - 3$ components are standard Gaussian noise.
  \begin{figure}[t]
    \centering
    \includegraphics[width=.9\linewidth]{imgs/distrs-asym}
    \caption{Visualization of the first three dimensions of the model distribution used in this work. It consists of 7 identical Gaussian distributions shown with different colours. Other $d - 3$ dimensions are standard Gaussian noise. }
  \label{fig:distrs:asym}
  \end{figure}
  
\subsection{Existing measures of discrepancy}
\label{sup:losses}
  
  $\mathcal{KL}$-divergence is a popular measure of discrepancy between two distributions, and, during training, is presented in the form of the maximum likelihood problem:
  \begin{EQA}[l]
    \mathcal{KL}(p ~\|~ q_{\vec{\theta}}) = -\E_{\vec{x} \sim p(\vec{x})} \log q_{\vec{\theta}}(\vec{x}) + const.
  \end{EQA}
  %
  Different models were built to optimize this kind of discrepancy (including autoregressive models~\citep{Ryder2018}, normalizing flows~\citep{Kobyzev2020}, energy-based models~\citep{LeCun2006}). The main downside of the maximization of the likelihood is that it explicitly depends on the partition function of the approximation. Thus either the models should be constructed in such a way that the partition function could be efficiently calculated, or expensive Monte-Carlo methods should be used to approximate it during the optimization. We can not use $\mathcal{KL}$-divergence to train TTDE because, although it has a tractable partition function, function in tensor-train format is not guaranteed to be positive.

  Fisher discrepancy loss does not depend on the partition function of the approximation:
  \begin{EQA}[ll]
    & \mathcal{L}(p, q_{\vec{\theta}}) \\
    = & \E_{\vec{x} \sim p(\vec{x})} \norm{\nabla \log p(\vec{x}) - \nabla \log q_{\vec{\theta}}(\vec{x})}^2 \\
    = & \E_{\vec{x} \sim p(\vec{x})} \norm{\nabla \log q_{\vec{\theta}}(\vec{x})}^2 - \E_{\vec{x} \sim p(\vec{x})} \Delta \log q_{\vec{\theta}}(\vec{x}) + const.
  \end{EQA}
  %
  Here $const$ depends only on $p$ and does not depends on $q_{\vec{\theta}}$. Because of the gradient of the logarithm, the normalization constant cancels out. The downside of this loss is that for complex models like neural networks, the Laplace operator is hard to calculate from both computational and numerical stability points. This loss can be used to train TTDE if we parameterize $\log p(x)$ instead of parameterizing $p(x)$ with the tensor-train model. However, in that case, we would lose the ability to calculate partition function and cumulative density function and thus would not be able to exact sample.

  Different versions of adversarial loss were created and successfully used to learn complex distributions like images or speech. They use a separate model as a critic during the training process. Consider the following two variants for vanilla GAN and WGAN, respectively:
  \begin{EQA}
    && \mathcal{L}(p, q_\theta) =\\
     & = & \max_D \left\{ \E_{\vec{x} \sim p(\vec{x})} \left[ \log D(\vec{x}) \right] + \E_{\vec{z} \sim p_{\vec{z}}(\vec{z})} \left[ \log(1 - D(G(\vec{z}))) \right] \right\},
    \\
    && \mathcal{L}(p, q_\theta) =\\
    & = & \max_D \left\{ \E_{\vec{x} \sim p(\vec{x})} \left[ D(\vec{x}) \right] - \E_{\vec{z} \sim p_{\vec{z}}(\vec{z})} \left[ D(G(\vec{z})) \right] \right\}.
  \end{EQA}
  %
  Generator $G$ maps latent variable $\vec{z}$ with known distribution $p_{\vec{z}}$ to the sample space $\vec{x}$, and discriminator $D$ tries to distinguish between real samples and generated samples. In these cases, $q_{\vec{\theta}}$ is defined implicitly. Separate choice of the critic architecture, instability of the optimization of the min-max problem, and the intractability of the implicit density function $q_{\vec{\theta}}$ are the problems that come with the power of adversarial models.

\subsection{Riemannian Optimization}
\label{subsec:riemannian}
  \paragraph{Orthogonalization.} 
  Left- and right-orthogonalization of the tensor-train decomposition is three sets of matrices: set of left-orthogonal cores $\vec{U}_1, \dots, \vec{U}_d$, set of right-orthogonal cores $\vec{V}_1, \dots, \vec{V}_d$ and set of unrestricted cores $\vec{S}_1, \dots, \vec{S}_d$, such that
  \begin{EQA}[l]
  \label{eq:orthogonalization}
    G_1 \times^1_2 \cdots \times^1_{d} G_d = \\ 
    \vec{U}_1 \times^1_2 \cdots \vec{U}_{i-1} \times^1_{i} \vec{S}_i \times^1_{i+1} \vec{V}_{i + 2} \cdots \times^1_{d} \vec{V}_{d}
  \end{EQA}
  for each $i$, where left-orthogonality means
  \begin{EQA}[l]
    \langle U_k[:, :, i], U_k[:, :, j] \rangle = \delta^i_j,
  \end{EQA}
  and right-orthogonality means
  \begin{EQA}[l]
    \langle V_k[i, :, :], V_k[j, :, :] \rangle = \delta^i_j.
  \end{EQA}  
  
  \paragraph{Tangent space.}
  Given the left- and right-orthogonalization of the given tensor-train decomposition of tensor $X$, tangent space $\set{T}_{\vec{X}}(\set{M})$ in that point could be constructed as follows:
  \begin{EQA}[l]
    \set{T}_{\vec{X}}(\set{M}) = \\
    \left\{ 
      \begin{array}{c}
         T = U_1 \times^1_2 \cdots U_{i - 1} \times^1_i S^{\delta}_i \times^1_{i+1} V_{i + 1} \cdots \times^1_d V_d, \\
        \text{where } 1 \leq i \leq d,\; S^{\delta}_i \in \R^{r_{i - 1} \times m \times r_i} 
      \end{array} 
    \right\}.
  \end{EQA}
  %
  Although tensor $T$ is presented as a sum of $d$ tensors of rank $r$, they share common cores. Because of that, $T$ can be represented with rank $2r$:
  \begin{EQA}[l]
    T = \\
      \left[ \begin{array}{cc} U_1 & S^{\delta}_1 \end{array} \right]
      \left[ \begin{array}{cc} V_2 & \\ S^\delta_2 & U_2 \end{array} \right] \cdots 
      \left[ \begin{array}{cc} V_{d-1} & \\ S^\delta_{d-1} & U_{d-1} \end{array} \right] 
      \left[ \begin{array}{c} S^{\delta}_d \\ V_d\end{array} \right]
  \end{EQA}
  
  \paragraph{Automatic differentiation.}
  If we define operator
  \begin{EQA}[l]
    T_{\vec{X}}(S^{\delta}_1, \cdots, S^{\delta}_d) = \sum_{i=1}^d U_1 \times^1_2 \cdots U_{i - 1} \times^1_i S^{\delta}_i \times^1_{i+1} V_{i + 1}
  \end{EQA}
  that maps delta-cores $\left\{ S^\delta_i \right \}_{i=1}^{d}$ into the point in tangent plane, then for any function $g(\vec{X}) : \R^{n_1 \times \cdots \times n_d} \rightarrow \R$ projection of the true gradient $\nabla(\vec{X})$ onto the tangent plane $\set{T}_{\vec{X}}(\set{M})$ could be efficiently calculated as follows:
  \begin{EQA}[l]
    \mathbb{P}_{\set{T}_{\vec{X}}(\set{M})} \nabla g(\vec{X}) = T_{\vec{X}}(\tilde{S}^\delta_1, \cdots, \tilde{S}^\delta_d),
  \end{EQA}
  where 
  \begin{EQA}[l]
    \tilde{S}^\delta_i = \left. \frac{\partial}{\partial S^\delta_i} g(T_{\vec{X}}(S^\delta_1, \cdots, S^\delta_d) \right|_{S^\delta_1 = S_1, S^\delta_2 = \vec{O}_2, \cdots, S^\delta_d = \vec{O}_d}.
  \end{EQA}
  %
  Here $S_1$ is defined in~\eqref{eq:orthogonalization}, and $\vec{O}_i$ is core with all elements equal to zero. All $\tilde{S}^\delta_i$ could be calculated using automatic differentiation of function $g \circ T_{\vec{X}}$.

\bibliography{supplement.bib}
  

\makeatletter\@input{xx.tex}\makeatother


\end{document}
