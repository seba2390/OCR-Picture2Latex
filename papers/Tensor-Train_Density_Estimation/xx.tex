\begin{filecontents}{ms.aux}
\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{plainnat}
\citation{Kingma2013}
\citation{goodfellow2014generative}
\citation{Oord2016}
\citation{Dinh2016}
\citation{Scott1977}
\citation{Wang2019}
\citation{DeCao2019}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Contributions of this work.}{1}{paragraph*.1}\protected@file@percent }
\citation{Dolgov2020}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of the capabilities of different density estimation models. *FFJORD does not use true log-likelihood in the training process and instead uses its unbiased estimate.\relax }}{2}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:comp}{{1}{2}{Comparison of the capabilities of different density estimation models. *FFJORD does not use true log-likelihood in the training process and instead uses its unbiased estimate.\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Why tensor-train is good for density approximation?}{2}{section.2}\protected@file@percent }
\newlabel{sec:theory}{{2}{2}{Why tensor-train is good for density approximation?}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Problem statement}{2}{subsection.2.1}\protected@file@percent }
\newlabel{eq:general-approx}{{1}{2}{Problem statement}{equation.2.1}{}}
\@EQ@newreference{eq:general-approx}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Proposed representation of the density}{2}{subsection.2.2}\protected@file@percent }
\newlabel{sec:basis}{{2.2}{2}{Proposed representation of the density}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Tensor-product basis.}{2}{paragraph*.3}\protected@file@percent }
\citation{Cohen2015,Khrulkov2017,Stoudenmire}
\newlabel{eq:approx-of-p}{{2}{3}{Tensor-product basis}{equation.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Tensor-train format.}{3}{paragraph*.4}\protected@file@percent }
\newlabel{eq:tt-of-alpha}{{3}{3}{Tensor-train format}{equation.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Computation in the TT-format.}{3}{paragraph*.5}\protected@file@percent }
\@EQ@newreference{eq:approx-of-p}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Multiplication of two tensors represented in tensor-train format. On each step, we store the contraction of the two prefixes of the lists of cores. Each such contraction can be updated from the previous step in $\mathcal  {O}\bigl  (\qopname  \relax m{max}(r_1^2, r_2^2) \qopname  \relax m{min}(r_1, r_2)\bigr  )$ time, which gives $\mathcal  {O}\bigl  (d \qopname  \relax m{max}(r_1^2, r_2^2) \qopname  \relax m{min}(r_1, r_2)\bigr  )$ complexity of the full product.\relax }}{3}{algocf.1}\protected@file@percent }
\newlabel{alg:inner_product}{{1}{3}{Computation in the TT-format}{algocf.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Squared TTDE.}{3}{paragraph*.7}\protected@file@percent }
\newlabel{par:sqr-ttde}{{2.2}{3}{Squared TTDE}{paragraph*.7}{}}
\@EQ@newreference{eq:tt-of-alpha}
\@EQ@newreference{eq:tt-of-alpha}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Sampling}{4}{subsection.2.3}\protected@file@percent }
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Algorithm to retrieve an exact sample $\boldsymbol  {x}$ from the density $q_{\boldsymbol  {\theta }}(\boldsymbol  {x})$ represented in TT-format.\relax }}{4}{algocf.2}\protected@file@percent }
\newlabel{alg:sampling}{{2}{4}{Sampling}{algocf.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Learning via Riemannian optimization}{4}{section.3}\protected@file@percent }
\newlabel{sec:optimization}{{3}{4}{Learning via Riemannian optimization}{section.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Loss function.}{4}{paragraph*.9}\protected@file@percent }
\newlabel{sec:theory:losses}{{3}{4}{Loss function}{paragraph*.9}{}}
\citation{Rakhuba2019,Steinlechner2016}
\citation{Rakhuba2019}
\citation{Rakhuba2019}
\newlabel{eq:loss-l2}{{4}{5}{Loss function}{equation.3.4}{}}
\@EQ@newreference{eq:loss-l2}
\@writefile{toc}{\contentsline {paragraph}{Computation of loss function and its derivatives.}{5}{paragraph*.10}\protected@file@percent }
\@EQ@newreference{eq:loss-l2}
\@EQ@newreference{eq:loss-l2}
\@writefile{toc}{\contentsline {paragraph}{Riemannian optimization and optimal step.}{5}{paragraph*.11}\protected@file@percent }
\newlabel{sec:theory:optimization}{{3}{5}{Riemannian optimization and optimal step}{paragraph*.11}{}}
\@EQ@newreference{eq:loss-l2}
\@EQ@newreference{eq:loss-l2}
\@writefile{toc}{\contentsline {paragraph}{Initialization.}{5}{paragraph*.12}\protected@file@percent }
\newlabel{sec:theory:initialization}{{3}{5}{Initialization}{paragraph*.12}{}}
\citation{SCOTT1979}
\citation{Scott1977}
\citation{goodfellow2014generative}
\citation{Kingma2013}
\citation{LeCun2006}
\citation{Ryder2018}
\citation{Kobyzev2020}
\citation{Kingma2018,Kim2020}
\citation{Grathwohl2018}
\citation{DeCao2019}
\citation{choiprobabilistic}
\citation{dennis2016algorithms}
\citation{KargasSF18,KargasS19,amiridi2021lowrank}
\citation{amiridi2021lowrank}
\citation{KargasS19}
\citation{Khrulkov2017}
\newlabel{eq:init-1d-task}{{5}{6}{Initialization}{equation.3.5}{}}
\@EQ@newreference{eq:init-1d-task}
\@EQ@newreference{eq:loss-l2}
\@writefile{toc}{\contentsline {section}{\numberline {4}Related work}{6}{section.4}\protected@file@percent }
\newlabel{sec:related}{{4}{6}{Related work}{section.4}{}}
\@EQ@newreference{eq:tt-of-alpha}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{6}{section.5}\protected@file@percent }
\newlabel{sec:experiments}{{5}{6}{Experiments}{section.5}{}}
\citation{Grathwohl2018}
\citation{Kingma2018}
\citation{Dinh2016}
\citation{Papamakarios2017}
\citation{Grathwohl2018}
\citation{papamakarios2017masked}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of TTDE and FFJORD models on 2-dimensional toy distributions.\relax }}{7}{figure.caption.13}\protected@file@percent }
\newlabel{fig:toy}{{1}{7}{Comparison of TTDE and FFJORD models on 2-dimensional toy distributions.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Toy examples}{7}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Model datasets and hyperparameter selection}{7}{subsection.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Experiment with mixture of 7 Gaussians in 3D with additional dimensions containing only noise. We report the maximum dimensionality for which approximation of the density converges to the true one for different initialization settings and optimization methods used.\relax }}{7}{table.caption.16}\protected@file@percent }
\newlabel{tab:max-dimension}{{2}{7}{Experiment with mixture of 7 Gaussians in 3D with additional dimensions containing only noise. We report the maximum dimensionality for which approximation of the density converges to the true one for different initialization settings and optimization methods used.\relax }{table.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Importance of initialization.}{7}{subsection.5.3}\protected@file@percent }
\newlabel{subsec:importance}{{5.3}{7}{Importance of initialization}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Real-world data}{7}{subsection.5.4}\protected@file@percent }
\newlabel{subsec:real-world}{{5.4}{7}{Real-world data}{subsection.5.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Methods and data.}{7}{paragraph*.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Approximations of ``two moons'' distribution by TTDE for different basis function set sizes and TT-ranks.\relax }}{8}{figure.caption.14}\protected@file@percent }
\newlabel{fig:rank-basis-moons}{{2}{8}{Approximations of ``two moons'' distribution by TTDE for different basis function set sizes and TT-ranks.\relax }{figure.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Average log-likelihood for several neural-network-based models on tabular UCI datasets. Gaussian fitted to the training data is reported as a baseline.   *On Hepmass and Miniboone datasets, which has the lowest number of training examples (300k and 30k respectively), we observe heavy overfitting. Lack of regularizations for the new model leads to poor results. Thus, it is an important direction for further development of the TTDE. \relax }}{8}{table.caption.17}\protected@file@percent }
\newlabel{tab:sqrttde-results}{{3}{8}{Average log-likelihood for several neural-network-based models on tabular UCI datasets. Gaussian fitted to the training data is reported as a baseline. \\ *On Hepmass and Miniboone datasets, which has the lowest number of training examples (300k and 30k respectively), we observe heavy overfitting. Lack of regularizations for the new model leads to poor results. Thus, it is an important direction for further development of the TTDE. \relax }{table.caption.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Quality measure.}{8}{paragraph*.19}\protected@file@percent }
\@EQ@newreference{eq:loss-l2}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Dependence of the approximation quality of the mixture of 128 Gaussians in 8 dimensional space on the rank of the TT decomposition.\relax }}{9}{figure.caption.15}\protected@file@percent }
\newlabel{fig:rank-asymmetric-8d}{{3}{9}{Dependence of the approximation quality of the mixture of 128 Gaussians in 8 dimensional space on the rank of the TT decomposition.\relax }{figure.caption.15}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Calculation of sliced total variation between two distributions based on samples from them.\relax }}{9}{algocf.3}\protected@file@percent }
\newlabel{alg:STV}{{3}{9}{Quality measure}{algocf.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Results.}{9}{paragraph*.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Dependence of the sliced total variation w.r.t. the training time for models trained on $6$-dimensional UCI POWER dataset.\relax }}{9}{figure.caption.22}\protected@file@percent }
\newlabel{fig:ffjord-tt-tv}{{4}{9}{Dependence of the sliced total variation w.r.t. the training time for models trained on $6$-dimensional UCI POWER dataset.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Dependence of the sampling time w.r.t. the number of samples to be generated for $6$-dimensional space for models trained on UCI POWER dataset. Our model outperforms its competitors and shows $2.6$, $2.5$, $1.4$ and $1.2$ times speedups compared to FFJORD, MAF, GLOW and Real NVP respectively.\relax }}{9}{figure.caption.23}\protected@file@percent }
\newlabel{fig:ffjord-tt-sampling}{{5}{9}{Dependence of the sampling time w.r.t. the number of samples to be generated for $6$-dimensional space for models trained on UCI POWER dataset. Our model outperforms its competitors and shows $2.6$, $2.5$, $1.4$ and $1.2$ times speedups compared to FFJORD, MAF, GLOW and Real NVP respectively.\relax }{figure.caption.23}{}}
\bibdata{ms.bib}
\bibcite{amiridi2021lowrank}{{1}{2021}{{Amiridi et~al.}}{{Amiridi, Kargas, and Sidiropoulos}}}
\bibcite{DeCao2019}{{2}{2019}{{Cao et~al.}}{{Cao, Aziz, and Titov}}}
\bibcite{choiprobabilistic}{{3}{}{{Choi et~al.}}{{Choi, Vergari, and Van~den Broeck}}}
\bibcite{Cohen2015}{{4}{2016}{{Cohen et~al.}}{{Cohen, Sharir, and Shashua}}}
\bibcite{dennis2016algorithms}{{5}{2016}{{Dennis}}{{}}}
\bibcite{Dinh2016}{{6}{2017}{{Dinh et~al.}}{{Dinh, Sohl{-}Dickstein, and Bengio}}}
\bibcite{Dolgov2020}{{7}{2020}{{Dolgov et~al.}}{{Dolgov, Anaya{-}Izquierdo, Fox, and Scheichl}}}
\bibcite{goodfellow2014generative}{{8}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{Grathwohl2018}{{9}{2019}{{Grathwohl et~al.}}{{Grathwohl, Chen, Bettencourt, Sutskever, and Duvenaud}}}
\bibcite{KargasS19}{{10}{2019}{{Kargas and Sidiropoulos}}{{}}}
\bibcite{KargasSF18}{{11}{2018}{{Kargas et~al.}}{{Kargas, Sidiropoulos, and Fu}}}
\bibcite{Khrulkov2017}{{12}{2018}{{Khrulkov et~al.}}{{Khrulkov, Novikov, and Oseledets}}}
\bibcite{Kim2020}{{13}{2020}{{Kim et~al.}}{{Kim, Kim, Kong, and Yoon}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{10}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{10}{Conclusion}{section.6}{}}
\bibcite{Kingma2018}{{14}{2018}{{Kingma and Dhariwal}}{{}}}
\bibcite{Kingma2013}{{15}{2014}{{Kingma and Welling}}{{}}}
\bibcite{Kobyzev2020}{{16}{2020}{{Kobyzev et~al.}}{{Kobyzev, Prince, and Brubaker}}}
\bibcite{LeCun2006}{{17}{2006}{{LeCun et~al.}}{{LeCun, Chopra, Hadsell, Ranzato, and Huang}}}
\bibcite{Papamakarios2017}{{18}{2017{a}}{{Papamakarios et~al.}}{{Papamakarios, Murray, and Pavlakou}}}
\bibcite{papamakarios2017masked}{{19}{2017{b}}{{Papamakarios et~al.}}{{Papamakarios, Murray, and Pavlakou}}}
\bibcite{Rakhuba2019}{{20}{2019}{{Rakhuba et~al.}}{{Rakhuba, Novikov, and Oseledets}}}
\bibcite{Ryder2018}{{21}{2018}{{Ryder et~al.}}{{Ryder, Golightly, McGough, and Prangle}}}
\bibcite{SCOTT1979}{{22}{1979}{{Scott}}{{}}}
\bibcite{Scott1977}{{23}{1977}{{Scott et~al.}}{{Scott, Tapia, and Thompson}}}
\bibcite{Steinlechner2016}{{24}{2016}{{Steinlechner}}{{}}}
\bibcite{Stoudenmire}{{25}{2016}{{Stoudenmire and Schwab}}{{}}}
\bibcite{Oord2016}{{26}{2016}{{van~den Oord et~al.}}{{van~den Oord, Kalchbrenner, and Kavukcuoglu}}}
\bibcite{Wang2019}{{27}{2019}{{Wang and Scott}}{{}}}
\newlabel{LastPage}{{}{12}{}{page.12}{}}
\xdef\lastpage@lastpage{12}
\xdef\lastpage@lastpageHy{12}
\gdef \@abspage@last{12}
\end{filecontents}
