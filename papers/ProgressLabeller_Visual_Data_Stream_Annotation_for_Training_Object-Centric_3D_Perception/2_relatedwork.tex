With ProgressLabeller, a user can scalably label new datasets with camera world pose, scene object poses and scene object segmentations. This process is enabled by fusing streaming RGB (or RGB-D) inputs into a single scene-wide representation, and then allowing a human user to input relevant 6-DoF information via 3D modelling interfaces (such as those provided by Blender \cite{blender}). This process demonstrates label stability even over long input video streams, and due to its functionality with direct RGB inputs, can label even difficult objects such as transparent cups.  We discuss below methods related to ProgressLabeller.

\subsection{Direct \& Human-in-the-loop labelling}
The creation of 2D segmentation data is analogous to the object detection, keypoint detection, or semantic segmentation tasks (depending on desired output labels). Tools such as LabelMe \cite{russell2008labelme} required users to directly interact with the underlying data to be labelled. This manual process was improved by model-assisted approaches such as Deep Extreme Cut \cite{maninis2018deep} which decreases the amount of user effort necessary to label images.
Shared autonomy and mixed-initiative methods have also been used in this approach, in which the user provides coarse pose or other estimations which are fine-tuned via a model-informed approach \cite{ye2021human}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
\subsection{End-to-End Labellers}
Previous tools have been created to enable this style of learning process. LabelFusion \cite{marion2018label} is perhaps the most commonly utilized example. LabelFusion utilizes streaming RGB-D inputs to create a dense reconstruction of the scene, which is then labelled semi-manually by aligning 3D object models to the 3D reconstruction. While this approach is typically robust, it relies on RGB-D input for reconstruction, and experiences difficulties under certain regimes. In particular, transparent objects cause problems for commonly employed depth sensor technologies, and long-running input streams typically result in large amounts of 'drift'. 

Some methods have been introduced to eliminate the need for CAD models in the labelling process. Singh et al. \cite{singh2021rapid} proposed a method which utilizes user labelled keypoints and bounding boxes to generate pose and segmentation labels. This frees the system from dependency on CAD models, but requires user interaction directly with the images. SALT \cite{stumpf2021salt} proposed utilizing GrabCut to generate 3D bounding boxes and image segmentation labels for relevant scenes. This allows removing the dependency on object masks while also allows the labelling of dynamic scenes such as human gait videos. 
% This method requires point cloud inputs however, which precludes the labelling of RGB only image streams.

Other works sought to improve the labelling procedure itself. EasyLabel \cite{suchi2019easylabel} allows for semi-automatic labelling of scenes via sequentially added objects.  This process is scalable, and generates high quality labels. However, it requires tight physical control over the scene to be labelled, which is not always feasible to obtain. Objectron \cite{ahmadyan2021objectron} utilized modern smartphone's AR capabilities combined with human-labelled 3D bounding boxes to scalably create a large scale dataset. This method however is susceptible to label drift during long-duration input videos. KeyPose \cite{liu2020keypose} specifically sought to generate labelled datasets for transparent objects. This method utilized stereoscopic images taken from a robot armature in order to avoid the problems of typical depth cameras have with transparent objects.
% StereOBJ-1M \cite{liu2021stereobj} improves the data collection efficiency in a setting with two more static cameras and more fiducial markers in the scene.