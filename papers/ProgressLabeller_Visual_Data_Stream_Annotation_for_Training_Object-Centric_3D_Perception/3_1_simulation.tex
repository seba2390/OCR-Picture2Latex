\subsection{Annotation accuracy estimation from simulation}

We verify the accuracy of annotations throughout this multi-view silhouette matching process by simulating an iterative object pose update process. In each iteration, given a certain camera frame, 
we assume the object will be translated in a plane parallel with its x-y plane, or rotated about z-axis (for better control), towards a pose that maximizes the Intersection-over-Union (IoU) between the rendered silhouette at current pose and ground truth.
% we assume the object will be translated or rotated to maximize the IoU between the object silhouette rendered at current pose and ground truth from that camera view.

\subsubsection{Problem Definition} Given set of $N$ images $I^{\{i\}}$ with their corresponding camera pose $T^{\{i\}}$ in the world frame, $i \in \{1, 2, 3, \ldots, N\}$. Our goal is to find ground truth object pose $T^{\text{obj}\{j\}}_{gt}$ in the world frame for all the objects $j \in \{1, 2, 3, \ldots, M\}$ in the scene. We define the projection operator as $S^{\{i, j\}} = \text{Proj}(T^{\{i\}}, T^{\text{obj}\{j\}})$, which render object $j$ given its CAD model, camera pose $T^{\{i\}}$ and object pose $T^{\text{obj}\{j\}}$ into an object texture/silouette $S^{\{i, j\}}$. Also defined the IoU operator as $\text{IoU}_{\text{obj}\{j\}}(I^{\{i\}}, \text{Proj}(T^{\{i\}}, T^{\text{obj}\{j\}}))$ to calculate the IoU for pixels in object $j$ between real image $I^{\{i\}}$ and synthetic texture/silouette $S^{\{i, j\}}$. 

% \begin{equation}
%     T^{\text{obj}\{j\}}_{gt} = \argmax_{T^{\text{obj}\{j\}}}\text{IoU}_{\text{obj}\{j\}}(I^{\{i\}}, \text{Proj}(T^{\{i\}}, T^{\text{obj}\{j\}}))
%     \label{eq:fact}
% \end{equation}

The multi-view texture/silhouette matching iterative update is proceeded with a goal to maximize the IoU. Given the pose for object $j$ in the $k$th iteration $T^{\text{obj}\{j\}}_{(k)}$, in ($k + 1$)th iteration:

\begin{equation}
    T^{\text{obj}\{j\}}_{(k+1)} = \argmax_{f[T^{\text{obj}\{j\}}_{(k)}]}\text{IoU}_{\text{obj}\{j\}}(I^{\{i\}}, \text{Proj}(T^{\{i\}}, f[T^{\text{obj}\{j\}}_{(k)}] )) 
    \label{eq:iterate}
\end{equation}
where $f[T^{\text{obj}\{j\}}_{(k)}]$ describes all possible translation start from the  $T^{\text{obj}\{j\}}_{(k)}$ 
% To limit the annotation freedom for a better human control, when viewing perpendicular to one view, only the translation 
that is within the plane $p$ or the rotation along the axis $\omega$ as shown in Figure \ref{fig:annotation_limitation}. So:

\begin{equation}
    f[T^{\text{obj}\{j\}}_{(k)}] = \exp^{\widehat{\xi}_1 \theta_1}  \exp^{\widehat{\xi}_2 \theta_2} T^{\text{obj}\{j\}}_{(k)}
    \label{eq:range}
\end{equation}
where ${\xi}_1 = \begin{bmatrix}-\omega \times v_o \\ \omega \end{bmatrix}$, ${\xi}_2 = \begin{bmatrix} v \\ 0 \end{bmatrix}$ are the twist coordinates for twist $\widehat{\xi}_1, \widehat{\xi}_2$.

% After plug Equation \eqref{eq:range} to Equation \eqref{eq:iterate}, we could get:

% \begin{align}
%     &T^{\text{obj}\{j\}}_{(k+1)} = \argmax_{\theta_1, \theta_2, v}\text{IoU}_{\text{obj}\{j\}}(I^{\{i\}}, \text{Proj}(T^{\{i\}}, T(\theta_1, \theta_2, v) )) \\
%     &T(\theta_1, \theta_2, v) = \exp^{\widehat{\xi}_1 \theta_1}  \exp^{\widehat{\xi}_2 \theta_2} T^{\text{obj}\{j\}}_{(k)}
%     \label{eq:iterate}
% \end{align}

% A more intuitively understanding is that during each iteration, we rotate the object along the axis $\omega$ and translate it within the plane $p$ to maximize the IoU between the projection silhouette $S^{\{i, j\}}$ and real image $I^{\{i\}}$. Simulation experiments in the coming section will demonstrate the convergence of this problem.


 \begin{figure}[htbp]
     \centering
     \includegraphics[width=0.8\columnwidth]{figure/annotation_limitation_resize.pdf}
     \caption{Diagram for an object shown under a camera. $c$ denote the location of camera and $c_x, c_y, c_z$ are its x, y, z axis. $p$ is a plane parallel to camera plane and passing through object's center $v_o$. $\omega$ the rotation direction parallel to $c_x$ and passing through $v_o$. $\theta_1$ is the magnitude of rotation radius. $v$ is the translation direction within the plane $p$ and $\theta_2$ is the translation magnitude. On the right hand side is the projection image, the object in the transparent color is the object with ground truth pose.}
     \label{fig:annotation_limitation}
 \end{figure}

\subsubsection{Simulation results}

We generate a CAD model sets with 44 different CAD models. For each run, we generate $T^{\text{obj}}_{gt}$ with a random rotation matrix and location at the origin. 40 cameras are created with their z axis pointing towards the origin and a random location at a sphere around the object. The initial pose $T^{\text{obj}}_{0}$ is generated by adding a random position noise from Gaussian distribution with variance of 10cm to origin and with a random 3D orientation. During each iteration, $v, \theta_1, \theta_2$ in Equation \ref{eq:range} are discretized for simulation. The result shows that it takes around 10.36 iterations for the algorithm to converge within 1mm location error and dot product larger than 0.99 between ground truth and converged rotation axes.