\begin{figure*}[htbp]
     \centering
     \includegraphics[width=\textwidth]{figure/vs_labelfusion_gt.png}
     \caption{From left to right, we show the 3D labeller view of LabelFusion, ProgressLabeller and cropped RGB image with masks from LabelFusion, ground truth and ProgressLabeller. The top and bottom row show a sample from HOPE and YCB dataset respectively. We find even ground truth pose labels have easily observable errors, shown as the spacing between black masks and object boundaries.}
     \label{fig:compare_label}
 \end{figure*}

In this section, we report results on several evaluation experiments on the data generated using ProgressLabeller. The output label quality is firstly evaluated on public datasets against ground truth. Then we introduce a large scale dataset with object pose annotations, and report the evaluation results of fine-tuning a state-of-the-art deep pose estimation on accuracy. The improved accuracy is further evaluated in robotic grasping experiment. Finally, we show two potential applications: labelling a complex scene with transparent objects in front of reflective backgrounds, and training a neural rendering model on a single object from collected data and do image synthesis.

%%%%%%%%%%%%%%%%%%%%
 \begin{table*}[htbp]
\small
   \centering
%   \begin{tabular}{@{}p{2cm}p{2.2cm}p{1cm}p{1cm}p{1.2cm}p{1.8cm}p{1.8cm}p{1.4cm}p{1cm}@{}}
   \begin{tabular}{@{}p{2cm}p{2.2cm}p{1cm}p{1cm}p{1.2cm}p{1.8cm}p{1.8cm}p{1.4cm}@{}}
     \toprule
     Dataset & Method & Mask IoU $\uparrow$ & Pos (cm) $\downarrow$ & Rot (quat) $\downarrow$ & ADD \hphantom{1cm} (AUC-0.1d) $\uparrow$ & ADD-S (AUC-0.1d) $\uparrow$ & Feature (Pixel) $\downarrow$\\% & Time (min) \\
     \midrule
     HOPE & LabelFusion$^*$    & 0.6729  & 1.5825 & 0.2561 & 0.7095 & 0.7101 & 4.1248\\% & 31 \\
      & ProgressLabeller     & 0.8600  & 1.0948 & 0.0801 & 0.9566 & 0.9566 & 1.9479\\% & 48 \\
      & Ground Truth        & - & - & - & - & - & 2.2752\\% & - \\
     \midrule
     YCB-Video & LabelFusion  & 0.9089 & 0.5165 & 0.0741 & 0.9977 & 0.9989 & 4.7365\\% & 29 \\
      & ProgressLabeller      & 0.8849 & 1.7537 & 0.0636 & 0.7642 & 0.7662 & 3.08\\% & 16 \\
      & Ground Truth         & - & - & - & - & - & 3.3868\\% & - \\
%     \midrule
%     IC-MI & LabelFusion     &  & 0 & 0 & 0 & 0 & 0 & 0 \\
%      & ProgressLabeller      &  & 0 & 0 & 0 & 0 & 0 & 0 \\
%      & Ground Truth         & - & - & - & - & - & 0 & - \\
     \midrule
     NOCS$^*$ & LabelFusion      & 0.7684 & 2.2825 & 2.9092 & 0.6436 & 0.6438 & -\\% & 12 \\
      & ProgressLabeller      & 0.8785 & 1.4949 & 2.6153 & 0.8674 & 0.8679 & -\\% & 15 \\
%      & Ground Truth         & - & - & - & - & - & 0 & - \\
     \midrule
     T-LESS$^*$ & LabelFusion*    & - & - & - & - & - & -\\% & - \\
      & ProgressLabeller      & 0.8810 & 0.6779 & 0.0856 & 0.6626 & 0.6642 & -\\% & 8 \\
%      & Ground Truth         & - & - & - & - & - & 0 & - \\
     \bottomrule
   \end{tabular}
   \caption{Results on feature matching distance between rendered object RGB image and original image. 'Pos (cm)', 'Rot (quat)' refers to positional error in centimeters and rotational error calculated as norm of difference in quaternions as in \cite{huynh2009metrics}. LabelFusion cannot work on T-LESS data streams at all, and it cannot reconstruct one of HOPE samples scene entirely, so we trim the sequence to 1/4 length. Object models in T-LESS and NOCS datasets do not have enough features to measure the feasure-based pixel distances.}
   \label{tab:example}
 \end{table*}

\subsection{Evaluation on label generation}
We evaluate the label quality with respect to pose accuracy against ground truth and time cost on ProgressLabeller versus LabelFusion \cite{marion2018label}. Specifically, we use both tools to label on 64 object instances from 8 sample sequences among the 4 public pose estimation datasets including YCB-Video \cite{xiang2017posecnn}, T-LESS \cite{hodan2017tless}, NOCS \cite{li2020category}, and HOPE \cite{hope_github}.% and IC-MI \cite{tejani2014latent}.

\noindent
\textbf{Evaluation metrics.} We use mask Intersection-over-Union (IoU) between rendered mask of object at labelled pose and at ground truth to evaluate the segmentation, and select 4 distance metrics to compare a labelled pose with ground truth pose, including 3D positional error $E_p$, 3D rotational error $E_r$, average pairwise distance (ADD) $E_{ADD}$ \cite{hinterstoisser2012model} and average pairwise distance in symmetric case (ADD-S) $E_{ADDS}$ \cite{xiang2017posecnn}, defined as follows:

\begin{align}
    &E_p = \norm{t-t^*}, E_r = \min \{\norm{q - q^*}, \norm{q + q^*} \}\\
    &E_{ADD} = \dfrac{1}{|\mathit{M}|} \sum_{x \in \mathit{M}} \norm{(Rx + t) - (R^* x + t^*)}\\
    &E_{ADDS} = \dfrac{1}{|\mathit{M}|} \sum_{x_1 \in \mathit{M}} \min_{x_2 \in \mathit{M}}\norm{(Rx_1 + t) - (R^* x_2 + t^*)}
\end{align}
where $\mathit{M} = \{x_i \in \mathbb{R}^3\}$ is the object 3D point set, and $R, q, t, R^*, q^*, t^*$ refers to the 3D rotation matrix, rotation represented in quaternion, 3D position vector of labelled pose and ground truth respectively. Specifically for ADD and ADD-S, we report the value of area under the accuracy-threshold curve obtained by varying the distance threshold from 0 to 0.1 diameter of objects (AUC-0.1d) following metrics among pose estimation papers.


Besides, from the observation that even ground truth labels are still not perfectly correct (re-projected object masks align with true area in RGB images from multi-view), we propose an RGB feature based evaluation on object pose without ground truth. 
Specifically, we render the object at labelled pose to get an image with only an object in front of black background, and extract SIFT features from both the rendered image and original RGB image, then calculate the average of feature matching distance in pixels. 
In practice, we assume the labelled pose is close to ground truth, and remove incorrect matching with a distance threshold of 10 pixels. Figure \ref{fig:compare_label} shows qualitative examples of labelled poses, where we observe more accurate re-projection results from ProgressLabeller than LabelFusion output as well as provided ground truth on masked RGB images. We believe the main error cause in LabelFusion is the accumulated depth sensing error during reconstruction. Another drawback of LabelFusion UI design is that, there is no measurement or display of object poses in the 3D scene, so the users need to switch back-and-forth between the 3D scene and the final generated images that has image masks rendered at labelled poses. Also its 3D model as well as rendering result doesn't display textures, so it's hard to deal with symmetric-shape objects in HOPE and NOCS. 

The quantitative results are shown in Table~\ref{tab:example}. From the comparison, we see  ProgressLabeller is more accurate and robust in most streams. LabelFusion has higher accuracy on YCB-Video dataset, which has a large feature-based pixel distance. For example, we found the pose annotation of object 006\_mustard\_bottle was flipped 180$^\circ$ in one scene. Besides, the two approaches took similar annotation time of 10-30 minutes per scene.


% \subsection{Ablation Study on Camera Trajectory Estimation}
\subsection{Multi-camera dataset creation}
From analysis of absolute accuracy result above and to evaluate robustness on existing deep pose estimation networks, we aim to create a more accurate, multi-camera, cluttered dataset on YCB objects \cite{calli2015ycb}. We mounted 3 RGB-D cameras with different sensing technologies, ASUS Xtion Pro Primesense Carmine 1.09 (structured light), Intel RealSense D435i (stereo) and RealSense L515 (LiDAR), on a Fetch robot and collected data streams at $\sim$15Hz when Fetch is slowly driving around the tabletop object scene with a constant speed. Each scene contains almost a bit more than full round, with 1K$\sim$3K paired RGB-D images from each camera. In this way, the collected data is free of motion blur and covers full round view of objects and occlusions. The dataset includes 16 scenes with 10 objects placed in both isolation and dense clutter, and another 4 scenes each with a single object for unit test. Figure \ref{fig:dataset} gives an example. It took 2 days collected and labelled the dataset, with about 120K images and 1.2M labelled object instances.
% More details of the dataset are included in the supplementary material.

% already shown in HOPE comparison
 \begin{figure}[htbp]
     \centering
     \includegraphics[width=\columnwidth]{figure/own_ycb.png}
     \caption{The top and bottom row shows RGB images in training and testing set respectively. From left to right, the images are taken from Primesense, RealSense D435i and L515.}
     \label{fig:dataset}
 \end{figure}


%%%%%%%%%%%%%%%%%
 \begin{table*}
   \centering
\small
   \begin{tabular}{@{}lcccccc@{}}
%  
     \toprule
     Train set/Test set & \multicolumn{2}{c}{Primesense} & \multicolumn{2}{c}{RealSense L515} & \multicolumn{2}{c}{RealSense D435i} \\
     \midrule
     Metric: AUC (0-10cm)& ADD & ADD-S & ADD & ADD-S & ADD & ADD-S \\
     \midrule
     Pretrained on Asus Xtion Pro Live & 21.63 & 41.16 & 47.43& 72.90 & 44.67&	68.00\\
     Fine-tuned on Primesense &  \textbf{60.02} & 	\textbf{79.21}& 55.32 & 77.85 & 49.75 & 73.35\\
     Fine-tuned on RealSense L515 & 33.45 &	58.16 & \textbf{68.84} &\textbf{83.69}  & 57.67 & 76.26\\
     Fine-tuned on RealSense D435i & 26.39 & 50.38 & 63.82& 82.52 & \textbf{64.64}	&\textbf{82.36}\\
     \bottomrule
   \end{tabular}
   \caption{The pose estimation accuracy of FFB6D cross-validated on datasets collected using three cameras.}
   \label{tab:add}
 \end{table*}

\subsection{6D object pose estimation fine-tuning}
Using the collected dataset, we fine-tuned one of the state-of-the-art deep pose estimation neural networks taking RGB-Depth images as input, FFB6D \cite{he2021ffb6d} over its pretrained model on YCB-Video dataset. In particular, we created 3 training sets by downsampling to 1/10 the training set collected by 3 cameras respectively and got 3 fine-tuned models by training on a RTX 3080 with batch size 6 for 20 epochs and default settings in other parameters. Then, we cross-validated the 3 fine-tuned models, along with pretrained model, on 3 test sets taken from 3 cameras.

\subsubsection{Evaluation on pose accuracy across different cameras}
We report the AUC for ADD and ADD-S metrics between 0 and 10cm, to match the original result in \cite{he2021ffb6d}. The result is shown in Table \ref{tab:add}, where we find the fine-tuned dataset recognizes the sensor modality and noise difference between 3 cameras as the test set ADD and ADD-S are mostly the highest on the same training set. 
% We include separate values of all 10 objects in the supplementary materials.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.95\columnwidth]{figure/bp_vs_gt.png}
    \caption{A synthesized RGB image using Blenderproc is shown on the left. The original image is shown on the right. The YCB objects are rendered and overlaid to original image according to labelled poses. Depth synthesized images are generated in the same way.}
    \label{fig:bp}
\end{figure}

\subsubsection{Evaluation against LabelFusion and synthetic data on pose accuracy and robotic grasping} We used 3 datasets to train FFB6D with the same setting as above. The first is the collected Primesense subset labelled by ProgressLabeller, the second is the same data labelled by LabelFusion, and the third is synthesized images by rendering YCB objects over both raw RGB and depth images according to the labelled poses using Blenderproc~\cite{denninger2019blenderproc}, which mostly maintain the same object pose distribution and sensor noise as real data, as shown in Figure~\ref{fig:bp}. 

\begin{table*}
  \centering
\small
  \begin{tabular}{@{}lccccccccc@{}}
%   \small
    \toprule
    \multicolumn{2}{c}{Network model}  & \multicolumn{2}{c}{Pretrained} & \multicolumn{2}{c}{Blenderproc}& \multicolumn{2}{c}{LabelFusion} & \multicolumn{2}{c}{ProgressLabeller} \\
    \midrule
    Object name & Grasp tolerance (cm) & ADD & Grasp   & ADD & Grasp  & ADD & Grasp   & ADD & Grasp \\
    \midrule
    002\_master\_chef\_can      & 0.36 & 25.16 & 0/5 & 48.67 & 3/5  & 44.78 & 4/5 & 55.42 & 5/5\\
    \midrule
    003\_cracker\_box           & 3.90 & 19.57 & 1/5 & 50.76 & 1/5  & 60.8 & 4/5 & 71.01 & 5/5\\
    % \midrule
    % 004\_sugar\_box             & 6.72 & 25.57 & 1/5 & 47.59 & 1/5  & 61.45 & 0/5 & 67.47 & 3/5\\
    \midrule
    % 004\_sugar\_box &  & 10.01 &  & 77.13 & \\
    005\_tomato\_soup\_can      & 3.65 & 27.74 & 1/5 & 51.12 & 2/5  & 65.48 & 4/5 & 66.79 & 5/5\\
    \midrule
    006\_mustard\_bottle        & 6.33 & 31.02 & 1/5 & 58.87 & 5/5  & 70.77 & 5/5 & 73.47 & 5/5\\
                                & 2.50 & 31.02 & 0/5 & 58.87 & 5/5  & 70.77 & 4/5 & 73.47 & 5/5\\
    \midrule
    007\_tuna\_fish\_can        & 7.09 & 15.14 & 2/5 & 26.41 & 3/5  & 30.02 & 5/5 & 39.87 & 5/5\\
                                & 2.04 & 15.14 & 2/5 & 26.41 & 4/5  & 30.02 & 4/5 & 39.87 & 5/5\\
    \midrule
    % 007\_tuna\_fish\_can &  & 10.65 &  & 54.70 & \\
    009\_gelatin\_box           & 7.60 & 13.25 & 2/5 & 32.79 & 4/5  & 51.45 & 5/5 & 55.24 & 5/5\\
                                & 3.14 & 13.25 & 2/5 & 32.79 & 3/5  & 51.45 & 5/5 & 55.24 & 5/5\\
                                & 1.62 & 13.25 & 2/5 & 32.79 & 4/5  & 51.45 & 5/5 & 55.24 & 4/5\\
    \midrule
    010\_potted\_meat\_can      & 4.75 & 19.68 & 5/5 & 39.56 & 3/5  & 47.48 & 4/5 & 50.64 & 5/5\\
                                & 2.08 & 19.68 & 1/5 & 39.56 & 4/5  & 47.48 & 4/5 & 50.64 & 4/5\\
                                & 0.78 & 19.68 & 2/5 & 39.56 & 0/5  & 47.48 & 5/5 & 50.64 & 5/5\\
    \midrule
    025\_mug                    & 1.24 & 19.10 & 1/5 & 60.70 & 3/5  & 49.31 & 1/5 & 66.53 & 4/5\\
                                % & 2.3 & 49.96 &  & 64.58 & \\ note: missing some experiment
                                
    \midrule
    040\_large\_marker          & 8.67 & 20.10 & 0/5 & 44.75 & 2/5  & 55.89 & 3/5 & 53.73 & 3/5\\
    \midrule
    overall                     & -    & -     & 22/75 & -   & 46/75 & - & 62/75 & - & 70/75\\
    \bottomrule
  \end{tabular}
  \caption{Grasp record comparison on part of YCB objects using grasp poses generated based on pose estimates from pretrained and fine-tuned FFB6D on the collected dataset, with Blenderproc data synthesis, LabelFusion annotations and ProgressLabeller annotations.}
  \label{tab:grasp}
\end{table*}

We tested pose estimation accuracy from models trained on the above 3 datasets, with respect to ADD-AUC and pose-based grasping success rate. For pose-based grasping, we manually defined grasp poses along the symmetrical axes on the object 3D models, and grouped them by the grasping tolerance, defined by the distance between two gripper fingers (10.39 cm for Fetch) minus the object's diameter along the grasping direction. Obviously, a smaller grasping tolerance requires more accurate pose estimates for a successful grasp. For example, 002\_master\_chef\_can is challenging to grasp as its tolerance is only 0.36 cm.
We repeated grasping on every tolerance for 5 times based on the pose estimates, and the success/failure statistics is shown in Table \ref{tab:grasp}. Overall, the fine-tuning over ProgressLabeller data improves the grasping success rate the most. Among experiments, we found in some cases the grasp is still successful given large rotational error, where the object was aligned to another pose during grasping, such as 002\_master\_chef\_can and 006\_mustard\_bottle when grasping from the side. Also, sometimes the objects were grasped along another direction, such as 009\_gelatin\_box and 010\_potted\_meat\_can, in this case, their actual grasp tolerance might change. We expect evaluation on object placement accuracy to reveal these errors. Otherwise, the grasping test results generally matches the pose accuracy presented in ADD. Figure~\ref{fig:grasping} shows an example of grasping experiment.

 \begin{figure}[htbp]
     \centering
     \includegraphics[width=\columnwidth]{figure/fetch_table_cleaning.png}
     \caption{The Fetch robot is grasping objects on tabletop based on estimated poses. The pose estimates are shown as projected point clouds and coordinate frames in top-right smaller images. The higher and lower frames correspond to pre-grasp and grasp poses.}
    %  The Fetch robot successfully grasps the 002\_master\_chef\_can from two directions using pose estimation based grasping. The top two images show the results by projecting the object cloud at estimated poses over RGB images, and the lower two coordinate frames represented as red-green-blue lines illustrates the calculated grasp poses and pre-grasp poses.}
     \label{fig:grasping}
 \end{figure}

\subsection{Other applications}


\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{figure/nsvf1.png}
    \caption{Comparison of best (left) and worst (right) validation pose renderings from the NSVF model. The smaller images with black background on top corners are the ground truth masks, and those on bottom corners are rendered output from NSVF, and the middle larger images are the renderings imposed onto the original captured image in greyscale. The best image (left) had an average pixelwise L2 error of $0.0067$, and the worst (right) had an average pixelwise L2 error of $0.1867$.}
    \label{fig:nsvf_fig}
\end{figure*}

\label{sec:other}
\subsubsection{Transparent Dataset Labelling}
We collected RGB-D videos with transparent cups and labelled their poses using ProgressLabeller. Figure \ref{fig:demo_trans} shows in detail that even there is no 3D points around the transparent area when we fused raw depth according to the estimated camera poses, the tool enables accurate pose labelling by matching the object's mask with RGB images. We believe a large-scale 3D dataset of transparent objects can be efficiently created using ProgressLabeller.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\columnwidth]{figure/trans.png}
    \caption{The labelling on transparent champagne cups. When there is no reliable depth (top-left), ProgressLabeller enables checking the pose validity by re-projection object models (bottom-left) to RGB images from different views (top-right and bottom-right).}
    \label{fig:demo_trans}
\end{figure}


\subsubsection{Training neural radiance fields on objects}
As a brief additional experiment, we provide prima facie data to show the effectiveness of our labeller in generating datasets for training a Neural Sparse Voxel Fields (NSVF) model \cite{liu2020neural}. This model permits rendering objects at novel poses given multiple input views with corresponding camera poses. We use ProgressLabeller to label a scene containing a 003\_cracker\_box, with 3509 images in total. From this dataset, we uniformly random sample 100 images with object poses for train, test and validation set respectively. We remove the background from the training images using the segmentation from labeller, and an NSVF model was trained to 75k total iterations on a dual GPU machine consisting of an Nvidia RTX 3060 TI and RTX 3070. After training, renderings were made at each camera pose given in the train, test, and validation sets. These renderings were evaluated via a pixel averaged L2 error against the ground truth segmented images produced by ProgressLabeller where each pixel channel was normalized to the range $[0,1]$. Those results presented in Table \ref{tab:nsvf_metrics}. Visual comparisons between the best and worst validation set renderings along with their associated errors are presented in Figure \ref{fig:nsvf_fig}.

\begin{table}[htbp]
  \centering
  \small
  \begin{tabular}{@{}lccc@{}}
    \toprule
    Dataset & Train & Validation & Test\\
    \midrule
    Per-pixel L2 error & 0.0108 & 0.0460  & 0.0409\\
    \bottomrule
  \end{tabular}
  \caption{Average pixelwise errors for the train, validation, and test sets from the NSVF model trained on ProgressLabeller outputs.}
  \label{tab:nsvf_metrics}
\end{table}