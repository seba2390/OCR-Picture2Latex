

In ProgressLabeller, we provide an interactive GUI for users to label object poses for a large amount of data in several steps. We incorporate camera pose estimation systems that reconstructed a 3D scene for aligning objects with, and calculate pose transforms that can be use to propagate the labelled object pose to all image frames. We provide interface to switch views among every RGB images to enable verification of labelled object pose by checking the alignment of re-projected mask with RGB images in multiple views.
In this work, we assume the 3D mesh models of objects are provided and the objects are static in the scene during data collection (in special cases like T-LESS dataset~\cite{hodan2017tless}, the objects are put onto a turntable and moving altogether while the cameras are static, the algorithm can still work because no environment background is captured by the cameras).
% semi-automated pipeline, assumption: static scene, known object model
\subsection{Camera pose estimation}
We incorporated ORB-SLAM3 \cite{campos2021orb}, KinectFusion \cite{newcombe2011kinectfusion} and COLMAP \cite{schonberger2016structure, schonberger2016pixelwise} to do RGB, depth or RGB-D camera pose estimation as well as reconstruction. ORB-SLAM3 is selected as the default method for its balanced speed and accuracy. COLMAP was tested and proved to be more accurate but the time cost of reconstruction over more than 1000 images is unaffordable.

\subsection{Object alignment by multi-view silhouette matching}
Different from methods that align object 3D models with reconstructed point clouds, our system created a multi-view graphical user interface that overlays the object model's projection onto the original RGB images, so that the object pose errors could be easily detected from areas with misalignment of object texture, silhouette and boundary, as shown in Figure~\ref{fig:blender_view}. Compared to depth reconstructed based methods, the pose label accuracy is improved based on higher accuracy of RGB than depth sensing. Besides, the system can also be used to label scenes with unreliable depth from transparent objects and backgrounds (see Section~\ref{sec:other}).

 \begin{figure}[htbp]
     \centering
     \includegraphics[width=0.95\columnwidth]{figure/blender_gui.png}
     \caption{Capture of ProgressLabeller user interface for multi-view re-projection checking. The top-left view shows the aligned object models with rendered texture at labelled poses. The bottom-left view and top-right view show the silhouettes and boundaries respectively at the same camera view, and the bottom-right view shows the boundaries from another view for validation.}
     \label{fig:blender_view}
 \end{figure}

% \cxt{simulation of such multi-view alignment pose annotation}

% We incorporate two implementations into pipeline in this work, COLMAP \cite{schonberger2016structure, schonberger2016pixelwise} and ORB-SLAM3 \cite{campos2021orb} to do RGB or RGB-D camera pose estimation as well as reconstruction. Generally, both of algorithms extract, match RGB features as landmarks and do bundle adjustment to calculate camera pose transforms. While ORB-SLAM3 is much faster in most cases, COLMAP does more frequent global bundle adjustment and is able to adapt to image sequences in random order, or with large disturbances in camera trajectory. As the two algorithms are complementary to each other, we integrate both implementations as available choices in our pipeline. It is worth mentioning we design the camera pose estimation as an individual module, so any 3D reconstruction implementation that gives camera pose estimates would possibly work.

% \cxt{todo: analysis of multi-view object render mask vs. gt on labelling pose}
\subsection{Semi-automatic labelling pipeline}

We build ProgressLabeller as a plugin on Blender \cite{blender}, which provides a good multiple view interface of overlaid RGB image, 3D reconstructed scene and objects for labelling and verifying poses. The overall procedure of labelling object poses includes several steps within the Blender graphical interface: 
\begin{enumerate}
    \item Import RGB(D) images and object 3D models into the 3D interactive workspace. Set parameters including camera intrinsics, camera pose estimation parameters and display settings, etc.
    \item Do camera pose estimation, then the estimated camera poses and a reconstructed 3D point cloud will be added to the workspace. Depth images are used to solve the scale of reconstruction.
    % \item When depth is not available, the scale of reconstruction needs to be solved by aligning some landmark object with RGB feature point cloud.
    \item (optional) When depth input is available, another point cloud fusing depth input based on estimated camera poses can be generated to provide a denser view of the entire scene and help find objects' rough positions.
    \item (optional) Do plane alignment for a better viewpoint and ease of correcting object pose
    % \item Manually align object with the 3D reconstructed point cloud to get a rough pose
    % \item (optional) Refine the object pose using local point cloud registration
    \item Align the object so that its re-projection matches the ground truth area in multiple views of RGB images
    \item Export labelled object poses and render segmentation masks, bounding box labels, etc.
\end{enumerate}

\noindent
In step 2, when there is only RGB image available, the scene scale (3D point cloud and camera's trajectory position) is unknown, to solve this scale problem, we take advantage of the known object sizes by dragging them to align with the 3D reconstruction and verify that in multiple different views.
In step 3, we implemented a depth fusion module based on the estimated camera poses from RGB reconstruction, based on the Signed Distance Functions as in KinectFusion \cite{newcombe2011kinectfusion}. The fused point cloud could give a rough reference of object locations.
In step 4, Iterative Closest Point (ICP) is used for aligning RGB feature or depth point cloud to X-Y plane in Blender.
In step 5, the object pose is labelled with visually ensured accuracy, which is the essential design that enables labelling of objects with only RGB images based on multi-view geometry. By re-projecting object's 3D model and check whether it aligns perfectly with the object's true textures or silhouettes in the RGB images from multiple views, users can fine-tune the object's pose and verify the error seamlessly until the object matches the images. 
% During this step, a dense reconstruction in 3D could help align the object's pose, as shown in Figure \ref{fig:blender_view}. However, the pipeline still works when there is no depth information available, and an example of labelling a transparent cup shown in Figure \ref{fig:demo_trans}.  

The entire labelling pipeline typically takes around 30 minutes for one data stream about 5K images. Data import and export takes about 20-30\% of time, with a rendering speed around 4-20 images per second depending on number of objects in the scene. The rendering could be done in parallel on a GPU if a vast amount of data is required. ORB-SLAM3 camera pose estimation takes about 10-20\% of time. Manual labelling and verification take the rest of time. 
% Compared with LabelFusion \cite{marion2018label}, our re-projection based verification directly shows the error to the user during the labelling process, and typically requires more tuning and generates more accurate output, which we will verify in the following experiments section.