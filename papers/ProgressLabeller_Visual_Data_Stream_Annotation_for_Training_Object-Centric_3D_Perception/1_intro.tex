%Compared with 2D annotations like bounding boxes and semantic masks, 3D annotations on visual images, such as object's 6D pose, could better direct robotic in scene understanding and interaction with the 3D world. 

Visual perception tasks often require vast amounts of labelled data due to their use of deep neural networks.   Such deep neural networks have outperformed traditional methods in object pose estimation \cite{labbe2020cosypose, he2021ffb6d, hodavn2020bop} when trained on public large-scale datasets \cite{xiang2017posecnn, hodan2017tless, hinterstoisser2012model}. 
However, considering the practice of deploying such systems in real-world robotics applications, such as semantic grasping and manipulation, current pose estimation systems can prove the difficulty of adaptation to different objects and settings without retraining with a customized large-scale dataset.

\begin{figure}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=\columnwidth]{figure/teaser.pdf}
    \captionof{figure}{The ProgressLabeller offers an interactive GUI for aligning all kinds of objects in the 3D scene to generate large-scale datasets with ground truth pose labels. The left image shows the rough 6D pose estimates from one state-of-the-art RGB-D deep models trained on public YCB dataset, and the right images shows fine-tuned pose estimates from the same model after retraining using generated data from ProgressLabeller. The pose estimates are then used for robotic grasping experiments.}
    \label{fig:teaser}
\end{figure}

In particular, our need for training data is a result of object labels for pose estimation being defined to specific 3D object models (both geometry shape and texture).  Learned models cannot be fine-tuned to transfer to similar object instances without additional training data. Recent work has made advances in category-level or unseen pose estimation \cite{li2020category, park2020latentfusion}. However, the objects included only cover a small set and there is no evidence showing the estimated pose is reliable enough for robotic manipulation. Further, the estimation results of deep neural networks are often vulnerable to environmental changes \cite{chen2019grip}, including different lighting conditions, occlusions and object's special appearance like transparent or reflective surfaces. Synthetic data generation with domain randomization and photo-realistic rendering \cite{to2018ndds, denninger2019blenderproc} could improve generalizability, but it is still challenging to simulate real-world lighting as well as the noise inherent in the sensor modality. We show in the experiment that the network trained using real data is still over-performing synthetic data.

To address the problem of adaptation for deep pose estimation systems and their application to robotic manipulation, we propose \textbf{ProgressLabeller} as a method and implementation for creating large customized datasets more efficiently. 
%To increase the data collection efficiency with low dependency on hardware, the system is supposed to label raw RGB(D) video frames without any landmarks like fiducial markers \cite{olson2011apriltag} or motion capture systems. 
Inspired by LabelFusion \cite{marion2018label}, ProgressLabeller collects training data of objects {\it in situ} in a mixed-initiative manner, similar in spirit to work by Gouravajhala et al.~\cite{gouravajhala2018eureca}.  It takes visual streams of color images that observe objects in a physical environment as input.  Objects in this stream only need to be labelled once by a human user through visual annotation. ProgressLabeller builds on recent advances in Structure-from-Motion \cite{schonberger2016structure} and visual SLAM \cite{mur2017orb} to produce both a 3D reconstruction and camera pose along the trajectory of the collected visual stream, where the  annotated object labels can be propagated to all frames.   

Compared to depth-based fusion methods, the color feature-based pipeline of ProgressLabeller suffers less noisy or invalid readings than that from depth sensing.  Further, the use of color by ProgressLabeller allows it to include objects that are transparent and reflective \cite{liu2021stereobj} into the pose estimation process,  as long as there exists textures from other objects or background.  From an interface perspective, our implementation of  ProgressLabeller aims to provide a more interactive design geared for users performing labeling tasks.  This interface design enables seamless labelled pose validation in different views by checking and correcting the discrepancy between the masks of re-projected object models and original RGB images (an example is shown in Figure \ref{fig:blender_view}). The intuition is that, if then the labelled pose is close to ground truth, the re-projected object mask should align with the object's true area in RGB images from multiple views.% with wide baselines for triangulation.

In this paper, we introduce ProgressLabeller as a semi-automatic approach to object 6D pose labelling on RGB(D) image sequence/video frames that works for transparent objects.  Our aim is to release ProgressLabeller as an open-source tool for more effective dataset generation for object and pose recognition by robots.
We evaluate the labelling accuracy of ProgressLabeller against LabelFusion on data stream samples from 4 public datasets with respect to segmentation mask and object pose accuracy. With the proposed system, we created a dataset of YCB objects \cite{calli2015ycb} (about 1.2M object instance labels) within 2 days of data collection and labelling. The dataset presents more challenges than the public YCB-Video dataset \cite{xiang2017posecnn} with more occlusions and better coverage of camera view directions, and collected using three different RGB-D cameras to evaluate the generalization across sensors. We fine-tuned a state-of-the-art RGB-D deep pose estimator \cite{he2021ffb6d} on our dataset and observed a large improvement on pose estimation accuracy and robotic grasping success rate, compared with the pretrained model on public dataset, as well as on the same amount of data from image-synthesis or LabelFusion annotation.  

%To summarize, our contributions are as follows:
%\begin{itemize}
%    \item An open-source semi-automatic object 6D pose labelling tool on RGB(D) image sequence/video frames that works for transparent objects and with higher label accuracy than previous method and public datasets.
%    \item A multi-camera large-scale RGB-D dataset with over 1M object pose labels with challenging occlusions.
%    \item Quantitative evaluation on pose labelling accuracy and ablation study of fine-tuning deep networks on pose estimation accuracy and robotic grasping success rate.
%\end{itemize}