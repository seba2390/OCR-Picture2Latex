We now discuss the stability of the reduced-order systems, obtained by using \Cref{algo:BT_qbdae}. For this, we consider only the autonomous part of the QB system as follows:
\begin{equation}\label{eq:auto_QB}
\dot{x}(t) = Ax(t) + H~x(t)\otimes x(t),
\end{equation}
where $x_{eq} = 0$ is a stable equilibrium. 
%\rd{ If we can construct a Lyapunov function $\cF(x)$, satisfying 
%\begin{equation*}
%\cF(x) >0\quad\mbox{and}\quad \dfrac{d}{dt}\cF(x)<0,\qquad \forall x\in \cB_{0,r}\backslash \{0\},
%\end{equation*}
%where $\cB_{0,r}$ is a ball of radius $r$ centered around $0$, then the system can reach to $x_{eq}$ asymptotically; hence the system is a locally asymptotically stable. \rd{check definition} }
{%\color{blue}
In the following, we discuss Lyapunov stability of $x_{eq}$. For this, we first note the definition of the latter stability.
\begin{definition}
	Consider a  QB system with $u\equiv 0$ \eqref{eq:auto_QB}. If there exists a Lyapunov function $\cF : \Rn \rightarrow \R$ such that 
	\begin{equation*}
	\cF(x) >0\quad\mbox{and}\quad \dfrac{d}{dt}\cF(x)<0,\qquad \forall x\in \cB_{0,r}\backslash \{0\},
	\end{equation*}
	where $\cB_{0,r}$ is a ball of radius $r$ centered around $0$, then $x_{eq} = 0$ is a locally asymptotically stable. 
\end{definition}
}
However, many other notions of the stability of nonlinear systems are available in the literature, for instance based on a certain dissipation inequality~\cite{bond2010compact}, which might be difficult to apply in the large-scale setting. In this paper, we stick to the notion of the Lyapunov-based stability for the reduced-order systems.
\begin{theorem}
Consider the QB system~\eqref{eq:Quad_bilin_Sys} with a  stable matrix  $A$. Let $P_\cT$ and $Q_\cT$ be its truncated reachability and observability Gramians, defined in \Cref{coro:gram_trc_re}, respectively. If the reduced-order system is determined as shown in \Cref{algo:BT_qbdae}, then for a Lyapunov function $\cF(\hx) = \hx^T\Sigma_1\hx$, we have
$$\cF(\hx) >0,\qquad \tfrac{d}{dt}(\cF(\hx)) <0\qquad \forall~ \hx \in \cB_{0,r}\backslash \{0\},$$
where $r = \dfrac{\sigma_{\min}(\cV^T\cG\cV)}{2 \|\Sigma_1\| \|\hH\|}$
and $\cG =  \cH^{(2)}(P_1\otimes Q_1) \left(\cH^{(2)}\right)^T + \sum_{k=1}^mN_k^TQ_1N_k +C^TC$ with $P_1$ and $Q_1$ being  the solutions of~\eqref{eq:linear_CG} and \eqref{eq:linear_OG}, respectively.
\end{theorem}\label{thm:stability}
\begin{proof}
First, we establish the relation between $\cV$, $\cW$, $Q_\cT$ and $\Sigma_1$. For this, we consider
\begin{align*}
\cW\Sigma_1 &= R V_1 \Sigma_1^{\tfrac{1}{2}} = R V_1\bbm \Sigma_1 & 0 \ebm^T U^T U_1 \Sigma_1^{-\tfrac{1}{2}}= R V\Sigma U^T U_1 \Sigma_1^{-\tfrac{1}{2}}\\
& = R R^TS^T U_1 \Sigma_1^{-\tfrac{1}{2}} = Q_\cT\cV.
\end{align*}
Keeping in mind  the above relation, we get
\begin{equation}\label{eq:red_gram}
\begin{aligned}
&\hA^T \Sigma_1 + \Sigma_1 \hA + \cV ^T\cG\cV  = \cV^T A^T \cW\Sigma_1 + \Sigma_1 \cW^T A \cV + \cV^T\cG \cV \\
&\qquad = \cV^T A^T Q_\cT \cV +\cV^T Q_\cT A \cV + \cV^T \cG \cV  = \cV^T(A^TQ_\cT + Q_\cT A + \cG)\cV = 0.
\end{aligned}
\end{equation}
%In the similar fashion, we can argue the following result
%\begin{align*}
%&\hA \Sigma_1 + \Sigma_1 \hA^T + \cW ^T\cG_2\cW =0.
%\end{align*}
Since $\cG$ is a positive  semidefinite matrix and $\cV$ has full column rank,  $\cV^T\cG\cV$ is also positive semidefinite. This implies that $\eta(\hA)~\leq~0$, where $\eta(\cdot)$ denotes the spectral abscissa of a matrix. Coming back to the Lyapunov function $\cF(\hx) = \hx^T\Sigma_1 \hx$, which is always greater than $0$  for all $\hat{x}\not=0$ due to $\Sigma_1$ being a positive definite matrix, we compute the derivative of the Lyapunov function as
 \begin{equation*}
 \begin{aligned}
  \dfrac{d}{dt}\cF(\hx) &= \dot{\hx}^T\Sigma_1\hx + \hx^T\Sigma_1\dot{\hx} \\
  &= \hx^T \hA^T \Sigma_1 \hx +  (\hx^T\otimes \hx^T)\hH^T \Sigma_1\hx + \hx^T \Sigma_1\hA\hx + \hx^T \Sigma_1 \hH (\hx\otimes \hx)\\
  &= \hx^T(\hA^T\Sigma_1 + \Sigma_1\hA) \hx +  (\hx^T\otimes \hx^T)\hH^T \Sigma_1\hx  + \hx^T \Sigma_1 \hH (\hx\otimes \hx).
 \end{aligned}
 \end{equation*}
 Substituting $\hA^T\Sigma_1 + \Sigma_1\hA = - \cV^T\cG\cV$ from \eqref{eq:red_gram} in the above equation yields
 \begin{align}\label{eq:2}
  \tfrac{d}{dt}\cF(\hx) &=  -\hx^T\cV^T\cG\cV \hx +  2\hx^T \Sigma_1 \hH (\hx\otimes \hx).
 \end{align}
As
 \begin{align*}
  & \hx^T \cV^T\cG\cV \hx \geq  \sigma_{\text{min}}(\cV^T\cG\cV) \|\hx\|^2,
 \end{align*}
implying
  \begin{align*}
   &  -\hx^T\cV^T\cG\cV x \leq -\sigma_{\text{min}}(\cV^T\cG\cV) \|\hx\|^2,
  \end{align*}
 inserting the above inequality in~\eqref{eq:2} leads to
  \begin{align*}
   \tfrac{d}{dt}\cF(\hx) &\leq  -\sigma_{\text{min}}(\cV^T\cG\cV) \|\hx\|^2 +  2\|\hx\|^3 \|\Sigma_1\| \|\hH\|.
  \end{align*}
%  For $\hx \in \cB_{0,r}\backslash \{0\}$, we have
%  $$\|\hx\| < \tfrac{\sigma_{\text{min}}(\cV^T\cG\cV)}{2\cdot \|\Sigma_1\|\cdot \|\hH\|}.$$
For locally asymptotic stability of the reduced-order system, we require
   \begin{align*}
   \tfrac{d}{dt}\cF(\hx) &\leq  -\sigma_{\text{min}}(\cV^T\cG\cV) \|\hx\|^2 +  2\|\hx\|^3 \|\Sigma_1\| \|\hH\| <0,
  \end{align*}
which gives rise to the following bound on $\|\hx\|$:
  $$\|\hx\| < \tfrac{\sigma_{\text{min}}(\cV^T\cG\cV)}{2 \|\Sigma_1\|  \|\hH\|}.$$
  This concludes the proof.
\end{proof}
%\begin{remark}
%Assume an $\kappa > 0$ such that $\eta(A+\kappa I) <0$ and let $P_t$ and $Q_t$ be the truncated Gramians~\eqref{}
%
%
%If we solve the generalized quadratic Lyapunov equations for Gramians $P$ and $Q$ (defined in~ \eqref{eq:cont_lyap} and \eqref{eq:obser_lyap}, respectively) with a shifted matrix $A$, given as $A + \kappa I$ and determine reduced-order system matrices $\hA$ and $\hH$ as shown in Theorem~\ref{thm:stability}, then the stability  region can be increased by factor $\frac{\kappa}{\|\Sigma_1\|\cdot\|\hH\|} $.
%\end{remark}
%\begin{theorem} Consider a balanced quadratic-bilinear system in the block form as follows:
%\begin{align*}
% A &= \begin{bmatrix} A_{11} & A_{12} \\ A_{12} & A_{22}\end{bmatrix} \quad H = \begin{bmatrix} H_1 \\ H_2\end{bmatrix},\quad N = \begin{bmatrix} N_1 \\ N_2\end{bmatrix},
% B = \begin{bmatrix} B_1 \\ B_2\end{bmatrix}  \quad C = \begin{bmatrix} C_1 & C_2\end{bmatrix} .
%\end{align*}
%Furthermore, let $(A,B)$ and $(A,C)$ be controllable and observable, respectively and assume matrix $A$ is stable, i.e., eigenvalues of matrix $A$ lie in the negative half plan. Also consider that $\sigma(\Sigma_1)\cup \sigma(\Sigma_2) = \emptyset$. Then, $\sigma(A_{11}) \subset \mathbb{C}_-$.
%\end{theorem}
%\begin{proof}
% Since, we assume that the quadratic-bilinear system is balanced. This implies
% \begin{equation}\label{eq:Con_gram}
%  A\Sigma + \Sigma A^T + BB^T + N\Sigma N^T + H(\Sigma\otimes \Sigma) H^T = 0
% \end{equation}
%and
% \begin{equation}
%  A^T\Sigma + \Sigma A + C^TC + N^T \Sigma N + H^{(2)}(\Sigma\otimes \Sigma) (H^{(2)})^T = 0.
% \end{equation}
%  Now, we consider the upper left block of~\eqref{eq:Con_gram} which can be written as follows:
% \begin{equation}
%  A_{11}\Sigma_{1} + \Sigma_1A_{11}^T + N_1\Sigma N_1^T + H_1\Sigma \otimes \Sigma H_1^T + B_1B_1^T = 0.
% \end{equation}
%We begin with an contradictory assumption that $A_{11}$ has the positive eigenvalues, i.e., $A_{11}x = \lambda x$ where $\lambda >0$ is eigenvalue and $x$ is the corresponding eigenvector. We multiply~\eqref{eq:Con_gram} with $x$ and $x^*$ from left and right side, respectively to get
% \begin{equation}
%  \bar{\lambda}x^*\Sigma x + \lambda x^*\Sigma x +  x^*H_1(\Sigma\otimes \Sigma) H_1^Tx +x^*N_1\Sigma N_1^Tx + x^*B_1B_1x = 0.
% \end{equation}
%Since $\Sigma$ and $\Sigma\otimes \Sigma$ are the positive definite matrices. Hence, this is contradiction to our assumption. Therefore, the eigenvalues of $A_{11}$ do not lie in the right half plan. Next, we assume that $A_{11}$ has eigenvalues which lie on imaginary axis. For this, we consider that the transformation matrix $T = \begin{bmatrix} T_{11} & 0 \\ 0 & I \end{bmatrix}$ such that $T_{11}A_{11}T_{11}^{-1} = \begin{bmatrix} F_{11} & 0 \\ 0& F_{22} \end{bmatrix}$ where $F_{11}$ and $F_{22}$ contain the negative eigenvalues and purely imaginary eigenvalues, respectively.  Let the transformed quadratic-bilinear system and Gramians to be as follows:
%\begin{align*}
% \tA &= \begin{bmatrix} F_{11}  & 0 & F_{13} \\ 0 & F_{22} & F_{23} \\ F_{31} & F_{32} & F_{33} \end{bmatrix}, &  \tB &= \begin{bmatrix} G_{1}  \\ G_2 \\ B_{2}  \end{bmatrix}, \quad \tH = TH(T\otimes T)^{-1}, \quad \tN = TNT^{-1},\\
% \tC &= \begin{bmatrix} J_1 & J_2 & C_2 \end{bmatrix},\quad& \tP &= \begin{bmatrix} P_{11}  & P_{12} & 0 \\ P_{12}^T  & P_{22} & 0 \\ 0 & 0 & \Sigma_2 \end{bmatrix},\qquad  \tQ = \begin{bmatrix} Q_{11}  & Q_{12} & 0 \\ Q_{12}^T  & Q_{22} & 0 \\ 0 & 0 & \Sigma_2 \end{bmatrix}.
%\end{align*}
%The Gramians $\tP$ and $\tQ$ satisfy the following Lyapnuov equations
% \begin{equation}\label{eq:Con_gramt}
%  \tA\tP + \tP \tA^T + \tB\tB^T + \tN \tP\tN^T + \tH(\tP\otimes \tP) \tH^T = 0
% \end{equation}
%and
% \begin{equation}
%  \tA^T\tQ + \tQ \tA + \tC^T\tC + \tN^T Q \tN^T +  \tilde{\cH}^{(2)}(\tP\otimes \tQ) (\tilde{\cH}^{(2)})^T = 0.
% \end{equation}
%% where $\tP$ and $\tQ$ are the solutions of the following Lyapnuov equations
%% \begin{equation}
%%  \tA\tP + \tP \tA^T + \tB\tB^T = 0
%% \end{equation}
%%and
%% \begin{equation}
%%  \tA^T\tQ + \tQ \tA + \tC^T\tC  = 0.
%% \end{equation}
%Moreover, since $\tN\tP\tN$, $\tN^T\tQ \tN$, $\tH(\tP\otimes \tP) \tH^T$ and $\tilde{\cH}^{(2)}(\tP\otimes \tQ)(\tilde{\cH}^{(2)})^T$ are the positive semi-definite matrices. Therefore, it is possible to compute Cholesky decomposition of these matrices, given as follows:
%\begin{align*}
% \tN\tP\tN &=  \mathscr{L}\mathscr{L}^T,\quad \tH(\tP\otimes \tP) \tH^T = \cL\cL^T,\\
% \tN^T\tQ \tN & = \mathscr{M}\mathscr{M}^T,\quad\tilde{\cH}^{(2)}(\tP\otimes \tQ) = \cM\cM^T,
%\end{align*}
%where $\mathscr{L}$ and $\cL$ are also partitioned as
%$$\mathscr{L} = \begin{bmatrix} \mathscr{L}_1^T & \mathscr{L}_2^T  & \mathscr{L}_3^T \end{bmatrix}^T,\qquad \text{and} \qquad \cL = \begin{bmatrix} \cL_1^T & \cL_2^T & \cL_3^T \end{bmatrix}^T ,$$
%and analogously, $\mathscr{M}$ and $\cM$ are also partitioned. If we consider the (2,2) block of ~\eqref{eq:Con_gramt}, then we obtain
%\begin{equation}\label{eq:1}
% F_{22}P_{22} + P_{22}F_{22}^* + B_2B_2^T + \mathscr{L}_2\mathscr{L}_2^T + \cL_2\cL_2^T = 0.
%\end{equation}
%Now, we consider the complex eigenvalue of $F_{22}$ and corresponding eigenvector as follows:
%$$y^*F_{22} = i\omega y^*\quad\text{with}\quad \omega>0. $$
%This also implies $F_{22}^*y = -j\omega y $. Next, we multiply~\eqref{eq:1} with $y^*$ and $y$ from left and right, respectively to obtain
%\begin{align*}
%  y^*F_{22}P_{22}y + y^*P_{22}F_{22}^*y + y^*G_2G_2^Ty + y^*L_2L_2^Ty &= 0,\\
%    j\omega y^*P_{22}y -j\omega y^*P_{22}y + y^*G_2G_2^Ty + y^*L_2L_2^Ty &= 0.
%\end{align*}
%This gives us $G_2^Ty = 0$, $\mathscr{L}^Ty$ and $\cL_2^Ty = 0$. As we know that $F_{22}$ contains only complex eigenvalue values. Therefore, we have
%$G_2^TY =0$, $\mathscr{L}^TY$ $\cL_2^TY = 0$ with invertible $Y$. Hence, $G_2 = 0$, $\mathscr{L}_2$and $\cL_2 = 0$. Using the similar argument, we get $J_2 = 0$, $\mathscr{M}_2 = 0$ and $\cM = 0$. Now, we consider the (1,2) block to get
%\begin{equation}
% F_{11}P_{12} + P_{12}F_{22}^T=0
%\end{equation}
%which gives us $P_{12} = 0$. Similarly, we obtain $Q_{12} = 0$. Finally we consider (2,3) block of two Lyapnuov equation which are as follows
%\begin{equation*}
% F_{23}\Sigma_2 + P_{22}F^*_{32} = 0\quad\text{and}\quad F_{23}^*\Sigma_2+Q_{22}F_{23} = 0
%\end{equation*}
%which gives
%\begin{equation*}
% F_{23}\Sigma_2^2 + P_{22}Q_{22}F_{23} = 0.
%\end{equation*}
%Since eigenvalues of $P_{22}Q_{22}$ are the subset of eigenvalues of $\Sigma_2^2$. Hence, $\sigma(P_{22}Q_{22}) \cap \Sigma_2 = \emptyset$. This leads to $F_{23} = 0$ and analogously $F_{32} = 0$. All these discussions gives us the following structure of matrices $\tA,\tB$ and $\tC$
%\begin{equation*}
% \begin{aligned}
% \tA &= \begin{bmatrix} F_{11}  & 0 & F_{13} \\ 0 & F_{22} & 0 \\ F_{31} & 0 & A_{22} \end{bmatrix},\quad \tB = \begin{bmatrix} G_{1}  \\ 0 \\ B_{2}  \end{bmatrix}, \quad
% \tC &= \begin{bmatrix} J_1 & 0 & C_2 \end{bmatrix}.
%\end{aligned}
%\end{equation*}
%This shows that $(\tA,\tB)$ is not controllable which contradicts our assumption. Therefore, $A_{11}$ does not have any eigenvalue which lies on imaginary axis. Hence, $\sigma(A_{11}) = \mathbb{C}_-$.
%\end{proof}
%Unlike the linear case, the stability of the reduced quadratic-bilinear systems not only depends on eigenvalues of $\hA$ but also other terms such as $\hH$ also plays an important role to determine the stability of the system. Therefore, we next seek to determine the region of stability of the reduced-order QBDAE.
%
%\begin{theorem}
% Consider asymptotically stable $A\in \Rnn$ and $\hA\in \Rrr$. Let  $S_r >0$ be a symmetric matrix and $R_r >0$  be the solution of the following Lyapunov equation
% \begin{equation*}
%\hA^TR_r + R_r\hA + S_r = 0.
%\end{equation*}
% Then, for a Lyapunov function $\cV(\hx) = \hx^TR_r\hx$, we have
%\begin{equation*}
% \dfrac{d}{dt}\cV(\hx) <0\quad \forall \hx \in \cB_{0,r}\backslash \{0\},
%\end{equation*}
%where $\cB_{0,r}$ is a ball of radius $r = \tfrac{\sigma_{\text{min}}(S_r)}{2\|R_r\|\|\hH\|}$.
%\end{theorem}
%\begin{proof}
% Since, we have a Lyapunov function as $\cV(\hx) = \hx^TR_r\hx$ with a positive definite matrix $R_r$. This implies $\cV(\hx) >0 $, by properties of positive definite matrices.
% Now, we have the derivative of the Lyapunov function which gives us
% \begin{equation}
% \begin{aligned}
%  \dfrac{d}{dt}\cV(\hx) &= \dot{x}^T_rR_r\hx + \hx^TR_r\dot{x}_r \\
%  &= \hx^T \hA^T R_r \hx +  (\hx^T\otimes \hx^T)\hH^T R_r\hx + \hx^T R_r\hA\hx + \hx^T R_r \hH (\hx\otimes \hx)\\
%  &= \hx^T(\hA^TR_r + R_r\hA) \hx +  (\hx^T\otimes \hx^T)\hH^T R_r\hx  + \hx^T R_r \hH (\hx\otimes \hx).
% \end{aligned}
% \end{equation}
% Moreover, We know that if reduced-order system is constructed using Algorithm~\eqref{}, then $\hA$ is asymptotically stable, i.e., $\sigma(\hA) \in \mathbb{C}_-$. This implies, we can choose the matrix $R_r >0$  which satisfies the following Lyapunov equation
% \begin{equation*}
%  \hA^TR_r + R_r\hA + S_r = 0, \qquad \text{with}\qquad S_r = S_r^T > 0
% \end{equation*}
% Substituting for $\hA^TR_r + R_r\hA = - S_r$ to get
% \begin{align}\label{eq:2}
%  \dfrac{d}{dt}\cV(\hx) &=  -\hx^TS_r \hx +  2\hx^T R_r \hH (\hx\otimes \hx).
% \end{align}
%Next, we know that
%\begin{align*}
% & x^T S_r x \geq  \sigma_{\text{min}}(S_r) \|x\|^2  \Rightarrow -x^TS_rx \leq -\sigma_{\text{min}}(S_r) \|x\|^2.
%\end{align*}
%Inserting the above inequality in~\eqref{eq:2} yields
% \begin{align}
%  \dfrac{d}{dt}\cV(\hx) &\leq  -\sigma_{\text{min}}(S_r) \|\hx\|^2 +  2\|\hx\|^3 \|R_r\| \|\hH\|.
% \end{align}
% For $\hx \in \cB_{0,r}\backslash \{0\}$, we have
% $$\|\hx\| < \tfrac{\sigma_{\text{min}}(S_r)}{2\|R_r\|\|\hH\|}.$$
% This implies
%  \begin{align*}
%  \dfrac{d}{dt}\xi(\hx) &\leq  -\sigma_{\text{min}}(S_r) \|\hx\|^2 +  2\|\hx\|^3 \|R_r\| \|\hH\| <0.
% \end{align*}
% Hence, the reduced-order system is locally stable for
% $$\|\hx\| < \tfrac{\sigma_{\text{min}}(S_r)}{2\|R_r\|\|\hH\|}.$$
%\end{proof}
