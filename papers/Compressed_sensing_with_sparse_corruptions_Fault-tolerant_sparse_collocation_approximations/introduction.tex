\section{Introduction}
The approximation of function values using point evaluations or samples is necessary in a wide number of applications. Much attention has been focused recently on the approximation technique of compressive sampling (CS): The ability to recover sparse linear representations of a function from a given dictionary. This is a particularly important problem in parametric uncertainty quantification (UQ) where the number of parameters translates into the number of variables on which an unknown function depends (the ``dimension" of the problem). It is common for dimension to be very large, and the number of degrees of freedom in classical approximation strategies generally grows exponentially with the dimension. This makes classical computational procedures for approximating functions infeasible for large dimensions. 

In contrast, compressive sampling seeks a sparse representation of a function using only a small number of samples or measurements, regardless of the parametric dimension. In a non-intrusive UQ pipeline, each function sample corresponds to a potentially large-scale simulation, and so minimizing the requisite number of samples is desirable. When functions are sparse or compressible in a given basis or dictionary, this reconstruction procedure has the potential to mitigate the exponentially debilitating curse of dimensionality. Algorithms in UQ that utilize compressive sampling have enjoyed great success in recent years \cite{KarniadakisUQCS,yang_alternating_2011,yin_minimization_2015,PengHamptonDoostantweighted,jakeman_generalized_2016,DoostanOwhadiSparse,JakemanEtAl_l1Enhance,GuoEtAlRandomizedQuad,NarayanZhouCCP}. For related theoretical contributions, see \cite{AdcockCSFunInterp,Adcockl1Pointwise,ChkifaDownwardsCS,HamptonDoostanCSPCE,RauhutWardWeighted,Rauhut,YanGuoXui_l1UQ}.

Missing from the sparse recovery UQ contributions above is a concrete strategy for fault-tolerant or resilient algorithms. Ensuring modeling resilience for UQ in the presence of system failures is essential for credible prediction on new and emerging massively parallel systems. Fault-tolerant algorithms in general have become necessary in computational science since node failures on distributed architectures can yield corrupted data (the frequency of which increases as the number of processors increases), or algorithmic run-time software failures can result in polluted simulation results. These failures can generate polluted measurements in unpredictable and sometimes undetectable ways \cite{bridges_fault-tolerant_2012}. 

Faults can occur  due  to complex  combination  of  internal  and  external  conditions  that are difficult to reproduce. For example, bits may suffer random corruption, or physical defects in hardware may cause data faults. Corruption errors during model simulation can be grouped into two main types, soft and hard. In this paper, we consider hard faults as errors that cause the simulation to terminate prematurely and/or return obvious, automatically detectable error values such as NaN or Inf.  Hard faults by this definition are easy to identify and mark for discard, thus obviating or ameliorating the need for fault-tolerant algorithms. 

In contrast, soft failures are essentially random systematic corruption of results that are not easily identifiable. These soft failures pose challenges in UQ: A soft failure  will not cause obvious failure in fault-intolerant UQ methods; however, incorrect model values caused by soft failures can significantly degrade an approximation. It is in this case that we require the development of robust and resilient algorithms that can, ideally, deliver constant levels of performance when faced with a few highly corrupted data points.
%Faults can occur  due  to complex  combination  of  internal  and  external  conditions  that are difficult to reproduce. For example soft faults can be caused by bits that are randomly corrupted bits but do not leave physical damage and  hard  failures can be caused by a physical defect.


To address this issue, fault-tolerant algorithms for UQ have been investigated in the context of multilevel Monte Carlo algorithms \cite{pauli_fault_thesis_2014,pauli_intrinsic_2015,pauli_fault_2014}, and in overdetermined least-squares polynomial recovery problems \cite{shin_correcting_2016}. To the best of our knowledge, there is no comprehensive research in the UQ literature on fault-tolerant sparse recovery algorithms, and in the compressive sampling literature only a handful of papers \cite{LaskaEtAlCorrupt,LiCorruptionsConstrApprox,NguyenTranCorrupt,stankovic_missing_2014,StuderEtAlCorrupt,DSuCSCorruptFourier,DSuCorrupted,WrightMaCorruption} deal with the problem of corrupted measurements.

The operative distinction in the problem we consider in this paper is a hardware or software fault resulting in occasional large-magnitude errors; we call this the problem of \textit{corruptions}. Existing CS algorithms are known to be stable with respect to small noise perturbations, but cannot handle sparse corruptions, i.e., situations when a small number of samples are highly corrupted with the corruption magnitudes much larger than typical noise. In this paper we present novel theory and application studies of a sparse corruptions algorithm for CS. The algorithm we use was considered in \cite{LiCorruptionsConstrApprox}, but we present more general theoretical guarantees on recovery, including practical guidance for the choice of algorithmic regularization parameters. 

For fault-tolerance in the context of the sparse recovery problem, the recovery properties of an ideal resilient algorithm would be agnostic to large-magnitude corruptions in a small number of function samples. As described above, these corruptions can arise due to unknown failure modes in computational models or because of large but intermittent measurement errors. Development of mathematical theory for the corrupted compressive sampling problem, and investigation of a corresponding resilient algorithm for sparse recovery of expansion coefficients are the central goals of this paper. The target applications we investigate are exemplars of a common task in UQ: recovery of approximately sparse expansion coefficients in an orthogonal polynomial (polynomial chaos) basis.

The theory and algorithms developed in this paper have the following features: 
\begin{itemize}
  \item The compressive sampling recovery theorems are uniform with respect to the function and the corruptions. That is, the recovery guarantees hold over all compressible functions having sparsely corrupted measurements for a single random sampling of measurements.
  \item The algorithm involves a tunable regularization parameter $\lambda$, and a theoretically optimal choice of this parameter is explicitly determined by our analytical results. This theoretically optimal value is defined only by the \textit{ratio} of measurement corruptions to signal sparsity. Since signal sparsity is frequently comparable to the number of measurements, this optimal $\lambda$ loosely translates into the fraction of measurement samples that are corrupted. From a user's point of view, our analysis thus suggests a value of $\lambda$ having knowledge only of the ratio of measurements believed to be corrupted.
  \item In experiments, we observe that optimal values of the regularization parameter are non-trivially dependent on the number of measurements, the signal sparsity, and the number of corruptions. We thus propose an iteratively reweighted algorithm for recovery that learns values of the regularization parameter. Our experiments suggest that these learned algorithmic parameters perform better than the value defined by our theoretical results, and thus this reweighted algorithm is more useful in practice. 
  \item The location and magnitude of the corruptions amongst the collection of function samples can be unknown, but the algorithm recovers those locations and the corresponding corruption values.
  \item The algorithm is robust to small, but non-sparse measurement errors -- e.g.\ due to noise, truncation of an infinite polynomial expansion or numerical error in computing function samples -- and moreover is \textit{noise-blind}.  That is to say, it requires no \textit{a priori} upper bound on such errors.
  \item \edits{The optimization problem we solve to compute solutions is from \cite{LiCorruptionsConstrApprox}, but our work is both a theoretical and practical advancement over the results in that reference. In order to show the solution computed is indeed the original sparse solution, \cite{LiCorruptionsConstrApprox} uses conditions on the restricted isometry constant (RIC) of the measurement matrix. Our results are a significant relaxation of previously reported conditions on the RIC (compare conditions on $\delta_{2s, 2k}$ in Lemma 2.3 of \cite{LiCorruptionsConstrApprox} versus our Theorem \ref{t:RIP_stable_robust}, equation \eqref{delta_cond}, and the discussion in Section \ref{ss:lambda-strategy}). The results for general sensing matrices in \cite{LiCorruptionsConstrApprox} are nonuniform with respect to the signal and corruptions support, and require certain models for the signal and corruptions; our results are uniform and require no model for the signal or corruptions, other than compressibility. Finally, our paper is also devoted to numerical investigation of the performance of the method, including practical guidance for choosing the regularization parameter $\lambda$; such thorough investigations are absent in \cite{LiCorruptionsConstrApprox}.}
\end{itemize}
We first introduce notation and summarize the main mathematical statements of this paper in Section \ref{sec:notation}. This is followed in Section \ref{sec:theory} by our theoretical analysis. Section \ref{sec:results} presents numerical results to complement our theoretical analysis and verify the practical efficacy of the algorithm.
