\section{Model problem and main results}\label{sec:notation}

Let $f: \bbR^d \rightarrow \bbR$ denote an unknown function, and let $\left\{ \phi_j\right\}_{j=1}^N$ be a given dictionary of functions, $\phi_j : \bbR^d \rightarrow \bbR$. For example, the functions $\phi_j$ are frequently multivariate polynomial chaos basis elements; our capstone numerical examples will show results from such a basis. In scenarios of interest, the size $N$ of the dictionary is very large.

The ultimate goal is to recover coefficients $x_j$ that determine the approximation
\begin{align}\label{eq:f-definition}
  f(\xi) = \sum_{j=1}^N x_j \phi_j(\xi) + n(\xi),
\end{align}
using samples of $f$, where $n(\xi)$ is an assumed small discrepancy term between the exact function and its $N$-term linear approximation in $\phi_j$\footnote{Our notation suggests that $n = n(\xi)$ depends explicitly and deterministically on $\xi$; however, our theory encompasses the case when $n$ is a stochastic variable or process, e.g., independent Gaussian random variable additive perturbations of the measurements.}. For the purposes of exposition we assume $| n(\xi)| \leq \epsilon$ for some known uniform noise bound $\epsilon$; we will show later that lack of \textit{a priori} knowledge for this bound only affects theoretical results in benign ways. As described above, we assume the vector $x = \left(x_1, \ldots, x_N \right)^T \in \bbR^N$ to be compressible. Sparsity or compressibility of a vector can be quantified via its best $s$-term approximation error,
\begin{align*}
  \sigma_{s}(x)_p = \inf_{\|\tilde{x}\|_0 \leq s} \left\| x - \tilde{x} \right\|_p,
\end{align*}
where $\|\cdot\|_p$ is the standard $\ell^p$ norm on vectors; for $p=0$, $\| x \|_0$ is the sparsity of $x$, i.e., the number of non-zero elements in the vector.


With $\left\{\xi_1, \ldots, \xi_m\right\} \subset \bbR^d$ a collection of samples of $\xi$, we have the corresponding corrupted function measurements,
\begin{align*}
  y_k &= f(\xi_k) + c_k = \sum_{j=1}^N x_j \phi_j\left(\xi_k\right) + n\left(\xi_k\right) + c_k, & k&=1, \ldots m,
\end{align*}
where the corruption vector $c = \left(c_1, \ldots c_m \right)^T \in \bbR^m$ is assumed to be $k$-sparse but can have large entries. %We assume that the allowable corruptions sparsity $k$ is at least linearly dependent on $m$, i.e., that there exists a universal $\alpha \in (0,1)$ such that
%\begin{align}\label{eq:corruptions-sparsity}
%  \alpha m \leq k
%\end{align}
To enforce an underdetermined system, we assume $m < N$. Defining the rectangular matrix $A$ with entries $(A)_{j,k} = \phi_k\left(\xi_j\right)$, then the unknown vectors $x$ and $c$ satisfy the underdetermined linear system
%Let $x \in \bbC^N$, $c \in \bbC^m$ and $n \in \bbC^m$ with $\| n \|_{2} \leq \epsilon$.  Suppose that
\begin{align}\label{eq:linear-system}
  y = A x + c + n \in \bbR^m.
\end{align}
In order to compute the solution $(x,c)$ having knowledge of only $A$ and $y$, we consider the following model problem (see also \cite{LiCorruptionsConstrApprox} and references therein):
\be{
\label{l1_lambda_recovery}
\min_{z \in \bbC^N, d \in \bbC^m} \| z \|_{1} + \lambda \| d \|_{1}\ \mbox{subject to $\| A z + d - y \|_{2} \leq \epsilon \sqrt{m}$}.
}
Let $(\hat{x},\hat{c})$ be a minimizer of this problem, where $\hat{x} \in \bbR^N$ and $\hat{c} \in \bbR^m$.  Our objective is to obtain conditions on $A$ (in particular, on the number of measurements $m$) and $\lambda$ such that the error
\bes{
\| \hat{x} - x \|_{2} + \| c - \hat{c} \|_{2}
}
can be bounded by the best approximation numbers $\sigma_{s}(x)_1$ and $\sigma_{k}(c)_1$, and the noise magnitude $\epsilon$. 
%\annote{Ben: you made a note that, e.g., $\sigma_s(x)_1$ is not defined yet. Isn't this defined in the second unnumbered equation of this section above? Or do I misunderstand your point?}

\subsection{Main results}

In all that follows, the statement $a \lesssim b$ means $a \leq C b$ for some universal constant $C$. Our first main result shows that stable and robust recovery of $x$ and $c$ is implied by a certain modification of the classical Restricted Isometry Property (RIP) which incorporates the sparse corruptions term (Definition \ref{d:RIPcorruptions}).  Specifically, Theorem \ref{t:RIP_stable_robust} establishes that if the matrix $A$ satisfies the RIP for the corruptions problem of order $(2s,2k)$ (see Definition \ref{d:RIPcorruptions}) with constant $\delta_{2s,2k}$ satisfying
\begin{align}\label{eq:delta_intro}
\delta_{2s,2k} &< \frac{1}{\sqrt{1+\left ( \frac{1}{2\sqrt{2}} + \sqrt{\eta} \right)^2}}, & \eta = \frac{s + \lambda^2 k}{\min\left\{ s, \lambda^2 k \right\}},
\end{align}
%\begin{align}\label{eq:eta}
%  \eta = \frac{s + \lambda^2 k}{\min\left\{ s, \lambda^2 k \right\}},
%\end{align}
then the following error bounds hold:
\begin{subequations}\label{eq:recovery-summary}
\begin{align}\label{eq:recovery-summary-l1}
  \left\| x - \hat{x} \right\|_1 + \lambda \left\| c - \hat{c} \right\|_1 &\lesssim \sigma_s(x)_1 + \lambda \sigma_k(c)_1 + \epsilon \sqrt{s + \lambda^2 k},\\\label{eq:recovery-summary-l2}
  \left\| x - \hat{x} \right\|_2 + \left\| c - \hat{c} \right\|_2 &\lesssim \left(1 + \eta^{1/4}\right) \left( \frac{\sigma_s(x)_1}{\sqrt{s}} + \frac{\sigma_k(c)_1}{\sqrt{k}} + \epsilon \right).
\end{align}
\end{subequations}
Our second main result (Theorem \ref{t:BOS-RIP}) provides explicit conditions on $m$, $s$ and $k$ for \R{eq:delta_intro} to hold for matrices of so-called bounded orthonormal system \cite[Chpt.\ 12]{FoucartRauhutCSbook}.  Specifically, suppose that $\{ \phi_j \}^{N}_{j=1}$ is an $L^2_{\dx{\nu}}(D)$-orthonormal system, where $\nu$ is a probability measure and $D \subset \bbR^d$ its support.  Define
\begin{align*}
  K \coloneqq \max_{j=1, \ldots, N} \sup_{\xi \in D} \left|\phi_j(\xi)\right| < \infty,
\end{align*}
and let $A = \left \{ \phi_j(\xi_i) \right \}^{m,N}_{i,j=1}$ where $\xi_1,\ldots,\xi_m$ are drawn i.i.d.\ according to $\nu$.  If 
\begin{align}\label{eq:m-bound}
\begin{aligned}
  m & \gtrsim \delta^{-2} \cdot K^2 \cdot s \cdot \left( \log^3(2s) \cdot \log(2 N) + \log \epsilon^{-1} \right),
  \\
    m & \gtrsim \delta^{-2} \cdot K \cdot s \cdot k,
  \end{aligned}
\end{align}
then with probability at least $1-\epsilon$, the restricted isometry constant $\delta_{2s,2k}$ of the scaled matrix $\frac{1}{\sqrt{m}} A$ satisfies $\delta_{2s,2k} \leq \delta$.

One can see from these estimates that optimizing $\eta$ over values of $\lambda$ yields a minimum value of $\eta = 2$ when $\lambda^2 = s/k$. Assuming $s \sim m$, this provides a concrete determination of the parameter $\lambda$ for use in \eqref{l1_lambda_recovery} having knowledge only of the ratio of corrupted measurements.  We note in passing that we do not believe that the second condition in \R{eq:m-bound} is sharp in the dependence on the product $s \cdot k$.  Improvement of this to a condition of the form
\be{
\label{optimal-conjecture}
m \gtrsim \delta^{-2} \cdot K \cdot k,
}
is left as a topic for future work.  Note that such a condition is known for Gaussian random matrices.  Moreover, a nonuniform recovery result with the scaling \R{optimal-conjecture} for exactly sparse coefficients $x$ and corruptions $c$ having random sign patterns was given in \cite{LiCorruptionsConstrApprox}.  See Section \ref{sec:rip-2} for further discussion.

%One of the main results is a familiar leveraging of bounded orthonormal systems: assume that the dictionary elements $\phi_j$ are an $L^2(\bbR^d)$ orthonormal system with respect to the probability measure $\nu$ on $\bbR^d$. Define
%\begin{align*}
%  K \coloneqq \max_{j=1, \ldots, N} \sup_{z \in \supp \nu} \left|\phi_j(z)\right| < \infty
%\end{align*}
%Assume the samples $z_m$ are drawn iid from $\nu$. If 
%\begin{align}\label{eq:m-bound}
%  m \gtrsim K^2 s \left( s \log N + k \log m + \log s + \log \theta^{-1} \right),
%\end{align}
%then with probability at least $1 - \theta$, the minimizer $(\hat{x}, \hat{c})$ of \eqref{l1_lambda_recovery} satisfies both
%\begin{subequations}\label{eq:recovery-summary}
%\begin{align}\label{eq:recovery-summary-l1}
%  \left\| x - \hat{x} \right\|_1 + \lambda \left\| c - \hat{c} \right\|_1 &\lesssim \sigma_s(x)_1 + \lambda \frac{\sigma_k(c)_1}{\sqrt{m}} + \epsilon \sqrt{s + \lambda^2 k},\\\label{eq:recovery-summary-l2}
%  \left\| x - \hat{x} \right\|_2 + \left\| c - \hat{c} \right\|_2 &\lesssim \left(1 + \eta^{1/4}\right) \left( \frac{\sigma_s(x)_1}{\sqrt{s}} + \frac{\sigma_k(c)_1}{\sqrt{m k}} + \epsilon \right),
%\end{align}
%\end{subequations}
%where 
%\begin{align}\label{eq:eta}
%  \eta = \frac{s + \lambda^2 k}{\min\left\{ s, \lambda^2 k \right\}}.
%\end{align}
%(The sample count criterion in \eqref{eq:m-bound} is not quite the correct chaining of the estimates later.)
%
%One can see from these estimates that optimizing $\eta$ over values of $\lambda$ yields a minimum value of $\eta = 2$ when $\lambda^2 = s/k$. This provides a concrete determination of the parameter $\lambda$ for use in \eqref{l1_lambda_recovery} having knowledge only of the ratio of corrupted measurements.

It is common in compressed sensing to assume some \textit{a priori} known noise bound $\epsilon$ based on the user's knowledge of measurement noise or truncation error.  Although there are some results that circumvent this assumption \cite{AdcockCSFunInterp,Adcockl1Pointwise}, they typically yield somewhat weaker recovery guarantees. However, in the context of the sparse corruptions theory presented above, such prior knowledge of $\epsilon$ is not necessary for stable recovery: The error introduced by an unknown noise $\epsilon$ can be passed into theoretical estimates as a penalty of size $\epsilon$. To see this, note that if we define $c' \coloneqq \frac{1}{\sqrt{m}}(c + n)$, then the system $y = A x + c +n$ can be written as $\frac{1}{\sqrt{m}} y = \frac{1}{\sqrt{m}} A x + c'$. Solving \eqref{l1_lambda_recovery} by setting $\epsilon = 0$ results in the $\epsilon=0$ version of the estimate \eqref{eq:recovery-summary-l2} with $c'$ replacing $c$. However, the normalized best $k$-term approximation error to $c'$ appearing in \eqref{eq:recovery-summary-l2} is stable with respect to noise perturbations:
\begin{align*}
  \frac{\sigma_{k}(c')_{1}}{\sqrt{k}} \leq \frac{1}{\sqrt{k m}} \left( \sigma_{k}(c)_1 + \| n \|_{1}\right) \leq \frac{\sigma_{k}(c)_1}{\sqrt{k m}} + \sqrt{\frac{m}{k}} \epsilon.
\end{align*}
Here $\epsilon \geq \| n \|_{\infty}$ is any bound for the perturbation $n$ in the uniform norm. Using \eqref{optimal-conjecture}, we see that $\sqrt{\frac{m}{k}} \epsilon \lesssim \epsilon$, which is on the same order as the estimate \eqref{eq:recovery-summary-l2} that uses \textit{a priori} knowledge of $\epsilon$. A similar argument holds for the bound \eqref{eq:recovery-summary-l1}. 

%\GR{
%I changed the equation reference here to the conjecture \R{optimal-conjecture}.  The upper bound \R{eq:corruptions-sparsity} provides only a lower bound for $\sqrt{m/k}$.  One downside is that we can currently only get $\sqrt{m/k} \lesssim \sqrt{s}$ for matrices of BOS due to \R{eq:m-bound}.  But I don't think we need to mention this explicitly.  
%}
%\annote{I've included back the ref to \eqref{eq:corruptions-sparsity}; doesn't that give the upper bound $\sqrt{m/k} \leq 1/\sqrt{\alpha}$? Or do I misunderstand your statement above? I am not necessarily convinced that \eqref{eq:corruptions-sparsity} is a good assumption, though.}

While our theoretical results are thus insensitive to ignorance about small noise levels, we caution that it is always a good idea to use such information in practical recovery algorithms if available, e.g.\ as the result of cross validation.  See, for example, \cite{DoostanOwhadiSparse,KarniadakisUQCS,JakemanEtAl_l1Enhance}.  

%\GR{As you know, in my previous papers I've tried to present theorems which avoid the assumption of knowing the bound $| n(z)| \leq \epsilon$ (which is normally unknown in practice).  Interestingly, for the corruptions problem I don't think we need to do this.  Indeed, suppose we consider a modified corruption term $c' = \frac{1}{\sqrt{m}}(c+n)$.  Then we have the system
%\bes{
%\frac{1}{\sqrt{m}} y = \frac{1}{\sqrt{m}} A x + c'.
%}  
%Note: I've renormalized here since it's the matrix $\frac{1}{\sqrt{m}} A$ that satisfies the RIP, not $A$.  Then, if we solve the equality-constrained $\ell^1$ minimization problem (i.e.\ $\epsilon = 0$) we would get an error bound of the form
%\bes{
%\| x - \hat{x} \|_{2} + \| c' - \widehat{c'}\|_{2} \lesssim \sigma_{1}(x)_1 / \sqrt{s} + \sigma_{k}(c')_{1} / \sqrt{k}
%}
%Now note that
%\bes{
%\sigma_{k}(c')_{1} \leq \frac{1}{\sqrt{m}}(\sigma_{k}(c)_1 + \| n \|_{1}) \leq \sigma_{k}(c)_1/\sqrt{m} + \sqrt{m} \epsilon.
%}
%Recall that we expect the matrix $A$ can recover with $k \approx \alpha m$ corruptions (for some $0 < \alpha < 1$).  So the second factor, which gets divided by $\sqrt{k}$ in the error estimate, is $\lesssim \epsilon$.
%}
%
%\GR{
%This is fine from a theoretical perspective.  Of course, in practice I'd always recommend using an estimate $\epsilon$ for the tail if it was available (or could be readily computed, e.g. via cross validation).  Case in point.  Suppose there were no corruptions.  Then we would expect a better reconstruction from just $\ell^1$ minimization on the coefficients than solving the modified $\ell^1$ minimization problem.  John gave an example of this effect in the numerical results he produced back in May.
%}

\subsection{Remarks on numerical results}
%\GR{We need to decide where best to introduce our main case studies.  My suggestion for this is that we look at the standard Legendre and Chebyshev polynomial cases, possibly with reweighting for the former.  However, I'm happy to discuss other options.  I've tentatively added a section later in the paper (\S 2.3.3) to discuss our main theoretical results for these cases.}
We postpone presenting numerical results until the end of this paper in Section \ref{sec:results}. However, some remarks on our findings are pertinent here in the context of the previous section's theory. First, the optimal value of $\lambda^2 = s/k$ that is suggested by \eqref{eq:delta_intro} does not appear to be the computationally optimal value of $\lambda$. \edits{That this fixed value of $\lambda$ is not the best is not surprising since the bounds \eqref{eq:recovery-summary} are derived using some loose inequalities. However, such bounds can be useful in understanding qualitative trends.} Results from our experimentation do suggest that large values of $\lambda$ more reliably recover corruptions when $s/k$ is large (see Figures \ref{f:model1} and \ref{f:model2}). This general trend in numerical results is consistent with the behavior of $\eta$ in \eqref{eq:delta_intro} as a function of $\lambda$ when $s/k$ is large.

%To ameliorate the uncertainty introduced by this discrepancy between our theoretically-motivated $\lambda$ and the actual value in practice, 
We address this discrepancy between the theory and empirical results by propose an iteratively reweighted $\ell^1$ optimization scheme (see \cite{CandesWakinBoydReweighted}) that learns and updates the value of $\lambda$. Our results show that this proposed algorithm performs much better in practice than algorithms that fix $\lambda$. \edits{However, we do not present any theory to support the observed superiority of reweighted $\ell^1$ optimization schemes for the corruptions problem.}

Many of our capstone numerical examples are from applications using polynomial chaos expansions, where the compressible function has an expansion in a multivariate orthogonal polynomial basis. To simplify the presentation of our results, we focus on such examples where the basis is a tensor-product Legendre polynomial or Chebyshev polynomial system. Much recent work has shown that randomly generating measurements using samples from standard distributions (e.g., the uniform distribution) can accurately and near-optimally recover orthogonal polynomial expansions from such basis sets \cite{Rauhut,JakemanEtAl_l1Enhance,YanGuoXui_l1UQ}. Recovery in more general polynomial spaces has been investigated \cite{HamptonDoostanCSPCE,jakeman_generalized_2016,GuoEtAlRandomizedQuad}, but these methods usually rely on sophisticated sampling strategies and optimal sampling schemes are still an active area of research.
