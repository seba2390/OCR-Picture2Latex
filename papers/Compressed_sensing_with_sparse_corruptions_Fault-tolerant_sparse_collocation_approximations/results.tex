\section{Numerical experiments}\label{sec:results}

We divide our numerical results into two main sections. The goal of Section \ref{sec:results-algorithm} is to study the behavior of numerical algorithms in the context of the theoretical estimates presented earlier. In particular, we investigate the influence that the regularization parameter $\lambda$ has on recovery properties. We confine these investigations to problems with manufactured sparsity so that systematic studies may be carried out. The lessons learned from these studies allow us to formulate and propose an iteratively reweighted alternative to the one-time optimization \eqref{l1_lambda_recovery}. \edits{Note that none of our theoretical error estimates apply to algorithms with weighted norms. However, weighted $\ell^1$ schemes can provide empirically superior results, e.g., \cite{KarniadakisUQCS}. Thus, we explore weighted algorithms because their use is natural from a practical point of view, but is not in the scope of our theoretical analysis.} Our simulations in this section use the SPGL1 package \cite{spgl1:2007,BergFriedlander:2008}.

The second collection of results, Section \ref{sec:results-pce}, focuses on more practical scenarios in scientific computing, dealing with recovery of sparse or compressible polynomial Chaos expansions of solutions to parameterized differential equations. Here we use the algorithmic lessons learned from Section \ref{sec:results-algorithm} to illustrate the efficacy and fault-tolerance of our approaches on realistic problems in the presence of measurement corruptions.

\subsection{Recovery of manufactured solutions with sparse corruptions}\label{sec:results-algorithm}

This section is primarily concerned with the generation of phase recovery diagrams for the sparse corruptions problem. \edits{In particular, our tests here are not necessarily motivated by sensing matrices and corruptions from function approximation, but instead are designed to understand behavior of the algorithms.} The following standard experiment for accomplishing this is carried out: We fix the number of measurements $m$ and the dictionary size $N$, and we vary the signal sparsity $s$ and the number of measurement corruptions $k$. For each $s$ and $k$ we generate an $s$-sparse signal $x$, and for a given model of a measurement matrix $A$, we generate $m$ measurements $y$ from the signal $x$, and subsequently corrupt (highly pollute) these measurements with a $k$-sparse vector $c$, whose non-zero entries are $C Z$, where $Z$ is a random draw from a certain probability distribution and $C > 0$ is a scaling constant. In this test, $Z$ is a standard normal random variable and $C = 1$.

We then run the recovery algorithm \eqref{l1_lambda_recovery} for a given value of $\lambda$, producing a recovered signal $\widehat{x}$ and measurement corruption vector $\widehat{c}$. We define the recovery as successful if $\left\| x - \widehat{x} \right\|^2 + \left\| c - \widehat{c} \right\|^2 < \epsilon_{\mathrm{tol}}$. In this test, we set the success tolerance to be $\epsilon_{\mathrm{tol}} = 10^{-4}$.

In the test above, the generation of $x$, and of $y$, and of $c$, are statistically independent\footnote{Measurement corruptions are generated as iid standard normal random variables, and support indices in a sparse vector are generated using the uniform probability law (draws without replacement) on the index set.}. For each $s$ and $k$, the above procedure is run $T \in \bbN$ times with independent draws, and an empirical estimate of the probability of ``success" is computed. In the phase transitions plots below, we use $T = 10$ simulations.

The phase transitions color each pixel, corresponding to a particular value of $s$ and $k$, according to the empirical success probability. The phase transition axes are $s/m$ and $k/m$, and thus each ranges in the interval $[0, 1]$, but we truncate to $[0, 0.5]$ in our plots because this region is sufficient to illustrate behavior.  We consider the following two models of measurement matrix $A$:
\bull{
\item Model 1: a Gaussian random matrix
\item Model 2: a randomly-subsampled Discrete Fourier Transform (DFT) matrix
}
Note that Model 2 is an example of a bounded orthonormal system.  We compare several different choices of $\lambda$ for each model.

%\subsubsection{Weighted $\ell_1$-minimization}
%We wish to solve problems of the form
%\be{\| Wx \|_1\quad\text{subject to}\quad\| Ax-y\|_2\le\epsilon}
%To avoid allow us to use standard un-weighted $\ell_1$-minimization
%software to solve this problem we set $\hat{x}=Wx$ and solve
%\be{\| \hat{x} \|_1\quad\text{subject to}\quad\|
%  B\hat{x}-y\|_2\le\epsilon}
%where $B=AW^{-1}$.  \GR{This is not actually what we do.  Instead, SPGL1 allows us to pass a vector weights as an option which it uses during the optimization procedure.  Solving the above problem with $B = A W^{-1}$ we found to be a bad approach usually, since it blows up the condition number of $B$ which usually means that SPGL1 will require a large number of iterations.  I suggest we just remove this section.  We should perhaps just mention that we use SPGL1 here (with a maximum number of iterations equal to 10,000 but otherwise all default tolerances).}

\subsubsection{Phase transition plots for fixed $\lambda$}

Figures \ref{f:model1} and \ref{f:model2} display the results for models 1 and 2 described above, respectively. Each figure shows an array of plots; the columns correspond to differing values of $m$, increasing from left to right; the rows correspond to differing values of $\lambda$, increasing from top to bottom, except the last two rows, which show the ``optimal" value of $\lambda = \sqrt{s/k}$ suggested by the theory, and the iterative reweighting procedure described in the next section. %Each figure shows four transitions plots: the top-left, top-right, and bottom-left are associated with fixed values of $\lambda$: $\lambda = 1$, $\lambda = \sqrt{s/k}$, and $\lambda = 1/\sqrt{\log(N/m) + 1}$, respectively.

Comparing the results for $\lambda = \sqrt{s/k}$ (row 5 in the plots) with the other plots with $\lambda$ fixed, we see that $\lambda = \sqrt{s/k}$ does not behave optimally in practice, even though this is suggested by our theory. Indeed, further experimentation reveals that the behavior of these transition plots changes notably when $m$ is varied. However, the following observations are consistent across all our runs:
\begin{itemize}
  \item When there are few corruptions relative to the signal sparsity ($k \ll s$), larger values of $\lambda$ tend to perform better. This general trend is consistent with the theory from previous sections: Our recovery results are stated in terms of a quantity $\eta$ defined in \eqref{eta_def}, and when $k \ll s$, we require large $\lambda$ to make $\eta$ small.
  \item When there are many corruptions relative to signal sparsity ($k \sim s$), smaller values of $\lambda$ tend to perform better. Again, this is consistent with the theory in terms of the parameter $\eta$.
\end{itemize}

%\GR{Argh, I've just realized the notational inconsistency here between $s_x,s_f$ and $s,k$.  I can re-run the numerical experiments, but the Gaussian ones will take some time to complete.  I doubt I'll get those done before CSE.  May I therefore recommend we present results only for model 2 for now?  I will send you the updated figures (which also correct a variety of typos and issues in the existing figures).}

%Figures \ref{f:model1} and \ref{f:model2} displays the results for models 1 and 2 respectively.  Interestingly, $\lambda=\sqrt{s_x/s_f}$ does not behave as well one might expect from the theory.  However, we caution that this only looks at one value of $m$.  Changing $m$ may affect the results.

%\begin{figure}
%\begin{center}
%  \resizebox{0.8\textwidth}{!}{
%    \begin{tabular}{cc}
%      \includegraphics[width=6.5cm]{Diagrams/PT_model1_m80_N256_lambda1.eps}&
%      \includegraphics[width=6.5cm]{Diagrams/PT_model1_m80_N256_lambda2.eps}\\
%      $\lambda = 1$ & $\lambda = \sqrt{s/k}$
%      \\
%      \includegraphics[width=6.5cm]{Diagrams/PT_model1_m80_N256_lambda3.eps}&
%      \includegraphics[width=6.5cm]{Diagrams/PT_model1_m80_N256_IterativeReweight}
%       \\
%      $\lambda = 1/\sqrt{\log(N/m)+1}$ & iteratively reweighted $\ell^1$
%    \end{tabular}
%  }
%  \caption{Phase transition for model $1$ with $m=80$ and $N=256$. Each grid point indicates a successful signal recovery probability for $T=10$ trials based on repeated random draws of $x$ and $c$. Red line are phase lines obtained from collecting the first point in each column where its probability is less than $p=0.8$, and finding a best fit for them using a polynomial with degree $3$.  } \label{f:model1}
%\end{center}
%\end{figure}

\begin{figure}
\begin{center}
  \resizebox{!}{0.43\textheight}{
    \begin{tabular}{ccc}
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m42_N256_T10_lambda1.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m84_N256_T10_lambda1.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m126_N256_T10_lambda1.eps}\\
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m42_N256_T10_lambda2.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m84_N256_T10_lambda2.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m126_N256_T10_lambda2.eps}\\
      %%\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m42_N256_T10_lambda3.eps}&
      %%\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m84_N256_T10_lambda3.eps}&
      %%\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m126_N256_T10_lambda3.eps}\\
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m42_N256_T10_lambda4.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m84_N256_T10_lambda4.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m126_N256_T10_lambda4.eps}\\
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m42_N256_T10_lambda5.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m84_N256_T10_lambda5.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m126_N256_T10_lambda5.eps}\\
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_optimallambda_m42_N256_T10.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_optimallambda_m84_N256_T10.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_optimallambda_m126_N256_T10.eps}\\
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_iterativelambda_m42_N256_T10.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_iterativelambda_m84_N256_T10.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_iterativelambda_m126_N256_T10.eps}

      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m42_N256_T10_lambda1-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m84_N256_T10_lambda1-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m126_N256_T10_lambda1-eps-converted-to.pdf}\\
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m42_N256_T10_lambda2-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m84_N256_T10_lambda2-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m126_N256_T10_lambda2-eps-converted-to.pdf}\\
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m42_N256_T10_lambda3.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m84_N256_T10_lambda3.eps}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m126_N256_T10_lambda3.eps}\\
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m42_N256_T10_lambda4-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m84_N256_T10_lambda4-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m126_N256_T10_lambda4-eps-converted-to.pdf}\\
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m42_N256_T10_lambda5-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m84_N256_T10_lambda5-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_fixedlambda_m126_N256_T10_lambda5-eps-converted-to.pdf}\\
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_optimallambda_m42_N256_T10-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_optimallambda_m84_N256_T10-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_optimallambda_m126_N256_T10-eps-converted-to.pdf}\\
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_iterativelambda_m42_N256_T10-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_iterativelambda_m84_N256_T10-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/gauss_pt/Gauss_PT_iterativelambda_m126_N256_T10-eps-converted-to.pdf}
    \end{tabular}
  }
  \caption{Phase transition for model $1$ with fixed $N = 256$, varying $m$ and $\lambda$. Each column represents varying values of $m$: from left to right, $m=42$, $m=84$, and $m=126$. Each row represents different values of $\lambda$: rows 1-4 correspond to $\lambda = 0.5, 1, 2, 3$, respectively. Row 5 uses the value $\lambda = \sqrt{s/k}$ that is suggested as optimal by the theory. Row 6 shows recovery using the iteratively reweighted $\ell^1$ algorithm. Each pixel is colored according to its probability of a successful signal recovery for $T=10$ trials based on repeated random draws of $x$ and $c$; yellow is probability 1, blue is probability 0. %Red line are phase lines obtained from collecting the first point in each column where its probability is less than $p=0.8$, and finding a best fit for them using a polynomial with degree $3$.  
  } \label{f:model1}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
  \resizebox{!}{0.43\textheight}{
    \begin{tabular}{ccc}
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m42_N256_T10_lambda1-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m84_N256_T10_lambda1-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m126_N256_T10_lambda1-eps-converted-to.pdf}\\
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m42_N256_T10_lambda2-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m84_N256_T10_lambda2-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m126_N256_T10_lambda2-eps-converted-to.pdf}\\
      %\includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m42_N256_T10_lambda3-eps-converted-to.pdf}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m84_N256_T10_lambda3-eps-converted-to.pdf}&
      %\includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m126_N256_T10_lambda3-eps-converted-to.pdf}\\
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m42_N256_T10_lambda4-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m84_N256_T10_lambda4-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m126_N256_T10_lambda4-eps-converted-to.pdf}\\
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m42_N256_T10_lambda5-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m84_N256_T10_lambda5-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_fixedlambda_m126_N256_T10_lambda5-eps-converted-to.pdf}\\
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_optimallambda_m42_N256_T10-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_optimallambda_m84_N256_T10-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_optimallambda_m126_N256_T10-eps-converted-to.pdf}\\
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_iterativelambda_m42_N256_T10-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_iterativelambda_m84_N256_T10-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/dft_pt/DFT_PT_iterativelambda_m126_N256_T10-eps-converted-to.pdf}
    \end{tabular}
  }
  \caption{Phase transition for model $2$ with fixed $N = 256$, varying $m$ and $\lambda$. Each column represents varying values of $m$: from left to right, $m=42$, $m=84$, and $m=126$. Each row represents different values of $\lambda$: rows 1-4 correspond to $\lambda = 0.5, 1, 2, 3$, respectively. Row 5 uses the value $\lambda = \sqrt{s/k}$ that is suggested as optimal by the theory. Row 6 shows recovery using the iteratively reweighted $\ell^1$ algorithm. Each pixel is colored according to its probability of a successful signal recovery for $T=10$ trials based on repeated random draws of $x$ and $c$; yellow is probability 1, blue is probability 0. %Red line are phase lines obtained from collecting the first point in each column where its probability is less than $p=0.8$, and finding a best fit for them using a polynomial with degree $3$.  
  } \label{f:model2}
\end{center}
\end{figure}


%\begin{figure}
%\begin{center}
%  \resizebox{0.8\textwidth}{!}{
%    \begin{tabular}{cc}
%      \includegraphics[width=6.5cm]{Diagrams/PT_model2_m80_N256_lambda1.eps}&
%      \includegraphics[width=6.5cm]{Diagrams/PT_model2_m80_N256_lambda2.eps}\\
%      $\lambda = 1$ & $\lambda = \sqrt{s/k}$
%      \\
%      \includegraphics[width=6.5cm]{Diagrams/PT_model2_m80_N256_lambda3.eps}&
%      \includegraphics[width=6.5cm]{Diagrams/PT_model2_m80_N256_IterativeReweight}
%      \\
%      $\lambda = 1/\sqrt{\log(N)}$ & iteratively reweighted $\ell^1$
%    \end{tabular}
%  }
%  \caption{Phase transition for model $2$ with $m=80$ and $N=256$. Each grid point indicates a successful signal recovery probability for $T=10$ trials based on repeated random draws of $x$ and $c$. Red line are phase lines obtained from collecting the first point in each column where its probability is less than $p=0.8$, and finding a best fit for them using a polynomial with degree $3$. }\label{f:model2}
%\end{center}
%\end{figure}

\subsubsection{Iteratively reweighted $\ell^1$ minimization}\label{sec:iteratively-reweighted}
The results from the previous section show that our \textit{a priori} postulated optimal values of $\lambda$ are not optimal in practice; this suggests that an adaptive learning of $\lambda$ may produce better results. See, for example, \cite{CandesWakinBoydReweighted}. This section introduces an iteratively reweighted $\ell^1$ optimization procedure that effects this learning of $\lambda$.

We compute minimizers $\hat{x}$ and $\hat{c}$ using an initial value of $\lambda$. We then update $\lambda$ based on $\hat{x}$ and $\hat{c}$, and then recompute minimizers with the new $\lambda$. Such an approach not only allows for a single parameter $\lambda$ to be updated, it also permits individual (i.e. non-equal) weights to be used for term in the regularization functional.  This aims to enhance recovery performance by both iteratively estimating an optimal weighting $\lambda$ between the coefficients and corruptions term, and iteratively estimating the support sets of $x$ and $c$.

We outline the procedure below:
%Algorithm for iteratively reweighted $\ell^1$ minimization.
\begin{itemize}
\item Step 1.\ Set $r=1$, $\mu_i=1$ for $i=1, \ldots, N$, and $\lambda_j = 1$ for $j=1,\ldots,m$. Prescribe noise tolerance $\epsilon$ and a small positive number $\eta > 0$.
\item Step 2.\ Compute the solution $(\widehat{x}, \widehat{c})$ to
\bes{
\min_{z \in \bbC^N, d \in \bbC^m} \| z \|_{1,\mu} + \| d \|_{1,\lambda}\ \mbox{subject to $\| A z + d - y \|_{2} \leq \epsilon$},
}
where $\nm{z}_{1,\mu} = \sum^{N}_{i=1} \mu_i | z_i|$ and $\nm{d}_{1,\lambda} = \sum^{m}_{j=1} \lambda_j |d_j|$.
\item Step 3.\ Update $\mu$ and $\lambda$ as follows:
  \begin{align}\label{eq:weights}
    \mu_{i} &= \frac{1}{\eta + |\hat{x}_i|},& \lambda_i &= \frac{1}{\eta + |\hat{c}_i|}.
  \end{align}
\item Step 4.\ If $r< r_{\max}$, set $r = r+1$ and go back to step 2, otherwise stop.
\end{itemize}
Numerical results in the bottom row of plots in Figures \ref{f:model1} and \ref{f:model2} show this approach (implemented with $r_{\max} = 10$ iterations) significantly improves the recovery over a fixed choice of $\lambda$. We therefore use this iteratively reweighted $\ell^1$ approach for optimization for all our simulations in the next section.

\anrev{
\subsubsection{Large corruption values}\label{sec:large-corruptions}
This section is devoted to understanding the behavior of our algorithm with respect to the magnitude of the corruptions. %In terms of algebraic quantities, we are interested in behavior when the signal magnitude $\|x\|$ and corruptions magnitude $\|c\|$ have substantially different magnitude. The case of interest is $\|c \| \gg \| x\|$.

We run the same experiment as outlined at the beginning of Section \ref{sec:results-algorithm} on Model 2 (the measurement matrix is a subsampled DFT matrix) using the iteratively reweighted algorithm outlined in Section \ref{sec:iteratively-reweighted}. For this test, we vary $C$ between $1$ and $10^6$, and choose the random variable $Z$ defining the corruptions as a standard Cauchy random variable.\footnote{The point of generating from a Cauchy distribution is to show that measurement corruption by heavy-tailed distributions does not adversely affect the algorithm's results.}

A straightforward application of the iteratively reweighted algorithm in Section \ref{sec:iteratively-reweighted} when $C$ is very large produces suboptimal results. The reason for this is the scale differential between $x$ and $c$, so that the algorithm heavily favors recovery of the corruptions and devotes little effort to recovering the signal. To overcome this limitation, we leverage a significant advantage of our algorithm: Corruption indices and values in the measurement vector are identified. This allows us to formulate a slight modification of the algorithm in Section \ref{sec:iteratively-reweighted}:
\begin{enumerate}
  \item Run the algorithm from Section \ref{sec:iteratively-reweighted}, generating computed solutions $\widehat{x}$ and $\widehat{c}$.
  \item If $\| \widehat{c} \| < C_{\mathrm{max}} \| y - \widehat{c} \|$, then return the solutions $\widehat{x}$ and $\widehat{c}$.
  \item If instead $\| \widehat{c} \| \geq C_{\mathrm{max}} \| \widehat{y} - \widehat{c} \|$, then define a support set for the vector $\widehat{c}$ as
    \begin{align*}
      S = \left\{ j=1, \ldots, m\;\; | \;\; \widehat{c}_j \geq \tau \left\| \widehat{c}\right\| \right\},
    \end{align*}
    and let $\widehat{c}_S$ equal to $\widehat{c}$ on $S$ and zero otherwise.
  \item Remove the large corruptions from the measurements and resolve with the measurements $\widetilde{y} \gets y - \widehat{c}_S$. This yields a new solution pair $\widetilde{x}$ and $\widetilde{c}$. Return $x = \widetilde{x}$ and $c = \widehat{c}_S + \widetilde{c}$.
\end{enumerate}
This procedure uses the algorithm to identify and remove highly corrupted measurements, and then uses another instance of the algorithm to accurately compute the signal. We use the procedure above with the choices $C_{\mathrm{max}} = 10$ and $\tau = \frac{1}{5 \sqrt{m}}$. 

We can now generate a phase transition plot for a fixed value of $C$. Figure \ref{fig:pt-corruptions-magnitude} shows the transition plots for values $C = 1, 10^3$, and $10^6$. We see that the algorithm detects and removes corruptions just as well when $C = 1$ as when $C = 10^6$.

\begin{figure}
\begin{center}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{ccc}
      \includegraphics[width=0.33\textwidth]{Diagrams/corruptions-magnitude/model2-m75-N256-T10-logC0-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/corruptions-magnitude/model2-m75-N256-T10-logC3-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/corruptions-magnitude/model2-m75-N256-T10-logC6-eps-converted-to.pdf}
    \end{tabular}
  }
  \caption{Phase transition for model $2$ with fixed $N = 256$ and $m= 75$, varying the corruptions magnitude $C$. (Left: $C = 1$. Middle: $C = 10^3$. Right: $C = 10^6$.) Each transition plot uses the iteratively reweighted algorithm outlined in Sections \ref{sec:iteratively-reweighted} with the augmentations described in Section \ref{sec:large-corruptions}. The recovery property of the corruptions algorithm is relatively agnostic to magnitude of the corruptions.}\label{fig:pt-corruptions-magnitude}
\end{center}
\end{figure}

\edits{
  \begin{remark}\label{rem:weights}
    The iteratively reweighted procedure in \eqref{eq:weights} updates weights for both the corruptions ($\lambda_i$) and the signal ($\mu_i$). Since our focus here is recovery of the corruptions, one may wonder which set of weights is more influential. We have conducted tests in this direction by performing an experiment parallel to the results in Figure \ref{fig:pt-corruptions-magnitude}, where we iteratively update $\lambda_i$ according to \eqref{eq:weights}, but keep $\mu_i$ fixed at unity for all $i$. Our results, shown in Figure \ref{fig:reweighted-fixedmu}, indicate that fixing the weights $\mu_i$ results in significant deterioration of the algorithm's performance when $C=1$. However, it results in notable improvement of the algorithm when $C = 10^3$ or $C = 10^6$. In the context of soft faults, the $C = 1$ behavior of the algorithm is more relevant since when $C \geq 10^3$ it is likely that the corruptions can be easily identified and removed by other means. In this small-$C$ context, allowing both sets of weights $\mu_i$ and $\lambda_i$ to vary appears to be beneficial. On the other hand, the deterioration of the algorithm for very large $C$ is an interesting phenomenon whose investigation we leave for future work.
\end{remark}

\begin{figure}
\begin{center}
  \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{ccc}
      %\includegraphics[width=0.33\textwidth]{figures/model2-m80-N256-T10-logC0.eps}&
      %\includegraphics[width=0.33\textwidth]{figures/model2-m80-N256-T10-logC3.eps}&
      %\includegraphics[width=0.33\textwidth]{figures/model2-m80-N256-T10-logC6.eps}\\
      \includegraphics[width=0.33\textwidth]{Diagrams/corruptions-magnitude/model2-m80-N256-T10-logC0-fixedmu-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/corruptions-magnitude/model2-m80-N256-T10-logC3-fixedmu-eps-converted-to.pdf}&
      \includegraphics[width=0.33\textwidth]{Diagrams/corruptions-magnitude/model2-m80-N256-T10-logC6-fixedmu-eps-converted-to.pdf}
    \end{tabular}
  }
  \caption{Diagram complementary to Figure 3, here using fixed signal weights $\mu_i = 1$ but varying the corruptions weights $\lambda_i$ in the iteratively reweighted algorithm described in Sections \ref{sec:iteratively-reweighted} and \ref{sec:large-corruptions}. Phase transition for model $2$ with fixed $N = 256$ and $m= 80$, varying the corruptions magnitude $C$. (Left: $C = 1$. Middle: $C = 10^3$. Right: $C = 10^6$.) The results indicate empirical superiority of the algorithm in Section \ref{sec:iteratively-reweighted} that allows both $\mu_i$ and $\lambda_i$ to vary, compared with fixing $\mu_i$.}\label{fig:reweighted-fixedmu}
\end{center}
\end{figure}
}

%We remark in passing that the practical challenge lies when the corruptions magnitude $C$ is relatively small compared to the signal magnitude. Very large values of $C$ result in measurement values that a practitioner would immediately recognize as unusual and could mark for removal. The strength of our algorithm is that it can identify corrupted measurements that are not apparent to a user. Nevertheless, the results of this section demonstrate that our algorithm is reasonably robust when $C$ is extremely large.


}

\subsection{Recovery of compressible polynomial Chaos expansions}\label{sec:results-pce}

In this section we test our algorithm on more realistic problems in UQ: sparse recovery of multivariate polynomial Chaos expansion coefficients with corrupted measurements. Polynomial chaos expansions (PCE)~\cite{xiu02,ghanem} have become a popular means of quantifying parametric uncertainty in expensive computer simulations. To formulate our problem using our earlier notation, let $f(\xi)$ denote a scalar-valued response of a model (e.g., a differential equation) where $\xi \in \bbR^d$ is a random parameter appearing in the model. The dependence of $y$ on $\xi$ thus encodes uncertainty in the response. We are interested in building the approximation $\xi \mapsto \sum_{n=1}^N x_n \phi_n(\xi)$, where $\left\{ \phi_n \right\}_{n=1}^N$ are computable orthonormal polynomials constructed from the probability density of the random vector $\xi$, and we wish to compute the unknown coefficients $x_n$. In a CS recovery procedure, we construct $m$ samples $\left\{ \xi_j \right\}_{j=1}^m$ of the random vector $\xi$, collect the measurements $y_j = f(\xi_j)$, and then attempt to find a sparse coefficient vector $x$ minimizing $\| y - A x\|$, where $A$ is the measurement matrix with entries $(A)_{j,n} = \phi_n(\xi_j)$. The underlying assumption is that $\xi \mapsto f(\xi)$ is expensive to evaluate so that $m$ should be as small as possible. To focus our study on the corruptions problem, we consider the case where the vector $y$ can have a sparse number of entries that are polluted by large-magnitude errors.
%The focus of our investigations here is on the corruptions problem, where the vector $y$ can have a sparse number of entries that are polluted by large-magnitude errors. As discussed in the introduction, this is essentially the problem of \textit{soft failures}.

The models $f(\xi)$ we consider here reflect the types of large scale models that are susceptible to soft failures. However, these test models can be evaluated repeatedly with almost zero probability of corruptions. Therefore, to simulate the effect of soft failures we randomly generate soft faults according to the corruptions model from Section \ref{sec:results-algorithm}. After constructing components of $y$ as $f(\xi)$, we pollute $k$ of these entries as described at the beginning of Section \ref{sec:results-algorithm}. In our tests below we fix a value $r \coloneqq k/m$, the ratio of corrupted measurements. 

%Specifically we first evaluate the models at $m$ samples $\xi_i$ for $i=1, \ldots, m$, and with a fixed ratio $0 < r < 1$, we randomly select $r m$ samples for pollution as
%\begin{align*}
%y(\xi_j)=e_j,\quad \forall\; j \textrm{ in $r m$ randomly selected indices from } \left\{1, \ldots, m \right\}%\V{z}\in \V{Z}_f, \quad e \sim N(0,\sigma_f^2)$$
%\end{align*}
%where $e_j \sim \mathcal{N}\left(0, \sigma_y^2\right)$. We set the standard deviation $\sigma_y$ of the failures to be some constant multiple $C$ of the absolute value of the mean of the function, i.e. $\sigma_y=C\mathbb{E}[y]$. Note that the model above is not the additive model for corruptions we have assumed throughout this paper for mathematical analysis, but is more representative of a realistic scenario where corruptions are unlikely to follow an additive model.

%In this section we investigate the peformance of $\ell_1$-minimization  for building polynomial chaos expansions (PCE) of the output of ordinary and partial differential equations in the presence of soft failures. Specifically we consider: (i) a elastic strain model of horizontal stresses in a media containing an inclusion of uncertain shape parametersized by two random vairables; (ii) A damped hamonic oscillator with 6 unknown parameters, including initial position and velocity; and (iii) an elliptic diffusion model subject to a random diffusivity field parameterized by a Karhunen Loeve expansion (KLE) of ten uniform variables. %Details of the three models are given in the appendex


%\subsubsection{Elliptic ODE with random inputs}
%In this section, we consider the polynomial approximation of a functional of the solution of the heterogeneous diffusion equation subject to uncertainty in the diffusivity coefficient. This problem has been used as a benchmark in other works~\cite{HamptonDoostanCSPCE,KarniadakisUQCS}.
%
%Attention is restricted to one-dimensional physical space to avoid unnecessary complexity, but the procedure described here can easily be extended to higher physical dimensions. Consider the following problem with $d \ge 1$ random parameters, where $w \in \bbR$ is the spatial variable:
%\begin{align}\label{eq:hetrogeneous-diffusion}
%  -\frac{d}{dw}\left[a(w,\xi)\frac{du}{dw}(w,\xi)\right] = 1&,\quad 
%  (w,\xi)\in(0,1)\times I_{\xi} \\\nonumber
%u(0,\xi)=0&,\quad u(1,\xi)=0.
%\end{align}
%Furthermore, assume that the random log-diffusivity satisfies
%\begin{equation}\label{eq:diffusivityZ}
%  \log(a(w,\xi))=\bar{a}+\sigma_a\sum_{j=1}^d\sqrt{\gamma_j}\varphi_j(w) \xi^{(j)},
%\end{equation} 
%where $\{\gamma_j\}_{j=1}^d$ and $\{\varphi_j(w)\}_{j=1}^d$ are, respectively, 
%the eigenvalues and eigenfunctions of the squared exponential covariance kernel 
%$$ C_a(w_1,w_2) = \exp\left[-\frac{(w_1-w_2)^2}{l_c^2}\right],$$
%and $\xi^{(j)}$ for $j=1, \ldots, d$ are the $d$ components of the vector $\xi$. The variability of the diffusivity field~\eqref{eq:diffusivityZ} is controlled by $\sigma_a$ and the correlation length $l_c$ which determines the decay of the eigenvalues $\gamma_j$. 
%
%In the following we use the iteratively reweighted $\ell^1$ algorithm from Section \ref{sec:iteratively-reweighted} to approximate the a quantity of interest $y$ defined by $y(\xi)=u(5/9,\xi)$ for dimension $d=10$ and random variables $\xi$.  We set $\bar{a}=0.1$, $l_c=1/10$ and set $\sigma_a=0.017$. The spatial solver for the model~\eqref{eq:hetrogeneous-diffusion} uses a spectral Chebyshev collocation with a high enough spatial resolution to neglect discretization errors in our analysis.

\anrev{
\subsubsection{Genz test functions}

We compare the algorithm presented in this paper against a classical $\ell^1$ minimization approach in the presence of measurement corruptions for the purposes of computing compressible PCE expansion coefficients of a function. A classical $\ell^1$ minimization algorithm sets the corruptions vector $d=0$ in \eqref{l1_lambda_recovery} and minimizes over all $x \in\bbR^N$.

Our function $f(\xi)$ will be one of the multidimensional test functions used by Genz \cite{genz_testing_1984}. For $\xi \in \bbR^d$, $d \in \bbN$, we investigate computing expansion coefficients for the following two functions on the hypercube $[-1,1]^d$:
\begin{align*}
  f(\xi) &= \exp \left[ - \frac{2}{\sqrt{d}} \sum_{j=1}^d \left(\xi_j - w_j \right)^2 \right], & w_j &= \frac{(-1)^j}{j + 1}, \hskip 10pt \text{(``Gaussian")} \\
  f(\xi) &= \prod_{j=1}^d \frac{d/4}{d/4 + \left(\xi - w_j\right)^2}, & w_j &= \frac{(-1)^j}{j + 1}, \hskip 10pt \text{(``Product Peak")}
\end{align*}
We use $d=4$ and $d=10$ in our tests, with the dictionary elements $\phi_n$ given by tensor-product Chebyshev polynomials of total degree $10$ and $4$, respectively, over $[-1,1]^d$. We set the corruptions ratio to the value $r = 0.1$ uniformly over all tests, and vary the corruptions magnitude $C$. After computing a coefficient vector $x$ solving either a classical $\ell^1$ problem or \eqref{l1_lambda_recovery}, we compute a discrete $\ell^2$ error metric defined by 
\begin{align*}
  \sqrt{\frac{1}{Q} \sum_{q=1}^Q \left(f_N(\tau_q) - f(\tau_q)\right)^2}, \hskip 15pt f_N(\xi) \coloneqq \sum_{n=1}^N x_n \phi_n(\xi)
\end{align*}
where $Q = 10^3$ for each test, and $\tau_q$ are iid samples drawn from the product Chebyshev distribution over $[-1,1]^d$.

Figure \ref{fig:genz-tests} shows the result of this test. (See the figure caption for additional details of the test.) The results indicate that when corruptions are present, a standard $\ell^1$ minimization algorithm suffers severe degradation of the quality of the computed expansion coefficients. However, the corruptions algorithm of this paper is able to compute accurate coefficients in the presence of corruptions, whether they have large or small magnitude. 

This example shows that there may be a penalty for using our algorithm when no corruptions are present. This is mostly easily noticed in the product peak example with no corruptions ($C = 0$): The corruptions algorithm of this paper computes a PCE that is less accurate than the result using a standard $\ell^1$ minimization approach. (Compare the black lines in row 3 of Figure \ref{fig:genz-tests}.)

\begin{figure}
  \begin{center}
  \resizebox{!}{0.40\textheight}{
    \begin{tabular}{cc}
      \includegraphics[width=.49\textwidth]{tikz/genz-gaussian/genz-gaussian-d4-classical.pdf}&
      \includegraphics[width=.49\textwidth]{tikz/genz-gaussian/genz-gaussian-d4.pdf}\\
      \includegraphics[width=.49\textwidth]{tikz/genz-gaussian/genz-gaussian-d10-classical.pdf}&
      \includegraphics[width=.49\textwidth]{tikz/genz-gaussian/genz-gaussian-d10.pdf}\\
      \includegraphics[width=.49\textwidth]{tikz/genz-product-peak/genz-product-peak-d4-classical.pdf}&
      \includegraphics[width=.49\textwidth]{tikz/genz-product-peak/genz-product-peak-d4.pdf}\\
      \includegraphics[width=.49\textwidth]{tikz/genz-product-peak/genz-product-peak-d10-classical.pdf}&
      \includegraphics[width=.49\textwidth]{tikz/genz-product-peak/genz-product-peak-d10.pdf}
    \end{tabular}
  }
\end{center}
\caption{Approximation of sparse representations for Genz test functions in the presence of measurement corruptions. Left: classical $\ell^1$ minimization. Right: The corruptions algorithm of this paper. The top two rows use a Genz Gaussian test function ($d=4$ and $d=10$), the bottom two rows use a Genz Product Peak test function ($d=4$ and $d=10$). 10\% of the measurements are corrupted in each test ($r = 0.1$), with varying values of the corruptions magnitude $C$. Results over a size $T=10$ ensemble are shown, with the mean error plotted with a solid curve, and shaded regions around the mean demarcated by the 20\% and 80\% quantiles.}
\label{fig:genz-tests}
\end{figure}
}

\subsubsection{Damped Harmonic Oscillator}
In this section we investigate the fault-tolerance of our algorithm for recovery of PCE coefficients in a damped linear oscillator subject to external forcing with six unknown parameters. The model is
\begin{align}\label{eq:random-oscillator}
\frac{d^2u}{dt^2}(t,\xi)+\gamma\frac{du}{dt}+k u=g\cos(\omega t),\\\nonumber
%\end{align}
%subject to the initial conditions
%\begin{equation}
u(0,\xi)=u_0(\xi),\quad \dot{u}(0,\xi)=u_1(\xi),
\end{align}
where we assume the damping coefficient $\gamma$, spring constant $k$,
forcing amplitude $g$ and frequency $\omega$, and the initial
conditions $u_0$ and $u_1$ are all uncertain, defining components of a 6-dimensional random vector $\xi$. We solve~\eqref{eq:random-oscillator} analytically to circumvent the impact of discretization errors in our study.

Defining $\xi=(\gamma,k,g,\omega,u_0,u_1)$, we restrict the components $\xi^{(j)}$ of $\xi$ to the following ranges:
\begin{align*}
  \xi^{(1)} &\in [0.08,0.12], & \xi^{(2)} &\in [0.03,0.04], & \xi^{(3)} &\in [0.08,0.12], \\
  \xi^{(4)} &\in [0.8,1.2], & \xi^{(5)} &\in [0.45,0.55], & \xi^{(6)} &\in [-0.05,0.05].
\end{align*}
%let $z_1\in[0.08,0.12]$, $z_2\in[0.03,0.04]$, $z_3\in[0.08,0.12]$, $z_4 \in [0.8,1.2]$, 
%$z_5\in[0.45,0.55]$, $z_6\in[-0.05,0.05]$. 
We define $I_\xi \in \bbR^6$ to be the range of $\xi$ defined by the product of these intervals. For any parameter realization in $I_\xi$ the harmonic oscillator is underdamped. In the following, we choose our quantity of interest as $f(\xi) = u(20,\xi)$. We set the corruptions magnitude $C$ as the mean of the function, i.e. $C = \mathbb{E}_\xi[f]$.

Figure \ref{fig:oscillator-approximation-errors-hard-comparison} compares, as a function of the number of measurements, the error in classical $\ell^1$ recovery for uncorrupted sparse recovery versus the iteratively reweighted version of the sparse corruptions $\ell^1$ optimization proposed in Section \ref{sec:iteratively-reweighted}. The results show that the sparse corruptions optimization notably outperforms standard $\ell^1$ minimization when corruptions are present, and is competitive without corruptions.
%\BLUE{
%Umm, it seems to me like the standard and reweighted algorithms give very similar performance.  Or an I missing something here?  Also, I'm not sure it's reasonable to conclude in the absence of corruptions that the reweighted algorithm `notably outperforms' standard $\ell^1$ minimization.  The differences look quite small.
%}


In Figure \ref{fig:oscillator-approximation-errors-rate-mag-comparison} we run the iteratively reweighted sparse corruptions optimization  but vary the corruptions rate $r$, and the corruptions magnitude $C$. The left-hand plot shows predictable behavior: increasing corruptions has deleterious effects on the error in recovery, but notably the algorithm is reasonably stable for increasing $r$. The right-hand plot shows that the algorithm is relatively insensitive to the magnitude of the corruptions.


\begin{figure}[ht]
\centering
\includegraphics[width=.39\linewidth]{tikz/damped-oscillator-d-6-p-7-errors-hard-comparison-standalone.pdf}
\includegraphics[width=.39\linewidth]{tikz/damped-oscillator-d-6-p-7-no-error-comparison-standalone.pdf}
\caption{Comparison of iteratively reweighted $\ell_1$-minimization with classical $\ell_1$-minimization ($\lambda=0$) when constructing a PCE of the $d=6$ harmonic oscillator in the presence of (left) corrupted data with $r=0.1$ and $C=1$ and (right) no failures.}
\label{fig:oscillator-approximation-errors-hard-comparison}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=.39\linewidth]{tikz/damped-oscillator-d-6-p-7-failure-rate-comparison-standalone.pdf}
\includegraphics[width=.39\linewidth]{tikz/damped-oscillator-d-6-p-7-failure-magnitude-comparison-standalone.pdf}
\caption{Effect of the corruption rate $r$ (left) and magnitude $C$ of corruption errors (right) on the PCE  of the $d=6$ harmonic oscillator constructed in the presence of failures using $\ell_1$-minimization with various choices of $\lambda$. To generate the left and rights plot we set $C=1$ and $r=0.1$, respectively.}
\label{fig:oscillator-approximation-errors-rate-mag-comparison}
\end{figure}

%\subsubsection{A parameterized inclusion}
%In this section, we consider a simple problem in computational mechanics where the we wish to estimate stresses in a media containing an inclusion, when the precise boundary of an inclusion is uncertain.
%We use a linear elastic formulation to model the response of the media to surface forces and measure horizontal stress at each sensor location.
%We assume that the material properties (Poisson ratio and Young's modulus) are different inside the inclusion and that these properties are known a prior.
%
%To simulate horizontal stresses we use the following linear elastic plane strain model,
%\begin{equation}\label{eq:linelast}
%\begin{cases}
%-\nabla \cdot \mathbf{\sigma(u)} = \mathbf{0}, & x\in \Omega = [-5,5]\times[0,2],\\
%\mathbf{u} = \mathbf{g}, & x\in \Gamma_D = \left\{(x,y)\in \Omega \ | \ x=0\right\},\\
%\mathbf{\sigma(u)} \mathbf{n} = \mathbf{t}, & x\in \Gamma_N = \partial \Omega \backslash \Gamma_D,
%\end{cases}
%\end{equation}
%%where $\Omega = [-5,5]\times[0,2]$ and $\partial \Omega = \Gamma_D \cap \Gamma_N$ and
%where $\mathbf{\sigma(u)}$ is given by the linear elastic
%constitutive relation,
%\[\mathbf{\sigma(u)} = \lambda (\nabla \cdot \mathbf{u})\mathbb{I} + \mu (\nabla \mathbf{u} +\nabla \mathbf{u}^T).\]
%We express this relation in terms of the Lam\'{e} parameters, $\lambda$ and $\mu$, which are related to
%the Poisson ratio, $\nu$, and Young's modulus, $E$, via the following expressions,
%\[ \mu = \frac{E}{2(1+\nu)}, \quad \lambda = \frac{E \nu}{(1+\nu)(1-2\nu)}.\]
%Now assume that there is an inclusion within the media defined by an ellipse
%\[ {\cal I} = \left\{(x,y)\in \Omega \ | \ \frac{1}{\alpha}(x-x_0)^2 + \frac{1}{\beta}(y-y_0)^2 \leq 1 \right\},\]
%where $x_0=y_0=0$ and $\alpha$ is uniformly distributed on $[0.5,1]$ and $\beta$ is uniformly distributed on $[0.25,0.5]$.
%The material properties are assumed to be known and are given by,
%\[\nu = \begin{cases}
%0.45, & (x,y)\in {\cal I},\\
%0.3, & \text{otherwise},
%\end{cases}, \quad
%E = \begin{cases}
%10.0, & (x,y)\in {\cal I},\\
%40.0, & \text{otherwise},
%\end{cases}.
%\]
%These material properties were not chosen to emulate any particular materials, just to demonstrate our proposed method.
%\begin{figure}
%\centering
%\scalebox{0.3}{\includegraphics[trim={1cm 4cm 2cm 5cm},clip]{tikz/oed_inclusion_poisson_ratio.jpg}}
%\caption{The computational domain and Poisson ratio showing the inclusion for a particular realization of the ellipsoid parameters.}
%\label{fig:inclusion_domain}
%\end{figure}
%
%Next let us impose homogeneous Dirichlet boundary conditions on the
%bottom boundary and stress free boundary conditions on the sides, and
%impose a uniform traction in the y-direction along the top boundary
%($\mathbf{t}_{\text{top}} = (0,-1)^T$).
%Equation~\ref{eq:linelast} was solved using a finite element discretization with piecewise linear basis functions defined on a uniform $400\times 80$ mesh resulting in a system with 64,962 degrees of freedom.
%The computational model is implemented using the Trilinos toolkit \cite{Trilinos-Overview} and each realization of the model requires approximately 1 second using 8 processors.
%
%Figures~\ref{}--\ref{} compare the performance of classical unweighted $\ell_1$ minimization with our proposed method for constructing Legendre Polynomial Chaos Expansions of degree 15 of the horizontal stress at the sensor location $(2.1,1.1)$ in the presence of corruptions. 
%Specifically Figure~\ref{} depicts the convergence of the two approaches for approximating the model response when the rate of corruptions is $10\%$ and the mag

%%% Local Variables:
%%% mode: latex
%%% TeX-master: CSCorruptionsv1
%%% End:
