\section{Introduction} \label{Introduction}



Over\blankfootnote{For a published version of this paper refer to Quantitative Evaluation of Systems: 18th International Conference, \url{https://doi.org/10.1007/978-3-030-85172-9_9}.} the past decades, there has been significant study on the scheduling of jobs in single-server queues. When preemption is allowed and processing times are known to the scheduler, the Shortest Remaining Processing Time (SRPT) policy is optimal in the sense that, regardless of the processing time distribution, it minimizes the number of jobs in the system at each point in time and hence, minimizes the mean sojourn time (MST) \cite{schrage1968letter}, \cite{schrage1966queue}. 
However, scheduling policies such as SRPT are rarely deployed in practical settings. A key disadvantage is that the assumption of knowing the exact job processing times prior to scheduling is not always practical to make. However, it is often possible to estimate the job processing times and use this approximate information for scheduling. The Shortest Estimated Remaining Processing Time (SERPT) policy is a version of SRPT that employs the job processing time estimates as if they were error-free and thus, schedules jobs based on their estimated remaining times. Motivated by the fact that estimates can often be obtained through machine learning techniques, Mitzenmacher \cite{mitzenmacher2019scheduling} studies the potential benefits of using such estimates for simple scheduling policies. For this purpose, a price for misprediction, the ratio between a job’s expected sojourn time using its estimated processing time and the job’s expected sojourn time when the job processing time is known is introduced, and a bound on this price is given. The results in \cite{mitzenmacher2019scheduling} suggest that naïve policies work well, and even a weak predictor can yield significant improvements under policies such as SERPT. However, this insight is only made when the job processing times have relatively low variance. As discussed below, when job processing times have high variance, underestimating even a single very large job can severely affect the smaller jobs' sojourn times. 

The work in \cite{mitzenmacher2019scheduling} has the optimistic viewpoint that it is possible to obtain improved performance by utilizing processing time estimates in a simple manner. The more pessimistic view is that when job processing times are estimated, estimation errors naturally arise, and they can degrade a scheduling policy's performance, if the policy was designed to exploit exact knowledge of job processing times \cite{lu2004size}. The SERPT policy may have poor performance when the job processing times have high variance and large jobs are underestimated. Consider a situation where a job with a processing time of 1000 enters the system and is underestimated by 10\%. The moment the job has been processed for 900 units (its estimated processing time), the server assumes that this job's estimated remaining processing time is zero, and until it completes, the job will block the jobs already in the queue as well as any new arrivals. This situation becomes more severe when both the actual job processing time and the level of underestimation increase. However, when the job processing times are generated from lower variance distributions, the underestimation of large jobs will not cause severe performance degradation \cite{mailach2017robustness}.


The Shortest Estimated Processing Time (SEPT) policy is a version of the Shortest Processing Time (SPT) policy that skips updating the estimated remaining processing times and prioritizes jobs based only on their estimated processing times. Experimental results show that SEPT has impressive performance in the presence of estimated job processing times, as well as being easier to implement than SERPT \cite{dell2019scheduling}. 

In this paper, we will discuss the problem of single-server scheduling when only estimates of the job processing times are available. In Section \ref{LiteratureReview}, we discuss the existing literature for scheduling policies that handle inexact job processing time information. Most of the existing literature analyzes and introduces size-based policies when the estimation error is relatively small, restricting applicability of the results. Furthermore, many simulation-based examinations only consider certain workload classes and are not validated over a range of job processing times and estimation error distributions. We propose a scheduling policy that exhibits desirable performance over a wide range of job processing time distributions, estimation error distributions, and workloads. 



The Gittins' Index policy \cite {gittins1979bandit}, a dynamic priority-based policy, is optimal in minimizing the MST in an M/G/1 queue \cite{aalto2009gittins}. When there are job processing time estimates, the Gittins' Index policy utilizes information about job estimated processing time, and the job processing time and estimation error distributions to decide which job should be processed next. The assumption of knowing these distributions before scheduling may be problematic in real environments. Furthermore, scheduling jobs using the Gittins' Index policy introduces computational overhead that may be prohibitive. While there are significant barriers to implementing the Gittins' Index policy, our proposed policy is motivated by the form of the Gittins' Index policy.

We make the following contributions: While the SEPT policy performs well in the presence of estimated job processing times \cite {dell2019scheduling}, we first introduce a heuristic that combines the merits of SERPT and SEPT.
Secondly, we specify the Gittins' Index policy given multiplicative estimation errors and restricted to knowing only the estimation error distribution. We show that our proposed policy, which we call the Size Estimate Hedging (SEH) policy, has performance close to the Gittins' Index policy. Similar to SERPT and SEPT, the SEH policy only uses the job processing time estimates to prioritize the jobs. Finally, we provide numerical results obtained by running a wide range of simulations for both synthetic and real workloads. The key observations suggest that SEH outperforms SERPT except in scenarios where the job processing time variance is extremely low. SEH outperforms SEPT whether the variance of the job processing times is high or low. With the presence of better estimated processing times in the system (low variance in the estimation errors), SEH outperforms SEPT and has performance close to the optimal policy (SRPT) if the estimation errors are removed. On the other hand, we observe that when the estimation errors have high variance, there is little value in using the estimated processing times. We also notice that the system load does not significantly affect the relative performance of the policies under evaluation. The SEH policy treats underestimated and overestimated jobs fairly, in contrast with other policies that tend to favor only one class of jobs. When the job processing time variance is high, the SEH and SEPT policies obtain a near-optimal mean slowdown value of $1$, indicating that underestimated large jobs do not delay small jobs. In terms of mean slowdown, SEH outperforms SEPT across all levels of job processing time variance.

The rest of the paper is organized as follows. Section \ref{LiteratureReview} presents the existing literature in scheduling single-server queues with estimated job processing times. Section \ref{SEH} defines our SEH policy and discusses its relationship to a Gittins' Index approach. Our simulation experiments are described in detail in Section \ref {EvaluationMethodology}.
We provide the results of our simulations in Section \ref{SimulationResultsplusInsights} and conclude and discuss future directions in Section \ref{Conclusion}.