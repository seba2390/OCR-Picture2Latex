\section{Evaluation Methodology} \label{EvaluationMethodology}
\subsection{Policies Under Evaluation} \label{PoliciesUnderEvaluation}

In this section, we introduce the size-based scheduling policies considered for
evaluation. As our baseline policy, we consider the SRPT policy when the exact
job processing times, given by $s$, are known before scheduling. The SRPT policy is an ``ideal'' policy since it assumes that there are no errors in estimating the processing time. 
\begin{itemize}

\item \textbf{SERPT policy} --- The SERPT policy is a version of SRPT that uses the estimates of job processing times as if they were the true processing times.




\item \textbf{SEPT policy} --- The SPT policy skips the SRPT policy's updating of remaining processing times and only schedules jobs based on their estimated processing time. 

\item \textbf{SEH and Gittins' Index policy} --- Our proposed SEH policy and the Gittins' Index policy are explained in detail in Section \ref{FormalDefiniton} and Section \ref{GittinsVsSEH}, respectively. 
\end{itemize}

All these policies fit into the ``scoring'' framework, and they assign scores to each job
and process the jobs in the queue in the descending order of their scores.
Moreover, pre-emption is allowed, and a newly-arrived job can pre-empt the
current job if it has a higher score. The score functions in  \eqref{eq:9},  \eqref{eq:10}, \eqref{eq:11}, \eqref{eq:8}, and \eqref{eq:6} show how we
calculate the scores for the SRPT, SERPT, SEPT, SEH, and Gittins' Index policy,
respectively. 

\subsection{Performance Metrics} \label{PerformanceMetrics}

We evaluate the policies defined in Section \ref{PoliciesUnderEvaluation} with respect to two performance metrics: MST and Mean Slowdown. When the job processing times have large variance, the sojourn times for small jobs and large jobs differ significantly. Thus, we use the per job slowdown, the ratio between a job's sojourn time and its processing time \cite{wierman2011fairness}.


\subsection{Simulation Parameters} \label{SimulationParameters}
We would like to evaluate the policies over a wide range of job processing time and error distributions. To generate this range of distributions, we fix the form of the distribution and vary the parameters. We use the same settings that Dell'Amico et al.\ \cite {dell2015psbs} use in their work. Table \ref{tab:1} provides the default parameter values that we use in our simulation study. We now provide details of our simulation model. Note that our policy fits into the SOAP framework of Scully et al.\ \cite{scully2018soap}, however as we are also evaluating mean slowdown, we chose simulation for evaluation.\\



\begin{table}[t]
\centering
\caption{Parameter Settings}
\begin{tabular}{ |p{2.2cm}|p{7cm}|p{2cm}|  }
 \hline

 \hline
 \textbf{Parameter} & \textbf{Definition} &\textbf{Default} \\
 \hline
 \# jobs   & the number of departed jobs     &$10,000$ \\ 
 $k$&   shape for Weibull job processing time distribution  & $0.25$   \\
 $\sigma$ & $\sigma$ in the Log-Normal error distribution & $0.5$ \\
 $\rho$ &   system load   & $0.9$ \\

 \hline

\end{tabular}

\label{tab:1}
\end{table}




\textbf{Job Processing Time Distribution} ---  We consider an $M/G/1$ queue where the processing time is generated according to a Weibull distribution. This allows us to model high variance processing time distributions, which better reflect the reality of computer systems (see \cite{crovella1998heavy} and \cite{harchol1999ect} for example).  In general, the choice of a Weibull distribution gives us the flexibility to model a range of scenarios. The shape parameter $k$ in the Weibull distribution allows us to evaluate both high variance (smaller $k$) and low variance (larger $k$) processing time distributions.  

Considering that the job processing time distribution plays a significant role in the scheduling policies' performance and size-based policies show different behaviors with high variance job processing time distributions, we choose $k=0.25$ as our default shape for the Weibull job processing time distribution. With this choice for $k$, the scheduling policies' performance is highly influenced by a few very large jobs that constitute a substantial percentage of the system's overall workload. 
We vary $k$ between $0.25$ and $2$, considering specific values of $0.25$, $0.375$, $0.5$, $0.75$, $1$, and $2$. We show that the SEH policy performs best in the presence of high variance job processing time distributions. \\



\textbf{Job Processing Time Error Distribution} --- We have chosen the Log-Normal distribution as our error distribution so that a job has an equal probability of being overestimated or underestimated. The Gittins' index for this estimation error distribution is shown in \eqref{eq:7}. The $\sigma$ parameter controls the correlation between the actual and estimated processing time, as well as the estimation error variance. By increasing the $\sigma$ value,  the correlation coefficient becomes smaller, and the estimation error variance increases, resulting in the occurrence of more large underestimations/overestimations (more imprecise processing times). We choose $\sigma=0.5$ as the default value that corresponds to a median relative error factor of $1.40$. We vary $\sigma$ between $0.25$ and $1$ with specific values of $0.25$, $0.375$, $0.5$, $0.75$, and $1$ to better illustrate the effect of $\sigma$ on the evaluated performance. \\

\textbf{System Load} --- Following Lu et al.\ \cite{lu2004size}, we consider $\rho = 0.9$ as the default load value and vary $\rho$ between $0.5$ (lightly loaded) and $0.95$ (heavily loaded) with increments of 0.05 and an additional system load of $0.99$. \\



\textbf{Number of Jobs} --- The number of jobs in each simulation run is $10,000$ and a simulation run ends when the first $10,000$ jobs that arrived to the system are completed. We fix the confidence level at $95\%$, and for each simulation setting, we continue to perform simulation runs until the width of the confidence interval is within $5\%$ of the estimated value. For low variance processing time distributions (larger $k$), $30$ simulation runs suffice; however, more simulation runs are required for high variance processing time distributions (smaller $k$).



