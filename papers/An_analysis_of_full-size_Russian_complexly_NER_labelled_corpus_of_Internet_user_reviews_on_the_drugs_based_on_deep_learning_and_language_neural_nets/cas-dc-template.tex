%% 
%% Copyright 2019 Elsevier Ltd
%% https://www.overleaf.com/project/5cefc22d7fdd0931c9e13e5b
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version. The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-dc documentclass for 
%% double column output.

\documentclass[a4paper,fleqn,longmktitle]{cas-dc}
%\documentclass[a4paper,fleqn]{cas-dc}
% \documentclass[a4paper,fleqn,longmktitle]{cas-sc}

%\usepackage[authoryear,longnamesfirst]{natbib}
%\usepackage[authoryear]{natbib}
\usepackage[numbers]{natbib}
\usepackage[T1,T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
% \usepackage{color, soul}  % Для выделения в тектсте
%\usepackage[english,russian]{babel}
% \usepackage{longtable}
%\usepackage{hyperref}
\usepackage{diagbox}

%%%Author definitions
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}
%%%
\newcommand{\Fp}{F_1^\text{partial}}
\newcommand{\Fe}{F_1^\text{exact}}

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}
\shorttitle{Russian language corpus with a developed deep learning neuronet complex to analyze it}
\shortauthors{SG Sboeva et~al.}

% \title [mode = title]{Russian language corpus with a developed machine learning complex for analyzing the safety and effectiveness of drugs on the base of internet sources}     
\title [mode = title]{An analysis of full-size Russian complexly NER labelled corpus of Internet user reviews on the drugs based on deep learning and  language neural nets }     

% \tnotemark[1,2]

% \tnotetext[1]{This document is the results of the research
%   project funded by the National Science Foundation.}

% \tnotetext[2]{The second title footnote which is a longer text matter
%   to fill through the whole text width and overflow into
%   another line in the footnotes area of the first page.}
% Сбоев А.Г., Сбоева С.Г., Молошников И.А., Грязнов А.В., Наумов А.В., Селиванов А.А., Рыбка Р.Б., Рыльков Г.В., Ильин В.А. представлена на сайте https://sagteam.ru/med-corpus/ , подготовленном по материалам проекта. Статус: планируется к опубликованию в журнале Artificial Intelligence in Medicine


\author[1,2]{AG Sboev}[type=editor, orcid=0000-0002-6921-4133]
\cormark[1]
% \fnmark[1]
\ead{sag111@mail.ru}
% \ead[url]{sag111@mail.ru}
\author[3]{SG Sboeva}
\ead{sboevasanna@mail.ru}

\credit{Conceptualization of this study, Methodology, Dataset}

\address[1]{NRC "Kurchatov institute", Moscow, Russia}
\address[2]{MEPhI National Research Nuclear University, Kashirskoye sh., 31, Moscow, 115409, Russia}
\address[3]{I.M. Sechenov First Moscow State Medical University (Sechenov University), Moscow, Russia}

%\address[4]{Kazan (Volga Region) Federal University, Kazan, Russia}

\author[1]{IA Moloshnikov}
\ead{ivan-rus@yandex.ru}

\author[1]{AV Gryaznov}
\ead{artem.official@mail.ru}

\author[1]{RB Rybka}
% \fnmark[2]
\ead{rybkarb@gmail.com}

\author[1]{AV Naumov}
\ead{sanya.naumov@gmail.com}

\author[1]{AA Selivanov}
\ead{aaselivanov.10.03@gmail.com}

\author[1]{GV Rylkov}
\ead{gvrylkov@mail.ru}

%\author[4]{VD Solovyev}
%\ead{maki.solovyev@mail.ru}
%%\author[2]{DV Gudovskikh}
%\ead{dvgudovskikh@gmail.com}


\author[1]{VA Ilyin}
\ead{ilyin0048@gmail.com }

% \author[2]{AV Evteeva}
% \ead{elis24.96@mail.ru}

 

% \address[2]{NRC "Kurchatov Institute'', Moscow, Russia}

% \cormark[2]
% \fnmark[1,3]

\cortext[cor1]{Corresponding author}
% \cortext[cor2]{Principal corresponding author}
% \fntext[fn1]{This is the first author footnote. but is common to third
%   author as well.}
\sloppy 
\begin{abstract}
We present the full-size Russian complexly NER-labeled corpus of Internet user reviews, along with an evaluation of accuracy levels reached on this corpus by a set of advanced deep learning neural networks to extract the  pharmacologically meaningful entities from Russian texts. The corpus annotation includes mentions of the following entities: Medication (33005 mentions), Adverse Drug Reaction (1778), Disease (17403), and Note (4490). Two of them – Medication and Disease – comprise a set of attributes. A part of the corpus has the coreference annotation with 1560 coreference chains in 300 documents. %In order tо select the most effective neuron models for further adaptation to Russian language texts, numerical analysis has been performed on CADEC and N2C2 corpora. Selected neuronet models were adapted to Russian-language texts. This justifies the usage of our corpus to estimate the current accuracy baseline of the problem for Russian texts.
Special multi-label model based on a language model and the set of features is developed, appropriate for presented corpus labeling. The influence of the choice of different modifications of the models: word vector representations, types of language models pre-trained for Russian, text normalization styles, and other preliminary processing are analyzed. The sufficient size of our corpus allows to study the effects of particularities of corpus labeling and balancing entities in the corpus. As a result, the state of the art for the pharmacological entity extraction problem for Russian is established on a full-size labeled corpus.
%, and is shown to be on par with the accuracy level for solving a similar task for other languages, which is 63.1\% for ADR recognition by the F1-exact metric. 
In case of the adverse drug reaction (ADR) recognition, it is 61.1 by the F1-exact metric that, as our analysis shows,  is on par with the accuracy level  for other language corpora  with similar characteristics and the ADR representativnes. The evaluated baseline precision of coreference relation extraction on the corpus is 71, that is higher the results reached on other Russian corpora.
%We present the full-size Russian complexly NER labeled corpus of Internet user reviews, along with an evaluation of accuracy levels reached with this corpus by the set of advanced deep learning neuron nets used for the extraction of pharmacologically meaningful entities from Russian texts. The corpus annotation includes mentions of the following entities: Medication (33005 mentions), Adverse Drug Reaction (1778), Disease (17403), and Note (4490). Two of them – Medication and Disease – comprise a set of attributes. То select the most effective neuron models for further adaptation to Russian language texts the calculational analysis was made on CADEC and N2C2 corpora. Selected neuron models were adapted to Russian language texts. which makes  It appropriate to estimate on the base of our corpus the current accuracy baseline of the problem for Russian texts. The special multilabel model basing on a complex of a language model and the set of features is developed, appropriated for presented corpus labeling. The influences of the choice of different modifications of models: word vector representations other types of pre-training, Russian language models, text normalization styles, and other preliminary processing are analyzed.  The sufficient size of our corpus has given a possibility to numerically obtain the effects of particularities of corpus labeling and balancing entities in corpus. As a result, a state-of-the-art level of accuracy for the pharmacological entity extraction problem for Russian is estimated on a full-size labeled corpus, and, as shown, has the accuracy level for solving the task similar for other languages, which are 63.1\% F1-exact for ADR entities recognition.
%63.1\%, 62.9, 84.2 F1-exact for ADR, Disease, and Medication entities recognition.
%Мы представляем первый аннотированный корпус со сложной разметкой (аннотацией) для текстов на русском языке, собранных из социальных источников. Коллекция текстов состоит из оценочных суждений пациентов о лекарственных препаратах различных фармакотерапевтических групп, которые не следуют формальным правилам грамматики и пунктуации русского языка. Количество отзывов в корпусе составило 1660, со средней длиной отзыва 896.38 символов. Аннотации содержат упоминания о таких понятиях, как Medication (17779), Adverse Drug Reaction (844), Disease (9285), Note (2319). Два из них (Medication и Disease) включают в себя ряд дополнительных подтегов. Аннотации были выполнены аннотаторами с медицинским образованием, которые пользовались специально разработанным совместно со специалистами по интеллектуальным методам анализа текстовых данных руководством. В работе также предложена baseline модель для задачи выделения медицинских сущностей, которая включает в себя нейросетевые конволюционные и рекуррентные слои, контекстно-зависимые векторные представления слов, условные случайные поля и дополнительные признаки слов полученные из словарей тонально-эмотивной лексики. Предложенная модель показала результаты сопоставимые с state-of-the-art результатами для данной задачи на стороннем корпусе данных, что показывает применимость предложенной модели.

\end{abstract}

% \begin{graphicalabstract}
% \includegraphics{figs/grabs.pdf}
% \end{graphicalabstract}

% \begin{highlights}
% \item Research highlights item 1
% \item Research highlights item 2
% \item Research highlights item 3
% \end{highlights}

\begin{keywords}
Pharmacovigilance \sep Annotated corpus \sep Adverse drug events\sep Social media \sep UMLS \sep MESHRUS \sep Information extraction \sep Semantic mapping \sep Machine learning \sep Neural Networks \sep Deep Learning

\end{keywords}


\maketitle
\section{Introduction}
Nowadays, a great amount of texts collected in the open Internet sources contains a vast variety of socially significant information. In particular, such information relates to healthcare in general, consumption sphere and evaluation of medicines by the population. Due to time limitations, clinical researches may not reveal the potential adverse effects of a medicine before entering the pharmaceutical market. This is a very serious problem in healthcare. Therefore, after a pharmaceutical product comes to the market, pharmacovigilance (PV) is of great importance. Patient opinions on the Internet, in particular in social networks, discussion groups, and forums, may contain a considerable amount of information that would supplement clinical investigations in evaluating the efficacy of a medicine. Internet posts often describe adverse reactions in real time ahead of official reporting, or reveal unique characteristics of undesirable reactions that differ from the data of health professionals. Moreover, patients openly discuss a variety of uses of various drugs to treat different diseases, including ``off-label'' applications. This information would be very useful for a PV database where risks and advantages of drugs would be registered for the purpose of safety monitoring, as well as the possibility to form hypotheses of using existing drugs for treating other diseases. This leads to an increasing need for the analysis of Internet information %from electronic sources
to assess the quality of medical care and drug provision. In this regard, one of the main tasks is the development of machine learning methods for extracting useful information from social media. % An active control on the base of social networks is implemented by a number of countries.
However, expert assessment of such amount of  text information is too laborious, therefore special methods 
 have to be developed with taking into account the presence in  these texts the informal vocabulary and  of reasoning.  
%To automatically analyze such amount of information, special methods have to be developed.
%However, the 
The quality of these methods directly depends on tagged corpora to train them. % In particular, the United States Food and Drug Administration is creating a base, using medical forums, where consumers discuss the experience of using drugs. That base would be a valuable resource for the analysis and development of the texts in machine learning. However, expert assessment of such texts is too laborious, while extracting medical information using conventional processing methods is difficult due to the use of the informal vocabulary and the presence of reasoning. % In this regard, one of the main tasks is the development of machine learning methods for extracting useful information from social media. It is an area of increasing interest, and even is becoming mandatory on the territory of the Eurasian Union: according to the decision of the Council of the Eurasian Economic Commission No. 87 of November 3, 2016 ``On Approving the Rules of Good Practice for PV of the Eurasian Economic Union'', registration certificate holders are obliged to monitor the Internet and digital health records regularly for potential reports on suspected undesirable reactions.
%Unfortunately, there still have been no annotated corpora for PV in Russia.
% In this paper, we present the first Russian corpus of such type with complex annotation, for which we propose the name Russian Drug Reviews by SagTeam initiative project (RDRS)~\footnote{Russian Drug Reviews by SagTeam initiative project (RDRS) 
In this paper, % we present the first Russian complexly annotated corpus, 
we present the full-size Russian complexly NER-labeled corpus of Internet user reviews, named Russian Drug Reviews corpus of SagTeam project (RDRS)\footnote{Corpora description is presented on \url{https://sagteam.ru/en/med-corpus/} }-  comprising the part with tagging on coreference relations. 
% In addition, we present a deep learning neural network complex to extract pharmacologically meaningful entities from a Russian text.
Also, we present model appropriate to the corpus multi-tag labelling  developed on base of the combination of XLM-RoBERTa-large model with the set of added features.


 In Section~\ref{sec:related_works}, we analyse the selected set of corpora comprising ADR (Adverse drug reaction) labels, but different by  fillings, labeling tags, text sizes and styles with a goal to analyse their influence on the ADR extraction precision.
The materials used to collect the corpus are outlined~in~Section~\ref{sec:corpus_collecting}, the technique of its annotation is described in Section~\ref{subsec:corpus_annotation}. The developed machine learning complex is presented in Section~\ref{sec:Methods}. The conducted numerical experiments are presented in Section~\ref{subsec:Experiments} and discussed in~Sections~\ref{sec:results}~and~\ref{sec:discussion} .

\section{Related works}
\label{sec:related_works}
%In the world science, research concerning the above-mentioned problems is conducted intensively, resulting in a great diversity of annotated corpora. These corpora can be divided into two groups: firstly, the ones of texts written by medics (clinical reports with annotations), and secondly, those of texts written by non-specialists, namely, by the Internet customers who used the drugs. The distinctive features of any corpus are the number of entities, the number of annotation types, and approaches to entity normalization. The diversity of these features makes it difficult to compare the accuracy of entity recognition on the base of different corpora.
%In world science, research concerning the above-mentioned problems is conducted intensively, resulting in a great diversity of annotated corpora. From the linguistic point of view,  these corpora can be divided into two groups: firstly, the ones of texts written by medics (clinical reports with annotations), and secondly, those of texts written by non-specialists, namely,  by the Internet customer

%who used the drugs. The variability of the natural language constructions in the speech of Internet users complicates the analysis of corpora based on Internet texts.The other distinctive features of any corpus are the number of entities, the number of annotated phrases definite types, also the number of its mutual uses in phrases, and approaches to entity normalization. The diversity of these features influences the accuracy of entity recognition on the base of different corpora.


 % Clinical corpora and corpora of Internet user texts, on the one hand, have a certain similarity, but on the other hand, essential differences. Both are partially based on the similar annotation schemes, normalization procedures, and can be analyzed by similar algorithms. However, the former --
 
% Clinical corpora and corpora of Internet user texts have   essential differences. Сlinical corpora have more types of annotated entities, mostly related to disease and treatment indication rather than pharmacology entities. Types of included entities are usually formulated stricter in clinical texts than in Internet user texts. In this survey we focus on text corpora of Internet users.

%The variability of the natural language constructions in the speech of Internet users complicates the analysis of corpora based on Internet texts. In this section, we strive to clarify how these reasons influence the accuracy of entity recognition, taking into account the differences of the task formulation in papers about analysis of clinical corpora or corpora of Internet user texts. We consider the corpora closest to the one presented in this article. Table~\ref{tab:table1_long_clinical_corpora} provides a summary of the relevant corpora.

% The variability of the natural language constructions in the speech of Internet users complicates the analysis of corpora based on Internet texts.Table ? provides a summary of the relevant corpora.
%In this section, we strive 
 In world science, research concerning the above-mentioned problems is conducted intensively, resulting in a great diversity of annotated corpora. From the linguistic point of view, these corpora can be distinguished  into two groups: firstly, the ones of texts written by medics (clinical reports with annotations), and secondly, those of texts written by non-specialists, namely, by the Internet customers who used the drugs. The variability of the natural language constructions in the speech of Internet users complicates the analysis of corpora based on Internet texts, вut there are the other distinctive features of any corpus : the number of entities, the number of annotated phrases definite types, also the number of its mutual uses in phrases, and approaches to entity normalization.  The diversity of these features influences the accuracy of entity recognition on the base of different corpora. Also the  tipes of entity labelling  and   used metrics of evaluating results may be various.   Not for each corpus  a necessary information is available.% In table N  we view   characteristics of 6 corpora: CADEC, n2c2-2018,Twits,PsyTar ,TwiMed Twitter,RUDR.
 Below we briefly describe 6 corpora: CADEC, n2c2-2018, Twitter annotated corpus, PsyTAR, TwiMed corpus, RuDReC. %In table N  we view   characteristics of these corpora and  evaluate its influence on accuracy of ADR extraction.
% Table ? provides a summary of the relevant corpora.

     
%\subsection{Corpora of clinical reports}

%\subsection{Corpora of clinical reports}
%\paragraph{CLEF corpus (CLinical E-Science Framework)~\cite{roberts2007clef}.} The basis of this corpus is a dataset of 565\,000 semantically annotated documents on 20\,234 deceased patients from Royal Marsden Hospital. The documents are of three types: clinical narratives, histopathology reports, and imaging reports. The annotators marked text fragments (\emph{spans}) with a type: drug, locus, and so on. The Clinical Narratives~\cite{roberts2008combining} part of the corpus contains 77 documents with mentions of the following entity types marked: \emph{Condition} (739 mentions), \emph{Intervention} (298), \emph{Investigation} (325), \emph{drug or device} (272), \emph{locus} (490). In addition, the annotators marked words that modify spans (such as negation), and marked relations between spans. Two or more spans may refer to the same entity, in which case they are \emph{coreferent}. Every document is marked up by two independent annotators, and the third one makes the final consensus annotation.
%\begin{table*}
%\caption{Summary of a few relevant corpora of annotated clinical texts.} 
%\label{tab:table1_long_clinical_corpora}
%\input{tables/clinical_corpora.tex}
%\end{table*}

%\paragraph{ShARe CLEF eHealth 2013~\cite{pradhan2013task} and 2014~\cite{mowery2014task}} are the corpora collected for the competitions on the medical texts information extraction task: CLEF eHealth 2013 Task 1, and CLEF eHealth 2014 Task 2 respectively. The 2013 corpus contains about 300 documents of 4 clinical report types: discharge summaries, radiology, electrocardiograms, and echocardiograms. The corpus of year 2014 is an extension, larger by 133 documents. The main dataset for both corpora is the \emph{Multiparameter Intelligent Monitoring in Intensive Care} (MIMIC)~\cite{johnson2016mimic}. The MIMIC comprises impersonal health data associated with about 40\,000 critical care patients and includes demographics, vital signs, laboratory tests, medications, etc. 

%\paragraph{SCCH (Corpus of Scientific Center of Children Health)~\cite{shelmanov2015information}} is an annotated corpus of clinical texts in Russian~\footnote{SCCH corpus website: http://nlp.isa.ru/datasets/clinical}
%~\cite{NlpIsa}
 %The corpus includes 112 medical records of more than 60 patients from the Scientific Center of Children Health with allergic and pulmonary disorders and diseases. It comprises discharge summaries, radiology, echocardiography, ultrasound diagnostics reports, and recommendations. All documents are depersonalized: names have been deleted and dates distorted. The markup scheme is partially similar to the scheme presented in ShARe CLEF eHealth 2014 Task 2 and includes the following entities: \emph{Disease}; \emph{Symptom}; \emph{Drug}; \emph{Treatment}; \emph{Body location}; \emph{Severity}; \emph{Course}. Currently, the documents contain the total of 45\,000 tokens (words, punctuation symbols, and so on), among which more than 7\,600 are annotated with entities and more than 4\,000 are annotated with attributes and relations.

%\paragraph{ADE (adverse drug effect) corpus~\cite{gurulingappa2012development}} consists of 2\,972 documents which have been randomly selected from nearly 30\,000 PubMed documents and annotated manually by three annotators. The corpus contains three types of entities: \emph{Drug}, \emph{Adverse effect}, and \emph{Dosage}. Annotators labeled relations in sentences between drugs and side effects; drugs and dosages; and all texts in corpus that do not contain any drug-related adverse effects. The goal was to extract adverse effects of drugs mentioned within the context of sentences. Mentions of drugs, disorders or dosages that did not fit into a relation were not annotated. With the purpose of preparing a larger dataset for a supervised classifier, the ADE corpus was expanded with machine-annotated drugs, conditions, and conditions that did not fall into adverse effect relations but were still within the same sentence. This corpus is called ADE-EXT~\cite{Gurulingappa2012ADEEXT}. It includes 2\,269 new drugs, 3\,437 new conditions and 5\,968 false relations (co-occurring drug-condition pairs that were not previously annotated by humans were considered false).

%\paragraph{MADE1.0 (Medication and Adverse Drug Events from Electronic Health Records)} corpus~\cite{jagannatha2019overview} contains a cohort of medications and ADE information annotated by experts. It includes 1\,089 depersonalized electronic health notes from 21 randomly selected cancer patients at the University of Massachusetts Memorial Hospital. The corpus provides a set of common evaluation tasks to assess the state of the art for natural language processing (NLP) systems applied to electronic health records supporting drug safety surveillance and PV. The MADE 1.0 was used in three shared NLP tasks: The named entity recognition (NER) task for medications and their attributes (dosage, route, duration, and frequency of taking), indications, ADEs, and severity. The relation identification (RI) task is the identification of relations between the named entities (medication-indication, medication-ADE, and attribute relations). The third shared task (NER-RI) evaluates NLP models that perform the NER and RI tasks jointly. A particularity of this corpus is that the annotators mark an ADE mention only in a direct linguistic cue that links an adverse effect to a drug name.
%The high quality of the corpus markup is provided thanks to selecting entities by pattern matching, which allows not to use additional normalization.
% четкость разметки корпуса обеспечивается использованием правил выделения сущностей по шаблону, что позволяет обходиться без дополнительной нормолизации. 

% Corpus annotation has straight rules for entities markup (ADE and Indication), they require entity to match a defined pattern or to have an interconnection with a medication as stated in rules description:
% %При разметке корпуса установлены четкие правила выделения сущностей (ADE и Indication) требующие соответствия четкому шаблону или обязательно иметь взаимосвязь с medication, как указано в описании этих правил:
% -- An ADE annotation requires a direct linguistic cue that links the adverse effect to a drugname, e.g., ''Patient had anaphylaxis after getting penicillin.''
% -- Indication An indication is annotated if it is explicitly linked to a medication, e.g., ``The patient was troubled with mouth sores and is being treated with Actiq.''

%\begin{table*}
%\caption{Summary of the existing clinical text corpora.} 
%\label{tab:info_clinic_corpora}
%\input{tables/cl_corp.tex}
%\end{table*}

%\paragraph{I2b2--NLP Data Set \#3~\cite{uzuner2010extracting}} has been developing since 2006. The corpus contains data of 1\,243 depersonalized summaries from the network of non-profit hospital and physicians ``Partners Healthcare'', which includes Brigham and Women's Hospital (BWH), and Massachusetts General Hospital (MGH). The annotation process was preceded with a golden standard markup, containing annotations of 17 texts. After that, an annotation guide was created, and different expert groups have annotated another 547 texts. Finally, the research group processed the annotation results and collected a dataset containing 251 annotated texts. The list of annotated entities includes: \emph{Medications}, \emph{Dosages}, \emph{Modes}, \emph{Frequencies}, \emph{Durations}, \emph{Reasons}. The corpus contains both structured and narrative components of clinical records. Thus, the first type of discharge summary is a formalised list structure, and the second is a narrative text. As shown in that work~\cite{uzuner2010extracting}, entities are much better extracted from structural information than from a narrative text. As a result, it gives one a possibility to reach higher accuracy without normalization.

%\paragraph{IxaMed corpus~\cite{oronoz2015creation}} is composed of electronic health records written in Spanish. The corpus consists of 142\,154 discharge reports from the outpatient consultations in the Galdakao-Usansolo Hospital. The documents were created between 2008 and 2012 by about 400 doctors from different services. Experts made modifications if it is necessary and tagged the \emph{adverse drug reaction} (ADR) events. All documents in the corpus were manually depersonalized by changing names and dates. All medical abbreviations were identified with the help of the dictionary by Yetano and Alberola~\cite{laguna2003diccionario}, while drug brand names were looked up in the BOTPLUS5\footnote{Drug Database BOTPLUS5 - \url{https://botplusweb.portalfarma.com/}}
%~\cite{botplusweb}
%database. Its gold standard is manually annotated by experts in pharmacology and pharmacovigilance. The annotation was performed in several stages. At first, the developers created an annotation guide and chose a preliminary marking system based on dictionaries and rules. Two independent annotators marked up 50 documents using the guide, matched the differences and refined the guide. Then, another 25 texts were annotated and joined with the first set to form a golden standard of 75 documents.

%\subsection{Normalization approaches applied to clinical corpora}
%For normalizing the entities in clinical corpora, different approaches for mapping to thesauruses were used. For instance, in ShARe CLEF eHealth 2013~\cite{pradhan2013task} and 2014~\cite{mowery2014task}, disease entities were mapped to a Concept Unique Identifier (CUI) according to the \emph{Systematized Nomenclature of Medicine -- Clinical Terms} (SNOMED CT) terminology~\cite{shahpori2010systematized}, which belongs to one of the \emph{Unified Medical Language System} (UMLS)~\cite{campbell1998unified} semantic types. In the CLEF corpus~\cite{roberts2007clef}, the normalization of entities is based on the types from the UMLS semantic network. The procedure of term normalization in SCCH~\cite{shelmanov2015information} is based on two thesauruses: UMLS Metathesaurus for disease, symptom, and body site identification; a thesaurus based on the State Register of Drugs (SRD)~\cite{SRD} for drug identification. The only Russian thesaurus present in UMLS is MeSHRUS (Medical Subject Headings)~\cite{NLM}, but it does not provide all nomenclature of drugs used in Russia. In the IxaMed corpus~\cite{oronoz2015creation}, the FreeLingMed system~\cite{oronoz2013automatic} is able to carry out medical named entity recognition, linking all the terms in SNOMED CT with their corresponding semantic tags (substances, disorders, procedures, findings). Another system, ProMiner~\cite{hanisch2005prominer}, was used in the ADE corpus~\cite{gurulingappa2012development} to map names of drugs and adverse effects to standard ontologies. In this case, the drug names were mapped to the Anatomical Therapeutic Chemical (ATC) classification system~\cite{miller1995new} using the DrugBank~\cite{wishart2006drugbank} dictionary. The ATC hierarchically classifies several drugs according to their pharmacotherapeutic properties. The names of adverse effects were mapped to the Medical Dictionary for Regulatory Activities (MedDRA)~\cite{brown1999medical} classification system. In the MADE 1.0~\cite{jagannatha2019overview} and I2b2--NLP \#3~\cite{uzuner2010extracting}, corpora there is no normalization.

%Table~\ref{tab:info_clinic_corpora} summarizes the information about the normalization types in the clinical corpora and presents their overall entity recognition accuracy (averaged over all entity types) measured by the F1 exact/partial metrics explained in Section~\ref{section:quality_metrics}.

% \begin{table*}
% \centering
% \caption{Summary of the existing Internet text corpora with evaluations of NER task.}
% \label{tab:info_social_corpora}
% \input{tables/table3.tex}
% \end{table*}

%\begin{table*}
%\caption{The F1 accuracy score of recognizing ADR entities %in the existing corpora}
%\label{tab:accuracy_for_ADR}
%\input{tables/table4.tex}
%\end{table*}


%\subsection{Corpora of internet user texts}

\subsection{Corpora description}

\paragraph{CADEC (corpus of adverse drug event annotations)
~\cite{karimi2015cadec}} is a corpus of medical posts taken from the AskaPatient~\footnote{Ask a Patient: Medicine Ratings and Health Care Opinions - \url{http://www.askapatient.com/}}
%~\cite{askapatient} 
forum and annotated by medical students and computer scientists. It collects ratings and reviews of medications from their consumers and contains consumer posts on 13 different drugs. There are 1253 posts with 7398 sentences. The following entities were annotated: Drug, ADR, Symptom, Disease, Findings. The annotation procedure involved 4 medical students and 2 computer scientists. In order to coordinate the markup, all annotators jointly marked up several texts, and after that the texts were distributed among them. All the annotated texts were checked by three corpus authors for obvious mistakes, e.g. missing letters, misprints, etc.

\paragraph{TwiMed corpus (Twitter and PubMed comparative corpus of drugs, diseases, symptoms, and their relations)}~\cite{alvaro2017twimed} contains 1000 tweets and 1000 sentences from Pubmed~\footnote{National Center for Biotechnology Information webcite - \url{http://www.ncbi.nlm.nih.gov/pubmed/}}
%~\cite{NCBI} 
for 30 drugs. It was annotated for 3\,144 entities, 2\,749 relations, and 5\,003 attributes. The resulting corpus was composed of agreed annotations approved by two pharmaceutical experts. The entities marked were Drug, Symptom, and Disease.

\paragraph{Twitter annotated corpus~\cite{sarker2016social}} consists of randomly selected tweets containing drug name mentions: generic and brand names of the drugs. The annotator group comprised pharmaceutical and computer experts. Two types of annotations are currently available: Binary and Span. The binary annotated part~\cite{sarker2015portable} consists of 10\,822 tweets annotated by the presence or absence of ADRs. Out of these, 1\,239 (11.4\%) tweets contain ADR mentions and 9583 (88.6\%) do not. 
The span annotated part~\cite{sarker2016social} consists of 2\,131 tweets (which include 1\,239 tweets containing ADR mention from the binary annotated part). The semantic types marked are: ADR, beneficial effect, indication, other (medical signs or symptoms). 

\paragraph{PsyTAR dataset~\cite{zolnoori2019psytar}} contains 891 reviews on four drugs, collected randomly from an online healthcare forum~\footnote{Ask a Patient: Medicine Ratings and Health Care Opinions - \url{http://www.askapatient.com/}}
%~\cite{askapatient}
. They were split into 6\,009 sentences. To prepare the data for annotation, regular expression rules were formulated to remove any personal information such as emails, phone numbers, and URLs from the reviews. The annotator group included pharmaceutical students and experts. They marked the following set of entities: ADR, Withdrawal Symptoms (WD), Sign, Symptom, Illness (SSI), Drug Indications (DI) and other. Sadly, the original corpus doesn't contain mentions boundaries in source texts. It complicates the NER task. In a paper \cite{basaldella2019bioreddit} presented version of the PsyTAR corpus in CoNLL format, where every word has corresponding tag of named entity. We use this version for comparison purposes.

\paragraph{n2c2-2018~\cite{n2c2-2018}}
 is a dataset from the National NLP Clinical Challenge of the Department of Biomedical Informatics (DBMI) at Harvard Medical School. 
The dataset contains clinical narratives, and builds on past medication extraction tasks, but examines a broader set of patients, diseases, and relations as compared with earlier challenges. 
%The dataset contains 505 anonymized clinical narratives from MIMIC-III database.
It was annotated by 4 paramedic students and 3 nurses. Label set includes medications and associated attributes, such as dosage (Dosage), strength of the medication (Strength), administration mode (Mode), administration frequency (Frequency), administration duration (Duration), reason for administration (Reason), and drug-related adverse reactions (ADEs).
% One of the subtasks of the challenge was: ``Can NLP systems automatically discover adverse event in clinical narratives?''. 
The number of texts was 505 (274 in training, 29 in development and 202 in test).


\paragraph{RuDReC~\cite{10.1093/bioinformatics/btaa675}}
Labeled part of RuDReC contains 500 reviews on drugs from a medical forum OTZOVIK. Two step annotation procedure was performed: on first step authors used 400 texts labeled according formats of site Sagteam [https://sagteam.ru/en/med-corpus/annotation/] by 4 experts of Sechenov First Moscow State Medical University - now participants of our projects; on second step they simplified labeling by deleting/uniting tags and annotated in addition 100 reviews. Totally in RuDReC and in proposed corpus RDRS 467 texts are coincident. An influence of differences in labelling of them on the ADR extraction accuracy presented in Section~\ref{sec:discussion}.


\subsection{Target vocabularies in the corpora normalization}
The normalization task of internet user texts is more difficult because of informal text style and more natural vocabulary. Still, as in the case of clinical texts, thesauruses are used. In particular, annotated entities in CADEC were mapped to controlled vocabularies: SNOMED CT, The Australian Medicines Terminology (AMT)~\cite{techrepAMT}, and MedDRA. Any span of text annotated with any tag was mapped to the corresponding vocabularies. If a concept did not exist in the vocabularies, it was assigned the ``concept\_less'' tag. In the TwiMed corpus, for Drug entities the SIDER database~\cite{kuhn2015sider} was used, which contains information on marketed medicines extracted from public documents, while for Symptom and Disease entities the MedDRA ontology was used. In addition, the  terminology of SNOMED CT concepts was used for entities, which belong to the Disorder semantic group. In the Twitter dataset~\cite{sarker2016social}, when annotating ADR mentions, they were set in accordance to their UMLS concept ID. Finally, in PsyTAR corpus, ADRs, WDs, SSIs and DIs entities were matched to UMLS Metathesaurus concepts and SNOMED CT concepts. No normalizations was applied to n2c2-2018 corpus. 

\subsection{Number of entities and their breakdown in the corpora}
In Table \ref{tab:adr_saturation}, we review the complexity characteristics of the selected corpora and evaluate the dependence of accuracy of extracting the ADR on them.
The overlap entities  are  only in few  of considered corpora but   their parts are relatively small, excluding CADEC, where there are the parts of overlap ADR entities, both continuous (5\%), and discontinuous (9\%). 
In this sense, CADEC, appears, is the most complicated corpus from selected, but having the largest numbers of ADR mentions and the largest value of the relation of ADR mention number to symptom  mention number. If the first factor complicates  the ADR identification, both others simplify. We could not find in literature the information  about  the  precision of the ADR  identification for all  corpora in view according metrics exact F1. However, on the base of data of Table \ref{tab:adr_saturation} we suggest the parameter of relation of the ADR mention number  to total number of corpus words is convenient to compare  the corpora, and we use it further named as "saturation".
%Data of the table demonstrate ( see column..),that the accuracy of extraction ADR grows with the  decrease of the relation of symptom (such as....) labels number to    ADR labels number, that is the consequence of their contexts similarity and complicates the ADR identification.% Other factors of such type are numbers of multiwords and overlap entities in the corpus. Conсerning numbers of multiwords their proportions in considered corpora may be considered as relatively similar in 10\% limit.  As for second one their numbers in considered corpora are, or relatively small, or they are  absent, excluding CADEC, where there is the parts of  overlap ADR  entities, as continuous(5\%), and discontinuous (9\%).

 % In this sense, CADEC  ,appears, is the most complicated corpus from selected, but having the largest numbers of ADR mentions and the largest value of t.he relation of ADR mention number to symptom  mention number. If the first factor complicates  the ADR  identification, both others simplify
%Concerning the influence of overlap entities in considered corpora, their parts are, or relatively small, or they are absent, excluding CADEC, where there are the parts of overlap ADR entities, both continuous(5\%), and discontinuous (9\%). In some papers, authors do not take them into account in calculations, which may influence results.%  See, for example, the article ..., where they were not taken into account and  F1 for ADR higher.


\subsection{Coreference task}
There is a problem, that some reviews present user opinion concerning the mentions of a particular tag in relation to more than one entity of real world: a drug, or disease, or the other entities. For example, some reviews may contain reports about use of multiple medications that may have different effects, so coreference annotation may be useful for detection of different mentions referred to the same drug. For English language there are few corpora for coreference resolution like CoNLL-2012~\cite{pradhan2012conll} or GAP~\cite{webster2018gap}, and even corpus of pharmacovigilance records with adversarial drug reactions annotations that includes coreference annotation (PHAEDRA)~\cite{thompson2018annotation}. The coreference problem  in Russian texts is slightly highlighted in a literature. Currently, there are only two corpora with coreference annotations for Russian language: Ru-Cor~\cite{azerkovich2014evaluating6887693} and corpus from shared task AnCor-2019~\cite{Ju2014RUEVAL2019EA}. The latter is a continuation and extension of the first. As for the methods the state-of-the-art approach is based on neural network trained end-to-end to solve two task at the same time: mention extraction and relations extraction. This approach was firstly introduced in \cite{lee2017end} and have been used in several papers\cite{lee2018higher, joshi2019bert, xu2020revealing, joshi2020spanbert, toshniwal2020learning} with some modifications to get higher scores on the coreference corpus CoNLL-2012~\cite{pradhan2012conll}.
%In   some papers authors do not  take  them into account, that influences on results. With regard to the information about taking into account the entity overlapping we refer readers to original papers, because,in some cases, it is presented very poor.
% As in the part concerning clinical corpora, in Table~\ref{tab:info_social_corpora} we summarize a set of papers addressing named entity recognition and type classification tasks on the presented social corpora. The average entity recognition scores $\Fe$ and $\Fp$ (described in Section~\ref{subsec:Quality_Metrics}) are presented for different normalization types.

%\begin{table*}
%\caption{Specifications of the corpus.}
%\label{tab:specification_of_MEDcorp}
%\input{tables/table6.tex}
%\end{table*}

\begin{table*}
\caption{A sample post for ``Глицин'' (Glycine) from otzovik.com. Original text is quoted, and followed by English translation in parentheses.}
\label{tab:sample_post_MEDcorp}
\input{tables/table5.tex}
\end{table*}

% \subsection{Comparison of entity identification accuracy in clinical and internet texts}
% The overall entity identification accuracy (see Tables~\ref{tab:info_clinic_corpora} and \ref{tab:info_social_corpora}) is higher in clinical texts by about 12\% (by the $\Fe$ metric) and 19\% by $\Fp$. This may be explained by the fact that clinical texts are more formal and strictly-structured than Internet texts. Table~\ref{tab:accuracy_for_ADR} confirms this conclusion for the case of identifying ADR mentions solely. There, the accuracy of ADR identification in clinical texts is about 9\% higher by $\Fe$.

%The results mentioned above are obtained using different approaches and features. Several works  ($\Fe = 0.676$~\cite{johri2014optimizing} on the ShARe CLEF eHealth 2014 corpus, $\Fp = 0.862$~\cite{shelmanov2015information} on SCCH, $\Fe = 0.49$~\cite{zolnoori2019systematic} on PsyTar) are based on rule-based approaches which use the UMLS dictionary, morphological, syntactical, and negation dependency features.
% Other works use a combination of rule-based approach with machine learning models like CRF or SVM  ($\Fe=0.856$, $\Fp=0.849$~\cite{patrick2010high} on the I2b2 corpus) along with dictionary (SNOMED CT) and traditionally morphology (prefix, suffix) feature set.
%Indeed, in the highly formalised clinical texts, even a simple classification model of SVM shows good results ($\Fe=0.71$~\cite{tang2013recognizing} on CLEF, $\Fe=0.75$, $\Fp=0.873$~\cite{roberts2008combining} on ShARe CLEF eHealth 2013) on base of such features as bag of words, part-of-speech (PoS) and semantic categories of words based on UMLS.

% However, in unformalised Internet texts, the conventional machine learning algorithms show inferior accuracy. For instance, CRF achieves $\Fe=0.703$~\cite{perez2017semi} on the IXAMed corpus, but just $\Fp=0.611$~\cite{wang2016mining} on the Twitter corpus. Both works use a wide range of features including word forms, linguistic features, PoS, semantic tags, word embedding, and SNOMED CT or COSTART, SIDER dictionaries respectively.
% An up-to-date level of accuracy on Internet texts is usually reached by a modern deep learning approach of bi-directional LSTM (bi-LSTM) ($\Fp=0.648$~\cite{gupta2018multi} on TwiMed) based on PoS features along with pretrained word embedding models. In some works, the bi-LSTM architecture additionally comprises a character-level representation CNN layer~\cite{li2017neural} ($\Fe=0.846$ on the ADE corpus), a CRF layer~\cite{dai2017medication} ($\Fe=0.806$ on CADEC) or a combination of CNN with recurrent neural network layers~\cite{wunnava2018bidirectional} ($\Fe=0.841$ on MADE1.0). 
% Therefore, in this work, in order to estimate the quality of our corpus we implement the deep learning bi-LSTM architecture along with an advanced neuronet model with use of up-to-date pretrained Russian language models.

% Как то засунуть модели машинного обучения. По каждой статье из таблиц 2,3,4 посмотреть (и написать):
% 1) метод такой то (SVM классификатор для слов текста), SVM классификатор для текста, CRF для слов.. и тд, 
% 2) признаки (словари, правила, нормлизоация, просто слова, буквы, embedding?),
% +Summary по Related Works

\begin{table*}
\centering
\caption{Numerical estimation of the corpora complexity on ADR level saturation. \newline Explanation of abbreviations of corpora names: TA – Twitter Annotated Corpus, TT – TwiMED Twitter, TP – TwiMED PubMed, N2C2 – n2c2-2018. Following the artcile \cite{gupta2018co}, we meant by the ADRs symptoms related to the drugs in TT and TP corps.  Explanation of abbreviations of metrics: f1-e – f1-exact, f1-am – f1-approximate match,  f1-r – f1-relaxed, f1-cs f1 - Classification of sentences with ADR, NA - data not available for download and analysis }% ? – не указана (наверное f1-approx match), f1-cs f1 - Classification of sentences with ADR}
\input{tables/adr_saturation.tex}
\label{tab:adr_saturation}
\end{table*}

\begin{table*}
\centering
\caption{Proportions of difficult cases in annotations. Discontinuous mentions are labeled phrases separated by words not related to it. A mention is overlapping if some of its words also labeled as another mention.}
\input{tables/types_saturation.tex}
\label{tab:types_saturation}
\end{table*}

\section{Corpus collecting} \label{sec:corpus_collecting}
\subsection{Corpus material} \label{subsec:corpus_material}
 
% \begin{table}[b]
% \caption{A sample post for ``Глицин'' (Glycine) from otzovik.com. English translate (original russian text)}
% \label{tab:sample_post_MEDcorp}
% \centering
% \input{tables/table5.tex}
% \end{table}

In this section, we report  the design of our corpus. Its basis were 2\,800 reviews from a medical section of the forum called OTZOVIK\footnote{OTZOVIK - Internet forum from which user reviews were taken - 
\url{http://otzovik.com}}
%~\cite{otzovik}
, which is dedicated to consumer reviews on medications. On that website there is a partition where users submit posts by filling special survey forms. The site offers two forms: simplified and extended, the latter being optional. In this form a user selects a drug name and fills out the information about the drug, such as: adverse effects experienced, comments, positive and negative sides, satisfaction rate, and whether they would recommend the medicine to friends. In addition, the extended form contains prices, frequency, scores on a 5-point scale for such parameters as quality, packing, safety, availability. A sample post for ``Глицин'' (Glycine) is shown in Table~\ref{tab:sample_post_MEDcorp}.

We used information only from the simplified form, since the users had rarely filled extended forms in their reviews. We considered only the fields Heading, General impression and Comment. Furthermore, some of the reviews are written in common language and do not follow formal grammar and punctuation rules. The consumers described not only their personal experience, but sometimes opinions of their family members, friends or others. 
%The main specifications of our corpus are shown in Table~\ref{tab:specification_of_MEDcorp}.


\subsection{Corpus Annotation} \label{subsec:corpus_annotation}
This section describes the corpus annotation methodology, including the markup composition, the annotation procedure with guidelines for complex cases, and software infrastructure for the annotation.
%В этой части представлено описание методологии разметки корпуса, включая состав аннотаций, процедуру разметки с указанием поведения в сложных случаях, а также программную инфрастуктуру для разметки.

\subsubsection{Annotation process}\label{subsec:annotation_process}
% Creating a reliably annotated corpus depends on experts a lot. 
The group of 4 annotators annotated  review texts using a guide developed jointly by machine learning experts and pharmacists. Two annotators were certified pharmacists, and the two others were students with pharmaceutical education. Reliability was achieved through joint work of annotators on the same set of documents, subsequently controlled by means of journaling. After the initial annotation round, the annotations were corrected three times with cross-checking by different annotators, after which the final decision was made by an expert pharmacist.
The corpus annotation comprised the following steps:
\begin{enumerate}
    \item First, a guide was compiled for the annotators. It included entities description and examples.
    \item Upon testing on a set of 300 reviews, the guide was corrected, addressing complex cases. During that, iterative annotation was performed, from 1 to 5 iterations for a text, while tracking for each text and each iteration the annotator questions, controller comments, and correction status.
    %Далее на 300 отзывах (на самом деле 2500 -- но это мы трудные -- про это не будем писать) было отлажено руководство по аннотации, добавлены сложные случаи. В процессе отладки формировалась таблица id текста и информация по итерации разметки. Количество итераций разметки при этом различалось по текстам (от 1 до 5). Для каждой итерации было зафиксировано: возникшие вопросы аннотатора, замечания проверяющего, статус исправлений.
    \item The resulting guide was used for annotating the remaining reviews. Two annotators marked up each review, and then a pharmacist checked the result. When complex cases were found, they were analyzed separately by the whole group of experts.
    %Итоговое руководство использовано для разметки оставшихся отзывов. Каждый отзыв размечался двумя анно татарами и проверялся фармацевтом. В процессе разметки встречались различные примеры, сложные для аннотирования. Их разбирали отдельно коллективно.
    \item The obtained markup was automatically checked for any inaccuracies, such as incomplete fragments of words selected as mentions, terms marked differently in different reviews, etc. Texts with such inaccuracies were rechecked.
    %После разметки всех отзывов была проведена автоматическая проверка на предмет наличия неточностей: выделение части слова тегом сущности с пропуском символов вначале или конце сущности, составление списка терминов размеченных по-разному в разных отзывах и др. Такие варианты перепроверялись отдельно.
\end{enumerate}

 To estimate an agreement between annotators   
14
we used the metric described by Karimi et al.  \cite{karimi2015cadec}.  According to this metric we calculated the agreement score for every document as the ratio between number of matched mentions and maximum number of mentions, annotated by one of the annotators in current document. Matched mentions are calculated depending on two flags $\alpha$ and $\beta$. The first one is the span strictness, it can be \textit{strict} or \textit{intersection}. If we do a strict spans comparison then only mentions with equal borders will be counted as matching, otherwise, we count mentions as matching if they at least are intersected each other. But every mention annotated by each annotator can be matched with the only mention annotated by the other annotator. $\beta$   is the tag strictness argument, which can be \textit{strict} or \textit{ignored}. It defines if we count matched mentions only when both annotators labeled them identically, or we count matched mentions only by borders, despite of labels. After calculation of agreement scores for all documents, we calculate the average score of the total agreement between two annotators. The average pairwise agreement among annotators is presented in table  \ref{tab:agreement}.
$$\mathrm{agreement}(i,j) = 100\frac{\mathrm{match}(A_i,A_j,\alpha,\beta)}{\mathrm{max}(|A_i|,|A_j|)}$$
Here $A_i$ and $A_j$ are lists of mentions annotated by annotators $i$ and $j$. $|A_i|$ and $|A_j|$ are numbers of elements in these lists.
\begin{table}
\caption{Average pair-wise agreement  between annotators} 
\label{tab:agreement} 
\input{tables/agreement_t.tex} 
\end{table}

The annotation was carried out with the help of the WebAnno-based toolkit, which is an open source project under the Apache License v2.0.
It has a web interface and offers a set of annotation layers for different levels of analysis.  Annotators proceeded according to the guidelines below.

\subsubsection{Guidelines applied in the course of annotation}
\label{subsec:Guidelines}
The annotation goal was to get a corpus of reviews in which named entities reflecting pharmacotherapeutic treatment are labelled, and annotate medication characteristic semantically. With this in mind, the objects of annotation were attributes of drugs, diseases (including their symptoms), and undesirable reactions to those drugs. The annotators were to label mentions of these three entities with their attributes defined below.
%Цель аннотирования -- получение корпуса примеров отзывов с выделенными и размеченными именованными сущностями, отражающими фармакотерапевтическую помощь и семантически аннотировать характеристику лекарственного препарата. Таким образом, корпус был аннотирован для создания атрибутов лекарственных препаратов, заболеваний (в том числе их симптомов) и нежелательных реакций от определенного препарата. We defined the entities of interest and manually annotated them. These entities and their definitions are introduced below.

\paragraph{Medication.} This entity includes everything related to the mentions of drugs and drugs manufacturers. Selecting a mention of such entity, an annotator had to specify an attribute out of those specified in Table~\ref{tab:medication}, thereby annotating it, for instance, as a mention of the attribute ``DrugName'' of the entity ``Medication'' .
In addition, the attributes ``DrugBrand'' and ``MedFrom'' were annotated with the help of lookup in an external source~\cite{SRD}.
%could be given optional attributes looked up in the register (described in Section~\ref{subsec:Normalization}), such as ``MedFrom'' in Table~\ref{tab:medication}.

\begin{table*}
\caption{Attributes belonging to the Medication entity}
\label{tab:medication}
\input{tables/medication.tex}
\end{table*}

\paragraph{Disease.} 
%This entity is associated with diseases or symptoms. It indicates the reason for taking the drug, the name of the disease and medications taken, indications for use.
This entity is associated with diseases or symptoms. It indicates the reason for taking a medicine, the name of the disease, and improvement or worsening of the patient state after taking the drug. 
Attributes of this entity are specified in Table~\ref{tab:Disease}.

\begin{table*}
\caption{Attributes belonging to the Disease entity} 
\label{tab:Disease} 
\input{tables/disease.tex}
\end{table*}

\paragraph{ADR.} This entity is associated with adverse drug reactions in the text. For example, one post said:
%\guillemotleft{}пропил курс и уже четвёртый день с 38.9 встать не могу\guillemotright{} (I finished the course, and I have a temperature of 38.9 and have been unable to get up for four days already). In this sentence phrases ``четвёртый день с 38.9'' and ``встать не могу'' (``temperature of 38.9'' and ``can’t get up) are annotated as ADR entities.
<<После недели приема Кортексина у ребенка начались судороги>> (After a week of taking Cortexin, the child began to cramp). In this sentence, the word ``судороги'' (``cramp'') is labeled as an ADR entity.

\paragraph{Note.} We use this entity when the author makes recommendations, tips, and so on, but does not explicitly state whether the drug helps or not. These include phrases like ``I do not advise''. For instance, the phrase <<Нет поддержки для иммунной системы>> (No support for the immune system) is annotated as a Note.

\begin{figure}
    \centering
    %\includegraphics[scale=0.3]{pictures/atc.png}
    \includegraphics[width=8.2cm]{pictures/webannoexample.png}
    \caption{Examples of markup. a) ``Spray Jadran Aqua Maris'', b) ``Rapid treatment of cold and flu'', c) ``IRS-19 + drink drops of Tonsilgon'' d) ``Amixin -- waste of time and money for treatment'', e) ``And once were these pills prescribed by my pediatrician''}
    \label{fig:webannoExample1}
\end{figure}

%All reviews can be divided into several levels on the complexity of the markup:
The typical situations that had to be handled during the annotation are the following:
\begin{enumerate}
    \item A simple markup, when a mention consists of 1 or more words and it related to a single attribute of entity. The annotators then just have to select a minimal but meaningful text fragment, excluding conjunctions, introductory words, and punctuation marks.
    \item Discontinuous annotation -- when mentions separated by words that are not part of it. It is then necessary to annotate mention parts and connect them. In such cases we use the ``concatenation'' relation. In the example (e) on Fig.~\ref{fig:webannoExample1} the words ``prescribed'' and ``pediatrician'' are annotated as a concatenated parts of mention of the attribute ``sourceInfoDrug''.
    %\item Intersecting annotations. An entity reflects a combination of several dissimilar values, and in this case several different tags are used. 
    \item Intersecting annotations. Words in a text can belong to mentions of different entities or attributes simultaneously. For example, in the sentence ``Rapid treatment of cold and flu'' (see Fig.~\ref{fig:webannoExample1}, example (b)), words ``cold'' and ``flu'' are mentions of attribute ``diseasename'', but at the same time the whole phrase is a mention of attribute ``BNE-Pos''. If a word or a phrase belongs to a mentions of different attributes or entities at the same time (for example, ``drugname'' and ``drugbrand''), it should be annotated with all of them: see, for instance, entity ``Aqua Maris'' in sentence ``Spray Jadran Aqua Maris'' (Fig.~\ref{fig:webannoExample1}, example (a)).
    \item Another complex situation is when an analogue (or, in some cases, several analogues) of the drugs are mentioned in a text, for example, when a customer wrote about a drug and then described an alternative that helped them. In this case, the ``Other'' attribute is used (example (c)).
    %Often there is such situation that a tag hierarchy occurs due to the fact that some subtags fall inside others. %-- не понятно!
\end{enumerate}

%Analysis shows that the most part of the entities has the second or the third complexity level requiring special attention during concepts definition and tags setting, 


Moreover, there often were author subjective arguments instead of explicit reports on the outcomes. We labeled that as a mention of entity ``Note''. For example, ``strange meds'', ``not impressed'', ``it is not clear whether it worked or not'', ``ambiguous effect'' (example (d) in Fig.~\ref{fig:webannoExample1}). 
%In addition, this entity is for the case of ambiguous reviews, i.e. when it is not possible to identify the entity  uniquely.


%\subsection{Normalization and classification of concepts}
\subsection{Classification based on categories of the ATC, ICD-10 classifiers and MedDRA terminology}
    \label{subsec:Normalization}
After annotation, in order to resolve possible ambiguity in terms we performed normalization and classification by matching the labeled mentions to the information from external official classifiers and registers. The external sources for Russian are described below.
% Под нормализацией мы понимаем сопоставление информации, встречающейся в тексте с информацией представленной во внешних официальных реестрах и классификаторах. Для русского языка среди таких внешних источников имеются:

\begin{itemize}
    \item 
    the 10-th revision of the International Statistical Classification of Diseases and Related Health Problems (ICD-10)~\cite{ICD-10} is an international classification system for diseases which includes 22 classes of diagnoses, each consisting of up to 100 categories. The ICD-10 makes it possible to reduce verbal diagnoses of diseases and health problems to unified codes.
    %Международная 	классификация болезней (МКБ-10) -- это 	международная система классификации 	болезней, которая содержит в себе 22 	класса медицинских диагнозов. МКБ позволяет сводить словесные формулировки 	диагнозов болезней и проблем, связанных со здоровьем, к унифицированным кодам.
    \item The Anatomical Therapeutic Chemical (ATC)~\cite{miller1995new} is an international medication classification system containing 14  anatomical main groups and 4 levels of subgroups. The ICD-10 and the ATC have a hierarchical structure, where ``leaves'' (terminal elements) are specified diseases or medications, and ``nodes'' are groups or categories. Every node has a code, which includes the code of its parent node.
    %Анатомо-терапевтически-химическая система классификации (АТХ или другое название АТС) содержащая классификацию лекарственных препаратов. -- Это международная система классификации лекарственных средств имеющая 14 подгрупп лекарственных препаратов. МКБ-10 и АТХ имеют иерархическую структуру листьями которой могут быть конкретные заболевания или препараты, а узлами — группы или категории.
    \item State Register of Medicinal Products (SRD)(``Государственный реестр лекарственных средств (ГРЛС)''~\cite{SRD} in Russian) is a register of detailed information about the medications certified in the Russian Federation. It includes possible manufacturers, dosages, dosage forms, ATC codes, indications, and so on.
    %Государственный реестр лекарственных средств (ГРЛС)\cite{SRD} -- реестр содержащий в себе подробную информацию о лекарственных препаратах, зарегистрированных в РФ. Включая возможных производителей, дозировки, лекарственные формы, коды АТХ, показания к применению и прочее.
    \item MedDRA\textregistered{}  the Medical Dictionary for Regulatory Activities terminology is the international medical terminology developed under the auspices of the International Council for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use (ICH)
\end{itemize}

Among the international systems of standardization of concepts, the most complete and large metathezaurus is UMLS, which combines most of the databases of medical concepts and observations, including MESH (and MESHRUS), ATC, ICD-10, SNOMED CT, LOINC and others. Every unique concept in the UMLS has an identification code CUI, using which one can get information about the concept from all the databases. However, within UMLS it is only the MESHRUS database that contains Russian language and can be used to associate words from our texts with CUI codes. 
% Среди международных систем стандартизации понятий наиболее полным и большим метатезаурусом является UMLS, который объединяет в себе большинство баз медицинских понятий и наблюдений, в том числе MESH (MESHRUS), ATC, ICD-10. Все уникальные понятия (концепты) в UMLS имеют идентифицирующий код CUI. По этому CUI можно получить информацию о концепте, которая содержится во всех базах. К сожалению в состав UMLS не входят русские базы кроме как перевод базы MESH и LOINC (Logical Observation Identifiers Names and Codes). База LOINC содержит в себе стандартизированные записи медицинских наблюдений. Её применение возможно в перспективе, однако требует подробного исследоhttps://www.overleaf.com/project/5cefc22d7fdd0931c9e13e5bвания формата записи этих наблюдений и состава базы. База MESH является словарём, предназначенным для индексации биомедицинской информации. Что делает её крайне полезной при анализе медицинских статей.


Classification was carried out by the annotators manually. For this purpose, we applied the procedure consisting of the following steps:
automatic grouping of mentions, manual verification of mention groups (standardization), matching the mention groups to the groups from the ATC and the ICD-10 or terms from MedDRA.

Automatic mentions grouping is based on calculating the similarity between two mentions by the Ratcliff/Obershelp algorithm~\cite{ratcliff1988pattern}, which is based on searching two strings for matching substrings. In the course of the analysis, every new mention is added to one of the existing groups $G$ if the mean similarity between the mention and all the group items is more than 0.8 (value deduced empirically), otherwise a new group is created. The $G$ set is empty at the start, and the first mention creates a new group with size 1. Each group is named by its most frequent mention. Next, the annotators manually check and refine the resulting set, creating larger groups or renaming them. Mentions of drug names were standardized according to State Register of Medicinal Products. That gave us 550 unique drug names mentioned in corpus.
% Нормализация проводилась вручную силами аннотаторов. Для этого разработана процедура, состоящая из нескольких этапов: автоматическая группировка понятий (стандартизация), ручная проверка групп понятий, сопоставление групп понятий с терминами баз АТХ и МКБ-10. Автоматическая группировка понятий выполняется на основе алгоритма Ратклиффа\Обершелла расчета расстояния между понятиями. Он основан на поиске совпадающих подстрок при сравнении двух строк. Анализируя множество сущностей, каждая новая сущность добавляется в одну из существующих групп G, если средняя близость со всеми сущностями этой группы больше 0.8 (подобрано эмпирически), иначе создается новая группа. Множество G изначально пустое и первая сущность сразу создаёт группу с единственной сущностью. Для обозначения каждой группы выбирается понятие, которое наиболее часто встречается в корпусе. Далее полученное множества групп проверяется и дорабатывается аннотатарами для создания более крупных групп или для изменения их названий.

After that, the group names for attributes ``Diseasename'', ``Drugname'' and ``Drugclass'' are manually matched with ICD-10 and ATC terms to assign term codes from the classifiers. As a result, 247 unique ICD-10 codes were matched against the 765 unique phrases, annotated as attribute ``Diseasename''; 226 unique ATC codes matched the 550 unique drug names; and 70 unique ATC codes corresponded to 414 unique phrases, annotated as ``Drugclass''. Some drug classes that were mentioned in corpus (such as homeopathy) did not have a corresponding ATC code, and were aggregated according to their anatomical and therapeutic classification in the SRD.

Standardized terms for ADR and Indications were manually matched with low level terms (LLT) or prefered terms (PT) from MedDRA. In table~\ref{tab:info_collected_corpus} we show the numbers of Unique PT terms that were matched with our mentions.

%После этого, полученные названия групп понятий, отмеченные тегами Diseasename и Drugname, вручную сопоставлялись с терминами АТХ и МКБ-10. В результате им назначались коды терминальных элементов или категорий соответствующих классификаторов. Например, в случаях, когда речь идёт о неуточнённом ОРВИ заболевании, назначался код категории заболеваний ОРВИ из МКБ-10. По итогам этой процедуры для сущностей типа Diseasename было сопоставлено 140 уникальных МКБ-10 кодов, упоминаемых 1333 раза в корпусе, и для сущностей типа Drugname сопоставлен 171 уникальный код АТХ с суммарным количеством упоминаний равным 2325.



\subsection{Statistics of the collected corpus}\label{subsec:Statistics_of_corpus}
We used UDPipe\cite{straka2016udpipe} package to parse the reviews, in order to get sentence segmentation, tokenization and lemmatization. Given this, we calculated that average number of sentences for the reviews is 10, average number of tokens is 152 (with a standard deviation of 44), average number of lemmas is 95 (standard deviation equals to 23). TTR (type/token ratio) was calculated as the ratio of the unique lemmas in a review to the amount of tokens in it. Average TTR for all reviews equals to 0.64.

Detailed information about the annotated corpus is presented in Table~\ref{tab:info_collected_corpus} including:
\begin{enumerate}
    \item The number of mentions for every attribute (``Mentions -- Annotated'' column in the table).
    \item The number of unique classes from classifiers or unique normalized terms described in Section~\ref{subsec:Normalization} matched with our mentions (``Mentions -- Classification \& normalization'').
    \item The number of words belonging to mentions of the attribute (``Mentions -- Number words in the mentions'').
%    \item We introduce the concept of entries -- the number of reviews that contain a standardized mention -- so as to be able to compare drugs and diseases by their coverage in the reviews, not taking into account multiple repetitions of the same standardized mention in a review. The ``Mentions -- Total entries'' column shows the total number of entries over all the standardized mentions of the corresponding attribute (this number can exceed the total number of reviews because some reviews contain several different standardized mentions).
    \item The number of reviews containing any mentions of the corresponding attribute (``Mentions -- Reviews coverage'').
    %Number of words related to each type of annotation and mapped to the MESHRUS cui codes. Where total number shows count of words and unique number shows unique CUI codes.
\end{enumerate}

\begin{table*}
\centering
\caption{General information about the collected corpus.} \label{tab:info_collected_corpus}
\input{tables/general_inf_about_corpus_2.tex}
\end{table*}

%Statistics computation included the following data: number of annotations (``Number of entities: annotated'' column), number of unique entities (``Number of entities: standardized'' column). 
%Unique entities were counted with preliminary use of the normalization procedure described earlier in Section~\ref{par:meshrus_features}.


%The last two columns contain results of the automated normalization of the MESHRUS dictionary. CUI index was assigned to individual words without markup, thus the numbers show how many times words with the index were in an entity and how many unique CUI indices were in a label of distinct type. The normalization procedure results are shown in a Table~\ref{tab:info_collected_corpus}.

The corpus contains consumer posts on drugs, mentioned 8 236 times and related to 226 ATC codes.  The most popular 20\% of the ATC codes (by the number of reviews with corresponding Drugname mentions) include 45 different codes which mentions appears in 2\,614 reviews (93\% of all reviews). Among them, 20 ATC codes were reviewed in more then 50 posts (2511 posts in total).
%Drugs statistics is shown in a Table~\ref{tab:drug_stat}.
% On a figure~\ref{} entries distribution is shown for the drugs with number of entries exceeding 20 in the annotated reviews corpus.

% of Anatomical Therapeutic Chemical (ATC) classification system~\cite{miller1995new}.
%The most popular groups of drugs are antiviral (74 drugs) and sedative (39 drugs). Partitions of the antiviral drugs presented in corpus are: 39.56\% (``Виферон'' (Viferon) (16.59\%), ``Ингавирин'' (Ingavirin) (13.02\%) and ``Ацикловир'' (Acyclovir) (10.92\%)) and partitons of the sedative drugs are: 16.8\% (``Глицин'' (Glycine) (32.51\%), ``Афобазол'' (Afobazol) (17.73\%), ``Корвалол'' (Corvalol) (7.88\%) and other).
The most popular ATC codes from 2nd level are: L03 ``Immunostimulants'' - 662 reviews (which is 23.6\% of corpus), J05 ``Antivirals for systemic use'' - 508 (18.5\%) reviews, N05 ``Psycholeptics'' - 449 (16.0\%), N02 ``Analgesics'' - 310 (11.1\%), N06 ``Psychoanaleptics'' - 294 (10.5\%). Most popular drugs among immunostimulants by the reviews count are: Anaferon (144 reviews), Viferon (140), Grippferon (71). Most popular antivirals for systemic use are following: Ingavirin (99), Kagocel (71) and Amixin (58).
%The most popular drug classes mentioned in corpus are antiviral (74 drugs) and sedative (39 drugs), with parts of entries from all drug class attribute entries equals to 43.54\% and 22.69\%.
%The most popular drug classes mentioned in corpus are antiviral (74 drugs) and sedative (39 drugs). The sums of entries of these drugs have parts from all drug name attribute entries equal to 48.52\% and 17.07\%  correspondingly.
%The proportions of entry counts of the most popular drugs to the total number of entries of antiviral drugs are: ``Виферон'' (Viferon) (6.9\%), ``Ингаверин'' (Ingavirin) (5.41\%) and ``Ацикловир'' (Acyclovir) (4.54\%). For the sedative drugs, these are: ``Глицин'' (Glycine) (16.38\%), ``Валериана'' (Valeriana) (14.39) ``Афобазол'' (Afobazol) (8.93\%).

The proportions of reviews about domestic drugs and foreign to the total number of reviews are 44.9\% and 39.7\% respectively. The remaining documents (15.4\%) contains mentions of multiple drugs both domestic and foreign or mentions of drugs which origin the annotators could not determine. Among the domestic drugs are following: Anaferon (144 reviews), Viferon (140), Ingavirin (99) and Glycine (98). Examples of mentioned foreign drugs: Aflubin (93), Amison (55),  Antigrippin (51) and Immunal (42).
%The proportions of domestic drugs and foreign drugs to the total number of drug entries are 38.8\% and 61.2\%, respectively. The foreign drugs with the highest entry percentages are: ``Афлубин'' (Aflubin) (2.37\%), ``Иммунал'' (Immunal) (1.22\%), ``Амизон'' (Amison) (1.18\%), and ``Антигриппин'' (Antigrippin) (1.18\%). The domestic ones are ``Виферон'' (Viferon) (3.34\%), ``Анаферон'' (Anaferon) (3.17\%), ``Глицин'' (Glycine) (2.79\%), and ``Ингавирин'' (Ingavirin) (2.62\%). 

%\begin{table}
%\centering
%\caption{Drugs statistics.}
%\label{tab:drug_stat}
%\input{tables/table_drugs_stat.tex}
%\end{table}

%Regarding disease names, the most frequent ones are ``острые респираторные инфекции верхних и нижних дыхательных путей'' (acute respiratory infections of the upper and lower respiratory tract) (554 entries); ``грипп и пневмония'' (influenza and pneumonia) (262 entries); ``вирусные инфекции, характеризующиеся поражениями кожи и слизистых оболочек'' (viral infections characterized by lesions of the skin and mucous membranes) (108); ``другие вирусные болезни'' (other viral diseases) (38) and others. The top 5 disease categories from the ICD-10 by the entries count are presented in Fig.~\ref{fig:Top_5_diseases}.

Regarding diseases, the most frequent ICD-10 top level categories are ``X - Diseases of the respiratory system'' (1122 reviews); ``I - Certain infectious and parasitic diseases'' (300 reviews); ``V - Mental and behavioural disorders'' (170 reviews); ``XIX - Injury, poisoning and certain other consequences of external causes'' (82 reviews). The top 5 low level codes from the ICD-10 by the number of reviews are presented in Fig.~\ref{fig:Top_5_diseases}.

\begin{figure}
    \centering
    %\includegraphics[scale=0.3]{pictures/atc.png}
    \includegraphics[width=8.5cm]{pictures/diseaseNameClass_11032021}
    %\caption{Top 5 disease categories from the ICD-10 by the number of entries in our corpus. F32.9: Unspecified depressive episode; J20.9: Unspecified acute bronchitis; B00-B09: Viral infections characterized by lesions of the skin and mucous membranes; J10-J18: Influenza and pneumonia; J00-J06: ARVI.}
    \caption{Top 5 low-level disease categories from the ICD-10 by the number of reviews in our corpus. J00-J06 - Acute upper respiratory infections, J11 - Influenza with other respiratory manifestations, virus not identified, B00 - Herpesviral [herpes simplex] infections, F51.0 - Nonorganic insomnia, T78.4 - Allergy, unspecified}
    \label{fig:Top_5_diseases}
\end{figure}

%In addition, adverse drug reactions (ADR) were considered. Undesirable reactions statistics are presented in Table \ref{tab:Undesirable_react_stat}.

%\begin{table}
%\centering
%\caption{Adverse drug reactions (ADR) statistics.}
%\label{tab:Undesirable_react_stat}
%\input{tables/table_undesirable_reactions_stat.tex}
%\end{table}


% Figure N. Top 5 diseases by entries. F32.9-Депрессивный эпизод неуточненный; J20.9-Острый бронхит неуточненный; B00-B09-Вирусные инфекции, характеризующиеся поражениями кожи и слизистых оболочек; J10-J18-Грипп и пневмония; J00-J06-ОРВИ.

%The basis for the acquisition and use of drugs by users is information from ``professional'' and ``non-professional'' sources.
%Analysing the consumers' motivation to acquire and use drugs (``sourceInfoDrug'' attribute) showed that the major part of the drugs, 793 (86.68\%) entries, were used on professional recommendations: medical or pharmaceutical specialists. 120 (13.31\%) entries of drug usage referred to advice of non-professional sources: relatives, friends, advertisement and so on.
Analysing the consumers' motivation to acquire and use drugs (``sourceInfoDrug'' attribute) showed that review authors mainly mention using drugs based on professional recommendations. 989 reviews contains references of doctor prescriptions, 262 - refers to pharmaceutical specialists recommendations and 252 - doctor recommendations. Some reviews reports about using drugs recommended by relatives (207 reviews), advertisement (97) or internet (15).

%The distribution heatmap of entry perсentages for different sources for the 20 most popular drugs is presented in Fig.~\ref{fig:top15_drug_heatmap}.
The heatmap, presented on Fig.~\ref{fig:top15_drug_heatmap}, shows percentages of reviews where popular drugs were co-occurred with different sources (sources were manually merged into 5 groups by annotators).
\begin{figure*}
    \centering
    %\includegraphics[width=18cm]{pictures/heatmap3.png}
    \includegraphics[width=12cm]{pictures/Drug_source.png}
    \caption{The distribution heatmap of reviews percentages for different sources of information for the 20 most popular drugs. The number in a cell means the percentage of reviews with the drug and particular source to the total number of reviews with this drug. If there were several different sources mentioned, it counted as ``mixed'' source}
    \label{fig:top15_drug_heatmap}
\end{figure*}
% Figure N. The distribution heatmap of entries parts from different sources for the 20 most popular drugs соединить в колонки друзья и знакомые; колонки фармацевт и провизор. Оставить 20 препаратов (или столько сколько потребуется).
%It could be seen that most recommendations are coming from professionals. For example ``Изопринозин'' (Isoprinosine) (used in 69.23\% cases by medical recomendations), ``Афлубин'' (Aflubun) (48.21\%), ``Анаферон для детей'' (Anaferon for children) (47.30\%) and others. However, for such drugs as ``Иммунал'' (Immunal) (14.29\%) or ``Парацетамол'' (Paracetamol) (7.41\%) the rate of usage on the advice of patients' acquaintances is close to doctors' recommendations or higher. ``Кагоцел'' (Kagocel) has the highest percentage for advertisement as the source (9.3\%) compared to other drugs.
It could be seen that most recommendations are coming from professionals. For example Isoprinosine (used in 65.85\% cases by medical prescription), Aflubun (44.09\%), Anaferon (47.30\%) and others. However, for such drugs as Immunal (11.9\%) or Valeriana (9.18\%) the rate of usage on the advice of patients' acquaintances is close to doctors' recommendations or higher. Amizon (12.73\%) and Kagocel (11.27\%) have the highest percentage for mass media (advertisement, internet and other) as the source  compared to other drugs.
% ``Генферон лайт'' (Genferon light) (used in 82.61\% cases by medical recommendations), ``Кортексин'' (Cortexin) (54.55\%), ``Тенотен'' (Tenoten) (52.17\%) and others
%It could be seen that professionals recommend only the limited set of drugs, including  ``Генферон лайт'' (Genferon light) (used in 82.61\% cases by medical recomendations), ``Кортексин'' (Cortexin) (54.55\%), ``Тенотен'' (Tenoten) (52.17\%) and others. But such drugs as ``Иммунал'' (Immunal) (14.29\%) or ``Афобазол'' (Aphobazolum) (8.33\%) used on the advice of patients acquaintances with close rate to doctors recommendations. ``Кагоцел'' (Kagocel) has the most high percentage for advertisement as source (9.3\%) compared to other drugs.

The distribution of the tonality (positive or negative) for the sources of information is presented in Fig.~\ref{fig:Distribution_drug_tonality}. A source is marked as ``positive'' if positive dynamic is appeared after the use of drug (i.e. review includes ``BNE-pos'' attribute). ``Negative'' tonality is marked if negative dynamic or deterioration in health has taken place or drug has had no effect (i.e. ``Worse'', ``ADE-Neg'' or ``NegatedADE'' mentions appear). Reviews with both effects were not taken into account. It follows from the diagram that drugs recommended by doctors or pharmacists are mentioned more often as having positive effect, while using drugs based on an advertisement often leads to deterioration in health.
\begin{figure}
    \centering
    %\includegraphics[width=8.5cm]{pictures/tonalsource.eps}
    \includegraphics[width=8.5cm]{pictures/source_tonality.png}
    
    \caption{Distribution of the tonality for the different sources. Number in brackets shows reviews count with the source of information, including reviews without reported effects or neutral reviews (with both good and bad effects)}
    \label{fig:Distribution_drug_tonality}
\end{figure}
% Figure N. Distribution of the tonality for the different sources.

% Figure N. Distribution of the undesirable reactions entries for the 29 most popular drugs.
%Diagrams in Fig.~\ref{fig:effects} show parts of reviews where drugs were mentioned along with labeled effects from all reviews with this drug (only top 20 drugs by entries count presented on figure). The following drugs have largest parts for ADR in  reviews: immunomodulator -- ``Изопринозин'' (Isoprinosine) (57.7\%), sleeping pills -- ``Донормил'' (Donormil) (45.5\%); antiviral -- ``Амизон'' (Amizon) (35.7\%), ``Генферон лайт'' (Genferon Light) (34.8\%), ``Амиксин'' (Amiksin) (30\%), etc.
Diagrams in Fig.~\ref{fig:effects} show parts of reviews where popular drugs were mentioned along with labeled effects. The following drugs have largest parts for ADR in reviews: immunomodulator -- ``Isoprinosine'' (48.8\% of reviews with this drug contains mentions of ADR), antiviral ``Amixin'' (40.0\%),  tranquilizer -- ``Aphobazolum'' (37.7\%), antiviral -- ``Amizon'' (36.4\%), antiviral -- ``Rimantadine'' (36.3\%).

%Users mention that some drugs causing negative dynamics after start or some period of using it (ADE-Neg). Examples of such drugs are ``Донормил'' (Donormil) (13\%), ``Кортексин'' (Cortexin) (9\%), ``Генферон лайт'' (Genferon Light) (8\%), ``Амиксин'' (Amiksin) (6\%), ``Глицин'' (Glycine) (6.6\%). Also homeopathic drugs were marked as the ones with no effect: ``Анаферон детский'' (Anaferon for children) (64\%), ``Анаферон'' (Anaferon) (54.6\%), ``Тенотен'' (Tenoten) (52\%).
%"Ингавирин" (Ingavirin) (54.8\%)
Users mention that some drugs causing negative dynamics after start or some period of using it (ADE-Neg). Examples of such drugs are ``Anaferon'' (3.5\% of reviews with this drug mention ADE-Neg effects), ``Viferon'' (2.1\%), ``Glycine'' (4.1\%), ``Ergoferon'' (3.6\%). 

%According to reviews some of the drugs causes deterioration in health after taking the course (``Worse'' label): immunomodulator -- ``Изопринозин'' (Isoprinozine) (15\%), ``ИРС19'' (IRS19) (13\%), ``Амиксин'' (Amiksin) (10\%), ``Парацетамол'' (Paracetamol) (7\%) and other.
According to reviews some of the drugs causes deterioration in health after taking the course (``Worse'' label): immunomodulator -- ``Isoprinosine'' (12.2\%), antiviral -- ``Ingavirin'' (10.1\%), ``Ergoferon'' (9.1\%) and other.
\begin{figure*}
    \centering
    \includegraphics[width=17cm]{pictures/Drugname_effect.png}
    \caption{Distributions of labels of effects reported by reviewers after using drugs. Top 20 drugs by the reviews count are presented. The number in brackets is the number of reviews with mentions of a drug. Diagrams show part of reviews mentioning a specific type of effect from the total amount of reviews with the drug}
    \label{fig:effects}
\end{figure*}

%Corpus contains consumer posts on 384 drugs mentioned in corpus 2360 times and related to the 36 drug classes according to classification from State Register of Drugs~\cite{SRD}.% of Anatomical Therapeutic Chemical (ATC) classification system~\cite{miller1995new}.
%The most popular groups of drugs are antiviral (74 drugs) and sedative (39 drugs). Partitions of the antiviral drugs presented in corpus are: 39.56\% (``Виферон'' (Viferon) (16.59\%), ``Ингавирин'' (Ingavirin) (13.02\%) and ``Ацикловир'' (Acyclovir) (10.92\%)) and partitons of the sedative drugs are: 16.8\% (``Глицин'' (Glycine) (32.51\%), ``Афобазол'' (Afobazol) (17.73\%), ``Корвалол'' (Corvalol) (7.88\%) and other).
%The most popular drug classes mentioned in corpus are antiviral (74 drugs) and sedative (39 drugs), with parts of entries from all drug class attribute entries equals to 43.54\% and 22.69\%.
%The most popular drug classes mentioned in corpus are antiviral (74 drugs) and sedative (39 drugs). The sums of entries of these drugs have parts from all drug name attribute entries equal to 48.52\% and 17.07\%  correspondingly.
%Partitions of entries of the most popular drugs from all entries of antiviral drugs presented in corpus are: ``Виферон'' (Viferon) (6.9\%), ``Ингаверин'' (Ingavirin) (5.41\%) and ``Ацикловир'' (Acyclovir) (4.54\%) and partitions of the sedative drugs are: ``Глицин'' (Glycine) (16.38\%), ``Валериана'' (Valeriana) (14.39) ``Афобазол'' (Afobazol) (8.93\%).



This corpus is used further to get a baseline accuracy estimate for the named entity recognition task.

\subsection{Coreference annotation}\label{subsec:Coreference_annotation}
To begin with, we used a state-of-the-art neural network model for coreference resolution~\cite{joshi2019bert}, and adapted it to Russian language by training on the corpus AnCor-2019.  After this we predicted coreference for reviews in our corpus. We chose 91 reviews which had more that 2 different drug names and disease names (after manual grouping described in \ref{subsec:Normalization})and more than 4 coreference clusters and 209 reviews which had more that 2 different drug names and more that 2 coreference clusters. These 300 reviews we gave to our annotators for manual checking of coreference clusters, predicted by model.

The annotators had guideline for coreference and a set of examples. According to guidelines they supposed to pay attention to mentions annotated with pharmacological types, pronouns and words typical for references (e.g. ``such'', ``former'', ``latter''). They didn't annotate as coreference following things:
\begin{itemize}
    \item mentions of reader (``I wouldn't recommend you to buy it if you don't want to waste money'');
    \item  split antecedents - when 2 or more mentioned entities also mentioned by a common phrase (``I tried Coldrex and after a while i decided to buy Antigrippin. Both drugs usually help me.'');
    \item generic mentions - phrases that describe some objects or events(e.g. ``Many doctors recommend this medication. Since I respect the opinion of doctors I decided to buy it.'' - doctors are not coreferent mentions);
    \item phrases that gives definitions to other (``Valeriana is a good sedative drug that usually helps me'' - ``Valeriana'' and ``sedative drug'' are not coreferent mentions).
\end{itemize}

The table \ref{tab:coref_number_comparison} shows the number of coreference clusters and mentions in 300 drug reviews from our corpus compared to corpus AnCor-2019. It should be noted that not all coreference mentions correspond to mentions of our main entity annotation, sometimes a single coreference mention can unite multiple medical mentions,or connect pronouns that are not involved in medical annotations.  The table \ref{tab:coref_medtypes} represents the number of medical mentions of various types that intersect coreference mentions. This corpus is used further to get a baseline accuracy estimate for the named entity recognition task.
%There is a problem, that  some reviews present user opinion  concerning   the entities of a definite tag in relation to more than one object of real world:  a drug or desease, or the other entity. Therefore , in addition to mentions annotations, we made a  relations annotation of the part of our corpus. Currently, there are only two corpora with coreference annotations for Russian language: Ru-Cor [49] and corpus from shared task AnCor-2019 [17].The latter is and extension of the first one.  As for the methods the state-of-the-art approach is based on neural network trained end-to-end to solve two task at the same time: mention extraction and relations extraction. This approach was firstly introduced in [24]and have been used in several papers[25, 16, 60, 15, 51]with some modifications to get higher scores on the coreference corpus CoNLL-2012 [36]. We used implementation from [16] that gives average f1 score 76.9 for CoNLL-2012 coreference task. 
% Помимо разметки сущностей мы также решили добавить в корпус разметку отношений. Так как в некоторых отзывах присутствует описание нескольких препаратов, применение которых имело разные последствия мы решили добавить разметку кореференции. В настоящее время для русского языка есть 2 корпуса с разметкой на кореференцию: RuCor[http://rucoref.maimbava.net/] и корпус с соревнования AnCor-2019[http://www.dialog-21.ru/media/4689/ budnikovzverevamaximova 2019 evaluating anaphora coreference resolution.pdf]. Причем второй является расширением и доработкой первого. Для начала мы провели предварительную разметку корпуса. Мы взяли state-of-the-art метод разметки кореференции [https://arxiv.org/pdf/1908.09091.pdf], обучили её на корпусе AnCor-2019. После чего разметили наши отзывы получившейся моделью. Далее мы провели анализ и выделили 91 отзыв в которых было более 2 разных названий препаратов или болезней (после ручной группировки) и более 4 цепочек кореференции, и 209 отзывов в которых было более двух названий препаратов и более двух корефрентных цепочек. Эти 300 отзывов мы дали разметчикам для исправления ошибок автоматической разметки. В таблице \ref{tab:coref_number_comparison} приведено количество получившихся кореферентных цепочек и упоминаний в нашем корпусе в сравнении с корпусом AnCor-2019. Стоит отметить, что  кореферентные упоминания не всегда соответствуют упоминаниям основной разметки, часто одно кореферентное упоминания может объединять несколько фармацвтических упоминаний. Или выделять местоимения, которые никак не задействованы в основной разметке. В таблице \ref{tab:coref_medtypes} приведено количество фармацевтических упоминаний разных типов, задействованных в кореферентных цепочках.

\begin{table}
\centering
\caption{Number of coreference chains and mentions compared to other Russian coreference corpus} \label{tab:coref_number_comparison}
\input{tables/coref_number_comparison.tex}
\end{table}

\begin{table}
\centering
\caption{Mentions types involved in coreference chains} \label{tab:coref_medtypes}
\input{tables/coref_medtypes.tex}
\end{table}

\section{Machine learning methods}\label{sec:Methods} %Methods of entities detection
\subsection{Entities detection problem}\label{subsubsec:Methodology} % Problem description
%КАК МЫ ПОДХОДИМ К ЗАДАЧЕ ВЫДЕЛЕНИЯ-ОПРЕДЕЛЕНИЯ ИМЕНоВАННЫХ СУЩНОСТЕЙ.

%\subsection{Models}\label{subsec:Model}
    \label{Model}
%mention detection and classification
We consider the problem of named entity recognition as a multi-label classification of tokens -- words and punctuation marks -- in sentences. % For each of the three entities -- ADR, Medication and Disease -- its own neural network is trained. That way,
  Phrases of different entities can intersect, so that one word can have several tags. 

The output for each token is a tag in the BIO format: the ``B'' tag indicates the first word of a phrase of the сonsidered entity, the ``I'' tag is used for subsequent words within the mention, and the ``O'' tag means that the word is outside of an entity mention.

% The input to the model is a sequence of features extracted from tokens. We use part-of-speech tags, word vector representations, common features and coded word characters as an input to the model %~\ref{fig:lstm}
%in all experiments and consider it the basic set of features. Further, in the tables with experiments, we will indicate only additional features and type of word vector representation model, implying that the basic features are present. 


%БЫЛО ТАК: 
%%To set the accuracy level of entity recognition in our corpora we used two models: the first(Model A)based on the input, which  is a sequence of features extracted from tokens: part-of-speech tags,  common features, word vector representations   FastText~\cite{bojanowski2017enriching}, ELMo~\cite{peters2018deep},  BERT,  words coded by characters with supervised neuronet of  Long short-term memory  (LSTM~\cite{hochreiter1997long}); the second (Model B) is a multi-model combining the  pretrained multilingual language model XLM-RoBERTa-large~\cite{xlm_conneau2019unsupervised} and  the  LSTM neural netwоrk  with features: morphological and dictionary. This model  allows  the multi-tagging output,  that is appropriated to our corpus. 


%ПРЕДЛАГАЮ ТАК: 
To set the accuracy level of entity recognition in our corpora we used two methods. The first (Model A) was based on BiLSTM neuralnet topology with different feature representation of input text: dictionaries, part of speech tags and several methods of word level representations, incl. FastText~\cite{bojanowski2017enriching}, ELMo~\cite{peters2018deep}, BERT, words character LSTM coding, etc. 
The second (Model B) was a multi-model combining the  pretrained multilingual language model XLM-RoBERTa~\cite{xlm_conneau2019unsupervised} and the LSTM neural network  with several most effective features. 
%Детали реализации обоих методов с описанием используемых признаков описаны далее.
Details of the implementation of both methods with a description of the used features are presented below. 

\subsection{Used features}\label{subsubsec:features} % and language models
\paragraph{Tokenization and Part-of-Speech tagging.} 
To preprocess the text we used UDPipe~\cite{straka2016udpipe} tool. After parsing each word get 1 of 17 different parts of speech. They are represented as a one-hot vector and used as an input for the neural network model. For model B, the text was segregated on phrases using UDPipe version 2.5. Long phrases splitted up into 45 word chunks.

\paragraph{Common features.} They are represented as a binary vector of answers to the following questions (1 if yes, 0 otherwise):
\begin{itemize}
    \item Are all letters capital?
    \item Are all letters in lowercase?
    \item Is the first letter capital?
    \item Are there any numbers in the word?
    \item Does more than a half of the word consist of numbers?
    \item Does the entire word consist of numbers?
    \item Are all letters Latin?
\end{itemize}


%Бинарный вектор из следующих признаков токенов (да -- 1, нет -- 0): все буквы заглавные?; все буквы строчные?; первая буква заглавная?; есть числа в слове?; больше половины слова из чисел?; слово состоит из чисел?; все буквы английские?. Для каждого слова с помощью библиотеки udpipe~\cite{straka2016udpipe} мы получаем его часть речи. Всего для русского языка udpipe выделяет 17 типов частей речи, которые представляются one-hot-encoding представлением и отправляются на вход нейронной сети.

\paragraph{Emotion markers.} Adding the frequencies of emotional words as extra features is motivated by the positive influence of these features on determining the author's gender~\cite{SueroMontero201498}.
%Введение в качестве дополнительных признаков частот эмоциональных слов обусловлено положительным влиянием данных признаков на определение пола автора, как показано, например, в~\cite{SueroMontero201498}.
Emotional words are taken from the dictionary%\footnote{Information Retrieval System ”Emotions and feelings in lexicographical parameters: Dictionary emotive vocabulary of the Russian language” - \url{http://lexrus.ru/default.aspx?p=2876}},
~\cite{emofeelDicts} 
which contains 37 emotion categories, such as <<Anxiety>>, <<Inspiration>>, <<Faith>>, <<Attraction>>, etc.
%Учитываются слова, содержащиеся в определённых эмоционало-эмотивных словарях, представленных в~\cite{emofeelDicts}, Всего словарями представлены 37 категорий отражающих эмоции, как например <<Беспокойство>>, <<Недовольство>> и тд;
On the basis of the $n$ available dictionaries, an $n$-dimensional binary vector is formed for each word, where each vector component reflects the presence of the word in a certain dictionary.

In addition, this word feature vector is concatenated with emotional features of the whole text. These features are LIWC and psycholinguistic markers.

The former is a set of specialized English Linguistic Inquiry and Word Count (LIWC) dictionaries~\cite{tausczik2010psychological}, adapted for the Russian language by linguists~\cite{litvinova2017deception}. The LIWC values are calculated for each document based on the occurrence of words in specialized psychosocial dictionaries. 
%Помимо эмотивных словарей используется набор специализированных словарей LIWC (англ. Linguistic Inquiry and Word Count), представленных в работе~\cite{Tausczik201024}. Значения LIWC рассчитываются для каждого документа на основе встречаемости слов из специализированных психосоциальных словарей, которые описывают лингвистические категории (количество слов определенных частей речи, некоторые лексико-тематические группы, частоту знаков препинания и т. д.). LIWC были адаптированы для русского языка лингвистами~\cite{litvinova2017deception}.

% На основе имеющихся словарей для каждого слова формируется бинарный вектор размерностью N - число словарей, в котором отражено наличие данного слова в определенном словаре.
Psycholinguistic text markers~\cite{sboev2015quantitative} reflect the level of the emotional intensity of the text. They are calculated as the ratio of certain frequencies of parts of speech in the text. We use the following markers: the ratio of the number of verbs to the number of adjectives per unit of text; the ratio of the number of verbs to the number of nouns per unit of text; the ratio of the number of verbs and verb forms (participles and adverbs) to the total number of all words; the number of question marks, exclamation points, and average sentence length. The combination of these features are referred to as "ton" in Table~\ref{tab:feat_top_exp}.
% Психолингвистические маркеры текста, представленные в работе~\cite{Sboev2015307}, рассчитываются как отношение определенных частот частей речи в тексте. В работе использованы следующие маркеры: соотношение количества глаголов к количеству прилагательных в единице текста; соотношение количества глаголов к количеству существительных в единице текста; отношение количества глаголов и глагольных форм (причастий и деепричастий) к общему количеству всех слов. Интегральная оценка эмотивности текста на основе психолингвистических маркеров; число вопросительных и восклицательных знаков, точек, средняя длина предложения.

\paragraph{Dictionaries.} 
The following dictionaries from open databases and registers are used as additional features for the neural network model.
%В данной секции описываются алгоритмы получения словарей, которые составляются на основе открытых баз данных/справочников и далее используются как дополнительный признак для модели нейронной сети.
%Схема сопоставления словам корпуса индексов из базы UMLS.
\begin{enumerate}
    \item Word vectors formed on base of the MESHRUS thesaurus as described in Appendix~\ref{subsec:meshrus_features}. The two approaches described in that section are referred to as MESHRUS and MESHRUS-2. The resulting CUI codes are encoded with one-hot representation.
    \item \textbf{Vidal}. 
    For each word, a binary vector is formed, which reflects belonging to categories from the Vidal medication handbook~\cite{Vidal2019}: adverse effects, drug names in English and Russian, diseases. The dataset words are mapped to the words or phrases from the Vidal handbook. To establish the categories, the same approach as for MESHRUS is used. The difference is that instead of setting indices for every word (as CUI in the UMLS) we assign a single index to all words of the same category. That way, words from the dataset are not mapped to special terms, but checked for category relations.
\end{enumerate}


\begin{figure}
\centering
\includegraphics[scale=0.37]{pictures/lstm_scheme.jpg}
\caption{The main architecture of the network. Input data goes to bidirectional LSTM, where the hidden states of forward LSTM and backward LSTM get concatenated, and the resulting vector goes to fully-connected layer with size 3 and SoftMax activation function. The output $p_{1}$, $p_{2}$, and $p_{3}$ are the probabilities for the word to belong to the classes B, I, and O, i.\,e. to have B, I, or O tag.}
\label{fig:lstm}
\end{figure}




\subsection{Word vector representations} 
It is the representation of word by a vector in a special space where words with similar meanings are close to each other. 
The following models were used: FastText~\cite{bojanowski2017enriching}, ELMo (Embeddings from Language Model)~\cite{peters2018deep}, and BERT (Bidirectional Encoder Representations from Transformer)~\cite{devlin2018bert}, XLM-RoBERTa~\cite{xlm_conneau2019unsupervised}.
%The following models were compared: FastText~\cite{bojanowski2017enriching}, ELMo (Embeddings from Language Model)~\cite{peters2018deep}, and BERT (Bidirectional Encoder Representations from Transformer)~\cite{devlin2018bert}. 
%Для выбора лучшей модели векторного представления слов(эмбеддинга) был проведен ряд экспериментов. Рассматривались модели FastText~\cite{bojanowski2017enriching},ELMo\cite{peters2018deep} и BERT\cite{devlin2018bert}. 
The approach of the FastText is based on the Word2Vec model principles: word distributions are predicted by their context, but FastText uses character trigrams as a basic vector representation. Each word is represented as a sum of trigram vectors that are the base for continuous bag of words or skip-grams algorithms~\cite{mikolov2013efficient}. Such a model is simpler to train due to decreased dictionary size: the number of character n-grams is less than the number of unique words. Another advantage of this approach is that morphology is accounted automatically, which is important for the Russian language.
%Идея FastText состоит в следующем: за основу берутся идеи Word2Vec-моделей, то есть распределение слов предсказываются по их контексту, но в качестве базового векторного представления берутся триграммы символов. Таким образом каждое слово представляется в итоге как сумма векторов его триграмм, а на этих суммах строятся конструкции CBOW или skip-gram\cite{mikolov2013efficient}. Данные модели проще и быстрее в обучении, так как размерность словаря уменьшается -- триграмм символов на порядок меньше, чем различных слов. Плюс при данном подходе морфология учитывается автоматически. В результате получаются представления, которые показывают себя заметно лучше для языков с богатой морфологией.

Instead of using fixed vectors for every word (like FastText does), ELMo word vectors are sentence-dependent. ELMo is based on The Bidirectional Language Model (BiLM), which learns to predict the next word in a word sequence. Vectors obtained with ELMo are contextualized by means of grouping the hidden states (and initial embedding) in a certain way (concatenation followed by weighed summation).
%В отличие от FastText, использующей фиксированный эмбеддинг для кждого слова, механизм ELMo построен так, что она строит для каждого слова свой эмбеддинг в зависимости от предложения. В основе ELMo лежит The Bidirectional Language Model (BiLM), которая учится предсказывать следующее слово в последовательности слов. ELMo comes up with the contextualized embedding through grouping together the hidden states (and initial embedding) in a certain way (concatenation followed by weighted summation).
However, predicting the next word in a sequence is a directional approach and therefore is limited in taking context into account. This is a common problem in training NLP models, and is addressed in BERT.

BERT is based on the Transformer mechanism, which analyzes contextual relations between words in a text. The BERT model consists of an encoder extracting information from a text and a decoder which gives output predictions. In order to address the context accounting problem, BERT uses two learning strategies: words masking and logic check of the next sentence. The first strategy implies replacing 15\% of the words on a token ``MASK'' which is later used as a target for the neural network to predict actual words. In the second learning strategy, the neural network should determine if two input sentences are logically sequenced or are just a set of random phrases. In BERT training, both strategies used simultaneously so as to minimize their combined loss function.
%BERT. В основе BERT лежит механизм Transformer, который изучает контекстные отношения между словами в тексте. Модель BERT состоит из энкодера, который принимает на вход текст и извлекает из него информацию, и декодера, который выдает предсказания. При обучении языковых моделей возникает проблема определения метода предсказания. Многие модели, в том числе FastText и ELMo, предсказывают следующее слово в последовательности -- это направленный подход, который по своей сути ограничивает контекстное обучение. Чтобы преодолеть эту проблему, BERT использует две стратегии обучения: маскировка отдельных слов и проверка следующего предложения на логичность. При первой стратегии 15 процентов слов заменяются на токен [MASK] и нейросеть должна научиться правильно предсказывать данные слова. При второй стратегии нейронной сети подаются на вход два предложения и она должна определить, является ли второе логичным продолжением первого или это некая случайная фраза, не имеющая никакого отношения к первой. При обучении модели BERT обе стратегии обучаются вместе с целью минимизации комбинированной функции потерь двух стратегий.
% %A set of experiments was conducted to choose the best model for word vector representation. compared in "Model A" based on BiLSTM topology For second mention detection method (Model B) we used 
%XLM-RoBERTa из письма вани 16.03.2021
XLM-RoBERTa model model a similar to BERT masked language model based on Transformers~\cite{vaswani2017attention}. Main differences between XLM-RoBERTa and BERT are following: XLM-RoBERTa was trained on larger multilingual corpus from CommonCrawl project which contains 2.5TB of texts. Russian is the second language by texts count in this corpus after English. XLM-RoBERTa was trained only for masked token prediction task, it didn't use the next sentence prediction loss. Minibatches during model training included texts in different languages. It used different tokenization algorithm, while BERT used WordPiece~\cite{schuster2012japanese}, this model used SentencePiece~\cite{kudo2018sentencepiece}. Vocabulary size in XLM-RoBERTa is 250K unique tokens for all languages. There is two versions of model: XLM-RoBERTa-base with 270M parameters and XLM-RoBERTa-large with 550M.


\subsection{Model architecture}%\label{subsubsec:model_topology}
\subsubsection{Model A - BiLSTM neural net}
The topology of Model A is depicted in Fig. ~\ref{fig:lstm}. The set of input features for this model was described above.
%\paragraph{Word characters coding.} 
Additionally for word coding we used characters convolution based neural network (see Fig. ~\ref{fig:char_cnn}), CharCNN~\cite{krizhevsky2012imagenet}.
First, each word is represented as a character sequence. The number of characters is a hyperparameter, which in this study has chosen empirically with the value of 52. If the word has fewer characters than this number, the remaining characters are filled with the <<PADDING>> symbol. The training dataset is used to make a character vocabulary that also includes special characters <<PADDING>> and <<UNKNOWN>>, the latter allowing for possible future occurrence of characters not present in the training set.
For coding each character embedding layer~\cite{gal2016theoretically} is used, which replaces every character from vocabulary appeared in a word to a corresponding real vector. In the beginning, the real vectors are initialized with values from random uniform distribution in the range of [-0.5; 0.5]. The size of real vectors is 30. Further, the matrix of coded characters of word is processed by convolution layer (with 30 filters and kernel size = 3)~\cite{dumoulin2016guide} and global maxpooling function that provided maximization function of all values for each filter~\cite{boureau2010theoretical}.

%Каждое слово представляется как последовательность символов, входящих в него. Размерность числа символов задается как гиперпараметр, в данной работе -- 52. Для слов, в которых символов не хватает, недостающие символы задаются пэддингами. На основе тренировочного множества мы создаем словарь символов, в который также включены особые символы «PADDING» и «UNKNOWN» для пэддинга и неизвестных символов соответственно. Эмбеддинг для сиволов инициализируется значениями из равномерного случайного распределения в диапазоне от [-0,5; 0,5]. Выходная размерность эмбеддинга -- 30. К векторам слов далее применяется конволюция и глобальный макспуллинг. Число фильтров конволюции -- 30.
%Схема извлечения признаков из символов слова на основе 1D сверточной нейронной сети.
\begin{figure}
    \centering
    \includegraphics[scale=0.3]{pictures/char_cnn.jpg}
    \caption{The scheme of character feature extraction on base of char convolution neural network. Each input vector after the embedding layer is expanded with two extra padding object (white boxes), $w_{(k1)}, w_{(k2)}, w_{(k3)}$ - weights of convolution filter $k$.}
    \label{fig:char_cnn}
\end{figure}
At the output of the model, we put either a fully connected layer~\cite{chiu2016named} or conditional random fields (CRF~\cite{lafferty2001conditional}), which output the probabilities for a token to have a B, I, or O tag for the corresponding entity (for instance, B-ADR, I-ADR, or O-ADR).
%Мы рассматриваем проблему извлечения именованных сущностей как мультиклассовую классификацию токенов в предложений. Модель принимает на вход последовательность признаков, извлекаемых из токенов, вычисляет вероятности отнесения их к предопределенным тегам и для каждого токена возвращает тег с максимальной вероятностью. 
%Для тэгов используется разметка BIO. Каждое слово кодируется следующим образом: тэг «O» означает, что слово находится за пределами сущности, тэг «B-ХХХ» используется для первого слова сущности, а тэг «I-ХХХ» - для последующих слов внутри сущности. Сущности разых типов в данном корпусе могут пересекаться, то есть у одного слова может быть несколько тэгов. 


% Что то ВВОДНОЕ...


% Below we briefly describe basic elements of Model A: LSTM neuron and CRF algorithm.
% \paragraph{LSTM.} 
% LSTM is a modification of a recurrent neural network (RNN) which computes on each time step $t$ a new hidden state $h_{t}$ on base of the previous hidden state $h_{t-1}$ and the input vector $x_{t}$ processed with an activation function %(e.g. hyperbolic tangent function).Though RNN is able to process long input sequences, its training is complicated due to ``gradient vanishing'' which occurs when propagating the error back through many time steps on each of which the activation function was applied. In order to address the problem,
% LSTM networks have an additional item -- memory cell $c_{t}$ which is a linear combination of $h_{t-1}$ and $x_{t}$. The LSTM memory cell in its processing interacts with 3 ``gates'': a) $f_{t}$ controls which part of the previous cell memory should be ``forgotten'', b) $i_{t}$ controls which part of the input should be saved in the memory cell, c) $o_{t}$ controls which part of memory cell will be outputted on each step as the cell hidden state. The value of a memory cell after receiving a new input $x_{t}$ is computed as follows:
% %В общем случае рекуррентная сеть на каждом шаге по времени считывает вектор $x_{t}$ и вычисляет новое скрытое состояние $h_{t}$ на основе предыдущего состояния $h_{t-1}$ и входа $x_{t}$ с использованием логистической функции, например тангенциальной. Хотя RNN и могут обрабатывать длинные последовательности входных объектов, их обучение затруднительно, так как повторное применение логистической функции на каждом шаге приводит к затуханию в сигнале ошибки во времени. В сетях на основе LSTM эта проблема решается добавлением дополнительного элемента -- ячейки памяти $c_{t}$, значение которой -- это результат линейной комбинации $h_{t-1}$ и $x_{t}$. Ячейка LSTM обрабатывает вход с трех различных каналов, которые контролируют: а) какую долю текущего входного примера сохранить в ячейке памяти $i_{t}$ и б) какую долю предыдущей памяти в ячейке забыть $f_{t}$ . Обновление значения в ячейке памяти после получения входа $x_{t}$ вычисляется следующим образом:
% \begin{gather*}
% i_{t} = \sigma(W_{ix} x_{t} + W_{ix}h_{t-1} + W_{ic} c_{t} + b_{i}),
% \\
% f_{t} = \sigma(W_{f x} x_{t} + W_{f h} h_{t-1} + W_{fc} c_{t-1} + b_{f }),
% \\
% c_{t} = f_{t} \circ c_{t-1} + i_t \circ \tanh (W_{cx}x_{t} + W_{ch} h_{t-1} + b_{c}),
% \end{gather*}
% where $\sigma$ is an activation function (e.g. sigmoid), $\tanh$ is another activation function applied element-wise to its argument vector, and $\circ$ is the Hadamard product. On each step, the $h_{t}$ value is modified with the third gate $o_{t}$:
% %где $\sigma$ – это логистическая функция, например сигмоидальная, $\cdot$ – произведение Адамара. Значение $h_{t}$ на каждом шаге контролируется третьим каналом $o_{t}$:
% \begin{gather*}
% o_{t} = \sigma(W_{ox} x_{t} + W_{oc} h_{t-1} + W_{oc} c_{t-1} + b_{o}),
% \\
% h_{t} = o_{t} \cdot tanh(c_{t})
% \end{gather*}

% Multiple LSTM layers could be used in series to increase the capacity and performance of an LSTM-based network (stacked LSTM topology). In that way in the research topology with 3 sequential LSTM layers show quality increasing in comparison with a single-layer LSTM. 
% %Для увеличения ёмкости и производительности сети на основе LSTM могут быть использованы последовательно несколько LSTM-слоев. Так, в нашей работе 3-х слойная LSTM показала прирост качества в сравнении с однослойной LSTM.

% \paragraph{CRF} 
% %It is an implementation of hidden Markov models (HMM), a graph model for the representation of joint probabilities of several random values.
% \cite{lafferty2001conditional}. It is defined as follows. Let $X$ be a random variables over a sequence to be classified, $Y$ -- random variables mapping into a sequence labels. Also, let $G = (V, E)$ be such a graph that $Y = (Y_{v})_{v \subset V}$, so $Y$ is indexed by vertices of $G$, then $(X, Y)$ is a conditional random field in case when each random variable $Y_{v}$ (that depends of $X$) has the Markov property with respect to the graph. Here $E$ reflects all dependencies between variables in a random field $(X, Y)$. 
% CRF represents the following distribution of random variables set:
% 	$$p(\bar{y}|\bar{x};w)=\frac{exp(\sum_{i}\sum_{j}w_{j} f_{j} (y_{i},y_{i-1},\bar{x},i))}{\sum_{y^{'}\subset Y}exp(\sum_{i}\sum_{j}w_{j} f_{j} (y_{i},y_{i-1},\bar{x}, i))}(*),$$
% where $f_{j}$ -- features functions, $w_{j}$ -- weights for feature function $j$.
% The task is to find $y^*$ values to maximize the equation
% $$
%     y^* = \operatornamewithlimits{argmax}_{y} \, \max P(\bar{y}|\bar{x};w).
% $$

% %CRF является реализацией Марковских случайных полей -- графовой модели для представления совместных вероятностей нескольких случайных величин. Определение GRF следующее. Пусть X -- случайные переменные над последовательностью, которую нужно разметить, а Y -- случайные переменные, соответствующие меткам последовательности, и пусть $G = (V, E)$ такой граф, что $Y = (Y_{v})_{v \subset V}$, так что $Y$ индексируется вершинами $G$, тогда $(X, Y)$ является условным случайным полем, когда каждая из случайных величин $Y_{v}$, зависимая X, подчиняется свойству Маркова\cite{lafferty2001conditional} относительно графа.
% %CRF представляет распределение набора случайных величин следующего вида:
% 	%\[p(\bar{y}|\bar{x};w)=\frac{exp(\sum_{i}\sum_{j}w_{j} f_{j} (y_{i},y_{i-1},\bar{x},i))}{\sum_{y^{'}\subset Y}exp(\sum_{i}\sum_{j}w_{j} f_{j} (y_{i},y_{i-1},\bar{x}, i))}(*),\]
% %где $_{j}$ — признаковые функции, $w_{j}$ — веса для j-й признаковой функции.	
% %Задача ставится следующим образом. Найти $y^*$, которые максимизируют 	уравнение (*):
% %	\[y^* = argmax_{y} max P(\bar{y}|\bar{x};w)\]

\subsubsection{Model B - XLM RoBERTa based multi-model}

To improve the model accuracy, we performed an additional training XLM-RoBERTa-base on two datasets:  the first we collected from the site \url{irecommend.ru} and the second was borrowed from unnannotated part of RuDReC \cite{tutubalina2020russian}.  Calculations of two epochs during three days and XLM-RoBERTa-large for one epoch during 5 days were performed using a computer with one Nvidia Tesla v100 and Huggingface Transformers library.  Further, we fine-tuned these models to solve the NER task. Figure \ref{fig:ner_finetune} demonstrates an algorithm of fine-tuning language models for NER. This is the commonly used fine-tuning algorithm of simple transformers project~\cite{rajapakse2019simpletransformers}. The linear layer with an activation function softmax was added to the model output to classify words. The developed multi-tag model implements the concatenation of fine-tuned language model with the vector of features (Vidal, MESHRUS, ton, and other). The LSTM neural net model processes then the resulting vector to implement the multi-tagged labeling. Figures \ref{fig:modelB_output}, \ref{fig:modelB_encoding} clarify a model topology. So the multi-tag model combines the above-mentioned fine-tuned language model with the simplified variant of Model A(without CRF and with the substitution of ELMo  word representation by the fine-tuned language model's output with class activities). During training the above-mentioned LSTM neural net model, this language model was not trained. We used the automatic selection of hyperparameters using Weights\&Biases~\cite{wandb} – sweeps for the total multi-tag model. It took about 24 hours on the computer with 3 Tesla K80 processing 6 agents. The 5-fold evaluation was used.

\subsubsection{Coreference model}%\label{subsubsec:model_topology}
For coreference resolution, we chose a state-of-the-art neural network architecture from \cite{joshi2019bert}.  The core feature of this model is the ability to learn the task of mentions detection, and the task of mentions linking and forming coreference clusters end to end at the same time, without separating these 2 tasks into different processes. The model uses the BERT language model to get input text word vector representations.
%NN uses BERT language model for encoding input text to get the $X={x}_n^N$ - sequence of word vectors. After that whole text is being cut into all possible spans (fragments of text) not bigger than predifined maximum span length, and span embeddings are generated $G={g}_k^K$. each span embedding is a concatenation of vectors of its first and last words, embedding vector of span length and vector obtained as weighted sum for all words in span by using an attention mechanism\cite{vaswani2017attention} $g_k(i,j)=[x_i,x_j,emb(j-i),att(x_{i:j}]$. After that every span is getting mention score $s^{mention}$ to find spans that a most likely are mentions of some entities. Span embedding passing through a fully connected layers with output layer of size 1 with linear activation function, same happens with span length embedding. Mention score is the sum of both outputs. After this, predefined percent of spans with highest scores it chosen for the further processing, and the rest of them are getting omitted. After that, fast antecedent scores are calculated, these scores shows for each span i what previous span j is most probably is a mention of the same entity. Fast antecedent scores are calculated as dot product of two span embeddings (one of which passed through a fully connected layers) plus sum of their $s^{mention}$ and embedding of the distance between spans also passed through a fully connected layer with output layer with 1 neuron and linear activation function. These fast antecedent scores allows us to filter most probable antecedents for every span. On the next step slow scores are computed for every span and its probable antecedents. Firstly we get embeddings for number of words and number of sentences between span and its antecedent. We concatenate it with span embedding, antecedent embedding and element wise product of the last two. After passing concatenated vector through fully connected layers with output layer with 1 neuron and a linear activation function we get the slow antecedent score. When we got slow antecedent scores and fast scores for every antecedent of the span we path sum of those scores through softmax function. Output values of the softmax are used as weights to get weighted sum of antecedents for every span. Concatenation of this weighted sum with the actual span embedding is passed through fully connected layer with sigmoid activation to get a value $f$ for an embedding update gate: $g_k^t=f*h+(1-f)*g^{t-1}$, where $h$ is the weighted sum of antecedent embeddings, $g^{t-1}$ - old span embedding, $g_k^t$ - updated span embedding. After repeating this last procedure 2 times sum of fast antecedent scores and slow antecedent scores are used as final scores to chose an antecedent for every span and form coreference clusters.
To adapt network architecture to Russian language we used RuBERT - BERT language model trained on the Russian part of Wikipedia and news data. We conducted experiments to tune neural network hyperparameters and training options to achive a better results, final hyperparameters were as follows: maximum span width = 30, maximum antecedents for every mention: 50, hidden fully connected layers size = 150, numbers of sequential hidden layers = 2,maximum epoch training: 200, language model learning rate = 1.0e-05, task model learning rate = 0.001,embedding sizes = 20.
  
\begin{figure}
    \centering
    %\includegraphics[scale=0.3]{pictures/atc.png}
    \includegraphics[width=8.0cm]{pictures/model_lm_fig.png}
    \caption{Fine tuning of language model for word classification task}
    \label{fig:ner_finetune}
\end{figure}

\begin{figure}
    \centering
    %\includegraphics[scale=0.3]{pictures/atc.png}
    \includegraphics[width=8.0cm]{pictures/model_lm_med_features_multioutput-Page-1_top.png}
    \caption{Model B architecture, fwn -- words, encoded with features}
    \label{fig:modelB_output}
\end{figure}

\begin{figure}
    \centering
    %\includegraphics[scale=0.3]{pictures/atc.png}
    \includegraphics[width=8.0cm]{pictures/model_lm_med_features_multioutput-Page-1_bottom.png}
    \caption{Model B architecture, words encoding method to obtain word representation vectors fwn is presented on the left and on the right there is multi-output for words classification}
    \label{fig:modelB_encoding}
\end{figure}

%=============================


% To improve the model accuracy, we performed an additional training XLM-RoBERTA-base on two datasets:  the first was performed in this work and contained texts from irecommend.ru. 
% The reviews from irecommend.ru, unlike otzovik.com, were larger and generally contained photo of drugs and user`s brief description of instructions for the use of drugs that can be useful for prepared language model. We collected reviews only from beauty and health (krasota-i-zdorovie) topic that more relevant for our task. All reviews were previously prepossessed for removing html tags and images. As a result 250 thousand of reviews from 1600 users were achieved. The average length of a review from irecommend.ru was 1000 tokens against 150 tokens for texts in otzovik.ru. 

% The second dataset was borrowed from [https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btaa675/5877427?login=true]. It contained more than 1.4M reviews from different web-sites: prodoctorov.ru, protabletky.ru, www.spr.ru, www.otzovik.com.
% % \begin{enumerate}
% %     \item prodoctorov.ru - ресурс, где пользователи могут оставить свой отзыв о работе своего доктора или клиники, в которой они проходили лечение. Включает в себя 388 тыс. отзывов, от более чем 15 тыс. авторов,
% %     \item protabletky.ru - ресурс, где публикуют оценки и отзывы практикующих врачей и пользователей о лекарственных средствах. Врачи при регистрации подтверждают свой статус, представляя диплом об окончании медицинского ВУЗа или действующий сертификат по специальности. Включает в себя: а) для врачей: 7451 отзыв от 995 уникальных авторов, б) для пользователей: 31,5 тыс. отзывов (информация об авторах отсутствует),
% %     \item www.spr.ru. — масштабный межрегиональный проект, содержащий каталоги организаций по каждому региону России, абсолютно по каждому городу, где размещено более 700 тысяч отзывов о различных предприятиях. Включает в себя 197 тыс. отзыва по 42 медицинским категориям, например "apteki", "skoraya-meditsinskaya-pomosch", "polikliniki-detskie" и т.д. Информация об авторах не представлена,
% %     \item www.otzovik.com - информационный портал, где десятки тысяч авторов пишут свои впечатления о разных вещах, ставят оценки, дают полезные советы, а также предупреждают о возможных недостатках и проблемах. Включает в себя 784 тыс. отзывов от 151 тыс. уникальных авторов из категории "Health facilities". (В статье КФУ: Health facilities and pharmacies - не знаю откуда они это взяли)
% % \end{enumerate}
% Thus the collection of these two part of raw texts was the most completed on current moment and used for training domain specific language model in our work. 

% Calculations of two epochs during three days and XLM-RoBERT-large for one epoch during 5 days were performed using two computers V100 and Huggingface Transformers library.  Further, we fine-tuned these models to solve the NER task. Figure N demonstrates an algorithm of fine-tuning language models for NER. This is the commonly used fine-tuning algorithm of simple transformers project[]. The linear layer with an activation function softmax was added to the model output to classify words. The developed multi-tag model implements the concatenation of fine-tuned language model with the vector of features (Vidal, meshrus, meshrus2, MedDRA, ton, and other). The LSTM neural net model processes then the resulting vector to implement the multi-tagged labeling. Figures 2,3 clarify a model topology. So the multi-tag model combines the above-mentioned fine-tuned language model with the simplified variant of Model A(without CRF and with the substitution of ELMO  word representation by the fine-tuned language model's output with class activities). During training the above-mentioned LSTM neural net model, this language model was not trained and had fixed hyperparameters from[]. We used the automatic selection of hyperparameters using Weights&Biases – sweeps for the total multi-tag model. It took about 24 hours on the computer with 3 Tesla K80 processing 6 agents. The 5-fold evaluation was used.

%Ввиду проблемы определения границ сущности мы вводим следующие две метрики:
\begin{comment}
In order to assess partially correct determination of mention boundaries, we employ two evaluation metrics:
\begin{enumerate}
	\item Exact mention matching $\Fe$;
	\item Partial matching $\Fp$.
\end{enumerate}

\paragraph{$\Fe$.} For every entity (in our case, ADR, Medication, and Disease) from the ground truth set we calculate precision, recall and $F_{1}$ as follows:
\begin{gather*}
\mathrm{precision} = \sum_{e_{s} \in E_{s}} \frac{\left [ e=e_{s} \right ]}{\left | E_{s} \right |}
\\
\mathrm{recall} = \sum_{e \in E} \frac{\left [ e=e_{s} \right ]}{\left | E \right |}
\\
\Fe = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}},
\end{gather*}
	where $e_{s}$ is a predicted mention, $e$ is the corresponding ground truth mention, $E$ is the ground truth set of mentions, $E_{s}$ is the set of mentions predicted by the model, $| E |$ is the number of items in $E$, $[e = e_{s}]$ is the Iverson bracket which is 1 if the mentions $e$ and $e_s$ are equal, and 0 otherwise.

%В случае если перед тэгом I-tag идет тэг O скрипт подсчета данной метрики заменяет I-tag на B-tag.
While comparing mentions by equality, if an O-tag precedes an I-tag, the latter is replaced with a B-tag.

\paragraph{$\Fp$.}
For every $i$-th sentence from the test dataset we calculated the values of $\mathrm{precision}_{i}$, $\mathrm{recall}_{i}$, and $F_{1i}$ using the following equations:
%For every $i$-th sentence in test are calculate $\mathrm{precision}_{i}$, $\mathrm{recall}_{i}$ and $F_{1i}$ using formulas below:
\begin{gather*}
	\mathrm{precision}_{i} = \frac{\left | t _{i} \cap t_{si}\right |}{\left | t_{si} \right |}
\\
	\mathrm{recall}_{i} = \frac{\left | t_{i} \cap t_{si}\right |}{\left | t_{i} \right |}
\\
	F_{1i} = 2 \cdot \frac{\mathrm{precision}_{i} \cdot \mathrm{recall}_{i}}{\mathrm{precision}_{i} + \mathrm{recall}_{i}}
\end{gather*}
where $t_{si}$ is the list of tokens of $i$-th sentence that were recognized by the model as parts of mentions, $t_{i}$ is the list of tokens belonging to ground truth mentions of $i$-th sentence, and $| t_{i} |$ is the list length (the number of tokens in $t_{i}$).
The final $\Fp$ is calculated as the mean of $F_{1i}$ over all $i$ in the set $T$ of sentences in the text that contain any mentions.:
	$$\Fp = \frac{1}{\left | T \right |}\sum_{i=1}^{|T|} F_{1i}.$$
\end{comment}

\section{Experiments}\label{subsec:Experiments}
\subsection{Methodology}
In the experiments, we pursued the following objectives: 
\begin{enumerate}
    \item To select most effective language model among the set: FastText, ELMo, and BERT;
    \item To evaluate the influence of different feature sets on the precision of  ADR mention extraction;
    \item To compare the level of precision for ADR mentions identification basing on our corpus in relation to one received  on available russian language data of similar type;
    \item To show  the influence of  such characteristics of  corpus texts on the precision of  ADR mention extraction, as the proportion between phrases with ADR  and without  it, between ADR mentions and INDICATION mentions, the corpus size and etc.         \item To evaluate the influence of the ADR tagging severity on the ADR identification precision.
\end{enumerate}

We made the accent on ADR because of its importance in practice and the complexity of identification given close relation to the context that stipulates this selection for model calibrations.
%Эксперименты проводились с различными подвыборками из собранного корпуса RDRS (далее RDRS_2800, где 2800 указатель на количество отзывов в итоговом корпусе), в том числе:
%1. Подвыборка с 1660 отзывами (далее RDRS_1660) - описание  ... (тот который "1660 12-09-2019?")... Для чего используется... 
%2. Подвыборка с 1250 отзывами (далее RDRS_1250_balanced) - описание  ... (тот который "2020-11-20_balance_noint_wnotes") . Для чего используется... 
%3. подвыборка с 610 отзывами (далее RDRS_610) - описание  ... (тот который "balance_wnotes_only_adr — ванин"). Для чего используется... 
%4. подвыборка с 1136 отзывами (далее RDRS_1136) - Описание ... (тот который "balance_wnotes_50_no_adr"). Для чего используется... 
%5. Подвыборка с 500 отзывами (далее RDRS_500) - выборка содержит те же 500 отзывов, тексты которых использованы для создания размеченной части корпуса RuDRec. Данная выборка будет использована для проведения сравнительных экспериментов предложенных в данной работе схемы разметки и нейросетевых моделей. Сравнение по количественным параметрам представлено в таблице ...

%Процесс создания метода для определения сущностей выполнялся параллельно сбору и аннотированию корпуса. Это позволило отчасти откалибровать методику разметки данных, а также пронаблюдать зависимость точности от объема пополнения корпуса. Т.к. корпус содержит большое количество классов сущностей различного уровня основным критерием выбора модели в рамках разработываемого решения была использована точность по определению сущностей типа ADR. Сущности этого типа наиболее сложны для автоматической детекции и наиболее значимы для исследований в области фармаконадзора, как показывают работы и соревнования последних лет ... Другой мотивацией, с технологической точки зрения, является отсутсвие у ADR иерархии подклассов, наблюдаемых у классов сущностей Disease и Medication. Это позволило заранее не усложнять модель и провести больший комплекс репрезентативных вычислительных расчетов. В результате технология определения сущностей, выбранная по результатам сравнительных исследований по точности определения ADR, легла в основу общего решения по определению всех классов и подклассов сущностей, аннотированных в созданном корпусе. 
\begin{table*}
\caption{Accuracy (\%) of recognizing ADR, Medication and Disease entities in our corpus (1600 reviews) by Model A with different language models.}
\label{tab:embedding}
\centering
\input{tables/embedding_exp.tex}
\end{table*}


\label{section:quality_metrics}
For models performance estimation, we used the chunking metric, which was introduced in the conll2000 shared task and has been used to compare named entity extraction systems since then. The implementation can be found here: \url{https://www.clips.uantwerpen.be/conll2000/chunking/}. The script receives as its input a file where each line contains a token, true tag and predicted tag. Tags could be "O" - if token doesn't belong to any mentions, "B-X" if token starts a mention of some type X, "I-X" if it continue a mention of type X. If tag "I-X" appears after "O", or "I-Y" (mention of other type) it's treated as "B-X" and starts a new mention. The script calculates the percentage of detected mentions that are correct (precision), the percentage of correct mentions that were detected (recall) and an $F_1$ score: $$F_1 = \frac{2*\mathrm{precision}*\mathrm{recall}}{\mathrm{precision} + \mathrm{recall}}$$
In our work we use F1-exact score that estimate accuracy of full entity matching. 

%Мы провели ряд экспериментов по:
%We conducted a set of experiments for:

%В качестве эмбеддингов рассматривались модели FastText, ELMo и BERT.

%\begin{table*}
%\centering
%\caption{The accuracy of our model on the CADEC %corpus compared to other models.}
%\label{tab:compare_all}
%\input{tables/compare_all.tex}
%\end{table*}

%\subsection{Pipeline}
\label{section:pipeline}
%UDPipe и т.д.
%Про перенос разметки:
%Для обработки текстов моделями машинного обучения используется представление текста в виде последовательности токенов (слов или частей слов), которым впоследствии сопоставляются точки в векторном пространстве признаков. Токенизация текста производится автоматически с помощью программной библиотеки UDPipe версии 2.4 [...]. С одной стороны, автоматический характер процедуры позволяет за малый промежуток времени обработать большое количество текстов. С другой стороны, данная процедура не является совершенной, что приводит к неоднозначности разбиения (например, токенам, в которых слово и знак препинания не разделены).
%После токенизации необходимо корректно перенести разметку именованных сущностей, которые отмечены в изначальном тексте, на полученные автоматически токены с учётом опечаток и особенностей в разметке со стороны экспертов (недоотмеченные символы, разрывная разметка, отметка только одного слова в составном термине) и артефактов процесса токенизации (например, токены со знаком препинания, слова через дефис как один токен, некорректные токены как следствие опечаток в изначальном тексте).
%Для решения данной задачи были реализованы следующие функции в процессе обработки текста:


%1) дополнительное разбиение токенов UDPipe по знакам препинания с учётом словаря терминологических и общеупотребимых исключений (например, "сердечно-сосудистый", "ИРС-19", "желудочно-кишечный", "кол-во", "В-каротин" и так далее);

%2) сопоставление границ выделенных в тексте сущностей с токенами;

%3) отслеживание разницы между изначально выделенными сущностями и перенесёнными для оценки корректности переноса;

%4) корректирование переноса разметки на основе серии правил.

%Процесс составления правил был итеративным и продолжался до тех пор, пока число сущностей в изначальной разметке и перенесённой не совпадало, а визуально не было значительных расхождений между начальным текстом сущностей и токенами, которые были отмечены как сущности.

%\begin{enumerate}[1.]
   \subsection{Finding the best embedding}
  % Для обучения Fasttext модели %использовалось 2 корпуса — отзывы о лекарствах и отзывы о больницах, также
%использовались предобученные вектора на корпусе commoncrawl\cite{commoncrawl} скачанные с сайта \url{https://fasttext.cc/docs/en/crawl-vectors.html}. Предобученная модель ELMo была взята из открытой библиотеки DeepPavlov~\cite{burtsev2018deeppavlov}. Данная модель обучена на корпусе Russian WMT News\cite{statmt}, который содержал 63M строк, 946M токенов и имело общий объем 12GB. Предобученная мультиязыкова модель BERT была взята из репозитория~\cite{google_research_bert} и дообучена нами на корпусе отзывов о лекарствах и больницах. Результаты экспериментов приведены в Таблице \ref{tab:embedding}. В итоге лучший результат показала модель ELMo. Всвязи с чем дальнейшие эксперименты по добавлению признаков и изменению топологии мы проводили, базируясь на ELMo. Результаты данных экспериментов показаты в таблице \ref{tab:feat_top_exp}.

    We considered the following embedding models: FastText, ELMo, and BERT. Two corpora were used to train the FastText model -- a corpus of reviews from Otzovik.com from the category "medicines" and a corpus of reviews from the category "hospitals"~\footnote{Reviews were taken from the Otzovik website from the categories "hospitals" and "medicines" - https://otzovik.com/health/},
    %~\cite{hospCorpus,drugCorpus}
    also we used vectors pretrained on the Commoncrawl corpus\footnote{\url{http://commoncrawl.org/}}.
    %~\cite{commoncrawl}
    The ELMo model which had been preliminarily trained on the Russian WMT News~\cite{statmt} was taken from the DeepPavlov~\footnote{\url{https://deeppavlov.readthedocs.io/en/master/intro/pretrained_vectors.html}}~\cite{burtsev2018deeppavlov} open-source library. The pretrained multilingual BERT model was taken from the Google repository~\footnote{\url{https://github.com/google-research/bert/}} and subsequently fine-tuned on the above-mentioned corpora of drug and hospital reviews. These pretrained models were used as input to our neural network model presented in Fig.~\ref{fig:lstm}. The dataset (the first version of our corpus contained 1600 reviews) was split into 5 folds for cross-validation. On each fold, the training set was split into training and validation sets in the ratio 9:1. Training was performed for a maximum of 70 epochs, with early stopping by the validation loss. Cross entropy was used as the loss function, with nAdam as the optimizer and cyclical learning rate mechanism~\cite{smith2017cyclical}. The results of the test experiments are given in Table~\ref{tab:embedding}, where the best results according to the F1-exact metric demonstrate ELMo. The composition of ELMo with BERT    worsens the precision. As a result, we used ELMo below to evaluate the influence of different features on ADR mention extraction precision.
    
   % \subsection{Comparing the numerical results of our model on the CADEC corpus to the known literature results.} In this case, models were trained on the CADEC corpus of drug reviews, from which the following objects are extracted: ADR, Drug, Symptoms, Findings, Disease. The work~\cite{miftahutdinov2017identifying} was devoted to the extraction of Disease and Drug entities, while the Disease entity was presented there as a combination of the tags ADR, Disease, Findings and Symptoms. The model was based on CRF with word2vec embeddings, called HealthVec, pretrained on the Health Dataset~\cite{miftahutdinov2017identifying}. Part-of-speech tags, word shape features, syntactic relations and dictionaries were used as features. Learning was preformed with 5-fold cross-validation.
    
   % The second part of Table~\ref{tab:compare_all} presents a comparison of our model, employing LSTM and various features, with that model. $\Fp$ and $\Fe$ calculated for the Disease and Drug entities are given as final estimates.
    
    %The work~\cite{tutubalina2017combination} was devoted to extracting only the ADR entity with the help of a model combining recurrent neural networks and CRF. The corpus was split into a training set and a testing set in the proportion 70:30. The first part of Table~\ref{tab:compare_all} compares our model to that work. 
    
    \subsubsection{The influence of different features on ADR recognition precision} To evaluate the influence of using any separated feature from those mentioned above on ADR precision, we conducted the series of experiments with Model A which results presented in Table \ref{tab:feat_top_exp}.
    \subsubsection{Choosing the best model topology}
    %\subsubsection{Choosing the best combinations of model topology with the selected features.}
    Next, we provide a set of experiments with Model A on the choice of topology: replacing the last fully-connected layer with a CRF layer, or changing the number of biLSTM layers. This was studied in combination with adding emotion markers, PoS and MESHRUS, MESHRUS-2 and Vidal dictionaries, as shown in Table~\ref{tab:feat_top_exp}. So, this made it possible to assess the accuracy level of Model A. To evaluate the effectiveness of XLM-RoBERTa-large, we ran it without features (see last row in Table~\ref{tab:feat_top_exp}). In view of the it's high precision exceeding the precision of Model A, we used it as basis to  create Model B.    
%\end{enumerate}
%Также так как наш корпус новый и результатов по нему никто ранее не получал, для общего сравнения качества нашей модели мы приводим результаты работы нашей модели на корпусе CADEC и сравниваем их с результатами, приведенными в~\cite{miftahutdinov2017identifying} и~\cite{tutubalina2017combination}. В Таблицах \ref{tab:compare_dialog} и \ref{tab:compare_hindawi} приведены сравнения нашей модели с моделями из~\cite{miftahutdinov2017identifying} и~\cite{tutubalina2017combination} соответственно. В данных работах модели обучались на корпусе отзывов о лекарственных препаратах CADEC, в котором выделены следующие сущности: ADR, Drug, Symptoms, Findings, Disease. 
%Конкретно работа~\cite{miftahutdinov2017identifying} посвящена выделению сущностей Disease и Drug, причем Disease был представлен как объединение тэгов ADR, Disease, Finding и Symptoms. В качестве итоговых оценок приведены $F_{1}\_{partial}$ и $F_{1}\_{exact}$ объединения Disease и Drug. Использовалась кроссвалидация на 5 фолдов. В основе модели лежит CRF с предобученными на медицинском корпусе эмбеддингами. В качестве признаков используются part-of-speech tags, word shape features, syntactic relations, and dictionaries. Таблица \ref{  tab:compare_dialog} иллюстрирует сравнение нашей модели с 3-х слойным LSTM и различными признаками с вышеописанной моделью.



%\begin{table*}
%\label{tab:compare_dialog}
%\centering
%\input{tables/compare_dialog.tex}
%\caption{Compare with~\cite{miftahutdinov2017identifying}}
%\end{table*}
%\begin{table*}
%\label{tab:compare_hindawi}
%\centering
%\input{tables/compare_hindawi.tex}
%\caption{Compare with~\cite{tutubalina2017combination}}
%\end{table*}


%В работе \ref{tab:compare_hindawi} приводится извлечение только ADR, для решения поставленной задачи берется модель, комбинирующая реккурентные нейронные сети и CRF. Корпус разделен на трэйн и тест в пропорции 70:30. Сравнение моделей приведено в Таблице \ref{tab:compare_hindawi}. 
%В итоге наша модель показала сравнимые результаты на корпусе CADEC, что говорит о её применимости к другим датасетам. 
% модель представлена как комбинация признаков и топологических добавок, показавших прирост в \ref{tab:feat_top_exp}. Модель обучалась на 70 эпохах. Использовалась кроссвалидация на 5 фолдов. Тренировочное множество в свою очередь разбиралось в соотношении 90:10 на трейн и валидацию. Лосс на валидации использовался как критерий раннего остонова. Функция потерь — кроссэнтропия, оптимизатор — nAdam. Шаг градиентного спуска циклический\cite{smith2017cyclical}.Результаты приведены в \ref{tab:result}.

%Loss function -- cross entropy, optimizer -- nAdam. The cyclical learning rate\cite{smith2017cyclical} was used. 

\begin{table*}
\centering
\caption{Entity recognition F1 score on our corpus (1600 reviews) of the models with different features and topology.}
\label{tab:feat_top_exp}
\input{tables/feature_top_exp.tex}
\end{table*}

 \subsubsection{The influence of  characteristics of corpus texts on the precision of ADR recognition} % recognition = mention extraction
 First of all, we conducted experiments on the corpus 2800 texts extended by texts similar to corpus 1600 texts to assess the change of precision in ADR identification with the rise of ADR mention number. As follows from the data in Table~\ref{tab:rdrs_subsets}, a direct increase in the number of reviews in the corpus gives only a small increase in the share of ADR-mentions per review (0.2 versus 0.22). So, its saturation by ADR stays lower than in most corpora from Table~\ref{tab:adr_saturation}. To study the effect of increasing saturation of the corpus by ADR mentions, we experimented with  sets of different sizes from the corpus with various ADR-mention shares per review:   of 1250 texts (average 1.4 ADR onto review) balanced with ADR and without ADR, of 610 texts(average 2.9 ADR onto review), of 1136 texts(average 1.5 ADR onto review), of 500 texts (average 1.4 ADR onto review). In all experiments, the model treated input texts as the set of independent phrases.  
  \subsubsection{The influence evaluations of annotation style of ADR on its recognition precision}%ADR mentions in the corpus texts on their extraction precision
  In this case, we ran two experiments to evaluate a difference in  ADR mention extractions: the first on the base of the set containing pure  ADR mentions and the second, including the doubtful bordering ADR mentions, annotated both ADR and NOTE.
  
\begin{table*}
\centering
\caption{Subsets of RDRS corpora with accordance to complexity of ADR level saturation. *Model B - XLM-RoBERTa part only score on RuDRec}
\input{tables/rdrs_subsets.tex}
\label{tab:rdrs_subsets}
\end{table*}

 \subsubsection{Evaluations of the precision of coreference  relations extractions on our corpus by models  trained on different corpora} After  annotators  manually  corrected  predicted coreference relations in our corpus, we splited it to train, validation and test subsets. Then we evaluated coreference resolution model trained on AnCor-2019 corpus and tested on our corpus and model trained on our corpus.  We also did same experiments on AnCor-2019 test subset. We also tried to combine both train sets.% To adapt network architecture to Russian language we used RuBERT - BERT language model trained on the Russian part of Wikipedia and news data. We used following hyper parameters during training: maximum span width = 30, maximum antecedents for every mention: 50, hidden fully connected layers size = 150, numbers of sequential hidden layers = 2,maximum epoch training: 200, language model learning rate = 1.0e-05, task model learning rate = 0.001,embedding sizes = 20
  
%The effect of different representations of input texts on results: by phrases or texts in whole ishttps://www.overleaf.com/project/5fd49964842ff4097aa3c0b1 analyzed in section...  

\section{Results}\label{sec:results}

%\begin{enumerate}
%    \item 
\subsection{Results of Model A in series of embedding comparison experiments} These results are presented in Table~\ref{tab:embedding} and demonstrate the superiority of the ELMo model. BERT leads to lower F1 values with larger deviation ranges, and with the FastText model the F1 score is the lowest. Consequently, in further experiments on adding features and changing the topology we use the ELMo embedding as the basic approach. 
%The test experiments' results are given in Table 11, where the best results according to the F1-exact metric demonstrate ELMo. 
The composition of ELMo with BERT worsens the precision. As a result, we used ELMo below to evaluate the influence of different features on ADR mention extraction precision.
%   \item 
%\subsection{Results of comparing our model to the literature.} 
%The purpose of presenting the comparative  data in Table~\ref{tab:compare_all} is to confirm the general quality of our model. For this, we present the results of our model on the CADEC corpus and compare them to the recent works~\cite{tutubalina2017combination,miftahutdinov2017identifying}. Even though after having to exclude the Russian-specific corpus features we did not perform the time-consuming extraction of analogous English features from CADEC, our model showed results comparable to the modern computational results on CADEC. This confirms the applicability of our model to evaluating the state of the art for the developed corpus.
    
%   \item 
\subsection{Results of choosing the best model topology and input feature set for Model A in comparison with XLM-RoBERTa-large results}
For our corpus, as shown in Table~\ref{tab:feat_top_exp}, various changes in features and topology were added to the basic model with ELMo embedding. First of all, we focused on the metric $\Fe$, since it reflects the quality of the model better. Adding features gave the greatest increase in the least-represented class ADR. As a result, a combination of dictionary features, emotion markers, 3-layers LSTM and CRF can achieve the highest quality increase in ADR and Disease entities. For Medication, the combination of ELMo and 3-layer LSTM showed slightly better results. But results of experiments with model A as a whole are worse than the results of XLM-RoBERTa-large, which was used as a basis of Model B. Therefore, we performed further experiments on the base of Model B founded on it, as the best.
%\end{enumerate}


\subsection{Results  of the influence evaluation of corpus texts characteristics on the precision of ADR mention recognition }


The direct rise of corpus volume up from 1600 to 2800 mentions results in the ADR identification precision increase on 13\% F1, 6\% F1 in Disease, 4\% F1 in Medication. Figure \ref{fig:size_and_f1_g1} shows a curve of dependence of ADR precision increase on the corpus size, which becomes stable out of 80\% corpus size. Such behaviour for other main subtags demonstrate similar courses (see Table \ref{tab:all_tags_model_B}). The rise of the  ADR share by balancing the corpus leads to a more significant increase in ADR precision on 21\% without significant Disease and Medication precision identification changes (see Table ~\ref{tab:integrate_scores}). The higher saturation by these tags, which in practice stays unchanged after balancing corpus, explains the last fact. Experiments on corpora with the saturation more closer to CADEC one showed the further increase of  the ADR identification precision, up to 71.3\% F1 on the corpus of 610 texts ADR (average 2.9 ADR onto review).



\begin{figure*}
    \centering
    
    %\includegraphics[width=8.5cm,trim={2cm 1.5cm 2cm 2.5cm},clip]{pictures/accuracy_saturation.eps}
    \includegraphics[width=\textwidth]{pictures/size_and_f1_all_3_fig.png}
    
    \caption{Dependency of the accuracy on the size of training set for different tags in RDRS 2800}
    \label{fig:size_and_f1_g1}
\end{figure*}


\begin{table}
\centering
\caption{F1-scores of the model B for RDRS 1250 and RDRS 2800. *Negative -- union of tags: Worse, NegatedADE, ADE-Neg.}
\input{tables/all_tags_2800_model_B.tex}
\label{tab:all_tags_model_B}
\end{table}



\begin{table}
\centering
\caption{The difference in accuracy for the 3 main tags depending on the size and balance of the corpus} \label{tab:integrate_scores}
\input{tables/integral_scores.tex}
\end{table}

\subsection{Results of experiments to evaluate the influence of annotation style of ADR mentions on ADR recognition precision}%in the corpus texts on ADR mention extraction precision
This case results of experiments on the balanced set allowed evaluating the effect of the relaxation of ADR annotation requirements in about 3\% of the precision increase as follows figure (see Fig.~\ref{fig:adr_saturation}).
\subsection{Results for the coreference model} Results, presented in table 14, shows that  the used model trained on the  subset of our corpus demonstrates a high result  on the   test subset of our corpus. The training on AnCor-2019 corpus or on  corpora AnCor-2019  with ours gives worse results.
% Results, presented in table 14, shows that model trained on AnCor-2019 has good generalization and performs well on both testing subsets, while model trained on train subset of our corpus has pretty poor scores for AnCor-2019 test set. The reason for this is that our texts are all represent same small domain and have similar structures, so trained model can’t me generalized on AnCor-2019 corpus with bigger variety of text sizes and genres.





\begin{table*}
\centering
\caption{Results of training coreference resolution model on different corpora}
\label{tab:coref_eval}
\input{tables/coref_eval.tex}
\end{table*}


%По мотивам работы (https://www.aclweb.org/anthology/U17-1009.pdf) мы рассчитали ряд количественных оценок с целью последующих сопоставлений получаемых точностей определения фармацевтически значимых сущностей для представленного в данной работе корпуса и существующих корпусов в мире. Представленная в table \ref{tab:adr_saturation} информация показывает, что точность определения сущностей, на примере ADR, сильно зависит от представительности этой сущности в корпусе по отношению к другим сущностям, в особенности симптомам заболеваний и причинам приема препарата. 





\section{Discussion}
\label{sec:discussion}
Currently, there are a significant diversity of full-sized labeled corpora in different languages to analyze the safety and effectiveness of drugs.  We present the first full-size Russian compound NER-labeled corpus - RDRS - of Internet user reviews with the  labelled coreference relations  in part of the corpus. Based on the developed neural net models results, we investigated this corpus place in this diversity depending on the corpora characteristics. The analysis made on base of the experiment sequence sets of different  saturation  by  the definite entity  extracted from the corpus allows to give the more realistic conclusion  about its quality in concerns to this entity. The results of developed model B on base of   XLM-RoBERTa-large outperforms the results of work~\cite{tutubalina2020russian} on 2.3\% for ADR recieved on the corpus of limited size that grounds  a quality of developed model B and an applicability of its results to  establish  the state of the art precision level of entity extraction on the created corpus. 
%In our opinion, a conclusion about its quality can only be made on base of calculation analysis by advanced models. With this in mind, we solved simultaneously two interconnected tasks: the Russian corpus annotation and the creation of the machine learning complex, testing it preliminary on the commonly acknowledged CADEC corpus. This complex was then used to evaluate the quality of our corpus. It should be noted that it is quite difficult to compare models even on the same corpus, as the authors often use different metrics and varied data splits. Hence, from the very onset we adhered to metrics similar to the works with which we planned to compare the results.

In general, the results of experiments with sets of different sizes and different saturation showed that  in case of ADR mention, a strong dependence  of the  ADR identification precision on corpus saturation  by them  exists (see Figure~\ref{fig:adr_saturation}). So the comparison of our corpus with any one of the close types, such as the СADEC, is necessary to conduct on the dataset of corpus examples with the close saturation by ADRs. The coreference relation extraction experiments show that despite the AnChor-2019 corpus is greater in the number of relations than our corpus; both corpora demonstrate similar precisions when the training set is from the first corpus, the testing set is from the other. But in the case of training on the set of examples from both corpora, we received worse results—these results directly on the essential difference in compositions of the corpora from different domains.

%table \ref{tab:rdrs_subsets}
\begin{figure}
    \centering
    
    %\includegraphics[width=8.5cm,trim={2cm 1.5cm 2cm 2.5cm},clip]{pictures/accuracy_saturation.eps}
    \includegraphics[width=8.5cm]{pictures/saturation_f1_conll.eps}
    
    \caption{Dependency of ADR recognition precision on their saturation in the corpora. Red line  - different subsets of our corpus (see Table~\ref{tab:rdrs_subsets}) with pure ADR annotation. Blue line - different subsets of our corpus with doubtful bordering annotation (annotated both ADR and NOTE), RuDREC - published accuracy for RuDREC corpus~\cite{tutubalina2020russian}, RuDREC\_our - our accuracy for RuDREC corpus, CADEC - published accuracy for CADEC corpus~\cite{li2020lexicon}}. 
    \label{fig:adr_saturation}
\end{figure}


%\begin{table}
%\centering
%\caption{Subsets of open access corpora (RUDrec (labeled %part) and CADEC) with accordance to complexity of ADR %level saturation}
%\input{tables/open_access_subsets.tex}
%\label{tab:open_access_subsets}
%\end{table}


\section{Conclusion}\label{sec:conclusion}
%The basic result of this work is the creation of the full-size Russian compound NER multytag-labeled cor-pus of Internet user reviews, along with an evaluation of accuracy levels reached on this corpus by advanced deep learning neural networks used to extract pharmacologically meaningful entities from Russian texts.The multilabel model basing on a language model ROBERT-XLM  and the selected set of features is developed, appropriated for presented corpus labeling.
The primary basic result of this work is the creation of the Russian full-size  NER multi-tag labeled corpus of the Internet user reviews, including the part of the corpus with annotated coreference relations. The multi-labeling model appropriated for presented corpus labeling based on combining a language model XLM-RoBERTa with the selected set of features is developed. The results obtained basing this model showed that the accuracy level of ADR extraction on our corpus is comparable to that obtained on corpora of other languages with similar characteristics. Thus, this level may be seen as state of the art on this task decision on Russian texts in view. The presence of the corpus part with annotated coreference relations allowed us to evaluate the precision of their extraction on texts of the profile under consideration.

 
  % The relatively low accuracy results for adverse drug effects (ADR) can be explained by the low representativeness of such entity type in the current version of the corpus.
 The developed neuronet complex may be used as a base for the replenishment of the corpus by ADR. This, along with including new entities and relations, is a goal of further work.


\section*{Acknowledgments}
This work has been supported by the Russian Science Foundation grant 20-11-20246 and carried out using computing resources of the federal collective usage center Complex for Simulation and Data Processing for Mega-science Facilities at NRC “Kurchatov Institute”, http://ckp.nrcki.ru/.



%\section{Bibliography}
%% Loading bibliography style file
%\bibliographystyle{model1-num-names}
\bibliographystyle{cas-model2-names}

% Loading bibliography database
\bibliography{cas-dc-template}


\appendix%\label{sec:supplementary}
\section{Appendix}
\subsection*{ADR recognition on the basis of the PsyTAR corpus} \label{subsec:psytar_adr}
PsyTAR corpus from  \cite{basaldella2019bioreddit} contains sentences in a CoNLL format.
This modification of a corpus is publicly available \footnote{Available at https://github.com/basaldella/psytarpreprocessor} and contains train, development and test parts. These parts contain 3535, 431, 1077 entities and 3851, 551, 1192 sentences respectively. We used XLM-RoBERTa-large model  that had been preliminary trained using text data from CommonCrowl project. Fine-tuning of this model provided only for ADR tag excluding WD, SSI, SD tags. The result on the test part was 71.1\% according to the F1 metric achieved with script from the CoNLL evaluation. 


%  XLM-RoBERTa-large was used to get the estimated evaluations for ADR recognition.
\subsection*{Features based on MESHRUS concepts}\label{subsec:meshrus_features}
%MESHRUS is a set of Russian concepts with predefined CUI codes from UMLS. MESHRUS table with are extracted from the UMLS database to dictionary (hash map) $D$: dictionary key -- concept, dictionary value -- concept's CUI from the database. MESHRUS concepts can contain multiple words or an entire sentence. We perform two approaches to automatically find and map named entities, corpus words and concepts.
 MeSH Russian (MESHRUS)~\cite{NLM} is a Russian version of the Medical Subject Headings (MESH) database~\footnote{Home page of the MeSH database site: \url{https://www.nlm.nih.gov/mesh/meshhome.html}}. MESH is a dictionary designed for indexing biomedical information that contains concepts from scientific journal articles and books and is intended for their indexing and searching. The MESH database is filled from articles in English; however, there exist translations of the database to different languages. We used the Russian version, MESHRUS. It is a less complete analogue of the English version, for example, it doesn't contain concept definitions.
    %MESHRUS — переведённый на русский вариант базы Medical Subject Headings. База MESH содержит понятия извлечённые из научных журнальных статей и книг и создана для их индексации и поиска. Наполнение базы MESH происходит с использованием статей на английском языке, однако существуют переводы базы на разные языки. Мы использовали русский вариант — MESHRUS, она является менее полным аналогом англоязычного варианта, например не содержит определений понятий.
MESHRUS contains a set of tuples $(k;v)$ matching Russian concepts $k$ with their relevant CUI codes $v$ from the UMLS thesaurus. A concept $k$ can consist of a word or a sequence of words. %We perform two approaches to automatically find and map concepts from MESHRUS to words from corpus. 

The following preprocessing algorithm is used%for mapping words from the corpus to concepts from the dictionary
: words are lemmatized, put into a single register and filtered by length, frequency and parts of speech. 
To automatically find and map concepts from MESHRUS to words from corpus we perform two approaches.  %for mapping words from the corpus to concepts from the dictionary
%MESHRUS представляет собой словарь русскоязычных концептов с установленными индексами CUI (UMLS), MESHRUS сoncepts are extracted from the UMLS database to dictionary (hash map) $D$: dictionary key -- concept, dictionary value -- concept's CUI from the database. Концепты в MESHRUS могут содержать несколько слов или целое предложение. Нами были созданы два подхода к автоматическому поиску и установлению соотвествия между именнованными сущностями, словами корпуса и концептами. 
%Для сопоставления формируется словарь корпуса W, в который входят слова длинною не менее 3-х символов, встречающиеся в корпуса не менее 5 раз, не являются знаками пунктуации и определенными частям речи (не принадлежат предлогам, союзам, знакам пунктуации). Все слова в словаре лемматизируются и приводятся к единому (нижнему) регистру.

%The first approach is to map the words filtered from W to MESHRUS concepts presented as list of tuples (푘; 푣), where k ∈ К are concepts— CUI of concepts.

The first approach is to map the filtered words $W=\{w_{i}\}_{i=0}^N$ from the corpus to MESHRUS concepts $\{k_j\}$. As a criterion for comparing words and concepts, we used the cosine similarity between their vector representations obtained using the FastText~\cite{bojanowski2017enriching} model (see Section \ref{subsubsec:features}): a word $w_{i}$ is assigned the CUI code $v_{j}$ (see Fig.~\ref{fig:umls}) whose corresponding concept $k_j$ has the highest similarity measure $\cos\left(\mathrm{FastText}(w_{i}), \mathrm{FastText}(k_{j})\right)$. If this similarity measure is lower than the empirical threshold $T = 0.55$, no CUI code is assigned to $w_i$.
\begin{figure}
    \centering
    \includegraphics[scale=0.27]{pictures/umls.jpg}
    \caption{The matching scheme between words of corpus and concepts of UMLS.}
    \label{fig:umls}
\end{figure}

%Первый подход заключается в сопоставлении отфильтрованных слов множества W с составленным словарем выгруженных концептов MESHRUS из UMLS. Словарь $D: K->V$, где $К$ -- это концепты, $V$ — CUI терминов.В качестве критерия сопоставления слов и концептов мы использовали косинусное расстояние между их векторными представлениями полученными с помощью модели FastText: $prox(vector(w_{i})$, $vector(k_{j}))$, где $vector(w_{i})$ -- векторное представления слова $w_{i}$, а $vector(k_{j})$) -- векторное представления концепта $k_{j}$. Слову $w_{i}$ назначается код CUI \ref{fig:umls} концепта $k_{j}$ с максимальным значением меры близости $prox(vector(w_{i})$, $vector(k_{j})) > T$, (значение Т=0.55 вычислино эмпирически).

%Ручная нормализация. Ручная нормализация включала в себя несколько этапов. 1-создавался файл в виде таблицы со столбцами "ключ-значение" где ключ это сущность из корпуса а значение стандартный вид сущности по международным классификаторам, Например человек пишет "Аспирина" и тогда стандартное значение будет "аспирин".
%Далее на основании файла все сущности переименовывались так как принято в АТХ например, после чего идет второй шаг нормализации. 
%2-- Далее в ручную смотрели коды атх через грлс и UMLS и смотрели также международное непатентованное навание, а также CUI и проставляли рядом с лп из таблицы
% drug - АТХ ()
% disease - МКБ ()

The second approach is based on the mapping of syntactically and lexically related phrases extracted at the sentence level. Prepositions, particles and punctuation are not taken. Syntactic features obtained from dependency trees achieved with UDpipe v2.5. 

For each word $w_{i}\subset W$, its adjacent words $[w_{i-1},w_{i+1}]$ are selected. Together with the word itself they form a lexical set $w_{i_l}$. Then, for the current word $w_{i}$ we find the word $w_{i_\text{parent}}$ that is its parent in the dependency tree (if there is no parent, then the syntactic set contains only $w_{i}$). These $w_{i_l}$ and $w_{i_\text{parent}}$ in turn form a syntactic set $w_{i_s}$. 

Similarly, such lexically and syntactically related sets $c_{j_l}$ and $c_{j_s}$ are formed for each filtered word $c_j$ of the concept from the MESHRUS dictionary: $c_{j_l} = [c_{j-1}, c_{j}, c_{j+1}]$, and $c_{j_s} = [c_{j}, c_{j_\text{parent}}]$.

%Второй подход основывается на сопоставлении синтаксически и лексически связанных словосочетаний извлекаемых на уровне предложения. Не учитываются так же слова относящиеся к предлогам, частицам и знаки пунктуации. 
% каждого слова $w_{i}\subsetW$ выбираются его соседние в предложении слова $[w_{i-1},w_{i+1}]$, которые вместе с самим словом формируют лексический набор $w_{i}_{l}$, а также слово $w_{i_{root}}$, являеющееся его родителем в синтаксическом дереве, (если родителя нет, тогда это слово не учитывается), которое в свою очерель вместе с $w_{i}$ формирует синтаксический набор $w_{i}_{s}$. 
%Аналогично каждого отфильтрованного по ЧР слова из концепта $c_{j}\subset concept$ словаря MESHRUS формируется лексически и синтаксически связанный набор - $c_{j}_{l} = [c_{j-1}, c_{j}, c{j+1}]$ и $c_{j}_{s} = [c_{j}, c_{j_{root}}]$.

Further, for each word $w_{i}\subset W$ and word $c_{j}\subset \text{concept}_{k}\subset$ MESHRUS, by analogy with the literature~\cite{shelmanov2015information}, the following metrics are calculated:
%Далее для каждого слова $w_{i}\subsetW$ и слова $c_{j}\subset concept\subset$ MESHRUS по аналогии с~\cite{shelmanov2015information} считаются следующие метрики:
\begin{enumerate}
	\item $\mathrm{lexical\_involvement}(w_{i}, c_{j}) = F_1 \left(\frac{|w_{i_l}\cap c_{j_l}|}{|w_{i_l}|}, \frac{|w_{i_l}\cap c_{j_l}|}{|c_{j_l}|} \right)$
	\item $\mathrm{cohesiveness}(w_{i}, c_{j}) = F_1 \left(\frac{|w_{i_s}\cap c_{j_s}|}{|w_{i_s}|}, \frac{|w_{i_s}\cap c_{j_s}|}{|c_{j_s}|} \right)$
    \item $\mathrm{centrality}$ which is 1 if the word $w_{i_\mathrm{parent}}$ of the syntax set $w_{i_s}$ is represented in the syntax set $c_{j_s}$ of words from the dictionary; 0 otherwise.
\end{enumerate}
Here $F_{1}(x, y)$ is the harmonic mean of $x$ and $y$, $|N|$ denotes the length of set $N$, and $M \cap N$ is the intersection of the two sets.
The final metric of similarity between the word $w_i$ and the dictionary concept $c_j$ is calculated as mean of all three metric values.
%\\$\mathrm{similarity}(w_{i},c_{j}) = \frac{1}{3} \cdot \left(\mathrm{lexical\_involvement} + \mathrm{centrality} + \mathrm{cohesiveness} \right)$.\\
%$f_{1}(x, y)$ - среднее гармоническое между величинами x и y.
%Итоговая метрика similarity схожести между словом и соотвествующим концепт из словаря строится как $similarity(w_{i},c_{j}) = \frac{lexical\_involvement + centrality + cohesiveness}{3}$.

For each word, its corresponding concept is selected by the highest similarity value provided that the similarity is greater than the specified threshold 0.6. 
%Для каждого слова подбирается концепт с наибольшим значением similarity при условии similarity больше заданного порога T. В данной работе порог T ставится равным 0.6.

%The normalization results are shown in Table~\ref{tab:info_collected_corpus}. The ``MESHRUS -- total'' column contains the number of words from $W$ that were annotated as parts of mentions of a particular attribute, the ``MESHRUS -- unique'' column shows the number of unique codes related to the mentions.
%В таблице 8 продемонстрированы результаты работы этой процедуры. В колонке Meshrus:total таблицы 8 приведено количество слов из W, которые пересекались с сущностями данного подкласса, а Meshrus: unique той же таблицы -- количество уникальных кодов, которые попадали под сущности данного подкласса.

% Table \ref{tab:meshrus_stat} contain the results of automated normalization with the help of the MESHRUS dictionary using the two normalization approaches. The numbers show how many times unique words with assigned CUI codes were labeled as parts of any mentions of the attribute under consideration and how many unique CUI indices were assigned to these words.

% \begin{table}
% \centering
% \caption{Statistic of automatic normalization according to MESHRUS tags by 2 methods.} \label{tab:meshrus_stat}
% \input{tables/meshrus_stat.tex}
% \end{table}



%\vskip3pt

% \bio{}
% Author biography without author photo.
% Author biography. Author biography. Author biography.
% \endbio

% \bio{figs/pic1}
% Author biography with author photo.
% Author biography. Author biography. Author biography.
% \endbio

\end{document}

  