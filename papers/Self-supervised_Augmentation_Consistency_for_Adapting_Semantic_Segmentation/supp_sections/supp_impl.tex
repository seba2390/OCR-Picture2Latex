% !TEX root = ../supp.tex

\paragraph{Photometric noise.}
Recall that our framework uses random Gaussian smoothing, greyscaling and colour jittering to implement the photometric noise.
We re-use the parameters for these operations from the MoCo-v2 framework \citesupp{chen2020mocov2}.
In detail, the kernel radius for the Gaussian blur is sampled uniformly from the range $[0.1, 2.0]$.
Note that this does not correspond to the actual filter size.\footnote{The Pillow Library \citesupp{clark2015pillow} internally converts the radius $r$ to the box length as $L = \sqrt{3 * r^2 + 1}$.}
The colour jitter, applied with probability $0.5$, implements a perturbation of the image brightness, contrast and saturation with a factor sampled uniformly from $[0.6, 1.4]$, while the hue factor is sampled uniformly at random in the range of $[0.9, 1.1]$.
We convert a target image to its greyscale version with probability \num{0.2}.
\cref{fig:photometric} demonstrates an example implementation of this procedure in Python.

\begin{figure}[t]
\lstinputlisting[language=Python]{supp_sections/code/photometric.py}
\vspace{-0.5em}
\caption{\textbf{Python implementation of the photometric noise.}}
\label{fig:photometric}
\vspace{-0.5em}
\end{figure}

\myparagraph{Constraint-free data augmentation.}
Similarly to the multi-scale cropping of the target images, we scale the source images randomly with a factor sampled uniformly from $[0.5, 1.0]$ prior to cropping.
However, we do not enforce the semantic consistency for the source data, since the ground truth of the source images is available.
For both the target and source images we also use random horizontal flipping.
%Including the random flipping to the consistency loss for the target data may lead to further accuracy gains, although is not part of the current implementation yet.
We additionally experimented with moderate rotation (both with and without semantic consistency), but did not observe a significant effect on the mean accuracy.

\begin{table*}[t!]
\footnotesize
\begin{tabularx}{\linewidth}{@{}>{\centering\arraybackslash}p{1.5em}>{\centering\arraybackslash}p{1.5em}>{\centering\arraybackslash}p{1.5em}|S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}S[table-format=2.1]@{\hspace{0.74em}}|S[table-format=2.1]@{}}
\toprule
CBT & IS & FL & {road} & {sidew} & {build} & {wall} & {fence} & {pole} & {light} & {sign} & {veg} & {terr} & {sky} & {pers} & {ride} & {car} & {truck} & {bus} & {train} & {moto} & {bicy} & {mIoU} \\
\midrule
 & & & 88.1 & 41.0 & 85.7 & 30.8 & 30.6 & 33.1 & 37.0 & 22.9 & 86.6 & 36.8 & 90.7 & 67.1 & 27.1 & 86.8 & 34.4 & 30.4 & 8.5 & 7.5 & 0.0 & 44.5 \\
\midrule
& & \cmark & 89.4 & 52.3 & 86.0 & \bfseries 34.0 & 32.6 & \bfseries 38.5 & 43.3 & 30.6 & 85.2 & 30.9 & 88.5 & 66.7 & 28.0 & 85.7 & 35.6 & 39.6 & 0.0 & 6.6 & 0.0 & 46.0 \\
& \cmark & & \bfseries 90.0 & 47.1 & 85.6 & 31.3 & 24.9 & 32.3 & 38.9 & 28.2 & \bfseries 87.3 & \bfseries 39.8 & 89.4 & \bfseries 67.7 & 28.6 & \bfseries 88.1 & 40.1 & 50.0 & 7.3 & 9.9 & 2.2 & 46.8 \\
\cmark & & & 89.3 & 39.0 & 85.1 & 33.2 & 26.1 & 32.4 & 41.8 & 25.2 & 86.3 & 27.4 & \bfseries 90.4 & 66.4 & 28.2 & 87.5 & 32.9 & 45.4 & 11.0 & 7.6 & 0.0 & 45.0 \\
\midrule
& \cmark & \cmark & 89.3 & 52.6 & 86.0 & 33.4 & 30.0 & 38.0 & 44.9 & 34.3 & 86.9 & 35.3 & 88.0 & 65.4 & 27.3 & 86.2 & 37.6 & 44.0 & 20.9 & 9.6 & 6.5 & 48.2 \\
\cmark & & \cmark & 89.3 & 52.2 & 86.1 & 34.2 & 31.5 & 37.0 & 43.4 & 36.3 & 85.2 & 30.7 & 86.6 & 66.2 & \bfseries 30.3 & 85.3 & 36.2 & 43.9 & \bfseries 29.2 & 6.8 & 8.6 & 48.4 \\
\cmark & \cmark & & 89.7 & 45.1 & 85.6 & 29.6 & 28.3 & 31.7 & 41.9 & 27.5 & 87.2 & 37.4 & 89.8 & 66.9 & 29.2 & 87.5 & 37.3 & 31.6 & 24.7 & 11.9 & 20.2 & 47.5 \\
\midrule
\cmark & \cmark & \cmark & \bfseries 90.0 & \bfseries 53.1 & \bfseries 86.2 & 33.8 & \bfseries 32.7 & 38.2 & \bfseries 46.0 & \bfseries 40.3 & 84.2 & 26.4 & 88.4 & 65.8 & 28.0 & 85.6 & \bfseries 40.6 & \bfseries 52.9 & 17.3 & \bfseries 13.7 & \bfseries 23.8 & \bfseries 49.9 \\
\bottomrule
\end{tabularx}
\caption{\textbf{Per-class IoU (\%)} on Cityscapes \emph{val} using a VGG-16 backbone in the GTA5 $\rightarrow$ Cityscapes setting. We study three components in more detail: class-based thresholding (CBT), importance sampling (IS) and the focal loss (FL). The mIoU of the settings in the last four rows are reproduced from the main text. Here, we elaborate on the per-class accuracy in a broader context of the supplementary experiments in the first four rows.}
\label{table:result_longtail}
%\vspace{-0.5em}
\end{table*}

\myparagraph{Training schedule.}
Our framework typically needs $150-200$K iterations in total (\ie~including the source-only pre-training) until convergence, as determined on a random subset of \num{500} images from the training set (see our discussion in \cref{sec:supp_eval} below).
This varies slightly depending on the backbone and the source data used.
This schedule translates to approximately \num{3} days of training with standard GPUs (\eg, Titan X Pascal with 12 GB memory) for both VGG-16 and ResNet-101 backbones.
Recall that we used \num{4} GPUs for our ResNet version of the framework, hence its training time is comparable to the VGG variant, which uses only \num{2} GPUs.
All our experiments use a constant learning rate for simplicity, but more advanced schedules, such as cyclical learning rates \cite{IzmailovPGVW18}, the cosine schedule \citesupp{chen2020mocov2,LoshchilovH17} or ramp-ups \cite{LaineA17}, may further improve the accuracy of our framework.
