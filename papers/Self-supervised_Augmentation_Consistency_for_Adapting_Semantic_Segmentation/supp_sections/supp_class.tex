% !TEX root = ../supp.tex

\subsection{A closer look at long-tail adaptation}
Recall that our framework features three components to attune the adaptation process to the long-tail classes: class-based thresholding (CBT), importance sampling (IS) and the focal loss (FL), which we summarily refer to as the \emph{long-tail components} in the following.
Disabling the long-tail components individually is equivalent to setting $\beta \rightarrow 0$ for CBT, using uniform sampling of the target images instead of IS or assigning $\lambda$ to \num{0} for the FL.
Here, we extend our ablation study of the GTA5 $\rightarrow$ Cityscapes setup with VGG-16 (\cf \cref{table:ablation} from the main text) and experiment with different combinations of the long-tail components.
\cref{table:result_longtail} details the per-class accuracy of the possible compositions.

We observe that the ubiquitous classes -- ``road'', ``building'', ``vegetation'', ``sky'', ``person'' and ``car'' -- are hardly affected;
it is primarily the long-tail categories that change in accuracy.
Furthermore, our long-tail components are mutually complementary.
The mean IoU improves when one of the components is active, from $44.5\%$ to up to $46.8\%$.
It is boosted further with two of the components enabled to $48.4\%$, and reaches its maximum for our model, $49.9\%$, when all three components are in place.

We further identify the following tentative patterns.
FL tends to improve classes ``wall'', ``fence'' and ``pole''.
CBT increases the accuracy of the ``traffic light'' category (which has high image frequency and occupies only a few pixels), but also rare classes, such as ``rider'', ``bus'' and ``train'' benefit from CBT, especially in conjunction with IS.
IS also enhances the mask quality of the classes ``bicycle'' and ``motorcycle''.
Nevertheless, we urge caution against interpreting the results for each class in isolation, despite such widespread practice in the literature.
Today's semantic segmentation models do not possess the notion of an `ambiguous' class prediction and each pixel receives a meaningful label.
By the pigeon's hole principle, this implies that the changes in the IoU of one class have an immediate effect on the IoU of the other classes.
Therefore, the benefits of individual framework components have to be understood in the context of their aggregated effect on multiple classes, \eg~using the mean IoU.
For instance, consider the class ``train'' for which IS appears to also decrease the IoU: while CBT together with FL achieve $29.2\%$ IoU, adding IS decreases the IoU to $17.3\%$.
However, the IoU of other classes increases (\eg, ``motorcycle'', ``bicycle''), as does the mean IoU.
Furthermore, only few classes reach their maximum accuracy when we enable all three long-tail components.
Yet, it is the setting with the best \emph{accuracy trade-off} between the individual classes, \ie~with the highest mean IoU.
Overall, the long-tail components improve our framework by $5.4\%$ mean IoU combined, a substantial margin.

\begin{table*}[t!]
\footnotesize
\begin{tabularx}{\linewidth}{@{}l|S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}|c@{}}
\toprule
Method & {road} & {sidew} & {build} & {wall} & {fence} & {pole} & {light} & {sign} & {veg} & {terr} & {sky} & {pers} & {ride} & {car} & {truck} & {bus} & {train} & {moto} & {bicy} & {mIoU} \\
\midrule
\multicolumn{21}{@{}l}{\scriptsize \textit{GTA5 $\rightarrow$ Cityscapes}} \\
\midrule
Baseline (ours) & 76.7 & 28.2 & 74.4 & 12.7 & 19.0 & 27.2 & 28.7 & 12.2 & 77.0 & 18.0 & 70.6 & 54.8 & 20.6 & 79.6 & 19.0 & 19.2 & 20.6 & 27.9 & 11.2 & 36.7 { \scriptsize{(37.1)}} \\
%SAC (ours) & 87.3 & 47.1 & 84.1 & 29.5 & 26.5 & 23.9 & 42.7 & 30.8 & 86.8 & 42.5 & 87.5 & 60.2 & 30.1 & 83.0 & 28.3 & 38.2 & 28.2 & 33.4 & 44.6 & 49.2 { \scriptsize{(49.9)}} \\
SAC-FCN (ours) & 86.3 & 45.6 & 84.4 & 30.3 & 27.1 & 24.8 & 42.8 & 35.2 & 86.9 & 39.7 & 88.0 & 62.3 & 32.1 & 84.1 & 28.4 & 43.7 & 31.9 & 29.4 & 45.8 & 49.9 { \scriptsize{(49.9)}} \\
\midrule
\multicolumn{21}{@{}l}{\scriptsize \textit{SYNTHIA $\rightarrow$ Cityscapes}} \\
\midrule
Baseline (ours) & 50.7 & 23.8 & 60.9 & 1.8 & 0.1 & 27.7 & 10.5 & 15.7 & 60.1 & \textemdash & 72.4 & 50.1 & 16.0 & 66.5 & \textemdash & 13.7 & \textemdash & 8.5 & 26.8 & 31.6 { \scriptsize{(34.4)}} \\
%SAC (ours) & 80.8 & 39.6 & 81.9 & 18.0 & 1.1 & 27.8 & 35.2 & 28.0 & 79.0 & \textemdash & 80.6 & 61.5 & 23.1 & 81.8 & \textemdash & 36.6 & \textemdash & 32.4 & 55.7 & 47.7 { \scriptsize{(49.1)}} \\
SAC-FCN (ours) & 74.7 & 34.2 & 81.4 & 19.8 & 1.9 & 27.2 & 34.8 & 27.2 & 80.0 & \textemdash & 86.3 & 61.5 & 20.8 & 82.5 & \textemdash & 31.2 & \textemdash & 32.0 & 53.9 & 46.8 { \scriptsize{(49.1)}} \\
\bottomrule
\end{tabularx}
\caption{\textbf{Per-class IoU (\%)} on Cityscapes \emph{val} using VGG-16 with FCN8s. For reference, the numbers in parentheses in the last column report the mean IoU of the DeepLabv2 architecture (\cf \cref{table:result_gta_to_city,table:synthia_gta_to_city} from the main text).}
\label{table:result_fcn}
%\vspace{-0.5em}
\end{table*}

\begin{table}[t]
\centering
\setlength{\tabcolsep}{0.8em}%
\begin{tabularx}{0.7\linewidth}{@{}X@{}S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]@{}}
\toprule
$\downarrow \zeta \hfill/\hfill \beta \rightarrow\;$ & {$0.0001$} & {$0.001$} & {$0.01$} \\
\midrule
$0.7$ & 47.9 & 48.8 & 46.7 \\
$0.75$ & 48.6 & 49.9 & 46.3 \\
$0.8$ & 48.2 & 49.8 & 45.6 \\
\bottomrule
\end{tabularx}
\caption{\textbf{Mean IoU (\%) on GTA5 $\rightarrow$ Cityscapes (val) with varying $\zeta$ and $\beta$.} Our framework maintains strong accuracy under different settings of $\zeta$ and $\beta$. Even with a poor choice (\eg, $\zeta = 0.8$, $\beta = 0.01$), it fares well \wrt the state of the art and outperforms many previous works (\cf \cref{table:result_gta_to_city} from the main text).}
\label{table:sensitivity}
\end{table}

\subsection{Hyperparameter search and sensitivity}
\label{sec:hyper_sensitivity}

To select $\zeta$ and $\beta$, we first experimented with a few reasonable choices ($\zeta \in (0.7, 0.8)$, $\beta \in (0.0001, 0.01)$)\footnote{While $\zeta$ may seem more interpretable (the maximum confidence threshold), a reasonable range for $\beta$ can be derived from $\chi_c$ for the long-tail classes, which is simply the fraction of pixels these classes tend to occupy in the image (see Eq.~3).} using a more lightweight backbone (MobileNetV2 \citesupp{Sandler2018:MIR}).
To measure performance, we use the mean IoU on the validation set (500 images from Cityscapes \textit{train}, as in the main text).

Here, we study our framework's sensitivity to the particular choice of $\zeta$ and $\beta$.
To make the results comparable to our previous experiments, we use VGG-16 and report the mean IoU on Cityscapes \textit{val} in \cref{table:sensitivity}.
We observe moderate deviation of the IoU \wrt $\zeta$.
A more tangible drop in accuracy with $\beta = 0.01$ is expected, as it leads to low-confidence predictions, which are likely to be inaccurate, to be included into the pseudo label.
We note that while a suboptimal choice of these hyperparameters leads to inferior results (with a standard deviation of $\pm 1.4$\% mIoU), even the weakest model with $\zeta = 0.8$ and $\beta = 0.01$ did not fail to considerably improve over the baseline (by $8.5$\% IoU, \cf \cref{table:result_gta_to_city} in the main text).


\begin{table*}[t!]
\footnotesize
\begin{tabularx}{\linewidth}{@{}l|S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}S[table-format=2.1]@{\hspace{0.6em}}|c@{}}
\toprule
Method & {road} & {sidew} & {build} & {wall} & {fence} & {pole} & {light} & {sign} & {veg} & {terr} & {sky} & {pers} & {ride} & {car} & {truck} & {bus} & {train} & {moto} & {bicy} & {mIoU} \\
\midrule
\multicolumn{21}{@{}l}{\scriptsize \textit{GTA5 $\rightarrow$ Cityscapes}} \\
\midrule
SAC-FCN (ours) & 87.5 & 45.2 & 85.0 & 29.2 & 26.4 & 23.3 & 44.2 & 32.0 & 88.3 & 52.6 & 91.2 & 65.2 & 35.0 & 86.0 & 24.4 & 32.8 & 31.4 & 36.9 & 40.5 & 50.4 { \scriptsize{(49.9)}} \\
SAC-VGG (ours) & 91.5 & 53.9 & 86.6 & 34.1 & 31.5 & 36.8 & 47.2 & 36.9 & 85.1 & 38.0 & 91.1 & 68.7 & 31.9 & 87.4 & 31.0 & 46.7 & 22.6 & 24.2 & 24.0 & 51.0 { \scriptsize{(49.9)}} \\
SAC-ResNet (ours) & 91.8 & 54.3 & 87.4 & 36.2 & 30.2 & 43.7 & 49.7 & 42.1 & 89.3 & 54.3 & 90.5 & 71.8 & 34.9 & 89.8 & 38.8 & 47.3 & 24.9 & 38.3 & 43.8 & 55.7 { \scriptsize{(53.8)}} \\
\midrule
\multicolumn{21}{@{}l}{\scriptsize \textit{SYNTHIA $\rightarrow$ Cityscapes}} \\
\midrule
SAC-FCN (ours) & 66.9 & 25.9 & 80.8 & 12.1 & 2.0 & 24.4 & 37.1 & 27.5 & 78.8 & \textemdash & 88.9 & 63.9 & 25.0 & 84.7 & \textemdash & 27.4 & \textemdash & 36.9 & 50.2 & 45.8 { \scriptsize{(46.8)}} \\
SAC-VGG (ours) & 70.4 & 29.7 & 83.6 & 11.6 & 1.8 & 34.2 & 41.2 & 29.2 & 81.0 & \textemdash & 87.1 & 67.9 & 25.4 & 75.9 & \textemdash & 34.3 & \textemdash & 42.5 & 57.5 & 48.3 { \scriptsize{(49.1)}} \\
SAC-ResNet (ours) & 87.4 & 41.0 & 85.5 & 17.5 & 2.6 & 40.5 & 44.7 & 34.4 & 87.9 & \textemdash & 91.2 & 68.0 & 31.0 & 89.3 & \textemdash & 33.2 & \textemdash & 38.6 & 49.9 & 52.7 { \scriptsize{(52.6)}} \\
\midrule
\multicolumn{21}{@{}l}{\scriptsize \textit{Fully supervised (Cityscapes)}} \\
\midrule
DeepLab-ResNet \cite{ChenPKMY18} & 97.9 & 81.3 & 90.4 & 48.8 & 47.4 & 49.6 & 57.9 & 67.3 & 91.9 & 69.4 & 94.2 & 79.8 & 59.8 & 93.7 & 56.5 & 67.5 & 57.5 & 57.7 & 68.8 & 70.4 \\
FCN-VGG \citesupp{ShelhamerLD17} & 97.4 & 78.4 & 89.2 & 34.9 & 44.2 & 47.4 & 60.1 & 65.0 & 91.4 & 69.3 & 93.9 & 77.1 & 51.4 & 92.6 & 35.3 & 48.6 & 46.5 & 51.6 & 66.8 & 65.3 \\
\bottomrule
\end{tabularx}
\caption[\textbf{Per-class IoU (\%)} on Cityscapes \emph{test}]{\textbf{Per-class IoU (\%)} on Cityscapes \emph{test}. In the last column, the numbers in parentheses report the mean IoU on Cityscapes \emph{val} from the previous evaluation scheme (\cf \cref{table:result_gta_to_city,table:synthia_gta_to_city} from the main text) for reference. SAC-FCN denotes our VGG-based model with FCN8s \citesupp{ShelhamerLD17} from \cref{sec:fcn}.}
\label{table:result_city_test}
%\vspace{-0.5em}
\end{table*}

\subsection{VGG-16 with FCN8s}
\label{sec:fcn}
A number of previous works (\eg, \cite{MustoZ20,Yang_2020_ECCV,0001S20}) used the FCN8s \citesupp{ShelhamerLD17} architecture with VGG-16, as opposed to DeepLabv2 \cite{ChenPKMY18}, adopted in other works (\eg, \cite{KimB20a,Wang_2020_ECCV}) and ours.
Such architecture exchange appears to have been dismissed in previous work as minor, which used only one of the architectures in the experiments.
However, the segmentation architecture alone may contribute to the observed differences in accuracy of the methods and, more critically, to the improvements otherwise attributed to other aspects of the approach.
To facilitate such transparency in our work, we replace DeepLabv2 with its FCN8s counterpart in our framework (with the VGG-16 backbone) and repeat the adaptation experiments from \cref{sec:exp}, \ie~using two source domains, GTA5 and SYNTHIA, and Cityscapes as the target domain.
We keep the values of the hyperparameters the same, with an exception of the learning rate, which we increase by a factor of \num{2} to $5\times 10^4$.
\cref{table:result_fcn} reports the results of the adaptation, which clearly show that our framework generalises well to other segmentation architectures.
Despite the FCN8s baseline model (source-only loss with ABN) achieving a slightly inferior accuracy compared to DeepLabv2 (\eg, $31.6\%$ \vs~$34.4\%$ IoU for SYNTHIA $\rightarrow$ Cityscapes), our self-supervised training still attains a remarkably high accuracy, $46.8\%$ IoU (\vs~$49.1\%$ with DeepLabv2).
This is substantially higher than the previous best method using FCN8s with the VGG-16 backbone, SA-I2I \cite{MustoZ20}: $+3.4\%$ on GTA5 $\rightarrow$ Cityscapes and $+5.3\%$ on SYNTHIA $\rightarrow$ Cityscapes.
