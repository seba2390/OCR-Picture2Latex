% !TEX root = ../supp.tex

The current strategy to evaluate domain adaptation (DA) methods for semantic segmentation is to use the ground truth of \num{500} randomly selected images from the Cityscapes \textit{train} split for model selection and to report the final model accuracy on the \num{500} Cityscapes \textit{val} images \cite{LianDLG19}.
In this work, we adhered to this procedure to enable a fair comparison to previous work.
However, this evaluation approach is evidently in discord with the established best practice in machine learning and with the benchmarking practice on Cityscapes \cite{CordtsORREBFRS16}, in particular.

The test set is holdout data to be used only for an unbiased performance assessment (\eg, segmentation accuracy) of the final model \citesupp{0082591}.
While it is conceivable to consult the test set for verifying a number of model variants, such access cannot be unrestrained.
This is infeasible to ensure when the test set annotation is public, as is the case with Cityscapes \textit{val}, however.
Benchmark websites traditionally enable a restricted access to the test annotation through impartial submission policies (\eg, limited number of submissions per time window and user), and Cityscapes officially provides one.\footnote{\url{https://www.cityscapes-dataset.com}}

We, therefore, suggest a simple revision of the evaluation protocol for evaluating future DA methods.
As before, we use Cityscapes \textit{train} as the training data for the target domain, naturally without the ground truth.
For model selection, however, we use Cityscapes \textit{val} images with the ground-truth labels.
The holdout test set for reporting the final segmentation accuracy after adaptation becomes Cityscapes \textit{test}, with the results obtained via submitting the predicted segmentation masks to the official Cityscapes benchmark server.

An additional advantage of this strategy is a clear interpretation of the final accuracy in the context of fully supervised methods that routinely use the same evaluation setup.
Also note that Cityscapes \textit{val} contains images from different cities than Cityscapes \textit{train} (which are also different from Cityscapes \textit{test}).
Therefore, it is more suitable for detecting cases of model overfitting on particularities of the city, since the validation set was previously a subset of the training images.

For future reference, we evaluate our framework (both the DeepLabv2 and FCN8s variants) in the proposed setup and report the results in \cref{table:result_city_test}.
To ease the comparison, we juxtapose our validation results reported in the main text (from \cref{table:result_fcn} for FCN8s).\footnote{To our best knowledge, no previous work published their results in this evaluation setting before.}
As we did not finetune our method to Cityscapes \emph{val} following the previous evaluation protocol, we expect the test accuracy on Cityscapes \emph{test} to be on a par with our previously reported accuracy on Cityscapes \emph{val}.
The results in \cref{table:result_city_test} clearly confirm this expectation: the segmentation accuracy on Cityscapes \emph{test} is comparable to the accuracy on Cityscapes \emph{val} (SYNTHIA $\rightarrow$ Cityscapes) or even tangibly higher (GTA5 $\rightarrow$ Cityscapes).
We remark that the remaining accuracy gap to the fully supervised model is still considerable (70.4\% \vs 55.7\% IoU achieved by our best DeepLabv2 model and 65.3\% \vs 51.0\% IoU compared to our best FCN8s variant), which invites further effort from the research community.

We hope that future UDA methods for semantic segmentation will follow suit in reporting the results on Cityscapes \emph{test}.
Owing to the regulated access to the test set, we believe this setting to offer more transparency and fairness to the benchmarking process, and will successfully drive the progress of UDA for semantic segmentation, as it has done in the past for the fully supervised methods.
