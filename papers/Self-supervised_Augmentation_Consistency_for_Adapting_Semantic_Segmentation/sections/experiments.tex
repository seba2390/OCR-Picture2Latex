% !TEX root = ../arxiv.tex

\paragraph{Datasets.}
In our experiments we use three datasets.
The Cityscapes dataset \cite{CordtsORREBFRS16} contains $\text{2048} \times \text{1024}$ images from real-world traffic scenes, split into \num{2975} images for training and \num{500} for validation.
The GTA5 dataset \cite{RichterVRK16} contains \num{24966} synthetic scenes with resolution $\text{1914} \times \text{1052}$ and pixelwise annotation aided by the GTA5 game engine.
We also use the SYNTHIA-RAND-CITYSCAPES subset of the SYNTHIA dataset \cite{RosSMVL16}, which contains \num{9400} synthetic images with resolution $\text{1280} \times \text{760}$ and a semantic annotation compatible with Cityscapes.


\begin{table*}
\footnotesize
\begin{tabularx}{\linewidth}{@{}X|S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}S[table-format=2.1]@{\hspace{0.7em}}|S[table-format=2.1]@{}}
\toprule
Method & {road} & {sidew} & {build} & {wall} & {fence} & {pole} & {light} & {sign} & {veg} & {terr} & {sky} & {pers} & {ride} & {car} & {truck} & {bus} & {train} & {moto} & {bicy} & {mIoU} \\
\midrule
\multicolumn{21}{@{}l}{\scriptsize \textit{Backbone: VGG-16}} \\
\midrule
CyCADA \cite{HoffmanTPZISED18} & 85.2 & 37.2 & 76.5 & 21.8 & 15.0 & 23.8 & 22.9 & 21.5 & 80.5 & 31.3 & 60.7 & 50.5 & 9.0 & 76.9 & 17.1 & 28.2 & 4.5 & 9.8 & 0.0 & 35.4 \\
ADVENT \cite{VuJBCP19} & 86.9 & 28.7 & 78.7 & 28.5 & 25.2 & 17.1 & 20.3 & 10.9 & 80.0 & 26.4 & 70.2 & 47.1 & 8.4 & 81.5 & 26.0 & 17.2 & 18.9 & 11.7 & 1.6 & 36.1 \\
CBST \cite{ZouYKW18} & 90.4 & 50.8 & 72.0 & 18.3 & 9.5 & 27.2 & 8.6 & 14.1 & 82.4 & 25.1 & 70.8 & 42.6 & 14.5 & 76.9 & 5.9 & 12.5 & 1.2 & 14.0 & 28.6 & 36.1 \\
PyCDA \cite{LianDLG19} & 86.7 & 24.8 & 80.9 & 21.4 & 27.3 & 30.2 & 26.6 & 21.1 & \bfseries 86.6 & 28.9 & 58.8 & 53.2 & 17.9 & 80.4 & 18.8 & 22.4 & 4.1 & 9.7 & 6.2 & 37.2 \\
PIT \cite{LvLCL20} & 86.2 & 35.0 & 82.1 & 31.1 & 22.1 & 23.2 & 29.4 & 28.5 & 79.3 & 31.8 & 81.9 & 52.1 & 23.2 & 80.4 & 29.5 & 26.9 & \bfseries 30.7 & 20.5 & 1.2 & 41.8 \\
FDA \cite{0001S20} & 86.1 & 35.1 & 80.6 & 30.8 & 20.4 & 27.5 & 30.0 & 26.0 & 82.1 & 30.3 & 73.6 & 52.5 & 21.7 & 81.7 & 24.0 & 30.5 & 29.9 & 14.6 & 24.0 & 42.2 \\
LDR \cite{Yang_2020_ECCV} & 90.1 & 41.2 & 82.2 & 30.3 & 21.3 & 18.3 & 33.5 & 23.0 & 84.1 & 37.5 & 81.4 & 54.2 & 24.3 & 83.0 & 27.6 & 32.0 & 8.1 & \bfseries 29.7 & 26.9 & 43.6 \\
FADA \cite{Wang_2020_ECCV} & \bfseries 92.3 & 51.1 & 83.7 & 33.1 & 29.1 & 28.5 & 28.0 & 21.0 & 82.6 & 32.6 & 85.3 & 55.2 & 28.8 & 83.5 & 24.4 & 37.4 & 0.0 & 21.1 & 15.2 & 43.8 \\
CD-AM \cite{Yang_2021_WACV} & 90.1 & 46.7 & 82.7 & \bfseries 34.2 & 25.3 & 21.3 & 33.0 & 22.0 & 84.4 & \bfseries 41.4 & 78.9 & 55.5 & 25.8 & 83.1 & 24.9 & 31.4 & 20.6 & 25.2 & 27.8 & 44.9 \\
SA-I2I \cite{MustoZ20} & 91.1 & 46.4 & 82.9 & 33.2 & 27.9 & 20.6 & 29.0 & 28.2 & 84.5 & 40.9 & 82.3 & 52.4 & 24.4 & 81.2 & 21.8 & 44.8 & 31.5 & 26.5 & \bfseries 33.7 & 46.5 \\
\midrule
Baseline (ours) & 81.5 & 28.6 & 79.5 & 23.2 & 21.1 & 31.3 & 28.2 & 18.5 & 75.6 & 14.9 & 72.2 & 58.0 & 17.1 & 81.1 & 19.7 & 26.3 & 13.7 & 12.9 & 2.1 & 37.1 \\
SAC (ours) & 90.0 & \bfseries 53.1 & \bfseries 86.2 & 33.8 & \bfseries 32.7 & \bfseries 38.2 & \bfseries 46.0 & \bfseries 40.3 & 84.2 & 26.4 & \bfseries 88.4 & \bfseries 65.8 & \bfseries 28.0 & \bfseries 85.6 & \bfseries 40.6 & \bfseries 52.9 & 17.3 & 13.7 & 23.8 & \bfseries 49.9 \\
\midrule
\multicolumn{21}{@{}l}{\scriptsize \textit{Backbone: ResNet-101}} \\
\midrule
PyCDA$^\dagger$ \cite{LianDLG19} & 90.5 & 36.3 & 84.4 & 32.4& 28.7 & 34.6 & 36.4 & 31.5 & 86.8 & 37.9 & 78.5 & 62.3 & 21.5 & 85.6 & 27.9 & 34.8 & 18.0 & 22.9 & 49.3 & 47.4 \\
CD-AM \cite{Yang_2021_WACV} & 91.3 & 46.0 & 84.5 & 34.4 & 29.7 & 32.6 & 35.8 & 36.4 & 84.5 & 43.2 & 83.0 & 60.0 & 32.2 & 83.2 & 35.0 & 46.7 & 0.0 & 33.7 & 42.2 & 49.2 \\
FADA \cite{Wang_2020_ECCV} & 92.5 & 47.5 & 85.1 & 37.6 & \bfseries 32.8 & 33.4 & 33.8 & 18.4 & 85.3 & 37.7 & 83.5 & 63.2 & \bfseries 39.7 & 87.5 & 32.9 & 47.8 & 1.6 & 34.9 & 39.5 & 49.2 \\
LDR \cite{Yang_2020_ECCV} & 90.8 & 41.4 & 84.7 & 35.1 & 27.5 & 31.2 & 38.0 & 32.8 & 85.6 & 42.1 & 84.9 & 59.6 & 34.4 & 85.0 & 42.8 & 52.7 & 3.4 & 30.9 & 38.1 & 49.5 \\
FDA \cite{0001S20} & 92.5 & 53.3 & 82.4 & 26.5 & 27.6 & 36.4 & 40.6 & 38.9 & 82.3 & 39.8 & 78.0 & 62.6 & 34.4 & 84.9 & 34.1 & 53.1 & 16.9 & 27.7 & 46.4 &  50.5 \\
SA-I2I \cite{MustoZ20} & 91.2 & 43.3 & 85.2 & 38.6 & 25.9 & 34.7 & 41.3 & 41.0 & 85.5 & \bfseries 46.0 & 86.5 & 61.7 & 33.8 & 85.5 & 34.4 & 48.7 & 0.0 & 36.1 & 37.8 & 50.4 \\
PIT \cite{LvLCL20} & 87.5  & 43.4  & 78.8  & 31.2  & 30.2  & 36.3  & 39.9  & 42.0  & 79.2  & 37.1  & 79.3  & 65.4  & 37.5  & 83.2  & 46.0  & 45.6  & 25.7  & 23.5  & 49.9 & 50.6 \\
IAST \cite{Mei_2020_ECCV} & \bfseries 93.8 & \bfseries 57.8 & 85.1 & 39.5 & 26.7 & 26.2 & 43.1 & 34.7 & 84.9 & 32.9 & \bfseries 88.0 & 62.6 & 29.0 & 87.3 & 39.2 & 49.6 & 23.2 & 34.7 & 39.6 & 51.5 \\
RPT$^\dagger$ \cite{ZhangQYNL020} & 89.2 & 43.3 & 86.1 & 39.5 & 29.9 & 40.2 & \bfseries 49.6 & 33.1 & \bfseries 87.4 & 38.5 & 86.0 & 64.4 & 25.1 & \bfseries 88.5 & 36.6 & 45.8 & \bfseries 23.9 & \bfseries 36.5 & \bfseries 56.8 &  52.6 \\
\midrule
Baseline (ours) & 80.2 & 29.3 & 76.8 & 23.8 & 21.9 & 37.7 & 35.4 & 21.1 & 79.8 & 21.3 & 75.0 & 59.5 & 17.5 & 83.5 & 22.4 & 33.4 & 13.0 & 30.7 & 12.3 & 40.8 \\
SAC (ours) & 90.4 & 53.9 & \bfseries 86.6 & \bfseries 42.4 & 27.3 & \bfseries 45.1 & 48.5 & \bfseries 42.7 & \bfseries 87.4 & 40.1 & 86.1 & \bfseries 67.5 & 29.7 & \bfseries 88.5 & \bfseries 49.1 & \bfseries 54.6 & 9.8 & 26.6 & 45.3 & \bfseries 53.8 \\
\bottomrule
\multicolumn{21}{@{}l}{\scriptsize $(^\dagger)$ denotes the use of PSPNet \cite{ZhaoSQWJ17} instead of DeepLabv2 \cite{ChenPKMY18}.} \\
\end{tabularx}
\caption{\textbf{Per-class IoU (\%) comparison} on GTA5 $\rightarrow$ Cityscapes adaptation, evaluated on the Cityscapes validation set.}
\label{table:result_gta_to_city}
\vspace{-0.5em}
\end{table*}

\myparagraph{Setup.}
We adopt the established evaluation protocol from previous work \cite{LianDLG19,TsaiHSS0C18,VuJBCP19}.
The synthetic traffic scenes from GTA5 \cite{RichterVRK16} and SYNTHIA \cite{RosSMVL16} serve as the source data, and the real images from the Cityscapes dataset as the target (obviously ignoring the available semantic labels).
This results in two domain adaptation scenarios depending on the choice of the source data: GTA5 $\rightarrow$ Cityscapes and SYNTHIA $\rightarrow$ Cityscapes.
As in previous work, at training time we only use the training split of the Cityscapes dataset and report the results on the validation split.
We measure the segmentation accuracy with per-class Intersection-over-Union (IoU) and its average, the mean IoU (mIoU).

\subsection{Implementation details}
We implement our framework in PyTorch \cite{NEURIPS2019_9015}.
We adopt DeepLabv2 \cite{ChenPKMY18} as the segmentation architecture, and evaluate our method with two backbones, ResNet-101 \cite{HeZRS16} and VGG16 \cite{SimonyanZ14a}, following recent work \cite{KimB20a,TsaiHSS0C18,TsaiSSC19,VuJBCP19,WangYWFXHHS20}.
Both backbones initialise from the models pre-trained on ImageNet \cite{imagenet_cvpr09}.
We first train the models with ABN \cite{LiWSHL18} (\cf~\cref{sec:training}), implemented via SyncBN \cite{NEURIPS2019_9015}, on multi-scale crops resized to $640 \times 640$ and a batch size of 16.
Next, training proceeds with the self-supervised target loss (\cf~\cref{sec:selftrain}) and the BatchNorm layers \cite{IoffeS15} frozen.
The batch size of $16$ comprises $8$ source images and $8$ target images at resolution $1024 \times 512$, which is a common practice \cite{Wang_2020_ECCV,0001S20}.
The target batch contains only two image samples along with $3$ random crops each (\ie~$N=3$ in \cref{sec:batch}), downscaled up to a factor of $0.5$.
As the photometric noise, we use colour jitter, random blur and greyscaling (see \cref{sec:supp_impl} for details).
The optimisation uses SGD with a constant learning rate of $2.5 \times 10^{-4}$, momentum $0.9$ and weight decay of $5 \times 10^{-4}$.
We accumulate the gradient in alternating source-target forward passes to keep the memory footprint in check.
Since the focal term in \cref{eq:loss} reduces the target loss magnitude \wrt the source loss, we scale it up by a factor of $5$ ($2$ for VGG-16).
We train our VGG-based framework on two TITAN X GPUs (12GB), while the ResNet-based variant requires four.
This is a substantially reduced requirement compared to recent work (\eg, FADA \cite{Wang_2020_ECCV} requires 4 Tesla P40 GPUs with 24GB memory).
Note that the momentum network is always in evaluation mode, has gradient tracking disabled, hence adds only around $35\%$ memory overhead.
For the momentum network, we fix $\gamma_\psi = 0.99$ and $T = 100$ in all our experiments.
For the other hyperparameters, we use $\gamma_\chi = 0.99$, $\zeta = 0.75$, $\beta = 10^{-3}$ and $\lambda = 3$.
\cref{sec:hyper_sensitivity} provides further detail on hyperparameter selection, as well as a sensitivity analysis of our framework \wrt $\zeta$ and $\beta$.
The inference follows the usual procedure of a single forward pass through the segmentation network at the original image resolution without any post-processing.

\subsection{Comparison to state of the art}
We compare our approach to the current state of the art on the two domain adaptation scenarios: GTA5 $\rightarrow$ Cityscapes in \cref{table:result_gta_to_city} and SYNTHIA $\rightarrow$ Cityscapes in \cref{table:synthia_gta_to_city}.
For a fair comparison, all numbers originate from single-scale inference.
In both cases, our approach, denoted as SAC (``Self-supervised Augmentation Consistency''), substantially outperforms our baseline (\ie~the source-only loss model with ABN, see \cref{sec:training}), and, in fact, sets a new state of the art in terms of mIoU.
Importantly, while the ranking of previous works depends on the backbone choice and the source data, we reach the top rank consistently in all settings.

\begin{table*}
\footnotesize
\begin{tabularx}{\linewidth}{@{}X|S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}S[table-format=2.1]@{\hspace{0.8em}}|S[table-format=2.1]@{\hspace{0.8em}}|S[table-format=2.1]@{}}
\toprule
Method & {road} & {sidew} & {build} & {wall} & {fence} & {pole} & {light} & {sign} & {veg} & {sky} & {pers} & {ride} & {car} & {bus} & {moto} & {bicy} & {mIoU$^\text{13}$} & {mIoU} \\
\midrule
\multicolumn{19}{@{}l}{\scriptsize \textit{Backbone: VGG-16}} \\
\midrule
PyCDA \cite{LianDLG19} & 80.6 & 26.6 & 74.5 & 2.0 & 0.1 & 18.1 & 13.7 & 14.2 & 80.8 & 71.0 & 48.0 & 19.0 & 72.3 & 22.5 & 12.1 & 18.1 & \textemdash & 35.9 \\
PIT \cite{LvLCL20} & 81.7 & 26.9 & 78.4 & 6.3 & 0.2 & 19.8 & 13.4 & 17.4 & 76.7 & 74.1 & 47.5 & 22.4 & 76.0 & 21.7 & 19.6 & 27.7 & \textemdash & 38.1 \\
FADA \cite{Wang_2020_ECCV} & 80.4 & 35.9 & 80.9 & 2.5 & 0.3 & 30.4 & 7.9 & 22.3 & \bfseries 81.8 & \bfseries 83.6 & 48.9 & 16.8 & 77.7 & 31.1 & 13.5 & 17.9 & \textemdash & 39.5 \\
FDA \cite{0001S20} & \bfseries 84.2 & 35.1 & 78.0 & 6.1 & 0.4 & 27.0 & 8.5 & 22.1 & 77.2 & 79.6 & 55.5 & 19.9 & 74.8 & 24.9 & 14.3 & 40.7 & \textemdash & 40.5 \\
CD-AM \cite{Yang_2021_WACV} & 73.0 & 31.1 & 77.1 & 0.2 & 0.5 & 27.0 & 11.3 & 27.4 & 81.2 & 81.0 & 59.0 & \bfseries 25.6 & 75.0 & 26.3 & 10.1 & 47.4 & \textemdash & 40.8 \\
LDR \cite{Yang_2020_ECCV} & 73.7 & 29.6 & 77.6 & 1.0 & 0.4 & 26.0 & 14.7 & 26.6 & 80.6 & 81.8 & 57.2 & 24.5 & 76.1 & 27.6 & 13.6 & 46.6 & \textemdash &41.1 \\
SA-I2I \cite{MustoZ20} & 79.1 & 34.0 & 78.3 & 0.3 & 0.6 & 26.7 & 15.9 & \bfseries 29.5 & 81.0 & 81.1 & 55.5 & 21.9 & 77.2 & 23.5 & 11.8 & 47.5 & \textemdash & 41.5 \\
\midrule
Baseline (ours) & 60.7 & 26.9 & 67.1 & 8.3 & 0.0 & 33.5 & 11.9 & 18.3 & 66.4 & 70.4 & 52.1 & 16.1 & 64.6 & 15.5 & 11.5 & 26.4 & 39.1 & 34.4 \\
SAC (ours) & 77.9 & \bfseries 38.6 & \bfseries 83.5 & \bfseries 15.8 & \bfseries 1.5 & \bfseries 38.2 & \bfseries 41.3 & 27.9 & 80.8 & 83.0 & \bfseries 64.3 & 21.2 & \bfseries 78.3 & \bfseries 38.5 & \bfseries 32.6 & \bfseries 62.1 & 56.2 & \bfseries 49.1 \\
\midrule
\multicolumn{19}{@{}l}{\scriptsize \textit{Backbone: ResNet-101}} \\
\midrule
ADVENT \cite{VuJBCP19} & 85.6 & 42.2 & 79.7 & 8.7 & 0.4 & 25.9 & 5.4 & 8.1 & 80.4 & 84.1 & 57.9 & 23.8 & 73.3 & 36.4 & 14.2 & 33.0 & \textemdash & 41.2 \\
PIT \cite{LvLCL20} & 83.1 & 27.6 & 81.5 & 8.9 & 0.3 & 21.8 & 26.4 & \bfseries 33.8 & 76.4 & 78.8 & 64.2 & 27.6 & 79.6 & 31.2 & 31.0 & 31.3 & \textemdash & 44.0 \\
PyCDA$^\dagger$ \cite{LianDLG19} & 75.5 & 30.9 & 83.3 & 20.8 & 0.7 & 32.7 & 27.3 & 33.5 & 84.7 & 85.0 & 64.1 & 25.4 & 85.0 & 45.2 & 21.2 & 32.0 & \textemdash & 46.7 \\
CD-AM \cite{Yang_2021_WACV} & 82.5 & 42.2 & 81.3 & \textemdash & \textemdash & \textemdash & 18.3 & 15.9 & 80.6 & 83.5 & 61.4 & 33.2 & 72.9 & 39.3 & 26.6 & 43.9 & 52.4 & \textemdash \\
FDA \cite{0001S20} & 79.3 & 35.0 & 73.2 & \textemdash & \textemdash & \textemdash & 19.9 & 24.0 & 61.7 & 82.6 & 61.4 & 31.1 & 83.9 & 40.8 & \bfseries 38.4 & 51.1 &52.5 & \textemdash \\
LDR \cite{Yang_2020_ECCV} & 85.1 & 44.5 & 81.0 & \textemdash & \textemdash & \textemdash & 16.4 & 15.2 & 80.1 & 84.8 & 59.4 & \bfseries 31.9 & 73.2 & 41.0 & 32.6 & 44.7 & 53.1 & \textemdash \\
SA-I2I \cite{MustoZ20} & 87.7 & \bfseries 49.7 & 81.6 & \textemdash & \textemdash & \textemdash & 19.3 & 18.5 & 81.1 & 83.7 & 58.7 & 31.8 & 73.3 & \bfseries 47.9 & 37.1 & 45.7 & 55.1 & \textemdash \\
FADA \cite{Wang_2020_ECCV} & 84.5 & 40.1 & 83.1 & 4.8 & 0.0 & 34.3 & 20.1 & 27.2 & 84.8 & 84.0 & 53.5 & 22.6 & 85.4 & 43.7 & 26.8 & 27.8 & \textemdash & 45.2 \\
IAST \cite{Mei_2020_ECCV} & 81.9 & 41.5 & 83.3 & 17.7 & \bfseries 4.6 & 32.3 & 30.9 & 28.8 & 83.4 & 85.0 & \bfseries 65.5 & 30.8 & 86.5 & 38.2 & 33.1 & 52.7 & \textemdash & 49.8 \\
RPT$^\dagger$ \cite{ZhangQYNL020} & 88.9 & 46.5 & 84.5 & 15.1 & 0.5 & 38.5 & 39.5 & 30.1 & 85.9 & 85.8 & 59.8 & 26.1 & \bfseries 88.1 & 46.8 & 27.7 & \bfseries 56.1 & \textemdash & 51.2 \\
\midrule
Baseline (ours) & 63.9 & 25.9 & 71.0 & 11.0 & 0.2 & 36.9 & 7.6 & 20.0 & 72.9 & 75.5 & 46.7 & 16.7 & 74.5 & 15.8 & 20.8 & 21.7 & 41.0 & 36.3 \\
SAC (ours) & \bfseries 89.3 & 47.2 & \bfseries 85.5 & \bfseries 26.5 & 1.3 & \bfseries 43.0 & \bfseries 45.5 & 32.0 & \bfseries 87.1 & \bfseries 89.3 & 63.6 & 25.4 & 86.9 & 35.6 & 30.4 & 53.0 & \bfseries 59.3 & \bfseries 52.6 \\
\bottomrule
\multicolumn{19}{@{}l}{\scriptsize $(^\dagger)$ denotes the use of PSPNet \cite{ZhaoSQWJ17} instead of DeepLabv2 \cite{ChenPKMY18}. \ \ mIoU$^\text{13}$ is the average IoU over 13 classes (\ie~excluding ``wall'', ``fence'' and ``pole'').} \\
\end{tabularx}
\caption{\textbf{Per-class IoU (\%) comparison} on SYNTHIA $\rightarrow$ Cityscapes adaptation, evaluated on the Cityscapes validation set.}
\label{table:synthia_gta_to_city}
\vspace{-0.5em}
\end{table*}


\myparagraph{GTA5 $\rightarrow$ Cityscapes (\cref{table:result_gta_to_city}).}
Our method achieves a clear improvement over the best published results \cite{MustoZ20,ZhangQYNL020} of $+3.4\%$ and $+1.2\%$ using the VGG-16 and ResNet-101 backbones, respectively.
Note that RPT \cite{ZhangQYNL020} and SA-I2I \cite{MustoZ20} have a substantially higher model complexity.
RPT \cite{ZhangQYNL020} uses PSPNet \cite{ZhaoSQWJ17}, which has a higher upper bound than DeepLabv2 in a fully supervised setup (\eg, $+5.7 \%$ IoU on PASCAL VOC \cite{ZhaoSQWJ17}); it requires extracting superpixels and training an encoder-decoder LSTM, thus increasing the model capacity and the computational overhead.
SA-I2I \cite{MustoZ20} initialises from a stronger baseline, BDL \cite{LiYV19}, and relies on a style transfer network and adversarial training.
While both RPT \cite{ZhangQYNL020} and SA-I2I \cite{MustoZ20} require multiple rounds of training, 3 and 6 (from BDL \cite{LiYV19}), respectively, we train with the target loss in a single pass.
Notably, compared to the previous best approach for VGG with a ResNet evaluation, SA-I2I \cite{MustoZ20}, our improvement with ResNet-101 is substantial, $+3.4\%$, and is comparable to the respective margin on VGG-16.

\myparagraph{SYNTHIA $\rightarrow$ Cityscapes (\cref{table:synthia_gta_to_city}).}
Here, the result is consistent with the previous scenario.
Our approach attains state-of-the-art accuracy for both backbones, improving by $7.6\%$ and $1.4\%$ with VGG-16 and ResNet-101 backbones over the best results previously published \cite{MustoZ20,ZhangQYNL020}.
Again, our method with ResNet-101 outperforms the previous best method with full evaluation, PyCDA \cite{LianDLG19}, by $5.9\%$ IoU.

Remarkably, in both settings our approach is more accurate or competitive with many recent works \cite{subhani2020learning,Wang_2020_ECCV,Yang_2020_ECCV} even when using a weaker backbone, \ie~VGG-16 instead of ResNet-101.
This is significant, as these improvements are not due to increased training complexity or model capacity, in contrast to these previous works.
Additional results, including the evaluation on Cityscapes \textit{test}, are shown in \cref{sec:supp_class,sec:supp_eval}.

\begin{table}
\footnotesize
\setlength\fboxsep{0pt}
\begin{tabularx}{\linewidth}{@{}>{\raggedleft\arraybackslash}m{18mm}|S[table-format=2.1]|X@{}}
\toprule
\multicolumn{1}{>{\centering\arraybackslash}m{18mm}|}{$\Delta$} & mIoU & Configuration \\
\midrule
-8.0 \cbar{36}{5} & 41.9 & No augmentation consistency \\
-6.4 \cbar{28.8}{5} & 43.5 & No momentum net ($\gamma_\psi = 0$, $T=1$) \\
-3.9 \cbar{17.6}{5} & 46.0 & No photometric noise \\
-2.6 \cbar{11.7}{5} & 47.3 & No multi-scale fusion \\
-2.4 \cbar{10.8}{5} & 47.5 & No focal loss ($\lambda = 0$) \\
-1.9 \cbar{8.6}{5} & 48.0 & Min. entropy fusion (\vs averaging) \\
-1.7 \cbar{7.7}{5} & 48.2 & No class-based thresholding ($\beta \rightarrow 0$) \\
-1.6 \cbar{7.2}{5} & 48.3 & No confidence regularisation  \\
-1.5 \cbar{6.8}{5} & 48.4 & No importance sampling \\
-0.6 \cbar{2.7}{5} & 49.3 & No horizontal flipping  \\
\toprule
0.0 & 49.9 & Full framework (VGG-16) \\
\bottomrule
\end{tabularx}
\caption{\textbf{Ablation study.} We use the GTA5 $\rightarrow$ Cityscapes setting with the VGG-based model to study the effect of the components of our framework by individually removing each. We report the mean IoU for the Cityscapes validation split.}
\label{table:ablation}
\vspace{-0.5em}
\end{table}


\begin{figure*}[t]
\begin{subfigure}{.02\linewidth}
	\scriptsize
	\vspace{-1.2em}
	\rotatebox{90}{\textbf{Ground truth}}\vspace{3.2em}
	\rotatebox{90}{Adapted $\leftarrow$ \textbf{VGG-16} $\rightarrow$ Baseline}\vspace{2.5em}
	\rotatebox{90}{Adapted $\leftarrow$ \textbf{ResNet-101} $\rightarrow$ Baseline}
\end{subfigure}%
\hspace{0.1em}%
\begin{subfigure}{.49\linewidth}
  \centering
    \includegraphics[width=0.49\linewidth]{figures/qualitative/gt/frankfurt_000001_004736_gtFine_labelTrainIds}
    \includegraphics[width=0.49\linewidth]{figures/qualitative/gt/frankfurt_000000_016286_gtFine_labelTrainIds.png}\\
	\includegraphics[width=0.49\linewidth]{figures/qualitative/gta/vgg16/baseline/frankfurt_000001_004736_gtFine_labelTrainIds.png}
    \includegraphics[width=0.49\linewidth]{figures/qualitative/gta/vgg16/baseline/frankfurt_000000_016286_gtFine_labelTrainIds.png}\\
    \includegraphics[width=0.49\linewidth]{figures/qualitative/gta/vgg16/final/frankfurt_000001_004736_gtFine_labelTrainIds.png}
    \includegraphics[width=0.49\linewidth]{figures/qualitative/gta/vgg16/final/frankfurt_000000_016286_gtFine_labelTrainIds.png}\\
    \includegraphics[width=0.49\linewidth]{figures/qualitative/gta/resnet101/baseline/frankfurt_000001_004736_gtFine_labelTrainIds.png}
    \includegraphics[width=0.49\linewidth]{figures/qualitative/gta/resnet101/baseline/frankfurt_000000_016286_gtFine_labelTrainIds.png}\\
    \includegraphics[width=0.49\linewidth]{figures/qualitative/gta/resnet101/final/frankfurt_000001_004736_gtFine_labelTrainIds.png}
    \includegraphics[width=0.49\linewidth]{figures/qualitative/gta/resnet101/final/frankfurt_000000_016286_gtFine_labelTrainIds.png}
    \caption{\scriptsize GTA5 $\rightarrow$ Cityscapes}
\end{subfigure}%
\hspace{0.3em}%
\begin{subfigure}{.49\linewidth}
  \centering
    \includegraphics[width=0.49\linewidth]{figures/qualitative/gt/frankfurt_000001_032018_gtFine_labelTrainIds.png}
    \includegraphics[width=0.49\linewidth]{figures/qualitative/gt/frankfurt_000001_047178_gtFine_labelTrainIds.png}\\
	\includegraphics[width=0.49\linewidth]{figures/qualitative/synthia/vgg16/baseline/frankfurt_000001_032018_gtFine_labelTrainIds.png}
    \includegraphics[width=0.49\linewidth]{figures/qualitative/synthia/vgg16/baseline/frankfurt_000001_047178_gtFine_labelTrainIds.png}\\
    \includegraphics[width=0.49\linewidth]{figures/qualitative/synthia/vgg16/final/frankfurt_000001_032018_gtFine_labelTrainIds.png}
    \includegraphics[width=0.49\linewidth]{figures/qualitative/synthia/vgg16/final/frankfurt_000001_047178_gtFine_labelTrainIds.png}\\
    \includegraphics[width=0.49\linewidth]{figures/qualitative/synthia/resnet101/baseline/frankfurt_000001_032018_gtFine_labelTrainIds.png}
    \includegraphics[width=0.49\linewidth]{figures/qualitative/synthia/resnet101/baseline/frankfurt_000001_047178_gtFine_labelTrainIds.png}\\
    \includegraphics[width=0.49\linewidth]{figures/qualitative/synthia/resnet101/final/frankfurt_000001_032018_gtFine_labelTrainIds.png}
    \includegraphics[width=0.49\linewidth]{figures/qualitative/synthia/resnet101/final/frankfurt_000001_047178_gtFine_labelTrainIds.png}
  \caption{\scriptsize SYNTHIA $\rightarrow$ Cityscapes}
\end{subfigure}
\caption{\textbf{Qualitative examples.} Our approach rectifies an appreciable amount of erroneous predictions from the baseline.}
\label{fig:qualitative}
\vspace{-0.5em}
\end{figure*}

\subsection{Ablation study}
To understand what makes our framework effective, we conduct an ablation study using the GTA5 $\rightarrow$ Cityscapes setting with the VGG-16 backbone.
We independently switch off each component and report the results in \cref{table:ablation}.
We find that two components, augmentation consistency and the momentum network, play a crucial role.
Disabling the momentum network leads to a $6.4 \%$ IoU decrease, while abolishing augmentation consistency leads to a drop of $8.0 \%$ IoU.
Recall that augmentation consistency comprises three augmentation techniques: photometric noise, multi-scale fusion and random flipping.
We further assess their individual contributions.
Training without the photometric jitter deteriorates the IoU more severely, by $3.9 \%$, compared to disabling the multi-scale fusion ($-2.6 \%$) or flipping ($-0.6 \%$).
We hypothesise that encouraging model robustness to photometric noise additionally alleviates the inductive bias inherited from the source domain to rely on strong appearance cues (\eg, colour and texture), which can be substantially different from the target domain.

Following the intuition that high-confidence predictions should be preferred \cite{subhani2020learning}, we study an alternative implementation of the multi-scale fusion.
For overlapping pixels, instead of averaging the predictions, we pool the prediction with the minimum entropy.
The accuracy drop by $1.9\%$ is somewhat expected.
Averaging predictions via data augmentation has previously been shown to produce well-calibrated uncertainty estimates \cite{ayhan2018test}.
This is important for our method, since it relies on the confidence values to select the predictions for use in self-supervision.
Importance sampling contributes $1.5 \%$ IoU to the total accuracy.
This is surprisingly significant despite that our estimates $\chi_{c,l}$ are only approximate (\cf \cref{sec:training}), but the overall benefit is in line with previous work \cite{GuptaDG19}.
Recall from \cref{eq:sbrt} that our confidence thresholds are computed per class to encourage lower values for long-tail classes.
Disabling this scheme is equivalent to setting $\beta \rightarrow 0$ in \cref{eq:sbrt}, which reduces the mean IoU by $1.7 \%$.
This confirms our observation that the model tends to predict lower confidences for the classes occupying only few pixels.
Similarly, the loss in \cref{eq:loss} without the focal term ($\lambda = 0$) and confidence regularisation ($m_{c^\ast\!,n} = 1$) are $2.4 \%$ and $1.6 \%$ IoU inferior.
This is a surprisingly significant contribution at a negligible computational cost.

\subsection{Qualitative assessment}
\cref{fig:qualitative} presents a few qualitative examples, comparing our approach to the naive baseline (\ie~source-only loss with ABN).
Particularly prominent are the refinements of the classes ``road'', ``sidewalk'' and ``sky'', but even small-scale elements improve substantially (\eg, ``person'', ``fence'' in the leftmost column).
This is perhaps not surprising, owing to our multi-scale training and the thresholding technique, which initially ignores incorrectly predicted pixels in self-supervision (as they initially tend to have low confidence).
Remarkably, the segment boundaries tend to align well with the object boundaries in the image, although our framework \emph{has no explicit encoding of spatial priors}, which was previously deemed necessary \cite{Chen0G18,TsaiSSC19,ZhangDG17,ZhangQYNL020}.
We believe that enforcing semantic consistency with data augmentation makes our method less prone to the contextual bias \cite{ShettySF19}, often blamed for coarse boundaries.
