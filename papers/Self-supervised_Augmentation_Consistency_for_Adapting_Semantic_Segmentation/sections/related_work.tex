% !TEX root = ../arxiv.tex

Most of the work on scene adaptation for semantic segmentation has been influenced by a parallel stream of work on domain adaptation (DA) and semi-supervised learning for image classification \cite{FrenchMF18,ganin2016domain,GrandvaletB04,LiWSHL18,LongC0J18}.
The main idea behind these methods is to formulate an upper bound on the target risk using the so-called $\mathcal{H} \Delta \mathcal{H}$-divergence \cite{Ben-DavidBCKPV10}.
In a nutshell, it defines the discrepancy between the marginals of the source and target data by means of a binary classifier.
In the following, we briefly review implementation variants of this idea in the context of semantic segmentation.

\myparagraph{Learning domain-invariant representations.}

Adversarial feature alignment follows the GAN framework \cite{ganin2016domain,NIPS2014_5423} and minimises the gap between the source and target feature representations in terms of some distance (\eg, Wasserstein in \cite{LeeBBU19}).
The discriminator can be employed at multiple scales \cite{Chen0G18,TsaiHSS0C18,Yang_2020_ECCV} and use local spatial priors \cite{ZhangQYNL020};
it can be conditional \cite{HongWYY18} and class-specific \cite{DuTYFXZYZ19,Luo0GYY19},
or align the features of `hard' and `easy' target samples \cite{PanSRLK20}.
Often, self-supervised losses, such as entropy minimisation \cite{VuJBCP19}, or a `conservative loss' \cite{zhu2018penalizing} assist in this alignment.

The alternative to adversarial feature alignment are more interpretable constraints, such as feature priors \cite{luo2019significance}, bijective source-target association \cite{KangW0ZH20} or aligning the domains directly in the image space with style transfer \cite{CycleGAN2017} used either alone \cite{WuHLUGLD18} or, most commonly, jointly with adversarial feature alignment \cite{ChangWPC19,ChenL0H19,GongLCG19,MustoZ20,Yang_2021_WACV,YangLSS20,ZhangQY0M18}.
One issue with style translation is to ensure semantic consistency despite the changes in appearance.
To address this, Hoffman \etal~\cite{HoffmanTPZISED18} use semantic and cycle-consistency losses, while Yang \etal~\cite{Yang_2020_ECCV} reconstruct the original image from its label-space representation.

These methods tend to be computationally costly and challenging to train, since they require concurrent training of one or more independent networks, \eg~discriminators or style transfer networks.
Although Yang and Soatto \cite{0001S20} obviate the need for style networks by incorporating the phase of a Fourier-transformed target image into a source sample, multiple networks have to be trained, each with its own pre-defined phase band.

\myparagraph{Self-training on pseudo labels.}
As a more computationally lightweight approach, self-training seeks high-quality pseudo supervision coming in the form of class predictions with high confidence.
Our work belongs to this category.
Most of such previous methods pre-compute the labels `offline', used subsequently to update the model, and repeat this process for several rounds \cite{Li_2020_ECCV,subhani2020learning,ZouYKW18,ZouYLKW19}.
More recent frameworks following this strategy have a composite nature: they rely on adversarial (pre-)training \cite{ChenCCTWS17,DongCSLX20,ZhengY20}, style translation \cite{ChoiKK19,0001S20} or both \cite{Mei_2020_ECCV,LiYV19,KimB20a,Wang_2020_ECCV,WangYWFXHHS20}.

\begin{table}
\footnotesize
\begin{tabularx}{\linewidth}{@{}X|ccccc|c@{}}
\toprule
Features & \shortstack{PIT \\ \cite{LvLCL20}} & \shortstack{LDR \\ \cite{Yang_2020_ECCV}} & \shortstack{SA-I2I \\ \cite{MustoZ20}} & \shortstack{IAST \\ \cite{Mei_2020_ECCV}} & \shortstack{RPT \\ \cite{ZhangQYNL020}} & Ours \\
\midrule
Adversarial training & & \cmark & \cmark & \cmark & \cmark & \\
\midrule
1-round training & \cmark & \cmark & (6) & (3) & (3) & \cmark \\
SOTA-VGG & & \cmark & \cmark & & & \cmark \\
SOTA-ResNet & & & & \cmark & \cmark  & \cmark \\
\bottomrule
\end{tabularx}
\caption{\textbf{Relation to state of the art.} Previous work reaches the state of the art in terms of IoU either with VGG-16 (SOTA-VGG) or ResNet-101 (SOTA-ResNet). Our framework uses neither adversarial training nor multiple training rounds (given in parentheses), yet outperforms the state of the art consistently in both cases.}
\label{table:related_work}
\end{table}

Training on co-evolving pseudo labels can be computationally unstable, hence requires additional regularisation.
Chen \etal~\cite{0001XC19} minimise the entropy with improved behaviour of the gradient near the saturation points.
Using fixed representations, be it from a `frozen' network \cite{Chen0G18,ZhengY20}, a fixed set of global \cite{LvLCL20} or self-generated local labels \cite{LianDLG19,TsaiSSC19,ZhangDG17}, further improves training robustness.

Overconfident predictions \cite{GuoPSW17} have direct consequences for the quality of pseudo labels.
Zou \etal~\cite{ZouYLKW19} attain some degree of confidence calibration via regularising the loss with prediction smoothing akin to temperature scaling \cite{GuoPSW17}.
Averaging the predictions of two classifiers \cite{Zheng_2020_IJCV}, or using Dropout-based sampling \cite{Cai_2020_CVPR,zhou2020uncertainty}, achieves the same goal.

\begin{figure*}[t]%
\subcaptionbox{\scriptsize Framework overview\label{fig:model_overview}}{%
    \def\svgwidth{0.44\linewidth}
    \input{figures/model/model.pdf_tex}
}\hfill
\subcaptionbox{\scriptsize Multi-scale crops and flips\label{fig:batch}}{%
    \def\svgwidth{0.26\linewidth}
    \input{figures/batch/batch.pdf_tex}
}\hfill
\subcaptionbox{\scriptsize Multi-scale fusion\label{fig:fusion}}{%
    \def\svgwidth{0.26\linewidth}
    \input{figures/batch/fusion.pdf_tex}
}
\caption{\textbf{Overview.} The segmentation network in our framework \emph{\subref{fig:model_overview}} maintains a slow copy of itself, the momentum network, which provides stable targets for self-supervision. In addition to encouraging semantic invariance \wrt the photometric noise, we facilitate consistent predictions across multiple scales and flips by first \emph{\subref{fig:batch}} feeding random multi-scale crops and flips to the momentum network and then \emph{\subref{fig:fusion}} fusing the predictions by simple averaging to produce the pseudo-supervision targets.} %
\label{fig:batch_fusion}%
\vspace{-0.5em}
\end{figure*}

\myparagraph{Spatial priors.}
Different from DA for classification, the characteristic feature of adaptation methods for segmentation is the use of spatial priors.
Local priors have been enforced patch-wise \cite{Chen0G18,LianDLG19,TsaiSSC19} and in the form of pre-computed super-pixels \cite{ZhangDG17,ZhangQYNL020}.
Although global spatial priors have also been used \cite{ZouYKW18}, their success hinges on the similarity of the semantic layout in the current benchmarks.

\myparagraph{Relation to our approach.}
As shown in Table~\ref{table:related_work}, our work streamlines the training process.
First, we do not use adversarial training, as feature invariance alone does not guarantee \textit{label} invariance \cite{JohanssonSR19,0002CZG19}.
Second, we train our model with co-evolving pseudo labels in one round.
Our framework bears resemblance to the noisy mean teacher \cite{XieLHL20} and combines consistency regularisation \cite{BachmanAP14,SajjadiJT16,SohnBCZZRCKL20,XieDHL020} with self-ensembling \cite{LaineA17,tarvainen2017mean}.
Similar approaches have been explored in medical imaging \cite{LiY0FH18,PeroneBBC19} and concurrent UDA work \cite{Wang_Yang_Betke_2021}, albeit limited in the scope of admissible augmentations.
We leverage photometric invariance, scale and flip equivariance \cite{WangZKSC20} to extract high-fidelity pseudo supervision instead of more computationally expensive sampling techniques \cite{KendallG17}.
Contrary to \cite{subhani2020learning}, we find that scale alone is not predictive of the label quality, hence we average the predictions produced at multiple scales and flips.
This parallels uncertainty estimation using test-time augmentation \cite{ayhan2018test}, but at training time \cite{BerthelotCGPOR19}.
