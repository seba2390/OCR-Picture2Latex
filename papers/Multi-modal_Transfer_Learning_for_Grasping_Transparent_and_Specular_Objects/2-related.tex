\section{Related Work}
\label{sec:related}

\subsection{Sensing Transparent and Specular Objects}

Sensing transparent and specular objects is a well-studied challenge in the computer vision community. 
Ihrke \etal\cite{ihrke2010transparent} provide a survey of recent approaches to transparent and specular object reconstruction.
Curless \etal\cite{curless1995better} perform space-time analysis on structured light sensing to achieve better triangulation on transparent objects.
Structured light sensing can also be paired with additional equipment like polarization lenses, light fields, or immersion in fluorescent or refractive liquids to detect transparent objects.
While structured light sensing is the closest to commercial sensing, the survey also presents methods that improve on multi-view stereo matching to detect transparent and specular objects.

Light field photography for depth reconstruction is another direction for detecting specular and transparent objects~\cite{6619203,wetzstein2011hand}. 
Light field photography has been used in robotics by 
Oberlin \etal\cite{oberlintime} applied light field photography to robot manipulation tasks like grasping non-Lambertian objects under running water.
However, this method requires capturing a dense set of images in a 3D volume over the scene of interest at both training and test time to construct suitable synthetic images for grasping.
In comparison, our proposed method requires a single, static RGB-D sensor, resulting in faster and simpler training and deployment.

Commercial RGB-D sensors (\eg~Intel RealSense, Microsoft Kinect, PrimeSense) use structured-light or time-of-flight techniques to estimate depth.
These techniques fail on transparent and specular surfaces, either allowing light emitted by the sensor to pass through or scattering it by reflection.
IR stereo and cross-modal stereo techniques have been used to improve depth reconstruction, but the reconstruction quality is still not comparable to that of Lambertian, or diffusely reflective, objects~\cite{alhwarin2014ir, chiu2011improving, mahler2018guest}. 
Lysenkov \etal\cite{lysenkov2013recognition, lysenkov2013pose} painted over transparent objects to create a dataset of paired transparent and opaque objects, but this approach scales poorly for objects with arbitrary geometries and material properties.
Our proposed method is able to use conventional RGB-D sensors without hardware and environmental modifications by combining depth and color information. 

\subsection{Grasp Synthesis}

Grasp synthesis refers to the problem of finding a stable robotic grasp for a given object and is a longstanding research problem in robotics. 
Approaches to grasp synthesis can be classified into analytic and empirical methods; see Bohg \etal\cite{bohg2013data} for a survey.
Analytic approaches use physics-based contact models to compute force closure on an object, using the shape and estimated pose of the target object~\cite{miller2003automatic, ten2017grasp, watkins2018multi}, but work poorly in the real world due to noisy sensing, simplified assumptions of contact physics, and difficulty in placing contact points accurately.

Empirical approaches, on the other hand, learn to predict the quality of grasp candidates from data on a diverse set of objects, images, and grasp attempts collected through human labeling~\cite{saxena2008robotic, jiang2011efficient, doi:10.1177/0278364914549607, 7139361}, self-supervision~\cite{pinto2016supersizing, levine2018learning}, or simulated data~\cite{depierre2018jacquard, doi:10.1177/0278364919859066, gualtieri2016high, mahler2017dex, satish2019policy}. 
Saxena \etal\cite{saxena2008robotic} trained a classifier on human-labeled RGB images to predict grasp points, triangulated the points on stereo RGB images, and demonstrated successful grasps on a limited set of household objects, including some transparent and specular objects.
However, the predicted grasp points for transparent and specular objects were limited to grasps on points where stereo triangulation was successful. 
The Cornell Grasping Dataset~\cite{jiang2011efficient}, consisting of 1k RGB-D images of objects and human-labeled grasps parameterized as an oriented bounding box, has been used to train many deep learning-based grasping methods~\cite{doi:10.1177/0278364914549607, doi:10.1177/0278364919859066, 7139361}. 
Self-supervised methods such as those by Pinto and Gupta~\cite{pinto2016supersizing} or Levine \etal\cite{levine2018learning} forego the need for human labels by training a robot to grasp directly from real grasp attempts, but these methods require tens of thousands of attempts to converge.

Recently, approaches trained on data gathered in simulation have demonstrated state-of-the-art performance.
The Jacquard dataset by Amaury \etal\cite{depierre2018jacquard} uses a grasp specification similar to the Cornell Grasping Dataset, contains simulated objects and grasp attempts, and has been successfully used for training by Morrison \etal's GG-CNN~\cite{doi:10.1177/0278364919859066}. 
Mahler \etal\cite{mahler2017dex} developed GQCNN, which was trained on a dataset of simulated grasps generated using analytic model, representing a hybrid empirical and analytic approach.  

As we will show, these depth-only grasping approaches fail on transparent and reflective objects. 
Note that GG-CNN could be modified to incorporate RGB images, which could potentially be used to grasp transparent and specular objects after training on simulated images (such as those in the Jacquard dataset~\cite{depierre2018jacquard}); however, such performance has not been demonstrated; this method has only been demonstrated for depth-based grasping of opaque objects.
In this work, we build upon the fully convolutional version of GQCNN (FC-GQCNN) proposed by Satish \etal\cite{satish2019policy}, but our method is agnostic to the specific network architecture used.
Our method does not require any real-world grasps or labeled data but instead relies on supervision transfer from \hl{a pre-trained depth network to obtain a multi-modal grasping method. The pre-trained depth network also may not require real-world grasps or human labels; for example, FC-GQCNN is trained entirely on simulated grasps.}

\subsection{Cross-modal Transfer Learning}
Supervision transfer has been explored in the past for tasks such as image classification and object detection~\cite{gupta2016cross, hoffman2016cross, li2018cross}.  These approaches are typically used to transfer image-based networks trained on ImageNet~\cite{deng2009imagenet} to depth-based or RGB-D based classification or detection networks.  To our knowledge, such approaches have not been used previously in the context of multi-modal grasping.  We show that such an approach can lead to greatly improved performance for grasping transparent and reflective objects, and can even improve performance on some opaque objects.
