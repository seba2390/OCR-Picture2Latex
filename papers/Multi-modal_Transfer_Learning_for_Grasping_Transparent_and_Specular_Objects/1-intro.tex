
\section{Introduction}
\label{sec:intro}

% \IEEEPARstart{R}{obotic} 
Robotic grasping is a key prerequisite for a variety of tasks involving robot manipulation.
Robust object grasping would enable a wide range of applications in both industrial and natural human environments. 
The challenge with grasping is that many factors influence the effectiveness of a grasp, such as gripper and object geometries, object mass distribution and friction, and environmental conditions like illumination.

Most state-of-the-art grasping methods rely on depth input from structured light or time-of-flight sensors to determine the best grasp for an object~\cite{satish2019policy, morrison2018closing, gualtieri2016high}.
Under normal operation, such devices emit light patterns onto a scene and use a receiver to construct depth based on changes in the returned pattern. 
However, such depth sensors fail to detect objects that are transparent, specular, refractive, or have low surface albedo~\cite{ihrke2010transparent}, causing depth-based grasp prediction methods to fail.
These failures can take the form of both missing depth readings, as is the case with specular objects that deflect structured light patterns, and incorrect depth values, which occur when the emitted light passes through transparent objects (see \fig{fig:intro}). 

Transparent and specular objects are common in a range of environments, such as in manufacturing facilities, retail spaces, and homes. 
Under certain lighting conditions and object properties, even seemingly opaque objects can exhibit sensor noise similar to transparency and specularity.
The ubiquity of objects with these challenging properties requires us to design methods capable of bridging the sensory gap so that robots can robustly grasp a diverse set of objects. 

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.455\columnwidth}
        \centering
        \includegraphics[width=0.99\columnwidth, trim=110 0 110 0, clip]{figures/intro/sawyer1.JPG}
    \end{subfigure}%
    \begin{subfigure}[b]{0.54\columnwidth}
        \centering
        \includegraphics[width=0.99\columnwidth, trim=-4 -8 0 0 0, clip]{figures/intro/color_im_1.png}
        \includegraphics[width=0.99\columnwidth, trim=-5 0 0 0]{figures/intro/depth_im.png}
    \end{subfigure}
    \caption{Transparent and specular objects provide poor depth readings with conventional depth sensors, posing a challenge for depth-based grasping techniques. (left) Robot workspace with fixed overhead sensor for grasping. (top right) Color image of scene from overhead sensor. (bottom right) Depth image of scene showing that most values in depth image are close to the table.}
    \label{fig:intro}
    \vspace{-1em}
\end{figure}

Our contribution in this work is a method for learning to grasp transparent and specular objects that leverages existing depth-based models.
Transparent and specular objects are more identifiable in RGB space, where transparencies and specularities produce changes in coloration, rather than the inaccurate or missing values that occur in depth space.
Therefore, we make use of both color and depth modalities in our approach.
We first train a color-based grasp prediction model from a depth-based one using \textit{supervision transfer}~\cite{hoffman2016cross}, a technique for transferring a learned representation from one modality to another. 
\hl{This transfer technique only requires paired RGB-D images and an existing depth-based grasping method from which to transfer; our method does not require robot grasp attempts nor human annotations}. 

We conduct real robot grasping experiments on both isolated objects and clutter to show that (1) the RGB-only network produces better grasp candidates for transparent and specular objects, compared to the depth-only network that it was trained from, and (2) the RGB-only network is complementary to the original depth model, such that combining the outputs of both models results in the best overall grasping performance on all three object types. 
We conduct additional experiments to demonstrate the robustness of our method against slight variations in illuminance, and we discuss failure cases as part of our analysis.
