\section{Conclusion}
\label{sec:conclusion}

We present an approach for improving grasping on transparent and specular objects, for which existing depth-based grasping methods perform poorly. 
Our method transfers information learned by a depth-based grasping network to RGB or RGB-D networks, enabling multi-modal perception. 
Our method for supervision transfer requires only real-world paired depth and RGB images, and does not require any human labeling nor real-world grasp attempts. 
We explore two avenues to multi-modal perception and demonstrate that making use of the RGB modality outperforms depth-only grasping in isolated object grasping as well as grasping in clutter.
The method is extensible to other robots, environments, and end effectors. 
One potential direction for future work may be to adaptively weight predictions from different modalities instead of averaging them.
Another is applying transfer learning techniques to other, less similar modalities like haptics and tactile feedback. \hl{Combining different sensor modalities might also be useful in determining the appropriate grasp height for each object.}

While we are able to get improved performance without using any real grasping data, we believe that real grasps can be used to further improve the performance of the network. We are also interested in extending this work to other types of grasping, such as 6-DOF, multi-fingered, or suction grasping.