\section{Approach}
\label{sec:method}
Here we describe our approach for supervision transfer, which enables us to transfer a grasping method trained in one modality $\mathcal{M}_d$ to also incorporate an additional modality $\mathcal{M}_s$ without needing \hl{any additional real grasp attempts, simulation, nor human-labeled data (other than the data used to train the initial uni-modal grasping method, which in our case is only simulated rendered depth data}~\cite{satish2019policy}).

\subsection{Problem Statement}
We assume that we initially have a grasping method that takes input from a given modality $\mathcal{M}_d$, such as depth.  
Specifically, we assume that we have a grasping method that, given a candidate grasp $q$ and an image $I_d$ of modality $\mathcal{M}_d$ (\eg a depth image), outputs a grasp score $G(q, I_d)$. \hl{We wish to transfer this scoring method to a new input modality $\mathcal{M}_s$ (\eg RGB). Ideally, this new modality $\mathcal{M}_s$ will allow our grasping method to succeed in grasping certain types of objects (e.g. transparent and specular) where the previous modality, $\mathcal{M}_d$, failed. In later sections, we will discuss combining these modalities to create more robust grasping methods}.

\hl{We assume access to} a dataset of image pairs $(I_d, I_s)$, where each pair consists of one image from each modality.  We assume that \hl{each pair of images was} taken at approximately the same time and thus represent images of the same scene under the two modalities $\mathcal{M}_d$ and $\mathcal{M}_s$. \hl{Paired images for RGB and depth modalities can be captured using commercially available} RGB-D sensors (\eg Intel RealSense, Microsoft Kinect, PrimeSense).

Note that these paired images can be collected without needing to perform any grasp attempts or human labeling, making the collection of this dataset very efficient.  Furthermore, because these paired images are collected in the real world, they contain all of the real-world noise and artifacts that one would encounter in a realistic setting, avoiding the need to create such artifacts in simulation. 

\subsection{Supervision Transfer for Multi-modal Perception}
\label{sec:suptransfer}
\hl{In attempting the modality transfer described above, we observe the following}: different input modalities (\eg depth vs RGB) have complementary advantages. 
In other words, data that is difficult for computing successful grasps in one modality might not be as difficult for another modality, and vice versa.  For example, transparent and reflective objects are extremely difficult for depth-based grasping methods, due to the resulting noise or missing data in the depth image.  However, our experiments show that \hl{RGB-based grasping methods have a much higher success rate for these objects}.  On the other hand, highly textured objects may present difficulties for RGB grasping methods, \hl{but these textures do not manifest in depth-based methods.}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/intro/diagram.png}
    \caption{We train a grasp quality CNN that takes RGB input by supervising the loss of the network on the output of a trained depth model for paired, unlabeled RGB-D image data.}
    \label{fig:suptransfer}
\end{figure}

Based on this observation, we first filter our dataset $D$ into a new dataset $D'$ for which we expect the grasping method of modality $\mathcal{M}_d$ to perform well. 
In other words, for images $I_d \in D'$, the grasp score $G(q, I_d)$ should have a high correlation with the success of an executed grasp. In our case, because $I_d$ is a depth image, our filtered dataset $D'$ contains only images of opaque objects, for which depth-based grasping methods typically perform well.

We then train a grasping method for modality $\mathcal{M}_s$ (\eg RGB) using supervision transfer~\cite{gupta2016cross, hoffman2016cross, li2018cross} over dataset $D'$.
For each paired image $(I_d, I_s)$ in dataset $D'$, we compute the grasping score $G(q, I_d)$ for the modality $\mathcal{M}_d$\hl{. Because of our filtering, this grasp score is likely to be accurate}. We then train a method for computing the grasping score \hl{$G_\phi(q, I_s)$} of the second modality $\mathcal{M}_s$ \hl{using the grasp score from modality $\mathcal{M}_d$ as the grasp label; thus we define the loss to be}
\begin{align}
    \mathcal{L}(\phi) = ||G(q, I_d) - G_\phi(q, I_s)||^2
\end{align}
For paired images of dataset $D'$, we train the grasping method on the new modality $\mathcal{M}_s$ (\eg RGB) to output the same grasping score as the score output of the previous grasping method on the original modality $\mathcal{M}_d$ (\eg depth). This procedure is shown in Figure~\ref{fig:suptransfer}.

\hl{Because of the complementary nature of the two sensors, this grasping score function will often perform well on data that was originally filtered out of $D$ and not included in $D'$, even though $G_\phi(q, I_s$) was only trained on data from $D'$.
Specifically, we filter out transparent and reflective objects from $D'$ because depth-based grasping methods perform poorly on these objects.}
Nonetheless, the image-based grasping method $G_\phi(q, I_s)$ still performs well on  images of transparent and reflective objects, because the difference in appearance for these objects in the RGB modality is much smaller than the difference in appearance for these objects in the depth modality.  Our experiments confirm this to be the case.

Further, because the modalities are complementary, we show that we can get the best performance by combining the grasping scores from the two modalities. 
Although there are many potential ways to do this, we evaluate two possibilities.  The \hl{``early fusion'' approach for combining modalities is to transfer from a depth-based grasping network to a RGB-D grasping network (``RGBD-ST", see }\fig{fig:method-rgbdst}\hl{). RGBD-ST takes as input both depth and RGB modalities concatenated together.
For our second, ``late-fusion'' approach, we fuse the scores of each modality, averaging the outputs of the depth-based grasping network with a RGB-based grasping network trained using supervision transfer. We} define the multi-modal grasping score as
\begin{align}
    G_\phi(q, I_d, I_s) = \frac{1}{2} \cdot (G(q, I_d) + G_\phi(q, I_s))
\end{align}
This method is referred to below as ``RGBD-M" (\hl{see} \fig{fig:method-rgbdm}). Both of these approaches share the benefits that they represent multi-modal grasping methods that were trained from a depth-based grasping method only using paired RGB and depth images, without requiring real grasp attempts or human labels.

\subsection{Implementation of Supervision Transfer}
\label{sec:suptransfer_impl}

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[scale=0.75]{figures/arch/fcgqcnn.pdf}
        \caption{FC-GQCNN \cite{satish2019policy}}
        \label{fig:method-fcgqcnn}
    \end{subfigure}%
    ~
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[scale=0.75]{figures/arch/rgb.pdf}
        \caption{RGB-ST}
        \label{fig:method-rgbst}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.18\textwidth}
        \centering
        \includegraphics[scale=0.75]{figures/arch/rgbd.pdf}
        \caption{RGBD-ST}
        \label{fig:method-rgbdst}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.34\textwidth}
        \centering
        \includegraphics[scale=0.75, trim={0 0 0 6pt}, clip]{figures/arch/rgbdm.pdf}
        \caption{RGBD-M}
        \label{fig:method-rgbdm}
    \end{subfigure}
    \caption{\hl{Diagrams of the four methods evaluated in this work. We compare against FC-GQCNN }\cite{satish2019policy}\hl{, which takes a depth image as input and outputs dense grasp scores over image coordinates $x,y$ and rotation $\theta$ about the depth axis. RGB-ST and RGBD-ST are both trained using supervision transfer, but differ in the input they accept (3-channel RGB or 4-channel RGB-D input). RGBD-M takes the outputs of the RGB and Depth networks and averages them to produce the final output.}}
    \label{fig:methods}
\end{figure*}

Our supervision transfer formulation is agnostic to the specific grasping method or representation we use for grasping in modality $\mathcal{M}_d$.
For this work, we use the Fully Convolutional Grasp Quality CNNs (FC-GQCNN) representation as the pre-trained depth model from Satish \etal\cite{satish2019policy}, although other depth-based grasping methods could equivalently be used.

FC-GQCNN learns a function $G(q_d, I_d)$ which predicts a grasp success rate for each grasp $q_d$ based on a depth image $I_d$.  In FC-GQCNN, grasps $q_d$ are parameterized as $q_d = (x, y, \theta, z)$, where $x$ and $y$ are horizontal planar coordinates designating the desired grasp point of the gripper, $z$ is the grasp depth relative to the camera, and $\theta$ is the clockwise rotation angle of the gripper about the vertical $z$ axis.  \hl{FC-GQCNN takes as input just a single depth image $I_d$ and outputs a 4-dimensional tensor of grasping scores, producing one score per binned ($x$,$y$,$z$) position as well as binned orientation coordinates $\theta$}. FC-GQCNN is designed to be fully convolutional in order to output dense predictions $G(q_d, I_d)$ across the entire depth image. \hl{Our methods, shown in Figure}~\ref{fig:methods}, \hl{use a similarly dense ($x$, $y$) output and the same output angular encoding $\theta$.}

We wish to use the output of FC-GQCNN to train an image-based grasping method  $G(q, I_s)$.  Because the image modality does not have access to depth information, for image-based grasping we change the grasping parameterization to just $q = (x, y, \theta)$, without including a parameter for the grasp depth $z$.  With this specification, each grasp starts at an approach height and moves down until it makes contact with either the table or an object before closing the gripper.  Due to the difference in grasp representations, we modify our loss slightly, to be:
\begin{align}
    \mathcal{L}(\phi; q, I_d, I_s) = ||\max_z G((q, z), I_d) - G_\phi(q, I_s)||^2
\end{align}
where $(q, z)$ is the concatenation of $z$ to a grasp $q = (x, y, \theta)$ to form the new grasp representation $(x, y, \theta, z)$. In other words, to compute the target grasp score for some grasp $q = (x, y, \theta)$, we append various depths $z$ to form a depth-based grasp parameterization $(x, y, z, \theta)$; for each of these grasp parameterizations we can compute the depth-based grasping score $G((q, z), I_d)$ using our depth-based grasping method (e.g. FC-GQCNN).  We then compute the maximum grasp score over the values of $z$ to obtain $\max_z G((q, z), I_d)$.

The network architecture that we use for image-based grasping is very similar to the architecture used in FC-GQCNN for depth-based grasping (\hl{see Appendix A}).  The only modification that we make is that we modify the first layer to accept a 3-channel RGB input rather than a 1-channel depth input. This is accomplished by adding an extra dimension to the first layer convolutional filters. In some of the experiments, we will alternatively use an RGB-D grasping network (``RGBD-ST"), in which case we modify the first layer to accept a 4-channel input, in a similar manner.

