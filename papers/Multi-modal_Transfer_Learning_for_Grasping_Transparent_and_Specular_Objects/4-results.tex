\section{Experimental Setup}
\label{sec:exp_setup}

Following the reproducibility guidelines for grasping research as presented in~\cite{mahler2018guest}, we describe our experimental setup and protocols below.

\subsection{Physical Components}

We use an ASUS Xtion Pro Live RGB-D sensor, fixed 0.7 m above and pointing down towards the workspace (see \fig{fig:dataset}).
Robot experiments were performed on a 7 DOF Rethink Robotics Sawyer robot equipped with an electric parallel jaw gripper, though our method can be applied to other robots and end-effectors.
The robot's workspace is an approximately 0.65 m $\times$ 0.38 m area that is reachable by the robot with a vertical grasp. 
Aluminum extrusions enclose the workspace to prevent objects from rolling or sliding out of the space.  

All experiments and network training were performed on an Ubuntu 16.04 machine with an NVIDIA GTX 1080 Ti GPU, a 2.1 GHz Intel Xeon CPU, and 32 GB RAM allocated per job. Grasp planning was implemented using off-the-shelf MoveIt! software.

\subsection{Training the Network}

We first collected a set of 100 opaque objects from home and office retail stores.
Using the ASUS Xtion Pro Live RGB-D sensor fixed above the workspace, we captured 200 paired RGB-D training images and 50 paired validation images of the objects in varying amounts of clutter and with lighting conditions ranging from standard office illuminance (approx. 500 lux) to dimmed illuminance (approx. 175 lux).
We resized the images to account for differences between our sensor's intrinsic parameters and those of the pretrained FC-GQCNN model.
To increase the amount of training data and improve domain robustness, we applied spatial augmentations (\eg random rotations and flips) and color-based augmentations (\eg hue, brightness, and contrast), generating approximately 20k paired training images.
This image dataset is available at the URL in the abstract.

The network architecture was implemented in Python using Tensorflow and Keras. 
The RGB or RGB-D network's weights were randomly initialized, and the model was trained to convergence using an Adam optimizer with cross-entropy loss~\cite{kahn2018self, kalashnikov2018qt}. 
We experimented with mean squared error loss, but it performed worse in initial experiments. 
The loss was supervised from the output of FC-GQCNN, taking the maximum over all values of $z$ as discussed in \sect{sec:suptransfer_impl}. Hyperparameters are provided in Appendix C.

\subsection{Test Objects}

We collected objects distinct from the training objects to form three sets of 15 test objects each, one set per category (see \fig{fig:dataset}).
For the opaque object set, we primarily use YCB~\cite{calli2015benchmarking} objects that fit within the 5 cm stroke width of our gripper. 
We collected our own transparent and specular object sets due to the lack of existing benchmark sets for these categories. 

Following typical procedures for grasping evaluations~\cite{mahler2017dex, viereck2017learning}, we remove bias related to object pose through the following procedure: objects are shaken in a box and then emptied onto the robot's workspace for each grasp attempt. This procedure is used for both isolated object grasping as well as for grasping in clutter.

\section{Experimental Results}
\label{sec:results}

We design experiments to answer the following questions:
\begin{itemize}
    \item To what extent can supervision transfer be used to grasp objects from new modalities (e.g. depth to RGB)?
    \item To what extent can supervision transfer from depth to RGB be used to learn to grasp transparent and reflective objects?
    \item Do the depth and image modalities complement each other? That is, will combining both modalities outperform either modality alone?
\end{itemize}

Note that grasping performance is not directly comparable with previous work like FC-GQCNN~\cite{satish2019policy} as we use a different robot, gripper, and depth sensor.

\begin{figure*}[t]
    \vspace{2pt}
    \centering
    \begin{subfigure}[b]{0.1875\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/setup/eval_scene_cropped.JPG}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=0.99\textwidth]{figures/setup/opaque_objects.JPG}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=0.99\textwidth]{figures/setup/translucent_objects.JPG}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=0.99\textwidth]{figures/setup/specular_objects.JPG}
    \end{subfigure}
\caption{Data collection setup and example images from the dataset of all three object types.}
\label{fig:dataset}
\end{figure*}

\begin{figure*}[h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=3cm, keepaspectratio]{figures/heatmap/color_im.png}
        \caption*{RGB image}
    \end{subfigure}
    \vspace*{3px}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[height=3cm]{figures/heatmap/depth_im.png}
        \caption*{Depth image}
    \end{subfigure}
    \vspace*{1px}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/heatmap/fcgqcnn_pred.png}
        \caption*{FC-GQCNN}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/heatmap/rgb_pred.png}
        \caption*{RGB-ST}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/heatmap/rgbd_pred.png}
        \caption*{RGBD-ST}
    \end{subfigure}
    \begin{subfigure}[b]{0.245\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{figures/heatmap/rgbdm_pred.png}
        \caption*{RGBD-M}
    \end{subfigure}
    \begin{subfigure}[b]{0.99\textwidth}
        \centering
        \includegraphics[width=0.60\textwidth]{figures/heatmap/colorbar.png}
    \end{subfigure}
    \caption{Probability heatmaps of grasping across methods for the max grasp score of a grasp with fingertips horizontal to the image, centered at the each pixel. Objects from each set are arrayed horizontally such that the top row is opaque objects, the next transparent, and the final one specular. }
    \label{fig:heatmap}
    \vspace{-2pt}
\end{figure*}

\subsection{Multi-modal Perception}
\label{sec:multimodal_perception}
We evaluate whether multi-modal perception that combines depth and RGB data is better than uni-modal perception using either depth or RGB data alone.  We refer to our method for Depth-to-RGB supervision transfer, described in Sections~\ref{sec:suptransfer} and~\ref{sec:suptransfer_impl}, as ``RGB-ST" (see \fig{fig:method-rgbst}).  

We evaluate two approaches to multi-modal perception, \hl{both of which are described in} Sections~\ref{sec:suptransfer} and~\ref{sec:suptransfer_impl}. 
The first \hl{``early-fusion"} approach uses supervision transfer to directly train an RGB-D grasp prediction network from a depth-based network, called ``RGBD-ST" (\hl{see }\fig{fig:method-rgbdst}).
The second \hl{``late-fusion"} approach involves taking the mean of the outputs of an RGB-only network and a depth-based network. 
Specifically, we take the mean of the RGB-ST and FC-GQCNN grasping networks; we call this multi-modal method ``RGBD-M" (\hl{see }\fig{fig:method-rgbdm}). 

The results are shown in Table~\ref{table:indiv_eval}.  
RGBD-ST and RGBD-M both significantly outperform depth-only grasping (FC-GQCNN) on transparent and specular objects, while maintaining comparable performance on opaque objects. 

We also see that the multi-modal methods perform similarly to the RGB-based grasping method (RGB-ST) on opaque and transparent objects, but outperform this method on specular objects. These results support the notion that combining both RGB and depth modalities gives better grasping performance than using either modality alone.

\begin{table}[h]
    \centering
    \caption{Isolated object grasping, averaged over five trials}
    \label{table:indiv_eval}
    \begin{tabular}{l ccc ccc}
      \toprule
        Method & Opaque & Transparent & Specular \\
        \midrule
        % \vspace{0.6em}
        FC-GQCNN$^*$ & 
            $\mathbf{0.92\pm0.06}$ & $0.40\pm0.08$ & $0.48\pm0.17$\\
        % RGB-G$^*$ &
        %     &  &  \\ 
        % RGB-C$^*$ & 
        %      &  & \\
            % \vspace{0.6em}
        RGB-ST$^\dagger$ & 
            $0.89\pm0.04$ & $0.79\pm0.09$ & $0.71\pm0.04$\\
        RGBD-ST$^\dagger$ & 
            $0.91\pm0.06$ & $0.77\pm0.08$ & $\mathbf{0.83\pm0.04}$ \\
        RGBD-M$^\dagger$ & 
            $0.91\pm0.14$ & $\mathbf{0.85\pm0.06}$ & $0.81\pm0.07$\\
      \bottomrule
    \end{tabular}
    % \vspace{\baselineskip}
    \vspace{7pt}
    \caption*{\footnotesize $^*$Trained on simulated grasps \\ $^\dagger$Trained on simulated grasps and opaque object images \\}
    \vspace{-3em}
\end{table}

\subsection{Grasping in Clutter}

We also evaluated our methods for grasping in clutter, as this is important for robots in various cluttered environments like homes and warehouses.   
The same test objects used in isolated object grasping were used for clutter experiments. 
Five trials of grasping in clutter were conducted for each object category. 
Following the procedure from Viereck \etal\cite{viereck2017learning}, a trial concluded after all objects were successfully grasped, 3 consecutive failed grasp attempts occurred, or all objects were outside the workspace.

To prevent a network from getting repeatedly stuck on attempting a bad but highly rated grasp, we randomly sample a 0.2m square crop of the input image and select the grasp location within that region with the maximum predicted success probability.
All methods including baselines performed similarly or worse without this sampling (see Appendix B).
Crops whose grasp probabilities all fall below below a threshold are discarded and resampled to avoid attempting grasps based on noisy sensor readings. 

\begin{table}[h]
    \centering
    \caption{Grasping in clutter, averaged over five trials}
    \label{table:clutter_eval}
    \begin{tabular}{l cc cc cc}
      \toprule
        Method 
            & Opaque
            & Transparent
            & Specular \\
        \midrule
        % \vspace{0.6em}
        FC-GQCNN$^*$
            & $0.84\pm0.06$
            & $0.23\pm0.21$
            & $0.35\pm0.16$ \\
        RGB-ST$^\dagger$
            & $0.77\pm0.11$
            & $\mathbf{0.67\pm0.10}$
            & $\mathbf{0.68\pm0.12}$ \\
        % \vspace{0.6em}
        RGBD-ST$^\dagger$
            & $0.86\pm0.09$
            & $0.67\pm0.27$
            & $0.35\pm0.10$ \\
        RGBD-M$^\dagger$ 
            & $\mathbf{0.97\pm0.15}$
            & $0.51\pm0.32$
            & $0.63\pm0.12$ \\
      \bottomrule
    \end{tabular}
    % \vspace{\baselineskip}
    \vspace{3pt}
    \caption*{\footnotesize $^*$Trained on simulated\ grasps \\ $^\dagger$Trained on simulated\ grasps and opaque object images \\}
    \vspace{-2em}
\end{table}

The results are shown in Table~\ref{table:clutter_eval}.
The results from grasping in clutter corroborate the result of isolated grasping. 
All methods perform well on opaque objects, although RGBD-M (averaging the output of depth-only grasping and RGB-only grasping networks) performs slightly better than the others.  On non-opaque objects (e.g. transparent and specular), FC-GQCNN (e.g. depth-only grasping) performs poorly.

Table~\ref{table:clutter_eval} shows that RGB-ST (RGB-only grasping) and RGBD-M (averaging the output of depth-only grasping and RGB-only grasping networks) perform well across all three object categories.  We note that, despite averaging across five trials, the results of grasping in clutter have relatively high variance and should be considered accordingly.  Overall, our main conclusions are similar to that of isolated object grasping from Section~\ref{sec:multimodal_perception}: depth-only grasping performs poorly on transparent and specular objects; with supervision transfer, we can obtain a method that performs much better on grasping transparent and specular objects while maintaining similar performance on opaque objects.
This method requires only paired RGB and depth images for training and does not require any real grasp attempts or human annotations, \hl{other than the simulated depth rendering data that was used to train the original FC-GQCNN}~\cite{satish2019policy} \hl{depth-based grasping method}.

\subsection{Lighting Variation Experiments}

We note that domain shifts like lighting can be a problem for RGB methods, as mentioned in previous work~\cite{mahler2018guest}. To enable our method to be robust to lighting variations, our training images were collected with slight lighting variations, and we applied color-based augmentations like brightness and contrast.

We conducted experiments to evaluate the robustness of the trained networks to lighting variations. We varied the lighting by moving a floor lamp around the robot workspace as shown in \fig{fig:lighting1} and performed the isolated object grasping experiments for RGBD-M. The additional lighting increased illumination to between 750 and 950 lux. With this variation in lighting, the RGBD-M network performed comparably, achieving grasp success rates of $0.81\pm0.12$ \hl{for transparent objects and $0.79\pm0.09$ for specular ones} (compare with \tbl{table:indiv_eval}).

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.49\columnwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/setup/light.JPG}
        \caption{Lighting setup.}
        \label{fig:lighting1}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\columnwidth}
        \centering
        \includegraphics[width=0.99\linewidth]{figures/fail/shadows1.JPG}
        \caption{Extreme lighting.}
        \label{fig:lighting2}
    \end{subfigure}
    \caption{\hl{(a) Setup for lighting variation experiments. Lighting is controlled using the overhead lights and floor lamp. (b) Failure case in the extreme lighting condition. The method predicts the best grasp to be on the object's shadow.
    }}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \caption{\hl{Failure case in the extreme lighting condition. The method predicts the best grasp to be on the object's shadow.}}
%     \label{fig:lighting2}
% \end{figure}

However, we found that the network performed poorly under more drastic lighting changes, in which \hl{we turned off the overhead lights and reduced the height of  the floor light}, dropping illumination to approx.\ 175 lux and causing long object shadows to appear.  
\hl{In this case, grasp performance dropped to $0.52\pm0.18$ on transparent objects and $0.60\pm0.12$ for specular ones}. % new sentence
In such extreme lighting conditions, we observed the method predicting grasps on shadows for transparent objects (\hl{see }Fig.~\ref{fig:lighting2}). Such drastic lighting would not normally occur in structured applications like bin-picking.

\subsection{Failure Cases}

In this section we discuss the most frequent and notable failure cases from our experiments. 
This section covers failures due to our approach, as well as external factors.
Some examples of failure cases discussed in this section can be seen in Fig.~\ref{fig:fail} and the supplementary video. 

Methods that used the depth modality like RGBD-ST and RGBD-M at times selected grasps that were highly rated by the depth network, but did not sufficiently account for transparencies or specularities (Fig.~\ref{fig:fail}, top left).
Both the color-based and depth-based networks at times failed to distinguish very transparent objects from the workspace surface, though this was rare and occurred far less frequently than with FC-GQCNN (Fig.~\ref{fig:fail}, top right).  
Object mass distribution and deformability were not accounted for by our methods (Fig.~\ref{fig:fail}, bottom row).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.35\columnwidth]{figures/fail/sharpener.png}
    \vspace{0.3em}
    \includegraphics[width=0.35\columnwidth]{figures/fail/cube.png}
    \includegraphics[width=0.35\columnwidth]{figures/fail/squeegee.png}
    \includegraphics[width=0.35\columnwidth]{figures/fail/weight.png}
    \caption{Examples of failure cases. (top left) Grasp does not account for transparent part of sharpener. (top right) Gripper fails to detect transparent plastic cube and grasps at table. (bottom left) Mass distribution of squeegee causes grasp to fail. (bottom right) Foil on top of balloon weight appears graspable but the gripper passes through.}
    \label{fig:fail}
\end{figure}

A failure case external to the methods evaluated involved our gripper hardware.
Our parallel electric gripper has a relatively small stroke width, and is unable to execute pinch grasps with a 5cm opening width.
This limitation causes grasps on thin parts of objects to fail, because the fingertips do not completely come together.
While it is possible to adjust the fingertips to be closer together to enable pinch grasps, the opening width of the gripper would be reduced,  which would prevent the gripper from being able to grasp large objects.
This issue reduced performance across all methods and would likely be mitigated by other grippers.

Since our paper focused on static grasping, our method fails to grasp objects that start rolling due to perturbation in clutter. 
Others have investigated ways to address this issue using closed-loop control techniques like visual servoing~\cite{morrison2018closing}.

