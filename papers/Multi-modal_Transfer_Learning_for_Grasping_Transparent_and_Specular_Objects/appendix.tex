% \documentclass{article}
% \usepackage[margin=1in]{geometry}
% \usepackage{array}
% \usepackage{graphicx}
% \usepackage{subcaption}
% \usepackage[utf8]{inputenc}
% \usepackage{amsmath, amsthm, amssymb}
% \usepackage{graphicx}
% \usepackage{booktabs}
% \usepackage{dsfont}
% \usepackage{xcolor}
% \usepackage{hyperref}
% \usepackage{color, soul}
% \usepackage[page]{appendix}

% \renewcommand\hl[1]{#1} % Turn off highlighting

% \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
% \newtheorem{prop}{Proposition}
% \renewcommand{\figurename}{Supplementary Figure}
% \newcommand{\todo}[1]{\textcolor{blue}{[\textbf{TODO:} #1]}}
% \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\begin{figure*}[t]
    % \renewcommand{\thefigure}{Supplementary Fig.}
    % \renewcommand\fnum@figure{Supplementary Fig.~\thefigure}
    % \renewcommand{\fnum@figure}{Supplementary Fig.~\thefigure}
    \centering
    \includegraphics[width=0.99\textwidth]{figures/arch/corl_complex.pdf}
    \caption{Architecture diagram for supervision transfer networks, adapted from the FC-GQCNN \cite{satish2019policy} architecture. The input can be either 3-channel RGB input or 4-channel RGB-D input. The output is a 3D array of grasp quality scores over image coordinates $x, y$ and rotation $\theta$ about the depth axis, discretized into 16 bins. The orange color accents correspond to ReLU activations and purple corresponds to sigmoid activation. The red layers are max pooling layers.}
    \label{fig:architecture}
\end{figure*}

\newpage
% \appendices

% \appendix
% \title{Appendix}
%\author{dheld}
%\date{January 2018}

% \renewcommand{\appendixpagename}{\centering Appendix}

% \begin{document}
\begin{appendices}

% \captionsetup[figure]{name={Supplementary Fig.}}


% \appendixpagename

\section{Network Architecture}
\label{appendix:architecture}

Fig.~\ref{fig:architecture} illustrates the architecture of the networks trained with supervision transfer.

\section{Evaluations without random cropping}
\label{appendix:nocrop}

Table \ref{table:clutter_eval_nocrop} provides results for grasping in clutter without random cropping.  

\begin{table}[h]
    \centering
    \caption{Performance on grasping in clutter by method without random cropping, averaged over five trials}
    \label{table:clutter_eval_nocrop}
    \begin{tabular}{l cc cc cc}
      \toprule
        Method 
            & Opaque
            & Transparent
            & Specular \\
        \midrule
        \vspace{0.6em}
        FC-GQCNN$^*$
            & $0.95\pm0.05$
            & $0.26\pm0.25$
            & $0.35\pm0.23$ \\
        RGB-ST$^\dagger$
            & $0.77\pm0.10$
            & $0.77\pm0.15$
            & $0.68\pm0.15$ \\
        % \vspace{0.6em}
        RGBD-ST$^\dagger$
            & $0.62\pm0.26$
            & $0.67\pm0.19$
            & $0.75\pm0.08$ \\
        RGBD-M$^\dagger$ 
            & $0.75\pm0.13$
            & $0.60\pm0.18$
            & $0.47\pm0.28$ \\
      \bottomrule
    \end{tabular}
    \vspace{\baselineskip}
    \caption*{\footnotesize $^*$Trained on simulated\ grasps \\ $^\dagger$Trained on simulated\ grasps and opaque object images \\}
    \vspace{-1em}
\end{table}

% \noindent Random cropping refers to sampling a 0.2m square crop from the input image and choosing the grasp with the highest probability from within the crop. This procedure helps prevent networks from repeatedly choosing highly rated false positive grasps. Crops which have do not have any objects in them, as determined by whether the max grasp probability within the crop falls below a hand-defined threshold, are discarded and a new crop is sampled. 

\noindent Random cropping refers to sampling a 0.2m square crop from the input image and choosing the grasp with the highest probability from within the crop. Crops which have do not have any objects in them, as determined by whether the max grasp probability within the crop falls below a hand-defined threshold, are discarded and a new crop is sampled. This procedure helps prevent networks from repeatedly choosing highly rated false positive grasps. However, the cropping threshold must be tuned based on the performance of the grasping network.  For our experiments, we used a threshold of 0.4.

\pagebreak
\section{Hyperparameters}
\label{appendix:hyperparams}

\noindent Hyperparameters for networks trained with supervision transfer are:

\begin{itemize}
    \item Learning rate: 1e-05
    \item Batch size: 64
    \item Number of rotation augmentations per image: 32
    \item Loss: Binary cross-entropy
\end{itemize}

\noindent The FC-GQCNN model we evaluated against was a pre-trained model from \url{https://berkeleyautomation.github.io/gqcnn/}.

% \small
% \bibliographystyle{unsrt}
% \bibliography{appendix}
\end{appendices}

% \end{document}