\section{Discussion}
Ensuring fairness in machine learning entails addressing the philosophical question of, given a particular setting, how fairness should formally be defined. Given a formal notion of fairness, the next question is how it can be achieved, and at what cost to accuracy. Over the past few years, FPR- and FNR-rate matching have emerged as compelling fairness notions deserving of attention. As we see from our experiments, fairness-unaware learning algorithms are sometimes extremely unfair according to these metrics. It is important, then, to ask what can be done to address this, and how accuracy will be impacted.

As learning optimal classifiers to match FPR and FNR across populations may be computationally intractable~\cite{woodworth}, it is natural that multiple approaches to this problem might emerge, each with its own pros and cons. Prior to our work, two groundbreaking papers had proposed approaches to ensuring FPR- and FNR-matching. Hardt et al.-style post-processing~\cite{hardt} is easy to implement and can be layered atop an already (unfairly) trained classifier. However, in some applications, because it does not integrate fairness in the learning process, it may be inherently sub-optimal. We illustrate this drawback in Section~\ref{sec:incorporating}. Some might also find post-processing distasteful, as it intentionally reduces accuracy on some individuals, in order to compensate for poor accuracy on others. The proxy-based approach of ~\cite{disparatemistreatment} makes nice use of the concept of disciplined convex-concave programming, however it has not been shown capable of lowering the unfairness below a certain (non-negligible) threshold.
%on the real-life data considered here, as is evident in the results discussed in Section~\ref{sec:experiments}. 
%As we have seen, our penalization approach empirically succeeds in eliminating almost all of the unfairness in each of the datasets we examined, while retaining high accuracy when compared to the equivalent model absent fairness considerations.

As we show, fairness can successfully be achieved in many real-world settings via the addition of a judiciously chosen penalty term in the learning objective. We hope that this penalization approach, and the proxy we introduce for imposing FPR- and FNR-matching, will expand and enrich the toolkit for state-of-the art fair learning, and will help bring the goal of fair learning within reach.


%The quest for achieving fairness in machine learning is divided into two equally important questions: $(1)$ What notion of fairness do we wish to adopt, and $(2)$ How, given this notion, we can produce the best possible results. We leave the first question to society, which might set different criteria for what's fair under different settings, and try to tackle the second question under the specific yet important notion of equalized odds. As learning such classifier in the general setting was proven to be hard~\cite{woodworth}, we consider ways to empirically learn such classifiers, as this is a setting in which decisions has to be made, and better understanding the underlying trade-off between accuracy and fairness in such real-life scenarios is a matter of great importance.

%Each of the approaches to fair learning that we compare in this work carries its pros and cons. Hardt et al.-style post-processing~\cite{hardt} is easy to implement and could be layered  atop an already (unfairly) trained classifier. However, in some applications, because it does not integrate fairness in the learning process, it may be inherently sub-optimal. Some might also find post-processing distasteful, as it intentionally reduces accuracy on some individuals, in order to compensate for poor accuracy on others. The proxy-based approach of ~\cite{disparatemistreatment} makes nice use of the concept of disciplined convex-concave programming, however it has not been shown capable of lowering the unfairness below a certain (non-negligible) threshold on the real-life data considered here. As we have seen in the last section, our method succeeds in eliminating almost all of the unfairness in all of the different real-life datasets that were examined, while retaining most of the accuracy when compared to the equivalent model absent fairness considerations.

%As the approach we presented in this work is proxy-based, it holds no formal guarantees. Instead, the general idea was to present and test the assumption that for the equalized odds notion of fairness, and on real-life data, the distance from the decision boundary of boundary-based classifiers might serve as a reliable, easy-to-use proxy for learning an equalized odds classifier. By doing so, we hope to shed more light on the ability to learn such fair classifiers, and the price that is incorporated in doing so. As the task of learning a fair classifier is hard in the general case, it is reasonable to expect that state-of-the-art in practical fair learning will best be served by a diverse toolkit of approaches. We hope that the present work will enrich that toolkit.


 


%Given any particular dataset and classification task, there is generally some inherent tension between accuracy and fairness. As the task of learning a fair classifier is in many cases computationally hard~\cite{woodworth}, it is reasonable to expect that state-of-the-art in practical fair learning will best be served by a diverse toolkit of approaches. We hope that the present work will enrich that toolkit. 