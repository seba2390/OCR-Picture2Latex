\section{Case Study: Fair Classification Using Logistic Regression} \label{casestudy}
In this section, we instantiate our approach for achieving fairness, in the context of logistic regression. We denote the trained parameters of our logistic regressor by $\theta \in \mathbb{R}^d$, and the log-likelihood of $\theta$ given training set $S$ by $ll(\theta;S)$. 

%In logistic regression, we fit the parameters $\theta\in\mathbb{R}^d$ of a model $h_{\theta}:\mathbb{R}^d\rightarrow[0,1]$, s.t. $h_{\theta}(x)=\dfrac{1}{1+e^{-\theta^{T}x}}$. Binary prediction is done using a cut-off parameter. Given a set of probabilistic assumptions, we can fit the parameters of logistic regression by maximizing the log-likelihood function of $\theta$. In what follows, we use 
%\begin{equation*}
%\Pr[y=1~|~x;\theta]=h_{\theta}(x),~ \Pr[y=0~|~x;\theta]=1-h_{\theta}(x)
%\end{equation*}

%The log-likelihood of $\theta$ in this case is
%\begin{equation*}
%ll(\theta;S)=\sum\limits_{i=1}^{n} y_{i}\log(h_{\theta}(x_i))+(1-y_{i})\log(1-h_{\theta}(x_i))
%\end{equation*}

We wish to solve the following optimization problem:


\begin{align}\label{eq:1}
\begin{split}
\underset{\theta}{\text{minimize}}
 ~~&L_{S}^{0\text{-}1}(\theta)\\
&+d_1|FPR_{A=0}(\theta;S)-FPR_{A=1}(\theta;S)|\\
&+d_2|FNR_{A=0}(\theta;S)-FNR_{A=1}(\theta;S)|
\end{split}
\end{align}
where $d_1, d_2 \geq 0$ are to be set up front, according to the desired trade-off between accuracy, FPR matching, and FNR matching. Applying our suggested relaxation ($R_{FP}$, $R_{FN}$ are to be set as either Absolute Value Difference or Squared Difference penalizers), and adding a standard $\ell_2$ regularization term, we get the following convex optimization problem:
\begin{align} \label{eq:2}
\begin{split}
\underset{\theta}{\text{minimize}}
~~&-ll(\theta;S) \\
&+c_1 R_{FP}(\theta;S) \\
&+c_2 R_{FN}(\theta;S)\\
&+q\left|\left|\theta\right|\right|_2^2 \\
\end{split}
\end{align}

For convenience, we will denote the objective in (\ref{eq:1}) by $\text{Objective}(\theta;S,d_1,d_2)$, and the objective in the proxy problem (\ref{eq:2}) by $\text{Proxy}(\theta;S,c_1,c_2,q)$. As the proxy is easy to solve using standard methods, we use it when optimizing, and then shift back to the original problem for estimating the quality of our results.

%Effectively, our (relaxed) penalizers are set under the assumption that the distance between the prediction and the true label serves as a reliable proxy for the 0-1 loss, i.e., when predictions are done with high confidence, close to 0 or 1, far from the decision boundary. The terms are minimized when the average distance is the same for both groups in $S^{pos}$ and for both groups in $S^{neg}$. 

%In order to maximize $ll(\theta;S)$, and at the same time seek fair solutions, we use the following gradient update rule:
%\begin{equation*}
%   \theta^{i+1} = \theta^{i} - \eta_t(\nabla_{\theta}(-ll)+C_1\nabla_{\theta}(R_{FPR})+C_2\nabla_{\theta}(R_{FNR})+C_3 \theta)
%\end{equation*}
%Where $\eta_t$  is the learning rate (gradient step size).