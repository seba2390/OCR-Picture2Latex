\section{Penalizing Unfairness} \label{regularization}

%\subsection{A Fairness-Inducing Penalization Scheme}
Our approach to learning a fair classifier integrates fairness considerations into the learning process by penalizing unfairness, and is inspired by the concept of {\em regularization}. Typically in regularization, the added penalty term is a function only of the learned hypothesis, penalizing for complexity in the model, aiming to prevent overfitting. Here, we introduce a new type of penalty, which is not only hypothesis-dependent, but is also {\em data-dependent}, and which is set at a group level rather at an individual level. As our goal is to learn a classifier that matches FPR and FNR rates across populations, we define two types of penalizers that aim at minimizing the differences between the FPR and FNR (respectively) across sub-groups in the population, for the trained classifier.\footnote{Our penalization scheme minimizes the differences between the empirical FPR and FNR as evaluated on the relevant sub-groups in the training set $S$, and relies on statistical guarantees proven in Woodworth et al.~(\citeyear{woodworth}) to yield fairness on the true underlying distribution $\mathcal{D}$, for a sufficiently large dataset drawn i.i.d. from $\mathcal{D}$.}  

%Following the notions of fairness from the previous section, we will consider the difference in FPR and the difference in FNR within the groups $A$ and $B$:

%\begin{align*}
%\begin{split}
%   \left|FNR(A)-FNR(B)\right| 
%   &= \left|-\dfrac{\sum\limits_{i\in N_A^{pos}}^{} \hat{y}_i}{\left|N_A^{pos}\right|} + \dfrac{\sum\limits_{i\in N_B^{pos}}^{} \hat{y}_i}{\left|N_B^{pos}\right|}\right| \\
%\end{split}
%\end{align*}

%The difference in the FPR of the two groups can be expressed in a similar fashion.

%\subsection{A Family of Fairness-Inducing Penalizers}
We focus our attention on boundary-based classifiers, which are trained in the form of a decision boundary in the feature space. In what follows we will assume the decision boundary is a hyperplane, and classification is therefore done in the following manner: Given a sample $x \in \mathbb{R}^d$, and a classifier $\hat{Y}$ specified by $\theta \in \mathbb{R}^d$, we predict $\hat{Y}(x) = sign(\theta^T x)$. We note that our approach also extends to the case of non-linear SVMs, by shifting to the space mapped into by the kernel function, and considering the problem of learning a decision hyperplane in that space. %\yahav{We have to maybe rephrase this.}

%In our context, the 0-1 loss is $\mathcal{L}_{0-1}(\hat{y},y) = \mathbbm{1}_{\hat{y}\neq y} = \mathbbm{1}_{f(x)\neq y}$.
%Since the 0-1 loss is non-differentiable, we use the margin from the decision boundary instead as a proxy for achieving equalized odds (with respect to the original 0-1 loss).
The first penalizer we propose is based on relaxing the 0-1 loss, to instead consider the margin from the decision boundary. We will penalize the difference in the average distance from the decision boundary across different values of  the protected attribute A.


%\textbf{Type I Penalizer: Distance from Decision Boundary, Absolute Value of Difference}

We define the \textbf{Absolute Value Difference (AVD)} FPR penalty term to be
\begin{align*}
R_{FP}^{AVD}(\theta;S) &= \left|\dfrac{\sum\limits_{x\in S_{00}}^{} \theta^T x}{\left|S_{00}\right|} - \dfrac{\sum\limits_{x\in S_{10}}^{} \theta^T x}{\left|S_{10}\right|}\right| \\
&= \left|\theta^T\underbrace{\left(\dfrac{\sum\limits_{x\in S_{00}}^{} x}{\left|S_{00}\right|} - \dfrac{\sum\limits_{x\in S_{10}}^{} x}{\left|S_{10}\right|}\right)}_{\overline{x}}\right| \\
&= \left|\theta^T \overline{x} \right|
\end{align*}
The FNR penalty term is defined analogously. We note that this penalizer is convex in $\theta$. In order for the penalizer to also be differentiable at 0, we define a second variant (using the same notation for $\overline{x}$), which we term the \textbf{Squared Difference (SD)} penalizer:
%\textbf{Type II Penalizer: Distance from Decision Boundary, Squared Difference}
\begin{equation*}
R_{FP}^{SD}(\theta;S) = \left(\theta^T \overline{x} \right)^2.
\end{equation*}
Again, we define the FNR penalizer analogously.