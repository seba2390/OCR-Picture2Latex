\section{Fair Learning}
\subsection{Classical Approach}
In classical machine learning theory, when considering a classification task, the objective is typically to minimize a loss function that reflects the errors the chosen classifier makes on a fresh sample of data. One might naturally adjust the loss function to penalize differently for different sorts of errors (false positive or false negative, in the binary case), however, a priori, the classical approach does not do anything to control the distribution of errors across different sub-populations.

\subsection{Preliminaries}
We now introduce notation we will use to formalize our fairness objectives. We will represent each data point (person) as a pair $(x, y) \in \mathbb{R}^d \times \{0,1\}$ with the following interpretation: $x \in \mathbb{R}^d$ represents the features of an individual; the first feature $x_1$ 
%\yahav{By using this notation, we need to shift the sample indices from subscript to superscript.} 
(which we assume to be binary) represents a protected attribute (e.g., subgroup membership, black vs.~white) and we will also write it as $A \in \{0,1\}$; and $y \in \{0, 1\}$ represents the true label (e.g., ``re-offended'' or ``did not re-offend''). A labeled data set $S=(x^i,y^i)_{i=1}^n$ is a collection of such data points. We will partition $S$ into groups according to each individual's protected attribute and true label:
\begin{equation*}
S_{ay} = \{x^i \in S : x^i_1=a, y^i=y\}, ~a,y \in \{0,1\}
\end{equation*}
We write $\hat{Y}: \mathbb{R}^d\rightarrow \{0,1\}$ for a classifier that, given an individual's features, including her protected attribute, predicts her label.

Then, given a data set $S$ and a classifier $\hat{Y}$, writing $\hat{y}^i = \hat{Y}(x^i)$, we can formally define the false positive rate (FPR) and false negative rate (FNR) of $\hat{Y}$ on $S$ as follows:
\begin{align*}
FPR(\hat{Y}) &= \frac{\left|\{i:\hat{y}^i=1,y^i=0\}\right|}{\left|\{i: y^i=0\}\right|}\\
FNR(\hat{Y}) &= \frac{\left|\{i:\hat{y}^i=0,y^i=1\}\right|}{\left|\{i:y^i=1\}\right|}
\end{align*}

Given a value $a \in \{0,1\}$ of the protected attribute $A$, we denote by $FPR_{A = a}(\hat{Y})$, $FNR_{A = a}(\hat{Y})$ the false positive and false negative rates of $\hat{Y}$ on $\{(x,y) \in S : x_1 = a\}$.
%\katrina{Where is the $FPR_{A = 0}$ notation defined? Please introduce it here.}


%We assume that our data points $(x,y)$ are drawn i.i.d. from a joint distribution $\mathcal{D} = (X,Y)$ over 
%When $S$ is sampled from a distribution $\mathcal{D}$ over data points, we will denote by $\hat{FPR}(\hat{Y}), \hat{FNR}(\hat{Y})$ the false positive rate and false negative rate, respectively, of $\hat{Y}$ on $\mathcal{D}$. \katrina{But that's ugly notation. And maybe unnecessary. Do we really use it? Worst case, make it $\widehat{FNR}$?}


%The next definitions are adopted from Hardt et al.~(\citeyear{hardt}) and Woodworth et al.~(\citeyear{woodworth}); here we abuse notation and, when clear, we write $\hat{Y}$ not just for the classifier, but for the random variable over outputs it induces.

%\begin{definition} (Equalized odds). We say a classifier $\hat{Y}$ satisfies equalized odds with respect to A and Y, if $\hat{Y}$ and A are independent conditioned on $Y$.
%\end{definition}
%In the binary case, equalized odds can be written as
%\begin{align*}
%\begin{split}
%   \mathbb{P} \left[ \hat{Y}=1~|~A=0,Y=y \right]= \mathbb{P} \left[ \hat{Y}=1~|~A=1,Y=y \right], \\
%    y\in\{0,1\}
%\end{split}
%\end{align*}

%One can relax the equalized odds notion to focus only on positive outcomes:
%\begin{definition} (Equal opportunity). We say a classifier $\hat{Y}$ satisfies equal opportunity with respect to A and Y if
%\begin{equation*}
%\mathbb{P} \left[ \hat{Y}=1~|~A=0,Y=1 \right] = \mathbb{P} \left[ \hat{Y}=1~|~A=1,Y=1 \right]
%\end{equation*}
%\end{definition}

%\yahav{If we keep this, note that we have to define $\hat{FPR}, \hat{FNR}.$}
%\begin{definition} ($\alpha$-discrimination)
%A binary classifier $\hat{Y}$ is $\alpha$-discriminatory with respect to protected attribute $A$ on the underlying distribution $\mathcal{D}$ if
%\begin{align*}
%\max\bigg\{&|\hat{FPR}_{A=0}(\hat{Y})-\hat{FPR}_{A=1}(\hat{Y})|, \\ &|\hat{FNR}_{A=0}(\hat{Y})-\hat{FNR}_{A=1}(\hat{Y})|\bigg\} = \alpha
%\end{align*} 
%\end{definition}