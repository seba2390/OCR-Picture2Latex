\begin{figure*}[ht]
  \includegraphics[width=1\textwidth]{incorporating-0-300-600-squared.png}
  \caption{  Toy example with two attributes, $A$ (protected) and $X_2$ (non-protected). Classifying according to $A$ is perfectly unfair and has loss $\epsilon$. Classifying according to $X_2$ is perfectly fair and has loss $2 \epsilon$. Any post-processing of the optimal classifier (which classifies according to $A$ and loses the information contained in $X_2$) to ensure fairness will have the worst possible loss, $0.5$.  Here, we depict the halfspace learned via our penalization approach, increasing the weight assigned to the fairness penalizers (left-to-right). We see that placing sufficient weight on the penalizer results in prediction entirely according to $X_2$.
  This illustration was implemented based on random sampling 5,000 points from the distribution $\mathcal{D}_{\epsilon}$ described in Section~\ref{sec:incorporating}, with $\epsilon$ = 0.1, and learning using the scheme described in Section~\ref{sec:experiments}, using the $R_{FP}^{SD}$ and $R_{FN}^{SD}$ penalizers described in Section~\ref{regularization}, placing weights of $c_1=c_2=c$ for $c\in\{0,300,600\}$. }
  \label{fig:incorporating}
\end{figure*}


\section{The Importance of Incorporating Fairness in the Learning Phase} \label{sec:incorporating}
We briefly illustrate a simple example (based on one in Woodworth et al.~(\citeyear{woodworth})) which demonstrates the potential impact of incorporating fairness considerations into the learning process, rather than post-processing a learned classifier for fairness. 


In the example, each data point 
%$x=\begin{pmatrix} x_1 \\ x_2\end{pmatrix}\in 
lies in $X = (X_1, X_2) = \{0,1\}^2$ and has two features---$X_1=A$ is the protected attribute, and $X_2$ is a non-protected attribute---and a label in $Y = \{0,1\}$. Given $\epsilon \in (0,\frac{1}{4})$, we define a distribution $\mathcal{D}_{\epsilon}$ over labelled examples as follows:
\begin{align*}
\Prob[Y=1] &= 0.5 \\ 
\Prob[A=y|Y=y] &= 1-\epsilon \\
\Prob[X_2=y|Y=y] &= 1-2\epsilon
\end{align*}

Note that $\mathcal{D}_{\epsilon}$ is defined s.t. $A \perp X_2 | Y$.

%In what follows, we will use the next definition:
%\begin{definition} ($\alpha$-discrimination)
%A binary classifier $\hat{Y}$ is $\alpha$-discriminatory with respect to protected attribute $A$ on the underlying distribution $\mathcal{D}$ if
%\begin{align*}
%\max\bigg\{&|\hat{FPR}_{A=0}(\hat{Y})-\hat{FPR}_{A=1}(\hat{Y})|, \\ &|\hat{FNR}_{A=0}(\hat{Y})-\hat{FNR}_{A=1}(\hat{Y})|\bigg\} = \alpha
%\end{align*} 
%\end{definition}
%\yahav{If we keep this, note that we have to define $\hat{FPR}, \hat{FNR}.$}

\paragraph{Post-Processing}
Assume the hypothesis class $\mathcal{H}$ is unconstrained, and contains all of the (possibly randomized) functions $h:X \rightarrow \{0,1\}$. Note that classifying according to $X_2$ alone is a completely fair classifier (the FPR and FNR are both equal across $A = 0$ and $A=1$) that achieves 0-1 loss $2\epsilon$, and thus provides an upper bound on
\begin{equation*}
\min\limits_{h \in \mathcal{H}} \{L_{\mathcal{D}_{\epsilon}}^{0\textnormal{-}1}(h):h~\text{is perfectly fair}\}
\end{equation*}

The Bayes optimal predictor with respect to the 0-1 loss is
\begin{equation*}
\hat{h}(X) = \argmax\limits_{y \in \{0,1\}} \Prob[Y=1|X=x]
\end{equation*}
which, in our case, gives $\hat{h}(X) = A$. This classifier has 0-1 loss of only $\epsilon$. However, in terms of fairness, it performs as badly as possible, as it induces the maximal possible differences in both the FPR and FNR rates across the two sub-populations in the distribution.  %\katrina{Just explain in words} $FPR_{A=0}(\hat{h})=0$, $FPR_{A=1}(\hat{h})=1$,  $FNR_{A=0}(\hat{h})=1$, and $FNR_{A=1}(\hat{h})=0$. %it is 1-discriminatory.

Any approach to post-processing this classifier for fairness (including, for example, the technique proposed by Hardt et al.~(\citeyear{hardt})) yields a classifier $\tilde{Y}$ that predicts 0 or 1 at random, each with probability 0.5. While $\tilde{Y}$ is a completely fair classifier, it only achieves trivial 0-1 loss of 0.5.

\paragraph{Incorporating Fairness in the Learning Process}
Now, assume that the hypothesis class $\mathcal{H}$ only contains (non-homogeneous) halfspaces $(w,b)$ over $\mathbb{R}^2$, and classification is done using the sign of $w^{T}X + b$, where $w \in \mathbb{R}^2$, and $b \in \mathbb{R}$.

Absent fairness considerations, the best separating halfspace (in terms of the 0-1 loss) would provide us with classifications identical (on the given distribution) to those of the Bayes optimal classifier, and thus would have 0-1 loss of $\epsilon$, while being maximally unfair. However, as shown in Figure~\ref{fig:incorporating}, using our proposed method of penalization (as described in detail in Section~\ref{casestudy}) yields a halfspace which induces equivalent performance on $\mathcal{D}_{\epsilon}$ as classifying according to $X_2$, resulting in 0-1 loss of $2\epsilon$ and perfect fairness.
