
\section{Experiments}\label{sec:experiments}
We validate our approach using multiple datasets containing real-life data from the fields of criminal risk assessment, credit, lending, and college admissions. In each of the datasets we select a binary feature and treat it as the protected attribute (e.g., race or gender), which is the feature we require our trained classifier to behave fairly upon. Our proposed method performs well on all of these datasets, succeeding in removing unfairness almost entirely, at a very modest price in terms of accuracy.


\begin{table*}[h]
\centering
\resizebox{\textwidth}{!}{
\def\arraystretch{1.2}

\begin{tabular}{c c c | c | c | c || c | c | c || c | c | c |}

\cline{4-12}
&&&
\multicolumn{9}{ c| }{\textbf{COMPAS Dataset}}
\\ \cline{4-12}
&&&
\multicolumn{3}{ c|| }{\textbf{FPR Considerations}}&
\multicolumn{3}{ c|| }{\textbf{FNR Considerations}}&
\multicolumn{3}{ c| }{\textbf{Both Considerations}}
\\ \cline{4-12}
&&&
 $\mathbf{Acc.}$ &  $\mathbf{D_{FPR}}$ &  $\mathbf{D_{FNR}}$ &  $\mathbf{Acc.}$ &  $\mathbf{D_{FPR}}$ &  $\mathbf{D_{FNR}}$ &  $\mathbf{Acc.}$ &  $\mathbf{D_{FPR}}$ &  $\mathbf{D_{FNR}}$
\\  \cline{4-12}
\vspace*{-0.5ex}
\\ \cline{1-2} \cline{4-12}
\multicolumn{1}{ |c  }{} &
\multicolumn{1}{ c|  }{  \textbf{Our Method (AVD Penalizers)}}  &&
$\mathbf{0.660}$    &  $\mathbf{0.01}$  &  $0.04$ &
$\mathbf{0.653}$    &  $0.02$   &  $\mathbf{0.04}$ &
$\mathbf{0.654}$    &  $\mathbf{0.02}$  &  $\mathbf{0.04}$
\\ \cline{1-2} \cline{4-12}
\multicolumn{1}{ |c  }{} &
\multicolumn{1}{ c|  }{  \textbf{Our Method (SD Penalizers)}}  &&
$\mathbf{0.664}$    &  $\mathbf{0.02}$  &  $0.09$ &
$\mathbf{0.661}$    &  $0.05$   &  $\mathbf{0.03}$ &
$\mathbf{0.661}$    &  $\mathbf{0.02}$  &  $\mathbf{0.03}$
\\ \cline{1-2} \cline{4-12}
\multicolumn{1}{ |c  }{} &
\multicolumn{1}{ c|  }{  Zafar et al.~(\citeyear{disparatemistreatment})}  &&
$0.660$    &   $0.06$    &   $0.14$  &
$0.662$    &   $0.03$    &   $0.10$  &
$0.661$    &   $0.03$    &   $0.11$
\\ \cline{1-2} \cline{4-12}
\multicolumn{1}{ |c  }{} &
\multicolumn{1}{ c|  }{  Zafar et al. Baseline~(\citeyear{disparatemistreatment})}  &&
$0.643$    &   $0.03$    &   $0.11$  &
$0.660$    &   $0.00$    &   $0.07$  &
$0.660$    &   $0.01$    &   $0.09$
\\ \cline{1-2} \cline{4-12}
\multicolumn{1}{ |c  }{} &
\multicolumn{1}{ c|  }{  Hardt et al.~(\citeyear{hardt})}  &&
$0.659$    &  $0.02$    &   $0.08$  &
$0.653$    &  $0.06$   &    $0.01$  &
$0.645$    &  $0.01$   &    $0.01$
\\ \cline{1-2} \cline{4-12}
\multicolumn{1}{ |c  }{} &
\multicolumn{1}{ c|  }{  \textbf{Vanilla Regularized Logistic Regression}}  &&
$\mathbf{0.672}$    &   $\mathbf{0.20}$    &   $\mathbf{0.30}$  &
$\mathbf{0.672}$    &   $\mathbf{0.20}$    &   $\mathbf{0.30}$  &
$\mathbf{0.672}$    &   $\mathbf{0.20}$    &   $\mathbf{0.30}$
\\ \cline{1-2} \cline{4-12}
\end{tabular}
}
\vspace{3mm}
\caption{Performance comparison on the COMPAS dataset. For the approaches in bold -- Accuracy, FPR difference and FNR difference are evaluated on the test set, averaging over five runs and using a 70-30 training/test split. The performance of the remaining three approaches is stated as reported in Zafar et al.~(\citeyear{disparatemistreatment}).} \label{table:comparison_results}
\end{table*}



\begin{figure*}[b]
  \includegraphics[scale=0.6]{compas0-400.png}
  \caption{COMPAS Dataset. Accuracy, FPR difference ($\mathbf{D_{FPR}}$), and FNR difference ($\mathbf{D_{FNR}}$) (all evaluated on the test set) of the learned classifier, as a function of the weight $c=c_1 = c_2 \geq 0$ placed on the fairness penalizer terms. On the left we use the Absolute Value Difference (AVD) penalizer, and the Squared Difference (SD) penalizer on the right, both as presented in Section~\ref{regularization}. ``Relaxed FPR/FNR Diff.'' plots the value of the relevant penalization term.} %In this particular run, parameters chosen for the absolute value relaxation were: $c=80, q_c=60$, and for the squared relaxation: $c=220, q_c=30$.}
  \label{fig:compas}
\end{figure*}


\subsection{Implementation}
\textbf{Our method} 
%We instantiate our method in the following way: Given dataset $Q$, we split it randomly into a training set $S$ (which we will use for learning) and a test set $T$ (which we will only use for reporting performance). 
For the purpose of comparison with  Zafar et al.~(\citeyear{disparatemistreatment}) and Hardt et al.~\cite{hardt} on the COMPAS data, we use a parameter $c$ to induce three possible combinations of weights on the FPR and FNR penalization terms: $c = c_1$ and $c_2 = 0$; $c_1 = 0$ and $c = c_2$; and $c = c_1 = c_2$. For the other three datasets, we consider only $c = c_1 = c_2$.\footnote{The reason for varying the values of $c$ in the training phase is since we shifted to a proxy problem, in which we rely on the distance from the decision boundary rather the actual classifications. 
%Our hope is that there is no need for a worst-case cross validation between all of the combinations of $c_1, c_2, c_3$, and that the training scheme we propose is sufficient. 
It is possible, of course, that even better results are attainable using our scheme with other combinations of $c_1, c_2$, and $q$.} To explore the accuracy/fairness trade-off curve for the relaxed optimization problem~(\ref{eq:2}), we train for different values of $c$, starting at $c=0$ (which is just standard logistic regression), and growing gradually.



Given a dataset $Q$ and fixing a $d_1, d_2 \in \{0, 1\}$ of interest, we use the following training scheme:
\begin{enumerate}
\item Split $Q$ at random into training set $S$ and test set $T$.
\item For each $c$, perform cross-validation on $S$ to select the corresponding best value $q_c$ for the regularization parameter.
\item For each $(c,q_c)$, let $\theta_c = \argmin\limits_{\theta} \text{Proxy}(\theta;S,c,c,q_c)$.
\item Select $\theta^* \in \argmin\limits_{\theta_c} \text{Objective}(\theta_c;S,d_1,d_2)$.
\item Evaluate performance using $\theta^*$ on test set $T$.
\end{enumerate}
We report the average of five such runs, each with a fresh training-test split.




%We instantiate our method by solving the relaxed optimization problem~(\ref{eq:2}), in place of the original, non-convex problem~(\ref{eq:1}).  
%We test our approach with three different combinations of weights on the penalization terms:
%\katrina{What are the $d$, and how are they related to the $c$s?}
%\begin{enumerate}
%\item FPR considerations only: $d_1 = 1, d_2 = 0$.
%\item FNR considerations only: $d_1 = 0, d_2 = 1$.
%\item Both FPR, FNR considerations, assigned similar significance: $d_1 = 1, d_2 = 1$.
%\end{enumerate}
%One could, of course, pick any other combination of the FPR and FNR penalty weights.

%\katrina{I don't understand how the below is distinct from the list above}
%Learning is done by training the parameters of a logistic regressor to solve~\ref{eq:2}, while picking the value of $c_1, %c_2$ as the following:
%\begin{enumerate}
%\item FPR considerations only: $c_1 = c \geq 0$, $c_2 = 0$.
%\item FNR considerations only: $c_1 = 0$, $c_2 = c \geq 0$.
%\item Both FPR, FNR considerations, assigned similar significance: $c_1 = c_2 = c \geq 0$
%\end{enumerate}



% We then cross-validate to pick the best $c_3$ (the weight on the standard $\ell_2$-regularization term) given $c$.\footnote{The reason for varying the values of $c$ in the training phase is since we shifted to a proxy problem, in which we rely on the distance from the decision boundary rather the actual classifications. 
%Our hope is that there is no need for a worst-case cross validation between all of the combinations of $c_1, c_2, c_3$, and that the training scheme we propose is sufficient. 
%It is possible, of course, that even better results are attainable using our scheme with other combinations of $c_1, c_2, c_3$.} For each such combination, we report results as the averages of multiple \katrina{how many?} different runs, each time splitting data randomly into training and test sets.
%\yahav{We need to shorten this description.}

We solve the relaxed convex optimization problem using the CVXPY solver. Due to stability issues with large training sets, we use a train/test split of 30-70 on the larger datasets, rather than 70-30 as on the COMPAS dataset\footnote{The code implementing our method can be found at https://github.com/jjgold012/lab-project-fairness}.

%
%
%We then report the results (as evaluated on the test set) attained by a regressor $\theta \in \mathbb{R}^d$ that minimizes (on the training set $S$) a weighted combination of the $0$-$1$ loss and the differences in FPR and FNR across populations:
%\begin{equation*}
%\begin{aligned}
%&\underset{\theta}{\text{argmin}}
%& & L_{S}^{0\text{-}1}(\theta) \\
%&&& + d_1|FPR_{A=0}(\theta;S)-FPR_{A=1}(\theta;S)| \\
%&&& + d_2|FNR_{A=0}(\theta;S)-FNR_{A=1}(\theta;S)|
%\end{aligned}
%\end{equation*}
%
%\katrina{What is $d_1$ vs. $c_1$ etc.?}



%For classification, we decided use a standard cut-off threshold of $c=0.5$. There are of course, further possible interactions between the FPR, FNR considerations, and picking a certain cut-off level. These are not straightforward, since  these interactions are data-specific. 



%allows for flexibility in picking the values of $c_1, c_2$, which reflect the significance we wish to place on the objectives of achieving accuracy, equal FPR, and equal FNR. As for $c_3$, we will want to find the value of it that achieves the best results, for any combined objective of accuracy and fairness defined by a specific selection of $c_1,c_2$. Therefore, given a specific selection of $c_1, c_2$, we apply cross-validation to select the value of $c_3$. 




We briefly describe the other algorithmic approaches to which we compare:\\
\textbf{Zafar et al.}~(\citeyear{disparatemistreatment}) performs optimization by considering a proxy for the bias: the covariance between the samples' sensitive attributes and the signed distance between the feature vectors of misclassified users and the classifier decision boundary.\\
\textbf{Zafar et al. Baseline}~(\citeyear{disparatemistreatment}) tries to enforce equal FP/FN rates on the different groups by introducing different penalties for misclassified data points with different sensitive attribute values during the training phase.\\
\textbf{Hardt et al.}~(\citeyear{hardt}) performs post-processing on a standard trained (unfair) logistic regressor, picking different decision thresholds for different groups, and possibly adding randomization.


\subsection{Experimental Results}

In what follows, we use the following notation, given a trained classifier $\hat{Y}$:
\begin{align*}
\mathbf{D_{FPR}}&=\left|FPR_{A=0}(\hat{Y})-FPR_{A=1}(\hat{Y})\right| \\ 
\mathbf{D_{FNR}}&=\left|FNR_{A=0}(\hat{Y})-FNR_{A=1}(\hat{Y})\right|
\end{align*}
The values $FPR_{A=0}(\hat{Y})$, $FPR_{A=1}(\hat{Y})$, $FNR_{A=0}(\hat{Y})$, $FNR_{A=1}(\hat{Y})$ are reported as evaluated on the test set.

\paragraph{The COMPAS Dataset\footnote{https://github.com/propublica/compas-analysis}} The Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) records from Broward County, Florida 2013-2014, made available online by ProPublica, are perhaps the best-studied data in the context of fairness.  The goal in this scenario is to successfully predict recidivism within two years, based on features such as age, gender, race, number of prior offenses, and charge degree. The dataset contains 5,278 samples. The protected attribute in this scenario is race, where $A$ indicates black or white. We filtered the dataset using the same features as Zafar et al.~(\citeyear{disparatemistreatment}), to allow for comparison.

%\begin{table}[h]
%\centering
%\begin{tabularx}{\columnwidth}{c|c|c|c}
%\hline
%  &  Recid. ($y = 1$)        & No Recid.  ($y = 0$)       & Total \\ \hline
%Black &  $ 1661   $ & $ 1514 $ &  $ 3175 $ \\ \hline
%White &  $ 822   $  & $1281  $ &  $ 2103 $ \\ \hline
%Total &  $ 2483  $  & $2795 $ &  $ 5278 $ \\\hline
%\end{tabularx}
%\caption{Statistics of the ProPublica COMPAS data.} \label{table:compas-stats}
%\label{tab:stats}
%\end{table}
%\vspace{-1em}

%\begin{table}[h]
%\centering
%\begin{tabularx}{\columnwidth}{c|c}
%\hline
%Feature  &  Description \\ \hline
%Age Category &  $<25$, between $25$ and $45$, $>45$ \\
%Gender &  Male or Female \\
%Race &  White or Black \\
%Priors Count &  0--37 \\
%Charge Degree &  Misconduct or Felony \\
%\hline
%2-year-recid. & Whether or not the  \\
%(target feature)  & defendant recidivated within two years
%\end{tabularx}
%\caption{Description of features used from ProPublica COMPAS data.} \label{table:compas-features}
%\label{tab:features}
%\end{table}




\begin{table*}[t]
\centering
\caption{A description of the datasets used, along with parameters of the training procedure used for each.}
\label{table:datasets_description}
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{Dataset} & \textbf{No. Samples} & \textbf{No. Features} & \textbf{Train/Test Split} & \textbf{No. Repetitions} & \textbf{No. Folds in CV} & \textbf{Protected Feature} & \textbf{Target Variable} \\ \hline
COMPAS           & 5,278                     & 5                          & 70-30                     & 5                        & 5                                 & Race                       & 2-Year-Recidivism        \\ \hline
Adult            & 30,162                    & 10                         & 30-70                     & 5                        & 5                                 & Gender                     & Income Over/Under 50K    \\ \hline
Default          & 30,000                    & 23                         & 30-70                     & 5                        & 3                                 & Gender                     & Defaulting On Payments   \\ \hline
Admissions       & 20,839                    & 17                         & 30-70                     & 5                        & 3                                 & Race                       & Passing Bar Exam         \\ \hline
\end{tabular}
\end{adjustbox}
\end{table*}


\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\def\arraystretch{1.2}

\begin{tabular}{c c c | c | c | c || c | c | c || c | c | c |}

\cline{4-12}
&&&
\multicolumn{3}{ c|| }{\textbf{Adult Dataset}}&
\multicolumn{3}{ c|| }{\textbf{Default Dataset}}&
\multicolumn{3}{ c| }{\textbf{Admissions Dataset}}
\\ \cline{4-12}
%&&&
%\multicolumn{3}{ c|| }{\textbf{Both Considerations}}&
%\multicolumn{3}{ c|| }{\textbf{Both Considerations}}&
%\multicolumn{3}{ c| }{\textbf{Both Considerations}}
%\\ \cline{4-12}
&&&
 $\mathbf{Acc.}$ &  $\mathbf{D_{FPR}}$ &  $\mathbf{D_{FNR}}$ &  $\mathbf{Acc.}$ &  $\mathbf{D_{FPR}}$ &  $\mathbf{D_{FNR}}$ &  $\mathbf{Acc.}$ &  $\mathbf{D_{FPR}}$ &  $\mathbf{D_{FNR}}$
\\  \cline{4-12}
\vspace*{-0.5ex}
\\ \cline{1-2} \cline{4-12}
\multicolumn{1}{ |c  }{} &
\multicolumn{1}{ c|  }{  \textbf{Our Method (AVD Penalizers)}}  &&
$\mathbf{0.776}$    &  $\mathbf{0.00}$  &  $\mathbf{0.04}$ &
$\mathbf{0.807}$    &  $\mathbf{0.00}$   &  $\mathbf{0.01}$ &
$\mathbf{0.950}$    &  $\mathbf{0.01}$  &  $\mathbf{0.00}$
\\ \cline{1-2} \cline{4-12}
\multicolumn{1}{ |c  }{} &
\multicolumn{1}{ c|  }{  \textbf{Our Method (SD Penalizers)}}  &&
$\mathbf{0.783}$    &  $\mathbf{0.00}$  &  $\mathbf{0.09}$ &
$\mathbf{0.806}$    &  $\mathbf{0.01}$   &  $\mathbf{0.02}$ &
$\mathbf{0.950}$    &  $\mathbf{0.00}$  &  $\mathbf{0.00}$
\\ \cline{1-2} \cline{4-12}
\multicolumn{1}{ |c  }{} &
\multicolumn{1}{ c|  }{  \textbf{Vanilla Regularized Logistic Regression}}  &&
$\mathbf{0.800}$    &   $\mathbf{0.08}$    &   $\mathbf{0.39}$  &
$\mathbf{0.807}$    &   $\mathbf{0.01}$    &   $\mathbf{0.05}$  &
$\mathbf{0.951}$    &   $\mathbf{0.16}$    &   $\mathbf{0.02}$
\\ \cline{1-2} \cline{4-12}
\end{tabular}
}
\vspace{3mm}
\caption{Performance on the Adult, Loan Default, and Admissions datasets, penalizing for both FPR and FNR difference. Accuracy, FPR difference and FNR difference are evaluated on the test set, averaging over five runs and using a 30-70 training/test split.} \label{table:comparison_results_rest}
\end{table*}


In Table~\ref{table:comparison_results}, we compare the performance of our approach with that of three other techniques from the literature. Each method was trained based on logistic regression.  As a basis for comparison, we also present the performance of vanilla logistic regression, absent fairness considerations, with the regularization parameter selected via cross-validation.\footnote{Zafar et al.~(\citeyear{disparatemistreatment}) do not incorporate regularization in any of the approaches they report.}
%Results are reported as the averages of 5 different runs \katrina{Is that still correct?}, each time splitting data evenly and randomly into training and test sets. 
Results for Zafar et al., Zafar et al. baseline, and Hardt et al. appear here as reported in Zafar et al.~(\citeyear{disparatemistreatment}).\footnote{Our method selects the classifier based on the training set only and reports its performance over the test set. Results for the three other approaches, reported by Zafar et al.~(\citeyear{disparatemistreatment}), are based on tuning parameters after seeing the trade-off curve over the test set, and reporting according to the best selection of these parameters.}
%\katrina{Perhaps here is the right place for a footnote about the discrepancy with the Zafar baseline}

We find that the vanilla logistic regressor (absent fairness considerations) results in significant unfairness, as $\mathbf{D_{FPR}}=0.20$, and $\mathbf{D_{FNR}}=0.30$. The overall accuracy of this classifier measured on the test set was $0.672$.\footnote{Zafar et al.~(\citeyear{disparatemistreatment}) report a slightly different baseline of: Accuracy = 0.668, $\mathbf{D_{FPR}}=0.18$, $\mathbf{D_{FNR}}=0.30$.} Our SD penalization approach empirically achieves approximately the same accuracy as the Zafar et al.~(\citeyear{disparatemistreatment}) approach, with significantly better fairness. It is difficult to compare fairness-accuracy tradeoffs with the Hardt et al.~(\citeyear{hardt}) approach, since their accuracy is significantly lower than ours. A more direct comparison is possible by noting that our learned classifier can be post-processed to improve its fairness at a direct cost to accuracy. Hence, we can achieve accuracy of $0.659$ with $\mathbf{D_{FPR}} = \mathbf{D_{FNR}} = 0.01$, which compares very favorably with the Hardt et al. accuracy rate of 0.645 given the same FPR and FNR rates.\footnote{For completeness, we note that using a 50-50 training-test split (again not using the test set for parameter selection), our method (SD, both considerations) produces a classifier that provides: Accuracy = 0.659, $\mathbf{D_{FPR}} = 0.01, \mathbf{D_{FNR}} = 0.05$. This classifier can be post-processed to achieve rates of: Accuracy = 0.655, $\mathbf{D_{FPR}} = \mathbf{D_{FNR}} = 0.01$.}

Figure \ref{fig:compas} illustrates the accuracy/fairness trade-offs achievable using our scheme. Increasing the weight $c$ on the proxy fairness penalizers results in reducing their magnitude. The figure also illustrates how our relaxed penalizers succeed in tracking the real FPR and FNR differences. 
%
%
%\katrina{Must rewrite the following paragraph}
%We observe that our method succeeds in eliminating unfairness almost completely on the COMPAS dataset, while retaining most of the accuracy, when compared to the vanilla logistic regression. We achieve very low difference rates when penalizing for achieving each of the FPR and FNR criteria individually, and also for both. We achieve preferable results comparing to Zafar et al. and Zafar et al. baseline in all 3 scenarios, and also comparing to Hardt et al. in the settings of false positive/false negative considerations only. In the setting of both considerations - The Hardt et al. method removes a larger portion of the unfairness, however it results in major accuracy loss as it achieves accuracy rate of 0.645 in comparison to our method which results in accuracy of 0.665, retaining most of the original accuracy rate while removing most of the unfairness.




%The Hardt et al.~\cite{hardt} approach as reported removes a smaller portion of the bias in the different scenarios, however for FP/FN constraints alone, it provides higher accuracy rates. The Zafar et al.~(\citeyear{disparatemistreatment}) approach as reported retains significant bias (in most cases), but in some cases  achieves slightly superior accuracy rates to the methods above. 

%These performance comparisons are incomplete in the sense that each of the compared techniques has the potential to trade off between accuracy and fairness, using some degree of parameter tuning; what we report here is only one point on the achievable trade-off frontier for each algorithm. The ``correct'' trade-off, and, in particular, the best manner in which to weigh unfairness in the FPR against unfairness in the FNR, are matters of opinion. We have chosen to report our method's performance under parameters designed to very aggressively mitigate unfairness, at some cost to the accuracy.

%It would certainly be desirable to evaluate these and other approaches to fair learning on other datasets and on different tasks, particularly on larger datasets, which might afford both greater accuracy and better bias-reduction. The present empirical evaluations, however, suggest that our regularization-based approach provides a new tool worthy of consideration---we succeed in almost entirely eliminating bias on the hold-out set, at a modest price in terms of accuracy.

%Due to the fact that our true objective includes the original non-convex penalization terms, our approach does not carry any formal guarantees. However, the ease of implementation, generality, and empirical results are encouraging. Figure~\ref{fig:test1} illustrates the rate of convergence to a fair, accurate classifier on this dataset.
%In terms of computation costs, given that at each iteration we must calculate the gradient according to the FPR and FNR regularizers, we are required to predict the labels for the entire training set at each step. 
%However, this does not pose a computational burden, as it is already required by the (classic) gradient descent algorithm in our logistic regressor fitting scheme. Furthermore, when given a sufficiently large dataset (one or two orders of magnitude larger than the one currently available for the COMPAS scores data), this could be relaxed to sampling only a mini-batch of samples from the training data set at each iteration (much as is done in stochastic gradient descent).






\subsection{Additional Datasets}


Table~\ref{table:datasets_description} provides summary statistics on each of the datasets on which we tested our approach. We also briefly describe the datasets below. 


{\bf The Adult Dataset}\footnote{http://archive.ics.uci.edu/ml/datasets/Adult} is based on 1994 US Census data. The task we consider is to predict whether the income of each individual is over or under 50K dollars per year, based on features such as occupation, marital status, and education. The protected attribute selected in this task is gender. 

{\bf The Loan Default Dataset}\footnote{{\scriptsize https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients}}
contains data regrading Taiwanese credit card users. The task we consider is to predict whether an individual will default on payments, based on features such as history of past payments, age, and the amount of given credit. The protected attribute is gender.

{\bf The Admissions Dataset}\footnote{http://www2.law.ucla.edu/sander/Systemic/Data.htm}
contains records of law school students who went on to take the bar exam. The task we consider is to predict whether a student will pass the exam based on features such as LSAT score, undergraduate GPA, and family income. The protected attribute is set to race.

Table~\ref{table:comparison_results_rest} describes the performance of our approach on these datasets, and Figures~\ref{fig:adult},~\ref{fig:default}, and~\ref{fig:lawschool} illustrate the fairness-accuracy trade-offs we achieve in each context. Overall, we see that unfairness is nearly eliminated while accuracy remains quite high. The dataset on which accuracy suffers most under our approach is the Adult dataset, which is also the dataset on which the vanilla regression is the most unfair.


\begin{figure*}[]
  \includegraphics[scale=0.6]{adult0-800.png}
  \caption{Adult Dataset. Fairness-Accuracy tradeoffs, as in Figure~\ref{fig:compas}.}
  \label{fig:adult}  
\end{figure*}



\begin{figure*}[]
  \includegraphics[scale=0.6]{default0-50.png}
  \caption{Loan Default Dataset. Fairness-Accuracy tradeoffs, as in Figure~\ref{fig:compas}.}
  \label{fig:default}
\end{figure*}



\begin{figure*}[]
  \includegraphics[scale=0.6]{admissions0-400.png}
  \caption{Admissions Dataset. Fairness-Accuracy tradeoffs, as in Figure~\ref{fig:compas}.}
  \label{fig:lawschool}
\end{figure*}


