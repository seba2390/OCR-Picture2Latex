\section{Related Work}

Approaches to algorithmic fairness generally fall into two categories---situations where no ground truth is known (or perhaps the notion of ground truth is not well-defined), and settings where the algorithm has access to labeled examples on which to learn (perhaps from historical examples). In situations without access to ground truth, typical approaches to fairness include changing the data (e.g., to prevent the learner from having direct/indirect access to attributes that are considered sensitive)~\cite{fairrepresentations,certifying,wordembeddings}, or adapting the classifier (e.g., to treat similar people similarly)~\cite{fairnessthroughawareness, fairnessbandits, kamishima}. When ground truth information is available, we wish to prevent situations where the algorithm errs {\em in favor} of one group within the population. In the specific context of criminal risk assessments, Berk et al.~(\citeyear{stateoftheart}) give a thorough comparison of various fairness notions. 
 
%As Chouldechova~(\citeyear{chouldechova}) points out, it is commonly the case that false positive errors (error against) are more likely on the group with the higher base rate, while false negatives (error in favor) are more likely on the group with the lower base rate, when learning a predictor over a dataset which consists of groups with different base rates (for example, in the COMPAS scenario, a different percentage of re-offenders among whites vs.~blacks). 

Both Kleinberg et al.~(\citeyear{inherent}) and Chouldechova~(\citeyear{chouldechova}) show that fair classification entails unavoidable trade-offs, and that there are a number of reasonable desiderata (calibration, matching false positive rates (FPR) across populations, and matching false negative rates (FNR) across populations), that cannot, in general, be achieved simultaneously~\cite{biasinevitable}. Follow-up work by Pleiss et al.~(\citeyear{calibration}) shows that even when calibration is compatible with a generalization of FPR- and FNR-matching, any algorithm achieving both must is no better than randomizing a percentage of the predictions of an existing classifier; further investigation of calibration as a criterion for fairness can be found in H{\'{e}}bert-Johnson et al.~(\citeyear{calibrationformasses}).

There are also computational challenges to fairness. Woodworth et al.~(\citeyear{woodworth}) show that even in the restricted case of learning linear predictors, assuming a convex loss function, and demanding that only the {\em sign} of the predictor needs to be non-discriminatory, the problem of matching FPR and FNR requires exponential time to solve in the worst case. They also point out that for many distributions and hypothesis classes, there may not exist a non-constant, deterministic, perfectly fair predictor. 
%It is therefore evident that any method aimed at performing well at the task of learning an equalized odds classifier has to consider further relaxations from the general case, and that in many settings of interest, fairness considerations must be actively integrated into the learning process, as also suggested by Berk et al.~\citeyear{stateoftheart}.

Despite these theoretical challenges, learning fair classifiers remains an important, practical problem that must be addressed on real data---decisions must be taken, and trade-offs must be made. To this end, there have been a number of recent specific technical proposals for achieving algorithmic fairness. 
%Hardt et al.~(\citeyear{hardt}) in particular propose two definitions: {\em equal opportunity} (equal false negative rates across all groups, when ``positive'' is the desirable label), and {\em equalized odds} (matching the rates of both false negatives and false positives). 
%In what follows, we focus on the literature on FPR- and FNR-matching.
The fairness objective we study in this paper, that of matching false positive and false negative rates across populations in classification tasks, has in particular received substantial attention in the literature. %\yahav{Do we want to cite relevant papers here?}\katrina{I think the subsequent paragraphs cover it fine.}


Hardt et al.~(\citeyear{hardt}) propose a post hoc approach for learning such a fair classifier, probabilistically flipping some of the decisions of a given (unfair) trained classifier in order to match FPR and FNR across populations. Their approach yields a predictor which is not restricted to any hypothesis class, and that is a (possibly randomized) function of the original (non-fair) learned predictor and of the sensitive attribute (population membership). Although this is an elegant and appealing idea, the Hardt et al.~approach only guarantees optimality for a strictly convex loss function and an unconstrained hypothesis class~\cite{woodworth}. Follow-up work of Woodworth et al.~(\citeyear{woodworth}), shows that, in many cases, any such post hoc approach might result in a highly sub-optimal classifier. As Woodworth et al.~conclude, {\em post-processing} an unfair classifier is sometimes insufficient to achieve the best possible combination of fairness and accuracy; rather, in some cases, fairness considerations should be actively integrated into the learning process. %\yahav{Not sure if ``perhaps'' is the right term here. There are settings in which post-processing alone was proven to fail.}.

Zafar et al.~(\citeyear{disparatemistreatment}) give one such approach to integrating FPR and FNR matching into learning. Their algorithm relaxes the (non-convex) fairness constraints into proxy conditions, each in the form of a convex-concave (or, difference of convex) function. They then heuristically solve~\cite{convexconcave} the resulting optimization problem for a convex loss function.

The approach of the present work is to incorporate a penalty for unfairness into the learning objective. This is inspired in part by Kamishima et al.~(\citeyear{kamishima}), who designed an unfairness penalty term based on a very different notion of fairness, referred to in their paper as \textit{indirect prejudice}, which restricts the amount of mutual information between the prediction and the sensitive attribute. %\yahav{I still think we should stress that this is an entirely different notion than ours, and maybe mention statistical parity.}
%, and is related to a different notion of fairness usually referred to in the literature as \textit{statistical parity} (see also \cite{fairnessthroughawareness}).

The present work introduces new penalty terms, designed to enforce matching of FPR and FNR. Our approach is easy to use, and general in the sense it can be plugged in and utilized in a range of learning settings concerning classification problems. The accuracy-fairness trade-offs of our approach empirically compare favorably with the algorithms of Zafar et al.~(\citeyear{disparatemistreatment}) and Hardt et al.~(\citeyear{hardt}) on the COMPAS dataset, and we further validate the performance of our approach on several additional datasets from other fields of interest.