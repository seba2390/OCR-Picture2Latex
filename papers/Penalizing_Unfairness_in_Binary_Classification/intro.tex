\section{Introduction}
As machine learning-based methods have become increasingly prevalent in decision-making processes
that crucially affect people's lives, accuracy is no longer the sole
measure of a learning algorithm's success. In settings such as loan approvals~\cite{tracking}, policing~\cite{stopandfrisk}, targeted advertisement~\cite{latanyasweeney}, college admissions, or criminal risk assessments~\cite{machinebias}, algorithmic fairness must be carefully taken into account in order to ensure the absence of discrimination~\cite{bigdatasdisparateimpact, whiteguyproblem}.

Concerns of unfairness in classification were at the center of a recent media stir regarding the potential hazards of computer algorithms for risk assessment in the criminal justice system~\cite{machinebias,senttoprison}. The COMPAS system~(\citeyear{compasguide}), developed by Northpointe, is a proprietary algorithm, widely used in the United States for risk assessment and recidivism prediction. 
At the center of the controversy was an investigative report by Angwin et al.~(\citeyear{machinebias}), who observed that although the COMPAS algorithm demonstrated similar accuracy on whites and blacks when used to label individuals as either high or low risk for recidivism, the {\em direction} of errors made on whites versus blacks was very different.
More specifically, the rate of individuals who were classified using the COMPAS algorithm to be ``high risk'' but who did not actually re-offend was almost twice as high for black individuals as for whites; among those who were classified as ``low risk'' and did actually re-offend, the rate was significantly higher for whites than it was for blacks~\cite{ppanalysis}.

At least theoretically, fairness could necessarily come at a very high cost to accuracy, but it is possible that the tension between fairness and accuracy is far less stark on real-world data. Despite this, to date, there have been only a handful of techniques for ensuring fairness in classification that have been proposed and tested empirically.

\paragraph{Contribution} Motivated by this pressing need, we propose a new, easy-to-use, general-purpose technique for mitigating unfairness in classification settings. The approach deepens our understanding of how fairness considerations can be incorporated directly into the learning process, as opposed to imposing fairness post hoc on an arbitrary, unfair, learned classifier. We validate the ability of our approach to achieve both fairness and high accuracy, implementing and testing it on multiple datasets pertaining to recidivism, credit, loan defaults, and law school admissions. We find that our approach empirically outperforms existing approaches, and that fairness is often achievable at nearly no cost to accuracy.

%This paper proposes a new approach, inspired by the concept of regularization, to mitigating such unfairness in learned %classifiers. Our approach is practical and computationally efficient. We demonstrate its promise on the the COMPAS scores %dataset\footnote{https://github.com/propublica/compas-analysis}, as well as datasets from other fields of interest such as %credit and college admissions.
