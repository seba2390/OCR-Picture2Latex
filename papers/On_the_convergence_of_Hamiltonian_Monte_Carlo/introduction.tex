
%This paper discusses the stability properties (recurrence, geometric ergodicity) of
We consider in this paper the Hamiltonian Monte
Carlo (HMC),  a Metropolis-Hastings
algorithm designed to sample  target probability density
$\pi$ on $\rset^d$. This method was first proposed by \cite{duane:1987}  in computational physics. It has later been introduced in the  statistics community in the early paper of \cite{neal:1993} and quickly  gained popularity; see for example \cite[chapter~9]{liu:2008}, \cite{neal:2011} and \cite{girolami:calderhead:2011}. The most attractive feature of the HMC algorithm is to allow the possibility of generating proposals -  obtained by integrating a system of Hamiltonian equations - that are far away from the current position but still having a high probability of being accepted. The HMC algorithm therefore offer promise for eliminating the random walk behavior of most classical Monte Carlo algorithms. The distance between the current state and the proposal is controlled by length of the time interval along which the Hamiltonian equations are integrated; see \cite[chapter~9]{liu:2008} and \cite{sanz-serna:2014}.

HMC algorithms have achieved many empirical successes. Recently, the theory on HMC have been addressed by many authors % have developed a set of theoretical results to explain elucidate the reasons for these successes
; see
\cite{byrne:girolami:2013,tang:srivastava:salakhutdinov:2014,
schofield:barker:gelman:cook:briffa:2016,betancourt-bernoulli:2017,livingstone:betancourt:byrne:girolami:2016}. An in depth discussion of the HMC method and a survey of the existing results are given in \cite{bou-rabee:sanz-serna:2018}.
% Most of the current developments in the
% MCMC community are  linked with the HMC methodology  and the impact of HMC in the applications is huge.

Consider a target probability density  $\pi$ on $\rset^d$ with respect to the Lebesgue measure,  defined for all $\q \in \rset^d$ by
\begin{equation}
\label{eq:def_density_pi}
\pi(\q) = \left. \rme^{-\F(\q)} \middle / \int_{\rset^d}\rme^{-\F(\tilde{\q})} \rmd \tilde{\q} \right.\eqsp,
\end{equation}
where $\F: \rset^d \to
\rset$ is a continuously differentiable function. Note that this representation implies that the density is nonzero everywhere (this can be relaxed; see \cite[Section~5.5.1]{neal:2011}).

The properties of Hamiltonian dynamics have been discussed in numerous
papers. We provide here only a brief outlook mainly aimed at
introducing the notations and the essence of the main ideas. We refer
the interested readers to the monograph \cite{leimkuhler:reich:2004}
and the surveys given in \cite[Chapter~9]{liu:2008}, \cite{neal:2011},
\cite{betancourt-bernoulli:2017} and
\cite{bou-rabee:sanz-serna:2018}. The key idea behind HMC is to
exploit the measure-preserving properties of Hamiltonian flow over an
extended phase space. For simplicity, we restrict our study to the
phase space $\rset^{2d}$. Hamiltonian dynamics describes the evolution
of a physical system which consists in the \emph{position}
$\q \in \rset^d$ and the \emph{momentum} $\p \in \rset^d$.
% In this case, the state of the system consists of the
 %The momentum is an auxiliary variable which is used to update the position.
The total energy of the system is given by the Hamiltonian function $\Ham$ defined for  $(\q,\p) \in \rset^d \times \rset^d$ by
 % , equipped with the $2d \times 2d$ canonical
% structure matrix
% \begin{equation}
% \label{eq:structure-matrix}
% J= \left[
%  \begin{array}{cc}
%       0_{d} &\Imatrix_{d} \\
%     -\Imatrix_{d} & 0_{d}   \\
%   \end{array}
% \right] \eqsp,
% \end{equation}
% where $0_d$ is the $d \times d$ zero matrix and $\Imatrix_d$ is the $d \times d$   identity matrix.
\begin{equation}
  \label{eq:def_ham}
\Ham(\q,\p) = \F(\q) + \norm[2]{\p}/2 \eqsp,
\end{equation}
which is the sum of a potential energy $U : \rset^d \to \rset$, a
function solely of the position, and the kinetic energy
$p \mapsto \norm[2]{p}/2$ (note that other choices of kinetic energy
have proposed recently, see \eg~\cite{livingstone:faulkner:roberts:2017} and \cite{lu:et:al:2016}).
% where $\F(\q)$, the \emph{potential energy} is a function solely of the position of the particle and $\norm[2]{\p}/2$ stands for the \emph{kinetic} energy. In the kinetic term it is usual to add a matrix of mass which is here for simplicity taken to be the identity matrix.
The system then evolves in time $(\q(t),\p(t))_{t \geq 0}$ according to Hamilton's equations on $\rset^d \times \rset^d$,
\begin{equation}
  \label{eq:hamil_ode}
  \begin{aligned}
\frac{\rmd }{\rmd t}
\left[
  \begin{array}{c}
    q(t) \\
    p(t) \\
  \end{array}
\right]
& = J^{-1} \nabla \Ham(\q(t),\p(t)) \eqsp, \\
\quad \text{where} \quad  J&= \left[
                                         \begin{array}{cc}
                                           0_{d \times d} & -\Id_{d \times d} \\
                                           \Id_{d \times d} & 0_{d \times d} \\
                                         \end{array}
                                       \right]
                                       \eqsp,
                                       \quad \nabla \Ham(\q,\p) = \begin{bmatrix} \nabla U(\q) \\ \p \end{bmatrix}           \eqsp.
                                     \end{aligned}
                                   \end{equation}
We denote by $(\varphi_t)_{t \geq 0}$ the differential flow associated to the system \eqref{eq:hamil_ode}. For each $t \in \rset$, $\varphi_t: \rset^{2d} \to \rset^{2d}$ is the map that associates to each $(\p_0,\q_0)$ the value at time $t$ of the (unique) solution of \eqref{eq:hamil_ode} that takes the value $(\p_0,\q_0)$ at time $t=0$. We shall assume hereafter that $\varphi_t(\p_0,\q_0)$ is defined for any $(\p_0,\q_0) \in \rset^{2d}$ and $t \in \rset$.

A mapping $\Phi: \rset^{2d} \to \rset^{2d}$ is said to be \emph{symplectic} if, at each point $(p,q) \in \rset^{2d}$, $\Jac_{\Phi}(p,q)^T J \Jac_{\Phi}(p,q)= J$, where $\Jac_{\Phi}(p,q)$ denotes the $2d \times 2d$ Jacobian of $\Phi$. Note that in particular, % if $\Phi$ is a symplectic transformation, then the determinant of $\Jac_{\Phi}$ is equal to $\pm 1$. Hence
symplectic transformations are volume preserving on $\rset^{2d}$.
An important property of Hamiltonian systems \eqref{eq:hamil_ode} is that, for each $t \in \rset$, $\varphi_t$ is a symplectic mapping; see \cite[Theorem~2.1]{bou-rabee:sanz-serna:2018}.

Another important property of Hamiltonian flow is the conservation of energy.
Since $J^{-1}$ is skew-symmetric, for any solution $(\q(t),\p(t))$ of \eqref{eq:hamil_ode}
\[
\frac{\rmd }{\rmd t} \Ham(\q(t),\p(t)) =\nabla \Ham(\q(t),\p(t))^T J^{-1} \nabla \Ham(\q(t),\p(t))= 0.
\]
Then, the value of the Hamiltonian function is preserved by the flow of the corresponding Hamiltonian system, $\Ham \circ \varphi_t= \Ham$ for each $t \in \rset$.


Denote by $S$ the momentum flip involution, $S(\q,\p)= (\q,-\p)$, $(\q,\p) \in \rset^{2d}$. A mapping $\Phi: \rset^{2d} \to \rset^{2d}$ is said to be \emph{reversible} with respect to $S$ (or $S$-reversible for short) if $ S \circ \Phi= \Phi^{-1} \circ S$. If the mapping $\Phi$ is differentiable, the $S$-reversibility implies that $|\det \Jac_{\Phi} (S \circ \Phi)|= |\det \Jac_{\Phi}|^{-1}$, where $\Jac_{\Phi}$ is the Jacobian of $\Phi$.
%For each $(\q,\p) \in \rset^{2d}$, $S (\nabla  \Ham(\q,\p))= - \nabla \Ham (S (\q,\p))$.
By uniqueness of solutions of
\eqref{eq:hamil_ode}, for all $t \in \rset$, the flow $\varphi_t$ is  a
$S$-reversible mapping. More precisely, if $(\q_0,\p_0)$ is the initial state and
$(\q(t),\p(t))= \varphi_t(\q_0,\p_0)$ is the state of the system after
$t$ units of times, then
\[
\varphi_t(\q(t),-p(t))= \varphi_t( S \circ \varphi_t(\q_0,\p_0)= (\q_0,-\p_0)=S (\q_0,\p_0) \eqsp.
\]


Consider the extended target distribution with density given for any $(\q,\p) \in \rset^{2d}$ by 
\begin{equation}
\label{eq:def_ext_pi}
 \tilde{\pi}(\q,\p) = Z^{-1}
\exp(-\Ham(\q,\p)) \eqsp,   \, \, Z= \int_{\rset^{2d}} \exp(-\Ham(\q,\p)) \rmd \q \rmd \p \eqsp.
\end{equation}
Since the flow $\varphi_t$ preserves the oriented volumes and the Hamiltonian, the probability measure with density $\tilde{\pi}$ is preserved by the flow $\varphi_t$, for each $t \in \rset$ and $A \in \borelSet(\rset^{2d})$ (the Borel sets of $\rset^{2d}$), $\tilde{\pi}(\varphi_t(A))= \tilde{\pi}(A)$ where, with a slight abuse in notations,
$\tilde{\pi}(A)= \int_{\rset^{2d}} \indi{A}{q,\p} \tilde{\pi}(\q,\p) \rmd\q \rmd \p$.

For the Hamiltonian function \eqref{eq:def_ham}, the density $\tilde{\pi}$ may be factorized
\[
\exp(-\Ham(\q,\p)) = \exp(-\F(\q)) \exp(- \norm[2]{\p}/2)
\]
and then, under the distribution $\tilde{\pi}$, the position $\q$ and the momentum $\p$ are independent,
the marginal distribution of the position has a probability density function proportional to the target distribution \eqref{eq:def_density_pi} and the momentum $\p$ is Gaussian with zero mean and identity covariance matrix.



In most cases, it is not possible to compute explicitly the solutions of \eqref{eq:hamil_ode}; discretization must be used instead.
 A crucial point in the construction of HMC sampler is that  symplectiness and $S$-reversibility can be preserved exactly by discretization, provided that we use a \emph{symplectic integrator} like the St√∂rmer-Verlet (referred to as \emph{leap-frog}) integrator. The Hamiltonian is not exactly preserved in the discretization, but it is expected that a sensible integrator conserves this quantity at least "approximately".
% At each iteration of the algorithm, a new value for the momentum variable $\p$ is drawn from its conditional distribution given the current position $\q$ and then the Hamiltonian flow \eqref{eq:hamil_ode} is approximated to produce the next iterate. In most implementations of the HMC algorithm, the momentum variable $\p$ is drawn independently from the past from a $d$-dimensional zero-mean Gaussian distribution and then the leap-frog integrator to approximate the Hamiltonian flow is applied. The HMC algorithm is  based on the leap-frog integrator:
 Given a time step $h \in \rset^*_+$ and a number of iterations $T \in \nset^*$, the St√∂rmer-Verlet integrator proceeds as follows: starting from an initial point $(\q_0,\p_0) \in
\rset^d \times \rset^d$, $T$ leap frog steps are performed, where for each $\ell \in \{0,\dots,T-1\}$ the $\ell$-th leap frog step is defined as
\begin{equation}
\label{eq:iteration_verlet}
  \begin{cases}
\p_{\ell+1/2} &= \p_\ell - (h/2) \nabla \F(\q_\ell)\\
\q_{\ell+1} &= \q_\ell +h\p_{\ell+1/2}\\
\p_{\ell+1} &= \p_{\ell+1/2}-(h/2) \nabla \F(\q_{\ell+1}) \eqsp.
  \end{cases}
\end{equation}
% Each individual leap-frog step starts with half-step for the momentum variables, then a full step for the position variable using the new value of the momentum variable and finally, perform a half-step for the momentum variable using the updated value of the position variable.
The sequence $(\q_\ell,\p_\ell)_{\ell \in \{0,\dots,T\}}$ is an
approximation of the solution of \eqref{eq:hamil_ode} at times $\{\ell h
\, : \, \ell \in \{0,\ldots, T\}\}$ started at $(\q_0,\p_0)$. This sequence defines a discrete dynamical system given for $\ell \in \{0,\ldots,T-1\}$ by
\begin{equation}
  (\q_{\ell+1},\p_{\ell+1}) =  \Psiverlet^{(1)}_{h/2} \circ \Psiverlet^{(2)}_h \circ \Psiverlet^{(1)}_{h/2} (\q_\ell,\p_\ell) = \Phiverlet^{(1)}_h(\q_\ell,\p_\ell) \eqsp,
\end{equation}
where for each $t \in \rset_+$, $\Psiverlet^{(1)}_t, \Psiverlet^{(2)}_t :\rset^{2d} \to \rset^{2d}$ are given for all $(\q,\p) \in \rset^{2d}$ by
\begin{equation}
  \label{eq:def_Psiverlet_0}
\Psiverlet^{(1)}_t(\q,\p) = (\q, \p-t\nabla \F(\q)) \eqsp, \quad\Psiverlet^{(2)}_t(\q,\p) = (\q+t\p, \p)
\end{equation}
These mappings in molecular dynamics are called the \emph{kick} (the system stays in its current configuration and the momentum is incremented by action of the force $\nabla U(\q)$) and the \emph{drift} (the position $q$ advances at constant speed while the momentum $\p$ remains constant). One iteration of the St√∂rmer-Verlet formula comprises two kicks of duration $h/2$ separated by a drift of duration $h$.
Define the sequence of iterates $\{\Phiverlet[h][\ell] : \rset^d \times \rset^d \to \rset^d \times \rset^d \, : \, \ell \in \nset^*\} $ for $\ell \geq 1$ by induction
\begin{equation}
  \label{eq:def_Phiverlet}
\Phiverlet[h][\ell+1] = \Phiverlet[h][\ell] \circ \Phiverlet[h][1] \eqsp, \quad
  % \Phiverlet(\q,p) = \parenthese{\q-(h^2/2) \nabla \F(\q) + h \p , \p - (h/2) \defEns{ \nabla \F(\q) + \nabla \F\parenthese{\q-(h^2/2) \nabla \F(\q) + h \p}}} \eqsp.
\end{equation}
Set for all $\ell \geq 1$,  %$\{\Phiverletq[h][\ell] : \rset^d \times \rset^d \to  \rset^d \, : \, \ell \in \nset^*\}$ by for all $\ell \geq 1$,
\begin{equation}
  \label{eq:def_Phiverletq}
  \Phiverletq[h][\ell] = \projq \circ \Phiverlet[h][\ell] \eqsp,
\end{equation}
where $ \projq : \rset^d \times \rset^d \to \rset^d$ is the projection on the first $d$ coordinates, for all $(\q,\p) \in \rset^d \times \rset^d$, $\projq(\q,\p) = \q$.
Thus, with our notation for all $\ell \in \{1,\ldots, T\}$, $ (\q_\ell,\p_\ell) =\Phiverlet[h][\ell](\q_0,\p_0)$ and $ \q_\ell = \Phiverletq[h][\ell](\q_0,\p_0)$

 Because each inner step in the leap-frog step are \emph{shear} transformations of the phase variable (only the position or the momentum are updated by a quantity that depends only on the variable that do not change), it is clear that this transformation is volume preserving (the Jacobian of each individual transformation is equal to $1$). Each inner leap frog step due of its symmetry is also $S$-reversible: starting from $(\q_{\ell+1},-\p_{\ell+1})$, applying the leap-frog step forward and then negating the momentum variable again, we obtain again $(\q_\ell,\p_\ell)$.

We now have all the background required to describe the HMC algorithm,
which is a special instance of Metropolis-Hastings algorithm aimed at sampling the  target distribution  $\pi$.
It is similar to most classical MCMC algorithm in that we propose a new point based on the current position and then either accept or reject it.
%Starting from $(\Q_0, \P_0) \in
%\rset^d \times \rset^d$ and given parameters $h >0$ and $T \in \nset^*$, the
%algorithm defines the Markov chain $(\Q_k,\P_k)_{k \geq 0}$ as
%follows.
Denote by $(\Q_k,\P_k)$ the value of the position and momentum at the $k$-th iteration of the algorithm. Each iteration of the algorithm may be decomposed into two steps, which
% . The first step modifies only the momentum. The second  step may change both the momentum and the position. Both steps
are constructed to leave the extended
distribution $\tilde{\pi}$ invariant; see \cite{neal:2011}, \cite{fang:sanz-serna:skeel:2016} and \cite[Theorem 5.2]{bou-rabee:sanz-serna:2018}.
In the first step, we draw $G_{k+1}$ from the $d$-dimensional normal
distribution with zero mean and identity covariance matrix, independent of $\{(\Q_j,\P_j)\}_{j=0}^k$.
% Since $\Q_k$ is not changed and $G_{k+1}$ is drawn from the stationary distribution of the momentum, this step leaves the joint distribution $\tilde{\pi}$ invariant.
In the second step, we set the initial conditions $(\Q_k,G_{k+1})$ and compute the position and the momentum after $T$ leapfrog steps. This move is accepted with probability $\alphaacc\defEns{(\Q_k,G_{k+1}),\Phiverlet[h][T](\Q_k,G_{k+1})}$ where
% related to $\tilde{\pi}$.
for all $(\q,\p) \in \rset^d \times \rset^d$, $(\tilde{\q},\tilde{\p}) \in \rset^d \times \rset^d$ by
\begin{equation}
\label{eq:def_acc_ratio}
  \alphaacc\defEns{(\q,\p),(\tilde{\q},\tilde{\p})} = \min \parentheseDeux{1,   \exp\parenthese{\Ham(\q,\p)-\Ham(\tilde{\q},\tilde{\p}})} \eqsp.
\end{equation}
It is easily seen that $\tilde{\pi}$ is invariant with respect to $\tilde{\pi}$ (see \cite{fang:sanz-serna:skeel:2016}).  Since $\tilde{\pi}$ \eqref{eq:def_ext_pi} is invariant with respect to the Markov kernel defined by the HMC algorithm on the extended state
space $\rset^d \times \rset^d$, it naturally implies that $\pi$ is a
stationary distribution for the Markov chain $(\Q_k)_{k \geq 0}$,
which is the process which we are interested in. The number of steps $T$ is either a deterministic quantity or a random variable independent of the current state. If the number of
steps $T=1$, then the algorithm reduces to the Metropolis Adjusted
Langevin Algorithm (MALA).



% The Markov kernel associated with $(\Q_k,\P_k)_{k \geq 0}$ is given for all $(\q,\p) \in \rset^d \times \rset^d$ and $\eventB \in \borelSet(\rset^d \times \rset^d)$ by
% \begin{align}
% \nonumber
%   \PkerhmcD[h][T]((\q,\p), \eventB) &=\int_{\rset^d} \indi{\eventB}{\Phiverlet[h][T](\q,\tilde{\p})} \ \alphaacc\defEns{(\q,\tilde{\p}),\Phiverlet[h][T](\q,\tilde{\p})} \frac{\rme^{-\norm[2]{\tilde{\p}}/2}}{ (2 \uppi)^{d/2}} \rmd \tilde{\p}
% \\
% %  \label{eq:def_kernel_0_double}
% &+  \updelta_{(\q,\p)}(\eventB) \,   \int_{\rset^d}  \parentheseDeux{1-\alphaacc\defEns{(\q,\tilde{\p}),\Phiverlet[h][T](\q,\tilde{\p})}} \frac{\rme^{-\norm[2]{\tilde{\p}}/2}}{ (2 \uppi)^{d/2}}  \rmd \tilde{\p} \eqsp.
% \end{align}

%The sequence  is itself a Markov chain.

Despite many recent advances, theoretical properties of the HMC algorithm are still not completely
understood. This paper addresses two important issues in the analysis of HMC algorithm: irreducibility and geometric ergodicity.

Irreducibility  plays
an essential role in the theory of Markov chains. In particular, it implies uniqueness of a
invariant distribution. The classical approach to derive
irreducibility of Hastings-Metropolis algorithms on $\rset^d$, outlined for example in \cite{mengersen:tweedie:1996}
\cite{roberts:tweedie:1996:biometrika}, is to use that the proposal
distribution admits a (sufficiently regular) transition density with respect to the Lebesgue
measure. % show that the associated Markov kernel is a
% $T$-chain, \ie admits an absolutely continuous part with respect to
% the Lebesgue measure.  is to admit a absolutely continuous the
% proposal of the HMC algorithm may not have a density with respect to
% the Lebesgue measure.
For HMC, this condition does not necessarily hold. HMC has been shown
to be irreducible in \cite{cances:legoll:stoltz} in the case where the
state space is compact and the potential is twice continuously
differentiable. In \cite{livingstone:betancourt:byrne:girolami:2016},
under appropriate conditions, irreducibility is shown for a version
of HMC where the number of leap-frog steps $T$ is random, independent
of the proposal, and such that $T=1$ with positive probability.  Under such assumption, irreducibility of HMC boils down to irreducibility of MALA which has been established in
\cite{roberts:tweedie:1996}.  In this paper, we establish the
irreducibility of the HMC algorithm under a general tail condition of
the target density which significantly relaxes the condition of
\cite{cances:legoll:stoltz} and
\cite{livingstone:betancourt:byrne:girolami:2016}.  This result
follows from a general irreducibility result for iterative Markov
models (derived under conditions which are weaker than the ones
reported in the literature) which we believe to be of independent
interest; see \Cref{sec:irred-class-iter}.  Our main tool to establish
irreducibility is the degree theory for continuous maps \cite{outerelo:ruiz:2009}.
% Establishing the irreducibility of the HMC kernel
% is therefore much more challenging than  for classical Metropolis-Hastings type algorithm.

In a second part, we establish the geometric ergodicity of the HMC
sampler under the assumptions that the potential $U$ is homogeneous
outside a ball (or is a perturbation of an homogeneous function) and
that the level sets are convex.  Our assumptions
imply that the proposal kernel of HMC satisfies an ‚Äòinwards
acceptance‚Äô property \cite{roberts:tweedie:1996}, which is essential
to show that HMC (and MALA) is geometrically ergodic.
% The conditions
% under which we derive geometric ergodicity of HMC can be made more
% general but the calculations are already involved which has kept us of relaxing the assumptions.

Our results complement the recent paper \cite{livingstone:betancourt:byrne:girolami:2016}. This paper provides a variety of conditions under which the HMC algorithm is not geometrically ergodic. It establishes the geometric ergodicity under the abstract 'inwards acceptance' property for which we provide verifiable sufficient conditions.

In \cite{bou:sanz:2017}, a variant of HMC, referred to as the
Randomized Hamiltonian Monte Carlo (RHMC), is analyzed. This method is
associated with a conti\-nuous-time Markov process for which
$\tilde{\pi}$ given by \eqref{eq:def_ext_pi} is invariant
\cite[Proposition 3.1]{bou:sanz:2017}.  However, sampling such a
process requires the exact Hamiltonian flow (and hence exact
integration of the Hamilton dynamics \eqref{eq:hamil_ode}). The use of
the Hamiltonian flow allows to by-pass the acceptance-rejection step
and makes the analysis easier.
By-passing the discretization step nevertheless reduces the
applicability of the results, since direct integration of the
Hamiltonian flow is most of the time not an option.
We numerically show on a simple example
that the conditions given by \cite{bou:sanz:2017} which imply
geometric ergodicity of RHMC are not sufficient in the case of HMC.
%random time which is distributed according to an exponential
%distribution. % \cite[Theorem 3.9]{bou:sanz:2017} shows that the Markov
% semi-group is geometrically ergodic which are different from ours.
% The main aim of this paper is to contribute to fill the gap between theory and practice.
%Second,
% Surprisingly enough, in our analysis the geometric ergodicity relies on a subtle property of the Verlet integrator, which has not been noticed in an earlier attempt on this topic \cite{livingstone:betancourt:byrne:girolami:2016}.

The paper is organized as follows. In \Cref{sec:ergodicity-hmc}, conditions upon which the
HMC kernel, associated with $(Q_k)_{k \in \nset}$, is irreducible, recurrent and Harris-recurrent are given.
%Comparison with earlier results will be mainly given in this Section.
% Their proof which consist in showing that the HMC Markov chain
% is irreducible with respect to the Lebesgue measure.
In \Cref{sec:geom-ergod-hmc}, conditions under which the HMC kernel is $V$-uniformly geometrically ergodic  are developed and discussed. Some general irreducibility results which are of independent interest, are stated in \Cref{sec:irred-class-iter}. The proofs are gathered in
\Cref{sec:postponed-proofs}.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
