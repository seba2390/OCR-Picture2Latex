\section{Evaluation Criteria and Setup}

In this section, we discuss the evaluation criteria, experimental setup, and
trace generation for our evaluation.

\subsection{Evaluation Criteria}
\label{sec:evaluation_criteria}

\paragraph{Research Questions.}
We first aim to address the following research questions
for \system{}'s \signatureSynthesizer:
%
\begin{itemize}[leftmargin=0.35in]
	%\setlength{\itemsep}{0em}
	\item [$\mathsf{QS_1}$.] How effective are the synthesized signatures?
	\item [$\mathsf{QS_2}$.] How scalable are the signature synthesizers?
	\item [$\mathsf{QS_3}$.] Does training set size impact the quality of signatures?
\end{itemize}

We next focus on evaluating the monitor component, when considering the warning
system implementation, by answering to the following research questions:

\begin{itemize}[leftmargin=0.49in]
	\item [$\mathsf{QWS_1}$.] How many messages/second can a monitor classify?
	\item [$\mathsf{QWS_2}$.] What is the energy consumption overhead for a monitor?
	\item [$\mathsf{QWS_3}$.] What type, and how many, warnings do the different
	monitors produce when \system is deployed on real cellular networks?
\end{itemize}

We then evaluate the monitor component, when considering the baseband implementation,
by answering the following research questions:

\begin{itemize}[leftmargin=0.46in]
	\item [$\mathsf{QBB_1}$.] What is the memory overhead induced by \system?
	\item [$\mathsf{QBB_2}$.] What is the computational overhead induced by \system?
\end{itemize}

\subsection{Experiment Setup}
In this subsection, we provide details on the experimental setup for both components.

\paragraph{Signature Synthesizer Evaluation Infrastructure.} We perform all the signature synthesizer evaluation on a 4.5GHz
Intel i7-7700K CPU running Ubuntu 16.04 on 16GB of RAM. We set a time out
of 3,600 seconds for these experiments.

\paragraph{PHOENIX Baseband Implementation.} We perform the baseband
implementation experiments by implementing \system into srsUE as described in
Section \ref{sec:baseband_implementation} on a 4.5 GHz Intel i7-7700K CPU
running Ubuntu 16.04 on 16GB of RAM
connected to a USRP board~\cite{usrp}.

Note that we do not measure the power consumption in this instantiation as any
meaningful measurement would require additional appropriate hardware. Additionally, the baseband implementation experiments do not leverage a stress test
as it is not clear how to achieve this with srsUE \cite{gomez2016srslte}.

\paragraph{Sample Sizes.} We consider different sizes of traces (50, 100, 250, 500, 1250, and 2500) in our evaluation. In each trace, $50\%$ are positive
and the rest are negative. To generate these traces, we used
the procedure mentioned in Section\ref{sec:trace_gathering}.

\paragraph{Training and Testing Separation.} To measure the effectiveness of the  signatures,
we create disjoint testing and training sets for each attack, containing
1000 benign and 1000 malicious traces using the procedure mentioned in Section~\ref{sec:trace_gathering}.


\paragraph{Monitor Evaluation Testbed.}
We perform all the monitor experiments on three different
COTS Android devices (see Table~\ref{tab:device_for_experiment}
for devices' details).
%\begin{figure}[t]
%  \centering
%	\includegraphics[width=.8\columnwidth]{figures/testbed.pdf}
%	\caption{Monitor Evaluation Testbed}
%	\label{fig:testbed}
%\end{figure}
Also, following the prior work~\cite{alter,lteinspector,kim_ltefuzz_sp19} we set up a similar 4G LTE testbed (consisting of eNodeB and EPC) using
srsLTE \cite{gomez2016srslte} and USRP B210~\cite{usrp} connected to Intel
Core i7 machines running Ubuntu 16.04 with 16 GB of memory.
% as shown in Figure \ref{fig:testbed}.

\paragraph{Effectiveness Evaluation.}
To evaluate effectiveness of the signatures, we implement \system to its entirety
and replay benign and malicious traces through srsLTE~\cite{gomez2016srslte}.

\paragraph{Efficiency Evaluation.}
To evaluate efficiency through a \emph{stress test}, we develop an application that serves
as an in-device network simulator by replaying the logs within the device. We use this  setup because software-defined radios
have inherent limitations on transmission bandwidth. Therefore, a high-volume of packets within a short time-interval cannot be injected to the device for stress testing, which is important for realizing our monitors' efficiency in real networks.

\paragraph{Set of Attacks}.
\label{sec:set_of_attacks}
We consider 15 attacks (Table~\ref{tab:all_attacks}) for our evaluation.
The reason for considering these 15 attacks are twofold:
(1) These attacks can serve as representatives of
most of the known vulnerabilities in 4G LTE control-plane layers;
and (2) They have at least one of the following characteristics:
(a) violation of temporal ordering of events;
(b)  triggered by rogue eNodeB or Mobility Management Entity (MME) at RRC or NAS layers.
% ;
%


\begin{table}[htbp]
  \centering
  \resizebox{.8\columnwidth}{!}{
	\begin{tabular}{|l|l|l|}
		\hline
		\multicolumn{1}{|c|}{\textbf{Phone Model}} & \multicolumn{1}{c|}{\textbf{CPU}} & \multicolumn{1}{c|}{\textbf{Operating System}} \\ \hline
		Pixel 3 & Qualcomm Snapdragon 845 \cite{snapdragon845} & Android 9 \\ \hline
		Nexus 6P & Qualcomm Snapdragon 810 \cite{snapdragon810} & Android 8.0.0 \\ \hline
		Nexus 6 & Qualcomm Snapdragon 805 \cite{snapdragon805} & Android 5.1.1 \\ \hline
	\end{tabular}
  }
	\caption{Specifications of devices used for evaluation.}
	\label{tab:device_for_experiment}
\end{table}


\subsection{Trace Generation for Evaluation}\label{sec:trace_gathering}
We now discuss how we generate traces for evaluating \system{}'s monitor and
optional signature synthesizer components. We use the following approach
to generate a large number of  traces containing undesired behavior to evaluate scalability of the
synthesizers. Also, a different set of traces generated with this approach is used to evaluate the
effectiveness of \system's monitor.

\subsubsection{Sessions, Traces, and Variants}
We now introduce the concepts of a \emph{session}, \emph{trace}, and \emph{variants of an attack session} used later.
A \textbf{session}, which can be logically viewed as a sequence of protocol messages,
starts off with the device sending a connection initiation request (e.g., \rrcConnectionRequest,
\attachRequest) and contains all messages (including the current connection initiation request message)
until the next connection initiation request is sent. Note that, we do not say that a session ends with a termination request to
facilitate sessions which end abruptly.  A \textbf{trace} is just a sequence of
sessions. We call a session \textbf{$\beta$-undesired-behavior-session} if the undesired behavior $\beta$
occurs in that session. For a canonical $\beta$-undesired-behavior session $s$ (obtained from the original source
of the undesired behavior discovery), we call another \textbf{$\beta$-undesired-behavior-session} $\hat{s}$ a variant of $s$, only if
$s\neq\hat{s}$.


\begin{figure}[t]
	\centering
	  \includegraphics[width=.9\columnwidth]{figures/rlf_report_variants.pdf}
	  \caption{$\beta$-undesired-behavior-session variants where $\beta$=privacy attack on the RLF report. The red arrow points to the location
	in a benign session where  both \securityModeCommand and \securityModeComplete
	would have appeared.}
	  \label{fig:measurement_report}
  \end{figure}


\begin{example}[$\beta$-undesired-behavior-session variants]
	%
	% Variants of Privacy Attack on the Measurement Report]
	For this example, we consider $\beta$=the privacy attack on the RLF report \cite{privacy_ndss16}.
	In its canonical form, this attack happens in a session when a device responds with the RLF report message in plaintext
	due to an unprotected \ueInformationRequest message sent by the adversary before establishing a
	security context (i.e., before receiving \securityModeCommand and sending \securityModeComplete).
	$4$ example variants of this $\beta$-undesired-behavior-session is shown in Figure \ref{fig:measurement_report}.
 These different variations differ in what messages were sent before and after to the exclusion
of the \securityModeCommand and \securityModeComplete
messages.   Variant $1$, the canonical session,
does not introduce any messages before or after skipping the Security Mode
procedure and just sends the unprotected \ueInformationRequest message to induce the device to respond with an
unprotected RLF report message. Variant $2$ introduces a variation prior to the skipping of the Security
Mode procedure (e.g., sending an identity request message). Variant $3$ introduces a variation after
the skipping of the Security Mode procedure, possibly by inquiring about the UEâ€™s capabilities through
the \ueCapabilityEnquiry message, before the plaintext \ueInformationRequest is sent by the adversary.
% an \ueInformationRequest message to obtain some other information prior to the attack.
Variant $4$ combines both Variants $2$ and $3$.
\end{example}


\begin{figure}[t]
  \centering
	\includegraphics[width=.8\columnwidth]{figures/trace_gathering.pdf}
	\caption{Trace Generation procedure.}
	\label{fig:trace_gathering}
\end{figure}

\subsubsection{Benign Trace Dataset}
To obtain benign traces, we use the MobileInsight \cite{mobile_insight}
crowd-sourced database. This database consists of log files captured by the
MobileInsight app and shared from users across the world; covering numerous
devices, networks, and countries. We decide to use this data rather than locally
captured benign traces to take into consideration other devices and networks,
which we do not have access to. We argue that this gives a better representation
as to how well the signatures would generalize in the real world trace, possibly
containing benign network failures.

From this dataset, we are able to obtain \textit{1,892} NAS layer traces which
contain over \textit{52K} messages, and as for RRC, we collect \textit{2,045}
RRC layer traces consisting of \textit{1.5M} messages. This large discrepancy
in the number of messages captured per layer can be attributed to the fact that
NAS traffic only serves as the communication between the UE and MME, while RRC
is responsible for the communication between the UE and the eNodeB and serves
as the backbone for NAS and other layers of the LTE protocol stack.

\textbf{Benign trace generation.} We use the collected MobileInsight
traces as seed traces and decompose them into individual sessions. In addition to
the message types in a session, we also capture relevant predicates from the data
(e.g., whether the identity request message warranted IMSI, IMEI, or GUTI).
After this step, suppose we have a total of $S$ number of sessions. If we want
 to generate $n$ benign traces of length $M$, then we will continue
the following process $n$ times. At each step, we will randomly pick $M$
benign sessions out of total $S$ sessions and concatenate them to create a new benign
trace. The process is shown on the left of the dotted vertical line in Figure \ref{fig:trace_gathering}.
After this process, we will obtain \emph{trace skeletons} comprising of individual message types and relevant
predicates. We then manually convert these trace skeletons to actual replayable benign traces
by choosing standard-compliant field values feasible in the testbed while respecting the different predicates. As an example,
if the benign trace skeleton in a session contained identity request with IMEI predicate, then we will create a
concrete packet reflecting that choice.


\subsubsection{Generating Malicious Traces}
A massive challenge with evaluating the effectiveness of
\system is the fact that no pre-existing
repository of vulnerable traces exists. To overcome this, we propose the generation of \emph{possibly} malicious
traces as shown in Figure \ref{fig:trace_gathering}. The trace generation has the following four steps.
(\ding{182}) The process starts with the manual implementation of all the attacks (and, their $\beta$-undesired-behavior-session variants)
as listed in Table~\ref{tab:all_attacks}. For doing so,
following the prior work \cite{kim_ltefuzz_sp19, privacy_ndss16, TORPEDO,
lteinspector, park2016white, how_not_to_break_crypto} we changed srsENB and srsEPC libraries in
srsLTE \cite{gomez2016srslte} to set up the rogue base station.
To collect the traces from the UE's perspective, we utilize SCAT \cite{SCAT}.
(\ding{183}) Once we have collected the concrete traces, we create skeletons of these traces akin to
to the benign trace generation process (i.e., capturing message types and relevant predicates).
After this process, for each attack, suppose we have $K$ skeletons  for $\beta$-undesired-behavior-session variants.
(\ding{184}) Suppose we want to generate $n$ possibly malicious traces of length $M$ for a given attack.
We will execute the following step $n$ times. At each step, we will first generate a
benign trace skeleton $bt$ of length $M$ using the procedure discussed above. Then,
we randomly choose $a_s$ attack variants out of $K$
(i.e., $1\leq a_s < min(M, K)$) and randomly replace $a_s$ of the benign sessions of $bt$ %(except the first benign session)
with the $a_s$ attack sessions to generate a possibly malicious  trace skeleton (see Figure \ref{fig:trace_gathering}).
(\ding{185}) For generating a concrete replayable malicious trace from a trace skeleton is a manual process and
attack-specific. Converting malicious trace skeletons to concrete traces require adding standard-compliant
field values while respecting the captured predicates.


\paragraph{Discussion.} Note that, all variants generated by the above process do not necessarily entail
an exploitable attack. This is not a limitation because the monitor has to be oblivious to whether a device is
susceptible to an attack or not, and instead should raise a warning irrespectively whenever it detects an attack attempt.
Taking the privacy attack on the RLF report as an example, the monitor should raise a warning whenever it receives
an unprotected \ueInformationRequest message before a security context is established without waiting for the
device to respond with an RLF report. For our evaluation, malicious traces that do not induce an attack are acceptable
as long as the trace contains an attack attempt. All variants can be found on
the following webpage \cite{phoenix_webpage}.




\begin{table}[]
	\centering
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{|l|c|c|c|c|}
	\hline
	\multicolumn{1}{|c|}{\textbf{Attack}} & \textbf{Paper} & \textbf{Layer} & \textbf{\# of Variations} & \textbf{Implication} \\ \hline
	AKA Bypass & \cite{kim_ltefuzz_sp19} & \newmoon & 18 & Eavesdropping \\ \hline
	Measurement Report & \cite{privacy_ndss16} & \newmoon & 26 & Location Tracking \\ \hline
	RLF Report & \cite{privacy_ndss16} & \newmoon & 21 & Location Tracking \\ \hline
	IMSI Cracking & \cite{TORPEDO} & \newmoon & 2 & Information Leak \\ \hline
	Paging with IMSI & \cite{TORPEDO} & \newmoon & 2 & Information Leak \\ \hline
	Attach Reject & \cite{privacy_ndss16} & \fullmoon & 4 & Denial of Service \\ \hline
	Authentication Failure & \cite{lteinspector} & \fullmoon & 25 & Denial of Service \\ \hline
	EMM Information & \cite{park2016white} & \fullmoon & 32 & Spoofing \\ \hline
	IMEI Catching & \cite{3gppNAS} & \fullmoon & 2 & Information Leak \\ \hline
	IMSI Catching & \cite{3gppNAS} & \fullmoon & 2 & Information Leak \\ \hline
	Malformed Identity Request & \cite{how_not_to_break_crypto} & \fullmoon & 2 & Information Leak \\ \hline
	Null Encryption & \cite{3gppNAS} & \fullmoon & 49 & Eavesdropping \\ \hline
	Numb Attack & \cite{lteinspector} & \fullmoon & 2 & Denial of Service \\ \hline
	Service Reject & \cite{privacy_ndss16} & \fullmoon & 14 & Denial of Service \\ \hline
	TAU Reject & \cite{privacy_ndss16} & \fullmoon & 6 & Denial of Service \\ \hline
	\end{tabular}
	}
\caption{All attacks considered, total number of derived variants and their implication. (\newmoon = RRC, \fullmoon = NAS)}
\label{tab:all_attacks}
\end{table}
