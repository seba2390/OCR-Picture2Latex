\section{Recent Advances in Optimization}
\label{sec:advances}

Recent successes in using machine learning to deal with challenging perception
and natural language understanding problems have spurred many advances in the
study of optimization algorithms as optimization is a building block in machine
learning. 
These new developments include efficient methods for large-scale optimization, 
methods designed to handle non-convex problems, methods incorporating the structural
constraints, and finally the revival of second-order methods.
While these developments address a different set of applications in machine
learning, they address similar issues as encountered in inverse optimization and
could be useful.
We highlight some of the works below.
We keep the discussion brief because numerous works have been done behind these
developments and an indepth and comprehensive discussion is beyond the scope of
this review.
Our objective is thus to delineate the general trends and ideas, and provide
references for interested readers to dig on relevant topics.

\medskip\noindent{\bf Stochastic optimization.}
The development in large-scale optimization methods is driven by the
availability of many large datasets, which are made possible by the rapid
development and extensive use of IT technology. 
In machine learning, a model is generally built by optimizing a sum of misfit on
the examples.
This finite-sum structure naturally invites the application of stochastic optimization algorithms. This is mainly due to the fact that stochastic algorithms recover the sought-after models more efficiently by employing small batches of data in each iteration, as opposed to the whole data-set.
The most well-known stochastic gradient based algorithm is the stochastic gradient descent (SGD).
To minimize a finite-sum objective function 
\begin{align}
\label{eq:finite_sum}
g(\mm) = \frac{1}{n} \sum_{i=1}^{n} g_{i}(\mm),
\end{align} 
in the big data regime where $ n \gg 1 $, 
the vanilla SGD performs an update 
\begin{equation}
	\mm_{k+1} = \mm_{k} - \lambda_{k} \grad g_{i_{k}}(\mm_{k}),
\end{equation}
where $i_{k}$ is randomly sampled from $1, \ldots, n$.
As compared to gradient descent, SGD replaces the full gradient 
$\grad g(\mm)$ by a stochastic gradient $g_{i_{k}}(\mm_{k})$ with its
expectation being the full gradient.
The batch version of SGD constructs a stochastic gradient by taking the average
of several stochastic gradients.

SGD is inexpensive per iteration, but suffers from a slow rate of convergence.
For example, while full gradient descent achieves a linear convergence rate for
smooth strongly convex problems, SGD only converges at a sublinear rate.
The slow convergence rate can be partly accounted by the variance in the
stochastic gradient.
Recently, variance reduction techniques have been developed, e.g.
SVRG \cite{johnson2013accelerating} and SDCA \cite{shalev2013stochastic}.
Perhaps surprisingly, such variants can achieve linear convergence rates on
convex smooth problems as full gradient descent does, instead of sublinear rates
achieved by the vanilla SGD.
There are also a number of variants with no known linear rates but have fast
convergence rates for non-convex problems in practice, e.g.,
AdaGrad~\cite{duchi2011adaptive}, RMSProp~\cite{tijmen2012rmsprop},
ESGD~\cite{dauphin2015equilibrated}, Adam~\cite{kingma2014adam}, and
Adadelta~\cite{zeiler2012adadelta}.
Indeed, besides efficiency, stochastic optimization algorithms also seem to be able to
cope with the nonconvex objective functions well, and play a key role in the
revival of neural networks as deep learning~\cite{jin2017escape,ge2015escaping, levy2016power}.

\medskip\noindent{\bf Nonconvex optimization.}
There is also an increasing interest in non-convex optimization in the machine
learning community recently.
Nonconvex objectives not only naturally occur in deep learning, but also occur
in problems such as tensor decomposition, variable selection, low-rank matrix
completion, e.g. see \cite{ge2015escaping,mazumder2011sparsenet,jain2013low}
and references therein.

As discussed above, stochastic algorithms have been found to be capable of
effectively escaping local minima.
There are also a number of studies which adapt well-known acceleration
techniques for convex optimization to accelerate the convergence rates of both
stochastic and non-stochastic optimization algorithms for nonconvex problems,
e.g.,~\cite{li2015accelerated,allen2016variance,reddi2016stochastic,sutskever2013importance}.

\medskip\noindent{\bf Dealing with structural constraints.}
Many problems in machine learning come with complex structural constraints.
The Frank-Wolfe algorithm (a.k.a. conditional gradient)
\cite{frank1956algorithm} is an algorithm for optimizing over a convex domain.
It has gained a revived interest due to its ability to deal with many structural
constraints efficiently.
It requires solving a linear minimization problem over the feasible set, instead
of a quadratic program as in the case of proximal gradient algorithms or
projected gradient descent.
Domains suitable for the Frank-Wolfe algorithm include simplices,
$\ell_p$-balls, matrix nuclear norm ball, matrix operator norm ball
\cite{jaggi2013revisiting}.

The Frank-Wolfe algorithm belongs to the class of linear-optimization-based
algorithms \cite{lan2016conditional,lan2017conditional}.
These algorithms share with the Frank-Wolfe algorithm the characteristic of
requiring a first-order oracle for gradient computation and an oracle for
solving a linear optimization problem over the constraint set.

\medskip\noindent{\bf Second-order methods.}
The great appeal of the second-order methods lies mainly in the observed empirical performance as well as some very appealing theoretical properties. For example, it has been shown that stochastic Newton-type methods in general, and Gauss-Newton in particular, can not only be made scalable and have low per-iteration cost~\cite{rodoas1,rodoas2,roszas,haber2000optimization,haber2012effective,doas12}, but more importantly, and unlike first-order methods, are very \emph{resilient} to many adversarial effects such as \emph{ill-conditioning}~\cite{romassn1,romassn2,pyrrm_ssn_nonuni}. As a result, for moderately to very ill-conditioned problems, commonly found in scientific computing, while first-order methods make effectively no progress at all, second-order counterparts are not affected by the degree of ill-conditioning. 
A more subtle, yet potentially more severe draw-back in using first-order methods, is that their success is tightly intertwined with \emph{fine-tunning} (often many) \emph{hyper-parameters}, most importantly, the step-size~\cite{berahas2017investigation}. In fact, it is highly unlikely that many of these methods exhibit acceptable performance on first try, and it often takes many trials and errors before one can see reasonable results. In contrast, second-order optimization algorithms involve much less parameter tuning and are less sensitive to the choice of hyper-parameters~\cite{berahas2017investigation, xu2017second}. 

Since for the finite-sum problem~\eqref{eq:finite_sum} with $ n \gg 1 $, the operations with the Hessian/gradient constitute major computational bottlenecks, a rather more recent line of research is to construct the inexact Hessian information using the application of \emph{randomized methods}. Specifically, for convex optimization, the stochastic approximation of the full Hessian matrix in the classical Newton's method has been recently considered in~\cite{byrd2011use, byrd2012sample, wang2015subsampled,pilanci2015newton, erdogdu2015convergence, romassn1, romassn2, pyrrm_ssn_nonuni, Agarwal2016SecondOS, mutny2016stochastic, ye2016revisiting, bollapragada2016exact, mutny2017parallel, berahas2017investigation,eisen2017large}. In addition to inexact Hessian, a few of these methods study the fully stochastic case in which the gradient is also approximated, e.g.,~\cite{romassn1, romassn2,bollapragada2016exact}. For non-convex problems, however, the literature on methods that employ randomized Hessian approximation is significantly less developed than that of convex problems. A few recent examples include the stochastic trust region~\cite{xu2017newton}, stochastic cubic regularization~\cite{xu2017newton,tripuraneni2017stochastic}, and noisy negative curvature method~\cite{liu2017noisy}. Empirical performance of many of these methods for some non-convex machine learning applications has been considered in~\cite{xu2017second}.

