\section{Inverse Problems}
\label{sec:inv_prob}

An inverse problem can be seen as the reverse process of a forward problem,
which concerns with predicting the outcome of some measurements given a complete
description of a physical system.
Mathematically, a physical system is often specified using a set of model
parameters $\mm$ whose values completely characterize the system.
The model space $\modelspace$ is the set of possible values of $\mm$.
While $\mm$ is usually arise as a parameter function, in practice it is often
discretized as a parameter vector for the ease of computation, typically using
the finite element method, the finite volume method, or the finite difference
method.
The forward problem can be denoted as 
\begin{equation} \label{eq:forward_prob}
	\mm \to \dd = \ff(\mm),
\end{equation}
where $\dd$ are the error-free predictions, and the above notation is a
shorthand for 
$\dd 
= (\dd_1, \ldots, \dd_s) 
= (\ff_1(\mm), \ldots, \ff_s(\mm))$,
with $\dd_{i} \in \mathbb{R}^{l}$ being the $i$-th measurement.
The function $\ff$ represents the physical theory used for the prediction and is
called the forward operator.
%The forward operator can be linear or, more interestingly, non-linear in parameter $\mm$. 
The observed outcomes contain noises and relate to the system via the following
the observation equation
\begin{equation} \label{eq:obs}
	\dd = \ff(\mm) + \bm{\eta},
\end{equation}
where $\bm{\eta}$ are the noises occurred in the measurements.
The inverse problem aims to recover the model parameters $\mm$ from such noisy
measurements.

The inverse problem is almost always ill-posed, because the same measurements
can often be predicted by different models. 
There are two main approaches to deal with this issue.
The Bayesian approach assumes a prior distribution $P(\mm)$ on the model and a
conditional distribution $P(\bm{\eta} \mid \mm)$ on noise given the model.
The latter is equivalent to a conditional distribution $P(\dd \mid \mm)$ on
measurements given the model.
Given some measurements $\dd$, a posterior distribution 
$P(\mm \mid \dd)$ on the models is then computed using the Bayes rule
\begin{equation}
	P(\mm \mid \dd) 
		\propto P(\mm) P(\dd \mid \mm).
\end{equation}
Another approach sees the inverse problem as a data fitting problem that finds
an parameter vector $\mm$ that gives predictions $\ff(\mm)$ that best fit the
observed outcomes $\dd$ in some sense.
This is often cast as an optimization problem
\begin{equation} \label{eq:invopt}
	\min_{\mm \in \modelspace}\quad 
		\psi(\mm, \dd),
\end{equation}
where the misfit function $\psi$ measures how well the model $\mm$ fits the data
$\dd$.
When there is a probabilistic model of $\dd$ given $\mm$, a typical choice of
$\psi(\mm, \dd)$ is the negative log-likelihood. 
Regularization is often used to address the issue of multiple solutions, and 
additionally has the benefit of stabilizing the solution, that is, the
solution is less likely to change significantly in the presence of outliers
\cite{vogelbook,ehn1, archer1995some}. 
Regularization incorporats some \textit{a priori} information on $\mm$ in the
form of a regularizer $R(\mm)$ and solves the regularized optimization
problem
\begin{equation} \label{eq:reg_invopt1}
	\min_{\mm \in \modelspace}\quad 
		\psi_{R,\alpha}(\mm, \dd) 
			\defeq \psi(\mm, \dd) + \alpha R(\mm),
\end{equation}
where $\alpha > 0$ is a constant that controls the tradeoff between prior
knowledge and the fitness to data.
The regularizer $R(\mm)$ encodes a preference over the models, with preferred
models having smaller $R$ values.
The formulation in \Cref{eq:reg_invopt1} can often be given a \textit{maximum a
posteriori (MAP)} interpretation within the Bayesian framework~\cite{scharf1991statistical}. 
Implicit regularization also exists in which there is no explicit term $R(\mm)$
in the objective~\cite{hansen1998,doas, doas5,rieder2005,rieder2010,hanke1}.

The misfit function often has the form $\phi(\ff(\mm), \dd)$, which
measures the difference between the prediction $\ff(\mm)$ and the observation
$\dd$.
For example, $\phi$ may be chosen to be the Euclidean distance between
$\ff(\mm)$ and $\dd$.
In this case, the regularized problem takes the form
\begin{equation} \label{eq:reg_invopt2}
	\min_{\mm \in \modelspace}\quad 
		\phi_{R,\alpha}(\mm, \dd) 
			\defeq \phi(\ff(\mm), \dd) + \alpha R(\mm),
\end{equation}
This can also be equivalently formulated as choosing the most preferred model
satisfying constraints on its predictions
\begin{equation} \label{eq:reg_invopt3}
	\min_{\mm \in \modelspace}\quad R(\mm), \quad
	 \text{ s.t. }\quad \phi(\ff(\mm), \dd) \le \rho.
\end{equation}
The constant $\rho$ usually relates to noise and the maximum discrepancy between
the measured and the predicted data, and can be more intuitive than $\alpha$.

\subsection{PDE-Contrained Inverse Problems}
\label{sec:application}
For many inverse problems in science and engineering, the forward model is not
given explicitly via a forward operator $\ff(\mm)$, but often conveniently
specified via a set of partial differential equations (PDEs).
For such problems, \Cref{eq:reg_invopt2} has the form
\begin{equation} \label{eq:pde-invopt}
	\min_{\mm \in \modelspace, \uu}\quad 
		\phi(P \cdot \uu, \dd) + \alpha R(\mm), \quad
			\text{ s.t. }\quad c_{i}(\mm, \uu_{i}) = 0, \quad i=1, \ldots, s,
\end{equation}
where $P \cdot \uu 
= (P_1, \ldots, P_s) \cdot (\uu_1, \ldots, \uu_s)
= (P_1 \uu_1, \ldots, P_s \uu_s)$ with $\uu_{i}$ being the field in the $i$-th
experiment, $P_{i}$ being the projection operator that selects fields at
measurement locations in $\dd_{i}$ (that is, $P_{i} \uu_{i}$ are the predicted values
at locations measured in $\dd_{i}$), and $c_{i}(\mm, \uu_{i}) = 0$ corresponds to the
forward model in the $i$-th experiment.
In practice, the forward model can often be written as 
\begin{equation} \label{eq:lin-model} 
	{\cal L}_{i}(\mm) \uu_{i} = \qq_{i}, \quad i = 1, \ldots, s,
\end{equation}
where ${\cal L}_{i}(\mm)$ is a differential operator, and $\qq_{i}$ is a term that
incorporates source terms and boundary values.

The fields $\uu_1, \ldots, \uu_s$ in \Cref{eq:pde-invopt} and
\Cref{eq:lin-model} are generally functions in two or three dimensional spaces,
and finding closed-form solutions is usually not possible.
Instead, the PDE-constrained inverse problem is often solved numerically by
discretizing \Cref{eq:pde-invopt} and \Cref{eq:lin-model} using the finite
element method, the finite volume method, or the finite difference method.
Often the discretized PDE-constrained inverse problem takes the form
\begin{equation} \label{eq:discretized-constrained}
	\min_{\mm \in \modelspace, \uu}\quad 
		\phi(P \uu, \dd) + \alpha R(\mm), \quad
			\text{ s.t. }\quad L_{i}(\mm) \uu_{i} = \qq_{i}, \quad i=1, \ldots, s,
\end{equation}
where $P$ is a block-diagonal matrix consisting of diagonal blocks 
$P_{1}, \ldots, P_{s}$ representing the discretized projection operators,
$\uu$ is the concatenation of the vectors $\uu_1, \ldots, \uu_s$ representing
the discretized fields,
and each $L_{i}(\mm)$ is a square, non-singular matrix representing the
differential operator ${\cal L}_{i}(\mm)$.
Each $L_{i}(\mm)$ is typically large and sparse.
We abuse the notations $P$, $\uu$ to represent both functions and their
discretized versions, but the meanings of these notations will be clear from
context.

The constrained problem in \Cref{eq:discretized-constrained} can be written in
an unconstrained form by eliminating $\uu$ using $\uu_{i} = L_{i}^{-1} \qq_{i}$,
\begin{equation} \label{eq:discretized-unconstrained}
	\min_{\mm \in \modelspace}\quad 
		\phi(P L^{-1}(\mm) \qq, \dd) + \alpha R(\mm), 
\end{equation}
where $L$ is the block-diagonal matrix with $L_1, \ldots, L_s$ as the diagonal
blocks, and $\qq$ is the concatenation of $\qq_1, \ldots, \qq_s$. Note that, as in the case of~\eqref{eq:reg_invopt2}, here we have $ \ff(\mm) = P L^{-1}(\mm) \qq $.

Both the constrained and unconstrained formulations are used in practice.
The constrained formulation can be solved using the method of Lagrangian
multipliers.
This does not require explicitly solving the forward problem as in the
unconstrained formulation.
However, the problem size increases, and the problem becomes one of finding a
saddle point of the Lagrangian, instead of finding a minimum as in the
constrained formulation.

%\subsection{Assumptions on the Noise}
%\label{noise_assumption}
%The developments of the methods and algorithms presented in this thesis are done under one of the following assumptions on the noise. In what follows $\mathcal{N}$ denotes the normal distribution.
%\begin{enumerate}[label = ({N}.\arabic*)]
%	\item \label{noise_iid} 
%	The noise is independent and identically distributed (i.i.d) as 
%	$\bm{\eta}_{i} \sim \mathcal{N}(0,\Sigma ), \forall i$, where $\Sigma \in \mathbb{R}^{l \times l}$ is the 
%	symmetric positive definite covariance matrix. 
%	\item \label{noise_inid} 
%	The noise is independent but \textit{not} necessarily identically distributed,
%	satisfying instead $\bm{\eta}_{i} \sim \mathcal{N}(0,\sigma^{2}_{i} \mathbb{I}), i = 1,2,\ldots,s$, where
%	$\sigma_{i} > 0$ are the standard deviations. 
%\end{enumerate}
%
%Henceforth, for notational simplicity, most of the algorithms and methods are presented for the special case of Assumption~\hyperref[noise_iid]{(N.1)} with $\Sigma = \sigma \mathbb{I}$. However, all of these methods and algorithms can be readily extended to the more general cases in a completely straightforward manner.


\subsection{Image Reconstruction}
\label{sec:image_reconstruction}
Image reconstruction studies the creation of 2-D and 3-D images from sets
of 1-D projections. 
The 1-D projections are generally line integrals of a function representing the
image to be reconstructed.
In the 2-D case, given an image function $f(x, y)$, the integral along the line
at a distance of $s$ away from the origin and having a normal which forms an
angle $\phi$ with the $x$-axis is given by the Randon transform
\begin{equation}
	p(s, \phi) 
		= \int_{-\infty}^{\infty} f(z \sin \phi + s \cos \phi,
			-z \cos \phi + s \sin \phi) dz.
\end{equation}

Reconstruction is often done via back projection, filtered back projection, or
iterative methods \cite{natterer2001mathematical,herman2009fundamentals}.
Back projection is the simplest but often results in a blurred reconstruction.
Filtered back projection (FBP) is the analytical inversion of the Radon transform and
generally yields reconstructions of much better quality than back projection.
However, FBP may be infeasible in the presence of discontinuities or noise.
Iterative methods generally takes the noise into account and assumes a
distribution on the noise.
The objective function is often chosen to be a regularized likelihood of the
observation, which is then iteratively optimized using the expectation
maximization (EM) algorithm.

\subsection{Objective Function}
\label{sec:obj}
One of the most commonly used objective function is the least squares criterion,
which uses a quadratic loss and a quadratic regularizer.
Assume that the noise for each experiment in~\eqref{eq:obs} is independently but
normally distributed, i.e.,
$\bm{\eta}_{i} \sim \mathcal{N}(0,\Sigma_{i} ), \forall i$, where $\Sigma_{i}
\in \mathbb{R}^{l \times l}$ is the covariance matrix.
Let $\Sigma$ be the block-diagonal matrix with $\Sigma_{1}, \ldots, \Sigma_{s}$
as the diagonal blocks.
The standard \textit{maximum likelihood} (ML) approach~\cite{scharf1991statistical}, leads
to minimizing the least squares (LS) misfit function 
\begin{equation} \label{eq:l2}
	\phi(\mm) \defeq \|\ff(\mm) - \dd\|_{\Sigma^{-1}}^2,
\end{equation}
where the norm $\|x\|_{A} = \sqrt{x^{\top} A x}$ is a generalization of the Euclidean
norm (assuming the matrix $A$ is positive definite, which is true in the case of
$\Sigma_{i}^{-1}$). In the above equation, we simply write the general misfit function
$\phi(\ff(\mm), \dd)$ as $\phi(\mm)$ by taking the measurements $\dd$ as fixed
and omitting it from the notation.
As previously discussed, we often minimize a regularized misfit function
\begin{equation} \label{eq:reg_l2}
	\phi_{R,\alpha}(\mm) \defeq  \phi(\mm) + \alpha R(\mm).
\end{equation}
The prior $R(\mm)$ is often chosen as a Gaussian regularizer
$R(\mm) 
= (\mm - \mm_{\text{prior}})^{\top} \Sigma_{m}^{-1} (\mm - \mm_{\text{prior}})$.
We can also write the above optimization problem as minimizing $R(\mm)$ under
the constraints
\begin{equation} \label{eq:objective_eq}
	\sum_{i=1}^{s} \|\ff_{i}(\mm) - \dd_{i}\| \leq \rho.
\end{equation}

The least-squares criterion belongs to the class of $\ell_p$-norm
criteria, which contain two other commonly used criteria:
the least-absolute-values criterion and the minimax criterion
\cite{tarantola2005inverse}.
These correspond to the use of the $\ell_{1}$-norm and the $\ell_{\infty}$-norm
for the misfit function, while the least squares criterion uses the
$\ell_{2}$-norm.
Specifically, the least-absolute-values criterion takes 
$\phi(\mm) \defeq \|\ff(\mm) - \dd\|_1$, 
and the minimax criterion takes
$\phi(\mm) \defeq \|\ff(\mm) - \dd\|_{\infty}$.
More generally, each coordinate in the difference may be weighted.
The $\ell_{1}$ solution is more robust (that is, less sensitive to outliers) than the
$\ell_{2}$ solution, which is in turn more robust than the $\ell_{\infty}$ solution
\cite{claerbout1973robust}.
The $\ell_{\infty}$ norm is desirable when outliers are uncommon but the data
are corrupted by uniform noise such as the quantization errors
\cite{clason2012fitting}.

Besides the $\ell_{2}$ regularizer discussed above, the $\ell_{1}$-norm is
often used too.
The $\ell_{1}$ regularizer induces sparsity in the model parameters, that is,
heavier $\ell_{1}$ regularization leads to fewer non-zero model parameters.

\subsection{Optimization Algorithms}
\label{sec:optimization}

Various optimization techniques can be used to solve the regularized data
fitting problem.
We focus on iterative algorithms for nonlinear optimization below as the
objective functions are generally nonlinear.
In some cases, the optimization problem can be transformed to a linear program.
For example, linear programming can be used to solve the least-absolute-values
criterion or the minimax criterion.
However, linear programming are considered to have no advantage over
gradient-based methods (see Section 4.4.2 in \cite{tarantola2005inverse}), and
thus we do not discuss such methods here.
Nevertheless, there are still many optimization algorithms that can be covered
here, and we refer the readers to
\cite{bjorck1996numerical,nocedal2006numerical}.

For simplicity of presentation, we consider the problem of minimizing a
function $g(\mm)$.
We consider iterative algorithms which start with an iterate $\mm_0$, and
compute new iterates using
\begin{equation}
	\mm_{k+1} = \mm_{k} + \lambda_{k} p_{k},
\end{equation}
where $p_{k}$ is a search direction, and $\lambda_{k}$ a step size.
Unless otherwise stated, we focus on unconstrained optimization.
These algorithms can be used to directly solve the inverse problem in
\Cref{eq:reg_invopt1}. 
We only present a selected subset of the algorithms available and have to omit
many other interesting algorithms. 
% For example, we do not discuss optimization on level set \cite{doas1,doas5,of}.

\medskip\noindent{\bf Newton-type methods.}
The classical Newton's method starts with an initial iterate $\mm_{0}$, and computes new
iterates using 
\begin{equation}
	\mm_{k+1} = \mm_{k} - \left(\grad^2 g(\mm_{k})\right)^{-1} \grad g(\mm_{k}),
\end{equation}
that is, the search direction is 
$p_{k} = - \left(\grad^2 g(\mm_{k})\right)^{-1} \grad g(\mm_{k})$, and the step
length is $\lambda_{k} = 1$.
The basic Newton's method has quadratic local convergence rate at a small
neighborhood of a local minimum.
However, computing the search direction $p_{k}$ can be very expensive, and thus
many variants have been developed. In addition, in non-convex problems, classical Newton direction might not exist (if the Hessian matrix is not invertible) or it might not be an appropriate direction for descent (if the Hessian matrix is not positive definite). 

For non-linear least squares problems, where the objective function $g(\mm)$ is
a sum of squares of nonlinear functions, the Gauss-Newton (GN) method is often
used~\cite{sun2006optimization}.
Extensions to more general objective functions as in \Cref{eq:l2} with covariance matrix $ \Sigma $ and arbitrary regularization as in \Cref{eq:reg_l2} is considered in~\cite{roszas}.
Without loss of generality, assume 
$g(\mm) = \sum_{i=1}^{s} (\ff_{i}(\mm) - \dd_{i})^{2}$.
At iteration $k$, the GN search direction $p_{k}$ is given by
\begin{equation} 
	\left( \sum_{i=1}^{s} {J}_{i}^{\top} {J}_{i} \right) p_{k}  = -\grad g,
\end{equation}
where the sensitivity matrix $J_i$ and the gradient $\grad g$ are given by
\begin{align}
	J_i &= \frac{\partial \ff_{i}}{\partial \mm}(\mm_{k}), \quad i = 1, \ldots , s,\\
	\grad g &= 2 \sum_{i=1}^s J_{i}^T(\ff_{i}(\mm_{k}) - \dd_{i}),
\end{align}
The Gauss-Newton method can be seen as an approximation of the basic Newton's
method obtained by replacing $\grad^2 g$ by $\sum_{i=1}^{s} J_i^{\top} J_{i}$.
The step length $\lambda_{k} \in [0, 1]$ can be determined by a weak line
search~\cite{nocedal2006numerical} (using, say, the Armijo algorithm starting
with $\lambda_{k} = 1$) ensuring sufficient decrease in $g(\mm_{k+1})$ as
compared to $g(\mm_{k})$.

Often several nontrivial modifications are required to adapt this prototype method
for different applications, e.g., dynamic regularization~\cite{doas1,hanke1,rieder2005,rieder2010} and more general~\emph{stabilized GN} studied~\cite{rodoas1,doas12}.
This method replaces the solution of the linear systems defining $p_{k}$ by $r$
preconditioned conjugate gradient (PCG) inner iterations, which costs $2r$
solutions of the forward problem per iteration, for a moderate integer value $r$. 
Thus, if $K$ outer iterations are required to obtain an acceptable solution then
the total work estimate (in terms of the number of PDE solves) is approximated
{\em from below} by $2(r+1) K s$. 

Though Gauss-Newton is arguable the method of choice within the inverse problem community, other Newton-type methods exist which have been designed to suitably deal with the non-convex nature of the underlying optimization problem include Trust Region~\cite{conn2000trust,xu2017newton} and the Cubic Regularization~\cite{xu2017newton,cartis2012evaluation}. These methods have recently found applications in machine learning~\cite{xu2017second}. Studying the advantages/disadvantages of these non-convex methods for solving inverse problems can be indeed a useful undertaking.

\medskip\noindent{\bf Quasi-Newton methods.}
An alternative method to the above Newton-type methods is the quasi-Newton
variants including the celebrated limited memory BFGS (L-BFGS)
\cite{liu1989limited,nocedal1980updating}.
BFGS iteration is closely related to conjugate gradient (CG) iteration. In particular, BFGS applied to a strongly convex 
quadratic objective, with exact line search as well as initial Hessian $ P $, is equivalent to preconditioned CG with preconditioner $ P $. However, as the objective function departs from being a simple quadratic, the number of iterations of L-BFGS could be significantly higher than that of GN or trust region. In addition, it has been shown that the performance of BFGS and its limited memory version is greatly negatively affected by the high degree if ill-conditioning present in such problems~\cite{romassn1, romassn2,pyrrm_ssn_nonuni}. These two factor are among the main reasons why BFGS (and L-BFGS) can be less effective compared with other Newton-type alternatives in many inversion applications~\cite{haber2004quasi}.

\medskip\noindent{\bf Krylov subspace method.}
A Krylov subspace method iteratively finds the optimal solution to an
optimization in a larger subspace by making use of the previous solution in a
smaller subspace.
One of the most commonly used Krylov subspace method is the conjugate gradient
(CG) method.
CG was originally designed to solve convex quadratic minimization problems of
the form $g(\mm) = \frac{1}{2} \mm^{\top} A \mm - b^{\top} \mm$.
Equivalently, this solves the positive definite linear system $A\mm = b$.
It computes a sequence of iterates $\mm_0, \mm_1, \ldots$ converging to the minimum
through the following two set of equations.
\begin{align}
	\mm_0 &=0, & r_0 &= b, & p_0 &= r_0, & \\
  \mm_{k+1} &= \mm_{k} + \frac{||r_k||_2^2}{p_{k}^{\top} A p_{k}} p_{k}, 
    &
  r_{k+1} &= r_{k} - \frac{||r_k||_2^2}{p_{k}^{\top} A p_{k}} A p_k,
    &
	p_{k+1} &= r_{k+1} + \frac{||r_{k+1}||_2^2}{||r_{k}||_2^2} p_{k}, & k \ge 0.
\end{align}
This can be used to solve the forward problem of the form 
$L_{i}(\mm) \uu_{i} = \qq_{i}$, provided that $L_{i}(\mm)$ is positive definite, which
is true in many cases.

CG can be used to solve the linear system for the basic Newton direction.
However, the Hessian is not necessarily positive definite and modification is
needed \cite{nocedal2006numerical}.

In general, CG can be generalized to minimize a nonlinear function $g(\mm)$ 
\cite{fletcher2013practical,dai2011nonlinear}.
It starts with an arbitrary $\mm_0$, and $p_1 = - \grad g(\mm_0)$, and computes
a sequence of iterates $\mm_1, \mm_2, \ldots$ using the equations below:
for $k \ge 0$,
\begin{align}
	\mm_{k+1} 
		&= \arg\min_{\mm \in \{\mm_k + \lambda p_k, \lambda \in \mathbb{R}\}} g(\mm), \\
	p_{k+1}
		&= -\grad g(\mm_{k+1}) + \beta_{k} p_k, 
		\qquad\text{ where }
		\beta_k = \frac{||\grad g(\mm_{k+1})||_2^2}{||\grad g(\mm_{k})||_2^2}.
\end{align}
The above formula for $\beta_k$ is known as the Fletcher-Reeves formula.
Other choices of $\beta_k$ exist.
The following two formula are known as the Polak-Ribiere and Hestenes-Stiefel
formula respectively.
\begin{align}
    \beta_{k}
		&= \frac{\langle \grad g(\mm_{k+1}) - \grad g(\mm_{k}),  \grad g(\mm_{k+1}) \rangle}
        {||\grad g(\mm_{k})||_2^2}, \\
    \beta_{k}
		&= \frac{\langle \grad g(\mm_{k+1}) - \grad g(\mm_{k}),  \grad g(\mm_{k+1}) \rangle}
        {p_{k}^{\top} (\grad g(\mm_{k+1}) - \grad g(\mm_{k}))}.
\end{align}
In practice, nonlinear CG does not seem to work well, and is mainly used
together with other methods, such as in the Newton CG method
\cite{nocedal2006numerical}.

\medskip\noindent{\bf Lagrangian method of multipliers.}
The above discussion focuses on unconstrained optimization algorithms, which are
suitable for unconstrained formulations of inverse problems, or unconstrained
auxiliary optimization problems in methods which solves the constrained
formulations directly.
The Lagrangian method of multipliers is often used to directly solve the
constrained version.
Algorithms have been developed to offset the heavier computational cost and
slow convergence rates of standard algorthms observed on the Lagrangian, which
is a larger problem than the constrained problem.
For example, such algorithm may reduce the problem to a smaller one, such as
working with the reduced Hessian of the Lagrangian~\cite{haber2000optimization},
or preconditioning~\cite{haber2001preconditioned,benzi2011preconditioning}.
These methods have shown some success in certain PDE-constrained optimization
problems.

Augmented Lagrangian methods have also been developed
(e.g.~\cite{ito1990augmented,abdoulaev2005optical}).
These methods constructs a series of penalized Lagrangians with vanishing
penalty, and finds an optimizer of the Lagrangian by successively optimizing the
penalized Lagrangians.

\subsection{Challenges}
\label{sec:challenges}

\noindent{\bf Scaling up to large problems.}
The discretized version of an inverse problem is usually of very large scale,
and working with fine resolution or discretized problems in high dimension is
still an active area of research.

Another challenge is to scale up to large number of measurements, which is
widely believed to be helpful for quality reconstruction of the model in
practice, with some theoretical support.
While recent technological advances makes many big datasets available, existing
algorithms cannot efficiently cope with such datasets.
Examples of such problems include electromagnetic data inversion in mining
exploration \cite{na,dmr,haasol,olhash}, seismic data inversion in oil
exploration \cite{fichtner,hel,rnkkda}, diffuse optical tomography (DOT)
\cite{arridge1999optical,boas}, quantitative photo-acoustic tomography (QPAT)
\cite{gaooscher,yuan}, direct current (DC) resistivity
\cite{smvoz,pihakn,haheas,HaberChungHermann2010,doas12}, 
and electrical impedance tomography (EIT) \cite{bbp,cin,van2013lost}.

It has been suggested that many well-placed experiments yield practical advantage in order to obtain reconstructions of acceptable quality. 
For the special case where the measurement locations as well as the discretization matrices do not change from one experiment to another, various approximation techniques have been proposed to reduce the effective number of measurements, which in turn implies a smaller scale optimization problem, under the unifying category of ``simultaneous sources inversion'' \cite{rodoas1,roszas,roosta2015randomized,haber2014simultaneous,kumar2014GEOPemc}. Under certain circumstances, even if the $P_{i}$'s are different across experiments (but $ L_{i} $'s are fixed), there are methods to transform the existing data set 
into the one where all sources share the same receivers, \cite{rodoas2}. 

\medskip\noindent{\bf Dealing with non-convexity.}
Another major source of difficulty in solving many inverse problems, is the high-degree of non-linearity and non-convexity in~\eqref{eq:forward_prob}. This is most often encountered in problems involving PDE-constrained optimization where each $ \ff_{i} $ corresponds to the solution of a PDE. Even if the output of the PDE model itself, i.e., the ``right-hand side'', is linear in the sought-after parameter, the solution of the PDE, i.e., the forward problem, shows a great deal of non-linearity. This coupled with a great amount of non-convexity can have significant consequences in the quality of inversion and the obtained parameter. Indeed, in presence of non-convexity, the large-scale computational challenges are exacerbated, multiple folds over, by the difficulty of avoiding (possibly degenerate) \emph{saddle-points} as well as finding (at least) a \emph{local minimum}. 

\medskip\noindent{\bf Dealing with discontinuity.}
While the parameter function of the model is often smooth, the parameter
function can be discontinuous in some cases.
Such discontinuities arise very naturally as a result of the physical properties
of the underlying physical system, e.g., EIT and DC resistivity, and require
non-trivial modifications to optimization algorithms, e.g.,~\cite{rodoas1,
doas12}. 
Ignoring such discontinuities can lead to unsatisfactory recovery results
\cite{tali,doas12,doasleit2010}.
The level set method \cite{osse} is often used to model discontinuous parameter
function.
This reparametrizes the discontinuous parameter function as a differentiable
one, and thus enabling more stable optimization \cite{doasleit2010}.
