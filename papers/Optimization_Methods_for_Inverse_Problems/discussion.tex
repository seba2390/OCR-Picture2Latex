\section{Discussion}
\label{sec:discussion}

Optimization is not only used in the data fitting approach to inverse problems,
but also used in the Bayesian approach.
An important problem in the Bayesian approach is the choice of the
parameters for the prior.
While these were often chosen in a somewhat ad hoc way, there are studies which
use sampling \cite{fox2016fast,agapiou2014analysis},
hierarchical prior models \cite{calvetti2007gaussian,calvetti2008hypermodels}, 
and optimization \cite{bardsley2010hierarchical,liu2017approximate} methods to
choose the parameters.
While choosing the prior parameters through optimization has found some success,
such optimization is hard and it remains a challenge to develop effective
algorithms to solve these problems.

For inverse problems with large number of measurements, solving each forward
problem can be expensive, and the mere evaluation of the misfit function may
become computationally prohibitive. 
Stochastic optimization algorithms might be beneficial in this case, because the
objective function is often a sum of misfits over different measurements.

The data fitting problem is generally non-convex and thus optimization
algorithms may be trapped in a local optimum.
Stochastic optimization algorithms also provide a means to escape the local
optima. 
%There are also problems which involve the optimization of an expectation
%(experimental design?), and thus are also naturally solved using stochastic
%optimization algorithms \cite{}.
Recent results in nonconvex optimization, such as those on accelerated methods,
may provide more efficient alternatives to solve the data fitting problem.

While box constraints are often used in inverse problems because they are easier to
deal with, simplex constraint can be beneficial. 
The Frank-Wolfe algorithm provides a efficient way to deal with the simplex
constraint, and can be a useful tool to add on to the toolbox of an inverse
problem researcher.

