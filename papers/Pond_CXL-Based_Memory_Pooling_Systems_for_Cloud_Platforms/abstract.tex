
% THIS IS THE OLD ABSTRACT FOR OSDI'22
\def \myabstract{%
Using data from a public cloud provider, we find that up to 25\%
of memory is \emph{stranded}, \ie, it is leftover after the servers'
cores have been rented to VMs.  Memory disaggregation promises
to reduce this stranding.  However, making
disaggregation practical for production cloud deployment remains
challenging.  For example, RDMA-based disaggregation involves too much
overhead for latency-sensitive workloads and transparent latency
management is incompatible with
virtualization acceleration.  The emerging Compute Express Link (CXL)
standard offers a low-overhead substrate upon which we can build
memory disaggregation while overcoming these challenges.  Thus, in
this paper, we propose \sys, a full-stack CXL-based disaggregation
system that meets the requirements of cloud providers.  \sys includes
a memory pool controller, and prediction-based system software and
distributed control plane designs.  Its predictions of VM latency
sensitivity and memory usage allow it to nearly perfectly split
workloads across local and pooled memory, mitigating the higher pool
latency.

Our analysis of production clusters shows that small pools of 8-16
sockets are sufficient to reduce stranding significantly.  It also
shows that $\sim$50\% of all VMs never touch 50\% of their rented
memory.  From emulated experiments with 150+ workloads and \sys's
memory split, we show that pooling incurs a configurable performance
loss between 1-5\%.  Finally, we show that \sys can achieve a 9-10\% reduction in
overall DRAM, which represents hundreds of millions of dollars
in cost savings for a large cloud provider.
%
}

%-----------------------------------------------------------------------
% First abstract for ASPLOS'23
%-----------------------------------------------------------------------
\def \myabstract{%
%
Cloud providers seek to achieve stringent performance requirements and low cost.
DRAM has emerged as a driver of cost and memory pooling promises to improve DRAM utilization.
However, pooling is challenging under cloud performance requirements.
%
This paper proposes \sys, the first memory pooling system that achieves cloud performance goals and effectively reduces DRAM cost.
\sys builds on the emerging Compute Express Link (CXL) interface and two insights.
First, our analysis shows that grouping 4-16 dual-socket (or 8-32 single-socket) servers is enough to achieve most of the benefits of pooling.
This enables a small-pool design with low access latency.
Second, \sys introduces novel machine learning models that can effectively predict how much NUMA-local and pool memory to allocate to a VM to resemble NUMA-local memory performance.
%
%Our analysis of production clusters shows that small pools of 8-32
%sockets are sufficient to reduce stranding significantly.  It also
%shows that $\sim$50\% of all VMs never touch 50\% of their rented
%memory.
Our evaluation with 150+ workloads shows that \sys only incurs 1-5\%
configurable performance loss while improving DRAM efficiency by
9-10\%.
%which represents hundreds of millions of dollars in cost savings
%for a large cloud provider.
%
}


%-----------------------------------------------------------------------
% Second abstract for ASPLOS'23
%-----------------------------------------------------------------------
\def \myabstract{%
%
Public cloud providers seek to meet stringent performance requirements and low hardware cost.
A key driver of performance and cost is main memory.
Memory pooling promises to improve DRAM utilization and thereby reduce costs.
However, pooling is challenging under cloud performance requirements.
%
This paper proposes \sys, the first memory pooling system that both meets cloud performance goals and significantly reduces DRAM cost.
\sys builds on the Compute Express Link (CXL) standard for load/store access to pool memory and two key insights.
First, our analysis of cloud production traces shows that pooling across 8-16 sockets is enough to achieve most of the benefits.
This enables a small-pool design with low access latency.
Second, it is possible to create machine learning models that can accurately predict how much local and pool memory to allocate to a virtual machine (VM) to resemble same-NUMA-node memory performance.
%
%Our analysis of production clusters shows that small pools of 8-32
%sockets are sufficient to reduce stranding significantly.  It also
%shows that $\sim$50\% of all VMs never touch 50\% of their rented
%memory.
Our evaluation with 158 workloads shows that \sys reduces DRAM costs by
7\% with performance within 1-5\% of same-NUMA-node VM allocations.
%which represents hundreds of millions of dollars in cost savings
%for a large cloud provider.
%
}


\begin{abstract}
%\vspace*{4pt}
\ni\textit{\myabstract}
%\vspace*{4pt}
\end{abstract}


% text previously commented out

% Combined, \sys represents a feasible way to achieve memory disaggregation in cloud platforms and reduce overall memory spend.
% \sys exposes the
% pool allocation for a VM as a ``zero-core'' virtual NUMA (\cvn) node to
% the VM's guest OS, guiding the OS in its own allocation decisions.
%% Since guest OSes prioritize allocating local memory, \cvn retains high
%% performance despite the pool pages.
%
% We motivate our design with the observation of two issues: \emph{memory stranding}, \ie memory that is leftover on a node after cores fully rented to VMs; and \emph{frigid memory},
%\ie, memory that is rented but then never touched (colder than cold memory).
%We provide the first public characterization of both phenomena in a commercial public cloud platform and show (1) production clusters can experience up to 25\% stranding,
%and (2) 50\% of VMs have more than 20\% frigid memory. We build a prediction model for frigid memory, and show that it can predict 20\% frigid memory with fewer than 5\% overpredictions.





%% DRAM makes up 50\% of server cost for large cloud providers like AWS, Azure, or GCP.
%% We find that up to 25\% of memory is \emph{stranded}, \ie is leftover on servers after their cores are fully rented to VMs.
%% Memory disaggregation is a promising direction for reducing cloud provider memory stranding.
%% However, making disaggregation practical for production deployment remains challenging, e.g., due to latency sensitive workloads and because existing latency mitigation techniques are not applicable.
%
%% We present an analysis of cloud cluster workloads that shows that small pools of 8-32 sockets are sufficient to reduce stranding.
%% We propose \sys, a memory pooling system that scales to the requirements of large cloud providers.
%% \sys includes a pool design, a new software stack and distributed control plane that effectively mitigate the latency impacts of pool memory.
%% Overall, \sys reduces DRAM requirements by 9\% across 100 clusters without affecting the performance for 98\% of all VMs.
