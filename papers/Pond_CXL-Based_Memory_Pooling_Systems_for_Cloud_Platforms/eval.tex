\section{Evaluation}
\label{sec-eval}

% to evaluate the performance of \sys design and
Our evaluation addresses the performance of \cvn VMs (\sec\ref{sec:eval:accessbit}, \sec\ref{sec-eval-split}), the accuracy of \sys's prediction models (\sec\ref{sec:eval:models}), and \sys's end-to-end DRAM savings (\sec\ref{eval:e2e}).


\begin{comment}
\begin{enumerate}[leftmargin=9pt]

\item[-] What is the performance of \cvn VMs?
  (\sec\ref{sec:eval:accessbit}, \sec\ref{sec-eval-split})

    \item[-] How accurate are \sys's prediction models?
        (\sec\ref{sec:eval:models})

    \item[-] What are \sys's end-to-end DRAM savings?
        (\sec\ref{eval:e2e})

    %\item[-] How does \sys compare to other hardware based
    %    disaggregation designs? (\sec\ref{sec-eval-vs}) \hcl{@Daniel,
    %    TODO}

\end{enumerate}
\end{comment}

\subsection{Experimental Setup}
\label{sec-eval-setup}

We evaluate the performance of our prototype using \numTotalApps\ cloud workloads.
Specifically, our workloads span in-memory databases and KV-stores (Redis~\cite{redis.web21}, VoltDB~\cite{voltdb.web21}, and TPC-H on MySQL~\cite{tpch.web21}), data and graph processing (Spark~\cite{hibench.web21} and GAPBS~\cite{gapbs.corr15}), HPC (SPLASH2x~\cite{parsec3.can16}), CPU and shared-memory benchmarks (SPEC CPU~\cite{speccpu2017.web21} and PARSEC~\cite{parsec.pact08}), and a range of \azure's internal workloads (Proprietary).
Figure~\ref{fig-cxl} overviews these workloads.
We quantify DRAM savings with simulations.

\myparagraph{Prototype setup} We run experiments on production servers at \azure and similarly-configured lab servers.
The production servers use either two Intel Skylake 8157M sockets with each 384GB of DDR4, or two AMD EPYC 7452 sockets with each 512GB of DDR4.
%and on CloudLab \ts{c220g5} nodes~\cite{cloudlabhw.web21}.
On Intel, we measure 78ns NUMA-local latency and 80GB/s bandwidth and 142ns remote latency and 30GB/s bandwidth (3/4 of a CXL \tms{8} link).
On AMD, we measure 115ns NUMA-local latency and 255ns remote latency.
Our BIOS disables hyper-threading, turbo-boost, and C-states.

We use performance results of VMs entirely backed by NUMA-local DRAM as our \emph{baseline}.
We present \cvn performance as
normalized slowdowns, \ie, the ratio to the baseline. Performance metrics are workload specific, \eg, job runtime, throughput and tail latency, \etc

Each experiment involves running the application with one of 7 \cvn sizes (as percentages of the workload's memory footprint in Figure \ref{fig-split-cdf}).
With at least three repetitions of each run and \numTotalApps\ workloads, our evaluation spans more than 3,500 experiments and 10,000 machine hours.
Most experiments used lab servers; we spot check outliers on production servers.

\myparagraph{Simulations}
Our simulations are based on traces of production VM requests and their placement on servers.
The traces are from randomly selected 100 clusters across 34 datacenters globally over 75 days.

The simulator implements different memory allocation policies and tracks each server and each pool's memory capacity at second accuracy.
Generally, the simulator schedules VMs on the same nodes as in the trace and changes their memory allocation to match the policy.
For rare cases where a VM does not fit on a server, \eg, due to insufficient pool memory, the simulator moves the VMs to another server.

\myparagraph{Model evaluation}
We evaluate our model with production resource logs.
About 80\% of VMs have sufficient history to make a sensitivity prediction.
Our deployment does not report each workload's perceived performance (opaque VMs).
We thus evaluate latency sensitivity model based on our \numTotalApps\ workloads.


%\subsection{\sys Performance Results}

\begin{comment}
\myparagraph{Finding 4} Workload slowdown is independent from working
set size (WSS) (Figure \ref{fig-cxl-cdf}c and \ref{fig-cxl-cdf}a pattern
differs). The WSS of $\sim$45\% of the workloads is less than 1GB.
Among those workloads, some take hard hits under CXL. For example, the
\ts{519.lbm\_r}'s WSS is only $\sim$400MB but its performance degrades
by 35\%. Similarly, we observe $\sim$31\% of the workloads' WSS is over
15GB, but many do not see significant performance impacts, such as
\ts{602.gcc\_s, tc\_kron, voltdb}, \etc

\myparagraph{Implications} WSS does not work as a predictor for
CXL-caused slowdown.  Since even small workloads are affected, there is
some hope that future research can show the effectiveness of
hardware-managed on-chip DRAM or HBM caches.


\myfinding{Finding 4}
%\note{I copied CPI stuff here (at least in the table they go together).
%Maybe they %should be separate?}
We find that nominal Cycles per Instruction (CPI) of workloads is not a predictor of pooled memory slowdown. CPI measures the average number of clock cycles
needed for one instruction and can represent the performance of CPUs.
Figure \ref{fig-cxl-cdf}d shows the CDF of ``CPI increase''
which is calculated in a way similar to the performance slowdown.
Interestingly, we observe an almost perfect match between the two CDF
lines, signifying the impact of CXL memory to the overall CPU
performance.

\myparagraph{Implications} While CPI degradation mirrors slowdowns, it is a posterior ``predictor'' and cannot be used without
running the workloads under pool memory beforehand. Thus, cloud
providers need better prediction methods (\sec\ref{sec-eval-pred}).

\myfinding{Finding 5} We find that more workloads become memory sensitive when using CXL memory due to latency constraints.
Under local DRAM, 27\% of the workloads' DRAM
bandwidth is $<$500MB/s, and 37\% <1GB/s. We also find memory intensive
workloads with 20\% exceeding 20 GB/s and the peak workload bandwidth
around 42GB/s (\ts{619.lbm\_s}).  Figure~\ref{fig-cxl-cdf}b shows the
CDF of bandwidth when using local DRAM and when emulating CXL.

Under CXL, observed bandwidth usage is significantly lower.  This is
mostly due to memory accesses being issued at a lower rate as the CPU
core waits due to higher latencies. We verified this observation using
hardware counters.
% to show that, on average \xxx\% more cycles are classified as DRAM
%latency bound when emulating CXL compared to when using local DRAM.  For
%a small fraction (\xxx\% of workloads), capping also happens due to our
%emulation testbed only offering 30GB/s of bandwidth.  The peak bandwidth
%usage is only \xxx\%.

\myparagraph{Implications} The maximum bandwidth usage of a single
workload (42GB/s) validates \sys's design decision to not interleave VMs
across CXL links.  An individual CXL \tms{8} link (40GB/s) appears to be
sufficient for a single VM.  However, multiple VMs can easily exceed the
bandwidth of a CXL \tms{8} link, which validates \sys's design to
connect multiple EMCs to each CPU socket.
%\note{Could have a few more points here on DRAM boundedness as this is
%interesting}
\end{comment}

\input{fig-accessbit}

\subsection{\cvn VMs on Production Nodes}
\label{sec:eval:accessbit}

We perform a small-scale experiment on \azure production nodes to validate \cvn VMs.
The experiment evaluates four internal workloads:
an audio/video conferencing application,
a database service, a key-value store, and a business analytics service.
To see the effectiveness of \cvn, we assume a correct prediction of untouched memory, \ie, the local footprint fits into the VM's local vNUMA node.
Figure~\ref{fig-accessbit} shows access bit scans over 48 hours from the video workload and a table that shows the traffic to the \cvn node for the four workloads.

\myfinding{}
We find that \cvn nodes are effective at containing the memory access to the local vNUMA node.
A small fraction of accesses goes to the \cvn node.
We suspect that this is in part due to the guest OS memory manager's metadata that is explicitly allocated on each vNUMA node.
We find that the video workload sends fewer than 0.25\% of memory accesses to the \cvn node.
Similarly, the other three workloads send 0.06-0.38\% of memory access to the \cvn node.
Accesses within the local vNUMA node are spread out

\myparagraph{Implications}
With a negligible fraction of memory accesses on \cvn, we expect negligible performance impact given a correct prediction of untouched memory.


\subsection{\cvn VMs in the Lab}
%
\label{sec-eval-split}


% HCL: CDF of split-slowdown and interleave-cdf (together)
\input{fig-split-cdf}
%\input{fig-split}

We scale up our evaluation to \numTotalApps\ workloads in a lab setting.
Since we fully control these workloads, we can now also explicitly measure their performance.
We rerun each workload on all-local memory, a correctly sized \cvn (0\% spilled), differently-sized \cvn nodes sized between 10-100\% of the workload's footprint.
Figure~\ref{fig-split-cdf} shows a violin plot of associated slowdowns.
This setup covers both normal behavior (all-local and 0\% spill) and misprediction behavior for latency sensitive workloads.
Thus, this is effectively a sensitivity study.

\myfinding{}
%
With a correct prediction of untouched memory, workload slowdowns have a similar distribution to all-local memory.

\myparagraph{Implications}
This performance result is expected since the \cvn node is rarely accessed (\sec\ref{sec:eval:accessbit}).
Our evaluation can thus assume no performance impact under correct predictions of untouched memory (\sec\ref{eval:e2e}).

\myfinding{}
%
For overpredictions of untouched memory (and correspondingly undersized local vNUMA nodes), the workload spills into \cvn.
Many workloads see an immediate impact on slowdown.
Slowdowns further increase if more workload memory spills into \cvn.
Some workloads are slowed down by as much as 30-35\% with 20-75\% of workload memory spilled and up to 50\% if entirely allocated on pool memory.
We use access bit scans to verify that these workloads indeed actively access their entire working set.

\myparagraph{Implications}
Allocating a fixed percentage of pool DRAM to VMs would lead to significant performance slowdowns.
There are only two strategies to reduce this impact: 1) identify which workloads will see slowdowns and 2) allocate untouched memory on the pool.
\sys employs both strategies.

\input{fig-ml-all}

\subsection{Performance of Prediction Models}
\label{sec:eval:models}

We evaluate \sys's prediction models (\sec\ref{sec:design:ml}) and its combined prediction model based on Eq.\eqref{eq:optimization}.

\vfive\subsubsection{Predicting Latency Sensitivity}
%
\label{sec-eval-pred}

\sys seeks to predict whether a VM is latency insensitive, \ie,
whether running the workload on pool memory would stay within the performance
degradation margin (\pdm).
We tested the model for \pdm between 1-10\% and on both 182\% and 222\% latency increases, but report details only for 5\% and 182\%.
Other \pdm values lead to qualitatively similar results.
\newtxt{The 222\% model is 16\% less effective given the same false positive rate target.}
We compare thresholds on
memory and DRAM boundedness~\cite{topdownanalysis.ispass14, tmam.web21} to
our RandomForest (\sec\ref{sec-impl}).

Figure~\ref{fig-ml-sensitivity}
shows the model's false positive rate as a function of the percentage of workloads
labeled as latency insensitive, similar to a precision-recall
curve~\cite{buckland1994relationship}.
Error bars show 99\% confidence from a 100-fold validation based on randomly
splitting into equal-sized training and testing datasets.

\myfinding{} While DRAM boundedness is correlated with
slowdown, we find examples where high slowdown occurs even for a small
percentage of DRAM boundedness. For example, multiple workloads exceed
20\% slowdown with just two percent of DRAM boundedness.

\myparagraph{Implication}
This shows the general hardness of predicting whether workloads exceed the \pdm.
Heuristic as well as predictors will make statistical errors.

%\input{fig-tmam} % HCL: now in fig-ml-all.tex

\myfinding{}
We find that ``DRAM bound'' significantly outperforms ``Memory bound'' (Figure~\ref{fig-ml-sensitivity}).
Our RandomForest performs slightly better than ``DRAM bound''.

\myparagraph{Implication}
Our RandomForest can place 30\% of workloads on the pool with only 2\% of false positives.


\input{fig-ml-prod-over}


\vfive\subsubsection{Predicting Untouched Memory}\label{eval:untouched}

\sys predicts the amount of untouched memory over a VM's future lifetime (\sec\ref{sec:design:ml}).
We evaluate this model using metadata and resource usage logs from 100 clusters over 75 days.
The model is trained nightly and evaluated on the subsequent day.
Figure~\ref{fig-ml-frigid} compares our GBM model to the heuristic that assumes a fixed fraction of memory as untouched across all VMs.
The figure shows the overprediction rate as a function of the average amount of untouched memory.
Figure~\ref{fig-ml-prod} shows a production version of the untouched memory model during the first 110 days of 2022.

\myfinding{}
\newtxt{We find that the GBM model is $5\times$ more accurate than the static policy, \eg, when labeling 20\% of memory as untouched,  GBM overpredicts only 2.5\% of VMs while the static policy overpredicts 12\%.}

\myparagraph{Implication}
\newtxt{Our prediction model identifies 25\% of untouched memory while only overpredicting 4\% of VMs.}


\myfinding{}
The production version of our model performs similarly to the simulated model.
Distributional shifts lead to some variability over time.

\myparagraph{Implication}
We find that accurately predicting untouched memory is practical and a realistic assumption.



\vfive\subsubsection{Combined Prediction Models}
\label{eval:combinedml}

\newtxt{We characterize \sys's combined models (Eq.\eqref{eq:optimization}) using ``scheduling mispredictions'', \ie, the fraction of VMs that will exceed the \pdm.
This incorporates the overpredictions of untouched memory, how much the model overpredicted, and the probability of this overprediction leading to a workload exceeding the \pdm.
Further, \sys uses its QoS monitor to mitigate up to 1\% of mispredictions.
Figure~\ref{fig-ml-combined} shows scheduling mispredictions as a function of the average amount of cluster DRAM that is allocated on its pools for 182\% and 222\% memory latency increases, respectively.}

\myfinding{}
\newtxt{\sys's combined model outperforms its individual models by finding their optimal combination.}
\myparagraph{Implication}
\newtxt{With a 2\%  scheduling misprediction target, \sys can schedule 44\% and 35\% of DRAM on pools with 182\% and 222\% memory latency increases, respectively.}




%After some investigation, we conclude that the system overpredicted the
%amount of untouched memory because at access bit collection time we failed to capture
%memory accesses early in the workload initialization. We adjust our tool accordingly to
%start capturing memory accesses right after VM starts booting.


\input{fig-ml-prod}


\subsection{End-to-end Reduction in Stranding}\label{eval:e2e}

\newtxt{We characterize \sys's end-to-end performance while constraining its rate of scheduling mispredictions.
Figure~\ref{fig-e2e} shows the reduction in aggregate cluster memory as a function of pool size for \sys under 182\% and 222\% memory latency increase, respectively, and a strawman static allocation policy.
We evaluate multiple scenarios; the figure shows \pdm=5\% and \tp=98\%.
In this scenario, the strawman statically allocates each VM with 15\% of pool DRAM.
About 10\% of VMs would touch the pool DRAM (Figure~\ref{fig-ml-frigid}).
Of those touching pool DRAM, we'd expect that about $\frac{1}{4}$ would see a slowdown exceeding a \pdm=5\% (Figure~\ref{fig-split-cdf}).
So, the strawman would have about 2.5\% of scheduling mispredictions.}
%In the second scenario, the strawman statically allocates each VM with 10\% of pool DRAM and has about 6\% scheduling mispredictions.


%

\myfinding{}
\newtxt{At a pool size of 16 sockets, \sys reduces overall DRAM requirements by 9\% and 7\% under 182\% and 222\% latency increases, respectively.
Static reduces DRAM by 3\%.
When varying \pdm between 1 and 10\% and \tp between 90 and 99.9\% we find the relative savings of the three systems to be qualitatively similar.}

\myparagraph{Implication}
\sys can safely reduce cost.
A QoS monitor that mitigates more than 1\% of mispredictions, can achieve more aggressive performance targets (\pdm).

\myfinding{}
Throughout the simulations, \sys's pool memory offlining speeds remain below 1GB/s and 10GB/s for 99.99\% and 99.999\% of VM starts, respectively.

\myparagraph{Implication}
\sys is practical and achieve design goals.

\input{fig-comb-model}




%%%%%%%% old and excluded for now

\begin{comment}
\subsubsection{Interleaving}
%
\label{sec-eval-interleave}

While CXL memory offer higher latency and lower bandwidth compared to local DRAM, a VM may gain higher aggregate
memory bandwidth by combining local and CXL memory.
Prior work has thus proposed to interleave local and CXL memory to mitigate some of the performance impacts of memory disaggregation~\cite{thymesisflow.micro20}.
With a zNUMA topology, the guest OS defaults to a
``local-first'' memory allocation policy to exploit locality and avoid
remote memory access.
We evaluate interleaving by configuring 4kB-page-level interleaving across local and CXL memory.
Figure \ref{fig-split-cdf}b compares a 50-50 interleaving configuration to local-first with a 50\% zNUMA.
%
%Figure \ref{fig-interleave} shows the detailed per workload results,
%and

\myfinding{}
The distribution of slowdowns under interleaving is similar to local-first.
Interleaving improves performance for $\sim$30\% of workloads and degrades performance for $\sim$45\% workloads.
For a small percentage (3\%), interleaving improves performance compared to socket-local DRAM.

\myparagraph{Implications}
This shows that interleaving alone is an insufficient slowdown mitigation technique and reiterates the need for \sys's novel mitigation techniques.

% Interleaving
%\input{fig-interleave-cdf}
%\input{fig-interleave} % -----> Bar plot, not shown for space




% VoltDB Latency results
% voltdb
\subsection{Need to think about this}
%
\label{sec-eval-vs}

\note{I kind of like this and liked the associated graph. We just need to come up with a story how this fits into the overall story. For example, we could compare to prior work that saw massive impacts for VoltDB such as ThymesisFLow and LEAP/Infinicache.}

In-memory NoSQL systems are popular and commonly deployed in various
scenarios for fast data storage. We ran two representatives of such
in-memory database and key-value store systems, Redis~\cite{redis.web21}
and VoltDB~\cite{voltdb.web21}. We use the popular YCSB benchmark
(workloads A--F)~\cite{ycsb.socc10} under a 2-node client/server setup
over a 10Gb NIC. We populate the server with 10M 1KB KVs and used 1024
connections to measure the maximum throughput. Figure \ref{fig-cxl}
shows that Redis throughput is affected by $\sim$20\% while VoltDB only
sees $<$5\% throughput drop. Further, we also measure the tail latencies
under various load (\ie, target ops/s) and key-value sizes (results not
shown for space), we find that the performance impact increases with
key-value size and load. This is mainly due to
%
%
Figure \ref{fig-kvlat} shows latencies at major percentiles (\eg, 95\th,
and 99\th). The dashed line represents CXL and the solid line is the
baseline (local DRAM backing). When the load less than 40K ops/s, we
don't see obvious latency difference. As the load further increases, CXL
shows higher latencies, especially for the 95\th and 99\th tail latency.
%

\end{comment}
