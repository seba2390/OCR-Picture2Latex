%new_intro attempt

\section{Introduction}
\label{sec-new-intro}

\myparagraph{Motivation}
Many public cloud customers deploy their workloads in the form of
virtual machines (VMs), for which they get virtualized compute with
performance approaching that of a dedicated cloud, but without having
to manage their own on-premises datacenter. This creates a major
challenge for public cloud providers: achieving excellent performance
for opaque VMs (\ie, providers do not know and should not inspect what
is running inside the VMs) at a competitive hardware cost.

A key driver of both performance and cost is main memory.  The gold
standard for memory performance is for accesses to be served by the
same NUMA node as the cores that issue them, leading to latencies in
tens of nanoseconds.  A common approach is to
preallocate all VM memory on the same NUMA node as the VM's cores.
Preallocating and statically pinning memory also facilitate the use
of virtualization
accelerators~\cite{nicpagefault.asplos17,tian2020coiommu,yassour2010dma,willmann2008protection,amit2011viommu,ben2010turtles},
which are enabled by default, for example, on AWS and
Azure~\cite{awsaccelnet,azureaccelnet}. At the same time,
DRAM has become a major portion of hardware cost due to its poor
scaling properties with only nascent
alternatives~\cite{memscaling.imw13, archshield.isca13, mutlu2015main,
  dramscaling.imw15, dramscalingchallenges.imw20,
  nextnewmemories.web19,micron3dxp.news21}.  For example, DRAM can be
50\% of server cost~\cite{cxlandgenz.web20}.

Through analysis of production traces from \azure,
%a large public cloud provider (called \azure for blind review),
we identify {\em memory stranding} as a dominant source of memory waste and a potential source of massive cost savings.
Stranding happens when all cores of a server are rented (\ie, allocated to customer VMs) but unallocated memory capacity remains and cannot be rented.
We find that up to 25\% of DRAM becomes stranded as more cores become allocated to VMs.

\myparagraph{Limitations of the state of the art} Despite this
significant amount of stranding, reducing DRAM usage in the public
cloud is challenging due to its stringent performance requirements.
For example, existing techniques for process-level memory
compression~\cite{softfarmem.asplos19,weiner2022tmo} require page
fault handling, which adds microseconds of latency, and moving away
from statically preallocated memory.

Pooling memory via memory disaggregation is a promising approach
because stranded memory can be returned to the disaggregated pool and
used by other servers.  Unfortunately, existing pooling systems also
have microsecond access latencies and require page
faults~\cite{memblade.isca09, lim2011disaggregated,
  orchdisaggmem.tc20, sysdisaggmem.hpca12, nicpagefault.asplos17,
  aifm.osdi20, angel2020disaggregation} or changes to the VM
guest~\cite{aifm.osdi20, semeru.osdi20, kona.asplos21, farm.nsdi14,
  remoteregions.atc18, kvdisagg.atc20, farview.corr21,
  sysdisaggmem.hpca12, resdisagg.osdi16, infiniswap.nsdi17,
  legoos.osdi18, softfarmem.asplos19, dcm.tc19, fastswap.eurosys20,
  leap.atc20, fluidmem.icdcs20, orchdisaggmem.tc20,ememdisagg.corr20}.

\myparagraph{Our work} This work describes \sys, {\em the first system
  to achieve both same-NUMA-node memory performance and competitive
  cost for public cloud platforms.}  To achieve this, \sys combines
hardware and systems techniques.  It relies on the Compute Express
Link (CXL) interconnect standard~\cite{cxlsite.web20}, which enables
cacheable load/store (\texttt{ld/st}) accesses to pooled memory on
Intel, AMD, and ARM processors~\cite{intelsapphire.web21, arm2021cxl,
  amdgenoa} at nanosecond-scale latencies.  CXL access via
loads/stores is a game changer as it allows memory to remain
statically preallocated while physically being located in a shared
pool.  However, even with loads/stores, CXL accesses still face higher
latencies than same-NUMA-node accesses.  \sys introduces systems
support for CXL-based pooling that dramatically
reduces the impact of this higher latency.

% oversub

%design philosophy

% later talk about related work including numa alloc

\begin{comment} % old paragraph
Hardware-based pooling~\cite{thymesisflow.micro20, clio.web21, zombieland.eurosys18, pberry.hotos19, dredbox.date16, memblade.isca09} can overcome this limitation, but state-of-the-art systems still have microsecond-latency~\cite{clio.web21}.
Hardware-based pooling has historically also not been available on mainstream platforms.
The Compute Express Link (CXL) interconnect standard~\cite{cxlsite.web20} overcomes both barriers and has superceded competing protocols~\cite{nextcxlgenz,sth2019cxl}.
CXL reduces latency by an order of magnitude to $\approx$100ns.
It also enables cacheable load\slash store (\texttt{ld/st}) accesses to pooled memory on Intel, AMD, and ARM processors~\cite{intelsapphire.web21, arm2021cxl, amdgenoa} and thus facilitates broad deployment with hypervisors statically pinning pool memory.
\end{comment}



\begin{comment} % old paragraph
In more detail, making memory pooling broadly used in the public cloud faces three challenges.
First, cloud providers seek to \emph{hide any additional memory access latency} from the vast majority of customers
who are not expert developers and workloads that do not explicitly manage
memory placement.
Second, cloud providers seek to \emph{preserve customer inertia} in that new technology should not require modification to the customer workloads or the guest OS.
Third, the system must be \emph{compatible with virtualization acceleration} which requires the whole address range of a VM to be statically pinned in hypervisor-level page tables~\cite{nicpagefault.asplos17,tian2020coiommu,yassour2010dma,willmann2008protection,amit2011viommu,ben2010turtles}.
For example, accelerated networking is enabled by default on AWS and Azure~\cite{awsaccelnet,azureaccelnet}.
\end{comment}

% Can cut next sentence, comparison to DSM
%Pool memory pages are owned by a single host connected to the CXL pool, which contrasts to distributed shared memory~\cite{munin.sosp91,dsm.tocs89}.


%: directly-connected CXL memory can be accessed with roughly the same latency as a single NUMA hop, and CXL

%how much of inefficiency is due to stranding and how much is due to underutilization?
%how would memory disagg help

% Deployment requirements



\sys is feasible because of four key insights.
%\emph{At the hardware layer, how should the cloud provider construct a CXL memory pool?}
%Prior work focuses on the mechanisms of pooling memory and leaves open how a provider would tradeoff pool size, access latencies, and cost savings.~\cite{thymesisflow.micro20, clio.web21, syrivelis2017software, memblade.isca09, lim2011disaggregated, orchdisaggmem.tc20, sysdisaggmem.hpca12, nicpagefault.asplos17, aifm.osdi20, angel2020disaggregation}.
First, by analyzing traces from 100 production clusters at \azure, we find that pool sizes between 8-16 sockets lead to sufficient DRAM savings.
The pool size defines the number of CPU sockets able to use pool memory.
Further, analysis of CXL topologies lead us to estimate that CXL will add 70-90ns to access latencies over same-NUMA-node DRAM with a pool size of 8-16 sockets, and add more than 180ns for rack-scale pooling.
We conclude that grouping 8 dual-socket (or 16 single-socket) servers is enough to achieve most of the benefits of pooling.

%\emph{How many cloud workloads can tolerate the additional latency of CXL pool memory accesses?}
\newtxt{Second, by emulating either 64ns or 140ns of memory access overheads, we find that 43\% and 37\% of 158 workloads are within 5\% of the performance on same-NUMA-node DRAM when entirely allocated in pool memory.
However, more than 21\% of workloads suffer a performance loss above 25\%.}
This emphasizes the need for small pools and shows the challenge with achieving same-NUMA-node performance.
%We train a machine learning (ML) model that can identify a subset of
%insensitive workloads ahead of time to be allocated on the memory pool.
This characterization also allows us to train a machine learning (ML) model that can identify a subset of insensitive workloads ahead of time to be allocated on the \sys memory pool.

%\emph{At the system software layer, how should the provider expose the pooled memory to guest OSes?}
Third, we observe through measurements at \azure that $\sim$50\% of all VMs touch less than 50\% of their rented memory.
Conceptually, allocating untouched memory from the pool should not have any performance impact even for latency-sensitive VMs.
We find that --- while this concept does not hold for the uniform address spaces assumed in prior work~\cite{memblade.isca09, lim2011disaggregated, orchdisaggmem.tc20, sysdisaggmem.hpca12, nicpagefault.asplos17, aifm.osdi20, angel2020disaggregation} --- it does hold if we expose pool memory to a VM's guest OS as a {\underline{z}ero-core virtual NUMA ({\bf \cvn}) node}, \ie, a node with memory but no cores, like Linux's CPU-less NUMA~\cite{cpulessnuma.web19}.
Our experiments show \cvn effectively biases memory allocations away from the \cvn node.
Thus, a VM with a \cvn sized to match its untouched memory will indeed not see any performance impact.

Fourth, \sys can allocate CXL memory with same-NUMA-node performance using correct predictions of {\bf a)} whether a VM will be latency-sensitive and {\bf b)} a VM's amount of untouched memory.
For incorrect predictions, \sys introduces a novel monitoring system that detects poor memory performance and triggers a mitigation that migrates the VM to use only same-NUMA-node memory.
Further, we find that all inputs to train and run \sys's ML models can be obtained from existing hardware telemetry with no measurable overhead.

%how to monitor given opaque vms?
%
%However, no prior work has combined predictions with scheduling VMs on disaggregated memory.

%Due to the nature of direct load/store access to disaggregated memory, CXL does not require page faults at the Guest or the Hypervisor and thus, it is compatible with existing pinned memory requirements.
%Additionally, we find that we can observe when our predictions are incorrect and lead to poor memory access performance despite the ``opaqueness of VMs''.
%This allows us to change a VM placement in the rare cases where it is needed to guarantee good performance.

\myparagraph{Artifacts}
CXL is still a year from broad deployment.
Meanwhile, deploying \sys requires extensive testing within \azure's system software and distributed software stack.
We implement \sys on top of an emulation layer that is deployed on production servers.
This allows us to prove the key concepts behind \sys by exercising the VM allocation workflow, \cvn, and by measuring guest performance.
Additionally, we support the four insights from above by reporting from extensive experiments and measurements in \azure's datacenters. We evaluate the effectiveness of pooling using simulations based on VM traces from 100 production clusters.

\myparagraph{Contributions} Our main contributions are:
\begin{itemize}
\item The first public characterization of memory stranding and untouched memory at a large public cloud provider.
\item The first analysis of the effectiveness and latency of different CXL memory pool sizes.
\item \sys, the first CXL-based full-stack memory pool that is practical and performant for cloud deployment.
\item An accurate prediction model for latency and resource management at datacenter scale. These models enable a configurable performance slowdown of 1-5\%.
\item \newtxt{An extensive evaluation that validates \sys's design including the performance of \cvn and our prediction models in a production setting.
  Our analysis shows that we can reduce DRAM needs by 7\% with a \sys pool spanning 16 sockets, which corresponds to hundreds of millions of dollars for a large cloud provider.}
%Exposing the \cvn node to the guest OS is very effective at producing this split.
%\item To-be-open-sourced research artifacts, including en emulation testbed, production traces, prediction models.
\end{itemize}


% add forward references

\begin{comment}
Unfortunately, memory disaggregation incurs a potential performance
penalty in that memory from the pool has higher access latency.  To
achieve the most DRAM savings, public cloud providers would prefer to
hide the additional latency from (1) the vast majority of customers
who are not expert developers and (2) workloads that do not manage
memory placement and performance explicitly.  The challenge is
achieving this in a practical and broadly applicable manner.

In more detail, making memory disaggregation practical for the public cloud faces multiple functional challenges.
First, we must preserve \emph{customer inertia}, \ie, require no modifications to customer workloads or the guest OS.
Second, the system must be \emph{compatible with virtualization acceleration} techniques such as direct I/O device assignment to VMs~\cite{intelvtd.web20, nicpagefault.asplos17} and SR-IOV~\cite{dong2008sr}.
Third, the system must be available as \emph{commodity hardware}.

Due to these requirements, most of the prior memory disaggregation work does not
apply: custom hardware-based designs~\cite{thymesisflow.micro20,zombieland.eurosys18, pberry.hotos19,
dredbox.date16, memblade.isca09}, systems that require changes to the VM guest~\cite{aifm.osdi20,
semeru.osdi20, kona.asplos21, farm.nsdi14, remoteregions.atc18, kvdisagg.atc20,
farview.corr21, sysdisaggmem.hpca12, resdisagg.osdi16, infiniswap.nsdi17, legoos.osdi18, softfarmem.asplos19, dcm.tc19, fastswap.eurosys20, leap.atc20, fluidmem.icdcs20, orchdisaggmem.tc20,ememdisagg.corr20}, and implementations that rely on page faults~\cite{nicpagefault.asplos17} are not deployable in the cloud today (see \sec\ref{sec:requirements}).

Fortunately, the emerging Compute Express Link (CXL) interconnect standard~\cite{cxlsite.web20} greatly facilitates fast and deployable disaggregated memory.
CXL enables native load\slash store (\texttt{ld/st}) accesses to disaggregated memory for Intel, AMD, and ARM processors~\cite{intelsapphire.web21, arm2021cxl, amdgenoa} and Samsung, Micron, and SK Hynix memory modules~\cite{cxlmembers, samsung2021cxl}. CXL also has a low performance impact: directly-connected CXL memory can be accessed with roughly the same latency as a NUMA hop.
CXL has become the best option for memory disaggregation by superseding competing protocols~\cite{nextcxlgenz,sth2019cxl}.


NUMA policies focus on moving compute close to memory,
\eg, a process or VM is scheduled on cores in the same NUMA domain as
the memory pages it
accesses~\cite{corbet2012autonuma,rao2010vnuma,liu2014optimizing}.
This is not applicable to CXL where memory pools do not have local
cores.  Two-tier systems focus on migrating hot pages to local
memory~\cite{hemem.sosp21,nimblepage.asplos19,zswap.web20,softfarmem.asplos19,thermostat.asplos17,ramos.ics11}.
Page migration is not supported in public cloud platforms due to
virtualization accelerators (\sec\ref{sec-des}).  Thus, providers
require a new way to overcome the challenges of higher memory access
latency.

\end{comment}


