\section{Related Work}
%
\label{sec-rel}


%Table \ref{tab-rel} compares \sys to state-of-the-art memory
%disaggregation approaches at the hardware (HW), hypervisor\slash host
%(VMM\slash OS), runtime, and application (App) levels.

{\ni\bf Hardware-level disaggregation:} Hardware-based
disaggregation designs~\cite{clio.web21, thymesisflow.micro20,
zombieland.eurosys18, pberry.hotos19, dredbox.date16, memblade.isca09,
optmemdisagg} are not easily deployable as they do not rely on commodity
hardware. For instance, ThymesisFlow~\cite{thymesisflow.micro20} and Clio~\cite{clio.web21}
propose FPGA-based rack-scale memory disaggregation designs on top of
{OpenCAPI}~\cite{opencapi.web21} and RDMA. Their hardware layer shares goals with \sys.
Their software goals differ fundamentally, \eg, ThymesisFlow advocates application changes
for performance, while \sys focuses on platform-level ML-driven pool
memory management that is transparent to users.

%Clio still suffer from a 2.5\us\ median latency by managing the
%processing power in the memory nodes natively with its customized %memory management schemes in hardware.

%the performance is not comparable to existing cloud
%offerings (25GB/s bandwidth and 950ns latency). As a result, it causes
%higher core utilization and much larger performance impact, \eg, for
%VoltDB experiences 50-150\% higher cpu utilization.
% Another example is that Latency in memcached: 34-64\% degradation
%Significantly lower IPC
%Proposed performance optimizations rely on application modifications
%Unclear security and blast radius story
%Opposite finding to us: interleaving better for them

\vfive
{\ni\bf Hypervisor\slash OS level disaggregation:}
Hypervisor\slash OS level approaches~\cite{sysdisaggmem.hpca12,
    resdisagg.osdi16, infiniswap.nsdi17, legoos.osdi18,
    softfarmem.asplos19, dcm.tc19, fastswap.eurosys20, leap.atc20,
fluidmem.icdcs20, orchdisaggmem.tc20, ememdisagg.corr20}
rely on page faults and access monitoring to maintain the
working set in local DRAM. Such OS-based approaches bring significant
overhead, jitter, and are incompatible with virtualization acceleration
(\eg, DDA).

%leverage fast
%networks and software abstractions for memory disaggregation, which
%render them incompatible with virtualization acceleration
%(\sec\ref{sec:requirements}). Moreover, these optimizations rely on
%software-based page access monitoring\slash migration\slash prefetching
%to achieve \us\ level latency at the cost of much CPU overhead. Yet,
%both the performance and resource efficiency make them sub-optimal for
%practical deployments.
%
%\hcl{show RDMA + page fault eval results}

%\input{tab-rel}

\input{fig-e2e}


\vfive
{\ni\bf Runtime/application level disaggregation:}
Runtime-based disaggregation designs~\cite{aifm.osdi20, semeru.osdi20,
kona.asplos21} propose customized APIs for remote memory access.
%Similarly, some
%applications (\eg, databases) are specifically designed to leverage
%remote memory %for
%scalability and performance~\cite{farm.nsdi14, herd.sigcomm14, ccnuma.eurosys18, namdb.vldb17}.
While effective, this approach requires
developers to explicitly use these mechanisms at the application level.



%through language-specific
%functions and libraries, causing an adoption barrier for the cloud.
\vfive
{\ni\bf Memory tiering:}
%
Prior works have considered the broader impact of extended memory
hierarchies and how to handle them~\cite{softfarmem.asplos19,
heteroos.isca17, thermostat.asplos17, flatflash.asplos19,
autotiering.atc21}. For example,
Google
% 's far memory implementation
achieves 6\us\ latency via proactive hot\slash cold page
detection and compression~\cite{zswap.web20, softfarmem.asplos19}.
%Intel offers a hardware implementation of caching for its Optane NVM devices~\cite{scalepm.fast20}.
%Thermostat~\cite{thermostat.asplos17} uses an online page classification
%method for efficient application-transparent and huge-page-aware two-tier memory
%management.
%
Nimble~\cite{nimblepage.asplos19} optimizes Linux's page tracking mechanism to
tier pages for increased migration bandwidth.
%
%HeMem~\cite{hemem.sosp21} manages page migrations between tiered
%DRAM/NVM with low-overhead and asynchronous policies (\eg, cpu events sampling).
%
\sys takes a different ML-based approach looking at memory pooling design at the platform-level and is orthogonal
to these works.
%\todo{Discuss page migration work ~\cite{hemem.sosp21,nimblepage.asplos19}}
% does not require tiering (orthogonal).
%
% \sys focuses on platform-level
%pool memory management to reduce stranding and is orthogonal to these
%existing data tiering optimizations.

% HCL: remove if no space
\vfive
{\ni\bf ML for systems:}
%
ML is increasingly applied to tackle systems problems,
such as cloud efficiency~\cite{smartharvest.eurosys21,
resourcecentral.sosp17}, memory\slash storage
optimizations~\cite{mldistore.mlsys21, llama.asplos20},
microservices~\cite{sinan.asplos21}, caching\slash prefetching policies
~\cite{nmprefetch.asplos21, dlcache.micro19}. We uniquely apply ML methods
for untouched memory prediction to support pooled memory provisioning to VMs
without jeopardizing QoS.

\vfive
{\ni\bf Coherent memory and NUMA optimizations:}
%
Traditional cache coherent NUMA architectures~\cite{ccnuma.isca97} use
specialized interconnects to implement a shared address space.
There are also system-level optimizations for NUMA, such as
NUMA-aware data placement~\cite{asymmemplacement.atc15} and proactive page
migration~\cite{autonuma.web19}.
%Distributed Shared Memory (DSM) systems offer a coherent global address
%space across the network (\eg, Ethernet)~\cite{munin.sosp91,dsm.tocs89}.
%
NUMA scheduling policies ~\cite{corbet2012autonuma,rao2010vnuma,liu2014optimizing} balance compute and memory across NUMA nodes.
\sys's ownership overcomes the need for coherence across the memory pool.
\cvn's zero-core nature
requires rethinking of existing optimizations which are largely
optimized for symmetric NUMA systems.
%\todo{Discuss NUMA scheduling stuff ~\cite{corbet2012autonuma,rao2010vnuma,liu2014optimizing}}




%Unfortunately, coherence
%equirements in DSM have traditionally led to high communication overheads.
%This design choice avoids the heavy coherence
%overhead in our external memory controller design.
%
%
%These optimizations can still benefit \sys design. Actually, \sys
%largely re-uses existing OS-level NUMA policies by exposing \cvn to
%guest VMs transparently.



\begin{comment}
%
{\bf Coherent Memory Sharing:} There are two bodies of work.
First, large ccNUMA architectures~\cite{ccnuma.isca97} use specialized
interconnects to implement a coherent shared address space.  Second,
Distributed Shared Memory (DSM) systems offer a coherent global address
space that is shared by user applications across
servers~\cite{munin.sosp91,dsm.tocs89}. Unfortunately, the coherence
requirement has traditionally lead to high communication overheads.
While coherence helps some applications to significant speedups, the
majority of workloads in the cloud are based on individual VMs that
communicate through RPCs. Each such VM runs on an individual system in
its own virtualized address space and thus does not take advantage of
coherent memory sharing.
%

{\bf Heterogeneous Memory Management:}
%
KLOCs~\cite{kloc.asplos21} focuses on OS-level I/O object grouping for
performance. They provide a new OS abstraction and use it to identify
and group kernel objects with similar hotness, reuse, and liveness.
\cite{nimblepage.asplos19} proposes OS level memory management
optimizations to improve page migration efficiency between fast/slow
memory tiers with no additional monitoring overhead. They mainly propose
several optimization techniques such as enabling THP for page migration,
increasing the concurrency of page migrations (multi-threading,
parallelized pipeline, etc.)
%
Measurement \cite{datatiering.eurosys16} presents a detailed study of
representative workloads like KV stores, in-memory databases, graph
analytics, etc. on heterogeneous memory systems. They prove that using a
mixed of small fast DRAM and large amount of slower NVM can achieve good
performance with careful data structure placement. They contribute a set
of tools and libraries to facilitate data placement.

%
HeteroOS~\cite{heteroos.isca17} is an OS-level solution for managing
memory heterogeneity in virtualized system. It requires extensive guest
OS changes and hypervisor-level proactive data placement/migration.
%
Thermostat~\cite{thermostat.asplos17} uses an online page classification
method for an efficient application-transparent and hugepage-aware page
management for two-tiered memory systems.
%
FlatFlash~\cite{flatflash.asplos19} unifies memory and byte-addressable
SSD as a unified memory space with  a lightweight and adaptive page
promotion mechanism between SSD and DRAM.
%
This project informs academia about these challenges and either shows
how to overcome them in a practical implementation â€“ or shows that
memory disaggregation is unfeasible, which opens up new lines of
research on solving these challenges. We are proposing and evaluating a
different interface, which is targeted at virtualized environments with
the advantage of SW transparency.
%
\end{comment}



