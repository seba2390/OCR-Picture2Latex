\section{Discussion}
\label{sec:discussion}

\myparagraph{Robustness of ML}
Similar to other oversubscribed resources (CPU~\cite{smartharvest.eurosys21} and disks~\cite{awsdisk}), customers may overuse resources to get local memory.
%The goals of \sys's ML models are aligned with customer interests, i.e., to give sufficient memory so that performance impacts are small.
When multiplexing resource for millions of customers, any individual customerâ€™s behavior will have a small impact.
Providers can also provide small discounts when resources are not fully utilized.

\myparagraph{Alternatives to static memory preallocation}
\sys is designed for compatibility static memory as potential workarounds are not yet practical.
The PCIe Address Translation Service (ATS/PRI)~\cite{pcieats.web09} enables compatibility with page faults.
Unfortunately, ATS/PRI-devices are not yet broadly available~\cite{nicpagefault.asplos17}.
Virtual IOMMUs~\cite{tian2020coiommu,amit2011viommu,ben2010turtles} allow fine-grained pinning but require guest OS changes and introduce overhead.

\begin{comment}
\myparagraph{Pool failures}
Compared to a non-CXL system, we introduce the EMC and the pool manager as additional points of failure.
We expect EMC failures to happen at a similar rate to CPU failures.
At \azure, CPUs failure rate are lower than 12 other components.
Pool manager failures affect efficiency, as we cannot reassign memory, but do not affect availability as they are not on the data path.
\end{comment}










\begin{comment}

Sensitivity with regard to our evaluation setup
\begin{itemize}
\item Memory performance affects different CPUs differently
\item We evaluated a bunch of CPUs and chose the one where impact was high
\end{itemize}


Achievable disaggregation scope
%\input{fig-discussion-scope}
\begin{itemize}
\item Our current design can trivially achieve a disaggregation scope of eight sockets. Assuming single-socket blades, each eight blades would be connected to three EMCS.
  We call each such set of eight blades, together with their EMCs a ``memory chassis''.
\item However, shifting capacitgy at even larger scopes is also achievable.
  For example, each socket can use two CXL links to connect to the EMCs in its chassis and use the third CXL link to connect to an EMC in another chassis.
  This way, if one chassis comes under memory pressure, we can alleviate this memory pressure over time via the sockets connected to other chassis. Specifically,the rack/pool manager (workflow B) would slowly move memory to the sockets in the first chassis via their CXL links to other chassis.
\item Figure~\ref{fig-discussion-scope} shows a sketch of how this architecture works.
\item We find that this architecture essentially performs like a memory pools across XX sockets.
\end{itemize}

New research directions and implications
\begin{itemize}
\item External memory controllers give rise to novel hardware-software codesign directions such as hotness tracking, deduplication, etc.
\item 12 year old PCIe standard (ATS/PRI) needs more attention -- nontrivial implementation challenges, e.g., incoming RDMA packet faster than page fault resolution, cite Mellanox ASPLOS
\item Need more active page movement work (also needs HW support)
\item Predictions for workload impact
\end{itemize}

\end{comment}
