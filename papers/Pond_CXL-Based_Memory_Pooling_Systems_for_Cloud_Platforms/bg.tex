
% not yet covered:
% DRAM pricing is flatlining and DDR5 more expensive

\input{fig-cxlreadwrite}

\section{Background}
\label{sec-bg}

{\bf Hypervisor memory management.}
Public cloud workloads are virtualized~\cite{firecracker.nsdi20}.
To maximize performance and minimize overheads, hypervisors perform minimal memory management and rely on
virtualization accelerators to improve I/O performance~\cite{intelvtd.web20,nicpagefault.asplos17,leapio.asplos20,sriov}.
Examples of common accelerators are direct I/O device assignment
(DDA)~\cite{intelvtd.web20,nicpagefault.asplos17} and Single Root I/O
Virtualization (SR-IOV)~\cite{leapio.asplos20,sriov}.
Accelerated networking is enabled by default on AWS and Azure~\cite{awsaccelnet,azureaccelnet}.
As pointed out in prior work, virtualization acceleration requires statically preallocating (or ``pinning'') a VM's entire address space~\cite{nicpagefault.asplos17,tian2020coiommu,yassour2010dma,willmann2008protection,amit2011viommu,ben2010turtles}.

\myparagraph{Memory stranding} Cloud VMs demand a vector of resources
(\eg, CPUs, memory, \etc)
~\cite{resourcecentral.sosp17, hadary2020protean,
googlejobpacking.cluster14, borg.eurosys15}.
Scheduling VMs thus leads to a multi-dimensional bin-packing
problem~\cite{binpackheuristics.web11, tetris.sigcomm14, hadary2020protean, bpbounds.stoc13}
which is complicated by constraints such as spreading VMs across multiple failure
domains.
Consequently, it is difficult to provision servers that closely
match the resource demands of the incoming VM mix.
When the DRAM-to-core ratio of VM arrivals and the server resources
do not match, tight packing becomes more difficult.
%
We define a resource as \emph{stranded} when
it is technically available to be rented to a customer, but is
practically unavailable as some other resource has exhausted. The typical scenario for {\em
memory stranding} is that all cores have been
rented, but there is still memory available in the server.
%Memory stranding is a lower bound on stranding because the availability of one or a few cores
%still leave memory stranded for VMs requiring more cores.  For example,
%even if there were one core still to be rented, we could not rent the
%resources given a workload mix where VMs demand two or more cores.



\myparagraph{Reducing stranding} Multiple
techniques can reduce memory stranding. For example,
oversubscribing cores~\cite{smartharvest.eurosys21,harvestslo.osdi20} enables more memory to be rented.
However, oversubscription only applies to a subset of VMs for performance reasons.
%Oversubscription also requires a few unrented cores and, thus, cannot address memory stranding.
Our measurements at \azure (\sec\ref{sec-strand}) include
clusters that enable oversubscription and still show significant memory stranding.
%An alternative is to reduce the cost of stranded memory by leveraging
%cheaper memory technologies, such as Intel Optane Persistent Memory
%(PMem)~\cite{intelpmem.web21}. Unfortunately, PMem's high latency
%requires performance mitigations such as page
%migrations~\cite{raybuck2021hemem,nimblepage.asplos19,zswap.web20,softfarmem.asplos19,thermostat.%asplos17,optaneval.memsys19,scalepm.fast20} which are not compatible with virtualization acceleration (\sec\ref{sec-des}).
% Additionally, new memory technologies such as PMem are only available
% from a single vendor, which entails sourcing risks for cloud
% providers.

The approach we target is to disaggregate a
portion of memory into a pool that is accessible by multiple
hosts~\cite{resdisagg.osdi16, fastnetdisagg.socc17,
thymesisflow.micro20}. This breaks the fixed hardware
configuration of servers.
By dynamically reassigning memory to different
hosts at different times, we can shift memory resources to where they
are needed, instead of relying on each individual server to be configured
for all cases pessimistically.
Thus, we can provision servers close to the average
DRAM-to-core ratios and tackle deviations via the memory pool.


%-----------------------------------------------------------------------
%\input{fig-stranding}
\input{fig-stranding2}


\myparagraph{Pooling via CXL}
\newtxt{CXL contains multiple protocols including \texttt{ld/st} memory semantics (CXL.mem) and I/O semantics (CXL.io).
CXL.mem maps device memory to the system address space.
Last-level cache (LLC) misses to CXL memory addresses translate into requests on a CXL port whose reponses bring the missing cachelines (Figure~\ref{fig-cxlreadwrite}).
Similarly, LLC write-backs translate into CXL data writes.
Neither action involves page faults or DMAs.
CXL memory is virtualized using hypervisor page tables and the memory-management unit and is thus compatible with virtualization acceleration.
The CXL.io protocol facilitates device discovery and configuration.
CXL 1.1 targets directly-attached devices, 2.0~\cite{cxl2spec.web20,cxl2whitepaper.web21} adds switch-based pooling, and 3.0~\cite{debendra2022fms,cxl3spec} standardizes switch-less pooling (\S\ref{sec-des}) and higher bandwidth.}

\newtxt{CXL.mem uses PCIe's eletrical interface with custom link and transaction layers for low latency.
%  protocol Muxing at the PHY level (vs higher level of the stack) helps deliver a low latency path for CXL.$Mem traffic. This eliminates the higher latency in the PCIe/ CXL.io path due to the support for variable packet size, ordering rules, access rights checks, etc., conforming the fundamental principle guiding the protocol choice made for CXL specifications
With PCIe 5.0, the bandwidth of a birectional \tms{8}-CXL port at a typical 2:1 read:write-ratio matches a DDR5-4800 channel.
CXL request latencies are largely determined by the CXL port.
Intel measures round-trip CXL port traversals at 25ns~\cite{debendra2022hoti} which, when combined with expected controller-side latencies, leads to an end-to-end overhead of 70ns for CXL reads, compared to NUMA-local DRAM reads.
While FPGA-based prototypes report higher latency~\cite{maruf2022tpp,gouk2022direct}, Intel's measurements match industry-expectations for ASIC-based memory controllers~\cite{maruf2022tpp,debendra2022hoti,cxl3spec}.}
%Latency targets in the upcoming CXL 3.0 specification~\cite[Table 13-2 in \S13]{cxl3spec} are 80ns for reads (Req-DRS) and 40ns for writes (RwD-NDR).}



