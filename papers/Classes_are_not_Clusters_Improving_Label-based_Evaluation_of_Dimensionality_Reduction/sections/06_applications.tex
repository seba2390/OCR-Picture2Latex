
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/fmnist_heatmap.pdf} \vspace{-6mm}
    \caption{Heatmaps detailing the CLM matrix of the Fashion-MNIST dataset ($M(X)$ in \autoref{sec:overcom}). The color of each cell depicts the CVM (DSC, \CHb{}) score measured for each pair of classes corresponding to rows and columns. \vspace{-2mm}}
    \label{fig:app_pp_heatmap}
\end{figure}



\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/app_hier.pdf}
    \vspace{-6mm}
    \caption{
    CLM distortion evaluation of a linear (PCA) and five nonlinear (t-SNE, UMAP, Isomap, LLE, and Densmap) unsupervised DR techniques. \rev{(A-D) Evaluation results with \ltc [\CHb{}/DSC] where} class labels are obtained from the hierarchical clustering of the original data at multiple granularity levels (x-axis). \ltc evaluates more coarse-grained (global) clusterings for higher levels. See details in \autoref{sec:app_hier}. 
    \rev{(E-F) Evaluation results of the techniques with T\&C (E) and KL Divergence (F). Note that for all figures, higher scores indicate better embeddings.} 
    \vspace{-3mm}}
    \label{fig:app_hier}
\end{figure*}


\section{Case Studies}

We report two case studies demonstrating the usefulness of \ltc to characterize DR techniques and their hyperparameters. 


\subsection{Examining the Effect of \textbf{\textit{t}}-SNE Perplexity}

\label{sec:apptsne}

\subsubsection{Objectives and Design}

We want to use \ltc to evaluate the reliability of the cluster structures from $t$-SNE embeddings (\autoref{sec:seneval}) depending on its perplexity hyperparameter $\sigma$. $\sigma$ adjusts the balance between local and global cluster structures \cite{wattenberg2016tsnetuning, cao17arxiv}. 
We generate the $t$-SNE embeddings of the 94 labeled datasets used for the scalability analysis   (\autoref{sec:scaleval})  using different $\sigma$ values ($\sigma \in \{2^i \mid i=0,\cdots, 10 \}$) and evaluate them using \ltc [\CHb{}] and \ltc [DSC].
We also inspect the $t$-SNE embeddings of the Fashion-MNIST \cite{xiao2017arxiv} dataset with various perplexity values ($\sigma \in \{4, 16, 64, 256, 1024\}$; \autoref{fig:app_pp}) to gain more qualitative insights.
Moreover, we compute the ``ground-truth'' CLM matrix of the Fashion-MNIST dataset (\autoref{fig:app_pp_heatmap}), where the $(i, j)$-th cell represents the CVM score (\CHb or DSC) of the $i$-th and $j$-th classes. Note that this CLM matrix is identical to $M(\mathbf{X})$ in \autoref{sec:overcom}. % It serves as ground truth to compare to the CLM of the embedding.
% We also compute CLM matrices in which $(i, j)$-th cell represents the  so that .
% For the comparison, we also apply the competitors we use in our evaluation except for CVMs. We exclude CVMs and CA-T\&C as they rely on the cluster-label matching assumption, thus having the threat to lead analysis toward erroneous conclusions (Refer to experiments D, E, and F in \autoref{sec:seneval}).

\subsubsection{Results and Discussions}


\newcommand{\greytext}[1]{\textcolor{gray}{#1}}
\newcommand{\orangetext}[1]{\textcolor{orange}{#1}}
\newcommand{\redtext}[1]{\textcolor{Red}{#1}}
\newcommand{\greentext}[1]{\textcolor{ForestGreen}{#1}}
\newcommand{\purpletext}[1]{\textcolor{Plum}{#1}}
\newcommand{\pinktext}[1]{\textcolor{VioletRed}{#1}}

%  and B depict the results associated to \ltc [\CHb{}] and \ltc [DSC], respectively.

In the case of \ltc [\CHb{}], we found a clear tradeoff between \lt and \lc (\autoref{fig:app_hp}A).
When $\sigma$ is low or high, \lt [\CHb{}] gives low scores to $t$-SNE embeddings, indicating more False Groups distortions, while \lc [\CHb{}] gives high scores, meaning fewer Missing Groups distortions. 
This means that $t$-SNE underrepresents the extent to which classes are clustered. %are mutually separated or individually condensed with low and high $\sigma$ (i.e., the degree of how well classes are clustered).
In contrast, when $\sigma$ has an intermediate value, \ltc [\CHb{}] indicate more Missing Groups and fewer False Groups distortions; hence, $t$-SNE exaggerates the degree to which classes are clustered. 

These results align well with the intent of $\sigma$. With low $\sigma$, $t$-SNE focuses more on a small number of neighbors, likely fewer than the clusters' sizes, interpreting each cluster as made of loosely-connected components in the data space. Thus, the embedding is more likely to split classes into several clusters in the embedding.
This phenomenon occurs in the Fashion-MNIST embedding (\autoref{fig:app_pp}); the \greytext{\textit{Sneaker}} class is less dense if $\sigma$ is low (\rev{region $\alpha_1$}) and relatively condensed when $\sigma$ has intermediate values (\rev{$\alpha_2$ and $\alpha_3$}).
For the latter, the number of neighbors that $t$-SNE focuses on will likely match the size of natural clusters within the original data. Therefore, $t$-SNE embeddings will tend to dismiss the inter-cluster connections, exaggerating the between-cluster distances. The number of neighbors that $t$-SNE focuses on with high $\sigma$ values will likely be bigger than the clusters' sizes. Thus, $t$-SNE will detect all data clusters as one densely-packed component and generate embeddings with smaller inter-cluster distances.

The relation between the \orangetext{\textit{Trouser}} and \redtext{\textit{Dress}} classes of the Fashion-MNIST embeddings (\autoref{fig:app_pp}) qualitatively verifies these hypotheses. Their DSC scores are almost maximum (the black circle in \autoref{fig:app_pp_heatmap}), meaning they slightly overlap in the data space. However, their distance in the embedding is exaggerated with intermediate $\sigma$ (\rev{$\beta_1$ and $\beta_2$}) compared to high $\sigma$ (\rev{$\beta_3$ and $\beta_4$}).
The same effect was observed qualitatively by Jeon et al. \cite{jeon21tvcg} while \ltc does so quantitatively. 


%We found that in \ltc [DSC], the tradeoff is weaker. 
Meanwhile, \lc [DSC] decreases slightly for intermediate values of $\sigma$ (\autoref{fig:app_hp}B dotted line). 
As \ltc [DSC] focuses more on class overlaps and less on between-class distances compared to \ltc [\CHb{}] (see D and E in \autoref{sec:eval}), it indicates that $t$-SNE preserves well the extent to which classes overlap regardless of $\sigma$.
To quantitatively validate these findings, we searched for the overlapped classes within the CLM matrices, assuming that $t$-SNE accurately depicts class overlap for all $\sigma$ values. We observed that the \greentext{\textit{Pullover}}, \purpletext{\textit{Coat}}, and \pinktext{\textit{Shirt}} classes overlap in the high-dimensional space (red circles in \autoref{fig:app_pp_heatmap}; both their DSC and \CHb class-pairwise scores are low). 
We found that these classes overlap in all embeddings in \autoref{fig:app_pp} (\rev{$\gamma_1$ to $\gamma_5$}), confirming our assumption. 


In summary, we can conclude that for non-overlapping classes in $t$-SNE embeddings, the amount of proximity between them depends essentially on $\sigma$ and is not indicative of the proximity of these classes in the data space: $t$-SNE is not trustworthy regarding the original distance between visually separated classes.  However, classes with strong overlaps in the data are depicted as overlapping in the embedding too: $t$-SNE is more trustworthy for overlapping classes. 
Such results align with the qualitative findings of Wattenberg et al. \cite{wattenberg2016tsnetuning}. 

Overall, these findings demonstrate the effectiveness of \ltc to enhance our understanding of the effect of $\sigma$ on $t$-SNE results. 
We conduct the same analysis utilizing the competitor measures we used in our evaluation (\autoref{sec:eval}); refer to Appendix E for the results. 






\subsection{Analyzing DR Techniques' Performance in Detail}

\label{sec:app_hier}

\subsubsection{Objectives and Design}

We use \ltc to analyze the quality of unsupervised DR techniques across fine-grained to coarse-grained cluster structures. 
We embed each of the previous 94 datasets using six DR techniques: $t$-SNE, PCA, UMAP, Isomap, LLE, and Densmap \cite{narayan21nature}. 
We also apply hierarchical clustering, getting 20 clustering partitions with different granularity levels for each of these datasets. 
The levels of granularity are obtained by thresholding the pairwise distances computed by Ward linkage \cite{ward64taylor} into 20 equal ranges.   
We use \ltc [\CHb{}] and \ltc [DSC] to evaluate the embeddings using each of the 20 clusterings as class labels.

\rev{
We also want to check whether the results obtained by \ltc align with the ones made by previous measures. 
We thus evaluate the embeddings using T\&C and KL divergence as representative local and global measures, respectively. We use the same hyperparameter setting with the sensitivity analysis (\autoref{sec:seneval}).
}

\subsubsection{Results and Discussions}
\autoref{fig:app_hier} depicts the results. 
LLE generates few Missing Groups distortions (highest \lc score; \autoref{fig:app_hier}B, D) at any level, but more False Groups distortions as the granularity level increases (\lt decreases; \autoref{fig:app_hier}A, C). \rev{This finding aligns with the fact that LLE obtains the worst KL divergence score among all techniques (\autoref{fig:app_hier}F).}
Such results are coherent with how LLE works, trying to reconstruct the ``local patches'' consisting of each point and its nearest neighbors while neglecting the overlap between the patches. 

There is a \lc downward trend across all other techniques as the level increases,
while \lc [DSC] shows higher scores than \lc [\CHb{}] (\autoref{fig:app_hier}B, D). 
This implies that Missing Groups distortions generally occur more for coarse-grained structures than for fine-grained ones; DR techniques exaggerate the separation between clusters at a global level.
$t$-SNE and UMAP especially give the worst  \lc scores because they focus on the preservation of local neighborhoods, casting doubts on their reliability in identifying global clusters. \rev{T\&C and KL divergence score provide strong evidence to the reliability of that claim. $t$-SNE and UMAP are in the top-2 highest ranks for T\&C but fail to do so for KL divergence. }

For \ltc except \lc [\CHb{}], PCA gets the best score at higher  granularity, suggesting that PCA is more reliable to conduct global tasks such as the density and similarity identification of clusters.
\rev{These results align with the fact that PCA earns the best score for KL divergence.}
The phenomenon confirms the experimental observation made by Xia et al. \cite{xia22tvcg}. This is also coherent with the fact that PCA embeds the data along the top two principal axes that preserve most of their variance, better representing coarse-grained structures than fine-grained ones.


We also find that Densmap, which is a variant of UMAP better preserving cluster density \cite{narayan21nature}, gets worse \lt [\CHb{}] scores than UMAP (\autoref{fig:app_hier}A) but better \lc [\CHb{}] scores (\autoref{fig:app_hier}B), at all levels.
This means that Densmap generates fewer Missing Groups but more False Groups distortions than UMAP.
As Densmap approximately maintains the cluster locations of UMAP \cite{narayan21nature}, such difference indicates that the clusters generally become bigger in Densmap compared to UMAP, hence the cluster density is relatively lower. 
Meanwhile, Densmap gets better \ltc [DSC] scores than UMAP for high granularity levels, confirming Densmap's advantage in investigating the overlap of clusters. \rev{The result is consistent with the KL divergence scores, indicating Densmap's advantage in preserving global structures when compared to UMAP (\autoref{fig:app_hier}F). }


These findings confirm the ability of \ltc to reveal the characteristics of DR methods over a wide range of clustering granularities.
\rev{
Although typical evaluation approaches of DR quality using both local and global measures (\autoref{fig:app_hier}E, F) \cite{jeon22vis, moor20icml, espadoto21tvcg} show consistent results, they 
} cannot reveal how the quality changes across granularity levels, as different measures are incomparable.

% but this approach hardly escapes from their original target granularity level of , limiting the it is possible only for a small range of graularity. 
%Analyzing DR quality consistently across a wide range of granularity levels is challenging with existing methodologies, showing a clear benefit of \ltc. 


%Overall, compared to the results with local distortion measures (\autoref{fig:tnc}), the results of \ltc reveal the tradeoff of LLE between alleviating false and missing group distortions, the increasing uncertainty regarding cluster compactness in the t-SNE and UMAP embeddings in global structures, the stronger capability of PCA to capture global data patterns, and the inaccuracy of Densmap in preserving the cluster compactness.







% \subsection{Analyzing Labeled Datasets' Difficulties in Detail}

%% maybe we can put this in the supplemental material 

% \subsubsection{Objectives and Design}

% \subsubsection{Results and Discussions}




% Here, we use \ltc along with hierarchical clustering \cite{murtagh12wires} to examine the trend of DR performance in a wide range of granularity levels. 


% Combined with hierarchical clustering \cite{murtagh12wires},  

% For example, we can make T\&C to focus on higher granularity by increasing the number of nearest neighbors they consider. Still, the strategy cannot largely cover the levels


% Different DR techniques focus on preserving the characteristics of the original high-dimensional data from different perspectives. 
% For example, while PCA aims to capture the global variance of data, $t$-SNE tries to preserve local neighborhood structure. 
% Here, due to narrow low-dimensional space, 



% The evaluation of DR embeddings is usually done by applying multiple distortion measures that focus on the different distortion types (e,g., local and global measures; refer to \autoref{sec:disme} for detail) \cite{jeon22vis, moor20icml, espadoto21tvcg}, as using a small number of measures can lead us to a biased conclusion. For instance, comparing $t$-SNE against PCA solely using global measures is unfair as $t$-SNE focuses on preserving local neighborhood structure. However, there is no consistent framework that can compare the performance of techniques 

% Using diverse distortion measures is a necessary condition in fairly comparing techniques' performance. For example, 

% Different DR techniques focus on preserving the characteristics of the original high-dimensional data from different perspectives. For example, while PCA aims to capture the global variance of data, $t$-SNE tries to preserve local neighborhood structure. 


% Dimensionality reduction methods are developed with different focuses to preserve certain data characteristics in low-dimensional representations, e.g., PCA aims to capture the global variance in data; t-SNE the neighborhood in data.
% Through \ltc, we would like to understand how the preservation performances of different DR methods change over data granularity.


