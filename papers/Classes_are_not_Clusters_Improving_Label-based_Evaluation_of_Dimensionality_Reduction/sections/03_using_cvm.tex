% \section{Notations}

% Here, we organize the notations we use throughout the paper.
% We define  $\mathbf{X}$ as a dataset of $N$ instances with $D$ attributes, in which $\mathbf{X} = \{ \mathbf{x}_i \in \mathbb{R}^D, i = 1, 2, \cdots, N\}$. 
% $\mathbf{Z}$ is a DR embedding of $\mathbf{X}$ with $d$ dimensions, where $\mathbf{Z} = \{ z_i \in \mathbb{R}^d, i = 1, 2, \cdots, N\}$ and $D > d$. 
% $\mathbf{C}$ is a partition of a dataset $\mathbf{X}$ or $\mathbf{Y}$ into $M$ classes, where $\mathbf{C}=\{C_1, C_2, \cdots, C_M\}$ 
% satisfying $C_i \cap C_j = \emptyset$ $\forall C_i, C_j \in 
% \mathbf{C}$ and $\cup^{M}_{i=1}C_i = \{1, 2, \cdots, N\}$. $\mathcal{C}$ can be driven from class labels of $\mathbf{X}$ (i.e., $\mathbf{C} = labels(\mathbf{X}) $) or a clustering technique (i.e., $\mathbf{C} = clust(\mathbf{X})$ where $clust$ denotes the type of a clustering technique). 

% $\mathbf{C}$ is a partition of $\mathbf{X}$ into $k$ classes (which can be both a result of clustering technique and class labels), where $\mathbf{C}=\{C_1, C_2, \cdots, C_k\}$ satisfying $C_i \cap C_j = \emptyset$ $\forall C_i, C_j \in 
% \mathbf{C}$ and $\cup^{k}_{i=1}C_i = \mathbf{X}$.


\section{General Label-based DR Evaluation Process}

\label{sec:cvmproblem}

The general process of label-based DR evaluation mostly relies on CVMs.
We describe what CVMs are and the process of using them to evaluate CLM. We then discuss the pitfalls of the process. 

% describe the process of using CVMs to evaluate CLM, then  and its pitfall.



\noindent
\textbf{Notations}
We define a high-dimensional data $\mathbf{X} = \{\mathbf{x}_i \in \mathbb{R}^D, i = 1, 2, \cdots, N\}$.
We denote the low-dimensional embedding of $\mathbf{X}$ as $\mathbf{Z} = \{\mathbf{z}_i \in \mathbb{R}^d \mid i = 1, 2, \cdots, N\}$, where $D > d$. 
For any set $\mathbf{S}\in\{\mathbf{X},\mathbf{Z}\}$, the distance function $\delta$ satisfies $\delta(x,y) \geq 0$, $\delta(x, y) = \delta(y,x)$ and $\delta(x,y)=0$ if $x=y$  $\forall x, y \in \mathbf{S}$.
A partition of $\mathbf{S}$ is defined as $\mathbf{P}=\{P_1, P_2, \cdots, P_k\}$ satisfying $P_i\subseteq \mathbf{S}$, $P_i \cap P_j = \emptyset$ and $\cup^{k}_{i=1}P_i = \mathbf{S}$.
If a partition is defined by class labels, we denote the partition as $\mathbf{P}_L$.
A clustering technique $C$ takes $\mathbf{S}$ and $\delta$ as input and returns a partition $\mathbf{P}_C$ of $\mathbf{S}$. 

% If we obtain a partition by running clustering technique $C$ over $\mathbf{X}$ and $\delta$, we denote the partition as $P_C$ (i.e., $P_C = C(\mathbf{X}, \delta)$). 



\subsection{Clustering Validation Measures}

\label{sec:cvmdesc}

Clustering validation measures (CVMs) evaluate how well-clustered the given partition (i.e., clustering) is in the given data. 
We use CVMs to find the optimal clustering technique or hyperparameter setting that produces the partition of the data that best matches its cluster structure.
CVMs are largely divided into two types: \textbf{internal CVM (IVM)}  \cite{liu10icdm, liu13tsmcb} and \textbf{external CVM (EVM)} \cite{wu09kdd}. 
IVMs evaluate a partition based on the internal structure of data.
Formally, the IVM score $m_{I}(\mathbf{P}, \mathbf{X}, \delta)$ quantifies how well the groups within the partition $\mathbf{P}$ of $\mathbf{X}$ are individually condensed and mutually separated in $\mathbf{X}$ based on  distance $\delta$. 
For example, the Silhouette Coefficient \cite{rousseuw87silhouette} examines how the within-group and between-group distances differ on average while using Euclidean distance as $\delta$. 
Alternatively, EVMs, such as the adjusted rand index \cite{vinh09icml}, rely on a ground truth partition $\mathbf{P}_{GT}$.
Here, the EVM score $m_{E}(\mathbf{P}, \mathbf{P}_{GT})$ simply quantifies the degree of matching between the given partition $\mathbf{P}$ and $\mathbf{P}_{GT}$, regardless of the internal cluster structure of $\mathbf{S}$. A higher score is assigned if $\mathbf{P}$ better matches with $\mathbf{P}_{GT}$. Data class labels $\mathbf{P}_L$ are typically used as ground truth $\mathbf{P}_{GT}$ \cite{farber10multiclust, jeon22arxiv2}. 



\subsection{Using CVM to Evaluate CLM}

\label{sec:cvmprocess}

We use CVMs to quantify the CLM of a DR embedding as a proxy for its reliability \cite{joia11lamp, xia22tvcg, becht19nature, yang21cellreports}.
The process depends on the type of CVM:

\noindent
\textbf{IVM-based evaluation }
For a given embedding $\mathbf{Z}$, distance function $\delta$, and class labels $\mathbf{P}_L$, $m_I(\mathbf{P}_L, \mathbf{Z}, \delta)$ represents the CLM between 
 $\mathbf{P}_L$ and $\mathbf{Z}$. 
The Silhouette Coefficient is widely adopted in the visualization community \cite{wang18tvcg, joia11lamp, loch15neurocomputing, xia22tvcg, etemadpour15ivapp}. 
The Davies-Bouldin index \cite{davies79tpami} is preferable in the context of star coordinates and Radviz \cite{angelini22tvcg, caro10pakdd}.
Notably, while Distance Consistency (DSC) \cite{sips09cgf} was designed for DR visual quality evaluation \cite{espadoto21tvcg, sedlmair12cgf, sedlmair15cgf}, it can also be viewed as a CVM since it considers only the separation of class labels in the embeddings.


\noindent
\textbf{EVM-based evaluation  }
Given $\mathbf{Z}$, $\delta$, $\mathbf{P}_{L}$, and a clustering technique $C$ providing a partition $\mathbf{P}_C = C(\mathbf{Z}, \delta)$ of the embedded data,
$m_E(\mathbf{P}_{C}, \mathbf{P}_L)$  represents CLM between $\mathbf{P}_L$ and $\mathbf{Z}$.
$K$-Means and the adjusted rand index are commonly used for $C$ and $m_E$, respectively \cite{zubaroglu20icbdr, xiang21fig, ji21jasa}. 


\rev{Notice that CVMs cannot account for the internal compactness of each class in isolation, but the CVM of a class partition will get worse if some of these classes lack compactness or split across several clusters.} 

\subsection{Pitfalls}

\label{sec:cvmassumption}
The general process of label-based DR evaluation promotes embeddings with good CLM regardless of the CLM of the original data (\autoref{sec:intro}).
In other words, the process examines the extent to which CLM is harmed in embeddings while assuming that the original data has good CLM.
Thus, if the assumption is broken, the process will frame embeddings that correctly represent overlapped classes to have False Groups distortions. 
As the process considers good CLM embeddings as high-quality ones, it is also incapable of detecting Missing Groups distortions that may arise from CLM amplification. These pitfalls were identified for the first time by Aupetit \cite{aupetit14beliv}. \rev{Our preliminary experiment confirms such a threat (Appendix D). The general process of label-based evaluation erroneously prefers DR techniques that maximize the separation among classes, instead of the ones that aim to preserve the original structure of data if the datasets have bad CLM.}
Here, we aim to introduce a new way of using class labels for DR evaluation that mitigates such a bias. 


% Here, we aim to introduce a new way that escapes from such bias. For example, 

% Previous study \cite{aupetit14beliv} proposed the modification of datasets as a potential solution. For example, we can merge overlapped classes or preserve one of them while removing the others. We can also use synthetic datasets \cite{jeon21tvcg, moor20icml, jeon22vis}, where we can guarantee that classes are well-clustered by design. However, there is no golden rule to verify that either modified or synthetic datasets have well-clustered classes \cite{jeon22arxiv2}. We can test and use the classes that are linearly or nonlinearly discriminated against each other \cite{aupetit14beliv, aupetit05neurocomputing}. However, classes can still be located close to each other even if they are separated by a boundary, which makes them hardly confirmed to be well-clustered.  Another possible solution can be using IVMs to check how well the classes are clustered in the original space \cite{jeon22arxiv}, but the problem is that we do not have a threshold to confirm well-clustered classes. The dataset with the best IVM score will certainly be valid, but using such an ideally-clustered dataset will be highly unrealistic and will hardly distinguish good and bad DR techniques. Moreover, some IVM has $\infty$ as its best value \cite{calinski74cis, davies79tpami, xie91tpami}, which allows no dataset to have the best IVM score. 

% Here, rather than finding or synthesizing datasets that fulfill the assumption, we suggest an enhanced way of using CVM (i.e., \lsc) that is free from the assumption (\autoref{sec:lsc}). 



%% intro 내용을 빼와야할듯


% For both evaluation processes of using IVM and EVM, the underlying assumption is tha

% In both 

% % \subsection{Process Description}

% % \label{sec:cvmprocess}

% The process of evaluating DR embedding using CVM differs by whether the measure is IVM or EVM. In the case of IVM, $IVM(\mathcal{C}, Y)$ 



% Another widely used strategy in evaluating cluster reliability is to rely on CVM. 
% The common strategy for utilizing CVM in DR evaluation is to use Internal CVM (IVM) \cite{liu10icdm, liu13tsmcb}. IVM is a subcategory of CVM which investigates the quality of clustering based on the internal structure of data. It gets the data and the groups (i.e., clusters) as input and checks how well the groups are individually condensed and mutually separated. For example, the Silhouette coefficient \cite{rousseuw87silhouette} examines how within-group and between-group distances differ. 
% To evaluate DR, the embedding and labeled classes are fed to IVM as input, and the output score is used to represent cluster reliability. Throughout the literature, the most widely used IVM is the Silhouette coefficient \cite{wang18tvcg, joia11lamp, loch15neurocomputing, xia22tvcg, etemadpour15ivapp}. In the context of Star coordinates \cite{} or Radviz \cite{}, the Davies-Bouldin index \cite{davies79tpami} is also commonly utilized.

% The most widely used IVM in 

% CVM-based DR evaluation can be divided into two subcategories: using Internal CVM (IVCM) 

% For instance, it is common to use the Silhouette coefficient \cite{rousseuw87silhouette} in DR evaluation \cite{joia11lamp, xia22tvcg, loch15neurocomputing}. 

% \subsection{Underlying Assumption and its Problem}

% \label{sec:cvmproblem}