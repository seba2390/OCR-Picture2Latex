


\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/sensitivity_data_ABC.pdf} \vspace{-5.5mm}
    \caption{The high-dimensional (HD) datasets and low-dimensional (LD) embeddings used in experiments A, B, and C of sensitivity analysis (\autoref{sec:seneval}). The experiments aim to check the distortion measures' ability to capture False Groups distortions. Class labels are mapped to colors. (A) The Coil-20 \cite{nene96tech} dataset and the embeddings generated by randomizing the positions of the embedded points with a certain probability. 
    (B) A HD dataset consists of six well-separated hyperballs (left) and its synthetic embeddings (right) made by initializing the embedding with six well-separated discs and gradually overlapping the discs in two different manners (B-1, 2). (C) The Fashion-MNIST \cite{xiao2017arxiv} dataset and the PCA embeddings with different numbers of principal components (PC); here we depict the UMAP projection of PCA embeddings if it has more than two PCs (i.e., dimensionality is higher than two). We depict the relation between explained variance ratio and the number of PC in the line chart next to the embeddings. \vspace{-1.5mm}}
    \label{fig:sendataabc}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/sensitivity_data_DEF.pdf} \vspace{-5.5mm}
    \caption{The low-dimensional (LD) embeddings and corresponding high-dimensional (HD) datasets represented as UMAP embeddings, used in experiments D, E, and F of sensitivity analysis 
    (\autoref{sec:seneval}) to examine distortion measures' ability to capture Missing Groups distortions.
    (D) An UMAP embedding of the Coil-20 \cite{nene96tech} dataset (right), and the variants of the Coil-20 dataset made by randomizing the coordinates of data points in HD space with a certain probability. 
    (E) A 2D embedding with six well-separated discs and synthetic HD datasets. We create the datasets by generating six 100D hyperballs and gradually overlapping them. (F) A 2D PCA embedding of the Fashion-MNIST dataset and corresponding HD datasets variants, created by slicing 20 principal components (PC) with different rankings. The line chart shows their corresponding explained variance ratio. \vspace{-2.5mm}}
    \label{fig:sendatadef}
\end{figure*}



\section{Quantitative Evaluations and Discussions}





We conduct quantitative experiments to evaluate \ltc with DSC and \CHb, i.e., \ltc [DSC] and \ltc [\CHb{}], respectively. In the sensitivity analysis  (\autoref{sec:seneval}), we check the accuracy of \ltc and competitors in quantifying distortions.
We also evaluate the runtime of the measures (\autoref{sec:scaleval}). 



\noindent
\textbf{Competitors.} 
We first consider all distortion measures without labels (\autoref{sec:disme}) as competitors. 
For global measures, we use KL divergence and DTM. T\&C and MRRE are used as representative local measures. MRRE [Missing] and MRRE [False] target Missing and False Neighbors, respectively.
We select S\&C as the sole pair of measures targeting cluster-level distortions. 
For the measures using labels (\autoref{sec:dislabel}), we first add CA-T\&C. We then select Silhouette and DSC as representative CVMs used in the general label-based evaluation. 
For T\&C, MRRE, and CA-T\&C, we average their score across $k$-nearest neighbor values: $k = [5, 10, 15, 20, 25]$, following Jeon et al. \cite{jeon21tvcg}. For KL divergence and DTM, we average the scores across different standard deviation values of Gaussian kernels $\sigma$: $[0.01, 0.1, 1]$, following Moor et al. \cite{moor20icml}. For S\&C, we use the default hyperparameter setting \cite{jeon21tvcg}. 



\subsection{Sensitivity Analysis}
\label{sec:seneval}

\label{sec:eval}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/sensitivity_exp.pdf} \vspace{-6mm}
    \caption{The results of the sensitivity analysis (\autoref{sec:seneval}; experiments A-F). Solid lines and dashed lines represent the measure that focuses on compression (e.g., False Groups, False Neighbors) and stretching (e.g., Missing Groups, Missing Neighbors), respectively. \rev{Dotted} lines represent global measures and CVMs. A pair of compression and stretching measures is represented with the same line color. 
    Measure names in red, blue, and purple correspond to our approach, the measures without labels (\autoref{sec:disme}), and the measures with labels (\autoref{sec:dislabel}), respectively.
    In summary, \lt (blue and orange bold line) and \lc (blue and orange dotted line) accurately detect Missing and False Groups distortions, respectively. Meanwhile, all other measures, including general label-based DR evaluation (i.e. DSC and Silhouette), fail to capture these distortions. \vspace{-3mm}}
    \label{fig:senexp}
\end{figure*}

We conduct six experiments (A-E) to examine \ltc{}'s sensitivity in quantifying False Groups (Fixed data and variable embeddings in experiments A, B, and C) or Missing Groups (Variable data and fixed embeddings in experiments D, E, and F) distortions.
The labeled data and embeddings used in the experiments can be found in \autoref{fig:sendataabc} (A, B, and C) and \autoref{fig:sendatadef} (D, E, and F).
In all of them, we run \ltc and competitors to evaluate the embeddings. 
% We design the first three experiments (A, B, and C) to check the sensitivity in capturing  distortions; experiments D to F were designed to do so for Missing Group distortions . 


\subsubsection{Objectives and Design}

\noindent
\textbf{Experiment A: Randomizing embeddings}
We examine whether \ltc and competitors can accurately quantify False Groups distortions. 
We generate a 2D UMAP embedding of the Coil-20 \cite{nene96tech}  dataset. We then create variants of the embedding with different levels of False Groups distortions by randomizing the location of the points.
We create 21 variants, ranging the replacement probability from 0\% (same as the original embedding) to 100\% (totally randomized) with an interval of 5\%. 
%We run \ltc and competitors to evaluate the embeddings. 
The original class assignments of Coil-20 are used as labels. 
We hypothesize that \lt will decrease as the replacement probability grows, properly capturing False Groups distortions, while \lc will ignore the distortions.
% Here, the measures that decrease as replacement probability grows are considered to properly capture False Group distortions, and the measures that stay still are regarded to properly quantify Missing Group distortions .



\noindent
\textbf{Experiment B: Overlapping discs}
We aim to check distortion measures' ability to precisely capture False Groups distortions, as with experiment A.
We create a high-dimensional dataset consisting of six hyperballs with a radius of 5 lying in 100 dimensions. 
We set the hyperballs to be equidistant ($=10$) from the origin. We then create an artificial 2D embedding consisting of six discs (radius of 1.5) evenly and equidistantly ($=4$) distributed around the origin $O$.
Data points and labels within each disc correspond to those of each hyperball.
The positions of each point within the disc and hyperball are determined randomly.
The label is also set based on the disc each point belongs to.
We gradually overlap the discs to artificially generate distortions. Here, we use two overlapping schemes to evaluate the sensitivity of \ltc in detail, resulting in two separate subexperiments (B-1, B-2). In B-1, three independent pairs of adjacent discs are overlapped; for each pair of discs $(A, B)$ with centers $C_A$, $C_B$, we adjusted $\angle C_A O C_B$ from $60^{\circ}$ to $0^{\circ}$ with an interval of $2.4^{\circ}$ (25 embedding variants in total). In B-2, we overlap all discs at once by moving them toward the origin; for each disc $A$, we gradually decrease $C_A O$ from 4 to 0 with an interval of 0.16 (25 embedding variants). 
%We evaluated the embeddings with \ltc and competitors.
We hypothesize that the \lt score will go down as False Groups distortions increase due to the overlap of the discs, while \lc will stay still. 
We also hypothesize that \lt will decrease more in B-2 than in B-1, as the overlap is larger.


\noindent
\textbf{Experiment C: Decreasing the dimension of the embedded space}
We generate False Groups distortions by decreasing the dimensionality of embedded space and check whether the measures can detect the distortions. 
We prepare the Fashion-MNIST \cite{xiao2017arxiv} as a high-dimensional dataset. We generate PCA embeddings with a decreasing number of top principal components (10 to 1 with an interval of 1; 10 embeddings in total). 
We expect the embeddings with a smaller number of principal components (i.e., embeddings lying in the space with fewer dimensions) to have more False Groups distortions as they have a smaller explained variance ratio (line chart in \autoref{fig:sendataabc}). 
We use the class assignments of the Fashion-MNIST dataset as labels.
Our hypothesis is that \lt will decrease as the dimensionality decreases, while \lc will stay still.
% We  consider the measure focusing on False Group to decrease as the dimensionality decreases as the proper measures for False Group distortions and consider the measures that stay still as proper ones quantifying Missing Group distortions .


\noindent
\textbf{Experiment D: Randomizing the original data}
We want to evaluate \ltc and competitors' capability in accurately quantifying Missing Groups distortions. 
We first generate a fixed 2D UMAP embedding of the Coil-20 \cite{nene96tech} dataset. 
We then generate the variants of the original data by mixing the points in the high-dimensional space with a fixed probability, producing Missing Groups distortions. We control the replacement probability from 0\% to 100\% with an interval of 5\%, resulting in 21 variants. 
The class assignments of the original data are used as labels.
We hypothesize that \lc will decrease as Missing Groups distortions increase (i.e., replacement probability increase), and that \lt will ignore the distortions.
% Here, proper measures that focus on False Group distortions should stay still, while the one focusing on Missing Group distortions should decrease as the replacement probability increase. 


\noindent
\textbf{Experiment E: Overlapping hyperballs}
We want to evaluate whether \ltc and competitors can precisely capture Missing Groups distortions. 
We prepare variants of high-dimensional data and fixed low-dimensional embedding consisting of six 100D hyperballs and corresponding 2D discs, respectively. 
The points within the same disc have the same label. All discs are well separated from each other.
We artificially overlap hyperballs to generate Missing Groups distortions. 
For each hyperball $A_H$, we gradually decrease $C_{A_H}O$ from 4 to 0 with an interval of 0.16 (25 variants in total). %We fix the embedding while using the variants as high-dimensional data.
We hypothesize that \lc will decrease as hyperballs overlap, while \lt will stay still.



\noindent
\textbf{Experiment F: Decreasing the dimension of the original data space}
We examine whether the distortion measures can detect the Missing Groups distortions made by the decrease in the dimensionality of the original data. We prepare a 2D PCA embedding of the Fashion-MNIST dataset. 
We then select ten 20D PCA embeddings with different sets of principal components as high-dimensional datasets; the $i$-th dataset variant consists of the $(i)$-th to $(i+19)$-th principal components, where $1 \leq i \leq 10$.
We expect the dataset with a higher order to have more Missing Groups distortions over the embedding as they have a smaller explained variance ratio (line chart in \autoref{fig:sendatadef}). 
We used the class assignments of the Fashion-MNIST dataset as labels.
We hypothesize that \lc will decrease as the starting index of principal components increases, while \lt will stay still.
%The measures that decrease  are considered proper measures for False Group distortions ; the measures that stay still are regarded as proper ones for Missing Group distortions .



\subsubsection{Results}

\autoref{fig:senexp} shows the results of our experiments that we comment on below. %. Here, we provide detailed descriptions for each result.

\noindent
\textbf{Experiment A} As the randomization probability grows, both \lt [DSC] and \lt [\CHb{}] similarly decrease linearly while \lc [DSC] and \lc [\CHb{}] slightly increase, confirming our hypothesis. 
Meanwhile, S\&C and local measures decrease regardless of the distortion type, while global measures slightly increase.
In the case of label-based measures, both CA-T\&C and the general CVM-based process (DSC and Silhouette) show mainly decreasing scores.

\noindent
\textbf{Experiment B} In B-1, as the overlap between the discs grows, both \lt [DSC] and \lt [\CHb{}] decrease in a similar manner, while \lc{}s stay still. 
Such results validate our hypothesis, confirming \ltc's capability in properly detecting False Groups distortions. 
Meanwhile, S\&C,  T\&C, and MRREs all decrease, while Steadiness, Trustworthiness, and MRRE [False] decrease more than Cohesiveness, CA-Continuity, and MRRE [Missing], respectively. 
Global measures stay still.
CA-T\&C partially succeed in properly detecting False Groups distortions; both CA-Continuity and CA-Trustworthiness decrease, but CA-Continuity's decrement was subtle compared to the one of CA-Trustworthiness. 
CVMs show a decreasing trend.
In B-2, the amount of decrement becomes bigger than in B-1 for \lt [DSC] and \lt [\CHb{}] while \lc{}s again stay still, confirming our second hypothesis.
The amount of decrement also becomes bigger than in B-1 for  T\&C, MRREs, and Cohesiveness, while Steadiness showed a similar drop as in B-1. 
In the case of KL divergence, DTM, and Silhouette, the patterns are almost identical to B-1 except that the scores rebound when the discs are nearly overlapped. 
The decrement becomes bigger also for CA-T\&C and DSC.


\noindent
\textbf{Experiment C} As the number of PCs decreases, \lt{}s decrease while \lc{}s stay still, validating our hypothesis. 
Global measures (KL divergence, DTM) stay still while all other measures decrease.

\noindent
\textbf{Experiment D} As we increase the randomization probability, both \lc [DSC] and \lc [\CHb{}] decrease, while \lt{}s stay still, verifying our hypothesis. However, while \lc [DSC] decreases right before the data are perfectly mixed, \lc [\CHb{}] decreases from the start. 
For local measures, both T\&C and MRREs decrease. 
Steadiness decreases, while Cohesiveness suddenly goes up after decreasing for a while.
Global (KL divergence, DTM) measures increase in general.
CA-Trustworthiness goes down while CA-Continuity stays still, and CVMs (DSC and Silhouette) stay still. 

\noindent
\textbf{Experiment E.} When the overlap between hyperballs increases, both \lc [DSC] and \lc [\CHb{}] decrease, while \lt{}s stay still, verifying our hypothesis. However, as in experiment D, \lc [DSC] and \lc [\CHb{}] decrease differently; while \lc [DSC] decreases right before the hyperballs perfectly overlap, \lc [\CHb{}] decreases before \lc [DSC] does. 
Meanwhile, local measures (T\&C, MRRE) decrease, while global measures (KL divergence, DTM) stay still.
Steadiness decreases while Cohesiveness temporarily pops up when Steadiness starts to decrease. 
CA-Trustworthiness maintains a maximum score while the CA-Continuity score increases before the perfect overlap of the hyperballs. CVMs stay still. 


\noindent
\textbf{Experiment F.} The results confirm our hypothesis;  as the starting index of the PCs that we slice increases, both \lc [DSC] and \lc [\CHb{}] decrease while \lt{}s stay still. 
Local measures (T\&C, MRRE) decrease, and global measures (KL divergence, DTM) stay still.
S\&C decrease, while Steadiness decreases more than Cohesiveness.
CA-T\&C show a similar trend; CA-Trustworthiness decreases, while CA-Continuity decreases to a smaller extent. CVMs stay still.

\subsubsection{Discussions}

\noindent
\textbf{\ltc and competitors' capability in detecting cluster-level distortions.}
The results from experiments A-C confirm that \lt is sensitive to False Groups distortions, while \lc is not, as we intended. 
Moreover, the difference between the B-1 and B-2 results validates \lt{}'s accuracy at measuring the amount of False Groups distortions.
The results from experiments D-F, on the other hand, confirm that \lc accurately captures Missing Groups distortions, while \lt ignores them.

The results also validate that previous measures fail to accurately detect the distortions or to distinguish specific distortion types.
Global measures (KL Divergence, DTM) hardly discover distortions for all six experiments. 
Local measures (T\&C, MRRE) fail to pinpoint specific distortion types; all measures decrease regardless of the type of distortion they aim to measure.
Cluster-level measures (S\&C) fail to distinguish False Groups distortions in experiments A-C. 
For experiments D-F, the situation is even worse; Steadiness reacts more sensitively to Missing Groups distortions although it was originally designed to aim at False Groups distortions.
CA-T\&C succeed in pinpointing False Groups distortions for B-1, but fails to do so for the remaining experiments. 

The general process of label-based DR evaluation based on CVMs (DSC and Silhouette) succeeds in detecting the False Groups distortions in experiments A-C.
However, in experiments D-F, the process fails to detect Missing Groups distortions. Moreover, the process does not have a specific focus on distortion type and thus cannot explain whether the False or Missing Groups distortions occurred.
Such results confirm the threat of using the general label-based evaluation of DR in practice, providing clear evidence for adopting \ltc instead.



% In experiments A-C, \lt{}s decrease while \lc{}s stay still, which indicates that \ltc precisely detects False Group distortions . Moreover, as the decrement of \lt{}s is significantly bigger in B-2 than in B-1, we can conclude that \ltc also has the ability to sensitively detect the amount of distortion that occurred.
% On the contrary, previous cluster measures and local measures are not able to pinpoint False Group distortions . 
% For all Experiments A, B, and C, they decreased regardless of whether they focus on stretching or compression. Moreover, they failed to sensitively capture the difference in the amount of False Group distortions between B-1 and B-2. 
% Global measures also have trouble detecting distortions for all three experiments. CVMs (i.e., the current way of evaluating DR based on label separability) succeed to detect the False Group distortions, but as they do not consist of a pair of measures targeting stretching and compression, 

% On the other hand, experiments D-F provide the evidence for the claim in which \ltc can precisely capture Missing Group distortion. As in experiments A-C, \ltc solely captures and distinguishes Missing Group distortion. Local measures fail to pinpoint Missing Group distortions as both the stretching- and compression-focusing measures decrease. For cluster measures, the situation is worse; Cohesiveness and CA-Continuity failed to capture Missing Group distortions for all three experiments.
% Moreover, in experiment F, Steadiness and CA-Trustworthiness, which focuses on compression distortion, decreased instead of their counterparts. This means that using S\&C or CA-T\&C has the threat of making an unreliable conclusion about DR performance. The results also indicate that global measures and CVMs hardly capture Missing groups distortion. 


% In summary, the experiments confirm that \ltc can precisely capture and distinguish Missing and False Group distortions, while previous measures lack the capability to do so. 


\noindent
\textbf{Effect of CVM choice on \ltc.}
\ltc{}s with two different CVMs (DSC or $CH_{btwn}$) show a consistent pattern in experiments A-C. However, they behave differently in experiments D and E; 
%\lc [DSC] decreases for larger CLM perturbation (right hand side), while 
\lc [$CH_{btwn}$] starts decreasing for the lower level of generated CLM distortions than \lc [DSC]. This observation may be CVM-specific as DSC and \CHb use different schemes in examining how the classes are clustered. 
In \lc [DSC], the score only drops when classes overlap. 
% For example, in experiment E, the proximity between hyperballs only does not affect  DSC unless they are linearly separable. 
Therefore, \lc [DSC]  is sensitive to Missing Groups distortions only if the \textit{overlapped} classes in the original space are more separated in the embedding. 
In contrast, $CH_{btwn}$ decreases as the proximity between classes increases, whether the classes overlap or not. Thus, when proximity increases, \lc [\CHb{}]  is more sensitive to Missing Groups distortions than \lc [DSC].
The results indicate that \CHb has a larger range of variation, being more sensitive to CLM than DSC, but it is less sensitive to class overlap. %does not sensitively regard the overlap of classes,
Creating a CVM both sensitive to CLM and class overlap while fulfilling our requirements (\autoref{sec:req}) constitutes an interesting future work. 

% cannot cover the full range of distortion

% The fact that \ltc [DSC] and \ltc [\CHb{}] capture Missing Group distortions at different scales validates the benefit of parametrization. We believe that using  both CVM options will make the evaluation of DR embeddings richer. Designing and testing new CVMs that fulfill the invariance requirements 

% If two hyperballs are linearly separable, the distance between their centers does not affect the DSC score regardless of their proximity. Therefore, \lc [DSC]
% regards the high-dimensional datasets to generate Missing Group distortions only if the hyperballs are sufficiently close and overlap with each other. 
% On the other hand, $CH_{btwn}$ decreases as the distances between the centers of hyperballs decrease, which does not necessarily require the overlap of spheres. \lc [\CHb{}] therefore considers Missing Group distortions to happen early.  

\noindent
\textbf{Discussions on the competitors.}
We discuss the patterns shown by competitors with more detail in Appendix B.


\rev{

\subsubsection{Sensitivity Analysis with the Class Labels Generated by Clustering Techniques}

We want to validate whether the results of our study are replicable with the labels that come from other sources. We thus conduct experiments A-F while generating class labels with clustering techniques (Appendix F). We find that \ltc show consistent results regardless of the sources of labels, while the general label-based DR evaluation process (i.e., CVMs) fails to do so. Such results confirm the robustness of the \ltc in evaluating the quality of DR embeddings.

}




%% TO APPENDIX
% \noindent



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/scalability.pdf}
    \vspace{-8.5mm}
    \caption{Results of the scalability analysis. Name and line colors match with \autoref{fig:senexp}. \ltc [DSC] (dark blue) is on par with CVMs (Silhouette, DSC), while \ltc [\CHb] is similar to most of the other measures. S\&C is the slowest. \vspace{-4mm}}
    \label{fig:scal}
\end{figure}



\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/perplexity.pdf} \vspace{-6mm}
    \caption{$t$-SNE embeddings of Fashion-MNIST \cite{xiao2017arxiv} data with diverse perplexity ($\sigma$) values. Combined with the class-pairwise CLM of the original dataset (\autoref{fig:app_pp_heatmap}), the patterns in the embeddings qualitatively support the findings about the effect of $\sigma$ revealed by \ltc (\autoref{fig:app_hp}; \autoref{sec:apptsne}). \vspace{-2mm}}
    \label{fig:app_pp}
\end{figure*}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/app_hp.pdf} \vspace{-6mm}
    \caption{Overall reliability of $t$-SNE embeddings according to the $\sigma$ value quantified by \ltc [DSC] and \ltc [\CHb{}]. For each $\sigma$ value, we average the score of the embeddings generated from 94 labeled datasets (95\% confidence interval shaded).  \vspace{-2mm}}
    \label{fig:app_hp}
\end{figure}



\subsection{Scalability Analysis}


\label{sec:scaleval}

\subsubsection{Objectives and Design}

We evaluate the scalability of \ltc against the competitors. 
We gather 96 labeled datasets \cite{jeon22arxiv2} that vary in dimensionality,  the number of data points, and the number of classes. We exclude two datasets as the implementation of S\&C provided by the authors\footnote{\href{https://github.com/hj-n/steadiness-cohesiveness}{github.com/hj-n/steadiness-cohesiveness}} fails to process them, resulting in 94 datasets (Appendix C).
We generate embeddings using $t$-SNE, UMAP, PCA, and random projection for all 94 datasets.
We check the overall execution time applying all measures to the embeddings, adding up the running times of the measures run in pairs (\ltc, T\&C, MRRE, S\&C, and CA-T\&C). We use the provided implementation for S\&C and scikit-learn \cite{pedregosa11jmlr} for the Silhouette.  We implement the remaining measures in Python with Numba parallel computing \cite{lam15llvm} to maximize the scalability.
We run the experiments on a Linux server with 40-core Intel Xeon Silver 4210 CPUs.

\subsubsection{Results and Discussion}

\autoref{fig:scal} show that the running time of \ltc highly depends on the CVM. Among all measures, DSC is the fastest, followed by \ltc [DSC]. If $CH_{btwn}$ is used as the CVM, \ltc becomes less scalable. Still, \ltc [$CH_{btwn}$] has scalability similar to local (T\&C, MRRE) and global (KL Divergence, DTM) measures and to CA-T\&C,  all being more than twice faster than S\&C.

