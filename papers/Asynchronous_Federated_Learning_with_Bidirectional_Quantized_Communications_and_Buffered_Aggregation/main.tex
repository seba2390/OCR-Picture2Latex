%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass[nohyperref]{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{enumitem}%for item spacing
\usepackage{booktabs} % for professional tables

\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\crefname{assumption}{assumption}{assumptions}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{\hfill Async FL with Bidirectional Quantized Communications \hfill \thepage}
\icmltitlerunning{\hfill Asynchronous FL with Bidirectional Quantized Communications and Buffered Aggregation \hfill \thepage}


% My stuff
\usepackage{enumitem}
\usepackage{xspace}
\newcommand{\algname}{{QAFeL}\xspace} % Could be QuanBuff, FedBuffQuan, QuanFedBuff, BiQuanFedBuff
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\expec}[1]{\mathbb{E}\left[#1\right]}

\newcommand{\order}[1]{\mathcal{O}\left(#1\right)}
\newcommand{\calQ}{\mathcal{Q}}
\newcommand{\calS}{\mathcal{S}}

\DeclareMathOperator{\topk}{top}
\DeclareMathOperator{\randk}{rand}
\DeclareMathOperator{\qsgd}{qsgd}
\DeclareMathOperator{\sign}{sign}

\usepackage[most]{tcolorbox}
\newtcbox{\mybox}[1][]{nobeforeafter,tcbox raise base,colframe=green!50!black,colback=green!10!white,top=0pt,bottom=0pt,left=0pt,right=0pt,before upper=\strut,#1}

\renewcommand{\algorithmicrequire}{\textbf{input:}}
\renewcommand{\algorithmicensure}{\textbf{output:}}

\usepackage{multirow, booktabs}

\begin{document}

\twocolumn[
    \icmltitle{Asynchronous Federated Learning with Bidirectional Quantized Communications and Buffered Aggregation}


    % List of affiliations: The first argument should be a (short)
    % identifier you will use later to specify author affiliations
    % Academic affiliations should list Department, University, City, Region, Country

    % You can specify symbols, otherwise they are numbered in order.
    % Ideally, you should not use this facility. Affiliations will be numbered
    % in order of appearance and this is the preferred way.
    \icmlsetsymbol{equal}{*}

    \begin{icmlauthorlist}
        \icmlauthor{Tomas Ortega}{uci}
        \icmlauthor{Hamid Jafarkhani}{uci}

    \end{icmlauthorlist}

    \icmlaffiliation{uci}{Center for Pervasive Communications \& Computing, University of California, Irvine, USA}

    \icmlcorrespondingauthor{Tomas Ortega}{tomaso@uci.edu}

    % You may provide any keywords that you
    % find helpful for describing your paper; these are used to populate
    % the "keywords" metadata in the PDF but will not be shown in the document
    \icmlkeywords{Federated Learning, Asynchronous, Quantized Communications, Compressed Communications}

    \vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{} % see icml2023.sty, \ICML@appearing, for the footnote

\begin{abstract}
    Asynchronous Federated Learning with Buffered Aggregation (FedBuff) is a state-of-the-art algorithm known for its efficiency and high scalability.
    However, it has a high communication cost, which has not been examined with quantized communications.
    To tackle this problem, we present a new algorithm (\algname), with a quantization scheme that establishes a shared ``hidden'' state between the server and clients to avoid the error propagation caused by direct quantization.
    This approach allows for high precision while significantly reducing the data transmitted during client-server interactions.
    We provide theoretical convergence guarantees for \algname and corroborate our analysis with experiments on a standard benchmark.
\end{abstract}

% Do not include author information or acknowledgements in your initial submission.
% Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited. For the final version of the paper, authors can add one extra page to the main body.
% ICML is double blind, but preprints submitted to arxiv are admissable
% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% Keep your abstract brief and self-contained, limiting it to one paragraph and roughly 4--6 sentences. Gross violations will require correction at the camera-ready phase.

\section{Introduction}
\label{sec:introduction}

Federated Learning (FL) is a distributed machine learning paradigm that enables training of models on decentralized data, without the need to share raw data~\cite{communication_efficient}.
It has gained significant attention in recent years for its ability to mitigate privacy concerns that come with collecting sensitive information from users in a central location.
Currently, FL is applied to various domains, such as natural language processing, computer vision, and healthcare~\cite{advances_open_problems}.

Many FL algorithms have been widely studied, such as FedAvg~\cite{fedavg_conv_Li}, FedProx~\cite{fedprox}, and FedSGD~\cite{communication_efficient}.
These algorithms operate in a synchronous manner, i.e., all clients send updates to a central server in synchronized rounds.
Since large-scale and dynamic systems are naturally asynchronous \cite{async_hetero}, there is a growing interest in studying asynchronous FL methods, where different clients can update their models and communicate with the server at different times.
While asynchrony introduces additional challenges such as stale gradients and stragglers, it eliminates the need to fit clients into time slots and allows the handling of clients that are slow to respond or have limited communication capabilities~\cite{async_edge}.

Fedbuff~\cite{FedBuff} is an asynchronous FL algorithm that introduces a buffer on the server side to store client updates before performing a global model update.
This is in contrast to previous versions of asynchronous FL, where the server sent a model update every time it received a client update, and the communication cost grew too much with the number of clients.
FedBuff is also compatible with privacy-preserving technologies, has theoretical convergence guarantees under weak assumptions, and is robust to client heterogeneity.
Its superiority over synchronous FL methods in terms of efficiency and fairness has been studied in~\cite{papaya}.
However, its communication cost has not been analyzed in the presence of communication compression.
In this paper, we address this issue with the integration of a bidirectional quantization scheme.
It is of particular interest to investigate the compound error produced by staleness and quantization, which results in a cross-error term that is not present in the separate analysis of both effects.
For more related work, see~\cref{appsec:related-work}.
\vspace{-.1cm}
\paragraph{Contributions.} As our key contributions, we
\begin{itemize}[topsep=0pt, noitemsep]
    \setlength{\itemsep}{0pt}%
    \item Integrate a bidirectional quantization scheme into FedBuff to create a \emph{``hidden'' shared state} between the server and clients and \emph{reduce communication costs}, while avoiding the error propagation caused by direct quantization. We call this algorithm Quantized Asynchronous Federated Learning (\algname{}).
    \item Provide a theoretical analysis of \algname{}'s convergence rate. We show that quantization does not change the complexity order and prove that \emph{FedBuff's rate can be recovered} in the limit of infinite precision quantization.
    \item Show that the \emph{cross-error} term is of smaller order than the individual error from \emph{staleness} and \emph{quantization}.
    \item Validate our findings through an experimental evaluation on a standard benchmark~\cite{LEAF}.%: determining if a celebrity is smiling or not in an image~\cite{LEAF, celeba}.
\end{itemize}


\section{System description}
\label{sec:system}
In both \algname and FedBuff, clients train asynchronously and send their local updates to the server when they have finished training.
Simultaneously, the server accumulates local updates in a buffer until it has reached its maximum capacity and then produces a server model update.
In FedBuff, every update is a \emph{full-precision} model.
Current machine learning models can be over tens of millions of parameters large; for example, ResNet-18 has around 11 million trainable parameters~\cite{resnet}.
If floating point numbers are stored in a standard 4 byte format, each client has to upload approximately 44 MB per update.
Since training occurs over thousands of iterations, clients would need to upload information of the order of GB or more.

To alleviate this communication cost, \algname \emph{compresses updates} using a quantizer, i.e., a lossy compressor defined as follows~\cite{error_feedback, optimal_compression, new_simpler_ef}.
% A quantizer includes an encoder that generates B bits (your example is a fixed-rate quantizer with 4 bits) and a decoder that looks up the quantized value from a table that is shared by both sides or a function agreed upon. Technically, this definition is not completely correct as it is the combination of the encoder and decoder (which is fine), but then the quantizers' in Fig. 1 include both encoder and decoder although encoder is at one side (e.g. client) and the decoder at the other side (e.g. the server). Perhaps, you want to keep the description but in the future, make sure that you have the correct presentation. If you're not sure about the role of encoder and decoder, please read the tutorial on quantization by Gray: A precise definition includes two functions: Enc(x)=i generates B bits in your case and Dec(i)=Q(x). Well, B is the compression parameter and \delta is a consequence of how you choose B.
\begin{definition}[Quantizer] \label{def:quantization}
    A quantizer, denoted by $Q: \mathbb{R}^d \to \mathbb{R}^d$, is a (possibly random) function that satisfies the following condition:
    \begin{equation} \label{eq:quantization}
        \mathbb{E}_Q \left[ \norm{Q(x) - x}^2 \right] \leq (1-\delta) \norm{x}^2,
    \end{equation}
    where $\delta > 0$ is a compression parameter and $\mathbb{E}_Q$ denotes the expectation with respect to the internal randomness of the quantizer $Q$.
    We use the terms quantizer and compression operator indistinctly in this work.
\end{definition}
Using a quantizer allows us to send messages with fewer bits compared to the full model.
\cref{fig:system_block_diagram} sketches the system block diagram in which the server broadcasts the quantized updates.
\begin{figure}[htbp]
    \centering
    \includegraphics[trim=0 .2cm 0 0,clip,width=\linewidth]{figures/system_diagram_2.pdf}
    \caption{\algname system block diagram, with $\qsgd$ example quantizers. For the definition of a $\qsgd$ quantizer, see \cref{example:quantizers}.}
    \label{fig:system_block_diagram}
\end{figure}
Both \algname and FedBuff can operate in networks with or without broadcast capabilities.
In this paper, we assume that both operate in the broadcast mode, that is, the server broadcasts the global update once its buffer is full.
For a note regarding the non-broadcast version, see~\cref{appsec:non-broadcast}.

Let us now describe how \algname works, and illustrate it with an execution timeline example in \cref{fig:timeline_diagram}, similar to the FedBuff analysis done in Figure 4 of \cite{papaya}.
\begin{figure}[htbp]
    \centering
    \includegraphics[trim=0 .1cm 0 0,clip,width=\linewidth]{figures/QAFeL3colors.pdf}
    \caption{\algname example timeline, with a server with a buffer for $K=2$ samples. Black arrows from clients to the server indicate quantized messages. Black arrows from server to the hidden model indicate a quantized broadcast message. Note that the hidden model is drawn separately from the server and the clients to indicate that it is synchronized.}
    \label{fig:timeline_diagram}
\end{figure}
The main difference with respect to FedBuff is that \algname uses a ``hidden'' model, or hidden state, that is shared between the clients and the server.
In practice, this is a model saved in the server's memory and each client, and is used as an approximation to the server model.
Note that the hidden model is different from a direct quantization of the server model, and is used to avoid propagating errors.
To begin training, both the server and the clients start with an initial pre-agreed upon model $x^0$, which is used to initiate the hidden state.
The server then asynchronously samples the clients and requests them to compute a local update.
A requested client will copy the current hidden state, $y_0 \leftarrow\hat x^t$, and perform $P$ local updates using the equation
\begin{equation}
    y_p \leftarrow y_{p-1} - \eta_{\ell} g_p(y_{p-1} ),
\end{equation}
where $\eta_{\ell}$ is the local learning rate and $g_p$ is a noisy gradient.
After the local updates are computed, the client sends the \emph{quantized} difference $Q_c(y_{P-1} - y_0)$ to the server to aggregate.
The server accumulates these updates in a buffer until it has $K$ samples and then performs a global update on the model using the equation
\begin{equation}
    x^{t+1} \leftarrow x^t + \eta_g \frac{\overline{\Delta}^t}{K},
\end{equation}
where $\overline{\Delta}^t$ is the sum of the local updates from the buffer.
The server then updates the hidden state by computing $q^t \leftarrow Q_s(x^{t+1} - \hat x^t)$ and broadcasting it to the clients.
The clients have a process in the background that collects $q^t$.
Then, both the server and the clients perform the hidden state update
\begin{equation}
    \hat x^{t+1} \leftarrow \hat x^t + q^t.
\end{equation}
The full pseudocode for \algname can be found in~\cref{appsec:pseudocode}.
Also, for a note regarding \algname's privacy considerations, see~\cref{appsec:privacy}.


\section{Formulation and convergence analysis}
\label{sec:formulation}
Let us formalize the problem as a minimization of the following sum of stochastic functions:
\begin{equation} \label{eq:minimization_problem}
    \min_{x \in \mathbb{R}^d} f(x) := \frac{1}{N} \sum_{n=1}^N \left( F_n(x) :=  \mathbb{E}_{\zeta_n}[F_n(x;\zeta_n)] \right),
\end{equation}
where $F_n$ is the loss function on Client $n$ and $N$ is the total number of clients.
Each function $F_n$ depends only on data collected locally, i.e., on Client $n$.
Our results can be easily extended to the weighted sum case.


Let us assume that $f$ achieves a minimum value $f^*$.
We make the standard assumptions from the literature \cite{adaptive-fl-optimization, fedavg_conv_Li, stich2018local, Yu-speedup, SCAFFOLD}, which are also used for FedBuff's analysis, \emph{except} the bounded hetereogenity assumption, which is not needed in our proof.
\begin{assumption}[Unbiased stochastic gradient]
    \label{ass:unbiased-stochastic-gradient}
    We assume that for all clients, $\mathbb{E}_{\zeta_n}[g_n(x; \zeta_n))] = \nabla F_n(x)$, $\forall x \in \mathbb{R}^d$.
\end{assumption}
\begin{assumption}[Bounded local variance]
    \label{ass:bounded-local-variance}
    We assume that for all clients, $$\mathbb{E}_{\zeta_n}[\norm{g_n(x; \zeta_n) - \nabla F_n(x)}^2] \leq \sigma^2_{\ell}, \quad \forall x \in \mathbb{R}^d.$$
\end{assumption}
\begin{assumption}[Bounded and L-smooth loss gradient]
    \label{ass:bounded-and-l-smooth-loss-gradient}
    We assume that each function $F_n$ is $L$-smooth, that is,
    $$\norm{\nabla F_n(x) - \nabla F_n(x')} \leq L \norm{x - x'}, \quad \forall x,x'\in \mathbb{R}^d,$$
    and its variance is bounded, i.e., $\norm{\nabla F_n}^2 \leq G$.
\end{assumption}

We also make one additional assumption, which is not standard for synchronous FL, but was introduced in \cite{FedBuff} for the buffered asynchronous setting.
\begin{assumption}[Bounded staleness when $K=1$]
    \label{ass:bounded-staleness}
    For all clients $n\in \{1, \ldots, N\}$ and for each server step $t$, the staleness $\tau_n(t)$, i.e., the difference in model versions between when Client $n$ begins local training and when its updates are applied to the global model, is not greater than a maximum allowed staleness, $\tau_{\max, K}$, when the buffer size $K = 1$.
\end{assumption}
As is the case with FedBuff, it is worth noting that the upper bound on staleness depends on the buffer size, $K$.
As the buffer size increases, the server updates less frequently, which reduces the number of server steps between when a client starts training and when its updates are applied to the global model.
If \cref{ass:bounded-staleness} is met, for any $K>1$, the maximum delay, $\tau_{\max,K}$, is at most $\lceil \tau_{\max,1}/K \rceil$; this is proven in Appendix A of \cite{FedBuff}.

\begin{proposition}[Complexity order.] \label{prop:complexity_order}
    Let us define the convergence rate as $R = \frac{1}{T} \sum_{t=0}^{T-1} \expec{\norm{\nabla f(x^t)}^2}$.
    Under \cref{ass:unbiased-stochastic-gradient,ass:bounded-local-variance,ass:bounded-and-l-smooth-loss-gradient,ass:bounded-staleness}, with local and global learning rates satisfying Condition \eqref{eq:condition_learning_rates} in \cpageref{eq:condition_learning_rates}, $\eta_\ell = \order{1 / (K\sqrt{TP})}$ and $\eta_g = \order{K}$, and defining $F^* := f(x^0) - f^*$,
    \begin{multline}
        R_{FedBuff} = \order{\frac{F^*}{\sqrt{TP}}}  + \order{ \frac{\sigma_{\ell}^2 + G}{K\sqrt{TP}} } \\
        + \order{\frac{ (\tau_{\max, 1}^2 + 1) (\sigma_{\ell}^2 + PG)}{TK^2}}.
    \end{multline}
    Furthermore, with an unbiased client quantizer $Q_c$,
    \begin{equation} \label{eq:R_algname}
        \begin{aligned}
            R_{\algname} & \leq R_{FedBuff} + \order{ \frac{(1-\delta_c)(\sigma_{\ell}^2 + G)}{K\sqrt{TP}} } \\
                         & + \order{\frac{(2-\delta_c)(\sigma_\ell^2 + G)}{\delta_s TK} }                    \\
                         & + \order{\frac{ \tau_{\max, 1}(1-\delta_c) (\sigma_{\ell}^2 + PG)}{TK^2} }.
        \end{aligned}
    \end{equation}
\end{proposition}
Note that the gradient bound $L$ is assimilated into the $\order{\cdot}$ notation, as done in FedBuff's analysis\footnote{The rate that appears in the original AISTATS 2022 paper has a minor error resulted from inaccuracy in Eq. (20) of that paper.}.
\cref{prop:complexity_order}'s proof is in \cref{appsec:convergence-analysis-proof}, where a version for unbiased client quantizers is also outlined.
Moreover, it is also shown how a geometric partial sum in the previous expression is bounded with $1/\delta_s$, but taking the limit without this bound shows that the orders $ \lim_{\delta_c, \delta_s \to 1} R_{\algname} = R_{FedBuff}$.
In other words, \algname recovers FedBuff's convergence rate in the case of infinite precision quantization.

There are three error terms in  \eqref{eq:R_algname}; (i)
the choice of client quantizer with an order $\order{1/\sqrt{T}}$; (ii) the choice of server quantizer with smaller order $\order{1/T}$; and (iii) the cross-error term from client quantization and staleness also with smaller order $\order{1/T}$.
The effects of the server quantizer and the cross-error term dissipate in time faster than the effect of the client quantizer.
Therefore, the choice of the client quantizer will affect the error order more than the choice of the server quantizer and also more than the staleness and quantization cross-error.

\begin{figure*}
    \centering
    \hfill
    \includegraphics[width=.4\textwidth]{figures/trips.pdf}
    \hfill
    \includegraphics[width=.4\textwidth]{figures/mbs.pdf}
    \hfill
    \caption{\algname and FedBuff's communication metrics to reach a validation accuracy (90\%) for different concurrency values (clients training in parallel). \algname is using 4-bit $\qsgd$ quantization at both server and client, therefore the MB broadcasted are simply the MB uploaded divided by the buffer size, which is 10.
    }
    \label{fig:Fig3FedBuff}
\end{figure*}
\section{Simulation results}
\label{sec:experiments}

We conduct a series of simulations to evaluate \algname's performance and confirmed our theoretical derivations.
Our simulations show how \algname's communication load is $8$ times smaller than that of FedBuff while maintaining the same convergence speed.

\paragraph{Setup.}
A detailed description of the experimental setup is provided in \cref{appsec:experimental-details}.
The parameters are adopted from~\cite{FedBuff} unless they are not defined in that paper.
The main traits are (i) We use the same hyperparameters as FedBuff, (ii) we model clients arriving at a constant rate, and (iii) to simulate the time delay between a client's download and upload operation, we sample from a half-normal distribution.
This distribution is selected as it provides the most accurate representation of the delay distribution observed in Meta's production FL system, see Appendix C of \cite{FedBuff}.

In addition to the standard metric for comparing synchronous and asynchronous FL methods (aka the number of client trips), we also present the number of bytes sent per message to illustrate \algname's benefits.

\paragraph{Dataset and model.}
We utilize the CelebA dataset and model inherited from \cite{FedBuff}, which follows the configuration of the standard LEAF benchmark \cite{LEAF}.
The CelebA dataset~\cite{celeba} is a large-scale image classification dataset featuring celebrity faces with a total of 202,599 images and 10,177 annotated identities.
The images in CelebA exhibit diverse variations in pose, expression, and appearance.
Our task is to detect whether a celebrity is smiling or not.

Our model's high-level architecture consists of a four-layer convolutional neural network (CNN) binary classifier, with a stride of 1, a padding of 2, and a dropout rate of 0.1.

We use 4-bit $\qsgd$ quantization for both client and server -- see \cref{example:quantizers} for a definition of $\qsgd$ quantization and other examples.
The choice of quantizer follows the results in \cref{fig:threefigures}, where we can see \algname's performance for different combinations of $\qsgd$ quantizers.
The results of \cref{fig:threefigures} are clearly consistent with the analysis presented in \cref{sec:formulation}, as the effect of the server quantizer is less pronounced than that of the client quantizer.

\paragraph{Results.}
\label{par:results}
We perform each experiment three times and report the mean and standard deviation of the client uploads.
\cref{fig:Fig3FedBuff} illustrates how \algname's communication costs are lower than those of FedBuff: we decrease the MB uploaded by  $5.2-8$ times and similarly decrease the MB broadcasted.
The number of client updates is only $1-1.5$ times higher, but using 4-bit $\qsgd$ quantization in both client and server accounts for approximately a $8$ times reduction in message size.
% Note that the decrease in the upload and broadcast required communication includes the extra client updates.
Note that the decrease in the total MB required for upload and broadcast include the extra client updates.
Additional results can be found in~\cref{appsec:additional-results}.


\section{Conclusion}
\label{sec:conclusion}
We study the impact of bidirectional quantization on the convergence rate of buffered asynchronous FL.
Using a hidden state scheme, we avoid error propagation.
We show that the cross-error term is of smaller order compared to the individual terms corresponding to staleness and quantization.
Empirical evaluation corroborates our analysis and shows how client quantization affects \algname's performance more than server quantization.
The theoretical analysis presented in this work can serve as a foundation for the development of quantization schemes that ensure a specific convergence rate, for both biased and unbiased server quantizers.
This approach can be used to design FL systems with bandwidth constraints, which is a common scenario.
\newpage
\bibliography{mybib}
\bibliographystyle{icml2023}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\onecolumn
\part*{Appendix}
\appendix
\input{appendix}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
