\section{The Proposed Approach}
\label{sec:method}




\subsection{Motivations}
\label{sec:motivation}

\noindent
The fMRI contains a structured and semantically rich representation of information since fMRI captures brain activities in the context of a person engaging in visual tasks. It provides ground truth data about how the human brain responds to visual stimuli. According to recent studies \cite{press2001visual, kanwisher2001faces, dwivedi2021unveiling}, the brain can be divided into several Regions of Interest (ROI), where each region holds a different function. Especially in the scope of visual cognition, these are 6 regions specified as Early retinotopic visual regions (\textit{prf-visualrois}), Body-selective regions (\textit{floc-bodies}), Face-selective regions (\textit{floc-faces}), Place-selective regions (\textit{floc-places}), Word-selective regions (\textit{floc-words}) and Anatomical streams (\textit{streams}). 
In summary, these regions are responses for processing information related to the human body, face, place, motion, or objects regarding identification, recognition, etc. Therefore, the fMRI signals from these regions can be used to train computer vision models, providing a reference for what the brain is ``seeing" during the training. By training vision models jointly with brain activities observed in fMRI scans, they can explore the relationship between the visual information processed by the brain and the visual information that models are learning. It can help uncover how the brain represents and processes visual information.









\subsection{Brainformer}


\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.99\columnwidth]{images/multi-scale-transformer_5.jpg}
    \vspace{-5mm}
    \caption{The details of Multi-scale fMRI Transformer module.}
    \label{fig:multi_scale}
    \vspace{-5mm}
\end{figure*}

As aforementioned, human intelligence is presented by the activity of individual brain regions of interest and how they interact with each other. Inspired by this concept, we introduce Brainformer, as shown in Figure \ref{fig:brainformer}, a model that takes fMRI signals from Regions of Interest in the brain as inputs. The proposed model comprises two primary modules. First, we introduce a Mutli-scale fMRI Transformer, as shown in Figure \ref{fig:multi_scale}, designed to uncover local patterns of brain activities within each ROI. Subsequently, we feed a sequence of ROI features derived from the output of the Mutli-scale fMRI Transformer into a conventional Transformer to estimate the correlation and interaction among multiple ROIs. In the following sections, we focus on the design of the Mutli-scale fMRI Transformer, as detailed in Section \ref{sec:sht}. Section \ref{sec:brain_cognitive_features} introduces brain cognitive features which represent for correlation between brain regions. Prior to that, we present an efficient way to extract features of raw signals in Section \ref{sec:fmri_feature}. Additionally, we introduce a novel technique called Brain 3D Voxel Embedding in Section \ref{sec:voxel_embeding}, aimed to preserve the spatial information of the signals.






\subsubsection{High-dimensional fMRI Feature Encoding}
\label{sec:fmri_feature}


Let $m^k$ be the fMRI signals of the $k^{th}$ region of interest in the brain. This signal can be constructed using Eqn. \eqref{eq:fmri}.
\begin{equation}
    \label{eq:fmri}
    m^k = \left[\delta(x_i, y_i, z_i)\right]_{i=0}^{N^k - 1}
\end{equation}
where $\delta$ is the function that takes the value of change in blood and oxygenation in the voxel coordinated at $(x_i, y_i, z_i)$. $N^k$ is the number of voxels in this region. This definition shows that fMRI is a 1D high-dimension signal due to the significant value of $N^k$, e.g., $N^k \approx 20K$.
A straightforward approach to model this signal is adopting linear or fully connected layers \cite{scotti2023reconstructing} and extracting its latent embedding. This approach, however, has two drawbacks. First, fully connected layers with high dimensional features are inefficient as it is challenging to learn useful information from the input space while maintaining a high memory usage of the model. Second, it focuses more on the global structure while ignoring the local patterns presented in the fMRI signal. To solve these two problems, we propose \texttt{Conv1D} to extract features of the fMRI signal. Formally, the embedding feature of fMRI is represented in Eqn. \eqref{eqn:fmri_embedding}.
\begin{equation} \label{eqn:fmri_embedding}
\small
\begin{split}
    \mathbf{r}^k & = \texttt{Conv1D}(m^k) \in \mathbb{R} ^ {N^k \times d_r}
\end{split}
\end{equation}
The $\mathbf{r}^k$ is the embedding features of $m^k$ after the convolution step, and $d_r$ is the embedding dimension.

\subsubsection{Brain 3D Voxel Embedding}

\label{sec:voxel_embeding}

As shown in Eqn \eqref{eq:fmri}, the fMRI signal is a compressed representation of Magnetic Resonance Imaging (MRI) containing detailed 3D information.
However, direct learning from the flattened signal will \textit{ignore the spatial structure}. Figure \ref{fig:voxel_problem} illustrates this problem. 
Consider two voxels, denoted as $v_1 = (x_1, y_1, z_1)$ and $v_2 = (x_2, y_2, z_2)$, which are closely situated in the 3D space of MRI, but in fMRI signals representation, they appear distant from each other. Therefore, it becomes crucial to incorporate spatial information for the model to uncover deeper structural features.
Meanwhile, prior studies \cite{scotti2023reconstructing, takagi2023high, chen2023seeing} still need to address this problem. In light of this concept, we introduce a new approach named \textit{Brain 3D Voxel Embedding} to capture the spatial architecture of fMRI more effectively.

Let $m_i^k$ be the voxel in the signal $m^k$ and $(x_i^k, y_i^k, z_i^k)$ are the 3D coordinates of this voxel. We use $\texttt{Linear}$ function to map these $(x_i^k, y_i^k, z_i^k)$ coordinates into the same dimension, i.e., $d_r$ dimension, as the fMRI embedding features in Eqn. \eqref{eqn:fmri_embedding} 
\begin{equation} \label{eqn:voxel_embedding}
\small
\begin{split}
    v_i^k &= \texttt{Linear}(x_i^k, y_i^k, z_i^k) \in \mathbb{R}^{d_r} \\
    v(m^k) &= \texttt{concat}\left[v_0^k, v_1^k, \dots, v_{N^k-1}^k\right] \in \mathbb{R}^{N^k \times d_r}
\end{split}
\end{equation}
where $v_i^k$ is the brain 3D voxel embedding of single voxel $i^{th}$, $v(m^k)$ is the embedding of the entire signal $m^k$.
Incorporating with \textit{Brain 3D Voxel Embedding} to the Eqn. \eqref{eqn:fmri_embedding}, we get the features of fMRI as shown in Eqn. \eqref{eqn:fmri_voxel_embedding}. 
\begin{equation} \label{eqn:fmri_voxel_embedding}
\small
\begin{split}
    \mathbf{r}^k &= \texttt{Conv1D}(m^k) + v(m^k)
\end{split}
\end{equation}
\subsubsection{Mutli-scale fMRI Transformer}

\label{sec:sht}


While extremely powerful for a wide range of natural language processing tasks, Transformers have limitations when handling long sequences. These limitations are primarily due to the quadratic complexity of the self-attention mechanism and the model's parameter count. In addition, the attention score might collapse and close to zero if the sequence length is considerable. 
Hence, using the typical transformer for lengthy fMRI signals 
is not a complete solution. 

To deal with the long sequence in our problem, we propose the Mutli-scale fMRI Transformer, shown in Figure \ref{fig:multi_scale}. It contains multiple levels, each level consisting of a Transformer block denoted as $\texttt{TransBlock}$. The length of the signal is reduced when passing through each level of the network until it is sufficient enough. 
The details of the Multi-scale fMRI Transformer are presented in Algorithm \ref{algo:sht}.
In particular, each level consists of  two steps:
\begin{itemize}
    \item We first employ a slicing window with $w$ of width that traverses the entire signal $\mathbf{r}^k$ from its beginning to the end, with a step size of $s$. This process decomposes $\mathbf{r}^k$ into smaller sub-sequences, denoted as $\mathbf{q}_i^k = \mathbf{r}^k\left[i * s:i * s+w\right] \quad 0 \leq i \leq n_s = \frac{N^k}{s}$ which has a computation-efficiency length for the Transformer. The adjacent subsequences overlap by a distance of $s$, preventing the loss of local information, and $n_s$ is the number of subsequences.

    \item We fed $\mathbf{q}_i^k$ into the $\texttt{TransBlock}$ to learn the patterns inside the signal and get the features $\Bar{\mathbf{q}}_{i}^k = \texttt{TransBlock}^k(\mathbf{q}_i^k)$. After that,  all the features $\Bar{\mathbf{q}}_{i}^k$ of subsequences are gathered to form a new sequence
    $\textbf{t} = \left[\Bar{\mathbf{q}}_{i}^k\right]_{i=0}^{n_s-1}$ and then passed into the next level. 
\end{itemize}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.96\columnwidth]{images/voxel_problem_4.png}
    \vspace{-5mm}
    \caption{Two voxels, denoted $v_1 = (x_1, y_1, z_1)$ and $v_2 = (x_2, y_2, z_2)$ that are located closely in the 3D space of MRI, but in the fMRI signals, they are far away from each other.}
    \label{fig:voxel_problem}
    \vspace{-6mm}
\end{figure}

\subsubsection{Brain Cognitive Features}
\label{sec:brain_cognitive_features}
Let $\left[\Bar{\mathbf{q}}^k\right]_{k=0}^{N_r}$ be the list of fMRI features that are the outputs from the Mutli-scale fMRI Transformer. $N_r$ is the number of regions in the brain. In particular, we define $N_r = 6$ as discussed in the section \ref{sec:motivation}. 
To learn the correlation between the ROIs, we adopt another feature embedding process by feeding $\left[\Bar{\mathbf{q}}^k\right]_{k=0}^{N_r}$ into a final $\texttt{TransBlock}$ and receiving the brain cognitive features, denoted as $\mathbf{q} = \texttt{TransBlock}(\left[\Bar{\mathbf{q}}^k\right]_{k=0}^{N_r})$. 



\subsection{Training Objectives}



Our framework is trained by a combination of Contrastive Loss, $\mathcal{L}_{con}$, and Brain fMRI Guidance Loss, $\mathcal{L}_{bfg}$, bellow:
\begin{equation} \label{eqn:contrastive}
\small
\begin{split}
    \mathcal{L} = \lambda_{con} \mathcal{L}_{con} + \lambda_{bfg} \mathcal{L}_{bfg}
\end{split}
\end{equation}
where $\lambda_{con}$ and $\lambda_{bfg}$ are the weights of the loss functions. In this paper, we select $\lambda_{con} = \lambda_{bfg} = 0.5$. The details of $\mathcal{L}_{con}$ and $\mathcal{L}_{bfg}$ are described in the sections below.
\subsubsection{Contrastive Loss}

Let $\mathbf{p} \in \mathbb{R}^{d_r}$ be the image features extracted by the image encoder, e.g., SwinTransformer, ConvNext. We employ contrastive loss \cite{chen2020simple} to align visual representation with the brain cognitive features $\mathbf{q}$ as in Eqn. \eqref{eqn:contrastive}.
\begin{equation} \label{eqn:contrastive}
\small
\begin{split}
    \mathcal{L}_{con} &= -\frac{1}{N}\sum_i^N \log\frac{\exp(\mathbf{p}_i \otimes \mathbf{q}_i/\sigma)}{\sum_j^N\exp(\mathbf{p}_i \otimes \mathbf{q}_j/\sigma)} \\
    &- \frac{1}{N}\sum_i^N \log\frac{\exp(\mathbf{q}_i \otimes \mathbf{p}_i/\sigma)}{\sum_j^N\exp(\mathbf{q}_i \otimes \mathbf{p}_j/\sigma)}
\end{split}
\end{equation}
where $\sigma$ is the learnable temperature factor,
$N$ is the number of samples, and $\otimes$ is the dot product. 

\subsubsection{Brain fMRI Guidance Loss}
\label{sec:guidance_loss}

Besides the contrastive loss that aligns the $\textit{global context}$ of brain signals and stimuli image, the Brain fMRI Guidance Loss aims to align the $\textit{local context}$. Since the ROI features $\Bar{\mathbf{q}}^k$ is extracted from a particular region of the brain, e.g., floc-bodies, floc-faces, etc., it embeds the information on how the brain perceives the objects inside the image.
If we can leverage these features as the guidance during training, 
the vision model can mimic the perception system of the human. 
\begin{algorithm}[!t]
\centering
\small
\caption{Mutli-scale fMRI Transformer}
\label{algo:sht}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} The feature $\mathbf{r}^k$ of $k^{th}$ fMRI signals , window size $w$, stride: $s$, number of level $h$. 
\STATE {\bfseries Output:} The fMRI features $\Bar{\mathbf{q}}^k$

\STATE{$\mathbf{x} \gets \mathbf{r}^k$}

\FOR{$j=1, \cdots, h$}
    \STATE{$\mathbf{t} \gets \text{empty list}$} \hfill \AlgCommentInLine{Store features of subsequences}
    \STATE{$L \gets |x|$}
    \FOR{$i=0, \cdots \frac{L}{s}$}
        \STATE{$\mathbf{q}_i^k \gets \mathbf{x}\left[i * s:i * s+w\right]$} \hfill \AlgCommentInLine{Getting subsequences}
        \STATE{$\Bar{\mathbf{q}}_{i}^k \gets \texttt{TransBlock}^k_j(\mathbf{q}_i^k)$}
        \STATE{$\mathbf{t} \gets \left[\mathbf{t} \quad \Bar{\mathbf{q}}_{i}^k\right]$} \hfill \AlgCommentInLine{Gather features of the subsequence}
    \ENDFOR
    \STATE{$\mathbf{x} \gets \mathbf{t}$}
\ENDFOR
\STATE{$\Bar{\mathbf{q}}^k \gets \texttt{TransBlock}(\mathbf{x})$}
\STATE{\textbf{return}  $\Bar{\mathbf{q}}^k$}
\end{algorithmic}
\end{algorithm}

Let $\Bar{\mathbf{p}}^k$ be the vision features representing a specific local context within the image, e.g., objects, persons, the background, or locations. These features can be achieved by passing pooling features of the vision model to the linear layer and then projected to the same dimension space as $\Bar{\mathbf{q}}^k$. Our goal is to encourage the visual perceptions of the vision model to be similar to the ROI features of the brain. This requires maximizing the similarity between two features, $\Bar{\mathbf{p}}^k$ and $\Bar{\mathbf{q}}^k$. Furthermore, it is important to emphasize that the brain's ROIs have distinct functions. Consequently, we must ensure the dissimilarity of $\Bar{\mathbf{q}}^k$ with respect to all $\Bar{\mathbf{q}}^{g}$, where $k \neq g$. To facilitate the two constraints, we propose the Brain fMRI Guidance Loss as in Figure \ref{fig:bs_loss} and in Eqn. \eqref{eqn:guidance}.
\begin{equation} \label{eqn:guidance}
\small
\begin{split}
    \mathcal{L}_{bfg} &= - \frac{1}{N} \sum_i^N \sum_k^{N_r} \log\frac{\exp(\Bar{\mathbf{p}}^k_i \otimes \Bar{\mathbf{q}}^k_i)}{\sum_g^{N_r}\exp(\Bar{\mathbf{p}}^k_i \otimes \Bar{\mathbf{q}}^g_i)}
\end{split}
\end{equation}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.96\columnwidth]{images/bs_loss_5.jpg}
    \vspace{-6mm}
    \caption{The circle and rectangle represent vision and fMRI features, respectively. Each color indicates a different object of interest that the human brain is processing. The Brain fMRI Guidance Loss aims to align visual and fMRI features of the same object while discriminating with features of other objects.}
    \label{fig:bs_loss}
\end{figure}
