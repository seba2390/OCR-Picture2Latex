\section{Related Work}
\label{sec:related_work}






\begin{figure*}[!t]
    \centering
    \includegraphics[width=1.99\columnwidth]{images/architecture_7.jpg}
    \vspace{-2mm}
    \caption{Brainformer utilizes fMRI signals from specific brain regions as input, extracting the local features representing patterns within each region. The $\texttt{TransformerBlock}$ measures the correlation among these regions to emulate brain activities. This information is subsequently transferred to the vision model through Contrastive Loss and Brain fMRI Guidance Loss.}
    \label{fig:brainformer}
    \vspace{-4mm}
\end{figure*}



\noindent
\textbf{Vision Pretraining}. Pretraining on the large-scale dataset such as ImageNet \cite{imagenet}, Instagram \cite{mahajan2018exploring}, or JFT \cite{zhai2022scaling} has become a popular approach for many visual recognition problems, e.g., classification, localization, or segmentation, etc. Despite this method has great success, it is limited by the number of annotated training data that is costly to collect. Recently, self-supervised training has been proposed to address this problem. For example, BEiT \cite{bao2021beit} inspired by BERT \cite{BERT} in natural language processing to make a masked image model. Similarly, SimMIM \cite{xie2022simmim} and MAE are proposed to randomly remove the image patches and then use the encoder-decoder model to regress the masked pixel values. Nonetheless, these methods use the vision modality as the only input and are unsuitable for tasks involving images and text.

\noindent
\textbf{Vision-Language Pretraining}. Significant advancements have occurred in vision-language pertaining \cite{radford2021learning, jia2021scaling, zhai2022lit, pham2023combined, wang2021simvlm, wang2022ofa, li2022blip, yu2022coca, zhai2023sigmoid, luo2023lexlip, wang2023equivariant}. This approach aims to encode both visual and text information into a model. Initial work, e.g., LXMERT \cite{tan2019lxmert}, UNITER \cite{chen2020uniter}, and VinVL \cite{zhang2021vinvl}, depended on pre-trained object detection modules like Fast(er) R-CNN \cite{ren2015faster} to extract visual representations of objects. Subsequent studies, such as ViLT \cite{kim2021vilt} and VLMo \cite{bao2022vlmo}, tried to integrate vision and language transformers and then train a multimodal transformer from scratch.
Recent work has proposed Image-Text foundation models that bridge the gap between image and text understanding. It is worth mentioning that CLIP \cite{clip} and ALIGN \cite{align} leverage dual-encoder architecture pre-trained on noise image-text pairs using contrastive learning. It has achieved robust image and text representation, suitable for cross-model alignment tasks and zero-shot image classification. Another line of research \cite{piergiovanni2022answer, wang2021simvlm, wang2022ofa} explores encoder-decoder models trained with generative losses, showcasing impressive performance in vision-language benchmarks while maintaining strong visual encoder capabilities for classification tasks.









\noindent
\textbf{Decoding functional MRI}.
Decoding visual information from fMRI signals has been studied for a decade \cite{haynes2005predicting, thirion2006inverse, kamitani2005decoding, cox2003functional, haxby2001distributed}. Most of these studies aimed to explore the hidden information inside the brain. It is a difficult task because of the low signal-to-noise ratio. Recently, with the help of deep learning, the authors in \cite{chen2023seeing, scotti2023reconstructing, takagi2023high, ozcelik2023natural, lin2022mind, ozcelik2023brain} proposed methods to reconstruct what humans see from fMRI signal using diffusion models. There are limited works \cite{nishida2020brain, nechyba1995human} that incorporate human brain activities for transfer learning. However, these methods still need to be improved in exploring the working mechanism of the human brain. 


Different from recent studies on decoding fMRI \cite{takagi2023high, scotti2023reconstructing, chen2023seeing}, our goal is not only to explore valuable vision information from fMRI signal, i.e., semantic and structure information but also utilize them as supervision for helping to enhance the recognition capability of the vision model, a novel approach that standout our work from previous efforts in the field. Although our approach also aims to extract feature representations of fMRI data, these previous methods still have limitations and do not apply to our problem. 
Firstly, prior approaches, such as those by Yu et al. \cite{takagi2023high} and Paul et al., \cite{scotti2023reconstructing} relied on linear models and MLP layers to generate fMRI feature representations. This approach cannot capture the local pattern of the signals, especially the non-linear or complicated patterns. Kim et al. \cite{kim2023swift} presented SwiFT for self-training fMRI in 4D data, which does not apply to our problem. On the other hand, MindVis \cite{chen2023seeing} utilized a Masked Brain Model inspired by MAE, which explored local patterns but ignored the correlations between multiple voxels, which describes how neurons interact inside the brain. Furthermore, MindVis failed to leverage the 3D spatial information in fMRI signals, which is crucial in understanding the relationships between voxels. Most notably, none of these approaches leverage characteristics of functional regions of interest (ROI) within the brain, which contain rich information about visual stimuli. 

