\section{Experimental Results}

\subsection{Datasets}

To pretrain Brainformer, we leverage the Natural Scenes Dataset (NSD) \cite{allen2022massive}, a comprehensive compilation of responses from eight subjects obtained through high-quality 7T fMRI scans.
Each subject was exposed to approximately 73,000 natural scenes, forming the basis for constructing visual brain encoding models. 
The fMRI responses of each image encompass the left hemisphere (LH) and right hemisphere (RH), with 19,004 and 20,544 voxels, respectively. 


\subsection{Brainformer Training}

We use data from seven subjects in NSD for training and leave one for testing. The images are resized to 224 $\times$ 224 before feeding into the vision model. We do not use any augmentations for the image because the fMRI signals depend on the input image. Any change in vision stimuli will affect human recognition. For the fMRI signals, we divide them into six regions of interest following \cite{algonauts} and feed them into Brainformer simultaneously. Experimentally, we select $\texttt{Conv1D}$ with kernel size and stride are 32 and 16, respectively. The Multi-scale fMRI Transformer is designed with a window size as $w = 64$, stride as $s = 32$, and number of levels as $h=2$. Feature dimensions of image and fMRI signals are projected into $d_r = 768$ space. For the vision model, we select Swin-S \cite{liu2021swin} and ConvNext-S \cite{liu2022convnet} as the image backbone. Brainformer is easily implemented in the Pytorch framework and trained on A100 GPUs. The initial learning rate is 0.0001 and decreases gradually following the ConsineLinear policy \cite{cosine}. A batch size of 64 per GPU is employed. Optimization is performed using AdamW \cite{adamw} with 100 epochs, with the entire training process concluding within a few hours. The pre-trained models are used for further downstream tasks, including COCO object detection \cite{coco} and ADE20K semantic segmentation \cite{ade20k}.

\subsection{Object Detection on COCO}
\textbf{Settings}. We conducted object detection and instance segmentation experiments using the COCO 2017 dataset, which comprises 118,000 training images, 5,000 validation images, and 20,000 test-dev images. Notably, we excluded images from the NSD dataset to prevent any data leakage, a subset of COCO. For these experiments, we utilized the MaskRCNN framework \cite{maskrcnn} implemented with mmdetect \cite{mmdetection} for enhanced performance efficiency. 

\noindent
\textbf{Performance}. The object detection and instance segmentation results are presented in Table \ref{tab:coco_object_detection}. In summary, our approach, which leverages fMRI signals for training, consistently outperforms the CLIP framework, where models rely solely on textual 
supervision. Specifically, when comparing Swin-S/CLIP to Swin-S/Random, the Swin-S/CLIP achieves approximately a 0.6\% improvement in box/AP and a 0.8\% improvement in seg/AP. However, our method Swin-S/Brainformer presents even better performance, surpassing Swin-S/CLIP by approximately 1.7\% and 3.9\% in object detection and instance segmentation, respectively, translating to a substantial 2.3\% and 4.7\% improvement compared to the same model Swin-S/Random without any pretraining method.
We also observe the same results with the ConvNext-S backbone, where Brainformer achieves around 2.3\% and 3.2\% higher than CLIP in box/AP and seg/AP.

\begin{table}[!b]
\centering
    \vspace{-5mm}
    \caption{Results of object detection and instance segmentation on COCO dataset}
    \label{tab:coco_object_detection}
    \vspace{-3mm}
    \begin{tabular}{c|c|ccc}
    \Xhline{1.0pt}
    \multicolumn{5}{c}{{(a) Object Detection}}  \\  
    \hline
    Backbone & Pretrain & AP$^\text{box}$ & AP$^\text{box}_\text{50}$ & AP$^\text{box}_\text{75}$ \\% & \#param. & FLOPs & FPS \\
    \hline
     Swin-S &  Random init & {41.3} & {63.4} & {45.5} \\
     Swin-S &  CLIP \cite{clip} & {41.9} & {64.0} & {46.0} \\
     Swin-S &  Brainformer & \textbf{43.6} & \textbf{65.8} & \textbf{47.4} \\
     \hline
     ConvNext-S &  Random init & {42.6} & {65.5} & {47.0}\\
     ConvNext-S &  CLIP \cite{clip} & {42.8} & {66.1} & {48.3} \\
     ConvNext-S &  Brainformer & \textbf{45.1} & \textbf{68.2} & \textbf{50.0} \\               
    \hline
    \multicolumn{5}{c}{{(b) Semantic Segmentation}}  \\
    \hline
     Backbone & Pretrain & AP$^\text{segm}$ & AP$^\text{segm}_\text{50}$ & AP$^\text{segm}_\text{75}$ \\% & \#param. & FLOPs & FPS \\
    \hline
     Swin-S &  Random init  & {38.4} & {60.8} & {41.3}\\
     Swin-S &  CLIP\cite{clip} & {39.2}  & {61.3} & {41.7}\\
     Swin-S &  Brainformer  & \textbf{43.1} & \textbf{63.1} & \textbf{43.3} \\
     \hline
     ConvNext-S &  Random init  & {39.8} & {61.7} & {42.9}\\
     ConvNext-S &  CLIP \cite{clip} & {40.8}  & {62.2} & {43.5} \\
     ConvNext-S &  Brainformer & \textbf{44.0} & \textbf{64.0} & \textbf{45.3} \\   
     \hline
\end{tabular}
\end{table}


\subsection{Semantic Segmentation on ADE20K}
\textbf{Settings}. For semantic segmentation, we conducted training using UpperNet \cite{uppernet} on the ADE20K database, which includes a wide spectrum of 150 semantic categories. This dataset comprises 25,000 images, with 20,000 allocated for training, 2,000 for validation, and 3,000 for testing. 

\noindent
\textbf{Performance}. Table \ref{tab:semantic_segmentation} presents the semantic segmentation results. From these results, Swin-S/Brainformer is $1.48\%$ mIoU (41.77 v.s 40.29) higher than  Swin-S/CLIP while ConvNext-S/Brainformer maintains the performance better than ConvNext-S/CLIP by $1.65\%$ mIoU.


\begin{table}[!t]
    \centering
    \caption{Results of semantic segmentation on ADE20K and brain activities response prediction on NSD.}
    \label{tab:semantic_segmentation}
    \vspace{-4mm}
    \begin{tabular}{c|c|c|c}
        \Xhline{1.0pt}
        & & \multicolumn{1}{c|}{ADE20K} & \multicolumn{1}{c}{NSD}\\
        \multicolumn{1}{c|}{Backbone} & \multicolumn{1}{c|}{Pretrain} & mIoU & PCC\\
        \hline
        Swin-S & Random init & 38.37 & 40.41 \\
        Swin-S & CLIP \cite{clip} & 40.29 & 41.25 \\
        Swin-S & Brainformer & \textbf{41.77} & \textbf{44.63} \\
        \hline
        ConvNext-S & Random init & 39.22 & 54.21 \\
        ConvNext-S & CLIP \cite{clip} & 41.27 & 55.70 \\
        ConvNext-S & Brainformer & \textbf{42.92} & \textbf{57.43}\\
        \hline
    \end{tabular}
    \vspace{-6mm}
\end{table}



\subsection{Human Brain Response Prediction NSD}
\textbf{Settings}. This experiment aims to predict the human brain response to complex natural scenes, as recorded during participants' observations \cite{algonauts, nsd}. The dataset contains responses from eight subjects in the NSD. We use seven subjects for pretraining CLIP and Brainformer, while one is reserved for the downstream task. We follow the evaluation protocols outlined in \cite{algonauts}, utilizing the Pearson Correlation Coefficient (PCC) score as our evaluation metric.\\
\noindent
\textbf{Performance}. The results of our brain response prediction are shown in Table \ref{tab:semantic_segmentation}. Significantly, Brainformer yields substantial performance improvements in this task. Specifically, Swin-S/Brainformer demonstrates approximately 4.22\% and 3.38\% better performance than Swin-S/Random and Swin-S/CLIP, respectively. Furthermore, it is worth noting that ConvNext-based models perform strong predictive capabilities, with ConvNext-S/Brainformer achieving a PCC of 57.43\%, approximately 1.73\% higher than ConvNext-S/CLIP.



