\section{Ablation Studies}

In this section, we study the efficiency of the Brain 3D Voxel Embedding as presented in Section \ref{sec:voxel_embeding}, Brain fMRI Guidance Loss in Section \ref{sec:guidance_loss}, hyper-parameters of Multi-scale fMRI Transformer in Section \ref{sec:sht}, and performance of Brainformer on different amounts of data. We select ConvNext-S as the backbone for these ablation studies. 

\noindent
\textbf{Brain 3D Voxel Embedding}.
In Table \ref{tab:ablation}, we provide an ablation study for 3D voxel embedding. The use of this embedding yields an improvement of approximately +3.2\% box/AP, +2.2\% seg/AP, +1.2\% mIoI, and +1.24\%PCC over the network employing traditional positional embedding in various tasks such as object detection, instance segmentation, semantic segmentation, and brain response prediction, respectively. These findings demonstrate the effectiveness of the 3D voxel embedding for fMRI signals.

\noindent
\textbf{Brain fMRI Guidance Loss}.
We also provide an ablation study for the usage of Brain fMRI Guidance Loss in Table \ref{tab:ablation}. The model trained with the guidance loss performs better than not using it. In particular, this loss function helps to improve +2.4\% box/AP, +3.3\% seg/AP, +1.12\% mIoU, and 1.18\%PCC for object detection, instance segmentation, semantic segmentation, and brain response prediction, respectively. We also investigate how Brain fMRI Guidance Loss facilitates the transfer of semantic and structured information from fMRI signals to the vision model. To accomplish this, we follow a three-step process. First, we extract the fMRI features and image features from Brainformer and the vision model, respectively. Second, we compute the cosine similarity between these feature vectors. Finally, we leverage GradCam, as described in \cite{grad_cam}, by analyzing the backward gradient of the similarity to generate an attention map. This map highlights specific image regions strongly correlated to the fMRI signals. We visualize this attention map in Figure \ref{fig:frmi_attention}. 
It illustrates the effectiveness of the Brain fMRI Guidance Loss in transferring brain activities from the fMRI signals to the vision model.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/attention_demo_3.png}
    \vspace{-3mm}
    \caption{Visual attention with respect to fMRI signals. 
    The first row is the input images. The second row is the corresponding attention maps of the vision model training with CLIP. The third row is the results of training with Brainformer without {Brain fMRI Guidance Loss}. The last row is the results with Brainformer and guidance of {Brain fMRI Guidance Loss}. \textbf{Best view in color}.}
    \vspace{-5mm}
    \label{fig:frmi_attention}
\end{figure*}

\noindent
\textbf{Hyper-parameters for Multi-scale fMRI Transformer}
We studied the effectiveness of the hyperparameters of Brainformer, i.e., window size and stride, on the overall performance.
As shown in Table \ref{tab:ablation}, the results highlight that smaller window sizes and strides lead to better performance. Increasing the window size from 64 to 128 resulted in a 2.8\% decrease in box/AP, a 2.4\% decrease in seg/AP, a 1.12\% decrease in mIoU, and a 1.23\% decrease in PCC for object detection, instance segmentation, semantic segmentation, and brain response prediction, respectively. This is attributed to the fact that a smaller window size allows Brainformer to capture more local information. Similarly, we observed similar results when keeping the window size constant and increasing the stride from 32 to 64, with larger strides causing the model to miss more information potentially.


\noindent
\textbf{Performance on different amounts of data}.
\label{sec:abl_amount_data}
We investigate the performance of Brainformer with respect to the amount of training data.
We train the Brainformer with a different number of subjects included in the pre-training step. The performance is reported in Table \ref{tab:ablation}. It is clear that when we use only one subject data, the performance is not much improved compared to the random initialization. However, when we increased the number of subjects, the performance of Brainformer also increased accordingly. With respect to 7 subjects' training data, the performance is boosted by +2.6\% for box/AP, +4.7\% for seg/AP, +3.77\% for mIoU, and 3.23\% for PCC.
\begin{table}[!b]
\small
\centering
\addtolength{\tabcolsep}{-2.5pt}
\vspace{-5mm}
\caption{Performance of Brainformer on various settings.}
\label{tab:ablation}
\vspace{-3mm}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|cc|c|c}
\Xhline{1.0pt}
 & \multicolumn{2}{c|}{COCO} & \multicolumn{1}{c|}{ADE20K} & \multicolumn{1}{c}{NSD} \\
& AP$^\text{box}$ & AP$^\text{segm}$ & mIoU & PCC \\
\hline

pos embed & 41.9 & 41.8 & 41.72 & 56.19 \\
3D voxel embed &  \textbf{45.1} & \textbf{44.0} & \textbf{42.92} & \textbf{57.43} \\
\hline
w/o Brain fMRI Guidance Loss & 42.7 & 40.7 & 41.80 & 56.25 \\
w/ Brain fMRI Guidance Loss & \textbf{45.1} & \textbf{44.0} & \textbf{42.92} & \textbf{57.43} \\
\hline
$w = 128, s = 64$  & 41.9 & 41.1 & 40.90 & 55.80 \\
$w = 128, s = 32$  & 42.3 & 41.6 & 41.80 & 56.20 \\
$w = 64, s = 32$  & \textbf{45.1} & \textbf{44.0} & \textbf{42.92} & \textbf{57.43} \\
\hline
$\#$ subjects = 1  & 42.5 & 39.3 & 39.15 & 54.20 \\
$\#$ subjects = 3  & 42.9 & 39.6 & 39.37 & 54.51 \\
$\#$ subjects = 5  & 43.4 & 40.4 & 40.08 & 55.01 \\
$\#$ subjects = 7  & \textbf{45.1} & \textbf{44.0} & \textbf{42.92} & \textbf{57.43} \\
\Xhline{1.0pt}
\end{tabular}
}
\end{table}

