\section{Introduction}
\label{sec:intro}


Recent studies in machine vision understanding \cite{resnet, densenet, krizhevsky2012imagenet, vaswani2017attention, liu2021swin, dosovitskiy2020image} have demonstrated the effectiveness of single-encoder models through pretraining on image datasets, e.g., ImageNet \cite{imagenet}. These methods are designed to acquire universal visual representations of objects that can be flexibly applied to various downstream tasks, including object detection and semantic segmentation. Nonetheless, the most significant limitation of these methods lies in the costly annotation process, mainly when applied at a large scale. In response to this challenge, self-supervised techniques have emerged \cite{bao2021beit, he2022masked, Nguyen_2023_CVPR}.
They aim to acquire visual representations without incurring human annotation expenses while delivering commendable performance compared to supervised methods. 

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.96\columnwidth]{images/abstract_figure_7.jpg}
    \vspace{-4mm}
    \caption{Given a pair of images and fMRI signals, Brainformer can explore the local patterns of fMRI signals from brain regions and discover how these regions interact with each other. \textbf{Best view in color}}
    \label{fig:enter-label}
    \vspace{-6mm}
    \label{fig:abstract_figure}
\end{figure}

Deep learning has recently experienced a surge in the development of foundational language models, e.g., BERT \cite{BERT}, GPT-2, GPT-3 \cite{brown2020language}, RoBERTa \cite{liu2019roberta}, T5\cite{raffel2020exploring}, BART\cite{lewis2019bart}. These models are typically trained on extensive datasets, often employing self-supervision at a large-scale web database. They have showcased their efficiency across a wide range of downstream tasks, utilizing strategies like zero-shot, few-shot, or transfer learning to extend the boundaries of large-scale modeling toward achieving human-level intelligence. Using the capabilities of text foundation models, a line of studies \cite{yuan2021florence, clip, jia2021scaling} has illustrated the feasibility of image-text foundation models. 
They encompass two encoders, i.e., an image encoder and a text encoder, jointly trained through contrastive learning. Apart from generating visual embeddings for tasks centered on vision alone, these image-text models can also generate textual embeddings that share the same latent space.

From the success of text as supervision for the visual model, we ask what had been posed in the early days of artificial intelligence: \textit{What if human brain behaviors can serve as the guiding force for the machine vision models?}. To answer this question, we found that Functional Magnetic Resonance Imaging (fMRI) is a neuroimaging technique that measures changes in blood flow and oxygenation in the brain, allowing researchers to observe brain activities in real-time. fMRI has provided valuable insights into various aspects of human cognition and neuroscience. For example,  fMRI can help identify the specific regions of the brain that are active during various cognitive tasks. Studies have shown that different intelligence-related tasks activate specific brain areas, such as problem-solving or memory. In addition, human intelligence is not just about the activity of individual brain regions but also about how different brain regions communicate. fMRI can reveal functional connectivity patterns, highlighting networks involved in various cognitive processes. 

While text typically reflects the outcomes of the recognition process, fMRI captures the dynamics of the cognitive processes involved. Consider, for instance, an examiner tasked with describing the context of an image, such as ``A man is sitting on the chair." Initially, they focus on the man and then shift attention to the chair, determining the interactions between these objects, specifically, the act of sitting. Subsequently, they conclude the context in the text form. When encountering this description for the first time, it remains unclear which object the examiner attended to first or whether other smaller objects influenced their perception. Notably, \textbf{this information can be encoded in fMRI signals}. By analyzing specific brain regions or regions of interest,  we can explore the whole recognition process of the human brain.
This work provides a promising approach to further studies and endeavors to bridge the gap between human intelligence and deep neural networks. 
The contributions of this work can be summarized as follows.

\noindent
\textbf{Contributions of this Work:} This paper presents a novel Transformer-based framework, named Brainformer, to analyze and leverage fMRI signals as human brain activities to supervise the machine vision learning model. 
We present a simple yet effective way to encode fMRI features. Then, a novel \textit{Brain 3D Voxel Embedding} approach is introduced to preserve spatial signal information. The proposed Brainformer explores the patterns within specific regions of interest in the human brain that signify the human \textit{attention process} via the Multi-scale fMRI Transformer module. It can learn the correlation between these regions, helping to mimic the human \textit{recognition process}. By incorporating these capabilities, Brainformer reveals mechanisms in the human recognition system.
Second, a new Brain fMRI Guidance Loss function is presented to distill information from fMRI extracted using Brainformer. It serves as a guidance signal to enhance the performance capabilities of vision models.
Finally, Brainformer is designed for self-supervised learning and trained as an end-to-end deep network. It consistently outperforms previous State-of-the-Art Self-supervised learning methods across standard benchmarks, i.e., object detection, instance segmentation, semantic segmentation, and brain response prediction.







    


    
    
    


    
    



    
    
    
    
    


    
    
    
    
    


    
    
    

