\pdfoutput=1
\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[final]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{multirow} 
\usepackage{multicol} 
\usepackage{arydshln}
\usepackage{graphicx}
%\usepackage{todonotes}

\usepackage{caption}

\usepackage{algorithm}
\usepackage{enumitem}

\usepackage{wrapfig}
\usepackage{ulem}
\usepackage{bm}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage[noend]{algpseudocode}
\algrenewcommand\alglinenumber[1]{{\sffamily\footnotesize#1}}
\algnewcommand{\LineComment}[1]{\State \(\triangleright\) #1}
\usepackage{todonotes}

%\usepackage[normalem]{ulem}
\newcommand{\canadd}[1]{{\color{blue}  \textbf{(can\_add: #1)}}}
\newcommand{\old}[1]{{\color{green} \sout{#1}}}
\newcommand{\zhengadd}[1]{{\color{red}  \textbf{(zheng\_add: #1)}}}
\newcommand{\erqunadd}[1]{{\color{yellow} \textbf{(erqun\_add:) #1}}}

\title{Generalized Data Weighting via Class-level Gradient Manipulation}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\author{%
  Can Chen\textsuperscript{1}\thanks{Equal contribution; Names listed in alphabetical order.}~, Shuhao Zheng\textsuperscript{1}\samethanks~, Xi Chen\textsuperscript{1}, Erqun Dong\textsuperscript{1}, Xue Liu\textsuperscript{1}, Hao Liu\textsuperscript{2}, Dejing Dou\textsuperscript{3}\\
  \\
  \textsuperscript{1}McGill University, \textsuperscript{2}The Hong Kong University of Science and Technology, \textsuperscript{3}Baidu Research\\
  \texttt{\{can.chen, shuhao.zheng, erqun.dong\}@mail.mcgill.ca}\\ \texttt{xi.chen11@mcgill.ca},
  \texttt{xueliu@cs.mcgill.ca}, \texttt{liuh@ust.hk}, \texttt{doudejing@baidu.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
Label noise and class imbalance are two major issues coexisting in real-world datasets.
%
To alleviate the two issues, state-of-the-art methods reweight each instance by leveraging a small amount of clean and unbiased data.
%
Yet, these methods overlook class-level information within each instance, which can be further utilized to improve performance.
%
To this end, in this paper, we propose \textbf{G}eneralized \textbf{D}ata \textbf{W}eighting (\textbf{GDW}) to simultaneously mitigate label noise and class imbalance by manipulating gradients at the class level.
%
To be specific, GDW unrolls the loss gradient to class-level gradients by the chain rule and reweights the flow of each gradient separately.
%
In this way, GDW achieves remarkable performance improvement on both issues. 
%
Aside from the performance gain, GDW efficiently obtains class-level weights without introducing any extra computational cost compared with instance weighting methods.
%
Specifically, GDW performs a gradient descent step on class-level weights, which only relies on intermediate gradients.
%
Extensive experiments in various settings verify the effectiveness of GDW.
%
For example, GDW outperforms state-of-the-art methods by $2.56\%$ under the $60\%$ uniform noise setting in CIFAR10. 
%
Our code is available at \url{https://github.com/GGchen1997/GDW-NIPS2021}.
\end{abstract}


\input{Sec1_introduction}
\input{Sec2_relatedwork}
\input{Sec3_method}
\input{Sec4_experiments}
\input{Sec5_conclusion}
\input{Sec6_acknowledgement}

\begin{thebibliography}{10}

  \bibitem{songLearningNoisyLabels2021}
  Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee.
  \newblock Learning from {{Noisy Labels}} with {{Deep Neural Networks}}: A
    {{Survey}}.
  \newblock {\em arXiv:2007.08199 [cs, stat]}, June 2021.
  
  \bibitem{heLearningImbalancedData2009}
  Haibo He and Edwardo~A. Garcia.
  \newblock Learning from {{Imbalanced Data}}.
  \newblock {\em IEEE Transactions on Knowledge and Data Engineering},
    21(9):1263--1284, September 2009.
  
  \bibitem{elhadySystematicSurveySensor2018a}
  Nancy~E. ElHady and Julien Provost.
  \newblock A {{Systematic Survey}} on {{Sensor Failure Detection}} and
    {{Fault}}-{{Tolerance}} in {{Ambient Assisted Living}}.
  \newblock {\em Sensors}, 18(7):1991, July 2018.
  
  \bibitem{tongxiaoLearningMassiveNoisy2015}
  {Tong Xiao}, {Tian Xia}, {Yi Yang}, {Chang Huang}, and {Xiaogang Wang}.
  \newblock Learning from massive noisy labeled data for image classification.
  \newblock In {\em 2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern
    Recognition}} ({{CVPR}})}, pages 2691--2699, {Boston, MA, USA}, June 2015.
    {IEEE}.
  
  \bibitem{alganLabelNoiseTypes2020b}
  G{\"o}rkem Algan and {\.I}lkay Ulusoy.
  \newblock Label {{Noise Types}} and {{Their Effects}} on {{Deep Learning}}.
  \newblock {\em arXiv:2003.10471 [cs]}, March 2020.
  
  \bibitem{zhuClassNoiseVs2004a}
  Xingquan Zhu and Xindong Wu.
  \newblock Class {{Noise}} vs. {{Attribute Noise}}: A {{Quantitative Study}}.
  \newblock {\em Artificial Intelligence Review}, 22(3):177--210, November 2004.
  
  \bibitem{frenayClassificationPresenceLabel2014a}
  Benoit Frenay and Michel Verleysen.
  \newblock Classification in the {{Presence}} of {{Label Noise}}: A {{Survey}}.
  \newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
    25(5):845--869, May 2014.
  
  \bibitem{zhaoLongTailDistributionsUnsupervised2012a}
  Qiuye Zhao and Mitch Marcus.
  \newblock Long-{{Tail Distributions}} and {{Unsupervised Learning}} of
    {{Morphology}}.
  \newblock In {\em Proceedings of {{COLING}} 2012}, pages 3121--3136, {Mumbai,
    India}, December 2012. {The COLING 2012 Organizing Committee}.
  
  \bibitem{vanhornDevilTailsFinegrained2017a}
  Grant Van~Horn and Pietro Perona.
  \newblock The {{Devil}} is in the {{Tails}}: Fine-grained {{Classification}} in
    the {{Wild}}.
  \newblock {\em arXiv:1709.01450 [cs]}, September 2017.
  
  \bibitem{pavonAssessingImpactClassImbalanced2011a}
  Reyes Pav{\'o}n, Rosal{\'i}a Laza, Miguel {Reboiro-Jato}, and Florentino
    {Fdez-Riverola}.
  \newblock Assessing the {{Impact}} of {{Class}}-{{Imbalanced Data}} for
    {{Classifying Relevant}}/{{Irrelevant Medline Documents}}.
  \newblock In Miguel~P. Rocha, Juan M.~Corchado Rodr{\'i}guez, Florentino
    {Fdez-Riverola}, and Alfonso Valencia, editors, {\em 5th {{International
    Conference}} on {{Practical Applications}} of {{Computational Biology}} \&
    {{Bioinformatics}} ({{PACBB}} 2011)}, Advances in {{Intelligent}} and {{Soft
    Computing}}, pages 345--353, {Berlin, Heidelberg}, 2011. {Springer}.
  
  \bibitem{patelReviewClassificationImbalanced2020a}
  Harshita Patel, Dharmendra Singh~Rajput, G~Thippa~Reddy, Celestine Iwendi, Ali
    Kashif~Bashir, and Ohyun Jo.
  \newblock A review on classification of imbalanced data for wireless sensor
    networks.
  \newblock {\em International Journal of Distributed Sensor Networks},
    16(4):1550147720916404, April 2020.
  
  \bibitem{dongClassRectificationHard2017a}
  Qi~Dong, Shaogang Gong, and Xiatian Zhu.
  \newblock Class {{Rectification Hard Mining}} for {{Imbalanced Deep Learning}}.
  \newblock In {\em 2017 {{IEEE International Conference}} on {{Computer Vision}}
    ({{ICCV}})}, pages 1869--1878, {Venice}, October 2017. {IEEE}.
  
  \bibitem{cuiClassBalancedLossBased2019}
  Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.
  \newblock Class-{{Balanced Loss Based}} on {{Effective Number}} of {{Samples}}.
  \newblock In {\em 2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and
    {{Pattern Recognition}} ({{CVPR}})}, pages 9260--9269, June 2019.
  
  \bibitem{sinhaClassWiseDifficultyBalancedLoss2021a}
  Saptarshi Sinha, Hiroki Ohashi, and Katsuyuki Nakamura.
  \newblock Class-{{Wise Difficulty}}-{{Balanced Loss}} for {{Solving
    Class}}-{{Imbalance}}.
  \newblock In Hiroshi Ishikawa, Cheng-Lin Liu, Tomas Pajdla, and Jianbo Shi,
    editors, {\em Computer {{Vision}} \textendash{} {{ACCV}} 2020}, Lecture
    {{Notes}} in {{Computer Science}}, pages 549--565, {Cham}, 2021. {Springer
    International Publishing}.
  
  \bibitem{johnsonSurveyDeepLearning2019a}
  Justin~M. Johnson and Taghi~M. Khoshgoftaar.
  \newblock Survey on deep learning with class imbalance.
  \newblock {\em Journal of Big Data}, 6(1):27, March 2019.
  
  \bibitem{szegedyRethinkingInceptionArchitecture2016a}
  Christian Szegedy, V.~Vanhoucke, S.~Ioffe, Jonathon Shlens, and Z.~Wojna.
  \newblock Rethinking the {{Inception Architecture}} for {{Computer Vision}}.
  \newblock {\em 2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)}, 2016.
  
  \bibitem{goldbergerTrainingDeepNeuralnetworks2017}
  J.~Goldberger and E.~{Ben-Reuven}.
  \newblock Training deep neural-networks using a noise adaptation layer.
  \newblock In {\em International {{Conference}} on {{Learning
    Representations}}}, 2017.
  
  \bibitem{liuClassificationNoisyLabels2014}
  Tongliang Liu and Dacheng Tao.
  \newblock Classification with {{Noisy Labels}} by {{Importance Reweighting}}.
  \newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
    38, November 2014.
  
  \bibitem{hendrycksUsingTrustedData2018}
  Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel.
  \newblock Using {{Trusted Data}} to {{Train Deep Networks}} on {{Labels
    Corrupted}} by {{Severe Noise}}.
  \newblock In {\em Advances in {{Neural Information Processing Systems}}},
    volume~31. {Curran Associates, Inc.}, 2018.
  
  \bibitem{hanCoteachingRobustTraining2018a}
  Bo~Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and
    Masashi Sugiyama.
  \newblock Co-teaching: Robust training of deep neural networks with extremely
    noisy labels.
  \newblock In {\em Advances in {{Neural Information Processing Systems}}},
    volume~31. {Curran Associates, Inc.}, 2018.
  
  \bibitem{linFocalLossDense2020}
  Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
  \newblock Focal {{Loss}} for {{Dense Object Detection}}.
  \newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
    42(2):318--327, February 2020.
  
  \bibitem{renLearningReweightExamples2018}
  Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
  \newblock Learning to {{Reweight Examples}} for {{Robust Deep Learning}}.
  \newblock In {\em Proceedings of the 35th {{International Conference}} on
    {{Machine Learning}}}, pages 4334--4343. {PMLR}, July 2018.
  
  \bibitem{shuMetaWeightNetLearningExplicit2019a}
  Jun Shu, Qi~Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng.
  \newblock Meta-{{Weight}}-{{Net}}: Learning an {{Explicit Mapping For Sample
    Weighting}}.
  \newblock In {\em Advances in {{Neural Information Processing Systems}}},
    volume~32. {Curran Associates, Inc.}, 2019.
  
  \bibitem{huLearningDataManipulation2019a}
  Zhiting Hu, Bowen Tan, Russ~R Salakhutdinov, Tom~M Mitchell, and Eric~P Xing.
  \newblock Learning {{Data Manipulation}} for {{Augmentation}} and
    {{Weighting}}.
  \newblock In {\em Advances in {{Neural Information Processing Systems}}},
    volume~32. {Curran Associates, Inc.}, 2019.
  
  \bibitem{wangOptimizingDataUsage2020b}
  Xinyi Wang, Hieu Pham, Paul Michel, Antonios Anastasopoulos, Graham Neubig, and
    J.~Carbonell.
  \newblock Optimizing {{Data Usage}} via {{Differentiable Rewards}}.
  \newblock In {\em International {{Conference}} on {{Machine Learning}}}, 2020.
  
  \bibitem{vahdatRobustnessLabelNoise2017a}
  Arash Vahdat.
  \newblock Toward {{Robustness}} against {{Label Noise}} in {{Training Deep
    Discriminative Neural Networks}}.
  \newblock In {\em Advances in {{Neural Information Processing Systems}}},
    volume~30. {Curran Associates, Inc.}, 2017.
  
  \bibitem{elkanFoundationsCostsensitiveLearning2001}
  Charles Elkan.
  \newblock The foundations of cost-sensitive learning.
  \newblock In {\em Proceedings of the 17th International Joint Conference on
    {{Artificial}} Intelligence - {{Volume}} 2}, {{IJCAI}}'01, pages 973--978,
    {San Francisco, CA, USA}, August 2001. {Morgan Kaufmann Publishers Inc.}
  
  \bibitem{chawlaSMOTESyntheticMinority2002a}
  Nitesh~V. Chawla, Kevin~W. Bowyer, Lawrence~O. Hall, and W.~Philip Kegelmeyer.
  \newblock {{SMOTE}}: Synthetic minority over-sampling technique.
  \newblock {\em Journal of Artificial Intelligence Research}, 16(1):321--357,
    June 2002.
  
  \bibitem{anandApproachClassificationHighly2010a}
  Ashish Anand, Ganesan Pugalenthi, Gary~B. Fogel, and P.~N. Suganthan.
  \newblock An approach for classification of highly imbalanced data using
    weighting and undersampling.
  \newblock {\em Amino Acids}, 39(5):1385--1391, November 2010.
  
  \bibitem{khanCostSensitiveLearningDeep2018}
  Salman~H. Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous~A. Sohel, and
    Roberto Togneri.
  \newblock Cost-{{Sensitive Learning}} of {{Deep Feature Representations From
    Imbalanced Data}}.
  \newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
    29(8):3573--3587, August 2018.
  
  \bibitem{kangDecouplingRepresentationClassifier2019}
  Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi
    Feng, and Yannis Kalantidis.
  \newblock Decoupling {{Representation}} and {{Classifier}} for
    {{Long}}-{{Tailed Recognition}}.
  \newblock In {\em International {{Conference}} on {{Learning
    Representations}}}, September 2019.
  
  \bibitem{liuLargeScaleLongTailedRecognition2019}
  Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella~X.
    Yu.
  \newblock Large-{{Scale Long}}-{{Tailed Recognition}} in an {{Open World}}.
  \newblock In {\em 2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and
    {{Pattern Recognition}} ({{CVPR}})}, pages 2532--2541, {Long Beach, CA, USA},
    June 2019. {IEEE}.
  
  \bibitem{wangLongtailedRecognitionRouting2020}
  Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella Yu.
  \newblock Long-tailed {{Recognition}} by {{Routing Diverse
    Distribution}}-{{Aware Experts}}.
  \newblock In {\em International {{Conference}} on {{Learning
    Representations}}}, September 2020.
  
  \bibitem{lakeHumanlevelConceptLearning2015a}
  Brenden~M. Lake, Ruslan Salakhutdinov, and Joshua~B. Tenenbaum.
  \newblock Human-level concept learning through probabilistic program induction.
  \newblock {\em Science}, 350(6266):1332--1338, December 2015.
  
  \bibitem{franceschiBilevelProgrammingHyperparameter2018a}
  Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and
    Massimiliano Pontil.
  \newblock Bilevel {{Programming}} for {{Hyperparameter Optimization}} and
    {{Meta}}-{{Learning}}.
  \newblock In {\em Proceedings of the 35th {{International Conference}} on
    {{Machine Learning}}}, pages 1568--1577. {PMLR}, July 2018.
  
  \bibitem{liuDARTSDifferentiableArchitecture2018}
  Hanxiao Liu, Karen Simonyan, and Yiming Yang.
  \newblock {{DARTS}}: Differentiable {{Architecture Search}}.
  \newblock In {\em International {{Conference}} on {{Learning
    Representations}}}, September 2018.
  
  \bibitem{wangLearningModelTail2017}
  Yu-Xiong Wang, Deva Ramanan, and Martial Hebert.
  \newblock Learning to {{Model}} the {{Tail}}.
  \newblock In {\em Advances in {{Neural Information Processing Systems}}},
    volume~30. {Curran Associates, Inc.}, 2017.
  
  \bibitem{jiangMentorNetLearningDataDriven2018a}
  Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~{Fei-Fei}.
  \newblock {{MentorNet}}: Learning {{Data}}-{{Driven Curriculum}} for {{Very
    Deep Neural Networks}} on {{Corrupted Labels}}.
  \newblock In {\em Proceedings of the 35th {{International Conference}} on
    {{Machine Learning}}}, pages 2304--2313. {PMLR}, July 2018.
  
  \bibitem{liLearningLearnNoisy2019a}
  Junnan Li, Yongkang Wong, Qi~Zhao, and Mohan~S. Kankanhalli.
  \newblock Learning to {{Learn From Noisy Labeled Data}}.
  \newblock In {\em 2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and
    {{Pattern Recognition}} ({{CVPR}})}, pages 5046--5054, {Long Beach, CA, USA},
    June 2019. {IEEE}.
  
  \bibitem{alganMetaSoftLabel2021a}
  G.~Algan and I.~Ulusoy.
  \newblock Meta {{Soft Label Generation}} for {{Noisy Labels}}.
  \newblock {\em 2020 25th International Conference on Pattern Recognition
    (ICPR)}, 2021.
  
  \bibitem{zhangDistillingEffectiveSupervision2020a}
  Zizhao Zhang, Han Zhang, Sercan~{\"O}. Arik, Honglak Lee, and Tomas Pfister.
  \newblock Distilling {{Effective Supervision From Severe Label Noise}}.
  \newblock In {\em 2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and
    {{Pattern Recognition}} ({{CVPR}})}, pages 9291--9300, June 2020.
  
  \bibitem{vyasLearningSoftLabels2020b}
  Nidhi Vyas, Shreyas Saxena, and Thomas Voice.
  \newblock Learning {{Soft Labels}} via {{Meta Learning}}.
  \newblock {\em arXiv:2009.09496 [cs, stat]}, September 2020.
  
  \bibitem{saxenaDataParametersNew2019a}
  Shreyas Saxena, Oncel Tuzel, and Dennis DeCoste.
  \newblock Data {{Parameters}}: A {{New Family}} of {{Parameters}} for
    {{Learning}} a {{Differentiable Curriculum}}.
  \newblock In {\em Advances in {{Neural Information Processing Systems}}},
    volume~32. {Curran Associates, Inc.}, 2019.
  
  \bibitem{howardMobileNetsEfficientConvolutional2017a}
  Andrew~G. Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, and Hartwig Adam.
  \newblock {{MobileNets}}: Efficient {{Convolutional Neural Networks}} for
    {{Mobile Vision Applications}}.
  \newblock {\em arXiv:1704.04861 [cs]}, April 2017.
  
  \bibitem{qinRethinkingSoftmaxCrossEntropy2020a}
  Zhenyue Qin, Dongwoo Kim, and Tom Gedeon.
  \newblock Rethinking {{Softmax}} with {{Cross}}-{{Entropy}}: Neural {{Network
    Classifier}} as {{Mutual Information Estimator}}.
  \newblock {\em arXiv:1911.10688 [cs, stat]}, September 2020.
  
  \bibitem{zhaoBetterAccuracyefficiencyTradeoffs2021a}
  Shuai Zhao, Liguang Zhou, Wenxiao Wang, Deng Cai, Tin~Lun Lam, and Yangsheng
    Xu.
  \newblock Towards {{Better Accuracy}}-efficiency {{Trade}}-offs: Divide and
    {{Co}}-training.
  \newblock {\em arXiv:2011.14660 [cs]}, March 2021.
  
  \bibitem{krizhevskyLearningMultipleLayers2009}
  A.~Krizhevsky.
  \newblock {\em Learning {{Multiple Layers}} of {{Features}} from {{Tiny
    Images}}}.
  \newblock {{MSc Thesis}}, University of Toronto, 2009.
  
  \bibitem{heDeepResidualLearning2016}
  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
  \newblock Deep {{Residual Learning}} for {{Image Recognition}}.
  \newblock In {\em 2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern
    Recognition}} ({{CVPR}})}, pages 770--778, June 2016.
  
  \bibitem{loshchilovSGDRStochasticGradient2016}
  Ilya Loshchilov and Frank Hutter.
  \newblock {{SGDR}}: Stochastic {{Gradient Descent}} with {{Warm Restarts}}.
  \newblock {\em International Conference on Learning Representations}, November
    2016.
  
  \bibitem{dengImageNetLargescaleHierarchical2009}
  Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~{Fei-Fei}.
  \newblock {{ImageNet}}: A large-scale hierarchical image database.
  \newblock In {\em 2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern
    Recognition}}}, pages 248--255, June 2009.
  
  \bibitem{kingmaAdamMethodStochastic2015}
  Diederik~P. Kingma and Jimmy Ba.
  \newblock Adam: A {{Method}} for {{Stochastic Optimization}}.
  \newblock {\em International Conference on Learning Representations}, 2015.
  
\end{thebibliography}
  
\input{Sec7_appendix}
\input{Sec8_checklist}

\end{document}
