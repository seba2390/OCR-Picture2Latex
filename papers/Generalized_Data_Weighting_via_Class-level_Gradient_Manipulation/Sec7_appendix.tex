\pdfoutput=1
\appendix

\section{Derivation of D1}
\label{section: Appendx_A}
%
Denote the logit vector as $\boldsymbol x$, we have
\begin{align}
    \boldsymbol p_j =& \frac{e^{\boldsymbol x_j}}{\sum_k e^{\boldsymbol x_k}} \\
    \mathcal{L} =& -\sum_j  \boldsymbol y_j\log{\boldsymbol p_j}.
\end{align}

For the target~(label) position $t$ we have $\boldsymbol y_t=1$  and
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial  \boldsymbol x_t} = \frac{\partial \mathcal{L}}{\partial \boldsymbol p_t}\frac{\partial \boldsymbol p_t}{\partial \boldsymbol x_t} = -\frac{\sum_k e^{\boldsymbol x_k}}{e^{\boldsymbol x_t}}\frac{e^{\boldsymbol x_t}\sum_k e^{\boldsymbol x_k}- e^{2\boldsymbol x_t}}{\left(\sum_k e^{\boldsymbol x_k}\right)^2} = \frac{e^{\boldsymbol x_t}}{\sum_k e^{\boldsymbol x_k}} - 1 = \boldsymbol p_t - \boldsymbol y_t.
\end{equation}

For any other position $j$~($j \neq t$), we have $\boldsymbol y_j = 0$ and 
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial \boldsymbol x_j} = \frac{\partial \mathcal{L}}{\partial \boldsymbol p_t}\frac{\partial \boldsymbol p_t}{\partial \boldsymbol x_j} = -\frac{\sum_k e^{\boldsymbol x_k}}{e^{\boldsymbol x_t}}\frac{-e^{\boldsymbol x_j+\boldsymbol x_t}}{\left(\sum_k e^{\boldsymbol x_k}\right)^2} = \frac{e^{\boldsymbol x_j}}{\sum_k e^{\boldsymbol x_k}} = \boldsymbol p_j - \boldsymbol y_j.
\end{equation}

Therefore, we can conclude that $\boldsymbol D_1$ = $\boldsymbol p - \boldsymbol y$.


\section{Zero-mean Constraint on Class-level Weights}
\subsection{Derivation}
\label{subsection: Appendix_B.1}
\begin{align}
    \label{eq: ana}
    \boldsymbol{\omega}_j(\boldsymbol{p}_j-\mathbf{y}_j) =& \boldsymbol{\omega}_j\boldsymbol{p}_j-\boldsymbol{\omega}_j\mathbf{y}_j\\
    =& \left(\sum_k \boldsymbol{\omega}_k\boldsymbol{p}_k\right)\frac{\boldsymbol{\omega}_j\boldsymbol{p}_j}{\sum_k \boldsymbol{\omega}_k\boldsymbol{p}_k}-\left(\sum_k \boldsymbol{\omega}_k\mathbf{y}_k\right)\frac{\boldsymbol{\omega}_j\mathbf{y}_j}{\sum_k \boldsymbol{\omega}_k\mathbf{y}_k}\\
    &\text{If $\mathbf{y}_j = 0$, the second term of (19) becomes $0$, therefore can be rewritten as $\boldsymbol{\omega}_t\mathbf{y}_j$} \nonumber\\ 
    =& \left(\sum_k \boldsymbol{\omega}_k\boldsymbol{p}_k\right){\boldsymbol{p}'_j}-\boldsymbol{\omega}_t\mathbf{y}_j\\
    =& \boldsymbol{\omega}_t\left({\boldsymbol{p}'_j} - {\mathbf{y}_j}\right) + \left(\sum_k{{\boldsymbol{\omega}_k} {\boldsymbol{p}_k}}-\boldsymbol{\omega}_t\right){\boldsymbol{p}'_j}.\label{app:eq:grad1'}
\end{align}

\subsection{Training without Zero-mean Constraint}
\label{subsection: Appendix_B.2}
\begin{equation} \label{app:eq2}
\begin{split}
f_{\boldsymbol \omega}\left(\nabla_{\boldsymbol \theta} \mathcal{L}\right) =& f_{\boldsymbol \omega}(\sum_j\frac{\partial \mathcal{L}_i}{\partial \boldsymbol{l}_j} \frac{\partial \boldsymbol{l}_j}{\partial \boldsymbol \theta}) \\
    =& \sum_j \boldsymbol{\omega}_j(\boldsymbol{p}_j-\mathbf{y}_j)\frac{\partial \boldsymbol{l}_j}{\partial \boldsymbol \theta} \\
    =& \boldsymbol{\omega}_{t} \sum_j({\boldsymbol{p}'_j}-\mathbf{y}_j)\frac{\partial \boldsymbol{l}_j}{\partial \boldsymbol \theta} + (\sum_k{{\boldsymbol \omega_k} {\boldsymbol{p}_k}}-\boldsymbol{\omega}_{t}) \sum_j {\boldsymbol{p}'_j}\frac{\partial \boldsymbol{l}_j}{\partial \boldsymbol \theta}.
\end{split}
\end{equation}




\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/zero_mean.png}
    \caption{Training curve without zero-mean constraint on CIFAR10 under 40\% uniform noise.}
    \label{fig:zero_mean}
\end{figure}
  
Without zero-mean constraint, the training becomes unstable.
%
We plot the training curve of the CIFAR10 40\% uniform noise setting in Figure~\ref{fig:zero_mean}.

\section{Label Noise Training Setting}
\label{section: Appendix_C}
Following the training setting of \cite{shuMetaWeightNetLearningExplicit2019a}, the classifier network is trained with SGD with a weight decay $5$e-$4$, an initial learning rate of $1$e-$1$ and a mini-batch size of $100$ for all methods. 
%
We use the cosine learning rate decay schedule~\cite{loshchilovSGDRStochasticGradient2016} for a total of 80 epochs.
%
The MLP weighting network is trained with Adam \cite{kingmaAdamMethodStochastic2015} with a fixed learning rate 1e-3 and a weight decay 1e-4.
%
For GLC, we first train 40 epochs to estimate the label corruption matrix and then train another 40 epochs to evaluate its performance.
%
Since Co-teach uses two models, each model is trained for 40 epochs for a fair comparison.
%
We repeat every experiment 5 times with different random seeds (seed=1, 10, 100, 1000, 10000, respectively) for network initialization and label noise generation.
%
We report the average test accuracy over the last 5 epochs as the model's final performance.
%
We use one V100 GPU for all the experiments.

\section{Further Experiments on Imbalanced Setting}
\label{section: Appendix_D}
\subsection{Class-level Weight Analysis}
We conduct one more experiment under the imbalance setting to better verify the interpretability of GDW.
%
As shown in Table~\ref{tab: imb_C}, we report the ratio of the number of increased $\boldsymbol\omega_i$ after gradient update on $C_j$ instances in one epoch, where $C_0$ is the largest class and $C_9$ is the smallest class.

Note that $\boldsymbol \omega_i$ on $C_i$ contains the "is $C_i$" information in the dataset. As a result, $\boldsymbol \omega_i$ on $C_i$ should be large for small classes and small for large classes. As shown above, the ratio of increased $\boldsymbol \omega_i$ on $C_i$ (the diagonal elements) increases from $0.036$ to $0.935$ as $i$ increases from $0$ to $9$.

On the other hand, $\boldsymbol \omega_i$ on $C_j$ ($j \neq i$) contains the "not $C_i$" information in the dataset. If $i$ is a large class, $\boldsymbol \omega_i$ on $C_j$ ($j \neq i$) should be large and vice versa. For $\boldsymbol \omega_i$ ($i=0, 1, 2, 3, 4$), the ratio of increased $\boldsymbol \omega_i$ on $C_j$ ($j \neq i$) are generally larger than $0.5$, and for $\boldsymbol \omega_i$ ($i=5, 6, 7, 8, 9$), the ratio of increased $\boldsymbol \omega_i$ on $C_j$ ($j \neq i$) are generally less than $0.5$. These results align with our analysis on the interpretable information of gradient flows.

\begin{table}[htbp]
    \centering
    \caption{Ratio of increased class-level weights under the imbalance setting.}
    \label{tab: imb_C}
    \begin{tabular}{@{}ccccccccccc@{}}
    \toprule
weight/class & \(C_0\) & \(C_1\) & \(C_2\) & \(C_3\) & \(C_4\) & \(C_5\)
& \(C_6\) & \(C_7\) & \(C_8\) & \(C_9\) \\
\midrule
%\endhead
\(\boldsymbol\omega_0\) & 0.036 & 0.968 & 0.973 & 0.972 & 0.965 & 0.974 & 0.972 &
0.976 & 0.956 & 0.973 \\
\(\boldsymbol\omega_1\) & 0.887 & 0.095 & 0.912 & 0.929 & 0.907 & 0.927 & 0.911 &
0.922 & 0.910 & 0.920 \\
\(\boldsymbol\omega_2\) & 0.848 & 0.844 & 0.141 & 0.839 & 0.822 & 0.845 & 0.818 &
0.847 & 0.829 & 0.802 \\
\(\boldsymbol\omega_3\) & 0.585 & 0.608 & 0.552 & 0.405 & 0.569 & 0.541 & 0.561 &
0.559 & 0.617 & 0.596 \\
\(\boldsymbol\omega_4\) & 0.474 & 0.521 & 0.420 & 0.460 & 0.509 & 0.455 & 0.456 &
0.482 & 0.467 & 0.512 \\
\(\boldsymbol\omega_5\) & 0.291 & 0.261 & 0.288 & 0.252 & 0.309 & 0.701 & 0.303 &
0.267 & 0.297 & 0.257 \\
\(\boldsymbol\omega_6\) & 0.199 & 0.189 & 0.169 & 0.198 & 0.196 & 0.222 & 0.778 &
0.195 & 0.207 & 0.182 \\
\(\boldsymbol\omega_7\) & 0.117 & 0.117 & 0.105 & 0.084 & 0.115 & 0.079 & 0.126 &
0.920 & 0.133 & 0.090 \\
\(\boldsymbol\omega_8\) & 0.115 & 0.124 & 0.178 & 0.185 & 0.184 & 0.174 & 0.191 &
0.181 & 0.862 & 0.137 \\
\(\boldsymbol\omega_9\) & 0.043 & 0.050 & 0.064 & 0.061 & 0.074 & 0.062 & 0.097 &
0.069 & 0.040 & 0.935 \\
\bottomrule
    \end{tabular}
\end{table}
\subsection{Experiments on Places-LT}

\begin{table}[htbp]
    \centering
    \caption{Test accuracy on Places-LT.}
    \label{tab:mix_placeLT}
    \begin{tabular}{@{}ccccc@{}}
\toprule
Method & L2RW & INSW & MW-Net & \textbf{GDW} \\
\midrule
%\endhead
Accuracy (\%) & \(15.08\) & \(17.80\) & \(18.08\) & \textbf{19.17} \\
\bottomrule
    \end{tabular}
    
\end{table}
We have conducted experiments on the Places-LT dataset \cite{liuLargeScaleLongTailedRecognition2019} and compared GDW with other meta-learning-based methods.
%
For all methods, the weight decay is set to $0.001$ and the batchsize is set to $64$. We adopt a $0.01$ initial learning rate and a cosine learning rate decay policy for $10$ epochs. The weight decay is set to $0.001$. The backbone network is ResNet18 and we use the ImageNet pre-trained model for initialization.

As shown in Table~\ref{tab:mix_placeLT}, GDW achieves the best performance among all the comparison methods and outperforms MWNet by $1.09\%$. This improvement is larger than that of CIFAR100. The reason is that GDW can manipulate class-level information and thus performs better on the dataset with a larger number of classes ($365$ in Places-LT and $100$ in CIFAR100). Besides, we can observe that L2RW performs the worst and the reason may be that L2RW suffers from unstable weights~\cite{shuMetaWeightNetLearningExplicit2019a}.

\section{Real-world Training Setting}
\label{section: Appendix_E}
Similar to \cite{shuMetaWeightNetLearningExplicit2019a}, we use the $7$k validation set as the meta set and the origin test set to evaluate the classifier's final performance.
%
For GLC, we first train $2$ epochs to estimate the label corruption matrix and then train another $3$ epochs to evaluate its performance.
%
Since Co-teach uses two models, each model is trained for $3$ epochs for a fair comparison.

\section{Experiments on Mixed Setting}
\label{section: Appendix_F}
We conduct further experiments to verify the performance of GDW in the mixed setting, i.e. the coexistence of label noise and class imbalance.
%
Specifically, we compare GDW with the mostly second-best method MW-Net \cite{shuMetaWeightNetLearningExplicit2019a} under the mixed setting of uniform noise and class imbalance on CIFAR10 and CIFAR100. 
%
As shown in Table~\ref{tab:mix}, GDW demonstrates great performance gain over MW-Net, which means GDW can simultaneously better tackle both problems. 

\begin{table}[htbp]
    \centering
    \caption{Test accuracy under mixed settings.}
    \label{tab:mix}
    \begin{tabular}{@{}ccccc@{}}
    \toprule
    Dataset & Noise Ratio & Imb Factor & MW-Net & GDW \\
    \midrule
    CIFAR10 & \(0.40\) & \(0.10\) & \(71.54\) & \(76.30\) \\
    CIFAR10 &\(0.60\) &\(0.10\) & \(61.62\) & \(70.24\) \\
    CIFAR10 & \(0.40\) & \(0.01\) & \(48.04\) & \(48.53\) \\
    CIFAR10 & \(0.60\) & \(0.01\) & \(39.51\) & \(40.07\) \\
    CIFAR100 &\(0.40\) &\(0.10\) & \(36.10\) & \(38.20\) \\
    CIFAR100 &\(0.60\) &\(0.10\) & \(24.80\) & \(25.40\) \\
    CIFAR100 &\(0.40\)& \(0.01\) & \(21.26\) & \(22.07\) \\
    CIFAR100 &\(0.60\) &\(0.01\) & \(12.75\) & \(14.15\) \\
    \bottomrule
    \end{tabular}
\end{table}