\pdfoutput=1
\section{Related Works}
\subsection{Non-Meta-Learning Methods for Label Noise}
%
Label noise is a common problem in classification tasks~\cite{zhuClassNoiseVs2004a,frenayClassificationPresenceLabel2014a,alganLabelNoiseTypes2020b}.
%
To avoid overfitting to label noise, \cite{szegedyRethinkingInceptionArchitecture2016a} propose label smoothing to regularize the model.
%
\cite{goldbergerTrainingDeepNeuralnetworks2017,vahdatRobustnessLabelNoise2017a} form different models to indicate the relation between noisy instances and clean instances.
%
\cite{liuClassificationNoisyLabels2014} estimate an importance weight for each instance to represent its value to the model.
%
\cite{hanCoteachingRobustTraining2018a} train two models simultaneously and let them teach each other in every mini-batch.
%
However, without a clean dataset, these methods cannot handle severe noise~\cite{renLearningReweightExamples2018}.
%
\cite{hendrycksUsingTrustedData2018} correct the prediction of the model by estimating the label corruption matrix via a clean validation set, but this matrix is the same across all instances.
%
Instead, our method generates dynamic class-level weights for every instance to improve training.

\subsection{Non-Meta-Learning Methods for Class Imbalance}
%
Many important works have been proposed to handle class imbalance~\cite{elkanFoundationsCostsensitiveLearning2001,chawlaSMOTESyntheticMinority2002a,anandApproachClassificationHighly2010a,dongClassRectificationHard2017a,khanCostSensitiveLearningDeep2018,cuiClassBalancedLossBased2019,kangDecouplingRepresentationClassifier2019,linFocalLossDense2020,sinhaClassWiseDifficultyBalancedLoss2021a}.
%
\cite{chawlaSMOTESyntheticMinority2002a,anandApproachClassificationHighly2010a} propose to over-sample
the minority class and under-sample the majority class.
%
\cite{elkanFoundationsCostsensitiveLearning2001,khanCostSensitiveLearningDeep2018} learn a class-dependent cost matrix to obtain robust representations for both majority and minority classes.
%
\cite{dongClassRectificationHard2017a,cuiClassBalancedLossBased2019,linFocalLossDense2020,sinhaClassWiseDifficultyBalancedLoss2021a} design a
reweighting scheme to rebalance the loss for each class.
%
These methods are quite effective, whereas they need to manually choose loss functions or hyper-parameters.
%
\cite{liuLargeScaleLongTailedRecognition2019,wangLongtailedRecognitionRouting2020} manipulate the feature space to handle class imbalance while introducing extra model parameters.
%
\cite{kangDecouplingRepresentationClassifier2019} decouple representation learning and classifier learning on long-tailed datasets, but with extra hyper-parameter tuning.
%
In contrast, meta-learning methods view instance weights as hyper-parameters and dynamically update them via a meta set to avoid hyper-parameter tuning.

\subsection{Meta-Learning Methods}
%
With recent development in meta-learning~\cite{lakeHumanlevelConceptLearning2015a,franceschiBilevelProgrammingHyperparameter2018a,liuDARTSDifferentiableArchitecture2018}, many important methods have been proposed to handle label noise and class imbalance via a meta set~\cite{wangLearningModelTail2017,jiangMentorNetLearningDataDriven2018a,renLearningReweightExamples2018,liLearningLearnNoisy2019a,shuMetaWeightNetLearningExplicit2019a,huLearningDataManipulation2019a,wangOptimizingDataUsage2020b,alganMetaSoftLabel2021a}.
%
\cite{jiangMentorNetLearningDataDriven2018a} propose MentorNet to provide a data-driven curriculum for the base network to focus on correct instances.
%
To distill effective supervision, \cite{zhangDistillingEffectiveSupervision2020a} estimate pseudo labels for noisy instances with a meta set.
%
To provide dynamic regularization, \cite{vyasLearningSoftLabels2020b,alganMetaSoftLabel2021a} treat labels as learnable parameters and adapt them to the modelâ€™s state.
%
Although these methods can tackle label noise, they introduce huge amounts of learnable parameters and thus cannot scale to large datasets.
%
To alleviate class imbalance, \cite{wangLearningModelTail2017} describe a method to learn from long-tailed datasets.
%
Specifically, \cite{wangLearningModelTail2017} propose to encode meta-knowledge into a meta-network and model the tail classes by transfer learning. 
 
\begin{table}
  \caption{Related works comparison. 
  %
"Noise" and "Imbalance" denote whether the method can solve label noise and class imbalance.
%
"Class-level" denotes whether the method utilizes class-level information in each instance, and "Scalability" denotes whether the method can scale to large datasets.}
  \label{tab: comparison_methods}
  \scalebox{0.6}{
  \centering
  \begin{tabular}{@{}c c c c c c c c c c c@{}}
    \toprule
     & Focal \cite{linFocalLossDense2020} & Balanced \cite{cuiClassBalancedLossBased2019} & Co-teaching \cite{hanCoteachingRobustTraining2018a} & GLC \cite{hendrycksUsingTrustedData2018} & L2RW \cite{renLearningReweightExamples2018} & INSW \cite{huLearningDataManipulation2019a} & MWNet \cite{shuMetaWeightNetLearningExplicit2019a} & Soft-label \cite{vyasLearningSoftLabels2020b} & Gen-label \cite{alganMetaSoftLabel2021a}& \textbf{GDW} \\
    \midrule
    Noise & \XSolid & \XSolid & \Checkmark & \Checkmark & \Checkmark & \Checkmark &\Checkmark &\Checkmark &\Checkmark &\Checkmark     \\
    Imbalance & \Checkmark& \Checkmark& \XSolid & \XSolid & \Checkmark &  \Checkmark & \Checkmark & \XSolid & \XSolid & \Checkmark     \\
    Class-level &  \XSolid & \XSolid &  \XSolid & \XSolid &  \XSolid & \XSolid &  \XSolid & \Checkmark &  \Checkmark & \Checkmark  \\
    Scalability & \Checkmark& \Checkmark& \Checkmark & \Checkmark & \Checkmark &  \XSolid & \Checkmark & \XSolid & \XSolid & \Checkmark     \\
    \bottomrule
  \end{tabular}
  }
\end{table}

%
Furthermore, many meta-learning methods propose to mitigate the two issues by reweighting every instance~\cite{renLearningReweightExamples2018,saxenaDataParametersNew2019a,shuMetaWeightNetLearningExplicit2019a,huLearningDataManipulation2019a,wangOptimizingDataUsage2020b}.
%
\cite{saxenaDataParametersNew2019a} equip each instance and each class with a learnable parameter to govern their importance.
%
By leveraging a meta set, \cite{renLearningReweightExamples2018,shuMetaWeightNetLearningExplicit2019a,huLearningDataManipulation2019a,wangOptimizingDataUsage2020b} learn instance weights and model parameters via bi-level optimization to tackle label noise and class imbalance.
%
\cite{renLearningReweightExamples2018} assign weights to training instances only based on their gradient directions.
%
Furthermore, \cite{huLearningDataManipulation2019a} combine reinforce learning and meta-learning, and treats instance weights as rewards for optimization.
%
However, since each instance is directly assigned with a learnable weight, INSW can not scale to large datasets.
%
Meanwhile, \cite{shuMetaWeightNetLearningExplicit2019a,wangOptimizingDataUsage2020b} adopt a weighting network to output weights for instances and use bi-level optimization to jointly update the weighting network parameters and model parameters.
%
Although these methods handle label noise and class imbalance by reweighting instances, a scalar weight for every instance cannot capture class-level information, as shown in Figure~\ref{fig:motiv}.
%
Therefore, we introduce class-level weights for different gradient flows and adjust them to better utilize class-level information.
%

We show the differences between GDW and other related methods in Table \ref{tab: comparison_methods}.


