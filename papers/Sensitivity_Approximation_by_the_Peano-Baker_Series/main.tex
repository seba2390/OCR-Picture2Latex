\documentclass[DIV=12]{scrartcl} % this is (kind of) the Europe version of article (a4 paper and margins)
\usepackage{lmodern}             % this (latin modern) is the vectorised form of the standard latex font (computer modern)
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% Personal style file
\usepackage{Nyquist_sty}
\usepackage[names,dvipsnames]{xcolor}
\usepackage{enumerate}

% For editing
\newcommand{\pn}[1]{{\color{red}\textbf{PN}: #1}}
\newcommand{\fm}[1]{{\color{blue}\textbf{FM}: #1}}
\newcommand{\oer}[1]{{\color{orange}\textbf{OE}: #1}}
\newcommand{\ak}[1]{{\color{YellowOrange}\textbf{AK}: #1}}

% math symbols 
%\newcommand{\err}{\textit{err}}
%\newcommand{\err}{\ensuremath{\varkappa}}
\newcommand{\err}{\ensuremath{e}}
\newcommand{\rerr}{\ensuremath{re}}
\newcommand{\BDF}{\texttt{CVODE\_BDF}}
\usepackage{amsmath,amssymb,amsthm,hyperref,authblk}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem*{assumption*}{Assumption}
\usepackage{microtype}           % this makes the justified text even better (general font related tweaks)
% \renewcommand\labelitemi{--}
\usepackage{ulem}
\usepackage[ruled,vlined]{algorithm2e}
\normalem

\usepackage{biblatex}

\usepackage{tikz}
\usetikzlibrary{arrows,positioning} 
\tikzset{
    %Define standard arrow tip
    >=stealth',
    %Define style for boxes
    nodesty/.style={
           rectangle,
           rounded corners,
           draw=gray, very thick,
           %text width=6.5em,
           minimum height=2em,
           inner sep = .7em,
           text centered},
    % Define arrow style
    arrsty/.style={
           ->,
           gray,
           thick,
           shorten <=2pt,
           shorten >=2pt,}
}

\addbibresource{bibliography.bib}

\setcapindent{0em}
\addtokomafont{captionlabel}{\bfseries}

\renewcommand\Authfont{\fontsize{12}{14.4}\selectfont}
\renewcommand\Affilfont{\fontsize{9}{10.8}\itshape}

\title{Sensitivity Approximation by the Peano-Baker Series}

\author[2,3]{Olivia Eriksson}
\author[3,4]{Andrei Kramer}
\author[1]{Federica Milinanni}
\author[1]{Pierre Nyquist}
\affil[1]{Department of Mathematics, KTH Royal Institute of Technology, Stockholm, Sweden}
\affil[2]{Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden}
\affil[3]{Science for Life Laboratory, Solna, Sweden}
\affil[4]{Department of Neuroscience, Karolinska Institute, Solna, Sweden}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
\noindent In this paper we develop a new method for numerically approximating sensitivities in parameter-dependent ordinary differential equations (ODEs). Our approach, intended for situations where the standard forward and adjoint sensitivity analysis become too computationally costly for practical purposes, is based on the Peano-Baker series from control theory. We give a representation, using this series, for the sensitivity matrix $\bfS$ of an ODE system and use the representation to construct a numerical method for approximating $\bfS$. We prove that, under standard regularity assumptions, the error of our method scales as $O(\Delta t ^2 _{max})$, where $\Delta t _{max}$ is the largest time step used when numerically solving the ODE. We illustrate the performance of the method in several numerical experiments, taken from both the systems biology setting and more classical dynamical systems. The experiments show the sought-after improvement in running time of our method compared to the forward sensitivity approach. For example, in experiments involving a random linear system, the forward approach requires roughly $\sqrt{n}$ longer computational time, where $n$ is the dimension of the parameter space, than our proposed method.

\end{abstract}

\noindent \textbf{Key words and phrases:} Sensitivity analysis; Peano-Baker series; ordinary differential equations; error analysis.\\

\noindent \textbf{MSC 2010 subject classifications:} 65L05; 65L20.

\section{Introduction}
\label{sec:intro}



Mathematical models are used in all areas of science and engineering to model more and more complex real-world phenomena. An important aspect of such modeling is to understand how changes and uncertainty in model parameters translate to the output of a model. The first question is the topic of \textit{sensitivity analysis}, an active research area at the intersection of several branches of mathematics and its applications; see e.g.\ \cite{Frank78, CS85, Cacuci03, STCR04, CIBN05} and references therein. Motivated by problems arising in systems biology, specifically concerning statistical inference and uncertainty quantification for models based on non-linear dynamical systems, in this paper we consider the rather classical question of local sensitivity analysis in ODE models. More precisely, we are interested in the design of efficient numerical methods for approximating the sensitivity matrix for such models.

The general starting point is a system of ordinary differential equations (ODEs), which we take to be of the form
\begin{equation*}
%\label{eq:ODE}
\begin{cases}
\dot{\bfx}(t) = f(\bfx(t),\bfu(t),\bfp),\\
\bfx(t_0) = \bfx_0,
\end{cases}    
\end{equation*}
where $\bfx(t)$ represents the state of the system at time $t$, $\bfu$ corresponds to external inputs into the system and $\bfp$ denotes a set of model parameters; a more precise definition, including the state spaces of the different quantities involved, is given in Section \ref{sec:problem}. 
For such a model, it is important to understand how changes in the parameter-vector $\bfp$ affect the output $\bfx$. We are interested in computing the derivatives $\partial x_i (t) / \partial p_j$, the \textit{sensitivities} of the model, for all component-combinations $x_i, p_j$. The sensitivities provide local information about the parameter space important for a variety of tasks within modeling where this space is explored,
e.g., quantifying uncertainty, finding optimal parameters and experimental design \cite{CS85, Cacuci03, CIBN05, CE17}. 

For all but very simple systems the sensitivities are not available in an (explicit) analytical form. Instead we have to turn to different types of approximations \cite{Cacuci03, CIBN05}. The two standard approaches for obtaining numerical approximations to the sensitivities of an ODE or PDE model are (i) the \textit{forward}, or \textit{variational} approach \cite{CS85, Cacuci03}, and (ii) the \textit{adjoint} approach \cite{Marchuk95, MAS96, CLPR03}. There is a vast literature on both methods and here we only give a brief review; for a comparison of the two approaches, see \cite{CE17}.

Forward sensitivity analysis is the more straightforward method of the two. It is based on finding an ODE system satisfied by the sensitivities $\partial x_i (t) / \partial p_j$, along with appropriate initial values. The solution to this system can then be approximated together with that of the original ODE for $\bfx$ \cite{Cacuci03, CE17}. It is well-known that for high-dimensional problems — $n_x\times n_p\gg 1$, where $n_x$ and $n_p$ are the dimension of the state space and parameter space, respectively — computation of the sensitivities with the forward sensitivity analysis becomes slow. As a consequence, this method can be too computationally costly for certain applications, and more efficient methods are needed.

Adjoint sensitivity analysis is designed to ease the computation of sensitivities with respect to $\bfp$ of an objective function
\begin{align*}
    G(\bfx, \bfp) = \int _0 ^T g(\bfx, t, \bfp) dt,
\end{align*}
for some function $g$ (certain smoothness assumptions are necessary for $g$). Alternatively, the method can be used to compute sensitivities of $g(\bfx (T), T, \bfp)$, that is a function only defined at the final time $T$. The method amounts to introducing an augmented objective function from $G$ by introducing Lagrange multipliers associated with the underlying ODE, and computing the derivatives $dG/dp_j$ of this objective function; see e.g.\ \cite{CLPR03, Marchuk95, MAS96} for the details.

In contrast to the forward sensitivity analysis, the adjoint method does not rely on the actual state sensitivities, $\partial x_i / \partial p_j$, but rather on the adjoint state process. It is possible, albeit impractical, to formulate the problem so that the adjoint sensitivity analysis computes $\partial x_i / \partial p_j$, the quantities of interest in this paper: define $g(\bfx,t,\bfp)=x_i(t)$ and use the adjoint sensitivity method to compute the sensitivities of $g$ at the specific times $T=t_k$. However, this must be repeated for each component $x_i$ of the state $\bfx$ and for each time $t_k$ at which an approximation of the sensitivity of the ODE model is sought. As a result, the use of adjoint sensitivity is not recommended for this purpose, especially when (i) the dimension of the state space is large, and (ii) there are many time points $t_k$ at which the sensitivities are to be computed.

 
Before moving to the general setting and problem formulation, we expand briefly on our particular motivation for looking at this problem; note that although our original motivation comes from systems biology, the problem of computing sensitivities in an ODE model is of great interest in a number of fields. 

Our motivation comes from uncertainty quantification for models of intracellular pathways, such as those studied in \cite{Eriksson19}. In this setting, the state process $\bfx(t)$ corresponds to the concentration of different compounds internal to the cell (e.g. \ proteins or protein complexes like CaMKII or PP2B, or calcium ions), 
and the parameters $\bfp = (p_1, \dots , p_{n_p})$ encode e.g.\ rate constants of the chemical reactions involved in the system.  We are interested in uncertainty quantification for the unknown parameters in a Bayesian setting. This requires computing posterior distributions, which in turn requires Markov chain Monte Carlo (MCMC) methods. Recent developments in the MCMC community propose to take a (differential-) geometric perspective on sampling, exploiting potential underlying geometrical concepts related to statistical inference \cite{GiroCal, TSA20, BFXKG18}. This relies on an appropriate choice of the metric tensor on the parameter space and a particular choice of interest is the expected Fisher information \cite{GiroCal}: for a generic random variable $Y$ and associated parameter $\theta$, with conditional density $p_{Y | \theta}$, the expected Fisher information is defined as
\begin{align*}
    %\label{eq:Fisher}
    F (\theta) = - E_{Y | \theta} \left[ \frac{\partial ^2}{ \partial \theta ^2} \log \left( p_{Y |\theta} (Y | \theta) \right)  \right].
\end{align*}
When combined with an ODE model, the entries of $F$ contain the sensitivities of the model and to efficiently implement corresponding MCMC methods, we first need an efficient method for repeatedly computing the corresponding sensitivities. Forward sensitivity analysis is too slow and because the full sensitivity matrix (see Section \ref{sec:problem} for the definition) is required, the adjoint sensitivity analysis is not appropriate for this type of problem. We therefore propose a new method, based on the Peano-Baker series, that can resolve the issue of computational burden; using the method in the context of MCMC sampling is ongoing work and in this paper we consider the method solely from a numerical analysis perspective.

The method is based on the representations of the sensitivity matrix given in Theorem \ref{thm:generalODESol} and Corollary \ref{cor:Sequi}, and is outlined in Algorithms \ref{alg:approx}, \ref{alg:PBSR} and \ref{alg:ExpAlg}. The main theoretical result of the paper is Theorem \ref{thm:error}, which gives the error rate of the proposed method in terms of the time discretisation used for solving the underlying ODE. The theoretical results are complemented by numerical experiments and comparison to the forward sensitivity analysis shows significant improvement. 

The remainder of the paper is organised as follows. In Section \ref{sec:problem} we give a precise formulation of the ODE model and the associated sensitivities of interest. Next, in Section \ref{sec:method} we derive analytical representations of the sensitivities in two different cases: when the solution is at equilibrium, %constant external input
and in the general case. In Section \ref{sec:genSolution} the analytical representations are used to propose a new method for approximating sensitivities in the ODE setting considered here. An error analysis of the proposed method is given in Section \ref{sec:errEst} and in Section \ref{sec:numerical} we present some numerical experiments that showcase the performance of the method. In particular we compare our method with the forward sensitivity approach and show superior performance. We end the paper with a brief discussion in Section \ref{sec:conclusion}.

\subsection{Notation}
The following notation is used. Elements in $\bR ^n$ are denoted by bold font, e.g. $\bfx, \bfy, \bfz$, whereas $x,y,z$ denote elements of $\bR$; note that the relevant dimension $n$ will change between different vectors. With some abuse of notation, $0$ denotes the zero element in $\bR ^n$ for any $n\geq 1$. The identity matrix is denoted by $\bfI$, or $I_n$ whenever we want to emphasise the size $n \times n$. For a matrix $\bfA$, the matrix exponential $e^\bfA$ is defined in the usual way:
\[
    e^\bfA = \sum _{h \geq 0} \frac{1}{h!} \bfA ^h. 
\]
Objects with a hat, such as $\hat f$ or $\hat \bfx$, refer to approximations. 

For $k \in \bN$ and an open set $A$ in $\bR ^n$, $C^k (A;\bR^m)$ is the space of continuous functions on $A$, taking values in $\bR ^m$, with continuous partial derivatives up to the $k$th order; for $m \geq 2$, the latter is defined in the usual way through natural projections. For a compact set $K$, $C^k(K; \bR ^m)$ refers to functions that are $C^k(A; \bR ^m)$ on some open neighbourhood $A$ of $K$. For a function $f: \bR ^n \to \bR ^m$, $\nabla f$ denotes the Jacobian matrix of $f$. For a normed space $(\calX, ||\cdot||)$, $C^0([0,T];\calX)$ denotes the space of continuous functions from $[0,T]$ to $\calX$ and $||\cdot || _{[0,T]}$ denotes the supremum norm over this space: for $g \in C^0 ([0,T];\calX)$,
\begin{align*}
    ||g||_{[0,T]} = \sup _{t \in [0,T]} ||g (t) ||.
\end{align*}
%
Unless otherwise stated, we will let $|| \cdot ||$ denote an arbitrary matrix norm; because we are working on a finite-dimensional space, the specific choice of matrix norm is not important for the results of this paper (discussed more in Section \ref{sec:errEst}). 
Additional notation that is particular to this work is defined as needed, particularly in the error analysis in Section \ref{sec:errEst}.

\section{Problem formulation}
\label{sec:problem}

The general problem of interest is to compute sensitivities for the solution of the parametrised initial value problem (IVP) %\eqref{eq:ODE}
\begin{equation}
\label{eq:ODE}
\begin{cases}
\dot{\bfx}(t) = f(\bfx(t),\bfu(t),\bfp).\\
\bfx(t_0) = \bfx_0.
\end{cases}    
\end{equation}
Here, for each $t \geq t_0$, the solution $\bfx (t)$ and control $\bfu (t)$ are in $\bR ^{n_x}$ and $\bR ^{n_u}$, respectively, and the parameter vector $\bfp$ is in $\bR ^{n_p}$. As noted in Section \ref{sec:intro}, although our interest in this problem comes from a desire to conduct uncertainty quantification for models in systems biology, the problem is much more general and arises in a wide variety of areas within applied mathematics. We therefore work in a general framework for the remainder of this paper. 

To avoid issues with the existence or uniqueness of solutions, we make the following assumption; we have not aimed for generality and the assumptions put forth in the paper can most likely be weakened considerably in specific settings.
\begin{assumption*}
%\label{ass:ODE}
The function $f: \bR ^{n_x} \times \bR ^{n_u} \times \bR ^{n_p} \to \bR ^{n_x}$ is uniformly globally Lipschitz in the first coordinate given any choices of $\bfu$ and $\bfp$.
\end{assumption*}
This assumption is made throughout the paper, without being referred to explicitly in the upcoming sections.

\begin{definition}
Given a solution $\bfx$ of the ODE system \eqref{eq:ODE}, the \textit{sensitivity} at time $t$ of the $l$-th component of $\bfx$ with respect to parameter $p_i$ is defined as
\begin{equation}
  S_{l}^{~i}(t)=\frac{\partial x_l(t)}{\partial p_i},\quad l=1,\dots,n_x,\;i=1,\dots,n_p.\label{eq:Sli}
\end{equation}
\end{definition}
By differentiating \eqref{eq:ODE} with respect to $\bfp$, we obtain that the time derivative of the sensitivities can be expressed in matrix form as 

\begin{equation}
\label{ODE_S}
    \dot{\bfS}(t) = \nabla_x f(t)\cdot \bfS(t) + \nabla_pf(t),
\end{equation}
where $\dot{\bfS}(t), \bfS(t), \nabla_p f(t)\in\mathbb{R}^{n_x\times n_p}$ are matrices with elements $(l,i)\in\{1,\dots,n_x\}\times\{1,\dots,n_p\}$ given, respectively, by
\[\dot{S}_{l}^{~i}(t)\,,\quad\quad S_{l}^{~i}(t)\quad\quad \text{and}\quad\quad\frac{\partial f_{l}(\bfx(t),\bfu(t),\bfp)}{\partial p_i}\,,\]
and $\nabla_xf(t)\in\mathbb{R}^{n_x\times n_x}$ with entries
\[
\nabla_xf(t)_{l}^{~j}=\frac{\partial f_{l}(\bfx(t),\bfu(t),\bfp)}{\partial x_j}, \ \ (l,j)\in\{1,\dots,n_x\}\times\{1,\dots,n_x\}.
\]

Because the initial condition $\bfx_0$ does not depend on $\bfp$, we have $S_{l}^i(0)=\frac{\partial x_{0,l}}{\partial p_i}=0$ and equation \eqref{ODE_S} is a linear ODE system with initial condition $\bfS\big|_{t=t_0}=\bfS_0=0\in\mathbb{R}^{n_x\times n_p}$.

Let $\{t_k\}_{k=1}^K$, with $t_0 < t_1 < \dots < t_K$, be the time instants at which we want to compute the sensitivity matrix $\bfS$, and let $\bfS_k\in\bR^{n_x\times n_p}$ denote the exact sensitivity matrices at times $t_k$, with $k=0,\dots,K$. We can then formulate $K$ ODE problems in an iterative fashion, 
\begin{equation}
\label{sensitivityODE}
    \begin{cases}
    \dot{\bfS}(t) = \nabla_xf(t)\cdot \bfS(t) + \nabla_pf(t)\\
    \bfS(t_{k})=\bfS_{k}
    \end{cases},\quad k=0,\dots,K-1,
\end{equation}
where the $(k+1)$th problem in the sequence is used to determine the solution at time step $t_{k+1}$, given the sensitivity matrix at the previous time step $t_k$ as initial condition. We will adopt a slight abuse of notation and refer to $\bfS$ as the sensitivity matrix, and similar for the corresponding approximations, although to be precise it is a matrix-valued function. 

The matrix of coefficients $\nabla_xf$ and the forcing term $\nabla_pf$ in \eqref{sensitivityODE} both depend on $\bfx(t), \bfu(t)$, and $\bfp$, and computing the solution $\bfS(t)$ therefore becomes difficult task in general. We address this problem by developing a new, efficient algorithm for approximating the sensitivity matrix $\bfS$ at times $\{t_k\}_{k=1}^K$.


A comment on the sequence $\{ t_k\} _{k=1} ^K$ of times at which $\bfS$ is to be evaluated is in place. In Section~\ref{sec:intro} we briefly discussed the specific application of approximating the sensitivity matrix of an ODE in the context of uncertainty quantification in biological models. There, as in many other application areas, it is natural to consider a sequence of time instants $t_i, i=1,\dots, n_t$, that represent times of measurements of experimental data. However, in experimental settings it is rather common to have measurements only at a small number $n_t$ of times. In order to obtain a good numerical approximation of \eqref{sensitivityODE}, we therefore need a finer collection of time steps. For this reason, even when there are physical time instants to consider, we introduce a finer collection of time steps $\tilde{t}_0,\dots,\tilde{t}_K$, such that it contains all the time steps at which measurements $\bfy_1,\dots,\bfy_{n_t}$ are available, i.e.\ $\{t_1,\dots,t_{n_t}\}\subset \{\tilde{t}_0,\dots,\tilde{t}_K\}$; the inclusion is enforced because in this experimental setting the goal is to approximate the sensitivity matrix $\bfS$ at the $n_t$ measurement times. Moreover, we let $\tilde{t}_0$ be equal to the time of the initial condition: $\tilde{t}_0=t_0$. To simplify and to conform to the notation used in \eqref{sensitivityODE}, we drop the tilde from the new time steps $\{\tilde{t}_k\}_{k=0}^K$, which in the following are denoted by $t_0,\dots,t_K$.   

\section{Analytical solutions for the sensitivity matrix $\bfS$}
\label{sec:method}
In this section we derive analytical representations for the sensitivity matrix $\bfS(t)\in\mathbb{R}^{n_x\times n_p}$ along the trajectory of the solution $\bfx(t)$ of the original ODE system \eqref{eq:ODE}. To simplify notation, we will here denote $\nabla_x f (\bfx(t), \bfu(t), \bfp)$ and $\nabla_p f(\bfx (t), \bfu(t), \bfp)$ as time dependent matrices $\bfA(t)\in\mathbb{R}^{n_x\times n_x}$ and $\bfB(t)\in\mathbb{R}^{n_x\times n_p}$, respectively, defined on a closed interval $I\subset\bR$ such that $[t_0;t_K]\subset I$. The system \eqref{sensitivityODE} then takes the form
\begin{equation}
\label{linODE}
    \begin{cases}
    \dot{\bfS}(t) = \bfA(t)\cdot \bfS(t) + \bfB(t),\\
    \bfS(t_{k})=\bfS_{k},
    \end{cases}
\end{equation}
which in general represents a first order inhomogeneous linear differential equation with non-constant coefficient matrix $\bfA(t)$ and forcing term $\bfB(t)$. Such systems are well-understood from a theoretical point of view \cite{BrockettRogerW.2015Fdls, RughWilsonJ1993Lst} and there exist a variety of numerical methods for solving them \cite{HNW93, HW10, GH10, SauerTim2006Na}. %\pn{Better placed in intro?}. 
Although efficient numerical methods are readily available for the $n_x$-dimensional system in $\bfx(t)$ \eqref{eq:ODE}, the potentially high-dimensional nature of the associated sensitivity problem \eqref{linODE} renders such methods inefficient in that setting. Indeed, since $\bfS\in\mathbb{R}^{n_x\times n_p}$, the total dimension of the ODE system for the sensitivity is in general much higher than the dimension $n_x$ of \eqref{eq:ODE}, in particular for large values of $n_x$. To remedy this, we approach the problem of approximating $\bfS(t)$ by exploiting some results from the theory for linear ODEs; some are versions of well-known results and we include them here to make the paper self-contained.

%\subsection{Exact solution}
In order to derive solutions of \eqref{linODE}, we start by considering the special case of a constant input function $\bfu (t) \equiv \bfu$, for some $\bfu$, and a solution $\bfx (t)$ that is in an equilibrium point, $\bfx (t) \equiv \bfx _{eq}$. This leads to a constant matrix of coefficients $\bfA(t)\equiv \bfA$ and constant forcing term $\bfB(t)\equiv \bfB$, 
thus the system \eqref{linODE} takes the form
\begin{equation}
\label{constantCoefficients}
    \begin{cases}
    \dot{\bfS}(t) = \bfA\cdot \bfS(t) + \bfB\,,\\
    \bfS(t_{k})=\bfS_{k}\,.
    \end{cases}
\end{equation}
The solution of this system is obtained using well-known results from ODE theory; we include a proof for completeness.
\begin{theorem}
\label{thm:constantMat}
    Let $\bfA\in\mathbb{R}^{n_x\times n_x}$, $\bfB\in\mathbb{R}^{n_x\times n_p}$ and $\bfS_k\in\mathbb{R}^{n_x\times n_p}$, and suppose that $\bfA$ is invertible. Under these assumptions, the solution $\bfS\in C^1(I;\mathbb{R}^{n_x\times n_p})$ at a time $t\in\bR$ of the first order linear ODE system \eqref{constantCoefficients} is given by 
    \begin{equation}
    \label{ssSolution}
    \bfS(t) = e^{(t-t_k)\cdot \bfA}\cdot \Biggl(\bfS_{k} +\biggl(\bfI-e^{-(t-t_k)\cdot \bfA}\biggr)\bfA^{-1}\bfB\Biggr).
\end{equation}
\end{theorem}
\begin{proof}
    Given the existence of the inverse $\bfA^{-1}$, we can express \eqref{constantCoefficients} as $\dot{\bfS}(t) = \bfA\cdot (\bfS(t) + \bfA^{-1}\bfB)$. Define the matrix-valued function $\bfR(t)\in\mathbb{R}^{n_x\times n_p}$ as 
    \begin{equation}
    \label{defR}
        \bfR(t):=\bfS(t) + \bfA^{-1}\bfB\,.
    \end{equation}  
    The initial condition for $\bfS(t)$ in the ODE problem \eqref{constantCoefficients} is $\bfS(t_k)=\bfS_k$, which leads to a similar initial condition for $\bfR(t)$, 
    \begin{equation}
    \label{defRk}
    \bfR(t_k) = \bfS_k+\bfA^{-1}\bfB=:\bfR_k\,.
    \end{equation}
    Because $\bfA^{-1}$ and $\bfB$ are constant matrices, the time derivative $\dot{\bfR}$ is given by
    \[
    \dot{\bfR}(t)=\dot{\bfS}(t)=\bfA\cdot (\bfS(t) + \bfA^{-1}\bfB)=\bfA\cdot \bfR(t).
    \] 
    It follows that $\bfR(t)$ satisfies the differential equation
    \begin{equation*}
        \begin{cases}
        \dot{\bfR}(t)=\bfA\cdot \bfR(t),\\
        \bfR(t_k)=\bfR_k\,.
        \end{cases}
    \end{equation*}
    The solution at time $t$ of this equation can be expressed as
    \begin{equation}
    \label{solR}
        \bfR(t) = e^{(t-t_k)\cdot \bfA}\cdot \bfR_k.
    \end{equation}
     From the time derivative of the matrix exponential, we obtain the time derivative of $\bfR$,
    \[
    \dot{\bfR}(t)=\frac{d }{dt}\left(e^{(t-t_k)\cdot \bfA}\cdot \bfR_k\right)=\bfA\cdot e^{(t-t_k)\cdot \bfA}\cdot \bfR_k = \bfA\cdot \bfR(t).
    \]
    Recalling the definitions \eqref{defR} and \eqref{defRk} of $\bfR$ and $\bfR_k$, respectively, the solution in \eqref{solR} can be reformulated in terms of $\bfS$: 
    \[
    \bfS(t) + \bfA^{-1}\bfB = e^{(t-t_k)\cdot \bfA}\cdot (\bfS_k+\bfA^{-1}\bfB),
    \]
    that leads directly to \eqref{ssSolution}.
\end{proof}

From a control-theoretic perspective, the matrix exponential $e^{(t-t_k)\cdot \bfA}$ corresponds to the so-called \textit{state-transition matrix} $\Phi(t;t_k)$ in the specific case of a constant matrix of coefficients $\bfA$. In general, we consider the first order homogeneous linear ODE system
\begin{equation}
    \label{homo}
\dot{\bfS}(t)=\bfA(t)\cdot \bfS(t),
\end{equation}
with a time dependent matrix of coefficients $\bfA(t)$, assumed to be continuous on the closed time interval $I$. In this more general setting, we define the state-transition matrix $\Phi(t;t_k)$ associated with $\bfA(t)$ as the matrix-valued function that, given any initial condition $\bfS(t_k)=\bfS_k$, allows us to represent the solution of \eqref{homo} at time $t\in I$ as
\[
\bfS(t) = \Phi(t;t_k)\cdot \bfS_k.
\]
The state-transition matrix $\Phi(t;s)$ is a well-studied object; see \cite{RughWilsonJ1993Lst},  \cite[Chap.~1, Sect.~3]{BrockettRogerW.2015Fdls} for a detailed description and properties. In particular, in this paper we make repeated use of the the following proposition, which is a combination of well-known results (see e.g.\ \cite{BrockettRogerW.2015Fdls, RughWilsonJ1993Lst}); we omit the proof.
\begin{proposition}
\label{prop:Phi}
The matrix-valued function $\Phi:I\times I\to\bR^{n_x\times n_x}$ satisfies the following properties: 
\begin{enumerate}
    \item\label{itm:first} $\Phi(s;s) = \bfI,$
    \item\label{itm:second} $\Phi(t;s)^{-1} = \Phi(s;t),$
    \item\label{itm:third} for a fixed $s\in I$, the matrix function $\Phi(\cdot;s):I\to\bR^{n_x\times n_x}$ satisfies the IVP 
    \begin{equation*}
        \begin{cases}
        \frac{d}{dt}\Phi(t;s)=\bfA(t)\cdot\Phi(t;s),\\
        \Phi(s;s)=\bfI.
        \end{cases}
    \end{equation*}
\end{enumerate}
\end{proposition}

%\begin{remark}
In the case of a constant matrix of coefficients $\bfA$, the exponential matrix $e^{(t-s)\cdot \bfA}$ is the state-transition matrix associated with $\bfS$ \cite[Chap.~1, Sect.~5]{BrockettRogerW.2015Fdls}, and as such it satisfies properties \ref{itm:first}-\ref{itm:third}. 
For time-dependent coefficient and forcing-term matrices $\bfA (t)$ and $\bfB (t)$, we can formulate the solution for the first order linear ODE system \eqref{linODE} in terms of the matrix-valued function $\Phi(\cdot;t_k): I \to \bR ^{n_x \times n_p}$ associated to $\bfA(t)$.
\begin{theorem}
\label{thm:generalODESol}
     Let $\bfA(\cdot)\in C^0(I;\mathbb{R}^{n_x\times n_x})$, $\bfB(\cdot)\in C^0(I;\mathbb{R}^{n_x\times n_p})$ and $\Phi(\cdot;\cdot)\in C^1(I\times I;\bR^{n_x\times n_x})$ the state-transition matrix associated to $\bfA(t)$. Let $t_k\in I$ and $\bfS(t_k)=\bfS_k\in\mathbb{R}^{n_x\times n_p}$. Then, the solution at time $t\in I$ of the first order inhomogeneous linear ODE system \eqref{linODE} is given by
    \begin{equation}
    \label{solutionODE}
     \bfS(t) = \Phi(t;t_{k})\cdot\Biggl(\bfS_{k} + \int_{t_{k}}^{t}\Phi(t_k;\tau)\bfB(\tau)d\tau\Biggr).
\end{equation}
\end{theorem}

\begin{proof}
We use a strategy analogous to the proof of Theorem \ref{thm:constantMat} and define the matrix-valued function $\bfR:I \to \bR ^{n_x \times n_p}$ by
    \begin{equation}
        \label{def_R}
        \bfR(t):=\Phi(t;t_k)^{-1}\cdot \bfS(t).
    \end{equation}
    It immediately follows that, for $t \in I$,
    \begin{equation}
    \label{s_phi_r}
        \bfS(t)=\Phi(t;t_k)\cdot \bfR(t).
    \end{equation}
By differentiating in time and using Property \ref{itm:third} of Proposition \ref{prop:Phi}, we obtain
    \[
    \dot{\bfS}(t)=\bfA(t)\cdot\Phi(t;t_k)\cdot \bfR(t)+\Phi(t;t_k)\cdot \dot{\bfR}(t)\,.
    \]
  Combining \eqref{s_phi_r} and the ODE \eqref{linODE} for $\bfS (t)$, it holds that 
    \[
    \bfA(t)\cdot\Phi(t;t_k)\cdot \bfR(t)+\Phi(t;t_k)\cdot \dot{\bfR}(t)=\bfA(t)\cdot \Phi(t;t_k)\cdot \bfR(t)+\bfB(t)\,.
    \]
    This implies that 
    \[\Phi(t;t_k)\cdot \dot{\bfR}(t)=\bfB(t)\,,
    \]
    from which it immediately follows that
    \begin{equation}
      \begin{split}
        \dot{\bfR}(t)&=\Phi(t;t_k)^{-1}\cdot \bfB(t) \\
        &= \Phi(t_k;t)\cdot \bfB(t)\,,
      \end{split}\label{ODE_R}
    \end{equation}
    where we use Property \ref{itm:second} of Proposition \ref{prop:Phi}.
    Using that $\Phi(t_k;t_k) = \bfI$ (Property \ref{itm:first} of Proposition \ref{prop:Phi}), we obtain the initial condition for $R(t)$, 
    \[
    \bfR_k:=\bfR(t_k)=\bfI\cdot \bfS(t_k)=\bfS_k\,.
    \]
    To obtain an expression for $\bfR (t)$, we integrate \eqref{ODE_R} from time $t_k$ to $t$: 
    \[
    \bfR(t) = \bfS_k + \int_{t_k}^t\Phi(t_k;\tau)\cdot \bfB(\tau)d\tau\,.
    \]
    The claimed representation \eqref{solutionODE} now follows from inserting this expression for $\bfR (t)$ in \eqref{s_phi_r}. 
\end{proof}

Theorem \ref{thm:generalODESol} shows that, if the state-transition matrix $\Phi(t;t_k)$ can be computed (for all relevant values $t_k, t$), then the solution for the general ODE system \eqref{linODE} can be obtained, which in turn solves the problem \eqref{sensitivityODE} for the sensitivity matrix. An explicit expression for $\Phi(t;s)$ is given by a result from control theory: as proved in \cite{PeanoBaker}, the state-transition matrix $\Phi(t;s)$ associated to the continuous coefficient matrix $\bfA(t)$ can be expressed in terms of the \textit{Peano-Baker series}:
\begin{equation}
\label{PBS}
    \Phi(t;s) = \sum_{n=0}^{\infty}\mathcal{I}_n(t;s),
\end{equation}
where the summands are defined recursively as
\begin{align}
    & \mathcal{I}_0(t;s)=\bfI, \nonumber\\
    & \mathcal{I}_{n+1}(t;s) = \int_{s}^t\bfA(\tau)\mathcal{I}_n(\tau;s)d\tau. \label{recursiveTerm}
\end{align}

In \cite{PeanoBaker} it is also proved that, assuming $\|\bfA(\cdot)\|$ locally integrable on the closed time interval $I$ containing $s$ and $t$, the series \eqref{PBS} converges compactly on $I$.

\begin{remark}
\label{remarkI_id}
We note that if $t=s$, then the integration interval in \eqref{recursiveTerm} has Lebesgue measure zero, from which it follows that $\mathcal{I}_{n}(s;s)=0$ for $n \ge 1$ and $\Phi(s;s)=\mathcal{I}_{0}(s;s)=\bfI$.
\end{remark}

\begin{remark}
As previously mentioned, for a constant matrix of coefficients $\bfA$ the state-transition matrix $\Phi(t;s)$ is given by $e^{(t-s)\cdot \bfA}$. In this case, it is possible to move the constant matrix $\bfA$ outside the integral in \eqref{recursiveTerm}. This leads to the recursive definition
\[
\mathcal{I}_n(t;s)=\frac{(t-s)^n}{n!}\cdot \bfA^{n},
\]
and thus
\[
\Phi(t;s)=\sum_{n=0}^{\infty}\frac{(t-s)^n}{n!}\cdot \bfA^{n} = e^{(t-s)\cdot \bfA} .
\]
\end{remark}

Consider the special case where the solution $\bfx$ of the underlying ODE \eqref{eq:ODE} has reached an equilibrium point $\bfx _{eq}$. At equilibrium we have a constant input function $\bfu(t)\equiv u$, and 
\[
    f(\bfx_{eq},\bfu,\bfp)=0.
\]
In this case, the gradients $\nabla_xf(\bfx(t),\bfu(t),\bfp)$ and $\nabla_p f(\bfx(t),\bfu(t),\bfp)$ have constant arguments $(\bfx_{eq},\bfu,\bfp)$, and thus become constant matrices. To simplify notation, we will omit the arguments and denote these matrices as $\nabla_xf=\nabla_xf(\bfx_{eq},\bfu,\bfp)$ and $\nabla_pf=\nabla_pf(\bfx_{eq},\bfu,\bfp)$.

Under this assumption on $\bfx$, the ODE system \eqref{sensitivityODE} for $\bfS$ is a first order linear ODE system with constant matrix of coefficients and constant forcing term. The solution is therefore provided by Theorem \ref{thm:constantMat}, with $\bfA = \grad _x f$ and $\bfB = \grad _p f$.
\begin{corollary}
\label{cor:Sequi}
Suppose that the solution $\{ \bfx (t) \} _{t \in I}$ of \eqref{eq:ODE} at time step $t_k$ has reached an equilibrium point $\bfx _{eq}$. Assume that $\nabla_xf(\bfx_{eq},\bfu,\bfp)$ is invertible. The solution of \eqref{sensitivityODE}, at time step $t_{k+1}$ is then given by
\begin{equation}
    \label{solutionAtEquilibrium}
    \bfS(t_{k+1})=e^{\Delta t _k \cdot \nabla_xf}\cdot \biggl(\bfS_k+(\bfI-e^{-\Delta t _k\cdot \nabla_xf})\cdot\left(\nabla_xf\right)^{-1}\cdot\nabla_pf\biggr).
\end{equation}
where we denote $\Delta t _k = t_{k+1} - t_k$.
\end{corollary}
This exact solution can be used as an approximation for $\bfS(t_{k+1})$ also when $\bfx (t)$ is not at an equilibrium point, with the approximation improving as $\bfx (t)$ moves closer to an equilibrium point. Understanding the performance of this approximation is left for future work; in our implementations we will use \eqref{solutionAtEquilibrium} as approximation when some criteria (explained in Section \ref{sec:numerical}) are fulfilled  to speed up computations further.

\section{Numerical approximations of $\bfS(t)$ based on the Peano-Baker series}
\label{sec:genSolution}
For the problem of finding the sensitivities $\bfS$ for the solution of the parametrised IVP \eqref{eq:ODE}, Theorem \ref{thm:generalODESol} gives a general representation of $\bfS$.  
Equipped with this representation, and Corollary \ref{cor:Sequi} for the special case when $\bfx$ has reached an equilibrium point, we are now ready to construct a numerical method for approximating the sensitivity matrix $\bfS$. 
 
In general we can not use the assumption of the solution of \eqref{eq:ODE} being at equilibrium, as in Section \ref{sec:method}. That is, in general the matrix of coefficients $\grad _x f (\bfx(t), \bfu(t) , \bfp)$ and the forcing term $\grad _p f (\bfx(t), \bfu(t) , \bfp)$ are not constant matrices. In fact, in the transient of the dynamical system \eqref{eq:ODE}, there could be drastic variations of these objects. Furthermore, not all systems converge to an equilibrium point (where we can eventually use \eqref{solutionAtEquilibrium} to compute $\bfS(t_{k+1})$). For example, it is not unusual in system biology to have the solution $\bfx(t)$ of \eqref{eq:ODE} that converges to a limit cycle. In such cases, the matrices $\grad _x f (\bfx(t), \bfu(t) , \bfp)$ and $\grad _p f (\bfx(t), \bfu(t) , \bfp)$ are in general never constant. In principle, the trajectory $\bfx(t)$ could also show a chaotic behaviour, although this is rare in biological systems. 

As previously mentioned, the $n_x$-dimensional ODE problem \eqref{eq:ODE} can be solved rather efficiently with standard off-the-shelf ODE solvers. 
We therefore assume to have available approximations $\hat{\bfx} _k$ of $ \bfx(t_k)$ at time steps $t_k,\;k=1,\dots,K$. 
For approximating the sensitivity matrix $\bfS$, we want to take advantage of the available $\hat{\bfx}_k$'s and avoid to exploit again an ODE solver for approximating $\bfx(t)$ at intermediate times $t \notin \{t_0, \dots, t_K\}$; we will only admit a linear interpolation of the numerical solution, i.e. if $t_{k'}\in(t_k;t_{k+1})$ we approximate $\bfx(t_{k'})\approx \hat{\bfx}_{k'}=(t_{k'}-t_k)\frac{\hat{\bfx}_{k+1}-\hat{\bfx}_k}{\Delta t_k}$. In addition to a solution $\bfx$ of \eqref{eq:ODE}, we assume to have access to the exact expression for the gradients $\nabla_x f(\bfx(t),\bfu(t),\bfp)$ and $\nabla_p f(\bfx(t),\bfu(t),\bfp)$.

For approximating $\bfS$, we propose a method that is based on approximating the state-transition matrix $\Phi(t;s)$ starting from the Peano-Baker series \eqref{PBS}. More specifically, the method can be divided into approximating the integrals in \eqref{solutionODE} and \eqref{recursiveTerm}, for which we can apply numerical integration, and approximating the series \eqref{PBS} itself, for which we use a truncation that is in a sense optimal (explained in more detail below and in Section \ref{sec:errEst}).

For approximating the integrals, because we know approximations of the values $\bfx_k$ for $k=1, \dots, K$, the endpoints of the integration intervals, we apply the trapezoidal rule.
In each of the recursive integrals $\mathcal{I}_{n}$ ($n\ge1$) in \eqref{recursiveTerm}, the integrands are of the form
\begin{equation}
\label{integrand}
    \nabla_x f(\bfx(\tau),\bfu(\tau),\bfp)\cdot\mathcal{I}_{n-1}(\tau;s),
\end{equation}
with $s=t_k$ when we want to compute $\bfS(t_{k+1})$ given $\bfS(t_k)$ (based on \eqref{solutionODE}). The trapezoidal rule requires us to evaluate \eqref{integrand} for $\tau = t_k$ and $\tau=t_{k+1}$. To this end, consider the approximated solutions $\hat \bfx _k$ and $\hat \bfx_{k+1}$ at time steps $t_k$ and $t_{k+1}$, respectively, given by the ODE solver. Inserting these into the gradients results in the approximations 
\begin{align}
\label{Jac_x}
    \nabla_x\hat{f}_k &:=\nabla_xf(\hat \bfx _k,\bfu(t_k),\bfp) \approx \nabla_xf(\bfx(t_k),\bfu(t_k),\bfp), \\
    \nabla_x\hat{f}_{k+1} &:=\nabla_xf(\hat \bfx _{k+1},\bfu(t_{k+1}),\bfp)\approx \nabla_xf(\bfx(t_{k+1}),\bfu(t_{k+1}),\bfp).\nonumber
\end{align}

Moreover, we recall that $\mathcal{I}_{0}(t_k;t_k)=\bfI$ (from Property \ref{itm:first} of Proposition \ref{prop:Phi}) and $\mathcal{I}_{n\ge1}(t_k;t_k)=0$ (from Remark \ref{remarkI_id}). Combining these with the trapezoidal rule gives the approximations,
\begin{align}
   \hat{\mathcal{I}}_{0,k} &:= I,\nonumber\\
    \hat{\mathcal{I}}_{1,k} &:= \frac{\Delta t_k}{2}\cdot\bigl(  \nabla_x\hat{f}_k + \nabla_x\hat{f}_{k+1}\bigr),\label{approxIntegrals}\\
    \hat{\mathcal{I}}_{n+1,k} &:= \frac{\Delta t_k}{2}\cdot\bigl( 0 + \nabla_x\hat{f}_{k+1}\cdot \hat{\mathcal{I}}_{n,k} \bigr),\quad n\ge2,\nonumber
\end{align}
respectively for $\calI_0(t_{k+1};t_k)$, $\calI_1(t_{k+1};t_k)$ and $\calI_{n+1}(t_{k+1};t_k), n\ge2$.

A study of the error introduced by the various approximations, presented in detail in Section \ref{sec:errEst}, shows that when the trapezoidal rule is used for the integrals in the Peano-Baker series, it is reasonable to truncate the series at $n=2$. That is, for $k=0,1,\dots,K-1$, we approximate $\Phi(t_{k+1};t_k)$ by
\begin{equation}
    \label{PhiApproximation}
\hat{\Phi}(t_{k+1};t_k) := \hat{\mathcal{I}}_{0,k} + \hat{\mathcal{I}}_{1,k} + \hat{\mathcal{I}}_{2,k}.
\end{equation}
In order to treat the integral in \eqref{solutionODE}, where the integrand is 
\[
\Phi(t_k;\tau)\cdot\nabla_pf(\bfx(\tau),\bfu(\tau),\bfp),
\]
a similar strategy can be used. Similar to \eqref{Jac_x}, we define the approximations of the gradients with respect to $\bfp$ as
\begin{gather}
\label{Jac_p}
    \nabla_p\hat{f}_k:=\nabla_pf(\hat \bfx _k,\bfu(t_k),\bfp)\approx \nabla_pf(\bfx(t_k),\bfu(t_k),\bfp),\\
    \nabla_p\hat{f}_{k+1}:=\nabla_pf(\hat \bfx _{k+1},\bfu(t_{k+1}),\bfp)\approx \nabla_pf(\bfx(t_{k+1}),\bfu(t_{k+1}),\bfp).\nonumber
\end{gather}

By the same reasoning as for \eqref{approxIntegrals}, we can truncate the Peano-Baker series at $n=2$ and approximate the terms of the truncated series by
\begin{align*}
    &\mathcal{I}_0(t_{k};t_{k+1}) = I,\\
    &\mathcal{I}_{1}(t_{k};t_{k+1}) \approx \frac{-\Delta t_k}{2}\bigl(  \nabla_x\hat{f}_k + \nabla_x\hat{f}_{k+1}\bigr)=-\hat{\mathcal{I}}_{1,k},\\
    &\mathcal{I}_{2}(t_{k};t_{k+1})\approx \frac{-\Delta t_k}{2}\bigl( 0 + \nabla_x\hat{f}_{k+1}\cdot (-\hat{\mathcal{I}}_{1,k}) \bigr)=\hat{\mathcal{I}}_{2,k}.
\end{align*}
The resulting approximation $\hat \Phi (t_k; t_{k+1}) $ of $\Phi (t_k; t_{k+1})$ is then defined as
\[\hat{\Phi}(t_k;t_{k+1}):=I - \hat{\mathcal{I}}_{1,k} + \hat{\mathcal{I}}_{2,k}.\]
Note that the minus sign in front of the second term is due to the integration interval being reversed compared to \eqref{PhiApproximation}.

Combining the approximation of $\Phi (t_k; t_{k+1})$ with the trapezoidal rule, and the property $\Phi(t_k;t_k)=\bfI$ (Property \ref{itm:first} of Proposition \ref{prop:Phi}), the integral in \eqref{solutionODE} can be approximated as 
\[
\int_{t_{k}}^{t_{k+1}}\Phi(t_{k};\tau)\nabla_pf(\bfx(\tau),\bfu(\tau),\bfp)d\tau \approx \frac{\Delta t_k}{2}\cdot\bigl(\nabla_p\hat{f}_k + \hat{\Phi}(t_k;t_{k+1})\cdot \nabla_p\hat{f}_{k+1}\bigr).
\]
Next, we combine this approximation with $\hat \Phi(t_{k+1};t_k)$ to obtain an approximation of the sensitivity matrix $\bfS$ at time $t_{k+1}$, given $\bfS(t_k)$:
\begin{align}
\label{eq:Approx}
    \bfS(t_{k+1})\approx  \hat{\Phi}(t_{k+1};t_k)\cdot\biggl(\bfS(t_k)+\frac{\Delta t_k}{2}\cdot\bigl(\nabla_p\hat{f}_k + \hat{\Phi}(t_k;t_{k+1})\cdot \nabla_p\hat{f}_{k+1}\bigr)\biggr).
\end{align}
The right-hand side of \eqref{eq:Approx} is the approximation, based on the Peano-Baker series, that we use for the solution $\bfS$ of the ODE problem \eqref{sensitivityODE}. The corresponding algorithm, henceforth referred to as the Peano-Baker Series (PBS) algorithm, is described in Algorithm~\ref{alg:approx}

\begin{algorithm}[H]
 \caption{Peano-Baker series algorithm for the sensitivity matrix $\bfS$ \label{alg:approx}}

\SetAlgoLined
\KwResult{$\hat{\bfS}_1,\dots,\hat{\bfS}_K$}
$\hat{\bfx}_k\approx \bfx(t_k),\quad k=1,\dots,K$\;

$\nabla_x\hat{f}_k = \nabla_xf(\hat{\bfx}_k,\bfu(t_k),\bfp)$\;

$\nabla_p\hat{f}_k = \nabla_pf(\hat{\bfx}_k,\bfu(t_k),\bfp)$\;

$\hat{\bfS}_0=0\in\mathbb{R}^{n_x\times n_p}$\;

 \For{$k\leftarrow 0$ \KwTo $K-1$}{
 $\Delta t_k = t_{k+1}-t_k$\;
 
  \eIf{$\nabla_xf\approx $ const \normalfont{\textbf{and}} $\nabla_pf\approx $ const }{
   $\hat{\bfS}_{k+1}=e^{\Delta t_k\cdot\nabla_x\hat{f}_k}\cdot\bigl(\hat{\bfS}
   _k+\bigl(I-e^{-\Delta t_k\cdot\nabla_x\hat{f}_k}\bigr)\cdot\nabla_x\hat{f}_k^{-1}\cdot\nabla_p\hat{f}_k\bigr)$
   }{
   $\hat{\mathcal{I}}_{1,k} = \frac{\Delta t_k}{2}\cdot\bigl(  \nabla_x\hat{f}_k + \nabla_x\hat{f}_{k+1}\bigr)$\;
 
 $\hat{\mathcal{I}}_{2,k} = \frac{\Delta t_k^2}{4}\cdot\bigl(\nabla_x\hat{f}_{k+1}\cdot(  \nabla_x\hat{f}_k + \nabla_x\hat{f}_{k+1}\bigr)\bigr)$\;
 
  $\hat{\Phi}(t_{k+1};t_k)=I+ \hat{\mathcal{I}}_{1,k}+\hat{\mathcal{I}}_{2,k}$\;
  
   $\hat{\Phi}(t_{k};t_{k+1})=I- \hat{\mathcal{I}}_{1,k}+\hat{\mathcal{I}}_{2,k}$\;
  
  $\hat{\bfS}_{k+1} = \hat{\Phi}(t_{k+1};t_k)\cdot\bigl(\hat{\bfS}_k+\frac{\Delta t_k}{2}\cdot\bigl(\nabla_p\hat{f}_k + \hat{\Phi}(t_k;t_{k+1})\cdot \nabla_p\hat{f}_{k+1}\bigr)\bigr)$\;
  }
 }

\end{algorithm}
In Section \ref{sec:numerical} we discuss some criteria for the if-statement in Algorithm~\ref{alg:approx}, i.e. when it is proper to use Corollary \ref{cor:Sequi} (exact in the case of $\bfx$ at equilibrium) to approximate $\bfS(t_{k+1})$.

\section{Error analysis of approximation of $\bfS$}
\label{sec:errEst}

In this section we carry out an error analysis for the approximation \eqref{eq:Approx} of the solution $\bfS$ of \eqref{sensitivityODE}. The errors involved are represented as vectors or matrices that correspond to the difference between an exact term and its approximation. In order to provide error estimates, and conclude on the order of convergence, we will consider the norm $\|\cdot\|$ of the errors; since we are working in finite-dimensional vector spaces, all norms are equivalent and the exact choice is irrelevant to the analysis. Recall also from Section \ref{sec:problem} that we have assumed enough regularity of the problem so that  existence and uniqueness of a solution of \eqref{eq:ODE} is guaranteed.



With a slight abuse of notation, we will abbreviate the arguments of the vector field $f$ and its derivatives (Jacobians, second and third order derivatives) from $(\bfx(t),\bfu(t),\bfp)$ to $(\bfx(t))$ only. For example, we denote $\nabla_xf(\bfx(t),\bfu(t),\bfp)$ by $\nabla_xf(\bfx(t))$). We recall the notation $\nabla_x\hat{f}_k$ and $\nabla_p\hat{f}_k$, introduced in \eqref{Jac_x} and \eqref{Jac_p}, for the approximated versions of the Jacobians with respect to $\bfx$ and $\bfp$, respectively. To further simplify the notation, we define $\calJ _k$ as
\[
    \calJ_k:=\int_{t_k}^{t_{k+1}}\Phi(t_k;\tau)\cdot \nabla_pf(\bfx(\tau))d\tau,
\]
and the corresponding approximation
\[
\hat{\calJ}_k:=\frac{\Delta t_k}{2}\cdot(\nabla_p\hat{f}_k+\hat{\Phi}(t_{k};t_{k+1})\cdot\nabla_p\hat{f}_{k+1}).
\]

Having established the notation, we can use Theorem \ref{thm:generalODESol} to formulate the exact solution of the ODE system \eqref{sensitivityODE} for the sensitivity matrix at time step $t_{k+1}$ as
\begin{equation}
    \label{exactSensitivity}
    \bfS(t_{k+1}) = \Phi(t_{k+1};t_k)\cdot\left(\bfS(t_k)+\calJ_k\right),
\end{equation}
while the approximate solution is defined as 
\begin{equation}
\label{approximateSensitivity}
\hat{\bfS}_{k+1} = \hat{\Phi}(t_{k+1};t_k)\cdot\left(\hat{\bfS}_k+\hat{\calJ}_k\right).
\end{equation}
Figure \ref{fig:errorPropagation} illustrates the terms and the corresponding errors in the approximation \eqref{approximateSensitivity}. Each node in the graph represents a new source of error and the directed edges show the propagation of error from one node (an approximation) to another, as a result of the approximation being used instead of the exact quantity.





\begin{figure}
\begin{tikzpicture}[node distance=1cm, auto,]
 %nodes
 \node[nodesty,align=center] (errODE) {$\bfx(t_k) = \hat{\bfx}_k + \err_k^{ODE}$\\$\bfx(t_{k+1}) = \hat{\bfx}_{k+1} + \err_{k+1}^{ODE}$};
 \node[nodesty, below left=1cm and -1.5 cm of errODE,align=center] (errJacx) {$\nabla_xf(\bfx(t_k))=\nabla_x\hat{f}_k+\err_k^{\nabla_x}$\\
 $\nabla_xf(\bfx(t_{k+1}))=\nabla_x\hat{f}_{k+1}+\err_{k+1}^{\nabla_x}$};
  \node[nodesty, below right=1cm and -2 cm of errODE,align=center] (errJacp) {$\nabla_pf(\bfx(t_{k}))=\nabla_p\hat{f}_k+\err_k^{\nabla_p}$\\
  $\nabla_pf(\bfx(t_{k+1}))=\nabla_p\hat{f}_{k+1}+\err_{k+1}^{\nabla_p}$};
 \node[nodesty, below left=1cm and -3 cm of errJacx] (I1) {$\mathcal{I}_{1}(t_{k+1};t_k)=\hat{\mathcal{I}}_{1,k}+\err_k^{\mathcal{I}_{1}}$};
 \node[nodesty, below right=.5cm and -.5 cm of I1] (I2) {$\mathcal{I}_{2}(t_{k+1};t_k)=\hat{\mathcal{I}}_{2,k}+\err_k^{\mathcal{I}_{2}}$};
 \node[nodesty, below left=1cm and -3.5 cm of I2,align=center] (Phi) {$\Phi(t_{k+1};t_k)=\hat{\Phi}(t_{k+1};t_k)+\err_k^{\Phi}$\\$\Phi(t_{k};t_{k+1})=\hat{\Phi}(t_{k};t_{k+1})+\err_k^{\Phi,inv}$};
 \node[nodesty, below =4cm of errJacp] (J) {$\calJ_k=\hat{\calJ}_k+\err_k^\calJ$};
 \node[nodesty, below=10cm of errODE] (S) {$\bfS(t_{k+1})=\hat{\bfS}_{k+1}+\err_{k+1}^{\bfS}$};
 \path[arrsty, bend right=10] (errODE) edge (errJacx);
 \path[arrsty, bend left=10] (errODE) edge (errJacp);
 \path[arrsty, bend right=10] (errJacx) edge (I1);
 \path[arrsty, bend left=10] (errJacx) edge (I2);
 \path[arrsty, bend left=10] (I1) edge (I2);
 \path[arrsty, bend right=10] (I1) edge (Phi);
 \path[arrsty, bend left=10] (I2) edge (Phi);
 \path[arrsty, bend right=10] (Phi) edge (S);
 \path[arrsty, bend right=10] (Phi) edge (J);
 \path[arrsty, bend left=10] (errJacp) edge (J);
 \path[arrsty, bend left=10] (J) edge (S);
 \path[arrsty] (S) edge[loop below] ();
\end{tikzpicture}
\caption{Error propagation graph}
\label{fig:errorPropagation}
\end{figure}

The main result of this section, Theorem \ref{thm:error}, is a characterisation of the error in the approximation of $\bfS$, at time step $k+1$, in terms of the largest time step $\Delta t _{max}$ used by the underlying ODE solver. Before we can state this result, we list the assumptions we make on the functions involved. The first assumption concerns the numerical solution $\hat{\bfx}_k$, at each time step $t_k$, of the ODE \eqref{eq:ODE}:
\[
    \err^{ODE}_{k} = \bfx(t_k)-\hat{\bfx}_k,
\]
 which is defined as an $n_x$ dimensional error vector. We make the following assumption on the local error of the underlying numerical method. 
\begin{assumption}
\label{as:errorODE}
The approximation $\hat \bfx _k$ of $\bfx (t_k)$, the solution of \eqref{eq:ODE} at time $t_k$, is of at least fourth order, i.e.\ $\|\err^{ODE}_{k}\|=o(\Delta t_k^3)$. 
\end{assumption}
Next, for partial derivatives of $f$ with respect to $\bfx$ and $\bfp$ we use the notation,
\begin{align*}
    \left(H_{xx}\right)^{jh}_l &:=\frac{\partial^2 f_l}{\partial x_h\partial x_j}, \\
    \left(H_{xp}\right)^{ih}_l &:=\frac{\partial^2 f_l}{\partial x_h\partial p_i}.
\end{align*}
\begin{assumption}
\label{as:boundedH}
    The matrices $H_{xx}(\bfx (t))$ and $H_{xp} (\bfx (t))$ of second partial derivatives with respect to $\bfx$ and $\bfp$ are both bounded with respect to the supremum norm $\| \cdot \|_{[0,T]}$.
\end{assumption}
The third assumption concerns boundedness of certain maps with respect to $\| \cdot \| _{[0,T]}$.
\begin{assumption}
\label{as:time}
The following maps are bounded with respect to $\| \cdot \| _{[0,T]}$:
\begin{enumerate}[label=(\ref{as:time}.\arabic*)]
    \item\label{as:time1} $t \mapsto \grad _x f(\bfx (t))$,
    \item\label{as:time2} $t\mapsto\frac{d^2}{dt^2}\nabla_xf(\bfx(t))$,
    \item\label{as:time3} $t\mapsto\frac{d^2}{dt^2}\left(\nabla_xf(\bfx(t))\calI_1(t;t_k)\right)$,
    \item\label{as:time4} $t\mapsto\frac{d^2}{dt^2}\left(\Phi(t_k;t)\cdot\nabla_pf(\bfx(t))\right)$.
\end{enumerate}
\end{assumption}
Assumption \ref{as:time1} is used in the study of the remainder term of the Peano-Baker series, after truncation at $n=2$. The remaining assumptions \ref{as:time2}–\ref{as:time4} are used for arguments involving the trapezoidal rule, which explains the second derivative with respect to time appearing.

The final assumption concerns the sensitivity matrix $\bfS$ and the integrands in the $\calJ _k$s. 
\begin{assumption}
\label{as:matrices}
There exists constants $C_S$ and $C_{\calJ}$, $C_S, C_\calJ \in (0,\infty)$, such that $C_{_\calJ}:=max_{j=0,\dots,K-1}\|\Phi(t_k;t)\cdot\nabla_pf(\bfx(t))\|_{[0,T]}$ and, for all $t \in [0,T]$, $C_S \geq \| \bfS (t) \|$.
\end{assumption}

The assumptions are stated not with the aim of full generality, but rather to provide enough regularity for the overall results to not get obscured by technical details. It is clear that the assumptions can be made both more explicit and less restrictive if aimed at specific examples. As an example, the assumptions of Lemma \ref{lemma:errI1} are satisfied if, for example, $f\in\calC^3(\bR^{n_x},\bR^{n_p},\bR^{n_u})$ and $u\in\calC^2([0,T])$. Similarly, if there is no time-dependent control in the ODE \eqref{eq:ODE}, more explicit error estimates can be obtained. We leave such refinements of the results for future work considering more specific formulations of the underlying IVP \eqref{eq:ODE}.

We are now ready to state the main theorem of this section.
\begin{theorem}
\label{thm:error}
Suppose Assumptions \ref{as:errorODE}–\ref{as:matrices} hold. Let $\Delta t_{max}:=\max_{h=0,\dots,K-1}\Delta t_h$ and $\Delta t_{min}:=\min_{h=0,\dots,K-1}\Delta t_h$, and assume that there exists a finite constant $\Delta$ such that $\frac{\Delta t_{max}}{\Delta t_{min}}\le \Delta$. Then, the error in the sensitivity matrix $\bfS$ at time step $t_{k+1}$ is
\begin{equation}
    \label{errSOrder}
    \|\err_{k+1}^{\bfS}\|=O(\Delta t_{max}^2).
\end{equation}
\end{theorem}

We proceed to prove Theorem \ref{thm:error} using a series of lemmas, corresponding roughly to the directed edges in Figure \ref{fig:errorPropagation}. The first source of error comes from the numerical solution $\hat{\bfx}_k$, at each time step $t_k$, of the ODE \eqref{eq:ODE}, represented by the uppermost node in Figure \ref{fig:errorPropagation}. Under Assumption \ref{as:errorODE}, the error $\err^{ODE}_{k}$ associated with $\hat \bfx _k$ is negligible compared to those of other approximations present in the proposed method.  

Next, we consider the approximations of the Jacobians, defined in in \eqref{Jac_x} and \eqref{Jac_p}. As shown by the two directed edges going from the uppermost node in Figure \ref{fig:errorPropagation}, the error $\err_k^{ODE}$ propagates to both these terms: the errors that arise in the Jacobians are due to the evaluation of the exact Jacobians $\nabla_x f$ and $\nabla_p f$ in the approximated solution $\hat{\bfx}_k$ instead of the exact counterpart $\bfx(t_k)$: 
\begin{align*}
    \err_k^{\nabla_x} = \nabla_xf(\bfx(t_k))-\nabla_x\hat{f}_k,\\
    \err_k^{\nabla_p} = \nabla_pf(\bfx(t_k))-\nabla_p\hat{f}_k.
\end{align*}
The following result is standard and included for completeness. 
\begin{lemma}
\label{lemma:Jacobians}
    Under Assumption \ref{as:boundedH}, the errors $\err_k ^{\grad _x}$ and $\err _k ^{\grad _p}$ are both of the same order as $\err _k ^{ODE}$.
\end{lemma}
\begin{proof}
The errors $\err_k ^{\grad _x}$ and $\err _k ^{\grad _p}$ are matrices of dimension $n_x \times n_x$ and $n_x \times n_p$, respectively. Let $\left(\err_{k}^{ODE}\right)_h$ denote the $h$th component of the $n_x$ dimensional error vector $\err_k^{ODE}$. The entries of $\err_k ^{\grad _x}$ and $\err _k ^{\grad _p}$ are then given by
\begin{align*}
    \left(\err_k^{\nabla_x}\right)_{l}^{~j} &=\frac{\partial f_{l}(\bfx(t_k))}{\partial x_j} - \frac{\partial f_{l}(\hat{\bfx}_k)}{\partial x_j}\\
    &=\frac{\partial^2f_{l}(\bfx(t_k))}{\partial x_h\partial x_j}\cdot \left(\bfx(t_k)-\hat{\bfx}_{k}\right)_h+O(\|\err_{k}^{ODE}\|^2)\\
    &=\left(H_{xx}(\bfx(t))\right)^{~jh}_l\left(\err_{k}^{ODE}\right)_h+O(\|\err_{k}^{ODE}\|^2),
\end{align*}
and
\begin{align*}
    \left(\err_k^{\nabla_p}\right)_{l}^{~i} &= \frac{\partial f_{l}(\bfx(t_k))}{\partial p_i} - \frac{\partial f_{l}(\hat{\bfx}_k)}{\partial p_i}\\
    &=\frac{\partial^2f_{l}(\bfx(t_k))}{\partial x_h\partial p_i}\cdot \left(\bfx(t_k)-\hat{\bfx}_{k}\right)_h+O(\|\err_{k}^{ODE}\|^2)\\
    &=\left(H_{xp}(\bfx(t))\right)^{~ih}_l\left(\err_{k}^{ODE}\right)_h+O(\|\err_{k}^{ODE}\|^2).
\end{align*}
Given these forms for the entries, we obtain the following bounds,
\begin{align*}
    \|\err_k^{\nabla_x}\| &\le C_1 \|H_{xx}(\bfx(t_k))\|\|\err_k^{ODE}\|+O(\|\err_k^{ODE}\|^2),\\
    \|\err_k^{\nabla_p}\| &\le C_2 \|H_{xp}(\bfx(t_k))\|\|\err_k^{ODE}\|+O(\|\err_k^{ODE}\|^2),
\end{align*}
where the constants $C_1>0$ and $C_2>0$ depend on the specific dimensions $n_x$ and $n_p$ considered and on the choice of norms. 

Under the assumptions on $H_{xx}(\bfx (t))$ and $H_{xp} (\bfx (t))$, there exist finite constants $C_{H_{xx}}:=\|H_{xx}\|_{[0,T]}$ and $C_{H_{xp}}:=\|H_{xp}\|_{[0,T]}$. By the definition of the supremum norm, we obtain uniform (with respect to time) bounds for the errors in the Jacobians:
\begin{align*}
    \|\err_k^{\nabla_x}\| &\le C_1C_{H_{xx}}\|\err_k^{ODE}\|+O(\|\err_k^{ODE}\|^2),\\
    \|\err_k^{\nabla_p}\| &\le  C_2C_{H_{xp}}\|\err_k^{ODE}\|+O(\|\err_k^{ODE}\|^2).
\end{align*}
This shows that the norm of either error is of the same order as the error in the ODE solution. 
\end{proof}
Note that, as the errors in the approximations of the two Jacobians are of the same order as the error in the ODE solution, if the latter is negligible so are the errors in $\nabla _x \hat f_k$ and $\nabla _p \hat f_k$.

Following the error propagation in Figure \ref{fig:errorPropagation}, the approximations of $\nabla_xf(\bfx(t_k))$ as $\nabla_x\hat{f}_k$, and its counterpart at time step $k+1$, are used to approximate the integrals $\mathcal{I}_1(t_{k+1};t_k)$ and $\mathcal{I}_2(t_{k+1};t_k)$, respectively, with $\hat{\mathcal{I}}_{1,k}$ and $\hat{\mathcal{I}}_{2,k}$, as defined in \eqref{approxIntegrals}; 

note that the approximation $\hat{\mathcal{I}}_{1,k}$ is used in $\hat{\mathcal{I}}_{2,k}$ (see Figure \ref{fig:errorPropagation}). The errors that arise are a result of both the application of the trapezoidal rule for numerical integration and the errors in the Jacobians, as described by Lemma \ref{lemma:Jacobians}. 
\begin{lemma}
\label{lemma:errI1}
Suppose Assumptions \ref{as:errorODE}, \ref{as:boundedH} and \ref{as:time2} hold. Then the error $\err_k^{trap,\mathcal{I}_1}$ due to the approximation of $\mathcal{I}_1(t_{k+1};t_k)$ by the trapezoidal rule is
\begin{equation}
    \label{errTrapOrder}
    \|\err_k^{trap,\mathcal{I}_1}\|=O(\Delta t_k^3),
\end{equation}
and the error $\err_k^{\mathcal{I}_{1}}$, from the approximation of $\mathcal{I}_1(t_{k+1};t_k)$ by $\hat{\calI}_{1,k}$, satisfies
\begin{equation}
    \label{errI1Order}
    \|\err_k^{\mathcal{I}_{1}}\|=O(\Delta t_k^3).
\end{equation}
\end{lemma}
\begin{proof}
It can be proved \cite[Sec. 5.2.1]{SauerTim2006Na} that the error of the trapezoidal rule applied to the integral of a function $g\in C^2(A,\bR)$, with $A\subset\bR$, on the interval $[a,b]\subset A$ is
\begin{equation}
    \label{trapezoidalRuleError}
    \int_a^bg(x)dx-\frac{b-a}{2}\cdot(g(a)+g(b))=-\frac{(b-a)^3}{12}\cdot g''(\xi),
\end{equation}
for some $\xi\in(a,b)$. Using the latter, the components of $\err_k^{trap,\mathcal{I}_1}$ are
\begin{equation}
    \label{errTrapIntermediate}
    \left(\err_k^{trap,\mathcal{I}_1}\right)_{l}^{~j}=-\frac{\Delta t_k^3}{12}\cdot\left( \frac{d^2}{dt^2}\nabla_xf(\bfx(t))\right)_l^{~j}\Bigg|_{t=\xi},
\end{equation}
where in general $\xi\in(t_k,t_{k+1})$ is different for each pair of indices $l,j$. From the properties of matrix norms, we have that for every $t\in[t_k,t_{k+1}]$ and $i,j=1,\dots,n_x$,
\[
\left|\left(\frac{d^2}{dt^2}\nabla_xf(\bfx(t))\right)_l^{~j}\right|\le C \left\lVert \frac{d^2}{dt^2}\nabla_xf(\bfx(t))\right\rVert.
\]
for a constant $C>0$ that only depends on the choice of norm $\|\cdot\|$.

Using Assumption \ref{as:time2} for $\frac{d^2}{dt^2}\nabla_xf(\bfx(t))$, there exist a constant $C' <\infty$ such that  $C'=\left\|\frac{d^2}{dt^2}\nabla_xf(\bfx(t))\right\|_{[0,T]}$. It follows that, for every $i,j=1,\dots,n_x$,
\[
\left|\left(\err_k^{trap,\mathcal{I}_1}\right)_{l}^j\right|\le\frac{CC'}{12}\Delta t_k^3.
\]

Using the fact that all matrix norms are equivalent, there exists a constant $C''>0$, which depends on the choice of norm, such that
\[ 
\|\err_k^{trap,\mathcal{I}_1}\|\le C'' \max_{i,j=1,\dots,n_x}\left|\left(\err_k^{trap,\mathcal{I}_1}\right)_{l}^{~j}\right|.
\]
Combined with the previous inequality this yields the following upper bound on the error from the trapezoidal rule,
\[
\|\err_k^{trap,\mathcal{I}_1}\|\le \frac{CC'C''}{12}\Delta t_k^3.
\]
This proves \eqref{errTrapOrder}.

Having established an order of convergence for the error due to the trapezoidal rule, we now expand the error arising from the approximation of $\calI _{1} (t_{k+1};t_k)$:
\begin{align*}
    \err_k^{\mathcal{I}_{1}} &= \mathcal{I}_1(t_{k+1};t_k) - \hat{\calI}_{1,k} \\
    &=  \mathcal{I}_1(t_{k+1};t_k) - \frac{\Delta t_k}{2}\cdot (\nabla_x\hat{f}_k + \nabla_x\hat{f}_{k+1})\nonumber\\
    &=\mathcal{I}_1(t_{k+1};t_k) -\frac{\Delta t_k}{2}\cdot(\nabla_xf(\bfx(t_k)) +\nabla_xf(\bfx(t_{k+1})))\\
    & \quad +\frac{\Delta t_k}{2}\cdot(\nabla_xf(\bfx(t_k)) -\nabla_x\hat{f}_k+\nabla_xf(\bfx(t_{k+1})) - \nabla_x\hat{f}_{k+1})\nonumber\\
    &=\err_k^{trap,\mathcal{I}_1} + \frac{\Delta t_k}{2}\cdot(\err_k^{\nabla_x}+\err_{k+1}^{\nabla_x})\label{errI1}.
\end{align*}
Together with Assumption \ref{as:errorODE} and Lemma \ref{lemma:Jacobians}, this yields the upper bound

\[
    \|\err_k^{\calI_1}\|\le \|\err_k^{trap,\calI_1}\|+o(\Delta t_k^4),
\]
which proves the claimed order of convergence \eqref{errI1Order}.
\end{proof}

With these estimates for the errors in $\nabla_x\hat{f}_k$ and $\hat{\calI}_{1,k}$, we now turn to the approximation of $\calI_{2,k}$. The first part of this lemma, concerning the error due to an application of the trapezoidal rule, is analogous to the first part of Lemma \ref{lemma:errI1}. 
\begin{lemma}
\label{lemma:errI2}
Suppose Assumptions \ref{as:errorODE}, \ref{as:boundedH}, \ref{as:time1}-\ref{as:time3} hold. Then, the error $\err_k^{trap,\mathcal{I}_2}$ due to the approximation of $\mathcal{I}_2(t_{k+1};t_k)$ by the trapezoidal rule is
\begin{equation}
    \label{errTrapOrder2}
    \|\err_k^{trap,\mathcal{I}_2}\|=O(\Delta t_k^3),
\end{equation}
and the error $\err_k^{\mathcal{I}_{2}}$, from the approximation of $\mathcal{I}_2(t_{k+1};t_k)$ by $\hat{\calI}_{2,k}$, is
\begin{equation}
    \label{errI2Order}
    \|\err_k^{\mathcal{I}_{2}}\|=O(\Delta t_k^3).
\end{equation}
\end{lemma}
\begin{proof}
The proof of \eqref{errTrapOrder2} is analogous to the proof of Lemma \ref{lemma:errI1}: by replacing $\err_k^{trap\calI_1}$ with $\err_k^{trap\calI_2}$, and $\frac{d^2}{dt^2}\nabla_xf(\bfx(t))$ with $\frac{d^2}{dt^2}\left(\nabla_xf(\bfx(t))\calI_1(t,t_k)\right)$, and defining
\[
\tilde{C}':=\left\|\frac{d^2}{dt^2}\left(\nabla_xf(\bfx(t))\calI_1(t,t_k)\right)\right\|_{[0,T]},
\] we obtain
\[
\|\err_k^{trap,\calI_2}\|\le \frac{C\tilde{C}'C''}{12}\Delta t_k^3.
\]
This proves the order of convergence \eqref{errTrapOrder2}.

To show \eqref{errI2Order}, we note that this error can be expressed as
\begin{align}
    \err_k^{\calI_2} &= \calI_2(t_{k+1};t_k)-\hat{\calI}_{2,k} \nonumber\\
    &=\calI_2(t_{k+1};t_k) - \frac{\Delta t_k}{2}\cdot\nabla_xf(\bfx(t_{k+1}))\cdot\calI_1(t_{k+1};t_k) \nonumber \\ 
    &\quad + \frac{\Delta t_k}{2}\cdot\left(\nabla_xf(\bfx(t_{k+1}))\cdot\calI_1(t_{k+1};t_k) 
    -\nabla_x\hat{f}_{k+1}\cdot\hat{\calI}_{1,k}\right) \nonumber\\
    &=\err_k^{trap,\calI_2}+ \frac{\Delta t_k}{2}\cdot\left(\nabla_xf(\bfx(t_{k+1}))\cdot\calI_1(t_{k+1};t_k) - \nabla_xf(\bfx(t_{k+1})) \cdot\hat{\calI}_{1,k} \right. \nonumber \\
    & \qquad \qquad \qquad \qquad \qquad  \left. +\nabla_xf(\bfx(t_{k+1})) \cdot\hat{\calI}_{1,k} -\nabla_x\hat{f}_{k+1}\cdot\hat{\calI}_{1,k}\right)\nonumber\\
    &=\err_k^{trap,\calI_2}+\frac{\Delta t_k}{2}\cdot\left(\nabla_xf(\bfx(t_{k+1})) \cdot \err_k^{\calI_1}+\err_{k+1}^{\nabla_x}\cdot\hat{\calI}_{1,k}\right)\label{errI2}.
\end{align}
Applying the norm operator to \eqref{errI2} and using the triangle inequality gives
\begin{equation*}
 %   \label{errtrapI2}
    \|\err_k^{\calI_2}\|\le \|\err_k^{trap,\calI_2}\|+\frac{\Delta t_k}{2}\cdot\left(\|\nabla_xf(\bfx(t_{k+1}))\|\|\err_k^{\calI_1}\|+\|\err_{k+1}^{\nabla_x}\|\|\hat{\calI}_{1,k}\|\right).
\end{equation*}
 From Assumption \ref{as:time1}, $\|\nabla_xf(\bfx(t_{k+1}))\|=O(1)$ and, from the definition of $\hat{\calI}_{1,k}$ (see \eqref{approxIntegrals}), we have $\|\hat{\calI}_{1,k}\|=O(1)$. From the upper bound in the last display, combined with \eqref{errTrapOrder2}, Lemmas \ref{lemma:Jacobians}-\ref{lemma:errI1} and Assumption \ref{as:errorODE}, we obtain 
\[
\|\err_k^{\calI_2}\|\le \|\err_k^{trap,\calI_2}\|+o(\Delta t_k^3),
\]
which proves the order of convergence \eqref{errI2Order}.
\end{proof}

Next, we move to the approximation of $\Phi(t_{k+1};t_k)$. As illustrated in Figure \ref{fig:errorPropagation}, this approximation depends directly on the approximations of $\calI_{1}(t_{k+1};t_k)$ and $\calI_{2}(t_{k+1};t_k)$, and thus Lemmas \ref{lemma:errI1}-\ref{lemma:errI2} will be used to obtain the order of convergence of the associated error.

\begin{lemma}
\label{lemma:errPhiOrder}
Suppose that Assumptions \ref{as:errorODE}, \ref{as:boundedH} and \ref{as:time1}-\ref{as:time3} hold. Then, the error $\err_k^{\Phi}$ in the approximation of $\Phi(t_{k+1};t_k)$ by $\hat{\Phi}(t_{k+1};t_k)$ is
\[
\|\err_k^{\Phi}\|=O(\Delta t_k^3).
\]
\end{lemma}
\begin{proof}
Recalling the definition \eqref{PhiApproximation} of the approximation $\hat{\Phi}(t_{k+1};t_k)$, we can express the associated $\err_k^{\Phi}$ as
\begin{align}
\err^{\Phi}_k &=\Phi(t_{k+1};t_k)-I_{n_x} - \hat{\calI}_{1,k} - \hat{\calI}_{2,k} \nonumber\\
&=\Phi(t_{k+1};t_k) - \sum_{n=0}^{2} \calI_n(t_{k+1};t_k)  \nonumber \\
&\quad + I_{n_x} + \calI_1(t_{k+1};t_k) + \calI_2(t_{k+1};t_k) - I_{n_x} - \hat{\calI}_{1,k} - \hat{\calI}_{2,k} \nonumber\\
&=\sum_{n=3}^\infty\calI_n(t_{k+1};t_k) + \err^{\calI_1}_k + \err^{\calI_2}_k\label{errPhi},
\end{align}

Lemmas \ref{lemma:errI1} and \ref{lemma:errI2} describe the behaviour of $\err_k^{\calI_1}$ and $\err_k^{\calI_2}$ in terms of $\Delta t_k$. To study the term $\sum_{n=3}^\infty\calI_n(t_{k+1};t_k)$, we recall from the definition \eqref{recursiveTerm} of the summands $\calI_n(t_{k+1};t_k)$, 
\[ 
\mathcal{I}_{n}(t;s) = \int_{s}^t \nabla_xf(\bfx(\tau))\mathcal{I}_{n-1}(\tau;s)d\tau, \ \ n \geq 1.
\]
Since $\calI_0(t,s)=I_{n_x}$, each such term can be expressed as
\[
\calI_n(t_{k+1};t_k) = \int_{t_k}^{t_{k+1}}\int_{t_k}^{\tau_1}\cdots\int_{t_k}^{\tau_{n-1}}\nabla_xf(\bfx(\tau_{n}))\cdots \nabla_xf(\bfx(\tau_1)) d\tau_{n}\cdots d\tau_1.
\]
Using this identity we can obtain an upper bound on the norm of $\sum_{n=3}^\infty\calI_n(t_{k+1};t_k)$, the part of the sum that is removed in the truncation term: 
\begin{equation}
    \label{majorationNormPBS}
    \begin{aligned}
    \left\|  \sum_{n=3}^\infty\calI(t_{k+1};t_k)\right\| &\le \sum_{n=3}^\infty\left\|\calI(t_{k+1};t_k) \right\| \\
    &=\sum_{n=3}^\infty\left\|\int_{t_k}^{t_{k+1}}\int_{t_k}^{\tau_1}\cdots\int_{t_k}^{\tau_{n-1}}\nabla_xf(\bfx(\tau_{n}))\cdots \nabla_xf(\bfx(\tau_1)) d\tau_{n}\cdots d\tau_1 \right\| \\
    &\le \sum_{n=3}^\infty\int_{t_k}^{t_{k+1}}\int_{t_k}^{\tau_1}\cdots\int_{t_k}^{\tau_{n-1}}\left\|\nabla_xf(\bfx(\tau_{n}))\cdots \nabla_xf(\bfx(\tau_1))\right\| d\tau_{n}\cdots d\tau_1 \\
    &\le \sum_{n=3}^\infty\int_{t_k}^{t_{k+1}}\int_{t_k}^{\tau_1}\cdots\int_{t_k}^{\tau_{n-1}}\left\|\nabla_xf(\bfx(\tau_{n}))\right\|\cdots \left\|\nabla_xf(\bfx(\tau_1))\right\| d\tau_{n}\cdots d\tau_1 \\
    &=\sum_{n=3}^\infty\frac{1}{n!} \left(\int_{t_k}^{t_{k+1}}\left\|\nabla_xf(\bfx(\tau))\right\|d\tau\right)^n\le\sum_{n=3}^\infty\frac{1}{n!} (C_{\nabla_x})^n(\Delta t_k)^n,
\end{aligned}
\end{equation}
with $C_{\nabla_x}:=\|\nabla_xf(\bfx(t))\|_{[0,T]}$, which is finite by Assumption \ref{as:time1}. The last equality is due to the fact that the multiple integrals 
\[ 
\int_{t_k}^{t_{k+1}}\int_{t_k}^{\tau_1}\cdots\int_{t_k}^{\tau_{n-1}}\left\|\nabla_xf(\tau_{n})\right\|\cdots \left\|\nabla_xf(\tau_1)\right\| d\tau_{n}\cdots d\tau_1,
\]
can be seen as the summands of the Peano-Baker series for the one dimensional ODE $\dot{z}(t)=\|\nabla_xf(t)\|\cdot z(t)$; since the terms $\|\nabla_xf(\tau_i)\|$ commute (being scalar functions), such multiple integrals are shown in \cite{PeanoBaker} to be equal to the simpler terms $\frac{1}{n!} \left(\int_{t_k}^{t_{k+1}}\left\|\nabla_xf(\tau)\right\|d\tau\right)^n$.

If we assume $\Delta t_k<1$ (we consider $\Delta t_{max}\to 0$), then $\Delta t_k^3\ge \Delta t_k^n$ for every $n\ge 3$, and we obtain the upper bound
 \begin{equation}
     \label{seriesMajoration}
     \left\|  \sum_{n=3}^\infty\calI(t_{k+1};t_k)\right\|\le (\Delta t_k)^3\sum_{n=3}^\infty\frac{1}{n!} (C_{\nabla_x})^n\le \Delta t_k^3\sum_{n\ge 0}\frac{1}{n!} (C_{\nabla_x})^n=\Delta t_k^3 e^{C_{\nabla_x}}.
 \end{equation}

Taking the norm of \eqref{errPhi}, and using the latter upper bound for $\| \sum_{n=3}^\infty\calI_n(t_{k+1};t_k) \|$,  we obtain
\[
\left\|\err^{\Phi}_k\right\|\le \Delta t_k^3 e^{C_{\nabla_x}}+\|\err_k^{\calI_1}\|+\|\err_k^{\calI_2}\|,
\]
where all three terms at the right-hand side are $O(\Delta t_k^3)$. This concludes the proof.
\end{proof}

Before we proceed with analysing the approximation of $\calJ _k$, which is the last term to consider before we move on to the approximation of $\bfS$ (the final node in Figure \ref{fig:errorPropagation}), we discuss the choice of truncating the Peano-Baker series at $n=2$. Introducing additional terms in the approximation (i.e., truncating after a larger number of summands) would lead to a higher power of $\Delta t_k$ in the upper bound \eqref{seriesMajoration}, which in turn would imply faster convergence. However, we rely on approximations of the summands in the Peano-Baker series rather than on the exact terms $\calI_n$, and these approximations retain an error of order $O(\Delta t_k^3)$ (see Lemmas \ref{lemma:errI1} and \ref{lemma:errI2}). Therefore, although including additional terms in the series would suggest a higher order of convergence, this would be cancelled by the $O(\Delta t_k^3)$ appearing in $\hat{\calI}_{1,k}$ and $\hat{\calI}_{2,k}$. 

We could also opt to truncate the Peano-Baker series at $n=1$ instead of $n=2$, i.e.\ retaining only $I_{n_x}$ and $\calI_{1}(t_{k+1};t_k)$. In this case, the opposite situation would arise: we would lower the power of $\Delta t_k$ in \eqref{seriesMajoration} to $\Delta t_k^2$, and the error $\err_k^{\Phi}$ would be $O(\Delta t_k^2)$. Thus, we would not benefit from the third order convergence of $\hat{\calI}_{1,k}$. The conclusion is that if the integrals in $\calI_1(t_{k+1};t_k)$ and $\calI_2(t_{k+1};t_k)$ are approximated with the trapezoidal rule (which produces $O(\Delta t_k^3)$ errors), then it is optimal to truncate the Peano-Baker series at $n=2$; optimal here means obtaining the highest possible order of convergence with as few summands as possible.

We now move to the analysis of the approximation error associated with $\calJ _k$, the final error to consider before proving Theorem \ref{thm:error}. Recalling the definition,
\[
    \calJ_k:=\int_{t_k}^{t_{k+1}}\Phi(t_k;\tau)\cdot \nabla_pf(\bfx(\tau))d\tau,
\]
we note that an application of the trapezoidal rule will lead to the state-transition matrix $\Phi(t_k;t_{k+1})$—the inverse of $\Phi(t_{k+1};t_k)$ (see Proposition \ref{prop:Phi})— appearing. However, for greater efficiency, we compute $\Phi(t_k;t_{k+1})$ by the Peano-Baker series, instead of computing the inverse $\Phi(t_{k+1};t_k)^{-1}$. In particular, we observe that $\Phi(t_k;t_{k+1})$ can be obtained from $\Phi(t_{k+1};t_k)$ by interchanging the limits of integration in each term $\calI_n$ (see \eqref{PBS} and \eqref{recursiveTerm}); interchanging the limits of integration, does not change the norm of an integral. Similarly, the approximations of the $\calI_n$s by the trapezoidal rule are the same (up to the sign) for both $\Phi(t_{k+1};t_k)$ and $\Phi(t_k;t_{k+1})$; thus, they have the same norm. By replicating the arguments we applied to $\Phi(t_{k+1};t_k)$ throughout Lemmas \ref{lemma:errI1}-\ref{lemma:errPhiOrder}, also on its inverse $\Phi(t_k;t_{k+1})$, we obtain the same bound for the error term $\err_k^{\Phi,inv}$:
\[
\err_k^{\Phi,inv}=\Phi(t_k;t_{k+1})-\hat{\Phi}(t_k;t_{k+1})=O(\Delta t_k^3).
\]


\begin{lemma}
\label{lemma:errJOrder}
Suppose that Assumptions \ref{as:errorODE}, \ref{as:boundedH}, \ref{as:time} hold. Then, the error in the error $\err_k^{trap,\calJ}$ associated with the approximation of the integral $\calJ_k$ by the trapezoidal rule is
\begin{equation}
    \label{errTrapJOrder}
    \|\err_k^{trap,\calJ}\|=O(\Delta t_k^3),
\end{equation}
and the whole error in the approximation of $\calJ_k$ with $\hat{\calJ}_k$ is
\begin{equation}
    \label{errJOrder}
    \|\err_k^{\calJ}\|=O(\Delta t_k^3).
\end{equation}
\end{lemma}
\begin{proof}
As in the proofs of Lemmas \ref{lemma:errI1} and \ref{lemma:errI2}, we can invoke the error of the trapezoidal rule \eqref{trapezoidalRuleError} and obtain
\begin{align*}
    \left(\err_k^{trap,\calJ}\right)_i^j=-\frac{\Delta t_k^3}{12}\cdot\left(\frac{d^2}{dt^2}\left(\Phi(t_k;t)\cdot\nabla_pf(\bfx(t))\right)\right)_i^j\Bigg|_{t_{\xi}},
\end{align*}
for some $\xi\in(t_k;t_{k+1})$. The proof of \eqref{errTrapJOrder} is now analogous to Lemmas \ref{lemma:errI1} and \ref{lemma:errI2} and we omit the details.


Moving to the error $\err_k^\calJ$, based on approximating the integral $\calJ_k$ with the trapezoidal rule, we first expand the error similar to what was done for $\err_k^{\calI_1}$ and $\err_k^{\calI_2}$:
\begin{align}
    \err_k^{\calJ} &= \int_{t_k}^{t_{k+1}}\Phi(t_k;\tau)\cdot \nabla_pf(\bfx(\tau))d\tau - \frac{\Delta t_k}{2}\cdot(\nabla_p\hat{f}_k+\hat{\Phi}(t_{k};t_{k+1})\cdot\nabla_p\hat{f}_{k+1}) \nonumber\\
    &= \int_{t_k}^{t_{k+1}}\Phi(t_k;\tau)\cdot \nabla_pf(\bfx(\tau))d\tau -\frac{\Delta t_k}{2}\cdot(\nabla_pf(\bfx(t_k))+\Phi(t_{k};t_{k+1})\cdot\nabla_pf(\bfx(t_{k+1})) \nonumber\\
    & \quad +\frac{\Delta t_k}{2}\cdot(\nabla_pf(\bfx(t_k))+\Phi(t_{k};t_{k+1})\cdot\nabla_pf(\bfx(t_{k+1})) \nonumber \\
    & \quad - \frac{\Delta t_k}{2}\cdot(\nabla_p\hat{f}_k+\hat{\Phi}(t_{k};t_{k+1})\cdot\nabla_p\hat{f}_{k+1}) \nonumber\\
    &=\err_k^{trap,\calJ}+\frac{\Delta t_k}{2}\cdot \left(\err_k^{\nabla_p}+\Phi(t_k;t_{k+1})\cdot\nabla_pf(\bfx(t_{k+1}))-\hat{\Phi}(t_k;t_{k+1})\cdot\nabla_p\hat{f}_{k+1}\right)\nonumber\\
    &=\err_k^{trap,\calJ}+\frac{\Delta t_k}{2}\cdot \bigl(\err_k^{\nabla_p}+\Phi(t_k;t_{k+1})\cdot\nabla_pf(\bfx(t_{k+1}))-\Phi(t_k;t_{k+1})\cdot\nabla_p\hat{f}_{k+1}\nonumber\\
    & \quad +\Phi(t_k;t_{k+1})\cdot\nabla_p\hat{f}_{k+1}-\hat{\Phi}(t_k;t_{k+1})\cdot\nabla_p\hat{f}_{k+1}\bigr)\nonumber\\
    &=
    \err_k^{trap,\calJ}+\frac{\Delta t_k}{2}\cdot \left(\err_k^{\nabla_p}+\Phi(t_k;t_{k+1})\cdot \err_{k+1}^{\nabla_p}+\err_k^{\Phi,inv}\cdot\nabla_p\hat{f}_{k+1}\right)\label{errJ}.
\end{align}


From the previous results, the terms in the parenthesis in \eqref{errJ} are all $O(\Delta t_k^3)$. Because the terms are multiplied by $\Delta t_k$, these errors become smaller than $\err_k^{trap,\calJ}$, hence
\[
\left\|\err_k^{\calJ}\right\|\le \left\|\err_k^{trap,\calJ}\right\|+o(\Delta t_k^3)=O(\Delta t_k^3) + o(\Delta t_k^3),
\]
from which the claim \eqref{errJOrder} follows .
\end{proof}

With Lemma \ref{lemma:errJOrder}, we now have estimates for all errors that propagate—as illustrated in Figure \ref{fig:errorPropagation}—into the approximation $\hat{\bfS}_{k+1}$ of the sensitivity matrix at time step $t_{k+1}$. We are now ready to prove Theorem \ref{thm:error}, the characterisation of the error in the approximation of the sensitivity matrix $\hat{\bfS}_{k+1}$ at time step $t_{k+1}$.  

\begin{proof}[Proof of Theorem \ref{thm:error}]
To estimate the error $\err_{k+1}^\bfS$ in the approximation of $\bfS$, we start from the exact expression for $\bfS(t_{k+1})$, given in \eqref{exactSensitivity}:
\[
    \bfS(t_{k+1}) = \Phi(t_{k+1};t_k)\cdot\left(\bfS(t_k)+\calJ_k\right).
\]
The approximation naturally takes a similar recursive form, using the approximations $\hat \Phi (t_{k+1};t_k)$ and $\hat \calJ _k$,
\[
    \hat \bfS _{k+1} = \hat \Phi (t_{k+1};t_k) \cdot\left(\hat \bfS _k+\hat \calJ_k\right).
\]
In order to analyse the associated error, first we replace every exact term in the definition of $\bfS (t_{k+1})$ with the sum of the corresponding  approximation and error; for example, we replace $\bfS(t_{k+1})$ with $\hat{\bfS}_{k+1}+\err_{k+1}^{\bfS}$. This leads to the following relation for the approximating terms and errors, implicitly defining the error $\err_{k+1}^\bfS$,
\begin{align*}
    \hat{\bfS}_{k+1}+\err_{k+1}^\bfS&=\left(\hat{\Phi}(t_{k+1};t_k)+\err_k^{\Phi}\right)\cdot\left(\left(\hat{\bfS}_k+\err_k^{\bfS}\right)+\left(\hat{\calJ}_k+\err_k^\calJ\right)\right).
\end{align*}
By expanding the product, we identify the expression $\hat{\Phi}(t_{k+1;t_k})\cdot \hat{S}_k+\hat{\calJ}_k$ on the right-hand side, which cancels $\hat{\bfS}_{k+1}$ on the left-hand side. 
As a result, the error in the sensitivity matrix can be expressed as
\begin{align*}
    \err_{k+1}^\bfS&=\hat{\Phi}(t_{k+1};t_k)\cdot\left(\err_k^\bfS+\err_k^{\calJ}\right)+\err_k^{\Phi}\cdot \left(\hat{\bfS}_k+\err_k^{\bfS}+\hat{\calJ}_k+\err_k^{\calJ}\right)\\
    &=\hat{\Phi}(t_{k+1};t_k)\cdot \err_k^\bfS+\hat{\Phi}(t_{k+1};t_k)\cdot \err_k^{\calJ}+\err_k^{\Phi}\cdot \left(\bfS(t_k)+\calJ_k\right),
\end{align*}
where in the last equality we re-introduced the exact terms for the sensitivity matrix at time step $t_k$ and the exact integral $\calJ_k$. This recursive expression can be expanded, using that $\err_0^\bfS=0$,
\[
\err_{k+1}^\bfS = \sum_{h=0}^k\left(\prod_{j=h+1}^k\hat{\Phi}(t_{j+1};t_j)\cdot\left(\hat{\Phi}(t_{h+1};t_h)\cdot \err_h^{\calJ}+\err_h^\Phi\cdot(\bfS(t_h)+\calJ_h)\right)\right).
\]
Taking the norm of the error, we obtain the following bound
\begin{align}
    \label{inequalityErrS}
   & \|\err_{k+1}^\bfS\| \nonumber \\
   &\quad \le \sum_{h=0}^k\left(\prod_{j=h+1}^k\left\|\hat{\Phi}(t_{j+1};t_j)\right\|\cdot\left(\|\hat{\Phi}(t_{h+1};t_h)\|\cdot \|\err_h^{\calJ}\|+\|\err_h^\Phi\|\cdot(\|\bfS(t_h)\|+\|\calJ_h\|)\right)\right).
\end{align}

To understand the convergence rate of the error $\err _{k+1} ^\bfS$, it now suffices to consider the terms on the right-hand side of \eqref{inequalityErrS}.

We start by considering $\hat \Phi (t_{j+1}; t_j)$. From the definition  \eqref{PhiApproximation}, applying the norm operator we obtain
\[
\left\|\hat{\Phi}(t_{j+1};t_j)\right\|\le 1 + \Delta t_j C_{\nabla_x} + \frac{\Delta t_j^2}{2}C_{\nabla_x}^2,
\]
for every $j=0,\dots, k$; here $C_{\grad _x} = \| \nabla _x f (\bfx (\cdot)) \| _{[0,T]}$, which is finite by Assumption \ref{as:time1}. This term is $O(1)$ for $\Delta t_j\to 0$. For arguments used later in the proof, it is convenient to define $C_{\Delta t}:=C_{\nabla_x}\cdot\left(1+\frac{\Delta t_{max}}{2}\right)$, which is not a constant, but depends on $\Delta t_{max}$ and $C_{\Delta t}\to C_{\nabla_x}$ as $\Delta t_{max}\to 0$. With this definition we can rewrite the upper bound as
\[
\left\|\hat{\Phi}(t_{j+1};t_j)\right\|\le 1 +C_{\Delta t}\Delta t_j.
\]

From Lemmas \ref{lemma:errJOrder} and \ref{lemma:errPhiOrder} we know that the error terms $\err_h^\calJ$ and $\err_h^{\Phi}$ are $O(\Delta t_h^3)$.

By Assumption \ref{as:matrices}, the exact sensitivity matrix at time step $t_h$ can be bounded as $\|\bfS(t_h)\|\le C_S$, hence $\|\bfS(t_h)\|=O(1)$, and $\|\Phi(t_h;t)\cdot\nabla_pf(\bfx(t))\|_{[0,T]}\le C_{\calJ}$ for every $h=0,\dots,k$. It follows that the integral term $\calJ_h$ can be bounded as
\[
\|\calJ_h\| = \left\|\int_{t_h}^{t_{h+1}}\Phi(t_h;\tau)\cdot\nabla_pf(\bfx(\tau))d\tau\right\|\le C_{\calJ}\Delta t_h,
\]
and we conclude that $\|\calJ_h\|=O(\Delta t_h)$.
As a consequence, the term 
\[
\|\hat{\Phi}(t_{h+1};t_h)\|\cdot \|\err_h^{\calJ}\|+\|\err_h^\Phi\|\cdot(\|\bfS(t_h)\|+\|\calJ_h\|),
\]
in \eqref{inequalityErrS} is $O(\Delta t_h^3)$ and we can define a new constant $0<C<\infty$, 
such that
\begin{align*}
\|\err_{k+1}^\bfS\|& \le\sum_{h=0}^k\left(\prod_{j=h+1}^k\left\|\hat{\Phi}(t_{j+1};t_j)\right\|\cdot C\Delta t_h^3\right) \\
&\le\sum_{h=0}^k\left(\prod_{j=h+1}^k\left(1 + C_{\Delta t}\Delta t_j\right)\cdot C\Delta t_h^3\right).
\end{align*}
Moreover, we can bound $\Delta t_j$ and $\Delta t_h$ by $\Delta t_{max}$, which gives an upper bound for $\err_{k+1}^\bfS$ in terms of $\Delta t_{max}$,
\[
\|\err_{k+1}^\bfS\|\le\sum_{h=0}^k\left(1 +C_{\Delta t} \Delta t_{max}\right)^{k-h}\cdot C\Delta t_{max}^3.
\]
The sum $\sum_{h=0}^k\left(1 + C_{\Delta t}\Delta t_{max}\right)^{k-h}$ can be rewritten as $\sum_{h=0}^k\left(1 + C_{\Delta t}\Delta t_{max}\right)^{h}$, which admits the closed formula
\begin{align}
    \sum_{h=0}^k\left(1 + C_{\Delta t}\Delta t_{max}\right)^{h} &= \frac{1-\left(1 + C_{\Delta t}\Delta t_{\max}\right)^{k+1}}{1-\left(1 +C_{\Delta t} \Delta t_{\max}\right)} \nonumber\\
    &=\frac{\left(1 + C_{\Delta t}\Delta t_{\max}\right)^{k+1}-1}{C_{\Delta t} \Delta t_{\max}}. \label{geomSum}
\end{align}
First, we consider the term $\left(1 + C_{\Delta t}\Delta t_{\max}\right)^{k+1}$ in the numerator. Since the exponent $k$ runs over $k=0,\dots,K-1$, and the term inside the parenthesis is non-negative, 
\begin{equation}
\label{inequalityExp}
\left(1 + C_{\Delta t}\Delta t_{\max}\right)^{k+1}\le \left(1 + C_{\Delta t}\Delta t_{\max}\right)^K.
\end{equation}

Second, the total number of time steps $K$ can be bounded from above and below,
\[\frac{T}{\Delta t_{max}}\le K\le \frac{T}{\Delta t_{min}}.\]
From this we can define $1\le\alpha<\infty$ and $0<\beta\le1$, dependent on $\Delta t_{max}$ and $\Delta t_{min}$, such that 
\begin{equation}
    \label{eq:K}
\alpha \frac{T}{\Delta t_{max}}=K=\beta \frac{T}{\Delta t_{min}}.
\end{equation}
It follows that $\alpha$ and $\beta$ are related as
\[\alpha = \beta \frac{\Delta t_{max}}{\Delta t_{min}}, \]
and combined with the assumption that there exists a constant $\Delta$ such that $\Delta t_{max} / \Delta t _{min} \leq \Delta$, and $\beta \in (0,1]$, this gives the upper bound $\alpha \leq \Delta$. Combining this with the first equality in \eqref{eq:K} for $K$ leads to $K\le \Delta \frac{T}{\Delta t_{max}}$. Inserting this in \eqref{inequalityExp} yields
\[
\left(1 + C_{\Delta t}\Delta t_{\max}\right)^{K}\le \left(1 + C_{\Delta t}\Delta t_{\max}\right)^{ \Delta \frac{T}{\Delta t_{max}}}.
\]

To finish the proof, we use that the function $g:(0,\infty)\to\bR$, $g(x)=(1+x)^{\frac{1}{x}}$ is monotonically decreasing and that $\lim_{x\to0^+}g(x)=e$. Since $C_{\Delta t}\Delta t_{max}$ is increasing in $\Delta t_{max}$, it follows that  
\[
(1+C_{\Delta t}\Delta t_{max})^{\frac{1}{C_{\Delta t}\Delta t_{max}}}<e,
\]
hence
\[\left(1 + C_{\Delta t}\Delta t_{\max}\right)^{ \Delta \frac{T}{\Delta t_{max}}}<e^{C_{\Delta t}T\Delta}.
\]
The previous results show that \eqref{geomSum} is bounded from above by
\[
\frac{e^{C_{\Delta t}T\Delta}-1}{C_{\Delta t}\Delta t_{max}}.
\]
Lastly, inserting this into the upper bound for $\err _{k+1} ^\bfS$ yields
\[
\|\err_{k+1}^\bfS\|\le \frac{e^{C_{\Delta t}T\Delta}-1}{C_{\Delta t}\Delta t_{max}}\cdot C\Delta t_{max}^3=\frac{C(e^{C_{\Delta t}T\Delta}-1)}{C_{\Delta t}}\Delta t_{max}^2.
\]
Since $C_{\Delta t}=O(1)$, this proves the claimed bound \eqref{errSOrder}.
\end{proof}

\section{Numerical Results}
\label{sec:numerical}
In this section we complement the theoretical analysis in Sections \ref{sec:genSolution}-\ref{sec:errEst} with numerical experiments, illustrating the performance of the proposed method in a number of examples. We first describe the implementation of the PBS algorithm, along with some modifications and an additional algorithm associated with Corollary \ref{cor:Sequi}, followed by a brief discussion of how the results are evaluated. We then present the results for numerical experiments in  four examples: two biological models, referred to as PKA and CaMKII, a random linear system and a dynamical system describing a Chua circuit.

\subsubsection*{Implementation and modifications of the PBS algorithm}
To test the accuracy and run time of the PBS algorithm, we have implemented it in the Julia language. For this purpose, we used the Julia packages \texttt{DifferentialEquations.jl} and \texttt{Sundials.jl} to solve the $n_x$ dimensional ODE system for the state variable $\bfx$. In particular, we used the solver \BDF \, from \texttt{Sundials.jl}, for which we set absolute and relative tolerances of 1e-6 and 1e-5, respectively. The ODE solver uses an adaptive time stepping, and thus returns the solution according to a non-uniform time grid, where time steps can be arbitrarily large (within the set time span). This corrupts the convergence of the PBS formula \eqref{eq:Approx} for the sensitivity matrix. To overcome this discrepancy between theory and implementation, at each iteration $k$ we refine the time step $[t_k,t_{k+1}]$ of size $\Delta t_k = t_{k+1}-t_k$ into $n_{int}$ uniformly spaced intervals $[t_k+\frac{i-1}{n_{int}}\Delta t_k,\,t_k+\frac{i}{n_{int}}\Delta t_k], i=1,\dots,n_{int}$. We then apply the PBS algorithm on each sub-interval and save the approximated sensitivity matrix $\hat{\bfS}_{k+1}$ corresponding to time $t_{k+1}$, thus discarding the auxiliary intermediate time steps. The number of sub-intervals $n_{int}$ must be such that the algorithm does not diverge; in our implementation of the algorithm we have set $n_{int}=\lceil 10\Delta t_k \|\nabla_xf(\hat{\bfx}_k)\| \rceil$.

This procedure of refining the time grid increases the running time of the algorithm, especially if the time span of the whole simulation is of the order of hundreds or thousands, as in the examples that will follow. One way to make the algorithm more efficient is to employ \eqref{solutionAtEquilibrium} from Corollary \ref{cor:Sequi}, i.e. the solution of the ODE for $\bfS$ when both the matrix of coefficients and the forcing term are constant: if there are time intervals $[t_k,t_{k+1}]$, as provided by the  ODE solver, in which the Jacobian $\nabla_xf$ is close to constant, then \eqref{solutionAtEquilibrium} provides a good approximation, without the need for a refinement of the time grid. In practice, to define ``close to constant'' for $\grad _x f$, in our experiments we require that the approximations $\hat{\bfx}_k$ and $\hat{\bfx}_{k+1}$ are such that $\|\nabla_xf(\hat{\bfx}_{k+1})-\nabla_xf(\hat{\bfx}_k))\|/\|\nabla_xf(\hat{\bfx}_k)\|< \e _{tol}$ for some tolerance level $\e _{tol}$. If this condition is satisfied, we use \eqref{solutionAtEquilibrium} to approximate $\hat{\bfS}_{k+1}$; in our experiments we chose $\e _{tol}=10^{-4}$.



If the condition is not satisfied, the PBS algorithm is more accurate than the approximation associated with \eqref{solutionAtEquilibrium}, the solution at equilibrium. However, if the value of $n_{int}$ is too high, either because of a large value $\Delta t_k$ or $\|\nabla_xf(\hat{\bfx}_k)\|$ far from constant, then the algorithm becomes inefficient. On the other hand, decreasing the number of sub-intervals $n_{int}$ could cause the algorithm to diverge. Therefore, we imposed an additional condition on $n_{int}$: if the refinement would lead to a number of intervals $n_{int}>n_{max}$, then we do not proceed with the refinement, and we apply \eqref{solutionAtEquilibrium} instead. In our simulations we set $n_{max} = 10$.

Combining the two conditions leads to the following \texttt{if}-statement for the algorithm:
\begin{equation}
    \label{eq:ifStat}
    \text{if}\quad \frac{\|\nabla_xf(\hat{\bfx}_{k+1})-\nabla_xf(\hat{\bfx}_k)\|}{\|\nabla_xf(\hat{\bfx}_k)\|} < 10^{-4}\quad\text{OR}\quad n_{int}>10,
\end{equation}
then apply \eqref{solutionAtEquilibrium}, without refinement of $[t_k,t_{k+1}]$. Consequently, the PBS algorithm (Algorithm~\ref{alg:approx}) becomes Algorithm \ref{alg:PBSR}, that we will call the PBS with refinement (PBSR) algorithm.

\begin{algorithm}
 \caption{Peano-Baker Series algorithm with Refinement (PBSR) for the sensitivity matrix $\bfS$ \label{alg:PBSR}}

\SetAlgoLined
\KwResult{$\hat{\bfS}_1,\dots,\hat{\bfS}_K$}
$\hat{\bfx}_k\approx \bfx(t_k),\quad k=1,\dots,K$\;
$\nabla_x\hat{f}_k = \nabla_xf(\hat{\bfx}_k,\bfu(t_k),\bfp)$\;
$\nabla_p\hat{f}_k = \nabla_pf(\hat{\bfx}_k,\bfu(t_k),\bfp)$\;
$\hat{\bfS}_0=0\in\mathbb{R}^{n_x\times n_p}$\;

 \For{$k\leftarrow 0$ \KwTo $K-1$}{
 $\Delta t_k = t_{k+1}-t_k$\;
 
 $n_{int} = \lceil 10\Delta t_k \|\nabla_xf(\hat{\bfx}_k)\| \rceil$\;
 
  \eIf{$\frac{\|\nabla_xf(\hat{\bfx}_{k+1})-\nabla_xf(\hat{\bfx}_k)\|}{\|\nabla_xf(\hat{\bfx}_k)\|}\le10^{-4}$\quad OR\quad $n_{int}>10$}{
   $\hat{\bfS}_{k+1}=e^{\Delta t_k\cdot\nabla_x\hat{f}_k}\cdot\bigl(\hat{\bfS}
   _k+\bigl(I-e^{-\Delta t_k\cdot\nabla_x\hat{f}_k}\bigr)\cdot\nabla_x\hat{f}_k^{-1}\cdot\nabla_p\hat{f}_k\bigr)$\;
   }{
   $\hat{\bfS}_{temp} = \hat{\bfS}_{k}$\;
   
   \For{$h\leftarrow 0$ \KwTo $n_{int}-1$}{
   $t_a = t_k + \frac{h}{n_{int}}(t_{k+1}-t_k)$\;
   $t_b = t_k + \frac{h+1}{n_{int}}(t_{k+1}-t_k)$\;
   $\Delta t = t_b - t_a$\;
   $\hat{\bfx}_a = \hat{\bfx}_k + \frac{h}{n_{int}}(\hat{\bfx}_{k+1}-\hat{\bfx}_k)$\;
   $\hat{\bfx}_b = \hat{\bfx}_k + \frac{h+1}{n_{int}}(\hat{\bfx}_{k+1}-\hat{\bfx}_k)$\;
   $\nabla_x\hat{f}_a = \nabla_xf(\hat{\bfx}_a,\bfu(t_a),\bfp)$\;
   $\nabla_x\hat{f}_b = \nabla_pf(\hat{\bfx}_b,\bfu(t_b),\bfp)$\;
   $\nabla_p\hat{f}_a = \nabla_pf(\hat{\bfx}_a,\bfu(t_a),\bfp)$\;
   $\nabla_p\hat{f}_b = \nabla_pf(\hat{\bfx}_b,\bfu(t_b),\bfp)$\;
   $\hat{\mathcal{I}}_{1} = \frac{\Delta t}{2}\cdot\bigl(  \nabla_x\hat{f}_a + \nabla_x\hat{f}_{b}\bigr)$\;
   $\hat{\mathcal{I}}_{2} = \frac{\Delta t^2}{4}\cdot\bigl(\nabla_x\hat{f}_{b}\cdot(  \nabla_x\hat{f}_a + \nabla_x\hat{f}_{b}\bigr)\bigr)$\;
   $\hat{\Phi}(t_b;t_a)=I+ \hat{\mathcal{I}}_{1}+\hat{\mathcal{I}}_{2}$\;
   $\hat{\Phi}(t_{a};t_{b})=I- \hat{\mathcal{I}}_{1}+\hat{\mathcal{I}}_{2}$\;
   $\hat{\bfS}_{temp} = \hat{\Phi}(t_{b};t_a)\cdot\bigl(\hat{\bfS}_{temp}+\frac{\Delta t}{2}\cdot\bigl(\nabla_p\hat{f}_a + \hat{\Phi}(t_a;t_b)\cdot \nabla_p\hat{f}_b\bigr)\bigr)$\;
  }
  $\hat{\bfS}_{k+1} = \hat{\bfS}_{temp}$\;
  }
 }
\end{algorithm}

An alternative to the PBSR algorithm 
is to never refine the intervals $[t_k,t_{k+1}]$ and always use Corollary \ref{cor:Sequi}: it will not be as accurate as the PBSR algorithm—especially when there are significant variations in $\nabla_xf$ within $[t_k,t_{k+1}]$—however there is no risk of divergence of the algorithm, even in absence of refinement. This is presented as Algorithm~\ref{alg:ExpAlg}, which we refer to as the Exponential algorithm (Exp), because of the exponential transition matrix that is used at each time step.

\begin{algorithm}
 \caption{Exponential algorithm (Exp) for the sensitivity matrix $\bfS$ \label{alg:ExpAlg}}
\SetAlgoLined
\KwResult{$\hat{\bfS}_1,\dots,\hat{\bfS}_K$}
$\hat{\bfx}_k\approx \bfx(t_k),\quad k=1,\dots,K$\;

$\nabla_x\hat{f}_k = \nabla_xf(\hat{\bfx}_k,\bfu(t_k),\bfp),$ \quad $\nabla_p\hat{f}_k = \nabla_pf(\hat{\bfx}_k,\bfu(t_k),\bfp)$\;

$\hat{\bfS}_0=0\in\mathbb{R}^{n_x\times n_p}$\;

 \For{$k\leftarrow 0$ \KwTo $K-1$}{
 $\Delta t_k = t_{k+1}-t_k$\;
 
   $\hat{\bfS}_{k+1}=e^{\Delta t_k\cdot\nabla_x\hat{f}_k}\cdot\bigl(\hat{\bfS}
   _k+\bigl(I-e^{-\Delta t_k\cdot\nabla_x\hat{f}_k}\bigr)\cdot\nabla_x\hat{f}_k^{-1}\cdot\nabla_p\hat{f}_k\bigr)$
 }
\end{algorithm}

\subsubsection*{Evaluation}
To evaluate the accuracy of the Algorithms \ref{alg:PBSR} and \ref{alg:ExpAlg}, we compared their approximations of the sensitivity matrix $\bfS$ with the result of the commonly used Forward Sensitivity (FS) method, implemented in Julia in the package \texttt{DiffEqSensitivity.jl}. The latter algorithm should provide an accurate solution to the sensitivity problem, albeit at a high computational cost: the FS algorithm solves at the same time the ODE for both the state variable $\bfx$ and the sensitivity matrix $\bfS$, by means of a new ODE system of dimension $n_x\times(n_p+1)$. For comparison, in the PBSR and Exp algorithms we only solve the $n_x-$dimensional ODE system for $\bfx$.


The comparison is done as follows: for each of the time steps $t_k, k=0,\dots,K$, we compute approximations of the sensitivity matrix $\bfS$ with the FS method ($\hat{\bfS}$), the PBSR ($\hat{\bfS}^{PBSR}_k$) and the Exponential ($\hat{\bfS}^{Exp}_k$) algorithms. We then compute the relative error of the PBSR and Exponential algorithms with respect to the FS method, at each time step $t_k$, as
\[\rerr^{PBSR}_k = \frac{\|\hat{\bfS}^{PBSR}_k-\hat{\bfS}^{FS}_k\|}{\|\hat{\bfS}^{FS}_k\|}, \quad\quad \rerr^{Exp}_k = \frac{\|\hat{\bfS}^{Exp}_k-\hat{\bfS}^{FS}_k\|}{\|\hat{\bfS}^{FS}_k\|}.\]
These relative errors are used to measure the performance of the two methods in the examples that follow.
\subsubsection*{Models for molecular signaling pathways (PKA and CaMKII models)}
We now move to the outcomes of the first numerical experiments. With the goal of considering models from systems biology (see Section \ref{sec:intro}) in mind, we applied the PBSR Algorithm \ref{alg:PBSR} and the Exp Algorithm \ref{alg:ExpAlg} to two models that describe molecular signaling pathways within neurons involved in learning and memory. In particular, in the mechanisms involved in the strengthening or weakening of neuron synapses, long term potentiation (LTP) and long term depression (LTD). 

A crucial role in signaling pathways is played by protein kinases and phosphatases, thanks to their ability to, respectively, phosphorylate and dephosphorylate substrate proteins. In the two models that we consider, the phoporylating role is performed by the cAMP-dependent protein kinase (PKA) and the Ca$^{2+}$/calmodulin-dependent protein kinase II (CaMKII), which give the two models their names: PKA and CaMKII-see respective references \cite{Church2021.03.14.435320} and \cite{NAIR2014277, Eriksson19} for an in-depth description of the two models and the GitHub repository associated with this paper\footnote{\url{https://github.com/federicamilinanni/JuliaSensitivityApproximation}} for details of their implementation. In the two ODE models, the state vectors $\bfx(t)$ represent the concentrations of the different forms of the modelled proteins and ions at time $t$; the dimension of the state space is $n_x=11$ in the PKA model, and $n_x=21$ in the CaMKII model. In both models the parameter vector $\bfp$ correspond to the kinetic constants that characterize the reactions in the underlying pathway. The dimension of the parameter space is $n_p=35$ in the PKA, and $n_p=59$ in the CaMKII model; the external inputs $\bfu$ are here treated as constant parameters and future work includes also considering time-varying functions.

In Figures \ref{fig:PKA} and \ref{fig:CaMKII} we show the relative errors $\rerr^{PBSR}$ (solid line) and $\rerr^{Exp}$ (dashed line) against the time step for the PKA and CaMKII model, respectively. The small circles superimposed on the solid line (PBSR algorithm) refer to the time steps at which the \texttt{if}-statement is satisfied and \eqref{solutionAtEquilibrium} is used. We observe that the results obtained by the PBSR algorithm are $1$ to $2$ order of magnitude more accurate than the Exp algorithm. 

To test the efficiency of the FS, PBSR and Exp algorithms, we applied them to the PKA and CaMKII models and we computed the averaged runtime (averaged over $100$ iterations). The timing results are listed in Table \ref{tab:timingPKACaMKII}. As expcted, the PBSR shows higher accuracy, with an increased computational cost, than the Exponential algorithm. Compared to the FS method, the PBSR algorithm is more efficient, in particular when the dimension of the system is rather high (e.g., in the CaMKII model). 

\begin{table}
\centering
\begin{tabular}{ c | c c c }
  & PKA & CaMKII \\ 
  \hline
 FS & $0.056\,s$ & $1.518\,s$ \\  
 PBSR & $0.047\,s$ & $0.110\,s$ \\
 Exp & $0.008\,s$ & $0.030\,s$
\end{tabular}
\caption{Average runtime (averaged over $100$ iterations) of FS, PBSR and Exp algorithms applied to the PKA and CaMKII models.}
\label{tab:timingPKACaMKII}
\end{table}


\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{PKAError.pdf}
    \caption{Relative error of the sensitivity matrix, against time step, for the PBSR (solid line) and the Exponential (dashed line) algorithms for the PKA model. The simulations are run over the time span $[0,600]$. The small circles superimposed on the solid line indicate the time steps at which the exponential transition matrix is used instead of the Peano-Baker series approximation.}
    \label{fig:PKA}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{CaMKIIError.pdf}
    \caption{Relative error of the sensitivity matrix, against time step, for the PBSR (solid line) and the Exponential (dashed line) algorithms for the CaMKII model. The simulations are run over the time span $[0,600]$. The small circles superimposed on the solid line indicate the time steps at which the exponential transition matrix is used instead of the Peano-Baker series approximation.}
    \label{fig:CaMKII}
\end{figure}

\subsubsection*{Random linear system}
In order to investigate numerically how an increase in the dimensions $n_x$ and $n_p$ impacts the two algorithms, we implemented a random linear system for which we can choose arbitrary dimensions $n_x$; for convenience  we set this to be equal to the parameter space dimension $n_p$ and denote both with $n$. 

The model is obtained by first generating a random matrix $\bfB$ of dimension $n \times n$ and a random vector $\bfp$ of length $n$, both with values uniformly distributed in the interval $[0,1]$. Next, we define $\bfA:=-\bfB^T\bfB$ and the input vector $\bfu$ as an $n$ dimensional vector of ones. We then define the corresponding (random) linear system as
\[\dot{\bfx}(t)=\bfA\bfx+\bfp^2+\bfu,\]
which we use to test how the dimension of the system affects the runtime of the three algorithms (PBSR, Exponential and FS).

In this example, the Jacobian is $\grad _x f$ is constant by construction—$\nabla _x f \equiv \bfA$—and the \texttt{if}-statement \eqref{eq:ifStat} in Algorithm~\ref{alg:PBSR} is always true and there would never be a refinement of the time intervals $[t_k;t_{k+1}]$, leading to the same output as the Exponential algorithm (in approximately the same computation time). Therefore, to test the approximation based on the Peano-Baker series, we eliminate the \texttt{if}-statement and always apply the Peano-Baker series approximation, including the refinement of the time grid, to approximate the state-transition matrix and compute $\bfS$.

We performed tests of the three algorithms on this type of random linear system of dimensions $n=5,10,\dots,95,100$. In Figure \ref{fig:TimevsSize} we show the average runtime (over $10$ iterations) of the FS algorithm (solid line), the PBSR algorithm (dashed line) and Exponential (dotted line). 
To determine how the runtime scales with the dimension $n$ of the system, we perform regressions on the form $\text{runtime} = a\cdot\text{dimension}^b$, using runtime data for each of the three algorithms. The results suggest a runtime of $O(n^{2.1})$ for the Exp algorithm, $O(n^{3.7})$ for the PBSR and $O(n^{4.2})$ for the FS. Therefore, according to this estimate, the PBSR algorithm gives an improvement of approximately $n ^{0.5}$ compared to the FS method.


\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{TimevsSize.pdf}
    \caption{Average running time (over 10 iterations) of the FS (solid line), PBSR (dashed line) and Exponential (dotted line) algorithms applied to random linear systems of dimension $5,10,\dots,100$.}
    \label{fig:TimevsSize}
\end{figure}

\subsubsection*{Dynamical system modelling Chua's circuit}
Given the higher efficiency of the Exponential algorithm observed for the random linear systems (see Figure \ref{fig:TimevsSize}), it is natural to ask whether the PBSR algorithm is worth the additional computational effort. The following example shows that it can indeed be the case: consider the three-dimensional dynamical system,
\begin{equation*}
    \begin{cases}
    \dot{x}_1 = p_1(x_2 - x_1 - f(x)),\\
    \dot{x}_2 = x_1 - x_2 + x_3,\\
    \dot{x}_3 = -p_2x_2,
    \end{cases}
\end{equation*}
with $f(x) = -8/7x_1+4/63 x_1^3$, parameters $p_1 = 7$, $p_2 = 15$, and initial condition $\bfx_0 = (0,0,-0.1)^T$. 
This system models Chua's circuit, an electrical circuit consisting of two capacitors and an inductor, and the choice of parameters and initial condition causes the system to converge to a limit cycle.

In this example, the Jacobian $\nabla_xf$ shows high variability within time steps $[t_k,t_{k+1}]$. The PBSR algorithm (Algorithm \ref{alg:PBSR}), because of the refinement of the time intervals, is able to capture these variations, and provides an approximation of the sensitivity matrix $\hat{\bfS}$ about one order of magnitude more accurate than the Exponential algorithm (Algorithm \ref{alg:ExpAlg}); the results are shown in Figure \ref{fig:Chua}. 

\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{ChuaError.pdf}
    \caption{Relative error of the sensitivity matrix, against time step, for the PBSR (solid line) and the Exponential (dashed line) algorithms for the Chua system. The simulations are run over the time span $[0,10]$. The small circles superimposed on the solid line indicate the time steps at which the exponential transition matrix is used instead of the Peano-Baker series approximation.}
    \label{fig:Chua}
\end{figure}



\section{Conclusion and future Work}
\label{sec:conclusion}
In this paper we address the problem of computing the sensitivity matrix of parameter-dependent ODE models in high-dimensional settings, where the forward or adjoint methods become too slow for practical purposes. This situation arises in, e.g., uncertainty quantification using Bayesian methods, where there can be a need to compute the sensitivity matrix at a large number of time steps and parameter values, 
and the parameter space is high-dimensional. 

We develop a new method based on the Peano-Baker series from control theory, which is used to derive a representation of the sensitivity matrix $\bfS$. By truncating the series and applying the trapezoidal rule for the integrals involved we construct an approximation to $\bfS$ amenable to numerical computation. In addition to the general representation of \ref{thm:generalODESol}, in Corollary \ref{cor:Sequi} we derive a simplified form in the setting of constant coefficients and forcing term in the ODE for $\bfS$. This lead to a second numerical method, referred to as the exponential algorithm, which is exact when the system is at equilibrium and a good approximation when the vector field of the ODE system has an almost-constant Jacobian $\nabla _x f$. In Section \ref{sec:numerical} we describe how the two methods can be combined to further speed up the numerical computations while maintaining high accuracy.

A rigorous error analysis shows that, under standard regularity assumptions, the proposed algorithm, based on the Peano-Baker series, admits a global error of order $O(\Delta t_{max} ^2)$. The analysis also shows that the proposed method is optimal in the sense of at what term the Peano-Baker series is truncated. This error analysis is complemented by several numerical experiments. We compare the performance of the different methods to that of the forward sensitivity method for two ODE models from systems biology, a random linear system and a system modelling Chua's circuit. The results show that both our algorithms produce accurate approximations of the sensitivity matrix with significant speed-up, which seems to increase rapidly with the dimensionality of the problem. In the dynamical system modeling Chua's circuit, the limit trajectory of the ODE is a limit cycle and thus the Jacobians never becomes (close to) constant. In this example the PBS algorithm produces approximations that have an accuracy one order of magnitude higher than that of the exponential algorithm, which motivates the use of the PBS algorithm for accuracy in general problems. A further motivation comes from the applications in neuroscience that we consider, where ODE models are commonly characterised by time-dependent inputs (e.g. Ca-spike trains). Here the framework is similar to the Chua's circuit example, where the Jacobians are time-variant, thus the PBS algorithm is expected to be more precise than the Exponential algorithm.

The results of this paper are expected to be beneficial for applications to MCMC methods in systems biology: the speed-up provided by the PBS(R) and Exponential algorithms with respect to forward sensitivity analysis should drastically increase the efficiency of the MCMC methods. As a first test we equipped an implementation of the SMMALA algorithm (in C, CVODES solver) with the near-steady-state sensitivity approximation method to compute the Fisher Information and posterior gradients and compared it to SMMALA with conventional forward sensitivity analysis. The (real) data used for MCMC was obtained at or near the steady-state of the system, so the approximation is justified. The near-steady-state sensitivity approximation allowed the SMMALA algorithm to sample approximately $100$ times faster from the posterior distribution than CVODES' forward sensitivity analysis\footnote{A speed-up of that magnitude is difficult to test thoroughly (with large sample sizes) as the slower sampler needs so much time to finish}. Similar tests on the PBSR algorithm are left for the future.

Future work also includes further investigation of the impact of different implementation aspects of the PBS algorithm—e.g.\ the switching criteria (based on the Jacobian $\grad _x f$) and the refinement of the time grid of the ODE solver—and properly introducing the method in MCMC sampling. Additional comparisons with existing methods and a better understanding of the methods performance, particularly in large-scale systems, is another important direction.   
\section*{Acknowledgement}
 The simulations were performed on resources provided by the Swedish National Infrastructure for Computing (SNIC) at LUNARC, Center for High Performance Computing.

\section*{Funding}
The study was supported by Swedish eScience Research Centre (SeRC), EU/Horizon 2020 no. 785907 (HBP SGA2) and no. 945539 (HBP SGA3).

The research of PN was supported in part by the Swedish Research Council (VR-2018-07050) and the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation.


\printbibliography

\end{document}
