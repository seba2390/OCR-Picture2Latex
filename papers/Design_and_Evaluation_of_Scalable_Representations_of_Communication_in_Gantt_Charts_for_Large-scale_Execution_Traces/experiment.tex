\section{Experiment Methodology}
\label{sec:methodology}

We evaluate the efficacy of the proposed design and the existing methods through a pre-registered~\footnote{
https://osf.io/zwrgk}
user study. The final design was refined through four pilot studies, two in-person task-design-focused pilots and two online holistic pilots. The study source code and information about the pilots is available in the supplemental materials.

\subsection{Experiment Overview}

We design a within-subjects study with one factor: representation type. This factor has three levels: {\em full}, {\em partial}, and {\em new}, where {\em full} and {\em partial} are the zoomed-out and zoomed-in versions of the existing Gantt chart and {\em new} is the design described in \autoref{sec:design}.

We measure four dependant variables: (1) total accuracy across all question types, (2) accuracy for pattern type classification questions, (3) accuracy for grouping classification questions, and (4) accuracy for stride differentiation questions. We measure accuracy as the ratio of correct answers / total answers, per participant. We chose to make the denominator total answers instead of \textit{total questions} to account for missing values in the dataset, e.g., where the platform failed to log a user's response properly. 

In classification questions, participants were shown a chart and asked which of the three pattern types it represented and then separately, whether the pattern was continuous or grouped. In discrimination questions, participants were shown two charts and asked if they exhibited the same stride. Participants were shown eight target patterns, rendered in each of the three styles, with two categorization questions (type, grouping) and two discrimination questions (both stride comparison against two full or two partial charts), resulting in (8 charts x 3 conditions x 2 categorizations) $+$ (8 charts x 3 conditions x 2 discriminations) $=$ 96 total trials. Five of these patterns were $>1000$ rows with each set to one  of the five configurations of structure and grouping we support with our new designs (\autoref{fig:abstract_designs}).
    
Chart density, pattern strides, and pattern grouping are randomized to increase generalizability across specific pattern instances and following our findings in \autoref{sec:prelim}. Chart density was varied between a binary selection of 1280 or 2560 PEs, to simulate dense charts common in reasonably sized parallel programs. We did not scale these charts to still-realistic sizes of 10,000 PEs as they were even less discernible and conclusions derived from these ``smaller'' charts would extend to even denser ones.  Pattern stride varies in the range of 2 to 10. Stride is kept relatively small in our pattern generation to accommodate possible configurations of grouped patterns which have relatively small grouping factors. However, as we discuss in \autoref{sec:analysisdiscussion}, this choice may have given {\em partial} representations an advantage. Patterns were grouped into bundles of 4 to 16 lines because they would be possible grouping configurations at low or high numbers of PEs.

We did not test the placement of the representations within a larger Gantt chart. Our rationale is that we first seek to understand efficacy in a simple isolated case so we can draw conclusions for further design.

    

\subsection{Procedure}
    The study begins with an informed consent document and short questionnaire regarding demographic information, including prior experience with programming, HPC, and performance analysis. There are then two experimental modules: classification and discrimination. Each module begins with a written tutorial explaining the context, seven practice questions with answers and explanations shown, and then the set of 48 recorded trials, one per screen. During the trials, a progress bar is shown. After both experimental modules, participants were asked which representation they preferred and given a free-form text fields to elaborate on their opinions of the experiment and tutorial.

    The classification tutorial introduces participants to the concepts of parallel resources, tracing, Gantt charts, and communication patterns therein, with emphasis on pattern types and grouping. The discrimination tutorial reinforces the concept of pattern stride. We divided questions into these two modules with separate tutorials following concerns about information overload in our first online pilot.
    Participant feedback in the second online pilot noted the tutorials were complex and long, but also went into appropriate depth. All participants rated them as ``moderately helpful'' (5), ``helpful'' (2), or ``very helpful'' (3).



\subsection{Task Design}

    We asked users to perform two types tasks:
    \vspace{-.5em}
    \begin{enumerate}[start=1, label={\bfseries T\arabic*}]
        \itemsep0em
        \item Classify a pattern in a chart
        \item Discriminate stride between two charts
    \end{enumerate}
    
    The classification task was broken into two separate questions, a three-alternative forced choice for the pattern type and a two-alternative forced-choice between grouped or continuous. These tasks were not necessarily asked in sequence. \autoref{fig:class-pattern} shows an example pattern type classification question. The discrimination task asks participants if two representations have the same stride. \autoref{fig:discrim-ex} shows an example.
    
    \begin{figure}[htb]
        \centering
        \includegraphics[width=\columnwidth]{figures/classification-pattern.png}
        \caption{A pattern type classification question. Users are prompted with a chart on the left side of the screen and given a 3-alternative forced choice prompt to select the underlying communication structure.}
        \label{fig:class-pattern}
    \end{figure}
 


For both types of tasks, participants were instructed to answer quickly with a suggestion to take less than 15 seconds per question. The rationale is that the use cases driving these questions are similarly quick, aiding the user in directing focus for further analysis. In practice, users responded much quicker with a mean response time of 4.3s and median response time of 3.1s. 
    
    \begin{figure}[htb]
        \centering
        \includegraphics[width=\columnwidth]{figures/discrimination-stride.png}
        \caption{An example of a stride discrimination question. Users are prompted with two charts, of different representation or number of rows and asked if they have the same stride.}
        \label{fig:discrim-ex}
    \end{figure}
    
    The tasks were designed iteratively through piloting. We decided to separate the pattern attributes of type, grouping, and stride following the first offline pilot and the preliminary study which both asked a more general discrimination task: if the charts had the ``same pattern.'' We found it difficult to disambiguate errors in these pilots, especially as participants took different meanings of the word `pattern.'
    
    We initially chose discrimination tasks based on the scenario that performance analysts might want to identify which regions in a Gantt chart are iterations of the same communication operation or even the same code. We switched pattern type and grouping to classification under the rationale that this identification was necessary to assess same-ness and that recognition tasks for these higher level abstractions are important. We kept stride as a discrimination task because the exact number is less important and as stated previously, dependent on several execution-time calculations.
    
    We also considered a drawing task, where participants would be asked to copy a pattern from one representation onto a blank background of a different size, as a holistic test. We ran a crayon and paper pilot of this approach but found the responses difficult to interpret, even with follow-up interviews.
    
    

\subsection{Participants}
    Ideally the participants in this experiment would all be familiar with HPC, a variety of HPC applications, HPC traces, Gantt charts and logical time. However, we previously struggled to find sufficient numbers of people with this expertise to participate in a quantitative study. As our preliminary study did not suggest differences in people based on their relative HPC experience, we decided to widen the pool to people familiar with computing in general. Our rationale was that these people could serve as a proxy for beginners in visual trace analysis, similar to the HPC workers we previously interviewed. Furthermore, they could understand the context given the tutorial.
    
    We arrived at a target of 35 participants based on our second online pilot which had 10 participants and was run on the target platform. Using the means and variances for each hypothesis from that pilot, we calculated the participants necessary to achieve a power of $1 - \beta=0.8$ with our target significance for each hypothesis and took the maximum participants required across all hypotheses.

    Using the Prolific~\cite{prolific} crowd sourcing platform, we recruited 43 participants who reported at least ``beginner'' level coding experience in their profile and were using desktop or laptop machines. Participants were payed a flat rate of \$9.00 for our estimated 35 minute experiment, reflecting a target wage of \$15/hr.
    
    We included four trivial questions, two in each module, to serve as guards. We rejected responses that failed one or more of these questions and collected data until we had 35 valid responses. We rejected eight responses based on these guards, of which five had total response times of less than half of the estimated experiment time. Of the valid responses, participants reported their ages in [20,56] with the median being 26. Twenty-eight reported as men and seven as women. 


\subsection{Experiment Platform}
    The experiment platform is a custom built website hosted on a Heroku web application server~\cite{experiment_platform}. The website is composed of a Flask~\cite{flask} (Python) server and a Javascript front-end. All charts were drawn with D3js~\cite{d3js}. The source code is included in the supplemental materials.
    
    Participants were restricted through the Prolific platform to desktop or laptop devices. Screen resolutions ranged from 1093 $\times$ 615 to 2560 $\times$ 1440, with a mode of 1920 $\times$ 1080.
    
    
\subsection{Hypotheses}

  Based on data from the second online pilot as well as our own design goals, we arrived at the following hypotheses for experiment:
    
    \vspace{-.5em}
    \begin{enumerate}[start=1, label={\bfseries H\arabic*}]
        \itemsep-.3em
        \item New designs will be approximately the same in accuracy as traditional full representations for identification of structure, grouping and stride when taken all-together.
        \item New designs will enable more accurate identification of communication trace structure, for sufficiently dense charts ($>1000$ rows/processes), than either partial or full representations.
        \item New designs will enable more accurate identification of communication trace structure "grouping", for sufficiently dense charts ($>1000$ rows/processes), than either partial or full representations.
        \item New designs will enable less accurate identification of communication trace pattern stride than either partial or full representations.
        \item Partial or "zoomed in" representations will result in less accurate identification of communication trace pattern stride, in addition to grouping and pattern identification than either full representations or new designs.
    \end{enumerate}

In designing the {\em new} representations, we chose to trade off exactness in stride for discriminability in pattern type and grouping. Thus, we expected the {\em new} representations to result in increased accuracy for pattern type (``structure'') and grouping (H2, H3), but worse accuracy for stride (H4). Based on pilot data, we expected these trade off to ``even out'' in overall accuracy between {\em new} and {\em full} (H1), with {\em partial} representations lagging based on pilot data (H5).