%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

% \documentclass[letterpaper, 10 pt, journal, twoside]{ieeetran}  % Comment this line out if you need a4paper
% Things to change:
% - Add in \thanks line for Doi
% - Add back \begin{IEEEkeywords} ... \end{IEEEkeywords} line.
% - Add back %\IEEEPARstart{F}{or} beginning

% \documentclass[letterpaper, 10 pt, journal, twoside]{ieeeconf}  % Comment this line out if you need a4paper
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

% ===
%
% --- For submission
% \usepackage[disable]{todonotes}
% \usepackage[final]{showlabels}

% --- For writing
\usepackage[disable]{todonotes}
% \usepackage[marginal]{showlabels}

%
% ====

\usepackage{xcolor}
\newcommand{\subparagraph}{}
\usepackage{titlesec}

\reversemarginpar % Move the todo notes towards the left margin.

\newcommand{\jviereck}[1]{\todo[backgroundcolor=white,inline]{TODO(jviereck): #1}}

\newcommand{\jviereckinline}[1]{\todo[backgroundcolor=white,inline]{TODO(jviereck): #1}}
\newcommand{\assert}[1]{\todo[backgroundcolor=orange,inline]{ASSERT: #1}}
\newcommand{\review}[1]{\todo[backgroundcolor=lightgray,inline]{ASSERT REVIEW: #1}}
\newcommand{\missingtext}[1]{\todo[color=magenta,inline]{WRITE ME(jviereck):
#1}}
\newcommand{\writeme}[1]{\missingtext{#1}}
\newcommand{\question}[1]{\todo[backgroundcolor=orange,inline]{Question: #1}}
\newcommand{\ludo}[1]{\todo[backgroundcolor=red,inline]{TODO(ludo): #1}}

% For SI units.
\usepackage{siunitx}

\usepackage{dsfont}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{bbold} % For identity symbol
\usepackage[font=small]{subcaption,caption} % For subfigure.

\usepackage{multirow} % Latex Tabel: To get rows over multiple rows
\usepackage{makecell} % For multiline in table, see, https://tex.stackexchange.com/a/176780

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\newcommand{\italic}[1]{\textit{#1}}
\renewcommand{\b}[1]{\bm{#1}}
\newcommand{\bl}[1]{\mathbf{#1}}


\newcommand{\fixut}{\b{f}^i_{\b{xu}_t}}
\newcommand{\fict}{\b{f}^i_{\b{c}_t}}
\newcommand{\Fit}{\b{F}^i_t}
\newcommand{\fxut}{\b{f}_{\b{xu}_t}}
\newcommand{\fxutreg}{\b{f}^{'}_{\b{xu}_t}}
\newcommand{\fxutsub}[1]{\b{f}_{\b{xu}_t, #1}}

\newcommand{\fct}{\b{f}_{\b{c}_t}}
\newcommand{\Ft}{\b{F}_t}

\newcommand{\dkl}{D_{\mathrm{KL}}}
\newcommand{\Dt}{\mathcal{D}_t}

\renewcommand{\xi}{\b{x}_{i}}
\newcommand{\yi}{\b{y}_{i}}

\newcommand{\kt}{\b{k}_t}
\newcommand{\Kt}{\b{K}_t}

\newcommand{\xt}{\b{x}_{t}}
\newcommand{\hxt}{\b{\hat{x}}_t}
\newcommand{\xT}{\b{x}_{T}}
\newcommand{\xtt}{\b{x}_{t + 1}}
\newcommand{\ut}{\b{u}_{t}}
\newcommand{\hut}{\b{\hat{u}}_t}
\newcommand{\uT}{\b{u}_{T}}
\newcommand{\utt}{\b{u}_{t + 1}}

\newcommand{\ph}{{\phi_h}}
\newcommand{\pk}{{\phi_k}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{remark}{Remark}

% \setlength{\abovedisplayskip}{0pt}
% \setlength{\belowdisplayskip}{0pt}
% \setlength{\belowcaptionskip}{-5pt}

% Load the cleveref last - see: https://tex.stackexchange.com/a/148701.
\usepackage[noabbrev, capitalise]{cleveref}

\IEEEoverridecommandlockouts                              % This command is only needed if
                                                          % you want to use the \thanks command

% \overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed

\renewcommand{\baselinestretch}{.97}

% \markboth{IEEE Robotics and Automation Letters. Preprint Version. Accepted July, 2018}
% {Viereck \MakeLowercase{\textit{et al.}}: Learning a Structured Neural Network Policy for a Hopping Task}
%
% \title{
% Learning a Structured Neural Network Policy for a Hopping Task
% }

\title{
\LARGE\bf Learning a Structured Neural Network Policy for a Hopping Task
}


% \author{Julian Viereck

\author{Julian Viereck$^{1, 2}$, Jules Kozolinsky$^{3}$, Alexander Herzog$^{4}$, Ludovic Righetti$^{1, 2}$
\thanks{This research was supported by New York University, the Max-Planck
Society, the Max Planck ETH center for learning systems and the European Union’s Horizon 2020 research and innovation
programme (grant agreement No 780684 and European Research Council's
grant No 637935).}% <-this % stops a space
\thanks{$^{1}$Tandon School of Engineering, New York University, USA}%
\thanks{$^{2}$Max Planck Institute for Intelligent Systems,
  Germany {\tt\footnotesize firstname.lastname@tuebingen.mpg.de}}%
\thanks{$^{3}$Ecole Normale Supérieure
  Paris-Saclay
  France {\tt\footnotesize jules.kozolinsky@ens-cachan.fr}}%
\thanks{$^{4}$X, Mountain View, California, USA {\tt\footnotesize alexherzog@x.team}}%
% \thanks{Digital Object Identifier (DOI): see top of this page.}%
}

\begin{document}

\maketitle
% \thispagestyle{empty}
% \pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

In this work we present a method for learning a reactive policy for a simple dynamic locomotion task involving hard impact and switching contacts where we assume the contact location and contact timing to be unknown. To learn such a policy, we use optimal control to optimize a local controller for a fixed environment and contacts. We learn the contact-rich dynamics for our underactuated systems along these trajectories in a sample efficient manner. We use the optimized policies to learn the reactive policy in form of a neural network. Using a new neural network architecture, we are able to preserve more information from the local policy and make its output interpretable in the sense that its output in terms of desired trajectories, feedforward commands and gains can be interpreted. Extensive simulations demonstrate the robustness of the approach to changing environments, outperforming a model-free gradient policy based methods on the same tasks in simulation. Finally, we show that the learned policy can be robustly transferred on a real robot.

%\jviereck{Make point about interpretability more clear.}
%\ludo{done}

% In this work we present a method to learn a policy capable of performing a dynamic task involving hard contact switches where the contact timing is unknown. Such a scenario arrises when a robot locomotes and the ground is changing. We tackle this task by using optimal control techniques to first find solutions a long a trjaectory. We present our way to learn the contact rich dynamics for an underactuated systems along these trajectories in a sample efficient manner. We use the information from the feedback controller to learn a network policy. This policy is capable of dealing with unseen scenarios. We propose a new network architecture, which increases the interpretability of the network output. We compare our results to a state of the art model-free gradient policy based method. We verify our method on various simulated tasks and also on real hardware.

% In this work, we attempt to learn a neural network policy for dynamic, underactuated locomotion tasks. Learning a policy for such a task is non trivial due to the dynamic, fast changing, non linear and contact rich dynamics of this task. We use existing trajectory optimization techniques to optimize a set of policies. For this, we present a method that allows to learn contact rich dynamics for underactuated systems in a sample efficient manner. Using a new kind of neural network architecture, we are able to preserve more control structure and information from the optimized policies. This way, the network output is more interpretable. We analyze the quality of the learned dynamics and the robustness as well as generalization of the learned network policy on a set of tasks for a simulated hopping task. We also inspect if the network has learned reasonable control structures from the optimized policies.

\end{abstract}

% \begin{IEEEkeywords}
% Model Learning for Control, Legged Robots
% \end{IEEEkeywords}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%
%\IEEEPARstart{F}{or}
For robots to be fully autonomous it is important that control policies deal with fast changes in the environment. This is particularly true for locomotion tasks involving dynamics and unknown contact situations.
%
Over the past years, trajectory optimization methods have been successfully applied to the control of complex locomotion tasks \cite{PosaT12,tassa2014control,Herzog-2016b,mastalli2016hierarchical}. These methods tend to require significant computation time and usually assume a static environment. A way to run the computation in real time and react to changes in the environment is by using model predictive control \cite{erez2013integrated,koenemann2015whole}. Lately, these techniques were verified on real hardware and have been proven robust for unspecified contact timing and location \cite{neunert2017whole}. However, they are still to slow to handle an unexpected disturbance at contact during fast motions such as jumping. Moreover, contact switching is known to be an issue for most trajectory optimization methods~\cite{posa2016optimization}.
\begin{figure}[th]
    \centering
%    \begin{subfigure}[b]{1.0\linewidth}
       \includegraphics[width=0.88\linewidth]{figures/jump_trajectory_single_column.pdf}
%    \end{subfigure}
    \caption{One jump sequence on the real robot running the feedback network policy (120ms between each frame).}
    \label{fig:fig:jump_traj_single}
    \vspace{-0.5cm}
\end{figure}

To alleviate the computational demand of online optimization methods, approaches that directly learn policies have been investigated.
For example, trajectories and neural networks are optimized together in~\cite{mordatch2014combining}. In Guided Policy Search (GPS)~\cite{levine2013guided}, trajectories are optimized using iterative Linear Quadratic Regulator (iLQR)~\cite{li2004iterative} with an augmented cost to enforce the neural network and trajectory optimization to converge to similar results.
The advantage of such methods is that it is not necessary to optimize online new control laws as the environment changes as long as the learned policy is able to generalize to the new environment.
However, to our best knowledge, these approaches have never been demonstrated on a real robot with under-actuated dynamics for unstable tasks with non-negligible impact dynamics.

Neural network policies have also been successfully trained using "model-free" reinforcement learning. While there has been significant progress in playing games~\cite{mnih2015human,silver2016mastering,silver2017mastering} and for the simulation of animated characters~\cite{heess2017emergence}, the required amount of samples to learn the policies is prohibitively high and it remains to be shown that such policies can be transferred on real hardware.

The advantage of learning local dynamic models to compute locally optimal control laws has been demonstrated a long time ago in~\cite{schaal94}. More recently, this has been exploited further in the context of the GPS framework~\cite{levine2015learning} and in~\cite{meier16} for inverse dynamics learning.
However, as we discuss in this paper, such approaches lead to ill-conditioned dynamics in the case of under-actuated systems and limiting their use to compute locally optimal control laws.
Learned dynamics models allows us to use sampling-based approaches with efficient optimal control methods such as iLQR when using a simulator. This proves useful when dealing with complex, discontinuous dynamics for which computing the gradient from the dynamics model can be a numerically expensive and ill-conditioned process.

In this paper, we aim to learn a reactive feedback policy
for tasks involving fast contact dynamics and under-actuation, that remains robust to changes in the environment and
in particular to contact changes.
In particular, we show 1) how local dynamic models can be learned for systems with under-actuated dynamics, 2) how iterative LQR with an adaptive receding horizon can be leveraged to learn unstable tasks and 3) how we can learn a policy neural network that is amenable to analysis by preserving the feedback/feedforward control structure. We call the last point interpretability of the network policy which is interesting to us as it opens a path to use optimal control techniques to analyse the network's output and thereby bridge the gap between classical control and (modern) learning techniques.
We conduct extensive simulation and real robot experiments on a single-legged robot performing a dynamic hopping task which exhibits under-actuated and discontinuous dynamics.
We show that our approach can generalize to unseen terrains. We compare the learned policy with more traditional torque output networks and show that it retains similar performance while allowing for an easier to analyze policy. We also show that it tends to perform better than
a model-free policy gradient reinforcement learning algorithm with significantly smaller sampling complexity in simulation. Finally, we demonstrate the capabilities of the trained policies on a real robot.
%
%
%
%To achieve high sampling efficiency, we learn local dynamics models. We demonstrate our approach on a real single-legged robot performing a dynamic hopping task. Our approach first optimizes policies along a trajectory and then learns a trajectory-free feedback policy using a neural network.
%%
%
%%When using neural networks as policies the networks often output the desired command only. Here we are interested in using different network architectures, exploring the opportunity to learn control structures that are more amenable to explcit analysis. In particular, we explore if it is possible to preserve more information from the initial trajectory optimization.


%\jviereck{Assert the following outline overview is still valid with the latest edits.}

%This paper is structured as follows: We describe our way to learn a dynamics model in~\cref{sec:dyn_learning}. Next, we outline the way we optimize trajectory centric feedback policies in~\cref{sec:ilq_policy_optimization}. We call these policies \italic{iLQR policies}.  In~\cref{sec:reactive_neural_network_policy}, we describe how we train our reactive neural networks and a model-free reinforcement learning base method. Afterwards, we present our performed experiments~(\cref{sec:experiments}) and results~(\cref{sec:results}), before closing with conclusion and future work~(\cref{sec:conclusion_and_future_work}).
%
%% Trajectory optimization and optimal control techniques have proved very successful over the past few years
%% to compute locally optimal policies for complex locomotion tasks \cite{PosaT12,tassa2014control,Herzog-2016b}. These methods are very versatile and can a priori generalize to arbitrary environments as a new optimization problem is solved all the time. However, these solutions are usually computationally intensive. Model predictive control \jviereck{Add references to model predictive control papers here} can reduce the computational demands by optimizing a shorter trajectory online. However, all these approches assume the contact location to be known and tend to re-solve over and over very similar optimization problems when
%% the system is in a similar environment. A way to avoid this repeated computation and deal with unpredictable contact interaction is to learning a general policy from each local optimization using for example neural networks. In guided policy search (GPS) ~\cite{levine2013guided}, policies are optimized using the iterative Linear Quadratic Regulator (iLQR)~\cite{li2004iterative} algorithm jointly with learning a neural network. Similar, the authors of~\cite{mordatch2014combining} also optimize trajectories and network policies together.
%% A very reactive controller is then learned by mapping states directly to actions.
%%
%% However, an algorithm that relies on a dynamics model needs to be accurate for very dynamic tasks such as jumping and it is then desirable to learn this model directly from data.
%% This is particularly true for tasks involving hard contacts where physical models and identification of the contact modes is very difficult.
%% In \cite{schaal94}, locally linear models are learned and used together with linear optimal control approaches. A similar idea is used in GPS ~\cite{levine2015learning} to learn the local linear models used in iLQR.
%% Local models able to handle non-stationary dynamics have also been recently explored in \cite{meier16} for inverse dynamics control.
%%
%% The combination of local dynamics learning, trajectory optimization and function approximation of the global policy is therefore a very promising approach to learn complex tasks from very little prior assumptions and potentially generalize beyond the locally optimized policies. Examples of such approaches have been successfully applied to manipulation tasks \cite{levine2015learning}.
%% However, it is not clear if such an approach could be directly applied to problems with
%% under-actuated, discontinuous and fast dynamics which is inherent to all locomotion tasks.
%% Indeed, learning local models can be problematic for dynamic phases that are uncontrollable and local trajectory optimization
%% can be very sensitive to discontinuous dynamics, as mentioned for examples in ~\cite{mordatch2015interactive}.
%% Moreover, a neural network that outputs directly a control torque is very difficult to analyze and looses
%% the semantic of the control structure.
%% Therefore, it might be very hard to reuse what has been learned for different tasks and to provide more formal guarantees on the control output.
%%
%% In this paper, we explore this paradigm (learning local models, local optimization and learning a more global policy) for dynamic tasks with under-actuated discontinuous dynamics.
%% In contrast to approaches such as GPS, we optimize the trajectory centric policies and then train the neural network independently. Training the network independently is simpler and we are able to achieve good results. In addition, this gives us more flexibility for learning the information from the trajectory policies in the neural network.
%% We introduce a new neural network architecture that allows to interpret the networks output in a more structured way and resembles more traditional control structures including a desired trajectory, feedback gains and feed-forward torques. We call this kind of architecture \italic{feedback neural network policy}.
%% We also adapt previous methods to learn local models to better handle uncontrollable dynamics.
%%
%% We simulate a hopper robot, which exhibits under-actuated and discontinuous dynamics while at the same time is simple enough for a more systematic evaluation of the resulting policies.
%% In particular, extensive simulation experiments show that the learned policy achieves similar performance than the original iLQR policy but can also generalize to unseen terrains and react to fast changes such as unseen disturbances. We analyze the learned policy and compare its performance with a more traditional torque output network as well as a model-free, reinforcement learning trained policy gradient method. Finally, we demonstrate the capabilities of the trained policies by controlling a real robot platform.
%
%
%% In our work, we optimize the trajectory centric policies and then train the neural network independently. This gives us more flexibility for learning the information from the trajectory policies in the neural network. In~\cite{mordatch2015interactive}, the authors use Contact-Invariant Optimization (CIO) for optimizing the trajectories and then compute reasonable feedback gains afterwards. Here, we use iLQR to optimize trajectories and get feedback matrixes right away. As described in~\cite{mordatch2015interactive}, using iLQR to optimize trajectories with hard, switching contacts is troublesome. We describe our solution to this issue in the following.
%
%%
%%Neural networks have been successfully used to learn policies in reinforcement learning
%%~\cite{mnih2013playing,levine2016end,lillicrap2015continuous}
%
%%Beside the work listed above, another line of work optimizes the parameters of a network policy directly (for instance ~\cite{schulman2015trust},~\cite{theodorou2010reinforcement}) or learn to approximate a value and Q function (for instance as in~\cite{lillicrap2015continuous}). These methods perform well on learning a mapping between states to actions. However, the final optimized network policies are hard to interpret, which we try to improve on in this work.
%
%%\jviereck{Not sure the research questions below should be prompted more explicit in the introduction or the current status is fine.}
%
%%\begin{itemize}
%%  \item Does introducing the dynamics prior help to reduce the number of required rollouts to learn the dynamics?
%%  \item Are the trained neural networks able to achieve similar performance than the iLQR policies for scenarios the iLQR policies were optimized for?
%%  \item Do the reactive neural network policies generalize better to unseen scenarios than the trajectory centric iLQR policies?
%%  \item Are the control structures learned by the feedback network policy meaningful?
%%\end{itemize}
%
%% \begin{figure*}[t]
%%     \centering
%%     % \begin{subfigure}[b]{1.0\linewidth}
%%         \includegraphics[width=1.0\linewidth]{figures/overview.pdf}
%%     % \end{subfigure}
%%     \caption{Optimization overview. In the first part, iLQR policies are optimized by sampling rollouts from the current policy, learning a local dynamics model and then optimizing the poilcy using iLQR. For two different initial starting positions a policy is optimized each. As a second step, training data is generated from the two policies. Finally, the training data is used to train a neural network. Here, we show the architecture of the feedback neural network policy.}
%%     \label{fig:overview}
%% \end{figure*}
%
%% \begin{figure*}[t]
%%     \centering
%%     % \begin{subfigure}[b]{1.0\linewidth}
%%         \includegraphics[width=1.0\linewidth]{figures/jump_trajectory.pdf}
%%     % \end{subfigure}
%%     \caption{One jump sequence on the real robot running the feedback network policy. The entire sequence is \SI{0.6}{\second} long.}
%%     \label{fig:jump_traj}
%% \end{figure*}
%%
\section{Dynamics Learning}
\label{sec:dyn_learning}
%
% Our goal is to learn a policy that can solve a simple locomotion task under hard, switching contacts in changing environments.
%
\subsection{Basic notation}
%
We denote the measured states of a robot at time $t$ by $\xt$. The actions sent to the robot are denoted by $\ut$. We work in a time discretized system. We write the cost at time $t$ as $\b{\ell}(\xt, \ut)$. For a trajectory $\b{\tau} = \{\b{x}_1, \b{u}_1, ..., \b{x}_T, \b{u}_T\}$ the trajectory cost is given by the sum over the costs at every timestep. We use stochastic control policies, which we model as Gaussian distributions $p(\ut|\xt) = \mathcal{N}(\b{\mu}_t, \b{\Sigma}_t)$. For modeling the dynamics, we assume a Gaussian model, where the mean is an affine function of the robot's current state and action:
%
\begin{align}
\label{eq:def_dynamics}
p(\xtt | \xt, \ut) = \mathcal{N}\left(\fxut \begin{bmatrix} \xt \\ \ut \end{bmatrix} + \fct, \Ft \right),
\end{align}
%
where $\Ft$ and $\fxut$ are matrices and $\fct$ a constant offset.
%
\subsection{Learning the dynamics model}
The first stage of the algorithm is to learn local dynamic models with hard contact switches. These models are
then used with an iterative LQR algorithm to obtain locally optimal policies.
%
Such local models were successfully learned for fully actuated robots to achieve tasks that did not involve
hard impact dynamics (e.g. \cite{levine2015learning}) by fitting affine dynamics models along recorded trajectories.
%In~\cite{levine2015learning}, the authors describe a sample efficient way to estimate the parameters of an affine dynamics model by fitting the parameters to recorded trajectories.
A dynamics model along a trajectory is learned by running a stochastic policy N times. Let $\b{s}^i_t$ be the transition at time $t$ for the $i$-th rollout, with $\b{s}^i_t=(\xt, \ut, \xtt)$. At every timestep, a Gaussian distribution is fitted assuming the transition model $p(\xt, \ut, \xtt) = \mathcal{N}(\b{\mu}_t, \b{\Sigma}_t)$. Computing the conditional probability distribution $p(\xtt | \xt, \ut)$ in closed form~~\cite[Chapter~2.3]{Bishop:2006:PRM:1162264} and comparing terms to Eq. \eqref{eq:def_dynamics}, one arrives at
%
\begin{align}
\fxut   &= \b{\Sigma}_{\xtt, \b{xu}_t} \b{\Sigma}^{-1}_{\b{xu}_{t}, \b{xu}_{t}} \label{eq:dyn_fxut}, \\
\fct    &= \b{\mu}_{\xtt} - \fxut \b{\mu}_{\b{xu}_t} \label{eq:dyn_fct}, \\
\Ft       &= \label{eq:prob_dyn_covar}
        \b{\Sigma}_{{\xtt},{\xtt}} - \b{\Sigma}_{{\xtt}, \b{xu}_{t}} \b{\Sigma}_{\b{xu}_{t}, \b{xu}_{t}}^{-1} \b{\Sigma_{\b{xu}_{t},\xtt}},
\end{align}
%
where the indices correspond to accessing the subparts of the vectors and matrices. To reduce the required sample count N, in~\cite{levine2015learning} a Gaussian Mixture Model (GMM) is fitted to all transitions $s^i_t$ along the N trajectories and also includes trajectories from previous iterations. The GMM is then used as a prior when estimating $p(\xt, \ut, \xtt)$.

Learning a dynamics model this way is very attractive as it is sample efficient and computationally simple. However, in the case
of underactuated dynamics, which correspond to the class of problems studied in this paper, such a
local regression approach does not work properly.
In our experiments, the dynamics model fitting always led to physically implausible,
ill-conditioned, $\fxut$ matrices in Eq. \eqref{eq:dyn_fxut},  independently of the number
of samples.
These issues stem from the fact that the floating-base position and velocity are not directly controllable when the robot is in free air. Current state of the art approaches \cite{levine2015learning} to learn local dynamic models are ill posed for these type of systems and lead to numerical issues that prevent their use for local policy optimization.
%Note that it is a general issue for underactuated system
%Note that this problem is not specific to our setup but general for any underactuated system as often studied in locomotion.

We address this issue by regularizing the dynamics matrix: we introduce a prior on the $\fxut$ dynamics matrix that assumes the change in state is close to identity.
%This corresponds to a system that is time-invariant.
To introduce a zero-mean prior, we define a shifted dynamics matrix $\fxutreg = \fxut - (\mathbb{1}~\huge0)$ and then assume a prior as $\fxutreg \sim \mathcal{N}(0, \lambda^{-1} \mathbb{1})$. Similarly, we change the transition points to $\b{s}^{i'}_t=(\xt, \ut, \xtt - \xt)$. Using results from~\cite[Section 3.4]{williams1998prediction}, the dynamics parameters are now estimated by:
%
\begin{align}
\b{f}_{\b{xu}_t, \textrm{ridge}}  =& \b{\Sigma}_{\xtt, \b{xu}_t} (\lambda \mathbb{1} + \b{\Sigma}_{\b{xu}_{t}, \b{xu}_{t}})^{-1} \label{eq:dyn_fxut_ridge} + (\mathbb{1}~\huge0), \\
\b{F}_{t, \textrm{ridge}}     = &\label{eq:prob_dyn_covar_ridge}
        \b{\Sigma}_{{\xtt},{\xtt}} - \{\b{\Sigma}_{{\xtt}, \b{xu}_{t}}  \\& \nonumber
        \qquad(\lambda \mathbb{1} + \b{\Sigma}_{\b{xu}_{t}, \b{xu}_{t}})^{-1} \b{\Sigma}_{\b{xu}_{t},\xtt} \}.
\end{align}
In our experiments, the regularization term remains small ($\lambda \sim 10^{-2}$) and yet is
sufficient to have stable learning.
%
\subsection{Handling torque limits}
%
We consider very dynamic tasks where output torques are often close to the robot's actuation limits.
This can cause issues when learning the dynamics as we might attempt to sample torque commands from $p(\ut|\xt)$ in~Eq. \eqref{eq:ilqr_policy} exceeding these limits. This can create a bias in the estimated dynamics. We eliminate this issue by using lower torque limit: we allow a maximum torque 90\% of the maximum torque. The remaining 10\% are then available for sampling when learning the dynamics.
%
\begingroup
\titlespacing{\section}{0pt}{1.0cm}{0.2\baselineskip}
\section{iLQR policy optimization}
\endgroup
\label{sec:ilq_policy_optimization}
%
%
To optimize locally optimal feedback policies around a trajectory, we follow the work by~\cite{levine2015learning}
and use iterative LQR, as introduced in \cite{li2004iterative,Sideris:2005hp}.
%
% TRIM:
% The feedback policies are optimized wrt. the trajectory cost using iLQR with a constraint on the update step. In particular, given a trajectory and an affine dynamics model as described above, the iLQR algorithm optimizes the current control policy by first computing the optimal update and gains. For this, the iLQR algorithm
%
%
iLQR is a single shooting method that computes approximations to the derivatives of the value~$\b{V}$ and $\b{Q}$ function to solve the backward Riccati equations and use the full model dynamics during the shooting phase. We denote with $\b{A}_{\b{xu}t}$ the Jacobian of $\b{A}$ and with $\b{A}_{\b{xu},\b{xu}t}$ its Hessian at time $t$ with respect to the concatenated state and action vector $\b{xu}$. The backwards pass recursion to compute the value and Q functions reads:

\begin{align}
\b{Q}_{\b{xu}t} &= \label{eq:q_xut}
  \b{\ell}_{\b{xu}t} + \fxut^\mathsf{T} \b{V}_{\b{x}{t+1}} +
  \fxut^\mathsf{T} \b{V}_{\b{x}, \b{x}{t+1}} \fct \\
\b{Q}_{\b{xu}, \b{xu}t} &= \label{eq:q_xuxut}
  \b{\ell}_{\b{xu}, \b{xu}t} +
  \fxut^\mathsf{T} \b{V}_{\b{x},\b{x}{t+1}}\fxut \\
\b{V}_{\b{x}t} &= \label{eq:v_xut}
  \b{Q}_{\b{x}t} - \b{Q}^\mathsf{T}_{\b{u},\b{x}t} \b{Q}^{-1}_{\b{u},\b{u}t} \b{Q}_{\b{u}t} \\
\b{V}_{\b{x},\b{x}t} &= \label{eq:v_xuxut}
  \b{Q}_{\b{x},\b{x}t} - \b{Q}^\mathsf{T}_{\b{u},\b{x}t} \b{Q}^{-1}_{\b{u},\b{u}t} \b{Q}_{\b{u},\b{x}t},
\end{align}

with the derivatives of $\b{V}$ for $t > T$ set to zero. In our problems, it is important to take into account the actuation
limits of the robot and we use the method described in ~\cite{tassa2014control} to compute the optimal control command.
% TRIM:
% The method solves the following constrained optimization problem:
%
% \begin{align}
%  \label{eq:traj_cost_min_kl_box}
% {\delta \b{u}_t}^* =& \argmin_{\delta \b{u}_t}
%   \b{Q}(\xt + \delta \b{x}_t, \ut + \delta \b{u}_t) \\
%   & \textrm{~s.t.~} (\ut + \delta \b{u}_t) \in [-b, b]^{\dim \ut} \nonumber
%   % BEFORE: eq:traj_cost_min_kl
% \end{align}
%
% where $b$ denotes the bounds on the torques and ${\delta \b{u}_t}^*$ is the optimal change in torque to minimize the loss along an optimized trajectory.
%
After the backwards pass, a new policy is synthesized in the shooting phase as
%
\begin{align}
  \label{eq:ilqr_policy}
  p(\ut|\xt) &= \mathcal{N}(\b{k}_t + \b{K}_t (\xt - \hat{\b{x}}_t), \b{\Sigma}_t)\\
  \b{k}_t &= \hat{\ut} + \alpha \delta \ut,
\end{align}
%
where $\hat{\ut}$ is the action along the provided trajectory, $\delta \ut$ is the computed change of action to minimize the local cost and $\hat{\b{x}}_t$ is the new desired trajectory. We denote the new feedforward action of this policy by $\b{k}_t$. Following~\cite{tassa2014control}, we zero entries in the feedback matrix $\b{K}_t$ that correspond to actions exceeding torque limits. The parameter $\alpha \in [0, 1]$ is used for linesearch with the intention to find the update step size corresponding to the maximum cost reduction.
%
\subsection{Parameter choices for unstable tasks}
% TRIM
% Using the parameter $\alpha \in [0, 1]$, one can perform a line search. This is necessary as using a full update of the torques with $\alpha=1$ might lead to an trajectory, for which the linear dynamics assumptions do not hold anymore. As the dynamics are not necessary valid anymore, the resulting trajectory might actually have a larger trajectory cost.
Instead of the linesearch, we use a trust region for the control update, as in~\cite{levine2015learning}.
We set $\alpha = 1$ and enforce a constraint between the new $p_\text{new}$ and previous $p_\text{old}$ optimized policy. The constraint is given by $\dkl(p_\text{old}(\ut|\xt)|p_\text{new}(\ut|\xt)) \leq \epsilon$, where $\dkl(p|q)$ denotes the Kullback-Leibler-Divergence. To enforce the constraint, the initial loss is augmented to be
%
\begin{align}
\tilde{\ell}(\xt, \ut) = \frac{1}{\eta}\ell(\xt, \ut) - \log p_\text{old}(\ut|\xt).
\end{align}
%
We perform a line search on $\eta$ in log space until the KL constraint is satisfied.
%
However, we made two important modifications compared to previous work.
First, we set $\epsilon = 0.5$ in our experiments. While it is also possible to use a heuristic
approach to adapt it during the optimization as in~\cite{levine2016end}, we noticed that
using a fixed $\epsilon$ turned out to give better
convergence results than using the heuristic with the benefit of a simpler algorithm.

Second, we restrict the policy's covariance to values typically around $\b{\Sigma}_t=0.01\mathds{1}$, instead of
using $\b{\Sigma}_t=\b{Q}^{-1}_{\b{u},\b{u}t},$ for the feedback policy, as was done in previous work \cite{levine2015learning}.
Indeed, we noticed that such a choice does not work properly in the case of impacts. In our experiments, we observed large eigenvalues of $\b{\Sigma}_t$ which  in turn created unstable robot behavior.
This effect is especially important when the robot is about to touch the ground. In this case, large uncertainties in the motion causes the robot to fall easily, preventing the optimization to find any stable gait.
%Therefore, we also restrict the policy's covariance to values typically around $\b{\Sigma}_t=0.01\mathds{1}$.
%
%Moreover, in our experiments, when setting
%%Therefore, we use a fixed value for $\epsilon = 0.5$.
%$\b{\Sigma}_t=\b{Q}^{-1}_{\b{u},\b{u}t},$ for the feedback policy as in~\cite{levine2015learning}, we observed large eigenvalues of $\b{\Sigma}_t$ which  in turn created unstable robot behavior.

%Moreover, in our experiments, when setting
%%Therefore, we use a fixed value for $\epsilon = 0.5$.
%$\b{\Sigma}_t=\b{Q}^{-1}_{\b{u},\b{u}t},$ for the feedback policy as in~\cite{levine2015learning}, we observed large eigenvalues of $\b{\Sigma}_t$ which  in turn created unstable robot behavior.
%This effect was especially important when the robot is about to touch the ground. In this case, too large uncertainties in the motion causes the robot to easily fall to the ground, preventing the optimization to converge. Therefore, we restricting the policy's covariance to values typically around $\b{\Sigma}_t=0.01\mathds{1}$.
%
%\subsection{Learning hopping motion using iLQR}
\subsection{Adaptive receding horizon length}
%
% Finding an initial good policy for contact rich dynamical systems can be cumbersome and limiting. Therefore, we start the iLQR optimization procedure from an randomly initialized policy.
%
We start the iLQR optimization procedure from a randomly initialized policy. As the task we study is intrinsically unstable, the initial policy is likely to make the robot fall. This can be a problem since the iLQR iteration is likely to not converge.
To tackle this issue, we propose to automatically increase the receding horizon length over the iterations instead of optimizing a task with a fixed horizon length.
It allows us to find optimal policies quickly, as the backward Riccati pass is able to better propagate the value function over
small horizon at the beginning when the trajectory is unstable.
Once the hopping motion is stable enough, the trajectory gets extended at a maximum of 50 ms per optimization iteration. In this context, stable enough means the robots base is high enough above the ground. The gradual extension of the optimization horizon allowed the algorithm to quickly find longer jumping trajectories.
%
% {\begingroup
% \titlespacing*{\subsection}{0pt}{1.0\baselineskip}{0.2\baselineskip}
\subsection{Cost function design}
% \endgroup}
Our cost function is constituted for two terms that describe
the different behaviors expected during different dynamic modes when performing a hopping motion.
We use the term $\ell_\textrm{jump}$ for situations when the robot is close to the ground to makes the robot jump. When the robot is in the air, we defined $\ell_\textrm{land}$ to put the leg in a useful position:
%
\begin{align}
        % \ell(h, \dot{h}, \phi_h, \dot{\phi_h}, \phi_k, \dot{\phi_k}, c, u_h, u_k), \\
    \label{eq:cost_hopper_jump}
    \ell_\textrm{jump}(\xt, \ut) = &
          0.2\, \left(\ph - 0.0\right)^2 + 0.2\, \left(\pk - 0.0\right)^2 +\\&  0.001\, \left({u_h}^2 + {u_k}^2\right) \nonumber \\
    \ell_\textrm{land}(\xt, \ut) = &
        0.2\,\left(\ph - 1.3\right)^2 + 0.2\, \left(\pk - 2.3\right)^2 + \\ & 0.001\, \left({u_h}^2 + {u_k}^2\right), \nonumber
\end{align}
%
where $\phi_h$ and $\phi_k$ denote the hip and knee angle and $u_h$ as well as $u_k$ the action applied at each of these joints (see Eq.\eqref{fig:hopper_setup}). We also tried to optimize a cost that reflects the desired goal more directly (like maximizing the upwards hip velocity). However, the above costs led to better and more reproducible results, even when using
ground truth dynamics.
%
During the backward pass of iLQR, the value and Q function derivatives are reset whenever the cost switches
to prevent the propagation of the previous cost in the policy.
%In addition, changing the costs and starting an independent iLQR optimization when the contact switches helps iLQR as it assumes smooth dynamics.
%
%
\section{Reactive neural network policy}
\label{sec:reactive_neural_network_policy}
%
The optimized iLQR policies are indexed by time, which is an issue in the case of contact dynamics as time
indexing assumes fixed timing for contact switching. Thus, if contact happens earlier or later than the policy prediction, it is likely lead to a fall.
It is therefore desirable to learn a feedback policy that is independent of time.
%
\begin{figure}
  \centering
    \vspace{0.2cm}
    \includegraphics[scale=0.65]{figures/experimental_setup.pdf}
    % \includegraphics[scale=0.1]{pictures/hopper_real_hardware.png}\qquad
  % {
    \setlength{\tabcolsep}{0.2em} % for the horizontal padding
    \renewcommand{\arraystretch}{1.2}% for the vertical padding
    \small
    \begin{tabular}[b]{l|l|l}
        Property & Simulation & Real\\ \hline
        $m_\text{Hip}$ & \SI{0.15}{\kilogram} & \SI{0.265}{\kilogram}\\ \hline
        $m_\text{Upper leg}$ & \SI{0.1}{\kilogram} & \SI{0.145}{\kilogram}\\ \hline
        $m_\text{Shank}$ & \SI{0.01}{\kilogram} & \SI{0.025}{\kilogram}\\ \hline
        \makecell[tl]{Leg length} & \SI{0.2}{\meter} & \SI{0.2}{\meter} \\ \hline
        \makecell[tl]{Max torque} & \SI{1.3}{\newton\meter} & \SI{0.675}{\newton\meter} \\ \hline
        \makecell[tl]{Joint damping} & -0.05 $\dot{\phi}_{h, k}$ & unknown  \\
    \end{tabular}
  % }
%
  \caption{Experimental platform: a single leg hopper with base constrained to move along the vertical axis.}
  \label{fig:hopper_setup}
  \vspace{-0.5cm}
\end{figure}
%
\subsection{Feedback network and torque network policies}
We train two different kinds of neural networks, a \italic{feedback neural network} and a \italic{torque neural network}, see Figure \ref{fig:network_overview}.
%
The torque neural network policy directly maps the input state $\b{x}$ to the action mean $\b{g}^\text{torque}_\theta(\b{x})$. This architecture has the advantage to directly map inputs to torques and has been successfully used in several previous works~\cite{levine2015learning, levine2016end, schulman2015trust}. However, it is hard to understand what the actual policy does, as we do not have access to an informative policy structure.
On the other hand, the feedback neural network policy maps the input state $\b{x}$ to a feedforward $\b{k}_\theta(\b{x})$ vector, feedback matrix $\b{K}_\theta(\b{x})$ and nominal position $\hat{\b{x}}_\theta(\b{x})$. Similar to the iLQR policy, the final action mean is then computed as $\b{g}^\text{feedback}_\theta = \b{k}_\theta(\b{x}) + \b{K}_\theta(\b{x}) (\b{x} - \hat{\b{x}}_\theta(\b{x}))$. This network architecture allows us a direct interpretation of the network output in terms of feedforward and feedback control paths which have very different physical interpretations. We discuss such interpretations in the experimental section.

For both kinds of networks, an action is computed as $\b{u}=\mathcal{N}(\b{g_\theta},\b{\Sigma}_\theta)$ and we use ELUs as an activation function. We trained networks with different parameters (number of layers, number of hidden neurons). Eventually, we choose a configuration with small number of parameters and that performs in the top 10\% on the training data over all trained networks. We use three fully connected layers with 10 hidden neurons for the torque neural network.
In the feedback neural network, the input state $\b{x}$ is passed through three fully connected layers with 10 hidden neurons. The last neurons are fully connected and output feedforward torques, feedback gains and desired joint positions.
\begin{figure}[t]
  \centering
  \vspace{0.2cm}
  % \includegraphics[width=0.7\linewidth]{figures/network_architecutres.pdf}
  \includegraphics[width=.9\linewidth]{figures/network_architecutres_small.pdf}
  \caption{Neural network architectures used to learn policies.
  %: The feedback neural network policy outputs a desired position, feedforward torque and feedback matrix. The torque neural network policy outputs a feedforward torque directly.
  }
  \label{fig:network_overview}
  \vspace{-0.5cm}
\end{figure}
%
% We are interested to see if it is possible to utilize and keep more information from the iLQR policy when training a network policy.
% The resulting policy would then be easier to interpret and analyse with formal control theoretic methods. It might also be easier to re-use parts of the learned policy for different tasks (e.g. the generated desired trajectories).
% We therefore train a second kind of network policy that we called feedback network policy.
%
%We train a second kind of network policy that we call feedback network policy.
%

%We train a second kind of network policy that we call feedback network policy.
% {\begingroup
% \titlespacing*{\subsection}{0pt}{1.0\baselineskip}{0.2\baselineskip}
\subsection{Training procedure}
% \endgroup}
%
The network policies are trained with two iLQR policies for trajectories of length \SI{1}{\second} using learned dynamics and the method described in Section \ref{sec:ilq_policy_optimization}. For each optimization iteration, we sample five trajectories to estimate the dynamics and use $k=5$ Gaussian mixture components for the GMM prior. Each policy is optimized for 50 iLQR iterations. The iLQR policies have different initial positions given by
%
\begin{align}
x^1_0 & =  \{h, \dot{h}, \phi_h, \dot{\phi_h}, \phi_k, \dot{\phi_k}, c\}\\
  & = \{0.2, 0.0, 1.3, 0.0, 2.3, 0.0, 0.0\} \nonumber \\
x^2_0 & = \{0.3, 0.0, 1.0, 0.0, 1.55, 0.0, 0.0\}.
\end{align}
%
we generate the training data for the network policies using these policies. We increase the covariance of the policies in Eq. \eqref{eq:ilqr_policy} in the range of $\gamma \in [0., 0.30]$ as $\b{\Sigma}_t=\gamma \mathbb{1}$ in steps of $0.01$. For a set $\b{\Sigma}_t$, we sample five trajectories for each policy. It means that we need in total 250 episodes to compute an iLQR policy. Beside the trajectories, we store for each state $\b{x}^i_t$ the commanded action by the iLQR policy without noise $\b{g}^i_t$, as well as $\b{k}^i_t$, $\b{K}^i_t$ and $\hat{\b{x}}^i_t$.

With this training data of $M$ points $\b{x}^i_t$ and associated data, we optimize different losses for each kind of network in a supervised fashion. The losses are defined as:
%
\begin{align}
\label{eq:loss_nn_feedback}
\mathcal{L}^{\text{torque}}_\theta = \frac{1}{M} \sum^{M}_{i=1} &\|\b{g}^i_t - \b{g}^\text{torque}_\theta(\b{x})\|^2_2 \\
\mathcal{L}^{\text{feedback}}_\theta = \frac{1}{M} \sum^{M}_{i=1} \{&\|\b{k}^i_t - \b{k}^\text{fb}_\theta(\b{x})\|^2_2 + \|\b{K}^i_t - \b{K}^\text{fb}_\theta(\b{x})\|^2_2  \nonumber + \\
& \|\hat{\b{x}}^i_t - \hat{\b{x}}^\text{fb}_\theta(\b{x})\|^2_2 \} \nonumber,
\end{align}
%
where $\|\cdot\|_2$ denotes the Euclidean norm. We compute the norm for the feedback matrices by reshaping the matrices into a 1D vector. We optimize the losses using the Adam optimizer and stochastic sampled mini-batches of size 1024. We found the best learning rate for the torque network policy to be $0.0001$ and for the feedback network policy to be $0.001$. We optimized the networks for 20'000 batches each.
%
%
\section{Experimental setup}
\label{sec:experiments}
%
In this section we describe the experimental setup used in this contribution. We also describe the setup
for the Proximal Policy Optimization \cite{schulman2017proximal}
that we use as comparison with a state of the art model free reinforcement
learning algorithm. In this paper, we only
consider simulations similar to the real experimental platform.
%
%
%used in this contribution to analyze 1) the efficiency of our dynamics regularization to learn local models, 2) the performance of the learned policy and its generalization capabilities, 3) the structure of the feedback network policy, 4) performance on real hardware.
%We also compare the network policies trained from the iLQR policies with a state of the art reinforcement learning algorithm, the model-free Proximal Policy Optimization (PPO)~\cite{schulman2017proximal}.
%
%
%In the experiments, we are interested to see
%if using our way to regularize the dynamics helps when learning the dynamics, the learned network policies are similar robust to the iLQR policies and how they generalize. In addition, does the feedback network policy lean an meaningful control structure?
%
\subsection{Performance metric}
To evaluate the performance of a policy, we define the ``Positive Hip Velocity Squared'' (PHVS) performance metric. This metric is computed as the mean of the upwards hip velocity squared, defined as:
%
\begin{align}
  \label{eq:policy_perf_metric}
  P(\tau) = \frac{1}{T} \sum_{t=1}^{T} \max(0, \dot{h}_t)^2.
\end{align}
%
This metric favors policies that make the hopper robot take off from the ground with a high velocity. Comparing many sample trajectories, this captures the notion of a decent performing hopping motion quite well.
%
\subsection{Experimental platform}
\label{sec:exp_setup}
%
The experimental platform is a custom-designed hopper robot. \cref{fig:hopper_setup} gives
an overview of the platform and important parameters.
The robot has two joints actuated by torque controlled brushless motors (T-motor Antigravity 4004 KV300), which allows for fast yet compliant motions. Each joint has an Avago Encoder with 5000 counts per resolution. The robot's unactuated base is constrained to move only in the vertical direction. An OptoForce OMD-20-SE-40N force sensor is mounted on the foot.

The robot is simulated using the \italic{Rigid Body Dynamics Library}~\cite{RBDL} implementation
of the Articulated Body Algorithm. We use a total momentum preserving contact model following~\cite[Section 3.4]{westervelt2007feedback}.


%We choose this contact model as it gives hard, real world realistic contact switches \jviereck{Adress though there is an explanation for contact model, it is not clear. Please explain it more. According to contact model, simulation results can be changed.}.

\begin{remark}
This contact model has been extensively used in the locomotion literature.
It gives a realistic, hard contact with discontinuity at impact and does not require to tune any parameters. Note that since there is only one contact point, the model is equivalent to a complementary constraint contact model without sliding friction since there is not force allocation problem. Due to the contact model, the dynamics is not
differentiable at impact which can therefore be an issue for gradient-based optimization methods. As we show in the following, our sampling-based learning approach allows us to optimize the robot motion. While the contact model does not capture sliding friction effects,
it was sufficient to learn a control policy robust on a real robot facing such effects.
\end{remark}

The control policies run in realtime at \SI{100}{\hertz} both in simulation and on the real robot. Besides damping at the joints, we assume no other source of friction in the simulation.
The measured robot state is its base-height above the ground, the hip and knee angle as well as velocities of these quantities.
For all the experiments, we use a binary contact state $c$, which is set to 1 in case the contact forces exceed a cutoff value.
Overall, the 7 dimensional state space is defined as ${\b{x} = (h, \dot{h}, \phi_h, \dot{\phi_h}, \phi_k, \dot{\phi_k}, c)}$.
%
% {\begingroup
% \titlespacing*{\subsection}{0pt}{1.0\baselineskip}{0.2\baselineskip}
\subsection{Comparison with Proximal Policy Optimization}
% \endgroup}
We compare our results with a policy trained using Proximal Policy Optimization (PPO)~\cite{schulman2017proximal}. We call this method PPO Network Policy. PPO is a state of the art model-free, reinforcement learning based method. The policy network is made of two hidden layers with 16 neurons each and ELU activation and is trained on four cores in parallel for 21,000 episodes each. We adjust the learning rate to 0.003 and follow otherwise the PPO1 baselines~\cite{baselines} implementation and it takes approximately 5000 episodes for PPO to converge. We also considered using an architecture similar to the feedback network. However, in our experiments we were not able to optimize such a network using PPO. This might be as for the feedback network, the policy needs to learn the feedforward, feedback and desired position, which are intertwined for the final control. Learning these three quantities from the reward signal alone is a more complicated task then learning to predict the control directly.
%
The learning scenario is the same as for the iLQR optimization and the system is reset to the $x^1_0$ and $x^2_0$ start position in an alternating fashion. An episode is terminated when the hip is less than \SI{0.1}{\meter} from the ground or the maximum of $T=100$ timesteps is reached. While there is a wide variety of techniques (dynamic perturbations,
sensor noise, etc) to make the PPO policy more robust, we omitted these to make the training of the PPO closer to the one for the local policies and thereby the results of the final network policies more comparable. We were not able to use the same cost function, as when using the two terms
$\ell_\textrm{jump}$ and $\ell_\textrm{land}$ the PPO algorithm learns a policy that remains and optimizes the $\ell_\textrm{land}$ cost all the time.
This result can be explained as switching to the $\ell_\textrm{jump}$ cost increases the overall cost locally. Therefore, the PPO algorithm learns to not jump. In order to still allow comparison with our approach,
we use a reward $r_t$ at each timestep $t$ that motivates upwards motion:
%
\begin{align}
  r_t =& -(\ph - 1.3)^2 - (\pk - 2.3)^2 + \max(0, \dot{h}_t)^2. \nonumber
\end{align}
%
The first two terms of this reward have an analogous function as the terms in $\ell_\textrm{land}$ to put the leg into a reasonable position for landing. These two terms are set to zero if the hip velocity is larger than \SI{0.2}{\meter/\second}. The third term motivates the robot to jump. When the episode ends at timestep $t$ we emit the penalty $ -(T - t)$ to motivate the optimization in finding trajectories lasting for the full $T$ timesteps.

% {\begingroup
% \titlespacing*{\section}{0pt}{10pt}{2.0\baselineskip}
\section{Results}
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth,clip,trim={0 10 0 10}]{figures/dyn_learning.eps}
%
    \caption{Performance comparison of iLQR optimized policies with different numbers of sampled trajectories for dynamics learning. The plot shows the mean value and one standard deviation for 500 optimization runs in simulation. Performance with ground truth dynamics is shown as a reference.}
%
    \label{fig:result_dyn_learning}
    \vspace{-0.6cm}
\end{figure}
%
\begin{figure*}[h]
    \centering
%    \begin{subfigure}[b]{1.0\linewidth}
        \includegraphics[width=1.0\linewidth,clip,trim={0 15 0 15}]{figures/eval_joint_random.pdf}
%        \caption{Action noise}
%        \label{fig:result_action_noise}
%    \end{subfigure}
%    \begin{subfigure}[b]{1.0\linewidth}
%        \includegraphics[width=1.0\linewidth]{figures/height_noise.eps}
%        \caption{Height noise}
%        \label{fig:result_height_noise}
%    \end{subfigure}
    \caption{a) and b) Performance for various levels of action noise when starting from the trained initial positions $x_0^1$ and $x_0^2$ making 10 rollouts with the same policy. c) Average performance over 100 random initial configurations for increasing action noise levels. d) and e): Average performance of 100 random initial configurations for increasing floor height and various slopes. }
    \label{fig:result_noise}
    \vspace{-0.7cm}
\end{figure*}
% \endgroup}
\label{sec:results}
In this section, we present and discuss the experimental results. The goal of our experiments it to analyze each element of the proposed algorithm, in particular: 1) the efficiency of our dynamics regularization to learn local models, 2) the performance of the learned policy and its generalization capabilities, 3) the structure of the feedback network policy, 4) performance on real hardware. When possible, we also compare our results with the PPO Network Policy. The total runtime on a MacBook Pro with 2.9 Ghz Intel Core i7 for optimizing the local policies, learning the dynamics and training the torque and feedback network is 5 minutes. On the same system, the PPO reward converges after 5:40 minutes.
%
\subsection{Dynamics regularization}
We aim to analyze the effect of the dynamics regularization and in particular how it helps to reduce the numbers required trajectories to fit the dynamics model. For this, we optimize iLQR policies in simulation for a trajectory of \SI{0.5}{\second} over 30 optimization iterations from the $x^1_0$ initial position. We increase the number of sampled trajectories to estimate the dynamics from 1 to 10, always using a GMM prior with 5 mixture components. The sample trajectories are created by running the current iLQR policy with $\b{\Sigma}_t=0.01\mathds{1}$ in \cref{eq:ilqr_policy} from $x^1_0$ on the system. We compute statistics over 500 runs for each configuration on the PHVS metric.
%

Without the prior we were not able to fit a good dynamic model to drive the iLQR optimization even when using up to $1000$ samples. Therefore, \cref{fig:result_dyn_learning} shows results only with the dynamics regularization.

As one can see, the PHVS performance increases till 5 sampled trajectories per optimization iteration. Assuming the increase in the PHVS performance is due to better estimated dynamics for the optimization, this also implies the estimated dynamics quality improves up to 5 samples. The achieved optimization performance is quite close to the one when using the ground truth dynamics provided by the simulator. Increasing the number of samples further has no significant effect anymore. Note that until 3 samples, the standard deviation in the performance is quite high. This indicates that the resulting trajectories are quite different in terms of performance for a lower number of dynamics samples and the optimization results vary a lot.
%
{\setlength{\tabcolsep}{0.5em} % for the horizontal padding
\renewcommand{\arraystretch}{1.2}% for the vertical padding
\begin{table}[h]
%
\begin{center}
\begin{subtable}[t]{1.0\linewidth}
\begin{tabular}{|c c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{~} & \multicolumn{4}{c|}{Max slope} \\
\multicolumn{2}{c|}{~} & 15 deg & 30 deg & 45 deg & 60   deg \\
\hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{Max height}}&  25\% & 9.4 $\pm$ 2.0 s & 8.4 $\pm$ 2.7 s & 8.3 $\pm$ 2.7 s & 6.0 $\pm$ 3.8 s\\
\cline{2-6}
& 50\% & 9.6 $\pm$ 1.8 s & 9.4 $\pm$ 1.8 s & 8.2 $\pm$ 2.9 s & 7.5 $\pm$ 3.3 s\\
\cline{2-6}
& 75\% & 9.6 $\pm$ 0.2 s & 8.5 $\pm$ 3.2 s & 8.0 $\pm$ 5.8 s & 6.4 $\pm$ 9.7 s\\
\cline{2-6}
& 100\% & 9.6 $\pm$  1.4 s & 8.9 $\pm$  2.6 s & 8.2 $\pm$  2.8 s & 6.6 $\pm$  3.3 s\\
\hline
\end{tabular}
\caption{Results for feedback network policy.}\label{table:res_marathon_feedback}
\end{subtable}
%
\vspace*{0.2cm}
%
\begin{subtable}[t]{1.0\linewidth}
\begin{tabular}{|c c|c|c|c|c|}
\cline{3-6}
\multicolumn{2}{c|}{~} & \multicolumn{4}{c|}{Max slope} \\
\multicolumn{2}{c|}{~} & 15 deg & 30 deg & 45 deg & 60   deg \\
\hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{Max height}}&
25\% & 10.0 $\pm$ 0.0 s & 6.5 $\pm$ 3.4 s & 2.6 $\pm$ 2.3 s & 1.7 $\pm$ 1.4 s\\
\cline{2-6}
& 50\% & 10.0 $\pm$ 0.0 s & 6.5 $\pm$ 3.0 s & 3.0 $\pm$ 1.9 s & 1.8 $\pm$ 1.4 s\\
\cline{2-6}
& 75\% & 10.0 $\pm$ 0.0 s & 6.9 $\pm$ 3.0 s & 2.3 $\pm$ 1.6 s & 1.9 $\pm$ 1.7 s\\
\cline{2-6}
& 100\% & 10.0 $\pm$ 0.0 s & 6.3 $\pm$ 3.3 s & 2.1 $\pm$  1.8 s & 1.9 $\pm$  1.8 s\\
\hline
\end{tabular}
\caption{Results for PPO network policy.}\label{table:res_marathon_ppo}
\end{subtable}
%
\end{center}
\vspace{-0.5cm}
\caption{Termination time (mean and standard deviation) for a 10 seconds run with the feedback and PPO network policies over 50 runs. Max height as percentage of leg length.}
\label{table:res_marathon}
\vspace{-0.3cm}
\end{table}}
\vspace{-0.3cm}
\subsection{Comparison of policies on trained initial states}
\label{sec:result_sim_perf}
%
As a basic validation test for the learned neural network policies, we compare their performance to the iLQR policies in simulation. For this, we perform a rollout of the network policies from each of the iLQR policies' starting positions. To quantify the robustness of the policies, we set $\b{\Sigma}_t$ when sampling an action for the policies to $\b{\Sigma}_t=\beta \mathds{1}$. This has a similar effect as adding noise on the policy's action and we call this \italic{action noise}. We perform rollouts with $\beta~\in~[0.0, 0.35]$ and compute the PHVS metric. All rollouts are made for a trajectory of length \SI{1}{\second}.

The results are shown in the two left plots of \cref{fig:result_noise}. As we can see, the performance of all the policies declines as the action noise increases. In particular, the torque and feedback network policies have similar performance as the iLQR policies. For the second initial position, the feedback network policy performs significantly better than the iLQR policies. While not as good as the feedback network policy, the torque network policy also performs better than the iLQR policies.
This is interesting, as both network policies are trained on the data from the iLQR policies but they lead to higher performance, especially as noise increase. Overall, the PPO Network Policy behaves similarly to the other network policies, though usually slightly worse.

The feedback and torque neural network policies can reproduce at least
the same performance as the iLQR policies and in certain cases they outperform those policies despite being optimized on the same initial conditions. PPO never performs better than the other network policies.
%
\vspace{-0.3cm}
\subsection{Generalization}
Next, we study the generalization capabilities of the learned policies.
First, we test the policies for random initial configurations on a flat ground.
Then we choose random starting position and change either the ground height (decreased at $t=\SI{0.3}{\second}$ between $[0\%, 100\%]$ of the robot's leg length) or
the slope (between $[-60,60]$ degrees). We perform each test for 100 different initial configurations. The robustness of the policies is determined by measuring the PHVS for different level of action noise as in \cref{sec:result_sim_perf}.
%

Figure 5c) shows the PHVS performance for various initial
starting positions. As expected, the iLQR policies perform the worst.
In contrast, the performance of the network policies is similar to the first two starting positions and sometimes even higher. These results are not surprising: The iLQR policies are trajectory-centric
in the sense that they are policies around a single trajectory. Starting their policy from a different initial state easily causes them to become unstable. In contrast, the network policies depend only on the current state and have no notion of time or initial position and can therefore react to a new changing starting position. The PPO policy shows similar results although always performing slightly worse.

When changing the ground height, the network policies perform very well for larger changes. While the PPO policy keeps a similar performance over various height changes it has a worse PHVS measure than the other policies. In the case of slope changes,
the jump often fails when the robot knee or hip enters in collision with the ground. In this case both PPO and torque policies degrade when the slope is too high.
Overall the feedback network policy perform best.
%
Finally, we test the policies in constantly changing environments.
We run them for \SI{10}{\second} while randomly changing the floor height and slope. More precisely, we let the neural network policy control the robot until the robot falls or hits the maximum rollout length of \SI{10}{\second}. Whenever the hopper is at an apex point, the floor height and floor slope are randomly changed. The new floor height and floor slope are sampled uniformly from $[-\text{maxFloorHeight}, \text{maxFloorHeight}]$ and $[-\text{maxFloorSlope}, \text{maxFloorSlope}]$. If the new floor configuration is in collision with the robot, new height and slopes are sampled. We compute the mean and standard deviation of the termination time over 50 rollouts.

The results for this experiment are shown in \cref{table:res_marathon}. We omit the results for the torque network policy as the results are qualitatively similar to the feedback policy.
%
Looking over all results in \cref{table:res_marathon_feedback}, the average terminal time of the feedback neural network policy stays roughly the same when the changes to the floor height increase. However, when the floor slope increases, the trajectory terminates earlier. In addition, the behavior can generalize to random changes in the floor slope and height.
%
The performance of the PPO network policy as shown in~\cref{table:res_marathon_ppo} is also more or less constant when changing the floor height. When changing the floor slope, the performance deteriorates quickly compared to the feedback neural network policy.
%
{\setlength{\tabcolsep}{0.5em} % for the horizontal padding
\renewcommand{\arraystretch}{1.2}% for the vertical padding
\begin{table}[h]
\vspace{-0.2cm}
\begin{center}

\begin{tabular}{|c c|c|c|c|c|}
\cline{3-5}
\multicolumn{2}{c|}{~} & \multicolumn{3}{c|}{Network Policy} \\
\multicolumn{2}{c|}{~} & Torque & Feedback & PPO \\
\hline
\multirow{4}{*}{\rotatebox[origin=c]{90}{Slope }}&
$25.0^{\circ}$ & 0.076 $\pm$ 0.017 & 0.128 $\pm$ 0.008 & 0.155 $\pm$ 0.004\\
\cline{2-5}
& $12.5^{\circ}$ & 0.128 $\pm$ 0.011 & 0.162 $\pm$ 0.004 & 0.217 $\pm$ 0.009\\
\cline{2-5}
& $0.0^{\circ}$ & 0.141 $\pm$ 0.007 & 0.182 $\pm$ 0.008 & 0.213 $\pm$ 0.008\\
\cline{2-5}
& $-12.5^{\circ}$ & 0.098 $\pm$ 0.008 & 0.125 $\pm$ 0.018 & 0.133 $\pm$ 0.011\\
\cline{2-5}
& moving slope & 0.097 $\pm$ 0.008 & 0.121 $\pm$ 0.020 & 0.167 $\pm$ 0.025\\
\hline
\end{tabular}
\vspace{-0.2cm}
\end{center}
%
\caption{Results on real hardware for \SI{2}{\second} runs (PHSV mean and one standard deviation computed over 5 runs).}
\label{table:res_real_policy_phvs}
\vspace{-0.2cm}
\end{table}}
\vspace{-0.3cm}
\subsection{Interpreting the learned control structure}
%
\begin{figure}
%    \vspace{0.2cm}
    \centering
%
%    \begin{subfigure}[b]{1.0\linewidth}
      \includegraphics[width=1.0\linewidth,clip,trim={0 15 0 25}]{figures/nominal_nn_height.eps}
%    \end{subfigure}
%
    \caption{Top plot: snapshots of the robot over time. Middle plots: desired trajectories output from the feedback network policy and corresponding simulated trajectories. Bottom plot: diagonal elements of the gain matrix output by the policy.}
%
    \label{fig:result_nominal_traj}
\vspace{-0.6cm}
\end{figure}
%
%\jviereck{"Furthermore, the notion of interpretability in the abstract is not covered by the manuscript."}
One of the goal of this work is to learn a control policy with a structure that can
be interpreted.
In order to check if such structure was indeed learned, we run the network from starting position $x^1_0$ for a length of \SI{1}{\second} and change the floor height by one leg length at $t=\SI{0.3}{\second}$ in simulation. We then analyze at the output of the network, in particular to the nominal trajectory and the gains outputs (\cref{fig:result_nominal_traj}).

We compare the simulated robot motion with the desired nominal trajectory output from the feedback network policy, i.e. for every state $\xt$ along the trajectory we compute $\hat{\b{x}}^\text{feedback}_\theta(\b{x}_t)$.
First we notice that the robot closely follows (with a slight delay) the desired joint position trajectories
output by the network. This suggests that the network correctly encoded desired joint positions.

When the floor height changes at $t=\SI{0.3}{\second}$ the desired base height from the feedback network policy does not increase rapidly: it asks the robot to come down (note that the base height while jumping is uncontrollable).
Around $t=\SI{0.7}{\second}$, the robot gets close to the ground. Here the desired base position increases, the robot should start to come up and after $t=\SI{0.7}{\second}$ smaller desired angles for the hip and knee joint. These are reasonable desired positions which would make the robot jump when properly tracked.

The bottom graph of \cref{fig:result_nominal_traj} shows the time evolution of gains that are responsible for producing hip (knee) torques from the hip (knee)
position error. At impact, these gains decrease very quickly, leading to a more compliant behavior to absorb the impact. Then the joints stiffen to allow the robot to jump. The gains remain stiff during the flying phase, allowing the robot to reposition its leg, until the next impact. Interestingly, at the moment where the next impact would have been if the ground position would not have changed (at $\simeq 0.5$s) the gains again decrease, which
suggests that the policy anticipated a contact and increased compliance.

Overall, the gain schedule is consistent with the jumping
motion (stiff to jump, compliant at landing) and the desired positions correspond to the actual motion of the robot. Note that, while this can seem obvious, it was not clear that these outputs would have any meaning because network outputs are redundant and the actual torque sent to the robot is a combination of the outputs of the policy network.
These results suggest that the learned network outputs have an interpretable meaning and can help better understand what the policy tries to achieve at different instants of time.
This is an interesting aspect as usually neural network torque policies do not
have a structure that afford an understanding of the behavior in terms of desired positions and leg stiffness.

%Overall, the output desired positions are meaningful and describe a desired jumping behavior. The actual trajectory closely follow the desired one with a small time delay, demonstrating that the output of the network is indeed meaningful.
%The network has learned a reasonable desired trajectory behavior from the iLQR policies. Interestingly, the output of the network can be analyzed in an intuitive manner.
%
\subsection{Transfer to real hardware}
Finally, we test the transfer of the learned policies on the real robot.
We compute the PHVS metric for each policy over five executions of $t=\SI{2.0}{\second}$ on the real robot
for flat ground and grounds with slopes between $-12.5^{\circ}$ and $25.0^{\circ}$ (maximum slopes before robot started to slip from initial configuration).
% , these were the maximum slopes for which the robot could stand in its initial configuration before slipping.
%For the case where the ground remains static, PPO network policy performs slightly better than the feedback network policy.
%However, when manually changing the ground orientation and height over a run of \SI{10}{\second}, the feedback network policy managed to continue hopping in four out of five trials. In contrast, the PPO network policy failed in all trials.
We also tested the policies when the slope and height of the ground was continuously changing.
As can be seen on the attached video, the right side of the ground is moved up and down by a motor at a constant frequency while the left side is fixed. The slope changes between $-5$ and $15$ degrees.

Result are shown in \cref{table:res_real_policy_phvs}.
For all experiments, the three policies never failed. The PPO policy has a slightly higher PHSV than
the network policy. When analyzing the results, we find that the robot jumps at the same height for both policies. The PPO policy tends to keep the leg more to the back and is more compliant which makes the robot come closer to the ground at contact and therefore needs a higher upward velocity, which seems to explain the difference in PHSV.
The torque policy systematically jumps lower than both PPO and feedback policies and has a much lower PHSV.

Interestingly, the dynamics of the simulated robot is quite different from the real robot. First, we underestimated
the mass distribution of the robot as can be seen in  \cref{fig:hopper_setup}. For the simulation,
we used the CAD model of the robot but forgot to take the electronics and sensors into account. Additionally, the simple contact model used in the simulation was sufficient to transfer the behaviors on the real robot where there are non trivial friction effects and slipping.
Still, the reactive feedback policies can be
transferred on the real robot and are robust to a changing environment.
We were also able to transfer policies learnt with PPO but it is important to note that the policies in this case are not interpretable and it takes an order of magnitude more samples to train.
%
%
%
%
\section{Discussion and Conclusion}
\label{sec:conclusion_and_future_work}
In this paper, we considered the problem of learning control policies for dynamic tasks with significant impact
dynamics. We extended previous methods to work with such systems by 1) proposing a regularization method to learn a sample efficient dynamics model with switching contacts for underactuated systems,
2) describing a modified iLQR algorithm with adaptive receding horizon length that take into account actuation torques
and 3) learning time-independent, reactive policies with two types of neural networks, and in particular with a network that preserves the structure of the LQR policy, making the network amenable to analysis.

As our results show, we are able to learn a dynamics model with only 5 sampled trajectories per iteration. The trained network policies perform better on the training scenarios and new validation scenarios than the iLQR policies. The feedback network policy performs better, though not significantly better than a network that learns to predict actions directly. However, the feedback network policy is capable in providing more interpretable control outputs.  We envision that such policies could be more easily analyzed with formal control methods, afford a physical interpretation of the behavior (e.g. in terms of impedance regulation)
and be easier to reuse across different tasks (e.g. the feedback gains). It will be the focus of our future work. Finally, experiments on the real robot demonstrate that the learned policies can be effectively transferred on a real hardware while retaining their generalization properties.

When comparing our method to PPO, a state of the art reinforcement learning method, we found the feedback network policy to always have similar or better performance than PPO, while using one order of magnitude fewer samples during training. When changing the ground in simulation, our method tended to outperforms the PPO network policy but both PPO and the feedback network policy showed good performance on the real robot.
It is likely that PPO would perform better if it were trained under a larger variety of environments. It would then be interesting to compare our approach and PPO on a larger set of environments to study their generalization capabilities further.
%
%In this work, we assume a non informative prior on the dynamics matrix for regularizing it. As future work, it would be interesting to explore the use of prior knowledge on the robot dynamics. The feedback network policy's objective $\mathcal{L}^{\text{feedback}}_\theta$ in Eq.\eqref{eq:loss_nn_feedback} assigns equal importance to all three predicted terms of the network. By investigating a better weighting and norm the quality of the network predictions might improve.
%Finally, it would be interesting to extend our approach with model-free trajectory optimization techniques to improve robustness on the real robot \cite{chebotar17}.

%\jviereck{Point out there are better ways to train PPO.}

\addtolength{\textheight}{5cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\bibliographystyle{IEEEtran}
\bibliography{refs}{}

\end{document}
