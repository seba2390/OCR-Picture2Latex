\section{Framework Formulation}






\subsection{Overview} %
\input{figs/framework}

To study how different platform mechanisms affect users' decisions in information disclosure, we first need to incorporate the users' data disclosure decisions into the recommendation process.
Thus, we propose a privacy aware recommendation framework where users can freely choose which data to disclose with the recommender system.
As illustrated in \cref{fig:framework}, the critical difference between our framework and traditional recommendation is that the platform can only use the sub-data disclosed by the users. 
For example, the user on the left in \cref{fig:privacy_rec} can choose to hide his sensitive demographic attributes (e.g., age, gender, and education) and only discloses the last behavior to the service provider. 


To enjoy the benefits of personalized services, users need to disclose their data to the recommender system to better model them.
Intuitively, more data the recommender system gets, better results the users can get.
However, disclosing data to the platform will increase users' privacy concerns, e.g., data abusing~\cite{Mayer:sp12:Third} and privacy leakage~\cite{zhang2021membership}.
Thus, under the privacy aware setting, users need to make information disclosure decisions based on the trade-off between anticipated privacy risks and potential utilities.
This idea can date back to \textit{Privacy Calculus Theory}~\cite{Laufer:si77:Privacy,Culnan:os99:Information}.

Before going into details, we first define the entire personal data $\di{}$ of user $i \in \mathcal{V}$'s as:
\begin{equation}
\di{} = \{{\scriptstyle \mathcal{D}_{i,a}, \mathcal{D}_{i,b}} \}= \{\{a_{i1},\dots, a_{iK}\}, \{b_{i1},\dots, b_{it_i}\}\},
\label{eq:di}
\end{equation}
where ${\scriptstyle \mathcal{D}_{i,a}}{=}\{a_{i1},\dots, a_{iK}\}$ denotes user $i$'s all profile attributes, $a_{ik}$ denotes the $k$-th profile attribute for user $i$, and $K$ is the number of profile attributes. ${\scriptstyle \mathcal{D}_{i,b}}{=}\{b_{i1},\dots, b_{it_i}\}$ denotes user $i$'s behaviors, $b_{ij}$ is the $j$-th behavior of user $i$, and $t_i$ is the last behavior timestamp. \czq{$\mathcal{V}$ is the set that includes all users in the platform.} %

A rational user is only willing to disclose data when she feels that she gains more from the platform than she loses in data disclosure.
Formally speaking, supposing user $i$ with whole data $\di{}$ currently discloses data $\si{} \subset \di{}$, now she tries to get a better recommendation results via disclosing more data $\si{'} \subset \di{}$ where $|\si{'}| > |\si{}|$, only if
\begin{equation}
    \texttt{U}_i(\si{'}) - \texttt{U}_i(\si{}) > \lambda_i\bigl(\texttt{C}_i( \si{'}) -\texttt{C}_i( \si{})\bigr),
\label{eq:base}
\end{equation}
where $\texttt{U}_i(x)$ denotes the utility that user $i$ can get from the platform with disclosed data $x$, function $\texttt{C}_i(x)$ measures the privacy cost paid by the user $i$ when she discloses the data $x$ to the platform, and $\lambda_i$ is the sensitive weight measuring how much user $i$ cares about her privacy.
Apparently, compared to privacy insensitive users (i.e., small $\lambda_i$), the platform needs to provide more performance improvements to attract privacy sensitive users (i.e., large $\lambda_i$) to disclose their data.
More details can be found in \cref{sec:user_type}.


\subsubsection{\textbf{User Objective.}}
Unlike traditional task settings where users can only passively accept recommendation results (i.e., without tools to optimize their objectives), in our framework, a rational user $i$ tends to maximize her utility $\texttt{U}_i(\si{})$ while minimize the privacy risk $\texttt{C}_i(\si{})$ by control the disclosed data $\si{}$.
The objective function for a specific user $i$ can be formalized as the following:
\begin{equation}
    \texttt{R}_i(\si{}) = -\lambda_i \texttt{C}_i(\si{}) + \texttt{U}_i( \si{}).
\label{eq:framework}
\end{equation}

The linear combination for user objective function follows the initial idea from \textit{Privacy Calculus Theory}~\cite{Laufer:si77:Privacy,Culnan:os99:Information} where both the recommendation performances from the platform and potential privacy cost are considered.
This formulation is also compatible with privacy related research in economics~\cite{farrell2012can,jin2017protecting,lin2019valuing}. %
They studied the micro-foundation on a user's intrinsic and instrumental preferences from disclosing personal information.
In our formulation, user's privacy cost $\texttt{C}_i(\si{})$ corresponds to intrinsic value for personal data (i.e., protecting the data from being obtained by others), while recommendation utility $\texttt{U}_i(\si{})$ corresponds to the instrumental value for personal data.


\subsubsection{\textbf{Platform Objective.}}
\label{sec:platform_obj}
In the proposed framework, the goal of a platform is still to maximize its revenue (e.g., purchases, clicks, or watching time) by improving the users' recommendation utility (e.g., providing more accurate results).
Thus, we define its objective as the summation of all users' recommendation utilities in \cref{eq:framework}\czq{, where $\mathcal{V}$ denotes all users in the platform}:
\begin{equation}
    \texttt{R}_{\text{p}} = \sum_{i \in {\scriptstyle \mathcal{V}}} \texttt{U}_i(\si{}).
\label{eq:platform}
\end{equation}
Considering the utility also depends on the recommendation model, the utility function $\texttt{U}_i(x)$ can be further defined as:
\begin{equation}
    \texttt{U}_i(\si{}) = \texttt{U}(\si{}) = \texttt{U}'(\si{},\, \texttt{M}_{{\scriptscriptstyle \mathcal{S}}}),
    \label{eq:updated_rec}
\end{equation}
where $\texttt{M}_{{\scriptscriptstyle \mathcal{S}}}: \si{} \rightarrow l_i$ ($l_i$ is recommendation results) is a recommendation model trained using all users' disclosed data ${\scriptstyle \mathcal{S}}=\{ {\scriptstyle \mathcal{S}_1},\dots, {\scriptstyle \mathcal{S}_{|\mathcal{V}|}}\}$ and $\texttt{U}'$ represents detailed recommendation utility function.
Here, without loss of generality, we assume that all users share the same utility function.
We will explore the personalized utility function in the future work.


\subsubsection{\textbf{Recommendation Utility Function}}
As shown in \cref{eq:framework} and \cref{eq:platform}, the recommendation utility $\texttt{U}$  occurs in the objective functions of both end users and the platform.
Here, we use the users' satisfaction with the results produced by the recommendation model to measure its utility.
It is worth noting that user satisfaction is still an open problem in \czq{recommender systems}.
\czq{Here, we simply quantify it by the user’s interactions} with the recommendation results, e.g., clicks, watches, and reads.
Based on such feedbacks, we can calculate different quantitative metrics as the utility in our framework, e.g., hit ratio and normalized discounted cumulative gain (NDCG)~\cite{ndcg}.
In this paper, we choose NDCG as the utility function $\texttt{U}$ for all users because of its widespread use \cite{NCF,kang2018self,Sun:cikm19:BERT4Rec}.


In traditional recommendation task, the platform can optimize this objective by only optimizing the model $\texttt{M}_{{\scriptscriptstyle \mathcal{S}}}$ since $\si{}= \di{}$ is a fixed, i.e., all users disclose their whole data.
However, this premise is broken in our proposed framework, where the user's disclosed data $\si{}$ is varying.
Thus, in our new framework, platforms also seek to attract users to share more data in other ways besides optimizing models, such as platform mechanism design.



\subsection{Platform Mechanism}
\label{sec:platform}

As mentioned before, the disclosed data $\si{}$ lives at the heart of the framework.
Ideally, user $i$ can freely choose any data $\si{}$ to disclose with the platform, e.g., choosing any profile attribute $a$ or behavior data $b$ as shown in \cref{fig:privacy_rec}.
However, in practice, such a degree of freedom is difficult to achieve for two reasons.
On the one hand, from the perspective of human-computer interaction, too fine granularity of disclosure choice (e.g., single behavior) can adversely hurt user experience~\cite{Zhang:hcs19:Proactive}.
On the other hand, although the privacy regulations ensure users the right to determine the use of their data, they do not stipulate how the service providers implement this function.


In practice, the platform usually designs some data disclosure mechanisms to provide the end users with several convenient options.
Here, we formulate the platform mechanism $\mathrm{G}=<\delta, \Pi>$ using two components, data split rule $\delta$ and disclosure choice spaces $\Pi$.
The data split rule $\delta$ is regarded as a function that reorganizes the original user data $\di{}$ using different granularity, and $\Pi$ denotes the space of all possible choices the platform provides to the user.
We illustrate a toy example in \cref{fig:mechanism}.

\subsubsection{\textbf{Data Split Rule}}
Since user data usually consists of two different data types (as in \cref{eq:di}), we defined $\delta$ as:
\begin{equation*}
    \delta (x) = \{\delta_a({\scriptstyle \mathcal{D}_{i,a}}), \delta_b({\scriptstyle \mathcal{D}_{i,b}}) \},
\end{equation*}
where $\delta_a$ and $\delta_b$ have similar forms that split the original data into several pieces according to the corresponding granularity and rules:
\begin{equation*}
    \{x_1, x_2,\dots,x_n\} \xrightarrow[\delta_a]{\delta_b}  \{x_1', x_2',\dots,x_m'\}, \,\, %
\end{equation*}
where $m \leq n$ and $x_j'$ is the candidate units for data disclosure. %
According to the segmentation rules, $x_j'$ can be several consecutive data points like $\{x_1, x_2, x_3\}$ or discontinuous random data like $\{x_5, x_{22}\}$.

$\delta_a$ aims to reorganize the user’s profile attributes.
The common approach is to keep original granularity (i.e., user can freely disclose any subset of attributes) or take all attributes as a whole (i.e., disclose all attributes or not).
Formally, it can be instantiated as:
\begin{equation*}
    \begin{aligned}
     \delta_a ({\scriptstyle \mathcal{D}_{i,a}}) &= \{a_{i1},\dots, a_{iK}\}\\
    \text{or}\quad  \delta_a ({\scriptstyle \mathcal{D}_{i,a}}) &= \{\{a_{i1},\dots, a_{iK}\}\}.
    \end{aligned}
\end{equation*}

Similarly, $\delta_b$ aims to transfer a user's original behavior data (e.g., thousands of clicks or more views) to few data disclosure options.
For example, ``percentage split'' with 10\% granularity divides a user's behavior sequence into 10 equal length subsequences, while ``daily split'' divides the user's behaviors by day.
Take ``percentage split'' with 10\% granularity as an example, it can be instantiated as: 
\begin{equation*}
    \delta_b ({\scriptstyle \mathcal{D}_{i,b}}) = \{{\scriptstyle \mathcal{S}_{i,b1}}, {\scriptstyle \mathcal{S}_{i,b2}}, \dots, {\scriptstyle \mathcal{S}_{i,b10}} \},
\end{equation*}
where ${\scriptstyle \mathcal{S}_{i,bj}} = \{b_{i,\lfloor 0.1 t_i*(j-1)\rfloor+1}, \dots, b_{i, \lfloor 0.1 t_i*j\rfloor}\}$ is the $j$-th candidate option of behavior data for user to disclosed.

\subsubsection{\textbf{Data Disclosure Choice Space $\Pi$}}

Assuming the platform has transferred user $i$'s original data $\di{}$ to $\delta(\di{}) = \{{\scriptstyle \mathcal{S}_{i,a1}},\dots,{\scriptstyle \mathcal{S}_{i,an}},$ $ {\scriptstyle \mathcal{S}_{i,b1}}, \dots,{\scriptstyle \mathcal{S}_{i,bm}}\}$, the platform can define data disclosure choice space $\Pi$ on these $m+n$ candidates as:
\begin{equation*}
    \begin{aligned}
    \Pi &= \{\Pi_1,\Pi_2, \dots, \Pi_N\}, \\
    \Pi_j & \sim [o_1, \cdots, o_k, \cdots, o_{n+m}], \quad o_k \in \{0,1\},
    \end{aligned}
\end{equation*}
where $o_k = 1$ denotes disclosing the $k$-th data in $\delta(\di{})$, while $o_k = 0$ means not; $\Pi_j$ is $j-$th data disclosure option that users can take; $N$ is the number of possible choices the platform provides to users.
For example, a full 0 vector $\Pi_j = [0, 0, \dots, 0]$ denotes that users can choose it to do not disclose any data.
More detailed instantiations can be found in \cref{sec:plat_mech}.


\input{figs/mechanism}
\subsubsection{\textbf{Platform Mechanism Design}}


With \czq{the} mechanism $\mathrm{G}=<\delta,\Pi>$, we can formally define the disclosed data $\si{}$ from user $i$.
Assuming user $i$'s original data $\di{}$ has been spited into candidates $\delta(\di{}) = \{{\scriptstyle \mathcal{S}_{i,1}}, \dots,{\scriptstyle \mathcal{S}_{i,m}}\}$\footnote{Here, we simplify the subscripts for easy description}. 
Then, $\si{}$ can be defined as the union of candidates in $\delta(\di{})$ selected  by a specific choice $ \alpha_i = \Pi_j \in \Pi$:
\begin{equation}
    \si{} = \alpha_i \otimes \mathrm{\delta}(\di{}) = \Bigl\{\scaleto{\bigcup_{\begin{subarray}{l}\,\,\, o_k=1\\\,\,\,  o_k \in \Pi_j\end{subarray}}}{20pt} {\scriptstyle \mathcal{S}_{i,k}} \Bigr\},
\label{eq:S_i}
\end{equation}
where $\delta$ is the platform data split rule and %
action $\alpha_i$ is sampled from user $i$'s privacy disclosure policy $\pi_i$, which decides the data to be disclosed. 
The operator $\otimes$ denotes the aggregation of the selected spilt data based on his choice $\alpha_i$.
\cref{fig:mechanism} illustrates a tiny example of the data disclosure process (i.e., generation process of $\si{}$) of a user with three profile attributes and four behaviors.


With formal definition of $\si{}$, we can  re-write the platform utility $\texttt{R}_{\text{p}}$ in \cref{eq:platform} using \czq{the} platform mechanism $\mathrm{G}$ and model $\texttt{M}_{{\scriptscriptstyle \mathcal{S}}}$ as below,
\begin{align}
     \texttt{R}_{\text{p}} |_{\mathrm{G=<\delta,\Pi>}} {=}  \sum_{i \in \mathcal{V}} \texttt{U}_i'(\si{}, \texttt{M}_{{\scriptscriptstyle \mathcal{S}}}) {=}   \sum_{i \in \mathcal{V}} \texttt{U}_i'(\alpha_i \otimes \mathrm{\delta}(\di{}) , \texttt{M}_{{\scriptscriptstyle \mathcal{S}}}).%
    \label{eq:platform_mechanism_optimization}
\end{align} 

One may figure out some possible optimal solutions towards the platform's best mechanisms.
However, the optimal platform mechanism design is another complex topic, usually considered from the view of game theory, and is out of scopes of this work.
Here, we take the first step, studying the data disclosure decision of users and platform revenues under several common mechanisms.



\subsection{Relationship with Privacy Preservation}


In this subsection, we will discuss the relationship between our work and privacy preservation, and clarify the position of our paper.

First, as claimed in the introduction, our work does not aim to propose a new method to directly protect the users' privacy data from privacy attacks.
The main motivation of this paper is to study the effectiveness of existing privacy mechanisms deployed in the platforms and further explore how different privacy mechanisms affect users' privacy decisions.
For this purpose, our proposed framework is model-agnostic.
The model $\texttt{M}_{\scriptscriptstyle \mathcal{S}}$ used here can be an ordinary recommendation model (e.g., NCF ~\cite{NCF} or GRU4Rec~\cite{Hidasi:ICLR2016:gru4rec}) or privacy-preserving recommendation models~\cite{Chen:TIST2020:Practical}.
For deploying a privacy-preserving model in our framework, we only need to modify the privacy cost function ($\texttt{C}_i$), taking into account the influence of protecting privacy, i.e., less privacy cost than a normal model when disclosing the same amount of data.
In this paper, for the convenience of analysis and considering that privacy-preserving technologies are not widely used in real-world applications, we only studied the proposed framework in ordinary recommendation models.
We leave the exploration of the impact of privacy-preserving technologies on users' privacy cost functions for future work. 

Second, our proposed framework provides users with the power to proactively trade off their privacy cost and recommendation utility.
From the final result point of view, users can optimize their data disclosure decision to discard those data that are not helpful for their recommendation results.
In this way, the proposed framework gives users the tool to achieve data minimization~\cite{Mireshghallah:WWW20:Not,biega2020operationalizing} by themselves, rather than waiting for the platform to implement the data minimization algorithms.
From this perspective, our proposed framework can be seen as  implicitly protecting user privacy.
Even if a privacy attack occurs, only part of users' disclosed data will be leaked.
Besides, giving users control over the recommendation process has also been found be effective in reducing their privacy concerns~\cite{Zhang2014-oa,Chen:CHI18:This}.
