\section{Simulation}

\subsection{motivation} %
The most efficient way to figure out the answers to the questions we posed in the introduction is to deploy the proposed framework on a real-world platform and analyze how users adopt different and complex privacy policies to optimize their rewards.
However, direct deployment of these strategies and investments is currently impractical due to the following reasons.


Firstly, the most important reason is that such an online experiment may lead to the decline of the recommendation performances as well as the user experience, which harms the platform's revenue.
In the real world, nearly all the companies determine their platform mechanism driven by interest, and the revenues of the platforms are highly correlated with the recommendation performances. 
Therefore, it's nearly impossible to persuade any platform to directly deploy proposed strategies and mechanism online without other benefits.  



Secondly, the experiments are \czq{built} upon several simplifications, mentioned in \cref{assumptions}, which poses challenges towards recommendation model training process.
For example, we assume when a user \czq{adjusts} his data disclosure policy, the recommendation system will forget his un-disclosed data. 
To facilitate such challenges, model unlearning or other privacy-preserving technologies are imposed.
However, in real-world applications, very few the e-commercial platforms have deployed these privacy-preserving technologies during the deep recommendation model training and evaluation processing.
As a result, we may still fail to guarantee the assumptions and simulation methodology becomes a substitution.










In summary, inspired by the success of simulation study on dynamic interactive problems in real-world applications~\cite{Ie:arxiv19:RecSim,krauth2020offline,lucherini2021t,yao2021measuring},  we employ the simulation to study the effects of the proposed framework and the possible game between users and the platform.















\subsection{Simplified Assumptions}
\label{assumptions}

To simplify the simulation process for easier analysis, we make some necessary assumptions to simplify the problem.

\begin{assumption}[Static Assumption] User $i$ optimizes her/his policy on the fixed data $\di{}$ which is not affected by user policy $\pi_i$.
\label{assumption:static}
\end{assumption}

Here static means the user data $\di{}$ is fixed during the simulation, but the disclosed data $\si{}$ produced by different user policies is dynamic. 
It is also the most common setting for recommendation task in research papers~\cite{Rendle:www10:Factorizing,Hidasi:ICLR2016:gru4rec,NCF,kang2018self,Sun:cikm19:BERT4Rec}.
In the simulation, we train the recommendation system $\texttt{M}_{{\scriptscriptstyle \mathcal{S}}}$ on the collected dynamic data $\mathcal{S}$ and validate the recommendation efficiency on a fixed test set. 
In real-world applications, the data $\di{}$, which contains the behavior data from the interaction with the recommender $\texttt{M}_{{\scriptscriptstyle \mathcal{S}}}$, is also dynamically changing with the user's policy $\pi_i$.
It is beyond the scope of this paper and we leave it as the future work. 


\begin{assumption}[Immediate Assumption] The recommendation model $\texttt{M}_{{\scriptscriptstyle \mathcal{S}}}$ can only use the data $\si{}$ currently disclosed by each user $i$.
\label{assumption:forget}
\end{assumption}
The motivation of this assumption is that an untrusted platform can leverage user $i$' all data $\di{}$ if it can use the data disclosed in previous actions.
Without this constraint, the privacy right discussed in this paper is meaningless.
To achieve this, the platform can retrain the model from scratch with new data $\si{'}$ or quick unlearn the data in $\si{}$ then finetune with data $\si{'}$~\cite{cao2015towards,bourtoule2021machine,chen2022recommendation}.


However, the \cref{assumption:forget} also raises a new challenge that the asynchronous changes of user policy bring intractable computation costs for the platform since each time the user changes the disclosed data, the platform needs to update the model.
Here, we make an assumption for simplifying the simulation, assuming all users realize that the platform will cyclically (e.g., once a day) collect their privacy decisions and update recommender systems.
\begin{assumption}
[%
Cyclical Assumption]
Platform cyclically collects user privacy choices, and then the platform updates the model using all newly disclosed data. 
\label{assumption:synchronization}
\end{assumption}



In summary, for easy analysis in simulations, we introduce these assumptions to ignore the time and dynamic effects in this feedback system, just like the traditional recommendation task formulation.
























\subsection{Platform Mechanism Simulation}
\label{sec:plat_mech}
In order to validate the effect of the platform mechanism, we adopt several mechanisms during simulation. 
For easy comparison, we utilize one mechanism at each experiment. %


\subsubsection{\textbf{Data Split Rule}}

\czq{In our simulation, we do not split the profile attribution and the user can determine whether to share all of their attributes.}
For behavior data, we apply ``percentage split'' as $\delta_b$ with different split granularity $p$ (e.g., 1/3) to split the behavior sequence into $1/p$ parts. 
One obvious advantage of ``percentage split'' is that it can normalize the size of user action space and decrease the inconvenience of the interaction between the user and the platform.

\subsubsection{\textbf{Data Disclosure Strategy}}
\label{sec:data_disclose_choice}

As the platforms have certain flexibility to implement different data disclosure strategies, we discuss three representative disclosure strategies used in our study for behavior data in this subsection.
These strategies determine the data disclosure action space $\Pi$ the user can choose.
For profile attributes, we found that all users tend not to disclose them in the experiments since these features are negligible for improving recommendation utility in the presence of behavior data.
Similar result that user profile features contribute very marginal to the recommendation results in the case of strong user behavior modeling on public benchmark datasets has also been reported in other works~\cite{kang2018self,Sun:cikm19:BERT4Rec}.
Thus, in the following study, we mainly focus on modeling only  behavior data.

The ``\textit{separate}'' rule gives the users the control to freely disclose any split personal data.
For this rule, the size of user $i$'s the action space is exponentially expended on the size of the spilt data set $|\delta_b ({\scriptstyle \mathcal{D}_{i,b}})|$, denoted as $2^{|\delta_b ({\scriptstyle \mathcal{D}_{i,b}})|}$. 
However, too many choices might make it difficult for users to make better data disclosure decisions.

Another data disclosure strategy named ``\textit{oldest continuous}'' provides users the choices to disclose continuous behavior data from the beginning time, such as selecting ``the oldest 33\% data''.
In this strategy, to disclose newer behavior data ${\scriptstyle \mathcal{S}_{i,bj}}$, users must also disclose all behavior data before it.
Take an already split data $\delta_b ({\scriptstyle \mathcal{D}_{i,b}}) = \{\scriptstyle \mathcal{S}_{i,b1}, \scriptstyle \mathcal{S}_{i,b2}, \scriptstyle \mathcal{S}_{i,b3}\}$ as an example, the action space provided by oldest continuous strategy is $\Pi = \{[0,0,0], [1,0,0], [1,1,0], [1,1,1]\}$, and its corresponding disclosed data is $\{\varnothing ,
    \{\! {\scriptstyle \mathcal{S}_{i,b1}} \!\},
    \{\! {\scriptstyle \mathcal{S}_{i,b1}} , {\scriptstyle \mathcal{S}_{i,b2}}\! \},$
    $\{ {\scriptstyle \mathcal{S}_{i,b1}}, {\!\scriptstyle \mathcal{S}_{i,b2}},$ ${\scriptstyle \mathcal{S}_{i,b3}}\} \}$.
``\textit{Latest continuous}'' mechanism is similar to ``oldest continuous'', with the only difference in the opposite direction.
The size of these two mechanisms' action spaces is $|\delta_b ({\scriptstyle \mathcal{D}_{i,b}})|$.









\subsection{User Policy Simulation}
\label{sec:user}


In this subsection, we introduce the simulation of user policy in our proposed framework.
As defined in \cref{eq:S_i}, the disclosed data $\si{}$ is result of the platform mechanism $\mathrm{G}$ and user's disclosure policy $\pi_i$. 
Meanwhile, in \cref{eq:updated_rec}, the recommendation utility $\texttt{U}_i( \si{})=\texttt{U}'(\si{},\, \texttt{M}_{{\scriptscriptstyle \mathcal{S}}})$ is also determined by the recommendation model $\texttt{M}_{{\scriptscriptstyle \mathcal{S}}}$, which is \czq{built} upon the all users' disclosed data $\mathcal{S}$. 
The reward of user $i$ may be varied even when $i$ keeps the disclosed data $\si{}$ unchanged since other users might change their disclosed data and the recommender system is changed.
Thus, the expectation rewards are considered rather than immediate value defined in \cref{eq:framework} and we assume all the users are rational and seek for the optimal privacy disclosure action  $\alpha_i^*$ to the optimal expected reward $E[ \texttt{R}_i | \alpha_i ] $ as his policy, i.e., %
\begin{equation}
    \begin{aligned}
    \alpha_i^{*} &= \argmax_{\alpha_i \in \Pi} E[ \texttt{R}_i | \alpha_i ]=  \argmax_{\si{\in [ \Pi \otimes \mathrm{\delta}(\di{}) ] }} E[ \texttt{R}_i(\si{)} ] \\
& =\argmax_{\alpha_i \in \Pi } E\Bigl[ -\lambda_i \texttt{C}_i\bigl( \alpha_i \otimes \mathrm{\delta}(\di{}) \bigr) + \texttt{U}_i\bigl( \alpha_i \otimes \mathrm{\delta}(\di{}) )\bigr) \Bigr].
    \end{aligned}
\label{eq:opt_pi}
\end{equation}

As mentioned before, recommendation utility $\texttt{U}_i$ has been discussed in \cref{sec:platform_obj}.
To study this objective, we need to define the privacy cost function $\texttt{C}_i$ and sensitive weight $\lambda_i$.



\subsubsection{\textbf{Privacy Cost Function}}
\label{sec:privacy_cost}
We simulate every user with the same cost function $\texttt{C}$ and leave the diversity of user privacy sensitivity to the parameter $\lambda_i$. 
Following current experiment specifications in the economics literature~\cite{lin2019valuing,tang2019value}, we model the privacy cost function as a linear summation\footnote{See the Eq. 2 in \cite{lin2019valuing} and the dis-utility from disclosure in the econometric specification session in \cite{tang2019value}.} of disclosed personal data.


\czq{According to the comprehensive survey on privacy value definition \cite{MKT-053}, people will measure the value of their privacy into the intrinsic value of privacy and the instrumental value of privacy.}
\czq{
The intrinsic loss indicates the sake of protecting their intrinsic private data, which measures the valuation on the intrinsic properties such as the education or the income levels. }
\czq{In this work, we denote the intrinsic loss towards the privacy cost on amount of the sharing user profile attributes.}
\czq{The instrumental value of privacy indicates how the transaction efficiency would be affected by sharing user data, especially the data generated in the applications. 
In this work we denote the privacy cost towards the percentage of shard user historical behavior data. 
Therefore, the privacy cost function is described below,}
\begin{equation}
    \texttt{C}_i(\si{})= \beta_i * | {\scriptstyle \mathcal{S}_{i,a}} | + \frac{| {\scriptstyle \mathcal{S}_{i,b}} |}{ | {\scriptstyle \mathcal{D}_{i,b}} |}
    \label{eq:cost_function0}
\end{equation}
\czq{where the first term indicates the intrinsic loss and the second term indicates the instrumental loss.} 
\czq{If user does not tend to disclose profile attribute, such privacy cost function can be simplified to the following format with the instrumental value alone.}
As mentioned in \cref{sec:plat_mech}, user tends not to disclose profile attributes $\scriptstyle \mathcal{D}_{i,a}$ due to no gains in our experiments, so we only consider behavior data here, i.e.,
\begin{equation}
    \texttt{C}_i(\si{})=\texttt{C}(\si{}) =  \frac{| {\scriptstyle \mathcal{S}_{i,b}} |}{ | {\scriptstyle \mathcal{D}_{i,b}} |}
    , %
    \label{eq:cost_function}
\end{equation}
where the $|x|$ is the number of elements in $x$.
Here, the percentage based measurement regards different amount of users' data equally. %


This reduced form specification is not unrealistic as it captures the substitution effect among personal data and incorporates the idea of constant marginal privacy cost. 
One might argue for a higher order functional to capture richer implications. 
However, there is little experimental evidence that the higher order form for privacy cost exists and how the functional form looks like.






\subsubsection{\textbf{Privacy Sensitive Weight}}
\label{sec:user_type}
For user $i$ who disclosed all her/his data (i.e., $\si{} = \di{}$), her/his privacy cost compared to not sharing any data (i.e., $\si{} = \varnothing $) is 
\begin{equation}
    \texttt{C}(\di{}) - \texttt{C}(\varnothing).
\label{eq:privacy_diff}
\end{equation}
Meanwhile, her/his anticipated recommendation utility compared to not sharing any data is:
\begin{equation}
    \texttt{U}(\di{}) - \texttt{U}(\varnothing).
\label{eq:utility_diff}
\end{equation}
We assume all users have accessed to the recommendation utility $\texttt{U}(\di{})=\texttt{U}'(\di{},\texttt{M}_{{\scriptscriptstyle \mathcal{D}}})$ computed on all the data $\di{}$ and the recommendation utility without their data $\texttt{U}(\varnothing)$ before they can take data disclosing actions, which can be regard as a prior knowledge, like the experiences before the platform adopted our framework.
With \cref{eq:privacy_diff} and \cref{eq:utility_diff}, we define the privacy sensitive weight $\lambda_i$ as: 
\begin{equation}
    \lambda_i = w_i  * \frac{\texttt{U}(\di{})  - \texttt{U}(\varnothing) } {  \texttt{C}(\di{}) - \texttt{C}(\varnothing) },
    \label{marginal_define}
\end{equation}
where $w_i$ indicates the diversity of user types towards privacy sensitivity.
The users with $w_i > 1$ is privacy sensitive users, as they will not be willing to disclose the corresponding data $\di{}$ if they only get $\texttt{U}(\di{})$ as before.
While users  with $w_i < 1$ are just the opposite. %
Therefore, the user's privacy sensitive weight is pre-computed, and the $\texttt{U}(\di{})$ can be regarded as the benchmark expectation of the platform.
The formulation of the privacy sensitive weight $\lambda_i$ also meets the idea from \cite{lin2019valuing}, where the heterogeneity from users' social demographic variety should also be explicitly characterized. %



\subsubsection{\textbf{Simulation Algorithm}}

As users behave rationally to find the optimal strategy with a trade-off of exploration and exploitation, it just meets the idea of the reinforcement learning algorithm. 
Therefore, we model each user as a unique agent and apply a multi-agent reinforcement learning method to simulate user possible policy adaptation. 
The recommender system is regarded as the environment to provide feedback, which is built upon the disclosed user data.
All agents' policies are optimized simultaneously by determining their actions, i.e., the disclosed data $\scriptstyle \mathcal{S}^t$ at simulation epoch $t$, which is used to train the recommendation model $\texttt{M}_{{\scriptscriptstyle \mathcal{S}}^{t}}$.
As mentioned before, users tend to find an optimal action over possible action space $\Pi$ to maximize his expected reward, which is determined by all agents in this dynamic MARL environment. 


We assume each user (agent) realizes this situation that the immediate reward is the result of all agents, but no communication or observation among agents is permitted. 
Then, each agent is concerned about her/his own utility and regards the environment as a dynamic system that is partially correlated to herself/himself. 
Now, it is simplified to a Multi-Armed Bandit problem~\cite{katehakis1987multi}.


However, the challenge of the exploration and explication problem also exists in our simulation. 
To address it, we adopt a simple but efficient method, Epsilon Greedy~\cite{sutton2018reinforcement} algorithm, to simulate user's policy $\pi_i$ as following, %
\begin{equation}
     \alpha_i^{t+1} = \left\{
\begin{array}{l l }
\alpha_i \sim \texttt{P}^t_i,
& \text{with possibility } \epsilon \\
\argmax_{\alpha_i} Q_i^t(\alpha_i), & \text{with possibility }  1-\epsilon \\
\end{array} \right. 
    \label{epsilon_greedy}
\end{equation}
where $Q_i^t(\alpha)$ is the user $i$'s estimation value at simulation epoch $t$ on action $\alpha$, and $\texttt{P}^t_i$ denotes a random sample policy.
To conduct an efficient policy exploration, we sample a less explored action with a higher possibility as following,
\begin{equation}
    \texttt{P}^t_i(\alpha) = \frac{ 1/ (N^{t-1}_i(\alpha) +1) } { \sum_{x \in \Pi} 1/(N^{t-1}_i(x) +1) },
    \label{random_rule}
\end{equation}
where $N^{t-1}_i(\alpha)$ represents the total number of action $\alpha$ was taken by user $i$ from start to the last simulation epoch $t{-}1$.
In convenience, we adopt the approximated expected estimation results and 
update it with the residual between the estimation $Q_i^{t-1}(\alpha_i^{t-1})$ and immediate reward  $\texttt{R}_i^{t-1}$ when she/he takes action $\alpha_i^{t-1}$ as following.
\begin{equation*}
       Q_i^t(\alpha) {=} \left\{\!\!\!
\begin{array}{l l}
Q_i^{t-1}(\alpha), &  \text{if } \alpha_i^{t-1} {\neq} \alpha \\
Q_i^{t{-}1}(\alpha) {+} \frac{1}{N^t_i(\alpha)} \bigl(\texttt{R}_i^{t-1}(\! \alpha {\otimes} \mathrm{\delta}(\di{})  ) {-} Q_i^{t{-}1}(\alpha) \bigr), &  \text{if }  \alpha_i^{t{-}1} {=} \alpha \\
\end{array} \right. 
\end{equation*}
where $\texttt{R}_i^{t-1}$ is user $i$-th immediate objective at simulation epoch $t{-}1$, computed by \cref{eq:framework}. 
$Q_i^0(\alpha) $ is the user $i$'s initial expected reward if she/he takes action $\alpha$. 
which is initialized to $0$ as users have no prior about their behaviors on the new dynamic environment.


In our simulation, we set initial $\epsilon=0.5$ for all agents and decay a half during the MARL training processing. The detailed decay epoch is co-related to the size of possible action space $\Pi$.
Here, we define it as $\epsilon = 0.5^{ t /(3 * |\Pi |) }  $, 
where $t$ is the epoch during the reinforcement learning training processing. 

\subsection{\textbf{Discussion}}
To figure out how the platform mechanism affects users' behavior, we turn to the simulation built upon several simplified assumptions. 
One fundamental assumption is the hypothesis of rational man, where users will seek their optimal policies to maximize their objectives. 
However, in the real-world scenarios, human behaviors are also affected by psychological factors, which should also be modeled in future work.
One detailed example is that some users may feel exhausted digging out all the potential privacy choices with the provided platform mechanism.
In our simulation, we assume there remains no mental cost when a user adjusts his policy. 
However, in the reality, some users may refuse to change their policy frequently, especially in complex user interaction applications.
For such situation, a convenient user interface (UI) could be a potential solution to mitigate users' fatigues. 
\czq{
Another important factor is that users may adjust their trust level towards the platform during their exploration. One detailed example is that if the platform or even the recommender system \cite{zhang2022pipattack} is easy to be attacked or the platform will abuse their disclosed data to other applications, they may re-consider their privacy sensitivity. 
Though some works have discussed the utilization of trusted platform or the privacy-preserved recommendation model, the possible effects on user psychological factors might be tackled by a dynamic modeling on the user privacy sensitive weights, which is out of the scope of this work.
}
We simplify the influences of the psychological factors in this work and leave the exploration of psychological effects in mechanism designs and UI designs for future works. 




