\section{Introduction}

Recommender systems play an essential role on today's web service platforms, e.g., e-commerce~\cite{Linden:IC03:Amazon,xie21explore} and social media~\cite{Covington:recsys16:Deep,Ying:kdd18:Graph}, since they can reduce users' cognitive load by automatically offering personalized services that match their interests and needs \cite{chin2007information}.
While the recommender systems greatly facilitate the distribution and acquisition of information, they also bring critical privacy concerns due to unsolicited gathering users' demographical and behavioral data ~\cite{Smith:mis96:Information,zhang2021membership}. %
Several regulations have been proposed recently to better protect personal data, e.g., General Data Protection Regulation (GDPR) in the European Union and the California Privacy Rights Act (CPRA) in the United States. %


On the one hand, various privacy-preserving methods have been proposed to protect users' data from leakage or abuse, like federated learning~\cite{Lin:sigir20:Meta,Muhammad:kdd20:FedFast,Qi:emnlp20:Privacy,Minto:recsys21:Stronger} and differential privacy (DP)~\cite{McSherry:kdd09:Differentially,Berlioz:recsys15:Applying,shin2018privacy,Gao:sigir20:DPLCF}.
On the other hand, to comply with laws like GDPR and CCPA, most platforms now provide users the choice to opt-in (under GDPR) or opt-out (under CCPA) of data disclosure.
Previous studies have shown that giving users control over their data closure can reduce their privacy concerns~\cite{Zhang2014-oa,Chen:CHI18:This}.
However, in practice, these platforms mostly only provide a very coarse-grained option for each user, named ``all or nothing'' binary mechanism in this paper, i.e., disclosing all data or none at all.
Usually, if the users choose to not disclose their data, they will either not be able to continue using the applications \footnote{Some applications will refuse to serve the users who refuse to disclose data by turning off the software when users clicks the refusal button, so as to force users to approve their data disclosure disclaimer and collect their data.} or not be able to enjoy the precisely personalized services.




This raises a question, \textit{is such an ``all or nothing'' binary mechanism the optimal choice?}
Obviously, some privacy sensitive users might choose not to disclose their data.
In this case, these users cannot
enjoy the benefits of personalized services.
At the same time, the platform revenues from these privacy sensitive users will decrease due to the disappointed personalized services and the platform also loses their data to train a better model.
It seems that such a strict mechanism might not be a satisfying choice for both parties in the ecology.
Therefore, we wonder whether there exists a better mechanism to take both the gains of the users and the revenues of the platform into account?




This paper aims to study how different privacy mechanisms affect users' decisions on information disclosure and how their decisions affect the recommendation model's performance and the platform's revenue.
For this purpose, we propose a privacy aware recommendation framework under \textit{privacy calculus theory}~\cite{Laufer:si77:Privacy,Culnan:os99:Information}.
Under this new setting, users need to calculate the trade-off between the anticipated privacy risks and the potential utilities, then \textit{proactively control which data to disclose}. 
In this way, all users' dispersed privacy preferences are fully accommodated.


For service providers, they naturally want to entice users to disclose as much data as possible.
For end users, they want to figure out how to enjoy the benefits of personalized services with minimal privacy risks.
Formally speaking, under this privacy aware recommendation task setting, we aim to study \textit{what will happen if the platforms give users fine-grained control over their personal data}.
More specifically, we investigate questions including:
\begin {enumerate} [itemsep=5pt, topsep=6pt, label=\roman*\upshape)] %
\item How do different platform mechanisms affect users' decisions in information disclosure? Is the ``all or nothing'' binary mechanism the best choice for the platform?
\item How do different recommendation models affect users' decisions in information disclosure? Can a platform attract users to disclose more data by optimizing the model to provide better services?
\end {enumerate} 


To answer these questions, we first formulate our idea in formal settings. %
Following current researches in economics~\cite{lin2019valuing,tang2019value}, we model the privacy cost as a linear summation of the user's disclosed personal data, meaning that the user loses control over such disclosed data, which also fits a fundamental notion in privacy calculus, i.e., the control over the data. %
Then recommendation performance (e.g., NDCG) is employed as the potential utility from users' disclosed data.
To formally define user privacy decisions, we formulate the platform mechanisms using two components, i.e., data split rule and data disclosure choice space, which define the choices a user can take.
Based on these simplified settings, we now can conduct experiments with different platform mechanisms or recommendation models to find answers to the above questions.


However, there is one big challenge for directly realizing our idea in real-world applications.
Direct deployment of the proposed framework in real-world applications might seriously harm the end usersâ€™ experiences and the revenues of platforms.
To address this challenge, inspired by the success of simulation studies in recommender systems~\cite{Ie:arxiv19:RecSim,krauth2020offline,lucherini2021t,yao2021measuring}, we propose to use simulations to study the effects of the proposed framework.
Specifically, we propose a reinforcement learning method to simulate users' privacy decision making on two benchmark datasets with three representative recommendation models and three user types (i.e. different privacy sensitivity).
The experimental results show that the platform mechanism with finer split granularity and more unconstrained disclosure strategy can bring better results for both end users and platforms than ``all or nothing'' binary mechanism adopted by most platforms.
In addition to mechanism design, the results also point out that optimizing model is another option for the platform to collect more data while protecting user privacy.

Our main contributions can be summarized as following:
\begin{itemize} [itemsep=5pt, topsep=10pt,] %
    \item We study an important and new problem in recommender systems and privacy protection, the effects of platform mechanisms on users' privacy decision.
    \item We propose a privacy aware recommendation framework that gives users control over their personal data. To the best of our knowledge, this is the first work to give users fine-grained control over implicit feedback data in recommendation.
    \item We formulate the process of users' privacy decision making and the platform's data disclosure mechanisms using mathematics language. Then we instantiate the platform's mechanisms with one data split rule and three data disclosure strategies that we proposed. %
    \item We propose a reinforcement learning method to simulate users' privacy decision making. The extensive simulations are conducted on two benchmark datasets with three representative recommendation models.
    \item %
    The extensive experimental results show the effectiveness of our proposed framework in protecting users' privacy.
    The results also shed some light on data disclosure mechanism design and model optimization.
\end{itemize}












