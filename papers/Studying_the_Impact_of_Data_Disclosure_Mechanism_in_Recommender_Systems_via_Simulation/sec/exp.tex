\section{Experiments}

\subsection{Research Question}
\begin{itemize}[itemsep=5pt, topsep=12pt,]
    \item \textbf{RQ1}: How do different platform mechanisms affect the recommendation performance and the data disclosure decisions of users with different privacy sensitivity?
    \item \textbf{RQ2}: What is the role of recommendation model in this framework? Can a more accurate model attract users to disclose more data?
    \item \textbf{RQ3}: How does user population composition affects the user behavior in this framework?
\end{itemize}
    

\subsection{Experiment Setup}


\begin{table}
    \centering
    \caption{Statistical details of the evaluation datasets.}
    \label{tab:dataset}
    \begin{tabular}{lrrrrr}
\toprule
Dataset & \#User & \#Item  & \#Interaction & Density \\ \midrule
ML-100k &637&   1278   & \num{90554}   & 11.12\%   \\ 
Yelp &    8338 &  \num{35476}   &  \num{760635}  & 0.26\%    \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Dataset}
 We conduct our experiments on two real-world representative datasets which vary in domains and sparsity:
\begin{itemize}
    \item \textbf{Movielens-100k}\footnote{\url{https://grouplens.org/datasets/movielens/100k/}} (ML-100k) \cite{Harper:TIIS16:MovieLens}: This is a popular benchmark dataset for evaluating recommendation algorithms. Here, we use the version that includes 100k user ratings.
    \item \textbf{Yelp}\footnote{\url{https://www.yelp.com/dataset}}. This is a popular and continuously updated dataset for business recommendation. The version we download for the experiments in this paper is Feb. 16 2021, i.e., all review records are written before Feb. 16 2021.
\end{itemize}

 Since we focus on recommendation based on implicit feedback, we follow the common practice to convert the numeric rating or a review into implicit feedback of 1 (i.e., indicating the user interacted with the item). %
 After that, we build the behavior sequence for each user by grouping and sorting their behaviors according to the timestamps.
 To properly simulate the information disclosure decision making process, we filter out the user with less than 40 interactions and items with less than 5 interactions.
 For efficiency reasons, we further subsample the users in Yelp, resulting in a dataset with 8338 users.
The statistics of the processed datasets are summarized in \cref{tab:dataset}.




\subsubsection{Simulation Setup}

For each user, we hold out the last item of the behavior data as the test data to compute recommendation utility \cite{NCF,kang2018self,Sun:cikm19:BERT4Rec}.
The rest of the behavior data is used for training simulation, treating the last interaction data in disclosed data as validation data and the remaining disclosed data for training data.

For recommendation utility evaluation, we adopt the widely used \textit{leave-one-out evaluation} \cite{NCF,kang2018self,Sun:cikm19:BERT4Rec} protocol with  NDCG@100 computed on the whole item set as the metric.
In particular, for Yelp, we compute the sampled metric with 1000 negative samples since the large item candidate set makes the results on Yelp are too small to simulate stably.
These sampled results are consistent with the scores on the whole candidate set \cite{krichene2020sampled}.

For privacy risk function, we use disclosed data percentage measurement according to \cref{sec:privacy_cost}, which is weighted by the sensitive weight $\lambda_i$ defined in \cref{marginal_define}.
To study the data disclosure decision making for users with different privacy sensitivity, we randomly divide users into three groups (each with $1/3$ users) with different privacy sensitive levels by adjusting the $w_i$, 
\begin{itemize} %
    \item $w_i = 0$: non-sensitive user who does not care privacy at all.
    \item $w_i = 1$: normal user who weights privacy risk and recommendation results in a relatively normal way.  
    \item $w_i = 10$: sensitive user who concerns more about privacy than recommendation utility.
\end{itemize}
To acquire the sensitive weight $\lambda_i$, the benchmark recommendation result $\texttt{U}(\di{})$ is computed based on GRU model with the whole dataset.
The assumption here is that the non-privacy aware framework that the user used before was based on GRU4Rec.
\czq{As this work focus on the effects on the user disclose choices, we also assume that there exists no privacy leak among data transmission period and users only access to their private recommendation results and the platform recommendation models are well protected with privacy guarantee. 
}
It is worth noting that we group users into three categories here just for the convenience of analyses and discussions in subsequent experiments.
In fact, according to \cref{marginal_define}, users in each category still have different privacy sensitivity $\lambda_i$.  

In the RL training, we model each user as an agent following the setup, and train 400 epochs with Epsilon Greedy algorithm. 
In each simulation epoch, the recommendation model is trained from scratch as discussed in \cref{assumption:forget}, i.e., the platform can only use the data that the user disclosed in the current simulation epoch.
The simulation epoch is enlarged to 3000 epochs in
``separate'' data disclosure strategy with $p=1/8$ due to the slow convergence.

\subsubsection{Recommendation Model}
To study the role of different models in users' data disclosure decision making, we conduct the simulations on different models, including two state of the art sequential recommendation models  and one CF model.
\begin{itemize}
    \item \textbf{GRU4Rec}~\cite{Hidasi:ICLR2016:gru4rec}: It uses GRU with ranking based loss to model user sequences for session based recommendation. 
    \item \textbf{BiSA} (\textbf{Bi}directional \textbf{S}elf-\textbf{A}ttention) \cite{kang2018self,Sun:cikm19:BERT4Rec}: It uses a self-attention architecture to capture users’ sequential behaviors and  achieves state-of-the-art performance on sequential recommendation. It usually can obtain better results than GRU4Rec with more powerful architecture.
    \item \textbf{NCF} \cite{NCF}: NCF models user–item interactions with a multi layer perceptron. It is included as a weaker model since it is not designed to capture the sequential information in user \czq{behaviors}.
\end{itemize}

We implement these models using \texttt{PyTorch}\footnote{The source code will be released after the review phase.}.
The \czq{hyper-parameters} are carefully tuned using a grid search to achieve optimal performances.
After tuning, the embedding size and hidden size is set to 128 for all the models, the dropout ratio is set to 0.2, the learning rate equals to 1e-3 for the models except BiSA with a learning rate 3e-4, and the number of negative samples is set to 16.
All models are trained with adam optimizer~\cite{kingma2015adam} with early stop.

\subsection{Study 0: Impact of User Profile Attribution}
\czq{Before conduct our experiment, we validate whether user feature perform a essential part of our latter experiments with Movielens-100k for example.
In Movielen, we include all provided 4 user profile attributes: sexual, age, job, and zip code.}
\czq{We firstly conduct experiments on the situation where all users submit their historical data with and without user profile with three mentioned models in the following tables}
\input{tabs/profile_rec_results}
\czq{From the \cref{tab:profile_rec_results} we observe that the user profile attributes introduce minor recommendation improvements on all three models if the recommendation systems have already absorbed enough user information from their historical data.}
\czq{In other words, when the platforms have already collected enough information for the recommender systems, users have minor incentives to disclose their user profiles attributes.}
\czq{we secondly conduct experiments to simulate the circumstances where all users are able to determine whether to disclose their history behavior data along with their profile attributes.}
\input{tabs/profile_gru_results}
\czq{In the following experiments, the guarantee that the recommender system collect sufficient user information may not hold, where we conduct simulations with the ``continuous'' data spilt rule on GRU models under 3 different granularity. 
In simplify, we set the $\beta_i=1/|{\scriptstyle \mathcal{D}_{i,b}} |$ according the \cref{eq:cost_function0}.
As we import the user profile attribution, the action spaces of agents are enlarged, which impose the convergence challenges.
Therefore, we restrict the users whose disclosure decision have at least 0.01 improvements on their utility than not to disclose.
The converged results are displayed on the \cref{tab:profile_gru_results} where we record the users who are willing to disclose their historical behavior data or their profile attributes.
From \cref{tab:profile_gru_results}, we can conduct two conclusions: First, barely users are stick to disclose their profile attribution among different data split granularity;
Second, the user disclosure policies towards their historical data are similar no matter their profile attribution is introduced or not. The converged number of the users who are willing to disclose their historical are similar from \cref{tab:profile_gru_results}. Regarding the slight influences on the user profile attributes, the introduce of the user profile attributes will directly enlarge the user possible action space, which impose more challenge on the multi-agent reinforcement learning processing and may require much more training cost. }

\czq{As a results, we only conduct experiments on user historical data and assume all users are refuse to disclose their user profile attribution on the latter experiments. 
}




\subsection{Study 1: Impact of Platform Mechanism}


We firstly conduct experiments on platform mechanisms specifying various data split granularity and data disclosure strategies with a widely-used sequential recommendation model GRU4Rec. 
We begin by answering which mechanism is preferred by users with different privacy sensitivity (types, hereafter). 

\input{tabs/strategy_eric}


\subsubsection{Split Granularity $p$}
We first validate how the split granularity affects users on the three aforementioned data disclosure strategies.
The recommendation results and the data disclosure percentage on different user types are reported in \cref{Rec_results} where all results are averaged on the last 20 epochs after convergence.
From the results, it can be observed that:
\begin{enumerate} [itemsep=4pt]
    \item Comparing the NDCG performances among different settings, we can derive a negative answer for the question in the introduction considering ``all or nothing'' binary choice (i.e., $p{=}1$) performs worst on all disclosure strategies for all datasets.
Even looking at the detailed results for user groups with different privacy sensitivity, $p{=}1$ still performs very poorly, if not the worst.
\item Comparing the results within different user groups, a prominent and expected result is that users who care about privacy are only willing to disclose very little data, especially privacy sensitive users.
Besides, normal users can obtain comparable recommendation results (even better on ML-100k) to non-sensitive users with much less data.
On the one hand, this indicates that our proposed framework can effectively protect users' privacy.
On the other hand, proactively controlling the disclosed data also allows users to improve their recommendation results by themselves.
\item For platform, finer split granularity can usually bring better performances in all three mechanisms for all datasets.
Unexpectedly, these superior performances are not always obtained through more disclosed data.
For example, under ``latest continuous'' rule, the overall recommendation performances for $p{=}1/16$ (16.31\% on ML-100k) are much better than $p{=}1/2$ (13.89\% on ML-100k) with less training data (40.45\% vs. 43.57\% on ML-100k).
\end{enumerate}



To figure out the reason for this phenomenon, we analyzed the distribution of user' data disclosure.
We reported the percentage of users who disclosed data in \cref{tab:sharing_result}.
The results show that more users turn to disclose data since finer granularity allows users to disclose a small amount of data for certain recommendation utilities.
Conversely, more users suffer from poor recommendations as they refuse to disclose data under coarse-grained granularity.

\input{tabs/data_per_eric}










\subsubsection{Data Disclosure Strategy}
\input{figs/smooth}
We study how data disclosure strategy affects users' decisions using three strategies with different degrees of freedom. %
The results are also reported in \cref{Rec_results}.
\input{tabs/model_eric}

It is easy to see that the flexible ``separate'' strategy is superior to other mechanisms within the same granularity. 
The ``separate'' strategy achieves better overall recommendation results with similar or even less disclosed data. %
One possible reason is that it enables users to freely disclose the data that benefits their recommendations.
In this way, users will discard those data that are not helpful for their recommendations, which is equivalent to data optimization by users.
It also explains why ``separate'' with $p{=}1/8$ outperforms ``all'' in ML-100K.
These results are consistent with the research in data minimization~\cite{chow2013differential,Wen:recsys18:Exploring,biega2020operationalizing}.


For the other two strategies, i.e., \czq{``latest continuous'' }and ``oldest continuous'' , the results show they perform not very consistently in different datasets.
This could be caused by the characteristics of different datasets.
It reminds us to design data disclosure mechanisms carefully according to the characteristics of data we deal with in real-world applications.







In summary, the platform mechanism affects both the possibility of a user to disclose and the volume of her/his disclosed data. 
Normally, a finer granularity and a more free data disclosure strategy can improve recommendation results for users and better protect users' privacy at the same time. 
However, the price is that a large action space leads to slow convergence on the optimal user policy. 
As shown in \cref{fig:granularity}, ``separate'' with $p{=}1/8$ (128 possible options) converges much slower than other mechanisms.
It indicates users might be hard to find their best policies under these fine-grained mechanisms.
Thus, we constraint users' possible choice number to 16 for all mechanisms for fair comparisons in the latter experiments.











\input{tabs/user_composition}

\subsection{Study 2: Impact of Recommender}


This study answers whether a better model (BiSA) or a worse model (NCF) will attract users to disclose more data or not. 
\cref{models_results} reports the results on three different data disclosure strategies with optimal granularity $p=1/16$ except the $p=1/4$ on ``separate'' for keeping the same number of data disclosing choices. 
It can be observed that:

\begin{enumerate}
    \item The results show that a more powerful model BiSA can attract sensitive users to disclose more data by improving the recommendation results for all platform mechanisms on all datasets.
    \item Though a better model usually can incentive users to disclose data, the total volume of data disclosed by normal users is not always increased. 
One reason is that marginal recommendation utility by disclosing more data may decrease on a better model considering the model already predicted precisely based on the disclosed data.
This phenomenon is prominent on the BiSA in ``latest continuous'' considering a better sequential model may rely less on older behavior data~\cite{kang2018self,Sun:cikm19:BERT4Rec}.
\end{enumerate}






In summary, 
the \cref{models_results} results suggest that the platform may pay more attention towards mechanism optimization while always stick to a better recommender system.













\subsection{Ablation Study}
Here, we study the impacts of compositions of user groups and the privacy sensitive hyper-parameter $w_i$. %
Due to page limitation, we only report the results based on GRU4Rec on ML-100k.

\subsubsection{User Group Compositions}

In this subsection, we adjust the user group composition where each user in the dataset is non-sensitive/normal/sensitive.
The average percentage of disclosed data and recommendation results are reported in \cref{tab:user_composition}.
The platform can get higher revenues when users are less concerned about their privacy risks.
Moreover, user group compositions play a more critical role in the platform's revenue than the data disclosure mechanisms.
This encourages the platform to take more actions on privacy protection to prevent users from becoming sensitive.







\subsubsection{Privacy Sensitive}

\input{figs/lambda}
We report the effects of the hyper-parameter $w_i$  in \cref{fig:lambda}.
The results show that all mechanisms perform quite stable on both recommendation results and data disclosure with different hyper-parameter $w$, especially when $w>5$.
The reason for these results is that a user will barely disclose their data unless she/he observes a significant improvement on the recommendation results when her/his privacy sensitivity is very high (e.g., $w > 5$).
This also demonstrates that the conclusions of our previous experiments are stable.

\subsection{Summarized Insights}
\czq{
In this subsection, we summarize several mentioned insights as below.
}
\begin{enumerate} [itemsep=4pt]
\item \czq{We derive a negative attitude towards the  ``all or nothing'' binary mechanism due to its worst performances compared to other proposed mechanisms.}
\item \czq{The platform mechanism affects both the possibility of a user to disclose and the volume of her/his disclosed data. 
A finer granularity mechanism will normally attract more users to disclose while the volume of her/his disclosed data and the recommendation performances are not always monotonically increasing. }
\item \czq{Though a better model usually can incentive users to disclose data, the total volume of data disclosed by normal users is not always increased due to the marginal effects, which suggests the platform may pay more attention towards mechanism optimization based on a specific recommender system rather than always stick to the recommender system optimization.}

\end{enumerate}
