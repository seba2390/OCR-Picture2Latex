\vspace{-7mm}
\section{Conclusions}
\vspace{-4mm} This paper has presented a machine-learning based predictive model to dynamically choose the sparse matrix storage format and
the associate computation kernel during GNN execution. Our model uses numerical features to characterize the input matrix to predict
the storage format to use for the next GNN layer. We evaluate our approach by applying it to five representative GNN models
running on a multi-core CPU using both real-world and synthetic datasets. Experimental results show that our approach gives an average
speedup of 1.17x (up to 3x) over the Pytorch default strategy and exhibits a good generalization ability.
