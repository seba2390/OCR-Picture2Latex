\vspace{-4mm}
\section{Experimental Results}
\vspace{-3mm}
\subsection{Overall Results}
\vspace{-2mm}

\begin{figure}[t!]
\centering
\subfigure[Per GNN model] {
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{min_max_bar.pdf}\\
        \label{fig:min_max_bar}
    \end{minipage}%
}%
\subfigure[Per Dataset] {
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{evaluation.pdf}\\
        \vspace{0.05cm}
        \label{fig:evaluation}
    \end{minipage}%
}%
\vspace{-3mm}
\caption{Speedup given by our approach over COO. GeoMean represents the geometric mean given by the previous performance. }
\vspace{-3mm}
\label{fig:MinMaxBar}
\end{figure}




Figure \ref{fig:min_max_bar} shows the speedup over the PyTorch COO sparse matrix storage format for each GNN model across our evaluation datasets. Here, the min-max bar show the variance across the evaluated datasets. In this experiment, we aim to optimize for speedups by setting $w$ of Eq. \ref{equ:nomarlization}. Moreover, in Section \ref{sec:optp} we show our approach can generalize to other settings of $w$.

As can be seen from the diagrams, choosing the right sparse matrix storage format can improve the GNN performance. Our approach delivers an average speedup of 1.3x (up to 3x) on GCN, which involves many SpMM computations when performing the graph convolution operations. Our approach gives less performance improvement on RGCN because the dataset that RGCN operates is a dense edge-based dataset that does not benefit from sparse matrix format selection. Furthermore, on a small number of datasets, where the COO is the best format, our approach shows a minor slowdown, less than 7\%, due to the overhead of feature extraction. But for the majority of the evaluated datasets, our approach gives a noticeable improvement over COO. Overall, our techniques give an average speedup of 1.17x across GNN models and evaluation datasets.

Figure \ref{fig:evaluation} shows the achieved performance per real-world graph dataset across models. For most of the datasets, our approach gives noticeable speedups across GNN.

\vspace{-2mm}
\subsection{Compare to Prior Methods}
\vspace{-2mm}
Table \ref{tab:sota_compare} compares our approach against a CNN and a decision tree model for choosing the matrix storage format, where our approach gives a better overall prediction accuracy. The CNN model gives a poor prediction accuracy when the model is trained on 300 synthetic matrices. While the performance of the CNN model can be improved by using more training data, doing so would incur a higher overhead. Table \ref{tab:sota_compare} confirms that a higher prediction accuracy does translate into better speedup performance, where our approach improves the CNN and the decision tree model by 27\% and 3\%, respectively.

\begin{table}[t!]
\caption{Comparing our XGBoost approach with prior work}
\vspace{-2mm}
    \centering
    \scriptsize
    \begin{tabular}{lrrr}
    \toprule
    \textbf{Model} & \textbf{Inference Time (s)} & \textbf{Prediction Accuracy (\%)} & \textbf{Realized Speedup}\\
    \midrule
    XGboost (ours) & 0.0008 & 89.1 & 1.17\\
    CNN \cite{zhao2018bridging,pichel2019sparse} & 0.002 & 66.8 & 0.86\\
    Decision-Tree \cite{sedaghati2015automatic} & 0.0002 & 83.8 & 1.14\\
    \bottomrule
    \end{tabular}
    \label{tab:sota_compare}
    \vspace{-3mm}
\end{table}

\vspace{-2mm}
\subsection{Compare to Oracle Performance\label{sec:oraclp}}
\vspace{-2mm}
\begin{figure*}[t!]

\begin{minipage}[t]{0.5\linewidth}
\includegraphics[width=\textwidth]{oracle.pdf}
\vspace{-5mm} \caption{Performance of our approach related to the Oracle performance.} \label{fig:oracle}
\end{minipage}
\begin{minipage}[t]{0.5\linewidth}
\includegraphics[width=1\textwidth]{normalized_acc.pdf}
\vspace{-6mm} \caption{Prediction accuracy of our approach when varying $w$ in Eq \ref{equ:nomarlization}. } \label{fig:normalized_acc}
\vspace{-9mm}
\end{minipage}
\vspace{-3mm}
\end{figure*}


Figure \ref{fig:oracle} compares our approach against \emph{a theoretically perfect predictor} for storage form selection, for which we call \emph{oracle}. We obtain the oracle performance by exhaustively profiling all candidate storage formats for each GNN layer to find out the best-performing format. The results show how close our predictive modeling approach is to the theoretical upper bound. Our approach achieves, on average,  89\% of the oracle performance. Our model can be further improved by using more training samples together with more representative features to characterise some of the input matrices better to improve the prediction accuracy.

\vspace{-3mm}
\subsection{Model Analysis\label{sec:optp}}
\vspace{-3mm} \cparagraph{Impact of optimization goal.} Our evaluation so far set $w$ to 1 of our optimization function
(Eq.~\ref{equ:nomarlization}) by solely optimizing for speeds. Figure \ref{fig:normalized_acc} shows prediction accuracy when we vary the
parameter settings. Our approach has a good generalization by giving the average accuracy of 90\%. This experiment shows that our approach
is flexible and can adapt to different optimization trade-offs.



\begin{figure}[t!]
\centering
\subfigure[Prediction Accuracy] {\label{fig:predictionAcc}
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{predictionAcc.pdf}\\
    \end{minipage}%s
}%s
\subfigure[Inference Time] { \label{fig:inferenceTime}
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{inferenceTime.pdf}\\
        \vspace{0.2cm}
    \end{minipage}%
}%
\centering
\vspace{-4mm}
\caption{Comparing our XGBoost model against alternative modeling techniques.}
\label{fig:alternatemodel}
\vspace{-5mm}
\end{figure}

\cparagraph{Alternative modeling techniques.} Figure \ref{fig:alternatemodel} compares our XGBoost-based predictor against three other
classification methods used in prior works for code optimization~\cite{wang2018machine}:  MLP neural network \cite{gardner1998artificial},
KNN (with $k=1$) \cite{zhang2007ml}, and SVM \cite{noble2006support}. All the alternative techniques were trained and evaluated using the
same method and training data as our model. In this experiment, we consider the model prediction accuracy and the time for making a
prediction. As can be seen from the diagram, our approach has the lowest runtime overhead while giving the highest accuracy when compared
to alternative modeling techniques. Since XGBoost is a decision-tree-based model, it also has the advantage of being interpretable because
its decision process can be followed by traversing the tree.


\cparagraph{Training and deployment overhead.} Training of our predictive model only needs to be performed once, after which the trained model can be applied to any matrices. Training is dominated by the generation of training data which takes in total less than a weekâ€™s machine time (Section \ref{sec:dg}). We can speed this up by using multiple machines. The overhead for learning the XGBoost model is negligible, less than 5 minutes.
 Our approach has a negligible runtime overhead compared to the GNN kernel execution time, the overhead of feature extraction and prediction is less than 3\% to the end-to-end kernel execution time.

\vspace{-4mm}
\subsection{Discussion}
\vspace{-3mm}
%There is room for improvement, and we discuss a few points here:

\cparagraph{Supporting other storage formats.} Our approach can be easily extended to support other sparse matrix storage formats. As we
formulate the storage format prediction as a classification problem, this can be achieved by adding a new class label (for the newly
supported format) into our training dataset. Doing so would also require providing the relevant SpMM kernel implementation. Other than
these, a large part of the training process and deployment can remain unchanged.

\cparagraph{Supporting GPU computation}. This work focuses on the CPU execution of GNN models due to the large graph datasets that a GNN
model typically processes. There are methods to support large-scale graph processing on GPUs such as GraphSAGE
\cite{hamilton2017inductive}. Our approach can be ported to support GPU processing. This will require using training data collected from
the targeting GPU to train our predictive model. %An interesting challenge is how to leverage the ideal CPU core to predict the storage
%format to use for the next GNN layer and perform the format translation ahead of time. Care should also be taken to amortize the overhead
%of CPU and GPU data communication.

\cparagraph{Optimize SpMM algorithms.} Optimizing SpMM computation is an active research field \cite{dalton2015optimizing}. It is interesting to investigate how the SpMM computation kernel can be tailored for GNN computation and what parameters can be opened to a tuning framework. As the best algorithm parameters are likely to change depending on the matrix input and the underlying hardware, an automatic machine learning-based approach similar to our approach is highly attractive.
