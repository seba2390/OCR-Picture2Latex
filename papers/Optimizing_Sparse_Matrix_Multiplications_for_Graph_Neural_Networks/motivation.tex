\vspace{-3mm}
\section{Motivation} \label{moti}
\vspace{-2mm}
\begin{table}[t!]
\caption{Input matrix sparsity from graph datasets}
    \centering
    \scriptsize
    \begin{tabularx}{\textwidth}{XXXX}
    \toprule
    \textbf{Name} & \textbf{Adj. Matrix Density} & \textbf{Adj. Matrix Size} & \textbf{Node Feature Vector Dimension}\\
    \midrule
    \rowcolor{Gray} CoraFull & 0.6\% & $19,793\times8,710$  & 19,793\\
    Cora & 1.27\% & $2,708\times1,433$ & 2,708\\
    \rowcolor{Gray} DblpFull  & 0.31\% & $17,716\times1,639$ & 17,716 \\
    PubmedFull & 10.02\% & $19,717\times500$ & 19,717\\
    \rowcolor{Gray} KarateClub & 2.94\% & $34\times34$ & 34\\
    % Mix Hop Synthetic & 1 & $5000\times2$ & 2\\
   % \rowcolor{Gray} Entities (AIFB) & 1 & $58,086\times2$ & 58,086 (edge type)\\
    \bottomrule
    \end{tabularx}
    \label{tab:Dataset_Detail}
    \vspace{-1mm}
\end{table}


\begin{figure}[t!]

\begin{minipage}[t]{0.5\linewidth}
\includegraphics[width=\textwidth]{beststoring_withdiffdataset.pdf}
\caption{The best-performing storage format per dataset.}
\label{fig:beststoring_withdiffdataset}
\vspace{-1mm}
\end{minipage}
\begin{minipage}[t]{0.5\linewidth}
\centering
\includegraphics[width=\textwidth]{LayerSparsity1.pdf}
\caption{Changes of the adjacency matrix density over GNN training epochs. }
\label{fig:LayerSparsity}
\end{minipage}
\vspace{-4mm}
\end{figure}

\vspace{-2mm}
As a motivating example, consider applying a
two-layered  graph convolution network (GCN) model ~\cite{kipf2016semi} to 5 real-life graph datasets (Table \ref{tab:Dataset_Detail}) using the 7 sparse matrix storage formats described in Section \ref{sec:sf}.

\vspace{-4mm}
\subsection{Setup}
\vspace{-2mm}
In this experiment, we consider five real-life graph datasets used in prior work
\cite{bojchevski2017deep}. Table \ref{tab:Dataset_Detail} summarizes the size and sparsity of the graph adjacency matrix, and the
dimension of the node feature vector (a dense vector). We
run the GCN model on a 2.0 GHz 20-core Intel Xeon CPU. We note that it is common to run a GNN on the CPU due to the large
memory footprint of graph processing \cite{bojchevski2017deep}.

\vspace{-4mm}
\subsection{Results}
\vspace{-2mm}
Figure \ref{fig:beststoring_withdiffdataset} shows the best-performing sparse matrix format for each dataset, when a format is used to encode the initial model input and used throughout the model training process. Here, we normalize the measured runtime against the time of the PyTorch-geometric default COO format. While COO gives the best
performance on \texttt{DBLPFull}, it leaves much room for performance improvement on other datasets. Furthermore, we also observe that the best-performing storage format varies depending on the input
dataset.

\begin{figure}[t!]
\centering
\subfigure[CoraFull] {\label{fig:CoraFull_b}
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{CoraFullSpeedup2.pdf}\\
    \end{minipage}%
}%
\subfigure[PubmedFull] { \label{fig:pubmedfull_b}
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=1\textwidth]{pubmedfullspeedup2.pdf}\\
        \vspace{0.05cm}
    \end{minipage}%
}%
\vspace{-3mm}
\caption{Performance improvement over the PyTorch-geometric default COO format on the CoraFull (a) and PubmedFull dataset (b) when using different sparse matrix format
to store the output of the first GNN layer.}
\label{fig:CoraFull_pubmedfull_b}
\vspace{-4mm}
\end{figure}

If we now consider Figure \ref{fig:LayerSparsity}, we see that the density of the input matrix increases as we iterate over the GNN model
on the \texttt{CoraFull} dataset. This is expected as a GNN tries to incorporate further neighbourhood information by iterating over the
graph, which in turn increases the reach and information propagation of a graph node. As can be seen in figure \ref{fig:CoraFull_pubmedfull_b}, CSR is the best format used to store the
neural network input (i.e., the feature and the adjacency matrix) for both the \texttt{CoraFull} and \texttt{PubmedFull} datasets. Thus, for a
model with a single layer GNN, CSR might be the best storage format. However, for a typical GNN model with multiple GNN layers, the
sparsity of the matrices processed by the latter layers can change, calling for a different storage format to be used. Specifically, for
\texttt{CoraFull} (figure \ref{fig:CoraFull_b}) used in our setting, using CSC, LIL and DIA after the first GNN layer can also give a
relatively good speedup over COO, but these format give no benefit on \texttt{PubmedFull} (Figure \ref{fig:pubmedfull_b}) because of the
changing distribution of the non-zero elements, the details can be seen in figure \ref{fig:CoraFull_pubmedfull_b}.

\cparagraph{Lesson learned.} This example shows that choosing the right sparse matrix storage format can have a significant performance
benefit, but the choice depends on the input data and the GNN layers. Therefore, the decision for storage format should be made on a per GNN layer basis during runtime.
