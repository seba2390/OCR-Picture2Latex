\vspace{-3mm}
\section{Background}
\vspace{-2mm}
\subsection{Graph Neural Networks}
\vspace{-2mm}

A GNN operates on a graph structure, where each graph node is associated with a
$d$-dimensional feature vector of numerical values known as embeddings. Edges between nodes indicate their relationship, quantified with edge weights. 
For a graph with $N$ nodes, the graph edges
are encoded in an $N \times N$ adjacency matrix, $A$, and the node embeddings are stored in an $N \times d$ feature matrix, $X$. 

Like most neural networks, a GNN model can have multiple layers. Each layer is represented by two functions: i) an aggregation function
and ii) an update function (i.e., a combination function). During training, a GNN takes as input the adjacency matrix, $A$, of the graph.
It then uses a neighbourhood aggregation scheme to update the feature vector of each graph node based on the feature vector of its
neighboring nodes. Feature aggregation is performed by first applying the aggregation function (e.g., reductions)
to collect the features of the neighbours for a given node and then updating each node's feature vectors using the updating
function. After repeating this process of updating node features for a fixed number of times, a readout function is
applied to aggregate the feature matrix to a single numerical vector to be used as the graph representation.

%\paragraph{GNN computation.} 
The aggregation and update functions used by a GNN layer are implemented using matrix multiplications.
Because the graph adjacency matrix, $A$, is sparse in many real-life graphs, the GNN matrix multiplications are often realized as SpMM to reduce the memory footprint and processing time \cite{huang2021understanding}. When profiling 5 representative GNN models (Section \ref{sec:platform}) on real-life datasets, we find that SpMM can account for 95\% of the GNN processing time.

\vspace{-3mm}
\subsection{Sparse Matrix Storage Formats\label{sec:sf}}
\vspace{-2mm}
Our work considers the following commonly used sparse matrix storage formats:

\cparagraph{COO.} The coordinate list (COO) stores a list of (row, column, value) tuples of non-zero elements. This is the default storage format used by PyTorch-geometric \cite{fey2019fast} for graph processing. 

\cparagraph{CSR.} The compressed sparse row (CSR) format uses three arrays to represent non-zero matrix elements, that respectively contain non-zero values, the beginning position of each row, and the column indices of non-zero elements. CSR is similar to COO, but compresses the row indices, hence the name.

\cparagraph{CSC.} The compressed sparse column format (CSC) is similar to CSR, with one exception for using an array to store the target matrix's row indices of non-zero elements instead of column indices as in CSR.

\cparagraph{DIA.} The diagonal format (DIA) stores non-zero elements along the diagonal direction of a matrix into a row of a 2-dimensional array. It is best suited for non-zero elements that appear along the diagonals of a matrix.

\cparagraph{BSR.}
The block sparse row format (BSR) evenly divides the input matrix into blocks. It is CSR with dense sub-matrices of fixed shape instead of scalar items.


\cparagraph{DOK.}
The dictionary of keys format (DOK) stores key-value pairs $<$(row,column), value$>$ in a dictionary (e.g., a hash table). Elements that are not presented in the dictionary are treated as zero elements.


\cparagraph{LIL.}
The linked list (LIL) format stores non-zero elements and their column indices in a linked list. This format uses a row-based linked list, where each row is a list of column indices of non-zero elements.

