\vspace{-6mm}
\section{Introduction}
\vspace{-3mm} In recent years, graph neural networks (GNNs) \cite{zhou2020graph} are shown to be effective in extracting information from graph structures like social networks with millions of nodes and billions of edges \cite{cui2018survey}. Indeed, GNNs account for over 90\% of the leading models in solving the open graph benchmark suite \cite{hu2020open,huang2021understanding}.

A GNN is designed to propagate and aggregate information across graph nodes. This is
achieved by applying a kernel function to a feature matrix of graph nodes, which captures the properties of nodes, as well as an adjacency matrix
that encodes the connectivity of graph edges. The kernel function is typically implemented using matrix multiplications \cite{zhou2020graph} that often dominate the GNN execution time during training and inference. Because most of the nodes in a real-life graph only have a small number of direct neighbors, the graph
adjacency matrix that a GNN kernel operates on is often sparse (i.e., many matrix elements are zeros). As a result, the matrix multiplication computation within a GNN is
essentially sparse matrix multiplication (SpMM) operations. 
% As we will show in the paper, SpMM can account for 95\% of the GNN execution time and cannot be ignored for performance optimization.

There is an extensive body of work in optimizing SpMM for scientific workloads \cite{gilbert2008unified}. Various sparse matrix storage
formats have been proposed to reduce the memory and computation overhead of SpMM \cite{greathouse2014efficient,langr2015evaluation}.
Studies have also shown that choosing the right storage format can have a significant impact on the SpMM performance
\cite{mehrabi2021learning}. Although SpMM performance optimization is a well-studied field in traditional high-performance computing (HPC)
domains, the benefit of sparse matrix storage format selection is unclear on the new GNN workloads. Existing deep learning frameworks like
PyTorch \cite{paszke2019pytorch} and Tensorflow \cite{abadi2016tensorflow} all use a single, static sparse matrix storage format across
graph inputs. Since GNNs are becoming an important application class, it is essential to understand how GNN performance can benefit from
sparse matrix format selection.

This paper presents the first study of sparse matrix storage selection on GNN performance. We consider five representative GNN
architectures and six commonly used sparse matrix storage formats. We empirically
demonstrate that choosing a suitable sparse matrix storage format  can  have  a  significant  performance  benefit,  but  the  right  format
changes depending on the input matrix. We show that unlike traditional HPC workloads, the matrix sparsity can change over time as the GNN iterates over the input graph; and as a result, the suitable format can vary throughout GNN execution.

In light of this observation, we employ machine learning to automatically construct a predictive model based on XGBoost \cite{chen2015xgboost} for sparse matrix format selection.
Our predictor predicts, at runtime, the sparse matrix storage format and the associate SpMM computation kernel for each GNN kernel. Our predictor is first trained \emph{off-line} using synthetic matrix data. Then, using a set of automatically tuned features of
the matrix input, the predictor determines the optimal storage format to use before entering a kernel. We showcase that our approach is
generally applicable and can adapt to various optimization goals to find different trade-offs between the memory overhead and execution
time.



We evaluate our approach by applying it to five GNN architectures running on multi-core CPUs using both real-life and synthetic graph data.
We compare our approach against two prior machine-learning methods \cite{sedaghati2015automatic,pichel2019sparse} for selecting sparse
matrix storage formats. Experimental results show that our approach gives better performance over alternative optimization strategies by
giving an average 1.17x speedup. The performance of our approach translates to average 89\% of the oracle, a theoretically perfect
predictor for storage form selection (Section \ref{sec:oraclp}). performance given by a theoretically perfect predictor.

This paper makes
the following contributions:

\begin{itemize}
\item It is the first paper to study sparse matrix storage format selection on GNN performance;
\item It shows how machine learning techniques can be employed to develop a runtime predictor for optimizing GNN sparse matrix format selection;
\item It provides quantified performance results of widely used sparse matrix storage formats on representative GNN architectures.
\end{itemize}
