\vspace{-5mm}
\section{Related Work}
\vspace{-4mm}

Several approaches have been proposed to optimize graph processing \cite{xie2020gnns}. Some provide new programming abstractions to
optimize vertex/node-centric or edge-centric  processing \cite{zhou2020graph}. For example, Pytorch-Geometric (PyG)
\cite{fey2019fast} and Deep Graph Library (DGL) \cite{wang2019deep} are two major frameworks for GNN computation. Both libraries rely on a
low-level, hand-optimized SpMM library, but they use a single sparse matrix storage format throughout the execution. %As a runtime solution,
%GNNAdvisor exploits work partitioning and scheduling techniques to accelerate distributed GNN training \cite{wang2020gnnadvisor}.
Our work complements these prior efforts by dynamically adapting the sparse matrix storage format and the associated computation kernel for
each GNN layer, which can be easily integrated with existing graph programming models.

Various sparse matrix storage formats have been proposed in the past \cite{langr2015evaluation}. Studies have shown
that there is no ``one-fit-for-all" storage format, and the right format can change from one matrix to the other
\cite{li2013smat,chen2020characterizing}. Methods have been proposed to dynamically choose sparse matrix storage format based on the input
workloads \cite{sedaghati2015automatic}. These include approaches build around analytical methods \cite{venkat2015loop} or
machine-learning-based predictive models \cite{chen2019optimizing}. The latter has the benefit of can be easily ported to different
architectures as machine learning learns from empirical observations rather than simplified assumptions used by an analytical model.
However, prior machine-learning-based solutions have been concentrated on optimizing sparse matrix-vector multiplication (SpMV) of
scientific workloads \cite{zhao2018bridging}. They choose a storage format at the beginning of the program execution but do not adjust the
format during application execution. No work so far has concerned choosing the sparse matrix storage format for GNN SpMM throughout program
execution. Our work is the first to do so.

Machine learning is a proven design methodology for systems modeling and optimization \cite{wang2009mapping,ren2017optimise,wang2018machine,zhang2018auto,wang2014integrating,zhang2020optimizing}. Studies have demonstrated
the success of applying machine learning for a wide range of code optimization tasks
\cite{tournavitis2009towards,wang2014automatic,cummins2017end,wang2010partitioning,ye2020deep,wang2020combining} In this work,
we employ machine learning techniques to develop an automatic approach to optimize GNN SpMM. We remark that our work does not seek to
advance machine learning algorithms; instead, it explores and applies a well-established modeling method to tackle the GNN SpMM
optimization problem.
