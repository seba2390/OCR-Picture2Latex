\vspace{-2mm}
\section{Experimental Setup}
\vspace{-2mm}
\subsection{Software and Hardware} \label{sec:platform}
\vspace{-3mm}
 \cparagraph{Evaluation platform.} Our hardware platform is a dual-socket multi-core server with two 20-core Intel Sky Lake Xeon Gold 6138
CPUs running at 2.0 Ghz with 192GB of RAM.
 Our evaluation platform runs Centos 7 with Linux kernel version 3.10. We test our approach on PyTorch
v1.4.0, running on the CPU.

\cparagraph{GNN models.}We apply our approach to 5 representative GNN architectures, including GCN, graph attention network (GAT)
\cite{gat2018graph}, relational graph convolutional neural network (RGCN) \cite{schlichtkrull2018modeling}, GNN with feature-wise linear
modulation (FiLM) \cite{brockschmidt2020gnn} and efficient graph convolutions (EGN)\cite{tailor2021adaptive}. We use the open-source
implementation provided by PyTorch-geometric library \cite{fey2019fast} by stacking two GNN layers to form a standard graph model.

\cparagraph{Datasets.} In our evaluation, we use two graph data suites, CoraFull \cite{xu2019crosslingual} and Entities \cite{schlichtkrull2018modeling}, containing a
total of 5 graph datasets with matrix sizes ranging from 19,793 to 58,086. To evaluate the generalization ability of our approach, we also
apply our approach to 100 synthetic matrices of different sizes and sparsity. For the synthetic data, we initialize weights in the adjacency matrices
by populating them with random single floating numbers between 0 and 1.0.

\vspace{-3mm}
\subsection{Evaluation Methodology\label{sec:priorwork}}
\vspace{-3mm} \cparagraph{Competitive methods.} We compare our approach against two closely related predictive methods for using machine
learning to choose the sparse matrix storage format. The first approach employs a convolutional neural network (CNN)
\cite{zhao2018bridging,pichel2019sparse}, and the second uses a decision tree model for format selection \cite{sedaghati2015automatic}. We
use an open-source implementation of ResNet \cite{paszke2019pytorch} as the CNN model.  To provide a fair comparison, we train all machine
learning models on the same training dataset using the methodology described in the source publications.

\cparagraph{Performance report.} We consider the end-to-end execution time, including the overhead of our predictive model (i.e., the time
spending on feature extraction, storage format transformation and model prediction). Our feature extraction process runs in parallel using
all CPU cores.   We measure the end-to-end training time by training each model on each dataset for 10 epochs. We run each matrix input 5
times and report the \emph{geometric mean} of the end-to-end training time and show the variations across different runs as a min-max bar.
Note that we only need to decide the matrix storage format once for each GNN layer across training epochs. Given that in our
evaluation, the sparse matrix distribution is similar across training epochs, and hence the overhead of our approach can be further
amortised across multiple training epochs.
