
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AUTHOR: Christos Faloutsos
% INSTITUTION: CMU
% DATE: April 2019
% GOAL: to streamline the paper presentations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% We present the overall algorithm of \method in Section~\ref{ssec:algo}.
% In Section~\ref{ssec:att}, to handle \nd, we construct our proposed \emphasis to draw attention to well-connected neighbors in Section~\ref{ssec:att}.
% With the help of \emphasis as well as \nea, we compute the compatibility matrix by our proposed closed-form formula \NEF in Section~\ref{ssec:comp}. 
% Analysis of convergence and complexity are given in Section~\ref{ssec:ta}.

% \subsection{Overall Algorithm} \label{ssec:algo}

% \reminder{FYI, I shrunk wording}
We propose \methodexp to exploit \nef for accurate and fast node classification.
First, we show how to pay attention to influential neighbors by the proposed \emphasis;
% to address noisy edges which are common in real-world graphs and can hurt the accuracy during exploitation.
% However, the noisy neighbors, which are very common in real-world graphs, can hurt the accuracy during exploitation.
% To address this issue, we first illustrate how to pay attention to only the influential neighbors, by the \emphasis.
% This also leads to a better estimation of the compatibility matrix. %, which helps in exploiting \nef.
and then we describe the details of \methodexp,
with theoretical analysis on its convergence and computational complexity.

\subsection{``Emphasis'' Matrix} \label{ssec:att}

\subsubsection{Rationale and Overview}

% \paragraph{Rationale}
% \reminder{christos will give intuition here, too}
% \reminder{lets move it AFTER the hypothesis testing, ie, to section 4.2}
% \reminder{Also, lets give a small, arithmetic example , say 5x5 matrix, before and after the 'emphasis'}
% \reminder{FYI: major changes below}.
% However, the compatibility matrix estimated with the adjacency matrix ${\boldsymbol A}$ is easily interfered with by noisy neighbors, i.e., weakly-connected pairs.
Not all neighbors are equally influential:
Best practice has shown that well-connected neighbors
(i.e., nodes `B', `C', and `D') have more influence on node `A' than the rest, as we illustrate in Fig.~\ref{fig:ematrix}.
Thus, 
% Simply exploiting \nef on the adjacency matrix can be affected by noisy neighbors, which becomes crucial when the observed labels are few.
% To address this issue, 
we propose \emphasis ${\boldsymbol A}^{*}$, a weighted version of ${\boldsymbol A}$, to pay attention to such neighbors.
% to the neighbors that are connected by higher-order structures. % in an unsupervised manner.
% shows how \emphasis successfully pays more attention to more influential edges, which are better connected in the graph.
% To incorporate the idea of \nd, where neighbors have different importances, we propose to replace the unweighted adjacency matrix ${\boldsymbol A}$ with a weighted one.
% The weight of edge $(i, j)$ reflects the influence of node $i$ for $j$.
% Previous studies either mine the cliques in the graph or learn a large set of parameters to do so, leading to a large penalty on run time and resources.
% We present an efficient solution to weigh ${\boldsymbol A}$ by embedding nodes into structure-aware representations via random walks and measuring their similarities via distances in the embedding space.
\methodest can also benefit from the \emphasis by replacing ${\boldsymbol A}$ with ${\boldsymbol A}^{*}$ in Alg.~\ref{algo:cmest}.
$\hat{\boldsymbol{H}}^{*}$ denotes the centered compatibility matrix estimated by the \emphasis.\looseness=-1
% \reminder{I'd suggest we drop the discussion on negative H*}
\hide{
Since the rows of $\boldsymbol{H}^{*}$ do not sum to one, we filter out the negative values and normalize the sum of each row to one.
This is done safely, where the negative values represent negligible relationships between nodes.\looseness=-1
}% end hide

\paragraph{Overview:}
Alg.~\ref{algo:rw} shows the details.
In short, it has $3$ steps:
\ben
\item {\bf Favors influential neighbors} by quickly approximating the node-to-node proximity using (non-backtracking) random walks with restarts (lines $2$-$5$);
% \item it takes the logarithm of the visit count, following best practice (line $6$)
\item {\bf Touches-up} the new node-proximity by applying a series of transformations (including the best-practice element-wise logarithm) on the proximity matrix (line $6$);
\item {\bf Symmetrizes and weighs} the adjacency matrix by using structural-aware embedding (lines $7$-$8$), 
% it symmetrizes and weighs the resulting adjacency matrix 
giving high weights to neighbors with similar embeddings (line $9$).
% (c1) using spectral embedding (line $7$ and $8$) and (c2) 
\een

\hide{
which we describe in a flashback way to give a better train of thought.
The main idea is that the neighbors of a node are more influential if they are more structurally similar to that node.
% However, directly measuring the node similarity in the graph is not trivial, or may be time consuming (e.g., by calculating motifs).
In line $12$, we construct the \emphasis by giving larger weights to edges between similar nodes in the embedding space.
% , measuring by the higher-order structural-aware representations.
The node representations can be obtained by decomposing a higher-order proximity matrix, but it is usually dense and may be biased towards nodes with large degrees.
To address these issues, we derive node representations by (a) approximating the higher-order proximity matrix by random walk (in line $1$-$8$) and (b) applying a series of transformations on the resulting proximity matrix (in line $9$-$11$).
}% end hid

% in line $9$-$11$ before decomposing, we apply a series of transformations on the proximity matrix.
% Although the proximity matrix can be exactly computed by powering the adjacency matrix, the computation is slow and most of its elements are noisy. 
% Thus, in line $1$-$8$, we approximate the higher-order proximity matrix by random walk, which is efficient and theoretically-guaranteed on convergence.

% The algorithm is depicted in Alg.~\ref{algo:rw}, which we describe in a flashback way to give a better train of thought.
% The neighbors of a node are more influential if they are more structurally similar to that node.
% However, directly measuring the node similarity in the graph is not trivial, or may be time consuming (e.g., by calculating motifs).
% Therefore, in line $12$, we construct the \emphasis by weighing the edges with the node similarity in the embedding space, measuring by the higher-order structural-aware representations.
% The node representations can be obtained by decomposing the higher-order proximity matrix, but it is usually dense and may be biased towards nodes with large degrees.
% To address these issues, in line $9$-$11$ before decomposing, we apply a series of transformations on the proximity matrix.
% Although the proximity matrix can be exactly computed by powering the adjacency matrix, the computation is slow and most of its elements are noisy. 
% Thus, in line $1$-$8$, we approximate the higher-order proximity matrix by random walk, which is efficient and theoretically-guaranteed on convergence.


% We represent nodes in $d$-dimensional vector space using Singular Value Decomposition (SVD) on a random walk the proximity matrix of the graph and captures information from pairwise connections.
% The one-step transition matrix is defined as ${\boldsymbol P}={\boldsymbol D}^{-1}{\boldsymbol A}$, where ${\boldsymbol D}^{-1}$ is the diagonal matrix with the reciprocals of degrees. Each neighbor has an equal probability of being visited.
% To reach a high-order proximity with finite length $L$, the transition matrix can be defined as ${\boldsymbol \Pi}_{(L)}=\sum_{l=1}^{L}{{\boldsymbol P}^{l}}$, where the matrix is dense after only a few steps.
% Another way to achieve this goal is by counting the motifs around the node, such as triangles and 4-cliques. 
% However, both methods suffer from computationally expensive costs, especially when $n$ is large.

\begin{figure}[t]
    \centering
    \subfloat[\label{fig:em1} Adjacency Matrix]
    {\includegraphics[height=1.1in]{FIG/em_1.pdf}}
    \hspace{6mm}
    \subfloat[\label{fig:em2} Emphasis Matrix]
    {\includegraphics[height=1.1in]{FIG/em_2.pdf}} 
    % \vspace{-2mm}
    \caption{\label{fig:ematrix} \underline{\smash{\Emphasis at work:}} it prefers well-connected neighbors, see edges B-C, B-D, and C-D.}
    % \vspace{-3mm}
\end{figure}

\begin{algorithm}[t]
\KwData{Adjacency matrix ${\boldsymbol A}$, number of trials $M$, number of steps $L$, and dimension $d$}
\KwResult{Emphasis matrix ${\boldsymbol A}^{*}$}
${\boldsymbol W}^{\prime} \leftarrow {\boldsymbol O}_{n \times n}$\;
\tcc{approximate proximity matrix by random walk}
\For{node $i$ in $G$}{
    \For{$m = 1, ..., M$}{
        \For{$j \in \mathcal{W}_{m}(i, L)$}{
            ${\boldsymbol W}^{\prime}_{ij} \leftarrow {\boldsymbol W}^{\prime}_{ij} + 1$\;
        }
    }
}
\tcc{masking, degree normalization and logarithm}
% \tcc{take logarithm (following best practice)}
${\boldsymbol W}_{n \times n} \leftarrow \log{({\boldsymbol D}^{-1}({\boldsymbol W}^{\prime} \odot {\boldsymbol A}))}$\; %  \tcp*{transformations}
% \tcc{Symmetrize}
${\boldsymbol U}_{n \times d}, {\boldsymbol \Sigma}_{d \times d}, {\boldsymbol V}_{d \times n}^{T} \leftarrow \text{SVD}({\boldsymbol W}, d)$\tcp*{embedding}
${\boldsymbol U} \leftarrow \sqrt{{\boldsymbol \Sigma}}{\boldsymbol U}$\tcp*{scaling}
\tcc{boost weights of close-embedded neighbors}
Weigh ${\boldsymbol A}^{*}_{n \times n}$, where ${\boldsymbol A}^{*}_{ij} = \mathcal{S}({\boldsymbol U}_{i}, {\boldsymbol U}_{j}), \forall \{i, j | {\boldsymbol A}_{ij} = 1\}$\;
Return ${\boldsymbol A}^{*}$\;
\caption{``Emphasis'' Matrix \label{algo:rw}}
\end{algorithm}

\begin{algorithm}[t]
\KwData{``Emphasis'' matrix ${\boldsymbol A}^{*}$, estimated compatibility matrix $\hat{\boldsymbol H}^{*}$, and initial belief $\hat{{\boldsymbol E}}$}
\KwResult{Final belief ${\boldsymbol B}$}
% ${\boldsymbol A}^{*} \leftarrow \text{``Emphasis''-Matrix}({\boldsymbol A}, d)$\;
% $\hat{{\boldsymbol H}}^{*} \leftarrow \text{Compatibility-Matrix-Estimation}({\boldsymbol A}^{*}, \hat{{\boldsymbol E}}, \mathcal{P})$\;
% \tcc{propagation}
$\hat{{\boldsymbol B}}_{(0)} \leftarrow {\boldsymbol O}_{n \times c}, t \leftarrow 0$\;
\While{$\lVert\hat{{\boldsymbol B}}_{(t+1)} - \hat{{\boldsymbol B}}_{(t)}\rVert_{1} > 1$}{
   $\hat{{\boldsymbol B}}_{(t+1)} \leftarrow \hat{{\boldsymbol E}} + f {\boldsymbol A}^{*}\hat{{\boldsymbol B}}_{(t)}\hat{{\boldsymbol H}}^{*}$\;
   $t \leftarrow t + 1$\;
}
Return ${\boldsymbol B} \leftarrow \hat{{\boldsymbol B}}_{(t)} + 1/c$\;
\caption{\methodexp \label{algo:main}}
\end{algorithm}

\subsubsection{Details}
\paragraph{Proximity Matrix Approximation}
A naive way to capture the higher-order information of a graph is to compute the proximity matrix through ${\boldsymbol W}={\boldsymbol A}^{L}$, which is prohibitively dense.
In practice, it is common that a node may have many neighbors, but many of them provide useless information for inferring its label.
Thus, we propose to do random walks with restarts from each node.
Given an approximated proximity matrix ${\boldsymbol W}^{\prime}$, ${\boldsymbol W}^{\prime}_{ij}$ records the number of times we visit node $j$ if we start a random walk from node $i$.
% and count how many times each $1$-step neighbor is visited, where 
Only the well-connected neighbors will be visited more often.
\hide{
In practice, it is common that a node may have many neighbors, but most of them provide useless information for inferring its label.
To address this and approximate ${\boldsymbol W}$ in a fast way, we utilize random walks.
Given an approximated proximity matrix ${\boldsymbol W}^{\prime}$, ${\boldsymbol W}^{\prime}_{ij}$ records the number of times we visit node $j$ if we start a random walk from node $i$. 
Each neighbor has the same probability of being visited, but only those structurally important ones are visited more frequently.
} % end hide
% To theoretically show why the approximation by random walks is effective, we prove that the neighbor distribution for each node converges after a number of trials:\looseness=-1
We theoretically show that this algorithm converges quickly:
% (see Lemma~\ref{lem:crw1});
% but we can make it converge even faster, using non-backtracking
% random walks (see Lemma~\ref{lem:crw2}):

\begin{lemma} [Convergence of Regular Random Walks] \label{lem:crw1}
With probability $1 - \delta$, the error $\epsilon$ between the approximated distribution and the true one for a node walking to its 1-hop neighbor by a regular random walk of length $L$ with $M$ trials is:
\begin{equation}
    \epsilon \leq \frac{\lceil (L - 1) / 2 \rceil}{L} \sqrt{\frac{\log{(2/\delta)}}{2LM}}
\end{equation}
\end{lemma}

\begin{proof}
% We prove Lemma~\ref{lem:crw1} in Appendix~\ref{ap:subsec:prooflemma12}.
See Appendix~\ref{ap:subsec:prooflemma12}.
\end{proof}

We can make the convergence even faster
% The convergence of the approximation can be made faster 
by using ``non-backtracking'' random walks.
Given the start node $s$ and walk length $L$, its function is defined as follows:
\begin{equation}
\mathcal{W}(s, L) = 
\begin{cases} (w_{0}=s, ..., w_{L}) \space 
\begin{array}{c}
w_{l} \in N{(w_{l-1})}, \forall l \in [1, L]
\\
w_{l-1} \neq w_{l+1}, \forall l\in [1, L-1]
\end{array}.
\end{cases}
\end{equation}
We improve Lemma~\ref{lem:crw1} to have a tighter bound of error $\epsilon$:
\begin{lemma} [Convergence of Non-Backtracking Random Walks] \label{lem:crw2}
With the same condition as in Lemma~\ref{lem:crw1}, the error $\epsilon$ by a non-backtracking random walks is:
\begin{equation}
    \epsilon \leq \frac{\lceil (L - 1) / 3 \rceil}{L} \sqrt{\frac{\log{(2/\delta)}}{2LM}}
\end{equation}
\end{lemma}

\begin{proof}
See Appendix~\ref{ap:subsec:prooflemma12}.
\end{proof}

% For example, when using regular random walks of length $L=4$ with $M=30$ trials, the estimated error by Lemma~\ref{lem:crw1} with probability $95\%$ is about $6.2\%$. Nevertheless, if we instead use non-backtracking random walks, the error is reduced to $3.1\%$, which is $2\times$ lower than the one by regular walks, indicating that the approximated distribution converges well to the true one. 
% Noting that unlike most methods, \method has few parameters in the algorithm. 
% The resulting low error $\epsilon$ indicates that the approximated distribution converges well to the true distribution. 
% Therefore, the parameters are insensitive.

\setlength{\tabcolsep}{1.2pt}
\begin{table*}[!t]
\caption{\underline{\smash{\method wins on \xophily and Heterophily datasets.}} Accuracy, running time, and relative time are reported.\label{tab:effecthet}}
% \vspace{-3mm}
\setlength\fboxsep{0pt}
\centering{\resizebox{0.97\textwidth}{!}{
\begin{tabular}{C{4cm} | C{1.8cm} R{1.4cm} R{1.4cm} | C{1.8cm} R{1.4cm} R{1.4cm} | C{1.8cm} R{1.4cm} R{1.4cm} | C{1.8cm} R{1.4cm} R{1.4cm}}

\hline

{\bf Dataset} 
& \multicolumn{3}{c|}{\bf Synthetic} & \multicolumn{3}{c|}{\bf Pokec-Gender} 
& \multicolumn{3}{c|}{\bf arXiv-Year} & \multicolumn{3}{c}{\bf Patent-Year} \\

\hline

\# of Nodes / Edges / Classes
& \multicolumn{3}{c|}{1.2M / 34.0M / 6} & \multicolumn{3}{c|}{1.6M / 22.3M / 2}
& \multicolumn{3}{c|}{169K / 1.2M / 5} & \multicolumn{3}{c}{1.3M / 4.3M / 5} \\
% \# of Nodes
% & \multicolumn{3}{c|}{1.2M} & \multicolumn{3}{c|}{1.6M}
% & \multicolumn{3}{c|}{169K} & \multicolumn{3}{c|}{1.3M} \\
% \# of Edges 
% & \multicolumn{3}{c|}{34.0M} & \multicolumn{3}{c|}{22.3M}
% & \multicolumn{3}{c|}{1.2M} & \multicolumn{3}{c|}{4.3M} \\
% \# of Classes 
% & \multicolumn{3}{c|}{6} & \multicolumn{3}{c|}{2}
% & \multicolumn{3}{c|}{5} & \multicolumn{3}{c|}{5} \\

Label Fraction
& \multicolumn{3}{c|}{4\%} & \multicolumn{3}{c|}{0.4\%}
& \multicolumn{3}{c|}{4\%} & \multicolumn{3}{c}{4\%}
\\

\hline

\nef Strength
& \multicolumn{3}{c|}{Strong \xophily} & \multicolumn{3}{c|}{Strong Heterophily}
& \multicolumn{3}{c|}{Weak \xophily} & \multicolumn{3}{c}{Weak Heterophily} \\

% \nef Strength
% & \multicolumn{3}{c|}{Strong} & \multicolumn{3}{c|}{Strong}
% & \multicolumn{3}{c|}{Weak} & \multicolumn{3}{c}{Weak} \\

% \nef Type
% & \multicolumn{3}{c|}{\xophily} & \multicolumn{3}{c|}{Heterophily}
% & \multicolumn{3}{c|}{\xophily} & \multicolumn{3}{c}{Heterophily} \\

\hline

Method
& Accuracy (\%) & Time (s) & Rel. Time
& Accuracy (\%) & Time (s) & Rel. Time
& Accuracy (\%) & Time (s) & Rel. Time
& Accuracy (\%) & Time (s) & Rel. Time
\\

\hline

\gcn   
& 16.7$\pm$0.0 & 3456 & \bronze{4.1$\times$}
& 51.8$\pm$0.1 & \bronze{2906} & \silver{3.4$\times$}
& 35.3$\pm$0.1 & \silver{132} & \silver{2.5$\times$}
& 26.0$\pm$0.0 & 894 & 2.3$\times$
\\

\appnp   
& 18.6$\pm$1.1 & 7705 & 9.2$\times$
& 50.9$\pm$0.3 & 6770 & 7.8$\times$
& 33.5$\pm$0.2 & 423 & 8.1$\times$
& \silver{27.5$\pm$0.2} & 2050 & 5.2$\times$
\\

% node2vec + MLP   
% & 72.1$\pm$0.6 & 29806 & 40.3$\times$
% & 55.3$\pm$0.2 & 54694 & 73.7$\times$
% & 37.6$\pm$0.5 & 9854 & 234.6$\times$
% & 25.7$\pm$0.1 & 11682 & 43.6$\times$
% \\
        
\hline

\mixhop  
& 16.7$\pm$0.0 & 58391 & 70.0$\times$
& 53.4$\pm$1.2 & 53871 & 62.1$\times$
& \gold{39.6$\pm$0.1} & 2983 & 57.4$\times$
& \bronze{26.8$\pm$0.1} & 18787 & 47.6$\times$
\\

\gprgnn
& 18.9$\pm$1.2 & 7637 & 9.1$\times$
& 50.7$\pm$0.2 & 6699 & \bronze{7.7$\times$}
& 30.1$\pm$1.4 & \bronze{400} & \bronze{7.7$\times$}
& 25.3$\pm$0.1 & 2034 & 5.1$\times$
\\

\hline
        
\hols
& \silver{46.1$\pm$0.1} & \bronze{1672} & \silver{2.0$\times$}
& \bronze{54.4$\pm$0.1} & 8552 & 9.9$\times$
& 34.1$\pm$0.3 & 566 & 10.9$\times$
& 23.6$\pm$0.0 & \bronze{510} & \bronze{1.3$\times$}
\\

% \linbp 
% & 24.0$\pm$0.0 & 59} & $\times$
% & \silver{57.1$\pm$0.3} & 314} & $\times$
% & 36.8$\pm$0.3 & 8.0} & $\times$
% & 48.1$\pm$0.1 & 317} & $\times$
% \\
        
\hline

\methodhom
& \bronze{45.6$\pm$0.1} & \gold{835} & \gold{1.0$\times$}
& \silver{56.9$\pm$0.2} & \silver{869} & \gold{1.0$\times$}
& \bronze{37.0$\pm$0.3} & \gold{52} & \gold{1.0$\times$}
& 24.3$\pm$0.0 & \silver{429} & \silver{1.1$\times$}
\\

\method
& \gold{80.4$\pm$0.0} & \silver{841} & \gold{1.0$\times$}
& \gold{67.3$\pm$0.1} & \gold{867} & \gold{1.0$\times$}
& \silver{38.9$\pm$0.1} & \gold{52} & \gold{1.0$\times$}
& \gold{28.7$\pm$0.1} & \gold{395} & \gold{1.0$\times$}
\\

\hline
\end{tabular}
}}
\end{table*}   

\setlength{\tabcolsep}{1.2pt}
\begin{table*}[!t]
\caption{\underline{\smash{\method wins on Homophily datasets.}} Accuracy, running time, and relative time are reported. \label{tab:effecthom}}
% \vspace{-3mm}
\setlength\fboxsep{0pt}
\centering{\resizebox{0.97\textwidth}{!}{
\begin{tabular}{C{4cm} | C{1.8cm} R{1.4cm} R{1.4cm} | C{1.8cm} R{1.4cm} R{1.4cm} | C{1.8cm} R{1.4cm} R{1.4cm} | C{1.8cm} R{1.4cm} R{1.4cm}}

\hline

{\bf Dataset} & \multicolumn{3}{c|}{\bf Facebook} & \multicolumn{3}{c|}{\bf GitHub} & \multicolumn{3}{c|}{\bf arXiv-Category} & \multicolumn{3}{c}{\bf Pokec-Locality} \\

\hline


\# of Nodes / Edges / Classes 
& \multicolumn{3}{c|}{22.5K / 171K / 4} & \multicolumn{3}{c|}{37.7K / 289K / 2} 
& \multicolumn{3}{c|}{169K / 1.2M / 40} & \multicolumn{3}{c}{1.6M / 22.3M / 10} \\
% \# of Nodes & \multicolumn{3}{c|}{22.5K} & \multicolumn{3}{c|}{37.7K} 
% & \multicolumn{3}{c|}{169K} & \multicolumn{3}{c|}{1.6M} \\
% \# of Edges & \multicolumn{3}{c|}{171K} & \multicolumn{3}{c|}{289K} 
% & \multicolumn{3}{c|}{1.2M} & \multicolumn{3}{c|}{22.3M} \\
% \# of Classes & \multicolumn{3}{c|}{4} & \multicolumn{3}{c|}{2} 
% & \multicolumn{3}{c|}{40} & \multicolumn{3}{c|}{10} \\

Label Fraction
& \multicolumn{3}{c|}{4\%} & \multicolumn{3}{c|}{4\%}
& \multicolumn{3}{c|}{4\%} & \multicolumn{3}{c}{0.4\%}
\\

\hline

Method
& Accuracy (\%) & Time (s) & Rel. Time
& Accuracy (\%) & Time (s) & Rel. Time
& Accuracy (\%) & Time (s) & Rel. Time
& Accuracy (\%) & Time (s) & Rel. Time
\\

\hline

\gcn   
& 67.0$\pm$0.8 & \silver{12} & \silver{2.0$\times$}
& \silver{81.0$\pm$0.6} & \silver{28} & \silver{2.2$\times$}
& 25.4$\pm$0.3 & 216 & \bronze{2.3$\times$}
& 17.3$\pm$0.4 & \bronze{4002} & \silver{2.9$\times$}
\\

\appnp   
& 50.5$\pm$2.2 & \bronze{46} & \bronze{7.7$\times$}
& 74.2$\pm$0.0 & \bronze{73} & \bronze{5.6$\times$}
& 19.4$\pm$0.6 & 1176 & 12.3$\times$
& 16.8$\pm$1.7 & 11885 & 8.6$\times$
\\

% node2vec + MLP   
% & 85.7$\pm$0.3 & 245 & 61.3$\times$
% & 86.2$\pm$0.6 & 1791 & 162.8$\times$
% & $\pm$ &  & $\times$
% & $\pm$ &  & $\times$
% \\
        
\hline

\mixhop  
& \bronze{69.2$\pm$0.7} & 296 & 49.3$\times$
& 77.8$\pm$1.3 & 526 & 40.5$\times$
& 33.0$\pm$0.6 & 3203 & 33.4$\times$
& 16.9$\pm$0.3 & 52139 & 37.9$\times$
\\

\gprgnn
& 51.9$\pm$1.5 & 47 & 7.8$\times$
& 74.1$\pm$0.1 & 75 & 5.8$\times$
& 19.7$\pm$0.3 & 1174 & 12.2$\times$
& 30.0$\pm$2.0 & 11959 & 8.7$\times$
\\

\hline
        
\hols
& \gold{86.0$\pm$0.4} & 934 & 155.7$\times$
& \bronze{80.8$\pm$0.5} & 126 & 9.7$\times$
& \silver{61.4$\pm$0.2} & 627 & 6.5$\times$
& \bronze{63.7$\pm$0.3} & 8139 & \bronze{5.9$\times$}
\\

% \linbp 
% & 51.7$\pm$2.4 & 0.7} 
% & 74.7$\pm$0.5 & 0.6}
% & 39.1$\pm$1.0 & 92}
% & 64.3$\pm$0.2 & 661}
% \\
        
\hline

\methodhom
& \silver{85.2$\pm$0.5} & \gold{6} & \gold{1.0$\times$}
& \gold{81.3$\pm$0.5} & \gold{13} & \gold{1.0$\times$}
& \gold{61.7$\pm$0.2} & \gold{96} & \gold{1.0$\times$}
& \gold{66.0$\pm$0.2} & \silver{1437} & \gold{1.0$\times$}
\\

\method
& \silver{85.2$\pm$0.5} & \gold{6} & \gold{1.0$\times$}
& \gold{81.3$\pm$0.5} & \gold{13} & \gold{1.0$\times$}
& \bronze{58.8$\pm$0.6} & \silver{108} & \silver{1.1$\times$}
& \silver{64.8$\pm$0.8} & \gold{1377} & \gold{1.0$\times$}
\\

\hline
\end{tabular}
}}
\end{table*}   

% In Algorithm~\ref{algo:rw} line $9$, 
\paragraph{Structural-Aware Node Representation}
% We represent nodes in $d$-dimensional embedding space efficiently using Singular Value Decomposition (SVD) on ${\boldsymbol W}^{\prime}$.
Based on ${\boldsymbol W}$, we apply a series of transformations to generate better and unbiased representations of nodes in a fast way.
An element-wise multiplication by ${\boldsymbol A}$ is done to keep the approximation of $1$-hop neighbor for each node, which is sparse but supplies sufficient information.
% as well as keeps the resulting matrix sparse. 
We use the inverse of the degree matrix ${\boldsymbol D}^{-1}$ to reduce the influence of nodes with large degrees.
This prevents them from dominating the pairwise distance by containing more elements in their rows. 
The element-wise logarithm rescales the distribution in ${\boldsymbol W}$, in order to enlarge the difference between smaller structures.
We use Singular Value Decomposition (SVD) for efficient rank-$d$ decomposition of sparse ${\boldsymbol W}$, and multiply the left-singular vectors ${\boldsymbol U}$ by the squared eigenvalues $\sqrt{{\boldsymbol \Sigma}}$ to correct the scale.

\paragraph{``Emphasis'' Matrix Construction}
Directly measuring the node similarity in the graph is not trivial, or may be time consuming (e.g., by counting motifs).
Therefore, we propose to compute the node similarity via the structural-aware node representations, which capture the higher-order information.
We construct the \emphasis ${\boldsymbol A}^{*}$ by weighing ${\boldsymbol A}$ with the node similarity, which is inversely proportional to the distance of node representations.
The intuition is that the nodes that are closer in the embedding space are better connected with higher-order structures. 
The similarity function is
$\mathcal{S}({\boldsymbol U}_{i}, {\boldsymbol U}_{j}) = e^{-\mathcal{D}({\boldsymbol U}_{ik}, {\boldsymbol U}_{jk})}$,
where $e \approx 2.718$ is the Euler's number. 
It is a universal law proposed by Shepard \cite{shepard1987toward}, which turns the distance into similarity via a negative exponential function, and bounds it from $0$ to $1$. 
While the function $\mathcal{D}$ can be any distance metric, we use Euclidean because it is empirically shown to work well.
% Since $\mathcal{S}({\boldsymbol U}_{i}, {\boldsymbol U}_{j}) = \mathcal{S}({\boldsymbol U}_{j}, {\boldsymbol U}_{i})$, ${\boldsymbol A}^{*}$ is still a symmetric matrix. 
% This is a convenient property, which will later be used for the fast computation of the spectral radius (see Lemma~\ref{lem:con}).

\subsection{\methodexp}
Alg.~\ref{algo:main} shows the algorithm of \methodexp. 
It takes as input the \emphasis ${\boldsymbol A}^{*}$,
%is constructed based on the adjacency matrix ${\boldsymbol A}$ of a graph, and then used to estimate 
the compatibility matrix $\hat{{\boldsymbol H}}^{*}$ estimated by ${\boldsymbol A}^{*}$, and the initial beliefs $\hat{{\boldsymbol E}}$.
It computes
% by ${\boldsymbol A}^{*}$.
% We initialize and propagate 
the beliefs $\hat{{\boldsymbol B}}$ iteratively 
% through ${\boldsymbol A}^{*}$ until they converge.
% In each iteration, we 
by aggregating the beliefs of neighbors through ${\boldsymbol A}^{*}$ until they converge.
% The interrelations between classes are modulated by the compatibility matrix $\hat{{\boldsymbol H}}^{*}$.
% weighted by the values in ${\boldsymbol A}^{*}$. 
This reusage of ${\boldsymbol A}^{*}$ aims to draw attention to the neighbors that are more structurally important. 
By handling the interrelations between classes with $\hat{{\boldsymbol H}}^{*}$, \method well exploits \nef for much better propagation.
% An early stopping criterion is included for more efficient propagation.
% , which is set to be lower than the minimum error $1 / \lg{nc}$, depending on the size of $\hat{{\boldsymbol B}}$.

\subsection{Theoretical Analysis} \label{ssec:ta}

\paragraph{Convergence Guarantee}
To ensure the convergence of \methodexp, we introduce a scaling factor $f$ during the iterations. 
A smaller $f$ leads to a faster convergence but distorts the results, thus we set $f$ to $0.9 / \rho{({\boldsymbol A}^{*})}$. 
Its exact convergence is:\looseness=-1
\begin{lemma}[Exact Convergence] \label{lem:con}
The criterion for the exact convergence of \methodexp is 
 $0 < f < 1/\rho{({\boldsymbol A}^{*})}$,
% \begin{equation}
%     \text{\methodexp exactly converges} \Leftrightarrow 0 < f < 1/\rho{({\boldsymbol A}^{*})},
% \end{equation}
where $\rho{(\cdot)}$ denotes the spectral radius of the given matrix.
\end{lemma}
\begin{proof}
% \vspace{-0.02in}
See Appendix~\ref{ap:subsec:proofcon}.
\end{proof}

% A smaller scaling factor leads to a faster convergence, nevertheless, distorts the results. 
% We recommend setting $f = 0.9 / \rho{({\boldsymbol A}^{*})}$. 
% We recommend a large eigenvalue close to $1$, setting $f = 0.9 / \rho{({\boldsymbol A}^{*})}$. 
% Since ${\boldsymbol A}^{*}$ is symmetric and sparse, the computation of the spectral radius can be done efficiently.

\paragraph{Complexity Analysis}
\methodexp uses sparse matrix representation of graphs and scales linearly. 
Its complexity is:
\begin{lemma} \label{lem:complexity}
% \methodexp scales linearly on the input size. % Keeping only the dominating terms, 
The time complexity of \methodexp is $O(m)$,
and the space complexity is $O(\max{(m, n \cdot L \cdot M)} + n \cdot c^{2})$.
\end{lemma}
\begin{proof}
% \vspace{-0.08in}
See Appendix~\ref{ap:subsec:proofcomplexity}.
\end{proof}