
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AUTHOR: Christos Faloutsos
% INSTITUTION: CMU
% DATE: April 2019
% GOAL: to streamline the paper presentations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \appendix 

% \title{Appendix} \label{sec:app}
% \reminder{[here we put self notes etc, that we will NOT include
% in the final paper]}

\section{Proof}

\subsection{Proof of Lemma~\ref{lem:nef}}
\label{ap:subsec:proofprop1}

\begin{proof}
% In the beginning, we introduce two necessary notations. $\text{vec}(\cdot)$ denotes the vectorization operator:
% \begin{equation}
%    \text{vec}({\boldsymbol X}) = [{\boldsymbol X}_{11}, \cdots, {\boldsymbol X}_{m1}, {\boldsymbol X}_{12}, \cdots, {\boldsymbol X}_{m2}, \cdots, {\boldsymbol X}_{mn}]^\top,
% \end{equation}
% where ${\boldsymbol X}$ is an $m \times n$ matrix, and ${\boldsymbol X}_{ij}$ denotes the element of ${\boldsymbol X}$ on the $i$-th row and the $j$-th column.
% Next, the Knronecker product of given two $m \times n$ matrices ${\boldsymbol X}$ and ${\boldsymbol Y}$ is:
% \begin{equation}
%     {\boldsymbol X} \otimes {\boldsymbol Y} = \begin{bmatrix} {\boldsymbol X}_{11}{\boldsymbol Y} & {\boldsymbol X}_{12}{\boldsymbol Y} & \cdots & {\boldsymbol X}_{1n}{\boldsymbol Y} \\ {\boldsymbol X}_{21}{\boldsymbol Y} & {\boldsymbol X}_{22}{\boldsymbol Y} & \cdots & {\boldsymbol X}_{2n}{\boldsymbol Y} \\ \vdots & \vdots & \ddots & \vdots \\ {\boldsymbol X}_{m1}{\boldsymbol Y} & {\boldsymbol X}_{m2}{\boldsymbol Y} & \cdots & {\boldsymbol X}_{mn}{\boldsymbol Y} \end{bmatrix}
% \end{equation}

We reformulate the BP equation and derive the final result by the closed formula of linear regression.
Given the features ${\boldsymbol X}$ and target ${\boldsymbol y}$, the closed formula of the weights ${\boldsymbol W}$ of linear regression is:
\begin{equation} \label{eqn:reg}
    {\boldsymbol W} = ({\boldsymbol X}^{T}{\boldsymbol X})^{-1}{\boldsymbol X}^{T}{\boldsymbol y}.
\end{equation}
The property of the mixed Kronecker matrix-vector product~\cite{bernstein2009matrix} is:\looseness=-1
\begin{equation} \label{eqn:mvp}
    \text{vec}{({\boldsymbol B}{\boldsymbol V}{\boldsymbol A}^{T})} = ({\boldsymbol A} \otimes {\boldsymbol B}){\boldsymbol v},
\end{equation}
where $\otimes$ denotes the Knronecker product, $\text{vec}(\cdot)$ denotes the vectorization operator, and the matrix ${\boldsymbol V} = \text{vec}^{-1}({\boldsymbol v})$ is the result of the inverse of the vectorization operator on ${\boldsymbol v}$.

To begin the derivation, we vectorize $\hat{{\boldsymbol E}} \approx {\boldsymbol A}\hat{{\boldsymbol E}}\hat{{\boldsymbol H}}$ into:
\begin{equation} \label{eq:pol}
    \text{vec}{(\hat{{\boldsymbol E}})} = \text{vec}{(({\boldsymbol A}\hat{{\boldsymbol E}})\hat{{\boldsymbol H}}{\boldsymbol I}_{c \times c})},
\end{equation}
where ${\boldsymbol I}_{c \times c}$ is a $c \times c$ identity matrix. 
By multipling one identity matrix by $\hat{{\boldsymbol H}}$, we can then use Eq.~\ref{eqn:mvp} to reformulate Eq.~\ref{eq:pol} into:
\begin{equation}
\text{vec}{(\hat{{\boldsymbol E}})} = ({\boldsymbol I}_{c \times c} \otimes ({\boldsymbol A}\hat{{\boldsymbol E}}))\text{vec}{(\hat{{\boldsymbol H}})}
\end{equation}
Let ${\boldsymbol X} = {\boldsymbol I}_{c \times c} \otimes ({\boldsymbol A}\hat{{\boldsymbol E}})$ and ${\boldsymbol y} = \text{vec}{(\hat{{\boldsymbol E}})}$ in Eq.~\ref{eqn:reg}, the closed-form solution of vectorized compatibility matrix is as follows:
\begin{equation}
    \text{vec}{(\hat{{\boldsymbol H}})} = ({\boldsymbol X}^{T}{\boldsymbol X})^{-1}{\boldsymbol X}^{T}{\boldsymbol y}
\end{equation}
\end{proof}

\subsection{Proof of Lemma~\ref{lem:crw1} and~\ref{lem:crw2}}
\label{ap:subsec:prooflemma12}
\begin{proof}
For a $L$-steps random walk sequence $S$ with $M$ trials, the sequence length $|S|$ is $LM$. We define the random variable $X$, denoting the probability of node $i$ will walk to its $j$-th neighbor:
\begin{equation}
    X = \mathbb{P}(\text{node } i \text{ walks to } N(i)_{j}) = \frac{\sum_{k=1}^{|S|}{\mathbbm{1}(N(i)_{j} = S_{k})}}{|S|},
\end{equation}
where $\mathbb{P}$ denotes the probability and $\mathbbm{1}$ denotes the indicator. With regular random walk in the graph without self-loops, the random variable $X$ is upper-bounded by $\frac{\lceil (L - 1) / 2 \rceil}{L}$. We can thus apply Hoeffding's inequality:
\begin{equation}
    \mathbb{P}(|\hat{\mu}_{|S|} - \mu| \geq \epsilon) \leq 2\exp{\frac{-2L^{3}Mt^{2}}{\lceil (L - 1) / 2 \rceil^{2}}},
\end{equation}
where $\hat{\mu}_{|S|}$ denotes the sampled mean of the given random variable, and $\mu$ denotes the expectation. Let $\delta = 2\exp{\frac{-2L^{3}Mt^{2}}{\lceil (L - 1) / 2 \rceil^{2}}}$, with probability $1 - \delta$, the error $\epsilon$ is:
\begin{equation}
    \epsilon = |\hat{\mu}_{|S|} - \mu| \leq \frac{\lceil (L - 1) / 2 \rceil}{L} \sqrt{\frac{\log{(2/\delta)}}{2LM}}
\end{equation}

With the help of non-backtracking random walk \cite{alon2007non}, we can further shrink the upper bound of $X$ into $\frac{\lceil (L - 1) / 3 \rceil}{L}$. Now, let $\delta = 2\exp{\frac{-2L^{3}Mt^{2}}{\lceil (L - 1) / 3 \rceil^{2}}}$, with probability $1 - \delta$, the error $\epsilon$ is now:\looseness=-1
\begin{equation}
    \epsilon = |\hat{\mu}_{|S|} - \mu| \leq \frac{\lceil (L - 1) / 3 \rceil}{L} \sqrt{\frac{\log{(2/\delta)}}{2LM}}
\end{equation}

\end{proof}

% \subsection{Proof of Lemma~\ref{lem:sp}}
% \label{ap:subsec:proofsp}
% \begin{proof}
% Without the emphasis matrix, propagation treats all the neighbors for each node the same.
% For the compatibility matrix, the default is to use homophily, where $\hat{{\boldsymbol H}} = {\boldsymbol I}_{c \times c}  - \frac{1}{c}$.
% Therefore, Algorithm~\ref{algo:main} line $6$ to $10$ then becomes standard \linbp without echo cancellation, and \method is reduced to \linbp.
% \end{proof}

\subsection{Proof of Lemma~\ref{lem:con}}
\label{ap:subsec:proofcon}
\begin{proof}
\method exactly converges if and only if \\$\rho{({\boldsymbol A}^{*})}\rho{(\hat{\boldsymbol H}^{*})} < 1$. 
${\boldsymbol H}^{*}$ is row-normalized, where $\rho{({\boldsymbol H}^{*})} = 1$ is a constant, and is less than $1$ after centering.
The scaling factor $f$ multiplied to the propagation has to be in the range of $(0, 1/\rho{({\boldsymbol A}^{*})})$ to meet the criterion of exact convergence.
\end{proof}

\vspace{-3mm}
\subsection{Proof of Lemma~\ref{lem:complexity}}

\label{ap:subsec:proofcomplexity}
\begin{proof}
% In the \neteffect phase, the time complexity for the Fisher's exact test is $O(\max{({\boldsymbol C})})$, where $\max{({\boldsymbol C})}$ is a constant bounded by $500$ in our algorithm. Therefore, \neteffect analysis takes $O(|{\boldsymbol e}^{'}| \cdot c^{2})$. 
For \methodest, since there are $c$ sets of parameters are independent, we can separate the problem into $c$ tasks, where each contains $c$ features and $|\mathcal{P}|$ samples. 
The complexity can then be reduced to $O(|\mathcal{P}| \cdot c^{3})$, and the efficient leave-one-out cross-validation only needs to be done once.
For \methodexp, for each random walk, each node visits at most $L \cdot M$ unique nodes, so the maximum number of non-zero elements in ${\boldsymbol W}$ is either $n \cdot L \cdot M$ if we have not walked through all the edges, or $m$ otherwise. 
SVD on ${\boldsymbol W}$ takes $O(d \cdot \max{(m, n \cdot L \cdot M)})$. 
It takes at most $O(m + n)$ for sparse matrix multiplication to run $t$ iterations.
Thus, the time complexity is $O(d \max{(m, n \cdot L \cdot M)} + |\mathcal{P}| \cdot c^{3} + m)$.
In practice, $c$, $|\mathcal{P}|$ and $t$ are usually small constants which are negligible, and $m$ is usually much larger. 
Keeping only the dominating terms, the time complexity is approximately $O(m)$.

${\boldsymbol W}$ contains at most $\max{(m, n \cdot L \cdot M)}$ non-zero elements. The Kronecker product at most contains $n \cdot c^{2}$ non-zero elements. 
% $\hat{{\boldsymbol B}}$ and $\hat{{\boldsymbol H}}$ contain at most $n \cdot c$ and $c^{2}$ non-zero elements, respectively. 
The space complexity is $O(\max{(m, n \cdot L \cdot M)} + n \cdot c^{2})$.
\end{proof}

% \subsection{Analytical Framework}
% \label{ap:framework}
% To illustrate the similarities, we draw upon final belief estimates from simplified \method and SGC. The simplified 2-class \method equation is as follows:
% \begin{equation}
%     \hat{y} = (I - A^*)^{-1} (\hat{E}),
% \end{equation}
% where $A^*$ is the emphasis matrix, and $\hat{E}$ is the initial belief.

% Similarly, let $y_0$ is the initial beliefs(labels) for nodes with known labels, the simplified 1-layer \sgc equation, assuming $A$ as K-step feature propagation, with linear regression (instead of logistic regression) is as follows:
% \begin{equation}
%     \hat{y} = A(A_{0}^{T} A_{0})^{-1}A_0^{T}y_{0},
% \end{equation}
% where $A$ is adjacency matrix, and $A_0$ represents truncated adjacency matrix for nodes with known labels.

\vspace{-3mm}
\section{Reproducibility} \label{sec:rep}
\vspace{-3mm}

\begin{table}[h]
\caption{Hyperparameters for GNNs \label{tab:hyper}}
\vspace{-4mm}
\centering{\resizebox{0.4\textwidth}{!}{
\begin{tabular}{C{2cm} | L{10.5cm}}
\hline
Method & Hyperparameters \\
\hline
\gcn & lr=0.01, wd=0.0005, hidden=16, dropout=0.5 \\ 
\appnp & lr=0.002, wd=0.0005, hidden=64, dropout=0.5, K=10, alpha=0.1 \\
% \dagnn & lr=0.01, wd=0.005, hidden=64, dropout=0.5, K=10 \\
\mixhop & lr=0.01, wd=0.0005, cutoff=0.1, layers1=[200, 200, 200], layers2=[200, 200, 200] \\
% \gcnii & lr=0.01, wd1=0.01, wd2=5e-4, hidden=64, layers=32, dropout=0.6, alpha=0.1, lamda=0.5 \\
\gprgnn & lr=0.002, wd=0.0005, hidden=64, dropout=0.5, K=10, alpha=0.1 \\
\hline
\end{tabular}
}}
\end{table}
\vspace{-6mm}

\subsection{Datasets} \label{ssec:datasets}
% \reminder{Group into citation networks, social networks, and point to the statistics in Table}

We include $3$ citation networks: ``arXiv-Year'' \cite{hu2020open}, ``Patent-Year'' \cite{leskovec2005graphs}, and ``arXiv-Category'' \cite{wang2020microsoft}, 
and $4$ social networks: ``Pokec-Gender'' \cite{takac2012data}, ``Facebook'' \cite{rozemberczki2019multiscale}, ``GitHub'' \cite{rozemberczki2019multiscale}, and ``Pokec-Locality'' \cite{takac2012data}.
``Synthetic'' is the enlarged graph in Fig.~\ref{fig:c2}, which contains strong \xophily \nef. 
Noisy edges are injected, and the dense blocks are constructed by creating higher-order structures.

% \begin{itemize}
%     \item \textbf{``Synthetic''} is a graph enlarged by the one in Figure~\ref{fig:crown}. It contains both heterophily and homophily \neteffect. Noisy edges are randomly injected in the background, and the dense blocks are constructed by randomly creating higher-order structures.
%     \item \textbf{``Pokec-Gender''}~\cite{takac2012data} is an online social network in Slovakia. Nodes are labeled by users' genders instead, and are removed if with unknown gender.
%     \item \textbf{``arXiv-Year''}~\cite{hu2020open} is a citation network between all CS arXiv papers. Nodes are labeled by the posted years.
%     \item \textbf{``Patent-Year''}~\cite{leskovec2005graphs} is the patent citation network from $1980$ to $1985$. Nodes are labeled by the application year, bucketized into five consecutive 3-year ranges.
%     \item \textbf{``Facebook''}~\cite{rozemberczki2019multiscale} is a page-to-page network of verified Facebook sites. Nodes are labeled by the categories such as politicians and companies.
%     \item \textbf{``GitHub''}~\cite{rozemberczki2019multiscale} is a social network of developers. Nodes are labeled as web or a machine learning developer.
%     \item \textbf{``arXiv-Category''}~\cite{wang2020microsoft} is the same dataset as ``arXiv-Year''. Nodes are labeled by the primary categories.
%     \item \textbf{``Pokec-Locality''}~\cite{takac2012data} is the same dataset as the Pokec-Gender dataset. Nodes are labeled by the uses' localities.
% \end{itemize}

% \subsection{Baselines} \label{ssec:appendixbaselines}
% \begin{itemize}
% \item {\bf \gcn\footnote{\url{https://github.com/tkipf/pygcn}}} \cite{kipf2016semi} is a well-known deep graph model, learning and aggregating the weights of two-hop neighbors.

% \item {\bf \appnp}\footnoteref{fn:gprgnn} \cite{klicpera2018predict} utilizes personalized PageRank to leverage the local information and a larger neighborhood.

% \item {\bf \mixhop}\footnote{\url{https://github.com/benedekrozemberczki/MixHop-and-N-GCN}} \cite{abu2019mixhop} mixes powers of the adjacency matrix to incorporate more than 1-hop neighbors in each  layer.

% \item {\bf \gprgnn}\footnote{\label{fn:gprgnn}\url{https://github.com/jianhao2016/GPRGNN}} \cite{chien2021adaptive} allows the learnable weights to be negative during propagation with Generalized PageRank. 

% \item {\bf \hols\footnote{\url{https://github.com/dhivyaeswaran/hols}}} \cite{eswaran2020higher} is a label propagation method with attention, by increasing the importance of a neighbor if they appear in the same motif at the same time.
 
% \item {\bf \linbp} \cite{DBLP:journals/pvldb/GatterbauerGKF15} is a linearized belief propagation with closed-form solution. We adopt the version without echo cancellation since the scaling factor will make it negligible.

% \end{itemize}

\subsection{Hyperparameters} \label{ssec:hyper}
For \method, we use random walks of length $4$ with $10$ trials except ``GitHub'', ``arXiv-Category'' and ``Pokec-Locality'', where we use $30$ trials. 
The decomposition rank is set to be $256$.
% , which is empirically shown to be enough in the embedding tasks.
The weights of \hols for different motifs are set to be equal.
% For GNNs, under the setting that the given labels are very few, it is impossible to separate a validation set. 
For GNNs, we train for $200$ epochs, which is sufficient enough for them to converge. 
% All the fully connected layers are replaced by the sparse version in order to fit into memory. 
% Both adjacency matrices and features are normalized and turn into sparse matrices if needed. 
For other hyperparameters, we use the default settings given by the authors, and give the details in Table~\ref{tab:hyper}.

% \subsection{Hardware} \label{ssec:hardware}
% Additionally, deep graph models are also evaluated on a GPU-machine with RTX A6000 GPU which has $48$GB GPU memory, $2.25$GHz CPU and $512$GB RAM if required.

% \newpage
% \tableofcontents

% \subsection{Ablation Study} \label{ssec:linbp}
% Furthermore, we compare \method with \linbp to display its advantages in Figure~\ref{fig:linbp}.
% The accuracy gap between them indicates the necessity of precisely estimating the compatibility matrix.
% Owing to \emphasis and \NEF, \method significantly improves the accuracy in most cases while adding negligible extra penalty on run time, providing the best trade-off compared with \linbp.

% \begin{figure}[]
% 	\centering
% 	\subfloat[\label{fig:linbp1} ``Synthetic'': Mixed with Strong \nef]
% 	{\includegraphics[scale=0.4]{FIG/linbp.pdf}} \\
% 	\vspace{-3mm}
% 	\subfloat[\label{fig:linbp2}] %``Pokec-Gender'': Heterophily with Strong \nef
% 	{\includegraphics[scale=0.38]{FIG/linbp_bar.pdf}}
% 	\vspace{-3mm}
% 	\caption{\underline{\smash{Ablation Study: \method provides the best}}\\
% 	\underline{\smash{trade-off between accuracy and running time compared}}\\
% 	\underline{\smash{with \linbp.}}}
% 	\label{fig:linbp}
% \end{figure}

\section{Scalability} \label{ssec:dollar}
% t3.small (3.3GHz, 2GB RAM): 0.023 * (t / 60)
% p3.2xlarge (1 V100 GPU): 3.06 * (t / 0.89 / 60)
% A100 to A6000: https://lambdalabs.com/blog/nvidia-rtx-a6000-benchmarks/
We select machines provided by AWS with comparable specs as we use for the experiments.
For CPU machine, we select t3.small with 3.3GHz CPU and 2GB RAM, which is faster than ours, and costs $\$0.023$ per hour.
For GPU machine, we select p3.2xlarge with a V100 GPU, which costs $\$3.06$ per hour.
According to \cite{NVIDIA}, it is 0.89 slower than the RTX A6000 GPU we use on running PyTorch.
The running time of \gcn on ``Pokec-Gender'' and ``Pokec-Locality'' are $673$ and $730$ seconds, respectively.
Using the provided information, the results in Table~\ref{tab:dollar} can be computed.