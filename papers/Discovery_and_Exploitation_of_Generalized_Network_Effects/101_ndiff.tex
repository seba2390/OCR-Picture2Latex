
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AUTHOR: Christos Faloutsos
% INSTITUTION: CMU
% DATE: April 2019
% GOAL: to streamline the paper presentations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \notice{{\em novelty}: NO citations, outside the 'survey'  -  they make \method seem incremental. }
% \notice{Give table of symbols and dfns}


% In this section, we present how to handle \ndiff by node representation and proposed ``emphasis'' matrix, which are introduced in Section~\ref{ssec:ne} and \ref{ssec:att} respectively.

In Section~\ref{ssec:ne}, we present how to handle \ndiff by embedding the nodes into structure-aware representations via random walks. We then use these representations to construct our proposed \emphasis to draw attention to well-connected neighbors in Section~\ref{ssec:att}.

% Table~\ref{tab:dfn} gives the list of symbols we use.

% \setlength{\tabcolsep}{1pt}
% \begin{table}[]
% % \begin{center}
% \centering{\resizebox{0.83\columnwidth}{!}{
% \begin{tabular}{| C{1.5cm} | L{6cm}|}
% \hline  
% Symbols & Definitions \\ 
% \hline
% $G$ & Undirected and unweighted graph \\ 
% ${\boldsymbol A}_{n \times n}$  & Adjacency matrix \\
% ${\boldsymbol D}_{n \times n}$  & Degree matrix \\
% ${\boldsymbol W}_{n \times n}$  & Proximity matrix \\
% ${\boldsymbol X}_{n \times d}$  & Node embedding \\
% ${\boldsymbol A}^{*}_{n \times n}$ & ``Emphasis'' matrix \\
% ${\boldsymbol H}_{c \times c}$  & Compatibility matrix \\
% ${\boldsymbol E}_{n \times c}$  & Explicit belief \\
% ${\boldsymbol B}_{n \times c}$  & Final belief \\
% ${\boldsymbol p}$  & Prior set\\
% \hline
% $n$  & Number of nodes \\
% $m$  & Number of edges \\
% $c$  & Number of classes \\
% $L$  & Number of steps \\
% $M$  & Number of random walks trials \\
% $f$  & Scaling factor \\
% \hline
% $N(i)$ & Neighbors of node $i$ \\
% $\circ$ & Element-wise multiplication of matrices \\
% $\log{(\cdot)}$ & Element-wise logarithm of matrix \\
% $\hat{\cdot}$ & Residual matrix form \\
% \hline
% \end{tabular} 
% }}
% \caption{Symbols and Definitions \label{tab:dfn}}
% % \end{center}
% \end{table}


\subsection{Node Representation} \label{ssec:ne}
We represent nodes in $d$-dimensional vector space using Singular Value Decomposition (SVD) on a random walk the proximity matrix of the graph which is a function of adjacency matrix ${\boldsymbol A}$ and captures information from pairwise connections.
The one-step transition matrix is defined as ${\boldsymbol P}={\boldsymbol D}^{-1}{\boldsymbol A}$, where ${\boldsymbol D}^{-1}$ is the diagonal matrix with the reciprocals of degrees. Each neighbor has an equal probability of being visited.
To reach a high-order proximity with finite length $L$, the transition matrix can be defined as ${\boldsymbol \Pi}_{(L)}=\sum_{l=1}^{L}{{\boldsymbol P}^{l}}$, where the matrix is dense after only a few steps.
Another way to achieve this goal is by counting the motifs around the node, such as triangles and 4-cliques. 
However, both methods suffer from computationally expensive costs, especially when $n$ is large.

To address this issue, we utilize random walks to approximate the high-order proximity matrix. We describe our random walk algorithm in Algorithm~\ref{algo:rw} from line $1$ to $10$. Given a proximity matrix ${\boldsymbol W}$, ${\boldsymbol W}^{'}_{ij}$ records the number of times we visit node $j$ if we start a random walk from node $i$. Each neighbor has the same probability of being visited in the unweighted graphs.
This ensures that only those structurally important neighbors are visited, and thus results in a sparse matrix.
We prove that the neighbor distribution for each node converged after a number of trials.

\begin{lemma} [Convergence of Regular Random Walks] \label{lem:crw1}
With probability $1 - \delta$, the error of estimated distribution $\epsilon$ for a node walking to its 1-hop neighbor by a regular random walk of length $L$ with $M$ trials is less than
\begin{equation}
    \epsilon \leq \frac{\lceil (L - 1) / 2 \rceil}{L} \sqrt{\frac{\log{(2/\delta)}}{2LM}}
\end{equation}
\end{lemma}

To make the estimation more efficient, we use non-backtracking random walk to enhance the speed of convergence.
Given the start node $s$ and walk length $L$, its function is defined as follows:
\begin{equation}
\mathcal{W}(s, L) = 
\begin{cases} (w_{0}=s, ..., w_{L}) \space 
\begin{array}{c}
w_{l} \in N{(w_{l-1})}, \forall l \in [1, L]
\\
w_{l-1} \neq w_{l+1}, \forall l\in [1, L-1]
\end{array},
\end{cases}
\end{equation}
where $N(i)$ denotes the neighbors of node $i$. Thus, with the same $L$ and $M$, we improve Lemma~\ref{lem:crw1} to have a tighter bound of $\epsilon$.

\begin{lemma} [Convergence of Non-Backtracking Random Walks] \label{lem:crw2}
With the same condition as in Lemma~\ref{lem:crw1}, the error $\epsilon$ by a non-backtracking random walks is less than
\begin{equation}
    \epsilon \leq \frac{\lceil (L - 1) / 3 \rceil}{L} \sqrt{\frac{\log{(2/\delta)}}{2LM}}
\end{equation}
\end{lemma}

\begin{proof}
We prove Lemma~\ref{lem:crw1} and ~\ref{lem:crw2} in Appendix~\ref{ap:subsec:prooflemma12}.
\end{proof}

For example, when using regular random walks of length $L=4$ with $M=30$ trials, the estimated error by Lemma~\ref{lem:crw1} with probability $95\%$ is about $6.2\%$. Nevertheless, if we instead use non-backtracking random walks, the error can be reduced to $3.1\%$, which is $2\times$ lower than the one by regular walks. Noting that unlike most methods, \method has few parameters in the algorithm. The resulting low error $\epsilon$ also indicates that the estimated distribution converges well to the true distribution. Therefore, the parameters are insensitive.

In Algorithm~\ref{algo:rw} line $11$, an element-wise multiplication by adjacency matrix ${\boldsymbol A}$ is done to keep the approximation of 1-hop neighbor for each node, which sufficiently supplies necessary information as well as keeps the resulting matrix sparse. 
We use ${\boldsymbol D}^{-1}$ to reduce the influence of nodes with larger degrees. This prevents them from dominating the pairwise distance by containing more elements in their rows. 
The element-wise logarithm taken in line $12$ aims to rescale the distribution in proximity matrix ${\boldsymbol W}$ so as to enlarge the difference between smaller structures.
% which is shown to be useful in several studies \cite{DBLP:journals/corr/ChenNAKF17, qiu2018network}.
We use SVD for efficient rank-$d$ decomposition of the proximity matrix ${\boldsymbol W}$ since it is sparse. We multiply the left-singular vectors ${\boldsymbol U}$ by the corresponding squared eigenvalue $\sqrt{{\boldsymbol \Sigma}}$ to correct the scale.


\begin{algorithm}[htbp]
% \SetAlgoVlined
\SetAlgoLined
% \SetNoLine
\LinesNumbered
\KwData{Adjacency matrix ${\boldsymbol A}$, number of trials $M$, and number of steps $L$}
\KwResult{Proximity matrix ${\boldsymbol W}$}
${\boldsymbol W}^{'} \leftarrow O_{n \times n}$\;
\tcc{approximate proximity matrix by random walk}
\For{node $i$ in $G$}{
    \For{$m = 1, ..., M$}{
        \For{$j \in \mathcal{W}(i, L)$}{
            ${\boldsymbol W}^{'}_{ij} \leftarrow {\boldsymbol W}^{'}_{ij} + 1$\;
        }
    }
}
\tcc{degree normalization and masking}
${\boldsymbol W} \leftarrow {\boldsymbol D}^{-1}({\boldsymbol W}^{'} \circ {\boldsymbol A})$\;
${\boldsymbol W} \leftarrow \log{({\boldsymbol W})}$\textcolor{blue}{\tcp*{only non-linearity}}
Return ${\boldsymbol W}$\;
\caption{Proximity Matrix Computation \label{algo:rw}}
\end{algorithm}

\subsection{ Proposed ``Emphasis'' Matrix} \label{ssec:att}
One of the novelties of \method is to draw the support from \ndiff, meaning that for each node, not all neighbors are equal, i.e., different neighbors have different influence levels to the node with respect to node labels.
To estimate the node similarity without node features, we compute distance of nodes in the embedding space. The intuition is that the nodes that are closer in the embedding space should be more similar. Given the embedding introduced in Section~\ref{ssec:ne}, the similarity function $\mathcal{S}$ is then given as follows:
\begin{equation} \label{eq:law}
    \mathcal{S}({\boldsymbol X}_{i}, {\boldsymbol X}_{j}) = e^{-\sqrt{\sum^{d}_{k=1}{({\boldsymbol X}_{ik} - {\boldsymbol X}_{jk})^{2}}}}, 
\end{equation}
where $e \approx 2.718$ denotes Euler's number. Equation~\ref{eq:law} is a universal law proposed by Shepard \cite{shepard1987toward}, connecting the similarity with distance via an exponential function. Here we use Euclidean to estimate the distance since it is one of the most popular distance metrics. Negative exponential distribution is used to bound the node similarity from 0 to 1, which will be close to 0 if the distance is larger than a certain value. Given an adjacency matrix ${\boldsymbol A}$ and node embedding ${\boldsymbol X}$, \emphasis ${\boldsymbol A}^{*}$ with weighted edges estimated by $\mathcal{S}$ is then defined as follows:
\begin{equation}
    \mathcal{E}({\boldsymbol A}, {\boldsymbol X}) = {\boldsymbol A}^{*} \text{, where } {\boldsymbol A}^{*}_{ij} = \mathcal{S}({\boldsymbol X}_{i}, {\boldsymbol X}_{j}), \forall \{i, j | {\boldsymbol A}_{ij} = 1\}
\end{equation}
Since $\mathcal{S}({\boldsymbol X}_{i}, {\boldsymbol X}_{j}) = \mathcal{S}({\boldsymbol X}_{j}, {\boldsymbol X}_{i})$, ${\boldsymbol A}^{*}$ is still a symmetric matrix. This convenient property allows us to use SVD for the fast computation of the spectral radius (see Section~\ref{ssec:con}).

