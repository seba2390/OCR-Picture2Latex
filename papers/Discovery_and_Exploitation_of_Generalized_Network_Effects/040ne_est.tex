
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AUTHOR: Christos Faloutsos
% INSTITUTION: CMU
% DATE: April 2019
% GOAL: to streamline the paper presentations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Given that a graph exhibits \nef, how can we estimate the all-pair relations between classes?
% \nef can be described by a compatibility matrix, which contains the class-wise strength of edges.
% In this section, we estimate \nef by computing the underlying compatibility matrix with the given graph and few node labels.
% We first propose to quantify \nef by a \emph{compatibility matrix}, which describes the all-pair relations with probabilities.
% We first discuss the defect of using edge counting for estimation.
A \emph{compatibility matrix} is a natural strategy to describe the relations, which has been widely used in the literature.
We propose \methodest, which turns the compatibility matrix estimation into an optimization problem based on a closed-form formula.
\methodest not only overcomes the limitation of naive edge counting, but is also robust to noisy observations even with few observed labels.
% However, if the edges are noisy, the quality of the estimation decreases, which hurts the accuracy during exploitation in our empirical study.
% To address this issue, we then illustrate how to only pay attention to the important neighbors, by the \emphasis.
% This not only leads to a much more precise estimation on compatibility matrix, it also helps on exploiting \nef, which will later be presented in Sec.~\ref{sec:neexp}.\looseness=-1

\subsection{Why NOT Edge Counting}
Given $c$ classes, a $c \times c$ compatibility matrix explains the relative relation between each class pair.
The graph represented in Fig.~\ref{fig:ecexgt} exhibits heterophily between class pairs $(1, 2)$ and $(3, 4)$, while it exhibits homophily in classes $5$ and $6$.
A compatibility matrix is commonly used in existing studies, but assumed to be given by domain experts, instead of being estimated.
For example, given a friendship graph with strong homophily, we can assume that the compatibility matrix looks like an identity matrix.
% , meaning that a class is most related to itself.
% For an easy example, given a friendship graph with two classes, talkative and silent people, we can assume that the compatibility matrix is an identity matrix, meaning that the graph is homophily.
However, in more complex scenarios such as citation networks where the labels are publish years, it is unclear how a compatibility matrix should look like.
% \reminder{can we cite another example, pub yr as label doesn't feel right, though true?}
% Even more, there is no standard way to estimate the ground truth of compatibility matrix.

A naive way to estimate a compatibility matrix is via counting labeled edges, but it has two limitations: 
1) rare labels will get neglected, and 
2) it is noisy or biased due to few labeled nodes.
The result is even more unreliable if the given labels are imbalanced.
In Fig.~\ref{fig:ecex}, we show an example where we upsample training labels $10\times$ only for class $1$ using the graph in Fig.~\ref{fig:c2}.
Edge counting in Fig.~\ref{fig:ecex1} biases towards the upsampled class and clearly fails to estimate the correct compatibility matrix in Fig.~\ref{fig:ecexgt}, while our proposed \methodest succeeds in Fig.~\ref{fig:ecex2}.
This commonly occurs in practice, since we observe only limited labels in node classification tasks, and becomes fatal if the observed distribution is different from the true one.

\subsection{Closed-Form Formula} \label{ssec:comp}
% \reminder{christos will give summary/intuition}
% \reminder{Jeremy, major re-write below}
How can we find an accurate compatibility matrix?
The compatibility matrix $\hat{{\boldsymbol H}}$ is an essential component for proper execution of BP in Eq.~\ref{eq:prop}.
In this subsection, we propose to estimate it through Lemma~\ref{lem:nef} and we present the rationale.

We begin the derivation by rewriting Eq.~\ref{eq:prop} of BP.
% Our goal is to estimate the compatibility matrix $\hat{{\boldsymbol H}}$. 
% The closed formula is given in Lemma~\ref{lem:nef} below.
The main insight is reminiscent of `leave-one-out' cross validation.
That is, we find $\hat{{\boldsymbol H}}$ that would make the results of the propagation (RHS of Eq.~\ref{eq:simple})
to the actual values (LHS  of Eq.~\ref{eq:simple}):
\begin{equation} \label{eq:simple}
\underbrace{\hat{{\boldsymbol E}}}_\text{reality} \approx
\underbrace{
{\boldsymbol A}\hat{{\boldsymbol E}}\hat{{\boldsymbol H}} }_\text{estimate}
\end{equation}

\hide{
As an essential component, the compatibility matrix is used by BP for proper propagation.
Therefore, we begin the derivation by simplifying Eq.~\ref{eq:prop} of BP.
If we initialize $\hat{{\boldsymbol B}}$ with $\hat{{\boldsymbol E}}$, and omit the addition of $\hat{{\boldsymbol E}}$ for the iterative propagation purpose, we have:
\begin{equation} \label{eq:simple_old}
\hat{{\boldsymbol B}} = {\boldsymbol A}\hat{{\boldsymbol E}}\hat{{\boldsymbol H}}
\end{equation}
}% end hide

\hide{
Our goal is to estimate the compatibility matrix $\hat{{\boldsymbol H}}$ of a given graph, so that the difference between belief propagated by the given priors $\mathcal{P}$ and the final belief is minimized.
Nevertheless, the final belief $\hat{{\boldsymbol B}}$ is not available before we run the propagation on the graph.
To address that, we mimic the leave-one-out cross-validation, by speculating the label of the node through its labeled neighbors, which turns Eq.~\ref{eq:simple} into $\hat{{\boldsymbol E}} \approx {\boldsymbol A}\hat{{\boldsymbol E}}\hat{{\boldsymbol H}}$.
% \begin{equation} \label{eq:simple2}
% 	\hat{{\boldsymbol E}} \approx {\boldsymbol A}\hat{{\boldsymbol E}}\hat{{\boldsymbol H}}
% \end{equation}
In other words, we aim to minimize the difference between initial belief of each node $i \in \mathcal{P}$ by the initial beliefs of its neighbors $N(i) \in \mathcal{P}$, i.e., $N(i) \cap \mathcal{P}$. 
Intuitively, the neighbors are able to estimate the belief for the node.
}

Formally, we want to minimize the difference between the reality and the estimate:
% We then formulate the optimization problem as follows:\looseness=-1
\begin{equation} \label{eq:opt}
    \min_{\hat{\boldsymbol H}}\sum_{i \in \mathcal{P}}{\sum_{u=1}^{c} \|{\hat{\boldsymbol E}_{iu}} - \sum_{k=1}^{c}{\sum_{j \in N(i) \cap \mathcal{P}}{\hat{\boldsymbol E}_{jk}}\hat{\boldsymbol H}_{kl}}}\|^2,
\end{equation}
where $N(i)$ denotes the neighbors of node $i$. 
In other words, we aim to minimize the difference between initial belief $\hat{\boldsymbol E}$ of each node $i \in \mathcal{P}$ by the ones of its neighbors $N(i) \in \mathcal{P}$, i.e., $N(i) \cap \mathcal{P}$. 
To estimate the compatibility matrix $\hat{{\boldsymbol H}}$, we solve the optimization problem in Eq.~\ref{eq:opt} with the proposed closed-form formula:
% by deriving the closed-form solution from Eq.~\ref{eq:simple2} as follows:
\begin{lemma}[\NEF (NEF)] \label{lem:nef}
Given adjacency matrix ${\boldsymbol A}$ and initial beliefs $\hat{{\boldsymbol E}}$, the closed-form solution of vectorized compatibility matrix $\text{vec}{(\hat{{\boldsymbol H}})}$ is:
\begin{equation}
\boxed{
    \text{vec}{(\hat{{\boldsymbol H}})} = ({\boldsymbol X}^{T}{\boldsymbol X})^{-1}{\boldsymbol X}^{T}{\boldsymbol y}
    }
\end{equation}
where ${\boldsymbol X} = {\boldsymbol I}_{c \times c} \otimes ({\boldsymbol A}\hat{{\boldsymbol E}})$ and ${\boldsymbol y} = \text{vec}{(\hat{{\boldsymbol E}})}$.
\end{lemma}
\begin{proof}
% Omitted for brevity. Proof in Appendix 
See Appendix~\ref{ap:subsec:proofprop1}.
\end{proof}

% As discussed before, we can replace it by ${\boldsymbol y} = \text{vec}{(\hat{{\boldsymbol E}})}$, and extract the ones that are corresponding to the priors $\mathcal{P}$. 
% With NEF, the optimization problem can now be solved by regression.

% \begin{figure}[]
% 	\centering
% 	\subfloat[\label{fig:ecex1} Balanced Prior]
% 	{\includegraphics[height=1.17in]{FIG/ec_example1.pdf}}
% 	\hspace{5mm}
% 	\subfloat[\label{fig:ecex2} Imbalanced Prior]
% 	{\includegraphics[height=1.17in]{FIG/ec_example2.pdf}} 
% 	\vspace{-3mm}
% 	\caption{\label{fig:ecex} \underline{\smash{Edge counting can not handle imbalanced case.}} Class $1$ is upsampled in this example.}
% \end{figure}

\begin{figure}[t]
    \centering
    \subfloat[\label{fig:ecexgt} Ground Truth]
    {\includegraphics[height=1.05in]{FIG/ec_example_gt.pdf}}
    % \hspace{1mm}
    \subfloat[\label{fig:ecex1} Edge Counting]
    {\includegraphics[height=1.05in]{FIG/ec_example_ec.pdf}}
    % \hspace{1mm}
    \subfloat[\label{fig:ecex2} \methodest]
    {\includegraphics[height=1.05in]{FIG/ec_example_ne.pdf}} 
    \vspace{-3mm}
    \caption{\label{fig:ecex} \underline{\smash{\methodest handles imbalanced case well}}, while edge counting fails. Labels of class $1$ is upsampled.}
    \vspace{-3mm}
\end{figure}

\subsection{\methodest}
The overall algorithm of \methodest is presented in Alg.~\ref{algo:cmest}.
In practice, we can use any form of adjacency matrix for the estimation.
The proposed NEF allows us to estimate the compatibility matrix by solving this optimization problem, but there still exists a practical challenge that need to be addressed.
With few labels, it is difficult to properly separate them into training and validation sets for the regression, and the estimation can easily be interfered by the noisy observations.
We thus use ridge regression with leave-one-out cross-validation (RidgeCV) instead of the regular linear regression.
This allows us to fully utilize the observations without having biases caused by random splits of training and validation sets.
Moreover, the regularization effect of RidgeCV makes the compatibility matrix more robust to noisy observations.
It is noteworthy that its computational cost is negligible.\looseness=-1

% \emph{Algorithm.}
% The overall process of estimation is shown in Algorithm~\ref{algo:cmest}.
% % In line $1$ to $2$, we filter out the labels having no network effect by analyzing the dataset.
% We extract the indices that are corresponding to the priors after the Kronecker product and vectorization in line $2$ to $7$.
% The optimization is then conducted in line $8$ to $10$ to estimate the compatibility matrix $\hat{{\boldsymbol H}}^{*}$.
% The negative value filtering and row normalization is done on line $11$.

% ---------------------------------------------------------------
% ---------------------------------------------------------------
% ---------------------------------------------------------------

% \subsection{\methodest}
% The overall algorithm of \methodest is presented in Alg.~\ref{algo:cmest}.
% Instead of using the original adjacency matrix ${\boldsymbol A}$, we use \emphasis ${\boldsymbol A}^{*}$ to pay attention to the neighbors that are connected with higher-order structures.
% Since the rows of the estimated matrix ${\boldsymbol H}$ do not sum to one in this approach, we filter out the negative values and normalize the sum of each row to one.
% This is done safely, where the negative values represent negligible relationships between nodes.

\begin{algorithm}[t]
\KwData{Adjacency matrix ${\boldsymbol A}$, initial belief $\hat{{\boldsymbol E}}$, and priors $\mathcal{P}$}
\KwResult{Estimated compatibility matrix $\hat{{\boldsymbol H}}$}
% ${\boldsymbol i} \leftarrow \emptyset$\textcolor{blue}{\tcp*{indices only related to priors}}
% \For{$p \in \mathcal{P}$}{
%     \For{$j=1,...,c$}{
%         ${\boldsymbol i} \leftarrow {\boldsymbol i} \cup \left\{ p + (j - 1) * c \right\}$\;
%     }
% }
${\boldsymbol X} \leftarrow {\boldsymbol I}_{c \times c} \otimes ({\boldsymbol A}\hat{{\boldsymbol E}})$\textcolor{blue}{\tcp*{feature matrix}}
${\boldsymbol y} \leftarrow \text{vec}{(\hat{{\boldsymbol E}})}$\textcolor{blue}{\tcp*{target vector}}
Extract indices ${\boldsymbol i}$ with nodes in priors $\mathcal{P}$\;
$\hat{{\boldsymbol H}} \leftarrow RidgeCV({\boldsymbol X}[{\boldsymbol i}], {\boldsymbol y}[{\boldsymbol i}])$\;
% Return $\text{row-normalize}(\max{(\hat{{\boldsymbol H}}^{*}, 0)})$\;
Return $\hat{\boldsymbol H}$\;
\caption{\methodest \label{algo:cmest}}
\end{algorithm}