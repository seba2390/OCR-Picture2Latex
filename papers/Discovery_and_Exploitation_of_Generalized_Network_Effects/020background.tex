
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AUTHOR: Christos Faloutsos
% INSTITUTION: CMU
% DATE: April 2019
% GOAL: to streamline the paper presentations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We introduce the notation and preliminary of this paper in the background, and list the baselines in related work.
% Table~\ref{tab:salesman} presents qualitative comparison of state-of-the-art approaches against our proposed \method.
% No competitor fulfills all the specs in Table~\ref{tab:salesman}.\looseness=-1

\subsection{Background}
\subsubsection*{Notation}
Let $G$ be an undirected and unweighted graph with $n$ nodes and $m$ edges with ${\boldsymbol A}$ as the adjacency matrix. ${\boldsymbol A}_{ij} = 1$ indicates that nodes $i, j$ are connected by an edge. Each node $i$ has a unique label $l(i) \in \{1, 2, \dots, c\}$, where $c$ denotes the number of classes. Let ${\boldsymbol E} \in \mathbb{R}^{\n \times \classes}$ be the initial belief matrix containing the prior information, i.e., the labeled nodes. 
${\boldsymbol E}_{ik} = 1$ if $l(i) = k$,
% the label of node $i$ is class $k$, 
and the rest entries of the $i^{th}$ row are all zeros. 
For the nodes without labels, all the entries corresponding to those nodes are set to $1 / c$. ${\boldsymbol H} \in \mathbb{R}^{\classes \times \classes}$ is a row-normalized compatibility matrix where ${\boldsymbol H}_{kl}$ denotes the relative influence of class $l$ on class $k$.
% A matrix ``centered around'' $k$ has all its entries close to $k$ and the average of the entries is exactly $k$. 
The residual of a matrix around $k$ is denoted as $\hat{{\boldsymbol Y}}$ and is defined as $\hat{{\boldsymbol Y}} = {\boldsymbol Y} - k\times\mathbbm{{\boldsymbol 1}}$ where ${\boldsymbol Y}$ is centered around $k$, and $\mathbbm{{\boldsymbol 1}}$ is matrix of ones.
\footnote{A matrix ``centered around'' $k$ has all its entries close to $k$ and the average of the entries is exactly $k$.}
% \footnote{For simplicity, we omit the word ``residual'' in the paper and show it by symbol.}

\subsubsection*{Belief Propagation} 
Belief Propagation~(BP) is a popular method for label inference in graphs~\cite{pearl2014probabilistic, DBLP:journals/pvldb/GatterbauerGKF15, DBLP:conf/pkdd/KoutraKKCPF11}.
\fabp~\cite{DBLP:conf/pkdd/KoutraKKCPF11} and \linbp \cite{DBLP:journals/pvldb/GatterbauerGKF15} accelerate BP by approximating the final belief assignment from BP. In particular, \linbp approximates the final belief as:
\begin{equation} \label{eq:prop}
	\hat{{\boldsymbol B}} = \hat{{\boldsymbol E}} + {\boldsymbol A}\hat{{\boldsymbol B}}\hat{{\boldsymbol H}},
\end{equation}
where $\hat{{\boldsymbol B}}$ is a residual final belief matrix, initialized with all zeros.
The compatibility matrix ${\boldsymbol H}$ and initial beliefs ${\boldsymbol E}$ are centered around $1 / c$ to ensure convergence.
\hols \cite{eswaran2020higher} is a BP-based method, which leverages higher-order graph structures, i.e. $k-$cliques.
It propagates the labels by incorporating the weights from higher-order cliques. 
Although it works well on homophily graphs, mining cliques is prohibitive for large graphs.

\subsection{Related Work}
Next we list recent baselines.
Table~\ref{tab:salesman} presents qualitative comparison of state-of-the-art approaches against our proposed \method.
Notice that only \method fullfills all the specs.

\subsubsection*{Analysis by Homophily Statistics}
While many studies \cite{zhu2020beyond, luan2021heterophily, lim2021large, ma2022is} utilize homophily ratio to measure how common the labels of the connected node pairs share the same class;
others \cite{ma2021homophily, du2022gbk} try to explain that heterophily may not be the root cause of the performance degradation.
Our work does not aim to criticize either studies; instead, it focuses on three very different aspects.
First, the low homophily ratios are determined to be heterophily in most studies, where the case with no \nef is neglected.
They are designed to identify the absence of homophily, instead of distinguishing different non-homophily cases.
Our proposed \methodtest correctly spots this case by a statistical test, where all the classes connect to other classes uniformly.
Second, while their claims are based on the accuracy of using both node features and graph structure for node classification, \methodtest analyzes the \nef in a model-free way and purely focuses the graph structure.
Third, unlike studies that calculated homophily ratio of datasets using all labels, which is impractical in real-world, \methodtest requires only a few labels in the training set.
We further show that better estimating and exploiting \nef leads to a better inference of labels by our \method.\looseness=-1

\setlength{\tabcolsep}{1.5pt}
\begin{table}[t]
% 	\centering
\caption{
    \underline{\smash{\method matches all specs}}, while baselines miss one or more of the properties.
    `?' denotes unclear from the original paper, and `N/A' denotes not applicable.
    \label{tab:salesman}
}
\centering{\resizebox{0.95\columnwidth}{!}{
    \begin{tabular}{ l | L{4.7cm} | C{0.55cm}  C{0.55cm}  C{0.55cm}  C{0.55cm} | C{0.55cm} }
        \hline
        \multicolumn{2}{c|}{\bf Property} & 
        \rotatebox{90}{BP \cite{DBLP:conf/pkdd/KoutraKKCPF11, DBLP:journals/pvldb/GatterbauerGKF15}} &
        \rotatebox{90}{HOLS \cite{eswaran2020higher}} & 
        \rotatebox{90}{General GNNs \cite{kipf2016semi, klicpera2018predict}} & 
        \rotatebox{90}{Het. GNNs \cite{abu2019mixhop, chien2021adaptive}} & 
        \rotatebox{90}{\bf \method} \\ 
        \hline
        \multirow{2}{*}{\bf 1. \theory} & 
        1.1. Statistical Test & 
        \CheckmarkBold & \CheckmarkBold & & & \CheckmarkBold \\ 
         & 
        1.2. Convergence Guarantee & 
        \CheckmarkBold & \CheckmarkBold & & & \CheckmarkBold \\ 
        \hline
        {\bf 2. \explain} & 
        2.1 Compatibility Matrix Estimation &
        N/A & N/A &  &  & \CheckmarkBold \\ 
        \hline
        \multirow{2}{*}{\bf 3. \general} &  
        3.1 Handle Heterophily &  
        ? & ? & ? & \CheckmarkBold & \CheckmarkBold \\ 
         &  
        3.2 Handle \nef &  
        ? & ? &  &  & \CheckmarkBold \\ 
        \hline
        \multirow{2}{*}{\bf 4. \scale} &
        4.1. Linear Complexity & 
        \CheckmarkBold &  & \CheckmarkBold & \CheckmarkBold & \CheckmarkBold \\
         &
        4.2. Thrifty & 
        \CheckmarkBold & \CheckmarkBold & ? & ? & \CheckmarkBold \\
        \hline
    \end{tabular}
}}
% \vspace{-0.1in}
\end{table}

\subsubsection*{Node Classification}
Recently, research has focused on models that learn embeddings using neural networks. 
% DeepWalk~\cite{perozzi2014deepwalk} learns embeddings by predicting the neighborhood that is characterized by random walks on the graph, while node2vec~\cite{grover2016node2vec} extends DeepWalk with breadth-first random walk strategies. 
% These embeddings along with available node labels are then used to learn classifiers.
Graph Convolutional Networks (GCN)~\cite{kipf2016semi} employ approximate spectral convolutions to incorporate neighborhood information.
\appnp \cite{klicpera2018predict} utilizes personalized PageRank to leverage the local information and a larger neighborhood.
% To account for \nd, Graph Attention Networks (GAT)~\cite{velivckovic2017graph, kim2020find} allow for assigning importance weights to neighborhoods.
% However, attention GNNs require node features, and need many learnable parameters, making it infeasible for large graphs.
Another branch of studies turn to break the general assumption of homophily.
\mixhop \cite{abu2019mixhop} mixes powers of the adjacency matrix to incorporate more than $1$-hop neighbors in each layer.
\gprgnn \cite{chien2021adaptive} allows the learnable weights to be negative during propagation with Generalized PageRank.
$\text{H}_{2}$GCN \cite{zhu2020beyond} separates the embeddings of $1$-hop and $2$-hop neighbors; nevertheless, it requires too much memory and thus can not handle large graphs.
\cite{lim2021large} introduces multiple large heterophily datasets, but the proposed solution \textsc{Linkx} is not applicable without node features.