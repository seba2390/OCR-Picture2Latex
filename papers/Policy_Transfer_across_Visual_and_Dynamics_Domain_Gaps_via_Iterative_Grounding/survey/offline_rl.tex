\section{Offline Reinforcement Learning}

\paragraph{SQIL~\citep{reddy2020sqil}:} 
Instead of learning a reward function from demonstration, this paper proposes to simply label +1 reward for transitions from demonstration and 0 reward for agent transitions and then train a policy with any off-policy RL. During training, a half of a mini-batch is sampled from expert transitions and another half is sampled from agent transitions. Hence, the agent tends to follow expert's trajectory and even it is off the expert's trajectory, it encourages to get back to an expert trajectory.