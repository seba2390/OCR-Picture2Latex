\section{Imitation Learning}

\subsection{Learning from Demonstration}

\paragraph{BC~\citep{pomerleau1989alvinn}:}
Learning a policy that mimics the expert policy represented by expert demonstrations in a supervised learning manner. A behavioral cloning policy is trained to predict $a$ from $o$ given a dataset $D=\{o_i, a_i\}$.

\paragraph{ABM~\citep{siegel2020keep}:}
The problem setup is offline RL and it updates the prior $\pi(a|s)$ with adaptive weights, which depend on a learned value approximation for each data sample (i.e. higher prior for $(s, a)$ with high value and lower prior for $(s, a)$ with low value).

\paragraph{GAIL~\citep{ho2016generative}:}
Most well-known IRL method these days. A discriminator network learns to distinguish expert transitions from agent transitions. Then, a policy uses the discriminator output as a reward signal and learns to fool the discriminator, which means the agent trajectory becomes similar to the expert trajectory. Due to unstable adversarial training, it is not easy to train and requires some implementation hack, such as weight initialization with a BC policy and early stopping.

\paragraph{RPL~\citep{gupta2019relay}:}
From demonstrations, a high-level policy is learned to predict a sub-goal ($\pi_{high}(s_t, s_{t+w}) \rightarrow s_{t+W_l} \forall w \in [1,W_h]$) and a low-level policy is trained to reach a sub-goal ($\pi_{low}(s_t, s_{t+w}) \rightarrow a_t \forall w \in [1,W_l]$).
This hierarchical framework can be fine-tuned for the downstream task using RL.

\paragraph{IRIS~\citep{mandlekar2019iris}:}


\paragraph{GTI~\citep{mandlekar2020learning}:}


\paragraph{T-REX~\citep{brown2019extrapolating} and D-REX~\citep{brown2019better}:}
T-REX learns a reward function from demonstrations with ranking. The objective to learn a reward can be written as

\begin{align}
  \label{eq:ranking_loss}
  \mathcal{L}_{\theta} = -\mathbb{E}_{\tau_i < \tau_j \sim \mathcal{D}} \left[
    \
    \log \frac{\exp \sum_{s_t \in \tau_j}{f_{\phi}(s_{t})}}{\exp \sum_{s_t \in \tau_i}{f_{\phi}(s_{t})} + \exp \sum_{s_t \in \tau_j}{f_{\phi}(s_{t})} }
  \right].
\end{align}

D-REX extends T-REX by generating ranked demonstrations by perturbing an expert policy with increasing scales of noises.

\paragraph{UPN~\citep{srinivas2018universal}:}

\subsection{Learning from Observations}

\paragraph{BCO~\citep{torabi2018behavioral}:}
Behavioral Cloning from Observation first collects rollouts from random exploration and learns an inverse model $\hat{a} \sim f(s, s')$. Then, it uses the inverse model to predict action label between two consecutive states in a demonstration and trains a BC policy with the state and predicted action, $(s, \hat{a})$.

\paragraph{Zero-shot Visual Imitation~\citep{pathak*2018zeroshot}:}


\paragraph{GAIfO~\citep{torabi2018generative}:}
A variant of GAIL~\citep{ho2016generative} for learning from observations. Instead of discriminating an expert transition and agent transition $(s, a)$, GAIfO discriminates an expert effect and agent effect $(s, s')$.
\readlater{I did not understand the detail of theoretical derivation.}

\paragraph{IDDM~\citep{yang2019imitation}:} 
Minimize causal entropy of the policy with the GAIfO formulation to close the gap between LfD and LfO.
\readlater{I did not understand the detail of theoretical derivation.}


\subsection{One-shot Imitation Learning}

\paragraph{One-shot Imitation~\citep{duan2017one-shot,finn2017one-shot}:}


\paragraph{SILO~\citep{lee2019silo}:}


\subsection{Third-Person Imitation}
Generally, in third-person imitation, we assume no action label is available since a demonstration is collected from an agent different from the learner (robot).

\paragraph{Perceptual Rewards~\citep{sermanet2017rewards}:}
It learns a perceptual reward function by segmenting demonstrations into fixed (3-4) segments.

\paragraph{TCN~\citep{sermanet2018time}:}
Time-contrastive loss is used to learn a latent representation between demonstrator and agent that can be used for an IRL style reward. The main idea is simple but implementation requires a lot of hack, such as augment a dataset with randomly executed robot videos.

\paragraph{Context Translation~\citep{liu2018imitation}:}
It translates a human demo frame to robot demo frame.

\paragraph{DAML~\citep{yu2018aone,yu2018bone}:}
Domain-Adaptive Meta-Learning to translate a human video to a robot action sequence.