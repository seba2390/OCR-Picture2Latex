\section{Sim2Real}

\paragraph{RCAN~\citep{james2018simtoreal}}
Learns mappings from random to canonical (simulation) images so that real world images can also be translated at test time.  Decouples randomization from policy learning and does not require any real world data. 

\paragraph{Dynamics Randomization~\citep{Peng_2018}}
Randomizing simulator dynamics during training so policies can generalize to new dynamics in real world.

\paragraph{GraspGAN~\citep{bousmalis2017using}}
Learns transformation from synthetic to real images with generative adversarial, task, and content loss.  Applies feature level domain adaptation on top of transformed images.

\paragraph{DANN~\citep{ganin2015domainadversarial}}
Domain-Adversarial Neural Networks.  Given labeled source domain data and unlabeled target domain data, learn transferable policy through features that are informative about main learning task but uninformative for domain.

\paragraph{SimOpt~\citep{chebotar2018closing}}
Learn a simulation parameter distribution for domain randomization by minimizing the discrepancy between real world observation trajectories and simulated observation trajectories.

\paragraph{Visual Domain Randomization~\citep{tobin2017domain}}
Randomizing simulation rendering for object localization in real world cluttered environment.

\paragraph{Deep Visuomotor Policy~\citep{zhu2018reinforcement}}
Training model-free visuomotor policy by using Hybrid IL/RL Reward and Leveraging Physical States in Simulation. Sim2Real Policy Transfer by doing domain randomization

\paragraph{MATL~\citep{Wulfmeier2017mutual}}
Given sim and real have the same state space and only the underlying dynamics are different. Use discriminator to generate a alignment reward to match the trajectories between sim and real. 

\paragraph{Inverse dynamics model~\citep{Malmir2020Robust}}
While training in simulation, learn a inverse dynamics model that, given two state observation, predicts the action taken in-between. In real environment, compare the predicted action by inverse dynamics model and the actual action taken, and according to the discrepancy between the two, add a disturbance to the next action.

\paragraph{CuNAS~\citep{Hochreiter2020Long}}
Before training, learn a LSTM that predicts the discrepancies between sim and real by exploring the environment with Goal Babbling. Then, during training, the LSTM model is used to update the simulator at every state.

\paragraph{Incremental Learning~\citep{Josifovski2020Continual}}
The agent learn incrementally from low-realism to high-realism and in the end learn in the real world

\paragraph{Imitating Animal~\citep{peng2020learning}}
Retarget source motions to robot's morphology using IK, then train a RL policy to reproduce the motion in simulation. Then, use domain adaptation to transfer to a real robot.

\paragraph{V2R via Visual Semantic Segmentation~\citep{Hong2018V2R}}
Trains a perception module that learns meta-states to guide the downstream RL control policy. The perception module takes as input RGB images and output states, which goes to control policy module. Then, the policy will react with an action. A visual-guidance module can be incorporated to adjust the meta-state to alter the behavior of the robot online.

\paragraph{Manipulate Deformable Objects without Demonstrations~\citep{Wu2020Deform}}

\paragraph{Iterative Design for Sim-to-Real~\citep{Xie2019Cassie}}


\paragraph{Closing Sim-to-Real Gap~\citep{chebotar2018closing}}

Jason's sim2real paper