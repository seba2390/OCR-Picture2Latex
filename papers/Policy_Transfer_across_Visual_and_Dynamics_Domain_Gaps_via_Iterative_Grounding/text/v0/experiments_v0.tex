\section{Experiments}

In this paper, we propose a policy transfer method with iterative grounding across visual and dynamics domains without task supervision in the target environment. Our method iteratively learns visual and action transformations with limited target environment data; uses these transformations to ground the source environment such that it emulates the target environment; and then trains a transferable policy in the grounded environment with rewards from the source environment.

Through our experiments, we aim to answer the following questions: (1)~Can our method effectively transfer a policy across visual and dynamics differences with minimal target environment information? (2)~How does our method scale to wider domain gaps when compared to prior work?  (3)~Can the iterative training process overcome poor initial datasets?


\subsection{Experimental Setup}

To test how well our method transfers a policy with various visual and physical domain gaps, we design two benchmark target environments, one with smaller domain gaps (Target-easy) and one with larger domain gaps (Target-hard), on five simulation-to-simulation transfer tasks: classic inverted pendulum, two locomotion, and two robotic manipulation tasks. We vary both physical parameters and visual appearances for the source and target environments.  The changes are summarized in \mytbref{tab:dynamics-diff} and \myfigref{fig:tasks}.  

For the evaluation metric of a policy transfer, we use the normalized target domain performance (reward or success rate).  For all methods and tasks, we report the average target environment performance and standard deviation over 5 random seeds unless otherwise stated.

%  We also report the normalized performance based on the target domain performance of a policy trained in the source environment as the lower bound and the target domain performance of a policy trained in the target environment as the upper bound. 
% We train our method for one grounding step for the easy target environment and five grounding steps for the hard target environment.  In each grounding step, we roll out the current policy in the target environment to gather 1k samples, for a total of 5k target environment samples across all grounding steps.  We then train the observation and action transformations each for a fixed number of epochs.  In the policy training stage, we train the policy in the grounded environment for a fixed number of environment interactions.  The transfer performance is evaluated in the target environment at the end of the policy training stage for every grounding step.  


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \makebox[0.27\linewidth]{Source}
        \makebox[0.27\linewidth]{Target-easy}
        \makebox[0.27\linewidth]{Target-hard}
    \end{subfigure}
    \\
    \vspace{0.3em}
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.27\linewidth]{figures/tasks/pendulum_source.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/pendulum_target.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/pendulum_target_hard.png}
        \caption{InvertedPendulum}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.27\linewidth]{figures/tasks/halfcheetah_source.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/halfcheetah_target.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/halfcheetah_target_hard.png}
        \caption{HalfCheetah}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.27\linewidth]{figures/tasks/walker_source.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/walker_target.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/walker_target_hard.png}
        \caption{Walker2d}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.27\linewidth]{figures/tasks/fetchreach_source.png}        
        \includegraphics[width=0.27\linewidth]{figures/tasks/fetchreach_target.png}        
        \includegraphics[width=0.27\linewidth]{figures/tasks/fetchreach_target_hard.png}
        \caption{Fetch-Reach}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.27\linewidth]{figures/tasks/sawyer_source.png}        
        \includegraphics[width=0.27\linewidth]{figures/tasks/sawyer_target.png}        
        \includegraphics[width=0.27\linewidth]{figures/tasks/sawyer_target_hard.png}
        \caption{Sawyer-Push}
    \end{subfigure}
    \caption{
        Visualize source domain (left) and target domains with small (middle) and large (right) domain gaps. While the target-easy environments slightly differ in color and lighting condition, the target-hard environments include drastic changes in camera viewpoint, texture, and background. Especially in the manipulation tasks, we use the Unity3D rendering engine to make the target-hard environments look more realistic. 
    }
    \label{fig:tasks}
\end{figure}


\subsection{Method \& Baselines}

To provide baselines for policy transfer performance, we evaluate multiple representative policy transfer methods that can handle visual and dynamics domain gaps as following:
\begin{itemize}
    \item \textbf{Robust RL} trains a policy by injecting additive Gaussian noise to the action space and augmenting images with random crop and color jitter~\citep{laskin2020reinforcement}. % This method does not require a modifiable simulator and can be applied easily on top of any RL method.  
    % For RL training, we use asymmetric SAC~\citep{pinto2017asymmetric}.

    \item \textbf{Domain Randomization (DR)}~\citep{tobin2017domain, peng2018sim} learns robust policies by randomizing the source environment's visual and dynamics parameters, including color, texture, lighting, viewpoint, friction, and armature. In order to provide a fair comparison, we define the randomization ranges to include both source and target environments (\textbf{DR-Wide}). For the cases where the DR policy is unable to learn to cover the full range, we also include a policy trained with a smaller randomization range (\textbf{DR-Narrow}). % The implementation details about domain randomization can be found in appendix, \mysecref{sec:dr_implementation}.

    \item \textbf{Learning Cross-Domain Correspondence (CC)}~\citep{zhang2021learning} utilizes a learned state-to-visual observation mapping and an action mapping to transfer a policy across input modalities and dynamics domains. We train CC with a dataset of 50k images in either domain, following the original experimental setup in ~\citet{zhang2021learning}.  Note that CC does not transfer a visual policy in the source domain but transfers a state-based policy to obtain a visual policy in the target environment.
    
    \item \textbf{Our Method} takes iterative grounding steps where we collect target environment data to update our grounded environment and policy, as described in \mysecref{sec:iterative_domain_alignment}. For these experiments, we start out with a dataset of 20k images from either domain and additionally collect 1k target environment samples per grounding step.  We execute 5 grounding steps for target-hard environments and 1 grounding step for target-easy environments.  
    
\end{itemize}



\begin{table}[t]
\centering
\begin{tabular}{ ccccc } 
 \toprule
 \multirow{2}{7.5em}{\centering Task} & \multirow{2}{6.5em}{\centering Parameter} & \multirow{2}{3em}{\centering Source} & \multicolumn{2}{c}{Target} \\ 
 & & & Easy & Hard \\
 \midrule
 InvertedPendulum & Pendulum mass & 4.895 & 50 & 200 \\ 
 \midrule
 HalfCheetah & Armature & 0.1 & 0.18 & 0.4 \\ 
 \midrule
 Walker2d & Torso mass & 3.534 & 5.4 & 10.0 \\ 
 \midrule
 \multirow{2}{7.5em}{\centering Fetch-Reach} & Action Rotation & 0 & 30 & 45$^{\circ}$\\
 & Action Bias & 0 & 0 & -0.5 \\
 \midrule
 Sawyer-Push & Puck mass & 0.01 & 0.03 & 0.05 \\
 \bottomrule
\end{tabular}
\vspace{1em}
\caption{Dynamics differences between source and the two target domains.  We chose different physical parameters to vary for each environment that would affect transfer performance.}
\label{tab:dynamics-diff}
\end{table}


\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.75\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/all_comparisons_legend.png}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/all_comparisons_InvertedPendulum.png}
        \caption{InvertedPendulum}
        \label{fig:result_step:invertedpendulum}
    \end{subfigure}
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/all_comparisons_HalfCheetah.png}
        \caption{HalfCheetah}
        \label{fig:result_step:halfcheetah}
    \end{subfigure}
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/all_comparisons_Walker2d.png}
        \caption{Walker2d}
        \label{fig:result_step:walker2d}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/all_comparisons_Fetch-Reach.png}
        \caption{Fetch-Reach}
        \label{fig:result_step:fetchreach}
    \end{subfigure}
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/all_comparisons_Sawyer-Push.png}
        \caption{Sawyer-Push}
        \label{fig:result_step:sawyer_push}
    \end{subfigure}
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/visual_ablation/visual_ablation_Walker2d.png}
        \caption{Walker2d (Ablation)}
        \label{fig:ablation}
    \end{subfigure}
    \caption{
    Performance of policies when evaluated in the target-hard domain.  (a-e) We report success rates for manipulation tasks and reward for other tasks. The baseline methods do not take online target environment interactions so we report their final evaluation performance after training.  (f) Ablation of our action transformation (over 3 random seeds). We evaluate the mean performance over 100 episodes for all methods. The results with normalized rewards can be found in appendix, \myfigref{fig:result_step_norm}.
    }
    \label{fig:result_step}
\label{fig:exps}
\end{figure*}


\subsection{Inverted Pendulum}

We first examine the InvertedPendulum task, which is a classic continuous control task~\citep{brockman2016openai}. We create the target environments by changing the pendulum mass,  color, and background color. For the target-hard environment, we make these changes more significant and tilt the camera angle and the background texture. We collect trajectories of 20k images by taking random actions for the task-agnostic dataset.

\myfigref{fig:result_all:invertedpendulum} shows that our method outperforms baseline methods in transferring to both target-easy and target-hard domains. Moreover, our method shows a high target data efficiency as it achieves nearly maximum reward starting from the first grounding step, as shown in \myfigref{fig:result_step:invertedpendulum}. 

We can also observe that DR-Narrow and DR-Wide achieve 60\% and 40\% success rates on the target-easy domain but fail to generalize to the target-hard domain.  Our results show that with the smaller randomization range, DR-Narrow learns the task well but fails to generalize beyond the training domain distribution. In contrast, DR-Wide is difficult to train as the resulting policy must learn to cover a large range of visual and physical domains; but once trained, it can perform better in the target environment with a large domain gap.

Note that the domain gaps both for the target-easy and target-hard environments are large enough that the Robust RL baseline performs as poorly as direct transfer of a policy trained in the source domain. With the Robust RL baseline, we can verify the difficulty of our experimental setup across all tasks.


\subsection{Locomotion}

HalfCheetah and Walker2d are two representative locomotion tasks~\citep{brockman2016openai}. For dynamics domain shifts, we vary the armature~\citep{zhang2021learning} and torso mass~\citep{desai2020imitation} for HalfCheetah and Walker2d, respectively. For the visual domain shifts of the target-easy environment, we make small changes in the agent and background color. For the target-hard environment, we create a domain gap in the visual style of DeepMind Control Suite~\citep{tassa2018deepmind} and change the camera viewpoint.

To study the effect of different datasets for initial visual transformation training for our method and correspondence training for CC, we collect two task-agnostic datasets, one using random actions and one with a policy trained to walk backwards. In the following results, we first compare absolute transfer performance by reporting the best performance out of the two datasets.  We then do an analysis on the effect of dataset quality in \mysecref{sec:dataset_analysis}.


% Similarly, the stable agent pose can make randomly collected data include diverse and meaningful behaviors, where CC can learn correspondences without need for additional data.

In the target-hard environment of HalfCheetah, DR-Wide performs better than our method (see \myfigref{fig:result_step:halfcheetah}) and CC achieves comparable results with ours.  We hypothesize that the stable agent pose and relative ease of the task can allow a policy to find conservative actions that work with a large range of dynamics. On the other hand, learning explicit domain correspondences can still have some errors, resulting in poorer policy performance.  CC also benefits from the stable agent pose because even task agnostic data can include diverse and meaningful behaviors for the task at hand.  Therefore, CC can learn the necessary correspondences for policy transfer without the need for additional data.

The Walker2D character is an example of a locomotion problem that is less stable and more prone to falling.  Here, we demonstrate the relative advantage of our method for more difficult control tasks where domain differences are more crippling.  Our method is able to achieve up to 75\% of the upper bound value \myfigref{fig:result_step_norm:walker2d} while the baselines cannot improve much beyond direct transfer.  In fact, the strongest baseline here is DR-Narrow, but it does not generalize well to the target-hard task outside of its training distribution.  In contrast, DR-Wide fails to learn a good policy for any domain because the randomization range required to fit the target environment is too challenging for this type of control task.

% Because our method learns an image to image translation, decoupled from the action transformation,


\begin{figure*}[t]
    \centering
    %\begin{subfigure}[t]{0.55\linewidth}
    \begin{subfigure}[t]{0.27\linewidth}    
        \includegraphics[width=\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_legend.png}
    \end{subfigure}
    \\
    % \begin{subfigure}[t]{0.49\linewidth}
    \begin{subfigure}[t]{0.19\linewidth}
        \includegraphics[width=\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_InvertedPendulum.png}
        \caption{InvertedPendulum}
        \label{fig:result_all:invertedpendulum}
    \end{subfigure}
    \begin{subfigure}[t]{0.19\linewidth}
    %\begin{subfigure}[t]{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_HalfCheetah.png}
        \caption{HalfCheetah}
        \label{fig:result_all:halfcheetah}
    \end{subfigure}
    %\\
    \begin{subfigure}[t]{0.19\linewidth}
    % \begin{subfigure}[t]{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_Walker2d.png}
        \caption{Walker2D}
        \label{fig:result_all:walker2d}
    \end{subfigure}
    \begin{subfigure}[t]{0.19\linewidth}
    % \begin{subfigure}[t]{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_Fetch-Reach.png}
        \caption{Fetch-Reach}
        \label{fig:result_all:fetch_reach}
    \end{subfigure}
    %\\
    \begin{subfigure}[t]{0.19\linewidth}
    % \begin{subfigure}[t]{0.49\linewidth}
        \includegraphics[width=\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_Sawyer-Push.png}
        \caption{Sawyer-Push}
        \label{fig:result_all:sawyer_push}
    \end{subfigure}
    \caption{
        Performance of policies transferred to the target domains with different sized domain gaps. 
    }
\label{fig:domain_exps}
\end{figure*}


\subsection{Manipulation}

We evaluate our method in two robotic manipulation tasks with the 7-DoF Fetch robot: Fetch-Reach~\citep{plappert2018robotics} and the 7-DoF Rethink Sawyer robot: Sawyer-Push. The robot must move its end effector to a goal position in Fetch-Reach or push a puck to a target position for Sawyer-Push. For the target-easy domain, we simply change colors, lighting conditions, and viewpoint (Fetch-Reach only); but in the target-hard domain, we emulate a realistic visual gap by generating more realistic background and changing viewpoint using Unity3D.  The dynamics differences for Sawyer-Push come from puck mass and friction. In Fetch-Reach we bias and rotate the input actions to model calibration error.  For the initial target domain dataset, we collect 20k images from either domain with random end-effector control.

In Fetch-Reach, our method achieves greater than 90\% success rate in both domain gaps (see \myfigref{fig:result_all:fetch_reach}), demonstrating a robustness to a wide visual gaps.  The visual transformation can handle drastic changes in viewpoint and background, as well as the more realistic rendering.  In addition, we attain a good success rate in just one grounding step \myfigref{fig:result_step:fetchreach}, showing the effectiveness of our grounded environment with only a few target environment interactions.

In Sawyer-Push, our method achieves a 75\% success rate in five grounding steps while all other methods fail to reach the goal in the target-hard environment. This task is more difficult due to the wide visual domain gap as well as the varying mass of the object which can be hard to adapt to based on task-agnostic data.  The increasing learning curve in \myfigref{fig:result_step:sawyer_push} shows the benefit of iterative domain alignment: our method shows low performance for the first three grounding steps but as the visual and action transformations provide better grounding of the source environment, the policy successfully transfers to the target environment.  Moreover, in the initial dataset, the robot is very unlikely to have moved the puck, making it difficult for CC to model the puck dynamics and therefore fails to transfer a policy.  For our method, the initial visual transformation also suffers and fails to translate a moving puck's position correctly but is improved over multiple iterations as the transformation is finetuned over a better data distribution.  

% Sawyer-Push, adapted from \citet{yuke2020robosuite}, a 7-DoF Sawyer robot must learn how to push a cylindrical puck to a marked goal location.  The robot is actuated by end effector position control constrained to the plane of the tabletop.  To emulate a realistic visual gap, we use Unity3d rendering to generate the target domain images with more realistic background and lighting as well as viewpoint changes.  The dynamics differences are puck mass and friction. We collect the initial dataset of 20k images from either domain with random end effector control.

% Sawyer-Slide is adapted from \citet{valassakis2020crossing}.  In this task, the Sawyer robot operates in joint-velocity space and the end effector is attached to a cardboard plate.  The objective is to slide a rectangular puck on the plate to a goal location by tilting the plate.  Similar to the Sawyer-Push task, we use varying background, lighting, viewpoint, puck mass and friction, and gather 20k images using random end effector control.

% Fetch-Reach~\citep{plappert2018robotics} asks a 7-DoF Fetch robotics arm to move its end-effector to the target position.



\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.7\linewidth}        
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/dataset_comparisons_legend.png}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/dataset_comparisons_HalfCheetah.png}
        \caption{HalfCheetah}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/dataset_comparisons_Walker2d.png}
        \caption{Walker2d}
    \end{subfigure}
    \caption{
        Performance of policies transferred to the target domain using initial datasets of different qualities.  ``Random" is collected by a random agent, ``Backwards" is collected by an agent trained to move in the backwards direction, and ``Expert" is collected by an expert agent on the current task.
    }
\label{fig:dataset_exps}
\end{figure}


\subsection{Effect of Initial Dataset Quality}
\label{sec:dataset_analysis}

One critical factor for domain adaptation is the quality of the data, specifically the relevance to the current task.  If the dataset does not align with the state distribution or behaviors of the current task, the learned correspondences may not generalize, and therefore fail to transfer a policy for the task.  We analyze the effect of dataset quality on the HalfCheetah and Walker2d tasks for our method and CC.  We use three different datasets, collected by a random policy (Random), a policy trained to move backwards (Backwards), and a policy trained on the current task (Expert).  Here, the Random and Backwards datasets do not align well with the task.  In all experiments, we use datasets of 20k images in both domains and 5 grounding steps and CC uses 50k images in both domains following its original experimental setup~\citep{zhang2021learning}.  We report the final transfer performance in terms of reward in \myfigref{fig:dataset_exps}.  With the exception of HalfCheetah Random data, our method consistently achieves better transfer than CC.  With our iterative training procedure we can obtain better task-aligned data through training, which mitigates the effects of a poor initial dataset.  In Walker2d, our method actually performs better with the Backwards dataset rather than the perfectly aligned Expert data.  The expert policy used for the experiment has little variability in its behavior, resulting in an image translation that does not generalize well to an imperfect agent.  Meanwhile, the backwards policy experienced a wide range of poses resulting in a more robust image translation for training that can be later improved.  While alignment of both datasets with the current task and the breadth of its distribution are key factors that affect the quality of a learned transformation, through iterative grounding, our method is able to overcome this issue.


\subsection{Ablation on Action Transformation}

To verify the importance of closing dynamics domain gaps, we compare the performance of our method with and without the action transformation.  In the ablated model, we only train the visual transformation during the grounding step and apply actions directly in each environment.  \myfigref{fig:ablation} shows that even without the action transformation, our method can achieve 0.5 normalized reward, whereas all baseline methods get 0. This shows the difficulty in generalizing to the visual domain of the target-hard environment and how well our method can bridge the visual domain gap using the visual transformation.
On the other hand, this ablated model demonstrates a large drop in performance compared to our full model, which quantifies the performance gains by correcting the dynamics gap. Furthermore, with the dynamics domain gap, the distribution of the target domain data can be far from the source domain data.  When this happens, the quality of the translation will not improve and can lead to worse transfer performance when dynamics differences are unaccounted for.  Thus, it is vital that our method addresses both visual and dynamics domain gaps for the full benefits of iterative grounding.
