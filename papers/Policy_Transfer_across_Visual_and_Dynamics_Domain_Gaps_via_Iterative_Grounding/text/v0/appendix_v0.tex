\appendix
\section{Appendix}


\subsection{Quality of Visual Transformations}

To evaluate the visual transformation quality, we show pairs of images from target and source domains that share the \textit{same underlying state}, and compare the difference between images generated by the visual transformation with source domain images and true target domain images, in \myfigref{fig:gen_imgs}. Before grounding, the puck disappears in the later frames because the Sawyer rarely moves the puck in the pretraining dataset.  After grounding, our visual transformation learns to translate the puck position correctly. 


\begin{figure}[hb]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \makebox[0.2\linewidth]{Source}
        \makebox[0.2\linewidth]{Translated}
        \makebox[0.2\linewidth]{Target}
        \makebox[0.2\linewidth]{Diff}
    \end{subfigure}
    \\
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/images/sawyer_beforegrounding.png}
        \caption{Before grounding step}
    \end{subfigure} 
    \\
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/images/sawyer_aftergrounding.png}
        \caption{After grounding step}
    \end{subfigure}
    \caption{
        Translated images across visual domains for the Sawyer-Push task using the visual transformation after pretraining (a) and after finetuning with one grounding step (b).  
        Top row of each section is a series of source environment images, bottom row is the corresponding translated target environment image.  
    }
    \label{fig:gen_imgs}
\end{figure}


\subsection{Results on Normalized Reward}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_legend.png}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_InvertedPendulum.png}
        \caption{InvertedPendulum}
        \label{fig:result_step_norm:invertedpendulum}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_HalfCheetah.png}
        \caption{HalfCheetah}
        \label{fig:result_step_norm:halfcheetah}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_Walker2d.png}
        \caption{Walker2d}
        \label{fig:result_step_norm:walker2d}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_Fetch-Reach.png}
        \caption{Fetch-Reach}
        \label{fig:result_step_norm:fetchreach}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_Sawyer-Push.png}
        \caption{Sawyer-Push}
        \label{fig:result_step_norm:sawyer_push}
    \end{subfigure}
    \caption{
        Comparisons of all methods on the target-hard task with normalized rewards.  The lower bound is the performance of a policy trained in the source environment, then evaluated in the target.  The upper bound is a policy training directly in the target environment.
    }
\label{fig:result_step_norm}
\end{figure}



\subsection{Environment Details}

\subsubsection{Locomotion Environments}
The locomotion environments are modified from OpenAI Gym~\citep{brockman2016openai} and use the same default action and state space. We created the target-easy visual domains by varying lighting and background color.  For the target-hard visual domain, we additionally varied background texture, character color, and viewpoint.

\subsubsection{Manipulation Environments}
Our manipulation environments included Fetch-Reach which is modified form OpenAI Gym, and Sawyer-Push which uses a simulation of he 7-DoF Rethink Sawyer. To create the dynamics of Fetch-Reach target environment we rotate the action vector around z-axis and add a bias to the third coordinate of the action vector. For Sawyer, we vary the friction and mass to create target environment dynamics. For visual of the target domain, we change colors,  lighting, and viewpoint in easy target environment, and we use Unity3D rendering with realistic lighting and background in the hard target environment.


\subsection{Baseline Implementations}

\subsubsection{Robust RL Implementation}
To train the robust RL baseline, we add an additive Gaussian noise to the action space and augmenting images with random crop and color jitter~\citep{laskin2020reinforcement}. 
For policy optimization, we use asymmetric SAC~\citep{pinto2017asymmetric} and use the same hyperparameters our method use~\mytbref{tab:sac_hyperparameter}.

\subsubsection{Domain Randomization Implementation}
\label{sec:dr_implementation}
We implemented domain randomization by modifying simulation parameters every iteration with uniformly sampled random values The sampling range for dynamics parameters are specified in Table. \ref{tab:dr_dynamics}. Examples images of visually randomized environments we used during training can be found in \myfigref{fig:dr_visual_wide} and \myfigref{fig:dr_visual_narrow}. 

\begin{table}[ht]
\centering
\caption{Physics parameters for domain randomization.}
\begin{tabular}{ ccccc } 
 \toprule
 \multirow{2}{7.5em}{\centering Task} & \multirow{2}{6.5em}{\centering Parameter} & \multicolumn{2}{c}{Target} \\ 
 & & Easy & Hard \\
 \midrule
 InvertedPendulum & Pendulum mass & $4 \sim 55$ & $4 \sim 220$ \\ 
 \midrule
 HalfCheetah & Armature & $0.08 \sim 0.25$ & $0.08 \sim 0.44$ \\ 
 \midrule
 Walker2d & Torso mass & $3 \sim 6$ & $3 \sim 11$ \\ 
 \midrule
 \multirow{2}{7.5em}{\centering Fetch-Reach} & Action Rotation & $-30^{\circ}\sim30^{\circ}$ & $-45^{\circ}\sim45^{\circ}$\\
 & Action Bias & $-0.55 \sim 0$ & $-0.55 \sim 0.55$ \\
 \midrule
\multirow{2}{7.5em}{\centering Sawyer-Push} & Puck mass & $0.01 \sim 0.033$ & $0.01 \sim 0.05$ \\ 
& Puck Friction & $ 2 \sim 3.3$ & $2 \sim 4.4$\\
 \bottomrule
\end{tabular}
\label{tab:dr_dynamics}
\end{table}


\subsubsection{Cross-Domain Correspondence Implementation}
\label{sec:cc_implementation}
We use the implementation provided by the original authors~\citep{zhang2021learning}.



\subsection{Implementation Details}

\subsubsection{\textbf{Policy Training}}
We use Asymmetric SAC (cite) to learn an RL policy.  The input to the actor is a stack of 3 consecutive image frames, originally 100x100 pixels and randomly cropped to 92x92.  The input to the critic is the state.  The actor network consists of a 4-layer CNN encoder with output feature space of dimension 50 and a 2-layer MLP with hidden dimensions of 1024, whose output parameterizes a Gaussian distribution over the action space.  For InvertedPendulum, Sawyer-Push, and Fetch-Reach, we train for 1e4 steps per policy training stage, which takes approximately 20 minutes to train on an  NVIDIA Titan X GPU. For HalfCheetah and Walker2D, we train for 2e5 steps, which takes approximately 3 hours.

\begin{table}[ht]
    \caption{SAC hyperparameters.}
    \label{tab:sac_hyperparameter}
    \centering
    \begin{tabular}{lc}
        \toprule
        Hyperparameter & Value \\
        \midrule
        Learning Rate & 0.0003 \\
        Learning Rate Decay & Linear decay \\
        Batch Size & 32 \\
        \# Epochs per Update & 10 \\
        Discount Factor & 0.99 \\
        Entropy Coefficient & 0.001 \\
        Reward Scale & 1  \\
        Normalization & False \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{\textbf{Visual Transformation: CycleGAN with Regularization}}
We base our CycleGAN implementation on (cite cycleGAN) and use the same hyperparameters and architectures.  Additionally, each state prediction network consists of a 4-layer CNN encoder with output dimension of 50 and a 2-layer MLP with hidden dimensions of size 256.  To train the source domain state prediction network, we use the Adam optimizer (cite) with learning rate 3e-4.  We initialize the target domain state prediction network with the weights of the source domain network and train only the top convolutional layer jointly with the CycleGAN generator networks.  During pretraining, we first train the source domain state prediction network for 40 epochs, then train the CycleGAN + target domain state prediction network for 40 epochs.  During finetuning, we train each network group for 5 epochs. Training time for the initial training is approximately 8 hours and for finetuning is approximately 20 minutes.

\subsubsection{\textbf{Action Transformation: Visual GARAT}}
We use GAIfO adversarial training to optimize the action transformation using a PPO agent with parameters listed in Table. \ref{tab:ppo_hyperparameter}.  The observation space of the agent and discriminator is the concatenation of the policy feature space, $f(o_t)$ (dim = 50) and action space of the environment.  The discriminator learns to differentiate $(f(o_t),a,f(o_{t+1}))$ tuples.  The agent and discriminator are both 2-layer MLPs with hidden dimensions of 1024.  Following \citet{desai2020imitation}, we add the output of the agent to the original action and use action smoothing, proposed in \citet{hanna2017grounded} with smoothing parameter 0.95, to get the transformed action.  Every grounding step, we train the action transformation for 10 epochs. Training time for the action transformation training in each grounding step is approximately 30 minutes.

\begin{table}[ht]
    \caption{PPO hyperparameters.}
    \label{tab:ppo_hyperparameter}
    \centering
    \begin{tabular}{lc}
        \toprule
        Hyperparameter & Value \\
        \midrule
        Rollout Size & 5000 \\
        Learning Rate & 0.0003 \\
        Learning Rate Decay & Linear decay \\
        Batch Size & 32 \\
        \# Epochs per Update & 5 \\
        Discount Factor & 0.5 \\
        Entropy Coefficient & 0.001 \\
        Clipping Ratio & 0.1  \\
        Normalization & False  \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_min.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_min1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_min2.png}
        \caption{InvertedPendulum}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_min.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_min0.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_min2.png}
        \caption{HalfCheetah}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_min.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_min1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_min2.png}
        \caption{Walker2d}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_min0.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_min1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_min2.png}
        \caption{Fetch-Reach}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_min.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_min1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_min2.png}
        \caption{Sawyer-Push}
    \end{subfigure}
    \caption{
        Narrow range visualize domain randomization examples, including color and lighting changes.
    }
    \label{fig:dr_visual_narrow}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_max.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_max1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_max2.png}
        \caption{InvertedPendulum}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_max.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_max1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_max2.png}
        \caption{HalfCheetah}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_max.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_max1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_max2.png}
        \caption{Walker2d}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_max0.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_max1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_max2.png}
        \caption{Fetch-Reach}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_max.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_max1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_max2.png}
        \caption{Sawyer-Push}
    \end{subfigure}
    \caption{
        Wide range visualize domain randomization examples, including viewpoint changes and more textures in addition to color and lighting changes.
    }
    \label{fig:dr_visual_wide}
\end{figure}