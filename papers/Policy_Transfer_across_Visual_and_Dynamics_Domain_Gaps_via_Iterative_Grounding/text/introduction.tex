\section{Introduction}

Deep reinforcement learning (RL) presents a promising framework for learning impressive robot behaviors~\citep{levine2016end, suarez2016framework, rajeswaran2018learning, jain2019learning, lee2021ikea}. Yet, performing RL directly on physical robots in a home or an office is impractical due to the lack of training supervision (e.g. reward function, state information) and the high cost of data collection, which holds true in most robot learning scenarios. 
One practical solution is \textit{policy transfer}, which first trains a policy under a more controllable environment (often a simulator) with cheaper data collection and easier access to reward and state information, and then deploys this well-trained policy to the target environment.  
Thus, our goal is to develop a method that can efficiently transfer a policy trained in a source environment to a target environment with minimal assumptions (i.e. no state and reward information), as illustrated in \myfigref{fig:teaser}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/teaser.pdf}
    \caption{
        We aim to transfer a policy trained in the source environment (top) which has full access to observation, state, and reward, to the target domain (bottom) with no access to state and reward. However, transferring a policy from one environment to another is challenging due to visual differences (e.g. lighting, viewpoint, clutter, background) and physical differences (e.g. friction, mass, robot calibration).      
    }
    \label{fig:teaser}
\end{figure}


However, RL policies trained in one environment tend to perform poorly in the other due to the \textit{visual domain gap} (e.g. lighting, viewpoint, clutter, and background) and \textit{dynamics domain gap} induced by physical differences (e.g. the mass of objects and calibration of the robot)~\citep{jakobi1995noise, tobin2017domain, peng2018sim}.
Hence, a variety of approaches propose to train a policy robust to domain gaps through noise injection~\citep{jakobi1995noise}, adversarial training~\citep{ganin2016domain, pinto2017robust, rajeswaran2017epopt}, learning domain-invariant representations~\citep{gupta2017learning}, or domain randomization~\citep{tobin2017domain, james2019sim, peng2018sim}. 
Yet, these approaches only transfer well within the training domain distribution, and training such robust policies becomes infeasible for large domain ranges. 
Instead, grounding-based policy transfer methods directly minimize the visual~\citep{rao2020rl, bousmalis2018using} and dynamics~\citep{hanna2017grounded, desai2020imitation} domain gap by grounding the source environment in the target environment.
Instead, domain adaptation methods learn explicit observation mappings~\citep{rao2020rl, bousmalis2018using} or action mappings~\citep{hanna2017grounded, desai2020imitation} to close the visual and dynamics domain gap respectively.  The mappings are then used to modify the domain of the policy or source environment to match that of the target environment.

These methods are designed to address only one type of domain gap at a time; however closing both visual and dynamics domain gaps is crucial for successful policy transfer as most robot learning scenarios include both. 

We propose \fullmethod~(\method) for policy transfer by modifying, or ``grounding'', the source environment to minimize both visual \textit{and} dynamics domain gaps to the target environment with minimal task information.
Concretely, we develop a novel policy transfer method via iterative environment grounding that alternates between (1) grounding the source environment in the target environment by learning both visual and dynamics transformations; and (2) training a policy on the grounded source environment, which has a smaller domain gap to the target environment while  still providing rich task supervision (e.g. reward and state information) from the source environment. 
Our iterative training gradually improves the grounded environment and the policy, leading to successful transfer to the target environment. For source environment grounding, we learn the visual and dynamics correspondences between the two environments from unpaired data from both domains via unsupervised correspondence learning algorithms. 
Note that our setup is robot learning friendly, as our method does not require any instrumentation for acquiring reward and state information in the target environment.

Our contributions are threefold: (1) we propose a grounded environment model that can handle both visual and dynamics domain gaps, (2) we propose \method, a novel policy transfer method with iterative environment grounding that gradually improves the grounded environment and policy, and (3) we develop a benchmark of locomotion and robotic manipulation tasks for policy transfer across both visual and dynamics domain gaps. On this benchmark, we demonstrate that our method can effectively transfer a policy to target environments with large visual and dynamics domain gaps where previous policy transfer approaches fail. 