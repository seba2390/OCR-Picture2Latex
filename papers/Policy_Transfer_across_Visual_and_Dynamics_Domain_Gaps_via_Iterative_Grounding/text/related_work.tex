\section{Related Work}

Efficient policy transfer between two different domains is a promising research direction for robot learning with applications in simulation-to-real and real-to-real transfer. However, the existence of visual and dynamics domain gaps between environments makes policy transfer challenging. The most naive approach is to finetune the policy in the target environment~\citep{rusu2017sim, julian2020finetune}. But, this is often not feasible if the reward function is not available at deployment and it requires a lot of expensive real-world interactions.

Instead of finetuning, \textbf{domain randomization} approaches randomize parameters of the source environment during training, resulting in a policy robust to a wide range of domains. 
Although domain randomization demonstrates promising results in manipulation~\citep{tobin2017domain, james2017transferring, peng2018sim} and locomotion~\citep{peng2020learning}, this approach is hardly applicable to real-to-real transfer scenarios, where the environment is not easily modifiable. It has also shown limited generalization as the domain gap becomes larger or if the target environment is outside of the training randomization range, as we show in \myfigref{fig:domain_exps}.
Similarly, a variety of approaches have been proposed to train a policy robust to domain gaps through adding random noise~\citep{jakobi1995noise}, learning domain-invariant features~\citep{gupta2017learning}, or adversarial training~\citep{ganin2016domain, pinto2017robust, rajeswaran2017epopt}. Yet, these approaches also work on a limited target domain distribution. 

Another avenue of work adapts a policy to a specific target domain by learning visual correspondences~\citep{bousmalis2018using, rao2020rl} to address the \textit{visual} gap across domains (e.g. simulation rendering to real-world image) .
Visual correspondences can be learned from unpaired data by optimizing cycle-consistency losses~\citep{zhu2017unpaired} with additional regularization, such as Q-function prediction from offline data~\citep{rao2020rl} or task success prediction~\citep{bousmalis2018using}.
However, even with the perfect visual correspondences, policy transfer can fail if the dynamics or physical properties of the target environment differ from those during training.

 To bridge the \textit{dynamics} gap between environments, prior approaches learn an action transformation to compensate the dynamics mismatch between domains based on learned dynamics models, explicitly~\citep{hanna2017grounded, desai2020stochastic, Malmir2020Robust} or implicitly~\citep{desai2020imitation, karnan2020reinforced}.
 One application of this action transformation is \textbf{environment grounding}, which grounds the source environment in the target environment, and trains a policy on the grounded environment.  The learned action transformations are used to modify the source environment such that the grounded source environment dynamics more closely match the target environment dynamics.  Then, a policy can be trained on this grounded environment, resulting in better transfer to the target environment. Yet, these methods only work when the source and target environments share the same state space, and hence cannot handle policy transfer with visual domain gaps. 

Recently, \citet{zhang2021learning} proposes to learn cross-domain correspondences for policy transfer across input modalities and physics differences, but their approach is limited to state-based policies in the source environment and is not suitable for transferring visual policies. In contrast, our method learns to close both the visual and dynamics domain gaps making policy transfer possible without the assumption of shared dynamics or the availability of a low-dimensional state representation.
