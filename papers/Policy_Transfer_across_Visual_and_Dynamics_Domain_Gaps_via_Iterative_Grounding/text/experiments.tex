\section{Experiments}

In this paper, we propose a policy transfer method with iterative grounding across visual and dynamics domains without task supervision in the target environment. IDAPT iteratively learns visual and action transformations with limited target environment data, uses these transformations to ground the source environment such that it mimics the target environment, and then trains a transferable policy in the grounded environment with rewards from the source environment.

Through our experiments, we aim to answer the following questions: (1)~Can IDAPT effectively transfer a policy across visual and dynamics differences with minimal target environment interactions? (2)~How does IDAPT scale to wider domain gaps when compared to prior work?  (3)~Can the iterative training process overcome poor initial datasets?


\subsection{Experimental Setup}

To test how well IDAPT transfers a policy with various visual and physical domain gaps, we design two benchmark target environments, one with smaller domain gaps (Target-easy) and one with larger domain gaps (Target-hard), on five simulation-to-simulation transfer tasks: classic inverted pendulum, two locomotion, and two robotic manipulation tasks. We vary both physical parameters and visual appearances for the source and target environments.  The changes are summarized in \mytbref{tab:dynamics-diff} and \myfigref{fig:tasks}.  

For the evaluation metric of a policy transfer, we use the target domain performance (reward or success rate).  For all methods and tasks, we report the average target environment performance and standard deviation over 5 random seeds unless otherwise stated.

%  We also report the normalized performance based on the target domain performance of a policy trained in the source environment as the lower bound and the target domain performance of a policy trained in the target environment as the upper bound. 


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \makebox[0.27\linewidth]{Source}
        \makebox[0.27\linewidth]{Target-easy}
        \makebox[0.27\linewidth]{Target-hard}
    \end{subfigure}
    \\
    \vspace{0.3em}
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.27\linewidth]{figures/tasks/pendulum_source.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/pendulum_target.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/pendulum_target_hard.png}
        \caption{InvertedPendulum}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.27\linewidth]{figures/tasks/halfcheetah_source.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/halfcheetah_target.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/halfcheetah_target_hard.png}
        \caption{HalfCheetah}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.27\linewidth]{figures/tasks/walker_source.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/walker_target.png}
        \includegraphics[width=0.27\linewidth]{figures/tasks/walker_target_hard.png}
        \caption{Walker2d}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.27\linewidth]{figures/tasks/fetchreach_source.png}        
        \includegraphics[width=0.27\linewidth]{figures/tasks/fetchreach_target.png}        
        \includegraphics[width=0.27\linewidth]{figures/tasks/fetchreach_target_hard.png}
        \caption{Fetch-Reach}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.27\linewidth]{figures/tasks/sawyer_source.png}        
        \includegraphics[width=0.27\linewidth]{figures/tasks/sawyer_target.png}        
        \includegraphics[width=0.27\linewidth]{figures/tasks/sawyer_target_hard.png}
        \caption{Sawyer-Push}
    \end{subfigure}
    \caption{
        Visualize source domain (left) and target domains with small (middle) and large (right) domain gaps. While the target-easy environments slightly differ in color and lighting condition, the target-hard environments include drastic changes in camera viewpoint, texture, and background. Especially in the manipulation tasks, we use the Unity3D rendering engine to make the target-hard environments look more realistic. 
    }
    \label{fig:tasks}
\end{figure}


\subsection{Baselines}

To provide baselines for policy transfer performance, we evaluate multiple representative policy transfer methods that can handle visual and dynamics domain gaps as following:
\begin{itemize}
    \item \textbf{Domain Randomization (DR)}~\citep{tobin2017domain, peng2018sim} learns robust policies by randomizing the source environment's visual and dynamics parameters, including color, texture, lighting, viewpoint, friction, and armature. In order to provide a fair comparison, we define the randomization ranges to include both source and target environments (\textbf{DR-Wide}). For the cases where the DR policy is unable to learn to cover the full range, we also include a policy trained with a smaller randomization range (\textbf{DR-Narrow}) that only includes source and target-easy domains.
    
    \item \textbf{Adaptive RL} learns a robust adaptive policy under domain randomized environments that can identify and adapt to the current domain on the fly.  This allows the policy to be more flexible and accommodate a wider training range of domains.  We implement this with an LSTM-based policy.

    \item \textbf{Learning Cross-Domain Correspondence (CC)}~\citep{zhang2021learning} utilizes a learned observation mapping and an action mapping to transfer a policy across input modalities and dynamics domains.  CC addresses difference in input modalities instead of visual domain gap so we use two different modes for an informative comparison.  \textbf{CC-State} learns a state-to-state observation mapping to transfer a state-based policy across dynamics differences.  \textbf{CC-Image} learns a state-to-image observation mapping to transfer a policy across input modalities and dynamics domains.  CC uses a dataset of 50k samples from both domains.  We additionally make CC iterative by gathering 1k target environment samples using the trained mappings between every iteration.
    
    \item \textbf{\method} (our method) takes iterative grounding and policy training steps, as described in \mysecref{sec:iterative_domain_alignment}. For these experiments, we start out with a dataset of 20k images from both domains and additionally collect 1k target environment samples per grounding step.  We execute 5 grounding steps for target-hard environments and 1 grounding step for target-easy environments.  This results in a total data usage of 25k interactions which is 50\% of that of the CC baselines.  
\end{itemize}

For further implementation details, please refer to appendix, \mysecref{sec:environment_details} for environments, \mysecref{sec:baseline_implementation} for baseline implementations, \mysecref{sec:idapt_details} for our method. 



\begin{table}[t]
\centering
\begin{tabular}{ ccccc } 
 \toprule
 \multirow{2}{7.5em}{\centering Task} & \multirow{2}{6.5em}{\centering Parameter} & \multirow{2}{3em}{\centering Source} & \multicolumn{2}{c}{Target} \\ 
 & & & Easy & Hard \\
 \midrule
 InvertedPendulum & Pendulum mass & 4.895 & 50 & 200 \\ 
 \midrule
 HalfCheetah & Armature & 0.1 & 0.18 & 0.4 \\ 
 \midrule
 Walker2d & Torso mass & 3.534 & 5.4 & 10.0 \\ 
 \midrule
 \multirow{2}{7.5em}{\centering Fetch-Reach} & Action Rotation & 0 & 30 & 45$^{\circ}$\\
 & Action Bias & 0 & 0 & -0.5 \\
 \midrule
 Sawyer-Push & Puck mass & 0.01 & 0.03 & 0.05 \\
 \bottomrule
\end{tabular}
\vspace{1em}
\caption{Dynamics differences between source and the two target domains.  We chose different physical parameters to vary for each environment that would affect transfer performance.}
\label{tab:dynamics-diff}
\end{table}


\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/all_comparisons_InvertedPendulum.png}
        \caption{InvertedPendulum}
        \label{fig:result_step:invertedpendulum}
    \end{subfigure}
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/all_comparisons_HalfCheetah.png}
        \caption{HalfCheetah}
        \label{fig:result_step:halfcheetah}
    \end{subfigure}
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/all_comparisons_Walker2d.png}
        \caption{Walker2d}
        \label{fig:result_step:walker2d}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/all_comparisons_Fetch-Reach.png}
        \caption{Fetch-Reach}
        \label{fig:result_step:fetchreach}
    \end{subfigure}
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/all_comparisons_Sawyer-Push.png}
        \caption{Sawyer-Push}
        \label{fig:result_step:sawyer_push}
    \end{subfigure}
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \includegraphics[width=0.7\linewidth]{figures/results/all_comparisons/all_comparisons_vertical_legend.png}
    \end{subfigure}
    \caption{
    Performance of policies when evaluated in the target-hard domain.  We report success rates for manipulation tasks and reward for other tasks. For CC and \method, we report performance after each training iteration.  The remaining methods do not take online target environment interactions so we report their final evaluation performance after training.  For IDAPT, we use a backwards walking dataset for (b), (c) and randomly collected datasets for all other tasks. IDAPT (Visual Only) ablates our action transformation (over 3 random seeds). We evaluate the mean performance over 100 episodes for all methods. The results with normalized rewards can be found in appendix, \myfigref{fig:result_step_norm}.
    }
    \label{fig:result_step}
\end{figure*}


\subsection{Inverted Pendulum}

We first examine the InvertedPendulum task, which is a classic continuous control task~\citep{brockman2016openai}. We create the target environments by changing the pendulum mass,  color, and background color. For the target-hard environment, we make these changes more significant, tilt the camera angle, and change the background texture. We collect trajectories of 20k images by taking random actions for the task-agnostic dataset.

\myfigref{fig:result_all:invertedpendulum} shows that our method outperforms baseline methods in transferring to both target-easy and target-hard domains. Moreover, IDAPT shows a high target data efficiency as it achieves nearly maximum reward starting from the first grounding step, as shown in \myfigref{fig:result_step:invertedpendulum}. 

We can also observe that DR-Narrow and DR-Wide achieve 60\% and 40\% success rates on the target-easy domain but fail to generalize to the target-hard domain.  Our results show that with the smaller randomization range, DR-Narrow learns the task well but fails to generalize beyond the training domain distribution. In contrast, DR-Wide is difficult to train as the resulting policy must learn to cover a large range of visual and physical domains; but once trained, it can perform better in the target environment with a large domain gap.


\subsection{Locomotion}

HalfCheetah and Walker2d are two representative locomotion tasks~\citep{brockman2016openai}. For dynamics domain shifts, we vary the armature~\citep{zhang2021learning} and torso mass~\citep{desai2020imitation} for HalfCheetah and Walker2d, respectively. For the visual domain shifts of the target-easy environment, we make small changes in the agent and background color. For the target-hard environment, we create a domain gap in the visual style of DeepMind Control Suite~\citep{tassa2018deepmind} and change the camera viewpoint.

To study the effect of different datasets for initial visual transformation training for IDAPT and correspondence training for CC, we collect two task-agnostic datasets, one using random actions and one with a policy trained to walk backwards. In the following results, we first compare absolute transfer performance by reporting the best performance out of the two datasets (random for CC, backwards for ours).  We then do an analysis on the effect of dataset quality in \mysecref{sec:dataset_analysis}.

In the target-hard environment of HalfCheetah, DR-Wide performs better than IDAPT (see \myfigref{fig:result_step:halfcheetah}), while Adaptive RL performs comparatively.  We hypothesize that the stable agent pose and relative ease of the task can allow a policy to find conservative actions that work with a large range of dynamics. On the other hand, learning explicit domain correspondences can still have some errors, resulting in poorer policy performance.  While CC-State also outperforms IDAPT, the performance difference to CC-Image demonstrates the added difficulty of the visual domain gap.

The Walker2D character is an example of a locomotion problem that is less stable and more prone to falling.  Here, we demonstrate the relative advantage of IDAPT for more difficult control tasks where domain differences are more crippling.  IDAPT is able to achieve up to 75\% of the upper bound value \myfigref{fig:result_step_norm:walker2d} while the baselines cannot improve much beyond direct transfer.  In fact, the strongest baseline here is DR-Narrow, but it does not generalize well to the target-hard task outside of its training distribution.  In contrast, DR-Wide fails to learn a good policy for any domain because the randomization range required to fit the target environment is too challenging for this type of control task.


\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.3\linewidth}
        \includegraphics[width=\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_InvertedPendulum.png}
        \caption{InvertedPendulum}
        \label{fig:result_all:invertedpendulum}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \includegraphics[width=\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_HalfCheetah.png}
        \caption{HalfCheetah}
        \label{fig:result_all:halfcheetah}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \includegraphics[width=\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_Walker2d.png}
        \caption{Walker2D}
        \label{fig:result_all:walker2d}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.3\linewidth}
        \includegraphics[width=\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_Fetch-Reach.png}
        \caption{Fetch-Reach}
        \label{fig:result_all:fetch_reach}
    \end{subfigure}
    \begin{subfigure}[t]{0.3\linewidth}
        \includegraphics[width=\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_Sawyer-Push.png}
        \caption{Sawyer-Push}
        \label{fig:result_all:sawyer_push}
    \end{subfigure}
    \begin{subfigure}[t]{0.29\linewidth}
        \centering
        \hspace{-5mm}
        \raisebox{95pt}{\includegraphics[width=0.6\linewidth]{figures/results/domain_comparisons/domain_gap_comparisons_legend.png}}
    \end{subfigure}
    \caption{
        Performance of policies transferred to the target domains with different domain gap sizes, target-easy and target-hard. Our proposed approach, IDAPT, outperforms the baselines on most environments and scales better to larger domain gaps.
    }
\label{fig:domain_exps}
\end{figure*}


\subsection{Manipulation}

We evaluate our method in two robotic manipulation tasks with the 7-DoF Fetch robot: Fetch-Reach~\citep{plappert2018robotics} and the 7-DoF Rethink Sawyer robot: Sawyer-Push. The robot must move its end effector to a goal position in Fetch-Reach or push a puck to a target position for Sawyer-Push. For the target-easy domain, we simply change colors, lighting conditions, and viewpoint (Fetch-Reach only); but in the target-hard domain, we emulate a realistic visual gap by generating more realistic backgrounds using Unity3D and changing viewpoint.  The dynamics differences for Sawyer-Push come from puck mass. In Fetch-Reach we bias and rotate the input actions to model calibration error.  For the initial target domain dataset, we collect 20k images from either domain with random end-effector control.

In Fetch-Reach, IDAPT achieves greater than 90\% success rate in both domain gaps (see \myfigref{fig:result_all:fetch_reach}), demonstrating a robustness to a wide visual gaps.  The visual transformation can handle drastic changes in viewpoint and background, as well as the more realistic rendering.  In addition, we attain a good success rate in just one grounding step (see \myfigref{fig:result_step:fetchreach}), showing the effectiveness of our grounded environment with only a few target environment interactions.


In Sawyer-Push, IDAPT achieves a 75\% success rate in five grounding steps. This task is more difficult due to the wide visual domain gap as well as the varying mass of the object which can be hard to adapt to based on task-agnostic data.  The increasing learning curve in \myfigref{fig:result_step:sawyer_push} shows the benefit of iterative domain alignment: IDAPT shows low performance for the first three grounding steps but as the visual and action transformations provide better grounding of the source environment, the policy successfully transfers to the target environment.  Moreover, in the initial dataset, the robot is very unlikely to have moved the puck, making it difficult for CC and IDAPT to model the puck dynamics or generate the correct visual translation, therefore failing to transfer a policy.  Both iterative CC and IDAPT are improved over multiple iterations as the transformation is finetuned over a better data distribution.


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.6\linewidth}        
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/dataset_comparisons_legend.png}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/dataset_comparisons_HalfCheetah.png}
        \caption{HalfCheetah}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/dataset_comparisons_Walker2d.png}
        \caption{Walker2d}
    \end{subfigure}
    \\
    \vspace{1em}
    \begin{subfigure}[t]{\linewidth}        
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/visual_dataset_grounding_legend.png}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/visual_dataset_grounding_HalfCheetah.png}
        \caption{HalfCheetah}
        \label{fig:dataset_exps:halfcheetah}
    \end{subfigure}
        \begin{subfigure}[t]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/visual_dataset_grounding_Walker2d.png}
        \caption{Walker2d}
        \label{fig:dataset_exps:walker2d}
    \end{subfigure}
    \caption{
        Performance of policies transferred to the target domain using initial datasets of different qualities.  ``Random'' is collected by a random agent, ``Backwards" is collected by an agent walking backwards, and ``Expert" is collected by an expert agent on the current task. We compare performances of our method and CC-State (top) and performances of our method over grounding steps (bottom) on different datasets.
    }
    \label{fig:dataset_exps}
\end{figure}


\subsection{Effect of Initial Dataset Quality}
\label{sec:dataset_analysis}

One critical factor for successful domain adaption is the quality of the data used to train transformations or correspondences, specifically its relevance to the current task.  If the dataset does not align with the state distribution or behaviors of the current task, the learned correspondences may not generalize, and therefore fail to transfer a policy for the task.  We analyze the effect of dataset quality on the HalfCheetah and Walker2d tasks for IDAPT and unmodified CC-State (without iterative training).  We use three different datasets, collected by a random policy (Random), a policy trained to move backwards (Backwards), and a policy trained on the current task (Expert).  Here, the Random and Backwards datasets do not align well with the task.  In all experiments, we use datasets of 20k images in both domains and 5 grounding steps and CC-State uses 50k images in both domains following its original experimental setup~\citep{zhang2021learning}.  We report the final transfer performance in terms of reward in \myfigref{fig:dataset_exps}. 

With the exception of HalfCheetah Random data, IDAPT consistently achieves better transfer than CC-State.  With our iterative training procedure we can obtain better task-aligned data through training, which mitigates the effects of a poor initial dataset.  In Walker2d, IDAPT actually performs better with the Backwards dataset rather than the perfectly aligned Expert data.  We hypothesize that this is due to the expert policy in this task having little variability in its behavior, resulting in an image translation that does not generalize well to an imperfect agent, making policy training harder.  Meanwhile, the backwards policy generated a wide range of poses resulting in a more robust image translation for training that can be later improved. While alignment of both datasets with the current task and the breadth of its distribution are key factors that affect the quality of a learned transformation, through iterative grounding, IDAPT is able to partially overcome this issue.


We further examine the use of mixed datasets, ``Random+Expert" and ``Random+Backwards+Expert", consisting of trajectories gathered from multiple different policies.  In \myfigref{fig:dataset_exps:halfcheetah} and \myfigref{fig:dataset_exps:walker2d}, it is clear that the choice of dataset impacts transfer performance. Notably, the Random dataset performs worst and is unable to improve over multiple grounding steps.  Meanwhile, the ``Random+Backwards+Expert'' dataset generally performs well, suggesting that in practice, a mixture dataset of many different behaviors will likely perform well even if some of those behaviors on their own would not result in good transformations.  


\subsection{Ablation on Action Transformation}

To verify the importance of closing dynamics domain gaps, we compare the performance of IDAPT with and without the action transformation.  In the ablated model, we only train the visual transformation during the grounding step and apply actions directly in each environment.  \myfigref{fig:result_step} shows a large drop in performance compared to our full model, which quantifies the performance gains by correcting the dynamics gap. Furthermore, with the dynamics domain gap, the distribution of the online target environment data can be far from the source environment data.  When this happens, the quality of the translation will not improve and can lead to worse transfer performance when dynamics differences are unaccounted for.  Thus, it is vital that IDAPT addresses both visual and dynamics domain gaps for the full benefits of iterative grounding.


\subsection{State Reconstruction Regularization for Visual Transformation Training}

We ablate the state reconstruction regularization in our visual transformation model.  We train IDAPT with and without the state reconstruction regularization and compare performance on the target environment with only the visual domain gap during the initial policy training phase. The results demonstrate that in general, our model with state reconstruction regularization achieves better transfer performance (see appendix, \myfigref{fig:norecon_ablation}).

We further perform the same ablation on target-hard environments over multiple grounding steps. The results in appendix, \myfigref{fig:norecon_grounding_ablation} show that the state reconstruction regularization improves performance significantly over all environments.  Qualitatively, we observe that the regularization helps minimize artifacts and stabilize CycleGAN training, resulting in better visual transformations.
