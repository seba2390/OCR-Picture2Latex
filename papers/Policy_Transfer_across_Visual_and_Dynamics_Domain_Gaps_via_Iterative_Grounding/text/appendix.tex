\appendix
\section{Appendix}

\subsection{Results on Normalized Reward}

In addition to \myfigref{fig:result_step}, we report the normalized results for the experiments in \myfigref{fig:result_step_norm}.  For each task, we normalize the reward or success rate between the target environment performance of a policy trained in the source environment (lower bound) and target environment (upper bound).  In every task except HalfCheetah, IDAPT results in more than 50\% of the optimal performance.


\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_legend.png}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_InvertedPendulum.png}
        \caption{InvertedPendulum}
        \label{fig:result_step_norm:invertedpendulum}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_HalfCheetah.png}
        \caption{HalfCheetah}
        \label{fig:result_step_norm:halfcheetah}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_Walker2d.png}
        \caption{Walker2d}
        \label{fig:result_step_norm:walker2d}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_Fetch-Reach.png}
        \caption{Fetch-Reach}
        \label{fig:result_step_norm:fetchreach}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/all_comparisons/normalized_rewards/all_comparisons_Sawyer-Push.png}
        \caption{Sawyer-Push}
        \label{fig:result_step_norm:sawyer_push}
    \end{subfigure}
    \caption{
        Comparisons of all methods on the target-hard task with normalized rewards.  The lower bound is the performance of a policy trained in the source environment, then evaluated in the target.  The upper bound is a policy training directly in the target environment.
    }
\label{fig:result_step_norm}
\end{figure}


\subsection{Additional Analysis}

\subsubsection{\textbf{Data Accumulation}}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.7\linewidth}        
        \includegraphics[width=\linewidth]{figures/results/accumulate_data_ablation/ablations_legend.png}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/results/accumulate_data_ablation/ablations_HalfCheetah.png}
        \caption{HalfCheetah}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/results/accumulate_data_ablation/ablations_Walker2d.png}
        \caption{Walker2d}
    \end{subfigure}
    \caption{
        Performance of policies transferred to the target domain using different data accumulation options.  For \method~(Accumulate Data), we used double the number of finetuning epochs.
    }
\label{fig:accumulate_data}
\end{figure}

For each grounding step, we chose to use the online dataset of 1k interactions to finetune the visual transformation model.  An alternative would be to use the accumulated data gathered at each grounding step along with the large task agnostic dataset to make use of all available data.  We compare the two in \myfigref{fig:accumulate_data} and find that there is no performance improvement with data accumulation.  In fact, we may have to train for significantly more epochs or use a data balancing scheme to achieve similar results.  We hypothesize that the use of a small number of finetuning epochs and the fact that the most recent online dataset is better aligned with the task mitigates any issues with forgetting or overfitting and is more computationally efficient.





\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_learning_curves_legend.png}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_learning_curves_InvertedPendulum.png}
        \caption{InvertedPendulum}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_learning_curves_HalfCheetah.png}
        \caption{HalfCheetah}
\    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_learning_curves_Walker2d.png}
        \caption{Walker2d}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_learning_curves_Fetch-Reach.png}
        \caption{Fetch-Reach}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_learning_curves_Sawyer-Push.png}
        \caption{Sawyer-Push}
    \end{subfigure}
    \caption{
        Learning curve of policy in the target visual environment, without dynamics differences, throughout policy training in source environment to convergence.  
    }
\label{fig:norecon_ablation}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_grounding_legend.png}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_grounding_InvertedPendulum.png}
        \caption{InvertedPendulum}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_grounding_HalfCheetah.png}
        \caption{HalfCheetah}
\    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_grounding_Walker2d.png}
        \caption{Walker2d}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_grounding_Fetch-Reach.png}
        \caption{Fetch-Reach}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/norecon_ablation/visual_grounding_Sawyer-Push.png}
        \caption{Sawyer-Push}
    \end{subfigure}    
    \caption{
    Policy transfer performance in target-hard environments with and without state reconstruction regularization. The results show that the state reconstruction regularization is essential for our method to learn the visual transformation.
    }
\label{fig:norecon_grounding_ablation}
\end{figure}




\subsubsection{\textbf{Additional Datasets Analysis}}

The quality of the task-agnostic dataset can affect the initial grounding of the training environment.  We investigate the effect of using datasets gathered by different policies (Random, Backwards, Expert) in HalfCheetah and Walker2d tasks.  In addition, we also use mixed datasets, ``Random+Expert'' and ``Random+Backwards+Expert'', consisting of trajectories gathered from multiple different policies.  We look at both the first policy training step in a target environment with visual domain gap only (\myfigref{fig:datasets_ablations}) is isolate the visual domain difference and performance over multiple grounding steps in the target-hard environment (\myfigref{fig:dataset_exps:halfcheetah} and \myfigref{fig:dataset_exps:walker2d}).  In both cases, it is clear that the choice of dataset impacts transfer performance.  Notably, the Random dataset performs worst and is unable to improve over multiple grounding steps.  Meanwhile, the ``Random+Backwards+Expert'' dataset performs well, suggesting that in practice, a mixture dataset of many different behaviors will likely perform well even if some of those behaviors on their own would not result in good transformations.  Furthermore, we see that for most datasets, the performance does improve over multiple grounding steps, which allows IDAPT to partially overcome poor initial transformations.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{\linewidth}        
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/visual_dataset_learning_curves_legend.png}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/visual_dataset_learning_curves_HalfCheetah.png}
        \caption{HalfCheetah}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\linewidth}
        \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/visual_dataset_learning_curves_Walker2d.png}
        \caption{Walker2d}
    \end{subfigure}
    \caption{
        Learning curve of policy in the target visual environment, without dynamics differences, throughout policy training in source environment to convergence.  We use different initial datasets composed of trajectories collected by a random policy, a policy trained to walk backwards, and the expert policy.  
    }
\label{fig:datasets_ablations}
\end{figure}


% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[t]{\linewidth}        
%         \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/visual_dataset_grounding_legend.png}
%     \end{subfigure}
%     \\
%     \begin{subfigure}[t]{0.48\linewidth}
%         \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/visual_dataset_grounding_HalfCheetah.png}
%         \caption{HalfCheetah}
%     \end{subfigure}
%         \begin{subfigure}[t]{0.48\linewidth}
%         \includegraphics[width=\linewidth]{figures/results/dataset_comparisons/visual_dataset_grounding_Walker2d.png}
%         \caption{Walker2d}
%     \end{subfigure}
%     \caption{
%       Transfer performance across grounding steps for different initial datasets.
%     }
% \label{fig:datasets_grounding_ablations}
% \end{figure}



\subsection{Quality of Visual Transformations}

To evaluate the visual transformation quality, we show pairs of images from target and source domains that share the \textit{same underlying state}, and compare the difference between images generated by the visual transformation with source domain images and true target domain images, in \myfigref{fig:gen_imgs}. Before grounding, the puck disappears in the later frames because the Sawyer rarely moves the puck in the pretraining dataset.  After grounding, our visual transformation learns to translate the puck position correctly. 

In HalfCheetah and Walker2D, we noticed that occasionally the colors of the checkered floor flipped in the translated image, resulting in large pixel-wise errors (\myfigref{fig:walker_flipped_floor}).  The repeated floor pattern and the gait of the backwards agent used to gather the initial data result in a uniform floor distribution that makes it difficult for the CycleGAN to learn the correct alignment.  However, this did not affect the transfer performance of the policy, demonstrating that the learned policy can be robust to errors in translation that are task irrelevant.  This is possibly due to an invariance introduced by the random cropping image augmentation during training.  

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \makebox[0.2\linewidth]{Source}
        \makebox[0.2\linewidth]{Translated}
        \makebox[0.2\linewidth]{Target}
        \makebox[0.2\linewidth]{Diff}
    \end{subfigure}
    \\
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/images/walker_flipped_floor.png}
    \end{subfigure}
    \caption{
        Translated images across visual domains for the Walker2D target-easy task using the visual transformation.  
    }
    \label{fig:walker_flipped_floor}
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \makebox[0.2\linewidth]{Source}
        \makebox[0.2\linewidth]{Translated}
        \makebox[0.2\linewidth]{Target}
        \makebox[0.2\linewidth]{Diff}
    \end{subfigure}
    \\
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/images/sawyer_beforegrounding.png}
        \caption{Before grounding step}
    \end{subfigure} 
    \\
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/images/sawyer_aftergrounding.png}
        \caption{After grounding step}
    \end{subfigure}
    \caption{
        Translated images across visual domains for the Sawyer-Push task using the visual transformation after pretraining (a) and after finetuning with one grounding step (b).  
        Top row of each section is a series of source environment images, bottom row is the corresponding translated target environment image.  
    }
    \label{fig:gen_imgs}
\end{figure}



\subsection{Environment Details}
\label{sec:environment_details}

\subsubsection{\textbf{Locomotion Environments}}
The locomotion environments are modified from OpenAI Gym~\citep{brockman2016openai} and use the same default action space and visual domain. We created the target-easy visual domains by varying lighting and background color.  For the target-hard visual domain, we additionally varied background texture, character color, and viewpoint.

\subsubsection{\textbf{Manipulation Environments}}
Our manipulation environments included Fetch-Reach which is modified form OpenAI Gym, and Sawyer-Push which uses a simulation of he 7-DoF Rethink Sawyer. To create the dynamics of Fetch-Reach target environment we rotate the action vector around z-axis and add a bias to the third coordinate of the action vector. For Sawyer, we vary the friction and mass to create target environment dynamics. For visual of the target domain, we change colors,  lighting, and viewpoint in easy target environment, and we use Unity3D rendering with realistic lighting and background in the hard target environment.


\subsection{Baseline Implementations}
\label{sec:baseline_implementation}


%\subsubsection{Robust RL Implementation}
%To train the robust RL baseline, we add an additive Gaussian noise to the action space with the noise variance set at 1\% of the action range in each dimension.  We augment the images with random crop, from 100x100 pixels down to 92x92 pixels, and color jitter~\citep{laskin2020reinforcement}.  We apply color jitter with Torchvision transforms and we vary brightness, contrast, and saturation in the range of $[0.8,1.2]$ and vary hue from $[-0.1,0.1]$.  We train InvertedPendulum for 5e4 environment steps, HalfCheetah and Walker2D for 5e5 steps, and Sawyer-Push and Fetch-Reach for 1e5 steps.  For policy optimization, we use asymmetric SAC~\citep{pinto2017asymmetric} and use the same hyperparameters our method uses~\mytbref{tab:sac_hyperparameter}.  We include learning curves in \myfigref{fig:baseline_learning_curves}.

\subsubsection{\textbf{Domain Randomization Implementation}}
\label{sec:dr_implementation}
We implemented domain randomization by modifying simulation parameters every iteration with uniformly sampled random values. The sampling range for dynamics parameters are specified in Table. \ref{tab:dr_dynamics}. Example images of visual randomization we used during training can be found in \myfigref{fig:dr_visual_wide} and \myfigref{fig:dr_visual_narrow}. Additionally we use random cropping from 100x100 pixels images to 92x92 pixels.  We train InvertedPendulum for 5e4 environment steps, HalfCheetah and Walker2D for 5e5 steps, and Sawyer-Push and Fetch-Reach for 1e5 steps. For policy optimization, we use asymmetric SAC~\citep{pinto2017asymmetric} and use the same hyperparameters our method uses~\mytbref{tab:sac_hyperparameter}.  We include learning curves in \myfigref{fig:baseline_learning_curves}.  For some environments, DR-Wide was unable to reach good performance even in the training environment.

\begin{table}[ht]
\centering
\caption{Physics parameters for domain randomization.}
\begin{tabular}{ ccccc } 
 \toprule
 \multirow{2}{7.5em}{\centering Task} & \multirow{2}{6.5em}{\centering Parameter} & \multicolumn{2}{c}{Target} \\ 
 & & Easy & Hard \\
 \midrule
 InvertedPendulum & Pendulum mass & $4 \sim 55$ & $4 \sim 220$ \\ 
 \midrule
 HalfCheetah & Armature & $0.08 \sim 0.25$ & $0.08 \sim 0.44$ \\ 
 \midrule
 Walker2d & Torso mass & $3 \sim 6$ & $3 \sim 11$ \\ 
 \midrule
 \multirow{2}{7.5em}{\centering Fetch-Reach} & Action Rotation & $-30^{\circ}\sim30^{\circ}$ & $-45^{\circ}\sim45^{\circ}$\\
 & Action Bias & $-0.55 \sim 0$ & $-0.55 \sim 0.55$ \\
 \midrule
\multirow{2}{7.5em}{\centering Sawyer-Push} & Puck mass & $0.01 \sim 0.033$ & $0.01 \sim 0.05$ \\ 
& Puck Friction & $ 2 \sim 3.3$ & $2 \sim 4.4$\\
 \bottomrule
\end{tabular}
\label{tab:dr_dynamics}
\end{table}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.7\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/baselines/baseline_learning_curves_legend.png}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/baselines/baseline_learning_curves_InvertedPendulum.png}
        \caption{InvertedPendulum}
        \label{fig:baseline_learning_curves:invertedpendulum}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/baselines/baseline_learning_curves_HalfCheetah.png}
        \caption{HalfCheetah}
        \label{fig:baseline_learning_curves:halfcheetah}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/baselines/baseline_learning_curves_Walker2d.png}
        \caption{Walker2d}
        \label{fig:baseline_learning_curves:walker2d}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/baselines/baseline_learning_curves_Fetch-Reach.png}
        \caption{Fetch-Reach}
        \label{fig:baseline_learning_curves:fetchreach}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results/baselines/baseline_learning_curves_Sawyer-Push.png}
        \caption{Sawyer-Push}
        \label{fig:baseline_learning_curves:sawyer_push}
    \end{subfigure}
    \caption{
        Learning curves of baseline methods on training environments.
    }
\label{fig:baseline_learning_curves}
\end{figure}

\subsubsection{\textbf{Adaptive RL Implementation}}
We use an asymmetric SAC agent and use LSTM for both actor and critic network. During training, the agent collects experiences from domain randomized environments using the same domain randomization parameters as DR-Wide. During network updates, we use randomly sampled sequences of 7 time steps to calculate actor and critic loss. To initialize LSTM states during network update, We use stored the internal states of LSTM in both actor and critic collected during episode rollouts. We use the same hyperparameters, training steps, and image cropping as Domain Randomization.  We found that adaptive policies are difficult to train in environments with early termination based on failure, specifically InvertedPendulum and Walker2D.  The flexibility of the adaptive policy is outweighed by the difficulty of training in these tasks and would require further engineering to provide a strong comparison.  We include learning curves in \myfigref{fig:baseline_learning_curves}.


\subsubsection{\textbf{Cross-Domain Correspondence Implementation}}
\label{sec:cc_implementation}
We use the implementation provided by the authors~\citep{zhang2021learning} for cross morphology transfer for \textbf{CC-State}.  This model learns action and state-to-state correspondences to transfer a state-based policy across dynamics differences.  For \textbf{CC-Image}, we modified the existing cross-modality and cross-morphology implementations to replicate the cross-physics-and-modality algorithm to the best of our ability using the same architectures and implementation details.  This model learns action and state-to-image correspondences to transfer a state-based policy across dynamics and modality differences.  To make this algorithm iterative, we gather 1k samples from both environments with the current policy and train the model on the online data for each iteration.



\subsection{IDAPT Implementation Details}
\label{sec:idapt_details}

\subsubsection{\textbf{Policy Training}}
We use Asymmetric SAC~\citep{pinto2017asymmetric} to learn an RL policy.  The input to the actor is a stack of 3 consecutive image frames, originally 100x100 pixels and randomly cropped to 92x92.  The input to the critic is the state.  The actor network consists of a 4-layer CNN encoder with output feature space of dimension 50 and a 2-layer MLP with hidden dimensions of 1024, whose output parameterizes a Gaussian distribution over the action space.  For InvertedPendulum, Sawyer-Push, and Fetch-Reach, we train for 1e4 steps per policy training stage, which takes approximately 20 minutes to train on an  NVIDIA Titan X GPU. For HalfCheetah and Walker2D, we train for 2e5 steps, which takes approximately 3 hours.

\subsubsection{\textbf{Visual Transformation: CycleGAN with Regularization}}
We base our CycleGAN implementation on~\citep{zhu2017unpaired} and use the same hyperparameters and architectures.  Additionally, each state prediction network consists of a 4-layer CNN encoder with output dimension of 50 and a 2-layer MLP with hidden dimensions of size 256.  To train the source domain state prediction network, we use the Adam optimizer~\citep{kingma2014adam} with learning rate 3e-4.  We initialize the target domain state prediction network with the weights of the source domain network and train only the top convolutional layer jointly with the CycleGAN generator networks.  During pretraining, we first train the source domain state prediction network for 40 epochs, then train the CycleGAN + target domain state prediction network for 40 epochs.  During finetuning, we train each network group for 5 epochs. Training time for the initial training is approximately 8 hours and for finetuning is approximately 20 minutes.

\subsubsection{\textbf{Action Transformation: Visual GARAT}}
We use GAIfO adversarial training to optimize the action transformation using a PPO agent with parameters listed in Table. \ref{tab:ppo_hyperparameter}.  The observation space of the agent and discriminator is the concatenation of the policy feature space, $f(o_t)$ (dim = 50) and action space of the environment.  The discriminator learns to differentiate $(f(o_t),a,f(o_{t+1}))$ tuples.  The agent and discriminator are both 2-layer MLPs with hidden dimensions of 1024.  Following \citet{desai2020imitation}, we add the output of the agent to the original action and use action smoothing, proposed in \citet{hanna2017grounded} with smoothing parameter 0.95, to get the transformed action.  Every grounding step, we train the action transformation for 10 epochs. Training time for the action transformation training in each grounding step is approximately 30 minutes.

\begin{table}[ht]
    \caption{SAC hyperparameters.}
    \label{tab:sac_hyperparameter}
    \centering
    \begin{tabular}{lc}
        \toprule
        Hyperparameter & Value \\
        \midrule
        Learning Rate & 0.0003 \\
        Learning Rate Decay & Linear decay \\
        Batch Size & 32 \\
        \# Epochs per Update & 10 \\
        Discount Factor & 0.99 \\
        Entropy Coefficient & 0.001 \\
        Reward Scale & 1  \\
        Normalization & False \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[ht]
    \caption{PPO hyperparameters.}
    \label{tab:ppo_hyperparameter}
    \centering
    \begin{tabular}{lc}
        \toprule
        Hyperparameter & Value \\
        \midrule
        Rollout Size & 5000 \\
        Learning Rate & 0.0003 \\
        Learning Rate Decay & Linear decay \\
        Batch Size & 32 \\
        \# Epochs per Update & 5 \\
        Discount Factor & 0.5 \\
        Entropy Coefficient & 0.001 \\
        Clipping Ratio & 0.1  \\
        Normalization & False  \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_min.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_min1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_min2.png}
        \caption{InvertedPendulum}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_min.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_min0.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_min2.png}
        \caption{HalfCheetah}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_min.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_min1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_min2.png}
        \caption{Walker2d}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_min0.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_min1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_min2.png}
        \caption{Fetch-Reach}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_min.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_min1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_min2.png}
        \caption{Sawyer-Push}
    \end{subfigure}
    \caption{
        Narrow range visualize domain randomization examples, including color and lighting changes.
    }
    \label{fig:dr_visual_narrow}
\end{figure}

\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_max.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_max1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/IP_max2.png}
        \caption{InvertedPendulum}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_max.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_max1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/HC_max2.png}
        \caption{HalfCheetah}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_max.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_max1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/WK_max2.png}
        \caption{Walker2d}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_max0.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_max1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/FR_max2.png}
        \caption{Fetch-Reach}
    \end{subfigure}
    \\
    \begin{subfigure}[t]{\linewidth}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_max.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_max1.png}
        \includegraphics[width=0.3\linewidth]{figures/dr_example/SP_max2.png}
        \caption{Sawyer-Push}
    \end{subfigure}
    \caption{
        Wide range visualize domain randomization examples, including viewpoint changes and more textures in addition to color and lighting changes.
    }
    \label{fig:dr_visual_wide}
\end{figure}