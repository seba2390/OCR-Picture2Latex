Deep learning models (and machine learning models in general) focus on solving one task at a time. However, applications often require more than one task to be performed simultaneously and the naive solution is to deploy in parallel one model for each task.

Applications imposing real-time constraints require small inference times and prohibit off-board computation, forcing the deployment of deep learning models in embedded systems, in which not only storage and memory available are limited but also computing power. This presents challenges in terms of resources, accentuated when deploying multiple models: weights storage and forward pass memory usage and computational complexity.

Multitask Learning \cite{Caruana} learns to solve tasks in parallel using a shared representation of a common input, improving the generalization capabilities of the models. Multitask networks share a base trunk and a number of task-specific branches emerging from it. Only the task-specific layers are computed separately.

In this work, we propose using multitask models to get benefits in terms of speed, memory usage and storage during deployment. For studying our approach, we train and evaluate a multitask model for both semantic segmentation and object detection. We highlight the challenges imposed by applying MTL, explain how they affect the performance of our model and show that it compares positively in terms of inference time, memory usage and model size against deploying one model per task.

