\documentclass[10pt,journal,compsoc]{IEEEtran}


%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}



% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
% IEEE Computer Society needs nocompress option
% requires cite.sty v4.0 or later (November 2003)
\usepackage[nocompress]{cite}
\else
% normal IEEE
\usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
% \usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
% \graphicspath{{../pdf/}{../jpeg/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
% \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
% or other class option (dvipsone, dvipdf, if not using dvips). graphicx
% will default to the driver specified in the system graphics.cfg if no
% driver is specified.
% \usepackage[dvips]{graphicx}
% declare the path(s) where your graphic files are
% \graphicspath{{../eps/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
% \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{approximation algorithms steps value holds ii gradient Algorithm sets}

\usepackage{graphicx}
\usepackage{subfigure} 
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{url}
\usepackage{float}
\usepackage{multirow}
\usepackage{color}
\usepackage{soul}

\def\rank{\text{rank}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{prop}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

\def \T{\mathbb T}
\def \R{\mathbb R}
\def\tU{\tilde{U}}
\def\tV{\tilde{V}}
\def\tS{\tilde{\Sigma}}
\def\l{\lambda}
\def\rank{\text{rank}}

\def \st{\;\text{s.t.}\;}
\def\etal{et al.\;}

\newcommand{\X}{\mathbf{X}}

% norms
\newcommand{\NM}[2]{\| #1 \|_{#2} }  
\newcommand{\TNN}[2]{\| #1 \|^{\text{(tnn})}_{#2}}   % truncated norm
\newcommand{\CAP}[2]{\| #1 \|^{\text{(cap)}}_{#2}}   % capped norm

\newcommand{\Prox}[2]{\text{prox}_{#1}(#2)}
\newcommand{\SO}[1]{\mathcal{P}_{\mathbf{\Omega}}(#1)}
\newcommand{\Tr}[1]{\text{tr}( #1 ) }
\newcommand{\Span}[1]{\text{span}(#1)}
\newcommand{\QR}[1]{\text{\sf QR}( #1 )}

\newcommand{\f}[1]{f(#1)}
\newcommand{\Diag}[1]{\text{Diag}(#1)}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline  \\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline \\\arraybackslash\hspace{0pt}}m{#1}}

% Replace "require" and "ensure" with "input" and "output" in algorithmic package of LaTeX
\renewcommand{\algorithmicrequire}{\textbf{Input:}}

\begin{document}
	
% The paper headers
\markboth{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (Accepted, 2018)}%
{}

% \markboth{IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,~Vol.~, No.~, August~}%
	
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Large-Scale Low-Rank Matrix Learning with Nonconvex Regularizers}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

%\author{\IEEEauthorblockN{Quanming Yao \quad James T. Kwok} \\
%\IEEEauthorblockA{Department of Computer Science and Engineering \\
%Hong Kong University of Science and Technology \\
%Hong Kong\\
%\{qyaoaa, jamesk\}@cse.ust.hk}%
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
%}


\author{
Quanming Yao, ~\IEEEmembership{Member IEEE},
James T. Kwok, ~\IEEEmembership{Fellow IEEE},
Taifeng Wang, ~\IEEEmembership{Member IEEE},
\\
and Tie-Yan Liu,
~\IEEEmembership{Fellow IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{
\IEEEcompsocthanksitem Q. Yao is with 4Paradigm Inc, Beijing, China. 
	E-mail: yaoquanming@4Paradigm.com
% <-this % stops an unwanted space
\IEEEcompsocthanksitem J. Kwok is 
	with the Department of Computer Science and Engineering, 
	Hong Kong University of Science and Technology, Clear Water Bay, 
	Hong Kong.
	E-mail: \{qyaoaa, jamesk\}@cse.ust.hk
% <-this % stops an unwanted space
\IEEEcompsocthanksitem T. Wang and T. Liu are 
	with Microsoft Research Asia, Beijing, China 100010.
	E-mails: \{taifengw, tyliu\}@microsoft.com
}% <-this % stops an unwanted space
%	\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}




% make the title area

\IEEEtitleabstractindextext{%
\begin{abstract}
Low-rank modeling has many important applications in computer vision and machine learning. 
While the matrix rank is often approximated by the convex nuclear norm, 
the use of nonconvex low-rank regularizers has demonstrated better empirical performance. 
However, the resulting optimization problem is much more challenging. 
Recent state-of-the-art requires an expensive full SVD in each iteration. 
In this paper, we show that for many commonly-used nonconvex low-rank regularizers, 
the singular values obtained from the proximal operator
can be automatically threshold.
This allows the proximal operator to be efficiently approximated by the power method. 
We then develop a fast proximal algorithm and its accelerated variant
with inexact proximal step. 
It can be guaranteed that
the squared distance between consecutive iterates
converges at a  rate of $O(1/T)$,
where $T$ is the number of iterations.
Furthermore,
we show the proposed algorithm can be parallelized, and the resultant algorithm
achieves nearly linear speedup w.r.t. the number of threads. 
Extensive experiments are performed on matrix completion and robust principal component
analysis. Significant speedup 
over the state-of-the-art
is observed. 
\end{abstract}

\begin{IEEEkeywords}
Low-rank matrix learning,
Nonconvex regularization,
Proximal algorithm,
Parallel algorithm,
Matrix completion
\end{IEEEkeywords}
}

\maketitle

% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
%\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:intro}

\IEEEPARstart{L}{ow}-rank matrix 
learning 
is a central issue in many machine learning and computer vision problems. For example, 
matrix completion
\cite{candes2009exact},
which is one of the most successful approaches in collaborative filtering,
assumes that the target rating matrix is low-rank. 
Besides 
collaborative filtering,
matrix completion has also been used on tasks such as
video and image processing \cite{hu2013fast,lu2016nonconvex,gu2016weighted}.
Another important use of low-rank matrix learning is robust principal component analysis (RPCA) \cite{candes2011robust}, 
which assumes that the target 
matrix 
is 
low-rank 
and also corrupted by  sparse noise.  RPCA has been popularly used in computer vision
applications such as shadow removal, background modeling
\cite{candes2011robust,sun2013robust,oh2016partial}, and
robust photometric stereo \cite{wu2010robust}.
Besides,
low-rank matrix learning has also been used in 
face recognition \cite{candes2011robust,sun2013robust},
domain adaptation \cite{li2017domain}
and 
subspace clustering \cite{liu2013robust,xiao2016robust,xiao2015falrr}.

However, minimization of the matrix rank 
is NP-hard \cite{candes2009exact}. To alleviate this problem, a common
approach is to use  
a convex surrogate such as 
the nuclear norm (which is the sum of singular values of the matrix).
It is known that the
nuclear norm is the tightest convex lower bound of the rank.
Though the nuclear norm is non-smooth, the resultant optimization problem can be 
solved efficiently using modern tools such as the
proximal algorithm \cite{ji2009accelerated,mazumder2010spectral,quan2015impute},
Frank-Wolfe algorithm \cite{zhang2012accelerated},
and active subspace selection \cite{hsieh2014nuclear}.

Despite the success of the nuclear norm, recently there have been numerous attempts to use
nonconvex surrogates that better
approximate the rank function.
The key idea is 
that the larger, and thus more informative, singular values
should be less penalized.
Example nonconvex low-rank regularizers include
the capped-$\ell_1$ penalty \cite{zhang2010analysis},
log-sum penalty (LSP) \cite{candes2008enhancing},
truncated nuclear norm (TNN) \cite{hu2013fast,oh2016partial},
smoothly clipped absolute deviation (SCAD) \cite{fan2001variable}, and
minimax concave penalty (MCP) \cite{zhang2010nearly}. 
They have been applied to various computer vision tasks,
such as image denoising \cite{gu2016weighted} and background modeling \cite{oh2016partial}.
Empirically, these nonconvex regularizers achieve
better recovery performance than the convex nuclear norm regularizer.
Recently, theoretical results have also been established \cite{gui2016towards}.

However, the resultant nonconvex optimization problem is much more challenging.
Most existing optimization algorithms that work with the nuclear norm cannot be
applied. A general approach that can still be used is the concave-convex procedure \cite{yuille-03}, which
decomposes the nonconvex regularizer into a difference of convex functions
\cite{zhang2010analysis,hu2013fast}.  However, a sequence of relaxed optimization problems have to be solved,
and can be computationally expensive \cite{gongZLHY2013,li2015accelerated}.
A more efficient approach is the recently proposed iteratively re-weighted nuclear norm (\textsf{IRNN}) algorithm
\cite{lu2016nonconvex}. It is based on the observation that existing nonconvex regularizers are 
concave with
non-increasing
super-gradients.
Each
\textsf{IRNN} 
iteration only involves computing the 
super-gradient of the regularizer and a singular value decomposition (SVD).
However, performing SVD on a $m\times n$ matrix 
takes $O(mn^2)$
time
(assuming $m\geq n$), and can be expensive on 
large
matrices.

Recently, the proximal algorithm has 
been used for nonconvex low-rank matrix learning \cite{hu2013fast,lu2016nonconvex,lu2015generalized,oh2016partial}.
However, it requires the full SVD to solve the proximal operator, 
which can be expensive.
In this paper, we observe that for the commonly-used nonconvex low-rank
regularizers 
	\cite{zhang2010analysis, candes2008enhancing, hu2013fast,oh2016partial, fan2001variable,
	zhang2010nearly},
the singular values obtained from the corresponding proximal operator can be automatically  
thresholded.
One then only needs to find the leading singular values/vectors in order to generate the next iterate. 
Moreover, instead of computing the proximal operator on a large matrix, one only needs to
use the matrix projected onto its leading subspace. The matrix size is  
significantly reduced and the proximal operator can be made much more efficient.
Besides, by using the power method \cite{halko2011finding},
a good approximation of this subspace can be efficiently obtained.  

While the proposed procedure can be readily used with the standard proximal algorithm,
its convergence properties are not directly applicable as
the proximal step here is only approximately solved.
In this paper, we will show that inexactness on the proximal step can be controlled.
The resultant algorithm, which will be called ``Fast Nonconvex Low-rank Learning
(\textsf{FaNCL})",
can be shown to have a
convergence rate of $O(1/T)$ (measured by the squared distance between consecutive
iterates).
This can be further speeded up  using acceleration, leading to the 
\textsf{FaNCL-acc} algorithm.

Effectiveness of the proposed
algorithms is demonstrated on two popular low-rank matrix learning applications,
namely matrix completion and robust principal component analysis (RPCA).  For matrix
completion, we show that additional speedup is possible by exploiting the problem's ``sparse
plus low-rank" structure; whereas for RPCA, we extend the proposed algorithm so that it can
handle the two parameter blocks involved in the RPCA formulation.
With the popularity of  multicore shared-memory platforms, we parallelize
the proposed algorithms so as to handle much larger data sets.
%\footnote{\url{http://www.trustedreviews.com/news/intel-8th-gen-core-specs-technology-explained-2952599}}

Experiments are performed on both synthetic and real-world data sets.
Results show that the proposed nonconvex low-rank matrix learning algorithms can be several orders faster than the state-of-the-art,
and outperform approaches based on matrix factorization and nuclear norm regularization.
Moreover, the parallelized variants achieve almost linear speedup w.r.t. the number of threads.

%{\color{red}
%Preliminary results have been reported in \cite{yao2015fast}.  
%In this journal version, we speed up the algorithm with acceleration, and show
%that it leads to faster convergence.  
%%Then, for applications on matrix completion and RPCA, the conference version is
%%based on the (non-accelerated) \textsf{FaNCL} algorithm.  On the other hand, this
%%submission includes also the accelerated \textsf{FaNCL-acc} algorithm
%%(Section~\ref{sec:accFaNCL}), which leads to faster convergence.  Moreover, the
%%algorithm becomes more complicated and so does the analysis.  For example,
%%consider RPCA, only one inexact proximal step is needed for \textsf{FaNCL}, but
%%possibly two for \textsf{FaNCL-acc}.  Thus, previous analysis in the conference
%%version can no longer be used.  Here, new convergence analysis results are
%%provided in Section~\ref{sec:rpca}.
%Besides, we show
%how the proposed algorithms
%can be parallelized.
%More extensive empirical evaluations are also performed on  both
%the sequential and parallel versions of the algorithms.
%%Furthermore, we add the two matrix completion data sets to the Section~\ref{sec:expgray} (gray images) and \ref{sec:exphyper} (hyperspectral images).
%}


In summary,
this paper has three main novelties:
(i) 
Inexactness on the proximal step can be controlled;
(ii) 
Use of acceleration for further speedup; (iii) Parallelization for much larger data sets. 
As can be seen
from Table~\ref{tab:comp}, 
the proposed \textsf{FaNCL-acc} 
is the only  parallelizable,
accelerated inexact
proximal 
algorithm 
on nonconvex problems.


\begin{table}[ht]
\centering
%\vspace{-10px}
\caption{Comparison of the proposed algorithms with existing algorithms.}
\vspace{-10px}
\begin{tabular}{ c | c | c | C{35px} | c }
	\hline
	method                                     & regularizer & acceleration & proximal step & parallel \\ \hline
	\textsf{APG} \cite{schmidt2011convergence} & convex      & yes          & inexact       & no       \\ \hline
	\textsf{GIST} \cite{gongZLHY2013}          & nonconvex   & no           & exact         & no       \\ \hline
	\textsf{GD} \cite{attouch2013convergence}  & nonconvex   & no           & inexact       & no       \\ \hline
	\textsf{nmAPG}  \cite{li2015accelerated}   & nonconvex   & yes          & exact         & no       \\ \hline
	\textsf{IRNN}  \cite{lu2016nonconvex}      & nonconvex   & no           & exact         & no       \\ \hline
	\textsf{GPG}  \cite{lu2015generalized}     & nonconvex   & no           & exact         & no       \\ \hline
	\textsf{FaNCL}                             & nonconvex   & no           & inexact       & yes      \\ \hline
	\textsf{FaNCL-acc}                         & nonconvex   & yes          & inexact       & yes      \\ \hline
\end{tabular}
\label{tab:comp}
\end{table}

\noindent
\textbf{Notation:}
In the sequel, 
 vectors are denoted by lowercase
boldface, matrices by uppercase boldface,
and the transpose by the superscript $(\cdot)^\top$. 
For a 
square matrix $\mathbf{X}$,
$\Tr{\X}$ is its trace.
For a rectangular matrix $\mathbf{X}$,
$\NM{\X}{F} = \sqrt{\Tr{\X^{\top} \X}}$ is its Frobenius norm, 
and $\NM{\X}{*} = 
%\sum_{i=1}^n 
\sum_i
\sigma_i( \X )$, where $\sigma_i( \X )$
is the $i$th leading singular value of $\X$, 
is the nuclear norm.
Given $\mathbf{x} = [x_i] \in \R^{m}$, 
$\Diag{\mathbf{x}}$ constructs a $m \times m$ diagonal matrix whose $i$th diagonal element is $x_i$.  
$\mathbf{I}$ denotes the identity matrix.
For a differentiable function $f$, we use $\nabla f$ for its gradient.
For a nonsmooth function,
we use $\partial f$ for its subdifferential,
i.e.,
$\partial f(\mathbf{x}) = \left\lbrace \mathbf{s} 
: f(\mathbf{y}) \ge f(\mathbf{x}) + \mathbf{s}^{\top} (\mathbf{y} - \mathbf{x}) \right\rbrace$.



%-----------------------------------------------------------------------------------------
% Review
%-----------------------------------------------------------------------------------------

\section{Background}
\label{sec:review}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proximal Algorithm}
\label{sec:prox}

%\footnote{+++ I think the AE is Dong Xu:
%	he suggests adding reference \cite{xiao2015falrr,xiao2016robust,li2017domain}}
%\footnote{+++ suggestion from the 3rd reviewer:
%	\cite{yang2013feature,ma2014knowledge}}
In this paper, 
we consider 
low-rank matrix learning 
problems  of the form
\begin{equation}
\label{eq:problem}
\min_{\X} F(\X) \equiv f(\X) + \lambda r(\X),
\end{equation} 
where $f$ is a smooth loss, $r$ is a nonsmooth
low-rank regularizer, and 
$\lambda$ is a regularization parameter.
We make the following assumptions on $f$.
\begin{itemize}
\item[A1.] $f$ is not necessarily convex, but is differentiable with $\rho$-Lipschitz continuous
gradient, i.e., $\NM{\nabla \f{\X_1} - \nabla \f{\X_2}}{F} \le \rho \NM{\X_1 - \X_2}{F}$.
Without loss of generality, we assume that $\rho \le 1$.

\item[A2.] $f$ is bounded below, i.e., $\inf \f{\X} > -\infty$,
and $\lim_{\NM{\X}{F} \rightarrow \infty} f(\X) = \infty$.
\end{itemize}

In recent years, the proximal algorithm
\cite{parikh2014proximal} has been popularly used for solving
(\ref{eq:problem}).
At iteration $t$, it produces 
\begin{equation} \label{eq:prosplt}
\X_{t+1} = \Prox{\frac{\lambda}{\tau} r}{\X_t - \frac{1}{\tau} \nabla f(\X_t)},
\end{equation} 
where
$\tau > \rho$ is the stepsize, and
\begin{align}
\Prox{\frac{\l}{\tau} r}{\mathbf{Z}}
\equiv \text{\normalfont Arg}\min_{\X} \frac{1}{2} \NM{\X - \mathbf{Z}}{F}^2  + \frac{\lambda}{\tau} r(\X)
\label{eq:proxoper}
\end{align}
is the proximal operator \cite{parikh2014proximal}.
The proximal step in (\ref{eq:prosplt}) can also be rewritten as
$\X_{t+1} = 
\text{\normalfont Arg}\min_{\mathbf{Y}} 
\Tr{\nabla \f{\X_t}^\top (\mathbf{Y} - \X_t)}
+  
\frac{\tau}{2} \NM{\mathbf{Y} - \X_t}{F}^2 
+ 
\l r(\mathbf{Y})$.
When $f$ and $r$ are convex,
the proximal algorithm converges to the optimal solution at a rate of $O(1/T)$,
where $T$ is the number of iterations.
This can be further accelerated
to 
$O(1/T^2)$, by replacing $\X_t$ in (\ref{eq:prosplt}) with a proper linear combination of $\X_t$ and $\X_{t-1}$
\cite{beck2009fast}.
Recently, the accelerated proximal algorithm has been extended to
problems
where $f$ or $r$ are nonconvex \cite{li2015accelerated,ghadimi2016accelerated}.
The state-of-the-art
is the nonmonotone accelerated proximal gradient
(\textsf{nmAPG}) algorithm \cite{li2015accelerated}. 
%{
%\color{red}
%Each iteration 
%performs a maximum of  two proximal steps (steps~4 and 8).  Acceleration 
%is performed in step~3.  The objective is then checked to determine whether 
%$\X^a_{t + 1}$ is accepted
%(step~5).
%}
As the problem is nonconvex, its convergence rate is still open.  
However, empirically it is much faster.

%{
%\begin{algorithm}[ht]
%	\color{red}
%	\caption{Nonmonotone APG (\textsf{nmAPG}) \cite{li2015accelerated}.}
%	
%	\begin{algorithmic}[1]
%		\REQUIRE choose $\tau > \rho$, $\delta > 0$ and $\eta \in [0, 1)$;
%		\STATE initialize $\X_0 = \X_1 = \X^a_1 = 0$, 
%		$\alpha_0 = \alpha_1 = 1$, $b_1 = F(\X_1)$ and $q_1 = 1$;
%		\FOR{$t = 1, 2, \dots, T $}
%		\STATE $\mathbf{Y}_t = \X_t + \frac{\alpha_{t - 1}}{\alpha_t}(\X^a_t - \X_{t - 1})
%		+ \frac{\alpha_{t-1} - 1}{\alpha_t}(\X_t - \X_{t - 1})$;
%		\STATE $\X^a_{t + 1} = \Prox{\frac{\lambda}{\tau} r}{\mathbf{Y}_t - \frac{1}{\tau}  \nabla f(\mathbf{Y}_t)}$;
%		\IF{$F(\X^a_{t + 1}) \le b_t - \frac{\delta}{2}\NM{\X^a_{t + 1} - \mathbf{Y}_t}{F}^2$}
%		\STATE $\X_{t + 1} = \X^a_{t + 1}$;
%		\ELSE
%		\STATE $\X^p_{t + 1} = \Prox{\frac{\lambda}{\tau} r}{\X_t - \frac{1}{\tau} \nabla f(\X_t)}$;
%		\STATE 
%		$\X_{t + 1} =
%		\begin{cases}
%		\X^a_{t + 1}
%		& \text{if}\; F(\X^a_{t + 1}) \le F(\X^p_{t + 1})
%		\\
%		\X^p_{t + 1}
%		& \text{otherwise}
%		\end{cases}
%		$;
%		\ENDIF 
%		\STATE $\alpha_{t + 1} = \frac{1}{2} (\sqrt{4\alpha_t^2 + 1} + 1)$;
%		\STATE $b_{t + 1} = \frac{1}{q_{t + 1}}( \eta q_t b_t + F(\X_{t + 1}) )$ where $q_{t + 1} = \eta q_t + 1$;
%		\ENDFOR
%		\RETURN $\X_{T + 1}$.
%	\end{algorithmic}
%	\label{alg:nmapg}
%\end{algorithm}
%}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Nonconvex Low-Rank Regularizers}
\label{sec:lowrankreg}

For the proximal algorithm to be successful, 
the underlying proximal operator 
has to be efficient.  
The following shows that the proximal operator  
of the 
nuclear norm $\NM{\cdot}{*}$
has a closed-form solution.

\begin{prop}[\!\!\! \cite{cai2010singular}] \label{lem:svt} 
$\Prox{\mu \NM{\cdot}{*}}{\X} 
= \mathbf{U} \left( \mathbf{\Sigma} - \mu \mathbf{I}\right)_{+} \mathbf{V}^{\top}$,
where $\mathbf{U} \mathbf{\Sigma} \mathbf{V}^{\top}$ is the SVD of $\X$, and $\mathbf{A}_{+} = [\max (A_{ij}, 0)]$. 
\end{prop}


Popular 
proximal algorithms 
for nuclear norm minimization
include the \textsf{APG} \cite{toh2010accelerated},
\textsf{Soft-Impute} \cite{mazumder2010spectral}
(and its faster variant \textsf{AIS-Impute} \cite{quan2015impute}),
and active subspace selection\cite{hsieh2014nuclear}.
For \textsf{APG},
the ranks of the iterates 
have to be estimated
heuristically. 
For \textsf{Soft-Impute}, \textsf{AIS-Impute} and active subspace selection,
minimization is performed inside
a subspace.
The smaller the span
of this subspace,
the smaller is the rank of the matrix iterate. 
For good performance, these methods
usually 
require
a much higher
rank.

While the (convex) nuclear norm makes low-rank 
optimization easier,
it may not be a good enough approximation of the matrix rank 
\cite{hu2013fast,lu2016nonconvex,lu2015generalized,gu2016weighted,oh2016partial}. As mentioned in
Section~\ref{sec:intro}, a number of nonconvex surrogates have been recently
proposed.  In this paper, we make the following assumption on the low-rank regularizer
$r$ in (\ref{eq:problem}), which is
satisfied by all nonconvex low-rank regularizers in Table~\ref{tab:lwregu}.

\begin{itemize}
	\item[A3.] $r$ is possibly non-smooth and nonconvex, and 
	of the form $r(\X)=\sum_{i = 1}^m \hat{r}(\sigma_i(\X))$, 
where 
$\hat{r}(0) = 0$
and 
$\hat{r}(\alpha)$ is concave and non-decreasing for $\alpha \ge 0$.
\end{itemize}

\begin{table}[ht]
\centering
%\vspace{-15px}
\caption{$\hat{r}$'s for some popular nonconvex low-rank regularizers. 
For the TNN regularizer, $\theta \in \{1,\dots,n\}$ is the number of leading singular values that are not penalized;
for SCAD, $\theta > 2$;
and for the others, $\theta > 0$.}
\vspace{-10px}
\begin{tabular}{c | c} \hline
	& $\mu\hat{r}(\sigma_i(\X))$
	\\ \hline
	capped-$\ell_1$ \cite{zhang2010analysis} & $\mu\min(\sigma_i(\X), \theta )$  \\ \hline
	LSP \cite{candes2008enhancing} & $\mu\log \left(\frac{1}{\theta} \sigma_i(\X) + 1\right)$  \\ \hline
	TNN \cite{hu2013fast,oh2016partial}
	& $\begin{cases}
	\mu\sigma_i(\X) & \text{if}\; i  > \theta \\
	0         & \text{otherwise}
	\end{cases}$
	\\ \hline
	SCAD \cite{fan2001variable} & 
	$\begin{cases}
	\mu \sigma_i(\X)                                                  &
	\text{if}\; \sigma_i(\X) \le \mu             \\
	\frac{-\sigma_i^2(\X) + 2 \theta \mu \sigma_i(\X) - \mu^2}{2(\theta - 1)} &
	\text{if}\; \mu < \sigma_i(\X) \le \theta\mu \\
	\frac{(\theta + 1) \mu^2}{2}                              & 
	\text{otherwise}
	\end{cases}$
	\\ \hline
	MCP \cite{zhang2010nearly} & 
	$\begin{cases}
	\mu \sigma_i(\X) - \frac{\sigma_i^2(\X)}{2 \theta} &
	\text{if}\; \sigma_i(\X) \le \theta \mu\\
	\frac{\theta \mu^2}{2}             & \text{otherwise}
	\end{cases}$
	\\ \hline
\end{tabular}
\label{tab:lwregu}
\end{table}

Recently,
the iteratively reweighted nuclear norm (\textsf{IRNN}) algorithm \cite{lu2016nonconvex}
has been proposed to handle this nonconvex low-rank matrix optimization problem. 
In each iteration,
it solves a subproblem
in which the original nonconvex regularizer is approximated by
a weighted version of the nuclear norm
$\NM{\X}{\mathbf{w}} = \sum_{i = 1}^m w_i \sigma_i(\X)$ 
and $0 \leq w_1 \le \dots \le w_m$.
The subproblem has a closed-form solution,
but SVD is needed which takes $O(m n^2)$ time.
Other solvers that are designed for specific nonconvex low-rank regularizers include \cite{sun2013robust} (for capped-$\ell_1$),
\cite{hu2013fast,oh2016partial} (for TNN),
and \cite{zhang2010nearly} (for MCP).
All these (including \textsf{IRNN}) perform SVD in each iteration,
which takes $O(m n^2)$ time and are slow.


While the proximal algorithm has mostly been used  on convex problems, 
recently it is also applied to nonconvex problems \cite{sun2013robust,hu2013fast,lu2016nonconvex,lu2015generalized,gu2016weighted,oh2016partial}.
In particular, the generalized proximal gradient (\textsf{GPG}) algorithm \cite{lu2015generalized} is a 
proximal algorithm which can handle all the above nonconvex regularizers.
In particular, 
the proximal operator
can be computed as follows.

\begin{prop} [Generalized singular value thresholding (GSVT) 
\cite{lu2015generalized}] 
\label{pr:proxReduce}
For any $r$ satisfying assumption~A3, 
$\Prox{\mu r}{\mathbf{Z}} = \mathbf{U}\Diag{\mathbf{y}^*} \mathbf{V}^{\top}$, where $\mathbf{U} \mathbf{\Sigma} \mathbf{V}^{\top}$ is SVD of $\mathbf{Z}$, 
and $\mathbf{y}^*= [y^*_i]$ with
\begin{equation} 
\label{eq:proRed}
y_i^* \in \text{\normalfont Arg}
\min_{y_i \ge 0} \frac{1}{2} \left(y_i - \sigma_i(\mathbf{Z})\right)^2 + \mu \hat{r}(y_i).
\end{equation}
\end{prop}

In \cite{lu2015generalized}, problem (\ref{eq:proRed}) is solved by fixed-point iteration.
However,
closed-form solutions 
indeed
exist for regularizers in Table~\ref{tab:lwregu}
\cite{gongZLHY2013}.
Nevertheless,
Proposition~\ref{pr:proxReduce}
still involves a full SVD,
which takes $O(m n^2)$ time.

Finally,
unlike nuclear norm minimization,
iterates generated by
algorithms for adaptive nonconvex regularization
(including 
\textsf{IRNN},
\textsf{GPG} and
the regularizer-specific algorithms in 
\cite{sun2013robust, hu2013fast,oh2016partial})
may not be low-rank.


%\subsection{Low-rank Matrix Learning Algorithms}
%\label{sec:lralgs}

\section{Proposed Algorithm}
\label{sec:proalg}
 
In this section, we show how the proximal algorithm for
nonconvex low-rank matrix regularization
can be made much faster.
First, 
Section~\ref{sec:auto}
shows that
the GSVT operator in Proposition~\ref{pr:proxReduce}
does not need
all singular values, which
motivates the development of an approximate GSVT in Section~\ref{sec:apprSVT}.
This approximation is used in the inexact proximal step in Section~\ref{sec:pstep},
and the whole proximal algorithm is shown in Section~\ref{sec:complete}.
Convergence is analysed in Section~\ref{sec:convana}.
Finally, Section~\ref{sec:accFaNCL} presents 
further speedup with the use of acceleration.




%%%%%%%%%%%%%%%%

\subsection{Automatic Thresholding of Singular Values}
\label{sec:auto}

The following 
Proposition 
\footnote{All Proofs are in Appendix~A.}
shows $y_i^*$
in (\ref{eq:proRed}) becomes zero when
$\sigma_i(\mathbf{Z})$ is smaller than a regularizer-specific threshold. 
%Proof can be found in Appendix.

\begin{prop}
\label{pr:proxSolution} 
There exists a threshold $\gamma > 0$
such that 
$y^*_i = 0$
when $\sigma_i(\mathbf{Z}) \le \gamma$.
\end{prop} 

%Together with Proposition~\ref{pr:proxReduce}, 
Thus, solving the proximal operator in (\ref{eq:proxoper}) only needs the leading singular values/vectors of $\mathbf{Z}$.
For the nonconvex regularizers in Table~\ref{tab:lwregu}, the following
Corollary shows that
simple closed-form solutions  of $\gamma$ can be obtained 
by examining the optimality conditions of (\ref{eq:proRed}).
%Proof can be found in Appendix~\ref{app:threh}.

\begin{corollary} \label{cor:threh} 
The $\gamma$   values 
for the following regularizers 
are:
\begin{itemize}
\item 
Capped-$\ell_1$: $\gamma = \min\left(\sqrt{2\theta \mu}, \mu\right)$;

\item LSP: $\gamma=\min\left(\frac{\mu}{\theta},\theta\right)$; 

\item TNN: $\gamma = \max\left(\mu, \sigma_{\theta + 1}(\mathbf{Z})\right)$;

\item SCAD: $\gamma = \mu$;

\item MCP: $\gamma = \sqrt{\theta} \mu$ if $0 < \theta < 1$, and $\mu$ otherwise.
\end{itemize}
\end{corollary} 
Corollary~\ref{cor:threh} can also be extended to the nuclear norm. 
Specifically,
it can be shown that $\gamma= \frac{\lambda}{\tau}$,
and $y_i^*=\max\left( \sigma_i(\X_{\text{gd}}) - \frac{\lambda}{\tau}, 0 \right)$.
However, since our focus is on nonconvex regularizers,  
it will not be pursued in the sequel.

%%%%%%%%%%%%%%%%

\subsection{Approximate GSVT}
\label{sec:apprSVT}

Proposition~\ref{pr:proxReduce} computes the proximal operator using exact SVD.
Due to automatic thresholding of the singular values
in Section~\ref{sec:auto},
this can be made more efficient by using
	partial SVD.
Moreover,  we will show in this Section that
the proximal operator only needs to be 
computed
on a much smaller matrix.

%%%%%%%%%%%%%%%%%%%%

\subsubsection{Reducing the Size of SVD}

Assume that $\mathbf{Z}$ has $\hat{k}$ singular values that are larger than $\gamma$.
We 
then
only need to perform a rank-$k$ SVD on $\mathbf{Z}$ with $k \ge \hat{k}$.
Let the rank-$\hat{k}$ SVD of $\mathbf{Z}$ be $\mathbf{U}_{\hat{k}} \mathbf{\Sigma}_{\hat{k}} \mathbf{V}_{\hat{k}}^{\top}$.
The following Proposition shows that 
$\Prox{\mu r}{\mathbf{Z}}$
can be obtained from the proximal operator on the smaller matrix
$\mathbf{Q}^{\top} \mathbf{Z}$.
%The proof can be found in Appendix~\ref{app:redGSVT}.
\footnote{We noticed a similar result in \cite{oh2018pami} after the conference version of this paper \cite{yao2015fast} has been accepted.  However, \cite{oh2018pami} only considers the case where $r$ is the nuclear norm regularizer.}


\begin{prop} \label{pr:redGSVT} 
Assume that $\mathbf{Q} \in \R^{m \times k}$, where $k \ge \hat{k}$, is orthogonal and
$\Span{\mathbf{U}_{\hat{k}}} \subseteq \Span{\mathbf{Q}}$.
Then,
$\Prox{\mu r}{\mathbf{Z}} = \mathbf{Q} \, \Prox{\mu r}{\mathbf{Q}^{\top} \mathbf{Z}}$.
\end{prop}

%%%%%%%%%%%%%%%%%%%%

\subsubsection{Obtaining an Approximate GSVT}
\label{sec:apprgsvt}

To obtain such a 
$\mathbf{Q}$,
we will use the 
power method 
(Algorithm~\ref{alg:powermethod}).
It has sound approximation guarantee, good empirical performance
\cite{halko2011finding}, and
has been recently used 
to approximate the SVT in nuclear norm minimization 
\cite{hsieh2014nuclear,quan2015impute}.  
As in \cite{hsieh2014nuclear},
we set the number of power iterations to 3.
Warm-start can be used via matrix $\mathbf{R}$ in
Algorithm~\ref{alg:powermethod}. 
This is particularly useful because of the iterative nature of proximal algorithm.
Obtaining an approximate $\mathbf{Q}$ using Algorithm~\ref{alg:powermethod} takes $O(m n k)$ time.
As in \cite{toh2010accelerated,mazumder2010spectral}, the 
PROPACK package \cite{larsen1998lanczos},
which is based on the Lanczos
algorithm,
can also be used to obtain $\mathbf{Q}$ in $O(m n k)$ time.
However, it finds $\mathbf{Q}$ exactly and
cannot benefit from warm-start. Hence, though it has the same time complexity as 
power method, empirically it is much less efficient \cite{quan2015impute}.

\begin{algorithm}[ht]
\caption{\textsf{Powermethod}$(\mathbf{Z}, \mathbf{R})$.}
\begin{algorithmic}[1]
	\REQUIRE $\mathbf{Z} \in \R^{m \times n}$, $\mathbf{R} \in \R^{n \times k}$
	and the number of power iterations $J=3$.
	\STATE $\mathbf{Y}_1 = \mathbf{Z} \mathbf{R}$;
	\FOR{$j = 1, 2, \dots, J$}
	\STATE $\mathbf{Q}_j = \QR{\mathbf{Y}_j}$;  
	
	// QR decomposition (returning only the $\mathbf{Q}$ matrix)
	\STATE $\mathbf{Y}_{j + 1} = \mathbf{Z} (\mathbf{Z}^{\top} \mathbf{Q}_j)$;
	\ENDFOR
	\RETURN $\mathbf{Q}_{J}$. 
\end{algorithmic}
\label{alg:powermethod}
\end{algorithm}

Algorithm~\ref{alg:apprGSVT} shows steps of the approximate GSVT.
Step~1 uses the power method to efficiently obtain an orthogonal matrix $\mathbf{Q}$ that approximates $\Span{\mathbf{U}_{\hat{k}}}$.
Step~2 
performs
a small SVD.
Though this SVD is still exact,
$\mathbf{Q}^{\top} \mathbf{Z}$ is much smaller than $\mathbf{Z}$ ($k \times n$ vs $m \times n$),
and
$\text{SVD}(\mathbf{Q}^{\top} \mathbf{Z})$
takes only $O(n k^2)$ time.
In step~3,
the singular values $\Sigma_{ii}$'s
are thresholded using Corollary~\ref{cor:threh}.
Steps~4-6 obtains an (approximate) $\Prox{\mu r}{\mathbf{Z}}$
using Proposition~\ref{pr:proxReduce}.
The time complexity for GSVT is reduced from $O(m n^2)$ 
to $O(m n k)$.


\begin{algorithm}[ht]
\caption{Approximate GSVT: \textsf{ApproxGSVT}$(\mathbf{Z}, \mathbf{R}, \mu)$.}
\begin{algorithmic}[1]
	\REQUIRE $\mathbf{Z} \in \R^{m \times n}$ 
	and $\mathbf{R} \in \R^{n \times k}$ for warm-start;
	\STATE $\mathbf{Q} = \text{\sf PowerMethod}(\mathbf{Z}, \mathbf{R})$;
	\STATE $[ \mathbf{U}, \mathbf{\Sigma}, \mathbf{V} ] = \text{\sf SVD}(\mathbf{Q}^{\top} \mathbf{Z})$;
	\STATE $a = $ number of $\Sigma_{ii}$'s that are $>\gamma$
	in Corollary~\ref{cor:threh};
	\STATE $\mathbf{U}_a = a$ leading columns of $\mathbf{U}$;
	\STATE $\mathbf{V}_a = a$ leading columns of $\mathbf{V}$;
	\STATE obtain $y^*_i$ from \eqref{eq:proRed} for all $i = 1, \dots, a$;
	\RETURN  low-rank components of $\tilde{\mathbf{X}}$
	($\mathbf{Q} \mathbf{U}_a$, $\Diag{[ y_1^*,\dots,y_{a}^* ]}$ and $\mathbf{V}_a^{\top}$),
	and $\mathbf{V}$.
\end{algorithmic}
\label{alg:apprGSVT}
\end{algorithm}

\subsection{Inexact Proximal Step}
\label{sec:pstep}

In order to compute the proximal step efficiently,
we will utilize the approximate GSVT in Algorithm~\ref{alg:apprGSVT}.
However,
the resultant proximal step is then inexact.
To ensure convergence of the resultant proximal algorithm,
we need to control the approximation quality of the proximal step.

First,
the following Lemma shows that the objective $F$ is always decreased (as $\tau > \rho$)
when the proximal step is computed exactly.

\begin{lemma}
\label{lem:desc}
{\normalfont  ( $\!\!\!\!$ \cite{gongZLHY2013}, \cite{attouch2013convergence})}
Let $\X_{\text{gd}} = \mathbf{X} - \frac{1}{\tau} \nabla f(\mathbf{X})$.
Then, 
we have
$F( \Prox{\frac{\lambda}{\tau} r}{\X_{\text{gd}}} ) 
\le
F(\mathbf{X}) - \frac{\tau - \rho}{2} 
\NM{\Prox{\frac{\lambda}{\tau} r}{\X_{\text{gd}}} - \mathbf{X}}{F}^2$.
\end{lemma}

Let the approximate 
proximal step 
solution  obtained at the $p$th iteration 
be $\tilde{\X}_p$.
Motivated by 
Lemma~\ref{lem:desc},
we control the quality of $\tilde{\X}_p$ by monitoring the objective value $F$.
Specifically,
we try to ensure that
\begin{align}
F( \tilde{\mathbf{X}}_p )
\le F(\mathbf{X}) - c_1 \NM{\tilde{\mathbf{X}}_p - \mathbf{X}}{F}^2,
\label{eq:decrease}
\end{align} 
where $c_1 = \frac{\tau - \rho }{4}$.
Note that this is less stringent than the condition in Lemma~\ref{lem:desc}.
The procedure is shown in Algorithm~\ref{alg:inexactPS}.  
If \eqref{eq:decrease} holds, 
we accept $\tilde{\mathbf{X}}_p$;
otherwise, 
we improve $\tilde{\mathbf{X}}_p$ by using $\tilde{\mathbf{V}}_{p - 1}$ to warm-start the next iterate.
The following Proposition shows convergence of Algorithm~\ref{alg:inexactPS}.
%The proof can be found in Appendix~\ref{app:apprGSVT}.

\begin{prop}
\label{pr:apprGSVT}
If $k \ge \hat{k}_{\X_{\text{gd}}}$,
where $\hat{k}_{\X_{\text{gd}}}$ is the number of singular values in $\X_{\text{gd}}$ larger than $\gamma$,
then
$\lim_{p \rightarrow \infty} \tilde{\mathbf{X}}_p = \Prox{\frac{\lambda}{\tau} r}{\X_{\text{gd}}}$.
\end{prop}

\begin{algorithm}[ht]
\caption{Inexact proximal step: \textsf{InexactPS}$(\mathbf{X}, \mathbf{R})$.}
\begin{algorithmic}[1]
	\REQUIRE $\mathbf{X} \in \R^{m \times n}$, and $\mathbf{R} \in \R^{n \times k}$ for warm-start;
	\STATE $\X_{\text{gd}} = \mathbf{X} - \frac{1}{\tau} \nabla f(\mathbf{X})$;
	\STATE $\tilde{\mathbf{V}}_0 = \mathbf{R}$;
	\FOR{$p = 1,2,\dots$}
	\STATE $[ \tilde{\mathbf{X}}_p, \tilde{\mathbf{V}}_p ]  = $
	\textsf{ApproxGSVT}
	$( \X_{\text{gd}}, \tilde{\mathbf{V}}_{p - 1}, \frac{\lambda}{\tau} )$;
	\STATE \textbf{if} $F( \tilde{\mathbf{X}}_p ) \le F(\mathbf{X}) - c_1 \NM{\tilde{\mathbf{X}}_p - \mathbf{X}}{F}^2$
	\textbf{then} break;
	\ENDFOR
	\RETURN 
	$\tilde{\mathbf{X}}_p$ and $\tilde{\mathbf{V}}_p$.
\end{algorithmic}
\label{alg:inexactPS}
\end{algorithm}

The use of inexact proximal steps has also been considered in \cite{attouch2013convergence,schmidt2011convergence}.  
However, $r$ 
in (\ref{eq:problem})
is assumed to be convex in \cite{schmidt2011convergence}.
Attouch \etal \cite{attouch2013convergence} considered nonconvex $r$, but they
	require a difficult and expensive condition to control inexactness
	(an example is provided in Appendix~B).


\subsection{The Complete Procedure}
\label{sec:complete}

The complete procedure  
for solving (\ref{eq:problem})
is shown in Algorithm~\ref{alg:FaNCL},
and will be called  
FaNCL (\underline{Fa}st \underline{N}on\underline{C}onvex \underline{L}owrank).
Similar to \cite{hsieh2014nuclear,quan2015impute}, we perform warm-start 
using the column spaces of the previous iterates ($\mathbf{V}_{t}$ and $\mathbf{V}_{t - 1}$).
For further speedup, 
we employ a continuation strategy at step~3 as in \cite{mazumder2010spectral,lu2016nonconvex,toh2010accelerated}.
Specifically,
$\lambda_t$ is initialized to a large value
and then
decreases
gradually.

\begin{algorithm}[ht]
\caption{\textsf{FaNCL} \!\! (Fast \! NonConvex Low-rank) \!\! algorithm.}
\begin{algorithmic}[1]
	\REQUIRE choose $\tau > \rho$, $\lambda_0 > \lambda$ and $\nu \in (0,1)$;
	\STATE initialize $\mathbf{V}_0, \mathbf{V}_1 \in \R^{n \times 1}$ as random Gaussian matrices and $\X_1 = 0$;
	\FOR{$t = 1,2,\dots T$}
	\STATE $\lambda_t = (\lambda_{t-1} -\lambda) \nu^t + \lambda$;
	\STATE $\mathbf{R}_t = \QR{[\mathbf{V}_t, \mathbf{V}_{t - 1}]}$; // warm start
	\STATE $[\X_{t + 1}, \mathbf{V}_{t + 1}] =$ \textsf{InexactPS}$(\X_t, \mathbf{R}_t)$;
	\ENDFOR 
	\RETURN $\X_{T + 1}$.
\end{algorithmic}
\label{alg:FaNCL}
\end{algorithm}

Assume that evaluations of $f$ and $\nabla f$ take 
$O(m n)$ time,
which is valid for many applications such as matrix completion and RPCA.
Let $r_t$ be the rank of $\X_t$ at the $t$th iteration, and $k_t = r_t + r_{t - 1}$.  In
Algorithm~\ref{alg:FaNCL}, step~4 takes $O(n k_t^2)$ time; and step~5 takes $O(m n p k_t)$ time
as $\mathbf{R}_t$ has $k_t$ columns.
The iteration time complexity 
is 
thus
$O(m n p k_t)$.
In the experiment, we set $p = 1$,
which is 
enough to guarantee (\ref{eq:decrease})
empirically.
The iteration time complexity 
	of Algorithm~\ref{alg:FaNCL} is thus reduced to
	$O( m n k_t )$.
In contrast, exact GSVT takes $O(m n^2)$ time, and is much slower as $k_t \ll n$.
Besides,
	the space complexity of
	Algorithm~\ref{alg:FaNCL} is $O( m n )$.

\subsection{Convergence Analysis}
\label{sec:convana}

The inexact proximal algorithm is first considered in
\cite{schmidt2011convergence}, which assumes $r$ to be convex.
This does not hold here as the regularizer is nonconvex.
The nonconvex
extension is considered in \cite{attouch2013convergence}.  However, 
it assumes the Kurdyka-Lojasiewicz condition  \cite{bolte2010characterizations} 
on $f$, which
does not hold
for $C^{\infty}$ functions
(including the commonly used square loss)
in general.
On the other hand,
we only assume that $f$ is Lipschitz-smooth.
Besides,
as discussed in Section~\ref{sec:pstep},
they use an 
expensive
condition to control inexactness of the proximal step.
Thus, their analysis cannot 
be applied here.  

In the following, we first
show that $r$,
similar to $\hat{r}$ in Assumption~A3 \cite{gongZLHY2013},
can be decomposed as a difference of convex functions.
%The proof is in Appendix~\ref{app:dc}.

\begin{prop} \label{pr:dc}
$r$ can be decomposed as $\breve{r} - \tilde{r}$,
where $\breve{r}$ and $\tilde{r}$ are convex.
\end{prop}

Based on this decomposition, we introduce the definition of critical point.

\begin{definition}
[\!\cite{hiriart85}]
If $\mathbf{0} \in \nabla f(\X) + \lambda \left( \partial \breve{r}(\X) - \partial \tilde{r}(\X) \right) $,
then $\X$ is a {\em critical point} of $F$.
\label{def:crti}
\end{definition}


The following Proposition
shows that Algorithm~\ref{alg:FaNCL} generates a bounded sequence.
%The proof is in Appendix~\ref{app:boundseq}.

\begin{prop} \label{pr:boundseq}
The sequence $\{\X_t\}$ generated from Algorithm~\ref{alg:FaNCL} is bounded,
and has at least one limit point.
\end{prop}

Let $\mathcal{G}_{\frac{\lambda}{\tau} r}(\X_t) = \X_t - \Prox{\frac{\lambda}{\tau}r}{\X_t - \frac{1}{\tau} \nabla f(\X_t)}$,
which is known as the proximal mapping of $F$ at $\X_t$ \cite{parikh2014proximal}.
If $\mathcal{G}_{\frac{\lambda}{\tau} r}(\X_t) = 0$,
$\mathbf{X}_t$ is a critical point of \eqref{eq:problem}
\cite{attouch2013convergence,gongZLHY2013}.
This motivates the use of $\| \mathcal{G}_{\frac{\lambda}{\tau} r}(\X_t) \|_2^2$ to measure convergence 
in \cite{ghadimi2016accelerated}.
However, $\| \mathcal{G}_{\frac{\lambda}{\tau} r}(\X_t) \|_2^2$
cannot be used here as $r$ is nonconvex and the proximal step is inexact.
As 
Proposition~\ref{pr:boundseq}
guarantees
the existence of limit points,
we use $\NM{\X_{t + 1} - \X_t}{F}^2$ instead to measure convergence.
If the proximal step is exact, 
$\| \mathcal{G}_{\frac{\lambda}{\tau} r}(\X_t) \|_F^2 = \| \X_{t + 1} - \X_t \|_F^2$.
The following Corollary
shows convergence of Algorithm~\ref{alg:FaNCL}.
%Its proof can be found in Appendix~\ref{app:rate}.

\begin{corollary} \label{cor:rate}
$\min_{t = 1, \dots, T} \NM{\X_{t + 1} - \X_t}{F}^2 \le \frac{F(\X_1) - \inf F}
{c_1 T}$.
\end{corollary}


The following Theorem shows  that
any limit point is also a critical point.
%The proof is in Appendix~\ref{app:convergence}.

\begin{theorem} \label{the:FaNCL:conv}
Assume that Algorithm~\ref{alg:inexactPS} returns $\mathbf{X}$ only when 
$\mathbf{X} = \Prox{\frac{\lambda}{\tau} r}{\mathbf{X} - \frac{1}{\tau} \nabla
f(\mathbf{X})}$ (i.e.,
the input is 
returned
as output only if  it
is the desired exact proximal step solution).
%it is a limit point).
Let $\{ \X_{t_j} \}$ be
a subsequence of $\{ \X_t \}$ generated by Algorithm~\ref{alg:FaNCL} such that
$\lim_{t_j \rightarrow \infty} \X_{t_j} = \X_*$.
Then, $\X_*$ is a critical point of \eqref{eq:problem}.
\end{theorem}




\subsection{Acceleration}
\label{sec:accFaNCL}

In convex optimization, acceleration has been commonly used to speed up convergence
of proximal algorithms \cite{beck2009fast}.  
Recently, it has also been extended to nonconvex optimization \cite{li2015accelerated,ghadimi2016accelerated}.
A state-of-the-art algorithm is the \textsf{nmAPG} \cite{li2015accelerated}.
In this section, we integrate \textsf{nmAPG} with \textsf{FaNCL}. The whole procedure
is shown in Algorithm~\ref{alg:FaNCLacc}.
The accelerated iterate is obtained in step~4.
If the resultant inexact proximal step solution can achieve a sufficient decrease (step~7)
as in \eqref{eq:decrease},
this iterate is accepted (step~8);
otherwise,
we 
choose the inexact proximal step solution obtained with the non-accelerated iterate
$\mathbf{X}_t$ (step~10).
Note that step~10 
is the same as step~5 of Algorithm~\ref{alg:FaNCL}.
Thus, the iteration time complexity of Algorithm~\ref{alg:FaNCLacc} is at most twice that of Algorithm~\ref{alg:FaNCL}, and still $O(m n k_t)$.  
Besides, its space complexity is $O(m n)$, 
which is the same as Algorithm~\ref{alg:FaNCL}.

There are several major differences between Algorithm~\ref{alg:FaNCLacc} and \textsf{nmAPG}.  First, the proximal step of Algorithm~\ref{alg:FaNCLacc} is only inexact.
To make the algorithm more robust, we do not allow nonmonotonous update (i.e., $F(\X_{t +
1})$ cannot be larger than $F(\X_t)$).
Moreover, we use a simpler acceleration scheme (step~4), in which only $\X_t$ and $\X_{t - 1}$ are involved.  
On matrix completion problems,
this allows using the ``sparse plus low-rank'' structure
\cite{mazumder2010spectral} to greatly reduce the iteration complexity
(Section~\ref{sec:matcomp}).  Finally, we do not require extra comparison of the objective at step~10. This further reduces the iteration complexity.

\begin{algorithm}[ht]
	\caption{Accelerated FaNCL algorithm (\textsf{FaNCL-acc}).}
	\begin{algorithmic}[1]
		\REQUIRE choose $\tau > \rho$, $\lambda_0 > \lambda$, $\delta > 0$ and $\nu \in (0,1)$;
		\STATE initialize $\mathbf{V}_0, \mathbf{V}_1 \in \R^{n}$ as random Gaussian matrices, $\X_0 = \X_1 = 0$
		and $\alpha_0 = \alpha_1 = 1$;
		\FOR{$t = 1,2,\dots T$}
		\STATE $\lambda_t = (\lambda_{t-1} -\lambda) \nu + \lambda$;
		\STATE $\mathbf{Y}_t = \X_t + \frac{\alpha_{t - 1} - 1}{\alpha_t}(\X_t - \X_{t - 1})$;
%		\STATE $\hat{Z}_t = Y_t - \frac{1}{\tau} \nabla \f{Y_t}$;
		\STATE $\mathbf{R}_t = \QR{[\mathbf{V}_t, \mathbf{V}_{t - 1}]}$; // warm start
		\STATE $\X^a_{t + 1} =$ \textsf{InexactPS}$(\mathbf{Y}_t, \mathbf{R}_t)$;
		\IF{$F(\X^a_{t + 1}) \le F(\X_t) - \frac{\delta}{2} \NM{\X^a_{t + 1} - \mathbf{Y}_t}{F}^2$}
		\STATE $\X_{t + 1} = \X^a_{t + 1}$;
		\ELSE
%		\STATE $Z_t = X_t - \frac{1}{\tau} \nabla f(X_t)$;
		\STATE $\X_{t + 1} =$ \textsf{InexactPS}$(\X_t, \mathbf{R}_t)$;
		\ENDIF
		\STATE $\alpha_{t + 1} = \frac{1}{2} (\sqrt{4\alpha_t^2 + 1} + 1)$;
		\ENDFOR 
		\RETURN $\X_{T + 1}$.
	\end{algorithmic}
	\label{alg:FaNCLacc}
\end{algorithm}


The following Proposition shows that Algorithm~\ref{alg:FaNCLacc} generates a bounded sequence.
%Proof can be found in Appendix~\ref{app:convacc}.

\begin{prop} \label{pr:convacc}
	The sequence $\{\X_t\}$ generated from Algorithm~\ref{alg:FaNCLacc} is bounded,
	and has at least one limit point.
\end{prop}

In Corollary~\ref{cor:rate}, $\NM{\X_{t + 1} - \X_t}{F}^2$ is used to measure progress
before and after the proximal step.  In Algorithm~\ref{alg:FaNCLacc}, the
proximal step may use the accelerated iterate $\mathbf{Y}_t$ or the non-accelerated iterate
$\X_t$. Hence, we
use
$\| \X_{t+1} - \mathbf{C}_t \|_F^2$,
where
$\mathbf{C}_t = \mathbf{Y}_t$ if step~8 is performed, and $\mathbf{C}_t = \X_t$ otherwise.
Similar to Corollary~\ref{cor:rate},
the following shows a $O(1/T)$ convergence rate.
%Proof can be found in Appendix~\ref{app:rateacc}.

\begin{corollary} \label{cor:rateacc}
$\min_{t = 1, \dots, T} \NM{\X_{t + 1} - \mathbf{C}_t}{F}^2 \le \frac{F(\X_1) - \inf F}{\min(c_1, \delta/2) T}$. 
\end{corollary}

On nonconvex optimization problems, the optimal convergence rate  for first-order methods is
$O(1/T)$ \cite{ghadimi2016accelerated}.  Thus, the convergence rate
of Algorithm~\ref{alg:FaNCLacc} (Corollary~\ref{cor:rateacc}) cannot improve that of
Algorithm~\ref{alg:FaNCL}
(Corollary~\ref{cor:rate}).
However, in practice,
acceleration can still significantly reduce the number of iterations 
on nonconvex problems 
\cite{li2015accelerated,ghadimi2016accelerated}.
On the other hand, as Algorithm~\ref{alg:FaNCLacc} may need a second proximal step 
(step~10), its iteration time complexity can be higher than that of Algorithm~\ref{alg:FaNCL}. 
However, this is much compensated by the speedup in convergence.
As will be demonstrated in Section~\ref{sec:exptmc},
empirically 
Algorithm~\ref{alg:FaNCLacc} is much faster.

The following Theorem shows  that
any limit point of the iterates from
Algorithm~\ref{alg:FaNCLacc} 
is also a critical point.
%Proof can be found in Appendix~\ref{app:critacc}.

\begin{theorem} \label{thm:critacc}
Let $\{ \X_{t_j} \}$ be
a subsequence of $\{ \X_t \}$ generated by Algorithm~\ref{alg:FaNCLacc} such that
$\lim_{t_j \rightarrow \infty} \X_{t_j} = \X_*$.
With the assumption in Theorem~\ref{the:FaNCL:conv},
$\X_*$ is a critical point of \eqref{eq:problem}.
\end{theorem}

%-----------------------------------------------------------------------------------------
% Special Formulations
%-----------------------------------------------------------------------------------------

\begin{table*}[ht]
\centering
%\vspace{-5px}
\renewcommand{\arraystretch}{1.2}
\caption{Comparison of the iteration time complexities, convergence rates and space
complexity
of various matrix completion solvers.  
Here, $k_t = r_t + r_{t - 1}$, $\nu \in (0,1)$ and integer $T_a > 0$ are constants.  For the
active subspace selection method (\textsf{active}) \cite{hsieh2014nuclear}, $T_s$ is the number of inner iterations  required.}
\vspace{-10px}
\begin{tabular}{c | c | c | c | c}
	\hline
	      regularizer        & method                                                   & convergence rate   & iteration time complexity                                       & space complexity                          \\ \hline
	        (convex)         & \textsf{APG} \cite{ji2009accelerated,toh2010accelerated} & $O(1/T^2)$         & $O(m n r_t)$                                                    & $O(mn)$                                   \\ \cline{2-5}
	      nuclear norm       & \textsf{active} \cite{hsieh2014nuclear}                  & $O(\nu^{T - T_a})$ & $O( \NM{\mathbf{\Omega}}{1} k_t T_{s})$                         & $O((m + n)k_t + \NM{\mathbf{\Omega}}{1})$ \\ \cline{2-5}
	                         & \textsf{AIS-Impute} \cite{quan2015impute}                & $O(1/T^2)$         & $O(\NM{\mathbf{\Omega}}{1} k_t + (m + n)k^2_t)$                 & $O((m + n)k_t + \NM{\mathbf{\Omega}}{1})$ \\ \hline
	fixed-rank factorization & \textsf{LMaFit} \cite{wen2012solving}                    & ---                & $O( \NM{\mathbf{\Omega}}{1} r_t + (m + n)r_t^2)$                & $O((m + n)r_t + \NM{\mathbf{\Omega}}{1})$ \\ \cline{2-5}
	                         & \textsf{ER1MP} \cite{wang2015orthogonal}                 & $O(\nu^T)$         & $O(\NM{\mathbf{\Omega}}{1})$                                    & $O((m + n)r_t + \NM{\mathbf{\Omega}}{1})$ \\ \hline
	       nonconvex         & \textsf{IRNN} \cite{lu2016nonconvex}                     & ---                & $O(m n^2)$                                                      & $O(mn)$                                   \\ \cline{2-5}
	                         & \textsf{GPG} \cite{lu2015generalized}                    & ---                & $O(m n^2)$                                                      & $O(mn)$                                   \\ \cline{2-5}
	                         & \textsf{FaNCL}                                           & $O(1/T)$           & $O\left(  \NM{\mathbf{\Omega}}{1} r_t + (m + n)r_t k_t \right)$ & $O((m + n)r_t + \NM{\mathbf{\Omega}}{1})$ \\ \cline{2-5}
	                         & \textsf{FaNCL-acc}                                       & $O(1/T)$           & $O\left(  \NM{\mathbf{\Omega}}{1} k_t + (m + n)k_t^2 \right)$   & $O((m + n)k_t + \NM{\mathbf{\Omega}}{1})$ \\ \hline
\end{tabular}
\label{tab:timecomp}
%\vspace{-10px}
\end{table*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Applications}
\label{sec:applications}

In this section,
we consider two important instances of problem \eqref{eq:problem},
namely, matrix completion \cite{candes2009exact}
and robust principal component analysis (RPCA) \cite{candes2011robust}.
%The accelerated \textsf{FaNCL-acc} algorithm (Algorithm~\ref{alg:FaNCLacc}) will be considered here.
For
matrix completion (Section~\ref{sec:matcomp}),
we will show that the proposed algorithm
%~\ref{alg:FaNCLacc} 
can be made even faster and require much less
memory by using the ``sparse plus low-rank" structure of the problem.
In Section~\ref{sec:rpca},
we show how the 
algorithm
%~\ref{alg:FaNCLacc}
can be extended to deal with 
the two parameter blocks
in RPCA.


%As will be shown in 
%Section~\ref{sec:exptmc},
%the accelerated \textsf{FaNCL} algorithm (Algorithm~\ref{alg:FaNCLacc}) is usually
%empirically faster than its non-accelerated variant. Hence,
%we will only consider 
%the accelerated variant here.



\subsection{Matrix Completion}
\label{sec:matcomp}

Matrix completion attempts to recover a low-rank matrix $\mathbf{O} \in \R^{m \times
n}$ by observing only some of its elements \cite{candes2009exact}.  
Let the 
observed positions be indicated by  
$\mathbf{\Omega} \in \{0,1\}^{m \times n}$, such that
$\Omega_{ij}=1$ if $O_{ij}$ is observed, and 0 otherwise.
Matrix completion can be formulated as an optimization
problem in (\ref{eq:problem}), with
\begin{align}
F(\X) = \frac{1}{2}\NM{\SO{\X - \mathbf{O}}}{F}^2 + \lambda r(\X),
\label{eq:matcomp}
\end{align}
where $[\SO{\X_{\text{gd}}}]_{ij} = A_{ij}$ if  $\Omega_{ij} = 1$ and $0$ otherwise.

\subsubsection{Utilizing the Problem Structure}

In the following, we focus on 
the accelerated \textsf{FaNCL-acc} algorithm (Algorithm~\ref{alg:FaNCLacc}), 
and show that its time and space complexities 
can be further reduced.
Similar techniques can also be used on 
the simpler non-accelerated \textsf{FaNCL} algorithm (Algorithm~\ref{alg:FaNCL}).

First, consider step~7
(of Algorithm~\ref{alg:FaNCLacc}), 
which checks the objectives.
Computing $F(\X_t)$ relies only on the observed positions in $\mathbf{\Omega}$ and the singular values of $\X_t$.
Hence, instead of explicitly constructing $\X_t$, 
we maintain 
the SVD 
$\mathbf{U}_t \mathbf{\Sigma}_t \mathbf{V}_t^{\top}$
of $\X_t$ and a sparse matrix $\SO{\X_t}$.
Computing $F(\X_t)$ then takes $O(\NM{\mathbf{\Omega}}{1} r_t)$ time.
Computing $F(\X^a_{t + 1})$ takes $O( \NM{\mathbf{\Omega}}{1} k_t )$ time, as $\mathbf{R}_t$
has rank $k_t$.
Next,
since $\mathbf{Y}_{t}$ is a linear combination of $\X_t$ and $\X_{t - 1}$ in step~4,
we can use the above SVD-factorized form and compute $\NM{\X^a_{t + 1} - \mathbf{Y}_t}{F}^2$ in $O((m + n) k^2_t )$ time.
Thus,
step~7 then takes $O( \NM{\mathbf{\Omega}}{1}k_t + (m + n)k_t^2 )$ time.


Steps~6 and 10 perform inexact proximal step. For the first proximal step
(step~6),
$\mathbf{Y}_t$ (defined in step~4) can be rewritten as $( 1 + \beta_t ) \X_t - \beta_t \X_{t - 1}$,
where $\beta_t = (\alpha_{t - 1} - 1)/\alpha_t$.
When it calls $\textsf{InexactPS}$,
step~1
of Algorithm~\ref{alg:inexactPS} has
%a special structure on $\X_{\text{gd}}$ 
\begin{align}
\X_{\text{gd}}
& = \mathbf{Y}_t + \frac{1}{\tau} \SO{\mathbf{Y}_t- \mathbf{O}}
\notag \\
& = (1 + \beta_t) \X_t - \beta_t \X_{t - 1}
+ \frac{1}{\tau} \SO{\mathbf{Y}_t - \mathbf{O}}.
\label{eq:zt}
\end{align}
The first two terms
involve low-rank matrices, while the last term
involves a sparse matrix.
This special ``sparse plus low-rank'' structure 
\cite{mazumder2010spectral}
%can speed up matrix multiplications.
is essential for the
matrix completion solver,
including the proposed algorithm,
to be efficient.
Specifically,
for any $\mathbf{V} \in \R^{n \times k}$,
$\X_{\text{gd}} \mathbf{V}$ 
can be obtained as
\begin{align}
\X_{\text{gd}} \mathbf{V}
\! = & (1 + \beta_t) \mathbf{U}_t \mathbf{\Sigma}_t (\mathbf{V}_t^{\top} \mathbf{V}) 
- \beta_t \mathbf{U}_{t - 1} \mathbf{\Sigma}_{t - 1} (\mathbf{V}_{t - 1}^{\top} \mathbf{V}) 
\notag \\
& + \frac{1}{\tau}\SO{\mathbf{O} - \mathbf{Y}_t} \mathbf{V}.
\label{eq:temp2}
\end{align}
Similarly,
for any $\mathbf{U} \in \R^{m \times k}$,
$\mathbf{U}^{\top} \X_{\text{gd}}$ 
can be obtained as
\begin{align}
\mathbf{U}^{\top} \X_{\text{gd}}
\! = & (1 + \beta_t) (\mathbf{U}^{\top} \mathbf{U}_t) \mathbf{\Sigma}_t \mathbf{V}_t^{\top} 
\! - \! \beta_t (\mathbf{U}^{\top} \mathbf{U}_{t - 1}) \mathbf{\Sigma}_{t - 1} \mathbf{V}_{t - 1}^{\top} 
\notag \\
& + \frac{1}{\tau} \mathbf{U}^{\top} \SO{\mathbf{O} - \mathbf{Y}_t}.
\label{eq:temp3}
\end{align}
Both \eqref{eq:temp2} and \eqref{eq:temp3} take $O((m + n) k_t k +
\NM{\mathbf{\Omega}}{1} k)$,
instead of $O(m n k)$),
time.
In contrast, existing algorithms for adaptive nonconvex regularizers (such as
\textsf{IRNN} \cite{lu2016nonconvex} and \textsf{GPG} \cite{lu2015generalized}) cannot utilize this special structure
and are slow, as will be demonstrated in Section~\ref{sec:exptmc}.




As $\mathbf{R}_t$ in step~5 of Algorithm~\ref{alg:FaNCLacc}
has $k_t$ columns, each call to 
approximate GSVT takes
$O( (m + n)k_t^2 + \NM{\mathbf{\Omega}}{1} k_t )$ time \cite{quan2015impute}
(instead of $O( m n k_t ) $).
Finally, 
step~5
in Algorithm~\ref{alg:inexactPS}
also takes
$O ( (m + n) k_t^2 + \NM{\mathbf{\Omega}}{1} k_t )$ time.
As a result, 
step~6 of Algorithm~\ref{alg:FaNCLacc}
takes a total of
$O(  (m + n)k_t^2 + \NM{\mathbf{\Omega}}{1} k_t ) $
time.
Step~10 is slightly cheaper 
(as no $\X_{t-1}$ is involved),
and its time complexity is
$O( (m + n)r_t k_t + \NM{\mathbf{\Omega}}{1} r_t )$.
Summarizing, the iteration time complexity of Algorithm~\ref{alg:FaNCLacc} is 
\begin{align}
O( (m + n) k_t^2 + \NM{\mathbf{\Omega}}{1} k_t ).
\label{eq:temp4}
\end{align}
Usually,
$k_t \ll n$ and $\NM{\mathbf{\Omega}}{1} \ll mn$ \cite{candes2009exact,mazumder2010spectral}.
Thus, \eqref{eq:temp4} is much cheaper than
the $O(m n k_t)$ complexity of 
\textsf{FaNCL-acc} on general problems
(Section~\ref{sec:accFaNCL}).

The space complexity is also reduced.
We only need to store the low-rank factorizations of $\X_t$ and $\X_{t-1}$, and the sparse matrices $\SO{\X_t}$ and $\SO{\X_{t-1}}$.
These take a total of $O(  (m + n)k_t + \NM{\mathbf{\Omega}}{1} )$ space (instead of $O(m n)$ in Section~\ref{sec:accFaNCL}).

Note that  these techniques can also be used on 
the simpler non-accelerated \textsf{FaNCL} algorithm (Algorithm~\ref{alg:FaNCL}), 
as discussed in the conference version 
of this paper \cite{yao2015fast}.
It can be
easily shown that its
iteration time complexity is
$O( (m + n) r_t k_t + \NM{\mathbf{\Omega}}{1} r_t )$,
and
its space complexity is $O( (m + n)r_t + \NM{\mathbf{\Omega}}{1})$
(as 
no $\X_{t - 1}$
is involved).

\subsubsection{Comparison with Existing Algorithms}

Table~\ref{tab:timecomp} shows the 
convergence rates,
iteration time complexities, 
and
space complexities of various matrix completion algorithms that
will be empirically compared in Section~\ref{sec:expts}. 
Overall, the proposed 
algorithms (Algorithms~\ref{alg:FaNCL} and \ref{alg:FaNCLacc}) enjoy fast convergence, 
cheap iteration complexity and low memory cost.
While  
Algorithms~\ref{alg:FaNCL} and \ref{alg:FaNCLacc} have
the same
convergence rate,
we will see in Section~\ref{sec:exptmc} that Algorithm~\ref{alg:FaNCLacc} 
(which uses acceleration)
is significantly faster.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Robust Principal Component Analysis (RPCA)}
\label{sec:rpca}

Given a noisy data matrix $\mathbf{O} \in \R^{m \times n}$, RPCA assumes that $\mathbf{O}$ can be approximated by the sum of a
low-rank matrix $\X$
plus some sparse noise $\mathbf{S}$ \cite{candes2011robust}. 
Its optimization problem is:
\begin{equation} \label{eq:rpca}
\min_{\X, \mathbf{S}} F(\X, \mathbf{S}) \equiv f(\X, \mathbf{S}) + \lambda r(\X) + \upsilon g(\mathbf{S}),
\end{equation} 
where 
$f(\X, \mathbf{S}) = \frac{1}{2}\NM{\X + \mathbf{S} - \mathbf{O}}{F}^2$,
$r$ is a low-rank regularizer, and $g$ is a sparsity-inducing regularizer.
Here, we allow both $r$ and $g$ to be nonconvex and nonsmooth. Thus, \eqref{eq:rpca} can be seen as a nonconvex extension of RPCA (which uses
the nuclear norm regularizer 
for $r$ 
and 
$\ell_1$-regularizer
for $g$).
Some examples of nonconvex $r$ are shown in Table~\ref{tab:lwregu},
and examples of nonconvex $g$ include the
$\ell_1$-norm,
capped-$\ell_1$-norm \cite{zhang2010analysis} and log-sum-penalty \cite{candes2008enhancing}.



While \eqref{eq:rpca} involves two blocks of parameters ($\X$ and $\mathbf{S}$), they are
not coupled together.  Thus, we can use the separable property 
of proximal operator
\cite{parikh2014proximal}:
\begin{align}
\Prox{\lambda r + \upsilon g}{[ \X, \mathbf{S} ]}
= 
[ 
\Prox{\lambda r}{\X},
\Prox{\upsilon g}{\mathbf{S}}
].
\label{eq:sep}
\end{align}
For many popular sparsity-inducing regularizers, computing $\Prox{\upsilon g}{\mathbf{S}}$
takes only $O(m n)$ time 
\cite{gongZLHY2013}.  For example, when $g(\mathbf{S}) = \sum_{i,j} |S_{ij}|$, 
$[\Prox{\upsilon g}{\mathbf{S}}]_{ij} = \text{sign}(S_{ij}) \max( |S_{ij}| - \upsilon, 0 )$,
where $\text{sign}(x)$ is the sign of $x$. 
However,
directly computing $\Prox{\lambda r}{\X}$ requires $O(m n^2)$ time and is expensive.
To alleviate this problem,
Algorithm~\ref{alg:FaNCLacc}
can be easily extended
to Algorithm~\ref{alg:FaNCrpca}.
The iteration time complexity, which is dominated by the inexact proximal steps  in
steps~6 and 13, is reduced to $O( m n k_t) $.


\begin{algorithm}[ht]
\caption{\textsf{FaNCL-acc} algorithm for RPCA.}
\begin{algorithmic}[1]
\REQUIRE choose $\tau > \rho, \lambda_0 > \lambda, \delta > 0, \nu \in (0,1)$ and $c_1 = \frac{\tau - \rho}{4}$;
	\STATE initialize $\mathbf{V}_0, \mathbf{V}_1 \in \R^{n}$ as random Gaussian matrices,
	$\X_0 = \X_1 = 0$, $\mathbf{S}_0 = \mathbf{S}_1 = 0$ and $\alpha_0 = \alpha_1 = 1$;
	\FOR{$t = 1,2,\dots T$}
	\STATE $\lambda_t = (\lambda_{t-1} -\lambda) \nu + \lambda$;
	\STATE $\begin{bmatrix} \mathbf{Y}_t^{\mathbf{X}} , \mathbf{Y}_t^{\mathbf{S}}\end{bmatrix}
	=
	\begin{bmatrix} \X_t , \mathbf{S}_t \end{bmatrix}$
	
	\quad $+ \frac{\alpha_{t - 1} - 1}{\alpha_t}\left( \begin{bmatrix} \X_t , \mathbf{S}_t \end{bmatrix} 
	- \begin{bmatrix} \X_{t - 1} , \mathbf{S}_{t - 1} \end{bmatrix}\right)$;
	
	\STATE $\mathbf{R}_t = \QR{[\mathbf{V}_t, \mathbf{V}_{t - 1}]}$; 
	\quad\quad // warm start
	\STATE $\X^a_{t + 1} \! = \textsf{InexactPS}(\mathbf{Y}_t^{\mathbf{X}}, \mathbf{R}_t)$;
	\STATE $\mathbf{S}^a_{t + 1} = \Prox{\frac{\upsilon}{\tau} g}{\mathbf{Y}^{\mathbf{S}}_t - \frac{1}{\tau} \nabla_{\mathbf{S}} \f{\mathbf{Y}_t^{\mathbf{X}}, \mathbf{Y}_t^{\mathbf{S}}}}$;
	
	\STATE $\Delta_t = \NM{\X^a_{t + 1} - \mathbf{Y}_t^{\mathbf{S}}}{F}^2 + \NM{\mathbf{S}^a_{t + 1} - \mathbf{Y}_t^{\mathbf{S}}}{F}^2$;
	
	\IF{$F(\X^a_{t + 1}, \mathbf{S}^a_{t + 1}) \le F(\X_t, \mathbf{S}_t) - \frac{\delta}{2} \Delta_t$}
	\STATE $\X_{t + 1} \! = \X^a_{t + 1}$;
	\STATE $\mathbf{S}_{t + 1} = \mathbf{S}^a_{t + 1}$;
	\ELSE
	
	\STATE $\mathbf{X}_{t + 1} \! = \text{\sf InexactPS}(\X_t, \mathbf{R}_t)$;
	\STATE $\mathbf{S}_{t + 1} = \Prox{\frac{\upsilon}{\tau} g}{\mathbf{S}_t - \frac{1}{\tau} \nabla_{\mathbf{S}} \f{\X_t, \mathbf{S}_t}}$;
	\ENDIF
	\STATE $\alpha_{t + 1} = \frac{1}{2} (\sqrt{4\alpha_t^2 + 1} + 1)$;
	\ENDFOR 
	\RETURN $\X_{T + 1}$ and $\mathbf{S}_{T + 1}$.
\end{algorithmic}
\label{alg:FaNCrpca}
\end{algorithm}

Convergence results 
in Section~\ref{sec:accFaNCL}
can be easily extended to this RPCA problem.
Proofs of the following can be found in Appendices~\ref{app:pr:convrpca},
\ref{app:raterpca}, and
\ref{app:thm:convrpca}.

\begin{prop} \label{pr:convrpca}
	The sequence $\left\lbrace [ \X_t, \mathbf{S}_t ] \right\rbrace$ generated from Algorithm~\ref{alg:FaNCrpca} is bounded,
	and has at least one limit point.
\end{prop}

\begin{corollary} \label{cor:raterpca}
	Let $\mathbf{C}_t = \left[ \mathbf{Y}^{\mathbf{X}}_t, \mathbf{Y}^{\mathbf{S}}_t \right]$ if
	steps~10 and 11 are performed,
	and $\mathbf{C}_t = [\X_t, \mathbf{S}_t]$ otherwise.
	Then, $\min_{t = 1, \dots, T} \NM{[\X_{t + 1}, \mathbf{S}_{t + 1}] - \mathbf{C}_t}{F}^2 \! \le \! \frac{F(\X_1, \mathbf{S}_1) - \inf F}{\min(c_1, \delta/2) T}$. 
\end{corollary}

\begin{theorem} \label{thm:convrpca}
Let $\left\lbrace  [\X_{t_j}, \mathbf{S}_{t_j}] \right\rbrace$ be
a subsequence of $\{[\X_t, \mathbf{S}_t]\}$ generated by
Algorithm~\ref{alg:FaNCrpca} such that
$\lim_{t_j \rightarrow \infty} \X_{t_j} = \X_*$
and $\lim_{t_j \rightarrow \infty} \mathbf{S}_{t_j} = \mathbf{S}_*$
With the assumption in Theorem~\ref{the:FaNCL:conv},
$[\X_*, \mathbf{S}_*]$ is a critical point of \eqref{eq:rpca}.
\end{theorem}

%Using separate property in \eqref{eq:sep},
The non-accelerated \textsf{FaNCL} (Algorithm~\ref{alg:FaNCL}) can be 
similarly
extended for RPCA.
Same to the accelerated \textsf{FaNCL},
its space complexity is also 
$O( m n k_t )$ 
and 
its per-iteration time complexity
is also
$O( m n k_t )$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[ht]
\centering
%\vspace{-5px}
\subfigure[$\mathbf{U}^{\top} \mathbf{X}$.]
{\includegraphics[height = 0.21\textwidth]
	{figures/parallel/UtX-parallel}
	\label{fig:parXV}}
\subfigure[$\mathbf{X} \mathbf{V}$.]
{\includegraphics[height = 0.21\textwidth]
	{figures/parallel/XV-parallel}
	\label{fig:parXU}}
\subfigure[Element-wise operation.]
{\includegraphics[height = 0.21\textwidth]
	{figures/parallel/Elem-parallel}
	\label{fig:elevise}}

\vspace{-10px}
\caption{Parallelization of different matrix operations. Here, the number of
	threads  $q$ is equal to 3. Each dotted path denotes operation of a thread.}
\label{fig:threetype}
\end{figure*}

\section{Parallel \textsf{FaNCL} for Matrix Completion}
\label{sec:parallel}

In this section, we show how the proposed algorithms can be parallelized.
We will only consider the matrix completion problem in \eqref{eq:matcomp}.
Extension to other problems, such as RPCA in Section~\ref{sec:rpca}, can be similarly performed.
Moreover, for simplicity of discussion, we focus on the  simpler
\textsf{FaNCL} algorithm
(Algorithm~\ref{alg:FaNCL}).
Its accelerated variant
(Algorithm~\ref{alg:FaNCLacc}) can be similarly parallelized and is shown in Appendix~C.


Parallel algorithms for matrix completion have been proposed in
\cite{gemulla2011large,yu2012scalable,recht2013parallel}. However,
they are based on stochastic gradient descent and matrix factorization, 
and cannot be directly used here.



\subsection{Proposed Algorithm}
\label{sec:basicidea}

Operations on a matrix $\mathbf{X}$ are often of the form:
(i) multiplications $\mathbf{U}^{\top} \mathbf{X}$ and $\mathbf{X} \mathbf{V}$ for some
$\mathbf{U}, \mathbf{V}$ (e.g., in \eqref{eq:temp2}, \eqref{eq:temp3});
and
(ii) 
element-wise operation (e.g., evaluation of $F(\mathbf{X})$ in 
\eqref{eq:decrease}).
A popular scheme in parallel linear algebra is block distribution \cite{bertsekas1997parallel}.
Assume that there are $q$ threads for parallelization.
Block distribution partitions
the rows and columns of $\mathbf{X}$ into $q$ parts, leading to a total of
$q^2$ blocks.
For Algorithm~\ref{alg:FaNCL}, 
the most important variables are the low-rank factorized form
$\mathbf{U}_t \mathbf{\Sigma}_t \mathbf{V}_t^{\top}$ of $\X_t$, 
and the sparse matrices $\SO{\X_t}, \SO{\mathbf{O}}$.
Using block distribution, they are simply partitioned as in $q^2$ blocks (Figure~\ref{fig:parti}).
Figure~\ref{fig:threetype} shows how computations of $\mathbf{X} \mathbf{V}$,
$\mathbf{U}^{\top} \mathbf{X}$ and element-wise operation can be easily parallelized. 

\begin{figure}[ht]
\centering
\subfigure[$\mathbf{U} \mathbf{\Sigma} \mathbf{V}^{\top}$.]
{\includegraphics[height = 0.145\textwidth]{figures/parallel/partition-lr} \label{fig:partilr} }
\subfigure[$\mathbf{O}$.]
{\includegraphics[height = 0.145\textwidth]{figures/parallel/partition-sp} \label{fig:partisp}}

\vspace{-10px}
\caption{Partitioning of variables $\mathbf{U} \mathbf{\Sigma} \mathbf{V}^{\top}$ and $\mathbf{O}$, 
	and three threads are used ($q = 3$).}
\label{fig:parti}
\end{figure}

The resultant parallelized version of
\textsf{FaNCL} 
is shown in Algorithm~\ref{alg:parallel}.
Steps that can be parallelized 
are marked with ``$\rhd$''.
Two new subroutines are introduced, 
namely,
\textsf{IndeSpan-PL}  
(step~6) which replaces \textsf{QR} factorization, and
\textsf{ApproxGSVT-PL} 
(step~9) which is the parallelized version of Algorithm~\ref{alg:apprGSVT}.
They will be discussed in more detail in the following Sections.
Note that Algorithm~\ref{alg:parallel} is 
equivalent to Algorithm~\ref{alg:FaNCL} except that it is parallelized.
Thus, the
convergence results in Section~\ref{sec:convana} still hold.

\begin{algorithm}[ht]
\caption{\textsf{FaNCL} in parallel: \textsf{FaNCL-PL}.}
\begin{algorithmic}[1]
	\REQUIRE choose $\tau > \rho$, $\lambda_0 > \lambda$ and $\nu \in (0,1)$;
	
	\STATE initialize $\mathbf{V}_0, \mathbf{V}_1 \in \R^{n}$ as random Gaussian matrices and $\X_1 = 0$;
	
	\STATE partition $\X_1$, $\SO{\X_1}$ and $\SO{\mathbf{O}}$;
	
	\STATE start $q$ threads for parallelization;
	
	\FOR{$t = 1,2,\dots T$}
	
	\STATE $\lambda_t = (\lambda_{t-1} -\lambda) \nu^t + \lambda$;
	
	\STATE $\rhd$ $\mathbf{R}_t = \textsf{IndeSpan-PL}\left( [\mathbf{V}_t, \mathbf{V}_{t - 1}] \right)$;
	
	\STATE $\rhd$ $(\X_{\text{gd}})_t = \X_t - \frac{1}{\tau} \SO{\X_t - \mathbf{O}}$;
	
	\FOR{$p = 1, 2,\dots$}
	\STATE $\rhd$ $[ \tilde{\mathbf{X}}_p, \mathbf{R}_t  ]   = 
	\textsf{ApproxGSVT-PL} ( (\X_{\text{gd}})_t, \mathbf{R}_t, \frac{\lambda}{\tau} )$;
	
	\STATE $\rhd$ $a_p = F(  \tilde{\mathbf{X}}_p )$;
	
	\STATE $\rhd$ $a_t \, = F (\X_t)$;
	
	\STATE $\rhd$ $a_F = \| \tilde{\mathbf{X}}_p - \X_t \|_F^2$;
	 
	\STATE \textbf{if} {$a_p \le a_t - c_1 a_F$} break; \textbf{end if}
	
	\ENDFOR
	
	\STATE $\rhd$ $\X_{t + 1} = \tilde{\X}_p$;
	
	\ENDFOR
	
	\RETURN $\X_{T + 1}$.
\end{algorithmic}
\label{alg:parallel}
\end{algorithm}

\subsubsection{Identifying the Span (Step~5)}
\label{sec:qrfact}

In step~4 of Algorithm~\ref{alg:FaNCL},
QR factorization is used 
to find the span of matrix
   $[\mathbf{V}_t, \mathbf{V}_{t - 1}]$.
This can be parallelized with the Householder transformation and Gaussian elimination
\cite{bertsekas1997parallel}, which however
is typically very complex. 
The following Proposition proposes a simpler method to identify the span of a matrix.
%Proof can be found in Appendix~\ref{app:qr}.


\begin{prop} \label{pr:qr}
Given a matrix $\X_{\text{gd}}$,
let the SVD of $\X_{\text{gd}} ^{\top} \X_{\text{gd}}$ be $\mathbf{V} \mathbf{\Sigma}
\mathbf{V}^{\top}$,
$\mathbf{w} = [w_i]$ where $ w_i = \Sigma_{ii}$ if $\Sigma_{ii} > 0$ and $1$ otherwise.
Then, 
$\X_{\text{gd}} \mathbf{V} \left( \Diag{\mathbf{w}} \right)^{-\frac{1}{2}}$ is orthogonal and contains $\Span{\X_{\text{gd}}}$.
\end{prop}

The resultant parallel algorithm is shown in Algorithm~\ref{alg:iden}.
Its time complexity is $O( (\frac{n}{q} + q)k^2 + k^3 )$.
Algorithm~\ref{alg:parallel} calls Algorithm~\ref{alg:iden} 
with 
input
$[\mathbf{V}_t, \mathbf{V}_{t - 1}]$, and
thus takes
$O( (\frac{n}{q} + q)k_t^2 + k_t^3 )$
time,
where $k_t = r_t + r_{t - 1}$.   
We do not parallelize steps~2-4,
	as only 
	$k \times k$ 
	matrices 
	are involved and $k$ is small. Moreover,
though step~3 uses SVD, it only takes $O(k^3)$ time.

\begin{algorithm}[ht]
\caption{Parallel algorithm to identify the span of $\X_{\text{gd}}$: $\textsf{IndeSpan-PL}(\X_{\text{gd}})$.}
\begin{algorithmic}[1]
	\REQUIRE matrix $\X_{\text{gd}} \in \R^{n \times k}$;
	\STATE $\rhd$ $\mathbf{B} = \X_{\text{gd}}^{\top} \X_{\text{gd}}$;
	\STATE $[\mathbf{U}, \mathbf{\Sigma}, \mathbf{V}] = \textsf{SVD}(\mathbf{B})$; 
	\STATE construct $\mathbf{w}$ as in Proposition~\ref{pr:qr};
	\STATE $\mathbf{V} = \mathbf{V} \Diag{\mathbf{w}}$;
	\STATE $\rhd$ $\mathbf{Q} = \X_{\text{gd}} \mathbf{V} $;
	\RETURN $\mathbf{Q}$. // $\mathbf{Q} = [\mathbf{Q}_1^{\top}, \dots, \mathbf{Q}_p^{\top}]^{\top}$
\end{algorithmic}
\label{alg:iden}
\end{algorithm} 

\subsubsection{Approximate GSVT (Step~8)}
\label{sec:inexactps}

The key steps in approximate GSVT (Algorithm~\ref{alg:apprGSVT}) are the power method and SVD.
The power method can be parallelized straightforwardly as in
Algorithm~\ref{alg:powermethodPL}, in which we also replace the \textsf{QR} subroutine with Algorithm~\ref{alg:iden}.

\begin{algorithm}[ht]
\caption{Parallel power method:
		\textsf{Powermethod-PL}$(\mathbf{Z}, \mathbf{R})$.}
	\begin{algorithmic}[1]
		\REQUIRE matrix $\mathbf{Z} \in \R^{m \times n}$, $\mathbf{R} \in \R^{n \times k}$.
		\STATE $\rhd$ $\mathbf{Y}_1 = \mathbf{Z} \mathbf{R}$;
		\FOR{$j = 1, 2, \dots, J$}
		\STATE $\rhd$ $\mathbf{Q}_{j} = \textsf{IndeSpan-PL}(\mathbf{Y}_j)$;  
		\STATE $\rhd$ $\mathbf{Y}_{j + 1} = \mathbf{Z} (\mathbf{Z}^{\top} \mathbf{Q}_{j})$;
		\ENDFOR
		\RETURN $\mathbf{Q}_{J}$. 
	\end{algorithmic}
	\label{alg:powermethodPL}
\end{algorithm}

As for SVD, multiple QR factorizations are usually needed for parallelization \cite{bertsekas1997parallel}, 
	which are complex as discussed in Section~\ref{sec:qrfact}.
The following Proposition performs it in a simpler manner.
%Proof can be found in Appendix~\ref{app:rsvd}.




\begin{prop} \label{pr:rsvd}
Given 
a matrix $\mathbf{B} \in \R^{n \times k}$,
let $\mathbf{P}  \in \R^{n \times k}$ be orthogonal and equals $\Span{\mathbf{B}}$, 
and the SVD of $\mathbf{P}^{\top} \mathbf{B}$ be $\mathbf{U} \mathbf{\Sigma} \mathbf{V}^{\top}$.
Then, 
the SVD of $\mathbf{B}$ is
$(\mathbf{P} \mathbf{U})\mathbf{\Sigma}\mathbf{V}$.
\end{prop}

The resultant parallelized procedure for approximate GSVT is shown in
Algorithm~\ref{alg:apprGSVTPl}.
At step~5,
a small  SVD is performed (by a single thread)
on  the $k \times k$ matrix
	$\mathbf{P}^{\top} \mathbf{B}$.
At step~8 of Algorithm~\ref{alg:parallel},
$\tilde{\X}_p$ is returned from Algorithm~\ref{alg:apprGSVTPl},
and we keep $\tilde{\X}_p$ in its low-rank factorized form.
Besides, when Algorithm~\ref{alg:apprGSVTPl} is called,
$\mathbf{Z} = (\X_{\text{gd}})_t$
and has the ``sparse plus low-rank'' structure mentioned earlier. Hence, \eqref{eq:temp2} and \eqref{eq:temp3} can be used to speed up
matrix multiplications\footnote{As no acceleration is used, $\beta_t$ 
in \eqref{eq:temp2} and \eqref{eq:temp3} 
is equal to zero
in these two equations.}.
As $\mathbf{R}_t$ has $k_t$ columns in Algorithm~\ref{alg:parallel},
\textsf{PowerMethod-PL} in step~1 takes
$O( \frac{k_t}{q} \NM{\mathbf{\Omega}}{1} + \frac{m + n}{q} k_t^2 )$ time, 
steps~2-6 take $O( (\frac{n}{q} + q)k_t^2 + k_t^3 )$ time,
and the rest takes $O(k_t)$ time.
The total time complexity 
for Algorithm~\ref{alg:apprGSVTPl} 
is 
$O( \frac{k_t}{q} \NM{\mathbf{\Omega}}{1} + \frac{m + n}{q} k_t^2 
+ (q + k_t)k_t^2 )$.


\begin{algorithm}[ht]
\caption{Approximate GSVT in parallel:
	\textsf{ApproxGSVT-PL}$(\mathbf{Z}, \mathbf{R}, \mu)$.}
\begin{algorithmic}[1]
	\REQUIRE partitioned matrix $\mathbf{Z} \in \R^{m \times n}$ and $\mathbf{R} \in \R^{n \times k}$;
	
	\STATE $\rhd$ $\mathbf{Q} = \text{\sf PowerMethod-PL}(\mathbf{Z}, \mathbf{R})$;
	
	\STATE $\rhd$ $\mathbf{B} = \mathbf{Z}^{\top} \mathbf{Q}$; 
	// $\mathbf{B} \in \R^{n \times k}$
	
	\STATE $\rhd$ $\mathbf{P} = \textsf{Iden-Span}(\mathbf{B})$;
	
	\STATE $\rhd$ $\X_{\text{gd}} = \mathbf{P}^{\top} \mathbf{B}$;
	
	\STATE $[\mathbf{U}, \mathbf{\Sigma}, \mathbf{V}] = \textsf{SVD}(\X_{\text{gd}})$;
	// $\mathbf{U}, \mathbf{\Sigma}, \mathbf{V}, \X_{\text{gd}} \in \R^{k \times k}$
	
	\STATE $\rhd$ $\mathbf{U} = \mathbf{P} \mathbf{U}$;
	
	\STATE $a = $ number of $\Sigma_{ii}$'s that are $>\gamma$ in Corollary~\ref{cor:threh};
	
	\STATE $\rhd$ $\mathbf{U}_a = a$ leading columns of $\mathbf{U}$;
	
	\STATE $\rhd$ $\mathbf{V}_a = a$ leading columns of $\mathbf{V}$;
	
	\FOR{$i = 1,2,\dots,a$}
	\STATE obtain $y^*_i$ from \eqref{eq:proRed};
	\ENDFOR
	
	\RETURN  
	the low-rank components of $\tilde{\mathbf{X}}$
	($\mathbf{Q} \mathbf{U}_a$, $\Diag{[ y_1^*,\dots,y_{a}^* ]}$ and $\mathbf{V}_a^{\top}$),
	and $\mathbf{V}$.
\end{algorithmic}
\label{alg:apprGSVTPl}
\end{algorithm}

\subsubsection{Checking of Objectives (steps~9-11)}
\label{sec:chkobj}

As shown in Figures~\ref{fig:elevise},
computation of $\NM{\SO{\mathbf{X}_t - \mathbf{O}}}{F}^2$ in $F(\cdot)$
can be directly parallelized and takes 
$O(\frac{1}{p}\NM{\mathbf{\Omega}}{1})$ time.
As $r$ only relies on $\mathbf{\Sigma}_t$, 
only one thread is needed to evaluate $r(\X_t)$.
Thus, computing $F(\X_t)$ takes $O( \frac{r_t}{q} \NM{\mathbf{\Omega}}{1} )$ time.
Similarly,
computing $F( \tilde{\mathbf{X}}_p )$
takes $O( \frac{k_t}{q} \NM{\mathbf{\Omega}}{1} )$ time.
As $\NM{\tilde{\X}_p - \mathbf{X}_t}{F}^2 
= \Tr{\tilde{\X}_p^{\top} \tilde{\X}_p - 2 \tilde{\mathbf{X}}_p^{\top} \mathbf{X}_t -
\mathbf{X}_t^{\top} \mathbf{X}_t}$, the low-rank factorized forms of $\tilde{\mathbf{X}}_p$ and $\mathbf{X}_t$ can be utilized.
From Figures~\ref{fig:parXV} and 
\ref{fig:parXU}, 
it can be performed in $O(\frac{m + n}{q} k_t^2 )$ time.
The time complexity for steps~9-11 in Algorithm~\ref{alg:parallel} is
$O(\frac{k_t}{q}\NM{\mathbf{\Omega}}{1}
+ \frac{m + n}{q}k_t^2)$.
The iteration time complexity for Algorithm~\ref{alg:parallel} is thus
$O( 
\frac{1}{q}((m + n) k_t^2
+ 
\NM{\mathbf{\Omega}}{1} k_t)
+ 
(q + k_t) k_t^2)$.
Compared with \eqref{eq:temp4},
the speedup w.r.t. the number of threads $q$
is almost linear.

\begin{table*}[ht]
	\centering
	%\vspace{-5px}
	\caption{$\!\!$ Matrix completion performance on the synthetic data.
		NMSE is scaled by $10^{-2}$, CPU time is in seconds and the number in brackets is data sparsity.}
	\vspace{-10px}
	\begin{tabular}{cc|ccc|ccc|ccc}
		\hline
		&                     &       \multicolumn{3}{c|}{$m=500$ ($12.43\%$)}       &       \multicolumn{3}{c|}{$m=1000$ ($6.91\%$)}       &       \multicolumn{3}{c}{$m=2000$ ($3.80\%$)}        \\
		&                     &          NMSE          & rank &         time         &          NMSE          & rank &         time         &          NMSE          & rank &         time         \\ \hline
		nuclear norm   &    \textsf{APG}     &     4.26$\pm$0.01      &  50  &     12.6$\pm$0.7     &     4.27$\pm$0.01      &  61  &     99.6$\pm$9.1     &     4.13$\pm$0.01      &  77  &   1177.5$\pm$134.2   \\ \cline{2-11}
		& \textsf{AIS-Impute} &     4.11$\pm$0.01      &  55  &     5.8$\pm$2.9      &     4.01$\pm$0.03      &  57  &     37.9$\pm$2.9     &     3.50$\pm$0.01      &  65  &    338.1$\pm$54.1    \\ \cline{2-11}
		&   \textsf{active}   &     5.37$\pm$0.03      &  53  &     12.5$\pm$1.0     &     6.63$\pm$0.03      &  69  &     66.4$\pm$3.3     &     6.44$\pm$0.10      &  85  &    547.3$\pm$91.6    \\ \hline\hline
		fixed rank    &   \textsf{LMaFit}   &     3.08$\pm$0.02      &  5   &     0.5$\pm$0.1      &     3.02$\pm$0.02      &  5   &     1.3$\pm$0.1      &     2.84$\pm$0.03      &  5   &     4.9$\pm$0.3      \\ \cline{2-11}
		&   \textsf{ER1MP}    &     21.75$\pm$0.05     &  40  &     0.3$\pm$0.1      &     21.94$\pm$0.09     &  54  &     0.8$\pm$0.1      &     20.38$\pm$0.06     &  70  &     2.5$\pm$0.3      \\ \hline\hline
		capped $\ell_1$ &    \textsf{IRNN}    & \textbf{1.98$\pm$0.01} &  5   &     14.5$\pm$0.7     &     1.99$\pm$0.01      &  5   &    146.0$\pm$2.6     & \textbf{1.79$\pm$0.01} &  5   &   2759.9$\pm$252.8   \\ \cline{2-11}
		&    \textsf{GPG}     & \textbf{1.98$\pm$0.01} &  5   &     14.8$\pm$0.9     &     1.99$\pm$0.01      &  5   &    144.6$\pm$3.1     & \textbf{1.79$\pm$0.01} &  5   &   2644.9$\pm$358.0   \\ \cline{2-11}
		&   \textsf{FaNCL}    & \textbf{1.97$\pm$0.01} &  5   &     0.3$\pm$0.1      &     1.98$\pm$0.01      &  5   &     1.0$\pm$0.1      & \textbf{1.79$\pm$0.01} &  5   &     5.0$\pm$0.4      \\ \cline{2-11}
		& \textsf{FaNCL-acc}  & \textbf{1.97$\pm$0.01} &  5   & \textbf{0.1$\pm$0.1} & \textbf{1.95$\pm$0.01} &  5   & \textbf{0.5$\pm$0.1} & \textbf{1.78$\pm$0.01} &      & \textbf{2.3$\pm$0.2} \\ \hline
		LSP       &    \textsf{IRNN}    & \textbf{1.96$\pm$0.01} &  5   &     16.8$\pm$0.6     & \textbf{1.89$\pm$0.01} &  5   &    196.1$\pm$3.9     & \textbf{1.79$\pm$0.01} &  5   &   2951.7$\pm$361.3   \\ \cline{2-11}
		&    \textsf{GPG}     & \textbf{1.96$\pm$0.01} &  5   &     16.5$\pm$0.4     & \textbf{1.89$\pm$0.01} &  5   &    193.4$\pm$2.1     & \textbf{1.79$\pm$0.01} &  5   &   2908.9$\pm$358.0   \\ \cline{2-11}
		&   \textsf{FaNCL}    & \textbf{1.96$\pm$0.01} &  5   &     0.4$\pm$0.1      & \textbf{1.89$\pm$0.01} &  5   &     1.3$\pm$0.1      & \textbf{1.79$\pm$0.01} &  5   &     5.5$\pm$0.4      \\ \cline{2-11}
		& \textsf{FaNCL-acc}  & \textbf{1.96$\pm$0.01} &  5   & \textbf{0.2$\pm$0.1} & \textbf{1.89$\pm$0.01} &  5   & \textbf{0.7$\pm$0.1} & \textbf{1.77$\pm$0.01} &      & \textbf{2.4$\pm$0.2} \\ \hline
		TNN       &    \textsf{IRNN}    & \textbf{1.96$\pm$0.01} &  5   &     18.8$\pm$0.6     & \textbf{1.88$\pm$0.01} &  5   &    223.1$\pm$4.9     & \textbf{1.77$\pm$0.01} &  5   &   3220.3$\pm$379.7   \\ \cline{2-11}
		&    \textsf{GPG}     & \textbf{1.96$\pm$0.01} &  5   &     18.0$\pm$0.6     & \textbf{1.88$\pm$0.01} &  5   &    220.9$\pm$4.5     & \textbf{1.77$\pm$0.01} &  5   &   3197.8$\pm$368.9   \\ \cline{2-11}
		&   \textsf{FaNCL}    & \textbf{1.95$\pm$0.01} &  5   &     0.4$\pm$0.1      & \textbf{1.88$\pm$0.01} &  5   &     1.4$\pm$0.1      & \textbf{1.77$\pm$0.01} &  5   &     6.1$\pm$0.5      \\ \cline{2-11}
		& \textsf{FaNCL-acc}  & \textbf{1.96$\pm$0.01} &  5   & \textbf{0.2$\pm$0.1} & \textbf{1.88$\pm$0.01} &  5   & \textbf{0.8$\pm$0.1} & \textbf{1.77$\pm$0.01} &      & \textbf{2.9$\pm$0.2} \\ \hline
	\end{tabular}
	\label{tab:sythmatcomp}
	%\vspace{-10px}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{sec:expts}

In this section, we perform experiments on matrix completion, RPCA  
and the parallelized variant of Algorithm~\ref{alg:FaNCLacc}.
We use a Windows server 2013 system with Intel Xeon E5-2695-v2 CPU (12 cores, 2.4GHz) and 256GB memory. 
All the
algorithms in Sections~\ref{sec:exptmc} and \ref{sec:expt2} are implemented in Matlab.
For Section~\ref{sec:expparallel},
we use C++, the Intel-MKL package
%\footnote{\url{https://software.intel.com/en-us/intel-mkl}}
for matrix operations, and the standard thread library
%\footnote{\url{http://www.cplusplus.com/reference/thread/thread/}}
for multi-thread programming.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Matrix Completion}
\label{sec:exptmc}

We compare a number of low-rank matrix completion solvers, including
models based on 
(i) the commonly used (convex) nuclear norm regularizer; 
(ii) fixed-rank factorization models \cite{wen2012solving,wang2015orthogonal},
which decompose the  observed matrix $\mathbf{O}$ into a product of 
rank-$k$ matrices $\mathbf{U}$ and $\mathbf{V}$. Its optimization problem can be written as:
$\min_{U,V}\frac{1}{2}\NM{\SO{\mathbf{U} \mathbf{V} - \mathbf{O}}}{F}^2+ \frac{\lambda}{2}(\NM{\mathbf{U}}{F}^2 + \NM{\mathbf{V}}{F}^2)$;
and 
(iii) nonconvex regularizers, including the capped-$\ell_1$ (with $\theta$ in
Table~\ref{tab:lwregu} set to $2 \lambda$), LSP
(with $\theta = \sqrt{\lambda}$), and TNN
(with $\theta = 3$).

The nuclear norm minimization algorithms to be compared include:
(i) Accelerated proximal gradient (\textsf{APG}) algorithm \cite{ji2009accelerated,toh2010accelerated}, 
with the partial SVD computed by the PROPACK package \cite{larsen1998lanczos};
(ii) \textsf{AIS-Impute} \cite{mazumder2010spectral}, an inexact and accelerated proximal algorithm.
The ``sparse plus low-rank'' structure of  the
matrix iterate is utilized to speed up computation (Section~\ref{sec:matcomp}); and
(iii) Active subspace selection 
%	\footnote{\url{http://www.cs.utexas.edu/~cjhsieh/nuclear_active_1.1.zip}}
(denoted ``\textsf{active}'') \cite{hsieh2014nuclear}, which adds/removes rank-one subspaces from the active set in each iteration.  
%The nuclear norm optimization problem is then reduced to a smaller problem defined only on this active set.
%We do not compare
%with the Frank-Wolfe algorithm \cite{zhang2012accelerated} and stochastic gradient descent \cite{avron2012efficient},
as they have been shown to be less efficient 
\cite{hsieh2014nuclear,quan2015impute}.
For fixed-rank factorization models
(where the rank is tuned by the validation set),
we compare with the two state-of-the-art algorithms:
(i) Low-rank matrix fitting (\textsf{LMaFit})
	algorithm
%	\footnote{\url{http://www.caam.rice.edu/~optimization/L1/LMaFit/download.html}}
	\cite{wen2012solving}; and
(ii) Economical rank-one matrix pursuit (\textsf{ER1MP}) \cite{wang2015orthogonal},
	which pursues a rank-one basis in each iteration.
We do not compare with the concave-convex procedure \cite{zhang2010analysis,hu2013fast}, since
it has been shown to be inferior to \textsf{IRNN} \cite{gongZLHY2013}.
For models with nonconvex low-rank regularizers, 
we compare with the following solvers:
(i) Iterative reweighted nuclear norm (\textsf{IRNN})	\cite{lu2016nonconvex};
(ii) Generalized proximal gradient (\textsf{GPG}) algorithm \cite{lu2015generalized}, with the
	underlying problem (\ref{eq:proRed}) solved using the closed-form solutions
	in \cite{gongZLHY2013}; and
(iii) The proposed \textsf{FaNCL} algorithm (Algorithm~\ref{alg:FaNCL}) and its
	accelerated variant \textsf{\textsf{FaNCL}-acc} 
	(Algorithm~\ref{alg:FaNCLacc}).
	We set $J = 3$ and $p = 1$.

All the algorithms are  stopped when the relative difference
in objective values between consecutive iterations is smaller than 
${10}^{-4}$.





\subsubsection{Synthetic Data}
\label{sec:matcomp:syn}

The observed $m\times m$ matrix is generated as 
$\mathbf{O} = \mathbf{U} \mathbf{V} + \mathbf{G}$,  where 
the elements 
of $\mathbf{U} \in \R^{m \times k}, \mathbf{V} \in \R^{k \times m}$ 
(with $k = 5$) are 
sampled i.i.d. from the standard normal distribution $\mathcal{N}(0,
1)$, and
elements of $\mathbf{G}$ sampled from $\mathcal{N}(0, 0.1)$.
A total of $\NM{\mathbf{\Omega}}{1} = 2 m k \log(m)$ random elements
in $\mathbf{O}$ are observed.  
Half of them are used for training, and the rest as validation
set for parameter tuning.
Testing is performed on the unobserved elements.


For performance evaluation,  
we use (i) the normalized mean squared error:
$\text{NMSE} = \NM{P_{\mathbf{\Omega}^{\bot}}(\mathbf{X} - \mathbf{U V})}{F} 
/ \NM{P_{\mathbf{\Omega}^{\bot}}(\mathbf{U V})}{F}$,
where $\mathbf{X}$ is the recovered matrix and
$\mathbf{\Omega}^{\bot}$
%= \mathbf{1} - \mathbf{\Omega}$ 
denotes the unobserved positions;
(ii) rank of $\mathbf{X}$;
and 
(iii) training CPU time.
We vary $m$ in $\{500, 1000, 2000\}$.
Each experiment is repeated five times.

Results
are shown in 
\footnote{For all tables in the sequel,
	the best and comparable results (according to the pairwise t-test with 95\% confidence) are highlighted.}
Table~\ref{tab:sythmatcomp}.
As can be seen, nonconvex regularization (capped-$\ell_1$, LSP and TNN) leads to much lower NMSE's than 
convex nuclear norm regularization and fixed-rank factorization.
Moreover, 
%as is also observed in \cite{mazumder2010spectral,avron2012efficient}, 
nuclear norm regularization
and \textsf{ER1MP} produce much higher ranks.
%Among the nonconvex solvers, \textsf{ER1MP} outputs a much higher rank.
In terms of speed
among the nonconvex low-rank solvers,
\textsf{FaNCL} is faster than 
\textsf{GPG} and \textsf{IRNN},
while \textsf{FaNCL-acc} is even faster. Moreover,
the larger the matrix, the higher are the speedups of \textsf{FaNCL} and \textsf{FaNCL-acc}
over \textsf{GPG} and \textsf{IRNN}.


Next,
we demonstrate 
the ability 
of \textsf{FaNCL} and \textsf{FaNCL-acc}
in maintaining low-rank iterates.
Figure~\ref{fig:rankmatcomp} shows 
$k$
(the rank
of $\mathbf{R}_t$)
and 
$\hat{k}_{\mathbf{X}_+}$
(the rank of $\X_{t + 1}$)
%$k$ and $\hat{k}_{\X_{\text{gd}}}$ (as defined in Proposition~\ref{pr:apprGSVT})
vs the number of iterations for $m=500$.
%In \textsf{FaNCL/FaNCL-acc}, $k$ is the rank of $\mathbf{R}_t$ and $\hat{k}_{\mathbf{X}_+}$ represents the rank of $\X_{t + 1}$.
As can be seen, $k \ge \hat{k}_{\X_{\text{gd}}}$, which agrees with the assumption in 
Proposition~\ref{pr:apprGSVT}.
Besides,
as \textsf{FaNCL/FaNCL-acc} converges,
$k$ and $\hat{k}_{\X_{\text{gd}}}$ gradually converge to the same value. Moreover,
recall that the data matrix is of size $500 \times 500$.
Hence, the ranks of the iterates obtained by both algorithms are low.

\begin{figure}[ht]
\centering
%\vspace{-5px}
\subfigure[capped-$\ell_1$.]{\includegraphics[width =
	0.24\textwidth]{figures/rank/syn-500-cap}}
\subfigure[LSP.]{\includegraphics[width =
	0.24\textwidth]{figures/rank/syn-500-lsp}}
%\qquad
%\subfigure[TNN.]{\includegraphics[width =
%	0.24\textwidth]{figures/rank/syn-500-tnn}}

\vspace{-10px}
\caption{$k$ and $\hat{k}_{\X_{\text{gd}}}$ 
	vs the number of iterations 
	on the synthetic data set with $m = 500$.
	The plot of TNN is similar and thus not shown.}
\label{fig:rankmatcomp}
%\vspace{-10px}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Recommendation Data sets}
\label{sec:recsys}

\noindent
\textbf{MovieLens}:
First, we perform
experiments on the popular
\textit{MovieLens}
%\footnote{\url{http://grouplens.org/datasets/movielens/}} 
data set
(Table~\ref{tab:recSys}),
which contain ratings of different users on movies.
We follow the setup in \cite{wang2015orthogonal},  
and use $50\%$ of the observed ratings for training, $25\%$ for validation and the rest for testing.
For performance evaluation, we use the root mean squared error on the test set
$\bar{\mathbf{\Omega}}$:
$\text{RMSE} = \sqrt{ \NM{\mathcal{P}_{\bar{\mathbf{\Omega}}}(\mathbf{X} - \mathbf{O})}{F}^2 / \NM{\mathbf{\bar{\Omega}}}{1}}$,
where $\mathbf{X}$ is the recovered matrix.
The experiment is repeated five times.

\begin{table}[ht]
\centering
\caption{Recommendation data sets used in the experiments.}
\vspace{-10px}
\begin{tabular}{cc | c | c | c }
\hline
                &                  & \#users & \#movies & \#ratings   \\ \hline
\textit{MovieLens} &  \textit{100K}   & 943     & 1,682    & 100,000     \\ \cline{2-5}
                &   \textit{1M}    & 6,040   & 3,449    & 999,714     \\ \cline{2-5}
                &   \textit{10M}   & 69,878  & 10,677   & 10,000,054  \\ \hline
\multicolumn{2}{c|}{\textit{netflix}} & 480,189 & 17,770   & 100,480,507 \\ \hline
\multicolumn{2}{c|}{\textit{yahoo}}  & 249,012 & 296,111  & 62,551,438  \\ \hline
\end{tabular}
\label{tab:recSys}
%\vspace{-10px}
\end{table}

Results are shown in Table~\ref{tab:movielen}. 
Again, nonconvex regularizers lead to the lowest RMSE's. 
Moreover, \textsf{FaNCL-acc}
is also the fastest among nonconvex low-rank solvers,
even faster than the state-of-the-art 
\textsf{GPG}.
In particular, 
\textsf{FaNCL} and its accelerated variant \textsf{FaNCL-acc}
are the only solvers (for nonconvex regularization) 
that can be run on the
\textit{MovieLens-1M} and \textit{10M} data sets.
Figure~\ref{fig:movielen:obj} compares the objectives vs CPU time 
for the nonconvex regularization solvers
on \textit{MovieLens-100K}.
As can be seen,
\textsf{FaNCL} and \textsf{FaNCL-acc} decrease the objective and RMSE much faster than the others.
Figure~\ref{fig:movie:rmse} 
shows the testing RMSEs on \textit{MovieLens-1M} and \textit{10M}.
As can be seen, \textsf{FaNCL-acc} is the fastest.
%\footnote{On these two
%	data sets, \textsf{ER1MP} easily overfits as the rank increases. Hence, the validation set
%	selects a smaller rank (relative to that obtained by the nuclear norm) and \textsf{ER1MP} stops
%	earlier. However, as can be seen, its RMSE is much worse.}

\begin{table*}[ht]
\centering
%\vspace{-5px}
\caption{Matrix completion results on the \textit{MovieLens} data sets.
	Methods are too slow to run are indicated as ``---''.
%The best results (according to the pairwise t-test with 95\% confidence) are highlighted.
}
\vspace{-10px}

\label{tab:movielen}
\begin{tabular}{cc|cc|cc|cc|cc|cc}
	\hline
	         &                     & \multicolumn{2}{c|}{\textit{MovieLens-100K}} & \multicolumn{2}{c|}{\textit{MovieLens-1M}} & \multicolumn{2}{c|}{\textit{MovieLens-10M}} & \multicolumn{2}{c}{\textit{netflix}} & \multicolumn{2}{c}{\textit{yahoo}} \\
	         &                     &           RMSE           &       rank        &           RMSE           &      rank       &           RMSE           &       rank       &           RMSE           &   rank    &           RMSE           &  rank   \\ \hline
	nuclear  &    \textsf{APG}     &     0.877$\pm$0.001      &        36         &     0.818$\pm$0.001      &       67        &           ---            &       ---        &           ---            &    ---    &           ---            &   ---   \\ \cline{2-12}
	  norm   & \textsf{AIS-Impute} &     0.878$\pm$0.002      &        36         &     0.819$\pm$0.001      &       67        &     0.813$\pm$0.001      &       100        &           ---            &    ---    &           ---            &   ---   \\ \cline{2-12}
	         &   \textsf{active}   &     0.878$\pm$0.001      &        36         &     0.820$\pm$0.001      &       67        &     0.814$\pm$0.001      &       100        &           ---            &    ---    &           ---            &   ---   \\ \hline\hline
	 fixed   &   \textsf{LMaFit}   &     0.865$\pm$0.002      &         2         &     0.806$\pm$0.003      &        6        &     0.792$\pm$0.001      &        9         &     0.811$\pm$0.001      &    15     &     0.666$\pm$0.001      &   10    \\ \cline{2-12}
	  rank   &   \textsf{ER1MP}    &     0.917$\pm$0.003      &         5         &     0.853$\pm$0.001      &       13        &     0.852$\pm$0.002      &        22        &     0.862$\pm$0.006      &    25     &     0.810$\pm$0.003      &   77    \\ \hline\hline
	 capped  &    \textsf{IRNN}    & \textbf{0.854$\pm$0.003} &         3         &           ---            &       ---       &           ---            &       ---        &           ---            &    ---    &           ---            &   ---   \\ \cline{2-12}
	$\ell_1$ &    \textsf{GPG}     & \textbf{0.855$\pm$0.002} &         3         &           ---            &       ---       &           ---            &       ---        &           ---            &    ---    &           ---            &   ---   \\ \cline{2-12}
	         &   \textsf{FaNCL}    & \textbf{0.855$\pm$0.003} &         3         & \textbf{0.788$\pm$0.002} &        5        &     0.783$\pm$0.001      &        8         &     0.798$\pm$0.001      &    13     &     0.656$\pm$0.001      &    8    \\ \cline{2-12}
	         & \textsf{FaNCL-acc}  &     0.860$\pm$0.009      &         3         &     0.791$\pm$0.001      &        5        & \textbf{0.778$\pm$0.001} &        8         &     0.795$\pm$0.001      &    13     &     0.651$\pm$0.001      &    8    \\ \hline
	  LSP    &    \textsf{IRNN}    &     0.856$\pm$0.001      &         2         &           ---            &       ---       &           ---            &       ---        &           ---            &    ---    &           ---            &   ---   \\ \cline{2-12}
	         &    \textsf{GPG}     &     0.856$\pm$0.001      &         2         &           ---            &       ---       &           ---            &       ---        &           ---            &    ---    &           ---            &   ---   \\ \cline{2-12}
	         &   \textsf{FaNCL}    &     0.856$\pm$0.001      &         2         & \textbf{0.786$\pm$0.001} &        5        & \textbf{0.779$\pm$0.001} &        9         &     0.794$\pm$0.001      &    15     &     0.652$\pm$0.001      &    9    \\ \cline{2-12}
	         & \textsf{FaNCL-acc}  & \textbf{0.853$\pm$0.001} &         2         &     0.787$\pm$0.001      &        5        & \textbf{0.779$\pm$0.001} &        9         & \textbf{0.792$\pm$0.001} &    15     & \textbf{0.650$\pm$0.001} &    8    \\ \hline
	  TNN    &    \textsf{IRNN}    & \textbf{0.854$\pm$0.004} &         3         &           ---            &       ---       &           ---            &       ---        &           ---            &    ---    &           ---            &   ---   \\ \cline{2-12}
	         &    \textsf{GPG}     & \textbf{0.853$\pm$0.005} &         3         &           ---            &       ---       &           ---            &       ---        &           ---            &    ---    &           ---            &   ---   \\ \cline{2-12}
	         &   \textsf{FaNCL}    &     0.865$\pm$0.016      &         3         & \textbf{0.786$\pm$0.001} &        5        &     0.780$\pm$0.001      &        8         &     0.797$\pm$0.001      &    13     &     0.657$\pm$0.001      &    7    \\ \cline{2-12}
	         & \textsf{FaNCL-acc}  &     0.861$\pm$0.009      &         3         & \textbf{0.786$\pm$0.001} &        5        & \textbf{0.778$\pm$0.001} &        9         &     0.795$\pm$0.001      &    13     & \textbf{0.650$\pm$0.001} &    7    \\ \hline
\end{tabular}
%\vspace{-10px}
\end{table*}

\begin{figure}[ht]
\centering
%\vspace{-5px}
\subfigure[capped-$\ell_1$. \label{fig:cap}]
{\includegraphics[width = 0.24\textwidth]{figures/recsys/100K-cap}}
\subfigure[LSP. \label{fig:lsp}]
{\includegraphics[width = 0.24\textwidth]{figures/recsys/100K-lsp}}

\vspace{-10px}
\caption{Objective vs CPU time for the capped-$\ell_1$ and LSP on 
\textit{MovieLens-100K}.  The plot of TNN is similar and thus not shown.}
\label{fig:movielen:obj}
%\vspace{-5px}
\end{figure}

\begin{figure}[ht]
\centering

\subfigure[\textit{1M}.]
{\includegraphics[width = 0.24 \textwidth]{figures/recsys/10M-all}}
\subfigure[\textit{10M}.]
{\includegraphics[width = 0.24 \textwidth]{figures/recsys/10M-all}}

\vspace{-10px}
\caption{RMSE vs CPU time on the \textit{MovieLens-1M} and \textit{10M} data sets.}
\label{fig:movie:rmse}
%\vspace{-5px}
\end{figure}

Figure~\ref{fig:rankmatcomp2} shows the ranks $k$ and $\hat{k}_{\X_{\text{gd}}}$ (as defined in Proposition~\ref{pr:apprGSVT}) vs the number of iterations on the \textit{MovieLens-100K} data set.  Recall that the data matrix is of size $943 \times 1682$.  Again, $k \ge \hat{k}_{\X_{\text{gd}}}$ and the ranks of the iterates obtained by both algorithms are low.

\begin{figure}[ht]
	\centering
	\subfigure[capped-$\ell_1$.]
	{\includegraphics[width = 0.24\textwidth]{figures/rank/100K-cap}}
	\subfigure[LSP.]
	{\includegraphics[width = 0.24\textwidth]{figures/rank/100K-lsp}}
%	\qquad
%	\subfigure[TNN.]
%	{\includegraphics[width = 0.24\textwidth]{figures/rank/100K-tnn}}
	
	\vspace{-10px}
	\caption{$k$ and $\hat{k}_{\X_{\text{gd}}}$ 
		vs the number of iterations 
		on the 
		\textit{MovieLens-100K} data set.
	The plot of TNN is similar and thus not shown.}
	\label{fig:rankmatcomp2}
	%\vspace{-15px}
\end{figure}

\noindent
\textbf{Netflix and Yahoo}:
Next, we perform experiments on two very large recommendation
data sets,
\textit{Netflix}
%\footnote{\url{http://archive.ics.uci.edu/ml/datasets/Netflix+Prize}} 
and \textit{Yahoo}
%\footnote{\url{http://webscope.sandbox.yahoo.com/catalog.php?datatype=c}}
(Table~\ref{tab:recSys}).
We randomly use $50\%$ 
of the observed ratings 
for training,
$25\%$ for validation and the rest for testing.
Each experiment is repeated five times. 

Results are shown in Table~\ref{tab:movielen}.
\textsf{APG}, \textsf{GPG} and \textsf{IRNN} cannot be run as the
data set
is large.
From Section~\ref{sec:recsys},
\textsf{AIS-Impute} has similar running time as \textsf{LMaFit} but inferior
performance,
and thus is not compared.
Again, the nonconvex regularizers converge faster, yield lower RMSE's and solutions of much
lower ranks.
Figure~\ref{fig:large:rmse} shows the RMSE vs time,
and \textsf{FaNCL-acc} is the fastest.



\begin{figure}[ht]
\centering
%\vspace{-5px}
\subfigure[\textit{netflix}.]
{\includegraphics[width = 0.24 \textwidth]{figures/recsys/netflix-all}}
\subfigure[\textit{yahoo}.]
{\includegraphics[width = 0.24 \textwidth]{figures/recsys/yahoo-all}}

\vspace{-10px}
\caption{RMSE vs CPU time on the \textit{netflix} and \textit{yahoo} data sets.}
\label{fig:large:rmse}
%\vspace{-10px}
\end{figure}

\begin{table*}[ht]
\centering
\caption{Results on grayscale image impainting. CPU time is in seconds.}
\vspace{-10px}

\begin{tabular}{c|c|c c c|c c c|c c c}
	\hline
	        &                     &           \multicolumn{3}{c|}{\textit{tree}}           &        \multicolumn{3}{c|}{\textit{rice}}        &            \multicolumn{3}{c}{\textit{wall}}            \\
	        &                     &           RMSE           & rank &         time         &           RMSE           & rank &      time      &           RMSE           & rank &         time          \\ \hline
	nuclear &    \textsf{APG}     &     0.433$\pm$0.001      & 180  &    308.7$\pm$71.9    &     0.224$\pm$0.002      & 148  & 242.3$\pm$27.8 &     0.222$\pm$0.001      & 165  &    311.5$\pm$31.2     \\ \cline{2-11}
	 norm   & \textsf{AIS-Impute} &     0.432$\pm$0.001      & 181  &    251.6$\pm$61.7    &     0.225$\pm$0.002      & 150  & 138.1$\pm$5.7  &     0.223$\pm$0.001      & 168  &    156.6$\pm$11.1     \\ \cline{2-11}
	        &   \textsf{active}   &     0.445$\pm$0.001      & 222  &   935.5$\pm$117.9    &     0.263$\pm$0.002      & 170  & 640.8$\pm$10.8 &     0.258$\pm$0.001      & 189  &    739.1$\pm$61.8     \\ \hline\hline
	 fixed  &   \textsf{LMaFit}   &     0.518$\pm$0.012      &  9   &     5.4$\pm$1.1      &     0.281$\pm$0.031      &  10  &  7.8$\pm$1.6   &     0.229$\pm$0.006      &  10  &      9.2$\pm$2.1      \\ \cline{2-11}
	 rank   &   \textsf{ER1MP}    &     0.473$\pm$0.001      &  19  &     1.5$\pm$0.2      &     0.295$\pm$0.002      &  22  &  2.2$\pm$0.6   &     0.269$\pm$0.003      &  20  &      1.2$\pm$0.1      \\ \hline\hline
	  LSP   &    \textsf{IRNN}    &     0.416$\pm$0.003      &  12  &    204.9$\pm$19.0    & \textbf{0.197$\pm$0.003} &  15  & 480.0$\pm$50.5 &     0.196$\pm$0.001      &  17  &     562.5$\pm$0.8     \\ \cline{2-11}
	        &    \textsf{GPG}     &     0.417$\pm$0.004      &  12  &    195.9$\pm$17.5    & \textbf{0.197$\pm$0.003} &  15  & 464.3$\pm$55.0 & \textbf{0.195$\pm$0.001} &  17  &    557.6$\pm$17.1     \\ \cline{2-11}
	        &   \textsf{FaNCL}    &     0.416$\pm$0.002      &  12  &     10.6$\pm$1.8     & \textbf{0.198$\pm$0.004} &  15  &  25.0$\pm$1.8  &     0.199$\pm$0.005      &  17  &     27.5$\pm$1.0      \\ \cline{2-11}
	        & \textsf{FaNCL-acc}  & \textbf{0.414$\pm$0.001} &  12  & \textbf{5.4$\pm$0.8} & \textbf{0.197$\pm$0.001} &  15  &  8.4$\pm$1.2   & \textbf{0.194$\pm$0.001} &  17  & \textbf{11.7$\pm$0.6} \\ \hline
\end{tabular}
\label{tab:gray}
%\vspace{-10px}
\end{table*}

\subsubsection{Image Data Sets}
\label{sec:expgray}

%\footnote{
%*** ok. but u can also do that after acceptance 
%+++ to save space,
%	remove columns on CPU time in Table~\ref{tab:gray} and \ref{tab:hyper},
%	and then merge them together?}
\noindent
\textbf{Grayscale Images:}
We use the images in \cite{hu2013fast}
(Figures~\ref{fig:gray}).
The
pixels are normalized
to zero mean and unit variance.
Gaussian noise from $\mathcal{N}(0, 0.05)$ 
is then added.
In each  image,
20\% of the pixels 
are randomly sampled as observations (half for training and the half for validation).
The task is to fill in the remaining 80\% of the pixels.
The experiment is repeated five times.
The LSP regularizer is used, as 
it usually has comparable or better performance than the capped-$\ell_1$
and TNN regularizers (as can be seen from Sections~\ref{sec:matcomp:syn} and \ref{sec:recsys}).
The experiment is repeated five times.

\begin{figure}[ht]
\centering
%\vspace{-10px}
\subfigure[\textit{rice}
(854$\times$960).
\label{fig:gray:rice}]
{\includegraphics[width = 0.13\textwidth]{figures/matcomp-new/rice}}
\quad
\subfigure[\textit{tree}.
(800$\times$800).
]
{\includegraphics[width = 0.13\textwidth]{figures/matcomp-new/tree}}
\quad
\subfigure[\textit{wall}
(841$\times$850).
]
{\includegraphics[width = 0.13\textwidth]{figures/matcomp-new/wall}
\label{fig:gray:wall}}

\vspace{-10px}
\caption{Grayscale images used in the experiment.}
\label{fig:gray}
%\vspace{-5px}
\end{figure}

Table~\ref{tab:gray} shows
the testing RMSE, rank obtained, and running time.
As can be seen, models based on low-rank
factorization (\textsf{LMaFit} and \textsf{ER1MP})
and nuclear norm regularization (\textsf{AIS-Impute})
have higher testing RMSE's than those using LSP regularization (\textsf{IRNN}, \textsf{GPG}, \textsf{FaNCL}, and \textsf{FaNCL-acc}).
Figure~\ref{fig:rmse:image}
shows 
convergence of the testing RMSE.
Among the LSP regularization 
algorithms,
\textsf{FaNCL-acc} is the fastest, which is then followed by
\textsf{FaNCL}, \textsf{GPG}, and \textsf{IRNN}.


\begin{figure}[ht]
	\centering
	%\vspace{-5px}
	\subfigure[\textit{rice}.]
	{\includegraphics[width = 0.2425\textwidth]{figures/matcomp-new/image-rice}}
	\subfigure[\textit{tree}.]
	{\includegraphics[width = 0.24\textwidth]{figures/matcomp-new/image-tree}}
%	\qquad
%	\subfigure[\textit{wall}.]
%	{\includegraphics[width = 0.24\textwidth]{figures/matcomp-new/image-wall}}
	
	\vspace{-10px}
	\caption{Testing RMSE vs CPU time (in seconds) on the grayscale images.
	The plot of \textit{wall} is similar and thus not shown.}
	\label{fig:rmse:image}
	%\vspace{-10px}
\end{figure}

%\subsubsection{Image Inpainting: Hyperspectral}
%\label{sec:exphyper}

\noindent
\textbf{Hyperspectral Images:}
In this experiment,
hyperspectral images
%\footnote{\url{https://sites.google.com/site/hyperspectralcolorimaging/dataset/fruits}}
(Figure~\ref{fig:hyper}) are used.
Each sample
is a 
$I_1 \times I_2 \times I_3$
tensor, where
$I_1\times I_2$ is the image size,
and $I_3$ is the number of frequencies
used to scan the object.
As in \cite{signoretto2011tensor},
we convert
this to a
$I_1 I_2 \times I_3$
matrix.
The pixels 
are normalized
to zero mean and unit variance,
and Gaussian noise from $\mathcal{N}(0, 0.05)$ is added.
1\% of the pixels are 
randomly sampled 
for training,
0.5\% for validation and 
the remaining
for testing. 
Again, we use the LSP regularizer.
The experiment is repeated five times.
As \textsf{IRNN} and \textsf{GPG} are slow (Sections~\ref{sec:matcomp:syn} and \ref{sec:recsys}), 
while \textsf{APG} and active subspace selection 
have been shown 
to be inferior to
\textsf{AIS-Impute} on the greyscale images, 
they will not be compared here.

\begin{figure}[ht]
\centering
%\vspace{-10px}
\subfigure[\textit{broccoli}.
\label{fig:gray:broccoli}]
{\includegraphics[width = 0.15\textwidth]{figures/matcomp-new/broccoli}}
\quad
\subfigure[\textit{cabbage}.]
{\includegraphics[width = 0.14\textwidth]{figures/matcomp-new/cabbage}}
\quad
\subfigure[\textit{corn}.
\label{fig:gray:corn}]
{\includegraphics[width = 0.15\textwidth]{figures/matcomp-new/corn}}

\vspace{-10px} 
\caption{Hyperspectral images used in the experiment, 
their sizes are 1312$\times$480$\times$49, 1312$\times$528$\times$49 and 1312$\times$480$\times$49, respectively.
One sample band of each image is shown.}
\label{fig:hyper}
%\vspace{-5px}
\end{figure}


Table~\ref{tab:hyper} shows the testing RMSE,  rank obtained and running time.
As on grayscale images,
\textsf{FaNCL} and \textsf{FaCNL-acc} have lower testing RMSE's than models based on low-rank factorization 
(\textsf{LMaFit} and \textsf{ER1MP})
and nuclear norm regularization
(\textsf{AIS-Impute}). 
Figure~\ref{fig:rmse:hyper} shows convergence of
the testing RMSE.
Again,
\textsf{FaNCL-acc} is much faster than \textsf{FaNCL}.

\begin{figure}[ht]
\centering
%\vspace{-5px}
\subfigure[\textit{broccoli}.]
{\includegraphics[width = 0.24\textwidth]{figures/matcomp-new/hyper-broccoli}}
\subfigure[\textit{cabbage}.]
{\includegraphics[width = 0.24\textwidth]{figures/matcomp-new/hyper-cabbage}}
%	\qquad
%	\subfigure[\textit{corn}.]
%	{\includegraphics[width = 0.24\textwidth]{figures/matcomp-new/hyper-corn}}

\vspace{-10px}
\caption{Testing RMSE vs CPU time (in seconds) on the hyperspectral images.
	The plot of \textit{corn} is similar and thus not shown.}
\label{fig:rmse:hyper}
%\vspace{-15px}
\end{figure}

\begin{table*}[ht]
\centering
%\vspace{-5px}
\caption{Results on hyperspectral image impainting.
	CPU time is in seconds.
	}

\vspace{-10px}
\begin{tabular}{c|c|c c c|c c c|c c c}
	\hline
           &                     &        \multicolumn{3}{c|}{\textit{broccoli}}         &         \multicolumn{3}{c|}{\textit{cabbage}}         &             \multicolumn{3}{c}{\textit{corn}}             \\
           &                     &           RMSE           & rank & time                &           RMSE           & rank & time                &           RMSE           & rank & time                    \\ \hline
	nuclear norm & \textsf{AIS-Impute} &     0.275$\pm$0.001      & 34   & 560$\pm$90          &     0.180$\pm$0.001      & 28   & 493$\pm$46          &     0.236$\pm$0.001      & 41   & 519.1$\pm$94.7          \\ \hline\hline
 fixed rank  &   \textsf{LMaFit}   &     0.302$\pm$0.001      & 2    & 6$\pm$2             &     0.181$\pm$0.001      & 3    & 6$\pm$3             &     0.265$\pm$0.002      & 5    & 3/9$\pm$1.5             \\ \cline{2-11}
           &   \textsf{ER1MP}    &     0.344$\pm$0.002      & 21   & 88$\pm$12           &     0.252$\pm$0.003      & 41   & 75$\pm$23           &     0.299$\pm$0.004      & 37   & 55.4$\pm$7.2            \\ \hline\hline
  LSP      &   \textsf{FaNCL}    & \textbf{0.252$\pm$0.003} & 9    & 6079$\pm$1378       & \textbf{0.149$\pm$0.001} & 10   & 3243$\pm$163        & \textbf{0.204$\pm$0.004} & 15   & 4672.8$\pm$967.8        \\ \cline{2-11}
           & \textsf{FaNCL-acc}  & \textbf{0.251$\pm$0.004} & 9    & \textbf{274$\pm$77} & \textbf{0.149$\pm$0.001} & 10   & \textbf{221$\pm$12} & \textbf{0.203$\pm$0.002} & 15   & \textbf{273.8$\pm$75.4} \\ \hline
\end{tabular}
\label{tab:hyper}
%\vspace{-5px}
\end{table*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Robust Principal Component Analysis}
\label{sec:expt2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Synthetic Data}
\label{sec:synrpca}

In this section, 
we first perform experiments on a synthetic data set.  The observed $m \times m$ matrix is
generated as $\mathbf{O} = \mathbf{U} \mathbf{V} + \tilde{\mathbf{S}} + \mathbf{G}$, where
elements  of
$\mathbf{U} \in \R^{m \times k}, \mathbf{V} \in \R^{k \times m}$ (with $k = 0.01m$) are 
sampled i.i.d. from $\mathcal{N}(0, 1)$,
and elements of $\mathbf{G}$ are sampled from $\mathcal{N}(0, 0.1)$.  
Matrix $\tilde{\mathbf{S}}$ is sparse, 
with $1\%$ of its elements randomly set to $5 \NM{\mathbf{U} \mathbf{V}}{\infty}$  or $- 5 \NM{\mathbf{U} \mathbf{V}}{\infty}$ with equal probabilities. 
The columns of $\mathbf{O}$ is then randomly split into training and test sets of equal size.
The standard $\ell_1$
regularizer
is 
used as the sparsity regularizer $g$ in (\ref{eq:rpca}), and
different convex/nonconvex low-rank regularizers are used as $r$.
Hyperparameters $\lambda$ and $\upsilon$ in \eqref{eq:rpca}
are tuned using the training set.  

\begin{table*}[ht]
\centering
%\vspace{-5px}
\caption{RPCA performance on synthetic data. 
	Here, NMSE is scaled by $10^{-3}$, and  CPU time is in seconds.}
\vspace{-10px}

\begin{tabular}{cc|ccc|ccc|ccc}
	\hline
	&                    &             \multicolumn{3}{c|}{$m=500$}             &            \multicolumn{3}{c|}{$m=1000$}             &             \multicolumn{3}{c}{$m=2000$}              \\
	&                    &          NMSE          & rank &         time         &          NMSE          & rank &         time         &          NMSE          & rank &         time          \\ \hline
	nuclear norm   &    \textsf{APG}    &     4.88$\pm$0.17      &  5   &     4.3$\pm$0.2      &     3.31$\pm$0.06      &  10  &     24.5$\pm$1.0     &     2.40$\pm$0.05      &  20  &    281.2$\pm$26.7     \\ \hline\hline
	capped-$\ell_1$ &    \textsf{GPG}    & \textbf{4.51$\pm$0.16} &  5   &     8.5$\pm$2.6      & \textbf{2.93$\pm$0.07} &  10  &     42.9$\pm$6.6     & \textbf{2.16$\pm$0.05} &  20  &    614.1$\pm$64.7     \\ \cline{2-11}
	& \textsf{FaNCL-acc} & \textbf{4.51$\pm$0.16} &  5   & \textbf{0.8$\pm$0.2} & \textbf{2.93$\pm$0.07} &  10  & \textbf{2.8$\pm$0.1} & \textbf{2.16$\pm$0.05} &  20  & \textbf{24.9$\pm$2.0} \\ \hline
	LSP       &    \textsf{GPG}    & \textbf{4.51$\pm$0.16} &  5   &     8.3$\pm$2.3      & \textbf{2.93$\pm$0.07} &  10  &     42.6$\pm$5.9     & \textbf{2.16$\pm$0.05} &  20  &    638.8$\pm$72.6     \\ \cline{2-11}
	& \textsf{FaNCL-acc} & \textbf{4.51$\pm$0.16} &  5   & \textbf{0.8$\pm$0.1} & \textbf{2.93$\pm$0.07} &  10  & \textbf{2.9$\pm$0.1} & \textbf{2.16$\pm$0.05} &  20  & \textbf{26.6$\pm$4.1} \\ \hline
	TNN       &    \textsf{GPG}    & \textbf{4.51$\pm$0.16} &  5   &     8.5$\pm$2.4      & \textbf{2.93$\pm$0.07} &  10  &     43.2$\pm$5.8     & \textbf{2.16$\pm$0.05} &  20  &    640.7$\pm$59.1     \\ \cline{2-11}
	& \textsf{FaNCL-acc} & \textbf{4.51$\pm$0.16} &  5   & \textbf{0.8$\pm$0.1} & \textbf{2.93$\pm$0.07} &  10  & \textbf{2.9$\pm$0.1} & \textbf{2.16$\pm$0.05} &  20  & \textbf{26.9$\pm$2.7} \\ \hline
\end{tabular}
\label{tab:synperformance}
%\vspace{-10px}
\end{table*}

For performance evaluation, 
we use the
(i) testing NMSE
$=\NM{(\X + \mathbf{S}) - \mathcal{P}_{\mathcal{T}}(\mathbf{UV+\tilde{S}}) }{F} / 
\NM{ 
\mathcal{P}_{\mathcal{T}}(\mathbf{UV+\tilde{S}} ) }{F}$, 
where $\mathcal{T}$ indices columns in the test set,
$\mathbf{X}$ and $\mathbf{S}$
are the recovered low-rank and sparse components, respectively;
(ii) accuracy on locating the sparse support of $\tilde{\mathbf{S}}$ (i.e.,
percentage of entries that $\tilde{S}_{ij}$ and $S_{ij}$ are nonzero or zero together);
(iii) 
the recovered rank
and (iv) CPU time.
We vary $m$ in $\{500, 1000, 2000\}$.
Each experiment is repeated five times.
Note that \textsf{IRNN}  and active subspace selection cannot be used here.
Their objectives are of the form ``smooth function plus low-rank
regularizer", but RPCA  also has a nonsmooth $\ell_1$ regularizer.
Similarly, \textsf{AIS-Impute} is only for matrix completion.
Moreover,
\textsf{FaNCL},
which has been shown to be slower than \textsf{FaNCL-acc},
will not be compared.

Results are shown in Table~\ref{tab:synperformance}.
The accuracies on locating the sparse support
are always 100\% 
for all methods, and thus  are
not shown.
Moreover, while both convex and nonconvex regularizers can perfectly recover the matrix rank and
sparse locations,
the nonconvex regularizers have lower NMSE's.
As in matrix completion, \textsf{FaNCL-acc} is much faster.
The larger
the matrix, the higher 
the speedup.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{Background Removal in Videos}

In this section, we use RPCA for background removal in videos. Four benchmark 
videos in \cite{candes2011robust,sun2013robust}
are used
(Table~\ref{tab:sumVideo}), and example frames are shown in
Figure~\ref{fig:exampleImages}.
As in \cite{candes2011robust}, the image background is considered low-rank, while the foreground moving objects contribute to the 
sparse component.

\begin{table}[ht]
\centering
%\vspace{-10px}
\caption{Videos used in the experiment.}
\vspace{-10px}
\begin{tabular}{c | c | c | c | c}
	\hline
	                 & {\em bootstrap} & {\em campus} & {\em escalator} & {\em hall} \\ \hline
	\#pixels / frame & 19,200          & 20,480       & 20,800          & 25,344     \\ \hline
	 total \#frames  & 9,165           & 4,317        & 10,251          & 10,752     \\ \hline
\end{tabular}
\label{tab:sumVideo}
%\vspace{-5px}
\end{table}

\begin{figure}[ht]
\centering
%\vspace{-10px}
\subfigure[{\em bootstrap}.]
{\includegraphics[width = 0.2\columnwidth]{figures/backgnd/Bootstrap}}
\subfigure[{\em campus}.]
{\includegraphics[width = 0.2\columnwidth]{figures/backgnd/Campus}} 
\subfigure[{\em escalator}.]
{\includegraphics[width = 0.2\columnwidth]{figures/backgnd/Escalator}} 
\subfigure[{\em hall}.]
{\includegraphics[width = 0.2\columnwidth]{figures/backgnd/Hall}}

\vspace{-10px}
\caption{Example image frames in the videos.}
\label{fig:exampleImages}
%\vspace{-5px}
\end{figure}

\begin{table*}[ht]
\centering
\caption{PSNR (in dB) and CPU time (in seconds) on the video background removal experiment. 
	The PSNRs for all the input videos are 16.47dB.}
\vspace{-10px}

\begin{tabular}{cc|cc|cc|cc|cc}
	\hline
	\multicolumn{2}{c|}{}         &    \multicolumn{2}{c|}{\textit{bootstrap}}    &      \multicolumn{2}{c|}{{\em campus}}      &     \multicolumn{2}{c|}{{\em escalator}}      &        \multicolumn{2}{c}{{\em hall}}         \\ 
	\multicolumn{2}{c|}{}         &          PSNR           &        time         &          PSNR           &       time        &          PSNR           &        time         &          PSNR           &        time         \\ \hline
	nuclear norm   &    \textsf{APG}    &     23.07$\pm$0.02      &     524$\pm$84      &     22.47$\pm$0.02      &     101$\pm$6     &     24.01$\pm$0.01      &     594$\pm$86      &     24.25$\pm$0.03      &     553$\pm$85      \\ \hline\hline
	capped-$\ell_1$ &    \textsf{GPG}    &     23.81$\pm$0.01      &    3122$\pm$284     &     23.21$\pm$0.02      &    691$\pm$43     &     24.62$\pm$0.02      &    5369$\pm$238     &     25.22$\pm$0.03      &    4841$\pm$255     \\ \cline{2-10}
	& \textsf{FaNCL-acc} &     24.05$\pm$0.01      & \textbf{193$\pm$18} &     23.24$\pm$0.02      &     53$\pm$5      & \textbf{24.68$\pm$0.02} &     242$\pm$22      &     25.22$\pm$0.03      & \textbf{150$\pm$10} \\ \hline
	LSP       &    \textsf{GPG}    &     23.93$\pm$0.03      &    1922$\pm$111     &     23.61$\pm$0.02      &    324$\pm$27     &     24.57$\pm$0.01      &    5053$\pm$369     & \textbf{25.37$\pm$0.03} &    2889$\pm$222     \\ \cline{2-10}
	& \textsf{FaNCL-acc} & \textbf{24.30$\pm$0.02} & \textbf{189$\pm$15} & \textbf{23.99$\pm$0.02} &     69$\pm$8      &     24.56$\pm$0.01      & \textbf{168$\pm$15} & \textbf{25.37$\pm$0.03} & \textbf{144$\pm$9}  \\ \hline
	TNN       &    \textsf{GPG}    &     23.85$\pm$0.03      &    1296$\pm$203     &     23.12$\pm$0.02      &    671$\pm$21     &     24.60$\pm$0.01      &    4091$\pm$195     &     25.26$\pm$0.04      &    4709$\pm$367     \\ \cline{2-10}
	& \textsf{FaNCL-acc} &     24.12$\pm$0.02      &     203$\pm$11      &     23.14$\pm$0.02      & \textbf{49$\pm$5} & \textbf{24.66$\pm$0.01} &     254$\pm$30      &     25.25$\pm$0.06      &     148$\pm$11      \\ \hline
\end{tabular}
\label{tab:videoPSNR}
\end{table*}



Given a video with $n$ image frames, each $m_1 \times m_2$ frame is first reshaped as a
$m$-dimensional column vector (where $m = m_1 m_2$), and
then all the frames are stacked together to form a $m \times n$ matrix.
The pixel values are normalized to $[0, 1]$, and
Gaussian noise from $\mathcal{N}(0, 0.15)$ is added.
The experiment is repeated five times.
For performance evaluation, we use the commonly used peak signal-to-noise ratio 
\cite{gu2016weighted}: PSNR $= - 10 \log_{10}(\frac{1}{m n} \NM{\mathbf{X} - \mathbf{O}}{F}^2)$
where 
$\mathbf{X} \in \R^{m \times n}$ is the recovered video,
and $\mathbf{O} \in \R^{m \times n}$ is the ground-truth.



Results are shown in Table~\ref{tab:videoPSNR}.
As can be seen, the nonconvex regularizers lead to 
better  PSNR's than the convex nuclear norm. 
Moreover, 
\textsf{FaNCL-acc} is much faster than \textsf{GPG}.
Figure~\ref{fig:bootstrap} shows PSNR vs CPU time on the {\em bootstrap} and {\em campus} data sets.
Again, \textsf{FaNCL-acc} converges to higher PSNR much faster.
	Results on {\em hall} and {\em escalator} are similar.

\begin{figure}[ht]
	\centering
	
	\subfigure[\textit{bootstrap}.]
	{\includegraphics[width = 0.24\textwidth]{figures/backgnd/bootstrap-psnr}}
	\subfigure[{\em campus}.]
	{\includegraphics[width = 0.24\textwidth]{figures/backgnd/campus-psnr}}
	
	\vspace{-10px}
	\caption{PSNR vs CPU time on the {\em bootstrap} and {\em campus} videos.}
	\label{fig:bootstrap}
	%\vspace{-10px}
\end{figure} 



\subsection{Parallel Matrix Completion}
\label{sec:expparallel}

In this section, we experiment with the proposed parallel algorithm in Section~\ref{sec:parallel}
on the \textit{Netflix} and \textit{Yahoo} data sets (Table~\ref{tab:recSys}).
We do not compare with factorization-based algorithms
\cite{yu2012scalable,recht2013parallel}, as they have 
inferior performance
(Section~\ref{sec:exptmc}).
The machine has 12 cores, and one thread is used for each core.
As suggested in \cite{yu2012scalable}, we randomly shuffle all the matrix columns and rows
before partitioning.  We use the LSP penalty (with $\theta = \sqrt{\lambda}$) and fix the
total number of iterations to 250.  The 
hyperparameters are the
same 
as in Section~\ref{sec:recsys}.
Experiments are repeated five times.

Convergence of the objective
for a typical 
run is shown in Figure~\ref{fig:plobj}.
As we have multiple threads running on a single CPU, we report the
clock time instead of CPU time.
As can be seen, the accelerated algorithms are much faster than the non-accelerated ones,
and parallelization provides further speedup.

\begin{figure}[ht]
\centering
%\vspace{-10px}
\subfigure[\textit{netflix}.]
{\includegraphics[width = 0.24\textwidth]{figures/parallel/netflix}}
\subfigure[\textit{yahoo}.]
{\includegraphics[width = 0.24\textwidth]{figures/parallel/yahoo}}

\vspace{-10px}
\caption{Objective value vs clock time for the sequential/parallel versions of \textsf{FaNCL} 
on the \textit{netflix} and \textit{yahoo} data sets.}
\label{fig:plobj}
\vspace{-5px}
\end{figure}

Figure~\ref{fig:clock} shows the 
per-iteration 
clock time 
with different numbers of threads.
As can be seen, the clock time decreases significantly with the number of threads.
Note that the curves for \textsf{FaNCL} and \textsf{FaNCL-acc} overlap.
This is because of the per-iteration time complexity of \textsf{FaNCL-acc} 
is only slightly higher than that of \textsf{FaNCL} (Section~\ref{sec:accFaNCL}).
Figure~\ref{fig:speedup} shows the speedup with different numbers of threads.
In particular, scaling is
better on \textit{yahoo}.
The observed entries in its partitioned data submatrices 
are distributed more evenly, 
which improves performance of parallel algorithms
\cite{gemulla2011large}.
Another observation  is that the speedup can be larger
than one.  As discussed in \cite{bertsekas1997parallel}, in performing multiplications
with a large sparse matrix, a significant amount of time is spent on indexing its nonzero
elements.  When the matrix is partitioned, each submatrix becomes smaller and easier to be  indexed.
Thus, the memory cache also becomes more effective.

\begin{figure}[ht]
\centering
\subfigure[Clock time per iteration. \label{fig:clock}]
{\includegraphics[width = 0.23\textwidth]{figures/parallel/speedup-time}}
\subfigure[Speedup. \label{fig:speedup}]
{\includegraphics[width = 0.23\textwidth]{figures/parallel/speedup}}

\vspace{-10px}
\caption{Clock time (seconds) per iteration and speedup
 vs the number of threads for parallel \textsf{FaNCL}.  
The dashed line in 
Figure~\ref{fig:speedup}
corresponds to linear speedup.}
\label{fig:paraspeedup}
\end{figure}

%-----------------------------------------------------------------------------------------
\section{Conclusion}

In this paper, we considered the challenging problem of nonconvex low-rank matrix optimization.
The key observations are that for  the popular low-rank regularizers, 
the singular values 
obtained from the proximal operator
can be automatically thresholded, and the proximal operator can be
computed on a smaller matrix.  This allows the proximal operator to be efficiently
approximated by the power method. 
We extended the proximal algorithm in this nonconvex optimization setting with acceleration and inexact proximal step.
We further parallelized the proposed algorithm,
which scales well w.r.t. the number of threads.
Extensive experiments on matrix completion and RPCA 
show that the proposed algorithm is much faster than the state-of-the-art.
It also demonstrates that nonconvex low-rank regularizers outperform the  standard
(convex) nuclear norm regularizer.

%{
%\color{red}
%In the parallel setting, 
%typically the observed entries are non-uniformly distributed in the partitioned matrices,
%and workloads in different threads are not well balanced.  One future direction is to
%allow asynchronized updates of the parallel algorithm.  This can help to reduce the waiting
%time for threads with light workloads, and makes more efficient use of the CPU.  Moreover,
%while parallel algorithms on multicore machines are easier to implement and do not have
%communication issues, they are less scalable than distributed algorithms 
%\cite{bertsekas1997parallel}.  To allow further scaleup to massive data sets, we will
%consider extending
%the proposed algorithms
%to a distributed computing environment.
%}

\section*{Acknowledgment}

This research was supported in part by
the Research Grants Council of the Hong Kong Special Administrative Region
(Grant 614513),
Microsoft Research Asia 
and 4Paradigm.

%-----------------------------------------------------------------------------------------
% bibliography
%-----------------------------------------------------------------------------------------
\bibliographystyle{IEEEtran}
\bibliography{bib}


%\begin{IEEEbiography}[{\includegraphics[width = 1\textwidth]{figures/bib/quanming}}]{Quanming Yao}
%obtained his Ph.D from Computer Science 
%and Engineer Department of Hong Kong University of Science and Technology (HKUST) in 2018,
%and bachelor degree in Electronic and Information Engineering from the Huazhong University of Science and Technology (HUST) in
%2013. His research interests focus on machine learning.
%Currently, he is a research scientist in 4Paradigm Inc. (Beijing, China). 
%He was awarded as Qiming star of HUST in 2012,
%Tse Cheuk Ng Tai research excellence prize from HKUST in 2015
%and Google PhD fellowship (machine learning) in 2016.
%\end{IEEEbiography}
%
%\begin{IEEEbiography}[{\includegraphics[width = 1\textwidth]{figures/bib/james}}]{James T. Kwok}
%(Fellow'17)
%received the PhD degree in
%computer science from the Hong Kong University
%of Science and Technology in 1996. He was
%with the Department of Computer Science,
%Hong Kong Baptist University, Hong Kong, as
%an assistant professor. He is currently a professor
%in the Department of Computer Science and
%Engineering, Hong Kong University of Science
%and Technology. His research interests include
%kernel methods, machine learning, 
%and artificial neural networks. He
%received the IEEE Outstanding 2004 Paper Award, and the Second
%Class Award in Natural Sciences by the Ministry of Education, People’s
%Republic of China, in 2008. He has been a program cochair for a number
%of international conferences, and served as an associate editor for
%the IEEE Transactions on Neural Networks and Learning Systems
%from 2006-2012. Currently, he is an associate editor for the Neurocomputing
%journal.
%\end{IEEEbiography}
%
%\begin{IEEEbiography}[{\includegraphics[width = 1\textwidth]{figures/bib/taifeng}}]{Taifeng Wang}
%is a lead researcher in Machine Learning group, Microsoft Research Asia. His research interests include machine learning, distributed system and search ads click prediction. Many of his technologies have been transferred to Microsoft’s products and online services, such as Bing, Microsoft Advertising, and Azure. He has published tens of papers and served as the PC members of many premium conferences such as KDD, WWW, SIGIR, IJCAI, and WSDM. 
%\end{IEEEbiography}
%
%\begin{IEEEbiography}[{\includegraphics[width = 1\textwidth]{figures/bib/tie-yan}}]{Tie-Yan Liu}
%(Fellow'17) received the Ph.D. degree from Tsinghua University, Beijing, China. He is a Principal Researcher/Research Manager with Microsoft Research Asia, Beijing, an Adjunct Professor at Carnegie Mellon University, Pittsburgh, PA, USA, and an Honorary Professor at the University of Nottingham, Nottingham, U.K. He has published hundreds of technical papers in peer-reviewed journals and top conferences, with tens of thousands of citations. He has received a number of best paper and most cited paper awards. Dr. Liu is an Associate Editor of TIST, TWEB, and Neurocomputing; an Editorial Board Member of Information Retrieval Journal, and Foundations \& Trends in Information Retrieval; and has served regularly as program committee chair or area chair of a number of top international conferences and workshops.
%% He is a distinguished member of the Association for Computing Machinery.
%\end{IEEEbiography}

\end{document}

\cleardoublepage

\appendices

\newcommand{\InexactPS}[1]{\text{\sf InexactPS}\left(  #1 \right)  }


\section{Proofs}
\label{app:proof}

\subsection{Proposition~\ref{pr:proxSolution}}
\label{app:proxSolution}

For simplicity of notations, we write $\sigma_i(\mathbf{\mathbf{Z}})$ as $\sigma_i$.
First,
we introduce the definition of super-gradient for a concave function
and two lemmas.

\begin{definition}[\cite{bertsekas1999nonlinear}]
For a concave function $f$,
its super-gradient is given by $g \in \hat{\partial} f \equiv \partial \left( - f \right)$.
\end{definition}
	



\begin{lemma}[\cite{bertsekas1999nonlinear}]
\label{lem:superg}
(i)
$\inf_{g \in \hat{\partial} \hat{r}(y)} g \ge 0$;
(ii) 
Assume that $y_j \ge y_i \ge 0$.
Then, $\sup_{g_j \in \hat{\partial} \hat{r}(y_j)} g_j \le \inf_{g_i \in \hat{\partial} \hat{r}(y_i)} g_i$.
\end{lemma}

\begin{lemma} \label{lem:sigandy}
(i)
$y_i^* - \max\left( \sigma_i - \mu g_i, 0 \right) = 0$,
where $g_i \in \hat{\partial}\hat{r}(y_i^*)$;
(ii) 
if $y_i^* > 0$, then $y_i^*$ increases with $\sigma_i$.
%If $y_i^* \neq 0$,\footnote{*** in the 1st part 
%$y_i^*$ is taken as a value; in the next part, 
%$y_i^*$ is taken as a fcn}
%$y_i^*$ 
%is a nondecreasing function of $\sigma_i$.
\end{lemma}

\begin{proof}
(Part (i)):
Let $g_i \in \hat{\partial}\hat{r}(y_i^*)$.
From the first-order optimality condition of \eqref{eq:proRed},
consider
the two possibilities:
\begin{itemize}
\item [(a)] \underline{$\sigma_i + \mu g_i \le 0$}:
In other  words, the optimal solution is achieved at the boundary,
and $y_i^* = 0$.

\item [(b)] \underline{$\sigma_i + \mu g_i > 0$}:
We have
$0 = y_i^* - \sigma_i + \mu g_i$,
and $y_i^* > 0$.
\end{itemize}

Combining
these two cases,
the relationship between $y_i^*$ and $\sigma_i$ can be expressed as
\begin{align}
y_i^* = \max\left( \sigma_i - \mu g_i, 0 \right).
\label{eq:temp29} 
\end{align}

\noindent
(Part (ii)):
Assume that $y_i^* > 0$.
Then, \eqref{eq:temp29} becomes
\begin{align}
y_i^* =  \sigma_i - \mu g_i.
\label{eq:temp31} 
\end{align}
Let $\sigma_i$ becomes larger as $\sigma_j$.
according to \eqref{eq:temp31},
we have two possibilities for its corresponding $y_j^*$, i.e., 
\begin{itemize}
\item 
\underline{$y_j^* > y_i^*$}:
Then,
$\sup_{g_j \in \hat{\partial} \hat{r}(y_j^*)} g_j \le \inf_{g_i
	\in \hat{\partial} \hat{r}(y_i^*)} g_i$
from Lemma~\ref{lem:superg}.
Together with the fact that $\sigma_j > \sigma_i$,
there exists a $y_j^*$ which is not smaller than $y_i^*$
to make \eqref{eq:temp29} hold. 

%\item \underline{$y_j^* = y_i^*$}:

\item 
\underline{$y_j^* \le y_i^*$}: 
Then,
$\inf_{g_j \in \hat{\partial} \hat{r}(y_j^*)} g_j \ge \sup_{g_i \in \hat{\partial} \hat{r}(y_i^*)} g_i$
from Lemma~\ref{lem:superg}.
However,
such a solution may not exist (e.g.,
when $\hat{r}(\alpha) = \alpha$).
\end{itemize}
Thus,
while there can be multiple solutions to ensure \eqref{eq:temp31},
the first case must exist.
We take the largest solution of all possible candidates. 
Thus,
if $\sigma_i$ gets larger,
$y_i^*$ also becomes larger.

\end{proof}

%As $y_i^*\neq 0$,
%we have  from (\ref{eq:proRed})
%$y_i^* = \sigma_i - \mu g_i$,
%where $g_i \in \hat{\partial} \hat{r}(y_i^*)$.
%Given another $\sigma_j$ where
%$\sigma_j \geq \sigma_i$, and
%assume that $y_j^* \neq 0$.
%Again,
%we have
%$y_j^* = \sigma_j - \mu g_j$,
%where $g_j \in \hat{\partial} \hat{r}(y_j^*)$.\footnote{*** ur orig proof looks weird. i
%simplified  it. dbl-chk}
%As $\sigma_j \geq \sigma_i$ and
%	$\sup_{g_j \in \hat{\partial} \hat{r}(y_j^*)} g_j \le \inf_{g_i \in \hat{\partial} \hat{r}(y_i^*)} g_i$
%	(Lemma~\ref{lem:superg}), 
%	we have $y_j^*\geq y_i^*$.

\noindent
\textit{Proof of Proposition~\ref{pr:proxSolution}.}
From Lemma~\ref{lem:sigandy}, 
we have
\begin{align*}
0 = y_i^* - \max\left( \sigma_i - \mu g_i, 0 \right).
\end{align*}
We can see that $y^* = 0$ once $\sigma_i - \mu g_i \le 0$.
However,
if $\sigma_i$ becomes smaller,
$\sigma_i - \mu g_i$ will reach $0$ before $\sigma_i$ reaches zero.
This comes from two facts.
First,
$y_i^*$ becomes smaller as $\sigma_i$ gets smaller (Lemma~\ref{lem:sigandy}),
but $\inf_{g_i \in \hat{\partial} \hat{r}(y_i^*)} g_i$ will not become smaller (Lemma~\ref{lem:superg}).
Second,
we have $\lim_{y \rightarrow 0^+} \inf_{g \in \hat{\partial} \hat{r}(y) } g > 0$.
An illustration of the relationships among $\sigma_i$, 
$y_i^*$ and $g_i$ is shown in the following Figure~\ref{fig:proof}.
Thus, there exists $\gamma > 0$ such that once $\sigma_i \le \gamma$, $\sigma_i - \mu g_i \le 0$, and $y_i^*$ becomes $0$.

\begin{figure}[ht]
\centering
\includegraphics[width=0.35\textwidth]{figures/proof/prop31}
%\vspace{-10px}
\caption{Illustration of Proposition~\ref{pr:proxSolution}.}
\label{fig:proof}
\end{figure}


%Consider the two cases
%in \eqref{eq:proRed}: 
%\begin{itemize}
%\item[(i)]
%\underline{$y_i^* = 0$}
%Obviously, $y_i^* \le \sigma_i$.
%	
%\item[(ii)]
%	\underline{$y_i^* \neq 0$:}
%	We have
%from (\ref{eq:proRed})
%	\begin{equation} 
%	0 = y_i^* - \sigma_i + \mu g_i,
%	\end{equation} 
%	where $g_i \in \hat{\partial}\hat{r}(y_i^*)$.
%	Besides,
%$y_i^* \le \sigma_i$ from Lemma~\ref{lem:superg}.
%\end{itemize}


%Next,
%we show the existence of such $\gamma$.
%Given a $\sigma_p < \sigma_i$.
%%as $\hat{r}$ is non-decreasing and concave,
%We have $y_p^* < y_i^*$ if case (ii) holds,
%which leads to $\inf_{g_p \in \hat{\partial} \hat{r}(y_p^*) } g_p \ge \inf_{g_i \in \hat{\partial} \hat{r}(y_i^*)}  g_i$ from Lemma~\ref{lem:superg}.
%Besides,
%for those nonconvex regularizers,
%we have $\lim_{y \rightarrow 0^+} \inf_{g \in \hat{\partial} \hat{r}(y) } g > 0$.
%Combining these two facts,
%$y_p^*$ will reach zero before $\sigma_p^*$ approaching zero



%Again,
%%from assumption A3, 
%as $\hat{r}$ is concave and non-decreasing,
%\begin{align*}
%\max_{g_j \in \partial \hat{r}(y_j^*)} g_j \le \min_{g_i \in \partial \hat{r}(y_i^*)} g_i.
%\end{align*}

\subsection{Corollary~\ref{cor:threh}}
\label{app:threh}

In this section,
we show how to derive the threshold $\gamma$ for the capped-$\ell_1$ penalty. Derivations
for the other penalties can be obtained similarly.

\subsubsection{Capped-$\ell_1$ Penalty}

\begin{proof}
Note that problem (\ref{eq:proRed}) considers each singular value separately.
For simplicity of notations, let $\sigma_{i}$ denote $\sigma_i(\mathbf{Z})$.
For the $i$th singular value, let
\begin{align*}
h(y_i) \equiv \frac{1}{2}\left(y_i - \sigma_i\right)^2 + \mu \min\left(y_i, \theta\right).
\end{align*}
Thus,
\begin{align*}
\text{\normalfont Arg}\min_{y_i \ge 0} h(y_i) = 
\begin{cases}
\text{\normalfont Arg}\min_{0 \le y_i \le \theta} h_1(y_i) 
\\
\text{\normalfont Arg}\min_{y_i > \theta} h_2(y_i) 
\end{cases},
\end{align*}
where
\begin{align*}
h_1(p_i) 
& = \frac{1}{2}(p_i - \sigma_i)^2 \! + \! \mu p_i,
\\
h_2(q_i) 
& = \frac{1}{2}(q_i - \sigma_i)^2 + \mu \theta.
\end{align*}
Note that $h_1$ is quadratic.
There are only three possibilities for $p_i^* = \text{\normalfont Arg}\min_{0 \le p_i \le \theta} h_1(p_i)$,
i.e.,
\begin{align*}
\min_{0 \le p_i \le \theta} h_1(p_i) 
\! = \!
\begin{cases}
\frac{1}{2} \sigma_i^2
\!\!\!\!& 
\text{if}\;
p_i^* = 0
\\
\mu \sigma_i
- \frac{1}{2}\mu^2 
\!\!\!\!&
\text{if}\;
p_i^* = \sigma_i - \mu
\\
\frac{1}{2}\left(\theta-\sigma_i\right)^2 + \mu \sigma_i
\!\!\!\!&\text{if}\;
p_i^* = \theta
\end{cases},
\end{align*}
and 
\begin{align}
p_i^* = 
\begin{cases}
0
& \text{if} \; 0 \le \sigma_i \le \mu
\\
\sigma_i - \mu
& \text{if} \;\mu < \sigma_i \le \mu + \theta
\\
\theta
& \text{otherwise}
\end{cases}.
\label{eq:temp5}
\end{align}

Let $q_i^* = \text{\normalfont Arg}\min_{q_i > \theta} h_2(q_i)$.
As $h_2$ is also quadratic
and $q_i^*$ cannot be $\theta$,
we have
\begin{align}
q_i^* =
\begin{cases}
\text{no solution}
& \text{if}\; 0 \le \sigma_i \le \theta \\
\sigma_i
& \text{otherwise}
\end{cases}.
\label{eq:temp9}
\end{align}
Note that when $\sigma_i \in [0, \theta]$,
there is no solution to $q_i^*$,
as $q_i^*$ can arbitrarily close to $\theta$. 
Since $h_1(\theta) = h_2(\theta)$, the possibility for $\theta = \text{\normalfont Arg}\min_{y_i \ge 0} h(y_i)$ is covered by $\text{\normalfont Arg}\min_{0 \le p_i \le \theta} h_1$.
	Thus, \eqref{eq:temp5} and \eqref{eq:temp9} have covered all possibilities of $y_i^*$.
Using them, 
we have
\begin{enumerate}
\item If $\theta \le \mu$, then
\begin{align*}
\min_{y_i \ge 0} 
h \! = \!
\begin{cases}
h_1(0)
& 0 \! \le \! \sigma_i \! \le \! \theta
\\
\min\left( h_1(0), h_2\left( \sigma_i \right)  \right)
& \theta \! < \! \sigma_i \! \le \! \mu
\\
\min\left( h_1(\sigma_i - \mu), h_2\left( \sigma_i \right)  \right)
&
\mu \! < \! \sigma_i \! \le \! \mu \! + \! \theta
\\
\min\left( h_1(\theta), h_2\left( \sigma_i \right)  \right)
&
\sigma_i \! > \! \mu \! + \! \theta
\end{cases}.
\end{align*}
In order to get $y_i^* = 0$,
we need
\begin{align*}
\min\left( h_1(0), h_2(\sigma_i) \right) = h_1(0),
\end{align*}
which leads to
\begin{align*}
\sigma_i \le \sqrt{ 2 \mu \theta }.
\end{align*}
Thus,
if 
$0 \le \sigma_i \le \min \left( \sqrt{2 \mu \theta}, \mu \right)$,
then $y_i^* = 0$.

%%\vspace{10px}

\item If $\theta > \mu$, then
\begin{align*}
\min_{y_i \ge 0} h 
\! = \!
\begin{cases}
h_1(0)
& 0 \! \le \! \sigma_i \! \le \! \mu
\\
h_1(\sigma_i - \mu)
& \mu \! < \! \sigma_i \! \le \! \theta
\\
\min\left( h_1(\sigma_i - \mu), h_2\left( \sigma_i \right)  \right)
&
\theta \! < \! \sigma_i \! \le \! \mu \! + \! \theta
\\
\min\left( h_1(\theta), h_2\left( \sigma_i \right)  \right)
&
\sigma_i \! > \! \mu \! + \! \theta
\end{cases}.
\end{align*}
%In order to get $y_i^* = 0$,
%we need
%\begin{align*}
%\min\left( h_1(0), h_2(\theta) \right) = h_1(0)
%\text{\; if \;}
%0 \le \sigma_i \le \mu.
%\end{align*}
Thus,
if 
$0 \le \sigma_i \le \mu$,
then we have $y_i^* = 0$.

\end{enumerate}

Finally,
combining the above two cases,
we can conclude that once $\sigma_i \le \min(\sqrt{2\theta \mu}, \mu)$,
then $y_i^* = 0$.
Thus,
$\gamma = \min(\sqrt{2\theta \mu}, \mu)$.
\end{proof}

%Taking the minimum over $\min h_1$ and $\min h_2$,
%the
%optimal $y^*$ is:
%\begin{align*}
%y_i^* =
%\begin{cases}
%(\sigma_i - \mu)_{+} 
%& \text{if} \; \sigma_i \le \theta + \frac{1}{2}\mu, \\
%\sigma_i            & 
%\text{otherwise}.
%\end{cases}
%\end{align*}
%Thus, 
%%for the capped-$\ell_1$ penalty, 
%$\gamma = \min ( \mu, \frac{1}{2}\mu + \theta)$.

%\noindent
%Combining with Proposition~\ref{pr:proxReduce}, we obtain the following:
%
%\begin{lemma}
%\label{lem:proCAP}
%When $r(\cdot)$ is the capped-$\ell_1$ penalty,
%$\Prox{\mu r}{\mathbf{Z}} = \mathbf{U} \Diag{[y_1^*,\dots,y_n^*]} \mathbf{V}^{\top}$, where
%\begin{align*}
%y_i^* =
%\begin{cases}
%(\sigma_i - \mu)_{+} & \text{if} \; \sigma_i \le \theta + \frac{1}{2}\mu \\
%\theta                           & \text{otherwise}
%\end{cases}.
%\end{align*}
%\end{lemma}

\subsection{Proposition~\ref{pr:redGSVT}}
\label{app:redGSVT}

\begin{proof}
First,
we introduce the following theorem.

\begin{theorem}
	[Separation theorem	\cite{rao1979separation}]
	\label{thm:sepsv}
Let $\X_{\text{gd}} \in \R^{m \times n}$ and $\mathbf{B} \in \R^{m \times r}$ with $\mathbf{B}^{\top} \mathbf{B} = \mathbf{I}$.  Then
	\begin{align*}
	\sigma_{i}\left(  \mathbf{B}^{\top} \X_{\text{gd}} \right) 
	\le \sigma_i(\X_{\text{gd}}),
	\;\text{for}\; 
	i = 1, \dots, \min(r, n).
	\end{align*}
%	where $t = m + n - r$.
\end{theorem}
Let the SVD of $\mathbf{Z}$ be $\mathbf{U} \mathbf{\Sigma} \mathbf{V}^{\top}$. 
$\mathbf{Z}$ can then be rewritten as
\begin{equation} \label{eq:tmp2}
\mathbf{Z} = 
[\mathbf{U}_{\hat{k}}; \mathbf{U}_{\bot}]
\begin{bmatrix}
\mathbf{\Sigma}_{\hat{k}} & \\
& \mathbf{\Sigma}_{\bot}
\end{bmatrix}
[\mathbf{V}_{\hat{k}}; \mathbf{V}_{\bot}]^{\top},
\end{equation} 
where $\mathbf{U}_{\hat{k}}$ contains the $\hat{k}$ leading columns of 
$\mathbf{U}$, and $\mathbf{U}_{\bot}$ the remaining columns.
Similarly,
$\mathbf{\Sigma}_{\hat{k}}$ (resp.
$\mathbf{V}_{\hat{k}}$) contains the $\hat{k}$ leading eigenvalues (resp. columns) of 
$\mathbf{\Sigma}$ (resp.
$\mathbf{V}$).
Let
\begin{align}
\tilde{\mathbf{u}}_i =  \mathbf{Q}^{\top} \mathbf{u}_i 
\text{\quad and \quad}
\tilde{\mathbf{v}}_i = \mathbf{v}_i,
\label{eq:temp1}
\end{align}
where $\mathbf{u}_i$ (resp. $\mathbf{v}_i$) is the $i$th column of $\mathbf{U}$ (resp. $\mathbf{V}$).
Then,
for $i = 1, \dots, \hat{k}$,
we have
\begin{eqnarray}
\tilde{\mathbf{u}}_i^{\top} \left(  \mathbf{Q}^{\top} \mathbf{Z} \right)  \tilde{\mathbf{v}}_i
& = &
\mathbf{u}_i^{\top} \left(  \mathbf{Q} \mathbf{Q}^{\top} \right) 
\mathbf{Z} \mathbf{v}_i
\notag
\\
& = &
\mathbf{u}_i^{\top} \mathbf{Z} \mathbf{v}_i
\label{eq:temp32}
\\
& =  &
\sigma_i(\mathbf{Z}),
\label{eq:temp8}
\end{eqnarray}
where \eqref{eq:temp32} is due to $\Span{ \mathbf{U}_{\hat{k}} } \subseteq \Span{\mathbf{Q}}$.
Hence,
\begin{eqnarray}
\sigma_i\left( \mathbf{Q}^{\top} \mathbf{Z} \right)  
= \sigma_i(\mathbf{Z}),
\text{\;for\;}
i = 1, \dots, \hat{k}.
\label{eq:temp28}
\end{eqnarray}
From Theorem~\ref{thm:sepsv},
by substituting $\mathbf{Q} = \mathbf{B}$ and $\X_{\text{gd}} = \mathbf{Z}$, we have
$\sigma_i( \mathbf{Q}^{\top} \mathbf{Z} )  \le \sigma_i(\mathbf{Z})$.
Combining with \eqref{eq:temp8},
we obtain that the rank-$\hat{k}$ SVD of $\mathbf{Q}^{\top} \mathbf{Z}$ is
	$( \mathbf{Q}^{\top} \mathbf{U}_{\hat{k}} )  \mathbf{\Sigma}_{\hat{k}} \mathbf{V}_{\hat{k}}^{\top}$,
with the corresponding left and right singular vectors contained in $\mathbf{Q}^{\top}
\mathbf{U}_{\hat{k}}$ and
$\mathbf{V}_{\hat{k}}$, respectively.


Again,
by Theorem~\ref{thm:sepsv},
we have 
\begin{align*}
\sigma_{\hat{k} + 1}\left( \mathbf{Q}^{\top} \mathbf{Z} \right) 
\le 
\sigma_{\hat{k} + 1}(\mathbf{Z})
\le \gamma.
\end{align*}
Besides, using (\ref{eq:tmp2}),
\begin{eqnarray}
\sigma_i\left( \mathbf{Q}^{\top} \mathbf{Z} \right) 
& \!\!\!\! = & \!\!\!\!\!\! \max_{ \tilde{\mathbf{u}}_i , \tilde{\mathbf{v}}_i } 
\tilde{\mathbf{u}}^{\top}_i \left(  \mathbf{Q}^{\top} \mathbf{U}_{\hat{k}} \mathbf{\Sigma}_{\hat{k}}  \mathbf{V}_{\hat{k}}^{\top} 
\! + \! \mathbf{Q}^{\top} \mathbf{U}_{\bot} \mathbf{\Sigma}_{\bot}  \mathbf{V}_{\bot}^{\top} \right)  
\tilde{\mathbf{v}}_i.
\notag
\end{eqnarray}
The first $\hat{k}$ singular values are from the term
$\mathbf{Q}^{\top} \mathbf{U}_{\hat{k}} \mathbf{\Sigma}_{\hat{k}} \mathbf{V}_{\hat{k}}$.
Hence,
\begin{align}
\sigma_{\hat{k} + 1}\left( \mathbf{Q}^{\top} \mathbf{Z} \right) 
= \max_{\tilde{\mathbf{u}}, \tilde{\mathbf{v}}}
\tilde{\mathbf{u}}^{\top} 
\left(  \mathbf{Q}^{\top} \mathbf{U}_{\bot} \mathbf{\Sigma}_{\bot}  \mathbf{V}_{\bot}^{\top} \right) 
\tilde{\mathbf{v}}
\le \gamma.
\label{eq:temp36}
\end{align}
Then,
\begin{align}
& \Prox{\mu r}{\mathbf{Q}^{\top} \mathbf{Z}}
\notag
\\
& = \Prox{\mu r}{
	\mathbf{Q}^{\top} \mathbf{U}_{\hat{k}} \mathbf{\Sigma}_{\hat{k}} \mathbf{V}_{\hat{k}}^{\top}
	+ \mathbf{Q}^{\top} \mathbf{U}_{\bot} \mathbf{\Sigma}_{\bot}  \mathbf{V}_{\bot}^{\top}
}
\notag
\\
& = \Prox{\mu r}{ \mathbf{Q}^{\top} \mathbf{U}_{\hat{k}} \mathbf{\Sigma}_{\hat{k}}  \mathbf{V}_{\hat{k}}^{\top} }
+
\Prox{\mu r}{ \mathbf{Q}^{\top} \mathbf{U}_{\bot} \mathbf{\Sigma}_{\bot}  \mathbf{V}_{\bot}^{\top} }
\label{eq:temp37}
\\
& = \Prox{\mu r}{\mathbf{Q}^{\top} \mathbf{U}_{\hat{k}} \mathbf{\Sigma}_{\hat{k}}  \mathbf{V}_{\hat{k}}^{\top} }.
\label{eq:temp34}
\end{align}
where \eqref{eq:temp37} follows from that $\mathbf{Q}^{\top} \mathbf{U}_{\hat{k}}$ (resp. $\mathbf{V}_{\hat{k}}$) is orthogonal to $\mathbf{Q} \mathbf{U}_{\bot}$ (resp. $\mathbf{V}_{\bot}$).
\eqref{eq:temp36} shows that there are only $\hat{k}$ singular values in $\mathbf{Q}^{\top} \mathbf{Z}$ larger than $\gamma$.
	Thus, $\Prox{\mu r}{ \mathbf{Q}^{\top} \mathbf{U}_{\bot} \mathbf{\Sigma}_{\bot}  \mathbf{V}_{\bot}^{\top} } = 0$
	and we get \eqref{eq:temp34}.
Finally,
\begin{align}
\mathbf{Q} \Prox{\mu r}{\mathbf{Q}^{\top} \mathbf{Z}}
& = \mathbf{Q} \left(  \mathbf{Q}^{\top} \mathbf{U}_{\hat{k}} \Prox{\mu r}{\mathbf{\Sigma}_{\hat{k}}} \mathbf{V}_{\hat{k}}^{\top} \right)  
\notag
\\
& = \mathbf{U}_{\hat{k}} \Prox{\mu r}{\mathbf{\Sigma}_{\hat{k}}} \mathbf{V}_{\hat{k}}^{\top}
\label{eq:temp35}
\\
&
= \Prox{\mu r}{\mathbf{Z}},
\label{eq:temp33}
\end{align}
where \eqref{eq:temp35} comes from $\Span{\mathbf{U}_{\hat{k}}} \subseteq \Span{\mathbf{Q}}$;
\eqref{eq:temp33}
comes from that rank-$\hat{k}$ SVD of $\mathbf{Z}$ is 
$\mathbf{U}_{\hat{k}} \mathbf{\Sigma}_{\hat{k}} \mathbf{V}_{\hat{k}}^{\top}$
and $\mathbf{Z}$ only has $\hat{k}$ singular values larger than $\gamma$.
\end{proof}


\subsection{Proposition~\ref{pr:apprGSVT}}
\label{app:apprGSVT}

\begin{proof}
First,
we introduce the following Lemmas.

\begin{lemma}[\cite{halko2011finding}]
\label{lem:power}
In Algorithm~\ref{alg:powermethod},
let the SVD of $\mathbf{Z}$ be $\bar{\mathbf{U}} \bar{\mathbf{\Sigma}}
\bar{\mathbf{V}}^{\top}$, and $\bar{\mathbf{U}}_k$ contain the first
$k$ columns of $\bar{\mathbf{U}}$.
We have
\begin{align*}
\NM{\mathbf{Q}_j \mathbf{Q}_j^{\top} - \bar{\mathbf{U}}_k \bar{\mathbf{U}}_k^{\top}}{F}
\le \alpha^{j - 1} \NM{\mathbf{C} \mathbf{C}^{\top} - \bar{\mathbf{U}}_k
\bar{\mathbf{U}}_k^{\top}}{F},
\end{align*}
where
$\alpha = \sigma_{k + 1}(\mathbf{Z}) / \sigma_{k}(\mathbf{Z}) \in (0, 1)$
and $\mathbf{C} = \QR{\mathbf{Z} \mathbf{R}}$.
\end{lemma}

\begin{lemma} 
\label{lem:impappr}
In Algorithm~\ref{alg:inexactPS},
let the rank-$k$ SVD of $\X_{\text{gd}}$ be $\mathbf{U}_k \mathbf{\Sigma}_k
\mathbf{V}_k^{\top}$, and
	\begin{equation} \label{eq:Bp}
	\mathbf{B}_p = \QR{\X_{\text{gd}} \tilde{\mathbf{V}}_{p}}.
	\end{equation} 
	Then,
	$\NM{ \mathbf{B}_p \mathbf{B}_p^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F} 
	\le \eta^p \NM{ \mathbf{B}_0 \mathbf{B}_0^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F}$,
	where
	$\eta \in (0, 1)$ is a constant.
\end{lemma}

\begin{proof}
%Since $\tilde{\mathbf{V}}_{p - 1}$ is used as input
%for the power method (step~4 of
%Algorithm~\ref{alg:inexactPS}),
%from Lemma~\ref{lem:power},
%\begin{align*}
%\NM{\mathbf{B}_{p} \mathbf{B}_{p}^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F}
%\le \eta \NM{\mathbf{B}_{p - 1} \mathbf{B}_{p - 1}^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F},
%\end{align*}
%where
%%$\mathbf{B}_{p - 1} = \QR{\X_{\text{gd}} \tilde{\mathbf{V}}_{p-1}}$ and
%$\eta = \left( \sigma_{k + 1}(\X_{\text{gd}}) / \sigma_k(\X_{\text{gd}}) \right)^{J - 1} \in (0, 1)$.
At the $p$th iteration of Algorithm~\ref{alg:inexactPS},
inside Algorithm~\ref{alg:powermethod} (step~3),
since $\mathbf{Z} = \X_{\text{gd}}$ and $\mathbf{R} = \mathbf{\tilde{V}}_{p - 1}$,
we have
\begin{align}
\mathbf{Q}_1  = \QR{\X_{\text{gd}} \tilde{\mathbf{V}}_{p - 1}} = \mathbf{B}_{p - 1}.
\label{eq:app18}
\end{align}
Then,
for $\tilde{\mathbf{V}}_{p}$,
%since $\mathbf{Z} = \X_{\text{gd}}$ and $\mathbf{R} = \mathbf{\tilde{V}}_{p - 1}$,
inside Algorithm~\ref{alg:apprGSVT} (step~2),
we have
\begin{align*}
\Span{\tilde{\mathbf{V}}_{p}}
= \Span{ \X_{\text{gd}}^{\top} \mathbf{Q} }.
\end{align*}
Thus,
\begin{align}
\Span{\X_{\text{gd}} \tilde{\mathbf{V}}_{p}}
& = \Span{\X_{\text{gd}} ( \X_{\text{gd}}^{\top} \mathbf{Q} ) }
\notag
\\
& = \Span{\X_{\text{gd}} ( \X_{\text{gd}}^{\top} \mathbf{Q}_J ) }
\label{eq:app16}
\\
& = \Span{\mathbf{Y}_{J + 1} },
\label{eq:app17}
\end{align}
where \eqref{eq:app16} comes from the fact that $\mathbf{Q}$ is returned after $J$ iterations of Algorithm~\ref{alg:powermethod};
and \eqref{eq:app17} from the definition of $\mathbf{Y}_{j + 1}$ at step~4 in Algorithm~\ref{alg:powermethod}.
Thus,
\begin{align}
\mathbf{Q}_{J + 1}
& =
\QR{\mathbf{Y}_{J + 1}}
\notag
\\
& =
\QR{\X_{\text{gd}} \tilde{\mathbf{V}}_{p}} 
=
\mathbf{B}_{p}.
\label{eq:app19}
\end{align}
Note that $\mathbf{C} = \mathbf{Q}_1$ in Lemma~\ref{lem:power}.
Together with \eqref{eq:app18} and \eqref{eq:app19},
we have
\begin{align*}
\NM{ \mathbf{B}_p \mathbf{B}_p^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F} 
& = \NM{ \mathbf{Q}_{J + 1} \mathbf{Q}_{J + 1}^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F} 
\\
& \le \alpha^J \NM{ \mathbf{Q}_{1} \mathbf{Q}_{1}^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F} 
\\
& = \eta \NM{ \mathbf{B}_{p - 1} \mathbf{B}_{p - 1}^{\top} - \mathbf{U}_k
\mathbf{U}_k^{\top}}{F},
\end{align*}
where $\eta = \alpha^J \in (0, 1)$.
Thus,
\begin{align*}
\NM{ \mathbf{B}_p \mathbf{B}_p^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F} 
\le \eta^p \NM{ \mathbf{B}_0 \mathbf{B}_0^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F}.
\end{align*}
\end{proof}

%we have 
%\footnote{$\surd$ please check the procedures in alg3 and alg4.
%	*** where does this come from?}
%\begin{align*}
%\NM{\mathbf{B}_{p + 1} \mathbf{B}_{p + 1}^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F}
%= \NM{\mathbf{B}_p \mathbf{B}_p^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F},
%\end{align*}
%
%
%
%Together with Lemma~\ref{lem:power},
%we have
%\begin{align*}
%\NM{\mathbf{B}_{p} \mathbf{B}_{p}^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F}
%\le \eta^p \NM{\mathbf{B}_0 \mathbf{B}_0^{\top} - \mathbf{U}_k \mathbf{U}_k^{\top}}{F},
%\end{align*}

\noindent
(Proof of Proposition~\ref{pr:apprGSVT})
For $\mathbf{B}_p$ in 
(\ref{eq:Bp}),
we have $\lim_{p \rightarrow \infty} \mathbf{B}_p = \mathbf{U}_k$ from Proposition~\ref{lem:impappr} 
where $\mathbf{U}_k$ comes from rank-$k$ SVD of $\X_{\text{gd}}$.
As $k \ge \hat{k}_{\X_{\text{gd}}}$,
$\Span{\mathbf{U}_{\hat{k}_{\X_{\text{gd}}}}} \subseteq \Span{\mathbf{U}_k}$.
Then, 
from Proposition~\ref{pr:redGSVT},
we have
\begin{align*}
\mathbf{U}_k \Prox{\frac{\lambda}{\tau}r}{\mathbf{U}_k^{\top} \X_{\text{gd}}}
= \Prox{\frac{\lambda}{\tau}r}{\X_{\text{gd}}}.
\end{align*}
Thus,
$\lim_{p \rightarrow \infty} \tilde{\X}_p = \Prox{\frac{\lambda}{\tau} r}{\X_{\text{gd}}}$.
\end{proof}

\subsection{Proposition~\ref{pr:dc}}
\label{app:dc}

\begin{proof}
First,
we introduce Lemma~\ref{lem:svscvx}.

\begin{lemma}[\cite{lewis2005nonsmooth}]
\label{lem:svscvx}
Let $\phi(\mathbf{X}) = \sum_{i = 1}^m f(\sigma_i(\mathbf{X}))$.
If $f$ is convex, $\phi$ is also convex on $\X$.
\end{lemma}
For $\hat{r}$ in Assumption A3, it can be rewritten  as
$\hat{r}(\alpha) = \hat{r}_1(\alpha) - \hat{r}_2(\alpha)$,
where $\hat{r}_1(\alpha) = \kappa \alpha$ (for some
constant
$\kappa$)
and 
$\hat{r}_2(\alpha) = \kappa \alpha - \hat{r}(\alpha)$.
Obviously, both
$\hat{r}_1$ and
$\hat{r}_2$
are convex.
Define
\begin{align*}
\breve{r}(\mathbf{X}) = \sum_{i = 1}^m \hat{r}_1(\sigma_i(\X)),
\text{\;and\;}
\tilde{r}(\mathbf{X}) = \sum_{i = 1}^m \hat{r}_2(\sigma_i(\X)).
\end{align*}
From Lemma~\ref{lem:svscvx}, 
both $\breve{r}$ and $\tilde{r}$ are convex.
Thus, $r$ can also be written as a difference of convex functions:
$r(\X) = \breve{r}(\X) - \tilde{r}(\X)$.
\end{proof}

\subsection{Proposition~\ref{pr:boundseq}}
\label{app:boundseq}

\begin{proof}
From step~5 of Algorithm~\ref{alg:FaNCL} (which ensures \eqref{eq:decrease}),
we have
\begin{align*}
F(\X_{t+1}) \le F(\X_t) - c_1 \NM{\X_{t + 1} - \X_t}{F}^2.
\end{align*}
Summing this from $t = 1$ to $T$, we have
\begin{align}
c_1 \sum_{t = 1}^T \NM{\X_{t+1} - \X_t}{F}^2 
& \leq F(\X_1) - F(\X_{T + 1}) 
\notag
\\
& \leq F(\X_1) - \inf F.
\label{eq:temp6}
\end{align}
As $F$ is bounded from below (Assumption A2),
\begin{align*}
a_1 \equiv F(\X_1) - \inf F
\end{align*}
is a positive constant.
Let $T \rightarrow \infty$, we have
\begin{equation}
\label{eq:temp7}
\sum_{t = 1}^{\infty}
\NM{\X_{t+1} - \X_t}{F}^2 \le 
\frac{a_1}{c_1}.
\end{equation}
From Assumption A2,
we also have
$\lim_{\NM{\X}{F} \rightarrow \infty} f(\X) \rightarrow \infty$.
which implies that $\max_{t = 1, \dots, \infty} \NM{\X_t}{F} < \infty$.
%\footnote{$\surd$ (i). because $\NM{\X}{F} = \infty$ makes $f(\X) = \infty$; (ii). please jump similar questions related to A2, all similar papers use similar proof \cite{li2015accelerated,gongZLHY2013,attouch2013convergence}, and they also write the proof in this way.  *** why}
Together with \eqref{eq:temp7},
$\{ \X_t \}$ is a bounded sequence 
with at least one limit point
\cite{bertsekas1999nonlinear}.
\end{proof}

\subsection{Corollary~\ref{cor:rate}}
\label{app:rate}

\begin{proof}
Combining \eqref{eq:temp6} and \eqref{eq:temp7},
we have
\begin{eqnarray*}
\min_{t = 1, \dots, T} \NM{\X_{t+1} - \X_t}{F}^2
& \le & \frac{1}{T}\sum_{t = 1}^T \NM{\X_{t+1} - \X_t}{F}^2
\\
& \le & \frac{1}{T} \sum_{t = 1}^{\infty} \NM{\X_{t+1} - \X_t}{F}^2\\
& \le  &
\frac{F(\X_1) - \inf F}{c_1 T}.
\end{eqnarray*}
\end{proof}

\subsection{Theorem~\ref{the:FaNCL:conv}}
\label{app:convergence}

\begin{lemma}
\label{lem:inexactps}
$\InexactPS{ \X, \mathbf{R} }$,
i.e., Algorithm~\ref{alg:inexactPS}, 
is a continuous function on its input $\X$ and $\mathbf{R}$.
\end{lemma}

\begin{proof}
Note that the operations inside Algorithm~\ref{alg:inexactPS} on $\X$ and $\mathbf{R}$ 
are matrix addition,
multiplication,
taking gradient of $f$ on $\X$, 
and \textsf{QR} and \textsf{SVD}.
Since
(i) matrix addition and multiplication are linear operators;
(ii) $f$ is a smooth function;
and (iii) \textsf{QR}$(\cdot)$ and \textsf{SVD}$(\cdot)$ are smooth operators on the input matrix \cite{dieci1999smooth}.
Thus, 
Algorithm~\ref{alg:inexactPS} is a continuous function of its input matrices.
\end{proof}

\begin{lemma}[\cite{attouch2013convergence,gongZLHY2013}]
\label{lem:proxctri}
If $\X = \Prox{\frac{\lambda}{\tau} r}{\X - \frac{1}{\tau} \nabla f(\X)}$,
then $\X$ is a critical point of \eqref{eq:problem}.
\end{lemma}

\begin{proof}(Theorem~\ref{the:FaNCL:conv})	
%First, we introduce Lemma~\ref{lem:proxctri}.
As $\{ \X_{t_j} \}$ is a subsequence of $\{ \X_t \} $ with 
limit point
$\X_*$,
%When $t_j \rightarrow \infty$, we have
\begin{align}
\lim\limits_{t_j \rightarrow \infty} \X_{t_j + 1} 
& = \lim\limits_{t_j \rightarrow \infty} \InexactPS{ \X_{t_j}, \mathbf{R}_{t_j}},
\label{eq:app20}
\\
& = \InexactPS{\lim\limits_{t_j \rightarrow \infty}\X_{t_j}, \lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j}},
\label{eq:app1}
\end{align}
where \eqref{eq:app20} is due to continuity of $\InexactPS{\cdot, \cdot}$ 
	(Lemma~\ref{lem:inexactps}). 
Then,
from \eqref{eq:temp7},
we have
$\lim\limits_{t \rightarrow \infty} \NM{\X_{t + 1 } - \X_{t}}{F}^2 = 0$.
which implies
\begin{align}
\lim\limits_{t_j \rightarrow \infty} \X_{t_j + 1 }
= \lim\limits_{t_j \rightarrow \infty} \X_{t_j}  = \X_*,
\label{eq:app2}
\end{align}
for $\{ \X_{t_j} \}$.
Combining \eqref{eq:app1} and \eqref{eq:app2}, 
we have
\begin{align*}
\lim\limits_{t_j \rightarrow \infty} \X_{*} 
& =  \InexactPS{ \lim\limits_{t_j \rightarrow \infty} \X_{t_j}, 
	\lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j}},
\\
& =  \InexactPS{ \X_*, \lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j} }.
\end{align*}
Thus,
$\X_* = \Prox{\frac{\lambda}{\tau} r}{\X_* - \frac{1}{\tau} \nabla f(\X_*)}$ holds by the assumption.
From Lemma~\ref{lem:proxctri},
$\X_*$ is a critical point of \eqref{eq:problem}.
\end{proof}

\subsection{Proposition~\ref{pr:convacc}}
\label{app:convacc}

\begin{proof}
Consider the two cases:
\begin{enumerate}
\item Step~8 in Algorithm~\ref{alg:FaNCLacc} is performed: Then,
\begin{align}
F(\X_{t + 1}) \le F(\X_t) - \frac{\delta}{2} \NM{\X_{t + 1} - \mathbf{Y}_t}{F}^2.
\label{eq:temp13}
\end{align}
\item Step~10 is performed: Then,
\begin{align}
F(\X_{t+1}) \le F(\X_t) - c_1 \NM{\X_{t + 1} - \X_t}{F}^2.
\label{eq:temp11}
\end{align}
\end{enumerate}
Partition the iterations $\{ 1, \dots, T \}$ into two sets
$\Omega^1_T$ and $\Omega^2_T$,
such that $t \in \Omega^1_T$ if step~8 is performed, 
and $t \in \Omega^2_T$ 
if step~10 is performed.
Sum \eqref{eq:temp13} and \eqref{eq:temp11} from $t = 1$ to $T$,
%\footnote{$\surd$ *** $t_{j_1}$ and $t_j$ should just be $t$. same in app A.10}
\begin{align}
& F(\X_1) - F(\X_{T + 1}) 
\notag \\
& \ge 
\!\! \sum_{t \in \Omega^1_T} \! \frac{\delta}{2} \NM{\X_{t + 1} - \mathbf{Y}_t}{F}^2
+
\!\! \sum_{t \in \Omega^2_T} \! c_1 \NM{\X_{t + 1} - \mathbf{X}_t}{F}^2.
\label{eq:temp14}
\end{align}
%where $c_2 = \min(c_1, \delta/2)$.
As $F$ is bounded from below (Assumption~A2),
\begin{align}
\sum_{t \in \Omega^1_{\infty}} \!\! \frac{\delta}{2} \NM{\X_{t + 1} - \mathbf{Y}_t}{F}^2
\! + \!\!\! \sum_{t \in \Omega^2_\infty} \!\! c_1 \NM{\X_{t + 1} - \mathbf{X}_t}{F}^2 \le a_1.
\label{eq:temp15}
\end{align}
where $a_1 = F(\X_1) - \inf F > 0$ is a constant.
Consider the three cases:
\begin{enumerate}
\item $|\Omega^1_{\infty}|$ is finite but $|\Omega^2_{\infty}|$ is infinite:
For $t_j \in \Omega^2_{\infty}$, 
we have from (\ref{eq:temp15})
\begin{align}
\sum_{t_j \in \Omega^2_{\infty}} \NM{\X_{t_j + 1} - \mathbf{X}_{t_j}}{F}^2 
\le \frac{a_1}{c_1}.
\label{eq:app7}
\end{align}
From Assumption A2, we also have
\begin{align}
\lim\limits_{\NM{\X}{F} \rightarrow \infty} f(\X) =\infty,
\label{eq:app8}
\end{align}
which indicates that $\max_{t_j = 1, \dots, \infty} \NM{\X_{t_j}}{F} < \infty$.
Together with \eqref{eq:app7},
the sequence $\{ \X_t \}$ is bounded,
which has at least one limit point \cite{bertsekas1999nonlinear}.

%\vspace{5px}

\item $|\Omega^1_{\infty}|$ is infinite but $|\Omega^2_{\infty}|$ is finite:
For $t_j \in \Omega^1_{\infty}$,
note that, 
$F(\X_{t_j + 1}) \le F(\X_{t_j})$ due to \eqref{eq:temp13} and \eqref{eq:temp11}.
From Assumption A2,
\begin{align*}
\inf_{\X} F(\X) > -\infty,
\end{align*}
then the sequence $\{ F(\X_{t_j}) \}$ is bounded.
Again,
from Assumption A2,
we have \eqref{eq:app8},
then $\{ \X_{t_j} \}$ is also bounded which has at least one limit point \cite{bertsekas1999nonlinear}.


%\begin{align*}
%\sum_{t_j \in \Omega^2_{\infty}} \NM{\X_{t_j + 1} - \mathbf{Y}_{t_j}}{F}^2 \le \frac{2 a_1}{\delta},
%\end{align*}
%Then,\footnote{*** "then"?? this sentence is not related to prev sentence, right?} $F$ is lower-bounded and $\lim_{\NM{\X}{F} \rightarrow \infty} F(\X) = \infty$ as Assumption A1.
%These indicates $\{ F(\X_{t_j}) \}$ is a bounded sequence.
%Since $F$ is a continuous function, 
%\begin{align}
%\lim\limits_{t_j \rightarrow \infty} \X_{t_j + 1} - \mathbf{Y}_{t_j} = 0.
%\label{eq:temp16}
%\end{align}
%Finally, $\{ \mathbf{Y}_{t_j} \}$ has the same limit points as $\{ \X_{t_j} \}$ due to \eqref{eq:temp16}.

%%\vspace{5px}

\item Both $|\Omega^1_{\infty}|$ and $|\Omega^2_{\infty}|$ are infinite:
As in the above two cases,
$\{ \X_t \}$ is bounded when either of $|\Omega^1_{\infty}|$ and $|\Omega^2_{\infty}|$ is infinite.
\end{enumerate}
Combining the above,
$\{ \X_t \}$ generated from Algorithm~\ref{alg:FaNCLacc} is bounded and has at least one limit point.
\end{proof}

\subsection{Corollary~\ref{cor:rateacc}}
\label{app:rateacc}

\begin{proof}
Let $c_2 = \min(\delta/2, c_1)$.
From \eqref{eq:temp14}, we have
\begin{align}
& F(\X_1) - F(\X_{T + 1}) 
\notag 
\\
& \ge 
c_2 (\sum_{t \in \Omega^1_T} \!\! \NM{\X_{t + 1} - \mathbf{Y}_t}{F}^2
+
\sum_{t \in \Omega^2_T} \!\! \NM{\X_{t + 1} - \mathbf{X}_t}{F}^2 )
\notag 
\\
& = c_2 \sum_{t = 1}^T \NM{\X_{t + 1} - \mathbf{C}_t}{F}^2.
\label{eq:app14}
\end{align}
Thus,
\begin{align*}
\min_{t = 1, \dots, T} \NM{\X_{t + 1} - \mathbf{C}_t}{F}^2
& \le \frac{1}{T} \sum_{t = 1}^T \NM{\X_{t + 1} - \mathbf{C}_t}{F}^2
\\
& \le \frac{1}{T} \sum_{t = 1}^{\infty} \NM{\X_{t + 1} - \mathbf{C}_t}{F}^2
\\
& \le \frac{F(\X_1) - \inf F}{c_2 T},
\end{align*}
where the last inequity comes from \eqref{eq:app14}.
\end{proof}

\subsection{Theorem~\ref{thm:critacc}}
\label{app:critacc}

\begin{proof}
Partition the iterations $\{ 1, \dots, \infty \}$ into two sets $\Omega^1_{\infty}$ and $\Omega^2_{\infty}$,
such that $t \in \Omega^1_{\infty}$ if step~8 is performed, and
$t \in \Omega^2_{\infty}$ 
if step~10 is performed.
Consider the three cases:
\begin{enumerate}
	\item $|\Omega^1_{\infty}|$ is finite but $|\Omega^2_{\infty}|$ is infinite:
	Let $\{ \X_{ t_j } \}$ be a subsequence of $\{ \X_{ t } \}$
	where $t \in \Omega^2_{\infty}$,
	and $\lim_{t_j \rightarrow \infty } \X_{ t_j }= \X_*$.
	From \eqref{eq:app7}, we have
	\begin{align}
	\lim_{t_j \rightarrow \infty} \X_{ t_j } 
	= \lim_{t_j \rightarrow \infty} \X_{ t_j + 1 } 
	= \X_*.
	\label{eq:app3}
	\end{align}
	Besides,
	using Lemma~\ref{lem:inexactps},
	we have
	\begin{align}
	\!\!\!\!
	\lim\limits_{t_j \rightarrow \infty}
	\X_{t_j + 1} 
	\! = \! \InexactPS{ \lim\limits_{t_j \rightarrow \infty}  \X_{t_j},
		\lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j} }.
	\label{eq:app4}
	\end{align}
	Combining \eqref{eq:app3} and \eqref{eq:app4},
	we have
	\begin{align*}
	\lim\limits_{t_j \rightarrow \infty}
	\X_{t_j + 1} 
	& = \InexactPS{ \lim\limits_{t_j \rightarrow \infty} \X_{t_j}, 
		\lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j}}
	\\
	& = \InexactPS{\X_{*}, \lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j}} = \X_*.
	\end{align*}
	Thus, by the assumption, 
	we also have
	\begin{align*}
	\X_* = \Prox{\frac{\lambda}{\tau} r}{\X_* - \frac{1}{\tau} \nabla f(\X_*)}.
	\end{align*}
	From Lemma~\ref{lem:proxctri},
	$\X_*$ is also a critical point of \eqref{eq:problem}.
	
	%%\vspace{5px}
	
	\item $|\Omega^1_{\infty}|$ is infinite but $|\Omega^2_{\infty}|$ is finite:
	Let $\{ \X_{ t_j } \}$ be a subsequence of $\{ \X_{ t } \}$
	where $t \in \Omega^1_{\infty}$,
	and $\lim_{t_j \rightarrow \infty } \X_{ t_j }= \X_*$.
	From \eqref{eq:temp14},
	we have
	\begin{align*}
	\sum_{t \in \Omega^1_{\infty}} \! \frac{\delta}{2} \NM{\X_{t + 1} - \mathbf{Y}_t}{F}^2 
	< \infty
	\end{align*}
	which indicates
	\begin{align}
	\lim\limits_{t_j \rightarrow \infty} \X_{t_j + 1} - \mathbf{Y}_{t_j} = 0.
	\label{eq:temp16}
	\end{align}

	From \eqref{eq:temp16},
	we have
	\begin{align}
	\lim_{t_j \rightarrow \infty} \mathbf{Y}_{ t_j } 
	= \lim_{t_j \rightarrow \infty} \X_{ t_j  + 1  } 
	= \X_*.
	\label{eq:app5}
	\end{align}
	Besides,
	using Lemma~\ref{lem:inexactps}, we have
	\begin{align}
	\!\!\!\!
	\lim\limits_{t_j \rightarrow \infty}
	\X_{t_j + 1} 
	\! = \! \InexactPS{
		\lim\limits_{t_j \rightarrow \infty} \mathbf{Y}_{t_j}, 
		\lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j}}.
	\label{eq:app6}
	\end{align}
	Combining \eqref{eq:app5} and \eqref{eq:app6},
	we have
	\begin{align*}
	\lim\limits_{t_j \rightarrow \infty}
	\X_{t_j + 1} 
	& = \InexactPS{ \lim\limits_{t_j \rightarrow \infty} \mathbf{Y}_{t_j},
		\lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j} }
	\\
	& = \InexactPS{ \X_{*}, \lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j} } = \X_*.
	\end{align*}
	Thus, by the assumption, 
	we also have
	\begin{align*}
	\X_* = \Prox{\frac{\lambda}{\tau} r}{\X_* - \frac{1}{\tau} \nabla f(\X_*)}.
	\end{align*}
	From Lemma~\ref{lem:proxctri},
	$\X_*$ is also a critical point of \eqref{eq:problem}.
	
	%%\vspace{5px}

	\item Both $|\Omega^1_{\infty}|$ and $|\Omega^2_{\infty}|$ are infinite:
	From the above two cases, we can see that the limit point $\X_*$ is also a critical point of \eqref{eq:problem} when either 
	$|\Omega^1_{\infty}|$ or $|\Omega^2_{\infty}|$ is infinite.
	
\end{enumerate}
Thus,
limit points of $\{ \X_t \}$ are also critical points of \eqref{eq:problem}. 
\end{proof}

\subsection{Proposition~\ref{pr:convrpca}}
\label{app:pr:convrpca}

\begin{proof}
Consider the two cases:
\begin{enumerate}
\item Steps~10 and 11 are performed: Then,
%\footnote{checking up to this point. pls chk the rest first}
\begin{align}
F(\X_{t + 1}, \mathbf{S}_{t + 1}) 
& \le F(\X_t, \mathbf{S}_t) 
\label{eq:temp19}
\\
- \frac{\delta}{2} & (\NM{\X_{t + 1} - \mathbf{Y}_t^{ \mathbf{X}} }{F}^2 + \NM{\mathbf{S}_{t + 1} 
- \mathbf{Y}_t^{ \mathbf{S}} }{F}^2).
\notag
\end{align}

\item Steps~13 and 14 are performed: Then,
\begin{align}
F(\X_{t + 1}, \mathbf{S}_{t + 1}) 
\le F(\X_t, \mathbf{S}_t) & 
\label{eq:temp20}
\\
- (c_1 \NM{\X_{t + 1} - \mathbf{X}_t}{F}^2 
+ & \frac{\tau - \rho}{2} \NM{\mathbf{S}_{t + 1} - \mathbf{S}_t}{F}^2).
\notag
\end{align}
\end{enumerate}

Partition $\{ 1, \dots, T \}$ into two sets as $\Omega^1_T$ and $\Omega^2_T$,
where $t \in \Omega^1_T$ if steps~10-11 are performed; otherwise, $t \in \Omega^2_T$ (and
steps~13-14 are performed).
Let
\begin{align*}
\Theta_T
= 
& \sum_{t \in \Omega^1_T} \!\! \frac{\delta}{2}
(\NM{\X_{t + 1} - \mathbf{Y}_t^{\mathbf{X}}}{F}^2 + \NM{\mathbf{S}_{t + 1} - \mathbf{Y}_t^{\mathbf{S}}}{F}^2)
\\
& + \sum_{t \in \Omega^2_T} \!\! (c_1 \NM{\X_{t + 1} - \mathbf{X}_t}{F}^2 
+ \frac{\tau - \rho}{2} \NM{\mathbf{S}_{t + 1} - \mathbf{S}_t}{F}^2).
\end{align*}
Summing \eqref{eq:temp19} and \eqref{eq:temp20} from $t = 1$ to $T$,
we have
\begin{align}
F(\X_1, \mathbf{S}_1) 
- F(\X_{T + 1}, \mathbf{S}_{T + 1}) 
\ge 
\Theta_T.
\label{eq:temp21}
\end{align}
As $F$ is bounded from below (Assumption~A2),
we have 
\begin{align}
\Theta_{\infty} \le a_2,
\label{eq:temp22}
\end{align}
where $a_2 = F(\X_1, \mathbf{S}_1) - \inf F$.
We consider the three cases:
\begin{enumerate}
	\item $|\Omega^1_{\infty}|$ is finite but $|\Omega^2_{\infty}|$ is infinite;
	For $t_j \in \Omega^2_{\infty}$,
	\begin{align*}
	\sum_{t_j \in \Omega^2_{\infty}} \NM{\X_{t_j + 1} - \mathbf{X}_{t_j}}{F}^2 
	& \le \frac{a_2}{c_1},
	\\
	\sum_{t_j \in \Omega^2_{\infty}} \NM{\mathbf{S}_{t_j + 1} - \mathbf{S}_{t_j}}{F}^2
	& \le \frac{2 a_2}{\tau - \rho}.
	\end{align*}
	Again from Assumption~A2,
	we have
	\begin{align}
	\lim\limits_{\NM{\X}{F} \rightarrow \infty \;\text{or}\; 
	\NM{\mathbf{S}}{F} \rightarrow \infty} f(\mathbf{X}, \mathbf{S})
	\rightarrow \infty.
	\label{eq:app9}
	\end{align}
	Thus,
	$\max_{t_j = 1, \dots, \infty} \NM{ [\mathbf{X}_{t_j}, \mathbf{S}_{t_j}] }{F} < \infty$,
	and the sequence $\{ [\mathbf{X}_{t_j}, \mathbf{S}_{t_j}] \}$ is bounded with at least one limit point \cite{bertsekas1999nonlinear}.
	
	%%\vspace{5px}
	
	\item $|\Omega^1_{\infty}|$ is infinite but $|\Omega^2_{\infty}|$ is finite:
	For $t_j \in \Omega^1_{\infty}$,
	from \eqref{eq:temp19} and \eqref{eq:temp20}, 
	we have 
	$	F(\mathbf{X}_{t_j + 1}, \mathbf{S}_{t_j + 1}) 
	\le F(\mathbf{X}_{t_j}, \mathbf{S}_{t_j})$.
	As, 
	$F$ is bounded from below (Assumption~A2), 
	the sequence $\{ F([\mathbf{X}_{t_j}, \mathbf{S}_{t_j}]) \}$ must be bounded.
	Besides,
	again from Assumption~A2,
	we have \eqref{eq:app9},
	which indicates
	the sequence $[\mathbf{X}_{t_j}, \mathbf{S}_{t_j}])$ must be bounded with at least one limit point \cite{bertsekas1999nonlinear}.

	%%\vspace{5px}
	
	\item Both $|\Omega^1_{\infty}|$ and $|\Omega^2_{\infty}|$ are infinite:
	As in the above two cases,
	$\{ [\mathbf{X}_{t}, \mathbf{S}_{t}] \}$ is bounded with at least one limit point
	once $|\Omega^1_{\infty}|$ or $|\Omega^2_{\infty}|$ is infinite.
\end{enumerate}
Thus,
the sequence $\{ [\mathbf{X}_{t}, \mathbf{S}_{t}] \}$ generated from
Algorithm~\ref{alg:FaNCLacc} is bounded and has at least one limit point.
\end{proof}

\subsection{Corollary~\ref{cor:raterpca}}
\label{app:raterpca}

\begin{proof}
Let $c_2 = \min(\delta/2, c_1)$. 
First, we have
\begin{align} 
& \sum_{t \in \Omega^1_T} \!\! \frac{\delta}{2}
(\NM{\X_{t + 1} - \mathbf{Y}_t^{\mathbf{X}}}{F}^2 + \NM{\mathbf{S}_{t + 1} - \mathbf{Y}_t^{\mathbf{S}}}{F}^2)
\notag
\\
& + \sum_{t \in \Omega^2_T} \!\! (c_1 \NM{\X_{t + 1} - \mathbf{X}_t}{F}^2 
+ \frac{\tau - \rho}{2} \NM{\mathbf{S}_{t + 1} - \mathbf{S}_t}{F}^2)
\notag
\\
& \ge c_2 \sum_{t = 1}^T \NM{[\X_{t + 1}, \mathbf{S}_{t + 1}] - \mathbf{C}_t}{F}^2.
\label{eq:app15}
\end{align}
Together with \eqref{eq:temp21} and \eqref{eq:temp22}, we have
\begin{align*}
\min_{t = 1, \dots, T} & \NM{[\X_{t + 1}, \mathbf{S}_{t + 1}] - \mathbf{C}_t}{F}^2
\\
& \le \frac{1}{T} \sum_{t = 1}^T \NM{[\X_{t + 1}, \mathbf{S}_{t + 1}] - \mathbf{C}_t}{F}^2
\\
& \le \sum_{t = 1}^{\infty} \NM{[\X_{t + 1}, \mathbf{S}_{t + 1}] - \mathbf{C}_t}{F}^2
\\
& \le \frac{F(\X_1, \mathbf{S}_1) - \inf F}{c_2 T},
\end{align*}
where the last inequality comes from \eqref{eq:app15}.
\end{proof}

\subsection{Theorem~\ref{thm:convrpca}}
\label{app:thm:convrpca}

\begin{proof}
Let $g = \breve{g} + \tilde{g}$ be the difference of convex decomposition of $g$.
As two blocks of variables are involved, 
its critical points are defined as follows.

\begin{definition}
[\cite{hiriart85}]
If $\X$ and $\mathbf{S}$ satisfy
\begin{align*}
\mathbf{0} 
& \in \nabla_{\mathbf{X}} f(\X, \mathbf{S}) \! + \! 
\lambda \left( \partial \breve{r}(\X) \! - \! \partial \tilde{r}(\X) \right),
\\
\mathbf{0}
& \in \nabla_{\mathbf{S}} f(\X, \mathbf{S}) + 
\lambda \left( \partial \breve{g}(\mathbf{S}) - \partial \tilde{g}(\mathbf{S}) \right),
\end{align*}
then $[\X, \mathbf{S}]$ is a {\em critical point} of $F$.
\end{definition}

\begin{lemma}[\cite{attouch2013convergence}] \label{lem:crtiblk}
If $\X$ and $\mathbf{S}$ satisfy
\begin{align*}
\mathbf{X} 
& = \Prox{\frac{\lambda}{\tau} r}{\mathbf{X} 
\! - \! \frac{1}{\tau} \nabla_{\mathbf{X}} f (\mathbf{X}, \mathbf{S}) },
\\
\mathbf{S} 
& = \Prox{\frac{\nu}{\tau} g}{\mathbf{S} - \frac{1}{\tau} \nabla_{\mathbf{S}} f (\mathbf{X}, \mathbf{S}) },
\end{align*}
then $[\X, \mathbf{S}]$ is a critical point of $F$.
\end{lemma}


Partition $\{ 1, \dots, \infty \}$ into two sets as $\Omega^1_{\infty}$ and $\Omega^2_{\infty}$,
where $t \in \Omega^1_\infty$ if steps~10-11 are performed; otherwise, $t \in \Omega^2_\infty$ (and
steps~13-14 are performed),
we consider three cases here.
\begin{enumerate}
	\item $|\Omega^1_{\infty}|$ is finite but $|\Omega^2_{\infty}|$ is infinite:
	Let $\{ [\mathbf{X}_{t_j}, \mathbf{S}_{t_j}] \}$ be a subsequence of $\{ [\mathbf{X}_{t}, \mathbf{S}_{t}] \}$ where $t \in \Omega^2_{\infty}$,
	and
	\begin{align*}
	\lim_{ t_j \rightarrow \infty } \left[ \mathbf{X}_{t_j}, \mathbf{S}_{t_j} \right]
	= \left[ \mathbf{X}_*, \mathbf{S}_* \right].
	\end{align*}
	From \eqref{eq:temp21},
	we have
	\begin{align*}
	\sum_{t = 1}^{\infty}
	\NM{\X_{t + 1} - \mathbf{X}_{t}}{F}^2
	< \infty,
	\quad
	\sum_{t = 1}^{\infty}
	\NM{\mathbf{S}_{t + 1} - \mathbf{S}_{t}}{F}^2
	< \infty.
	\notag
	\end{align*}
	These indicate
	\begin{align}
	\lim\limits_{t_j \rightarrow \infty}
	\X_{t_j + 1} - \mathbf{X}_{t_j}
	& = \mathbf{0},
	\label{eq:temp25}
	\\
	\lim\limits_{t_j \rightarrow \infty}
	\mathbf{S}_{t_j + 1} - \mathbf{S}_{t_j}
	& = \mathbf{0}.
	\label{eq:temp38}
	\end{align}
	From \eqref{eq:temp25}, we have
	\begin{align}
	\lim_{ t_j \rightarrow \infty } \mathbf{X}_{t_j + 1}
	= \lim_{ t_j \rightarrow \infty } \mathbf{X}_{t_j} = \mathbf{X}_*.
	\label{eq:app10}
	\end{align}
	Combing \eqref{eq:temp25} and \eqref{eq:app10},
	we have
	\begin{align}
	\!\!\!\!\!\!\!\!\!\!\!\!\!
	\lim\limits_{t_j \rightarrow \infty}
	\mathbf{X}_{t_j + 1} 
	& = \lim\limits_{t_j \rightarrow \infty} \InexactPS{ \mathbf{X}_{t_j}, \mathbf{R}_{t_j}}
	\label{eq:app21}
	\\
	& = \InexactPS{ \lim\limits_{t_j \rightarrow \infty} \mathbf{X}_{t_j},
		\lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j}}
	\notag
	\\
	& =  \InexactPS{ \mathbf{X}_*, \lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j} } = \X_*.
	\notag
	\end{align}
	where \eqref{eq:app21} comes from Lemma~\ref{lem:inexactps}.
	Thus,
	\begin{align}
	\X_* = \Prox{\frac{\lambda}{\tau} r}{\X_* - \frac{1}{\tau} \nabla_{\X} f(\X_*, \mathbf{S}_*) }
	\label{eq:temp26}
	\end{align}
	holds by the assumption.
	Then,
	the proximal operator is always exact for $\mathbf{S}$. 
	Using \eqref{eq:temp38},
	we have 
	\begin{align}
	\lim\limits_{t_j \rightarrow \infty}
	\mathbf{S}_{t_j + 1} 
	& =
	\lim\limits_{t_j \rightarrow \infty}
	\Prox{\frac{\mu}{\tau} g}{\mathbf{S}_{t_j} - \frac{1}{\tau} \nabla_{\mathbf{S}} f(\mathbf{X}_{t_j}, \mathbf{S}_{t_j})}
	\notag
	\\
	& =
	\Prox{\frac{\mu}{\tau} g}{\mathbf{S}_{*} - \frac{1}{\tau} \nabla_{\mathbf{S}} f(\mathbf{X}_{*}, \mathbf{S}_{*})}
	\notag
	\\
	& = \mathbf{S}_*
	\label{eq:temp27}
	\end{align}
	Combining with \eqref{eq:temp26} and \eqref{eq:temp27},
	$[\mathbf{X}_*, \mathbf{S}_*]$ is a critical point of \eqref{eq:rpca} by using Lemma~\ref{lem:crtiblk}.
	
	%%\vspace{5px}
	
	\item $|\Omega^1_{\infty}|$ is infinite but $|\Omega^2_{\infty}|$ is finite:
	Let $\{ [\mathbf{X}_{t_j}, \mathbf{S}_{t_j}] \}$ be a subsequence of $\{ [\mathbf{X}_{t}, \mathbf{S}_{t}] \}$ where $t \in \Omega^1_{\infty}$,
	and
	\begin{align*}
	\lim_{ t_j \rightarrow \infty }
	 \left[ \mathbf{X}_{t_j}, \mathbf{S}_{t_j} \right]
	= \left[ \mathbf{X}_*, \mathbf{S}_* \right].
	\end{align*}
	From \eqref{eq:temp21},
	we have
	\begin{align*}
	\!\!\!\!\!\!\!\!\!
	\sum_{t_j \in \Omega^2_{\infty}} 
	\!\!\!
	\NM{\X_{t_j + 1} \! - \! \mathbf{Y}^{\mathbf{X}}_{t_j}}{F}^2 
	\le \infty,
	\sum_{t_j \in \Omega^2_{\infty}} 
	\!\!\!
	\NM{\mathbf{S}_{t_j + 1} 
		\! - \! \mathbf{Y}^{\mathbf{S}}_{t_j}}{F}^2
	\le \infty,
	\end{align*}
	and then
	\begin{align}
	\lim\limits_{t_j \rightarrow \infty}
	\X_{t_j + 1} - \mathbf{Y}^{\mathbf{X}}_{t_j}
	& = \mathbf{0},
	\label{eq:temp23}
	\\
	\lim\limits_{t_j \rightarrow \infty}
	\mathbf{S}_{t_j + 1} - \mathbf{Y}^{\mathbf{S}}_{t_j}
	& = \mathbf{0}.
	\label{eq:temp24}
	\end{align}
	Thus,
	\begin{align}
	\lim_{ t_j \rightarrow \infty } \mathbf{X}_{t_j + 1}
	= \lim_{ t_j \rightarrow \infty } \mathbf{Y}^{\mathbf{X}}_{t_j} 
	= \mathbf{X}_*.
	\label{eq:app11}
	\end{align}

	
	Combing \eqref{eq:temp23} and \eqref{eq:app11},
	we have
	\begin{align}
	\!\!\!\!\!\!\!\!\!
	\lim\limits_{t_j \rightarrow \infty}
	\mathbf{X}_{t_j + 1} 
	& = \lim\limits_{t_j \rightarrow \infty} \InexactPS{  \mathbf{Y}^{\mathbf{X}}_{t_j}, \mathbf{R}_{t_j} }
	\label{eq:app22}
	\\
	& = \InexactPS{ \lim\limits_{t_j \rightarrow \infty} \mathbf{Y}^{\mathbf{X}}_{t_j},
		\lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j} }
	\notag
	\\
	& =  \InexactPS{ \mathbf{X}_*, \lim\limits_{t_j \rightarrow \infty} \mathbf{R}_{t_j} }
	= \X_*.
	\notag
	\end{align}
	where \eqref{eq:app22} comes from Lemma~\ref{lem:inexactps}.
	Thus,
	\begin{align}
	\X_* = \Prox{\frac{\lambda}{\tau} r}{\X_* - \frac{1}{\tau} \nabla_{\X} f(\X_*, \mathbf{S}_*) }
	\label{eq:app12}
	\end{align}
	holds by the assumption.
	Then,
	the proximal operator is always exact for $\mathbf{S}$.
	Using \eqref{eq:temp24}, 
	\begin{align}
	\lim\limits_{t_j \rightarrow \infty}
	\mathbf{S}_{t_j + 1} 
	& = \!\!\!
	\lim\limits_{t_j \rightarrow \infty}
	\Prox{\frac{\mu}{\tau} g}{\mathbf{Y}^{\mathbf{S}}_{t_j} - \frac{1}{\tau} \nabla_{\mathbf{S}} f(\mathbf{Y}^{\mathbf{X}}_{t_j}, \mathbf{Y}^{\mathbf{S}}_{t_j})}
	\notag
	\\
	& =
	\Prox{\frac{\mu}{\tau} g}{\mathbf{S}_{*} - \frac{1}{\tau} \nabla_{\mathbf{S}} f(\mathbf{X}_{*}, \mathbf{S}_{*})}
	\notag
	\\
	& = \mathbf{S}_*
	\label{eq:app13}
	\end{align}
	Combining with \eqref{eq:app12} and \eqref{eq:app13},
	$[\mathbf{X}_*, \mathbf{S}_*]$ is a critical point of \eqref{eq:rpca} by using Lemma~\ref{lem:crtiblk}.
	
	%%\vspace{5px}
	
	\item Both $|\Omega^1_{\infty}|$ and $|\Omega^2_{\infty}|$ are infinite:
	As above, 
	either $|\Omega^1_{\infty}|$ or $|\Omega^2_{\infty}|$ is infinite,
	a limit point $[\mathbf{X}_*, \mathbf{S}_*]$ is a critical point of \eqref{eq:rpca}.
\end{enumerate}
Thus,
the limit points of the sequence $\{ [\mathbf{X}_t, \mathbf{S}_t] \}$ are also critical points of \eqref{eq:rpca}.
\end{proof}

\subsection{Proposition~\ref{pr:qr}}
\label{app:qr}

\begin{proof}
As the SVD of $\X_{\text{gd}}^{\top} \X_{\text{gd}}$ is $\mathbf{V} \mathbf{\Sigma} \mathbf{V}^{\top}$,
the SVD of $\X_{\text{gd}}$ can be written as $\mathbf{U} \mathbf{\Sigma}^{\frac{1}{2}} \mathbf{V}^{\top}$
where $\mathbf{U}$ is an orthogonal matrix containing the span of $\X_{\text{gd}}$.
From the construction of $\mathbf{w}$,
we have
\begin{align*}
\X_{\text{gd}} \mathbf{V}
\left(  \Diag{\mathbf{w}} \right)^{-\frac{1}{2}}
& = \mathbf{U} \mathbf{\Sigma}^{\frac{1}{2}} ( \mathbf{V}^{\top} \mathbf{V} ) 
\left( \Diag{\mathbf{w}} \right)^{-\frac{1}{2}}
\\
& = \mathbf{U} \mathbf{\Sigma}^{\frac{1}{2}} 
\left( \Diag{\mathbf{w}} \right)^{-\frac{1}{2}}.
\end{align*}
Consider the two cases.
\begin{enumerate}
\item $\X_{\text{gd}}$ is of full column rank: Then,
\begin{align*}
\mathbf{U} \mathbf{\Sigma}^{\frac{1}{2}}
\left( \Diag{\mathbf{w}} \right)^{ - \frac{1}{2}} 
=
\mathbf{U} \left( \mathbf{\Sigma}^{\frac{1}{2}}
\mathbf{\Sigma}^{ - \frac{1}{2}} \right) 
= 
\mathbf{U},
\end{align*}
which contains
the span of $\X_{\text{gd}}$.

\item Assume that $\X_{\text{gd}}$ has $k$ columns and its rank is $\bar{k} < k$: Then,
\begin{align*}
&
\mathbf{U} \mathbf{\Sigma}^{\frac{1}{2}} \left( \Diag{\mathbf{w}} \right)^{- \frac{1}{2}} 
\\
&
=\mathbf{U}
\text{Diag}\left( \Sigma_{11}^{\frac{1}{2}}, \dots, \Sigma_{\bar{k} \bar{k}}^{\frac{1}{2}}, 
0, \dots, 0 \right)
\\
& \quad\quad\;
\text{Diag}
\left( 
\Sigma_{11}^{-\frac{1}{2}}, \!\dots\!, \Sigma_{\bar{k} \bar{k}}^{-\frac{1}{2}}, 1, \!\dots\!, 1
\right)
= \left[ \mathbf{U}_{\bar{k}}, \mathbf{0} \right],
\end{align*}
where $\mathbf{U}_{\bar{k}}$ contains the first $\bar{k}$ columns of $\mathbf{U}$.
As $\X_{\text{gd}}$ is only of rank $\bar{k}$, 
$\mathbf{U} \mathbf{\Sigma}^{\frac{1}{2}} \left( \Diag{\mathbf{w}} \right)^{- \frac{1}{2}}$ again covers the span of $\X_{\text{gd}}$.
\end{enumerate}
The Proposition then follows.
\end{proof}

\subsection{Proposition~\ref{pr:rsvd}}
\label{app:rsvd}

\begin{proof}
Let the SVD of $\mathbf{B}$ be $\bar{\mathbf{U}} \bar{\mathbf{\Sigma}} \bar{\mathbf{V}}^{\top}$.
Then,
\begin{align*}
\mathbf{P}^{\top} \mathbf{B}
= (  \mathbf{P}^{\top} \bar{\mathbf{U}} )  \bar{\mathbf{\Sigma}} \bar{\mathbf{V}}^{\top}.
\end{align*}
Note that
\begin{align*}
(  \mathbf{P}^{\top} \bar{\mathbf{U}} )^{\top} \mathbf{P}^{\top} \bar{\mathbf{U}} 
& = \bar{\mathbf{U}}^{\top} ( \mathbf{P} \mathbf{P}^{\top} ) \bar{\mathbf{U}} 
\\
& = \bar{\mathbf{U}}^{\top} ( \mathbf{\bar{U}} \mathbf{\bar{U}}^{\top} ) \bar{\mathbf{U}} 
= \mathbf{I},
\end{align*}
where the second equality comes from 
\begin{align}
\Span{\mathbf{P}} = \Span{\mathbf{\bar{U}}}.
\label{eq:temp30}
\end{align}
Thus, 
the SVD of $\mathbf{P}^{\top} \mathbf{B}$ is
$(  \mathbf{P}^{\top} \bar{\mathbf{U}} ) \bar{\mathbf{\Sigma}} \bar{\mathbf{V}}$.
As a result, 
we have $\mathbf{V} = \bar{\mathbf{V}}$, $\mathbf{\Sigma} = \bar{\mathbf{\Sigma}}$.
Finally,  from $\mathbf{U} = \mathbf{P}^{\top} \mathbf{\bar{U}}$, we have
\begin{align*}
\mathbf{P} \mathbf{U} = \mathbf{P} \mathbf{P}^{\top} \bar{\mathbf{U}}
= \mathbf{\bar{U}} ( \mathbf{\bar{U}}^{\top} \bar{\mathbf{U}} ) 
= \bar{\mathbf{U}},
\end{align*} 
where the second equality again comes from \eqref{eq:temp30}.
\end{proof}



\section{The Checking Condition
	in \cite{attouch2013convergence}}
\label{sec:extchk}

%\subsection{Extra Condition}

In \cite{attouch2013convergence}, 
an approximate $\tilde{\mathbf{X}}_p$ is
accepted if
$\exists \X_{\text{gd}} \in \partial \breve{r}(\tilde{\mathbf{X}}_p) - \partial \tilde{r}(\tilde{\mathbf{X}}_p)$,
where $\tilde{r}$ and $\breve{r}$ are convex functions
such that 
$\NM{\X_{\text{gd}} + \nabla f(\tilde{\mathbf{X}}_p)}{F}^2 \le b \NM{\tilde{\mathbf{X}}_p - \mathbf{X}}{F}^2$ for some constant $b > 0$.
Thus,
to find such $\X_{\text{gd}}$,
we first need to compute 
$\partial \breve{r}(\tilde{\mathbf{X}}_p)$ and $\partial \tilde{r}(\tilde{\mathbf{X}}_p)$.

Taking the LSP regularizer as an example.
Using Proposition~\ref{pr:dc},
we can decompose $r(\tilde{\mathbf{X}}_p)$ as $\breve{r}(\tilde{\mathbf{X}}_p) +
\tilde{r}(\tilde{\mathbf{X}}_p)$, where
\begin{eqnarray*}
	\breve{r}(\tilde{\mathbf{X}}_p) 
	& = & \frac{1}{\theta} \NM{\tilde{\mathbf{X}}_p}{*}, 
	\\
	\tilde{r}(\tilde{\mathbf{X}}_p) 
	& = & \sum_{i = 1}^n 
	\left[ \frac{\sigma_i(\tilde{\mathbf{X}}_p)}{\theta} - \log\left( 1 + \frac{\sigma_i(\tilde{\mathbf{X}}_p)}{\theta} \right) \right].
\end{eqnarray*}
Let the SVD of $\tilde{\mathbf{X}}_p$ be $\mathbf{U} \mathbf{\Sigma} \mathbf{V}^{\top}$.
Assume that $\tilde{\mathbf{X}}_p$ has $k$ singular values larger than $0$.
Let $\mathbf{U}_{k}$ (resp. $\mathbf{V}_{k}$) be the matrix containing the
first $k$ columns of $\mathbf{U}$ (resp. $\mathbf{V}$).
Then, 
\begin{align*}
\partial \breve{r}(\tilde{\mathbf{X}}_p) 
=\frac{1}{\theta} \left( \mathbf{U}_{k} \mathbf{V}_{k}^{\top} +\mathbf{B} \right),
\end{align*}
where $\mathbf{B} \in \{ \mathbf{C} : \mathbf{U}_{k}^{\top} \mathbf{C} = 0, 
\mathbf{C} \mathbf{V}_{k} = 0, \;\text{and}\; \sigma_1(\mathbf{C}) \le 1 \}$.
Let $\mathbf{c} = [c_i]$ with $c_i = \frac{1}{\theta} - \frac{1}{\sigma_i(\X_p) + \theta}$.
Then,
\begin{align*}
\partial \tilde{r}(\tilde{\mathbf{X}}_p) 
= \mathbf{U} \Diag{\mathbf{c}}  \mathbf{V}^{\top}.
\end{align*}
Thus,
a full SVD on $\tilde{\mathbf{X}}_p$ is needed, 
which is expensive and impractical for large matrices.

\newpage

\section{Parallel \textsf{FaNCL-acc}}
\label{app:parFaNCL}

Algorithm~\ref{alg:accparallel} shows
the parallel version of \textsf{FaNCL-acc}.
Acceleration is performed at step~6.
The first inexact proximal step is performed at steps~8-18.
Step~19
checks
whether
the accelerated iterate is
accepted.
If the condition fails,
a second inexact proximal step is performed at steps~22-32.
Note that the algorithm is equivalent to Algorithm~\ref{alg:FaNCLacc}, and
thus the convergence analysis in Section~\ref{sec:accFaNCL} still holds.


\begin{algorithm}[]
	\caption{\textsf{FaNCL-acc} in parallel: \textsf{FaNCL-acc-PL}.}
	\begin{algorithmic}[1]
		\REQUIRE choose $\tau > \rho$, $\lambda_0 > \lambda$, $\delta > 0$ and $\nu \in (0,1)$;
		
		\STATE initialize $\mathbf{V}_0, \mathbf{V}_1 \in \R^{n}$ as random Gaussian matrices, $\X_0 = \X_1 = 0$
		and $\alpha_0 = \alpha_1 = 1$;
		
		\STATE partition $\X_0$, $\X_1$, $\SO{\X_0}$, $\SO{\X_1}$ and $\SO{\mathbf{O}}$;
		
		\STATE start $q$ threads for parallelization;
		
		\FOR{$t = 1,2,\dots T$}
		
		\STATE $\lambda_t = (\lambda_{t-1} -\lambda) \nu^t + \lambda$;
		
		\STATE $\rhd$ $\mathbf{Y}_t = \X_t + \frac{\alpha_{t - 1} - 1}{\alpha_t}(\X_t - \X_{t - 1})$;
		
		\STATE $\rhd$ $\mathbf{R}_t = \textsf{IndeSpan-PL}\left( [\mathbf{V}_t, \mathbf{V}_{t - 1}] \right)$;
		
		\STATE $\rhd$ $\left( \X_{\text{gd}} \right)^a_t = \mathbf{Y}_t - \frac{1}{\tau} \SO{\mathbf{Y}_t - \mathbf{O}}$;
		
		\FOR{$p = 1, 2,\dots$}
		\STATE $\rhd$ $[ \tilde{\mathbf{X}}_p, \mathbf{R}_t ]  = 
		\textsf{ApproxGSVT-PL} ( (\X_{\text{gd}})^a_t, \mathbf{R}_t, \frac{\lambda}{\tau} )$;
		
		\STATE $\rhd$ $a_p = F(  \tilde{\mathbf{X}}_p )$;
		
		\STATE $\rhd$ $a_t \, = F (\X_t)$;
		
		\STATE $\rhd$ $a_F = \| \tilde{\mathbf{X}}_p - \X_t \|_F^2$;
		
		\IF{$a_p \le a_t - c_1 a_F$}
		\STATE break;
		\ENDIF
		
		\ENDFOR
		
		\STATE $\rhd$ $\X^a_{t + 1} = \tilde{\mathbf{X}}_p$;
		
		\IF{$F(\X^a_{t + 1}) \le F(\X_t) - \frac{\delta}{2} \NM{\X^a_{t + 1} - \mathbf{Y}_t}{F}^2$}
		
		\STATE $\rhd$ $\X_{t + 1} = \X^a_{t + 1}$;
		
		\ELSE
		
		\STATE $\rhd$ $(\X_{\text{gd}})_t = \X_t - \frac{1}{\tau} \SO{\X_t - \mathbf{O}}$;
		
		\FOR{$p = 1, 2,\dots$}
		\STATE $\rhd$ $[ \tilde{\mathbf{X}}_p, \mathbf{R}_t ]  = 
		\textsf{ApproxGSVT-PL} ( (\X_{\text{gd}})_t, \mathbf{R}_t, \frac{\lambda}{\tau} )$;
		
		\STATE $\rhd$ $b_p = F(  \tilde{\mathbf{X}}_p )$;
		
		\STATE $\rhd$ $b_t \, = F (\X_t)$;
		
		\STATE $\rhd$ $b_F = \| \tilde{\mathbf{X}}_p - \X_t \|_F^2$;
		
		\IF{$b_p \le b_t - c_1 b_F$}
		\STATE break;
		\ENDIF
		
		\ENDFOR
		
		\STATE $\rhd$ $\X_{t+1} = \tilde{\X}_p$;
		
		\ENDIF
		
		\STATE $\alpha_{t + 1} = \frac{1}{2} (\sqrt{4\alpha_t^2 + 1} + 1)$;
		
		\ENDFOR
		
		\RETURN $\X_{T + 1}$.
	\end{algorithmic}
	\label{alg:accparallel}
\end{algorithm}


\end{document}

