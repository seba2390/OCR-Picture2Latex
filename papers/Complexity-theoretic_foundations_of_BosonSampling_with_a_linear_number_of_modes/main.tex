\documentclass[11pt]{article}
\pdfoutput=1 

\usepackage{authblk}
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amssymb,amsfonts,setspace}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{braket}
\usepackage{float}
\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{hyperref}  
\usepackage{cleveref} 
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{thmtools, thm-restate}
\usepackage{makecell}
\usepackage{thmtools}
\hypersetup{colorlinks=true,urlcolor=[rgb]{0,0,0.5},citecolor=[rgb]{0,0.5,0},linkcolor=[rgb]{0,0.5,0}}

\setlength{\parskip}{4pt}
\setlength{\parindent}{0pt}


\usepackage{graphicx}
\graphicspath{ {./Images/}}

 \theoremstyle{plain}
 \newtheorem{fact}{Fact}
 \theoremstyle{plain}
 \newtheorem{lem}{Lemma}
 \newtheorem*{lem*}{Lemma}
 \theoremstyle{plain}
 \newtheorem{thm}{Theorem}
 \newtheorem*{thm*}{Theorem}
 \theoremstyle{plain}
 \newtheorem{exa}{Example}
   \theoremstyle{plain}
 \newtheorem{prop}{Proposition}
 \theoremstyle{plain}
 \newtheorem{corr}{Corollary}
  \newtheorem*{corr*}{Corollary}
 \theoremstyle{plain}
 \newtheorem{obs}{Observation}
 \theoremstyle{remark}
 \newtheorem*{rem*}{Remark}
	\theoremstyle{remark}
	 \newtheorem{rem}{Remark}
  \theoremstyle{plain}
 \newtheorem{defn}{Definition}	 
   \theoremstyle{plain}
 \newtheorem{conj}{Conjecture}
    \theoremstyle{plain}
 \newtheorem{res}{Result}
    \theoremstyle{plain}
 \newtheorem{problem}{Problem}
  \newtheorem*{problem*}{Problem}
   \newtheorem*{conj*}{Conjecture}

\newcommand{\FBPP}{\class{FBPP}}
\newcommand{\Per}{\operatorname{Per}}
\newcommand{\Haar}{\mathrm{Haar}}
\newcommand{\UPE}{\mathrm{UPE}}
\newcommand{\USPE}{\mathrm{USPE}}
\newcommand{\UPEap}{|\UPE|^2_\pm}
\newcommand{\UPEmp}{|\UPE|^2_\times}
\newcommand{\USPEap}{|\USPE|^2_\pm}
\newcommand{\USPEmp}{|\USPE|^2_\times}

\newcommand{\superap}{\lvert \mathrm{SUPER}\rvert^2_\pm}
\newcommand{\supermp}{\lvert \mathrm{SUPER}\rvert^2_\times}


\newcommand{\Smn}{\mathcal{S}_{m,n}}
\newcommand{\Scmn}{\mathcal{S}^c_{m,n}}

\newcommand{\Complex}{\mathbb C}
\newcommand{\ep}{\epsilon}
\newcommand{\eps}{\epsilon} 
\newcommand{\e}{\mathrm{e}}
\newcommand{\ot}{\otimes}
\newcommand{\ii}{\mathrm{i}}

\renewcommand{\exp}{\mathrm{exp}} 
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\DEF}{\coloneqq}
\DeclareMathOperator{\tr}{tr}
\newcommand{\cm}{M}

\renewcommand{\H}{\mathcal{H}}
\newcommand{\W}{\mathcal{W}} 
\newcommand{\J}{\mathcal{J}} 

\DeclareMathOperator{\vol}{Vol}
\DeclareMathOperator*{\Hloc}{\mathcal{H}_{loc}}
\DeclareMathOperator{\Rept}{Re}
\newcommand{\LOB}{\mathrm{LO}_b} 

\newcommand{\U}{\mathrm{U}} 
\newcommand{\SU}{\mathrm{SU}} 
\newcommand{\SO}{\mathrm{SO}} 
\newcommand{\USP}{\mathrm{USp}}
\newcommand{\Spin}{\mathrm{Spin}}

\renewcommand{\u}{\mathfrak{u}} 
\newcommand{\su}{\mathfrak{su}} 
\newcommand{\so}{\mathfrak{so}} 
\newcommand{\usp}{\mathfrak{usp}}
\newcommand{\g}{\mathfrak{g}} 
\renewcommand{\k}{\mathfrak{k}}

\newcommand{\defeq}{\coloneqq}
\newcommand{\diff}{\mathop{}\!\mathrm{d}}
\newcommand{\Expec}{\mathbb E}
\newcommand{\Prob}{\Pr}
\newcommand{\Sphere}{{\mathbf S}}
\newcommand{\One}{{\mathbf{1}}}
\newcommand{\C}{\mathbb{C}} 
\newcommand{\R}{\mathbb{R}} 
\newcommand{\T}{\mathbb{T}} 
\newcommand{\M}{\mathbb{M}} 
\newcommand{\Lie}{\mathrm{Lie}} 
\newcommand{\Herm}{\mathrm{Herm}}  
\newcommand{\supp}{\mathrm{supp}} 
\newcommand{\Aut}{\mathrm{Aut}}  
\newcommand{\Ad}{\mathrm{Ad}}
\newcommand{\Inn}{\mathrm{Inn}}  
\newcommand{\I}{\mathbb{I}} 
\renewcommand{\P}{\mathbb{P}} 
\newcommand{\E}{\mathbb{E}} 
\renewcommand{\O}{\mathcal{O}} 
\newcommand{\N}{\mathcal{N}} 
\newcommand{\Q}{\mathcal{Q}} 
\newcommand{\PP}{\mathcal{P}} 
\newcommand{\sharP}{\#\mathsf{P}} 
\renewcommand{\ket}[1]{\left| #1 \right>} 
\renewcommand{\bra}[1]{\left< #1 \right|} 
\renewcommand{\braket}[2]{\langle #1 | #2 \rangle} 
\newcommand{\ketbra}[2]{\left| #1 \rangle\langle #2 \right|}
\newcommand{\n}{\mathbf{n}} 
\newcommand{\m}{\mathbf{n}} 
\newcommand{\x}{\mathbf{x}} 
\newcommand{\y}{\mathbf{y}} 
\newcommand{\z}{\mathbf{z}} 
\renewcommand{\n}{\mathbf{n}}
\renewcommand{\v}{\mathbf{v}} 
\newcommand{\w}{\mathbf{w}} 
\newcommand{\muu}{\bm{\mu}}
\renewcommand{\SS}{\mathbb{S}} 
\newcommand{\D}{\mathcal{D}} 
\renewcommand{\L}{\mathcal{L}} 
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}} 
\newcommand{\Z}{\mathcal{Z}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\A}{\mathcal{A}} 
\newcommand{\B}{\mathcal{B}}
\newcommand{\Cl}{\mathcal{C}} 
\renewcommand{\S}{\mathcal{S}} 
\newcommand{\Span}{\mathrm{span}} 
\newcommand\dgg{^{\dagger}}
\newcommand{\id}{\mathbb{I}}
\newcommand\abs[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand\av[1]{\left\langle #1 \right\rangle}
\newcommand\poly[1]{\mathrm{poly}(#1)}
\newcommand{\bPh}{\boldsymbol{\phi}}
\newcommand{\bVar}{\boldsymbol{\varphi}}
\newcommand{\oX}{\tilde{X}}
\newcommand{\Hbos}[2]{\mathcal{S}_{#1,#2}} 
\newcommand{\Hbosc}[3]{\mathcal{S}_{#1,#2}^{(#3)}}

\begin{document} 	
 \title{Complexity-theoretic foundations of BosonSampling with a linear number of modes}
 \date{}
\author[1]{Adam Bouland}
\author[2]{Daniel Brod}
\author[1]{Ishaun Datta}
\author[3]{Bill Fefferman}
\author[4,5]{Daniel Grier}
\author[6]{Felipe Hern\'{a}ndez}
\author[7,8]{Micha\l{} Oszmaniec}

\affil[1]{Department of Computer Science, Stanford University}
\affil[2]{\normalsize{Instituto de F\'{i}sica, Universidade Federal Fluminense}}
\affil[3]{Department of Computer Science, University of Chicago}
\affil[4]{Department of Computer Science and Engineering, UC San Diego}
\affil[5]{Department of Mathematics, UC San Diego}
\affil[6]{Department of Mathematics, MIT}
\affil[7]{Center for Theoretical Physics, Polish Academy of Sciences}
\affil[8]{NASK National Research Institute}


\maketitle	
\begin{abstract}
    BosonSampling is the leading candidate for demonstrating quantum computational advantage in photonic systems. While we have recently seen many impressive experimental demonstrations, there is still a formidable distance between the complexity-theoretic hardness arguments and current experiments.  One of the largest gaps involves the ratio of photons to modes---all current hardness evidence assumes a ``high-mode'' regime in which the number of linear optical modes scales at least quadratically in the number of photons.  By contrast, current experiments operate in a ``low-mode'' regime with a linear number of modes.  In this paper we bridge this gap, bringing the hardness evidence for the low-mode experiments to the same level as had been previously established for the high-mode regime.  This involves proving a new worst-to-average-case reduction for computing the Permanent which is robust to both large numbers of row repetitions and also to distributions over matrices with correlated entries.
\end{abstract}

\section{Introduction}\label{sec:intro}

In the decade since it was proposed by Aaronson and Arkhipov \cite{Aaronson2013}, BosonSampling has become one of the most promising candidates for achieving quantum computational advantage---the experimental demonstration of a quantum computation which exponentially surpasses classical computers. This requires a task which is both experimentally feasible and has strong complexity-theoretic evidence for hardness. We have now seen several experimental demonstrations of BosonSampling \cite{wang2019boson} and its Gaussian variant at scale \cite{GaussBSExperiment2020, zhong2021phase, madsen2022quantum}, as well as substantial work building the theory of these experiments and bringing them closer to feasibility (see e.g.\ \cite{hamilton2017gaussian,chakhmakhchyan2017boson,deshpande2021quantum,grier_brod_2021}). 


Despite this progress, there is still a formidable distance between experiment and theory. One of the most notable gaps involves the ratio between the number of modes and number of photons.  The original BosonSampling proposal calls for a ``high-mode regime''\footnote{Also called the dilute limit.} in which $n$ photons are passed through an $m=\Omega(n^2)$-mode Haar-random interferometer followed by the measurement of each mode in the photon number basis.\footnote{We note that the original paper used $m=\omega(n^5)$ modes, but they showed that an $O(n^2)$ experiment would suffice under a plausible random matrix theory conjecture, and subsequent work proved variants of BosonSampling like Bipartite BosonSampling \cite{grier_brod_2021} can be analyzed with as few as $O(n^2)$ modes. } By contrast, all experiments to date operate in a ``low-mode regime'' in which the number of modes scales linearly in the number of modes, $m=\Theta(n)$. Understanding this low-mode regime  has long been cited as a major open problem going back to the original paper \cite{Aaronson2013} which asked explicitly:
\begin{center}
\begin{minipage}{.8\linewidth}
\begin{center}
\textit{``Can we reduce the number of modes needed for our linear-optics experiment, perhaps from $O(n^2)$ to $O(n)$?''}
\end{center}
\end{minipage}
\end{center}
It is reasonable to conjecture that the need for such a high number of modes is an artifact of current proof techniques, rather than intrinsic to the hardness of the sampling problem.  For one, state-of-the-art classical simulation algorithms are not able to take advantage of the low-mode regime to achieve dramatically faster runtimes\footnote{To be precise, these algorithms run in time which is exponential in the number of photons, but merely polynomial in the number of modes. In other words, high-mode experiments with few photons provably cannot lead to exponential quantum advantage.} \cite{clifford2018classical}, albeit some improvements are possible \cite{clifford2020faster,newmichalpaper}.  Furthermore, 
the low-mode regime is sufficient to perform universal quantum computation \cite{knill2001scheme}.

However, analyzing the hardness of BosonSampling in the low-mode regime is quite challenging for two reasons.  First, current proof techniques rely heavily on a property of the high-mode regime known as the ``Bosonic Birthday Paradox'' \cite{arkhipov_kuperberg} which ensures that most measurement outcomes are collision-free---i.e., have a single photon in each occupied output mode.  By contrast, low-mode BosonSampling has a large number of collisions.  Second, in the high-mode regime the probability of each outcome is the squared permanent of a submatrix of the Haar random unitary that encodes the interferometer.  These submatrices have i.i.d.\  Gaussian entries, which are convenient to analyze.  In the low-mode regime, the relevant submatrices do not have i.i.d.\  entries and this is the case even in the absence of collisions.

In this work we overcome these obstacles and build the complexity-theoretic foundations for BosonSampling in the low-mode regime, answering Aaronson and Arkhipov's question in the affirmative. The starting point, following Aaronson and Arkhipov, is to prove that hardness of classical approximate sampling from low-mode BosonSampling experiments follows from the hardness of an appropriate average-case hardness conjecture:

\begin{thm}[Informal] \label{thm:nosamplerinformal}
    Assuming average-case hardness of computing output probabilities of low-mode BosonSampling experiments, there is no efficient randomized classical algorithm to sample from the output distribution of such an experiment to inverse polynomial additive error.
\end{thm}

Our main results give strong evidence in favor of this average-case hardness conjecture. In particular we show it is \textsf{\#P}-hard to compute the output probabilities of random low-mode experiments, and also provide numerical evidence for anticoncentration in this regime. 
This brings low-mode BosonSampling to essentially the same level of theoretical support as high-mode experiments.

\vspace{-1em}
\subsection{Proof Sketch}

The first obstacle present in the low-mode regime---the presence of photon collisions in typical outcomes---breaks a property of the experiment known as \emph{hiding}. Hiding\footnote{``Hiding'' also refers to an input type conversion problem in the original paper \cite{Aaronson2013}, but in subsequent papers was used only to describe this symmetry property.} is the property that all outputs of the experiment are symmetrical over the choice of random experiment. This simple property plays a surprisingly key role in current quantum advantage arguments. The basic reason is that these arguments try to show no approximate classical sampler exists to small total variation distance error. If all outputs are on equal footing then this error can be spread over all outputs by Markov's inequality. However if only a few outputs matter for the hardness arguments, one might worry the approximate sampler could corrupt these outputs only, and the arguments become implausible. This symmetry property trivially holds for Random Circuit Sampling, IQP, Fermion Sampling, and many other advantage schemes, and also trivially carries over to BosonSampling experiments in the high-mode regime. However in the low-mode regime this symmetry fails spectacularly---the output space shatters into an exponential number of incomparable output types each with probability mass that is relatively well spread.\footnote{That is, it is unclear if any particular output type occurs with a probability that scales as an inverse polynomial.}

We instead proceed by formulating a modified version of the Stockmeyer counting reduction which does not require the hiding symmetry.  We choose a uniformly random outcome of the experiment and use Stockmeyer counting to estimate the probability of this outcome. 
By Markov's inequality we can ensure that most output probabilities of the approximate sampler are mostly correct. However, this modified reduction comes at a cost---to show hardness of sampling, it no longer suffices to show hardness of computing a single type of output, but instead we must now show an \emph{entire suite} of hardness results for \emph{most} outputs of the experiments. More formally, for low-mode BosonSampling the output space consists of an exponential number of incomparable collision patterns---i.e., unordered lists of occupation numbers of modes. We need to argue it is hard to estimate the output probabilities of most collision patterns under a suitable measure. 

Our next step is to show such a suite of average-case hardness results of computing most outputs of low-mode experiments:
\begin{thm}[Informal] \label{thm:maininformal} It is $\sharP$-hard to compute most output probabilities of most BosonSampling experiments  in the low-mode regime to within additive error $e^{-O(n\log n)}$.\footnote{We note that this additive error is dependent on the output type of the outcome, though this dependence is subleading in the exponent.} 
\end{thm}This is nearly what we need to show hardness of sampling via the modified Stockmeyer reduction ($e^{-O(n)}$ robustness).
Here we need to overcome both of the major differences that distinguish the low and high-mode regimes.  First, one must deal with the presence of collisions---to do this we identify a collection of output types which cover a large fraction of the output probability distribution of typical experiments. This requires a careful combinatorial accounting of typical collision patterns in typical outputs of low-mode experiments. Once a suitable collision pattern is identified, we need to show average-case hardness for computing outputs of that collision type over the random choice of interferometer. This is equivalent to showing hardness of computing the permanent of a large submatrix of a random unitary, with a particular pattern of repeated rows. 

In the high-mode regime this average-case hardness argument proceeds using a variant of Lipton's argument.
The basic idea is that since a Gaussian is perturbed only slightly by shifting and rescaling, one can in some sense ``sneak'' a tiny amount of a worst-case matrix into an average-case matrix. This uses an entry-by-entry analysis of the matrix. This proof strategy breaks in the low-mode case since the relevant submatrices are far from i.i.d.\ Gaussian \cite{jiang2006many}---instead, the entries come from a highly correlated measure.
We show that surprisingly, this highly correlated distribution is nonetheless approximately shift-and-scale invariant so that we recover Lipton's proof. To do this, we directly study the probability density of the singular values of a submatrix of a Haar-random unitary \cite{collins2003integrales,reffy}.
We reduce the desired invariance property to estimating the gradient of this probability density, which we show is equivalent to proving sharp tail bounds on the maximum singular value. Our desired bounds require that we go beyond generic concentration inequalities such as Levy's lemma or log-Sobolev inequalities, and instead we derive them from high-dimensional geometry. Considering the ubiquity of the Haar measure over unitaries, we expect that this bound may be of independent interest.

Finally we address the issue of anticoncentration of low-mode experiments. Anticoncentration is a necessary ingredient for converting additive estimates of output probabilities to multiplicative estimates---which we conjecture to be hard. It remains open to prove anticoncentration for all variants of BosonSampling, but there has been partial progress in this direction \cite{nezami2021permanent}, as well as  numerical evidence for anticoncentration in the high-mode regime \cite{Aaronson2013}. However, as one reduces the number of modes, one might worry that anticoncentration might begin to fail, both due to the row repetitions in the submatrices, and the correlations between submatrix entries. This could be an issue as many of the known attacks on quantum advantage schemes hold in the non-anticoncentration regime, e.g.\ constant-depth random circuits \cite{Napp2020}. To alleviate this concern and to support our anticoncentration conjecture, we provide numerical evidence that anticoncentration holds in the low-mode regime, and indeed has very similar behavior to the i.i.d.\  Gaussian case. 

\section{Notation} 
In this section we collect notation used throughout the paper.
We use the letter $n$ to denote the number of photons and $m$ the number of modes.  
We use $\mathcal S_{m,n}$ for the set of possible collision patterns $S=(s_1,\ldots,s_m)$ where $s_j$ denotes the number of photons measured in the $j$-th mode.  More precisely, we define
\[
\mathcal{S}_{m,n}
= 
\{(s_1,s_2,\ldots,s_m) \mid  s_j\in\mathbb N_{\geq 0} \text{ and } \sum_{j=1}^m s_j = n\}.
\]
By a stars-and-bars counting argument, the cardinality of the set
$\mathcal{S}_{m,n}$ of collision patterns is
\[
|\mathcal{S}_{m,n}| = \binom{m+n-1}{n}.
\]
Throughout, we assume the standard initial state, i.e.\ $S=(1,\ldots,1,0,\ldots,0)$ with the first $n$ modes occupied each by $1$ boson.

The number of nonzero entries in a collision pattern $S$ is referred to as the number of \textit{clicks}.  

We use $\mathcal{U}_{m,n}$ to refer to the uniform distribution on $\mathcal{S}_{m,n},$ i.e.\ the distribution such that every outcome $S\in\mathcal{S}_{m,n}$ has probability $1/\vert\Smn\vert.$

Now we describe another probability distribution on $\mathcal{S}_{m,n}$ induced
by a BosonSampling experiment with an $m\times m$ unitary matrix $U$.
For unitary $U$ and outcome $S$, the matrix $U_S$ is defined by taking the $m \times n$ left submatrix of $U$ and repeating the $i$th row $s_i$ many times. The probability distribution $\Pr_U$ on $\mathcal{S}_{m,n}$ induced 
by $U$ is given by
\begin{equation} \label{eq:probability}
    \Pr_U(S) := \frac{\left|\Per(U_S)\right|^2}{\prod_{i=1}^m s_i!}.
\end{equation}
Moreover, we use $\mathcal{D}$ to refer to the probability distribution on $n\times n$ matrices $U_S$ induced by drawing $U\sim \Haar(m)$ and $S\sim\mathcal U_{m,n}.$ 

\section{Hardness of approximate sampling: efficient classical simulation of $m=O(n)$ BosonSampling collapses \textsf{PH}}\label{sec:classical-hardness}

In this section, we argue that efficient classical simulation of BosonSampling in the $m=O(n)$ regime would imply a collapse of the Polynomial Hierarchy, by a reduction from sampling (the experiment's distribution over outcomes) to computing (outcome probabilities). 
Our proof is based on an average-case hardness conjecture for so-called Sub-Unitary Permanent Estimation with Repetitions, or $\mathrm{SUPER}.$
In subsequent sections, we provide evidence for the hardness of this problem. 

The key ideas behind the hardness proof are as follows: imagine there exists a classical sampler for a low-mode BosonSampling experiment, namely a probabilistic polynomial-time algorithm that produces an outcome $S$ from a distribution close in $l_1$ distance to the experiment's. By Markov's inequality, most of the sampler's estimates to the true outcome probabilities are reasonably accurate. However, a celebrated result due to Stockmeyer, known as the approximate counting method, estimates those very outcome probabilities in probabilistic polynomial-time with access to an \textsf{NP} oracle \cite{stockmeyer1983complexity}. Stockmeyer's method exploits that unlike for a quantum computation, the classical sampler can be treated as a deterministic algorithm that takes a random input. As $\mathsf{BPP}^\mathsf{NP}$ lies within the Polynomial Hierarchy but we conjecture that it is $\sharP$-hard to estimate probabilities of most outcomes from most low-mode experiments, the existence of a classical sampler collapses \textsf{PH}--violating a bedrock assumption of complexity theory. 

Why introduce a new conjecture to argue for the classical intractability of low-mode BosonSampling? As described in Section \ref{sec:intro}, the need for a new basis to demonstrate hardness of sampling comes from the two major differences between the high-mode and low-mode settings, namely collisions and correlations. In the low-mode regime, outcome probabilities are not given by permanents of submatrices of Haar unitaries as in the high-mode regime; instead, the matrices of interest have repeated rows due to collisions. Moreover, whether or not there are collisions the relevant matrices in low-mode BosonSampling manifestly do not have approximately i.i.d.\ Gaussian entries. Row repetitions and correlated entries disconnect standard assumptions in the BosonSampling literature, such as the Permanent-of-Gaussians Conjecture, from the low-mode regime. 
To this end, we define the following problem about estimating average-case output probabilities. 

\begin{problem}[Sub-Unitary Permanent Estimation with Repetitions, $\superap$]
\label{prob:super}


We define a family of problems parametrized by error scaling function $\varepsilon \colon \mathbb N^m \to \mathbb R^+$. Let distribution $\mathcal{D}$ on $n\times n$ matrices $U_S$ be obtained by drawing $U\sim\Haar(m)$ and $S\sim\mathcal U_{m,n}$. Given $U_S\sim\mathcal{D}$ and error probability $\delta>0,$ output $z\in\mathbb{C}$ such that $|z - |\Per(U_S)|^2 | \le \varepsilon(S)$
with probability at least $1 - \delta$ over choice of $U_S$.
\end{problem}


This problem arises naturally from estimating probabilities of random outcomes from random interferometers. We conjecture that $\superap$ is $\sharP$-hard, in particular:

\begin{conj}\label{conj:UPEap}

$\superap$ is $\sharP$-hard to additive imprecision $\frac{1}{\poly{n}}\prod_i s_i!/\vert\Smn\vert.$ 
\end{conj}

Here, $\prod_i s_i!$ is a normalization factor inherited from bosonic statistics; see Eq.~\ref{eq:probability}. By contrast, in high-mode BosonSampling there is only one output type and this term is trivially 1.

\Cref{thm:no-sampler} (stated informally in Section \ref{sec:intro} as \Cref{thm:nosamplerinformal}) shows that Conjecture \ref{conj:UPEap} implies quantum computational advantage for $m=O(n)$ BosonSampling. 

\begin{thm}[No classical sampler]\label{thm:no-sampler}
    A classical approximate sampler implies a solution to $\superap$ with additive imprecision $\frac{1}{\poly{n}}\prod_i s_i!/\vert\Smn\vert$ in $\mathsf{BPP}^\mathsf{NP}. $ Assuming Conjecture \ref{conj:UPEap}, this collapses $\mathsf{PH}$. 
\end{thm}

We modify the usual Stockmeyer reduction to choose a uniformly random outcome. This subtlety does not arise in high-mode BosonSampling, where up to permuting the modes all outcomes are the same. The proof is deferred to Appendix \ref{sec:stockmeyer}.
\vspace{-1em}
\section{Anticoncentration in the presence of correlations}\label{sec:anticonc}

In this section we comment on anticoncentration. Roughly speaking, anticoncentration is the property that most outcome probabilities for most BosonSampling experiments are not much smaller than the inverse of the domain size.
Although in existing quantum advantage arguments anticoncentration is formally neither necessary nor sufficient for hardness, this approximate ``flatness'' property nevertheless plays an important role. In particular, anticoncentration is a necessary ingredient to convert additive estimates of output probabilities, which we conjecture to be hard, to multiplicative estimates. In this way, it provides a ``sanity check'' on hardness conjectures that involve additive error. 
Moreover, classical easiness results often exploit failure to anticoncentrate \cite{harvardXEB, Napp2020}. While it remains open to prove anticoncentration for any variant of BosonSampling, there has been partial progress from e.g.\ \cite{nezami2021permanent} and strong numerical evidence in the $m=O(n^2)$ regime. 

As described in Section \ref{sec:intro}, for $m=O(n)$ we depart from the high-mode regime in two ways that might pose a threat to anticoncentration. 
In $m=O(n)$ BosonSampling, submatrices of $m\times m$ Haar random matrices do not have independent entries, intuitively because $n$ is large enough relative to $m$ to ``feel'' the unitarity constraint \cite{jiang2006many}. Moreover, these submatrices have row repetitions due to collisions. 
Despite such correlated entries, we provide evidence that anticoncentration holds in this regime. Anticoncentration reformulates additive estimates of output probabilities as multiplicative estimates, which in Section 
\ref{sec:worst-case} we prove to be hard in the worst case.
Additionally, below we present numerical evidence for anticoncentration and observe behavior similar to that in the i.i.d.\ Gaussian case.

For $m=O(n),$ the appropriate formulation of anticoncentration is as follows:

\begin{conj}[Anticoncentration of Probabilities]
\label{conj:anticoncentration_probs}
There exists some polynomial such that for all $n > 0$, $m \ge n$, and $\delta > 0$,
$$
\Pr_{\substack{U \sim \Haar(m) \\ S \sim \mathcal U_{m,n}}} \left[ \frac{|\Per(U_S)|^2}{\prod_i s_i!} < \vert\Smn\vert^{-1} \frac{1}{\poly{m, 1/\delta}} \right] < \delta.
$$
\end{conj}

In Fig.~\ref{fig:anticoncentration}, we show numerical evidence for \Cref{conj:anticoncentration_probs} in the form of plots of random BosonSampling probabilities (i.e., $|\Per(U_S)|^2/\prod_i s_i!$). 
Recall that the conjecture posits that these probabilities are on the order of $1/\vert\Smn\vert$ and in particular do not grow smaller relative to this bound as $n$ grows. 
A key takeaway from the figure is that the difference between this bound and random probabilities is quite stable as $n$ increases (shown in particular on the right in Fig.~\ref{fig:anticoncentration}), consistent with the conjecture. If the trend we see numerically were to continue for larger permanents---we show data up to  30 photons---then this would demonstrate the anticoncentration conjecture.

\begin{figure}
\centering
    \input{logProbabilityBoxPlot}
    \input{residualBoxPlot}
\caption{Box plots for the distribution of random probabilities in the $m = 2n$ regime. For each $n$, we calculated $\ln(|\Per(U_S)|^2/\prod_i s_i!)$ for 20  Haar random matrices $U \in \mathbb C^{2n \times 2n}$ and 20 uniformly random outcomes $S$. \textit{Left:} The blue box plots depict this distribution and the red dashed line show the anticoncentration bound (i.e., $- \ln \vert\Smn\vert$).
\textit{Right:} The blue box plots depict the same distribution shifted by the anticoncentration bound, hence why the red dashed line appears at $0$. Each box plot has the same format: min, 1st quartile, median, 3rd quartile, max.}
\label{fig:anticoncentration}
\end{figure}

Anticoncentration motivates a reformulation of SUPER in terms of multiplicative error. 


\begin{conj}
\label{conj:UPEmp}The following family of problems, denoted $\supermp,$ is $\sharP$-hard: given $U_S\sim\mathcal{D}$ as above, 
output $z \in \mathbb C$ such that 
\[
\left|z - |\Per(U_S)|^2 \right| \le (\poly{m})^{-1} |\Per(U_S)|^2
\]
with probability at least $3/4$ over choice of $U_S$.
\end{conj}

Conjectures \ref{conj:anticoncentration_probs} and \ref{conj:UPEmp} immediately yield that $\superap$ is $\sharP$-hard to error $  \prod_i s_i! \vert\Smn\vert^{-1},$ so we recover the same hardness condition as Conjecture \ref{conj:UPEap}, which by Section \ref{sec:classical-hardness} again implies no classical sampler. 

\section{Worst-case hardness of permanents with repetitions} \label{sec:worst-case}

In Section \ref{sec:classical-hardness}, we introduced Sub-Unitary Permanent Estimation with Repetitions ($\mathrm{SUPER}$). In this section, we show that even when we relax the unitarity constraint and consider simply submatrix permanents with repetitions, the problem is still $\sharP$-hard. That is, here we demonstrate hardness of computing permanents of \textit{worst-case} submatrices with average-case repetitions.\footnote{Whereas in high-mode BosonSampling there is only one type of collision pattern, namely $n$~1's and $m-n$~0's, recall from Section \ref{sec:intro} that in low-mode BosonSampling the output space shatters into exponentially many incomparable output types that have well-spread probability mass. The consequence is that for low-mode BosonSampling, we always have to consider statements for \textit{most} outcomes $S$, namely average-case repetitions.} This plays an integral role in proving the hardness of computing the permanent of \textit{average-case} submatrices with average-case repetitions, as it forms the base problem of our worst-to-average-case reduction in Section \ref{sec:avgcase}.


\begin{thm}[$\sharP$-hard permanents of worst-case submatrices]\label{thm:worst}
    For most collision patterns $S\in\Smn$ and ``worst-case submatrix''  $A \in \{0, 1\}^{c \times n}$, it is $\sharP$-hard to compute $\Per(A_S).$
\end{thm}

The proof of Theorem \ref{thm:worst} uses the following two lemmas. 


\begin{lem}[Almost all outcomes have $\Theta(n)$ clicks]\label{lem:clicksinformal}
    For $m=\alpha n,$ almost all outcomes $S\sim\mathcal U_{m,n}$ have at least $\frac{\alpha}{\alpha+2}n$ clicks. That is, outcomes with at least $\frac{\alpha}{\alpha+2}n$ clicks are a $(1-o(1))$ fraction of $\Smn.$
\end{lem}

Lemma \ref{lem:clicksinformal} is proved in Appendix \ref{sec:manyclicks}.

\begin{lem} 
\label{lem:rep_embedding}

Given collision pattern $S\in\Smn$ with $c$ clicks and matrix $X \in \{0, 1\}^{c \times c}$, there is a poly-time constructible matrix $A \in \{0, 1\}^{c \times n}$ such that 
\[
\Per(A_S) = \Per(X) \prod_{i=1}^c s_i!
\]
\end{lem}


\Cref{lem:rep_embedding} follows directly from Lemma 24 of \cite{grier_brod_2021}. The general idea is as follows. We start with $c \times c$ matrix $X,$ which encodes some $\sharP$-hard problem. We append to it a $c \times k$ matrix $Y$ to make a larger $c \times (k+c)$ matrix, $A$. Finally, we repeat $k$ rows of the latter matrix according to a repetition pattern $(s_1, s_2 \ldots s_c)$ to make a $(k+c) \times (k+c)$ matrix $A_S$. The construction is shown in  \Cref{Fig:repetitions}. By an appropriate choice of $Y$, we can prove that $\Per A_S$ is proportional to $\Per X$. In particular, the result of \Cref{lem:rep_embedding} is obtained from Lemma 24 of \cite{grier_brod_2021} by replacing $B'_S$ and $B'$ with $A_S$ and $A$, $A$ with $X$, $k_s$ with $k$, and by setting $z=0$ and $y_{i,j}^{(l)}=1$. 

\begin{figure}
\centering    \includegraphics[width=0.75\columnwidth]{figures/WorstcaseBS.pdf}
    \caption{The figure shows how to extend the $c \times c$ matrix $X$ to handle $k$ row repetitions. The first step is to create the matrix $(X|Y)$ by appending $k$ extra columns, as described in the text. The matrix $Z$ just corresponds to all extra rows that are added according to the repetition pattern.}
    \label{Fig:repetitions}
\end{figure}

We illustrate the construction with a small, concrete example. Consider the following $3\times3$ matrix $X$:
\begin{equation}
X = \left(\begin{array}{c c c}
x_{1,1} & x_{1,2} & x_{1,3} \\
x_{2,1} & x_{2,2} & x_{2,3} \\
x_{3,1} & x_{3,2} & x_{3,3}
\end{array}\right),
\end{equation}
and suppose $S=(4,3,1)$, i.e., we need to repeat the first row three additional times, and the second row two additional times. This corresponds to an 8-photon BosonSampling outcome where we observed $c=3$ nonempty output modes. 

We now choose a $Y$ matrix as follows. For each \emph{extra} copy of a row we will need to add later, $Y$ must contain one unit-vector column with nonzero element matching that row. More concretely, if row $i$ will be repeated $s_i-1$ times, then $Y$ needs to have $s_i-1$ columns whose $i$th element is 1 and all others are zero. For our particular example we build the following matrix
\begin{equation} A= \label{eq:example}
\left(\begin{array}{ c c c | c c c c c}
x_{1,1} & x_{1,2} & x_{1,3} & 1 & 1 & 1 & 0 & 0 \\
x_{2,1} & x_{2,2} & x_{2,3} & 0 & 0 & 0 & 1 & 1\\
x_{3,1} & x_{3,2} & x_{3,3} & 0 & 0 & 0 & 0 & 0
\end{array}\right).
\end{equation}
And, after the repetition of all appropriate rows,
\begin{equation}
A_S = \left(\begin{array}{ c c c | c c c c c}
x_{1,1} & x_{1,2} & x_{1,3} & 1 & 1 & 1 & 0 & 0 \\
x_{1,1} & x_{1,2} & x_{1,3} & 1 & 1 & 1 & 0 & 0\\
x_{1,1} & x_{1,2} & x_{1,3} & 1 & 1 & 1 & 0 & 0\\
x_{1,1} & x_{1,2} & x_{1,3} & 1 & 1 & 1 & 0 & 0\\
x_{2,1} & x_{2,2} & x_{2,3} & 0 & 0 & 0 & 1 &  1\\
x_{2,1} & x_{2,2} & x_{2,3} & 0 & 0 & 0 & 1 &  1\\
x_{2,1} & x_{2,2} & x_{2,3} & 0 & 0 & 0 & 1 &  1\\
x_{3,1} & x_{3,2} & x_{3,3} & 0 & 0 & 0 & 0 & 0
\end{array}\right)
\end{equation}
It is easy to check that 
\begin{equation}
\Per(A_S) = 4!3!\Per(X).
\end{equation}
In the most general case, by using this choice for $Y$, we get that
\begin{equation}\label{eq:PerAsPoly}
\Per(A_S) = \Per(X) \prod_{i=1}^l s_i!
\end{equation}
where $k = n-c$. Note that, if $X$ is a $\{0,1\}$ matrix, so is $A_S$.

One way to see why this construction works is to consider the Laplace expansion for the permanent. Recursively apply the Laplace expansion $k$ times, eliminating the $k$ columns which do not include $X$, one at a time. It becomes clear that the choices of 0's and 1's in $Y$ are such that the only $c \times c$ submatrices of $A_S$ that contribute to the expansion are $(\prod_{i=1}^l s_i!)$ identical copies of $X$. 


Equipped with Lemmas \ref{lem:clicksinformal} and \ref{lem:rep_embedding}, we can now prove Theorem \ref{thm:worst}.
\begin{proof}[Proof of Theorem \ref{thm:worst}]
    We begin by drawing a uniformly random outcome $S,$ i.e.\ $S\sim\mathcal U_{m,n}.$ By \Cref{lem:clicksinformal}, with high probability $S$ has $c=\Theta(n)$ clicks. Consequently, computing the permanent of matrices $X\in\{0,1\}^{c\times c}$ is $\sharP$-hard by the landmark result of \cite{valiant1979}. By \Cref{lem:rep_embedding}, taking as input our randomly drawn $S$ and a matrix $X\in\{0,1\}^{c\times c},$ in polynomial time one can construct our ``worst-case submatrix'' $A \in \{0, 1\}^{c \times n}$ in such a way that $\Per(A_S) = \Per(X) \prod_{i=1}^l s_i!.$ Thus $\Per(A_S)$ is $\sharP$-hard as well, completing the proof.  
\end{proof}

\section{Worst-to-average-case reduction}\label{sec:avgcase}

In this section we prove our main result, Theorem \ref{thm:main}, that $\superap$ is $\sharP$-hard to additive accuracy $\exp(-O(n \log n))$. In Section \ref{sec:worst-case}, recall we proved the hardness of computing permanents for worst-case submatrices with average-case repetitions. Here, we present a worst-to-average-case reduction that proves $\sharP$-hardness for average-case submatrices with average-case repetitions. Finally, by Reverse Embedding (see Appendix \ref{sec:stockmeyer}, Lemma \ref{lem:reverse-embedding}), this is equivalent to $\sharP$-hardness for average-case unitaries and average-case repetitions---namely, the hardness of $\mathrm{SUPER},$ Problem \ref{prob:super}.
Were the robustness, meaning the additive error tolerance, of our result to be improved to $1/|\Smn| = \exp(-O(n)),$ it would prove quantum computational advantage conditional \emph{only} on the non-collapse of \textsf{PH}.
As mentioned in Section \ref{sec:intro}, the main challenge we need to overcome is correlations between matrix entries which breaks the original \cite{Aaronson2013,Bouland2021}-style proofs.



As a reminder, the overall scheme of these reductions is an interpolation argument inspired by Lipton's self-reducibility of the permanent, which exploits its polynomial structure to show that average-case instances are as hard as in the worst case \cite{lipton1991}. In particular, by taking a convex combination in variable $t$ of an average-case instance and a worst-case instance, the permanent is a univariate polynomial in $t.$\footnote{In this way, our worst-to-average-case reduction is over the subspace of matrices, not the subgroup of unitaries.} Then, by estimating values of the polynomial for small $t$ by the average-case algorithm, one can extrapolate to $t=1,$ the permanents of which are proved hard in Section \ref{sec:worst-case}.

The interpolation is carried out by the Robust Berlekamp-Welch algorithm  developed in \cite{Bouland2021}, which adapts to polynomials over $\mathbb{C}$ the well-known Berlekamp-Welch algorithm \cite{BW} for polynomial interpolation over finite fields. For $m>2n^\gamma$, $\gamma\geq 1,$ Robust Berlekamp-Welch demonstrates that an algorithm to solve instances of Problem \ref{prob:super} to additive tolerance $\exp(-(\gamma + 4) n \log n - O(n))$ implies an efficient algorithm to compute worst-case-hard matrices under $\mathsf{BPP}^\mathsf{NP}$ reductions. Thus we demonstrate average-case hardness of $\superap$ to additive error $\exp(-5n\log n - O(n)).$ This robustness is slightly better than that known for $m=O(n^2)$ BosonSampling--5 rather than 6--but on account of the smaller size of $\vert\mathcal S_{m,n}\vert$ in low-mode BosonSampling, the target robustness is commensurately larger, $\exp(-O(n)).$ Thus a small gap remains between the robustness we can prove and the robustness to which we conjecture hardness.  Closing the robustness gap remains open for all quantum advantage schemes. 


The interpolation arguments of \cite{Aaronson2013} and \cite{Bouland2021} proceed by an entry-by-entry analysis of i.i.d.\ Gaussian matrices. As described in Section \ref{sec:intro}, for low-mode BosonSampling the matrix entries instead come from a highly correlated measure. Although in the low-mode setting this seemingly crucial i.i.d.\ property no longer holds, 
we nonetheless prove a total variation distance bound on the distribution of average-case instances under translation and dilation that allows one to recover the interpolation argument. Theorem \ref{thm:appx-invariance} shows that despite correlations, the distributions are perturbed only mildly under these transformations.

Our proof of Theorem \ref{thm:appx-invariance} develops tools that, given the ubiquity of the Haar measure over unitaries across the quantum information literature, we expect may be of independent interest. Starting with the probability density of the singular values of a submatrix of a Haar-random unitary, derived first by \cite{collins2003integrales}, we reduce the approximate invariance property to proving sharp tail bounds on the largest singular value. However, the bounds we need are stronger than those obtainable by blackbox concentration inequalities like Levy's lemma or log-Sobolev inequalities as used in prior work  \cite{singal2022implementation}. Instead, we carry out a more fine-grained analysis of the maximum singular value close to $1$ by directly analyzing the geometry of high-dimensional spheres. 

\subsection{Truncations of Haar-random unitaries: approximate invariance property}\label{ssec:appx-invariance}

As described above, in our worst-to-average-case reduction we smuggle a tiny amount of a worst-case matrix $A$ into a random matrix $X(t)$ like so: 
\begin{equation*}
X(t) = (1-t) X_0 + t A \text{ for } t\in\left[0,1\right],
\end{equation*}

in such a way that the permanent of $X(t)$ is (by assumption in the reduction) approximately correct with high probability for small $t,$ but at $t=1$  is the permanent of $A,$ which is hard to compute. This is possible in the i.i.d.\  Gaussian setting of high-mode BosonSampling because Gaussians are perturbed only slightly in TVD by shifting and rescaling in the manner above. It is not clear that this approximate shift-and-scale invariance should hold for the highly correlated measure on truncated Haar unitaries from which $X_0$ is drawn. Theorem \ref{thm:appx-invariance} shows that surprisingly, it does.


\begin{restatable}[Approximate shift-and-scale invariance]{thm}{appxinvariance}
\label{thm:appx-invariance}

Let $\mu$
be the probability measure on $n\times n$ complex-valued matrices induced by taking
$n\times n$  submatrices of a Haar-random $m\times m$ unitary matrix, with each entry scaled by $\sqrt{m}$ to have unit variance.
Fixing
a ``worst-case'' matrix $A\in\{0,1\}^{n \times n}$, we define the random variable
\begin{equation}\label{eq:smuggled}
X(t) = (1-t) X_0 + t A.
\end{equation}
We let $\mu_t$ be the probability measure for the random matrix $X(t)$.
Note that $\mu_0=\mu$.

If $m = \lceil(2+\epsilon)n\rceil$ and $n\geq 2\epsilon^{-1}$ then
\[
\|\mu_t - \mu\|_{TV} \leq C_\epsilon n^2 t.
\]
\end{restatable}

In other words, the distribution of $X_t$ is close to that of $X_0$ when $A$ is a $\{0,1\}$-valued matrix\footnote{More generally, it suffices for matrix $A$ to have bounded entries so that $\norm{A}_{\text{HS}} \leq Cn$ for constant $C$.} and $t = O(1/n^2).$ Notably, an upper bound on $l_1$ distance between distributions of submatrices is also an $l_1$ upper bound on distributions of submatrices with repeated rows by the data processing inequality. Thus the bound bears on the unitary submatrices with repeated rows that come about in $m=O(n)$ BosonSampling.  

Finally, the conditions of Theorem \ref{thm:appx-invariance} mean that our worst-to-average-case reduction directly addresses $m>2n.$ For $m\leq 2n,$ one could alternately interpolate by the Cayley path between worst and average-case instances \cite{movassagh_quantum_2020,movassagh2023hardness}, as is done in Fermion Sampling \cite{fermionsampling}, though the robustness is worse than that attained from the Lipton-style proof for $m>2n$ using current techniques. We leave open whether or not it is possible to obtain similar robustness in the $m\leq 2n$ regime. 


We prove Theorem \ref{thm:appx-invariance} in Appendix \ref{sec:appx-invariance-pf}. 


\subsection{Lipton-style interpolation and Robust Berlekamp-Welch} 

In Section \ref{sec:classical-hardness}, we show that Conjecture \ref{conj:UPEap}, that $\superap$ is $\sharP$-hard to additive error $\exp(-O(n))$, implies no efficient classical sampler. Here, we provide strong evidence in favor of Conjecture \ref{conj:UPEap}. In particular, in Theorem \ref{thm:main} we prove a necessary condition for the conjecture to hold. 

The idea of Lipton's proof of average-case hardness of the permanent is to take an entry-wise convex combination of a matrix $A$ whose permanent is hard and a matrix $X_0$ whose permanent we assume we can compute approximately correctly for most instances,
producing matrix $X(t)$ as in Eq.~\ref{eq:smuggled}. The squared permanent of such an object is a univariate polynomial in $t$ of degree $2n.$ For small $t,$ points on this polynomial are instances of $\superap.$ At $t=1$ is a (squared) permanent proved $\sharP$-hard to compute in Section \ref{sec:worst-case}. Thus to show $\superap$ is $\sharP$-hard as well, one need only interpolate from points close to $t=0$ to $t=1.$ 
How close to $t=0$ our points must be so they are approximately correct with constant probability is established by Theorem \ref{thm:appx-invariance} as $t=O(1/n^2).$

Robust Berlekamp-Welch, presented below as Theorem \ref{thm:robust_berlekamp_welch}, provides an algorithm for interpolation over the complex field. The algorithm runs in polynomial time with access to an $\textsf{NP}$ oracle, but this is sufficient as we are already proving $\sharP$-hardness under $\mathsf{BPP}^\mathsf{NP}$ reductions.
While one could alternately use standard Lagrange interpolation, this would suffice only to prove a weaker result with hardness to a smaller amount of additive error.

\begin{thm}
[Robust Berlekamp-Welch \cite{Bouland2021}]
\label{thm:robust_berlekamp_welch}
Let $p(x)$ be any degree $d$ polynomial. There is a $\mathsf{P}^\mathsf{NP}$ algorithm for the following task: \\
\begin{tabular}{r l}
\emph{Input:} & \begin{minipage}[t]{.8\linewidth}
List of data points $(x_1, y_1), \ldots, (x_{100d^2}, y_{100d^2})$ where the $x_i$ are equally spaced on the interval $[0, \Delta]$ ($\Delta < 1$) and $\Pr[|y_i - p(x_i)| \ge \delta] < 1/4$.
\end{minipage} \\
\emph{Output:} & Estimate $z \in \mathbb C$ such that $|z - p(1)| \le \delta e^{d \log \Delta^{-1} + O(d)}$ with probability at least $3/4$.
\end{tabular}
\end{thm}

The error bound on the estimate to $p(1)$
arises because the maximum blowup under extrapolation from $\Delta$ to $1$ for a degree $d$ polynomial is $(1/\Delta)^d$.\footnote{Cf. Remez inequality.}
The subleading correction $\delta \exp(O(d))$ comes from the error incurred on $\left[0,\Delta\right]$ by points that are $\delta-$close to the true values, by an invocation of Paturi's bound \cite{Paturi1992}.

Our main result, stated informally in Section \ref{sec:intro} as Theorem \ref{thm:maininformal}, is below.
\begin{thm}[$\sharP$-hardness of $\mathrm{SUPER}$]\label{thm:main}
    $\superap$ is $\sharP$-hard to additive error $e^{-5n\log(n) - O(n)}.$
\end{thm}

\begin{proof}
Our worst-to-average-case reduction is as follows: the average-case algorithm simply computes the squared permanents of $100d^2 = 400n^2$ matrices corresponding to points on $\left[0, \Delta\right].$ We satisfy the requirement of \Cref{thm:robust_berlekamp_welch} that most points are $\delta-$close to the true values with sufficiently high probability by taking $\Delta=O(1/n^2)$ and invoking Theorem \ref{thm:appx-invariance}. Given these values as input, by Theorem \ref{thm:robust_berlekamp_welch} Robust Berlekamp-Welch outputs an approximation to $|\Per(A)|^2/m^n$ that with probability at least $3/4$ is $\delta \exp(4n\log(n) + O(n))-$close,
equivalently $\delta \exp(5n\log(n) + O(n))-$close to $|\Per(A)|^2$ for $m=O(n).$\footnote{The $m^n$ factor comes about because we rescale each row by $\sqrt{m}$ to apply Theorem \ref{thm:appx-invariance}, multiplying the permanent by $m^{n/2},$ then square the permanent.}
As $A$ is an integer matrix, an estimate of its permanent with error less than $1$ is exact. Thus $\delta < \exp(-5n\log(n) - O(n))$ additive error on the squared permanents computed by the average-case algorithm suffice to compute the precise permanent of $A,$ which is $\sharP$-hard.
\end{proof}

Notably, we achieve a robustness exponent of 5 compared to 6 for $m=\Theta(n^2)$ BosonSampling. However, for $m=O(n)$ the target robustness is larger, namely $1/|\Smn| = e^{-O(n)}.$ Closing the robustness gap altogether remains open for all quantum computational advantage proposals. 

\section{Discussion}

Our work better connects the theory of BosonSampling to its empirical implementation.
Aaronson and Arkhipov's foundational work led to a number of important extensions which improved our understanding of the complexity of BosonSampling---from generalizations to Gaussian BosonSampling, to improving the robustness of average-case hardness arguments, to efficiently spoofing or verifying of experiments, to characterizing the effects of noise. 
While many of the more empirical works have focused on the low-mode regime due to its connection with experiment, most of the theoretical arguments have  focused on high-mode regime and may need to be re-investigated in this new context.

Many interesting questions remain.
For example, can we improve the robustness of average-case hardness of these experiments to $e^{-O(n)}$, i.e.\ to be only off by a constant in the exponent as with high-mode BosonSampling \cite{Bouland2021}?
Do spoofing algorithms for BosonSampling become easier or harder in the low-mode case?
How few modes are needed for intractability, for example, for which $\alpha$ do $m=\alpha n$ experiments have the best evidence for hardness,
and are there any fundamental limits on this constant? 

\section*{Acknowledgments}
A.B. and I.D. were supported in part by the AFOSR under grant FA9550-21-1-0392.
A.B. and B.F. were supported in part by the DOE QuantISED grant DE-SC0020360. 
A.B. was supported in part by the U.S. DOE Office of Science under Award Number DE-SC0020266.
D.J.B. acknowledges financial support from Brazilian funding agencies FAPERJ, CNPq, and Instituto Nacional de
Ciencia e Tecnologia de Informa\c{c}\~ao Qu\^antica (INCT-IQ).
B.F. acknowledges support from AFOSR (FA9550-21-1-0008). M.O was  supported by the TEAM-NET project
co-financed by the EU within the Smart Growth Operational Program (contract no. POIR.04.04.00-00-
17C1/18-00).  This material is based upon work partially
supported by the National Science Foundation under Grant CCF-2044923 (CAREER) and by the U.S. Department of Energy, Office of Science, National Quantum Information Science Research Centers (Q-NEXT).
F.H. was supported by the Fannie and John Hertz Fellowship.
This work was done in part while A.B., I.D., B.F., and D.G. were visiting the Simons Institute for the Theory of Computing.

\bibliographystyle{alpha}
\bibliography{bosref.bib}

\newpage
\section*{Appendices}

\appendix

\section{Stockmeyer reduction for hardness of sampling}\label{sec:stockmeyer}

In this section we prove Theorem \ref{thm:no-sampler}, that Conjecture \ref{conj:UPEap} implies no efficient classical algorithm for $m=O(n)$ BosonSampling assuming the Polynomial Hierarchy does not collapse. 

We first formalize the notion of a classical sampler:

\begin{defn}[Classical sampler]\label{def:sampler}
A classical sampler is a probabilistic polynomial-time algorithm that, given as input $n\times n $ matrix $U_S$ generated as in Problem \ref{prob:super}, and error $\beta\geq0$ in unary\footnote{That error $\beta$ is provided to the classical sampler in unary signifies that smaller $\beta$ corresponds to longer runtime.}, outputs a sample $S=(s_1,\ldots,s_m)$ from distribution $\mathcal{P}'$ such that 
\[\norm{\mathcal{P}'-\mathcal{P}}_{TV} \leq \beta,\]
    where $\mathcal{P}$ is the outcome distribution of the BosonSampling experiment.
\end{defn}

As is standard among hardness of sampling results, we reduce classical sampling to approximately computing output probabilities. The reduction uses Stockmeyer's approximate counting algorithm \cite{stockmeyer1983complexity}, which runs in \textsf{BPP} with access to an \textsf{NP} oracle\footnote{The idea of Stockmeyer's algorithm is to estimate the probability of any outcome by estimating the number of random strings that cause the sampler to output that outcome. This uses that a classical randomized algorithm can be treated as a deterministic algorithm that takes a random input.}. It is the ability in $\mathsf{BPP}^\mathsf{NP}$ to approximate output probabilities, which are expressed in terms of squared matrix permanents we conjecture to be $\sharP$-hard, that draws a contradiction if \textsf{PH} is infinite.

To prove the Stockmeyer reduction we require a technical lemma we term ``reverse embedding,'' which allows us to formally connect the classical sampler, which receives as input an $n\times n$ matrix drawn from the correlated distribution $\mathcal{D}$ of sub-unitaries with row repetitions, and Stockmeyer's algorithm, which receives as input a unitary. Reverse embedding arises whenever there is a mismatch between algorithm input types. Notably, this technicality is absent in Random Circuit Sampling, where the input to both the classical sampler and Stockmeyer is the circuit unitary.

\begin{lem}[Reverse Embedding]\label{lem:reverse-embedding}
    Given as input an $n\times n$ matrix $U_S$ drawn from distribution $\mathcal{D}$ as in Problem \ref{prob:super}, in $\mathsf{BPP}^\mathsf{NP}$ one may output a Haar-random unitary matrix $U$ containing as a submatrix the distinct rows of $U_S$.
\end{lem}
In particular, the distinct rows of $U_S$ form a submatrix of the $m \times n$ truncation of $U,$ i.e.\ its leftmost $n$ columns.

\begin{proof}
    In \textsf{BPP}, generate unitary $U\sim\Haar(m)$ and with the \textsf{NP} oracle, post-select on $U$ containing the distinct rows of $U_S$ as a submatrix of the leftmost $n$ columns of $U.$ By the definition of a marginal distribution, such a $U$ exists.
\end{proof}

\begin{thm*}[Stockmeyer reduction] \label{thm:stockmeyer}

If there exists a classical sampler as in Def. \ref{def:sampler}, then there exists in $\mathsf{BPP}^\mathsf{NP}$ a solution to $\superap$ to additive imprecision $\frac{1}{\poly{n}}\prod_i s_i!/\vert\Smn\vert$, where the $(s_i)$ come from $S\sim\mathcal U_{m,n}$ in Problem \ref{prob:super}. 
\end{thm*}

\newpage
\begin{proof}
Suppose a classical sampler exists. Recall from Def.~\ref{def:sampler} it is given as input $U_S\sim\mathcal D.$ By Reverse Embedding (Lemma \ref{lem:reverse-embedding}), we can in $\mathsf{BPP}^\mathsf{NP}$ generate a Haar-random unitary matrix $U$ containing $U_S$ as a submatrix. It is this unitary $U$ that Stockmeyer's algorithm will take as input below. Moreover, we inherit parameter $\delta$ from Problem \ref{prob:super} and set $\beta:=\delta/16.$

For notational convenience, define $p_S:=\mathcal P(S)$ and $q_S:= \mathcal{P}'(S),$ the true probability of outcome $S$ and the probability the sampler draws $S,$ respectively. Observe that because $\mathcal P$ and $\mathcal P'$ are $\beta-$close in total variation distance, $p_S$ and $q_S$ are close with high probability:

\[\mathbb E_{U_S} \vert p_S-q_S\vert = 
\frac{1}{\vert\Smn \vert} \sum_{S \in \Smn} \vert \mathcal P(S) -\mathcal{P}'(S) \vert \leq 
\frac{1}{\vert\Smn\vert} \sum_{S \in \mathbb N^m} \vert \mathcal P(S) -\mathcal{P}'(S) \vert =
\frac{2 \norm{\mathcal P - \mathcal P'}_{TV}}{\vert\Smn\vert} \leq
\frac{2\beta}{\vert\Smn\vert}.
\]

By Markov's inequality, 

\[
\Pr_{U_S\sim\mathcal D}\left[ \vert p_S - q_S \vert > \frac{2\beta k}{\vert \Smn\vert} \right] < \frac{1}{k}.
\]

Setting $k := 4/\delta$ and recalling $\beta = \delta / 16$, we have
$$
\Pr_{U_S\sim\mathcal D} \left[ \vert p_S-q_S\vert > \frac{1}{2 |\Smn |} \right] < \frac{\delta}{4}.
$$
To obtain a good approximation to $q_S$ and therefore by proxy $p_S$ by the above, run Stockmeyer's algorithm that in $\mathsf{BPP}^\mathsf{NP}$ takes as input unitary $U$ and for any outcome $T$ estimates $q_T$ as $\widetilde{q}_T,$ such that
\[
\Pr\left[ |\widetilde q_S - q_S| > \frac{1}{\poly{n}}\cdot q_S \right] < \frac{\delta}{2}.
\]
The accuracy of estimate $\widetilde{q}_S$ depends on the magnitude of $q_S,$ which trivially satisfies 
$\mathbb E_{U_S} [q_S] \leq |\Smn|^{-1},$ as $q_S\leq 1.$ So again by Markov, 
\[
\Pr_{U_S\sim\mathcal D} \left[q_S > \frac{k}{|\Smn|}\right] < \frac{1}{k}.
\]

Finally, we set $k := 4/\delta$.
We want to bound how far the Stockmeyer approximation to the classical sampler's output probability for $S$ can differ from the true probability by more than $(\poly{n}|{\Smn}|)^{-1}$ in additive terms. We do so by a union bound: 
\begin{align*}
\Pr \left[ |\widetilde q_S - p_S| > \frac{1}{\poly{n}|{\Smn}|} \right]
&\le \Pr \left[ |\widetilde q_S - q_S| > \frac{1}{2} \cdot \frac{1}{\poly{n}|{\Smn}|} \right] +
\Pr \left[ |q_S - p_S| > \frac{1}{2} \cdot \frac{1}{|{\Smn}|} \right] \nonumber\\
&\le \Pr\left[q_S > \frac{4}{\delta |{\Smn}|} \right] +
\Pr \left[ |\widetilde q_S - q_S| > \frac{1}{\poly{n}} q_S \right]\\& +
\Pr \left[ |p_S-q_S| > \frac{1}{2} \cdot \frac{1}{|{\Smn}|} \right] \nonumber\\
&<  \frac{\delta}{4} + \frac{\delta}{2} + \frac{\delta}{4},
\end{align*}
where the probability is over $U_S$ and the randomness of the approximate counting procedure. By Eq.~\ref{eq:probability}, this gives an estimate to $\vert\operatorname*{Per}(U_S)\vert^2$ to $\pm \frac{1}{\poly{n}}\prod_i s_i!/\vert\Smn\vert.$ 

\end{proof}

\section{Random measurement outcomes have many occupied modes}\label{sec:manyclicks}

Crucial to our hardness proof is the property that bosons do not bunch too much, that is that there are $\Theta(n)$ occupied modes upon measurement; we will often refer to the number of occupied modes as \textit{clicks}. This property is important because the bosonic transition amplitudes are proportional to permanents of matrices with repeated rows according to the collision profile. The rank of this matrix is precisely the number of clicks. 

There exists an efficient classical algorithm to compute the permanent of constant rank matrices, due to Barvinok \cite{barvinok1996two}. In this way, were bosons to occupy $O(1)$ modes in the high-collision regime then their transition amplitudes would be easy to compute. That this is \textit{not} the case for $m=\Theta(n)$
BosonSampling is the content of this section. Lemma \ref{lem:manyclicks} shows that $m=\alpha n$ BosonSampling is dominated by outcomes with at least $(\frac{\alpha}{\alpha+2})n$ clicks.
Thus, the matrices induced by collisions in this regime inherit the hardness of computing permanents for almost all outcomes.

Throughout, we write the outcome i.e.\ collision pattern as a tuple of occupation numbers $S = (s_1,\ldots,s_m).$ 
We use $\mathcal S_{m,n}$ for the set of possible outcomes $S=(s_1,\cdots,s_m)$ where $s_j$ denotes the number of photons measured in the $j$-th mode. Similarly, we use $\mathcal{S}^c_{m,n}$ to denote the subset of $\mathcal{S}_{m,n}$ of outcomes with exactly $c$ clicks, i.e., $c$ occupied modes, $c \coloneqq \lvert \{s_i \mid s_i\geq 1\}\rvert $, where $c\in\{1,\ldots,\min(m,n)\}.$ Observe $\mathcal{S}_{m,n} = \bigcup_{c} \mathcal{S}^c_{m,n}.$

The proof of Lemma \ref{lem:manyclicks} makes use of the following intermediary result.

\begin{lem}[Combinatorics of $c$ clicks]\label{lem:states-with-c-clicks}
    For $n,m \in \mathbb{N}$ and $c\in\{1,\ldots,\min(m,n)\},$ \[\lvert \Scmn \rvert = \binom{m}{c} \binom{n-1}{n-c}.\]
\end{lem}

\begin{proof}
The number of states with $c$ clicks are enumerated by first choosing which $c$ of the $m$ modes to ``click,'' i.e.\ allotting one boson each to $c$ modes from among $m.$ This contributes a factor of $\binom{m}{c}.$ Next, we allot the remaining $n-c$ bosons to the $c$ selected modes. The factor for this contribution is given by the standard stars-and-bars argument as $\binom{(n-c)+ c - 1}{n - c} = \binom{n-1}{n-c}.$
\end{proof}

We also make use of well-established bounds on binomial coefficients.

\begin{lem}[See \cite{MacWilliams}, Chapter 10]\label{lem:binomial-bounds}

Suppose $\lambda n \in \mathbb{N},$ where $0<\lambda < 1.$ Then, 
    \[b \cdot \frac{2^{nH_2(\lambda)}}{\sqrt{n\lambda(1-\lambda)}} \leq \binom{n}{\lambda n} \leq B \cdot \frac{2^{nH_2(\lambda)}}{\sqrt{n\lambda(1-\lambda)}},
    \]

    for some constants $0<b<B$ and where $H_2(p)$ is the binary entropy function. 
\end{lem}

\begin{proof}[Proof sketch.]
    Repeated application of the Stirling series bounds on $n!.$
\end{proof}

Now we may prove Lemma \ref{lem:manyclicks} (stated informally in Section \ref{sec:worst-case} as Lemma \ref{lem:clicksinformal}), that almost all outcomes in $\mathcal S_{m,n}$ have many clicks.

\begin{lem}[States with $\Theta(n)$ occupied modes dominate in the $m=O(n)$ regime] \label{lem:manyclicks}
    Let $m=\alpha n$ for any fixed $\alpha>0,$ and take $c_\ast = \lfloor \frac{\alpha}{\alpha+2}n \rfloor.$ Then, 
    \[\frac{\sum_{c = c_\ast + 1}^{\min(m,n)}\lvert\Scmn\rvert}{\vert\Smn\vert} = 1-o(1).\] That is, for $m=\alpha n$ almost all outcomes $S$ chosen uniformly at random have $\Theta(n)$ clicks.
\end{lem}

The consequence of Lemma \ref{lem:manyclicks} is that for almost any measured outcome of $m=O(n)$ BosonSampling,  the number of clicks $c$  is sufficiently large for $\operatorname*{Per}(X)$ to be $\sharP$-hard in Section \ref{sec:worst-case}.

\begin{proof}
    We show that $\sum_{c=0}^{c_\ast}\lvert\Scmn\rvert/\vert\Smn\vert = o(1).$ Explicit computation by Lemma \ref{lem:states-with-c-clicks} gives that

    \[\frac{\vert\mathcal{S}^{c+1}_{m,n}\vert}{\vert\mathcal{S}^{c}_{m,n}\vert} = \frac{(m-c)(n-c)}{c (c+1)}.\]

    
Observe that $\vert\mathcal{S}^{c+1}_{m,n}\vert/\vert\mathcal{S}^{c}_{m,n}\vert > 1 $ holds for any $c 
< \frac{\alpha}{\alpha+1}n,$ so that for our choice of $c_\ast = \lfloor \frac{\alpha}{\alpha+2}n \rfloor,$

\[\vert\Smn^1\vert < \vert\Smn^2\vert < \ldots < \vert\Smn^{c_\ast}\vert \implies \sum_{c=0}^{c_\ast} \vert\Smn^c\vert < c_\ast \vert \Smn^{c_\ast}\vert.\]

Thus, proving that $c_\ast \vert \Smn^{c_\ast}\vert$ is asymptotically dominated by $\vert \Smn \vert$ suffices to prove the claim. 

Using Lemma \ref{lem:states-with-c-clicks}, we may express $c_\ast \vert\Smn^{c_\ast}\vert/\vert\Smn\vert$ in terms of binomial coefficients, which in turn we use Lemma \ref{lem:binomial-bounds} to upper bound:

\begin{align}
c_\ast \cdot \frac{ \vert \Smn^{c_\ast}\vert}{\vert\Smn\vert} &= c_\ast \cdot \frac{\binom{m}{c_\ast} \binom{n-1}{n-c_\ast}}{\binom{n+m-1}{n}} 
\\[1em]&= 
\lfloor \frac{\alpha}{\alpha+2}n \rfloor \cdot \frac{\binom{~\alpha n~}{\lfloor \frac{\alpha}{\alpha+2}n \rfloor} \binom{~~n-1~~}{n - \lfloor \frac{\alpha}{\alpha+2}n \rfloor}}{\binom{~(\alpha+1)n~}{n}} \label{eq:ratio-of-binoms}
\\[1em]&= \mathcal{O}\left(\frac{2^{\alpha nH_2(\frac{1}{\alpha+2})} \cdot 2^{nH_2(\frac{2}{\alpha+2}
)}}{2^{(\alpha+1)nH_2(\frac{1}{\alpha+1})}}\right), \label{eq:competing-exponentials}
\end{align}
where in Eq.~\ref{eq:ratio-of-binoms} we have used that $m=\alpha n$ and to get Eq. \ref{eq:competing-exponentials} we apply Lemma \ref{lem:binomial-bounds} thrice, twice to upper bound the two binomial coefficients in the numerator and once to lower bound the binomial coefficient in the denominator. Our dropping $\poly{n}$ factors in Eq.~\ref{eq:competing-exponentials} is justified by the observation that for any $\alpha>0,$ the difference of exponents in the numerator and denominator of Eq.~\ref{eq:competing-exponentials} is negative:

\begin{equation}\label{eq:diff-of-exps}
\alpha H_2(\frac{1}{\alpha+2}) + H_2(\frac{2}{\alpha+2}) - (\alpha+1)H_2(\frac{1}{\alpha+1}) < 0,
\end{equation}

so that Eq.~\ref{eq:competing-exponentials} goes as $2^{-a\cdot n} = o(1)$ for positive $a.$
Eq.~\ref{eq:diff-of-exps} follows because by explicit computation, the unique zero of the LHS on $[0,\infty)$ is at $\alpha=0,$ and moreover its derivative with respect to $\alpha$ at $\alpha=0$ is negative. This decaying exponential in $n$ dominates the $\poly{n}$ factors coming from Lemma \ref{lem:binomial-bounds} and the linear-in-$n$ term coming from the factor of $c_\ast,$ so that indeed the outcomes with $\Theta(n)$ clicks asymptotically dominate. 

\end{proof}

\newpage
\section{Proof of shift-and-scale invariance}\label{sec:appx-invariance-pf}
\appxinvariance*

As a first step, we note that we can express the probability measure $\mu_\theta$
using dilations $D_\rho$ and
translations $T_H$ applied to the measure $\mu$.
We will define these operations in terms of their actions on probability densities;
we write $\mu = P(X)dX$ where $P\in L^1(\Complex^{n\times n})$ is a continuous density.

Given $\rho\in\Complex$, the dilation
operator $D_\rho$ is defined on probability densities on $\Complex^{n\times n}$ by
\[
(D_\rho P)(X) = |\rho|^{-2n^2} P(X/\rho).
\]
The factor of $\rho^{-2n^2}$ comes from the fact that the (real) dimension of
$\Complex^{n\times n}$ is $2n^2$, and ensures that $D_\rho P$ remains a probability density.

The translation $T_H$ is defined by
\[
(T_H P)(X) = P(X-H).
\]
It is clear that $T_A P$ is the probability density of the distribution of $X+A$
when $X$ is sampled from $P(X)dX$.


The proof of Theorem \ref{thm:appx-invariance} is divided into two steps, given as Lemmas \ref{lem:approximate-invariance} and \ref{lem:gradient-bounds}. 
With the definitions above, we can say that if $X_0$ is sampled from the probability
density $P$, then $X(t)$ is sampled from $T_{t A}D_{1-t} P$.
Lemma \ref{lem:approximate-invariance} characterizes the total variation distance between these
probability distributions in terms of an integral of $|\nabla P|$, for which we prove a bound in Lemma \ref{lem:gradient-bounds}.

For convenience, given matrices $A,B\in\Complex^{N\times N}$
we write $|A|$ for the Hilbert-Schmidt norm 
and $A\cdot B$ for the corresponding inner product.


\begin{lem}
\label{lem:approximate-invariance}
Let $P\in L^1(\Complex^N)$ be any integrable function.  Then for any
$A$,
\begin{equation}
\label{eq:translation-invariance}
\int |P(X) - T_A P(X)| \diff X \leq \int |A\cdot\nabla P(X)|\diff X.
\end{equation}
Moreover for any $|\delta|\leq \frac{1}{10}$,
\begin{equation}
\label{eq:dilation-invariance}
\int |P(X) - D_{1+\delta}P(X)| \diff X
\leq 10 |\delta| \big(N\int |P(X)|\diff X + \int |X| |\nabla P(X)|\diff X).
\end{equation}
\end{lem}
\begin{proof}
First we prove~\eqref{eq:translation-invariance} under the assumption that $P$
is continuously differentiable.  In this case we use the fundamental theorem of calculus 
in the form $f(1)-f(0) = \int_0^1 f'(t)\diff t$ applied
to the function $f(t) = P(X-tA)$, which gives
\begin{equation}
P(X) - P(X-A) = -\int_0^1 A \cdot \nabla P(X-tA) \diff t.
\end{equation}
Writing $P(X-A) = T_AP(X)$ and using the triangle inequality above we estimate
\begin{align*}
|P(X)-T_AP(X)| &= |P(X)-P(X-A)| \\
&\leq \int_0^1 |A\cdot \nabla P(X-tA)| \diff t.
\end{align*}

Integrating over $X$ and applying Fubini's theorem and then using the change of variables $Y=X-tA$
we have
\begin{equation}
\begin{split}
\int |P(X) - T_AP(X)| \diff X 
&= \int \int_0^1 |A\cdot \nabla P(X-tA)| \diff t \diff X \\
&= \int_0^1 (\int |A\cdot \nabla P(X-tA)| \diff X) \diff t \\
&= \int_0^1 \diff t \int |A\cdot \nabla P(Y)| \diff Y.
\end{split}
\end{equation}
The integral over $t$ now evaluates to $1$ and we have proved the result.
The extension to all $L^1$ functions follows from the density of smooth
functions in $L^1$.

Next we prove~\eqref{eq:dilation-invariance}.  Fixing $X$, define
\[
f_X(t) = |1+t\delta|^{-2N} P((1+t\delta)^{-1}X).
\]
Then we compute
\begin{align*}
f_X'(t) = &-2N |1+t\delta|^{-2N-2}\Rept(\delta^* (1+\delta t)) P((1+t\delta)^{-1}X) \\
&\qquad + |1+t\delta|^{-2N} (-\delta)(1+t\delta)^{-2}X\cdot \nabla P((1+t\delta)^{-1}X).
\end{align*}
In particular, using the bound $|\delta|\leq 1/10$, we obtain
\[
|f_X'(t)| \leq 10|1+t\delta|^{-2N} \delta ( NP((1+t\delta)^{-1}X) + |X||\nabla P((1+t\delta)^{-1}X)|).
\]
Now we use the triangle inequality with the fundamental theorem of calculus as before to estimate
\begin{align*}
\int |P(X)-D_{1+\delta}P(X)|\diff X
&= \int |f_X(0) - f_X(1)|\diff X  \\
&\leq \int_0^1 \int |f_X'(t)|\diff X \diff t \\
&\leq 10\delta  \int_0^1 \int |1+t\delta|^{-2N} [N|P((1+t\delta)^{-1}X)| + |X||\nabla P((1+t\delta)^{-1}X)]\diff X \diff t \\
&\leq 10\delta  \int_0^1 \int N|P(X)| + |X||\nabla P(X)]\diff X \diff t.
\end{align*}
In the last line we rescaled the variable $X$, which eliminated the factor of $|1+t\delta|^{-2N}$.
This concludes the proof.
\end{proof}

Having expressed translation and dilation of probability density $P$ in terms of an integral of $|\nabla P|,$ we bound the integral with an explicit formula in Lemma \ref{lem:gradient-bounds}.

\begin{lem}
\label{lem:gradient-bounds}
Let $P \in L^1(\Complex^{n\times n})$ be the probability density associated to the measure $\mu$,
and suppose $m\geq (2+\eps)n$.  Then there exists a constant $C_\eps$ such that
\begin{equation}
\label{eq:BV-bd}
\int |\nabla P(X)|\diff X \leq C_\eps n
\end{equation}
and also
\[
\int |X| |\nabla P(X)| \diff X \leq C_\eps n^2.
\]
\end{lem}

Equation~\eqref{eq:BV-bd} shows that for
$\{0,1\}$-valued matrices $A$ (for which $\|A\|_{HS}\leq n$),
\[
\int |A\cdot \nabla P(X)| = \int \|A\|_{HS} \|\nabla P(X)\|_{HS} \leq C_\eps n^2.
\]
Thus the bounds from Lemma~\ref{lem:gradient-bounds} combined with Lemma~\ref{lem:approximate-invariance}
complete the proof of Theorem~\ref{thm:appx-invariance}.

The proof of Lemma \ref{lem:gradient-bounds} makes use of the following result about the largest singular values of submatrices
of Haar-random unitaries, which we prove in the next subsection.
\begin{lem}
\label{lem:max-sing}
If $Z$ is the $n\times n$ submatrix of a  Haar-random $m\times m$ unitary with $m = \lceil(2+\eps)n\rceil$
and $n\geq 2\eps^{-1}$, then
\[
\Expec (1-\sigma_{max}(Z))^{-2} \leq C_\eps.
\]
In fact, one can take $C_\eps = 10^{3\eps^{-1}}$.
\end{lem}

\begin{proof}[Proof of Lemma~\ref{lem:gradient-bounds}] 
    We first write down an explicit formula for the density $P$.
As computed by B. Collins in~\cite{collins2003integrales}\footnote{See also the thesis~\cite[Eq.~61 on p.~61]{reffy} and~\cite{Aaronson2013}.}
for $m\geq 2n$, the density $P$ for the measure $\mu$ 
is given by
\[
P(X) = c_{m,n}\prod_{i\in [n]} (1-\frac{\sigma_i^2}{m})^{m-2n} I_{\sigma_i^2 \leq m},
\]
where $\sigma_i$ are the singular values of the matrix $X$.
A first step to computing $\nabla P$ is to first take the derivatives with respect to the
singular values,
\begin{align*}
\partial_{\sigma_j} P(X)
&=
-c_{m,n}
2\sigma_j
\frac{(m-2n)}{m}(1-\frac{\sigma_j^2}{m})^{m-2n-1} I_{\sigma_j^2\leq m}
\prod_{i\not=j} (1-\frac{\sigma_i^2}{m})^{m-2n} I_{\sigma_i^2\leq m} \\
&= -2\sigma_j \frac{m-2n}{m} \big(1-\frac{\sigma_j^2}{m}\big)^{-1} P(X).
\end{align*}
We can use this to bound $|\nabla P(X)|$.  To do this, we use a singular value decomposition
to reduce to the case that $X$ is the diagonal matrix $\Sigma$ with entries $\sigma_i$.  The derivative
of the singular values with respect to the off-diagonal entries is $0$ (it suffices to check this fact
for $2\times 2$ matrices).  The same is true for the imaginary parts of the diagonal.  Therefore
\begin{equation}
\label{eq:nablaP-est}
\|\nabla P(X)\|
= \big(\sum_{j\in[n]} |\partial_{\sigma_j} P(X)|^2\big)^{1/2}.
\end{equation}
To simplify this expression we use $(1-\sigma_i^2/m)^{-1} \leq (1-\sigma_{max}^2/m)^{-1}$
in the computation of $\partial_{\sigma_i} P(X)$,
\[
\partial_{\sigma_j} P(X) \leq 2 \sigma_j(1-\frac{\sigma_{max}^2}{m})^{-1} P(X),
\]
and therefore the Euclidean norm of the gradient can be bounded as follows
\[
|\nabla P(X)| \leq 2 \big(\sum_j \sigma_j^2\big)^{1/2} (1-\frac{\sigma_{max}^2}{m})^{-1} P(X)
\]

Integrating over $X$ and then using the Cauchy-Schwartz inequality we have
\[
\int |\nabla P(X)|\diff X
\leq 2 \big(\Expec \sum_j \sigma_j^2\big)^{1/2}\big(\Expec (1-\frac{\sigma_{max}^2}{m})^{-2}\big)^{1/2}
\]
where the expectation is over $X$ sampled from $\mu$.  Note that for the first expectation
$\sum_j \sigma_j^2 = |X|^2$ is the square of the Frobenius norm.  
Therefore
\[
\Expec \sum_j \sigma_j^2 = n^2,
\]
so
\[
\int |\nabla P| \diff X \leq 2 n \big(\Expec (1-\frac{\sigma_{max}^2}{m})^{-2}\big)^{1/2}.
\]
Noting that $\sigma_{max}/\sqrt{m}$ has the law of the max singular value of $Z$ (the unscaled submatrix of a Haar-random unitary), 
Lemma~\ref{lem:max-sing} implies that the expectation above is bounded by some $C_\eps$.  Therefore for $C'_\eps = 2\sqrt{C_\eps}$,
\begin{equation}
\int |\nabla P|\diff X \leq C'_\eps n.
\end{equation}

To estimate $\int |X||\nabla P|$, we use that $|X|^2=\sum \sigma_j^2$ and therefore
\[
|X||\nabla P| \leq 2 |X|^2 (1-\frac{\sigma_{max}^2}{m})^{-1} P(X).
\]
Integrating and using the Cauchy-Schwartz inequality it follows that
\[
\int |X||\nabla P| \diff X \leq 2 \big(\Expec |X|^4\big)^{1/2} \big(\Expec (1-\frac{\sigma_{max}^2}{m})^{-2}\big)^{1/2}
\leq C''_\eps n^2.
\]
This completes the proof of Lemma~\ref{lem:gradient-bounds}.
\end{proof}


\subsection{Bound on the maximum singular value of a submatrix}
In this subsection we prove Lemma~\ref{lem:max-sing}
 on the expectation $\Expec (1-\sigma(Z))^{-2}$ of a submatrix
 of a Haar-random unitary.  

In fact we will focus instead on the following  bound for the probability that the maximum singular value is close to $1$.

\begin{lem}
\label{lem:prob-max-sing}
Let $Z$ be the $n\times n$ upper left square submatrix of a
Haar-random $m\times m$ unitary matrix, and let $\delta < 1/2$.  Then
\[
\Prob(\sigma_{max}(Z)\geq 1-\delta) < 2^{5m+n+1} \delta^{2m-4n}.
\]
\end{lem}

Before proving Lemma~\ref{lem:prob-max-sing}, we show how it implies Lemma~\ref{lem:max-sing}.

\begin{proof}[Proof of Lemma~\ref{lem:max-sing} using Lemma~\ref{lem:prob-max-sing}]
Using the layer-cake formula,
\begin{equation}
\Expec (1-\sigma_{max}(Z))^{-2} =
\int_0^\infty \Prob( (1-\sigma_{max}(Z))^{-2} \geq t)\diff t.
\end{equation}
For $0\leq t\leq K$ we use the fact that the integrand is bounded by $1$.  Then, upon rearranging and
then applying Lemma~\ref{lem:prob-max-sing} we have
\begin{equation}
\begin{split}
\Expec (1-\sigma_{max}(Z))^{-2}
&\leq K + \int_K^\infty \Prob( (1-\sigma_{max}(Z))^{-2} \geq t)\diff t \\
&= K + \int_K^\infty \Prob( \sigma_{max}(Z) \geq  1- t^{-1/2})\diff t \\
&\leq K + 2^{5m+n+1} \int_K^\infty t^{2n-m} \diff t.
\end{split}
\end{equation}
We compute the latter integral using $m=\lceil(2+\eps)n\rceil$ and $n > 2\eps^{-1}$ to find
\begin{equation}
\begin{split}
\Expec (1-\sigma_{max}(Z))^{-2}
&\leq K + 2^{6m} \int_K^\infty t^{-\eps n} \diff t\\
&= K + 2^{6m} (\eps n-1)^{-1} K^{1-\eps n} \\
&\leq  K + 2^{6m} (\eps n/2)^{-1} K^{-\eps n/2}.
\end{split}
\end{equation}
We apply this bound with the choice $K = 500^{1/\eps}$ to conclude.
\end{proof}

\subsection{Interlude: facts about spheres}
In this section we use $\omega_n$ to denote the volume of a ball of radius $r$ in $n$ dimensions.  There is an
explicit formula for $\omega_n$ in terms of the Gamma function,
\begin{equation}
\label{eq:sphere-vol}
\omega_n = \frac{\pi^{n/2}}{\Gamma(n/2+1)}r^n.
\end{equation}
The surface area of an $(n-1)$-dimensional sphere in $n$ dimensions is given by $n\omega_nr^{n-1}$.

We give an elementary calculation regarding the surface area of the intersection of a sphere
and a ball.
\begin{lem}
\label{lem:cap-volume}
Let $w_0\in\Sphere^{2n-1}$ be any unit vector and let $\delta<1$.  Then
\[
\vol_{2n-1} \{w\in\Sphere^{2n-1} \mid \|w-w_0\|\leq \delta\}
\geq \omega_{2n-1} 2^{-n-1/2} \delta^{2n-1}.
\]
\end{lem}

\begin{figure}
\centering
\includegraphics{figures/spherical-cap-fig.pdf} 
\caption{The spherical cap in a $\delta$ radius around $w_0$.  The shaded region is a spherical cap, whose boundary is a circle (in general, $d-2$-dimensional sphere).  The area of the cap is strictly larger than the area of the flat disk of radius $\sin\theta$ with the same boundary.  The value of $\theta$ is determined by the equation
$(1-\cos(\theta))^2 + (\sin\theta)^2 = \delta$, which can be seen by inspecting the right triangle drawn with vertex at $w_0$ and hypotenuse of length $\delta$.}
\label{fig:sphere-fig}
\end{figure}

\begin{proof}
The set $\{w\in\Sphere^{2n-1} \mid \|w-w_0\|\leq\delta\}$ forms a spherical cap whose boundary is a
$2n-2$-dimensional sphere of radius $\sin(\theta)$, where $\theta$ is the angle satisfying
\[
\delta^2 = (1-\cos\theta)^2 + \sin^2\theta.
\]
See Figure~\ref{fig:sphere-fig} for a diagram of the spherical cap.
\
In terms of $r=\sin\theta$, which is the radius of the boundary of the spherical cap, we can write
\[
\delta^2 = (1-\sqrt{1-r^2})^2 + r^2.
\]
When $r<1$ (which is guaranteed by $\delta < 1$),
$(1-\sqrt{1-r^2}) < r$, so $\delta^2 \leq 2r^2$.  Therefore $r \geq \delta / \sqrt{2}$.

The volume of the spherical cap is larger than the volume of the flat disk of radius $r$ with the same boundary,
which concludes the proof.
\end{proof}

\begin{lem}
\label{lem:small-coords}
If $X=(x_1,x_2,\cdots,x_n)$ is sampled uniformly from the unit sphere, then
\[
\Prob(\sum_{j=1}^k x_j^2 \leq \delta) \leq 2^{n/2} \delta^k.
\]
\end{lem}
\begin{proof}
The first $k$ coordinates of $X$ are sampled from a probability distribution with density
\[
p_{k,n}(x_1,\dots,x_k) = \frac{\Gamma(n/2)}{\Gamma((n-k)/2)\pi^{k/2}} \big(1 - \sum_{j=1}^k x_j^2\big)^{(n-k)/2-1}.
\]
In particular the density is bounded by $\frac{\Gamma(n/2)}{\Gamma((n-k)/2) \pi^{k/2}}$, and the event that
$\sum_{j=1}^k x_j^2 \leq \delta$ is a ball of volume $\omega_k \delta^k$.  Using~\eqref{eq:sphere-vol}
to express $\omega_k$, we arrive at the bound
\[
\Prob(\sum_{j=1}^k x_j^2 \leq \delta) \leq \frac{\Gamma(n/2)}{\Gamma((n-k)/2)\Gamma(k/2+1)} \delta^k.
\]
Note that the prefactor involving the ratio of Gamma functions is the binomial coefficient $\binom{n/2-1}{k/2}$,
and is therefore bounded by $2^{n/2}$, which concludes the proof.
\end{proof}

\subsection{Proof of Lemma~\ref{lem:prob-max-sing}}

We think of the probability $\Prob(\sigma_{max}(Z) \geq 1-\delta)$ as the expectation of the indicator function
$\Expec \One_{\sigma_{max}(Z)\geq 1-\delta}$.  Though the indicator function is difficult to work with, fortunately
we can bound it from above by a simpler function $f(Z)$ given by
\[
f_\delta(Z) := \vol_{2n-1}\{w\in \Sphere^{2n-1} \mid \|Zw\| \geq 1-2\delta\}
= \int_{\Sphere^{2n-1}} \One_{\|Zw\| \geq 1-2\delta} \diff w.
\]
Here we think of an $n$-dimensional complex vector $w$ as a $2n$-dimensional real-valued vector.

We break the proof of Lemma~\ref{lem:prob-max-sing} into two parts.  In the first part, we show that
$f_\delta(Z)$ controls the indicator function $\One_{\sigma_{max}(Z)\geq 1-\delta}$.  In the second part,
we compute the expectation of $f_\delta(Z)$.

For the first part, our aim is to prove that, when $\delta < 1$,
\begin{equation}
\label{eq:indicator-vs-f}
f_\delta(Z) \geq \omega_{2n-1}2^{-n-1/2} \delta^{2n-1}
\One_{\sigma_{max}(Z) \geq 1-\delta}.
\end{equation}
This is equivalent to showing that, when $\sigma_{max}(Z)\geq 1-\delta$,
\[
\vol_{\Sphere^{2n-1}}\{w\in \Sphere^{2n-1} \mid \|Zw\| \geq 1-2\delta\}
\geq \omega_{2n-1}2^{-n-1/2} \delta^{2n-1}.
\]
Now suppose that $\sigma_{max}(Z)\geq 1-\delta$, so that in particular there exists a unit
vector $w_0$ such that $\|Zw_0\| \geq 1-\delta$.  Then, for unit vectors $w$ such that $\|w-w_0\|\leq \delta$,
\[
\|Zw\| \geq \|Zw_0\| - \|Z(w-w_0)\| \geq 1-2\delta.
\]
Therefore
\[
\vol_{\Sphere^{2n-1}}\{w\in \Sphere^{2n-1} \mid \|Zw\| \geq 1-2\delta\}
\geq
\vol_{\Sphere^{2n-1}}\{w\in \Sphere^{2n-1} \mid \|w-w_0\|\leq \delta\}.
\]
Now~\eqref{eq:indicator-vs-f} follows from Lemma~\ref{lem:cap-volume}.

For the second part of the proof we bound $\Expec f_\delta(Z)$.  Using rotational symmetry
we have
\begin{equation}
\begin{split}
\Expec f_\delta(Z) &= \int_{\Sphere^{2n-1}} \Prob(\|Zw\|\geq 1-2\delta) \diff w \\
&= \omega_{2n-1} \Prob(\|Ze\| \geq 1-2\delta),
\end{split}
\end{equation}
where $e=(1,0,\cdots,0)$.  The random vector $Ze$ consists of the first $n$ coordinates of the first column of a
Haar-random unitary, which is simply a uniformly random vector in the unit sphere.  Therefore
\[
\Prob(\|Ze\|\geq 1-2\delta) = \Prob(\sum_{j=1}^{2n} x_j^2 \geq (1-2\delta)^2).
\]
when $(x_1,\cdots,x_{2m})$ is sampled from the unit sphere.  Rewriting the latter probability as the
event that the final $2m-2n$ coordinates are small and then applying Lemma~\ref{lem:small-coords} we find
\[
\Prob(\sum_{j=1}^{2n} x_j^2 \geq (1-2\delta)^2)
= \Prob(\sum_{j=2n+1}^{2m} x_j^2 \leq 4(\delta - \delta^2))
\leq 32^{m-n} \delta^{2m-2n}.
\]

Combining our two bounds on $f_\delta(Z)$ we have
\begin{equation}
\omega_{2n-1} 2^{-n-1/2} \delta^{2n-1} \Prob(\sigma_{max}(Z) \geq 1-\delta)
\leq
\Expec f_\delta(Z)
\leq \omega_{2n-1} 32^{m-n} \delta^{2m-2n}.
\end{equation}
Lemma~\ref{lem:prob-max-sing} now follows from rearranging.

\end{document}






















