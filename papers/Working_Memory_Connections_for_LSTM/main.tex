%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%%
%% $Id: prletters-template-with-authorship.tex 69 2013-07-15 10:15:25Z rishi $
%%
%% This template has no review option
%% 
%% Use the options `twocolumn,final' to obtain the final layout
% \documentclass[times,5p,authoryear]{elsarticle}
\documentclass[times,authoryear]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

%% Stylefile to load PR Letters template
% \usepackage{prletters}
\usepackage{framed,multirow}

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{latexsym}

% Following three lines are needed for this document.
% If you are not loading colors or url, then these are
% not required.
\usepackage{url}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage[noadjust]{cite}
\definecolor{newcolor}{rgb}{.8,.349,.1}


\def \ie {\emph{i.e.}}
\def \eg {\emph{e.g.}}
\def \etal {\emph{et al.}}

\newcommand{\rev}[1]{\textcolor{black}{#1}}

\journal{Neural Networks}

\begin{document}
% \clearpage

%\input{sections/authorship}
%
%\input{sections/graphical_abstract}
%
%\ifpreprint
%  \setcounter{page}{1}
%\else
%  \setcounter{page}{1}
%\fi

\begin{frontmatter}

\title{Working Memory Connections for LSTM}

\author[1]{Federico Landi\corref{cor1}} 
\cortext[cor1]{Corresponding author: 
  % Tel.: +39-059-2058792;  
  % fax: +0-000-000-0000;
  }
\ead{federico.landi@unimore.it}
\author[1]{Lorenzo Baraldi}
\author[1]{Marcella Cornia}
\author[1]{Rita Cucchiara}

\address[1]{Department of Engineering ``Enzo Ferrari'', University of Modena and Reggio Emilia, Modena, Italy}

%\received{1 February 2021}
%\finalform{}
%\accepted{}
%\availableonline{}
%\communicated{}

\begin{abstract}
Recurrent Neural Networks with Long Short-Term Memory (LSTM) make use of gating mechanisms to mitigate exploding and vanishing gradients when learning long-term dependencies. For this reason, LSTMs and other gated RNNs are widely adopted, being the standard \textit{de facto} for many sequence modeling tasks. Although the memory cell inside the LSTM contains essential information, it is not allowed to influence the gating mechanism directly. In this work, we improve the gate potential by including information coming from the internal cell state. The proposed modification, named Working Memory Connection, consists in adding a learnable nonlinear projection of the cell content into the network gates. This modification can fit into the classical LSTM gates without any assumption on the underlying task, being particularly effective when dealing with longer sequences.
Previous research effort in this direction, which goes back to the early 2000s, could not bring a consistent improvement over vanilla LSTM.
As part of this paper, we identify a key issue tied to previous connections that heavily limits their effectiveness, hence preventing a successful integration of the knowledge coming from the internal cell state. We show through extensive experimental evaluation that Working Memory Connections constantly improve the performance of LSTMs on a variety of tasks. Numerical results suggest that the cell state contains useful information that is worth including in the gate structure.
\end{abstract}

\begin{keyword}
Long Short-Term Memory Networks\sep Cell-to-Gate Connections\sep Gated RNNs \sep Language Modeling \sep Image Captioning
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

%\linenumbers

\section{Introduction}
\label{sec:introduction}
\input{sections/introduction}

\section{Related Work}
\label{sec:related}
\input{sections/related}

\section{Proposed Method}
\label{sec:method}
\input{sections/method}

\section{Experiments and Results}
\label{sec:results}
\input{sections/results}

\section{Discussion}
\label{sec:discussion}
\input{sections/discussion}

\section{Conclusion}
\label{sec:conclusion}
\input{sections/conclusion}

\section*{Acknowledgments}
This work has been supported by ``Fondazione di Modena'' under the project ``AI for Digital Humanities'' and by the national project ``IDEHA: Innovation for Data Elaboration in Heritage Areas'' (PON ARS01\_00421), cofunded by the Italian Ministry of University and Research.

\bibliographystyle{model2-names}
\bibliography{refs}

% \section*{Supplementary Material}

% Supplementary material that may be helpful in the review process should be prepared and provided as a separate electronic file. That file can then be transformed into PDF format and submitted along with the manuscript and graphic files to the appropriate editorial office.

\end{document}
