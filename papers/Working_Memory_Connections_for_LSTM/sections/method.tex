In this section, we present a complete overview of Working Memory Connections. First, we recall the LSTM equations. Second, we explain the modifications introduced in our design. Finally, we motivate the choices behind WMCs \textit{w.r.t.}~other approaches. Specifically, we identify key problems in previous cell-to-gate connections that hinder the learning process, and we show that the proposed solution does not suffer from these weaknesses.

\subsection{LSTM}
The core idea behind Long Short-Term Memory networks is to create a constant error path between subsequent time steps.
Being $\mathbf{x}_t$ the input vector at time $t$ we can write the rollout equations for a vanilla LSTM as:
\begin{align}
    &\mathbf{g}_t = \text{tanh}
                    (\mathbf{W}_{gx}\mathbf{x}_{t} +
                    \mathbf{W}_{gh} \mathbf{h}_{t-1} +
                    \mathbf{b}_g) \\
    &\mathbf{i}_t = \sigma
                    (\mathbf{W}_{ix}\mathbf{x}_{t} +
                    \mathbf{W}_{ih} \mathbf{h}_{t-1} +
                    %\mathbf{W}_{ic} \mathbf{c}_{t-1} +
                    \mathbf{b}_i)
                    \label{eq:i} \\
    &\mathbf{f}_t = \sigma
                    (\mathbf{W}_{fx}\mathbf{x}_{t} +
                    \mathbf{W}_{fh} \mathbf{h}_{t-1} +
                    %\mathbf{W}_{fc} \mathbf{c}_{t-1} +
                    \mathbf{b}_f)
                    \label{eq:f} \\
    &\mathbf{c}_t = \mathbf{f}_t \odot \mathbf{c}_{t-1} +
                    \mathbf{i}_t \odot \mathbf{g}_t \\
    &\mathbf{o}_t = \sigma
                    (\mathbf{W}_{ox}\mathbf{x}_{t} +
                    \mathbf{W}_{oh} \mathbf{h}_{t-1} +
                    %\mathbf{W}_{oc} \mathbf{c}_{t} +
                    \mathbf{b}_o)
                    \label{eq:o} \\
    &\mathbf{h}_t =  \mathbf{o}_t \odot \text{tanh}(\mathbf{c}_t).
                    \label{eq:h}
\end{align}
Here, $\mathbf{g}$ is the block input, $\mathbf{i}$, $\mathbf{f}$, and $\mathbf{o}$ are respectively the input, forget, and output gates, $\mathbf{c}$ represents the memory cell value, and $\mathbf{h}$ is the block output. In this notation, $\sigma$ is the sigmoid function and $\odot$ denotes element-wise Hadamard product.
In its first formulation~\citep{hochreiter1997long}, LSTM did not include the multiplicative forget gate. However, being able to forget about past inputs~\citep{gers2000forget} allows LSTM to tackle longer sequences while not hindering the back-propagation of the error signal.

% \begin{figure*}[!t]
%     \centering
%     \begin{minipage}[t][][t]{0.60\textwidth}
%         \includegraphics[width=0.98\textwidth]{figures/fig1.pdf}
%         \subcaption{\label{fig:1a}}
%     \end{minipage}
%     \begin{minipage}[t][][t]{0.35\textwidth}
%         \includegraphics[width=0.98\textwidth]{figures/c_t.pdf}
%         \subcaption{\label{fig:1b}}
%     \end{minipage}
% \caption{Comparison between a vanilla LSTM gate, a peephole connection, and a Working Memory Connection (a). The cell state $c_t$ may grow linearly with the number of time steps. Peephole connections directly expose $c_t$, creating a key issue (b). Data for this plot is taken from the first training iterations of the sequential MNIST (see \textsection~\ref{sec:smnist}).}
% \label{fig:figure_1}
% % \vspace{-0.15cm}
% \end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[height=3.8cm]{figures/fig1.pdf}
    \caption{Comparison between a vanilla LSTM gate, a peephole connection, and a Working Memory Connection.}
\label{fig:1a}
% \vspace{-0.15cm}
\end{figure*}

\subsection{Working Memory Connections}
In the following, we introduce Working Memory Connections, which enable the memory cell to influence the value of the gates through a set of recurrent weights. Given a proper design for the connection, we argue that there is a practical advantage in letting the cell state influence the gating mechanisms in the LSTM block directly. In fact, the cell state $\mathbf{c}_t$ provides unique information about the previous time steps that are not present in $\mathbf{h}_t$. 
%
For instance, $\mathbf{h}_t$ may be close to zero as a consequence of the output gate saturating towards zero (see Eq.~\ref{eq:h}), while $\mathbf{c}_t$ may be growing and changing as a result of a sequence of input vectors. In that case, since the cell state cannot control the output gate, the LSTM block is forced to learn which particular value in the input vector is the marker that signals to open the output gate. Instead, with an appropriate connection strategy, the LSTM block could learn a mapping between the cell internal state and the gate values.

Our solution employs a set of recurrent weights  $W_{\star c}$, $\star \in \{\mathbf{i}, \mathbf{f}, \mathbf{o}\}$ and a nonlinear activation function to model a connection between memory cell and gates. The application of a non-linearity on the memory cell is coherent with the present LSTM structure: as it can be noticed from Eq.~\ref{eq:h}, a nonlinear activation function is applied to $\mathbf{c}_t$ before the Hadamard product with $\mathbf{o}_t$\footnote{Previous works~\citep{Greff2017LSTMAS} have also shown that removing this non-linearity leads to a significant loss in terms of performance.}. 
\rev{In light of the above-mentioned intuitions, we modify Eq.~\ref{eq:i},~\ref{eq:f}, and~\ref{eq:o} by exposing the cell state $\mathbf{c}_t$ at time $t$ through a protection mechanism as follows:} %(we provide a formal analysis supporting this insight in Sec.~\ref{subsec:ph}).
\begin{align}
    &\mathbf{i}_t = \sigma
                    (\mathbf{W}_{ix}\mathbf{x}_{t} +
                    \mathbf{W}_{ih} \mathbf{h}_{t-1} +
                    \text{tanh}(\mathbf{W}_{ic} \mathbf{c}_{t-1}) +
                    \mathbf{b}_i) 
                    \label{eq:pv2i} \\
    &\mathbf{f}_t = \sigma
                    (\mathbf{W}_{fx}\mathbf{x}_{t} +
                    \mathbf{W}_{fh} \mathbf{h}_{t-1} +
                    \text{tanh}(\mathbf{W}_{fc} \mathbf{c}_{t-1}) +
                    \mathbf{b}_f) 
                    \label{eq:pv2f} \\
    &\mathbf{o}_t = \sigma
                    (\mathbf{W}_{ox}\mathbf{x}_{t} +
                    \mathbf{W}_{oh} \mathbf{h}_{t-1} +
                    \text{tanh}(\mathbf{W}_{oc} \mathbf{c}_{t}) +
                    \mathbf{b}_o),
                    \label{eq:pv2o}
\end{align}
where $\mathbf{W}_{\star c} \mathbf{c}_t$ denotes a general linear transformation.

At a first glance, Working Memory Connections may seem redundant in the gate structure. In fact, $h_{t-1}$ depends from the value of $c_{t-1}$ (Eq.~\ref{eq:h}). This impression is misleading, as the proposed connections introduce two main aspects of novelty. First, the non-linear activation function operates on three different projections of the cell state, one for each gate type. Second, Eq.~\ref{eq:pv2o} shows that the connection on the output gate depends on $c_t$, rather than on $c_{t-1}$, hence allowing for a more responsive control of the output dynamics of the entire LSTM block.

%Since this strategy includes actively the content of the memory cell $\mathbf{c}_t$ in the gate mechanisms, we name it \textit{Working Memory Connection}.
%In the following lines, we motivate the use of the nonlinearity in our novel connection.
%While doing so, we compare with a similar type of connections, namely peephole connections~\citep{gers2000recurrent}, and formally point out a key issue in their formulation. We will then show that working memory connections, thanks to the shielding provided by the nonlinear activation function, do not suffer from the same limitations.

\subsection{Advantages of Working Memory Connections}
\label{subsec:ph}
To formally motivate the improvement given by Working Memory Connections, we start by considering the local gradients of the gates in which the cell interaction is added. We limit our formal analysis to the input gate $\mathbf{i}_t$, but our reasoning can be generalized to $\mathbf{f}_t$ and $\mathbf{o}_t$.
%
If we denote by $\mathbf{\bar i}_t$ the argument of the sigmoid activation function (Eq.~\ref{eq:pv2i}) at time $t$:
\begin{align}
        \mathbf{\bar i}_t &= \mathbf{W}_{ix}\mathbf{x}_{t} +
                                     \mathbf{W}_{ih} \mathbf{h}_{t-1} +
                                     \text{tanh}(\mathbf{W}_{ic} \mathbf{c}_{t-1}) +
                                     \mathbf{b}_i ,
                                     %\label{eq:pv2}
\end{align}
\rev{then the local gradient of the input gate $\mathbf{i}_t$ is expressed by:
\begin{align}
     \frac{\partial\mathbf{i}_t}{\partial\mathbf{\bar i}_t} &=
     \frac{\partial}{\partial\mathbf{\bar i}_t} \sigma \left(\, \mathbf{\bar i}_t \right) =
     %\sigma^\prime \left(\, \mathbf{\bar i}_t \right) = 
                          \textit{diag}\left[
                          \sigma\left(\, \mathbf{\bar i}_t \right) \odot
                          \left(\mathbf{1} - \sigma\left(\, \mathbf{\bar i}_t \right) \right)
                          \right],
\end{align}
where $\mathbf{1}$ denotes a vector of ones, and $\textit{diag}[\mathbf{x}]$ indicates a diagonal $\text{N} \times \text{N}$ matrix whose diagonal contains the N elements of vector $\mathbf{x}$.}

\rev{From here, we can easily derive the local gradients on the recurrent weights $\mathbf{W}_{ix}$, $\mathbf{W}_{ih}$, and $\mathbf{W}_{ic}$ at time $t$:
\begin{align}
    \frac{\partial\mathbf{i}_t}{\partial\mathbf{W}_{ix}} &= 
    \frac{\partial\mathbf{i}_t}{\partial\mathbf{\bar i}_t} \otimes \mathbf{x}_t , \\
    \frac{\partial\mathbf{i}_t}{\partial\mathbf{W}_{ih}} &= 
    \frac{\partial\mathbf{i}_t}{\partial\mathbf{\bar i}_t} \otimes \mathbf{h}_{t-1} , \\
    \frac{\partial\mathbf{i}_t}{\partial\mathbf{W}_{ic}} &=
    \delta \mathbf{\hat i}_t \otimes \mathbf{c}_{t-1} ,
    \label{eq:di1}
\end{align}
where $\otimes$ denotes the outer product of two vectors, and:
\begin{align}
    \delta \mathbf{\hat i}_t &= \frac{\partial\mathbf{i}_t}{\partial\mathbf{\bar i}_t} \odot
    \left( \mathbf{1} - \text{tanh}^2 (\,\mathbf{W}_{ic} \mathbf{c}_{t-1} ) \, \right).
    \label{eq:di2}
\end{align}}
%
Now, let's consider what happens as $t$ grows: we observe that $\mathbf{x}_t$ and $\mathbf{h}_t$ are bounded to a limited interval. In particular, $\mathbf{x}_t$ is a sample of the input data, and $\mathbf{h}_t$ is bounded in the interval $[-1, 1]$ by construction. Instead, the cell $\mathbf{c}_t$ can grow linearly with the number of recursive steps, making its domain extremely task-dependent.
%
This is a well-known problem, which motivated the introduction of the forget gate in the original LSTM structure~\citep{gers2000forget}. Despite this, the range of possible values of $c_t$ cannot be restricted to a fixed domain.
%
The hyperbolic tangent non-linearity helps to avoid an excessive influence of the unbounded cell state in the gate mechanics, hence preventing unwanted saturation. As it can be seen in Eq.~\ref{eq:pv2i},~\ref{eq:pv2f}, and~\ref{eq:pv2o}, the term related to the cell state is bounded in the interval $[-1, 1]$. Additionally, it helps screen the connection weights $\mathbf{W}_{\star c}$ from unstable updates.

Even if $\mathbf{c}_t$ grew linearly with the number of time steps, its influence on the sigmoid argument would be mitigated, and it could not take the sigmoid function into its saturated regime against the other two terms driven by $\mathbf{x}$ and $\mathbf{h}$ respectively.
On the other hand, the growth of the cell state would push the hyperbolic tangent towards its own saturated regime. This behavior helps protect the weight matrix employed in the connection from unstable updates.

\smallbreak
\noindent\textbf{Peephole Connections and their Limitations.}
We now turn our attention to a related connection, namely the peephole connection~\citep{gers2000recurrent}, which is no longer common in the LSTM formulation.
%
Peephole connections were introduced by Gers and Schmidhuber in~\citep{gers2000recurrent}, and enrich the LSTM equations with recurrent weights $\mathbf{W}_{\star c}$, $\star \in \{\mathbf{i}, \mathbf{f}, \mathbf{o}\}$:
\begin{align}
    &\mathbf{i}_t = \sigma
                    (\mathbf{W}_{ix}\mathbf{x}_{t} +
                    \mathbf{W}_{ih} \mathbf{h}_{t-1} +
                    \mathbf{W}_{ic} \mathbf{c}_{t-1} +
                    \mathbf{b}_i)
                    \label{eq:ph_i} \\
    &\mathbf{f}_t = \sigma
                    (\mathbf{W}_{fx}\mathbf{x}_{t} +
                    \mathbf{W}_{fh} \mathbf{h}_{t-1} +
                    \mathbf{W}_{fc} \mathbf{c}_{t-1} +
                    \mathbf{b}_f) \\
    &\mathbf{o}_t = \sigma
                    (\mathbf{W}_{ox}\mathbf{x}_{t} +
                    \mathbf{W}_{oh} \mathbf{h}_{t-1} +
                    \mathbf{W}_{oc} \mathbf{c}_{t} +
                    \mathbf{b}_o),
\end{align}
with $\mathbf{W}_{\star c}$ generally constrained to be diagonal~\citep{graves2013generating, Greff2017LSTMAS}. While this formulation allows for a more precise control of the gates, there are two issues that limit its effectiveness.
\rev{In this case, the local gradient at time $t$ is expressed by:
\begin{align}
    \frac{\partial\mathbf{i}_t}{\partial\mathbf{\bar i}_t} &=
    %\sigma^\prime \left(\, \mathbf{\bar i}_t \right) = 
    \frac{\partial}{\partial\mathbf{\bar i}_t} \sigma \left(\, \mathbf{\bar i}_t \right) =
                          \textit{diag}\left[
                          \sigma\left(\, \mathbf{\bar i}_t \right) \odot
                          \left( \mathbf{1} - \sigma\left(\, \mathbf{\bar i}_t \right) \right)
                          \right] ,
\end{align}
}
with $\mathbf{\bar i}_t$ being the argument of the sigmoid function in Eq~\ref{eq:ph_i}:
\begin{align}
    \mathbf{\bar i}_t &= \mathbf{W}_{ix}\mathbf{x}_{t} +
                                     \mathbf{W}_{ih} \mathbf{h}_{t-1} +
                                     \mathbf{W}_{ic} \mathbf{c}_{t-1} +
                                     \mathbf{b}_i .
\end{align}
\rev{In light of this difference, Eq.~\ref{eq:di1} and~\ref{eq:di2} become:
\begin{align}
    \frac{\partial\mathbf{i}_t}{\partial\mathbf{W}_{ic}} &= 
    \frac{\partial\mathbf{i}_t}{\partial\mathbf{\bar i}_t} \otimes \mathbf{c}_{t-1}  .
    \label{eq:deltaw}
\end{align}
}
\begin{figure*}[!t]
    \centering
    \includegraphics[height=4.2cm]{figures/c_t.pdf}
    \caption{The cell state $c_t$ may grow linearly with the number of time steps. Peephole connections directly expose $c_t$, creating a key issue (b). Data for this plot is taken from the first training iterations of the sequential MNIST (see \textsection~\ref{sec:smnist}).}
\label{fig:1b}
% \vspace{-0.15cm}
\end{figure*}
%

\rev{We observe that, both in Eq.~\ref{eq:pv2i} and in Eq~\ref{eq:ph_i}, the magnitude of the product $\mathbf{W}_{ic} \mathbf{c}_{t-1}$ can in principle grow unbounded.
The activation function introduced in WMCs squashes this term into a closed bounded interval. In peephole connections, hovewer, this term is added inside the gate without an adequate protection (see Fig.~\ref{fig:1a}).
%
The result is that, in the peephole formulation, the sigmoid function applied immediately after could be pushed towards its saturating regime independently from the value of $\mathbf{x}_t$ and $\mathbf{h}_t$.
In theory, the LSTM block can recover from this situation by setting all the weights in the peephole connection to $0$, but in practice this might not happen if the sigmoid gate is saturated most of the time. Even if the two other summands can compensate for the growth of $\mathbf{W}_{\star c}\mathbf{c}$, hence letting gradients flow through the gate, there is still a key issue that hinders learning. In fact, as shown in Eq. \ref{eq:deltaw}, the gradients on the recurrent peephole weights grow linearly with $\mathbf{c}$, making updates unstable.}

To exemplify this behavior, we report the Euclidean norm of $\mathbf{c}_t$ during the early training stages in Fig.~\ref{fig:1b}. After a small number of time steps, the content of the cell floods the gates of the peephole LSTM. A possible consequence would be that both the input and the forget gates would saturate towards $1$. In our example, this aspect leads to an additional and uncontrolled growth of the magnitude of $\mathbf{c}_t$. 
As it can be seen, Working Memory Connections exhibit a much more regular behavior than peepholes and can prevent the uncontrolled growth of the memory cell.
