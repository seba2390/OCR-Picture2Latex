%
\begin{figure*}[t!]
\setlength{\tabcolsep}{.3em}
\centering
\renewcommand{\arraystretch}{0.5}
\begin{tabular}{cc}
    \scriptsize\textbf{Adding Problem -- T=200} &  \scriptsize\textbf{Adding Problem -- T=400}\\
    \includegraphics[height=0.315\linewidth]{figures/add200.pdf}
     &  
     \includegraphics[height=0.315\linewidth]{figures/add400.pdf}
     \\
    \scriptsize\textbf{Copying Task -- T=100} & \scriptsize\textbf{Copying Task -- T=200}\\
    \includegraphics[height=0.315\linewidth]{figures/copy100.pdf} 
    &
   \includegraphics[height=0.315\linewidth]{figures/copy200.pdf}
    \\
    \scriptsize\textbf{Sequential MNIST} & \scriptsize\textbf{Permuted MNIST} \\
    \includegraphics[height=0.315\linewidth]{figures/smnist.pdf} 
     &  
    \includegraphics[height=0.315\linewidth]{figures/pmnist.pdf}
\end{tabular}
\caption{Comparison among traditional LSTM, the proposed LSTM with working memory connections, and peephole LSTM. We investigate three different tasks: the adding problem (top), the copying task (center), and the sequential/permuted MNIST (bottom). In all the plots, shading indicates the standard error of the mean.}
% \vspace{-0.1cm}
\label{fig:plots}
\end{figure*}

The effectiveness of Working Memory Connections and their general benefits can be appreciated in many different tasks. The proposed experiments cover a wide area of applications: two different toy problems, digit recognition, language modeling, and image captioning. While the analysis on simple tasks helps to clarify the inherent advantages of the proposed approach, results on more challenging real-world applications motivate a wider adoption of our novel connections, especially for long sequences.
We compare our model (LSTM-WC) to a traditional LSTM and to an LSTM with peephole connections (LSTM-PH).

\subsection{Adding Problem and Copying Tasks}
In the adding problem~\citep{hochreiter1997long}, the input to the network consists of a series of $T$ pairs $(n_t, f_t)$, with $0\le t<T$. The first element $n_t$ is a real-valued number between 0 and 1, and $f_t$ is a corresponding marker. In the entire sequence, only two markers $f_i$ and $f_j$ are set to 1, while the others are set to 0. The goal is to predict the sum of the corresponding real-valued items $n_i + n_j$, for which $f = 1$. In our experiments, we test with $T=200$ and $T=400$, and we measure the performance using mean squared error.
%
%implementation detail
For this experiment, the networks have hidden size $N=128$ and train for $200$ epochs. We optimize the parameters using SGD with Nesterov update rule. The learning rate is $10^{-2}$ (momentum factor $0.9$) and the batch size is $128$. We also clip the gradient norm to $1.0$.
%
Results are reported in Fig.~\ref{fig:plots} (top), where we plot the MSE on the test set for every epoch of training.
LSTM-WM achieves the best convergence time for $T=200$, while the final performance on this setup is similar among the three models. The effectiveness of WMCs is striking in the $T=400$ setup. In fact, the proposed model solves the adding problem around epoch $145$, while the other two architectures cannot learn the task and are stuck on the trivial solution.

In the copying task~\citep{hochreiter1997long}, the network observes a sequence of $10$ input symbols, waits for $T$ time steps (we use $T=100$ and $T=200$), and then must reproduce the same sequence as output. For this experiment, we adopt the same setup described in~\citep{arjovsky2016unitary}.
We keep the same implementation details described for the adding problem, except that we train for $500$ epochs.
In Fig.~\ref{fig:plots} (center), we plot the test accuracy achieved by the three models at each epoch. In both setups, WMCs play an important role in terms of final performance and convergence time. As in the adding problem, the performance gain given by the proposed architecture is more evident when working on longer sequences: for $T=200$, WMCs outperform peephole LSTM and vanilla LSTM by around $+25\%$ and $+40\%$.

% For these experiments, the networks have hidden size $N=128$ and train for $200$ (adding problem) and $500$ epochs (copying task). For both of the tasks, we optimize the parameters using SGD with Nesterov update rule. The learning rate is $10^{-2}$ (momentum factor $0.9$) and the batch size is $128$. We also clip the gradient norm to $1.0$.

\subsection{Permuted Sequential MNIST}
\label{sec:smnist}
The sequential MNIST (sMNIST)~\citep{le2015simple} is the sequential version of the MNIST digit recognition task~\citep{lecun1998gradient}. In this task, the image pixels are fed sequentially to the network (from left to right, and top to bottom). The permuted sequential MNIST (pMNIST) is a sequential version of the MNIST digit recognition problem in which the pixels are permuted in a random but fixed order. In both tasks, the goal is to predict the correct digit label after the last input pixel. Following the setup proposed in~\citep{arpit2018h}, we use $50$k images for training, $10$k for validation, and $10$k to test our models.
%
The experimental setup is as follows. We set the hidden size to $N=128$ for all the networks, and train for $200$ epochs using SGD with learning rate $10^{-2}$ and batch size $128$ (momentum $0.9$ and Nesterov update rule). We clip the gradient norms to $1.0$.

Fig.~\ref{fig:plots} (bottom) reports the mean test accuracy of the three LSTM variants for both setups. We report the standard error of the mean as a shaded area. For the sMNIST task, peephole LSTM performs slightly better than vanilla LSTM. LSTM with Working Memory Connections, instead, outperforms the competing architectures in terms of final accuracy and convergence speed. In particular, our architecture employs only 50 epochs to get above $92\%$ accuracy, while other models are still generally stuck around $65\%$ (vanilla LSTM) and $82\%$ (LSTM-PH). In this experiment, we also find out that WMCs help stabilize training. In fact, the area given by the standard error of the mean is much thicker for our approach than for the other two variants, in particular during the early stages of training.
On the pMNIST task, all the models achieve good final results, with LSTM with Working Memory Connections still being the best option.

\input{tables/smnist}

Numerical results, reported in Table~\ref{table:smnist}, confirm that our model outperforms the classic LSTM by a discrete margin ($+0.47\%$ and $+1.03\%$ on the sequential and permuted MNIST respectively). Since WMCs introduce additional learnable parameters in the LSTM structure, we also compare with vanilla and peephole LSTM with increased hidden size (256 instead of 128). Note that, in this setting, LSTM and LSTM-PH have more than $2\times$ the number of learnable parameters of LSTM-WM. Despite this, LSTM-WC achieves the best results on both tasks. It is worth noting that, while additional parameters in vanilla LSTM improves the results on pMNIST, they are not helpful in the sMNIST task. The flexibility given by WMCs, instead, allows the proposed model to achieve the best result in both setups.
%
Always in Table~\ref{table:smnist}, we compare with two state-of-the-art RNNs~\citep{le2015simple,arjovsky2016unitary}, and with a training algorithm for LSTM~\citep{arpit2018h}. The proposed LSTM-WC outperforms the competitors in terms of test accuracy.

\input{tables/ptb}

\subsection{Penn Treebank (PTB) Character-Level Language Modeling}
Character-level language modeling requires to predict a single character at each time step given an observed sequence of text. In our experiments on the Penn Treebank (PTB) dataset~\citep{marcus1993building}, we evaluate the performance of the three different LSTM variants in terms of test mean bits per character (BPC), where lower BPC denotes better performance. We report the results in Table~\ref{tab:ptb_lm}, where we compare truncated back-propagation through time ($T_{PTB}$) over 150 and 300 steps. Since our connection introduces new learnable weights, we consider an additional setup in which we keep a fixed number of parameters for the three networks.
For this experiment, we follow the setup proposed by~\citet{merityAnalysis}, with the only exception that we employ a single LSTM layer instead of three.
%
%LSTM with Working Memory Connections (LSTM-WC) performs considerably better than the competitors.
The advantage of using Working Memory Connections is more evident for equal number of hidden units, where the proposed architecture overcomes the vanilla LSTM and peephole LSTM by a significant margin. Even when the number of parameters is fixed for all the models, LSTM-WC outperforms the competitors by $0.035$ and $0.041$ BPC for $T_{PTB} = 150$ and $T_{PTB}=300$ respectively. It is worth noting that peephole LSTM performs similarly to or even worse than vanilla LSTM on this task.

\subsection{Image Captioning}
We evaluate the performance of our LSTM with Working Memory Connections on the image captioning task, which consists of generating textual descriptions for images. We apply our approach to two different captioning models: Show and Tell~\citep{vinyals2015show} and Up-Down~\citep{anderson2018bottom}. The first model includes a single LSTM layer and does not employ attention, while the second is composed of two LSTM layers and integrates attention mechanisms over image regions. We use the Microsoft COCO dataset~\citep{lin2014microsoft} following the splits defined in~\citep{karpathy2015deep}. To represent images, we employ a global feature vector extracted from the average pooling layer of ResNet-152~\citep{he2016deep} for the Show and Tell model, and multiple feature vectors extracted from Faster R-CNN~\citep{ren2015faster} for the Up-Down architecture. We train both models with Adam optimizer~\citep{kingma2015adam} using a learning rate equal to $10^{-4}$. All other hyper-parameters are left the same as those suggested in the original papers.

\input{tables/captioning}

\begin{figure}[!t]
\centering
\setlength{\tabcolsep}{.4em}
\begin{tabular}{cc}
\multicolumn{2}{c}{\scriptsize \textbf{No Attention, ResNet-152}} \\
    \includegraphics[height=0.35\linewidth]{figures/show_and_tell_gap_meteor.pdf} 
    &
    \includegraphics[height=0.35\linewidth]{figures/show_and_tell_gap_cider.pdf} 
    \\
\multicolumn{2}{c}{\scriptsize\textbf{Attention, Faster R-CNN}} \\
    \includegraphics[height=0.35\linewidth]{figures/up_down_gap_meteor.pdf}
     &  
    \includegraphics[height=0.35\linewidth]{figures/up_down_gap_cider.pdf}
\end{tabular}
\caption{Metric gaps on the image captioning task for increasing instruction lengths.}
\label{fig:captioning}
% \vspace{-0.2cm}
\end{figure}

Numerical results are reported in Table~\ref{tab:captioning} using standard captioning evaluation metrics (\ie~BLEU-1, BLEU-4~\citep{papineni2002bleu}, METEOR~\citep{banerjee2005meteor}, ROUGE~\citep{lin2004rouge}, CIDEr~\citep{vedantam2015cider}, and SPICE~\citep{anderson2016spice}). For all these, higher metric results indicate better performance, with CIDEr being the metric that best correlates with human judgment.
In both settings, our LSTM-WM outperforms traditional LSTM and LSTM-PH by a clear margin. Specifically, LSTM-WM improves the vanilla LSTM results by $2.0$ CIDEr points on the model without attention and $0.8$ CIDEr points on the model with attention over image regions, demonstrating the contribution of WMCs also for this task.
As an additional comparison, we replace the LSTM layers with GRU layers. Numerical results suggest that there is not a clear advantage in using GRUs instead of LSTMs for this task. 
In Fig.~\ref{fig:captioning}, we plot the metric gap between LSTM-WM and the two competitors in terms of METEOR and CIDEr. On the X-axis we report the length of the generated captions, meaning that we consider the first $x$ words of each predicted sentence. On the Y-axis, a $0$ value means that our proposal performs equally, \ie~has no performance gap \textit{w.r.t.} the competitor, while a higher value indicates better performance for our model. With this analysis, we aim to check whether the improvement given by WMCs can be restricted to a particular subset of the dataset. As one can observe, the metric gap generally increases with the caption length, especially \textit{w.r.t.} peephole LSTM. We can deduce that the contribution of WMCs escalates with the number of time steps.
