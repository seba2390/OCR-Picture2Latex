Recurrent Neural Networks (RNNs)~\citep{Elman1990FindingSI,Rumelhart1986LearningRB} are a family of architectures that process sequential data by means of internal hidden states. The set of parameters of the network is shared across time steps, allowing the RNN to process inputs of variable length. 
As RNNs suffer from the so-called exploding and vanishing gradient problem (EVGP)~\citep{bengio1993problem,hochreiter1991untersuchungen}, which hinders the learning of long-term dependencies~\citep{bengio1994learning, pascanu2013difficulty}, previous works have proposed to enrich the recurrent cell with gating mechanisms~\citep{hochreiter1997long,jing2019gated}.
For instance, Long Short-Term Memory networks (LSTMs)~\citep{hochreiter1997long} use gates to control the information flow towards and from the memory cell and to regulate the forgetting process~\citep{gers2000forget}.
LSTMs are adopted in a wide number of tasks, such as neural machine translation~\citep{bahdanau2015neural,sutskever2014sequence}, speech recognition~\citep{graves2013speech}, and also vision-and-language applications like image and video captioning~\citep{vinyals2015show,xu2015show,baraldi2017hierarchical}. 

In this paper, we propose a novel cell-to-gate connection that modifies the classic LSTM block. Our formulation is general and improves LSTM overall performance and training stability without any particular assumption on the underlying task.
In the \textit{vanilla} LSTM formulation, the gates are controlled by the current input of the block and its previous output, which acts as the hidden state for the network. The long-term memory cell, instead, is employed to store information during the forward pass and provides a safe path for back-propagating the error signal. 
%
We argue that the content stored in the memory cell could be useful to regulate the gating mechanisms, too.
\rev{The key element of our design is a connection between the memory cell and the gates with a protection mechanism that prevents the cell state from being exposed directly. We draw inspiration from the gated \textit{read} operation employed to reveal the cell content at the block output, and enrich it with a learnable projection.} In this way, the LSTM block can use the knowledge in the cell (acting as a long-term memory) to control the evolution of the whole network in the short-term.

A similar concept in cognitive psychology and neuroscience is the so-called \textit{working memory}~\citep{ericsson1995long}, a type of memory employed, for instance, \textit{to retain the partial results while solving an arithmetic problem without paper, or to combine the premises in a lengthy rhetorical argument}~\citep{hernandez2018neuroethics}. Although definitions are not unanimous, working memory is said to be a cognitive system acting as a third type of memory between long-term and short-term memory. Our connections share this characteristic with working memory. For this reason, we call them \textit{Working Memory Connections} (WMCs).

A first attempt to fuse the information of the cell in the gates was made with the design of \textit{peepholes}~\citep{gers2000recurrent}: direct multiplicative connections between the memory cell and the gates. This approach has not been largely adopted in literature, as recent studies report mixed results~\citep{Greff2017LSTMAS} and discourage their use. Since our idea recalls the rationale of peephole connections, we provide a large comparison with this previous work. By doing so, we point out the major issues in the peephole formulation that hinder effective learning and attest that WMCs do not suffer from the same problems.
In our experiments, we show that an LSTM equipped with Working Memory Connections achieves better results than comparable architectures, thus reflecting the theoretical advantages of their design. In particular, WMCs surpass vanilla LSTM and peephole LSTM in terms of final performances, stability during training, and convergence time. All these aspects testify the advantage in letting the cell state participate in the gating dynamics. In order to support our conclusions, we conduct a thorough experimental analysis covering a wide area of current research topics.

\rev{To sum up, our contribution is mainly three-fold. First, we present a modification of LSTM in which traditional gates are enriched with Working Memory Connections, linking the memory cell with the gates through a protection mechanism. Then, we demonstrate that exposing the LSTM internal state directly and without a proper protection yields unstable training dynamics that compromise the final performance.} Finally, we show the effectiveness of the proposed solution in a variety of tasks, ranging from toy problems with very long-term dependencies (adding problem, copy task, and sequential MNIST) to language modeling and image captioning.
