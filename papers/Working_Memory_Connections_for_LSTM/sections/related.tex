Long Short-Term Memory networks~\citep{hochreiter1997long} aim to mitigate the exploding and vanishing gradient problem~\citep{hochreiter1991untersuchungen, bengio1994learning} with the use of gating mechanisms. Since its introduction, LSTM has gained a lot of attention for its flexibility and efficacy in many different tasks.
%
To simplify the LSTM structure,~\citet{liu2020simplified} propose to exploit the content of the long-term memory cell in a recurrent block with only two gates. However, this model neglects the importance of the LSTM output. While this might be useful for simple tasks, it is unlikely to generalize to more complex settings.~\citet{arpit2018h} propose to modify the path of the gradients in order to stabilize training with a stochastic algorithm specific to LSTM optimization. This direction of work is not in contrast with our goal, and could possibly be integrated with our proposal since our connection does not require a specific setup to be optimized.
%
Among the LSTM variants, the Gated Recurrent Unit (GRU)~\citep{cho2014LearningPR,cho2014properties} is the most popular and common architecture~\citep{chung2014empirical}, and features a coupling mechanism between input and forget gates~\citep{Greff2017LSTMAS}.
%
A recent line of research aims to tailor the LSTM structure for specific tasks. For instance,~\citet{baraldi2017hierarchical} propose a hierarchical model for video captioning, while other works incorporate convolutional models into the LSTM structure~\citep{XIAO2020173, LI201841}.
While these works propose a modification of the LSTM towards a specific goal, we propose a general and powerful idea that adapts to a large set of different tasks.

Recently, models based on self-attention, such as Transformer architecture~\citep{vaswani2017attention} and its variants, are achieving state-of-art performances on many different tasks, and also for sequence modeling. For instance, language representations based on BERT~\citep{devlin2018bert} can be finetuned with an additional output layer to obtain state-of-art results on many language-based tasks. However, RNNs require much fewer parameters and operations to run than Transformer-based architectures and are still widely adopted.
Moreover, LSTMs still have a large market in embedded systems and edge devices for their low computational and memory requirements.