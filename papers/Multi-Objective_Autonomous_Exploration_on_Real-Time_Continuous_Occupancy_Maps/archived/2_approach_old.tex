\section{Technical Approach}\label{sec:2}
\lipsum[1]

Our exploration system consists of two sub-modules: mapping module and planning module. We take advantage of the state-of-the-art continuous occupancy mapping---Bayesian Hilbert Mapping\cite{senanayake2017bayesian} as the mapping part, and our proposed Pareto Monte Carlo Tree Search\cite{chen2019pareto} as the planning part. We utilize the multi-objective Pareto Monte Carlo tree search to make our system efficiently explore the environment through combining both probability information and frontier dynamics. In each time step, occupied and free training points are sampled from incoming laser measurements. Then the collected training data will be used to train our occupancy prediction model and a local occupancy map with any arbitrary resolution could be obtained. From this occupancy map, we can further compute an entropy map that assigns low values to places that have been explored and high values to places that remain unexplored and a frontier dynamics map which reflects the dynamic changes on occupancy map. With the occupancy map and the two reward maps, our Pareto Monte Carlo tree search will return us a trajectory which has the highest comprehensive accumulative reward.

\subsection{Real-time Bayesian Hilbert Mapping}\label{subsec:2.1}


Bayesian Hilbert Map is an extension of Hilbert Map~\cite{ramos2016hilbert}, which represents the environment with a continuous occupancy map by using a logistic regression classifier in a Hilbert Space. In Bayesian Hilbert Mapping, the occupancy prediction model is defined using a sigmoid function:

\begin{equation}
    \label{eq:logi}
    P(y=1|\mathbf{x}, \mathbf{w}) = \left ( 1 + \exp(\mathbf{w}^T\Phi(\mathbf{x})) \right )^{-1} = \sigma(-\mathbf{w}^T\Phi(\mathbf{x})),
\end{equation}
where $y=1$ means the probability in Eq.~\ref{eq:logi} is an occupancy probability; $\mathbf{x}$ is the query point, which is represented as the coordinate of the point; $\mathbf{w}$ is the model parameters which need to be optimized; $\Phi(\cdot)$ is a new feature vector which is transformed from the original input vector $\mathbf{x}$.

% defined as:

% \begin{equation}
%     \label{eq:feature}
%     \Phi(\mathbf{x}) = \left ( 1, k(\mathbf{x},\tilde{\mathbf{x}}_1), k(\mathbf{x}, \tilde{\mathbf{x}}_2), \cdots, k(\mathbf{x}, \tilde{\mathbf{x}}_D) \right )
% \end{equation}

% where $k(\mathbf{x},\tilde{\mathbf{x}}_i)$ is the hinged features obtained by computing a RBF kernel on the query point and the $i^{th}$ hinge point:

% \begin{equation}
%     \label{eq:rbf}
%     k(\mathbf{x}, \tilde{\mathbf{x}}_i) = \exp(-\gamma\left \| \mathbf{x} - \tilde{\mathbf{x}}_i \right \|^2)
% \end{equation}

% where $\gamma$ is the bandwidth which control the `spread' of the kernel. A large $\gamma$ indicates a flat shape of kernel while a small $\gamma$ leads to a sharp kernel shape. The hinge point $\tilde{\mathbf{x}}_i$s are manually predefined in the space where to be mapped. The $D$ in Eq.~\ref{eq:feature} is the number of the predefined hinge points. In \cite{senanayake2017bayesian}, a full-Bayesian approach was proposed to learn the parameter vector $\mathbf{w}$. 

However, since the likelihood function is a sigmoid function, no analytical solution for the posterior distribution $P(\mathbf{w}|\mathbf{x}, \mathbf{y})$ could be obtained. Therefore, the posterior is approximated by another distribution $Q$:
\begin{equation}
    \label{eq:posterior}
    Q(\mathbf{w}, \alpha) \approx P(\mathbf{w}|\mathbf{x}, \mathbf{y}) = \frac{P(\mathbf{y}|\mathbf{x}, \mathbf{w})P(\mathbf{w|\alpha})P(\alpha)}{P(\mathbf{y})},
\end{equation}
where $\alpha$ is an additional introduced parameter due to the approximation. A variational inference method was used to learn the parameters. More details could be seen in \cite{senanayake2017bayesian}.

% \begin{figure}[htbp]
%     \centering
%     \includegraphics[width=0.618\linewidth]{figs/ch.png}
%     \caption{\small
%     Convex-hull-based hinge points selection strategy.
%     \textit{Left:} Orange triangles are predefined hinge points in global scale while blue points are local training data.
%     A convex hull is formed based on the training data;
%     \textit{Right:} The hinge points which fall into the convex hull are selected as the hinge points to predict the local occupancy probability.
%     }
%     \label{fig:ch_strategy}
% \end{figure}

To make Bayesian Hilbert Mapping more applicable for real-time scenarios, we propose a convex-hull-based hinge points selection strategy. Before the exploration task starts, a set of hinge points are defined in the whole area which is to be explored. For each time step, a convex hull is formed on top of the training points. This newly obtained convex hull will select the hinge points to generate the new feature vector for predicting the local occupancy probability. The hinge points which are not within the area covered by the training data will be discarded for current local prediction. By only including those important hinge points in our training and prediction, the computation time will be saved. As the robot is moving forward, the predicted value will be saved in a global map database in a way that no computation is needed for places where are already explored.

\subsection{Pareto Monte Carlo Tree Search} \label{subsec:2.2}
% Intuitive introduction to ParetoMCTS
Monte Carlo tree search (MCTS) methods are best-first search algorithms which expands the most promising subtrees first according to a specific rule.
The most widely used variant applied the Upper Confidence bound to Trees (UCT) which balances exploitation of the recently discovered most promising child node and exploration of alternatives which may turn out to be a superior choice at later time.
Pareto Monte Carlo tree search (ParetoMCTS) is a multi-objective extension of MCTS, seeking optimal decisions that any objective cannot be further improved without hurting other objectives --- Pareto optimality.  

% Pareto MCTS can be broken down into four steps
In each searching iteration, ParetoMCTS can be broken down into four main steps:
\begin{enumerate}
    \item \textit{Selection}: starting at the root node, ParetoUCB node selection policy is applied recursively to descend through the tree until an expandable node with unvisited (unexpanded) children is encountered.
    \item \textit{Expansion}: an action is chosen from the aforementioned expandable node.  A child node is constructed according to this action and connected to the expandable node.
    \item \textit{Simulation}: a simulation is run from the new node based on the predefined default policy. In our case, the default policy is moving forward.
    The average reward vector of this rollout trajectory is then returned.
    \item \textit{Back-propagation}: the obtained reward is backed up or back-propagated to each visited node in the selection and expansion steps to update the statistics in these nodes.
    Each node stores the number of times it has been visited and the cumulative average reward obtained by simulations starting from this node.
\end{enumerate}
These steps are repeated until the maximum number of iterations is reached or a given time limit is exceeded.
We refer the interested reader to \cite{chen2019pareto} for more details.

% How do we adapt ParetoMCTS to this work?
In \cite{chen2019pareto}, ParetoMCTS was applied to environmental monitoring application where obstacle avoidance was not taken into account.
In our autonomous exploration task, however, the goal of the robot is not only searching informative path but also avoid collision.
To this end, we modified the selection and simulation steps so that only collision-free actions are considered.

% Inputs and outputs
% \WZ{Entropy reward map}

% \WZ{How do we define frontier dynamics and treat it as an objective for ParetoMCTS?}

The inputs of ParetoMCTS include the current robot pose, occupancy map, entropy map and frontier dynamics map. In this paper, we assume the localization could be perfectly accessed at any time. Since we can have an occupancy map with a specific resolution from BHM, we can compute a respective entropy map with the specified resolution from:
\begin{equation}
    \label{eq:entropy}
    H_p(\mathbf{m}_i) = -p_i\log(p_i) - (1-p_i)log(1-p_i),
\end{equation}
where $\mathbf{m}_i$ is the $i^th$ query point while $p_i$ is the predicted occupancy probability at $\mathbf{m}_i$. The frontier dynamics map is computed by extracting the difference between two consecutive occupancy maps.

The output of ParetoMCTS is the most informative trajectory (red line in Fig.\,\ref{fig:pareto_mcts}) for exploration given the current perception information.
One can simply follow this trajectory and plan again when reaching the end.
However, considering that the perception information is continuously updating, we only take the first action (blue line in Fig.\,\ref{fig:pareto_mcts}) in the resulting trajectory and replan again in order to avoid newly encountered obstacles and keep the informative trajectory up to date.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1\linewidth]{figs/puct.png}
    \caption{Demonstration of searching an informative trajectory in a confined environment given two exploration reward maps.
    The green dots represent the resulting search tree.
    Red line shows the best trajectory and blue line is the best next action.
    Reward map 1 indicates that the upper-right corner has higher exploration reward while the second one suggests that the upper part is worth visiting.
    The tree expansion is first biased towards to the high reward area, and then it start scattering in order not to compromise any objective.
    }
    \label{fig:pareto_mcts}
\end{figure}

% Encountered problems when applying ParetoMCTS to this problem and how are we gonna solve them

