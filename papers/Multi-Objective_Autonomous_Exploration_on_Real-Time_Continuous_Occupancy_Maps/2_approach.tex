
%\vspace{-15pt}
\section{Technical Approach}\label{sec:approach} %\vspace{-10pt}
%=== Bayesian Hilbert Map ===
Bayesian Hilbert Map~\cite{senanayake2017bayesian} is an extension of Hilbert Map~\cite{ramos2016hilbert}, which represents the environment with a \textit{continuous occupancy map} by using a logistic regression classifier in a Hilbert space.
The probability of a query location $\mathbf{x}$ being occupied, i.e., $y=1$, can be computed by
% The occupancy prediction model in BHM is defined using a sigmoid function:
\begin{equation}
    \label{eq:logi}
    p(y=1|\mathbf{x}, \mathbf{w}) = \left ( 1 + \exp(\mathbf{w}^T\Phi(\mathbf{x})) \right )^{-1} = \sigma(-\mathbf{w}^T\Phi(\mathbf{x})),
\end{equation}
where $\mathbf{w}$ is the model parameters to be optimized, $\Phi(\mathbf{x})$ is a radial basis function (RBF) feature transformation of $\mathbf{x}$ centered at some spatially fixed points called hinge points, and $\sigma(\cdot)$ represents the logit function.
% where $y=1$ means the probability in Eq.~\eqref{eq:logi} is an {\em occupancy} probability; $\mathbf{x}$ is the query point, which is represented as the coordinate of the point that we want to know the occupancy; $\mathbf{w}$ is the model parameters which need to be optimized during training; $\Phi(\cdot)$ is a new feature vector which is transformed from the original input vector $\mathbf{x}$.
%
As a Bayesian approach, BHM places a factorized Gaussian prior $p(\mathbf{w}|\mathbf{\alpha})=\mathcal{N}\left(\mathbf{w}|\mathbf{0},\operatorname{diag}(\mathbf{\alpha})\right)$ on the model parameter and infers its posterior.
Analytical posterior is intractable due to the nonconjugacy between the Gaussian prior and Bernoulli likelihood.
% However, since the likelihood function is a sigmoid function, no analytical solution for the posterior distribution $P(\mathbf{w}|\mathbf{x}, \mathbf{y})$ could be obtained.
Therefore, the posterior $p(\mathbf{w}, \mathbf{\alpha}|\mathbf{x}, \mathbf{y})$ is approximated by a variational distribution $q(\mathbf{w}, \mathbf{\alpha})$, and we implicitly minimize the Kullback-Leibler (KL) divergence between $q$ and $p$ so that the approximate distribution is close to the true posterior~\cite{senanayake2017bayesian}.
% \begin{equation}
%     \label{eq:posterior}
%     Q(\mathbf{w}, \alpha) \approx P(\mathbf{w}|\mathbf{x}, \mathbf{y}) = \frac{P(\mathbf{y}|\mathbf{x}, \mathbf{w})P(\mathbf{w|\alpha})P(\alpha)}{P(\mathbf{y})},
% \end{equation}
% where $\alpha$ is an additionally introduced parameter due to the approximation. A variational inference method was used to learn the parameters.
% More details can be found in .

%=== Monte-Carlo tree search part ===
Monte Carlo tree search (MCTS) methods belong to best-first search algorithms which expand the most promising subtrees first.
We have been developing the Pareto Monte Carlo tree search (ParetoMCTS)~\cite{chen2019pareto}, which is a multi-objective extension of MCTS that seeks for the {\em Pareto optimal decisions} where any objective cannot be further improved without hurting (or compromising) other objectives. % --- Pareto optimality.
Starting from the root node, we select the child node according to Pareto Upper Confidence Bound (UCB) criterion at each level until an expandable node with unexpanded children is encountered.
A random child node is expanded and its value is evaluated through forward simulation.
We update the statistics in all the visited nodes using the estimated value to bias the searching process in the next round.
\begin{figure}[htbp] %\vspace{-10pt}
    \centering
    \includegraphics[width=1\linewidth]{figs/puct.png}
    \caption{Demonstration of searching an informative trajectory in a confined environment given two exploration reward maps.
    The green dots represent the resulting search tree.
    Red line shows the best trajectory and blue line is the best next action.
    Reward map 1 (middle figure with grey gradient color) indicates that the upper-right corner has higher exploration reward while the reward map 2  (right) suggests that the upper part is worth visiting.
    The tree expansion is first biased towards the high reward area, and then it starts scattering in order not to compromise any objective. \vspace{-10pt}
    }
    \label{fig:pareto_mcts}
\end{figure}
Fig.\,\ref{fig:pareto_mcts} shows the result of a ParetoMCTS when optimizing two exploration objectives in a confined environment.
ParetoMCTS grows an asymmetric tree towards the high-reward area and tries to optimize both objectives.
The resulting tree is collision-free and satisfies robot's motion constraints.
%For more details about ParetoMCTS, we refer the reader to \cite{chen2019pareto}.

In this paper, the inputs of ParetoMCTS include current robot pose, a occupancy map, a entropy map and a frontier dynamics map.
% we assume the localization could be perfectly accessed at any time. Since we can have an occupancy map with a specific resolution from BHM, we can compute a respective entropy map with the specified resolution from:
Given a grid $\mathbf{m}_i$ and its occupancy probability $P_i$, the entropy is given by
\begin{equation}
    \label{eq:entropy}
    H(\mathbf{m}_i) = -P_i\log(P_i) - (1-P_i)\log(1-P_i).
\end{equation}
% where $\mathbf{m}_i$ is the $i^{th}$ query point while $p_i$ is the predicted occupancy probability at $\mathbf{m}_i$.
We defined the frontier dynamics map to be the absolute difference between two consecutive occupancy maps.
% The frontier dynamics map is computed by extracting the difference between occupancy maps at two consecutive time steps.

ParetoMCTS outputs the most informative trajectory (red line in Fig.\,\ref{fig:pareto_mcts}) for exploration given the current perception information.
One can simply follow this trajectory and plan again when reaching the end.
However, considering that the perception information is continuously updating, we only take the first action (blue line in Fig.\,\ref{fig:pareto_mcts}) in the resulting trajectory and replan in a receding horizon manner to avoid newly encountered obstacles and keep the informative trajectory up to date.