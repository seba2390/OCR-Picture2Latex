

\section{Alternating Minimization with Randomization}

We first consider reducing the space complexity of the ADMM
approach. The idea is simple: we only maintain the TT format of the
input tensor rather than the input tensor itself.  This leads the
following optimization problem:
\begin{align}
    \min_{\mG} \left[ \frac{1}{2n} \|Y - \mathfrak{X}(X(\mG))\|^2 + \lambda_n \| X(\mG)\|_{s,T} \right]. \label{opt:als0}
\end{align}
Remember that $\mG=\{G_k\}_k$ is the set of TT components and $X(\mG)$
is the tensor given by the TT format with $\mG$. Now we only need to
store the TT components $\mG$, which drastically improves the space
efficiency.



\subsection{Randomized TT Schatten norm}

Next, we approximate the optimization of the Schatten TT norm. To avoid
the computation of exponentially large-scale SVDs in the Schatten TT norm,
we employ a technique called the ``very sparse random
projection''~\cite{li2006very}. The main idea is that, if the size of a
matrix is sufficiently larger than its rank, then its singular values
(and vectors) are well preserved even after the projection by a sparse
random matrix. This motivates us to use the Schatten TT norm over the
random projection.

Preliminary, we introduce tensors for the random projection. Let
$D_1,D_2 \in \mathbb{N}$ be the size of the matrix after projection.
For each $k = 1,\ldots,K-1$ and parameters, let
$\Pi_{k,1} \in \R^{D_1 \times I_1 \times \cdots \times I_{k}}$ be a
tensor whose elements are independently and identically distributed as follows:
\begin{align}
    [\Pi_{k,1}]_{d_1,i_1, \ldots,i_k} = 
    \begin{cases}
        +\sqrt{s/d_1} \quad & \mbox{~with probability~}1/2s,\\
        0 \quad & \mbox{~with probability~}1-1/s,\\
        -\sqrt{s/d_1} \quad & \mbox{~with probability~}1/2s,        \label{def:pi}
    \end{cases}
\end{align}
for $i_1,\ldots,i_k$ and $d_1 = 1,\ldots,D_1$.
Here, $s>0$ is a hyperparameter controlling sparsity.
Similarly, we introduce a tensor
$\Pi_{k,2} \in \R^{D_2 \times I_{k+1} \times \cdots \times I_{K-1}}$ that is defined in the same way as $\Pi_{k,1}$.
%
With $\Pi_{k,1}$ and $\Pi_{k,2}$, let
$\mP_k : \mX \to \R^{D_1 \times D_2}$ be a random projection operator
whose element is defined as follows:
\begin{align}
    [\mP_k(X)]_{d_1,d_2} 
  &= \sum_{j_1=1}^{I_1}  \cdots \sum_{j_{K}=1}^{I_{K}} [\Pi_{k,1}]_{d_1,j_1,\ldots,j_{k}}  [X]_{j_1,\ldots,j_K}[\Pi_{k,2}]_{d_2,j_{k+1},\ldots,j_{K}}. \label{eq:random1} 
%  &=  \sum_{(j_1, \ldots, j_k) \in \pi^{(k)}_{1}} \sum_{(j_{k+1}, \ldots, j_K) \in \pi^{(k)}_{2}} [\Pi_{k,1}]_{d_1,j_1,\ldots,j_{k}}  [X]_{j_1,\ldots,j_K}[\Pi_{k,2}]_{d_2,j_{k+1},\ldots,j_{K}}. \label{eq:random1} 
%\\
%  &= \sum_{j_1=1}^{I_1}  \cdots \sum_{j_{K}=1}^{I_{K}} [\Pi_{k,1}]_{d_1,j_1,\ldots,j_{k}} [G_1]_{j_1}  \cdots [G_K]_{j_K} [\Pi_{k,2}]_{d_2,j_{k+1},\ldots,j_{K}}. \label{eq:random2}
\end{align}
%In the second line we used the fact that $X$ has the TT format
%$X = X(\mG)$. 
Note that we can compute the above projection by using the facts that
$X$ has the TT format and the projection matrices are sparse.  Let
$\pi^{(k)}_{j}$ be a set of indexes of non-zero elements of
$\Pi_{k,j}$. Then, using the TT representation of $X$,
\eqref{eq:random1} is rewritten as
\begin{align*}
  [\mP_k(X(\mG))]_{d_1,d_2}  
  =& \sum_{(j_1, \ldots, j_k) \in \pi^{(k)}_{1}} [\Pi_{k,1}]_{d_1,j_1,\ldots,j_{k}} [G_1]_{j_1}  \cdots [G_k]_{j_k}
\\
  &\sum_{(j_{k+1}, \ldots, j_K) \in \pi^{(k)}_{2}} [G_k]_{j_{k+1}} \cdots [G_K]_{j_K} [\Pi_{k,2}]_{d_2,j_{k+1},\ldots,j_{K}},
\end{align*}
If the projection matrices have only $S$ nonzero elements (i.e.,
$S = |\pi^{(1)}_j|= |\pi^{(2)}_j|$), the computational cost of the
above equation is $O(D_1D_2SKR^3)$.


The next theorem guarantees that the Schatten-1 norm of $\mP_k(X)$
approximates the original one.
% 
\begin{theorem} \label{thm:random} Suppose $X\in\mX$ has TT rank
  $(R_1, \ldots,R_k)$.  Consider the reshaping operator $Q_k$ in
  \eqref{def:ttnorm}, and the random operator $\mP_k$ as
  \eqref{eq:random1} with tensors $\Pi_{k,1}$ and $\Pi_{k,2}$ defined
  as \eqref{def:pi}.  
  If $D_1, D_2 \ge \max\{ R_k, 4 (\log(6 R_k) + \log(1 / \epsilon))/\epsilon^2 \}$, and all the singular vectors $u$ of $Q(X)_k$ are well-spread as
  $ \sum_j |u_j|^3 \le \epsilon/(1.6 k \sqrt{s})$, we have
  \begin{align*}
      \frac{1-\epsilon}{R_k} \|Q_k(X)\|_{s} \leq \| \mP_k (X)\|_{s} \leq (1+\epsilon) \|Q_k(X)\|_{s},
  \end{align*}
  with probability at least $1 - \epsilon$.

\if0
    \textcolor{red}{ [OLD]
    Then, for any $k$ and $\epsilon > 0$, with probability at least [TBW] with a constant $c_{p} > 0$, the following relation holds
    \begin{align*}
        (1-\epsilon)^{1/2}\|Q_k(X)\|_{s} \leq \| \mP_k (X)\|_{s} \leq (1+\epsilon)^{1/2}\|Q_k(X)\|_{s},
    \end{align*}
    when $D_1 \geq R_k$ and $D_2 \geq R_k$ are satisfied.
    }
    \fi
\end{theorem}

Note that the well-spread condition can be seen as a stronger version of the incoherence assumption which will be discussed later.
%since $\sum_i |u_i|^3 \le C$ for all singular vector $u$ implies $\| P_U e_i \|_2 C^{1/4}$ for all $i$.



\subsection{Alternating Minimization}

%Preliminarily, we consider $X$ with TT decomposition as a function of $G_k$. 
%Namely, we fix $\mG \backslash \{G_k\}$ and define a tensor as
%\begin{align*}
%    X_k(\mG) := X(G_k ; \mG \backslash \{G_k\}).
%\end{align*}
%Then, the optimization problem \eqref{opt:als} is rewritten as
%\begin{align}
%    \min_{G_k} \left[ \frac{1}{2n}\|Y - \mathfrak{X}(X_k(\mG))\|^2 + \lambda_n \| X_k(\mG)\|_{s,T} \right], k = 1,\ldots, K. \label{opt:als}
%\end{align}
%By the setting of TT decomposition, $G_k$ and it is a $3$-way tensor with size $I_k \times R_{k-1} \times R_k$.
%Thus, we require only a small size of computational memory to handle $G_k$ as the control variable.

% $\mG \backslash \{G_k\} := \{G_{k'}\}_{k'\not=k}$

Note that the new problem \eqref{opt:als0} is non-convex because
$X(\mG)$ does not form a convex set on $\mX$.  However, if we fix
$\mG$ except for $G_k$, it becomes convex with respect to
$G_k$. Combining with the random projection, we obtain the following
minimization problem:
\begin{align}
    \min_{G_k} \left[ \frac{1}{2n} \|Y - \mathfrak{X} (X(\mG))\|^2 + \frac{\lambda_n}{K-1} \sum_{k'=1}^{K-1} \|\mP_{k'}(X (\mG))\|_{s} \right]. \label{opt:als2}
\end{align}
We solve this by the ADMM method for each $k = 1,\ldots,K$.  Let
$g_k \in \R^{I_kR_{k-1}R_k}$ be the vectorization of $G_k$, and
$W_{k'}\in \R^{D_1 \times D_2}$ be a matrix for the randomly projected matrix.
The augmented Lagrangian function is then given by
$L_k(g_k, \{W_{k'}\}_{k'=1}^{K-1}, \{\beta_{k'}\}_{k'=1}^{K-1})$,
%\begin{align*}
%    &L_k(g_k, \{W_{k'}\}_{k'=1}^{K-1}, \{\beta_{k'}\}_{k'=1}^{K-1}) \\
%    &= \frac{1}{2n} \|Y - \Omega g_k \|^2 + \lambda_n\sum_{k'=1}^{K-1}  \|Z_{k'}\|_{s^*}  + \sum_{k'=1}^{K-1} \left\{ \beta_{k'}^{T} (\Gamma_{k'} g_k - \mbox{vec}Z_{k'}) + \frac{1}{2} \|\Gamma_{k'} g_k - \mbox{vec}Z_{k'}\|^2 \right\},
%\end{align*}
where $\{\beta_{k'} \in \R^{D_1D_2} \}_{k'=1}^{K-1}$ are the Lagrange
multipliers.  Starting from initial points
$(g_k^{(0)}, \{W_{k'}^{(0)}\}_{k'=1}^{K-1},
\{\beta_{k'}^{(0)}\}_{k'=1}^{K-1})$,
the $\ell$-th ADMM step is written as follows:
\begin{align*}
    &g_k^{(\ell + 1)} = \left( \Omega^T \Omega / n + \eta \sum_{k'=1}^{K-1} \Gamma_{k'}^{T}\Gamma_{k'} \right)^{-1}  \left( \Omega^T Y / n + \frac{1}{K-1}\sum_{k'=1}^{K-1} \Gamma_{k'}^{T}( \eta \tilde{V}_k(W_{k'}^{(\ell)}) - \beta_{k'}^{(\ell)} )  \right) ,\\
    &W_{k'}^{(\ell + 1)} = \mbox{prox}_{\lambda_n / \eta} \left( \tilde{V}_k^{-1} ( \Gamma_{k'} g_k^{(\ell+1)} +\beta_{k'}^{(\ell)} ) \right),~~k'=1,\ldots,K-1, \\
    &  \beta_{k'}^{(\ell + 1)} = \beta_{k'}^{(\ell)} + (\Gamma_{k'} g_k^{(\ell + 1)} - \tilde{V}_k(W_{k'}^{(\ell + 1)})),~~k'=1,\ldots,K-1.
\end{align*}
Here, $\Gamma^{(k)} \in \R^{D_1 D_2 \times I_kR_{k-1}R_k}$ is the matrix
imitating the mapping
$G_k\mapsto\mP_k(X(G_k;\mG \backslash \{G_k\}))$, $\tilde{V}_k$ is a vectorizing operator of $D_1 \times D_2$
matrix, and $\Omega $ is an $n \times I_kR_{k-1}R_k$
matrix of the operator
$\mathfrak{X} \circ X( \cdot ; \mG \backslash \{G_k\})$ with respect
to $g_k$.
%
Similarly to the convex approach, we iterate the ADMM steps until
convergence. We refer to this algorithm as \emph{TT-RALS}, where RALS stands for randomized ALS.

%The time and space complexities are as follows. To update
%$g_k^{(\ell)}$, we need $O(I^3R^6)$ time for the inversion of the
%$IR^2 \times IR^2$ matrix. The SVD thresholding requires $O(D^3SR^3)$
%time.
%
%$O(nK^2IR^2)$

The time complexity of TT-RALS at the $\ell$-th iteration is
$O((n + KD^2)KI^2R^4)$; the details are deferred to
Supplementary material.
%
%, computing the parameters requires $O(K(n+KD^2)I^2R^4)$,
%inverting an $IR^2 \times IR^2$ matrix for updating $g_k^{(\ell)}$ for
%all $k$ requires $O(KI^3R^6)$, and conducting the random projection
%and thresholding its requires $O(D^3SKR^3)$ times.  
%
The space complexity is $O(n + KI^2R^4)$, where $O(n)$ is for $Y$ and
$O(KI^2R^4)$ is for the parameters.


%\knote{Describe time and space complexities of TT-RALS}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "TTcomp_NIPS2017.tex"
%%% End:
