\section{Related Work}

%We compare the TT-ADMM and TT-RALS with the existing tensor completion methods with TT-decomposition.
To solve the tensor completion problem with TT decomposition,
Wang \etal \cite{wang2016tensor} and Grasedyck \etal \cite{grasedyck2015alternating} developed
algorithms that iteratively solve minimization problems with respect
to $G_k$ for each $k = 1,\ldots,K$. Unfortunately, the adaptivity of
the TT rank is not well discussed.  \cite{wang2016tensor} assumed that
the TT rank is given. Grasedyck \etal \cite{grasedyck2015alternating} proposed a grid
search method. However, the TT rank is determined by a single
parameter (i.e., $R_1=\dots=R_{K-1}$) and the search method lacks its
generality.  Furthermore, the scalability problem remains in both
methods---they require more than $O(I^K)$ space.

Phien et al.(2016) \cite{phien2016efficient} proposed a convex
optimization method using the Schatten TT norm.  However, because they
employed an alternating-type optimization method, the global
convergence of their method is not guaranteed. Moreover, since they
maintain $X$ directly and perform the reshape of $X$ several times,
their method requires $O(I^K)$ time.

%TT-ADMM is the convex optimization method which can be guaranteed to
%converge to the global optimum via the ADMM approach and can select
%the rank adaptivity.  Also, the statistical error is theoretically
%evaluated by the value of TT rank.  Though the advantages, TT-ADMM
%cannot avoid the computational burden from controlling $X$ directly.
%
%TT-RALS is the method which can solve the computational complexity
%problem.  Since TT-RALS does not handle $X$ but controls only
%$\{G_k\}_{k=1}^K$, the computational burden is avoided.  Also, TT-RALS
%can select the TT rank adaptivity and its statistical performance is
%evaluated.  By setting the random projection parameter $s$
%sufficiently small, the time and space complexity does not increase
%exponentially as $K$ grows.

Table~\ref{tab:contribution} highlights the difference between the
existing and our methods. We emphasize that our study is the first
attempt to analyze the statistical performance of TT decomposition. In
addition, TT-RALS is only the method that both time and space
complexities do not grow exponentially in $K$.

\begin{table}[htbp]
  \centering
  {\small
  \begin{tabular}{rccccc}
    \hline
    Method & \shortstack{Global\\Convergence} & \shortstack{Rank\\Adaptivity} & \shortstack{Time\\Complexity}& \shortstack{Space\\Complexity}& \shortstack{Statistical\\Bounds}\\
    \hline
    TCAM-TT\cite{wang2016tensor}&        & & $O(nIKR^4)$ & $O(I^K)$ & \\
    ADF for TT\cite{grasedyck2015alternating}          &          & (search) &$O(KIR^3 + nKR^2)$& $O(I^K)$& \\
    SiLRTC-TT\cite{phien2016efficient}      & & \checkmark & $O(I^{3K/2})$ & $O(KI^K)$ & \\
   	\textbf{TT-ADMM}              &\checkmark & \checkmark & $O(K I^{3K/2})$ & $O(I^K)$ &\checkmark\\
    \textbf{TT-RALS}              &      &  \checkmark  & $O((n + KD^2)KI^2R^4)$ & $O(n + KI^2R^4)$ &\checkmark\\
    \hline
  \end{tabular}
  }
  \caption{Comparison of TT completion algorithms, with $R$ is a parameter for the TT rank such that $R = R_1 = \cdots = R_{K-1}$, $I = I_1 = \cdots = I_K$ is dimension, $K$ is the number of modes, $n$ is the number of observed elements, and $D$ is the dimension of random projection.}
  \label{tab:contribution}
\end{table}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "TTcomp_NIPS2017.tex"
%%% End:
