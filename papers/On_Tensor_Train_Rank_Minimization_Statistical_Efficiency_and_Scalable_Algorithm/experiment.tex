\section{Experiments}
    \begin{figure}[htbp]
            \begin{center}
                \includegraphics[width=0.95\hsize]{simulate.pdf}
            \end{center}
            \caption{Synthetic data: the estimation error
              $\|\hat{X} - X^*\|_F$ against SRR $\sum_k \sqrt{R_k}$
              with the order-$4$ tensor $(K=4)$ and the order-$5$
              tensor $(K=5)$. For each rank and $\lambda_n$, we
              measure the error by 10
              trials with different random seeds, which affect both
              the missing pattern and the initial
              points. \label{fig:validation}}
    \end{figure}

        \begin{table}
          \caption{Electricity data: the prediction error and the runtime (in seconds). \label{fig:elec}}
\begin{center}
        	\begin{tabular}{c|cc|cc|cc|cc}
				        	& \multicolumn{2}{|c|}{$K=5$} 		& \multicolumn{2}{|c}{$K=7$} 		& \multicolumn{2}{|c}{$K=8$} 		& \multicolumn{2}{|c}{$K=10$} 		\\ \hline
				Method    & 	Error		& 		Time		& 		Error	& 		Time		& 		Error	& 		Time		& 		Error	& 		Time		\\ \hline
				TCAM-TT &		0.219		&		2.174		&		0.928	&		27.497		&		0.928	&		146.651		&		N/A		&		N/A			\\
				ADF for TT &	0.998		&		1.221		&		1.160	&		23.211		&		1.180	&		278.712		&		N/A		&		N/A			\\
				SiLRTC-TT &		0.339		&		1.478		&		0.928	&		206.708		&		N/A		&		N/A			&		N/A		&		N/A			\\
				TT-ADMM &		0.221		&		0.289		&		1.019	&		154.991		&		1.061	&		2418.00		&		N/A		&		N/A  		\\
				TT-RALS &		0.219		&		4.644		&		0.928	&		4.726		&		0.928	&		7.654		&		1.173	&		7.968				
        	\end{tabular}        
\end{center}
        \end{table}


\subsection{Validation of Statistical Efficiency}

Using synthetic data, we verify the theoretical bounds derived in
Theorems~\ref{thm:convex} and \ref{thm:als}.  We first generate TT
components $\mG^*$; each component $G_k^*$ is generated as
$G_k^* = G_k^{\dagger} / \|G_k^{\dagger}\|_F$ where each element of
$G_k^{\dagger}$ is sampled from the i.i.d.  standard normal
distribution. Then we generate $Y$ by following the generative model
\eqref{model:obs} with the observation ratio $n/\prod_k I_k = 0.5$ and
the noise variance $0.01$.
%
We prepare two tensors of different size: an order-$4$ tensor of size
$8\times 8\times 10\times 10$ and an order-$5$ tensor of size
$5\times 5\times 7\times 7\times 7$.  At the order-$4$ tensor, the TT
rank is set as $(R_1,R_2,R_3)$ where $R_1,R_2,R_3 \in \{3,5,7\}$.  At
the order-$5$ tensor, the TT rank is set as $(R_1,R_2,R_3,R_4)$ where
$R_1,R_2,R_3,R_4 \in \{2,4\}$.  For estimation, we set the size of
$G_k$ and $\Pi_k$ as $10$, which is larger than the true TT rank.  The
regularization coefficient $\lambda_n$ is selected from $\{1,3,5\}$.
The parameters for random projection are set as $s = 20$ and $D_1=D_2=10$.

Figure \ref{fig:validation} shows the relation between the estimation
error and the sum of root rank (SRR) $\sum_k \sqrt{R_k}$.  The result of TT-ADMM shows that 
the empirical errors are linearly related to SSR which is shown by the theoretical result.
The result of TT-RALS roughly replicates the theoretical relationship.

%These results meet Theorem
%\ref{thm:convex} and Theorem \ref{thm:als}. 

\subsection{Higher-Order Markov Chain for Electricity Data}

We apply the proposed tensor completion methods for analyzing the
electricity consumption data~\cite{Lichman:2013}.  The dataset
contains time series measurements of household electric power
consumption for every minutes from December 2006 to November 2010 and
it contains over $200,000$ observations.

The higher-order Markov chain is a suitable method to represent
long-term dependency, and it is a common tool of time-series
analysis~\cite{hamilton1994time} and natural language
processing~\cite{jurafsky2014speech}.  Let $\{W_t\}_{t}$ be
discrete-time random variables take values in a finite set $B$, and
the order-$K$ Markov chain describes the conditional distribution of
$W_t$ with given $\{W_{\tau}\}_{\tau < t}$ as
$P(W_t | \{W_{\tau}\}_{\tau < t}) = P(W_t | W_{t-1}, \ldots,
W_{t-K})$.
As $K$ increases, the conditional distribution of $W_t$ can include
more information from the past observations.

%For practical use, the higher-order Markov chain has two problems: the memory requirement and unobserved path.
%Since it is necessary to storage $|B|^{M+1}$ numerics to represent the $M$-th Markov chain, there exists computational burden to compute the transition.
%Also, the variation of the realized path increases with large $M$, a part of the path is not observed unless the length of the observation is sufficiently long.

We complete the missing values of $K$-th Markov transition of the
electricity dataset.  We discretize the value of the dataset into $10$
values and set $K \in \{ 6,7,8,10\}$.  Next, we empirically estimate
the conditional distribution of size $10^K$ using $200,000$
observations.  Then, we create $X$ by randomly selecting $n=10,000$
elements from the the conditional distribution and regarding the other
elements as missing. After completion by the TT methods, the
prediction error is measured.  We select hyperparameters using a grid
search with cross-validation.

Figure~\ref{fig:elec} compares the prediction error and the runtime.
When $K=5$, the rank adaptive methods achieve low estimation errors.
As $K$ increases, however, all the methods except for TT-RALS suffers
from the scalability issue. Indeed, at $K=10$, only TT-RALS works and
the others does not due to exhausting memory.
%For accuracy, TT-RALS outperforms the others because it selects the TT rank appropriately.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "TTcomp_NIPS2017.tex"
%%% End:
