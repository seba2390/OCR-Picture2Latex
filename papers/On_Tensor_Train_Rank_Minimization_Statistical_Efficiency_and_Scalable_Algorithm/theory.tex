
\section{Theoretical Analysis}

In this section, we investigate how the TT rank and the number of
observations affect to the estimation error. Note that all the proofs
of this section are deferred to Supplementary material.


\subsection{Convex Solution}

To analyze the statistical error of the convex problem~\eqref{eq:prob}, we assume the
\textit{incoherence} of the reshaped version of $X^*$.
\begin{assumption}{(Incoherence Assumption)}\label{asmp:incoherence_convex}
There exists $k \in \{1,\ldots,K\}$ such that a matrix $Q_k(X^*)$ has orthogonal singular vectors $\{ u_r \in \R^{I_{\leq k}}, v_r \in  \R^{I_{k<}}\}_{r = 1}^{R_k}$ satisfying
	\begin{align*}
          \max_{1 \leq i \leq I_{<k}}\|P_{U}e_{i}\| \leq ( \mu R_k / I_{\leq k})^{\frac{1}{2}}
          \quad \text{and} \quad
          \max_{1 \leq i \leq I_{<k}}\|P_{V}e_{i}\| \leq ( \mu R_k / I_{k<})^{\frac{1}{2}}
	\end{align*}
	with some $0 \leq \mu < 1$.
	Here, $P_U$ and $P_V$ are linear projections onto spaces spanned by $\{u_r\}_r$ and $\{v_r\}_r$; $\{e_i\}_i$ is the natural basis.
\end{assumption}
Intuitively, the incoherence assumption requires that the singular
vectors for the matrix $Q_k(X^*)$ are well separated.
This type of assumption is commonly used in the matrix and tensor
completion studies~\cite{candes2012exact,candes2010matrix,
  zhang2016exact}.
%
Under the incoherence assumption, the error rate of the solution of \eqref{eq:prob} is derived.
\begin{theorem} \label{thm:convex} Let
  $X^*\in\mX$ be a true tensor with TT rank $(R_1,\dots,R_{K-1})$, and let
  $\hat{X}\in\mX$ be the minimizer of \eqref{opt:general}.  Suppose that
  $\lambda_n \geq \|\mathfrak{X}^*(\mE)\|_{\infty}/n$ and that Assumption
  \ref{asmp:incoherence_convex} for some $k' \in \{1,2,\ldots,K\}$ is
  satisfied.  If
	\begin{align*}
		n \geq C_{m'} \mu_{k'}^2 \max\{I_{\leq k'}, I_{k'<}\} R_{k'} \log^3  \max\{I_{\leq k'}, I_{k'<}\}
	\end{align*}
	with a constant $C_{m'}$, then with probability at least $1-( \max\{I_{\leq k'}, I_{k'<}\})^{-3}$ and with a constant $C_X$,
	\begin{align*}
		\|\hat{X} - X^*\|_F \leq  C_{X} \frac{\lambda_n}{K}\sum_{k=1}^{K-1} \sqrt{R_k }.
	\end{align*}
\end{theorem}

Theorem \ref{thm:convex} states that the bound for the statistical
error gets larger as the TT rank increases.  In other words,
completing a tensor is relatively easy as long as the tensor has small TT
rank.  Also, when we set $\lambda_n \to 0$ as $n$ increases, we can
state the consistency of the minimizer.


The result of Theorem \ref{thm:convex} is similar to that obtained from the studies on
matrix completion \cite{candes2010matrix, negahban2011estimation} and
tensor completion with the Tucker decomposition or SVD
\cite{tomioka2011statistical, zhang2016exact}.  Note that, although
Theorem~\ref{thm:convex} is for tensor completion, the result can
easily be generalized to other settings such as the tensor recovery or
the compressed sensing problems.


\subsection{TT-RALS Solution} \label{sec:theory}

%Let $\mG^{t}$ be the set of $G_k$ after repeating the TT-RALS steps
%for $t$ times.  When the update of $\mG^t$ is sufficiently small, we
%define $\hat{\mG} := \mG^t$ as the estimator of $\mG^*$ and
%$X(\hat{\mG})$ as the estimator of $X^*$ by TT-RALS.


Prior to the analysis, let $\mG^*$ be the true TT components such that
$X^* = X(\mG^*)$.  For simplification, we assume that the elements of
$\mG^*$ are normalized, i.e., $\|G_k\|=1, \forall k$, and an
$R_k \times I_{k-1}I_k$ matrix reshaped from $G_k^*$ has a $R_k$ row
rank.

In addition to the incoherence property (Assumption \ref{asmp:incoherence_convex}), we introduce an additional assumption on the initial point of the ALS iteration.

\begin{assumption}{(Initial Point Assumption)}\label{asmp:intial}
    Let $\mG^{\text{init}} := \{G_k^{\text{init}}\}_{k=1}^K$ be the initial point of the ALS iteration procedure.
    Then, there exists a finite constant $C_{\gamma}$ that satisfies
    \begin{align*}
        \max_{k \in \{1,\ldots,K\}} \|G_k^{\text{init}} - G_k^*\|_F \leq C_{\gamma}.
    \end{align*}
\end{assumption}

Assumption~\ref{asmp:intial} requires that the initial point is
sufficiently close to the true solutions $\mG^*$.  Although the ALS
method is not guaranteed to converge to the global optimum in general,
Assumption~\ref{asmp:intial} guarantees the
convergence to the true solutions~\cite{suzuki2016minimax}. Now we can evaluate the error
rate of the solution obtained by TT-RALS.
%This assumption is used in
%the field of ALS \cite{suzuki2016minimax}.


\begin{theorem} \label{thm:als} Let $X(\mG^*)$ be the true tensor
  generated by $\mG^*$ with TT rank $(R_1,\dots,R_{K-1})$, and
  $\hat{\mG} = \mG^t$ be the solution of TT-RALS at the $t$-th 
  iteration.  Further, suppose that Assumption~\ref{asmp:incoherence_convex} for some $k' \in \{1,2,\ldots,K\}$ and
  Assumption~\ref{asmp:intial} are satisfied, and suppose that Theorem \eqref{thm:random} holds with $\epsilon > 0$ for $k=1,\dots,K$.  Let $C_m,C_A,C_B > 0$ be $0 < \chi < 1$ be some
  constants.  If
    \begin{align*}
        n \geq C_{m} \mu_{k'}^2 R_{k'} \max\{I_{\leq k'}, I_{k'<}\} \log^3  \max\{I_{\leq k'}, I_{k'<}\},
    \end{align*}
    and the number of iterations $t$ satisfies
        $t \geq (\log \chi)^{-1}\{\log ( C_B  \lambda_n K^{-1}(1+\epsilon)\sum_{k} \sqrt{R_{k}} ) - \log C_{\gamma} \}$,
     then with probability at least $1-\epsilon( \max\{I_{\leq k'}, I_{k'<}\})^{-3}$ and for $\lambda_n \geq \|\mathfrak{X}^*(\mE)\|_{\infty}/n$,
    \begin{align}\label{eq:als-bound}
        \|X(\hat{\mG}) - X(\mG^*)\|_F \leq C_A  (1+\epsilon) \lambda_n \sum_{k=1}^{K-1} \sqrt{R_{k}}.
    \end{align}
\end{theorem}

Again, we can obtain the consistency of TT-RALS by setting
$\lambda_n \to 0$ as $n$ increases.  Since the setting of $\lambda_n$
corresponds to that of Theorem \ref{thm:convex}, the speed of
convergence of TT-RALS in terms of $n$ is equivalent to the speed of
TT-ADMM.

By comparing with the convex approach (Theorem~\ref{thm:convex}), the
error rate becomes slightly worse.  Here, the term
$\lambda_n \sum_{k=1}^{K-1} \sqrt{R_{k}}$ in \eqref{eq:als-bound}
comes from the estimation by the alternating minimization, which
linearly increases by $K$.  This is because there are $K$ optimization
problems and their errors are accumulated to the final solution.
%
The term $(1+\epsilon)$ in \eqref{eq:als-bound} comes from the random
projection.  The size of the error $\epsilon$ can be arbitrary small
by controlling the parameters of the random projection $D_1,D_2$ and
$s$. 
%
%Also, we provide the error bound of TT-RALS without the random
%projection.
%%
%\begin{corollary} \label{cor:rals}
%	Suppose that the settings of Theorem \ref{thm:als} hold.
%	Let $X(\hat{\mG})$ be the solution of TT-RALS without the random projection operator $\mP_k$.
%	Then, with sufficiently large $n,t$ and high probability,
%    \begin{align*}
%        \|X(\hat{\mG}) - X(\mG^*)\|_F \leq C_A  \lambda_n \sum_{k=1}^{K-1} \sqrt{R_{k}}.
%    \end{align*}
%\end{corollary}
%%






%The error bound of Theorem \ref{thm:als} is similar to that of TT-convex in Theorem \ref{thm:convex}.
%Namely, when the initial points of the ALS are close to $\mG^*$ and the number of ALS iteration is sufficiently large, the ALS can obtain the same performance to TT-Convex.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "TTcomp_NIPS2017.tex"
%%% End:
