\section{Convex Formulation of TT Rank Minimization}

%\knote{Replace TT-rank by TT rank}

To adopt TT decomposition to the convex optimization problem as
\eqref{opt:general}, we need the convex surrogation of TT rank.  For
that purpose, we introduce the \textit{Schatten TT
  norm}~\cite{phien2016efficient} as follows:
\begin{align}
	\|X\|_{s,T} := \frac{1}{K-1} \sum_{k=1}^{K-1} \|Q_k(X)\|_{s} := \frac{1}{K-1}\sum_{k=1}^{K-1} \sum_{j} \sigma_j(Q_k(X)), \label{def:ttnorm}
\end{align}
where $Q_k : \mX \to \R^{I_{\leq k} \times I_{k<}}$ is a reshaping
operator that converts a tensor to a large matrix where the first $k$
modes are combined into the rows and the rest $K-k$ modes are combined
into the columns. Oseledets \etal \cite{oseledets2011tensor} shows that the matrix
rank of $Q_k(X)$ can bound the $k$-th TT rank of $X$, implying that the
Schatten TT norm surrogates the sum of the TT rank. Putting the
Schatten TT norm into \eqref{opt:general}, we obtain the following
optimization problem:
\begin{align}
	\min_{X \in \mX} \left[ \frac{1}{2n}\| Y - \mathfrak{X}(X) \|^2 + \lambda_n\|X\|_{s,T} \right]. \label{eq:prob}
\end{align}
%Here, $\lambda_n$ is a penalty coefficient.


\subsection{ADMM Algorithm}

To solve \eqref{eq:prob}, we consider the
augmented Lagrangian function
$L(x, \{Z_k\}_{k=1}^{K-1}, \{\alpha_k\}_{k=1}^{K-1})$,
%as
%\begin{align*}
%	&L(x, \{Z_k\}_{k=1}^{K-1}, \{\alpha_k\}_{k=1}^{K-1}) \\
%	&= \frac{1}{n}\|\tilde {\Omega}x - Y\|_F^2 + \lambda_n \sum_{k=1}^{K-1} \|Z_k\|_s  + \sum_{k=1}^{K-1} \left( \eta \alpha_k^T (x - \mbox{vec}Z_k) + \frac{\eta}{2}\|x - \mbox{vec}Z_k\|_F^2 \right),
%\end{align*}
where $x \in \mathbb{R}^{\prod_k I_k}$ is the
vectorization of $X$, $Z_k$ is a reshaped matrices with size
$I_{\leq k} \times I_{k<}$, and
$\alpha_k \in \mathbb{R}^{\prod_k I_k}$ is a coefficient for
constraints.  Given initial points
$(x^{(0)}, \{Z_k^{(0)}\}_k, \{\alpha_k^{(0)}\}_k)$, the $\ell$-th step
of ADMM is written as follows:
\begin{align*}
	&x^{(\ell + 1)} = \left( \tilde{\Omega}^T Y+ n \eta \frac{1}{K-1}\sum_{k=1}^{K-1} (V_k(Z_k^{(\ell)}) - \alpha_k^{(\ell)})\right) / (1 + n \eta K),\\
	&Z_k^{(\ell + 1)} = \mbox{prox}_{\lambda_n / \eta} (V_k^{-1}(x^{(\ell + 1)} + \alpha_k^{(\ell)})), ~~ k = 1,\ldots,K,\\
	& \alpha_k^{(\ell + 1)} = \alpha_k^{(\ell)} + (x^{(\ell + 1)} - V_k(Z_k^{(\ell + 1)})), ~~  k = 1,\ldots,K.
\end{align*}
Here, $\tilde{\Omega}$ is an $n \times \prod_{k=1}^{I_k}$ matrix that
works as the inversion mapping of $\mathfrak{X}$; 
%$./$ denotes an element-wise division; 
$V_k$ is a vectorizing operator of an $I_{\leq k} \times I_{k<}$
matrix; $\mbox{prox}(\cdot)$ is the shrinkage operation of the
singular values as $\mbox{prox}_{b}(W) = U \max\{S-bI,0\}V^T$, where
$USV^T$ is the singular value decomposition of $W$; $\eta>0$ is a
hyperparameter for a step size.  We stop the iteration when the
convergence criterion is satisfied (e.g. as suggested by
Tomioka \etal \cite{tomioka2011statistical}). Since the Schatten TT
norm~\eqref{def:ttnorm} is convex, the sequence of the variables of
ADMM is guaranteed to converge to the optimal solution (Theorem
5.1, \cite{gandy2011tensor}). We refer to this algorithm as \emph{TT-ADMM}.

TT-ADMM requires huge resources in terms of both time and space.  For
the time complexity, the proximal operation of the Schatten TT norm,
namely the SVD thresholding of $V_k^{-1}$, yields the dominant, which is
$O(I^{3K/2})$ time. For the space complexity, we have $O(K)$ variables
of size $O(I^K)$, which requires $O(KI^K)$ space.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "TTcomp_NIPS2017.tex"
%%% End: