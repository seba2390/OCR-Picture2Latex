\appendix

\if0
\begin{center}
	\begin{Large}
		Supplementary Material for \\
		``On Tensor Train Rank Minimization : Statistical Efficiency and Scalable Algorithm''
	\end{Large}
\end{center}
\fi

\section{Proof of Theorem \ref{thm:random}}

The theorem is obtained immediately by combining Li et al.~\cite{li2006very} and Mu et al.~\cite{mu2011accelerated}.

Let $\Pi: \mathbb{R}^n \to \mathbb{R}^k$ be a sparse random projection defined by 
\begin{align}
  \Pi_{ij} = \begin{cases} +\sqrt{s/k} & \text{probability } 1/2s, \\ 0 & \text{probability } 1 - 1/s, \\ -\sqrt{s/k} & \text{probability } 1/2s. \end{cases}
\end{align}
Then the following theorem holds.
\begin{lemma}[Lemma 4 in \cite{li2006very}]
\label{lem:sparseJL}
Let $u$ be a unit vector. Then
$\sqrt{k} (\Pi u)_i \to N(0,1)$ and $k \| \Pi u \|_2^2 \to \chi_k^2$ in law with the convergence rate 
\begin{align}
  |P(\sqrt{k} (\Pi u)_i < t) - P(N(0,1) < t)| \le 0.8 \sqrt{s} \sum_j |u_j|^3.
\end{align}
%If $s = \Omega(\alpha^2 \log (n / \epsilon \delta))$ and $k = \Omega(\log (1/\delta)/\epsilon^2)$, for every vector $x \in \mathbb{R}^n$ satisfying $\| x \|_\infty \le \alpha \| x \|_2$, we have
%\begin{align}
%  (1 - \epsilon) \| x \|_2 \le \| \Pi x \|_2 \le (1 + \epsilon) \| x \|_2.
%\end{align}
%with probability at least $1 - O(\delta)$.
\qed
\end{lemma}
Suppose that $\sum_j |u_j|^3 \le e^{-k \epsilon^2/4} / (1.6 k \sqrt{s})$. 
Then 
\begin{align}
  |P(\|\Pi u\|_2^2 \in [1-\epsilon, 1+\epsilon]) - P(\chi_k^2  \in [k(1-\epsilon), k(1+\epsilon)])| \le e^{-k \epsilon^2/4}.
\end{align}
By using 
\begin{align}
  P(\chi_k^2  \in [k(1-\epsilon), k(1+\epsilon)]) \le 2 e^{-k (\epsilon^2 - \epsilon^3)/4},
\end{align}
we have
\begin{align}
  P(\| \Pi u \|_2^2 \in [1-\epsilon, 1+\epsilon]) \le 3 e^{-k (\epsilon^2 - \epsilon^3)/4}.
\end{align}

The preservation of $L_2$ norm implies the preservation of the Schatten-$1$ norm as follows.
\begin{lemma}[Restatement of Theorem~1 in \cite{mu2011accelerated}]
\label{lem:schattenbound}
Let $Z$ be an $m \times n$ matrix with rank $r$.
If $k \ge r$ and $\| \Pi u \|_2^2 \in [1-\epsilon, 1+\epsilon]$ for all singular vectors $u$ of $Z$, we have
\begin{align}
  \sqrt{(1 - \epsilon)/r} \| Z \|_s \le \| \Pi Z \|_s \le \sqrt{1 + \epsilon} \| Z \|_s.
\end{align}
\qed
\end{lemma}

Now we prove Theorem~\ref{thm:random}.
Let $Z = Q_k(X)$ be a $\prod_{k'=1}^k I_{k'} \times \prod_{k'=k+1}^K I_{k'}$ matrix obtained by reshaping tensor $X$.
Since $X$ is a TT of rank $(R_1, \ldots, R_K)$, the rank of matrix $Z$ is at most $R_k$.
By applying Lemma~\ref{lem:schattenbound} twice, we obtain
\begin{align}
  \frac{1 - \epsilon}{R_k} \| Z \|_s \le \| \Pi Z \|_s \le (1 + \epsilon) \| Z \|_s
\end{align}
with some probability. 
If $\sum_j |u_j|^3 \le e^{-k \epsilon^2/4} / (1.6 k \sqrt{s})$ for all singular vectors of $Z$, 
the probability is at least $1 - 6 R_k e^{-k \epsilon^2/4}$ since there are $2 R_k$ singular vectors.

\section{Proof of Theorem \ref{thm:convex}}

\begin{proof}

Since $\hat{X}$ is the minimizer of the optimization problem, we have the following basic inequality
\begin{align*}
	\frac{1}{2n}\|Y - \mathfrak{X}(\hat{X})\|^2 + \lambda_n \sum_{k=1}^{K-1} \|Q_k(\hat{X})\|_{s} \leq \frac{1}{2n}\|Y - \mathfrak{X}(\hat{X}^*)\|^2 + \lambda_n  \sum_{k=1}^{K-1} \|Q_k(X^*)\|_{s}.
\end{align*}
Using the relation that
\begin{align*}
	\|Y - \mathfrak{X}(\hat{X})\|^2& = \|(Y - \mathfrak{X}(X^*)) -( \mathfrak{X}(\hat{X}) - \mathfrak{X}(X^*))\|^2 \\
	&= \|Y - \mathfrak{X}(X^*)\|^2 + \| \mathfrak{X}(\hat{X}) - \mathfrak{X}(X^*)\|^2 - 2\langle Y - \mathfrak{X}(X^*), \mathfrak{X}(\hat{X}) - \mathfrak{X}(X^*)\rangle,
\end{align*}
we rewrite the basic inequality as
\begin{align*}
	\frac{1}{2n}\| \mathfrak{X}(\hat{X}) - \mathfrak{X}(X^*)\|^2 \leq \frac{1}{n} \langle \mathfrak{X}(\hat{X}) - \mathfrak{X}(X^*), \mE \rangle + \lambda_n \sum_{k=1}^{K-1} \left( \|Q_k(X^*)\|_s - \|Q_k(\hat{X})\|_s \right).
\end{align*}
Define the error $\Delta := \hat{X} - X^*$.
Applying the H\"older's inequality, we have
\begin{align*}
	&\frac{1}{n}\langle \mathfrak{X}(\Delta) , \mE \rangle = \frac{1}{n} \langle \Delta , \mathfrak{X}^*(\mE) \rangle = \frac{1}{n}\frac{1}{K-1}\sum_{k=1}^{K-1} \langle Q_k(\Delta), \mathfrak{X}^*(\mE) \rangle \\
	&\leq \frac{1}{n} \frac{1}{K-1}\sum_{k=1}^{K-1} \|\mE\|_{\infty} \|Q_k(\Delta)\|_{s} \leq \frac{\lambda_n }{K-1} \sum_{k=1}^{K-1}\|Q_k(\Delta)\|_{s} ,
\end{align*}
where $\mathfrak{X}^*$ is an adjoint operator of $\mathfrak{X}$ and the last inequality holds by the setting of $\lambda_n$.
Also, the triangle inequality and the linearity of $Q_k(\cdot)$ yield
\begin{align*}
	\|Q_k(X^*)\|_s - \|Q_k(\hat{X})\|_s \leq \|Q_k(\Delta)\|_s.
\end{align*}
Then, we bound the inequality as
\begin{align*}
	\frac{1}{2n}\|\mathfrak{X}(\Delta)\|^2 &\leq \frac{\lambda_n }{K-1} \sum_{k=1}^{K-1}\|Q_k(\Delta)\|_{s} + \frac{ \lambda_n}{K-1} \sum_{k=1}^{K-1}\|Q_k(\Delta)\|_s =  \frac{2\lambda_n}{K-1}\sum_{k=1}^{K-1}\|Q_k(\Delta)\|_{s} .
\end{align*}
To bound $\|Q_k(\Delta)\|_{s}$,  we apply the result of Lemma 1 in \cite{negahban2011estimation} and Lemma 2 in \cite{tomioka2011statistical}.
Along with proof of the lemmas, we obtain the property that a rank of $Q_k(\Delta)$ is bounded by $2R_k$, thus the Cauchy-Schwartz inequality implies
\begin{align*}
	\|Q_k(\Delta)\|_s \leq  \sqrt{2R_k }\|Q_k(\Delta)\|_F.
\end{align*}
Then we obtain
\begin{align*}
	\|\mathfrak{X}(\Delta)\|_F^2 &\leq  \frac{2 \lambda_n}{K-1}\sum_{k=1}^{K-1} \sqrt{2R_k }\|Q_k(\Delta)\|_F.
\end{align*}

We apply the completion theory by \cite{candes2010matrix, candes2012exact} to bound $\|\mathfrak{X}(\Delta)\|_F^2$ below.
Let $k' \in \{1,\ldots,K\}$ be the index which satisfies Assumption \ref{asmp:incoherence_convex}, and we have $\|\mathfrak{X}(\Delta)\|^2 = \|\tilde{\mathfrak{X}}(Q_{k'}(\Delta))\|^2$ where $\tilde{\mathfrak{X}}$ is a rearranging operator for the reshaped tensor.
Then, Theorem 7 in \cite{candes2010matrix} yields that 
\begin{align*}
	\|Q_{k'}(\Delta)\|_F \leq \left(  \sqrt{\frac{48  \min\{I_{\leq k'}, I_{k'<}\}}{n}}+ 1 \right) \|\tilde{\mathfrak{X}}(Q_{k'}(\Delta))\|,
\end{align*}
with probability at least $1-( \max\{I_{\leq k'}, I_{k'<}\})^{-3}$ and
\begin{align*}
	n \geq C_{m'} \mu_{k'}^2  \max\{I_{\leq k'}, I_{k'<}\} R_{k'} \log^3  \max\{I_{\leq k'}, I_{k'<}\},
\end{align*}
with a constant $C_{m'} > 0$.
%Here, we define $\tilde{I}_{k'}^+ := \max \{ \prod_{\ell \leq k'} I_{\ell}, \prod_{\ell > k'} I_{\ell}\}$ and  $\tilde{I}_{k'}^- := \max \{ \prod_{\ell \leq k'} I_{\ell}, \prod_{\ell > k'} I_{\ell}\}$.
Then we obtain that 
\begin{align*}
	\frac{1}{n}\|\tilde{\mathfrak{X}}(Q_{k'}({\Delta}))\|^2 \geq   C_{\kappa'} \|Q_{k'}(\Delta)\|_F^2 = C_{\kappa'} \|\Delta\|_F^2,
\end{align*}
where $C_{\kappa} = (144  \min\{I_{\leq k'}, I_{k'<}\} + 3n)^{-1} > 0$.

Finally, we have
\begin{align*}
	\|\Delta\|_F^2 &\leq  C_{\kappa '}^{-1 } \frac{2 \lambda_n}{K-1}\sum_{k=1}^{K-1} \sqrt{2R_k }\|Q_k(\Delta)\|_F \\
	&= C_{\kappa '}^{-1 }\|\Delta\|_F \frac{2 \lambda_n}{K-1}\sum_{k=1}^{K-1} \sqrt{2R_k }\\
	&= 3C_{\kappa '}^{-1 }\|\Delta\|_F \frac{\lambda_n}{K}\sum_{k=1}^{K-1} \sqrt{2R_k }.
\end{align*}
Dividing both hands side by $\|\Delta\|_F$ provides the result.

\end{proof}


\section{Proof of Theorem \ref{thm:als}}

Preliminarily, we introduce an alternative formation of the optimization problem.
For each $k \in \{1,2,\ldots,K\}$, we rewrite the term $X_k (\mG)$ as
\begin{align*}
	G_k \times_2 G_{<k} \times_3 G_{k<},
\end{align*}
where $\times_j$ denotes the $j$-mode product (for detail, see \cite{kolda2009tensor}).
Here, $G_{<k}$ is a tensor with size $R_k \times I_1 \times \cdots \times I_{K-1}$ and its element is given as
\begin{align*}
	[G_{<k}]_{r,j_1,\ldots,j_{k-1}} = [G_1]_{j_1,:,:}[G_2]_{j_2,:,:} \cdots [G_{k-1}]_{j_{k-1},:,r},
\end{align*}
for $j_{k'} = 1,\ldots,I_{k'}$ and $r = 1,\ldots,R_k$.
Namely, $G_{<k}$ is the left side of tensor train decomposition of $X$ than $G_k$.
Similarly, $G_{k<}$ is a tensor with size $R_{k+1} \times I_{k+1} \times \cdots \times I_{K}$ and its element is given as
\begin{align*}
	[G_{k<}]_{r,j_{k+1},\ldots,j_{K}} = [G_{k+1}]_{j_{k+1},r,:}\cdots [G_{K}]_{j_{K},:,:},
\end{align*}
for $j_{k'} = 1,\ldots,I_{k'}$ and $r = 1,\ldots,R_{k+1}$.
Using the result, the ALS optimization problem \eqref{opt:als2} is rewritten as
\begin{align}
%	\min_{G_k} \left[ \frac{1}{2n} \|Y - \mathfrak{X}(G_k \times_2 G_{<k} \times_3 G_{k<})\|^2 + \frac{\lambda_n}{K-1}\sum_{k'=1}^{K-1} \|Q_{k'}(G_k \times_2 G_{<k} \times_3 G_{k<})\|_{s} \right]. \label{opt:als3}
	\min_{G_k} \left[ \frac{1}{2n} \|Y - \mathfrak{X}(G_k \times_2 G_{<k} \times_3 G_{k<})\|^2 + \frac{\lambda_n}{K-1}\sum_{k'=1}^{K-1} \|\mP_{k'}(G_k \times_2 G_{<k} \times_3 G_{k<})\|_{s} \right]. \label{opt:als3}
\end{align}
When $k=1$, we set $G_{<k} = 1$.
Similarly, when $k=K$, $G_{k<} = 1$ holds.

Using the formula, we investigate the convergence of $G_k$ by fixing other elements as $G_{<k} = \tilde{G}_{<k}$ and $G_{k<} = \tilde{G}_{k<}$.
Let $\{G_k^*\}_{k^*=1}^K$ be a set of tensor which formulates the true tensor $X^*$.
Also, $G_{<k}^*$ and $G_{k<}^*$ are defined similarly.
To evaluate the convergence, we introduce that
\begin{align*}
	\Xi(\tilde{\mG}) := \max_{k \in \{1,\ldots,K\}} \left[\|\tilde{G}_k - G_k^*\|_F \right].
\end{align*}

We obtain the following lemma which evaluates the optimization of \eqref{opt:als3} with given $\tilde{G}_{<k}$ and $\tilde{G}_{k<}$.
\begin{lemma} \label{lem:als_1}
	For each $k \in \{1,\ldots,K\}$, consider the optimization \eqref{opt:als3} with respect to $G_k$ with given $\tilde{\mG}$.
	Then, with probability at least $1-( \max\{I_{\leq k'}, I_{k'<}\})^{-3}$ and 
	\begin{align*}
		n \geq C_{m} \mu_{k'}^2  \max\{I_{\leq k'}, I_{k'<}\} R_{k'} \log^n  \max\{I_{\leq k'}, I_{k'<}\},
	\end{align*}
	we obtain
	\begin{align*}
			\|\hat{G}_k - G_k^*\| \leq 6 (C_{\kappa} C_K)^{-1} \left\{ 2(K-1)C_K(1+n^{-1})  \Xi(\tilde{\mG})  +   \frac{2 \lambda_n (2 + \epsilon) }{K-1}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}}\right\}.
	\end{align*}
\end{lemma}
\begin{proof}

Our proof takes following four steps: 1) derive a basic inequality from the optimality condition, 2) bound terms of the RHS of the basis inequality, 3) bound below the LHS of the basis inequality, and 4) combine the result.

\paragraph{Step 1.} Derive a basic inequality.

By the optimality condition of \eqref{opt:als3} with given $\tilde{G}_{<k}$ and $\tilde{G}_{k<}$, we have
\begin{align}
	 &\frac{1}{2n} \|Y - \mathfrak{X}(\hat{G}_k \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|^2 + \frac{\lambda_n}{K-1}\sum_{k'=1}^{K-1} \|\mP_{k'}(\hat{G}_k \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}  \notag \\
	 & \quad \leq \frac{1}{2n} \|Y - \mathfrak{X}(G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|^2 + \frac{\lambda_n}{K-1}\sum_{k'=1}^{K-1} \|\mP_{k'}(G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}. \label{cond:opt}
\end{align}
Using the triangle inequality and the linearity of $\mathfrak{X}$ and the mode product $\times_j$, we obtain 
\begin{align*}
	&\|Y - \mathfrak{X}(\hat{G}_k \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|^2 \\
	&=\|\{Y - \mathfrak{X}(G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\} -\{\mathfrak{X}(\hat{G}_k \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}) - \mathfrak{X}(G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\} \|^2 \\
	&= \|\{Y - \mathfrak{X}(G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\} -\mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})  \|^2 \\
	& = \|Y - \mathfrak{X}(G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|^2 + \| \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})  \|^2  \\
	& \quad -2 \langle Y - \mathfrak{X}(G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}), \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}) \rangle.
\end{align*}
Substituting the result into \eqref{cond:opt}, we obtain that 
\begin{align}
	 &\frac{1}{2n} \| \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})  \|^2 + \frac{\lambda_n}{K-1}\sum_{k'=1}^{K-1} \|\mP_{k'}(\hat{G}_k \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}  \notag \\
	 & \quad \leq \frac{1}{n} \langle Y - \mathfrak{X}(G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}), \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}) \rangle \notag \\
	 & \quad \quad  + \frac{\lambda_n}{K-1}\sum_{k'=1}^{K-1} \|\mP_{k'}(G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}. \label{ineq:basic0}
\end{align}
About the regularization term, we apply the following inequality
\begin{align}
	&\frac{\lambda_n}{K-1}\sum_{k'=1}^{K-1} \|\mP_{k'}(G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s} - \frac{\lambda_n}{K-1}\sum_{k'=1}^{K-1} \|\mP_{k'}(\hat{G}_k \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s} \notag \\
	&\leq \frac{\lambda_n}{K-1}\sum_{k'=1}^{K-1} \|\mP_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s} \notag \\
	&= \frac{\lambda_n}{K-1}\sum_{k'=1}^{K-1} \left(\|Q_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}  \right. \notag \\
	& \quad  \left. -  \|\mP_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s} -  \|Q_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}  \right), \label{ineq:pq}
\end{align}
by using the triangle inequality and the linearity of the random projection operator $\mP_{k'}$.
Here, we apply Theorem \ref{thm:random} with $\epsilon$ and obtain
\begin{align*}
	 &\left( \frac{1 - \epsilon}{2 R_{k'}} - 1 \right)\|Q_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s} \\
	 &\quad \leq \|\mP_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s} -  \|Q_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s} \\
	 &\quad \leq \epsilon \|Q_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}.
\end{align*}
Here, the denominator in the left hand side follows Lemma 1 in \cite{negahban2011estimation}.
Then we have
\begin{align*}
	 &\left| \|\mP_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s} -  \|Q_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}  \right| \\
	 &\leq \max\{\epsilon,| (1-\epsilon)/(2R_{k'}) - 1| \}\|Q_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}\\
	 & \leq (1 + \epsilon) \|Q_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}.
\end{align*}
Using this result and continue \eqref{ineq:pq} then we have
\begin{align}
	\eqref{ineq:pq} \leq \frac{\lambda_n}{K-1}\sum_{k'=1}^{K-1} (2+\epsilon)\|Q_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}. \label{ineq:pq2}
\end{align}

About the first term of the RHS of \eqref{ineq:basic0}, we decompose it as
\begin{align*}
	&\frac{1}{n} \langle Y - \mathfrak{X}(G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}), \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}) \rangle \\
	&= \frac{1}{n} \langle Y - \mathfrak{X}(G_k^* \times_2 G_{<k}^* \times_3 G_{k<}^*), \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}) \rangle \\
	&\quad  + \frac{1}{n} \langle  \mathfrak{X}(G_k^* \times_2 (\tilde{G}_{<k} - G_{<k}^*)\times_3 G_{k<}^*), \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}) \rangle \\
	&\quad  + \frac{1}{n} \langle  \mathfrak{X}(G_k^* \times_2 G_{<k}^*\times_3 (\tilde{G}_{k<} - G_{k<}^*)), \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}) \rangle \\
	&\quad  + \frac{1}{n} \langle  \mathfrak{X}(G_k^* \times_2 (\tilde{G}_{<k} - G_{<k}^*)\times_3 (\tilde{G}_{k<} - G_{k<}^*)), \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}) \rangle \\
	&=: T_0 + T_1 + T_2 + T_3.
\end{align*}
About the term $T_0$, we use the observation model \eqref{model:obs} and the adjoint operator $\mathfrak{X}^*$ then obtain
\begin{align*}
	T_0 &= \frac{1}{n} \langle \mE, \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<} )\rangle \\
	&= \frac{1}{n} \langle  \mathfrak{X}^*(\mE), (\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<} \rangle.
\end{align*}
Since the reshaping does not affect the value of the inner product, we continue to evaluate $T_0$ as
\begin{align*}
	T_0 &=\frac{1}{n} \langle  \mathfrak{X}^*(\mE), (\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<} \rangle \\
	& =\frac{1}{n(K-1)} \sum_{k'=1}^{K-1} \langle Q_{k'}(\mathfrak{X}^*(\mE)), Q_{k'} ( (\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}) \rangle \\
	& \leq \frac{1}{n(K-1)} \sum_{k'=1}^{K-1} \|Q_{k'}(\mathfrak{X}^*(\mE))\|_{\infty} \| Q_{k'} ( (\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s} \\
	&= \frac{1}{n(K-1)}\|\mathfrak{X}^*(\mE)\|_{\infty} \sum_{k'=1}^{K-1}  \| Q_{k'} ( (\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s} \\
	& \leq \frac{\lambda_n}{(K-1)} \sum_{k'=1}^{K-1}  \| Q_{k'} ( (\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}.
\end{align*}
The first inequality follows the H\"older's inequality, and the second inequality is derived by the setting of $\lambda_n$.

Substituting \eqref{ineq:pq2} and the bounds with $T_1,T_2,T_3$ and $T_0$ into \eqref{ineq:basic0}, finally we obtain
\begin{align}
	 &\frac{1}{2n} \| \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})  \|^2  \notag \\
	 & \leq T_1 + T_2 + T_3 + \underbrace{\frac{\lambda_n }{K-1} \sum_{k'=1}^{K-1} (2+\epsilon) \|Q_{k'}((G_k^* - \hat{G}_k) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\|_{s}}_{=:T_4}. \label{ineq:basic1}
\end{align}
Here, we obtain the basic inequality.


\paragraph{Step 2.} Bound the RHS of the basic inequality.


For brevity, we introduce notation 
\begin{align*}
	\tilde{\Delta}_k := (\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}.
\end{align*}

We bound $T_1$ by using the Cauchy-Schwartz inequality as
\begin{align*}
T_1 &= \frac{1}{n} \langle  \mathfrak{X}(G_k^* \times_2 (\tilde{G}_{<k} - G_{<k}^*)\times_3 G_{k<}^*), \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}) \rangle \\
	& \leq \frac{1}{n} \|\mathfrak{X}(G_k^* \times_2 (\tilde{G}_{<k} - G_{<k}^*)\times_3 G_{k<}^*)\| \|\mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\| \\
	& \leq \frac{1}{n} \|G_k^* \times_2 (\tilde{G}_{<k} - G_{<k}^*)\times_3 G_{k<}^*\|_F \|(\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}\|_F\\
	& \leq \frac{1}{n} \|G_k^* \times_2 (\tilde{G}_{<k} - G_{<k}^*)\times_3 G_{k<}^*\|_F \|\tilde{\Delta}_k\|_F,	
\end{align*}
here we use the relation $\|\mathfrak{X}(X)\|^2 \leq \|X\|_F^2$ for all $X \in \Theta$.
We introduce a constant $c_k$ for $k = 1,\ldots,K$ which satisfying $c_k \geq \| A \times_k G_k^* \|_F / \|A\|_F$ where $A$ is a tensor with proper size.
Since we suppose that the reshaped matrix from $G_k^*$ has $R_k$ row rank, we can guarantee that $c_k$ is positive and finite.
Using $c_k$, we have
\begin{align*}
	&\|G_k^* \times_2 (\tilde{G}_{<k} - G_{<k}^*)\times_3 G_{k<}^*\|\\
	&\leq c_k \prod_{k' > k}c_{k'} \|\tilde{G}_{<k} - G_{<k}^*\|_F \\
	& \leq c_k \prod_{k' > k}c_{k'} \sum_{k' < k} \|\tilde{G}_{k'} - G_{k'}^*\|_F \prod_{\ell < k, \ell \neq k'} c_{\ell} \\
	& \leq \prod_{k' \geq k}c_{k'} (k-1) \prod_{\ell < k}c_{\ell} \Xi(\tilde{\mG}) \\
	&= (k-1) \prod_{k'=1}^K c_{k'} \Xi(\tilde{\mG}).
\end{align*}
Here, we define $C_K :=  \prod_{k'=1}^K c_{k'}$, we obtain
\begin{align}
	T_1 \leq \frac{1}{n}(k-1)C_K\Xi(\tilde{\mG}). \label{ineq:t1}
\end{align}

Similarly, we obtain
\begin{align}
	T_2 \leq \frac{1}{n}(K-k)C_K\Xi(\tilde{\mG}). \label{ineq:t2}
\end{align}

About $T_3$, we have
\begin{align*}
	T_3 &=  \frac{1}{n} \langle  \mathfrak{X}(G_k^* \times_2 (\tilde{G}_{<k} - G_{<k}^*)\times_3 (\tilde{G}_{k<} - G_{k<}^*)), \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<}) \rangle \\
	&\leq  \frac{1}{n} \|G_k^* \times_2 (\tilde{G}_{<k} - G_{<k}^*)\times_3 (\tilde{G}_{k<} - G_{k<}^*)\|_F \|\Delta_k\|_F.
\end{align*}
We evaluate the first norm as
\begin{align*}
	&\|G_k^* \times_2 (\tilde{G}_{<k} - G_{<k}^*)\times_3 (\tilde{G}_{k<} - G_{k<}^*)\|_F \\
	&\leq c_k \left( \|\tilde{G}_{<k}\times_3 (\tilde{G}_{k<} - G_{k<}^*)\|_F + \|{G}_{<k}^*\times_3 (\tilde{G}_{k<} - G_{k<}^*)\|_F \right) \\
	& \leq \frac{2}{n} (K-1) C_K \Xi(\tilde{\mG}).
\end{align*}
Then, we have
\begin{align}
	T_3 \leq \frac{2}{n}(K-1)C_K\Xi(\tilde{\mG}). \label{ineq:t3}
\end{align}



To bound $T_4$, we apply the same line of the proof of Theorem \ref{thm:convex}.
Along with Lemma 1 in \cite{negahban2011estimation}, we bound the Schatten norm of $Q_k(\tilde{\Delta}_k)$ and apply the Cauchy-Schwartz inequality, then obtain
\begin{align*}
	T_4 \leq \frac{2 \lambda_n (2+\epsilon)}{K-1} \sum_{k'=1}^{K-1} \sqrt{2R_{k'}}  \|Q_{k'}(\tilde{\Delta}_{k})\| =  \frac{2 \lambda_n (2+\epsilon)}{K-1}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}}  \|\tilde{\Delta}_{k}\|.
\end{align*}

Combining the bound with \eqref{ineq:t1}, \eqref{ineq:t2} and \eqref{ineq:t3}, we update the bound \eqref{ineq:basic1} as
\begin{align}
	\frac{1}{2n} \| \mathfrak{X}(\tilde{\Delta}_k)\|^2 &\leq \frac{3(K-1)C_K}{n}\Xi(\tilde{\mG}) \|\mathfrak{X}(\tilde{\Delta}_k)\| + \frac{2 \lambda_n (2+\epsilon)}{K-1}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}}  \|\tilde{\Delta}_k\| \notag \\
	& \leq \frac{3(K-1)C_K}{n}\Xi(\tilde{\mG}) \|\tilde{\Delta}_k\|_F + \frac{2 \lambda_n (2+\epsilon)}{K-1}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}}  \|\tilde{\Delta}_k\|  . \label{ineq:basic2}
\end{align}


\paragraph{Step 3.} Bound below the LHS of the basic inequality.



%Substitute it into \eqref{ineq:basic2}, we have
%\begin{align}
%	\frac{1}{6n} \| \mathfrak{X}(\Delta_k)\|^2 -\frac{8}{n}(K-1)^2 C_k^2 \Xi^2(\tilde{\mG}) \leq \frac{4(K-1)C_K}{n}\Xi(\tilde{\mG}) \|\mathfrak{X}(\tilde{\Delta}_k)\| + \frac{\lambda_n K}{K-1}\sum_{k'=1}^{K-1} \sqrt{R_{k'}}  \|\tilde{\Delta}_k\|. \label{ineq:basic3}
%\end{align}

We apply the matrix completion theory developed by \cite{candes2010matrix} and \cite{candes2012exact}.
Let $k' \in \{1,\ldots,K\}$ be the index satisfying Assumption \ref{asmp:incoherence_convex}.
Since the value of the $L^2$-norm and the Frobenius norm is invariant to the shape of tensors, we compare the value of $\|Q_{k'}(\Delta_k)\|_F$ and $\|\tilde{\mathfrak{X}}(Q_{k'}(\Delta_k))\|_F$ with $k'$ instead of $\|\Delta_k\|_F$ and $\|\mathfrak{X}(\Delta_k)\|_F$.

For the matrix $Q_{k'}(X^*)$, we apply Assumption \ref{asmp:incoherence_convex} and obtain that $Q_{k'}(X^*)$ has the $\mu_{k'}$-incoherence property.
%Also, we obtain that $Q_{k'}( G_k^* \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<})$ satisfies $\tilde{\mu}$-incoherent property with $\tilde{\mu} = *****$.
Then, we apply Theorem 2 and Theorem 7 in \cite{candes2010matrix}, we obtain the following inequality as
\begin{align*}
	\|Q_{k'}(\tilde{\Delta}_{k})\|_F \leq \left(  \sqrt{\frac{48  \min\{I_{\leq k'}, I_{k'<}\}}{n}}+ 1 \right) \|\mathfrak{X}(Q_{k'}(\tilde{\Delta}_{k}))\|,
\end{align*}
with probability at least $1-( \max\{I_{\leq k'}, I_{k'<}\})^{-3}$ and
\begin{align*}
	n \geq C_{m} \mu_{k'}^2  \max\{I_{\leq k'}, I_{k'<}\} R_{k'} \log^3  \max\{I_{\leq k'}, I_{k'<}\},
\end{align*}
with a constant $C_m > 0$.
Then we obtain that
\begin{align*}
	&\frac{1}{n}\|\mathfrak{X}(\tilde{\Delta}_k)\|^2 = \frac{1}{n}\|\tilde{\mathfrak{X}}(Q_{k'}(\tilde{\Delta}_k))\|^2 \\
	& \geq (144  \min\{I_{\leq k'}, I_{k'<}\} + 3n)^{-1} \|Q_{k'}(\tilde{\Delta}_k)\|_F^2 =: C_{\kappa} \|Q_{k'}(\tilde{\Delta}_k)\|_F^2 = C_{\kappa} \|\tilde{\Delta}_k\|_F^2,
\end{align*}
where $C_{\kappa} > 0$ since $n \leq \prod_k I_k$.
Using this result into \eqref{ineq:basic2}, we have
\begin{align*}
	\frac{C_{\kappa}}{6} \|\tilde{\Delta}_k\|_F^2 \leq \frac{3(K-1)C_K}{n}\Xi(\tilde{\mG}) \|\tilde{\Delta}_k\|_F + \frac{2 \lambda_n (2+\epsilon)}{K-1}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}}  \|\tilde{\Delta}_k\|_F.
\end{align*}
Then we obtain the inequality
\begin{align*}
	\frac{C_{\kappa}}{6} \|\tilde{\Delta}_k\|_F^2 \leq \frac{3(K-1)C_K}{n}\Xi(\tilde{\mG}) \|\tilde{\Delta}_k\|_F + \frac{2 \lambda_n (2+\epsilon) }{K-1}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}}  \|\tilde{\Delta}_k\|_F.
\end{align*}
We divide the both hands side by $\|\tilde{\Delta}_k\|_F$ about the first term, and consider the root about the second term, then we have
\begin{align}
	\frac{C_{\kappa}}{6} \|\tilde{\Delta}_k\|_F & \leq \frac{3(K-1)C_K}{n}  \Xi(\tilde{\mG}) + \frac{2 \lambda_n (2+\epsilon)}{K-1}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}},  \label{ineq:basic5}
\end{align}
by using the property $\|\mathfrak{X}(X)\| \leq \|X\|$ for all $X \in \mX$.


Finally, we define
\begin{align*}
	\Delta_k := (\hat{G}_k - G_k^*) \times_2 G_{<k}^* \times_3 G_{k<}^*,
\end{align*}
and compare $\Delta_k$ and $\tilde{\Delta}_k$ as
\begin{align*}
	\|\Delta_k\| \leq \|\tilde{\Delta}_k\| + \|\tilde{\Delta}_k - \Delta_k\|.
\end{align*}
We evaluate the last term by the same way of the step 2 as
\begin{align*}
	&\|\tilde{\Delta}_k - \Delta_k\|_F \\
	& \leq \left\| (\hat{G}_k - G_k^*) \times_2 (G_{<k}^* \times_3 G_{k<}^* - \tilde{G}_{<k} \times_3 \tilde{G}_{k<})\right\|_F \\
	& \leq 2 c_k \left\{\left\| (G_{<k}^* - \tilde{G}_{<k} ) \times_3 G_{k<}^*\right\|_F + \left\| \tilde{G}_{<k} \times_3 ( G_{k<}^* -  \tilde{G}_{k<})\right\|_F  \right\} \\
	& \leq 2 (K-1) C_K \Xi(\tilde{\mG}).
\end{align*}
Then, we have
\begin{align*}
	\|\Delta_k\|_F - 2 (K-1) C_k\Xi(\tilde{\mG}) \leq  \|\tilde{\Delta}_k\|_F.
\end{align*}
Substituting the result into \eqref{ineq:basic5}, we obtain
\begin{align}
	&\frac{C_{\kappa}}{6} \|\Delta_k\|_F \leq   2(K-1)C_K(1+n^{-1})  \Xi(\tilde{\mG})  + \frac{2 \lambda_n (2+\epsilon)}{K-1}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}}. \label{ineq:basic6}
\end{align}


\paragraph{Step 4.} Combining the results.


Substituting the result of the step 3 into \eqref{ineq:basic6}, we finally obtain
\begin{align*}
	&\|\hat{G}_k - G_k^*\| \leq  6 (C_{\kappa} C_K)^{-1} 2(K-1)C_K(1+n^{-1})  \Xi(\tilde{\mG})  + \frac{2 \lambda_n (2+\epsilon)}{K-1}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}}.
\end{align*}

\begin{comment}
We investigate a lower bound of $\|\mathfrak{X}(\Delta_k)\|^2$.
Preliminarily, we consider the following inequality
\begin{align*}
	&(\hat{G}_k - G_k^*) \times_2 G^*_{<k} \times_3 G^*_{k<}	 \\
	&=(\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 \tilde{G}_{k<} + (\hat{G}_k - G_k^*) \times_2 (G_{<k}^* - \tilde{G}_{<k}) \times_3 \tilde{G}_{k<} \\
	& \quad + (\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 (G_{k<}^* - \tilde{G}_{k<}) + (\hat{G}_k - G_k^*) \times_2 (G_{<k}^* - \tilde{G}_{<k})  \times_3 (G_{k<}^* - \tilde{G}_{k<}),
\end{align*}
then the linear operator $\mathfrak{X}$ satisfies the following equation
\begin{align*}
	&\mathfrak{X}((\hat{G}_k - G_k^*) \times_2 G^*_{<k} \times_3 G^*_{k<})	 \\
	&=\mathfrak{X}((\hat{G}_k - G_k^*) \times_2 (G_{<k}^* - \tilde{G}_{<k}) \times_3 \tilde{G}_{k<})  + \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 (G_{k<}^* - \tilde{G}_{k<}))\\
	& \quad + \mathfrak{X}(\Delta_k) + \mathfrak{X}((\hat{G}_k - G_k^*) \times_2 (G_{<k}^* - \tilde{G}_{<k})  \times_3 (G_{k<}^* - \tilde{G}_{k<})).
\end{align*}
Here, we define 
\begin{align*}
	\Delta_k^* := (\hat{G}_k - G_k^*) \times_2 G^*_{<k} \times_3 G^*_{k<},
\end{align*}
then the triangle inequality yields
\begin{align*}
	&\|\mathfrak{X}(\Delta_k^*)\| \leq  \|\mathfrak{X}(\Delta_k)\| \\
	& \quad + \|\mathfrak{X}((\hat{G}_k - G_k^*) \times_2 (G_{<k}^* - \tilde{G}_{<k}) \times_3 \tilde{G}_{k<})\|  + \|\mathfrak{X}((\hat{G}_k - G_k^*) \times_2 \tilde{G}_{<k} \times_3 (G_{k<}^* - \tilde{G}_{k<})) \|\\
	& \quad  + \|\mathfrak{X}((\hat{G}_k - G_k^*) \times_2 (G_{<k}^* - \tilde{G}_{<k})  \times_3 (G_{k<}^* - \tilde{G}_{k<}))\|.
\end{align*}
\end{comment}

\end{proof}

We back to the proof of Theorem \ref{thm:als}.
Based on the result of Lemma \ref{lem:als_1}, we will take two steps: (a) evaluate the distance between $X(\tilde{\mG})$ and $X(\mG^*)$, and (b) show the convergence as the ALS iteration proceeds. 


\paragraph{Step (a).} Evaluate the distance between $X(\tilde \mG)$ and $X(\mG^*)$.


For brevity, we introduce new notation for $X(\mG)$.
Using the tensor product, we denote
\begin{align*}
	X(\mG) = G_1 \times_2 G_2 \times_3 \cdots \times_{K-1} G_{K-1} \times_K G_K.
\end{align*}
Then, we evaluate the distance between $X(\mG)$ and $X(\mG^*)$ as
\begin{align*}
	&X(\tilde\mG) - X(\mG^*)\\
	&=\tilde G_1 \times_2 \cdots \times_K \tilde G_K - G_1^* \times_2 \cdots \times_K G_K^* \\
	&=(\tilde G_1 \times_2 \cdots \times_{K-1} \tilde G_{K-1}\times_K \tilde G_K - \tilde G_1 \times_2 \cdots \times_{K-1} \tilde G_{K-1} \times_K G_K^*) \\
	& \quad + (\tilde G_1 \times_2 \cdots \times_{K-1} \tilde G_{K-1} \times_K G_K^* - \tilde G_1 \times_2 \cdots \times_{K-1} G_{K-1}^* \times_K G_K^*) \\
	& \cdots \\
	& \quad +  (\tilde G_1 \times_2 G_2^* \times _3\cdots \times_{K-1} G_{K-1}^* \times_K G_K^* - G_1^* \times_2 \cdots \times_K G_K^*) \\
	&= \sum_{k=1}^{K}  \tilde G_{<k} \times_{k} (\tilde G_{k} -  G_{k}^*) \times_{k+1}   G_{k<}^*.
\end{align*}



Then, we consider the Frobenius norm as
\begin{align*}
	&\|X(\tilde\mG) - X(\mG^*)\|_F^2 \\
	&= \left\| \sum_{k=1}^{K}  \tilde G_{<k} \times_{k} (\tilde G_{k} -  G_{k}^*) \times_{k+1}   G_{k<}^* \right\|_F^2 \\
	&= \sum_{k=1}^{K} \sum_{k'=1}^{K} \left\langle  \tilde G_{<k} \times_{k} (\tilde G_{k} -  G_{k}^*) \times_{k+1}   G_{k<}^*,  \tilde G_{<k'} \times_{k'} (\tilde G_{k'} -  G_{k'}^*) \times_{k'+1}   G_{k'<}^* \right\rangle \\
	&= \sum_{k=1}^K  \left\|   \tilde G_{<k} \times_{k} (\tilde G_{k} -  G_{k}^*) \times_{k+1}   G_{k<}^* \right\|_F^2 \\
	&\quad + \sum_{k=1}^{K} \sum_{k'\neq k} \left\langle  \tilde G_{<k} \times_{k} (\tilde G_{k} -  G_{k}^*) \times_{k+1}   G_{k<}^*,  \tilde G_{<k'} \times_{k'} (\tilde G_{k'} -  G_{k'}^*) \times_{k'+1}   G_{k'<}^* \right\rangle.
\end{align*}
As same as the proof of Lemma \ref{lem:als_1}, we bound the first term as
\begin{align*}
	\left\|   \tilde G_{<k} \times_{k} (\tilde G_{k} -  G_{k}^*) \times_{k+1}  \tilde G_{k<} \right\|_F^2 \leq C_K^2\Xi^2(\tilde{\mG}).
\end{align*}
For the second term, we obtain
\begin{align*}
	&\left \langle  \tilde G_{<k} \times_{k} (\tilde G_{k} -  G_{k}^*) \times_{k+1}  \tilde G_{k<},  \tilde G_{<k'} \times_{k'} (\tilde G_{k'} -  G_{k'}^*) \times_{k'+1}  \tilde G_{k'<} \right\rangle \\
	&\leq \left\| \tilde G_{<k} \times_{k} (\tilde G_{k} -  G_{k}^*) \times_{k+1}  \tilde G_{k<}\right\|_F \left\| \tilde G_{<k'} \times_{k'} (\tilde G_{k'} -  G_{k'}^*) \times_{k'+1}  \tilde G_{k'<} \right\|_F \\
	&\leq  C_K^2\Xi^2(\tilde{\mG}).
\end{align*}
Combining the results, we obtain 
\begin{align*}
	\|X(\tilde\mG) - X(\mG^*)\|_F^2 \leq (K+K^2)C_k^2 \Xi^2(\tilde{\mG}).
\end{align*}

\paragraph{Step (b).} Show convergence with the ALS iteration.

Let $\mG^{t}$ be a set $\mG$ obtained by $t$-th ALS iteration.
By the result of the step (a), we have
\begin{align*}
	\|X(\mG^t) - X(\mG^*)\|_F^2 \leq (K+K^2)C_K^2 \Xi(\mG^t).
\end{align*}
Applying the result of Lemma \ref{lem:als_1}, let $\hat{G}_k^t$ be the minimizer of optimization of \eqref{opt:als3} with the $t$-th ALS iteration, we obtain for each $t = 1,2,\ldots$,
\begin{align*}
	&\Xi(\mG^t) = \max_{k} \|\hat{G}_k^t - G_k^*\|_F \leq 6 (C_{\kappa} C_K)^{-1} 2(K-1)C_K(1+n^{-1})  \Xi(\tilde{\mG})  + \frac{2 \lambda_n (2+\epsilon)}{K-1}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}}.
\end{align*}
The inequality holds since $\mG^{t-1}$ is the fixed $\tilde{\mG}$ for the $t$-th ALS iteration.
We define the contraction coefficient
\begin{align*}
	\chi := 12C_{\kappa}^{-1} (K-1)C_K(1.5+n^{-1}),
\end{align*}
and using the assumption that $\chi < 1$, we have
\begin{align}
	\Xi(\mG^t) \leq \max \left\{ \chi^t \Xi(\mG^0), 6 (C_{\kappa} C_K)^{-1}   \frac{2 \lambda_n (2 + \epsilon)}{K}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}} \right\}, \label{ineq:xi}
\end{align}
where $\mG^0$ is an initial value of $\mG$.
With Assumption \ref{asmp:intial}, we set $t$ sufficiently large as 
\begin{align*}
	t \geq (\log \chi)^{-1} \left\{\log \left( 6 (C_{\kappa} C_K)^{-1}  \left( \frac{2 \lambda_n (2 + \epsilon)}{K}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}}\right) \right) - \log \Xi(\mG^0)\right\},
\end{align*}
we obtain
\begin{align*}
	\|X(\mG^t) - X(\mG^*)\|_F^2 \leq 12 C_{\kappa}^{-1} K^2C_K \left( \frac{2 \lambda_n (2 + \epsilon)}{K}\sum_{k'=1}^{K-1} \sqrt{2R_{k'}} \right)^2.
\end{align*}
As we set $\hat{X} := X(\mG^t)$, we obtain the result.

\qed



\if0

\section{Technical Lemmas}

\begin{lemma}\label{lem:neg}
	When $\lambda_n \gtrsim \|\mathfrak{X}^*(\mE)\|_{s,\infty}$ is satisfied, we have
	\begin{enumerate}
		\item $\mbox{rank}(\Delta_{(k)'}) \leq 2 r_k$ for each $k = 1,\ldots, K-1$.
		\item $\sum_{k=1}^{K-1} \|\Delta_k''\|_s \leq 3 \sum_{k=1}^{K-1} \|\Delta_{(k)}'\|_s + \sum_{j > r_k} \sigma_j(X^*_{(k)})$ .
	\end{enumerate}
\end{lemma}

\begin{lemma}\label{lem:neg_als}
	When $\lambda_n \gtrsim \|\mathfrak{X}^*(\mE)\|_{s,\infty}$ is satisfied, we have
	\begin{enumerate}
		\item $\mbox{rank}(\Delta_{(k)'}) \leq 2 r_k$ for each $k = 1,\ldots, K-1$.
		\item $\sum_{k=1}^{K-1} \|\Delta_k''\|_s \leq 3 \sum_{k=1}^{K-1} \|\Delta_{(k)}'\|_s + \sum_{j > r_k} \sigma_j(X^*_{(k)})$ .
	\end{enumerate}
\end{lemma}


\bc{
\begin{lemma} \label{lem:inc_2}
	When assumption \ref{asmp:incoherence_convex} is satisfied, $\tilde{\mG}$ is $\tilde{\mu}$-incoherent with
	\begin{align*}
		\tilde{\mu} = \mu + \frac{ C_h^2 \Xi^2(\tilde{\mG})I_k}{R_k}.
	\end{align*}
\end{lemma}
\begin{proof}
Consider the following value 
\begin{align*}
		\max_{i_k,r_{k-1}} \left\{ \left\| P_{\tilde{G}_k,r_{k-1}} (e_{i_k}) \right\| \right\} &\leq \max_{i_k,r_{k-1}} \left\{\left\| P_{G_k^*,r_{k-1}} (e_{i_k}) \right\| + \left\|[\tilde{G}_k]_{i_k,r_{k-1},:} - [G_k^*]_{i_k,r_{k-1},:}\right\| \right\} \\
		&= \left\| P_{G_k^*,\overline{r}_{k-1}} (e_{\overline{i}_k}) \right\| + \left\|[\tilde{G}_k]_{\overline{i}_k,\overline{r}_{k-1},:} - [G_k^*]_{\overline{i}_k,\overline{r}_{k-1},:}\right\| ,
\end{align*}
where $\overline{i}_k$ and $\overline{r}_{k-1}$ are the maximizer.
Then, we use a constant $C_h > 0$ and continue the inequality as
\begin{align*}
		& \leq \left(\frac{ \mu  R_{k}}{I_k} \right)^{1/2} + C_h \left\|\tilde{G}_k - G_k^*\right\| \\
		& \leq  \left(\frac{ \mu  R_{k}}{I_k} \right)^{1/2} + C_h \Xi(\tilde{\mG}) \\
		& \leq  \left\{ \left(\mu + \frac{ C_h^2 \Xi^2(\tilde{\mG})I_k}{R_k}\right)\frac{   R_{k}}{I_k} \right\}^{1/2},
\end{align*}
by $\mu$-coherence of $\mG^*$ and the equivalence property of norms.
Then, we define 
\begin{align*}
	\tilde{\mu} :=\left(\mu + \frac{ C_h^2 \Xi^2(\tilde{\mG})I_k}{R_k}\right),
\end{align*}
and obtain the result.
\end{proof}
\begin{lemma} \label{lem:rsc}
	Suppose that $\Delta \in \Theta$ is a difference between a matrix $Z^* \in \R^{J_1 \times J_2}$ and its estimator $\hat{Z}$, and $Z^*$ satisfies
	\begin{align*}
		\max_{1\leq j_1 \leq J_1, 1 \leq j_2 \leq J_2}[Z^*]_{i,j} \leq \frac{\mu' \sqrt{R}}{\sqrt{J_1 J_2}}.
	\end{align*}
	Then, we have
	\begin{align*}
		\|\hat{Z} - Z^*\|_F \leq ***.
	\end{align*}
\end{lemma}
\begin{proof}
The proof is along with \cite{candes2010matrix} and others.
\end{proof}
}

\bc{
First, we fix $k'$ and state the incoherence property of $Q_{k'}(X^*)$.
Note that since $\mG^*$ is $\mu$-incoherent, we obtain
\begin{align*}
	|[[G_k^*]_{i_k} [G_{k+1}^*]_{i_{k+1}}]_{r_k,r_{k+1}}| \leq \left( \sum_{r_k = 1}^{R_k} [G_k^*]_{i_k,r_{k-1},r_k} \right)^{1/2}\left( \sum_{r_{k+1} = 1}^{R_{k+1}} [G_{k+1}^*]_{i_{k+1},r_k,r_{k+1}} \right)^{1/2} \leq \frac{\mu R_k}{\sqrt{I_k I_{k+1}}},
\end{align*}
for $k \in \{1,\ldots,K-1\}$ and any $r_{k-1}$ and $r_{k+1}$.
}

\fi
