\section{Preliminaries}

\subsection{Notation}

Let $\mX \subset \R^{I_1 \times \cdots \times I_K}$ be the space of
order-$K$ tensors, where $I_k$ denotes the dimensionality of the $k$-th
mode for $k=1,\dots,K$.  For brevity, we define
$I_{<k} := \prod_{k'<k}I_{k'}$; similarly, $I_{\leq k}, I_{k<}$ and
$I_{k \leq}$ are defined.  For a vector $Y \in \R^d$, $[Y]_i$ denotes
the $i$-th element of $Y$.  Similarly, $[X]_{i_1,\ldots,i_K}$ denotes
the $(i_1,\ldots,i_K)$ elements of a tensor $X\in\mX$. Let
$[X]_{i_1,\ldots,i_{k-1},:,i_{k+1},\ldots,i_K}$ denote an
$I_k$-dimensional vector
$(X_{i_1,\ldots,i_{k-1},j,i_{k+1},\ldots,i_K})_{j=1}^{I_k}$ called the
mode-$k$ fiber.  For a vector $Y \in \R^d$, $\|Y\| = (Y^T Y)^{1/2}$
denotes the $\ell_2$-norm and $\|Y\|_{\infty} = \max_i|[Y]_i|$ denotes
the max norm.  For tensors $X,X' \in \mX$, an inner product is defined
as
$\langle X,X' \rangle := \sum_{i_1,\ldots,i_K =1}^{I_1 \dots I_K}
X(i_1,\ldots,i_K)X'(i_1,\ldots,i_K)$
and $\|X\|_{F} = \langle X,X \rangle^{1/2}$ denotes the Frobenius
norm.  For a matrix $Z$, $\|Z\|_s := \sum_{j} \sigma_{j}(Z)$ denotes
the Schatten-1 norm, where $\sigma_j(\cdot)$ is a $j$-th singular value
of $Z$.

\subsection{Tensor Train Decomposition}

%\textit{Tensor train (TT) decomposition} is a tensor factorization
%method with a matrix product representation
%\cite{oseledets2010tt,oseledets2011tensor}.  
Let us define a tuple of positive integers $(R_1, \ldots, R_{K-1})$
and an order-$3$ tensor $G_k \in \R^{I_k \times R_{k-1} \times R_k}$
for each $k = 1,\ldots,K$.  Here, we set $R_0 = R_K = 1$.  Then, TT
decomposition represents each element of $X$ as follows:
\begin{align}
	X_{i_1,\ldots,i_K} = [G_1]_{i_1,:,:} [G_2]_{i_2,:,:} \cdots [G_K]_{i_K,:,:}. \label{eq:tt}
\end{align}
Note that $[G_k]_{i_k,:,:}$ is an $R_{k-1} \times R_k$ matrix.  We
define $\mG := \{G_k\}_{k=1}^K$ as a set of the tensors, and let $X(\mG)$
be a tensor whose elements are represented by $\mG$ as
\eqref{eq:tt}.  The tuple $(R_1, \ldots, R_{K-1})$ controls
the complexity of TT decomposition, and it is called a \textit{Tensor
  Train (TT) rank}.  Note that TT decomposition is universal, i.e.,
any tensor can be represented by TT decomposition with sufficiently
large TT rank~\cite{oseledets2010tt}.


When we evaluate the computational complexity, we assume the shape of
$\mG$ is roughly symmetric. That is, we assume there exist
$I,R\in\mathbb{N}$ such that $I_k=O(I)$ for $k=1,\dots,K$ and
$R_k=O(R)$ for $k=1,\dots,K-1$.


\subsection{Tensor Completion Problem}

Suppose there exists a true tensor $X^* \in \mX$ that is unknown, and
a part of the elements of $X^*$ is observed with some noise.  Let
$S \subset \{(j_1,j_2,
\ldots,j_K)\}_{j_1,\ldots,j_K=1}^{I_1,\ldots,I_K}$
be a set of indexes of the observed elements and
$n := |S| \leq \prod_{k=1}^K I_k$ be the number of observations.  Let
$j(i)$ be an $i$-th element of $S$ for $i=1,\ldots,n$, and $y_i$
denote $i$-th observation from $X^*$ with noise.  We consider the
following observation model:
\begin{align}
	y_i = [X^*]_{j(i)} + \epsilon_i, \label{model:obs}
\end{align}
where $\epsilon_i$ is i.i.d. noise with zero mean and variance
$\sigma^2$.  For simplicity, we introduce  observation vector
$Y := (y_1, \ldots, y_n)$, noise vector
$\mE := (\epsilon_1, \ldots , \epsilon_n)$, and rearranging operator
$\mathfrak{X} : \mX \to \mathbb{R}^n$ that randomly picks the elements of $X$.
%  $[\mathfrak{X}(X)]_i = [X]_{j(i)}$.
Then, the model \eqref{model:obs} is rewritten as follows:
\begin{align*}
	Y = \mathfrak{X}(X^*) + \mE.
\end{align*}

%%%
The goal of tensor completion is to estimate the true tensor $X^*$
from the observation vector $Y$.  Because the estimation problem is
ill-posed, we need to restrict the degree of freedom of $X^*$, such as
rank. Because the direct optimization of rank is difficult, its convex
surrogation is alternatively
used~\cite{candes2012exact,candes2010matrix, krishnamurthy2013low,
  zhang2016exact, phien2016efficient}.  For tensor
completion, the convex surrogation yields the following optimization
problem
\cite{gandy2011tensor,liu2013tensor,signoretto2011tensor,tomioka2010estimation}:
\begin{align}
	\min_{X \in \Theta} \left[ \frac{1}{2n} \|Y - \mathfrak{X}(X)\|^2 + \lambda_n \|X\|_{s^*} \right], \label{opt:general}
\end{align}
where $\Theta \subset \mX$ is a convex subset of $\mX$, 
%and
%$\Omega : \Theta \to \R_+$ is a regularization for tensors, 
$\lambda_n\geq 0$ is a regularization coefficient, and
$ \|\cdot\|_{s^*}$ is the overlapped Schatten norm defined as
$ \|X\|_{s^*} := \frac{1}{K} \sum_{k=1}^K \|\tilde{X}_{(k)}\|_s$.
Here, $\tilde{X}_{(k)}$ is the $k$-unfolding matrix defined by
concatenating the mode-$k$ fibers of $X$.  The overlapped Schatten
norm regularizes the rank of $X$ in terms of Tucker
decomposition~\cite{negahban2011estimation, tomioka2011statistical}.
Although the Tucker rank of $X^*$ is unknown in general, the convex
optimization adjusts the rank depending on $\lambda_n$.

To solve the convex problem~\eqref{opt:general}, the ADMM algorithm is often
employed~\cite{boyd2011distributed,tomioka2010estimation,
  tomioka2011statistical}.  Since the overlapped Schatten norm is not
differentiable, the ADMM algorithm avoids the differentiation of the
regularization term by alternatively minimizing the augmented
Lagrangian function iteratively.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "TTcomp_NIPS2017.tex"
%%% End:
