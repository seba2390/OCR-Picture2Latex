\section{Introduction}

Tensor decomposition is an essential tool for dealing with data
represented as multidimensional arrays, or simply, tensors. Through
tensor decomposition, we can determine latent factors of an input
tensor in a low-dimensional multilinear space, which saves the storage
cost and enables predicting missing elements. Note that, a different
multilinear interaction among latent factors defines a different
tensor decomposition model, which yields a ton of variations of tensor
decomposition. For general purposes, however, either Tucker
decomposition~\cite{tucker1966some} or \emph{CANDECOMP/PARAFAC (CP)
  decomposition}~\cite{harshman1970foundations} model is commonly
used.

In the past three years, an alternative tensor decomposition model,
called \emph{tensor train (TT)}
decomposition~\cite{oseledets2011tensor} has actively been studied in
machine learning communities for such as approximating the inference
on a Markov random field~\cite{novikov2014putting}, modeling
supervised learning~\cite{novikov2016exponential, NIPS2016_6211},
analyzing restricted Boltzmann machine~\cite{chen2017equivalence}, and
compressing deep neural networks~\cite{novikov2015tensorizing}.
%~\cite{novikov2014putting,novikov2015tensorizing,novikov2016exponential,NIPS2016_6211,chen2017equivalence}.
A key property is that, for higher-order tensors, TT decomposition
provides more space-saving representation called TT format while
preserving the representation power. Given an order-$K$ tensor (i.e., a
$K$-dimensional tensor), the space complexity of Tucker decomposition
is exponential in $K$, whereas that of TT decomposition is linear
in $K$. Further, on TT format, several mathematical
operations including the basic linear algebra operations can be performed
efficiently~\cite{oseledets2011tensor}.
%TT
%decomposition is now actively used in machine learning context such as
%modeling supervised
%learning~\cite{novikov2016exponential,NIPS2016_6211}, analyzing
%restricted Bolzmann machine~\cite{chen2017equivalence}, and deep
%neural networks~\cite{novikov2015tensorizing}.


%
%To compute TT decomposition, several optimization methods have been
%proposed. For example, tensor compression (i.e. tensor elements are
%fully observed) via iterative singular value decomposition
%(SVD)~\cite{oseledets2010tt} and tensor completion (i.e. tensor
%elements are partially missing) via alternating
%minimization~\cite{grasedyck2015alternating,wang2016tensor} or convex
%formulation~\cite{phien2016efficient}. \knote{Edit after confirming the time complexity of our methods}

Despite its potential importance, we face two crucial limitations when
applying this decomposition to a much wider class of machine learning problems.
%
First, its statistical performance is unknown. In Tucker decomposition
and its variants, many authors addressed the generalization error and
derived statistical bounds
(e.g. \cite{tomioka2011statistical,tomioka2013convex}). For example,
Tomioka \etal \cite{tomioka2011statistical} clarify the way in which using the convex
relaxation of Tucker decomposition, the generalization error is
affected by the rank (i.e., the dimensionalities of latent factors),
 dimension of an input, and number of observed elements. In
contrast, such a relationship has not been studied for TT decomposition
yet.
%
Second, standard TT decomposition algorithms, such as alternating
least squares (ALS)~\cite{grasedyck2015alternating,wang2016tensor} ,
require a huge computational cost. The main bottleneck arises from the
singular value decomposition (SVD) operation to an ``unfolding''
matrix, which is reshaped from the input tensor. The size of the
unfolding matrix is huge and the computational cost grows
exponentially in $K$. 

%Second, there are many rank parameters that are not easy to
%determine. This could be done by using a regularizer that surrogates
%the sum of ranks (e.g. \cite{tomioka2011statistical}). However, this
%convex relaxation incurs further computational cost, which is
%unrealistic to compute \knote{Show actual computational cost}.

In this paper, we tackle the above issues and present a scalable yet
statistically-guaranteed TT decomposition method. We first introduce a
convex relaxation of the TT decomposition problem and its optimization
algorithm via the alternating direction method of multipliers (ADMM).
Based on this, a statistical error bound for tensor completion is
derived, which achieves the same statistical efficiency as the convex
version of Tucker decomposition does. Next, because the ADMM algorithm
is not sufficiently scalable, we develop an alternative method by using a
randomization technique. At the expense of losing the global
convergence property, the dependency of $K$ on the time complexity is
reduced from exponential to quadratic. In addition, we show that a
similar error bound is still guaranteed.
%
In experiments, we numerically confirm the derived bounds and
empirically demonstrate the performance of our method using a real
higher-order tensor.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "TTcomp_NIPS2017.tex"
%%% End: