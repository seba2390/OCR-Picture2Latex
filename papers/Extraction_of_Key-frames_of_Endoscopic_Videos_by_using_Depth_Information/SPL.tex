
%% bare_jrnl.tex
% V1.4b
% 2015/08/26
% by Michael Shell
% see yrjfgjhttp://www.michaelshell.org/
% for current contact information.
%
% This is a skeleton file demonstrating the use of IEEEtran.cls
% (requires IEEEtran.cls version 1.8b or later) with an IEEE
% journal paper.
%
% Support sites:
% http://www.michaelshell.org/tex/ieeetran/
% http://www.ctan.org/pkg/ieeetran
% and
% http://www.ieee.org/

%%*************************************************************************
% Legal Notice:
% This code is offered as-is without any warranty either expressed or
% implied; without even the implied warranty of MERCHANTABILITY or
% FITNESS FOR A PARTICULAR PURPOSE! 
% User assumes all risk.
% In no event shall the IEEE or any contributor to this code be liable for
% any damages or losses, including, but not limited to, incidental,
% consequential, or any other damages, resulting from the use or misuse
% of any information contained here.
%
% All comments are the opinions of their respective authors and are not
% necessarily endorsed by the IEEE.
%
% This work is distributed under the LaTeX Project Public License (LPPL)
% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
% distributed and modified. A copy of the LPPL, version 1.3, is included
% in the base LaTeX documentation of all distributions of LaTeX released
% 2003/12/01 or later.
% Retain all contribution notices and credits.
% ** Modified files should be clearly indicated as such, including  **
% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  \usepackage[hidelinks]{hyperref}
  \usepackage{subfigure}
  \usepackage{hyperref}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{balance}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Extraction of Key-frames of Endoscopic Videos by using Depth Information}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Pradipta Sasmal,\IEEEmembership{}
Avinash~Paul,\IEEEmembership{}
M.K.~Bhuyan,\IEEEmembership{~Senior Member,~IEEE}, and Yuji Iwahori
       % <-this % stops a space
\thanks{P. Sasmal, Avinash Paul
       and~M.K.~Bhuyan are with the Department
of Electronics and Electrical Engineering, Indian Institute of Technology Guwahati, India.Yuji Iwahori is with Department of Computer Science, Chubu University, Kasugai, Japan.
*Author 1 and Author 2 have equal contribution.
E-mails: (s.pradipta, paul18, and mkb)@iitg.ac.in, iwahori@isc.chubu.ac.jp.}% <-this % stops a space
%\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
%\thanks{Manuscript received April 19, 2005; revised August 26, 2015.}
}

% note the % following the last \IEEEmembership and also \thanks - 
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
% 
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
\markboth{IEEE Signal Processing Letters}%
{Sasmal  \\MakeLowercase{\textit{et al.}}: Extraction of Key-frames of Endoscopic Videos by using Depth Information}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
% 
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.




% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
%Monocular Depth Estimation(MDE) is the present area of research in 3D reconstruction. Finding depth from a single image is an unconstrained problem given many scenes in 3D world can have the same projected image. The shift is therefore towards learning methods to solve this problem. Attempts have been made to solve it as a per pixel regression problem, however, supervised learning methods require a lot of training data. It is difficult to acquire depth data without using stereo cameras or expensive depth sensors as is the case with endoscopy videos. Thus unsupervised methods are being given more importance.
%Depth estimation in endoscopic video frames imparts clinical relevance to a physician. 3D reconstruction of the monocular images helps in diagnosis, and surgical planning. 
A deep learning-based monocular depth estimation (MDE) technique is proposed for selection of most informative frames (key frames) of an endoscopic video. In most of the cases, ground truth depth maps of polyps are not readily available and that is why the transfer learning approach is adopted in our method. An endoscopic modalities generally capture thousands of frames. In this scenario, it is quite important to discard low-quality and clinically irrelevant frames of an endoscopic video while the most informative frames should be retained for clinical diagnosis. In this view, a key-frame selection strategy is proposed by utilizing the depth information of polyps.
% 
%Wireless Capsule Endoscopy(WCE) is the new breakthrough in non-invasive diagnostic methods. However, WCE takes a lot of time and contains thousands of frames. It thus becomes important for a framework to summarize the video sequences to reduce intervention time for the physician. Here we have extended MDE to the endoscopy domain and introduced a key-frame selection method that is based on depth information. Depth information in video summarization  has been used for the first time, to the best of our knowledge.
% 
In our method, image moment, edge magnitude, and key-points are considered for adaptively selecting the key frames. One important application of our proposed method could be the 3D reconstruction of polyps with the help of extracted key frames. %Adaptive thresholding is used to combine the above measures to select the best frames in a sequence. This not only excludes redundancies in endoscopy frames but also is robust to occlusions.
%In most of the cases ground truth depth maps of polyps are not readily available and that is why the transfer learning approach is adopted in this method.
%% The deep network used in our model is trained on various 3D data sets for zero-shot learning. 
%Finally, depth maps are used to reconstruct the 3D surface of a polyp region. It gives a surgeon a real-time 3D view of the polyp surface for resection which involves detaching the polyp from its mucosa layer. 
Also, polyps are localized with the help of extracted depth maps.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Polyps,
Monocular depth,  Key-frames.
\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}

%Getting a 3D view of the environment greatly helps in scene understanding and navigation. Estimating depth is something that comes to us naturally. This is due to our stereo vision and our prior learning of the depth of the world around us. Finding depth is however, a difficult problem computationally.% Computers, however, find it computationally complex estimating depths. This owes to large scale training data required for the same.
% 
%
%Finding depth from a single image or Monocular Depth Estimation is an ill-posed problem given a number of real world scenes can have the same projected image. MDE becomes especially relevant when we don't have access to stereo images or expensive sensors for predicting depth. This is true especially for endoscopic images given their monocular nature and difficulty in finding correspondence in images containing polyp information. 
%
%Traditionally, MDE required a lot of ground truth depth data for proper training. This can be considered a regression problem per-pixel basis. However, the dataset we had had no CT ground truth available making supervised learning not feasible. Thus, we concentrated our efforts on unsupervised methods for finding depth. Our dataset has convex polyps which have depth information in them. 
%
%There are a few works on MDE using unsupervised learning. However, even the state of the art in MDE didn’t give proper results in the endoscopy dataset. This is because most of the datasets are trained outdoors in city roads, for example, the KITTI and Cityscapes datasets that are most commonly used for training. Thus, we went for a zero-shot learning approach for depth estimation. 
%
%Wireless Capsule Endoscopy(WCE) is the new advancement in the medical world. It is non-invasive and painless compared to traditional methods. However, with this ease of operation comes the challenge of having no control over the footage obtained through this method. Moreover, as WCE moves through the help of the peristalsis movement, it is hard to control the motion and orientation of the camera. Here comes the need to obtain only the essential frames called key-frames that are required for further analysis. 
%
%We went for a three criteria method to eliminate the redundant frames. First, we used Hue’s image moments which give shape and texture feature similarities between pairs of images. We found the moment distance between consecutive frames thus, eliminating the frames that had fewer variations from adjacent ones. Second, it was based on our visual observation of the frames with good depth that they had well-defined edges and higher edge magnitudes than those having poor depth information. Thus, we found the edge magnitudes of each frame and selected ones with strong edges as our candidates for key-frames. 
%
%We went for a third criterion to make our model robust to occlusions. In the third criterion, we considered no of ORB features as a criterion for discarding the frames that had occlusions or had no polyp in the frame. Also, the less computation time for ORB compared to SIFT and SURF made it a clear choice. 3D reconstruction was better when the polyp is near the center of the frame. ORB comes in handy here due to its selective importance to the central region of the image, unlike the other two methods. 
%
%We fused the three criteria by taking adaptive weights for each. We considered variance in each of the variables to assign weights. The variable with more variation in terms of first difference gets more weight in being selected as the criteria for finding key-frames. Thus our method successfully handles varying nature of images seen in endoscopic videos. 
%
%The obtained key-frames clearly showed variations encountered in the video clearly serving their purpose of video summarization. 
%The images along with their corresponding depth maps can be used for 3d reconstruction using several GUIs like Facebook’s 3D image viewer which relies solely on depth information to output a 3D mesh of the input image. 
%
%3D modeling of the polyp surface can help in resection operations. This is because the shape and boundary of the polyp along with its size can be accurately calibrated and shown in real-time to make it easier for the physician to perform the surgery. 
%
%%% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps.
% 
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% 
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by the IEEE):
% \IEEEPARstart{A}{}demo file is ....
% 
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% 
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.
%\IEEEPARstart{T}{his} demo file is intended to serve as a ``starter file''
%for IEEE journal papers produced under \LaTeX\ using
%IEEEtran.cls version 1.8b and later.
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)
%I wish you the best of success.
%
%\hfill mds
% 
%\hfill August 26, 2015
%
%\subsection{Subsection Heading Here}
%Subsection text here.
%
%% needed in second column of first page if using \IEEEpubid
%\IEEEpubidadjcol
%\section{Literature review}
%Recently, depth estimation, especially Monocular depth estimation(MDE) has attained higher significance given its importance to scene understanding, autonomous driving and 3D reconstruction. Finding depth from a single RGB image is an unconstrained problem, since a large number of real-world scenes can give the same 2D image, thus the same depth map. We humans perceive depth from cues such as perspective, prior knowledge of sizes of objects, or occlusion. Several methods both supervised and unsupervised have been employed for estimating depth.
%
%Learning methods have given great results in MDE since their inception. The earliest ones being that by Eigen et al. \cite{eigen2014depth} 
%They have introduced a  multi-scale information approach that takes care of both the global scene structure and local neighboring pixel information. A scale-invariant loss has been used for MDE. Similarly, Xu et al. \cite{ricci2018monocular} 
%formulated MDE as a Continuous Random Field problem(CRF). The authors fused the multi-scale estimation computed from the inner semantic layers of CNN with a CRF framework.
%
%
%Instead of finding continuous depth maps, Fu et al.  \cite{fu2018deep} have estimated depth using an ordinal regression approach. They introduced a space-increasing discretization method by allowing larger depths to have a lesser influence on the depth maps than the depths which are perceived closer during training. Supervised methods, however, require a lot of ground truth depth information, which is difficult to acquire due to the high cost of depth sensors or the unfeasibility of using stereo cameras in all scenes.
%
%This leads us to unsupervised or semi-supervised methods of MDE. Garg et al. \cite{garg2016unsupervised} used binocular stereo image pairs for training CNNs and then minimised a loss function formed by the wrapping of the left view image into its right stereo pair. Godard et al. \cite{godard2017unsupervised} improved upon this by using the left-right consistency criterion and trained CNN on stereo images and output the disparity to estimate depth. The authors introduced a new CNN architecture that computes end-to-end MDE. This is trained with a better reconstruction loss function.
%
%However, even the state of the art in unsupervised MDE methods, i.e. Monodepth \cite{godard2017unsupervised} model, has limited application in in-vivo images like that of endoscopic datasets. This because most models leverage outdoor scenes \cite{geigerwe} for training and a few indoor scenes \cite{saxena2007learning}, and use high end sensors or stereo cameras unlike the WCE method which captures monocular images. Thus, we needed a method that could be used for MDE in diverse environments even in those which lack ground truth depth data at scale like ours. This paves way for a zero-shot learning approach like that used by lasinger et al. \cite{lasinger2019towards}.
%
%WCE is a long process that takes nearly 8 hours capturing close to 50000 frames. This data is bulky and a large part of it is redundant frames that need to be removed.%\cite{lee2013reducing} 
% Some methods employ segmentation and localization on endoscopy frames. Li et al. \cite{li2012comparison} have worked on tumor detection by using texture features in endoscopy videos. Additionally, Tjoa et al. \cite{tjoa2002automated} used color information by selecting chromatic features. These methods are however anomaly specific.
%
%Recent work is on summarizing videos instead of detecting anomalies like bleeding or ulceration \cite{li2010wireless}. Iakovidis et al. \cite{iakovidis2010reduction} used clustering based methods for video summarization. Researchers are working on visual attention models and to building on it using saliency maps for finding key-frames from videos \cite{hua2005generic}. To the best of our knowledge, our method is the first to find key-frames using depth information to be used for 3D reconstruction.
%
%A framework for selecting key-points in an image, especially in the endoscopy domain requires special attention. In these lines, Oriented FAST and Rotated BRIEF(ORB) \cite{rublee2011orb} is used for key-point finding, in place of SIFT or SURF. ORB is particularly suited for its low time complexity(15 times faster than SURF and 300 times compared to SIFT). ORB handles noise better than SURF or SIFT.It is rotation invariant and but not scale invariant which is however, not required in frames of the same endoscopy video.
%
%The depth images obtained from MDE needed to be viewed in a framework for 3D reconstruction. Facebook's 3D image viewer was used for this purpose. Moreover, this gives an AR view of the polyp surface which can be used for finding size of the polyps. This could help in polyp removal surgeries which require the physician to view the polyp root to be able to properly dissect the polyp from its root.

Wireless Capsule Endoscopy (WCE) is a non-invasive modality to monitor the conditions of the internal viscera of a human body. WCE moves along the gastro-intestinal (GI) tract to capture images. It is extensively used to detect polyps in colon regions, which become cancerous if left untreated. Colorectal cancer is the third most prevalent cancer today \cite{siegel2017colorectal}. The capsule moves under the peristalsis movement, and it is very difficult to control the motion and orientation of the camera. Thus, redundant and clinically non-significant frames are generally obtained in a video sequence. 
WCE takes nearly 8 hours, capturing close to 50000 frames.
A large part of the data is clinically not significant and needs to be removed \cite{lee2013reducing}.

Several methods have been proposed for detection and localization of polyps in endoscopy frame \cite{li2012comparison}\cite{tjoa2002automated}.
A recent work focusing on video summarization instead of anomalies detection like bleeding or ulceration is proposed by Li \textit{et al.} \cite{li2010wireless}. Iakovidis \textit{et al.} \cite{iakovidis2010reduction} used clustering-based methods for video summarization. Researchers are working on visual attention models, like saliency maps for finding key-frames of videos \cite{hua2005generic}. 
%In our method, the key-frames are detected using depth information.
Malignant polyps usually have a convex shape and are more textured compared to benign polyps. Getting a 3D view of the polyp surface can greatly help in resection \cite{law2016endoscopic}. A good 3D reconstruction of an object in an image entails dense depth estimation. The 3D view gives shape and size information of a polyp. Depth estimation of endoscopic images is a challenging task as the endoscopic images are monocular. 
%%%%%%%%%%%%%%spl%%%%%%%%%%%%%
%Recently, depth estimation, especially monocular depth estimation (MDE) has gained high research interest. 
%%This is due to its application in scene understanding, robotics, autonomous driving and Augmented Reality (AR). 
%Finding depth from a single image is an unconstrained problem since a large number of real-world scenes can give the same 2D image, resulting in the same depth maps.
%% Humans perceive depth from cues such as perspective, prior knowledge of sizes of objects, or occlusion.
%In literature, both supervised and unsupervised based methods have been employed for estimating depth.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Eigen \textit{et al.}, \cite{eigen2014depth} introduced a  multi-scale information approach which takes care of both global scene structure and local neighboring pixel information. A scale-invariant loss is used for MDE. Similarly, Xu \textit{et al.} \cite{ricci2018monocular} formulated MDE as a continuous random field problem (CRF). They fused the multi-scale estimation computed from the inner semantic layers of a CNN with a CRF framework. Instead of finding continuous depth maps, Fu et al.  \cite{fu2018deep} estimated depth using an ordinal regression approach.
% A space-increasing discretization method is introduced by allowing objects at larger depths to have a lesser influence on the depth maps than the objects nearer to the observer.

Depth is generally obtained using sensors like LIDAR, Kinect, or by using stereo cameras. Sensors are expensive and stereo cameras are not generally used in endoscopy due to several restrictions. Obtaining ground-truth training data for depth estimation is very difficult in endoscopic imaging, and so, supervised methods are not feasible for endoscopic image classification. Finding correspondence between two images for 3D reconstruction is also difficult in endoscopy videos. It is quite difficult to find corresponding features across the frames.

Hence, unsupervised and semi-supervised methods are employed for MDE. Garg et al. \cite{garg2016unsupervised} used binocular stereo image pairs for the training of CNNs and then minimized a loss function formed by the wrapping of the left view image into its right of the stereo pair. Godard et al. \cite{godard2017unsupervised} improved this method by using the left-right consistency criterion. 
They trained CNNs on stereo images but used a single image for inference. They introduced a new CNN architecture that computes end-to-end MDE. The network was trained with an efficient reconstruction loss function.
The state-of-the-art unsupervised MDE method, i.e., Monodepth \cite{godard2017unsupervised} model has limited application in in-vivo images like endoscopic images. This is due to the fact that most models leverage outdoor scenes \cite{geigerwe} and a few indoor scenes \cite{saxena2007learning} for training, and they use high-end sensors or stereo cameras, while the WCE method only captures monocular images. Hence, it is important to devise a method that can perform MDE in medical imaging datasets which generally do not have ground truth depth information. That is why, a transfer learning approach is adopted in our method for estimating depth. Transfer learning refers to a learning method where what has been learned in one setting is exploited to improve generalization in another setting \cite{goodfellow2016deep}. Zero-shot learning is the extreme case of transfer learning where no labeled examples are present. In our method, a zero-shot learning approach for MDE \cite{lasinger2019towards} is employed.

The proposed method consists of two main steps. The first step focuses on depth estimation, and the second step extracts key-frames. As mentioned above, a zero-shot learning approach is adopted for depth estimation in endoscopic videos. We propose a framework to select the most informative frames of an endoscopic video sequence. Our method employs a three criteria approach to identify the key-frames. Subsequently, these key-frames can be used for 3D reconstruction. Our method is unique in a sense that it considers depth information to find key-frames. Finally, any of the selected key-frames can then be used for 3D reconstruction using a GUI. Experimental results clearly demonstrate the effectiveness of our method in selecting the key-frames, and subsequent polyp visualization.



\section{Proposed method}
\subsection{Depth estimation}
Due to insufficient ground truth depth data in endoscopy video datasets, a transfer learning approach is adopted for MDE in our proposed method. Lasinger et al. \cite{lasinger2019towards} proposed a zero-shot learning for depth estimation. 
%%%%%%%%%%spl
%Most of the methods for MDE are trained on outdoor environments or limited indoor scenes. These models thus show a large bias to the scenes on which they are trained.
%%%%%%%%%%%%%%
We used a pre-trained model trained on diverse datasets by Lasinger et al. \cite{lasinger2019towards} in our work. The model was trained for depth maps obtained in three different ways. First, the dataset contains depth maps obtained using LIDAR sensors. This method gives depth maps of high quality. Second, the Structure from Motion (SfM) approach is employed to estimate the depth. The third method of getting depth information from stereo images of 3D movies dataset. It uses optical flow to find motion vectors from each of the stereo images. Then, the left-right image disparity is used to find a depth map. 

%The dataset contains images that have varying aspect ratios. Sometimes, black bars on frame borders are appeared in estimated depth maps. So, all the images are cropped to extract only the center portion of the frame. This ensures images of varying aspect ratios can be handled by the framework. Moreover, the method focuses more on the central part of the image frame.
%
%Using distance of an object from the camera as a measure to predict depth leads to sparse 3D reconstructions. This is because, depth is estimated by tracking the corresponding features over a series of frames. Then, the induced parallax is used for triangulation and depth estimation. However, for distant features (like sky) the resultant parallax will be small and won't allow proper reconstruction. Using inverse depth converts all distance objects having very high depth values to inverse depth values near to zero. Thus, distant objects like sky are not considered while estimating depth. This addresses the issue of finding correspondences for distant objects.

%The disparity map is found by using stereo matching using optical flow. Optical flow successfully handles moderate displacements. The horizontal component of the flow vectors is used as a reference for finding a disparity map. Optical flow is estimated taking either the left or right image as a reference and finding flow from the other. Next, the consistency between both left and right is calculated to discard the pixels with more than one-pixel disparity.
\begin{figure}
\centering     %%% not \center
\includegraphics[width=0.97\linewidth]{./IMAGES/Framework1.png}
\caption{Proposed method of finding key-frames}
\label{process}
\end{figure}

%The datasets on which the model is trained is unique in a sense that it contains both positive and negative disparities. However, training on ground truth data from different sources has a number of constraints. First, the dataset contains images that have only depth (from LIDAR sensors) or disparity images. Second, data obtained from the SfM technique gives depth images for which scale is not known. Third, the 3D movies dataset gives a ground truth depth which has an unknown shift.

\textbf{Pre-trained network architecture}. A ResNet- based architecture as proposed by Xian et al. \cite{xian2018monocular} is used for depth estimation. Adam optimizer is used with a learning rate of    
$ 10^{-4} $ for layers which are randomly initiated and $ 10^{-5} $ for layers initialized with pre-trained weights. Decay rates for the optimizer are set at $\beta_{1}=.9$ and $ \beta_{2}=.999 $, training uses a batch size of 8. Due to different image aspect ratios, images are cropped and augmented for training.

\textbf{Loss function}. A shift and scale invariant loss function is chosen to address the problems pertaining to training on three different datasets. Let $\mathbf{d}$ $\in$  $\mathbb{R}^\mathit{N}$ be the computed inverse depth and $\mathbf{d'}$ $ \in $ $\mathbb{R}^\mathit{N}$ be ground truth inverse depth, where $\mathit{N}$ is the number of pixels in a frame. Here $s$ and $t$ represent scale and shift, respectively and they are positive real numbers. This can be represented in a vector form by taking $\vec{\mathbf{d}}_{i}$=$(\mathbf{d}_{i},1)^\intercal$ and $\mathbf{p}$=$(s,t)^\intercal$ and thus the loss function becomes: 
%\begin{equation}
%\begin{aligned}
%\mathcal{L} = \underset{s,t}{\text{min}} && \dfrac{1}{2n} \sum_{i=1}^{n}(s\mathbf{d}_{i} + t - %\mathbf{d'}_{i})^2
%\end{aligned}
%\end{equation}

%This can be represented in vector form by taking $\vec{\mathbf{d}_{i}}$=$(\mathbf{d}_{i},1)^%\intercal$ and $\mathbf{p}$=$(s,t)^\intercal$ and thus the loss function becomes,

\begin{equation}\label{eq:a}
\begin{aligned}
\mathcal{L}(\mathbf{d}_{i},\mathbf{d'}_{i}) = \underset{\mathbf{p}}{\text{min}} && \dfrac{1}{2\mathit{N}} \sum_{i=1}^{\mathit{N}}({\vec{\mathbf{d}}_{i}}^\intercal\mathbf{p}-\mathbf{d'}_{i})^2
\end{aligned}
\end{equation}

The closed-form solution is given as:

\begin{equation}
\begin{aligned}
 \mathbf{p}^{opt}= (\sum_{i=1}^{\mathit{N}}\vec{\mathbf{d}}_{i}{\vec{\mathbf{d}}_{i}}^\intercal)^{-1}(\sum_{i=1}^{\mathit{N}}\vec{\mathbf{d}}_{i}\mathbf{d'}_{i})
\end{aligned}
\end{equation}

Substituting  $\mathbf{p}^{opt}$ into ($\ref{eq:a}$) we get:

\begin{equation}\label{eq:2}
\begin{aligned}
\mathcal{L} (\mathbf{d}_{i},\mathbf{d'}_{i}) = \underset{\mathbf{p}}{\text{min}} && \dfrac{1}{2\mathit{N}} \sum_{i=1}^{\mathit{N}}({\vec{\mathbf{d}}_{i}}^\intercal\mathbf{p}^{opt}-\mathbf{d'}_{i})^2
\end{aligned}
\end{equation}

\textbf{Regularization term}. A multi-scale scale-invariant regularization term is used which does gradient matching to the depth inverse space. This biases discontinuities to be sharp and coincide with ground truth discontinuities. 
The regularization term can be defined as,

\begin{equation}
\mathcal{L}_{r} (\mathbf{d}_{i},\mathbf{d'}_{i}) = \dfrac{1}{\mathit{N}}\sum_{j=1}^{k}\sum_{i=1}^{\mathit{N}}(|\Delta_{x} {Q_{i}}^k|+|\Delta_{y} {Q_{i}}^k|)
\end{equation}

where,
\begin{equation}
Q_{i}={\vec{\mathbf{d}}_{i}}^\intercal\mathbf{p}^{opt}-\mathbf{d'}_{i}
\end{equation}

Here $Q^{k}$ gives the difference of inverse depth maps at a scale $k$. Also, the scale is applied before finding $x$ and $y$ gradients.

\textbf{Modified loss function}. The final loss function for a training set of size $M$, taking into consideration of the regularization term becomes:

\begin{equation}
\mathcal{L}_{final} = \dfrac{1}{M}\sum_{i=1}^{M}\mathcal{L} (\mathbf{d}^{i},(\mathbf{d'})^{i})+ \alpha \mathcal{L}_{r} (\mathbf{d}^{i},(\mathbf{d'})^{i})
\end{equation}
Here $ \alpha  $ is taken as 0.5.


\subsection{Selection of key-frames}
During the colonoscopy, not all the captured frames are clinically significant. Most of the frames may have redundant information, or may not be useful from a diagnostic perspective. Such frames need to be discarded and the clinically informative frames need to be retained. 
It is also strenuous and computationally intensive for a physician to investigate each frame of a video sequence. 
Thus, we propose a key-frame selection technique. Subsequently, 3D reconstruction is done to perform further analysis of the polyps. The key-frame selection method is given in Fig.~\ref{process}.

\textbf{Colour space conversion}. Our dataset contains images which are in RGB color space. Taking clues from the human visual system which works on saliency, we changed the color space from RGB to COC which gives a better perception in the medical imaging \cite{engel1997colour}.
%Three channels in the RGB colorspace are converted into four channels in the COC colourspace. These are used to calculate two opposite colour pairs i.e. red-green and blue-yellow. Let R, G, B indicate the respective components of the image, and, RG and BY the opposite colour pairs, then the color pairs are found from modified channels as follows:
%
%
%\begin{gather}
%R'=R-(G + B)/2\\
%G'=G-(R + B)/2\\
%B'=B-(R + G)/2\\
%Y'=(R + G)/2-|R-G|/2-B
%\end{gather}
%\begin{gather}
%RG= R'-G'\\
%BY= B'-Y'
%\end{gather}
%The final image $F$ is obtained by combining an intensity channel $I$ with the red-green and blue-yellow channels as follows:
%\begin{gather}
%I=\dfrac{R+G+B}{3}\\
%F=I+RG+BY
%\end{gather}


The image is subsequently used to find key-frames. A frame should satisfy three criteria before being selected as a key-frame. Firstly, it should be significantly different from neighboring frames. Second, the key-frame should give significant depth information of a polyp. Third, the polyp should not be occluded in the key-frame. We ensured that the above requirements are met, and they are formulated as follows:

\textbf{Image moment}: Image moments give the information of the shape of a region along with its boundaries and texture. Hu moments \cite{hu1962visual} are considered as they are invariant to affine transformation, and moment distances of consecutive frames are used to identify the redundant frames of a video.  
%\begin{gather}
%I_1=m_{20}+m_{02}\\
%I_{2}=(m_{20}-m_{02})^2+4m_{11}^2\\
%I_{3}=(m_{30}-3m_{12})^2+(3m_{21}-3m_{03})^2\\
%I_{4}=(m_{30}+m_{12})^2+(m_{21}+m_{03})^2
%\end{gather}
%\begin{multline}
%I_{5}=(m_{30}-3m_{12})(m_{30}+m_{12})\\
%[(m_{30}+m_{12})^2-3(m_{21}+m_{03})^2]\\
%+(3m_{21}-m_{03})(m_{21}+m_{03})\\
%[3(m_{30}+m_{12})^2-(m_{21}+m_{03})^2]
%\end{multline}
%\begin{multline}
%I_{6}=(m_{20}-m_{02})[(m_{30}+m_{12})^2-(m_{21}\\+m_{03})^2]+4m_{11}(m_{30}+m_{12})(m_{21}+m_{03})
%\end{multline}
%\begin{multline}
%I_{7}=(3m_{30}-m_{12})(m_{30}+m_{12})\\
%[(m_{30}+m_{12})^2-3(m_{21}+m_{03})^2]\\
%+(m_{21}-3m_{03})(m_{21}+m_{03})\\
%[3(m_{30}+m_{12})^2-(m_{21}+m_{03})^2]
%\end{multline}
%Subsequently, the moment difference between consecutive frames are calculated. 
The frames with a higher moment distance will be considered as a key frame. The moment distance $d$ between two images is calculated as:

\begin{equation}
d=\sum_{i=1}^{i=7}{(I_{i}-I'_{i})^2}
\end{equation}

%\begin{figure}[t]
%\centering     %%% not \center
%\subfigure[]{\label{fig:c1}\includegraphics[width=.49\linewidth]{./IMAGES/moment_distance.png}}
%\subfigure[]{\label{fig:c2}\includegraphics[width=.49\linewidth]{./IMAGES/Edge_magnitude.png}}
%\\
%\subfigure[]{\label{fig:c3}\includegraphics[width=.49\linewidth]{./IMAGES/No_of_keypoints.png}}
%\subfigure[]{\label{fig:c4}\includegraphics[width=.49\linewidth]{./IMAGES/Fused_frame_score.png}}
%\caption{Plot of Moment distance $\subref{fig:c1}$, Edge density $\subref{fig:c2}$, Number of key-points $\subref{fig:c3}$ and the total fused score $\subref{fig:c4}$ vs frame number.}
%\label{Fig:chart}
%\end{figure}
\begin{figure}[t]
\centering     %%% not \center
\includegraphics[width=.95\linewidth]{./IMAGES/chartDiagram.png}
\caption{Plot of Moment distance, Edge density, Number of key-points and the total fused score vs frame number.}
\label{Fig:chart}
\end{figure}

\textbf{Edge density}: In our proposed method, the key-frames which have significant depth information are only considered for 3D reconstruction of a polyp. It is observed that the polyp images having more edges have more depth information. The edge information can be obtained with the help of the gradient magnitude of an image. Before finding the gradients, images were smoothed using a Gaussian kernel.

Horizontal and vertical gradients are obtained using Sobel operators $S_x$ and $S_y$ and then the gradient magnitude $\Delta S$ is calculated as follows:
%\begin{multicols}{2}
%\begin{equation}
%\textbf{$S_x$}=
%\begin{bmatrix}
%+1 & 0& -1\\
%+2 & 0& -2\\
%+1 & 0& -2
%\end{bmatrix}
%\end{equation}\break
%\begin{equation}
%\textbf{$S_y$}=
%\begin{bmatrix}
%+1 & +2& +1\\
%0 & 0& 0\\
%-1 & -2& -1
%\end{bmatrix}
%\end{equation}

%\end{multicols}
\begin{equation}
\Delta S=\sqrt{(S_{x})^2+(S_{y})^2}
\end{equation}

\textbf{Key-point detection}:
%However, the key-frames selected by the above two methods may not be good enough for 3D reconstruction.  This is due to the fact that the polyp may be occluded in some the selected key frames, and we need those key frames in which the polyp is maximally visible.
The proposed moment-based key-frame detection method may capture some occluded frames. So, the objective is to select non-occluded key-frames from a group of key-frames which were extracted by our proposed image moment and edge density-based criteria.  For this, a key-point detection based technique is used.

For key-point detection and extraction, we used ORB (Oriented FAST and Rotated BRIEF). ORB operates on an image to obtain a scale pyramid. It also takes into account the orientation of the image patch around the key point. Moreover, ORB is computationally faster and robust to noises in endoscopic images. The frames containing a lesser number of ORB points correspond to occluded polyps. 

\textbf{Adaptive key-frame selection}.
After finding the moment distance (d), edge magnitude (s), and the number of ORB points (p), we normalize these scores using min-max normalization.
% This is done so that each of the three scores is reduced to the range of 0 to 1 with both values inclusive. Instead of adding the three scores directly we use dynamic weights to capture the changes in a video.

The variable having greater variance is given more weight-age. Here, $w_i$ is the weight of the normalized score. To consider intra-variable changes, we used the sum of the magnitude of difference between consecutive frame scores as a measure to find weights. We then normalized this score to be used as weights for finding a fused score. The weights are given by:
 \begin{gather}
d_{1}=\sum_{i=1}^{n}{|d_{i}-d'_{i}|} ,
s_{1}=\sum_{i=1}^{n}{|s_{i}-s'_{i}|} ,
p_{1}=\sum_{i=1}^{n}{|p_{i}-p'_{i}|}
\end{gather}
\begin{gather}
w_{1}=\dfrac{d_{1}}{d_{1}+s_{1}+p_{1}},
w_{2}=\dfrac{s_{1}}{d_{1}+s_{1}+p_{1}}, 
w_{3}=\dfrac{p_{1}}{d_{1}+s_{1}+p_{1}}
\end{gather}
\begin{equation}
f=w_{1}d_{1}+w_{2}s_{1}+w_{3}p_{1}
\end{equation}

%%%%%%%
%\begin{figure}[t]
%\label{Fig:con}
%\centering     %%% not \center
%\begin{subfigure}
%\includegraphics[width=.46\linewidth]{./IMAGES/moment_distance.png}
%\subcaption{a}
%\end{subfigure}
%\begin{subfigure}
%\includegraphics[width=.44\linewidth]{./IMAGES/Edge_magnitude.png}
%\subcaption{b}
%\end{subfigure}
%\\
%\includegraphics[width=.44\linewidth]{./IMAGES/No_of_keypoints.png}
%\includegraphics[width=.44\linewidth]{./IMAGES/Fused_frame_score.png}
%\caption{Plot of moment distance $\ref{fig:c1}$, Edge density $\ref{fig:c2}$, Number of key-points $\ref{fig:c3}$ and the total fused score $\ref{fig:c4}$ vs frame number.}
%\end{figure} 

Here, $d_1$, $s_1$, $p_1$ are the sum of magnitudes of difference between consecutive frame scores and $f$ is the fused score obtained by adaptively weighting the three frame scores. The frames with the highest fused scores are selected according to a threshold value. The variance of each criterion with frame number is shown in Fig.~\ref{Fig:chart}.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

\section{Experimental Results}
The proposed method is evaluated on the publicly available dataset. This dataset contains colonoscopic video sequences from three classes, namely adenoma, serrated and hyperplasic. The adenoma class contains 40 sequences, serrated contains 15, while hyperplasic contains 21 sequences \cite{mesejo2016computer}. In this work, we consider only the frames from the adenoma (malignant) class because this class needs the maximum attention of the physician.

%\\$\href{http://www.depeca.uah.es/colonoscopy_dataset/}{http://www.depeca.uah.es/colonoscopy\_dataset/}$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%spl
%\begin{figure}[t]
%\centering     %%% not \center
%\rotatebox{90}{\scriptsize Convex polyps}
%\subfigure{\label{fig:1}\includegraphics[width=.23\linewidth]{./IMAGES/4.png}}
%\subfigure{\label{fig:2}\includegraphics[width=.23\linewidth]{./IMAGES/6.png}}
%\subfigure{\label{fig:3}\includegraphics[width=.23\linewidth]{./IMAGES/10.png}}
%\subfigure{\label{fig:4}\includegraphics[width=.23\linewidth]{./IMAGES/33.png}}\\
%\rotatebox{90}{\scriptsize Patchy polyps}
%\subfigure{\label{fig:5}\includegraphics[width=.23\linewidth]{./IMAGES/1.png}}
%\subfigure{\label{fig:6}\includegraphics[width=.23\linewidth]{./IMAGES/25.png}}
%\subfigure{\label{fig:7}\includegraphics[width=.23\linewidth]{./IMAGES/31.png}}
%\subfigure{\label{fig:8}\includegraphics[width=.23\linewidth]{./IMAGES/367.png}}
%\caption{Some images of endoscopy dataset: the first row are the examples of convex polyps and the second row are the examples of patchy polyps.}
%\label{dataset}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
\centering  
\subfigure{\label{fig:1}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/frame36.jpg}}
\subfigure{\label{fig:6}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/36.png}}
\subfigure{\label{fig:111}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/frame3.jpg}}
\subfigure{\label{fig:116}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/3.png}}
\subfigure{\label{fig:101}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/D2/frame10.jpg}}
\subfigure{\label{fig:121}\includegraphics[height=0.80cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/D2/10.png}}\\

\subfigure{\label{fig:2}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/frame67.jpg}}
\subfigure{\label{fig:7}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/67.png}}
\subfigure{\label{fig:112}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/frame20.jpg}}
\subfigure{\label{fig:117}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/20.png}}
\subfigure{\label{fig:102}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/D2/frame34.jpg}}
\subfigure{\label{fig:122}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/D2/34.png}}\\

\subfigure{\label{fig:3}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/frame88.jpg}}
\subfigure{\label{fig:8}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/88.png}}
\subfigure{\label{fig:113}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/frame40.jpg}}
\subfigure{\label{fig:118}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/40.png}}
\subfigure{\label{fig:103}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/D2/frame71.jpg}}
\subfigure{\label{fig:123}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/D2/71.png}}\\

\subfigure{\label{fig:4}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/frame457.jpg}}
\subfigure{\label{fig:9}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/457.png}}
\subfigure{\label{fig:114}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/frame70.jpg}}
\subfigure{\label{fig:119}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/70.png}}
\subfigure{\label{fig:104}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/D2/frame108.jpg}}
\subfigure{\label{fig:124}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/D2/108.png}}\\

\subfigure{\label{fig:5}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/frame560.jpg}}
\subfigure{\label{fig:10}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/560.png}}
\subfigure{\label{fig:115}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/frame101.jpg}}
\subfigure{\label{fig:120}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/101.png}}
\subfigure{\label{fig:105}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/D2/frame144.jpg}}
\subfigure{\label{fig:125}\includegraphics[height=0.8cm,width=.1533\linewidth]{./IMAGES/depth_images/depth_images/D2/144.png}}\\

\caption{Key-frames obtained by our method and their corresponding depth maps. The polyp is visible from different viewing angles in these selected frames.}
\label{answer}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%For this work, we considered only narrow band images (NBI) as they require less preprocessing. The adenoma class contains 40 video sequences of different patients. It contains both patchy and convex polyp sequences. In this work, the frames which contain convex polyps are taken for estimating the depth.
%%%%%%%%%spl
% A few convex and patchy polyp images of the dataset are shown in Fig.~\ref{dataset}.

Our method performs better than the state-of-the-art MDE methods. The depth estimation results are shown in Fig.~\ref{comp}, where the first column represents the input images, while the second and the third column show the comparative results between monodepth model \cite{godard2017unsupervised} and zero-shot cross-dataset transfer pre-trained model \cite{lasinger2019towards}. This clearly shows that monodepth performs well in outdoor environments than our method. However, the Zero-shot learning method is more accurate in predicting depth in endoscopic images. 

\begin{figure}[h]
\centering     %%% not \center
\rotatebox{90}{\scriptsize Input image}
\subfigure{\label{fig:a}\includegraphics[width=.61\linewidth,height=1.4cm]{./IMAGES/images.jpeg}}
\subfigure{\label{fig:b}\includegraphics[width=.3\linewidth,height=1.4cm]{./IMAGES/frame36.jpg}}\\
\rotatebox{90}{\scriptsize Monodepth \cite{godard2017unsupervised}}
\subfigure{\label{fig:c}\includegraphics[width=.61\linewidth,height=1.4cm]{./IMAGES/kitti_disp.jpeg}}
\subfigure{\label{fig:d}\includegraphics[width=.3\linewidth,height=1.4cm]{./IMAGES/frame36_disp.jpeg}}\\
\rotatebox{90}{\scriptsize Zero-shot \cite{lasinger2019towards}}
\subfigure{\label{fig:e}\includegraphics[width=.61\linewidth,height=1.4cm]{./IMAGES/test.png}}
\subfigure{\label{fig:f}\includegraphics[width=.3\linewidth,height=1.4cm]{./IMAGES/36.png}}
\caption{Comparison of MDE on two input images, one outdoor and the other one is an endoscopy image. The depth map by Monodepth \cite{godard2017unsupervised} performs well for outdoor environment while giving unsatisfactory results for the endoscopy image . However, the zero-shot learning method \cite{lasinger2019towards} clearly performs well for medical images but cannot accurately estimate the depth in outdoor scenes. }
\label{comp}
\end{figure}
%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering     %%% not \center
\includegraphics[width=\linewidth,height=5cm]{./IMAGES/segmentation_1.png}
\caption{Polyp boundary detection using depth map; Column 1: Original endoscopic image, Column 2: Generated depth maps, Column 3: Detected polyp boundary using canny edge detection algorithm, Column 4: Edge refinement using connected component analysis.  First three rows of image samples are taken from CVC clinic database \cite{bernal2015wm}, the last two rows of images are frames taken from a video sequence of the publicly available dataset \cite{mesejo2016computer}.}
\label{segmentation:examples}
\end{figure}
%%%%%%%%%%%%%%%%%%%%for spl only%%%%%%%%%%%%%%%%
%Moreover, our model is trained three different ways of obtaining depth maps. This is opposed to models trained only on one such method. Also, the scale and shift-invariant loss function helps train on diverse datasets. Thus, the zero-shot learning method for depth estimation works well to even in medical imaging datasets like ours where there is insufficient ground truth depth maps for proper training. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our method is the first-of-its-kind in which key-frames are extracted from an endoscopic video using depth maps. Also, it is robust to occlusions. As redundant frames are discarded in our method, it is more convenient for physicians to analyze important frames of a video sequence. As explained earlier, the moment distance criterion between consecutive frames is used to ensure that redundant frames are identified, and then discarded. The edge magnitude criterion leverages the depth images data to select the best frames. Frames with fewer ORB points have occluded polyps and these frames are redundant. Adaptive thresholding is used to apply three criteria to obtain essential frames for 3D reconstruction.
% An example of key-frames and their depth maps are shown in Fig.~\ref{answer}.


The selected key-frames are finally used to reconstruct the 3D surface of the polyp. We have used Facebook's 3D image GUI to view the reconstructed polyp surface, the link to the video is shown here: 
$\href{https://youtu.be/PJKfk0Mqu2I}{https://youtu.be/PJKfk0Mqu2I}$. 3D visualization of a polyp helps in surgeries involving the removal of the polyp from its root. This gives better visualization of polyps for diagnosis. Fig.~\ref{answer} shows some of the results of key-frame extraction and the corresponding depth maps. No publicly available datasets or methods using them that predict depth maps from endoscopic frames exist. Thus, a comparison between different methods for predicting depth from endoscopic images couldn't be performed. 

Another application of our proposed method could be     
automatic segmentation of polyps in endoscopic images. The depth maps generated by our proposed method can further be used for polyp localization. The canny edge detector is used over the depth maps and subsequently, polyp boundary is determined by using connected component analysis. Fig. \ref{segmentation:examples} shows localized polyps in some of the endoscopic image samples. The segmentation performance on some of the sequences of the CVC clinic database is shown in Table \ref{tab:performance}.
% This database contains 29 endoscopic video sequences with a total of 612 images. Sequences having big and elevated polyps are considered for this study. 
 We defined mIoU as the mean intersection over the union of the segmented polyp masks to the ground truth masks. In polyp segmentation, an IoU score of $\geq$ 0.5 is generally considered good \cite{yamada2019development}. 
%Therefore, our proposed framework could be potentially used for 3D visualization and polyp segmentation in endoscopic video frames.
%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[]
\centering
\caption{Key frame selection and segmentation performance using our method on some of the sequences of CVC clinic database (Sequences with only the elevated polyps are considered)}
\begin{tabular}{ccc}
\hline\hline
\multicolumn{1}{l}{\textbf{Sequence}} & \multicolumn{1}{l}{\textbf{\#Key frames}}  & \multicolumn{1}{l}{\textbf{mIoU $>$ 0.5?}} \\ \hline
104-126                               & 7                                        & Yes                                      \\ \hline
127-151                               & 11                                       & Yes                                      \\ \hline
298-317                               & 2                                        & Yes                                      \\ \hline
343-363                               & 7                                        & Yes                                      \\ \hline
384-408                               & 13                                       & Yes                                      \\ \hline
409-428                               & 8                                        & Yes                                      \\ \hline
479-503                               & 20                                       & Yes                                      \\ \hline
504-528                               & 6                                        & Yes                                      \\ \hline
572-591                               & 4                                        & Yes                                      \\ \hline
592-612                               & 5                                        & Yes                                      \\ \hline
\end{tabular}
\label{tab:performance}
\end{table}
\section{Conclusion}

%Monocular depth estimation is quite important for the segmentation and identification of polyps. 
Our proposed method can determine depth maps using a zero-shot learning approach. 
%The zero-shot learning method performs well on previously unseen classes like endoscopic images. 
%Through this, we extended MDE to in-vivo images which would be useful to analyze medical images. 
The essential frames are picked out from WCE videos with the help of depth information and the proposed three criteria selection strategy. The selection of a threshold value for the final fused score must be empirically set to extract the key-frames. Experimental results show the efficacy of the proposed method in selecting key frames from endoscopic videos and subsequent segmentation of detected polyps in the key frames with the help of extracted depth maps.
%Our method is robust to occlusions in endoscopic frames. 
%The three criteria method successfully finds key-frames that ease the work of a physician to analyze the endoscopic videos. 
Also, the 3D model could be used in clinical diagnosis and surgeries.
One possible extension of this work could be the visualization of polyps in detected key frames in an augmented reality framework.

\section*{Acknowledgement}
The work of Yuji Iwahori was supported by the JSPS Grant-in-Aid Scientific Research (C) under Grant 20K11873 for the endoscope and other medical image researches




% and the determination of surface characteristics like albedo, actual surface color, etc. This information would greatly help in the classification of different types of polyps. 
%The depth maps found via MDE can also be fused with RGB images for polyp segmentation. %The key-frame selection method used here will be extended to frames containing patchy polyps by using saliency maps.

 
 
 %Also while considering frames for finding the key-points, we have also included the part of the frame which contains circular borders. This needs to be removed else, it hinders in finding the number of ORB points found in a frame as key-points are also estimated in the black region.
 






% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.
%
%% you can choose not to have a title for an appendix
%% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.
%
%
%% use section* for acknowledgment
%\section*{Acknowledgment}
%
%
%The authors would like to thank...
%
%
%% Can use something like this to put references on a page
%% by themselves when using endfloat and the captionsoff option.
%\ifCLASSOPTIONcaptionsoff
%  \newpage
%\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}
%
%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
%
%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

\bibliographystyle{IEEEtran}
\balance
\bibliography{Reference_1}


% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}
%
%% insert where needed to balance the two columns on the last page with
%% biographies
%%\newpage
%
%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


