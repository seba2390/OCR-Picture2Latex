\documentclass[sigconf]{acmart}

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
% \acmDOI{10.1145/1122445.1122456}
\acmDOI{xx.xxxx/xxxxxxx.xxxxxxx}


\acmConference[]{*}
\acmBooktitle{}

\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage{amsmath, bm}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[thicklines]{cancel}
\usepackage{bbding}
\usepackage{tabularx}
\usepackage[linewidth=1pt]{mdframed}
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title{From Prediction To Diagnosis: An Identifiable Cognitive Diagnosis Framework for Modelling Examinee Traits}
% \title{Towards the Identifiability and Explainability of Modeling Examinee Traits in Cognitive Diagnosis}
\title{Identifiable Cognitive Diagnosis with Encoder-decoder for Modelling Students' Performance}
% \title{Towards Identifiable Cognitive Diagnosis for Examinee Modeling}
\renewcommand{\shorttitle}{} 

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Jiatong Li}
\email{satosasara@mail.ustc.edu.cn}
\affiliation{%
  \institution{School of Data Science, University of Science and Technology of China}
  % \streetaddress{P.O. Box 1212}
  \city{Hefei}
  \state{Anhui Province}
  \country{China}
  \postcode{230027}
}

\author{Qi Liu}
\email{qiliuql@ustc.edu.cn}
\affiliation{%
  \institution{School of Computer Science, University of Science and Technology of China}
  \city{Hefei}
  \state{Anhui Province}
  \country{China}
  \postcode{230027}
}

\author{Fei Wang}
\email{wf314159@mail.ustc.edu.cn}
\affiliation{%
  \institution{School of Computer Science, University of Science and Technology of China}
  \city{Hefei}
  \state{Anhui Province}
  \country{China}
  \postcode{230027}
}

\author{Jiayu Liu}
\email{jy251198@mail.ustc.edu.cn}
\affiliation{%
  \institution{School of Computer Science, University of Science and Technology of China}
  \city{Hefei}
  \state{Anhui Province}
  \country{China}
  \postcode{230027}
}

\author{Zhenya Huang}
\email{huangzhy@ustc.edu.cn}
\affiliation{%
  \institution{School of Computer Science, University of Science and Technology of China}
  \city{Hefei}
  \state{Anhui Province}
  \country{China}
  \postcode{230027}
}

\author{Enhong Chen}
\email{cheneh@ustc.edu.cn}
\affiliation{%
  \institution{School of Computer Science, University of Science and Technology of China}
  \city{Hefei}
  \state{Anhui Province}
  \country{China}
  \postcode{230027}
}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Jiatong Li, et al.}

\begin{abstract}
  Cognitive diagnosis aims to diagnose students' knowledge proficiencies based on their response scores on exam questions, which is the basis of many domains such as computerized adaptive testing. Existing cognitive diagnosis models (CDMs) follow a proficiency-response paradigm, which views diagnostic results as learnable embeddings that are the cause of students' responses and learns the diagnostic results through optimization. However, such a paradigm can easily lead to unidentifiable diagnostic results and the explainability overfitting problem, which is harmful to the quantification of students' learning performance. To address these problems, we propose a novel identifiable cognitive diagnosis framework. Specifically, we first propose a flexible diagnostic module which directly diagnose identifiable and explainable examinee traits and question features from response logs. Next, we leverage a general predictive module to reconstruct response logs from the diagnostic results to ensure the preciseness of the latter. We furthermore propose an implementation of the framework, i.e., ID-CDM, to demonstrate the availability of the former. Finally, we demonstrate the identifiability, explainability and preciseness of diagnostic results of ID-CDM through experiments on four public real-world datasets.
\end{abstract}

\keywords{}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\vspace{-10pt}
\section{Introduction}\label{sec:intro}
\par Cognitive diagnosis aims to model examinee traits (e.g., knowledge mastery levels) based on their response performance on exam questions, and is fundamental to many downstream user modeling tasks such as examinee performance prediction \cite{daud2017adv,yao2021pro,gao2022dcd}, computerized adaptive testing \cite{ghosh2021bobcat,wang2023gmocat}, and online course recommendation \cite{yin2020mooc,wang2020pet}. Figure \ref{fig:cd-overview} shows a toy example of the workflow of cognitive diagnosis models (CDMs) \cite{Leighton2007}. First, examinees (Adam and Paul) answer questions ($e_1\sim e_4$) that require 6 knowledge concepts (e.g., \textit{Set}). Next, a CDM assesses examinee traits (e.g., knowledge mastery levels in Figure \ref{fig:cd-overview}) and question features, and output diagnostic results (e.g., knowledge mastery levels in epoch $n$ in Figure \ref{fig:cd-overview}). Then diagnostic results are presented for examinees to quantitatively assess their learning performance and provided for downstream tasks mentioned above. 

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{pic/cd-overview.pdf}
  \vspace{-20pt}
  \caption{A toy example of cognitive diagnosis under the proficiency-response paradigm. }
  \Description{} 
  \label{fig:cd-overview}
  \vspace{-15pt}
\end{figure}

\par In the literature, existing CDMs are \textit{score-prediction-based} that follow a \textit{proficiency-response} paradigm in assessing examinee traits and question features, such as Deterministic Input, Noisy ``And'' gate model (DINA) \cite{Torre2009} and Neural Cognitive Diagnosis Model (NeuralCDM) \cite{WangF2022}. As Figure \ref{fig:sr-para} shows, under the proficiency-response paradigm, CDMs view examinee traits and question features as trainable input embeddings and estimate them through optimization. For example, in the right part of Figure \ref{fig:cd-overview}, examinees' knowledge mastery levels and question features are firstly initialized by random values, and question information is transformed to pre-given knowledge factors, such as the question-knowledge mapping matrix (namely Q-matrix) defined in \cite{Tatsuoka1983}. Next, these parameters are input into the predictive module of CDMs to predict response scores. Finally, examinee traits and question features are assessed through the parameter optimization process.

\begin{figure}[t]
  \centering 
  \includegraphics[width=\linewidth]{pic/rpr-paradigm.pdf}
  \vspace{-20pt}
  \caption{The existing proficiency-response paradigm and the proposed response-proficiency-response paradigm of CDMs. $\Theta$ denotes examinee knowledge mastery levels. $\Psi$ denotes question features. $\Pi^{kc}$ denotes observable knowledge factors. $\hat{R}$ denotes predicted response scores.}\label{fig:sr-para}
  \vspace{-15pt}
\end{figure}
 
\par However, the proficiency-response paradigm in cognitive diagnosis would inevitably confront the \textbf{non-identifiability}  \cite{Xu2018, Xu2019identifiability} problem and \textbf{explainability overfitting} problem of diagnostic results, which is harmful to the quantification of examinees' learning performance. First, because of the randomness of the parameter optimization process, the diagnostic result is \textit{unidentifiable}, i.e., CDMs cannot ensure outputting the same diagnostic results for examinees (questions) with the same response score distribution. For example in Figure \ref{fig:cd-overview}, Adam and Paul have the same response score distribution (correct on $e_1, e_2$, incorrect on $e_3, e_4$). With response scores as the only evidence of cognitive diagnosis, the estimation of Adam and Paul's knowledge mastery levels ought to be equal. However, due to the randomness existing in the optimization algorithm of score-prediction-based CDMs (e.g., random sample order in mini-batch gradient descent), diagnostic results of Adam and Paul can be different (see knowledge mastery levels in epoch $n$ in Figure \ref{fig:cd-overview}). From the examinees' point of view, identifiability is significant because response scores are the only evidence of their knowledge mastery levels. However, the non-identifiability problem is ubiquitous for score-prediction-based CDMs. For example, Figure \ref{fig:pdf-overview} shows the histogram of the discrepancy of knowledge profiencies of examinee pairs with the same response distribution in a real-world dataset Math1 (see Section \ref{sec:exp-setup} for dataset description.), diagnosed by NeuralCDM \cite{WangF2022}. We can observe from the cumulative curve that over 50\% of knowledge mastery level differences exceed 0.01, which means that diagnostic results of these examinee pairs are unequal even though their response logs are the same. Therefore, over 50\% of these diagnostic result pairs are unidentifiable. 

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{pic/pdf-dist-theta-ncdm-math1.pdf}
  \vspace{-5pt}
  \caption{The histogram of the discrepancy of knowledge mastery levels of examinees with the same response distribution in Math1 dataset, diagnosed by NeuralCDM \cite{WangF2022}.}
  \label{fig:pdf-overview}
  \vspace{-5pt}
\end{figure}


\par Second, in cognitive diagnosis, the explainability of CDMs is the ability that diagnostic results truly reflect real examinee traits such as knowledge mastery levels. Originating from educational psychology \cite{Reckase2009}, the explainability of CDMs is ensured by the monotonicity assumption \cite{WangF2022} that the response score is monotonically increasing at any dimension of corresponding examinee traits (see Definition \ref{def:mono}). In the proficiency-response paradigm, the monotonicity assumption depends on the property of the predictive module of CDMs and is learned in a transductive manner. For example, the predictive module of NeuralCDM is a full connection neural network with non-negative weights which ensures the increasing monotonicity of examinee knowledge mastery levels to relevant response scores in traing data. However, this can easily lead to the overfitting of examinees' knowledge mastery levels in the aspect of explainability, i.e., although diagnostic results highly satisfy the monotonicity assumption in the training data, they can hardly satisfy the assumption in the test data. This problem is observed in real-world datasets in Section \ref{sec:exp-explainability} and summarized as the \textit{explainability overfitting} phenomenon in this paper. With the explainability overfitting phenomenon, the diagnostic result of a examinee is biased and cannot represent his/her cognitive ability on the whole response logs. As a result, this phenomenon decreases the explainability of the diagnostic result of CDMs and is urgent to be solved.

\par To address these problems, inspired by encoder-decoder frameworks widely used in recommender systems and natural language processing, we propose a novel \textit{response-proficiency-response} paradigm for cognitive diagnosis (see Figure \ref{fig:sr-para}) and design a identifiable cognitive diagnosis framework (ID-CDF) based on the paradigm. ID-CDF leverages the advantage of the encoder-decoder framework to satisfy the identifiability of diagnostic results. In addition, compared to existing encoder-decoder models which lacks encoder output explainability and cannot model the interaction of users and items (e.g., U-AutoRec \cite{Sedhain2015} focusing on user rating vector), ID-CDF is capable for respectively diagnosing explainable examinee traits and question features and modeling their complex interaction. Moreover, ID-CDF learns the explainability of examinee traits in an inductive manner, which alleviates the explainability overfitting problem of existing CDMs. Specifically, we first design a diagnostic module to encode examinee response logs and question response logs to examinee traits and question features respectively which guarantees the identifiability through one-to-one mapping and explainability through monotonicity assumption (see Definition \ref{def:mono}). Second, we propose a general predictive module to reconstruct response logs to ensure the preciseness of diagnostic results. Next, we present an implementation of ID-CDF, i.e., ID-CDM, to illustrate the feasibility of the framework. We demonstrate the identifiability, explainability and preciseness of diagnostic results of ID-CDF by experiments on four public real-world datasets. We further explore the statistical feature of diagnostic results of ID-CDF, and find that the distribution of examinee traits is highly correlated with examinee's overall performance. To our best knowledge, this is the first work that introduces the response-proficiency-response paradigm to cognitive diagnosis. 


\vspace{-5pt}
\section{Related Work}

% \subsection{Cognitive Diagnosis Model}
\par \textbf{Cognitive Diagnosis Model}. Existing cognitive diagnosis models (CDMs) are score-prediction-based. Deterministic Input, Noisy `And' gate model (DINA) \cite{Torre2009} is a discrete CDM that assumes examinee mastery levels are binary, and typically uses the EM algorithm to learn examinee and question parameters from response logs. Item Response Theory (IRT) \cite{Fischer1995,Brzezinska2020} is a continuous CDM. In the two-parameter IRT (2PL-IRT) model \cite{Fischer1995}, a examinee $i$'s ability is modeled as a scalar $\theta_i$, while a question $j$ is modeled as discrimination $a_j$ and difficulty $b_j$, then the predicted response score given the examinee ability and the question feature is $P(r_{ij}=1|\theta_i,a_j,b_j)=\frac{1}{1+\exp\{-a_j(\theta_i-b_j)\}}$, where $r_{ij}$ denotes the response score. Examinee abilities and question features are estimated by parameter optimization methods, such as full Bayesian statistical inference with MCMC sampling \cite{Gelfand1990,Hastings1970} or variational inference \cite{Wu2020}. Multidimensional Item Response Theory (MIRT) \cite{Reckase2009} further extends examinee ability and question difficulty to multidimensional, while the interaction function is still logistic-like. Recently, deep learning methods \cite{Yeung2019, WangF2022} have also been applied to cognitive diagnosis to get more accurate diagnostic results. For instance, Neural Cognitive Diagnosis Model (NeuralCDM) \cite{WangF2022} uses a three-layer positive full connection neural network to capture the complex interaction between examinees and questions.  However, these CDMs would inevitably face the non-identifiability problem and the explainability overfitting phenomenon.

\par \textbf{Encoder-decoder Framework}. Encoder-decoder framework originates from statistical machine translation \cite{ChoMGBBSB2014}, and leads to many classical works such as Transformer \cite{Vaswani2017}. The basic idea of the framework is that a sequence of symbols with a variable length can be encoded to a fixed-length semantic vector, and the vector representation can be decoded to another sequence of symbols. Following this idea, the framework has also been applied to other fields, such as recommender systems \cite{Sedhain2015,WuDZE2016,Kang2021,Li2017} and fake news detection \cite{WuRZZN2023,RazaD2022,nan2021mdfend}, although the input vector is sometimes non-sequence. For instance, in the recommender system, AutoRec \cite{Sedhain2015} is an encoder-decoder collaborative filtering model which encodes a user (item) rating vector to a hidden vector, then decodes the hidden vector to the reconstructed rating vector to predict unobserved entries of the input vector. Based on AutoRec, collaborative denoising autoencoder (CDAE) \cite{WuDZE2016} introduces dropout \cite{SrivastavaHKSS2014} and user embedding at the input layer to get a better prediction of the rating matrix. Collaborative variational autoencoder (CVAE) \cite{Li2017} uses a Bayesian generative model to consider both rating and content (e.g., text) for recommendation in multimedia scenarios.
\par A usually overlooked property of the encoder-decoder framework is the identifiability of the semantic representation \cite{DideriksenDT2022}, i.e., the same input vector leads to the same semantic representation. As mentioned above, identifiability is also crucial in cognitive diagnosis. However, the original encoder-decoder framework cannot be directly applied to cognitive diagnosis for two reasons. First, in cognitive diagnosis, both examinee ability and question feature are unknown and required to be diagnosed simultaneously, while the existing encoder-decoder model can only focus on a single target (e.g., U-AutoRec focusing on user and I-AutoRec focusing on item \cite{Sedhain2015}) and cannot model the interaction between examinees and questions. Second, the explainability of diagnostic results is significant in cognitive diagnosis, while the semantic representation of existing encoder-decoder models is hardly explainable. As a result, it is necessary to design an encoder-decoder model specifically for cognitive diagnosis.


\vspace{-5pt}
\section{Identifiable Cognitive Diagnosis Framework}
\begin{figure*}[t]
  \centering 
  \includegraphics[width=\linewidth]{pic/ID-CDF-new.pdf} 
  \vspace{-10pt}
  \caption{The structure of identifiable cognitive diagnosis framework (ID-CDF).}
  \label{fig:id-cd}
  \vspace{-15pt}
\end{figure*} 

\subsection{Preliminary}
\par In this part, we give necessary mathematical notations for cognitive diagnosis. Then we give the definition of the cognitive diagnosis task. Finally, we give the definition of identifiability and monotonicity assumption. 
\vspace{-7pt}
\subsubsection{Mathematical Notations and Task Definition}\label{sec:Preliminary}
\par To begin with, $S = \{s_1, s_2, \ldots, s_N\}$ denotes the examinee set, where $N$ is the number of examinees. $E = \{e_1,\ldots, e_M\}$ denotes the question set, where $M$ is the number of questions. $C = \{c_1,\ldots,c_K\}$ denotes the knowledge concept set, where $K$ is the number of knowledge concepts. $Q=(q_{jk})_{M\times K}$ denotes the question-knowledge mapping matrix manually labeled by experts, namely Q-matrix \cite{Tatsuoka1983}, which denotes what knowledge concepts are required by questions to correctly respond. For each component in the Q-matrix, $q_{jk}=1$ denotes that question $e_j$ requires knowledge concept $c_k$ to correctly respond, otherwise $q_{jk}=0$. For each item in response logs, $r_{ij}\in\{0,1\}$ denotes the dichotomous response score of examinee $s_i$ on question $e_j$, where $r_{ij}=1$ means a correct response while $r_{ij}=0$ means an incorrect response. The total response log is a set of tuples, i.e., $R=\{(s_i, e_j, r_{ij})|s_i\in S, e_j\in E, r_{ij}\in\{0,1\}\}$. Examinee knowledge mastery levels are represented by $\Theta = \{\bm{\theta}_i| s_i\in S\}$. Question features are represented by $\Psi = \{\bm{\psi}_j|e_j\in E\}$. Next, the cognitive diagnosis task is defined as follows:
\vspace{-5pt}
\begin{definition}
  \textbf{Cognitive diagnosis task}. Given response log set $R$ and the Q-matrix $Q$, the goal of the cognitive diagnosis task is to mine identifiable and explainable examinee traits $\Theta$. 
\end{definition}
\vspace{-9pt}

\subsubsection{Identifiability and Explainability of CDMs}\label{sec:Assumption}
\par The identifiability and explainability of diagnostic results are crucial to CDMs because examinees depend on the diagnostic result to quantitatively assess their learning performance \cite{Xu2018,Xu2019identifiability,Reckase2009,WangF2022}. In the literature, the explainability of the diagnostic result is usually ensured by the monotonicity assumption \cite{Reckase2009,WangF2022}. Here, we give and describe the definition of identifiability and the monotonicity assumption.
\vspace{-5pt}
\begin{definition}\label{def:identifiability}
  \textbf{Identifiability}. Examinee/Question diagnostic results of a CDM is identifiable if, for any pair examinee/question with the same response score distribution, their diagnostic results are also the same, i.e.,

  \vspace{-5pt}
  \begin{equation}
    \begin{aligned}
      \tilde{R}_i = \tilde{R}_j &\Rightarrow \bm{\theta}_i = \bm{\theta}_j, \forall s_i, s_j \in S, i\neq j, \\
      \hat{R}_i = \hat{R}_j &\Rightarrow \bm{\psi}_i = \bm{\psi}_j, \forall e_i, e_j \in E, i\neq j, \\
    \end{aligned}
    \vspace{-5pt}
  \end{equation}
  where $\tilde{R}_i\subset R$ and $\hat{R}_j\subset R$ denotes the total response log (response score distribution) of examinee $s_i$, and question $e_j$ respectively.
\end{definition}
\vspace{-5pt}
\par For instance, in an examination, examinee $s_1$ and $s_2$ got exactly the same score on each question. If the diagnostic result of them is identifiable, then $s_1$'s knowledge mastery level ought to be equal to $s_2$'s knowledge mastery level, i.e., $\bm{\theta_1} = \bm{\theta_2}$. From users' point of view, identifiability is important, because the evaluation of their knowledge mastery level is based on the comparison of their response scores. Therefore, it is unreasonable for users if $s_1$'s mastery level on a knowledge concept is different from $s_2$.
\vspace{-5pt}
\begin{definition}\label{def:explainability}
  \textbf{Explainability}. The explainability of examinees' diagnostic results is defined as the ability they correctly reflect examinees' true mastery levels of knowledge concepts.
  \vspace{-5pt}
\end{definition}

\par For example, if a examinee has mastered the knowledge concept `\textit{Inequality}', then the component of the examinee's trait (diagnostic result) on this knowledge concept should be high so that the diagnostic result can correctly reflect the fact that the examinee has mastered the knowledge concept. However, it is difficult to directly keep the explainability of diagnostic results because examinees' true mastery levels are unobservable. As a result, in cognitive diagnosis, the explainability of diagnostic results is usually indirectly satisfied by the monotonicity assumption \cite{Reckase2009,WangF2022}:

\vspace{-5pt}
\begin{definition}\label{def:mono}
  \textbf{Monotonicity assumption}. The probability of every examinee's correctly answering a question is monotonically increasing at any relevant component of his/her knowledge mastery level. Formally, the monotonicity assumption is equivalent to:
  \begin{equation}
    \bm{\theta_i}^{(l)} \succeq \bm{\theta_j}^{(l)} \Leftrightarrow r_{il} \geq r_{jl}, \forall s_i, s_j \in S,\,\,e_l \in E,
  \end{equation}
where $\bm{\theta_i}^{(l)} (s_i\in S, e_l\in E)$ denotes the relevant component of $s_i$'s knowledge mastery level $\bm{\theta}_i$ to question $e_l$.
\end{definition}
\vspace{-5pt}

\par For score-prediction-based CDMs, the monotonicity assumption usually depends on the monotonicity property of the interaction function \cite{WangF2022}. For traditional CDMs such as DINA \cite{Torre2009} and IRT \cite{Brzezinska2020}, the interaction function is usually linear, thus inherently satisfying the monotonicity assumption. For deep learning-based CDMs such as NeuralCDM \cite{WangF2022}, the weight parameter of the interaction function is limited to be non-negative to satisfy the assumption.
 
\vspace{-5pt}
\subsection{Framework Overview}\label{sec:Framework Overview}


\par The structure of ID-CDF is shown in Figure \ref{fig:id-cd}. Following the respon-se-proficiency-response paradigm defined in Figure \ref{fig:sr-para}, ID-CDF first utilizes an examinee diagnostic module and a question diagnostic module to diagnose examinee traits (i.e., knowledge profiency) and question features (i.e., knowledge difficulty) from response logs respectively. Next, diagnostic results are input to the predictive module (the interaction function) to predict response scores. In the training of ID-CDF, diagnostic modules learn the complex mapping between examinee/question response patterns and examinee traits / question features. Meanwhile, the interaction function is also trained to learn the complex interaction between examinee traits and question features to reconstruct response scores. 


\par \textbf{Diagnostic Module}. The diagnostic Module aims to directly diagnose identifiable and explainable examinee traits and question features from response logs. Different from score-prediction-based CDMs, the main advantage of ID-CDF is that the identifiability and explainability of diagnostic results can be directly satisfied by applying the identifiability condition and the monotonicity condition to the diagnosis process. Specifically, in ID-CDF, response logs are first transformed to response vectors. For examinee $i$, let $\bm{x}_i^{(s)} = (x_{i1}, x_{i2},\ldots,x_{iM})^\top$ denote his/her response vector. For question $j$, let $\bm{x}_j^{(e)} = (x_{1j},x_{2j},\ldots,x_{Nj})^\top$ denote its response vector. Here, $x_{ij},i=1,\ldots,N,j=1\ldots,M$ is defined as follows:
\begin{equation}\label{eq:r2x}
  x_{ij}=\left\{\begin{aligned}
    1&,\, \text{if}\,\,\, r_{ij}=1,\\
    0&,\, \text{if}\,\,\, (s_i,e_j,0)\notin R\,\,\text{and}\,\,(s_i,e_j,1)\notin R,\\
    -1&,\, \text{otherwise}.\\
  \end{aligned}\right.
\end{equation}

\par Next, ID-CDF utilizes an examinee diagnostic function $\mathcal{F}(\cdot)$ and a question diagnostic function $\mathcal{G}(\cdot)$ to diagnose examinee traits and question features respectively, as shown in the following:
% \vspace{-5pt}
\begin{equation}
  \bm{\theta}_i = \mathcal{F}\left(\bm{x}_i^{(s)}; \omega^{(s)}\right),\,\,\,\,i=1,2,\ldots,N
\end{equation}
\vspace{-5pt}
\begin{equation}
  \bm{\psi}_j = \mathcal{G}\left(\bm{x}_j^{(e)}; \omega^{(e)}\right),\,\,\,\,j=1,2,\ldots,M,
\end{equation}
where $\bm{\theta}_i$ denotes examinee traits, and $\bm{\psi}_j$ denotes question features. All $\omega^{(\cdot)}$ denote latent parameters of diagnostic functions that reflect the diagnostic process, and can be learned from data. In the design of diagnostic functions, the identifiability condition and the monotonicity condition must be satisifed, which are defined in the following.

\begin{definition}\label{def:id}
  \textbf{Identifiability Condition.} A diagnostic function satisfies the identifiability condition if and only if there does not exist individual-specific factors in the function that affect the diagnostic results.
\end{definition}
\par Here individual-specific factors means those latent parameters that can be distinguished by individuals such as examinee embeddings indexed by exminee ID. In score-prediction-based CDMs, all diagnostic results are individual-specific factors which would inevitably confront the non-identifiability problem, because the randomness in the optimization causes different update of individual factors. With the identifiability condition, the identifiability of ID-CDF can be rigorously satisifed. A detailed proof of the identifiablity of ID-CDF is available at Appendix \ref{sec:proof}.

\begin{definition}
  \textbf{Monotonicity Condition.} For any examinee diagnostic function $\mathcal{F}:R\to\Theta$, the function satisfies the monotonicity condition if and only if it is monotonically increasing at any dimension of response vectors, i.e., $\frac{\partial\mathcal{F}}{\partial x_{ij}^{(s)}}\geq 0, \forall j = 1,2,\ldots,M$.
\end{definition}
\par The monotonicity condition is only applied to the examinee diagnostic module (as shown in Figure \ref{fig:id-cd}) because the monotonicity assumption is targeted at examinee traits. Compared to score-prediction-based CDMs, ID-CDF has a better explainability on unobserved data. Specifically, for score-prediction-based CDMs, as examinee traits are learned by optimization on training response logs, the monotonicity assumption can only be satisfied on training response logs. On the other hand, ID-CDF is an inductive learning method \cite{street1995inductive} that directly induces the monotonicity of diagnostic results from observed data distribution and extends it to unobserved data. Since all examinee traits are diagnosed by the public diagnostic module rather than learned from individual-specific factors, the diagnostic module can capture the general monotonicity from different examinees' response vectors. 

\par \textbf{Predictive Module}. The predictive module aims to reconstruct response scores from examinee traits and question features to ensure the preciseness of diagnostic results. In ID-CDF, the predictive module consists of pre-given knowledge factors (Q-matrix in Figure \ref{fig:id-cd} that specifies the mapping between questions and knowledge concepts), and a flexible interaction function that models the complex interaction between examinees and questions. The reconstruction process is formally defined in the following:
\begin{equation}\label{eq:id-cdf-pm}
  y_{ij} = \mathcal{H}\left(\bm{\theta}_i\odot \bm{q}_j, \bm{\psi}_j\odot\bm{q}_j;\omega^{(p)}\right),
  \vspace{-5pt}
\end{equation}
where $\mathcal{H}$ denotes the interaction function. The $q_j$ denotes the binary vector of question $j$ in the Q-matrix which indicates required knowledge concepts of the question. The $\odot$ denotes element-wise product, which is used to mask irrelevant knowledge concepts in the training of ID-CDF. The $\omega^{(p)}$ denotes learnable latent parameters of $\mathcal{H}(\cdot)$ that models the complex interaction between examinees and questions. In ID-CDF, the interaction function is flexible that can be integrated in various forms, which depends on the actual demand for cognitive diagnosis.

 
\par \textbf{Loss Function}. In cognitive diagnosis, response scores are usually binary. As a result, we utilize the cross entropy between the output $y$ and true score $r$ as the loss function of ID-CDF, as shown in Eq \eqref{eq:loss}:
\vspace{-5pt}
\begin{equation}\label{eq:loss}
  \mathcal{L}(\Omega) = -\sum_{(s_i,e_j,r_{ij})\in R}\left(r_{ij}\log y_{ij} + (1-r_{ij})\log(1-y_{ij})\right),
  \vspace{-5pt}
\end{equation}
where $\Omega = \left(\omega^{(s)},\omega^{(e)},\omega^{(p)}\right)$ denotes parameters of ID-CDF.


 
\subsection{ID-CDM: An Implementation of ID-CDF}\label{sec:id-cdm}
\par In this part, we present an Identifiable Cognitive Diagnosis Model (ID-CDM) as an implementation of ID-CDF to demonstrate the feasibility of ID-CDF.
% \subsubsection{Diagnostic Module}
\par \textbf{Diagnostic Module}. In ID-CDF, the pivotal principle of the design of the diagnostic module is to satisfy the identifiability condition and the monotonicity condition. To this end, in ID-CDM, we adopt multi-layer perceptrons (MLPs) with parameter constraints to learn the diagnosis process from data, while keeping the two principle conditions. Specifically, the examinee diagnostic module is defined as follows:

\vspace{-2pt}
\begin{gather}
  f_1 = \sigma (W^{(s)}_1\times \bm{x}^{(s)}_i + b^{(s)}_1),\\
  \bm{\theta}_i = \sigma (W^{(s)}_2\times f_1 + b^{(s)}_2),
  \vspace{-5pt}
\end{gather}
where $\sigma(\cdot)$ denotes the sigmoid activation function. In this examinee diagnostic module, the learnable parameters can be represented as $\omega^{(s)} = \left(W^{(s)}_1,W^{(s)}_2,b^{(s)}_1,b^{(s)}_2\right)$. The $W^{(s)}_1$ and $W^{(s)}_2$ are constraint to be positive to satisfy the monotonicity condition. Since there does not exist any individual-specific factors, this module satisfies the identifiability condition.
\par Meanwhile, the question diagnostic module is defined as:

\vspace{-10pt}
\begin{gather}
  g_1 = \sigma (W^{(e)}_1\times \bm{x}^{(e)}_j + b^{(e)}_1), \\
  g_2 = \sigma (W^{(e)}_2\times g_1+ b^{(e)}_2), \\
  \bm{\psi}_j = \sigma (W^{(e)}_3\times g_2 + b^{(e)}_3),
  \vspace{-5pt}
\end{gather}
where the learnable parameters can be represented as $\omega^{(e)} = \left(W^{(e)}_1,\right.$ $\left.W^{(e)}_2, W^{(e)}_3, b^{(e)}_1, b^{(e)}_2, b^{(e)}_3\right)$. Because there does not exist any indi-vidual-specified factors, this module also satisfies the identifiability condition.

\par \textbf{Predictive Module}. In the predictive module, we also adopt neural networks to learn the complex interaction between examinees and questions. Specifically, we first utilize single-layer perceptrons to aggregate knowledge concept-wise diagnostic results to low-dimensional features to gain more effective representations of examinees and questions. Next, we utilize an MLP to reconstruct response scores from aggregated representations.
\par To begin with, the aggregation layer of diagnostic output is defined as following:
\begin{gather}
  \bm{\alpha}_i = \sigma(W^{(u)}\times (\bm{\theta}_i\odot\bm{q}_j) + b^{(u)}),\\
  \bm{\phi}_j = \sigma(W^{(v)}\times (\bm{\psi}_j\odot\bm{q}_j) + b^{(v)}).
\end{gather}
\par Next, aggregated representations of examinee $s_i$ and question $e_j$ are input to a three-layer MLP to reconstruct response scores:

\vspace{-7pt}
\begin{gather}\label{eq:id-cdm-fin1}
  z_1 = \sigma(W^{(c)}_1\times (\bm{\alpha}_i - \bm{\phi}_j) + b^{(c)}_1),\\\label{eq:id-cdm-fin2}
  z_2 = \sigma(W^{(c)}_2\times z_1 + b^{(c)}_2),\\\label{eq:id-cdm-fin3}
  y_{ij} = \sigma(W^{(c)}_3\times z_2 + b^{(c)}_3).
\end{gather}
\par In the predictive module, parameters can be represented as $\omega^{(p)} $ $= \left(W^{(u)}, W^{(v)}, W^{(c)}_1, W^{(c)}_2, W^{(c)}_3, b^{(u)}, b^{(v)}, b^{(c)}_1, b^{(c)}_2, b^{(c)}_3\right)$. These parameters can be learned together with $\omega^{(s)}$ and $\omega^{(e)}$ in the training of ID-CDM.


\vspace{-5pt}
\section{Experiment}\label{sec:exp}

\subsection{Experiment Overview}
\par In this section, we conduct experiments on four real-world datasets to demonstrate the identifiability, explainability and preciseness of ID-CDF\footnote{Our code will be released after the publication of this paper.}. The experiments aim to answer four research questions in the following:

\begin{itemize}[leftmargin=*]
  \item \textbf{RQ1}: How is the identifiability of diagnostic results of ID-CDF and other CDMs?
  \item \textbf{RQ2}: How is the explainability of diagnostic results of ID-CDF and other CDMs?
  \item \textbf{RQ3}: Can diagnostic results of ID-CDF accurately reflect examinees' response performances?
  \item \textbf{RQ4} What statistical features do outputs of ID-CDF have?
\end{itemize}


\vspace{-10pt}
\subsection{Expermental Setup}\label{sec:exp-setup}
\par \textbf{Dataset description}. In the experiment, we choose four public real-world datasets to validate the performance of ID-CDF, including two online K-12 mathematical test datasets, i.e., ASSIST (ASSISTments 2009-2010 ``skill builder'') \cite{Feng2009} and Algebra (Algebra | 2006-2007) \cite{Stamper2010}, and two offline high school mathematical exam datasets, i.e., Math1 and Math2 \cite{Liu2018}. A summary of the datasets is presented in Table \ref{tab:dataset} in Appendix \ref{app:data}. In the preprocessing of datasets, for online test datasets, we reserve only the first attempt of examinees answering a question inspired by \cite{Huang2019}. To ensure that each examinee has enough response logs to make cognitive diagnosis, we remove examinees with less than 15 response logs. For the Algebra dataset, we randomly select 100,000 questions for our experiment. Next, 80\% of each examinee's response log is randomly split as a train set, while the rest 20\% serves as the test set. In the train set, 90\% of each examinee's response log is used for model training, while the rest 10\% is used for model validation.

\begin{figure}[t]\centering 
  \includegraphics[width=0.7\linewidth]{pic/rq2-copydata.pdf}
  \vspace{-10pt}
  \caption{An illustration of data augmentation in RQ1. The response matrix is copied for examinees (questions) to get shadow examinees (questions) with the same response logs.}
  \label{fig:rq2-copydata}
  \vspace{-15pt}
\end{figure}

\begin{table*}[t]\centering
  \caption{Identifiability Score (IDS $\uparrow$) of diagnostic results of CDMs (RQ1). $\mathcal{I}(X)$ indicates whether $X$ is identifiable.}\label{tab:ids}
  \vspace{-10pt}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{ccccccccccc}
    \toprule
    \multirow{2}{*}{CDM} & \multicolumn{5}{c}{IDS $\uparrow$ of Examinee Diagnostic Result $\Theta$} & \multicolumn{5}{c}{IDS $\uparrow$ of Question Diagnostic Result $\Psi$} \\
    \cmidrule(lr){2-6}\cmidrule(lr){7-11}
     & ASSIST & Algebra & Math1 & Math2 & $\mathcal{I}(\Theta)$ & ASSIST & Algebra & Math1 & Math2 & $\mathcal{I}(\Psi)$\\
    \midrule
    DINA & 0.550\footnotesize$\pm$0.003 & 0.092\footnotesize$\pm$0.004 & 0.451\footnotesize$\pm$0.006 & 0.368\footnotesize$\pm$0.009 & \XSolidBrush & 0.208\footnotesize$\pm$0.001 & 0.160\footnotesize$\pm$0.000 & 0.193\footnotesize$\pm$0.019 & 0.214\footnotesize$\pm$0.038 & \XSolidBrush \\
    IRT & 0.691\footnotesize$\pm$0.004 & 0.698\footnotesize$\pm$0.005 & 0.690\footnotesize$\pm$0.004 & 0.688\footnotesize$\pm$0.003 & \XSolidBrush & 0.376\footnotesize$\pm$0.001 & 0.371\footnotesize$\pm$0.000 & 0.543\footnotesize$\pm$0.035 & 0.540\footnotesize$\pm$0.036 & \XSolidBrush \\
    MIRT & 0.047\footnotesize$\pm$0.001 & 0.045\footnotesize$\pm$0.000 & 0.046\footnotesize$\pm$0.000 & 0.047\footnotesize$\pm$0.001 & \XSolidBrush & 0.041\footnotesize$\pm$0.000 & 0.042\footnotesize$\pm$0.000 & 0.085\footnotesize$\pm$0.005 & 0.076\footnotesize$\pm$0.006 & \XSolidBrush \\
    NCDM & 0.857\footnotesize$\pm$0.001
    & 0.409\footnotesize$\pm$0.001 & 0.662\footnotesize$\pm$0.005 & 0.597\footnotesize$\pm$0.006 & \XSolidBrush & 0.616\footnotesize$\pm$0.000 &  0.480\footnotesize$\pm$0.000 & 0.420\footnotesize$\pm$0.012 & 0.307\footnotesize$\pm$0.009 & \XSolidBrush \\
    NCDM-Const & 0.897\footnotesize$\pm$0.001 & 0.701\footnotesize$\pm$0.005 & 0.688\footnotesize$\pm$0.003 & 0.635\footnotesize$\pm$0.006 & \XSolidBrush & 0.968\footnotesize$\pm$0.000 & 0.989\footnotesize$\pm$0.000 & 0.915\footnotesize$\pm$0.010 & 0.916\footnotesize$\pm$0.009 & \XSolidBrush \\
    CDMFKC & 0.621\footnotesize$\pm$0.001 & 0.390\footnotesize$\pm$0.001 & 0.613\footnotesize$\pm$0.015 & 0.553\footnotesize$\pm$0.011 & \XSolidBrush & 0.618\footnotesize$\pm$0.000 & 0.481\footnotesize$\pm$0.000 & 0.408\footnotesize$\pm$0.012 & 0.297\footnotesize$\pm$0.011 & \XSolidBrush \\
    \midrule
    ID-CDM-nEnc & 0.613\footnotesize$\pm$0.001 & 0.375\footnotesize$\pm$0.002 & 0.595\footnotesize$\pm$0.008 & 0.524\footnotesize$\pm$0.026 & \XSolidBrush & 0.601\footnotesize$\pm$0.000 & 0.495\footnotesize$\pm$0.000 & 0.401\footnotesize$\pm$0.007 & 0.304\footnotesize$\pm$0.008 & \XSolidBrush \\
    ID-CDM & \textbf{1.000\footnotesize$\pm$0.000}  & \textbf{1.000\footnotesize$\pm$0.000} & \textbf{1.000\footnotesize$\pm$0.000} & \textbf{1.000\footnotesize$\pm$0.000} & \CheckmarkBold & \textbf{1.000\footnotesize$\pm$0.000} & \textbf{1.000\footnotesize$\pm$0.000} & \textbf{1.000\footnotesize$\pm$0.000} & \textbf{1.000\footnotesize$\pm$0.000} & \CheckmarkBold \\
    \bottomrule
  \end{tabular}
  }
  \vspace{-10pt}
\end{table*}



\noindent\textbf{Baselines}. We compare the performance of ID-CDM with five typical score-prediction-based CDMs and two encoder-decoder models in our experiment. These baselines are described as follows. 

\begin{itemize}[leftmargin=*]
  \item \textbf{DINA} \cite{Torre2009} is a score-prediction-based CDM which models examinee abilities as binary knowledge proficiencies, and models question features by `guess' and `slip' probabilities. 
  \item \textbf{IRT} \cite{Brzezinska2020} is a score-prediction-based CDM which models scalar examinee abilities, question difficulties and question discrimination through a logistic-like interaction function.
  \item \textbf{MIRT} \cite{Reckase2009} is a score-prediction-based CDM that extends scalar examinee abilities and question difficulties in IRT to multidimensional situations.
  \item \textbf{NeuralCDM} \cite{WangF2022} is a score-prediction-based CDM. NeuralCDM utilizes a novel monotonic neural network to learn the complex interaction between examinees and questions from data, and is capable for diagnose knowledge concept-wise examinee abilities and question difficulties.
  \item \textbf{CDMFKC} \cite{ShengLi2022} is a score-prediction-based CDM which utilizes an elaborately designed neural network to model the influence of knowledge concepts on examinees' learning performance.
  \item \textbf{U-AutoRec} \cite{Sedhain2015} is an encoder-decoder model which utilizes an autoencoder to learns user traits from historical response logs.
  \item \textbf{CDAE} \cite{WuDZE2016} is an encoder-decoder model which utilizes a denoising autoencoder to facilitate robustness in learning user traits from historical response logs.
\end{itemize}

\noindent\textbf{Training setting}. In the training setting, all models are implemented with PyTorch using Python. The dimension of diagnostic results of MIRT is set to 16. The dimension of transformed diagnostic results of ID-CDM is set to 64. The dimension of U-AutoRec is set to the number of knowledge concepts so that we can explore in RQ3 whether existing encoder-decoder models can generate explainable diagnostic results. All model parameters are initialized with Xavier normal method \cite{GlorotB2010}, and optimized with the Adam algorithm \cite{KingmaB2014}. All experiments are run on a Linux server with thirty-two 2.10GHz Intel Xeon E5-2620 v4 CPUs and six NVIDIA GTX 1080Ti GPUs.

\vspace{-10pt}
\subsection{Identifiability Evaluation (RQ1)}
\par In this part, we design a novel experiment to quantitatively evaluate the identifiability of various cognitive diagnosis models. Our motivation is that the identifiability of CDMs can be evaluated by measuring the discrepancy between traits of examinees with the same response distribution. The smaller the discrepancy, the better the identifiability. To this end, inspired by data augmentation in computer vision and natural language processing\cite{ghiasi2021augmentation,bayer2023augmentation}, we first propose a data augmentation process to access examinees / questions with the same response distributions (namely ``shadow'' examinees / questions), as illustrated in Figure. \ref{fig:rq2-copydata}. Next, we propose a novel evaluation metric, namely Identifiability Score (IDS), to quantitatively the identifiability of CDMs on augmented data.

\par Specifically, IDS aims to be monotonically decreasing at the discrepancy between traits of examinees with the same response distribution. In addition, IDS equals one if and only if examinee traits are identifiable. To achieve this goal, we define IDS of examinee traits $\Theta$ as follows:

\vspace{-10pt}
\begin{equation}\label{eq:ids}
  IDS(\Theta) = \frac{1}{Z}\sum_{i \in S}\sum_{j \in S}\frac{I(\bm{r}_{i}=\bm{r}_j) \land I(i \neq j)}{\left[1 + dist(\bm{\theta}_i, \bm{\theta}_j)\right]^2},
\end{equation}
where $Z = \sum_{i \in S}\sum_{j \in S} I(\bm{r}_{i}=\bm{r}_j) \land I(i \neq j)$. The $dist(\bm{\theta}_i, \bm{\theta}_j)$ is the manhattan distance \cite{Craw2010} between examinee $i$'s traits and examinee $j$'s traits which serves as the discrepancy measurement. As mentioned above, $IDS(\Theta)$ is monotonically decreasing at $dist(\bm{\theta}_i, \bm{\theta}_j)$. \textbf{Examinee traits are identifiable if and only if} $IDS(\Theta) = 1$ (i.e., $dist(\theta_i,\theta_j)=0, \forall r_i=r_j\land i\neq j$). Similarly, we can also evalute the identifiability of question features $\Psi$ by calculating $IDS(\Psi)$.
\par We evaluate the identifiability of score-prediction-based CDMs and ID-CDM. Furthermore, we also explore through \textbf{ablation studies} the impact of ramdom initialization in score-prediction-based CDMs and diagnostic modules in ID-CDF on the identifiability of diagnostic results. For the former, we initialize diagnostic results of NCDM by constant values (namely NCDM-Const) and compare its IDS with that of the original NCDM. For the latter, we remove diagnostic modules of ID-CDM (namely ID-CDM-nEnc) and compare its IDS with that of the original ID-CDM. 
\par The experimental results are presented in Table \ref{tab:ids}. Within score-prediction-based CDMs, the IDS of neural network models (i.e., NCDM, NCDM-Const, CDMFKC) are significantly higher than that of most traditional CDMs except IRT, which demonstrates the effectiveness of neural network in enhancing the identifiability of diagnostic results. In fact, IRT models examinee traits and question difficulties as unidimensial variables, which essentially decreases the discrepancy between diagnostic results of different examinees / questions. As for other CDMs, the use of Q-matrix not only clarifies the mapping between questions and knowledge concepts, but limits the feasible space of diagnostic results so that their IDS are magnified to some extent. The evidence is the comparison between MIRT and other multidimensional CDMs. In this experiment, MIRT is the only multidimensional CDM that excludes the Q-matrix. Although its examinee trait dimension ($D = 16$) is largely lower than that of other multidimensional CDMs ($D=$ \#Knowledge concepts), its IDS is significantly lower than that of others. 
\par Comparing between our proposed ID-CDM and baseline CDMs, the IDS of ID-CDM always reach the maximum value (i.e., IDS = 1), which demonstrate the rigorous identifiability of our proposed method.  Moreover, in ablation studies, as ID-CDM-nEnc is unidentifiable, it can be concluded that the identifiability of ID-CDM is ensured by the diagnostic module of ID-CDF. Besides, although the comparision between NCDM-Const and NCDM illustrates that the constant initialization of score-prediction-based CDMs indeed improves their IDS, the improvement is limited because randomness still exists in the optimization process. As a result, constantly intialized score-prediction-based CDMs are still unidentifiable.

\begin{table}[t]\centering
  \caption{Degree of Consistency ($\overline{DOC}$ $\uparrow$) on test data (RQ2).}\label{tab:doc}
  \vspace{-0.3cm}
  \begin{tabular}{ccccc}
    \toprule
    Model & ASSIST & Algebra & Math1 & Math2 \\
    \midrule
    U-AutoRec & 0.613 & 0.572 & 0.527 & 0.552 \\
    DINA & 0.689 & 0.636 & 0.673 & 0.655 \\
    NCDM & 0.666 & 0.642 & 0.608 & 0.608 \\
    CDMFKC & 0.664 & 0.641 & 0.595 & 0.575 \\
    \midrule
    ID-CDM-nMono & 0.534 & 0.578 & 0.390 & 0.504 \\
    \textbf{ID-CDM} & \textbf{0.702} & \textbf{0.645} & \textbf{0.693} & \textbf{0.686} \\
    \bottomrule 
  \end{tabular}
  \vspace{-10pt}
\end{table}

\begin{figure}[t]\centering
  \includegraphics[width=0.9\linewidth]{pic/RGE.pdf}
  \vspace{-5pt}
  \caption{REO $\downarrow$ of CDMs (RQ2). The smaller the REO, the closer the explainabilities on training data and test data.}\label{fig:reo}
  \vspace{-15pt}
\end{figure}

\begin{figure*}[t]
  \centering
  \subfloat[ACC $\uparrow$ of models]{
    \includegraphics[width=0.32\linewidth]{pic/accs.pdf}
  }
  \subfloat[RMSE $\downarrow$ of models]{
    \includegraphics[width=0.32\linewidth]{pic/rmses.pdf}
  }
  \subfloat[F1 $\uparrow$ of models]{
    \includegraphics[width=0.32\linewidth]{pic/f1s.pdf}
  }
  \vspace{-10pt}
  \caption{Results of examinee performance prediction (RQ3).}\label{fig:prediction}
  \vspace{-10pt}
\end{figure*} 

\begin{figure*}[t]
  \centering
    \includegraphics[width=0.95\linewidth]{pic/umap_idcdm.pdf}
    \vspace{-10pt}
  \caption{Clustering of examinee traits diagnosed by ID-CDM (RQ4). Points are colored with correct rates.}\label{fig:cluster}
  \vspace{-15pt}
\end{figure*}

\vspace{-5pt}
\subsection{Explainability Evaluation (RQ2)}\label{sec:exp-explainability}
\par In this part, we evaluate the explainability of examinee traits diagnosed by CDMs from the aspect of monotonicity assumption. Moreover, we also quantitatively evalute the explainability overfitting problem of score-prediction-based CDMs mentioned above. In evaluating the explainability of examinee traits, our motivation is that the order of explainable examinees' knowledge proficiencies should be consistent with the order of response scores on relevant questions. To this end, inspired by previous works \cite{FoussPRS2007}, we propose the Degree of Consistency (DOC) as the evaluation metric. Given question $e_l, l=1,2,\ldots,M$, $DOC$ is defined as follows:

\vspace{-10pt}
\begin{equation}
DOC(e_l)\!=\!\frac{\sum_{i,j}\delta (r_{il},\! r_{jl})\sum_{k=1}^K q_{lk}\!\land\! J(l,i,j)\!\land\!\delta(\theta_{ik},\!\theta_{jk})}{\sum_{i,j}\delta (r_{il},\! r_{jl})\sum_{k=1}^K\! q_{lk}\!\land\! J(l,i,j)\!\land\! I(\theta_{ik}\!\neq\!\theta_{jk})},
\vspace{-5pt}
\end{equation}
where $\delta(x,y)=1$ if $x>y$ and $\delta(x,y)=0$ otherwise. $J(l,i,j) = 1$ if both $s_i$ and $s_j$ has answered question $e_l$ and $J(l,i,j) = 0$ otherwise. $I(\cdot)$ denotes the indicator function. The $DOC$ is in $[0,1]$. The higher the DOC, the better the examinee traits satisfy the monotonicity assumption on the question. Next, we calculate the average DOC as the measurement of the explainability of examinee traits, i.e., $\overline{DOC} = \frac{1}{M}\sum_{l=1}^{M}DOC(e_l)$. The $\overline{DOC}$ is also in $[0,1]$. The higher the $\overline{DOC}$, the stronger the explainability of examinee traits.
\par Next, to explore the explainability overfitting problem of CDMs, we aim to compare $\overline{DOC}$ on test data with $\overline{DOC}$ on training data for different CDMs. To this end, we propose the Rate of Explainability Overfitting ($REO$) to measure the discrepancy between them. The $REO$ is defined as follows:

\vspace{-5pt}
\begin{equation}\label{eq:reo}
  REO(\mathcal{D}_{train}, \mathcal{D}_{test}) = 1 - \frac{\overline{DOC}(\mathcal{D}_{test})}{\overline{DOC}(\mathcal{D}_{train})},
  \vspace{-5pt}
\end{equation}
where $\mathcal{D}_{train}, \mathcal{D}_{test}$ denotes training data and test data respectively. The $REO$ indeed evaluates the rate of discrepancy between $\overline{DOC}(\mathcal{D}_{test})$ and $\overline{DOC}(\mathcal{D}_{train})$ to $\overline{DOC}(\mathcal{D}_{train})$. The $REO$ is generally in $[0,1]$. \textbf{The larger the $REO$, the more serious the explainability overfitting problem of a CDM is.}

\par We evaluate the explainability of score-prediction-based CDMs (DINA, NCDM, CDMFKC), encoder-decoder models (U-AutoRec, CDAE) and ID-CDM. We also conduct an ablation study where we remove the monotonicity condition of ID-CDM to get ID-CDM-nMono to explore the impact of the monotonicity condition on the identifiability of ID-CDM. IRT and MIRT are excluded in this experiment because they cannot generate knowledge concept-wise examinee traits. We further evaluate the explainability overfitting of CDMs (DINA, NCDM, CDMFKC, ID-CDM). The experimental results are shown in Table \ref{tab:doc} and Figure \ref{fig:reo}. From Table \ref{tab:doc}, we first observe that the $\overline{DOC}$ of encoder-decoder models is always lower than that of CDMs, which means that traditional encoder-decoder models are incapable for diagnosing explainable examinee traits. On the other hand, the $\overline{DOC}$ of ID-CDM is always higher than that of baselines, which illustrates that ID-CDM has the state-of-the-art explainability of examinee traits. In addition, the $\overline{DOC}$ of ID-CDM-nMono is much lower than that of ID-CDM in all cases. This observation demonstrates the decisive impact of the monotonicity condition of ID-CDM on the explainability of examinee traits. Next, from Figure \ref{fig:reo}, we can observe that the $REO$ of ID-CDM is significantly lower than other baseline CDMs, which means that the discrepancy between the explainability of ID-CDM on test data and training data is much smaller than that of baselines. As a result, ID-CDM indeed alleviates the explainability overfitting of score-prediction-based CDMs.

\vspace{-5pt}
\subsection{Examinee Score Prediction (RQ3)}\label{sec:exp-accuracy}
\par In cognitive diagnosis, it is hard to directly evaluate the accuracy of diagnostic outputs. A common solution is to evaluate the response score prediction performance of CDMs to indirectly assess the accuracy of diagnostic outputs. To this end, we evaluate the score prediction performance of models from both the classification aspect and the regression aspect. We utilize Accuracy (ACC), F1-score (F1), Rooted Mean Square Error (RMSE) as the evaluation metrics. The classification threshold is $0.5$. To guarantee fairness, we utilize the diagnostic output (the encoder output) of ID-CDM from the training data rather than the test data to predict examinees' performance in the test data.
\par The experimental results are shown in Figure \ref{fig:prediction}. We can observe from Figure \ref{fig:prediction} that the performance of ID-CDM on examinee score prediction exceeds the performance of baselines in most cases. Actually, ID-CDM utilizes neural networks to learn the complicated diagnostic process, it can also capture the complex interaction between examinees and questions similar to NCDM and CDMFKC. This property ensures the preciseness of diagnostic results of ID-CDM while keeping the identifiability and explainability.
 

\vspace{-5pt}
\subsection{Examinee traits clustering (RQ4)}\label{sec:exp-case}
\par To explore the statistical features of diagnostic results of ID-CDF, we adopt a widely-used dimension reduction algorithm UMAP \cite{mcinnes2020umap} to visualize examinee traits diagnosed by ID-CDM. Then we color points with examinee correct rates to study whether examinee traits can be well clustered according to correct rates. The experimental result is shown in Figure \ref{fig:cluster}. We can observe from the figure that the distributions of examinee trait points are ribbon-like in all of the datasets. Moreover, the position of points on the extension direction of these ribbon-like distribution statistically reflects the examinee correct rates, and the direction is locally linear. For instance, in Math2, the more a point lies in the right-bottom part of the ``ribbon'', the lower the correct rate of the point. These observations evidently demonstrate that the distribution of examinee traits diagnosed by ID-CDM is highly consistent with the distribution of examinee correct rates. This means that there is a strong correlation between the examinee traits inferred from the model and their actual performance, as measured by the correct rates on the exam questions. As a result, examinees' performance can be well discriminated by the relative position of their traits in the full distribution of examinee traits. Indeed, this interesting property is unique to ID-CDF compared to other CDMs and encoder-decoder models. A comparison to baselines is available at Appendix \ref{app:etc}.


\vspace{-5pt}
\section{Conclusion}
\par In this paper, to overcome the non-identifiability problem and the explainability overfitting problem in cognitive diagnosis, we proposed a response-proficiency-response paradigm and an identifiable cognitive diagnosis framework (ID-CDF) in the paradigm. Spe-cifically, we first designed a diagnostic module to directly diagnose examinee traits and question features from response vectors. By adopting the proposed identifiability condition and monotonicity condition, we ensure the identifiablity an explainability of diagnostic results. Next, we proposed a general predictive module to reconstruct response scores from diagnostic results to ensure the preciseness of the latter. We further gave an implementation of ID-CDF, i.e., ID-CDM, and experimentally validated the performance of ID-CDM on four real-world datasets. We hope this work provides a new perspective in the domain of cognitive diagnosis and inspires further studies in the future.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.

\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}

\newpage

\appendix 
\section{Appendix}
\subsection{the Identifiability of ID-CDF}\label{sec:proof}
% \vspace{-5pt}
\begin{theorem}\label{theorem:identifiability}
  If the identifiability condition in definition \ref{def:id} is satisfied, then diagnostic results of ID-CDF are always identifiable.
\vspace{-5pt}
\end{theorem}

\begin{proof}
Suppose $\bm{x}^{(s)}_i$ and $\bm{x}^{(s)}_j$ are response vectors of examinee $s_i$ and $s_j$ respectively, and $\bm{x}^{(s)}_i = \bm{x}^{(s)}_j$. According to the definition of the diagnostic function $\mathcal{F}(\cdot)$, since there does not exist any external individual-specific factors that influence examinee traits, we can conclude that $\mathcal{F}(x^{(s)}_i;\omega^{(s)}) = \mathcal{F}(x^{(s)}_j;\omega^{(s)})$, i.e., $\theta_i = \theta_j,\forall s_i, s_j\in S$. The identifiability of question features can be proved in the same way.
\end{proof}
% \vspace{-5pt}

\subsection{The Computational Complexity of ID-CDM}\label{sec:complexity} 
\par The computational complexity of ID-CDM consists of two parts, i.e., diagnosis complexity $T_{diag}$ and prediction complexity $T_{pred}$. 
\noindent\par \textbf{Diagnosis Complexity}.Given $N$ examinees, $M$ questions, $K$ knowledge concepts, $H$ hidden layers, and the dimension of a hidden layer as $D$. Then there are $O(N\cdot D + M\cdot D)$ calculations in input layers, $O(H\cdot D^2)$ in hidden layers, and $O(D\cdot K)$ calculations in output layers. Given a pair of examinee response logs and question response logs, the diagnosis computational complexity is $T_{diag}=O((N+M)\cdot D + H\cdot D^2 + 2\cdot D\cdot K)$.
\noindent\par \textbf{Prediction Complexity}. Given conditions above, the predictive module of ID-CDM first aggragetes diagnostic results to low dimensional representations by single-layer perceptrons, where the computational complexity is $O(K\cdot D)$. Then the aggregated representations are input to a MLP to reconstruct the response score, where the computational complexity is $O(H\cdot D^2)$. As a result, the prediction computational complexity is $T_{pred} = O(K\cdot D + H\cdot D^2)$.


\subsection{A Summary of Datasets}\label{app:data}
\par A summary of the four real-world datasets is shown in Table \ref{tab:dataset}.

\begin{table}[t]\centering 
  \caption{Dataset summary.}\label{tab:dataset}\vspace{-0.4cm}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{l | r r r r}
    \toprule
    Statistics & ASSIST & Algebra & Math1 & Math2\\
    \midrule
    % Source of data & online test & online test & offline exam & offline exam \\
    \# Examinees &  4,163 & 1,336 & 4,209 & 3,911 \\
    \# Questions & 17,746 & 100,000 & 20 & 20 \\
    \# Knowledge concepts &  123 & 491 & 11 & 16 \\
    \# Response logs & 324,572 & 322,808 & 84,180 & 78,220 \\
    \# KC$^1$ per question & 1.19 & 1.12 & 3.35 & 3.20 \\
    % STD (\#Knowledge concepts per question) & 0.47 & 0.36 & 1.31 & 0.87 \\
    \# Answers per examinee & 107.26 & 259.49 & 20.0 & 20.0 \\
    % STD (\#Response logs per examinee) & 155.25 & 293.97 & 0.0 & 0.0 \\
    Correct rate & 0.654 & 0.795 & 0.424 & 0.415 \\
    \bottomrule
    \multicolumn{5}{l}{\footnotesize $^1$``KC'' denotes knowledge concepts.}\\
  \end{tabular}
  }
  \vspace{-10pt}
\end{table}

\subsection{Case Study}
\par We present an examinee's diagnostic results in Math1 to show the feature of ID-CDM, as shown in Figure \ref{fig:case}. In the right part of the figure, NCDM (Shadow) denotes diagnostic results of NCDM on the shadow examinee with the same response score distribution. We have several analysis of the diagnostic results. First, both the diagnostic results of ID-CDM and NCDM are explainable. It should be noticed that the order of correct rates is not rigorously consistent with the order of knowledge proficiencies, because the difficulty of questions on different knowledge concepts are not exactly equal. Second, the diagnostic result of ID-CDM is more discriminated than that of NCDM. The range of diagnostic results of ID-CDM is approximately from 0.5 to 1.0, while that of NCDM is approximately from the 0.4 to 0.6. The low variance of NCDM is consistent with the observation in \cite{WangF2022}. Third, the non-identifiability of NCDM. Comparing NCDM and NCDM (Shadow), their diagnostic results are not equal.

\begin{figure}[t]
  \centering 
  \subfloat[Correct rates on K1$\sim$K5]{
    \includegraphics[width=0.55\linewidth]{pic/correct_rate.pdf}
  }
  \subfloat[Diagnostic results on K1$\sim$K5]{
    \includegraphics[width=0.4\linewidth]{pic/diagnostic_results_radar.pdf}
  }
  \vspace{-5pt}
  \caption{Examinee diagnostic results in Math1.}\label{fig:case}
  \vspace{-10pt}
\end{figure}

% ABCabc123

\subsection{Examinee Traits Clustering}\label{app:etc}
\par Detailed examinee traits clustering results are shown in Figure \ref{fig:cluster_a1}, Figure \ref{fig:cluster_a2}, Figure \ref{fig:cluster_a3} and Figure \ref{fig:cluster_a4}. A summarization of the statistical features of these examinee traits is listed as follows.
\par \textbf{ID-CDM and ID-CDM-nMono (Figure \ref{fig:cluster_a1})}. In ID-CDM and ID-CDM-nMono, the distribution of examinee traits is highly correlated with the distribution of correct rates. If we view correct rates as labels (0.5 as the threshold), then examinees are linearly separatable. Furthermore, comparing the result of ID-CDM and ID-CDM-nMono, the monotonicity condition actually tightens the distribution of examinee traits in the orthogonal direction of the changing of correct rates, which enhances the correlation between the distribution of examinee traits and correct rates.
\par \textbf{NCDM and CDMFKC (Figure \ref{fig:cluster_a2})}. In NCDM and CDMFKC, the distrbution of examinee traits is partially correlated with the distribution of correct rates. In Math1 and Math2 dataset, examinees are also linearly separatable if we view correct rates as labels. However, the shape of the distribution of examinee traits in the four datasets is irrelevant to the examinee traits. Moreover, in ASSIST and Algebra, examinee traits are not linearly separatable, and some points with negative labels (i.e., correct rate $<$ 0.5) have been mixed with others with positive labels. Comparing Figure \ref{fig:cluster_a2} and Figure \ref{fig:cluster_a1}, we conclude that the diagnostic module of ID-CDF enables CDMs to learn the correlation between the shape of the distribution of examinee traits and the distribution of correct rates, which enhances the discrimination ability of CDMs.
\par \textbf{DINA and MIRT (Figure \ref{fig:cluster_a3})}. In DINA and MIRT, the distribution of examinee traits is almost uncorrelated with the distribution of correct rates except for results of DINA in Math1 and Math2. Although DINA can diagnose knowledge concept-wise examinee traits, and its logistic-like interaction function is intrinsically explainable, the dichotomity of examinee traits limits the ability of DINA to capture the correlation between examinee traits and response patterns in large-scale data that consists of hundreds of knowledge concepts and hundreds of thousands of response logs, such as ASSIST and Algebra. As for MIRT, the distribution of examinee traits is irrelevant to correct rates because MIRT models examinees by low-dimensional latent traits whose components are not knowledge concept-wise thus lack explainability.
\par \textbf{U-AutoRec and CDAE (Figure \ref{fig:cluster_a4})}. In U-AutoRec and CDAE, the distribution of examinee traits is partially correlated with the distribution of correct rates, which demonstrates the effectiveness of the encoder-decoder structure in capturing the distribution examinee traits. However, compared to results of ID-CDM and ID-CDM-nMono in Figure \ref{fig:cluster_a1}, examinee traits of U-AutoRec and CDAE are not linearly separatable, and the shape of these distributions are not well correlated with correct rates. These results indicate that the seperately designed examinee and question diagnostic modules and the monotonicity condition of ID-CDM effectively facilitates its ability to capture the correlation between examinee traits and correct rates, which actually makes diagnostic results of ID-CDM more feasible.


\begin{figure*}[hb]
  \centering
  \subfloat[Examinee traits diagnosed by ID-CDM (our proposal).]{
    
      \includegraphics[width=0.85\linewidth]{pic/umap_idcdm.pdf}
    }
  

  \subfloat[Examinee traits diagnosed by ID-CDM-nMono (ablation study).]{
      \includegraphics[width=0.85\linewidth]{pic/umap_idcdm-nmono.pdf}
  }
  \caption{Examinee traits clustering (Part 1). Each point denotes an examinee's traits, colored by his/her correct rate.}\label{fig:cluster_a1}
\end{figure*}

\begin{figure*}[hb]
  \centering
  \subfloat[Examinee traits diagnosed by CDMFKC.]{
      \includegraphics[width=0.85\linewidth]{pic/umap_cdmfkc.pdf}
  }

  \subfloat[Examinee traits diagnosed by NCDM.]{
      \includegraphics[width=0.85\linewidth]{pic/umap_ncdm.pdf}
  }
  
  \caption{Examinee traits clustering (Part 2). Each point denotes an examinee's traits, colored by his/her correct rate.}\label{fig:cluster_a2}
\end{figure*}

\begin{figure*}[hb]
  \centering
  \subfloat[Examinee traits diagnosed by DINA.]{
      \includegraphics[width=0.85\linewidth]{pic/umap_dina.pdf}
  }

  \subfloat[Examinee traits diagnosed by MIRT.]{
      \includegraphics[width=0.85\linewidth]{pic/umap_mirt.pdf}
  }
  
  \caption{Examinee traits clustering (Part 3). Each point denotes an examinee's traits, colored by his/her correct rate.}\label{fig:cluster_a3}
\end{figure*}

\begin{figure*}[hb]
  \centering
  \subfloat[Examinee traits diagnosed by U-AutoRec.]{
      \includegraphics[width=0.85\linewidth]{pic/umap_uautorec.pdf}
  }

  \subfloat[Examinee traits diagnosed by CDAE.]{
      \includegraphics[width=0.85\linewidth]{pic/umap_cdae.pdf}
  }

  \caption{Examinee traits clustering (Part 4). Each point denotes an examinee's traits, colored by his/her correct rate.}\label{fig:cluster_a4}
\end{figure*}

%%
% %% If your work has an appendix, this is the place to put it.
% \appendix

% \section{Appendix.A}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
