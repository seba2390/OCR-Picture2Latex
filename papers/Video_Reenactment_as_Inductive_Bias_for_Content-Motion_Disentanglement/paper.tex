% !TeX encoding = UTF-8
% !TeX spellcheck = en_US
% !BIB program=biber
\PassOptionsToPackage{dvipsnames}{xcolor}
\documentclass[journal]{IEEEtran}




\usepackage{etoolbox}

\usepackage{xr}
\externaldocument{supplementary}

\usepackage[%
style=numeric-comp,%
sortcites=true,%
sorting=none,%
url=false,%
doi=false,%
eprint=false,%
isbn=false,%
]{biblatex}
\addbibresource{abrv.bib}
\addbibresource{paper.bib}


\usepackage{graphicx}
\graphicspath{{./img/}}
\usepackage[labelformat=simple]{subcaption}
\captionsetup{font=footnotesize, labelfont={bf}, labelsep=period}
\renewcommand\thesubfigure{(\alph{subfigure})}
\renewcommand\thesubtable{(\alph{subtable})}

\newcommand{\boximg}[2]{\parbox[c]{#1\subfigsz}{\includegraphics[width=#1\subfigsz]{#2}}}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\pgfplotsset{compat=1.16,
  grid style=dashed,
  ymajorgrids=true,
}

\newlength{\subfigsz}

\usetikzlibrary{
  calc,
  shapes,
  arrows,
  shadows,
  positioning,
  decorations.shapes,
  decorations.markings,
  fit,
  bayesnet,
  matrix,
}

\usepgfplotslibrary{
  groupplots,
  colorbrewer,
  statistics,
  fillbetween,
}


\usepackage{booktabs}           %
\usepackage{multirow}
\usepackage{arydshln}           %


\usepackage{siunitx}
\sisetup{
  separate-uncertainty,
  table-format=1.2(3),
  detect-all,
}
\robustify\bf

\makeatletter
\def\adl@drawiv#1#2#3{%
  \hskip.5\tabcolsep
  \xleaders#3{#2.5\@tempdimb #1{1}#2.5\@tempdimb}%
  #2\z@ plus1fil minus1fil\relax
  \hskip.5\tabcolsep}
\newcommand{\cdashlinelr}[1]{%
  \noalign{\vskip\aboverulesep
    \global\let\@dashdrawstore\adl@draw
    \global\let\adl@draw\adl@drawiv}
  \cdashline{#1}
  \noalign{\global\let\adl@draw\@dashdrawstore
    \vskip\belowrulesep}}
\makeatother

\usepackage{pdflscape}


\usepackage{amsmath}
\interdisplaylinepenalty=2500
\usepackage{mathtools}
\usepackage{commath}
\usepackage{amsfonts}           %
\usepackage{nicefrac}           %
\usepackage{microtype}          %

\newcommand{\conc}{\mathbin{\Vert}}


\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\kl}{\operatorname{KL}\infdivx}
\newcommand{\skl}{\operatorname{SKL}\infdivx}

\newcommand{\rpm}{\raisebox{.2ex}{$\scriptstyle\pm$}}

\newcommand\givenbase[1][]{\:#1\lvert\:}
\let\given\givenbase

\newcommand\reallywidehat[1]{\ThisStyle{%
  \setbox0=\hbox{$\SavedStyle#1$}%
  \stackengine{-1.0\ht0+.5pt}{$\SavedStyle#1$}{%
    \stretchto{\scaleto{\SavedStyle\mkern.15mu\char'136}{1.5\wd0}}{1.4\ht0}%
  }{O}{c}{F}{T}{S}%
}}
\newcommand{\hkl}{\widehat{\operatorname{KL}}\infdivx}

\newcommand{\CI}{\mathrel{\perp\mspace{-10mu}\perp}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\makeatletter
\DeclareRobustCommand\bigop[1]{%
  \mathop{\vphantom{\sum}\mathpalette\bigop@{#1}}\slimits@
}
\newcommand{\bigop@}[2]{%
  \vcenter{%
    \sbox\z@{$#1\sum$}%
    \hbox{\resizebox{\ifx#1\displaystyle.9\fi\dimexpr\ht\z@+\dp\z@}{!}{$\m@th#2$}}%
  }%
}
\makeatother

\DeclareMathOperator*{\E}{\mathbb{E}}

\let\oldtimes\times
\def\times{{\mkern1mu\oldtimes\mkern1mu}}

\makeatletter
\DeclareRobustCommand{\rvdots}{%
  \vbox{
    \baselineskip4\p@\lineskiplimit\z@
    \kern-\p@
    \hbox{.}\hbox{.}\hbox{.}
}}
\makeatother


\usepackage{algorithm}
\usepackage{algorithmic}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\hypersetup{
  colorlinks,
  linkcolor = BrickRed,
  citecolor = NavyBlue,
  urlcolor  = WildStrawberry,
}
\usepackage{url}


\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{{e.g}\onedot} \def\Eg{{E.g}\onedot}
\def\ie{{i.e}\onedot} \def\Ie{{I.e}\onedot}
\def\cf{{cf}\onedot} \def\Cf{{Cf}\onedot}
\def\etc{{etc}\onedot} \def\vs{{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{{et al}\onedot}  \def\aka{a.k.a\onedot}
\def\adhoc{{ad hoc}\xspace}
\makeatother


\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}

\title{Video Reenactment as Inductive Bias for Content-Motion Disentanglement}

\author{J.~F. Hernández~Albarracín,
        and A. Ramírez~Rivera,~\IEEEmembership{Senior Member,~IEEE}%
\thanks{%
Juan F. Hernández Albarracín is with Institute of Computing, University of Campinas, SP, Brazil, e-mail \texttt{juan.albarracin@ic.unicamp.br}.  Adín Ramírez Rivera is with Department of Informatics, University of Oslo, Norway, e-mail \texttt{adinr@uio.no}; and part of this work was done at the Institute of Computing, University of Campinas.}%
\thanks{Juan F. Hernández Albarracín was funded by the São Paulo Research Foundation (FAPESP) under grant No.~2017/16144-2.  A. Ramírez Rivera was funded by the Brazilian National Council for Scientific and Technological Development (CNPq) under grant No.~307425/2017-7; and in part by FAPESP under grant No.~2019/07257-3. Juan F. Hernández Albarracín and A. Ramírez Rivera were funded by the Coordena\c{c}\~ao de Aperfei\c{c}oamento de Pessoal de N\'ivel Superior---Brasil (CAPES)---Finance Code 001.}%
\thanks{The source code is available at \url{https://gitlab.com/mipl/mtc-vae}.}
%\thanks{This paper has supplementary downloadable material available at \url{http://ieeexplore.ieee.org}, provided by the author.  The material includes a supplementary document with additional results. Contact \texttt{adinr@uio.no} for further questions about this work.}
\thanks{Pre-print to appear in IEEE Trans.\ on Image Processing.}
\thanks{Digital Object Identifier \href{https://doi.org/10.1109/TIP.2022.3153140}{10.1109/TIP.2022.3153140}.}
}

\markboth{IEEE Transactions on Image Processing}%
{Hern\'andez~Albarrac\'{i}n \& Ram\'{i}rez~Rivera: Video Reenactment as Inductive Bias for Content-Motion Disentanglement}


\maketitle

\begin{abstract}
Independent components within low-dimensional representations are essential inputs in several downstream tasks, and provide explanations over the observed data.
Video-based disentangled factors of variation provide low-dimensional representations that can be identified and used to feed task-specific models.
We introduce MTC-VAE, a self-supervised motion-transfer VAE model to disentangle motion and content from videos.
Unlike previous work on video content-motion disentanglement, we adopt a chunk-wise modeling approach and take advantage of the motion information contained in spatiotemporal neighborhoods.
Our model yields independent per-chunk representations that preserve temporal consistency.
Hence, we reconstruct whole videos in a single forward-pass.
We extend the ELBO's log-likelihood term and include a Blind Reenactment Loss as an inductive bias to leverage motion disentanglement, under the assumption that swapping motion features yields reenactment between two videos.
We evaluate our model with recently-proposed disentanglement metrics and show that it outperforms a variety of methods for video motion-content disentanglement.
Experiments on video reenactment show the effectiveness of our disentanglement in the input space where our model outperforms the baselines in reconstruction quality and motion alignment.
\end{abstract}

\begin{IEEEkeywords}
Disentangled representations, Video reenactment, Variational inference, Generative models, Self-supervised learning.
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Introduction}
\label{sec:introduction}

\IEEEPARstart{W}{hile} the goal of representation learning is to obtain low-dimensional vectors useful for a diverse set of tasks, Disentangled Representation Learning (DRL) captures independent factors of variation within the observed data.
These disentangled representations are robust and interpretable, simplify several downstream tasks like classification and Visual Question Answering~\cite{Locatello2020aaai}, and support diverse content generation tasks~\cite{Chen2020tip,Ramesh2021}.
DRL shifted from unsupervised to weakly- and self-supervised methods, as inductive biases have shown to be fundamental in Deep Generative Models (DGM)~\cite{Locatello2019, Shu2020}.
DRL methods from video separate \emph{time independent} (\aka content) from \emph{dependent} (\aka motion) factors of variation.
While content features must be forced to have a low variance throughout the sequence, motion ones are expected to change.

Disentangling information from videos is of major importance since it can ease tasks that depend on the spatiotemporal data.
For instance, prediction tasks could rely on the independent representations of the objects or only on their temporal information.
These independence could not only ease the load on the downstream tasks but also enforce fairness and privacy over the data.
DRL from videos has been approached as a sequential learning process forcing temporal consistency among frames.
This problem is commonly addressed with Recurrent Neural Networks (RNN), due to their capacity of modeling temporal data of variable length.
Although architectures based exclusively on 3D Convolutional Neural Networks (3D-CNN) have been used in general representation learning from videos for downstream tasks~\cite{Carreira2017, Feichtenhofer2019}, few works rely only on convolutional architectures for DRL and posterior video generation~\cite{Wang2020, Aich2020}, despite their capacity of modeling whole videos, as they are constrained to fixed-length sequences.

Taking into account the great suitability of Variational Autoencoders (VAE) for unsupervised tasks~\cite{Su2020,Shi2018},
we propose a self-supervised DRL model that takes advantage of local spatio-temporal regularity to reconstruct videos by disentangling their content and motion while learning a robust representation space.
Motion-Transfer Chunk Variational Autoencoder (MTC-VAE) is a Variational Autoencoder that models temporal segments (\aka chunks) as independent random variables, maps them into a disentangled latent distribution, and maps them back consistently.
When modeling chunks as independent, the reconstructed videos may not be temporally consistent.
Hence, we preserve the temporal dependency that naturally exists among the chunks by assuming a Markovian relation between consecutive chunks at inference time.
To enforce it, we incorporate two inductive biases in our model:
(i)~We assume content features as stationary and motion ones as non-stationary in our model's log-likelihood.
(ii)~Video Reenactment (VR) is equivalent to swapping the motion representation of two videos and mapping them to the input space.
We show that this duality (independence at generation time, and dependence at inference time) is successful at representing video sequences for both disentanglement and reconstruction.

Our contributions are: (i)~A self-supervised DGM for VR and content-motion disentanglement from arbitrary-length videos through a simple 3D-CNN architecture in a single forward pass, improving over existing methods.
(ii)~Even assuming chunk independence, we significantly ease the disentangled motion-content feature inference and consistent video reconstruction, due to our inductive biases, and the self-supervised representation learning scheme.
(iii)~We show, that chunk-wise is better suited for DRL and video synthesis than frame-wise modeling for long videos.
Moreover, we highlight that, unlike SotA VR models, MTC-VAE is suited to learn disentangled low-dimensional representations.
VR models rely on entangled high-dimensional features and bypass information through the architecture to achieve better reconstruction at the cost of bloated features.
In contrast, our objective is to obtain independent factors of variation that are expressive enough for simple generators to create natural videos.

\section{Related Work}
\label{sec:soa}

\subsection{General Disentangled Representation Learning}
Seminal works on DRL are mostly unsupervised, and the majority rely on VAEs.
InfoGAN~\cite{Chen2016}, however, is the most relevant exception.
It uses control variables (categorical, discrete, or continuous) in the latent representation as inductive biases while penalizing mutual information among the latent units in an adversarial framework.
$\beta$-VAE~\cite{Higgins2017} includes the $\beta$ hyper-parameter into the VAE's ELBO to leverage independence among the latent scalars, leading to a higher-quality disentanglement.
Later approaches (\eg, $\beta$-TCVAE~\cite{Chen2018dr} and FactorVAE~\cite{Kim2018}) penalize Total Correlation among the latent scalars, yielding a better trade-off between disentanglement and reconstruction quality.
The ground-breaking work by \textcite{Locatello2019} showed that unsupervised methods for DRL are extremely weak.
Posterior works have shifted to weakly- and self-supervised approaches.
Hence, our proposed MTC-VAE introduces inductive biases in the latent space, such as explicit latent factors to represent content and motion features, with sufficient encoded information to guarantee VR from them.

\subsection{Disentangled Representations from Video}
\label{sec:soa_vdrl}
These works focus on disentangling time-dependent from time-independent features for each frame of the video and then enforcing inter-frame consistency.
Common setups of these approaches perform pose-content disentanglement while achieving consistency using RNNs and GANs~\cite{Denton2017, Villegas2017, Hsieh2018, Ge2018}.
Instead of pose-content disentanglement, some works separate deterministic from stochastic features~\cite{Denton2018, Lee2019savp}.
Most of the works in this area are applied to video prediction, but recent ones have started to be tested on VR tasks~\cite{Li2018,Wang2020,Aich2020,Zhu2020}.
Few of them \cite{Wang2020, Aich2020} rely on 3D-convolutional generators, but are constrained to fixed-length videos.
The rest use RNNs to capture the temporal relation between frames or segments at generation time, to perform either video reconstruction, prediction, or sequence-to-sequence translation.
Although MTC-VAE models dependent chunks at inference time, it assumes independence at generation time.
These assumptions simplify the tasks of reconstruction and VR since, to reconstruct a chunk of a video, it does not need to reconstruct the previous ones.
Therefore, the chunkwise approach takes the best of both worlds at not being constrained either to fixed-length-sequences or sequential generation.

\subsection{Video Reenactment}
\label{sec:soa_vr}
Recent methods on VR work in the domain of human faces~\cite{Zakharov2019, Nirkin2019, Chen2018, Zhou2019aaai}, human poses~\cite{Chan2019, Zhou2019, Liu2019tg, Yang2020}, or objects in general~\cite{Bansal2018, Siarohin2019, Siarohin2021, Zhao2018, Xie2020}.
Their main objective is to generate realistic videos, while the representation is either irrelevant or a secondary objective.
Instead, DRL models hold this objective as primary.
Most of these methods rely on warping techniques assisted by spatial transformer networks~\cite{Jaderberg2015} for frame-wise conditional video generation.
To apply such transformations, the generator requires high-dimensional spatial information that would normally be lost in a low-dimensional latent representation.
Hence, they either map to latent spaces that are larger than the original input space, to preserve spatial information, or bypass this information through skip connections from the encoder to the decoder.
Thus, a low-dimensional latent representation is not enough to represent the whole video.
In contrast, our proposal reconstructs videos while learning low-dimensional and factorized representations.
We highlight that our method reconstructs videos exclusively from low-dimensional representations.
Due to this restriction, we expect the perceptual quality and motion complexity of rendered videos to be higher in VR methods in comparison to DRL ones.
Despite this limitation, we consider our work as a step towards bridging these two areas.

\section{Proposed Approach: MTC-VAE}
\label{sec:method}

Given that content changes at a much slower rate than motion in a video, we propose to extract disentangled representations from local spatiotemporal neighborhoods (\aka chunks).
Content information of neighboring chunks changes so slowly that we may assume that it remains constant throughout a scene, while motion presents rapid changes.
Unlike existing frame-wise approaches, we use chunks to better capture the temporal characteristics of the video (\cf Section~\ref{sec:ablation} for the impact of the temporal windows), and their relations to obtain a self-supervised learning signal.

MTC-VAE contains only 3D-convolutional streams and, unlike recurrent approaches, models chunks as independent random variables for the generative pass, yet Markovian-dependent for the inference one.
Our formulation starts diverging from a standard two-latent-priors VAE when we extend our $\log p(x)$ to leverage inter-chunk consistency, which helps to reconstruct realistic videos, even though chunks are independently generated.
We go further and introduce the self-supervised \textit{blind reenactment loss} (BRL): another inductive bias that blindly simulates VR between two videos.

\subsection{Chunk-wise Video Modeling}
\label{sec:modeling}

We represent the video $x = (x_{k})_{k=1}^K$ as a sequence of $K$ non-overlapping and equally-sized chunks~$x_{k}$ of length $c$.%
\footnote{%
  For brevity, we assume that $c$ divides the length of the video.
  However, we can model arbitrary-length videos by padding incomplete chunks to match $c$.}
Similarly, we define $w = \left(w_k\right)_{k=1}^K$ as the sequence of motion representations of each~$x_k$.
For the $k$-th chunk, we model the content and motion as independent latent variables $z$ and $w_k$, respectively.
We assume $z$ to be unique and shared across the chunks, as content remains constant through time.
Fig.~\ref{fig:graphical_model} depicts the graphical model for a video~$x$.
\begin{figure}%
  \centering%
  \resizebox{0.5\linewidth}{!}{\input{img/gm_vae}}
  \caption{%
  In the generative model (solid arrows), $K$ chunks $\{x_{k}\}$ (observed) share the same content~$z$, while having their own motion~$w_k$.
During inference (dashed arrows), the latent variables $z$ and~$w_k$ are inferred from each chunk, while each chunk~$x_k$ also depends on the previous one.
  }
  \label{fig:graphical_model}%
\end{figure}

Different from common frame-wise approaches, where~$w$ normally depend on previous frames, in the generative phase, we model all the motion representations $\{w_k\}$ as independent random variables.
This assumption simplifies the generation process since it lets us generate a particular chunk without having to consider the previous ones in the video.
A unique $z$ for all the chunks sets an implicit dependence of each chunk to the whole video in the inference phase of the model.

Being the chunks independent, the joint probability of the model is the product of the conditionals of each chunk and their latent variables, \ie,
\begin{equation}
p(x, w, z) = p(z)\prod_k p(x_{k} \given w_k, z)p(w_k).
\end{equation}
We model the generative process of a single chunk through a VAE~\cite{Kingma2013}, with content encoder $q_\phi(z \given x_{k})$, motion encoder $q_\gamma(w_k \given x_{k})$, and decoder $p_\theta(x_{k} \given w_k, z)$ with parameters ($\phi$, $\gamma$, $\theta$), updated to maximize of the evidence lower bound (ELBO) of the expected log-likelihood
\begin{align}
\argmax_{\phi, \gamma, \theta} \E_{\tilde{q}(x_{1:k})} \sum_k \Big\lbrace & \E_{q_\phi}\E_{q_\gamma} \left[\log p_\theta(x_k \given w_k, z) \right] \nonumber \\
& - \kl{q_\gamma(w_k \given x_k)}{p(w_k)} \nonumber \\
& - \kl{q_\phi(z \given x_k)}{p(z)} \Big\rbrace.
\label{eq:elbo_whole}
\end{align}
Fig.~\ref{fig:logp} shows the pipeline to calculate the ELBO~(\ref{eq:elbo_whole}).
We maximize the expected reconstruction loss over the two latent variables \wrt their distributions $q_\phi(z \given x_{k})$ and $q_\gamma(w_k \given x_{k})$ (first term), and minimize the Kullback-Leibler divergence between these distributions \wrt their priors.
We compute their expected value \wrt the empirical distribution of the chunks $\tilde{q}(x_{1:k}) = \prod_k q(x_{k} \given x_{k-1})$ that models a Markovian temporal relation between them.\footnote{We assume the first chunk to be distributed through $q(x_1 \given x_0) \equiv q(x_1)$ to simplify the notation.}
We approximate the chunk distribution through a sampling process on the videos, and model all prior distributions as standard Gaussians.
To generate a new video from the chunk posterior, we concatenate the expected values of the chunk posteriors, directly provided by the decoder.
See Appendix~\ref{sec:elbo} for further detail and proof of our formulation.

\begin{figure}[tb]%
  \centering%
  \resizebox{\linewidth}{!}{\input{img/logp}}
 \caption{%
  We feed consecutive chunks $\{x_k\}_{k=1}^O$ to the encoders $q_\phi$ and $q_\gamma$, yielding their representations, $\{w_k\}_{k=1}^O$ and $\{z_j\}_{j=1}^O$.
  We concatenate all combinations of $z_j$'s and $w_k$'s, and decode them to obtain the p.d.f.\ parameters $\rho_{j,k}$ for the $k$-th chunk posteriors $p_\theta(x_k \given w_k, z_j)$.
  Every posterior from $w_k$ must generate $x_k$.
  We maximize the log-likelihood of each chunk under the corresponding set of posteriors.
  Chunk posteriors \textcolor{red}{relate} with the original chunks through $\mathcal{L}_r$.
  The latent prior distributions \textcolor{blue}{relate} through $\mathcal{L}_a$ and $\mathcal{L}_m$.
  We \textcolor{blue!40!green}{sample} from the chunk posterior by applying the Sigmoid function to the output of the decoder.
  }
  \label{fig:logp}%
\end{figure}

Our architecture consists of two encoders $q_\phi(z \given x_{k})$ and $q_\gamma(w_k \given x_{k})$, and one decoder $p_\theta(x_{k})$\@.
All of them have five 3D-convolutional layers, with Batchnorm and ReLU activations.
The number of filters in the hidden layers of the decoder is double the number of filters in the encoders.

\subsection{Inter-Chunk Consistency}
\label{sec:inter-chunk-consistency}

As shown in Equation~\ref{eq:elbo_whole}, we can train a VAE to independently reconstruct chunks.
However, the independence assumption at generation time may cause the videos to not be smoothly rendered between chunks.
To solve this issue, we force our model to yield a unique content representation~$z$, regardless of the chunk from which it is inferred.

We part from the assumption that content is constant throughout the video, and so does its latent representation $z \sim q_\phi(z \given x_{k})$---\cf Section~\ref{sec:modeling}.
To force our model to learn this constraint, we train it to maximize $\log p_\theta(x_{k} \given w_k,z_j)$ for every~$j$, \ie, maximize the log-likelihood of a chunk $x_{k}$ given its own motion~$w_k$ and any~$z_j$ content representation---\cf Fig.~\ref{fig:logp}.
We extend the $\log p(x_{k} \given  w_k, z)$ term~(\ref{eq:elbo_whole}) to fulfill this constraint.
So our final reconstruction loss is
\begin{equation}
\label{eq:logp_x}
\mathcal{L}_{r}(\theta, \phi,\gamma) = \sum_{k=1}^O\sum_{j=1}^O \left[\log p_\theta(x_{k} \given w_k,z_j)\right],
\end{equation}
where $z_j \sim q_\phi(z \given x_{j})$, $w_k \sim q_\gamma(w_k \given x_{k})$, and $O$ is defined as the \textit{order of the model} that restricts the number of chunks used to calculate the loss.
As Fig.~\ref{fig:logp} shows, the decoder outputs the distribution parameters $\rho_{j,k}$ of each chunk likelihood $p_\theta(x_{k} \given w_k,z_j)$, used in $\mathcal{L}_{r}$.
Due to its combinatory nature, it is impractical to apply $\mathcal{L}_r$ to all the chunks.
Hence, for each forward pass, we consider only a sequence of $O \le K$ consecutive chunks of $x$, starting at a random frame.

The second and third terms of the expected log-likelihood~(\ref{eq:elbo_whole}) correspond to the regularization terms of the motion and content distributions, respectively.
That is, we compute
\begin{align}
  \label{eq:lm}
  \mathcal{L}_{m}(\gamma)&= -\sum_{k=1}^O \kl{q_\gamma(w_k \given x_k)}{p(w_k)}, \text{ and} \\
  \label{eq:la}
  \mathcal{L}_{a}(\phi)&= -\sum_{k=1}^O \kl{q_\phi(z \given x_k)}{p(z)},
\end{align}
on $O$ consecutive chunks instead of the whole video---\cf Fig.~\ref{fig:logp}.

Unlike other variational inference methods of grouped observations~\cite{Locatello2020,Mathieu2016,Bouchacourt2018,Hosoya2019}, we opted for the extended log-probability term~(\ref{eq:logp_x}), considering different combinations of appearance features, to yield stronger gradients for chunk-consistency, instead of averaging the shared representations in the group.

\subsection{Blind Reenactment Loss}
\label{sec:brl}

Our proposed Blind Reenactment Loss (BRL) loss increases the likelihood $\log p(x_{k} \given w_k, z)$ of our ELBO given any encoded chunks.
It aims at leveraging content-motion disentanglement by doing VR between a source video~$S$ and a driving video~$D$.
The motion representation of $S$ is replaced by the one of $D$, to reconstruct a reenacted video with the object of interest from $S$ moving like the one in $D$.
This translation can be achieved uniquely if the content and motion representations of both videos are disentangled.
The main difficulty is that, in principle, we would need to train our model with ground-truth reenacted videos.
However, we opt for self-supervised training and take advantage of our chunk-based approach.

Consider two chunks $s_i$ and~$s_j$ from $S$, and one chunk $d_l$ from $D$.
Assuming constant content throughout the video, if we independently reenact $s_i$ and~$s_j$ \wrt $d_l$, the two reconstructed chunks must be the same since $s_i$ and $s_j$ have the same content.
To achieve this objective, we force the corresponding chunk posteriors $p(x_{k} \given w_k, z)$ to be equivalent, \ie, $p(x \given w_l^d, z_j^s) \equiv p(x \given w_l^d, z_i^s)$, where $z_i^s \sim q(z \given s_i)$, $z_j^s \sim q(z \given s_j)$, and $w_l^d \sim q(w_k \given d_l)$, by minimizing the KL divergence between every two posteriors that fit the described case.
Let
\begin{align}
  \label{eq:brl}
\mathcal{L}_b & (\theta,\phi,\gamma) = \nonumber \\
& -\sum_{l=1}^O\sum_{j=1}^O\sum_{i=1}^O \operatorname{SKL} \Big(p_\theta(x \given  w_l^d,z_j^s) \;\big\| p_\theta(x \given w_l^d,z_i^s) \Big).
\end{align}
be our BRL, where $\skl{P}{Q}= \frac{1}{2}(\kl{P}{Q} + \kl{Q}{P})$ is a symmetrical operator.
This loss involves two empirical distributions of unobservable samples, so we are not aware, at training time, of whether the sampled videos are correctly reenacted.
If there is disentanglement, posteriors sharing the same motion of $D$ and \textit{any} content of $S$ must be equivalent, regardless of their samples.

The BRL must be optimized along with $\mathcal{L}_r$~(\ref{eq:logp_x}) to prevent posterior collapse.
Notice that, if $O = 1$, then $j=i=1$ and $\mathcal{L}_b=0$, so this objective can only be optimized for $O \geq 2$.

\subsection{General Loss Function}

We define the general objective to be maximized as
\begin{equation}
\mathcal{L} = \mathcal{L}_r + \lambda\mathcal{L}_b + \beta(\mathcal{L}_a + \mathcal{L}_m),
\end{equation}
where $\beta$ comes from $\beta$-VAE by \cite{Higgins2017}, and $\lambda$ weights $\mathcal{L}_b$\@.
Each element in the batch is conformed by a sequence of $O$ chunks, so $\mathcal{L}$ can be calculated independently for every element.

\section{Experiments}
\label{sec:experiments}

\begin{table*}[tb]
  \caption{Performance for content-motion disentanglement and data realism. (*\,$c=1$)}
  \label{tab:baselines}
  \scriptsize
  \centering
  \resizebox{\linewidth}{!}{%
  \begin{tabular}{cSSSSS[table-format=3.2(4)]cSSSSS[table-format=3.2(4)]}
    \toprule
    &   {FVAE $\uparrow$} & {MIG $\uparrow$} & {SAP $\uparrow$} & {SSIM $\uparrow$} & {FID $\downarrow$}
    & & {FVAE $\uparrow$} & {MIG $\uparrow$} & {SAP $\uparrow$} & {SSIM $\uparrow$} & {FID $\downarrow$} \\
    \cmidrule{2-6} \cmidrule{8-12}
    & \multicolumn{5}{c}{3dShapes} & & \multicolumn{5}{c}{LPC} \\
    \cmidrule{2-6} \cmidrule{8-12}
    $\beta$-TCVAE & .50(2) & \bf .01(1) &     .11(8)  &     .53(10) &    140.25(5113) & &     .81(3) &     .02(02) &     .00(00) &     .64(1) &     80.27(317) \\
    dis-VAE       & .50(0) &     .00(0) &     .08(6)  &     .40(3)  & \bf 71.24(1235) & &     .92(2) &     .02(02) &     .01(01) &     .78(1) &     71.70(176) \\
    SVG-LP        & .50(0) & \bf .01(0) &     .03(02) &     .54(5)  &    136.00(8112) & &     .63(2) &     .00(00) &     .00(00) & \bf .79(2) &     62.75(987) \\
    MTC-VAE       & .50(2) & \bf .01(0) & \bf .41(14) &     .67(6)  &    119.47(5100) & & \bf .93(6) & \bf .11(11) & \bf .60(40) &     .67(1) & \bf 41.72(331) \\
    MTC-VAE*      & .50(1) & \bf .01(0) &     .39(11) & \bf .73(2)  &    100.80(4682) & &     .86(1) &     .00(00) &     .11(03) &     .67(1) &     42.59(409) \\
    \cmidrule{2-6} \cmidrule{8-12}
    & \multicolumn{5}{c}{CK+} & & \multicolumn{5}{c}{MMNIST} \\
    \cmidrule{2-6} \cmidrule{8-12}
    $\beta$-TCVAE &     .79(5) & \bf .03(2) &     .06(4)  &     .50(7)  &    116.74(2470) & &     .66(7) &     .04(4) &     .04(4)  & \bf .71(3) &     152.56(1693) \\
    dis-VAE       &     .71(2) &     .01(1) &     .04(2)  &     .61(5)  &     71.48(309)  & &     .64(4) &     .02(2) &     .03(2)  &     .70(2) &     149.43(973) \\
    SVG-LP        &     .70(6) &     .02(1) &     .04(2)  &     .02(0)  & \bf 38.79(1763) & &     .52(1) &     .01(0) &     .02(2)  &     .58(2) &     179.08(5049) \\
    MTC-VAE       & \bf .86(4) &     .02(1) & \bf .13(5)  &     .66(12) &     63.13(2250) & & \bf .95(4) & \bf .11(7) & \bf .10(5)  &     .68(1) & \bf 102.11(99) \\
    MTC-VAE*      &     .85(2) & \bf .03(1) &     .05(2)  & \bf .68(13) &     76.16(1937) & &     .91(4) &     .09(5) &     .09(4)  &     .69(1) &     186.25(2355) \\
    \cmidrule{2-6} \cmidrule{8-12}
    & \multicolumn{5}{c}{dSprites} & & \multicolumn{5}{c}{MUG} \\
    \cmidrule{2-6} \cmidrule{8-12}
    $\beta$-TCVAE &     .57(3) &     .00(0) &     .04(1) & \bf .79(3) &     79.34(618) & &     .74(5) & \bf .05(4) &     .23(03) &     .51(01) &     44.78(332) \\
    dis-VAE       &     .70(2) &     .01(0) &     .01(0) & \bf .79(0) &     97.07(173) & & \bf .76(3) &     .01(1) &     .11(03) & \bf .78(01) &     62.84(345) \\
    SVG-LP        &     .61(5) &     .00(0) &     .00(0) & \bf .79(2) &     98.14(920) & &     .64(4) &     .02(1) &     .38(04) &     .50(15) &    101.59(3700) \\
    MTC-VAE       &     .91(2) & \bf .04(1) & \bf .10(1) &     .78(0) & \bf 57.18(643) & &     .72(4) &     .01(1) &     .73(05) &     .63(02) & \bf 28.79(115) \\
    MTC-VAE*      & \bf .92(1) &     .02(2) &     .01(0) &     .77(1) &    105.79(586) & &     .70(9) &     .04(2) & \bf .76(10) &     .66(06) &     43.86(1315) \\
    \bottomrule
  \end{tabular}}

\end{table*}

We evaluated MTC-VAE in DRL, VR, and downstream tasks.
Although MTC-VAE does not require labels in training time, we used labels to asses disentanglement, and to split the training and testing datasets.
We detail the implementation of the model and the experimental setup in Appendix~\ref{sec:implementation}.

\textbf{Datasets.}
(i)~Cohn-Kanade (CK+) facial dataset~\cite{Kanade2000, Lucey2010}, (ii)~Liberated Pixel Cup (LPC) sprites, (iii)~Moving MNIST (MMNIST)~\cite{Srivastava2015}, (iv)~Deepmind's dSprites, (v)~Deepmind's 3dShapes, and (vi)~Multimedia Understanding Group (MUG) facial dataset~\cite{Aifanti2010}.
We generated videos from the images of dSprites and 3dShapes, forming sequences of objects moving in linear and curved trajectories, or changing their perspective.
Each dataset contains \num{10000} videos, except for CK+ (\num{320}), LPC (\num{200000}), and MUG (\num{700}).
We report the average model performance in a $5$-fold cross-validation setup ($80\%$ for training and $20\%$ for testing).
Appendix~\ref{sec:data} provides further detail about the datasets, as well as the factors of variation.

\textbf{Baselines.}
We compared our method against the Disentangled Sequential Autoencoder (dis-VAE) \cite{Li2018}, SVG-LP \cite{Denton2018}, and $\beta$-TCVAE \cite{Chen2018dr}.
The first two are frame-wise approaches that disentangle time-dependent from time-independent factors.
Although SVG-LP namely disentangles deterministic from stochastic features, they force the deterministic features to remain constant, while the stochastic ones change from frame to frame, like a content-motion modeling.
$\beta$-TCVAE is an unsupervised disentanglement model, tested so far on images, so we extended it to 3D-CNNs to support chunks.

\textbf{Hyper-parameters.}
After a hyper-parameter search in the models (see details in Appendix~\ref{sec:implementation}), we tuned the $\beta$ parameter and the latent space size.
For dSprites, LPC and MMNIST, $\beta = 1$, and $\beta = 5$ for the other datasets.
Regarding the latent space dimensionality (where each dimension is a \textit{latent unit}), $\text{dim}(z)=14$, $\text{dim}(w_k)=7$ for CK+, LPC, and MUG, $\text{dim}(z)=12$, $\text{dim}(w_k)=6$ for 3dShapes, $\text{dim}(z)=12$, $\text{dim}(w_k)=4$ for dSprites, and $\text{dim}(z)=8$, $\text{dim}(w_k)=4$ for MMNIST.
We performed ablation studies on $\lambda$, $c$, $O$, and $\beta$ (\cf Section~\ref{sec:ablation} and Appendix~\ref{sec:detailed_results}).

\subsection{Content-Motion Disentanglement}
\label{sec:experiments_disentanglement}

We obtained the latent representations from the trained models for the test set and, using ground-truth labels, we calculated the Mutual Information Gap (MIG)~\cite{Chen2018dr}, the FactorVAE (FVAE) disentanglement metric~\cite{Kim2018}, and the Separated Attribute Predictability Score (SAP)~\cite{Kumar2018}.

Assessing disentanglement quality is narrowly application-related~\cite{Eastwood2018,Ridgeway2018}.
We adhere to the criteria defined by \textcite{Ridgeway2018}, by which we may evaluate disentanglement based on either \emph{modularity} (\ie, each unit contains information of at most one factor), \emph{compactness} (\ie, each factor is ideally encoded by at most one unit) or \emph{explicitness} (\ie, each factor is easily recovered from its code).

Since our objective is to encode two factors of variation (content and motion) in various latent units, our main interest is modularity.
Compactness, although desirable, is expected to not be fulfilled, as content and motion are complex factors that can barely be represented in few latent units.
Explicitness is important to estimate the effectiveness of disentangled representations for downstream tasks, like classification.

MIG and SAP heavily penalize representations that are not compact, by depending on the mean difference between the first and second most predictive/informative units.
Hence, FVAE is the metric that interests us the most, as it measures both modularity and explicitness.
We report results on MIG and SAP for completeness since, besides assessing compactness, to some extent, MIG also assesses modularity, and SAP, explicitness.

For $\beta$-TCVAE and MTC-VAE, we split every test video into chunks and calculated one latent vector per chunk.
For dis-VAE and SVG-LP, we obtained one vector per frame.
We aggregated the multiple factors, provided in 3dShapes, dSprites, and LPC, into two categories: time-dependent and time-independent, yielding two factors, to reduce the risk of over-estimation of disentanglement performance, due to pairs of disentangled factors while the rest are entangled.

Table~\ref{tab:baselines} shows the performance of the models on the content-motion disentanglement.
We included the results obtained for the frame-wise version of MTC-VAE (\ie, $c=1$) to compare against dis-VAE and SVG-LP\@.
Both the chunk and frame versions of MTC-VAE are the ones with the best disentanglement performance, followed by $\beta$-TCVAE\@ and dis-VAE.
It is remarkable that SVG-LP uses skip connections from the encoder to the decoder, so most of the appearance information is not in the latent representation.
This is reflected in the fact that it attained the poorest performance.
In general, the chunk version of MTC-VAE outperforms the frame version.

\begin{table}[tb]
\caption{Multi-factor disentanglement. (*\,$c=1$)}
\label{tab:mf}
\scriptsize
\centering
\begin{tabular}{crSSS}
\toprule
 & & {FVAE $\uparrow$} & {MIG $\uparrow$} & {SAP $\uparrow$} \\
\midrule
\multirow{5}{*}{3dShapes}
& $\beta$-TCVAE &     .21(3) &     .07(4) &     .03(2) \\
& dis-VAE       &     .19(1) &     .03(1) &     .01(1) \\
& SVG-LP        &     .18(1) &     .02(1) &     .01(0) \\
& MTC-VAE       &     .27(5) & \bf .19(7) & \bf .08(3) \\
& MTC-VAE*      & \bf .31(2) &     .14(5) &     .05(2) \\
\midrule
\multirow{5}{*}{dSprites}
& $\beta$-TCVAE &     .28(1) &     .02(1) &     .01(0) \\
& dis-VAE       &     .28(0) &     .02(1) &     .01(0) \\
& SVG-LP        &     .29(0) &     .00(0) &     .00(0) \\
& MTC-VAE       & \bf .33(2) & \bf .11(1) & \bf .02(0) \\
& MTC-VAE*      &     .29(1) &     .07(2) &     .01(0) \\
\midrule
\multirow{5}{*}{LPC}
& $\beta$-TCVAE &     .32(7) &     .16(9) &     .03(1) \\
& dis-VAE       &     .22(1) &     .04(1) &     .06(5) \\
& SVG-LP        &     .17(0) &     .01(0) &     .01(1) \\
& MTC-VAE       &     .41(7) & \bf .21(5) & \bf .89(1) \\
& MTC-VAE*      & \bf .43(6) &     .18(3) & \bf .89(0) \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure*}[tb]
\centering
  \scriptsize
  \setlength{\subfigsz}{.32\linewidth}
  \setlength\tabcolsep{1.5pt}
  \begin{tabular}{cccccc}
    Source & Driving & Source & Driving & Source & Driving \\
    \boximg{.15}{MUG_s.png} &
    \boximg{.75}{MUG_d.png} &
    \boximg{.15}{LPC_s.png} &
    \boximg{.9}{LPC_d.png} &
    \boximg{.15}{dSprites_s.png} &
    \boximg{.75}{dSprites_d.png} \\
    $\beta$-TCVAE & \boximg{.75}{MUG_betaTC_p10000102.png} &
    $\beta$-TCVAE & \boximg{.9}{LPC_betaTC_p10000054.png} &
    $\beta$-TCVAE & \boximg{.75}{dSprites_betaTC_p10000002.png} \\
    dis-VAE & \boximg{.75}{MUG_dis_p10000102.png} &
    dis-VAE & \boximg{.9}{LPC_dis_p10000054.png} &
    dis-VAE & \boximg{.75}{dSprites_dis_p10000002.png} \\
    SVG-LP  & \boximg{.75}{MUG_SVG_p10000102.png} &
    SVG-LP  & \boximg{.9}{LPC_SVG_p10000054.png} &
    SVG-LP  & \boximg{.75}{dSprites_SVG_p10000002.png} \\
    MTC-VAE & \boximg{.75}{MUG_MTC_p10000102.png} &
    MTC-VAE & \boximg{.9}{LPC_MTC_p10000054.png} &
    MTC-VAE & \boximg{.75}{dSprites_MTC_p10000002.png}
  \end{tabular}
\caption{
  Reenactment results.  Each set shows the reenacted video of each method with the appearance of \textit{source} and the motion of \textit{driving}.
}
\label{fig:comparison}
\end{figure*}

Although MTC-VAE is trained for motion-content disentanglement, we can argue that this task can be used as a step towards multi-factor disentanglement.
To show our point, we calculated MIG, FVAE, and SAP considering all the factors of variation provided in the datasets' metadata.
Table~\ref{tab:mf} shows the results for 3dShapes, dSprites, and LPC since the others only provide motion-content labels.
In all cases, MTC-VAE (both frame and chunk versions) significantly outperforms the baselines.
The second best method was $\beta$-TCVAE, which is expected since it has been already tested on multi-factor disentanglement for images.
Table~\ref{tab:mf} demonstrates that multi-factor disentanglement is a significantly harder task, but it is remarkable that MTC-VAE features are more disentangled than the others, even when the model was not trained for this specific task.
We provide a list and a description of the factors of variation considered for each dataset in Appendix~\ref{sec:implementation}.

\subsection{Video Reenactment}
\label{sec:reenactment}

\begin{figure*}[tb]
\resizebox{\linewidth}{!}{\input{img/ablation_c}}
\caption{Study of the chunk size \vs several metrics. The gray area shows one standard deviation away from the average plot.}
\label{fig:cs}
\end{figure*}

\begin{figure*}[tb]
  \resizebox{\linewidth}{!}{\input{img/ablation_brl}}
  \caption{Ablation on BRL\@. Light colors indicate the absence of $\mathcal{L}_b$ ($\lambda=0$), while dark colors indicate its presence ($\lambda=1$).}
  \label{fig:brl}
\end{figure*}

\begin{figure}[tb]
\centering
\scriptsize
  \setlength{\subfigsz}{\linewidth}
  \setlength\tabcolsep{1.5pt}
  \begin{tabular}{cc}
    Source  & Driving \\
    \boximg{.14}{CK_abl_c_source.jpg} & \boximg{.84}{CK_abl_c_driving.jpg} \\
    $c=1$ & \boximg{.84}{CK_abl_c_1.jpg} \\
    $c=3$ & \boximg{.84}{CK_abl_c_k.jpg} \\
    \boximg{.14}{MMNIST_abl_c_source.jpg} & \boximg{.84}{MMNIST_abl_c_driving.jpg} \\
    $c=1$ & \boximg{.84}{MMNIST_abl_c_1.jpg} \\
    $c=3$ & \boximg{.84}{MMNIST_abl_c_k.jpg}
  \end{tabular}
\caption{Qualitative comparison of performances for the frame version of MTC-VAE ($c=1$) and the chunk version with a temporal neighborhood of $c=3$ in reenactment quality and inter-chunk consistency.}
\label{fig:ablaton_c}
\end{figure}

We generated \num{10000} videos, each one from a source video $S$ and driving video $D$.
For $\beta$-TCVAE and MTC-VAE, we fixed the content representation of the first chunk of $S$, replicated it, and concatenated each replica to the motion representation of each chunk in $D$\@.
Due to the assumption of appearance preservation throughout the video, our model must be able to reconstruct the video from the appearance representation of any of their chunks.
We decided to use the first chunk of each video for easinesses in the implementation.
The reenacted video was obtained by decoding the resulting vectors.
For dis-VAE, we obtained the content representation from the mean of the frames' appearances and sequentially calculated the motion representations.
For SVG-LP, we obtained the representation from the inference model of the first frame of $S$ and concatenated it with each representation yielded by the learned prior on each frame of $D$.
For $\beta$-TCVAE, since we do not know which units correspond to content and which ones to motion, we considered the classification scheme used to calculate the FVAE metric, which returns an estimate of the units that are more likely to represent either content and motion.
Based on these criteria, we swapped the units that are more likely to represent motion from $D$ to $S$.

Our metrics are frame-wise Structural Similarity (SSIM)~\cite{Wang2004} to quantify identity preservation after reenactment (\ie, whether the reenacted video contains the content of $S$ and no leaked content of $D$), and frame-wise Fréchet Inception Distance (FID)~\cite{Heusel2017} to assess the realism of the reenacted videos.
Table~\ref{tab:baselines} shows the performance of the models for SSIM and FID\@.
In half of the cases, MTC-VAE outperforms the baselines, but its superiority is not as significant as it is in disentanglement.

Due to the lack of metrics to assess that the reenacted video mimics $D$, we provide a qualitative assessment between videos reenacted by the models and their corresponding source videos.
Fig.~\ref{fig:comparison} shows some examples.
It can be seen that MTC-VAE yields reenacted videos that are better synchronized \wrt $D$ than the baselines.
Also, in terms of sharpness, identity preservation, and inter-chunk consistency, MTC-VAE shows a clear advantage.
In general, dis-VAE was more successful in representing time-dependent features than $\beta$-TCVAE\@.
Qualitatively, SVG-LP yielded the poorest reenactment.

Additional results are in Appendix~\ref{sec:detailed_pictures}.
We explored the limits of our model on high-resolution videos (Appendix~\ref{sec:hq}) and on a real-world human-action dataset (Appendix~\ref{sec:taichi}).
Although it has shown to be robust in high-resolution videos, our experiments on human-action datasets make evident the fact that exclusively-CNN-based architectures fall short in reconstructing large motions~\cite{Siarohin2018, Balakrishnan2018}, like the ones done by the human body.
We show that the yielded representations are successful in capturing the semantics of the content and motion of the videos, which suggests that our model obtains meaningful representations of any kind of data.
However, its effectiveness for reconstruction and reenactment is restricted to motions with fewer degrees of freedom (like simple trajectories, facial expressions, and a reduced set of human actions).
These experiments reveal that the bottleneck of the model is the decoder.

\subsection{Ablation Studies}
\label{sec:ablation}

We conducted ablation studies to determine the impact of the chunk size ($c$), the order of the model ($O$), the hyperparameter $\beta$, and the presence/absence of the Blind Reenactment Loss ($\lambda$).
Figs.~\ref{fig:cs} and~\ref{fig:brl} show, respectively, charts on the ablative study on $c \in \{1,3,5,7,9\}$ and $\lambda \in \{0,1\}$.
In Appendix~\ref{sec:detailed_results}, we present complete examples with all the cases on the ablation study, tables with the detailed scores, and the ablation on $O$.

In Fig.~\ref{fig:cs}, we plotted the curves of the metrics as a function of~$c$.
Most of them peaked in \num{3} or \num{5} for FVAE and SAP, meaning that middle-sized chunks are preferable.
For SSIM, when $c > 5$, there is a slight decrease on performance and, although for $c \le 5$ performance is similar, it reaches is lowest variability at $c = 5$ (\cf gray curve).
FID shows a heterogeneous behavior among the datasets.
For CK+ and LPC, the greater the chunk size, the better the performance while the opposite stands for 3dShapes.
For MMNIST, middle values attain the best performance, while LPC shows its worst performance at the same values.
Table~\ref{tab:cs} presents more detailed results.

Although there is a pattern in most of the metrics pointing to a better performance with middle-sized chunks, numerically, the impact on the chunk size may be little significant for the metrics considered.
A more explicit impact on the performance of using chunks ($c > 1$) instead of frames ($c = 1$) is qualitatively evidenced in both reenactment quality and inter-chunk consistency.
As we do not count on metrics to quantify such properties, we depict in Fig.~\ref{fig:ablaton_c} the perceptual difference of performance between the frame and the chunk version of MTC-VAE.
Both CK+ and MMNIST show poor reenactment performance for $c=1$.
This suggests that wider temporal neighborhoods eases motion encoding, to be transferred between videos more accurately, as well as it also eases smoothness.
We show a thorough comparison in Appendix~\ref{sec:detailed_pictures}.

Fig.~\ref{fig:brl} shows the impact of BRL on the performance metrics.
The boxes correspond to the distribution of the five experiments associated with each configuration, due to the 5-fold cross-validation scheme.
Boxes with light colors indicate the performance when $\lambda = 0$, and the ones with dark colors when $\lambda = 1$.
Regarding disentanglement, it can be seen that the positive impact of the BRL is significant in general for FVAE, except for the 3dShapes datasets.
For MIG and SAP, the impact is not that significant, however, this is expected, since both metrics measure compactness, and the BRL loss is not designed for this objective.
Regarding reconstruction metrics (SSIM and FID), its impact was not significant and, in the case of FID, it showed to decrease the performance in dSprites, LPC and MMNIST\@.
Regarding the order of the model, we concluded that optimal values of $O$ are $2$ or $3$, depending on the length of the videos in the dataset (\cf Appendix~\ref{sec:detailed_results}).
Since the complexity of the model is quadratic \wrt to $O$, higher values are not worth considering.

\subsection{Performance on Downstream Tasks}
\label{sec:dt}

\begin{table}[tb]
  \caption{Content (C)/Motion (M) classification accuracy. (*\,$c=1$)}
  \label{tab:2fdt}
  \scriptsize
  \centering
  \setlength\tabcolsep{2.3pt}

  \resizebox{\linewidth}{!}{%
  \begin{tabular}{@{}lSScSScSS@{}}
    \toprule
    & \multicolumn{2}{c}{3dShapes} &
    & \multicolumn{2}{c}{CK+} &
    & \multicolumn{2}{c}{dSprites} \\
    \cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
    & {C} & {M} &
    & {C} & {M} &
    & {C} & {M} \\
    \cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
    $\beta$-TCVAE &     .53(5) &     .44(5) & &     0.90(2) &     .52(5) & &     .22(1) &     .62(2) \\
    dis-VAE       &     .48(1) &     .42(1) & & \bf 1.00(0) &     .62(4) & &     .54(6) &     .59(1) \\
    SVG-LP        &     .11(2) &     .11(1) & &     0.87(4) &     .60(2) & &     .00(0) &     .60(1) \\
    MTC-VAE       & \bf .95(1) & \bf .59(1) & &     0.97(1) & \bf .68(7) & & \bf .61(1) & \bf .63(3) \\
    MTC-VAE*      &     .46(1) &     .41(2) & &     0.94(1) &     .63(8) & &     .20(8) &     .60(3) \\
    \cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
    & \multicolumn{2}{c}{LPC} &
    & \multicolumn{2}{c}{MMNIST} &
    & \multicolumn{2}{c}{MUG} \\
    \cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
    $\beta$-TCVAE &     .11(1) & \bf .99(1) & &     .32(03) & \bf .26(5) & & \bf 1.00(0) &     .54(6) \\
    dis-VAE       &     .43(2) &     .95(1) & & \bf .54(08) &     .16(1) & & \bf 1.00(0) &     .55(2) \\
    SVG-LP        &     .00(0) &     .54(3) & &     .14(01) &     .14(1) & &     0.48(4) &     .34(1) \\
    MTC-VAE       &     .65(1) &     .93(4) & &     .48(10) &     .20(6) & & \bf 1.00(0) & \bf .79(5) \\
    MTC-VAE*      & \bf .68(3) &     .97(1) & &     .45(07) &     .19(2) & & \bf 1.00(0) &     .70(8) \\
    \bottomrule
  \end{tabular}}
\end{table}


\begin{table}[tb]
  \caption{Classification accuracy in multiple factors. (*\,$c=1$)}
  \label{tab:nfdt}
  \scriptsize
  \centering
  \setlength\tabcolsep{4pt}
  \begin{tabular}{@{}rSSSSS@{}}
    \toprule

    Factor & {$\beta$-TCVAE} & {dis-VAE} & {SVG-LP} & {MTC-VAE} & {MTC-VAE*} \\
    \midrule & \multicolumn{5}{c}{3dShapes} \\
    \cmidrule{2-6}
    Floor hue & .94(04) & \bf 1.00(0) & .27(05) &     .99(1) & .99(1) \\
    Wall hue & .95(07) & \bf 1.00(0) & .53(11) &     .97(3) & .97(3) \\
    Obj. hue & .82(10) & \bf 1.00(0) & .17(02) &     .95(4) & .95(4) \\
    Init. size & .66(25) &     0.97(4) & .66(18) & \bf .98(3) & .97(4) \\
    Final size & .30(09) &     0.25(2) & .46(06) & \bf .51(3) & .50(3) \\
    Shape & .25(06) &     0.26(2) & .25(02) & \bf .38(2) & .37(3) \\
    Init. persp. & .20(06) &     0.17(0) & .25(01) & \bf .28(3) & .27(2) \\
    Final persp. & .17(05) &     0.18(1) & .15(01) & \bf .25(5) & .24(2) \\
    \midrule & \multicolumn{5}{c}{dSprites} \\
    \cmidrule{2-6}
    R & .02(00) & .03(0) & .01(00) & \bf .07(00) & .04(1) \\
    G & .02(00) & .03(0) & .01(00) & \bf .08(01) & .04(1) \\
    B & .03(01) & .03(0) & .01(00) & \bf .07(01) & .03(0) \\
    Shape & .46(02) & .45(1) & .34(01) & \bf .51(05) & .50(4) \\
    Scale & .44(03) & .50(1) & .18(01) & \bf .56(03) & .47(3) \\
    Rot. & .12(01) & .10(1) & .03(00) & \bf .49(04) & .10(4) \\
    Traj. & .62(02) & .59(1) & .60(01) & \bf .63(03) & .60(3) \\
    \midrule & \multicolumn{5}{c}{LPC} \\
    \cmidrule{2-6}
    Body &     .18(01) & 0.54(4) & .13(00) & \bf 0.97(01) & \bf .97(1) \\
    Gender &     .60(04) & 0.91(3) & .50(00) & \bf 1.00(00) &     .98(2) \\
    Shirt &     .72(03) & 0.96(3) & .66(04) & \bf 1.00(00) &     .85(8) \\
    Pants &     .68(04) & 0.89(2) & .18(01) & \bf 0.99(00) &     .87(7) \\
    Hair & \bf .97(02) & 0.89(4) & .28(01) &     0.91(04) &     .92(1) \\
    Hat &     .69(01) & 0.88(2) & .56(00) & \bf 1.00(00) &     .99(1) \\
    Action &     .62(04) & 0.64(3) & .61(04) &     0.64(03) & \bf .69(3) \\
    Perspective &    .94(03) & \bf 1.00(0) & .58(03) &     0.72(14) &   .98(2) \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure*}[tb]
\centering
  \begin{subfigure}{.32\textwidth}
    \includegraphics[width=\textwidth]{traverse_full.png}
    \caption{Full latent space}
  \end{subfigure}
  \begin{subfigure}{.32\textwidth}
    \includegraphics[width=\textwidth]{traverse_full_a.png}
    \caption{Content subspace}
  \end{subfigure}
  \begin{subfigure}{.32\textwidth}
    \includegraphics[width=\textwidth]{traverse_full_m.png}
    \caption{Motion subspace}
  \end{subfigure}
\caption{%
  Latent-space traversals on LPC.
  The upper and lower sequences are, respectively, the start and endpoints of the traversals.}
\label{fig:traversals}
\end{figure*}

\begin{figure}[tb]
\centering
  \setlength{\subfigsz}{\linewidth}
  \setlength\tabcolsep{1.5pt}
  \begin{tabular}{rc}
    Original & \boximg{.8}{traverse_up.png} \\
    Hair color & \boximg{.8}{traverse_hair_color_5.png} \\
    Hairstyle & \boximg{.8}{traverse_hair_style_7.png} \\
    Shirt & \boximg{.8}{traverse_shirt_color_11.png} \\
    Pants & \boximg{.8}{traverse_pants_color_13.png} \\
    Perspective & \boximg{.8}{traverse_perspective_14_18_20.png}
  \end{tabular}
\caption{Some controllable visual traits by traversing specific latent units.}
\label{fig:controllable}
\end{figure}

To evaluate the robustness of the learned disentangled representations, we extracted them from the datasets, and trained a Linear Support Vector Machine to assess whether they are linearly separable.
We chose a simple classifier, as more sophisticated ones are prone to work around weaker representations, hindering the comparison between our model and the baselines.
We tested the models in (i)~content-motion and (ii)~multi-factor classification.

For the first scenario, we used the same ground-truth labels to calculate appearance/motion disentanglement, and report the obtained accuracies in Table~\ref{tab:2fdt}, showing that recognizing content is easier than actions.
In most of the datasets, our model outperforms the baselines in both content and motion.

For the second scenario, we used the same ground-truth labels to calculate multi-factor disentanglement.
This scenario was harder for all the models (\cf Table~\ref{tab:nfdt}).
However, ours outperformed the rest in most of the cases.
This is expected since none of them was trained for multi-factor disentanglement.
Notice that each row in Table~\ref{tab:nfdt} is a classification scheme on different sets of classes.
\Eg, for dSprites, factor \textit{R} represents the red RGB contribution of the shape, so it is a 256-class problem, while factor \textit{Shape} is a 4-class problem, as there are only four different shapes in the dataset (\cf Table~\ref{tab:factors}).
In both scenarios, the chunk-wise version of our model outperformed the frame-wise version (MTC-VAE*) most of the times.

\subsection{Latent-Space Traversals}
\label{sec:traversals}

We include some examples of latent-space traversals on the LPC dataset, to show how MTC-VAE could be used for conditional video generation.
Fig.~\ref{fig:traversals} shows three trajectories, between two videos~$x_0$ and~$x_1$, separated by 5 steps.
The leftmost trajectory traverses the whole latent space, so it is possible to see the complete transformation from~$x_0$ to~$x_1$.
The central trajectory is done in the content subspace while remaining stationary in the motion space, so it can be seen how the endpoint is a video with the appearance of $x_1$ and the motion of $x_0$.
The opposite can be observed in the rightmost trajectory, which only traverses the motion subspace.

All the trajectories are linear, so it is expected that examples in the middle do not look plausible, due to a high probability of sampling outside either~$q_\phi(z\given x_k)$ or~$q_\gamma(w_k\given x_k)$.
To correctly traverse the latent space requires awareness of its topology.
We leave as future work to explore more sophisticated methods to traverse the space of our model~\cite{Ye2019, Song2020cyb}.

Fig.~\ref{fig:controllable} shows some examples of controllable video generation.
We highlight that we do not expect to perform this task perfectly, as we focus exclusively on content-motion disentanglement, so it is normal that visual traits that should be independent (\eg hair color and skin color) happen to be entangled in the representation.
However, it is possible to independently traverse each latent unit of the space and manually check which visual traits were affected.
The sequences of Fig.~\ref{fig:controllable} are the endpoints of the trajectories (Appendix~\ref{sec:detailed_pictures} shows the complete trajectories), and each one shows a visual trait that was affected by traversing latent units.
Most of them were affected by only one unit: hair color ($z[5]$), hairstyle ($z[7]$), shirt color ($z[11]$), and pants color ($z[13]$).
Motion-related units were more difficult to traverse, since independent motion traits of the video remain more entangled than the appearance ones, as shown by our results on multi-factor classification (Table~\ref{tab:nfdt}).
This means that traversals have a high risk of sampling outside the support of $q_\gamma(w_k\given x_k)$.
The last example in Fig.~\ref{fig:controllable} was constructed by traversing~$w[0]$, $w[4]$, and~$w[6]$, and it is clear that we sampled outside $q_\gamma(w_k\given x_k)$.
This set of experiments show that it is possible to interpret, to some extent, the meaning of the components of the latent representations.

\section{Conclusion}
\label{sec:conclusions}

Our proposed MTC-VAE for content-motion disentanglement learns to represent videos as a consistent sequence of chunks that are independent at generation time, but dependent at inference time.
It considers two extensions to the VAE formulation: (i)~training the model such that each chunk implicitly contains information about the whole video under the assumption of content invariability, while separating motion per chunk, and (ii)~using the task of video reenactment as an inductive bias to leverage the learning of independent content and motion representations.
MTC-VAE yields less latent vectors to represent a video (one per chunk, instead of per frame).
To reconstruct one video, it is trained with chunks modeled as independent random variables at generation time.
Given that a chunk does not depend on the reconstruction of the previous one, all chunks in a video can be reconstructed in a single forward-pass.
The experiments show the capacity of our chunk-wise approach in learning time-dependent and -independent representations from videos as well as the positive impact of video reenactment as an inductive bias to improve such representations.
Our ablative study on the size of the chunks shows a better disentanglement and VR performance of middle-sized chunks, over the frame-wise approach.
We also showed the superiority of MTC-VAE for multiple-factor disentanglement, even though it was not explicitly trained for more than two factors.
We explored the limits of our model in additional experiments on high-resolution videos (Appendix~\ref{sec:hq}) and on a real-world human-action dataset (Appendix~\ref{sec:taichi}).
These experiments reveal that the bottleneck of the model is the decoder, whose enhancement we leave for future work as well as exploring different latent and data priors, and devising fusion strategies for the chunks to yield more informative gradients and a better reconstruction, as well as disentanglement quality.

\input{appendix.tex}
\printbibliography


\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.1in,clip,keepaspectratio]{juan}}]{Juan F. Hern\'{a}ndez Albarrac\'{i}n}
is a Ph.D. candidate at University of Campinas, Brazil. He has a bachelor degree in Computer Engineering from National University of Colombia (2014), and a M.Sc. degree in Computer Science from University of Campinas (2017). He has experience in Machine Learning and Computer Vision, focusing on evolutionary computing, deep learning, and generative models applied for image/video classification and synthesis.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.1in,clip,keepaspectratio]{adin}}]{Ad\'{i}n Ram\'{i}rez Rivera} (S'12, M'14, SM'21) received his B.Eng.\ degree in Computer Engineering from Universidad de San Carlos de Guatemala (USAC), Guatemala in 2009.  He completed his M.Sc.\ and Ph.D.\ degrees in Computer Engineering from Kyung Hee University, South Korea in 2013.  He is currently an Associate Professor at the Department of Informatics, University of Oslo, Norway.  His research interests are video understanding (including video classification, semantic segmentation, spatiotemporal feature modeling, and generation), and understanding and creating complex feature spaces.
\end{IEEEbiography}

\vfill

\end{document}


