\section{APPENDIX }\label{section1}

\subsection{Preliminaries on Kalman Filter}
Given $x_{0}\in \mathbb{R}^{p}$, let $x(t)\in \mathbb{R}^{p}$, where $\forall t\in [0,T]$, denote the state of a dynamic system governed by the mathematical model $\mathcal{M}_{t,t+\Delta t}[ x(t)]$, with $\Delta t>0$ and $p\in\mathbb{N}$, such that:
\begin{equation}\label{modello}
\left\{\begin{array}{ll}
x(t+ \Delta t)&=\mathcal{M}_{t, t+ \Delta t}(x(t)), \ \ \forall t,t+\Delta t \in [0,T]\\
x(0)&=x_{0}
\end{array}, \right.
\end{equation}  
and let:
\begin{equation}\label{osservazioni}
y(t+\Delta t)=\mathcal{H}_{t+\Delta t}[x(t+\Delta t)],
\end{equation}
denote the so called observations, where 
\begin{equation}
\mathcal{H}_{t+\Delta t}: \ x(t+\Delta t) \in \mathbb{R}^{p}\mapsto y(t+\Delta t) \in \mathbb{R}^{d}\,, \quad d \in \mathbb{N} \quad d <<p \, ,
\end{equation}
denotes the observations mapping including transformations and grid interpolations.\\
In the context of DA methods, KF aims to bring the state $x(t)$ as close as possible to the measurements/observations $y(t)$. One can do this by discretizing  then optimize or first optimize and then discretize. Here we use the first discretize then optimize approach. Chosen $r\in \mathbb{N}$, we consider $r+2$ points in $[0,T]$, $\{t_{k}\}_{k=0,1,\ldots,r+1}$,  where $t_{k}=k\Delta t$\footnote{More in general, $t_k$ could not be equi distributed} and $\Delta t= \frac{T}{r+1}$. We use the following set up of  KF. \\

\noindent \textbf{KF set up}: Given 
\begin{itemize}
\item $x_{k}\equiv x(t_{k})\in \mathbb{R}^{p}$: the state  at $t_{k}$, where $k=0,1,\ldots,r+1$; 
\item $ x_{0}$: the state at time $t_{0}\equiv 0$ (or the vector of initial conditions);
\item $M_{k,k+1}\in \mathbb{R}^{p\times p}$: discretization of a linear approximation of $\mathcal{M}_{t_{k},t_{k+1}}$, for $k=0,1,\ldots,r$;\footnote{
 For simplicity of notations our experiments concern  a linear model. With a nonlinear model $\mathcal{M}_{t,t+\Delta t}[ x(t)]$, $M_{k,k+1}\in \mathbb{R}^{n\cdot pv \times n\cdot pv}$ is 
the tangent linear operator (Jacobian) of $\mathcal{M}_{t,t+\Delta t}[ x(t)]$.
Thus, in  EKF, a linearized
and approximate equation is used for the prediction of
error statistics.}
\item $m\in \mathbb{N}$:  number of observations;
\item $y_{k}\equiv y(t_{k})\in \mathbb{R}^{m\cdot d}$:  observations vector;
\item $b_{k}\in \mathbb{R}^{ p}$: input control vector;
\item $H_{k+1}\in \mathbb{R}^{m\cdot d \times p}$: discretization of a linear approximation of $\mathcal{H}_{t_{k+1}}$ (also called observations operator)  with $p>m$, for $k=0,1,\ldots,r$;
\item $w_{k}\in \mathbb{R}^{p}$ and $v_{k}\in \mathbb{R}^{m\cdot d}$: model and observation additive errors with normal distribution and zero mean such that $E[w_{k}v_{i}^{T}]=0$, for $i,k=0,1,\ldots,r+1$, where $E[\cdot]$ denotes the expected value;
\item $Q_{k}\in \mathbb{R}^{p\times p}$ and $R_{k}\in \mathbb{R}^{m\cdot d \times m\cdot d}$: covariance matrices of the errors on the model and on the observations, respectively i.e.
\begin{equation}
Q_{k}:=E[w_{k}w_{k}^{T}] \quad R_{k}:=E[v_{k}v_{k}^{T}] \quad \textit{$\forall$ $k=0,1,\ldots,r+1$}.
\end{equation}
\noindent These matrices are symmetric and positive definite.
\end{itemize}

\noindent \textbf{KF problem:} KF problem  consists in calculating  $\widehat{x}_{k+1}$, an estimate  of  $x_{k+1}\in \mathbb{R}^{p}$ where:
\begin{equation}\label{sistema_kalmandiscreto}
x_{k+1}=M_{k,k+1}x_{k}+b_{k}+w_{k}, \quad \textit{$\forall k=0,1,\ldots,r$}
\end{equation}
should be such that 
\begin{equation}\label{problema1}
y_{k+1}=H_{k+1}{x}_{k+1}+v_{k+1},\quad \textit{$\forall k=0,1,\ldots,r$}.
\end{equation}



\noindent KF problem  is a recursive estimator, this means that only the estimated state from the previous time step ($x_{k}$)  and the current observation ($y_{k+1}$) are used to compute the estimate of current state ($x_{k+1}$). It is also referred to as predictor-corrector estimator \cite{sorenson}  alternating a forecast step, when the covariance is evolved, with an analysis step in which  covariance of the filtering conditional is updated. In the following we give main steps of KF and its related algorithm in Table 1.  \\

\noindent \textbf{KF method}. Given ${x}_{0}\in \mathbb{R}^{p}$, $P_{0}\in \mathbb{R}^{p\times p}$ and posed $\widehat{x}_{0}\equiv x_{0}$, for each $k=0,1,\ldots,r$  KF method is composed by two main steps.
\begin{itemize}
\item[(i)] Predictor phase.
\begin{itemize}
\item Compute  state estimate:
\begin{equation}\label{stimapredetta}
x_{k+1}=M_{k,k+1}\widehat{x}_{k}+b_{k};
 \end{equation}
\item Compute  error covariance matrices: 
\begin{eqnarray}\label{predictedmatrix}
P_{k+1}=&M_{k,k+1}P_{k}M_{k,k+1}^{T}+Q_{k}\\ \nonumber
S_{k+1}=&H_{k+1}P_{k+1}H_{k+1}^{T}+R_{k+1} 
\end{eqnarray}
\item Compute  KF gain:
\begin{equation}\label{guadagnodikalman}
K_{k+1}=P_{k+1}H_{k+1}^{T}S_{k+1}^{-1}.
\end{equation}
\end{itemize}
\item[(ii)] Corrector phase.
\begin{itemize}

\item Update  covariance matrix:
\begin{equation}\label{updateP}
P_{k+1}=(I-K_{k+1}H_{k+1})P_{k+1},
\end{equation}
\item Update state estimate:
\begin{equation}\label{stimapredetta1}
\widehat{x}_{k+1}=x_{k+1}+K_{k+1}(y_{k+1}-H_{k+1}x_{k+1}).
\end{equation}
\end{itemize}
\end{itemize}

In  Table \ref{tab} KF algorithm is sketched and in Table \ref{tab2}, we detail its  time complexity. 
\footnotesize
\begin{table}[ht!]
\caption{KF Algorithm}
\label{tab}
\begin{tabular}{|l|}
\hline
\textbf{procedure}  KF (in: ${x}_{0},\,$r$,\,y_{1},\ldots,y_{r+1},M_{0,1},\ldots,M_{r,r+1},H_{1},\ldots,H_{r+1},$\\$Q_{1},\ldots,Q_{r+1},R_{0},\ldots,R_{r},$  out: $\widehat{x}_{1},\ldots,\widehat{x}_{r+1})$\\
\textbf{Define} $P_{0}$\\
\textbf{Define} $\widehat{x}_{0}\equiv x_{0}$\\
\textbf{reapeat}\\
\textbf{Compute}  $x_{k+1}\leftarrow M_{k,k+1}\widehat{x}_{k}+b_{k}$ as in (\ref{stimapredetta})\\
\textbf{Compute} $P_{k+1}\leftarrow M_{k,k+1}P_{k}M_{k,k+1}^{T}+Q_{k}$ as in (\ref{predictedmatrix})\\
\textbf{Compute} $S_{k+1}\leftarrow H_{k+1}P_{k+1}H_{k+1}^{T}+R_{k+1}$ as in (\ref{predictedmatrix})\\
\textbf{Compute}  Cholesky factor: $S_{k+1}\leftarrow chol(H_{k+1}P_{k+1}H_{k+1}^{T}+R_{k+1})$\\
\textbf{Compute} inverse of the Cholesky factor: $S_{k+1}^{-1}$\\
\textbf{Compute}  KF gain: $K_{k+1}\leftarrow P_{k+1}H_{k+1}^{T}(S_{k+1}^{-1})^{T}S_{k+1}^{-1}$ as in (\ref{guadagnodikalman})\\
\textbf{Compute} $P_{k+1}\leftarrow (I-K_{k+1}H_{k+1})P_{k+1}$ as in (\ref{updateP})\\
\textbf{Compute}  KF estimate: $\widehat{x}_{k+1}\leftarrow x_{k+1}+K_{k+1}(y_{k+1}-H_{k+1}x_{k+1})$ as in (\ref{stimapredetta1})\\
\textbf{until} ($k=r$)\\
\textbf{end procedure} \\
\hline
\end{tabular}
\end{table}


\footnotesize
\label{tab2}
\begin{table}
\captionsetup{singlelinecheck=off}
\caption[]{Time complexity of the KF procedure, where 
\begin{itemize}
\item $n\in \mathbb{N}$: dimension of the model state;
\item $m\in \mathbb{N}$: dimension of  observations vector, where $m<n$;
\item $H_{k+1}\in \mathbb{R}^{m\times n}$: observations operator at time $t_{k+1}$;
\item $F_{k+1}:=H_{k+1}P_{k+1}H_{k+1}^{T}+R_{k+1}\in \mathbb{R}^{m\times m}$, where $P_{k+1}\in \mathbb{R}^{n\times n}$ and $R_{k+1}$ are the error covariance matrix in (\ref{predictedmatrix}) and covariance of the error on the observations.
\item MV: matrix-vector product;  SUMV: sum of vectors; SUMM; MM: matrix-matrix product;  CHOL: Cholesky factor; INV: matrix inversion
\end{itemize}} 
\label{tab2}
\begin{tabular}{|r|l|l|l|}
\hline
data & operations & cost \\
\hline
$x_{k+1}$ & MV + SUMV  &$O(n^{2})+O(n) $\\  
$P_{k+1}$& 2 MV + SUMM & $O(2n^{3})$ + $O(n^{2})$\\
$F_{k+1}$ & 2 MM+SUMM &$O(n^{2}m)$+ $O(m^{2})$\\
$S_{k+1}$ & CHOL of $F_{k+1}$ & $O(\frac{m^{3}}{6})$\\
$S_{k+1}^{-1}$ & INV & $O(\frac{m^{3}}{6})$\\
$(S_{k+1}^{-1})^{T}S_{k+1}^{-1}$ & MM & $O(\frac{m^{3}}{3})$\\
$K_{k+1}$ & MM & $O(m^{2}n)$\\
$\widehat{x}_{k+1}$& MV+SUMV+MV+SUMV & $O(nm)$+ $O(m)$+ $O(mn)$+ $O(n)$\\
$P_{k+1}$& MM + SUMM + MM & $O(n^{2}m)$+ $O(n^{2}m)$+ $O(n^{3})$\\
\hline 
\end{tabular}
\end{table}
\subsection{Preliminaries on DD}
\noindent We first introduce reduction  of a matrix $B\in \mathbb{R}^{m \times n}$ and  extension  of a vector $w\in \mathbb{R}^{s}$.\\

\noindent \textbf{Definition (reduction of matrices):}
Let $B=[B^{1} \ B^{2} \ \ldots \ B^{n}] \in \mathbb{R}^{m \times n}$ be a matrix with $m,n\ge 1$ and $B^{j}$ the $j-th$ column of $B$ and $I_{j}=\{1,\ldots,j\}$ and $I_{i,j}=\{i,\ldots,j\}$ for $i=1,\ldots,n-1$ and $j=2,\ldots,n$. The reduction of $B$ to the set $I_{j}$ is:
\begin{equation}
|_{I_{j}}: \ B\in \mathbb{R}^{m\times n} \rightarrow \ B|_{ I_{j}}=[B^{1} \ B^{2} \ \ldots \ B^{j}]\in \mathbb{R}^{m \times j}, \quad \textit{ $j=2,\ldots,n$},
\end{equation}
and to $I_{i,j}$
\begin{eqnarray}{l}
|_{ I_{i,j}}: \ B\in \mathbb{R}^{m\times n} \rightarrow \ B|_{ I_{i,j}}=[B^{i} \ B^{i+1} \ \ldots \ B^{j}]\in \mathbb{R}^{m \times j-i},  \\ 
\textit{ $i=1,\ldots,n-1$, $j=2,\ldots,n$}, 
\end{eqnarray}
where $B|_{ I_{j}}$ and $B|_{ I_{i,j}}$ denote the reductions of  $B$ to $I_{j}$ and $I_{i,j}$, respectively.\\[.2cm]
\textbf{Definition (Extension  of vectors):}
Let $w=[w_{t} \ w_{t+1} \ \ldots \ w_{n}]^{T} \in \mathbb{R}^{s}$ be a vector with $t\ge 1$, $n>0$, $s=n-t$ and $I_{1,r}=\{1,\ldots,r\}$, $r>n$. The extension  of $w$  to  $I_{r}$ is:
\begin{equation}
EO_{I_{r}}: \ w\in \mathbb{R}^{s} \rightarrow \ EO_{I_{r}}(w)=[\bar{w}_{1} \ \bar{w}_{2} \ \ldots \ \bar{w}_{r}]^{T} \in \mathbb{R}^{r},
\end{equation}
where, for $i=1,\ldots,r$, it is: 
\begin{equation}
\bar{w}_{i} =\left\{ \begin{array}{ll} 
w_{i} & \textit{if $t\le i\le n$}\\
0 & \textit{if $i> n$ and $i<t$ \quad .}\\
\end{array}. \right.
\end{equation}





\subsection{DD-KF algorithm}
Table \ref{tab4} shows DD-KF algorithm while its time complexity is detailed in Table  \ref{tab5}.
 

\footnotesize
\begin{table}[ht!]
\caption{DD-KF procedure on $\Omega_{i} \times \Delta_{j}$, $i=1,2$ and $j=1,\ldots,L$.}
\label{tab4}
\begin{tabular}{|l|}
\hline
\textbf{procedure} DD-KF(in:\,$i$,$x_{0}, \bar{s}_{j},s_{j},s_{j-1,j},b_{\bar{s}_{j-1}},\ldots,b_{\bar{s}_{j}-s_{j}},b_{\bar{s}_{j-1}+1},\ldots,y_{\bar{s}_{j}-s_{j}},$ \\ $M_{\bar{s}_{j-1},\bar{s}_{j-1}+1},\ldots,M_{\bar{s}_{j}+s_{j}-1,\bar{s}_{j}+s_{j}},H_{\bar{s}_{j-1}},\ldots,H_{\bar{s}_{j}+s_{j}-1},Q_{\bar{s}_{j-1}},\ldots,Q_{\bar{s}_{j}+s_{j}-1},R_{\bar{s}_{j-1}+1},\ldots,R_{\bar{s}_{j}+s_{j}},$ \\ out:$\widehat{x}_{1,\bar{s}_{j-1}+1},\ldots,\widehat{x}_{1,\bar{s}_{j-1}+s_{j}})$\\
\textbf{Define} $P_{1}$, $P_{2}$, $P_{1,2}$, $P_{2,1}$ \\
\textbf{Define}  reductions of matrix $M_{\bar{s}_{j-1},\bar{s}_{j-1}+1}$: $M_{1},\ M_{1,2},M_{2,1}, M_{2}$ as in (\ref{deco_M2})\\
\textbf{If} $i=1$\\
\textbf{Define} $h=2$\\
else\\
\textbf{Define} $h=1$\\
\textbf{EndIf}\\
\textbf{If}  $(j=1)$ \\
\textbf{Define} the vector of  initial conditions on $\Delta_{1}: \widehat{x}_0\equiv x_0$\\
\textbf{Elseif} \\
\textbf{Receive} the vector of  initial conditions on $\Delta_{j}: \widehat{x}_{\bar{s}_{j-1}}$\\ 
\textbf{Endif}\\
\textbf{repeat}\\
\textbf{Send and Receive}  boundary conditions from adjacent domains\\
\textbf{Compute} $b_{i,k}$ as in (\ref{b_k})\\
\textbf{Compute}  $x_{i,k+1}\leftarrow M_{i}
\widehat{x}_{i,k}+b_{k}|_{I_{i}}+b_{i,k}$ as in (\ref{stimapredetta_DD})\\
\textbf{Compute} $C_{\Omega_{1} \leftrightarrow \Omega_2}$ as in (\ref{mat_C})\\
\textbf{Compute} the predicted covariance matrices:  $P_{1,2}$, $P_{2,1}$ as in (\ref{matP})\\
\textbf{Compute}  $P_{\Omega_{1} \leftrightarrow \Omega_{2}}$, $P_{\Omega_2 \leftrightarrow \Omega_1}$ as in (\ref{matP_in})\\
\textbf{Compute} the predicted covariance matrices: $P_{1}$, $P_{2}$ as in (\ref{predictedmatrix_DD})\\
\textbf{Compute} $R_{1,2}\leftarrow H_{k+1}|_{I_{2}}P_{2,1}H_{k+1}^{T}+H_{k+1}|_{I_{1}}P_{1,2}H_{k+1}|_{I_{2}}^{T}$ as in (\ref{R12})\\
\textbf{Compute} $F\leftarrow H_{k+1}|_{I_{1}}P_{1}H_{k+1}|_{I_{1}}^{T}+H_{k+1}|_{I_{2}}P_{2}H_{k+1}|_{I_{2}}^{T}+R_{1,2}+R_{k+1}$ as in (\ref{matF})\\
\textbf{Compute} the Cholesky factor: $S\leftarrow chol(F)$\\
\textbf{Compute} inverse of the Cholesky factor: $S^{-1}$\\
\textbf{Compute}  DD-KF gain: $K_{i}\leftarrow (P_{i}H_{k+1}|_{I_{1}}^{T}+P_{i,h}H_{k+1}|_{I_{2}}^{T})(S_{1}^{-1})^{T}S^{-1}$ as in (\ref{guadagnodikalman_DD})\\
\textbf{Compute} $P_{i}\leftarrow (I-K_{i}H_{k+1}|_{I_{i}})P_{i}-K_{i}H_{k+1}|_{I_{h}}P_{h,i}$ as in (\ref{P1-P2})\\
\textbf{Compute} $P_{i,h} \leftarrow (I-K_{k+1}H_{k+1}|_{I_{1}})P_{i,h}-K_{i}H_{k+1}|_{I_{h}}P_{h}$ as in (\ref{P12-P21})\\
\textbf{Compute}  DD-KF estimate: $\widehat{x}_{1,k+1}\leftarrow x_{i,k}+K_{i}[y_{k+1}-(H_{k+1}|_{I_{i}}x_{i,k+1}+H_{k+1}|_{I_{h}}x_{h,k+1})]$ as in (\ref{stimastato_DD})\\
\textbf{until} ($k=\bar{s}_{j-1}+s_{j}$)\\
\textbf{Send} the vector of initial conditions on $\Delta_{j+1}$: $\widehat{x}_{\bar{s_{j}}-1}\equiv \widehat{x}_{\bar{s}_{j}}$\\
\textbf{end procedure} \\
\hline
\end{tabular}
\end{table}

\footnotesize
\begin{table}
\captionsetup{singlelinecheck=off}
\caption[]{Time complexity of DD-KF procedure, where 
\begin{itemize}
\item $n\in \mathbb{N}$: dimension of the model state;
\item $n_i \in \mathbb{N}$: dimension of the model state on $\Omega_i$, where $n_i <n$, $i=1,2$; 
\item $m\in \mathbb{N}$: dimension of  observations vector, where $m<n$;
\item MV: matrix-vector product;  SUMV: sum of vectors; SUMM: sum of matrices; MM: matrix-matrix product;  CHOL: Cholesky factor; INV: matrix inversion
\end{itemize}} 
\label{tab5}
\begin{tabular}{|r|l|l|l|}
\hline
data & operations & cost \\
\hline
$b_{1,k}$ & MV  &$O(n^{2})+O(n) $\\  
$x_{1,k+1}$& MV + 2 SUMM & $O(2n^{3})$ + $O(n^{2})$\\
$C_{\Omega_1 \leftrightarrow \Omega_2}$ & 2 MM+ 2 MM+ 2 MM + 2 MM + 3 SUMM &$O(2n_1^{2}n_2)$+ $O(2n_1^2n_2)$+$O(2n_1n_2^2)$+$O(n_1^2)$\\
$P_{\Omega_1 \leftrightarrow \Omega_2}$ & 2 MM + 2 MM + SUMM& $O(n_2n_1^2+n_1^3)$+$O(n_2^2n_1^3)$+$O(n_1^2)$\\
$P_{1,2}$ & 2 MM + SUMM & $O(n_1^2n_2)$+$O(n_1n_2^2)$+$O(n1n_2)$\\
$R_{1,2}$ & 2 MM + 2MM + SUMM & $O(2n_1n_2m)$+$O(2n_1m^2)$+$O(m^2)$\\
$F$ & 2 MM + 2MM + 4 SUMMM & $O(n_1m)$+$O(n_2m)$+$O(4m^2)$\\
$S$ & CHOL of $F$& $O(m^3/6)$ \\
$S^{-1}$ & INV & $O(m^3/6)$\\
$(S^{-1})^TS^{-1}$  & MM & $O(m^3/3)$\\
$K_1$& SUMM+MM & $O(2mn_1)+O(m^2n_1)$\\
$\hat{x}_{1,k+1}$& 2 MV  + SUMV + MV + SUMV & $O(n_1m+n_2m)+O(m) + O(mn_1)+O(n_1)$ \\
$P_1$& 2MM+2SUMM & $O(2n_1^2m)+O(2n_1m)$\\
$P_{1,2}$ & 2MM + 2SUMM & $O(2n_1^2m)+O(2n_1m)$\\
\hline 
\end{tabular}
\end{table}

\begin{definition} \textbf{Scale-up factor \cite{JSC,CAI}}
 Let $T_{KF}(m,n)$, $T_{DD-KF}(m,n_{i})$, $i=1,\ldots,n_{sub}$ be  time complexity of  KF algorithm and DD-KF algorithm. We introduce the following ratio:
\begin{equation}\label{factor}
Sc^f_{n_{sub},1}(m,n)=\frac{T_{KF}(m,n)}{n_{sub}\cdot T_{DD-KF}(m,n/n_{sub})}.
\end{equation}
\end{definition}


\noindent Following result allows us to analyze the behaviour of the scale up factor.\\


\noindent \textbf{Proposition}
Let  $r \equiv n/n_{sub}$. It holds that
\begin{equation}\label{factor_alfa}
Sc^f_{n_{sub},1}(m,n)=\alpha(n,m,n_{sub})n_{sub}^{2}
\end{equation} 
where
\begin{equation}
\alpha(n,m,n_{sub})=\frac{a_{3}+a_{2}\frac{1}{n}+a_{1}\frac{1}{n^{2}}+a_{0}\frac{1}{n^{3}}}{b_{3}+b_{2}\frac{1}{r}+b_{1}\frac{1}{r^{2}}+b_{0}\frac{1}{r^{3}}}.
\end{equation}


\noindent \underline{\bf Proof}.

$T_{KF}(m,n)=O(p_{1}(n))$, $T_{DD-KF}(m,r)=O(p_{2}(r))$ are polynomials of degree 3 i.e. $p_{1},p_{2}\in \Pi_{3}$. We let
\begin{equation}\label{coeff}
\begin{array}{ll}
a_{3}=3 \quad a_{2}=2m+3 \quad a_{1}=2m^{2}+2m+1 \quad a_{0}=\frac{2}{3}m^{3}+m^{2}+m\\
b_{3}=12 \quad b_{2}=4m+6 \quad b_{1}=3m^{2}+10m+2 \quad b_{0}=\frac{2}{3}m^{3}+5m^{2}+m-s\\
\end{array}
\end{equation}
and we write $T_{KF}(m.n)$, $T_{DD-KF}(m,r)$ as follows
\begin{equation}
\begin{array}{ll}
T_{KF}(m,n)=a_{3}n^{3}+a_{2}n^{2}+a_{1}n+a_{0}\\
T_{DD-KF}(m,n)=b_{3}r^{3}+b_{2}r^{2}+b_{1}r+b_{0}
\end{array}.
\end{equation}
It holds
\begin{equation}
\begin{array}{ll}
Sc^f_{n_{sub},1}(m,n)&=\frac{a_{3}n^{3}+a_{2}n^{2}+a_{1}n+a_{0}}{n_{sub}(b_{3}r^{3}+b_{2}r^{2}+b_{1}r+b_{0})}\cdot r^{3}\cdot \frac{1}{r^{3}}=\frac{a_{3}n_{sub}^{3}+a_{2}\frac{n_{sub}^{3}}{n}+a_{1}\frac{n_{sub}^{3}}{n^{2}}+a_{0}\frac{n_{sub}^{3}}{n^{3}}}{n_{sub}(b_{3}+b_{2}\frac{1}{r}+b_{1}\frac{1}{r^{2}}+b_{0}\frac{1}{r^{3}})}\\
\\
&=\frac{n_{sub}^{3}\cdot(a_{3}+a_{2}\frac{1}{n}+a_{1}\frac{1}{n^{2}}+a_{0}\frac{1}{n^{3}})}{n_{sub}(b_{3}+b_{2}\frac{1}{r}+b_{1}\frac{1}{r^{2}}+b_{0}\frac{1}{r^{3}})}=\frac{a_{3}+a_{2}\frac{1}{n}+a_{1}\frac{1}{n^{2}}+a_{0}\frac{1}{n^{3}}}{b_{3}+b_{2}\frac{1}{r}+b_{1}\frac{1}{r^{2}}+b_{0}\frac{1}{r^{3}}}\cdot n_{sub}^{2}
\end{array}
\end{equation}
and 
\begin{equation}\label{factor_alfa2}
Sc^f_{n_{sub},1}(m,n)=\alpha(n,m,n_{sub})n_{sub}^{2}
\end{equation} 
where
\begin{equation}
\alpha(n,m,n_{sub})=\frac{a_{3}+a_{2}\frac{1}{n}+a_{1}\frac{1}{n^{2}}+a_{0}\frac{1}{n^{3}}}{b_{3}+b_{2}\frac{1}{r}+b_{1}\frac{1}{r^{2}}+b_{0}\frac{1}{r^{3}}}.
\end{equation}

%\begin{enumerate}

%\item For $n\rightarrow +\infty$, $r$ kept fixed, so that   $p$ increases too,  we have  $\alpha(n,m,n_{sub}) \sim 1 $.
%\item For $n$ fixed and $n_{sub} \sim n$ we get $r \sim 1$. We have that $\alpha(n,m,n_{sub})\leq 1$. 
%\end{enumerate}

%and 
%\begin{equation}\label{bound_alfa}
%Sc^f_{n_{sub},1}(m,n) \sim n_{sub}^{2}
%\end{equation} 
\normalsize
This result says that  the performance gain of DD-KF algorithm in terms of  reduction of time complexity, scales as the number of sub domains squared, where the scaling factor depends on parameter $\alpha(n,m,n_{sub})$.


\subsection{SWE set up}
Neglecting the Coriolis force and frictional forces and assuming unit width, SWEs are:
\begin{equation}
\left \{
\begin{array}{ll}
\frac{\partial h}{\partial t}+\frac{\partial vh}{\partial {x}}=0\\
\frac{\partial vh}{\partial t}+\frac{\partial (v^{2}h+\frac{1}{2}gh^{2})}{\partial {x}}=0
\end{array}
\right . 
\end{equation}
where the independent variables ${x}$ and $t$ make up the spatial dimension and time. The dependent
variables are $h$, which is the height with respect to the surface and $v$, i.e. 
the horizontal velocity, while $g$ is the gravitational acceleration.\\
In order to write  SWEs in a compact form, we introduce the vectors
\begin{equation}
\textbf{x}:=\left[\begin{array}{ll}
h\\
vh
\end{array}\right]\quad 
f(\textbf{x}):=\left[\begin{array}{ll}
vh\\
v^{2}h+\frac{1}{2}gh^{2}
\end{array}\right],
\end{equation}
the SWEs can be rewritten as follows
\begin{equation}\label{80}
\frac{\partial \textbf{x}}{\partial t}+\frac{\partial f(\textbf{x)}}{\partial {x}}=0.
\end{equation}
The initial boundary problem for the SWEs is 
\begin{equation}
\left\{\begin{array}{lllll}
\textbf{x}(t+\Delta t,x)=\mathcal{M}_{t,t+\Delta t}(\textbf{x}(t,x)) \quad \forall t, t+\Delta t \in [0,1.5]\\
\textbf{x}(0,x):=\left[\begin{array}{ll}h(0,x)\\
h(0,x)v(0,x)
\end{array}\right] =\left[\begin{array}{ll} 2+\sin(2\pi x)\\
0 \end{array} \right] \quad \forall x \in \Omega \end{array}\right.
\end{equation}
with the following reflective boundary conditions on $v$ and free boundary conditions on $h$
\begin{equation}\label{boundary_cond}
\textbf{x}(t,0):=\left[\begin{array}{ll}h(t,x_{1})\\
-h(t,x_{1})v(t,x_{1})
\end{array}\right]\quad
\textbf{x}(t,x_{n_{x}-1}):=\left[\begin{array}{ll}h(t,x_{n_{x}-2})\\
-h(t,x_{n_{x}-2})v(t,x_{n_{x}-2})
\end{array}\right] \quad \forall t\in \Delta
\end{equation} 
where
\begin{equation}\label{83}
\mathcal{M}_{t,t+\Delta t}(\textbf{x}(t,x)):=\textbf{x}(t,x)-\int_{t}^{t+\Delta t} \frac{\partial f(\textbf{x})}{\partial x} ds.
\end{equation}
We note that  $\mathcal{M}_{t,t+\Delta t}(\textbf{x}(t,x))$ should include Coriolis forces and also frictional forces, if they are present in the SWEs; in particular, as these quantities  show off as the right hand side of (\ref{80}), they will be also included in the right hand side of (\ref{83}) as additive terms under  the integral. \\ 
The state of the system at each time $t_{k+1}\in \Delta$, $k=0,1,\ldots,nt-2$ is:
\begin{equation}
\textbf{x}(t_{k+1})\equiv \textbf{x}_{k+1}:=\left[\begin{array}{lll}
x[1]_{k+1}\\
x[2]_{k+1}  
\end{array}\right]\in \mathbb{R}^{2(n_{x}-2)}
\end{equation}
where
\begin{equation}
 \begin{array}{lll}
  x[1]_{k+1}&\:= \{x[1](t_{k+1},x_{i})\}_{i=1,\ldots,n_{x}-1} &:=\{h(t_{k+1},x_{i})\}_{i=1,\ldots,n_{x}-1} \in \mathbb{R}^{(n_{x}-2)} \\ x[2]_{k+1}&:= \{x[2](t_{k+1},x_{i})\}_{i=1,\ldots,n_{x}-1}&:=\{v(t_{k+1},x_{i})h(t_{k+1},x_{i})\}_{i=1,\ldots,n_{x}-1} \in \mathbb{R}^{(n_{x}-2)} 
 \end{array}.
 \end{equation}
\noindent From  Lax-Wendroff scheme \cite{LeVeque}, we obtain the following discrete formulation of the SWEs
\begin{equation}\label{modello_swes}
\textbf{x}_{k+1}=M_{k,k+1}\textbf{x}_{k}+b_{k}+w_{k}
\end{equation}
where 
\begin{equation}
M_{k,k+1}=\left[\begin{array}{lllllllllllll}
M[1]_{k,k+1} \ \ O_{n_{x}-2}\\
M[2,1]_{k,k+1} \ \ M[2]_{k,k+1}
\end{array}\right] \in \mathbb{R}^{2(n_{x}-2)\times 2(n_{x}-2)} 
\end{equation}
with $O_{n_{x}-2}\in \mathbb{R}^{n_{x}-2\times n_{x}-2}$ the null matrix and $M[1]_{k,k+1}\in \mathbb{R}^{(n_{x}-2)\times (n_{x}-2)}$, $M[2,1]_{k,k+1}\in \mathbb{R}^{(n_{x}-2)\times (n_{x}-2)}$, $M[2]_{k,k+1}\in \mathbb{R}^{(n_{x}-2)\times (n_{x}-2)}$ the following tridiagonal matrices 
{\footnotesize
\begin{equation}
M[1]_{k,k+1}=\left[\begin{array}{lllllllllllll}
\psi_{1}^{k} & \eta_{2}^{k} & & & \\
-\eta_{1}^{k}& \psi_{2}^{k}  &  \eta_{3}^{k} & &  \\
& \ddots & \ddots & \ddots   \\
& &-\eta_{n_{x}-4}^{k} & \psi_{n_{x}-3}^{k}  &\eta_{n_{x}-2}^{k}  \\
& &  &-\eta_{n_{x}-3 }^{k} & \psi_{n_{x}-2}^{k}
\end{array}\right]
\end{equation}
\begin{equation}
M[2,1]_{k,k+1}=\left[\begin{array}{lllllllllllll}
0 & \chi_{2}^{k}& & & \\
-\chi_{1}^{k}& 0 &  \chi_{3}^{k} & & \\
& \ddots & \ddots & \ddots   \\
& &-\chi_{n_{x}-4}^{k} & 0  &\chi_{n_{x}-2}^{k}  \\
& &  &-\chi_{n_{x}-3 }^{k}  & 0
\end{array}\right]
\end{equation}
\begin{equation}
M[2]_{k,k+1}=\left[\begin{array}{lllllllllllll}
\phi_{1}^{k} & \xi_{2}^{k}& & & \\
-\xi_{1}^{k}& \phi_{2}^{k} & \xi_{3}^{k} & & \\
& \ddots & \ddots & \ddots   \\
& &-\xi_{n_{x}-4}^{k} & \phi_{n_{x}-3}^{k}  &\xi_{n_{x}-2}^{k}  \\
& &  &-\xi_{n_{x}-3}^{k}  & \phi_{n_{x}-2}^{k}
\end{array}\right]
\end{equation}
and the vector $b\in \mathbb{R}^{2(n_{x}-2)}$
\begin{equation}
b_{k}:=\left[\begin{array}{ll}
b[1]_{k}\\
b[2]_{k}
\end{array}\right]\in \mathbb{R}^{2(n_{x}-2)}
\end{equation}
with 
\begin{equation}\label{b[1]}
b[1]_{k}:=\left[\begin{array}{ll}
\eta_{0}^{k}h_{0}^{k}\\
0\\
\vdots\\
0\\
\eta_{n_{x}-1}^{k}h_{n_{x}-1}^{k}
\end{array}\right]\in \mathbb{R}^{n_{x}-2} \quad 
b[2]_{k}:=\left[\begin{array}{ll}
\xi_{0}^{k}v_{0}^{k}h_{0}^{k}-\chi_{0}^{k}h_{0}^{k}\\
0\\
\vdots
\\
0
\\
\xi_{n_{x}-1}^{k}h_{n_{x}-1}^{k}+\xi_{n_{x}-1}^{k}v_{n_{x}-1}^{k}h_{n_{x}-1}^{k}
\end{array}\right]\in \mathbb{R}^{n_{x}-2} 
\end{equation}
}
\noindent where 
\begin{displaymath}
\begin{array}{lllllll}
\eta_{i}^{k}&:=\frac{\alpha}{2}[v_{i}^{k}+\alpha((v_{i}^{k})^{2}+\frac{1}{2}gh_{i}^{k})] & \quad  \psi_{i}^{k}&:=1-4\alpha^{2}((v_{i}^{k})^{2}+\frac{1}{2}gh_{i}^{k})\\
\xi_{i}^{k}&:=\frac{\alpha}{2}[\alpha + v_{i}^{k}] & \quad  \phi_{i}^{k}&:=\left(1+4\alpha^{2}\right)
\end{array}
\end{displaymath}
\begin{displaymath}
\chi_{i}^{k}:=\frac{1}{2}gh_{i}^{k}
\end{displaymath}
with $\alpha:=\frac{\Delta t}{2\Delta x}$ and $h_{i}^{k}:=h(t_{k},x_{i})$, $v_{i}^{k}:=v(t_{k},x_{i})$.\\
We note that the discrete model in (\ref{modello_swes}) can be rewritten as follows
\begin{equation}\label{1-2_model}
\begin{array}{ll}
x[1]_{k+1}={M}[1]_{k,k+1}x[1]_{k}+b[1]_{k}\\
x[2]_{k+1}={M}[2]_{k,k+1}x[2]_{k}+\tilde{b}[2]_{k}
\end{array}
\end{equation}
with 
\begin{equation}\label{tilde_b[2]}
\tilde{b}[2]_{k}:={b}[2]-M[2,1]_{k,k+1}x[1]_{k}.
\end{equation}
In particular, if we define 
\begin{equation}\label{S-C}
S:=\max \left(|v-\sqrt{gh}|,|v+\sqrt{gh}|\right),
\end{equation}
the stability condition of  Lax-Wendroff is
\begin{equation}\label{S-Cbis}
S\cdot \frac{\Delta t}{\Delta x} \le 1,
\end{equation}
for the condition (\ref{S-Cbis}) to be satisfied, at each iteration $k=0,1,\ldots,nt$ we choose: $\Delta t=0.8\cdot \frac{\Delta x}{S}$.
\\
\begin{enumerate}
\item Decomposition Step.
\begin{itemize}
\item Decomposition of  $\Omega$ into two subdomains with overlap region:
\begin{equation}
\begin{array}{ll} 
\Omega_{1}=[0,x_{n_{1}}] \quad \textrm{with $D_{n_{1}+1}(\Omega_1)=\{x_{i}\}_{i=0,1,\ldots,n_1}$ }\\
\Omega_{2}=[x_{n_{1}-s+1},1] \quad  \textrm{with $D_{n_{2}+1}({\Omega}_{2})=\{x_{i}\}_{i=n_{1},\ldots,n-1}$}
\end{array}
\end{equation}
\item Decomposition of  $\Delta$ into two subsets with overlap region:
\begin{equation}
\begin{array}{ll}
\Delta_{1}=[0,t_{s_{1}-1}] \quad \textrm{with $D_{s_{1}}(\Delta_{1})=\{t_{k}\}_{k=0,\ldots,s_{1}-1}$}  \\
\Delta_{2}=[t_{s_{1}-s_{1,2}+1},1.5] \quad  \textrm{with $D_{s_{2}}(\Delta_{2})=\{t_{k}\}_{k=s_{1}-s_{1,2}+1,\ldots,nt-1}$}.
\end{array}
\end{equation}
\end{itemize}
\item Local initial conditions on  $\Delta_{1}$ and $\Delta_{2}$:
\begin{equation}
\begin{array}{ll}
x[1]_{0}^{\Delta_{1}}=h(0,x)=:x[1]_{0}^{\Delta_{0}}\\
x[2]_{0}^{\Delta_{1}}=h(0,x)v(0,x)=:x[2]_{0}^{\Delta_{0}}
\end{array}\quad
\begin{array}{ll}
x[1]_{{s_{1}-1}}^{\Delta_{2}}=x[1]_{{s_{1}-1}}^{\Delta_{1}}\\
x[2]_{{s_{1}-1}}^{\Delta_{2}}=x[2]_{{s_{1}-1}}^{\Delta_{1}}
\end{array}.
\end{equation}
 
\item Model reduction in (\ref{1-2_model}).
\\
Decomposition of matrices $M[1]_{k+1}\in \mathbb{R}^{n_{x}-2}$ at each step $k=0,1,\ldots,nt-2$ 
{\footnotesize
\begin{equation}
M[1]_{k,k+1}^{1}:=M[1]_{k,k+1}|_{I_{1}\times I_{1}}=\left[\begin{array}{lllllllllllll}
\psi_{1}^{k} & \eta_{2}^{k} & & & \\
-\eta_{1}^{k}& \psi_{2}^{k} &  \eta_{3}^{k} & &  \\
& \ddots & \ddots & \ddots   \\
& &-\eta_{n_{1}-2}^{k} & \psi_{n_{1}-1}^{k}  &\eta_{n_{1}}^{k}  \\
& &  &-\eta_{n_{1}-1 }^{k} &  \psi_{n_{1}}^{k} 
\end{array}\right]\in \mathbb{R}^{n_{1}\times n_{1}}
\end{equation}
\begin{equation}
M[1]_{k,k+1}^{1,2}:=M[1]_{k,k+1}|_{I_{1}\times I_{2}}=\left[\begin{array}{lllllllllllll}
0 &\cdots  &\cdots &\cdots &0 &\cdots &0 \\
0& \cdots &\cdots  &\cdots &0 &\cdots & 0 \\
\vdots & &  & & & &\vdots \\
& &  & & \\
0&\cdots &0& \eta_{n_{1}+1}^{k} &0    & \cdots  &0  \\
\end{array}\right]\in \mathbb{R}^{n_{1}\times n_{2}}
\end{equation}
\begin{equation}
M[1]_{k,k+1}^{2,1}:=M[1]_{k,k+1}|_{I_{1}\times I_{2}}=\left[\begin{array}{lllllllllllll}
0 & \cdots&0 &0 &\eta_{n_{1}}^{k} &0 &\cdots &0 \\
0& \cdots &  0 &\cdots &0 &0 &\cdots &0 \\
\vdots& & & & \vdots &\vdots & & \vdots \\
& &  & & \\
0& \cdots &  0 &\cdots &0&0 &\cdots &0 \\
\end{array}\right]\in \mathbb{R}^{n_{2}\times n_{1}}
\end{equation}
\begin{equation}
M[1]_{k,k+1}^{2}:=M[1]_{k,k+1}|_{I_{2}\times I_{2}}=\left[\begin{array}{lllllllllllll}
\psi_{n_{1}}^{k} & \eta_{n_{1}+1}^{k} & & & \\
-\eta_{n_{1}}^{k}& \psi_{n_{1}+1}^{k} &  \eta_{n_{1}+2}^{k} & &  \\
& \ddots & \ddots & \ddots   \\
& &-\eta_{n_{x}-4}^{k} & \psi_{n_{x}-3}^{k} &\eta_{n_{x}-2}^{k}  \\
& &  &-\eta_{n_{1}-3 }^{k} & \psi_{n_{x}-2}^{k}
\end{array}\right]\in \mathbb{R}^{n_{2}\times n_{2}}.
\end{equation}
}
\noindent We  proceed in the same way to get the decomposition of  matrix $M[2]_{k,k+1} \in \mathbb{R}^{n_{x}-2\times n_{x}-2}$ in (\ref{1-2_model}) into $M[2]_{k,k+1}^{1} \in \mathbb{R}^{n_{1}\times n_{1}}$, $M[2]_{k,k+1}^{1,2}\in \mathbb{R}^{n_{1}\times n_{2}}$, $M[2]_{k,k+1}^{2,1}\in \mathbb{R}^{n_{2}\times n_{1}}$ and $M[2]_{k,k+1}^{2}\in \mathbb{R}^{n_{2}\times n_{2}}$.



\item Decomposition of  $H_{k+1}\in \mathbb{R}^{m\times (n_{x}-2)}$ into $H_{k+1}^{1}\in \mathbb{R}^{m\times n_{1}}$ and $H_{k+1}^{2}\in \mathbb{R}^{m\times n_{2}}$ as in (\ref{mat_H}).
\end{enumerate}



\subsection{DD-KF method.}
\begin{enumerate}
\item For $j=1,2$ and $k=\bar{s}_{j-1}+1,\ldots,\bar{s}_{j-1}+s_{j}$, we apply  DD-KF method on $\Delta_{1}$ and $\Delta_{2}$ by considering the following matrices:
\begin{equation}
\begin{array}{llllll}
M_{1}[i]&\equiv M[i]_{k,k+1}^{1} &\quad M_{1,2}[i]&\equiv M[i]_{k,k+1}^{1,2}\\
M_{2}[i]&\equiv M[i]_{k,k+1}^{2}& \quad M_{2,1}[i]&\equiv M[i]_{k,k+1}^{2,1}\\
P_{1}&=O_{n_{1}\times n_{1}}& \quad  P_{2}&=O_{n_{2}\times n_{2}}\\ P_{1,2}&=O_{n_{1}\times n_{2}} & \quad P_{2,1}&=O_{n_{2}\times n_{1}}
\end{array}.
\end{equation}
In particular, we note:
\begin{equation}
    M_{1,2}[i]\equiv \left[\begin{array}{lll} O & M_{1,3}[i]\\ 
    O& M_{2,3}[i]
    \end{array}\right]\quad  M_{2,1}[i]\equiv \left[\begin{array}{lll} \tilde{M}_{1,2}[i] & O\\ 
     \tilde{M}_{2,1}[i] & O
    \end{array}\right].
\end{equation}
\item Send and receive  boundary conditions from  adjacent domains and compute the vectors:
\begin{equation}
b_{1,k}[i]=\left[\begin{array}{lll} M_{1,3}[i]\\ 
    M_{2,3}[i]
    \end{array}\right]\widehat{x}_{2,k}[i]|_{\Gamma_{1}} \quad
    b_{2,k}[i]=\left[\begin{array}{lll} \tilde{M}_{1,2}[i]\\ 
    \tilde{M}_{2,1}[i]
    \end{array}\right]\widehat{x}_{1,k}[i]|_{\Gamma_{2}}.
\end{equation}
\item For $j=1,2$ and $k=\bar{s}_{j-1}+1,\ldots,\bar{s}_{j-1}+s_{j}$, compute the predicted state estimates $x_{1,k+1}[i]\in \mathbb{R}^{n_{1}}$ and $x_{2,k+1}[i]\in \mathbb{R}^{n_{2}}$ for $ i=1,2$, as follows:
\begin{equation}
\begin{array}{ll}
x_{1,k+1}[i]=M_{1}[i]\widehat{x}[i]_{k}^{\Delta_{j-1}}|_{I_{1}}+\bar{b}[i]_{k}|_{I_{1}}+b_{1,k}\\
x_{2,k+1}[i]=M_{2}[i]\widehat{x}[i]_{\bar{s}_{j}}^{\Delta_{j-1}}|_{I_{2}}+\bar{b}[i]|_{I_{2}}+b_{2,k}
\end{array},
 \end{equation}
 with 
 \begin{equation}
 \bar{b}[i]_{k}=\left\{\begin{array}{lll}
 b[1]_{k} & \textrm{in (\ref{b[1]})} & \textrm{if $i=1$}\\
  \tilde{b}[2]_{k} &\textrm{in (\ref{tilde_b[2]})} & \textrm{if $i=2$} 
 \end{array}.\right.
 \end{equation}
\item For $j=1,2$ and $k=\bar{s}_{j-1},\ldots,\bar{s}_{j-1}+s_{j}-1$, compute DD-KF estimates as in (\ref{stimastato_DD}) on $\Delta_{j}$ i.e. $ \widehat{x}_{1,k+1}^{\Delta_{j}}[i] \equiv \widehat{x}[i]_{1,k+1}\in \mathbb{R}^{n_{1}}$ and $\widehat{x}_{2,k+1}^{\Delta_{j}}[i] \equiv \widehat{x}[i]_{2,k+1}\in \mathbb{R}^{n_{2}}$. In particular, in the time interval $\Delta_{j}$, by considering the boundary conditions in (\ref{boundary_cond}), DD-KF estimates on $\Omega_{1}$ and $\Omega_{2}$ are 
\begin{equation}
\begin{array}{ll}
\widehat{x}_{k+1}^{\Omega_{1}\times \Delta_{j}}[i] &:=\left[\begin{array}{ll}
{x}(t_{k+1},0)[i]\\
\widehat{x}_{1,k+1}^{\Delta_{j}}[i]\\
\widehat{x}_{2,k+1}^{\Delta_{j}}[i](1)
\end{array}\right]\in \mathbb{R}^{n_{1}+1} \\
\widehat{x}_{k+1}^{\Omega_{2}\times \Delta_{j}}[i] &:=\left[\begin{array}{ll}
\widehat{x}_{1,k+1}^{\Delta_{j}}[i](n_{1}-s)\\
\widehat{x}_{2,k+1}^{\Delta_{j}}[i]\\
{x}(t_{k+1},x_{n_{x}-1})[i]
\end{array}\right]\in \mathbb{R}^{n_{2}+1} 
\end{array}
\end{equation}
where $\widehat{x}_{r,k+1}^{\Delta_{j}}[i](1)$, $\widehat{x}_{r,k+1}^{\Delta_{j}}[i]( n_{1}-s)$ are the first and the $n_{1}-s$ components of $\widehat{x}_{r,k+1}^{\Delta_{j}}[i]$, $r=1,2$.
We refer to 
\begin{equation}
\widehat{x}_{k+1}[i]^{\Omega\times \Delta_{j}}:=\left[\begin{array}{ll}
\widehat{x}_{k+1}^{\Omega_{1}\times \Delta_{j}}[i]\\
\widehat{x}_{k+1}^{\Omega_{1,2}\times \Delta_{j}}[i]\\
\widehat{x}_{k+1}^{\Omega_{2} \times \Delta_{j}}[i]
\end{array}\right]\in \mathbb{R}^{n_{x}}
\end{equation}
as the estimate of the wave height and velocity (if $i=1,2$ respectively) obtained by applied the DD-KF method on $\Omega$, where on the spatial overlap $\Omega_{1,2}$, we have considered the arithmetic mean between DD-KF estimates i.e.
\begin{equation}
 \widehat{x}_{k+1}^{\Omega_{1,2}\times \Delta_{j}}[i]:=\left[\begin{array}{ll}
 \frac{\widehat{x}_{k+1}^{\Omega_{1}\times \Delta_{j}}[i]|_{I_{1,2}
}+ \widehat{x}_{k+1}^{\Omega_{2}\times \Delta_{j}}[i]|_{I_{1,2}
}}{2} \end{array}\right]
\end{equation}
with $I_{1,2}$ the index set defined in (\ref{set_indici}).
\end{enumerate}
\clearpage

