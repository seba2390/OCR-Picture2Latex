%% Template for a preprint Letter or Article for submission
%% to the journal Nature.
%% Written by Peter Czoschke, 26 February 2004
%%

\documentclass{nature}

%% make sure you have the nature.cls and naturemag.bst files where
%% LaTeX can find them
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{paralist}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{morefloats}
\usepackage{multirow}
\usepackage{color}
\usepackage{paralist}
\usepackage{todonotes}
\usepackage{wrapfig}
%\usepackage[misc]{ifsym}
\usepackage{bbding}
\usepackage{tabularx}
\usepackage{enumitem}
%\usepackage{figcaps}
%\usepackage[printfigures]{figcaps}

\renewcommand{\baselinestretch}{1.0}

\newcommand{\rev}[1]{\textcolor{red}{#1}}
\newcommand{\revb}[1]{\textcolor{blue}{#1}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  
\newcommand\blfootnote[1] % Used to show acknowledgement when using llncs
{
  \begingroup
  \renewcommand\thefootnote{}
  \footnotetext{#1}
  \addtocounter{footnote}{-1}
  \endgroup
}

\hyphenation{trans-lation}

\title{Learning Multi-Site Harmonization of Magnetic Resonance Images Without Traveling Human Phantoms}

%% Notice placement of commas and superscripts and use of &
%% in the author list

\author{Siyuan Liu\textsuperscript{1} and Pew-Thian Yap\textsuperscript{1,\Envelope}}
\begin{document}

\maketitle

\begin{affiliations}
 \item Department of Radiology and Biomedical Research Imaging Center (BRIC), University of North Carolina at Chapel Hill, NC, U.S.A.
\end{affiliations}

\begin{abstract}
Harmonization improves data consistency and is central to effective integration of diverse imaging data acquired across multiple sites.
Recent deep learning techniques for harmonization are predominantly supervised in nature and hence require imaging data of the same human subjects to be acquired at multiple sites. 
Data collection as such requires the human subjects to travel across sites and is hence challenging, costly, and impractical, more so when sufficient sample size is needed for reliable network training. Here we show how harmonization can be achieved with a deep neural network that does not rely on traveling human phantom data.
Our method disentangles site-specific appearance information and site-invariant anatomical information from images acquired at multiple sites and then employs the disentangled information to generate the image of each subject for any target site. We demonstrate with more than 6,000 multi-site T1- and T2-weighted images that our method is remarkably effective in generating images with realistic site-specific appearances without altering anatomical details. Our method allows retrospective harmonization of data in a wide range of existing modern large-scale imaging studies, conducted via different scanners and protocols, without additional data collection.
\end{abstract}

\blfootnote{\noindent \Envelope~Corresponding author: Pew-Thian Yap (\texttt{ptyap@med.unc.edu})}

\section*{Introduction}
Magnetic resonance imaging (MRI) is a non-invasive and versatile technique that provides good soft tissue contrasts useful for diagnosis, prognosis, and treatment monitoring.
Since MRI experiments are costly and time-consuming, modern large-scale MRI studies typically rely on multiple imaging sites to collaboratively collect data with greater sample sizes for more comprehensive coverage of factors that can affect study outcomes, such as age, gender, geography, socioeconomic status, and disease subtypes. Notable examples of multi-site studies include the Adolescent Brain Cognitive Development (ABCD)\cite{Jernigan2018The}, 
%the Human Connectome Project (HCP)\cite{Essen2012The}, 
the Alzheimer's Disease Neuroimaging Initiative (ADNI)\cite{Mueller2005The}, and the Australian Imaging, Biomarkers and Lifestyle Flagship Study of Aging (AIBL)\cite{Ellis2009The}.

Multi-site data collection leads inevitably to undesirable elevation of non-biological variability introduced by differences in scanners\cite{Shinohara2017Volumetric} and imaging protocols\cite{Pomponio2020Harmonization}. 
Protocols can be prospectively harmonized by selecting for the individual sites the acquisition parameters that result in images with maximal inter-site consistency.
%\cite{Velden2020Harmonization,Jones2013Quantification}.
However, prospective harmonization requires extensive data acquisition for parameter tuning, needs to be performed before each study, and does not allow for correction of data collected in studies that have already taken place.
Moreover, significant inter-site variability can still occur in data collected with harmonized acquisition protocols simply due to irreconcilable differences between scanners\cite{Shinohara2017Volumetric}.
%\todo{Any other problems/challenges associated with prospective harmonization?}
Retrospective MRI harmonization\cite{Yu2018Statistical} overcomes these limitations by performing post-acquisition correction and is hence applicable to existing studies for
%aim to eliminate non-biological variability and 
%so that quantitatively and qualitatively consistent images can be produced for standardization of automated image processing and for visual inspections by reviewers, 
improving the accuracy, reliability, and reproducibility of downstream processing, analyses, and inferences. 

\begin{table*}[!t]
	\renewcommand\arraystretch{1.5}
	\scriptsize
	\centering
	\caption{Existing MRI harmonization methods}
	\begin{tabularx}{\textwidth}{l|l|X|X}
		\specialrule{2pt}{8pt}{0pt}
		\multicolumn{2}{l|}{Category}                                                                                          & Method               & Summary        \\ \hline
		%		Statistic-based & Parametric resetting  & Statistical analysis\cite{Velden2020Harmonization} & Resetting MRI acquisition parameters based on evaluations of effects of different parameter settings \\ \cline{2-4} 
		%		& Statistic modeling & Mixed-effects modeling\cite{Jones2013Quantification} & Building a mixed-effects model of multiple statistic metrics\\  \hline
		Statistics & Intensity normalization & Histogram matching\cite{He2013Intensity,Nyul1999On,Shah2011Evaluating} & Align distributions of voxel intensity values based on an image template constructed from several control subjects.\\ \cline{3-4} 
		& & White stripe\cite{Shinohara2014Statistical}  & Normalize intensity values based on patches of normal appearing white matter (NAWM). Rescaled intensity values are biologically interpretable as units of NAWM.\\ \cline{3-4}
		&  & Multi-site image harmonization by cumulative distribution function alignment (MICA)\cite{Wrobel2020Intensity} & Transform voxel intensity values of one scan to align with the intensity cumulative distribution function (CDF) of a second scan.\\ \cline{2-4}
		& Batch effect adjustment & Removal of Artificial Voxel Effect by Linear regression (RAVEL)\cite{Fortin2016Removing} & Estimate using a control region of the brain the latent factors of unwanted variation common to all voxels.\\ \cline{3-4} 
		&  & Combating batch effects (ComBat)\cite{Fortin2017Harmonization,Fortin2018Harmonization} & Identify batch-specific transformation to express all data in a common space. \\ \hline
		Learning  & Machine learning & Regression ensembles with patch learning for image contrast agreement (REPLICA)\cite{Jog2017Random} & Supervised training of a random forest for nonlinear regression of a target contrast.\\ \cline{3-4}
		& & NeuroHarmony\cite{Garcia-Dias2020Neuroharmony} & Supervised training of a random forest for nonlinear regression of a target contrast determined based on prescribed regions.\\ \cline{2-4}
		& Deep learning & DeepHarmony\cite{Dewey2019Deep} & Supervised training of a U-Net to produce images with a consistent contrast.\\ \cline{3-4} 
		& & Disentangled Latent Space (DLS)\cite{Dewey2020A} & Supervised training of a deep neural network using disentangled anatomical and contrast components.\\ \cline{3-4}
		& & Unlearning of dataset bias\cite{Dinsdale2021Deep} & Supervised training of a deep neural network to learn scanner-invariant features and then reducing scanner influence on network predictions in tasks of interest.\\ \cline{3-4}
		& & \textbf{MURD} & \textbf{Unsupervised} harmonization of images using content and style disentanglement learned from jointly multi-site images.\\ \cline{3-4}
		\specialrule{2pt}{-1.5pt}{-10pt}
	\end{tabularx}\label{tab:overview}
\end{table*}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=.99\textwidth]{overall.pdf}
		\vspace{-5pt}
	\caption{\textbf{Overview of MURD.}
		\textbf{a}, Multi-site representation disentanglement. Given image sets from $N$ sites $\{\mathcal{X}_i\}_{i=1}^N$, MURD disentangles each image set $\mathcal{X}_i$ in a site-invariant content space $\mathcal{C}$ and a site-specific style space $\mathcal{S}_i$ with the content features aligned in the content space $\mathcal{C}$. This is achieved using content-style disentangled cycle translation (CS-DCT).
		\textbf{b}, Site-specific CS-DCT. An image from the $i$-th site $x_i$ is encoded in content space $\mathcal{C}$ and style space $\mathcal{S}_i$ to obtain content features $c_i$ and style features $s_i$. Style features of the $j$-th site $s_j$ ($j\neq i$) are generated using style generator $G^{\text{S}}_j$. $s_j$ and $c_i$ decoded to generate a harmonized image for the $j$-th site $\tilde{x}_j$, which is in turn encoded to generate content features $\tilde{c}_j$ for reconstructing image $\hat{x}_i$ with style features $s_i$.
%		An image $x_i$ from the image set of the $i$-site $\mathcal{X}_i$ is encoded in site-invariant content space $\mathcal{C}$ and in site-specific style space $\mathcal{S}_i$ to obtain content features $c_i$ and style features $s_i$, respectively.
%		Style features $s_j$ in style space $\mathcal{S}_j$ for site $j\neq i$ are generated using style generator $G^{\text{S}}_j$.
%		Content features $c_i$ and style features $s_j$ are decoded to generate harmonized image $\tilde{x}_j$, which is in turn encoded in the content space $\mathcal{C}$ to obtain content features $\tilde{c}_j$.
%		Content features $\tilde{c}_j$ and style features $s_i$ are decoded to reconstruct image $\hat{x}_i$.
		\textbf{c}, Reference-specific CS-DCT. Unlike site-specific CS-DCT in \textbf{b}, style features $s_j$ are obtained by encoding a reference image $x_j$ in style space $\mathcal{S}_j$.
		\textbf{d}, The MURD framework. MURD implements CS-DCT by employing a site-shared content encoder, a site-specific style encoder, a site-shared generator, a site-specific style generator, and a site-specific discriminator for content encoding, style encoding, decoding, style generation, and adversarial learning, respectively. MURD is constrained by four types of losses: consistency losses $\mathcal{L}_\text{cyc}$, $\mathcal{L}_\text{cont}$ and $\mathcal{L}_\text{sty}$, adversarial loss $\mathcal{L}_\text{adv}$, content alignment loss $\mathcal{L}_\text{ca}$, and identity loss $\mathcal{L}_\text{id}$.
%		MURD takes a $i$-site image $x_i$ as input and utilizes content encoder $E^{\text{C}}$ and style encoder $E^{\text{S}}_i$ to extract content features $c_i$ and style features $s_i$, respectively.
%		$c_i$ together with $j$-site style features $s_j$, which are generated from style generator $G^{\text{S}}_j$, are fed to generator $G$ to generate harmonized image $\tilde{x}_j$, which discriminator $D_j$ then classifies as being either real or fake.
%		Content features $\tilde{c}_j$ and style features $\tilde{s}_j$ of the harmonized image $\tilde{x}_j$ are encoded via content encoder $E^{\text{C}}$ and style encoder $E^{\text{S}}_j$, respectively, and are respectively pixel-wise consistent with $c_i$ and $s_j$.
%		$\tilde{c}_j$ and $s_i$ are fed to $G$ to reconstruct image $\hat{x}_i$, which is pixel-wise and gradient-wise consistent with $x_i$.
%		$c_i$ and $s_i$ are fed to $G$ to generate the identity image $\bar{x}_i$ of $x_i$.  
		%		\textbf{e}, Network architecture of generator $G$.
		%		\textbf{f}, Network architecture of content encoder $E^{\text{C}}$.
		%		\textbf{g}, Network architecture of style encoder $E^{\text{S}}_i$.
		%		\textbf{h}, Network architecture of style generator $G^{\text{S}}_j$.
		%		\textbf{i}, Network architecture of discriminator $D_j$.
		\label{fig:framework}
	}
\end{figure*}

Existing retrospective harmonization methods are either 
%\rev{statistics},\todo{Are statistics-based method retrospective harmonization method?]} 
statistics-based or learning-based (Table~\ref{tab:overview}).
%Statistics methods\cite{Velden2020Harmonization,Jones2013Quantification} attempt to match statistics of image features such as those derived from tissue segmentation for image harmonization.
%Harmonization can be done by aligning \cite{Velden2020Harmonization,Jones2013Quantification} image-derived features such 
%, which are performed by either statistical analysis or  mixed-effects modeling, 
%
%utilize specific measures obtained by some analysis methods (e.g., segmentation) to evaluate images acquired with different parametric settings and then select optimal ones, but these specific measures are not always available in practice due to the lack of appropriate analysis methods.
Statistics-based methods align intensity distributions via intensity normalization\cite{Shah2011Evaluating,Nyul1999On,He2013Intensity,Shinohara2014Statistical,Wrobel2020Intensity} or batch effect adjustment\cite{Fortin2016Removing,Fortin2017Harmonization,Fortin2018Harmonization}.
However, these methods are typically limited to whole-image, but not detail-specific, harmonization.
%
Learning-based methods translate images between sites via nonlinear mappings determined using machine learning\cite{Jog2017Random,Garcia-Dias2020Neuroharmony} or deep learning\cite{Dewey2019Deep,Dewey2020A,Dinsdale2021Deep}, with or without supervision.
Machine learning methods predict harmonized images using regression models learned, typically with supervision,  with hand-crafted features.
In contrast, deep learning techniques automatically extract features pertinent to the harmonization task. Supervised methods typically require training data acquired from traveling human phantoms, which might not always be available for large-scale, multi-site, or longitudinal studies.
Unsupervised deep learning techniques\cite{Liu2017Unsupervised,Zhu2017Toward,Isola2017Image} determine mappings between sites using unpaired images and therefore avoid the need for traveling human phantom data. These methods are however unscalable to large-scale multi-site MRI harmonization as they typically learn pair-wise mappings between sites.
For $N$ sites, these methods learn $N(N-1)$ mappings for all site pairs and therefore require a large amount of data for learning the multitude of network parameters. These pair-wise methods are also ineffective by not fully and jointly utilizing complementary information available from all sites. 
%Scalability is therefore limited.

%To improve multi-site learning efficiency, unsupervised one-to-many translation methods\cite{Anoosheh2018ComboGAN,Choi2018StarGAN} utilize all-site unpaired images to simultaneously learn mappings between one site and the other available sites in a unified framework, which first encode images to features in a latent space specified by a site label (e.g., one-hot vector) and then decode these features back to specified-site images.
%These methods assume that all encoded features should be site-specific, the fact however is that only partial features are site-specific (e.g., brain contrast) and many of them are site-invariant (e.g., brain anatomical structure).
%This assumption might introduce anatomical structure changes and contribute to harmonization failure.

%Here we consider the multi-site MRI harmonization as image translations among multiple sites.

Here, we draw inspiration from recent advancements in multi-domain image-to-image trans\-lation\cite{Anoosheh2018ComboGAN,Choi2018StarGAN,Choi2020StarGANv2} and introduce a unified  framework for simultaneous multi-site harmonization using only a single deep learning model. Our method, called multi-site unsupervised representation disentangler (MURD), decomposes an image into  anatomical content that is invariant across sites and appearance style (e.g., intensity and contrast) that is dependent on each site. Harmonized images are generated by combining the content of the original image with styles specific to the different sites.
% by specifying either site (site-guided) or reference image (reference-guided).
More specifically, encoding an image in site-invariant and site-specific feature spaces is achieved via two encoders, i.e., a content encoder that captures anatomical structures common across sites and a style encoder that captures style information specific to each site.
%
A site-harmonized image is generated via a generator that combines the extracted content with the style associated with a target site. 
The target style can be specified by a reference image from a site or by a randomized style code generated by a style generator specific to each site. The latter allows multiple appearances to be generated in relation to natural style variations associated with each site.
%The site-specific style feature can be generated by a style generator with random Gaussian noise as input or extracted from a reference image from specific site via the style encoder.
%Considering multiple sites, both style generator and style encoder are designed with multiple output branches, each of which provides style features for a specific site.
The network is trained with losses that are designed to promote full representation disentanglement and to maximally preserve structural details. MURD is summarized in Figure~\ref{tab:overview} and detailed in the Methods section.
%In addition to network design, we introduce a style consistency loss, a content consistency loss, and a content alignment loss to achieve fully representation disentanglement and employ an adversarial loss to ensure the reality of harmonized images.
%We also employ a cycle consistency loss to avoid degradation of structural details in pixel and gradient levels and use an identity loss to preserve pixel-wise image quality.
%The architecture of MURD is summarized in Figure 1 and detailed in the Methods section.

%To improve multi-site learning efficiency, unsupervised one-to-many translation methods\cite{Anoosheh2018ComboGAN,Choi2018StarGAN} utilize all-site unpaired images to simultaneously learn mappings between one site and the other available sites in a unified framework, which first encode images to features in a latent space specified by a site label (e.g., one-hot vector) and then decode these features back to specified-site images.
%These methods assume that all encoded features should be site-specific, the fact however is that only partial features are site-specific (e.g., brain contrast) and many of them are site-invariant (e.g., brain anatomical structure).
%This assumption might introduce anatomical structure changes and contribute to harmonization failure.
%\rev{To solve this issue, several methods\cite{Anoosheh2018ComboGAN,Choi2018StarGAN} have been proposed to learn many-to-many mappings among all available sites by specifying target site with a site label (e.g., one-hot vector) or site features.
%Note that there exists an assumption for these method that extracted features in a common latent space can be used to learn all mappings under the control of site information (label or features), the fact however is that only partial extracted features are site-invariant (e.g., brain anatomical structure), some of them are site-specific (e.g., brain contrast).
%This assumption might increase the instability of these mappings and then yields incorrect harmonization.}\todo{Needs to be rewritten to make it easy to understand.}
%To this end, disentangled representation learning was proposed to learn the domain-invariant content information and the domain-specific style information in a supervised or unsupervised fashion via some property constraints (e.g., adversarial constraints\cite{Mathieu2016Disentangling,Denton2017Unsupervised}, cycle consistency\cite{Jha2018Disentangling, Huang2018Multimodal, Lee2020DRIT} and group observations\cite{Bouchacourt2018Multi}).
%Although some methods have been proposed to 
%\rev{Even though there exist content features (e.g., brain anatomical structure) that can be learned from images of all sites, most unsupervised deep learning based methods cannot make fully use of the entire training data.}
%To address this issue, disentangled representation learning (DRL) can encode the semantics (e.g., brain contrast) of all images within each site into a site-specific style representation and take the remaining property as the site-invariant content representation (e.g. brain anatomical structure).
%DRL can be supervised\cite{Dewey2020A}, given the style and content features for each training image as labels, or unsupervised, taking a set of images as input without any other information to learn a complete representation disentanglement via some property constraints (e.g., adversarial constraints\cite{Mathieu2016Disentangling,Denton2017Unsupervised}, cycle consistency\cite{Jha2018Disentangling, Huang2018Multimodal, Lee2020DRIT} and group observations\cite{Bouchacourt2018Multi}, etc).
%Compared to supervised DRL, unsupervised DRL is more flexible, efficient and effective due to no labor-intensive processing.
%However, most unsupervised DRL methods only can learn one-to-one mappings between two sites and typically do not produce consistently good results.
%In this context, learning fully-unsupervised representation disentanglement on all sites to construct a unified framework is highly ambitious and is work in progress.




%\todo{
%	1. What does `devoid of variability of parameters' distributions' mean?
%	2. Double check whether RAVEL belongs to the correct category.
%	3. Description of NeuroHarmony not clear.
%	4. `Network predictions' of what?
%}

\section*{Results}
\begin{figure*}[!t]
	\centering
	\includegraphics[width=.94\textwidth]{gen_ref.pdf}
%	\setlength{\abovecaptionskip}{0pt}
%	\setlength{\belowcaptionskip}{-10pt}
	\caption{\textbf{Harmonization of T1- and T2-weighted Images.}
		Site-specific harmonization of \textbf{a}, T1-weighted images and \textbf{b}, T2-weighted images.
%		The original images are shown in the first column. 
		Reference-specific harmonization of \textbf{c}, T1-weighted images and \textbf{d}, T2-weighted images.
		The original images are shown in the first column
%		 and the reference images are shown in the first row.	 
		and the harmonized images are shown for GE, Philips and Siemens respectively in the second to fourth columns.
		MURD is remarkably effective in harmonizing contrasts and preserving details.\label{fig:gen_ref}}
\end{figure*}
%\begin{figure*}[!t]
%	\centering
%	\includegraphics[width=.94\textwidth]{gen.pdf}
%	%	\setlength{\abovecaptionskip}{0pt}
%	%	\setlength{\belowcaptionskip}{-10pt}
%	\caption{\textbf{Site-specific harmonization of T1- and T2-weighted Images.}
%		Site-specific harmonization of \textbf{a}, T1-weighted images and \textbf{b}, T2-weighted images.
%		The original images are shown in the first column
%		and the harmonized images are shown for GE, Philips and Siemens respectively in the second to fourth columns.
%		MURD is remarkably effective in harmonizing contrasts and preserving details.\label{fig:gen}}
%\end{figure*}
%\begin{figure*}[!t]
%	\centering
%	\includegraphics[width=.94\textwidth]{ref.pdf}
%	%	\setlength{\abovecaptionskip}{0pt}
%	%	\setlength{\belowcaptionskip}{-10pt}
%	\caption{\textbf{Reference-specific harmonization of T1- and T2-weighted Images.}
%		Reference-specific harmonization of \textbf{c}, T1-weighted images and \textbf{d}, T2-weighted images.
%		The original images are shown in the first column
%		and the harmonized images are shown for GE, Philips and Siemens respectively in the second to fourth columns.
%		MURD is remarkably effective in harmonizing contrasts of reference images and preserving details.\label{fig:ref}}
%\end{figure*}
%\todo{Could we rearrange b so that the original images are in the first column? This will make a and b more consistent.}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.95\textwidth]{comp_quan.pdf}
\caption{\textbf{Numerical Evaluation of Harmonization Outcomes.}
	Quantitative evaluation conducted for the GE, Philips, and Siemens sites using FID and KID as metrics for \textbf{a}, T1-weighted images ($n=600$ slices per site) and \textbf{b}, T2-weighted images ($n=600$ slices per site) from the validation dataset, and \textbf{c}, T1-weighted images ($n=60000$ slices per site) and \textbf{d}, T2-weighted images ($n=60000$ slices per site) from the generalizability dataset. 
	Bullseyes show the means and error bars show the standard errors on the means.
%	The sample sizes of GE, Philips, Siemens images from small validation dataset, are 600, 600, 600, respectively, and these from large generalizability, respectively, are 60000, 60000 and 60000. 
	MURD, both site-specific (SS) and reference-specific (RS), yields lower FID and KID values that are closer to the reference values than DRIT++\cite{Lee2020DRIT} and StarGAN-v2\cite{Choi2020StarGANv2}. The FID and KID values for the generalizability dataset are largely consistent with those of the validation dataset.
%	Note that the FID and KID values of unharmonized images are typically over 300.
\label{fig:comp_quan}
}
\end{figure*}
%\todo{What are the typical FID and KID values before harmonization?}\rev{Over 300. It's too large, so I didn't present them in the figure.}
%\todo{Instead of `SG' and `RG', just use `Site' and `Image' in the legend.}

\begin{figure*}[!t]
\centering
\includegraphics[width=.95\textwidth]{ol_comp_quan.pdf}
\caption{
	\textbf{Validation via the Traveling Human Phantom Dataset.} 
	Evaluation of harmonized \textbf{a}, T1-weighted images and \textbf{b}, T2-weighted images from the traveling human phantom dataset for the following harmonization tasks: GE to Philips (GP, $n=60$ slices), Philips to GE (PG, $n=60$ slices), GE to Siemens (GS, $n=600$ slices), Siemens to GE (SG, $n=600$ slices), Philips to Siemens (PS, $n=120$ slices), and Siemens to Philips (SP, $n=120$ slices). 
	Segmentation consistency of \textbf{c}, T1-weighted images and \textbf{d}, T2-weighted images from the traveling human phantom dataset with and without harmonization: GP ($n=1$ volume), PG ($n=1$ volume), GS ($n=5$ volumes), SG ($n=5$ volumes), PS ($n=2$ volumes), and SP ($n=2$ volumes).
	The lines show the means and the shaded regions show the standard errors on the means. 
%	For traveling human phantoms dataset, the sample sizes of GP, PG, GS, SG, PS and SP are 60, 60, 120, 120, 120, 120, respectively. 
	MURD yields the best performance in terms of MAE, MS-SSIM, and PSNR. The improvement in DSCs indicates that MURD harmonization significantly increases consistency of tissue segmentation.
	\label{fig:ol_comp_quan}
}
\end{figure*}

%\begin{figure*}[!t]
%	\centering
%	\includegraphics[width=.9\textwidth]{seg.pdf}
%	\caption{
%		\textbf{Harmonization and Segmentation Consistency.} 
%		Segmentation consistency of \textbf{a}, T1-weighted images and \textbf{b}, T2-weighted images from the traveling human phantoms dataset with and without harmonization: GE to Philips (GP, $n=1$ volume), Philips to GE (PG, $n=1$ volume), GE to Siemens (GS, $n=5$ volumes), Siemens to GE (SG, $n=5$ volumes), Philips to Siemens (PS, $n=2$ volumes), and Siemens to Philips (SP, $n=2$ volumes). The improvement in DSCs indicates that MURD harmonization significantly increases consistency of tissue segmentation. The lines show the means and the shaded regions show the standard errors on the means.}
%	\label{fig:seg}
%\end{figure*}
%\todo{Specify the unit of sample size.}
%\todo{Sample size only 5 slices per case?}

\begin{figure*}[!t]
	\centering
	\includegraphics[width=.99\textwidth]{continuous.pdf}
	\caption{
		\textbf{Continuous and Incremental Harmonization.} 
		Continuous harmonization of \textbf{a}, T1-weighted images and \textbf{b}, T2-weighted images between two sites, demonstrating that 
		MURD smoothly changes image contrasts without altering anatomical details thanks to its ability to disentangle content and style information.\label{fig:continuous}}
\end{figure*}

\subsection{Materials}
We demonstrate the effectiveness of MURD on brain T1- and T2-weighted images of children 9 to 10 years of age
acquired through the Adolescent Brain Cognitive Development (ABCD) study\cite{Jernigan2018The} using scanners from different vendors, including General Electric (GE), Philips, and Siemens. Informed consents were obtained from all participants\cite{Jernigan2018The}.
Although the imaging protocols were matched across scanners at the different imaging sites\cite{Casey2018ABCD}, inter-scanner variability in appearance is still significant (Figure~\ref{fig:gen_ref}).
%
For simplicity, we group the images and treat them as coming from three \emph{virtual} sites---the GE, Philips, and Siemens sites.
%
We trained and tested MURD separately for T1-weighted images and T2-weighted images. For each modality, we trained MURD using a modest sample size of 20 images 
%(10-year-olds) 
 per vendor. Three datasets per modality were used for testing different aspects of MURD:
\vspace{-\baselineskip}
%\begin{inparaenum}[(i)]
\begin{enumerate}[itemsep=0pt,topsep=0pt,partopsep=0pt,parsep=0pt,label=(\roman{enumi})]
	\item \textbf{Validation Dataset} -- A small but diverse dataset of 10 images 
%	(10-year-olds) 
	per vendor, carefully selected to be structurally different from the images in the training dataset. The purpose of this testing dataset is to test the effectiveness of MURD beyond the training dataset.
	\item \textbf{Generalizability Dataset} -- A large dataset of 1000 images 
%	(9- to 10-year-olds) 
	randomly selected for each vendor. This testing dataset is allowed to be deliberately much larger than the training dataset to test the generalizability of MURD.
%	 both within and across age.
	\item \textbf{Human Phantom Dataset} -- A traveling human phantom dataset of 1 subject scanned on both GE and Philips scanners, 5 subjects scanned on both GE and Siemens scanners, and 2 subjects scanned on both Philips and Siemens scanners. This testing dataset allows numerical evaluation to ensure that anatomical structures are preserved after harmonization. Images for each subject were aligned using Advanced Normalization Tools (ANTS)\cite{Avants2009Advanced}.
\end{enumerate}
\vspace{-\baselineskip}
%\end{inparaenum}
%We denote the SA, LA and THP datasets for T1- and T2-weighted MR images as SA\_T1, LA\_T1, THP\_T1, SA\_T2, LA\_T2, and THP\_T2, respectively.
Note that the training dataset and the three testing datasets are mutually exclusive with no overlapping samples. 
Considering both modalities and all three vendors, more than 6000 images were used for evaluation. 60 central axial slices were extracted from each image volume for training and testing.
%We extracted 60 axial slices from each image volume, resulting in a total of 1200, 1200 and 1200 axial slices respectively from the 20 GE, 20 Philips and 20 Siemens T1-/T2-weighted training scans.
%For T1- and T2-weighted images from small validation dataset, 600, 600 and 600 axial slices, respectively, were extracted from the 10 GE, 10 Philips and 10 Siemens scans.
%For T1- and T2-weighted images from large generalizability dataset, 60000, 60000 and 60000 axial slices, respectively, were extracted from the 1000 GE, 1000 Philips and 1000 Siemens scans.
%For T1- and T2-weighted images from traveling human phantoms dataset, to eliminate the longitudinal effects, Advanced Normalization Tools (ANTS)\cite{Avants2009Advanced} was first utilized for rigid and non-rigid registration. 
%Then, 60 and 60 axial slices, respectively, were extracted from GE and Philips scans of 1 subject, 120 and 120 axial slices, respectively, were extracted from GE and Siemens scans of 2 subjects, 120 and 120 axial slices were extracted from Philips and Siemens scans of 2 subjects.

\subsection{MURD Harmonizes Contrasts and Preserves Details}
We demonstrate the effectiveness of MURD harmonization on
%the site-guided and reference-guided 
%harmonization using MURD by 
T1- and T2-weighted images (Figure~\ref{fig:gen_ref}). Note that MURD allows harmonization with respect to either a site
%\footnote[1]{We use the word `site' here for simplicity. The word can refer to imaging protocols or scanners.} 
or a reference image. The former amounts to harmonization with respect to an image randomly drawn from a site image distribution. 
MURD is remarkably effective in adapting contrast and preserving details when harmonizing images from a source site with a target site (Figures~\ref{fig:gen_ref}a and b).
%,  harmonizes images from the original site to the target sites with great appearance-changing and content-preservation performance, where the anatomical content details of original-site images are highly preserved and the appearance styles are remarkably harmonized to these of target-site images. 
%Even for the self-harmonization of original-site images, which harmonizes images to themselves, the content and style details keep highly consistent.
When the source and target sites are identical, MURD retain both contrast and details.
When given a reference image, MURD harmonizes the contrasts of images from a source site with the reference image but preserves the anatomical details of the original images (Figures~\ref{fig:gen_ref}c and d).

%that our method MURD harmonizes original-site images to the target sites with the appearance styles given by reference images, where the harmonized images preserve the anatomical content details of original images and present the appearance styles of the reference images.

\subsection{MURD Outperforms State-of-the-Art Methods}
To further demonstrate the effectiveness and superiority of MURD, we compared it with two closely-related state-of-the-art unsupervised methods, i.e., DRIT++\cite{Lee2020DRIT} and StarGAN-v2\cite{Choi2020StarGANv2}, which are designed respectively for dual-domain and multi-domain image-to-image translation.
DRIT++ and StarGAN-v2 were implemented and trained as described in the original papers\cite{Lee2020DRIT,Choi2020StarGANv2}.
DRIT++ was trained once for every pair of sites. StarGAN-v2 was trained concurrently for multiple sites, similar to MURD.
We quantitatively compared the visual quality of the harmonized images using two metrics, i.e., the Frech\'{e}t inception distance (FID)\cite{Heusel2017Gans} and the kernel inception distance (KID)\cite{Binkowski2018Demystifying}, which reflect distribution discrepancy between two sets of images
%\todo{Maybe include a sentence on why both FID and KID are needed here.} 
%\todo{Maybe briefly justify why DRIT++ StarGAN-v2 can be used for harmonization.}
%Compared to indirect quality-related metrics (e.g., likelihood)\cite{Borji2019Pros}, these metrics evaluate the discrepancy between two sets of images 
in a manner that correlates well with human judgment\cite{salimans2016improved}. FID and KID are respectively computed based on the Frech\'{e}t distance and maximum mean discrepancy (MMD) of features from the last average pooling layer of the Inception-V3\cite{Szegedy2016Rethinking} trained on the ImageNet\cite{deng2009imagenet}. 
%\todo{Is this description for FID or KID? What does this mean conceptually?} 
FID and KID were computed at the slice level for the harmonized images with respect to the training images of the target sites.
% and means and standard errors were calculated across the harmonized images.
For site-specific harmonization, 10 target-site harmonized images were generated for each testing image of the source site with 10 randomly generated style codes.
%, and then the means and standard errors of FID and KID values between the harmonized images of all testing images and target-site training images were calculated.
For reference-specific harmonization, each image in the source site was harmonized with respect to 10 reference images randomly selected from the testing images of a target site.
%
%We can see from the quantitative comparison results of harmonized T1- and T2-weighted images from HP dataset
%<<<<<<< Updated upstream
%The results (Figures~\ref{fig:comp_quan}a and b) indicate that MURD outperforms DRIT++ and StarGAN-v2 both in terms of FID and KID.
%The FID and KID values of the reference-guided MURD are much closer to the real FID values than these of the other methods, 
%The FID values between training and testing images from the same site are provided as references (denoted as ``Real"). 
%From the quantitative results of harmonization of MURD on generalizability dataset (Figures~\ref{fig:comp_quan}c and d), we can observe that most FID and KID values are close to real values and keep highly consistent with these on the validation dataset. 
%=======
The results (Figures~\ref{fig:comp_quan}a and b) indicate that MURD outperforms DRIT++ and StarGAN-v2 for both FID and KID. For comparison, reference FID and KID values were computed between the training and testing images from the same site (denoted as ``Reference"). 
The FID and KID values given by MURD are significantly closer to the reference values than DRIT++ and StarGAN-v2. 
MURD harmonization of the generalizability dataset (Figures~\ref{fig:comp_quan}c and d) yields FID and KID values that are highly consistent with the validation dataset and  close to the reference values. This indicates that, although trained using a modest dataset, the model is generalizable to a much larger dataset.
%>>>>>>> Stashed changes
%\todo{For site-specific, the harmonized images are `random'. How many instances were generated? How are the FID and KID values computed for these instances?}

%\todo{Any numerical results to show that details are not lost after harmonization?}\rev{FID and KID can reflects the structure and appearance changes. We also use SSIM, MS-SSIM on the human phantoms dataset, which can indicate the better details preservation.}

\subsection{MURD Efficacy Validated via Traveling Human Phantom Data}
The human phantom dataset allows direct quantitative evaluation of the effects of harmonization on consistency of structure and appearance.
%To demonstrate the better harmonization quality of MURD, we also compared it with DRIT++ and StarGAN-v2 on T1- and T2-weighted images from human phantoms dataset.
Based on multiple metrics, including mean absolute error (MAE), multi-scale structural similarity index (MS-SSIM), and peak signal-to-noise ratio (PSNR), MURD significantly outperforms DRIT++ and StarGAN-v2, indicating better harmonization of contrast and preservation of anatomical details (Figures~\ref{fig:ol_comp_quan}a and b). 
%The results (Figure~\ref{fig:ol_comp_quan}) using various evaluation metrics on human phantoms dataset, we observe that both site-guided and reference-guided MURD outperforms the other methods significantly on various evaluation metrics, especially reference-guided MURD achieves the best performance on all evaluation metrics, indicating its better harmonization quality.

\subsection{MURD Improves Tissue Segmentation Consistency}
Segmentation of brain tissues is sensitive to variation in image contrast and under- or over-segmentation might happen owing to differences in acquisition protocols.
%To further demonstrate the usability of the images harmonized by site-guided MURD,  
We applied Brain Extraction Tool (BET)\cite{Smith2002Fast} and FMRIB's Automated Segmentation Tool (FAST)\cite{Zhang2001Segmentation} on T1- and T2-weighted images in the human phantom dataset for brain extraction and tissue segmentation.
Tissue segmentation consistency before and after harmonization was measured using the Dice similarity coefficient (DSC) with the tissue segmentation maps from the target site as references. The results (Figures~\ref{fig:ol_comp_quan}c and d) indicate that DSCs are improved remarkably by harmonization using MURD.

\subsection{MURD Supports Continuous and Incremental Harmonization}
We further demonstrate that MURD completely disentangles content and style information by visualization via continuous and incremental harmonization.
We generated images with between-site appearances to aid visual inspection of how appearance changes gradually between sites and whether unwanted anatomical alterations are introduced in the process.
%The images were generated via linear interpolation of the style features of two sites.
%with respect to coefficient $\beta$ on site-specific style features generated by style generator for producing inter-site and intra-site style features.
Intermediate style features are calculated based on the style features of Site A, $s_\text{A}$, and Site B, $s_\text{B}$, via $(1-\beta)s_\text{A}+\beta s_\text{B}$, where $\beta\in [0,1]$.
%For intra-site style generation, considering that style generator produces central style feature in latent space, we applied bidirectional feature interpolation which allows $\beta$ taking values between -0.5 to 0.5.
The intermediate style features and the content features of an image are then used to generate an intermediate image. 
The results (Figures~\ref{fig:continuous}) indicate that MURD gradually and smoothly changes image appearance without altering anatomical details.
%Within each site, MURD produces harmonized images with continuously varied appearance without details loss.
%The reasonable results of continuous harmonization demonstrate the effectiveness of our proposed content-style representation disentanglement for multi-site harmonization.

\subsection{Discussion}
We have demonstrated that MURD is remarkably effective in harmonizing MR images by removing non-biological site differences and at the same time preserving anatomical details. MURD network training involves only site-labeled images and requires no traveling human phantom data. This flexibility allows data from existing large-scale studies to be harmonized retrospectively without requiring additional data to be collected.
%MURD is therefore useful for longitudinal and large-scale studies which require consistent MR imaging for analysis or diagnosis.
%MURD is a flexible method for harmonization irrespective of scanners or protocols. 
%For training, the user only needs to label the images as site $i$ ($i\in[1,N]$). 
%No human traveling phantom is need to be taken.

We have shown that MURD yields superior performance over DRIT++\cite{Lee2020DRIT} and StarGAN-v2\cite{Choi2020StarGANv2}.
For every pair of sites, DRIT++ embeds images in a site-invariant content space capturing information shared across sites and a site-specific style space. The encoded content features extracted from an image of one site are combined with style features from another site to synthesize the corresponding harmonized image. Learning is unsupervised and hence paired data is not required. However, DRIT++ is not scalable due to the need to learn all mappings for all site pairs.
DRIT++ is also ineffective because it cannot fully utilize the entire training data and can only learn from two sites at each time, causing it to miss global features that can be learned from images of all sites. 
Failure to fully utilize training data likely limits the quality of generated images. 
%
Unlike DRIT++, StarGAN-v2\cite{Choi2020StarGANv2} is scalable and performs image-to-image translations for multiple sites using only a single model. It has been applied to the problem of MRI harmonization\cite{Liu2021Style} with promising results.
%\todo{Did they compare with some baseline methods?}\rev{No comparison with other methods.}
In addition to greater scalability, StarGAN-v2 generates images of higher visual quality owing to its ability to jointly consider the information offered by images from all sites.  
%the generalization capability behind the multi-task learning setting.
StarGAN-v2, however, does not explicitly disentangle images into structural and appearance information. This introduces the possibility of altering anatomical details during harmonization via style transfer.
%\rev{, such that the anatomical details might inevitably be changed during style transferring.}
%\todo{What is the advantage of disentanglement as done in MURD?}
In contrast,  MURD enforces explicit disentanglement of  content and style features by jointly considering images from all sites, allowing it to produce harmonized images with diverse appearances with significantly better preservation of anatomical details (see Supplementary Figures~1--3).
Disentanglement safeguards harmonization against altering image anatomical contents and allows gradual and controllable harmonization via interpolation of style features. 
%This disentanglement releases deep learning based harmonization constraints that generating harmonized images using all features by only transferring style features to achieve high-quality harmonization performance.
%It also allows an interpretable explicit control of inter-site continuous harmonization using linearly interpolated style features.
%\todo{Conceptually, how does the representation disentanglement improve harmonization results?}

%Recently, unsupervised multi-site MRI harmonization has been attempted via CALAMITI\cite{Zuo2021Information} and the direct application of StarGAN-v2\cite{Liu2021Style}.

%\begin{inparaenum}[1)]
%\item \textit{Harmonization specification:} 
The harmonization target is specified for DRIT++ and StarGAN-v2 by a reference image. The appearance of the harmonized image is determined by the style features extracted from the reference image. 
In addition to a reference image, the harmonization target for MURD can be specified by a site label, which determines the output branch of the style generator and the style encoder. A latent code sampled from the standard Gaussian distribution determines an appearance specific to the site. 
%The latent code is continuous and allows smooth intra-site appearance variation, which provides a controllable style adjustment for subsequent human observation, diagnosis, and automatic analysis.

%\todo{What si the importance being able to generate intra-site variation in contrast?}

%In the reference-guided setting, our method MURD is performed in the same ways as above two methods, which utilize a style encoder to extract style features for subsequent harmonization using generator or decoder.

%\item \textit{Learning algorithm:} 
A recent MRI harmonization method, called CALAMITI\cite{Zuo2021Information}, relies on intra-site supervised image-to-image translation and unsupervised domain adaptation for multi-site harmonization. 
%Intra-site T1- and T2-weighted images of the same subjects are used to train a disentanglement network, which is then adapted to a new site to extract the content features to be combined \rev{[Please complete and ensure description is concise and correct.]}. 
This requires training a disentangled representation model with intra-site multi-contrast images (T1- and T2-weighted images) of the same subjects and retraining the model for a new site via domain adaptation\cite{Varsavsky2020Thomas}.
%
%The learning algorithm of our method MURD differs from that of CALAMITI, but same as StarGAN-v2. 
Unlike CALAMITI,  MURD requires only images from a single contrast and can learn multi-site harmonization simultaneously without needing fine-tuning or retraining. 
%\end{inparaenum}
%Domain adaptation typically involves optimizing a large number of parameters. In contrast, MURD can be expanded efficiently for additional sites by incrementally training add-on components to the generator (see Methods for more details).

%For unseen-site images, our method MURD can adopt incremental learning with only a small-amount parameter optimization of newly added output branches, 
 

%adding specific output branches to style generator, style encoder and discriminator and then training them after freezing trained model.
%We also note that MURD can be used to unsupervised domain adaptation due to the extracted site-invariant content features to improve machine learning algorithms for specific cross-site tasks such as diagnosis, quality control and segmentation.
%\rev{MURD can also harmonize across resolutions, allowing lower-resolution images from one site to be ``super-resolved'' when harmonized with respect to higher-resolution images from another site (Supplementary Figure~9). For example, lower-resolution and higher-resolution images scanned from GE, Philips and Siemens scanners can be considered as six individual sites with respect to different appearances and then MURD can learn harmonization mappings among all sites simultaneously.}\todo{Depending on the final results, this paragraph might need to be removed.}
%\todo{More detail needed here.}


%\todo{Discuss ``Information-based Disentangled Representation Learning for Unsupervised MR Harmonization'' and ``Style Transfer Using Generative Adversarial Networks for Multi-Site MRI Harmonization''.}

\begin{methods}
We consider the multi-site harmonization problem as image-to-image translation among multiple sites and propose an end-to-end multi-site unsupervised representation disentangler (MURD, see Figure~\ref{fig:framework}) to learn content-style disentangled cycle translation mappings that translate images forward and backward between any two sites. We employ two encoders to respectively embed each image in a site-invariant content space, which captures anatomical information, and a site-specific style space, which captures appearance information, and a generator to produce a harmonized image using the encoded content features and site-specific style features.

%\item adopt two encoders to embed images into a site-invariant content space, which contains anatomical information, and a site-specific style space, which captures appearance information, and adopt a generator to create harmonized/reconstructed images using the encoded content and specific style features;
%\item realize content-style disentanglement, hinging on determining the site-invariant content space using two strategies: content and style consistency losses to keep content and style features consistent across sites respectively and an alignment mechanism to ensure the site-invariance of content space. 

%\begin{inparaenum}[(i)]
%	\item consider the multi-site harmonization problem as image-to-image translation among multiple sites;
%	\item propose an end-to-end multi-site unsupervised representation disentangler (MURD, see Figure~\ref{fig:framework}) to learn content-style disentangled cycle translation mappings that translate images forward and backward between any two of all sites;
%	\item adopt two encoders to embed images into a site-invariant content space, which contains anatomical information, and a site-specific style space, which captures appearance information, and adopt a generator to create harmonized/reconstructed images using the encoded content and specific style features;
%	\item realize content-style disentanglement, hinging on determining the site-invariant content space using two strategies: content and style consistency losses to keep content and style features consistent across sites respectively and an alignment mechanism to ensure the site-invariance of content space. 
%\end{inparaenum}

%\begin{figure*}[!t]
%	\centering
%	\includegraphics[width=.99\textwidth]{overall.pdf}
%%	\vspace{-10pt}
%	\caption{\textbf{Overview of MURD.}
%		\textbf{a}, Multi-site representation disentanglement. Given image sets from $N$ sites $\{\mathcal{X}_i\}_{i=1}^N$, MURD disentangles each image set $\mathcal{X}_i$ in a site-invariant content space $\mathcal{C}$ and a site-specific style space $\mathcal{S}_i$, where the content features are aligned in the content space $\mathcal{C}$. This is achieved using content-style disentangled cycle translation (CS-DCT).
%		\textbf{b}, Site-specific CS-DCT. 
%		An image $x_i$ from the image set of the $i$-site $\mathcal{X}_i$ is encoded in site-invariant content space $\mathcal{C}$ and in site-specific style space $\mathcal{S}_i$ to obtain content features $c_i$ and style features $s_i$, respectively.
%		Style features $s_j$ in style space $\mathcal{S}_j$ for site $j\neq i$ are generated using style generator $G^{\text{S}}_j$.
%		Content features $c_i$ and style features $s_j$ are decoded to generate harmonized image $\tilde{x}_j$, which is in turn encoded in the content space $\mathcal{C}$ to obtain content features $\tilde{c}_j$.
%		Content features $\tilde{c}_j$ and style features $s_i$ are decoded to reconstruct image $\hat{x}_i$.
%		\textbf{c}, Reference-specific CS-DCT. Unlike site-specific CS-DCT in \textbf{b}, style features $s_j$ are obtained by encoding a reference image $x_j$ in style space $\mathcal{S}_j$.
%		\textbf{d}, The MURD framework. MURD takes an image $x_i$ from $\mathcal{X}_i$ as input and utilizes content encoder $E^{\text{C}}$ and style encoder $E^{\text{S}}_i$ to extract content features $c_i$ and style features $s_i$, respectively.
%		Content features $c_i$ together with a specific-site style $s_j$, which is either generated from style generator $G^{\text{S}}_j$ or encoded from the reference image $x_j$, are fed to generator $G$ to generate harmonized image $\tilde{x}_j$, which discriminator $D_j$ then classifies as being either real or fake.
%		Content features $\tilde{c}_j$ and style features $\tilde{s}_j$ of the harmonized image $\tilde{x}_j$ are encoded via  content encoder $E^{\text{C}}$ and style encoder $E^{\text{S}}_j$, respectively, and are respectively pixel-wise consistent with $c_j$ and $s_j$.
%		$\tilde{c}_j$ and $s_i$ are fed to $G$ to reconstruct image $\hat{x}_i$, which is pixel-wise and gradient-wise consistent with $x_i$.
%		$c_i$ and $s_i$ are fed to $G$ to generate the identity image $\bar{x}_i$ of $x_i$.  
%%		\textbf{e}, Network architecture of generator $G$.
%%		\textbf{f}, Network architecture of content encoder $E^{\text{C}}$.
%%		\textbf{g}, Network architecture of style encoder $E^{\text{S}}_i$.
%%		\textbf{h}, Network architecture of style generator $G^{\text{S}}_j$.
%%		\textbf{i}, Network architecture of discriminator $D_j$.
%		\label{fig:framework}
%	}
%\end{figure*}

\subsection{Content-Style Disentangled Cycle Translation (CS-DCT)}
Given image sets $\{\mathcal{X}_i\}_{i=1}^N$ from $N$ distinct sites, MURD utilizes content-style disentangled cycle translation (CS-DCT) to disentangle each image set $\mathcal{X}_i$ in a site-invariant content space $\mathcal{C}$ and a site-specific style space $\mathcal{S}_i$ (Figure~\ref{fig:framework}a).
CS-DCT, realized with sequential forward and backward translation, can be site-specific (Figure~\ref{fig:framework}b) or reference-specific (Figure~\ref{fig:framework}c).
MURD jointly learns site-invariant content features $c_i\in\mathcal{C}$ and site-specific style features $s_i\in\mathcal{S}_i$ from image $x_i\in\mathcal{X}_i$, and utilize generator $G$ to construct the harmonized image $\tilde{x}_{j}\in\mathcal{X}_{j}$ ($j\neq i$) using content features $c_i$ and style features $s_{j}\in\mathcal{S}_j$ in forward translation.
%, where $p(\cdot)$ is a permutation operator.
Style features $s_j$ are generated by a style generator $G^{\text{S}}_j$ or extracted from a reference image $x_j\in\mathcal{X}_j$. In backward translation, MURD extracts the content features $\tilde{c}_{j}$ from the harmonized image $\tilde{x}_{j}$, which are in turn fed with style features $s_i$ to generator $G$ to reconstruct image $\hat{x}_i$, which is required to be consistent with the input image $x_i$.

\subsection{Multi-site Unsupervised Representation Disentangler (MURD)}
MURD implements CS-DCT in a end-to-end manner via five modules (Figure~\ref{fig:framework}d): 
%\begin{inparaenum}[(i)]
\begin{enumerate}[itemsep=0pt,topsep=0pt,partopsep=0pt,parsep=0pt,label=(\roman{enumi})]
\item A content encoder $E^{\text{C}}$, shared for all sites, to extract content features from an image in a site-invariant space $\mathcal{C}$;
\item A style encoder $E^{\text{S}}_i$ for each site $i$ to extract style features from an image in a site-specific style space $\mathcal{S}_i$;
\item A generator $G$, shared for all sites, to synthesize images using content and style features;
\item A style generator $G^{\text{S}}_i$ for each site $i$ to yield style features $s_i$ that reflect the appearance style of images from the site;
and 
\item A discriminator $D_i$ for each site $i$ to distinguish between real and generated images.
\end{enumerate}
%\end{inparaenum}
Specifically, given an input image $x_i$ from image set $\mathcal{X}_i$, content encoder $E^{\text{C}}$ and style encoder $E^{\text{S}}_i$, respectively, extract content features $c_i$ and style features $s_i$.
For a random site $j\neq i$, style generator $G^{\text{S}}_j$ takes a latent code $z$ randomly sampled from a standard normal distribution $\mathcal{N}(0,1)$ as input to create a $j$-site style features $s_j$.
With content features $c_i$ and style features $s_j$, generator $G$ constructs harmonized image $\tilde{x}_j$, which the discriminator $D_j$ then classifies as being either real or fake using an adversarial loss $\mathcal{L}_\textrm{adv}$:
\begin{equation}
\mathcal{L}_{_\textrm{adv}}=\mathbb{E}[\log D_i(x_i)]+\mathbb{E}[\log(1-D_j(\tilde{x}_j))].
\end{equation}
%Since the content encoder $E^{\text{C}}$ encodes images in a site-invariant content space, 
Content encoder $E^{\text{C}}$ and style encoder $E^{\text{S}}_j$ are used to extract content features $\tilde{c}_j$ and style features $\tilde{s}_j$ from $\tilde{x}_j$.
%Due to the site-specific property of style features, 
%The $j$-th output of style encoder $E^{\text{S}}_j$ is employed to extract style feature $\tilde{s}_j$ from $\tilde{x}_j$.
%So far, no other content feature is added in addition to the content feature of $x_i$, 
The consistency between $c_i$ and $\tilde{c}_j$ is enforced by a pixel-wise content consistency loss $\mathcal{L}_\text{cont}$:
\begin{equation}
\mathcal{L}_\text{cont}=\|c_i-\tilde{c}_j\|_1.
\end{equation}
The consistency between $s_j$ and $\tilde{s}_j$ is enforced by a pixel-wise style consistency loss $\mathcal{L}_\text{sty}$:
\begin{equation}
	\mathcal{L}_\text{sty}=\|s_i-\tilde{s}_j\|_1.
\end{equation}
Content site-invariance is enforced by a content alignment loss $\mathcal{L}_\text{ca}$:
\begin{equation}
\mathcal{L}_\text{ca}=\text{KL}(\mathcal{N}(c_i,I)\|\mathcal{N}(0,I)),
\end{equation}
where $\text{KL}(\cdot\|\cdot)$ is the Kullback–Leibler divergence and $I$ is an identity matrix.
Content features $c_i$ are randomly perturbed during the feed-forward step:
\begin{equation}
c_i=E^{\text{C}}(x_i)+\eta,\quad \eta\sim\mathcal{N}(0,I).
\end{equation}
%<<<<<<< Updated upstream
%When the content and style features are completely disentangled, the style feature $s_j$ is consistent with style feature $\tilde{s}_j$. This consistency is constrained by a style consistency loss $\mathcal{L}_\text{sty}$ at the pixel level:
%\begin{equation}
%\mathcal{L}_\text{sty}=\|s_i-\tilde{s}_j\|_1.
%\end{equation}
%=======
%>>>>>>> Stashed changes
The reconstructed image $\hat{x}_i$ of $x_i$ is produced by generator $G$ using content features $\tilde{c}_j$ and style features $s_i$. The consistency between $x_i$ and $\hat{x}_i$ is ensured by a pixel-wise and gradient-wise cycle consistency loss $\mathcal{L}_\text{cyc}$:
\begin{equation}
\mathcal{L}_\text{cyc}=\|x_i-\hat{x}_i\|_1+\lambda_\text{g}\|g(x_i)-g(\hat{x}_i)\|_1,
\end{equation}
where $g(\cdot)$ is the image gradient function and $\lambda_\text{g}$ is the loss weight for the gradient loss term.
Furthermore, an identity image $\bar{x}_i$ can also be constructed by generator $G$ using content features $c_i$ and style features $s_i$, which are identical to $x_i$ when $c_i$ and $s_i$ are completely disentangled.
We devise an identity loss to measure the pixel-wise difference between $x_i$ and $\bar{x}_i$ as
\begin{equation}
\mathcal{L}_\text{id}=\|x_i-\bar{x}_i\|_1.
\end{equation}
All modules are optimized with total loss function $\mathcal{L}$:
\begin{equation}\label{eq:TotalLoss}
\min_{E^{\text{C}},E^{\text{S}}_i,G,G^{\text{S}}_i}\max_{D_i} \mathcal{L}_\text{adv}+\lambda_\text{cont}\mathcal{L}_\text{cont}+\lambda_{ca}\mathcal{L}_\text{ca}+\lambda_\text{sty}\mathcal{L}_\text{sty}+\lambda_\text{cyc}\mathcal{L}_\text{cyc}+\lambda_\text{id}\mathcal{L}_\text{id},\quad i\in[1,N],
\end{equation}
where $\lambda_\text{cont}$, $\lambda_\text{ca}$, $\lambda_\text{sty}$, $\lambda_\text{cyc}$, and $\lambda_\text{id}$ are loss weights used for controlling the contributions of the respectively terms in the loss function.
Training ends when all modules are optimized, such that the optimized discriminator classifies the harmonized images into one of two categories with equal probability.
See Supplementary Figures~4--7 for the effects of the individual loss functions.

During inference, content features $c_i$ of input image $x_i$ are extracted using content encoder $E^{\text{C}}$, and style features $s_j$ are either generated using style generator $G^{\text{S}}_j$ or extracted from a reference image $x_j$. Harmonized image $\tilde{x}_j$ is created by generator $G$ using $c_i$ and $s_j$.

\subsection{Network Architecture}
The architectures of the components of MURD are 
%illustrated in Figures~\ref{fig:framework}e-i and are 
described next.
%\begin{inparaenum}[(i)]
%	\item a site-shared content encoder $E^{\text{C}}$, that embeds an input image to the site-invariant latent space $\mathcal{C}$ to extract the content feature;
%	\item a site-specific style encoder $E^{\text{S}}_i$, that encodes an input image to the site-specific latent space $\mathcal{S}_i$ for site $i$ to extract the $i$-site style feature;
%	\item a site-shared generator $G$, that generates harmonized/reconstructed images using specific content and style features;
%	\item a site-specific style generator $G^{\text{S}}_j$, that embeds a latent code sampled from standard normal distribution to the site-specific latent space $\mathcal{S}_j$ for site $j$ to generate $j$-site style feature $s_j$;
%	and \item a site-specific discriminator $D_j$, that learns to distinguish real or harmonized  images.
%\end{inparaenum}
%We describe next the architectural details for each network module of MURD. 

\textbf{Content Encoder} Content encoder $E^{\text{C}}$ is shared among all sites and extracts the content features $c_i$ of an input image $x_i$ through three convolutional blocks and 4 residual blocks.
Each convolutional block is composed of three sequential layers, i.e., convolution, instance normalization (IN)\cite{Ulyanov2017Improved}, and leaky ReLU (LReLU) activation.
Each residual block consists of a nonlinear mapping branch and a shortcut branch. The nonlinear mapping branch is constructed by a series of layers, i.e., convolution, IN, LReLU, convolution, and IN. The shortcut branch is an identity mapping of the block input.
We use an IN layer instead of a batch normalization layer\cite{Ulyanov2017Improved} to accelerate model convergence and maintain independence between features.
All normalized feature maps are activated by LReLU with a negative slope of 0.2.

\textbf{Style Encoder} Style encoder $E_i^{\text{S}}$ is composed of site-shared and site-specific subnetworks.
The site-shared subnetwork is constructed by a convolution layer, four pre-activation residual blocks, and a global average pooling layer.
The site-specific subnetwork is composed of $N$ fully connected layers corresponding to the $N$ individual sites.
The pre-activation residual block is constructed by integrating LReLU activation followed by a convolution layer with unit stride into a residual block, where an average pooling layer is adopted to downsample the intermediate features and the shortcut branch is implemented by an average pooling layer and a convolution layer with unit kernel size and stride. 
Note that we extract style features without IN layers since IN removes feature means and variances, which contain important style information.
The output dimension is set to 64. Style features $s_i$ have the same dimension.

\textbf{Generator} Site-shared generator $G$ merges content features $c_i$ and style features $s_j$ to create a harmonized image $\tilde{x}_j$ using four residual blocks identical to the content encoder, two upsampling blocks and a convolution layer with hyperbolic tangent (tanh) activation. The upsampling block consists of deconvolution, IN, and LReLU activation.

\textbf{Style Generator} Style generator $G^{\text{S}}_j$ consists of a multilayer perception (MLP) with $N$ output branches.
% where $N$ indicates the number of sites. 
Six fully connected layers are shared among all sites, followed by a fully connected layer for each site.
We set the dimensions of the latent code, the hidden layer, and the style features to 16, 256, and 64, respectively.
We randomly sample the latent code $z$ from the standard Gaussian distribution.

\textbf{Discriminator} Discriminator $D_j$ consists of site-shared and site-specific subnetworks, similar to the style encoder. 
Specifically, three convolutional blocks and a global average pooling are shared among all sites, followed by a specific fully connected layer for real/fake classification for each site. 

\subsection{Implementation Details} 
MURD was implemented using Tensorflow. Evaluation was based on a machine with a CPU (i.e., Intel i7-8700K) and a GPU (i.e., NVIDIA GeForce GTX 1080Ti 11GB). The ADAM optimizer with $1\times10^{-4}$ learning rate was utilized for minimization based on the loss function. For all datasets, we used $\lambda_\text{cont}=10$, $\lambda_\text{ca}=0.01$, $\lambda_\text{sty}=10$, $\lambda_\text{cyc}=10$, $\lambda_\text{g}=0.1$, and $\lambda_\text{id}=10$ for the corresponding losses.
For all datasets, every three adjacent slices in each volume were inserted into three channels. Each channel
was then normalized to have a range between -1 and 1 and zero-padded to 256$\times$256.

%\subsection{Expansion for Additional Sites}
%%\todo{A brief description here on how MURD can be extended for additional sites and how the network can be trained efficiently.}
%MURD can be expanded for harmonization of additional sites by adding specific output branches to style generator, style encoder and discriminator for each additional site.
%We load the trained model and freeze their weights except these of content encoder.
%We then utilize additional-site data to train the newly added branches and content encoder, where content encoder is trained with an additional distillation loss that prevents the model from forgetting previously learned sites.
%\todo{This is an incremental learning algorithm, not implemented in our paper, but can be further investigated as an application of MURD. I think this might mislead to misunderstanding of the other parts.}


\end{methods}

\begin{addendum}
	\item[Acknowledgments] This work was supported in part by the United States National Institutes of Health (NIH) grants EB006733 and MH125479. 
	\item[Competing Interests] The authors declare no competing interests.
	\item[Correspondence] Correspondence and requests for materials
	should be addressed to Pew-Thian Yap~(email: ptyap@med.unc.edu).
	\item[Data Availability] The Adolescent Brain Cognitive Development (ABCD) 2.0 data used in this study can be obtained from the National Institutes of Mental Health Data Archive (NDA) (\url{http://nda.nih.gov/}).
%	\item[Code Availability] The source code for this study is publicly available on Zenodo.
%	 (\url{https://zenodo.org/record/4773176}).
	\item[Author Contribution] S.L. designed the framework and network architecture, carried out the implementation, performed the experiments, and analyzed the data. S.L. and P.-T.Y. wrote and revised the manuscript.
%	SL, KHT, and PTY revised the manuscript.
	P.-T.Y. conceived the study and were in charge of overall direction and planning. 
%	All work was done at the University of North Carolina at Chapel Hill.
\end{addendum}

%\newpage
\subsection{References}
\footnotesize
%\bibliographystyle{naturemag}
%\bibliography{reference}

\begin{thebibliography}{10}
	\expandafter\ifx\csname url\endcsname\relax
	\def\url#1{\texttt{#1}}\fi
	\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
	\providecommand{\bibinfo}[2]{#2}
	\providecommand{\eprint}[2][]{\url{#2}}
	
	\bibitem{Jernigan2018The}
	\bibinfo{author}{Jernigan, T.~L.}, \bibinfo{author}{Brown, S.~A.} \&
	\bibinfo{author}{Dowling, G.~J.}
	\newblock \bibinfo{title}{The adolescent brain cognitive development study}.
	\newblock \emph{\bibinfo{journal}{Journal of Research on Adolescence}}
	\textbf{\bibinfo{volume}{28}}, \bibinfo{pages}{154--156}
	(\bibinfo{year}{2018}).
	
	\bibitem{Mueller2005The}
	\bibinfo{author}{Mueller, S.~G.} \emph{et~al.}
	\newblock \bibinfo{title}{The {A}lzheimer's disease neuroimaging initiative}.
	\newblock \emph{\bibinfo{journal}{Neuroimaging Clinics of North America}}
	\textbf{\bibinfo{volume}{15}}, \bibinfo{pages}{869--877}
	(\bibinfo{year}{2005}).
	
	\bibitem{Ellis2009The}
	\bibinfo{author}{Ellis, K.~A.} \emph{et~al.}
	\newblock \bibinfo{title}{The {A}ustralian {I}maging, {B}iomarkers and
		{L}ifestyle ({AIBL}) study of aging: Methodology and baseline characteristics
		of 1112 individuals recruited for a longitudinal study of {A}lzheimer's
		disease}.
	\newblock \emph{\bibinfo{journal}{International Psychogeriatrics}}
	\textbf{\bibinfo{volume}{21}}, \bibinfo{pages}{672--687}
	(\bibinfo{year}{2009}).
	
	\bibitem{Shinohara2017Volumetric}
	\bibinfo{author}{Shinohara, R.} \emph{et~al.}
	\newblock \bibinfo{title}{Volumetric analysis from a harmonized multisite brain
		{MRI} study of a single subject with multiple sclerosis}.
	\newblock \emph{\bibinfo{journal}{American Journal of Neuroradiology}}
	\textbf{\bibinfo{volume}{38}}, \bibinfo{pages}{1501--1509}
	(\bibinfo{year}{2017}).
	
	\bibitem{Pomponio2020Harmonization}
	\bibinfo{author}{Pomponio, R.} \emph{et~al.}
	\newblock \bibinfo{title}{Harmonization of large {MRI} datasets for the
		analysis of brain imaging patterns throughout the lifespan}.
	\newblock \emph{\bibinfo{journal}{{NeuroImage}}}
	\textbf{\bibinfo{volume}{208}}, \bibinfo{pages}{116450}
	(\bibinfo{year}{2020}).
	
	\bibitem{Yu2018Statistical}
	\bibinfo{author}{Yu, M.} \emph{et~al.}
	\newblock \bibinfo{title}{Statistical harmonization corrects site effects in
		functional connectivity measurements from multi-site {fMRI} data}.
	\newblock \emph{\bibinfo{journal}{Human Brain Mapping}}
	\textbf{\bibinfo{volume}{39}}, \bibinfo{pages}{4213--4227}
	(\bibinfo{year}{2018}).
	
	\bibitem{Shah2011Evaluating}
	\bibinfo{author}{Shah, M.} \emph{et~al.}
	\newblock \bibinfo{title}{Evaluating intensity normalization on {MRIs} of human
		brain with multiple sclerosis}.
	\newblock \emph{\bibinfo{journal}{Medical Image Analysis}}
	\textbf{\bibinfo{volume}{15}}, \bibinfo{pages}{267--282}
	(\bibinfo{year}{2011}).
	
	\bibitem{Nyul1999On}
	\bibinfo{author}{Udupa, L. G. N. J.~K.}
	\newblock \bibinfo{title}{On standardizing the mr image intensity scale}.
	\newblock \emph{\bibinfo{journal}{Magenetic Resonance in Medicine}}
	\textbf{\bibinfo{volume}{42}}, \bibinfo{pages}{1072--1081}
	(\bibinfo{year}{1999}).
	
	\bibitem{He2013Intensity}
	\bibinfo{author}{He, Q.}, \bibinfo{author}{Shiee, N.}, \bibinfo{author}{Reich,
		D.~S.}, \bibinfo{author}{Calabresi, P.~A.} \& \bibinfo{author}{Pham, D.~L.}
	\newblock \bibinfo{title}{Intensity standardization of longitudinal images
		using 4d clustering}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of International Symposium
			on Biomedical Imaging}} (\bibinfo{year}{2013}).
	
	\bibitem{Shinohara2014Statistical}
	\bibinfo{author}{Shinohara, R.~T.} \emph{et~al.}
	\newblock \bibinfo{title}{Statistical normalization techniques for magnetic
		resonance imaging}.
	\newblock \emph{\bibinfo{journal}{{NeuroImage}: Clinical}}
	\textbf{\bibinfo{volume}{6}}, \bibinfo{pages}{9--19} (\bibinfo{year}{2014}).
	
	\bibitem{Wrobel2020Intensity}
	\bibinfo{author}{Wrobel, J.} \emph{et~al.}
	\newblock \bibinfo{title}{Intensity warping for multisite {MRI} harmonization}.
	\newblock \emph{\bibinfo{journal}{NeuroImage}} \textbf{\bibinfo{volume}{223}},
	\bibinfo{pages}{117242} (\bibinfo{year}{2020}).
	
	\bibitem{Fortin2016Removing}
	\bibinfo{author}{Fortin, J.-P.}, \bibinfo{author}{Sweeney, E.~M.},
	\bibinfo{author}{Muschelli, J.}, \bibinfo{author}{Crainiceanu, C.~M.} \&
	\bibinfo{author}{Shinohara, R.~T.}
	\newblock \bibinfo{title}{Removing inter-subject technical variability in
		magnetic resonance imaging studies}.
	\newblock \emph{\bibinfo{journal}{{NeuroImage}}}
	\textbf{\bibinfo{volume}{132}}, \bibinfo{pages}{198--212}
	(\bibinfo{year}{2016}).
	
	\bibitem{Fortin2017Harmonization}
	\bibinfo{author}{Fortin, J.-P.} \emph{et~al.}
	\newblock \bibinfo{title}{Harmonization of multi-site diffusion tensor imaging
		data}.
	\newblock \emph{\bibinfo{journal}{NeuroImage}} \textbf{\bibinfo{volume}{161}},
	\bibinfo{pages}{149--170} (\bibinfo{year}{2017}).
	
	\bibitem{Fortin2018Harmonization}
	\bibinfo{author}{Fortin, J.-P.} \emph{et~al.}
	\newblock \bibinfo{title}{Harmonization of cortical thickness measurements
		across scanners and sites}.
	\newblock \emph{\bibinfo{journal}{NeuroImage}} \textbf{\bibinfo{volume}{167}},
	\bibinfo{pages}{104--120} (\bibinfo{year}{2018}).
	
	\bibitem{Jog2017Random}
	\bibinfo{author}{Jog, A.}, \bibinfo{author}{Carass, A.}, \bibinfo{author}{Roy,
		S.}, \bibinfo{author}{Pham, D.~L.} \& \bibinfo{author}{Prince, J.~L.}
	\newblock \bibinfo{title}{Random forest regression for magnetic resonance image
		synthesis}.
	\newblock \emph{\bibinfo{journal}{Medical Image Analysis}}
	\textbf{\bibinfo{volume}{35}}, \bibinfo{pages}{475--488}
	(\bibinfo{year}{2017}).
	
	\bibitem{Garcia-Dias2020Neuroharmony}
	\bibinfo{author}{Garcia-Dias, R.} \emph{et~al.}
	\newblock \bibinfo{title}{Neuroharmony: A new tool for harmonizing volumetric
		{MRI} data from unseen scanners}.
	\newblock \emph{\bibinfo{journal}{{NeuroImage}}}
	\textbf{\bibinfo{volume}{220}}, \bibinfo{pages}{117127}
	(\bibinfo{year}{2020}).
	
	\bibitem{Dewey2019Deep}
	\bibinfo{author}{Dewey, B.~E.} \emph{et~al.}
	\newblock \bibinfo{title}{{DeepHarmony}: A deep learning approach to contrast
		harmonization across scanner changes}.
	\newblock \emph{\bibinfo{journal}{Magnetic Resonance Imaging}}
	\textbf{\bibinfo{volume}{64}}, \bibinfo{pages}{160--170}
	(\bibinfo{year}{2019}).
	
	\bibitem{Dewey2020A}
	\bibinfo{author}{Dewey, B.~E.} \emph{et~al.}
	\newblock \bibinfo{title}{A disentangled latent space for cross-site {MRI}
		harmonization}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of International Conference
			on Medical Image Computing and Computer-Assisted Intervention (MICCAI)}},
	\bibinfo{pages}{720--729} (\bibinfo{publisher}{Springer International
		Publishing}, \bibinfo{year}{2020}).
	
	\bibitem{Dinsdale2021Deep}
	\bibinfo{author}{Dinsdale, N.~K.}, \bibinfo{author}{Jenkinson, M.} \&
	\bibinfo{author}{Namburete, A.~I.}
	\newblock \bibinfo{title}{Deep learning-based unlearning of dataset bias for
		{MRI} harmonisation and confound removal}.
	\newblock \emph{\bibinfo{journal}{{NeuroImage}}}
	\textbf{\bibinfo{volume}{228}}, \bibinfo{pages}{117689}
	(\bibinfo{year}{2021}).
	
	\bibitem{Liu2017Unsupervised}
	\bibinfo{author}{Liu, M.-Y.}, \bibinfo{author}{Breuel, T.} \&
	\bibinfo{author}{Kautz, J.}
	\newblock \bibinfo{title}{Unsupervised image-to-image translation networks}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of Neural Information
			Processing Systems (NeurIPS)}}, \bibinfo{pages}{700--708}
	(\bibinfo{year}{2017}).
	
	\bibitem{Zhu2017Toward}
	\bibinfo{author}{Zhu, J.-Y.} \emph{et~al.}
	\newblock \bibinfo{title}{Toward multimodal image-to-image translation}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of Neural Information
			Processing Systems (NeurIPS)}}, \bibinfo{pages}{465--476}
	(\bibinfo{year}{2017}).
	
	\bibitem{Isola2017Image}
	\bibinfo{author}{Isola, P.}, \bibinfo{author}{Zhu, J.-Y.},
	\bibinfo{author}{Zhou, T.} \& \bibinfo{author}{Efros, A.~A.}
	\newblock \bibinfo{title}{Image-to-image translation with conditional
		adversarial networks}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of IEEE Conference on
			Computer Vision and Pattern Recognition (CVPR)}}, \bibinfo{pages}{1125--1134}
	(\bibinfo{year}{2017}).
	
	\bibitem{Anoosheh2018ComboGAN}
	\bibinfo{author}{Anoosheh, A.}, \bibinfo{author}{Agustsson, E.},
	\bibinfo{author}{Timofte, R.} \& \bibinfo{author}{Van~Gool, L.}
	\newblock \bibinfo{title}{Combogan: Unrestrained scalability for image domain
		translation}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of the IEEE Conference on
			Computer Vision and Pattern Recognition (CVPR) Workshops}}
	(\bibinfo{year}{2018}).
	
	\bibitem{Choi2018StarGAN}
	\bibinfo{author}{Choi, Y.} \emph{et~al.}
	\newblock \bibinfo{title}{{StarGAN}: Unified generative adversarial networks
		for multi-domain image-to-image translation}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of the IEEE Conference on
			Computer Vision and Pattern Recognition (CVPR)}} (\bibinfo{year}{2018}).
	
	\bibitem{Choi2020StarGANv2}
	\bibinfo{author}{Choi, Y.}, \bibinfo{author}{Uh, Y.}, \bibinfo{author}{Yoo, J.}
	\& \bibinfo{author}{Ha, J.-W.}
	\newblock \bibinfo{title}{{StarGAN v2}: Diverse image synthesis for multiple
		domains}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of the IEEE/CVF Conference
			on Computer Vision and Pattern Recognition (CVPR)}} (\bibinfo{year}{2020}).
	
	\bibitem{Lee2020DRIT}
	\bibinfo{author}{Lee, H.-Y.} \emph{et~al.}
	\newblock \bibinfo{title}{{DRIT++}: Diverse image-to-image translation via
		disentangled representations}.
	\newblock \emph{\bibinfo{journal}{International Journal of Computer Vision}}
	\textbf{\bibinfo{volume}{128}}, \bibinfo{pages}{2402--2417}
	(\bibinfo{year}{2020}).
	
	\bibitem{Casey2018ABCD}
	\bibinfo{author}{Casey, B.} \emph{et~al.}
	\newblock \bibinfo{title}{The adolescent brain cognitive development ({ABCD})
		study: Imaging acquisition across 21 sites}.
	\newblock \emph{\bibinfo{journal}{Developmental Cognitive Neuroscience}}
	\textbf{\bibinfo{volume}{32}}, \bibinfo{pages}{43--54}
	(\bibinfo{year}{2018}).
	
	\bibitem{Avants2009Advanced}
	\bibinfo{author}{Avants~B., S.~G., Tustison~N.}
	\newblock \bibinfo{title}{Advanced normalization tools}.
	\newblock \emph{\bibinfo{journal}{Insight Journal}}  (\bibinfo{year}{2009}).
	
	\bibitem{Heusel2017Gans}
	\bibinfo{author}{Heusel, M.}, \bibinfo{author}{Ramsauer, H.},
	\bibinfo{author}{Unterthiner, T.}, \bibinfo{author}{Nessler, B.} \&
	\bibinfo{author}{Hochreiter., S.}
	\newblock \bibinfo{title}{{GAN}s trained by a two time-scale update rule
		converge to a local nash equilibrium.}
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of Neural Information
			Processing Systems (NeurIPS)}} (\bibinfo{year}{2017}).
	
	\bibitem{Binkowski2018Demystifying}
	\bibinfo{author}{Bi\'{n}kowski, M.}, \bibinfo{author}{Sutherland, D.},
	\bibinfo{author}{Arbel, M.} \& \bibinfo{author}{Gretton, A.}
	\newblock \bibinfo{title}{Demystifying {MMD} {GAN}s}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of International Conference
			on Learning Representations (ICLR)}}, \bibinfo{pages}{1--36}
	(\bibinfo{year}{2018}).
	
	\bibitem{salimans2016improved}
	\bibinfo{author}{Salimans, T.} \emph{et~al.}
	\newblock \bibinfo{title}{Improved techniques for training {GAN}s}.
	\newblock In \emph{\bibinfo{booktitle}{Advances in Neural Information
			Processing Systems}}, vol.~\bibinfo{volume}{29}, \bibinfo{pages}{2234--2242}
	(\bibinfo{year}{2016}).
	
	\bibitem{Szegedy2016Rethinking}
	\bibinfo{author}{Szegedy, C.}, \bibinfo{author}{Vanhoucke, V.},
	\bibinfo{author}{Ioffe, S.}, \bibinfo{author}{Shlens, J.} \&
	\bibinfo{author}{Wojna, Z.}
	\newblock \bibinfo{title}{Rethinking the {I}nception architecture for computer
		vision}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of Computer Vision and
			Pattern Recognition ({CVPR})}} (\bibinfo{publisher}{{IEEE}},
	\bibinfo{year}{2016}).
	
	\bibitem{deng2009imagenet}
	\bibinfo{author}{Deng, J.} \emph{et~al.}
	\newblock \bibinfo{title}{{ImageNet}: A large-scale hierarchical image
		database}.
	\newblock In \emph{\bibinfo{booktitle}{Computer Vision and Pattern Recogntion
			(CVPR)}} (\bibinfo{year}{2009}).
	
	\bibitem{Smith2002Fast}
	\bibinfo{author}{Smith, S.~M.}
	\newblock \bibinfo{title}{Fast robust automated brain extraction}.
	\newblock \emph{\bibinfo{journal}{Human Brain Mapping}}
	\textbf{\bibinfo{volume}{17}}, \bibinfo{pages}{143--155}
	(\bibinfo{year}{2002}).
	
	\bibitem{Zhang2001Segmentation}
	\bibinfo{author}{Zhang, Y.}, \bibinfo{author}{Brady, M.} \&
	\bibinfo{author}{Smith, S.}
	\newblock \bibinfo{title}{Segmentation of brain {MR} images through a hidden
		markov random field model and the expectation-maximization algorithm}.
	\newblock \emph{\bibinfo{journal}{{IEEE} Transactions on Medical Imaging}}
	\textbf{\bibinfo{volume}{20}}, \bibinfo{pages}{45--57}
	(\bibinfo{year}{2001}).
	
	\bibitem{Liu2021Style}
	\bibinfo{author}{Liu, M.} \emph{et~al.}
	\newblock \bibinfo{title}{Style transfer using generative adversarial networks
		for multi-site {MRI} harmonization}.
	\newblock \emph{\bibinfo{journal}{bioRxiv}}  (\bibinfo{year}{2021}).
	
	\bibitem{Zuo2021Information}
	\bibinfo{author}{Zuo, L.} \emph{et~al.}
	\newblock \bibinfo{title}{Information-based disentangled representation
		learning for unsupervised {MR} harmonization}.
	\newblock In \emph{\bibinfo{booktitle}{International Conference on Information
			Processing in Medical Imaging (IPMI)}} (\bibinfo{year}{2021}).
	
	\bibitem{Varsavsky2020Thomas}
	\bibinfo{author}{Varsavsky, T.} \emph{et~al.}
	\newblock \bibinfo{title}{Test-time unsupervised domain adaptation}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of Medical Image Computing
			and Computer Assisted Intervention (MICCAI)}}, \bibinfo{pages}{428--436}
	(\bibinfo{year}{2020}).
	
	\bibitem{Ulyanov2017Improved}
	\bibinfo{author}{Ulyanov, D.}, \bibinfo{author}{Vedaldi, A.} \&
	\bibinfo{author}{Lempitsky, V.}
	\newblock \bibinfo{title}{Improved texture networks: maximizing quality and
		diversity in feed-forward stylization and texture synthesis}.
	\newblock In \emph{\bibinfo{booktitle}{Proceedings of IEEE Conference on
			Computer Vision and Pattern Recognition (CVPR)}}, \bibinfo{pages}{6924--6932}
	(\bibinfo{year}{2017}).
	
\end{thebibliography}

\end{document}
