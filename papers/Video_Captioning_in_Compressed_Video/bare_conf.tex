
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[a4paper,conference]{IEEEtran}
% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

\ifCLASSINFOpdf
\else
   \usepackage[dvips]{graphicx}
\fi
\usepackage{url}
\usepackage{graphicx}
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{amsmath}
\usepackage{amsfonts} 
\usepackage{amssymb}

\begin{document}



%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Video Captioning in Compressed Video}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
% \author{\IEEEauthorblockN{Mingjian Zhu}
% \IEEEauthorblockA{Zhejiang University, China\\
% School of Engineering, Westlake University\\
% Email: zhumingjian@westlake.edu.cn}
% \and
% \IEEEauthorblockN{Chenrui Duan}
% \IEEEauthorblockA{Zhejiang University, China\\
% School of Engineering, Westlake University\\
% Email: duanchenrui@westlake.edu.cn}
% }



\author{
	Mingjian Zhu$^{1,2}$, Chenrui Duan$^{1,2}$, Changbin Yu$^{1,2}$\\
		\normalsize $^1$Zhejiang University, China\\
			\normalsize $^2$School of Engineering, Westlake University, China \\
			\normalsize \{zhumingjian, duanchenrui, yu\_lab\}@westlake.edu.cn\\		
	}

	
\maketitle

%
%\IEEEpeerreviewmaketitle


% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
%
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3},
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area


% As a general rule, do not put math, special symbols or citations
% in the abstract


\begin{abstract}
Existing approaches in video captioning concentrate on exploring global frame features in the uncompressed videos, while the free of charge and critical saliency information already encoded in the compressed videos is generally neglected. We propose a video captioning method which operates directly on the stored compressed videos. To learn a discriminative visual representation for video captioning, we design a residuals-assisted encoder (RAE), which spots regions of interest in I-frames under the assistance of the residuals frames. First, we obtain the spatial attention weights by extracting features of residuals as the saliency value of each location in I-frame and design a spatial attention module to refine the attention weights. We further propose a temporal gate module to determine how much the attended features contribute to the caption generation, which enables the model to resist the disturbance of some noisy signals in the compressed videos. Finally, Long Short-Term Memory is utilized to decode the visual representations into descriptions. We evaluate our method on two benchmark datasets and demonstrate the effectiveness of our approach.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.



\section{Introduction}

Video captioning, which aims to automatically describe video content in natural language, becomes progressively popular in the communities of computer vision and natural language. Earlier works on video captioning~\cite{venugopalan2014translating,xu2017learning, wang2018reconstruction} mostly adopt the typical deep encoder-decoder framework and achieve encouraging performance. In the encoding stage, the features of sampled frames are extracted by CNN, when RNN is employed to encode the sequences of features into visual representations. Then a decoder is utilized to translate the representations into descriptive sentences. But such a typical framework basically suffers from two severe drawbacks. First, a pretrained CNN is generally employed to directly extract the features of the global RGB images, which neglects that spatial distribution of significant signals in a single frame is generally imbalanced and lacks explicitly procedure of highlighting the meaningful signals in the salient regions. To tackle this problem, some previous works~\cite{wang2018spotting, tu2017video} propose region level attention mechanisms. However, these methods generate spatial attention maps mainly by exploring the information from the RGB image itself. An efficient method with the capability of simultaneously exploiting the information from RGB images and incorporating other visual sources is believed to achieve better performance and is eagerly needed. Second, an obvious fact is that a video can be compressed to a fairly small size, which implies that an uncompressed video contains high information redundancy. The boring and repeating patterns of decoded frames in videos will largely drown the interesting signals and hinder the exploration of essential information. These are common problems existing in video understanding tasks.
\begin{figure}[!t]
%\centering
\includegraphics[width=1.0\linewidth]{figure/figure1.pdf}
\caption{Example I-frames (top) and residuals (bottom) shown in two rows. The residuals frames contain lots of noisy movement patterns, which correspond to the regions of interest in I-frame. (Best viewed in color)}
\label{fig:motivation}
\end{figure}

To address these issues, recent works~\cite{wu2018compressed, shou2019dmc, wang2019fast} apply deep models directly on compressed videos, which is encoded by the compression standards like MPEG-4, H.264, and HEVC. Tackling video understanding problem in compressed videos brings lots of benefits. First, the saliency information is already embedded in the compressed videos. Accessing inherent information costs little computation. Second, the compression technique reduces excess information in a video and keeps the most essential signals, i.e., the saliency signals. The saliency signals leave out the visual appearance variations and highlight the prominent information.
Generally, an encoded video is comprised of a number of GOPs (group of pictures) and the GOP contains two kinds of frames: I-frames and P-frames~\cite{wu2018compressed, shou2019dmc, wang2019fast}. I-frame in a compressed video can be regarded as a regular complete image. P-frames encode only the 'change' among the frames. P-frames can be reconstructed by residuals and motion vector~\cite{wang2019fast}. As the example provided in Fig.~\ref{fig:motivation}, The residuals, though noisy, resemble optical flow~\cite{sun2018optical} and locate the areas with visually drastic changes in the video. The salient regions in the residuals frames exhibit notable motion patterns, which correspond to the actions or objects in the videos and human descriptions. In the video captioning task, the captions for a video are labeled by humans, and human attention tends to be attracted by the salient regions where the visual appearance changes noticeably. Following this observation, we propose to explore and leverage the saliency information embedded in the compressed videos to guide the generation of spatial attention.


\begin{figure*}[!htb]
\centering
\includegraphics[width=1.0\linewidth]{figure/figure2.pdf}

\caption{Illustration of our video captioning framework. It contains two components: (1) residuals-assisted encoder (2) decoder. Specifically, residuals-assisted encoder contains the spatial attention module and temporal gate module, and automatically selects the most correlated visual features as the input to the decoder. The decoder, i.e., LSTM in this framework, is utilized to generate the description word by word.
(Best viewed in color)}

\label{fig:framework}
\end{figure*}

In this paper, we propose a novel video captioning method, called video captioning in compressed video (VCCV). This method generates the spatial attention map for the I-frame, with the assistance of saliency information in the residuals. We also design a gate mechanism to adaptively fuse visual features for description generation with the motivation of retaining only the positive impact of the residuals. Our proposed method takes advantage of signals from different sources(i.e. I-frame and residuals). The main contributions of our work can be summarized as follows: 

\begin{itemize}
\item We propose an end-to-end deep framework to efficiently exploit I-frame and residuals in the compressed videos. As far as we know, we are the first to tackle video captioning task with the usage of the compressed signals. Besides, comparing with previous video understanding methods on compressed videos, we exploit the saliency information in residuals frame to assist the generation of spatial attention map in the deep model.

\end{itemize}

\begin{itemize}
\item  Our residuals-assisted encoder (RAE) attends salient regions in I-frame with the assistance of the residuals. Besides, the gate mechanism in RAE fuses visual features and inputs them to the decoder, which enhances the robustness of the method against the noise in the residuals. 
\end{itemize}
 
\begin{itemize}
\item Our method is evaluated on two public benchmarks: MST-VTT~\cite{xu2016msr} and Charades~\cite{sigurdsson2016hollywood} and achieves results that rival state-of-the-art methods.
\end{itemize}


\section{Related Work}

\subsection{Video Captioning}
\noindent \textbf{Template Based Video Captioning.} Inspired by the progress in image captioning, some earlier works employ the template to synthesize the sentence in video captioning task. Template based language model can ensure the grammatical correctness.~\cite{kojima2002natural} correlates concept hierarchy of actions with semantic features of human motions for video captioning task.~\cite{rohrbach2013translating} proposes to employ conditional random fields and a template model to generate a sentence for a video. Although template based methods can ensure the completeness of sentences, their generated descriptions are inflexible.
% \noindent \textbf{Template based video captioning:} Earlier works on video captioning employ template to synthesize the captions. A well-defined template and well-designed semantic attributes detection mechanism can effectively boost the performance. ~\cite{kojima2002natural} proposes to bridge the semantic gap between natural language and video images by constructing a concept hierarchy of actions. ~\cite{rohrbach2013translating} learns a CRF to predict the semantic representation of visual content and employ templates to generate sentences. ~\cite{xu2015jointly} proposes a joint embedding model, a video model, and a language model. The language model employs dependency-tree structure model to generate sentences.
\\
\\
\noindent \textbf{Encoder-Decoder Based Video Captioning.} 
Mean Pool method~\cite{venugopalan2014translating} extracts the features of each frame by using CNN, mean pool the features across the entire video, and input this to the LSTM network. a-LSTM~\cite{gao2017video} introduces semantic cross-view loss and relevance loss to enforce the consistency between the features of video and language. Attention Fusion ~\cite{hori2017attention} propose a  method to selectively attend specific modalities of features, which provides a way to fuse multimodal information for video captioning. %hLSTMat framework~\cite{song2017hierarchical} employs hierarchical LSTMs, temporal attention, and adjusted temporal attention mechanism. The hierarchical LSTMs leverages both the low-level visual information and high-level language information to generate sentences. Temporal attention and adjusted temporal attention mechanism are designed to pay attention to specific frames in a video and decide whether to leverage visual information or language information, separately.
%~\cite{yang2018video} combines adversarial learning and LSTM for video captioning. The generator in the LSTM-GAN architecture generates sentences while the discriminator verifies the sentences. 
RecNet~\cite{wang2018reconstruction} contains three parts: the CNN-based encoder, the LSTM-based decoder, and the reconstructor. The proposed two types of reconstructors aim to minimize the diversity between the original and reproduced video features.  \cite{wang2018reconstruction} also proposes to exploit both video-to-sequence and sequences-to-video flows for video captioning. SAM~\cite{wang2018spotting} designs a model to spot and aggregate the salient regions in the video. This method explores the information from the RGB image to locate the salient regions, while our VCCV exploits the residuals frames in the compressed video.~\cite{zhu2019attention} proposes Attention-based Densely Connected Long Short-Term Memory which makes all previous cells connected to the current cell and the updating of the current state related to all its previous states.%~\cite{barati2019critic} exploits reinforcement learning to detect and describe temporal segments by leveraging the correlation structure of the events.

\subsection{Video Understanding Model on Compressed Videos}
~\cite{zhang2018video, khatoonabadi2017compressed} propose methods for saliency estimation in compressed videos, without using deep models. Only a few previous works apply deep learning directly on compressed videos and neither of them tackles video captioning task. For recognizing the actions in the compressed videos, CoViAR~\cite{wu2018compressed} develops three independent deep CNN models, which are trained on data of different modalities: I-frame, residuals, and motion vector. Their outputs are fused together as the final prediction. However, the correlation among the three modalities cannot be fully exploited because of the model independence. We argue that the relationships among different modalities should be further explored to generate more discriminative visual representation. In order to achieve state-of-art performance, CoViAR employs optical flow feature, which consumes lots of computation. To avoid optical flow computation in the inference stage, DMC-Net~\cite{shou2019dmc} leverages Generative Adversarial Networks (GAN) to approximate optical flow with the usage of residuals and motion vector. However, optical flow is still needed in training. Motion aided Memory
Network (MMNet)~\cite{wang2019fast} utilizes residuals as input and leverages motion vector to align the hidden features. The generated cell features of the modified LSTM are aggregated to the detection network for the video object detection task. MMNet reveals that special patterns exist in the features generated by compressed signals. This finding inspires us to explore the potential of fully exploiting the patterns to contribute to the visual encoding stage in video captioning task. However, according to the above three works, the signals in compressed videos are noisy, which hinders their direct application as features for video encoding. To address this issue, we propose a more practical method, which generates attended spatial representation in the deep model by exploiting the embedded saliency information in the compressed videos.


\section{METHOD}
As shown in Fig.~\ref{fig:framework}, the proposed VCCV method is comprised of two components, namely the residuals-assisted encoder and the decoder. The residuals-assisted encoder is designed to encode the visual content information, while the decoder is used for generating sentences. Different from many previous works, which explore the relationship between visual and context information in video-sentences pairs, we focus on designing a video captioning model that effectively generates discriminative visual representations. %attends to salient regions under the assistance of the residuals in compressed domain.

A compressed video contains I-frames and residuals. $N$ I-frames are sampled uniformly in the compressed video. We also collect the residuals frame in the corresponding GOP of each I-frame.  The shapes of the sampled I-frames $P_I$ and residuals $P_r$ are $(N, H_P, W_P, C)$, where $H_P$, $W_P$, and $C$ denote the height, width, and the channels of the frames, respectively. We leverage two pretrained CNNs to extract the features of I-frames and their corresponding rough attention maps from residuals: 
\begin{equation}
\begin{aligned}
V_I = CNN_{I}(P_I),\\ A_{r} = CNN_{r}(P_{r}),
\end{aligned}
\end{equation}
where $ V_I \in \mathbb{R}^{ N\times H \times W \times D_I}$ is the extracted feature of I-frame, and $A_r \in \mathbb{R}^{N \times H \times W \times D_r}$ is the feature of the residuals. Since the salient regions in residuals frames correspond to notable actions or objects in I-frames, $A_r$ can be considered as the rough attention map. $D_I$ and $D_r$ are the output channels of pretrained backbone model $CNN_I$ and $CNN_r$, respectively. Note that we extract and store $V_I$ and $A_r$ before training our proposed model.

\subsection{Residuals-Assisted Encoder}
\label{section:Residuals-Assisted Encoder}
%After the pre-processing procedure,
After the pre-processing procedure, the resulting features $V_I$ and $A_r$ are fed into residuals-assisted encoder to generate attended visual representation $F_G^{att}$:
\begin{equation}
\begin{aligned}
F_G^{att} &= RAE(V_I, A_r, h_{t-1})
\end{aligned}
\end{equation}
where $h_{t-1}$ denotes the hidden state of decoder in time step $t-1$. We conduct the average pooling operations along the spatial dimensions to get the following features:
\begin{equation}
\begin{aligned}
V_{I}^{c} &= \frac{1}{H \times W} \sum_{h=1}^{H}\sum_{w=1}^{W} V_I, \\
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
 A_{r}^{c} &= \frac{1}{H \times W}\sum_{h=1}^{H}\sum_{w=1}^{W} A_{r},\\
\end{aligned}
\end{equation}
We also conduct the average pooling operations along the channel dimensions:
\begin{equation}
\begin{aligned}\label{channel pooling}
V_{I}^{conv} &= Conv(V_I), \\
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
V_{I}^{s} &= \frac{1}{D_r} \sum_{d=1}^{D_r} V_{I}^{conv},\\
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
A_{r}^{s} &= \frac{1}{D_r} \sum_{d=1}^{D_r} A_r,\\
\end{aligned}
\end{equation}
where the convolution operation in Eq.\ref{channel pooling} reduces the channel dimension of $V$ from $D_I$ to $D_r$ and we train this layer along with the encoder-decoder model in an end-to-end manner. $V_I^s \in \mathbb{R}^{N \times H \times W}$, $A_r^s \in \mathbb{R}^{N \times H \times W}$ and $V_I^{conv} \in \mathbb{R}^{N \times H \times W \times D_r}$ are input to spatial attention module (SAM), while $V_I^c \in \mathbb{R}^{N \times D_I}$ and $A_r^c \in \mathbb{R}^{N \times D_r}$ are fed into temporal gate module (TGM).\\
\textbf{Spatial Attention Module.}  The SAM refines the rough attention map by incorporating the spatial information from I-frame and context information from decoder. For each I-frame and its corresponding residuals frame in the same GOP, SAM produces the refined attention map $\alpha_{R}$ by using $h_{t-1}$, $V_I^s$ and $A_r^s$:
\begin{equation}
\begin{aligned}
\alpha_{R} &= \tanh(\mathbb{E}_{\alpha} (W_{t} h_{t-1}) + W_{I} V_I^{s} + W_{r} A_{r}^{s} )\label{eq:alpha_R}
\end{aligned}
\end{equation}
where $W_{t}, W_{I}, W_{r}$ are learnable weights. $\mathbb{E}_{\alpha} (\cdot)$ operation expands the size of tensor from  $H \times W$ to $N \times H \times W$.
The attention weights of each region are computed by applying softmax function along the spatial dimensions:
\begin{equation}
\begin{aligned}
A_{R} &=\frac{exp(\alpha_{R})}{\sum_{h=1}^{H}\sum_{w=1}^{W} exp(\alpha_{R})}
\end{aligned}
\end{equation}
After that, we obtain the attended global feature $F_{R}^{att}$ by a weighted average over the regional features:
\begin{equation}
\begin{aligned}
F_{R}^{att} &=  \frac{1}{H \times W}\sum^{H}_{h=1} \sum^{W}_{w=1} \mathbb{E}_{A}({A_{R}}) \odot V_{I}^{conv},
\end{aligned}
\end{equation}
where $\mathbb{E}_{A}(\cdot)$ expands the size of $A_R$ from $N \times H \times W$ to $N \times H \times W \times D_r$. $\odot$ denotes the element-wise hadamard product.\\
\textbf{Temporal Gate Module.} Although the attended global feature contains sufficient appearance information, it cannot eliminate the negative impact of noisy signals in the compressed videos. To further alleviate this problem, our proposed TGM fuses the original visual feature $V_{I}^{c}$ and the spatially attended feature $F_{R}^{att}$ with a generated confidence score.
Considering the context information in the hidden state of the LSTM-based text decoder, the distribution of noisy signals in residuals, and the visual information from the original extracted feature, the confidence score $G \in \mathbb{R}^{N}$ is defined as:
\begin{equation}
\begin{aligned}
G &= \tau (W_G^{T} \cdot ( W_{Gt} h_{t-1} + W_{Gr} A_{r}^{c} + W_{GI} V_I^c))
\end{aligned}
\end{equation}
where $W_G$, $ W_{Gt}$, $ W_{Gr}$ and $W_{GI}$ denote the parameters to be learned,
and $\tau$ is the sigmoid activation function. Different from temporal attention mechanism in previous work~\cite{tu2017video, song2017hierarchical}, our proposed TGM controls how much the spatial attended feature and the original extracted feature respectively contribute to the encoded visual representation, which differs in the temporal dimension. The final encoded visual representation $F_G^{att}$ is defined as:
\begin{equation}
\begin{aligned}
F_{G}^{att} &= \mathbb{E}_{G}(G) \odot W_{GR}F_{R}^{att} + \mathbb{E}_{G}(1-G) \odot W_{GI}V_I^c
\end{aligned}
\end{equation}
where $\mathbb{E}_{G}(\cdot)$ expands the sizes of $G$ and $(1-G)$ from $N$ to $N \times D_I$. Again, $\odot$ denotes the element-wise product. By taking advantage of two features, the fusion mechanism can efficiently alleviate the disturbance from some noisy residuals. 
After obtaining the visual representation $F_{G}^{att}$ in the TGM, we further perform the ordered combination of the following operations to this representation: average pooling along frames dimension, linear transformation, ReLU, and dropout.

\subsection{Decoder}
\label{section:Decoder}
Our decoder 
adopts the widely used Long Short-Term Memory (LSTM)~\cite{hochreiter1997long} and takes video representation $F_G^{att}$ from RAE as input:
\begin{equation}
\begin{aligned}
h_t, c_t = LSTM([F_G^{att};x_{t-1}], (h_{t-1}, c_{t-1})), 
\end{aligned}
\end{equation}
 where $[\cdot;\cdot]$ stands for the vector concatenation operation. $h_t$ and $c_t$ are the hidden state and cell state of the LSTM at time step $t$. $x_{t-1}$ is the texture feature of the word at time step $t-1$. After obtaining the hidden state $h_t$, a linear layer is applied after the LSTM layer and a softmax layer are leveraged to produce the probability distribution over all the vocabulary words. In training stage, the ground truth word at previous time step is utilized when predicting the current word. In testing stage, the previous predicted word is input to the LSTM. We employ beam search method~\cite{freitag2017beam} to generate sentences. The framework can be trained in an end-to-end manner and the loss function we are optimizing is the log-likelihood:
\begin{equation}
   \mathop{max}\limits_{\theta}\sum_{t=1}^T log P_r(y_t|F_G^{att}, y_{t-1};\theta),
\end{equation}
where $T$ denotes the length of the sentences. $y_t$ stands for the word of $t$-th time step and $\theta$ denotes the trainable parameters in our method.

\section{Experiments}
\subsection{Datasets, Metrics, Preprocessing and Details}
\noindent\textbf{Datasets.}
{MSR-VTT dataset}~\cite{xu2016msr} is a large-scale video description dataset for evaluating video captioning methods.
It contains 10K web video clips with 41.2 hours in 20 categories. Each video
clip is annotated with about 20 natural sentences. We divide the dataset into training, validation, and testing with 6,513 clips, 497 clips, and 2,990 clips respectively as the original split in the MSR-VTT dataset.

{Charades dataset}~\cite{sigurdsson2016hollywood}  contains 9,848 video clips with 27,847 video descriptions.  Following the official split~\cite{sigurdsson2016hollywood}, we utilize 1863 video clips for evaluation, and the other video clips are used for model development.


% \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  %表格自动换行
\begin{table*}[htb]
    \normalsize
	%\renewcommand\arraystretch{1.05}
	\centering
	\caption{Comparison of model variants on MSR-VTT dataset. (The best scores are \textbf{bold})}
	\begin{tabular}{l|c|c|c|c|c|c|c}
		\hline
		{Model} & BLEU@1 & BLEU@2 & BLEU@3 & BLEU@4 & METEOR & CIDEr & ROUGE-L\\ 
		 %decoded frame & 34.6 & 25.6   &37.6 &57.7\\
		%\tabincell{l}{decoded frame w $\mathcal{M}$} &- &- &- &- \\
		\hline
		{I-frame} &71.1 &55.7 &41.7  &29.9 &23.5 &29.7 &55.0 \\
		\hline
        % \tabincell{l}{I-frame + attention} &{37.6} &{27.5}  &{44.7} &{58.5}　&{37.6} &{27.5}  &{44.7} &{58.5}\\
%  		\hline
        {RAE w/o gate and residuals} &78.1 &63.0 &49.1 &{37.6} &{27.5}  &{44.7} &{58.5} \\
        %\tabincell{l}{RAE w/o gate \\and residuls} &{37.6} &{27.5}  &{44.7} &{58.5} &{16.3} &{17.5}  &{20.5} &{41.5}\\
 		\hline
		{{RAE w/o gate}}  &78.6 &64.5 &51.2 &39.6 &27.8 &\textbf{46.0} &59.8 \\
		\hline
% 		{{RAE(full)}}       &\textbf{39.2} & \textbf{27.7} & \textbf{44.5} & \textbf{59.3}\\
%      	\hline
		{{RAE(full)}}  &\textbf{78.6} &\textbf{64.8} &\textbf{51.7}   &\textbf{40.0} & \textbf{28.0} & {45.7} & \textbf{60.0}\\
		\hline
	\end{tabular}%
	\label{tab:model varient msrvtt}%

\end{table*}%


% \newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  %表格自动换行
\begin{table*}[htb]
\normalsize
	%\renewcommand\arraystretch{1.05}
	\centering
	\caption{Comparison of model variants on Charades dataset. (The best scores are \textbf{bold})}

	\begin{tabular}{l|c|c|c|c|c|c|c}
		\hline
		{Model} & BLEU@1 & BLEU@2 & BLEU@3 & BLEU@4 & METEOR & CIDEr & ROUGE-L\\ 
		 %decoded frame & 34.6 & 25.6   &37.6 &57.7\\
		%\tabincell{l}{decoded frame w $\mathcal{M}$} &- &- &- &- \\
		\hline
		{I-frame} &36.9 &27.6 &18.6 &12.3 &14.0 &8.0 &38.4\\
		\hline
        % \tabincell{l}{I-frame + attention} &{37.6} &{27.5}  &{44.7} &{58.5}　&{37.6} &{27.5}  &{44.7} &{58.5}\\
%  		\hline
        {RAE w/o gate and residuals} &{52.2} &{36.3}  &{24.2} &{16.3} &{17.5}  &{20.5} &{41.5}\\
        %\tabincell{l}{RAE w/o gate \\and residuals} &{37.6} &{27.5}  &{44.7} &{58.5} &{16.3} &{17.5}  &{20.5} &{41.5}\\
 		\hline
		{{RAE w/o gate}}  &\textbf{54.2} &36.5 &{23.9} &15.9 &18.0 &{20.6} &41.3\\
		\hline
% 		{{RAE(full)}}       &\textbf{39.2} & \textbf{27.7} & \textbf{44.5} & \textbf{59.3}\\
%      	\hline
		{{RAE(full)}} &{53.2} & \textbf{36.6} & \textbf{24.3} & \textbf{16.5} & \textbf{18.0} & \textbf{21.0} & \textbf{41.5}\\
		\hline
	\end{tabular}%
	\label{tab:model varient charades}%
\end{table*}%

\noindent\textbf{Evaluation Metrics.} In this paper, we employ several common metrics to evaluate our proposed models: BLEU~\cite{papineni2002bleu}, METEOR ~\cite{denkowski2014meteor}, CIDEr~\cite{vedantam2015cider} and ROUGE-L~\cite{lin2004automatic}. These metrics are widely used in image/video captioning tasks. For a fair evaluation, We utilize the Microsoft COCO evaluation toolkit~\cite{chen2015microsoft} to compute all the values in this paper and report them as percentage(\%).
 
% \begin{table}[ht]
% 	\small
% 	\renewcommand\arraystretch{1.05}
% 	\centering
% 	\caption{The performance comparison with the state-of-art methods on MSR-VTT dataset. V, C, R-N and A denote VGG, C3D, N-layer ResNet, and audio features, respectively.
%     “-” means that the results are not provided.  (The best scores are \textbf{bold})}
    
% \begin{tabular}{l|c|c|c|c}
% \hline
% {Methods}  
% & B@4  & M & C &R\\ \hline
% \hline
% v2t\_navigator(C+A)~\cite{jin2016describing} &40.8 &28.2 &44.8 &60.9\\
% Aalto(G+C)~\cite{shetty2016frame} &39.8 &26.9 &45.7 &59.8\\
% VideoLAB(R+C+A)~\cite{ramanishka2016multimodal} &39.1 &27.7 &44.1 &60.6\\
% \hline
% Mean Pool~\cite{venugopalan2014translating} &30.4 &23.7 &35.0 &52.0\\
% %STAT~\cite{tu2017video} &37.4 &26.6 &41.5 &-\\
% %aLSTMs~\cite{gao2017video} &38.0 &26.1 &43.2 &-\\
% MP-LSTM(G+C+A)&35.7 &25.6 &38.1 &-\\
% LSTM-E(G+C+A)&36.1 &25.8 &38.5 &-\\
% Attention Fusion(V+C+A)~\cite{hori2017attention} &39.7 &25.5 &40.0 &-\\

% hLSTMat(R)~\cite{song2017hierarchical} &38.3 &26.3 &- &-\\
% MA-LSTM(G+C+A)~\cite{xu2017learning} &36.5 &26.5 &41.0 &-\\
% %TDDF~\cite{zhang2017task} &37.3 &27.8 &43.8 &-\\
% M$^3$(V+C)~\cite{wang2018m3} &38.1 &26.6 &- &-\\
% %LSTM-GAN~\cite{yang2018video} &36.0 &26.1 &- &-\\
% %MS-RNN~\cite{song2018deterministic} &39.8 & 26.1 &- &-\\

% %RecNet~\cite{wang2018reconstruction} &39.1 &26.6 & 42.7 &-\\
% PickNet(R)~\cite{chen2018less} &41.3 &27.7 &44.1 &59.8\\
% MCNN+MCF(R)~\cite{wu2018multi} &38.1 &27.2 &- &-\\
% %OA-BTG~\cite{zhang2019object} &41.4 &28.2 &46.9 &-\\
% %MARN~\cite{pei2019memory} &40.4 &28.1 &47.1 &60.7\\
% TDConvED(R)~\cite{chen2019temporal} &39.5 &27.5 &42.8 &-\\
% GRU-EVE(I+C)~\cite{aafaq2019spatio} &38.3 &28.4 &48.1 &60.7\\

% 		\hline
% 		 {UFVC(V)}  & {37.6} & {27.1} & {42.2} & {58.3}\\
% 		 {UFVC(R-18)}  & {36.8} & {26.9} & {42.3} & {57.9}\\
% 		 {UFVC(R-50)}  & {39.9} & {27.6} & {45.0} & {59.4}\\
% 		 {UFVC(R-152)} & {39.5} & {28.1} & {45.9} & {59.5}\\
% 		 {UFVC(R-152 + A)}  & {41.7} & {28.4} & {46.2} &{60.7}\\
% 		 \textbf{UFVC(R-152 + A + C)}  &\textbf{42.4} & \textbf{29.1} & \textbf{48.2}
% 		 & \textbf{61.0} \\
% 		\hline
% \end{tabular}%
% \label{tab:msr-vtt}%
% \end{table}%

% \subsection{Experimental Setup}
% \begin{figure*}[!htb]
% %\centering
% \includegraphics[width=1.0\linewidth]{figure/example.pdf}
% \caption{\textbf{Qualitative results on MSVD dataset.} We provide qualitative results in different experimental settings for an intuitive understanding of the performance. 
% The comparison of the generated captions shows the superiority of our method.}
% \vspace{-0.2cm}
% \label{fig:example results}
% \end{figure*}
\noindent\textbf{Preprocessing.} 
We leverage the CNN model as a visual feature extractor and compute the descriptor with a fixed number of 20 frames. I-frame is a full image. The features of the I-frames are extracted by a CNN pretrained in Imagenet dataset~\cite{russakovsky2015imagenet}. We extract the features of I-frame in our method with backbone model ResNet-152~\cite{he2016deep} and the size of the extracted features from the penultimate layer is $2048\times7\times7$. %Unlike I-frame, there is no residuals images collected in the published dataset. An intuitive solution is extracting the residuals frames online and training a specially designed CNN for the residuals, and the decoder in an end-to-end manner. Considering an such online feature extraction method consumes large amount of time and memory. We adopt the offline feature extraction method. 
Similar to~\cite{wu2018compressed}, We train a  ResNet-18 model for video action recognition by utilizing residuals frames in HMDB-51 dataset~\cite{jhuang2011large}. For feature extraction, we store the activations from the penultimate layer of this model with a size of $512\times7\times7$. 


In the description preprocessing stage, a vocabulary for the corpus is built. All words in the captions are converted to lower case and the sentences are tokenized. The extremely rarely seen words in the vocabulary which appear less than 3 times are also removed.% After preprocessing, the vocabulary of the MSR-VTT dataset contains 13,067 words and the MSVD dataset has a vocabulary with about 16,000 words. 
The max length of sentences is set as 50 and the provided caption with length more than that number will be cut short. During the training phase,  we add a begin-of-sentence $<$BOS$>$ tag at the beginning of each caption, and an end-of-sentence $<$EOS$>$ tag at its end.  In the testing phase, we input the $<$BOS$>$ tag to the decoder for the first time-step, then the word is predicted one by one until the $<$EOS$>$ tag appears. Each word in the processed sentences will be converted into a one-hot vector. The unseen words in the vocabulary are set to the $<$UNK$>$ flags.


 

\noindent\textbf{Training Details.} We optimize our model using Adam~\cite{kingma2014adam} optimizer with an initial learning rate of $1\times10^{-4}$. In our model, the word embedding size is set as 500 and the dimension of the LSTM hidden size is set to be 512. The dimension of visual representation input to the decoder is 2048. To alleviate the overfitting problem, We apply dropout with a rate of 0.5 on the output of LSTM. The training batch size is set to 8 and the beam size in testing is 5.


\subsection{Ablation Study}
In Tab.~\ref{tab:model varient msrvtt} and Tab.~\ref{tab:model varient charades}, we perform ablation study to further investigate the contribution from each component of our proposed RAE to the whole system. %All the methods in Tab.\ref{tab:model varient} employ the same structure of decoder and differ in the encoder. 
All the model variants are based on the ResNet-152 backbone model and our proposed method achieves the best result in both MSR-VTT dataset and Charades dataset. 
%Compared with the full version of RAE, the proposed method without temporal gate module is decreased by 1.0\% for BLEU@4, 0.1\% for METEOR, 0.8\% for CIDEr and 0.3\% for ROUGE-L. We further remove the extracted feature of residuals in Eq.~\ref{eq:alpha_R}. As can be seen, the performance reduces 0.9\% in BLEU@4, 0.5\% for METEOR, 0.4\% for CIDEr, and 0.7\% for ROUGE-L. The I-frame method is the simplified model of Mean Pool~\cite{venugopalan2014translating} in Tab.~\ref{tab:msr-vtt} with the second layer of the LSTM in decoder removed. We average the extracted features of 20 I-frames to obtain the visual representations, which are fed into LSTM decoder for description generation. It can be observed that our framework performs better than I-frame method by a large margin. The results in 
In MSR-VTT dataset, compared with the full version of RAE, the scores of the proposed method without temporal gate module are decreased. We further remove the extracted feature of residuals in Eq.~\ref{eq:alpha_R} and the performance reduces in terms of all metrics, which demonstrates that the information in residuals can bring performance improvement. These results imply that our proposed SAM and TGM lead to a performance boost. The I-frame method is the simplified model of Mean Pool~\cite{venugopalan2014translating} in Tab.~\ref{tab:msr-vtt} with the second layer of the LSTM in decoder removed. We average the extracted features of 20 I-frames to obtain the visual representations, which are fed into the LSTM decoder for description generation. It can be observed that our framework performs better than I-frame method by a large margin. The results in the Charades dataset also validate the effectiveness of each component in RAE. 


\subsection{Comparison of different methods}
Our method proposes that exploring the saliency information in the compressed videos benefits the generation of video descriptions.
To validate the effectiveness of this method, we compare our video captioning method with the existing methods on the MSR-VTT dataset in Tab~\ref{tab:msr-vtt} and Charades dataset in Tab.~\ref{tab:charades}. R, C, and A denote ResNet-152~\cite{he2016deep}, C3D~\cite{tran2015learning}, and audio~\cite{hershey2017cnn} features, respectively.%To make a fair comparison with these methods, we report the results on differnet kinds of individual feature: Inception-ResNet-V2, ResNet-152, GoogleNet, VGG, C3D, audio features and the combination of them. 
 As can be seen, our method achieves superior performance than previous works on two datasets. We also make a comparison with the methods that rank top-3 in the MSR-VTT Challenge and denote them as v2t\_navigator~\cite{jin2016describing}, Aalto~\cite{shetty2016frame}, and VideoLAB~\cite{ramanishka2016multimodal}. It can be seen that our method achieves the best performance across the three metrics. %We report the results on differnet kinds of individual feature: VGG16, Resnet-18, Resnet-50, Resnet-152, C3D, audio features and the combination of them.%Among the compared methods in previous works, aLSTMs~\cite{gao2017video} keeps the semantic consistence of visual content and description by mapping two kinds of features into a joint space. hLSTMat~\cite{song2017hierarchical} proposes temporal attention to select visual frames for predicting related words and adjusted temporal attention to decide when to use visual information and when to depend on language model. LSTM-GAN~\cite{yang2018video} introduce the concept of adversarial learning into video captioning task. Different from them, our UFVC approach concentrates on leveraging residuals information to attend the salient regions in the I-frame and further generate discriminative visual representations.

  
% \begin{table}[ht]
% 	\small
% 	\renewcommand\arraystretch{1.05}
% 	\centering
% 	\caption{The performance comparison with different features on MSR-VTT dataset.(The best scores are \textbf{bold})}
% \begin{tabular}{l|c|c|c|c}
% \hline
% {Methods}  
% & B@4  & M & C &R\\ \hline
% {UFVC(V)}  & {37.6} & {27.1} & {42.2} & {58.3}\\
% {UFVC(R-18)}  & {36.8} & {26.9} & {42.3} & {57.9}\\
% {UFVC(R-50)}  & {39.9} & {27.6} & {45.0} & {59.4}\\

% \hline
% \end{tabular}%
% \label{tab:msr-vtt model features}%
% \end{table}%


%\subsection{Results on the Charades Dataset}
%In Tab.~\ref{tab:charades}, we show a comparison of our proposed method and previous methods on Charades dataset. As can be seen, our model achieves the best performance in terms of CIDEr, METEOR and ROUGE-L. 

\vspace{0.1cm}
\begin{table}[!h]
\normalsize
	\renewcommand\arraystretch{1.05}
	\centering
	\caption{The performance comparison with the previous methods on MSR-VTT dataset. (The best scores are \textbf{bold})}

\begin{tabular}{l|c|c|c}
\hline
{Methods}  
& BLEU@4  & METEOR & CIDEr \\ 
\hline
v2t\_navigator~\cite{jin2016describing} &40.8 &28.2 &44.8 \\
Aalto~\cite{shetty2016frame} &39.8 &26.9 &45.7 \\
VideoLAB~\cite{ramanishka2016multimodal} &39.1 &27.7 &44.1 \\
\hline
Mean Pool~\cite{venugopalan2014translating} &30.4 &23.7 &35.0 \\


%MP-LSTM&35.7 &25.6 &38.1 &-\\
%LSTM-E&36.1 &25.8 &38.5 &-\\
Attention Fusion~\cite{hori2017attention} &39.7 &25.5 &40.0 \\

%hLSTMat~\cite{song2017hierarchical} &38.3 &26.3 &- &-\\
MA-LSTM~\cite{xu2017learning} &36.5 &26.5 &41.0 \\

%M$^3$~\cite{wang2018m3} &38.1 &26.6 &- &-\\
%LSTM-GAN~\cite{yang2018video} &36.0 &26.1 &- &-\\
%MS-RNN~\cite{song2018deterministic} &39.8 & 26.1 &- &-\\
a-LSTM~\cite{gao2017video} &38.0 &26.1 &43.2 \\
TDDF~\cite{zhang2017task} &37.3 &27.8 &43.8 \\
PickNet~\cite{chen2018less} &41.3 &27.7 &44.1 \\
RecNet~\cite{wang2018reconstruction} &39.1 &26.6 & 42.7 \\
MCNN+MCF~\cite{wu2018multi} &38.1 &27.2 &42.1 \\
STAT~\cite{yan2019stat} &39.3 &27.1 &43.8 \\
DenseLSTM~\cite{zhu2019attention} &38.1 &26.6 &42.8 \\
TDConvED~\cite{chen2019temporal} &39.5 &27.5 &42.8 \\
%OA-BTG~\cite{zhang2019object} &41.4 &28.2 &46.9 &-\\
%MARN~\cite{pei2019memory} &40.4 &28.1 &47.1 &60.7\\
%GRU-EVE~\cite{aafaq2019spatio} &38.3 &28.4 &48.1 &60.7\\

\hline
{VCCV(R)} & {40.0} & {28.0} & {45.7} \\
{VCCV(R + A)}  &{42.2} & {28.8} & {47.2} \\
\textbf{VCCV(R + A + C)}  &\textbf{42.8} & \textbf{29.3} & \textbf{48.7} \\
% {UFVC(R + A)}  &\textbf{43.2} & {28.5} & {47.2} &{61.1}\\
% \textbf{UFVC(R + A + C)}  &{43.0} & \textbf{29.1} & \textbf{48.2} & \textbf{61.3} \\

\hline
\end{tabular}

\label{tab:msr-vtt}
\end{table}

\vspace{0.1cm}

\begin{table}[!h]
    \normalsize
	\renewcommand\arraystretch{1.05}
	\centering
	\caption{The performance comparison with the previous methods on Charades dataset. (The best scores are \textbf{bold})}

	\begin{tabular}{l|c|c|c}
		\hline
		{Methods}  
		%\cline{2-5} 
	    &BLEU@4 &METEOR &CIDEr  \\ \hline
		%\hline
        S2VT~\cite{Venugopalan_2015_ICCV} &11.0 &16.0 &14.0 \\
        SA~\cite{yao2015describing}  &7.6 &14.3 &18.1  \\
		MAAM~\cite{Fakoor2016Memory} &11.5 &17.6 &16.7 \\
		TSA-ED~\cite{wu2018interpretable}  &13.5 &17.8 &20.8  \\
		\hline
		{\textbf{VCCV(R)}} &\textbf{16.5} &\textbf{18.0} &\textbf{21.0}\\ 
	%\textbf{UFVC(R-152 + C)}  & \textbf{39.3} & \underline{27.0} &\textbf{42.8} & -\\
		\hline
	\end{tabular}%

	\label{tab:charades}%
\end{table}



% \begin{table}[ht]
% 	\xiaozihao
% 	\renewcommand\arraystretch{1.05}
% 	\centering
% 	\caption{The performance comparison with the state-of-art methods on MSVD dataset. (The best scores are \textbf{bold})}
% 	%\vspace{0.3cm}
% 	\begin{tabular}{l|c|c|c|c}
% 		\hline
% 		{Methods}  
% 		%\cline{2-5} 
% 	& B@4  & M & C & R\\ \hline
% 	\hline
% 		%\hline
%         %BAE~\cite{baraldi2017hierarchical} &42.5 &32.4 &63.5 &-\\
%         STAT~\cite{tu2017video} & 51.1 & {32.7} & {67.5} & -\\
%         %MA-LSTM~\cite{xu2017learning} & 52.3 & {33.6} & {70.4} & -\\
%         TDDF~\cite{zhang2017task} & 45.8 & {33.3} & {73.0} & 69.7\\
% 		aLSTMs~\cite{gao2017video} & 50.8 & {33.3} & {74.8} & -\\
%         DMRM~\cite{yang2017catching} & 51.1 & {33.6} & {74.8} & -\\
%         %hLSTMat~\cite{song2017hierarchical} & 53.0 & {33.6} & {73.8} & -\\
%         %mGRU~\cite{zhu2017bidirectional} & 53.8 & {34.5} & {81.2} & -\\
% 		%LSTM-GAN~\cite{yang2018video} & {42.9} & {30.4} & {-} & -\\
% 		MCNN+MCF~\cite{wu2018multi}  & {46.5} & {33.7} &{75.5} & -\\
%         PickNet~\cite{chen2018less} &46.1 &33.1 &76.0 &69.2 \\
%         %CAM-RNN~\cite{zhao2019cam} &42.4 &33.4 &54.3 &69.4\\
%         %CGSTI~\cite{xu2019context} &- &31.8 &72.1 &-\\
%         %SVCDV~\cite{qi2019sports} &50.9 &33.5 &70.3 &-\\
%         %STAT~\cite{yan2019stat} & \textbf{52.0} &33.3 &73.8 &-\\
%         %Topic-Guid~\cite{chen2019generating} &49.2 &34.2 &77.6 &71.0 \\
%         %DenseLSTM~\cite{zhu2019attention} &50.4 &32.9 &72.6 &-\\
%         % M$^3$~\cite{wang2018m3} &52.8 &33.3 &- &- \\
% 		%SeqVLAD~\cite{xu2018sequential} & 51.0 & 35.2 & 86.0 & -\\
% 		%\hline
% 		%\hline
% 		%MS-RNN~\cite{song2018deterministic} & 53.3 & 33.8 & 74.8 & -\\
% 		%\hline
% 		%RecNet~\cite{wang2018reconstruction}  & {52.3} &{34.1} & {80.3} &69.8\\
% 		%\hline
% 		%TSA-ED~\cite{wu2018interpretable} &51.7 &34.0 &74.9 &-\\
%         TDConvED~\cite{chen2019temporal} &\textbf{53.3} &33.8 &76.4 &-\\
%         %GRU-EVE~\cite{aafaq2019spatio} &47.9 &\textbf{35.0} &78.1 &71.5 \\
%         %MARN~\cite{pei2019memory} &48.6 & {35.1} & {92.2} & 71.9\\
%         %OA-BTG~\cite{zhang2019object} &56.9 & {36.2} & {90.6} & -\\
% 		%\hline

% 		\hline
% 	%{UFVC(VGG)}  & {39.3} & {27.0} &{42.8} & -\\
% 	%{UFVC(R-18)}  & {39.3} & {27.0} &{42.8} & -\\
% 	%{UFVC(R-50)}  & {39.3} & {27.0} &{42.8} & -\\
% 	{UFVC(R-152)}  &{50.3} &{34.4} &{82.8} &{71.7}\\
% 	{UFVC(R-152 + C)}  &{51.6} & \textbf{34.9} &\textbf{85.0} &\textbf{72.0}\\
	
% 	%\textbf{UFVC(R-152 + C)}  & \textbf{39.3} & \underline{27.0} &\textbf{42.8} & -\\
% 		\hline
% 	\end{tabular}%
% 	\label{tab:msvd}%
% \end{table}



% \begin{table}[ht]
% 	\xiaozihao
% 	\renewcommand\arraystretch{1.05}
% 	\centering
% 	\caption{The performance comparison with the state-of-art methods on MSVD dataset. (The best scores are \textbf{bold})}
% 	%\vspace{0.3cm}
% 	\begin{tabular}{l|c|c|c|c}
% 		\hline
% 		{Methods}  
% 		%\cline{2-5} 
% 	& B@4  & M & C & R\\ \hline
% 	\hline
% 		%\hline
% 		%aLSTMs~\cite{gao2017video} & 50.8 & {33.3} & {74.8} & -\\
% %        STAT~\cite{tu2017video} & 51.1 & {32.7} & {67.5} & -\\	
%         BAE(R+C)~\cite{baraldi2017hierarchical} &42.5 &32.4 &63.5 &-\\
%         MA-LSTM(G+C)~\cite{xu2017learning} & 52.3 & {33.6} & {70.4} & -\\
%         %DMRM~\cite{yang2017catching} & 51.1 & {33.6} & {74.8} & -\\
%         hLSTMat(R)~\cite{song2017hierarchical} & 53.0 & {33.6} & {73.8} & -\\
%         %mGRU~\cite{zhu2017bidirectional} & 53.8 & {34.5} & {81.2} & -\\
%         TDDF(V+C)~\cite{zhang2017task} & 45.8 & {33.3} & {73.0} & 69.7\\
%         PickNet(R)~\cite{chen2018less} &46.1 &33.1 &76.0 &69.2 \\
%         M$^3$(G+C)~\cite{wang2018m3} &52.8 &33.3 &- &- \\
% 		%SeqVLAD~\cite{xu2018sequential} & 51.0 & 35.2 & 86.0 & -\\
% 		%\hline
% 		%LSTM-GAN~\cite{yang2018video} & {42.9} & {30.4} & {-} & -\\
% 		%\hline
% 		%MS-RNN~\cite{song2018deterministic} & 53.3 & 33.8 & 74.8 & -\\
% 		%\hline
% 		%MCNN+MCF~\cite{wu2018multi}  & {46.5} & {33.7} &{75.5} & -\\
% 		%\hline
% 		TSA-ED(R)~\cite{wu2018interpretable} &51.7 &34.0 &74.9 &-\\
% 		%\hline
% 		%TSA-ED ~\cite{wu2018interpretable}  & 51.7 & 34.0 & 74.9  & -\\
%         TDConvED(R)~\cite{chen2019temporal} &53.3 &33.8 &76.4 &-\\
%         GRU-EVE(I+C)~\cite{aafaq2019spatio} &47.9 &35.0 &78.1 &71.5 \\
%         %MARN~\cite{pei2019memory} &48.6 & {35.1} & {92.2} & 71.9\\
%         %OA-BTG~\cite{zhang2019object} &56.9 & {36.2} & {90.6} & -\\
% 		%\hline
% 		%RecNet$_{local}$~\cite{wawng2018reconstruction}  & {38.1} & \textbf{27.2} &  {42.1}\\
% 		\hline
% 	{UFVC(R-152)}  & {39.3} & {27.0} &{42.8} & -\\
	
% 	\textbf{UFVC(R-152 + C)}  & \textbf{39.3} & \underline{27.0} &\textbf{42.8} & -\\
% 		\hline
% 	\end{tabular}%
% 	\label{tab:msvd}%
% \end{table}

% \subsection{Qualitative Analysis}
% We show some representative examples in Fig.~\ref{fig:example results} to demonstrate the effectiveness of our model.

% For an explicit understanding of the performance, some qualitative results of different experimental settings are provided in Fig.~\ref{fig:example results}. From the results, we can see in the given examples that the captions generated by the VCD are better than the model variants. For example, in the first case, the basic method only recognizes that \emph{a person is cooking}, which is not specific for this scene. The VCD can further generate a more specific caption: \emph{a person is frying a pan}. But \emph{frying} and \emph{pan} are mismatched here. Rather, the rethinking network with all the components can describe the content perfectly, i.e. \emph{a person is frying some food}.

\begin{figure*}[!h]
\centering
\includegraphics[width=1.0\linewidth]{figure/qualitative_results.pdf}
\caption{Example results of our model on the MSR-VTT dataset. The generated captions and the ground truth are shown. The corresponding spatial attention maps are generated by our VCCV model and visualized as heatmaps. (Best viewed in color)}

\label{fig:Qualitative}
\end{figure*}

\subsection{Qualitative Analysis}
In Fig.~\ref{fig:Qualitative}, we present some video clips and their corresponding descriptions, both generated and reference. The green sentences are generated by the baseline method (I-frame model in Tab.~\ref{tab:model varient msrvtt} and Tab.~\ref{tab:model varient charades}), while the blue sentences are generated by VCCV. The red sentences are the reference captions labeled by human. From the results, we can see that our model generates relevant captions while attending to the salient regions of the frames. For example, in the case (a), the basic method only recognizes that \emph{a man is wrestling}, which is not specific for this scene. The RAE can further generate a more specific caption: \emph{two men are wrestling in a competition}. It can be seen that the captions generated by the RAE are more closed to the ground truth than that by the baseline method.






% \begin{figure*}[!h]
% \center
% %\section{}
% \includegraphics[width=0.9\linewidth]{figure/supplymentary materials.pdf}
% %\vspace{-1.5cm}
% \caption{Example results of our model on the MSR-VTT dataset. We utilize green, blue, and red color to mark the sentences generated by different methods as in Fig. 3. 
% (Best viewed in color)}
% \label{fig:supplementary}
% \end{figure*}

\section{Conclusion}
In this paper, we propose a deep model for video captioning directly on compressed videos. The proposed video captioning method spots salient regions of I-frames under the assistance of the spatial information in residuals frames. In addition, a gate mechanism named temporal gate module can efficiently alleviate the disturbance caused by some noisy residuals frames and retain the most essential features for caption generation. Extensive experiments conducted on MSR-VTT and Charades show that our method achieves results that rival state-of-the-art methods.

\bibliographystyle{IEEEtran}
\bibliography{IEEEfull}

% \begin{thebibliography}{1}
% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.
% \end{thebibliography}


% that's all folks
\end{document}


