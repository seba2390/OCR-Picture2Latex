%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.

\copyrightyear{2023}
\acmYear{2023}
\setcopyright{acmcopyright}\acmConference[ASPDAC '23]{28th Asia and South Pacific Design Automation Conference}{January 16--19, 2023}{Tokyo, Japan}
\acmBooktitle{28th Asia and South Pacific Design Automation Conference (ASPDAC '23), January 16--19, 2023, Tokyo, Japan}
\acmPrice{15.00}
\acmDOI{10.1145/3566097.3567872}
\acmISBN{978-1-4503-9783-4/23/01}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%=====================================================
% Our added packages
%=====================================================
% \usepackage{hyperref}
\usepackage{multirow}
\usepackage{soul}
\usepackage[skip=0.5\baselineskip]{caption}

\usepackage{tikz}

%\usepackage{cite}
\usepackage{amsmath,amsfonts}
%\usepackage{amssymb}
\usepackage{algorithm,algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


%=====================================================
%% Commands for identifying terms not to be revealed in double-blind review
%=====================================================
\newcommand{\MVU}{MVU}
\newcommand{\MatrixVectorUnit}{Matrix Vector Unit}
\newcommand{\barvinn}{BARVINN}
\newcommand{\BARVINN}{BARVINN}
\newcommand{\PITO}{Pito}
\newcommand{\Pito}{Pito: RISC-V-based Controller}
\newcommand{\pito}{Pito}
% \newcommand{\barrel}{multi-threaded}
\newcommand{\barrel}{barrel}

%=====================================================
%% Change tracking text colouring commands
%=====================================================
\newcommand{\hossein}[1]{{\color{red} #1}}
\newcommand{\hosseinstrike}[1]{{\color{red} \st{#1}}}
\newcommand{\sean}[1]{{\color{blue} #1}}
\newcommand{\yvon}[1]{{\color{green} #1}}
\newcommand{\jp}[1]{{\color{green} #1}}

\usepackage{color, colortbl}
\definecolor{ao}{rgb}{0.0, 0.5, 0.0}
\definecolor{amber}{rgb}{1.0, 0.75, 0.0}

\newcommand{\todo}[1]{{\color{red} #1}}
\newcommand{\inprogress}[1]{{\color{amber} #1}}
\newcommand{\done}[1]{{\color{ao} #1}}

%=====================================================

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{BARVINN: Arbitrary Precision DNN Accelerator Controlled by a RISC-V CPU}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
%\author{Ben Trovato}
%\authornote{Both authors contributed equally to this research.}
%\email{trovato@corporation.com}
%\orcid{1234-5678-9012}
%\author{G.K.M. Tobin}
%\authornotemark[1]
%\email{webmaster@marysville-ohio.com}
%\affiliation{%
%  \institution{Institute for Clarity in Documentation}
%  \streetaddress{P.O. Box 1212}
%  \city{Dublin}
%  \state{Ohio}
%  \country{USA}
%  \postcode{43017-6221}
%}



 \author{Mohammadhossein Askarihemmat\textsuperscript{1}, Sean Wagner\textsuperscript{2}, Olexa Bilaniuk\textsuperscript{3}, \\ Yassine Hariri\textsuperscript{4}, Yvon Savaria\textsuperscript{1}, Jean-Pierre David\textsuperscript{1}}
 \affiliation{%
  \institution{\textsuperscript{1}Ecole Polytechnique Montreal \country{Canada},
  \textsuperscript{2} IBM \city{Toronto}\country{Canada},
  \textsuperscript{3} Mila \city{Montreal}\country{Canada},
  \newline
  \textsuperscript{4} CMC Microsystems \city{Kingston}\country{Canada}}
{\{mohammad.hossein.askari-hemmat, yvon.savaria, jpdavid\}}@polymtl.ca,  wagnerse@ca.ibm.com, olexa.bilaniuk@mila.quebec, hariri@cmc.ca
}
    
 % \author{Sean Wagner}
 % \affiliation{%
 %  \institution{IBM}
 %  \city{Markham}
 %  \state{Ontario}
 %  \country{Canada}
 %  }
 % \email{wagnerse@ca.ibm.com}

 % \author{Olexa Bilaniuk}
 % \affiliation{%
 %  \institution{Mila}
 %  \city{Montreal}
 %  \state{Quebec}
 %  \country{Canada}
 %  }
 % \email{olexa.bilaniuk@mila.quebec}
 

 % \author{Yassine Hariri}
 % \affiliation{%
 %  \institution{CMC Microsystems}
 %  \city{Kingston}
 %  \state{Ontario}
 %  \country{Canada}
 %  }
 % \email{hariri@cmc.ca}

 % \author{Yvon Savaria}
 % \affiliation{%
 %  \institution{Ecole Polytechnique Montreal}
 %  \city{Montreal}
 %  \state{Quebec}
 %  \country{Canada}
 %  }
 % \email{yvon.savaria@polymtl.ca}

 % \author{Jean-Pierre David}
 % \affiliation{%
 %  \institution{Ecole Polytechnique Montreal}
 %  \city{Montreal}
 %  \state{Quebec}
 %  \country{Canada}
 %  }
 % \email{jpdavid@polymtl.ca}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{AskariHemmat et al}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
We present a DNN accelerator that allows inference at arbitrary precision with dedicated processing elements that are configurable at the bit level. Our DNN accelerator has 8 Processing Elements controlled by a RISC-V controller with a combined 8.2 TMACs of computational power when implemented with the  recent Alveo U250 FPGA platform. We develop a code generator tool that ingests CNN models in ONNX format and generates an executable command stream for the RISC-V controller. We demonstrate the scalable throughput of our accelerator by running different DNN kernels and models when different quantization levels are selected. Compared to other low precision accelerators, our accelerator provides run time programmability without hardware reconfiguration and can accelerate DNNs with multiple quantization levels, regardless of the target FPGA size. BARVINN is an open source project and it is available at \href{https://github.com/hossein1387/BARVINN}{https://github.com/hossein1387/BARVINN}.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010583.10010600.10010628.10010629</concept_id>
  <concept_desc>Hardware~Hardware accelerators</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010521.10010542.10010294</concept_id>
  <concept_desc>Computer systems organization~Neural networks</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{neural networks, hardware acceleration, FPGA, low-precision}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Deep neural networks (DNNs) traditionally rely on floating point computations. These operations are slow and costly in terms of power consumption and required silicon area compared to fixed-point/integer operations. One way to accelerate computation in a DNN is to use less precision for computation via quantization \cite{DBLP:journals/corr/HubaraCSEB16}. This also reduces memory consumption as well as energy consumption. For instance, in a 45~nm process, 
%32-bit integer multiplication and addition take 3.1 pJ and 0.1pJ, respectively \cite{IEEE:ISSCC}.
8-bit integer multiplication and addition take 0.2~pJ and 0.03~pJ, respectively, while the same operations with 32-bit floating-point values requires 3.7~pJ for multiplication and 0.9~pJ for addition \cite{IEEE:ISSCC}. On an Intel Core i7 4770 running at 3.4GHz, multiplication is more than 3 times faster for fixed-point compared to floating-point \cite{LIMARE}.
With recent quantization techniques, these benefits are available with little to no loss in model performance and accuracy. In \cite{Esser2020LEARNED,lee2021network}, their quantization schemes showed accuracy losses of 1-3\% at 2-bit precision on most classification and object detection models.
Table \ref{tab:lsq_quant} illustrates the result of applying Learned Scale Quantization (LSQ) \cite{Esser2020LEARNED} with different bit precisions on different models and tasks. Quantized models offer accuracy similar to full precision models, while having smaller size.


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\centering

\caption{Effects of Quantization on Accuracy and Model Size.}

\begin{tabular}{|l|c|l|l|c|c|}
\hline
\multicolumn{1}{|c|}{Task} & Dataset & \multicolumn{1}{c|}{Model} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Precision\\ A/W\end{tabular}} & \begin{tabular}[c]{@{}c@{}}Acc/\\ MAP\end{tabular} & \begin{tabular}[c]{@{}c@{}}Size\\ (MB)\end{tabular} \\ \hline
\multirow{4}{*}{Classification} & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}CIFAR \\ 100\end{tabular}} & \multirow{4}{*}{ResNet18} & LSQ(2/2) & 76.81 & 2.889 \\ \cline{4-6} 
 &  &  & LSQ(4/4) & 76.92 & 5.559 \\ \cline{4-6} 
 &  &  & LSQ(8/8) & 78.45 & 10.87 \\ \cline{4-6} 
 &  &  & FP32 & 76.82 & 42.8 \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Object \\ Detection\end{tabular}} & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}VOC-\\ 2007\end{tabular}} & \multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}SSD300-\\ ResNet18\end{tabular}} & LSQ(2/2) & 0.61 & 10.34 \\ \cline{4-6} 
 &  &  & LSQ(4/4) & 0.60 & 11.81 \\ \cline{4-6} 
 &  &  & LSQ(8/8) & 0.68 & 14.77 \\ \cline{4-6} 
 &  &  & FP32 & 0.59 & 32.49 \\ \hline
%\multirow{4}{*}{Segmentation} & \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Spinal \\ Cord \\ Gray \\ Matter\end{tabular}} & \multirow{4}{*}{U-NET} & LSQ(2/2) & 0.54 & 1.15 \\ \cline{4-6} 
% &  &  & LSQ(4/4) & 0.55 & 2.31 \\ \cline{4-6} 
% &  &  & LSQ(8/8) & 0.57 & 4.62 \\ \cline{4-6} 
% &  &  & FP32 & 0.56 & 18.48 \\ \hline
\end{tabular}
\label{tab:lsq_quant}
\vspace{-1cm}
\end{table}
% Discussion about mixed precision:

%Quantization can trade off performance (especially speed) for network accuracy and memory requirements. 
Mixed-precision quantization \cite{micikevicius2017mixed,8954415,DBLP:conf/iclr/UhlichMCYGTKN20,DBLP:conf/aaai/YuLSH021,Bulat_2021_ICCV} further provides finer control to reach an optimal solution by learning different precisions for each layer of a network. In \cite{8954415}, the authors illustrate that using their mixed-precision framework, they reduced model latency and energy consumption by a factor of almost 2$\times$ with little drop in accuracy compared with an 8-bit quantized model. 
%In some scenarios, the bit precision of the computation can be changed at run time. In \cite{Bulat_2021_ICCV} the authors propose a framework based on mixed-precision quantization that optionally allows dynamically changing bit-width at run time.

Fully benefiting from low-precision in a DNN requires hardware that natively supports low-precision computations. Commodity hardware can  perform arbitrary precision arithmetic by transforming data-layout and computing with bit-wise instructions \cite{10.1145/3368826.3377912}. However, this approach is extremely costly for general processors, because of the overhead for shifting, masking and packing bits to the correct format. At the time of writing this paper, there are no commercially available general processors (CPU or GPU) that can efficiently process data in arbitrary precision.  

In this paper, we propose an arbitrary low-precision DNN hardware accelerator called \BARVINN{}. 
%The purpose of our accelerator is to fill the need for arbitrary precision computation in DNNs. 
Our accelerator is software programmable and can be integrated in the RISC-V standard development flow. It is designed as a highly optimized computational pipeline for DNNs that introduces low hardware overhead and offers low-power operation.
% On the other hand, our accelerator is a parameterized hardware unit and, as we will discuss later, one can add extra processing elements with little resource utilization. 
The contributions of our paper are as follows: 

\begin{itemize}
    \item Implementation of a DNN hardware accelerator with arbitrary fixed-point low-precision for matrix-vector multiply operations at high-throughput and low power.
    \item Implementation of a custom embedded RISC-V CPU to control an array of DNN accelerators by software.
%    \item We propose an FPGA accelerator with software support to accelerate DNNs with different size and precision.
    % \item We propose an efficient run time configurable accelerator that process various DNNs with different sizes, without needing hardware reconfiguration.
    % \item **power consumption FPGA vs GPU **does not seem supported by the current content of the paper.
    \item Data structures for efficiently storing and processing weights and activations for high-throughput serial computation.
    \item Development of a software code generator for transforming DNNs into RISC-V code that executes on our accelerator.
%% SW: REMOVED THIS ITEM
%\item We provide a performance comparison between our accelerator and other FPGA and GPU based accelerators.


\end{itemize}


In section \ref{sec:related_works}, we review relevant DNN  accelerators from the literature. Section \ref{sec:design} presents the architecture of \BARVINN{}. In section \ref{sec:perf_analysis}, a detailed performance analysis of \BARVINN{} is provided and compared with other DNN accelerators. 

\section{Related works}
\label{sec:related_works}

% TODO (SW): Add intro sentences to build case
Several DNN hardware accelerators supporting quantization and low-precision have been presented in recent years for both FPGA and ASIC targets. Here, we discuss the accelerator architectures most relevant to our work.

%\textbf{FINN}\cite{umuroglu2017finn}
%    is an optimized, templated Vivado HLS C++ library of common DNN layers that enables high-throughput, ultra-low latency DNN architectures. It offers a reconfigurable dataflow processing approach by enabling customizable parallelism. Matrix-Vector-Threshold-Unit (MVTU) can be reconfigured by controlling the number of Processing Elements (PEs) and SIMD    units, exploiting trade-offs between footprint and throughput. FINN uses a custom software toolchain to train models for low precision execution supported by optimized HLS libraries to create DNN IPs. Deployment is enabled with a Python runtime while its hardware is synthesized for target FPGAs. However, the out-of-the-box FINN flow requires understanding of FINN's internals to run models outside of the FINN model repository. Furthermore, FINN requires that all DNN layers be implemented in the logic at once. This limits the size of the DNN to the amount of logic resources available on a given FPGA.

Recent FPGA-based accelerators include FINN \cite{umuroglu2017finn, blott2018finnr}, DNNBuilder \cite{zhang2018dnnbuilder}, and FILM-QNN \cite{sun2022film}. In FINN and DNNBuilder, a software toolchain is used to map a trained DNN to generated logic modules that are integrated together. An overall processing pipeline is generated and then synthesized for the target device. The advantage of this approach is that the logic efficiently implements a specific DNN with minimal overhead on a device that can be reconfigured to different DNNs at different times. However, this approach requires that all DNN layers be implemented in the logic all at once, which limits the size of the DNN to the amount of logic resources available on a given FPGA. While FINN supports low-precision down to binary and DNNBuilder down to 4-bit, neither supports arbitrary and mixed-precision at different DNN layers. In contrast, FILM-QNN does support DNN models of arbitrary sizes and quantized DNNs with mixed precisions. However, it is limited to only 4- or 8-bit weights and 5-bit activations due to a bit-packing scheme used with the DSP blocks in the FPGA.

Several ASIC accelerator designs support arbitrary precision. Bit Fusion \cite{sharma2018bit} uses a large array of 2-bit processing elements that can be fused together to perform up to 8-bit operations. Loom \cite{sharify2018loom}, and BitBlade \cite{ryu2019bitblade} employ bit serial computation schemes for added flexibility.
While bit-serial computation of any single math computation (e.g. multiplication) inherently requires additional clock cycles and latency over bit-parallel circuits, these designs exploit the large number of computations in a DNN that can be done in parallel. This is done by implementing a large number of bit-serial computational units operating simultaneously, which  compensates high latency with high throughput. For example, the Loom engine consists of $128 \times 16 = 2048$ Serial Inner-Product units (SIPs), each of which performs 16 $1 \times 1$-bit products per cycle.

%\textbf{Loom}\cite{sharify2018loom} % https://arxiv.org/pdf/1706.07853.pdf
%    is an accelerator for quantized DNNs that is capable of variable-precision inference, in both DNN weights and activations, at the \textit{sub-layer} level. This enables Loom to exploit variability in precision as needed across layers, or to trade bit-precision (and therefore speed) for model accuracy. Loom supports variable precision by exploiting bit-serial execution, rather than a bit-parallel one.
%    A Loom engine consists of $128 \times 16 = 2048$ Serial Inner-Product units (SIPs), each of which performs 16 $1 \times 1$-bit products per cycle. An $m \times n$-bit product requires $mn$ clock cycles, but the small size of the SIPs allows a very large number of them to be synthesized, compensating high latency with high throughput.
    
%\textbf{DNNBuilder}\cite{zhang2018dnnbuilder}
%    is a toolchain that automatically generates efficient FPGA implementations of DNNs from their descriptions in a high-level framework such as TensorFlow. To support a variety of DNNs and FPGAs, DNNBuilder uses a parameterized processing engine (PE) that is tuned under the latency, throughput and resource constraints of the target FPGA. Arbitrary quantization is supported for both weights and activations, and across layers, though only 4-, 8- and 16-bit are demonstrated. DNNBuilder was demonstrated with as an accelerator capable of 4 TOPS at 8-bit precision. Like FINN, the entire DNN is implemented in the generated logic, which limits the size of the DNN that can be fit onto the FPGA.
    
% \textbf{SnowFlake}\cite{zaidy2018snowflake} % https://www.emc2-ai.org/assets/docs/asplos-18/paper1.pdf
%     is a scalable FPGA-hosted accelerator for neural network inference. It is composed of ``compute clusters'' (one only on the Zynq ZC706), each of which contains a memory interface and a RISC-type processor driving 4 ``compute units'', interconnected by a data distribution network. Each such compute unit contains 4 vector multiply-accumulate (vMAC) units performing sixteen $16 \times 16$-bit scalar MACs per clock cycle. A 16KB per-vMAC buffer stores neural-network weights and a 64KB, shared, per-compute-unit buffer stores data vectors. A total of 256 MACs / clock cycle / cluster can be performed. The RISC processor is 5-stage pipelined and orchestrates data movements and computations using instructions from a custom ISA.
%     The SnowFlake design does not support sub-16-bit precision math, preferring to focus on high occupancy of wide MAC units. At 250 MHz, one compute cluster delivers 64 GMACs ($16 \times 16$-bit) with an efficiency of 91-95\% for several well-known CNN architectures, such as AlexNet, GoogleNet and ResNet.
    
%\textbf{RaPID}\cite{venkataramani2021rapid} is a specialized ASIC accelerator targeting ultra-low-precision neural networks. It supports both training and inference, and both floating-point (IEEE FP16, Hybrid-FP8) and fixed-point (2/4-bit) numbers. RaPiD contains a ring bus interconnecting 4 cores, each containing an $8\times8$ systolic array of processing elements. Each PE is equipped with a 128-bit-wide SIMD unit with separate FP and INT pipelines. This separation allows \textit{double-pumping} the INT pipeline to be used for inference, allowing for as much as an 8x increase in throughput and 10.6x higher energy efficiency. When RaPiD is implemented with 4 cores in 7 nm EUV technology, it can be clocked at 1.5GHz, to achieve 12.8 TFLOPS in FP16, 25.6 TFLOPS in HFP8, and 102.4 TOPS in INT4.


\section{Architecture}
\label{sec:design}
%\BARVINN{} is a Barrel RISC-V Neural Network Accelerator.
%The purpose of \barvinn{} is to fill the need for arbitrary precision DNN acceleration of inference workloads. 
\BARVINN{} is designed to provide high-throughput and software programmability, while supporting DNNs of arbitrary size and type.
The high-level architecture of BARVINN is illustrated in Figure \ref{fig:Barvinn_top}.
It consists of the following main components: 1) an array of \MatrixVectorUnit{}s (\MVU{}) \cite{bilaniuk2019bitslice}, and 2) a RISC-V CPU
called \pito{} \cite{9401617}
as a controller for the \MVU{} array. The \MVU{}s accelerate common DNN computations such as GEMV, GEMM, and convolutions along with other operations such as batch normalization, ReLU activation, and quantization. \pito{} coordinates the computations in the \MVU{} array while also handling data transfers to and from the host system. 

\begin{figure*}[!h]
  \centering
    \includegraphics[width=\textwidth, height=10.89cm]{figures/BARVINN_TOP.png}
    \small
\caption{\BARVINN{} hardware architecture with MVU array and Pito RISC-V controller. Right side is MVU detail.}
\label{fig:Barvinn_top}
\vspace{-2mm}
\end{figure*}

As it is not possible to foresee all possible neural networks that may crop up in the literature in the future, high-level sequencing of tensor operations for BARVINN is done in software. 
To control the array of processing elements, unlike the aforementioned accelerators, BARVINN uses the standard RISC-V RV32I ISA. This allows us to leverage the pre-existing software ecosystem. Furthermore, by using a CPU that supports a well known ISA, BARVINN is more flexible and it can be adapted to support new DNN architectures.

%Each component is detailed in the following sections.

% \begin{figure*}[!h]
 %\hspace*{-1.3cm}  
%   \centering
%     \includegraphics[width=1.0\textwidth]{figures/BARVINN_TOP.png}
%     \small
% \caption{ BARVINN overall architecture}
% \label{fig:Barvinn_top}
% \vspace{-2mm}
% \end{figure*}


\subsection{\MatrixVectorUnit{}s}
The base configuration of \barvinn{} is implemented with an array of 8 \MVU{}s. Figure \ref{fig:Barvinn_top} shows each \MVU{} is a 64-element vector pipeline with several modules: a) a Matrix Vector Product unit (MVP), b) RAMs for activations/weights/scalers/biases, c) a scaler unit, d) a pooling/activation unit, and e) a quantizer. \MVU{}s compute 64 output vector elements per clock cycle using a 64 element input data vector from the activation RAM and a 64$\times$64 element matrix from the weight RAM. Activation and weight RAMs store data in low-precision. MVP units operate in low-precision, while subsequent units in the pipeline operate in high-precision fixed-point.

% %% VERTICAL FIGURE
% \begin{figure}[t]
% %\hspace*{-1.3cm}  
%   \centering
%     \includegraphics[width=8.0cm]{figures/mvu_arch_vertical.png}
%     \small
% \caption{The architecture of a \MVU{}.
% %consists of the following computation blocks: a Matrix-Vector Product (MVP), shift-accumulator (shacc), a high-precision multiplier/adder (scaler), a max pooler and a ReLU activation (pool/relu) unit, and quantizer/serializer (quantser). Each computational block is an array of 64 parallel pipelines. 
% Dashed lines indicate bit-serial connections and solid lines indicate high-precision connections. Labels on the connections indicate the bus widths.}
% \label{fig:Barvinn_top}
% \vspace{-2mm}
% \end{figure}

%% HORIZONTAL FIGURE
%\begin{figure*}[!h]
%\hspace*{-1.3cm}  
%  \centering
%    \includegraphics[width=1.0\textwidth]{figures/mvu_arch_v2.png}
%    \small
%\caption{The architecture of a \MVU{}.
%consists of the following computation blocks: a Matrix-Vector Product (MVP), shift-accumulator (shacc), a high-precision multiplier/adder (scaler), a max pooler and a ReLU activation (pool/relu) unit, and quantizer/serializer (quantser). Each computational block is an array of 64 parallel pipelines. 
%Dashed lines indicate bit-serial connections and solid lines indicate high-precision connections. Labels on the connections indicate the bus widths.}
%\label{fig:mvu_arch}
%\vspace{-2mm}
%\end{figure*}

To justify our design choice of operating on 64 element vectors, we analysed over 50 models available at the ONNX Model Zoo \cite{onnx_zoo} to check the input channel size of convolution layers. Figure \ref{fig:ONNX_ZOO_ANALYSIS} illustrates a distribution of input channel size of all layers among those models. We found that 79\% of these models use convolution with input channel sizes that are multiples of 64. 
%Also, based on our analysis and according to Figure \ref{fig:ONNX_ZOO_ANALYSIS}, 
%At least 36\% of layers having channel depths that are not multiples of 64 are either 1 or 3 channels, typically the first convolution layer.


\subsubsection{Matrix-Vector Product Units}

Matrix operations are carried out by the MVP units. They compute on fixed-point arbitrary precision operands from 1- to 16-bit. Each MVP has 64 vector-vector product (VVP) pipelines. Each VVP has 64 input lanes with 1-bit multipliers, followed by an addition tree with 8-bit output, as shown in Figure \ref{fig:vvp_arch}. 
On every cycle, 64 bits from the activation RAM are broadcasted to each of the 64 VVPs, while a 64$\times$64 matrix tile from the weight RAM is read with each row of the tile sent to separate VVPs. The VVPs compute a 64-element dot product on 1-bit operands in each pipeline. With 64 VVPs per MVP, the overall output is a 64-element vector. 

%In the case where the input operands are binary (i.e. 1-bit), an MVP is capable of performing at each clock cycle a binary matrix-vector product of the following size:
%\begin{itemize}
%    \setlength\itemsep{1pt}
%    \item[] Input Vector of $1 \times 64$ with 1 bit precision
%    \item[] Weight Matrix of $64 \times 64$ with 1 bit precision
%    \item[] Output Vector of $1 \times 64$
%\end{itemize}
%Larger vectors/matrices are divided into tiles with dimensions in multiples of 64.

MVPs compute arbitrary bit precision dot-products using the bit-serial scheme of \cite{bilaniuk2019bitslice}. Weights and activations can be unsigned or 2's-complement signed fixed-point. Bit-depth is set independently for both, thus allowing for mixed precision. The bit-serial dot-product, shown in Algorithm \ref{algo:bitserial}, is a multi-cycle sequence starting with the most significant bits (MSB) from 64 elements of the activation and weight tensors. Bits are multiplied in each lane and results are added together across lanes in an addition tree producing an 8-bit dot product. This is added to an accumulator/shifter (see Figure \ref{fig:vvp_arch}). The MSB$\times$MSB result represents the highest order-of-magnitude partial sum of the overall dot product. The MVP then computes the next lower order-of-magnitude partial sum by drawing the needed bit combinations of the operands. When a change in the order-of-magnitude is made, the accumulator is shifted left by 1-bit to align to the order-of-magnitude prior to adding the addition tree output. MVPs are fully pipelined, allowing them to work on different bit combinations at different stages without stalling. The operation completes when the dot products of the least significant bits (LSB) of the operands are computed and accumulated. For $b_w$-bit weights and $b_a$-bit activations, the overall operation takes $b_w b_a$ cycles to compute one tile of the output vector. The precision of the operands is configured separately for each \MVU{}, thus each \MVU{} can process different layers with different bit precisions.

\begin{algorithm}
\caption{Bit-serial dot-product}\label{algo:bitserial}
\begin{algorithmic}[1]
\STATE $b_a$, $b_w$: activation and weight bit precisions
\STATE $x, w$: activation and weight vectors of size $n$
\STATE $j, k$: bit position for activations and weights
\STATE $accumulator \gets 0$
\FOR{$i \gets b_w + b_a$ to $1$}
    \FORALL{$(j,k)$ where $j+k == i$}
        \FOR{$l \gets 0$ to $n-1$}
            \STATE $onebitprod = x_j[l] \times w_k[l]$
            \STATE $accumulator \gets accumulator + onebitprod$
        \ENDFOR
    \ENDFOR
    \item shift $accumulator$ left 1-bit
\ENDFOR
\STATE $output \gets accumulator$
\end{algorithmic}
\end{algorithm}

%% Architecture comparison with other accelerators
Our bit-serial dot product scheme differs from other architectures. The computation scheme in BitFusion is based on computing the individual products of the overall dot-product, that are then summed. This requires a large number of shift-registers to align and sum partial products. \BARVINN{} and BitBlade instead interchange the ordering of the computation such that partial products of the same magnitude from all individual products are computed first and then summed. This reduces the number shifters needed. \BARVINN{} additionally serializes the computation of partial products of different magnitude, requiring only a single fixed shifter and a single adder tree, whereas BitBlade requires 16 variable shifters and 17 adder trees. \BARVINN{} maintains throughput despite this serialized scheme by parallelizing across a wider number of input operands and producing a larger number of output products per clock cycle. BitFusion and BitBlade are further limited to operand sizes 2, 4, and 8-bit, whereas \MVU s in \BARVINN{} and SIPs in Loom support operands of any bit-depth down to 1-bit. However, Loom's data loading scheme restricts the efficiency for general matrix multiply operations when the weight bit depth is below 16, whereas \BARVINN{}s is able to maintain full throughput down to 1-bit.


\begin{figure}[t]
        \centering
            \includegraphics[width=8.0cm]{figures/onnx_zoo_analysis_cropped.png}
        \caption{Channel sizes in models from ONNX Model Zoo.}
        %The majority (79.1\%) of  networks use input channel sizes that are multiple of 64.} 
        \label{fig:ONNX_ZOO_ANALYSIS}
        \vspace{-2.5mm}
\end{figure}

%%this task is by storing values in MSB transposed format in memory. This format of saving data in memory allows \MVU{} to read-only as many words as the operand precision specifies. Since all the computations are happening in this format, the user should not worry about memory layout except when it wants to read results or write inputs (such as input image) into \MVU{} RAMs. To solve this issue, there is a data transposer module that transposes the data to the correct format. Data transposer’s job is to write input data (that is stored in a processor RAM in linear format) into \MVU{} RAM in a transposed format. The input word can be packed with 2, 4, 8 or 16 bits of data. Given the input data precision (prec) the transposer will unpack, transpose and store them in the correct format. Once the \MVU{} word is prepared, data transposer will go into BUSY state in which it will ignore any incoming new input data. At this point, the transposed data will be written into \MVU{} word. Once complete, it will go back into IDLE state and it will wait for a new posedge on start signal to start the process all over again.

\subsubsection{Memories and Data Layout}\label{memorylayout}
\label{subsec:mem_layout}
Activation and weight RAMs store data in a bit-transposed format shown in Figure \ref{fig:bit-trans_mem_layout} to exploit bit-serial computation. When precision is greater than 1 bit, tensor elements are organized in blocks where bits of the same order-of-magnitude are stored in the same memory word starting with the MSBs in the lowest address. A block of $n$ elements with precision $b$ requires $b$ memory words of width $n$. Activation vector elements are in blocks of 64
%to align with the input data port size of the MVP, 
while weights matrix elements are in blocks of 4096 bits in order to load a 64$\times$64 matrix tile.
%to align with the MVP weight port size. 
A transposer module transforms input data from the host into the needed bit-transposed format. Transposition is only needed on the first layer of a DNN since \MVU{}s write back to activation RAM in the bit-transposed format. Weights are pre-processed by a toolchain on the host and loaded into weights RAMs in the expected bit-transposed format.

\begin{figure}[!h]
%\hspace*{-1.3cm}  
  \centering
    \includegraphics[width=8.0cm]{figures/bit-trans_mem_layout.png}
    \small
\caption{Bit-transposed data format for arbitrary precision.}
\label{fig:bit-trans_mem_layout}
\vspace{-2mm}
\end{figure}

The layout of the tensors in the RAMs depends on the operation to be performed. For GEMV, activations are organized as vectors with blocks of 64 elements, while weight matrices are organized as a set of 64$\times$64 tiles. For 2D convolutions, layout of activations is $NHWC$, where the channel dimension $C$ is the innermost dimension, followed by width $W$, height $H$, and batch size $N$. The $C$ dimension is the innermost since several common DNNs such as ResNet typically have hidden layer channel depths that are powers of 2, and hence align to the 64 input lanes of the VVPs. When there are more than 64 channels, the first 64 channels are stored in the first block, the second 64 channels are stored into the second block and so on. As an example, an input tensor of [N=1, H=8, W=8, C=256] with 2-bit precision, will have 4 channel blocks, each block will have 64 rows of 2 by 64-bit elements.

Our weight tensor memory layout for 2D convolutions is designed to support efficient execution by interleaving the input channel dimension $C_i$ and output channel dimension $C_o$. Each weight memory word contains 64 subsets from the $C_o$ dimension, with each subset containing 64 elements from the $C_i$ dimension. A contiguous block of $b_w$ words that stores a complete set of bits for the needed weight precision is referred to as a channel block $C_b$. The layout for 2D convolution weights is $C_{o,s}F_HF_WC_b$, where $C_{o,s} = C_o/64$ are output channel sets, and the kernel size is $(F_W,F_H)$. 
%This allows a single \MVU{} to compute 64 output channels in parallel efficiently.
%As shown in Figure \ref{fig:conv2d_filter_mem_layout}, 

% NOTE: can chop out if need space
%\begin{figure}[!h]
%\hspace*{-1.3cm}  
%  \centering
%    \includegraphics[width=8.0cm]{figures/conv2d_filter_mem_layout.png}
%    \small
%\caption{Memory layout for 2D convolution filter weights in the bit-transposed format. Filters have a kernel size $(F_W,F_H)$, $C_i$ input channels and $C_o$ output channels that are interleaved into $C_b$ channel blocks and $C_{o,s}$ channel block sets.}
%\label{fig:conv2d_filter_mem_layout}
%\vspace{-2mm}
%\end{figure}
 % end \sean{}
 
 % SW - Paragraph on banked memory
 % Note (SW): wrote, but then removed this discussion. The design as of writing the paper did not implement the banked memory and conflict detection mechanisms as intended. While changes are being made to the design as of 2022-10, it is not ready and would affect the synthesis results. So, to not invalidate the resource usage table, this paragraph is removed for now and may be used in a future update publication.
 %\sean{The activation RAM is organized as a set of 32 dual-ported memory banks. This allows for endpoints to read/write to this memory simultaneously so long as there is no conflict in the banks being accessed. Activation memory can be access through interfaces to the \MVU{} interconnect, the \pito{} controller, and the \MVU{} itself. To resolve conflicts, conflict detection units for read and write (\emph{CDRU} and \emph{CDWU}) select which interface has priority.}


\subsubsection{Job Configuration and Execution}

\MVU{}s are programmed to perform jobs such as GEMV or Conv2D operation. A controller sets configuration registers that orchestrate the sequence of calculations and memory reads to complete an operation in the \MVU{}s. %A job is started with the controller raising the start signal. 
Once the job is finished, the \MVU{} will generate an interrupt to the controller, indicating that the job is finished and results are ready to be sent back to the host or to trigger subsequent operations on the same \MVU{} or other \MVU{}s. While a \MVU{} is busy, it can be programmed to prepare the next job to minimize idle time. 

%\begin{figure}[!h]
%\hspace*{-1.3cm}  
%  \centering
%    \includegraphics[width=8.5cm]{figures/mvu_job_config.png}
%    \small
%\caption{Timing diagram for configuring and running an \MVU{} job. For sake of brevity, all configuration parameters are represented by \emph{configs} signal.}
%\label{fig:mvu_job_config}
%\vspace{-2mm}
%\end{figure}

% ***REVISE THIS SENTENCE- What is the Key set of parameters **** 
%A key set of parameters, such as data precision and memory offsets,  configure the
Each \MVU{} contains address generation units (AGU) that drive the memory access pattern across the activation and weight RAMs. 
%\MVU{}s have one AGU for the activations and one AGU for the weights.
The access pattern is managed by a set of up to five nested loops with parameters setting the number of iterations and the forward or backward address jumps to make on each iteration. The address jump scheme reduces the logic to a set of small accumulators to control the loops and small adders to compute addresses. Innermost loops are usually set to stride over the bit depth of the activations and weights. Outer loops are used to iterate over the bit combinations for the serial dot-product procedure and over the dimensions of the tensors. 
%Each \MVU{} also computes memory offsets to point to the needed bit positions as the programmed operation iterates over the various bit combinations. 
For GEMV, two nested loops are required for both activations and weights. Conv2D operations are programmed to compute one row of the output activation map per job, requiring four nested loops.

%\sean{The memory layout described in the previous section allows \MVU{}s to efficiently compute matrix multiplications between input vectors and the weight matrices. However, in large convolutional neural networks, many matrix multiplications must be performed. 
%One of the most common ways to perform convolution is to slide the weight tensor over input. Figure\ref{fig:slide_window} illustrates this operation.
%\begin{figure}[!h]
%\hspace*{-1.3cm}  
%  \centering
%    \includegraphics[width=8.5cm]{figures/slide_window_valid.png}
%    \small
%\caption{ Timing diagram for configuring an \MVU{} job}
%\label{fig:slide_window}
%\vspace{-2mm}
%\end{figure}
%As you can see in Figure\ref{fig:slide_window}, if we just slide the weight tensor over input, not all dot products are valid. Luckily, 
%For a given stride, padding and weight shape, we pre-compute the patterns of memory accesses by the \MVU{} to compute an operation such as GEMV or convolution. 
%Each \MVU{} includes addresses generators that can be programmed to implement a series of nested loops that can be used to move across the input data and weight tensors. Addresses generators have a set of length parameters that set the bounds of each nested loop, and a set of associated address jump parameters that are used to compute the next memory address to move to in a given loop.
%} % end \sean{}


\begin{figure}[t]
    \hspace*{-1.3cm}  
      \centering
        \includegraphics[width=7.0cm]{figures/vvp_arch.png}
        \small
    \caption{VVP unit with a shifter-accumulator. Bit $j$ from 64 elements of the activation tensor $x$ and bit $k$ from 64 elements of the weight tensor $w$ are input in a bit-serial fashion. Note that some input bits and layers of the 5-deep adder tree are not shown.}
    %Multipliers produce 1-bit products and the addition tree produces an 8-bit sum at each cycle.}
    \label{fig:vvp_arch}
    \vspace{-4mm}
\end{figure}

\subsubsection{Pipeline Modules}
\label{sec:Pipeline Modules}
Each \MVU{} has modules downstream from the MVP to implement other DNN operations including a multiplier/adder unit, a pooling/ReLU unit, and a quantizer/serializer unit. These modules operate at high-precision. Fixed-point multiplier/adder units (\emph{Scaler} in Figure \ref{fig:Barvinn_top}), compute DNN operations such as batch normalization and quantization scaling as in LSQ \cite{Esser2020LEARNED}. Scalers multiply the MVP output by a 16-bit operand sourced 
%either from a configuration register ($scaler_b$ in Figure \ref{fig:mvu_arch}) or 
from the scaler RAM. In an FPGA, the multiplier is $27\times16$, which aligns with the port widths of on-chip fixed DSP units. An adder that follows adds 32-bit fixed-point bias terms from bias RAM. Scaler and bias RAMs have independent AGUs. The module that follows combines max pooling and ReLU (\emph{Pool/ReLU} in Figure \ref{fig:Barvinn_top}), implemented as a comparator with an internal register. For ReLU, the incoming value is checked against the register initially set to 0. The combined MaxPool/ReLU is implemented by programming  \MVU s to produce data in the sequence needed for a MaxPool window.

The pipeline ends at the quantization/serialization unit (\emph{QuantSer} in Figure \ref{fig:Barvinn_top}). It takes 32-bit fixed-point data from each of the 64 data paths and serializes them into 64 1-bit outputs. It is programmed to set the output bit-depth and the MSB position from the input word. Combined with scaler units, this is used to implement quantization schemes such as LSQ \cite{Esser2020LEARNED}. Serialized outputs of each datapath are grouped into a single 64-bit word that is sent either to the activation RAM of the same \MVU{}, or to a different \MVU{} via an interconnect.

\subsubsection{Interconnect}

\MVU{}s can send data to each other via an interconnect implemented as an 8-way crossbar switch with broadcast capability. A source \MVU{} is programmed to send its output results in a serialized fashion to a given address in the activation memory of a destination \MVU{}(s). At a destination \MVU{}, a fixed-priority arbitration scheme to the write port of the target MVU activation RAM is used. The interconnect is given highest priority, followed by the controller, then lastly the \MVU{} itself. When multiple \MVU{}s attempt to write to the same destination \MVU{}, a fixed priority scheme determines which \MVU{} can write to its memory.


\subsubsection{DNN Mapping}
% TODO (SW): doing laps to support larger networks

%The \MVU{}s in the array can be programmed to work on different tasks within a DNN model with different configurations, or be programmed to work together to compute a single task. In the former case, 
Each \MVU{} can be assigned to different layers of a DNN, such as convolutions and fully-connected layers. 
%In the latter case,
Alternatively, a single layer can be split between multiple \MVU{}s with each \MVU{}s processing a subset of the input activations and/or weights. Partial results are forwarded from one \MVU{} to another via the interconnect to process subsequent layers of the network, thus creating an overall processing pipeline through the array. By sending partial results from one \MVU{} to another, subsequent \MVU{}s can begin processing as soon as sufficient data has been received from previous layers. For instance, a \MVU{} processing a $3\times3$ convolution requires only 3 rows of  activations from the previous layer to produce one output row of the layer it is processing. This avoids the need to wait until all outputs from a layer are generated, which reduces latency and idle time. Furthermore, the ability to immediately process partial layer outputs by subsequent \MVU{}s keeps on-chip storage requirements low, since only the partial set of activations required to produce the next layer partial output needs to be stored.
%, as opposed to storing the complete output activations of the previous layer. 
% \begin{figure*}[!h]
%   \centering
%     \includegraphics[height=8cm]{figures/MVU_Compute_Modes.pdf}
%     \small
% \caption{The }
% \label{fig:mvu_arch}
% \vspace{-2mm}
% \end{figure*}

Depending on the performance goal, \BARVINN{} can execute a DNN in either Pipelined mode or Distributed mode.
In Pipelined mode (Figure \ref{fig:mvu_laps}.a), the \MVU{} array can process up to 8 convolutions and fully-connected layers all at once. Each \MVU{} can be configured to use different precisions. In cases where a DNN model contains more than 8 layers, the \MVU{} array can be programmed to process the entire model by dividing it into subsets of up to 8 layers each. Each \MVU{} can be loaded with weights from layers in each subset, either all from the start of processing if there is sufficient weight memory available in each \MVU{} or on-the-fly from external memory if not. Output activations from the last \MVU{} in the chain can also be stored temporarily in off-chip memory and fetched later in the case where the first \MVU{} is still processing data from the current lap. 
In the Distributed mode, to minimize latency, the objective is to process single batch inputs as fast as possible. As can be seen in Figure \ref{fig:mvu_laps}.b, in this mode, the computation of a single layer is broken into 8 independent computation regions. All \MVU{}s will be programmed to share the same set of weights. To make sure an \MVU{} computation is independent of those performed on other  \MVU{}s, the user might need to copy the input regions that are shared between computation units. The programmability of \BARVINN{} allows the user to mix and match these execution modes for different layers and models to achieve highest performance.


\begin{figure}[!h]
%\hspace*{-1.3cm}  
  \centering
  \begin{tikzpicture}

    \draw (0, 0) node[inner sep=0] {\includegraphics[width=8.5cm]{figures/MVU_Compute_Modes.png}};
    \draw (-2, -3.6) node {a. Pipelined mode};
    \draw (1.5, -3.6) node {b. Distributed mode};


\end{tikzpicture}
    \small
\caption{Execution flow of a DNN on the \MVU{} array in Pipelined (a) and Distributed (b) modes. In Pipelined mode each \MVU{} processes one layer at a time. In distributed mode, the computation of a single layer is distributed among multiple \MVU{}s.}
\label{fig:mvu_laps}
\vspace{-4mm}
\end{figure}

%The execution patterns supporting different DNN models can become complex. Hence, coordination of jobs issued to the \MVU{}s is carried out in software by the \Pito{}.
 % end \sean

\subsection{\Pito{}}

To make use of \MVU{}s for neural networks, a control unit is required. The controller is a \barrel{} RISC-V processor designed to control the 8 \MVU{}s using separate but communicating hardware threads (harts) that each manage their respective \MVU{}s. DNN layers are executed either in distributed or in a pipelined fashion, depending on whether the DNN is compiled to maximize throughput or minimize latency. This design allows \MVU{}s to complete tensor operations independently of each other. The drawback is that it requires 8 microprocessors to execute the 8 programs. We instead amortized the fixed costs of the processor by adopting \barrel{} processing. With a 8-way threaded processor, we may assign one thread to control each of the \MVU{}s. Because every thread comes up for execution only every 8 clock cycles, the five pipeline stages (fetch, decode, execute, data read \& writes and commit) can be completely hidden. Branch prediction units are unnecessary. Since tensor operations can require hundreds of cycles to execute on a \MVU{}, the \barrel{} processor can fully turn over dozens of times in the interim, allowing each thread to issue the next command to its \MVU{} in a few instructions.

%A barrel processor is a form of a fine-grain multithreading processor that exploits thread-level parallelism by switching between different threads on each clock cycle (Hennessey and Patterson,2011). The aim is to maximize the overall utilization of the processor’s resources, and instruction throughput. This is similar to the technique of simultaneous multi-threading (SMT) that is used in modern superscalar processors. However, unlike SMT superscalar processors, barrel processors do not issue more than one instruction per clock cycle. Instead, a single execution pipeline is shared by all threads. 
%
We adopted a Harvard architecture and divided the instruction and data RAM, 8KB each, and shared between all harts. This gives a 1K word space to store data and instructions to control each \MVU{}. The processor executes instructions following compilation order and without any further scheduling. A hart scheduler provides access to the required resources for the hart at each stage. In the fetch stage, each hart loads instructions from the instruction RAM. The program counter (PC) and register file for each hart is different and the hart scheduler indicates which register should be accessed at a given time. The Decode stage decodes instruction and loads source registers or an immediate operand. Our RISC-V controller is compatible with RV32I RISC-V ISA with minimal support for privilege specification to make Control and Status Registers (CSRs) and Interrupts available to interface with the \MVU{} array. In addition to the base CSRs, we have added 74 \MVU{}-specific CSRs to allow software to control the processing element array. These CSRs control different settings within an MVU such as weight and activation precision, AGU's jump settings, input, weight and output memory address and pipeline module selection as described in \ref{sec:Pipeline Modules}. 


% In Section ***section number instead of title *** Examples, we have provided example codes that show how to program these CSRs to submit a job to \MVU{}.

% \begin{figure}[!h]
% %\hspace*{-1.3cm}  
%   \centering
%     \includegraphics[width=6.0cm]{figures/sw_stack.png}
%     \small
% \caption{BARVINN software stack. Our code generator will take in a model described in ONNX and it will generate }
% \label{fig:BARVINN_SW_STACK}
% %\vspace{-2mm}
% \end{figure}


\subsection{Code Generator}

\BARVINN{} performs GEMM/GEMV, Convolutions, Maxpooling and activation (ReLU). However, it is up to the user to sequence the operations within a DNN with software. To facilitate this, we developed a code generator that takes a DNN described in ONNX \cite{onnx} and configuration settings (weight/input/output precision), and  generates RISC-V code for each operation. The code generator exports weights to the bit-transposed format described in section \ref{subsec:mem_layout}. Since each \MVU{} works on 64-bit words, the code generator tiles each weight tensor in blocks of 64$\times$64. When this cannot be done (either tensor input channel or output channel is not a multiple of 64), we pad the corresponding tile. Currently, our code generator does not apply graph optimization techniques. Also, for now, our code generator supports Pipelined mode execution. In the following section, we used our code generator to map PyTorch models to micro kernel codes which can then be directly used by \barvinn{}.


\section{Performance Analysis and Results}
\label{sec:perf_analysis}
%We analyzed the performance of BARVINN under different DNN kernels. We provide such analysis across different design parameters. Finally, we will demonstrate performance analysis of running modern machine learning models on BARVINN.


%\subsection{Convolution And Matrix Multiplication }
%The convolution operation is a compute bound kernel. 
%The performance of running convolutions on \barvinn{} using different precision is shown in Figure \ref{fig:BARVINN_CONV_PERF}. The input tensor size is 1$\times$64$\times$32$\times$32, with padding of 1 and a stride of 1. For a 3$\times$3 kernel, a typical scalar processor requires 580,608 multiplications and 516,096 additions to compute this convolution. Assuming a modern processor performs a multiply accumulation in a single instruction and requires only one cycle to complete, a scalar processor requires 580,608 cycles to complete this task. In \barvinn{}, \ref{fig:BARVINN_CONV_PERF}, a single \MVU{} performs this task in only 32,400 clock cycles when 2 bit precision is used for both weights and activations. For lower latency applications, the \MVU{} array can be programmed to compute this kernel in only 4050 clock cycles. 

%\begin{figure}[!h]
%    \centering
%        \includegraphics[width=8.0cm]{figures/KernelvsPerf.png}
%        \small
%        \caption{Number of cycles for a 2D convolution over different weight/activation precision \sean{using one \MVU{}}. Each surface corresponds to kernel size. Input tensor size is 1$\times$64$\times$32$\times$32.}
%        \label{fig:BARVINN_CONV_PERF}
%        \vspace{-2mm}
%\end{figure}

%While this analysis omits other factors such as memory transfers, cache miss rates, etc., it shows that \barvinn{} performs low bit-count operations in much lower number of cycles. Another factor to take into consideration is that these results are applicable to each \MVU{}. With a proper job scheduling, all the \MVU{}s can work simultaneously on bigger kernels or can be pipelined back-to-back to increase throughput.  

%In Section \ref{subsec:mem_layout}, we discussed different memory layout for Convolution and Matrix Multiplication. The difference between running a GEMM/GEMV or Convolution is in the way parameters are stored in RAM and in the way we configure the AGUs. Hence, we observe similar performance for GEMM/GEMV kernels.


% \begin{figure}[!h]
%    \centering
%        \includegraphics[width=8.0cm]{figures/barvinn_perf_per_kernel.png}
%        \small
%        \caption{Number of cycles for a 2D convolution over different weight/activation precision \sean{using one \MVU{}}. Each surface corresponds to kernel size. Input tensor size is 1$\times$64$\times$32$\times$32.}
%        \label{fig:BARVINN_CONV_PERF}
%        \vspace{-2mm}
% \end{figure}



    
% \begin{table}[]
% \caption{Layer information of Resnet9 for CIFAR10 dataset and computation cost on BARVINN. First and last layers are intentionally not quantized and computed on the host; all other layers are quantized with 2 bits for activation and 2 bits for weights. }
% \begin{tabular}{cl|l|l|}
% \hline
% \multicolumn{1}{|c|}{Layer} & \multicolumn{1}{l|}{Input}                & \multicolumn{1}{l|}{Output}               &  Cycles \\ \hline\hline
% \multicolumn{1}{|c|}{conv0}            &  \multicolumn{1}{l|}{{[}1, 3, 32, 32{]}}   & \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  &  N/A      \\ \hline
% \multicolumn{1}{|c|}{conv1}            &  \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  & \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  &  34560  \\ \hline
% \multicolumn{1}{|c|}{conv2}            &   \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  & \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  &  34560  \\ \hline
% \multicolumn{1}{|c|}{conv3}            &   \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  & \multicolumn{1}{l|}{{[}1, 128, 16, 16{]}} &  17280  \\ \hline
% \multicolumn{1}{|c|}{conv4}            &  \multicolumn{1}{l|}{{[}1, 128, 16, 16{]}} & \multicolumn{1}{l|}{{[}1, 128, 16, 16{]}} &  32256  \\ \hline
% \multicolumn{1}{|c|}{conv5}            &  \multicolumn{1}{l|}{{[}1, 128, 16, 16{]}} & \multicolumn{1}{l|}{{[}1, 256, 8, 8{]}}   &  16128  \\ \hline
% \multicolumn{1}{|c|}{conv6}            &  \multicolumn{1}{l|}{{[}1, 256, 8, 8{]}}   & \multicolumn{1}{l|}{{[}1, 256, 8, 8{]}}   &  27648  \\ \hline
% \multicolumn{1}{|c|}{conv7}            & \multicolumn{1}{l|}{{[}1, 256, 8, 8{]}}   & \multicolumn{1}{l|}{{[}1, 512, 4, 4{]}}   &  13824  \\ \hline
% \multicolumn{1}{|c|}{conv8}            &  \multicolumn{1}{l|}{{[}1, 512, 4, 4{]}}   & \multicolumn{1}{l|}{{[}1, 512, 4, 4{]}}   &  18432  \\ \hline
% \multicolumn{1}{|c|}{fc}            &  \multicolumn{1}{l|}{{[}1, 512, 4, 4{]}}   & \multicolumn{1}{l|}{{[}1, 10{]}}          &  N/A      \\ \hline
% \multicolumn{1}{l}{}        &                                           & Total:           & 194688 \\ \cline{3-4} 
% \end{tabular}
% \label{tab:resnet9_perf}
% \end{table}


%\subsection{ BARVINN Performance Analysis Over  Non-Native Channel Size}
%\subsection{Non-Native Channel Size}    

%\barvinn{} expects weight tensors to be in tiles of 64$\times$64. In case where the code generator encounters a layer that is either too small for tiling or does not fully fit within a tile of 64$\times$64, the code generator automatically injects padding to the corresponding kernel. For activations, the code generator takes the additional padding into account.


\subsection{Experimental Setup}
\label{sec:pef_neural_barvinn}
To illustrate the performance of \barvinn{}, we chose the ResNet9 image classifier model for the CIFAR10 dataset. We trained and quantized a ResNet9 model on CIFAR10 using LSQ \cite{Esser2020LEARNED} and used the residual distillation \cite{NEURIPS2020_657b96f0} technique to remove shortcut connections (Plain CNN models). In many image classification DNN models such as ResNet \cite{He_2016_CVPR}, the input to the first layer typically consists of less than 64 channels. Furthermore, due to sensitivity of the first and last layer to information loss, most state-of-the-art compression and quantization methods do not apply optimization on input and output layers \cite{Esser2020LEARNED}, hence keeping these layers untouched and in full precision. We have adopted the same technique to compute first and last layers on the host or on the RISC-V controller.

Table \ref{tab:resnet9_quant} shows the performance of ResNet9 on CIFAR10 in the PyTorch framework. Once we were satisfied with the performance of our quantized model, we exported the trained model to ONNX and then used our code generator.
%to generate kernel codes for \barvinn{}. 
Table \ref{tab:resnet9_perf} illustrates the per layer computation cost of running ResNet9 on \barvinn{} with 2-bit activations and weights. All convolutions use a padding of 1. As discussed before, we skipped running the first and last layer on \barvinn{} and we kept them in their original format. The overall computation takes 194,688 cycles to complete. 

Our design was written in Verilog and synthesized using Xilinx Vivado 2021.1 
for the Xilinx Alveo U250 accelerator card. 
Synthesis results for the  RISC-V controller, the processing array, and the accelerator are presented in Table~\ref{tab:fpga_impl}. 
Power consumption was estimated using the software tools in Vivado.

% In the next section, we compare the result of running this model on other accelerators.



\begin{table}[]
\centering

\caption{ResNet9 with different bit precision on CIFAR10}
\begin{tabular}{|l|l|l|l|}
\hline
ResNet9 Model                  & Precision & Accuracy & Size (Bytes)\\ \hline\hline
Original       & Fp32      &     90.8\%     & 19605141\\ \hline
Plain-CNN & Fp32      &       91.1\%   & 18912487\\ \hline
Quantized Plain-CNN  & Int2      & 89.2\% & 1181360\\ \hline
\end{tabular}
\label{tab:resnet9_quant}
\end{table}

% OLD TABLE (SW)
\iffalse
\begin{table*}[]
\caption{Layer information of ResNet9 for CIFAR10 dataset and computation cost on BARVINN. The first and last layers are intentionally not quantized; all other layers are quantized with 2 bits for activation and 2 bits for weights.}
\begin{tabular}{clllllll|l|l|}
\hline
\multicolumn{1}{|c|}{Layer} & \multicolumn{1}{l|}{Layer Name}            & \multicolumn{1}{l|}{Type}   & \multicolumn{1}{l|}{Input}                & \multicolumn{1}{l|}{Output}               & \multicolumn{1}{l|}{Kernel}               & \multicolumn{1}{l|}{Stride}     & Padding          & Precision (W, A, O)        & Cycles \\ \hline\hline
\multicolumn{1}{|c|}{0}            & \multicolumn{1}{l|}{conv1.weight}          & \multicolumn{1}{l|}{conv}   & \multicolumn{1}{l|}{{[}1, 3, 32, 32{]}}   & \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  & \multicolumn{1}{l|}{{[}64, 3, 3, 3{]}}    & \multicolumn{1}{l|}{1} & 1 & {[}32, 32, 32{]} & 0      \\ \hline
\multicolumn{1}{|c|}{1}            & \multicolumn{1}{l|}{layer1.0.conv1.weight} & \multicolumn{1}{l|}{conv}   & \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  & \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  & \multicolumn{1}{l|}{{[}64, 64, 3, 3{]}}   & \multicolumn{1}{l|}{1} & 1 & {[}2, 2, 2{]}    & 34560  \\ \hline
\multicolumn{1}{|c|}{2}            & \multicolumn{1}{l|}{layer1.0.conv2.weight} & \multicolumn{1}{l|}{conv}   & \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  & \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  & \multicolumn{1}{l|}{{[}64, 64, 3, 3{]}}   & \multicolumn{1}{l|}{1} & 1 & {[}2, 2, 2{]}    & 34560  \\ \hline
\multicolumn{1}{|c|}{3}            & \multicolumn{1}{l|}{layer2.0.conv1.weight} & \multicolumn{1}{l|}{conv}   & \multicolumn{1}{l|}{{[}1, 64, 32, 32{]}}  & \multicolumn{1}{l|}{{[}1, 128, 16, 16{]}} & \multicolumn{1}{l|}{{[}128, 64, 3, 3{]}}  & \multicolumn{1}{l|}{1} & 1 & {[}2, 2, 2{]}    & 17280  \\ \hline
\multicolumn{1}{|c|}{4}            & \multicolumn{1}{l|}{layer2.0.conv2.weight} & \multicolumn{1}{l|}{conv}   & \multicolumn{1}{l|}{{[}1, 128, 16, 16{]}} & \multicolumn{1}{l|}{{[}1, 128, 16, 16{]}} & \multicolumn{1}{l|}{{[}128, 128, 3, 3{]}} & \multicolumn{1}{l|}{1} & 1 & {[}2, 2, 2{]}    & 32256  \\ \hline
\multicolumn{1}{|c|}{5}            & \multicolumn{1}{l|}{layer3.0.conv1.weight} & \multicolumn{1}{l|}{conv}   & \multicolumn{1}{l|}{{[}1, 128, 16, 16{]}} & \multicolumn{1}{l|}{{[}1, 256, 8, 8{]}}   & \multicolumn{1}{l|}{{[}256, 128, 3, 3{]}} & \multicolumn{1}{l|}{1} & 1 & {[}2, 2, 2{]}    & 16128  \\ \hline
\multicolumn{1}{|c|}{6}            & \multicolumn{1}{l|}{layer3.0.conv2.weight} & \multicolumn{1}{l|}{conv}   & \multicolumn{1}{l|}{{[}1, 256, 8, 8{]}}   & \multicolumn{1}{l|}{{[}1, 256, 8, 8{]}}   & \multicolumn{1}{l|}{{[}256, 256, 3, 3{]}} & \multicolumn{1}{l|}{1} & 1 & {[}2, 2, 2{]}    & 27648  \\ \hline
\multicolumn{1}{|c|}{7}            & \multicolumn{1}{l|}{layer4.0.conv1.weight} & \multicolumn{1}{l|}{conv}   & \multicolumn{1}{l|}{{[}1, 256, 8, 8{]}}   & \multicolumn{1}{l|}{{[}1, 512, 4, 4{]}}   & \multicolumn{1}{l|}{{[}512, 256, 3, 3{]}} & \multicolumn{1}{l|}{1} & 1 & {[}2, 2, 2{]}    & 13824  \\ \hline
\multicolumn{1}{|c|}{8}            & \multicolumn{1}{l|}{layer4.0.conv2.weight} & \multicolumn{1}{l|}{conv}   & \multicolumn{1}{l|}{{[}1, 512, 4, 4{]}}   & \multicolumn{1}{l|}{{[}1, 512, 4, 4{]}}   & \multicolumn{1}{l|}{{[}512, 512, 3, 3{]}} & \multicolumn{1}{l|}{1} & 1 & {[}2, 2, 2{]}    & 18432  \\ \hline
\multicolumn{1}{|c|}{9}            & \multicolumn{1}{l|}{fc.weight}             & \multicolumn{1}{l|}{matmul} & \multicolumn{1}{l|}{{[}1, 512, 4, 4{]}}   & \multicolumn{1}{l|}{{[}1, 10{]}}          & \multicolumn{1}{l|}{(10, 512)}            & \multicolumn{1}{l|}{N/A}        & N/A              & {[}32, 32, 32{]} & 0      \\ \hline
\multicolumn{1}{l}{}               &                                            &                             &                                           &                                           &                                           &                                 &                  & Total:           & 194688 \\ \cline{9-10} 
\end{tabular}
\label{tab:resnet9_perf}
\end{table*}
\fi

% NEW TABLE

\begin{table}[]
\centering

    \caption{ResNet9 layers for CIFAR10 dataset and computation cost.  All layers are quantized to 2-bit for activation and weights, except for the first and last layers.}
    \begin{tabular}{lll|l|l|}
    \hline
    \multicolumn{1}{|l|}{Layer} & \multicolumn{1}{l|}{Input}                & Kernel               & Output               & Cycles \\ \hline\hline
    \multicolumn{1}{|l|}{conv0} & \multicolumn{1}{l|}{{[}3, 32, 32{]}}   & {[}64, 3, 3, 3{]}    & {[}64, 32, 32{]}  & N/A    \\ \hline
    \multicolumn{1}{|l|}{conv1} & \multicolumn{1}{l|}{{[}64, 32, 32{]}}  & {[}64, 64, 3, 3{]}   & {[}64, 32, 32{]}  & 34560  \\ \hline
    \multicolumn{1}{|l|}{conv2} & \multicolumn{1}{l|}{{[}64, 32, 32{]}}  & {[}64, 64, 3, 3{]}   & {[}64, 32, 32{]}  & 34560  \\ \hline
    \multicolumn{1}{|l|}{conv3} & \multicolumn{1}{l|}{{[}64, 32, 32{]}}  & {[}128, 64, 3, 3{]}  & {[}128, 16, 16{]} & 17280  \\ \hline
    \multicolumn{1}{|l|}{conv4} & \multicolumn{1}{l|}{{[}128, 16, 16{]}} & {[}128, 128, 3, 3{]} & {[}128, 8, 8{]}   & 32256  \\ \hline
    \multicolumn{1}{|l|}{conv5} & \multicolumn{1}{l|}{{[}128, 8, 8{]}}   & {[}256, 128, 3, 3{]} & {[}128, 8, 8{]}   & 16128  \\ \hline
    \multicolumn{1}{|l|}{conv6} & \multicolumn{1}{l|}{{[}128, 8, 8{]}}   & {[}256, 256, 3, 3{]} & {[}256, 4, 4{]}   & 27648  \\ \hline
    \multicolumn{1}{|l|}{conv7} & \multicolumn{1}{l|}{{[}256, 4, 4{]}}   & {[}512, 256, 3, 3{]} & {[}256, 4, 4{]}   & 13824  \\ \hline
    \multicolumn{1}{|l|}{conv8} & \multicolumn{1}{l|}{{[}256, 4, 4{]}}   & {[}512, 512, 3, 3{]} & {[}512, 4, 4{]}   & 18432  \\ \hline
    \multicolumn{1}{|l|}{fc}    & \multicolumn{1}{l|}{{[}512, 4, 4{]}}   & {[}10, 512{]}        & {[}10{]}           & N/A    \\ \hline
                                &                                           &                      & Total:               & 194688 \\ \cline{4-5} 
    \end{tabular}
    \label{tab:resnet9_perf}
\vspace{-0.5cm}
\end{table}


\subsection{ Discussion  }

We compared \BARVINN{} with FINN \cite{umuroglu2017finn}, which is a templated Vivado HLS C++ library of common DNN layers. Like \BARVINN{}, FINN can generate hardware for arbitrary precision, but is not software programmable. Hence, once the FINN hardware is generated, the user cannot change the computation data stream. We attempted to compare the performance of \BARVINN{} with FINN using the ResNet9 model we used earlier. However, at the time of writing, FINN supports simple linear topologies and we were not able to get performance metrics for our model. Instead, we used the available CIFAR10-CNV model from the FINN repository that was tuned for the FINN dataflow for our comparison. Table \ref{tab:finn_vs_barvinn} shows the performance of \BARVINN{} and FINN. For this experiment, we used different precisions for weights and activation. For both tools, we used the performance estimation numbers for frames per second (FPS). For FINN, we used the default folding configurations publicly available in FINN-example repository \cite{finn-example}. As illustrated in Table \ref{tab:finn_vs_barvinn}, we provide 7-15 times better throughput
%. However, since our accelerator architecture is fixed, this throughput is provided 
albeit with higher LUT usage. On the other hand, for higher bit precisions, FINN provides a better FPS/LUT, suggesting a scalable solution for bigger models. 

We also compared the performance on a ResNet-50 model. Table \ref{tab:resnet_perf} shows our estimated FPS for \BARVINN{} executing in Pipelined mode along with reported performance for FINN \cite{finn-example} synthesized for the Xilinx U250 and for FILM-QNN \cite{sun2022film} synthesized for the Xilinx ZCU102 FPGA. While FINN has the highest FPS, \BARVINN{} shows the best performance per Watt.
According to the FINN-example repository \cite{finn-example}, a fine-tuned ResNet50 model, requires more than 87\% of Alveo U250 accelerator's resources. This shows the limits of FINN dealing with bigger models. \BARVINN{} requires the same LUT usage regardless of the model size and bit-width. 

% We compared the performance of \barvinn{} with Nvidia Jetson Nano board. We used the \textit{onnxruntime-gpu} Python package for model execution. Note that we were unable to run the CNV model on the Jetson Nano board since it uses custom operations that are not available for our target board. Instead, we used the ResNet9 model that from Section \ref{sec:pef_neural_barvinn}.  After 1000 runs, an average FPS of 120.83 on batch size of 1 with an average power consumption of 6.6 W and 18.3 frames per watt was recorded. With \barvinn{}, we achieved an FPS of 31001.9 and 1937.6 for 2 and 8 bits respectively, resulting in 1441.6 and 90.1 frames per watt respectively. With the aforementioned throughput and power consumption, we conclude that our accelerator has better throughput per watt than programmable devices, such as the Jetson Nano board.

% FINN can produce double the frame per second throughput that Barwinn provides. However, it uses 87\% of the available resources on an Alveo U250 board. This indicates that FINN produces a highly parallel computation flow. Although this approach results in a better throughput, in our opinion, it will not scale for larger models. In comparison, our accelerator can scale better for larger models. Our accelerator uses only 26\% of the available resources with half the throughput. However, by adding two more PEs to our PE array, we get better throughput with much less resource utilization. 



% \begin{table}[]
% \caption{Estimated Performance of Running Different Neural Networks Using FINN Vs Our Framework On Alveo U250 Board}
% \begin{tabular}{l|l|l|l|l|l|l|l|}
% \cline{2-8}
% & Model    & FPS    & kLUT      & \begin{tabular}[c]{@{}l@{}}BRAM\\ 18K\end{tabular} & DSP  & Prec & \begin{tabular}[c]{@{}l@{}}Freq \\ MHz\end{tabular} \\ \hline
% \multicolumn{1}{|l|}{\multirow{2}{*}{Ours}} & Resnet50 & 2491   & 350172  &  & 512  & 1/2  & 250   \\ \cline{2-8}
% \multicolumn{1}{|l|}{}                      & MBV1     & 70     & 350172  &  & 512  & 4/4  & 250    \\ \hline
% \multicolumn{1}{|l|}{\multirow{2}{*}{FINN}} & Resnet50 & 4428   & 1171875 & 2884   & 1536 & 1/2 & 250 \\\cline{2-8} 
% \multicolumn{1}{|l|}{}                      & MBV1     & 2987 & 615807  & 1231     & 96   & 4/4 & 333 \\ \hline
% \end{tabular}
% \label{tab:finn_vs_barvinn}
% \end{table}


% \begin{table}[]
% \caption{Estimated performance of running FINN's cnv model on cifar Vs our framework on Alveo U250 board when different precision is used.}
% \begin{tabular}{l|l|l|l|l|l|l|}
% \cline{2-7}
%  & \#PEs & kLUT & \begin{tabular}[c]{@{}l@{}}BRAM\\ 18K\end{tabular} & DSP & \begin{tabular}[c]{@{}l@{}}Freq\\ MHZ\end{tabular} & FPS \\ \hline
% \multicolumn{1}{|l|}{\multirow{2}{*}{Ours}} & 8 & 350.2 (26.1\%) &  & 512 & 250 & 2491 \\ \cline{2-7} 
% \multicolumn{1}{|l|}{} & 10 & 395.1 (29.46\%) &  & 640 & 250 & 4982 \\ \hline
% \multicolumn{1}{|l|}{FINN} & N/A & 1172 (87.38\%) & 2884 & 2884 & 333 & 4428 \\ \hline
% \end{tabular}
% \label{tab:finn_vs_barvinn}
% \end{table}

\begin{table}[]
\caption{Post-synthesis resource utilization of \barvinn{}.}
\begin{tabular}{|l|c|c|c|}
\hline
Resource    & \pito{} RISC-V     & \MVU{} Array       & Overall  \\ \hline\hline
LUT         & 10454    &  190625   &       201079    \\ \hline
BRAM      & 15       &    1312     &      1327      \\ \hline
% FF          &  537647   &   116699  &            \\ \hline
% IO          &  11373    &   36602   &            \\ \hline
DSP         &   0       & 512       &      512      \\ \hline
Dynamic Power          &  0.410 W    &   21.066 W  &      21.504 W      \\ \hline
Frequency          &  250 MHz    &   250 MHz  & 250 MHz    \\ \hline
\end{tabular}
\label{tab:fpga_impl}
\end{table}


\begin{table}[]
\centering
\caption{Estimated performance of running CNV model on CIFAR10 on Alveo U250 when different bit precision is used.}
\begin{tabular}{l|c|r|r|r|r|r|}
\cline{2-7}
 & \begin{tabular}[c]{@{}l@{}}Bits\\ (W/A)\end{tabular} & kLUT & \begin{tabular}[c]{@{}l@{}}BRAM\end{tabular} & DSP & FPS & \begin{tabular}[c]{@{}l@{}}FPS/\\ kLUT\end{tabular} \\ \hline 
\multicolumn{1}{|l|}{\multirow{3}{*}{Ours}} & 1/1 & 201.1 (15.0\%) & 1327 & 512 & 61035 & 303.5\\ \cline{2-7} 
\multicolumn{1}{|l|}{} & 1/2 & 201.1 (15.0\%)  & 1327 & 512 & 30517 & 151.7 \\ \cline{2-7} 
\multicolumn{1}{|l|}{} & 2/2 & 201.1 (15.0\%)  & 1327 & 512 & 15258 & 75.8\\ \hline
\multicolumn{1}{|l|}{\multirow{3}{*}{FINN}} & 1/1 & 28.2 (2.1\%) & 150 & 0 & 7716 &  273.6\\ \cline{2-7} 
\multicolumn{1}{|l|}{} & 1/2 & 19.8(1.47\%) & 103 & 0 & 2170 & 109.6\\ \cline{2-7} 
\multicolumn{1}{|l|}{} & 2/2 & 24.3(1.81\%) & 202 & 0 & 2170 & 89.3\\ \hline
\end{tabular}
\label{tab:finn_vs_barvinn}
\end{table}

\begin{table}[]
\centering
\caption{Performance for ResNet-50 model on ImageNet.}
\begin{tabular}{l|c|r|r|r|}
\cline{2-5}
 & \begin{tabular}[c]{@{}l@{}}Bits (W/A)\end{tabular} &  Clock Freq. & FPS & \begin{tabular}[c]{@{}l@{}}FPS/Watt\end{tabular} \\ \hline 
\multicolumn{1}{|l|}{\multirow{1}{*}{Ours}} & 1/2 & 250 MHz & 2296 & 106.8 \\ \cline{1-5} 
\multicolumn{1}{|l|}{\multirow{1}{*}{FINN-R \cite{finn-example}\cite{blott2018finnr}}} & 1/2 & 178 MHz & 2873 & 41.0 \\ \cline{1-5}
\multicolumn{1}{|l|}{\multirow{1}{*}{FILM-QNN \cite{sun2022film}}} & 4(8)/5 & 150 MHz & 109 & 8.4 \\ \cline{1-5} 
\end{tabular}
\label{tab:resnet_perf}
\vspace{-0.2cm}
\end{table}

% \subsection{Comparison with FINN}
    
\section{Conclusion}
\label{sec:conclusion}
In this paper, we presented an FPGA-based DNN accelerator that supports arbitrary bit precision computations. We tested the performance of \BARVINN{} over different DNN kernels and models with different bit precision. For model deployment, we developed a code generator tool that takes in a model in ONNX format and generates RISC-V assembly code for the controller. Compared to other low precision accelerators, we provide a programmable solution which makes \BARVINN{} more flexible.  With the programmable \MVU s, the user can run different models regardless of their size. \BARVINN{} allows trading off throughput and latency by running DNN layers either in distributed or in pipeline modes. Unlike other low precision accelerators, our proposed solution offers implementing various trade-offs through software and the end user can control them for each individual layer without FPGA reconfiguration at run time. Compared to programmable accelerators, \BARVINN{} was shown to provide a better throughput per Watt performance. 

% BARVINN consists of an array of Matrix-Vector Units and a RISC-V CPU as a controller. For model deployment, we have developed a code generator tool that takes in a model in ONNX format and generates RISC-V assembly code for the controller. We tested the performance of BARVINN over different machine learning kernels and model with different bit precision. 

% ***I find the conclusion short and somewhat light and weak. I recommend enriching the conclusion with a qualitive-quantitative results that shows the advantages of the solution from this paper. Comparabe throuput with less resources and enhanced scalability/flexilibity ...**


% \section*{Acknowledgements}

% %%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
The authors acknowledge support for this project from the IBM AI Horizons Network, CMC Microsystems, Fonds de Recherche du Quebec–Nature et Technologies (FRQNT), MITACS and from the NSERC COHESA Strategic Research Network.

\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}


%%
%% If your work has an appendix, this is the place to put it.
%\appendix


% \newpage

% \textbf{TODO:}
% \\
% \todo{To Do}\\
% \inprogress{In Progress}\\
% \done{Done}

% \begin{enumerate}
%     \item \done{Add BARVINN to the title (email ASP-DAC)}
%     \item \done{Can we add new pages (our names is enough to spill to 7th page)-> ask Yvon to pay}
%     \item \done{Add comparison to FILM-QNN}
%     \item \done{Figure 4, split it into overall architecture (without colors and execution flow) and another figure with MVU array execution modes with color.}
%     \item \done{Add citation for Pito and MVU}
%     \item \done{Add a sentence to show why CPU is needed : 1-for fine grain control (Reviewer 3), 2-adapt to new DNN architecture }
%     \item \done{For VVP architecture, we need at least one extra page.}
%     \item \inprogress{(Optional) Add comparison to other bit-serial architecture.}
%     \item \done{Add link to github page.}
%     \item \done{Add Acknowledgment}
%     \item \done{Make sure Pito can run at 250Mhz}
%     \item \done{Look into the FPS number for Resnet50 }
    
% \end{enumerate}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
