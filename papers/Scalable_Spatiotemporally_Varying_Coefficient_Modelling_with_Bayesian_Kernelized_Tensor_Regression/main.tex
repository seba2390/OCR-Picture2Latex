\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
%\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%




\usepackage{amsmath,amsthm}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{subfigure}
%\usepackage{subfig}
\graphicspath{{graphics/}}
% \usepackage{algorithm}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
% \SetKwInOut{Initialization}{}
\usepackage{algorithmic}
\usepackage{tabularx,booktabs}
\usepackage{tikz}
\usepackage{xcolor}
\definecolor{darkblue}{rgb}{0.0,0.5,0.5}
%\usepackage[colorlinks]{hyperref}
%\hypersetup{colorlinks,breaklinks,linkcolor=blue,urlcolor=blue,anchorcolor=blue,citecolor=blue}
\usepackage{booktabs,caption}
\usepackage{multirow}
\usepackage{lscape}
% \usepackage{subcaption}
\usepackage{epstopdf}
\usepackage{lineno}
\usepackage{subfigure}

\newcommand{\given}{\;\middle|\;}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bd}[1]{\boldsymbol{#1}}





\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if0\blind
{
  \title{\bf Scalable Spatiotemporally Varying Coefficient modeling with Bayesian Kernelized Tensor Regression}
  \author{Mengying Lei,
        Aur\'elie Labbe,
        Lijun Sun 
    \thanks{Mengying Lei and Lijun Sun (Corresponding author) are with the Department of Civil Engineering, McGill University, Montreal, Quebec, H3A 0C3, Canada. E-mail: mengying.lei@mail.mcgill.ca (Mengying Lei), lijun.sun@mcgill.ca (Lijun Sun). Aur\'elie Labbe is with the Department of Decision Sciences, HEC Montr\'eal, Montreal, Quebec, H3T 2A7, Canada. E-mail: aurelie.labbe@hec.ca.}
    }
  \date{}
  \maketitle
} \fi

\if1\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Title}
\end{center}
  \medskip
} \fi

% \bigskip


% {\color{red}
% \begin{itemize}
%     \item highlight, misassigned data
%     \item highlight, relationship with linear model of coregionalization
%     \item highlight, relationship with spatial factor model, and spatiotemporal factor model from Lopes
%     \item use different hyperparameters for each component
%     \item use better probabilistic score, CRPS, Interval Score, etc
% \end{itemize}
% }


\begin{abstract}
As a regression technique in spatial statistics, the spatiotemporally varying coefficient model (STVC) is an important tool for discovering nonstationary and interpretable response-covariate associations over both space and time. However, it is difficult to apply STVC for large-scale spatiotemporal analyses due to its high computational cost. To address this challenge, we summarize the spatiotemporally varying coefficients using a third-order tensor structure and propose to reformulate the spatiotemporally varying coefficient model as a special low-rank tensor regression problem. The low-rank decomposition can effectively model the global patterns of the large data sets with a substantially reduced number of parameters. To further incorporate the local spatiotemporal dependencies, we use Gaussian process (GP) priors on the spatial and temporal factor matrices. We refer to the overall framework as Bayesian Kernelized Tensor Regression (BKTR). For model inference, we develop an efficient Markov chain Monte Carlo (MCMC) algorithm, which uses Gibbs sampling to update factor matrices and slice sampling to update kernel hyperparameters. We conduct extensive experiments on both synthetic and real-world data sets, and our results confirm the superior performance and efficiency of BKTR for model estimation and parameter inference.
\end{abstract}

\noindent%
{\it Keywords:} Gaussian process; Tensor regression; Bayesian framework; Multivariate spatiotemporal processes; Spatiotemporal modeling
\vfill

% \newpage
\spacingset{1.5} % DON'T change the spacing!
\section{Introduction}
\label{sec:intro}

Local spatial regression aims to characterize the nonstationary and heterogeneous associations between the response variable and corresponding covariates observed in a spatial domain \citep{banerjee2014hierarchical,cressie2015statistics}. This is achieved by assuming that the regression coefficients vary locally over space. Local spatial regression offers enhanced interpretability of complex relationships, and has become an important technique in many fields, such as geography, ecology, economics, environment, public health and climate science, to name but a few. In general, a local spatial regression model for a scalar response $y$ can be written as:
\begin{equation}\label{eq:svc}
    y(\boldsymbol{s}) = \bd{x}(\boldsymbol{s})^{\top} \bd{\beta}(\bd{s})+ \epsilon(\bd{s}),
\end{equation}
where $\bd{s}$ is the index (e.g., longitude and latitude) for a spatial location, $\bd{x}(\bd{s})\in\mathbb{R}^P$ and $\bd{\beta}(\bd{s})\in\R^{P}$ are the covariate vector and the regression coefficients at location $\bd{s}$, respectively, and $\epsilon(\bd{s})\sim \text{i.i.d.}\,\mathcal{N}(0,\tau^{-1})$ is a white noise process with precision $\tau$.




There are two common methods for local spatial regression analysis---the Bayesian spatially varying coefficient model (SVC) \citep{gelfand2003spatial} and the geographically weighted regression (GWR) \citep{fotheringham2003geographically}. SVC is a Bayesian hierarchical model where regression coefficients are modelled using Gaussian processes (GP) with a kernel function to be learned \citep{rasmussen2006}. For a collection of $M$ observed locations, the original SVC 
 developed by \citet{gelfand2003spatial} imposes a prior such that $\text{vec}(\bd{\beta}_{\text{mat}}^{\top})\sim \mathcal{N} (\bd{1}_{M\times 1}\otimes\bd{\mu}_{\beta}, \bd{K}_s\otimes \bd{\Lambda}^{-1})$, where $\bd{\beta}_{\text{mat}}$ is a $M\times P$ matrix of all coefficients, $\text{vec}(\bd{X})$ denotes vectorization by stacking all columns in $\bd{X}$ as a vector, $\bd{\mu}_{\beta}$ represents the overall regression coefficient vector used to constructed the mean, 
 $\bd{K}_s$ is a $M\times M$ spatial correlation matrix, $\bd{\Lambda}$ is a $P\times P$ precision matrix for covariates, and $\otimes$ denotes the Kronecker product. 
 In this paper, for simplicity, we use a zero-mean GP to specify $\bd{\beta}$, and the global effect of covariates can be learned (or removed) through a linear regression term as in \citet{gelfand2003spatial}. Also, setting $\bd{K}_s$ as a correlation matrix simplifies the covariance specification, since the variance can be captured by scaling $\bd{\Lambda}^{-1}$. This formulation is equivalent to having a matrix normal distribution $\bd{\beta}_{\text{mat}}\sim \mathcal{MN}_{M\times P}\left(\bd{0},\bd{K}_s,\bd{\Lambda}^{-1}\right)$. GWR was developed independently using a local weighted regression approach, in which a bandwidth parameter is used to calculate the weights (based on a weight function) for different observations, with closer observations carrying larger weights. In practice, the bandwidth parameter is either pre-specified based on domain knowledge or tuned through cross-validation. However, it has been shown in the literature that the estimation results of GWR are highly sensitive to the selection of the bandwidth parameter \citep[e.g.,][]{finley2011comparing}. Compared with GWR, the Bayesian hierarchical framework of SVC provides more robust results and allows us to learn the hyperparameters of the spatial kernel $\bd{K}_s$, which is critical to understanding the underlying characteristics of the spatial processes. In addition, by using Markov chain Monte Carlo (MCMC), we can not only explore the posterior distribution of the kernel hyperparameters and regression coefficients, but also perform out-of-sample prediction. 




The formulation in Eq.~\eqref{eq:svc} can be easily extended to local spatiotemporal regression to further characterize the temporal variation of coefficients. For a response matrix  $\boldsymbol{Y}\in\mathbb{R}^{M\times N}$ observed from a set of locations $S=\{\bd{s}_1,\cdots,\bd{s}_M\}$ over a set of time points $T=\{t_1,\cdots,t_N\}$, the local spatiotemporal regression model defined on the Cartesian product $S \times T = \left\{\left(\bd{s}_m, t_n\right) : m =
1,\ldots, M, \  n = 1,\ldots, N\right\}$ can be formulated as:
\begin{equation} \label{Eq:STVC-point}
    y\left(\bd{s}_m,t_n\right)=\boldsymbol{x}\left(\bd{s}_m,t_n\right)^{\top}\boldsymbol{\beta}\left(\bd{s}_m,t_n\right)+\epsilon\left(\bd{s}_m,t_n\right),
\end{equation}
where we use $m=1,\cdots,M$ and $n=1,\cdots,N$ to index rows (i.e., location) and columns (i.e., time point), respectively, $y\left(\bd{s}_m,t_n\right)$ is the $(m,n)$th element in $\bd{Y}$, and $\boldsymbol{x}\left(\bd{s}_m,t_n\right)$  and $\bd{\beta}\left(\bd{s}_m,t_n\right)$ are the covariate vector and coefficient vector at location $\bd{s}_{m}$ and time $t_{n}$, respectively. Based on this formulation, \citet{huang2010geographically} extended GWR to geographically and temporally weighted regression (GTWR) by introducing more parameters to quantify spatiotemporal weights in the local weighted regression. For SVC, \citet{gelfand2003spatial} suggested using a separable kernel structure to build a spatiotemporally varying coefficient model (STVC), which assumes that $\left[\boldsymbol{\beta}\left(\bd{s}_1,t_1\right);\cdots;\bd{\beta}\left(\bd{s}_M,t_1\right);\bd{\beta}\left(\bd{s}_1,t_2\right);\cdots;\boldsymbol{\beta}\left(\bd{s}_M,t_N\right) \right]\sim \mathcal{N}(\bd{0}, \bd{K}_t\otimes \bd{K}_s \otimes \bd{\Lambda}^{-1})$, where $\bd{K}_t$ is a $N\times N$ kernel matrix defining the correlation structure for the $N$ time points. Note that with this GP formulation, it is not necessary for the $N$ time points to be uniformly distributed. If we parameterize the regression coefficients in Eq.~\eqref{Eq:STVC-point} as a third-order tensor $\bd{\mathcal{B}}\in \R^{M\times N\times P}$ with mode-3 fiber  $\bd{\mathcal{B}}(m,n,:)=\boldsymbol{\beta}\left(\bd{s}_m,t_n\right)$, the above specification is equivalent to having a tensor normal distribution $\bd{\cal{B}}\sim \mathcal{TN}_{M\times N\times P}\left(\bd{0},\bd{K}_s,\bd{K}_t,\bd{\Lambda}^{-1} \right)$. % The Kronecker structure in the covariance of $\boldsymbol{\beta}$ enables to learn the kernel hyperparameters in
However, despite the elegant 
separable kernel-based formulation in STVC, the model is rarely used in real-world practice mainly due to the high computational cost. For example, for a fully observed matrix $\bd{Y}$ with corresponding spatiotemporal covariates, updating kernel hyperparameters and the coefficients $\bd{\beta}$ in each MCMC iteration requires time complexity of $\mathcal{O}\left(M^3N^3\right)$ and $\mathcal{O}\left(M^3N^3P^3\right)$, respectively.%, due to the inversion (or Cholesky factorization) of the % $MNP\times MNP$  covariance matrix.


In this paper, we provide an alternative estimation strategy---Bayesian Kernelized Tensor Regression (BKTR)---to perform Bayesian spatiotemporal regression analysis on large-scale data sets. Inspired by the idea of reduced rank regression and tensor regression \cite[e.g.,][]{izenman1975reduced,zhou2013tensor,bahadori2014fast,guhaniyogi2017bayesian}, we use low-rank tensor factorization to encode the dependencies among the three dimensions in $\bd{\mathcal{B}}$ with only a few latent factors. To further incorporate local spatial and temporal dependencies, we use GP priors on the spatial and temporal factor vectors following \citet{luttinen2009variational}, thus translating the default tensor factorization into a kernelized factorization model. With a specified tensor rank $R$, the time complexity becomes $\mathcal{O}\left(R^3\left(M^3+N^3+P^3\right)\right)$, which is substantially reduced compared with the default STVC formulation. In addition to the spatial and temporal framework, we also consider in this paper the case where a  proportion of response matrix $\boldsymbol{Y}$ can be unobserved or corrupted, given observed values of the covariates $\boldsymbol{X}$. Such a scenario is very common in many real-world applications, such as traffic state data collected from emerging crowdsourcing and moving sensing systems (e.g., Google Waze) for example, where observations are inherently sparse in space and time. %sensor malfunctions or network communication errors are common causes of the ``missingness problem''.
We show that the underlying Bayesian tensor decomposition structure allows us to effectively estimate both the model coefficients and the unobserved outcomes even when the missing rate of $\boldsymbol{Y}$ is high. We conduct numerical experiments on both synthetic and real-world data sets, and our results confirm the promising performance of BKTR.





\section{Related Work}
\label{sec:RelatedWork}


% \subsection{Related Work}

The key computational challenge in SVC/STVC is how to efficiently and effectively learn a multivariate spatial/spatiotemporal process (i.e., $\bd{\beta}_{\text{mat}}$ in Eq.~\eqref{eq:svc} and the third-order spatiotemporal tensor $\bd{\cal{B}}$ in Eq.~\eqref{Eq:STVC-point}). For a general multivariate spatial process which is fully observed on the Cartersian product with white noise, a popular approach is to use separable covariance specification on which one can leverage property of Kronecker products to substantially reduce the computational cost \citep{saatcci2012scalable, wilson2014fast}. However, for SVC, we cannot benefit from the Kronecker property directly since the data $\bd{y}$ is obtained through a linear transformation of $\bd{\beta}_{\text{mat}}$. In this case, computing the inverse of an $MP\times MP$ matrix becomes inevitable when sampling those space-varying coefficients. Existing frameworks for SVC essentially adopt a two-step approach \citep{gelfand2003spatial,finley2020bayesian}: (1) update only kernel hyperparameters and $\bd{\mu}$ by marginalizing $\bd{\beta}$ with cost $\mathcal{O}\left(M^3\right)$; and 
(2) after burn-in, use composition sampling on of the obtained MCMC samples to obtain samples for $\bd{\beta}$ with cost $\mathcal{O}\left(M^3P^3\right)$. For STVC, the corresponding costs in the two steps are $\mathcal{O}\left(M^3N^3\right)$ and $\mathcal{O}\left(M^3N^3 P^3\right)$, respectively. The high computational cost in step (2) is the key issue that limits the application of SVC/STVC in practice. 




%in which complex dependencies exist both within and across dimensions. The most popular strategy is to model the variable using a GP with a separable covariance matrix, which has been used in spatiotemporal analysis \cite[e.g.,][]{cressie1999classes,gelfand2003spatial,stein2005space,zhang2007maximum} and multi-output GP \cite[e.g.,][]{bonilla2007multi,alvarez2012kernels}.
%Although computationally it is possible to benefit from the Kronecker structure of the separable covariance by applying singular value decomposition when updating kernel hyperparameters, the property is no longer available when the data contains missing entries 
%The well-known methods % for multivariate/multidimensional spatiotemporal data 
%include the linear model of coregionalization (LMC) \citep{schmidt2003bayesian, gelfand2004nonstationary}, which is effective in modeling nonstationary spatial processes, and spatial factor models, which has been explored and applied by \cite{gamerman2008spatial, ren2013hierarchical}. % {\color{red}his article builds upon the popular linear models of
% coregionalization (Bourgault and Marcotte, 1991; Goulard
% and Voltz, 1992; Wackernagel, 2003; Gelfand et al., 2004;
% Chiles and Delfiner, 2009; Genton and Kleiber, 2015). model with a matrix-normal distribution as a prior for
% an unknown linear transformation on latent spatial
% processes, (ii) extending classes of spatial factor models
% for spatially misaligned data, and (iii) accounting for
% multiple outcomes over very large number of locations.
% Spatial factor models have been explored by Wang and
% Wall (2003), Lopes et al. (2008), Ren and Banerjee (2013),
% and Taylor-Rodriguez et al. (2019). Lopes et al. (2008)
% provide an extensive discussion on how hierarchical
% models emerged from dynamic factor models. Ren and
% Banerjee (2013) proposed low-rank specifications for
% spatially varying factors to achieve dimension reduction,
% but such low-rank specifications tend to oversmooth
% the latent process from massive data sets containing
% millions of locations.}
%In all these models, the cubic time complexity in learning kernel hyperparameters and latent variables becomes the critical bottleneck. Several solutions have been proposed to address this computational challenge in GP regression, such as low-rank approximation for $\bd{\Lambda}^{-1}$ \citep{bonilla2007multi}, using sparse approximation based on inducing points  \citep{quinonero2005unifying,alvarez2008sparse} or compactly supported kernels \citep{luttinen2012efficient} to model $\bd{K}_s$ and $\bd{K}_t$, and fast kernel learning by leveraging the Kronecker product structure  \citep{saatcci2012scalable,wilson2014fast}. However, these approaches cannot be extended directly since the observed data $y$ is no longer a direct sample from the GP.


% {\color{red} missed some GP for Kronecker, like (Wilson) you presented in group meeting.}


Our work follows a different approach. Instead of modeling $\bd{\beta}$ directly using a GP, we parameterize the third-order tensor $\bd{\cal{B}}$ for STVC using a probabilistic low-rank tensor decomposition \citep{kolda2009tensor}. The idea is inspired by recent studies on low-rank tensor regression/learning \cite[see e.g.,][]{zhou2013tensor,bahadori2014fast,rabusseau2016low,yu2016learning,guhaniyogi2017bayesian,yu2018tensor}. The low-rank assumption not only preserves the global patterns and higher-order dependencies in the variable, but also greatly reduces the number of parameters. In fact, without considering the spatiotemporal indices, we can formulate Eq.~\eqref{Eq:STVC-point} as a scalar tensor regression problem \citep{guhaniyogi2017bayesian} by reconstructing each $\bd{x}(\bd{s}_m,t_n)$ as a sparse covariate tensor of the same size as $\bd{\cal{B}}$. However, for spatiotemporal data, the low-rank assumption alone cannot fully characterize the strong local spatial and temporal consistency. To better encode local spatial and temporal consistency, existing studies have introduced graph Laplacian regularization in defining the loss function \cite[e.g.,][]{bahadori2014fast,rao2015collaborative}. Nevertheless, this approach also introduces more parameters (e.g., those used to define distance/similarity function and weights in the loss function) and it has limited power in modeling complex spatial and temporal processes. The most relevant work is a Gaussian process factor analysis model \citep{luttinen2009variational} developed for a completely different problem---spatiotemporal matrix completion, in which different GP priors are assumed on the spatial and temporal factors and the whole model is learned through variational Bayesian inference. \citet{lei2022bayesian} presents an MCMC scheme for this model, in which slice sampling is used to update kernel hyperparameters and Gibbs sampling is used to update factor matrices. We follow a similar idea as in \citet{luttinen2009variational} and \citet{lei2022bayesian} to parameterize the coefficients $\bd{\beta}$ and develop MCMC algorithms for model inference.




\section{Bayesian Kernelized Tensor Regression}
\label{sec:meth}


\subsection{Notations}
Throughout this paper, we use lowercase letters to denote scalars, e.g., $x$, boldface lowercase letters to denote vectors, e.g., $\boldsymbol{x}\in\mathbb{R}^{M}$, and boldface uppercase letters to denote matrices, e.g., $\boldsymbol{X}\in\mathbb{R}^{M\times N}$. The $\ell_{2}$-norm of $\boldsymbol{x}$ is defined as $\|\boldsymbol{x}\|_{2}=\sqrt{\sum_{m}x_{m}^{2}}$. For a matrix $\boldsymbol{X}\in\mathbb{R}^{M\times N}$, we denote its $(m,n)$th entry by $x_{m,n}$. We use $\bd{I}_{N}$ to denote an identity matrix of size $N\times N$. Given two matrices $\boldsymbol{A}\in\mathbb{R}^{M\times N}$ and $\boldsymbol{B}\in\mathbb{R}^{P\times Q}$, the Kronecker product is defined as $\boldsymbol{A}\otimes\boldsymbol{B}=
\footnotesize{\begin{bmatrix}
  a_{1,1}\boldsymbol{B} & \cdots & a_{1,N}\boldsymbol{B} \\
  \vdots & \ddots & \vdots \\
  a_{M,1}\boldsymbol{B} & \cdots & a_{M,N}\boldsymbol{B}
\end{bmatrix}}\in\mathbb{R}^{MP\times NQ}$. If $\boldsymbol{A}=[\boldsymbol{a}_{1},\cdots,\boldsymbol{a}_{N}]$ and $\boldsymbol{B}=[\boldsymbol{b}_{1},\cdots,\boldsymbol{b}_{Q}]$ have the same number of columns, i.e., $N=Q$, then the Khatri-Rao product is defined as the column-wise Kronecker product $\boldsymbol{A}\odot\boldsymbol{B}=[\boldsymbol{a}_{1}\otimes\boldsymbol{b}_{1},\cdots,\boldsymbol{a}_{N}\otimes\boldsymbol{b}_{N}]\in\mathbb{R}^{MP\times N}$. The vectorization $\operatorname{vec}(\bd{X})$ stacks all column vectors in $\bd{X}$ as a single vector. Following the tensor notation in \citet{kolda2009tensor}, we denote a third-order tensor by $\boldsymbol{\mathcal{X}}\in\mathbb{R}^{M\times N\times P}$ and its mode-$k$ $(k=\text{1},\text{2},\text{3})$ unfolding by $\boldsymbol{X}_{(k)}$, which maps a tensor into a matrix. The mode-3 fibers and frontal slices of $\boldsymbol{\mathcal{X}}$ are denoted by $\boldsymbol{\mathcal{X}}(m,n,:)\in\mathbb{R}^{p}$ and $\boldsymbol{\mathcal{X}}(:,:,p)\in\mathbb{R}^{M\times N}$, respectively. Finally, we use $\operatorname{ones}(M,N)$ and $\boldsymbol{1}_{M}\in\mathbb{R}^{M}$ to represent a $M\times N$ matrix and a length $M$ column vector of ones, respectively.


\subsection{Tensor CP Decomposition}
For a third-order tensor $\boldsymbol{\mathcal{A}}\in\mathbb{R}^{M\times N\times P}$, the CANDECOMP/PARAFAC (CP) decomposition factorizes $\boldsymbol{\mathcal{A}}$ into a sum of rank-one tensors \citep{kolda2009tensor}:
\begin{equation} \label{Eq:CP decomposition}
    \boldsymbol{\mathcal{A}}=\sum_{r=1}^{R}\boldsymbol{u}_{r}\circ\boldsymbol{v}_{r}\circ\boldsymbol{w}_{r},
\end{equation}
where $R$ is the CP rank, $\circ$ represents the outer product, $\boldsymbol{u}_{r}\in\mathbb{R}^{M}$, $\boldsymbol{v}_{r}\in\mathbb{R}^{N}$, and $\boldsymbol{w}_{r}\in\mathbb{R}^{P}$ for $r=1,\cdots,R$. The factor matrices that combine the vectors from the rank-one components are denoted by $\boldsymbol{U}=[\boldsymbol{u}_{1},\cdots,\boldsymbol{u}_{R}]\in\mathbb{R}^{M\times R}$, $\boldsymbol{V}\in\mathbb{R}^{N\times R}$, and $\boldsymbol{W}\in\mathbb{R}^{P\times R}$, respectively. We can write Eq.~\eqref{Eq:CP decomposition} in the following matricized form:
\begin{equation} \label{Eq:unfolding-CP}
    \begin{split}
        \boldsymbol{A}_{(1)}=\boldsymbol{U}\left(\boldsymbol{W}\odot\boldsymbol{V}\right)^{\top},~ 
        \boldsymbol{A}_{(2)}=\boldsymbol{V}\left(\boldsymbol{W}\odot\boldsymbol{U}\right)^{\top},~ 
        \boldsymbol{A}_{(3)}=\boldsymbol{W}\left(\boldsymbol{V}\odot\boldsymbol{U}\right)^{\top},
    \end{split}
\end{equation}
which relates the mode-$k$ unfolding of a tensor to its polyadic decomposition. % An example of tensor calculation is given in Supplementary material Section A.
% for better illustration.
% for a $3\times 3\times 2$ tensor
% Figure~\ref{fig:tensor_illu} 

% \begin{figure}[!t]
% \centering
% \includegraphics[width=0.93\textwidth]{graphical/tensor_illu2.pdf}
% \caption{Illustration of tensor calculation for a $3\times 3\times 2$ tensor $\boldsymbol{\mathcal{A}}$. $\boldsymbol{\mathcal{A}}$ is constructed by a rank 2 ($R=2$) CP decomposition based on factor matrices $\boldsymbol{U}$, $\boldsymbol{V}$ and $\boldsymbol{W}$.}
% \label{fig:tensor_illu}
% \end{figure}

\subsection{Model Specification}

Let $\boldsymbol{\mathcal{X}}$ be an $M\times N\times P$ tensor, of which the $(m,n)$th mode-3 fiber is the covariate vector at location $\bd{s}_{m}$ and time $t_n$, i.e., $\boldsymbol{\mathcal{X}}{(m,n,:)}=\boldsymbol{x}\left(\bd{s}_m,t_n\right)$. For example, in the application of spatiotemporal modeling on bike-sharing demand that we illustrate later (see Section~\ref{sec:application}), the response matrix $\boldsymbol{Y}\in\mathbb{R}^{M\times N}$ is a matrix of daily departure trips for $M$ bike stations over $N$ days, and the tensor variable $\boldsymbol{\mathcal{X}}\in\mathbb{R}^{M\times N\times P}$ represents a set of $P$ spatiotemporal covariates for the corresponding locations and time. Using $\boldsymbol{y}\in\mathbb{R}^{MN}$ to denote $\operatorname{vec}(\boldsymbol{Y})$, Eq. (\ref{Eq:STVC-point}) can be formulated as:
\begin{equation} \label{Eq:STVC-vector}
    \boldsymbol{y}=\left(\boldsymbol{I}_{MN}\odot\boldsymbol{X}_{(3)}\right)^{\top}\operatorname{vec}\left(\boldsymbol{B}_{(3)}\right)+\boldsymbol{\epsilon},
\end{equation}
where $\boldsymbol{X}_{(3)}$ and $\boldsymbol{B}_{(3)}$ are the unfoldings of $\boldsymbol{\mathcal{X}}$ and $\boldsymbol{\mathcal{B}}$, respectively, the Khatri-Rao product $\left(\boldsymbol{I}_{MN}\odot\boldsymbol{X}_{(3)}\right)^{\top}$ is a $MN\times MNP$ block diagonal matrix, and $\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0},\tau^{-1}\boldsymbol{I}_{MN})$. Assuming that $\boldsymbol{\mathcal{B}}=\sum_{r=1}^{R}\boldsymbol{u}_{r}\circ\boldsymbol{v}_{r}\circ\boldsymbol{w}_{r}$ admits a CP decomposition with rank $R\ll\min\{M,N\}$, we can rewrite Eq. \eqref{Eq:STVC-vector} as:
\begin{equation} \label{Eq:STVC-decompose}
    \boldsymbol{y}=\tilde{\bd{X}}\operatorname{vec}\left(\boldsymbol{W}(\boldsymbol{V}\odot\boldsymbol{U})^{\top}\right)+\boldsymbol{\epsilon},
\end{equation}
where $\tilde{\bd{X}}=\left(\boldsymbol{I}_{MN}\odot\boldsymbol{X}_{(3)}\right)^{\top}$ denotes the expanded covariate matrix. The number of parameters in \eqref{Eq:STVC-decompose} is $R(M+N+P)$, which is substantially smaller than $M N P$ in \eqref{Eq:STVC-vector}.


Local spatial and temporal processes are critical to the modeling of spatiotemporal data. However, as mentioned, the low-rank assumption alone cannot encode such local dependencies. To address this issue, we assume specific GP priors on $\bd{U}$ and $\bd{V}$, respectively, following the GP factor analysis strategy \citep{luttinen2009variational}, and then develop a fully Bayesian approach to estimate the model in Eq.~\eqref{Eq:STVC-decompose}. Figure~\ref{fig:illustration-BKTL} illustrates the proposed framework, which is referred to as \textit{Bayesian Kernelized Tensor Regression} (BKTR) in the remainder of this paper. The graphical model of BKTR is shown in Figure~\ref{fig:graphical-BKTL}.
% shows the graphical model of the proposed framework, which is referred to as \textit{Bayesian Kernelized Tensor Regression} (BKTR) in the following of this paper. We illustrate the proposed BKTR framework in Figure~\ref{fig:illustration-BKTL}.

\begin{figure}[!t]
\centering
\includegraphics[width=0.78\textwidth]{graphical/framework_illustration2.pdf}
\caption{Illustration of the proposed BKTR framework.}
\label{fig:illustration-BKTL}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.36\textwidth]{graphical/BKTL-graphical.pdf}
\caption{Graphical model of BKTR.}
\label{fig:graphical-BKTL}
\end{figure}



As mentioned in the introduction, in real-world applications the dependent data is often partially observed on a set $\Omega$ of observation indices, with $|\Omega|< MN$. This means that we only observe a subset of entries $y_{m,n}$, for $\forall(s_m,t_m) \in \Omega$.
%Particularly, the missing data problem is a fundamental issue for applications in transportation system, because the traffic data (e.g., traffic speed/demand) is easily missing and corrupted due to many reasons such as sensor malfunctions. 
We denote by  $\boldsymbol{D}\in\mathbb{R}^{M\times N}$ a binary indicator matrix with $d_{m,n}=1$ if $(m,n)\in\Omega$ and $d_{m,n}=0$ otherwise, and denote by $\boldsymbol{O}$ a  binary matrix of $|\Omega|\times MN$ formed by removing the rows corresponding to the zero values in $\operatorname{vec}(\boldsymbol{D})$ from a $MN\times MN$ identity matrix. The vector of observed data can be obtained by
$\boldsymbol{y}_{\Omega}=\boldsymbol{O}\boldsymbol{y}$. Therefore, we have:
\begin{equation} \label{eq:y_omega}
% \small
    \boldsymbol{y}_{\Omega}\sim\mathcal{N}\left(\boldsymbol{O}\left(\tilde{\bd{X}}\operatorname{vec}\left(\boldsymbol{W}(\boldsymbol{V}\odot\boldsymbol{U})^{\top}\right)\right),\tau^{-1}\boldsymbol{I}_{|\Omega|}\right).
\end{equation}



For spatial and temporal factor matrices $\boldsymbol{U}$ and $\boldsymbol{V}$, we use identical GP priors on the component vectors:
\begin{equation} \label{eq:factorUV}
% \small
\begin{aligned}
    \boldsymbol{u}_{r}&\sim\mathcal{GP}\left(\boldsymbol{0},\boldsymbol{K}_{s}\right),~r=1,\cdots,R, \\
    \boldsymbol{v}_{r}&\sim\mathcal{GP}\left(\boldsymbol{0},\boldsymbol{K}_{t}\right),~r=1,\cdots,R, \\
\end{aligned}
\end{equation}
where $\boldsymbol{K}_{s}\in\mathbb{R}^{M\times M}$ and $\boldsymbol{K}_{t}\in\mathbb{R}^{N\times N}$ are the spatial and temporal covariance matrices built from two valid kernel functions $k_{s}(\bd{s}_m,\bd{s}_{m^{\prime}};\phi)$ and $k_{t}(t_n,t_{n^{\prime}};\gamma)$, respectively, with $\phi$ and $\gamma$ being kernel length-scale hyperparameters. Note that we also restrict $\boldsymbol{K}_{s}$ and $\boldsymbol{K}_{t}$ to being correlation matrices by setting the variance to one, and we capture the variance through $\boldsymbol{W}$. We reparameterize the kernel hyperparameters as log-transformed variables to ensure their positivity and assume normal priors on them, i.e., $\log(\phi)\sim\mathcal{N}(\mu_{\phi},\tau_{\phi}^{-1}),~\log(\gamma)\sim\mathcal{N}(\mu_{\gamma},\tau_{\gamma}^{-1})$.
% \begin{equation}
% \begin{aligned}
% \log(\phi)&\sim\mathcal{N}(\mu_{\phi},\tau_{\phi}^{-1}),  \\
% \log(\gamma)&\sim\mathcal{N}(\mu_{\gamma},\tau_{\gamma}^{-1}). \\
% \end{aligned}
% \end{equation}
For the factor matrix $\boldsymbol{W}$, we assume all columns follow an identical zero-mean Gaussian distribution with a conjugate Wishart prior on the precision matrix:
\begin{equation} \label{eq:factorW}
% \small
\begin{aligned}
    \boldsymbol{w}_{r} &\sim\mathcal{N}\left(\boldsymbol{0},\boldsymbol{\Lambda}_{w}^{-1}\right),~r=1,\cdots,R, \\
    \boldsymbol{\Lambda}_{w} &\sim\mathcal{W}\left(\boldsymbol{\Psi}_{0},\nu_{0}\right),
    \end{aligned}
\end{equation}
where $\boldsymbol{\Psi}_{0}$ is a $P\times P$ positive-definite scale matrix and $\nu_{0}>P-1$ denotes the degrees of freedom. Finally, we suppose a conjugate Gamma prior $\tau\sim\text{Gamma}(a_{0},b_{0})$ on the noise precision $\tau$ defined in Eq.~(\ref{eq:y_omega}).





\subsection{Model Inference}
We use Gibbs sampling to estimate the model parameters, including coefficient factors $\{\boldsymbol{U},\boldsymbol{V},\boldsymbol{W}\}$, the precision $\tau$, and the precision matrix $\boldsymbol{\Lambda}_{w}$. For the kernel hyperparameters $\{\phi,\gamma\}$ whose conditional distributions are not easy to sample from, we use the slice sampler. %We next introduce the detailed sampling procedure.


\subsubsection{Sampling the coefficient factor matrices $\{\boldsymbol{U},\boldsymbol{V},\boldsymbol{W}\}$}  

Sampling the factor matrices can be considered as a Bayesian linear regression problem. Taking $\bd{W}$ as an example, we can rewrite Eq.~\eqref{eq:y_omega} as:
\begin{equation} \label{Eq:STVC-sampleW}
    \boldsymbol{y}_{\Omega}\sim \mathcal{N}\left(\boldsymbol{O}\left(\tilde{\bd{X}}\left((\boldsymbol{V}\odot\boldsymbol{U})\otimes\boldsymbol{I}_{P}\right)\operatorname{vec}(\boldsymbol{W})\right),\tau^{-1}\boldsymbol{I}_{|\Omega|}\right),
\end{equation}
where $\bd{U},\bd{V}$ are known and $\operatorname{vec}(\boldsymbol{W})$ is the coefficient to estimate. Considering that the priors of each component vector $\boldsymbol{w}_{r}$ are independent and identical, the prior distribution over the whole vectorized $\boldsymbol{W}$ becomes  $\operatorname{vec}(\boldsymbol{W})\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}_{R}\otimes\boldsymbol{\Lambda}_{w}^{-1})$. Since both likelihood and prior of $\operatorname{vec}(\boldsymbol{W})$ follow Gaussian distributions, its posterior is also Gaussian with mean $\boldsymbol{\mu}_{W}^{*}$ and precision $\boldsymbol{\Lambda}_{W}^{*}$ such as:
\begin{equation} \notag
\begin{aligned}
    \boldsymbol{\Lambda}_{W}^{*}&=\tau\boldsymbol{H}_{W}^{\top}\boldsymbol{H}_{W}+\boldsymbol{\Lambda}_{W},\ \  \boldsymbol{\Lambda}_{W}=\boldsymbol{I}_{R}\otimes\boldsymbol{\Lambda}_{w}, \\
    \boldsymbol{\mu}_{W}^{*}&=\tau(\boldsymbol{\Lambda}_{W}^{*})^{-1}\boldsymbol{H}_{W}^{\top}\boldsymbol{y}_{\Omega}, \\
    \boldsymbol{H}_{W}&=\boldsymbol{O}\left(\tilde{\bd{X}}\left((\boldsymbol{V}\odot\boldsymbol{U})\otimes\boldsymbol{I}_{P}\right)\right).
\end{aligned}
\end{equation}


The posterior distributions of $\boldsymbol{U}$ and $\boldsymbol{V}$ can be obtained similarly using different tensor unfoldings. In order to sample $\boldsymbol{U}$, we use the mode-1 unfolding in Eq.~\eqref{Eq:unfolding-CP} and reconstruct the regression model with $\operatorname{vec}(\bd{U})$ as coefficients: %, such as:
\begin{equation} \label{Eq:YonU}
\boldsymbol{y}_{\Omega}=\boldsymbol{O}\left(\tilde{\bd{X}}_{U}\left((\boldsymbol{W}\odot\boldsymbol{V})\otimes\boldsymbol{I}_{M}\right)\operatorname{vec}(\boldsymbol{U})\right)+\boldsymbol{\epsilon}_{\Omega},
\end{equation}
where {{$\tilde{\bd{X}}_{U}=\left(\boldsymbol{X}_{(3)}\odot\boldsymbol{I}_{MN}\right)^{\top}\in\mathbb{R}^{MN\times MNP}$}} and  $\boldsymbol{\epsilon}_{\Omega}\sim\mathcal{N}\left(\boldsymbol{0},\tau^{-1}\boldsymbol{I}_{|\Omega|}\right)$. Thus, the posterior of $\operatorname{vec}(\boldsymbol{U})$ has a closed form---a Gaussian distribution with mean $\boldsymbol{\mu}_{U}^{*}$ and precision $\boldsymbol{\Lambda}_{U}^{*}$, where
% $ p\left(\operatorname{vec}(\boldsymbol{U})\given - \right)=\mathcal{N}\left(\boldsymbol{\mu}_{U}^{*},(\boldsymbol{\Lambda}_{U}^{*})^{-1}\right)$, where
\begin{equation}\notag
\begin{aligned}
    \boldsymbol{\Lambda}_{U}^{*}&=\tau\boldsymbol{H}_{U}^{\top}\boldsymbol{H}_{U}+\boldsymbol{K}_{U}^{-1}, \boldsymbol{K}_{U}=\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{s}, \\
    \boldsymbol{\mu}_{U}^{*}&=\tau(\boldsymbol{\Lambda}_{U}^{*})^{-1}\boldsymbol{H}_{U}^{\top}\boldsymbol{y}_{\Omega}, \\
    \boldsymbol{H}_{U}&=\boldsymbol{O}\left(\tilde{\bd{X}}_{U}\left((\boldsymbol{W}\odot\boldsymbol{V})\otimes\boldsymbol{I}_{M}\right)\right).
\end{aligned}
\end{equation}
The posterior for  $\operatorname{vec}(\boldsymbol{V})$ can be obtained by applying the mode-2 tensor unfolding. It should be noted that the above derivation provides the posterior for the whole factor matrix, i.e., $\left\{\operatorname{vec}(\bd{U}),\operatorname{vec}(\bd{V}),\operatorname{vec}(\bd{W})\right\}$. We can further reduce the computing cost by sampling $\left\{\bd{u}_r,\boldsymbol{v}_{r},\boldsymbol{w}_{r}\right\}$ one by one for $r=1,\cdots,R$ as in \citet{luttinen2009variational}. In this case, the time complexity in learning these factor matrices can be further reduced to $\mathcal{O}(R(M^3+N^3+P^3))$ at the cost of slow/poor mixing.


\subsubsection{Sampling kernel hyperparameters $\{\phi,\gamma\}$}
As shown in Figure~\ref{fig:graphical-BKTL}, sampling kernel hyperparameters conditional on the factor matrices should be straightforward through the Metropolis-Hastings algorithm. However, in practice, conditioning on the latent variables $\{\boldsymbol{U},\boldsymbol{V}\}$ in such hierarchical GP models usually induces sharply peaked posteriors over $\{\phi,\gamma\}$, making the Markov chains mix slowly and resulting in poor updates \citep{murray2010slice}. To address this issue, we integrate out the latent factors from the model to get the marginal likelihood, and sample $\phi$ and $\gamma$ from their marginal posterior distributions based on the slice sampling approach \citep{neal2003slice}.


% we adopt the reparameterization and slice sampling approach proposed in \citet{murray2010slice} by introducing a set of auxiliary variables as a noisy version of the factor matrices to sample the kernel hyperparameters more efficiently. 


% To sample kernel hyperparameters, we integrate out 


% We illustrate below the sampling process of $\phi$. 

Let's consider for example the hyperparameter of $\boldsymbol{K}_s$, i.e., $\phi$. As $\operatorname{vec}(\boldsymbol{U})\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{K}_{U})$ with  $\boldsymbol{K}_{U}=\boldsymbol{I}_{R}\otimes\boldsymbol{K}_s$, we integrate over $\operatorname{vec}(\boldsymbol{U})$ in Eq.~\eqref{Eq:YonU} and obtain:
\begin{equation}
    \log{p\left(\boldsymbol{y}_{\Omega}\given\phi,\boldsymbol{V},\boldsymbol{W},\tau,\boldsymbol{\mathcal{X}}\right)}=-\frac{1}{2}\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}^{-1}\boldsymbol{y}_{\Omega}-\frac{1}{2}\log{\left|\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}\right|}-\frac{|\Omega|}{2}\log{2\pi},
\end{equation}
% where {\small{$\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}=\boldsymbol{O}\tilde{\bd{X}}_{U}\left((\boldsymbol{W}\odot\boldsymbol{V})\otimes\boldsymbol{I}_{M}\right)\boldsymbol{K}_{U}\left((\boldsymbol{W}\odot\boldsymbol{V})^{\top}\otimes\boldsymbol{I}_{M}\right)\tilde{\boldsymbol{X}}_{U}^{\top}\boldsymbol{O}^{\top}+\tau^{-1}\boldsymbol{I}_{|\Omega|}$}}. 
where $\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}=\boldsymbol{H}_{U}\boldsymbol{K}_{U}\boldsymbol{H}_{U}^{\top}+\tau^{-1}\boldsymbol{I}_{|\Omega|}$ and $\boldsymbol{H}_{U}=\boldsymbol{O}\left(\tilde{\bd{X}}_{U}\left((\boldsymbol{W}\odot\boldsymbol{V})\otimes\boldsymbol{I}_{M}\right)\right)\in\mathbb{R}^{|\Omega|\times MR}$.
% $\boldsymbol{X}_{U}'=\boldsymbol{O}\tilde{\bd{X}}_{U}\left((\boldsymbol{W}\odot\boldsymbol{V})\otimes\boldsymbol{I}_{M}\right)\in\mathbb{R}^{|\Omega|\times MR}$
The marginal posterior of $\phi$ becomes:
% \begin{equation} \label{Eq:Marginal}
% \begin{aligned}
% \small
% \log{p\left(\phi\given\boldsymbol{y}_{\Omega},\boldsymbol{V},\boldsymbol{W},\tau,\boldsymbol{\mathcal{X}}\right)}\propto&\log{p(\phi)}-\frac{1}{2}\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}^{-1}\boldsymbol{y}_{\Omega}-\frac{1}{2}\log{\left|\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}\right|} \\
% \propto&\log{p(\phi)}+\frac{1}{2}\tau^2\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{X}_{U}'\left(\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{s}^{-1}+\tau\left(\boldsymbol{X}_{U}'\right)^{\top}\boldsymbol{X}_{U}'\right)^{-1}(\boldsymbol{X}_{U}')^{\top}\boldsymbol{y}_{\Omega} \\
% &-\frac{1}{2}\log{\left|\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{s}^{-1}+\tau(\boldsymbol{X}_{U}')^{\top}\boldsymbol{X}_{U}'\right|}-\frac{R}{2}\log{\left|\boldsymbol{K}_{s}\right|},
% \end{aligned}
% \end{equation}
\begin{equation} \label{Eq:Marginal}
\begin{aligned}
% \small
\log{p\left(\phi\given\boldsymbol{y}_{\Omega},\boldsymbol{V},\boldsymbol{W},\tau,\boldsymbol{\mathcal{X}}\right)}\propto&\log{p(\phi)}-\frac{1}{2}\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}^{-1}\boldsymbol{y}_{\Omega}-\frac{1}{2}\log{\left|\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}\right|} \\
\propto&\log{p(\phi)}+\frac{1}{2}\tau^2\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{H}_{U}\left(\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{s}^{-1}+\tau\boldsymbol{H}_{U}^{\top}\boldsymbol{H}_{U}\right)^{-1}\boldsymbol{H}_{U}^{\top}\boldsymbol{y}_{\Omega} \\
&-\frac{1}{2}\log{\left|\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{s}^{-1}+\tau\boldsymbol{H}_{U}^{\top}\boldsymbol{H}_{U}\right|}-\frac{R}{2}\log{\left|\boldsymbol{K}_{s}\right|},
\end{aligned}
\end{equation}
where we compute $\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}^{-1}\boldsymbol{y}_{\Omega}$ based on the Woodbury matrix identity, and use the matrix determinant lemma to compute $\log{\left|\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}\right|}$. The detailed derivation is given in Appendix~\ref{appA}. The slice sampling approach is robust to the selection of the sampling scale and easy to implement. Sampling $\gamma$ can be achieved in a similar way. Note that, as mentioned, the sampling is performed on the log-transformed variables to avoid numerical issues. The detailed sampling process for kernel hyperparameters is provided in Appendix~\ref{appB}. % Algorithm~\ref{alg:HyperSample}.
We can also introduce different kernel functions (or the same kernel function with different hyperparameters) for each factor vector $\bd{u}_r$ and $\boldsymbol{v}_{r}$ as in \citet{luttinen2009variational}. In such a case, the marginal posterior of the kernel hyperparameters can be derived in a similar way as in Eq.~\eqref{Eq:Marginal}.



% First, we generate an auxiliary data $\boldsymbol{G}^{u}=[\boldsymbol{g}_{1}^{u},\cdots,\boldsymbol{g}_{R}^{u}]\in\mathbb{R}^{M\times R}$ where each column $\boldsymbol{g}_{r}^{u}\sim\mathcal{N}(\boldsymbol{u}_{r},\alpha\boldsymbol{I}_{M})$, and $\alpha$ is a specified variance. This auxiliary variable can be regarded as a surrogate data of $\boldsymbol{U}$, and the joint distribution of $\boldsymbol{G}^{u}$ and $\boldsymbol{U}$ conditional on $\phi$ can be written as:
% \begin{equation}
% \begin{split}
% p\left(\boldsymbol{U},\boldsymbol{G}^{u}\given\phi\right)&=\prod_{r=1}^R p\left(\boldsymbol{u}_{r}\given\boldsymbol{g}_{r}^{u},\phi\right)p\left(\boldsymbol{g}_{r}^{u}\given\phi\right),\\
% \left.\boldsymbol{g}_{r}^{u}\right|\phi\sim\mathcal{N}&\left(\boldsymbol{0},\alpha\boldsymbol{I}_{M}+\boldsymbol{K}_{s}\right), ~
% \left.\boldsymbol{u}_{r}\right|\boldsymbol{g}_{r}^{u},\phi\sim\mathcal{N}\left(\boldsymbol{\mu}_{r},\boldsymbol{\Sigma}\right), 
% \end{split}
% \end{equation}
% where $\boldsymbol{\mu}_{r}=\alpha^{-1}\boldsymbol{\Sigma}\boldsymbol{g}_{r}^{u}$ and $\boldsymbol{\Sigma}^{-1}=\alpha^{-1}\boldsymbol{I}_{M}+(\boldsymbol{K}_{s})^{-1}$. Next, we simulate a whitened Gaussian variable $\boldsymbol{Q}^{u}=[\boldsymbol{q}_{1}^{u},\cdots,\boldsymbol{q}_{R}^{u}]\in\mathbb{R}^{M\times R}$ with $\boldsymbol{q}_{r}^{u}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}_{M})$. After the Cholesky decomposition $\boldsymbol{\Sigma}=\boldsymbol{L}_{\Sigma}\boldsymbol{L}_{\Sigma}^{\top}$, we can sample $\phi$ conditioning on the transformed whitened variables $\boldsymbol{U}^{\prime}$, where each column is defined as $\boldsymbol{u}_{r}^{\prime}=\boldsymbol{L}_{\Sigma}\boldsymbol{q}_{r}^{u}+\boldsymbol{\mu}_{r}$. Specifically, the posterior distribution of $\phi$ becomes:
% \begin{equation}
%     p\left(\phi\given\boldsymbol{y}_{\Omega},\boldsymbol{Q}^{u},\boldsymbol{G}^{u}\right)\propto p\left(\boldsymbol{y}_{\Omega},\boldsymbol{Q}^{u},\boldsymbol{G}^{u},\phi\right)
%     =p\left(\boldsymbol{y}_{\Omega}\given\boldsymbol{U}'\right)p\left(\boldsymbol{G}^{u}\given\phi\right)p\left(\phi\right).
% \end{equation}
% We use slice sampling as in \citet{murray2010slice} to get samples from the above distribution. This approach is robust to the selection of the sampling scale and easy to implement. Sampling $\gamma$ can be achieved in a similar way. Note that, as mentioned, the sampling is performed on the log-transformed variables to avoid numerical issues. The detailed sampling process for kernel hyperparameters is summarized in Appendix-A Algorithm~\ref{alg:HyperSample}.


% \subsubsection{Sampling $\boldsymbol{\Lambda}_{w}$}
% Given the conjugate Wishart prior, the posterior distribution of $\boldsymbol{\Lambda}_{w}$ is $\left.\boldsymbol{\Lambda}_{w}\right|\boldsymbol{W},\boldsymbol{\Psi}_{0},\nu_{0}\sim\mathcal{W}\left(\boldsymbol{\Psi}^{*},\nu^{*}\right)$, % \begin{equation} \label{Eq:Lambdaw}
% %     \left.\boldsymbol{\Lambda}_{w}\right|\boldsymbol{W},\boldsymbol{\Psi}_{0},\nu_{0}\sim\mathcal{W}\left(\boldsymbol{\Psi}^{*},\nu^{*}\right),
% % \end{equation}
% where $[\boldsymbol{\Psi}^{*}]^{-1}=\boldsymbol{W}\boldsymbol{W}^{\top}+\boldsymbol{\Psi}_{0}^{-1}$ and $\nu^{*}=\nu_{0}+R$.




% \subsubsection{Sampling the coefficient factor matrices $\{\boldsymbol{U},\boldsymbol{V},\boldsymbol{W}\}$}  

% Sampling the factor matrices can be considered as a Bayesian linear regression problem. Taking $\bd{W}$ as an example, we can rewrite Eq.~\eqref{eq:y_omega} as:
% \begin{equation} \label{Eq:STVC-sampleW}
%     \boldsymbol{y}_{\Omega}\sim \mathcal{N}\left(\boldsymbol{O}\left(\tilde{\bd{X}}\left((\boldsymbol{V}\odot\boldsymbol{U})\otimes\boldsymbol{I}_{P}\right)\operatorname{vec}(\boldsymbol{W})\right),\tau^{-1}\boldsymbol{I}_{|\Omega|}\right),
% \end{equation}
% where $\bd{U},\bd{V}$ are known and $\operatorname{vec}(\boldsymbol{W})$ is the coefficient to estimate. Considering that the priors of each component vector $\boldsymbol{w}_{r}$ are independent and identical, the prior distribution over the whole vectorized $\boldsymbol{W}$ becomes  $\operatorname{vec}(\boldsymbol{W})\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}_{R}\otimes\boldsymbol{\Lambda}_{w}^{-1})$. Since both likelihood and prior of $\operatorname{vec}(\boldsymbol{W})$ follow Gaussian distributions, its posterior is also Gaussian with mean $\boldsymbol{\mu}_{W}^{*}$ and precision $\boldsymbol{\Lambda}_{W}^{*}$ such as:
% \begin{equation} \notag
% \begin{aligned}
%     \boldsymbol{\Lambda}_{W}^{*}&=\tau\boldsymbol{H}_{W}^{\top}\boldsymbol{H}_{W}+\boldsymbol{\Lambda}_{W}, \boldsymbol{\Lambda}_{W}=\boldsymbol{I}_{R}\otimes\boldsymbol{\Lambda}_{w}, \\
%     \boldsymbol{\mu}_{W}^{*}&=\tau(\boldsymbol{\Lambda}_{W}^{*})^{-1}\boldsymbol{H}_{W}^{\top}\boldsymbol{y}_{\Omega}, \\
%     \boldsymbol{H}_{W}&=\boldsymbol{O}\left(\tilde{\bd{X}}\left((\boldsymbol{V}\odot\boldsymbol{U})\otimes\boldsymbol{I}_{P}\right)\right).
% \end{aligned}
% \end{equation}


% The posterior distributions of $\boldsymbol{U}$ and $\boldsymbol{V}$ can be obtained similarly using different tensor unfoldings. In order to sample $\boldsymbol{U}$, we use the mode-1 unfolding in Eq.~\eqref{Eq:unfolding-CP} and reconstruct the regression model with $\operatorname{vec}(\bd{U})$ as coefficients: %, such as:
% \begin{equation}
% \boldsymbol{y}_{\Omega}=\boldsymbol{O}\left(\tilde{\bd{X}}_{U}\left((\boldsymbol{W}\odot\boldsymbol{V})\otimes\boldsymbol{I}_{M}\right)\operatorname{vec}(\boldsymbol{U})\right)+\boldsymbol{\epsilon}_{\Omega},
% \end{equation}
% where $\tilde{\bd{X}}_{U}=\left(\boldsymbol{X}_{(3)}\odot\boldsymbol{I}_{MN}\right)^{\top}$ and  $\boldsymbol{\epsilon}_{\Omega}\sim\mathcal{N}(\boldsymbol{0},\tau^{-1}\boldsymbol{I}_{|\Omega|})$. Thus, the posterior of $\operatorname{vec}(\boldsymbol{U})$ has a closed form---a Gaussian distribution with mean $\boldsymbol{\mu}_{U}^{*}$ and precision $\boldsymbol{\Lambda}_{U}^{*}$, where
% % $ p\left(\operatorname{vec}(\boldsymbol{U})\given - \right)=\mathcal{N}\left(\boldsymbol{\mu}_{U}^{*},(\boldsymbol{\Lambda}_{U}^{*})^{-1}\right)$, where
% \begin{equation}\notag
% \begin{aligned}
%     \boldsymbol{\Lambda}_{U}^{*}&=\tau\boldsymbol{H}_{U}^{\top}\boldsymbol{H}_{U}+\boldsymbol{K}_{U}^{-1}, \boldsymbol{K}_{U}=\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{s}, \\
%     \boldsymbol{\mu}_{U}^{*}&=\tau(\boldsymbol{\Lambda}_{U}^{*})^{-1}\boldsymbol{H}_{U}^{\top}\boldsymbol{y}_{\Omega}, \\
%     \boldsymbol{H}_{U}&=\boldsymbol{O}\left(\tilde{\bd{X}}_{U}\left((\boldsymbol{W}\odot\boldsymbol{V})\otimes\boldsymbol{I}_{M}\right)\right).
% \end{aligned}
% \end{equation}
% The posterior for  $\operatorname{vec}(\boldsymbol{V})$ can be obtained by applying the mode-2 tensor unfolding. It should be noted that the above derivation provides the posterior for the whole factor matrix, i.e., $\left\{\operatorname{vec}(\bd{U}),\operatorname{vec}(\bd{V}),\operatorname{vec}(\bd{W})\right\}$. One can further reduce the computing cost by sampling $\left\{\bd{u}_r,\boldsymbol{v}_{r},\boldsymbol{w}_{r}\right\}$ one by one for $r=1,\cdots,R$ as in \citet{luttinen2009variational}. In this case, the time complexity in learning these factor matrices can be further reduced to $\mathcal{O}(R(M^3+N^3+P^3))$ at the cost of slow/poor mixing.



\subsubsection{Sampling $\boldsymbol{\Lambda}_{w}$}
Given the conjugate Wishart prior, the posterior distribution of $\boldsymbol{\Lambda}_{w}$ is $\left.\boldsymbol{\Lambda}_{w}\right|\boldsymbol{W},\boldsymbol{\Psi}_{0},\nu_{0}\sim\mathcal{W}\left(\boldsymbol{\Psi}^{*},\nu^{*}\right)$, % \begin{equation} \label{Eq:Lambdaw}
%     \left.\boldsymbol{\Lambda}_{w}\right|\boldsymbol{W},\boldsymbol{\Psi}_{0},\nu_{0}\sim\mathcal{W}\left(\boldsymbol{\Psi}^{*},\nu^{*}\right),
% \end{equation}
where $[\boldsymbol{\Psi}^{*}]^{-1}=\boldsymbol{W}\boldsymbol{W}^{\top}+\boldsymbol{\Psi}_{0}^{-1}$ and $\nu^{*}=\nu_{0}+R$.


\subsubsection{Sampling the precision $\tau$}
Since we used a conjugate Gamma prior, the posterior distribution of $\tau$ is also a Gamma distribution with shape ${a^{*}=a_{0}+\frac{1}{2}|\Omega|}$ and rate ${b^{*}=b_{0}+\frac{1}{2}\left\|\boldsymbol{y}_{\Omega}-\boldsymbol{O}\left(\tilde{\bd{X}}\operatorname{vec}\left(\boldsymbol{W}(\boldsymbol{V}\odot\boldsymbol{U})^{\top}\right)\right)\right\|_{2}^{2}}$.
% where $\small{a^{*}=a_{0}+\frac{1}{2}|\Omega|}$, $\small{b^{*}=b_{0}+\frac{1}{2}\left\|\boldsymbol{y}_{\Omega}-\boldsymbol{O}\left(\tilde{\bd{X}}\operatorname{vec}\left(\boldsymbol{W}(\boldsymbol{V}\odot\boldsymbol{U})^{\top}\right)\right)\right\|_{2}^{2}}$.
% \begin{equation}\notag
% \begin{aligned}
%     a^{*}&=a_{0}+\frac{1}{2}|\Omega|, \\
%     b^{*}&=b_{0}+\frac{1}{2}\left\|\boldsymbol{y}_{\Omega}-\boldsymbol{O}\left(\tilde{\bd{X}}\operatorname{vec}\left(\boldsymbol{W}(\boldsymbol{V}\odot\boldsymbol{U})^{\top}\right)\right)\right\|_{2}^{2}.
% \end{aligned}
% \end{equation}


\begin{algorithm}[!ht]
\footnotesize
\caption{MCMC sampling process of $\text{BKTR}(\boldsymbol{y}_{\Omega},\boldsymbol{\mathcal{X}},R,K_{1},K_{2})$}
\label{alg:BKTL}
{Initialize $\{{\boldsymbol{U},\boldsymbol{V},\boldsymbol{W}}\}$ as normally distributed random values, $\phi=\gamma=1$, and ${\boldsymbol{\Lambda}_{w}\sim\mathcal{W}(\boldsymbol{I}_{P},P)}$. Set $\mu_{\phi}=\mu_{\gamma}=\log (1)$, $\tau_{\phi}=\tau_{\gamma}=10$, and $a_0=b_0=10^{-4}$. 
% and $\alpha=10^{-2}$.
} \\
\For{$k=1:K_{1}+K_{2}$}{
\textbf{Sample kernel hyperparameters $\{\phi,\gamma\}$} using the Algorithm in Appendix~\ref{appB}; \\
\textbf{Sample hyperparameters $\boldsymbol{\Lambda}_{w}$ from a Wishart distribution:} $\left.\boldsymbol{\Lambda}_{w}\right|-\sim\mathcal{W}\left(\left(\boldsymbol{W}\boldsymbol{W}^{\top}+\boldsymbol{\Psi}_{0}^{-1}\right)^{-1},\nu_{0}+R\right)$; \\
\textbf{Sample factor $\operatorname{vec}(\boldsymbol{U})$ from a Normal distribution:}  $\left.\operatorname{vec}(\boldsymbol{U})\right|-\sim\mathcal{N}\left(\boldsymbol{\mu}_{U}^{*},\left(\boldsymbol{\Lambda}_{U}^{*}\right)^{-1}\right)$, $\boldsymbol{\mu}_{U}^{*}=\tau(\boldsymbol{\Lambda}_{U}^{*})^{-1}\boldsymbol{H}_{U}^{\top}\boldsymbol{y}_{\Omega}$, $\boldsymbol{\Lambda}_{U}^{*}=\tau\boldsymbol{H}_{U}^{\top}\boldsymbol{H}_{U}+\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{s}^{-1}$, $\boldsymbol{H}_{U}=\boldsymbol{O}\left(\left(\boldsymbol{X}_{(3)}\odot\boldsymbol{I}_{MN}\right)^{\top}\left((\boldsymbol{W}\odot\boldsymbol{V})\otimes\boldsymbol{I}_{M}\right)\right)$; \\
\textbf{Sample factor $\operatorname{vec}(\boldsymbol{V})$ from a Normal distribution:}  $\left.\operatorname{vec}(\boldsymbol{V})\right|-\sim\mathcal{N}\left(\boldsymbol{\mu}_{V}^{*},\left(\boldsymbol{\Lambda}_{V}^{*}\right)^{-1}\right)$, $\boldsymbol{\mu}_{V}^{*}=\tau(\boldsymbol{\Lambda}_{V}^{*})^{-1}\boldsymbol{H}_{V}^{\top}\boldsymbol{y}_{\Omega}^{\top}$, $\boldsymbol{\Lambda}_{V}^{*}=\tau\boldsymbol{H}_{V}^{\top}\boldsymbol{H}_{V}+\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{t}^{-1}$, $\boldsymbol{H}_{V}=\boldsymbol{O}^{\prime}\left(\left(\boldsymbol{X}_{(3)}^{\top}\odot\boldsymbol{I}_{MN}\right)^{\top}\left((\boldsymbol{W}\odot\boldsymbol{U})\otimes\boldsymbol{I}_{N}\right)\right)$, where $\boldsymbol{O}^{\prime}\in\mathbb{R}^{|\Omega|\times MN}$ is a matrix removing the rows corresponding to the zeros in $\operatorname{vec}(\boldsymbol{D}^{\top})$ from $\boldsymbol{I}_{MN}$, $\boldsymbol{X}_{(3)}^{\top}$ is the mode-3 unfolding of $\boldsymbol{\mathcal{X}}^{\top}\in\mathbb{R}^{N\times M\times P}$, whose frontal slices are the transpose matrices of frontal slices of $\boldsymbol{\mathcal{X}}$, and $\boldsymbol{y}_{\Omega}^{\top}=\boldsymbol{O}^{\prime}\operatorname{vec}\left(\boldsymbol{Y}^{\top}\right)$; \\
\textbf{Sample factor $\operatorname{vec}(\boldsymbol{W})$ from a Normal distribution:}  $\left.\operatorname{vec}(\boldsymbol{W})\right|-\sim\mathcal{N}\left(\boldsymbol{\mu}_{W}^{*},\left(\boldsymbol{\Lambda}_{W}^{*}\right)^{-1}\right)$, $\boldsymbol{\mu}_{W}^{*}=\tau(\boldsymbol{\Lambda}_{W}^{*})^{-1}\boldsymbol{H}_{W}^{\top}\boldsymbol{y}_{\Omega}$, $\boldsymbol{\Lambda}_{W}^{*}=\tau\boldsymbol{H}_{W}^{\top}\boldsymbol{H}_{W}+\boldsymbol{I}_{R}\otimes\boldsymbol{\Lambda}_{w}$, $\boldsymbol{H}_{W}=\boldsymbol{O}\left(\left(\boldsymbol{I}_{MN}\odot\boldsymbol{X}_{(3)}\right)^{\top}\left((\boldsymbol{V}\odot\boldsymbol{U})\otimes\boldsymbol{I}_{P}\right)\right)$; \\
\textbf{Sample precision $\tau$ from a Gamma distribution:} $\left.\tau\right|-\sim\text{Gamma}\left(a_{0}+\frac{1}{2}|\Omega|,b_{0}+\frac{1}{2}\left\|\boldsymbol{y}_{\Omega}-\boldsymbol{O}\left(\tilde{\bd{X}}\operatorname{vec}\left(\boldsymbol{W}(\boldsymbol{V}\odot\boldsymbol{U})^{\top}\right)\right)\right\|_{2}^{2}\right)$; \\
\If{$k>K_{1}$}{
{Collect the sample} ${\boldsymbol{U}^{(k-K_1)}=\bd{U},\boldsymbol{V}^{(k-K_1)}=\bd{V},\boldsymbol{W}^{(k-K_1)}=\bd{W}}$;\\
Compute $\boldsymbol{\mathcal{B}}^{(k-K_1)}$ using $\boldsymbol{\mathcal{B}}=\sum_{r=1}^{R}\boldsymbol{u}_{r}\circ\boldsymbol{v}_{r}\circ\boldsymbol{w}_{r}$.} 
} 
\Return $\{\boldsymbol{\mathcal{B}}^{(k)}\}_{k=1}^{K_2}$ to approximate the posterior coefficients and estimate the unobserved data.
\end{algorithm}



\subsection{Model Implementation}

We summarize the implementation of BKTR in Algorithm~\ref{alg:BKTL}. For the MCMC inference, we run $K_1$ iterations as burn-in and take the following $K_2$ samples for estimation.



% \section{Related Work and Model Properties}
% \section{Related Work}
% \label{sec:RelatedWork}


% % \subsection{Related Work}

% Our key research question is how to efficiently and effectively model a high-dimensional latent variable (i.e., the third-order spatiotemporal tensor $\bd{\cal{B}}$), in which complex dependencies exist both within and across dimensions. The most popular strategy is to model the variable using a GP with a separable covariance matrix, which has been used in spatiotemporal analysis \cite[e.g.,][]{cressie1999classes,gelfand2003spatial,stein2005space,zhang2007maximum} and multi-output GP \cite[e.g.,][]{bonilla2007multi,alvarez2012kernels}.
% %Although computationally it is possible to benefit from the Kronecker structure of the separable covariance by applying singular value decomposition when updating kernel hyperparameters, the property is no longer available when the data contains missing entries
% However, the cubic time complexity in learning kernel hyperparameters becomes the critical bottleneck. Several solutions have been proposed to address this computational challenge, such as low-rank approximation \citep{bonilla2007multi}, sparse approximation \citep{quinonero2005unifying,alvarez2008sparse}, and using compactly supported kernels  \citep{luttinen2012efficient}.

% Our work follows a different approach. Instead of modeling $\bd{\beta}$ directly using a GP, we parameterize the third-order tensor $\bd{\cal{B}}$ using a  low-rank tensor decomposition \citep{kolda2009tensor}. The idea is inspired by recent studies on low-rank tensor regression/learning \cite[see e.g.,][]{zhou2013tensor,bahadori2014fast,rabusseau2016low,yu2016learning,guhaniyogi2017bayesian,yu2018tensor}. The low-rank assumption not only preserves the global patterns and higher-order dependencies in the variable, but also greatly reduces the number of parameters. In fact, without considering the spatiotemporal indices, we can formulate Eq.~\eqref{Eq:STVC-point} as a scalar tensor regression problem \citep{guhaniyogi2017bayesian} by reconstructing each $\bd{x}(\bd{s}_m,t_n)$ as a sparse covariate tensor of the same size as $\bd{\cal{B}}$. However, for spatiotemporal data, the low-rank assumption alone cannot characterize the strong local spatial and temporal consistency. To better encode side information, existing studies have introduced graph Laplacian regularization in defining the loss function \cite[e.g.,][]{bahadori2014fast,rao2015collaborative}. Nevertheless, this approach also introduces more parameters (e.g., those used to define distance/similarity function and weights in the loss function) and it has limited power in modeling complex spatial and temporal processes. The most relevant work is a Gaussian process factor analysis model \citep{luttinen2009variational} developed for a completely different problem---spatiotemporal matrix completion, in which different GP priors are assumed on the spatial and temporal factors and the whole model is learned through variational Bayesian inference. We follow a similar idea to parameterize the coefficients $\bd{\beta}$ and develop MCMC algorithms for model inference.


% \begin{itemize}
%     \item The equivalent kernel expression for B by using our decomposed model
%     % \item The column-wise updating scheme (appendix)
% \end{itemize}

% why not directly integrate (computation dilemma)


\section{Simulation Study}
\label{sec:simulation}

In this section, we evaluate the performance of BKTR on three simulated data sets. All simulations are performed on a laptop with a 6-core Intel Xenon 2.60 GHz CPU and 32GB RAM. Specifically, we conduct three studies: (1) a low-rank structured association analysis to test the estimation accuracy and statistical properties of BKTR with different rank settings and in scenarios with different observation rates, (2) a small-scale analysis to compare BKTR with STVC and a pure low-rank tensor regression model, and (3) a moderately sized analysis to test the performance of BKTR on more practical STVC modeling.
% effect of rank on coefficient reconstruction and prediction of non-observed entries.
% and (3) a large-scale and real-world analysis on spatiotemporal bike-sharing demand.



\subsection{Simulation 1: performance and statistical properties of BKTR}


\subsubsection{Simulation setting}

To fully evaluate the properties of BKTR, we first simulate a data set where $\boldsymbol{\mathcal{B}}$ is constructed following an exact CP decomposition. We generate $M=300$ spatial locations in a $[0,10]\times[0,10]$ square and $N=100$ uniformed distributed time points in $[0,10]$. The variable ${\boldsymbol{\mathcal{X}}\in\mathbb{R}^{300\times100\times5}}$ ${(P=5)}$ contains an intercept and four spatiotemporal covariates as follows:
\begin{equation} \label{Eq:X_generate}
% \small
\begin{aligned}
    \bd{\cal{X}}&(:,:,p=1)=\operatorname{ones}(M,N),~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\text{(for the intercept)}\\
    \boldsymbol{\mathcal{X}}&(:,:,p=2,3)=\boldsymbol{1}_{N}^{\top}\otimes\boldsymbol{x}_{s}^{p},\text{with } \boldsymbol{x}_{s}^{p}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}_{M}),~~~~\text{(covariates only vary with space)}\\
    \boldsymbol{\mathcal{X}}&(:,:,p=4,5)=\boldsymbol{1}_{M}\otimes(\boldsymbol{x}_{t}^{p})^{\top},\text{with }\boldsymbol{x}_{t}^{p}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}_{N}).\text{ (covariates only vary with time)}
\end{aligned}
\end{equation}
We set the true CP rank $R=10$ and generate the coefficient variable $\boldsymbol{\mathcal{B}}\in\mathbb{R}^{300\times100\times5}$ as $\boldsymbol{\mathcal{B}}=\sum_{r=1}^{R}\boldsymbol{u}_{r}\circ\boldsymbol{v}_{r}\circ\boldsymbol{w}_{r}$, where $\boldsymbol{u}_{r}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{K}_{s}),~\boldsymbol{v}_{r}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{K}_{t}),~\boldsymbol{w}_{r}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{\Lambda}_{w}^{-1})$ for $r=1,\cdots,R$. % below:
% \begin{equation}
%     \begin{aligned} \notag
%         \boldsymbol{u}_{r}&\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{K}_{s}), ~\boldsymbol{v}_{r}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{K}_{t}), \\ \boldsymbol{w}_{r}&\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{\Lambda}_{w}^{-1}), ~r=1,\cdots,R, \\
%         \boldsymbol{\mathcal{B}}&=\sum_{r=1}^{R}\boldsymbol{u}_{r}\circ\boldsymbol{v}_{r}\circ\boldsymbol{w}_{r},
%     \end{aligned}
% \end{equation}
% where 
$\boldsymbol{K}_{s}\in\mathbb{R}^{300\times300}$ and $\boldsymbol{K}_{t}\in\mathbb{R}^{100\times100}$ are computed from a Matern 3/2 kernel $k_s(\bd{s}_{m},\bd{s}_{m'})=\sigma_{s}^2\left(1+\frac{\sqrt{3}d}{\phi}\right)\exp{\left(-\frac{\sqrt{3}d}{\phi}\right)}$ ($d$ is the distance between locations $\bd{s}_{m}$ and $\bd{s}_{m'}$) and a squared exponential (SE) kernel $k_t(t_{n},t_{n'})=\sigma_{t}^2\exp{\left(-\frac{(t_{n}-t_{n'})^2}{2\gamma^{2}}\right)}$, respectively, with variances $\sigma_s^2=\sigma_t^2=2$, length-scales $\phi=\gamma=1$, and $\boldsymbol{\Lambda}_{w}\sim\mathcal{W}(\boldsymbol{I}_{P},P)$. Based on $\boldsymbol{\mathcal{X}}$ and $\boldsymbol{\mathcal{B}}$, the spatiotemporal data $\boldsymbol{Y}\in\mathbb{R}^{300\times100}$ is then generated following Eq.~(\ref{Eq:STVC-vector}) with $\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0},\tau^{-1}\boldsymbol{I}_{MN})$ where $\tau^{-1}=1$.

%We randomly mask $x$\% of the data $\boldsymbol{Y}$ as missing and evaluate the performance of BKTR with the true rank setting. We varied the value of $x$ in $(0.1,0.3, 0.5, 0.7, 0.9)$. Performance is evaluated with respect to estimation accuracy of both coefficients and unobserved (missing) data, type I error and power in significance testing of the parameters using credible intervals. We evaluate the sensitivity to rank specification by comparing BKTR with different rank settings. We also assess the effect of missing scenarios by comparing BKTR performances under different missing ratios. In this experiments, we assume the kernel functions to be known, i.e., $k_s$ is Matern 3/2 and $k_t$ is SE, and also include in $\boldsymbol{\mathcal{X}}$  an extra covariate (drawn from the standard normal distribution) unrelated with the outcome (the corresponding estimated coefficients should be close to zero). For each setting, we replicate the simulation 40 times and run the MCMC sampling with $K_1=1000$ and $K_2=500$.

We test the performance of BKTR under different settings in order to evaluate its sensitivity to i) the rank specification defined by the user, and ii) the proportion of observed responses, i.e., $\frac{|\Omega|}{MN}$. For i), we specify ${R=\{4,7,10,13,16,20,25,30,35,40\}}$ and randomly sample 50\% of the data $\boldsymbol{Y}$ as observed values. For ii), we applied the model under 5 different observed response rates, where we randomly sample (90\%, 70\%, 50\%, 30\%, 10\%) of $\boldsymbol{Y}$ as observed values and evaluate the performance of BKTR with the true rank setting ($R=10$). In all settings, we assume the kernel functions to be known, i.e., $k_s$ is Matern 3/2 and $k_t$ is SE. We also include in the analysis a 6th covariate in $\boldsymbol{\mathcal{X}}$ drawn from the standard normal distribution and unrelated with the outcome (the corresponding estimated coefficients should be close to zero). %in order to evaluate type-\uppercase\expandafter{\romannumeral1} errors.
For each setting, we replicate the simulation 40 times and run the MCMC with $K_1=1000$ and $K_2=500$.
% The MCMC sampling was performed with $K_1=1000$ and $K_2=500$. Each setting is replicated 40 times and average performance metrics are reported. 

% Performance is evaluated with respect to estimation accuracy of both coefficients and unobserved (missing) data, coverage of credible intervals, type-\uppercase\expandafter{\romannumeral1} error and power in significance test of the parameters based on credible intervals. 
In each setting, we measure the estimation accuracy on $\boldsymbol{\mathcal{B}}$ and the prediction accuracy on $\boldsymbol{y}_{\Omega^{c}}$ (unobserved data) using mean absolute error (MAE) and root mean square error (RMSE) between the true values and the corresponding posterior mean estimations, where the true $\boldsymbol{\mathcal{B}}$ values for the random non-significant 6th covariate are set to zero. For evaluating the performance on uncertainty estimation, we assess interval coverage (CVG) \citep{heaton2019case}, interval score (INT), and continuous rank probability score (CRPS) \citep{gneiting2007strictly} of the 95\% credible intervals (CI) of all $\boldsymbol{\mathcal{B}}$ values. Note that all the CIs are obtained based on the $K_2$ samples after burn-in. The detailed definitions of all these performance metrics are given in Appendix~\ref{appC}.


 

% In the context of significance testing of the estimated coefficients based on credible intervals, we assess coverage, type-\uppercase\expandafter{\romannumeral1} error and power. Specifically, we compute the \textbf{average coverage} of 95\% credible intervals (CI) of all $\boldsymbol{\mathcal{B}}$ values, where the true $\boldsymbol{\mathcal{B}}$ values for the random non-significant 6th covariate are set to zero. \textbf{Average power} and \textbf{Type-\uppercase\expandafter{\romannumeral1} error} are computed for the four significant covariates and for the 6th non-significant covariate, respectively, as the proportion of CIs that do not contain zero. Note that all the CIs are obtained based on the $K_2$ samples after burn-in. The detailed definitions of all these performance metrics are given in Appendix \ref{appC}.


\subsubsection{Results}


% the proposed BKTR model offers low estimation error on coefficient tensor $\boldsymbol{\mathcal{B}}$ and missing data $\boldsymbol{y}_{\Omega^{c}}$, and provides valid significance testing results


Table~\ref{tab:simu1} shows the performance metrics (mean$\pm$std calculated from the 40 replicated simulations) of BKTR in the case where the true rank is specified ($R=10$) and when 50\% of $\boldsymbol{Y}$ is partially observed. We can see that the CVG of 95\% CI of $\boldsymbol{\mathcal{B}}$ is around 95\% as expected, the INT is small meaning a narrow estimation interval, and the CRPS is also a small value implying a low probabilistic estimation error, indicating that the proposed BKTR provides valid inference for $\boldsymbol{\mathcal{B}}$. We also plot some estimation results of one chain/simulation in Figure~\ref{fig:simu1_new}, where Panel (a) illustrates the temporal evolution of the estimated coefficients for each covariate at a given location, and Panel (b) maps spatial surfaces of $\boldsymbol{\mathcal{B}}$ for each covariate at a given time point. These plots show that BKTR can effectively approximate the spatiotemporally varying relationships between the spatiotemporal data and the corresponding covariates.


% We can see that the average coverage is around 95\% as expected, the average power is high, and the Type-\uppercase\expandafter{\romannumeral1} error is close to 5\%, indicating that the proposed BKTR model provides valid inference. %In addition, BKTR also offers low estimation error on coefficient tensor $\boldsymbol{\mathcal{B}}$ and missing data $\boldsymbol{y}_{\Omega^{c}}$.
% We also plot some estimation results of one chain/simulation in Figure~\ref{fig:simu1_new}, where Panel (a) illustrates the temporal evolution of the estimated coefficients for each covariate at a given location, and Panel (b) maps spatial surfaces of $\boldsymbol{\mathcal{B}}$ for each covariate at a given time point. These plots show that BKTR can effectively approximate the spatiotemporally varying relationships between the spatiotemporal data and the corresponding covariates.


\begin{table}[!t]
\footnotesize
    \centering
    \caption{Performance of BKTR ($R=10$) on Simulation 1, with 50\% $\boldsymbol{Y}$ missing.}
    \begin{tabular}{ccccc}
    \toprule
    $\text{MAE}_{\boldsymbol{\mathcal{B}}}$/$\text{RMSE}_{\boldsymbol{\mathcal{B}}}$ & $\text{MAE}_{\boldsymbol{y}_{\Omega^c}}$/$\text{RMSE}_{\boldsymbol{y}_{\Omega^c}}$ & CVG & INT & CRPS \\
    \midrule
    0.21$\pm$0.02/0.33$\pm$0.04 & 0.91$\pm$0.01/1.15$\pm$0.02 & 94.83\%$\pm$0.75\% & 1.04$\pm$0.10 & 0.15$\pm$0.01 \\
    \bottomrule
    \end{tabular}
    \label{tab:simu1}
\end{table}



% \begin{figure}[!t]
% \centering
% \includegraphics[width=0.9\textwidth]{graphical/simu1-temporal.pdf}
% \caption{Estimated coefficients at location \#  3 ($m=3$), i.e., $\boldsymbol{\mathcal{B}}(3,:,:)$.}
% \label{fig:temporal_simu1}
% \end{figure}

\begin{figure}[!t]
% \footnotesize
\centering
\subfigure[Estimated coefficients at location $m=3$, i.e., $\boldsymbol{\mathcal{B}}(3,:,p=1,2,3,4,5,6)$ (mean with 95\% CI in blue, true value in black).]{
\includegraphics[width=0.68\textwidth]{graphical/simu1-temporal.pdf}
}
\subfigure[Interpolated spatial surfaces of true value, estimated value (posterior mean), and absolute estimation error of the coefficients at time point $n=20$, i.e., $\boldsymbol{\mathcal{B}}(:,20,p=1,2,3,4,5,6)$. Black circles denote the positions of sampled locations.]{
\includegraphics[width=0.94\textwidth]{graphical/simu1-spatial2.pdf}
}
\caption{BKTR ($R=10$) estimated coefficients for Simulation 1 (50\% of $\boldsymbol{Y}$ is observed).}
\label{fig:simu1_new}
\end{figure}


% \begin{figure}[!ht]
% \centering
% \subfigure[Coefficients ($\boldsymbol{\mathcal{B}}(3,:,6)$) estimated by BKTR with different rank settings (when 50\% of data is observed).% (under 50\% random missing).
% ]{
% \includegraphics[width=0.99\textwidth]{graphical/simu1-temporaCompare_Rank.pdf}
% }
% \subfigure[Coefficients ($\boldsymbol{\mathcal{B}}(3,:,3)$) estimated by BKTR (with $R=10$) under different observation rates.]{
% \includegraphics[width=0.99\textwidth]{graphical/simu1-temporaCompare_Missing.pdf}
% }
% \caption{\small{Comparison of BKTR in different settings for Simulation 1. (a) and (b) plot the estimated $\boldsymbol{\mathcal{B}}$ (mean with 95\% CI) of one simulation at location \#3 ($m=3$) for the 6th and 3rd covariates (i.e., $\boldsymbol{\mathcal{B}}(3,:,6)\text{ and }\boldsymbol{\mathcal{B}}(3,:,3)$), respectively.}}
% \label{fig:simu1_new_compare}
% \end{figure}


\begin{figure}[!t]
\centering
\subfigure[{Effect of the rank $R$.}]{
\includegraphics[width=0.95\textwidth]{graphical/simu1-rank_update.pdf}
}
\subfigure[{Effect of the observation rate, i.e., $\frac{|\Omega|}{MN}$.}]{
\includegraphics[width=0.95\textwidth]{graphical/simu1-missing_update.pdf}
}
\caption{{Sensitivity test of BKTR for Simulation 1. For each case, the figure shows the boxplot and mean value of corresponding metrics calculated from the 40 replications.}}
\label{fig:simu1_new_rank_missing}
\end{figure}


\begin{figure}[!t]
\centering
\subfigure[Coefficients ($\boldsymbol{\mathcal{B}}(3,:,6)$) estimated by BKTR with different rank settings (when 50\% of data is observed).% (under 50\% random missing).
]{
\includegraphics[width=0.95\textwidth]{graphical/simu1-temporaCompare_Rank.pdf}
}
\subfigure[Coefficients ($\boldsymbol{\mathcal{B}}(3,:,3)$) estimated by BKTR (with $R=10$) under different observation rates.]{
\includegraphics[width=0.95\textwidth]{graphical/simu1-temporaCompare_Missing.pdf}
}
\caption{Comparison of BKTR in different settings for Simulation 1. (a) and (b) plot the estimated $\boldsymbol{\mathcal{B}}$ (mean with 95\% CI) of one simulation at location \#3 ($m=3$) for the 6th and 3rd covariates (i.e., $\boldsymbol{\mathcal{B}}(3,:,6)\text{ and }\boldsymbol{\mathcal{B}}(3,:,3)$), respectively.}
\label{fig:simu1_new_compare}
\end{figure}




The performances of BKTR with different rank specifications and 50\% of the data points observed are compared in Figure~\ref{fig:simu1_new_rank_missing} (a). We see that when the rank is over-specified (larger than the true value, i.e., $R>10$), BKTR can still achieve reliable estimation accuracy for both coefficients and unobserved data, even when $R$ is much larger than 10, e.g., $R=40$. The CVG for the 95\% CI of $\boldsymbol{\mathcal{B}}$ is also maintained around 95\% when $R>10$, the corresponding INT and CRPS keep a low value. This indicates that BKTR is able to provide accurate estimation for the coefficients along with high quality uncertainty even when the model includes redundant parameters. In addition, Figure~\ref{fig:simu1_new_rank_missing} (b) illustrates the effect of the observation rate of $\boldsymbol{Y}$, where the results of BKTR (specifying $R=10$) with different proportions of observed data  are compared. We observe that BKTR has a stable low estimation error when the rate of observed data decreases, and it continues to obtain valid CIs despite the increased lengths of CI for lower observation rates (i.e., the INT becomes larger). These results demonstrate the powerful applicability of the proposed Bayesian framework. 
% The temporal estimations of coefficients in different cases are provided in Supplementary material Section E.1.
We further show the temporal estimations of coefficients at a given location obtained in different cases in Figure~\ref{fig:simu1_new_compare}. The plots in Panel (a) indicate that higher rank specification generates broader intervals, which is consistent with the results given in Figure~\ref{fig:simu1_new_rank_missing} (a), i.e., the INT slightly increases with over-specified ranks. Panel (b) shows that BKTR can accurately estimate time-varying coefficients even when only 10\% of the response data is observed, proving its high modeling capacity.




% The performances of BKTR with different rank specifications and 50\% of the data points observed are compared in Figure~\ref{fig:simu1_new_rank_missing} (a). We see that when the rank is over-specified (larger than the true value, i.e., $R>10$), BKTR can still achieve reliable estimation accuracy for both coefficients and unobserved data, even when $R$ is much larger than 10, e.g., $R=40$. The average coverage for the 95\% CI is also maintained around 95\% when $R>10$, but the corresponding average power and Type-\uppercase\expandafter{\romannumeral1} error slightly decrease. This indicates that when the model includes redundant parameters, BKTR tends to provide wider intervals for the estimation. In addition, Figure~\ref{fig:simu1_new_rank_missing} (b) illustrates the effect of the observation rate of $\boldsymbol{Y}$, where the results of BKTR (specifying $R=10$) with different proportions of observed data  are compared. We observe that BKTR has a stable low estimation error when the rate of observed data decreases, and it continues to obtain valid CIs despite the increased lengths of CI for lower observed rates (i.e., the average power becomes lower). These results demonstrate the powerful applicability of the proposed Bayesian framework. 
% % The temporal estimations of coefficients in different cases are provided in Supplementary material Section E.1.
% We further show the temporal estimations of coefficients at a given location obtained in different cases in Figure~\ref{fig:simu1_new_compare}. The plots in Panel (a) indicate that higher rank specification generates broader intervals, which is consistent with the results given in Figure~\ref{fig:simu1_new_rank_missing} (a), i.e., the average power and Type-\uppercase\expandafter{\romannumeral1} error become lower with over-specified ranks. Panel (b) shows that BKTR can accurately estimate time-varying coefficients even when only 10\% of the response data is observed, proving its high modeling capacity.


% The results of BKTR ($R=10$) in different missing scenarios are compared in Figure~\ref{fig:simu1_new_rank_missing} (b), to test the effect of missing ratio.



\subsection{Simulation 2: model comparison on a small data set}
\subsubsection{Simulation setting} 
In this simulation, we generate a small data set in which the true $\boldsymbol{\mathcal{B}}$ is directly generated following a separable covariance matrix. Specifically, we simulate 30 locations $(M=30)$ in a $[0,10]\times[0,10]$ square and $N=30$ uniformly distributed time points in $[0,10]$. We then generate a small synthetic data set ${\left\{\boldsymbol{Y}\in\mathbb{R}^{30\times30},\boldsymbol{\mathcal{X}}\in\mathbb{R}^{30\times30\times3}\right\} (P=3)}$ following Eq.~\eqref{Eq:STVC-vector} with:
\begin{equation} \notag
% \footnotesize
\begin{aligned}
    \bd{\cal{X}}(:,:,1)&=\operatorname{ones}(M,N),\ 
    \bd{\cal{X}}(:,:,2)=\boldsymbol{1}_{N}^{\top}\otimes\boldsymbol{x}_{s},\ \bd{\cal{X}}(:,:,3)=\boldsymbol{1}_{M}\otimes\boldsymbol{x}_{t}^{\top},
    \\
% \boldsymbol{x}_{s}&\sim\mathcal{N}(\bd{0},\boldsymbol{I}_{M}), \ \boldsymbol{x}_{t}\sim\mathcal{N}(\bd{0},\boldsymbol{I}_{N}),\\ %\boldsymbol{1}_{N}
    \operatorname{vec}\left(\boldsymbol{B}_{(3)}\right)&\sim\mathcal{N}\left(\boldsymbol{0},\boldsymbol{K}_{t}\otimes\boldsymbol{K}_{s}\otimes\boldsymbol{\Lambda}_{w}^{-1}\right),~\boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{0},\tau^{-1}\boldsymbol{I}_{MN}),
\end{aligned}
\end{equation}
where ${\boldsymbol{x}_{s}\sim\mathcal{N}(\bd{0},\boldsymbol{I}_{M})}$, ${\boldsymbol{x}_{t}\sim\mathcal{N}(\bd{0},\boldsymbol{I}_{N})}$, ${\boldsymbol{K}_{s}}$ and ${\boldsymbol{K}_{t}}$ are still computed from a Matern 3/2 kernel and a SE kernel, respectively, and ${\boldsymbol{\Lambda}_{w}\sim\mathcal{W}(\boldsymbol{I}_{P},P)}$. For model parameters, we set the kernel variance hyperparameters to $\sigma_s^2=\sigma_t^2=2$, and consider combinations of data noise variance $\tau^{-1}\in\{0.2,1,5\}$, and kernel length-scale hyperparameters $\phi=\gamma\in\{1,2,4\}$. We specify the CP rank ${R=10}$ for BKTR estimation and compare BKTR with the original STVC model \citep{gelfand2003spatial} and a pure low-rank Bayesian probabilistic tensor regression (BTR) model without imposing any spatiotemporal GP priors, using the same rank as BKTR. % We note that BTR corresponds to the default tensor regression where all data points are assumed to be independent. 
For both BKTR and STVC, we assume that the kernel functions are known, i.e., $k_s$ is Matern 3/2 and $k_t$ is SE, and use MCMC to estimate the unknown kernel hyperparameters. As in the previous simulations, we add a normally distributed non-significant random covariate in $\boldsymbol{\mathcal{X}}$ when fitting the model. We replicate each simulation 25 times and run the MCMC sampling with $K_1=1000$ and $K_2=500$.

\subsubsection{Results} 


% \begin{figure}[!t]
% \centering
% \includegraphics[width=0.77\textwidth]{graphical/smallSimu1.pdf}
% \caption{Estimation results on simulation 1. (a) Comparison of the estimated coefficients. We show an example of the approximated coefficients (mean with 95\% CI) for $(m=3,p=2)$. The top and bottom subplots compare BKTR with STVC and BTR, respectively. (b) Trace plots of the kernel hyperparameters estimated by BKTR.}
% \label{Fig:Simu1}
% \end{figure}


Table~\ref{tab:simu2} presents the accuracy (mean$\pm$std) of the estimated coefficient tensor $\bd{\cal{B}}$ obtained from the 25 simulations. We also report the average computing time for each MCMC iteration. As we can see, BKTR with rank $R=10$ achieves similar accuracy as STVC and even performs better when increasing the data noise or kernel length-scales. This means that BKTR is more robust for noisy data and the low-rank assumption can fit the spatiotemporal relations better when they vary more smoothly/slowly. In addition, BKTR also clearly outperforms BTR, which confirms the importance of introducing GPs to encode the spatial and temporal dependencies. These results suggest that the proposed kernelized CP decomposition can approximate the true coefficient tensor relatively well even if the true $\bd{\cal{B}}$ does not follow an exact low-rank specification. In terms of computing time, we see that BKTR is much faster than STVC. Due to the high cost of STVC, it becomes infeasible to analyze a data set of moderate size, e.g., $M,N=100$, and $P=10$. 


\begin{table}[!t]
\footnotesize
    \centering
    \caption{{Performance comparison for Simulation 2 $\left(\text{MAE}_{\boldsymbol{\mathcal{B}}}/\text{RMSE}_{\boldsymbol{\mathcal{B}}}\right)$.}}
    \begin{tabular}{ll|ccc}
    \toprule
    Settings & & STVC & BKTR  & BTR \\
    \midrule
    \multirow{3}{*}{$\tau^{-1}=0.2$} & $\phi=\gamma=1$ & 0.64$\pm$0.16/1.00$\pm$0.29 & 0.81$\pm$0.19/1.32$\pm$0.32 & 1.00$\pm$0.29/1.57$\pm$0.49 \\
    & $\phi=\gamma=2$ & 0.63$\pm$0.15/1.09$\pm$0.23 & 0.61$\pm$0.18/1.06$\pm$0.34 & 0.94$\pm$0.35/1.54$\pm$0.62  \\
    & $\phi=\gamma=4$ & 0.53$\pm$0.11/0.88$\pm$0.19 & 0.40$\pm$0.13/0.70$\pm$0.28 & 0.74$\pm$0.29/1.24$\pm$0.54 \\
    \cmidrule(r){1-5}
    \multirow{3}{*}{$\tau^{-1}=1$} & $\phi=\gamma=1$ & 0.82$\pm$0.10/1.25$\pm$0.24 & 0.79$\pm$0.18/1.23$\pm$0.33 & 1.00$\pm$0.25/1.55$\pm$0.44 \\
    & $\phi=\gamma=2$ & 0.63$\pm$0.07/0.92$\pm$0.15 & 0.60$\pm$0.12/0.96$\pm$0.22 & 0.85$\pm$0.23/1.33$\pm$0.41 \\
    & $\phi=\gamma=4$ & 0.54$\pm$0.09/0.74$\pm$0.16 & 0.45$\pm$0.09/0.71$\pm$0.17 & 0.73$\pm$0.22/1.18$\pm$0.42 \\
    \cmidrule(r){1-5}
    \multirow{3}{*}{$\tau^{-1}=5$} & $\phi=\gamma=1$ & 1.09$\pm$0.12/1.60$\pm$0.19 & 0.89$\pm$0.14/1.32$\pm$0.23 & 1.10$\pm$0.16/1.66$\pm$0.27 \\
    & $\phi=\gamma=2$ & 0.92$\pm$0.12/1.31$\pm$0.18 & 0.75$\pm$0.11/1.11$\pm$0.20 & 1.01$\pm$0.16/1.52$\pm$0.30 \\
    & $\phi=\gamma=4$ & 0.78$\pm$0.08/1.08$\pm$0.14 & 0.61$\pm$0.07/0.89$\pm$0.14 & 0.90$\pm$0.15/1.37$\pm$0.29 \\
    \cmidrule(r){1-5}
    \multicolumn{2}{l|}{Average computing time} & $\approx$15.44 sec/iter. & $\approx$0.011 sec/iter. & $\approx$0.008 sec/iter. \\
    \bottomrule
    \end{tabular}
    \label{tab:simu2}
\end{table}


Figure~\ref{fig:simu2_temporal} shows the estimation results of the three approaches for one chain/simulation when $\tau^{-1}=1, \phi=\gamma=1$. We plot an example for the third covariate at location \#8 ($m=8, p=3$) to show the temporal variation of the coefficients estimated by the three methods. More comparison results in other settings are given in Appendix~\ref{appD1}. As can be seen, BKTR and STVC generate similar results and most of the coefficients are contained in the 95\% CI for these two models. Although BTR can still capture the general trend, it fails to reproduce the local temporal dependency due to the absence of spatial/temporal priors. To further verify the mixing of the GP hyperparameters, we run the MCMC for BKTR for 5000 iterations and show the trace plots and probability distributions (after burn-in) for $\phi$ and $\gamma$ obtained when $\tau^{-1}=1, \phi=\gamma\in\{1,2,4\}$ in Figure~\ref{fig:simu2_kernelHyper}. We can see that the Markov chains for the kernel hyperparameters mix fast and well, and a few hundred iterations should be sufficient for posterior inference.



\begin{figure}[!t]
\centering
\includegraphics[width=0.68\textwidth]{graphical/simu2-temporal-1.pdf}
\caption{{Comparison of the estimated coefficients when $\tau^{-1}=1,\phi=\gamma=1$. We show the approximated coefficients (mean with 95\% CI) for the third covariate at location \#8.}}
\label{fig:simu2_temporal}
\end{figure}


\begin{figure}[!t]
\centering
\subfigure[Estimated hyperparameters of $\boldsymbol{K}_{s}$.]{
\includegraphics[width=0.475\textwidth]{graphical/simu2-hyperphi-marginal2.pdf}
}
% \subfigure[Estimated Kernel hyperparameters of $\boldsymbol{K}_{t}$ (i.e., $\gamma$), in different settings.]{
\subfigure[Estimated hyperparameters of $\boldsymbol{K}_{t}$.]{
\includegraphics[width=0.475\textwidth]{graphical/simu2-hypergamma-marginal2.pdf}
}
\caption{{Kernel hyperparameters estimated by BKTR for Simulation 2 when $\tau^{-1}=1$.}}
\label{fig:simu2_kernelHyper}
\end{figure}



\subsection{Simulation 3: STVC modeling on a moderate-sized data set}
\subsubsection{Simulation setting}
To evaluate the performance of BKTR in more realistic settings where the data size is large and the relationships are usually captured without low-rank structure, we further generate a relatively large synthetic data set of the same size as in Simulation 1, i.e., $M=300$ locations and $N=100$ time points. We generate the covariates $\boldsymbol{\mathcal{X}}\in\mathbb{R}^{300\times100\times5}~(P=5)$ according to Eq.~(\ref{Eq:X_generate}). The coefficients are simulated using $\operatorname{vec}\left(\boldsymbol{B}_{(3)}\right)\sim\mathcal{N}\left(\boldsymbol{0},\boldsymbol{K}_{t}\otimes\boldsymbol{K}_{s}\otimes\boldsymbol{\Lambda}_{w}^{-1}\right)$. The parameters $\left\{\boldsymbol{K}_{s},\boldsymbol{K}_{t},\boldsymbol{\Lambda}_{w}\right\}$ and $\boldsymbol{\epsilon}$ are specified as in Simulation 1, i.e., $\boldsymbol{K}_{s}$ and $\boldsymbol{K}_{t}$ are computed from a Matern 3/2 and a SE kernel respectively with hyperparamaters $\{\sigma_s^2=\sigma_t^2=2, \phi=\gamma=1\}$, $\boldsymbol{\Lambda}_{w}\sim\mathcal{W}(\boldsymbol{I}_{P},P)$, and $\tau^{-1}=1$. We randomly select 50\% of the generated data $\boldsymbol{Y}$ as observed responses. To assess the effect of the rank specification, we try different CP rank settings $R$ in $\{5,10,\cdots,60\}$ and compute MAE and RMSE for both $\boldsymbol{\mathcal{B}}$ and $\boldsymbol{y}_{\Omega^c}$. We replicate the experiment with each rank setting 15 times and run the MCMC sampling with $K_1=1000$ and $K_2=500$.


% both $\text{RMSE}_{\boldsymbol{\mathcal{B}}}$ and $\text{RMSE}_{\bd{{y}}_{\Omega^c}}$, which quantify the accuracy of the estimated coefficients and the prediction accuracy for the unobserved data, respectively. 


% by kronecker product of spatial and temporal GP processes

% $M=300$ locations, $N=100$ time points, and $P=5$ covariates following  Eq.~(\ref{Eq:STVC-vector}) with:
% \begin{equation}
% \begin{aligned}\notag
%     \boldsymbol{\mathcal{X}}(:,:,i)&=\boldsymbol{1}_{N}^{\top}\otimes\boldsymbol{x}_{s}^{i},\boldsymbol{x}_{s}^{i}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}_{M}),~i=2,3, \\
%     \boldsymbol{\mathcal{X}}(:,:,j)&=\boldsymbol{1}_{M}\otimes(\boldsymbol{x}_{t}^{j})^{\top},\boldsymbol{x}_{t}^{j}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{I}_{N}),~j=4,5. 
% \end{aligned}
% \end{equation}



\subsubsection{Results}

% \begin{figure}[!t]
% \centering
% \subfigure[Estimation results for $\bd{\cal{B}}(:,20,5)$, i.e., coefficients for the 4th predictor at the 20th time point.]{
% \includegraphics[width=0.65\textwidth]{graphical/largeSimuErrorSpacePlot3.pdf}
% }
% \subfigure[Estimation of coefficients.]{
% \includegraphics[width=0.32\textwidth]{graphical/largeSimuB3.pdf}
% }
% % \subfigure[Plots of hyperparameters]{
% % \includegraphics[width=0.225\textwidth]{graphical/largeSimuHyper.pdf}
% % }
% \subfigure[Rank effect.]{
% \includegraphics[width=0.31\textwidth]{graphical/largeSimuError2.pdf}
% }
% \caption{{BKTR modeling results on simulation 2. (a) Spatial plots of the true coefficients, estimated coefficients (posterior mean), and absolute estimation error of the coefficients for $(n=20,p=5)$. (b) Two examples of the estimated coefficients (mean with 95\% CI) for $(m=6,p=3)$ and $(m=7,p=5)$. (c) Estimation error of the coefficients and missing entries with respect to the tensor rank.}}
% \label{Fig:Simu2}
% \end{figure}




Figure~\ref{fig:simu3_new} shows the temporal behaviours of the estimated coefficients of four covariates in one simulation when $R=40$. % for one simulation and 4 covariates.
The spatial patterns of the corresponding coefficients are provided in  Appendix~\ref{appD2}. As one can see, BKTR can effectively reproduce the true values with acceptable errors. Figure~\ref{fig:simu3_rank} shows the effect of the rank. We see that choosing a larger rank $R$ gives better accuracy in terms of both $\text{MAE}_{\boldsymbol{\mathcal{B}}}/\text{RMSE}_{\boldsymbol{\mathcal{B}}}$ and $\text{MAE}_{\boldsymbol{y}_{\Omega^c}}/\text{RMSE}_{\bd{{y}}_{\Omega^c}}$. However, the accuracy gain becomes marginal when $R>30$. This demonstrates that the proposed Bayesian treatment offers a flexible solution in terms of model estimation and parameter inference.


% \begin{figure}[!t]
% \centering
% \subfigure[Estimated coefficients (mean with 95\% CI) at location $m=3$, i.e. $\boldsymbol{\mathcal{B}}(3,:,p=1,4,5,6)$.]{
% \includegraphics[width=0.96\textwidth]{graphical/simu3-temporal.pdf}
% }
% \subfigure[Interpolated spatial surfaces of the coefficients at time point $n=20$, i.e. $\boldsymbol{\mathcal{B}}(:,20,p=1,2,4,6)$, where black circles denote the positions of sampled locations.]{
% \includegraphics[width=0.93\textwidth]{graphical/simu3-spatial.pdf}
% }
% \caption{BKTR ($R=40$) estimated coefficients for Simulation 3.}
% \label{fig:simu3_new}
% \end{figure}

\begin{figure}[!t]
\centering
% \subfigure[Estimated coefficients (mean with 95\% CI) at location $m=3$, i.e. $\boldsymbol{\mathcal{B}}(3,:,p=1,4,5,6)$.]
{
\includegraphics[width=0.95\textwidth]{graphical/simu3-temporal.pdf}
}
% \subfigure[Interpolated spatial surfaces of the coefficients at time point $n=20$, i.e. $\boldsymbol{\mathcal{B}}(:,20,p=1,2,4,6)$, where black circles denote the positions of sampled locations.]{
% \includegraphics[width=0.93\textwidth]{graphical/simu3-spatial.pdf}
% }
\caption{BKTR ($R=40$) estimated coefficients for Simulation 3. The figure plots the estimated $\boldsymbol{\mathcal{B}}$ (mean with 95\% CI) of 4 covariates at location $m=3$, i.e., $\boldsymbol{\mathcal{B}}(3,:,p=1,4,5,6)$.}
\label{fig:simu3_new}
\end{figure}


\begin{figure}[!t]
\centering
\includegraphics[width=0.56\textwidth]{graphical/simu3-rank.pdf}
\caption{The effect of rank for Simulation 3. The figure shows the boxplot and mean of the estimation error on $\boldsymbol{\mathcal{B}}$ and $\boldsymbol{y}_{\Omega^c}$ with respect to the tensor rank.}
\label{fig:simu3_rank}
\end{figure}


% {\color{red} why B does not contain mean part}

\section{Bike-sharing Demand modeling}
\label{sec:application}

\subsection{Data description}

In this section we perform local spatiotemporal regression on a large-scale bike-sharing trip data set collected from BIXI\footnote{\url{https://bixi.com}}---a docked bike-sharing service in Montreal, Canada. BIXI operates 615 stations across the city and the yearly service time window is from April to November. We collect the number of daily departures for each station over $N=196$ days (from April 15 to October 27, 2019). We discard the stations with an average daily demand of less than 5, and only consider the remaining $M=587$ stations. The matrix $\bd{Y}$ contains $13.0\%$ unobserved/corrupted values in the raw data, and we only keep the rest as the observed set $\Omega$. We follow the analysis in \citep{faghih2014land} and \citep{wang2021modeling}, and consider two types of spatial covariates: (a) features related to cycling infrastructure, and (b) land-use and built environment features. For temporal covariates, we mainly consider weather conditions and holidays. Table~\ref{tab:variables} lists the 13 spatial covariates $(\boldsymbol{x}_{s}^{p}\!\in\!\mathbb{R}^{M})$ and 5 temporal covariates $(\boldsymbol{x}_{t}^{p}\!\in\!\mathbb{R}^{N})$ used in this analysis. The final number of covariates is $P\!=\!13+5+1\!=\!19$, including the intercept. In constructing the covariate tensor $\boldsymbol{\mathcal{X}}$, we follow the same approach as in the simulation experiments. Specifically, we set $\bd{\cal{X}}(:,:,1)\!=\!\operatorname{ones}(M,N)$, and fill the rest of the tensor slices with  $\boldsymbol{1}_{N}^{T}\otimes\boldsymbol{x}_{s}^{p}~(p=2:14)$ for the spatial covariates and $\boldsymbol{1}_{M}\otimes(\boldsymbol{x}_{t}^{p})^{\top}~(p=15:19)$ for the temporal covariates. Given the difference in magnitudes, we normalize all the covariates to $[0,1]$ using a min-max normalization. Since we use a zero-mean GP to parameterize $\bd{\beta}$, we normalize departure trips by removing the global effect of all covariates through a linear regression and consider $\bd{y}$ to be the unexplained part. The coefficient tensor $\bd{\cal{B}}$ contains more than $2\times 10^6$ entries, preventing us from using STVC directly. 


\begin{table}[!ht]
\footnotesize
%\setlength{\tabcolsep}{0.7mm}
    \centering
    \caption{Description summary of independent variables.}
    \begin{tabular}{ll|p{13cm}}
    \toprule
    % \multicolumn{2}{l}{Covarites} & \\
    % \midrule
    \multirow{2}{*}{spatial} & (a) & length of cycle paths; length of major roads; length of minor roads; station capacity; \\
    \cmidrule(r){2-3}
    & (b) & numbers of metro stations, bus stations, and bus routes; numbers of restaurants, universities, other business \& enterprises; park area; walkscore; population; \\
    \midrule
    \multicolumn{2}{l|}{temporal}  & daily relative humidity; daily maximum temperature;  daily mean temperature; total precipitation; dummy variables for holidays. \\
    \bottomrule
    \end{tabular}
    % \caption{Description summary of independent variables.}
    \label{tab:variables}
\end{table}

\subsection{Experimental setup}

For the spatial factors, we use a Matern 3/2 kernel as a universal prior, i.e., $k_s(\bd{s}_{m},\bd{s}_{m'})=\left(1+\frac{\sqrt{3}d}{\phi}\right)\exp{\left(-\frac{\sqrt{3}d}{\phi}\right)}$, where $d$ is the haversine distance between locations $\boldsymbol{s}_{m}$ and $\boldsymbol{s}_{m'}$, and $\phi$ is the spatial length-scale. The Matern class kernel is commonly used as a prior kernel function in spatial modeling. For the temporal factors, we use a locally periodic correlation matrix by taking the product of a periodic kernel and a SE kernel, i.e., $k_t(t_{n},t_{n'})=\exp{\left(-\frac{2\sin^2{(\pi(t_n-t_{n'})/T)}}{\gamma_{1}^{2}}-\frac{(t_n-t_{n'})^2}{2\gamma_{2}^{2}}\right)}$, where $\gamma_1$ and $\gamma_2$ denote the length-scale and decay-time for the periodic component, respectively, and we fix the period as $T=7$ days. This specification  suggests a weekly temporal pattern that can change over time, and it allows us to characterize both the daily variation and the weekly trend of the coefficients. We set the CP rank $R=20$, and run MCMC with  $K_1=500$ and $K_2=500$ iterations. 


\subsection{Results}



\begin{figure}[!t]
\centering
\subfigure[Temporal plots of coefficients.]{
\includegraphics[width=0.65\textwidth]{graphical/BIXI_TemporalCoeffi3.pdf}
}
\subfigure[Spatial maps of coefficients on August 1, 2019 $(n=109)$.]{
\includegraphics[width=0.8\textwidth]{graphical/BIXICoe_New_small_newCombine3.pdf}
}
\caption{{BKTR estimated coefficients for BIXI demand data. (a) Temporal plots of coefficients for 4 spatial covariates at location \#23 $(m=23)$ and 4 temporal covariates at location \#2 $(m=2)$, which show the mean with 95\% CI. (b) Spatial maps showing the posterior mean for 8 covariates.}}
\label{Fig:BIXI_Coe}
\end{figure}



Figure~\ref{Fig:BIXI_Coe} demonstrates some examples of the estimated coefficients, with panel (a) showing temporal examples of $\bd{\cal{B}}(\bd{s}_{m},:,:)$ and panel (b) showing spatial examples of $\bd{\cal{B}}(:,t_{n},:)$. As we can see in Figure~\ref{Fig:BIXI_Coe}(a), the temporal variation of the coefficients for both spatial and temporal covariates are interpretable. The coefficients (along with CI describing the uncertainty) allow us to identify the most important factors for each station at each time point. For example, we observe a similar variation over a week and a general long-term trend from April to October. Furthermore, the magnitude of the coefficients is much larger during the summer (July/August) compared to the beginning and end of the operation period, which is as expected as the outdoor temperature drops. Overall, for the spatial variables, the positive correlation of walkability and the negative impact caused by the length of major roads indicate that bicycle demands tend to be higher in densely populated neighborhoods. For the temporal covariates, precipitation, humidity and the holiday variable all relate to negative coefficients, implying that people are less likely to ride bicycles in rainy/humid periods and on holidays. The spatial distributions of the coefficients in Figure~\ref{Fig:BIXI_Coe}(b) also demonstrate consistent estimations, where the effects of covariates tend to be more obvious in the downtown area, involving coefficients with larger absolute values. Again, one can further explore the credible intervals to evaluate the significance of each covariate at a given location. It should be noted that the estimated variance $\tau^{-1}$ for $\boldsymbol{\epsilon}$ gives a posterior mean of $5.01\times 10^{-3}$ with a 95\% CI of $[4.94, 5.07]\times 10^{-3}$. This variance is much smaller compared with the variability of the data, confirming the importance of varying coefficients in the model. In summary, BKTR produces interpretable results for understanding the effects of different spatial and temporal covariates on bike-sharing demand. These findings can help planners to evaluate the performance of bike-sharing systems and update/design them. 



\subsection{Spatial interpolation}

Due to the introduction of GP, BKTR is able to perform spatial prediction, i.e., kriging, for unknown locations, and such results can be used to select sites for new stations. To validate the spatial prediction/interpolation capacity, we randomly select 30\% locations of the BIXI data set to be missing, and compute the prediction accuracy for these missing/testing data ($\boldsymbol{y}_{\Omega^c}$). Given that STVC is not applicable for a data set of such size, we compare BKTR (rank $R=20$) with SVC (spatially varying coefficient processes model) \citep{gelfand2003spatial} which omits the modeling in time dimension. We also test BKTR and SVC in the case where both 30\% locations and 30\% time periods are missing. The prediction errors are given in Table~\ref{tab:predictionBIXI}, where we show the RMSE and MAPE (mean absolute percentage error) on $\boldsymbol{y}_{\Omega^c}$. MAPE is used to illustrate the relative error, and the definition is also provided in Appendix~\ref{appC}. It is obvious that BKTR offers better performances with lower estimation errors in both missing scenarios, indicating its effectiveness in spatial prediction for real-world spatiotemporal data sets that often contain complicated missing/corruption patterns.



% \begin{table}[!t]
%     \centering
%     \caption{{Prediction performance for BIXI bike-sharing demand $\left(\text{RMSE}_{\boldsymbol{y}_{\Omega^c}}/\text{MAPE}_{\boldsymbol{y}_{\Omega^c}}\right)$.}}
%     \begin{tabular}{l|ccc}
%     \toprule
%     Models & 30\% location missing & 50\% location missing & 30\% location and 30\% time missing \\
%     \midrule
%     SVC & 0.26/5.84 & 0.24/10.61 & 0.20/4.22 \\
%     % SVC-approximate &  \\
%     % BTR ($R=20$) &  \\
%     BKTR ($R=20$) & \textbf{0.22}/\textbf{4.45} & \textbf{0.23}/\textbf{8.06} & \textbf{0.17}/\textbf{3.24} \\
%     % BKTR ($R=10$) & \\
%     \bottomrule
%     \end{tabular}
%     \label{tab:predictionBIXI}
% \end{table}



\begin{table}[!t]
\footnotesize
    \centering
    \caption{Prediction performance for BIXI bike-sharing demand $\left(\text{RMSE}_{\boldsymbol{y}_{\Omega^c}}/\text{MAPE}_{\boldsymbol{y}_{\Omega^c}}\right)$.}
    \begin{tabular}{l|cc}
    \toprule
    Models & 30\% location missing & 30\% location with 30\% time missing \\
    \midrule
    SVC & 0.26/5.84 & 0.20/4.22 \\
    % SVC-approximate &  \\
    % BTR ($R=20$) &  \\
    BKTR ($R=20$) & \textbf{0.22}/\textbf{4.45} & \textbf{0.17}/\textbf{3.24} \\
    % BKTR ($R=10$) & \\
    \bottomrule
    \end{tabular}
    \label{tab:predictionBIXI}
\end{table}


% {\color{red} plot factors and hyperparameters (convergence)

% we care about the hyperparameters and the beta, not about the latent factors}


\section{Discussion}
\label{sec: discuss}


\subsection{Identifiability and convergence of the model}

In the regression problem studied in this paper, the kernel/covariance hyperparameters, $\text{i.e., }\{\phi,\gamma\}$, interpret/imply the correlation structure of the data, and the coefficients, i.e., $\boldsymbol{\mathcal{B}}$, reveal the spatiotemporal processes of the underlying associations. Thus, we consider the identifiability and convergence of kernel hyperparameters and the coefficients to be crucial. In contrast, note that the identifiability of latent factors decomposed from $\boldsymbol{\mathcal{B}}$ are not that important. For instance, our model is invariant in applying the same column permutation to the three factor matrices. From the results in simulation experiments (see Figure~\ref{fig:simu2_kernelHyper}), it is clear that the Markov chains for kernel hyperparameters converge fast and well when using the proposed BKTR model. BKTR can also yield identifiable hyperparameters and reliable estimations for the coefficients. 



% It should be noted that the identifiability and convergence of kernel/covariance hyperparameters ($\phi$,$\gamma$) and the coefficients ($\boldsymbol{\mathcal{B}}$) are crucial in the spatiotemporal regression problem studied in this paper. 


% \begin{itemize}
%     \item the identifiability of factors are not important/required, since it does not affect the estimation results
%     \item the identifiability of hyperparameters is what we cared, it implies the correlation structure of the data
% \end{itemize}


% \subsection{The effect of rank and missing rate}
\subsection{The superiority of the BKTR framework}
% As we can see from the test of different rank settings, BKTR 

% The calculation problem of STVC when there exists missing data 

As we can see from the test of different rank settings and observation rates in the simulation experiments (see Figure~\ref{fig:simu1_new_rank_missing} and \ref{fig:simu3_rank}), BKTR is able to provide high estimation accuracy and valid CIs even with a much larger or over-specified rank, and also effectively estimates the coefficients and the unobserved output values when only 10\% of the data is observed. This indicates the advantage of the proposed Bayesian low-rank framework. Since we introduce a fully Bayesian sampling treatment for the kernelized low-rank tensor model which is free from parameter tuning, BKTR can estimate the model parameters and hyperparameters even when only a small number of observations are available. Thus, the model can avoid the over-fitting issue and consistently offer reliable estimation results, implying its effectiveness and usability for real-world complex spatiotemporal data analysis.


Another benefit of the proposed framework is the highly improved computing efficiency. Compared to the original STVC approach \citep{gelfand2003spatial}, which requires $\mathcal{O}\left(|\Omega|^3\right)$ for hyperparameter learning and $\mathcal{O}\left(|\Omega|^3P^3\right)$ for coefficient variable learning, BKTR reduces both of these computational costs to $\mathcal{O}\left(R^3\left(M^3+N^3+P^3\right)\right)$. The complexity can be further decreased to $\mathcal{O}\left(R\left(M^3+N^3+P^3\right)\right)$ by updating the correlation matrices for each column of factors separately and meanwhile sampling factors column by column. Such substantial gains in computing time allows us to analyze large-scale real-world spatiotemporal data and multi-dimensional relations, where generally the STVC is infeasible. 


% for learning kernel hyperparameters from   and latent variables to $\mathcal{O}\left(R^3\left(M^3+N^3+P^3\right)\right)$ $\mathcal{O}\left(\|\Omega\|^3\right))$ to $\m$ 

% Bayesian framework

% \subsection{Convergence}


\subsection{Counterpart kernel expression for $\boldsymbol{\beta}$}

It has been shown that the classical low-rank tensor regression is equivalent to learning a multi-linear GP model with covariance matrix being the Kronecker product of several low-rank kernels capturing the correlations across different dimensions \citep{chu2009probabilistic,yu2018tensor}. Furthermore, the general probabilistic CP decomposition for the coefficient tensor $\boldsymbol{\mathcal{B}}$ can be formulated as:
\begin{equation}
    \boldsymbol{\mathcal{B}}=\sum_{r=1}^{R}s_r\times\left( \boldsymbol{u}_{r}\circ\boldsymbol{v}_{r}\circ\boldsymbol{w}_{r}\right),  \ s_r\sim \mathcal{N}\left(0,1\right) \text{ for } r=1,\cdots,R, \ R\le \min \{M,N\}.
\end{equation} % By integrating out $s_r$, 
By marginalizing over $\boldsymbol{\beta}$, we have the marginal distribution of the data 
\begin{equation}\label{eq:GPCP}
    \boldsymbol{y}_\Omega \;|\; \bd{U},\bd{V},\bd{W} \sim  \mathcal{N}\Big(\boldsymbol{0},\boldsymbol{O}\underbrace{\left(\sum_{r=1}^R\left(\boldsymbol{v}_{r}\boldsymbol{v}_{r}^{\top}\right)\otimes\left(\boldsymbol{u}_{r}\boldsymbol{u}_{r}^{\top}\right)\otimes\left(\boldsymbol{w}_{r}\boldsymbol{w}_{r}^{\top}\right)\right)}_{\boldsymbol{K}_{\beta}}\boldsymbol{O}^{\top}+\tau^{-1}\boldsymbol{I}_{|\Omega|}\Big).
\end{equation}
In this sense, the CP decomposition-based tensor regression is equivalent to learning $\boldsymbol{\beta}$ with a covariance matrix comprised of the sum of $R$ separable kernel matrices in which each component is a rank-one matrix. The key difference is that in BKTR we impose spatial/temporal GP priors on $\boldsymbol{u}_{r}\in\mathbb{R}^{M}\sim\mathcal{GP}(\boldsymbol{0},\boldsymbol{K}_{s})$ and  $\boldsymbol{v}_{r}\in\mathbb{R}^{N}\sim\mathcal{GP}(\boldsymbol{0},\boldsymbol{K}_{t})$, and a Wishart prior on  $\boldsymbol{w}_{r}\in\mathbb{R}^{P}\sim\mathcal{N}(\boldsymbol{0},\boldsymbol{\Lambda}^{-1})$.
% (similar to Eq.~\eqref{eq:factorUV} and \eqref{eq:factorW}).

% Based on the connection between tensor regression and multi-linear GP, we can write the equivalent kernel expression for the original coefficient variable in STVC modeling (i.e., $\boldsymbol{\beta}=\left[\boldsymbol{\beta}\left(\bd{s}_1,t_1\right);\cdots;\bd{\beta}\left(\bd{s}_M,t_1\right);\bd{\beta}\left(\bd{s}_1,t_2\right);\cdots;\boldsymbol{\beta}\left(\bd{s}_M,t_N\right) \right]\in\mathbb{R}^{MNP}$ or $\operatorname{vec}\left(\boldsymbol{B}_{(3)}\right)$) when using the proposed BKTR approach:
% \begin{equation}
%     \boldsymbol{\beta}\sim\mathcal{N}\Big(\boldsymbol{0},\underbrace{\sum_{r=1}^R\left(\boldsymbol{v}_{r}\boldsymbol{v}_{r}^{\top}\right)\otimes\left(\boldsymbol{u}_{r}\boldsymbol{u}_{r}^{\top}\right)\otimes\left(\boldsymbol{w}_{r}\boldsymbol{w}_{r}^{\top}\right)}_{\boldsymbol{K}_{\beta}}\Big),
% \end{equation}
%This suggests that BKTR based on the CP decomposition of $\boldsymbol{\mathcal{B}}$ is equivalent to learn $\boldsymbol{\beta}$ with a correlation matrix comprised by the sum of $R$ rank-one kernel matrices. We put GP priors on $\{\boldsymbol{u}_r,\boldsymbol{v}_r,\boldsymbol{w}_r\}$ to encode local dependencies/variations in three dimensions of the association process.


We can also choose the probabilistic Tucker decomposition \citep{kolda2009tensor} to parameterize $\bd{\beta}$ as a rank-$(R_1,R_2,R_3)$ Tucker tensor with $R_1< M$, $R_2< N$ and $R_3< P$:
\begin{equation}
    \boldsymbol{\mathcal{B}}=\sum_{r_1=1}^{R_1}\sum_{r_2=1}^{R_2}\sum_{r_3=1}^{R_3}s_{r_1,r_2,r_3}\times\left( \boldsymbol{u}_{r_1}\circ\boldsymbol{v}_{r_2}\circ\boldsymbol{w}_{r_3}\right),
\end{equation}
where $\bd{\cal{S}}=\left(s_{r_1,r_2,r_3}\right)\in \mathbb{R}^{R_1\times R_2 \times R_3}$ is the core tensor with $s_{r_1,r_2,r_3}\sim \mathcal{N}\left(0,1\right)$ for $r_1=1,\cdots,R_1,\ r_2=1,\cdots,R_2,\ r_3=1,\cdots,R_3$. % Assuming that each element in $\bd{\cal{S}}$ has a prior of $s_{r_1,r_2,r_3}\sim \mathcal{N}\left(0,1\right)$ for $r_1=1,\cdots,R_1,\ r_2=1,\cdots,R_2,\ r_3=1,\cdots,R_3$. 
We have the following marginal Gaussian distribution for observed data: % after marginalizing $\bd{\cal{S}}$:
% \begin{equation}\label{eq:GPTucker}
%         \boldsymbol{\beta} \;|\; \bd{U},\bd{V},\bd{W} \sim  \mathcal{N}\Big(\boldsymbol{0}, \underbrace{\boldsymbol{V}\boldsymbol{V}^{\top}\otimes\boldsymbol{U}\boldsymbol{U}^{\top}\otimes\boldsymbol{W}\boldsymbol{W}^{\top}}_{\boldsymbol{K}_{\beta}}\Big),
% \end{equation}
\begin{equation}\label{eq:GPTucker}
        \boldsymbol{y}_{\Omega} \;|\; \bd{U},\bd{V},\bd{W} \sim  \mathcal{N}\Big(\boldsymbol{0}, \boldsymbol{O}\underbrace{\left(\boldsymbol{V}\boldsymbol{V}^{\top}\otimes\boldsymbol{U}\boldsymbol{U}^{\top}\otimes\boldsymbol{W}\boldsymbol{W}^{\top}\right)}_{\boldsymbol{K}_{\beta}}\boldsymbol{O}^{\top}+\tau^{-1}\boldsymbol{I}_{|\Omega|}\Big),
\end{equation}
where $\boldsymbol{U}\in\mathbb{R}^{M\times R_1}$, $\boldsymbol{V}\in\mathbb{R}^{N\times R_2}$, and $\boldsymbol{W}\in\mathbb{R}^{P\times R_3}$. As we can see, this formulation imposes a similar separable kernel structure as in the original STVC model \citep{gelfand2003spatial}. When comparing Eq.~\eqref{eq:GPTucker} with $\bd{\beta}\sim \mathcal{N}(\bd{0}, \bd{K}_{\beta_t}\otimes \bd{K}_{\beta_s} \otimes \bd{\Lambda}_{\beta}^{-1})$ in the original STVC, we see that the Tucker decomposition-based formulation translates to learning a low-rank approximation for each component correlation/covariance matrix in the separable kernel matrix \citep{yu2018tensor}. Note, however, that in Eq.~\eqref{eq:GPTucker} the spatial and temporal kernels are linear (e.g., $\bd{K}=\bd{U}\bd{U}^{\top}$) and thus they are nonstationary. To incorporate spatial and temporal dependencies, Tucker-based BKTR can also be designed by imposing spatial and temporal GP priors on the columns of $\bd{U}$ and $\bd{V}$. For example, if we assume each column of $\bd{U}$ follows an independent zero-mean GP, i.e., $\boldsymbol{u}_{r_1}\sim\mathcal{GP}\left(\boldsymbol{0},\boldsymbol{K}_{s}\right)$ for $r_1=1,\cdots,R_1$ with $\boldsymbol{K}_{s}$ being a spatial correlation matrix, we have the covariance matrix $\bd{K}=\bd{U}\bd{U}^{\top}\sim {\cal{W}}_M\left(\bd{K}_{s},R_1\right)$, which follows a singular Wishart distribution with degrees of freedom $R_1<M$ and scale matrix $\bd{K}_{s}$ \citep{uhlig1994singular}.
%For example, if we assume columns of $\bd{U}$ follow independent mean-zero Gaussian processes $\boldsymbol{u}_{r}\sim\mathcal{GP}\left(\boldsymbol{0},\boldsymbol{K}_{s}\right)$ for $r=1,\cdots,R_1$ where $\boldsymbol{K}_{s}$ is a spatial correlation matrix, the covariance $\bd{K}=\bd{U}\bd{U}^{\top}$ actually becomes a spatial Wishart process ${\cal{SW}}_M\left(R_1,\bd{I}_M,\bd{K}_s\right)$ with $R_1$ degrees of freedom \citep{gelfand2004nonstationary}. 
Thus, Tucker-BKTR becomes a computationally tractable approximation of STVC and its performance will largely depend on how well $\bd{K}_{\beta_s}$, $\bd{K}_{\beta_t}$ and $\bd{\Lambda}_{\beta}^{-1}$ can be approximated by the low-rank singular Wishart structure (up to a scaling constant). Overall, we can see that kernelized matrix/tensor decomposition provides a new approach to model multivariate spatial/spatiotemporal processes. 

%The model inference of Tucker-BKTR is similar to the process of CP-BKTR, with one additional latent variable $\bd{\cal{S}}$ to be sampled. Essentially, Tucker decomposition allows us to use a much smaller rank than that of CP, and thus we can expect the inference of latent factor matrices to be more efficient.

%We expect the time complexity for the inference of kernel hyperparameters and latent variables in Tucker-BKTR is similar to the costs when using CP decomposition, since the the dimension of the core tensor is often much smaller than the data size.




The low rank factorization-based models provide an alternative approach to model multivariate spatial/spatiotemporal processes, in particular when we have a large number of features (i.e., $P\gg 1$). It should be noted that  estimating/marginalizing $\boldsymbol{U}$, $\boldsymbol{V}$ and $\boldsymbol{W}$ as latent factor matrices in Bayesian tensor regression/decomposition is much easier than estimating them as GP latent variables in the covariance function in Eqs.~\eqref{eq:GPCP} and \eqref{eq:GPTucker} \citep{lawrence2003gaussian,li2009latent,yu2018tensor}. Again, % we would like to highlight that 
one can apply different kernel functions/hyperparameters as the prior for different columns of the factors, making the model more adaptive and flexible in characterizing the complex spatiotemporal patterns in the data. This will also lead to a more flexible Wishart distribution. 




%{\color{red} Equivalent to Spatial Wishart Processes with degrees of freedom $R_1$ and $R_2$ (Gelfand, Test Paper) \citet{gelfand2004nonstationary}}

%These equivalent kernel expressions also indicate that, by applying the proposed BKTR framework, we provide a solution to offer such expressive low-rank multi-linear kernels, which are usually unavailable to optimize directly since the number of parameters is unaffordable when taking the whole kernel matrix as model parameters.


%The learned covariance matrix of $\boldsymbol{\beta}$ in BKTR is more expressive than that in the original STVC model \citep{gelfand2003spatial}, where the correlation matrix in each dimension is calculated from a single GP, i.e., $\boldsymbol{K}_{\beta}=\boldsymbol{K}_{\beta_t}\otimes\boldsymbol{K}_{\beta_s}\otimes\boldsymbol{K}_{\beta_w}$. Note that we can also approximate $\boldsymbol{\beta}$ using Tucker decomposition, and the equivalent covariance matrix for $\boldsymbol{\beta}$ becomes a multi-linear correlation matrix in the form of the Kronecker product of three low-rank kernel matrices, i.e., $\boldsymbol{K}_{\beta}=\boldsymbol{V}\boldsymbol{V}^{\top}\otimes\boldsymbol{U}\boldsymbol{U}^{\top}\otimes\boldsymbol{W}\boldsymbol{W}^{\top}$, where $\boldsymbol{U}\in\mathbb{R}^{M\times R}$, $\boldsymbol{V}\in\mathbb{R}^{N\times R}$, and $\boldsymbol{W}\in\mathbb{R}^{P\times R}$ ($R\ll\{M,N\}$) are low-rank factor matrices and follow GPs. 







% \begin{equation}
% \begin{aligned}
%     \operatorname{vec}\left(\boldsymbol{B}_{(3)}\right)&\sim\mathcal{N}\left(\boldsymbol{0},\boldsymbol{K}_{\boldsymbol{\beta}_{t}}\otimes\boldsymbol{K}_{\boldsymbol{\beta}_{s}}\otimes\boldsymbol{\Lambda}_{\boldsymbol{\beta}}^{-1}\right), \\
%     \boldsymbol{K}_{\boldsymbol{\beta}_{s}}=\boldsymbol{U}_{\beta}&\boldsymbol{U}_{\beta}^{\top}, \boldsymbol{K}_{\boldsymbol{\beta}_{t}}=\boldsymbol{V}_{\beta}\boldsymbol{V}_{\beta}^{\top},\boldsymbol{\Lambda}_{\boldsymbol{\beta}}^{-1}=\boldsymbol{W}_{\beta}\boldsymbol{W}_{\beta}^{\top},
% \end{aligned}
% \end{equation}
% where $\boldsymbol{U}_{{\beta}}\in\mathbb{R}^{M\times R}$, $\boldsymbol{V}_{\beta}\in\mathbb{R}^{N\times R}$, $\boldsymbol{W}_{\beta}\in\mathbb{R}^{P\times R}$ ($R\ll\{M,N\}$) are the factor matrices and assumed to follow different GPs (similar to Eq.~(\ref{eq:factorUV}) and (\ref{eq:factorW})). This means that BKTR is equivalent to learn a multi-linear correlation matrix comprised by low-rank kernel matrices (i.e., $\boldsymbol{K}_{\boldsymbol{\beta}_{s}}$, $\boldsymbol{K}_{\boldsymbol{\beta}_t}$) for $\boldsymbol{\beta}$. These low-rank kernel matrices can be factorized into lower-dimensional factor matrices that are generated from GP models.

% {\color{red} in fact, this also lead to a non-stationary kernel. Each column in $U$ follows a stationary GP but $U*U^{t}$ is a non-stationary kernel}

% We solve the low-rank kernel from the counterpart regression problem, decreasing the parameters of kernel, making this solution available. 

% directly decomposed from $\boldsymbol{\mathcal{B}}$, i.e., $\boldsymbol{U}$, $\boldsymbol{V}$, and $\boldsymbol{W}$, generating from GP models.


% i.e., $\boldsymbol{K}_{\boldsymbol{\beta}_{s}}$, $\boldsymbol{K}_{\boldsymbol{\beta}_t}$ are low-rank kernel, and the matrices decomposed from the kernel matrices, i.e., are generated from GP models.

% BKTR equals to learn coefficient tensor following a tensor normal prior 

% There are some connections between low-rank tensor regression and Gaussian process models with low-rank kernel 

% BKTR equals to learn a   low-rank kernel, i.e., 


\section{Conclusion}
\label{sec: conclu}

This paper introduces an effective solution for large-scale local spatiotemporal regression analysis. We propose parameterizing the model coefficients using low-rank CP decomposition, which greatly reduces the number of parameters from $M\times N\times P$ to $R(M+N+P)$. Contrary to previous studies on tensor regression, the proposed model BKTR goes beyond the low-rank assumption by integrating GP priors to characterize the strong local spatial and temporal dependencies. The framework also learns a low-rank multi-linear kernel which is expressive and able to provide insights for nonstationary and complicated processes. Our numerical experiments on both synthetic data and real-world data suggest that BKTR can reproduce the local spatiotemporal processes efficiently and reliably. 

% {\color{red} Can place different kernel (length-scale) for each factor. as in the original GPFA paper of Luttien }

% {\color{red} Also provides some insight for non-stationary process: the multilinear kernel is non-stationary.}

There are several directions for future research. In the current model, the CP rank $R$ needs to be specified in advance. One can make the model more flexible and adaptive by introducing a reasonably large core tensor with a multiplicative gamma process prior such as in \citet{rai2014scalable}. The separable kernel assumption might be too restrictive, so the model can be further extended by introducing nonseparable kernel structure. In terms of GP priors, BKTR is flexible and can accommodate different kernels (w.r.t. function form and hyperparameter) for different factors such as in \citet{luttinen2009variational}. The combination of different kernels can also produce richer spatiotemporal dynamics and multiscale properties. In terms of computation, one can further reduce the cost in the GP learning (e.g., $\mathcal{O}(M^3)$ for a spatial kernel) by further introducing sparse approximation techniques such as inducing points and predictive processes \citep{quinonero2005unifying,banerjee2014hierarchical}. Lastly, the regression model itself can be extended from continuous to generalized responses, such as binary or count data, by introducing proper link functions  \citep[see e.g.,][]{polson2013bayesian}. 


\section*{Acknowledgment}
This research is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC), the Fonds de recherche du Qu\'ebec-Nature et technologies (FRQNT), and the Canada Foundation for Innovation (CFI). Mengying Lei would like to thank the Institute for Data Valorization (IVADO) for providing a scholarship to support this study.








\bibliographystyle{agsm}

\bibliography{ref}





\begin{center}
\section*{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}
\addcontentsline{toc}{section}{Appendices}
\renewcommand{\thesubsection}{\Alph{subsection}}


\subsection{Calculation of the marginal likelihood}\label{appA}

We use the Woodbury matrix identity ${\footnotesize{(\boldsymbol{A}+\boldsymbol{C}\boldsymbol{B}\boldsymbol{C}^{\top})^{-1}=\boldsymbol{A}^{-1}-\boldsymbol{A}^{-1}\boldsymbol{C}\left(\boldsymbol{B}^{-1}+\boldsymbol{C}^{\top}\boldsymbol{A}^{-1}\boldsymbol{C}\right)^{-1}}}\\{\footnotesize{\boldsymbol{C}^{\top}\boldsymbol{A}^{-1}}}$ to compute $\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}^{-1}$, and further calculate $\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}^{-1}\boldsymbol{y}_{\Omega}$ in Eq.~\eqref{Eq:Marginal} as follows:
\begin{equation}\notag
\begin{aligned}
\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}^{-1}\boldsymbol{y}_{\Omega}&=\boldsymbol{y}_{\Omega}^{\top}\left(\boldsymbol{H}_{U}\boldsymbol{K}_{U}\boldsymbol{H}_{U}^{\top}+\tau^{-1}\boldsymbol{I}_{|\Omega|}\right)^{-1}\boldsymbol{y}_{\Omega} \\
&=\boldsymbol{y}_{\Omega}^{\top}\left(\tau\boldsymbol{I}_{|\Omega|}-\tau^2\boldsymbol{H}_{U}\left(\boldsymbol{K}_{U}^{-1}+\tau\boldsymbol{H}_{U}^{\top}\boldsymbol{H}_{U}\right)^{-1}\boldsymbol{H}_{U}^{\top}\right)\boldsymbol{y}_{\Omega} \\
&=\tau\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{y}_{\Omega}-\tau^2\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{H}_{U}\left(\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{s}^{-1}+\tau\boldsymbol{H}_{U}^{\top}\boldsymbol{H}_{U}\right)^{-1}\boldsymbol{H}_{U}^{\top}\boldsymbol{y}_{\Omega},
\end{aligned}
\end{equation}
where $\boldsymbol{H}_{U}=\boldsymbol{O}\tilde{\bd{X}}_{U}\left((\boldsymbol{W}\odot\boldsymbol{V})\otimes\boldsymbol{I}_{M}\right)$ with $\tilde{\boldsymbol{X}}_{U}=\left(\boldsymbol{X}_{(3)}\odot\boldsymbol{I}_{MN}\right)^{\top}$. Based on the matrix determinant lemma $\left|\boldsymbol{A}+\boldsymbol{C}\boldsymbol{B}\boldsymbol{C}^{\top}\right|=\left|\boldsymbol{B}^{-1}+\boldsymbol{C}^{\top}\boldsymbol{A}^{-1}\boldsymbol{C}\right|\det{(\boldsymbol{B})}\det{(\boldsymbol{A})}$, the determinant in Eq.~\eqref{Eq:Marginal}, i.e., $\log{\left|\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}\right|}$, is computed as below:
\begin{equation} \notag
\begin{aligned}
\log{\left|\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}\right|}&=\log{\left|\boldsymbol{H}_{U}\boldsymbol{K}_{U}\boldsymbol{H}_{U}^{\top}+\tau^{-1}\boldsymbol{I}_{|\Omega|}\right|} \\
&=\log{\left|\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{s}^{-1}+\tau\boldsymbol{H}_{U}^{\top}\boldsymbol{H}_{U}\right|}+R\log{\left|\boldsymbol{K}_{s}\right|}-|\Omega|\log{\tau}.
\end{aligned}
\end{equation}
Therefore, the log marginal posterior of $\phi$ is updated with:
\begin{equation} \notag
\begin{aligned}
\log{p\left(\phi\given\boldsymbol{y}_{\Omega},\boldsymbol{V},\boldsymbol{W},\tau,\boldsymbol{\mathcal{X}}\right)}\propto&\log{p(\phi)}-\frac{1}{2}\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}^{-1}\boldsymbol{y}_{\Omega}-\frac{1}{2}\log{\left|\boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}\right|} \\
\propto&\log{p(\phi)}+\frac{1}{2}\tau^2\boldsymbol{y}_{\Omega}^{\top}\boldsymbol{H}_{U}\left(\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{s}^{-1}+\tau\boldsymbol{H}_{U}^{\top}\boldsymbol{H}_{U}\right)^{-1}\boldsymbol{H}_{U}^{\top}\boldsymbol{y}_{\Omega} \\
&-\frac{1}{2}\log{\left|\boldsymbol{I}_{R}\otimes\boldsymbol{K}_{s}^{-1}+\tau\boldsymbol{H}_{U}^{\top}\boldsymbol{H}_{U}\right|}-\frac{R}{2}\log{\left|\boldsymbol{K}_{s}\right|}.
\end{aligned}
\end{equation}



\subsection{Sampling algorithm for kernel hyperparameters}\label{appB}


The detailed sampling process for kernel hyperparmameters is provided in Algorithm~\ref{alg:HyperSample}.


\begin{algorithm}[!ht]
% \small
\caption{Sampling process for kernel hyperparameter $\phi$}
\label{alg:HyperSample}
\KwIn{$\phi$, $\boldsymbol{V},\boldsymbol{W},\boldsymbol{y}_{\Omega},\boldsymbol{\mathcal{X}},\tau$.}
\KwOut{next $\phi$.}
Initialize slice sampling scale $\rho=\log(10)$. \\
Compute $\boldsymbol{K}_{s}, \boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi}, \log{p\left(\phi\given\boldsymbol{y}_{\Omega},\boldsymbol{V},\boldsymbol{W},\tau,\boldsymbol{\mathcal{X}}\right)}$ corresponding to $\phi$; \\
% \begin{algorithmic} [1]
% Generate surrogate data: $\boldsymbol{G}^{u}=[\boldsymbol{g}_{1}^{u},\cdots,\boldsymbol{g}_{R}^{u}]$ with $\boldsymbol{g}_{r}^{u}\sim\mathcal{N}(\boldsymbol{u}_{r},\alpha\boldsymbol{I}_{M})$; \\
% Calculate the implied whitened variable: $\boldsymbol{q}_{r}^{u}=\boldsymbol{L}_{\Sigma}^{-1}\left(\boldsymbol{u}_{r}-\boldsymbol{\mu}_{r}\right),~r=1,\cdots,R$, ~~~ ~~~~~~~ where $\boldsymbol{\Sigma}=\left(\alpha^{-1}\boldsymbol{I}_{M}+(\boldsymbol{K}_{s})^{-1}\right)^{-1}$, $\boldsymbol{\mu}_{r}=\alpha^{-1}\boldsymbol{\Sigma}\boldsymbol{g}_{r}^{u}$, and $\boldsymbol{\Sigma}=\boldsymbol{L}_{\Sigma}\boldsymbol{L}_{\Sigma}^{\top}$; \\
Calculate the sampling range: $\delta\sim\text{Uniform}(0,\rho)$, $\phi_{\min}=\phi-\delta$, $\phi_{\max}=\phi_{\min}+\rho$; \\
Draw $\eta\sim\text{Uniform}(0,1)$; \\
\While{True}{
Draw proposal $\phi'\sim\text{Uniform}(\phi_{\min},\phi_{\max})$; \\
Compute $\boldsymbol{K}_{s}', \boldsymbol{K}_{\left.\boldsymbol{y}\right|\phi'}, \log{p\left(\phi'\given\boldsymbol{y}_{\Omega},\boldsymbol{V},\boldsymbol{W},\tau,\boldsymbol{\mathcal{X}}\right)}$ corresponding to $\phi'$; \\
% Compute $\boldsymbol{K}_{s}', \boldsymbol{\Sigma}', \boldsymbol{\mu}_{r}'$ corresponding to $\phi'$; \\
% Compute $\boldsymbol{u}_{r}'=\boldsymbol{L}_{\Sigma'}\boldsymbol{q}_{r}^{u}+\boldsymbol{\mu}_{r}', r=1,\cdots,R$,~$\boldsymbol{U}'=[\boldsymbol{u}_{1},\cdots,\boldsymbol{u}_{R}]$; \\
% \uIf{$\frac{p\left(\boldsymbol{y}_{\Omega}\given\boldsymbol{U}',\boldsymbol{V},\boldsymbol{W},\boldsymbol{\mathcal{X}},\tau\right)\mathcal{N}\left(\boldsymbol{G}^{u}\given\boldsymbol{0},\boldsymbol{K}_{s}'+\alpha\boldsymbol{I}_{M}\right)p(\phi')}{p\left(\boldsymbol{y}_{\Omega}\given\boldsymbol{U},\boldsymbol{V},\boldsymbol{W},\boldsymbol{\mathcal{X}},\tau\right)\mathcal{N}\left(\boldsymbol{G}^{u}\given\boldsymbol{0},\boldsymbol{K}_{s}+\alpha\boldsymbol{I}_{M}\right)p(\phi)}>\eta$}
\uIf{$\exp{\big(\log{p\left(\phi'\given\boldsymbol{y}_{\Omega},\boldsymbol{V},\boldsymbol{W},\tau,\boldsymbol{\mathcal{X}}\right)}-\log{p\left(\phi\given\boldsymbol{y}_{\Omega},\boldsymbol{V},\boldsymbol{W},\tau,\boldsymbol{\mathcal{X}}\right)}\big)}>\eta$}
{\Return $\phi'$; \textbf{break};}
\uElseIf{$\phi'<\phi$}
    {$\phi_{\min}=\phi'$;}
\Else{$\phi_{\max}=\phi'$.}
}
% \end{algorithmic}
\end{algorithm}



\subsection{Evaluation metrics}\label{appC}

The metrics for assessing the estimation accuracy on coefficients $\boldsymbol{\mathcal{B}}$ ($\text{MAE}_{\boldsymbol{\mathcal{B}}}/\text{RMSE}_{\boldsymbol{\mathcal{B}}}$) and unobserved output/response entries $\boldsymbol{y}_{\Omega^c}$ ($\text{MAE}_{\boldsymbol{y}_{\Omega^c}}/\text{RMSE}_{\boldsymbol{y}_{\Omega^c}}/\text{MAPE}_{\boldsymbol{y}_{\Omega^c}}$) are defined as:
\begin{equation*}
\begin{aligned}
\footnotesize
&\text{MAE}_{\boldsymbol{\mathcal{B}}}=\frac{1}{MNP}\sum_{m=1}^{M}\sum_{n=1}^{N}\sum_{p=1}^{P}\left|b_{mnp}-\hat{b}_{mnp}\right|, \text{RMSE}_{\boldsymbol{\mathcal{B}}}=\sqrt{\frac{1}{MNP}\sum_{m=1}^{M}\sum_{n=1}^{N}\sum_{p=1}^{P}\left(b_{mnp}-\hat{b}_{mnp}\right)^2}, \\
&\text{MAE}_{\boldsymbol{y}_{\Omega^c}}=\frac{1}{MNP-|\Omega|}\sum_{i\notin\Omega}\left|y_{i}-\hat{y}_{i}\right|, \text{RMSE}_{\boldsymbol{y}_{\Omega^c}}=\sqrt{\frac{1}{MNP-|\Omega|}\sum_{i\notin\Omega}\left(y_{i}-\hat{y}_{i}\right)^2}, \\
&\text{MAPE}_{\boldsymbol{y}_{\Omega^c}}=\frac{1}{MNP-|\Omega|}\sum_{i\notin\Omega}\left|\frac{y_{i}-\hat{y}_{i}}{y_{i}}\right|,
\end{aligned}
\end{equation*}
where $\hat{b}_{mnp}$ and $b_{mnp}$ are the $(m,n,p)$th element of the estimated and the setting/true coefficient tensor, respectively, $y_{i}$ and $\hat{y}_{i}$ are the actual value and estimation of $i$th unobserved data, respectively. The criteria related to quantifying the uncertainty performance, i.e., CVG (interval coverage), INT (interval score), and CRPS (continuous rank probability score) of the 95\% posterior CI for the estimated $\boldsymbol{\mathcal{B}}$ values, are defined as follows:
\begin{equation} \notag
\begin{aligned}
\text{CVG}=&\frac{1}{MNP}\sum_{m=1}^{M}\sum_{n=1}^{N}\sum_{p=1}^{P}\mathbbm{1}\left\{{b}_{mnp}\in\left[l_{mnp},u_{mnp}\right]\right\}, \\
\text{INT}=&\frac{1}{MNP}\sum_{m=1}^{M}\sum_{n=1}^{N}\sum_{p=1}^{P}\left(u_{mnp}-l_{mnp}\right)+\frac{2}{\alpha}\left(l_{mnp}-b_{mnp}\right)\mathbbm{1}\{b_{mnp}<l_{mnp}\} \\
&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~+\frac{2}{\alpha}\left(b_{mnp}-u_{mnp}\right)\mathbbm{1}\{b_{mnp}>u_{mnp}\}, \\
\text{CRPS}=&-\frac{1}{MNP}\sum_{m=1}^{M}\sum_{n=1}^{N}\sum_{p=1}^{P}\sigma_{mnp}\Bigg[\frac{1}{\sqrt{\pi}}-2\psi\left(\frac{b_{mnp}-\hat{b}_{mnp}}{\sigma_{mnp}}\right) \\
&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~-\frac{b_{mnp}-\hat{b}_{mnp}}{\sigma_{mnp}}\left(2\Phi\left(\frac{b_{mnp}-\hat{b}_{mnp}}{\sigma_{mnp}}\right)-1\right)\Bigg], \\
\end{aligned}
\end{equation}
where $\psi$ and $\Phi$ denote the pdf (probability density function) and cdf (cumulative distribution function) of a standard normal distribution, respectively; $\sigma_{mnp}$ is the standard deviation (std.) of the estimation values after burn-in for $(m,n,p)$th entry of $\boldsymbol{\mathcal{B}}$, i.e., the std. of $\left\{\Tilde{\boldsymbol{\mathcal{B}}}^{(k)}\right\}_{k=K_1+1}^{K_1+K_2}$, $\alpha=0.05$, $[l_{mnp},u_{mnp}]$ denotes the 95\% central estimation interval for each coefficient value, and $\mathbbm{1}\{\cdot\}$ represents an indicator function that equals 1 if the condition is true and 0 otherwise.









% For the estimated coefficient tensor, we use $\text{CI}_{m,n,p}^{\boldsymbol{\mathcal{B}}}$ to denote the 95\% posterior CI of its $(m,n,p)$th element. The criteria related to the significance test, i.e., Average coverage, Average power, and Type-\uppercase\expandafter{\romannumeral1} error, are defined as follows:
% \begin{equation*}
% \begin{aligned}
% % \footnotesize
% \text{Average coverage}~(95\% \text{CI})&=\frac{1}{MNP}\sum_{m=1}^{M}\sum_{n=1}^{N}\sum_{p=1}^{P}\mathbbm{1}_{\text{CI}_{m,n,p}^{\boldsymbol{\mathcal{B}}}}\left({b}_{m,n,p}\right), \\
% \text{Average power}~(95\% \text{CI})&=1-\frac{1}{MN(P-1)}\sum_{m=1}^{M}\sum_{n=1}^{N}\sum_{p=2}^{P-1}\mathbbm{1}_{\text{CI}_{m,n,p}^{\boldsymbol{\mathcal{B}}}}(0), \\
% \text{Type-\uppercase\expandafter{\romannumeral1} error}~(95\% \text{CI})&=1-\frac{1}{MN}\sum_{m=1}^{M}\sum_{n=1}^{N}\mathbbm{1}_{\text{CI}_{m,n,p=P}^{\boldsymbol{\mathcal{B}}}}(0),
% \end{aligned}
% \end{equation*}
% where $\mathbbm{1}_{\text{CI}_{m,n,p}^{\boldsymbol{\mathcal{B}}}}(x)=1$ if $x\in\text{CI}_{m,n,p}^{\boldsymbol{B}}$, else $\mathbbm{1}_{\text{CI}_{m,n,p}^{\boldsymbol{\mathcal{B}}}}(x)=0$. Note that we assume the last spatiotemporal covariate (i.e., $p=P$) to be the random non-significant covariates.















\subsection{Supplementary results for simulation experiments}
% \label{appD}

% \subsubsection{Simulation 1}

% For Simulation 1, we show the temporal estimations of coefficients at a given location obtained in different cases in Figure~\ref{fig:simu1_new_compare}. The plots in Panel (a) indicate that higher rank specification generates broader intervals, which is consistent with the results given in Figure~\ref{fig:simu1_new_rank_missing} (a), i.e., the average power and Type-\uppercase\expandafter{\romannumeral1} error become lower with over-specified ranks. Panel (b) shows that BKTR can accurately estimate time-varying coefficients even when only 10\% of the response data is observed, proving its high modelling capacity.



% \begin{figure}[!ht]
% \centering
% \subfigure[Coefficients ($\boldsymbol{\mathcal{B}}(3,:,6)$) estimated by BKTR with different rank settings (when 50\% of data is observed).% (under 50\% random missing).
% ]{
% \includegraphics[width=0.99\textwidth]{graphical/simu1-temporaCompare_Rank.pdf}
% }
% \subfigure[Coefficients ($\boldsymbol{\mathcal{B}}(3,:,3)$) estimated by BKTR (with $R=10$) under different observation rates.]{
% \includegraphics[width=0.99\textwidth]{graphical/simu1-temporaCompare_Missing.pdf}
% }
% \caption{\small{Comparison of BKTR in different settings for Simulation 1. (a) and (b) plot the estimated $\boldsymbol{\mathcal{B}}$ (mean with 95\% CI) of one simulation at location \#3 ($m=3$) for the 6th and 3rd covariates (i.e., $\boldsymbol{\mathcal{B}}(3,:,6)\text{ and }\boldsymbol{\mathcal{B}}(3,:,3)$), respectively.}}
% \label{fig:simu1_new_compare}
% \end{figure}



\subsubsection{Simulation 2}\label{appD1}

For Simulation 2, Figure~\ref{fig:simu2_temporal_2} shows the estimation results of STVC, BKTR, and BTR for one chain when $\tau^{-1}=1, \phi=\gamma\in\{2,4\}$. We still give the example for the third covariate at location \#8 ($m=8, p=3$) to compare the estimated temporal variation of the coefficients by the three methods.


\begin{figure}[!ht]
\centering
\subfigure[$\tau^{-1}=1,\ \phi=\gamma=2$.]{
\includegraphics[width=0.67\textwidth]{graphical/simu2-temporal-2.pdf}
}
\subfigure[$\tau^{-1}=1,\ \phi=\gamma=4$.]{
\includegraphics[width=0.67\textwidth]{graphical/simu2-temporal-3.pdf}
}
\caption{Comparison of the estimated coefficients for Simulation 2, where $\tau^{-1}=1,\phi=\gamma\in\{2,4\}$. We show the approximated coefficients (mean with 95\% CI) for the third covariate at location \#8.}
\label{fig:simu2_temporal_2}
\end{figure}



\subsubsection{Simulation 3}\label{appD2}

For Simulation 3, Figure~\ref{fig:simu3_new_2} gives the spatial patterns of the BKTR estimated coefficients of four covariates in one simulation when $R=40$.



\begin{figure}[!ht]
\centering
% \subfigure[Estimated coefficients (mean with 95\% CI) at location $m=3$, i.e. $\boldsymbol{\mathcal{B}}(3,:,p=1,4,5,6)$.]
% {
% \includegraphics[width=0.99\textwidth]{graphical/simu3-temporal.pdf}
% }
% \subfigure[Interpolated spatial surfaces of the coefficients at time point $n=20$, i.e. $\boldsymbol{\mathcal{B}}(:,20,p=1,2,4,6)$, where black circles denote the positions of sampled locations.]
{
\includegraphics[width=0.91\textwidth]{graphical/simu3-spatial.pdf}
}
\caption{BKTR ($R=40$) estimated coefficients for Simulation 3. The figure shows the interpolated spatial surfaces of the estimated coefficients at time point $n=20$, i.e. $\boldsymbol{\mathcal{B}}(:,20,p=1,2,4,6)$, where black circles denote the positions of sampled locations.}
\label{fig:simu3_new_2}
\end{figure}








\end{document}
