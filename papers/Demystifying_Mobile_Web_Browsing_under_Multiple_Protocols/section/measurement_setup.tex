\section{Measurement Setup}

\begin{figure}[htbp]
	\centering
    \includegraphics[width=0.6\textwidth]{images/experiment_setup.png}
    \caption{Experiment Setup} \label{fig:experiment_setup}
\end{figure}

In this section, we introduce the infrastructural platforms and how we build the testbed to conduct our experiments as depicted in~\ref{fig:experiment_setup}.

\subsection{Methodology}
We need to address that loading a Web page is not as simple as fetching all resources. For example, while loading a page, browsers usually do not download any images until JavaScript and CSS files are fetched and processed. CSS files may import other CSS files, and the browser can not know about that in advance. Some scripts generate new resources for the browsers to fetch. \cite{Wang:NSDI13} shows that dependencies between network activities (fetching Web objects) and computation activities(HTML parsing, JavaScript execution) have significant impacts on page loading time, so we divide our experiments into two parts. First, we develop our own HTTP client ignoring complicated dependencies between network activities and computation activities in real browsers. We just consider these four protocols as transport protocols to download all resources of Web pages. Then, we conduct our experiments with complete page load dependencies using real browsers to analyze how different protocol perform in real browsing process.

\subsubsection{Experiments as Transport protocols}
 Loading a Web page in browsers is a complex process, consisting of network activities, including DNS lookup, TCP connection setup, resources downloading and so on, and computation activities, including parsing HTML file, parsing Javascript file, executing Javascript code and so on. What's more, network activities and computation activities are interdependent. For example, while loading a page, browsers usually do not download any images until JavaScript and CSS files are fetched and processed. We develop our own client to fetch Web pages ignoring page load complicated dependencies between network activities and computation activities. We just consider these four protocols as transport protocols to simplify our analysis. In order to analyze how different Web pages with various object sizes and object numbers may affect Web pages' performance for each protocol, we synthesize Web pages with pre-specified object sizes and object numbers. Meanwhile, we emulate different network conditions between client and server using \textit{Traffic Control}. We vary different Round Trip Times (RTT), bandwidths, packet loss rates between client and server.
 
 \subsubsection{Experiments with real browsers}
 
With the consideration of real processes of loading a Web page, we conduct our experiments with real browsers for each protocol, respectively. We clone the landing pages of Alexa top 200 websites, which have both mobile version and desktop version, into our local server, and visit them using different protocols through emulated network conditions using \textit{Traffic Control} to keep the consistency across experiments. We vary different packet loss rates, bandwidths, and RTTs to simulate different network conditions and analyze how these network settings affect performance of different protocols. We focus on a user-perceived latency of these pages, so we measure Web page performance using the \textbf{Page Load Time (PLT)} metric. We compare PLTs when loading Web pages using real browsers with HTTP/2, HTTP, and SPDY. PLT is calculated from initiation (when you click on a page link or type in a Web address) to completion (when the page is fully loaded in the browser). We listen the \textit{onLoad} event emitted by browser to get the PLTs.

\subsection{Experiment Settings}

In our experiments, we need the same server implementation to provide HTTP(S), SPDY and HTTP/2 stacks or else our results would be biased. We set up a Nginx proxy~\cite{nginx}, which can transfer Web pages through four protocols, respectively. We need to address that HTTP/2 and SPDY could work without SSL as the standard defines, but both Chrome and Firefox do require the SSL. We employ HTTP/2 and SPDY with encryption. We use Chrome for Android platform to load Web pages we collect, which supports both all the four protocols. We develop a measurement software communicating with Chrome with Chrome Debugging Protocol\footnote{We can communicate with Chrome using Chrome Debugging Protocol to execute commands and listen to events emitted when involving page load, \url{https://developer.chrome.com/devtools/docs/protocol/0.1/index}} to automate our experiments. For each run, we set up network conditions using \textit{Traffic Control} (TC), and control Chrome to load specific Web page. Meanwhile, our measurement software will automatically export a \textit{HTTP archive record} (HAR) format file when page load finished, which record all the requests and responses involved in loading a Web page as well as PLT. 
 
Thus, we need to keep the consistency across experiments. We clone the landing pages of the Alexa top 200 websites that have corresponding mobile version into our local server and convert all external links to local links. Overall, we collect 400 Web pages, consisting of 200 desktop pages and 200 corresponding mobile pages. Meanwhile, we use Linux Traffic Control (TC) to emulate different network to ensure browsers load pages under consistent network conditions for each protocol.

\begin{table}[!htbp]\centering
\caption{Factors may affect HTTP/2 and HTTPS performance.}\label{tab:factors}
\normalsize
\begin{tabular}{|c|c|c|}
\toprule
    \textbf{Factor} & \textbf{Range} & \textbf{High}\\
\midrule
    \textbf{RTT} & 20ms, 100ms, 200ms & $ \geq 100ms $\\
    \hline
    \textbf{bandwidth} & 1Mbps, 10Mbps & $ \geq 10Mbps $\\
    \hline
    \textbf{pkt loss} & 0, 0.005, 0.01, 0.02 & $ \geq 0.01 $\\
    \hline
    \textbf{object size} & 100B, 1K, 10K, 100K, 1M & $ \geq 1K $\\
    \hline
    \textbf{\# of object} & 2, 8, 16, 32, 64, 128, 512 & $ \geq 64 $\\
\bottomrule
\end{tabular}
\end{table}

\textbf{Metrics.} We compare page load time (PLT) when loading Web pages using real browsers with each protocols. PLT is calculated from initiation (when you click on a page link or type in a Web address in the browser) to completion (when the page is fully loaded in the browser). We listen the \textit{onLoad} event emitted by browser to get the PLT. When experiment with our own client, we just download all resources of a page and we calculate the PLT from the time when our client initiates the first request to the time when our client receives the last byte of a page.

\textbf{Server.} We use ThinkPad S3 as our server, which is a 64-bit machine with 1.9GHz 4 core CPU and 8GB memory and Ubuntu 14.04. We could switch the transfer protocol among HTTP, HTTPS, SPDY, and HTTP/2 on Nginx server. We turn off GZIP encoding of Nginx server to keep the exact size of Web objects and Web pages.

\textbf{Client.} We have mentioned we divide our experiments into two parts before. When we compare SPDY, HTTP/2, HTTP, and HTTPS as a transport protocol, we develop our own client based on OkHttp~\cite{okhttp}. For experiment in real browser with different protocols, we load Web pages in Chrome for Android installed in Nexus 6 with 3GB RAM and powered by a 2.7GHz Qualcomm Snapdragon 805 with quad-core CPU (APQ 8084-AB) running Android Lollipop. When analyzing how devices affect the Web page performanceWe, we also conduct experiments using Samsung Galaxy Note 2 with 2 GB RAM and powered by a 1.6GHz quad-core CPU running Android KitKat. By default, we show our experiments conducted on Nexus 6. 

\textbf{Settings.} In this paper, we consider five factors external to HTTP that may affect performance, including network conditions (e.g. packet loss, bandwidth, and RTT), and features of Web pages (e.g. object size, number of objects). Table~\ref{tab:factors} shows the factor settings in our experiments. We use \textit{Traffic Control (TC)} of Linux kernel to set up various network conditions. The RTT values include 20ms (good WiFi), 100ms, and 200ms (3G). The bandwidth (bw) values include 1Mbps (3G) and 10Mbps (WiFi). We vary random packet loss rates from 0\% to 2\%~\cite{Dukkipati:SIGCOMM11}. We also consider a wide range of Web object size (100B-1MB) and number of objects (2-512) when we synthesize Web pages. We define a threshold for each factor, so that we can classify each setting as being high or low for our further analysis.

\textbf{Work flow.} For experiment with real browsers, we develop a measurement software communicating with Chrome with Chrome Debugging Protocol to automate our experiments. For each run, we set up network conditions using TC, and control Chrome to load specific Web page. For example, we send ``Page.navigation'' command with specific URL to chrome, and Chrome will load the target Web page. Meanwhile, our measurement software will automatically export a HAR-format file when page load finished, which record all the requests and responses involved in loading a Web page as well as PLT. We will clear browsers' cache before loading another Web page. For experiments with our own client, our client will automatically download all pages and record the PLT for each page.