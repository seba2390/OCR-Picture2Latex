\subsection{Defense methods}
% ----A new version for first paragraph ----
A variety of theoretically principled~\citep{raghunathan2018certified, sinha2017certifiable, raghunathan2018semidefinite, wong2018scaling, wong2018provable, gowal2018effectiveness} and empirical defense approaches \citep{bai2021recent} were proposed to enhance robustness since the discovery of adversarial examples. 
Theoretically principled methods focus on certifying robustness to adversarial perturbations under a given norm, using variety of techniques such as randomized smoothing \citep{cohen2019certified}. However, empirical defence methods in general, and in particular adversarial training, still yield preferable results.
% \citep{wu2020skip}

Among the empirical defence techniques, we can find: \textit{adversarial regularization}---adding various regularization terms to the loss functions to enhance robustness (e.g., encouraging logits for clean and adversarial examples to be similar) \citep{kurakin2016atscale, madry2017towards, zhang2019theoretically, wang2019improving, kannan2018adversarial, jin2022enhancing}, \textit{curriculum-based adversarial training}---taking incremental approach when learning PGD adversaries in the goal of improving generalization on clean data while still preserving robustness (e.g., gradually increasing the number of PGD iterations to avoid overfitting the adversarial examples) \citep{cai2018curriculum, zhang2020attacks, wang2019convergence}, \textit{ensemble adversarial training}---where clean data is augmented with adversarial
examples generated from different target models instead of
a single model \citep{tramer2017ensemble, pang2019improving, yang2020dverge}, \textit{adversarial training with adaptive attack budget}---where we change the perturbation budget to prevent over-confident predictions and achieve better exploration of the manifold \citep{ding2018mma, cheng2020cat}, \textit{semi-supervised and unsupervised adversarial training}---several methods theoretically and empirically demonstrated how unlabeled data can reduce the sample complexity gap between
standard training and adversarial training  \citep{carmon2019unlabeled, uesato2019labels, zhai2019adversarially}, \textit{robust self and pre-training}---other works integrate self-supervised pretraining tasks such as Selfie, Rotation and Jigsaw together with adversarial examples \citep{jiang2020robust, chen2020adversarial}, \textit{efficient adversarial training}---due to the high cost of adversarial training, efficient methods aim to keep the favorable performance of adversarial training while reducing the computational and time costs \citep{shafahi2019adversarial, wong2020fast, andriushchenko2020understanding, zhang2019you}, and many other techniques such as adversarial training based on feature scatter \citep{zhang2019defense}, adversarially robust distillation \citep{goldblum2020adversarially}, hypersphere embedding \citep{pang2020boosting}, and augmenting adversarial examples by interpolation \citep{lee2020adversarial}. In an additional research direction, researchers suggested to add new dedicated building blocks to the network architecture for improved robustness \citep{xie2019intriguing, xie2019feature, liu2020towards}. \citet{liu2020towards} hypothesised that different adversaries belong to different domains, and suggested gated batch normalization which is trained with multiple perturbation types. \cite{guo2020meets} focused on searching robust architectures against adversarial examples. Others works presented improved robustness by combining data augmentation techniques and generated data \citep{rebuffi2021fixing, rebuffi2021data}, where the latest is also the current state-of-the-art in robustness. 

Our work belongs to the the family of adversarial regularization techniques, for which we elaborate on common and best performing methods, and highlight the differences compared to our method. 

\citet{madry2017towards} proposed a technique, commonly referred to as Adversarial Training (AT), to minimize the cross entropy loss on adversarial examples generated by PGD (without using the natural examples). \citet{zhang2019theoretically}
suggested to decompose the prediction error for adversarial examples as the sum of the natural error and boundary error, and provided differentiable upper bounds on both terms. Motivated by this decomposition, they suggested a technique called TRADES that uses the Kullback-Leibler (KL) divergence as a regularization term that will push the decision boundary away from the data. They do so by applying the KL-divergence on the logits of clean examples and their adversarial counterparts. 
%Meaning, TRADES loss is the cross entropy loss on clean examples and the KL-divergence on the logits of clean examples and their adversarial counterparts. 
\citet{wang2019improving} suggested that misclassified examples have a significant impact on 
final robustness, and proposed a technique called MART that differentiate between correctly classified and miss-classified examples during training by weighting the KL-divergence between the clean and adversarial logits using the probability of the classifier on the correct label. 
%MART loss function is composed of boosted cross entropy (BCE) on adversarial examples and the KL-divergence on the logits of clean examples and their adversarial counterparts, where the KL-divergence is weighted 

Another area of research aims at revealing the connection between the loss weight landscape
%, which is the loss change with respect to the weights,
and adversarial training \citep{prabhu2019understanding, yu2018interpreting, wu2020adversarial}. Specifically,~\citet{wu2020adversarial} identified 
a correlation between the flatness of weight loss landscape and robust generalization gap. They proposed
the
Adversarial Weight Perturbation (AWP) mechanism that is integrated into existing adversarial training methods and generates adversarial perturbations on both the inputs and the network weights. More recently, this approach was formalized from a theoretical standpoint by~\citet{tsai2021formalizing}. However, this method forms a double-perturbation mechanism that perturbs both inputs and weights,
which may incur a significant increase in calculation overhead. We demonstrate how DIAL improves results also when combined with AWP, named $\DIAL_{\awp}$.
In Section \ref{related-loss-func} we elaborate about the loss functions of the different compared methods.

A related approach to ours, called ATDA,
was presented by \citet{song2018improving}.
They proposed to add several constrains to the loss function in order to enforce domain adaptation: correlation alignment and maximum mean discrepancy~\citep{borgwardt2006integrating, sun2016deep}. While the objective is similar, using ideas from domain adaptation for learning better representation, we address it in two different ways. Our method fundamentally differs from~\citet{song2018improving} since we do not enforce domain adaptation by adding specific constrains to the loss function. Instead, we let the network learn the domain invariant representation directly during the optimization process, as suggested by~\citet{ganin2015unsupervised,ganin2016domain}. Moreover,~\citet{song2018improving} focused mainly of Fast Gradient Sign Method (FGSM) attack, which is a one step variant of PGD attack. We empirically demonstrate the superiority of our method in Section~\ref{experiments}. 
In a concurrent work, \citet{qian2021improving} utilized the idea of exploiting local and global data information, and suggested to generate the adversarial examples by attacking an additional domain classifier.

\subsection{Theoretical analysis of robust generalization}
Several works investigated the sample complexity requires the ensure adversarial generalization compared to the non-adversarial counterpart.
\citet{schmidt2018adversarially} has shown that there exists a distribution (mixture of Gaussians) where ensuring robust generalization necessarily requires more data than standard learning. This has been furthered investigated in a distribution-free models via the Rademacher complexity, VC dimension, and fat-shattering dimension ~\citep{yin2019rademacher,attias2019improved,khim2018adversarial,awasthi2020rademacher,cullina2018pac,montasser2019vc,tsai2021formalizing,attias2022characterization,attias2022adversarially} and additional settings~\citep{diochnos2018adversarial,carmon2019unlabeled}.

% specific case of distributions
% suggested that the difficulty of training robust models stems, at least partially, from the significantly larger sample complexity required for robust learning, compared to that of standard training.
% chmidt et al. [41] show that in a simple model, learning a classifier with non-trivial adversarially robust accuracy requires substantially more samples than achieving good standard accuracy.

