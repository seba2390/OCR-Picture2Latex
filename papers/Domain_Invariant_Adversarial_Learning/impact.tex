Adversarial examples illustrate a fundamental vulnerability of deep neural networks, and manage to break state-of-the art DNNs in various fields. As these DNNs are deployed in critical systems, such as autonomous vehicles and facial recognition systems, it becomes crucial to build models which are robust against such attacks. These kind of system cannot tolerate attacks that can cost in human lives.
For this reason, we proposed DIAL to improve models' robustness against adversarial attacks.
We hope that it will help in building more secure models for real-world applications.  DIAL 
is comparable 
to the state-of-the-art methods we tested
in terms of training times and other
resources.
%consumption
%requires comparable training times and
%other resource consumption
%resource requirements compared 
%to the other adversarial training
%methods we tested. With 
That said, this work is not without limitations: adversarial training is still a computationally expensive procedure that requires extra computations compared to standard training, 
with the concomitant environmental costs.
%results in negative influence on the environment. 
Even though incorporating our method introduced improved standard accuracy, adversarial training still degrades the standard accuracy. 
Moreover, models are trained to be robust using well known threat models such as the bounded $\ell_{p}$ norms. However, once a model is deployed, we cannot control the type of attacks it faces from sophisticated adversaries. Thus, the general problem
is still very far from being fully solved.
%Therefore, the research community is still heading a long way to go until we will manage to achieve sustainable robustness in real world scenarios.
