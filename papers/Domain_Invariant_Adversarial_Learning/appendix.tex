\newpage
\section{Domain Invariant Adversarial Learning Algorithm}
\label{algo-appendix}
Algorithm~\ref{DIAL-algorithm} describes a pseudo-code of our proposed $\DIAL_{\ce}$ variant. As can be seen, a target domain batch is not given in advance as with standard domain-adaptation task. Instead, for each natural batch we generate a target batch using adversarial training. The loss function is composed of natural and adversarial losses with respect to the main task (e.g., classification), and from natural and adversarial domain losses. By maximizing the losses on the domain we aim to learn a feature representation which is invariant to the natural and adversarial domain, and therefore more robust.

\input{Algo.tex}

\section{additional results on CIFAR-100 and SVHN}
\label{cifar100-svhn-appendix}

\begin{table}[!ht]
  \caption{Robustness against white-box, black-box attacks and Auto-Attack (AA) on SVHN. Black-box attacks are generated using naturally trained surrogate model and applied to the best performing robust models.}
  \label{black-and_white-svhn-appendix}
  \vskip 0.15in
  \centering
  \small
  \begin{tabular}{l@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c}
    \toprule
    & & \multicolumn{4}{c}{White-box} & \multicolumn{4}{c}{Black-Box}  \\
    \cmidrule(r){3-6} 
    \cmidrule(r){7-10}
    Defense Model & Natural & PGD$^{20}$ & PGD$^{100}$  & PGD$^{1000}$  & CW$^{\infty}$ & PGD$^{20}$ & PGD$^{100}$ & PGD$^{1000}$  & CW$^{\infty}$ & AA \\
    \midrule
    % Madry et al. & 89.90 & 53.23 & 49.45 & 49.23 & 48.25 & 86.44 & 86.28 & 86.18 & 86.42 & 45.25 \\
    TRADES & 90.35 & 57.10 & 54.13 & 54.08 & 52.19 & 86.89 & 86.73 & 86.57 & 86.70 &  49.5 \\
    $\DIAL_{\kl}$ (Ours) & 90.66 & \textbf{58.91} & \textbf{55.30} & \textbf{55.11} & \textbf{53.67} & 87.62 & 87.52 & 87.41 & 87.63 & \textbf{51.00} \\
    $\DIAL_{\ce}$ (Ours) & \textbf{92.88} & 55.26  & 50.82 & 50.54 & 49.66 & \textbf{89.12} & \textbf{89.01} & \textbf{88.74} & \textbf{89.10} &  46.52  \\
    \bottomrule
  \end{tabular}
\end{table}



\begin{table}[!ht]
  \caption{Robustness against white-box, black-box attacks and Auto-Attack (AA) on CIFAR100. Black-box attacks are generated using naturally trained surrogate model and applied to the best performing robust models.}
  \label{black-and_white-cifar100-appendix}
  \vskip 0.15in
  \centering
  \small
  \begin{tabular}{l@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c@{\hspace{1\tabcolsep}}c}
    \toprule
    & & \multicolumn{4}{c}{White-box} & \multicolumn{4}{c}{Black-Box}  \\
    \cmidrule(r){3-6} 
    \cmidrule(r){7-10}
    Defense Model & Natural & PGD$^{20}$ & PGD$^{100}$  & PGD$^{1000}$  & CW$^{\infty}$ & PGD$^{20}$ & PGD$^{100}$ & PGD$^{1000}$  & CW$^{\infty}$ & AA \\
    \midrule
    TRADES & 58.24 & 30.1 & 29.66 & 29.64 & 25.97 & 57.05 & 56.71 & 56.67 & 56.77 & 24.92 \\
    $\DIAL_{\kl}$ (Ours) & 58.47 & \textbf{31.19} & \textbf{30.50} & \textbf{30.42} & \textbf{26.91} & 57.16 & 56.81 & 56.80 & 57.00 & \textbf{25.87} \\
    $\DIAL_{\ce}$ (Ours) & \textbf{60.77} & 27.87 & 26.66 & 26.61 & 25.98 & \textbf{59.48} & \textbf{59.06} & \textbf{58.96} & \textbf{59.20} & 23.51  \\
    \bottomrule
  \end{tabular}
\end{table}

\newpage

\section{CIFAR-10 Additional Experimental Setup details}
\label{cifar10-additional-setup}
\paragraph{Additional defence setup.} For being consistent with other methods, the natural images are padded with 4-pixel padding with 32-random crop and random horizontal flip. Furthermore, all methods are trained using SGD with momentum 0.9. For $\DIAL_{\kl}$, we balance the robust loss with $\lambda=6$ and the domains loss with $r=4$. For $\DIAL_{\ce}$, we balance the robust loss with $\lambda=1$ and the domains loss with $r=2$. 
For DIAL-AWP, we used the same learning rate schedule used in ~\cite{wu2020adversarial}, where the initial 0.1 learning rate decays by a factor of 10 after 100 and 150 iterations. For black-box attacks, we used two types of surrogate models (1) naturally trained surrogate model, with natural accuracy of 95.61\% and (2) surrogate model trained using one of the adversarial training methods.

% \newpage
\section{Benchmarking the State-of-the-art on MNIST}
\label{mnist-results}
\paragraph{Defence setup.} We use the same CNN architecture as used in~\cite{zhang2019theoretically} which consists of four convolutional layers and three fully-connected layers. Sidelong this architecture, we integrate a domain classification layer. To generate the adversarial domain dataset, we use a perturbation size of $\epsilon=0.3$. We apply 40 iterations of inner maximization with perturbation step size of 0.01. Batch size is set to 128 and the model is trained for 100 epochs. Similar to the other methods, the initial learning rate was set to 0.01, and decays by a factor of 10 after 55 iterations, 75 and 90 iterations. All the models in the experiment are trained using SGD with momentum 0.9. For our method, we balance the robust loss with $\lambda=6$ and the domains loss with $r=0.1$. 

\paragraph{White-box/Black-box robustness.} We evaluate all defense models using PGD$^{40}$, PGD$^{100}$, PGD$^{1000}$ and CW$_{\infty}$ ($\ell_{\infty}$ version of~\cite{carlini2017towards} attack optimized by PGD-100) with step size 0.01. We constrain all attacks by the same perturbation $\epsilon=0.3$. 
For our black-box setting, we use a naturally trained surrogate model with natural accuracy of 99.51\%. As reported in Table~\ref{black-and_white-mnist}, our method achieves improved robustness over the other methods under the different attack types, while preserving the same level of natural accuracy, and even surpassing the naturally trained model. We should note that in general, the improvement margin on MNIST is more moderate compared to CIFAR-10, since MNIST is an easier task than CIFAR-10 and the robustness range is already high to begin with. Additional results are available in Appendix~\ref{additional_res}.


\begin{table}[!ht]
  \caption{Robustness against white-box, black-box attacks and Auto-Attack (AA) on MNIST. Black-box attacks are generated using naturally trained surrogate model and applied to the best performing robust models.}
  \label{black-and_white-mnist}
  \vskip 0.15in
  \centering
  \begin{tabular}{lllllllll}
    \toprule
    & & \multicolumn{3}{c}{White-box} & \multicolumn{3}{c}{Black-Box} \\
    \cmidrule(r){3-5} 
    \cmidrule(r){6-8}
    Defense Model & Natural & PGD$^{40}$ & PGD$^{100}$ & CW$^{\infty}$ & PGD$^{40}$ & PGD$^{100}$ & CW$^{\infty}$ & AA \\
    \midrule
    TRADES & 99.48 & 96.07 & 95.52 & 95.69 & 98.12 & 97.86 & 98.21 & 92.79 \\
    MART & 99.38 & 96.99 & 96.11 & 95.98 & 98.16 & 97.96 & 98.28 & 93.30 \\
    AT & 99.41 & 96.01 & 95.49 & 95.78 & 98.05 & 97.73 & 98.20 & 88.50 \\
    ATDA & 98.72 & 96.82 & 96.26 & 96.31 & 97.74 & 97.28 & 97.76 & 93.31 \\
    $\DIAL_{\kl}$ (Ours)  & 99.46 & 97.05 & 96.06 & 96.17 & 98.14 & 97.83 & 98.14 & \textbf{93.68} \\
    $\DIAL_{\ce}$ (Ours)  & $\mathbf{99.52}$ & $\mathbf{97.61}$ & $\mathbf{96.91}$ & $\mathbf{97.00}$ & $\mathbf{98.41}$ & $\mathbf{98.12}$ & $\mathbf{98.48}$ & 93.43 \\
    \bottomrule
  \end{tabular}
\end{table}


\section{Additional Results on MNIST and CIFAR-10} \label{additional_res}
In Table~\ref{pgd1000_attack} we present additional results using the PGD$^{1000}$ threat model. We use step size of 0.003 and constrain the attacks by the same perturbation $\epsilon=0.031$. Table~\ref{awp_variants} presents a comparison of our method combined with AWP to other the variants of AWP that were presented in ~\cite{wu2020adversarial}. In addition, in Table~\ref{f1-awp} we add the F$_{1}$-robust scores for different variants of AWP.

% \begin{table}
%   \caption{CW$_{\infty}$ attack on MNIST and CIFAR-10 on white-box and black-box settings}
%   \label{cw_inf_attack}
%   \centering
%   \begin{tabular}{llllll}
%     \toprule
%     & \multicolumn{2}{c}{MNIST} & \multicolumn{2}{c}{CIFAR-10} \\
%     \cmidrule(r){2-3} 
%     \cmidrule(r){4-5}
%     Defense Model & White-box & Black-box & White-box & Black-box  \\
%     \midrule
%     TRADES ~\cite{zhang2019theoretically} & 95.69 & 98.21 & 54.2 & 83.91 \\
%     MART ~\cite{wang2019improving} & 95.98 & 98.28 & 53.09 & 82.8   \\
%     Madry et al. ~\cite{madry2017towards} & 95.78 & 98.2  & 53.99 & 83.92    \\
%     Song et al.   ~\cite{song2018improving}& 96.31 & 97.76 & 41.01 & 75.35 \\
%     $\DIAL_{\ce}$ (Ours) &  $\mathbf{96.41}$ & $\mathbf{98.35}$  & 50.15 & $\mathbf{88.92}$  \\
%     $\DIAL_{\kl}$ (Ours) & 96.17 & 98.14 & $\mathbf{55}$ & 84.05 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

\begin{table}[!ht]
  \caption{PGD$^{1000}$ attack on MNIST and CIFAR-10 on white-box and black-box settings.}
  \label{pgd1000_attack}
  \vskip 0.15in
  \centering
  \begin{tabular}{llllll}
    \toprule
    & \multicolumn{2}{c}{MNIST} & \multicolumn{2}{c}{CIFAR-10} \\
    \cmidrule(r){2-3} 
    \cmidrule(r){4-5}
    Defense Model & White-box & Black-box & White-box & Black-box  \\
    \midrule
    TRADES & 95.22 & 97.81 & 56.43 & 83.80 \\
    MART & 95.74 & 97.89 & 56.55 & 82.47 \\
    AT & 95.36 & 97.78 & 54.40 & 83.96 \\
    ATDA & 96.20 & 97.34 & 41.02 & 75.11 \\
    $\DIAL_{\ce}$ (Ours) &  $\mathbf{96.78}$ & $\mathbf{98.10}$  & 51.57 & $\mathbf{88.22}$  \\
    $\DIAL_{\kl}$ (Ours) & 95.99 & 97.89 & $\mathbf{56.73}$ & 84.00 \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[!ht]
  \caption{Robustness comparison of DIAL-AWP and other variants of AWP that do not require additional data under the $\ell_{\infty}$ threat model.}
  \label{awp_variants}
  \vskip 0.15in
  \centering
  \begin{tabular}{lllll|l}
    \toprule
    \cmidrule(r){1-5}
    Defense Model & Natural & PGD$^{20}$ & PGD$^{100}$ & CW$_{\infty}$ & AA\\
    \midrule
    DIAL-AWP (Ours) & $\mathbf{85.91}$ & $\mathbf{61.10}$ & $\mathbf{59.86}$ & $\mathbf{57.67}$ & $\mathbf{56.78}$  \\
    TRADES-AWP \citep{wu2020adversarial} & 85.36 & 59.27 & 59.12 & 57.07 & 56.17    \\
    MART-AWP \citep{wu2020adversarial} & 84.43 & 60.68 & 59.32 & 56.37 & 54.23  \\
    AT-AWP \citep{wu2020adversarial} & 85.57  &  58.14 & 57.94 & 55.96 & 54.04 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[!ht]
  \caption{F$_{1}$-robust measurement on AWP variants based on white-box attack.}
  \label{f1-awp}
  \vskip 0.15in
  \centering
  \begin{tabular}{ll}
    \toprule
    \cmidrule(r){1-2}
    Defense Model & F$_{1}$-robust \\
    \midrule
    DIAL-AWP (Ours) & $\mathbf{0.69753}$  \\
    TRADES-AWP \citep{wu2020adversarial} & 0.68162   \\
    MART-AWP \citep{wu2020adversarial} & 0.68857  \\
    AT-AWP \citep{wu2020adversarial} & 0.67381 \\
    \bottomrule
  \end{tabular}
\end{table}

% \section{Results on Street View House Numbers (SVHN)}
% \label{res_svhn}
% We performed additional experiments on the Street View House Numbers (SVHN) dataset \cite{netzer2011reading}. We used the PreAct ResNet-18 \cite{he2016identity} architecture on which we integrate a domain classification layer. The adversarial training is done using 10-step PGD adversary with perturbation size of $\epsilon=0.031$ and a step size of 0.003. The batch size is 128, weight decay is $7e^{-4}$ and the model is trained for 100 epochs. We balance the adversarial loss with $\lambda=6$ and the domains loss with $r=2$. Similar to the MNIST experiment setup, the initial learning rate was set to 0.01, and decays by a factor of 10 after 55 iterations, 75 and 90 iterations. We compared DIAL to Madry et al. \cite{madry2017towards} and TRADES \cite{zhang2019theoretically}. The evaluation is done using different $\ell_{\infty}$ adversaries: PGD$^{20}$, PGD$^{100}$, PGD$^{1000}$ and CW$_{\infty}$ with step size 0.003. We also compared the different methods using the Auto-Attack (AA). Results are averaged over 3 restarts. As can be seen by the results in Table~\ref{svhn}, DIAL presents a consistent improvement in robustness compared to the other methods under variety of attacks while also improving the natural accuracy.


% \begin{table}[!ht]
%   \caption{Robustness comparison of DIAL to Madry et al. and TRADES defense models on the SVHN dataset under different PGD white-box attacks and the ensemble Auto-Attack (AA)}
%   \label{svhn}
%   \centering
%   \begin{tabular}{llllll|l}
%     \toprule
%     \cmidrule(r){1-5}
%     Defense Model & Natural & PGD$^{20}$ & PGD$^{100}$ & PGD$^{1000}$ & CW$_{\infty}$ & AA\\
%     \midrule
%     $\DIAL_{\kl}$ (Ours) & $\mathbf{90.66}$ & $\mathbf{58.91}$ & $\mathbf{55.30}$ & $\mathbf{55.12}$ & $\mathbf{53.67}$  & $\mathbf{51.00}$  \\
%     Madry et al. & 89.90 & 53.23 & 49.45 & 49.23 & 48.25 & 45.25  \\
%     TRADES & 90.35 & 57.10 & 54.13 & 54.08 & 52.19 & 49.50 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \newpage

% \section{Results on CIFAR-100}
% \label{res_cifar100}
% We performed additional experiments on the CIFAR-100 dataset ~\cite{krizhevsky2009learning}. We used the PreAct ResNet-18 \cite{he2016identity} architecture on which we integrate a domain classification layer. The adversarial training is done using 10-step PGD adversary with perturbation size of $\epsilon=0.031$ and a step size of 0.007. The batch size is 128, weight decay is $7e^{-4}$ and the model is trained for 100 epochs. We balance the adversarial loss with $\lambda=6$ and the domains loss with $r=4$. Similar to the CIFAR-10 experiment setup, the initial learning rate was set to 0.1, and decays by a factor of 10 after 75 and 90 iterations. We compared DIAL to Madry et al. \cite{madry2017towards} and TRADES \cite{zhang2019theoretically}. The evaluation is done using different $\ell_{\infty}$ adversaries: PGD$^{20}$, PGD$^{100}$, PGD$^{1000}$ and CW$_{\infty}$ with step size of 0.003. We also compared the different methods using Auto-Attack (AA). Results are averaged over 3 restarts. As can be seen by the results in Table~\ref{cifar100}, DIAL presents a consistent improvement in robustness compared to the other methods under variety of attacks while also improving the natural accuracy.

% \begin{table}[!h]
%   \caption{Robustness comparison of DIAL to Madry et al. and TRADES defense models on the CIFAR-100 dataset under different PGD white-box attacks and the ensemble Auto-Attack (AA)}
%   \label{cifar100}
%   \centering
%   \begin{tabular}{llllll|l}
%     \toprule
%     \cmidrule(r){1-5}
%     Defense Model & Natural & PGD$^{20}$ & PGD$^{100}$ & PGD$^{1000}$ & CW$_{\infty}$ & AA\\
%     \midrule
%     $\DIAL_{\kl}$ (Ours) & $\mathbf{58.47}$ & $\mathbf{31.24}$ & $\mathbf{30.50}$ & $\mathbf{30.42}$ & $\mathbf{26.91}$  & $\mathbf{25.87}$  \\
%     Madry et al. & 56.73 & 29.51 & 28.45 & 28.39 & 26.60 & 24.12  \\
%     TRADES & 58.24 & 30.10 & 29.66 & 29.65 & 25.97 & 24.92 \\
%     \bottomrule
%   \end{tabular}
% \end{table}

\newpage
\section{Extened results on Unforeseen Corruptions}
\label{corruptions-apendix}
We present full accuracy results against unforeseen corruptions in Tables \ref{corruption-table1} and \ref{corruption-table2}. 
% We also visualize it in Figure \ref{spider-full-graph}. 

\begin{table}[!ht]
  \caption{Accuracy (\%) against unforeseen corruptions.}
  \label{corruption-table1}
  \vskip 0.15in
  \centering
  \tiny
  \begin{tabular}{lcccccccccccccccccc}
    \toprule
    Defense Model & brightness & defocus blur & fog & glass blur & jpeg compression & motion blur & saturate & snow & speckle noise  \\
    \midrule
    TRADES & 82.63 & 80.04 & 60.19 & 78.00 & 82.81 & 76.49 & 81.53 & 80.68 & 80.14 \\
    MART & 80.76 & 78.62 & 56.78 & 76.60 & 81.26 & 74.58 & 80.74 & 78.22 & 79.42 \\
    AT &  83.30 & 80.42 & 60.22 & 77.90 & 82.73 & 76.64 & 82.31 & 80.37 & 80.74 \\
    ATDA & 72.67 & 69.36 & 45.52 & 64.88 & 73.22 & 63.47 & 72.07 & 68.76 & 72.27 \\
    DIAL (Ours)  & \textbf{87.14} & \textbf{84.84} & \textbf{66.08} & \textbf{81.82} & \textbf{87.07} & \textbf{81.20} & \textbf{86.45} & \textbf{84.18} & \textbf{84.94} \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}[!ht]
  \caption{Accuracy (\%) against unforeseen corruptions.}
  \label{corruption-table2}
  \vskip 0.15in
  \centering
  \tiny
  \begin{tabular}{lcccccccccccccccccc}
    \toprule
    Defense Model & contrast & elastic transform & frost & gaussian noise & impulse noise & pixelate & shot noise & spatter & zoom blur \\
    \midrule
    TRADES & 43.11 & 79.11 & 76.45 & 79.21 & 73.72 & 82.73 & 80.42 & 80.72 & 78.97 \\
    MART & 41.22 & 77.77 & 73.07 & 78.30 & 74.97 & 81.31 & 79.53 & 79.28 & 77.8 \\
    AT & 43.30 & 79.58 & 77.53 & 79.47 & 73.76 & 82.78 & 80.86 & 80.49 & 79.58 \\
    ATDA & 36.06 & 67.06 & 62.56 & 70.33 & 64.63 & 73.46 & 72.28 & 70.50 & 67.31 \\
    DIAL (Ours) & \textbf{48.84} & \textbf{84.13} & \textbf{81.76} & \textbf{83.76} & \textbf{78.26} & \textbf{87.24} & \textbf{85.13} & \textbf{84.84} & \textbf{83.93}  \\
    \bottomrule
  \end{tabular}
\end{table}


% \begin{figure}[!ht]
%   \centering
%   \includegraphics[width=12cm]{figures/spider_full.png}
%   \caption{Accuracy comparison with all tested methods over unforeseen corruptions.}
%   \label{spider-full-graph}
% \end{figure}



\section{Transfer Learning Settings}
\label{transfer-learning-settings}
The models used are the same models from previous experiments. We follow the common procedure of “fixed-feature” setting, where only a linear layer on top of the pre-trained network is trained. 
We train a linear classifier on CIFAR-100 on top of the pre-trained network which was trained on CIFAR-10. We also train a linear classifier on CIFAR-10 on top of the pre-trained nwork which was trained on CIFAR-100. We train the linear classifier for 100 epochs, and an initial learning rate of 0.1 which is decayed by a factor of 10 at epochs 50 and 75. We used SGD optimizer with momentum 0.9.

% \newpage
\section{Extended visualizations} \label{additional_viz}
In Figure~\ref{tsne2}, we provide additional visualizations of the different adversarial training methods presented above. We visualize the models outputs (logits) using t-SNE with two components on the natural test data and the corresponding adversarial test data generated using PGD$^{20}$ white-box attack with step size 0.003 and $\epsilon=0.031$ on CIFAR-10. 
% Additionally, to further support our claim that DIAL learns better feature embedding, we also visualize in Figure \ref{tsne-feat} the feature space embedding under the same settings.

In Figure \ref{feat_layer_stat2} we visualize statistical differences between natural and adversarial examples in the feature representation layer. Specifically, we show the differences in mean and std on thirty random feature values from the feature representation layer as we pass through a network the natural test examples and their corresponding adversarial examples. We present the results on same network architecture (WRN-34-10), trained using three different training procedures: naturally trained network, network trained using standard adversarial training (AT) \cite{madry2017towards}, and DIAL on the CIFAR-10 dataset. When the statistical characteristics of each feature differ from each other, it implies that the features layer is less domain invariant. That is, smaller differences in mean/std yields a better invariance to adversarial examples. One can observe that for DIAL, there is almost no differences between the mean/std of natural examples and their corresponding adversarial examples. Moreover, for the vast majority of the features, DIAL present smaller differences compared to the naturally trained model and the model trained with standard adversarial training. Best viewed in colors.

% \begin{figure}[ht]
% \centering
%   \begin{subfigure}{13cm}
%     \centering\includegraphics[width=13cm]{figures/Mean_difference_30_features.png}
%     \caption{Mean difference comparison}
%   \end{subfigure}
%   \begin{subfigure}{13cm}
%     \centering\includegraphics[width=13cm]{figures/Std_difference_30_features.png}
%     \caption{Standard deviation difference comparison}
%   \end{subfigure}
%   \caption{Mean and std differences comparison between DIAL, naturally trained model and model trained using standard adversarial training on thirty random features from the features layer on the CIFAR-10 dataset with WRN-34-10 architecture. Each bar represents the absolute difference between the means/std of the natural examples and the mean/std of their corresponding adversarial examples on this same feature.}
%   \label{feat_layer_stat2}
% \end{figure}

\begin{figure}[ht]
\centering
  \subfigure[Mean difference comparison]{\includegraphics[width=0.5\textwidth]{figures/Mean_difference_30_features.png}}
  \subfigure[Standard deviation difference comparison]{\includegraphics[width=0.5\textwidth]{figures/Std_difference_30_features.png}}
  \caption{Mean and std differences comparison between DIAL, naturally trained model and model trained using standard adversarial training on thirty random features from the features layer on the CIFAR-10 dataset with WRN-34-10 architecture. Each bar represents the absolute difference between the means/std of the natural examples and the mean/std of their corresponding adversarial examples on this same feature.}
  \label{feat_layer_stat2}
\end{figure}


% \begin{figure}[ht]
% \centering
%   \begin{subfigure}{6cm}
%     \centering\includegraphics[width=5cm]{figures/mart_test.png}
%     \caption{\textbf{MART} embedded model output on natural test data}
%   \end{subfigure}
%   \begin{subfigure}{6cm}
%     \centering\includegraphics[width=5cm]{figures/mart_adversarial.png}
%     \caption{\textbf{MART} embedded model output on adversarial test data}
%   \end{subfigure}
  
%   \begin{subfigure}{6cm}
%     \centering\includegraphics[width=5cm]{figures/madry_test.png}
%     \caption{\textbf{AT} embedded model output on natural test data}
%   \end{subfigure}
%   \begin{subfigure}{6cm}
%     \centering\includegraphics[width=5cm]{figures/madry_adversarial.png}
%     \caption{\textbf{AT} embedded model output on adversarial test data}
%   \end{subfigure}
  
%     \begin{subfigure}{6cm}
%     \centering\includegraphics[width=5cm]{figures/atda_test.png}
%     \caption{\textbf{ATDA} embedded model output on natural test data}
%   \end{subfigure}
%   \begin{subfigure}{6cm}
%     \centering\includegraphics[width=5cm]{figures/atda_adversarial.png}
%     \caption{\textbf{ATDA} embedded model output on adversarial test data}
%   \end{subfigure}
%   \caption{t-SNE embedding of model output in two-dimensional space for MART, AT, and ATDA under natural and adversarial test data from CIFAR-10.}
%   \label{tsne2}
% \end{figure}

% \paragraph{Domain adaptation theory.}

\begin{figure}[H]
\centering
  \subfigure[\textbf{MART} embedded model output (logits) on natural test data]{\includegraphics[width=0.35\textwidth]{figures/mart_test.png}}
  \subfigure[\textbf{MART} embedded model output (logits) on adversarial test data]{\includegraphics[width=0.35\textwidth]{figures/mart_adversarial.png}}
  
    \subfigure[\textbf{AT} embedded model output (logits) on natural test data]{\includegraphics[width=0.35\textwidth]{figures/madry_test.png}}
  \subfigure[\textbf{AT} embedded model output (logits) on adversarial test data]{\includegraphics[width=0.35\textwidth]{figures/madry_adversarial.png}}
  
    \subfigure[\textbf{ATDA} embedded model output (logits) on natural test data]{\includegraphics[width=0.35\textwidth]{figures/atda_test.png}}
  \subfigure[\textbf{ATDA} embedded model output (logits) on adversarial test data]{\includegraphics[width=0.35\textwidth]{figures/atda_adversarial.png}}
  \caption{t-SNE embedding of model output, \textbf{logits}, in two-dimensional space for MART, AT, and ATDA under natural and adversarial test data from CIFAR-10.}
  \label{tsne2}
\end{figure}


\newpage
\section{In-depth Analysis}
Additionally, we present two additional results and visualizations that can show our performance difference is solid.
First, in Figure \ref{tsne-feat} we present a 2d T-SNE plot on the \textbf{features} layer (unlike the logits layer , which is more related to the quantitative results), to further demonstrate that our method indeed learns a domain invariant feature representation better than the other methods, and compared it to TRADES.
Second, we wish to demonstrate that our performance gain is not a due to specific aspects of the domain (e.g., improvement only on specific classes). To do so, we visualize in figures \ref{trades-dial-adv}, \ref{trades-dial-clean} the classification improvement obtained by our method on each class in CIFAR-10. As can be seen, on the natural examples, our method improved the accuracy on all classes, and on the adversarial examples, our method improves robustness on 9 out of 10 classes. This further demonstrates the generalization of our approach.



\begin{figure}[ht]
\centering
    \subfigure[\textbf{DIAL} embedded \textbf{features} on natural test data]{\includegraphics[width=0.35\textwidth]{figures/feat_domain_with_ce_test.png}}
  \subfigure[\textbf{DIAL} embedded \textbf{features} on adversarial test data]{\includegraphics[width=0.35\textwidth]{figures/feat_domain_ce_adversarial.png}}

    \subfigure[\textbf{TRADES} embedded \textbf{features} on natural test data]{\includegraphics[width=0.35\textwidth]{figures/feat_trades_test.png}}
  \subfigure[\textbf{TRADES} embedded \textbf{features} on adversarial test data]{\includegraphics[width=0.35\textwidth]{figures/feat_trades_adversarial.png}}
  
    
%     \subfigure[\textbf{AT} embedded model output on natural test data]{\includegraphics[width=0.35\textwidth]{figures/feat_madry_test.png}}
%   \subfigure[\textbf{AT} embedded model output on adversarial test data]{\includegraphics[width=0.35\textwidth]{figures/feat_madry_adversarial.png}}
  \caption{Visualizing the two-dimensional T-SNE \textbf{feature space embedding} of DIAL and TRADES for (1) natural test data, and (2) adversarial test data from CIFAR-10.}
  \label{tsne-feat}
\end{figure}


\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/trades_dial_kl_adv_per_cls.png}
  \caption{$\DIAL_{\kl}$ and TRADES Robustness (\%) for each class on CIFAR-10. Adversarial examples are generated using PGD-20. Our method manages to improve robustness over TRADES on 9 out of 10 classes. Green annotation presented the difference percentage improvement.}
  \label{trades-dial-adv}
\end{figure}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/trades_dial_ce_clean_per_cls.png}
  \caption{$\DIAL_{\ce}$ and TRADES natural accuracy (\%) for each class on CIFAR-10. Our method manages to improve natural accuracy on all 10 classes. Green annotation presented the difference percentage improvement.}
  \label{trades-dial-clean}
\end{figure}

% \newpage
% \section{Related work loss function comparison}
% \label{related-loss-func}
% In Table \ref{loss-func-comaprison} we further illustrate the loss functions of the different methods. 
% For $\TRADES_{\awp}$, the process involves running TRADES loss function, and then running weight perturbation as defined in equation (10) in \citep{wu2020adversarial}.
% These loss functions can be compared to our proposed loss functions $\DIAL_{\ce}$ and $\DIAL_{\kl}$ which are detailed in section \ref{dial-loss-section}, where the main difference lies in the new natural and adversarial domain losses.

% \begin{table}[ht]
%   \caption{Loss function comparison. Let (x,y) be the natural example and its corresponding label. Let x' be the adversarial example generated from x. $\text{CE}$ refers to the cross-entropy loss function, $\text{KL}$ refers to the KL-divergence loss function, and BCE is the boosted cross entropy loss function. $\mathcal{L}_{CORAL}$ and $\mathcal{L}_{MMD}$ correspond to the correlation alignment and maximum mean discrepancy~\citep{borgwardt2006integrating, sun2016deep}, respectively. $\mathcal{L}_{margin}$ minimize the intra-class variations and maximize the inter-class variations \citep{song2018improving}.
%   $\lambda$ is a hyper parameters to control the ratio between different losses, and $r$ is the reversal ratio hyper parameter. 
%   Let $G_f(\cdot;\theta_f)$ be the feature extractor neural network with parameters $\theta_f$. Let $G_y(\cdot;\theta_y)$ be the label classifier with parameters $\theta_y$, and let $G_d(\cdot;\theta_d)$ be the domain classifier with parameters $\theta_d$. That is, $G(\cdot;\theta) = G_y(G_f(\cdot;\theta_f);\theta_y)$ is essentially the standard model definition (e.g., wide residual network).We define source domain label $d$ as 0 (for natural examples) and target domain label $d^{'}$ as 1 (for adversarial examples).
%   For convenience we present the loss function on a single example.}
%   \vskip 0.1in
%   \label{loss-func-comaprison}
%   \centering
%   \small
%   \begin{tabular}{l|c}
%     \toprule
%     Method & Loss function \\
%     \midrule
%     AT & $\text{CE}(G(x';\theta),y)$ \\
%     \midrule
%     TRADES & $ \text{CE}(G(x;\theta),y) + \lambda \cdot \text{KL}(G(x';\theta) \parallel G(x;\theta))$ \\
%     \midrule
%     MART & $ 
%     \text{BCE}(G(x';\theta),y) + \lambda \cdot \text{KL}(G(x';\theta) \parallel G(x;\theta))\cdot(1- G(x;\theta)_y)$ \\
%     \midrule
%     ATDA & $ \text{CE}(G(x';\theta),y) + \text{CE}(G(x;\theta),y) + \mathcal{L}_{CORAL} + \mathcal{L}_{MMD} + \mathcal{L}_{margin}$ \\
%     \midrule
%     \midrule
%     $\DIAL_{\ce}$ & $\text{CE}(G(x;\theta),y) + \lambda\cdot\text{CE}(G(x';\theta),y)-r(\text{CE}(G_d(G_f(x;\theta_f);\theta_d),d) + \text{CE}(G_d(G_f(x';\theta_f);\theta_d),d'))$ \\
%     \midrule
%     $\DIAL_{\kl}$ & $\text{CE}(G(x;\theta),y) + \lambda\cdot\text{KL}(G(x';\theta) \parallel G(x;\theta)) -r(\text{CE}(G_d(G_f(x;\theta_f);\theta_d),d) + \text{CE}(G_d(G_f(x';\theta_f);\theta_d),d'))$ \\
%     \bottomrule
%   \end{tabular}
% \end{table}