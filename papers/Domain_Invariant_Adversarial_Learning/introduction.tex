Deep learning models have achieved impressive success on a 
%large 
wide
range of challenging tasks. However, their performance was shown to be brittle 
%to 
in the face of
\textit{adversarial examples}: small,
imperceptible
perturbations in the input that 
drastically alter the classification
%flips the decision 
\citep{carlini2017adversarial, carlini2017towards, goodfellow2014explaining, kurakin2016adversarial, moosavi2016deepfool, szegedy2013intriguing, tramer2017ensemble, dong2018boosting, tabacof2016exploring, xie2019improving, rony2019decoupling}.
The problem of
designing reliable robust models has gained significant attention in the arms race against adversarial examples.
Adversarial training \citep{szegedy2013intriguing, goodfellow2014explaining, madry2017towards, zhang2019theoretically} has been proposed as one of the most effective approaches to defend against such examples, and can be described as solving the following min-max optimization problem:
\blfootnote{Our source code is available at \url{https://github.com/matanle51/DIAL}}

\begin{center}
$\min_{\theta}\mathbb{E}_{(x,y)\sim \calD}\sqprn{\max_{x':\norm{x'-x}_p\leq \epsilon}L\paren{x',y;\theta}},$
\end{center}

where $x'$ is the $\epsilon$-bounded perturbation in the $\ell_p$ norm
and $L$ is the loss function.
Different unrestricted attacks methods were also suggested, such as adversarial deformation, rotations, translation and more \citep{brown2018unrestricted, engstrom2018rotation, xiao2018spatially, alaifari2018adef, gilmer2018motivating}.

% Adversarial examples are crafted by taking natural data x with label y and add adversarial perturbation $\mathbb{\alpha}$
% such that $x'= x + \alpha$ such that a classifier $h$ outputs  $h(x') \neq y$. Adversarial examples can be defined as a $\epsilon$-bounded perturbations ~\cite{szegedy2013intriguing} using an $L_p$ norm $||x'-x||_p\leq\epsilon$
% In our work, we address the case where $\Delta=\{ \delta \in \mathbb{R}^{d}  : ||\delta||_p \leq \epsilon \}$ is the set of all possible $\epsilon$-bounded perturbations. 
% Adversarial examples can also be generated by unrestricted attacks such as adversarial deformation, adversarial rotations, adversarial translation and more  ~\cite{brown2018unrestricted, engstrom2018rotation, xiao2018spatially, alaifari2018adef, gilmer2018motivating}. However, the choice of $\epsilon$-bounded perturbations is the most commonly used approach. 
The resulting min-max optimization problem can be hard to solve in general. Nevertheless, in the context of $\epsilon$-bounded perturbations, the problem is often tractable in practice. The inner maximization is usually approximated by generating adversarial examples using projected gradient descent (PGD) \citep{kurakin2016atscale, madry2017towards}. A PGD adversary starts with randomly initialized perturbation and iteratively adjust the perturbation while projecting it back into the $\epsilon$-ball:

\begin{center}
    $x_{t+1}=\Pi_{\mathbb{B}_\epsilon{(x_0)}}\paren{x_{t} + \alpha\cdot \sign(\nabla_{x_{t}}L(G(x_{t}),y))},$
\end{center}


where $x_0$ is the natural example (with or without random noise), and $\Pi_{\mathbb{B}_\epsilon{(x)}}$ is the projection operator onto the $\epsilon$-ball, $G$ is the network, and $\alpha$ is the perturbation step size.  As was shown by \citet{athalye2018obfuscated}, PGD-based adversarial training was one of the few defenses that were not broken under strong attacks.

% That said, the gap between robust accuracy on adversarial examples and natural accuracy on natural examples 
That said, the gap between robust and natural accuracy remains large
for many tasks such as CIFAR-10 \citep{krizhevsky2009learning} and ImageNet \citep{deng2009imagenet}. Generally speaking, \citet{tsipras2018robustness} suggested that robustness may be at odds with natural accuracy, and usually the trade-off is inherent. Nevertheless, a growing body of work aimed to improve the standard PGD-based adversarial training introduced by \citet{madry2017towards} in various ways such as improved adversarial loss functions and regularization techniques \citep{kannan2018adversarial, wang2019improving, zhang2019theoretically}, semi-supervised approaches\citep{carmon2019unlabeled, uesato2019labels, zhai2019adversarially}, adversarial perturbations on model weights \citep{wu2020adversarial}, utilizing out of distribution data \citep{lee2021removing}
%, robust pre-training \citep{jiang2020robust, chen2020adversarial} 
and many others. 
%See related work for more details.
We refer to related work
for a more extensive literature review.

\paragraph{Our contribution.} In this work, we  propose a novel approach to regulating
the tradeoff between robustness and natural accuracy. In contrast to the aforementioned works, our method enhances adversarial training by enforcing a feature representation that is invariant across the natural and adversarial domains. We incorporate the idea of Domain-Adversarial Neural Networks (DANN)~\citep{ ganin2015unsupervised, ganin2016domain} directly into the adversarial training process. DANN is a representation learning approach for domain adaptation, designed to ensure that predictions are made based on invariant feature representation that cannot discriminate between source and target domains. This technique is modular and can be easily incorporated into any standard adversarial training algorithm. Intuitively, the tasks of adversarial training and of domain-invariant representation have a similar goal: given a source (natural) domain $X$ and a target (adversarial) domain $X'$, we hope to achieve $g(X) \approx g(X')$, where $g$ is a feature representation function (i.e., neural network). As we present in section \ref{theory}, our work is also theoretically motivated by the domain adaptation generalization bounds.

In a comprehensive battery of experiments on MNIST~\citep{lecun1998gradient}, SVHN~\citep{netzer2011reading}, CIFAR-10~\citep{krizhevsky2009learning} and CIFAR-100~\citep{krizhevsky2009learning} datasets,
we demonstrate that by enforcing domain-invariant representation learning using DANN simultaneously with adversarial training, we gain a significant and consistent improvement in both robustness and natural accuracy compared to other state-of-the-art adversarial training methods, under Auto-Attack~\citep{croce2020reliable} and various strong PGD~\citep{madry2017towards}, and CW~\citep{carlini2017towards} adversaries in white-box and black-box settings. 
Additionally, we evaluate our method using unforeseen ``natural'' corruptions~\citep{hendrycks2018benchmarking}, unforeseen adversaries (e.g., $\ell_{1}$, $\ell_{2}$), transfer  learning, and perform ablation studies.
Finally, we offer a novel score function for  quantifying the robust-natural accuracy trade-off.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/dann_new5.png}
  \caption{Illustration of the proposed architecture to enforce domain invariant representation. The feature extractor and label classifier form the a regular DNN architecture that can be used for the main natural task. The domain classifier is incorporated alongside the label classifier. The reversal gradient layer multiplies the gradient by a negative number during the back-propagation.}
  \label{dann}
\end{figure}
