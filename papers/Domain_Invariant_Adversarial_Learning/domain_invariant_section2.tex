In this section, we introduce our Domain Invariant Adversarial Learning (DIAL) approach for adversarial training. The source domain is the natural dataset, and the target domain is generated using adversarial attack on the natural domain. We aim to learn a model that has low error on the source (natural) task (e.g., classification) while ensuring that the internal representation cannot discriminate between the natural and adversarial domains. %By doing so 
In this way,
we enforce additional regularization on the feature representation, which 
enhances robustness.
%results in a better robust representation. 

%%%%%%%%%%%%%%%%%%%%%%% theory %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The benefits of invariant representation to adversarial examples}
The motivation behind the proposed method is to enforce an invariant feature representation to adversarial perturbations. Given a natural example $x$ and its adversarial counterpart $x'$, if the domain classifier manages to distinguish between 
them, this means
%$x$ and $x'$, this implies 
that the perturbation has induced a 
significant
difference in the feature representation. We impose an additional loss on the natural and adversarial domains in order to discourage this behavior. 
% Let $\calD$ be a distribution on the instance space $\calX$. Let $\calZ \subseteq \R^d$ be the feature space and $\calY$ to be the label space. The learner aims to learn an unknown target function $c : \calX \rightarrow \calY$
% given an i.i.d. sample $\calS = \{x_i,c(x_i)\}_{i=1}^n$.
% The learning procedure is comprised from learning a representation function $r:\calX \rightarrow \calZ$ and a function $h\in \calH$ where $\calH$ is a fixed hypotheses class. 
% % We denote by $\tilde{\calD}$ the induced distribution over the feature space $\calZ$.
% The error of the learner on natural examples is $\E_{x\sim \calD} \sqprn{(r\circ h)(x) \neq c(x)}$ and on adversarial examples $\E_{x\sim \calD} \sqprn{\max_{x':\norm{x'-x}_p\leq \epsilon}(r\circ h)(x') \neq c(x)}$. The goal is to keep them both as low as possible. In order to do so, the representation function should be invariant to natural examples and their adversarial counterparts. That is, $\norm{r(x)-}$

% feature representation. Such a representation serves two main objectives: it should be discriminative for the main learning task, while being non-discrimative between natural examples and their adversarial counterparts.


We demonstrate that the feature representation layer does not discriminate between natural and adversarial examples, namely $G_f(x;\theta_f)\approx G_f(x';\theta_f)$. Figure  \ref{feat_layer_stat} presents the scaled mean and standard deviation (std)
of the
absolute differences between the natural examples from test and their 
corresponding 
adversarial 
examples on different features from the feature representation layer.
Smaller differences in the mean and std
imply a higher domain invariance
%yields better domain invariant. 
--- and indeed, DIAL achieves
near-zero differences almost across the board.
%We can observe that for DIAL, most of the differences are zero, or near-zero.
Moreover, DIAL's feature-level invariance almost consistently 
outperforms
%better compared to 
the naturally trained model (model trained without adversarial training), and the model trained using standard adversarial training
techniques~\citep{madry2017towards}. We provide additional features visualizations in Appendix \ref{additional_viz}. 

Recently, other communities also discovered the benefits of adopting analogous architectures to DANN, such as the contrastive learning community which used similar architecture to improve representation learning \citep{dangovski2021equivariant, wang2021residual}

\begin{figure}[ht]
\centering
  \subfigure[Mean difference comparison]{\includegraphics[width=0.47\textwidth]{figures/Mean_difference_5_features.png}}
  \subfigure[Standard deviation difference comparison]{\includegraphics[width=0.47\textwidth]{figures/Std_difference_5_features.png}}
    \caption{We visualize the (a) Mean and (b) standard deviation (std) differences comparison between three models: (1) Naturally trained model (without adversarial training), named Clean. (2) Model trained using standard adversarial training, named standard AT, and (3) Model trained using our method, DIAL.
    We visualize five random features from the features layer. Each bar represent the difference between the means/std of the natural examples and the mean/std of their corresponding adversarial examples on this same feature.}
  \label{feat_layer_stat}
  \label{pgd_cifar10}
\end{figure}


% \begin{figure}[h]
% \centering
%   \begin{subfigure}{6cm}
%     \centering\includegraphics[width=6cm]{figures/Mean_difference_5_features.png}
%     \caption{Mean difference comparison}
%   \end{subfigure}
%   \begin{subfigure}{6cm}
%     \centering\includegraphics[width=6cm]{figures/Std_difference_5_features.png}
%     \caption{Standard deviation difference comparison}
%   \end{subfigure}
%   \caption{Mean and std differences comparison between DIAL, naturally trained model and model trained using standard adversarial training on five random features from the features layer. Each bar represent the difference between the means/std of the natural examples and the mean/std of their corresponding adversarial examples on this same feature.}
%   \label{feat_layer_stat}
% \end{figure}



% \newpage



\subsection{Model architecture and regularized loss function}
\label{dial-loss-section}
Let us define the notation for our domain invariant robust architecture and loss. Let $G_f(\cdot;\theta_f)$ be the feature extractor neural network with parameters $\theta_f$. Let $G_y(\cdot;\theta_y)$ be the label classifier with parameters $\theta_y$, and let $G_d(\cdot;\theta_d)$ be the domain classifier with parameters $\theta_d$. 
That is, $G_y(G_f(\cdot;\theta_f);\theta_y)$ is essentially the standard model (e.g., wide residual network~\citep{zagoruyko2016wide}), while in addition, we have a domain classification layer to enforce a domain invariant on the feature representation. An illustration of the architecture is presented in Figure~\ref{dann}.

Given a training set $\{{(x_i,y_i)}\}_{i=1}^{n}$, the natural loss is defined as:
%Figure~\ref{dann} illustrates the high level model architecture. 
%Note the $G_y(G_f(\cdot;\theta_f);\theta_y)$ is exactly the standard architecture used for classification (e.g., wide residual network).
\begin{center}
$\mathcal{L}_{\nat}^y=\frac{1}{n}\sum_{i=1}^{n}{\text{CE}(G_y(G_f(x_i;\theta_f);\theta_y),y_i)}$. 
\end{center}

We consider two basic
forms
of
%variants to represent 
the robust loss. One is the standard cross-entropy (CE) loss between the predicted probabilities and the actual label, which we refer to later as $\DIAL_{\ce}$. The second is the Kullback-Leibler (KL) divergence between the adversarial and natural model outputs (logits), i.e., the class probabilities between the natural examples and their adversarial counterparts, which we refer to as $\DIAL_{\kl}$.

\begin{center}
$\mathcal{L}_{\robloss}^{\ce}= \frac{1}{n}\sum_{i=1}^{n}{\text{CE}(G_y(G_f(x'_i;\theta_f);\theta_y),y_i)}$,

$\mathcal{L}_{\robloss}^{\kl}= \frac{1}{n}\sum_{i=1}^{n}{\text{KL}(G_y(G_f(x'_i;\theta_f); \theta_y)  \parallel G_y(G_f(x_i;\theta_f); \theta_y))}$.
\end{center}

where $\{{(x'_{i},y_i)}\}_{i=1}^{n}$ are the generated corresponding adversarial examples. Next, we define source domain label $d_i$ as 0 (for natural examples) and target domain label $d_i^{'}$ as 1 (for adversarial examples). Then, the natural and adversarial domain losses are defined as:

\begin{center}$
\mathcal{L}_{\nat}^d=\frac{1}{n}\sum_{i=1}^{n}{\text{CE}(G_d(G_f(x_i;\theta_f);\theta_d),d_i)}$,

$\mathcal{L}_{\adv}^d=\frac{1}{n}\sum_{i=1}^{n}{\text{CE}(G_d(G_f(x_i^{'};\theta_f);\theta_d),d'_i)}$.
\end{center}

We can now define the full domain invariant robust loss:

\begin{center}
$\DIAL_{\ce} = \mathcal{L}_{\nat}^y + \lambda\mathcal{L}_{\robloss}^{\ce} -r(\mathcal{L}_{\nat}^d + \mathcal{L}_{\adv}^d)$,

$\DIAL_{\kl} = \mathcal{L}_{\nat}^y + \lambda\mathcal{L}_{\robloss}^{\kl} -r(\mathcal{L}_{\nat}^d + \mathcal{L}_{\adv}^d)$.
\end{center}

The goal is to \textit{minimize} the loss on the natural and adversarial classification while \textit{maximizing} the loss for the domains. 
% This way we can achieve feature representation which is domain invariant. 
The {\em reversal-ratio} hyper-parameter %marked as 
$r$
is inserted into the network layers as a gradient reversal layer~\citep{ ganin2015unsupervised, ganin2016domain}  that leaves the input unchanged
during forward propagation and reverses the gradient by multiplying it with a negative scalar
during the back-propagation. 
The reversal-ratio parameter is initialized to a small value and is gradually increased to $r$, as the main objective converges.
This enforces a domain-invariant representation as the training progress: a larger value enforces a higher fidelity to the domain.
\hide{
During the next phase, the standard back-propagation algorithm is modified as follows: instead of updating with a positive multiple of the gradient, scalar $-r$
is used.
\ak{OLD:
This parameter is tuned during the training phase to facilitate the main task converge at the beginning of the training, and enforce a domain invariant representation as the training progress. 
}}
A comprehensive algorithm description can be found in Appendix \ref{algo-appendix}.


% \subsection{Proposed defense algorithm - DIAL}
% Algorithm~\ref{DIAL-algorithm}, which can be found in Appendix \ref{algo-appendix}, describes a pseudo-code of our proposed $\DIAL_{\ce}$ variant. As can be seen, a target domain batch is not given in advance as with standard domain-adaptation task. Instead, for each natural batch we generate a target batch using adversarial training. The loss function is composed of natural and adversarial losses with respect to the main task (e.g., classification), and from natural and adversarial domain losses. By maximizing the losses on the domain we aim to learn a feature representation which is invariant to the natural and adversarial domain, and therefore more robust.

\subsection{Theoretical Analysis through the Lens of Generalization Bounds}
\label{theory}
Our method refers to the natural and adversarial examples as two distinct domains, where the source domain consists of natural examples and the target domain is the adversarially perturbed examples. Given this assumption, we can adapt the generalization bounds from \citet{mansour2009domain} to theoretically justify our approach.

\paragraph{Preliminaries.}
Let $H$ be a set of functions mapping $X$ to $Y$ and let $\mathcal{L}:Y \times Y\rightarrow\mathbb{R}_+$ be a loss function over $Y$.
The $\mathcal{L}_Q(f, g)$ loss functional is defined as the expected loss for any
two functions $f,g : X \rightarrow Y$ in $H$, and any distribution $Q$ over $X$:
\begin{center} 
$\mathcal{L}_Q(f,g) = \mathbb{E}_{x \sim Q}[L(f(x),g(x))]$.
\end{center}

We also denote $f_Q: X \rightarrow Y$ as the 
% ground truth/labeling 
target function on examples drawn from $Q$. 
% that is, we want to predict $f(x)$.
That is, the error of a function $h\in H$ is defined as 
$\mathcal{L}_Q(h,f_Q) = \mathbb{E}_{x \sim Q}[L(h(x),f_Q(x))]$.

% Let $h_{D}^{\star}=\arg \min_{h\in H} E_{(x,y)\sim D}[h(x)\neq y]$ be the optimal natural function, and respectively let $h_{D_{\text{adv}}}^{\star}=\arg \min_{h\in H} E_{(z,y)\sim D_{\text{adv}}}[h(z)\neq y]$ be the optimal robust function.

We define the  discrepancy distance $\disc_L$ between
two distributions $Q_1$ and $Q_2$ over $X$ by:
\begin{center} 
$\disc_L(Q_1,Q_2)=\max_{h,h'\in H} |\mathcal{L}_{Q_1}(h,h')-\mathcal{L}_{Q_2}(h,h')|$.
\end{center}

\paragraph{Reduction to domain adaptation.} Given source (natural) distribution $D$, we define the target (adversarial) distribution $D_{\text{adv}}$, such that every pair $(x,y)$ in the support of $D$ is mapped to $(z,y)$, where $z\in \mathbb{B}_{\epsilon}(x)$ and $D_{\text{adv}}(z,y)=D(x,y)$.

Let $h_{D}^{\star}=\arg \min_{h\in H}\mathcal{L}_D(h,f_{D})$ be the optimal natural function, and let $h_{D_{\text{adv}}}^{\star}=\arg \min_{h\in H}\mathcal{L}_{D_{\text{adv}}}(h,f_{D_{\text{adv}}})$ be the optimal robust function.
Additionally, we assume that the labeling function $f_{D_{\text{adv}}}$ is the same as $f_D$.

Under these assumptions, we reduce our problem to a domain adaptation one, which consists of selecting a hypothesis $h \in H$ with a small expected
loss according to the target distribution.

We can now adapt Theorem 8 in \citep{mansour2009domain} to our case and bound the adversarial loss by:

\begin{equation}
\begin{aligned}
\label{bound}
\mathcal{L}_{D_{\text{adv}}}(h,f_{D_{\text{adv}}})
&\leq
\overbrace{ \mathcal{L}_{D_{\text{adv}}}(h_{D_{\text{adv}}}^{\star}, f_{D_{\text{adv}}})}^{\text{adv}}
+
\overbrace{\mathcal{\mathcal{L}}_{D}(h,h_{D}^{\star})}^{\text{natural}}
\\
&+
\underbrace{\mathcal{L}_{D}(h^{\star}_{D},h^{\star}_{D_\text{adv}})}_{\text{trade-off}}
+\underbrace{\disc_{L}(D_{\text{adv}}, D)}_{\text{discrepancy}}.
\end{aligned}
\end{equation}

The bound on the adversarial loss consists of four terms: \textit{first term} - approximation error of $H$ for adversarial distribution. \textit{second term} - estimation error on natural examples for the output function on the true distribution (compared to the optimal $h \in H$). \textit{third term} - trade-off error (depends on $H$, $D$, $D_{adv}$, and not the algorithm output) that represents the fraction of points for which the optimal $h \in H$ on adversarial examples are wrong, compared to the optimal $h \in H$ on clean examples. \textit{forth term} - the discrepancy distance between $D$ and $D_{adv}$.

Therefore, the adversarial loss bounded in \eqref{bound} depends, in addition to the natural and adversarial losses, on the discrepancy distance, $\disc_{L}$, between the two distributions. The discrepancy distance will decrease as the two distributions $D$ and $D_{\text{adv}}$ will be similar to one another.

Our adversarial training method, DIAL, minimizes the adversarial and natural losses, and simultaneously learns an invariant feature representation between the adversarial and natural domains. This representation ensures that adversarial examples and their natural counterparts will be invariant to the domain they were taken from, and therefore makes the two distributions $D$ and $D_{\text{adv}}$ similar to each other.
\textit{Altogether, DIAL also minimizes the discrepancy distance, which in turn leads to minimizing the upper bound on the adversarial error}.


% \paragraph{White box.}

% $$\ell^{0\text{-}1}(f,x,y) = I[f(x)\neq y]$$ 
% $$\ell^{0\text{-}1}_{U}(f,x,y) = \sup_{z\in U(x)}I[f(z)\neq y]$$ 


% $$
% h_{D}^{\star}=\arg \min_{h\in H} E_{(x,y)\sim D}[\ell^{0\text{-}1}(h,x,y)]
% $$

% $$
% h_{rob}^{\star}=\arg \min_{h\in H} E_{(x,y)\sim D}[\ell_U^{0\text{-}1}(h,x,y)]
% $$

% % Let $h_{D}^\star = \arg \min_{h\in H} (err_{D}\paren{h;D})$ be the optimal natural function, and  $h_{rob}^\star = \arg \min_{h\in H} (err_{rob}\paren{h;D})$ the optimal robust function respectively.




% % $$err\paren{h;D}=E_{(x,y)\sim D}I\{h(x)\neq y\}$$

% $$
% \mathcal{L}^D_{nat}(f_1,f_2) = E_{(x,y)\sim D}[\ell^{0\text{-}1}(f_1,x,y)\neq \ell^{0\text{-}1}(f_2,x,y)]
% $$

% $$
% \mathcal{L}^D_{rob}(f_1,f_2) = E_{(x,y)\sim D}[\ell^{0\text{-}1}_U(f_1,x,y)\neq \ell^{0\text{-}1}_U(f_2,x,y)]
% $$

% $$
% disc_{D,U}=max_{h_1,h_2\in H} |\mathcal{L}^D_{nat}(h_1,h_2)-\mathcal{L}^D_{rob}(h_1,h_2)|
% $$


% Given source (natural) distribution $D$, we define the target (robust) distribution
% $D_{rob}(f)$.
% For $(x,y)$ and $f$, define
% $u(x,y,f) = {\arg \max}_{z\in B_r(x)}\{f(z)\neq y\}$. Then
% $D_{rob}(f)(z,y)=D(x,y)$ for all $(x,y)\in D$.
% % support:
% % $$
% % \{D(z,y)=D(x,y): z = {\arg \max}_{z\in B_r(x)}\{f(z)\neq y\}, D(x,y)>0\}
% % $$
% % $$
% % \{D(z,y): \forall{(x,y) \in D: z = {\arg \max}_{z\in B_r(x)}\{f(z)\neq y\}}
% % $$









%%%%%%%%%%%%%%%%%%%%%%% theory %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Related work loss function comparison}
\label{related-loss-func}
In Table \ref{loss-func-comaprison} we further illustrate the loss functions of the different methods. 
For $\TRADES_{\awp}$, the process involves running TRADES loss function, and then running weight perturbation as defined in equation (10) in \citep{wu2020adversarial}.
These loss functions can be compared to our proposed loss functions $\DIAL_{\ce}$ and $\DIAL_{\kl}$ which are detailed in section \ref{dial-loss-section}, where the main difference lies in the new natural and adversarial domain losses. We colored in red the unique terms of our method.

\begin{table}[ht]
  \caption{Loss function comparison. Let (x,y) be the natural example and its corresponding label. Let x' be the adversarial example generated from x. $\text{CE}$ refers to the cross-entropy loss function, $\text{KL}$ refers to the KL-divergence loss function, and BCE is the boosted cross-entropy loss function. $\mathcal{L}_{CORAL}$ and $\mathcal{L}_{MMD}$ correspond to the correlation alignment and maximum mean discrepancy~\citep{borgwardt2006integrating, sun2016deep}, respectively. $\mathcal{L}_{margin}$ minimize the intra-class variations and maximize the inter-class variations \citep{song2018improving}.
  $\lambda$ is a hyper parameters to control the ratio between different losses, and $r$ is the reversal ratio hyper parameter. 
  Let $G_f(\cdot;\theta_f)$ be the feature extractor neural network with parameters $\theta_f$. Let $G_y(\cdot;\theta_y)$ be the label classifier with parameters $\theta_y$, and let $G_d(\cdot;\theta_d)$ be the domain classifier with parameters $\theta_d$. That is, $G(\cdot;\theta) = G_y(G_f(\cdot;\theta_f);\theta_y)$ is essentially the standard model definition (e.g., wide residual network).We define source domain label $d$ as 0 (for natural examples) and target domain label $d^{'}$ as 1 (for adversarial examples).
  For convenience, we present the loss function on a single example.}
  \vskip 0.1in
  \label{loss-func-comaprison}
  \centering
  \small
  \begin{tabular}{l|c}
    \toprule
    Method & Loss function \\
    \midrule
    AT & $\text{CE}(G(x';\theta),y)$ \\
    \midrule
    TRADES & $ \text{CE}(G(x;\theta),y) + \lambda \cdot \text{KL}(G(x';\theta) \parallel G(x;\theta))$ \\
    \midrule
    MART & $ 
    \text{BCE}(G(x';\theta),y) + \lambda \cdot \text{KL}(G(x';\theta) \parallel G(x;\theta))\cdot(1- G(x;\theta)_y)$ \\
    \midrule
    ATDA & $ \text{CE}(G(x';\theta),y) + \text{CE}(G(x;\theta),y) + \mathcal{L}_{CORAL} + \mathcal{L}_{MMD} + \mathcal{L}_{margin}$ \\
    \midrule
    \midrule
    $\DIAL_{\ce}$ & $\text{CE}(G(x;\theta),y) + \lambda\cdot\text{CE}(G(x';\theta),y) \textcolor{red}{-r(\text{CE}(G_d(G_f(x;\theta_f);\theta_d),d) + \text{CE}(G_d(G_f(x';\theta_f);\theta_d),d'))}$ \\
    \midrule
    $\DIAL_{\kl}$ & $\text{CE}(G(x;\theta),y) + \lambda\cdot\text{KL}(G(x';\theta) \parallel G(x;\theta)) \textcolor{red}{-r(\text{CE}(G_d(G_f(x;\theta_f);\theta_d),d) + \text{CE}(G_d(G_f(x';\theta_f);\theta_d),d'))}$ \\
    \bottomrule
  \end{tabular}
\end{table}