 \section{Information-leakage Attacks}\label{sec:dataleakage}
In this section, we discuss techniques which demonstrate information-leakage through various processor components such as ShM, GlM and texture memory (Section \ref{sec:leakcomponents}), and through GPU binaries (Section \ref{sec:leakagebinaries}). 
Also, we discuss leakage in different contexts, such as web-browsing/graphics applications (Section \ref{sec:leakagewebbrowser}) and virtualized systems (Section \ref{sec:leakagevirtualized}). Finally, we discuss use of taint-analysis technique for tracking the flow of sensitive data (Section \ref{sec:taintanalysis}) and use of SMM for protecting sensitive data (Section \ref{sec:kimSMM}).


\subsection{Leakage through processor components}\label{sec:leakcomponents}   
  
Pietro et al. \cite{pietro2016cuda} show information leakage in ShM, GlM and registers by using CUDA memory (de)allocation commands. Let $P_b$  and $P_m$ refer to a benign and a malicious process, respectively. For attacking the ShM, assume that $P_b$ runs a kernel which copies a vector from GlM to ShM. This kernel is run $Q$ times. The size of the vector is equal to that of ShM. $P_m$ runs a kernel which reads ShM and this kernel is also run for $Q$ times.   Specifically, $P_m$ allocates a vector in ShM which is of the same size as ShM. Then, it copies data from this vector to a vector in GlM. Using this, $P_m$ can read the entire data written by $P_b$ in ShM. The only requirement is that $P_m$ should be scheduled before $P_b$ terminates. The attack sequence is summarized in Figure \ref{fig:sequenceForLeakage}(a). They also observe that, before the host executes {\tt exit()} function, the ShM is zeroed. 
    
\begin{figure} [h]
\centering
\includegraphics[scale=0.40]{wideSlide2-crop.pdf}
\caption{The sequences for causing leakage on (a) ShM (b) GlM and (c) registers \cite{pietro2016cuda} }\label{fig:sequenceForLeakage}
\end{figure}

For attacking global memory, assume that the $P_b$ allocates 4 vectors (V1 to V4) of equal size using {\tt cudaMalloc}. Then, it initializes V1 and V2 and copies them to V3 and V4, respectively. Then $P_b$ terminates and $P_m$ is scheduled. Using Unix sockets, correct synchronization is maintained between them. $P_m$ also allocates four vectors of the same size as done by $P_b$. However, instead of initializing, $P_m$ executes the same kernel code as executed by $P_b$ and copies V3 and V4 to host memory. They observe that $P_m$ finds the same data as written by $P_b$. Thus, the entire data written on global memory can be leaked in a deterministic manner, as long as $P_m$ allocates the same amount of memory as released by $P_b$. Figure \ref{fig:sequenceForLeakage}(b) summarizes the sequence for causing leakage in GlM. 

For leaking data through the register, they exploit ``register spilling'' mechanism . A process can reserve more registers than those available on the GPU. Variables which cannot be placed in the register are placed in GlM and this is termed as register spilling. An attacker can exploit this to access GlM reserved for other CUDA contexts, even when the benign process owns them and has not freed them using {\tt cudaFree}. This makes this attack very dangerous. As for the attack procedure, $P_b$ writes to GlM multiple times and $P_m$ attempts to read the memory allocated to $P_b$. $P_m$ reserves a fixed number of registers and copies their content to host memory. They executed $P_b$ and $P_m$ simultaneously and observed that $P_m$ could read the memory reserved by $P_b$ before it is released. Thus, the attacker can bypass runtime access primitives.  However, this attack does not allow interfering with the computations performed by the benign process. Also, this attack was successful on Kepler GPU but not on Fermi GPU. The event sequence for causing leakage in registers is summarized in Figure \ref{fig:sequenceForLeakage}(c). Table \ref{tab:summaryOfCUDALeaks} summarizes the three attacks proposed by them. 

\begin{table}[htbp]
  \centering 
  \caption{Characteristics of attacks on ShM, GlM and registers \cite{pietro2016cuda}}
    \begin{tabular}{|l|l|p{9cm}|} \hline
          & Leakage & Conditions \\ \hline
    Shared Memory & Complete & $P_m$ should be scheduled before $P_b$ terminates  
    \\ \hline
    Global Memory & Complete & $P_{b}$ has terminated and $P_{m}$ allocates the same amount of memory as $P_{b}$ \\ \hline
    Registers & Partial & None \\ \hline
    \end{tabular}%
  \label{tab:summaryOfCUDALeaks}%
\end{table}% 
 
 
 
As for mitigation of attacks on GlM and ShM, they propose the use of data-shredding (i.e., zeroing).  For thwarting register-attacks, the GPU driver should (1) forbid spilling to memory locations in GlM which are already reserved by other processes and (2) reset the locations of spilled registers after releasing them. 
 
 
 
 
Bellekens et al. \cite{bellekens2015data} discuss ways in which remanent sensitive data may be leaked from the global, shared and texture memory of a GPU. For GlM, a benign user transfers data from the host to the GPU using {\tt cudaMalloc} and {\tt cudaMemcpy}. Then, the adversary executes a program which allocates same or larger amount of memory using {\tt cudaMalloc}. Then, it dumps the left-over data in GPU memory to the CPU. By allocating the memory, the adversary can partially or fully recover the data in GPU memory. 


For leaking data from ShM, they send the data to GlM and then store it in ShM. Then, the adversary runs a program which requires a maximum size uninitialized array in ShM. Using standard I/O, the adversary can get the data of ShM or dump it to global memory. For leaking data from the texture memory, they first transfer the data from CPU and bind it into texture memory using {\tt cudaBindTextureToArray}. Then, the adversary can use the same approach as that used for ShM to leak the data. 

They note that different grades of GPUs, viz., server, consumer and mobile grades are vulnerable to such leaks. Further, data remanence can be avoided only by using ``hard reboot'' since the ``GPU reset'' mechanism does not prevent such leaks. As for software schemes for mitigating data leaks, they propose writing zero or misleading data during initialization. This can be performed using a separate thread. Also, along with the benign process, a secondary process may be run on the GPU. In case of abrupt termination of the benign process, the secondary process can immediately erase the data to avoid data-leak. 
 


Patterson \cite{patterson2013vulnerability} notes that unlike CPUs, GPUs do not implement ASLR (``address space layout randomization'') or virtual memory. Hence, on GPUs, pointer allocations repeatedly return the same addresses and thus, the data-address can be determined. In the absence of virtual memory, logical addresses of different processes access the same physical memory \cite{mittal2017SurveyTLB}. Since GPUs do not erase the memory, the data persists in memory even after program termination. This attack can be extended to find specific data types. They also note that this attack succeeded even across different users and login sessions. 
    
\subsection{Leakage through binaries}\label{sec:leakagebinaries}

Bellekens et al. \cite{bellekens2016strategies} note that CUDA compilers generate ELF binaries which have high-level assembly PTX code. Note that ELF is a file format for shared binaries, object code and executable files.  This information about assembly code may be used for malicious purposes or reverse engineering. They show that by modifying and compiling PTX code in a just-in-time manner, even simple dynamic analysis can reveal details about the source-code. Combining this with debugging tools allows performing both dynamic and static analysis of the binaries. To thwart these attacks, they suggest distributing GPU-architecture specific binaries without PTX portion, instead of distributing platform-independent binaries because the former makes it difficult to perform reverse engineering. They also highlight the default parameters of NVCC compiler which generate binaries that reveal information about function declaration and parameters. Also, binaries can reveal more information than debugging tools. They further suggest strategies for thwarting binary analysis, e.g., obfuscating the function/variable names.



  
\subsection{Leakage in browsing and graphics applications}\label{sec:leakagewebbrowser} 

Lee et al. \cite{lee2014stealing} note that GPUs do not erase global, shared and local memories, which makes them vulnerable to attacks. They discuss two attack points: end of GPU context (EoC) and end of GPU kernel (EoK), which attack GPU during program execution and after program termination, respectively. Figure \ref{fig:stealingWebpagesAttack}(a) shows the normal GPU execution. In EoC attack, the GPU memory is dumped after the victim frees its memory, as shown in Figure  \ref{fig:stealingWebpagesAttack}(b). Thus, if the victim does not clear its global memory, the adversary can leak global memory data, along with constant data, kernel code,  and their call-by-value arguments. Based on the memory usage history, the adversary can observe when the victim has (de)allocated global memory. 

  \begin{figure} [htbp]
\centering
\includegraphics[scale=0.35]{EoC_EoK1-crop.pdf}
\caption{Illustration of (a) normal execution (b) EoC attack and (c) EoK attack \cite{lee2014stealing}}\label{fig:stealingWebpagesAttack}
\end{figure}
 

In EoK attack, shown in Figure \ref{fig:stealingWebpagesAttack}(c), the registers (termed as ``private memory'' in OpenCL \cite{opencl_nvidiagpu})  and shared memory of victim's kernel are stolen after termination of the victim's kernel.  Since GPUs do not allow pre-emptive scheduling, long GPU programs are run using multiple same/different kernels. These kernels produce and consume intermediate results which can be leaked by EoK. In NVIDIA GPUs where the L1 cache and shared memory can exchange space \cite{harris2013sharedmemory}, EoK attack can also obtain the data from the L1 data cache. 

In EoC attack, the adversary waits till the victim program's GPU context ends and gets the final data from the global memory, if it has not been cleared by the victim program. The adversary dumps the data to the host memory only after the GPU context ends.
In EoK attack, the adversary targets to get the intermediate data between kernels of the victim program. The adversary runs a loop to dump data to the host memory after each kernel ends. The loop ends when the GPU context ends.
When multiple users are running their kernels on the GPU, the data of only the last kernel can be attacked by EoK, however, the EoC attack can succeed even when multiple victims run kernels sequentially. On AMD GPUs, the available memory size cannot be measured at runtime, and hence, they use the change in kernel execution time for detecting when a victim is using the GPU. The EoC attack is not required in AMD GPUs since an adversary can obtain the whole global memory of an AMD GPU due to its memory management features. 

They demonstrate their attacks using Firefox and Chromium web-browsers. These browsers use GPUs for accelerating webpage rendering and leave rendered webpage textures in the global memory. The textures are stored in a rearranged form using proprietary schemes. Also, due to the use of virtual and page-based memory and presence of non-color data, reconstructing the original texture is infeasible. Hence, they instead seek to infer the visited webpages based on the rearranged textures. The adversary collects memory dumps of certain webpages on the \textit{same GPU} as the victim. Then, he compares them with memory dumps of an unknown webpage. This attack can accurately guess most of the webpages. Further, if the adversary collects image snapshots of certain webpages on \textit{any GPU}, and compares them with memory dumps of unknown webpages, he can still guess nearly half of the webpages. The limitation of their technique is that it uses only side-channel data and cannot obtain raw images from GPU memories. Also, it can match the target against the websites in  a pre-decided list only. 

Zhou et al. \cite{zhou2017vulnerable} discuss an attack on GPU where a nonprivileged adversary can leak sensitive information from the remanent raw data in GPU memory. They first detect data partitions which have a high likelihood of being parts of images. The challenge in this is that the metadata of images are generally stripped. Hence, the accurate metadata such as image dimensions cannot be easily measured. Second, GPUs also store and process non-image data which makes it difficult to distinguish the image data. To resolve these issues, they use many features of image data. Initially, the adversary overwrites entire memory that it can access with a fixed value, e.g., 0xFF. Then, the malicious program executes in the background and monitors the free GPU memory space. A sudden increase in available GPU memory indicates that a victim has released GPU memory portion. The malicious program copies this and since GPU does not erase the memory before reallocation, the adversary can analyze the memory.  

Then, they split the memory dumps into fixed-size partitions and merge those partitions which are contiguous in memory space and are expected to be pixels. The partition size needs to be smaller than a single image but large enough that any large white region does not break-down the image. For example, they find that a partition size of 4K balances these tradeoffs. The partitions with all 0xFF are removed since they are unlikely to have been modified after initialization. Also, the partitions with all 0x00 are removed since these partitions have been zeroed by the OS/application. The remaining contiguous blocks are merged to form a tile. Then, remanent partitions of the victim program are extracted. Although skipping partitions with all 0xFF may leave out white spaces of an image, the remaining image is still meaningful and this justifies the choice of 0xFF as the canary. 
     
To identify partitions storing graphical data, they note that pixels are stored as four 8b channels which show red, green, blue and alpha components, respectively. The alpha component, which is 0x00 or 0xFF, shows pixel transparency. Thus, by detecting 0x00 or 0xFF, one can ascertain an opaque pixel or an unused channel. Thus, by examining the alpha value, a graphical partition can be detected. Finally, heading/trailing elements having 0x00/0xFF values are removed and middle values are retained. Generally, a tile has only single image, however, some tiles may have portions from multiple images if they are at nearby locations in the memory.  
 
Then, they detect image area in the tiles with imprecise boundaries, based on insights that (1) successive rows/columns of an image show high similarities which can be detected in the frequency domain (2) before loading into GPU, the image is decompressed and its similarities are preserved. Finally, they rearrange recovered images in proper order. They show that their technique can recover images even under high noise and after performing image transformations such as changing the contrast/brightness. They demonstrate  the recovery of the opened tabs, address bar and page-body from Google chrome, figure-portions and text-lines from recently-opened Adobe Reader documents, whole or portions of images from MATLAB.

 


Similarly, Zhang et al. \cite{zhang2015forensically} study the relationship between a graphic and its organization in GPU memory. Based on this, they present a scheme for visually recovering a graphic from the data stored in GPU memory.

\subsection{Leakage in virtualization and cloud-computing scenarios}\label{sec:leakagevirtualized} 

Maurice et al.  \cite{maurice2014confidentiality} study data leakage in native, virtualized and cloud computing scenarios. The procedure for testing data leakage is as follows. They use a \textit{stain} program which writes pre-determined strings in global memory and two detect programs which search the string written by the \textit{stain} program in GlM. The first detect program uses CUDA API function calls and does not require root privileges. The second detect function uses ``PCI configuration space'' which requires root privileges. Note that PCI configuration space is the medium through which the PCI can automatically configure the devices attached to its bus. 
 


Between stain and detect functions, other actions are also performed. If the detect program can find the string, data leakage is assumed to have occurred. Table \ref{tab:dataleakScenarios} shows scenarios where leakage occurs. In native environment (see `S. No.' 1 in Table \ref{tab:dataleakScenarios}), leakage happens on user-switch, assuming persistence was ON, i.e., the driver remains loaded even when no application is accessing GPU. On soft reboot and GPU reset, leakage happens only if ECC was disabled. In other words, data erase happens as a side-effect of ECC and not due to a security scheme. No leakage happens on hard reset. Thus, GPU maintains data as long as power is turned on.


\begin{table}[htbp]
  \centering
  \caption{Overview of the attacks and results \cite{maurice2014confidentiality}. \cmark \,indicates a leak, and \xmark \, indicates no successful leak. N/A means that the attack is not applicable. $\dagger$: in cloud setup, there is not guarantee that after releasing one VM, the next VM will run on the same physical machine. $\star$: access through PCI configuration space requires root privilege. } 
     \begin{tabular}{|c|l|c|c|c|c|c|c|}\hline
        \multirow{2}[0]{*}{S. No.} & \multirow{2}[0]{*}{Setup} &       \multirow{2}[0]{*}{ECC}  & \multicolumn{5}{c|}{Actions between taint and search } \\ \cline{4-8}
      & &   & switch user     & soft reboot     & reset GPU     & kill VM and start another     & hard reboot\\ \hline
    \multicolumn{8}{|c|}{Access using CUDA API functions} \\ \hline
     \multirow{2}[0]{*}{1} & \multirow{2}[0]{*}{Native} & on    &\cmark     & \xmark     & \xmark   & \multirow{2}[0]{*}{N/A} & \xmark \\ \cline{3-6} \cline{8-8}
       &   & off   & \cmark     & \cmark     & \cmark     &       & \xmark \\  \hline 
    \multirow{2}[0]{*}{2}& \multirow{2}[0]{*}{Virtualized} & on    & \cmark     & \xmark     & \xmark     & \xmark     & \xmark \\ \cline{3-8}
    &      &  off  & \cmark     & \cmark     & \cmark     & \cmark     & \xmark \\ \hline
    \multirow{2}[0]{*}{3}& \multirow{2}[0]{*}{Cloud} & on    & \cmark     & \xmark     & \xmark     & \multirow{2}[0]{*}{N/A$\dagger$} & \multirow{2}[0]{*}{N/A} \\ \cline{3-6} 
    &       & off   & \cmark     & \cmark     &  \cmark    &       &  \\ \hline
    & \multicolumn{7}{|c|}{PCI configuration space access} \\ \hline
     \multirow{2}[0]{*}{4}& \multirow{2}[0]{*}{Native} & on   & \multirow{2}[0]{*}{N/A$\star$} & \xmark     & \xmark     & \multirow{2}[0]{*}{N/A} & \xmark \\ \cline{3-3}\cline{5-6}\cline{8-8}
     &       & off   &       & \cmark     & \cmark     &       & \xmark \\ \hline
     5 & Virtualized & -     & N/A$\star$  & \xmark     & \xmark     & \xmark     & \xmark \\ \hline
    6 & Cloud & -     &N/A$\star$   & \xmark     & \xmark     & N/A$\dagger$  & N/A \\ \hline
     \end{tabular}%
  \label{tab:dataleakScenarios}%
\end{table}%



 

In a virtualized environment (see `S. No.' 2 in Table \ref{tab:dataleakScenarios}), the attacker has full control on the VM and its virtualized GPU. Here, the results are similar as that in a native environment. To study the impact of the hypervisor, they generate a guest VM, run stain function and then destroy the VM. Then, they generate another VM and run detect function. They note that the second guest VM can read the data left by the first guest VM. Clearly, the hypervisor fails to ensure isolation between VMs. The results in a cloud environment (see `S. No.' 3 in Table \ref{tab:dataleakScenarios}) are similar to that in a virtualized environment. Thus, in the cloud, if the attacker can run the VM on the same machine as the previous user, he can leak the data of the user.

Accessing GPU through CUDA API provides only a partial view of the memory which is accessible by MMU. An attacker with root-privilege can use PCI configuration space to access the entire memory, bypassing the MMU. In native environment (see `S. No.' 4 in Table \ref{tab:dataleakScenarios}), the leakage was observed after soft reboot and GPU reset, but not after a hard reset. However, no leakage was observed in virtualized and cloud environment (see `S. No.' 5 and 6 in Table \ref{tab:dataleakScenarios}). 

They note that erasing the memory at allocation time avoids leakage through CUDA APIs, but not through PCI, because in this case, no memory allocation is performed. Hence, it is better to erase the memory at the time of deallocation. Also, cloud providers can erase memory before allocating an instance to the user. Finally, a user can erase the memory before freeing it, although this incurs a performance penalty and may not erase entire memory due to issues such as fragmentation and indeterminism in the behavior of CUDA memory manager.
 
  


 

