\subsection{Covert channel attack}\label{sec:covertchannel}

Naghibijouybari et al. \cite{naghibijouybari2017constructing} note that, as GPUs begin to support multiprogramming, they also become vulnerable to covert-channel attacks   between two concurrently running kernels. 
They assume that the trojan and the spy kernels, which belong to two different applications,  mutually communicate covertly. To ensure  that these kernels co-reside and share resources, they first reverse engineer the scheduling policies of the block and warp schedulers. They note that if the number of blocks in every kernel equals or exceeds the number of SMs on GPU such that no block exhausts the resources of an SM, they can concurrently-reside in an SM.  
The contention on SM is also determined by another level of sharing. Every warp is associated with one of the multiple warp schedulers present on every SM. Warps sharing the same scheduler have different degrees of contention due to  the competition for the issue BW assigned to the scheduler. Since warps are assigned to the warp schedulers in a round-robin manner, the kernel parameters of trojan and spy can be chosen to co-locate them on the same SM and even the same warp scheduler. 
   
They discuss three types of CCs: through caches, functional units and GlM. Cache CC is demonstrated using constant cache, since the small size of L1 and L2 caches allows easy creation of contention. They first deduce the characteristics of constant memory and cache at different levels of hierarchy. For CC through L1 cache, they run two concurrent kernels using two streams on GPU. The trojan kernel encodes `1' or `0' by causing contention or staying idle, respectively. To cause contention on one set, the trojan accesses a single set.  The spy also accesses the same set and records the latency. A high latency implies that a value `1' was transmitted since the data was replaced by a trojan and low latency implies transmission of value `0'. 
  
When co-residency of two kernels on same SM cannot be realized, communication can be achieved through L2 constant cache, which is shared between all SMs. CC creation on the L2 cache is similar to that on L1 cache, except that the size of array accessed by spy/trojan are different due to different cache parameters.  
     
For creating CC through FUs, they observe that contention occurs only between warps belonging to the same warp scheduler. Also, to create contention, both kernel and spy need to send instructions to the same special functional unit (such as the FU performing {\tt \_\_sinf} operation). They ensure that the thread-blocks of the two kernels are co-located on each SM. The spy kernel executes multiple {\tt \_\_sinf} operations. To send `0', the trojan stays idle, and hence, spy observes a latency corresponding to  no contention. To send `1', the trojan executes  {\tt \_\_sinf} operations, and hence, its warps are scheduled to issue instructions along with the spy warps. In this case, the spy observes higher latency indicating presence of contention. Extending the above observation, kernels on different warp schedulers can create parallel CCs for increasing BW.
 

Creating CC through GlM is suitable to provide extra contention when kernels do not reside on the same SM. Due to the high GlM  BW, use of normal load/store instructions does not create contention. Hence, they use atomic operations which cause contention due to a limited number of atomic units. The trojan and spy access two different arrays in GlM and perform atomic operations on a specific GlM address. Although atomic operations are slow, CC through them achieves BW which is comparable to that with other CCs. Table \ref{tab:covertchannelsummary} summarizes the characteristics of CCs through different processor-components and BW achieved by each CC. 

 
\begin{table}[htbp]
  \centering
  \caption{Conditions for constructing CC through various components and the  BW achieved (a ballpark figure) \cite{naghibijouybari2017constructing}. Two kernels refer to trojan and spy kernels.}
    \begin{tabular}{|l|p{12cm}|c|}
    \hline
          & \multicolumn{1}{|c|}{Condition} & BW achieved \\
    \hline
    L1 Cache & Both kernels should reside on the same SM & 40Kbps \\
    \hline
    L2 cache & Both kernels use shared L2 cache but may not not reside on same SM & 20Kbps \\
    \hline
    FU    & Thread-blocks of two kernels should co-reside on every SM since contention happens only between warps of the same warp-scheduler. Also, both kernels need to issue an operation to the same special functional unit & 30Kbps \\
    \hline
    GlM   & Both kernels perform atomic operations on two different arrays located in global memory & 40Kbps \\
    \hline
    \end{tabular}%
  \label{tab:covertchannelsummary}%
\end{table}%


They further discuss strategies for increasing CC BW by achieving synchronization. This avoids loss of BW due to drift between kernels or sequential launch of kernels. Also, they parallelize the transfer to allow communication between multiple trojans and spies. Further, to remove noise in CC due to co-location of other workloads, they modify block-scheduler such that only  trojan and spy kernels and no other workload can run on the same SM. For example, if the spy requests maximum amount of ShM (or threads, registers, etc.)  and the trojan does not request ShM, then they can be co-located on the same SM, but no other workload requiring ShM can co-locate on the same SM. 

To thwart these CCs, GPU resources can be partitioned so that communicating kernels cannot measure mutual contention. For example, cache partitioning  \cite{mittal2017SurveyCachePart} can be used or co-location of different kernels can be avoided. While incurring performance overhead, these approaches can thwart side and covert channel attacks due to uncontrolled access to shared resources. Similarly, by allowing pre-emption, introducing randomness in scheduling policies and noise in latency-measurements, the challenges in launching the attack can be increased. Also, cache bypassing \cite{mittal2016SurveyCacheBypassing} can be used to prevent sensitive data from being stored in the cache. 


  
  

  


  


 








  

  
  

   
