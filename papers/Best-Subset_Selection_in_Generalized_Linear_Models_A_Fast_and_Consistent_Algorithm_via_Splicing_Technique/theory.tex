\section{Theoretical Guarantees}\label{sec:theorical-properties}
This section starts by presenting assumptions for the theoretical analysis.
The statistical performance guarantees and convergence analysis of our algorithm are depicted in Sections~\ref{sec:statistical-guarantees} and~\ref{sec:computational-properties}, respectively;
followed by high-level proofs in Sections~\ref{sec:sketch}.
Unless otherwise specified, detailed proofs are relegated to\if0\informsMOR{
Supplementary Materials.
}\else{
Section~\ref{sec:proofs}.
}\fi
Without loss of generality, assume that the design matrix $\mathbf{X} = (\boldsymbol{x}_1,\ldots,\boldsymbol{x}_n)^\top$ has $\sqrt{n}$-normalized columns, i.e., $\mathbf{X}_j^{\top}\mathbf{X}_j=n,\ j=1,2,\ldots,p$.

\subsection{Assumptions}
\label{sec:assumptions}
% To ensure identifiability, we consider the following two concepts. We say $\mathbf{X}$ satisfied the spectrum restricted condition (SRC) with order $s$ and spectrum bound $\{m_{s},M_{s}\}$, i.e.,
% \begin{align*}
% 0<m_s\leq \frac{\|\mathbf{X}_{\mathcal{A}}\mathbf{u}\|_2^2}{n\|\mathbf{u}\|_2^2}\leq M_s <\infty,\forall \mathbf{u}\neq 0, \mathbf{u} \in \mathbb{R}^{|\mathcal{A}|}, ~\mathrm{with} ~ \mathcal{A} \subset \fullset ~\mathrm{and} ~ |\mathcal{A}|\leq s.
% \end{align*}
% The spectrum of the off-diagonal sub-matrices of $\mathbf{X}^\top\mathbf{X}$ can be bounded by the sparse orthogonality constant $\nu_s$ defined as the smallest number such that \label{con:off-diagonal-spectrum}
% $$\nu_s \geq \frac{\|\mathbf{X}_\mathcal{A}^\top\mathbf{X}_{\mathcal{B}}\mathbf{u}\|_2}{n\|\mathbf{u}\|_2}, \forall \mathbf{u}\neq 0, \mathbf{u}\in \mathbb{R}^{|\mathcal{B}|}~\mathrm{with} ~ \mathcal{A}, \mathcal{B} \subset \fullset, |\mathcal{A}|\leq s, |\mathcal{B}|\leq s ~\mathrm{and} ~ \mathcal{A}\cap \mathcal{B}= \varnothing.$$


We present assumptions below to establish the theoretical properties of our algorithm.
\begin{enumerate}[label=(A\arabic*), start=1]
\item There exists a constant $c_0 \in (0, 1]$ such that, the function $b(\theta)$ is three times differentiable with $c_0\leq b''(\theta)\leq (c_0)^{-1}$ and $|b'''(\theta)| \leq (c_0)^{-1}$ in its domain. \label{con:bound-variance}
\end{enumerate}
%\begin{remark}
Assumption \ref{con:bound-variance} avoids infinitely large or small variances of the response \citep{rigollet2012kullback} as well as to ensure the existence of the Fisher information for statistical inference \citep{fan2013tuning}. This assumption is valid for several commonly used GLMs, such as linear regression, logistic regression, and Poisson regression, where the coefficient estimation at any active set $\mathcal{A}$ should be bounded for the latter two.
%\end{remark}
\begin{enumerate}[label=(A\arabic*), start=2]
\item For the model error $W_i=y_i-\mathbb{E}[y_i]$ ($i = 1, \ldots, n$),
$W_1, \ldots, W_n$ follow a sub-Gaussian distribution, i.e., there exist universal positive constants $c_1,c_2$ such that
$\mathbb{P}(|W_i|\geq t)\leq c_1\exp\{-c_2t^2\}$ for any $t>0$. \label{con:subgaussian}
\end{enumerate}
%\begin{remark}
Assumption \ref{con:subgaussian} is usually assumed in GLMs \citep{sahandnnegahbanUnifiedFrameworkHighDimensional2012}.
This assumption holds when the response comes from Gaussian, Bernoulli, or other distributions with bound support (e.g., the gamma distribution).
%\end{remark}

Before presenting the following assumption, we introduce the spectrum-restricted condition (SRC).
A matrix $\mathbf{M}$ enjoying the SRC with order $s$ and spectrum bound $\{m_{s},M_{s}\}$ means:
\begin{align*}
0<m_s\leq \frac{\|\mathbf{M}_{\mathcal{A}}\mathbf{u}\|_2^2}{n\|\mathbf{u}\|_2^2}\leq M_s <\infty,\forall \mathbf{u}\neq 0, \mathbf{u} \in \mathbb{R}^{|\mathcal{A}|}, ~\mathrm{with} ~ \mathcal{A} \subset \fullset ~\mathrm{and} ~ |\mathcal{A}|\leq s.
\end{align*}
\begin{enumerate}[label=(A\arabic*), start=3]
\item
$\mathbf{X}$ fulfills the SRC with order $2s$ and spectrum bound $\{m_{2s},M_{2s}\}$. \label{con:src}
\end{enumerate}
%\begin{remark}
Assumption \ref{con:src} is a widely used identifiability condition in the literature on high-dimensional GLMs \citep{shi2019linear} and was used to investigate the theory of LASSO and MCP \citep{zhang2008sparsity, zhang2010nearly}.
\ref{con:src} implies that the spectrum of the off-diagonal sub-matrices of $\mathbf{X}^\top\mathbf{X}$ is bounded by a constant $\nu_s$ that
is defined as the smallest value such that \label{con:off-diagonal-spectrum}
$$\nu_s \geq \frac{\|\mathbf{X}_\mathcal{A}^\top\mathbf{X}_{\mathcal{A}^\prime}\mathbf{u}\|_2}{n\|\mathbf{u}\|_2}, \forall \mathbf{u}\neq 0, \mathbf{u}\in \mathbb{R}^{|\mathcal{A}^\prime|}~\mathrm{with} ~ \mathcal{A}, \mathcal{A}^\prime \subset \fullset, |\mathcal{A}|\leq s, |\mathcal{A}^\prime|\leq s ~\mathrm{and} ~ \mathcal{A}\cap \mathcal{A}^\prime= \varnothing.$$
% Assumption \ref{con:off-diagonal-spectrum} is the sparse orthogonality constant and
% Assumption \ref{con:src} is the sparse restricted assumption \citep{zhang2008sparsity}.
%\end{remark}
\begin{enumerate}[label=(A\arabic*), start=4]
\item Let
$C_1 \coloneqq \frac{1}{c_0^2}\left(\frac{M_s}{2c_0} + \frac{\nu_s^2}{m_sc_0^3} + \frac{M_sv_s^2}{2m_s^2c_0^5}\right)\left[( 2 + \Delta+ \frac{2\nu_s}{m_sc_0^2})^2 + (2+\Delta)^2\right]\left(\frac{\nu_s}{m_sc_0^2}\right)^2$
and $C_2 \coloneqq \frac{m_sc_0}{2} - \frac{\nu_s^2}{m_sc_0^3} - \frac{M_sv_s^2}{2m_s^2c_0^5}$,
there exists a constant $\Delta > 0$ such that $\gamma_s\coloneqq \frac{C_1}{(1-\Delta)C_2}<1$.
% , $s\leq s_{\max}$. Note that by the monotonicity of $\gamma_s$ on $s$, it suffices to have $\gamma_{s_{\max}}<1$.
\label{con:technical-assumption}
\end{enumerate}
%\begin{remark}
\ref{con:technical-assumption} is a technical assumption. Roughly speaking, we expect $\nu_s$ to be reasonably small for its establishment. Here we derive sufficient conditions in a simpler form for \ref{con:technical-assumption}.
By \ref{con:src} and Lemma~20 in \citet{huang2017constructive}, we have $\nu_s \leq \kappa_s \coloneqq \max \{1-m_{2s}, M_{2s}-1\}$. As a result, it suffices to have $f(c_0,\,\kappa_s) > 0$, where
% and the intuition behind it are given below.
% which is closely related to the the restricted isometry property \citep{candes2005decoding} constant $\delta_{2s}$ for $\mathbf{X}$.
\begin{align*}
f(c_0,\,\kappa_s)
&\coloneqq \frac{(1-\kappa_s)c_0}{2}-\frac{\kappa_s^2}{(1-\kappa_s)c_0^3}-\frac{(1+\kappa_s)\kappa_s^2}{2(1-\kappa_s)^2c_{0}^5} \\
\quad-& \frac{1}{c_0^2}\left(\frac{1+\kappa_s}{2c_0}+\frac{\kappa_s^2}{(1-\kappa_s)c_0^3}+ \frac{(1+\kappa_s)\kappa_s^2}{2(1-\kappa_s)^2c_0^5}\right)\left[\left(2+\frac{2\kappa_s}{(1-\kappa_s)c_0^2}\right)^2+4\right]\frac{\kappa_s^2}{(1-\kappa_s)^2c_0^4}.
\end{align*}
% Due to $f$'s complicated form, we visualize it as a surface in Figure~\ref{fig:sufficient_condition},
% from which simpler sufficient assumptions like $\{c_0\geq 0.9,\ \kappa_s\leq 0.1\}$ or $\{c_0 = 1,\ \kappa_s\leq 0.183\}$ can be derived.
It holds if sufficient assumptions are made, such as $\{c_0\geq 0.9,\ \kappa_s\leq 0.1\}$ or $\{c_0 = 1,\ \kappa_s\leq 0.183\}$. The latter, in particular, corresponds to the case of linear models, and the sufficient assumption $\kappa_s\leq 0.183$ is identical to its counterpart in Remark 2 in \citet{zhu2020polynomial}.

%\end{remark}
% \begin{figure}[htbp]
% \centering
% % \includegraphics[scale=0.1]{figure/sufficient_assumption.png}
% \vspace{-30pt}
% \includegraphics[scale=0.6]{figure/sufficient_condition.pdf}
% \vspace{-30pt}
% \caption{Visualization of $f(c_{0},\kappa_s)$ as a surface, where the $x$-axis is $c_0$, the $y$-axis is $\kappa_s$, and the $z$-axis is $f(c_0, \kappa_s)$.
% The area $\{(c_0, \kappa_s)|f(c_0, \kappa_s)>0\}$ is colored in red, and blue vice versa.
% The two yellow markers are instances when Assumption \ref{con:technical-assumption} is satisfied.}
% \label{fig:sufficient_condition}
% \end{figure}
\begin{enumerate}[label=(A\arabic*), start=5]
\item $\frac{1}{\min\limits_{j\in \mathcal{A}^*} |\beta_j^*|^2} = o \left(\frac{ n}{s\log p\log\log n}\right)$ \label{con:minimal-signal}
\item $\tau_s = \Theta \left({s\log p\log\log n}\right)$ \label{con:loss-reduce-threshold}
\end{enumerate}
Assumption~\ref{con:minimal-signal} sets a minimal magnitude of the regression coefficient and is a commonly employed assumption for variable selection in GLMs \citep{fan2013tuning}. Assumption~\ref{con:loss-reduce-threshold} ensures that the threshold $\tau_s$ can control random errors and prevent unnecessary splicing iterations that reduce loss negligibly. % and enabling necessary splicing iterations (w.r.t upper bound of $\tau_s$).}
%{\color{red}
%\begin{enumerate}[label=(A\arabic*), start=7]
%	\item $k_{\max} = s$ \label{con:k-max}
%\end{enumerate}
%}
\begin{enumerate}[label=(A\arabic*), start=7]
% % \item $|\mathcal{A}^*| = o(\frac{n}{\log p\log\log n})$. \label{con:true-size}
\item $s_{\max} = O\left((\frac{{n}}{\log p})^{\frac{1}{4}}\right)$. \label{con:maximum-size}
\item There exists a constant $c_3$ such that $\max\limits_{1\leq i \leq n}\max\limits_{1\leq j \leq p} |x_{ij}| \leq c_3$. \label{con:x_bound}
\end{enumerate}
\begin{remark}
% Assumption \ref{con:true-size} imposes that the size of the true active set cannot be too large.
% Assumption \ref{con:true-size} can ensure Algorithm~\ref{alg:abess} not to select too small support size.
Assumption \ref{con:maximum-size} controls the growing rate of $s_{\max}$, which ensures that our algorithm avoids selecting an excessively large support size. When establishing model selection consistency on over-fitted models, the Assumption \ref{con:maximum-size} controls the log-likelihood function's third and higher order derivatives. A stronger assumption is presented in Theorem 2 of \citet{fan2013tuning}, where $s_{\max} = O(\min\{ n^{1/6}(\log p)^{-1/3}, n^{1/8}(\log p)^{-3/8}\})$ is assumed for the case of bounded responses. Assumption~\ref{con:x_bound} is a common assumption in high-dimensional GLMs \citep*[see, e.g.,][]{feiEstimationInferenceHigh2021}, which requires that the predictors are uniformly bounded. This assumption is reasonable since preprocessing can be employed to normalize the predictor matrix $\mathbf{X}$. 
% Ignoring the factor $\log \log n$, Assumption \ref{con:minimal-signal} and \ref{con:true-size} require $n$ to scale as $n = \Omega(s_{\max}\log p)$ while
\end{remark}

\subsection{Statistical Guarantees: Support Recovery}\label{sec:statistical-guarantees}
The statistical properties of the support set $\hat{\mathcal{A}}$ returned by Algorithm~\ref{alg:fbess} are presented in the following.
% Theorem~\ref{lemma:consistency1} shows that Algorithm~\ref{alg:fbess} can recover the true support set when the true support size is known.
\begin{lemma}\label{lemma:consistency1}
%{\color{red}Let $\boldsymbol \beta^*$ be the underlying $s^*$-sparse coefficient vector and $\mathcal{A}^*$ be its support set},
When $s^* \leq s$,
under Assumptions \ref{con:bound-variance}-\ref{con:loss-reduce-threshold}, we have
\begin{equation*}
\mathbb{P}(\hat{\mathcal{A}} \supseteq \mathcal{A}^*) \geq 1 - \delta_1 - \delta_2 - \delta_3,
\end{equation*}
where
$\delta_i = O\left(\exp\left\{\log p - K_i\frac{n}{s} \min\limits_{j\in\mathcal{A}^*} |\boldsymbol{\beta}_j^*|^2 \right\}\right)$
for some constant $K_i>0\ (i = 1,\,2,\,3)$.
\end{lemma}
Lemma~\ref{lemma:consistency1} indicates that our algorithm has screening property --- the estimated support set can cover the true support with high probability.
% Theorem~\ref{lemma:consistency1} indicates that Algorithm~\ref{alg:fbess} identifies
% the true support set with high probability.
Furthermore,
Assumption \ref{con:minimal-signal} ensures that $\delta_1,\ \delta_2,\ \delta_3 \rightarrow 0$ as $n \rightarrow \infty$, which leads to the following asymptotic result in Corollary~\ref{corollary:exact-recovery}.
Corollary~\ref{corollary:exact-recovery} guarantees Algorithm~\ref{alg:fbess} covers the true support set with probability 1
when the sample size is sufficiently large,
which can be witnessed by the numerical results displayed in Figure~\ref{fig:rate_binomial}.
\begin{corollary}\label{corollary:exact-recovery}
If assumptions in Lemma~\ref{lemma:consistency1} hold and $s = s^*$, we have
\begin{equation*}
\mathbb{P}(\hat{\mathcal{A}} = \mathcal{A}^*) \to 1, \textup{ as } n \to \infty.
\end{equation*}
\end{corollary}
The establishment of support recovery relies on the idea that,
if some elements of the true active set are omitted,
the splicing procedure improves the quality of the active set and thus
reduce the value of the loss function.
We rigorously prove this statement via proof by contradiction shown in
\if0\informsMOR{
Supplementary Materials.
}\else{
Section~\ref{sec:proofs}.
}\fi
Our idea discriminates from other approaches that design
the sparse-pursuit operators such that its corresponding the fixed point is the true active set \citep{li2017quadratic, yuan2018ghtp, huang2020support}. Their proposal has yet to leverage the information of loss function to decide convergence, and thus, some additional assumptions are to be imposed. Furthermore, the following theorem guarantees Algorithm~\ref{alg:abess} enjoys the consistency of best-subset recovery property, i.e.,
it can recover the true active set with high probability.

\begin{theorem}[Consistency of Best-Subset Recovery]\label{thm:consistency4}
% Assumed
% assumptions in Lemma \ref{lemma:consistency1} hold. If
Suppose
Assumptions \ref{con:bound-variance}-\ref{con:x_bound} hold, then for some positive constant $\alpha > 0$ and sufficient large $n$,
Algorithm~\ref{alg:abess} identifies the true active set (i.e., $\hat{\mathcal{A}}_{\hat{s}} = \mathcal{A}^*$), with probability at least $1 - O(p^{-\alpha})$.
\end{theorem}
%\begin{theorem}\label{thm:consistency3}
%(Consistency in Best-subset selection)	Denote $(\hat{\beta}, \hat{d}, \hat{\mathcal{A}}, \hat{\mathcal{I}})$ is the solution of Algorithm~\ref{alg:fbess} given support size $T$, and suppose Assumption \ref{con:bound-variance}-\ref{con:off-diagonal-spectrum} holds. If $T = |\mathcal{A}^*|$ and $\frac{1}{1+\Delta}c_{-}(T)\geq 2 c_{+}(T) \big((2+\Delta)\frac{\theta_{T,T}}{c_-(T)}\big)^2$, Then we have
%	\begin{equation*}
% \mathbb{P}(\hat{\mathcal{A}} = \mathcal{A}^*) \geq 1 - \delta_1 - \delta_2 - \delta_3.
%	\end{equation*}
%In addition, under Assumptions \ref{con:src}, we have
%	\begin{equation*}
%	\lim_{n\rightarrow \infty} \mathbb{P}(\hat{\mathcal{A}}=\mathcal{A}^*) = 1.
%	\end{equation*}
%\end{theorem}
%\begin{proof}
%When $T = |\mathcal{A}^*|$, we have $|\mathcal{I}_{12}| = |\mathcal{I}_{21}|$ and $|\mathcal{A}_{12}| = |\mathcal{A}_{21}| $. From the proof of \textbf{Lemma \ref{lemma:consistency1}}, we have
%$$\mathbb{P}\left( (2+\Delta)\theta_{T,T} \|\beta^*_{\mathcal{I}_{1}}\|_2 \geq c_{-}(T)\|\beta^*_{\mathcal{I}_{12}}\|_2 \right)\geq 1 - \delta_1,$$
%and
%\begin{align*}
%\mathbb{P}\left(\|\beta^*_{\mathcal{A}_{12}}\|_2 \leq (2+\Delta)\frac{\theta_{T,T}}{c_{-}(T)}\|\beta_{\mathcal{I}_1}^*\|_2\right)\geq & 1-\delta_2.
%\end{align*}
%Therefore, similar to the argument of \textbf{Lemma \ref{lemma:consistency1}}, we can show that
%	\begin{equation*}
% \mathbb{P}(\hat{\mathcal{A}} = \mathcal{A}^*) \geq 1 - \delta_1 - \delta_2 - \delta_3.
%	\end{equation*}
%By Assumptions \ref{con:src}, we have
%	\begin{equation*}
%	\lim_{n\rightarrow \infty} \mathbb{P}(\hat{\mathcal{A}}=\mathcal{A}^*) = 1.
%	\end{equation*}
%\end{proof}

% The following theorem guarantees Algorithm~\ref{alg:abess} enjoys the consistency of best-subset recovery property, i.e.,
% it can recover the true active set with high probability, which can be witnessed by
% the numerical results displayed in Figure~\ref{fig:rate_binomial}.
% \begin{theorem}\label{thm:consistency4}
% (Consistency of Best-Subset Recovery) Assumed assumptions in Lemma \ref{lemma:consistency1} hold.
% If Assumptions \ref{con:maximum-size} and \ref{con:x_bound} hold, then with probability at least $1 - O(p^{-\alpha})$,
% for some positive constant $\alpha > 0$ and sufficient large $n$,
% Algorithm~\ref{alg:abess} identifies the true active set, i.e., $\hat{\mathcal{A}}_{\hat{s}} = \mathcal{A}^*$.
% \end{theorem}
% \begin{remark}
% %Compare to assumption \ref{con:src}, we need a stronger assumption for true size $|\mathcal{A}^*|$.
% Assumption \ref{con:true-size} can ensure Algorithm~\ref{alg:abess} not to select too small support size. The assumption of maximum support size $s_{\max}$ guarantees our algorithm does not select too big a support size.
% \end{remark}
% From Theorem~\ref{thm:consistency4},
% Algorithm~\ref{alg:abess} exactly identifies the true support set with high probability.
% and Figure~\ref{fig:rate_gamma}.
% Let $\hat{\boldsymbol \beta}^{o}$ be the least square estimator given the true active set $\mathcal{A}^*$,
% so called the oracle estimator, then the following Corollary~\ref{thm:asymtotic}
% guarantees the sparse coefficient estimation given by Algorithm~\ref{alg:abess} equals to $\hat{\boldsymbol \beta}^{o}$ with high probability.
% \begin{corollary}\label{thm:asymtotic}
% (Asymptotic properties)
% Suppose assumptions in Theorem~\ref{thm:consistency4} hold,
% then, with a high probability,
% Algorithm~\ref{alg:abess} outputs a oracle estimator:
% $\mathbb{P}\{\hat{\boldsymbol\beta}_{\hat{s}} = \hat{\boldsymbol\beta}^{o}\} =1 - O(p^{-\alpha})$
% for some $\alpha > 0$ and sufficient large $n$.
% \end{corollary}

\subsection{Computational Properties}\label{sec:computational-properties}
% \begin{theorem}\label{thm:num_of_iter}
% Denote by $(\boldsymbol \beta^q, \mathcal{A}^q)$ the output of the $k$th iteration of Algorithm~\ref{alg:fbess} given support size $s$.
% Suppose that Assumptions \ref{con:bound-variance}-\ref{con:loss-reduce-threshold} hold and $|\mathcal{A}^*|\leq s$.
% Then, with a probability at least $1- \delta_{1}-\delta_{2}-\delta_{3} - \delta_{4}(t)$, we have:
% \begin{enumerate}
% \item[(i)] The total number of iterations needed for structure recovery is upper bounded by
% \begin{align*}
% \mathcal{A}^{*} \subseteq \mathcal{A}^q, \quad \text{if } k > \log_{\frac{1}{\gamma_{s}}}\bigg[\frac{l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)}{(1 - \Delta)C_{2}n\min\limits_{j\in \mathcal{A}^*} |\boldsymbol{\beta}_j^*|^2}\bigg];
% \end{align*}
% \item[(ii)] The $\ell_2$-error of parameter estimation is upper bounded by
% \begin{align*}
% \|\boldsymbol \beta^q-\boldsymbol \beta^{*}\|_{2} & \leq \frac{\left[l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)\right]\left(\frac{\nu_s}{m_sc_0^2}+1\right)}{(1-\Delta)C_{2}n}\gamma_{s}^q + \frac{t}{m_s c_0}.
% \end{align*}
% \end{enumerate}
% Here, $\delta_{1}$, $\delta_{2}$, $\delta_{3}$ are defined in Theorem~\ref{lemma:consistency1},
% and for any $0<t=O\left(\min\limits_{j\in \mathcal{A}^*} |\boldsymbol{\beta}_j^*|\right)$,
% $\delta_{4}(t) \coloneqq O\left(\exp\left\{\log p - K_4\frac{nt^2}{s}\right\}\right)$.
% % $C_2$ and $\Delta$ are defined in Assumptions \ref{con:technical-assumption}.
% % \begin{align*}
% % \delta_{4}(t): = O\left(\exp\left\{\log p - K_4\frac{nt^2}{s}\right\}\right).
% % \end{align*}
% \end{theorem}
Denote $(\boldsymbol \beta^q, \mathcal{A}^q)$ as the output of the $q$-th iteration of Algorithm~\ref{alg:fbess}. The following theorem characterizes the total number of iterations needed for screening support.

\begin{theorem}\label{thm:num_of_iter}
Suppose Assumptions~\ref{con:bound-variance}-\ref{con:loss-reduce-threshold} hold and $s^* \leq s$.
Then, with probability at least $1- \delta_{1}-\delta_{2}-\delta_{3}$, we have $\mathcal{A}^{*} \subseteq \mathcal{A}^q$ if
\begin{align*}
q > \log_{\frac{1}{\gamma_{s}}}\bigg[\frac{l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)}{(1 - \Delta)C_{2}n\min\limits_{j\in \mathcal{A}^*} |\boldsymbol{\beta}_j^*|^2}\bigg],
\end{align*}
where $\delta_{1}$, $\delta_{2}$, $\delta_{3}$ are defined in Lemma~\ref{lemma:consistency1}.
% $C_2$ and $\Delta$ are defined in Assumptions \ref{con:technical-assumption}.
% \begin{align*}
% \delta_{4}(t): = O\left(\exp\left\{\log p - K_4\frac{nt^2}{s}\right\}\right).
% \end{align*}
\end{theorem}
Theorem~\ref{thm:num_of_iter} shows that, within a few
number of iterations, the splicing technique leads to an active set that covers the true active set
(see Proof of Theorem~\ref{thm:num_of_iter} in \if0\informsMOR{Supplementary Material}\else{Section~\ref{sec:proof_num_of_iter}}\fi).
Particularly, a large magnitude of the minimum coefficient lessens
the number of iterations covering the true active set.
% \begin{remark}
% If $s = |\mathcal{A}^*|$, similar to \citet{huang2017constructive}, we can prove Algorithm~\ref{alg:fbess} stops at most $O\Big(\log(|\mathcal{A}^*|R)\Big)$ iteration, where $R = \max\{|\boldsymbol{\beta}_i^*|, i\in \mathcal{A}^*\}/\min\{|\boldsymbol{\beta}_i^*|, i\in \mathcal{A}^*\}$.
% \end{remark}

From Theorem~\ref{thm:num_of_iter}, Algorithm~\ref{alg:fbess} converges with a few iteration.
As such, the proposed Algorithm~\ref{alg:abess} can achieve a polynomial computational time complexity with high probability.
%On the basis of Theorem~\ref{thm:num_of_iter}, the computational complexity of Algorithm~\ref{alg:fbess} is derived below.
%\begin{corollary}\label{thm:complexity}
%	Suppose the assumptions in Theorem~\ref{thm:num_of_iter} hold
%	and Algorithm \ref{alg:fbess} runs with a support size $s$, then
%	\begin{enumerate}
% \item[(i)] If $s<s^*$, its computational complexity is given by
% \begin{align*}
% O\left(\frac{l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)}{s\log p\log\log n}(k_{\max}ns^2 + k_{\max}np)\right);
% \end{align*}
% \item[(ii)] otherwise, for $s\geq s^*$, with probability at least $1- \delta_{1}-\delta_{2}-\delta_{3} - O(p^{1-K\log\log n})$ for some constant $K$, Algorithm \ref{alg:fbess} will successfully cover the support with its computational complexity being
% \begin{align*}
% O\left(\left[\log_{\frac{1}{\gamma_{s}}} \frac{l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)}{s\log p\log\log n}\right](k_{\max}ns^2 + k_{\max}np)\right).
% \end{align*}
%	\end{enumerate}
%	% \begin{align*}
%	% O\Big(\log_{\frac{1}{\gamma_{s}}} \left[\frac{l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)}{s\log p\log\log n}\right]\mathrm{I}(s\geq s^{*}) + \frac{l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)}{s\log p\log\log n}\mathrm{I}(s<s^{*})\Big)\times O(k_{\max}ns^2 + k_{\max}np).
%	% \end{align*}
%\end{corollary}
\begin{theorem}\label{thm:complexity}
Suppose Assumptions \ref{con:bound-variance}-\ref{con:x_bound} hold and
Algorithm~\ref{alg:abess} successfully select the true model, i.e., $\hat{\mathcal{A}} = \mathcal{A}^*$
(by Theorem \ref{thm:consistency4}, this event is true with a large probability for sufficiently large sample size $n$),
then the computational complexity of Algorithm~\ref{alg:abess} for a given $s_{\max}\geq s^*$ is
\begin{align*}
O\Big(\big(s^2_{\max}\log_{\frac{1}{\gamma_{s_{\max}}}} \left[\frac{l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)}{s^*\log p\log\log n}\right] + \frac{l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)}{\log p\log\log n}\big)\big( k_{\max}ns_{\max} + k_{\max}p \big)\Big).
\end{align*}
% Here $\gamma_s$ is defined in Assumptions \ref{con:technical-assumption}.
\end{theorem}
% Note that by Lemma \ref{lemma:consistency1} the uncertainty $1- \delta_{1}-\delta_{2}-\delta_{3}$ is introduced to exclude the event under which Algorithm \ref{alg:fbess} will fail, while an extra term $O(p^{1-K\log\log n})$ is inevitable for controlling the computational complexity.
From Theorem~\ref{thm:complexity}, the proposed Algorithm~\ref{alg:abess} achieves a polynomial computational complexity with high probability---
roughly speaking, the complexity is controlled by
the sample size $n$, the dimensionality $p$, the square of sparsity level $s$,
and logarithmic terms of them.
As such, Algorithm~\ref{alg:fbess} has competitive computational properties for other state-of-the-art variable selection methods such as the LASSO.
The numerical results presented in Figure~\ref{fig:simu_runtime} certify the competitive computational advantage.
% The results is formally established in Theorem~\ref{thm:complexity}.
% \begin{theorem}\label{thm:complexity}
% Suppose assumptions \ref{con:bound-variance}-\ref{con:x_bound} hold and
% Algorithm~\ref{alg:abess} successfully select the true model, i.e., $\hat{\mathcal{A}} = \mathcal{A}^*$
% (by Theorem \ref{thm:consistency4}, this event is true with a large probability for sufficiently large sample size $n$),
% then the computational complexity of Algorithm~\ref{alg:abess} for a given $s_{\max}\geq s^*$ is
% \begin{align*}
% O\Big(\big(s^2_{\max}\log_{\frac{1}{\gamma_{s_{\max}}}} \left[\frac{l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)}{s^*\log p\log\log n}\right] + \frac{l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)}{\log p\log\log n}\big)\big( k_{\max}ns_{\max} + k_{\max}p \big)\Big).
% \end{align*}
% % Here $\gamma_s$ is defined in Assumptions \ref{con:technical-assumption}.
% \end{theorem}

Finally, we present a theorem demonstrating the gap between estimating the coefficients at the $q$-th iteration and the true coefficients.
\begin{theorem}\label{thm_error_bound}
Under Assumptions~\ref{con:bound-variance}-\ref{con:loss-reduce-threshold},
if $| \mathcal{A}^* | \leq s$, the $\ell_2$-error of parameter estimation is upper bounded by
% \begin{align*}
% \|\boldsymbol \beta^q-\boldsymbol \beta^{*}\|_{2} & \leq \frac{\left[l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)\right]\left(\frac{\nu_s}{m_sc_0^2}+1\right)}{(1-\Delta)C_{2}n}\gamma_{s}^q + \frac{\sqrt{\frac{s\log p}{n}}}{m_s c_0},
% \end{align*}
\begin{align*}
\|\boldsymbol \beta^q-\boldsymbol \beta^{*}\|_{2} & \leq \frac{\left[l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)\right]\left(\frac{\nu_s}{m_sc_0^2}+1\right)}{(1-\Delta)C_{2}n}\gamma_{s}^q + \frac{t}{m_s c_0},
\end{align*}
with probability at least $1- \delta_{1}-\delta_{2}-\delta_{3} - \delta_{4}(t)$, where
$\delta_{4}(t) \coloneqq O\left(\exp\left\{\log p - K_4\frac{nt^2}{s}\right\}\right)$ for any $t>0$. % =O\left(\min\limits_{j\in \mathcal{A}^*} |\boldsymbol{\beta}_j^*|\right)
\end{theorem}
Theorem~\ref{thm_error_bound} depicts our estimator's $\ell_2$ error bound.
\citet{li2017quadratic} derive an similar parameter estimation error bound in $\ell_2$ norm with order $O(\sqrt{\frac{s\log p}{n}})$.
In fact, for any constant $\delta_4>0$, there exists a constant $K(\delta_4)>0$ (depending only on $\delta_4$) such that if we set $t = K(\delta_4)\sqrt{\frac{n}{s\log p}}$,
then with probability at least $1-\delta_{1}-\delta_{2}-\delta_{3} - \delta_{4}$, our $\ell_2$ error bound attains the same order as that in \citet{li2017quadratic}
when $q\geq \log_{\frac{1}{\gamma_s}} \Big( \sqrt{\frac{n}{s\log p}} \frac{\left[l_n(\boldsymbol 0) + l_n(\boldsymbol\beta^*)\right]\left(\frac{\nu_s}{m_sc_0^2}+1\right)}{(1-\Delta)C_{2}n} \Big)$.

% {\color{red}(Jin Zhu)We begin with a study on the computational complexity of our algorithm.
% Experiments are carried out for different dimensions, ranging from 50 to 250, with step size 10.
% Sample size is fixed at $500$. Runtime is recorded and presented in Figure \ref{fig:simu_runtime_low},
% {\color{red}(Jin Zhu)We begin with a study on the computational complexity of our algorithm.
% Experiments are carried out for different dimensions, ranging from 50 to 250 with step size 10.
% Sample size is fixed at $500$. Runtime is recorded and presented in Figure \ref{fig:simu_runtime_low},
% which shows runtime of ABESS algorithm grows with a polynomial rate as the dimension increases.

% This part is devoted to verify the consistency of best-subset recovery,
% that is, for sufficiently large $n$, ABESS will select the true support with high probability.
% We conduct simulation for increasingly large sample sizes for demonstration.
% Specifically, n increases from 100 to 10000 with equal step size in the log scale, true sparsity level is fixed as 8.
% Simulation results are given in Figure~\ref{fig:simu_sparse_recovery}.
% Under different dimensions and correlation coefficients,
% ABESS algorithm finally discover the true support exactly as the sample size increases,
% which coincides with our theoretical results in section \ref{sec:theory}.
% }

% \begin{figure}[]
% \centering
% \begin{subfigure}[b]{\textwidth}
% \centering
% \includegraphics[width=\textwidth]{runtime_low_dim_binomial.pdf}
% \caption{$Binomial$}
% \end{subfigure}
% \begin{subfigure}[b]{\textwidth}
% \centering
% \includegraphics[width=\textwidth]{runtime_low_dim_poisson.pdf}
% \caption{$Poisson$}
% \end{subfigure}
% \caption{Runtime}
% \label{fig: simu_runtime_low}
% \end{figure}


\subsection{A Sketch of the Proofs} \label{sec:sketch}

Here we give a brief high-level description of proofs of the lemma and the four main theorems, pointing out their main ideas and essential ingredients and where the assumptions are used.

To prove Lemma \ref{lemma:consistency1}, we show by contradiction that with a high probability, any support which fails to cover the true support will not be the output of Algorithm \ref{alg:fbess} --- there exists a splicing iteration that can sufficiently decrease the loss function and thus continue the splicing procedure.
Specifically,
let $\mathcal{I}_{1}:=\hat{\mathcal{I}}\cap\mathcal{A}^{*}$ be
the indices of relevant variables that are excluded from the estimated support,
we suppose Algorithm \ref{alg:fbess} fails in support recovery, i.e., $\mathcal{I}_{1}\neq\varnothing$.
On one hand, using Assumptions \ref{con:bound-variance} and \ref{con:src},
the loss under $\hat{\boldsymbol \beta}$ output by Algorithm \ref{alg:fbess} satisfies:
\begin{align}\label{eq:sketch_1}
l_n(\hat{\boldsymbol \beta}) - l_n(\boldsymbol \beta^*) \geq nC_2 \|\boldsymbol \beta^*_{\mathcal{I}_1} \|_2^2 - \varepsilon(\hat{\mathcal{A}}),
\end{align}
where $C_2$ is defined in Assumption~\ref{con:technical-assumption} and
$\varepsilon(\cdot)$ represents error terms converge in probability to $0$ with a convergence rate specified by the sub-Gaussian assumption \ref{con:subgaussian}.
On the other hand, with a well-chosen splicing size $k_0$, we denote the active set obtained after the splicing iteration by $\tilde{\mathcal{A}} = (\hat{\mathcal{A}}\setminus S_{k_0,1}) \cup S_{k_0,2}$.
By the rules of the splicing procedure and again using Assumptions~\ref{con:bound-variance} and \ref{con:src}, we can obtain that
\begin{align}\label{eq:sketch_2}
l_n(\tilde{\boldsymbol \beta}) - l_n(\boldsymbol \beta^*) \leq nC_1 \|\boldsymbol \beta^*_{\mathcal{I}_1} \|_2^2 + \varepsilon(\tilde{\mathcal{A}}),
\end{align}
where $\tilde{\boldsymbol \beta}$ is the minimizer of $l_n(\boldsymbol{\beta})$ under the constraint $\boldsymbol{\beta}_{\tilde{\mathcal{A}}^c} = \mathbf{0}$ and $C_1$ is defined in Assumption~\ref{con:technical-assumption}.
Intuitively, \eqref{eq:sketch_1} and \eqref{eq:sketch_2} indicate that $l_n(\hat{\boldsymbol \beta}) - l_n(\boldsymbol \beta^*)$ and $l_n(\tilde{\boldsymbol \beta}) - l_n(\boldsymbol \beta^*)$ can be roughly approximated by a term proportional to $\|\boldsymbol \beta^*_{\mathcal{I}_1}\|_{2}$ ---
the magnitude of true coefficients that are excluded from the estimated ones.
Combining \eqref{eq:sketch_1}, \eqref{eq:sketch_2} and Assumptions~\ref{con:technical-assumption}-\ref{con:loss-reduce-threshold}, we derive that, with high probability, there will be a contradiction, which completes the proof of Lemma~\ref{lemma:consistency1}.

When $s < s^*$, it can be shown that the difference $l_n(\hat{\boldsymbol \beta}^s) - l_n(\hat{\boldsymbol \beta}^*)$ dominates the penalty term of GIC, i.e. $F(\hat{\boldsymbol \beta}^s) > F(\hat{\boldsymbol \beta}^*)$, where $\hat{\boldsymbol \beta}^s$ is the output of Algorithm 1 given support size $s$. On the other hand, when $s>s^*$, the penalty term of GIC plays the leading role, i.e., $F(\hat{\boldsymbol \beta}^s) > F(\hat{\boldsymbol \beta}^*)$. Combining these two aspects, the GIC attains its minimum at the support size $s^*$.

The proofs of Theorems~\ref{thm:num_of_iter} and \ref{thm_error_bound} rely heavily on intermediate results in the proof of Lemma \ref{lemma:consistency1}. A crucial inequality on which we spend much effort establishing is
\begin{align}\label{eq:sketch_3}
l_n(\boldsymbol \beta^{q})-l_n(\boldsymbol \beta^*) & \leq \gamma_{s}\Big[l_n(\boldsymbol \beta^{q-1})-l_n(\boldsymbol \beta^*)\Big] + R_1,
\end{align}
where $\gamma_s<1$ is a constant defined in Assumption \ref{con:technical-assumption} and $R_1$ is some small remainder term to be controlled. Applying \eqref{eq:sketch_3} recurrently and using
\begin{align}\label{eq:sketch_4}
l_n({\boldsymbol \beta}^{q}) - l_n(\boldsymbol \beta^*) \geq nC_2 \|\boldsymbol \beta^*_{\mathcal{I}_1} \|_2^2 - \varepsilon({\mathcal{A}^{q}})
\end{align}
which is essentially identical to~\eqref{eq:sketch_1}, we can obtain the bound of the number of iterations described in Theorem \ref{thm:num_of_iter}. From Theorem~\ref{thm:num_of_iter}, the loss function decreases drastically as iterations when $s \geq s^*$. When $s<s^*$, the iterations of Algorithm~\ref{alg:abess} can be determined by thresholding to exclude useless splicing. Thus, the computational complexity of Algorithm~\ref{alg:abess} is presented in Theorem~\ref{thm:complexity}.
Finally, using the above inequalities involving $l_n({\boldsymbol \beta})$ for certain $\boldsymbol \beta$ and Assumption~\ref{con:src} that characterizes the curvature of $l_n$, we can naturally derive the upper bound for $\|\boldsymbol \beta^q-\boldsymbol \beta^{*}\|_{2}$ stated in Theorem~\ref{thm_error_bound}.

%As a complementary discussion of Assumption~\ref{con:loss-reduce-threshold},
%we note that only $\tau_s = O \left({s\log p\log\log n}\right)$ is required to establish Lemma~\ref{lemma:consistency1},
%as it avoids the threshold $\tau_s$ being too large to allow necessary splicing iterations.
%One may simply set $\tau_s = 0$ and Algorithm~\ref{alg:fbess} still converges.
%However, to prove its polynomial computational complexity, $\tau_s = \Omega \left({s\log p\log\log n}\right)$ is required to:
%(i) reduce useless iterations when the model is underestimated, and
%(ii) avoid any unnecessary splicing iteration via terminating the algorithm once the true support has been covered,
%as we can see in the proof of Corollary~\ref{thm:complexity}.
