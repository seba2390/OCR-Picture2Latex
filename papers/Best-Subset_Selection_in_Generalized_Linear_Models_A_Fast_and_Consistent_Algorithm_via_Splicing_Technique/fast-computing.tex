\section{Efficient Implementation: Details}\label{sec:efficient-implementation}
We implement the proposed algorithm in a highly efficient \textsf{abess} library
with both Python and R interfaces.
As can be seen from the numerical experiments below,
\textsf{abess} can have competitive or even less running times
compared with other well-known sparse learning software like \textsf{glmnet}.
\textsf{abess} achieves extreme efficiency by leveraging efficient implementation to reduce the time consumption on computing forward and backward sacrifices.
% These include an adaptive grid of tuning parameters, continuation, active set updates, greedy cyclic ordering of
% coordinates, correlation screening, and a careful accounting of floating point operationsâ€”some of these
% heuristics (as specified below) appear in prior work
% for deriving highly efficient algorithms for the Lasso (e.g., glmnet).
In the following, we provide a detailed description of the efficient implementation.

%\subsection{Best-Subset Selection Path: Warm-Start Initialization}
%In this part, we describe an efficient procedure to compute
%along a best-subset selection path, i.e., computing the solution of Algorithm~\ref{alg:fbess}
%on a series of support size: $1 < 2 < \cdots < s_{\max}$.
%The procedure is summarized in Algorithm~\ref{alg:abess}.
%% Inspired by the work of \citet{friedman2010regularization},
%% we use the $s_i$ solution as a warm start for $s_{i+1}$.
%Algorithm~\ref{alg:abess} runs Algorithm~\ref{alg:fbess} on various support sizes ranging from small to large.
%% Fortunately, it allows using the output of Algorithm~\ref{alg:fbess} to build a better active set,
%% allowing Algorithm~\ref{alg:fbess} to reach convergence with less splicing iteration.
%Notice that, in Step 4 of Algorithm~\ref{alg:abess},
%we set the initial support set of the next support size as the union of
%the selected subset returned by Algorithm~\ref{alg:fbess} and the variable with the largest forward sacrifice,
%which refers to ``warm-starts initialization''.
%Both existing numerical experiments and theoretical analysis suggest that
%such an initialization is remarkably efficient \citep{friedman2010regularization, barutConditionalSureIndependence2016}.
%From our experience, by setting the initialization as the input of Algorithm~\ref{alg:abess},
%we can reduce the number of iterations in the \textbf{repeat-until} loop in Algorithm~\ref{alg:fbess},
%whittling down the overall runtime.
% \footnote[1]{The adaptivity refers to adapting the warm-start initialization.}
% Algorithm~\ref{alg:abess} describes the procedure mentioned above in detail.
%\begin{algorithm}[htbp]
% \caption{P\underline{a}thwise \underline{Be}st-\underline{S}ubset \underline{S}election for GLM (\textbf{ABESS})}
% \label{alg:abess}
% \begin{algorithmic}[1]
% \REQUIRE A dataset set $\{(\boldsymbol{x}_i, y_i)\}^{n}_{i=1}$ and the maximum support size $s_{\max}$.
% \STATE $\mathcal{A}^0 \leftarrow \{ \arg\max\limits_j | (\frac{\partial l_n( \boldsymbol \beta )}{\partial \boldsymbol \beta} |_{\boldsymbol \beta = {\boldsymbol 0} })_j | \}$
% \FOR {$s = 1, \ldots, s_{\max}$}
% \STATE $(\hat{\boldsymbol \beta}_{s}, \hat{\mathcal{A}}_{s}, \hat{\boldsymbol{d}}_s) \leftarrow \textbf{BESS-GLM}\left( \{(\boldsymbol{x}_i, y_i)\}^{n}_{i=1}, \mathcal{A}^0, k_{\max}, \tau_s \right)$.
% \STATE $\mathcal{A}^0 \leftarrow \hat{\mathcal{A}}_{s} \cup \{ \arg\max\limits_j|(\hat{\boldsymbol{d}}_s)_j| \}$.
% \ENDFOR
% \ENSURE $\{ (\hat{\boldsymbol \beta}_{s_i}, \hat{\mathcal{A}}_{s_i} ) \}_{i=1}^{\max}$.
% \end{algorithmic}
%\end{algorithm}

\subsection{Simplified Convex Optimization and Early Stopping}\label{sec:approximiated-newton-update}

In Algorithm~\ref{alg:fbess}, we need to solve convex optimization problems:
$\tilde{\boldsymbol \beta} \leftarrow \arg\min\limits_{\beta_{\mathcal{I}} = 0} l_n(\boldsymbol\beta ).$
Directly solving it via a convex optimization solver would consume a large time.
To leverage the sparsity nature of this problem, we can turn to solve a simplified problem:
$$\tilde{\boldsymbol \beta}_{\mathcal{A}} \leftarrow \arg\min\limits_{\boldsymbol{\beta}_{\mathcal{A}}}- \sum_{i=1}^n\{y_i \boldsymbol{\beta}_{\mathcal{A}}^\top (\boldsymbol{x}_i)_{\mathcal{A}} - b(\boldsymbol{\beta}_{\mathcal{A}}^\top (\boldsymbol{x}_i)_{\mathcal{A}}) + c(y_i,\phi)\} $$
and pad zero entries to obtain $\tilde{\boldsymbol \beta}$.
The simplified problem solves the regression coefficient on a much smaller dataset, and thus it is computationally appealing.
Since the simplified problem has no closed-form solution, we must perform iterative algorithms to solve it.
The Newton method is one of the most popular methods to solve this problem.
More precisely, we conduct Newton's updates
% \begin{equation}\label{eqn:primary_fit_update}
% \begin{split}
% {\boldsymbol \beta}_{\mathcal{A}}^{m+1} \leftarrow \boldsymbol \beta_{\mathcal{A}}^m - \Big( \left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \boldsymbol \beta_{{\mathcal{A}}} )^2 }\right|_{\boldsymbol \beta = \boldsymbol \beta^m} \Big)^{-1} \Big( \left.\frac{\partial l_n( \boldsymbol \beta )}{ \partial \boldsymbol \beta_{{\mathcal{A}}} }\right|_{\boldsymbol \beta = \boldsymbol \beta^m} \Big),
% \end{split}
% \end{equation}
until $\| {\boldsymbol \beta}_{\mathcal{A}}^{m+1} - {\boldsymbol \beta}_{\mathcal{A}}^{m}\|_2 \leq \epsilon$ or $m > k$,
where $\epsilon$ is convergence tolerance and $k$ is the maximal number of Newton's updates.
Generally, setting $\epsilon = 10^{-6}$ and $k = 80$ returns a desirable coefficient estimation.
% In general, the inverse of the second derivative in \eqref{eqn:primary_fit_update} is computationally expensive, so we use its diagonalized version to approximate it. The update then makes the following changes:
% \begin{equation}\label{eqn:fast_primary_fit_update}
% \begin{split}
% {\boldsymbol \beta}_{\tilde{\mathcal{A}} }^{m+1} \leftarrow \boldsymbol \beta_{\tilde{\mathcal{A}} }^m - \rho D \Big( \left.\frac{\partial l_n( \boldsymbol \beta )}{ \partial \boldsymbol \beta_{\tilde{\mathcal{A}}} }\right|_{\boldsymbol \beta = \boldsymbol \beta^m} \Big),
% \end{split}
% \end{equation}
% where $D = \textup{diag}( (\left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \boldsymbol \beta_{\tilde{\mathcal{A}_{1}}} )^2 }\right|_{\boldsymbol \beta = \boldsymbol \beta^m} )^{-1}, \ldots, (\left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \boldsymbol \beta_{\tilde{\mathcal{A}}_{|A|}} )^2 }\right|_{\boldsymbol \beta = \boldsymbol \beta^m} )^{-1})$
% and $\rho$ is step size.
% While using the approximation increases iteration time, it avoids a high level of computational complexity when computing the matrix inversion.

Notice that not all of the candidate active sets considered in Algorithm~\ref{alg:fbess} associate with a better solution such that $L - l_{n}(\tilde{\boldsymbol{\beta}}) > \tau_s$,
This inspires us to stop Newton's updates on these active candidates early sets to reduce unnecessary Newton's updates.
More precise, after each Newton's update, we check whether the heuristic criterion
$l_1 - (k - m) \times (l_2 - l_1) > L - \tau_s$ holds,
where $l_1 = l_n({\boldsymbol\beta}_{\mathcal{A}}^{m}), l_2 = l_n({\boldsymbol\beta}_{\mathcal{A}}^{m+1})$.
If so, it implies the remaining $k - m$ times Newton update can potentially lead to a lower loss, and thus,
it has no reason to stop Newton's updates; otherwise, we can stop Newton's updates.
The heuristic criterion is designed based on the fact that the convergence rate of Newton's iterative method is quadratic \citep{nocedalNumericalOptimization1999}.
It considerably improves the efficiency of computing forward sacrifices and
retain the high quality of coefficient estimations.

\subsection{Importance-Priority Splicing}\label{sec:importance-priority-splicing}
Considerable speedup is achieved in a high-dimensional regime by employing an importance-priority splicing procedure,
which is motivated by an active set update for the Lasso \citep{lassoscreening2012tibshirani}.
Specifically, a heavy computational burden comes up when
computing forward sacrifices for $p - s$ variables in the inactive set, especially since $p$ is very large.
To alleviate this burden, after completing forward and backward sacrifices through all the variables, we perform the splicing iteration on a small but essential subset: the union of
the selected variables and $d$ ($\ll p$) variables with the more significant forward sacrifices.
Once the splicing iteration converges, we re-compute sacrifices for all the variables and update the important subset.
If the updated important subset is the same as the previous one,
Algorithm~\ref{alg:fbess} is done. Otherwise, the above procedure is repeated.
From our numerical experience, the procedure mentioned above can save lots of time;
meantime, it keeps Algorithm~\ref{alg:fbess} enjoying the statistical properties
presented in Section~\ref{sec:theorical-properties}.
