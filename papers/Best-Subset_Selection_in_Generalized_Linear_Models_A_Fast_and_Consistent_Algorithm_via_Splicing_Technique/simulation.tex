\section{Simulation Studies}\label{sec:simulation}
In this section, we are mainly interested in the empirical performance of the ABESS algorithm on logistic regression and Poisson regression.
Logistic regression is widely used for classification tasks, and Poisson regression is appropriate when the response is a count.
\if0\informsMOR{In ``Additional Simulation'' of Supplementary Material, we }\else{We }\fi
also consider the performance of ABESS algorithm on multi-response linear regression (a.k.a., multi-task learning).
Before formally analyzing the simulation results,
we illustrate our simulation settings in Section~\ref{subsec:setup}.
% This subsection develops parallel with Section \ref{subsec:logistic}.

%In this section, we study the empirical performance of ABESS for GLM on two generalized linear models,
%logistic regression and gamma regression,
%where logistic regression is widely used for classification and
%gamma regression model is useful for modeling positive continuous response variables.
%Before formally studying logistic regression and gamma regression in Section~\ref{subsec:logistic} and Section~\ref{subsec:gamma}, respectively, we illustrate our simulation setting in Section~\ref{subsec:setup}.

\subsection{Setup}\label{subsec:setup}
To synthesize a dataset, we generate multivariate Gaussian realizations $\boldsymbol{x}_1, \ldots, \boldsymbol{x}_n \overset{i.i.d.}{\sim} \mathcal{MVN}(0,\Sigma)$,
where $\Sigma$ is a $p$-by-$p$ covariance matrix.
%We generate i.i.d error $\epsilon\sim N(0,\sigma^2)$.
%Define the signal to noise ratio (SNR) by $SNR = \frac{\beta^{\top}\Sigma\beta}{\sigma^2}$.
We consider two covariance structures for $\Sigma$: the independent structure ($\Sigma$ is an identity matrix)
and the constant structure ($\Sigma_{ij} = \rho^{I(i\neq j)}$ for some positive constant $\rho$). The value of $\rho$ and $p$ will be specified later.
We set the true regression coefficient $\boldsymbol{\beta}^*$ as a sparse vector with $k$ non-zero entries that have equi-spaced indices in $\{1, \ldots, p\}$.
Finally, given a design matrix $\mathbf{X} = (\boldsymbol{x}_1, \ldots, \boldsymbol{x}_n)^\top$ and $\boldsymbol{\beta}^*$,
we draw response realizations $\{y_i\}_{i=1}^n$ according to the GLMs.

We assess our proposal via the following criteria.
First, to measure the performance of subset selection,
we consider the probabilities of covering true active and inactive sets: $\mathbb{P}(\mathcal{A}^* \subseteq \hat{\mathcal{A}})$ and
$\mathbb{P}(\mathcal{I}^* \subseteq \hat{\mathcal{I}})$ (here, $\mathcal{I}^* = (\mathcal{A}^*)^c$).
We also consider exact support recover probability as $\mathbb{P}(\mathcal{A}^* = \hat{\mathcal{A}})$.
Since the probability is unknown, we empirically examine the proportion of recovery for the active set, inactive set, and exact recovery in 200 replications for instead.
As for parameter estimation performance, we examine relative error (ReErr) on parameter estimations:
$\|\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}^*\|_{2} /\|\boldsymbol{\beta}^*\|_{2}$.
Finally, computational efficiency is directly measured by the runtime.

In addition to our proposed algorithms, we compare classical variable selection methods: LASSO \citep{tibshirani1996regression}, SCAD \citep{fan2001variable}, and MCP \citep{zhang2010nearly}.
%, and a recently proposed coordinate descent (CD) method for $\ell_0$-regularized classification \citep{antoine2021l0learn}.
For all these methods, we apply 10-fold cross-validation (CV) and the GIC to select the tuning parameter, respectively.
% For all these methods, we apply 10-fold cross-validation (CV) to select the tuning parameter.
% ABESS also uses generalized information criterion (GIC) \citep{fan2013tuning} because,
% by combining GIC, ABESS can consistently recover $\mathcal{A}^*$ under linear models \citep{zhu2020polynomial}.
The software for these methods is available at R CRAN (\url{https://cran.r-project.org}).
The software of all methods is summarized in Table~\ref{tab:implementation-details}.
All experiments are carried out on an R environment in a Linux platform with Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz. 
% Note that, all experiments result are based on 200 random synthetic datasets.
%Ubuntu platform with Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz.

% Model selection methods such as cross-validation and information criteria are widely used.
% Recently, \citet{fan2013tuning} explored generalized information criterion (GIC) in tuning parameter selection for
% penalized likelihood methods under GLM.
% Here, we use a GIC-type information criterion to recovery support size, which is defined as:
% $\mathrm{F}(\hat{\boldsymbol \beta}) = l_n( \hat{\boldsymbol \beta} ) + |\text{supp}(\hat{\boldsymbol \beta})| \log(p) \log\log n.$
% Intuitively speaking, the model complexity penalty term $|\text{supp}(\hat{\boldsymbol \beta})| \log p \log\log n$ is set to prevent over-fitting,
% where the term $\log\log n$ with a slow diverging rate is used to prevent under-fitting.
% Combining the Algorithm~\ref{alg:fbess} with GIC, we select the support size that minimizes the $F(\hat{\boldsymbol{\beta}})$.}

% \begin{table}[htbp]
% \caption{Implementation details for all methods.
% The values in the parentheses indicate the version number of R packages.}\label{tab:implementation-details}
% \centering
% \begin{tabular}{ccc}
% \toprule
% Method & Software & Tuning method \\
% \midrule
% ABESS-GIC & abess (0.4.0) & GIC \\
% LASSO-GIC & glmnet (4.1-3) & GIC \\
% SCAD-GIC & ncvreg (3.13.0) & GIC \\
% MCP-GIC & ncvreg (3.13.0) & GIC \\
% CD-GIC & L0Learn (2.0.3) & GIC \\
% ABESS-CV & abess (0.4.0) & 10-folds CV \\
% LASSO-CV & glmnet (4.1-3) & 10-folds CV \\
% SCAD-CV & ncvreg (3.13.0) & 10-folds CV \\
% MCP-CV & ncvreg (3.13.0) & 10-folds CV \\
% CD-CV & L0Learn (2.0.3) & 10-folds CV \\
% \bottomrule
% \end{tabular}
% \end{table}
\begin{table}[htbp]
\caption{Software for all methods.
The values in the parentheses indicate the version number of R packages.The tuning parameter within the MCP/SCAD penalty is fixed at 3/3.7.}\label{tab:implementation-details}
\centering
\if0\informsMOR{
% \begin{tabular}{ccccccc}
% \toprule
% Method & ABESS & LASSO & SCAD & MCP & CD \\
% \midrule
% Software & \textsf{abess} (0.4.0) & \textsf{glmnet} (4.1-3) & \textsf{ncvreg} (3.13.0) & \textsf{ncvreg} (3.13.0) & \textsf{L0Learn} (2.0.3) \\
% Tuning & sparsity $s$ & $\ell_1$ penalty & $\lambda$ & $\lambda$& $\lambda$ \\
% \bottomrule
% \end{tabular}
\begin{tabular}{cccccc}
    \toprule
    Method & ABESS & LASSO & SCAD & MCP \\
    \midrule
    Software & \textsf{abess} (0.4.0) & \textsf{glmnet} (4.1-3) & \textsf{ncvreg} (3.13.0) & \textsf{ncvreg} (3.13.0) \\
    Tuning & sparsity $s$ & $\ell_1$ penalty & $\lambda$ & $\lambda$ \\
    \bottomrule
    \end{tabular}
}\else{
% \begin{tabular}{ccccccc}
% \hline
% Method & ABESS & LASSO & SCAD & MCP & CD \\
% \hline
% Software & \textsf{abess} (0.4.0) & \textsf{glmnet} (4.1-3) & \textsf{ncvreg} (3.13.0) & \textsf{ncvreg} (3.13.0) & \textsf{L0Learn} (2.0.3) \\
% Tuning & sparsity $s$ & $\ell_1$ penalty & {\color{red}SCAD penalty} & {\color{red}MCP penalty} & $\ell_0$ penalty \\
% \hline
% \end{tabular}
\begin{tabular}{cccccc}
\hline
Method & ABESS & LASSO & SCAD & MCP \\
\hline
Software & \textsf{abess} (0.4.0) & \textsf{glmnet} (4.1-3) & \textsf{ncvreg} (3.13.0) & \textsf{ncvreg} (3.13.0) \\
Tuning & sparsity $s$ & $\ell_1$ penalty & {\color{red}SCAD penalty} & {\color{red}MCP penalty} \\
\hline
\end{tabular}
}\fi
\end{table}
% We implement our proposal in an R package abess \citep{zhu-abess-arxiv}.

\subsection{Logistic Regression}\label{subsec:logistic}
% In this subsection, we illustrate the power of ABESS on logistic regression, which is one of the most popular GLMs widely used for classification tasks.
% In terms of logistic regression, the response $y_i$ is a binary variable following a Bernoulli distribution $B(1, p_i)$,
% where $p_i \coloneqq \mathbb{P}(y_i=1)$ is determined by $\log(\frac{p_i}{1-p_i}) = \boldsymbol x_i^\top \boldsymbol{\beta }$.
% Here, the link function is known as the logit function, defined by $logit(p) = \log(\frac{p}{1-p})$.
% As a result, the negative log-likelihood is given by
% \begin{equation*}
% l_n(\boldsymbol\beta) = -\sum_{i=1}^{N}\left\{y_{i} \boldsymbol {x}_i^\top \boldsymbol \beta-\log \left(1+e^{\boldsymbol {x}_i^\top \boldsymbol \beta}\right)\right\}.
% \end{equation*}
% Empirically, we generate $x_i$ and $\beta$ as described in Section \ref{subsec:setup}.
% Binary response $y_i$ is then drawn from the Bernoulli distribution according to (\ref{eqn:formula_binomial}).
% Let $H_j = \sum\limits_{i=1}^{n} \frac{e^{\boldsymbol {x}_i^\top \hat{\boldsymbol \beta}}}{(1 + e^{\boldsymbol {x}_i^\top \hat{\boldsymbol \beta}})^2} x_{ij}^2$ and
% the gradient of $l_n(\boldsymbol{\beta})$ at $\hat{\boldsymbol{\beta}}$ be $\hat{\boldsymbol d} = -\sum\limits_{i=1}^{n}(y_i - \frac{e^{\boldsymbol {x}_i^\top \hat{\boldsymbol \beta}}}{1 + e^{\boldsymbol {x}_i^\top \hat{\boldsymbol \beta}}}) \boldsymbol {x}_i$,
% \eqref{eqn:approx_sacrifice} can be explicit expressed as:
% $\xi_j = H_j (\hat{\boldsymbol{\beta}}_j)^2$ for $j\in \mathcal{A}$ and
% $\zeta_j = H_j^{-1}( \hat{\boldsymbol d}_j )^2$ for $j\in \mathcal{I}$.
% \begin{equation*}
% \begin{aligned}
% % \hat{\boldsymbol d} &= -\sum_{i=1}^{n}(y_i - \frac{e^{\boldsymbol {x}_i^\top \boldsymbol \beta}}{1 + e^{\boldsymbol {x}_i^\top \boldsymbol \beta}}) \boldsymbol {x}_i. \\
% \xi_j
% & = H_j (\hat{\boldsymbol{\beta}}_j)^2, j\in \mathcal{A},\\
% \zeta_j
% & = H_j^{-1}
% ( \hat{\boldsymbol d}_j )^2, j\in \mathcal{I}.
% \end{aligned}
% \end{equation*}
% Given the explicit expression of \eqref{eqn:approx_sacrifice},
% we can conduct Algorithm~\ref{alg:abess} to estimate $\boldsymbol{\beta}$.

The dimension $p$ is fixed as 500 for the logistic regression model. For the constant correlation case, we set $\rho = 0.4$.
The non-zero coefficients $\boldsymbol{\beta}^*_{\mathcal{A}^*}$ are set to be $(2,2,8,8,8,8,10,10,10,10)^\top$. 
Now we compare methods listed in Table~\ref{tab:implementation-details}.
Figures~\ref{fig:rate_binomial} and \ref{fig:ReErr_binomial} present the results on subset selection and parameter estimation when the sample size increases. Out of clarity, we omit the CV results here and defer these results to the Additional Figures in Supplementary Material.


\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figure/rate_binomial_gic.pdf}
\if0\informsMOR{
\vspace{-30pt}
}\fi
\caption{Performance on subset selection under logistic regression when covariates have independent correlation structure (Upper) and constant correlation structure (Lower), measured by three kinds probabilities: $\mathbb{P}(\mathcal{A}^* \subseteq \hat{\mathcal{A}})$, $\mathbb{P}(\mathcal{I}^* \subseteq \hat{\mathcal{I}})$, and $\mathbb{P}(\mathcal{A}^* = \hat{\mathcal{A}})$ that are presented in Left, Middle and Right panels, respectively.
}
\label{fig:rate_binomial}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figure/ReErr_binomial_gic.pdf}
\if0\informsMOR{
\vspace{-10pt}
}\fi
\caption{Performance on parameter estimation under logistic regression models when covariance matrices have independent correlation structure (Left) and exponential correlation structure (Right). The $y$-axis is the median of ReErr in a log scale.}
\label{fig:ReErr_binomial}
\end{figure}

As depicted in the left panel of Figure~\ref{fig:rate_binomial}, the probability $\mathbb{P}(\mathcal{A}^* \subseteq \hat{\mathcal{A}})$ approaches 1 as the sample size increases, indicating that all methods, except LASSO in the high correlation setting, can provide a no-false-exclusion estimator when the sample size is sufficiently large. However, when considering $\mathbb{P}(\mathcal{I}^* \subseteq \hat{\mathcal{I}})$, as observed in the middle panel of Figure~\ref{fig:rate_binomial}, the LASSO estimator consistently exhibits false inclusions, and the SCAD/MCP estimator shows false inclusions when the covariates are highly correlated. In contrast, only ABESS guarantees that $\mathbb{P}(\mathcal{I}^* \subseteq \hat{\mathcal{I}})$ approaches 1 for large sample sizes. 

Furthermore, as evident from the right panel of Figure~\ref{fig:rate_binomial}, ABESS accurately recovers the true subset under both correlation settings. While SCAD and MCP can also achieve exact support recovery given a sufficient sample size, ABESS demonstrates support recovery consistency with the smallest sample size, particularly when variables are correlated. It is important to note that although our theory imposes restrictions on the correlation among a small subset of variables (see Assumption~\ref{con:technical-assumption}), our algorithm still performs effectively in the constant correlation setting. This setting (i.e., $\rho=0.4$) violates Assumption~\ref{con:technical-assumption} as the correlation between any two variables exceeds 0.183, which is the maximum acceptable pairwise correlation satisfying Assumption~\ref{con:technical-assumption}.

Moving on to Figure~\ref{fig:ReErr_binomial}, it illustrates the superiority of ABESS in parameter estimation. ABESS visibly outperforms other methods in the small sample size regime and maintains highly competitive performance as the sample size increases. This superiority in parameter estimation is not surprising, as ABESS yields an oracle estimator when the support set is correctly identified. Although SCAD and MCP do not provide algorithmic guarantees for finding the local minimum, they exhibit competitive parameter estimation performance due to their asymptotic unbiasedness. Conversely, the LASSO estimator is biased and performs the worst among all the methods.

%\begin{figure}
%	\centering
%	\includegraphics[width=\textwidth]{figure/Performance_binomial.pdf}
%	\caption{Performance comparison under two correlation structures: independent and exponential. (A) Performance for subset selection, measured by support recover probability. (B) Performance for parameter estimation, measured by median ReErr. (C) Average runtime, measured in seconds. L0Learn is omitted since its runtime is far longer than others.}
%	\label{fig:Performance_binomial}
%\end{figure}

\subsection{Poisson Regression}\label{seubsec:poisson}
% As regard to Poisson regression, the response $y_i$ is a integer variable following a Poisson distribution $\mathcal{P}(\lambda_i)$ where $\lambda_i = \exp(\boldsymbol x_i^\top \boldsymbol{\beta})$.
% As a result, the negative log-likelihood is given by
% \begin{equation*}
% l_n(\boldsymbol\beta) = -\sum_{i=1}^{N}\left\{y_{i} \boldsymbol {x}_i^\top \boldsymbol\beta - e^{\boldsymbol {x}_i^\top \boldsymbol \beta} -\log(y_i!)\right\}.
% \end{equation*}
% Empirically, we generate $x_i$ and $\beta$ as described in Section \ref{subsec:setup}.
% Binary response $y_i$ is then drawn from the Bernoulli distribution according to (\ref{eqn:formula_binomial}).
% Let $H_j = \sum\limits_{i=1}^{n} \exp(\boldsymbol {x}_i^\top \hat{\boldsymbol \beta}) x_{ij}^2$ and
% the gradient of $l_n(\boldsymbol{\beta})$ at $\hat{\boldsymbol{\beta}}$ be $\hat{\boldsymbol d} = -\sum\limits_{i=1}^{n}(y_i - \exp(\boldsymbol {x}_i^\top \hat{\boldsymbol \beta})) \boldsymbol {x}_i$,
% \eqref{eqn:approx_sacrifice} can be explicit expressed as:
% $\xi_j = H_j (\hat{\boldsymbol{\beta}}_j)^2$ for $j\in \mathcal{A}$ and
% $\zeta_j = H_j^{-1}( \hat{\boldsymbol d}_j )^2$ for $j\in \mathcal{I}$.
% Given the explicit expression of \eqref{eqn:approx_sacrifice},
% we can conduct Algorithm~\ref{alg:abess} to estimate $\boldsymbol{\beta}$.


For the Poisson regression model, we consider a fixed $p$ value of 500, and set $\rho = 0.2$ for the constant correlation case. The non-zero coefficients $\boldsymbol{\beta}^*_{\mathcal{A}^*}$ are specified as $(1, 1, 1)^\top$. Figures~\ref{fig:rate_poisson_gic}-\ref{fig:ReErr_poisson_gic} present the evaluation of subset selection and parameter estimation quality. Examining Figures~\ref{fig:rate_poisson_gic}, we observe that for ABESS/SCAD/MCP, the probabilities $\mathbb{P}(\mathcal{A}^* \subseteq \hat{\mathcal{A}})$, $\mathbb{P}(\mathcal{I}^* \subseteq \hat{\mathcal{I}})$, and $\mathbb{P}(\mathcal{A}^* = \hat{\mathcal{A}})$ gradually approach 1 as the sample size $n$ increases. In contrary, the LASSO, regardless of the highest inclusion probability for $\mathcal{A}^*$, still has a chance of including ineffective variables, especially when variables are correlated. Comparing ABESS, SCAD, and MCP, it is evident that ABESS achieves the highest exact selection probability, followed by SCAD and MCP. Similar to the results in logistic regression, ABESS achieves exact selection of the effective variables with the smallest sample size under the constant correlation structure.
Regarding the quality of parameter estimation, the ReErr of all methods reasonably decreases as the sample size $n$ increases. Again, ABESS exhibits the least estimation error in terms of the $\ell_2$-norm, which coincides with the results on logistic regression. It is worth noting that our method demonstrates consistency and polynomial complexity under Poisson regression, even though it violates the sub-Gaussian assumption. This is because the current framework of proofs allows for the relaxation of Assumption~\ref{con:subgaussian} to a sub-exponential distribution assumption, enabling the establishment of similar theoretical properties.

\begin{figure}[htbp]
\centering
\includegraphics[width=1.0\textwidth]{figure/rate_poisson_gic.pdf}
\if0\informsMOR{
\vspace{-30pt}
}\fi
\caption{Performance on subset selection under Poisson regression when covariates have independent correlation structure (Upper) and constant correlation structure (Lower), measured by three kinds probabilities: $\mathbb{P}(\mathcal{A}^* \subseteq \hat{\mathcal{A}})$, $\mathbb{P}(\mathcal{I}^* \subseteq \hat{\mathcal{I}})$, and $\mathbb{P}(\mathcal{A}^* = \hat{\mathcal{A}})$ that are presented in Left, Middle and Right panels, respectively.}
\label{fig:rate_poisson_gic}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figure/ReErr_poisson_gic.pdf}
\if0\informsMOR{
\vspace{-5pt}
}\fi
\caption{Performance on parameter estimation under Poisson regression models when covariance matrices have independent correlation structure (Left) and exponential correlation structure (Right). The $y$-axis is the median of ReErr in a log scale.}
\label{fig:ReErr_poisson_gic}
\end{figure}

\subsection{Computational analysis}

We compare the runtime of different methods in Table~\ref{tab:implementation-details} for the logistic regression and Poisson regression models in Sections~\ref{subsec:logistic} to \ref{seubsec:poisson}. The runtime results are summarized in Figure~\ref{fig:simu_runtime}, indicating that ABESS demonstrates superior computational efficiency compared to state-of-the-art variable selection methods. For instance, when $n = 3000$, ABESS is at least four times faster than its competitors in logistic regression under an independent correlation structure. Furthermore, regardless of logistic regression or Poisson regression, ABESS exhibits similar computational performance, while other competitors run much faster when the pairwise correlation is higher. Lastly, it is important to note that the runtime of ABESS scales polynomially with sample sizes, aligning with the complexity presented in Theorem~\ref{thm:complexity}.
%In contrast, the runtime of other methods grows more rapidly as the sample size increases
%and appears like a quadratic function of the sample size in the independent scenario.
%Increasing iteration numbers for convergence may lead to this result.
%Moreover, ABESS-GIC is faster than ABESS-CV, demonstrating the superiority of the proposed adaptive parameter tuning procedure.
% Finally, according to the computational comparison presented in Figure~\ref{fig runtime_poisson_gic}, the ABESS has the least runtime and is much faster than the MCP and SCAD when variables are independent.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figure/runtime_binomial_gic.pdf}
\includegraphics[width=0.8\textwidth]{figure/runtime_poisson_gic.pdf}
\if0\informsMOR{
\vspace{-10pt}
}\fi
\caption{Average runtime (measured in seconds) on logistic regression (Upper panel) and Poisson regression (Lower panel). The results on two types of covariances matrix $\Sigma$, the independent correlation structure and constant correlation structure, are presented in the left and right panels, respectively. The error bars represent two times the standard errors.
}
\label{fig:simu_runtime}
\end{figure}

%% For the MOR template, uncomment this line and comment on the code blocks

\if1\informsMOR
{
\input{../appendix_numerical}
}\fi
