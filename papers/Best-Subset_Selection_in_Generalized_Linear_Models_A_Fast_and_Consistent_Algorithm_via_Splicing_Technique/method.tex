\section{Algorithm}\label{sec:methodology}
In this section, we present our fast and consistent best-subset selection algorithm. Firstly, we introduce an algorithm for selecting the best subset when the support size is known. Subsequently, we address the scenario where the knowledge of the support size is unavailable and introduce a novel information criterion to determine the optimal support size.

\subsection{Splicing Method For GLM}
% This section derives an iteration algorithm for solving \eqref{eq:best-subset-glm} given a fixed integer $s$.
Imagine we begin with an arbitrary guess $\mathcal{A} \subseteq \{1, \ldots,p\}$ for the best subset with cardinality $|\mathcal{A}|=s$.
Let $\mathcal{I} = \mathcal{A}^c$, and we shall refer to $\mathcal{A}$ and $\mathcal{I}$ as the active set and inactive set, respectively.
% $$l_{\mathcal{A}} = \min_{\beta_{\mathcal{I}}=0} l_n(\beta).$$
The coefficient estimator under $\mathcal{A}$ is:
\begin{align*}\label{eqn:optimal_beta}
\hat{\bm \beta}=\arg\min_{\boldsymbol \beta_{\mathcal{I}}=0} l_n(\boldsymbol \beta).
\end{align*}
% From the definition of $\hat{\boldsymbol{\beta}}$,
% it can be directly known that
% $\hat{\boldsymbol{\beta}}_j \neq 0$ for $j \in \mathcal{A}$ and $\hat{\boldsymbol{\beta}}_j = 0$ for $j \in \mathcal{I}$.
As follows, define the gradient of $l_n({\boldsymbol \beta})$ at $\hat{\bm \beta}$ as:
\begin{align*}
\hat{\boldsymbol d}
= \left.\frac{\partial l_n( \boldsymbol \beta )}{\partial \boldsymbol \beta}\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta}}.
% = -\sum_{i=1}^{n} (y_i - \mathbb{E}[y_i | \mathbf{x}_i]) \mathbf{x}_i
% = \mathbf{X} \mathbf{w}.
\end{align*}
% where $\mathbf{w} = (y_1 - \hat{\mu}_1, \ldots, y_n - \hat{\mu}_n)^\top$.
% Similarly, we have $\hat{\boldsymbol{d}}\neq 0$ for $j \in \mathcal{I}$ and $\hat{\boldsymbol{d}}_j = 0$ for $j \in \mathcal{A}$.
% And thus, the support sets of $\hat{\boldsymbol{d}}$ and $\hat{\boldsymbol{\beta}}$ are complementary.
Following that, we will introduce two critical concepts for deriving our algorithm.
\begin{itemize}
\item Backward sacrifice: the magnitude of discarding the $j$-th variable in $\mathcal{A}$, i.e.,
\begin{equation}\label{eqn:Delta}
\xi^*_j = l_n\big(\hat{\boldsymbol \beta} |_{\mathcal{A} \setminus \{j\}} \big) - l_n(\hat{\boldsymbol \beta}).
\end{equation}
Intuitively, the $j$-th variable in $\mathcal{A}$ associated with a larger $\xi^*_j$ is
more relevant to the response.
\item Forward sacrifice: the magnitude of adding the $j$-th variable in $\mathcal{I}$ into $\mathcal{A}$, i.e.,
\begin{equation}\label{eqn:delta}
\zeta^*_j = l_n(\hat{\boldsymbol \beta}) - l_n(\hat{\boldsymbol \beta} + \hat{\boldsymbol t} |_{j}),
\end{equation}
where $\hat{\boldsymbol t} = \arg\min\limits_{\boldsymbol t} l_n(\hat{\boldsymbol \beta} + \boldsymbol{t} |_{j})$.
As with the backward sacrifice, a larger $\zeta^*_j$ indicates that the $j$-th variable in $\mathcal{I}$ is more critical for modeling the response.
\end{itemize}
However, it is worth noting that the two sacrifices are incomparable due to their association with different support sets.

The computation of forward sacrifices is time-consuming because an iterative algorithm is needed to minimize $l_n(\hat{\boldsymbol \beta} + {\boldsymbol t} |_{j})$ for all $j \in \mathcal{I}$.
We introduce the following approximations to mitigate this.
%\begin{remark}
According to the definition of $\hat{t}_j$ and the Taylor's expansion, we have
$$\hat{t}_j = -\Big( \left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j} )^2 }\right|_{\boldsymbol \beta = \bar{\boldsymbol \beta} } \Big)^{-1} \left.\frac{\partial l_n( \boldsymbol \beta )}{\partial \beta_{j}}\right|_{\boldsymbol \beta = \hat{ \boldsymbol\beta} },$$
where $\bar{\boldsymbol \beta} = \hat{\boldsymbol \beta} + (1 - a) \boldsymbol t|_j \; (0 < a < 1)$.
Additionally, using Taylor's expansion and simple algebra, for any $j\in \mathcal{I}$, the forward sacrifice \eqref{eqn:Delta} can be expressed as follows:
\begin{align*}
\zeta^*_j
= - \left.\frac{\partial l_n( \boldsymbol \beta )}{\partial \beta_{j}}\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta} } \hat{t}_j - \frac{1}{2}\left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j} )^2 }\right|_{\boldsymbol \beta = \bar{\boldsymbol \beta}} (\hat{t}_j)^2
\approx \frac{1}{2}\Big( \left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j} )^2 }\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta} } \Big)^{-1} ( \hat{\boldsymbol d}_j )^2.
\end{align*}
Similarly, for any $j\in \mathcal{A}$, the backward sacrifice \eqref{eqn:delta} can be approximated by
\begin{align*}
\xi^*_j
= - \left.\frac{\partial l_n( \boldsymbol\beta )}{\partial \beta_{j}}\right|_{\boldsymbol \beta = \hat{\boldsymbol\beta} } \hat \beta_j + \frac{1}{2}\left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j})^2 }\right|_{\boldsymbol \beta = \bar{\bar{\boldsymbol\beta} }} (\hat\beta_j)^2
\approx \frac{1}{2}\left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j})^2 }\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta}} (\hat {\boldsymbol \beta}_j)^2,
\end{align*}
where $\bar{\bar{\boldsymbol \beta} }= \hat{\boldsymbol \beta}|_{\mathcal{A} \setminus \{j\} } + a^\prime \hat{\boldsymbol \beta} |_j \; (0 < a^\prime < 1).$
% where $ d_j = \left.\frac{\partial l_n( \boldsymbol \beta )}{\partial \beta_{j}}\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta} }$.
We can see that the sacrifices $\xi^*_j$ and $\zeta^*_j$ can be approximated by the weighted squared values of $\hat{\boldsymbol \beta}_{j}$ and $\hat{\boldsymbol d}_{j}$, respectively.
%\end{remark}
%{\color{red}(\citep{bertsimas2021sparse} provides a closed form expression for the first-order derivative of the loss function $l_n$ with respect to each feature j. The expression involves the dual variable associated with the minimization problem. How do these derivatives relate to the  sacrifice criteria proposed in the present paper.)}
%{\color{blue}I would like to highlight some differences. (i) The existence of these derivatives heavily relies on the ridge penalty in the objective; on the contrary, the sacrifices can always be defined. (ii) When the ridge penalty is considered in our method, the backward sacrifice has a form:
%\begin{align*}
% \xi^*_j \approx \frac{1}{2} \left(\left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j})^2 }\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta}} + \frac{1}{\gamma} \right) (\hat {\boldsymbol \beta}_j)^2.
%\end{align*}
%Compared with the derivates with form: $-\frac{1}{2\gamma} (\hat{\boldsymbol{\beta}}_j)^2$, the backward sacrifice also considers the second-order derivate of the loss function. Finally, the backward sacrifice is directly used for updating subset selection, while these derivates are used to solve an optimization problem that finds another sparse solution.
%}

% Intuitively, for $j \in \mathcal{A}$ (or $j \in \mathcal{I}$),
% the bigger the magnitude of $\xi_j$ (or $\zeta_j$) is,
% the more important the variable $j$ is.
% The main idea of our algorithm is swapping ``irrelevant'' variables in $\mathcal{A}$ and
% ``important'' variables in $\mathcal{I}$,
% it may result in a higher quality solution.
% This intuition motivates our splicing method. %Moreover, since We are more interested in the ranking than the sacrifices' values, the unknown parameter $\sigma^2$ can be ignored, i.e., $\sigma^2 \equiv 1$.
%\noindent\textbf{Remark 2}: In backward regression (or forward regression), we only focus on the variable whose loss gives the most statistically insignificant deterioration (or significant improvement) of the model. However, in our method, we rank the importance of the variables in the active and inactive sets separately and then splice the active and inactive sets.
Our algorithm's central idea is to swap ``irrelevant'' variables in $\mathcal{A}$ for
``important'' variables in $\mathcal{I}$, which may result in a higher-quality solution.
This intuitive idea is referred to as the ``splicing'' technique. Specifically, denote
\begin{equation}\label{eqn:approx_sacrifice}
\begin{split}
\xi_j & \coloneqq \left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j})^2 }\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta}} (\hat {\boldsymbol \beta}_j)^2 ~(\textup{for}~ j \in \mathcal{A}), \\
\zeta_j & \coloneqq \Big( \left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j} )^2 }\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta} } \Big)^{-1} ( \hat{\boldsymbol d}_j )^2 ~(\textup{for}~ j \in \mathcal{I}),
\end{split}
\end{equation}
given any splicing size $k\leq s$, and
\begin{equation}\label{eqn:splicing_set}
\begin{split}
\mathcal{S}_{k, 1}= \{j\in \mathcal{A}: \sum_{i \in \mathcal{A}} \mathrm{I}( \xi_j \geq \xi_i) \leq k\},
\;
\mathcal{S}_{k, 2}= \{j\in \mathcal{I}: \sum_{i \in \mathcal{I}} \mathrm{I}( \zeta_j\leq \zeta_i) \leq k\},
\end{split}
\end{equation}
% $\mathcal{S}_{k, 1}$ represents $k$ irrelevant variables in $\mathcal{A}$ and $\mathcal{S}_{k, 2}$ represents $k$ relevant variables in $\mathcal{I}$.
splicing $\mathcal{A}$ and $\mathcal{I}$ by swapping $\mathcal{S}_{k, 1}$ and $\mathcal{S}_{k, 2}$, resulting in a candidate active set $\tilde{\mathcal{A}} = (\mathcal{A}\setminus \mathcal{S}_{k, 1}) \cup \mathcal{S}_{k, 2}$.
Denote $\tilde{\mathcal{I}} = (\tilde{\mathcal{A}} )^c$ and $\tilde{\boldsymbol \beta}=\arg\min\limits_{\boldsymbol \beta_{\tilde{\mathcal{I}}}=0} l_n(\boldsymbol \beta)$.
If $l_n(\tilde{\boldsymbol \beta})$ is significantly smaller than $l_n(\hat{ \boldsymbol \beta})$,
then we believe $\tilde{\mathcal{A}}$ surpasses $\mathcal{A}$,
and we should update the active set: $\mathcal{A} \leftarrow \tilde{\mathcal{A}}$.
The active set can be iteratively updated using the splicing technique until no visible reduction in loss is possible. The above argument is summarized in Algorithm~\ref{alg:fbess}.
\begin{algorithm}[htbp]
\caption{\underline{Be}st-\underline{S}ubset \underline{S}election for \underline{GLM} given support size $s$ (\textbf{BESS-GLM})}
\label{alg:fbess}
\begin{algorithmic}[1]
\REQUIRE A dataset $\{(\boldsymbol{x}_i, y_i)\}^{n}_{i=1}$, an initial active set $\mathcal{A}^0$ with $s$ elements,
the maximum splicing size $k_{\max} (\leq s)$, and a threshold $\tau_s$.
\STATE Initialize: $q \leftarrow -1$, $\mathcal{I}^0 \leftarrow (\mathcal{A}^0)^c$,
$\boldsymbol \beta^0 \leftarrow \arg\min\limits_{\boldsymbol \beta_{\mathcal{I}^0}=0} l_n(\boldsymbol \beta)$, and
$\boldsymbol d^{0} \leftarrow \left.\frac{\partial l_n( \boldsymbol \beta )}{\partial \boldsymbol \beta}\right|_{\boldsymbol \beta = \boldsymbol \beta^0 }$.
\REPEAT
\STATE $q \leftarrow q + 1$, and $L \leftarrow l_n(\boldsymbol \beta^q)$.
\FOR {$k=1, \ldots, k_{\max}$}
\STATE $\xi_j \leftarrow \left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j})^2 }\right|_{\boldsymbol \beta = {\boldsymbol \beta}^q} ({\boldsymbol \beta}^q_j)^2, j\in \mathcal{A}^q$,
and
$\zeta_j \leftarrow \Big(\left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j} )^2 }\right|_{\boldsymbol \beta = {\boldsymbol \beta}^q} \Big)^{-1}( {\boldsymbol d}^q_j )^2, j\in \mathcal{I}^q$.
\STATE Update the candidate active set via splicing: $\tilde{\mathcal{A}} \leftarrow (\mathcal{A}^q \backslash \mathcal{S}_{k,1}) \cup \mathcal{S}_{k,2}$, where
\begin{align*}
\mathcal{S}_{k,1} = \{j\in \mathcal{A}^q: \sum_{i \in \mathcal{A}^q} \mathrm{I}( \xi_j \geq \xi_i) \leq k\}, \; \mathcal{S}_{k,2} = \{j\in \mathcal{I}^q: \sum_{i \in \mathcal{I}^q} \mathrm{I}( \zeta_j \leq \zeta_i) \leq k\}.
\end{align*}
\STATE Let $\tilde{\mathcal{I}} \leftarrow (\tilde{\mathcal{A}})^c$,
solve $\tilde{\boldsymbol \beta} \leftarrow \arg\min\limits_{\boldsymbol \beta_{\tilde{\mathcal{I}}}=0} l_n(\boldsymbol \beta)$
and compute
$\tilde{\boldsymbol d} \leftarrow \left.\frac{\partial l_n( \boldsymbol \beta )}{\partial \boldsymbol \beta}\right|_{\boldsymbol \beta = \tilde{\boldsymbol \beta} }.$
\IF {$L - l_n(\tilde{\boldsymbol \beta})>\tau_s$}
\STATE $L \leftarrow l_n(\tilde{\boldsymbol \beta} )$, $({\mathcal{A}^{q+1}} , {\mathcal{I}^{q+1}} , {\boldsymbol \beta}^{q+1}, {\boldsymbol d}^{q+1}) \leftarrow (\tilde{\mathcal{A}} ,\tilde{\mathcal{I}} , \tilde{\boldsymbol\beta}, \tilde{\boldsymbol d })$, and break from the \textbf{for} loop.
\ENDIF
\ENDFOR
\UNTIL{$\mathcal{A}^{q+1} = \mathcal{A}^{q}$}
\ENSURE $(\boldsymbol \beta^{q}, \mathcal{A}^{q}, \boldsymbol{d}^{q})$.
\end{algorithmic}
\end{algorithm}
% We summarize our argument in the above as \textbf{GBESS}($s$) and
% Splicing($\boldsymbol \beta$, $\boldsymbol d$, $\mathcal{A}$, $\mathcal{I}$, $s$) in
% Algorithm~\ref{alg:fbess} and Algorithm~\ref{alg:splicing}, respectively.
% \begin{algorithm}[htbp]
% \caption{Best-Subset Selection for GLM given support size $s$ (\textbf{BESS-GLM})}
% \label{alg:fbess}
% \begin{algorithmic}[1]
% \REQUIRE $X$, $y$, $\mathcal{A}^0$ with $s$ elements, $k_{\max}\leq s$, and a threshold $\tau_s$.
% \STATE Initial $q = -1$, $\mathcal{I}^0 = (\mathcal{A}^0)^c$,
% $\boldsymbol \beta^0 = \arg\min\limits_{\boldsymbol \beta_{\mathcal{I}}=0} l_n(\boldsymbol \beta)$, and
% $\boldsymbol d^{0} = \left.\frac{\partial l_n( \boldsymbol \beta )}{\partial \boldsymbol \beta}\right|_{\boldsymbol \beta = \boldsymbol \beta^0 }$.
% \REPEAT
% \STATE $q \leftarrow q + 1$.
% \STATE $(\boldsymbol \beta^{q+1}, \boldsymbol d^{q+1}, \mathcal{A}^{q+1}, \mathcal{I}^{q+1}) \leftarrow $ Splicing($\boldsymbol \beta^{q}$, $\boldsymbol d^{q}$, $\mathcal{A}^q$, $\mathcal{I}^q$, $k_{\max}$, $\tau_s$).
% \UNTIL{$\mathcal{A}^{q+1} = \mathcal{A}^{q}$}
% \ENSURE $(\boldsymbol \beta^{q}, \mathcal{A}^{q}, \boldsymbol{d}^{q})$.
% \end{algorithmic}
% \end{algorithm}
% \begin{algorithm}[htbp]
% \caption{Splicing($\boldsymbol \beta$, $\boldsymbol d$, $\mathcal{A}$, $\mathcal{I}$, $k_{\max}$, $\tau_s$)}
% \label{alg:splicing}
% \begin{algorithmic}[1]
% \REQUIRE $\hat{\boldsymbol \beta}$, $\hat{\boldsymbol d}$, $\mathcal{A}$, $\mathcal{I}, k_{\max}, \tau_s$.
% \STATE Initial $L = l_n(\boldsymbol \beta)$,
% $\xi_j = \left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j})^2 }\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta}} (\hat {\boldsymbol \beta}_j)^2, j\in \mathcal{A}$,
% and
% $\zeta_j = \Big(\left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j} )^2 }\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta} } \Big)^{-1}( {\boldsymbol d}_j )^2, j\in \mathcal{I}$.
% \FOR {$k=1, \ldots, k_{\max}$}
% \STATE Update active set as
% $\tilde{\mathcal{A}} = (\mathcal{A} \backslash \mathcal{S}_{k,1}) \cup \mathcal{S}_{k,2}$, $\tilde{ \mathcal{I} } = (\mathcal{I} \backslash \mathcal{S}_{k,2}) \cup \mathcal{S}_{k,1}$, where
% \begin{align*}
% \mathcal{S}_{k,1} = \{j\in \mathcal{A}: \sum_{i \in \mathcal{A}} \mathrm{I}( \xi_j \geq \xi_i) \leq k\}, \; \mathcal{S}_{k,2} = \{j\in \mathcal{I}: \sum_{i \in \mathcal{I}} \mathrm{I}( \zeta_j \leq \zeta_i) \leq k\}.
% \end{align*}
% \STATE Compute $\tilde{\boldsymbol \beta} = \arg\min\limits_{\boldsymbol \beta_{\tilde{\mathcal{I}}}=0} l_n(\boldsymbol \beta)$ and
% $\tilde{\boldsymbol d} = \left.\frac{\partial l_n( \boldsymbol \beta )}{\partial \boldsymbol \beta}\right|_{\boldsymbol \beta = \tilde{\boldsymbol \beta} }.$
% \IF {$L - l_n(\tilde{\boldsymbol \beta})>\tau_s$}
% \STATE $L \leftarrow l_n(\tilde{\boldsymbol \beta} )~\text{and}~ ({\mathcal{A}} , {\mathcal{I}} , {\boldsymbol \beta}, {\boldsymbol d}) \leftarrow (\tilde{\mathcal{A}} ,\tilde{\mathcal{I}} , \tilde{\boldsymbol\beta}, \tilde{\boldsymbol d }).$
% \ENDIF
% \ENDFOR
% \ENSURE $(\hat{\boldsymbol \beta}, \hat{\boldsymbol d}, \hat{\mathcal{A}}, \hat{\mathcal{I}})$.
% \end{algorithmic}
% \end{algorithm}

\begin{remark}
Algorithm~\ref{alg:fbess} necessitates repeated splicing until the active set converges. A good initial guess for the active set $\mathcal{A}_0$ would accelerate the convergence of Algorithm~\ref{alg:fbess}.
The output of sure independent screening for GLMs \citep{jianqingfanSureIndependenceScreening2010} provides an intuitive setting for the initial active set $\mathcal{A}_0$.
To be more precise, we perform predictor-wise regression:
\begin{align*}
% (\hat{\boldsymbol{\beta}}_{j, 0}^M, \hat{\boldsymbol{\beta}}_j^M)
% \leftarrow
% \arg\min_{\beta_0, \beta} - \sum_{i=1}^n\{y_i (\beta_0 + x_{ij}\beta) - b(\beta_0 + x_{ij}\beta) + c(y_i,\phi)\},
% \textup{ for } j=1, \ldots, p.
\hat{{\beta}}_j^M
\leftarrow
\arg\min_{\beta} - \sum_{i=1}^n\{y_i x_{ij}\beta - b(x_{ij}\beta) + c(y_i,\phi)\},
\textup{ for } j=1, \ldots, p.
\end{align*}
The initial set $\mathcal{A}_0$ is then set as $\left\{1 \leq j \leq p_{n}:|\hat{{\beta}}_{j}^{M}| \geq \gamma_{s} \right\}$,
with $\gamma_s$ being the $s$-th largest value among $\{ |\hat{{\beta}}_{1}^{M}|, \ldots, |\hat{{\beta}}_{p}^{M}| \}$.
\end{remark}
\begin{remark}
When $s > s^*$, after the algorithm has covered the true active set, further splicing of irrelevant variables reduces the loss slightly but has no effect on subset selection.
The threshold $\tau_s$ is used to prevent such unnecessary splicing. We choose $\tau_s = 0.01 s\log p\log\log n$ in our implementation based on Assumption~\ref{con:loss-reduce-threshold} in Section~\ref{sec:assumptions}.
\end{remark}
% \begin{remark}
% In our implementation, we consider the standard iterative Newton-Raphson least square (IRLS) method
% and gradient descent to optimize the non-linear loss function under a certain support set.
% IRLS is a second-order optimization method and convergence to the minimum in a few iterations,
% yet the computation of the full Hessian matrix is consuming when $s$ is large.
% Therefore, we employ gradient descent to
% solve the optimization problem, equipped with the diagonal elements in the Hessian matrix.
% \end{remark}
\begin{remark}
The maximum splicing size is specified by $k_{\max}$.
% Our numerical result presented in Figure~\ref{fig:simu_sparse_recovery}
Our numerical results indicate that $k_{\max}$ generally has a negligible effect on identifying the relevant variables.
However, $k_{\max}$ makes a tradeoff between the computational time required by the \textup{\textbf{repeat-until}} loop and the \textup{\textbf{for}} loop.
% A large $k_{\max}$ would consider many possible active sets,
% a part of which may not be better than before but consume computational time.
% A small $k_{\max}$ would improve the quality of active sets slowly and increase the splicing time within \textup{\textbf{repeat-until}} loop.
% {\color{red}(
More precisely, a larger $k_{\max}$ considers more possible active sets in a single \textup{\textbf{for}} loop
but may result in less execution of the \textup{\textbf{repeat-until}} loop.
On the other hand, a smaller $k_{\max}$ consumes less time in a single \textup{\textbf{for}} loop but may result in more times of repetitions.
% )}
Our numerical experience suggests $k_{\max} = 2$ or $5$ is a reasonable choice.
\end{remark}
\begin{remark}
Algorithm~\ref{alg:fbess} naturally terminates since the loss monotonously decreases.
More importantly, it runs extremely fast.
It is clear that the computational complexity of Algorithm~\ref{alg:fbess} is dominated by computing the following:
(i) regression coefficients with fixed support sets;
(ii) the gradient of loss function at the estimated coefficients; and
(iii) the diagonal elements of the Hessian matrix of the loss function.
For (i), the standard GLM textbook's iterative reweighted least squares algorithm can solve it in $O(n s^2)$ loops.
According to (ii), the gradient elements are $\{ \sum\limits_{i=1}^n x_{ij} (y_i - b^\prime(\boldsymbol{x}_i^\top \boldsymbol{\beta})) \}_{j=1}^p$,
% where $\mathbf{w} = , \ldots, y_n - b^\prime(\boldsymbol{x}_n^\top \boldsymbol{\beta}))^\top$,
% In terms of (ii), it is shown to be $\mathbf{X} \mathbf{w}$,
% where $\mathbf{w} = (y_1 - b^\prime(\boldsymbol{x}_1^\top \boldsymbol{\beta}), \ldots, y_n - b^\prime(\boldsymbol{x}_n^\top \boldsymbol{\beta}))^\top$,
and thus can be computed in $O(np)$ loops.
For (iii), the diagonal elements equal to $\{ \sum\limits_{i=1}^n b^{\prime\prime}(\boldsymbol{x}_i^\top \boldsymbol{\beta}) x_{ij}^2 \}_{j=1}^{p}$,
and can be computed using $O(np)$ loops.
When combined with the analysis for the number of iterations within the \textbf{\textup{repeat-until}} loop in Theorem~\ref{thm:num_of_iter},
the computational complexity of Algorithm~\ref{alg:fbess} is roughly controlled by $O(n (p + s^2))$.
On the other hand, searching all possible subsets requires $O(n s^2 p^s)$ loops.
\end{remark}
% {\color{red}(\citep{bertsimas2020sparse} propose a polynomial-time algorithm where, at each iteration, a new support of size k is computed. However, unlike in the present paper, it can be arbitrarily different from the support of the previous iteration. The authors should at least cite and comment on the connection between the two methods. Please include it in the numerical benchmark as well.)}
% {\color{blue}
% \begin{remark}
% We would like to highlight the difference between Algorithm 1 and other algorithms (e.g., \citep{bertsimas2020sparse}, \citep{bertsimas2021sparse}) for solving Problem (1). These algorithms select variables associated with the s smallest of vector $(-\frac{\gamma}{2} \alpha ^{\top} X_j X_j^\top \alpha)_{j=1,\ldots,p} = (-\frac{1}{2\gamma } \beta_j^2 ) _{j=1,\ldots,p}$, where $\alpha$ is the dual variable. These magnitudes are similar to the sacrifices of our method in the active set. In contrast to comparing all the variables simultaneously as these algorithms, our algorithm compares the sacrifices of variables in the active set and inactive set, respectively, and then splices the irrelevant variables in the active set and the relevant variable in the inactive set. It is worth noting that the support of these algorithms can be arbitrarily different from the support of the previous iteration. Our method possesses this property if we set $k_{max}=s$.
% \end{remark}}
%\begin{remark}
We conclude this section by discussing the rationale for leveraging the splicing concept.
Splicing has been demonstrated to be a novel technique for solving the best-subset selection problem under a linear model in polynomial time (in terms of $n$ and $p$) and with a high probability \citep{zhu2020polynomial}.
Empirical evidence indicates that splicing can produce high-quality solutions for best-subset regression problems. Indeed, when dimensionality $p = O(10^5)$ and sample size $n = O(10^3)$, it can effectively solve best-subset selection in half a minute and produce sparse coefficient estimation with desirable predictive power \citep{zhu-abess-arxiv}.
However, the algorithm is not a straightforward generalization because theoretical validation of the splicing concept is significantly more complicated than \citet{zhu2020polynomial} for two reasons.
To begin, because the approximations for the sacrifices \eqref{eqn:Delta} and \eqref{eqn:delta} serve as the splicing criteria, it cannot be deduced directly that splicing results in a higher quality subset selection estimation. The second difficulty level stems from the adaptability and potential complexity of the loss function's form. Our rigorous analysis eliminates both difficulties and establishes statistical properties for Algorithm~\ref{alg:fbess} in Lemma~\ref{lemma:consistency1}.
%\end{remark}

% \noindent\rule{\textwidth}{1pt}
% \textbf{Algorithm 1}: \textbf{GBESS($s$)} Best-Subset Selection for GLM given support size $s$\\
% \vspace{-0.5cm}
% \noindent\rule{\textwidth}{0.8pt}
% \vspace{-0.5cm}
% \begin{enumerate}
% \item Input: $X$, $y$, $\mathcal{A}^0$ and positive integer $s$, $k_{\max}\leq s$
% \item Initial $\mathcal{I}^0 = (\mathcal{A}^0)^c$ and ($\boldsymbol \beta^0$, $ \boldsymbol d^0$)
% \begin{eqnarray*}
% \boldsymbol \beta^0 &= & \arg\min\limits_{\text{supp}(\boldsymbol \beta) = \mathcal{A}^0} l_n(\boldsymbol \beta)\\
% \boldsymbol d^{0} &= & \left.\frac{\partial l_n( \boldsymbol \beta )}{\partial \boldsymbol \beta}\right|_{\boldsymbol \beta = \boldsymbol \beta^0 }.
% \end{eqnarray*}

% \item \textbf{For} $q=0,1,\dots$, \textbf{do}

% $(\boldsymbol \beta^{q+1}, \boldsymbol d^{q+1}, \mathcal{A}^{q+1}, \mathcal{I}^{q+1})$ = Splicing($\boldsymbol \beta^{q}$, $\boldsymbol d^{q}$, $\mathcal{A}^q$, $\mathcal{I}^q$, $s$, $k_{\max}$)

% \textbf{If} $(\mathcal{A}^{q+1}, \mathcal{I}^{q+1}) = (\mathcal{A}^{q}, \mathcal{I}^{q})$, \textbf{then} stop\\
% $~~~~$ \textbf{else} $q = q+1$\\
% %\end{enumerate}
% \textbf{end for}
% \item Output $(\hat{\boldsymbol \beta}, \hat{\mathcal{A}})= (\boldsymbol \beta^{q+1}, \mathcal{A}^{q+1})$
% \end{enumerate}
% \noindent\rule{\textwidth}{1pt}

% \noindent\rule{\textwidth}{1pt}
% \textbf{Algorithm 2}: Splicing($\boldsymbol \beta$, $\boldsymbol d$, $\mathcal{A}$, $\mathcal{I}$, $s$, $k_{\max}$)\\
% \vspace{-0.5cm}
% \noindent\rule{\textwidth}{0.8pt}
% \vspace{-0.5cm}
% \begin{enumerate}
% \item \textbf{Input}: $\boldsymbol \beta$, $\boldsymbol d$, $\mathcal{A}$, $\mathcal{I}$ and $s$
% \item Initial $L = l_n(\boldsymbol \beta)$, $\xi_j = \frac{1}{2}\left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j})^2 }\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta}} (\hat \beta_j)^2, j\in \mathcal{A}$ and
% $\zeta_j = \frac{1}{2}\Big( \left.\frac{\partial^2 l_n( \boldsymbol \beta )}{ (\partial \beta_{j} )^2 }\right|_{\boldsymbol \beta = \hat{\boldsymbol \beta} } \Big)^{-1}
% \Big( d_j \Big)^2, j\in \mathcal{I}$
% \item \textbf{For} $k=1,2,\dots, k_{\max}$, \textbf{do}
% \begin{eqnarray*}
% \mathcal{S}_{k,1} &= &\{j\in \mathcal{A}: \sum_{i \in \mathcal{A}} \mathrm{I}( \xi_j \geq \xi_i) \leq k\}\\
% \mathcal{S}_{k,2} &= &\{j\in \mathcal{I}: \sum_{i \in \mathcal{I}} \mathrm{I}( \zeta_j \leq \zeta_i) \leq k\}
% \end{eqnarray*}
% Let $ \tilde{\mathcal{A}} = (\mathcal{A} \backslash \mathcal{S}_{k,1}) \cup \mathcal{S}_{k,2}$, $\tilde{ \mathcal{I} } = (\mathcal{I} \backslash \mathcal{S}_{k,2}) \cup \mathcal{S}_{k,1}$ and compute
% \begin{align*}
% \tilde{\boldsymbol \beta} = \arg\min_{\text{supp}(\boldsymbol\beta) = \tilde{\mathcal{A}} } l_n(\boldsymbol\beta )~\text{and}~\tilde{\boldsymbol d} = \left.\frac{\partial l_n( \boldsymbol \beta )}{\partial \boldsymbol \beta}\right|_{\boldsymbol \beta = \tilde{\boldsymbol \beta} }
% \end{align*}
% \textbf{If} $L > l_n(\tilde{\boldsymbol \beta} ) $, \textbf{then}
% \begin{align*}
% L = l_n(\tilde{\boldsymbol \beta} )~\text{and}~ (\hat{\mathcal{A}} , \hat{\mathcal{I}} , \hat{\boldsymbol \beta}, \hat{\boldsymbol d})&= (\tilde{\mathcal{A}} ,\tilde{\mathcal{I}} , \tilde{\boldsymbol\beta}, \tilde{\boldsymbol d })
% \end{align*}
% \textbf{End for}
% \item \textbf{return($\hat{\boldsymbol \beta}, \hat{\boldsymbol d}, \hat{\mathcal{A}} , \hat{\mathcal{I}} $)}
% \end{enumerate}
% \noindent\rule{\textwidth}{1pt}

\subsection{Adaptive Best-Subset Selection}
In this part, we design a data-driven procedure to determine the optimal support size $s$. Model selection methods such as cross-validation and information criteria are widely used techniques. Recently, \citet{fan2013tuning} explored generalized information criterion (GIC) in tuning parameter selection for penalized likelihood methods under GLMs. In particular, we introduce a GIC-type information criterion to recovery support size, which is defined as follows:
\begin{equation}\label{eq:gic}
F(\hat{\boldsymbol \beta}) = l_n( \hat{\boldsymbol \beta} ) + |\text{supp}(\hat{\boldsymbol \beta})| \log(p) \log\log n.
\end{equation}
Intuitively speaking, the model complexity penalty term $|\text{supp}(\hat{\boldsymbol \beta})| \log p \log\log n$ in \eqref{eq:gic} is set to prevent over-fitting, the term $\log\log n$ with a slow diverging rate is used to prevent under-fitting. Combining the Algorithm~\ref{alg:fbess} with GIC, we select the support size that minimizes the $F(\hat{\boldsymbol{\beta}})$. Specifically, we consider a series of candidate support sizes, then we conduct Algorithm~\ref{alg:fbess} for each fixed candidate support size and compute the corresponding GIC via E.q.~\eqref{eq:gic}. The support size that minimizes the GIC is chosen. The procedure above is detailed in Algorithm~\ref{alg:abess}. According to Theorem~\ref{thm:consistency4}, Algorithm~\ref{alg:abess} guarantees consistent best-subset selection with high probability.
\begin{algorithm}[htbp]
\caption{Adaptive Best-Subset Selection for GLM (\textbf{ABESS})}
\label{alg:abess}
\begin{algorithmic}[1]
\REQUIRE A dataset set $\{(\boldsymbol{x}_i, y_i)\}^{n}_{i=1}$ and the maximum support size $s_{\max}$.
\STATE $\mathcal{A}^0 \leftarrow \{ \arg\max\limits_j | (\frac{\partial l_n( \boldsymbol \beta )}{\partial \boldsymbol \beta} |_{\boldsymbol \beta = {\boldsymbol 0} })_j | \}$
\FOR {$s = 1, \ldots, s_{\max}$}
\STATE $(\hat{\boldsymbol \beta}_{s}, \hat{\mathcal{A}}_{s}, \hat{\boldsymbol{d}}_s) \leftarrow \textbf{BESS-GLM}\left( \{(\boldsymbol{x}_i, y_i)\}^{n}_{i=1}, \mathcal{A}^0, k_{\max}, \tau_s \right)$.
% \STATE $\textup{GIC}_s \leftarrow F(\hat{\boldsymbol \beta}_s)$
\STATE $\mathcal{A}^0 \leftarrow \hat{\mathcal{A}}_{s} \cup \{ \arg\max\limits_j|(\hat{\boldsymbol{d}}_s)_j| \}$.
\ENDFOR
\STATE Compute the size of the support set that minimizes GIC: $\hat{s} \leftarrow \arg\min\limits_s F(\hat{\boldsymbol \beta}_s)$.
\ENSURE $(\hat{\boldsymbol \beta}_{\hat{s}}, \hat{\mathcal{A}}_{\hat{s}} )$.
\end{algorithmic}
\end{algorithm}
\begin{remark}
According to Condition \ref{con:maximum-size} in Section \ref{sec:assumptions}, a reasonable choice for $s_{\max}$ is $s_{\max} = [(\frac{{n}}{\log p})^{\frac{1}{4}}]$, where $[a]$ returns the nearest integer of $a$.
\end{remark}
\begin{remark}
In Algorithm~\ref{alg:abess}, Algorithm~\ref{alg:fbess} is run on different support sizes, from a small to a large one. Fortunately, it allows exploiting the latest output of Algorithm~\ref{alg:fbess} to construct a better active set to help Algorithm~\ref{alg:fbess} reach convergence with less splicing iteration. Precisely, in Step 4 of Algorithm~\ref{alg:abess}, we combine the selected best subset returned by Algorithm~\ref{alg:fbess} together with the variable with the most significant forward sacrifice and set it as an initial active set for the following support size. It is the so-called ``warm starts initialization''. Both theory and numerical experiments suggest such initialization is remarkably efficient \citep{barutConditionalSureIndependence2016, friedman2010regularization}.
\end{remark}
% \noindent\rule{\textwidth}{1pt}
% \textbf{Algorithm 2} : \textbf{ABESS} Adaptive Best-Subset Selection \\
% \vspace{-0.5cm}
% \noindent\rule{\textwidth}{0.8pt}
% \vspace{-0.5cm}
% \begin{enumerate}
% \item Input: $\boldsymbol X$, $\boldsymbol y$, $\mathcal{A}^0$, $\mathcal{I}^0$, set $s_{\max} = \frac{n}{\log(p)\log\log n}$ %$\hat{\beta}_0=0$ and $\hat{d}_0 = X' y$.
% \item \textbf{For} $s = 1,2,\ldots,s_{\max}$, \textbf{do}
% $$(\hat{\boldsymbol \beta}_{s}, \hat{\mathcal{A}}_{s})= \textbf{GBESS}(s)~\text{or}~ \textbf{FastGBESS}(s)$$
% \textbf{end for}
% \item Compute the minimum information:
% $$\hat{s} = \arg\min_s \log l_n( \hat{\boldsymbol \beta}_s ) + K(\hat{\boldsymbol \beta}_s) \log(p) \log\log n$$
% Output $(\hat{\boldsymbol \beta}_{\hat{s}}, \hat{\mathcal{A}}_{\hat{s}} )$
% \end{enumerate}
% \noindent\rule{\textwidth}{1pt}
