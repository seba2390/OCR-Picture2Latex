\section{Introduction}
Generalized linear models \citep*[GLMs, ][]{mccullagh1989generalized} can model a family of continuous or discrete responses with a set of predictors.
% It is a broad term encompassing various regression models, including linear regression, logistic regression, and gamma regression.
It generalizes linear regression by assuming responses follow an exponential distribution with a mean determined by a linear combination of predictors.
Specifically, let $\{(\boldsymbol{x}_i, y_i)\}^{n}_{i=1}$ be a dataset with $n$ observations
in which $y_i$ is a response and $\boldsymbol x_i = (x_{i1}, \ldots, x_{ip})^\top$ is
a $p$-dimensional predictor vector.
GLMs assume the density function of $y_i$ given $\boldsymbol x_i$ is:
$f(y_i;\boldsymbol{x}_i^\top \boldsymbol{\beta},\phi) = \exp\{y_i\boldsymbol{x}_i^\top \boldsymbol{\beta} - b(\boldsymbol{x}_i^\top \boldsymbol{\beta}) + c(y_i,\phi)\}$,
where $\boldsymbol \beta \in \mathbb{R}^p$ is a regression coefficient vector,
$b(\cdot)$ and $c(\cdot)$ are some suitably chosen known functions
such that $b'(\boldsymbol{x}_i^\top \boldsymbol{\beta}) = \mathbb{E}[y_i|\boldsymbol {x}_i]$.
% $g(\mu_i)=\theta_i$ is the link function, and $\phi$ is a known scale parameter.
% Here, we assume that there is no intercept effect on $\theta_i$, and we keep this form throughout the paper to simplify presentations.
It is worth noting that $b(\cdot)$ is the critical component in determining the distribution of $y_i$;
% Different distributions and link functions lead to different GLMs.
for instance, let $\theta = \boldsymbol{x}_i^\top \boldsymbol{\beta}$,
taking $b(\theta)=\frac{1}{2}\theta^2$ results in the ordinary linear model
$y_i = \boldsymbol{x}_i^\top \boldsymbol{\beta} + \epsilon_i$, where $\epsilon_i \sim \mathcal{N}(0, 1)$.
Due to their flexibility, GLMs include many frequently-used models like logistic regression, Poisson regression, gamma regression, etc.
Therefore, GLMs adapt to various scientific and engineering disciplines,
including economics, biology, and geography.

Scientific research and data analysis in the modern era frequently collect a large number of predictors for GLMs in order to model responses,
i.e., $p$ is large in comparison to $n$.
It is critical to select a minimally adequate subset of predictors to fit responses accurately,
as this results in a sparse GLM that is accurate and interpretable.
Besides, when $p \approx O(n)$ or $p \gg O(n)$,
minimizing the negative log-likelihood function for $\boldsymbol \beta$,
$$l_n(\boldsymbol \beta)= - \sum_{i=1}^n\{y_i\boldsymbol {x}_i^\top \boldsymbol \beta - b(\boldsymbol {x}_i^\top\boldsymbol \beta) + c(y_i,\phi)\},$$
yields an estimated coefficient with significant fluctuation or even no unique solution.
At this time, pursuing a sparse regression coefficient vector is indispensable.
%Denote the true sparse regression coefficient as $\boldsymbol{\beta}^*$ and its support set as $\mathcal{A}^*$ (also known as the best subset or true active set) with size $s^* \coloneqq |\mathcal{A}^*|$, where $\mathbb{E}[y_i|\boldsymbol {x}_i] = b'(\boldsymbol{x}_i^\top \boldsymbol{\beta}^*)$.
We seek an efficient algorithm to solve the following best-subset selection problem:
\begin{equation}\label{eq:best-subset-glm}
\hat{\boldsymbol \beta} \leftarrow \arg\min\limits_{\boldsymbol \beta} l_n(\boldsymbol \beta), ~\textup{subject to:}~ \|\boldsymbol \beta\|_0 \leq s,
\end{equation}
%such that $\textup{supp}(\hat{\boldsymbol \beta}) = \mathcal{A}^*$.
where $\|\boldsymbol\beta\|_0 = \sum\limits_{j=1}^p \mathrm{I} (\beta_j \neq 0)$ and $\mathrm{I}(\cdot)$ is the indicator function. Since the cardinality constraint on $\boldsymbol \beta$ is non-convex,
finding such an algorithm is not trivial.
% Additionally, $s$ is frequently unknown in practice and must be determined through data.

To this end, the mathematical and statistical communities have spent decades studying
how to select the best subset. One of the most well-known methods is exhaustive enumeration in conjunction with the Akaike/Bayesian information criterion \citep{hocking1967selection, akaike1998information, schwarz1978estimating, anderson2004model}.
The branch-and-bound algorithm improves exhaustive searching by rejecting suboptimal subsets without direct evaluation \citep{lamotte1970computational, narendra1977branch}.
% Motivated by the phenomenal advancements in mixed-integer optimization,
% \citep{bertsimasForumAlgorithmicApproach2016},
% \citet{logistic2017dimitris} and
% \citet{sato2016feature} suggest using
% mixed-integer nonlinear optimization to find the optimal subset in logistic regression with hundreds of variables.
% Very recently, {\color{red}\citet{antoine2021l0learn, bertsimas2021sparse} developed algorithms for
% the best-subset selection under logistic regression models with tens of thousands of variables.}
Unfortunately,
% {\color{red}in the worst cases},
this method continues to impose a sharp increase in computational cost when $p$ grows, as determining the exact best subset is NP-hard, even for linear regression \citep{natarajan1995sparse}.
Empirical evidence is that a widely used software implementing this algorithm \citep{calcagnoGlmultiPackageEasy2010} is limited to datasets with no more than 50 variables \citep{wen2017bess}.

Numerous researchers have attempted to circumvent the NP-hardness.
One important direction is to develop statistical relaxation methods that take advantage of continuous penalties that promote sparsity.
The well-known relaxation methods are the least absolute shrinkage and selection operator \citep*[LASSO,][]{tibshirani1996regression},
elastic net \citep{zouRegularizationVariableSelection2005},
Dantzig selector \citep{emmanuelcandesDantzigSelectorStatistical2007}, smoothly clipped absolute deviation \citep*[SCAD,][]{fan2001variable},
minimax concave penalty \citep*[MCP, ][]{zhang2010nearly}, and truncated $\ell_1$ penalty \citep{xiaotong2012tlp}.
The proposed penalty functions have been extended to GLMs \citep{parkL1regularizationPathAlgorithm2007, jamesGeneralizedDantzigSelector2009, fan2011scad}.
While the majority of relaxation methods can be solved in polynomial time, some of them (for example, the LASSO) lack desirable statistical properties
due to the resulting biased estimations \citep{zhang2008sparsity}.

Another primary direction is the development of algorithms for approximating the best-subset solutions.
A well-known approximation strategy is forward stepwise model selection --- iteratively adding a variable that is highly correlated with the current residuals,
referring to the orthogonal matching pursuit in the machine learning community \citep{mallet1993omp, lozano2011gomp}.
Many researchers believe this procedure is excessively greedy and results in an unsatisfactory solution \citep {weisberg2005applied, friedman2009elements}.
To improve the adaptiveness of forward selection,
\citet{blumensathCompressedSensingNonlinear2013} extend
the iterative hard thresholding algorithm under linear models \citep{blumensathIterativeHardThresholding2009}
to nonlinear objective functions.
\citet{bahmaniGreedySparsityconstrainedOptimization2013}
propose a gradient support pursuit method for best-subset selection under logistic regression.
\citet{wen2017bess} develop a primal-dual active set algorithm for logistic regression that builds on the algorithm developed for linear models \citep{huang2017constructive}.
% {\color{red}\citet{bertsimas2020sparse} consider a dual sub-gradient algorithm motivated by the condition on the equivalent of Problem~\eqref{eq:best-subset-glm} and its Boolean relaxation.}
We refer readers to \citet{zhouGlobalQuadraticConvergence2021} for an excellent review of these methods.
Although $\ell_0$-penalized GLM learning problems are not equivalent to the problem~\eqref{eq:best-subset-glm},
some literature designs algorithms starting with $\ell_0$-penalized GLM learning.
For example, \citet{beckSparsityConstrainedNonlinear2013, antoine2021l0learn} develop coordinate descent algorithms for computing the coordinate-wise minimizer of $\ell_0$-penalized regression.
Unfortunately, the methods above have not been demonstrated to be both statistically recovery-guaranteed and computationally efficient for the best-subset selection under GLMs.
For example, the coordinate-wise minimizer cannot guarantee the recovery of the best subset. At the same time, the primal-dual active set algorithm may sink into periodic iteration even for linear models \citep{foucartHardThresholdingPursuit2011}.
% particularly when the size of the best subset is unknown.


\subsection{Our Proposal and Contributions}

% Specifically, we develop
% sacrifices which are essential concepts for the splicing method, and introduce GIC-type information to select the sparsity level.
% Recently, a fast iterative splicing method was proposed for best-subset selection for linear regression \citep{zhu2020polynomial}.
%It can improve the quality of subset selection by exchanging some irrelevant variables in the active set (non-zero coefficients) and some relevant variables in the inactive set (zero coefficients).
% Splicing method possesses excellent properties, including finite convergence and recovery of the actual model.
% In this paper, we deal with GLM with cardinality constraint via a splicing algorithm.
% We establish nice properties of the proposed method, include computational complexity and recovery of true model.
% Splicing method not only guarantees a polynomial computational complexity but also produces oracle estimator under mild conditions.
% It can improve the quality of subset selection via sequencing and exchanging.

%%%%%%%%%% content about statistical consistency %%%%%%%%%%
% Our primary objective is to contribute a reliable best-subset selection algorithm for GLM that is also certifiably efficient in the mean time.
% To accomplish this,
% we divide the procedure for choosing the optimal subset into two nested sections.
% In the first section, we select the subset with the maximum likelihood for fixed subset size.
% Specifically,
% our algorithm promotes the selected subset iteratively by including ``crucial'' variables and excluding `irrelevant'' variables,
% both of which are defined by low-cost computational criteria.
% This iteration procedure avoids the tedious task of enumerating all possible fixed-size subsets and ensures that the best subset is correctly included after a few iterations.
% We then design an information criterion to determine the model size in the second section.
% The information criterion evaluates the solutions generated by the algorithm in the first section and chooses the solution that makes a reasonable trade-off between model complexity and accuracy.
% Extensive analysis demonstrates that the second step returns the best subset with high probability.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our primary objective is to contribute a reliable, certifiably efficient, best-subset selection algorithm for GLM. To accomplish this, our algorithm promotes the $s$-cardinality selected subset by excluding ``irrelevant'' variables, including the same amount of ``crucial'' variables, both of which are defined by low-cost computational criteria. The inclusion-and-exclusion iteration continues until the model can sufficiently fit the data. This iterative algorithm avoids the tedious task of enumerating all possible fixed-size subsets. More impressively, our theoretical analysis ensures the correct selection of the best subset after a few iterations. The computational efficiency of our algorithm is also reflected by its effortless convergence. At the same time, the other best subset approximated methods require additional assumptions to ensure convergence to the fixed points of sparsity-pursuit operators. We then design an information criterion to determine the optimal $s$ in a data-driven manner. The information criterion evaluates the solutions with different cardinality and chooses the one that makes a reasonable trade-off between model complexity and accuracy. Theoretical analysis demonstrates that the underlying true subset can be identified in polynomial time with high probability by integrating the information criterion.
% Furthermore, our attention to the delicate computational aspects make our proposed algorithms
% comparable (and at times faster) in speed to the fastest
% proxy algorithms (e.g., glmnet and ncvreg).

This section concludes with the following summary of our contributions:
\begin{itemize}
\item We propose a new fast algorithm for solving the problem of best-subset selection in GLMs based on the following:
(i) an efficient technique for iteratively improving the quality of selected subset and
(ii) an information criterion for choosing the size of the selected subset.
A toolkit \textsf{abess} implementing our algorithm is freely available on CRAN at: \url{https://cran.r-project.org/web/packages/abess}.
\item We theoretically demonstrate that the new algorithm is high-speed.
To put it another way, a few steps of algorithmic iterations reach a stable solution with great certainty.
Additionally, its computational complexity is proportional to the sample size and the number of variables, comparable to the most widely used variable selection method for GLMs---the LASSO.
\item We rigorously establish our proposed algorithm's best-subset-recovery property with a high probability in GLMs, which extends the theoretical guarantees for linear models \citep{zhu2020polynomial}. This extension is not trivial because adapting GLMs to various supervised learning tasks complicates log-likelihood analysis with complex forms.
% Upon this property, we conclude the recovery of the best subset when the sparse level is correctly specified.
\item Our simulation studies provide compelling evidence that our method surpasses state-of-the-art methods in terms of subset selection and parameter estimation across various regression models, including logistic regression, Poisson regression, and multiple-response regression. Furthermore, our method exhibits exceptional performance by achieving a remarkable fourfold speedup compared to the LASSO algorithm. %Our real data analysis reveals that the proposal includes several genes influencing the P53 gene's mutation status.
\end{itemize}


\subsection{Organization and Notations}

The rest of the paper is organized as follows. In Section~\ref{sec:methodology}, we propose a novel technique for improving the selected subset in GLMs and design a new iterative best-subset selection algorithm upon this technique. Section~\ref{sec:theorical-properties} establishes our algorithm's theoretical properties, accompanied by high-level proofs. In Section~\ref{sec:efficient-implementation}, we describe the implementation details for the fast computing of our proposal. Section~\ref{sec:simulation} evaluates the proposed method's empirical performance using artificial datasets.
\if1\informsMOR{
We provide proof of our main results in Section~\ref{sec:proofs}.
}\else{
%A real-world data analysis, as described in Section~\ref{sec:real-data-analysis}, demonstrates the practical benefits of our algorithms.
}\fi
The paper concludes with a few remarks in Section~\ref{sec:conclusion-and-discussion}.
\if0\informsMOR{
Due to space constraints, we relegate the detailed proofs of primary theoretical results and additional numerical experiments to Supplementary Material.
}\fi

Below, we define a few useful notations for the content.
For any vector $\boldsymbol \beta = (\beta_1,\ldots,\beta_p)^\top \in \mathbb{R}^p$,
the $\ell_0$-norm of $\boldsymbol \beta$ is defined as $\|\boldsymbol\beta\|_0 = \sum\limits_{j=1}^p \mathrm{I} (\beta_j \neq 0)$ where $\mathrm{I}(\cdot)$ is the indicator function,
and we define the $\ell_q$-norm of $\boldsymbol \beta$ by $\|\boldsymbol\beta\|_q = ( \sum\limits_{j=1}^p |\beta_j|^q)^{1/q}$, where $q\in[1,\infty)$.
Let $\fullset = \{1, \ldots, p\}$, for any set $\mathcal{A}\subseteq \fullset$,
denote $|\mathcal{A}|$ as its cardinality, $\mathcal{A}^c= \fullset\backslash \mathcal{A}$ as the complement of $\mathcal{A}$ and $\boldsymbol\beta_{\mathcal{A}} =(\beta_j, j\in \mathcal{A}) \in \mathbb{R}^{|\mathcal{A}|}$.
The support set for the vector $\boldsymbol\beta$ is defined as $\mathrm{supp}(\boldsymbol\beta) = \{ j : \beta_j\neq 0\}$.
For a matrix $\mathbf{X} \in \mathbb{R}^{n\times p}$,
define $\mathbf{X}_{\mathcal{A}} = (\mathbf{X}_j, j\in \mathcal{A})\in \mathbb{R}^{n\times |\mathcal{A}|}$,
where $\mathbf{X}_j$ is the $j$-th column of $\mathbf{X}$.
For any vector $\boldsymbol t \in \mathbb{R}^p$ and index set $\mathcal{A}$,
we define $\boldsymbol t|_{\mathcal{A}}$ as a $p$-dimensional vector
whose the $j$-th entry $(\boldsymbol t|_{\mathcal{A}})_j$ is equal to $t_j$ if $j\in \mathcal{A}$ and zero otherwise.
For example,
% $\boldsymbol {\hat \beta}|_{\mathcal{A}}$ is the vector who se $j$th entry is $\hat \beta_j$ if $j\in \mathcal{A}$ and zero otherwise;
$\boldsymbol t|_{\{j\}}$ denotes the vector whose the $j$-th entry is $t_j$ and zero otherwise.
We also simplify $\boldsymbol t|_{\{j\}}$ as $\boldsymbol t|_{j}$ for notational convenience. Denote the true sparse regression coefficient as $\boldsymbol{\beta}^*$ and its support set as $\mathcal{A}^*$ (also known as a best subset or true active set) with size $s^* \coloneqq |\mathcal{A}^*|$, where $\mathbb{E}[y_i|\boldsymbol {x}_i] = b'(\boldsymbol{x}_i^\top \boldsymbol{\beta}^*)$.