\documentclass[11pt,reqno]{amsart}
\usepackage{graphics,amsmath,amssymb,epsfig,stmaryrd,color,amsaddr}
\usepackage{subfigure,amsfonts,geometry,array}
\geometry{hmargin=2.5cm, vmargin=2cm}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{amsthm}
\usepackage{mathabx} 
\usepackage{flafter}
\usepackage[svgnames]{xcolor}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{hyperref}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
\usepackage{natbib}

\usepackage{enumerate}
\usepackage{multirow}

\makeatletter
\renewcommand\@endtheorem{\vvv@endmarker\endtrivlist\@endpefalse}
\newcommand\vvv@endmarker{%
  {\nobreak\hfil\penalty50
  \hskip2em\vadjust{}\nobreak\hfil\openbox
  \parfillskip=0pt \finalhyphendemerits=0 \par
  \penalty 10000 \parskip=0pt\noindent}\ignorespaces}
\makeatother

\oddsidemargin +0.15in
 \evensidemargin +0.15in
\topmargin 30pt \textheight 8.1in \textwidth 6in
\linespread{1.3}\parskip .05in
\newtheorem{theorem}{Theorem}
\theoremstyle{plain}
\newtheorem{acknowledgement}{Acknowledgement}
\newtheorem{algorithm}{Algorithm}
\newtheorem{axiom}{Axiom}
\newtheorem{case}{Case}
\newtheorem{claim}{Claim}
\newtheorem*{assumption*}{Assumption}
\newtheorem{assumption}{Assumption}
\newtheorem{conclusion}{Conclusion}
\newtheorem{condition}{Condition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{criterion}{Criterion}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{exercise}{Exercise}
\newtheorem{lemma}{Lemma}
\newtheorem{notation}{Notation}
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{solution}{Solution}
\newtheorem{summary}{Summary}
\newtheorem{pilot}{Pilot}
\newenvironment{continued}[1][continued]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{varproof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\numberwithin{equation}{section}

\setlength{\extrarowheight}{-0.7cm}


\let\origtheassumption\theassumption


\begin{document}
\nonstopmode

\title[Marginal Treatment Effects with a Misclassified Treatment]{Marginal Treatment Effects with a Misclassified Treatment}
\author{Santiago Acerenza, Kyunghoon Ban, and D\'esir\'e K\'edagni}
\address{ORT Uruguay, Iowa State, and UNC-Chapel Hill} 
\noindent \date{\scriptsize{The first draft was of May 1, 2021. The present version is as of \today. We thank Isaiah Andrews, Alberto Abadie, Ot\'avio Bartalotti, Chris Bollinger, Helle Bunzel, Augustine Denteh, Sukjin Han, Guido Imbens, Shakeeb Khan, Brent Kreider, Matt Masten, Arnaud Maurel, Anna Mikusheva, Francesca Molinari, Ismael Mourifi\'e, Pierre Nguimkeu, Vitor Possebom, Thomas Richardson, Adam Rosen, Eric Tchetgen Tchetgen, Denni Tommasi, Takuya Ura, Lina Zhang, seminar participants at Cornell, Duke, Iowa State, UMass Amherst (Statistics), Syracuse, Tilburg, U. Iowa, Harvard-MIT, Rochester, U. Georgia, Wharton (Statistics), Simon Institute, AfES 2021, NASMES 2021, IAAE 2021, LACEA LAMES 2021, SEA 2021 for helpful discussions and comments. All errors are ours.
Corresponding address: 102 Gardner Hall, CB 3305 University of North Carolina Chapel Hill, NC 27599, USA. Email address: dkedagni@unc.edu}}

\begin{abstract}
This paper studies identification of the marginal treatment effect (MTE) when a binary treatment variable is misclassified. We show under standard assumptions that the MTE is identified as the derivative of the conditional expectation of the observed outcome given the true propensity score, which is partially identified. We characterize the identified set for this propensity score, and then for the MTE. We show under some mild regularity conditions that the sign of the MTE is \textit{locally identified}. We use our MTE bounds to derive bounds on other commonly used parameters in the literature. We show that  our bounds are tighter than the existing bounds for the local average treatment effect. We illustrate the practical relevance of our derived bounds through some numerical and empirical results.
\end{abstract}
\maketitle
 \maketitle
{\footnotesize \textbf{Keywords}: Heterogeneous treatment effects, misclassification, instrumental variable, set identification.

\textbf{JEL subject classification}: C14, C31, C35, C36.}
\clearpage

\section{Introduction}
The existence of measurement error in a treatment variable makes the identification of many parameters used in the causal inference literature challenging. When the treatment variable is binary, it is well-understood in the literature that the measurement error is nonclassical, that is, it depends on the true treatment. Even in the homogeneous treatment effect framework, measurement errors in a binary regressor can result in severe identification deterioration of regression coefficients \citep{Kreider2010}. \cite{Ura2018} appears to be the first to investigate the identifying power of an instrumental variable (IV) in the (unobserved) heterogeneous treatment effect model when the treatment is endogenous and mismeasured. He derives bounds on the local average treatment effect (LATE) when a binary instrument is available.\footnote{\cite{Mahajan2006} and \cite{Lewbel2007} allow for heterogeneous treatment effect through observed covariates.} \cite{Calvi_al2018} propose a new estimand for the LATE, called the measurement robust LATE (MR-LATE), and obtain point-identification in an alternative framework. \cite{Yanagi2019} shows that with the help of exogenous covariates, point-identification of the LATE can be obtained under some conditions when there is misclassification in the treatment. \cite{Tommasi2020} extend the results in \cite{Ura2018}, and \cite{Calvi_al2018} to the case with multivalued discrete instruments.

This paper investigates the identification of the MTE in settings where a binary treatment variable is misclassified, and a valid instrument is available. We show under standard assumptions that the MTE is identified as the derivative of the conditional expectation of the observed outcome given the true propensity score, which is partially identified. We provide a tractable characterization (which is an outer set) of the identified set for this propensity score, and then for the MTE. We show under some mild regularity conditions that the sign of the MTE is \textit{locally identified}. We also derive functional sharp bounds for this propensity score when the misclassification is non-differential. We use our MTE bounds to derive bounds on other commonly used parameters in the literature. In particular, we show that  our bounds for the LATE are tighter than the existing \citeauthor{Ura2018}'s (\citeyear{Ura2018}) and \citeauthor{Tommasi2020}'s (\citeyear{Tommasi2020}) bounds when the instrument is discrete. We illustrate the practical relevance of our derived bounds through some numerical and empirical results. More precisely, we apply our methodology on data from the third wave of the Indonesia Family Life Survey to measure marginal returns to upper secondary or higher schooling while allowing for the possibility that education be mismeasured. We find that the return is heterogeneous and weakly decreases with the unobserved schooling cost. 

Several papers have extensively studied issues related to misclassification in treatment variables. See, for example, \cite{Aigner1973}, \cite{Bollinger1996}, \cite{Hausman_al1998}, \cite{Molinari2008}, \cite{Hu2008}, \cite{HuSchennach2008}, etc. Recently, \cite{Mahajan2006}  uses an additional instrument, which he called ``instrument-like variable,'' to nonparametrically identify the regression function in models with a misclassified binary regressor. Considering the same model as \cite{Mahajan2006} under different assumptions, \cite{Lewbel2007} also uses a ``second'' instrument to nonparametrically identify the average treatment effect (ATE) when the treatment is misclassified. However, their results hold when the true treatment is exogenous. \cite{DiTraglia2019} show that the identification result in \cite{Mahajan2006} does not extend to the case of an endogenous treatment. In that context, they derive bounds on the average treatment effect under standard assumptions. \cite{Nguimkeu_al2019} study a homogenous treatment effect linear regression model in which a  binary  regressor  is  potentially  misclassified  and  endogenous. They use exclusion restrictions for both the participation equation and measurement error equation to identify the regression coefficient with endogenous participation and one-sided endogenous misreporting. 
\cite{Millimet2011} studies the performance of several commonly used estimators in the causal inference literature when there is measurement error in a binary treatment, and warns researchers about the consequences of ignoring the presence of measurement errors.
\cite{Kreideral2012} partially identify the average effects of food stamps on health outcomes of children when participation is endogenous and misreported by using relatively weak nonparametric assumptions and information from auxiliary data. Our paper studies potential heterogeneity in the treatment effect through the marginal treatment effect when the treatment is endogenous and mismeasured. A more related work to ours is \cite{Ura2020}, who investigates heterogeneous treatment effects in the presence of a misclassified endogenous binary treatment variable through the instrumental variable quantile regression model. 

Our paper also complements the work of \cite{Battistinal2011} and \cite{Battistinal2014}, who investigate the identification of average returns to education in the United Kingdom when attainment is potentially measured with error. While these authors focus on average returns, we investigate marginal returns, which may reveal (unobserved) heterogeneity in the treatment effects that would otherwise be hidden when looking at only average effects. Our work is also related to \citeauthor{Chalak2017}'s (\citeyear{Chalak2017}) who discusses the interpretation of various estimands (Wald, local IV) when the instrument is mismeasured, but the treatment variable is correctly measured. Differently from his framework, the instrument is observed with no error, while the treatment variable is potentially misreported. Recently, \cite{Jiangal2020} study identification in the binary IV model when allowing for simultaneous measurement errors in the instrument, treatment and/or outcome. They derive sharp bounds on the LATE assuming non-differential measurement errors and a valid IV. Our framework encompasses multivalued discrete/continuous outcomes and instruments, while the treatment variable is maintained binary. But, we only allow for misclassification in the treatment variable. Using a framework similar to ours, \cite{Possebom2021} derives complementary identification results for the MTE, allowing for dependence between the instrument and the misclassification variable. While we focus on the case where the instrument is completely randomly assigned, we show in Appendix \ref{apx:zeps} how our approach can be extended to the situation where there is dependence between the instrument and the misclassification variable. This extension is similar to \citeauthor{Ura2018}'s (\citeyear{Ura2018}) in the LATE framework. Note that \cite{KasaharaShimotsu2021} study identification in regression models when an endogenous binary regressor is misclassified, allowing for correlation between the instrument and the misclassification error. They show identification of the regression coefficient when a ``special'' covariate in the outcome equation is excluded from the misclassification probability. While they allow for heterogeneity in the average effect through observed covariates, we focus on unobserved heterogeneity using marginal treatment effects. 


The remainder of the paper is organized as follows. Section \ref{anaF} introduces the model and discusses the assumptions. Section \ref{Ident} presents the main identification results, Section \ref{falserates} discusses how information about the false positive/negative misclassification rates can be helpful for our identification approach. Section \ref{EmpRel} illustrates the empirical relevance of the MTE bounds, Section \ref{ext:discrete} presents some extensions, Section \ref{App} provides a real world empirical example, and Section \ref{conclusion} concludes. Proofs of the main results are relegated to the appendix.

\clearpage
\section{Analytical Framework}\label{anaF}
Consider the following model:\footnote{We show in Lemma \ref{misc:specific} in Appendix \ref{apx:spec} that the specification $D=D^* (1-\varepsilon) +(1-D^*)\varepsilon$ is without loss of generality. Existing papers such as \cite{Ura2018}, \cite{Calvi_al2018}, \cite{Tommasi2020}, among others have considered the following specification $D=D^* \varepsilon_1 +(1-D^*)\varepsilon_0$. We show that the binary nature of $D$ and $D^*$ imposes that $\varepsilon_0+\varepsilon_1=1$ (see Lemma \ref{misc:specific}). This restriction brings some extra information that helps in our identification strategy.}
\begin{eqnarray}\label{seq1}
\left\{ \begin{array}{lcl}
     Y&=&Y_1D^*+Y_0(1-D^*)\\ \\
     D^*&=&\mathbbm{1}\left\{V\leq P(Z)\right\}\\ \\
     D&=&D^* (1-\varepsilon) +(1-D^*)	 \varepsilon
     \end{array} \right.
\end{eqnarray}
where the vector $(Y,D,Z)$ represents the observed data, while the vector $(Y_1, Y_0, V, D^*,\varepsilon)$ is latent. In this model, $Y \in \mathcal Y$ is the observed outcome, $D^*\in \left\{0,1\right\}$ is the unobserved true treatment variable, while $D\in \left\{0,1\right\}$ is the observed mismeasured treatment, $\varepsilon \in \left\{0,1\right\}$ is an indicator for misreporting, $Y_0$ and $Y_1$ are the potential outcomes that would have been observed if the true treatment $D^*$ had been externally set to 0 and 1, respectively. The variable $Z \in \mathcal Z$ is an instrument, and $P(.)$ is a nontrivial/nonconstant function. In this paper, we are interested in identifying the marginal treatment effect defined as 
$$MTE(p)\equiv \mathbb E[Y_1-Y_0\vert V=p].$$

\begin{example}[Marginal returns to schooling (leading example)]\label{ex1}
In this example, we assume that the researcher is interested in measuring marginal returns to college education. It is well-documented that education is usually mismeasured. For example, \cite{Black_etal2003} find that more than a third of respondents to the U.S. Census claiming to hold a professional degree have no such  degree. In this case, the variable $Y$ is earnings/wage, and $D$ is the indicator for college degree. The variable $Z$ could be distance to college. The latent variable $V$ could be interpreted as an index for the cost of going to college (which includes the financial cost, the opportunity cost, the psychological cost, etc.), while $Y_1$ is the potential earnings for someone with a college degree, and $Y_0$ is the potential earnings for someone without a college degree. The variable $D^*$ is the individual's true indicator for college degree. 
\end{example}

\begin{example}[Marginal effects of masks]\label{ex2}
The variable $Y$ could be the indicator that an individual tests positive to Covid-19, $D^*$ the indicator that the individual actually wears masks, $D$ the indicator that the individual reports wearing masks, $V$ the disutility/discomfort (cost) of wearing masks, and $Z$ could be a shifter for the benefit of wearing masks (number of children).\footnote{Children are less likely to get the virus, and therefore less likely to contaminate others. Hence, the variable number of children in the households is likely to satisfy the exclusion restriction assumption. We may add the number of adults in the households as a control variable to the model. However, the validity of number of children as instrument remains questionable, as is the distance to college instrument in Example~\ref{ex1}.} People could report wearing masks while they actually do not (for example, because of social pressure). They could also pretend to wear mask while they do not wear it properly. On the other hand, someone could report not wearing regularly a mask (because of political reasons for example), while she actually does (because of her underlying health conditions). These facts could lead to misclassification in the report of mask wearing, and therefore induce some bias in the measurement of the marginal effects of mask wearing on the positivity rate.  $Y_1$ could be the indicator that the individual tests positive to Covid-19 while she is wearing masks, and $Y_0$ could be the indicator that the individual tests positive to Covid-19 while she is not wearing masks. The effect of wearing masks on the positivity rate could be heterogeneous. Healthy individuals tend to think that they are immune or they will survive if they are infected. For them, the disutility of wearing masks may be higher, and they may be less likely to wear masks. We are interested in measuring the effect of wearing masks on the positivity rate for different levels of the disutility, i.e., $\mathbb E\left[Y_1-Y_0 \vert V=p\right]$. 
\end{example}

We will use the following assumptions for identification:
\begin{assumption}[Random assignment]\label{RA}
The instrument $Z$ is independent of $\left(Y_d,V,\varepsilon\right)$, i.e., $Z\ \indep\ \left(Y_d,V,\varepsilon\right)$, for each $d\in \left\{0,1\right\}$.
\end{assumption}
Assumption \ref{RA} requires that $Z$ be a valid instrument, in the sense that it is statistically independent of all the unobservables in the model. This is a commonly used assumption in the literature. 
Note that the model (\ref{seq1}) implicitly assumes that exclusion restriction holds, i.e., $Y_{dz}=Y_d$ for all $d$ and $z$. For this reason, we do not state this assumption explicitly. Assumption \ref{RA} requires more than the standard random assignment assumption when there is no misclassification (i.e., $\varepsilon=0$ a.s.), which states that $Z$ is statistically independent of $(Y_d,V)$ for each treatment arm $d$. Assumption \ref{RA} extends the standard random assignment assumption to include the misclassification variable $\varepsilon$. Strictly speaking, we should have called this assumption ``\textit{extended random assignment}.'' We are abusing notation by simply calling it \textit{random assignment}. As discussed in \cite{Possebom2021}, the independence between the instrument $Z$ and the misclassification variable $\varepsilon$ could be too restrictive in practice. We discuss how this assumption can be relaxed in Appendix \ref{apx:zeps}. Note however that this assumption has been considered in existing work such as \cite{Ura2018}, \cite{Calvi_al2018}, \cite{Tommasi2020}, etc.

In our framework, the measurement error is nonclassical by definition of the model. Indeed, we can rewrite $D=D^*+(1-2D^*)\varepsilon$. So, the measurement error $(1-2D^*)\varepsilon$ is dependent on the true unobserved treatment $D^*$. This fact is well-documented and understood in the literature. See \cite{Aigner1973}, \cite{Mahajan2006}, \cite{Lewbel2007}, \cite{Kreideral2012}, \cite{Ura2018}, \cite{Yanagi2019}, etc. 

\begin{assumption}[Absolute continuity of $V$]\label{Cont}
The latent variable $V$ is absolutely continuous. Without loss of generality, the unconditional distribution of $V$ is uniform over $[0,1]$, and the support of the function $P(z)$ is included in $[0,1]$.
\end{assumption}
This assumption is standard in the literature and has been considered in \cite{heckman1999,heckman2001,heckman2005structural}, \cite{carneirolee2009}, \cite{checkman2010,heckman2011}, etc. It does not require that the conditional density of $V$ given $\varepsilon$ exists, as will be apparent in the different specifications we consider in the appendix. This assumption implies the following:
\begin{eqnarray}\label{eq:mix}
\mathbb P(\varepsilon=1)F_{V\vert \varepsilon=1}(p)+\mathbb P(\varepsilon=0)F_{V\vert \varepsilon=0}(p)=p\ \text{ for all } p\in[0,1],
\end{eqnarray}
where $F_{V\vert \varepsilon}$ denotes the conditional distribution of $V$ given $\varepsilon$.

\begin{assumption}[Continuous instrument]\label{Cont_inst}
The instrument $Z$ is continuous such that the support of the random variable $P(Z)$ is an interval.%
\end{assumption}
Assumption \ref{Cont_inst} is also standard in the literature and is crucial for our identification methodology for the MTE.  In Section \ref{ext:discrete}, we show how our methodology can be used to identify multiple LATEs when the instrument is discrete. Identification results for the MTE with discrete instruments have been developed in \cite{Acerenza2021}, which built on insights from the current paper and \cite{Mogstadal2018}.

\begin{assumption}[Upper bound on misclassification rate]\label{Bound:mis}
The (unconditional) misclassification rate $\alpha \equiv \mathbb P(\varepsilon=1)$ has a known upper bound $\bar{\alpha}$, that is, $\alpha \in [0,\bar{\alpha}]$. 
\end{assumption}
A similar assumption to Assumption \ref{Bound:mis} has been considered in \cite{Horowitz1995}, \cite{Kreideral2007}, \cite{Molinari2008}, \cite{Kreideral2012}, etc. In this assumption, we only impose an upper bound on the extent of the misclassification, as we allow for the treatment to be correctly classified. One can alternatively place a lower bound on the misclassification probability too. For example, one can combine information from different sources (e.g., government, universities, etc.) to bound the extent of the misclassification from below as well as from above. For instance, universities can provide information on the number of individuals who actually have a college degree. This information can help identify $\mathbb P(D^*=1)$. One can then use the absolute difference between $\mathbb P(D^*=1)$ and $\mathbb P(D=1)$ as $\underline{\alpha}$, a lower bound for the misclassification rate.\footnote{Indeed, we have $D=D^*+(1-2D^*)\varepsilon$, which implies $\mathbb E[D-D^*]=\mathbb E[(1-2D^*)\varepsilon]=\mathbb E[(1-2D^*)\vert \varepsilon=1]\mathbb P(\varepsilon=1)$. Therefore, $\vert \mathbb E[D-D^*] \vert= \vert \mathbb E[(1-2D^*) \vert \varepsilon=1]\vert \mathbb P(\varepsilon=1)\leq \mathbb P(\varepsilon=1)$, since $1-2D^* \in \{-1,1\}$. Hence, we can set $\underline{\alpha}=\vert \mathbb E[D-D^*] \vert=\vert \mathbb P(D=1)-\mathbb P(D^*=1)\vert$.} We can also use this information to provide a value for $\bar{\alpha}$.\footnote{We have $\mathbb P(\varepsilon=1)=\mathbb P(\varepsilon=1,D=1)+\mathbb P(\varepsilon=1,D=0)=\mathbb P(\varepsilon=1\vert D=1)\mathbb P(D=1)+\mathbb P(\varepsilon=1,D^*=1)=\mathbb P(\varepsilon=1\vert D=1)\mathbb P(D=1)+\mathbb P(\varepsilon=1\vert D^*=1)\mathbb P(D^*=1)$. If $\mathbb P(\varepsilon=1\vert D=1) \leq \bar{\alpha}_0$, and $\mathbb P(\varepsilon=1\vert D^*=1) \leq \bar{\alpha}_1$, then we can set $\bar{\alpha}=\bar{\alpha}_0\mathbb P(D=1)+\bar{\alpha}_1\mathbb P(D^*=1)$. In the context of example \ref{ex1}, we can set $\bar{\alpha}_0=1/3$ (following \cite{Black_etal2003}), and $\bar{\alpha}_1=0$ (assuming someone with a degree is unlikely to misreport).}
The case $\bar{\alpha}=1$ corresponds to the scenario where the researcher is agnostic about the range of the misclassification rate. All our derived results still hold in this case. 

\section{Identification Results}\label{Ident}
\subsection{Identification of the MTE} 
We have
\begin{eqnarray}
\mathbb E\left[Y\vert P(Z)=p\right] &=& \mathbb E\left[Y_1 D^* + Y_0 (1-D^*) \vert P(Z)=p\right], \nonumber \\
&=& \mathbb E\left[Y_1 \mathbbm{1}\left\{V \leq p\right\} + Y_0 \mathbbm{1}\left\{V > p\right\}\right], \nonumber \\
&=& \int^{p}_0\mathbb E\left[Y_1 \vert V=v\right]dv + \int_{p}^1\mathbb E\left[Y_0 \vert V=v\right]dv, \label{eq:mte}
\end{eqnarray}
where the first equality holds from the definition of the model, the second holds from Assumption \ref{RA}, and the third equality holds from Assumption \ref{Cont}. 
Under Assumption \ref{Cont_inst}, we can differentiate each side of the equation with respect to $p$. Hence, we obtain
\begin{eqnarray*}
\frac{\partial \mathbb E[Y \vert P(Z)=p] }{\partial p}= \mathbb E\left[Y_1 - Y_0 \vert V=p\right].
\end{eqnarray*}
Below, we summarize this result in Lemma \ref{thm1}.
\begin{lemma}\label{thm1}
Suppose that model (\ref{seq1}) along with Assumptions \ref{RA}--\ref{Cont_inst} hold. Then, the marginal treatment effect is identified as 
\begin{eqnarray*}
MTE(p)= \frac{\partial \mathbb E[Y \vert P(Z)=p] }{\partial p},
\end{eqnarray*}
where the function $P(z)$ is partially identified as explained below.
\end{lemma}
The result in Lemma \ref{thm1} shows that the marginal treatment still has the local instrumental variable (LIV) interpretation as in \cite{heckman1999,heckman2001,heckman2005structural}, except that the true propensity score $P(z)$ is now set-identified, because of the presence of misclassification. The intuition is that the conditional expectation $\mathbb E[Y\vert P(Z)=p]$ can be decomposed between the treatment and control groups as usual, even if these groups are unobserved due to the presence of misclassification. 
The above result holds whether there is misclassification or not, since the model assumes that misreporting does not have a direct effect on the outcome variable. Now, the fact that the treatment and control groups are observed with some noise leaves the propensity score $P(z)$ partially identified. 

\subsection{Identification of $P(z)$}
For any Borel set $A$, we have 
\begin{eqnarray}
\mathbb P(Y\in A, D=1 \vert Z=z)&=&\mathbb P(Y\in A, D=1, D^*=1 \vert Z=z) + \mathbb P(Y\in A, D=1, D^*=0 \vert Z=z),\nonumber\\
&=& \mathbb P(Y_1\in A, \varepsilon=0, V \leq P(z)) + \mathbb P(Y_0\in A, \varepsilon=1, V > P(z)), \label{eq1}
\end{eqnarray}
where the first equality holds from the law of total probability, and the second equality follows from the definition of the model and Assumption \ref{RA}. In the special case where~$A=\mathcal Y$, we have
\begin{eqnarray}\label{eq:mis0}
\mathbb P(D=1 \vert Z=z)&=& \mathbb P(\varepsilon=0, V \leq P(z)) + \mathbb P(\varepsilon=1, V > P(z)).
\end{eqnarray}
When there is no misclassification in the treatment, i.e., $\varepsilon=0$ a.s., then $P(z)$ is identified as the propensity score $\mathbb P(D=1 \vert Z=z)$, since the distribution of $V$ is normalized to be uniform over $[0,1]$. When the treatment is \textit{completely} misclassified, i.e., $\varepsilon=1$ a.s., $P(z)$ is identified under the previous normalization as $\mathbb P(D=0 \vert Z=z)$. We can rewrite the last equality as follows: 
\begin{eqnarray}
\mathbb P(D=1 \vert Z=z)&=& (1-\alpha) F_{V\vert \varepsilon=0}(P(z)) + \alpha (1-F_{V\vert \varepsilon=1}(P(z))). \label{eq2}
\end{eqnarray}

We have $\mathbb P(D^*=1\vert Z=z)=\mathbb P(V \leq P(z))=P(z)$. Thus, $P(z)$ is the true (unidentified) propensity score.
We show that the propensity score $P(z)$ is partially identified using Equations (\ref{eq:mix}) and (\ref{eq2}). Equation (\ref{eq2}) implies
\begin{eqnarray*}
\mathbb P(D=1 \vert P(Z)=p)&=& (1-\alpha) F_{V\vert \varepsilon=0}(p) + \alpha (1-F_{V\vert \varepsilon=1}(p)).
\end{eqnarray*}
For now, we assume $\alpha \in (0,1)$, since the cases where $\alpha\in \{0,1\}$ can be dealt with separately. %
Combining this with Equation (\ref{eq:mix}), and solving for $F_{V\vert \varepsilon=0}(p)$ and $F_{V\vert \varepsilon=1}(p)$ in the system of equations, we obtain:
\begin{eqnarray*}
F_{V\vert \varepsilon=1}(p)&=& \frac{p+\alpha-\mathbb P(D=1\vert P(Z)=p)}{2 \alpha},\\
F_{V\vert \varepsilon=0}(p)&=& \frac{p-\alpha+\mathbb P(D=1\vert P(Z)=p)}{2(1-\alpha)}.
\end{eqnarray*}
Therefore, the above functions need to satisfy all required conditions for a cumulative distribution on $[0,1]$: monotonicity, right-continuity, $F_{V\vert \varepsilon=1}(0)=F_{V\vert \varepsilon=0}(0)=0$, and $F_{V\vert \varepsilon=1}(1)=F_{V\vert \varepsilon=0}(1)=1$. In general, it will be difficult to nonparametrically characterize the sharp identification region for the propensity score function $P(z)$ using those conditions. We are going to focus on the monotonicity condition and the fact that the probabilities $F_{V\vert \varepsilon=1}(P(z))$ and $F_{V\vert \varepsilon=0}(P(z))$ lie between 0 and 1. For any $z$ and $z'$ such that $P(z') <P(z)$, we have:
\begin{eqnarray*}
0 &\leq& F_{V\vert \varepsilon=0}(P(z))- F_{V\vert \varepsilon=0}(P(z')) \leq 1,\\
0 &\leq& F_{V\vert \varepsilon=1}(P(z))- F_{V\vert \varepsilon=1}(P(z')) \leq 1.
\end{eqnarray*}
which implies
\begin{eqnarray*}
0 &\leq& \frac{\mathbb P(D=1 \vert Z=z)-\mathbb P(D=1 \vert Z=z')+P(z)-P(z')}{2(1-\alpha)} \leq 1,\\
0 &\leq& \frac{P(z)-P(z')-\mathbb P(D=1 \vert Z=z)+\mathbb P(D=1 \vert Z=z')}{2\alpha} \leq 1.
\end{eqnarray*}
This latter inequalities respectively imply
\begin{eqnarray}
-\mathbb P(D=1 \vert Z=z)+\mathbb P(D=1 \vert Z=z') &\leq& P(z)-P(z') \label{eq:mtecont1}\\
&& \leq 2(1-\alpha)-\mathbb P(D=1 \vert Z=z)+\mathbb P(D=1 \vert Z=z'),\nonumber\\
\mathbb P(D=1 \vert Z=z)-\mathbb P(D=1 \vert Z=z') &\leq& P(z)-P(z') \label{eq:mtecont2}\\
&& \leq 2 \alpha + \mathbb P(D=1 \vert Z=z)-\mathbb P(D=1 \vert Z=z'). \nonumber
\end{eqnarray}
In the special case where $P(z')=0$, we have
\begin{eqnarray*}
-\mathbb P(D=1 \vert Z=z)+\mathbb P(D=1 \vert P(Z)=0) &\leq& P(z)\\
&& \leq 2(1-\alpha)-\mathbb P(D=1 \vert Z=z)+\mathbb P(D=1 \vert P(Z)=0),\\
\mathbb P(D=1 \vert Z=z)-\mathbb P(D=1 \vert P(Z)=0) &\leq& P(z)\\
&& \leq 2 \alpha + \mathbb P(D=1 \vert Z=z)-\mathbb P(D=1 \vert P(Z)=0).
\end{eqnarray*}
Using the condition that $F_{V\vert \varepsilon=1}(0)=0$, we identify $\mathbb P(D=1 \vert P(Z)=0)=\alpha$. Therefore, the above constraints on $P(z)$ become
\begin{eqnarray*}
\begin{array}{lcccl}
\alpha-\mathbb P(D=1 \vert Z=z) &\leq& P(z) &\leq& 1-\alpha+\mathbb P(D=0 \vert Z=z),\\ \\
\mathbb P(D=1 \vert Z=z)-\alpha &\leq& P(z) &\leq&  \alpha + \mathbb P(D=1 \vert Z=z).
\end{array}
\end{eqnarray*}
Similar argument holds for the special case where $P(z)=1$, but this yields the above same constraints on $P(z')$.
Hence, the following proposition holds. 
\begin{proposition}\label{prop1}
Suppose that model (\ref{seq1}) along with Assumptions \ref{RA}, \ref{Cont}, and \ref{Bound:mis} hold. We have the following bounds on $P(z)$: $LB(z) \leq P(z) \leq UB(z)$, where 
\begin{eqnarray*}
\begin{array}{lcccl}
LB(z)\equiv \inf_{\alpha \in [0, \bar{\alpha}]}\max\left\{\mathbb P(D=1\vert Z=z)-\alpha, \alpha-\mathbb P(D=1\vert Z=z)\right\},\\ \\
UB(z)\equiv \sup_{\alpha \in [0, \bar{\alpha}]} \min\left\{\mathbb P(D=1\vert Z=z)+\alpha, (1-\alpha)+\mathbb P(D=0\vert Z=z)\right\}.
\end{array}
\end{eqnarray*}
These bounds are pointwise sharp. 
\end{proposition}
In Appendix \ref{proofprop1}, we provide two different specifications for the relationship between the decision to misreport $\varepsilon$ and the unobserved heterogeneity $V$ that achieve the above bounds on $P(z)$. However, these bounds are not necessarily \textit{functionally sharp} in the language of \cite{Mourifie2020}, as taking the difference of the bounds for $P(z)$ and $P(z')$ will not necessarily yield the tightest bounds for the difference $P(z)-P(z')$. We show this in Subsection \ref{anabounds} below. Intuitively, note that the pointwise bounds on $P(z)$ in Proposition \ref{prop1} are derived using only information from the first stage equation. We are going to use information from the second stage equation to tighten the bounds on $P(z)-P(z')$.  

\begin{remark}
The bounds in Proposition \ref{prop1} are non-informative if $\bar{\alpha}=1$. Indeed, if $\alpha=\mathbb P(D=1\vert Z=z)$, then the lower bound on $P(z)$ is 0, and if $\alpha=\mathbb P(D=0\vert Z=z)$, then the upper bound on $P(z)$ is 1. Also, note that the bounds on $P(z)$ are monotonic in $\bar{\alpha}$: bigger values of $\bar{\alpha}$ yield wider bounds, while smaller values of $\bar{\alpha}$ lead to narrower bounds.
\end{remark}

For the rest of the paper, we derive our results for each value of $\alpha \in [0,\bar{\alpha}]$. As in Proposition \ref{prop1}, one can take the infimum of the lower bound on the parameter of interest over the range $[0,\bar{\alpha}]$, and similarly the supremum of the upper bound over $[0,\bar{\alpha}]$.  

\subsection{Analytical bounds for the MTE}\label{anabounds}
In this subsection, we provide analytical bounds on the MTE. 
Define $\Delta_{YZ}(z',z)\equiv \mathbb E[Y\vert Z=z]-\mathbb E[Y\vert Z=z']$ and $\Delta_{DZ}(z',z)\equiv \mathbb E[D\vert Z=z]-\mathbb E[D\vert Z=z']$. Inequalities (\ref{eq:mtecont1}) and (\ref{eq:mtecont2}) imply the following bounds on the $P(z)-P(z')$ when $0\leq P(z') < P(z)\leq 1$.
\begin{eqnarray*}
\left \lvert \Delta_{DZ}(z',z)\right \rvert \leq P(z)-P(z') \leq \min\left\{1,2\alpha+\Delta_{DZ}(z',z), 2(1-\alpha)-\Delta_{DZ}(z',z)\right\}. 
\end{eqnarray*}
These above bounds on the difference $P(z)-P(z')$ can be tightened using Equations \eqref{eq:fsharp2} and \eqref{eq:fsharp3} below. 
Notice that the model implies the following index sufficiency result:
\begin{eqnarray}
\mathbb P\left(Y\in A, D=d\vert P(Z)=P(z)\right)=\mathbb P\left(Y\in A, D=d\vert Z=z\right) \text{ for all  $z$ and $d$.} \label{eq:fsharp1}
\end{eqnarray} 
Indeed, similar to Equation (\ref{eq1}), the following holds under Assumption \ref{RA}:
\begin{eqnarray*}
\mathbb P(Y\in A, D=1 \vert P(Z)=P(z)) &=& \mathbb P(Y_1\in A, \varepsilon=0, V \leq P(z)) + \mathbb P(Y_0\in A, \varepsilon=1, V > P(z)).
\end{eqnarray*}
From Equation (\ref{eq1}), we can show under Assumptions \ref{RA} and \ref{Cont} that 
\begin{eqnarray}
\mathbb P(Y\in A, D=1 \vert Z=z) 
&=& \int^{P(z)}_0\mathbb P\left(Y_1\in A, \varepsilon=0 \vert V=v\right)dv \nonumber \\
&& \qquad \qquad + \int_{P(z)}^1\mathbb P\left(Y_0\in A, \varepsilon=1 \vert V=v\right)dv. \label{eq:fsharp2}
\end{eqnarray}
A similar result holds for the observed control group:
\begin{eqnarray}
\mathbb P(Y\in A, D=0 \vert Z=z) &=& \int^{P(z)}_0\mathbb P\left(Y_1\in A, \varepsilon=1 \vert V=v\right)dv \nonumber \\
&& \qquad \qquad + \int_{P(z)}^1\mathbb P\left(Y_0\in A, \varepsilon=0 \vert V=v\right)dv. \label{eq:fsharp3}
\end{eqnarray}
The above derived equalities allow us to characterize the functional identified set for $P(z)$.
\begin{definition}
The identified set for the function $P: \mathcal Z \rightarrow [0,1]$ is the collection $$\left\{P(z): \ 0\leq P(z) \leq 1,\ z \in \mathcal Z\right\}$$ such that there exists a joint distribution on $(Y_0, Y_1, \varepsilon, V, Z)$ that satisfies model (\ref{seq1}),  Assumptions \ref{RA}, \ref{Cont}, \ref{Bound:mis}, and Equations (\ref{eq:fsharp1})--(\ref{eq:fsharp3}).
\end{definition}
This characterization of the identified set for $P(z)$ is broad, but less tractable. We are going to derive analytical expressions for the MTE bounds based on the previous results. 

The density versions of Equations \eqref{eq:fsharp2} and \eqref{eq:fsharp3} hold. We first consider Equation \eqref{eq:fsharp2}. For two values $z$ and $z'$ of the instrument $Z$, we have


 
\begin{eqnarray*}
f_{Y,D\vert Z}\left(y, 1 \vert z\right) - f_{Y,D\vert Z}\left(y, 1 \vert z'\right)&=& \int^{P(z)}_{P(z')}f_{Y_1,\varepsilon\vert V}(y,0\vert v)dv\\
&&\qquad \qquad - \int^{P(z)}_{P(z')}f_{Y_0,\varepsilon\vert V}(y,1\vert v)dv,
\end{eqnarray*}
where $f_{X\vert W}\left(x \vert w\right)$ is the conditional density of $X$ given $\left\{W=w\right\}$ that is absolutely continuous with respect to a known dominating measure $\mu_{X}$.
Using the triangle inequality, we have
\begin{eqnarray*}
\left \lvert f_{Y,D\vert Z}\left(y, 1 \vert z\right) - f_{Y,D\vert Z}\left(y, 1 \vert z'\right) \right \rvert  &\leq&  \int^{P(z)}_{P(z')}f_{Y_1,\varepsilon\vert V}(y,0\vert v)dv\\
 && \qquad + \int^{P(z)}_{P(z')}f_{Y_0,\varepsilon\vert V}(y,1\vert v)dv.
\end{eqnarray*}
Therefore, by integrating each side of the last inequality over the support $\mathcal Y$, and using the Fubini-Tonelli theorem, we have
\begin{eqnarray*}
\int_{\mathcal Y}\left \lvert f_{Y,D\vert Z}\left(y, 1 \vert z\right) - f_{Y,D\vert Z}\left(y, 1 \vert z'\right) \right \rvert d \mu_{Y}(y) &\leq&  \int^{P(z)}_{P(z')}\mathbb P(\varepsilon=0\vert V=v)dv+\int^{P(z)}_{P(z')}\mathbb P(\varepsilon=1\vert V=v)dv,\\
&=& \int^{P(z)}_{P(z')}dv= P(z)-P(z').
\end{eqnarray*}
Hence, we have $TV_{(Y,D=1)}(z',z) \leq P(z)-P(z')$, where 
\begin{eqnarray*}
TV_{(Y,D=d)}(z',z) \equiv \int_{\mathcal Y}\left \lvert f_{Y,D\vert Z}\left(y, d \vert z\right) - f_{Y,D\vert Z}\left(y, d \vert z'\right) \right \rvert d \mu_{Y}(y).
\end{eqnarray*}
Using a similar argument on Equation \eqref{eq:fsharp3}), we have $TV_{(Y,D=0)}(z',z) \leq P(z)-P(z')$.
Therefore, we obtain the following bounds on the difference $P(z)-P(z')$:
\begin{eqnarray*}
&&\max\left\{\left \lvert \Delta_{DZ}(z',z)\right \rvert, TV_{(Y,D=1)}(z',z), TV_{(Y,D=0)}(z',z)\right\}\\
 && \qquad \qquad \qquad \leq P(z)-P(z') \leq \min\left\{1,2\alpha+\Delta_{DZ}(z',z), 2(1-\alpha)-\Delta_{DZ}(z',z)\right\}. 
\end{eqnarray*}
We can show that $\max\left\{TV_{(Y,D=1)}(z',z), TV_{(Y,D=0)}(z',z) \right\} \geq \left \lvert \Delta_{DZ}(z',z)\right \rvert$.\footnote{To show this, use the fact that for any $\mu$-integrable function $h$, $\left \lvert \int_{\mathcal Y} h(y) d \mu_{Y}(y) \right \rvert \leq \int_{\mathcal Y} \left \lvert h(y) \right \rvert d \mu_{Y}(y)$, and $\int_{\mathcal Y} f_{Y,D\vert Z}\left(y, d \vert z\right) d \mu_{Y}(y) = \mathbb P(D=d\vert Z=z)$.} Consequently, we have
\begin{eqnarray}\label{eq:pzbounds}
&&\max\left\{TV_{(Y,D=1)}(z',z), TV_{(Y,D=0)}(z',z)\right\}\nonumber\\
 && \qquad \leq P(z)-P(z') \leq \min\left\{1,2\alpha+\Delta_{DZ}(z',z), 2(1-\alpha)-\Delta_{DZ}(z',z)\right\}. 
\end{eqnarray}
These above bounds on $P(z)-P(z')$ are tighter than the ones one would get by taking the difference of the pointwise bounds derived previously in Proposition \ref{prop1}. 

Define
\begin{eqnarray*}
LB_p(z',z) &\equiv& \max\left\{TV_{(Y,D=1)}(z',z), TV_{(Y,D=0)}(z',z)\right\},\\
UB_p(z',z) &\equiv& \min\left\{1,2\alpha+\Delta_{DZ}(z',z), 2(1-\alpha)-\Delta_{DZ}(z',z)\right\}.
\end{eqnarray*} 
Suppose $LB_p(z',z) \neq 0$ and $UB_p(z',z) \neq 0$. Then, the following holds. 
\begin{eqnarray*}
\left\{ \begin{array}{lcl}
     \frac{\Delta_{YZ}(z',z)}{UB_p(z',z)} \leq \frac{\Delta_{YZ}(z',z)}{P(z)-P(z')} \leq \frac{\Delta_{YZ}(z',z)}{LB_p(z',z)}\ \text{ if }\ \Delta_{YZ}(z',z) \geq 0,&&\\ \\
      \frac{\Delta_{YZ}(z',z)}{LB_p(z',z)} \leq \frac{\Delta_{YZ}(z',z)}{P(z)-P(z')}  \leq \frac{\Delta_{YZ}(z',z)}{UB_p(z',z)}\ \text{ if }\ \Delta_{YZ}(z',z) < 0.&&
     \end{array} \right.
\end{eqnarray*}
Hence, we have
\begin{eqnarray*}
&& \min\left\{\frac{\Delta_{YZ}(z',z)}{UB_p(z',z)},  \frac{\Delta_{YZ}(z',z)}{LB_p(z',z)}\right\}\\
&& \qquad \qquad \leq \frac{\mathbb E[Y\vert P(Z)=P(z)]-\mathbb E[Y\vert P(Z)=P(z')]}{P(z)-P(z')} \leq \\
&& \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \max\left\{\frac{\Delta_{YZ}(z',z)}{UB_p(z',z)},  \frac{\Delta_{YZ}(z',z)}{LB_p(z',z)}\right\}.
\end{eqnarray*}
Therefore, we can take the limit of each side when $z'$ goes to $z$. Suppose that $\lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{UB_p(z',z)}$, $\lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{LB_p(z',z)}$, and $\lim_{z' \rightarrow z} \frac{\mathbb E[Y\vert P(Z)=P(z)]-\mathbb E[Y\vert P(Z)=P(z')]}{P(z)-P(z')}$ exist.\footnote{Then, we have $\lim_{z' \rightarrow z} \frac{\mathbb E[Y\vert P(Z)=P(z)]-\mathbb E[Y\vert P(Z)=P(z')]}{P(z)-P(z')}=\frac{\partial \mathbb E[Y \vert P(Z)=p] }{\partial p}\vert_{p=P(z)}=MTE(P(z))$. When the first two limits do not exist, we replace them by $\lim\inf$ and $\lim\sup$ in the lower and upper bounds of (\ref{eq:anabounds}).} Then, using %
the fact that the functions $\min$ and $\max$ are continuous, and assuming that $P(z)$ is continuous in $z$, we obtain
\begin{eqnarray}
&& \min\left\{\lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{UB_p(z',z)},  \lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{LB_p(z',z)}\right\} \nonumber\\
&& \qquad \qquad \leq MTE(P(z))\leq \label{eq:anabounds}\\
&& \qquad \qquad \qquad \qquad \max\left\{\lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{UB_p(z',z)},  \lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{LB_p(z',z)}\right\}. \nonumber
\end{eqnarray}
These bounds may not be sharp, but they provide a \textit{tractable outer set} of the identified set for $MTE(P(z))$. In practice, we may set $P(z)$ to be equal to the midpoint of its bounds derived in the previous subsection. We summarize the results in the following proposition.

\begin{proposition}\label{prop:mtebounds}
Suppose that model (\ref{seq1}) along with Assumptions \ref{RA}, \ref{Cont}, \ref{Cont_inst} and \ref{Bound:mis} hold. Also, suppose that $P(z)$ is continuous in $z$ and $\lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{UB_p(z',z)}$, $\lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{LB_p(z',z)}$, and $\lim_{z' \rightarrow z} \frac{\mathbb E[Y\vert P(Z)=P(z)]-\mathbb E[Y\vert P(Z)=P(z')]}{P(z)-P(z')}$ exist. The following statements hold:
\begin{enumerate}[(i)]
\item If $\bar{\alpha}=0$, then $MTE(P(z))$ is point-identified as
\begin{eqnarray}\label{eq:late}
\lim_{z' \rightarrow z} \frac{\mathbb E[Y\vert Z=z]-\mathbb E[Y\vert Z=z']}{\mathbb E[D\vert Z=z]-\mathbb E[D\vert Z=z']}.
\end{eqnarray}
\item If $\bar{\alpha} > 0$, then $MTE(P(z))$ is partially-identified:
\begin{eqnarray}\label{eq:mtebounds}
\min\left\{0,  \lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{LB_p(z',z)}\right\} \leq MTE(P(z))\leq \max\left\{0,  \lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{LB_p(z',z)}\right\}. 
\end{eqnarray}
\end{enumerate}
\end{proposition}
The proof on Proposition \ref{prop:mtebounds} is shown in Appendix \ref{proof:mtebounds}. This proposition shows that when there is no misclassification ($\bar{\alpha}=0$), our bounds collapse to a point, which is the standard MTE estimand as a limit of a generalized LATE.  When $\bar{\alpha} >0$, the sign of the $MTE$ is \textit{locally identified} for each value $z$. Equation \eqref{eq:mtebounds} shows that our derived bounds are not changing with $\bar{\alpha} >0$. In particular, the bounds in Equation \eqref{eq:mtebounds} remain valid even when the researcher is agnostic about the misclassification probability ($\bar{\alpha}=1$).  The numerical example below illustrates how informative the bounds can be in practice, depending on the underlying structure in the data generating process. 

\subsection*{Numerical illustration} \label{numeric1}
We consider the following data generating process (DGP)
\begin{eqnarray}\label{eq:ex}
\left\{ \begin{array}{lcl}
     Y &=& \beta D^*+U \\ \\
     D^* &=& \mathbbm{1}\left\{V \leq \Phi(2 Z)\right\} \\ \\
     D &=& D^*(1-\varepsilon)+(1-D^*)\varepsilon \\ \\
     \varepsilon &=& \mathbbm{1}\left\{\xi \leq \alpha \right\} 
     \end{array} \right.
\end{eqnarray}
where $V=\Phi(V^*)$, $\xi=\Phi(\xi^*)$ , $\begin{pmatrix}
\beta \\ U\\V^*\\ \xi^* \\Z 
\end{pmatrix} \sim \mathcal N(\mu,\Sigma)$, with
$\mu=\begin{pmatrix}
2\\2\\0\\ 0 \\ 2
\end{pmatrix}$, and 
$\Sigma=
\begin{pmatrix}
1 & 0.5&-0.5&0.5& 0  \\ 
 0.5 & 1 & 0.5&0.5& 0\\
 -0.5 & 0.5 & 1&\rho & 0\\
 0.5 & 0.5 & \rho & 1& 0\\
 0 & 0 & 0 & 0 & 1
\end{pmatrix}$.
Details on the DGP are provided in Subsection \ref{numeric:apx1} in the appendix.
As we can see on Figure \ref{fig.ey1.exam0921}, the upper bound becomes closer to the true MTE as the misclassification rate $\alpha$ approaches 0 or 1. The MTE is positive for all values of $p$ in this example, and our bounds identify this sign. In some cases, our derived bounds appear tight and informative. The coefficient $\rho$ captures the degree of dependence between the unobserved heterogeneity $V$ and the indicator for misreporting $\varepsilon$. It is unclear how this dependence parameter $\rho$ affects the bounds.  

\begin{figure}[h]
    \includegraphics[width=\textwidth]{Numerical_Approx_10272021.png}\\
    \begin{flushleft}
	\footnotesize{* The black line represents the true MTE from the model.}\\
    \footnotesize{* The red circles (blue triangles) are the approximated upper (lower) bound at each grid point of $p$.}\\
    \end{flushleft}
    \centering \caption{Numerical illustration of the MTE bounds}
	\label{fig.ey1.exam0921}
\end{figure}

Figure \ref{fig.ey1.exam0921_r0} in Appendix \ref{numeric:apx1} displays the MTE bounds, the local IV estimand and the true MTE curve for $\rho=0$ and different values of $\alpha$. In most cases, we see that the standard LIV estimand does not lie within our derived bounds. Therefore, ignoring the presence of measurement errors in the treatment variable may result in an important bias in the MTE function. Our proposed bounds always cover the true MTE function, when our identifying assumptions holds.

\clearpage

\subsection{Sharp characterization under non-differential misclassification assumption} In this subsection, we are going to characterize the (sharp) identified set for the MTE. We add the non-differential measurement error assumption to the set of our identifying assumptions.
\begin{assumption}[Non-differential misclassification]\label{NDE}
The misclassification variable $\varepsilon$ is independent of $Y_d$ conditional on $V$, i.e., $\varepsilon\ \indep\ Y_d \vert V$, for each $d\in \left\{0,1\right\}$.
\end{assumption}
This assumption states that conditional on the unobserved heterogeneity that drives the selection into treatment, misreporting is independent of the potential outcomes. Combined with Assumption \ref{RA}, it implies that misreporting is independent of the outcome conditional on the true treatment. This assumption could be too restrictive, as there may exist some returns to misreporting. In our leading example, there could exist some ``returns to lying'' about college completion, as discussed in \cite{HuLewbel2012}, and \cite{DiTraglia2019}.

For the sake of simplicity, we assume in this subsection that the distribution of $V$ given $\varepsilon$ is absolutely continuous. We have 
\begin{eqnarray}
\mathbb P(Y\in A, D=1 \vert P(Z)=p)  &=& \mathbb P(Y_1\in A, \varepsilon=0, V \leq p) + \mathbb P(Y_0\in A, \varepsilon=1, V > p),\nonumber \\
&=& (1-\alpha) \int^{p}_0\mathbb P\left(Y_1\in A \vert V=v, \varepsilon =0\right)f_{V\vert \varepsilon=0}(v)dv\nonumber \\
&& + \alpha \int_{p}^1\mathbb P\left(Y_0\in A \vert V=v, \varepsilon =1\right)f_{V\vert \varepsilon=1}(v)dv. \label{eq:main}
\end{eqnarray}
where the first equality follows from the results derived in the previous subsection, and the second equality holds from the law of iterated expectations.
Therefore, by taking the derivatives of both sides of this equality with respect to $p$, we obtain the following:
\begin{eqnarray}
\frac{\partial \mathbb P(Y\in A, D=1 \vert P(Z)=p) }{\partial p}
&=& (1-\alpha) f_{V\vert \varepsilon=0}(p) \mathbb P\left(Y_1\in A \vert V=p, \varepsilon =0\right) \nonumber\\
&& - \alpha f_{V\vert \varepsilon=1}(p) \mathbb P\left(Y_0\in A \vert V=p, \varepsilon =1\right), \nonumber\\
&=& (1-\alpha) f_{V\vert \varepsilon=0}(p) \mathbb P\left(Y_1\in A \vert V=p\right) \nonumber\\
&&\qquad \qquad- \alpha f_{V\vert \varepsilon=1}(p) \mathbb P\left(Y_0\in A \vert V=p\right), \label{eq:mte1}
\end{eqnarray}
where the second equality holds under Assumption \ref{NDE}.
Similarly, we can show that 
\begin{eqnarray}
\frac{\partial \mathbb P(Y\in A, D=0 \vert P(Z)=p) }{\partial p} &=&  \alpha f_{V\vert \varepsilon=1}(p) \mathbb P\left(Y_1\in A \vert V=p\right) \nonumber\\
&& \qquad \qquad  - (1-\alpha) f_{V\vert \varepsilon=0}(p) \mathbb P\left(Y_0\in A \vert V=p\right). \label{eq:mte2}
\end{eqnarray}

Applying equality (\ref{eq:mte1}) to the special case where $A=\mathcal Y$, and using the fact that $f_{V}(p)=1$ (since $V\sim \mathcal U_{[0,1]}$), we have 
\begin{eqnarray*}
(1-\alpha) f_{V\vert \varepsilon=0}(p) - \alpha f_{V\vert \varepsilon=1}(p) &=& \frac{\partial \mathbb P(D=1 \vert P(Z)=p) }{\partial p},\\
(1-\alpha) f_{V\vert \varepsilon=0}(p) +\alpha f_{V\vert \varepsilon=1}(p)&=& 1.
\end{eqnarray*}
Therefore,
\begin{eqnarray*}
f_{V\vert \varepsilon=0}(p)  &=&\frac{1+ \frac{\partial \mathbb P(D=1 \vert P(Z)=p) }{\partial p}}{2(1-\alpha)},\\
f_{V\vert \varepsilon=1}(p)  &=&\frac{1- \frac{\partial \mathbb P(D=1 \vert P(Z)=p) }{\partial p}}{2\alpha}.
\end{eqnarray*}
Hence, the function $P$ must satisfy the following conditions:
\begin{eqnarray}
 && \frac{1+ \frac{\partial \mathbb P(D=1 \vert P(Z)=p) }{\partial p}}{2(1-\alpha)} \geq 0,\label{eq:P1}\\
&&  \frac{1- \frac{\partial \mathbb P(D=1 \vert P(Z)=p) }{\partial p}}{2\alpha} \geq 0, \label{eq:P2}\\
&& \int^1_0  \frac{1+ \frac{\partial \mathbb P(D=1 \vert P(Z)=p) }{\partial p}}{2(1-\alpha)} dp = 1,\label{eq:P3}\\
&& \int^1_0 \frac{1- \frac{\partial \mathbb P(D=1 \vert P(Z)=p) }{\partial p}}{2\alpha} dp = 1 \label{eq:P4},\\
&& 0 \leq \frac{\alpha f_{V\vert \varepsilon=1}(p)\kappa_0(A;p)-\left(1-\alpha\right)f_{V\vert \varepsilon=0}(p)\kappa_1(A;p)}{\alpha f_{V\vert \varepsilon=1}(p)-\left(1-\alpha\right)f_{V\vert \varepsilon=0}(p)} \leq 1, \label{eq:P5}\\
&& 0 \leq \frac{\left(1-\alpha \right) f_{V\vert \varepsilon=0}(p)\kappa_0(A;p)-\alpha f_{V\vert \varepsilon=0}(p)\kappa_1(A;p)}{\alpha f_{V\vert \varepsilon=1}(p)-\left(1-\alpha\right)f_{V\vert \varepsilon=0}(p)} \leq 1, \label{eq:P6}\\
&&\int^1_0 \left(1-\alpha \right) f_{V\vert \varepsilon=0}(p) \frac{\alpha f_{V\vert \varepsilon=1}(p)\kappa_0(A;p)-\left(1-\alpha\right)f_{V\vert \varepsilon=0}(p)\kappa_1(A;p)}{\alpha f_{V\vert \varepsilon=1}(p)-\left(1-\alpha\right)f_{V\vert \varepsilon=0}(p)} dp\nonumber\\
&& \qquad \qquad \qquad \mathbb = P(Y\in A, D=1\vert P(Z)=1), \label{eq:P8}%
\end{eqnarray}
\begin{eqnarray}
&&\int^1_0 \alpha f_{V\vert \varepsilon=1}(p) \frac{\left(1-\alpha \right) f_{V\vert \varepsilon=0}(p)\kappa_0(A;p)-\alpha f_{V\vert \varepsilon=0}(p)\kappa_1(A;p)}{\alpha f_{V\vert \varepsilon=1}(p)-\left(1-\alpha\right)f_{V\vert \varepsilon=0}(p)} dp\nonumber\\
&& \qquad \qquad \qquad \mathbb = P(Y\in A, D=1\vert P(Z)=0), \label{eq:P9}\\
&&\mathbb P\left(Y\in A, D=d\vert Z=z\right) = \mathbb P\left(Y\in A, D=d\vert P(Z)=P(z)\right), \label{eq:P7}
\end{eqnarray}
for all $p$ such that $\frac{\partial \mathbb P(D=1 \vert P(Z)=p) }{\partial p}\neq 0$, all $d\in\{0,1\}$, and all Borel set $A \subset \mathcal Y$, where $\kappa_1(A;p)=\frac{\partial \mathbb P(Y\in A, D=1 \vert P(Z)=p) }{\partial p}$, and $\kappa_0(A;p)=\frac{\partial \mathbb P(Y\in A, D=0 \vert P(Z)=p) }{\partial p}$. The constraints (\ref{eq:P5}) and (\ref{eq:P6}) come from the fact that $\mathbb P\left(Y_1\in A \vert V=p\right)$ and $\mathbb P\left(Y_0\in A \vert V=p\right)$ are probabilities. In fact, using equalities (\ref{eq:mte1}) and (\ref{eq:mte2})
we can solve for $\mathbb P\left(Y_1\in A \vert V=p\right)$ and $\mathbb P\left(Y_0\in A \vert V=p\right)$ as follows:
\begin{eqnarray*}
\mathbb P\left(Y_1\in A \vert V=p\right) &=&\frac{\alpha f_{V\vert \varepsilon=1}(p)\kappa_0(A;p)-\left(1-\alpha\right)f_{V\vert \varepsilon=0}(p)\kappa_1(A;p)}{\alpha f_{V\vert \varepsilon=1}(p)-\left(1-\alpha\right)f_{V\vert \varepsilon=0}(p)} ,\\
\mathbb P\left(Y_0\in A \vert V=p\right) &=& \frac{\left(1-\alpha \right) f_{V\vert \varepsilon=0}(p)\kappa_0(A;p)-\alpha f_{V\vert \varepsilon=0}(p)\kappa_1(A;p)}{\alpha f_{V\vert \varepsilon=1}(p)-\left(1-\alpha\right)f_{V\vert \varepsilon=0}(p)}.
\end{eqnarray*}
Equations (\ref{eq:P8}) and (\ref{eq:P9}) are like terminal conditions, and come from Equation (\ref{eq:main}).
Note that conditions (\ref{eq:P3}) and (\ref{eq:P4}) are equivalent, and come from the fact that density functions integrate to 1, while Equations (\ref{eq:P1}) and (\ref{eq:P2}) are the non-negativity conditions for density functions. 
The following proposition holds.
\begin{proposition}\label{prop:sharp}
Suppose that model (\ref{seq1}) along with Assumptions \ref{RA}--\ref{NDE} hold. In addition, suppose that the distribution of $V$ given $\varepsilon$ is absolutely continuous. For a given $\alpha \in [0,\bar{\alpha}]$, the constraints (\ref{eq:P1})--(\ref{eq:P7}) yield the (sharp) identified set for the function $P: \mathcal Z \rightarrow [0,1]$, and therefore for the MTE.
\end{proposition}
The proof of Proposition \ref{prop:sharp} is given in Appendix \ref{sharpnessproof}. Although this proposition provides sharp identification region for the propensity score $P(z)$ and the MTE, this identified set is not tractable. It is important to point out that the identified set for the MTE in the proposition is \textit{uniformly sharp} in the sense that the joint distribution on $(Y_1,Y_0,V,\varepsilon,Z)$ that achieves each element in this latter is the same across $p$. 

\section{How can knowledge about false positive/negative rates help?}\label{falserates}
\subsection{When false positive/negative misclassification probabilities do not depend on the instrument}\label{specialcase}
Equation (\ref{eq:mis0}) implies
\begin{eqnarray*}
\mathbb P(D=1\vert Z=z) &=& \mathbb P(\varepsilon=0\vert V \leq P(z))\mathbb P(V\leq P(z)) + \mathbb P(\varepsilon=1\vert V > P(z))\mathbb P(V > P(z)),\\
&=& (1-\alpha_1(z)) P(z) +\alpha_0(z) (1-P(z)),\\
&=& (1-\alpha_0(z)-\alpha_1(z)) P(z) + \alpha_0(z),
\end{eqnarray*}
where the second equality holds from Assumption \ref{Cont}, $\alpha_1(z)\equiv \mathbb P(\varepsilon=1\vert V \leq P(z))$ is the false negative misclassification rate, and $\alpha_0(z)\equiv \mathbb P(\varepsilon=1\vert V > P(z))$ is the false positive misclassification rate. Suppose that $\alpha_0(z)+\alpha_1(z) < 1$.\footnote{This constraint is known as the \textit{monotonicity condition} is the literature on misclassification, and is different from the monotonicity restriction imposed on the treatment selection in model (\ref{seq1}).} Then,
\begin{eqnarray*}
P(z)= \frac{\mathbb P(D=1\vert Z=z)-\alpha_0(z)}{1-\alpha_0(z)-\alpha_1(z)}.
\end{eqnarray*}
The misclassification rate functions $\alpha_0(z)$ and $\alpha_1(z)$ can be partially identified using the conditions that they lie within the interval $[0,1]$, and that $P(z)\in[0,1]$. We are going to follow the literature \citep{Hausman_al1998} to first assume that the misclassification rates $\alpha_0(z)$ and $\alpha_1(z)$ are constant across $z$.\footnote{Note however that in a recent paper, \cite{HaiderStephens2020} show that this assumption is invalid in routine empirical settings. In Subsection \ref{semipara}, we discuss how one can allow the false positive/negative rates to depend on $z$.} The true propensity score $P(z)$ is therefore identified up to the misclassification probabilities $\alpha_0$ and $\alpha_1$ as follows:
\begin{eqnarray*}
P(z)= \frac{\mathbb P(D=1\vert Z=z)-\alpha_0}{1-\alpha_0-\alpha_1}.
\end{eqnarray*}
The following lemma holds. 
\begin{lemma}\label{symmetry}
Suppose Assumption \ref{RA} holds. Then, false positive rate $\alpha_0(z)$ and false negative rate $\alpha_1(z)$ do not depend on $z$ if and only if the misclassification is symmetric, i.e., $\alpha_0(z)=\alpha_1(z)$. %
\end{lemma}

Lemma \ref{symmetry} shows that under Assumption \ref{RA} the false positive rate $\alpha_0(z)$ and false negative rate $\alpha_1(z)$ are constant across $z$ if and only if the misclassification is symmetric, i.e., $\alpha_0=\alpha_1=\alpha$. This misclassification is symmetric if $\varepsilon$ is independent of $V$ (exogenous misclassification).

From the previous paragraph, we conclude that in the scenario where false positive and false negative rates do depend not on the instrument, they must be equal under our identifying assumptions. Hence, in such a scenario, we have
\begin{eqnarray*}
P(z)= \frac{\mathbb P(D=1\vert Z=z)-\alpha}{1-2\alpha},
\end{eqnarray*}
where $\alpha$ is partially identified:
\begin{eqnarray*}
\alpha \in \left\{[0,1/2)\cup(1/2,1], \text{ and } 0 \leq \frac{\mathbb P(D=1\vert Z=z)-\alpha}{1-2\alpha} \leq 1 \text{ for all } z\right\},
\end{eqnarray*}
Hence, the MTE is partially identified as a function of $\alpha$:
\begin{eqnarray*}
MTE(p ; \alpha)&=& \frac{\partial \mathbb E[Y \vert P(Z)=p] }{\partial p},\\
&=& \frac{\partial \mathbb E\left[Y \vert \mathbb P(D=1\vert Z)=(1-2\alpha)p+\alpha\right] }{\partial p},\\
&=& (1-2\alpha) LIV\left((1-2\alpha)p+\alpha\right),
\end{eqnarray*}
where $LIV(p)\equiv \frac{\partial \mathbb E[Y \vert \mathbb P(D=1\vert Z)=p] }{\partial p}$ is the local instrumental variable (LIV) estimand. 


\subsection*{Numerical illustration of this special case}\label{num}
We assume in this illustration that the researcher knows that the rate of misclassification $\alpha$ is less than 1/2.
Consider the same example from the previous section (\ref{eq:ex}) where $\rho=0$ (i.e., $\varepsilon$ is independent of $V$). 
Details on this illustration are given in Subsection \ref{numeric2:apx} in the appendix.

\begin{figure}[h]
    \begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{1031_sim_half.png}
    \centering
    \caption{Numerical illustrations of $MTE(p;\tilde{\alpha})$}
    \label{fig.0127A_half}
    \end{minipage}
\end{figure}


Figure \ref{fig.0127A_half} displays the MTE bounds for different misclassification probabilities $\alpha$ and different values of $\bar{\alpha}$. On the one hand, the rows show the identification regions when $\alpha$ takes values $0.1$, $0.3$, and $0.4$, respectively. On the other hand, the columns display the MTE bounds when $\bar{\alpha}$ is equal to $0.1$, $0.3$, and $0.4$, respectively. 
Eight different discretized values $\tilde{\alpha}$ within the support $[0,\bar{\alpha}]$ are used to show the corresponding plots for the MTE bounds.
We note that the identification region for the MTE grows with $\bar{\alpha}$ for each value of the true misclassification rate $\alpha$. When $\bar{\alpha}$ is sufficiently large, the bounds contain the true MTE (see the last column). However, when $\bar{\alpha}$ is set too small, the true MTE lies outside the bounds, and the model is misspecified (see the first column). This suggests that researchers should do some sensitivity analysis by trying different values of $\bar{\alpha}$.

\subsection{When false positive/negative rates depend on the instrument}\label{semipara}
When the misclassification is asymmetric in the sense that false positive and false negative rates depend on the instrument, without further assumptions the researcher can use the bounds we derive in Section \ref{Ident}. However, there may exist some parametrization of the misclassification probabilities that can yield tighter bounds. 





\section{Empirical Relevance of the MTE bounds}\label{EmpRel}
The identification of the MTE can help reveal the presence of heterogeneity in the treatment effect. It can also be useful in the estimation of policy relevant treatment effect parameters (PRTEs) or conventional parameters such as the ATE, the average treatment effect on the treated (ATT), the average treatment effect on the untreated (ATU), the LATE, etc. Tables \ref{integral} and \ref{weights}, which we borrow from \cite{heckman2005structural}, show the link between the MTE and those parameters. Unlike the weights in \cite{heckman2005structural}, the weights for the parameters ATT, ATU, and PRTE are not point-identified in our setting. They are only partially identified, as is the true propensity score $P(Z)$. Like the MTE, these policy parameters are also partially identified. In the next section, we explicitly derive analytical bounds for different LATEs when the instrument is multivalued discrete. 
\begin{table}[!htbp]
		\centering
		\caption{{Treatment effects as weighted averages of the $MTE$}} \label{integral}
		\begin{tabular}{l}
			\hline
			\hline

			\\

			$ATE = \mathbb{E}\left[Y_{1} - Y_{0} \right] = \int_{0}^{1} MTE\left(p\right) \cdot \omega_{ATE}\left(p\right) \, \text{d} p$ \\

			\\

			\\

			$ATT = \mathbb{E}\left[Y_{1} - Y_{0}\vert D^*=1\right] = \int_{0}^{1} MTE \left(p\right) \cdot \omega_{ATT}\left(p\right) \, \text{d} p$ \\

			\\

			\\

			$ATU = \mathbb{E}\left[Y_{1} - Y_{0} \vert D^*=0 \right] = \int_{0}^{1} MTE \left(p\right) \cdot \omega_{ATU}\left(p\right) \, \text{d} p$ \\

			\\

			\\

			$LATE(\underline{p}, \overline{p}) = \mathbb{E}\left[Y_{1} - Y_{0} \vert V \in \left[\underline{p}, \overline{p}\right] \right] = \int_{\underline{p}}^{\overline{p}} MTE\left(p\right) \cdot \omega_{LATE}\left(p\right) \, \text{d} p$ \\

			\\

			\\

			$PRTE = \dfrac{\mathbb{E}\left[Y_{a} - Y_{a^{\prime}} \right]}{\int_{0}^{1} \left(F_{P_{a^{\prime}}}\left(p\right) - F_{P_{a}}\left(p\right)\right)\left(p\right) \, \text{d} p } = \int_{0}^{1} MTE \left(p\right) \cdot \omega_{PRTE}\left(p, a, a^{\prime}\right) \, \text{d} p$\\

			\\
			for two policies $a$ and $a^{\prime}$ that affect only $Z$\\

			\\

			\hline
		\end{tabular}
	\end{table}

	\begin{table}[!htb]
		\centering
		\caption{{Weights}} \label{weights}
		\begin{tabular}{l}
			\hline
			\hline

			\\

			$\omega_{ATE}\left(p\right) = 1$ \\

			\\

			$\omega_{ATT}\left(p\right) = \dfrac{\int_{p}^{1} f_{P\left(Z\right)}\left(u\right) \, \text{d} u}{\int_{\hspace{3pt} 0}^{1} \left(\int_{p}^{1} f_{P\left(Z\right)}\left(u\right) \, \text{d} u \right)\, \text{d} p}$ \\

			\\

			$\omega_{ATU}\left(p\right) = \dfrac{\int_{0}^{p} f_{P\left(Z\right)}\left(u\right) \, \text{d} u}{\int_{\hspace{3pt} 0}^{1} \left( \int_{0}^{p} f_{P\left(Z\right)}\left(u\right) \, \text{d} u \right)\, \text{d} p}$ \\

			\\

			$\omega_{LATE}\left(p\right) = \dfrac{1}{\overline{p} - \underline{p}}$ \\

			\\

			$\omega_{PRTE}\left(p, a, a^{\prime}\right) = \dfrac{F_{P_{a^{\prime}}}\left(p\right) - F_{P_{a}}\left(p\right)}{\int_{\hspace{3pt} 0}^{1}  F_{P_{a^{\prime}}}\left(p\right) - F_{P_{a}}\left(p\right)\, \text{d} p}$ \\

			\\

			\hline
		\end{tabular}
	\end{table}


\section{Extension to discrete instruments}\label{ext:discrete}
\begin{assumption}[Discrete instrument]\label{Discrete}
The instrument $Z$ is discrete with support $\{z_1, z_2, \ldots, z_K\}$ and the propensity score $p_\ell \equiv \mathbb P\left[D^*=1 \vert Z=z_\ell\right]$ satisfies $0 \leq p_{1} < p_{2} < \ldots < p_{K} \leq1.$
\end{assumption}
This assumption states that the ordering of the true propensity score is known, but the support $\{z_1, z_2, \ldots, z_K\}$ of the instrument does not necessarily have the same ranking. This assumption does require monotonicity in the propensity score. %
For example, when the false positive and negative rates do not depend on $z$, we have shown is Subsection \ref{specialcase} that the propensity score can be written as:
\begin{eqnarray*}
P(z)= \frac{\mathbb P(D=1\vert Z=z)-\alpha}{1-2\alpha}.
\end{eqnarray*}
As we can see, in such a case, the ordering of the true propensity score $p_\ell$ is the same as that of the reported propensity score $\mathbb P(D=1\vert Z=z_\ell)$. 
	
We sum up Equations \eqref{eq:fsharp2} and \eqref{eq:fsharp3}, and take the difference for $z_{\ell}$ and $z_{\ell-1}$, respectively. Combining this with the index sufficiency result \eqref{eq:fsharp1}, we have
\begin{eqnarray*}
&&\mathbb P(Y\in A\vert P(Z)=p_{\ell}) - \mathbb P(Y\in A \vert P(Z)=p_{\ell-1})\\
&&\qquad \qquad= \int^{p_{\ell}}_{p_{\ell-1}}\mathbb P\left(Y_1\in A \vert V=v\right)dv - \int_{p_{\ell-1}}^{p_{\ell}}\mathbb P\left(Y_0\in A \vert V=v\right)dv,\\
&& \qquad \qquad= (p_{\ell}-p_{\ell-1})\mathbb P\left(Y_1\in A \vert p_{\ell-1}<V\leq p_{\ell}\right) - (p_{\ell}-p_{\ell-1})\mathbb P\left(Y_0\in A \vert p_{\ell-1}<V\leq p_{\ell}\right)\nonumber.
\end{eqnarray*}
Therefore,
\begin{eqnarray*}
\mathbb P\left(Y_1\in A \vert p_{\ell-1}<V\leq p_{\ell}\right) - \mathbb P\left(Y_0\in A \vert p_{\ell-1}<V\leq p_{\ell}\right)= \frac{\mathbb P(Y\in A\vert P(Z)=p_{\ell}) - \mathbb P(Y\in A \vert P(Z)=p_{\ell-1})}{p_{\ell}-p_{\ell-1}}.
\end{eqnarray*}
The analog of the result holds with expectations. Hence, we identify the MTE up to the function $P(z)$ as follows:
\begin{eqnarray*}
\mathbb E\left[Y_1-Y_0 \vert p_{\ell-1}<V\leq p_{\ell}\right]&=& \frac{\mathbb E[Y\vert P(Z)=p_{\ell}] - \mathbb E[Y \vert P(Z)=p_{\ell-1}]}{p_{\ell}-p_{\ell-1}},\\
&=& \frac{\mathbb E[Y\vert Z=z_{\ell}] - \mathbb E[Y \vert Z=z_{\ell-1}]}{p_{\ell}-p_{\ell-1}}.
\end{eqnarray*}

We have
\begin{eqnarray}\label{eq:newpzbounds}
p_\ell-p_{\ell-1}=\left(p_L-p_1\right)-\sum_{k\neq \ell}\left(p_k-p_{k-1}\right).
\end{eqnarray}
Equations (\ref{eq:pzbounds}) and (\ref{eq:newpzbounds}) imply the following additional bounds on $p_\ell-p_{\ell-1}$:
\begin{eqnarray*}
&&\max\left\{TV_{(Y,D=1)}(z_1,z_L), TV_{(Y,D=0)}(z_1,z_L)\right\}\\
&& \qquad \qquad -\sum_{k\neq \ell}\min\left\{1,2\alpha+\Delta_{DZ}(z_{k-1},z_k), 2(1-\alpha)-\Delta_{DZ}(z_{k-1},z_k)\right\}\nonumber\\
 && \qquad \leq p_\ell-p_{\ell-1} \leq\\ 
 && \qquad \qquad \min\left\{1,2\alpha+\Delta_{DZ}(z_1,z_L), 2(1-\alpha)-\Delta_{DZ}(z_1,z_L)\right\} \\
 && \qquad \qquad \qquad \qquad  - \sum_{k\neq \ell}\max\left\{TV_{(Y,D=1)}(z_{k-1},z_k), TV_{(Y,D=0)}(z_{k-1},z_k)\right\}. 
\end{eqnarray*}
Therefore, the following bounds hold for $p_\ell-p_{\ell-1}$: 
\begin{eqnarray*}
LB_p(z_{\ell-1},z_\ell) \leq p_\ell-p_{\ell-1}  \leq UB_p(z_{\ell-1},z_\ell),
\end{eqnarray*}
where
\begin{eqnarray*}
LB_p(z_{\ell-1},z_\ell) &\equiv& \max\left\{LB^1_p(z_{\ell-1},z_\ell),LB^2_p(z_{\ell-1},z_\ell)\right\},\\
UB_p(z_{\ell-1},z_\ell) &\equiv& \min\left\{UB^1_p(z_{\ell-1},z_\ell),UB^2_p(z_{\ell-1},z_\ell)\right\},\\
LB^1_p(z_{\ell-1},z_\ell) &=& \max\left\{TV_{(Y,D=1)}(z_{\ell-1},z_\ell), TV_{(Y,D=0)}(z_{\ell-1},z_\ell)\right\},\\
LB^2_p(z_{\ell-1},z_\ell) &=& \max\left\{TV_{(Y,D=1)}(z_1,z_L), TV_{(Y,D=0)}(z_1,z_L)\right\}\\
&& \qquad \qquad -\sum_{k\neq \ell}\min\left\{1,2\alpha+\Delta_{DZ}(z_{k-1},z_k), 2(1-\alpha)-\Delta_{DZ}(z_{k-1},z_k)\right\},\\ 
UB^1_p(z_{\ell-1},z_\ell) &=& \min\left\{1,2\alpha+\Delta_{DZ}(z_{\ell-1},z_\ell), 2(1-\alpha)-\Delta_{DZ}(z_{\ell-1},z_\ell)\right\},\\
UB^2_p(z_{\ell-1},z_\ell) &=& \min\left\{1,2\alpha+\Delta_{DZ}(z_1,z_L), 2(1-\alpha)-\Delta_{DZ}(z_1,z_L)\right\} \\
 && \qquad \qquad \qquad \qquad  - \sum_{k\neq \ell}\max\left\{TV_{(Y,D=1)}(z_{k-1},z_k), TV_{(Y,D=0)}(z_{k-1},z_k)\right\}.
\end{eqnarray*} 

The proposition below holds.
\begin{proposition}\label{propdiscrete}
Suppose that model (\ref{seq1}) along with Assumptions \ref{RA}--\ref{Cont_inst}, and \ref{Discrete} hold. Then, we have the following bounds for $LATE(p_{\ell-1},p_{\ell})\equiv\mathbb E\left[Y_1-Y_0 \vert p_{\ell-1}<V\leq p_{\ell}\right]$:
\begin{eqnarray}
&& \min\left\{\frac{\Delta_{YZ}(z_\ell,z_{\ell-1})}{UB_p(z_\ell,z_{\ell-1})}, \frac{\Delta_{YZ}(z_\ell,z_{\ell-1})}{LB_p(z_\ell,z_{\ell-1})}\right\} \nonumber\\
&& \qquad \qquad \leq LATE(p_{\ell-1},p_{\ell})\leq \\
&& \qquad \qquad \qquad \qquad \max\left\{\frac{\Delta_{YZ}(z_\ell,z_{\ell-1})}{UB_p(z_\ell,z_{\ell-1})}, \frac{\Delta_{YZ}(z_\ell,z_{\ell-1})}{LB_p(z_\ell,z_{\ell-1})}\right\}. \nonumber
\end{eqnarray}
\end{proposition}
At this point, we do not have a result on the sharpness of the bounds in Proposition~\ref{propdiscrete}. This could be investigated in future work. However, when $\alpha$ is completely unknown (i.e., $\bar{\alpha}=1$), these bounds are tighter than the existing bounds in \cite{Tommasi2020}. Moreover, when $\alpha$ is completely unknown and the instrument $Z$ is binary, the bounds in Proposition~\ref{propdiscrete} are tighter than the existing \cite{Ura2018} bounds, which appear to be the tightest in the literature before our work. In such a case, we show in Appendix \ref{proofUra} that the bounds are sharp. Indeed, when $Z$ is binary and $\alpha$ is completely unknown (i.e., $\bar{\alpha}=1$), our bounds on $p_1-p_0$ are
\begin{eqnarray*}
\max\left\{TV_{(Y,D=1)}(0,1), TV_{(Y,D=0)}(0,1)\right\} \leq p_1-p_0 \leq 1,
\end{eqnarray*}
while \citeauthor{Ura2018}'s (\citeyear{Ura2018}) bounds are
\begin{eqnarray*}
\frac{1}{2} TV_{(Y,D=1)}(0,1) + \frac{1}{2} TV_{(Y,D=0)}(0,1) \leq p_1-p_0 \leq 1.
\end{eqnarray*}
The improvement over the existing bounds mainly stems from the fact that these bounds were not derived using the constraint $\varepsilon_0+\varepsilon_1=1$ that the specification $D=D^* \varepsilon_1 +(1-D^*)\varepsilon_0$ imposes. As we prove in Lemma \ref{misc:specific} in Appendix \ref{apx:spec}, this constraint is without loss of generality. The results in Proposition~\ref{propdiscrete} hold under the same assumptions that the existing papers have used.   


\section{Empirical illustration}\label{App}
To illustrate our methodology, we use data from the third wave of the Indonesia Family Life Survey (IFLS) fielded from June through November 2000. We build upon \cite{Carneiroal2017} who estimate average and marginal returns to schooling in Indonesia using a semiparametric selection model. The authors use exogenous geographic variation in access to upper secondary schools to identify their model when ignoring the presence of measurement errors in the treatment variable. In their analysis, these researchers control for several family and village characteristics, namely father’s and mother’s education, an indicator of whether the community of residence was a village, religion, whether the location of residence is rural, province dummies, and distance from the village of residence to the nearest health post.

 The IFLS is a household and community level panel survey that was conducted in 1993, 1997 and 2000. The sample was drawn from 321 randomly selected villages, spread among 13 Indonesian provinces containing 83\%  of the population, and consists of males aged 25--60 employees in public and private sectors. Females are excluded from the sample because of low labor force participation, self-employed workers are also excluded because it is difficult to measure their earnings. The sample size is 2608.

Following \cite{Carneiroal2017}, we define the dependent variable in the  analysis as the log of the hourly wage $(Y)$, which is constructed from self-reported monthly wages and hours worked per week. The treatment variable $(D)$ is the indicator that the individual has an upper secondary or higher education (i.e., he completed at least 10 years of education). As we argue in Example~\ref{ex1}, people often misreport their education level. So, we observe their true education level with some measurement errors.
The control variables $(X)$ are indicator variables for age, indicators for the level of schooling completed by each of the parents (no education, elementary education, secondary education, and an indicator for unreported parental education), an indicator for whether the individual was living in a village at age 12, indicators for the province of residence, an indicator of rural residence, and distance (in kilometers) from the office of the head of the community of residence to the nearest community health post. 

The instrumental variable $(Z)$ for schooling is the distance (in kilometers) from the office of the community head to the nearest secondary school. The main assumption from \cite{Carneiroal2017} is that if we consider two individuals with equally educated parents, with the same religion, living in a village which is located in an area that is equally rural, in the same province, and at the same distance of a health post, then distance to the nearest secondary school is uncorrelated with direct determinants of wages other than schooling. 
The authors present evidence that this assumption is likely to hold, suggesting that the IV is valid. In particular, they show that, once the previously mentioned variables are controlled for, there is no dependence between the distance to the nearest secondary school and whether the individual ever failed a grade in elementary school, how many times he repeated a grade in elementary school, and whether he had to work while attending elementary school. In addition, they show (using a different sample) that the distance variable is unrelated to test scores (Math, Bahasa, Science, and Social Studies) in elementary school.  However, the validity of the distance to the nearest secondary school instrument remains highly questionable. For this reason, this exercise should be seen as illustrative.

\subsection*{Estimation Results}
The estimation procedure follows the same steps as in the numerical illustration in Subsection \ref{specialcase}, with the exception that we do not know the true DGP.
We consider the specification $P(z)=\frac{\mathbbm{P}(D=1 \vert Z=z)-\alpha}{1-2 \alpha}$, which we obtain under the symmetric misclassification assumption as described in Subsections \ref{specialcase}. As we discussed, this scenario occurs when there is exogenous misclassification:  $\varepsilon\ \indep\ V$. The scenario under exogenous misclassification implies that the conditional distribution of $V$ given $\varepsilon=1$ is linear. 
Next, we consider alternative assumptions about the misclassification mechanism in Appendix \ref{apx:sup}, where $\varepsilon$ depends on $V$ and the conditional distributions of $V$ given $\varepsilon=1$ are respectively concave and convex. The results are roughly similar across all specifications. %


\subsubsection*{Choice of $\bar{\alpha}$} As previously discussed in Section \ref{anaF}, following the comment below Assumption~\ref{Bound:mis}, we assume that someone who has at least an upper secondary education in Indonesia will proudly and truthfully report it. Hence, the false negative rate $\mathbb P(\varepsilon=1\vert D^*=1)$ is set to zero. Furthermore, following \cite{Black_etal2003}, we assume that among those who report having at least an upper secondary schooling, the proportion of misclassification is bounded by 1/3, i.e., $\mathbb P(\varepsilon=1\vert D=1) \leq 1/3$. Therefore, $\alpha=\mathbb P(\varepsilon=1)=\mathbb P(\varepsilon=1\vert D=1)\mathbb P(D=1)+\mathbb P(\varepsilon=1\vert D^*=1) \mathbb P(D^*=1) \leq \frac{1}{3}\mathbb P(D=1)$. Hence, we set $\bar{\alpha}=\frac{1}{3}\mathbb P(D=1) \approx 0.139$. However, to check the sensitivity of our results with respect to the choice of $\bar{\alpha}$, we try three other values for $\bar{\alpha}: 0.1, 0.05, 0.025.$

\subsubsection*{Implementation}
As mentioned above, for sensitivity analysis reasons, we consider four candidates for the upper bound on the misclassification rate $\overline{\alpha} \in \{0.139, 0.1, 0.05, 0.025\}$.
For $\overline{\alpha}=0.139$ as an example, we consider 15 grid points for $\alpha$ within $[0, \overline{\alpha}]$.\footnote{We consider 15, 11, 6, and 6 grid points for $\overline{\alpha}$ = 0.139, 0.1, 0.05, and 0.025, respectively.}
After we estimate $\mathbb{E}[D|Z=z, X=x]$ using a logit specification to obtain the observed propensity scores for each individual, we estimate the true propensity scores $P(z)$ for each of the 15 different $\alpha$ values on the grids within $[0, \overline{\alpha}]$.\footnote{As the logit specification forces the propensity scores estimates to lie over [0, 1], we trim the estimated true propensity scores to lie between $[\delta, 1-\delta]$ with $\delta = 0.0001$ \citep{carneirolee2009}.}
Afterwards, we estimate the MTE nonparametrically using a local quadratic approximation, as recommended by \cite{fan1996local} for estimating a first-order derivative, using each of the 15 possible values of the true propensity score.
To do so, we use either (a) a Gaussian kernel and a bandwidth of 0.27 as used in the original work by \cite{Carneiroal2017} or (b) the \textit{R} package \textit{nprobust} developed by \cite{2019arXiv190600198C}. Note that the plug-in approach is asymptotically valid when the bandwidth $h$ used in the nonparametric estimation goes to zero as the sample size $n$ goes to infinity, since the first stage estimator converges at the parametric rate $\sqrt{n}$, and the second stage nonparametric estimator converges at $\sqrt{n h}$. The first stage estimation bias does not vanish asymptotically with fixed bandwidth. For this reason, we prefer the \textit{nprobust} method.\footnote{Note that the \textit{nprobust} package uses the epanechnikov kernel, and the optimal bandwidth is chosen from a direct plug-in implementation of MSE-optimal choices which considers whether an evaluation point is interior or boundary. However, whenever it is not applicable, the rule-of-thumb implementation of MSE-optimal choices is selected.}

In order to control for exogenous covariates in a tractable manner, we assume the partially linear regression model, and implement the \cite{robinson1988} approach, following \cite{Carneiroal2017}. More precisely, we specify
\begin{eqnarray*}
	Y_1 &=& \lambda_1 + X \beta_1 + U_1, \\
	Y_0 &=& \lambda_0 + X \beta_0 + U_0,
\end{eqnarray*}
where $(U_0,U_1)\ \indep\ (Z,X)$. Since $Y=Y_0+(Y_1-Y_0)D^*$, we have
\begin{eqnarray*}
	\mathbb{E}[Y \vert X, P] &=& \lambda_0 + X \beta_0 + P (\lambda_1 - \lambda_0) + PX(\beta_1 - \beta_0) + \mathbb E[U_0\vert P] + \mathbb{E}[U_1 - U_0 \vert D^*=1, X, P]P, \\
	&=& \lambda_0 + X \beta_0 + P (\lambda_1 - \lambda_0) + PX(\beta_1 - \beta_0) + K(P),
\end{eqnarray*}
where $K(P)\equiv \mathbb E[U_0\vert P] + \mathbb{E}[U_1 - U_0 \vert D^*=1, X, P]P$, and $P\equiv P(Z,X)=\mathbb P(D^*=1\vert X, Z)$.

Therefore, we have 
\begin{eqnarray*}
	MTE(x, p) &=& \frac{\partial \mathbb{E}[Y \vert X, P] }{\partial P} \bigg\vert _{X=x, P=p}, \\
	&=&  (\lambda_1 - \lambda_0) + X(\beta_1 - \beta_0) + K'(P) \Big\vert _{X=x, P=p}.
\end{eqnarray*}


The estimation procedure for a given set of true propensity score estimates $P$ is as follows. First, we save residuals from a set of nonparametric regressions of $Y$, $X$, and $XP$ on $P$.
Then, we regress the residualized $Y$ on the residualized $X$ and $PX$ to obtain $\beta_0$ and $\beta_1 - \beta_0$.
Finally, we run a local quadratic regression of the residual $R\equiv Y - [X \beta_0 + PX(\beta_1 - \beta_0)]$ on $P$ to obtain $(\lambda_1 - \lambda_0) + K'(P)$ as a first derivative.

Figure \ref{fig.f0102naive} shows the results assuming no misclassification ($\overline{\alpha} = 0$), while Figure \ref{fig.f0102} shows the results of the Gaussian kernel and \textit{nprobust} estimations for each row under the 3 different values of $\overline{\alpha} \in \{0.139, 0.1, 0.05, 0.025\}$, with the histograms of the estimated true propensity score evaluated at $\overline{\alpha}$.
For each $\overline{\alpha} \in \{0.139, 0.1, 0.05, 0.025\}$, the 95\% confidence bounds of MTE region are obtained by taking the union of each of the confidence bands for the MTE over the grid points $\alpha$ in $[0, \overline{\alpha}]$.
We use 250 bootstrap replications to obtain the standard errors of the MTE estimates for the Gaussian kernel estimation, while we use the robust standard errors available from the \textit{nprobust} package for the second estimation method.
Finally, we use the same simulation-based method as \cite{Carneiroal2017} to obtain the average parameters such as ATE, ATT, ATU, and AMTE (average marginal treatment effect).\footnote{We obtain AMTE using a metric of $\vert P - V \vert < \zeta$ for 3 different values of $\zeta \in \{0.1, 0.05, 0.01\}$, and note that it is equivalent to MPRTE (marginal policy relevant treatment effect) for policy alternatives of the form $P_{a} = P + a$ when $a \rightarrow 0$ \citep{checkman2010}.} See Tables \ref{tab:ar1} and \ref{tab:amte1}. 
Inference on these parameters also relies on 250 bootstrap replications for both the Gaussian kernel and the \textit{nprobust} methods (see Tables~\ref{tab:ar1ci} and \ref{tab:amte1ci}).

\begin{figure}[h]
    \begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{Export_01042022_F0102_naive.PNG}
    \centering
    \caption{95\% Confidence Bands for MTE ($\overline{\alpha} = 0$)}
    \label{fig.f0102naive}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{Export_01042022_F0102.PNG}
    \centering
    \caption{95\% Confidence Region for MTE}
    \label{fig.f0102}
    \end{minipage}
\end{figure}


\begin{table}[!htbp] \centering 
  \caption{Estimated Bounds of Average Returns to Upper Secondary Schooling} 
  \label{tab:ar1} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{ATE}&\multicolumn{2}{c}{ATT}&\multicolumn{2}{c}{ATU} \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	& $0.080$ & $0.137$ & $0.098$ & $0.215$ & $0.068$ & $0.099$ \\ 
									& 0.1	& $0.106$ & $0.137$ & $0.127$ & $0.215$ & $0.077$ & $0.099$ \\ 
									& 0.05	& $0.118$ & $0.137$ & $0.178$ & $0.215$ & $0.077$ & $0.082$ \\ 
									& 0.025	& $0.129$ & $0.137$ & $0.200$ & $0.215$ & $0.078$ & $0.082$ \\ 		\hline  \\ 
\multirow{4}{*}{\texttt{nprobust}}	& 0.139	& $0.024$ & $0.134$ & $0.143$ & $0.702$ & $$-$0.272$ & $0.099$ \\ 
									& 0.1 	& $0.024$ & $0.134$ & $0.147$ & $0.702$ & $$-$0.272$ & $0.099$ \\ 
									& 0.05 	& $0.024$ & $0.134$ & $0.366$ & $0.702$ & $$-$0.272$ & $$-$0.170$ \\   
									& 0.025 & $0.083$ & $0.136$ & $0.494$ & $0.702$ & $$-$0.272$ & $$-$0.128$ \\   
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 

\begin{table}[!htbp] \centering 
  \caption{95\% Confidence Intervals of Average Returns to Upper Secondary Schooling} 
  \label{tab:ar1ci} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{ATE}&\multicolumn{2}{c}{ATT}&\multicolumn{2}{c}{ATU} \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	& $0.002$ & $0.253$ & $$-$0.014$ & $0.384$ & $$-$0.134$ & $0.299$ \\ 
									& 0.1	& $0.017$ & $0.253$ & $0.002$ & $0.384$ & $$-$0.134$ & $0.299$ \\ 
									& 0.05	& $0.017$ & $0.253$ & $0.034$ & $0.384$ & $$-$0.134$ & $0.299$ \\ 
									& 0.025	& $0.019$ & $0.253$ & $0.045$ & $0.384$ & $$-$0.134$ & $0.299$ \\ 	\hline  \\ 
\multirow{4}{*}{\texttt{nprobust}}	& 0.139	& $$-$0.187$ & $0.455$ & $$-$0.011$ & $1.215$ & $$-$0.771$ & $0.309$ \\
									& 0.1 	& $$-$0.187$ & $0.455$ & $$-$0.006$ & $1.215$ & $$-$0.771$ & $0.309$ \\ 
									& 0.05 	& $$-$0.187$ & $0.455$ & $0.141$ & $1.215$ & $$-$0.771$ & $0.242$ \\ 
									& 0.025 & $$-$0.187$ & $0.455$ & $0.175$ & $1.215$ & $$-$0.771$ & $0.246$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 

\begin{table}[!htbp] \centering 
  \caption{Estimated Bounds of AMTE to Upper Secondary Schooling} 
  \label{tab:amte1} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{$\zeta=$0.1}&\multicolumn{2}{c}{$\zeta=$0.05}&\multicolumn{2}{c}{$\zeta=$0.01}& \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	&    $0.078$ & $0.155$ & $0.078$ & $0.155$ & $0.079$ & $0.155$ \\ 
									& 0.1	&    $0.105$ & $0.155$ & $0.106$ & $0.155$ & $0.107$ & $0.155$ \\  
									& 0.05	&    $0.132$ & $0.155$ & $0.133$ & $0.155$ & $0.134$ & $0.155$ \\  
									& 0.025	&    $0.145$ & $0.155$ & $0.146$ & $0.155$ & $0.146$ & $0.155$ \\  \hline  \\ 
\multirow{4}{*}{\texttt{nprobust}}	& 0.139 &    $0.099$ & $0.180$ & $0.098$ & $0.153$ & $0.091$ & $0.148$ \\ 
									& 0.1 	&    $0.099$ & $0.180$ & $0.098$ & $0.153$ & $0.091$ & $0.148$ \\ 
									& 0.05 	&    $0.111$ & $0.180$ & $0.107$ & $0.153$ & $0.098$ & $0.148$ \\ 
									& 0.025 &    $0.155$ & $0.181$ & $0.133$ & $0.156$ & $0.124$ & $0.150$ \\  
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}  

\begin{table}[!htbp] \centering 
  \caption{95\% Confidence Intervals of AMTE to Upper Secondary Schooling} 
  \label{tab:amte1ci} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{$\zeta=$0.1}&\multicolumn{2}{c}{$\zeta=$0.05}&\multicolumn{2}{c}{$\zeta=$0.01}& \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	& $0.005$ & $0.250$ & $0.004$ & $0.250$ & $0.004$ & $0.250$ \\ 
									& 0.1	& $0.025$ & $0.250$ & $0.025$ & $0.250$ & $0.025$ & $0.250$ \\ 
									& 0.05	& $0.045$ & $0.250$ & $0.046$ & $0.250$ & $0.046$ & $0.250$ \\ 
									& 0.025	& $0.055$ & $0.250$ & $0.055$ & $0.250$ & $0.055$ & $0.250$ \\ 	\hline  \\ 
\multirow{4}{*}{\texttt{nprobust}}	& 0.139 & $$-$0.013$ & $0.350$ & $$-$0.027$ & $0.304$ & $$-$0.111$ & $0.366$ \\   
									& 0.1 	& $$-$0.013$ & $0.350$ & $$-$0.027$ & $0.304$ & $$-$0.065$ & $0.299$ \\    
									& 0.05 	& $$-$0.003$ & $0.350$ & $$-$0.018$ & $0.304$ & $$-$0.046$ & $0.292$ \\   
									& 0.025 & $0.011$ & $0.350$ & $$-$0.008$ & $0.307$ & $$-$0.017$ & $0.295$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}  


\begin{table}[!htbp] \centering 
  \caption{Point Estimate Bounds and 95\% Confidence Bounds of PRTE to Upper Secondary Schooling} 
  \label{tab:prte01} 
\begin{tabular}{@{\extracolsep{5pt}} cc|ccccc} 
\\[-1.8ex]\hline 
\hline \\
\multirow{2}{*}{$a$}&\multirow{2}{*}{$\overline{\alpha}$}&\multicolumn{3}{c}{Point Estimates}&\multicolumn{2}{c}{Confidence Bounds} \\
&& $\alpha=0$  & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{$0.05$}	&0.139 & \multirow{4}{*}{$0.3762$} 	& $0.3762$ & $0.4277$ & $0.2822$ & $0.5188$ \\ 
						&0.1 &								& $0.3762$ & $0.4188$ & $0.2822$ & $0.5113$ \\ 
						&0.05 &  							& $0.3762$ & $0.3999$ & $0.2822$ & $0.4934$ \\ 
						&0.025 &							& $0.3762$ & $0.3884$ & $0.2822$ & $0.4823$ \\  \hline  \\
\multirow{4}{*}{$0.1$}	&0.139 & \multirow{4}{*}{$0.3792$} 	& $0.3792$ & $0.4272$ & $0.2842$ & $0.5183$ \\ 
						&0.1 &								& $0.3792$ & $0.4196$ & $0.2842$ & $0.5125$ \\ 
						&0.05 & 							& $0.3792$ & $0.4020$ & $0.2842$ & $0.4963$ \\ 
						&0.025 &  							& $0.3792$ & $0.3911$ & $0.2842$ & $0.4858$ \\  \hline  \\
\multirow{4}{*}{$0.15$}	&0.139 & \multirow{4}{*}{$0.3825$}	& $0.3825$ & $0.4268$ & $0.2864$ & $0.5180$ \\ 
						&0.1 &								& $0.3825$ & $0.4207$ & $0.2864$ & $0.5141$ \\ 
						&0.05 & 							& $0.3825$ & $0.4045$ & $0.2864$ & $0.4997$ \\ 
						&0.025 &							& $0.3825$ & $0.3940$ & $0.2864$ & $0.4897$ \\  \hline  \\
\multirow{4}{*}{$0.2$}	&0.139 & \multirow{4}{*}{$0.3861$} 	& $0.3861$ & $0.4263$ & $0.2887$ & $0.5183$ \\ 
						&0.1 &								& $0.3861$ & $0.4220$ & $0.2887$ & $0.5160$ \\ 
						&0.05 &  							& $0.3861$ & $0.4072$ & $0.2887$ & $0.5034$ \\ 
						&0.025 & 							& $0.3861$ & $0.3972$ & $0.2887$ & $0.4941$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}  


\subsection*{Discussion}
First of all, Figure \ref{fig.f0102naive} shows that the \textit{nprobust} method (second column) seems to better reveal the heterogeneity in the standard LIV estimate than the Gaussian kernel approach (first column). When there is no measurement error in the schooling variable  ($\overline{\alpha} = 0$), we see that the return to upper secondary schooling is strongly positive for individuals who face low costs ($V < 0.25 $), close to zero for those who face medium costs ($0.25 < V < 0.8$), and negative for those whose costs are very high ($V>0.8$). This heterogeneity was revealed in the original work of \cite{Carneiroal2017} when considering the point estimate of the MTE, but it disappears when considering the confidence intervals of the MTE, because they used the Gaussian kernel approach which shows less heterogeneity.

On the one hand, in Figure \ref{fig.f0102}, when $\overline{\alpha}=0.139$ (first row) or $\overline{\alpha}=0.1$ (second row), we observe that the MTE is weakly positive for individuals at all cost margins between completing an upper secondary education level or not. The estimate of the identified set for the MTE suggests that the return to upper secondary schooling is heterogeneous in Indonesia. Individuals who face smaller costs tend to have higher marginal returns compared to those who face higher costs. However, the MTE is generally not significant for most values of the schooling cost. 

On the other hand, when $\overline{\alpha}=0.05$ (third row) or $\overline{\alpha}=0.025$ (fourth row), the heterogeneity in the return becomes more apparent with the \textit{nprobust} approach. Even when allowing for the presence of misreporting in the education variable, the MTE is heterogeneous across individuals. Individuals at lower cost margins have higher and positive returns, while those at medium cost margins show returns close to zero, and those at higher cost margins seem to have lower and negative returns. The average marginal returns vary between -6\% and 35\% for values of $\zeta$ between 0.01 and 0.1 when using the \textit{nprobust} inference, while they vary between 3\% and 25\% when using the Gaussian kernel inference method.

Furthermore, the ATU (return for people who did not complete an upper secondary schooling had they done so) is not statistically different from zero, as their confidence bounds contain zero for both methods. However, the ATT (return for people who completed at least an upper secondary schooling) lies between 0.2\% and 38.4\% with the Gaussian kernel method, and between -0.6\% and 121.5\% with the \textit{nprobust} approach. Similarly, the ATE lies roughly between 2\% and 25\% with the Gaussian kernel method, and between -19\% and 46\% with the \textit{nprobust} approach. These results confirm that the MTE helps reveal heterogeneity in the return, which would otherwise be hidden when considering only the ATE, the ATT, and the ATU.

Overall, note that the patterns shown in Figure \ref{fig.f0102} are consistent across all other specifications considered in the appendix. The MTE curve seems weakly decreasing for all values of the schooling cost. 

Another advantage of the MTE is that it helps answer policy-relevant questions. For example, as in \cite{SasakiUra2021}, consider a counterfactual policy that exposes fraction $a$ of people with less than upper secondary schooling to upper secondary or higher education. More precisely,  assume that the counterfactual policy has the form $P^* = P+a (1-P)$, where $P^*$ is the new propensity score that results from the policy. What would be the effect of such a policy on wages? To answer this question, we compute bounds on the policy-relevant treatment effect parameter for values $a \in \{0.05,0.1,0.15,0.2\}$. We use \citeauthor{SasakiUra2021}'s (\citeyear{SasakiUra2021}) inferential method to construct confidence bounds on the PRTE parameter.\footnote{While \cite{SasakiUra2021} use a probit specification for the potentially misclassified propensity score, we use a logit specification to be consistent with \cite{Carneiroal2017}. For this reason, our point estimates of the PRTE parameter are different from \citeauthor{SasakiUra2021}'s.} Table \ref{tab:prte01} summarizes the results. For the four values of $a$, the PRTE varies between 28\% and 52\%. These results suggest that the policy that induces about 5--20\% of the population to upper secondary education is expected to increase wages by 28--52\% per treated individual on average.

\section{Conclusion}\label{conclusion}
In this paper, we show that the MTE is generally partially identified in the presence of misclassification. We show that the MTE is equal to the derivative of the expectation of the observed outcome conditional on the true propensity score, which is partially identified. We provide nonparametric characterization of the identified set for the propensity score and the MTE. We show under some mild regularity conditions that the sign of the MTE is \textit{locally identified}. We use our MTE bounds to derive bounds on other commonly used parameters in the literature. We show that  our bounds are tighter than the existing bounds for the local average treatment effect. We illustrate the methodology numerically and empirically. We investigate the measurement of the return to upper secondary schooling in Indonesia, and find that the return is heterogeneous for people at the cost margin. Overall, marginal returns seem weakly decreasing with the schooling cost.   

We have not developed a formal inference method for the analytical bounds for the MTE in this work. We believe that constructing a confidence set for these bounds could be worth exploring in future research. Also, future research could explore identification of the MTE in the presence of misclassification when imperfect instruments are available. 

\clearpage
\appendix

\section{Discussion about the model specification}\label{apx:spec}
One might think that the specification $D=D^* (1-\varepsilon) +(1-D^*)\varepsilon$ is too restrictive. But, it is general. To show this claim, we prove the following lemma.
\begin{lemma}\label{misc:specific}
For any two binary variables $D$ and $D^*$, there exist two binary variables $\varepsilon_1$ and $\varepsilon_0$ such that 
\begin{eqnarray}
D=D^* \varepsilon_1 +(1-D^*)\varepsilon_0,
\end{eqnarray}
where $\varepsilon_0+\varepsilon_1=1$.
\end{lemma}
\begin{proof}
Let $D$ and $D^*$ be two binary variables. We can write $D=D^*+(D-D^*)=D^*+\xi$, where $\xi=D-D^*$. Since $D$ and $D^*$ are binary, we have $$\left(D,D^*,\xi\right) \in \left\{(0,0,0),(1,1,0),(1,0,1),(0,1,-1)\right\}.$$ We can see that $D=1-D^*$ if $\xi\in\{-1,1\}$ and $D=D^*$  if $\xi\in\{0\}$. Hence, we can write $D=D^*\left(\mathbbm{1}\left\{\xi\in\{0\}\right\}\right)+(1-D^*) \mathbbm{1}\left\{\xi\in\{-1,1\}\right\}$. By setting $\varepsilon_0=\mathbbm{1}\left\{\xi\in\{-1,1\}\right\}$ and $\varepsilon_1=\mathbbm{1}\left\{\xi\in\{0\}\right\}$, we have $D=D^* \varepsilon_1 +(1-D^*)\varepsilon_0$. Since $\xi\in\{-1,0,1\}$, we have $\varepsilon_0+\varepsilon_1=\mathbbm{1}\left\{\xi\in\{-1,1\}\right\}+\mathbbm{1}\left\{\xi\in\{0\}\right\}=\mathbbm{1}\left\{\xi\in\{-1,0,1\}\right\}=1$. 
\end{proof}



\section{Proofs of Proposition \ref{prop1}}\label{proofprop1}
Since the function $\max$ and $\min$ are continuous, there exist $\alpha^*$ the $\inf$ in the lower bound on $P(z)$ is attained. Similar result holds for the upper. We propose two misclassification scenarios that yield the lower or upper bound for each value of $\alpha$.
\subsection{$\varepsilon=\mathbbm{1}\{V\leq \alpha\}$} Here, we assume that the misclassification occurs when the unobserved heterogeneity $V$ is less than or equal to $\alpha$. Then, we have $\mathbb P(\varepsilon=1)=\alpha$, and $F_{V\vert \varepsilon=1}(p)=\min\{p/\alpha,1\}$. Hence, the conditional distribution of $V$ given $\varepsilon=1$ is concave. Using Equation (\ref{eq:mix}), we obtain that $F_{V\vert \varepsilon=0}(p)=\frac{p-\min\{p,\alpha\}}{1-\alpha}$. 

If $\alpha \leq P(z)$, then Equation (\ref{eq2}) implies
\begin{eqnarray*}
\mathbb P(D=1\vert Z=z)=(1-\alpha)\left(\frac{P(z)-\alpha}{1-\alpha}\right)+\alpha(1-\min\{P(z)/\alpha,1\})=P(z)-\alpha,
\end{eqnarray*}
which leads to 
\begin{eqnarray}
P(z)=\mathbb P(D=1\vert Z=z)+\alpha.
\end{eqnarray}

If $\alpha \geq P(z)$, then Equation (\ref{eq2}) implies
\begin{eqnarray*}
\mathbb P(D=1\vert Z=z)=(1-\alpha)\left(\frac{P(z)-P(z)}{1-\alpha}\right)+\alpha(1-P(z)/\alpha)=\alpha-P(z),
\end{eqnarray*}
which in turn implies 
\begin{eqnarray}
P(z)=\alpha-\mathbb P(D=1\vert Z=z).
\end{eqnarray}

\subsection{$\varepsilon=\mathbbm{1}\{V> 1-\alpha\}$} Given this specification for the misclassification, we have $\mathbb P(\varepsilon=1)=\alpha$, and $F_{V\vert \varepsilon=0}(p)=\min\{\frac{p}{1-\alpha},1\}$. From there, we have $F_{V\vert \varepsilon=1}(p)=\max\{0,1-\frac{1-p}{\alpha}\}$. Hence, the conditional distribution of $V$ given $\varepsilon=1$ is convex.

From Equation (\ref{eq2}), we have:
\begin{eqnarray*}
(1-\alpha)\min\left\{\frac{P(z)}{1-\alpha},1\right\}+\alpha \left(1-\max\left\{0,1-\frac{1-P(z)}{\alpha}\right\}\right)=\mathbb P(D=1\vert Z=z).
\end{eqnarray*}

If $1-\alpha \leq P(z)$, the above equation becomes:
\begin{eqnarray*}
(1-\alpha)+\alpha \left(1-1+\frac{1-P(z)}{\alpha}\right)=\mathbb P(D=1\vert Z=z),
\end{eqnarray*}
which implies 
\begin{eqnarray}
P(z)=\mathbb P(D=0\vert Z=z)+1-\alpha.
\end{eqnarray}

If $1-\alpha \geq P(z)$, the equation becomes:
\begin{eqnarray*}
(1-\alpha)\frac{P(z)}{1-\alpha}+\alpha (1-0)=\mathbb P(D=1\vert Z=z),
\end{eqnarray*}
which implies 
\begin{eqnarray}
P(z)=\mathbb P(D=1\vert Z=z)-\alpha.
\end{eqnarray}



\section{Proof of Proposition \ref{prop:mtebounds}}\label{proof:mtebounds}
\begin{enumerate}[(i)]
\item Suppose $\bar{\alpha}=0$. Then, there is no misclassification, and the testable implications of the model are given by: for any $(z',z)$ such that $P(z') < P(z)$,
$f_{Y,D\vert Z}(y,1\vert z)-f_{Y,D\vert Z}(y,1\vert z')\geq 0$ and $f_{Y,D\vert Z}(y,0\vert z)-f_{Y,D\vert Z}(y,0\vert z')\leq 0$. Therefore, $TV_{(Y,D=1)}(z',z)=\Delta_{DZ}(z',z)=TV_{(Y,D=0)}(z',z)$, which implies $LB_p(z',z)=\Delta_{DZ}(z',z)$. In this case, we also have $UB_p(z',z)=\Delta_{DZ}(z',z)$. Thus, the result holds. 
\item Suppose $\bar{\alpha} > 0$. Then, for any $\alpha$ such that $0 <\alpha \leq \bar{\alpha}$, $UB_p(z',z)\neq 0$ for any $(z',z)$, and $\lim_{z' \rightarrow z} UB_p(z',z)\neq 0$. Therefore, $\lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{UB_p(z',z)}=0$.
\end{enumerate}


\section{Proof of sharpness when the instrument is binary}\label{proofUra}
In this section, we assume that the researcher has no information about the misclassification rate $\alpha$.
Suppose $\Delta_{DZ}(0,1)\neq 0$, and for each $d$, either $f_{Y,D\vert Z}(y,d\vert 1)-f_{Y,D\vert Z}(y,d\vert 0)\geq 0$ or $f_{Y,D\vert Z}(y,d\vert 1)-f_{Y,D\vert Z}(y,d\vert 0)\leq 0$ for all $y$. Define 
\begin{eqnarray*}
LB^1_p(0,1) &\equiv& \max\left\{TV_{(Y,D=1)}(0,1), TV_{(Y,D=0)}(0,1)\right\}=\lvert \Delta_{DZ}(0,1) \rvert,\\
\tilde{P}(0) &\equiv& \frac{1}{2}\left[1+\frac{LB^1_p(0,1)}{\Delta_{DZ}(0,1)}\left(1-2\mathbb E[D\vert Z=1]\right)\right],\\ %
\tilde{P}(1) &\equiv& \tilde{P}(0) +LB^1_p(0,1).
\end{eqnarray*}
Notice that $\tilde{P}(0)$ and $\tilde{P}(1)$ are well-defined probabilities. Define a joint distribution on $(\tilde{Y}_0,\tilde{Y}_1,\tilde{V}, \tilde{\varepsilon})$. 
\begin{eqnarray*}
&& \mathbb P(\tilde{V} \leq v)=v,\\
&& \mathbb P(\tilde{Y}_1\leq y_1,\tilde{\varepsilon}=1\vert \tilde{V} \leq \tilde{P}(z))= \mathbb P(Y\leq y_1,D=0\vert Z=z),\\
&& \mathbb P(\tilde{Y}_1\leq y_1,\tilde{\varepsilon}=0\vert \tilde{V} \leq \tilde{P}(z))= \mathbb P(Y\leq y_1,D=1\vert Z=z),\\
&& \mathbb P(\tilde{Y}_0\leq y_0,\tilde{\varepsilon}=0\vert \tilde{V} > \tilde{P}(z))= \mathbb P(Y\leq y_0,D=0\vert Z=z),\\
&& \mathbb P(\tilde{Y}_0\leq y_0,\tilde{\varepsilon}=1\vert \tilde{V} > \tilde{P}(z))= \mathbb P(Y\leq y_0,D=1\vert Z=z),
\end{eqnarray*}
for each $z \in\{0,1\}$. Define 
\begin{eqnarray*}
&&\mathbb P(\tilde{Y}_1\leq y_1, \tilde{Y}_0 \leq y_0, \tilde{\varepsilon}=\ell, \tilde{V} \leq \tilde{P}(z)\vert Z=\check{z}) =\\
 && \qquad \qquad  \mathbb P(\tilde{Y}_1\leq y_1\vert \tilde{\varepsilon}=\ell, \tilde{V} \leq \tilde{P}(z)) \mathbb P(\tilde{Y}_0 \leq y_0\vert \tilde{\varepsilon}=\ell, \tilde{V} \leq \tilde{P}(z)) \mathbb P( \tilde{\varepsilon}=\ell\vert \tilde{V} \leq \tilde{P}(z)) \tilde{P}(z),\\
&&\mathbb P(\tilde{Y}_1\leq y_1, \tilde{Y}_0 \leq y_0, \tilde{\varepsilon}=\ell, \tilde{V} > \tilde{P}(z)\vert Z=\check{z}) =\\
&& \qquad \mathbb P(\tilde{Y}_1\leq y_1\vert \tilde{\varepsilon}=\ell, \tilde{V} > \tilde{P}(z)) \mathbb P(\tilde{Y}_0 \leq y_0\vert \tilde{\varepsilon}=\ell, \tilde{V} > \tilde{P}(z)) \mathbb P( \tilde{\varepsilon}=\ell\vert \tilde{V} > \tilde{P}(z)) (1-\tilde{P}(z)),
\end{eqnarray*}
for each $\ell \in\{0,1\}$, and each $z \in\{0,1\}$.

We can verify that the above proposed joint distribution is well-defined, $Z\ \indep\ (\tilde{Y}_0,\tilde{Y}_1,\tilde{V}, \tilde{\varepsilon})$ (Assumption \ref{RA}), and $\tilde{V} \sim \mathcal U_{[0,1]}$ (Assumption \ref{Cont}), the joint distribution is compatible with the data, that is, it satisfies the following conditions: 
\begin{eqnarray*}
\mathbb P(Y\in A, D=1 \vert Z=z) &=& \mathbb P(\tilde{Y}_1\in A, \tilde{\varepsilon}=0, \tilde{V} \leq \tilde{P}(z)) + \mathbb P(\tilde{Y}_0\in A, \tilde{\varepsilon}=1, \tilde{V} > \tilde{P}(z)),\\
\mathbb P(Y\in A, D=0 \vert Z=z) &=& \mathbb P(\tilde{Y}_1\in A, \tilde{\varepsilon}=1, \tilde{V} \leq \tilde{P}(z)) + \mathbb P(\tilde{Y}_0\in A, \tilde{\varepsilon}=0, \tilde{V} > \tilde{P}(z)),
\end{eqnarray*}
for all $z$, and all Borel set $A$.

Now, we are going to show that $\mathbb E[\tilde{Y}_1-\tilde{Y}_0 \vert \tilde{P}(0) < \tilde{V} \leq \tilde{P}(1)]=\frac{\mathbb E[Y\vert Z=1]-\mathbb E[Y\vert Z=0]}{LB^1_p(0,1)}$. From the last equations, we have
\begin{eqnarray*}
\mathbb P(Y\in A \vert Z=z) &=& \mathbb P(\tilde{Y}_1\in A, \tilde{V} \leq \tilde{P}(z)) + \mathbb P(\tilde{Y}_0\in A, \tilde{V} > \tilde{P}(z)).
\end{eqnarray*}
Therefore,
\begin{eqnarray*}
\mathbb P(Y\in A \vert Z=1) - \mathbb P(Y\in A \vert Z=0) &=& \mathbb P(\tilde{Y}_1\in A, \tilde{P}(0) < \tilde{V} \leq \tilde{P}(1))\\
&&\qquad \qquad - \mathbb P(\tilde{Y}_0\in A, \tilde{P}(0) < \tilde{V} \leq \tilde{P}(z)).
\end{eqnarray*}
Using Bayes' rule, we have
\begin{eqnarray*}
&& \mathbb P(Y\in A \vert Z=1) - \mathbb P(Y\in A \vert Z=0) \\
&&\qquad \qquad \left[\mathbb P(\tilde{Y}_1\in A \vert \tilde{P}(0) < \tilde{V} \leq \tilde{P}(1))- \mathbb P(\tilde{Y}_0\in A \vert \tilde{P}(0) < \tilde{V} \leq \tilde{P}(1))\right] \left(\tilde{P}(1)-\tilde{P}(0)\right).
\end{eqnarray*}
Hence,
\begin{eqnarray*}
\mathbb P(\tilde{Y}_1\in A \vert \tilde{P}(0) < \tilde{V} \leq \tilde{P}(1))- \mathbb P(\tilde{Y}_0\in A \vert \tilde{P}(0) < \tilde{V} \leq \tilde{P}(1))=\frac{\mathbb P(Y\in A \vert Z=1) - \mathbb P(Y\in A \vert Z=0)}{\tilde{P}(1)-\tilde{P}(0)} 
\end{eqnarray*}
Finally, the expectation version of the result also holds, and we obtain
\begin{eqnarray*}
\mathbb E[\tilde{Y}_1 -\tilde{Y}_0 \vert \tilde{P}(0) < \tilde{V} \leq \tilde{P}(1)]=\frac{\mathbb E[Y \vert Z=1] - \mathbb E[Y\vert Z=0]}{LB^1_p(0,1)}. 
\end{eqnarray*}
We have just shown that one of the bounds for the $LATE$ is achieved. Next, we will show that the other bound $\mathbb E[Y \vert Z=1] - \mathbb E[Y\vert Z=0]$  is also achievable. This case implies that $P(1)-P(0)=1$, which is possible only when $P(1)=1$ and $P(0)=0$, that is, there is full compliance: $D^*=Z$. This is possible if $\mathbb P(D=0\vert Z=1)=\mathbb P(D=1\vert Z=0)$. Indeed, $\tilde{P}(1)=1$ and $\tilde{P}(0)=0$ imply that $\mathbb P(D=1\vert Z=0)=\mathbb P(\tilde{\varepsilon}=1)$, and $\mathbb P(D=0\vert Z=1)=\mathbb P(\tilde{\varepsilon}=1)$.
 
\section{Proof of sharpness under non-differential measurement error} \label{sharpnessproof}
\begin{proof}
For each $\alpha \in (0,1)$, we need to find a joint distribution on the vector $\left(\tilde{Y}_1, \tilde{Y}_0, \tilde{V}, \tilde{\varepsilon}, Z\right)$, such that it satisfies model (\ref{seq1}) and Assumptions \ref{RA}, \ref{Cont}, and \ref{NDE}, and induces the joint distribution on $(Y,D,Z)$. For any function $P(z)$ satisfying the constraints in (\ref{eq:P1})-(\ref{eq:P7}), define:  
\begin{eqnarray*}
\mathbb P(\tilde{\varepsilon}=1 \vert Z=z)&=& \alpha,\\
f_{\tilde{V}\vert \tilde{\varepsilon}=0, Z=z}(p)  &=&\frac{1+ \frac{\partial \mathbb P(D=1 \vert P(Z)=p) }{\partial p}}{2(1-\alpha)},\\
f_{\tilde{V}\vert \tilde{\varepsilon}=1, Z=z}(p)  &=&\frac{1- \frac{\partial \mathbb P(D=1 \vert P(Z)=p) }{\partial p}}{2\alpha},\\
\mathbb P(\tilde{Y}_1 \leq y \vert \tilde{V}=p,\tilde{\varepsilon}=1, Z=z) &=& \frac{\alpha f_{\tilde{V}\vert \tilde{\varepsilon}=1}(p)\kappa_0(y;p)-\left(1-\alpha\right)f_{\tilde{V}\vert \tilde{\varepsilon}=0}(p)\kappa_1(y;p)}{\alpha f_{\tilde{V}\vert \tilde{\varepsilon}=1}(p)-\left(1-\alpha\right)f_{\tilde{V}\vert \tilde{\varepsilon}=0}(p)},\\
\mathbb P(\tilde{Y}_1 \leq y \vert \tilde{V}=p,\tilde{\varepsilon}=0, Z=z) &=& \frac{\alpha f_{\tilde{V}\vert \tilde{\varepsilon}=1}(p)\kappa_0(y;p)-\left(1-\alpha\right)f_{\tilde{V}\vert \tilde{\varepsilon}=0}(p)\kappa_1(y;p)}{\alpha f_{\tilde{V}\vert \tilde{\varepsilon}=1}(p)-\left(1-\alpha\right)f_{\tilde{V}\vert \tilde{\varepsilon}=0}(p)},\\
\mathbb P(\tilde{Y}_0 \leq y \vert \tilde{V}=p,\tilde{\varepsilon}=1, Z=z) &=& \frac{\left(1-\alpha\right) f_{\tilde{V}\vert \tilde{\varepsilon}=0}(p)\kappa_0(y;p)-\alpha f_{\tilde{V}\vert \tilde{\varepsilon}=1}(p)\kappa_1(y;p)}{\alpha f_{\tilde{V}\vert \tilde{\varepsilon}=1}(p)-\left(1-\alpha\right)f_{\tilde{V}\vert \tilde{\varepsilon}=0}(p)},\\
\mathbb P(\tilde{Y}_0 \leq y \vert \tilde{V}=p,\tilde{\varepsilon}=0, Z=z) &=& \frac{\left(1-\alpha\right) f_{\tilde{V}\vert \varepsilon=0}(p)\kappa_0(y;p)-\alpha f_{\tilde{V}\vert \varepsilon=1}(p)\kappa_1(y;p)}{\alpha f_{\tilde{V}\vert \tilde{\varepsilon}=1}(p)-\left(1-\alpha\right)f_{\tilde{V}\vert \varepsilon=0}(p)},
\end{eqnarray*}
where $\kappa_1(y;p)=\frac{\partial \mathbb P(Y\leq y, D=1 \vert P(Z)=p)}{\partial p}$, and $\kappa_0(y;p)=\frac{\partial \mathbb P(Y\leq y, D=0 \vert P(Z)=p) }{\partial p}$.
Define $$\mathbb P(\tilde{Y}_0 \leq y_0, \tilde{Y}_1 \leq y_1 \vert \tilde{V}=p,\varepsilon=\ell, Z=z)=\mathbb P(\tilde{Y}_0 \leq y_0 \vert \tilde{V}=p,\varepsilon=\ell, Z=z)\mathbb P(\tilde{Y}_1 \leq y_1 \vert \tilde{V}=p,\varepsilon=\ell, Z=z).$$ 
It is easy to check that the above quantities are well-defined probabilities/distributions under the constraints (\ref{eq:P1})-(\ref{eq:P9}), and the vector $\left(\tilde{Y}_1, \tilde{Y}_0, \tilde{V}, \tilde{\varepsilon}, Z\right)$ satisfies Assumptions \ref{RA}, \ref{Cont}, and \ref{NDE}.
Define
\begin{eqnarray}\label{seq2}
\left\{ \begin{array}{lcl}
     \tilde{Y}&=&\tilde{Y}_1\tilde{D}^*+\tilde{Y}_0(1-\tilde{D}^*)\\ \\
     \tilde{D}^*&=&\mathbbm{1}\left\{\tilde{V}\leq P(Z)\right\}\\ \\
     \tilde{D}&=&\tilde{D}^* (1-\tilde{\varepsilon}) +(1-\tilde{D}^*) \tilde{\varepsilon}
     \end{array} \right.
\end{eqnarray}
We will now show that the vector $\left(\tilde{Y}, \tilde{D}, Z \right)$ has the same distribution as the vector $(Y,D,Z)$. %
We have
\begin{eqnarray*}
\mathbb P(\tilde{Y}\leq y, \tilde{D}=1 \vert Z=z) &=&\mathbb P(\tilde{Y}\leq y, \tilde{D}=1 \vert P(Z)=P(z)),\\
&=& (1-\alpha) \int^{P(z)}_0\mathbb P\left(\tilde{Y}_1\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =0\right)f_{\tilde{V}\vert \tilde{\varepsilon}=0}(v)dv\\
&& + \alpha \int_{P(z)}^1\mathbb P\left(\tilde{Y}_0\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =1\right)f_{\tilde{V}\vert \tilde{\varepsilon}=1}(v)dv,\\
&=& (1-\alpha) \int^{1}_0 \mathbb P\left(\tilde{Y}_1\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =0\right)f_{\tilde{V}\vert \tilde{\varepsilon}=0}(v)dv\\
&& -(1-\alpha) \int^{1}_{P(z)} \mathbb P\left(\tilde{Y}_1\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =0\right)f_{\tilde{V}\vert \tilde{\varepsilon}=0}(v)dv\\
&& + \alpha \int_{P(z)}^1\mathbb P\left(\tilde{Y}_0\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =1\right)f_{\tilde{V}\vert \tilde{\varepsilon}=1}(v)dv,\\
&=& (1-\alpha) \int^{1}_0 \mathbb P\left(\tilde{Y}_1\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =0\right)f_{\tilde{V}\vert \tilde{\varepsilon}=0}(v)dv\\
&& + \int^{1}_{P(z)} -(1-\alpha) f_{\tilde{V}\vert \tilde{\varepsilon}=0}(v) \mathbb P\left(\tilde{Y}_1\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =0\right)\\
&& + \alpha f_{\tilde{V}\vert \tilde{\varepsilon}=1}(v) \mathbb P\left(\tilde{Y}_0\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =1\right)dv,
\end{eqnarray*}
where the first equality holds from Equation (\ref{eq:P7}).
Given the definition of our DGP, we have
\begin{eqnarray*}
&& -(1-\alpha) f_{\tilde{V}\vert \tilde{\varepsilon}=0}(v) \mathbb P\left(\tilde{Y}_1\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =0\right)+ \alpha f_{\tilde{V}\vert \tilde{\varepsilon}=1}(v) \mathbb P\left(\tilde{Y}_0\leq y \vert \tilde{V}=v, \tilde{\varepsilon}=1\right)\\
&& \qquad = - \kappa_1(y;v)= - \frac{\partial \mathbb P(Y\leq y, D=1 \vert P(Z)=v)}{\partial v}.
\end{eqnarray*}
Therefore,
\begin{eqnarray*}
&&\int^{1}_{P(z)} -(1-\alpha) f_{\tilde{V}\vert \tilde{\varepsilon}=0}(v) \mathbb P\left(\tilde{Y}_1\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =0\right) + \alpha f_{\tilde{V}\vert \tilde{\varepsilon}=1}(v) \mathbb P\left(\tilde{Y}_0\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =1\right)dv\\
&& \qquad = \mathbb P(Y\leq y, D=1 \vert P(Z)=P(z))-\mathbb P(Y\leq y, D=1 \vert P(Z)=1),\\
&& \qquad = \mathbb P(Y\leq y, D=1 \vert Z=z)-\mathbb P(Y\leq y, D=1 \vert P(Z)=1)
\end{eqnarray*}
At this point, it remains to show that 
\begin{eqnarray*}
 \mathbb P(Y\leq y, D=1 \vert P(Z)=1) = (1-\alpha) \int^{1}_0 \mathbb P\left(\tilde{Y}_1\leq y \vert \tilde{V}=v, \tilde{\varepsilon} =0\right)f_{\tilde{V}\vert \tilde{\varepsilon}=0}(v)dv.
\end{eqnarray*}
This equality holds from condition (\ref{eq:P8}).

Similarly, $\mathbb P(\tilde{Y}\leq y, \tilde{D}=0 \vert Z=z)=\mathbb P(Y\leq y, D=0 \vert Z=z)$.
\end{proof}

\section{Proof of Lemma \ref{symmetry}}\label{apx:prop_robust}
\begin{proof}\label{proof:symmetry}
Suppose that $\alpha_0(z)=\alpha_0$ and $\alpha_1(z)=\alpha_1$ for all $z$. Then 
\begin{eqnarray*}
\mathbb P(\varepsilon =1)&=& \mathbb P(\varepsilon=1\vert Z=z),\\
\mathbb P(\varepsilon =1)&=& \mathbb P(D^*=1 \vert Z=z) \mathbb P(\varepsilon=1\vert D^*=1, Z=z) + \mathbb P(D^*=0 \vert Z=z) \mathbb P(\varepsilon=1\vert D^*=0, Z=z),\\
\mathbb P(\varepsilon =1)&=& \alpha_1(z) \mathbb P(D^*=1 \vert Z=z)  + \alpha_0(z) \mathbb P(D^*=0 \vert Z=z),\\
\mathbb P(\varepsilon =1)&=& \alpha_1 \mathbb P(D^*=1 \vert Z=z)  + \alpha_0 \mathbb P(D^*=0 \vert Z=z),\\
\mathbb P(\varepsilon =1)&=& \alpha_1 \mathbb P(D^*=1 \vert Z=z)  + \alpha_0 (1-\mathbb P(D^*=1 \vert Z=z)),\\
\mathbb P(\varepsilon =1)&=&( \alpha_1-\alpha_0) \mathbb P(D^*=1 \vert Z=z)  + \alpha_0,
\end{eqnarray*}
where the first equality holds from Assumption \ref{RA}, the second holds from the law of total probability and Bayes' rule, the third holds from the definition of $\alpha_0(z)$ and $\alpha_1(z)$, and the fourth holds from our assumption that $\alpha_0(z)$ and $\alpha_1(z)$ are constant across $z$. 

If $\alpha_0 \neq \alpha_1$, then $\mathbb P(D^*=1 \vert Z=z)=\frac{\mathbb P(\varepsilon=1)-\alpha_0}{\alpha_1-\alpha_0},$ which is constant across $z$, which contradicts the relevance condition that $P(z)$ is a nontrivial function of $z$, since $P(z)=\mathbb P(D^*=1 \vert Z=z)$ under Assumption \ref{RA}. Therefore, $\alpha_0=\alpha_1.$ Hence, $\alpha_0(z)=\alpha_1(z)=\alpha.$

Suppose now that the misclassification is symmetric in the sense that $\alpha_0(z)=\alpha_1(z)=\alpha(z)$. Then, a similar derivation as above yields
\begin{eqnarray*}
\mathbb P(\varepsilon =1)&=& \alpha_1(z) \mathbb P(D^*=1 \vert Z=z)  + \alpha_0(z) \mathbb P(D^*=0 \vert Z=z),\\
\mathbb P(\varepsilon =1)&=& \alpha(z) \mathbb P(D^*=1 \vert Z=z)  + \alpha(z) \mathbb P(D^*=0 \vert Z=z),\\
\mathbb P(\varepsilon =1)&=& \alpha(z)\left[\mathbb P(D^*=1 \vert Z=z)  + \mathbb P(D^*=0 \vert Z=z)\right],\\
\mathbb P(\varepsilon =1)&=& \alpha(z)
\end{eqnarray*} 
From the last equality, we deduce that $\alpha(z)=\mathbb P(\varepsilon =1)=\alpha.$ Therefore, $\alpha_0(z)=\alpha_1(z)=~\alpha.$
\end{proof}

\section{Allowing for dependence between misclassification and IV}\label{apx:zeps}

Similarly to Equation \ref{eq:mte}, we have under $Z\ \indep\ (Y_d, V)$, $d=0,1$:
\begin{eqnarray*}
f_{Y \vert Z}\left(y\vert z\right)
&=&  \int^{P(z)}_0 f_{Y_1\vert V}\left(y \vert v \right)dv + \int_{P(z)}^1f_{Y_0 \vert V}\left(y \vert v \right)dv.
\end{eqnarray*}
Hence, for any $P(z') < P(z)$ we have
\begin{eqnarray*}
f_{Y \vert Z}\left(y\vert z\right) - f_{Y\vert Z}\left(y \vert z'\right)&=& \int^{P(z)}_{P(z')}f_{Y_1\vert V}(y \vert v)dv\\
&&\qquad \qquad - \int^{P(z)}_{P(z')}f_{Y_0 \vert V}(y \vert v)dv,
\end{eqnarray*}
Using the triangle inequality, we have
\begin{eqnarray*}
\left \lvert f_{Y \vert Z}\left(y \vert z\right) - f_{Y \vert Z}\left(y \vert z'\right) \right \rvert  &\leq&  \int^{P(z)}_{P(z')}f_{Y_1 \vert V}(y \vert v)dv\\
 && \qquad + \int^{P(z)}_{P(z')}f_{Y_0\vert V}(y \vert v)dv.
\end{eqnarray*}
Therefore, by integrating each side over the support $\mathcal Y$ and using the Fubini-Tonelli theorem, we have
\begin{eqnarray*}
\int_{\mathcal Y}\left \lvert f_{Y \vert Z}\left(y \vert z\right) - f_{Y \vert Z}\left(y \vert z'\right) \right \rvert d \mu_{Y}(y) &\leq&  \int^{P(z)}_{P(z')} dv+\int^{P(z)}_{P(z')}dv= 2\left(P(z)-P(z')\right).
\end{eqnarray*}
Hence, we have $TV_{Y}(z',z) \leq P(z)-P(z') \leq 1$, where 
\begin{eqnarray*}
TV_{Y}(z',z) \equiv \frac{1}{2} \int_{\mathcal Y}\left \lvert f_{Y \vert Z}\left(y \vert z\right) - f_{Y \vert Z}\left(y \vert z'\right) \right \rvert d \mu_{Y}(y).
\end{eqnarray*}
Then, we have 
\begin{eqnarray*}
&& \min\left\{\frac{\Delta_{YZ}(z',z)}{TV_{Y}(z',z)},  \Delta_{YZ}(z',z) \right\}\\
&& \qquad \qquad \leq \frac{\mathbb E[Y\vert P(Z)=P(z)]-\mathbb E[Y\vert P(Z)=P(z')]}{P(z)-P(z')} \leq \\
&& \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \max\left\{\frac{\Delta_{YZ}(z',z)}{TV_{Y}(z',z)},  \Delta_{YZ}(z',z)\right\}.
\end{eqnarray*}
Therefore, the following bounds hold for the MTE:
\begin{eqnarray}
&& \min\left\{\lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{TV_{Y}(z',z)}, 0 \right\} \nonumber \\
&& \qquad \qquad \leq MTE(P(z))\leq \\
&& \qquad \qquad \qquad \qquad \max\left\{\lim_{z' \rightarrow z} \frac{\Delta_{YZ}(z',z)}{TV_{Y}(z',z)},  0\right\}. \nonumber
\end{eqnarray}
These bounds are wider those derived in Subsection \ref{anabounds} under Assumptions \ref{RA}-\ref{Bound:mis}.

\clearpage
\section{Additional empirical results}\label{apx:sup}
\begin{figure}[h]
    \begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{0218_condcdf.png}
    \centering
    \caption{Illustration of $F_{V | \varepsilon = 1 }$ under Various Specifications}
    \label{fig.condcdf}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{Export_01042022_F0304.PNG}
    \centering
    \caption{95\% Confidence Region for MTE ($\varepsilon = \mathbbm{1} \{ V \leq \alpha \}$)}
    \label{fig.f0304}
    \end{minipage}
\end{figure}


\begin{table}[!htbp] \centering 
  \caption{Estimated Bounds of Average Returns to Upper Secondary Schooling ($\varepsilon = \mathbbm{1} \{ V \leq \alpha \}$)} 
  \label{tab:ar3} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{ATE}&\multicolumn{2}{c}{ATT}&\multicolumn{2}{c}{ATU} \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	&  $0.083$ & $0.170$ & $0.067$ & $0.259$ & $0.075$ & $0.138$ \\ 
									& 0.1	&  $0.110$ & $0.170$ & $0.092$ & $0.259$ & $0.075$ & $0.138$ \\ 
									& 0.05	&  $0.137$ & $0.161$ & $0.215$ & $0.257$ & $0.075$ & $0.084$ \\ 
									& 0.025	&  $0.137$ & $0.148$ & $0.215$ & $0.234$ & $0.078$ & $0.084$ \\ 			\hline \\
\multirow{4}{*}{\texttt{nprobust}}	& 0.139	&  $0.009$ & $0.295$ & $$-$0.050$ & $0.817$ & $$-$0.272$ & $0.115$ \\ 
									& 0.1 	&  $0.071$ & $0.295$ & $0.051$ & $0.817$ & $$-$0.272$ & $0.115$ \\ 
									& 0.05 	&  $0.134$ & $0.295$ & $0.478$ & $0.817$ & $$-$0.272$ & $$-$0.127$ \\ 
									& 0.025 &  $0.134$ & $0.286$ & $0.601$ & $0.767$ & $$-$0.273$ & $$-$0.082$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 

\begin{table}[!htbp] \centering 
  \caption{95\% Confidence Intervals of Average Returns to Upper Secondary Schooling ($\varepsilon = \mathbbm{1} \{ V \leq \alpha \}$)} 
  \label{tab:ar3ci} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{ATE}&\multicolumn{2}{c}{ATT}&\multicolumn{2}{c}{ATU} \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	& $$-$0.004$ & $0.269$ & $$-$0.085$ & $0.435$ & $$-$0.134$ & $0.304$ \\ 
									& 0.1	& $0.019$ & $0.269$ & $$-$0.062$ & $0.435$ & $$-$0.134$ & $0.304$ \\ 
									& 0.05	& $0.022$ & $0.262$ & $0.045$ & $0.434$ & $$-$0.134$ & $0.299$ \\ 
									& 0.025	& $0.022$ & $0.255$ & $0.045$ & $0.410$ & $$-$0.134$ & $0.299$ \\ 		\hline \\
\multirow{4}{*}{\texttt{nprobust}}	& 0.139	& $$-$0.187$ & $0.606$ & $$-$0.266$ & $1.420$ & $$-$0.771$ & $0.385$ \\ 
									& 0.1 	& $$-$0.187$ & $0.606$ & $$-$0.178$ & $1.420$ & $$-$0.771$ & $0.385$ \\  
									& 0.05 	& $$-$0.187$ & $0.606$ & $$-$0.013$ & $1.420$ & $$-$0.771$ & $0.321$ \\  
									& 0.025 & $$-$0.187$ & $0.596$ & $$-$0.013$ & $1.379$ & $$-$0.771$ & $0.321$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 

\begin{table}[!htbp] \centering 
  \caption{Estimated Bounds of AMTE to Upper Secondary Schooling ($\varepsilon = \mathbbm{1} \{ V \leq \alpha \}$)} 
  \label{tab:amte3} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{$\zeta=$0.1}&\multicolumn{2}{c}{$\zeta=$0.05}&\multicolumn{2}{c}{$\zeta=$0.01}& \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	&   $0.085$ & $0.167$ & $0.085$ & $0.166$ & $0.084$ & $0.166$ \\ 
									& 0.1	&   $0.109$ & $0.167$ & $0.109$ & $0.166$ & $0.109$ & $0.166$ \\  
									& 0.05	&   $0.155$ & $0.165$ & $0.155$ & $0.164$ & $0.155$ & $0.163$ \\  
									& 0.025	&   $0.155$ & $0.160$ & $0.155$ & $0.159$ & $0.155$ & $0.159$ \\   \hline \\
\multirow{4}{*}{\texttt{nprobust}}	& 0.139	&   $0.076$ & $0.197$ & $0.044$ & $0.155$ & $0.015$ & $0.148$ \\ 
									& 0.1 	&   $0.117$ & $0.197$ & $0.106$ & $0.155$ & $0.092$ & $0.148$ \\  
									& 0.05 	&   $0.123$ & $0.197$ & $0.106$ & $0.155$ & $0.096$ & $0.148$ \\  
									& 0.025 &   $0.179$ & $0.196$ & $0.146$ & $0.156$ & $0.137$ & $0.148$ \\  
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}  

\begin{table}[!htbp] \centering 
  \caption{95\% Confidence Intervals of AMTE to Upper Secondary Schooling ($\varepsilon = \mathbbm{1} \{ V \leq \alpha \}$)} 
  \label{tab:amte3ci} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{$\zeta=$0.1}&\multicolumn{2}{c}{$\zeta=$0.05}&\multicolumn{2}{c}{$\zeta=$0.01}& \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	& $0.003$ & $0.259$ & $0.002$ & $0.257$ & $0.001$ & $0.257$ \\ 
									& 0.1	& $0.024$ & $0.259$ & $0.024$ & $0.257$ & $0.023$ & $0.257$ \\ 
									& 0.05	& $0.060$ & $0.259$ & $0.060$ & $0.257$ & $0.060$ & $0.257$ \\ 
									& 0.025	& $0.060$ & $0.254$ & $0.060$ & $0.253$ & $0.060$ & $0.253$ \\ 			\hline \\
\multirow{4}{*}{\texttt{nprobust}}	& 0.139	& $$-$0.032$ & $0.358$ & $$-$0.086$ & $0.304$ & $$-$0.165$ & $0.292$ \\  
									& 0.1 	& $0.001$ & $0.358$ & $$-$0.025$ & $0.304$ & $$-$0.073$ & $0.292$ \\ 
									& 0.05 	& $0.001$ & $0.358$ & $$-$0.025$ & $0.304$ & $$-$0.053$ & $0.292$ \\   
									& 0.025 & $0.003$ & $0.366$ & $$-$0.005$ & $0.307$ & $$-$0.011$ & $0.293$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}  


\begin{table}[!htbp] \centering 
  \caption{Point Estimate Bounds and 95\% Confidence Bounds of PRTE to Upper Secondary Schooling ($\varepsilon = \mathbbm{1} \{ V \leq \alpha \}$)} 
  \label{tab:prte03} 
\begin{tabular}{@{\extracolsep{5pt}} cc|ccccc} 
\\[-1.8ex]\hline 
\hline \\
\multirow{2}{*}{$a$}&\multirow{2}{*}{$\overline{\alpha}$}&\multicolumn{3}{c}{Point Estimates}&\multicolumn{2}{c}{Confidence Bounds} \\
&& $\alpha=0$  & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{$0.05$}	&0.139 & \multirow{4}{*}{$0.3762$} 	& $0.2333$ & $0.3762$ & $0.1193$ & $0.4702$ \\
						&0.1 &								& $0.2489$ & $0.3762$ & $0.1420$ & $0.4702$ \\ 
						&0.05 &  							& $0.2963$ & $0.3762$ & $0.1975$ & $0.4702$ \\ 
						&0.025 &							& $0.3353$ & $0.3762$ & $0.2394$ & $0.4702$ \\  \hline  \\
\multirow{4}{*}{$0.1$}	&0.139 & \multirow{4}{*}{$0.3792$} 	& $0.2341$ & $0.3792$ & $0.1209$ & $0.4742$ \\ 
						&0.1 &								& $0.2493$ & $0.3792$ & $0.1430$ & $0.4742$ \\ 
						&0.05 & 							& $0.2997$ & $0.3792$ & $0.1999$ & $0.4742$ \\ 
						&0.025 &  							& $0.3389$ & $0.3792$ & $0.2418$ & $0.4742$ \\  \hline  \\
\multirow{4}{*}{$0.15$}	&0.139 & \multirow{4}{*}{$0.3825$}	& $0.2354$ & $0.3825$ & $0.1210$ & $0.4786$ \\ 
						&0.1 &								& $0.2505$ & $0.3825$ & $0.1440$ & $0.4786$ \\ 
						&0.05 & 							& $0.3039$ & $0.3825$ & $0.2029$ & $0.4786$ \\ 
						&0.025 &							& $0.3430$ & $0.3825$ & $0.2446$ & $0.4786$ \\  \hline  \\
\multirow{4}{*}{$0.2$}	&0.139 & \multirow{4}{*}{$0.3861$} 	& $0.2358$ & $0.3861$ & $0.1215$ & $0.4834$ \\ 
						&0.1 &								& $0.2526$ & $0.3861$ & $0.1450$ & $0.4834$ \\ 
						&0.05 &  							& $0.3088$ & $0.3861$ & $0.2063$ & $0.4834$ \\ 
						&0.025 & 							& $0.3475$ & $0.3861$ & $0.2476$ & $0.4834$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}  

\begin{figure}[h]
    \begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth]{Export_01042022_F0506.PNG}
    \centering
    \caption{95\% Confidence Region for MTE ($\varepsilon = \mathbbm{1} \{ V > 1 - \alpha \}$)}
    \label{fig.f0506}
    \end{minipage}
\end{figure}


\begin{table}[!htbp] \centering 
  \caption{Estimated Bounds of Average Returns to Upper Secondary Schooling ($\varepsilon = \mathbbm{1} \{ V > 1 - \alpha \}$)} 
  \label{tab:ar5} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{ATE}&\multicolumn{2}{c}{ATT}&\multicolumn{2}{c}{ATU} \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	& $$-$0.006$ & $0.137$ & $0.110$ & $0.215$ & $$-$0.123$ & $0.112$ \\ 
									& 0.1	& $$-$0.006$ & $0.137$ & $0.139$ & $0.215$ & $$-$0.123$ & $0.082$ \\ 
									& 0.05	& $0.065$ & $0.137$ & $0.196$ & $0.215$ & $$-$0.012$ & $0.082$ \\ 
									& 0.025	& $0.121$ & $0.137$ & $0.206$ & $0.215$ & $0.066$ & $0.082$ \\ 	\hline \\
\multirow{4}{*}{\texttt{nprobust}}	& 0.139	& $$-$0.012$ & $0.134$ & $0.236$ & $0.702$ & $$-$0.359$ & $$-$0.024$ \\ 
									& 0.1 	& $$-$0.012$ & $0.134$ & $0.236$ & $0.702$ & $$-$0.359$ & $$-$0.027$ \\ 
									& 0.05 	& $0.011$ & $0.134$ & $0.337$ & $0.702$ & $$-$0.359$ & $$-$0.146$ \\ 
									& 0.025 & $0.011$ & $0.134$ & $0.520$ & $0.702$ & $$-$0.359$ & $$-$0.272$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 

\begin{table}[!htbp] \centering 
  \caption{95\% Confidence Intervals of Average Returns to Upper Secondary Schooling ($\varepsilon = \mathbbm{1} \{ V > 1 - \alpha \}$)} 
  \label{tab:ar5ci} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{ATE}&\multicolumn{2}{c}{ATT}&\multicolumn{2}{c}{ATU} \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	& $$-$0.162$ & $0.260$ & $$-$0.021$ & $0.384$ & $$-$0.389$ & $0.320$ \\
									& 0.1	& $$-$0.162$ & $0.260$ & $$-$0.003$ & $0.384$ & $$-$0.389$ & $0.320$ \\ 
									& 0.05	& $$-$0.083$ & $0.260$ & $0.036$ & $0.384$ & $$-$0.273$ & $0.320$ \\ 
									& 0.025	& $$-$0.008$ & $0.254$ & $0.044$ & $0.384$ & $$-$0.170$ & $0.305$ \\ \hline \\
\multirow{4}{*}{\texttt{nprobust}}	& 0.139	& $$-$0.318$ & $0.455$ & $0.056$ & $1.215$ & $$-$0.898$ & $0.252$ \\ 
									& 0.1 	& $$-$0.318$ & $0.455$ & $0.056$ & $1.215$ & $$-$0.898$ & $0.252$ \\ 
									& 0.05 	& $$-$0.318$ & $0.455$ & $0.095$ & $1.215$ & $$-$0.898$ & $0.252$ \\ 
									& 0.025 & $$-$0.318$ & $0.455$ & $0.189$ & $1.215$ & $$-$0.898$ & $0.226$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table} 

\begin{table}[!htbp] \centering 
  \caption{Estimated Bounds of AMTE to Upper Secondary Schooling ($\varepsilon = \mathbbm{1} \{ V > 1 - \alpha \}$)} 
  \label{tab:amte5} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{$\zeta=$0.1}&\multicolumn{2}{c}{$\zeta=$0.05}&\multicolumn{2}{c}{$\zeta=$0.01}& \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	& $0.081$ & $0.155$ & $0.081$ & $0.155$ & $0.082$ & $0.155$ \\
									& 0.1	& $0.089$ & $0.155$ & $0.093$ & $0.155$ & $0.094$ & $0.155$ \\
									& 0.05	& $0.120$ & $0.155$ & $0.123$ & $0.155$ & $0.124$ & $0.155$ \\
									& 0.025	& $0.148$ & $0.155$ & $0.149$ & $0.155$ & $0.149$ & $0.155$ \\ \hline \\
\multirow{4}{*}{\texttt{nprobust}}	& 0.139	& $0.118$ & $0.180$ & $0.123$ & $0.190$ & $0.130$ & $0.234$ \\
									& 0.1 	& $0.118$ & $0.180$ & $0.123$ & $0.164$ & $0.130$ & $0.171$ \\
									& 0.05 	& $0.142$ & $0.180$ & $0.137$ & $0.164$ & $0.132$ & $0.159$ \\
									& 0.025 & $0.142$ & $0.180$ & $0.137$ & $0.156$ & $0.132$ & $0.153$ \\
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}  

\begin{table}[!htbp] \centering 
  \caption{95\% Confidence Intervals of AMTE to Upper Secondary Schooling ($\varepsilon = \mathbbm{1} \{ V > 1 - \alpha \}$)} 
  \label{tab:amte5ci} 
\begin{tabular}{@{\extracolsep{5pt}} cc|cccccccccc} 
\\[-1.8ex]\hline 
\hline \\
&&\multicolumn{2}{c}{$\zeta=$0.1}&\multicolumn{2}{c}{$\zeta=$0.05}&\multicolumn{2}{c}{$\zeta=$0.01}& \\
&$\overline{\alpha}$& LB & UB & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{Gaussian}			& 0.139	& $$-$0.009$ & $0.250$ & $$-$0.009$ & $0.250$ & $$-$0.010$ & $0.250$ \\ 
									& 0.1	& $$-$0.009$ & $0.250$ & $$-$0.005$ & $0.250$ & $$-$0.004$ & $0.250$ \\  
									& 0.05	& $0.023$ & $0.250$ & $0.026$ & $0.250$ & $0.027$ & $0.250$ \\ 
									& 0.025	& $0.052$ & $0.250$ & $0.053$ & $0.250$ & $0.054$ & $0.250$ \\ 	\hline \\
\multirow{4}{*}{\texttt{nprobust}}	& 0.139	& $$-$0.016$ & $0.350$ & $$-$0.011$ & $0.320$ & $$-$0.011$ & $0.403$ \\ 
									& 0.1 	& $$-$0.016$ & $0.350$ & $$-$0.011$ & $0.304$ & $$-$0.011$ & $0.329$ \\ 
									& 0.05 	& $$-$0.016$ & $0.350$ & $$-$0.011$ & $0.304$ & $$-$0.011$ & $0.294$ \\  
									& 0.025 & $$-$0.016$ & $0.350$ & $$-$0.011$ & $0.311$ & $$-$0.011$ & $0.301$ \\  
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}  

\begin{table}[!htbp] \centering 
  \caption{Point Estimate Bounds and 95\% Confidence Bounds of PRTE to Upper Secondary Schooling ($\varepsilon = \mathbbm{1} \{ V > 1 - \alpha \}$)} 
  \label{tab:prte05} 
\begin{tabular}{@{\extracolsep{5pt}} cc|ccccc} 
\\[-1.8ex]\hline 
\hline \\
\multirow{2}{*}{$a$}&\multirow{2}{*}{$\overline{\alpha}$}&\multicolumn{3}{c}{Point Estimates}&\multicolumn{2}{c}{Confidence Bounds} \\
&& $\alpha=0$  & LB & UB & LB & UB \\
\hline \\
\multirow{4}{*}{$0.05$}	&0.139 &\multirow{4}{*}{$0.3762$} 	& $0.3762$ & $0.5345$ & $0.2822$ & $0.6262$ \\ 
						&0.1 &  							& $0.3762$ & $0.4926$ & $0.2822$ & $0.5817$ \\ 
						&0.05 &  							& $0.3762$ & $0.4517$ & $0.2822$ & $0.5419$ \\ 
						&0.025 &							& $0.3762$ & $0.4162$ & $0.2822$ & $0.5082$ \\  \hline  \\
\multirow{4}{*}{$0.1$}	&0.139 &\multirow{4}{*}{$0.3792$} 	& $0.3792$ & $0.5303$ & $0.2842$ & $0.6213$ \\ 
						&0.1 &  							& $0.3792$ & $0.4909$ & $0.2842$ & $0.5799$ \\ 
						&0.05 & 							& $0.3792$ & $0.4526$ & $0.2842$ & $0.5433$ \\ 
						&0.025 &  							& $0.3792$ & $0.4182$ & $0.2842$ & $0.5110$ \\  \hline  \\
\multirow{4}{*}{$0.15$}	&0.139 &\multirow{4}{*}{$0.3825$}	& $0.3825$ & $0.5257$ & $0.2864$ & $0.6160$ \\ 
						&0.1 &  							& $0.3825$ & $0.4889$ & $0.2864$ & $0.5780$ \\ 
						&0.05 & 							& $0.3825$ & $0.4535$ & $0.2864$ & $0.5448$ \\ 
						&0.025 &							& $0.3825$ & $0.4204$ & $0.2864$ & $0.5141$ \\  \hline  \\
\multirow{4}{*}{$0.2$}	&0.139 &\multirow{4}{*}{$0.3861$} 	& $0.3861$ & $0.5206$ & $0.2887$ & $0.6103$ \\ 
						&0.1 &  							& $0.3861$ & $0.4866$ & $0.2887$ & $0.5758$ \\ 
						&0.05 &  							& $0.3861$ & $0.4543$ & $0.2887$ & $0.5464$ \\ 
						&0.025 & 							& $0.3861$ & $0.4227$ & $0.2887$ & $0.5175$ \\ 
\hline \\[-1.8ex] 
\end{tabular} 
\end{table}  


\section{Details on the numerical illustrations}
\subsection{Numerical illustration in Section \ref{numeric1}}\label{numeric:apx1}
In this example, we have:
\begin{eqnarray*}
\mathbb E[Y\vert P(Z)=P(z)] &=& \mathbb E[\beta D^* \vert P(Z)=P(z)]+\mathbb E[U \vert P(Z)=P(z)],\\
&=& \mathbb E[\beta \mathbbm{1}\{V\leq P(z)\}]+\mathbb E[U],\\
&=& \mathbb E[\beta \vert V\leq P(z)] \mathbb P(V\leq P(z))+\mathbb E[U],\\
&=& \int^{P(z)}_{0} \mathbb E[\beta \vert V=v] dv+\mathbb E[U],\\
&=& \int^{P(z)}_{0} \mathbb E[\beta \vert V^*=\Phi^{-1}(v)] dv+\mathbb E[U].
\end{eqnarray*}
Hence, $MTE(P(z))=\mathbb E\left [\beta\vert V^*=\Phi^{-1}(P(z))\right]=-0.5 \Phi^{-1}(P(z))+2$.

Moreover, note that
\begin{equation*}
    \mathbb P(\varepsilon = 1) = \mathbb P(\xi \leq \alpha) = \alpha,
\end{equation*} 
and thus it can be shown that
\begin{eqnarray*}
F_{V \vert \varepsilon=1}(p) &=& \mathbb P(V \leq p  \vert  \varepsilon = 1), \\
&=& \frac{\mathbb P(V \leq p, \xi \leq \alpha)}{\mathbb P (\xi \leq \alpha)}, \\
&=& \alpha ^{-1} \Phi_2 \big[ \Phi^{-1}(p), \Phi^{-1}(\alpha); \rho \big], \\
&\equiv& \alpha ^{-1} C(p, \alpha; \rho),
\end{eqnarray*}
where $\Phi_2 [\cdot, \rho]$ is the bivariate normal distribution function with a correlation coefficient of $\rho$, and $C(p, \alpha; \rho)$ is the bivariate normal copula.

Similarly, we have
\begin{eqnarray*}
F_{V \vert \varepsilon=0}(p) &=& \mathbb P(V \leq p  \vert  \varepsilon = 0), \\
&=& \frac{\mathbb P(V \leq p, \xi > \alpha)}{\mathbb P(\xi > \alpha)}, \\
&=& (1-\alpha) ^{-1} \Big\{ \Phi_2 \big[ \Phi^{-1}(p), \Phi^{-1}(1); \rho \big] - \Phi_2 \big[ \Phi^{-1}(p), \Phi^{-1}(\alpha); \rho \big] \Big\}, \\
&=& (1-\alpha) ^{-1} \Big\{ \Phi \big[ \Phi^{-1}(p) \big] - \Phi_2 \big[ \Phi^{-1}(p), \Phi^{-1}(\alpha); \rho \big] \Big\}, \\
&\equiv& (1-\alpha) ^{-1} \big[ p - C(p, \alpha; \rho) \big],
\end{eqnarray*}
and note that these two conditional distributions satisfy (\ref{eq:mix}).

Thus, using (\ref{eq2}), we have
\begin{eqnarray*}
\mathbb E[D \vert Z=z] %
&=& (1-\alpha) F_{V \vert \varepsilon=0}\big(P(z)\big) + \alpha[1-F_{V \vert \varepsilon=1}\big(P(z)\big)], \\
&=& P(z) + \alpha - 2C(P(z), \alpha; \rho),
\end{eqnarray*}
and
\begin{eqnarray*}
\Delta_{YZ}(z', z) &=& \mathbb E[Y \vert Z=z] - \mathbb E[Y \vert Z=z'], \\
&=& \int^{P(z)}_{P(z')} \mathbb E[\beta \vert V^*=\Phi^{-1}(v)] dv,
\\
\Delta_{DZ}(z', z) &=& \mathbb E[D \vert Z=z] - \mathbb E[D \vert Z=z'], \\
&=& P(z) - P(z') - 2\Big\{ C\big(P(z), \alpha; \rho \big) - C\big(P(z'), \alpha; \rho \big) \Big\},
\end{eqnarray*}

Hence, we have the upper bound of MTE\footnote{Note that this upper bound does not exploit the information from $TV_{(Y,D=d)}(z',z)$.} as follows:\footnote{This implicitly assumes that $P(z)$ is known to be monotone because it states that as $z'$ increases $p'$ also increases (same direction).}
\begin{eqnarray*}
    \lim_{z' \uparrow z} \frac{\Delta_{YZ}(z',z)}{\vert \Delta_{DZ}(z',z) \vert} = \lim_{p' \uparrow p} \frac{\int^{p}_{p'} \mathbb E[\beta \vert V^*=\Phi^{-1}(v)] dv}{\vert p-p'- 2[ C(p, \alpha; \rho ) - C(p', \alpha; \rho ) ] \vert},
\end{eqnarray*}
or using L'H\^{o}pital rule and properties of the copula $C$ \citep{meyer2013bivariate}, 
\begin{eqnarray*}
    \lim_{z' \uparrow z} \frac{\Delta_{YZ}(z',z)}{\vert \Delta_{DZ}(z',z) \vert} = \lim_{p' \uparrow p} \frac{\Delta_{DZ}(z',z)}{\vert \Delta_{DZ}(z',z) \vert} \cdot \frac{\frac{1}{2}\Phi^{-1}(p')-2}{\Big\vert -1 + 2\Phi \big[\frac{\Phi^{-1}(\alpha)-\rho \Phi^{-1}(p')}{\sqrt{1-\rho^2}}\big] \Big\vert},
\end{eqnarray*}
whenever the limit exists.
Note that the limit does not exist if $p = \alpha = \frac{1}{2}$ or if $\Phi^{-1}(\alpha) = \rho \Phi^{-1}(p)$.

\subsection*{MTE bounds and LIV estimand}
\begin{figure}[h]
    \includegraphics[width = 4.6in]{Numerical_Approx_11242021_r0}\\
    \begin{flushleft}
	\footnotesize{* The black line represents the true MTE from the model.}\\
    \footnotesize{* The red circles (blue triangles) are the approximated upper (lower) bound at each grid point of $p$.}\\
     \footnotesize{* The green squares are the local IV estimand at each grid point of $p$.}
    \end{flushleft}
    \centering \caption{Numerical illustration of the MTE bounds and the LIV estimand}
	\label{fig.ey1.exam0921_r0}
\end{figure}
\clearpage

\subsection{Details on the numerical illustration of the special case} \label{numeric2:apx}
We assume in this illustration that the researcher knows that the rate of misclassification $\alpha$ is less than 1/2.
Consider the same example from the previous section (\ref{eq:ex}) where $\rho=0$ (i.e., $\varepsilon$ is independent of $V$). 
Note that
\begin{eqnarray*}
    \mathbb P(D=1\vert Z=z) &=& P(z) + \alpha - 2P(z)\alpha
    \\
    &=& P(z)(1-2\alpha) + \alpha
\end{eqnarray*}
because we have $\lim_{\rho \to 0^+} C(P(z), \alpha; \rho) = P(z)\alpha$ \citep{meyer2013bivariate}.
Hence, the following can be verified for the identification region for $\alpha$:
\begin{eqnarray*}
    \inf_z \mathbb P(D=1\vert Z=z) = \inf_z \mathbb P(D=0\vert Z=z)
    = \alpha,\\
    \sup_z \mathbb P(D=1\vert Z=z) = \sup_z \mathbb P(D=0\vert Z=z)
    =1-\alpha.
\end{eqnarray*}

Moreover, because we have 
\begin{eqnarray*}
    \mathbb E[Y \vert \mathbb P(D=1 \vert Z)=p] &=& \mathbb E[\beta D^* + U \vert P(Z)(1-2\alpha) + \alpha=p] \\
    &=& \mathbb E\bigg[\beta D^* + U \Big\vert Z = \frac{1}{2} \Phi ^{-1} \Big( \frac{1}{1-2\alpha}(p-\alpha)\Big)\bigg] \\
    &=& \int_{0}^{P(\frac{1}{2} \Phi ^{-1} ( \frac{1}{1-2\alpha}(p-\alpha)))} \mathbb E [\beta \vert V^* = \Phi ^{-1}(v) ]dv + \mathbb E[U] \\
    &=& \int_{0}^{\frac{1}{1-2\alpha}(p-\alpha)} \bigg[ -\frac{1}{2} \Phi ^{-1}(v)+2 \bigg] dv
\end{eqnarray*}
and 
\begin{eqnarray*}
    LIV(p) &=& \frac{\partial \mathbb E[Y \vert \mathbb P(D=1 \vert Z)=p]}{\partial p} \\
    &=& \frac{1}{1-2\alpha}\bigg( -\frac{1}{2} \Phi ^{-1} \Big( \frac{1}{1-2\alpha}(p-\alpha)\Big) + 2 \bigg)
\end{eqnarray*}
for $\alpha \neq \frac{1}{2}$, we verify the true MTE lies within the identification region as follows:
\begin{eqnarray*}
    MTE(p;\tilde{\alpha})&=& (1-2\tilde{\alpha}) LIV \big( (1-2\tilde{\alpha})p + \tilde{\alpha} \big) \\
    &=& \frac{1-2\tilde{\alpha}}{1-2\alpha}\bigg( -\frac{1}{2} \Phi ^{-1} \Big( \frac{1}{1-2\alpha}\big( (1-2\tilde{\alpha})p + \tilde{\alpha} -\alpha \big)\Big) + 2 \bigg) \\
    &=& - \frac{1}{2}\Phi ^{-1}(p) + 2 \quad \textrm{if} \quad \tilde{\alpha} = \alpha
\end{eqnarray*}

\clearpage
\bibliographystyle{jpe}
\bibliography{mybib}

\end{document}