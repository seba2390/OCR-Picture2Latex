\section{The Semantic Urban Mesh Dataset}\label{sec:framework}
\subsection{Dataset Specification}

We have used Helsinki's 3D texture meshes as input and annotated them as a benchmark dataset of semantic urban meshes. 
The Helsinki's raw dataset covers about 12 $ km^2 $, and it was generated in 2017 from oblique aerial images that have about a 7.5 $cm$  ground sampling distance (GSD) using an off-the-shelf commercial software namely ContextCapture~\citep{contextcap}.
The source images have three colour channels (i.e., red, green, and blue) and are collected from an airplane with five cameras that have $80\%$ length coverage and $60\%$ side coverage.
To recover the 3D water bodies that do not fulfil the Lambertian hypothesis, 2D vector maps and ortho-photos are used when performing the surface reconstruction.
Furthermore, processing like aerial triangulation, dense image matching, and mesh surface reconstruction were all performed with ContextCapture.
It should be noticed that the entire region of Helsinki is split into tiles, and each of them covers about 250 $ m^2 $~\citep{kalasatamaReport}.
As shown in Figure \ref{fig:overview},  we have selected the central region of Helsinki as the study area, which includes 64 tiles and covers about 4 $km^2$ map area (8 $km^2$ surface area) in total.   

\subsection{Object Classes}
We define the semantic categories for urban meshes by the most common objects in the urban environment with unambiguous geometry and texture appearance.
Moreover, each triangle face is assigned to a label of one of the six semantic classes. 
Ambiguous regions (which account for about 2.6\% of the total mesh surface area), such as shadowed regions or distorted surfaces, are labelled as unclassified (see Figure \ref{fig:ambigious}).
The object classes we consider in the benchmark dataset are: 
\begin{itemize}
	\item \textbf{terrain}: roads, bridges, grass fields, and impervious surfaces;
	\item \textbf{building}: houses,high-rises, monuments, and security booths;
	\item \textbf{high vegetation}: trees, shrubs, and bushes;
	\item \textbf{water}: rivers, sea, and pools;
	\item \textbf{vehicle}: cars, buses, and lorries;  
	\item \textbf{boat}: boats, ships, freighters, and sailboats;
	\item \textbf{unclassified}: incomplete objects like buses and trains, distorted surfaces like tables, tents and facades, construction sites, underground walls.
\end{itemize}

\begin{figure}[!tb]
	\includegraphics[height=0.48\textwidth]{figures/overview_grids/yaxis.png}
	\begin{subfigure}[t]{0.48\textwidth}
		\includegraphics[width=\linewidth]{figures/overview_grids/texture_global_birdsview00.png}
		\includegraphics[width=\linewidth]{figures/overview_grids/xaxis.png}
		\label{fig:textop}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.48\textwidth}		
		\includegraphics[width=\linewidth]{figures/overview_grids/semantic_global_birdsview00.png}
		\vspace*{-0.78cm}
		\begin{center}
		\includegraphics[width=0.8\linewidth]{figures/semantic_results/semantic_legend2.png}
		\end{center}
		\label{fig:semtop}
	\end{subfigure}
	\vspace*{-0.7cm}
	\caption{Overview of the semantic urban mesh benchmark.
	Left: the texture meshes covering about 4 $km^2$ map area. Right: the ground truth meshes.
	More views of the same scene (with different visualization styles) are shown in Figures \ref{fig:texside} and \ref{fig:semside}.}
	\label{fig:overview}
\end{figure}

\begin{figure}[!tb]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\includegraphics[width=\linewidth]{figures/ambigious/shadow_tex_zoom.png}
		\caption{}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.48\textwidth}
		\includegraphics[width=\linewidth]{figures/ambigious/shadow_fc_zoom.png}
		\caption{}
	\end{subfigure}
	\begin{subfigure}[t]{0.48\textwidth}
		\includegraphics[width=\linewidth]{figures/ambigious/distort_tex_zoom.png}
		\caption{}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.48\textwidth}
		\includegraphics[width=\linewidth]{figures/ambigious/distort_fc_zoom.png}
		\caption{}
	\end{subfigure}
	\caption{Ambiguous regions are labelled as unclassified (in black). 
		(a) Shadow region with texture.
		(b) Shadow region with semantic colour.
		(c) Distorted region with texture.
		(d) Distorted region with semantic colour.} 
	\label{fig:ambigious}
\end{figure}


\subsection{Semi-automatic Mesh Annotation}  \label{sec:mesh_annota}
Rather than manually labelling each triangle face of the raw meshes, we design a semi-automatic mesh labelling framework to accelerate the labelling process. Figure~\ref{fig:pipeline} shows the overall pipeline of our labelling workflow.

Given the fact that urban environments consist of a large number of planar regions in the data, we opt to label the data at the segment level instead of individual triangle faces. 
Specifically, we over-segment the input meshes into a set of planar segments. 
These segments can enrich local contextual information for feature extraction and serve as the basic annotation unit to improve annotation efficiency.

\begin{figure}[!tb]
	\centering
	\includegraphics[width=\textwidth]{figures/pipeline/pipeline_L1.png}
	\caption{The pipeline of the labelling workflow.}
	\label{fig:pipeline}
\end{figure}

Instead of randomly choosing a mesh tile as input for annotation and refinement, which is insufficient for manual annotation progress, we favour picking a mesh tile that is more difficult to classify.
Similar to active learning, we first compute the feature diversity (see Equation \ref{eq:fea_div}) to optimally select a mesh tile containing a variety of classes and objects at different scales and complexity.
The feature diversity $F_{m}$ of tile $m$ is computed as
\begin{equation}\label{eq:fea_div}
	F_{m}=\frac{\sum_{i=1}^{N_{f}}\left ( f_i - \bar{f} \right )^{2}}{N_{f}}
\end{equation}
where $f_i$ represents each handcrafted feature which describe in Section \ref{sec:initial_seg}, and $\bar{f}$ is mean value of a $N_{f}$ dimensional feature vector.
To acquire the first ground truth data, we manually annotate the mesh (with segments) that is selected with the highest feature diversity.
Then, we add the first labelled mesh into the training dataset for the supervised classification.
Specifically, we use the segment-based features as input for the classifier, and the output is a pre-labelled mesh dataset.
Next, we use the mesh annotation tool to manually refine the pre-labelled mesh according to the feature diversity.
Finally, the new refined mesh will be added to the training dataset to improve the automatic classification accuracy incrementally.


\subsubsection{Initial Segmentation}\label{sec:initial_seg}

To avoid redundant computations of numerous triangles, we first apply mesh over-segmentation (i.e., linear least-squares fitting of planes) based on region growing on the input data to group triangle faces into homogeneous regions~\citep{lafarge2012creating}.
Such grouped regions are beneficial for computing local contextual features.
We then extract both geometric and radiometric features from those mesh segments as follows: 
\begin{itemize}
	\item[$\bullet$] \textit{Eigen-based features} are computed from the covariance matrix of the triangle vertices with respect to the average centre within each segment, which is beneficial for identifying urban objects with various surface distributions.
	The linearity $= (\lambda_{1} - \lambda_{2}) / \lambda_{1}$, sphericity $= \lambda_{3}/ \lambda_{1}$ and change of curvature $= \lambda_{3} / (\lambda_{1} + \lambda_{2} + \lambda_{3})$ are computed based on the three eigenvalues $\lambda_{1} \geq \lambda_{2} \geq \lambda_{3}\geq 0$.
	The local eigenvectors $\mathbf{n}_{i} $ and the unit normal vector $\mathbf{n}_{z} $ along Z-axis are used to compute the verticality $=1-\left | \mathbf{n}_{i}\cdot \mathbf{n}_{z} \right | $~\citep{hackel2016fast}.
	Note that many eigen-based features have been studied in literature~\citep{hackel2016fast,west2004context,weinmann2013feature}, and some of them were designed for and tested on LiDAR point clouds. 
	\textcolor{ao}{
	These eigen-based features are mostly computed per point based on its spherical neighbourhood, which often contains noise and does not form a surface. 
	Our chosen eigen-based features are defined on a segment representing the surface of a mesh, and thus they can capture non-local geometric properties of an object.
	}
	Additionally, in this work, we have tested all eigen-based features from the literature~\citep{hackel2016fast}, and we only present the ones that are effective for texture meshes.
	\item[$\bullet$] \textit{Elevation} is divided into absolute elevation $z_{a}$, relative elevation $z_{r}$ and multiscale elevations $z_{m}$.
	Where $z_{a}$ is the average elevation of the segment;
	the relative elevation is computed as $z_{r} = z_{a}-z_{r_{min}}$;
	the multiscale elevation~\citep{Verdie2015,Rouhani2017} $z_{m} = \sqrt{\frac{z_{a} - z_{min}}{z_{max} - z_{min}}}$.
	And $z_{r_{min}}$ denotes the lowest elevation of the local largest ground segment computed within a cylindrical neighbourhood with 30 meters radius around the segment centre.
	$z_{min}$ and $z_{max}$ represent the local minimum and maximum elevation values of a cylindrical neighbourhood within the scale of 10 meters, 20 meters, and 40 meters.
	Such large cylindrical neighbourhoods allow to find the local ground considering the resilience to hilly environments, \textcolor{ao}{and the square root ensures that small relative height values (i.e., values smaller than 1 $ m $) get a larger elevation attribute to enlarge elevation differences between small objects and the local ground (e.g., cars against the ground, boats against the water surfaces).}
	More importantly, due to the influence of terrain fluctuations and various scales of urban objects, the elevation of these three categories can complement each other.
	\item[$\bullet$] \textit{Segment area} is computed as $area(S_k) = \sum_{i = 1}^{N} area(f_i) $, where $f_i$ denotes a triangle of the segment $S_k$, and $N$ denotes the total number of triangles in $S_k$.
	\item[$\bullet$] \textit{Triangle density} is defined as $density(S_k) = \frac{N}{area(S_k)} $,  which reveals the object complexity, especially for adaptive urban meshes.
	\item[$\bullet$] \textit{Interior radius of 3D medial axis transform (InMAT)}~\citep{ma20123d,peters2016robust} of a segment $S_k$ is formulated as $r_k = \frac{\sum_{i=1}^{M} r_i}{M}$, where $M$ denotes the total number of triangle vertices of $S_k$, and $r_i$ denotes the interior radius of the shrinking ball that touches the vertex $v_i$ within the segment $S_k$. 
	It is designed to distinguish objects with different scales. 
	\item[$\bullet$] \textit{HSV colour-based features} are derived from the RGB channel of the entire texture map.
	We use the HSV colour space since it can better differentiate different objects than RGB.
	We compute the average colour, the variance of the colour distribution of all pixels within each segment, and we further discretize it into a histogram that consists of 15 bins of the hue channel, five bins of the saturation channel, and five bins of the value channel.
	\item[$\bullet$] \textit{Greenness} $a_{g}$ is used to classify objects that are similar to green vegetation.
	Specifically, it is computed according to the averaged RGB colour of each segment via $a_{g}=G-0.39\cdot R-0.61\cdot B$~\citep{mckinnon2017comparing}. 
\end{itemize}
	All the above features are concatenated into a 44-dimensional feature vector used by our random forest (RF) classifier in the initial segmentation. 

\subsubsection{Annotation Tool for Refinement}

Because of the under-segmentation errors and the imperfect results of the semantic mesh segmentation process, we design a mesh annotation tool (see Figure \ref{fig:annotator}) to manually correct the labelling errors.
Our mesh annotation tool is developed based on the labelling tool of CGAL~\citep{cgal:eb-20b}.

\begin{figure}[!tb]
	\centering
	\includegraphics[width=\textwidth]{figures/annotator/annotator.png}
	\caption{The interface of our annotation tool for 3D texture meshes. }
	\label{fig:annotator}
\end{figure}

As shown in Table \ref{tab:annotation_operation}, it consists of three operation categories: view, selection, and annotation.
The	view operations provide essential functions for the user to manipulate the scene camera, such as translate, rotate, zoom, or set the new pivot for the scene.
In addition, to use textures as a reference for labelling, we map texture and face colour with a certain degree of transparency, and we visualize the segment border to differentiate each segment. 

\begin{table}[!tb]
	\centering
	\noindent\adjustbox{max width=0.8\textwidth}
	{
		\begin{threeparttable}
			\centering
			\begin{tabular}{ccc}
				\toprule
				Categories & Operations & Objects \\
				\midrule
				\multirow{4}[2]{*}{View} & Translate & Camera \\
				& Rotate & Camera \\
				& Zoom in / out & Camera \\
				& Set pivot & Camera \\
				\midrule
				\multirow{6}[2]{*}{Selection} & Multi-selection / Lasso & Triangles / Segments \\
				& Expand / Reduce & Triangles / Segments \\
				& Semantic selection & Segments \\
				& Split region & Segments \\
				& Planar region extraction & Triangles \\
				& Split mesh & Triangles \\
				\midrule
				\multirow{3}[2]{*}{Annotation} & Probability slider & Segments \\
				& Segment area slider & Segments \\
				& Progress bar & Triangles \\
				& Switch semantic view & Triangles \\ 
				& Labelling & Triangles / Segments \\
				\bottomrule
			\end{tabular}%
		\end{threeparttable}
	}
	\caption{Basic operations in our annotation tool.} 
	\label{tab:annotation_operation}%
\end{table}%


The	selection operations allow the user to select or deselect either triangle faces (see Figure \ref{fig:tri_sel}) or segments (see Figure \ref{fig:seg_sel}) freely via a brush or a lasso.
Specifically, the face selection operation is used to fix the under-segmentation errors and generate new segments, and the segment selection operation is to fix incorrect segment labels.

\begin{figure}[!tb]
	\centering
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\linewidth]{figures/pipeline/tri_select_a.png}
		\caption{}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\linewidth]{figures/pipeline/tri_select_b.png}
		\caption{}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\linewidth]{figures/pipeline/tri_select_c.png}
		\caption{}
	\end{subfigure}
	\caption{An example of labelling by selecting triangles using the lasso tool (blue edges: segment boundaries). 
		(a) Before selection.
		(b) Lasso selection result (in red).
		(c) The correct label has been assigned to the selected region. 
		In this example, the label of the selected region has been changed from `ground' to `vehicle'.
	} 
	\label{fig:tri_sel}
\end{figure}


\begin{figure}[!tb]
	\centering
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\linewidth]{figures/pipeline/seg_select_a.png}
		\caption{}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\linewidth]{figures/pipeline/seg_select_b.png}
		\caption{}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\linewidth]{figures/pipeline/seg_select_c.png}
		\caption{}
	\end{subfigure}
	\caption{An example of segment labelling. 
		(a) Part of a wall of the building was previously labelled as `high vegetation' (in green).
		(b) Segment selection result (in red).
		(c) The label of the selected segment has been corrected with the new label `building'.
	}
	\label{fig:seg_sel}
\end{figure}

We also allow the user to edit the selection of each individual segment with splitting functions (see Figure \ref{fig:pnp_func}) and automatic extraction of the most planar region (see Figure \ref{fig:seg_func}). 
As for splitting, we first detect the potential planar and non-planar segments marked by user strokes, and then the non-planar one is split according to the vertex-to-plane distance.
It allows generating candidate non-planar regions (with respect to the detected planar segment) for the user to edit, and
it is useful to split a segment that covers large non-planar regions or contains more than one dominant planar area.
To extract the most planar region, we apply the region growing algorithm~\citep{lafarge2012creating} within the selected segment to automatically generate the candidate triangle faces with user-defined thresholds (i.e., the maximum distance to the plane, the maximum accepted angle, and the minimum region size).
Such an operation allows the user to filter out some small bumpy regions of the selected segment.

\begin{figure}[!tb]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\includegraphics[width=\linewidth]{figures/annotator/pnp_pipeline1.png}
		\caption{}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.48\textwidth}
		\includegraphics[width=\linewidth]{figures/annotator/pnp_pipeline2.png}
		\caption{}
	\end{subfigure}
	\caption{An example splitting planar and non-planar regions. 
		(a) The user draws a stroke (in red) across the border of the non-planar segment and the planar segment. 
		(b) The detected non-planar segment has been split into two parts (i.e., a non-planar region shown in red and a planar segment shown in green).
	} 
	\label{fig:pnp_func}
\end{figure}

\begin{figure}[!tb]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\includegraphics[width=\linewidth]{figures/annotator/planar_split_pipeline1.png}
		\caption{}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.48\textwidth}
		\includegraphics[width=\linewidth]{figures/annotator/planar_split_pipeline3.png}
		\caption{}
	\end{subfigure}
	\caption{Editing an individual segment. 
		(a) A segment is selected (highlighted in green) for splitting. 
		(b) Automatic extraction of the most planar region (shown in red) within the selected segment according to user-defined thresholds.} 
	\label{fig:seg_func}
\end{figure}

Besides, probability and area-based sliders and a progress bar are provided in the annotation panel to improve annotation efficiency and experience, respectively. 
Specifically, the probability slider is introduced for the user to visually inspect the segments that are most likely misclassified.
Moreover, the user can further use it to inspect a specific class by switching the view to highlight a specific semantic class.
The segment area slider is used to identify isolated tiny segments, which commonly appear as errors.
The progress bar is used to indicate the estimated labelling progress during the annotation.
After performing the selection, the user can easily assign the corresponding label to the selected area.
