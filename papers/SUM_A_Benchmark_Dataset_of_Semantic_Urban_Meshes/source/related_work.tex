\section{Related Work}
\label{sec:relaw}

\begin{sidewaystable}
\noindent\adjustbox{max width=\textwidth}
{
\begin{threeparttable}
	\begin{tabular}[t]{@{}lcccC{0.15\linewidth}ccC{0.16\linewidth}C{0.15\linewidth}C{0.2\linewidth}C{0.2\linewidth}c@{}} 
	\toprule
	Name && Platforms & Year & Data Type & Area\tnote{a} / Length & Classes & Points / Triangles & RGB & Automatic Pre-labelling & Annotation & Time Cost (hours)\\
	\midrule
	\textbf{Oakland 3D}~\citep{munoz2009contextual}        && MLS & 2009 & Point Cloud & 1.5 $km$  & 5  & 1.6 $ M $  & No  & No                       & 3D Manually            & Not reported\\
	\textbf{Paris-rue-Madame}~\citep{serna2014paris}       && MLS & 2014 & Point Cloud & 0.16 $km$ & 17 & 20 $ M $   & No  & 2D semantic segmentation & 3D Semi-manually       & Not reported\\	
	\textbf{iQmulus}~\citep{vallet2015terramobilita}       && MLS & 2015 & Point Cloud & 10 $km$   & 8  & 300 $ M $  & No  & No                       & 2D Semi-manually       & Not reported\\
	\textbf{Semantic3D}~\citep{hackel2017semantic3d}       && TLS & 2017 & Point Cloud & -        & 8  & 4000 $ M $ & Yes & No                       & 2D \& 3D Semi-manually  & Not reported\\
	\textbf{Paris-Lille-3D}~\citep{roynard2018paris}       && MLS & 2018 & Point Cloud & 1.94 $km$ & 9  & 143 $ M $  & No  & No                       & 3D Manually            & Not reported\\
	\textbf{SemanticKITTI}~\citep{behley2019semantickitti} && MLS & 2019 & Point Cloud & 39.2 $km$ & 25 & 4549 $ M $ & No  & No                       & 3D Manually            & 1700        \\
	\textbf{Toronto-3D}~\citep{tan2020toronto}             && MLS & 2020 & Point Cloud & 1.0 $km$  & 8  & 78.3 $ M $ & Yes & No                       & 3D Manually            & Not reported\\
	\textbf{ISPRS}~\citep{niemeyer2014contextual}          && ALS & 2012 & Point Cloud & 0.1 $km^2$  & 9  & 1.2 $ M $   & No & No                       & 3D Manually & Not reported\\
	\textbf{AHN3}~\citep{ahn2019}                         && ALS & 2019 & Point Cloud & 41,543 $km^2$ & 4 & 415.43 $ B $\tnote{b} & No & 3D semantic segmentation & 3D Manually & Not reported\\
	\textbf{DublinCity}~\citep{zolanvari2019dublincity}    && ALS & 2019 & Point Cloud & 2.0 $km^2$     & 13 & 260 $ M $             & No & No                       & 3D Manually & 2500        \\
	\textbf{DALES}~\citep{varney2020dales}                 && ALS & 2020 & Point Cloud & 10.0 $km^2$    & 8  & 505.3 $ M $           & No & 3D semantic segmentation & 3D Manually & Not reported\\
	\textbf{LASDU}~\citep{ye2020lasdu}                     && ALS & 2020 & Point Cloud & 1.02 $km^2$    & 5  & 3.12 $ M $            & No & No                       & 3D Manually & Not reported\\
	\textbf{ETHZ RueMonge}~\citep{brostow2009semantic,riemenschneider2014learning} && Auto-mobile camera & 2014 & Mesh & 0.7 $km$ & 9 & 1.8 $ M $ (lowres)\tnote{c} & Yes (per vertex)\tnote{d} & 2D over-segmentation & 2D Semi-manually & 230 (701 frames)\tnote{e} \\
	\textbf{Campus3D}~\citep{li2020campus3d}       && UAV camera & 2020 & Point Cloud & 1.58 $km^2$ & 14 & 937.1 $ M $  & Yes           & No & 2D \& 3D Manually       & Not reported\\
	\textbf{SensatUrban}~\citep{hu2021towards}     && UAV camera & 2020 & Point Cloud & 6 $km^2$    & 13 & 2847.1 $ M $ & Yes           & No & 3D Manually            & 600 \\	
	\textbf{Swiss3DCities}~\citep{can2020semantic} && UAV camera & 2020 & Point Cloud & 2.7 $km^2$  & 5  & 226 $ M $    & Yes           & No & 3D Manually (on mesh)  & 144 (1 $ M $ Triangles)\tnote{f} \\	
	\textbf{Hessigheim 3D}~\citep{laupheimer2020association,kolle2021h3d} && UAV Lidar \& camera & 2021 & Point Cloud \& Mesh & 0.19 $km^2$ & 11 & 125.7 $ M $ / 36.76 $ M $\tnote{g}  & Yes (texture)\tnote{h} & No & 3D Manually\tnote{i} & Not reported\\
	\textbf{SUM-Helsinki (Ours)}                   && Airplane camera & 2021 & Mesh        & 4 $km^2$    & 6  & 19 $ M $     & Yes (texture)\tnote{h} & 3D over-segmentation \& 3D semantic segmentation & 3D Semi-manually  & 400 \\ 
	\bottomrule
	\end{tabular}
  \begin{tablenotes}
  	\item[a] The area was measured in a 2D map.
  	\item[b] The number of total points (i.e., 415.43 billion) is estimated.
	\item[c] The low-resolution meshes contain 1.8 million triangle faces, according to the publications.
	\item[d] An RGB colour was assigned to each triangle vertex.
	\item[e] The frames were from video sequences.
	\item[f] About one million triangles (16 tiles) from simplified mesh were labelled, which took around 6 to 12 hours per tile.
	\item[g] The number of LiDAR points is 125.7 million and the number of triangle faces is 36.76 million.
	\item[h] The colour of each triangle face corresponds to a patch of the texture image.
	\item[i] The LiDAR point clouds were manually annotated and the labels were transferred to the mesh.
  \end{tablenotes}
\end{threeparttable}
}
\caption{Comparison of existing 3D urban benchmark datasets.} 
\label{tab:overview}
\end{sidewaystable}

Urban datasets can be captured with different sensors and be reconstructed with different methods, and the resulting datasets will have different properties.
Most benchmark urban datasets focus on point clouds, whereas our semantic urban benchmark dataset is based on textured triangular meshes.

The input of the semantic labelling process can be raw or pre-labelled urban datasets such as the automatically generated results from over-segmentation or semantic segmentation (see Section~\ref{sec:mesh_annota}).
Regardless of the input data, it still needs to be manually checked and annotated with a labelling tool, which involves selecting a correct semantic label from a predefined list for each triangle (or point, depending on the dataset) by users.
In addition, some interactive approaches can make the labelling process semi-manual.                                                                                                                                        
However, unlike our proposed approach, the labelling work of most of the 3D benchmark data does not take full advantage of over-segmentation and semantic segmentation on 3D data, and interactive annotation in the 3D space.      
%

We present in this section an overview of the publicly available semantic 3D urban benchmark datasets categorised by sensors and reconstruction types (see Table \ref{tab:overview}).
More specifically, we elaborate on the quality, scale, and labelling strategy of the existing urban datasets regarding semantic segmentation.


\subsection{Photogrammetric Products}

\subsubsection{Dense Point Clouds} The \emph{Campus3D}~\citep{li2020campus3d} is to our knowledge the first aerial point cloud benchmark.
The coarse labelling is conducted in 2D projected images with three views, and the grained labels are refined in 3D with user-defined rotation angles. 
The dataset covers only the campus of the National University of Singapore and is thus not representative of a typical urban scene.

\emph{SensatUrban}~\citep{hu2021towards} is another example of the photogrammetric point clouds covering various urban landscapes in two cities of the UK\@.
The semantic points are manually annotated via the off-the-shelf software tool CloudCompare~\citep{Girardeau-Montaut2016}, and the overall annotation is reported to have taken around 600 hours.
The dataset also contains several areas without points, especially for water surfaces and regions with dense objects. 
The leading causes are the Lambertion surface assumption during the image matching and the inadequate image overlapping rate during the flight.   

Similarly, the \emph{Swiss3DCities}~\citep{can2020semantic} was recently released that covers three cities in Zurich but twice smaller than the SensatUrban.
The annotation work was conducted on a simplified mesh in the software Blender~\citep{blender}, and then the semantics were transferred to the mesh vertices, which are regarded as point clouds, via the nearest neighbour search.    
The mesh simplification may result in the loss of small-scale objects such as building dormers and chimneys, and the automatic transfer of the labels could have introduced errors in the ground truth. 

\subsubsection{Triangle Meshes} To the best of our knowledge, the \emph{ETHZ RueMonge 2014}~\citep{riemenschneider2014learning} is the first urban-related benchmark dataset available as surface meshes.
The label for each triangle is obtained from projecting selected images that are manually labelled from over-segmented image sequences~\citep{brostow2009semantic}. 
In fact, due to the error of multi-view optimisation and the ambiguous object boundary within triangle faces, the datasets contain many misclassified labels, making them unsuitable for training and evaluating supervised-learning algorithms.

\emph{Hessigheim 3D}~\citep{laupheimer2020association,kolle2021h3d} is a small-scale semantic urban dataset consisting of highly dense LiDAR point clouds and high resolution texture meshes.
Particularly, the mesh is generated from both LiDAR point cloud and oblique aerial images in a hybrid way.
The labels of point clouds are manually annotated in CloudCompare~\citep{Girardeau-Montaut2016}, and the labels of the mesh are transferred from the point clouds by computing the majority votes per triangle.
However, if the mesh triangle has no corresponding points, some faces may remain unlabelled which resulted in about 40\% unlabelled area.
In addition, this dataset contains non-manifold vertices, which makes it difficult to use directly. 

\subsection{LiDAR Point Clouds}
Unlike photogrammetric point clouds, LiDAR point clouds usually do not contain colour information.
To annotate them properly, additional information is often required, e.g.\ images or 2D maps.
LiDAR point cloud benchmark datasets are more common than photogrammetric ones.

\subsubsection{Street-view Datasets}
The \emph{Oakland 3D}~\citep{munoz2009contextual} is one of the earliest mobile laser scanning (MLS) point cloud datasets, which was designed for the classification of outdoor scenes. 
It has five hand-labelled classes with 44 sub-classes, but without colour information and semantic categories like roof, canopy, or interior building block, which are typical for all street-view captured datasets.

Compared to \emph{Oakland 3D}, \emph{Paris-rue-Madame}~\citep{serna2014paris} is a relatively smaller dataset which used the 2D semantic segmentation results for 3D annotation.
Specifically, the point clouds were projected onto images to extract the objects hierarchically with several unsupervised segmentation and classification algorithms.

Although the 2D pre-labelled generation is fully automatic, different semantic categories require different segmentation algorithms resulting in difficulties in the classification of multiple classes. 

The \emph{iQmulus dataset}~\citep{vallet2015terramobilita} is a 10 $ km $ street dataset annotated based on projected images in the 2D space. 
Specifically, the user first needs to extract objects by editing the image with a polyline tool and then assigns labels to the extracted object regions.  
Some automatic functions are made for polyline editing in this framework, but the entire annotation pipeline is still complicated.

Unlike other street view datasets, \emph{Semantic3D}~\citep{hackel2017semantic3d} is a dataset consisting of terrestrial laser scanning (TLS) point clouds (the scanner is not moving and scans are made from only a few viewpoints).
It has eight classes and colours were obtained by projecting the points onto the original images.
There are two annotation methods: (1) annotating in 3D with an iterative model-fitting approach on manually selected points; (2) annotating in a 2D view by separate background from a drawn polygon in CloudCompare~\citep{Girardeau-Montaut2016}.
Although it covers many urban scenes and includes RGB information, the acquired objects are incomplete because of the limited viewpoints and occlusions. 

The other three typical MLS point cloud datasets that were manually labelled are \emph{Paris-Lille-3D}~\citep{roynard2018paris}, \emph{SemanticKITTI}~\citep{behley2019semantickitti}, and \emph{Toronto-3D}~\citep{tan2020toronto}. 


\subsubsection{Aerial-view Datasets}
As for ALS benchmark point clouds, representative datasets are \emph{ISPRS}~\citep{niemeyer2014contextual}, \emph{DublinCity}~\citep{zolanvari2019dublincity}, and \emph{LASDU}~\citep{ye2020lasdu} covering various scales of city landscapes and were annotated manually with off-the-shelf software. 
Instead of fully manual annotation, the \emph{Dayton Annotated LiDAR Earth Scan (DALES)}~\citep{varney2020dales} used digital elevation models (DEM) to distinguish ground points with a certain threshold, the estimated normal to label the building points roughly, and satellite images to provide contextual information as references for annotators to check and label the rest of data.
Similarly, the AHN3 dataset~\citep{ahn2019} was semi-manually labelled by different companies with off-the-shelf software.
Besides, since the ALS measurement is conducted in the top view direction, unlike oblique aerial cameras, the obtained point clouds often miss facade information to a certain degree. 
