\section{Experiments}\label{sec:expri}
\subsection{Data Split}

To perform the semantic segmentation task, we randomly select 40 tiles from the annotated 64 tiles of Helsinki as training data, 12 tiles as test data, and 12 tiles as validation data (see Figure \ref{fig:datadesp} (a)).
For each of the six semantic categories, we compute the total area in the training and test dataset to show the class distribution.
As shown in Figure \ref{fig:datadesp} (b), some classes, like vehicles and boats, only account for less than $5\%$ of the total area,
while the building and terrain together comprise more than $70\%$.
The unbalanced classes impose significant challenges for semantic segmentation based on supervised learning.

\begin{figure}[!tb]
	\begin{subfigure}[b]{0.38\textwidth}
		\includegraphics[width=\linewidth]{figures/overview_grids/grids.png}
		\label{fig:grids}
		\caption{}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[b]{0.58\textwidth}		
		\includegraphics[width=\linewidth]{figures/plots/class_statistics.png}
		\label{fig:class_statistics}
		\caption{}
	\end{subfigure}
	%	\vspace*{-1cm}
	\caption{Overview of the data used in our experiment. (a) The distribution of the training, test, and validation dataset. (b) Semantic categories of training (including validation dataset) and test dataset.}
	\label{fig:datadesp}
\end{figure}

\subsection{Evaluation Metric}
Since the triangle faces in the meshes have different sizes, we compute the surface area for semantic evaluation instead of using the number of triangles. The performance of semantic mesh segmentation is measured in precision, recall, F1 score, and intersection over union (IoU) for each object class. The evaluation of the whole test area is applied with overall accuracy (OA), mean per-class accuracy (mAcc), and mean per-class intersection over union (mIoU).

\subsection{Evaluation of Initial Segmentation}
We have implemented the semantic mesh segmentation and annotation tool in C++ using the open-source libraries include CGAL~\citep{cgal:eb-20b}, Easy3D~\citep{easy3d2018nan}, and ETHZ random forest~\citep{ethzrf}. 

Our proposed pipeline for initial segmentation only takes a few input parameters, which are shown in Table \ref{tab:params}.
The over-segmentation is intended to find all planar regions in the model, for which we set the distance threshold to 0.5 meters. This threshold value specifies the minimum geometric features we would like the over-segmentation method to identify. 
In other words, the region growing-based over-segmentation method will not be able to distinguish two parallel planes with a distance smaller than this threshold.
We set the angle threshold to 90 degrees, which is large enough to cope with high levels of noise (e.g., the distance value is small, but the angle between the triangle normal and the plane normal is large).
Moreover, the minimum area is set to zero to allow planar segments of any arbitrary size.
As for the random forest classifier, \textcolor{ao}{we set the parameters initially to those of Rouhani et al.~\citep{Rouhani2017} followed by fine-tuning using the validation data.}
Specifically, using 100 trees is sufficient to guarantee the stability of the model, and using the depth of 30 is adequate to avoid over-fitting and under-fitting for training.
\begin{table}[H]
	\centering
	\noindent\adjustbox{max width=0.6\textwidth}
	{
		\begin{threeparttable}
				\centering
				\begin{tabular}{ccc}
					\toprule
					Method & Parameters & Value \\
					\midrule
					\multirow{3}[2]{*}{Region Growing} & Minimum area & 0 $m^2$ \\
					& Distance to plane & 0.5 \textcolor{ao}{$m$} \\
					& Accepted angle & $90^{\circ}$ \\
					\midrule
					\multirow{2}[2]{*}{Random Forest} & Number of trees & 100 \\
					& Maximum depth & 30 \\
					\bottomrule
				\end{tabular}%
		\end{threeparttable}
	}
	\caption{Parameters used in our approach.} 
	\label{tab:params}
\end{table}%

Rather than classifying about 19 million triangle faces (i.e., the entire dataset), we use 515,176 segments that are clustered during over-segmentation.
Although both semantic segmentation and labelling refinement can benefit from mesh over-segmentation, the degree of the under-segmentation error cannot be avoided.
Since our mesh over-segmentation does not intend to retrieve the individual objects and the purpose is to perform semantic segmentation, we measure the maximum achievable performance by calculating the \textcolor{ao}{IoU} instead of using under-segmentation errors to evaluate it.
The \textcolor{ao}{upper bound IoU} of each class we could achieve for semantic segmentation is presented in Table \ref{tab:oveal_re}, and the \textcolor{ao}{upper bound mean IoU (mIoU)} over all classes is about $ 90.9\% $ as shown in Table \ref{tab:fea_abla}.
In addition, the results of our experiment in Tables \ref{tab:oveal_re} and \ref{tab:fea_abla} are reported based on the average performance of ten times experiments with the same configuration.

\begin{table}[H]
	\centering
	\noindent\adjustbox{max width=1.0\textwidth}
	{
		\begin{threeparttable}
			\centering
			\begin{tabular}{cccccC{0.15\linewidth}}
				\toprule
				Class & Precision  (\%) & Recall  (\%) & F1 scores  (\%) & IoU  (\%) & Upper bound IoU  (\%) \\
				\midrule
				Terrain & 87.7 & 94.3 & 90.9 & 83.3 & 93.9 \\
				High Vegetation & 96.3 & 93.8 & 95.0 & 90.5 & 96.2 \\
				Building & 94.6 & 97.7 & 96.1 & 92.5 & 99.0 \\
				Water & 97.0 & 88.3 & 92.5 & 86.0 & 92.7 \\
				Vehicle & 77.9 & 41.7 & 54.4 & 37.3 & 73.2 \\
				Boat & 77.9 & 7.5 & 13.7 & 7.4 & 90.5 \\
				\bottomrule
			\end{tabular}%
		\end{threeparttable}
	}
	\caption{Overall evaluation of our method.
	The \textit{Upper bound IoU} refers to the maximum achievable IoU in theory.}
	\label{tab:oveal_re}
\end{table}%


For semantic segmentation, a detailed evaluation of each class is listed in Table \ref{tab:oveal_re}, and we achieve about $ 93.0\% $ overall accuracy and $ 66.2\% $ mIoU as shown in Table \ref{tab:fea_abla}. % 
The qualitative evaluation of it is shown in Figure~\ref{fig:semantic_qualitive}.
As shown in Figure~\ref{fig:semantic_qualitive} (e), most of the prediction errors occur at small-scale objects such as vehicles and boats due to fewer training samples and errors from over-segmentation. 

\begin{table}[H]
	\centering
	\noindent\adjustbox{max width=1.0\textwidth}
	{
		\begin{threeparttable}
			\centering
			\begin{tabular}{C{0.4\linewidth}cccc}
				\toprule
				Model & OA (\%) & mAcc (\%) & mIoU (\%) & \textcolor{ao}{$\Delta$mIoU (\%)} \\
				\midrule
				Upper bound (Perfect) & 98.1 & 91.6 & 90.9 & —— \\
				\textbf{Ours (best)} & 93.0 & 70.6 & 66.2 & 0.0 \\
				Without sphericity & 93.0 & 70.5 & 66.1 & -0.1 \\
				Without segment area & 92.9 & 70.5 & 66.0 & -0.2 \\
				Without triangle density & 92.9 & 70.4 & 66.0 & -0.3 \\
				Without variance HSV & 92.9 & 70.3 & 65.9 & -0.3 \\
				Without absolute elevation & 93.0 & 70.2 & 65.9 & -0.3 \\
				Without relative elevation & 92.9 & 70.3 & 65.8 & -0.4 \\
				Without curvature & 92.9 & 70.2 & 65.8 & -0.4 \\
				Without multiscale elevations & 92.8 & 69.8 & 65.1 & -1.1 \\
				Without linearity & 91.8 & 66.6 & 62.0 & -4.2 \\
				Without greenness & 91.9 & 66.6 & 61.9 & -4.3 \\
				Without InMat & 91.6 & 66.4 & 61.6 & -4.6 \\
				Without average HSV & 91.7 & 66.1 & 61.4 & -4.8 \\
				Without verticality & 91.4 & 66.1 & 61.3 & -4.9 \\
				Without HSV histogram bins & 91.5 & 66.0 & 61.1 & -5.1 \\
				\bottomrule
			\end{tabular}%
		\end{threeparttable}
	}
	\caption{Ablation study of the features in our approach.
	The \textit{Upper bound (Perfect)} refers to the maximum achievable performance in theory.}
	\label{tab:fea_abla}
\end{table}%


To better understand the relevance of the features, we measure the feature importance and perform ablation studies (see Table \ref{tab:fea_abla}).
We can observe that the radiometric features (which account for $ 62.8\% $) are more important than geometric ones (which account for $ 37.2\% $).
Moreover, after removing individual feature vectors, the performance will decline, indicating each feature contributes to the best results. 


\begin{figure}[H]
	\begin{adjustwidth}{-3cm}{-3cm}
		\centering
		\begin{tabular}{ccccc}	
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/texture1.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/segments1.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/predict1.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/truth1.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/error1.png}\\
			\midrule
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/texture2.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/segments2.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/predict2.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/truth2.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/error2.png}\\
			\midrule
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/texture3.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/segments3.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/predict3.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/truth3.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/error3.png}\\
			\midrule
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/texture4.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/segments4.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/predict4.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/truth4.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/error4.png}\\		
			\midrule
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/texture5.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/segments5.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/predict5.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/truth5.png}&
			\includegraphics[width=0.25\textwidth]{figures/semantic_results/error5.png}\\	
			\begin{footnotesize} \textbf{(a)} Original \end{footnotesize}&
			\begin{footnotesize} \textbf{(b)} Segments \end{footnotesize}&
			\begin{footnotesize} \textbf{(c)} Predictions \end{footnotesize}&
			\begin{footnotesize} \textbf{(d)} Truth \end{footnotesize}&
			\begin{footnotesize} \textbf{(e)} Error maps \end{footnotesize}
			\\	
		\end{tabular}
	\end{adjustwidth}
	\centering
	\includegraphics[width=0.9\textwidth]{figures/semantic_results/semantic_legend.png}
	\caption{Part of our semantic segmentation results. 
		The first column shows the input texture meshes; 
		the second column shows the over-segmentation results; 
		the third column shows the predicted semantic meshes; 
		the fourth column shows the ground truth meshes; 
		the last column shows the error maps (red: errors; green: correct labels).}
	\label{fig:semantic_qualitive}
\end{figure}

\subsection{Evaluation of Competition Methods}
To the best of our knowledge, none of the state-of-the-art deep learning frameworks of 3D semantic segmentation can directly be used on large-scale texture meshes.
Additionally, although the data structures of point clouds and meshes are different, the inherent properties of geometry in the 3D space of the urban environment are nearly identical.
In other words, they can share the feature vectors within the same scenes.
Consequently, we sample the mesh into coloured point clouds (see Figure \ref{fig:samplingpts}) with a density of about 10 $pts/m^2$ as input for the competing deep learning methods.
In particular, we use Montecarlo sampling~\citep{cignoni1998metro} to generate randomly uniform dense samples, and we further prune these samples according to Poisson distributions~\citep{corsini2012efficient} and assign the colour via searching the nearest neighbour from the textures.

\begin{figure}[!tb]
	\centering
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\linewidth]{figures/sampling/texture.png}
		\caption{Texture mesh}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\linewidth]{figures/sampling/wireframe.png}
		\caption{Wireframe}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\linewidth]{figures/sampling/sampling.png}
		\caption{Sampled point cloud}
	\end{subfigure}
	\caption{Sampling point cloud from texture meshes. Our sampled points preserve both geometric and radiometric information of the original mesh.} 
	\label{fig:samplingpts}
\end{figure}

To evaluate and compare with the current state-of-the-art 3D deep learning methods that can be applied to a large-scale urban dataset, we select five representative approaches (i.e., PointNet~\citep{qi2017pointnet}, PointNet++~\citep{qi2017pointnet++}, SPG~\citep{landrieu2018large}, KPConv~\citep{thomas2019kpconv}, and RandLA-Net~\citep{hu2020randla}). 
We perform all the experiments on an NVIDIA GEFORCE GTX 1080Ti GPU.
Note that these deep learning-based methods downsample the input point clouds significantly as a pre-processing step. 
In our experiments, the point sampling density is limited by the GPU memory, and increasing or decreasing the sampling density within a reasonable range may lead to slightly different performance.
\textcolor{ao}{
It should be noted that no matter how dense the input point clouds are, almost all state-of-the-art deep learning architectures (such as PointNet, PointNet++, RandLaNet, KPConv, and SPG, etc.) downsample the input point clouds significantly, and they are still able to learn effective features for classification. 
Besides, different deep learning-based point cloud classification frameworks exploit different strategies for downsampling the input points.
}
In addition, we also compare with the joint RF-MRF~\citep{Rouhani2017}, which is the only competition method that directly takes the mesh as input and without using GPU for computation.


The hyper-parameters of all the competing methods are tuned \textcolor{ao}{according to the validation data} to achieve the best results we could acquire.  
Besides, the results of each competitive method (see Table \ref{tab:compare_table}) are demonstrated in average performance based on ten times experiments with the same setting. 
From the comparison results, as shown in Table \ref{tab:compare_table}, we found that our baseline method outperforms other methods except for KPConv.
Specifically, our approach outperforms RF-MRF with a margin of $5.3\%$ mIoU, and deep learning methods (not including KPConv) from $16.7\%$ to $29.3\%$ mIoU.
Compared with the KPConv, the performance of our method is much more robust, which can be observed from Table \ref{tab:compare_table} that the standard deviation of our method is close to zero (i.e., the standard deviation of mIoU of our method is about $0.024\%$).  
The reason is that in our method, we set 100 trees in the random forest to ensure the stability of the model, but in KPConv, the kernel point initialization strategy may not be able to select some parts of the point cloud, which leads to the instability of the results.
Furthermore, compared with all deep learning pipelines, our method is conducted on a CPU and uses much less time for training (including feature computation).
This can be explained by the fact that we have fewer input data (triangles versus points), and the time complexity of our handcrafted features computation is much lower than the features learned from deep learning.

\begin{table}[!tb]
	\noindent\adjustbox{max width=\textwidth}
	{
		\begin{threeparttable}
			\centering
			\begin{tabular}{ccC{0.1\linewidth}ccccccccc}
				\toprule
				 & Terrain & High Vegetation & Building & Water & Vehicle & Boat & mIoU & OA & mAcc & mF1 & \textcolor{ao}{$t_{train}$}\\
				\midrule
				PointNet~\citep{qi2017pointnet} & 56.3 & 14.9   & 66.7 & 83.8 & 0.0 & 0.0 & 36.9 ± 2.3 & 71.4 ± 2.1 & 46.1 ± 2.6 & 44.6 ± 3.2 & 1.8\\
				RandLaNet~\citep{hu2020randla} & 38.9 & 59.6   & 81.5 & 27.7 & 22.0 & 2.1 & 38.6 ± 4.6 & 74.9 ± 3.2 & 53.3 ± 5.1 & 49.9 ± 4.8 & 10.8\\
				SPG~\citep{landrieu2018large} & 56.4 & 61.8 & 87.4   & 36.5 & 34.4 & 6.2 & 47.1 ± 2.4  & 79.0 ± 2.8 & 64.8 ± 1.2 & 59.6 ± 1.9 & 17.8\\
				PointNet++~\citep{qi2017pointnet++} & 68.0 & 73.1   & 84.2 & 69.9 & 0.5 & 1.6 & 49.5 ± 2.1 & 85.5 ± 0.9 & 57.8 ± 1.8 & 57.1 ± 1.7 & 2.8\\
				RF-MRF~\citep{Rouhani2017} & 77.4 & 87.5 & 91.3 & 83.7 & 23.8 & 1.7 & 60.9 ± 0.0 & 91.2 ± 0.0 & 65.9 ± 0.0 & 68.1 ± 0.0 & \textbf{1.1}\\
				KPConv~\citep{thomas2019kpconv} & \textbf{86.5} & 88.4 & \textbf{92.7} & 77.7 & \textbf{54.3} & \textbf{13.3} & \textbf{68.8} ± 5.7 & \textbf{93.3} ± 1.5 & \textbf{73.7} ± 5.4 & \textbf{76.7} ± 5.8 & 23.5\\
				\textbf{Baseline}  & 83.3          & \textbf{90.5} & 92.5 & \textbf{86.0} & 37.3 & 7.4 & 66.2 ± 0.0 & 93.0 ± 0.0 & 70.6 ± 0.0 & 73.8 ± 0.0 & 1.2\\
				\bottomrule
			\end{tabular}%
		\end{threeparttable}
	}
	\caption{Comparison of various semantic segmentation methods on the new benchmark dataset. 
		The results reported in this table are per-class IoU (\%), mean IoU (mIoU, \%) ± standard deviation, Overall Accuracy (OA, \%) ± standard deviation, mean class Accuracy (mAcc, \%) ± standard deviation, mean F1 score (mF1, \%) ± standard deviation, and the time cost of training (\textcolor{ao}{$t_{train}$}, hours). 
		The running times of SPG include both feature computation and graph construction, and RF-MRF and our baseline method include feature computation.
		We repeated the same experiment ten times and presented the mean performance.} 
	\label{tab:compare_table}
\end{table}%

\subsection{Evaluation of Annotation Refinement}
Following the proposed framework, a total of 19,080,325 triangle faces have been labelled, which took around 400 working hours.
Compared with a triangle-based manual approach, we estimate that our framework saved us more than 600 hours of manual labour.
Specifically, we have measured the labelling speed with these two different approaches on the same mesh tile consisting of 309,445 triangle faces and 8,033 segments.
It took around 17 hours for manual labelling based on triangle faces, while with our segment-based semi-automatic approach, it took only 6.5 hours.  

We also evaluate the performance of semantic segmentation with different amounts of input training data on our baseline approach with the intention of understanding the required amount of data to obtain decent results. 
Specifically, we use ten sets of different training areas with ten times experiments with the same configuration of each set, and we linearly interpolate the results as shown in Figure \ref{fig:tr_area}. 
From Figures \ref{fig:tr_area_miou}, \ref{fig:tr_area_oa}, and \ref{fig:tr_area_std}, we can observe that our initial segmentation method only requires about $10\%$ (equal to about 0.325 $km^2$) of the total training area to achieve acceptable and stable results. 
In other words, using a small amount of ground truth data, our framework can provide robust pre-labelled results and significantly reduce the manually labelling efforts.


\begin{figure}[!tb]
	\centering
	\begin{subfigure}[t]{0.31\textwidth}
		\includegraphics[width=\linewidth]{figures/plots/training_area_mIoU.png}
		\caption{\textcolor{ao}{mIoU}}
		\label{fig:tr_area_miou}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.31\textwidth}
		\includegraphics[width=\linewidth]{figures/plots/training_area_OA.png}
		\caption{\textcolor{ao}{OA}}
		\label{fig:tr_area_oa}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}[t]{0.32\textwidth}
		\includegraphics[width=\linewidth]{figures/plots/training_area_std.png}
		\caption{\textcolor{ao}{Standard deviation}}
		\label{fig:tr_area_std}
	\end{subfigure}
	\caption{Effect of the amount of training data on the performance of the initial segmentation method used in the semi-automatic annotation. We repeated the same experiment ten times for each set of training areas and presented the mean performance.}
	\label{fig:tr_area}
\end{figure}
