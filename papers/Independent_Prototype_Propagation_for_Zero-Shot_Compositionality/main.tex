\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021
\PassOptionsToPackage{numbers}{natbib}

% ready for submission
\usepackage[preprint]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[all]{hypcap}

\newcommand{\gjb}[1]{\textcolor{cyan}{[\textbf{GJB}: #1]}}

\bibliographystyle{unsrtnat}

\title{Independent Prototype Propagation\\ for Zero-Shot Compositionality}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Frank Ruis \\
  University of Twente \& TNO\\
  \texttt{research@frank-ruis.nl} \\
   \And
   Gertjan J. Burghouts \\
   TNO \\
   \texttt{gertjan.burghouts@tno.nl} \\
   \And
   Doina Bucur \\
   University of Twente \\
   \texttt{d.bucur@utwente.nl} \\
}

\begin{document}

\maketitle

\begin{abstract}

Humans are good at compositional zero-shot reasoning; someone who has never seen a zebra before could nevertheless recognize one when we tell them it looks like a horse with black and white stripes. Machine learning systems, on the other hand, usually leverage spurious correlations in the training data, and while such correlations can help recognize objects in context, they hurt generalization. To be able to deal with underspecified datasets while still leveraging contextual clues during classification, we propose ProtoProp, a novel prototype propagation graph method. First we learn prototypical representations of objects (e.g., zebra) that are conditionally independent w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate the independent prototypes through a compositional graph, to learn compositional prototypes of novel attribute-object combinations that reflect the dependencies of the target distribution. The method does not rely on any external data, such as class hierarchy graphs or pretrained word embeddings. We evaluate our approach on AO-Clever, a synthetic and strongly visual dataset with clean labels, and UT-Zappos, a noisy real-world dataset of fine-grained shoe types. We show that in the generalized compositional zero-shot setting we outperform state-of-the-art results, and through ablations we show the importance of each part of the method and their contribution to the final results.

\end{abstract}

\section{Introduction}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{img/intro_clean.pdf}
    \caption{\textbf{ProtoProp} A sketch of our proposed method; we learn conditionally independent prototypical representations of visual primitives in the form of objects (e.g., horse) and attributes (e.g., stripes). The prototypes are then propagated through a compositional graph, where they are combined into novel compositional prototypes to recognize both seen and unseen classes (e.g., zebra). }
    \label{fig:intro}
    \medskip
    \small
\end{figure}

As humans, hearing the phrase `a tiny pink penguin reading a book' can conjure up a vivid image, even though we have likely never seen such a creature before. This is because humans can compose their knowledge of a small number of visual primitives to recognize novel concepts~\citep{hebart2020revealing}, a property which \citet{lake2017building} argue is one of the key building blocks for human intelligence missing in current artificial intelligence systems. Machines, on the other hand, are largely data-driven, and usually require many labeled examples from various viewpoints and lighting conditions in order to recognize novel concepts. Since visual concepts follow a long-tailed distribution~\citep{salakhutdinov2011learning, liu2019large}, such an approach makes it near impossible to gather sufficient examples for all possible concepts. Compounded by that, in the absence of sufficient data, vanilla convolutional neural networks will use any correlation they can find to classify training samples, even when they are spurious~\citep{kim2019learning}. In this work, we aim to tackle both of these issues.

Compositional Zero-Shot Learning (CZSL)~\citep{misra2017red} is the problem of learning to model novel objects and their attributes as a composition of visual primitives. Previous works in CZSL \citep{misra2017red, purushwalkam2019task, li2020symmetry} largely ignore the dependencies between classes with shared visual primitives, and the spurious correlations between attributes and objects. More recently, \citet{atzmon2020causal} tackle the latter by ensuring conditional independence between attribute and object representations, while \citet{naeem2021learning} explicitly promote dependencies between the primitives and their compositions. While the independence approach improves generalization, it hurts accuracy on seen classes by removing useful correlations. The explicit dependencies, on the other hand, can share these useful correlations with unseen classes, but there will always be some that are spurious, hurting generalization. 

In this work, we propose to take advantage of the strengths of both approaches, respectively, by learning independent visual representations of objects and attributes, and by learning their compositions for the target classes. First, we represent visual primitives by learning local independent prototypical representations. Prototype networks \citep{snell2017prototypical} learn an embedding function, where inputs of the same class cluster around one prototypical representation of that class. Here we adopt such a function for learning prototypes of objects and attributes. Next, we leverage a compositional graph to learn the dependencies between the independent prototypes on the one hand, and the desired seen and unseen classes on the other, by propagating the prototypes to compositional nodes. Here, the compositional graph allows some information to be shared between objects that share attributes, e.g., between tigers and zebras. The proposed method, ProtoProp, is outlined in Figure 1.

\textbf{Our main contributions are}: 1) We propose a novel graph propagation method that learns to combine local, independent, attribute and object prototypes into one compositional prototype that can accurately detect unseen compositional classes. 2) A spatial attention-based pooling method that allows us to obtain differentiable attribute and object patches for use in an independence loss function. 3) Our method effectively deals with bias from an underspecified dataset by learning conditionally independent representations that then take on the dependencies of the desired target distribution. 4) We validate through ablations the importance of each part of the method (local prototypes vs semantic embeddings, independence loss, backbone finetuning) and their contribution to the final results. 5) We show that we improve on state-of-the-art results on two challenging compositional zero-shot learning benchmarks: $2.5$ to $20.2$\% harmonic mean improvement on AO-Clevr~\citep{atzmon2020causal} and $3.1\%$ harmonic mean improvement on UT-Zappos~\citep{yu2014fine} compared to the best existing method.

\section{Related work}
\textbf{Compositional zero-shot learning} (CZSL) methods aim to recognize unseen compositions from known visual primitives in the form of attributes and objects. One line of work considers embedding the visual primitives in the image feature space. \citet{misra2017red} use the weight vectors of linear SVMs as embeddings for the visual primitives, which they transform to recognize unseen compositions. \citet{li2020symmetry} look at attribute-object compositions through the lens of symmetry inspired by group theory. A different line of work considers a joint embedding function on the image, attribute, and object triplet, allowing the model to learn dependencies between the image and its visual primitives. \citet{purushwalkam2019task} train a set of modular networks together with a gating network that can `rewire' the classifier conditioned on the input attribute and object pair. \citet{atzmon2020causal} take a causal view of CZSL, trying to answer which intervention caused the image. They apply conditional independence constraints to the representations of the visual primitives, sacrificing accuracy on the seen data but performing well on unseen data by removing correlations that are useful but hurt generalization to novel compositions. \citet{naeem2021learning}, on the other hand, explicitly promote the dependency between all primitives and their compositions within a graph structure, though they rely on pretrained semantic embeddings which may differ in distribution from the visual concepts they describe.

Our proposed method combines the strengths from \citet{atzmon2020causal} and \citet{naeem2021learning} by first learning conditionally independent representations of the visual primitives, which are then propagated through a compositional graph to learn the dependency structure of the desired target distribution. Unlike most other methods, we are not reliant on pretrained word embeddings, but instead learn the representations for our visual primitives directly from the training data. Instead of a linear kernel like \citet{atzmon2020causal}, we use a Gaussian kernel for our independence loss as we find that its ability to capture higher-order statistics is beneficial in terms of accuracy. Like \citet{naeem2021learning}, we train our model fully end-to-end, including the feature extractor, as learning a good embedding is often more beneficial than an overly complicated method applied to suboptimal embeddings~\citep{tian2020rethinking}.

\textbf{Prototypical networks} \citep{snell2017prototypical} aim to learn an embedding function where inputs of the same class cluster around one prototypical representation of that class. They measure L2 distance between samples in pixel space, which is sensitive to non-semantic similarities between images, such as objects from different classes with a similar background and light conditions. \citet{li2018deep} move the similarity metric to latent space and utilise an autoencoder to directly visualize the prototypes, but they only test on simple MNIST-like benchmarks. \citet{chen2019looks} extend the method to multiple local prototypes per class, where prototypes are compared to local image patches instead of the entire average-pooled output of a CNN, and evaluate on the more complicated fine-grained bird classification dataset CUB \citep{WahCUB_200_2011}. We take a similar local-prototype approach, but we use direct attribute and object supervision, enabling parameter sharing between classes and allowing the prototypes to be used for zero-shot classification. While for most methods these prototypes are used for the final classification, in our case they are an intermediate representation.

\textbf{Graph neural networks} (GNNs) are models that can work directly on the structure of a graph. They have been first proposed by~\citet{gori2005new}, later elaborated upon by~\citet{scarselli2008graph} and popularised by~\citet{kipf2017semi} in their work on the Graph Convolutional Neural Network (GCN). GNNs grow more popular every year, and a lot of improvements have been proposed recently~\citep{wu2020comprehensive}. Just like the GCNs were inspired by CNNs, most improvements are inspired by other areas of deep learning such as attention mechanisms in Graph Attention Networks~\citep{velickovic2018graph}, but this jump from existing deep learning fields to GNNs has overlooked simpler methods. \citet{pmlr-v97-wu19e} have taken a step back and stripped down GNNs to their simplest parts by removing nonlinearities and collapsing weight matrices until all that was left was feature propagation followed by a linear model. In the same vein,~\citet{huang2021combining} show that a simple Multilayer Perceptron (MLP) ignoring graph structure followed by a label propagation post-processing step often greatly outperforms GNNs. These simplified methods are mostly limited to transductive settings (test nodes are available at train time) and graphs that exhibit strong homophily (similar nodes are connected). Many real-world graphs fit those criteria, including the graph we use in this work.

\section{Method}
\label{sec:method}
In compositional zero-shot learning (CZSL), we have a set of images $X$ and compositional classes with compositional labels $Y \subseteq A \times O$, where $A$ is a set of attribute labels and $O$ a set of object labels. The labels are subdivided into $Y = Y_s \cup Y_u$, where $Y_s$ are the seen labels for the training set and $Y_u$ unseen labels for the validation and test sets, with $Y_s \cap Y_u = \emptyset$. We denote the training data consisting of only seen compositional classes as $X_s$. Finally, CZSL assumes that each attribute and object is seen in at least one training data point, or more formally $\forall p \in A \cup O~\exists y \in Y_s$ s.t. $p \cap y \neq \emptyset$.

\subsection{Prototype-based representations}
\label{sec:proto_method}
In this section we describe how we learn local attribute and object prototypes, before they are propagated and combined into compositional prototypes in Section~\ref{sec:prop}. Prototypes are an average representation of a target class, with strong generalization and interpretability properties. We employ a local prototype approach similar to \citet{chen2019looks}, though we use direct supervision, encouraging the feature extractor to learn local image representations that encode the attribute and object labels. We use a ResNet-18~\citep{he2016deep} backbone for fair comparison to other CZSL methods, but the method is backbone agnostic. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{img/prototypes_orthogonal_arrows.pdf}
    \caption{\textbf{Local prototypes with softmax pooling}: Local image patches $x_{ij}$ from the layer just before the average pooling layer of a ResNet-18 are compared to prototype vectors $p_k$ through a similarity function, here a dot product. This outputs a compatibility score $s$, which is optimized via $\mathcal{L}_{CE}$ (cf. Section \ref{sec:proto_method}). Similar to spatial attention, these patch-prototype compatibility scores are passed through a softmax function and via Hadamard (pointwise) product a weighted sum $z_k$ of the original feature map is calculated, which are passed to the HSIC function to promote independence between object and attribute prototypes (cf. Section \ref{sec:hsic_method})}
    \label{fig:prototypes}
\end{figure}

Figure \ref{fig:prototypes} shows an example of a prototype layer. The layer has a set of prototype vectors $\mathbf{P} = \{p_j\}_{j=1}^k$, $p_j \in \mathbb{R}^C$ where each target class is represented by one prototype. $k = |Y_i|$ is the number of attribute or object targets, and $i \in \{0, 1\}$ indicates if we are training on attribute or object labels respectively. The layer takes as input the $H \times W \times C$ output of a CNN just before its final pooling layer, and calculates a compatibility score $s = \langle x_{ij}, p_k \rangle $ (e.g., cosine similarity, L2 norm, dot product) between the input patches $x_{ij} \in \mathbb{R}^C$ over the spatial dimensions $H \times W$ and the prototypes. We find that a dot product similarity metric leads to better generalization. To save on parameters and avoid overfitting, we do not use a fully connected layer. Instead, the maximum patch-prototype similarity score is used directly as the compatibility score for the corresponding prototype's class. This compatibility score is optimized through a cross-entropy loss function:
%
\begin{equation}
\mathcal{L}_{CE} = \frac{1}{|S|} \sum_{s, y~\in S} - \log\left(\frac{\exp(s[y_i])}{\sum_j^{|s|} \exp(s[j])}\right)
\end{equation}

Here, with $f_p$ as our prototype layer, $S = \{ s, y \in \left(f_p(X_s) \mid Y_s \right)\}$ is the set of prototype similarity scores and target labels for the training data. $i \in \{0, 1\}$ indicates if we are training on attribute or object labels respectively.

Cross-entropy loss naturally encourages clustering between samples of the same class and separation from other classes, but we find that an additional loss to push these properties even more is beneficial. For that purpose we adopt the cluster and separation costs proposed by \citet{chen2019looks}:
\begin{equation}
\text { Clst }=\frac{1}{|X|} \sum_{i=1}^{|X|} \min_{j: p_{j} \in \mathbf{P}_{y_{i}}} \min_{ z \in x_{ij}}\left\|z-p_{j}\right\|_{2}^{2} \qquad \text { Sep }=-\frac{1}{|X|} \sum_{i=1}^{|X|} \min_{j: p_{j} \notin \mathbf{P}_{y_{i}}} \min_{z \in x_{ij}}\left\|z-p_{j}\right\|_{2}^{2}
\end{equation}

The costs $\operatorname{Clst}$ and $\operatorname{Sep}$ encourage a higher degree of clustering between prototypes of the same class and a greater distance to prototypes of different classes, respectively. The separation cost is only applied to the object prototypes, as the attribute prototypes benefit from less distant representations.

Our Hilbert-Schmidt Independence Criterion (HSIC) loss (details in Section \ref{sec:hsic_method}) requires the input patches with the highest prototype similarity scores as its input, but that would require the use of the argmax function which has a gradient of 0 almost everywhere. Similar to spatial attention, we use the softmax function over the similarity map as a differentiable proxy. The softmax outputs used as weights in a weighted sum of the input patches $x_{ij}$ serve as an approximation of the input patch with the highest similarity score (cf. Figure \ref{fig:prototypes}), and ensure that each patch containing information about attribute or object labels is affected by the independence loss proportional to the strength of their appearance (i.e. their similarity score).

\subsection{Attribute-object independence}
\label{sec:hsic_method}
To combat the bias in the training data we leverage a differentiable independence metric such that the model can learn representations for the attributes of images that are uniformly distributed w.r.t. their object labels and vice versa.

The Hilbert-Schmidt Independence Criterion (HSIC)~\citep{gretton2007kernel} is a kernel statistical test of independence between two random variables $A$ and $B$. In the infinite sample limit $\operatorname{HSIC}(A,~B) = 0 \iff A \perp \!\!\! \perp  B$ as long as the chosen kernel is universal in the sense of \citet{steinwart2001influence}. We use a Gaussian kernel, as we find that its ability to
capture higher-order statistics is beneficial in terms of accuracy, and follow \citet{gretton2007kernel} in setting the kernel size to the median distance between points. See the Appendix for more details. We define our independence loss function as follows, slightly deviating from \citet{atzmon2020causal}:
\begin{equation}
\mathcal{L}_{hsic} = \lambda_h \frac{\operatorname{HSIC}(z_a,~O) + \operatorname{HSIC}(z_o,~A)}{2} 
\end{equation}

where $\lambda_h$ is a hyperparameter controlling the contribution of $\mathcal{L}_{hsic}$ to the final loss, $z_a$ is the softmax-pooled output of the attribute prototype layer (cf. Section \ref{sec:proto_method}), $z_o$ the softmax-pooled output of the object prototype layer, and $O$ and $A$ the corresponding one-hot encodings of the object and attribute labels respectively.

\subsection{Prototype propagation graph}

Figure~\ref{fig:arch} shows the full architecture. Two types of prototype layers are trained, one on the object labels and one on the attribute labels, leaving us with centroids with confounding information removed. The prior knowledge about the compositional target classes and the attributes they afford can be represented as a compositional graph $G = (\mathbf{P}_a \cup \mathbf{P}_o \cup \mathbf{C}_y, E)$. It consists of the attribute prototypes $\mathbf{P}_a$, object prototypes $\mathbf{P}_o$, and compositional classes $\mathbf{C}_y \subseteq A \times O$ (a subset of which have no training samples). The edges are undirected, $E = \{x, y~|~ x \in \mathbf{P}_a \cup \mathbf{P}_o,~ y \in \mathbf{C}_y : x \in y \}$, i.e. all attribute (e.g., red) and object (e.g., cube) nodes are connected to the compositional classes they are a part of (e.g., red cube). This corresponds to a bipartite graph with the attribute and object prototypes in one set and the compositional classes in the other. In the case of AO-Clevr it is a complete bipartite graph, but for real-world datasets such as UT-Zappos this is not the case as, e.g., not all shoes are available in all materials.

\label{sec:prop}
\begin{figure}[]
    \centering
    \includegraphics[width=\textwidth]{img/architecture_new.pdf}
    \caption{\textbf{ProtoProp: overview of proposed method.} Local attribute and object prototypes are trained with independence (HSIC) loss and mapped to nodes in a simplified graph neural network, which learns to combine them into compositional prototypes of both seen and unseen classes. The maximum score in the dot product similarity map between the compositional prototypes and average-pooled backbone output is the final classification prediction.}
    \label{fig:arch}
    \medskip
    \small
\end{figure}

As defined above, the conditionally independent prototypes are mapped directly to the nodes of the graph (cf. Figure \ref{fig:arch}), and through shared weights a Graph Neural Network (GNN) learns to propagate the prototypes to the compositional nodes to form new compositional prototypes. The compositional nodes are initialized with zeros. Our GNN is a 2-layer GCN~\citep{kipf2017semi}; inspired by SGC~\citep{pmlr-v97-wu19e} we remove all nonlinearities as we find that simple linear combinations lead to better generalization:
%
\begin{equation}
\mathbf{X}^{\prime} = {\mathbf{\hat{D}}^{-1/2} \mathbf{\hat{A}}
\mathbf{\hat{D}}^{-1/2} } \mathbf{X} \mathbf{\Theta}
\end{equation}

Here $\textbf{X}$ are the input nodes, $\mathbf{\Theta}$ a learnable weight matrix, $\mathbf{\hat{A}} = \mathbf{A} + \mathbf{I}$ denotes the adjacency matrix with inserted self-loops, and $\hat{D}_{ii} = \sum_{j} \hat{A}_{ij}$ its diagonal degree matrix.

We then compute another dot product similarity map $s_c$ between the compositional prototypes and the average-pooled output of the backbone, which is again optimized via cross-entropy loss function. Our final compositional classification is $y = \text{argmax}(s_c)$, i.e., the class corresponding to the compositional prototype with the maximum similarity score. The shared weights in the GNN ensure that the model learns a general composition of attributes and objects that can generalize to unseen compositional classes. By initializing the node features with independent prototypes, the graph is able to learn the dependencies encoded by the compositional graph, including the novel zero-shot classes, instead of the biases of the training data.


\section{Experiments}
\subsection{Implementation}
\label{sec:implementation}
We implement our model using the PyTorch library~\citep{NEURIPS2019_9015}, using some of the boilerplate code provided by~\citet{nagarajan2018attributes} and \citet{purushwalkam2019task}. The GNN is implemented using PyTorch Geometric~\citep{FeyLenssen2019}. For our independence loss we use a normalized implementation of HSIC provided by \citet{DBLP:conf/aaai/MaLK20}\footnote{The code will be available upon acceptances.} We use the Adam~\citep{Kingma2014} optimizer. Experiments have been run on an 11GB GeForce GTX 1080 Ti graphics card.
On AO-Clevr we reach our best result in 1-5 epochs on all splits, at \textasciitilde5 minutes per epoch. On UT-Zappos we reach our best result in \textasciitilde30 epochs at \textasciitilde1.5 minutes per epoch. See the Appendix for detailed hyperparameters and grid search ranges.

\subsection{Evaluation}
\label{sec:eval}
We evaluate our approach on two CZSL datasets. Previous works also evaluate on MIT-States~\citep{isola2015discovering}, but \citet{atzmon2020causal} conclude through a large-scale user study that the dataset has a level of \textasciitilde70\% label noise, making it too noisy for evaluating compositionality.

\textbf{AO-Clevr}~\citep{atzmon2020causal, johnson2017clevr} is a synthetic dataset consisting of 3 types of objects (sphere, cube, cylinder) and 8 attributes (red, purple, yellow, blue, green, cyan, gray, brown), with 24 compositional classes in total. There are 6 splits with a varying ratio of unseen to seen classes, ranging from 2:8 to 7:3, allowing insights into the performance of models as the proportion of unseen classes increases.

\textbf{UT-Zappos}~\citep{yu2014fine} is a fine-grained dataset of types of shoes with 12 objects (e.g., boat shoes), 16 attributes (e.g., leather), and 116 compositional classes. We use the split proposed by \citet{purushwalkam2019task}, with 83 seen classes in the training set, 15 seen and 15 unseen classes in the validation set, and 18 seen and 18 unseen classes in the test set.

\textbf{Metrics}: like other recent works we adopt the generalized zero-shot evaluation protocol proposed by \citet{purushwalkam2019task, chao2016empirical}. Instead of evaluating only on the unseen classes as in the Closed setting, the Generalized setting considers both seen and unseen classes in the validation and test sets. To account for the inherent bias towards seen classes,  \citet{chao2016empirical} add a calibration bias term to the activations of unseen classes. As the value of the bias is varied between $-\infty$ and $+\infty$, they draw a curve with the accuracy on the unseen classes on the y axis and the accuracy on the seen classes on the x axis, and report the Area Under the Curve (AUC). The harmonic mean is defined as $2\cdot\frac{Acc_s \cdot Acc_u}{Acc_s + Acc_u}$, where $Acc_s$ is the accuracy on the seen classes and $Acc_u$ the accuracy on the unseen classes. It penalizes large differences between the two metrics and as such indicates how well the model performs on both seen and unseen classes simultaneously. We also follow prior works in reporting the Closed seen accuracy where only the seen classes are considered (corresponding to a bias of $-\infty$) and closed unseen accuracy where only the unseen classes are considered (corresponding to a bias of $+\infty$). For the results on AO-Clevr we report the seen and unseen accuracy components of the harmonic mean in accordance to \citet{atzmon2020causal}. \citet{atzmon2020causal} do not perform post-hoc bias calibration, but instead handle the seen-unseen bias during training.

Like \citet{naeem2021learning}, we train our feature extractor end-to-end with the rest of our model. Other methods keep the backbone fixed, but \citet{naeem2021learning} have shown that they perform worse when allowed to finetune as they will then start to overfit. While our method is designed to take full advantage of the backbone, we show our results with a fixed backbone in our ablations.

\subsection{Results}
For each of the results we select the best model by the best harmonic mean on the validation set and report the results for all metrics on the test set. The results with error bars come from 5 training runs with random initializations. We note that, unlike other methods, Causal~\citep{atzmon2020causal} does not perform post-hoc bias calibration,  but instead tackles the bias against seen classes during training, which is a benefit of their approach.

\textbf{AO-Clevr} Figure~\ref{fig:results_clevr} shows the results on AO-Clevr. We also report the results for CGE~\citep{naeem2021learning} (after a grid search using their own codebase) as they have not reported experiments on this dataset. See the Appendix for the exact values and error bars. We observe that for the 3:7, 5:5, 6:4 and 7:3 splits, one or more colors are not part of the training data, making a large portion of the validation and test classes impossible to classify through compositional methods on those splits. Our method performs better than the compared methods on all splits, with minimal impact on the seen accuracy. The improvements are in the range of $2.5$ to $20.2$\% for the harmonic mean and $3.3$ to $17.0$\% for the unseen accuracy, with higher gains as the proportion of unseen classes increases significantly.

\begin{figure}[htb]
    \centering
    \includegraphics[scale=0.67,clip,trim=0 2mm 0 2mm]{img/Harmonic_performance.pdf}
    \includegraphics[scale=0.67,clip,trim=14mm 2mm 0 2mm]{img/Unseen_performance.pdf}
    \includegraphics[scale=0.67,clip,trim=14mm 2mm 0 2mm]{img/Seen_performance.pdf}
    
    \caption{\textbf{AO-Clevr results} Plots of the seen and unseen accuracy and their harmonic mean (y-axis) as the ratio of unseen:seen compositional classes (x-axis) increases. ProtoProp consistently outperforms state-of-the-art methods, especially when the portion of unseen classes grows.}
    \label{fig:results_clevr}
\end{figure}

\textbf{UT-Zappos} Table~\ref{tab:results_zappos} shows the results on UT-Zappos. Because earlier works only report a single best run, we report the average over 5 random initializations including standard error for the two best performing previous models (CGE and TMN), using their own codebase and reported hyperparameters. We find that, especially for this dataset, the error bars are important since it is highly susceptible to random initialization; a rare few of our runs could reach a harmonic mean of 60+ while the average is around 50, with similar observations for other methods. Our method outperforms earlier methods on all metrics except for the closed unseen accuracy where CGE performs slightly better. Our method performs especially well when both seen and unseen classes are taken into account, as evidenced by the AUC and harmonic mean improvements.

\begin{table}[]
\centering
\caption{\textbf{UT-Zappos results}, ProtoProp improves on state-of-the-art results in the AUC, Closed seen, and harmonic mean metrics.}
\label{tab:results_zappos}
\begin{tabular}{lllll}

\hline
Method           & AUC           & Closed seen   & Closed unseen & Harmonic \\ \hline
AttOp \citep{nagarajan2018attributes} & $25.9$          & $59.8$          & $54.2$          & $40.8$          \\
LE+ \citep{misra2017red}  & $25.7$          & $53.0$          & $61.9$          & $41.0$          \\
SymNet \citep{li2020symmetry} & $23.9$          & $53.3$          & $57.9$          & $39.2$          \\
TMN \citep{purushwalkam2019task} & $24.7 \pm 4.4$          & $58.8 \pm 1.4$          & $49.4 \pm 4.7$          &  $40.7 \pm 2.0$        \\
Causal \citep{atzmon2020causal} & $23.3 \pm 0.3$     & -             & $55.4 \pm 0.8$     & $31.8 \pm 1.7$          \\
CGE \citep{naeem2021learning} & $32.5 \pm 1.5$ & $61.0 \pm 0.9$ & $\mathbf{65.9 \pm 1.2}$ & $47.1 \pm 1.7$ \\ \hline
ProtoProp (ours) &  $\mathbf{34.7 \pm 0.8}$ & $\mathbf{62.1 \pm 0.9}$ & $65.5 \pm 0.2$ & $\mathbf{50.2 \pm 1.3}$ \\ \hline
\end{tabular}
\end{table}

\subsection{Ablations}
To determine the effect of our visual features in the form of prototypes, in contrast to the semantic features in most prior works, we perform ablations where we replace our prototypes with pretrained word embeddings. We also take a look at the importance of the independence loss function and finetuning the backbone.

\textbf{Importance of prototypes} Table \ref{tab:ablation} shows the results of our ablations on the 4:6 split on AO-Clevr. Row 1 shows the results for our proposed method without the independence loss function, in row 2 we add the independence loss but freeze the backbone, and row 3 shows the results with both independence and finetuning, as described in the method section. The prototypes perform worse than semantic embeddings when used as node features without the independence loss, but with the independence loss they receive significant gains and come out on top with an $8.3\%$ increase in harmonic mean accuracy over the semantic features. With a frozen backbone the harmonic mean for our method is just slightly higher ($0.8\%$) than Causal on the same split, though it reaches that accuracy in a fraction of the time. 

\textbf{Visual vs. semantic} For the ablation in row 4 we use semantic word embeddings (word2vec~\citep{mikolov2013distributed}) as node features, which is equivalent to~\citet{naeem2021learning} with a simplified GCN. In row 5 we also train our local prototypes but don't use them during classification, only to influence the local features the feature extractor learns. Training the local prototypes improves the results slightly even when they are not used for the final classification, by encouraging the backbone to learn local features that represent the target attributes and objects.

\begin{table}[]
\centering
\caption{\textbf{Ablation on the AO-Clevr 4:6 split}: Checkmarks indicate whether prototypes are used as node features, semantic vectors as node features, local prototypes are trained with independence loss, or the backbone is frozen respectively. Prototypes trained when semantic node features are used are not part of the final classification but still improve the feature extractor output.}
\label{tab:ablation}
\begin{tabular}{lcccccc}
\hline
Node Features & Indep. Proto & Finetune & Seen & Unseen & Harmonic \\ \hline
  Visual &   &  \checkmark    & $94.5 \pm 0.1$     & $77.0 \pm 2.8$       & $84.8 \pm 1.7$         \\
  Visual & \checkmark &     & $78.2 \pm 1.1$ & $73.0 \pm 1.0$ & $75.5 \pm 0.8$ \\
  Visual & \checkmark &   \checkmark   & $\mathbf{97.9 \pm 1.0}$     & $\mathbf{95.5 \pm 0.9}$       & $\mathbf{96.7 \pm 0.7}$         \\ \hline
 Semantic &   &   \checkmark  & $95.4 \pm 1.1$    & $82.4 \pm 0.7$       & $88.4 \pm 0.1$         \\
 Semantic &  \checkmark &  \checkmark    & $97.3 \pm 1.2$     & $84.5 \pm 1.4$    & $90.4 \pm 0.9$         \\ \hline
\end{tabular}
\end{table}

\textbf{Effect of independence} Table~\ref{tab:ablation} shows that the prediction accuracy breaks down when not using the independence loss. Figure~\ref{fig:hsic_plot} provides some intuition for this through a t-SNE~\citep{van2008visualizing} plot of softmax-pooled attribute (in this case color) representations from our model on AO-Clevr. Without the loss there are three strongly separated clusters per color (one per shape), and many colors are embedded closer to different colors of the same shape. When we do use the independence loss, each of the colors is part of their own single cluster and the only correlations left are those between visually similar colors, improving the accuracy of the compositional prototypes after the propagation step.

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{img/hsic_plot_new.png}
    \caption{t-SNE plot of softmax-pooled color patches for AO-Clevr that were trained with (left) and without (right) HSIC loss, where with the HSIC loss we get a homogeneous grouping of the attributes and no fragmentation by spurious confounders.}
    \label{fig:hsic_plot}
\end{figure}

\section{Limitations}
\label{sec:limitations}
Like typical CZSL works, our method is limited to a single attribute and object label per image, and each individual attribute and object needs to be seen in at least one training data point. Existing attribute datasets are often either noisy or do not work in the aforementioned single-attribute setting. But if this initial hurdle of quality datasets is overcome, compositional methods make it easier to handle the addition of new classes, especially for rare objects with little to no available images. To extend the scope to more realistic settings and other datasets, in future work we would extend the method to multiple attributes, e.g., through attribute prototype regression~\citep{xu2020attribute}, or by leveraging textual descriptions that can be scraped from the web in a semi-supervised fashion~\citep{zhu2018generative}.

The hyperparameters can be difficult to tune, as there are hyperparameters for the backbone, prototype layer, independence loss, and compositional GNN. We do find, however, that the prototype layers can be tuned separately from the rest, as the best performing individual prototypes and the hyperparameters that led there also perform best when used in conjunction with the GNN. As such, most hyperparameters can be adopted from existing prototype-based classification works that have been trained on a similar dataset.

\section{Conclusion}
In order to have a chance at recognizing the long-tailed distribution of visual concepts, our models need to become better at recognition through shared visual primitives. To this end,  we have proposed a novel prototype propagation method for compositional zero-shot learning. Our method learns prototypes of visual primitives that are independent from the other visual primitives they appear with, and propagates those prototypes through a compositional graph in order to recognize unseen compositions. The method works with just the attribute and object annotations, and, as such, is not reliant on external sources of information, such as hierarchy graphs or pretrained semantic embeddings. We evaluate our work on two CZSL benchmarks, and improve on state-of-the-art results, especially with large fractions of unseen classes, with minimal impact on the accuracy of the seen classes.

\bibliography{references}

\appendix

\section{Appendix}
\subsection{Hyperparameters}
\label{ap:hyperparam}
\textbf{AO-Clevr} Like \citet{atzmon2020causal} we performed grid searches over the following splits: \{2:8, 5:5, 6:4, 7:3\}. We used the largest batch size that could fit in memory on our limited hardware, which was 256 for an image size of 224x224. For the learning rate (Adam~\citep{Kingma2014} optimizer) we searched in the range of \{0.001, 0.0001, 1e04, 5e-4, 5e-5\}, with weight decay \{0, 5e-4. 5e-5\}. We chose a weight decay of 5e-5 and learning rate of 5e-4 until the 4:6 split and 1e-4 afterwards.

Prototype dimension: \{256, 300, 512\}, backbone output dimension: \{256, 300, 512\}, Graph layers: \{1, 2, 3\}, graph hidden dimension: \{256, 512, 1024, 2048, 4096\}, $\lambda_h$: \{0, 1, 5, 10, 25, 50\}, Clst: \{0, 0.1, 0.05, 0.01, 0.005\}, Sep: \{0, 0.1, 0.05, 0.01, 0.005\}.

We chose a prototype dimension of 256, backbone output of 512, 2 graph layers, graph hidden dimension of 512, $\lambda_h$ of 10, Clst and Sep of 0.01.

\textbf{UT-Zappos} we again used the Adam optimizer, with learning rate in the ranges \{5e-5, 5e-4, 5e-3\}, and weight decay \{0, 5e-4. 5e-5\}, where we chose a learning rate and and weight decay of 5e-5 and a batch size of 128. For the rest of the parameters we searched the same ranges as above, where the same choices were optimal as for AO-Clevr.


\subsection{Hilbert-Schmidt Independence Criterion}
\label{ap:hsic}
The (biased) empirical HSIC estimator~\citep{gretton2007kernel} is defined as:
$$ \operatorname{HSIC}(U, V) = \frac{1}{m^{2}} \operatorname{trace}(\mathbf{K H L H}) $$

\noindent Where $\mathbf{K}$ and $\mathbf{L}$ are $m \times m$ matrices with entries $k_{ij}$ and $l_{ij}$,   $\mathbf{H} = \mathbf{I} \frac{1}{m}\mathbf{1}\mathbf{1}^\top$ , and $\mathbf{1}$ is a $1 \times m$ vector of ones. The elements of $\mathbf{K}$ and $\mathbf{L}$ are outputs of a kernel function over the inputs $U$ and $V$ such as the (universal) gaussian kernel $k_{i j} :=\exp \left(-\sigma^{-2}\left\|u_{i}-u_{j}\right\|^{2}\right)$ where $\sigma$ is the kernel size. We follow \citet{gretton2007kernel} in setting the kernel size to the median distance between points, but universality of the gaussian kernel holds for any kernel size. The empirical estimator has a bias in the order of $O(m^{-1})$ which is negligible at even moderate sample sizes.

\subsection{AO-Clevr Results}
\label{ap:clevr_results}
\begin{table}[htb]
\caption{\textbf{AO-Clevr Results}: ProtoProp consistently outperforms state of the art methods, especially when the portion of unseen classes grows.}
\label{tab:results_clevr}
\begin{tabular}{llll|lll}
\hline
\textbf{}    & ProtoProp (ours)     &                 &                   & Causal \citep{atzmon2020causal}     &                 &                    \\ 
U:S & Seen & Unseen & Harmonic & Seen & Unseen & Harmonic \\ \hline
2:8 & $98.6 \pm 0.6$     & $99.3 \pm 0.4$       & $98.9 \pm 0.3$         & $89.7 \pm 1.9$ & $77.7 \pm 1.4$ & $83.2 \pm 1.2$ \\
3:7 & $96.3 \pm 0.9$     & $81.7 \pm 0.1$       & $88.4 \pm 0.4$         & $80.9 \pm 3.6$ & $72.2 \pm 1.0$ & $75.7 \pm 2.3$ \\
4:6 & $97.9 \pm 1.0$     & $95.5 \pm 0.9$       & $96.7 \pm 0.7$         & $84.1 \pm 1.8$ & $67.4 \pm 2.0$ & $74.7 \pm 1.7$ \\
5:5 & $96.7 \pm 0.2$     & $63.1 \pm 0.4$       & $76.4 \pm 0.3$         & $83.8 \pm 0.8$ & $47.1 \pm 4.5$ & $59.8 \pm 3.9$ \\
6:4 & $95.6 \pm 3.7$     & $38.6 \pm 4.7$       & $54.6 \pm 3.9$         & $86.1 \pm 2.9$ & $26.9 \pm 0.5$ & $40.9 \pm 1.0$ \\
7:3 & $91.5 \pm 4.6$     & $39.3 \pm 1.6$       & $54.9 \pm 0.6$         & $69.3 \pm 6.1$ & $22.8 \pm 3.0$ & $33.7 \pm 4.2$ \\ \hline
\end{tabular}
\vskip 2mm

\begin{tabular}{llll|lll}
\hline
\textbf{}    & CGE~\citep{naeem2021learning}~~~~~~~~~~~~~     &                 &  &  TMN~\citep{purushwalkam2019task}  &             &            \\ 
U:S & Seen & Unseen & Harmonic & Seen & Unseen & Harmonic  \\ \hline
2:8 & $96.7 \pm 2.0$     & $96.0 \pm 1.1$       & $96.4 \pm 1.5$  & $85.8 \pm 0.9$ & $79.7 \pm 4.4$ & $82.4 \pm 2.1$   \\
3:7 & $98.2 \pm 0.7$     & $74.4 \pm 3.4$       & $84.6 \pm 2.5$ & $86.5 \pm 0.3$ & $62.2 \pm 4.9$ & $72.0 \pm 3.4$    \\
4:6 & $95.5 \pm 1.1$     & $80.4 \pm 1.3$       & $87.3 \pm 0.7$  & $83.8 \pm 2.6$ & $68.1 \pm 4.1$ & $74.5 \pm 1.6$    \\
5:5 & $94.1 \pm 2.5$     & $55.4 \pm 0.4$       & $69.7 \pm 0.9$  & $84.7 \pm 2.2 $ & $38.0 \pm 3.0$ & $51.5 \pm 2.2$   \\
6:4 & $95.4 \pm 0.1$     & $33.2 \pm 0.7$       & $49.3 \pm 0.7$  & $83.7 \pm 0.4$ & $18.1 \pm 2.9$ & $29.1 \pm 3.7$   \\
7:3 & $77.9 \pm 0.1$    &  $22.3 \pm 0.2$       & $34.7 \pm 0.3$  & $88.1 \pm 2.5$ & $5.8 \pm 0.9$ & $10.8 \pm 1.6$   \\ \hline
\end{tabular}

\end{table}

\end{document}