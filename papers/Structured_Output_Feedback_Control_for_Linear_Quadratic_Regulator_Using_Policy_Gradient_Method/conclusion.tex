% \documentclass[main]{subfiles}
% \begin{document}
\section{Conclusion}\label{sec:conclusion}
In this paper, we considered the non-convex optimization problem with convex constraints
based on the output feedback version of LQR problems
under the assumption that system parameters are unknown.
To solve the problem, we proposed the policy gradient algorithm based on the gradient projection method and the zeroth order optimization.
We proved its global convergence to $\varepsilon$-stationary points with high probability.
We also proposed the variance reduction method using the baseline technique and proved that it is almost optimal.
In the numerical experiments, we showed that the baseline technique significantly reduces the variance in the gradient estimation and the model free method can achieve low LQR cost. 

Policy Gradient Projection can be extended to the objective function with regularization terms
using the proximal gradient method. In this setting, we are able to consider trade-offs
between cost function and structure such as sparsity~\cite{park2020structured}.
However, the convergence analysis would be more difficult,
and it is left for a future work.
In addition, the convergence of the gradient method with fixed step size could be slow since the smooth constant $L$ can be large depending on the initial feedback gain $K_0$. To overcome this issue, the gradient method with adaptive step size in the model based setting was considered in~\cite{bu2019lqr} and optimization methods on Riemannian manifolds were studied in~\cite{talebi2022policy}. Therefore, applying the adaptive step size to the model free algorithm is one of the important directions of future works.
Other interesting directions of future works would be
analysis for the natural policy gradient method~\cite{kakade2001natural} or other variants of the policy gradient method.

% \end{document}