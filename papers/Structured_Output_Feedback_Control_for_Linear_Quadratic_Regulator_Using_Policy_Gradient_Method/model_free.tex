\section{Model Free Algorithm} \label{sec:model-free}
In this section, we consider problem \eqref{eq:problem} in the model free setting. That is, we assume that $(A, B, C, \mathcal D)$ in system \eqref{eq:system} is unknown. First, we introduce a gradient estimation algorithm
based on the derivative-free optimization and show that the estimated gradient is close to the exact gradient with high probability.
Then, we provide Policy Gradient Projection Algorithm in Algorithm \ref{alg:mfgp}.
Despite the error in the estimated gradient, we can prove the global convergence to $\varepsilon$-stationary points. 
Finally, we propose a variance reduction method using the baseline technique and prove its optimality.
\subsection{Gradient estimation}
In the model free setting, the exact gradient of the objective function $f(K)$
in~\eqref{eq:objectivefunction} cannot be accessed directly.
Then, based on the zeroth-order or derivative-free optimization approach~\cite{fazel2018global},
we propose Algorithm~\ref{alg:gradest1}
to calculate the stochastic estimate $\hat \grad f(K)$ of the gradient $\grad f(K)$.

\begin{algorithm}
  \caption{Gradient Estimation}
  \label{alg:gradest1}
  \begin{algorithmic}[1]
    \REQUIRE{$K\in \Omega, N>0, r > 0, \tau > 0$}
    \FOR{$i=1$ to $N$}
    \STATE{Sample $U_i$ from the uniform distribution $\mathcal{S}$ over matrices with $\normF{U_i} = \sqrt{mp}$.}
    \STATE{Simulate the system
      \begin{align}
        \dot x_i(t)  = Ax_i(t)+Bu_i(t),\quad
        y_i(t)       = Cx_i(t),
      \end{align}
      where $u_i(t) = -(K+rU_i)y_i(t)$, $x_i(0) \sim \mathcal D$
      until time $\tau$ and
      calculate the empirical cost %$c_i$ by
    %  \begin{align}
       $c_i = \int_0^\tau \qty[y_i^\top(t) Q y_i(t) + u_i(t)^\top R u_i(t)]dt$.
     % \end{align}
     }
    \ENDFOR
    \STATE{Define the estimated gradient by
      \begin{align}
        \hat \grad f(K) = \frac{1}{rN}\sum_{i=1}^N c_iU_i. \label{eq:hatgrad}
      \end{align}}
    \RETURN $\hat \grad f(K)$.
  \end{algorithmic}
\end{algorithm}


In this section, we assume the following in addition to Assumption \ref{assume:sigma-hurwitz}.
\begin{assumption}\label{assume:gradest}
  \indent
  \begin{itemize}
    \item ${U}_i$ and $x_i(0)$ are independent.
    \item $K\in S(a)$ with a given constant $a$.
    \item The distribution $\mathcal D$ of initial state $x(0) \sim \mathcal{D}$ satisfies $\norm{x(0)}\leq P\quad \text{a.s.}$ for a constant $P > 0$.
  \end{itemize}
\end{assumption}
The second assumption is justified in Section~\ref{sec:mfgp}.


The following lemma ensures $K+rU \in S(2a)$ for sufficiently small $r$.
\begin{lemma}
  \label{lem:smallr}
  There exists $r_0  > 0$ such that
  for any $r \leq r_0$, $K\in S(a)$ and $U$ such that $\norm{U} = \sqrt{mp}$, we have $K+rU\in S(2a)$.
\end{lemma}
\begin{proof}
Using the same argument as Lemma 4 in~\cite{mohammadi2021convergence}, we obtain the result. \qed
\end{proof}

The following theorem is an extension of Lemma 27 in Supplementary material of~\cite{fazel2018global} 
to output feedback control.
To prove this, we derive some inequalities in Appendix~\ref{sec:proof-theorem2}.

\begin{theorem}\label{thm:totalerror}
Let $\hat{\nabla} f(K)$ be defined as \eqref{eq:hatgrad}.
  For any $\varepsilon' > 0$ and $\delta > 0$, set $r = O(\varepsilon'), \tau = O(\log 1/\varepsilon'), N = O((\log 1/\delta)/\varepsilon'^4)$. Then, we have
  \begin{align}
    &\normF{\hat \grad f(K)-\grad f(K)} \leq \varepsilon' \label{eq:high-probability},
  \end{align}
  for any $K\in S(a)$,
  with probability greater than $1-\delta$.
\end{theorem}
\begin{proof}
  See Appendix~\ref{sec:proof-theorem2}. \qed
\end{proof}

\subsection{Convergence properties}\label{sec:mfgp}
In this section, we prove the global convergence of the policy gradient method in the model free setting.
We show a model free control algorithm, Policy Gradient Projection, in Algorithm~\ref{alg:mfgp}.
The positive integer $T$ is the iteration number, $\alpha$ is the step size, $K_0$ is the initial point of the feedback gain $K$,
and $\proj$ is the projection onto $\Omega$ with respect to the Frobenius norm.
The termination condition $\normF{K_{i+1}-K_i}\leq \varepsilon\alpha$ is added for technical reasons.
\begin{algorithm}
  \caption{Policy Gradient Projection}
  \label{alg:mfgp}
  \begin{algorithmic}[1]
    \REQUIRE{$T > 0, \alpha > 0, \varepsilon > 0, K_0\in \Omega, N > 0, r>0, \tau>0$}
    \FOR{$i=0$ to $T-1$}
    \STATE{Calculate $\hat \grad f(K_i)$ using Algorithm \ref{alg:gradest1}.}
    \STATE{$K_{i+1} := \proj\qty(K_i-\alpha \hat \grad f(K_{i}))$.}
    \IF{$\normF{K_{i+1}-K_i}\leq \varepsilon\alpha$}
    \RETURN $K_i$
    \ENDIF
    \ENDFOR
    \RETURN $K_T$
  \end{algorithmic}
\end{algorithm}

Lemma~\ref{lem:projection} describes the property of orthogonal projections,
which is essential to our convergence analysis.
\begin{lemma}
  \label{lem:projection}
  Let $\proj:\R^{m\times p}\to \Omega$ be an orthogonal projection onto $\Omega$.
  For any $x\in \Omega$ and $y\in \R^{m\times p}$, we have
%  \begin{align}
  $\q<x-\proj(y), y-\proj(y)>  \leq 0$.
%  \end{align}
\end{lemma}
\begin{proof}
  See Theorem 6.41 of \cite{beck2017first}. \qed
\end{proof}

The following definition is required to show our main result.
\begin{definition}
  \label{def:epsst}
  For positive constants $\alpha$ and $\varepsilon$,
  $K$ is called an $\varepsilon$-stationary point if $\normF{G_\alpha(K)} \leq \varepsilon$, where $K^+ := \proj(K-\alpha\grad f(K))$ and $G_\alpha(K) := \frac{1}{\alpha}(K^+-K)$.
\end{definition}

The following theorem is a main result, which is an extension of Theorem 4.2 in~\cite{fatkhullin2021optimizing} to the model-free and constrained problems. 
The proof is based on the proof for projected gradient method without gradient error for $L$-smooth functions~\cite{beck2017first}. However, in the presence of gradient errors, the termination condition and some extra arguments to bound the effect of the difference between the true and estimated gradients are required.

\begin{theorem}\label{thm:modelfreeconvergence}
  Assume that the constants $N, r, \tau$ satisfies the condition in Theorem~\ref{thm:totalerror} with $\varepsilon' = \lambda \varepsilon$
  for the given constants $0<\lambda<1, \varepsilon > 0, \delta > 0$.
  Let $\{K_i\}_{i=0}^{T'}$ be the sequence generated by the Algorithm~\ref{alg:mfgp},
  where $T'$ is the total number of iterations, which can be different from $T$ due to the terminate condition.
  For a step size $\alpha \in \qty(0, \frac{2(1-\lambda)}{L})$,
  where $L$ is the Lipschitz constant of $\grad f$ on $S_0$,
  we have the following result with probability greater than $1-T\delta$.
  \begin{itemize}
    \item The sequence $\{K_i\}_{i=0}^{T'}$ remains in $S$ and $\{f(K_i)\}_{i=0}^{T'}$ is strictly decreasing. That is, for any $0\leq i\leq T'-1$,
          \begin{align}
            f(K_{i+1}) < f(K_i).\label{eq:tmp4}
          \end{align}
    \item If $T > \frac{f(K_0)}{\varepsilon^2\alpha^2\qty(\frac{1-\lambda}{\alpha}-\frac{L}{2})}$,
          $K_{T'}$ is a $(1+\lambda)\varepsilon$-stationary point.
  \end{itemize}
\end{theorem}
\begin{proof}
  We define $G'_\alpha(K) = \frac{1}{\alpha}{v}$, where $v:=\hat{K}^+-K$ and $\hat{K}^+ := \proj(K-\alpha\hat \grad f(K))$.
  First, we show that if $\normF{G'_\alpha(K)} \leq \varepsilon$ and \eqref{eq:high-probability} holds,
  $K \in S_0$ is a $(1+\lambda)\varepsilon$-stationary point.
  \begin{align}
    \normF{G_\alpha(K)} & \leq \normF{G'_\alpha(K)}+\frac{1}{\alpha}\normF{K^+-\hat{K}^+} \\
                & \leq \varepsilon+\normF{\grad f(K)-\hat\grad f(K)}                \leq (1+\lambda)\varepsilon.
  \end{align}
  The second inequality holds, because projections onto convex sets are contractive,
  and the last inequality follows from~\eqref{eq:high-probability}.

  Next, we show $\normF{G'_\alpha(K_{T'})} \leq \varepsilon$ with high probability.
  The termination condition ensures $\normF{G'_\alpha(K_i)} > \varepsilon$ for $i = 0, \dots, T'-1$.
  Assume that $\normF{G'_\alpha(K)} > \varepsilon$ and Eq.~\eqref{eq:high-probability} with $\varepsilon' = \lambda \varepsilon$ holds for $K\in S_0$.
    We define $K_t:=K+tv$ and $t^* := \max\qty{t > 0 \mid f(K_{t'}) \leq f(K_0)\quad (0\leq \forall t' \leq t)}$.
  Then, Lemma~\ref{lem:projection} yields
  \begin{align}
    \q<\hat \grad f(K), v> & \leq -\frac{1}{\alpha}\normF{v}^2\label{eq:mf-proj}
  \end{align}
  and $L$-smoothness of $f(K)$ on $S_0$ implies
  \begin{align}
    f(K_t)-f(K) & \leq \q<\grad f(K), K_t-K>+\frac{L}{2}\normF{K_t-K}^2\\
    &=t\q<\grad f(K), v>+\frac{Lt^2}{2}\normF{v}^2.\label{eq:mf-L-smooth}
  \end{align}
  
  By adding both sides of the equations \eqref{eq:mf-proj} multiplied by $t$ and \eqref{eq:mf-L-smooth}, we obtain
  \begin{align}
    f(K_t)&-f(K)% \\
%    & \leq  \qty(\frac{Lt^2}{2}-\frac{t}{\alpha}+t\frac{\normF{\grad f(K)-\hat \grad f(K)}}{\normF{v}})\normF{v}^2 \\
                %& <
 <\qty(\frac{Lt^2}{2}-t\frac{1-\lambda}{\alpha})\normF{v}^2.\label{eq:mf-sufficient-decrease}
  \end{align}
  where we used
%  The last inequality follows from~
\eqref{eq:high-probability} and $G'_\alpha(K)=\normF{v}/\alpha > \varepsilon$.
  For $t = t^*$, we have
  \begin{align}
      0 &= f(K_{t^*})-f(K)\leq \qty(\frac{L{t^*}^2}{2}-{t^*}\frac{1-\lambda}{\alpha})\normF{v}^2
  \end{align}
  Since $\norm{v} > 0$, we have $t^* \geq \frac{2(1-\lambda)}{L\alpha}\geq 0$.
  Therefore,~\eqref{eq:mf-sufficient-decrease} holds for $t = 1$.
  Eq.~\eqref{eq:mf-sufficient-decrease} with $K=K_i$ and $t=1$ leads us to
  \begin{align}
    f(K_{i+1})-f(K_i) & < \qty(\frac{L}{2}-\frac{1-\lambda}{\alpha})\normF{K_{i+1}-K_i}^2 <0,
  \end{align}
  because $\alpha \in (0, \frac{2(1-\lambda)}{L})$.
  Thus,~\eqref{eq:tmp4} holds for any $0\leq i\leq T'-1$.

  If $T' < T$, the termination condition ensures $\normF{G'_\alpha(K_{T'})} \leq \varepsilon$.
  Therefore, it suffices to show $T' < T$.
  Since $T' \leq T$ by definition, suppose that $T' = T$.
  Then, we obtain
  \begin{align}
    f(K_0)  \geq f(K_0)-f(K_{T})%=\sum_{i = 0}^{T-1} f(K_i)-f(K_{i+1})   \\
          % & \geq\qty(\frac{1-\lambda}{\alpha}-\frac{L}{2}) \sum_{i = 0}^{T-1} \normF{K_{i+1}-K_i}^2  \\
          \geq \qty(\frac{1-\lambda}{\alpha}-\frac{L}{2})T\varepsilon^2\alpha^2.
  \end{align}
  The assumption $T > \frac{f(K_0)}{\varepsilon^2\alpha^2\qty(\frac{1-\lambda}{\alpha}-\frac{L}{2})}$
  yields $f(K_0)>f(K_0)$, which is a contradiction. Thus, $T'<T$, that is, $\normF{G'_\alpha(K_{T'})}\leq \varepsilon$.
  From Theorem~\ref{thm:totalerror}, the probability that Eq. \eqref{eq:high-probability} holds for $K = K_i(i=0, \dots, T')$ is greater than
  $(1-\delta)^{T'+1}\geq 1-T\delta$.
  This completes the proof.\qed
\end{proof}


The convergence rate $T = O(1/\varepsilon^2)$ is essentially the same as the rate of projected gradient method without gradient error for $L$-smooth functions~\cite{beck2017first}. For sample complexity, the total number of samples $TN = O(\log(1/\delta)/\varepsilon^6)$ is worse than $O(\log(1/\delta)/\varepsilon^4)$ of the zeroth-order proximal gradient descent with two points evaluation in~\cite{ghadimi_mini-batch_2016} since we cannot evaluate the two cost function values with two different feedback gains for the same initial state due to the randomness of the initial state. Note that the same rate to ours was obtained for discrete-time state feedback LQR problems in the model free setting~\cite{hambly2021policy} but not known for the model free and output feedback setting.

\subsection{Variance reduction}
Policy gradient methods tend to suffer from a large variance, which leads to slow learning~\cite{grondman2012survey}.
The use of baseline is one of the variance reduction techniques for policy gradient methods~\cite{grondman2012survey}.
State-depending functions are often used as a baseline, because it does not add any bias to the estimated gradient~\cite{degris2012model-free, mnih2016asynchronous}.
In this section, we propose to use the finite horizon cost function as a baseline and show its optimality.

For a baseline function $b(x)$, the estimated gradient $\bar \grad f$ is defined as
\begin{align}
    \bar \grad f(K)  &:= \frac{1}{rN}\sum_{i=1}^N (\tilde f_\tau(K+rU_i;x_i(0))-b(x_i(0)))U_i,\label{eq:grad-baseline-b}
\end{align}
where the finite horizon cost function is defined as
\begin{align}
    \tilde f_\tau(K;x(0)) := \int_0^\tau y^\top(t) ( Q + K^\top RK)y(t)dt\label{eq:ftau},
\end{align}
which satisfies $\lim_{\tau \to \infty} \tilde f_\tau(K;x(0)) = \tilde f(K;x(0))$.
Because 
\begin{align}
    \hat \grad f(K) = \frac{1}{rN}\sum_{i=1}^N \tilde f_{\tau}(K+r U_i;x_i(0))U_i,
\end{align}
the estimated gradient $\bar \grad f(K)$ satisfies
\begin{align}
  E\qty[\bar \grad f(K)] & = E_{x_i(0)\sim \mathcal{D}, U_i \sim \mathcal{S}}\qty[\hat \grad f(K)] \\
  & \quad - \frac{1}{rN}\sum_{i=1}^N E_{x_i(0)\sim \mathcal{D}, U_i \sim \mathcal{S}}\qty[b(x_i(0))U_i] \\
                   %      & = E_{x_i(0)\sim \mathcal{D}, U_i\sim \mathcal{S}}\qty[\hat \grad f(K)] \\
   % &\quad - \frac{1}{rN}\sum_{i=1}^N E_{x_i(0)\sim \mathcal{D}, U_i\sim \mathcal{S}}\qty[b(x_i(0))]E[U_i] \\
                         & = E_{x_i(0)\sim \mathcal{D}, U_i\sim \mathcal{S}}\qty[\hat \grad f(K)].
\end{align}
The second equality holds from the assumption that $U_i$ and $x_i(0)$ are independent, and
% the last equality follows from
$E[U_i] = 0$.
Thus, the bias in $\bar \grad f$ is the same as the one in $\hat \grad f$.

In terms of the variance, the baseline $b(x(0)) = \tilde f_\tau(K;x(0))$ is almost optimal for small $r$.
\begin{theorem}\label{thm:baseline}
  For $a > 0$, $r \leq r_0$, $\tau > 0$, and $K\in S(a)$,
  the optimal baseline $b^*(x(0))$ which minimizes the variance of the estimated gradient~\eqref{eq:grad-baseline-b} is given by
  \begin{align}
    b^*(x(0)) = E_{U\sim \mathcal{S}}\qty[\tilde f_\tau(K+rU;x(0))],\label{eq:b-star}
  \end{align}
  where $\mathcal{S}$ is defined in Algorithm~\ref{alg:gradest1}
  and
  \begin{align}
    \lim_{r\to 0}b^*(x(0)) = \tilde f_\tau(K;x(0)).\label{eq:limit}
  \end{align}
\end{theorem}
\begin{proof}
  % Let $b(x)$ be any function of the initial state $x$.
  % Using $b(x)$ as a baseline, the estimated gradient is given by
    % $\bar \grad f(K) := \frac{1}{rN}\sum_{i=1}^N \qty(\tilde % f_\tau(K+rU_i;x_i(0))-b(x_i(0)))U_i$.
  Since $\qty{x_i(0)}_{i=1}^N, \qty{U_i}_{i=1}^N$ are assumed to be independent, $\qty{f_\tau(K;x_i(0))-b(x_i(0)))U_i}_{i=1}^N$ are independent and we have
  \begin{align}
    &V\qty[\bar \grad f(K)] \label{eq:baseline-variance}\\
                           &= \frac{1}{r^2N}\left[E_{x(0)\sim \mathcal{D}, U\sim \mathcal{S} }\qty[\normF{(\tilde f_\tau(K+rU;x(0))-b(x(0)))U}^2]\right.        \\
                           & \quad\quad-\left.\normF{E_{x(0)\sim \mathcal{D}, U\sim \mathcal{S} }[(\tilde f_\tau(K+rU;x(0))-b(x(0)))U]}^2\right]\nonumber
  \end{align}
  for any baseline $b(x)$.
  Because we have assumed that $x(0)$ and $U$ are independent and $E_{U\sim \mathcal{S}}[U] = 0$,
  \begin{align}
      E_{x(0)\sim \mathcal{D}, U\sim \mathcal{S}}\qty[b(x(0))U] = E_{x(0) \sim \mathcal{D}}\qty[b(x(0))]E_{U\sim \mathcal{S}}[U] = 0.
  \end{align}
  Thus, the expectation $E[(\tilde{f}_\tau(K+rU;x(0))-b(x(0)))U]$ is independent of the choice of a baseline $b(x)$,
  that is,
  \begin{align}
& E_{x(0)\sim \mathcal{D}, U \sim \mathcal{S}}\qty[(\tilde f_\tau(K+rU;x(0))-b(x(0)))U] \\ =& E_{x(0)\sim \mathcal{D}, U \sim \mathcal{S}}\qty[\tilde f_\tau(K+rU;x(0))U].
  \end{align}
  Then, the second term in \eqref{eq:baseline-variance} is independent of the choice of $b(x)$ and we just need to minimize the first term in \eqref{eq:baseline-variance},
  \begin{align}
    & E_{x(0) \sim \mathcal{D}, U\sim \mathcal{S}}\qty[\norm{\qty(\tilde f_\tau(K+rU;x(0))-b(x(0)))U}^2_F] \\
     =& mp E_{x(0)\sim \mathcal{D}}\qty[E_{U\sim S}\qty[\qty(\tilde f_\tau(K+rU;x(0))-b(x(0)))^2]].
  \end{align}
  Since the expectation $E_{U\sim \mathcal{S}}\qty[\tilde f_\tau(K+rU;x(0))]$ minimizes the mean squared error $E_{U\sim S}\qty[\qty(\tilde f_\tau(K+rU;x(0))-b(x(0)))^2]$ for any $x(0)$, 
  the optimal baseline is given by \eqref{eq:b-star}.
  Eq.~\eqref{eq:limit} follows from the continuity of $\tilde f_\tau(K+rU;x(0))$.\qed
\end{proof}

Based on Theorem~\ref{thm:baseline}, we propose to use $\tilde f_\tau(K;x(0))$ as a baseline.
In the model free setting, $\tilde f_\tau(K;x(0))$ cannot be computed directly for a given $x(0)$ in the same manner as $c_i$ because we cannot specify the initial state in the estimation procedure of $c_i$.
Therefore, we provide the estimation procedure for $\tilde f_\tau(K;x(0))$ in Algorithm~\ref{alg:estimate-f}. In the following, we define $\bar{y}(t;x(0)) = [y(t)^\top, y(t+h_1)^\top, \dots, y(t+h_{D-1})^\top]^\top \in \R^{pD}(0\leq h_1<h_2<\dots<h_{D-1}=T)$.
\begin{algorithm}
  \caption{Estimate $\tilde f_{\tau}(K;x(0))$}
  \label{alg:estimate-f}
  \begin{algorithmic}[1]
    \REQUIRE{$K\in S, x(0) \in \R^n, s > 0$.}
    \FOR{$i=1$ to $n(n+1)/2$}
    \STATE{Simulate system~\eqref{eq:system} for $x(0) = x_i(0) \sim \mathcal D$ until time $s$.}
    \ENDFOR
    \STATE{Solve the following equations for $\hat P(X)$.
    \begin{align} 
        & \bar y(0;x_i(0))^\top \hat P(K)\bar y(0;x_i(0)) \\
        & = \tilde f_s(K;x_i(0)) + \bar y(s;x_i(0))^\top \hat P(K)\bar y(s;x_i(0)),\label{eq:bellmaneq}
    \end{align}}
    where $\tilde f_s$ is defined as \eqref{eq:ftau}.
    \STATE{Define $\hat f_\tau(K;x(0)) = \bar y(0;x(0))^\top \hat P(K)\bar y(0;x(0)) - \bar y(\tau;x(0))^\top \hat P(K)\bar y(\tau;x(0))$.}
    \RETURN $\hat f_\tau(K;x(0))$.
  \end{algorithmic}
\end{algorithm}
The following theorem ensures that the estimated cost $\hat f_\tau(K;x(0))$ is equal to $\tilde f_\tau(K;x(0))$.
\begin{theorem}\label{thm:estimate-f}
  For any $T > 0$, $D > 2(n-1) + \frac{T}{2\pi} \beta$, where $\beta = 2(\norm{A}_2 + \norm{B}_2\norm{C}_2\kappa(a))$,
  if $x_j(0)x_j(0)^\top - x_j(s)x_j(s)^\top(j=1, \dots, \frac{n(n+1)}{2})$
  are linearly independent on $\mathbb{S}^n$ for $s$ in Algorithm~\ref{alg:estimate-f}, then $\hat f_\tau(K;x(0)) = \tilde f_\tau(K;x(0))$ for any $K \in S$ and $x(0)$.
\end{theorem}
\begin{proof}
  See appendix~\ref{proof:thm:estimate-f}.
\end{proof}

The following theorem shows that
the assumption of Theorem~\ref{thm:estimate-f} holds with probability 1.

\begin{theorem}
If the distribution $\mathcal {D}$ has a probability density function, $x_j(0)x_j(0)^\top - x_j(s)x_j(s)^\top(j=1, \dots, \frac{n(n+1)}{2})$ are linearly independent on $\mathbb{S}^n$ with probability 1.
\end{theorem}
\begin{proof}
Let $v_i = x_i(0)x_i(0)^\top - x_i(s)x_i(s)^\top$ and $V_m$ be a linear subspace generated by $\qty{v_i}_{i=1}^m$ ($1 \leq m \leq \frac{n(n+1)}{2}$).
For $m < \frac{n(n+1)}{2}$, $V_m$ is a proper subspace of $\mathbb{S}^n$ and there exists $\tilde v_m \neq O \in \mathbb{S}^n$ orthogonal to $V_m$.
Since $x_i(s) = e^{A_Ks}x_i(0)$, we have
\begin{align}
    &\langle v_{m+1}, \tilde v_m\rangle\\
    &= \langle x_{m+1}(0)x_{m+1}(0)^\top - e^{A_Ks}x_{m+1}(0)x_{m+1}(0)^\top e^{A_K^\top s}, \tilde v_m\rangle\\
    %&= x_{m+1}(0)^\top \tilde v_m x_{m+1}(0) - x_{m+1}(0)^\top e^{A_K^\top s}\tilde v_m e^{A_Ks}x_{m+1}(0)\\
    &= x_{m+1}(0)^\top \tilde v'_m x_{m+1}(0),
\end{align}
    where $\tilde v'_m := \tilde v_{m} - e^{A^\top_Ks}\tilde v_{m}e^{A_Ks}$.
    Note $\tilde v'_m \neq O$, because $A_K$ is \textit{Hurwitz} and the solution to the discrete Lyapunov equation $v - e^{A^\top_Ks} v e^{A_Ks} = 0$ is only $v = O$. Since $\tilde v'_m \in \mathbb{S}^n$, there exists an orthogonal matrix $U$ such that $U^\top \tilde v'_m U = \operatorname{diag}(\mu_1, \dots, \mu_n)$, where $\qty{\mu_i}_{i=1}^n$ are eigenvalues of $\tilde v'_m$. Without loss of generality, we assume $\mu_n \neq 0$ since $\tilde v'_m \neq O$. For $x_{m+1}(0) \sim \mathcal {D}$, let $z = Ux_{m+1}(0)$. Then, $x_{m+1}(0)^\top \tilde v'_m x_{m+1}(0) = \sum_{i=1}^n \mu_i z_i^2$. Since $\mathcal{D}$ has a probability density function, the distribution of $z$ has also a probability density function $g(z_1, \dots, z_n)$. For $z_1, \dots, z_{n-1}$ such that $\int_{z_n} g(z_1, \dots, z_{n})\mathrm{d}z_n > 0$, the conditional probability density function of $z_n$ is given by $\frac{g(z_1, \dots, z_n)}{\int_{z_n} g(z_1, \dots, z_{n})\mathrm{d}z_n}$ and the conditional probability that $z_n$ satisfies $\sum_{i=1}^n \mu_i z_i^2 = 0$ is zero, because there are at most two $z_n$ in $\R$ which satisfy $z_n^2 = \frac{\sum_{i=1}^{n-1} \mu_i z_i^2}{\mu_n}$. 
    Therefore, the probability that $\langle v_{m+1}, \tilde v_m\rangle = x_{m+1}(0)^\top \tilde v'_m x_{m+1}(0) = 0$ is zero.
    %Since $v_{m+1} \in V_m$ means $\langle v_{m+1}, \tilde v_m\rangle = 0$,
    That is,
    $v_{m+1} \notin V_m$ with probability 1. 
    By induction, we obtain the result. \qed
\end{proof}
% \end{document}