\subsection{Proof of Lemma 1}\label{sec:proof-lemma1}
\begin{proof}
  For $X$, see Lemma 16 in~\cite{mohammadi2021convergence}.

  Let $\mu$ be the largest eigenvalue of $Y$ and
  $v$ be a normalized eigenvector corresponding to $\mu$.
  Note that $\norm{Y}_2 = \mu$ since $Y\succeq 0$.
  In the following, we consider the case $\norm{C v}\geq \xi$ and the case $\norm{C v}\leq \xi$ separately.
  First, we consider the case $\norm{Cv}\geq \xi$.
  Using $f(K) = \tr(YC^\top(Q+K^\top RK)C)$, we have
%  \begin{align}
    $a  \geq \tr(YC^\top(Q+K^\top RK)C)  \geq \tr(YC^\top Q C)$.
 % \end{align}
  Since $\mu vv^\top \preceq Y$,
%  \begin{align}
    $\tr(YC^\top Q C)  \geq \tr(\mu vv^\top C^\top Q C) = \mu(Cv)^\top Q Cv$.
  %\end{align}
  Therefore,
    $a  \geq \mu(Cv)^\top Q Cv
       \geq \mu\norm{Cv}^2 \lmin(Q) 
       \geq \mu\xi^2 \lmin(Q)$ and
    $\mu \leq \frac{a}{\xi^2\lmin(Q)}$.
  Next, we consider the case $\norm{Cy}\leq \xi$.
  From \eqref{eq:lyapunovY}, we have
  \begin{align}
    v^\top \qty(A_K Y + Y A_K^\top + \Sigma) v & = \mu v^\top (A_K+A_K^\top)v+v^\top \Sigma v = 0.
  \end{align}
  Since $A_{K_0}+A_{K_0}^\top \leq -2\sigma I$ and $\norm{Cv}\leq \xi$, we obtain
    $v^\top (A_K+A_K^\top)v = v^\top (A_{K_0}+A_{K_0}^\top)v - v^\top (B(K-K_0)C+(B(K-K_0)C)^\top)v \leq  -\sigma$.
  Therefore,
    $-\mu\sigma+v^\top \Sigma v  \geq 0$,
  and thus,
    $\norm{Y}_2 = \mu \leq \frac{\norm{\Sigma}_2}{\sigma}$.
  Combining both cases, we have $\norm{Y}_2 \leq \mathfrak{Y}(a)$.

  For $Y'$, we have
  \begin{align}
       -\frac{\norm{BECY+(BECY)^\top}_2}{\lmin(\Sigma)}\Sigma
       & \preceq -BECY-(BECY)^\top \\
       & \preceq \frac{\norm{BECY+(BECY)^\top}_2}{\lmin(\Sigma)}\Sigma,
  \end{align}
  and Lemma A.2 in~\cite{fatkhullin2021optimizing} yields
  $
    Y^- \preceq Y' \preceq Y^+,
  $
  where 
  \begin{align}
      Y^+ = \frac{\norm{BECY+(BECY)^\top}_2}{\lmin(\Sigma)}Y,\quad Y^- =-Y^+.
  \end{align}
  Therefore,
  \begin{align}
    \norm{Y'}_2  \leq \norm{Y^+}_2 
                 \leq \frac{2\norm{B}_2\norm{C}_2\norm{Y}_2^2}{\lmin(\Sigma)}.\label{eq:norm-Y-prime-mid}
  \end{align}
  Applying the bound on $\norm{Y}_2$ to \eqref{eq:norm-Y-prime-mid}, we have the result.
\end{proof}
\subsection{Proof of Theorem 2}\label{sec:proof-theorem2}
The total error $\normF{\hat \grad f(K) - \grad f(K)}$ can be divided into the bias term $\normF{E\qty[\hat \grad f(K)] - \grad f(K)}$ and the variance term $\normF{\hat \grad f(K) - E\qty[\hat \grad f(K)]}$.

First, we bound the bias term.
The estimated gradient $\hat \grad f(K)$ in~\eqref{eq:hatgrad} can be expressed in the form
\begin{align}
  \hat \grad f(K) & = \frac{1}{Nr}\sum_{i=1}^N \tilde f_\tau(K+rU_i;x_i(0))U_i.\label{eq:hatgrad-ftau}
\end{align}

For any initial state $x(0)$ and $r > 0$, we define 
the smooth function $g_r(K)$ by
$
  g_r(K) := E_{U\sim \mathcal{B}}\qty[f(K+rU)],
$
where $\mathcal{B}$ is the uniform distribution over the set $\{U\in \R^{m\times p}\mid \norm{U}\leq \sqrt{mp}\}$.
Then, the bias in $\hat \grad f(K)$ can be divided into two parts as follows.
\begin{align}
  &\normF{\grad f(K)-E\qty[\hat\grad f(K)]}\leq \normF{\grad f(K)-\grad g_r(K)} \\
  &+ \normF{\grad g_r(K)-E\qty[\hat\grad f(K)]}, \label{eq:bias}
\end{align}
where the expectation is taken over $x_i(0)\sim \mathcal D$ and $U_i \sim \mathcal S$.

For the first term in \eqref{eq:bias}, we have the following bound.
\begin{lemma}\label{lem:biasr}
  For any $K\in S(a)$ and $r \leq r_0$,
  $
    \normF{\grad f(K)-\grad g_r(K)} \leq Lr\sqrt{mp},
  $
  where $L$ is the Lipschitz constant defined as~\eqref{eq:L} of $\grad f$ on $S(2a)$.
\end{lemma}
\begin{proof}
  From $L$-smoothness of $f$, we have $\normF{\grad f(K) - \grad f(K+rU)}\leq Lr\sqrt{mp}$. Therefore, $\normF{\grad f(K)-\grad g_r(K)} = E\qty[\normF{\grad f(K)-\grad f(K+rU)}]\leq Lr\sqrt{mp}$. \qed
\end{proof}

Lemma 26 in Supplementary material of \cite{fazel2018global} implies
  $\grad g_r(K)  = E\qty[\frac{1}{r}\tilde f(K+rU;x_0)U]$,
and \eqref{eq:hatgrad-ftau} yields
 $E\qty[\hat \grad f(K)]  = E\qty[\frac{1}{r} \tilde f_\tau(K+rU;x_0)U]$,
where the expectation is taken over $x_0\sim \mathcal D$ and $U\sim \mathcal S$.
By using these relations, we have the following upper bound of the second term in~\eqref{eq:bias}.
\begin{align}
         &\normF{\grad g_r(K)- E\qty[\hat\grad f(K)]}\\
         &\leq \frac{1}{rN}\sum_{i=1}^N E\qty[\abs{\tilde f(K+rU_i;x_i)-\tilde f_\tau(K+rU_i;x_i)}\normF{U_i}].\label{eq:biastau}
\end{align}
To bound the right-hand side, we introduce the following lemma.
\begin{lemma}\label{lem:norm-x-t}
  For $K\in S(a)$ with $a\in \R$ and $x(t)$, which follows \eqref{eq:closedloop}, we have
  \begin{align}
    \norm{x(t)} & \leq \frac{2\mathfrak{Y}(a)\mathfrak{A}(a)}{\lmin(\Sigma)}e^{-(\lmin(\Sigma)/\mathfrak{Y}(a))t}\norm{x(0)}^2,\label{eq:norm-x-t}
  \end{align}
  where 
  $
    \mathfrak{A}(a) = \norm{A}_2+\norm{B}_2\norm{C}_2\kappa(a).
  $
\end{lemma}
\begin{proof}
  From Lemma 12 in \cite{mohammadi2021convergence}, we have
  $
    \norm{e^{At}}_2^2 \leq \qty(\norm{Y}_2/\lmin(Y))e^{-(\lmin(\Sigma)/\norm{Y}_2)t}.
  $
  Therefore,
  \begin{align}
    \norm{x(t)}^2 %& \leq %\norm{e^{At}}_2^2\norm{x(0)}^2  \\
                   \leq \qty(\norm{Y}_2/\lmin(Y))e^{-(\lmin(\Sigma)/\norm{Y}_2)t}\norm{x(0)}^2.\label{eq:lem12}
  \end{align}
  Lemma \ref{lem:xybound} yields $\norm{Y}_2  \leq \mathfrak{Y}(a)$.
  This and Lemma A.5 in~\cite{fatkhullin2021optimizing} imply
  \begin{align}
    \lmin(Y) &\geq \frac{\lmin(\Sigma)}{2\norm{A_K}_2} 
    \geq \frac{\lmin(\Sigma)}{2(\norm{A}_2+\norm{B}_2\norm{C}_2\kappa(a))}.
  \end{align}
  Substituting these inequalities into \eqref{eq:lem12}, we obtain \eqref{eq:norm-x-t}.\qed
\end{proof}

We are in a position to obtain an upper bound on the left side of~\eqref{eq:biastau}.
\begin{lemma}
  \label{lem:biastau}
  For any $\tau \geq 0$, $r \leq r_0$ and $K\in S(a)$,
  \begin{align}
    \normF{\grad g_r(K)-E\qty[\hat\grad f(K)]} =
    O(e^{-\eta \tau}/r),\label{eq:biastau2}
  \end{align}
  where $\eta = \lmin(\Sigma)/\mathfrak{Y}(2a)$.
\end{lemma}
\begin{proof}
  Lemma~\ref{lem:smallr} implies that $K+rU_i \in S(2a)$, and we have
  \begin{align}
    \normF{\grad g_r(K)-E\qty[\hat\grad f(K)]}  &\leq  \frac{1}{r} E\qty[x(\tau)^\top Xx(\tau)\normF{U}]   \\
    &\leq \frac{\sqrt{mp}}{r} \norm{X}_2E\qty[\norm{x(\tau)}^2],
  \end{align}
  where
  $x(t)$ follows
    $\dot x(t)  = A_{K+rU} x(t)$, and
  we used the fact
  \begin{align}
    \tilde f(K+rU;x(0))-\tilde f_\tau(K+rU;x(0)) & = x(\tau)^\top Xx(\tau),
  \end{align}
  where $X$ is the solution to~\eqref{eq:lyapunovX}.
  From Lemma \ref{lem:norm-x-t}, we have
  \begin{align}
    \norm{x(\tau)}^2 & \leq \frac{2\mathfrak{Y}(2a)\mathfrak{A}(2a)}{\lmin(\Sigma)}e^{-(\lmin(\Sigma)/\mathfrak{Y}(2a))\tau}\norm{x(0)}^2.
  \end{align}
  Thus, \eqref{eq:biastau2} holds,
  because $E[\norm{x(0)}^2]=\tr(\Sigma)$.\qed
\end{proof}

Next, we obtain an upper bound of the variance term.
\begin{lemma}
  \label{lem:variance}
  For any $\varepsilon > 0, \delta > 0$ and $K\in S(a)$, if $N = O((\log 1/\delta)/\varepsilon^4)$, we have
  \begin{align}
    \Probability\qty(\normF{\hat \grad f(K)-E\qty[\hat \grad f(K)]}\geq \varepsilon)\leq \delta
  \end{align}
\end{lemma}
\begin{proof}
    Using matrix Bernstein inequality~\cite{tropp2012user}, we obtain the result in the same way with Lemma 27 in Supplementary material of~\cite{fazel2018global}. \qed
\end{proof}

Combining Lemma~\ref{lem:biasr},~\ref{lem:biastau}, and~\ref{lem:variance} completes the proof of Theorem~\ref{thm:totalerror}.

\subsection{Proof of Theorem~\ref{thm:estimate-f}}\label{proof:thm:estimate-f}
For any $t \geq 0$, the observation $\bar y(t;x(0))$ is determined by
%\begin{align}
    $\bar y(t;x(0)) = Fx(t)$
%\end{align}
with $F := [C \quad Ce^{(A-BKC) h_1} \cdots Ce^{(A-BKC) h_{D-1}}]^\top$. 
Conversely, $x(t)$ is determined by $\bar y(t;x(0))$ if $D$ is large enough.
\begin{lemma}\label{lem:reconstruction}
  Let $\beta = 2(\norm{A}_2+\norm{B}_2\norm{C}_2\kappa(a))$.
  For any $T>0$, if $D>2(n-1)+\frac{T}{2\pi}\beta$, $F$ is column full rank and
%  \begin{align}
    $x(t)  = F^+\bar y(t;x(0))$
    %\label{eq:x-from-y-bar}
  %\end{align}
  with $F^+ = (F^\top F)^{-1}F^\top$. 
\end{lemma}
\begin{proof}
  Let $\beta' = \max_{i, j}(\Im(\lambda_i(A_K)-\lambda_j(A_K)))$.
   Theorem 2 in \cite{modares2016optimal}
   and the assumption that $(A, C)$ is observable
   imply that if $D>2(n-1)+\frac{T}{2\pi}\beta'$,
   $F$ is column full rank.
  Therefore, $F^+$ is well-defined and
  \begin{align}
      (F^\top F)^{-1}F^\top\bar y(t;x(0)) &= (F^\top F)^{-1}F^\top F x(t) = x(t).
  \end{align}
  Thus, it is sufficient to show $\beta \geq \beta'$.
  We have
  \begin{align}
      \beta' \leq 2\max_{i}(|\lambda_i(A_K)|)\leq 2\norm{A_K}_2\leq \beta,
  \end{align}
  which completes the proof.\qed
\end{proof}

As a corollary, we can show $\tilde f(K;x(t))$ can be expressed as a quadratic form in terms of $\bar y(t;x(0))$.
\begin{corollary}\label{cor:f-from-y-bar}
  For any $x(0)$,
  \begin{align}
    \tilde f(K;x(t)) & = \bar y(t;x(0))^\top P(K)\bar y(t;x(0)),\label{eq:f-from-y-bar}
  \end{align}
  where $P(K) = (F^+)^\top X F^+$.
\end{corollary}

According to \cite{modares2016optimal}, the matrix $P(K)$ satisfies the Bellman equations \eqref{eq:bellmaneq}
for any $s \geq 0$ and $\qty{x_j(t)}_{j=1, \dots, \frac{n(n+1)}{2}}$, which follow system~\eqref{eq:system}.


\begin{lemma} \label{lem:f-from-y-bar}
  Assume that $x_j(0)x_j(0)^\top - x_j(s)x_j(s)^\top(j=1, \dots, \frac{n(n+1)}{2})$
  are linearly independent on $\mathbb{S}^n$.
  For any solution $\hat P(K)$ of~\eqref{eq:bellmaneq}, initial condition $x(0)$, and $t \geq 0$,
  we have
  \begin{align}
    \tilde f(K;x(t)) = \bar y(t;x(0))^\top \hat P(K)\bar y(t;x(0)).
  \end{align}
\end{lemma}
\begin{proof}
  Let $v_j = x_j(0)x_j(0)^\top - x_j(s)x_j(s)^\top$
  and $w_j = \bar{y}(0;x_j(0))\bar{y}(0;x_j(0))^\top - \bar{y}(s;x_j(0)) \bar{y}(s;x_j(0))^\top$.
  We define $V$ by the linear space generated by $\{v_j\}_j$ and $W$ by the linear space generated by $\{w_j\}_j$.
  Since $V \subset \mathbb{S}^n$ and $\dim V = \dim \mathbb{S}^n = \frac{n(n+1)}{2}$, we have $V = \mathbb{S}^n$.
  Thus, the set $\{v_i\}_i$ is a basis of $\mathbb{S}^n$.
  Let $v = x(t)x(t)^\top\in \mathbb{S}^n$ for $x(0)\in \R^n$.
  Then, there exists the sequence $\{\alpha_i\}_i$ such that
    $v  = \sum_{i=1}^{\frac{n(n+1)}{2}} \alpha_i v_i$.
  Define the linear map $\mathcal{F}:V\to W$ by
    $\mathcal{F}(v')  = Fv'F^\top$,
  where $v'\in V$.
  Note that $\mathcal{F}(v_j) = w_j$.
  From~\eqref{eq:bellmaneq}, we have
    $\q<\hat P(K), \mathcal F(v_j)> = \q<P(K), \mathcal F(v_j)>$, and thus
  %\begin{align}
    $\q<\hat P(K), \mathcal F(v)> =\q<P(K), \mathcal F(v)>$.
  %\end{align}
  %The first and last equalities hold because the inner product $\q<\cdot, \cdot>$
  %is linear with respect to the second argument.
  Then, Eq.~\eqref{eq:f-from-y-bar} yields
  \begin{align}
    \q<P(K), \mathcal F(v)> & = \bar y(t;x(0))^\top P(K)\bar y(t;x(0))  = \tilde f(K;x(t)).
  \end{align}
  Therefore,
  \begin{align}
    \bar y(t;x_0)^\top \hat P(K)\bar y(t;x_0) &= \q<\hat P(K), \mathcal F(v)>% =\q<P(K), \mathcal F(v)> \\
    = \tilde f(K;x(t)),
  \end{align}
  which completes the proof.\qed
\end{proof}


Since $\tilde f_\tau(K;x(0)) = \tilde f(K;x(0)) - \tilde f(K;x(\tau))$, Corollary \ref{cor:f-from-y-bar} and Lemma \ref{lem:f-from-y-bar} ensure that Theorem \ref{thm:estimate-f} holds.

