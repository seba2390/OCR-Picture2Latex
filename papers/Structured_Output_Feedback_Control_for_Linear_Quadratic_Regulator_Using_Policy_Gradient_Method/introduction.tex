% \documentclass[ex_article]{subfiles}
% \begin{document}
\section{Introduction}\label{sec:intro}
% The Linear Quadratic Regulator (LQR) is a well-studied framework in the optimal control theory.
% The purpose of the LQR problem is to determine the optimal feedback controller which minimizes a quadratic cost function for a linear time invariant system.
% The optimal feedback control is usually given by a static linear state feedback and the optimal feedback gain is obtained by solving the algebraic Riccati equation (ARE)~\cite{lewis2012optimal}.
% To solve AREs, iterative methods are proposed in~\cite{hewer1971iterative, kleinman1968iterative}.
% Recently, the global convergence of the gradient descent for state and output feedback LQR problems was shown in [4] using smoothness and Lipschitz continuity on the sublevel sets of the objective function.
% In addition, the projected gradient method to obtain a feedback gain with sparsity pattern for state feedback LQR problems is studied in~\cite{bu2019lqr} and~\cite{talebi2022policy} considered linearly constrained problem for state feedback LQR problems.
% These approaches to the optimal control design require the complete knowledge of the system.

% In real-world application, the full knowledge of the dynamics might not be available.

%The field of Reinforcement Learning (RL) has been developing rapidly
%with a variety of applications such as robotic manipulation~\cite{gu2017deep},
%game playing \cite{silver2017mastering}, and recommendation tasks~\cite{zhao2017deep}.
%Recently, the

Linear Quadratic Regulator (LQR), which is a well-studied framework in the optimal control theory, has been revisited from the Reinforcement Learning (RL) perspective.
% Some RL methods for LQR problems have been studied for state feedback control.
% For example, Q-learning for discrete-time LQR problems was proposed in~\cite{bradtke1993reinforcement}.
For policy gradient methods, the global linear convergence to the global optima was obtained in~\cite{fazel2018global, mohammadi2021convergence}.
To obtain structured policy, Structured Policy Iteration of state feedback gains for LQR problems with a regularization term was proposed in~\cite{park2020structured}
and the local linear convergence to a stationary point was provided.
In addition, the projected gradient method for model-free state feedback LQR problems with convex constraints were studied in~\cite{hambly2021policy}. For the model based setting, the projected gradient method was studied in~\cite{bu2019lqr} and~\cite{talebi2022policy} considered linearly constrained problem for state feedback LQR problems.

%Although the state feedback control has been studied from various perspectives,

However, it is difficult to observe the entire state.
That is, only some outputs are available in practice.
The static output feedback control is a practical approach to deal with such situations.
For model based control design, some iterative methods are found in~\cite{makila1987computational}
and recently, the global convergence of the gradient descent for output feedback LQR problems was shown in~\cite{fatkhullin2021optimizing} using smoothness and Lipschitz continuity on the sublevel sets of the LQR objective function.
A model free algorithm was also proposed in~\cite{zhu2015adaptive} based on integral RL.
However, policy gradient methods for static output feedback problems in the model free setting have not been well studied.


In this study, we consider a policy gradient method for the LQR problem with structured constraints
for the static output feedback control under the assumption that system parameters are unknown,
in contrast to many existing works~\cite{fazel2018global, mohammadi2021convergence, hambly2021policy}, which studied the policy gradient method for state feedback LQR problems and~\cite{fatkhullin2021optimizing}, which studied gradient methods in the model based setting.
The structured constraints are naturally introduced due to the system structure such as linear port-Hamiltonian systems~\cite{Jacob2012linear}.

{\it Our contribution:}
The main contributions of this paper are summarized as follows:
\begin{itemize}
    \item To solve the LQR problem with structured constraints in the model free setting, we propose a policy gradient projection algorithm with a gradient estimation procedure.
    \item We show the global convergence to $\varepsilon$-stationary points of our proposed algorithm using the LQR objective function properties such as bounded sublevel sets, $L$-smoothness on sublevel sets, and dependency on horizon time.
    In addition, we show that the feedback gain obtained by the proposed method asymptotically stabilizes the closed-loop system.
    We also provide the sample complexity of the gradient estimation procedure.
    \item We propose a variance reduction method using the baseline technique and show its suboptimality.
\end{itemize}


{\it Paper organization:}
In Section \ref{sec:problem}, we introduce the LQR problem for output feedback control with structured constraints.
In Section \ref{sec:properties}, we show some properties of the objective function on sublevel sets.
In Section \ref{sec:model-free}, we propose the gradient estimation method and the policy gradient projection algorithm in the model free setting.
We then show that the algorithm outputs an $\varepsilon$-stationary point with high probability.
In addition, we provide a variance reduction method and show its asymptotic optimality.
In Section \ref{sec:experiments}, we conduct some numerical experiments and show properties of our proposed method.
Section \ref{sec:conclusion} is devoted to conclusion.

{\it Notation:}
For a vector $v \in \C^n$, $v^\top$ and $v^*$ denote the transpose and conjugate transpose of $v$, respectively.
The symbol $I$ and $O$ denote the identity matrix and the zero matrix, respectively.
The symbol $\mathbb{S}^n$ denotes the set of $n\times n$ symmetric matrices.
For a matrix $A\in \R^{n\times m}$, $\normF{A}$ and $\norm{A}_2$ represent Frobenius and spectral norm of $A$, respectively,
$\lambda_i(A)$ denotes the $i$-th eigenvalue of $A$ indexed as $\Re(\lambda_1(A)) \leq \dots \leq \Re(\lambda_n(A))$,
and $\vectorize(A) \in \R^{nm}$ denotes the vectorized form of $A$.
For matrices $A, B \in \R^{n\times m}$, the inner product $\q<A, B>$ is defined as $\q<A, B> = \tr(AB^\top)$
and $A\circ B$ denotes the Hadamard product of $A$ and $B$.
Given a symmetric matrix $S \in \mathbb{S}^n$, $\lmin(S)(\lmax(S))$ denotes the minimum (maximum) eigenvalue of $S$.
Given a random variable $X$ which follows the distribution $\mathcal D$,
$E_{X \sim \mathcal{D}}[X]$ or just $E[X]$ denotes the expectation over $X \sim \mathcal D$
and $V[X]$ denotes the variance of $X$. For $z\in \C$, $\Re(z)$ ($\Im(z)$) denotes the real (imaginary) part of $z$.
% \end{document}