% \documentclass[main]{subfiles}
% \begin{document}
\section{Numerical Experiments}\label{sec:experiments}
In this section, we numerically demonstrate that the policy gradient projection algorithm can solve
the LQR problem efficiently in the model free setting.
Based on \cite{fatkhullin2021optimizing}, we consider the problem \eqref{eq:problem} with
$A  =(J-G)H$,
$B  = \ones(10, 4)+\frac{1}{2}\rand(10, 4)$,
$C  = \ones(2, 10)+\frac{1}{2}\rand(2, 10)$,
$Q  = I$,
$R  = I$,
where
$J = \tilde J - \tilde J^\top$, $\tilde J = \randn(10, 10)$,
$G = \tilde G$  $\tilde G^\top$, $\tilde G = \randn(10, 10)$,
$H = \tilde H \tilde H^\top$, $\tilde H = \randn(10, 10)$,
$\ones(a, b)$ is $a\times b$ matrix of ones,
$\rand(a, b)$ is $a\times b$ matrix with all entries distributed as the uniform distribution on $[0, 1]$,
and $\randn(a, b)$ is $a\times b$ matrix with all entries distributed as the standard normal distribution.
We assume the distribution $\mathcal{D}$ is the uniform distribution on $[-1, 1]^n$.
Since $J$ is skew-adjoint and $G, H$ are positive definite,
$A$ is \textit{Hurwitz}, as mentioned in Section \ref{sec:problem}.
Therefore, $A_{K_0}$ is \textit{Hurwitz} for $K_0 = O$.
We set the parameters $r = 0.01, \tau = 100$
and define $\Omega$ by
\begin{align}
	\Omega = \{K\in \R^{4\times 2}\mid K\circ S = O\},\quad S =\begin{pmatrix}
		1 & 1 & 0 & 0 \\
		0 & 0 & 1 & 1
	\end{pmatrix}^\top.
\end{align}

Fig.~\ref{fig:totalerror} illustrates that
the mean and standard deviation of 20 trials of the relative error
$\frac{\normF{\bar \grad f(K)-\grad f(K)}}{\normF{\grad f(K)}}$
in gradient estimation.
The relative error with variance reduction was much smaller than that of Algorithm 1.

Fig.~\ref{fig:convergence-curve} illustrates the mean and variance of 20 trials of the convergence curve of Algorithm~\ref{alg:mfgp},
where we set $\alpha = 2\cdot 10^{-4}, 2\cdot 10^{-5}$ and $N=15$ for Algorithm ~\ref{alg:mfgp} with baseline and $N=70$ for Algorithm~\ref{alg:mfgp} without baseline. This is because the estimation procedure of the baseline requires additional $\frac{n(n+1)}{2}=55$ samples.
Because of the large variance of the estimated gradient, Algorithm~\ref{alg:mfgp} without baseline made the system unstable quickly in the case of $\alpha=2\cdot 10^{-4}$.
As shown in Fig.~\ref{fig:convergence-curve}, the convergence rate of Algorithm~\ref{alg:mfgp} is sublinear and Algorithm~\ref{alg:mfgp} with baseline is more stable than Algorithm~\ref{alg:mfgp} without baseline even if we take into account additional 55 samples required to estimate the baseline.

\begin{figure}[t]
	\centering
	\begin{minipage}{0.48\columnwidth}
		\centering
		\includegraphics[width=1\columnwidth]{total_error.pdf}
		\caption{Relative error in gradient estimation.}
		\label{fig:totalerror}
	\end{minipage}
	\begin{minipage}{0.48\columnwidth}
		\centering
		\includegraphics[width=1\columnwidth]{model_free2.pdf}
		\caption{Convergence curve of Algorithm \ref{alg:mfgp}.}
		\label{fig:convergence-curve}
	\end{minipage}
\end{figure}


% \end{document}