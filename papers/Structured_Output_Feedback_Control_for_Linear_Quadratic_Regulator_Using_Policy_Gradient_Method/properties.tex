% \documentclass[ex_article]{subfiles}
% \begin{document}
\section{Properties of the Objective Function}\label{sec:properties}
In this section, we prove some properties of the objective function $f(K)$ in~\eqref{eq:problem}
for the convergence analysis of the gradient method presented in Section~\ref{sec:model-free}.


\subsection{Norm bounds}
In this section, we show some matrix norm inequalities.

We define the sublevel set by
$
  S(a) = \{K \in S \mid f(K)\leq a\},
$
where $S$ is defined as \eqref{eq:S}. Thus, all elements in $S(a)$ are stabilizing feedback gains.

Using the same argument as Lemma C.2 in~\cite{fatkhullin2021optimizing}, we have $\norm{K}\leq \kappa(a)$ for $K\in S(a)$, where
\begin{align}
    \kappa(a)&:=\frac{2\norm{B}_2\norm{C}_2a}{\lmin{(\Sigma)}\lmin{(R)}\lmin{(CC^\top})}+\frac{\norm{A}_2}{\norm{B}_2\norm{C}_2}.
\end{align}

Next, for $K \in S(a)$,
we provide an upper bound on the solutions to the following Lyapunov equations:
\begin{align}
  A_K^\top X + X A_K + C^\top(Q+K^\top R K)C   & = 0, \label{eq:lyapunovX}     \\
  A_K Y + Y A_K^\top + \Sigma                  & = 0, \label{eq:lyapunovY}     \\
  A_K Y' + Y'A_K^\top - \qty(BECY+(BECY)^\top) & = 0,\label{eq:lyapunovYprime}
\end{align}
where $A_K$ is defined in~\eqref{eq:AK} and $E \in \R^{m\times p}$ is a given matrix.
Note that $X$, $Y$, and $Y'$ uniquely exist, because
$K \in S(a)$ implies that
$A_K$ is \textit{Hurwitz}~\cite{bellman1957notes}.
To simplify the notation,
using $\sigma := -\frac{1}{2}\lmax(A_{K_0}+A_{K_0}^\top)>0$,
we define
\begin{align}
  \xi & := \frac{1}{4\norm{B}_2\kappa(a)},\,\, \mathfrak{X}(a) :=\frac{a}{\lmin(\Sigma)},   \label{eq:frakX} \\
  \mathfrak{Y}(a)  & :=\max\qty(\frac{a}{\xi^2\lmin(Q)}, \frac{\norm{\Sigma}}{\sigma}),                               \label{eq:frakY}\\
  \mathfrak{Y}'(a) &:=2\frac{\norm{B}_2\norm{C}_2\mathfrak{Y}(a)^2}{\lmin(\Sigma)}\label{eq:frakYprime}
\end{align}
where $K_0$ satisfies 3 in Assumption \ref{assume:sigma-hurwitz}.



\begin{lemma}\label{lem:xybound}
  Let $X, Y, Y'$ be the solution to~\eqref{eq:lyapunovX},~\eqref{eq:lyapunovY}, and~\eqref{eq:lyapunovYprime}, respectively.
  Assume that $\normF{E} = 1$.
  Then, for any $K\in S(a)$,
  %\begin{align}
    $\norm{X}_2  \leq \mathfrak{X}(a)$,\,\,
    $\norm{Y}_2  \leq \mathfrak{Y}(a)$,\,\, 
    $\norm{Y'}_2 \leq \mathfrak{Y}'(a)$.%\label{eq:xybound}
  %\end{align}
\end{lemma}

\begin{proof}
See Appendix \ref{sec:proof-lemma1}. \qed
\end{proof}

\subsection{$L$-smoothness of $f(K)$ }

A differentiable function is called $L$-smooth if its gradient is $L$-Lipschitz continuous.
For our objective function, we have the following result.
\begin{theorem}
  For any $a\in \R$, $f(K)$ in~\eqref{eq:problem} is $L$-smooth on $S(a)$ with the constant 
  \begin{align}
    L & := 2\lmax(R)\normF{C}^2\mathfrak{Y}(a) \label{eq:L}\\
    &+4\qty(\sqrt{n}\normF{R}\kappa(a)\normF{C}+n\normF{B} \mathfrak{X}(a)) \mathfrak{Y}'(a) \normF{C},\nonumber
  \end{align}
  where $\mathfrak{Y}(a)$ and $\mathfrak{Y}'(a)$ are defined in~\eqref{eq:frakY} and~\eqref{eq:frakYprime}. %respectively.
\end{theorem}
\begin{proof}
Theorem 3.15 in~\cite{fatkhullin2021optimizing} cannot be applied to our setting directly, because $\lambda_1(C^\top QC)$ may be $0$.
However, by replacing the norm bounds in the proof of Theorem 3.15 with those in Lemma 1, we obtain the result. \qed
\end{proof}

% \end{document}