\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}

%\usepackage[utf8]{inputenc} % allow utf-8 input
%\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
%\usepackage{url}            % simple URL typesetting
%\usepackage{booktabs}       % professional-quality tables
%\usepackage{amsfonts}       % blackboard math symbols
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
\usepackage[final]{graphicx}
\setcounter{tocdepth}{3}
\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip\noindent\keywordname\enspace\ignorespaces#1}


%\documentclass[12pt,a4paper,final]{article}
%\usepackage[T1]{fontenc}
%\usepackage[latin1,ansinew]{inputenc}
%\usepackage[usenames,dvipsnames]{color}
%\usepackage[danish]{babel}
%\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,bbm}
%\usepackage{amsmath,amsfonts,amssymb,amsthm,bbm}
%\usepackage{showkeys}
%\usepackage[font=scriptsize, labelfont=bf]{caption}
%\usepackage{fancyhdr}
%\pagestyle{fancy}
%\usepackage{verbatim}
\usepackage{color}
\usepackage{algorithm2e}
\usepackage{algorithmic}
%Margin notes:
%\usepackage{fixme}
%\fxusetheme{color}
%\marginparwidth=1.2in

%\frenchspacing
%\theoremstyle{plain} 
\newcommand{\R}{\mathbb R}
\newcommand{\Z}{\mathbb Z}
\newcommand{\N}{\mathbb N}
\newcommand{\dd}{\mathrm d}
\newcommand{\A}{\underline{\underline{A}}}
\newcommand{\B}{\underline{\underline{B}}}
\setlength{\parindent}{16pt}
\newcommand{\F}{\underline{\underline{F_{n}}}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\Ba}{\mathcal{B}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\FM}{\mathcal{FM}}

% Less whitespace:
\setlength{\belowcaptionskip}{-10.5pt}

\newlength\myindent
\setlength\myindent{2em}
\newcommand\bindent{%
  \begingroup
  \setlength{\itemindent}{\myindent}
  \addtolength{\algorithmicindent}{\myindent}
}
\newcommand\eindent{\endgroup}

\begin{document}

\mainmatter

\title{Stochastic Development Regression on Non-Linear Manifolds}

\author{Line K\"uhnel and Stefan Sommer}
\institute{Department of Computer Science, University of Copenhagen\\
\url{kuhnel@di.ku.dk},\ \url{sommer@di.ku.dk}}

\maketitle

\begin{abstract}
We introduce a regression model for data on non-linear manifolds. The 
model describes the relation between a set of manifold valued 
observations, such as shapes of anatomical objects, and Euclidean 
explanatory variables. The approach is based on stochastic development 
of Euclidean diffusion processes to the manifold. Defining the data 
distribution as the transition distribution of the mapped stochastic 
process, parameters of the model, the non-linear analogue of design 
matrix and intercept, are found via maximum likelihood. The model is 
intrinsically related to the geometry encoded in the connection of the 
manifold. We propose an estimation procedure which applies the Laplace approximation of the likelihood function. A simulation study of the performance of the model is performed and the model is applied to a real dataset of Corpus Callosum shapes.
\keywords{Regression, Statistics on Manifolds, Non-linear Statistics, Frame Bundle, Stochastic Development}
\end{abstract}

\section{Introduction}

%\textbf{SKRIV NOGET MERE OM HVORFOR AT VI BLIVER NØDT TIL AT KIGGE P\AA STOCHASTISKE STIER!! Giv en god forklaring med hvorfor at vi kigger på infinetisemale skridt! God ide m\aa ske at se p\aa\ at man ogs\aa\ kan se regression med en tidsafhængighed og at det er denne vi generaliserer! Skriv hvordan man definerer en normalfordeling ved at simulere brownian motions p\aa\ euklidisk rum der s\aa\ giver en normal fordeling her og at en normalfordeling p\aa\ manifold er da givet ved at transportere disse brownian motions til manifold!}

A main focus in computational anatomy is to study the shape of anatomical objects. Performing statistical analysis of anatomical objects is however challenging due to the non-linear nature of shape spaces. The established statistical theory for Euclidean data does not directly allow us to answer questions like: How does a treatment affect the deformation of an organ? or: Is it possible to categorize sick and healthy patients based on the shape of the subject's organs?
%The interest for statistical analysis of anatomical objects has grown remarkably the past years as a consequence of the increasing quality of imaging technology. A problem encountered when trying to apply already established statistical theory for Euclidean spaces is that the collection of such objects usually belong to a non-linear space. Nevertheless it would still be interesting to be able to answer questions like, how does a treatment affect the deformation of an anatomical object, or can we categorize sick and healthy patients based on the shape of such objects?

Shape spaces are typically non-linear and often equipped with manifold structure. Examples of manifold-valued shape data include landmarks, curves, surfaces, and images with warp variation. The lack of vector space structure for manifold-valued data implies that addition and scalar multiplication are not defined. Several concepts in statistics rely on addition and scalar multiplication, these including mean value, variance, and regression models. Hence, in order to make inference on manifold-valued data, generalization of Euclidean statistical theory is necessary. 

This paper focuses on generalization of regression models to manifolds. The aim is to model the relation between Euclidean explanatory variables and a manifold-valued response. The regression model has, as an example, applications in computational anatomy~\cite{younes_evolutions_2009}. The proposed model can for example be used to analyze how age affects the shape of Corpus Callosum~\cite{geoReg}.

Several approaches have previously been proposed for defining normal distributions on manifolds~\cite{pennec_intrinsic_2006,sommer_anisotropic}. In \cite{sommer_anisotropic}, the distribution is defined based on Brownian motions in $\R^m$ and the fact that normal distributions on $\R^m$ can be defined as transition distributions of Brownian motions. The normal distribution on the manifold is then defined as the transition distribution of the stochastic development of the Euclidean Brownian motion~\cite{hsu_stocAnMan}. The proposed regression model will be defined in a similar manner. The construction can be considered intrinsic as it only depends on the connection of the manifold, e.g. the Levi-Civita connection of a Riemannian manifold. It does not rely on linearization of the manifold, and it naturally includes the effect of curvature in the mapping of the stochastic processes.

%The target of this work, is to propose a regression model for describing the relation between explanatory variables defined in a Euclidean space and a manifold-valued response variable. This proposed model could for example be used to analyze how age affects the shape of corpus callosum~\cite{geoReg}. The model does hence have applications in computational anatomy, in which a main goal is to analyze the anatomical variation in organs~\cite{younes_evolutions_2009}.{\color{red} dette afsnit can flyttes op}

In Euclidean linear regression, the relation between explanatory variables, $\boldsymbol{X}$, and a response variable, $\boldsymbol{y}$, is modeled by an affine function of $\boldsymbol{X}$,
\begin{align}
    \boldsymbol{y} = \boldsymbol{a} + \boldsymbol{X}\boldsymbol{b} + \boldsymbol{\varepsilon}.
    \label{ereg}
\end{align}
Due to the lack of vector space structure, alternatives for modeling relations between the given variables, $\boldsymbol{X}$ and $\boldsymbol{y}$, are needed in the non-linear situation. Several ideas have previously been introduced and a selection of these will be described in Section \ref{sec:Background}.

In this paper, the regression model is considered as a transported linear regression defined in $\R^m$. This approach is inspired by the transport of normal distributions defined in~\cite{sommer_anisotropic}. Notice that the linear regression model $(\ref{ereg})$ can be generalized to situations in which several observations are observed over time,
\begin{align}
    \boldsymbol{y}_t = \boldsymbol{a}_t + \boldsymbol{X}_t\boldsymbol{b} + \boldsymbol{\varepsilon}_t, \ \ \text{for} \ \ t\in [t_1,t_2].
\label{treg}
\end{align}
 Our approach suggests to define the regression model by transportation of stochastic processes, $Z_t = \boldsymbol{a}_t + \boldsymbol{X}_t\boldsymbol{b} + \boldsymbol{\varepsilon}_t$, in $\R^m$ on to the manifold in order to obtain the relation to the response variable, $\boldsymbol{y}$ (see Figure \ref{fig:model}).

% In the proposed model, the relation is defined by transportation of stochastic paths. Based on the explanatory variables stochastic paths are defined and then transported to the manifold by stochastic development through the frame bundle of the manifold. Each observation is then modeled as a noisy measure of a stochastic variable following a distribution of endpoints of stochastic paths on the manifold (see Figure \ref{fig:LinFig}).

\begin{figure}
\centering
\includegraphics[scale = 0.35]{ManEustoc.pdf}
\caption{The idea behind the proposed regression model. Stochastic processes in $\R^m$ is transported to $\M$, by stohcastic development $\varphi$, to model the relation between the explanatory variables and the response $y\in\M$.}
\label{fig:model}
\end{figure}

The paper will be structured as follows. In Section \ref{sec:Background}, we give a discussion on previous methods developed for regression on manifolds. Section \ref{sec:dev} presents a short description of development of stochastic paths from a Euclidean space to the manifold. Section \ref{sec::Mod} introduces the proposed model, followed by a description of the estimation procedure in Section \ref{sec:Est}. In Section \ref{sec:sim} and \ref{sec:datEx}, illustrative examples are considered for the application and performance of the model. The paper is ended by a discussion of the defined model in Section \ref{sec:Dis}.

\section{Background}
\label{sec:Background}

Multiple approaches have been proposed for generalizing regression models to non-linear manifolds. The methods consider the regression problem in different situations. In this paper we will consider the case of Euclidean exaplanatory variables and a manifold-valued response. There have been several works describing regression models for manifold-valued data in other situations~\cite{cheng_local_2013,aswani_regression_2011,loubes_kernel-based_2009,steinke_non-parametric_2009}.

% For a response variable taking values in a Euclidean space with manifold-valued explanatory variables, local linear regression has been proposed. In \cite{cheng_local_2013}, it is assumed that explanatory variables are defined on a low-dimensional manifold embedded in an ambient space of higher dimension. Local linear regression is made based on projections of the explanatory observation to a tangent space on the manifold. Local regression is also considered in \cite{aswani_regression_2011} in which explanatory observations in a neighborhood of a pre-specified point $\boldsymbol{x}_0$ are assumed to form a manifold described by a chart. In the neighborhood of $\boldsymbol{x}_0$, the regression function is then estimated.  Besides local linear regression models, \cite{loubes_kernel-based_2009} introduces a generalization of the Euclidean kernel rule for classification of a manifold-valued variable. In~\cite{steinke_non-parametric_2009}, a regression manifold is proposed between manifolds. The regression function is estimated by minimizing the regularized empirical risk.

Regression models for describing the relation between a manifold-valued response and Euclidean explanatory variables have also previously been introduced. Examples include \cite{lin_extrinsic_2015} in which an extrinsic regression model is introduced, and \cite{shi_intrinsic_2009}, which defines an intrinsic regression model where the parameter vector is estimated by minimizing the total sum of squares based on the Riemannian manifold distance. Another example is the geodesic regression model introduced in \cite{geoReg}, which is a generalization of the linear regression model in Euclidean spaces. The relation is here modeled by a geodesic described by an initial velocity dependent on an explanatory variable and a starting point on the manifold.%However, one could imagine that a geodesic model was not able to fit the relation in any given situation, like linear regression models also have their restrictions.

In this paper, we will take a different view on how to relate the response and explanatory variables. Instead of considering the relation as being modeled by geodesics on the manifold as in \cite{geoReg}, we will describe the relation by stochastic paths transported from the space of explanatory variables to the manifold. By defining the regression model using stochastic paths, we are able to model non-geodesic relations, incorporate several explanatory variables, and consider random effects in the model. Non-geodesic relations have been considered by others before. An example is~\cite{singh_splines_2015} in which the geodesic regression model from~\cite{geoReg} is generalized in order to model more complex shape changes. The regression function is in this case fitted by piecewise cubic splines that describes the variation of one explanatory variable. In~\cite{hong_parametric_2016}, a regression model is introduced, in which the non-geodesic relation is obtained by time-warping. Others have proposed to model the non-geodesic relation by either a generalized polynomial regression model or by non-linear kernel-based regression~\cite{hinkle_polynomial_2012,yuan_local_2012,banerjee_nonlinear_2015,banerjee_nonlinear_2016,davis_population_2007}. On the contrary,~\cite{singh_hierarchical_2016} introduces the Hierarchical Geodesic Model which are able to consider several explanatory variables including random variables, but assumes nested observations and does only consider geodesic relations. A regression model, which incorporates both a non-geodesic relation and several explanatory variables, is proposed in~\cite{cornea_regression_2017}. This work defines an intrinsic regression model on Riemannian symmetric spaces, in which the regression function is obtained by minimizing the conditional mean of residuals defined by the log-map.

%These problems have also been considered before in the case of longitudinal data. In (Hiera!!!!) hierarchical linear models are generalized to hierarchical geodesic models on the manifold. These models use geodesics to model individual and group deformation of shapes through time.   

 In addition to describing the proposed model, we perform estimation of model parameters by maximum likelihood using the transition density on the manifold. The model does not linearize the manifold as in many of the local regression models, but instead take into account the curvature of the manifold at each point as encoded in the connection through the mapping of the stochastic process.

\section{Stochastic Development}
\label{sec:dev}

In this section we give a brief description of stochastic development of curves in $\R^m$ to the manifold. The reader is referred to~\cite{hsu_stocAnMan,sommer_anisotropically_2016,sommer_modelling_2016} for a deeper description of this concept.
% We will not go into details with the concepts, but only give a short description of what is used in this article. The reader is encouraged to consider \cite{hsu_stocAnMan} for additional theory on stochastic differential geometry.

 Let $\mathcal{M}$ be a $d$-dimensional manifold provided with a connection $\nabla$ and metric $g$. The connection is necessary for transportation of tangent vectors along curves on the manifold. A frequently used connection is the Levi-Civita connection coming from a Riemannian structure on $\M$. Let $\partial_i$ for $i=1,\ldots,d$ denote a coordinate frame on $\M$ and let $dx^i$ be the corresponding dual frame. A connection $\nabla$ is given in terms of its Christoffel symbols defined by $\nabla_{\partial_i} \partial_j = \Gamma_{ij}^k \partial_k$. For the Levi-Civita connection, the Christoffel symbols are given by
\begin{align}
    \Gamma_{ij}^k = \frac{1}{2}g^{kl}(\partial_i g_{jl} + \partial_j g_{il} - \partial_l g_{ij})
\label{eq:Chris}
\end{align}
in which $g_{ij}$ is the components of $g$ in the coordinate basis, i.e. $g = g_{ij}dx^idx^j$, and $g^{ij}$ is the inverse components.
% satisfying $g_{il}g^{lj}=\delta_i^j$ with $\delta_i^j$ denoting the Kronecker delta.

%In order to define the stochastic development, one has to consider the frame bundle $\FM$. 
%The connection is necessary for the transportation of frames along curves on the frame bundle $\mathcal{FM}$.
 Consider the frame bundle $\mathcal{FM}$ being the set of tuples $(y,\nu)$ in which $y\in\M$ and $\nu$ is a frame for the tangent space $T_y\M$. Let $\pi\colon\mathcal{FM}\to\mathcal{M}$ be the projection map given by $\pi(y,\nu) = y$ for $(y,\nu)\in\FM$. A smooth curve $U_t$ on $\FM$ is a smooth selection of frames, i.e. for every $t\in I$, $U_t = (y_t,\nu_t)$ in which $\nu_t\colon\R^d\to T_{\pi(U_t)}\mathcal{M}$ is a frame.
% The connection $\nabla$ is necessary for transport of frames along curves on $\mathcal{FM}$.

 Given a connection $\nabla$, the tangent space of the frame bundle, $T\FM$, splits into a horizontal and a vertical part, $T\FM = H\FM\oplus V\FM$. The horizontal subspace explains infinitesimal changes of the base point on the manifold. On the other hand, tangent vectors in $V\FM$ describe changes of the frame $\nu$ keeping the base point fixed. Given a tangent vector $v\in T_y\mathcal{M}$ and a frame $\nu$, a vector in $H_{(y,\nu)}\FM$ can be defined by horizontal lift. The horizontal lift of a tangent vector $v$ is the unique horizontal vector $w\in H_{(y,\nu)}\FM$, satisfying $\pi_\star w = v$, where $\pi_\star\colon H_{(y,\nu)}\mathcal{FM}\to T_y\mathcal{M}$ is induced by the projection $\pi$. The horizontal lift of $v$ will be denoted $h_l(v)$.

Consider a probability space $(\Omega,\mathcal{F},P)$ and a stochastic process $X_t\colon\Omega\to \mathcal{W}(\R^m)$, where $\mathcal{W}(\R^m)$ denotes the path space of $\R^m$. The stochastic development of $X_t$ to $\FM$ can be defined as a solution, $U_t$, of the Stratonovich stochastic differential equation,
\begin{align}
	dU_t = \sum_{i=1}^d H_i(U_t)\circ dX^i_{t},
\label{eq:stocDev}
\end{align}
where $\circ$ symbolizes a Stratonovich stochastic differential equation. The vector fields $H_1,\ldots,H_d$ denotes a basis for the horizontal subspace of $T\FM$. Given a point $u = (y,\nu)\in\FM$, $H_i$ are defined as $H_i(u) = h_l(\nu(e_i)), \ i = 1,\ldots,d$, where $e_1,\ldots,e_d$ is the canonical basis for $\R^d$.
%The space of horizontal vectors are denoted $H_{(y,Y_\alpha)}\mathcal{FM}$ and splits the tangent space of the frame bundle together with the vertical subspace $V_{(y,Y_\alpha)}\FM$, i.e. $T_{(y,Y_\alpha)}\FM = H_{(y,Y_\alpha)}\FM\oplus V_{(y,Y_\alpha)}\FM$.  
% Based on a tangent vector $v\in T_y\mathcal{M}$ for a frame $Y_\alpha$ we can relate a vector in $H_{(y,Y_\alpha)}\FM$ by the horizontal lift. The horizontal lift is given by the unique horizontal vector $w^\star\in H_{(y,Y_\alpha)}\FM$, such that $\pi_\star w^\star = v$ where $\pi_\star\colon H_{(y,Y_\alpha)}\mathcal{FM}\to T_y\mathcal{M}$ induced by the projection $\pi$.
%Based on the canonical basis vectors, a basis for $H\FM$ can be defined by
%Given these horizontal lifts of tangent vectors on $\M$ we can define the horizontal vector fields based on the canonical basis vectors. Let $e_1,\ldots,e_d$ denote the canonical basis vectors, then $H_i$ is defined to be
%\begin{align}
%	H_i(Y_\alpha) = (Y_\alpha(e_i))^\star, i = 1,\ldots,d,
%\end{align}
%which is the horizontal lift  of the tangent vector $Y_\alpha(e_i)\in T_{y}\mathcal{M}$ for a frame $Y_\alpha$. 
% Based on the horizontal vector fields $H_1,\ldots,H_d$ the stochastic development of $X_t$ is given as the solution to the stratonovich stochastic differential equation
%\begin{align}
%	dU_t = \sum_{i=1}^d H_i(U_t)\circ dX^i_{t}.
%\label{eq:stocDev}
%\end{align}
 A path $Y_t$ on the manifold $\M$ can then be obtained by the projection of $U_t$ onto $\M$ by the projection map $\pi$, i.e. $Y_t = \pi(U_t)$.

Consider two processes $X^1_t,X^2_t$ in $\R^m$, $t\in [0,T]$ for $T > 0$, for which $X^1_0 = X^2_0 = \boldsymbol{x}_0$ and $X^1_T = X_T^2$. If $Y^1_t$, $Y^2_t$ denotes the stochastic development of $X^1_t$ and $X^2_t$ respectively on $\mathcal{M}$, then it does not in general hold that $Y^1_T = Y^2_T$ on $\M$ due to the curvature of the manifold.

%\subsection{Parallel Transport of Frames}

%As a part of the stochastic development of a curve $X_t$, the frame considered at the starting point for the curve $U_t$ on $\FM$ is parallel transported along the curve $X_t$. The transportation of a frame induces a natural transport of inner products on the frame bundle. Consider a frame $Y_{\alpha}$ for the tangent space $T_y\M$. Based on this frame there is a natural inner product on $T_y\M$ given by
%\begin{align}
%    \langle v,w\rangle_{Y_{\alpha}} = \langle Y_{\alpha}^{-1}v,Y_{\alpha}^{-1}w\rangle_{\R^m}.
%\end{align}
%These inner products on $T_y\M$ can be lifted to an inner product on $H_{(y,Y_\alpha)}\FM$ given by
%\begin{align}
%    \langle v,w\rangle = \langle \pi_\star v,\pi_\star w\rangle_{Y_{\alpha}}.
%\end{align} 
%This inner product, then induces a sub-Riemannian metric $g_{\FM}\colon T\FM^\star\to H\mathcal{FM}$ defined as
%\begin{align}
%    \langle v,g_{\FM}(\xi)\rangle = \xi(v), \ \forall v\in H_{(y_t,u_t)}\FM.
%\end{align}
%Consider coordinates $(y^i,Y_\alpha^i)$ of $\FM$, such that a frame $Y_\alpha = Y_\alpha^i\,\frac{\partial}{\partial y^i} = \sum_{i=1}^d Y_\alpha^i\,\frac{\partial}{\partial y^i}$. In the rest of this section we will restrict our selves to the einstein notation $Y_\alpha^i\,\frac{\partial}{\partial y^i}$. Let $\Gamma^h_{ij}$ denote the Christoffel symbols for the connection $\nabla$, then given coordinates on the frame bundle a basis for the horizontal subspace is given by
%\begin{align}
%    D_i = \frac{\partial}{\partial y^i} - \Gamma^{j_\beta}_i\frac{\partial}{\partial Y^j_\beta},
%\end{align}
%for $i,\alpha=1,\ldots,d$ and where $\Gamma^{j_\beta}_i = \Gamma^j_{ik}Y_\beta^k$. A basis for the veritcal subspace is, $D_{i_\alpha} = \partial_{Y_\alpha^i}$. Moreover a coframe for the dual space of $H\FM$ and $V\FM$, respectively, can be obtained by
%\begin{align}
%    D^i = dy^i, \quad D^{i_\alpha} = \Gamma^{i_\alpha}_j dy^j + dY^i_\alpha, \quad \text{for} \quad i,\alpha = 1,\ldots,d.
%\end{align}
%Let $\xi_i\in H\FM^\star$ and $\xi_{i_\alpha}\in V\FM^\star$ for $i,\alpha = 1,\ldots,d$, then based on the basis and coordinates $(y^i,Y_\alpha^i)$ the sub-Riemannian metric $g_{\FM}$ has the form
%\begin{align}
%    g_{\FM}(\xi_iD^i + \xi_{i_\alpha}D^{i_\alpha}) = W^{ji}\xi_iD_j,
%\end{align}
%where $W^{ji} = \delta^{\alpha\beta}Y_\alpha^jY_\beta^i$ and $\delta^{\alpha\beta} = 1$ for $\alpha = \beta$ and $0$ otherwise. This theory is based on \cite{gsi15} in which a full description of how to derive the metric is given. 
%
%The sub-Riemannian metric $g_{\FM}$ is used to write down the evolution equations of the stochastic development when parallel transport of a full frame on $\FM$ is considered. However sometimes we wish to base the stochastic development on parallel transport of a subset of the basis vectors of a frame $Y_\alpha$. Consider $k<d$ basis vectors of the frame $Y_\alpha$ and define $\tilde{Y}_\alpha\colon\R^k\to T_y\M$ to be the reduced frame on the frame bundle of rank $k$, $\mathcal{F}^k\M$. On this frame bundle a cometric can be defined by $g_{\mathcal{F}^k\M} + \lambda g_R$ for some riemannian metric $g_R$ and $g_{\mathcal{F}^k\M}$ induced by the inner product of the $k$ tangent vectors of the reduced frame. Based on \cite{gsi15} it can be shown that the sub-Riemannian metric $g_{\mathcal{F}^k\M}$ is given by
%\begin{align}
%    g_{\mathcal{F}^k\M}(\xi_iD^i + \xi_{i_\alpha}D^{i_\alpha}) = W^{ji}\xi_iD_i \quad \text{for} \quad i,\alpha =1,\ldots,k,
%\end{align}
%where $W^{ji} = \delta^{\alpha\beta}Y^j_\alpha Y^i_\beta + \lambda g_R^{ji}$. Notice that the two cases are almost similar, the only differences is that when considering $k<d$ tangent vectors the term $\lambda g_R^{ji}$ is added to $W^{ji}$ and only $k$ basis elements for the frame and dual frame of $H\FM$, $V\FM$ have to be used. Based on this new sub-Riemannian...
%
%\textbf{Skriv noget om hvordan evolution ligningerne ser ud!!}

\begin{figure}
\centering
\includegraphics[scale = 0.37]{PosterPic.pdf}
\caption{Illustration of the regression model. Stochastic processes $z_t^{i}$, defined in $(\ref{eq:paths})$, are transported through the frame bundle $\FM$ to $\M$, with stochastic development, $\varphi$. Each observation $y_i$ is then modelled as a noisy member of the endpoint distribution of the transported $z_t^{i}$ processes. The model supports cases where the endpoint noise $\tilde{\varepsilon}$ perturbes $y_i$ in the ambient space $\R^k$ in which $\M$ is embedded.}
\label{fig:LinFig}
\end{figure}

\section{Model}
\label{sec::Mod}

Let $\M$ be a $d$-dimensional manifold embedded in the ambient space $\R^k$ for some $k\geq d$ and consider a response variable $y$ in $\M$. Let $\nu_{y_0}\colon\R^d\to T_{y_0}\M$ be a frame for the tangent space at a reference point $y_0\in\M$. Assume that $\boldsymbol{y}_1,\ldots,\boldsymbol{y}_n\in\R^{k}$ are $n$ realizations of $y\in\M$ and let $\boldsymbol{x}_i = (x^1_i,\ldots,x^m_i)\in\R^{m}$ denote the vector of explanatory variables for the $i$'th observation. Notice that the realizations of $y$ are assumed to lie in the ambient space $\R^k$ and not required to be in $\M$. This construction allows for observations measured with noise which are not necessarily observed as elements of $\M$.

The strategy of the proposed model is to define stochastic processes according to the generalized linear regression in $(\ref{treg})$ and transport these to the manifold by stochastic development. All stochastic processes are defined for $t\in [0,T]$ for a $T > 0$. Consider for each observation $i$ the stochastic process $z_t^{i}\colon\Omega\to\mathcal{W}(\R^m)$, solution to the stochastic differential equation,
\begin{align}
    dz^i_t = \beta dt + \tilde{W} dX^i_t + d\varepsilon_t.
\label{eq:paths}
\end{align}
The first term, $\beta dt$, is a fixed drift for $\beta\in\R^m$. $\tilde{W} dX_t^i$ is the dependence of the explanatory variables with $X_t^{i}\colon\Omega\to\mathcal{W}(\R^m)$ being a stochastic process satisfying $X_0^{i}(\omega) = 0$ and $X_{T}^{i}(\omega) = \boldsymbol{x}_i$ for $\omega\in\Omega$. The matrix $\tilde{W}$ is a $m\times m$-dimensional matrix with columns relating to the basis vectors of the frame $\nu_{y_0}$ on $\M$. Consider the matrix $W$ with columns consisting of basis vectors of $\nu_{y_0}$. If $\M$ has a Riemannian metric, then $W = U\tilde{W}$, in which $U$ denotes a $d\times m$ orthonormal matrix with respect to the metric.
%When estimating $W$, it is often convenient to consider $W = U\tilde{W}$, where $\tilde{W}$ is an $m\times m$-matrix and $U$ is a $d\times m$-matrix orthonormal matrix with respect to a Riemannian metric on $M$.
Notice that this model can incorporate both fixed and random explanatory variables. If the $j$'th explanatory variable, $x_i^j$, is a random effect, $X_t^{ij}$ is modeled as a Brownian bridge, while it for fixed effects are modeled as a constant drift. The random error, $\varepsilon_t$, is modeled as a multidimensional Brownian motion on $\R^m$. 
%As in Euclidean linear regression, the model can only give meaningful parameter estimates when the number of explanatory variables is less than $\mathrm{dim}(\M) = d$.

The $i$'th observation $y_i$ is modeled as a noisy endpoint of the stochastic development of $z_t^i$. If $m<d$ only a reduced frame $\tilde{\nu}_{y_0}$ is used for the stochastic development of $z_t^i$. The reduced frame is considered as we are only interested in the effect of frame vectors associated to the explanatory variables. The basis vectors of $\tilde{\nu}_{y_0}$ corresponds to the columns of $W$.
% The stochastic process $z_t^{i}$ is then transported to the manifold by stochastic development. In this lies parallel transport of the reduced frame $\nu_{y_0}$ consisting of the basis vectors corresponding to the columns in $W$. Notice that only the reduced frame is parallel transported as we are only interested in the effect of the frame vectors associated to explanatory variables. 
Given the reference point $y_0\in\M$, define stochastic processes $Y_t^{i}$ as the stochastic development of $z_t^{i}$. Let $\mathcal{Y}^T_{i}\colon\Omega\to\M$ be a random variable following the distribution of endpoints of the stochastic development $Y_t^{i}$. Then
\begin{align}
    y_{i} = \mathcal{Y}^T_i + \tilde{\varepsilon}_{i},
\end{align}
where $\tilde{\varepsilon}_{i}\sim\mathcal{N}(\boldsymbol{0},\tau^2\mathbb{I}_d)$ 
%is assumed to be a normally distributed vector in $\R^d$, with mean $0$ and covariance matrix $\tau^2\mathbb{I}_d$,
represents the random measurement error that pulls the realization, $y_i$, from the manifold. In Figure \ref{fig:LinFig}, the two steps of the model are illustrated. First, the stochastic development of $z_t^i$ are defined on the frame bundle and finally, this stochastic development is projected to the manifold.

Notice that in the case $\M = \R^k$ with the standard connection on $\R^k$, the proposed model reduces to the regular regression model for data in $\R^k$. Assume $y\in\R^k$ and that $X_t^i$ is a vector from $0$ to $\boldsymbol{x}_i$. Then $\beta$ and $y_0$ relates to the intercept, $W$ is the matrix of regression coefficients and $\varepsilon_t$ and $\tilde{\varepsilon}$ the iid. random noise. 
%In this case $z_t^{i}$, would just be a straight line given by the regular linear regression model and as the stochastic development of a straight line into $\R^k$ is just a straight line starting at $y_0$, we see that the model reduces to a linear regression model in $\R^k$. 

%Notice that by considering regression models on $\M$ in this manner, a natural time variable is introduced to the model. Hence the observations $\boldsymbol{y}_i$ should be considered as a discrete observation of the unknown stochastic path $Y_{it}$ describing the evolution of $y$ from the reference point $y_0$ to $\boldsymbol{y}_i$. It is based on these data that the parameters $y_0$ and $\tilde{X}_\alpha$ will be estimated in Section \ref{sec:Est}. The problem which have to be asressed, is how to make a reasonable estimate of the missing data $Y_{it}$. If multivariate data were considered, i.e. several observations over time for each individual, then it might be interesting to model the whole path $Y_{it}$ as a member of the distribution of stochastic developments $Y_t^{ij}$ instead of just the endpoints $y_i$. In this situation, based on a prediction of $Y_{it}$, it would be possible to interpret the evolution of $y$ for an individual between two time points.

\section{Estimation}
\label{sec:Est}

The reference point $y_0$, the matrix $W$, the drift $\beta$, and the variance parameter $\tau^2$ are the parameters of the model.
% These parameters correspond to the parameters of the Euclidean linear regression model \eqref{ereg}.
 These parameters can be estimated in several ways. This section describes a Laplace approximation of the marginal likelihood function which are used for finding optimal parameter estimates. We could alternatively use a Monte Carlo EM based procedure using simulations of the missing data, $Y_t^i$ for $t\in [0,T]$, to optimize the complete data likelihood. This will be considered in future works.

Laplace approximation can be used to determine a linear approximation of a non-linear likelihood function~\cite{kass_approximate_1989}. Let $\theta$ denote the vector of parameters, and $d\boldsymbol{x}_t$ a discretization of the process $X_t$ at $n_s+1$ time-points. Hence $d\boldsymbol{x}_t$ is a vector of length $n\cdot m\cdot n_s$, in which $n_s$ denotes the number of time steps, $n$ the number of observations, and $m$ the number of explanatory variables. Let $f(y\lvert\theta)$ be the conditional density of the response $y\in\M$ given $\theta$ and $p(d\boldsymbol{x}_t\lvert\theta)$ the density of the discretization of $X_t$ given $\theta$. To find the optimal parameter vector, $\theta$, the following likelihood has to be optimized,
\begin{align}
L(\theta;\boldsymbol{y}) &= f(y\lvert\theta) = \int f(y\lvert d\boldsymbol{x}_t,\theta)p(d\boldsymbol{x}_t\lvert\theta) d(d\boldsymbol{x}_t) = \int e^{-n h(d\boldsymbol{x}_t)} \ d(d\boldsymbol{x}_t),
\end{align}
where $h(d\boldsymbol{x}_t) = -\frac{1}{n} \log f(y\lvert d\boldsymbol{x}_t,\theta) - \frac{1}{n}\log p(d\boldsymbol{x}_t\lvert\theta)$.
The Laplace approximation of $L$ is then given by
\begin{align}
L(\theta;\boldsymbol{y}) \approx f(y\lvert d\boldsymbol{x}^{o}_t,\theta)p(d\boldsymbol{x}^{o}_t\lvert\theta)(2\pi)^{\frac{mn_s}{2}}\lvert\Sigma\lvert^{\frac{1}{2}} n^{-\frac{mn_s}{2}},
\label{eq:apL}
\end{align}
in which $d\boldsymbol{x}^{o}_t = \text{argmax}_{d\boldsymbol{x}_t}\{-h(d\boldsymbol{x}_t)\}$ and $\Sigma = \left(D^2 h(d\boldsymbol{x}_t)\right)^{-1}$, the inverse of the Hessian of $h(d\boldsymbol{x}_t)$. The approximated likelihood is then optimized wrt. $\theta$ to obtain the estimated parameters. In the following simulation study, the Laplace approximation is used for parameter estimation. The code for the estimation algorithm as well as the simulation study below was implemented in Theano~\cite{2016arXiv160502688short}. The code is available at \url{https://bitbucket.org/stefansommer/theanodiffgeom}.

%The other estimation procedure considered for parameter estimation is the Monte Carlo EM~\cite{delyon_simulation_2006}. Consider the paths $Y_t = (Y_t^i)_{i=1,\ldots,n}$ as missing data and let $P^y_{\theta_k}$ denote the distribution of $y$ wrt. the parameter vector $\theta_k$. In the following, $f_\theta(x)$ is the density of $x$ wrt. $\theta$. 
%The goal of Monte Carlo EM is to optimize the complete data likelihood
%\begin{align}
%    Q(\theta\mid\theta_k) = E_{P^y_{\theta_k}}\left[\log f_\theta(Y_t)\right] = \sum_{i=1}^n E_{P^y_{\theta_k}}\left[\log f_\theta(Y_t^i)\right],
%\end{align}
%using simulations of $Y_t^i$ from the distribution $P^y_{\theta_k}$, where $\theta_k$ is the estimated parameter vector in the $k$'th iteration. It is difficult to sample from the distribution $P^y_{\theta_k}$. To sample the paths $Y_t^i$ wrt. $\theta_k$ the processes $X_t^i$ are generated, transformed into $z^i_t$ processes which are then transported to the manifold, $\mathcal{M}$. Hence the paths $X_t^i$ can be considered as missing data instead of the $Y_t^i$ paths and the complete data likelihood function can be written as 
%\begin{align}
%    Q(\theta\mid\theta_k) = \sum_{i=1}^n E_{P_{\theta_k}}\left[\log f_\theta(X_t^i,y_i)\right],
%\end{align} 
%where $P_{\theta_k}$ is the distribution of $X_t^i\mid y_i$ wrt. $\theta_k$. Sampling directly from $P_{\theta_k}$ is still a difficult task. Instead, the optimization function is rewritten such that simulations from the distribution of Brownian bridges, $X_t^i$, are considered. Let $D$ denote the distribution of Brownian bridges, thus the optimization function $Q$ can be rewritten as
%\begin{align}
%    Q(\theta\mid\theta_k) = \sum_{i=1}^n E_D\left[\log f_\theta(X_t^i,y_i)\frac{f_{\theta_k}(X_t^i\lvert y_i)}{f_{\theta_k}(X^i_t)}\right].
%\end{align}
%In this case estimation of $Q$ can be seen as an importance sampling problem. The $Q$ function is then approximated by
%\begin{align}
%    \tilde{Q}_k(\theta) = \frac{1}{N}\sum_{j=1}^N\sum_{i=1}^n\log f_\theta(X^{ij}_t,y_i)\frac{f_{\theta_k}(X_t^{ij}\lvert y_i)}{f_{\theta_k}(X_t^{ij})},
%\label{eq:approxQ}
%\end{align}
%in which $N$ denotes the number of simulations of the Brownian bridges $X_t^{i}$. %The optimization algorithm is given in details in Algorithm~\ref{alg}.
%In the following simulation study, the Lapplace approximation procedure is used for parameter estimation.

%\begin{algorithm} \DontPrintSemicolon \SetAlgoLined 
%\KwResult{Estimates of the parameters $y_0$, $W$, $\sigma^2$ and $\tau^2$.}  
%\tcp{Initialize parameters} 
%Initialize $\theta^0$\;
%\For{$k=1$ to $k_{\max}$}{ 
%	\tcp{First Step: Simulation}
%        \quad Simulate $N$ brownian motions $\omega_t$.\;
%        \quad Calculate $Z_t^i$ wrt. $\theta_k$.\;
%    \tcp{Second Step: Gradient Calculation}
%    	\quad Calculate gradient of $\tilde{Q}_k(\theta)$ given by eq. $(\ref{eq:approxQ})$.\;
%    \tcp{Third Step: Update Parameters}
%		\quad Update $\theta_k$ by taking a gradient step: $\theta_{k+1} = \theta_k - \gamma\cdot\nabla\tilde{Q}_k(\theta_k)$.\;
%}
%\caption{Inference in the model.}\label{alg}
%\end{algorithm}

%\section{Applications}
%\label{sec:explussim}
%
%We here aim at illustrating properties of the model on simulated synthetic data and for testing parameter estimation. Moreover, we will give an example of the application of the model to a real dataset of human corpora callosa. To estimate parameters in these examples, the Laplace Approximation were used.

\section{Simulation Study}
\label{sec:sim}

%In this section we aim at illustrating properties of the model on simulated synthetic data and for testing parameter estimation. 
This section investigates properties of the model on simulated synthetic data. Two setups will be introduced, both considering landmark representations of shapes. The data are assumed to lie in a manifold defined in the LDDMM (Large Deformation Diffeomorphic Metric Mapping) framework~\cite{shapes}.

 In the LDDMM framework, deformations of shapes are modeled as smooth flows which are solutions to ordinary differential equations defined by vector fields. A point $q\in\M$ is a finite number of landmarks, $q = (x_1^1,x_1^2,\ldots,x_{n_l}^1,x_{n_l}^2)$. The metric on $\M$ is given by $g(v,w) = \sum_{i,j}^{n_l} vK^{-1}(x_i,x_j)w$, where $K^{-1}$ denotes the inverse of a kernel $K$. In this simulation study $K$ is the Gaussian kernel with standard deviation, $\sigma = 0.5$. Based on this metric the Levi-Civita connection can be obtained by calculating the Christoffel symbols defined in $(\ref{eq:Chris})$.

To begin with, we consider estimation of $\tilde{W}$ and $y_0$ and investigate the performance of the estimation procedure. The shapes that will be considered consists of $8$ landmarks generated from the unit circle with landmarks located at $0,\frac{\pi}{4},\frac{\pi}{2},\ldots,\frac{3\pi}{2},\frac{7\pi}{4}$ radians. The center plot of Figure \ref{fig:simCirc} shows the unit circle with the chosen frame for each landmark. The number of explanatory variables are set to $m=2$ and the variables are drawn from a normal distribution with mean $0$ and standard deviation $2$. The other parameters are set to
\begin{align}
    \tilde{W} &= \begin{pmatrix}
0.2 & 0.1 \\
0.1 & 0.2
\end{pmatrix}, \ \tau = 0.1
\label{eq:simT}
\end{align}

\begin{figure}
\centering
\includegraphics[scale = 0.35, trim = 60 30 30 30, clip]{SimCircles.pdf}
\caption{The figures show the simulation of a dataset. (left) The stochastic paths in $\R^m$ are shown, where the vector of explanatory variables for each observation $i$ is represented by a green dot. (center) The true frame for the simulated data as well as the reference shape are plotted. (right) The simulated observations are shown, with the stochastic developments as the red processes.}
\label{fig:simCirc}
\end{figure}

 In Figure \ref{fig:simCirc} is shown an example of simulated observations as well as the sample paths $X_t^i$. A total of 50 datasets were sampled, in which each consisted of 20 observations. For each simulated dataset, the $\tilde{W}$ matrix was estimated. Each of the estimated distrubtions for the entries of $\tilde{W}$ are shown in Figure \ref{fig:densFig}. By the results, we conclude that the estimated parameters are fairly stable between the different simulations and that the true values are well centered in each distribution. For this simulation, the estimation procedure is thus able to estimate the true $\tilde{W}$ parameters that were specified in the model.


% In the first study we consider  we are going to estimate the  In the case of a drift term we ran the optimization for a single dataset to check the estimation procedure. In this case we estimated all parameters besides the variance $\tau^2$. The results are shown in Table \textcolor{red}{????}.

%By Table \textcolor{red}{????} we see that the estimation procedure converges and that it converges towards the true parameters.

%To make a deeper investigation of the performance of the estimation procedure we made a small study to see how the estimation procedure reacted to different datasets. The number of simulated datasets was set to 50 and for each dataset the $\tilde{W}$ parameters were estimated. The results are shown in Figure \ref{fig:densFig}. From the results we see that the estimated parameters did not vary that much for the different datasets, so the estimation procedure is good at reproducing the true $\tilde{W}$ parameters that were specified in the model.

\begin{figure}
\centering
\includegraphics[scale = 0.37,trim = 5 5 5 10,clip]{ParadensB.pdf}
\caption{The distribution of the estimated $\tilde{W}$ parameters. The red horizontal lines show the true parameters given in $(\ref{eq:simT})$.}
\label{fig:densFig}
\end{figure}

 Three similar datasets, as explained above, were sampled with different number of observations, 20, 60 and 100 respectively. The matrix $\tilde{W}$ as well as the reference point $y_0$ were estimated for each of the three datasets. In this case, the estimated $\tilde{W}$ matrix was found to be
\begin{align}
\hat{W}_{20} = \begin{pmatrix}
0.206 & 0.136 \\
0.147 & 0.322
\end{pmatrix}, \quad \hat{W}_{60} = \begin{pmatrix}
0.22 & 0.11 \\
0.11 & 0.21
\end{pmatrix}, \quad \hat{W}_{100} = \begin{pmatrix}
0.205 & 0.104 \\
0.115 & 0.214
\end{pmatrix}
\end{align}
while the estimated reference points are shown in Figure \ref{fig:refCirc}. By increasing the number of observations, we conclude that the estimated parameters $\tilde{W}$ and $y_0$ converge towards the true parameters.

\begin{figure}
\centering
\begin{minipage}{0.5\textwidth}
    \includegraphics[scale = 0.4, trim = 15 10 10 10, clip]{y0est.pdf}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
    \includegraphics[scale = 0.4, trim = 15 10 10 10, clip]{y0est60100.pdf}
\end{minipage}
\caption{(left) The estimated reference point $y_0$ (red) for the dataset with $20$ observations. (right) The estimated $y_0$ for $60$ (cyan) and $100$ (red) observations. In both plots, the initial (green) and the true reference circle (blue) are shown.}
\label{fig:refCirc}
\end{figure}

In the second study, we consider the problem of estimating the frame matrix $U$. In this case, each observation consists of 3 landmarks that were generated from a setup shown in Figure \ref{fig:Vec}. We only consider one explanatory variable, meaning that only one frame vector has to be estimated for each landmark. The true frame vectors for each landmark was set to a vertical unit vector. In the estimation procedure, the frame vectors were initialized with the Euclidean linear regression estimate. In Figure \ref{fig:Vec} is shown the true (red), the initial (green) and the estimated frame (blue) for each landmark. The estimation procedure converges to a good estimate of the true frame. Estimation of the initial frame was considered for different number of observations, but the estimated frame did not seem to converge for increasing number of observations. The difference in the parameter estimates might therefore be a result of either the linear approximation of the likelihood or that the optimal solution of the initial frame is not unique.

\begin{figure}
\centering
\includegraphics[scale = 0.5, trim = 20 20 20 20,clip]{estimatedVectors.pdf}
\caption{Comparison of the estimated (blue), initial (green) and true frame vectors (red).}
\label{fig:Vec}
\end{figure}

\section{Data Example}
\label{sec:datEx}

We now apply the model to a real dataset consisting of landmark representations of Corpus Callosum (CC) shapes. The model is used to describe the effect of age on CC shapes. The manifold considered is the same as that introduced in Section \ref{sec:sim}, but in this case $\sigma = 0.1$. Again the Levi-Civita connection is used.

 A subset of the CC dataset is plotted in Figure \ref{fig:CCFrame}. For model fitting, a dataset of $20$ CC shapes was considered with age values ranging from $22$ to $78$. The model was fitted to CC shapes represented by a subset of $20$ landmarks. We did not incorporate a drift term in the model, and only the frame and $\tilde{W}$ has been estimated. The refrence point was set to the mean shape (Figure \ref{fig:CCFrame}) and $\tau = 0.1$.

 The estimated frame for the $20$ landmarks are shown in Figure \ref{fig:CCFrame} on top of the mean shape. The weight matrix was estimated as $\tilde{W} = -0.0002$. Given the low estimate of $\tilde{W}$ and hence a small frame matrix $W$, the result of this experiment suggests a low age effect on CC for these data.  
% Examples of sampled Corpus Callosum shapes predicted from the model is shown in Figure \textcolor{red}{???}.

\begin{figure}
\centering
\begin{minipage}{0.5\textwidth}
    \includegraphics[scale = 0.4, trim = 15 10 10 10, clip]{DataCCfull.pdf}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
    \includegraphics[scale = 0.4, trim = 15 10 10 10, clip]{CCFrameOnlyEst20full.pdf}
\end{minipage}
\caption{(left) A subset of the Corpus Callosum data. (right) The mean shape with the estimated frame for the $20$ landmarks used in the model fitting.}
\label{fig:CCFrame}
\end{figure}

%In Section \ref{simu}, we applied the model to a simulated data set in order to check the reliability of the model. In this section, we will apply the model to a real data set consisting of landmark representations of corpus callosum shapes. The problem we will examine is how age is related to the shape of the corpus callosum.

% Each corpus callosum consists of \textcolor{red}{???} landmarks. A subset of these are plotted in Figure \textcolor{red}{???}. We fitted the model to the data without a drift term. The estimated parameters in this model are
%\begin{align}
%    \hat{W} &= ????, \ \hat{\tau} = ?????
%\end{align}
%In Figure \textcolor{red}{????} are shown the estimated reference shape. In this case it does not seem like the shape of corpus callosum is highly dependent on age. However, based on the estimated parameters we are then able to simulate the shape of corpus callosum for different ages. An example of this is shown in Figure \textcolor{red}{????}.

\section{Discussion}
\label{sec:Dis}

A method was proposed for modeling the relation between a manifold-valued response and Euclidean explanatory variables. The relation was modeled by transport of stochastic paths from $\R^m$ to the manifold. The stochastic paths defined on $\R^m$ was given as solutions to a stochastic differential equation with a contribution from a fixed drift, a stochastic process related to the explanatory variables, and a random noise assumed to follow a multidimensional Brownian motion. The response variable was then modeled as a noisy observation of a stochastic variable following the distribution of the endpoints of the transported process. The proposed model is intrinsic and based on a connection on the manifold without making linearization of the non-linear space. Moreover, a likelihood based estimation procedure were described using Laplace approximation of the marginal likelihood. We experimentally illustrated the model and the parameter estimation using a simulation study and a real data example.
 
Other procedures could be used for estimation of parameters. As an example, the Monte Carlo EM procedure could be used to optimize the complete data likelihood based on simulations of the missing data. Another example is to approximate the distribution of the response by moment matching.
%Furthermore, we applied the method to capture the relation between age and shapes of human Corpora Callosum shapes.

An interesting problem to investigate is how to make variable selection in the model. As the contribution from the explanatory variables is defined in comparison with the frame basis vectors, one idea is to exclude those explanatory variables which corresponds to frame vectors parallel to the curve. These frame vectors will not contribute to the stochastic development and hence will not be important for explaining the relation to the response variable.

 An important assumption of the manifold considered, is that the manifold is equipped with a connection. In this paper, the Levi-Civita connection was used, but several other connections could have been chosen. It would be interesting to explore how the choice of connection affects the model.

As it is possible to transport stochastic paths from a manifold to a Euclidean space, the model could be generalized to handle situations in which a Euclidean response variable is compared to manifold-valued explanatory variables. Based on such a model, one might be able to make categorization of individuals based on manifold-valued shapes.

%\begin{itemize}
%\item[-] Hvad gjorde vi?
%\item[-] Hvad kunne v\ae re sp\ae ndende at g\o re nu?
%\item[-] Interessant om man kan lave model selektion, ved at fjerne frames der er parallelle med flytte retningen!
%\item[-] Situationen med response p\aa\ euklidisk rum og covariate p\aa\ mangfoldighed!
%\end{itemize}

%\newpage
%\section{Bibliography}

\bibliographystyle{plain}
\input{Regression.bbl}
%\bibliography{Line}

\end{document}

