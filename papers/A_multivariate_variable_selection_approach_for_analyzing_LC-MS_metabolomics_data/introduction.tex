Metabolomics aims to provide a global snapshot (quantitative or
qualitative) of the metabolism at a given time and by extension, a
phenotypic information, see \cite{Nicholson1999}. 
To this end, it studies the concentration in small molecules called metabolites
that are the end products of the enzymatic machinery of the cell.
Indeed, minor variations in gene or protein expression levels that are
not observable via high throughput experiments may have an influence
on the metabolites and hence on the phenotype of interest. Thus,
metabolomics is a promising approach that can advantageously
complement usual transcriptomic and proteomic analyses.

In metabolomics, the analysis  of the biological samples is often performed
using High Resolution Mass Spectrometry (HRMS), Nuclear Magnetic Resonance (NMR)  or
Liquid  Chromatography-Mass Spectrometry  (LC-MS)   
and produces a large number of features (hundreds or thousands) 
that can explain a difference between two or more
populations, see \cite{Zhang2012}. It is well-known that the identification
of metabolites discriminating these populations remains a major bottleneck in metabolomics and therefore 
the selection of relevant features (variables) is a crucial step in the
statistical analysis of the metabolomic data, as explained in \cite{Verdegem2016}.

Different supervised machine learning approaches have been  used in
metabolomics during the last few years, see \cite{Saccenti2013,
  Ren2015, Boccard201618}. Among them the most widely used is 
the partial least squares-discriminant analysis (PLS-DA) which has recently been extended to sPLS-DA (sparse partial least squares-discriminant analysis)
by \cite{LeCao2011} to include a variable selection step. 
Nevertheless, \cite{Grissa2016}
 highlight the need for new development in the process of
 features selection that would take into account the specificity of
 metabolomics data which is the dependence that may exist between the
 different metabolites. In this perspective, our paper proposes a novel feature selection
 methodology which consists in a
 variable selection approach based on the Lasso criterion in a
 multivariate setting taking into account the dependence that may exist between the
 different metabolites.


More precisely, let us consider a classical metabolomics experiment
where $n$ samples have been collected and analyzed. This results in an $n \times q$ data matrix where $q$ stands for the number of metabolites.
When the $n$ samples have been obtained under various conditions, we
are typically interested in understanding the effect of each condition
on each metabolite. In the case where $C$ experimental conditions are
compared, $n_c$ denotes the number of replicates under condition $c$, where
$c \in \{1, \dots C\}$ and $\sum_{c=1}^Cn_c = n$. 
We further denote $Y_{c, r}^{(j)}$ the centered LC-MS signal obtained
for the $j$th metabolite ($j \in \{1, \dots q\}$) under Condition $c$ 
for Replicate $r$ ($r \in \{1, \dots n_c\}$). 
In the following, the set of conditions will be called the
``factor'', each specific condition being a ``level'' of this factor. 
The most popular model to analyze quantitative observations $Y$ as a
function of a qualitative variable, that is a factor,
is the analysis of variance (ANOVA) model, which we write here as follows:
\begin{equation}\label{eq:model:anova:simple}
 Y_{c, r}^{(j)} = \mu_c^{(j)} + E_{c, r}^{(j)},
\end{equation}
where the observations $\{Y_{c, r}^{(j)}\}$ are assumed to be centered, so that $\mu_c^{(j)}$ can be interpreted as the effect of Condition $c$ (Level $c$) on
Metabolite $j$ and where the residual terms $\{E_{c, r}^{(j)}\}$ are
assumed to be independent and identically distributed (i.i.d.)
zero-mean Gaussian random variables. 
The goal of such a modeling is to highlight which effects among the
$\mu_1^{(j)},\mu_2^{(j)},\dots,\mu_C^{(j)}$ are the most significant
for the metabolite $j$ since the $\{Y_{c, r}^{(j)}\}$ are assumed to be centered.

When the whole $n \times q$ data matrix is considered instead of a
  single column $j$, 
the model can be summarized in the following matrix form:
\begin{equation}\label{eq:model:matriciel}
\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{B}+\boldsymbol{E},
\end{equation}
where $\boldsymbol{Y}=(Y_{i,j})_{1\leq i\leq n,\; 1\leq j\leq q}$ is
the $n\times q$ observation matrix, $\boldsymbol{X}$ is the $n\times p$ design matrix,
$\boldsymbol{B}$ is the $p\times q$ coefficient matrix and
 $\boldsymbol{E}=(E_{i,j})_{1\leq i\leq n,\; 1\leq j\leq q}$ is the
 $n\times q$ matrix of residual errors. Observe that $p$
 corresponds to the number of explicative variables,
   which is simply $C$ in Model~\eqref{eq:model:anova:simple}.
For notational simplicity, the indices $c,r$ in $Y_{c,r}^{(j)}$ are
summarized in a unique index $i$ in $\{1, \dots n\}$.

In this paper, we pay a special attention to the potential dependence that may exist
among the columns of $\boldsymbol{Y}$, namely the different metabolites. To this aim, we shall assume that
for each $i$ in $\{1,\dots,n\}$,
\begin{equation}\label{eq:def_E}
(E_{i,1},\dots,E_{i,q})\sim\mathcal{N}(0,\boldsymbol{\Sigma}_q),
\end{equation}
where $\boldsymbol{\Sigma}_q$ denotes the covariance matrix of the
$i$th row of the residual error matrix. Note that the model defined
by (\ref{eq:model:matriciel}) and (\ref{eq:def_E}) is usually called 
a general linear model or a multivariate linear model which 
has been extensively studied in \cite{mardia:kent:1979}.


The simplest assumption regarding the dependence structure of the
noise is $\boldsymbol{\Sigma}_q=\sigma^2\boldsymbol{I}_q$, where $\boldsymbol{I}_q$
denotes the $q\times q$ identity matrix. In this case the different columns of $\boldsymbol{Y}$ are assumed to be independent.
In more general cases, the matrix
$\boldsymbol{\Sigma}_q$ models the dependence between the different
columns of $\boldsymbol{Y}$, namely the dependence between the metabolites.
In the following, we shall moreover assume that
$(E_{i,1},\ldots,E_{i,q})$ and $(E_{k,1},\ldots,E_{k,q})$ are independent, when $i\neq k$, which
means that the individuals are assumed to be independent.

The problem of finding which parameters are significant among the $(\mu_c^{(j)})_{1\leq
  c\leq C,1\leq j\leq q}$  in Model~(\ref{eq:model:anova:simple})
boils down to finding the non null coefficients in the matrix
$\boldsymbol{B}$ in Model (\ref{eq:model:matriciel}) and hence can be seen as a variable selection problem
in the general linear model.
Several approaches can be considered for solving this issue: either 
\textit{a posteriori} methods such as classical statistical tests in ANOVA
models, see \cite{mardia:kent:1979,faraway:2005}
or methods embedding the variable selection such as Lasso-type
methodologies initially proposed by \cite{Tib96}. However, a raw application of such approaches does not 
take into account the potential dependence between the
different columns of $\boldsymbol{Y}$. This drawback will be illustrated in Section
\ref{sec:num_exp}.


The goal of our paper is twofold: First, to remedy the limitations of these approaches
by proposing a method for estimating the dependence between the columns
of $\boldsymbol{Y}$ and second, to deal with the potentially high number of
variables by using a Lasso-type approach taking into
  account this dependence. For this purpose, we shall propose a three-step inference
strategy further detailed hereafter.

The paper is organized as follows. Our method is described in Section \ref{sec:stat_inf}.
To support our methodology, some numerical experiments on synthetic data are provided in Section \ref{sec:num_exp}.
Finally, an application to a metabolomics data set produced by the analysis of African
copals samples is given in Section \ref{sec:real}. 


%%% Local Variables:
%%% mode: latex
%%% eval: (TeX-PDF-mode 1)
%%% TeX-master: "perrot_levy_chiquet_arxiv.tex"
%%% ispell-local-dictionary: "en_US"
%%% eval: (flyspell-mode 1)
%%% End: 