The strategy that we propose can be summarized as follows.
\begin{itemize}
\item \textsf{First step:} Fitting a one-way ANOVA to each column of the observation matrix 
  $\boldsymbol{Y}$ in order to have access to an
  estimation $\widehat{\boldsymbol{E}}$ of the residual matrix
  $\boldsymbol{E}$.
\item \textsf{Second step:} Estimating the matrix
  $\boldsymbol{\Sigma}_q$ by using the methods described in Sections
  \ref{subsec:param} and \ref{subsec:nonparam}. Then, choosing the
  most convenient estimator $\widehat{\boldsymbol{\Sigma}}_q$
  thanks to a statistical test described in Section \ref{sec:whitening_test}.
\item \textsf{Third step:} Thanks to the estimator $\widehat{\boldsymbol{\Sigma}}_q$, transforming the
  data in order to remove the dependence between the columns of
  $\boldsymbol{Y}$. Such a transformation will be called ``whitening'' hereafter.
  Then, applying to these
  transformed observations the Lasso approach described in Section \ref{sec:lasso}.
\end{itemize}

The first step provides  a first estimate $\widetilde{\boldsymbol{B}}$
of $\boldsymbol{B}$. An estimate of the residual  matrix is then
defined                                                             as
$\widehat{\boldsymbol{E}}    =    \boldsymbol{Y}   -    \boldsymbol{X}
\widetilde{\boldsymbol{B}}$.
In the following, we shall focus on the last two steps.



\subsection{Estimation of the dependence structure of $\boldsymbol{E}$}\label{sec:estim_sigma_q}


We propose hereafter to model each row of $\boldsymbol{E}$ as realizations 
of a stationary process and hence we shall use time-series models in order
to  describe the  dependence structure of  $\boldsymbol{E}$. We refer the reader to \cite{brockwell:davis} for further
details on time series modeling.   

We  shall consider  hereafter  a  large variety  of  models
ranging from the simplest parametric to the most general nonparametric
dependence  modeling.  In  each case  we  focus on  the estimation  of
$\boldsymbol{\Sigma}_q^{-1/2}$     since    using     the    following
transformation:
\begin{equation}\label{eq:modele:blanchi_est}
\boldsymbol{Y}\;\boldsymbol{\Sigma}_q^{-1/2}=\boldsymbol{X}\boldsymbol{B}\;\boldsymbol{\Sigma}_q^{-1/2}
+\boldsymbol{E}\;\boldsymbol{\Sigma}_q^{-1/2}
\end{equation}
removes the dependence between the columns of $\boldsymbol{Y}$. Such a procedure will be called ``whitening'' hereafter.



\subsubsection{Parametric dependence}\label{subsec:param}
The simplest model among the parametric models is the autoregressive process
of order 1 denoted AR(1). More precisely, for each $i$ in
$\{1,\dots,n\}$, $E_{i,t}$ satisfies the following equation:
\begin{equation}\label{eq:AR1}
E_{i,t}-\phi_1 E_{i,t-1}=W_{i,t},\textrm{ with } W_{i,t}\sim WN(0,\sigma^2),
\end{equation}
where $\phi_1$ is a real number and $WN(0,\sigma^2)$ denotes a zero-mean white noise process of variance $\sigma^2$, namely, if $Z_{t}~\sim~WN(0,\sigma^2)$, then
$\PE(Z_t)=0$, $\PE(Z_t Z_{t'})=0$ if $t\neq t'$ and $\PE(Z_t^2)=\sigma^2$.

In this case, the inverse of the square root of the covariance matrix
$\boldsymbol{\Sigma}_q$ of $(E_{i,1},\dots,E_{i,q})$ has a simple closed-form 
expression given by
\begin{equation}\label{eq:Sigmaq-1/2}
\boldsymbol{\Sigma}_q^{-1/2} =\left(
\begin{matrix}
\sqrt{1-\phi_1^2} & -\phi_1 & 0 & \cdots & 0\\
0 & 1 & -\phi_1 & \cdots & 0 \\
0 & 0 & \ddots & \ddots & \vdots \\
\vdots & \vdots &  \ddots & \ddots & -\phi_1 \\
%0 & 0 & \cdots &  1 &  & -\phi_1\\
0 & 0 & \cdots  & 0 & 1 &
\end{matrix}
\right).
\end{equation}
Hence, to obtain the expression of $\widehat{\boldsymbol{\Sigma}}_q^{-1/2}$, it is enough to have an estimation of the parameter $\phi_1$
and to replace it in (\ref{eq:Sigmaq-1/2}). For this, we use the
estimator $\widehat{\boldsymbol{E}}$ of the residual errors matrix
obtained by fitting a standard ANOVA model to the observations, which
corresponds to the first step of our method. Then
$\phi_1$ is estimated by $\widehat{\phi}_1$  defined by
$$
\widehat{\phi}_1=\frac1n\sum_{i=1}^n \widehat{\phi}_{1,i},
$$
where $\widehat{\phi}_{1,i}$ denotes the estimator of $\phi_1$ obtained by the classical Yule-Walker equations from 
$(\widehat{E}_{i,1},\dots,\widehat{E}_{i,q})$, see \cite{brockwell:davis} for more
details. 

More generally, it is also possible to have access to
$\boldsymbol{\Sigma}_q^{-1/2}$ for more complex processes such as the ARMA($p,q$) process defined
as follows: For each $i$ in $\{1,\dots,n\}$,
\begin{equation}\label{eq:ARMA}
E_{i,t}-\phi_1 E_{i,t-1}-\dots-\phi_p E_{i,t-p} =W_{i,t}+\theta_1
W_{i,t-1}+\dots\theta_q W_{i,t-q},
\end{equation}
where $W_{i,t}\sim WN(0,\sigma^2)$, the $\phi_i$'s and the $\theta_i$'s are real numbers.

\subsubsection{Nonparametric dependence case}\label{subsec:nonparam}

In the situation where a parametric modeling is not relevant for
$\boldsymbol{\Sigma}_q$, it can be estimated by
\begin{equation}
\widehat{\boldsymbol{\Sigma}}_q=\left(
\begin{matrix}
\widehat{\gamma}(0) & \widehat{\gamma}(1) & \cdots & \widehat{\gamma}(q-1)\\
\widehat{\gamma}(1) & \widehat{\gamma}(0) & \cdots & \widehat{\gamma}(q-2)\\
\vdots & & & \\
\widehat{\gamma}(q-1) & \widehat{\gamma}(q-2) & \cdots & \widehat{\gamma}(0)
\end{matrix}
\right),
\end{equation}
with
$$
\widehat{\gamma}(h)=\frac1n\sum_{i=1}^n \widehat{\gamma}_{i}(h),
$$
where $\widehat{\gamma}_{i}(h)$ is the standard autocovariance
estimator of $\gamma_i(h)=\PE(E_{i,t}E_{i,t+h})$, for all $t$. Usually, $\widehat{\gamma}_{i}(h)$ is 
referred to as the empirical autocovariance of the
$\widehat{E}_{i,t}$'s at lag $h$  (\textit{i.e.}\,the
  empirical covariance between 
$(\widehat{E}_{i,1}, \dots ,\widehat{E}_{i,n-h})$ and $(\widehat{E}_{i,h+1}, \dots ,\widehat{E}_{i,n})$).
For a definition of the standard autocovariance estimator we
refer the reader to Chapter 7 of \cite{brockwell:davis}.
The matrix $\widehat{\boldsymbol{\Sigma}}_q^{-1/2}$ is then obtained by inverting the Cholesky factor of $\widehat{\boldsymbol{\Sigma}}_q$.


\subsubsection{Choice of the whitening modeling}\label{sec:whitening_test}

In order to decide which dependence modeling is the most adapted to
the data at hand we propose hereafter a statistical test.
If the whitening modeling used is well chosen then each row of 
$\widetilde{\boldsymbol{E}}=\widehat{\boldsymbol{E}}\widehat{\boldsymbol{\Sigma}}_q^{-1/2}$
should be a white noise. 

One of the most popular approach for testing whether a random process
is a white noise is the Portmanteau test which is based on the
Bartlett theorem, for further details we refer the reader to
\cite[Theorem 7.2.2]{brockwell:davis}. By this theorem, we get that
under the null hypothesis $(H_0)$: ``For each $i$ in $\{1,\dots,n\}$,
$(\widetilde{E}_{i,1},\ldots,\widetilde{E}_{i,q})$ is a white noise'',
\begin{equation}\label{eq:stat_test_1}
q\sum_{h=1}^H \widehat{\rho}_i(h)^2\approx \chi^2(H),\textrm{ as }
q\to\infty,
\end{equation}
for each $i$ in $\{1,\dots,n\}$,
where $\widehat{\rho}_i(h)$ denotes the empirical autocorrelation of
$(\widetilde{E}_{i,t})_t$ at lag $h$ and $\chi^2(H)$ denotes the
chi-squared distribution with $H$ degrees of freedom. Thus, by (\ref{eq:stat_test_1}),
we have at our disposal a $p$-value for each $i$ in $\{1,\dots,n\}$
that we denote by $\textrm{Pval}_i$.
In order to have a single $p$-value instead of $n$, we shall consider 
\begin{equation}\label{eq:stat_test_2}
q\sum_{i=1}^n\sum_{h=1}^H \widehat{\rho}_i(h)^2\approx \chi^2(nH),\textrm{ as }
q\to\infty,
\end{equation}
where the approximation comes from the fact that the rows of $\widetilde{\boldsymbol{E}}$
are assumed to be independent. Equation (\ref{eq:stat_test_2}) thus provides a $p$-value: $\textrm{Pval}$.
Hence, if $\textrm{Pval}<\alpha$, the null hypothesis $(H_0)$ is
rejected at the level $\alpha$, where $\alpha$ is usually equal to 5\%. In such a situation taking the dependence into account and estimating the dependence
by one of the previous methods should highly improve the modeling and the variable selection step.

\subsection{Estimation of $\boldsymbol{B}$}\label{sec:lasso}

\subsubsection{Lasso based approach}

Let us first explain briefly the usual framework in which the Lasso
approach is used. We consider a high-dimensional linear model of the following form
\begin{equation}\label{eq:model_vec}
\mathcal{Y}=\mathcal{X}\mathcal{B}+\mathcal{E},
\end{equation}
where $\mathcal{Y}$, $\mathcal{B}$ and $\mathcal{E}$ are vectors. Note that, in
high-dimensional linear models, the matrix $\mathcal{X}$ has usually more columns than rows which means that
the number of variables is larger than the number of observations but
$\mathcal{B}$ is usually a sparse vector, namely it contains a lot of
null components.

In such models a very popular approach initially proposed by \cite{Tib96} consists in using the Least Absolute
Shrinkage eStimatOr (LASSO) criterion for estimating $\mathcal{B}$ defined as follows for a positive $\lambda$:
\begin{equation}\label{eq:lasso}
\widehat{\mathcal{B}}(\lambda)=\textrm{Argmin}_\mathcal{B}\left\{\|\mathcal{Y}-\mathcal{X}\mathcal{B}\|_2^2+\lambda\|\mathcal{B}\|_1\right\},
\end{equation}
where, for $u=(u_1,\dots,u_n)$, $\|u\|_2^2=\sum_{i=1}^n u_i^2$ and $\|u\|_1=\sum_{i=1}^n |u_i|$, which is usually called the $\ell_1$-norm of the vector $u$. Observe that the first term of (\ref{eq:lasso})
is the classical least-squares criterion and that $\lambda\|\mathcal{B}\|_1$ can be seen as a penalty term. The interest of such
a criterion is the sparsity enforcing property of the  $\ell_1$-norm ensuring that the number of non-zero components of the estimator 
$\widehat{\mathcal{B}}$ of $\mathcal{B}$ is small for large enough values
of $\lambda$.
Such a criterion is very relevant in our framework since the problem of finding the significant variables
boils down to finding the non null coefficients in the matrix
$\boldsymbol{B}$.

This methodology cannot be directly applied to our model since we have
to deal with  matrices and not with vectors. However,  as explained in
Appendix A,  Model (\ref{eq:model:matriciel})  can be rewritten  as in
\eqref{eq:model_vec}    where    $\mathcal{Y}$,   $\mathcal{B}$    and
$\mathcal{E}$   are  vectors   of  size   $nq$,  $pq$   and  $nq$,
respectively.  Hence,  retrieving  the   positions  of  the  non  null
components in $\mathcal{B}$  is a first approach  for finding relevant
variables.

However, this approach does not take into account the dependence between the columns of 
$\boldsymbol{Y}$. Hence,
we propose hereafter a modified version of the standard Lasso criterion (\ref{eq:lasso}) taking into account this potential dependence.

As explained previously, our contribution consists first in ``whitening'' the observations, namely removing the dependence that may exist within the observations
matrix, by multiplying (\ref{eq:model:matriciel}) on the right by
$\widehat{\boldsymbol{\Sigma}}_q^{-1/2}$, see
(\ref{eq:modele:blanchi_est}) where $\boldsymbol{\Sigma}_q^{-1/2}$ is
replaced by $\widehat{\boldsymbol{\Sigma}}_q^{-1/2}$. 
By using the same vectorization trick that allows us to transform Model (\ref{eq:model:matriciel}) into Model (\ref{eq:model_vec}),
the Lasso criterion can be applied to the vectorized version of Model
(\ref{eq:modele:blanchi_est}) where $\boldsymbol{\Sigma}_q^{-1/2}$ is
replaced by $\widehat{\boldsymbol{\Sigma}}_q^{-1/2}$.
The specific expressions of $\mathcal{Y}$, $\mathcal{X}$, $\mathcal{B}$ and $\mathcal{E}$ are given in Appendix B.

Note that this idea of ``whitening'' the observations
has also been proposed by \cite{rothman:2010} in which the estimation of $\boldsymbol{\Sigma}_q$ and $\boldsymbol{B}$
is performed simultaneously. An implementation is available in the R package \textsf{MRCE}.
In our approach, $\boldsymbol{\Sigma_q}$ is estimated first and then
its estimator is used in (\ref{eq:modele:blanchi_est}) instead of 
$\boldsymbol{\Sigma}_q$ before applying the Lasso criterion. Hence, our method can be seen as a variant of the MRCE method
in which $\boldsymbol{\Sigma}_q$ is estimated beforehand. Moreover,
after some numerical experiments, we observed that for the values of
$n$ and $q$ that we aim at using, the computational
  burden of the approach designed by \cite{rothman:2010} is too high
  for addressing our datasets, contrary to ours. Hence, in the following, we shall
not consider the method of \cite{rothman:2010} anymore. 

\subsubsection{Model selection issue}\label{sec:model_selection}

We can see that the estimator defined in (\ref{eq:lasso})
depends on a parameter $\lambda$ which tunes the sparsity level in $\widehat{\mathcal{B}}$. We propose to mix two standard
approaches to estimate the positions of the non null components in $\mathcal{B}$. 

We first use the  10-fold cross-validation method to choose
the $\lambda$ denoted $\lambda_{\textrm{CV}}$ minimizing the
cross-validation criterion.
This $\lambda$ is then used in the stability selection approach of
\cite{meinshausen:buhlmann:2010} which guarantees the robustness of the selected variables. This latter approach
can be described as follows.
The vector of observations $\mathcal{Y}$ is randomly split into
several subsamples of size $nq/2$ which is possible
  thanks to the whitening step.
For each subsample, the LASSO criterion is applied with $\lambda=\lambda_{\textrm{CV}}$
and the indices $i$ of the non null $\widehat{\mathcal{B}}_i$ are stored. 
Then, for a given threshold, we keep in the final
set of selected variables only the variables appearing a number of times larger than this threshold. 
In practice, we generated $5000$ subsamples of
$\mathcal{Y}$. 

Concerning the choice of the final threshold: we
propose either to take the one leading to the largest $p$-value of the
whitening test described in (\ref{eq:stat_test_2}) or the threshold 1. As we
shall see in Section \ref{sec:num_exp}, with
the first choice, mostly all the positions of the non null variables
in $\mathcal{B}$ are retrieved with some false positive. With the
second choice, all the true positions are
not recovered but there are no false positive. Moreover, the second choice guarantees a stability of the selected variables since
only the variables which are chosen at each of the 5000 subsamplings of the data are finally kept.



%%% Local Variables:
%%% mode: latex
%%% eval: (TeX-PDF-mode 1)
%%% TeX-master: "perrot_levy_chiquet_arxiv.tex"
%%% ispell-local-dictionary: "en_US"
%%% eval: (flyspell-mode 1)
%%% End: 