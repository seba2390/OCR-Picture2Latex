\section{Introduction}
Visual scene understanding has traditionally focused on identifying \emph{objects in images} -- learning to predict their presence (\ie~image classification \cite{krizhevsky2012imagenet, szegedy2015going, he2016deep}) and spatial extent (\ie~object detection \cite{girshick2014rich, redmon2016you, liu2016ssd} or segmentation \cite{lin2017feature}). These object-centric techniques have matured significantly in recent years, however, representing scenes as collections of objects fails to capture relationships which may be essential for scene understanding.

A recent work \cite{Johnson2015CVPR} has instead proposed representing visual scenes as graphs containing objects, their attributes, and the relationships between them. These \emph{scene graphs} form an interpretable structured representation of the image that can support higher-level visual intelligence tasks such as captioning \cite{wu2017image, Lu2018Neural}, visual question answering \cite{antol2015vqa, teney2016graph, wu2017image, wang2017vqa, wang2017fvqa, johnson2017clevr}, and image-grounded dialog \cite{das2016visual}. While scene graph representations hold tremendous promise, extracting scene graphs from images -- efficiently and accurately -- is challenging. The natural approach of considering every pair of nodes (objects) as a potential edge (relationship) -- essentially reasoning over fully-connected graphs -- is often effective in modeling contextual relationships but scales poorly (quadratically) with the number of objects, quickly becoming impractical. 
The naive fix of randomly sub-sampling edges to be considered is more efficient but not as effective since the distribution of interactions between objects is far from random -- take Fig.~\ref{fig:teaser}(a) as an example, it is much more likely for a `car' and `wheel' to have a relationship than a `wheel' and `building'. Furthermore, the types of relationships that typically occur between objects are also highly dependent on those objects. 

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=1\textwidth, trim=0 0.5cm 0 1cm, clip]{figures/Fig1.pdf}
  \end{center}
\caption{Given an image (a), our proposed approach first extracts a set of objects visible in the scene and considers possible relationships between all nodes (b). Then it prunes unlikely relationships using a learned measure of `relatedness', producing a sparser candidate graph structure (c). Finally, an attentional graph convolution network is applied to integrate global context and update object node and relationship edge labels.}
\label{fig:teaser}
\end{figure}

\xhdr{Graph R-CNN.} In this work, we propose a new framework, Graph R-CNN, for scene graph generation which effectively leverages object-relationship regularities through two mechanisms to intelligently sparsify and reason over candidate scene graphs. Our model can be factorized into three logical stages: 1) object node extraction, 2) relationship edge pruning, and 3) graph context integration, which are depicted in Fig.~\ref{fig:teaser}.
% 
In the object node extraction stage, we utilize a standard object detection pipeline \cite{ren2015faster}. This results in a set of localized object regions as shown in Fig.~\ref{fig:teaser}b.
%
We introduce two important novelties in the rest of the pipeline to incorporate the real-world regularities in object relationships discussed above.
%
First, we introduce a relation proposal network (RePN) that learns to efficiently compute \emph{relatedness scores} between object pairs which are used to intelligently prune unlikely scene graph connections (as opposed to random pruning in prior work). A sparse post-pruning graph is shown in Fig.~\ref{fig:teaser}c.
%
Second, given the resulting sparsely connected scene graph candidate, we apply an attentional graph convolution network (aGCN) to propagate higher-order context throughout the graph -- updating each object and relationship representation based on its neighbors. In contrast to existing work, we predict per-node edge attentions, enabling our approach to learn to modulate information flow across unreliable or unlikely edges. We show refined graph labels and edge attentions (proportional to edge width) in Fig.~\ref{fig:teaser}d.

To validate our approach, we compare our performance with existing methods on the Visual Genome \cite{krishna2017visual} dataset and find that our approach achieves an absolute gain of 5.0 on Recall@50 for scene graph generation \cite{xu2017scene}. We also perform extensive model ablations and quantify the impact of our modeling choices.

\xhdr{Evaluating Scene Graph Generation.} %
%
Existing metrics for scene graph generation are based on recall of 
$\langle$subject, predicate, object$\rangle$ triplets (e.g.~\texttt{SGGen} from \cite{krishna2017visual}) 
or of objects and predicates given ground truth object localizations (e.g.~\texttt{PredCls} and \texttt{PhrCls} from \cite{krishna2017visual}). 
In order to expose a problem with these metrics, consider a method that mistakes the boy in Fig.~\ref{fig:teaser}a as a man but 
otherwise identifies that he is 1) standing behind a fire hydrant, 
2) near a car, and 
3) wearing a sweater. Under the triplet-based metrics, 
this minor error (boy vs man) would be heavily penalized despite most of the boy's relationships being correctly identified. 
Metrics that provide ground-truth regions side-step this problem by focusing strictly on relationship prediction 
but cannot accurately reflect the test-time performance of the entire scene graph generation system. 

To address this mismatch, we introduce a novel evaluation metric (\texttt{SGGen+}) that more holistically evaluates 
the performance of scene graph generation with respect to objects, attributes (if any), and relationships. 
Our proposed metric \texttt{SGGen+} computes the total recall for singleton entities (objects and predicates), 
pair entries $\langle$object, attribute$\rangle$ (if any), and triplet entities $\langle$subject, predicate, object$\rangle$. 
We report results on existing methods under this new metric and find our approach also outperforms the state-of-the-art significantly. 
More importantly, this new metric provides a more robust and holistic measure of similarity between generated and ground-truth scene graphs.

\xhdr{Summary of Contributions.} Concretely, this work addresses the scene graph generation problem by introducing 
a novel model (Graph R-CNN), which can leverage object-relationship regularities, 
and proposes a more holistic evaluation metric (\texttt{SGGen+}) 
for scene graph generation. 
We benchmark our model against existing approaches on standard metrics and this new measure -- outperforming existing approaches.