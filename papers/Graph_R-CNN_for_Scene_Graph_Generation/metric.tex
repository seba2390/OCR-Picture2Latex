\section{Evaluating Scene Graph Generation}

Scene graph generation is naturally a structured prediction problem over attributed graphs, and how to correctly and efficiently evaluate predictions is an under-examined problem in prior work on scene graph generation. We note that graph similarity based on minimum graph edit distance has been well-studied in graph theory \cite{gao2010survey}; however, computing exact solution is NP-complete and approximation APX-hard \cite{lin1994hardness}.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=1\textwidth]{figures/Fig3.pdf}
  \end{center}
 \vspace{-3mm}
\caption{A example to demonstrate the difference between \texttt{SGGen} and \texttt{SGGen+}. Given the input image (a), its ground truth scene graph is depicted in (b). (c)-(e) are three generated scene graphs. For clarity, we merely show the connections with \textit{boy}. At the bottom of each graph, we compare the number of correct predictions for two metrics.}
\label{fig:metric}
 \vspace{-1mm}
\end{figure}

Prior work has circumvented these issues by evaluating scene graph generation under a simple triplet-recall based metric introduced in \cite{xu2017scene}. Under this metric which we will refer to as \texttt{SGGen}, the ground truth scene graph is represented as a set of $\langle \mathtt{object, relationship, subject} \rangle$ triplets and recall is computed via exact match. That is to say, a triplet is considered `matched' in a generated scene graph if all three elements have been correctly labeled, and both \texttt{object} and \texttt{subject} nodes have been properly localized (\ie, bounding box IoU $>$ 0.5). While simple to compute, this metric results in some unintuitive notions of similarity that we demonstrate in Fig.~\ref{fig:metric}.

Fig.~\ref{fig:metric}a shows an input image overlaid with bounding box localizations of correspondingly colored nodes in the ground truth scene graph shown in (b). (c), (d), and (e) present erroneously labeled scene graphs corresponding to these same localizations. Even a casual examination of (c) and (d) yields the stark difference in their accuracy -- while (d) has merely mislabeled the boy as a man, (c) has failed to accurately predict even a single node or relationship! Despite these differences, neither recalls a single complete triplet and are both scored identically under \texttt{SGGen}  (\ie, 0).
%

To address this issue, we propose a new metric called \texttt{SGGen+} as the augmentation of \texttt{SGGen}. \texttt{SGGen+} not only considers the triplets in the graph, but also the singletons (object and predicate). The computation of \texttt{SGGen+} can be formulated as:
\begin{equation}
Recall = \frac{C(O) + C(P) + C(T)}{N}
\end{equation}
where $C(\cdot)$ is a counting operation, and hence $C(O)$ is the number of object nodes correctly localized and recognized; $C(P)$ is for predicate. Since the location of predicate depends on the location of subject and object, only if both subject and object are correctly localized and the predicate is correctly recognized, we will count it as one. $C(T)$ is for triplet, which is the same as \texttt{SGGen}. Here, $N$ is the number of entries (the sum of number of objects, predicates and relationships) in the ground truth graph. In Fig.~\ref{fig:metric}, using our \texttt{SGGen+}, the recall for graph (c) is still 0, since all predictions are wrong. However, the recall for graph (d) is not 0 anymore since most of the object and all predicate predictions are correct, except for one wrong prediction for the red node. Based on our new metric, we can obtain a much comprehensive measurement of scene graph similarity.