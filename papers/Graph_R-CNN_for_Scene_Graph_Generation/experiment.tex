\section{Experiments}
Recently, there are some inconsistencies in existing work on scene graph generation in terms of data preprocessing, data split, and evaluation. This makes it difficult to systematically benchmark progress and cleanly compare numbers across papers. So we first clarify the details of our experimental settings.

\textbf{Datasets}. There are a number of splits of the Visual Genome dataset that have been used in the scene graph generation literature \cite{xu2017scene,li2017scene,zhang2017relationship}. The most commonly used is the one proposed in \cite{xu2017scene}. Hence, in our experiments, we follow their preprocessing strategy and dataset split. After preprocessing, the dataset is split into training and test sets, which contains 75,651 images and 32,422 images, respectively. In this dataset, the top-frequent 150 object classes and 50 relation classes are selected. Each image has around 11.5 objects and 6.2 relationships in the scene graph.

\textbf{Training}. For training, multiple strategies have been used in literature. In \cite{xu2017scene,li2017scene,newell2017pixels}, the authors used two-stage training, where the object detector is pre-trained, followed by the joint training of the whole scene graph generation model. To be consistent with previous work \cite{xu2017scene,li2017scene}, 
we also adopt the two-stage training -- 
we first train the object detector and then train the whole model jointly until convergence. 

\textbf{Metrics}. We use four metrics for evaluating scene graph generation, including three previously used metrics and our proposed \texttt{SGGen+} metric:
\begin{itemize}[noitemsep]
\setlength\itemsep{0em}
\item \textbf{Predicate Classification (PredCls)}: The performance for recognizing the relation between two objects given the ground truth locations.
\item \textbf{Phrase Classification (PhrCls)}: The performance for recognizing two object categories and their relation given the ground truth locations.
\item \textbf{Scene Graph Generation (SGGen)}: The performance for detecting objects (IoU $>$ 0.5) \emph{and} recognizing the relations between object pairs.
\item \textbf{Comprehensive Scene Graph Generation (SGGen+)}: Besides the triplets counted by \texttt{SGGen}, it considers the singletons and pairs (if any), as described earlier.
\end{itemize}

\textbf{Evaluation}. 
In our experiments, we multiply the classification scores for subjects, objects and their relationships, then sort them in descending order. Based on this order, we compute the recall at top 50 and top 100, respectively. Another difference in existing literature in the evaluation protocol is w.r.t. the \texttt{PhrCls} and \texttt{PredCls} metrics. Some previous works \cite{li2017scene,newell2017pixels} used different models to evaluate along different metrics. However, such a comparison is unfair since the models could be trained to overfit the respective metrics. For meaningful evaluation, we evaluate a single model -- the one obtained after joint training -- across all metrics. 

\subsection{Implementation Details}
We use Faster R-CNN \cite{ren2015faster} associated with VGG16 \cite{simonyan2014very} as the backbone based on the PyTorch re-implementation \cite{jjfaster2rcnn}. During training, the number of proposals from RPN is 256. For each proposal, we perform ROI Align \cite{he2017mask} pooling, to get a $7 \times 7$ response map, which is then fed to a two-layer MLP to obtain each proposal's representation. In RePN, the projection functions $\Phi(\cdot)$ and $\Psi(\cdot)$ are simply two-layer MLPs. During training, we sample 128 object pairs from the quadratic number of candidates. We then obtain the union of boxes of the two objects and extract a representation for the union. The threshold for box-pair NMS is 0.7. In aGCN, to obtain the attention for one node pair, we first project the object/predicate features into 256-d and then concatenate them into 512-d, which is then fed to a two-layer MLP with a 1-d output. For aGCN, we use two aGCN layers at the feature level and semantic level, respectively. The attention on the graph is updated in each aGCN layer at the feature level, which is then fixed and sent to the aGCN at the semantic level.

\textbf{Training}. As mentioned, we perform stage-wise training -- we first pretrain Faster R-CNN for object detection, and then fix the parameters in the backbone to train the scene graph generation model. SGD is used as the optimizer, with initial learning rate 1e-2 for both training stages. 

\subsection{Analysis on New Metric}

\jianwei{We first quantitatively demonstrate the difference between our proposed metric \texttt{SGGen+} and \texttt{SGGen}. We compare them by perturbing ground truth scene graphs. We consider assigning random incorrect labels to objects; perturbing objects 1) without relationships, 2) with relationships, and 3) both. We vary the fraction of nodes which are perturbed among \{20\%, 50\%, 100\%\}. Recall is reported for both metrics. As shown in Table~\ref{Table:Exp_SGGen+}, \texttt{SGGen} is completely insensitive to the perturbation of objects without relationships (staying at 100 consistently) since it only considers relationship triplets. Note that there are on average 50.1\% objects without relationships in the dataset, which \texttt{SGGen} omits. On the other hand, \texttt{SGGen} is overly sensitive to label errors on objects with relationships (reporting 54.1 at only 20\% perturbation where the overall scene graph is still quite accurate). Note that even at 100\% perturbation the object localizations and relationships are still correct such that \texttt{SGGen+} provides a non-zero score, unlike \texttt{SGGen} which considers the graph entirely wrong. Overall, we hope this analysis demonstrates that \texttt{SCGen+} is more comprehensive compared to \texttt{SCGen}.}

\begin{table}[t]
\centering
\resizebox{0.99\columnwidth}{!}{
\setlength{\tabcolsep}{6 pt}
  \begin{tabular}{l c c c c c c c c c c c c c c}
   \toprule
   {Perturb Type} & \multicolumn{1}{c}
{{none}} & \multicolumn{3}{c}{{w/o relationship}} & \multicolumn{3}{c}{{w/ relationship}} & \multicolumn{3}{c}{{both}} \\
\cmidrule(r){2-2}
\cmidrule(r){3-5}
\cmidrule(r){6-8}
\cmidrule(r){9-11}
{Perturb Ratio} & \multicolumn{1}{c}{0\%} & \multicolumn{1}{c}{20\%}  & \multicolumn{1}{c}{50\%} & \multicolumn{1}{c}{100\%} & \multicolumn{1}{c}{20\%} & \multicolumn{1}{c}{50\%} & \multicolumn{1}{c}{100\%} & \multicolumn{1}{c}{20\%} & \multicolumn{1}{c}{50\%} & \multicolumn{1}{c}{100\%}\\
   \cmidrule(r){2-2}
   \cmidrule(r){3-4}
   \cmidrule(r){6-7}
   \cmidrule(r){8-9}
   \cmidrule(r){1-11}
   {SGGen}  & 100.0 & 100.0 & 100.0 & 100.0 & 54.1 & 22.1 & 0.0 & 62.2 & 24.2 & 0.0 \\
   {SGGen+} & 100.0 & 94.5 & 89.1 & 76.8 & 84.3 & 69.6 & 47.9 & 80.1 & 56.6 & 22.8 \\
\bottomrule
\end{tabular}}\\[3pt]
\caption{Comparisons between \texttt{SGGen} and \texttt{SGGen+} under different perturbations.}
\label{Table:Exp_SGGen+}
\end{table}

\subsection{Quantitative Comparison}
\begin{table}[t]
\footnotesize
\setlength{\tabcolsep}{2.5pt} 
\center
\resizebox{0.95\columnwidth}{!}{
  \begin{tabular}{c l c c c c c c c c}
   \toprule
  & \multicolumn{1}{c}{} & \multicolumn{2}{c}{SGGen+}  & \multicolumn{2}{c}{SGGen} & \multicolumn{2}{c}{PhrCls}  & \multicolumn{2}{c}{PredCls} \\    
   %\cmidrule(r){2}
   \cmidrule(r){3-4}
   \cmidrule(r){5-6}
   \cmidrule(r){7-8}
   \cmidrule(r){9-10}
	& Method & R@50 & R@100 & R@50 & R@100 & R@50 & R@100 & R@50 & R@100\\
    %\midrule
    \cmidrule(r){2-10}
	& IMP \cite{xu2017scene} & - & - & 3.4 & 4.2 & 21.7 & 24.4 & 44.8 & 53.0 \\
    & MSDN \cite{li2017scene} & - & -  & 7.7 & 10.5 & 19.3 & 21.8 & 63.1 & 66.4 \\
    & Pixel2Graph \cite{newell2017pixels} & - & - & 9.7 & 11.3 & 26.5 & 30.0 & 68.0 & 75.2 \\
    %\midrule
    \cmidrule(r){2-10}
    & IMP$^\dagger$ \cite{xu2017scene} & 25.6 & 27.7 & 6.4 & 8.0 & 20.6 & 22.4 & 40.8 & 45.2 \\
    & MSDN$^\dagger$ \cite{li2017scene} & 25.8 & 28.2  & 7.0 & 9.1 & 27.6 & 29.9 & 53.2 & 57.9 \\ 
    & NM-Freq$^\dagger$ \cite{zellers2017neural} & 26.4 & 27.8  & 6.9 & 9.1 & 23.8 & 27.2 & 41.8 & 48.8 \\       
	& Graph R-CNN (Us) & \textbf{28.5} & \textbf{35.9} & \textbf{11.4} & \textbf{13.7} & \textbf{29.6} & \textbf{31.6} & \textbf{54.2} & \textbf{59.1} \\ 
\bottomrule
\end{tabular}}
\caption{Comparison on Visual Genome test set \cite{krishna2017visual}. We reimplemented IMP \cite{xu2017scene} and MSDN \cite{li2017scene} using the same object detection backbone for fair comparison.}
\label{Table:Exp_main}
\end{table}

We compare our Graph R-CNN with recent proposed methods, including Iterative Message Passing (IMP) ~\cite{xu2017scene}, Multi-level scene Description Network (MSDN)~\cite{li2017scene}. Furthermore, we evaluate the neural motif frequency baseline proposed in \cite{zellers2017neural}. Note that previous methods often use slightly different pre-training procedures or data split or extra supervisions. For a fair comparison and to control for such orthogonal variations, we reimplemented IMP, MSDN and frequency baseline in our codebase. Then, we re-train IMP and MSDN based on our backbone -- specifically, we used the same pre-trained object detector, and then jointly train the scene graph generator until convergence. We denote these as IMP$^\dagger$ and MSDN$^\dagger$. Using the same pre-trained object detector, we report the neural motif frequency baseline in \cite{zellers2017neural} as NM-Freq$^\dagger$.

We report the scene graph generation performance in Table~\ref{Table:Exp_main}. The top three rows are numbers reported in the original paper, and the bottom four rows are the numbers from our re-implementations. First, we note that our re-implementations of IMP and MSDN (IMP$^\dagger$ and MSDN$^\dagger$) result in performance that is close to or better than the originally reported numbers under some metrics (but not all), which establishes that the takeaway messages next are indeed due to our proposed architectural choices -- relation proposal network and attentional GCNs. Next, we notice that Graph R-CNN outperforms IMP$^\dagger$ and MSDN$^\dagger$. This indicates that our proposed Graph R-CNN model is more effective to extract the scene graph from images. Our approach also outperforms the frequency baseline on all metrics, demonstrating that our model has not just learned simple co-occurrence statistics from training data, but rather also captures context in individual images. More comprehensively, we compare with IMP and MSDN on the efficiency over training and inference. IMP uses 2.15$\times$ while MSDN uses 1.86$\times$ our method. During inference, IMP is 3.27$\times$ while MSDN is 3.80$\times$ slower than our Graph R-CNN. This is mainly due to the simplified architecture design (especially the aGCN for context propagation) in our model.

\subsection{Ablation Study}
\begin{figure}[t]
\begin{center}
\includegraphics[ width=1.0\textwidth]{figures/Fig4.png}
\end{center}%
\caption{Per category object detection performance change after adding RePN.}
\label{fig:ap}
\end{figure}

\begin{table}[t]\footnotesize
\setlength{\tabcolsep}{1.8pt}
\centering
\resizebox{0.95\columnwidth}{!}{
\begin{tabular}{c c c c ccccccccc}
\toprule
\multirow{3}{*}{RePN} & \multirow{3}{*}{GCN} & \multirow{3}{*}{aGCN} & {Detection} & \multicolumn{2}{c}{SGGen+} & \multicolumn{2}{c}{SGGen} & \multicolumn{2}{c}{PhrCls} & \multicolumn{2}{c}{PredCls}\\
\cmidrule(r){4-4}
\cmidrule(r){5-6} 
\cmidrule(r){7-8}
\cmidrule(r){9-10}
\cmidrule(r){11-12}
& & & mAP@0.5 & R@50 & R@100 & R@50 & R@100 & R@50 & R@100 & R@50 & R@100 \\
\midrule
 - & - & - &  20.4    &  25.9 & 27.9 & 6.1 & 7.9 & 17.8  & 19.9 & 33.5 & 38.4 \\
\checkmark & - & - &  \textbf{23.6} & 27.6 & 34.8 &  8.7 & 11.1  & 18.3  &  20.4 & 34.5 & 39.5 \\
\checkmark & \checkmark & - & 23.4 & 28.1 & 35.3 & 10.8 & 13.4 & 27.2  & 29.5 & 52.3 & 57.2 \\
\checkmark & - & \checkmark & 23.0 & \textbf{28.5} & \textbf{35.9} & \textbf{11.4} & \textbf{13.7} & \textbf{29.4} & \textbf{31.6} & \textbf{54.2} & \textbf{59.1}    \\
\bottomrule
\end{tabular}}\\[5pt]
\caption{Ablation studies on Graph R-CNN. We report the performance based on four scene graph generation metrics and the object detection performance in mAP@0.5.}
\label{Table:Exp_Ablation}
\end{table}

In Graph R-CNN, we proposed two novel modules -- relation proposal network (RePN) and attentional GCNs (aGCN). In this sub-section, we perform ablation studies to get a clear sense of how these different components affect the final performance. The left-most columns in Table~\ref{Table:Exp_Ablation} indicate whether or not we used RePN, GCN, and attentional GCN (aGCN) in our approach. 
The results are reported in the remaining columns of Table~\ref{Table:Exp_Ablation}. We also report object detection performance mAP@0.5 following Pascal VOC's metric \cite{everingham2012pascal}.

In Table~\ref{Table:Exp_Ablation}, we find RePN boosts \texttt{SGGen} and \texttt{SGGen+} significantly. This indicates that our RePN can effectively prune the spurious connections between objects to achieve high recall for the correct relationships. We also notice it improves object detection significantly. In Fig.~\ref{fig:ap} we show the per category object detection performance change when RePN is added. For visual clarity, we dropped every other column when producing the plot. We can see that almost all object categories improve after adding RePN. Interestingly, we find the detection performance on categories like \textit{racket}, \textit{short}, \textit{windshield}, \textit{bottle} are most significantly improved. 
Note that many of these classes are smaller objects that have strong relationships with other objects, e.g.~ rackets are often carried by people. Evaluating \texttt{PhrCls} and \texttt{PredCls} involves using the ground truth object locations. Since the number of objects in images (typically $<$25) is much less than the number of object proposals (64), the number of relation pairs is already very small. As a result, RePN has less effect on these two metrics.

By adding the aGCNs into our model, the performance is further improved. These improvements demonstrate that the aGCN in our Graph R-CNN can capture meaningful context across the graph. We also compare the performance of our model with and without attention. We see that by adding attention on top of GCNs, the performance is higher. This indicates that controlling the extent to which contextual information flows through the edges is important. These results align with our intuitions mentioned in the introduction. Fig.~\ref{fig:qualitative} shows generated scene graphs for test images. With RePN and aGCN, our model is able to generate higher recall scene graphs. The green ellipsoids shows the correct relationship predictions in the generated scene graph.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.95\textwidth, trim=0 1.2cm 1.2cm 0, clip]{figures/Fig5.pdf}
  \end{center}
\caption{Qualitative results from Graph R-CNN. In images, blue and orange bounding boxes are ground truths and correct predictions, respectively. In scene graphs, blue ellipsoids are ground truth relationships while green ones denote correct predictions.}
\label{fig:qualitative}
\end{figure}


