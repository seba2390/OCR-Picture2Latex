\section{Importance of Features in Coreference}
\label{ch:improvements:related}
{\newcite{uryupina07}}'s thesis is one of the most thorough analyses of linguistically motivated features for coreference resolution.
She examines a large set of linguistic features, 
i.e.\ string match, syntactic knowledge, semantic compatibility, discourse structure and salience, 
%that are suggested by theoretical studies
and investigates their interaction with coreference relations.
She shows that even imperfect linguistic features, which are extracted using error-prone preprocessing modules, 
boost the performance
and argues that coreference resolvers could and should benefit from linguistic theories.
Her claims are based on analyses on the MUC dataset.
\newcite{ng02a}, \newcite{yang04}, \newcite{ponzetto06b}, \newcite{bengtson08}, and \newcite{recasens09} also study the importance of features in coreference resolution.

Apart from the mentioned studies, which are mainly about the importance of individual features,
studies like \newcite{bjoerkelund12}, \newcite{fernandes12}, and \newcite{uryupina15} generate new features by combining basic features.
%For instance, based on ``mention type'' and ``string match'' features,
%one can create the feature ``anaphor mention type $\land$ string match''.
{\newcite{bjoerkelund12}} do not use a systematic approach for combining features.
%\newcite{fernandes12} and \newcite{uryupina15} 
%follow a systematic approach for creating combinatorial features.
{\newcite{fernandes12}} use the Entropy guided Feature Induction (EFI) approach \cite{fernandes12entropy}
to automatically generate discriminative feature combinations.
The first step is to train a decision tree on a dataset in which
each sample consists of features describing a mention pair. 
The EFI approach traverses the tree from the root in a depth-first order 
and recursively builds feature combinations. 
Each pattern that is generated by EFI starts from the root node.
As a result, EFI tends to generate long patterns.
%Figure~\ref{fernandes_tree} demonstrate a sample decision tree and the set of feature conjunctions that are extracted based on this tree.
%In order to limit the maximum template length, 
%\newcite{fernandes12} prune the initial decision tree at the depth five.
A decision tree does not represent all patterns of data.
Therefore, it is not possible to explore all feature combinations from a decision tree.
%Finally, \newcite{bjoerkelund12} use the EFI approach in order to generate templates for feature combinations 
%instead of exploring informative feature-values.

{\newcite{uryupina15}} propose an alternative approach to EFI.
They formulate the problem of generating feature combinations as a pattern mining approach.
%Let an item to be the pair of a feature and one of its corresponding values, i.e.\ feature$_i$=value$_j$.
%A pattern is a set of one or more items.
%The goal of a pattern mining approach is to mine a set of patterns, ideally the set of all patterns,
%that satisfies the specified criteria.
They use the Jaccard Item Mining (JIM) algorithm\footnote{\url{http://www.borgelt.net/jim.html}} \cite{segond11}.
%for mining patterns of feature-values.
%They abstract away the set of mined patterns to generate feature templates, 
%e.g.\ pattern \{type=proper, number=singular\} is abstracted to the feature template ``type+number''.
%Similar to \newcite{fernandes12}, all feature templates are then converted to binary features.
They show that the classifier that uses the JIM features significantly outperforms the one that employs the EFI features.