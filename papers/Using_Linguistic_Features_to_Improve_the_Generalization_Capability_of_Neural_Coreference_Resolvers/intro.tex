\section{Introduction}
Coreference resolution is the task of recognizing different expressions that refer to the same entity.
The referring expressions are called mentions.
For instance, the sentence ``[Susan]$_1$ sent [her]$_1$ daughter to a boarding school'' contains two coreferring mentions. ``her'' is an anaphor which refers to the antecedent ``Susan''.

The availability of coreference information benefits various Natural Language Processing (NLP) tasks including 
automatic summarization, question answering, machine translation and information extraction. 
Current coreference developments are almost only targeted at improving scores on the CoNLL official test set.
However,  
the superiority of a coreference resolver on the CoNLL evaluation sets does not necessarily indicate
that it also performs better on new datasets.
For instance, the ranking model of \newcite{clarkkevin16a}, the reinforcement learning model of \newcite{clarkkevin16b} 
and the end-to-end model of \newcite{leekenton17} are three recent coreference resolvers,
among which the model of \newcite{leekenton17} performs the best and that of \newcite{clarkkevin16b} 
performs the second best on the CoNLL development and test sets.
However, if we evaluate these systems on the WikiCoref dataset \cite{ghaddar16a}, 
which is consistent with CoNLL with regard to coreference definition and annotation scheme, 
the performance ranking would be in a reverse order\footnote{The single model of \newcite{leekenton17} is used here.}.

In \newcite{moosavi17b}, we investigate the generalization problem in coreference resolution
and show that there is a large overlap between the coreferring mentions in the CoNLL training and evaluation sets.
Therefore, higher scores on the CoNLL evaluation sets do not necessarily indicate a better coreference model. They
may be due to better memorization of the training data.
As a result, despite the remarkable improvements in coreference resolution,
the use of coreference resolution in other applications is mainly limited to the use of simple rule-based 
systems, e.g.\ \newcite{lapata05a},\newcite{yu2016unsupervised}, and \newcite{elsner08b}.

In this paper, we explore the role of linguistic features for improving generalization.
The incorporation of linguistic features is considered as a potential solution for building more generalizable NLP 
systems\footnote{E.g.\ there is a dedicated workshop for this topic \url{https://sites.google.com/view/relsnnlp}.}.
While linguistic features\footnote{We refer to features that are based on linguistic intuitions, e.g.\ string match, 
or are acquired from linguistic preprocessing modules, e.g.\ POS tags, as linguistic features.} were shown to be important for coreference resolution, e.g.\ \newcite{uryupina07} and \newcite{bengtson08},
state-of-the-art systems no longer use them and mainly rely on word embeddings and deep neural networks. 
Since all recent systems are using neural networks, we focus on the effect of linguistic features on a neural coreference resolver. 
%TODO in which many of such features are assumed to be learned implicitly during training.

The contributions of this paper are as follows: 

\squishlist
\item We show that linguistic features are more beneficial for a neural coreference resolver if we incorporate 
features and subsets of their values that are informative for discriminating coreference relations.
Otherwise, employing linguistic features with all their values  
only slightly affects the performance and generalization.
\item We propose an efficient discriminative pattern mining algorithm, called EPM, for determining (feature, value) pairs that are informative for the given task.
%EPM is a discriminative pattern mining approach.
We show that while the informativeness of EPM mined patterns is on-par with those of its counterparts,
it scales best to large datasets.\footnote{The EPM code is available at \url{https://github.com/ns-moosavi/epm}}  
\item By improving generalization, 
we achieve state-of-the-art performance on all examined out-of-domain evaluations. 
Our out-of-domain performance on WikiCoref is on-par with that of \newcite{ghaddar16b}'s coreference resolver, which is a system specifically designed for WikiCoref and uses its domain knowledge.
\squishend