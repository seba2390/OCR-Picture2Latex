\section{Discussions}
\paragraph{Is EPM the only solution?}
The general finding of this paper is that incorporating linguistic features in which the irrelevant or noisy values are discarded,
results in significant boost in generalization of the baseline coreference resolver.
However, our results do not indicate that EPM is the only possible method for selecting informative feature-values.
First, EPM is a discriminative pattern mining approach that scales better than its counterparts
and therefore is applicable for a large data like all mention-pairs of the CoNLL training data.
On the other hand, for smaller datasets, other discriminative pattern mining approaches are also applicable.
Second, it is also possible to consider each feature-value as a feature and then apply a 
feature selection algorithm to select informative feature-values.
Third, it is also possible to 
Benefits of EPM are: (1) it is scalable, (2) it explores all combinations of frequent feature-values up to $\Theta_l$,
and (3) it is independent of the learning algorithm, e.g. baseline coreference resolver.

\paragraph{What matters in the success of EPM?} 
(1) the frequency condition: patterns should be frequent otherwise the algorithm also picks noises, e.g.\ annotation errors, and very specific properties of the training data. To confirm the importance of the frequency measure, use apply EPM with the minimum frequency threshold of one. The result of deep-coref top-pairs model in which the resulting feature-values were incorporated were    
(2) the discriminative power of pattern: the JIM vs. EPM 

\paragraph{Why incorporating all features is not useful?}
A possible explanation is that, as \newcite{moosavi17b} show, 
lexical similarity is already a very strong signal in the CoNLL dataset;
a coreference resolver can achieve a lot by only focusing on lexical features.
Therefore, additional features should provide a strong enough signal to 
encourage the classifier to focus on new features as well.



 
