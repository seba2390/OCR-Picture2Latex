\section{Baseline Coreference Resolver}
deep-coref \cite{clarkkevin16a} and e2e-coref \cite{leekenton17} are among the best performing coreference resolvers 
from which e2e-coref performs better on the CoNLL test set.
deep-coref is a pipelined system, i.e.\ a mention detection first determines the list of candidate mentions with their corresponding features.
It contains various coreference models including the mention-pair, mention-ranking, and entity-based models.
The mention-ranking model of deep-coref has three variations: (1) ``ranking'' uses the slack-rescaled max-margin training objective of \newcite{wiseman15},
(2) ``reinforce'' is a variation of the ``ranking'' model in which the hyper-parameters are set in a reinforcement learning framework \cite{sutton1998reinforcement},
and (3) ``top-pairs'' is a simple variation of the ``ranking'' model that uses a probabilistic objective function and is used for pretraining the ``ranking'' model. 

e2e-coref is an end-to-end system that jointly models mention detection and coreference resolution.
It considers all possible (start, end) word spans of each sentence as candidate mentions.
Apart from a single model, e2e-coref includes an ensemble of five models.

We use deep-coref as the baseline in our experiments.
The reason is that 
%extracting linguistic features for all (start, end) word spans of e2e-coref is not computationally efficient.
some of the examined features require the head of each mention to be known, e.g. head match, 
while e2e-coref mentions do not have specific heads and heads are automatically determined using an attention mechanism.
We also observe that if we limit e2e-coref candidate spans to those that correspond to deep-coref's detected mentions,
the performance of e2e-coref drops to a level on-par with deep-coref\footnote{
The CoNLL score of the e2e-coref single model on the CoNLL development set drops from $67.36$ to $65.81$, 
while that of the deep-coref ``ranking'' model is $66.09$.}.
%
\section{Examined Features}
\label{sect:base_features}
The examined linguistic features include string match, syntactic, shallow semantic and discourse features.
\textbf{Mention-based} features include:
\squishlist
\item Mention type: proper, nominal or pronominal
\item Fine mention type: proper, definite or indefinite nominal, or the citation form of pronouns
\item Gender: female, male, neutral, unknown
\item Number: singular, plural, unknown
\item Animacy: animate, inanimate, unknown
\item Named entity type: person, location, organization, date, time, number, etc.
\item Dependency relation: enhanced dependency relation \cite{SCHUSTER16.779} of the head word to its parent
\item POS tags of the first, last, head, two words preceding and following of each mention
\squishend
%(1) mention type (proper, nominal or pronominal), (2) fine mention type (proper, definite or indefinite nominal, or the citation form of pronouns),
%(3) gender, (4) number, (5) animacy, (6) named entity type, (7) dependency relation (lexicalized) of the head word, and 
%(8) POS tags of the first, last, head, two preceding and two following words of each mention.

\textbf{Pairwise} features include:
\squishlist
\item Head match: both mentions have the same head, e.g.\ ``red hat'' and ``the hat''
\item String of one mention is contained in the other, e.g.\ ``Mary's hat'' and ``Mary''
\item Head of one mention is contained in the other, e.g.\ ``Mary's hat'' and ``hat''
\item Acronym, e.g.\ ``Heidelberg Institute for Theoretical Studies'' and ``HITS'' 
\item Compatible pre-modifiers: the set of pre-modifiers of one mention is contained in that of the other, e.g.\ ``the red hat that she is wearing'' and ``the red hat''
\item Compatible\footnote{One value is unknown, or both values are identical.}\ gender,\ e.g.\ ``Mary'' and ``women''
\item Compatible number, e.g.\ ``Mary'' and ``John''
\item Compatible animacy, e.g.\ ``those hats'' and ``it'' 
\item Compatible attributes: compatible gender, number and animacy, e.g.\ ``Mary'' and ``she'' 
\item Closest antecedent that has the same head and compatible premodifiers, e.g.\ ``this new book'' and ``This book'' in ``Take a look at this new book. This book is one of the best sellers.''
\item Closest antecedent that has compatible attributes, e.g.\ the antecedent ``Mary'' and the anaphor ``she'' in the sentence ``John saw Mary, and she was in a hurry''
\item Closest antecedent that has compatible attributes and is a subject, e.g.\ the antecedent ``Mary'' and the anaphor ``she'' in the sentence ``Mary saw John, but she was in a hurry'' 
\item Closest antecedent that has compatible attributes and is an object, e.g.\ ``Mary'' and ``she'' in ``John saw Mary, and she was in a hurry''
\squishend
% (1) head match, (2) string of one mention is contained in the other, (3) head of one mention is contained in the other, 
%(4) acronym, (5) compatible pre-modifiers, i.e.\ the set of pre-modifiers of one mention is contained in that of the other, 
%(6) compatible\footnote{One value is unknown, or both values are the same.} gender, (7) compatible number, (8) compatible animacy, (9) compatible attributes, i.e.\ compatible gender, number and animacy, 
%(9) closest antecedent that has compatible attributes (10) closest antecedent that has compatible attributes and is a subject, 
%(11) closest antecedent that has compatible attributes and is an object, and (12) closest antecedent that has the same head and compatible premodifiers.
The last three features are similar to the discourse-level features discussed by \newcite{uryupina07},
which are created by combining \emph{proximity}, \emph{agreement} and \emph{salience} properties.
She shows that such features are useful for resolving pronouns.
we estimate proximity by considering the distance of two mentions.
% and they also consider the gender, number and animacy agreements.
The salience is also incorporated by discriminating subject or object antecedents.  
We do not use any gold information. All features are extracted using Stanford CoreNLP \cite{corenlp}.
