\begin{table*}[!htb]
    \begin{center}\footnotesize
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}l|l|@{\hskip3pt}rrr@{\hskip3pt}|@{\hskip3pt}rrr@{\hskip3pt}|@{\hskip3pt}rrr@{\hskip3pt}|r@{\hskip3pt}||@{\hskip3pt}rrr@{\hskip3pt}}
     \multicolumn{2}{c}{} & \multicolumn{3}{c}{MUC} &
     \multicolumn{3}{c}{$B^3$} & \multicolumn{3}{c}{CEAF$_e$} & \multicolumn{1}{c}{CoNLL} & \multicolumn{3}{c}{LEA} \\ \hline
     \multicolumn{1}{c}{}& \multicolumn{1}{c}{} &R & P & F$_1$ & R & P & F$_1$ & R & P & F$_1$ &  & R & P & F$_1$\\ \hline
     %\multicolumn{14}{c}{\textbf{CoNLL test set}} \\ \hline
     \hline
     %\multicolumn{14}{c}{deep-coref} \\ \hline
     \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{ deep-coref}}} 
     & ranking & $70.43$ & $79.57$ & $74.72$ & $58.08$ & $69.26$ & $63.18$ & $54.43$ & $64.17$ & $58.90$ & $65.60$ & $54.55$ & $65.68$ & $59.60$ \\
     & reinforce & $69.84$ & $79.79$ & $74.48$ & $57.41$ & $70.96$ & $63.47$ & $55.63$ & $63.83$ & $59.45$ & $65.80$ & $53.78$ & $67.23$ & $59.76$\\ 
     %ranking-enhanced & $70.08$ & $81.07$ & $75.18$ & $57.90$ & $71.58$ & $64.02$ & $55.07$ & $65.09$ & $59.66$ & $66.20$ & $54.39$ & $68.12$ & $60.49$  \\ 
     &top-pairs & $69.41$ & $79.90$ & $74.29$ & $57.01$ & $70.80$ & $63.16$ & $54.43$ & $63.74$ & $58.72$ & $65.39$ & $53.31$ & $67.09$ & $59.41$  \\ 
	 %& \multicolumn{14}{c}{\textbf{+Linguistic Features}} \\ 
	 %& +linguistic & $69.81$ & $79.46$ & $74.33$ & $57.23$ & $70.35$ & $63.11$ & $54.96$ & $62.74$ & $58.59$ & $65.34$ & $53.48$ & $66.64$ & $59.34$  \\
     &{+EPM} & $71.16$ & $79.35$ & $75.03$ & $59.28$ & $69.70$ & $64.07$ & $56.52$ & $64.02$ & $60.04$ & $66.38$ & $55.63$ & $66.11$ & $60.42$ \\
     &+JIM & $69.89$ & $80.45$ & $74.80$ & $57.08$ & $71.58$ & $63.51$ & $55.36$ & $64.20$ & $59.45$ & $65.93$ & $53.46$ & $67.97$ & $59.85$\\
     \hline
     \parbox[t]{2mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{ e2e}}} 
     %\multicolumn{14}{c}{e2e-coref} \\ \hline
     & single & $74.02$ & $77.82$ & $75.88$ & $62.58$ & $67.45$ & $64.92$ & $59.16$ & $62.96$ & $61.00$ & $67.27$ & $58.90$ & $63.79$ & $61.25$ \\
     & ensemble & $73.73$ & $80.95$ & $77.17$ & $61.83$ & $72.10$ & $66.57$ & $60.11$ & $65.62$ & $62.74$ & $68.83$ & $58.48$ & $68.81$ & $63.23$\\
     \hline
    \end{tabular}
    }
    \end{center}
    \caption{\footnotesize Comparisons on the CoNLL test set. 
    The F$_1$ gains that are statistically significant: (1) ``+EPM'' compared to ``top-pairs'', ``ranking'' and ``JIM'', (2) ``+EPM'' compared to ``reinforce'' based on MUC, B$^3$ and LEA, (3) ``single'' compared to ``+EPM'' based on MUC and B$^3$, and (4) 
``ensemble'' compared to other systems. Significance is measured based on the approximate randomization test ($p<0.05$) \cite{noreen89}.
    }
    \label{tab:in-domain-evaluations}
\end{table*}

\section{Impact of Informative Feature-values}
\subsection{Experimental Setup}
\label{ch:improvements_baseline}
%For coreference evaluation, we use \emph{MUC} \cite{vilain95}, \emph{B}$^3$ \cite{bagga98b},
%\emph{CEAF}$_e$ \cite{luoxiaoqiang05a}, \emph{LEA} \cite{moosavi16b},
%and the \emph{CoNLL} score, i.e.\ the average F$_1$ value of \emph{MUC}, \emph{B}$^3$, and \emph{CEAF}$_e$.
For determining informative feature-values, 
we extract all features for all mention-pairs\footnote{Each mention is paired with all the preceding mentions.} of the CoNLL training data 
and then apply EPM on this 
data.
In order to prevent learning annotation errors and specific properties of the training data, 
we consider a pattern as frequent if it occurs in coreference relations of at least 
$m$ different coreferring anaphors ($m=20$).
%e.g.\ patterns that only occur in the coreferent relations of a specific anaphor
%that has more than $m$ antecedents will not be included.
Since the majority of mention-pairs are non-coreferent and we are not interested in patterns for non-coreferring relations, 
we also consider the coreference probability of each pattern $p$, i.e.\ $\frac{|\{X_i|p \in X_i \land c(X_i)=coreferent\}|}{|\{X_i|p \in X_i\}|}$, 
in the post-pruning function. 
The coreference probability should be higher than a threshold ($60\%$ in our experiments),
so we only mine patterns that are informative for coreferring mentions.%\footnote{Setting the coreference probability to higher values results in only long and specific patterns.}.

For the coreference resolution experiments, instead of incorporating informative patterns, we incorporate feature-values that are included in the informative patterns mined by EPM.
The reason
is that deep-coref, or any other recent coreference resolver, uses a deep neural network,
which has a fully automated feature generation 
process.
We add these feature-values 
as binary features.

By setting $\Theta_l$ to five,\footnote{We observe that using larger $\Theta_l$ values will result in many over-specified patterns.} EPM results in 
13 pairwise feature-values,
112 POS tags, i.e.\ 53 POS for anaphors and 59 for antecedents,
25 dependency relations,
26 mention types (mention types or fine mention types),
and finally, 14 named entity tags.\footnote{Following the previous studies that show different features are of different importance for various types of mentions, e.g.\ \newcite{denis08} and \newcite{moosavi17a},
we mine a separate set of patterns for each type of anaphor.
These resulting feature-values are the union of informative feature-values for all types of anaphora.}

Based on the observation in Section~\ref{sect:all}, we use the top-pairs model of deep-coref as the baseline to employ additional features, i.e. ``+EPM'' is the top-pairs model in which EPM feature-values are incorporated.
%We measure significant based on the approximate randomization test ($p<0.05$) \cite{noreen89}.

%\footnote{We set the minimum frequency threshold and $\Theta_l$ hyper-parameters 
%only based on the resulting informative feature-values on the training data. }.
%TODO None of the values of the gender, number and animacy features 
%are among the selected feature-values while
%the corresponding compatibility feature-values, i.e.\ ``compatible number=true'', ``compatible gender=true'' and ``compatible animacy=true'', 
%are informative.
%This way we exclude patterns that only occur in the coreferent relations of a specific anaphor
%that has more than 20 antecedents.
%In order to discriminate between coreferent relations of various anaphora, 
%all coreferent relations of a same anaphor are specified with a unique positive label
%that is different from positive labels of other coreferring anaphora.
%All non-coreferring mention pairs are specified with a zero label.
%TODO
%Since we are not interested in patterns for non-coreferring relations, 
%we also consider the coreference probability of each pattern $p$, i.e.\ $\frac{|\{X_i|p \in X_i \land c(X_i)=coreferent\}|}{|\{X_i|p \in X_i\}|}$, 
%in the post-processing step. 
%The coreference probability should be higher than a threshold ($60\%$ in our experiments),
%so we only mine patterns that are informative for coreferring mentions.
%ones\footnote{Setting the coreference probability to higher values results in only long and specific patterns.}.
%The p-value thresholds for both $G^2$ and binomial tests are set to $0.01$.
%For the experiments of this section, we set $\Theta_l$ to 
%five\footnote{Larger values increase the number of informative patterns considerably and result in many specific patterns.}.
%TODO Following the previous studies that show different features are of different importance for various types of mentions \cite{denis08,lassalle13,moosavi17a},
%we mine a separate set of features for each type of anaphor, i.e.\ proper, nominal and pronominal.
%To do so, we separate mention-pairs based on the type of anaphor and mine a separate set of patterns for each type of anaphor.
%TODO Above features are all extracted using the Stanford CoreNLP preprocessing modules\footnote{Available at \url{https://stanfordnlp.github.io/CoreNLP/coref.html}}.
\iffalse

\label{ch:improvements:first_q}
In this section, we show that 
1) the incorporation of all linguistic feature-values does not have a considerable effect on in-domain evaluations,
and 2) the incorporation of informative feature-values significantly improve the performance.

\begin{table}[htbp]
    \begin{center}\footnotesize
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{@{}l|l|@{\hskip3pt}r@{\hskip3pt}|@{\hskip3pt}r@{\hskip3pt}|@{\hskip3pt}r@{\hskip3pt}|r@{\hskip3pt}||@{\hskip3pt}r@{\hskip3pt}}
     \multicolumn{2}{c}{} & \multicolumn{1}{c}{MUC} &
     \multicolumn{1}{c}{$B^3$} & \multicolumn{1}{c}{CEAF$_e$} & \multicolumn{1}{c}{CoNLL} & \multicolumn{1}{c}{LEA} \\ \hline
     %\multicolumn{1}{c|}{}&R & P & F$_1$ & R & P & F$_1$ & R & P & F$_1$ &  & R & P & F$_1$\\ \hline
     %\multicolumn{6}{c}{\textbf{CoNLL development set}} \\ \hline
     \hline
     %\multicolumn{6}{c}{ranking} \\ \hline
     \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize  ranking}}} 
     & base  & $74.31$  & $64.23$  & $59.73$ & $66.09$ & $60.47$  \\
	 & +all  & $74.35$  & $63.96$  & $60.19$ & $66.17$ & $60.20$  \\
     & {+EPM}  & $74.91$ & $65.07$ & $60.77$ & $66.92$ & $61.46$  \\
     \hline
     %\multicolumn{6}{c}{top-pairs} \\ \hline
     \parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{\scriptsize  top-pairs}}} 
     & base  & $73.95$ & $63.98$ & $59.52$  & $65.82$ & $60.07$ \\ 
     & +all & $74.32$ & $64.45$ & $60.19$ & $66.32$ & $60.62$ \\
     & +EPM & $74.92$ & $65.03$& $60.88$ & $66.95$ & $61.34$ \\ 
     \hline
    \end{tabular}
    }
    \end{center}
    \caption{Impact of linguistic features on deep-coref models on the CoNLL development set.
    The F$_1$ gains of ``+EPM'' are all statistically significant.}
    \label{tab:top-pair-vs-ranking}
\end{table}
\fi
\iffalse
The impact of linguistic features in deep-coref is shown in Table~\ref{tab:top-pair-vs-ranking}.
The rows ``base'' show the base results of deep-coref's ``ranking'' and ``top-pairs'' models. 
``+all'' represents the results of the mention-ranking models in which the feature set of Section~\ref{sect:base_features} is employed.
For ``+all'', 
the gender, number, animacy and mention type features, which have less than five values, 
are converted to binary features. 
%TODO i.e.\ each feature=value is considered as a binary feature.  
Named entity and POS tags, and dependency relations
are represented as learned embeddings.
As we see, incorporating all the linguistic features bridges the gap between the performance of 
``top-pairs'' and ``ranking''.
However, it does not improve over the ``ranking'' model.
\fi
%Therefore, we use EPM to choose informative values, if any, of the existing features.
%If a feature does not occur in any informative pattern, 
%we exclude it from the set of features.
%\footnote{It takes from two to six hours for mining informative patterns for different types of anaphors.} 
%including 
%TODO deep-coref itself has six pairwise features, i.e.\ three speaker match, exact string match, refined head match and relaxed string match.
%It has a hidden layer of size 1000 on top of the pairwise features.
%The output of the hidden layer is then combined with the output of the hidden layer of word embeddings. 
%Apart from these features, we do not change any other parameter in deep-coref.
%The deep-coref model in which the EPM feature-values are incorporated is referred to as ``+EPM'' in our experiments.
\iffalse
The inclusion of EPM feature-values results in significant improvements.
The F$_1$ gains of ``+EPM'' 
are all statistically significant ($p<0.05$) based on the approximate randomization test \cite{noreen89}.
The results show that linguistic features in which noisy and irrelevant values are discarded
are beneficial for improving the performance of a neural coreference resolver.
%TODO It is worth noting that, as mentioned in Section~\ref{ch:improvement:base_features}, averaged word embeddings are not included in any of the ``+EPM'' experiments. 

For the ``+EPM'' experiments,
the ``top-pairs'' and ``ranking'' models have on-par performance
while ``top-pairs'' uses a simpler objective function and it is used for pretraining the ``ranking'' model.
Henceforth, 
we use the ``top-pairs'' model as the baseline, i.e.\ ``+EPM'' refers to the ``top-pairs'' model with EPM feature-values.
\fi
\subsection{Impact on In-domain Performance}
The performance of the ``+EPM'' model compared to recent state-of-the-art coreference models on the CoNLL test set is presented in Table~\ref{tab:in-domain-evaluations}.
%The ``top-pairs'', ``ranking'', ``reinforce'' rows represent the results of deep-coref's coreference models. 
%The ``+EPM'' row represents the results of deep-coref's top-pairs model in which the EPM feature-values are added.
%The F$_1$ gains of ``+EPM'' compared to all ``top-pairs'', ``ranking'', and ``reinforce'' models are statistically significant.
%The only exception is the difference between ``+EPM'' and ``reinforce'' based on the \emph{CEAF$_e$} metric,
%which is not significant.
The ``single'' and ``ensemble'' rows represent the results of the single and ensemble models of e2e-coref. 

We also compare EPM with the pattern mining approach used by \newcite{uryupina15}, i.e.\ Jaccard Item Mining (JIM).
For a fair comparison, while \newcite{uryupina15} used mined patterns for extracting feature templates, we use them for selecting feature-values.
We run the JIM algorithm on 
the same data and with the same setup as that of EPM.\footnote{
We set the minimum frequency, maximum pattern length and $score^+$ threshold parameters of JIM to 20, 5 and 0.6.}
This results in nine pairwise features, 
260 POS tags, 38 dependency relations, 32 mention types, and 18 named entity tags.
The ``+JIM'' row shows the results of deep-coref top-pairs model 
in which these feature-values are incorporated.
As we see, EPM feature-values result in significantly better performance than those of JIM
while the number of EPM feature-values is considerably less than JIM.
%The difference between the performance of ``+JIM''
%and ``top-pairs'' is statistically significant only based on \emph{MUC}, and \emph{CEAF$_e$} metrics.
%
\begin{table}[!htb]
    \begin{center}\footnotesize
    \begin{tabular}{@{}l|r|r|r|r|r@{\hskip3pt}}
     \multicolumn{1}{c}{} & \multicolumn{1}{c}{MUC} & \multicolumn{1}{c}{$B^3$} & \multicolumn{1}{c}{CEAF$_e$} & \multicolumn{1}{c}{CoNLL} & \multicolumn{1}{c}{LEA} \\ \hline
     %\multicolumn{1}{c|}{}& F$_1$ & F$_1$ & F$_1$ &  & R & P & F$_1$\\ \hline
     \hline
     +EPM  & $74.92$ & $65.03$ & $60.88$ & $66.95$ & $61.34$ \\ \hline
     -pairwise & $74.37$ & $64.55$ & $60.46$ & $66.46$ &$60.71$\\
     -type & $74.71$ & $64.87$ & $61.00$ & $66.86$ & $61.07$\\
     -dep & $74.57$ & $64.79$ & $60.65$ & $66.67$ & $61.01$\\
     -NER & $74.61$ & $65.05$ & $60.93$ & $66.86$ & $61.27$\\
     -POS & $74.74$ & $65.04$ & $60.88$ & $66.89$ & $61.30$\\ \hline
     +pairwise & $74.25$ & $64.33$ & $60.02$ & $66.20$ & $60.57$ \\ 
     \hline
    \end{tabular}
    \end{center}
    \caption{Impact of different EPM feature groups on the CoNLL development set.}
    \label{tab:feature-ablation}
\end{table}

\begin{table*}[!htb]
    \begin{center}\footnotesize
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}l|@{\hskip3pt}l|@{\hskip3pt}rrr@{\hskip3pt}|@{\hskip3pt}rrr@{\hskip3pt}|@{\hskip3pt}rrr@{\hskip3pt}|r@{\hskip3pt}||@{\hskip3pt}rrr@{\hskip3pt}}
     \multicolumn{2}{c}{} & \multicolumn{3}{c}{MUC} &
     \multicolumn{3}{c}{$B^3$} & \multicolumn{3}{c}{CEAF$_e$} & \multicolumn{1}{c}{CoNLL} & \multicolumn{3}{c}{LEA} \\ \hline
     \multicolumn{1}{c}{}& \multicolumn{1}{c}{} &R & P & F$_1$ & R & P & F$_1$ & R & P & F$_1$ &  & R & P & F$_1$\\ \hline
      %\multicolumn{14}{c}{\textbf{WikiCoref}} \\ \hline \hline
         %  \multicolumn{14}{c}{deep-coref} \\ \hline
     \parbox[t]{2mm}{\multirow{5}{*}{\rotatebox[origin=c]{90}{ deep-coref}}} 
      %& conll & $58.59$ & $66.63$ & $62.35$ & $44.40$ & $54.87$ & $49.08$ & $42.47$ & $51.47$ & $46.54$ & $52.65$ & $40.36$ & $50.73$ & $44.95$ \\ 
      &ranking & $57.72$ & $69.57$ & $63.10$ & $41.42$ & $58.30$ & $48.43$ & $42.20$ & $53.50$ & $47.18$ & $52.90$ & $37.57$ & $54.27$ & $44.40$  \\ 
      &reinforce & $62.12$ & $58.98$ & $60.51$ & $46.98$ & $45.79$ & $46.38$ & $44.28$ & $46.35$ & $45.29$ & $50.73$ & $42.28$ & $41.70$ & $41.98$ \\
      &top-pairs & $56.31$ & $71.74$ & $63.09$ & $39.78$ & $61.85$ & $48.42$ & $40.80$ & $52.85$ & $46.05$ & $52.52$ & $35.87$ & $57.58$ & $44.21$\\ 
      &{+EPM} & $58.23$ & $74.05$ & $\boldsymbol{65.20}$ & $43.33$ & $63.90$ & $51.64$ & $43.44$ & $56.33$ & $\boldsymbol{49.05}$ & $\boldsymbol{55.30}$ & $39.70$ & $59.81$ & $\boldsymbol{47.72}$ \\  
      %&+all & $57.29$ & $72.65$ & $64.06$ & $39.68$ & $62.96$ & $48.68$ & $41.61$ & $54.29$ & $47.11$ & $53.29$ & $35.89$ & $58.83$ & $44.58$ \\ \hline
     %      \multicolumn{14}{c}{e2e-coref} \\ \hline
     \parbox[t]{2mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{ e2e}}} 
      & single & $60.14$ & $64.46$ & $62.22$ & $45.20$ & $51.75$ & $48.25$ & $38.18$ & $43.50$ & $40.67$ & $50.38$ & $40.70$ & $47.56$ & $43.86$ \\
      & ensemble & $59.58$ & $71.60$ & $65.04$ & $44.64$ & $60.91$ & $51.52$ & $40.38$ & $49.17$ & $44.35$ & $53.63$ & $40.73$ & $56.97$ & $47.50$\\ \hline
     & {G\&L} & $66.06$ & $62.93$ & $64.46$ & $57.73$ & $48.58$ & $\boldsymbol{52.76}$ & $46.76$ & $49.54$ & $48.11$ & $55.11$ & - & - & -\\ \hline
    \end{tabular}
    }
    \end{center}
    \caption{Out-of-domain evaluation on the WikiCoref dataset. The highest $F_1$ scores are boldfaced.}
    \label{tab:out-domain-deep-coref}
\end{table*}
\paragraph{Feature Ablation}
Table~\ref{tab:feature-ablation} shows the effect of each group of EPM feature-values, i.e.\ pairwise features, mention types, 
dependency relations, named entity tags and POS tags, on the performance of ``+EPM''.
The performance of ``+EPM'' from which each of the above feature groups is removed, one feature group at a time, 
is represented as ``-pairwise'', ``-types'', ``-dep'', ``-NER'', and ``-POS'', respectively.
%``-pairwise'', ``-types'', ``-dependency'', ``-NER'', and ``-POS'' 
%represent the performance of the ``+EPM'' model in which the corresponding feature group, i.e.\ pairwise, mention type, dependency relations, named entity tags, or POS tags feature-values,
%is removed from EPM feature-values.
The POS and named entity tags have the least 
and the pairwise features have the most significant effect.
Since pairwise features have the most significant effect, 
we also perform an experiment in which only pairwise features are incorporated in the ``top-pairs'' model, i.e.\ ``+pairwise''.
The results of ``-pairwise'' compared to ``+pairwise'' show that pairwise feature-values 
have a significant impact, but only when they are considered in combination with other EPM feature-values.
%

%TODO
%Some feature-values capture competing information,
%e.g.\ POS tags of previous and following words of a mention can implicitly capture whether a mention is a subject or an object, 
%while this information is explicitly captured by dependency relations.
%However,
%we keep them both among the set of feature-values.
%The inclusion of competing features can result in more robust models 
%on test data in which the values of some of these features are noisy or missing \cite{sutton2006reducing}.
%