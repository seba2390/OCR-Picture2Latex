\section{Preliminary Analysis}
\label{Sec:pre_analysis}
In this section, we discuss the inner working of GNNs, conduct preliminary analysis to show the issues of GNN with sparse labels and verify that densifying graphs by connecting similar nodes can potentially alleviate the issue.

\subsection{Notations}
We use $\mathcal{G}=(\mathcal{V},\mathcal{E}, \mathbf{X})$ to denote an attributed graph, where $\mathcal{V}=\{v_1,...,v_N\}$ is the set of $N$ nodes, $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$ is the set of edges, and $\mathbf{X}=\{\mathbf{x}_1,...,\mathbf{x}_N\}$ is the set of attributes of $\mathcal{V}$. $\mathbf{A} \in \mathbb{R}^{N \times N}$ is the adjacency matrix of the graph $\mathcal{G}$, where $\mathbf{A}_{ij}=1$ if nodes ${v}_i$ and ${v}_j$ are connected, otherwise $\mathbf{A}_{ij}=0$. In 
our setting, only a limited number of nodes $\mathcal{V}_L=\{v_1,...,v_l\}$ are provided with labels $\mathcal{Y}=\{\mathbf{y}_1,...,\mathbf{y}_l\}$, where $\mathbf{y}_i \in \mathbb{R}^C$ is a one-hot vector of node $v_i$'s label for multi-class classification. Note that the topology of the graph $\mathcal{G}$ could be noisy such as poisoned by adversarial edges or containing inherent noises, which leads to poor performance.
\label{sec:3_1}

\subsection{Basic Design and Inner Working of GNNs}
In this subsection, we briefly introduce the common architecture of graph neural networks (GNNs). 
Generally, GNNs adopt message-passing mechanism to learn node representations, i.e., they update the representation of a node by aggregating the representations of the neighborhood nodes. The updating process of the $k$-th layer in GNNs could be written as:
\begin{equation}
\begin{aligned}
    \mathbf{a}^{(k)}_v & = \text{AGGREGATE}^{(k-1)}(\{\mathbf{h}^{(k-1)}_u: u \in \mathcal{N}(v)\}),
    \label{eq:GNN_a} \\
    \mathbf{h}^{(k)}_{v} & =\text{COMBINE}^{(k)}(\mathbf{h}^{(k-1)}_v, \mathbf{a}_v^{(k)}),
    % \label{eq:GNN_h}
\end{aligned}
\end{equation}
where $\mathbf{h}^{(k)}_v$ is the representation vector of node $v \in \mathcal{V}$ at the $k$-th layer and $\mathcal{N}(v)$ is the set of neighborhoods of $v$. 
During the training of node classification, the representations of labeled nodes are used to give prediction and obtain the training loss to minimize.
With the message-passing mechanism, after $K$-layers of GNN, the node representation of $v_i$ would capture the node features and structure information of the $K$-hop neighborhoods of $v_i$, and thus facilitating downstream tasks. 
In other words, in GNN, \textit{one labeled node would make the $K$-hop neighborhood participate in the training of GNN}, which is one reason that GNNs have great ability in leveraging unlabeled nodes for semi-supervised node classification. 

\subsection{Analysis of GNNs with Sparse Labels}

In this subsection, we conduct preliminary analysis on real-world graphs to show the issues of GNNs when limited labeled nodes are available for training, which paves us a way to design robust GNNs for alleviating the label sparsity issue. The analysis is based on three widely used datasets, i.e., Citeseer~\cite{sen2008collective}, Cora and Cora-ML~\cite{mccallum2000automating}.

Generally, GNNs, such as GCN and GAT, rely on the classification loss of the labeled nodes to learn the parameters,  which is effective when we have adequate labeled nodes. However, when the size of labeled node set $\mathcal{V}_L$ is small and the graph is sparse, only a small portion of nodes would be involved in the training. This may lead to poor performance of GNNs. More specifically, for a $K$-layer GNN, the nodes involved in the training phase include the labeled nodes and the unlabeled nodes within $K$-hop distance of labeled nodes. We usually set $K$ as 2 to 3 because deep GNNs have over-smoothing issue~\cite{li2018deeper}. Since real-world graphs are usually sparse, the $K$-hop neighbors of the labeled nodes would be limited as well. Thus, when $\mathcal{V}_L$ is small, only a small portion of nodes would be involved in training, making GNNs less effective in leveraging unlabeled nodes.

We analyze how the label rate affects the rates of uninvolved nodes of real-world datasets for a two layer GNN. We vary label rates from 0.01 to 0.25. The average uninvolved node rates and the standard deviations are shown in Fig. \ref{fig:1_a}. From the figure, we observe that (\textbf{i}) when the label rate is high, say above 0.1, most of the nodes are involved in training GNN. The benefit of further increasing label rate is marginal as the 2-hop neighbors of labeled nodes could overlap. This is one reason that GNNs have great ability for semi-supervised node classification with small but adequate amount of labeled nodes, and the increase of labeled nodes can marginally improve the performance; (\textbf{ii}) As the label rate decreases from 0.1, the uninvolved node rate increases significantly, i.e., the majority of nodes are not involved in the training. This indicates that GNNs would have difficulty in handling sparsely labeled graphs.

\begin{figure}[t]
\centering
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figure/unlabeled_rate.png} 
    \vskip -0.5em
    \caption{Impacts of label rate}
    \label{fig:1_a}
\end{subfigure}
%\vspace{-1em}
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figure/edge.png} 
    \vskip -0.5em
    \caption{Impacts of graph density}
    \label{fig:1_b}
\end{subfigure}
\vspace{-1.2em}
\caption{The impacts of label rate and density of graph to uninvolved node rate in the training phase. }
% \label{fig:abl}
\vskip -1.5em
\end{figure}

Although a higher label rate could help to reduce the uninvolved node rate, it can be expensive to obtain more labels~\cite{gallagher2008leveraging}. Thus, we need an alternative approach to effectively use the labels. From the analysis above, one potential solution is to make the graph denser so that one labeled node could have more neighbors to be involved in the training of GNN. To verify it, we randomly add different amount of edges to the three graphs. We denote the number of edges of the new graph as $|\mathcal{E}_A|$ and that of raw graph as $|\mathcal{E}|$. We fix label rate as 0.01. The impact of the graph density on the uninvolved node rate is presented in Fig.~\ref{fig:1_b}. From the figure, we observe that when $|\mathcal{E}_A|/|\mathcal{E}|$ increases from 1 to 3, i.e., we add two times the number of original edges, the uninvoled node rate drops significantly. For example, it drops from 0.8 to around 0.3 on Citeseer. 

As real-world graphs such as social networks have many pairs of nodes who are similar but not connected together, the analysis above shows that it is promising to predict links to densify the graph, which can help the message passing of GNNs to alleviate the issue of limited labeled nodes. In addition, these predicted edges can also be directly used to regularize the predicted labels of unlabeled nodes, i.e., if two nodes are more likely to have a link, they are more likely to have the same labels.

\label{sec:3_3}
\subsection{Problem Definition}
Our preliminary analysis shows that predicting links to densify the graph can potentially alleviate the label sparsity issue.  In addition, the link prediction can potentially down-weight or eliminate noisy edges as noisy edges usually connect nodes with low node attribute similarity.
Therefore, we aim to simultaneously eliminate noisy edges and densify the graph with a link predictor and train a robust GNN on the new graph. The problem is formally defined as:
\newtheorem{problem}{Problem}
\begin{problem}
Given an attributed graph $\mathcal{G}=(\mathcal{V},\mathcal{E}, \mathbf{X})$ with edge set $\mathcal{E}$ might contain a small amount of noisy edges, and a small set of labeled nodes $\mathcal{V}_L \in \mathcal{V}$ with the corresponding labels in $\mathcal{Y}$, simultaneously learn adjacency matrix $\mathbf{S} \in [0,1]^{N \times N}$ which down-weights/removes noisy edges and completes missing links by a link predictor $f_E:(v_i,v_j) \rightarrow \mathbf{S}_{ij}$, and a GNN on the learned graph for node classification, i.e., $f_{\mathcal{G}}:(\mathbf{S}, \mathbf{X}) \rightarrow \hat{\mathcal{Y}}$, where $\mathbf{S}_{ij}$ indicates the weight of edge linking $v_i$ and $v_j$ and $\hat{\mathcal{Y}}$ is the set of predictions for unlabeled nodes.
\end{problem}
