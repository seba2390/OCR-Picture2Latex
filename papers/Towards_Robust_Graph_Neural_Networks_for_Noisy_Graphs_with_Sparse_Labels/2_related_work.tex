\section{Related Work}
\label{sec:related_work}
% In this section, we review the related work, which includes graph neural networks, and robust graph neural networks. 

\subsection{Graph Neural Networks}
Graph Neural Networks (GNNs) have shown their great power in modeling graph structured data for various applications~\cite{wang2019semi,wang2018cross,zhao2020semi,dai2021say,zhao2021graphsmote}.
To generalize neural networks for graphs, two categories of GNNs are proposed, i.e., spectral-based~\cite{bruna2013spectral,henaff2015deep,kipf2016semi,levie2018cayleynets} and spatial-based~\cite{velivckovic2017graph,hamilton2017inductive,chen2018fastgcn,chiang2019cluster}. \citeauthor{bruna2013spectral} \cite{bruna2013spectral} first propose spectral-based GNNs by defining graph convolution with spectral graph theory. For instance, GCN~\cite{kipf2016semi} simplifies the convolutional operation by using the first order approximation. Spatial-based graph convolution is defined in spatial domain, which updates node representation by aggregating its neighbors' representations \cite{niepert2016learning,gilmer2017neural,hamilton2017inductive}. 
For example, self-attention of neighbor nodes is leveraged in graph attention network (GAT) \cite{velivckovic2017graph}. Moreover, various spatial methods are proposed to solve the scalability issue~\cite{chen2018fastgcn,chiang2019cluster} and learn deeper GNNs~\cite{chen2020simple}.  Recently, to alleviate the problem of lacking labeled nodes, many efforts are taken to explore GNNs using self-supervision, which aims to learn better node representations with pretext tasks~\cite{sun2019multi,li2018deeper,kim2021find,zhu2020self,jin2020self,dai2021towards}. For instance, superGAT~\cite{kim2021find} deploys edge prediction in GAT to guide the learning of attention for better representations. SE-GNN~\cite{dai2021towards} deploys contrastive learning to benefit the similarity modeling for self-explainable GNN.

Inspired by the great success of GNNs, methods that construct graphs and adopt GNNs for data without explicit relational structure are also explored~\cite{henaff2015deep,chen2019multi,jiang2019semi,dai2021nrgnn}. Generally, a graph would be built based on certain rules~\cite{henaff2015deep,chen2019multi} or be learned in an end-to-end model~\cite{jiang2019semi,dai2021nrgnn}. Our RS-GNN is inherently different from these methods as we eliminate/down-weight the noisy edges and predict the missing edges for robust GNNs on noisy graphs with limited labels. 

\subsection{Robust GNNs}
Although GNNs have obtained great achievements, they are vulnerable to adversarial attacks~\cite{wu2019adversarial,dai2018adversarial,zugner2018adversarial,zugner2019adversarial}. Based on the objective, the adversarial attacks on GNNs can be split into two categories, i.e., targeted attack~\cite{dai2018adversarial,zugner2018adversarial} and non-targeted attack~\cite{zugner2019adversarial}. Targeted attack methods aim to degrade the performance of the GNNs on target nodes. 
For instance, \textit{nettack}~\cite{zugner2018adversarial} adds adversarial perturbations to a graph to attack targeted nodes. Non-targeted attack aims to reduce the overall performance of GNNs. For example, \textit{metattack}~\cite{zugner2019adversarial} poisons the graph globally to achieve non-targeted attack with meta-learning. To defend against adversarial attacks, many efforts are taken recently~\cite{zhu2019robust,wu2019adversarial,entezari2020all,jin2020graph,tang2020transferring,zhang2020gnnguard}. \cite{wu2019adversarial} prune the perturbed edges based on Jaccard similarity of node features. Another preprocessing method by low-rank approximation of adjacent matrix is investigated~\cite{entezari2020all}. Pro-GNN~\cite{jin2020graph} is the most similar work to ours, which learns a clean graph structure by low-rank constraint. However, they only tackle the adversarial edges and their computational cost is very large due to the direct learning of the graph and the sparse low-rank constraint.
This work is inherently different from these methods as: (i) we study a novel problem of developing robust GNN for both noisy graphs and label sparsity issues; and (ii) the proposed RS-GNN simultaneously tackles the two issues by learning an link predictor to 
down-weight noisy edges and connecting nodes with high similarity to facilitate message-passing; 
and (iii) RS-GNN uses link predictor instead of direct graph learning to save computational cost. 