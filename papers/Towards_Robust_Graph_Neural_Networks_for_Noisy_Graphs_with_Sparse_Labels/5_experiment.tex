\section{Experiments}

\label{Sec:experiments}



\begin{table*}[t]
    \small
    \centering
    \caption{Node classification performance (Accuracy(\%)$\pm$Std) on various types of noisy graphs}
    \vskip -1.5em
    \begin{tabularx}{0.985\textwidth}{|p{0.05\textwidth}|p{0.14\textwidth}|CC>{\centering\arraybackslash}p{0.1\linewidth}C>{\centering\arraybackslash}p{0.1\linewidth}>{\centering\arraybackslash}p{0.08\linewidth}CC|}
    \hline
    Dataset & Graph & GCN & SuperGAT &Self-Training & RGCN & GCN-jaccard & GCN-SVD & Pro-GNN & Ours \\
    \hline
    
    \multirow{4}{*}{Cora}
        &Raw Graph            & 65.5 $\pm 0.5$& 69.0 $\pm 1.7$ & 67.9 $\pm 0.9$ & 63.0 $\pm 0.7$ &65.7 $\pm 0.6$ & 62.9 $\pm 1.1$  & 65.9 $\pm 1.3$ & \textbf{75.3} $\pm \textbf{0.6}$\\
        &Random Noise        & 59.2 $\pm 0.7$ & 58.8 $\pm 0.4$ & 63.1 $\pm 0.5$ &51.5 $\pm 0.7$ & 57.8 $\pm 1.4$ & 51.5 $\pm 0.7$ & 56.1 $\pm 3.0$ & \textbf{71.8} $\pm \textbf{1.5}$\\
        &Non-Targeted Attack  & 26.8 $\pm 2.5$ & 41.5 $\pm 1.6$ & 29.6 $\pm 0.4$ &30.4 $\pm 1.0$ & 48.3 $\pm2.0$ & 37.1 $\pm 1.4$ & 41.7 $\pm 5.7$& \textbf{70.8} $\pm \textbf{0.7}$  \\
        &Targeted Attack      & 45.3 $\pm 1.2$& 44.4 $\pm 1.3$ &46.7 $\pm 2.1$ &40.3 $\pm 1.0$ & 49.5 $\pm 1.0$ & 44.8 $\pm 0.7$ & 49.7 $\pm 0.9$ & \textbf{67.8} $\pm \textbf{1.2}$ \\

    \hline
    \multirow{4}{*}{Cora-ML}
        &Raw Graph         & 72.4 $\pm 0.8$ & 73.8 $\pm 1.4$ & 72.7 $\pm 1.4$ & 72.9 $\pm 0.7$ & 71.0 $\pm 1.2 $ & 71.1 $\pm 1.0$ & 62.0 $\pm 1.5$ & \textbf{75.6} $\pm \textbf{0.4}$\\
        &Random Noise       & 62.3 $\pm 0.6$ & 63.7 $\pm 0.9$ & 62.8 $\pm 1.3$ & 61.4 $\pm 1.1$ & 61.3 $\pm 0.5$ & 62.6 $\pm 0.6$ & 57.1 $\pm 2.1$ & \textbf{72.9} $\pm \textbf{0.7}$\\
        &Non-Targeted Attack & 13.2 $\pm 1.4$ & 18.6 $\pm 1.5$ & 15.0 $\pm 0.7$ & 11.0 $\pm 1.0$ & 48.9 $\pm 5.3$ & 16.3 $\pm 0.6$ & 18.2 $\pm 2.4$ & \textbf{73.2} $ \pm \textbf{1.2}$ \\
        &Targeted Attack     & 55.7 $\pm 0.7$ & 56.5 $\pm 1.7$ & 57.7 $\pm 1.2$ & 54.6 $\pm 0.6$ & 61.2 $\pm 0.9$ & 53.0 $\pm 0.8$ & 55.1 $\pm 1.6$ & \textbf{70.8} $\pm \textbf{0.7}$ \\

    \hline
    \multirow{4}{*}{Citeseer}
        &Raw Graph           & 64.8 $\pm 1.4$ & 64.2 $\pm 1.7$ & 65.7 $\pm 1.1$ & 56.6 $\pm 1.2$ & 62.2 $\pm 2.0$ & 61.3 $\pm 2.0$ & 60.6 $\pm 2.0 $  & \textbf{71.2} $\pm \textbf{1.4}$\\
        &Random Noise       & 57.0 $\pm 1.2$ & 54.6 $\pm 1.3$ & 58.7 $\pm 2.1$ & 48.2 $\pm 1.2$ & 61.1 $\pm 2.8$ & 48.3 $\pm 1.6$ & 54.4 $\pm 2.6$ & \textbf{68.8} $\pm \textbf{1.5}$\\
        &Non-Targeted Attack & 26.6 $\pm 2.5$ & 42.3 $\pm 2.6$ & 28.8 $\pm 2.7$ &26.6 $\pm 1.1$ & 57.9 $\pm 2.7$ & 41.7 $\pm 1.6$ & 41.6 $\pm 3.1$ & \textbf{68.0} $\pm \textbf{0.4}$ \\
        &Targeted Attack     & 43.9 $\pm 1.7$ & 42.9 $\pm 0.4$ & 47.6 $\pm 1.2$ &35.3 $\pm 1.5$ & 52.5 $\pm 2.3$ & 40.5 $\pm 0.7$ & 48.1 $\pm 1.6$ & \textbf{67.2} $\pm \textbf{1.3}$\\
    \hline
    \multirow{4}{*}{Pubmed}
    & Raw Graph          & 85.9 $\pm 0.1$ & 86.0 $\pm 1.2$ & 86.1 $\pm 0.2$ & 85.1 $\pm 0.1$ & 86.0 $\pm 0.1$  & 83.0 $\pm 0.1$ & 86.1 $\pm 0.1$ & \textbf{86.9} $\pm \textbf{0.1}$  \\
    & Random Noise & 80.5 $\pm 0.1$ & 79.8 $\pm 0.1$ & 81.2 $\pm 0.2$ & 79.7 $\pm 0.1$ &  83.0 $\pm 0.1$  & 82.0 $\pm 0.1$ &  85.1 $\pm 0.2$ &  \textbf{86.4} $\pm \textbf{0.1}$\\
    & Non-Targeted Attack & 73.7 $\pm 0.2$ & 73.8 $\pm 0.2$ & 73.5 $\pm 0.3$ & 73.8 $\pm 0.3$ & 84.4 $\pm 0.1$ & 83.0 $\pm 0.1$ & 86.0 $\pm$ 0.1 & \textbf{86.3} $\pm \textbf{0.1}$ \\
    & Targeted Attack & 76.5 $\pm 0.1$ & 75.6 $\pm 0.1$ & 76.8 $\pm 0.2$ & 76.2 $\pm 0.2$  & 82.7 $\pm 0.2$ &78.1 $\pm 1.3$ & 79.1 $\pm 0.1$ & \textbf{84.3} $\pm \textbf{0.2}$ \\
    \hline
    \end{tabularx}
    
    \label{tab:results}
    \vskip -1.2em
\end{table*}

In this section, we evaluate the proposed RS-GNN on noisy graphs with limited labels to answer the following research questions:
\begin{itemize}[leftmargin=*]
    \item \textbf{RQ1} How robust is the proposed framework on various types of noisy graphs with limited labeled nodes?
    \item \textbf{RQ2} How does the proposed framework perform under various label rates and graph sparsity levels?
    \item \textbf{RQ3} What are the contributions of link predictor and label smoothness regularization from predicted edges on RS-GNN?
\end{itemize}
\subsection{Experimental Settings}
\label{Sec:ex_settings}


\subsubsection{Datasets} 
\label{Sec:datasets}
For a fair comparison, we conduct experiments on four widely used benchmark datasets, i.e., Cora, Cora-ML, Citeseer and Pubmed~\cite{sen2008collective}.
The statistics of the datasets are presented in the Table \ref{tab:dataset} in Appendix. Note that the split of validation and testing on all datasets are the same as described in the cited papers to keep consistence. For the training set, we randomly sample 1\% of nodes as the labeled set for Cora, Cora-ML and Citeseer. For Pubmed, we randomly sample 10\% of nodes to compose the labeled set. The training node set doesn't overlap with the validation and test sets. 

\subsubsection{Noisy Graphs}
To show RS-GNN is robust to various structural noises, we evaluate RS-GNN on the following types of noises:
\begin{itemize}[leftmargin=*]
    \item \textbf{Raw Graphs}: They are the original graphs of the benchmark datasets which may contain inherent structural noise.
    \item \textbf{Random Noise}: We randomly inject fake edges and remove normal edges to add random noise to graphs.
    \item \textbf{Non-Targeted Attack}: 
    We adopt \textit{metattack}~\cite{zugner2019adversarial} to poison the graph structures by adding and removing edges, which aims to reduces the overall performance of GNNs on the whole graph. 
    \item \textbf{Targeted Attack}: It aims to lead the GNN to misclassify target nodes. Following~\cite{tang2020transferring}, we randomly select 15\% nodes as target nodes and apply \textit{nettack}~\cite{zugner2018adversarial} to perturb the graph structure. 

\end{itemize}




\subsubsection{Baselines} We compare RS-GNN with the representative and state-of-the-art GNNs, and robust GNNs against adversarial attacks:
\begin{itemize}[leftmargin=*]
    \item \textbf{GCN}~\cite{kipf2016semi}: GCN is a representative GNN which defines Graph convolution with spectral analysis.
     \item \textbf{SuperGAT}~\cite{kim2021find}: This extends GAT~\cite{velivckovic2017graph} with self-supervised learning. Edge prediction is deployed as the pretext task to guide the learning of attention to facilitate the message-passing.
    \item \textbf{Self-Training}~\cite{li2018deeper}: This is a self-supervised learning method. A GCN is firstly trained on given labels. Then, confident pseudo labels would be added to the label set to improve the GCN.
    \item \textbf{RGCN}~\cite{zhu2019robust}: It uses Gaussian distributions as representations to absorb the effects of adversarial edges. 
    \item \textbf{GCN-jaccard}~\cite{wu2019adversarial}: GCN-Jaccard eliminates edges that connect nodes with low Jaccard similarity, then apply GCN on the graph.
    \item \textbf{GCN-SVD}~\cite{entezari2020all}: This preprocessing method is based on low rank assumption. Low-rank approximation of the perturbed graph is used to train GNNs against adversarial attacks.
    \item \textbf{Pro-GNN}~\cite{jin2020graph}: It applies low-rank and sparsity constraints to learn a clean graph structure close to the noisy graph structure. 
\end{itemize}
For all the baselines, we use the implementation from the repository DeepRobust~\cite{li2020deeprobust}. All the hyperparameters of the baselines are tuned on the validation set to make a fair comparison with RS-GNN.

\subsubsection{Implementation Details}
\label{sec:implementation}
\textit{Each experiment is conducted 5 times} and average results with standard deviations are reported. The hyperparameters are tuned based on the performance of validation set. More specifically, for RS-GNN, we vary $\alpha$ as  \{0.003, 0.03, 0.3, 3, 30 \}, and $\beta$ as \{0.01, 0.03, 0.1, 0.3, 1\}. For all experiments, $T_l$, $T_h$, $\sigma$, and $Q$ are fixed as 0.1, 0.8, 100, and 50, respectively. $K$ is set as 100, 300, 400 and 10 for Cora, Cora-ML, Citeseer and Pubmed, respectively. More details about the hyperparameters sensitivity is discussed in Sec. \ref{Sec:para_analysis}.
A one-hidden layer MLP with 64 filters is applied as the link predictor. We use GCN as the backbone of RS-GNN. Various GNNs can be used in RS-GNN and we leave it as a future work. 


\begin{figure}[t]
\centering
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.85\linewidth]{figure/meta_ptb.pdf} 
    \vskip -0.5em
    \caption{Metattack}
    % \label{fig:1_a}
\end{subfigure}
%\vspace{-1em}
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.85\linewidth]{figure/random_ptb.pdf} 
    \vskip -0.5em
    \caption{Random Noise}
    % \label{fig:1_b}
\end{subfigure}
\vspace{-1.2em}
\caption{Robustness under different Ptb rates on Cora.  }
\label{fig:ptb}
\vskip -1.5em
\end{figure}

\subsection{Performance on Noisy Graphs}
To answer \textbf{RQ1}, we first compare RS-GNN with the baselines on various noisy graphs. We then evaluate the performance of RS-GNN on the graphs with different levels of structural noise.



\subsubsection{Comparisons with baselines}
We conduct experiments on four types of noisy graphs, i.e., raw graphs, graphs with random noise, non-targeted attack perturbed graphs and targeted attack perturbed graphs. The perturbation rate of non-targeted attack and targeted attack is 0.15. The perturbation rate of random noise is set as 0.3. Since we focus on noisy graph with sparse labels, we set the label rates as 0.01 for Cora, Cora-ML, Citeseer and 0.1 for Pubmed. The results are reported in Table \ref{tab:results}, where we can observe:

\begin{itemize}[leftmargin=*]
    \item With limited labeled nodes, GCN even hardly performs well on raw graph, which indicates the necessity of investigating method to address the challenge of sparsely labeled graphs. Though recent GNNs such as SuperGAT and Self-Training can improve the performance with self-supervised learning, our RS-GNN still outperforms them by a large margin. This shows the effectiveness of graph densification in dealing with sparsely labeled graphs.
    \item The structural noise further degrades the performance of GCN, but its impact to RS-GNN is negligible. RS-GNN achieves better results than the state-of-the-art robust GNNs. This indicates RS-GNN could eliminate the effects of the noisy edges.
    \item Compared with the preprocessing methods and Pro-GNN, RS-GNN achieves higher accuracy on the sparsely labeled graphs perturbed by attack methods. 
    This is because the baselines only focus on eliminating potential noisy edges, which will even result in less involvement of unlabeled nodes. 
    By contrast, RS-GNN can down-weights/removes the adversarial edges to defend the adversarial attacks and densify the graph to facilitate the message passing for predictions of unlabeled nodes.
    
\end{itemize}



\subsubsection{Robustness Under Different Ptb Rates } 
To show that RS-GNN is resistant to different levels of structural noise, we vary the perturbation rate as $\{0\%, 5\%, 10\%, \dots, 25\%\}$ and compare the performance of RS-GNN with the most effective baselines. The label rate is fixed as 0.01. Since we have similar observations on other datasets, we only report the average accuracy and standard deviation on Cora in Figure \ref{fig:ptb}. From the figure, we make following observations:
\begin{itemize}[leftmargin=*]
    \item As the perturbation rate increases, the performance of all the baselines drop significantly, which is as expected. Though the performance of RS-GNN also drops, it is much stable and consistently outperforms the baselines, which shows the robustness of RS-GNN against various levels of attacks and random noise; and %  the proposed framework RS-GNN  For the noisy graph perturbed by \textit{metattack}, the performance of GCN degrades significantly. Our proposed RS-GNN could achieve high performance under high perturbation rate and consistently outperforms the baselines, which demonstrates RS-GNN could well defend attacks under different perturbation rates.
    \item  Compared with GCN, RS-GNN uses GCN as backbone but significantly outperforms GCN, especially when the perturbation rate is large, which shows the effectiveness of eliminating the effects of noisy edges and densifying the graph to benefit the predictions given limited labels. % consistently achieve high performance on the graph containing various amounts of noisy edges. It demonstrates that RS-GNN is robust when applied to different levels of random structural noises. \suhang{TODO}
\end{itemize}

\begin{figure}[t]
\centering
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.85\linewidth]{figure/random_cora_2.pdf} 
    \vskip -0.8em
    \caption{Cora}
    % \label{fig:1_a}
\end{subfigure}
%\vspace{-1em}
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.85\linewidth]{figure/random_cora_ml_2.pdf} 
    \vskip -0.8em
    \caption{CoraML}
    % \label{fig:1_b}
\end{subfigure}
\vspace{-1.5em}
\caption{ Distributions of the weights of normal and noisy edges on the generated graph.}
\label{fig:weight}
\vskip -1em
\end{figure}

\subsection{Analysis of the Learned Graph} 
To demonstrate that RS-GNN could alleviate negative effects of noisy edges by downweighting the noisy edges, we investigate the distribution of the learned edge weights $\mathbf{S}_{ij}$ of normal and noisy edges in this subsection. The edge weight  distributions of graphs perturbed by random noise with 30\% perturbation rate on Cora and Cora-ML are shown in Fig.~\ref{fig:weight}.  From this figure, we observe: (\textbf{i}) The weights of noisy edges are significantly lower than the weights of normal edges, which indicates RS-GNN manages to reduce the effects of noisy edges for robust GNN; and (\textbf{ii}) Although most normal edges have higher weights, some of their weights are very low, which implies inherent noise exists in the graph and RS-GNN is able to get rid of such inherent structural noise. 

We also provide more details about the number of involved unlabeled nodes with the learned graph in Appendix~\ref{sec:app_graph}, which proves RS-GNN can enhance the involvement of unlabeled nodes.






\begin{figure}[t]
\centering
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figure/clean_label_rate.pdf} 
    \vskip -0.5em
    \caption{Raw Graph}
    % \label{fig:1_a}
\end{subfigure}
%\vspace{-1em}
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figure/ptb_label_rate.pdf} 
    \vskip -0.5em
    \caption{Metattack with 15\% Ptb}
    % \label{fig:1_b}
\end{subfigure}
\vspace{-1.2em}
\caption{Performance on Cora with different label rates.  }
\label{fig:label_rate}
\vskip -0.8em
\end{figure}


\subsection{Impacts of Label Rate and Graph Sparsity}

To answer \textbf{RQ2}, we study the impacts of the number of labeled nodes and sparsity of the graph by varying the label rate and edge rate of the graph. The hyperparameters are selected with the process described in Sec. \ref{sec:implementation}. Each experiment is conducted 5 times and average accuracy with standard deviation are reported.



\subsubsection{Impacts of Label Rate} We vary label rates as \{0.01, 0.02,\dots, 0.06\}. Experiments are conducted on raw graphs and graphs perturbed by \textit{mettack} to study the effectiveness of RS-GNN under various label rates. The results on Cora are shown in Fig.~\ref{fig:label_rate}. We have similar observations on other datasets. From Fig.~\ref{fig:label_rate}, we observe:
\begin{itemize}[leftmargin=*]
    \item Generally, as the increase of label rate, the performances of all the methods increase, which is as expected.
    \item For the raw graph, though RS-GNN consistently outperforms the baselines, as the label rate increases, the improvement of RS-GNN becomes marginal. This is because the raw graph doesn't contain much noise. Thus, as label rate increases to 6\%, there are already adequate labels. Since higher label rates would result in more unlabeled nodes involving in the training, the effects of densifying graphs and label smoothness become less significant; %when the label rate is high enough to involve most of the unlabeled nodes.
    \item For the metattack graph, as the label rate increases, RS-GNN still significantly outperforms baselines. That's because the training graph contains a lot of adversarial edges. Though we have enough training labels, the adversarial edges can still contaminate the message passing of GNNs. But RS-GNN can eliminate noisy edges and densify the graph, thus having better results.
\end{itemize}



\subsubsection{Impacts of Graph Sparsity} As RS-GNN can generate dense graphs, it should have the ability to handle sparse graphs. Thus, we randomly select $x\%$ edges from the raw graph to build graphs of different sparsity levels. We vary edge rate $x\%$ from 20\% to 100\% with a step of 40\%. Since we are interested in how the sparsity of the graph could affect RS-GNN in generating dense graphs, we only focus on the performance on raw graphs. 
The average results of 5 runs on Citeseer are reported in Table~\ref{tab:sparsity}. From the table, we have the following observations:
\begin{itemize}[leftmargin=*]
    \item As the edge rate decreases, the performance of all the methods decrease, which is because message-passing of GNNs becomes ineffective on very sparse graphs;
    \item RS-GNN consistently outperforms the baselines. In particular, when the graph becomes more sparse, the improvement of RS-GNN over the baselines becomes larger. For example, the improvement of RS-GNN over GCN on Citeseer is 6.4\% when Edge Rate is 100\%, and becomes 9.2\% when Edge Rate is 20\%, which shows the importance of generating edges for densifying the graph and smoothing predictions with the learned graph. 
\end{itemize}


\begin{table}[t]
    \small
    \centering
    \caption{Accuracy (\%) on Citeseer in different sparsity levels. }
    \vskip-1.5em
    \begin{tabularx}{0.96\linewidth}{>{\centering\arraybackslash}p{0.20\linewidth}CCC}
    \toprule
    Edge Rate (\%) & GCN & Pro-GNN & RS-GNN\\
    \midrule
    % \multirow{3}{*}{Cora} 
    % & 20 & 51.9 $\pm 2.0$ & 49.5 $\pm 0.8$ & \textbf{64.7} $\pm \textbf{1.7}$\\ 
    % % & 40 & 53.9 $\pm 1.2$ & 51.2 $\pm 0.7$ & \textbf{66.3} $\pm \textbf{1.2}$ \\
    % & 60 & 62.0 $\pm 0.3$ & 62.5 $\pm 0.5$ & \textbf{68.5} $\pm \textbf{1.7}$\\
    % % & 80 & 64.2 $\pm 0.5$ & 64.6 $\pm 0.2$ & \textbf{72.8} $\pm \textbf{1.4}$ \\
    % & 100 & 65.5 $\pm 0.5$ & 65.9 $\pm 1.1$ & \textbf{75.3} $\pm \textbf{0.6}$\\
    % \hline

    20 & 54.5 $\pm 1.2$ & 55.2 $\pm 1.6$ & \textbf{63.7} $\pm \textbf{2.2}$\\
    % & 40 & 56.8 $\pm 1.1$ & 58.6 $\pm 1.5$ & \textbf{67.3} $\pm \textbf{1.8}$\\
    60 & 58.7 $\pm 1.8$ & 58.3 $\pm 2.4$ &
    \textbf{69.8} $\pm \textbf{1.1}$\\
    % & 80 & 60.1 $\pm 2.2$ &60.2 $\pm 1.3$ & \textbf{70.4} $\pm \textbf{0.7}$\\
    100 & 64.8 $\pm 1.4$ & 60.6 $\pm 2.0$ &
    \textbf{71.2} $\pm \textbf{1.4}$\\
    \bottomrule
    \end{tabularx}
    \label{tab:sparsity}
    \vskip -1.em
\end{table}




%\vspace{-1em}

\begin{figure}[h]
\centering
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.85\linewidth]{figure/neta_abl.pdf} 
    \vskip -0.5em
    \caption{Nettack}
    \label{fig:abla_neta}
\end{subfigure}
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.85\linewidth]{figure/abla_meta.pdf} 
    \vskip -0.5em
    \caption{Metattack with 15\% Ptb}
    \label{fig:abla_meta}
\end{subfigure}
\vspace{-1.3em}
\caption{Ablation studies on Cora with different label rates.}
\label{fig:abl}
\vskip -1.8em
\end{figure}
\subsection{Ablation Study}
To answer \textbf{RQ3}, we conduct ablation studies to understand the effects of graph densification, graph purification and label smoothness regularization. In RS-GNN, the link predictor densify the graph to enhance the performance on unlabeled nodes. To demonstrate the effects of adding edges with the link predictor, we remove the process of adding edges and obtain RS-GNN$\backslash$A. 
To testify the effectiveness of the label smoothness regularization based on the generated graph, we eliminate the label smoothness regularization and get RS-GNN$\backslash$U. To show our link predictor can eliminate the effects of noisy edges, we compare a variant named as RS-GNN$\backslash$AU which only use the link predictor to denoise graphs. Graph desification and label smoothness are not applied in RS-GNN$\backslash$AU. 
We also implement a variant named as RS-GNN$_{GCN}$ which uses GCN as link predictor to show that the noisy edges would largely affects the GNNs for link prediction. Hyperparameters selection follows the process in Sec~\ref{sec:implementation}. We only show the results on the Cora graph perturbed with \textit{metattack} and random noise, because similar trends are observed on other datasets. Results are presented in Fig.~\ref{fig:abl}. From this figure, we observe that: 
\begin{itemize}[leftmargin=*]
    \item RS-GNN performs much better than RS-GNN$\backslash$A and RS-GNN$\backslash$U, which shows that densifying graphs and label smoothness with the learned graph can address the label sparsity issue;
    \item With the increase of label rate, the gap between RS-GNN and RS-GNN$\backslash$U will be narrowed. This is consistent with our analysis that higher label rates would involve more unlabeled nodes;
    \item RS-GNN$_{GCN}$ performs much worse than RS-GNN, which indicates adversarial edges would impair GCN and result in a poor link predictor for denoising and densification.
\end{itemize} 


\begin{figure}[t]
\centering
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.98\linewidth]{figure/cora_para_clean_2}
    \vskip -0.5em
    \caption{Raw Graph}
    \label{fig:para_raw}
\end{subfigure}
%\vspace{-1em}
\begin{subfigure}{0.49\columnwidth}
    \centering
    \includegraphics[width=0.98\linewidth]{figure/cora_para_ptb_2}
    \vskip -0.5em
    \caption{Metattack with 15\% Ptb}
    \label{fig:para_meta}
\end{subfigure}
\vspace{-1em}
\caption{Parameter sensitivity analysis on Cora.}
\label{fig:para}
\vskip -1.7em
\end{figure}


\subsection{Parameter Sensitivity Analysis}
\label{Sec:para_analysis}
In this subsection, we explore the sensitivity of the most crucial hyperparameters $\alpha$ and $\beta$ which are in the final objective function of RS-GNN. The analysis about other hyperparameters is presented in the supplementary material. $\alpha$ controls how well the link predictor reconstructs the noisy graph and $\beta$ controls the contribution of label smoothness. To investigate the effects of $\alpha$ and $\beta$, we vary the values of $\alpha$ as $\{0.003, 0.03, 0.3, 3, 30\}$ and $\beta$ as $\{0.01, 0.03, 0.1, 0.3, 1, 3\}$ on Cora. The results are shown in Fig~\ref{fig:para}. In the raw graph, when $\alpha$ is large, the accuracy is stable and high. But if the $\alpha$ is too large in the perturbed graph, the performance would decrease. This difference is due to the noise levels of the raw graph and the perturbed graph. The structural noise in the perturbed graph is severe, faithfully reconstructing the perturbed graph with high $\alpha$ would lead to a poor link predictor. As for the $\beta$, a value between 0.03 to 0.3 generally gives good performance, which eases the parameter selection.