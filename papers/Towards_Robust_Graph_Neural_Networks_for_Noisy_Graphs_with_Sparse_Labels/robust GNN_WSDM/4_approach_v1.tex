\section{Proposed Framework -- RS-GNN}
\label{sec:methodology}
In this section, we present the details of the proposed RS-GNN. 
The main challenges are: (i) given the noisy graph, how can we learn a link predictor which can down-weight/eliminate noisy edges and densify the graph; and (ii) how to simultaneously use the  learned graph for node classification. As the graph topology is noisy, we cannot directly apply a GNN on $\mathcal{G}$ to predict edges because the message passing would magnify the negative effects of the noisy edges. Generally, nodes sharing similar features tend to connect to each other; while noisy edges tend to connect nodes of dissimilar nodes. Thus, we propose 
to learn a MLP-based link predictor which predicts links using node attributes. The more similar the node features of two nodes are, the larger weights the link predictor will assign. Thus, the link predictor is able to down-weight or eliminate noisy edges in the initial graph. Meanwhile, the edge predictor can predict missing links to alleviate label sparsity issue. We design a novel feature similarity weighted edge-reconstruction loss to train the link predictor so as to reduce the negative effects of noisy edges on the link predictor.
% we propose to use the clean node attributes to learn a dense and clean graph, and adopt $\mathbf{A}$ to supervise the learning process as most links in $\mathbf{A}$ are clean. 
An illustration of the framework is shown in Figure \ref{fig:framework}, which contains a link predictor $f_E$ and a GCN classifier $f_{\mathcal{G}}$. The link predictor $f_E$ takes node features as input to learn a dense adjacency matrix $\mathbf{S}$, aiming to remove adversarial edges and assign edges that benefit predictions. The GCN classifier $f_{\mathcal{G}}$ takes $\mathbf{S}$ and node features $\mathbf{X}$ to predict the node labels with the node features. Finally, label smoothness constraint based on the predicted edges will be added to the predictions of unlabeled nodes to further alleviate label sparsity issue. Next, we give the details of each component.


\subsection{Link Prediction}
As the given graph contains structural noises and has missing edges, we propose to learn a new graph that down-weights noisy edges to eliminate their negative effects and completes the missing links to facilitate  GNN in dealing noisy graphs with sparse labels. 

\noindent\textbf{Building Link Predictor.} Generally, noisy edges connect two nodes with dissimilar node features; while nodes of similar features are likely to have similar labels and should be connected. Therefore, we propose to predict edge weights and missing edges between nodes using nodes features.
Specifically, for node $v_i$, a MLP takes its node attributes $\mathbf{x}_i$ to learn its node representation as: $\mathbf{z}_i = MLP(\mathbf{x}_i)$.
% \end{equation}
With the node representations, we predict the weight $w(i,j)$ between $v_i \in \mathcal{V}$ and $v_j \in \mathcal{V}$ as:
\begin{equation}
    w(i,j) = f(\mathbf{z}_i^T \mathbf{z}_j),
    \label{eq:MLP}
\end{equation}
where $f$ is the activation function. For $f$, we use ReLU instead of sigmoid as we find that when the learned adjacency matrix is used as the input of GCN, the use of sigmoid function will lead to gradient vanishing, which is consistent with previous observations~\cite{he2017neural}. Note that we use MLP instead of a GNN as the link predictor because the graph structure is noisy and the message passing of GNN could magnify the negative effects.


\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figure/Framework.png}
    \vskip -1em
    \caption{An illustration of the proposed RS-GNN.}
    \label{fig:framework}
    \vskip -1.8em
\end{figure}

\noindent\textbf{Learning Link Predictor.} Our goal is to learn a link predictor which can (i) assign small weights to two nodes of different features so as to eliminate noisy edges; and (ii) assign larger weights to two nodes of similar node features so as to densify the graph to facilitate message passing.
As for many real-world graphs, similar nodes tend to link together and linked nodes usually have high feature similarity. Thus, to learn a good link predictor $f_E$, we utilize the adjacency matrix reconstruction as the loss function. % Though the graph is noisy, it usually only contains a very small portion of noisy edges. For example, adversarial attacks on graph structures are carried out within a limited perturbation budget to make the perturbations unnoticeable. 
%The clean edges (1) and clean negative samples (0) will dominate the loss function in training the link predictor, resulting in a good link predictor.
Since the graph is sparse, the adjacency matrix $\mathbf{A}$ contains many zero entries. Directly adopting adjacency matrix reconstruction as the loss function would (i) result in poor performance as the link predictor will be biased on predicting missing links; and (ii) require large computational cost as we need to calculate $N^2$ edges. To address this problem, negative sampling~\cite{mikolov2013distributed} is adopted, i.e., for each $v_j \in \mathcal{N}(v_i)$, we randomly sample $Q$ nodes that's not connected to $v_i$ and use them as negative samples. 

However, a small portion of edges in $\mathbf{A}$ are noisy, which might have negative effects in training the predictor. To mitigate the negative effects of noisy edges and to learn a link predictor that can assign lower weights to edges that link dissimilar nodes, we propose to reweight the positive and negative samples based on the feature similarity of two nodes. Specifically, for node  $v_i$ and its positive sample $v_j \in \mathcal{N}(v_i)$, we minimize $\exp(-\frac{\|\mathbf{x}_i-\mathbf{x}_j\|^2}{\sigma^2}) (w(i,j)-1)^2$, where $\sigma$ is the hyperparameter to control the variance of the sample weights. Thus, if the node features of $v_i$ and $v_j$ are similar, $A_{ij}$ is likely to be a clean edge and $\exp(-\frac{\|\mathbf{x}_i-\mathbf{x}_j\|^2}{\sigma^2})$ would be large. Minimizing the loss will force $w(i,j)$ to be close to 1; while if the features are dissimilar, then $A_{ij}$ is likely to be a noisy edge and $\exp(-\frac{\|\mathbf{x}_i-\mathbf{x}_j\|^2}{\sigma^2})$ would be small, thus minimizing the loss will have little effect on $w(i,j)$. Similarly, for $v_i$ and its negative sample $v_n$, we minimize $\exp(\frac{\|\mathbf{x}_i-\mathbf{x}_n\|^2}{\sigma^2}) (w(i,n)-0)^2$. If the node features of $v_i$ and $v_n$ are dissimialr, then $\exp(\frac{\|\mathbf{x}_i-\mathbf{x}_n\|^2}{\sigma^2})$ is large, minimizing the loss would make $w(i,n)$ close to 0 as expected. With the weight defined in this way, the loss for training the link predictor is:
\begin{equation}
\small
\begin{aligned}
    \mathcal{L}_E = \sum_{v_i \in \mathcal{V}} & \sum_{v_j \in \mathcal{N}(v_i)} \Big[\exp(-\frac{\|\mathbf{x}_i-\mathbf{x}_j\|^2}{\sigma^2}) (w(i,j)-1)^2 \\
    & +   \sum_{n=1}^{Q} \cdot \mathbb{E}_{v_n \sim P_n(v_i)} \exp(\frac{\|\mathbf{x}_i-\mathbf{x}_n\|^2}{\sigma^2}) (w(i,n)-0)^2\big],
    \label{eq:edge}
\end{aligned}
\end{equation}
where $P_n(v_i)$ is the distribution of sampling negative nodes for $v_i$, which is a uniform distribution. With the loss function Eq.(\ref{eq:edge}), the link predictor would be able to downweight the noisy edges and densify the graph to facilitate the learning of robust GNN on noisy graph with limited labels. 

\noindent\textbf{Graph Denoising and Densification.} 
With the link predictor, we could apply the learned weights to the existing edges and drop edges whose predicted weights are small to eliminate the negative effects of noisy/adversarial edges. Moreover, to increase the involvement of unlabeled nodes to facilitate the message passing of GNNs, we also link nodes that have large weights predicted by the link predictor. However, if we predict weights of all pairs of nodes, the computation cost will be very large because we will train a link predictor and a GNN classifier end-to-end as shown in Sec.~\ref{sec:4_4}, which means we need to do prediction in each iteration. To save the computational cost, for each node $v_i$, we first construct a candidate subset $\mathcal{S}(v_i)$, which contains $K$ nodes having the largest cosine similarities with $v_i$ in the raw feature space $\mathbf{X}$. Note that this only needs to be done once. Since nodes not in $\mathcal{S}(v_i)$ are not likely to be connected with $v_i$, we only need to compute weights between $v_i$ and $\mathcal{S}(v_i)$. The whole process of obtaining a clean and dense adjacency matrix $\mathbf{S}$ could be formally stated as:
\begin{equation}
    \mathbf{S}_{ij} = \left\{ \begin{array}{ll}
         w(i,j) & \mbox{if $w(i,j) > T_l$ and $v_j \in \mathcal{N}(v_i) \cup \mathcal{S}(v_i)$} ;\\
        0 & \mbox{else},\end{array} \right.
        \label{eq:generate_graph}
\end{equation}
where $\mathcal{N}(v_i)$ are neighbors of $v_i$ in the noisy graph, and $T_l$ is a threshold to determine whether we should keep/add the edge.  With the above operation, those noisy edges would be assigned smaller weights or even dropped, which mitigate the negative effects of noisy edges. Meanwhile, more edges are introduced to facilitate the message passing of GNNs during training.

\subsection{GNN for Node Classification}
With the learned adjacency matrix $\mathbf{S}$, we can apply GNNs to learn the node representation as $\mathbf{H} = GNN(\mathbf{S}, \mathbf{X})$. 
% \begin{equation}
%     \mathbf{H} = GNN(\mathbf{S}, \mathbf{X})
% \end{equation}
Note that the proposed framework is a flexible framework which can facilitate various GNNs such as GAT~\cite{velivckovic2017graph} and GIN~\cite{xu2018powerful}. With the node representation, the label of node $v_i$ can be predicted as
$\hat{y}_i = softmax(\mathbf{h}_i)$, where $\mathbf{h}_i$ is the representation of node $v_i$. Then, the training loss is:
\begin{equation}
\small
    \mathcal{L}_{GNN} = \sum_{v_i \in \mathcal{V}_L} l(\mathbf{\hat{y}}_i, \mathbf{y}_i)
    \label{eq:GNN_dense}
\end{equation}
where $l(\mathbf{\hat{y}}_i, \mathbf{y}_i)$ is the cross entropy between $\hat{y}_i$ and $\mathbf{y}_i$. Since $\mathbf{S}$ is denser than the original graph, more unlabeled nodes are involved in the training even with limited amount of labeled nodes, thus making the propagation of information more efficient.

\subsection{Label Smoothness on Unlabeled Nodes}
%As it is discussed in Sec. \ref{sec:3_3}, purely relying on densifying the graphs could not ensure all the unlabeled nodes participate the training process when the available labels is extremely limited. 
Though the dense graph $\mathbf{S}$ can help to include more unlabeled nodes in the loss function, their information is propagated through the message-passing mechanism instead of being directly used in the training loss. To further alleviate the issue of limited labeled nodes, we  propose to adopt the predicted weighted edges for label smoothness regularization. The basic idea is the larger weights of an edge $S_{ij}$ is, the more likely that $v_i$ and $v_j$ have the same label~\cite{wang2019knowledge}.
% To ensure the unlabeled nodes are involved, we propose to further apply label smoothness regularization to unlabeled nodes and their neighbors to utilize the self-supervision from the generated graph adjacency matrix. 
%The label smoothness posits the adjacent items in the graph are likely to have similar labels~\cite{wang2019knowledge,zhang2007hyperparameter}. Since the graph might be perturbed to link nodes with different labels, the label smoothness is based on the generated graph instead of the provided graph.The nodes predicted to be likely linked by link predictor are forced to have similar predictions. 
Thus, for an unlabeled node $v_i$, if its edge weight with node $v_j$ is larger than a threshhold $T_h$, i.e., $S_{ij} > T_h$, we want their predicted labels to be similar with each other. This can be formally written as
%This self-supervision term can be written as:
\begin{equation}
    \mathcal{L}_u = \sum_{v_i \in \mathcal{V}_u}\sum_{v_j \in \mathcal{V}} \mathbf{T}_{ij} \|\mathbf{\hat{y}}_i-\mathbf{\hat{y}}_j\|^2,
\end{equation}
where $\mathcal{V}_u$ denotes the set of unlabeled nodes, $\mathbf{\hat{y}}_i$ and $\mathbf{\hat{y}}_j$ represent the predictions of node $v_i \in \mathcal{V}_u$ and $v_j \in \mathcal{V}$, respectively. $\mathbf{T}_{ij}=\mathbf{S}_{ij}$ if $\mathbf{S}_{ij}>T_h$; otherwise 0.
In this way, we explicitly smooth the predicted labels between unlabeled nodes and nodes that are similar to them. By including $\mathbf{T}_{ij}$ in $\mathcal{L}_u$, edge weights are also considered. 

\subsection{Final Objective Function of RS-GNN} \label{sec:4_4}
With the link predictor denoising and densifying the graph with the supervision from $\mathbf{A}$, the GNN adopting the learned graph for label prediction and the label smoothness regularization from the generated graph, the final loss function can be written as
\begin{equation}
    \mathop{\arg \min}_{\theta_E,\theta_{\mathcal{G}}} \mathcal{L}_{GNN} + \alpha \mathcal{L}_E + \beta \mathcal{L}_u,
    \label{eq:final}
\end{equation}
where $\theta_E$ and $\theta_{\mathcal{G}}$ are parameters of link predictor $f_E$ and GNN classifier $f_\mathcal{G}$, respectively. $\alpha$ and $\beta$ are hyperparameters to balance the contributions of reconstructing the adjacency matrix with $f_E$ and label smoothness regularization. The proposed framework is an end-to-end framework where we simultaneously learn the link predictor and utilize the predicted edges for training a robust GNN to alleviate the noisy graph and limited labeled nodes issues. The training algorithm is shown in the supplementary material.
% \suhang{The training algorithm of RS-GNN is presented in XXX. We need to mention the supplementary material}

% \subsection{A Training Algorithm of RS-GNN}
% The training algorithm of RS-GNN is presented in Algorithm \ref{alg:Framwork}. In line 1, link predictor $f_E$ and GCN classifier $f_{\mathcal{G}}$ are randomly initialized with Xavier initialization~\cite{glorot2010understanding}. In line 2, we generate the graph with $f_E$. 
% Then the link predictor and GCN classifier are jointly trained in an end-to-end manner by Eq. (\ref{eq:final}) in line 3. Adam optimizer~\cite{kingma2014adam} with learning rate set as 0.001 is applied to update the parameters of link predictor and GCN classifier.
% \label{sec:app_alg}
% \begin{algorithm}[t] 
% \caption{ Training Algorithm of RS-GNN.} 
% \label{alg:Framwork} 
% \begin{algorithmic}[1]
% \REQUIRE
% $\mathcal{G}=(\mathcal{V},\mathcal{E}, \mathbf{X})$, $\mathcal{Y}$, $K$, $Q$ $T_l$, $T_h$, $\sigma$, $\alpha$ and $\beta$.
% \ENSURE $f_{\mathcal{G}}$ and $f_E$
% \STATE Randomly initialize the parameters of $f_{\mathcal{G}}$ and $f_E$.
% \REPEAT 
% % \STATE Randomly select $Q$ negative samples for each node
% \STATE Obtain the generated graph $\mathbf{S}$ with $f_E$ by Eq.(\ref{eq:generate_graph}).
% \STATE Jointly optimize the GCN classifier parameters $\theta_{\mathcal{G}}$ and the link predictor parameters $\theta_E$ by Eq.(\ref{eq:final}). 

% \UNTIL convergence
% \RETURN $f_{\mathcal{G}}$ and $f_E$
% \end{algorithmic}
% \end{algorithm}



