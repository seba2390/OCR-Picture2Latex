\appendix
\begin{table}[t]
    \small
    \caption{Statistics of datasets.}
    \vskip-1em
    \centering
    \begin{tabularx}{0.98\linewidth}{p{0.15\linewidth}CCCC}
    \toprule
         & Cora & Cora-ML & Citeseer & Pubmed   \\
    \midrule
    \#nodes & 2,485 & 2,810 & 2,110 & 19,717\\
    \#edges & 5,069 & 7,981 & 3,668 & 44,338\\
    \#features & 1,433 & 2,879 & 3,703 & 500\\
    \#classes & 7 & 7 & 6 & 3\\
    \bottomrule
    \end{tabularx}
    \label{tab:dataset}
    % \vskip -1em
\end{table}
\section{A Training Algorithm of RS-GNN}
\label{sec:app_alg}
\begin{algorithm}[t!] 
\caption{ Training Algorithm of RS-GNN.} 
\label{alg:Framwork} 
\begin{algorithmic}[1]
\REQUIRE
$\mathcal{G}=(\mathcal{V},\mathcal{E}, \mathbf{X})$, $\mathcal{Y}$, $K$, $Q$ $T_l$, $T_h$, $\sigma$, $\alpha$ and $\beta$.
\ENSURE $f_{\mathcal{G}}$ and $f_E$
\STATE Randomly initialize the parameters of $f_{\mathcal{G}}$ and $f_E$.
\REPEAT 
% \STATE Randomly select $Q$ negative samples for each node
\STATE Get the denoised and densified graph  $\mathbf{S}$ with $f_E$ by Eq.(4).
\STATE Input the learned graph $\mathbf{S}$ and node attributes $\mathbf{X}$ to GCN classifier $f_{\mathcal{G}}$ to get robust predictions.
\STATE Jointly optimize the GCN classifier parameters $\theta_{\mathcal{G}}$ and the link predictor parameters $\theta_E$ by Eq.(7). 

\UNTIL convergence
\RETURN $f_{\mathcal{G}}$ and $f_E$
\end{algorithmic}
\end{algorithm}
The training algorithm of RS-GNN is presented in Algorithm \ref{alg:Framwork}. In line 1, link predictor $f_E$ and GCN classifier $f_{\mathcal{G}}$ are randomly initialized. In line 2, we generate the graph with $f_E$. 
Then the link predictor and GCN classifier are jointly trained in an end-to-end manner by Eq. (7) in line 3. 
Adam optimizer with learning rate set as 0.001 is applied to update all the parameters.



\begin{table}[h]
    \small
    \centering
    \caption{Number of involved unlabeled nodes}
    \vskip -1em
    \begin{tabularx}{0.95\linewidth}{p{0.24\linewidth}XXXX}
    \toprule
        Dataset &  Cora & CoraML & Citeseer & Pubmed \\
    \midrule
        Raw Graph&  212 & 447 & 168 & 12,430 \\
        Generated Graph & 1,383 & 2,161 & 955 & 18,555\\
    \bottomrule
    \end{tabularx}
    % \vskip -1.2em
    \label{tab:neighbs}
\end{table}
\section{More details of the Learned Graph}

\label{sec:app_graph}
Since RS-GNN aims to densify the graphs to benefit predictions in sparsely labeled graphs, we compare the number of involved unlabeled nodes in raw and generated graphs. More specially, in a two layer GNN, the neighbors of labeled nodes within two hops will participate in the training process. The generated graphs are attained by training RS-GNN on graphs perturbed by random noise. We binarize weighted edges by setting 0.5 as the threshold. The comparisons are given in Table \ref{tab:neighbs}. We can find that more unlabeled nodes are involved in the training with the generated graphs, which implies that RS-GNN could promote predictions of unlabeled nodes by densifying graphs.
\section{The Impacts of Hyperparmeters}
\label{sec:app_hyperparameter}




\begin{table}[t]
    \small
    \centering
    \caption{The impacts of hyperparameter $K$.}
    \vskip -1em
    \begin{tabularx}{0.88\linewidth}{XCCCC}
    \toprule
    $K$  &  50 & 100 & 200 & 400    \\
    \midrule
    Cora &  66.4 $\pm 1.8$ & \textbf{70.8} $\pm \textbf{0.7}$ & 69.5 $\pm 2.9$ & 68.2 $\pm 3.3$\\
    Cora-ML & 44.8 $\pm 1.2$ & 53.8 $\pm 2.7$ & \textbf{73.2} $\pm \textbf{1.2}$ & 69.0 $\pm 5.0$\\
    Citeseer & 63.3 $\pm 2.0$ & 66.0 $\pm 1.4$ & \textbf{68.0} $\pm \textbf{0.4}$ & 67.8 $\pm 1.4$\\
    \bottomrule
    
    \end{tabularx}

    \label{tab:para_K}
    % \vskip -1em
\end{table}

\begin{table}[t]
    \small
    \centering
    \caption{The impacts of hyperparameter $T_h$.}
    \vskip -1em
    \begin{tabularx}{0.88\linewidth}{XCCCC}
    \toprule
    $T_h$  &  0.6 & 0.7 & 0.8 & 0.9    \\
    \midrule
    Cora &  68.3 $\pm 0.7$ & 68.9 $\pm 1.5$ & \textbf{70.8} $\pm \textbf{0.7}$ & 69.8 $\pm 2.1$\\
    Cora-ML & 64.8 $\pm 4.1$ & 68.2 $\pm 3.6$ & \textbf{73.2} $\pm \textbf{1.2}$ & 69.2 $\pm 4.8$ \\
    Citeseer & 66.6 $\pm 1.7$ & 67.5 $\pm 2.1$ & \textbf{68.0} $\pm \textbf{0.4}$ & 67.8 $\pm 2.2$\\
    \bottomrule
    
    \end{tabularx}

    \label{tab:para_t}
    % \vskip -1em
\end{table}

\begin{table}[t]
    \small
    \centering
    \caption{The impacts of hyperparameter $T_l$.}
    \vskip -1em
    \begin{tabularx}{0.88\linewidth}{XCCCC}
    \toprule
    $T_l$  &  0.0 & 0.05 & 0.1 & 0.2   \\
    \midrule
    Cora &  65.5 $\pm 2.8$ & 68.5 $\pm 3.3$ & \textbf{70.8} $\pm \textbf{0.7}$ & 70.3 $\pm 1.4$\\
    Cora-ML & 65.9 $\pm 2.6$ & 72.5 $\pm 1.3$ & \textbf{73.2} $\pm \textbf{1.2}$ & 69.6 $\pm 3.9$ \\
    Citeseer & 65.8 $\pm 0.6$ & 66.8 $\pm 0.8$ & \textbf{68.0} $\pm \textbf{0.4}$ & 66.6 $\pm 1.3$\\
    \bottomrule
    
    \end{tabularx}

    \label{tab:para_l}
    % \vskip -1em
\end{table}

\begin{table}[t]
    \small
    \centering
    \caption{The impacts of hyperparameter $\sigma$.}
    \vskip -1em
    \begin{tabularx}{0.88\linewidth}{XCCCC}
    \toprule
    $\sigma$  &  30 & 100 & 300 & 1000    \\
    \midrule
    Cora &  70.2 $\pm 1.2$ & \textbf{70.8} $\pm \textbf{0.7}$ & 70.1 $\pm 1.1$ & 68.9 $\pm 2.8$\\
    Cora-ML & 72.7 $\pm 1.0$ & \textbf{73.2} $\pm \textbf{1.2}$ & 72.5 $\pm 0.8$ & 72.4 $\pm 0.5$ \\
    Citeseer & 66.1 $\pm 1.3$ & \textbf{68.0} $\pm \textbf{0.4}$ & 67.3 $\pm 0.9$ & 66.5 $\pm 1.0$\\
    \bottomrule
    
    \end{tabularx}

    \label{tab:para_sigma}
    % \vskip -1em
\end{table}
\textbf{Impacts of $K$.}
When we add edges with the link predictor, for each node, we select $K$ nodes with the largest cosine similarity as candidate node set to predict the links to reduce the computational cost. To investigate how the selection of $K$ would influence the training, we vary $K$ as \{50, 100, 200, 400\} and report the average accuracy of 5 runs on Cora, Cora-ML and Citeseer that are perturbed by \textit{metattack} in Table~\ref{tab:para_K}. The perturbation rate is set as 0.15. The label rate is set as 0.01 which is the same as that of main paper. We can observe that with the increase of $K$, the performance would firstly increase a lot then slightly decrease. Because when $K$ is small, there are not adequate candidate nodes to predict links for each node. In this situation, the learned graph will be still sparse, which leads to poor performance on the noisy graphs with sparse labels. When $K$ is very large, for a node $v$, nodes that dissimilar with $v$ in raw features space would also be added into the candidate set. As a result, the performance slightly decrease.

\textbf{Impacts of $T_h$.}
When we apply the label smoothness regularization based on the generated graph, we will smooth the predictions of nodes linked by predicted links whose weights are larger than $T_h$.
To investigate how the setting of $T_h$ affects the label smoothness regularization, we vary $T_h$ as  $\{0.6,0.7,0.8,0.9\}$. 
We conduct experiments on the graphs perturbed by \textit{metattack}. The perturbation rate is set as 0.15. The label rate is set as 0.01. Other parameters follows the same settings in the main paper. Average results of 5 runs are reported in Table~\ref{tab:para_t}.  It shows that $T_h$ should be set as an appropriate value such as 0.8 to benefit the predictions with label smoothness.

\textbf{Impacts of $T_l$.} When we deniose and desify the graph, a $T_l$ is applied to the results of link predictor to determine whether we should keep/add the links. We vary the value of $T_l$ as $\{0.0, 0.05, 0.1, 0.2\}$ to investigate the influence of $T_l$. Experiments are conducted on the graphs perturbed by \textit{metattack} with the perturbation rate set as 0.15. The average results of 5 runs are reported in Table~\ref{tab:para_l}.  As we can see, with the increase of $T_l$, the performance will firstly increase and then decrease. Because when $T_l$ is very small, a lot of down-weighted noisy edges are not removed, which degrades the performance of RS-GNN. If $T_l$ is too large, the size of assigned links will be limited and some normal edges are likely to be deleted. Thus, the performance will drop when $T_l$ is too large.

\textbf{Impacts of $\sigma$.} In Eq.(3) of our main paper, a parameter $\sigma$ is used to control the variance of the weights of positive samples and negative samples when we train the link predictor with the loss of reconstructing the noisy graph. We vary the value of $\sigma$ as $\{30, 100, 300, 1000\}$ and fix other hyperparameters. Similarly, experiments are conducted of the Cora, Cora-ML, and citeseer graphs perturbed by \textit{metattack} with the perturbation rate set as 0.15. The results are presented in Table~\ref{tab:para_sigma}. From this table we could observe that when the $\sigma$ is set too large, the performance will decrease. When $\sigma$ is very large, the weights of all the negative samples and positive samples will be similar, which results a poor link predictor affected by noisy edges. This demonstrates the effectiveness of reweighting the samples based on raw feature similarity. However, if the $\sigma$ is too small, the variance of sample weights would be too large, which negatively affects the learning of link predictor.