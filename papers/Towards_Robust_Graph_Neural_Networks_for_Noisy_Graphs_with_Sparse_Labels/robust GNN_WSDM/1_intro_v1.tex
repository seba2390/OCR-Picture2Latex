\section{Introduction}
Graph Neural Networks (GNNs)~\cite{kipf2016semi,hamilton2017inductive} have made remarkable achievements in modeling graphs from various domains such as social networks~\cite{hamilton2017inductive}, financial system~\cite{wang2019semi}, and recommendation system~\cite{wang2019knowledge}. The success of GNNs relies on the message-passing mechanism~\cite{kipf2016semi,hamilton2017inductive}, where node representations are updated by aggregating the information from neighbors. With this mechanism, the node representations capture node features, information of neighbors and local graph structure, which facilitate various tasks, especially semi-supervised node classification. 

Although GNNs have shown great ability in modeling graphs, their performance can degrade significantly when trained on graphs with \textit{noisy edges} and/or \textit{limited labeled nodes}.
\textit{First}, due to the message passing, GNNs are vulnerable to adversarial or noisy edges. For example, as shown in Fig.~\ref{fig:illustration}, poisoning attacks~\cite{zugner2019adversarial} add/delete carefully chosen edges to the graph. These adversarial edges (shown in red) usually connect nodes of different labels or features, thus contaminating the neighborhoods of nodes, propagating noises/errors to node representations. In addition, inherent edge noises also exist in real-world graphs. For instance, in social networks, bots tend to build connections with normal users to spread misinformation~\cite{ferrara2016rise}, which can also harm the performance of GNNs for bot detection. 
\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figure/ill.pdf}
    \vskip -1.9em
    \caption{An illustration of down-weighting/removing noise edges and densifying the graph for better performance.}
    \label{fig:illustration}
    \vskip -2em
\end{figure}
\textit{Second}, for many applications, graphs are often sparsely labeled such as cell phone network for fraud detection~\cite{gallagher2008using}. 
Label sparsity can severely reduce the involvement of unlabeled nodes during message passing, leading to poor performance. Generally, in a $K$-layer GNN, a labeled node aggregates its $K$-hop neighborhood information, thus making many unlabeled nodes in $K$-hop neighborhood participate in the training, which is one major reason that GNNs can leverage unlabeled nodes for semi-supervised node classification. However,  as verified in our preliminary analysis in Fig.~\ref{fig:1_a} of Sec.~\ref{sec:3_3}, when the number of labeled nodes decreases, the amount of unlabeled nodes participating in training drops quickly, making message passing less effective.
These shortcomings of GNNs hinder the adoption of GNNs for many real-world applications. Thus, it is important to develop robust GNNs that can simultaneously handle noisy graphs with sparse labels.

However, developing robust GNNs for graphs with noisy edges and limited labeled nodes is challenging. \textit{First}, the training graph itself is noisy, i.e., noisy edges are mixed with the normal edges. Thus, we need supervision in down-weighting or eliminating noisy edges. \textit{Second}, alleviating the limited label issue requires more labels, while obtaining more labeled nodes is time-consuming and expensive. Hence, we need alternative approaches to more effectively utilize the limited labels. Some initial efforts~\cite{wu2019adversarial,jin2020graph,tang2020transferring,jin2020graph} have been taken to alleviate the effects of the adversarial edges such as pruning edges by using node similarity~\cite{wu2019adversarial}, and adopting Gaussian distribution as node representations to absorb noises~\cite{zhu2019robust}. To address the problem of sparsely labeled graphs, some methods~\cite{sun2019multi,li2018deeper,peng2020self} propose to obtain better representations by training GNNs with self-supervised learning tasks such as pseudo label prediction~\cite{sun2019multi,li2018deeper} and global context predictions~\cite{peng2020self}. 
However, little efforts are taken for robust GNNs that can simultaneously handle noisy edges and label sparsity.



Since both the noisy edges and limited labeled nodes harm the message passing of GNNs and message passing is directly related to the graph structure, we argue that learning a denoised and dense graph guided by the raw attributed graph is promising to facilitate message passing for robust GNNs. \textit{First}, for many graphs such as social networks, nodes with similar features and labels tend to be linked~\cite{liben2007link}, while noisy edges would link nodes of dissimilar features~\cite{wu2019adversarial}. 
Thus, we can use node attributes to predict the links. For existing links, the link predictor will assign small weights to links connecting nodes of dissimilar features while large weights to links connecting nodes of similar features, thus alleviating negative issue of noisy edges during message passing. \textit{Second}, 
real-world graphs are usually very sparse, containing many missing edges. With the link predictor, nodes that are potentially to be linked could be identified. Densifying the graph by linking similar nodes would induce more unlabeled nodes to become neighbors of labeled nodes with the same labels as shown in Fig.~\ref{fig:illustration}, which can alleviate the label sparsity issue. 
In addition, since adjacent nodes tend to have the same labels, the predicted new links can be used to further regularize the label predictions of unlabeled nodes. Though promising, the work on down-weighting noisy edges and densifying graph for robust GNN on noisy graphs with sparse labels are rather limited. 

Therefore, in this paper, we investigate a novel problem of developing robust noise-resistant GNNs with limited labeled nodes by learning a denoised and densified graph. In essence, we need to solve two challenges: (i) how to effectively learn a link predictor from the noisy graph which can eliminate noisy edges and densify the graph; and (ii) how to simultaneously use the learned graph to learn a structural noise-resistant GNNs with limited labeled nodes. To address these challenges, we propose a novel framework named 
robust structural noise-resistant GNN (RS-GNN)~\footnote{Codes are available at: https://github.com/EnyanDai/RSGNN}. RS-GNN adopts the node attributes and supervision from the noisy edges to
denoise and dense graph, which can alleviate the negative effects of noisy edges and facilitate the message passing between unlabeled nodes and labeled nodes. The learned graph is used as input for learning a GNN. RS-GNN also adopts the predicted edges to further explicitly regularize the predictions of unlabeled nodes to alleviate the label sparsity issue. In summary, our main contributions are:
\begin{itemize}[leftmargin=*]
    \item We study a new problem of learning robust noise-resistant GNNs with limited labeled nodes;
    \item We propose a novel framework RS-GNN, which can simultaneously learn a denoised and densified graph and a robust GNN on noisy graphs with limited labeled nodes; and
    \item We conduct extensive experiments on real-world datasets to demonstrate the robustness of RS-GNN on both noisy/clean graphs with limited labeled nodes. 
\end{itemize}
