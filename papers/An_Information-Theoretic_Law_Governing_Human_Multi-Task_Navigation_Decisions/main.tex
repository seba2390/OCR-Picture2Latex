%% ****** Start of file apstemplate.tex ****** %
%%
%%
%%   This file is part of the APS files in the REVTeX 4.2 distribution.
%%   Version 4.2a of REVTeX, January, 2015
%%
%%
%%   Copyright (c) 2015 The American Physical Society.
%%
%%   See the REVTeX 4 README file for restrictions and more information.
%%
%
% This is a template for producing manuscripts for use with REVTEX 4.2
% Copy this file to another name and then work on that file.
% That way, you always have this original template file to use.
%
% Group addresses by affiliation; use superscriptaddress for long
% author lists, or if there are many overlapping affiliations.
% For Phys. Rev. appearance, change preprint to twocolumn.
% Choose pra, prb, prc, prd, pre, prl, prstab, prstper, or rmp for journal
%  Add 'draft' option to mark overfull boxes with black boxes
%  Add 'showkeys' option to make keywords appear
\documentclass[aps,pre,twocolumn,groupedaddress]{revtex4-2}
%\documentclass[aps,pre,preprint,groupedaddress]{revtex4-2}
%\documentclass[aps,prl,preprint,superscriptaddress]{revtex4-2}
%\documentclass[aps,prl,reprint,groupedaddress]{revtex4-2}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
% You should use BibTeX and apsrev.bst for references
% Choosing a journal automatically selects the correct APS
% BibTeX style file (bst file), so only uncomment the line
% below if necessary.
%\bibliographystyle{apsrev4-2}
\newcommand{\sjg}[1]{\textcolor{orange}{#1}}
\newcommand{\nes}[1]{\textcolor{red}{#1}}
\begin{document}

% Use the \preprint command to place your local institutional report
% number in the upper righthand corner of the title page in preprint mode.
% Multiple \preprint commands are allowed.
% Use the 'preprintnumbers' class option to override journal defaults
% to display numbers if necessary
%\preprint{}

%Title of paper
\title{An Information-Theoretic Law Governing Human Multi-Task Navigation Decisions}

% repeat the \author .. \affiliation  etc. as needed
% \email, \thanks, \homepage, \altaffiliation all apply to the current
% author. Explanatory text should go in the []'s, actual e-mail
% address or url should go in the {}'s for \email and \homepage.
% Please use the appropriate macro foreach each type of information

% \affiliation command applies to all authors since the last
% \affiliation command. The \affiliation command should follow the
% other information
% \affiliation can be followed by \email, \homepage, \thanks as well.
\author{Nicholas Sohre$^{1}$}
\email[]{sohre007@umn.edu}
\author{Alisdair O. G. Wallis$^{2}$}
\author{Stephen J. Guy$^{1}$}
\email[]{sjguy@umn.edu}
\affiliation{Dept. of Computer Science \& Engineering, University of Minnesota, Minneapolis, MN 55455, USA$^{1}$ \\ Tesco PLC, Tesco House, Shire Park, Kestrel Way, Welwyn Garden City, AL7 1GA, UK$^{2}$}
%\homepage[]{Your web page}
%\thanks{}
%\altaffiliation{}

%Collaboration name if desired (requires use of superscriptaddress
%option in \documentclass). \noaffiliation is required (may also be
%used with the \author command).
%\collaboration can be followed by \email, \homepage, \thanks as well.
%\collaboration{}
%\noaffiliation

%1-Understanding human decision behavior enables more human-aware kinds of AI
%2-New kinds of shopper data allows new and exciting analysis
%3-We take a new dataset and use it to study long term shopping decisions
%4-We propose a cognitive based analysis of deciding which item to go to next (assume you want closest, but make some inversions)
%5-We hypothesize that the chance of making a mistake (inversion) is a function of both the inter-item distance and each items' distances from you
%6-We discover the inversion rates in the data follow a Fitts law (robust across weekend vs weekday, week to week?)
%7-We propose a stochastic simulation model where inversions are the result of noisy estimates of actual item distances (gaussian, linearly increasing std dev w/ distance)
%8-We show the simulation model follows our intuition about relative and absolute distances, and that it analytically reproduces the Fitts law (optionally motivate linearity of absolute distance with phi / diff2 plot)
%-We show empirically that our model run on the same baskets in the data well-matches several decision trends in the data with high accuracy
\date{\today}

\begin{abstract}
To better understand the process by which humans make navigation decisions when tasked with multiple stopovers, we analyze motion data captured from shoppers in a grocery store. We discover several trends in the data that are consistent with a noisy decision making process for the order of item retrieval, and decompose a shopping trip into a sequence of discrete choices about the next item to retrieve. Our analysis reveals that the likelihood of inverting any two items in the order is monotonically bound to the information-theoretic entropy of the pair-wise ordering task. Based on this analysis, we propose a noisy distance estimation model for predicting the order of item retrieval given a shopping list. We show that our model theoretically reproduces the entropy-governed trend seen in the data with high accuracy, and in practice matches the trends in the data when used to simulate the same shopping lists. Our approach has direct applications to improving simulations of human navigation in retail and other settings.

\end{abstract}

% insert suggested keywords - APS authors don't need to do this
\keywords{Motion Planning, Data-Driven Methods, Machine Learning}

%\maketitle must follow title, authors, abstract, and keywords
\maketitle

% body of paper here - Use proper section commands
% References should be done using the \cite, \ref, and \label commands
% 1. Psychophysics
% -- McKinzie, Shannon, Fitts, Welford, others?
% 2. Analyzing / Simulating Human Motion
%  a-- Local
%   ---- Power Law, ORCA?
%  b-- Global
%   ---- Alisdair paper, SPNets, Cognitive Science?
%  c-- Multi-Task planning
%   ---- ? 
%Others?
%narrative possibilities:
% A) analysis / modeling is at either very high level or very low level scale
% B) models focus on understanding / predicting which items will be purchased, but not the order (which is important for flow / congestion) 

\section{Introduction\label{intro}}
Understanding human flow through indoor buildings is important for various layout design tasks such as evacuation planning, product placement, and security. Advancements in technologies such as computer vision and motion-tracking have enabled the large scale collection long-term motion data, enabling new analyses aimed at incorporating these high level decisions into human flow models. Here, we take a data-driven approach to analyzing the navigation decisions of shoppers in a grocery store. 

\textit{Path data}, or varying spatial configurations of individuals as a function of time, provides valuable insight into human navigational behavior in a variety contexts~\cite{hui2009path}. Various works have utilized path data both as a means to understand and simulate human behavior, using different assumptions and conceptualizations to achieve specific research goals. Some of these works involve learning to predict human behavior from path data, from inferring high level flow models for retail floor optimization~\cite{ying2019customer}, to frameworks based on random walks~\cite{gutierrez2016active}, to crowd simulations~\cite{karamouzas2014universal}. Other works focus on analysis of behaviors, such as comparing human route selection to the theoretical optimum~\cite{hui2009research}, or discovering behavioral patterns~\cite{larson2005exploratory,karamouzas2018crowd}. Often these analyses are coupled with various mobility models for predicting human movement, designed to operate on different scales, from global migratory patterns~\cite{riascos2012long} and city level traffic~\cite{camargo2019diagnosing,piovani2018measuring} to more local path planning~\cite{lima2016understanding,bailenson2000initial} and collision avoidance~\cite{karamouzas2014universal,van2011reciprocal,helbing1995social}. Less explored is the mid-level scale of multi-task planning, such as how fair-goers might visit multiple attractions, or how customers determine which item to pick up next from a shopping list.

In this work, we use path data to better understand the process by which human shoppers make decisions about navigation when shopping. We focus on characterizing the task of selecting a next item to retrieve, and perform multiple analyses that provide insights into high level features governing shopper decisions. While numerous external factors affect this process, such as building layout and other human factors (specific in-store attractions, unplanned purchases~\cite{massara2014impulse}, or tendency to follow the perimeter~\cite{farley1966stochastic}), we adopt a general formulation of a shopping trip as a series of decisions given a predefined list of items, and utilize a large dataset of  shopper trajectories to gain insights about how the navigation process may be described and modeled from the path data. 

The rest of this paper is structured as follows. In Section~\ref{sec:data}, we describe the dataset as well as preprocessing steps and give some formal definitions that support our analysis. In Section~\ref{sec:analysis} we model shopping trips as a series of discrete navigation decisions that produces an item sequence, identify several high level trends, and propose a measure of decision difficulty that governs the suboptimalities in the data when decomposed into pairwise decision tasks. In Section~\ref{sec:independence} we incorporate our findings into a general decision model for each step of a shopping trip. In Section~\ref{sec:simulation} and propose a stochastic decision model that is theoretically guaranteed to produce the same trends seen in the data, but in practice matches the trends with high accuracy.

%The dataset consists of motion-tracked shopper \textit{trajectories} $\mathbf{\mathcal{J}} = \{\mathbf{j_{1}},\mathbf{j_{2}...\mathbf{j_{|\mathcal{J}|}}\}}$ with each $\mathbf{j_{i}} =\{(x_{t}^{j_{i}},y_{t}^{j_{i}}), t\in [0...m_{i}]\}$ is a time series of 2D positions in a retail store, along with disjoint point-of-sale data describing \textit{baskets} $\mathbf{\mathcal{B}} = \{\mathbf{b_{1}},\mathbf{b_{2}}...\mathbf{b_{|\mathcal{B}|}}\}$ where each $\mathbf{b_{k}} = \left(\{p_{1}^{b_{k}},p_{2}^{b_{k}},...p_{|b_{k}|}^{b_{k}}\}, c_{b_{k}\right)$ is a set of items purchased together from the same time period as the paths, annotated with transaction times $c_{b_{k}}$. We associate the paths with their most likely basket to create  by cross-referencing the transaction time and till location of a basket with paths having positions near the till at that time, as well as matching low-velocity sample clusters (or \textit{stops}) in the paths with the basket's item locations. In total, the matched data represents more than 17,000 shopping trips over a period of two weeks. 

\section{Dataset\label{sec:data}}

Here we use an anonymized dataset consisting of item sequences from shoppers in a retail store (\textit{paths}), corresponding to individual transactions from point-of-sale records of sets of items purchased together (\textit{baskets}). Each sequence reflects the order in which the items were retrieved. The items are embedded in a 2D representation of the store layout corresponding to product shelf locations. Additionally we use the set of 2D wall obstacles representing the sales floor layout to compute features such as walking distance between items. Figure ~\ref{fig:entropy:example_match} shows a contextualized example of a single shopping trip as it would appear in the data. The data contains over 13,000 such basket sequences spanning a period of two weeks.

%We use these item orderings to cast each shopping trip $\mathcal{\mathbf{J}}$ in the data as a series of discrete decisions: 
We use these item orderings to derive a set of decision points for each shopping trip, where each decision point~$\mathbf{p}$ represents the set of remaining items the shopper eventually retrieves (but had not yet retrieved as of that point in the trip). 
%\begin{equation}
%\mathcal{\mathbf{J}} = \{\mathbf{p}_{1},\mathbf{p}_{2}...\mathbf{p}_{n}\}
%\label{eq:entropy:J}
%\end{equation}
%
%\noindent where each decision $\mathbf{p}$ represents a juncture at which a shopper must decide the next item to retrieve from the set of items remaining to be collected. 
The item sequences (and corresponding decision points) for each shopping trip represent paths over the fully connected item graph $\mathcal{G} = (\mathcal{V},\mathcal{E})$ where each vertex $v\in\mathcal{V}$ represents the spatial embedding of an item shelf in the store (we assign the shelf's position to be its corner), and the edges $e_{ij}\in\mathcal{E}$ represent the straight line connections between co-visible vertices $v_{i}$ and $v_{j}$. We augment this graph with an additional vertex $v_{start}$ for the entrance to the store to serve as the shopper location for the first decision point in a trip. We use this graph to compute the \textit{shortest path} walking distance between any two items for analysis.
%A navigation decision $\mathbf{p}$ involves selecting a next item to retrieve at a containing shelf $v$. For our analysis, we identify as a feature of interest the travel distances represented by  the \textit{shortest path} along the edges in $\mathcal{E}$ between the current location $v_{i}$ and each available item's shelf at $v_{j}$ contained in $d$. 

%We note that while items can have multiple placements in a store floor arrangement (multiple $v$ at different locations for the same $p$), we consider all instances of the item in the basket to be retrieved at the time of the closest approach the path took to any $v$ containing the item. 
Given a list of items left to collect $i \in \{1,2,...n\}$ we can decompose a decision point into the set of available (shortest walking path) travel distances it represents:
\begin{equation}
\mathbf{p} = \{d_{1},d_{2}...d_{n}\}
\label{eq:entropy:d}
\end{equation}
 
\noindent where each distance $d_i$ is the length of the shortest path over $\mathcal{G}$ from the item's shelf location to the shopper location (either $v_{start}$ or the vertex of the most recently chosen item's location).  We extract for analysis only those $\mathbf{p}$ containing more than one item, as having only a single item remaining does not present a choice to the shopper. The final set of decisions we use for analysis contains over 104,000 decision points. We use the distances in these decision points to predict shopper navigation choices.

%We build a predictive model by comparing the observed shopper item sequences to a locally optimal one and extracting relative sub-optimalities in the shopper paths.

%and an analysis to uncover the latent features that drive them

% Put \label in argument of \section for cross-referencing
%\section{\label{}}
\begin{figure}
    \centering
    \includegraphics[width=0.7\columnwidth]{figures/store_item_order.pdf}%
    \caption{(Color online) An example item sequence from the dataset, embedded in the abstracted store layout (black obstacles) and product shelf embedding (blue x's).\label{fig:entropy:example_match}}
\end{figure}
\section{Data Analysis\label{sec:analysis}}
% 4. We propose a cognitive based analysis of deciding which item to go to next (assume you want closest, but make some inversions)
%For the sake of analysis, we define the following terms and quantities as they related to a shopper's decisions in the multi-task problem of item retrieval. 

\subsection{Decisions}

 The navigation decisions in the data show that shoppers are generally efficient. Despite not necessarily having full knowledge of all item locations or store layout, about 79\% of the time, the next item a shopper picks up is the closest item to their current location $v_{j}$, having distance
$d^{*} = \text{min}_{i}(d_{i}\in\mathbf{p})$. While always picking the next closest item is not a globally optimal strategy (which involves solving a traveling salesperson problem, studied by~\cite{hui2009research} in the context of shopping), we refer to it as ``locally optimal" in the sense that it is the best path a person could take without knowing other future items they have not yet gathered (an approach suggested but not explored in~\cite{hui2009path}). This suggests the formulation of a shopping trip where each decision involves forming an estimate of which item left to collect is the closest, and navigating there. For the sake of analysis, we denote the next selected item for retrieval as $\hat{i}$, which is the index into a given $\mathbf{p}$ that reflects which item was chosen. Then $d_{\hat{i}}$ represents the distance to the chosen item. Similarly we define $i^{*}$ such that $d_{i^{*}} = d^{*}$ to be the index of the closest item for the decision point.

%The difficutly of avoiding an inversion is well modeled by the entropy of the decision
Due to the local nature of available information (and potentially limited familiarity shoppers may have with the store layout), cases where the closest item is not chosen ($\hat{i} \ne i^{*}$) naturally arise. The presence of these suboptimal local choices are consistent with other studies of human route selection and cognitive tasks where suboptimalities are found to be a natural part of these processes~\cite{lima2016understanding,hui2009research}. A suboptimal choice occurs whenever the chosen item's distance was larger than the optimal distance for that decision point. We call these choices \textit{inversions}, where the preference order of the chosen item with respect to the optimal one has been flipped. 

An analysis of the inversions in the set of decision points reveals a strong trend involving $d^{*}$ and the likelihood of making a locally optimal choice (i.e., $\hat{d} = d^{*}$). Figure~\ref{fig:inversion_trends} (left) shows this trend, where the likelihood of choosing the optimal item at a decision point decreases as a function of increasing $d^{*}$. As this distance (and necessarily the distance of all other remaining items) increases, it becomes more difficult to consistently choose the closest item, converging toward the same likelihood as a random selection (shown in red).

\begin{figure}
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/inversion_trends.pdf}%
    \caption{(Color online) \textit{left}: The likelihood of choosing the locally optimal (closest) item decreases as a function of distance to the closest item for shopper paths (grey region is the 98\% confidence interval), and eventually converges to that of a random choice. \textit{right}: The likelihood of not choosing the closer of two items in a sub-task as a function of the difference in distances (to the shopper) between them. The random choice is shown for reference (always 0.5 for two items). \label{fig:inversion_trends}}
\end{figure}

\subsection{Sub-Tasks}
In addition to the set of per-decision inversions, we perform a decomposition of the shopping decisions to produce a larger set of inversions for analysis. To do this, we extract all pair-wise comparisons $(\hat{d},d_{i})$ from the data, each of which represents a possible inversion of the chosen item $\hat{d}_{i}$ with some alternative $d_{i}$. This yields a dataset of over $883,000$ such item pairs, which we refer to as \textit{sub-tasks}, in which a shopper estimates the closer of the two. 

The extracted pairs represent a subset of all the possible sub-tasks (and potential pair-wise inversions) that exist in the full decisions. Sub-task samples having $\hat{d} > d_{i}$ constitute pair-wise inversions, whose inversion amount can be measured as how much farther the shopper traveled then they would have by choosing item $i$. A sub-task can be alternatively decomposed as the pair $(F,C)$, where $F = \text{max}(\hat{d},d_{i})$ is the larger of the two, and $C = \text{min}(\hat{d},d_{i})$ is the smaller. This enables an inversion analysis based on both the closer item distance $C$ as well as the relative item distance $F-C$ (always a positive value). 

The right side of Figure~\ref{fig:inversion_trends} shows an analysis of the sub-tasks. Here, the chance of a pair-wise inversion falls off as the items grow farther apart in their relative distances to the shopper. This suggests that as the difference of relative distances grows, it becomes easier to distinguish which is closer.

\section{An Information-Theoretic Law for Inversion Likelihood}
\label{sec:inversion_law}
\subsection{Measuring Difficulty}
\label{sec:difficulty}
As is evident from the non-uniformity of both trends examined in Section~\ref{fig:inversion_trends}, some  decisions and sub-tasks are more likely to see inversions than others. Given the assumption that shoppers desire efficient paths, these trends serve as evidence that some scenarios present more difficult estimation tasks. Here we adopt a description of difficulty that follows from information theory, which is the \textit{Shannon entropy} of the sub-task. In the context of pair-wise decisions, the Shannon entropy is the minimum number of bits required to represent the item distances such that they can be reliably ordered. Formally, given the distance $F$ of the farther of the two items in a sub-task and the distance $C$ of the closer item, the entropy $H$ can be computed as
%These sub-tasks take a form similar to tasks seen in psychophysics works studying accuracy in cognitive or motor tasks where humans must select a target with some error tolerance \nes{citation}. In these cases, the \textit{difficulty} of the tasks are well-modeled by the corresponding \textit{information capacity} $IC$ required to perform the task, which is an adaptation of the Shannon-Hartley theorem defined as: 

%\begin{equation}
% \label{eq:info_cap}
%  IC = log_2\left(1+ \frac{A}{W}\right)
%\end{equation}
\begin{equation}
 \label{eq:entropy}
  \text{\textit{H}} = \log_2\left(\frac{F}{(F-C)}\right).
\end{equation}

We adopt the entropy of a sub-task as a measure of difficulty, and define the difficulty $D$ of a sub-task as:
\begin{equation}
 \label{eq:difficulty}
  \text{\textit{D}} = \log_2\left(\frac{F}{(F-C) + \epsilon}\right).
\end{equation}
\noindent where $\epsilon = 0.01$ places an upper bound on difficulty at $F=C$. We call $W = F-C$ the \textit{tolerance} of the task, as it is the maximum relative error in distance estimation that preserves their rank order. 
This difficulty measure is consistent with (and inspired by) those proposed in other psychophysical studies of human cognition~\cite{moyer1967time,fitts1964information}. 

When inversion rate is graphed as a function of difficulty (Figure~\ref{fig:D_data}), we see a clear monotonic relationship between difficulty and inversion chance: $P(\hat{d} = F)$ (ie, choosing a farther than necessary item to go to next) increases as difficulty gets larger (with some noise affecting the trend at higher difficulties that occur less frequently in the data). Additionally, the inversion rates naturally converge with random selections at a maximum of 50\%. As the ability for shoppers to distinguish between closer and farther items saturates at around $D=5$, indicating that the information carrying capacity of the item selection process of shoppers for our data was around 5 bits.
 %inversion rates follow difficulty
\begin{figure}
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/D_data.pdf}%
    \caption{(Color online) The chance of choosing the farther of two items in a sub-task as a function of difficulty (entropy) score for the shopper data (black w/ grey 98\% confidence interval) and random choice (red).\label{fig:D_data}}
\end{figure}

% 5. Two intuitive trends fall out of difficulty score: inversion chance is a function of both the inter-item distance and each items' distances from you

This cognitive difficulty not only drives inversion rates, but also captures both empirical trends in Figure~\ref{fig:inversion_trends}. The closer together in distance from the shopper ($F-C$ is small), the larger $D$ becomes, consistent with Figure~\ref{fig:inversion_trends} (right) where there is greater confusion between item pairs having small distance differences. Additionally, the farther away the closer item is (large $F$), the greater $D$ becomes, matching the left side decision level trend showing lower chance of choosing optimally.

\subsection{Independent Perceptual Error Model\label{sec:independence}}
% 7. We propose a stochastic simulation model where inversions are the result of noisy estimates of actual item distances (gaussian, linearly increasing std dev w/ distance)
To extend the scope of the analysis in Section~\ref{sec:difficulty} to multi-item decisions, we note that the empirical trends for misordering items are consistent with a selection process that involves independent assessments of perceived item distances. We introduce $\Tilde{d}_{i}$ as a noisy, estimated distance to an item that incorporates uncertainty into a shopper's decision. The selection process can then be modeled as forming estimates for each item, then choosing the item $i$ that is estimated to be closest. In light of the analysis from Section~\ref{sec:analysis}, we note that the uncertainty of an item's estimated distance should grow with the true distance, and the ability to discriminate between them should diminish with close relative distances to the shopper. To meet these criteria, we design a generative model of noisy item distance estimation and model the noisy estimated distance, $\Tilde{d}_{i}$, as follows:
\begin{equation}
 \label{eq:gaussian_err}
  \Tilde{d}_{i} = d_{i} + \epsilon_{i},\; \epsilon_{i} \sim \mathcal{N}(0,\alpha d_{i})
\end{equation}
\noindent where the standard deviation $\alpha d_{i}$ is a linear function of the item's true distance, and each item's noise $w_{i}$ is sampled independently. For a pairwise sub-task, the chance of inversion can be computed directly from the gaussian noise model:

\begin{align}
%P\left(&S = 1 | d(p_{1}),d(p_{2}) \right)  d(p_{1})<d(p_{2})\\
P(\hat{d} > d_{i}) &= P(\hat{d} = F \;|\; F, C) \nonumber\\
 &= \frac{1}{2} 
 \left[ 
 1 + \text{erf}\left( 
 \frac{F - C}
 {\alpha \sqrt{2(C^2+F^2)}} 
 \right) 
 \right] 
\label{eq:gauss_inv}
\end{align}
\noindent where $F$ and $C$ represent the farther and closer distances of the sub-task $(\hat{d}, d_{i})$ respectively.

The supplementary materials provide a derivation of Equation \ref{eq:gauss_inv}. This analytical expression for the inversion chance both enables theoretical properties of the model to be guaranteed, and is efficient to compute, making it practical to directly fit $\alpha$ to the inversion chances in the data via numerical optimization techniques. Here we fit $\alpha$ to the trend shown in~\ref{fig:inversion_trends} (\textit{right}), as it has good data support over the entire domain (i.e., very tight confidence bounds across the x axis). Using BFGS gradient descent optimization, with a mean-squared error loss, yields $\alpha=0.30$ as a minimizing value. The optimization was performed using the \textit{optim} function of the \textit{stats} package for statistical computing language R~\cite{Rlanguage}.

We can use Equation~\ref{eq:gauss_inv} to drive an agent-based simulation of navigation based on the basket data (See Section~\ref{sec:simulation} for further discussion of simulation details). Despite this predictive model using only a single parameter, the resulting simulated paths closely match the human data on a variety of metrics across a large span of item distances and decision difficulties (See Figure~\ref{fig:sub_task_level}).

Additionally, this noisy distance estimation model has key theoretical properties that guarantee the simulations will follow the same general trends seen in Figures~\ref{fig:inversion_trends} and~\ref{fig:D_data} independent of the choice of $\alpha$. First, the distribution for choosing between $N$ items converges to uniform as the relative distances become closer in magnitude (this can be seem by taking the limit as $F$ approaches $C$ in from equation~\ref{eq:gauss_inv}). Second, the chance of chance of pairwise inversions monotonically approaches the asymptote of 0.5 both as relative item-agent distances decrease and as distance to the closest item increases. Finally, our model provably recovers the monotonic relationship between difficulty and chance of inversion  (see the supplementary material for a proof).

% %even with single parameter, high accuracy
% We note some additional critical properties of our proposed noisy decision model. First, the distribution for choosing between $N$ items converges on a uniform distribution as their relative distances to the agent become closer in magnitude, as equal distances to the agent will produce the same standard deviation for each Gaussian (that is, there is always equal chance of choosing between any items that are the same distance from the agent). This follows directly from equation~\ref{eq:gauss_inv} by setting $F = C$. Second, this method is guaranteed to recover the monotonicity in the trends from Figure~\ref{fig:inversion_trends} on sub-tasks, with increasing chance of inversion both as relative item-agent distances decrease (smaller $F-C$), as well as increasing distance of the closer item. As a result, it also preserves the monotonic relationship between sub-task difficulty and inversion rate. The supplementary Appendix contains proofs of these properties.


\begin{figure}
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/sub_task_level.pdf}%
    \caption{(Color online) \textit{left}: the difficulty plot from Figure~\ref{fig:D_data} with overlaid simulation inversion rate for the independent distance estimation model (solid blue line)
    \textit{right}: observed inversion rates from simulated paths overlaid on the trend from Figure~\ref{fig:inversion_trends} (right). \label{fig:sub_task_level}}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.95\columnwidth]{figures/decision_level.pdf}%
    \caption{(Color online)  \textit{left}: observed optimal choice rates from simulated paths overlaid on the same data from Figure~\ref{fig:inversion_trends} (left).
    \textit{right}: The log frequency of observed inversion sizes (ie, the extraneous distance travelled to the chosen item over the closest ) for simulated (blue) and shopper (black) item decisions. Inset is the density for inversion sizes cropped to show the knee of the curve.\label{fig:decision_level}}
\end{figure}

\section{Simulation Method\label{sec:simulation}}
The generative model from Section~\ref{sec:independence} is easily incorporated into a simulation for predicting a shopping trip given a basket of items to be collected, a spatial embedding of the items, and obstacles representing a store layout. We propose a stochastic agent-based simulation that does this with an execution strategy as follows. The agent begins at $v_{start}$ and has available a list of all $(d_{i},v_{i})$ tuples corresponding to where each item in the list can be picked up in the store. Noise is independently sampled according to equation~\ref{eq:gaussian_err} to produce a $\Tilde{d}_{i}$ for each candidate $(v_{i})$, and the agent selects the one having the smallest estimated distance as the next navigation target. The agent then navigates to the chosen item location using a shortest path planner over a visibility graph of the item locations. This strategy is repeated until all items have been collected (see the supplementary material for details).

%SJG - I deleted these:
% To evaluate the simulation model, we run simulations of the same baskets seen in the data. Figure~\ref{fig:sub_task_level} shows the sub-task analyses applied to the simulated data overlaid on the shopper data. On the left, we see that the simulated item sequences not only show that the monotonic relationship between difficulty and inversion chance extend to full decision tasks, but also closely match the human data. Similarly, on the right we see that the fit alpha produces a very tight relationship between the simulation and data trends as a function of the item distance difference ($F-C$).

%try to write a sentence that treats these symetrically (inversion chance vs inversion amount)
To validate the simulation's accuracy at the decision level, we compare both the likelihood of inversions in the simulations to the shopper data, as well as the inversion magnitudes.
Figure~\ref{fig:decision_level} (left) shows the simulation results for the decision-level data trend from Section~\ref{sec:analysis}, where the likelihood of choosing the optimal item (the complement of inversion chance) matches well the shopper data compared to a random choice. On the right is a comparison of the log frequencies of \textit{inversion amounts} for the decisions, defined as $\hat{d} - d^{*}$. An inversion amount describes how much farther the chosen item was to the shopper than the closest item for a decision. Here, the simulation method continues to show good alignment with the shopper data, matching the actual inversion amounts with high accuracy.

The parameter $\alpha$ can be fit to data or used to tune behavior (for example, $\alpha \rightarrow 0$ will approach purely locally optimal decisions, which could be used to emulate shopper familiarity with the store layout). Additionally, the simulation technique is agnostic of store layout, and may be directly extended (or re-fit given new data) on any new store layout while retaining all the same properties that well describe the observed human behaviors. 

%As an additional validation analysis, we generate shopping routes for baskets from a single day (around 1,000 trips) using the simulation method and compare them to the shopper routes in the data using a heatmap of positions over time. To create the paths between item pickups, an agent is animated along the shortest walking path between items using a preferred-velocity model, slowing to a stop at each item location. The position of the simulated agent is logged each time step at a rate of 10hz. 

%Figure~\ref{fig:sim_paths} shows the simulation results for several baskets in the data overlaid on their corresponding human paths. The simulated paths take similar general routes as the human paths with very similar orderings. Most of the differences come from local perturbations in the item sequence as opposed to larger shifts in the route as a whole. Some differences in path shape occur due to the straight line travel and shortest path planning of the agent between items, but we note this method can be easily incorporated into existing simulation methods that support other desired path features (for example, force-based dynamics that produce smoother paths when avoiding obstacles or other agents).
%\begin{figure}
%    \centering
%    \includegraphics[width=0.95\columnwidth]{figures/rendered_paths.pdf}%
%    \caption{Several simulated paths on baskets from the dataset are shown. The top row contains two runs of the same basket, showing a more stable overall route with local ordering perturbations. While inter-item traversal features in the paths differ due to execution strategy, the item sequences generated by the simulation (purple) well match the types of sequences seen in the data (red) \label{fig:sim_paths}}
%\end{figure}
%We fit $\alpha$ to the extracted sub-tasks described in Section~\ref{sec:analysis}. Since for two items, the chance of inversion can be computed directly from the gaussian formulation, we fit $\alpha$ using a negative log likelihood loss function on the binary cross entropy between the theoretical inversion rate of each subtask and the outcome seen in the data. We use the $BFGS$ numerical optimization as implemented by the $optim$ function of the statistical programming language $R$~\cite{R-language}, and find the best alpha to be $\approx 0.315$ for our dataset. 


% Formualtion
% Error as independent gaussian w/ uncertainty as linear function of distance
% Analyticallly show Fitts law, incorporates absoulte and relative distance
% Fit alpha to Fitts law
% Show simulation method comparisons with other models (random, always closest, dichotomous?)
%Entropy metric?

\section{Conclusions\label{conclusions}}
In this work, we have performed a novel analysis of shopper path data to gain insight about multi-task human navigation decisions. When viewed as a sequence of decisions among remaining items, the data shows that shoppers very typically choose the next closest item to retrieve. The chance of item selection inversions in the data (that is, going to an item that was not the closest) follows our proposed sub-task difficulty measure monotonically. We observe that an independent error estimation model with a linear relationship between a true item's distance and uncertainty well captures these trends. Based on these findings we propose an agent-based method for simulating the order of item retrieval given a basket and a store layout. The simulated data recovers the relationship between the chance of inversions and sub-task difficulty, and in practice well matches the shopper data along several comparison metrics.
\vspace{1em}

%A limitation of this work is in our assumption that all the items that will eventually retrieved are known in advance. when making decisions. Our dataset does not provide information about whether or not a customer planned to purchase the item before choosing to navigate to it. A more rich path dataset with related information may be be helpful for this kind of analysis, however we expect our results to be robust to these kinds of purchases.
% If in two-column mode, this environment will change to single-column
% format so that long equations can be displayed. Use
% sparingly.
%\begin{widetext}
% put long equation here
%\end{widetext}

% figures should be put into the text as floats.
% Use the graphics or graphicx packages (distributed with LaTeX2e)
% and the \includegraphics macro defined in those packages.
% See the LaTeX Graphics Companion by Michel Goosens, Sebastian Rahtz,
% and Frank Mittelbach for instance.
%
% Here is an example of the general form of a figure:
% Fill in the caption in the braces of the \caption{} command. Put the label
% that you will use with \ref{} command in the braces of the \label{} command.
% Use the figure* environment if the figure should span across the
% entire page. There is no need to do explicit centering.

% \begin{figure}
% \includegraphics{}%
% \caption{\label{}}
% \end{figure}

% Surround figure environment with turnpage environment for landscape
% figure
% \begin{turnpage}
% \begin{figure}
% \includegraphics{}%
% \caption{\label{}}
% \end{figure}
% \end{turnpage}

% tables should appear as floats within the text
%
% Here is an example of the general form of a table:
% Fill in the caption in the braces of the \caption{} command. Put the label
% that you will use with \ref{} command in the braces of the \label{} command.
% Insert the column specifiers (l, r, c, d, etc.) in the empty braces of the
% \begin{tabular}{} command.
% The ruledtabular enviroment adds doubled rules to table and sets a
% reasonable default table settings.
% Use the table* environment to get a full-width table in two-column
% Add \usepackage{longtable} and the longtable (or longtable*}
% environment for nicely formatted long tables. Or use the the [H]
% placement option to break a long table (with less control than 
% in longtable).
% \begin{table}%[H] add [H] placement to break table across pages
% \caption{\label{}}
% \begin{ruledtabular}
% \begin{tabular}{}
% Lines of table here ending with \\
% \end{tabular}
% \end{ruledtabular}
% \end{table}

% Surround table environment with turnpage environment for landscape
% table
% \begin{turnpage}
% \begin{table}
% \caption{\label{}}
% \begin{ruledtabular}
% \begin{tabular}{}
% \end{tabular}
% \end{ruledtabular}
% \end{table}
% \end{turnpage}
% Create the reference section using BibTeX:
% If you have acknowledgments, this puts in the proper section head.

\begin{acknowledgments}
This work was supported in part by the National Science Foundation under grants IIS-1748541 and CHS-1526693. 
\end{acknowledgments}

\bibliography{apssamp}
% Specify following sections are appendices. Use \appendix* if there
% only one appendix.
\end{document}
%
% ****** End of file apstemplate.tex ******

