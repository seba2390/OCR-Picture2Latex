
\input{p01_1_introduction_table}



\section{Introduction}


In the past several years, explainability has become a prominent issue in machine learning, addressing concerns about the safety and ethics of using large, opaque models for decision-making. As interest has grown in explanations for understanding model behavior, so has interest grown in soliciting gold-standard explanations from human annotators and using them to inject useful inductive biases into models \cite{hase_when_2021}. Many such explanation datasets have become available recently \cite{wiegreffe_teach_2021}.

A common format for explanations in NLP is the \textit{rationale}, 
a subset of input tokens that are relevant to the decision.
A popular architecture for generating such explanations is the \textit{rationale model}, an explain-then-predict architecture which first 
extracts a rationale from the input and
then makes a prediction from 
the rationale-masked text (that is, only the tokens included in rationale) \cite{lei_rationalizing_2016, deyoung_eraser_2019}. Without  external supervision on this rationale, we typically pursue parsimony via a sparsity objective.
Table~\ref{tab:intro_rationale_example}A shows an example unsupervised rationale.

With the benefit of a human-annotated rationale for the true label, we can begin to understand model mistakes in terms of reliance on inappropriate features (and correct them). In the example above, the unsupervised rationale suggests that the model's mistake is due to missing key information about how many friends Susan has (i.e., ``five''). Forcing the model to see these key tokens by only using the human rationale as the input 
fixes this mistake (Table \ref{tab:intro_rationale_example}B). 
Prior work has shown that this is not a fluke. For some datasets, human rationales consistently improve model accuracy over baseline when used as an input mask, by orienting model attention toward informative tokens and away from confounding ones
\cite{carton_evaluating_2020}. 


Knowing that human rationales contain useful predictive signal, the key question becomes: \textbf{can we improve model prediction accuracy by incorporating human rationales into training?} 

Numerous approaches to using human rationales in training have been tried, including: regularizing the parameters of a (linear) model \cite{zaidan_using_2007}; regularizing model output gradients \citep{ross_right_2017}; regularizing internal transformer attention weights \cite{jayaram_human_2021}; and direct supervision on a rationale model \cite{deyoung_eraser_2019}, which serves as our baseline approach in this paper. These approaches have generally failed to significantly improve model prediction accuracy \cite{hase_when_2021}. 

A quality these prior approaches have in common is treating human rationales as \textit{internally and collectively uniform} in predictive utility. That is, any token included in the human rationale is treated as equally important to include in the input representation; vice versa for tokens excluded. Furthermore, all human rationales are weighted equally.

The reality, we demonstrate empirically via ablation studies in \secref{sec:analysis}, is that the predictive utility of human rationales is distributed unevenly between tokens in a rationale, and unevenly between rationales in a dataset. Based on this analysis, we suggest that learning objectives which weight every token equally (accuracy in the case of direct supervision), and every rationale equally, are not optimal for improving downstream model accuracy. 

We operationalize these hypotheses in four distinct modifications to the baseline rationale model architecture. Three of these modify the naive token-wise accuracy supervision objective, and the fourth implements ``selective supervision'', ignoring unhelpful human rationales in training. 








Evaluating on three datasets, our proposed methods produce varying levels of improvement over both a baseline BERT model and a baseline BERT-to-BERT supervised rationale model,
ranging from  substantial for \multirc (3\%) to marginal for \esnli (0.4\%).
Additionally,
 our methods also improve rationale prediction performance. 


Taken together, our results demonstrate the importance of considering the variance of predictive utility both between and within human rationales as a source of additional training signal.
Our proposed modifications 
help pave the way toward truly effective and general \learningfromexplanation.


