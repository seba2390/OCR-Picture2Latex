\section{Data}

We consider three datasets in this work.
All three are document-query text comprehension tasks, where the task is to determine whether the query is true or false given the document.
We use the train, development, test splits offered by \citet{deyoung_eraser_2019}.
Table~\ref{tab:data} shows the basic statistics of each dataset based on the training set.






\begin{itemize}[leftmargin=*, topsep=-2pt, itemsep=-2pt]

\item \textbf{\multirc} \citep{khashabi2018looking}. A reading comprehension dataset of 32,091 document-question-answer triplets that are true or false.
Rationales consist of 2-4 sentences from a document that are required to answer the given question.



\item \textbf{\fever} \cite{thorne2018fever}. A fact verification dataset of 76,051 snippets of Wikipedia articles paired with 
claims that they support or refute. 
Rationales consist of a single contiguous sub-snippet, so the basic unit of rationale is sentence.




\item \textbf{\esnli} \cite{camburu2018snli}. A textual entailment dataset of 568,939 short snippets and claims for which each snippet either refutes, supports, or is neutral toward. 
Input texts are much shorter than \multirc and \fever, and rationales are at the token level.

\end{itemize}





\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l>{\raggedleft\arraybackslash}p{1.5cm}>{\raggedleft\arraybackslash}p{1.5cm}>{\raggedleft\arraybackslash}p{1.5cm}}
    \toprule
        Dataset & {Text length} & { Rationale length} & Rationale granularity \\
        \midrule
        \multirc & 336.0 & 52.0 & sentence \\
        \fever & 355.9 & 47.0 & sentence \\
        \esnli & 23.5 & 6.1 & token \\
         \bottomrule
    \end{tabular}
    \caption{Basic statistics of the datasets.} 
    \label{tab:data}
\end{table}
