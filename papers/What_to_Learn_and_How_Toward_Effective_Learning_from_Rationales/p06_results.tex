
\section{Results}
\label{sec:p06_results}

\begin{table*}[]
\scriptsize
\centering
\setlength\tabcolsep{5pt}
\begin{tabular}{@{}llccccccccc@{}}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multirow{2}{*}{\textbf{Acc.}} & \multicolumn{3}{c}{\textbf{Rationale prediction}}                     & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}r@{}}Human \\ Suff. Acc.\end{tabular}}} & \multicolumn{4}{c}{\textbf{Methods}}                                                                                                                                                                                                \\ \cmidrule(lr){4-6} \cmidrule(l){8-11} 
                                  & \multicolumn{1}{c}{}                                &                                & \textbf{F1}           & \textbf{Prec.}        & \textbf{Rec.}         &                                                                                       & \multicolumn{1}{c}{\textbf{Masking}} & \multicolumn{1}{c}{\textbf{Granularity}} & \textbf{\begin{tabular}[c]{@{}c@{}}Pos. class \\ weight\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Selective \\ supervision\end{tabular}} \\ \midrule
\multirow{4}{*}{\textbf{MultiRC}} & \textbf{BERT baseline}                              & 68.1                           & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 73.9                                                                                  & \multicolumn{1}{c}{-}                & Tokens                                   & -                                                                     & -                                                                         \\
                                  & \textbf{Unsupervised rationale model}                      & 67.2                           & 22.2                  & 18.5                  & 27.9                  & 71.2                                                                                  & [MASK]                               & Tokens                                   & -                                                                     & -                                                                         \\
                                  & \textbf{Supervised rationale model}                        & 67.0                           & 46.5                  & 41.5                  & 52.9                  & 70.8                                                                                  & [MASK]                               & Tokens                                   & \multicolumn{1}{r}{1.0}                                               & \multicolumn{1}{r}{No}                                                    \\
                                  & \textbf{Best overall model}                         & \textbf{71.2}                  & \textbf{57.1}                  & \textbf{44.9}                  & \textbf{78.4}                  & \textbf{74.5}                                                                                  & Embeddings                           & Sentences                                & \multicolumn{1}{r}{5.0}                                               & \multicolumn{1}{r}{No}                                                    \\ \midrule
\multirow{4}{*}{\textbf{FEVER}}   & \textbf{BERT baseline}                              & 90.2                           & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 89.4                                                                                  & \multicolumn{1}{c}{-}                & Tokens                                   & -                                                                     & -                                                                         \\
                                  & \textbf{Unsupervised rationale model}                      & 88.3                           & 22.6                  & 20.5                  & 25.1                  & 88.7                                                                                  & [MASK]                               & Tokens                                   & -                                                                     & -                                                                         \\
                                  & \textbf{Supervised rationale model}                        & 90.7                           & 68.4                  & 61.7                  & 76.7                  & 91.1                                                                                  & [MASK]                               & Tokens                                   & \multicolumn{1}{r}{1.0}                                               & \multicolumn{1}{r}{No}                                                    \\
                                  & \textbf{Best overall model}                         & \textbf{91.5}                  & \textbf{81.2}                  & \textbf{83.5}                  & \textbf{79.1}                  & \textbf{91.6}                                                                                  & Embeddings                           & Sentences                                & \multicolumn{1}{r}{1.0}                                               & \multicolumn{1}{r}{No}                                                    \\ \midrule
\multirow{4}{*}{\textbf{E-SNLI}}  & \textbf{BERT baseline}                              & 89.7                           & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} & 73.9                                                                                  & \multicolumn{1}{c}{-}                & Tokens                                   & -                                                                     & -                                                                         \\
                                  & \textbf{Unsupervised rationale model}                      & 88.9                           & 40.6                  & 28.2                  & 72.6                  & 85.0                                                                                  & [MASK]                               & Tokens                                   & -                                                                     & -                                                                         \\
                                  & \textbf{Supervised rationale model}                        & 87.8                           & 58.7                  & \textbf{47.7}                  & 76.0                  & 89.4                                                                                  & [MASK]                               & Tokens                                   & \multicolumn{1}{r}{1.0}                                               & \multicolumn{1}{r}{No}                                                    \\
                                  & \textbf{Best overall model}                         & \textbf{90.1}                           & \textbf{59.6}                  & 45.5                  & \textbf{86.2}                  & \textbf{92.3}                                                                                  & Embeddings                           & Tokens                                   & \multicolumn{1}{r}{3.0}                                               & \multicolumn{1}{r}{No}                                                    \\ \bottomrule
\end{tabular}
\caption{Best-performing model variant compared to baseline models. 
}
\label{tab:baseline_results}
\end{table*}



\subsection{Experiment Setup}

Our goal in this experiment is to understand the impact of 
our four proposed model/training modifications.
We do this with a comprehensive scan: 
We try three positive rationale supervision class weights $\lambda_{su}^1$ (\{0, 2, 4\}), and toggle sentence-level rationalization, importance embedding, selective supervision on and off.
In addition, we vary rationale supervision loss weight $\lambda_{su}$ in \{0.5, 1, 2\}.
This resulted in 72 models for \multirc and \fever, and 36 models for \esnli (for which sentence-level rationalization is not applicable). 

The best resultant model is our {\em best overall model}.
The best model with $\lambda_{su^1}=1$ (i.e., identical class weights for human rationales) and no other learning strategy enabled is our baseline {\em supervised rationale model}.
We additionally train three {\em unsupervised rationale models} with sparsity weights 0.15, 0.25, and 0.35, selecting as representative the one which produced the sparsest rationales while maintaining a reasonable level of accuracy (because in this architecture, there is invariably a trade-off between accuracy and sparsity). 


To evaluate the performance of our models, we consider both accuracy of the predicted labels ($\hat{y}$) and performance of rationale prediction in terms of F1, precision, and recall.
We use Pytorch Lightning \citep{falcon2019pytorch} for training with a learning rate of 2e-5 and gradient accumulation over 10 batches for all models. Early stopping was based on validation set loss with a patience of 3, evaluated every fifth of an epoch. Training was performed 
on two 24G NVidia TITAN RTX GPUs. 


\subsection{Model Performance}
\label{subsec:p06_results_baseline_comparison}



Table \ref{tab:baseline_results} compares our best overall model against the baselines, and presents the learning strategies used in the models.

\para{Prediction accuracy.} 
For \multirc, this best model includes every proposed modification (sentence-level rationalization, importance embeddings, class weights) except for selective supervision, and yields a 3-point improvement from the baseline accuracy of 68.1\% to 71.2\%. 
We observe a more modest improvement on \fever, with the best model using sentence-level rationalization and importance embeddings, and scoring a 1-point improvement from 90.2\% to 91.5\%. We note, however, that this approaches the accuracy of the model with access to a human rationale oracle (91.6\%).
Finally, we observe a tiny improvement of 0.4\% on \esnli, though our proposed methods do improve upon the baselines of unsupervised and supervised rationale model, which causes a performance drop. 

A McNemar's significance test with 
Bonferroni correction between the best and baseline model finds that the accuracy improvement is significant for \multirc and \fever ($p=$2e-7 and 3e-6 respectively) and not significant for \esnli ($p=0.1$). 
The limited improvement in \esnli echos the performance drop in \figref{fig:combined_unadapted_sufficiency_accuracy} without adaptation, suggesting that human rationales in this dataset are too idiosyncratic to improve model performance.




\begin{table}[]
\small
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Method}}}             & \multicolumn{3}{c}{\textbf{Coefficients}}                                                   \\ \cmidrule(l){2-4} 
\multicolumn{1}{c}{}                                             & \multicolumn{1}{c}{\textbf{MultiRC}} & \multicolumn{1}{c}{\textbf{FEVER}} & \textbf{E-SNLI} \\ \midrule
\textbf{Sentences}                                                        & .015***                              & .001                               & -               \\
\textbf{Class weights}                                                    & .017***                              & .007***                            & .005            \\
\textbf{Importance embeddings}                                                       & .012***                              & .006***                            & -.010**         \\
\textbf{Selective supervision} & 0.004                                & -.006***                           & -.032***        \\ \bottomrule
\end{tabular}
\caption{Regression coefficients for effect each proposed method on overall prediction accuracy}
\label{tab:regression_table}
\end{table}

\para{Factor analysis.}
We use regression analysis to understand the impact of the different modifications on model accuracy.
Table \ref{tab:regression_table} suggests that rationale class weighting has the highest positive effect on accuracy across datasets. Importance embeddings have a positive effect for \multirc and \fever and a negative effect for \esnli, while sentence-level rationalization improves only \multirc. 

Selective supervision is found to have a non-existant or negative effect across all three datasets. Table \ref{tab:selective_supervision} details this result, showing model accuracy and rationale performance for the best model with (yes) vs. without (no) selective supervision. 
If our method succeeded, F1 for high-HSA examples would increase from the ``No'' to the ``Yes'' models and remain flat or decrease for low-HSA examples.
Indeed, we observe lower rationale F1 for low-HSA examples, but the rationale F1 also drops substantially for high-HSA examples,
possibly because of the reduced available training data.  



\begin{table}[]
\centering
\small
\begin{tabular}{rlrrr}
\hline
\multicolumn{1}{l}{\multirow{2}{*}{\textbf{Dataset}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}l@{}}Sel.\\ Sup.\end{tabular}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Acc.}}} & \multicolumn{2}{c}{\textbf{F1.}} \\ \cline{4-5} 
\multicolumn{1}{l}{}                                  &                                                                               & \multicolumn{1}{c}{}                               & \textbf{High-HSA}   & \textbf{Low-HSA}   \\ \hline
\multirow{2}{*}{\textbf{MultiRC}}                     & \textbf{No}                                                                   & \textbf{71.2}                                      & \textbf{59.3}   & \textbf{57.2}  \\
                                                      & \textbf{Yes}                                                                  & 71.0                                               & 56.2            & 54.1           \\ \hline
\multirow{2}{*}{\textbf{FEVER}}                       & \textbf{No}                                                                   & \textbf{91.5}                                      & \textbf{79.0}   & \textbf{72.5}  \\
                                                      & \textbf{Yes}                                                                  & 90.6                                               & 61.2            & 57.0           \\ \hline
\multirow{2}{*}{\textbf{E-SNLI}}                      & \textbf{No}                                                                   & \textbf{90.1}                                      & \textbf{61.2}   & \textbf{48.0}  \\
                                                      & \textbf{Yes}                                                                  & 88.8                                               & 49.0            & 44.9           \\ \hline
\end{tabular}
\caption{Label accuracy and predicted rationale F1 for high- versus low-HSA examples.}
\label{tab:selective_supervision}
\end{table}

\para{Rationale performance.}
Although our modifications are designed to improve label prediction performance, 
they also improve rationale prediction performance in most cases. The only exception is the reduced precision in \esnli compared to the supervised rationale model. 


\input{p06_1_example_table}

\input{p06_2_nle_example_table}
\subsection{Qualitative Analysis}


Table \ref{tab:rationale_examples} shows three examples, each drawn from a different dataset, to illustrate different outcomes. For each example, we show the human rationale and predicted rationales for both the baseline supervised rationale model and our best overall model. Incorrect predictions are colored red.  

Example \ref{tab:rationale_examples}a shows an instance sampled from \multirc where our best model, with higher recall and sentence-level rationalization, more successfully captures the (sufficient) information present in the human rationale, allowing for a correct prediction where the supervised rationale model fails. 

Example \ref{tab:rationale_examples}b presents a contrasting example from the \fever dataset. The human rationale omits important context, that Legendary Entertainment is a subsidiary of Wanda Group, making it harder to infer that it is \textit{not} a subsidiary of Warner Bros. Our best model succeeds at capturing this snippet in its rationale, but still predicts the incorrect label, illustrating that  
a sufficient (for humans) rationale does not always produce a correct label.


Finally, example \ref{tab:rationale_examples}c shows a case where the baseline supervised rationale model succeeds while our best model fails. This is a hard-to-interpret example, mainly a demonstration of the limitations of rationales as an explanatory device for certain kinds of task. This begs a question: how relevant are rationales as an explanation or learning mechanism when models like GPT-3 \cite{brown_language_2020} are increasingly capable of human-level \textit{natural language} explanations (Table \ref{tab:nle_rationale_examples})?

Our position is that however an explanation is presented, meaning is still localized within text, so rationales can still serve as a useful interface for scrutinizing or controlling model logic, even if they require additional translation to be comprehensible to humans. Works that hybridize the two ideas such as \citet{zhao_lirex_2020} may represent a good way of resolving this issue. 














