

\begin{table*}[]
\small
\centering
\begin{tabular}{@{}rlrrrrrr@{}}
\toprule
\multicolumn{1}{l}{\multirow{2}{*}{\textbf{Dataset}}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Role}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Accuracy}}} & \multicolumn{3}{c}{\textbf{Rationale prediction}}                                                              & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Human \\ Suff. Acc.\end{tabular}}}} \\ \cmidrule(lr){5-7}
\multicolumn{1}{l}{}                                  &                                  & \multicolumn{1}{c}{}                               & \multicolumn{1}{c}{}                                   & \multicolumn{1}{c}{\textbf{F1}} & \multicolumn{1}{c}{\textbf{Precision}} & \multicolumn{1}{c}{\textbf{Recall}} & \multicolumn{1}{c}{}                                                                                      \\ \midrule
\multirow{8}{*}{\textbf{MultiRC}}                     & \textbf{Sentences}               & Best with                                          & 71.2                                                   & 57.1                            & 44.9                                   & 78.4                                & 74.5                                                                                                      \\
                                                      & \textbf{Sentences}               & Best without                                       & 70.6                                                   & 41.6                            & 27.7                                   & 84.1                                & 75.8                                                                                                      \\ \cmidrule(l){2-8} 
                                                      & \textbf{Class-weights}           & Best with                                          & 71.2                                                   & 57.1                            & 44.9                                   & 78.4                                & 74.5                                                                                                      \\
                                                      & \textbf{Class-weights}           & Best without                                       & 70.8                                                   & 55.2                            & 66.1                                   & 47.4                                & 76.5                                                                                                      \\ \cmidrule(l){2-8} 
                                                      & \textbf{Importance embeddings}   & Best with                                          & 71.2                                                   & 57.1                            & 44.9                                   & 78.4                                & 74.5                                                                                                      \\
                                                      & \textbf{Importance embeddings}   & Best without                                       & 71.0                                                   & 53.6                            & 39.7                                   & 82.5                                & 75.8                                                                                                      \\ \cmidrule(l){2-8} 
                                                      & \textbf{Selective supervision}   & Best with                                          & 71.0                                                   & 53.6                            & 39.7                                   & 82.5                                & 75.8                                                                                                      \\
                                                      & \textbf{Selective supervision}   & Best without                                       & 71.2                                                   & 57.1                            & 44.9                                   & 78.4                                & 74.5                                                                                                      \\ \midrule
\multirow{8}{*}{\textbf{FEVER}}                       & \textbf{Sentences}               & Best with                                          & 91.5                                                   & 81.2                            & 83.5                                   & 79.1                                & 91.6                                                                                                      \\
                                                      & \textbf{Sentences}               & Best without                                       & 91.3                                                   & 72.4                            & 61.3                                   & 88.5                                & 91.6                                                                                                      \\ \cmidrule(l){2-8} 
                                                      & \textbf{Class-weights}           & Best with                                          & 91.5                                                   & 79.6                            & 73.1                                   & 87.3                                & 91.8                                                                                                      \\
                                                      & \textbf{Class-weights}           & Best without                                       & 91.5                                                   & 81.2                            & 83.5                                   & 79.1                                & 91.6                                                                                                      \\ \cmidrule(l){2-8} 
                                                      & \textbf{Importance embeddings}   & Best with                                          & 91.5                                                   & 81.2                            & 83.5                                   & 79.1                                & 91.6                                                                                                      \\
                                                      & \textbf{Importance embeddings}   & Best without                                       & 91.4                                                   & 80.0                            & 74.9                                   & 85.9                                & 91.8                                                                                                      \\ \cmidrule(l){2-8} 
                                                      & \textbf{Selective supervision}   & Best with                                          & 90.6                                                   & 56.4                            & 41.4                                   & 88.6                                & 90.4                                                                                                      \\
                                                      & \textbf{Selective supervision}   & Best without                                       & 91.5                                                   & 81.2                            & 83.5                                   & 79.1                                & 91.6                                                                                                      \\ \midrule
\multirow{6}{*}{\textbf{E-SNLI}}                      & \textbf{Class-weights}           & Best with                                          & 90.1                                                   & 59.6                            & 45.5                                   & 86.2                                & 92.3                                                                                                      \\
                                                      & \textbf{Class-weights}           & Best without                                       & 89.9                                                   & 62.2                            & 55.7                                   & 70.4                                & 92.0                                                                                                      \\ \cmidrule(l){2-8} 
                                                      & \textbf{Importance embeddings}   & Best with                                          & 90.1                                                   & 59.6                            & 45.5                                   & 86.2                                & 92.3                                                                                                      \\
                                                      & \textbf{Importance embeddings}   & Best without                                       & 89.9                                                   & 33.5                            & 20.2                                   & 100.0                               & 72.5                                                                                                      \\ \cmidrule(l){2-8} 
                                                      & \textbf{Selective supervision}   & Best with                                          & 88.8                                                   & 49.0                            & 33.2                                   & 93.4                                & 84.0                                                                                                      \\
                                                      & \textbf{Selective supervision}   & Best without                                       & 90.1                                                   & 59.6                            & 45.5                                   & 86.2                                & 92.3                                                                                                      \\ \bottomrule
\end{tabular}
\caption{Comparison of best model with each proposed factor against best model without that factor. }
% \sam{Do I need both this and the regression?}
% \chenhao{depending on space, I prefer what you had earlier, grouped by sentences, class weights, embeddings, and selective supervision, this can go to appendix. This table is closer to typical ablation.}
\label{tab:factor_comparison}
\end{table*}

\section{Detailed Factor Analysis}

Table \ref{tab:factor_comparison} compares, for each proposed method, the performance of the best model using that method and the best model not using it. The story shown here is similar to the regression analysis in Table \ref{tab:regression_table}, but one new insight is that the improvement in model prediction performance appears to be driven by the sentence-level rationalization method, as it cuts down on stray tokens dropped from or added to the predicted rationales. 

\section{Rationale Perturbation on FEVER and E-SNLI}
\label{subsec:appendix-generalization}
Furthering the analysis in \secref{subsec:rationale-recall-analysis}, we extend the human rationale perturbation experiment to FEVER and E-SNLI. 
% More specifically, we examine the effect of rationale recall on original task performance for FEVER and E-SNLI in \secref{subsec:other-datasets}. 

% To mitigate the negative effect of using a model trained on full data to analyze shorter rationale text, we repeat the analysis using a model co-trained on both rationalized and full data in \secref{subsec:calibrated-model}.

% \subsubsection{Generalization to other datasets}
% \label{subsec:appendix-generalization-datasets}
\figref{fig:fever_perturbation_plots} show the result for \fever. \figref{fig:fever_perturbation_plots_all} shows that the baseline accuracy is so high for this dataset that to match just the baseline accuracy for FEVER, we require near perfect prediction of human rationales. 

Moreover, even for documents with HSA $=1$, the model performance drops below baseline on dropping just $\sim$ 10\% tokens (synonymous with rationale recall = $\sim$0.9) in \figref{fig:fever_perturbation_plots_suffacc_1}. Interestingly, the model performance remains consistently above the baseline when adding non-rationale tokens (synonymous with decreasing rationale precision). In comparison, the model performance for MultiRC in \figref{fig:multirc_perturbation_plots_suffacc_1} drops below baseline after dropping $\sim$50\% of the tokens. 

For \fever examples with HSA $=0$ (\figref{fig:fever_perturbation_plots_suffacc_0}), the model performance remains below the baseline accuracy consistently, supporting the second hypothesis in \secref{subsec:rationale-recall-analysis}. The near-perfect need to predict rationales in FEVER may explain behind the difference in improvements of model performance between MultiRC and FEVER.

\figref{fig:ensli_perturbation_plots} covers \esnli. We see that the model performance decreases after dropping rationale tokens (signifying decreasing recall) and it consistently remains below the baseline. In contrast, the model performance shows a slight improvement after adding non-rationale tokens (signifying decrease in rationale precision). Moreover, for documents with HSA $=1$, the model performance drops below baseline at $\sim$3\% for dropping and swapping rationale tokens, where as the model performance plateaus with addition of non-rationale tokens. These insights highlights the substantial challenges in learning from explanations for E-SNLI.

% \label{subsubsec:other-datasets}

\def \dataset {fever}
\def \basedir {new_plots}
\begin{figure*}[h]
\centering
\begin{subfigure}[t]{0.41\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{\basedir/\dataset/sufficiency-permutations-all-samples.png}
    \caption{All samples}
    \label{fig:fever_perturbation_plots_all}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.85\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-correct.png}
    \caption{Human sufficiency-accuracy = 1}
    \label{fig:fever_perturbation_plots_suffacc_1}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.83\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-incorrect.png}
    \caption{Human sufficiency-accuracy = 0}
    \label{fig:fever_perturbation_plots_suffacc_0}
\end{subfigure}
\caption{Performance of corrupted rationale for FEVER. Model performance drops below baseline accuracy immediately on both dropping human rationales (i.e., recall $\downarrow$) and adding non-rationale tokens (i.e., precision $\downarrow$). For HSA $=1$, model performance remains consistently above baseline on adding non-rationale tokens (i.e. precision $\downarrow$)}
\label{fig:fever_perturbation_plots}
\end{figure*}


\def \dataset {ensli}
\def \basedir {new_plots}
\begin{figure*}[h]
\centering
\begin{subfigure}[t]{0.41\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{\basedir/\dataset/sufficiency-permutations-all-samples.png}
    \caption{All samples}
    \label{fig:ensli_perturbation_plots_all}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.85\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-correct.png}
    \caption{Human sufficiency-accuracy = 1}
    \label{fig:ensli_perturbation_plots_suffacc_1}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.83\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-incorrect.png}
    \caption{Human sufficiency-accuracy = 0}
    \label{fig:ensli_perturbation_plots_suffacc_0}
\end{subfigure}
\caption{Performance of corrupted rationales for E-SNLI. Model performance for human rationale remains below baseline accuracy and slightly increases with addition of non-rationale tokens (i.e. precision $\downarrow$).  Even for HSA $=1$, model performance drops below baseline accuracy at just $\sim$4\% corruption.}
\label{fig:ensli_perturbation_plots}
\end{figure*}

\section{Rationale Perturbation for Adapted Models}

We perform the same perturbation analysis on calibrated model trained on both full and rationalized input, for which distribution shift from masking are less of a concern. 

In \figref{fig:multirc_cal_perturbation_plots}, for MultiRC, we find that model performance plateaus with addition of non-rationale tokens and drops quickly with rationale tokens even for a calibrated model. This observation is consistent for FEVER (\figref{fig:fever_cal_perturbation_plots}). 

For \esnli, we find different properties using a calibrated BERT model compared to the standard BERT model show in \figref{fig:ensli_perturbation_plots_all}. 

In contrast to MultiRC and FEVER, we find that the model performance drops more rapidly with the addition of non-rationale tokens compared to removal of rationale tokens. This is consistent for documents with HSA $=1$, suggesting that for E-SNLI, rationale precision maybe more important when using a calibrated model. Similar to FEVER, we see the model performance drop below the baseline with very little corruption of rationales, echoing the need to perfectly mimic human rationalization for effective learning from rationales for this dataset.


\label{subsubsec:calibrated-model}



\def \dataset {multirc-cal}
\def \basedir {new_plots}
\begin{figure*}[h]
\centering
\begin{subfigure}[t]{0.41\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{\basedir/\dataset/sufficiency-permutations-all-samples.png}
    \caption{All samples}
    \label{fig:multirc_cal_perturbation_plots_all}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.85\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-correct.png}
    \caption{Human sufficiency-accuracy = 1}
    \label{fig:multirc_cal_perturbation_plots_suffacc_1}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.83\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-incorrect.png}
    \caption{Human sufficiency-accuracy = 0}
    \label{fig:multirc_cal_perturbation_plots_suffacc_0}
\end{subfigure}
\caption{Performance of corrupted rationales for MultiRC using a calibrated model.
Model performance decreases consistently when we drop human rationales (i.e., recall $\downarrow$), where as the model performance stays high as we add non-rationale tokens (i.e., precision $\downarrow$).
The impact of recall is moderated when HSA$=1$.}
\label{fig:multirc_cal_perturbation_plots}
\end{figure*}




\def \dataset {fever-cal}
\def \basedir {new_plots}
\begin{figure*}[h]
\centering
\begin{subfigure}[t]{0.41\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{\basedir/\dataset/sufficiency-permutations-all-samples.png}
    \caption{All samples}
    \label{fig:fever_cal_perturbation_plots_all}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.85\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-correct.png}
    \caption{Human sufficiency-accuracy = 1}
    \label{fig:fever_cal_perturbation_plots_suffacc_1}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.83\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-incorrect.png}
    \caption{Human sufficiency-accuracy = 0}
    \label{fig:fever_cal_perturbation_plots_suffacc_0}
\end{subfigure}
\caption{Performance of corrupted rationales for FEVER using a calibrated model.
Model performance decreases quickly when we drop human rationales (i.e., recall $\downarrow$), where as the model performance remains above baseline as we add non-rationale tokens (i.e., precision $\downarrow$).}
\label{fig:fever_cal_perturbation_plots}
\end{figure*}

\def \dataset {ensli-cal}
\def \basedir {new_plots}
\begin{figure*}[h]
\centering
\begin{subfigure}[t]{0.41\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{\basedir/\dataset/sufficiency-permutations-all-samples.png}
    \caption{All samples}
    \label{fig:ensli_cal_perturbation_plots_all}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.85\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-correct.png}
    \caption{Human sufficiency-accuracy = 1}
    \label{fig:ensli_cal_perturbation_plots_suffacc_1}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.83\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-incorrect.png}
    \caption{Human sufficiency-accuracy = 0}
    \label{fig:ensli_cal_perturbation_plots_suffacc_0}
\end{subfigure}
\caption{Performance of corrupted rationales for E-SNLI using a calibrated model.
Model performance decreases quickly when we add non- rationale tokens (i.e., precision $\downarrow$), where as the model performance drops less rapidly as we drop rationale tokens (i.e., recall $\downarrow$).
}
\label{fig:ensli_cal_perturbation_plots}
\end{figure*}

\section{GPT-3 Prompt}
\label{subsec:appendix-gptprompt}
We generate a zero-shot GPT-3 \cite{brown_language_2020} explanation using the Davinci model variant on the OpenAI playground\footnote{https://beta.openai.com/playground}, and a modified version of the prompt proposed by \citet{wiegreffe_reframing_2021}: 

\begin{quote}

Let's explain classification decisions. 

A big dog catches a ball on his nose. 

question: A big dog is sitting down while trying to catch a ball. 

entailment, contradiction, or neutral? 

\end{quote}

A second step prompting for an explanation is not needed, as GPT-3 gives its prediction in the form of a natural language explanation. 