\section{Related Work}

\subsection{Rationalization}

The extractor-predictor rationale model proposed by \citet{lei_rationalizing_2016} and described in more detail in \secref{sec:methods}, is an approach to feature attribution, which is one among many families of explanation methods (see \citet{vilone_explainable_2020} for a recent survey). 


Recent work has extended the original architecture in various ways, including replacing the use of reinforcement learning with differentiable binary variables \cite{bastings_interpretable_2020,deyoung_eraser_2019}, alternatives to the original sparsity objective \cite{paranjape_information_2020, antognini_rationalization_2021}, and additional modules which change the interaction dynamics between the extractor and predictor \cite{carton_extractive_2018, yu_rethinking_2019, chang_invariant_2020}. Pipeline models \cite{lehman_inferring_2019} are similar, but train the two modules separately rather than end-to-end. 

Rationale models are a powerful approach to NLP explanations because of how specific objectives can be put on the properties of the rationale, but they have some downsides. First, they are unstable, the extractor often collapsing to all-0 or all-1 output \cite{deyoung_eraser_2019, yu_rethinking_2019}. We introduce an engineering trick in \secref{sec:methods} that appears to lessen this risk. Also, with end-to-end training comes the risk of information leakage between the extractor and predictor \cite{jethani_have_2021, hase_leakage-adjusted_2020, yu_understanding_2021}. This idea of leakage plays a part in how we estimate explanation predictive utility in section \secref{sec:analysis}. 



\subsection{Learning from Explanations}

\citet{wiegreffe_teach_2021} present a review of explainable NLP datasets, a number of which have been incorporated into the ERASER collection and benchmark \citep{deyoung_eraser_2019}. 

Early work in learning from human explanations include \citet{zaidan_using_2007} and \citet{druck_active_2009}, and a line of work termed ``explanatory debugging'' \citep{kulesza_principles_2015,lertvittayakumjorn_explanation-based_2021}. More recent work spans a variety of approaches, categorized by \citet{hase_when_2021} into regularization (e.g., \citet{ross_right_2017}), data augmentation (e.g., \citet{hancock2018training}), and supervision over intermediate outputs (e.g., \citet{deyoung_eraser_2019, jayaram_human_2021}).


Significant improvements to model accuracy as a result of explanation learning have proven elusive. Studies occasionally claim such improvement, such as \citet{rieger_interpretations_2020}, which observes general improvements on a medical vision task.
More commonly their claims pertain to secondary objective such as explanation quality (e.g., \citet{plumb_regularizing_2020}), 
robustness (e.g., \citet{ross_right_2017}, \citet{srivastava_robustness_2020}),
or few-shot learning (e.g., \citet{yao_refining_2021}). 
\citet{hase_when_2021} gives an overview of the problem and discusses circumstances under which learning from explanations is liable to work. Our paper contributes to this discussion by considering the variance of training signal quality both within and between human rationales, and how to exploit these variances. 






