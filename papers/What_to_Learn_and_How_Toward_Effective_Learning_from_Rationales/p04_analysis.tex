\begin{figure*}[h]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth, trim=0 0 230 0, clip]{figures/combined_basic_best_val_sufficiency_accuracy.png}
        \caption{Fine-tuned on full input (unadapted).}
        \label{fig:combined_unadapted_sufficiency_accuracy}
    \end{subfigure}
    \begin{subfigure}[t]{0.6\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/combined_adapted_best_val_sufficiency_accuracy.png}
        \caption{Fine-tuned on both full and human-rationalized input (adapted).}
        \label{fig:combined_adapted_sufficiency_accuracy}
    \end{subfigure}
   
    \caption{Baseline performance vs. human sufficiency-accuracy for \rationalizedinputs with token removal and [MASK] token substitution.
    As \rationalizedinputs are different from the full text inputs that the original training set includes, we build a calibrated model where the model is trained on both full text inputs and \rationalizedinputs. }
    \label{fig:combined_sufficiency_accuracy}
\end{figure*}




\section{Analysis}
\label{sec:analysis}



To understand properties of human rationales for the purpose of \learningfromexplanation,
we analyze the effect of human rationales when they are used 
as inputs to a trained model. 


\subsection{Human Rationales have Predictive Utility}

A basic question about the viability of \learningfromexplanation~is whether human rationales bear the potential for 
improving model performance. That is, do human explanations successfully reveal useful tokens while occluding confounding tokens, such that a model evaluated only on the revealed tokens is able to get improved performance relative to the full input? We refer to such rationale-redacted inputs as {\em \rationalizedinputs}.



\def \dataset {multirc}
\def \basedir {new_plots}
\begin{figure*}[h]
\centering
\begin{subfigure}[t]{0.41\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{\basedir/\dataset/sufficiency-permutations-all-samples.png}
    \caption{All samples}
    \label{fig:multirc_perturbation_plots_all}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.85\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-correct.png}
    \caption{Human sufficiency-accuracy = 1}
    \label{fig:multirc_perturbation_plots_suffacc_1}
\end{subfigure}
\begin{subfigure}[t]{0.28\textwidth}
    \centering
    \includegraphics[width=0.83\textwidth, trim=0 0 270 0, clip]{\basedir/\dataset/sufficiency-permutations-incorrect.png}
    \caption{Human sufficiency-accuracy = 0}
    \label{fig:multirc_perturbation_plots_suffacc_0}
\end{subfigure}
\caption{Sufficiency-accuracy of human rationales on baseline BERT model with increasing levels of corruption via swaps, drops and additions.
Model performance decreases quickly when we drop rationale tokens
, but stays high as we add non-rationale tokens.
These effects are moderated by HSA.}
\label{fig:multirc_perturbation_plots}
\end{figure*}

We define \textit{sufficiency-accuracy} (\textbf{SA}) as how accurate the model is across a corpus of \rationalizedinput.
This is an aggregate measure, similar to \textit{sufficiency} as defined in \citet{deyoung_eraser_2019} but focused on absolute performance rather than similarity to baseline model output.
We refer to the sufficiency-accuracy of the human rationales as \textit{human sufficiency-accuracy} (\textbf{HSA}).

Estimating sufficiency-accuracy is problematic. The natural way to probe whether the tokens in a rationale are sufficient for an accurate prediction is to remove the non-included tokens from the input, run the model on just the included tokens, and assess its accuracy. But a version of the input where a majority of tokens are removed or masked (by a [MASK] special token in the case of BERT), is out-of-distribution relative to the training data, which has no removal or masking. This difference may lead to unpredictable output from the model when tested on masked input. This \textbf{masking-is-OOD} problem has not received much discussion in the literature, though \citet{jacovi_aligning_2021} propose to mitigate it with random masking during model training. The effect of this problem will be to underestimate the sufficiency-accuracy of rationales tested against an un-adapted model. 

The opposite problem stems from overfitting rather than OOD issues: \textbf{label leakage}. A human rationale may contain signal about the true label that goes beyond the semantics of the tokens included in the rationale, and a model trained on human-rationalized input may learn to pick up on these spurious signals. A known example is in \esnli, where annotators had different explanation instructions based on their chosen label. This issue is discussed in several recent papers \cite{yu_understanding_2021, jethani_have_2021, hase_leakage-adjusted_2020}, albeit mostly concerning model-generated rather than human explanations. The effect of this problem will be to overestimate the sufficiency-accuracy of rationales tested against an adapted model. 

\figref{fig:combined_sufficiency_accuracy} shows sufficiency-accuracy results for human rationales on both unadapted and adapted models. We expand on the analysis presented by \citet{carton_evaluating_2020} by showing results for both masking-via-removal and masking-via-[MASK]-token-substitution.

\figref{fig:combined_unadapted_sufficiency_accuracy} shows that token removal suffers less from the masking-is-OOD problem on an unadapted model than [MASK] token substitution. [MASK] token substitution results in lower accuracy across the board, while removal improves baseline accuracy for \multirc, matches it for \fever, and lowers it for \esnli. 

With adaptation (\figref{fig:combined_adapted_sufficiency_accuracy}), token removal and [MASK] token substitution have near-identical effects, improving accuracy by a large margin for \multirc and \esnli, and a small margin for \fever. The near-100\% sufficiency-accuracy for \esnli is probably due to label leakage. 


If an unadapted model is liable to underestimate sufficiency model, and an adapted model to overestimate, then we suggest that the potential benefit of \learningfromexplanation lies somewhere between the two. On this hypothesis, this figure suggests that \multirc has a large potential benefit, \fever a small one, and \esnli an unclear benefit depending on how much of the predictive utility of \esnli rationales is due to label leakage. The results in \secref{sec:p06_results} ultimately bear out these expectations. 







\subsection{Importance of Rationale Accuracy}
\label{subsec:rationale-recall-analysis}



We focus on \multirc, where evaluating a non-rationale-adapted fine-tuned BERT model on human-rationalized data results in a sufficiency-accuracy of 74\%, a significant improvement over the normal test accuracy of 68\%. But how robust is this improvement to rationale prediction error? We examine how the sufficiency-accuracy of human rationales changes as they are corrupted by random addition, dropping, and swapping of tokens.

In this analysis, an $N\%$ drop removes $N\%$ of tokens from each rationale in the dataset, reducing recall to $100-N$. An $N\%$ addition adds tokens numbering $N\%$ the size of each rationale, from the set of non-rationale tokens, reducing precision to $\frac{100}{100+N}$. An $N\%$ swap performs both operations, swapping $N\%$ of rationale tokens for the same number of non-rationale tokens.

The ``dropped'' curve in \figref{fig:multirc_perturbation_plots_all}
shows that human rationales afford improved accuracy over the baseline until roughly 40\% of tokens have been dropped from them, suggesting that a minimum of 60\% recall is needed to derive an advantage from human rationales over the full input. Per the ``added'' curve, adding the same number of irrelevant tokens to the rationale has a much less severe impact on accuracy, suggesting 
that errors of omission are significantly worse than errors of inclusion for \learningfromexplanation.

\figref{fig:multirc_perturbation_plots_suffacc_1} and \ref{fig:multirc_perturbation_plots_suffacc_0} respectively show the effect of this perturbation on high- and low-sufficiency-accuracy human rationales, which constitute 74\% and 26\% of rationales respectively for this model. High-SA rationales follow a similar trend to the whole population, but the recall requirement is lower than \figref{fig:multirc_perturbation_plots_all} to exceed model accuracy with the full input (the ``dropped'' curve meets the blue line at 50\%).
In comparison, low-SA rationales demonstrate interesting properties. 
These rationales actually have a sabotaging effect in a quarter of cases: the model would have an accuracy of 27\% with the full input, which is lowered to 0\% by the presence of these rationales. Also, addition and dropping have a similar effect in mitigating this sabotage.
Similar results hold on \fever and \esnli except the apparent required recall is much higher ($>$90\%) for both methods (see the appendix), indicating challenges for \learningfromexplanation on these datasets.


In summary, our analyses inspire two general observations about \learningfromexplanation:
1) moving away from naive accuracy (toward recall, for example) as a rationale supervision objective, and 2) focusing on useful rationales over harmful ones.
