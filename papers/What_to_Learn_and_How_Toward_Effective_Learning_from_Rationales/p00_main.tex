% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
% \usepackage[review]{acl}
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{stfloats} % for doing bottom-placed tables

\usepackage{xcolor}
\usepackage{soul}
% \usepackage{ulem}
\usepackage{makecell}
\newcommand{\esnli}{E-SNLI\xspace}
\newcommand{\fever}{FEVER\xspace}
\newcommand{\multirc}{MultiRC\xspace}

\newif\ifhidecomments
\hidecommentsfalse
% \hidecommentstrue

\ifhidecomments
    \newcommand{\surya}[1]{}
    \newcommand{\sam}[1]{}
    \newcommand{\chenhao}[1]{}
\else
    \newcommand{\chenhao}[1]{\textcolor{blue}{[#1 ---\textsc{CT}]}}
    \newcommand{\sam}[1]{\textcolor{red}{[#1 ---\textsc{SC}]}}
    \newcommand{\surya}[1]{\textcolor{green}{[#1 ---\textsc{SK}]}}
\fi


\captionsetup[subfigure]{aboveskip=2pt,belowskip=2pt}
\captionsetup[table]{aboveskip=2pt,belowskip=2pt}
\captionsetup[figure]{aboveskip=2pt,belowskip=2pt}
\setlength{\dbltextfloatsep}{8pt}
\setlength{\dblfloatsep}{8pt}
\setlength{\floatsep}{8pt}
\setlength{\textfloatsep}{8pt}


% Aliases 

\newcommand{\para}[1]{\noindent{\bf #1}\xspace}
% \newcommand{\para}{\paragraph}

\newcommand{\figref}[1]{Fig.~\ref{#1}}
\newcommand{\equationref}[1]{Eq.~\ref{#1}}
\newcommand{\secref}[1]{\S\ref{#1}}
\newcommand{\tableref}[1]{Table~\ref{#1}}
\newcommand{\roberta}{RoBERTa\xspace}
\newcommand{\wikiattack}{WikiAttack\xspace}
\newcommand{\sst}{SST\xspace}
\newcommand{\movie}{Movie\xspace}
\newcommand{\movies}{Movie\xspace}
\newcommand{\nontoxic}{no-attack\xspace}
\newcommand{\toxic}{personal-attack\xspace}
\newcommand{\qastyle}{document/query-style\xspace}
\newcommand{\rationale}{{\bm \alpha}}
\newcommand{\data}{{\bm x}}
\newcommand{\parsimony}{parsimony\xspace}
\newcommand{\Parsimony}{Parsimony\xspace}

\newcommand{\learningfromexplanation}{learning from rationales\xspace}
\newcommand{\lfe}{LFR\xspace}
\newcommand{\rationalizedinput}{rationalized input\xspace}
\newcommand{\rationalizedinputs}{rationalized inputs\xspace}
\newcommand{\vect}[1]{\ensuremath{\bm{#1}}}
\newcommand{\vecx}{\vect{x}}
\newcommand{\vecalpha}{\vect{\alpha}}




\title{What to Learn, and How: Toward Effective Learning from Rationales}


\author{Samuel Carton\\
  University of Chicago \\
  \texttt{carton@uchicago.edu   }
  \And
  Surya Kanoria\\
  University of Colorado Boulder \\
    \texttt{   surya.kanoria@colorado.edu   } \\

  \AND
  Chenhao Tan \\
  University of Chicago \\ 
  \texttt{   chenhao@uchicago.edu}
  }


\begin{document}
\maketitle
\begin{abstract}


Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision. While intuitive, this idea has proven elusive in practice. 
We make two observations about human rationales via empirical analyses:
1) maximizing rationale supervision accuracy is not necessarily the optimal objective for improving model accuracy; 
2) human rationales vary in whether they provide sufficient information for the model to exploit for prediction.
Building on these insights, we propose several novel loss functions and learning strategies, and evaluate their effectiveness on three datasets with human rationales. Our results demonstrate consistent improvements over baselines in both label and rationale accuracy, including a 3\% accuracy improvement on MultiRC. Our work highlights the importance of understanding properties of human explanations and exploiting them accordingly in model training.




\end{abstract}

\input{p01_introduction}
\input{p02_related}
\input{p03_data}
\input{p04_analysis}
\input{p05_methods}
\input{p06_results}
\input{p07_discussion}

\para{Acknowledgments.}
We thank anonymous reviewers for their feedback, and members of the Chicago Human+AI Lab for their insightful suggestions. 
This work is supported in part by research awards from Amazon, IBM, Salesforce, and NSF IIS-2126602.

\bibliography{anthology,custom,sam_zotero}
\bibliographystyle{acl_natbib}

\newpage 
\appendix
\input{p08_appendix}

\end{document}
