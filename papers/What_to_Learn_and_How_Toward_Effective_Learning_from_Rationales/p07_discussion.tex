\section{Discussion}


The analysis in section \secref{sec:analysis} explores the limits of potential improvement from \learningfromexplanation. 
It suggests two insights toward improved \learningfromexplanation: 1) 
that insofar as they boost model accuracy, 
not all human rationale tokens are equally valuable, e.g., with false positives causing less degradation than false negatives; 
and 2) we could in principle boost label accuracy with good rationale accuracy on useful (high-SA) rationales and low accuracy on useless (low-SA) ones. 

We exploit these two insights with four modifications to the baseline architecture. Three of these diverge from flat rationale supervision accuracy: rationale supervision class weighting, sentence-level rationalization, and importance embeddings. The last, selective supervision, pursues utility-discriminative weighting during model training. 

Taken together, our proposed methods yield a substantial 3\% improvement over baseline performance for \multirc, a 1\% improvement on \fever, and a tiny .4\% improvement on \esnli, mirroring the potential improvements observed in the analysis. We find that all three token supervision methods are useful in achieving this, while selective supervision has a marginal or negative effect. 


In summary, our results support the potential for \learningfromexplanation in certain datasets, and demonstrate the importance of understanding the properties of human rationales to properly exploit them for this purpose.
We believe that these two insights are useful steps towards effective \learningfromexplanation, and could yield even greater improvements if operationalized optimally.

\para{Limitation.}
A limitation of our analysis is that all three datasets are document-query style reading comprehension tasks, as opposed to, e.g., sentiment analysis. Because of the popularity of this type of task in NLP benchmarks, this type of dataset represents a majority of what is available in 
the ERASER collection \cite{deyoung_eraser_2019}. By contrast, sentiment is often scattered throughout a text, so human rationales for sentiment are likely to contain redundant signal, which could impact their predictive utility. We leave a more comprehensive survey of NLP tasks for future work. 



