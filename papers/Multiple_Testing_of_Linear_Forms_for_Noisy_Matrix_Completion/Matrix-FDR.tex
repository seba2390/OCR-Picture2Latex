\documentclass[12pt]{article}
\usepackage[margin=.88in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb,subcaption}
\usepackage[dvips]{graphicx}
\usepackage{comment}

\usepackage[shortlabels]{enumitem}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{blkarray}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{url}
\usepackage{longtable}
\usepackage{stmaryrd}
\usepackage{mwe}
\usepackage{mathtools}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}

\newcommand{\diag}{{\rm diag}}
\newcommand{\dist}{{\rm dist}}
\newcommand{\subscript}[2]{$#1 _ #2$}
\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother



\usepackage[dvipsnames]{xcolor}
\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}
\newcommand{\cyan}{\color{cyan}}
\newcommand{\green}{\color{ForestGreen}}

\newcommand{\argmin}{\mathop{\rm arg\min}}
\newcommand{\argmax}{\mathop{\rm arg\max}}
\newcommand{\Vect}{\operatorname{Vec} }
\newcommand{\tF}{{\rm F}}
\newcommand{\SVD}{{\rm SVD}}
\newcommand{\tr}{{\rm tr}}
\newcommand{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\renewcommand{\baselinestretch}{1}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bdelta}{\sfw}

%\newcommand{\bT}{\mathbf{T}}
\newcommand{\bSigma}{\mathbf{\Sigma} }
\renewcommand{\baselinestretch}{1.4}  % single spaced
%\usepackage[singlespacing]{setspace}

\def\wt{\widehat}
\def\sfw{\mathsf{w}}
\def\DMT{D_{\textsf{\tiny MT}}}



\def\calA{{\mathcal A}}
\def\calB{{\mathcal B}}
\def\calC{{\mathcal C}}
\def\calD{{\mathcal D}}
\def\calE{{\mathcal E}}
\def\calF{{\mathcal F}}
\def\calG{{\mathcal G}}
\def\calH{{\mathcal H}}
\def\calI{{\mathcal I}}
\def\calJ{{\mathcal J}}
\def\calK{{\mathcal K}}
\def\calL{{\mathcal L}}
\def\calM{{\mathcal M}}
\def\calN{{\mathcal N}}
\def\calO{{\mathcal O}}
\def\calP{{\mathcal P}}
\def\calQ{{\mathcal Q}}
\def\calR{{\mathcal R}}
\def\calS{{\mathcal S}}
\def\calT{{\mathcal T}}
\def\calU{{\mathcal U}}
\def\calV{{\mathcal V}}
\def\calW{{\mathcal W}}
\def\calX{{\mathcal X}}
\def\calY{{\mathcal Y}}
\def\calZ{{\mathcal Z}}



\def\AA{{\mathbb A}}
\def\BB{{\mathbb B}}
\def\CC{{\mathbb C}}
\def\DD{{\mathbb D}}
\def\EE{{\mathbb E}}
\def\FF{{\mathbb F}}
\def\GG{{\mathbb G}}
\def\HH{{\mathbb H}}
\def\II{{\mathbb I}}
\def\JJ{{\mathbb J}}
\def\KK{{\mathbb K}}
\def\LL{{\mathbb L}}
\def\MM{{\mathbb M}}
\def\NN{{\mathbb N}}
\def\OO{{\mathbb O}}
\def\PP{{\mathbb P}}
\def\QQ{{\mathbb Q}}
\def\RR{{\mathbb R}}
\def\SS{{\mathbb S}}
\def\TT{{\mathbb T}}
\def\UU{{\mathbb U}}
\def\VV{{\mathbb V}}
\def\WW{{\mathbb W}}
\def\XX{{\mathbb X}}
\def\YY{{\mathbb Y}}
\def\ZZ{{\mathbb Z}}



\newtheorem{Example}{Example}  
\newtheorem{Definition}{Definition}
\newtheorem{Theorem}{Theorem}
\newtheorem{Assumption}{Assumption}

\newtheorem{Lemma}{Lemma}
%\theoremstyle{remark}
\newtheorem{Remark}{Remark}
\theoremstyle{plain}
\newtheorem{Corollary}{Corollary}

\newtheorem{Proposition}{Proposition}


\title{Multiple Testing of Linear Forms for Noisy Matrix Completion}
%\title{Controlling False Discovery Rate in Making Recommendations$^3$ }
%\title{Controlling Extraneous Recommendation Rate by Multiple Testing$^4$}
%\title{Making Quality Recommendations by Testing Multiple Hypotheses}


\author{Wanteng Ma$^1$, Lilun Du$^2$, Dong Xia$^3$ and Ming Yuan$^4$\\
~ \\
$^{1,3}$Department of Mathematics, Hong Kong University of Science and Technology\\
$^2$Department of Management Sciences,  City University of Hong Kong\\
$^4$Department of Statistics,  Columbia University\\
}
\date{(\today)}

\begin{document}


\maketitle

\footnotetext[3]{Dong Xia’s research was partially supported by Hong Kong RGC Grant GRF 16301622.} 
\footnotetext[4]{Ming Yuan's research was supported in part by NSF Grants DMS-2015285 and DMS-2052955.}

\begin{abstract}
Many important tasks of large-scale recommender systems can be naturally cast as testing multiple linear forms for noisy matrix completion. These problems, however, present unique challenges because of the subtle bias-and-variance tradeoff of and an intricate dependence among the estimated entries induced by the low-rank structure. In this paper, we develop a general approach to overcome these difficulties by introducing new statistics for individual tests with sharp asymptotics both marginally and jointly, and utilizing them to control the false discovery rate (FDR) via a data splitting and symmetric aggregation scheme. We show that valid FDR control can be achieved with guaranteed power under nearly optimal sample size requirements using the proposed methodology. Extensive numerical simulations and real data examples are also presented to further illustrate its practical merits.
\end{abstract}

\begin{sloppypar}

%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}\label{sec:intro}
%%%%%%%%%%%%%%%%%%%%%%

Popularized by the Netflix prize \citep{bennett2007netflix}, matrix completion techniques have emerged as an essential tool for large-scale collaborative-filtering-based recommender systems. See, e.g., \cite{resnick1997recommender,schafer2007collaborative, koren2009matrix, davidson2010youtube, mcauley2013hidden,das2017survey}. Consider, more specifically, representing the ratings of $d_1$ users on $d_2$ products/items by a $d_1\times d_2$ matrix. For all practical purposes, both $d_1$ and $d_2$ can be very large yet only a rather small number of the entries can be observed. The idea is that if the interaction between users and products can be approximately captured by a handful of latent user-specific and product-specific characteristics, then it is possible to infer the whole user-item rating matrix from these sparsely observed entries, and hence recommend products to users who may be genuinely interested in them. Since the pioneering works of \cite{candes2009power,candes2010matrix,candes2012exact}, a lot of impressive progress has been made to make these techniques more accurate and scalable, and to better understand the statistical and computational underpinnings of the problem. See, e.g., \cite{cai2010singular,keshavan2010matrix,recht2010guaranteed,gross2011recovering,koltchinskii2011nuclear,liu2011universal,negahban2011estimation,rohde2011estimation,tsybakov2011nuclear,negahban2012restricted, sun2012calibrated, klopp2014noisy,cai2015rop,cai2016matrix,gao2016optimal}, among numerous others. 

Most of these existing works study recommender systems from an estimation perspective and investigate how well the user-item matrix can be estimated or reconstructed collectively. These are clearly relevant metrics for evaluating recommender systems. For example, the Netflix prize uses root mean squared error as the gold standard for the competition. Yet they do not account for the fact that only a subset of the products can be recommended to a user and as such estimation accuracy may not be directly translated into the quality of these recommendations. Instead, various classical notions for binary classification such as precision and recall are often adopted in practice to evaluate the quality of top recommendations. See, e.g., \cite{herlocker2004evaluating}. This subtlety has significant statistical implications. First of all, making quality recommendations requires a more careful uncertainty quantification. Consider recommending between a blockbuster movie and an independent film to a user. Even if both estimated ratings are similar and favorable, the uncertainty associated with the estimated rating for the former is likely to be much smaller as it has been viewed by a much greater number of people. It could therefore be more prudent to recommend it over the latter. On the other hand, as each recommendation incurs uncertainty, when making a list of recommendations, it is more helpful to assess their quality collectively rather than individually. For example, the percentage of relevant recommendations among all recommended products could be a more meaningful measure than the chance of a specific recommendation being relevant. Both aspects draw immediate comparison with multiple testing problems, for example, in high-throughput gene expression studies where, among thousands of genes, a small subset that are likely to behave differently between control and treatment groups are sought. See, e.g., \cite{storey2003statistical, efron2007correlation, efron2012large}. Our work is inspired by this analogy and examines the problem of item recommendations from a multiple testing perspective.

For the sake of generality, we shall adopt the framework of trace regression where each observation is a random pair $(X,Y)$ with $X\in\R^{d_1\times d_2}$ and $Y\in\R$. The random matrix $X$ is sampled uniformly from the orthonormal basis $\mathfrak{E}=\{e_{i}e_{j}^{\top}: 1\le i\le d_1, 1\le j\le d_2\}$ where $\{e_{i}\}$ is the canonical basis vectors of an Euclidean space of conformable dimensions. The response variable $Y$ is related to $X$ via
\begin{equation}\label{eq:NMC}
	Y=\langle  M, X\rangle +\xi
\end{equation}
where $\langle M, X\rangle=\tr(M^{\top}X)$, and the independent measurement error $\xi$ is assumed to be a centered sub-Gaussian random variable. Our goal is to infer the true user-product preference matrix $M$ from i.i.d. copies of $(X,Y)$ when $M$ is of (approximately) low rank and the number of observations is much smaller than $d_1d_2$. Specifically, the task of deciding if product $j$ should be recommended to user $i$ can be cast as testing the null hypothesis, denoted by $H_{0,ij}$, about the $(i,j)$ entry of the true user-product matrix $M$, e.g., product $j$ is irrelevant to user $i$, against the alternative, denoted by $H_{a,ij}$, that user $i$ is interested in product $j$. Likewise, item recommendations in general amount to testing collectively all null hypotheses $H_{0,ij}$, $1\le i\le d_1$ and $1\le j\le d_2$. More broadly, one may consider testing about multiple linear forms, $\langle M,T \rangle$ for a family of $T\in \cH\subset \R^{d_1\times d_2}$. For example, one may consider $T$ of the form $e_ie_{j_1}^\top-e_ie_{j_2}^\top$ to determine between two products ($j_1$ and $j_2$) which one to recommend to a user ($i$). This multiple testing framework allows us to address, among others, two most pertinent questions for recommender systems: which items should we recommend so that we can ensure a certain percentage of recommendations are relevant, or click-through rate; given a list of recommendations, what percentage of recommendations are relevant. Both questions can be naturally rephrased in terms of the so-called false discovery rate (FDR), commonly used in the context of multiple testing. 

Since its introduction in the seminal paper by \cite{benjamini1995controlling}, FDR has proven to be an extremely useful notion in a wide variety of areas including bioinformatics \citep{jung2005sample,roeder2009genome,brzyski2017controlling}, neuroimaging \citep{perone2004false,chumbley2010topological}, and finance \citep{barras2010false,bajgrowicz2012technical}, to name a few. Numerous methodologies have also been developed to control FDR in multiple testing. Notable examples includes \cite{benjamini2001control,sarkar2002some,wu2008false,clarke2009robustness,barber2015controlling,candes2018panning,barber2019knockoff}, among many others. There are, however, considerable new challenges when considering multiple testing in the context of item recommendations or matrix completion, both in defining test statistics for individual hypothesis and in how to utilize them effectively to improve the overall performance.

In most if not all of the existing literature of multiple testing, the individual test statistics are either given or naturally defined. For matrix completion, however, finding the right test statistics is arguably one of the most difficult steps for statistical inferences. Common estimators for entries of the underlying matrix do not admit an explicit expression, which creates technical obstacles to characterize their bias and variance. This challenge is already in full display when testing a single hypothesis which occurs, for example, when deciding on whether to recommend a specific product to a particular user. See, e.g., \cite{chen2019inference, xia2021statistical,farias2022uncertainty,chen2023statistical,gui2023conformalized, shao2023distribution}. The problem is exacerbated when dealing with multiple hypotheses where more refined bounds for the convergence of test statistics are needed both for controlling the FDR and to ensure power without unnecessary sample size and signal-to-noise ratio restriction. We shall introduce a new test statistic especially suitable for such purposes. It builds upon recent developments \citep[e.g.,][]{chen2019inference,xia2021statistical} for inferring a single entry and is based upon a more precise characterization of variance than earlier works. In particular, it can be shown that, with the improved variance estimate, the new statistic converges to normal distribution at a faster rate, both marginally and jointly, and is thus more suitable for use in multiple testing.

Most procedures for FDR control were developed, at least initially, assuming that the individual test statistics are independent of each other. How to handle complicated dependency structure, as is the case for matrix completion, remains a critical issue and an actively researched subject in multiple testing. See, e.g., \cite{efron2007correlation, leek2008general, fan2017estimation, li2017rate, du2021false, fithian2022conditional}. A common strategy to deal with dependence is data splitting. See, e.g., \cite{roeder2009genome, song2015split, barber2019knockoff, zou2020new, du2021false, dai2022false, dai2023scale}, for a number of recent examples and applications of data splitting schemes. In particular, \cite{du2021false} showed that the FDR can be properly controlled as long as the individual test statistics have nearly symmetric null distribution and the dependence among them is sufficiently weak. To make use of this insight, we derive the asymptotic correlation of our proposed individual test statistics. Interestingly, for many item recommendation tasks, these statistics are only weakly correlated and hence, the FDR can be controlled accordingly. In other settings where the test statistics can be strongly correlated, our explicit characterization of their dependence structure also suggests ways to ``whitening'' and ``screening'' so that FDR can still be controlled under minimal sample size requirement.

The rest of the paper is organized as follows. In the next section, we shall introduce our test statistics for a single linear form and study its asymptotic properties. Section \ref{sec:FDR-SDA} discusses how these individual test statistics can be aggregated to test multiple linear forms. Section \ref{sec:strong-corr} introduces a whitening and screening scheme to address situations where the test statistics could be strongly correlated. Numerical experiments, both simulated and real-world data examples, are presented in Section \ref{sec:experiments}. We conclude with a few remarks in Section \ref{sec:remark}. Due to space limitation, all proofs, as well as further examples and discussions, are relegated to the Supplement.

Throughout the paper, let $\|\cdot\|$ denote the spectral norm of a matrix and the $\ell_2$-norm of a vector, and  denote $\|M\|_{2,\max}:=\max_{i\in d_1}\|e_i^{\top}M\|$. Define $\|R\|_{\max}=\max_{i,j}|R_{ij}|$ and $\|R\|_{\infty}:=\max_{i\in[q]} \|e_i^{\top} R\|_{\ell_1}$ for a matrix $R$. Note that $\|\cdot\|_{\max}$ and $\|\cdot\|_{\infty}$ are equivalent for a vector. 

\section{Individual Tests}\label{sec:clt}

We begin with testing a single hypothesis:
\begin{equation}\label{eq:general-rtt}
	H_{0T}: \langle M, T\rangle =\theta_{T}\qquad {\rm vs}\qquad  H_{aT}: \langle M, T\rangle \neq \theta_{T}
\end{equation}
for some fixed $T\in\R^{d_1\times d_2}$ and pre-specified value $\theta_T\in \R$, based on $n$ independent observations $\cD:=\{(X_i,Y_i): 1\le i\le n\}$ following the trace regression model \eqref{eq:NMC}. Recall that $\xi$ in \eqref{eq:NMC} is sub-Gaussian noise with mean $0$ and variance $\sigma_\xi$ such that $\E \exp{(\lambda \xi)}\le \exp{(c^2 \sigma_\xi^2\lambda^2 /2 )}$ for some constant $c>0$. Following the convention, we shall assume that the singular vectors of $M$ are incoherent:
\begin{equation}\label{eq:incoherence}
	\max \left\{\sqrt{\frac{d_1}{r}}\|U\|_{2, \max }, \sqrt{\frac{d_2}{r}}\|V\|_{2, \max }\right\} \leq \mu,
\end{equation}
where $r$ is the rank of $M$, $\|\cdot\|_{2,\max}$ denotes the maximum row-wise $\ell_2$-norm, and $M=U\Lambda V^\top$ its singular value decomposition. This ensures that the entries of $M$ are delocalized so that it can be recovered even if some entries are not observed. In what follows, we shall denote by $\lambda_{\max}$ and $\lambda_{\min}$ the largest and smallest nonzero singular values, respectively, of $M$, and $\kappa_0$ the ratio between the two, i.e., its condition number.

For brevity, we consider two-sided tests here but our discussion can be applied to one-sided tests straightforwardly. We shall also assume, without loss of generality, that $d_1\ge d_2$, in what follows. Our goal of this section is to develop a test statistic for \eqref{eq:general-rtt} that is readily applicable for testing a large number of hypotheses. The problem of testing a single linear form \eqref{eq:general-rtt} has been previously investigated by \cite{xia2021statistical}. See also \cite{chen2019inference,farias2022uncertainty, chen2023statistical}, among others, for treatment of the special case when $T=e_ie_j^\top$. The tests proposed in these earlier works however cannot be directly used for multiple testing. For example, the test statistics from \cite{xia2021statistical}, and similarly others, converge to normal distribution at a rate no faster than $\sqrt{\log d_1/d_2}$. This is too slow for our purpose because it puts an unnecessary limit on the number of hypotheses we can test, regardless of how large the sample size ($n$) is.

We start by estimating $\langle M, T\rangle$. A general approach consists of three steps: initialization, bias-correction and low rank projection. More specifically, assume that, without loss of generality, $n$ is an even number with $n=2n_0$. We split $\cD$ into two sub-samples:
$$
\cD_1=\big\{(X_i, Y_i)\big\}_{i=1}^{n_0}\quad {\rm and}\quad \cD_2=\big\{(X_i, Y_i)\big\}_{i=n_0+1}^{n}.
$$
Assume that an initial estimating procedure is available so that there exists an initial estimate $\widehat{M}_1^{\mathsf{init}}$ (or $\widehat{M}_2^{\mathsf{init}}$) from $\cD_1$ (or $\cD_2$) such that for any $\tau\ge 1$,
\begin{equation}\label{eq:init-est}
	\norm{\widehat{M}_1^{\mathsf{init}} - M}_{\max} \le C  \sigma_\xi \mu \kappa_0 \sqrt{\frac{\tau r^2 d_1 \log ^2 d_1}{n }} ,
\end{equation}
with probability at least $1-d_1^{-\tau}$, for some constant $C>0$. Here $\|\cdot\|_{\max}$ represents the maximum entry-wise magnitude. 
This requirement for initialization is fairly weak and satisfied, in particular, by several recently developed matrix completion techniques, including those from \cite{wei2016guarantees,ma2018implicit,chen2020noisy,xia2021statistical,cai2022generalized} among others. For brevity, in what follows, we shall assume $\tau$ is large enough to ensure that $n=O(d_1^{2\tau})$.

To correct the bias of initial estimates, we then define
$$
\widehat{M}_1^{\mathsf {unbs }}=\widehat{M}_1^{\mathsf{init}}+\frac{d_1 d_2}{n} \sum_{i=n_0+1}^n\left(Y_i-\left\langle\widehat{M}_1^{\mathsf{init}}, X_i\right\rangle\right) X_i 
$$
and similarly
$$
\widehat{M}_2^{\mathsf {unbs }}=\widehat{M}_2^{\mathsf{init}}+\frac{d_1 d_2}{n} \sum_{i=1}^{n_0}\left(Y_i-\left\langle\widehat{M}_2^{\mathsf{init}}, X_i\right\rangle\right) X_i 
$$
Unfortunately this debiasing may lead to a significant increase in variance, we shall again trade off between bias and variance by low-rank projection, yielding an estimate
$$
\widehat{M}=\frac{1}{2}[\cP_r(\widehat{M}_1^{\mathsf {unbs }})+\cP_r(\widehat{M}_2^{\mathsf {unbs }})],
$$
where $\cP_r(\cdot)$ is the best rank-$r$ approximation of a matrix, i.e., the projection of a matrix to row and column spaces spanned by its first $r$ singular vectors. Finally we shall estimate $\langle M, T\rangle$ by $\langle \widehat{M}, T\rangle$. Inferences about $\langle M, T\rangle$ can naturally be made by studying the distribution of $\langle \widehat{M}, T\rangle$. Under certain regularity conditions, one can show that
\begin{equation}\label{eq:xyclt}
\frac{\langle \widehat{M}, T\rangle-\langle M, T\rangle}{\sigma_\xi(\|U^\top T\|_{\rm F}^2+\|TV\|_{\rm F}^2)^{1/2}\sqrt{d_1d_2/n}}\to_dN(0,1),
\end{equation}
as $n, d_1,d_2\to\infty$, where $\|\cdot\|_{\rm F}$ denotes Frobenius norm. See, e.g., \cite{chen2019inference,xia2021statistical,cai2022uncertainty}. We can use this result to test \eqref{eq:general-rtt} for a fixed $T$. But if we want to test for a family of hypothesis $\{H_{0T}: T\in \cH\}$, then more refined bounds on the convergence rates of \eqref{eq:xyclt} are needed.

Our key insight is that the slow convergence rates obtained in earlier works can be attributed to the fact that the variance of $\langle \widehat{M}, T\rangle$ in \eqref{eq:xyclt} is not sufficiently precise. More specifically, \eqref{eq:xyclt} uses the following variance approximation:
\begin{equation}\label{eq:xyvar}
{n\over d_1d_2}\cdot \texttt{var}(\langle \widehat{M}, T\rangle)\approx \sigma_\xi^2(\|U^\top T\|_{\rm F}^2+\|TV\|_{\rm F}^2).
\end{equation}
While this is a good first order approximation, one can derive an improved approximation. Specifically, we shall show that 
\begin{equation}\label{eq:var}
{n\over d_1d_2}\cdot \texttt{var}(\langle \widehat{M}, T\rangle)\approx \sigma_\xi^2\|\cP_M(T)\|_{\rm F}^2,
\end{equation}
where
$$\cP_M(A)=UU^\top AVV^\top+UU^\top AV_\perp V_\perp^T+U_\perp U_\perp^TAVV^\top$$
and $U_\perp$ and $V_\perp$ are orthonormal matrices whose columns span the orthogonal complements of the left and right singular spaces of $M$ respectively. Note that
$$
\|\cP_M(T)\|_{\rm F}^2=\|U^\top T\|_{\rm F}^2+\|TV\|_{\rm F}^2-\|U^\top TV\|_{\rm F}^2
$$
so that the difference between the two variance approximations \eqref{eq:xyvar} and \eqref{eq:var} is the term $\sigma_\xi^2\|U^\top TV\|_{\rm F}^2$. It is instructive to consider the special case of estimating one entry of $M$, i.e., $T=e_ie_j^\top$. Denote $\|\cdot\|$ the $\ell_2$-norm of a vector. 
Then the difference becomes $\sigma_\xi^2\|e_i^\top U\|^2\|e_j^\top V\|^2$ which is of smaller order than the approximation \eqref{eq:xyvar}: $\sigma_\xi^2(\|e_i^\top U\|^2+\|e_j^\top V\|^2)$, in light of the incoherence condition \eqref{eq:incoherence}. Indeed, \eqref{eq:xyclt} immediately yields:
$$
{\langle \widehat{M}, T\rangle-\langle M, T\rangle\over \sigma_\xi\|\cP_M(T)\|_{\rm F}\sqrt{d_1d_2/n}}\to_dN(0,1).
$$
However, the enhanced variance approximation can significantly improve the rate of convergence as the following theorem shows.

\begin{Theorem}\label{thm:asymp-normal}
Suppose that the sample size $n \geq C_1 \mu^2 r d_1 \log d_1$, and
$$
\lambda_{\min}\geq C_2 \mu \sigma_{\xi}\kappa_0^2  \sqrt{\frac{ r  d_1^3 \log ^2 d_1}{n}},
$$
for some constants $C_1,C_2>0$. Then there exists a constant $C_3>0$ such that for any $T
\in \R^{d_1\times d_2}$,
\begin{eqnarray*}
\sup _{t \in \mathbb{R}}\left|\mathbb{P}\left(\frac{\langle \widehat{M}, T\rangle-\langle M, T\rangle}{\sigma_{\xi}\|\cP_M(T) \|_\tF \cdot \sqrt{d_1 d_2 / n}} \leq t\right)-\Phi(t)\right| \\
\le C_3\left(  \frac{ \kappa_0 \mu^2 \|T\|_{\ell_1} }{ \|\cP_M(T)\|_{\mathrm{F}} } \frac{\sigma_{\xi}  }{\lambda_{\min} } \sqrt{\frac{ r^2 d_1^2 \log ^2 d_1}{n}} + \mu \kappa_0 \sqrt{\frac{r^2 d_1 \log ^3 d_1}{n}}\right). % \tau\gamma_{}(n, d_1, d_2) \sqrt{\log d_1} 
\end{eqnarray*}
\end{Theorem}

Of course, to use the asymptotic normality established above for testing \eqref{eq:general-rtt}, we need to estimate the variance. An intuitive choice is to estimate $\sigma_\xi^2$ by
$$
\widehat{\sigma}_{\xi}^2= \frac{1}{2n_0}\sum_{i=n_0+1}^{n}\big(Y_i-\langle \widehat{M}_1^{\rm init}, X_i\rangle\big)^2+\frac{1}{2n_0}\sum_{i=1}^{n_0}\big(Y_i-\langle \widehat{ M}_2^{\rm init}, X_i\rangle \big)^2,
$$
and $\cP_M(T)$ by $\cP_{\widehat{M}}(T)$. We shall therefore consider the following test statistic:
$$
W_T:=\frac{\langle \widehat{M}, T\rangle-\theta_T}{\widehat{\sigma}_{\xi}\|\cP_{\widehat{M}}(T) \|_\tF \cdot \sqrt{d_1 d_2 / n}}.
$$
The following result shows that the asymptotic normality continues to hold using these variance estimates.

\begin{Theorem}\label{thm:asymp-normal-varest} 
	Under the assumptions of Theorem \ref{thm:asymp-normal}, if $H_{0T}$ holds, then
$$
\sup _{t\in \mathbb{R}}\left|\mathbb{P}\left(W_{T} \leq t\right)-\Phi\left(t\right)\right|
\le C_3\left(  \frac{ \kappa_0 \mu^2 \|T\|_{\ell_1} }{ \|\cP_M(T)\|_{\mathrm{F}} } \frac{\sigma_{\xi}  }{\lambda_{\min} } \sqrt{\frac{ r^2 d_1^2 \log ^2 d_1}{n}} + \mu \kappa_0 \sqrt{\frac{r^2 d_1 \log ^3 d_1}{n}}\right). % \tau\gamma_{}(n, d_1, d_2) \sqrt{\log d_1} 
$$
\end{Theorem}

\section{Multiple Tests}\label{sec:FDR-SDA}

We now turn our attention to testing a family of hypothesis $\{H_{0T}: T\in \cH\}$ for a subset $\cH\subset \R^{d_1\times d_2}$. In particular, we can take $\cH=\{e_i e_j^\top: 1\le i\le d_1, 1\le j\le d_2\}$ for testing preferences of all user-item pairs. Denote the number of tests $\abs{\cH}=q$. Without loss of generality, assume that the linear forms are linearly independent so that the $q$ is no larger than $d_1 d_2$.  Denote the null set by $\cH_0$, i.e., $\cH_0=\left\{T\in \cH: \langle M,T\rangle=\theta_T \right\}$ and the non–null set $\cH_1=\cH\setminus \cH_0$, with cardinality $q_0$ and $q_1$ respectively. In addition, we shall also assume that there exists a constant $\beta_0>0$ such that for all $T\in \cH$,
\begin{equation}\label{eq:alignment}
	\norm{\cP_M(T)}_\tF\ge\beta_0 \|T\|_{\rm F}\sqrt{r\over d_1}.
\end{equation}
Recall that from the previous section, $\norm{\cP_M(T)}_\tF$ is proportional to the (asymptotic) variance of the test statistic with respect to a linear form $T$. When $\norm{\cP_M(T)}_\tF=0$, the linear form $\langle M, T\rangle=0$ and estimates with faster rate of convergence can be obtained. This condition avoids such pathological situations. Similar assumptions are also made in earlier works. See, e.g., \cite{xia2021statistical}. Write
\begin{equation}\label{eq:fixed-asymp}
h_n:=\kappa_0 \mu^2   \sup_{T\in \cH}\left\{\frac{ \|T\|_{\ell_1} }{ \|T\|_{\mathrm{F}} }\right\} \frac{\sigma_{\xi}  }{\lambda_{\min} \beta_0 } \sqrt{\frac{ r d_1^3 \log ^2 d_1}{n}} + \mu \kappa_0 \sqrt{\frac{r^2 d_1 \log ^3 d_1}{n}},
\end{equation}
where, for brevity, we omit the dependence of $h_n$ on $d_1$. In light of Theorem \ref{thm:asymp-normal-varest}, with appropriate initial estimates, we have
$$
\left|\mathbb{P}\left(W_{T} \leq t\right)-\Phi\left(t\right)\right|\lesssim h_n
$$
for all $T\in \cH$.

\subsection{Symmetric Data Aggregation}
With the asymptotic normality of $W_T$, it is possible to directly apply \cite{benjamini1995controlling} style of methods to control the FDR in an asymptotic sense. However, doing so may put an unreasonable limit on the number ($q$) of tests under consideration. This is due to the fact that the test statistic $W_T$ has much heavier tail than that in classic multivariate normal mean problems. As a result, while $W_T$ converges to $N(0,1)$ in distribution for any linear form $T$ as long as signal strength is large enough, it does not necessarily converge in fourth-order or higher-order moments. Indeed, it can be shown that the $2k$-th order moment ($k\ge 2$) of $W_T$ for a properly chosen linear form $T$ is lower bounded by
\begin{equation}\label{eq:W-kth-moment}
   \sqrt[2k]{ \E \abs{W_{T}}^{2k}} \gtrsim \left(\frac{d_1 d_2}{n}\right)^{1/4}.
\end{equation}
If $d_1\asymp d_2 \asymp d $, and $n \asymp d^{1+\epsilon }$ for some $\epsilon\in(0,1)$, then we have $\E \abs{W_{T}}^{2k} \gtrsim d^{(1-\epsilon)k/2}$. See supplement for proof of \eqref{eq:W-kth-moment}.

Thankfully much more powerful approaches can be developed by exploiting other salient features of $W_T$ entailed by its asymptotic normality. In particular, we shall adopt a general strategy introduced by \cite{du2021false}. More specifically, we first construct two groups of (conditionally) independent asymptotic symmetric statistics $\{W_{T}^{(1)}: T\in\cH\}$ and $\{W_{T}^{(2)}: T\in \cH\}$ by data splitting.  After that, we aggregate them by multiplication: $W_T^{\mathsf{Rank} }= W_T^{(1)} \cdot W_T^{(2)} $. Finally, we rank each $W_T^{\mathsf{Rank} }$ and choose a data-driven threshold by taking advantage of symmetricity:
\begin{equation}\label{eq:dd-threshold}
    L:=\inf \left\{t>0: \frac{\#\left\{T: W_T^{\mathsf{Rank} }<-t\right\}}{\#\left\{T: W_T^{\mathsf{Rank} }>t\right\} \vee 1} \leq \alpha\right\},
\end{equation}
given any FDR level $\alpha\in (0,1)$, and reject $H_{0T}$ if $W_T^{\mathsf{Rank}}>L$. Details are given in Algorithm \ref{alg:matrix-fdr}. Hereafter, we denote $M_T:=\langle M, T\rangle$ for simplicity. 

\begin{algorithm}[h]
	\caption{Matrix FDR Control}
	\label{alg:matrix-fdr}
	\begin{algorithmic}[1]
		\REQUIRE Hypotheses $\left\{H_{0T}: M_T=\theta_T, T\in \cH\right\}$, data splits $\cD_0$, $\cD_1$, $\cD_2$, rank $r$, FDR level $\alpha$.
		\STATE{ Use $\cD_0$ to construct an initial estimate $\widehat{M}_{\mathsf{init}}$ }
		\STATE{ Apply bias-correction and low-rank projection using the second part of data $\cD_1$ and the third part of data $\cD_2$, respectively, and obtain two groups of test statistics:
			\begin{equation*}
				W_{T}^{(1)}:= \frac{\langle\widehat{M}^{(1)}, T\rangle - \theta_T}{ \widehat{\sigma}^{(1)}_{\xi}\|\calP_{\widehat M^{(1)}}T\|_{\rm F} \sqrt{d_1 d_2/n} }, \quad W_{T}^{(2)}:= \frac{\langle \widehat{M}^{(2)}, T\rangle - \theta_T}{ \widehat{\sigma}^{(2)}_{\xi }\|\calP_{\widehat M^{(2)}}(T)\|_{\rm F}\sqrt{d_1 d_2/n}}, \quad T\in \cH
			\end{equation*}
		}
		
		\STATE { Compute the final ranking statistics by $W_T^{\mathsf{Rank}}=W_{T}^{(1)}W_{T}^{(2)}$, and then choose a data-driven threshold $L$ by \eqref{eq:dd-threshold}.}
%			\begin{equation}\label{eq:threshold-alg}
%				L:=\inf \left\{t>0: \frac{\sum_{T\in\cH}\bbI\left( W_T^{\mathsf{Rank}}<-t\right)}{\sum_{T\in\cH} \bbI \left( W_T^{\mathsf{Rank}}>t\right) \vee 1} \leq \alpha\right\}
%		\end{equation}}
		\STATE{Reject $H_{0T}$ if $W_T^{\mathsf{Rank}}>L$.}
	\end{algorithmic}
\end{algorithm}

Here we split the data such that $\abs{\cD_0} \asymp \abs{\cD_1}=\abs{\cD_2}=n$ in general. Note that $\widehat{M}^{(1)},\widehat{\sigma}^{(1)}_{\xi}$ are computed from $\widehat{M}_{\mathsf{init}}$ and $\cD_1$; while $\widehat{M}^{(2)},\widehat{\sigma}^{(2)}_{\xi}$ are computed from $\widehat{M}_{\mathsf{init}}$ and $\cD_2$. Clearly, conditional on $\cD_0$, $ W_{T}^{(1)}$ and $ W_{T}^{(2)}$ are independent. By the definition of $L$,  we have
$$
\text{FDP} = \frac{\sum_{T\in \cH  } \bbI(W_T^{\mathsf{Rank}} <-L )}{\left( \sum_{T\in \cH  } \bbI(W_T^{\mathsf{Rank}} >L ) \right) \vee 1 } \frac{\sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank}} >L ) }{\sum_{T\in \cH  } \bbI(W_T^{\mathsf{Rank}} <-L )  }  \le \alpha \frac{\sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank}} >L ) }{ \sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank}} <-L )}.
$$
The crux of our argument is that the ratio on the rightmost hand side is approximately $1$ by virtue of the symmetry of $W_T^{\mathsf{Rank}}$. To do so we need to first investigate the dependence among multiple test statistics. 

We remark that, in addition to multiplying the two tests statistics $W_T^{\mathsf{Rank}}=W_{T}^{(1)}W_{T}^{(2)}$, other ways of aggregating the two test statistics are also possible. See, e.g., \cite{dai2022false}. Notable examples including $\min\{W_{T}^{(1)},W_{T}^{(2)}\}$ and $W_{T}^{(1)}+W_{T}^{(2)}$ that have been studied earlier by \cite{xing2021controlling,dai2022false,dai2023scale}. Our choice of the multiplicative data aggregation is motivated by an observation that for testing about the multivariate normal mean, it can be more powerful than the other two choices. See supplement for detailed discussion.

\subsection{Dependence among Test Statistics}\label{sec:dependence}
One of the main challenges for multiple testing is how to account for the dependence structure among test statistics. To this end, we shall first derive the asymptotic distribution for the joint distribution of two estimated linear forms. In particular, for two matrices $T_1, T_2\in \R^{d_1\times d_2}$, it can be shown that
\begin{equation}\label{eq:corr}
\texttt{corr}(\langle \widehat{M}, T_1\rangle, \langle \widehat{M}, T_2\rangle)\approx \frac{\left\langle \cP_M(T_1) ,\cP_M(T_2)  \right\rangle }{ \norm{\cP_M(T_1)}_{\tF}\norm{\cP_M(T_2)}_{\tF} }=:	\rho_{T_1,T_2}.
\end{equation}
More specifically, we have


\begin{Theorem}\label{thm:asymp-two-var} 
	Suppose that the sample size $n \geq C_1 \mu^2 r d_1 \log d_1$, and
	$$
	\lambda_{\min}\geq C_2 \mu \sigma_{\xi}\kappa_0^2  \sqrt{\frac{r  d_1^3 \log ^2 d_1}{n}},
	$$
	for some constants $C_1,C_2>0$. For any two matrices $T_1, T_2\in \R^{d_1\times d_2}$ such that $|\rho_{T_1,T_2}|<1$, we have
	\begin{equation*}
		\begin{aligned}
			&\sup _{t_1,t_2 \in \mathbb{R}}\left|\mathbb{P}\left(\frac{\langle \widehat{M}, T_1\rangle-\langle M, T_1\rangle}{\sigma_{\xi}\|\cP_M(T_1) \|_\tF \cdot \sqrt{d_1 d_2 / n}} \leq t_1, \frac{\langle \widehat{M}, T_2\rangle-\langle M, T_2\rangle}{\sigma_{\xi}\|\cP_M(T_2) \|_\tF \cdot \sqrt{d_1 d_2 / n}} \leq t_2\right)-\Phi_{
				\rho_{T_1,T_2}}\left(t_1,t_2\right)\right|   \\
			& \le C_3\left[\frac{\kappa_0 \mu^2\sigma_{\xi}}{\lambda_{\min}} \left({\|T_1\|_{\ell_1}\over \|\cP_M(T_1)\|_{\rm F}}+{\|T_2\|_{\ell_1}\over \|\cP_M(T_2)\|_{\rm F}}\right)\sqrt{\frac{r^2 d_1^2 \log ^2 d_1}{n}}+ \mu \kappa_0 \sqrt{\frac{r^2 d_1 \log ^3 d_1}{n}} \right],
		\end{aligned}
	\end{equation*}
	where $\Phi_\rho(\cdot,\cdot)$ is the cumulative distribution function of bivariate normal distribution $N(0,((1, \rho)^\top, (\rho, 1)^\top))$. Moreover, if both $H_{0T_1}$ and $H_{0T_2}$ hold, then
	\begin{equation*}
		\begin{aligned}
			&\sup _{t_1,t_2 \in \mathbb{R}}\left|\mathbb{P}\left(W_{T_1} \leq t_1, W_{T_2} \leq t_2\right)-\Phi_{
				\rho_{T_1,T_2}}\left(t_1,t_2\right)\right|   \\
			& \le C_3\left[\frac{\kappa_0 \mu^2\sigma_{\xi}}{\lambda_{\min}} \left({\|T_1\|_{\ell_1}\over \|\cP_M(T_1)\|_{\rm F}}+{\|T_2\|_{\ell_1}\over \|\cP_M(T_2)\|_{\rm F}}\right)\sqrt{\frac{r^2 d_1^2 \log ^2 d_1}{n}}+ \mu \kappa_0 \sqrt{\frac{r^2 d_1 \log ^3 d_1}{n}} \right].
		\end{aligned}
	\end{equation*}
\end{Theorem}

This result explicitly characterizes the dependence between two test statistics which is critical for the FDR control in multiple testing. In particular, we shall separate pairs of linear forms in the null hypotheses into strongly correlated:
\begin{equation}\label{eq:weak-corr-1}
	\begin{aligned}
		\cH_{0,\text{strong} }^2 := \left\{ (T_1,T_2)\in\cH_0\times \cH_0 : \rho_{T_1,T_2}\ge c q_0^{-\nu} \right\},
	\end{aligned}
\end{equation}
where $\nu>0$ can be any fixed small number and $c>0$ is some universal constant, and weakly correlated $\cH_{0,\text{weak} }^2:=(\calH_0\times \calH_0)\setminus \cH_{0,\text{strong} }^2$. The proportion of  all linear form pairs that are strongly correlated is therefore
$$\beta_{\mathsf{s} } := \frac{ \abs{ \cH_{0,\text{strong} }^2 }   }{\abs{ \cH_{0}^2 }}.$$

Under the incoherent assumptions,
$$
\rho_{T_1,T_2}\le  \frac{ \mu^4 r \norm{T_1}_{\ell_1 }\norm{T_2}_{\ell_1 } }{\beta^2_0 \norm{T_1}_{\tF}\norm{T_2}_{\tF} }\frac{1}{d_2}+\frac{\abs{\left\langle T_1 T_2^\top , UU^\top \right\rangle} +\abs{\left\langle T_1^\top T_2 , VV^\top \right\rangle} }{\norm{\cP_M(T_1)}_\tF \norm{\cP_M(T_2)}_\tF}
$$
Thus, two linear forms $(T_1, T_2)$ are weakly correlated if $T_1^\top T_2=\boldsymbol{0}$, $T_1 T_2^\top=\boldsymbol{0}$ and
\begin{equation}\label{eq:weak-corr-2}
		\frac{ \mu^2 \norm{T_1}_{\ell_1 }\norm{T_2}_{\ell_1 } }{\beta^2_0 \norm{T_1}_{\tF}\norm{T_2}_{\tF} } \le C.
\end{equation}
\eqref{eq:weak-corr-2} holds when $T_1$, $T_2$ are sparse, i.e., the number, $s_0$, of nonzero entries in $T_1$ and $T_2$ is of the order $O(\beta_0^2)$. Note that these conditions concern the linear forms only and do not depend on $M$. In fact, we can use this to show that in many practical examples related to item recommendations, the linear forms are weakly correlated, regardless of the underlying matrix $M$.

\paragraph{Inference of a submatrix.} Consider the inference problem with indexing matrices $\calH=\{e_i e_j^\top : l_1\le i\le l_2, l_3\le j\le l_4  \}$, where $l_2-l_1 \asymp d_1 $, $l_4-l_3 \asymp d_2 $. This can represent recommendation tasks in problems including Netflix prize \citep{bennett2007netflix}, or gene-disease association discovery \citep{natarajan2014inductive}, among others. Here we have the number of tests of order $O(d_1 d_2)$. Since $\|T\|_{\ell_1} /\|T\|_{\mathrm{F}}=1$ for any $T\in \cH$, condition \eqref{eq:weak-corr-2} is easily satisfied. Therefore, at most $O(d_1)$ pairs are strongly correlated (share the same row/column) for each linear form so that $\beta_{\mathsf{s} }\lesssim 1/d_2$.  
	
\paragraph{Inference of entrywise comparisons.} We can also consider comparison between two entries $M_{i_1,j_1}$ and $M_{i_2,j_2}$: $\cH=\{e_{i_1} e_{j_1}^\top- e_{i_2} e_{j_2}^\top: l_1\le i_1,i_2\le l_2, l_3\le j_1,j_2\le l_4  \}$. If $l_2-l_1 \asymp d_1 $, $l_4-l_3 \asymp d_2 $, then the total number of tests is of the order $O(d_1^2 d_2^2)$. Similar to before, $\|T\|_{\ell_1} /\|T\|_{\mathrm{F}}=\sqrt{2}$ for any $T\in \cH$ so that there are at most $O(d_1^2 d_2)$ pairs that can be strongly correlated (share the same row/column) for each linear form. This again yields $\beta_{\mathsf{s} }\lesssim 1/{d_2}$.
	
	
\paragraph{Inference of several user/feature groups.} For many applications, groupwise recommendation \citep{bi2018multilayer} is of interest. This can be formulated as testing $H_{0T}: \sum_{i\in G_k} M_{ij}\le \theta_{kj}$ vs $H_{1T}: \sum_{i\in G_k} M_{ij}>\theta_{kj} $, where $(G_1,\ldots,G_K)$ is a partition of the $[d_1]$. In other words $\cH=\{\sum_{i\in G_k}e_ie_j^\top: 1\le k\le K, 1\le j\le d_2\}$.
Note that $\|T\|_{\ell_1} /\|T\|_{\mathrm{F}}=\sqrt{\abs{G_k} }$ for all $T\in \cH$. If $K=\Omega(d_2)$, then 
$$\beta_{\mathsf{s} }\lesssim \frac{d_2K(K+d_2)}{d_2^2K^2}\lesssim\frac{1}{d_2}.$$

\subsection{Theoretical Guarantees}
A crucial aspect to understand the efficacy of a multiple testing procedure is the signal strength of the non-null set, i.e., $|\langle M, T\rangle-\theta_T|$ for $T\in\cH_1$. Recall that $\|\widehat{M}-M\|_{\max}\le\mu \kappa_0 \sigma_{\xi}\sqrt{r^2 d_1 \log ^2 (d_1)/n}$, which implies that
\begin{equation*}
\abs{\left\langle M-\widehat M ,T  \right\rangle} \le \norm{\widehat{M}-M}_{\max} \norm{T}_{\ell_1}\le  \mu \kappa_0 \sigma_{\xi}\sqrt{\frac{r^2 d_1 \log ^2 d_1}{n}} \norm{T}_{\ell_1}.
\end{equation*}
Thus, a signal can be consistently identified if
\begin{equation}\label{eq:strong-T}
\frac{ \abs{\langle M, T\rangle-\theta_T } }{  \norm{T}_{\ell_1} \sqrt{ \log (q \vee d_1) } } \ge C_{\mathsf{gap}  } \cdot \mu \kappa_0 \sigma_{\xi}\sqrt{\frac{r^2 d_1 \log ^2 d_1}{n}}
\end{equation}
for a sufficiently large constant $C_{\mathsf{gap}}>0$. Denote by $\cS$ the set of all linear forms $T\in \cH$ such that \eqref{eq:strong-T} holds. Note that $\beta_s$ and $\eta_n:=|\cS|$ are the most essential quantities in characterizing the effectiveness of FDR control and power guarantee for multiple testing. We are now in position to state our main result.

\begin{Theorem}\label{thm:weak-cor-fdr}
Suppose that
$$
\left(\sqrt{\beta_{\mathsf{s}}} \vee  h_n \right)  \frac{q_0}{\eta_n}   \to 0. 
$$
There exists a universal constants $C>0$ such that if the sample size $n\ge C \mu^2 r d_1 \log d_1$, then
$$
\mathrm{FDP}:=\frac{\sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank}} >L ) }{\left( \sum_{T\in \cH  } \bbI(W_T^{\mathsf{Rank}} >L ) \right) \vee 1 }\le \alpha(1+o_p(1))
$$
and
$$
\mathrm{POWER}:= \frac{\sum_{T\in \cH_1  } \bbI(W_T^{\mathsf{Rank}} >L ) }{q_1} \ge \frac{\eta_n}{q_1}(1-o_p(1)).
$$
\end{Theorem}

The first claim implies that
\begin{equation}\label{eq:fdr-exp}
	\text{FDR}=\E (\text{FDP})\le \alpha(1+o(1)),
\end{equation}
which can be used for control of FDR. On the other hand, if nearly all signals are strong in that $\eta_n/q_1\to 1$, then the second claim indicates that $\text{POWER}\to_p 1$. 

For clarity, we stated the asymptotic bounds for FDP and POWER in Theorem \ref{thm:weak-cor-fdr}. Our proof actually establishes stronger results in a nonasymptotic form. Theorem \ref{thm:weak-cor-fdr} is a direct consequence of these nonasymptotic results that will be presented in the Supplement. It is also worth noting that both the sample size and signal-to-noise ratio (implied by the condition on $h_n$) requirements of Theorem \ref{thm:weak-cor-fdr} are comparable to those for estimation \citep{keshavan2010matrix_b,ma2018implicit,xia2021statistical}. This immediately suggests that we can effectively control FDR under conditions of weak correlation, provided the underlying matrix can be consistently recovered.

\section{Whitening and Screening}\label{sec:strong-corr}
Theorem \ref{thm:weak-cor-fdr} shows that the symmetric data aggregation method can control FDR effectively if the number of strongly correlated linear form pairs is sufficiently small relative to the number of strong signals, i.e., $\sqrt{\beta_{\mathsf{s}}}q_0/\eta_n\to 0$. While this is plausible in many applications, as we have argued, there are also situations in which this may not be the case. We now discuss how this condition can be further relaxed thanks to the explicit characterization of the correlation among test statistics. In particular, as advocated by \cite{du2021false}, we proceed to apply symmetric data aggregation after appropriate screening and whitening. Interestingly, by exploiting the explicit characterization of the dependence among $W_T$s, we can develop a more general and intuitive theoretical framework to study the power and FDR control for matrix completion.

More specifically, denote the collection of test statistics obtained from Algorithm \ref{alg:matrix-fdr} as $ Z^{(i)}=\left[W^{(i)}_{T_1},  W^{(i)}_{T_2}, \dots, W^{(i)}_{T_q} \right]^\top\in\RR^q$, for $i=1, 2$. By Theorem \ref{thm:asymp-two-var}, $Z^{(i)}{\approx}_d N(\sfw, R)$ where $\textsf{w}\in\RR^{q}$ with the $i$-th entry $\sfw_i=\big(\langle M, T_i\rangle-\theta_{T_i}\big)/\big(\sigma_{\xi}\|\calP_M(T_i)\|_{\rm F}\sqrt{d_1d_2/n}\big)$ and $R=(\rho_{T_j,T_k})_{1\le j,k\le q}$. If $R$ is known, then $R^{-1/2}Z^{(i)}\approx_d N(R^{-1/2} \sfw, I_q)$ has independent coordinates and thus allows for better FDR control. However, such a whitening step can also mask the nonzero coordinates of $\sfw$, which, as suggested by \cite{du2021false}, can be estimated by Lasso. Of course, $\rho_{T_j,T_k}$ is unknown, but it can nonetheless be estimated by
$$
\widehat{\rho}_{T_j,T_k} = \frac{\left\langle \cP_{\widehat{M}_{\rm init}} (T_j) ,\cP_{\widehat{M}_{\rm init}}(T_k)  \right\rangle }{ \norm{\cP_{\widehat{M}_{\rm init}}(T_j)}_{\tF}\norm{\cP_{\widehat{M}_{\rm init}}(T_k)}_{\tF} }.
$$
In summary, we shall consider the following algorithm detailed in Algorithm~\ref{alg:matrix-sda}. 

\begin{algorithm}[htbp]
\caption{Matrix FDR Control with Whitening and Screening}
\label{alg:matrix-sda}
\begin{algorithmic}[1]
\REQUIRE Hypotheses $\left\{H_{0T_i}: M_{T_i}=\theta_{T_i}, i\in [q]\right\}$, data splits $\cD_0$, $\cD_1$, $\cD_2$, rank $r$, FDR level $\alpha$, regularization parameter $\lambda\ge 0$.
\STATE{ Apply Algorithm \ref{alg:matrix-fdr} to get  $Z^{(1)}$, $Z^{(2)}$ from $\{\mathcal{D}_0, \mathcal{D}_1\}$ and $\{\mathcal{D}_0, \mathcal{D}_2\}$ respectively}
\STATE {From $\mathcal{D}_1$, obtain a covariance estimate $\widehat{R}=(\wt\rho_{T_i, T_j})_{i,j=1}^q$ using $\wt M^{(1)}$ estimated from Algorithm \ref{alg:matrix-fdr}, that is
\begin{equation*}
    \widehat{\rho}_{T_i,T_j} = \frac{\left\langle \cP_{\widehat{M}^{(1)}} (T_i) ,\cP_{\widehat{M}^{(1)}}(T_j)  \right\rangle }{ \norm{\cP_{\widehat{M}^{(1)}}(T_i)}_{\tF}\norm{\cP_{\widehat{M}^{(1)}}(T_j)}_{\tF} } ,
\end{equation*}
Solve LASSO estimator
\begin{equation*}
    \wt \sfw^{(1)}: = \argmin_{\sfw\in \R^q } \left\{ \frac{1}{2}\big\|\wt R^{-1/2}(Z^{(1)} - \sfw)\big\|^2 + \lambda\norm{\sfw}_{\ell_1} \right\}.
\end{equation*}

}
\STATE {Denote $\calA:={\rm supp}(\wt \sfw^{(1)})$ the support of $\wt \sfw^{(1)}$. Run linear regression on $\calA$ with new design matrix $\wt R^{-1/2}_{\calA}$ and response $\wt R^{-1/2}Z^{(2)}$ to get asymptotically symmetric statistics $\wt\sfw^{(2)}$, where
\begin{equation*}
\wt\sfw_{\calA}^{(2)}:=\big(\wt R_{\calA}^{-1/2\top} \wt R_{\calA}^{-1/2}\big)^{-1}\wt R_{\calA}^{-1/2\top}\wt R^{-1/2}Z^{(2)}\quad {\rm and}\quad \wt \sfw^{(2)}_{\calA^{\rm c}}=0
\end{equation*}
with variance estimate $\widehat{\sigma}_{\sfw i}^2:= e_i^{\top}\big(\wt R_{\calA}^{-1/2\top} \wt R_{\calA}^{-1/2}\big)^{-1} e_i $ for $i\in\calA$.
}
\STATE { Compute the final ranking statistics of each $T_i$ by $\sfw_{T_i}^{\mathsf{Rank}}=\wt\sfw_{i}^{(1)}\wt \sfw_{i}^{(2)}/\widehat{\sigma}_{\sfw i}$, and then choose a data-driven threshold $L$ by 
\begin{equation*}
    L:=\inf \left\{t>0: \frac{\sum_{i=1}^q\bbI\left( \sfw_{T_i}^{\mathsf{Rank}}<-t\right)}{\sum_{i=1}^q \bbI \left( \sfw_{T_i}^{\mathsf{Rank}}>t\right) \vee 1} \leq \alpha\right\}.
\end{equation*}}
\STATE{Reject $H_{0T_i}$ if $\sfw_{T_i}^{\mathsf{Rank}}>L$}
\end{algorithmic}
\end{algorithm}

Here $\wt R_{\calA}^{-1/2}$ is the submatrix of $\wt R^{-1/2}$ with only columns indexed by $\calA$. Similarly, $\wt\sfw_{\calA}$ is the subvector of $\wt\sfw$ with only coordinates indexed by $\calA$. Note that Algorithm \ref{alg:matrix-fdr} can be treated as a special case of Algorithm \ref{alg:matrix-sda} by choosing the regularization parameter $\lambda=0$. However, as we argue below, with an appropriate choice of $\lambda>0$, the whitening and screening may lead to a more effective multiple testing procedure. In addition, a more concrete example of testing entries of submatrix of $M$ is given in the supplement to demonstrate the impact of whitening and screening.

It is clear that the efficacy of Algorithm \ref{alg:matrix-sda} hinges upon the reduction of dependence among test statistics with Lasso screening. We can show that, under mild regularity conditions, the asymptotic covariance matrix of $\wt \sfw^{(2)}_\calA$ is given by
$$
Q^{\ast}:=\big(R_{\calA}^{-1/2\top} R_{\calA}^{-1/2}\big)^{-1}.
$$
Similar to before, write
$$
\cH_{0\mathcal{A} ,\text{strong} }^2 = \left\{ (T_i,T_j)\in\calA_0 \times \calA_0 :  \abs{ Q^{\ast}_{jk}}/\sqrt{Q^{\ast}_{kk}Q^{\ast}_{jj}} \ge c|\calA|^{-\nu}  \right\},
$$
where $\calA_0=\calA\cap \cH_0$. Denote by %q^{' 2}_{0}
$$
\beta_{\mathsf{s} }': = \frac{ \abs{\cH_{0\mathcal{A},\text{strong} }^2}   }{\abs{\calA_0}^2 }.
$$
In other words, $\beta_{\mathsf{s} }'$ represents the proportion of strongly correlated pairs after whitening and screening. Likewise, we shall write $\eta_n'=\abs{\cS'}$ where $\cS'$ is the set of strong signals. To define strong signal, write
$$T_{\cH}= \left[ \begin{array}{c}
		\Vect(T_1)^\top  \\
		\Vect(T_2)^\top \\
		\vdots\\
		\Vect(T_q)^\top
	\end{array} \right]\in \R^{q\times d_1 d_2} $$ 
Then the limiting covariance matrix of $W_T$s is given by 
$$
\Sigma:= \big(\big<\calP_M(T_j),  \calP_M(T_k)\big>\big)_{1\leq j,k\leq q}=T_{\cH}(I_{d_1 d_2} - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top  ) T_{\cH}^\top.
$$
Define
\begin{equation}\label{eq:strong-text}
\cS' = \left\{ T\in \cH: \frac{ \abs{\langle M, T\rangle-\theta_T } }{   \norm{T}_{\ell_1} \sqrt{ q_1 \log (q \vee d) } } \ge C_{\mathsf{gap} } \cdot \mu \kappa_1^{3/2} \sqrt{\frac{r^2 d_1 \log ^2 d_1}{n}} \right\},
\end{equation}
where $\kappa_1=\lambda_{\max}(\Sigma)/\lambda_{\min}(\Sigma)$ is the condition number of $\Sigma$.

Let $T_{\calH}$ be a $q\times d_1d_2$ matrix with $i$-th row being ${\rm vec}(T_i)$ and define ${\rm supp}(T_{\calH}):=\cup_{i=1}^q {\rm supp}(T_i)$. Let $\|\cdot\|$ denote the spectral norm of a matrix and the $\ell_2$-norm of a vector, and  denote $\|M\|_{2,\max}:=\max_{i\in d_1}\|e_i^{\top}M\|$. By definition, we have $\|T_{\calH}\|_{2,\max}=\max_{i\in[q]}\|T_i\|_{\rm F}$. Here define $\|R\|_{\infty}:=\max_{i\in[q]} \|e_i^{\top} R\|_{\ell_1}$ for a matrix $R$. Note that $\|\cdot\|_{\max}$ and $\|\cdot\|_{\infty}$ are equivalent for a vector. 
We have the following theoretical guarantee for Algorithm \ref{alg:matrix-sda}.
\begin{Theorem}\label{thm:matrix-fdr-strong}
 Let $T_{\calH}$ be a $q\times d_1d_2$ matrix with $i$-th row being ${\rm vec}(T_i)$ and define ${\rm supp}(T_{\calH}):=\cup_{i=1}^q {\rm supp}(T_i)$. Suppose that $q_{0}'$ a uniform upper bound for $\abs{\calA_0}$ and  
$$
\left(\sqrt{\beta_{\mathsf{s}}'} \vee  \left( h_n +\big\|\sfw_{\calA^c}\big\|_{\infty}\right)\right)  \frac{q_0'}{\eta_n'}   \to 0, 
$$
and
$$
\lambda_{\min}\gg 
    \left(\norm{ R^{-1}  }_\infty + \frac{  \norm{T_{\calH} }}{ \norm{T_{\calH} }_{2,\max} } \left( |\operatorname{supp}(T_{\calH} )|\wedge \sqrt{d_2} \right) \right) \max_{T\in\calH}\left\{ \frac{ \|T\|_{\ell_1} }{ \|T\|_{\mathrm{F}} } \right\} \sigma_\xi \sqrt{\frac{ q d_1^3 \log d_1 }{n}}.
$$
Then there are universal constants $C_1, C_2>0$ such that if $n\ge C_1 \mu^2 r d_1 \log d_1$ and regularization parameter $\lambda=C_2\sqrt{\log d_1 +\log q}$ in Algorithm~\ref{alg:matrix-sda}, then
$$
\mathrm{FDP}=\frac{\sum_{T\in \cH_0  } \bbI(\sfw_{T}^{\mathsf{Rank}} >L ) }{\left( \sum_{T\in \cH  } \bbI(\sfw_T^{\mathsf{Rank}} >L ) \right) \vee 1 } \le \alpha(1+o_p(1))
$$
and
$$
\mathrm{POWER}= \frac{\sum_{T\in \cH_1  } \bbI(\sfw_T^{\mathsf{Rank}} >L ) }{q_1} \ge \frac{\eta_n' }{q_1}(1-o_p(1)).
$$   
\end{Theorem}

Note that the covariance matrix $\Sigma=\big(\langle \calP_{M}(T_i), \calP_M(T_j)\rangle\big)_{i,j\in[q]}$ is not known and our whitening procedure uses an estimate in its place. The additional lower bound of $\lambda_{\min}$ in Theorem \ref{thm:matrix-fdr-strong} is in place to ensure that the estimated covariance matrix indeed can be used to ``whiten'' the test statistics. It is also worth pointing out that we do not require the sure-screening condition of Lasso. Such conditions are common in the literature. See , e.g., \cite{roeder2009genome,barber2019knockoff,du2021false,dai2023scale}. For our purpose, weak signals can be entertained as long as $\|\sfw_{\calA^c}\|_{\infty}$ is sufficiently small.

%%%%%%%%%%%%%%%%%%%%%
\section{Numerical Experiments}\label{sec:experiments}
%%%%%%%%%%%%%%%%%%%%%
To complement our theoretical development, we also conducted several sets of numerical experiments to further demonstrate the practical merits of the proposed methodology.

\subsection{Simulation Studies}
We begin with a series of simulation studies aimed at illustrating the impact of several key aspects of our approach.

\subsubsection{Variance of linear forms}
In Section \ref{sec:clt},  we have presented the asymptotic normal test statistics for linear forms with a more accurate characterization of its variance. To justify the accuracy of our variance $\norm{\cP_M(T)}_\tF$, we show the simulation of empirical distribution functions of our test statistics $W_T$ in Theorem \ref{thm:asymp-normal} against former test statistic in \eqref{eq:xyclt} whose variance is characterized by $(\|U^\top T\|_{\rm F}^2+\|TV\|_{\rm F}^2)^{1/2}$ in \cite{xia2021statistical}. We plot the difference between empirical distribution functions $\bar{F}_n(z)$ and standard normal distribution function $\Phi(z)$ by sampling 10,000 independent realizations of test statistics. The result is shown in Figure \ref{fig:variance-comparison}. It is clear that our methods share a more precise asymptotic normal rate given smaller errors of $\bar{F}_n(z)-\Phi(z)$, especially for small sample size $N$.
% in Theorem \ref{thm:asymp-normal}
\begin{figure}[H]
\centering
% \includegraphics[width=0.5\textwidth]
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/distance_between_N-10000,2k4.png}
    \caption{$n=2400$ }
    \label{fig:variance-comparison-1}
\end{subfigure}
 \begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/distance_between_N-10000,3k.png}
    \caption{$n=3000$ }
    \label{fig:variance-comparison-2}
\end{subfigure}
     \begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\textwidth]{figures/distance_between_N-10000,3k6.png}
    \caption{$n=3600$ }
    \label{fig:variance-comparison-3}
\end{subfigure}

 \caption{The difference between empirical distribution functions and $\Phi(z)$.  Here, we compare our $W_T$ with the former method \citep{xia2021statistical}. We set the matrix with $d_1=d_2=\lambda_{\min}=400$, and $r=3$, and vary the number of random samples $n$ in noisy matrix completion. }
 \label{fig:variance-comparison}
\end{figure}

\subsubsection{Data aggregation under weak dependency}
We first evaluate our Algorithm \ref{alg:matrix-fdr} by simulations to corroborate two important properties of the proposed method: (1) the validity of FDR control for multiple testing of linear forms; (2) the power boost by data splitting and data aggregation; we randomly sample a low-rank matrix of dimension $d_1=d_2=1000$, rank $r=3$, with signal strength $\lambda_{\min}=1000$. The number of observations used for debiasing $n=10^4$, and the noises $\xi\sim N(0,1^2)$. We use the known true matrix as initialization for clearer comparisons. We first verify the FDR control in weak dependency by performing blockwise matrix tests: we test each entry in $M(1:300,1:200)$ by $H_{0,ij}: M_{ij}-m_{ij}=0$ versus $H_{1,ij}: M_{ij}-m_{ij}\neq 0$. We randomly assign non-null hypotheses to these $300\times 200=60,000$ entries with probability $p=0.2$, which leads to the following settings of $m_{ij}$:
\begin{equation}\label{eq:construct_H0}
    M_{ij}-m_{ij}=\begin{cases}
        \mu_{ij}, & \text{ with probability } p=0.2 ;\\
         0, & \text{ otherwise }
    \end{cases}  
\end{equation}
Here $\mu_{ij}$ are randomly-generated signals with fixed absolute mean: $\E \abs{\mu_{ij}}=\mu$. We run Algorithm \ref{alg:matrix-fdr} and compare different methods of data aggregation (see Section~\ref{sec:compare} for more details): I. multiplication; II. minimum absolute value with sign multiplication; III. adding absolute values with sign multiplication; IV. BHq with no data splitting. Here BHq with no data splitting means that we use data $\cD_1$ and $\cD_2$ together to construct asymptotic normal test statistics and then compute their $p$-values by the normal distribution. More specifically, we describe the BHq selection for linear forms as follows:
\begin{enumerate}
    \item Use $\cD_0$ to construct an initial estimate $\widehat{M}_{\mathsf{init}}$ 
    \item Following the construction of $W_T$, but use both the second and third part of data $\cD_1$, $\cD_2$ to de-bias $\widehat{M}_{\mathsf{init}}$
    \item Project the debiased matrix on the low-rank structure and get test statistics $W_T^{\mathsf{all} }$ for each linear form $T$.
    \item Computing two-sided $p$-value $P_i=2(1-\Phi(\abs{W_{T_i}^{\mathsf{all}} }))$
    \item Feature selection by BHq method: finding the largest $k$ such that $P_{(k)} \leq \frac{k}{q} \alpha$, and rejecting null hypothesis $H_{0,T_i }$ with  $P_i\le P_{(k)}$.
\end{enumerate}
This BHq selection relies on the asymptotic normality of high-dimensional features and serves as a good counterpart to our methods. 
The result presented in Figure \ref{fig:fdr-simu} clearly shows the excellent performance of multiplication in data aggregation with respect to both FDR control and power. By Section \ref{sec:dependence}, the blockwise matrix tests here can be treated as the weakly correlated case. Although the BHq method \cite{benjamini1995controlling} is guaranteed to be effective in the linear model, it cannot exactly control the FDR at level $\alpha$ in our matrix completion problem. The reason this happens might be due to the heavy tail property of $W_T$ described in Proposition \ref{eq:W-kth-moment} and the correlation of test statistics caused by low-rank projection.

% huge number of tests in our problem. When the $q$ is very large ($q=60,000$ in our case), the joint asymptotic normality of all the $q$ statistics is not strong enough to support BHq method. 

% \begin{figure}
% \centering
% \begin{subfigure}{0.4\textwidth}
%      \includegraphics[width=\textwidth]{figures/aggregation-FDR.png}
%      \caption{FDR control }
%      \label{fig:fdr-simu-1}
%  \end{subfigure}
%  \begin{subfigure}{0.4\textwidth}
%      \includegraphics[width=\textwidth]{figures/aggregation-power.png}
%      \caption{Power control }
%      \label{fig:fdr-simu-2}
%  \end{subfigure}
%  \caption{FDR control \& Power of different data aggregation schemes in blockwise matrix tests with $\alpha=0.1$  }
%  \label{fig:fdr-simu}
% \end{figure}


\begin{figure}
\centering
     \includegraphics[width=\textwidth]{figures/weak-corr-d=1000.png}
 \caption{FDR control \& Power of different data aggregation schemes in blockwise matrix tests with $\alpha=0.1$.  Here the signal is defined by $\mu$ in eq. (\ref{eq:construct_H0}). }
 \label{fig:fdr-simu}
\end{figure}

\subsubsection{Whitening and screening}
We now evaluate Algorithm \ref{alg:matrix-fdr} and Algorithm \ref{alg:matrix-sda} and show the advantages of de-correlation. For computational concerns, we slighted modify the implementation of Algorithm~\ref{alg:matrix-sda}, which does not affect the theoretical guarantees. See Algorithm~\ref{alg:matrix-sda-practical} in the Supplement. 
To this end, we apply our methods to the entry comparisons between rows: we test $q=400$ differences between first row $M(1,1:400)$ and second row $M(2,1:400)$, with $H_{0,T_i}$: $M_{1,i}-M_{2,i}=0$.  The linear forms are in the same rows, meaning that they are correlated. Since the complicated correlation structure of features, here we measure the overall correlation of our case by the proportion of related pairs: 
% use $N=4000$ for debiasing and
\begin{equation*}
    \varrho^*(z)=\frac{\sum_{i,j\in[q] } \bbI\left( \abs{\rho_{T_i, T_j} }> z\right)   }{q^2},
\end{equation*}
where $\rho_M (T_i,T_j)$ indicates the correlation of two linear form $M_{T_i}$ and $M_{T_j}$ and is given by \eqref{eq:corr}. 
Here $\varrho^*(z)$ can be treated as a measure of the strength of correlation $\beta_{\mathsf{s}}$. In this entry comparison problem, we have $\varrho^*(0.2)=0.3838 $, which means that an indispensable proportion of feature pairs are correlated. For the SDA method, we use a known correlation matrix. 
The performance of Algorithm \ref{alg:matrix-fdr} and Algorithm \ref{alg:matrix-sda} with different data aggregation methods are summarized in Figure \ref{fig:fdr-sda-simu}.
%\varrho^*(0.2)=0.4737
\begin{figure}
\centering
     \includegraphics[width=\textwidth]{figures/400-minus-p=038.png}
 \caption{FDR control \& Power of different data aggregation schemes in row tests with $\alpha=0.1$. Here the signal is defined by $\mu$ in eq. (\ref{eq:construct_H0}). }
 \label{fig:fdr-sda-simu}
\end{figure}
We also plot the ROC curves of different methods given two different signal levels. The result is presented in Figure \ref{fig:sda-roc}.
  \begin{figure}[H]
  \centering
\begin{subfigure}{0.45\textwidth}
         \includegraphics[width=\textwidth]{figures/SDA-ROC-sig=1.png}
     \caption{ROC curve with $\text{signal} = 1$}
     \label{fig:sda-roc-1}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
         \includegraphics[width=\textwidth]{figures/SDA-ROC-sig=2.png}
     \caption{ROC curve with $\text{signal} = 2$}
     \label{fig:sda-roc-2}
\end{subfigure}
\caption{ROC curve for different test statistics. Here the signal is defined by $\mu$ in eq. (\ref{eq:construct_H0}).}
     \label{fig:sda-roc}
     \end{figure}

% \begin{figure}
% \centering
% \begin{subfigure}{0.4\textwidth}
%      \includegraphics[width=\textwidth]{figures/SDA-FDR-2.png}
%      \caption{FDR control }
%      \label{fig:fdr-sda-simu-1}
%  \end{subfigure}
%  \begin{subfigure}{0.4\textwidth}
%      \includegraphics[width=\textwidth]{figures/SDA-power.png}
%      \caption{Power control }
%      \label{fig:fdr-sda-simu-2}
%  \end{subfigure}
%  \caption{FDR control \& Power of different data aggregation schemes in row tests with $\alpha=0.1$ }
%  \label{fig:fdr-sda-simu}
% \end{figure}
In Figure \ref{fig:fdr-sda-simu}, the SDA method can effectively control the FDR level at $\alpha=0.1$, with a notable power enhancement, while the BHq method on the other hand, fails to control the FDR given the strong correlation between features. Moreover, without de-correlation and screening, simple data aggregation methods also fail to control the FDR due to dependency.
We can thereby draw the conclusion that our algorithm based on SDA outperforms others in the highly correlated case with the help of screening and de-correlation. The ROC curves in Figure \ref{fig:sda-roc} also clearly show the advantages of our data aggregation methods in feature selections.
%, which safely controls the FDR at the given level with relatively high power.

\subsubsection{Heavy-tailed noises}
While our theories are established for sub-Gaussian noise, we observe that the proposed methods are very robust to heavy-tailed noise. This section showcases the performance of our algorithms in the existence of heavy-tailed noises, e.g., $t$-distribution and exponential distribution, and compares the performances of different methods. We consider moderate and strong correlations, respectively. Here $M$ is randomly generated with dimensions $d_1=d_2=400$, rank $r=3$, $\lambda_{\min}=400$, and the noise is fixed with a standard deviation $\sigma_\xi=0.4$. The sample size is set by $n=3000$. We focus on the following tasks: (i) entry comparisons between rows; (ii) entry comparisons within a block. More specifically, in the entry comparison task between rows, we compare $H_{0,T}$: $M_{i,j}-M_{i+1,1}=0$ for every $1\le i\le 4$ and $j\ge 2$. That is, we compare each entry with the first entry of the next row; in the entry comparison task within a block, we compare $H_{0,T}$: $M_{i,j}-M_{1,1}=0$ for every  $1\le i\le 4$ and $j\ge 2$. For these two tasks, we all have $q=1596$, but the correlation structures and levels are different. That is, (i) entry comparisons between rows, $\varrho^*(0.2)=0.4541$; (ii) entry comparisons within a block, $\varrho^*(0.2)=0.9514$. Here, (i) and (ii) can be viewed as examples of moderate and strong correlations.

\begin{figure}
\centering
\begin{subfigure}{0.8\textwidth}
     \includegraphics[width=1\textwidth]{figures/horizontal-moderate-gaussian.png}
     \caption{Sub-Gaussian noises}
     \label{fig:fdr-moderate-1}
 \end{subfigure}
 \begin{subfigure}{0.8\textwidth}
     \includegraphics[width=1\textwidth]{figures/horizontal-moderate-exp.png}
     \caption{Exponential noises }
     \label{fig:fdr-moderate-2}
 \end{subfigure}
  \begin{subfigure}{0.8\textwidth}
     \includegraphics[width=\textwidth]{figures/horizontal-moderate-t.png}
     \caption{Student-t noises }
     \label{fig:fdr-moderate-3}
 \end{subfigure}
 \caption{FDR control \& Power of different data aggregation schemes for entry comparisons between rows with $\alpha=0.1$ when the noises are heavy-tailed distributed}
 \label{fig:fdr-moderate}
\end{figure}
%%%%%%%%%%%%
\begin{figure}
\centering
\begin{subfigure}{0.8\textwidth}
     \includegraphics[width=\textwidth]{figures/horizontal-strong-sub-Gaussian.png}
     \caption{Sub-Gaussian noises}
     \label{fig:fdr-strong-1}
 \end{subfigure}
 \begin{subfigure}{0.8\textwidth}
     \includegraphics[width=\textwidth]{figures/horizontal-strong-exp.png}
     \caption{Exponential noises }
     \label{fig:fdr-strong-2}
 \end{subfigure}
  \begin{subfigure}{0.8\textwidth}
     \includegraphics[width=\textwidth]{figures/horizontal-strong-t.png}
     \caption{Student-t noises }
     \label{fig:fdr-strong-3}
 \end{subfigure}
 \caption{FDR control \& Power of different data aggregation schemes for entry comparisons within a block with $\alpha=0.1$ when the noises are heavy-tailed distributed}
 \label{fig:fdr-strong}
\end{figure}
We report all the results in Figure \ref{fig:fdr-moderate} and Figure \ref{fig:fdr-strong}. In both moderate and strong correlation cases, the BHq method shows unstable FDR control, while our proposed SDA method always performs well even under strong correlation. The SDA method is also robust with respect to heavy-tailed noises.
All the simulations in this section display the averaged performance of multiple independent runs. 


\subsection{Real Data Examples}
\subsubsection{MovieLens}
This section applies our methods to the MovieLens dataset for multiple testing and FDR control. MovieLens \citep{harper2015movielens}, as a commonly used dataset in matrix completion problems, records millions of people’s expressed preferences for movies (rated from 1-5). The dataset can be viewed as a huge, sparse matrix with heavily incomplete observations. MovieLens dataset is broadly used in matrix completion \citep{hastie2015matrix,monti2017geometric,xia2021statistical} and other machine learning tasks. The dataset is available on \url{https://grouplens.org/datasets/movielens/}. To demonstrate the reliability of the performance, we removed users with ratings less than 20 movies, resulting in 100,000 ratings (0-5) from 943 users on 1682 movies (where 0 stands for unrated movies).
%\begin{itemize}
%    \item 100,000 ratings (0-5) from 943 users on 1682 movies (where 0 stands for unrated movies);
%    \item Users who have rated at least 20 movies;
%    \item Simple demographic info for the users.
%\end{itemize}
We assume the latent low-rank structure of this user-rating matrix with $r=10$. We select $q=1000$ 
adjacent and observed entry pairs, aiming to compare 
\begin{equation*}
    H_{0,ij}: M(i,j)-M(i,j+1)=0 \text{ versus } H_{1,ij}: M(i,j)-M(i,j+1)> 0,
\end{equation*}
for a group of suitable entries $(i,j)$.
 Notice that since in the noisy matrix completion problem, we have the observation $Y(i, j)=M(i, j)+\xi(i, j)$, which means that the ground truth $M(i, j)$ is always unknown, we adopt the process in \cite{xia2021statistical} that treats $\mathbb{I}\left(Y\left(i, j+1\right)>Y\left(i, j\right)\right)$ as a proxy to differentiate $H_1$ from $H_0$. %When the noise is symmetric, $\mathbb{I}\left(Y\left(i, j\right)>Y\left(i, j+1\right)\right)$ is more likely to happen on the alternative set. 
 
 Empirically, instead of initializing our algorithm by data splitting, for all the methods, we use fast Riemannian gradient descent \citep{wei2016guarantees,cai2022generalized} on the whole data set to initialize our algorithms and then randomly split data into two parts $\cD_1$, $\cD_2$ to perform debiasing and data aggregation on $\cD_1$, $\cD_2$.
We first verify the symmetric property of our test statistics on MovieLens Data. Towards that end, we first set our hypotheses $m_{ij}= Y(i,j)- Y(i,j+1)$ and construct asymptotic statistics on $\cD_1$, $\cD_2$ to mimic null test statistics. Here we still use $Y(i,j)- Y(i,j+1)$ as a proxy of $M(i,j)- M(i,j+1)$. The distribution of the corresponding $W_T^{(1)}$, $W_T^{(2)}$ can be found in Figure \ref{fig:symm-null-movielens}, showing clearly the symmetric properties of null hypotheses.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{figures/symm-null-1k.png}
 % \begin{subfigure}{0.4\textwidth}
 %     \includegraphics[width=\textwidth]{figures/symm-null.png}
 %     \caption{Distribution of $W_T^{(1)}$ under $\cD_0$ }
 %     \label{fig:symm-null-movielens-1}
 % \end{subfigure}
 %  \begin{subfigure}{0.4\textwidth}
 %     \includegraphics[width=\textwidth]{figures/symm-null-D2.png}
 %     \caption{Distribution of  $W_T^{(2)}$  under $\cD_1$ }
 %     \label{fig:symm-null-movielens-2}
 % \end{subfigure}
 \caption{Symmetric distribution of all the test statistics under the null set given $\langle M, T\rangle=m_T$ for all $T$. The test statistics are observed to preserve a good symmetric property both on $\cD_1$ and $\cD_2$.}
 \label{fig:symm-null-movielens}
\end{figure}
We then apply our methods to the entrywise comparison task. Given a total of $q=1000$, the number of instances for $\mathbb{I}\left(Y\left(i, j\right)>Y\left(i, j+1\right)\right)$ is $q_1=262$. We perform the tests for this one-sided hypothesis testing by dropping out hypotheses with negative test statistics on both $\cD_1$ and $\cD_2$. The $p$-values for BHq are also adjusted correspondingly. The outcomes are concisely presented in Table \ref{tab:tab1}. The result table clearly shows that the SDA method outperforms other data aggregation methods and the BHq method in terms of false discovery rate control. The ineffectiveness of the first three simple data aggregation methods can be attributed to the high correlation of entry pairs, as adjacent entry pairs within a row are selected. When $\alpha$ is significantly small, SDA tends to be more conservative, which leads to good FDR control, while other methods remain to keep large FDRs. The result also shows business implications: instead of excessively recommending movies to users, the SDA can better select target users that are truly interested in the movies to increase the accuracy of the recommendation. By adopting our method for recommendation, the movie company can increase its profit while avoiding losing potential customers.


\begin{table}
	\centering
	\begin{tabular}{lc|cccc}
		\hline\hline
		Level $\alpha$ & Method & False discoveries & True discoveries & FDP  \\ \hline
		$\alpha = 0.01$ & Multiplication & 13  & 59 & 0.1806  \\
		& Minimum & 13 & 58 & 0.1831 \\
		& Addition & 13 & 60 & 0.1781  \\ 
            & SDA & \textbf{0} & {18} & \textbf{0}\\ 
            & BHq & 1 & 26 & 0.0370  \\ 
  \hline
  		$\alpha = 0.05$ & Multiplication & 20  & 84 & 0.1923  \\
		& Minimum & 20 & 83 & 0.1942 \\
		& Addition & 20 & 84 & 0.1923  \\ 
            & SDA & \textbf{2} & {25} & \textbf{0.0741}\\ 
            & BHq & 10 & 53 & 0.1587  \\ 
  \hline
  		$\alpha = 0.1$ & Multiplication & 24 & \textbf{95} & \textbf{0.2017}  \\
		& Minimum & 24 & 94 & 0.2034   \\
		& Addition & 25 & 95 & 0.2083   \\ 
            & SDA & \textbf{8} & {49} & \textbf{0.1404} \\ 
            & BHq & 22 & 76 & 0.2245  \\ 
    \hline
    $\alpha = 0.2$ & Multiplication & 33 & 108 & 0.2340  \\
		& Minimum & 33 & 108 & 0.2340   \\
		& Addition & 33 & 108 & 0.2340   \\ 
            & SDA & \textbf{23} & 89 & \textbf{0.2054} \\ 
            & BHq & 36 & 115 & 0.2384 \\ 
  \hline \hline
	\end{tabular}
	\caption{Numbers of the discovered entry pairs with FDP by different data aggregation methods under various levels on MovieLens data.}
	\label{tab:tab1}
\end{table}

\subsubsection{Rossmann sales dataset}
We use the Rossmann sales dataset that has recently been studied for uncertainty quantification in matrix completion \citep{farias2022uncertainty,gui2023conformalized}. The Rossmann sales dataset records over 3,000 drug stores run by Rossmann in 7 European countries. The training set contains daily sales of 1115 drug stores on workdays from Jan 1, 2013, to July 31, 2015. The data matrix is thus of dimension $1115\times780$, where two dimensions represent drug stores and workdays, respectively. The unit of sales data is 1K. The dataset is very dense with about $80\%$ valid (non-zero sells) observations of the full matrix; thus, we apply random masking to get sparse observations and use other data only to initialize the algorithm. In this example, we use $20\%$ of the total records as each one split and apply Algorithm \ref{alg:matrix-fdr} on the two splits of the data that are properly processed. Noticing that most observed entries are given, we use the observations as true $M_{ij}$ and perform multiple entrywise tests \eqref{eq:ross}. We select the first $q=20,000$ entries sorted by rows with records in the whole dataset as our target $\cH$.
Since we select a relatively large $q$, according to Section \ref{sec:dependence}, the problem is weakly correlated, which means simple data aggregation is enough to control FDR. We randomly assign null and non-null features by \eqref{eq:construct_H0} but only consider positive signals. In this case, the ratio of non-null is $p=0.3$, and we assume the latent low-rank $r=30$. Specifically, we simultaneously test
\begin{equation}\label{eq:ross}
      H_{0,ij}: M_{ij}=m_{ij}\text{ vs } H_{1,ij}: M_{ij}> m_{ij}, \text{ for all } (i,j)\in \cH.
\end{equation}
We present the result in Figure \ref{fig:fdr-Rossmann} and the ROC curves in Figure \ref{fig:fdr-Rossmann-roc}. The Rossmann sales dataset is available at \url{https://www.kaggle.com/c/rossmann-store-sales}.


\begin{figure}
\centering
\begin{subfigure}{1\textwidth}
     \includegraphics[width=\textwidth]{figures/Rossmann-a=02-p=03.png}
     \caption{ Empirical FDP and power at  FDR control level $\alpha=0.2$}
     \label{fig:fdr-Rossmann-1}
 \end{subfigure}
 \begin{subfigure}{1\textwidth}
     \includegraphics[width=\textwidth]{figures/Rossmann-a=01-p=03.png}
     \caption{Empirical FDP and power at  FDR control level $\alpha=0.1$}
     \label{fig:fdr-fdr-Rossmann-2}
     
 \end{subfigure}

 \caption{FDR control \& Power of different data aggregation schemes for Rossmann sales testing. Here the signals indicate the sizes of $\abs{M_{ij}-m_{ij} }$ which are scaled  by $10^3$}
 \label{fig:fdr-Rossmann}

\end{figure}

  \begin{figure}
  \centering
\begin{subfigure}{0.45\textwidth}
         \includegraphics[width=\textwidth]{figures/Rossmann_ROC-sig=1.png}
     \caption{ROC curve with $\text{signal} = 1$}
     \label{fig:fdr-fdr-Rossmann-roc-1}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
         \includegraphics[width=\textwidth]{figures/Rossmann_ROC-sig=2.png}
     \caption{ROC curve with $\text{signal} = 2$}
     \label{fig:fdr-fdr-Rossmann-roc-2}
\end{subfigure}
\caption{ROC curve for different test statistics in Rossmann sales dataset}
     \label{fig:fdr-Rossmann-roc}
     \end{figure}
Here, three different data aggregation methods, together with BHq method, are compared. For this one-sided problem, we also drop out features that have negative statistics on $\cD_1$ and $\cD_2$. From Figure \ref{fig:fdr-Rossmann}, it is clear that the data aggregation method with multiplication performs better regarding both FDR control and power. Data aggregation by taking minimum absolute values performs close to our aggregation method with multiplication in power, but it has larger FDPs. Data aggregation by adding absolute values behaves conservatively in the problem. The BHq method appears to be more conservative compared to the data aggregation methods, particularly at the FDR control level of $\alpha = 0.2$. Moreover, from the ROC curves in Figure \ref{fig:fdr-Rossmann-roc}, we can observe the obvious advantage of our data aggregation methods against the BHq method.



\section{Concluding Remarks}\label{sec:remark}
In this paper, motivated by large-scale recommender systems, we study the problem of multiple testing for linear forms in noisy matrix completion and develop a general framework to control the FDR. Our approach is based upon a new test statistic for testing linear forms that enjoy sharper asymptotics than existing ones in the literature and an effective data splitting and symmetric aggregation scheme that can be shown to be especially suitable in the context of matrix completion.

Our approach can potentially be extended to many other problems with structural high-dimensional features. For example, one possible direction is the FDR control for tensor completion. Indeed, multiple testing in multilinear arrays presents a number of additional technical challenges as it requires much-involved analysis of singular subspace perturbations. As such, inferences in general for low-rank multilinear arrays are largely unexplored. We shall leave these intriguing problems for future investigation.

\end{sloppypar}


%\newpage
\bibliography{reference}
\bibliographystyle{apalike}

\newpage

\appendix
\begin{center}
{\bf\LARGE Supplement to ``Multiple Testing of Linear Forms in Noisy Matrix Completion"}
\end{center}
\smallskip

To ease the understanding, here we list several important notations frequently encountered while reading our main text and proofs in Table \ref{tab:tab-notations}.
\begin{table}[htbp]
	\centering
	\begin{tabular}{|l|l|}
		\hline\hline
  Notation \ & Meaning \\
  \hline
		$q$, $q_1$, $q_0$ \ & number of all, non-null, and null tests respectively  \\ 
  \hline
    $W_T^{(i)}$ \ & test statistic of linear form $M_T:=\langle M, T\rangle$ constructed from the $i$th data split \\ 
    \hline
     $\mu$  & parameter for incoherence condition \\ %defined in Assumption \ref{asp:incoherence} 
     \hline
     $\beta_0$ &  alignment parameter  \\ %defined in Assumption \ref{asp:alignment} 
     \hline
    $\alpha_d$ & dimension ratio of matrix $M$: $\alpha_d:= d_1/d_2$ \\
    \hline
    $\kappa_0$ & condition number of matrix $M$ \\
    \hline
    $\gamma_n$  & accuracy of initial estimation $\norm{\widehat{M}^{\mathsf{init}} - M}_{\max} \le C  \sigma_\xi \gamma_n$, which may take  $\gamma_n=\sqrt{\frac{r^2 d_1 \log ^2 d_1}{n}}$ \\
    \hline
    $\beta_T$  &  sparsity level of all indexing matrices: $\beta_T := \max_{T\in \cH}{ { \norm{T}_{\ell_1} }/{ \norm{T}_\tF } } $ \\
    \hline 
    $\cP_M(\cdot)$ & projection operators  $\cP_M^{}(T):= T- \cP_M^{\perp}(T)= T- U_{\perp} U^\top_{\perp} T V_{\perp}V^\top_{\perp}$ \\
    \hline
    $s_T$ & variance of testing $M_T$ induced by random sampling: $s_T=\norm{\cP_M(T) }_\tF$ \\
     \hline
    $h_n$  & asymptotic normal rate defined in \eqref{eq:fixed-asymp} \\
    \hline
    $\beta_{\mathsf{s} }$ &  proportion of strongly correlated linear form pairs defined in \eqref{eq:strong-T} \\
    \hline
    $\eta_n$ &  number of strong signals \\
    \hline
    $\kappa_1$ & condition number of covariance matrix $\Sigma=\big(\big<\calP_M(T_j), \calP_M(T_k)\big>\big)_{1\leq j,k\leq q}$ \\
    \hline
    $\kappa_T$ & shrinkage of variances caused by low-rank projection $\kappa_T = \norm{T_{\cH} }/\norm{\Sigma}^{1/2}$ \\
    \hline
    $\kappa_\infty$ & maximum row-wise $\ell_1$-norm of inverse correlation matrix: $\kappa_\infty=\norm{R^{-1}}_\infty:=\max_{i}\|e_i^{\top}R\|_{\ell_1}$ \\
    \hline
    $q_n$, $q_{0n}$ & cardinality of support after screening $q_n=\abs{\cA}$, and  $q_{0n}=\abs{\cA\cap \cH_0 }$ \\
    \hline
    % $\Bar{q}_{0n}$ &  an uniform upper bound of the cardinality $\abs{\cA\cap \cH_0 }$\\
    % \hline
    $\beta_{\mathsf{s} }'$,  $\eta_n'$&  proportion $\beta_{\mathsf{s} }$ and number of strong signals after screening\\
    % \hline
    % $\kappa_T$ & the shrinkage of the scale $\norm{T_{\calH}}_2$ after projection $\cP_M$: $\kappa_T:= \norm{T_{\calH}}_2/\norm{S}_2^{0.5} $\\
    
     
  \hline \hline
	\end{tabular}
	\caption{Important notations used in the main text
 }
	\label{tab:tab-notations}
\end{table}

\section{Additional Results}

\subsection{Effect of Screening and Whitening}
In this subsection, we shall discuss an example of testing about a submatrix of $M$ to further illustrate the effect of screening and whitening. In particular, we shall show how whitening can weaken the dependence in $Q^{\ast}$, compared with the un-whitened $R_{\calA,\calA}$, where
$$
Q^{\ast}:=\big(R_{\calA}^{-1/2\top} R_{\calA}^{-1/2}\big)^{-1}=R_{\calA,\calA} - R_{\calA,\calA^c}R_{\calA^c,\calA^c}^{-1}R_{\calA,\calA^c}^\top.
$$
To this end, we define the total test matrix $T_{\calH}=[P_{d\times d},\boldsymbol{0}_{d\times(d^2-d) }]$, where we set $d_1=d_2=q=d$. Thus, the covariance matrix of our un-standardized test statistics is
\begin{equation*}
\begin{aligned}
        \Sigma = & T_{\calH}\left(I_{d^2} -  U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)T_{\calH}^\top =  P\left(I_d - \left(U_\perp U_\perp^\top\right)_{11}V_\perp V_\perp^\top \right) P^\top \\ 
        & = P \left(VV^\top +u_{11}V_\perp V_\perp^\top \right)P^{\top} = P \left[V, V_\perp\right] \left[\begin{matrix} I_r & 0 \\0 & u_{11} I_{d-r}
        \end{matrix}\right] \left[V^\top; V_\perp^\top \right]P^\top.
\end{aligned}
\end{equation*}
Here $u_{11}= \left(UU^\top\right)_{11}$. Without loss of generality, let $P=I_d$ be a diagonal matrix, i.e., testing multiple entries in the first row. The $q\times q$ covariance matrix  $\Sigma=\big[u_{11}I_q+(1-u_{11})VV^{\top}\big]$, showing that the test statistics under noisy matrix completion are always correlated, due to the low-rank projection. This underscores the difficulties of multiple testing in matrix completion problems. Nevertheless, it is clear from textbook results of Multivariate Statistics that the total variance ${\rm tr}\big(\Sigma_{\calA,\calA}-\Sigma_{\calA,\calA^{\rm c}}\Sigma_{\calA^{\rm c},\calA^{\rm c}}^{-1}\Sigma_{\calA^{\rm c}, \calA}\big)\leq {\rm tr}(\Sigma_{\calA,\calA})$, which is smaller than the total variance of the unscreened statistics ${\rm tr}(\Sigma)$.

A special case of  multiple testing is defined by making 
\begin{equation*}
    P=\left[\begin{matrix} I_{\calA} & B \\0 & I_{\calA^{c} }
        \end{matrix}\right]\left[V, V_\perp\right]^{\top},
\end{equation*}
where for simplicity, we assume $\calA$ is just the index set from the first $\abs{\calA}$ dimensions. This gives us the covariance matrix
\begin{equation*}
\begin{aligned}
        \Sigma = \left[\begin{matrix} \Lambda +u_{11} BB^\top & u_{11}B \\u_{11}B^\top & u_{11} I_{\calA^{c} }
        \end{matrix}\right].
\end{aligned}
\end{equation*}
Here $\Lambda$ is a diagonal matrix of the size $\abs{\calA}\times\abs{\calA}$  with the first $r$ diagonals equal to $1$, and others equal to $u_{11}$.
Obviously, this covariance matrix shows that the test statistics can be highly correlated since $R_{\calA,\calA} = D_{\calA}^{-\frac{1}{2}}\left( \Lambda +u_{11} BB^\top \right)D_{\calA}^{-\frac{1}{2}}  $ contains off-diagonal elements determined by $B$. Here $D_{\calA}$ represents the the first $\abs{\calA}$ diagonal elements of $\Sigma$. However, the screening shows that
\begin{equation*}
\begin{aligned}
        Q^{\ast}&=\big(R_{\calA}^{-1/2\top} R_{\calA}^{-1/2}\big)^{-1}=R_{\calA,\calA} - R_{\calA,\calA^c}R_{\calA^c,\calA^c}^{-1}R_{\calA,\calA^c}^\top \\
        &=  D_{\calA}^{-\frac{1}{2}}\left( \Lambda +u_{11} BB^\top \right)D_{\calA}^{-\frac{1}{2}} -   D_{\calA}^{-\frac{1}{2}}  u_{11}^{\frac{1}{2}} B B^\top  u_{11}^{\frac{1}{2}} D_{\calA}^{-\frac{1}{2}} \\
        & =  D_{\calA}^{-\frac{1}{2}} \Lambda D_{\calA}^{-\frac{1}{2}},
\end{aligned}
\end{equation*}
with no off-diagonal elements. This indicates that our screening and whitening procedure in the noisy matrix completion model can reduce the correlation of test statistics.

\subsection{Non-asymptotic Bounds for FDR and Power}
Here, we present a general non-asymptotic version of our theoretical guarantees.

\subsubsection{Weak dependence}

\begin{Theorem}
	Under the conditions of Theorem \ref{thm:weak-cor-fdr},
    \begin{itemize}
\item[(a)] with probability at least   
$$
1-C_2\varepsilon^{-2}\log(\frac{q_0 }{\alpha \eta_n})\left( \left(\frac{\beta_{\mathsf{s}} q_0^2 }{ \alpha^2\eta_n^2}\right)^{\frac{1}{2}} + \left(\frac{h_n q_0}{\alpha \eta_n} +(\alpha\eta_n q_0)^{-\nu/2} \right)^{\frac{1}{2}}\right)- C_2 h_n,
$$
Algorithm \ref{alg:matrix-fdr} achieves false discovery proportion
% $n \geq C_1 \alpha_d \kappa_0^6 \mu_{}^6 r^3 d_1 \log ^2 d_1$,
\begin{equation}\label{eq:thm-weak-cor-fdr}
    \mathrm{FDP}:=\frac{\sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank}} >L ) }{\left( \sum_{T\in \cH  } \bbI(W_T^{\mathsf{Rank}} >L ) \right) \vee 1 } \le \alpha(1+\varepsilon),
\end{equation}
for any $\varepsilon\in(0,1)$. 
\item[(b)] with probability at least
$$
1-C_2 \log(\frac{q_0 }{\alpha \eta_n})\left( \left(\frac{\beta_{\mathsf{s}} q_0^2 }{ \alpha^2\eta_n^2}\right)^{\frac{1}{2}} + \left(\frac{h_n q_0}{\alpha \eta_n}+(\alpha\eta_n q_0)^{-\nu/2} \right)^{\frac{1}{2}}\right)- C_2 \varepsilon^{-1} h_n,
$$
Algorithm~\ref{alg:matrix-fdr} can select the strong signals with power 
\begin{equation}\label{eq:thm-weak-cor-power}
   \mathrm{POWER}:= \frac{\sum_{T\in \cH_1  } \bbI(W_T^{\mathsf{Rank}} >L ) }{q_1} \ge (1-\varepsilon)\frac{\eta_n}{q_1}.
\end{equation}
\end{itemize}
\end{Theorem}

Note that Part (a) also implies that
\begin{equation}\label{eq:fdr-exp-exact}
     \text{FDR}=\E (\text{FDP})\le \alpha+ C_2h_n+C_2\alpha^{\frac{2}{3}}\log(\frac{q_0 }{\alpha \eta_n})\left( \left(\frac{\beta_{\mathsf{s}} q_0^2 }{ \alpha^2\eta_n^2}\right)^{\frac{1}{6}} + \left(\frac{h_n q_0}{\alpha \eta_n}\right)^{\frac{1}{6}} +\left(\alpha\eta_n q_0\right)^{-\frac{\nu}{12}}  \right).
\end{equation}

\subsubsection{Whitening and screening}

\begin{Theorem}
Under the settings of Theorem \ref{thm:matrix-fdr-strong}, suppose that
\begin{equation*}
    \left(\norm{R^{-1}  }_\infty +\kappa_1 \frac{\norm{T_{\calH} } }{\norm{ \Sigma }^{1/2}}\left( \frac{\operatorname{supp}(T_{\calH} )}{\sqrt{d_2}}\wedge 1 \right)  \right)\frac{\beta_T \mu \sigma_\xi }{\beta_0 \lambda_{\min} }\sqrt{\frac{\kappa_1\alpha_d q d_1^2 d_2 \log d_1 }{n}}=o(1).
\end{equation*}
With the regularization level $\lambda=C\sqrt{\log d_1+\log q}$, the Algorithm \ref{alg:matrix-sda} attains an FDP 
\begin{equation*}
     \mathrm{FDP}=\frac{\sum_{T\in \cH_0  } \bbI(\sfw_{T}^{\mathsf{Rank}} >L ) }{\left( \sum_{T\in \cH  } \bbI(\sfw_T^{\mathsf{Rank}} >L ) \right) \vee 1 } \le \alpha(1+\varepsilon),
\end{equation*}
for any $\varepsilon\in(0,1)$ with probability at least 
\begin{equation*}
    1-C_1\varepsilon^{-2}\log(\frac{ q_{0}' }{\alpha \eta_n'})\left( \left(\frac{\beta_{\mathsf{s}}'  q_{0}^{'2} }{ \alpha^2\eta_n^{'2}}\right)^{\frac{1}{2}} + \left(\frac{C_\infty \left( h_n+\norm{ \sfw_{\calA^c} }_{\infty} \right)  q_{0}' }{\alpha \eta_n'} +(\alpha\eta_n'  q_{0}')^{-\nu/2} \right)^{\frac{1}{2}}\right)- C_\infty  \left( h_n+\norm{ \sfw_{\calA^c} }_{\infty} \right),
\end{equation*}
where $C_\infty$ is a constant involving $\wt R$ and $\calA$ only, defined later in Proposition~\ref{prop:OLS-normal}. Moreover, the power is guaranteed to be lower bounded by:
\begin{equation*}
   \mathrm{POWER}= \frac{\sum_{T\in \cH_1  } \bbI(\sfw_T^{\mathsf{Rank}} >L ) }{q_1} \ge (1-\varepsilon)\frac{\eta_n' }{q_1},
\end{equation*}
with a probability at least
\begin{equation*}
  1-C_1 \log(\frac{ q_{0}' }{\alpha \eta_n'})\left( \left(\frac{\beta_{\mathsf{s}}'  q_{0}^{'2} }{ \alpha^2\eta_n^{'2}}\right)^{\frac{1}{2}} + \left(\frac{C_\infty  \left( h_n+\norm{ \sfw_{\calA^c} }_{\infty} \right)  q_{0}'}{\alpha \eta_n'}+(\alpha\eta_n'  q_{0}')^{-\nu/2} \right)^{\frac{1}{2}}\right)- C_1 C_\infty  \varepsilon^{-1} \left( h_n+\norm{ \sfw_{\calA^c} }_{\infty} \right).
\end{equation*}
\end{Theorem}

Since we further have 
\begin{equation*}
    \begin{aligned}
        \norm{ \Sigma }^{\frac{1}{2}} \ge \norm{e_i^\top T_{\calH}\left(I_{d_1 d_2} -  U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)  }_2 =\norm{\cP_M(T_i)}_\tF \ge \beta_0\sqrt{\frac{r}{d_1}} \norm{T_i}_\tF,
    \end{aligned}
\end{equation*}
i.e., $ \norm{ \Sigma }^{\frac{1}{2}}  \ge \beta_0\sqrt{\frac{r}{d_1}}\max_{i}\norm{T_i}_\tF= \beta_0\sqrt{\frac{r}{d_1}} \norm{T_{\calH}}_{2,\max} $, and 
\begin{equation*}
    \frac{  \norm{T_{\calH} }  }{ \norm{ \Sigma }^{\frac{1}{2}}  } \left( \frac{\operatorname{supp}(T_{\calH} )}{\sqrt{d_2}}\wedge 1 \right) \le \frac{\sqrt{\alpha_d }}{\sqrt{r}\beta_0}\cdot \frac{  \norm{T_{\calH} } }{ \norm{T_{\calH} }_{2,\max} } \left( \operatorname{supp}(T_{\calH} ) \wedge \sqrt{d_2} \right),
\end{equation*}
we can convert this signal requirement to a stronger but clearer one presented in Theorem \ref{thm:matrix-fdr-strong}. In the subsequent proofs, we shall prove the non-asymptotic versions of Theorem \ref{thm:weak-cor-fdr} and \ref{thm:matrix-fdr-strong}. 

\subsection{Finite-sample Guarantees for Whitening and Screening}
Notice that, in our method of FDR control with whitening and screening, the condition of the correlation structure is defined on the asymptotic correlation matrix $Q^{\ast}:=\big(R_{\calA}^{-1/2\top} R_{\calA}^{-1/2}\big)^{-1}$. However, conditional on $\cD_0$ and $\cD_1$, the covariance of our test statistics is determined by $\wt \sfw_i^{(2)}$ and is sample-related, which is $Q:=(\wt R_{\calA}^{-1/2\top} \wt R_{\calA}^{-1/2})^{-1}\wt R_{\calA}^{-1/2\top} \wt R^{-1/2} R  \wt R^{-1/2} \wt R^{-1/2}_{\calA} (\wt R_{\calA}^{-1/2\top} \wt R_{\calA}^{-1/2})^{-1}$. The following Proposition \ref{prop:weak-cor-aftscr} will show that, as long as the signal strength of $M$ is strong enough, the estimation of $R$ will be accurate enough such that the data-driven $Q$ is also weakly correlated.
\begin{Proposition}[Finite-sample guarantee of weak correlation after screening]\label{prop:weak-cor-aftscr}
If there exists a large absolute constant $C_0>0$ such that the matrix signal strength satisfies
\begin{equation*}
    \frac{\kappa_1^{1.5} \norm{T_{\calH} } \sigma_{\xi} }{ \lambda_{\min} \norm{ \Sigma }^{\frac{1}{2}}  } \left( \frac{\operatorname{supp}(T_{\calH} )}{\sqrt{d_2}}\wedge 1 \right)  \cdot \sqrt{\frac{ d_1^2 d_2 \log d_1}{n}} \lesssim \frac{1}{q^{\nu}},
\end{equation*}
% \begin{equation*}
% \lambda_{\min}(M)\geq C_0
%  %   \frac{ \sigma_{\xi} \norm{T_{\calH}} }{ \big\|\DMT^{1/2}\Sigma \DMT^{1/2}\big\|^{1/2}} \cdot \left( \frac{|\operatorname{supp}(T_{\calH} )|}{\sqrt{d_2}}\wedge 1 \right) \cdot \sqrt{\frac{q^{2\nu} d_1^2 d_2 \log d_1}{n}} , 
%  % \frac{\sqrt{\alpha_d }}{\sqrt{r}\beta_0}\cdot \frac{  \norm{T_{\calH} }_{2} }{ \norm{T_{\calH} }_{2,\max} } \left( \operatorname{supp}(T_{\calH} ) \wedge \sqrt{d_2} \right)
%   % \big( |\operatorname{supp}(T_{\calH} )|\wedge\sqrt{d_1} \big) \cdot \sigma_{\xi}\sqrt{\frac{q^{2\nu} d_1^2 d_2 \log d_1}{n}} ,
%      \frac{  \norm{T_{\calH} }}{ \norm{T_{\calH} }_{2,\max} } \left( |\operatorname{supp}(T_{\calH} )|\wedge \sqrt{d_2} \right) \cdot \sigma_{\xi}\sqrt{\frac{q^{2\nu} d_1^3\log d_1}{n}} ,
% \end{equation*}
then the weak correlation condition also holds for finite-sample covariance matrix $Q$, i.e., $\beta_{\textsf{s}}'$ is defined as the proportion of strongly correlated pairs using $Q$ instead of $Q^{\ast}$.  
\end{Proposition}

% A trivial upper bound of the ratio $\|T_{\calH}\|_2/ \norm{\Sigma}_{2}^{\frac{1}{2}}$ is $q^{1/2}$. However, 
% the ratio  is bounded in many interesting cases. For entry-wise tests where each linear form $T_i=e_{k_i}e_{k_j}^{\top}$, this ratio is always equal to one however large $q$ is (but smaller than $d_1d_2$, of course). Therefore, the proposed SDA method is still valid even if the goal is to test an entire row or column of $M$, which are strongly correlated. Proposition~\ref{prop:weak-cor-aftscr} suggests that a stronger matrix signal strength might be necessary if the number of tests becomes larger. 


\begin{Proposition}[LASSO screening]\label{prop:lasso-scr}
By choosing regularization level $\lambda=C\sqrt{\log d_1 +\log q} $, LASSO can recover the signal with precision 
    \begin{equation*}
    \abs{\wt\sfw^{(1)}_i -\sfw_i} \le C \kappa_1^{1.5} \sqrt{q_1 (\log d_1 +\log q) }+ h_n \abs{\sfw_i },
\end{equation*}
uniformly for all $i\in[q]$ with probability at least $1-C d_1^{-2}\log d_1 $ for some universal constant $C>0$, as long as the sample requirement of SDA holds.
% $$\left(\kappa_\infty +\kappa_1 \frac{\norm{T_{\calH} }_{2} }{\norm{ \Sigma }_2^{\frac{1}{2}}} \right)\frac{\beta_T \mu \sigma_\xi }{\beta_0 \lambda_{\min} }\sqrt{\frac{\kappa_1\alpha_d q d_1^2 d_2 \log d_1 }{n}}=o(1). $$ 
Moreover, under this condition, if $T_i\in \cS'$, then LASSO can surely select feature $i$. 
\end{Proposition}

 In our method, LASSO is used for pre-selection.
 % and we only require it to achieve sure-screening, i.e., $\PP(\cH_1 \subseteq \cA )\to 1$. This commonly used condition \citep{roeder2009genome,barber2019knockoff,du2021false,dai2023scale} can be achieved by choosing conservative $\lambda$ \citep{buhlmann2014high}. This doesn't mean that LASSO can separate non-null from null features.
 In fact, we always deliberately choose a weak regularization level so that most true signals and many false positives are included in $\cA$, at the cost of power loss. Here, we do not require the sure-screening condition of LASSO that is commonly used in \cite{roeder2009genome,barber2019knockoff,du2021false,dai2023scale}. We stress that our theory can hold with non-identified signals as long as $\norm{ \sfw_{\calA^c} }_{\infty}$ is small enough.
 

We exploit the symmetricity of $\wt\sfw^{(2)}$ obtained by linear regression after LASSO. This symmetricity, described in the following Proposition \ref{prop:OLS-normal}, serves as a counterpart of Theorem \ref{thm:asymp-normal} in the weakly correlated case.
\begin{Proposition}[Linear regression after screening]\label{prop:OLS-normal} Suppose $T_i\in \cA\cap\cH_0$. Denote an upper bound of variance shrinkage effect of screening on $\cA$ as 
$$C_\infty: = \sup_{i\in \cA }  \frac{ 1 \vee \norm{e_i^\top \left(\wt R_{\mathcal{A}}^{-1/2\top} \wt R_{\mathcal{A}}^{-1/2}\right)^{-1} \wt R_{\mathcal{A}}^{-1/2\top}\wt R^{-1/2}_{\mathcal{A}^c} }_{\ell_1} }{ \sqrt{Q_{ii}}}.$$
% $\norm{\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top}\mathbf{X}_{\mathcal{A}^c} }_\infty \le C_\infty $, and the variance shrinkage effect: $\sup_{i}\frac{s_{T_i}}{Q_{ii} }=\beta_2$. 
Here, we slightly abuse the notation by treating $\cA$ as an index set of numbers. Conditional on $\cD_0$ and $\cD_1$, we have
\begin{equation*}
   \abs{ \PP\left( \frac{\wt\sfw^{(2)}_i}{\sqrt{Q_{ii}} }\le t \middle| \cD_0, \cD_1  \right)-\Phi (t)}\le C \cdot C_\infty \left(h_n+\norm{ \sfw_{\calA^c} }_{\infty}\right).
\end{equation*}
\end{Proposition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Here, $C_\infty $ can be viewed as a special kind of coherence condition that has been broadly used in LASSO selection \citep{donoho2001uncertainty,zhao2006model,wainwright2009sharp}. Here, $\norm{ \sfw_{\calA^c} }_{\infty}$ measures the error caused by inconsistent screening.
%in the sparse case as long as the data after screening share weak dependency.



\subsection{A Practical Alternative to Algorithm \ref{alg:matrix-sda}}
Note that Algorithm \ref{alg:matrix-sda} involves the computation of the correlation coefficient matrix. In practice, one could also use the following alternative to Algorithm \ref{alg:matrix-sda} that avoids computing the inverse of diagonal elements of the covariance matrix but at the same time enjoys the same theoretical guarantees.
\begin{algorithm}[H]
	\caption{Matrix FDR Control with Whitening and Screening}
	\label{alg:matrix-sda-practical}
	\begin{algorithmic}[1]
		\REQUIRE Hypotheses $\left\{H_{0T}: \langle M, T\rangle=\theta_T , T\in \cH\right\}$, data splits $\cD_0$, $\cD_1$, $\cD_2$, rank $r$, FDR level $\alpha$.
		\STATE{ Use $\cD_0$ to construct an initial estimate $\widehat{M}_{\mathsf{init}}$}
		\STATE{ Apply proposed asymptotic test statistics to the second part of data $\cD_1$ and the third part of data $\cD_2$ respectively to get un-normalized test statistics $\mathbf{W}^{(1)}$ and $\mathbf{W}^{(2)}$, where
			\begin{equation*}
				\mathbf{W}_i^{(k)}= \widehat{s}^{(k)}_{T_i} W_{T_i}^{(k)}= \frac{\widehat{M}^{(k)}_{T_i} - \theta_{T_i}}{ \widehat{\sigma}^{(k)}_{\xi} \sqrt{d_1 d_2/n} }, \ k=1,2, \text{ and }  \widehat{D}=\operatorname{diag}\left(\widehat{s}_{T_1}^{(1)},\dots,\widehat{s}_{T_p}^{(1)} \right).
			\end{equation*}
   Here $\wt s_{T_i}^{(k)}=\big\|\calP_{\wt M^{(k)}}(T_i)\big\|_{\rm F}$ is an estimate of $s_{T_i}=\big\|\calP_M(T_i)\big\|_{\rm F}$. 
		}
		\STATE { Obtain a covariance matrix estimate $\widehat{\Sigma }$ by $\widehat{U}$, $\widehat{V}$ from $\cD_0$ and $\cD_1$:
			\begin{equation}
				\widehat{\Sigma }= T_{\calH}(I_{d_1 d_2} - \widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top  ) T_{\calH}^\top, 
			\end{equation}
			and write $\mathbf{X}= \widehat{\Sigma }^{-\frac{1}{2}}$.  Construct response $\mathbf{y}_1 = \mathbf{X}\mathbf{W}^{(1)}  $, and solve LASSO
			
			\begin{equation*}
				\wt\sfw^{(1)} = \argmin_{\sfw\in \R^q } \left\{ \frac{1}{2}\norm{\mathbf{y}_1 - \mathbf{X}\widehat{D} \sfw }^2 + \lambda\norm{\sfw}_{\ell_1} \right\}.
			\end{equation*}
			
		}
		\STATE {Denote $\cA$ as the support set of LASSO solution $\wt\sfw^{(1)}$. We then have the separation $\mathbf{X} = \left[ \mathbf{X}_{\cA}, \mathbf{X}_{\cA^c}  \right]$. We run linear regression on $\cA$ with new loading matrix $\mathbf{X}_{\cA} $ and response $\mathbf{y}_2 = \mathbf{X}\mathbf{W}^{(2)}$ from $\cD_2$ to get  asymptotic symmetric statistics $\wt\sfw^{(2)}$, where
			
			\begin{equation*}
				\wt\sfw^{(2)}_i=\left\{\begin{array}{cc}
					e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top} \mathbf{y}_2, & i \in \mathcal{A} \\
					0, & i \in \mathcal{A}^c
				\end{array}\right.
			\end{equation*}
			with variance estimate $\widehat{\sigma}_{w i}^2= e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} e_i $.
		}
		\STATE { Compute the final ranking statistics of each $T_i$ by $\wt\sfw_{T_i}^{\mathsf{Rank}}=\wt\sfw_{i}^{(1)}\wt\sfw_{i}^{(2)}/\widehat{\sigma}_{w i}$, and then choose a data-driven threshold $L$ by 
			\begin{equation*}
				L=\inf \left\{t>0: \frac{\sum_{T\in\cH}\bbI\left(\wt\sfw_T^{\mathsf{Rank}}<-t\right)}{\sum_{T\in\cH} \bbI \left(\wt\sfw_T^{\mathsf{Rank}}>t\right) \vee 1} \leq \alpha\right\}.
		\end{equation*}}
		\STATE{Reject $H_{0T_i}$ if $\wt\sfw_{T_i}^{\mathsf{Rank}}>L$}
	\end{algorithmic}
\end{algorithm}
It can be easily checked that $\wt\sfw^{(1)}_i$ and $\wt\sfw^{(2)}_i/\widehat{\sigma}_{w i}$ share the same representation as in Algorithm \ref{alg:matrix-sda}. For brevity of notations, our following proofs (presented in Sections~\ref{sec:proof-aftscr}-\ref{sec:proof-fdr-strong}) of theories in Section~\ref{sec:strong-corr} will be based on the quantities and notations in Algorithm \ref{alg:matrix-sda-practical} rather than that in Algorithm \ref{alg:matrix-sda}.



\subsection{Comparison of Data Aggregation Methods } \label{sec:compare}
The empirical success of data splitting in multiple testing leads to the problem of how to choose data aggregation methods for split data and what the theoretical explanations are behind them. In this section, we probe into the power behavior of different data aggregation methods to answer this question. Indeed, existing literature have scarcely compared the power of different FDR control procedures. Here we list some notable attempts: \cite{genovese2006false} found that the $p$-value weighting can improve the power compared with the original BHq method; \cite{scott2015false} showed simulation evidence that FDR regression improves the power upon traditional FDR control methods; for knockoff procedure,  \cite{liu2019power,weinstein2020power} focused on explaining the power behavior of knockoff under special designs. 
%However, all these attempts fail to be transferred to the case of data aggregation methods and compare the power boosted by data splitting. 
However, all these attempts have been unsuccessful in transferring to the case of data aggregation methods and in comparing the power enhancement achieved through data splitting.
We compare our methods with other combination schemes in a simple mean-testing problem. Actually, several data aggregation methods have been proposed in \cite{dai2022false} by the so-called ``mirror statistic" design. Namely, for any dimension $i\in [q]$, we derive two independent test statistics $X^1_i$, $X^2_i$ from two groups of data. Then we combine each pair of $X^1_i$, $X^2_i$ by 
\begin{equation}\label{eq:mirror}
	W(X^1_i,X^2_i)=\operatorname{sign}(X^1_i X^2_i )f(\abs{X^1_i},\abs{X^2_i} ).
\end{equation}
Possible candidates of $f(u,v)$ are 
\begin{equation}\label{eq:mirror-choice}
	f_1(u, v)=u v, \quad f_2(u, v)= \min (u, v),\quad f_3(u, v)=u+v.
\end{equation}

Among these choices, $f_2$ and $f_3$ have been discussed in \cite{xing2021controlling,dai2023scale,dai2022false} and $f_3$ is said to be nearly optimal with respect to power under certain conditions \citep{dai2022false}. Our method can be viewed as a special kind of mirror statistic design by choosing $f_1(u, v)=u v$. This amounts to computing the Hadamard product
of two test statistic vectors $X^1$, $X^2\in \R^{p}$. 
Interestingly, in practice, it is sometimes observed that $f_1$ can outperform other methods; see \cite{dai2023scale,du2021false} for examples. Here, we explain this empirical finding from a Bayesian perspective by mixture model. Consider the multiple testing problem that we observe $q$-dimensional vector $X$ sampled from the model 
\begin{equation}\label{eq:toy-model}
	\begin{aligned}
		X=\boldsymbol{\delta} + \boldsymbol{\xi},
	\end{aligned}
\end{equation} 
where noise $\boldsymbol{\xi}$ is independent multivariate gaussian with $\Sigma=I_q$ (or weakly dependent). The signals $\boldsymbol{\delta}$ are sparse and independent from an unknown non-zero prior $\mathbf{\Theta}$ in the sense that in each dimension $i\in [q]$, $\delta_i=0 $ or $\delta_i\sim \mathbf{\Theta} $, and $\pi_1:= \#\{\mu_i\sim \mathbf{\Theta}  \}/q \to 0$.  Our tests are 
$$\cH_{0i}: \delta_i=0 \text{ versus } \cH_{1i}: \delta_i\neq 0, \text{ for every } i\in[q].  $$

To examine the impact of data aggregation, suppose two observations $X^1$, $X^2$ are given, and we aim to control the FDR by data aggregation in \eqref{eq:mirror} with a certain threshold $L_\alpha$. When $q \to \infty$, the performance of this data-splitting-based method can actually be explained by a mixture model. Consider a prior $H_0: \delta=0$, and $H_1: \delta\sim \mathbf{\Theta} $, with $\PP(H_0)=1-\pi_1, \ \PP(H_1)=\pi_1$ and a variable $Y$ with mixture distribution $Y|H_0\sim W(\xi_1,\xi_2)$, and $Y|H_1\sim W(\delta+\xi_1,\delta+\xi_2)$. Here $\xi_1, \xi_2$ are independent standard normal variables. When   all the dimensions of $X$ are independent or weakly correlated, we have 
\begin{equation*}
	\frac{\#\left\{i: W_i>t\right\}}{q} \to \PP(Y>t) 
\end{equation*}
% \begin{equation}
	%    \frac{\#\left\{T: W_i<-t\right\}}{\#\left\{T: W_i>t\right\} \vee 1} \to \frac{\PP(Y<-t)}{\PP(Y>t) }
	% \end{equation}
uniformly for any $t$. 
% Denote 
% \begin{equation}
	%     L_\alpha =\min \left\{t>0: \frac{\PP(Y<-t)}{\PP(Y>t) } = \alpha\right\}, \le  \frac{\PP(Y<-t)}{\PP(Y>t)} =\alpha
	% \end{equation}
% which is the limit of $L$ and is only determined by the mixture distribution of $Y$. Then,
The limiting behavior of data aggregation method $W$ given any threshold $L$ is summarized as follows:
\begin{equation}\label{eq:limiting_fdr}
	\begin{aligned}
		\text{FDR}_W (L) &= \frac{\PP(Y>L,H_0)}{ \PP(Y>L) } = \frac{(1-\pi_1)\PP(Y>L|H_0)}{(1-\pi_1)\PP(Y>L|H_0) + \pi_1 \PP(Y>L|H_1) },  \\
		\text{Power}_W(L) & = \PP(Y>L|H_1),
	\end{aligned}
\end{equation}
where the limiting power is the expectation with respect to $ \mathbf{\Theta}  $: $\PP(Y>L|H_1) = \E_{\mathbf{\Theta}} \PP(Y>L|\delta,H_1)$. Suppose we can specify a  threshold $L_\alpha$ that controls the limiting FDR at exact level $\alpha$, i.e.,
\begin{equation}\label{eq:limiting_thres}
	L_\alpha =\min \left\{L>0: \text{FDR}_W ( L) = \alpha\right\}, 
\end{equation}
where $L_\alpha$ is determined by both FDR level $\alpha$ and aggregation function $W$. Then, at the same FDR level $\alpha$, the power of different data aggregation methods is only decided by the mixture distribution $Y$ induced by aggregation function $W$. To compare the limiting power of different aggregation method $W_j(u,v) = \operatorname{sign}(u v )f_j(\abs{u},\abs{v} )$, $j=1,2,3$, we denote $L_{\alpha j} $ as the corresponding threshold by \eqref{eq:limiting_thres}. It suffices to compare $\operatorname{Power}_{W_j}(L_{\alpha j} )$. This is equivalent to comparing the quantities $\operatorname{Power}_{W_j}(L_{p j} )$ where $L_{p j}$ is the $p$-th quantile of  null distribution $Y_j|H_0\sim W_j(\xi_1,\xi_2)$. The rationale is as follows. For the same quantile $p$, if the $\operatorname{Power}_{W_j}(L_{p j} )$ is larger, then in order to achieve the same FDR level, one must have a smaller threshold, thus the corresponding $\PP(Y_i>L |H_0) $ tends to be larger. It is clear that given the threshold $L_\alpha$ that controls the limiting FDR at exact level $\alpha$, we have $\PP(Y>L_\alpha |H_0) $ proportional to  $\PP(Y>L_\alpha |H_1) $, which implies that larger $\PP(Y>L |H_0) $  leads to a larger power.

If  $\operatorname{FDR}_{W_j}(L_{\alpha j} ) = \alpha $, then we have

\begin{equation*}
	\begin{aligned}
		\PP(Y>L_\alpha|H_0) =  \frac{\pi_1}{1-\pi_1} \PP(Y>L_\alpha|H_1)\frac{\alpha}{1-\alpha}\le c \pi_1,
	\end{aligned}
\end{equation*}
which indicates that to reach any fixed FDR level $\alpha$, the quantity $\PP(Y>L_\alpha|H_0)$ will decrease at the rate $O(\pi_1)$. We thus choose $p = O(\pi_1)$ and $L_{p j}$ satisfying $ \PP(Y_j>L_{p j}|H_0) = p$ for $j=1,2,3$. Let $z= \sqrt{p} \delta$. We will use Talyor expansion and compare the derivatives of $ \PP(Y_j>L_{p j}|z,H_0) = p$ with respect to $z$.

\begin{Theorem}\label{thm:power-comparison}
	Consider the limiting behaviors \eqref{eq:limiting_fdr} of different data aggregation methods in \eqref{eq:mirror-choice} characterized by the mixture model stated above. When achieving the same FDR level $\alpha$ and $\pi_1\to 0$, we have the following asymptotic power relation:	
	\begin{equation*}
		\begin{aligned}
			\operatorname{Power}_{W_1}(L_{\alpha 1} )\ge \operatorname{Power}_{W_2}(L_{\alpha 2} ) \ge \operatorname{Power}_{W_3}(L_{\alpha 3} ),
		\end{aligned}
	\end{equation*}
	for any bounded prior $\mathbf{\Theta} $: $\PP( \abs{\delta}\le \delta_0|\mathbf{\Theta}  )\to 1$ where $\delta_0 = o(\sqrt{\frac{1}{\pi_1}})$.
\end{Theorem}

Here, we allow the bound $\delta_0$ to go to infinity as long as its order is of $o(\sqrt{\frac{1}{\pi_1}})$. This theorem offers a theoretical justification for the superiority of our data aggregation method over other common alternatives, a conclusion that aligns with our empirical findings in \cite{dai2023scale,du2021false}.




Intuitively, when the two-sided tails of mixture distribution are more unbalanced (left-skewed) and $\PP(Y>t)$ decreases slower, the threshold $L_\alpha$ tends to be smaller and thus the null and non-null distributions can be well-separated. In Figure~\ref{fig:c0}, we present a simulation of the density of mixture distributions and $\PP(Y>L_\alpha|H_1)$ given different data aggregation methods.

\begin{figure}
	\centering
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/Density.png}
		\caption{Density of mixture distribution}
		\label{fig:c1}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth}
		\includegraphics[width=\textwidth]{figures/Power.png}
		\caption{$\PP(Y>L_\alpha|H_1)$ }
		\label{fig:c2}
	\end{subfigure}
	\caption{Simulation of mixture distribution $Y$ under different constructions}
	\label{fig:c0}
\end{figure}
It is observed that $f_1$ generates a narrower mixture distribution with unbalanced tails starting to decrease slowly when $t$ is moderate, and the limiting power of $f_1$ is the highest among the three combinations.


\section{Proofs of Main Results}

\subsection{Proof of Theorems \ref{thm:asymp-normal}, \ref{thm:asymp-normal-varest}}
\begin{proof}
    The general proof strategy is similar to that of \cite{xia2021statistical} but with a more involved discussion at the core step: separating the negligible part of the statistic out of the sum of the i.i.d part, followed by controlling the sum of the i.i.d. part by the Berry-Essen theorem. We start with the decomposition of our statistics. Denote $\widehat{\Delta}=M-\widehat{M}_{\mathsf{init} }$, and 
    \begin{equation*}
        \begin{aligned}
          \widehat{M}^{\mathsf {unbs }} = M+ \underbrace{ \overbrace{\frac{d_1 d_2}{n} \sum_{i\in I_2 } \xi_i X_i}^{\widehat{Z}_1} +\overbrace{\left(\frac{d_1 d_2}{n} \sum_{i\in I_2 }\left\langle\widehat{\Delta}, X_i\right\rangle X_i-\widehat{\Delta}\right)}^{\widehat{Z}_2 } }_{\widehat Z }.
    \end{aligned}
    \end{equation*}
Here we denote $I_2$ the index set of observations in the sample $\calD_2$. 
    To ease the notation, we denote the initialization $\gamma(n,d_1,d_2,\tau)= \gamma_{n}$ such that $\norm{\widehat{M}^{\mathsf{init} }-M }_{\max}\le \gamma_{n}$, which holds with probability at least $1-d_1^\tau$. 
We separate the vanishing part out of the asymptotic normal part:
    \begin{equation*}
        \begin{aligned}
          \widehat{M}_{T}-M_T = \left\langle \widehat{U}\widehat{U}^\top \widehat{Z} \widehat{V}\widehat{V}^\top - UU^\top \widehat{Z}VV^\top, T \right\rangle + \left\langle \widehat{U}\widehat{U}^\top M \widehat{V}\widehat{V}^\top - M, T \right\rangle + \left\langle UU^\top \widehat{Z}VV^\top, T\right\rangle.
    \end{aligned}
    \end{equation*}
We shall show that, after proper rescaling: (1) $\left\langle \widehat{U}\widehat{U}^\top \widehat{Z} \widehat{V}\widehat{V}^\top - UU^\top \widehat{Z}VV^\top, T \right\rangle$ is vanishing; (2) $\left\langle \widehat{U}\widehat{U}^\top M \widehat{V}\widehat{V}^\top - M, T \right\rangle + \left\langle UU^\top \widehat{Z}VV^\top, T\right\rangle$ is asymptotic normal. The scale we require is exactly $\sqrt{d_1 d_2/n}\sigma_{\xi}\norm{\cP_M(T) }_{\tF}$. Define $E_0$ as the event that the initial estimate $\widehat{M}_{\mathsf{init}}$ constructed from $\cD_0$ follows the error bound $\gamma(n,d_1,d_2)$. Then it holds that $E_0\in \sigma (\cD_0)$ and $\PP (E_0)\ge 1- d_1^{-\tau}$.

The first part $\left\langle \widehat{U}\widehat{U}^\top \widehat{Z} \widehat{V}\widehat{V}^\top - UU^\top \widehat{Z}VV^\top, T \right\rangle$ is vanishing with the rate described in Lemma \ref{lemma:vanishing-part}.

\begin{Lemma}\label{lemma:vanishing-part}
        Under the assumptions of incoherence and sufficient signal strength, there exists an absolute constant $C>0$ such that conditional on $E_0$, if $n \geq C d_1 \log d_1$, then with probability at least $1-6 \log d_1 \cdot d_1^{-\tau}$, the inequality 
$$
 \abs{\left\langle \widehat{U}\widehat{U}^\top \widehat{Z} \widehat{V}\widehat{V}^\top - UU^\top \widehat{Z}VV^\top, T \right\rangle}  \leq C_2 \tau \|T\|_{\ell_1} \mu^2 \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{r d_1^2 d_2 \log d_1}{n}} \cdot \sigma_{\xi} \sqrt{\frac{r d_1 \log d_1}{n}}
$$
uniformly holds for every $T$.
%\ref{asp:init}-\ref{asp:signals}
    \end{Lemma}


We turn to prove the asymptotic normality of part $\left\langle \widehat{U}\widehat{U}^\top M \widehat{V}\widehat{V}^\top - M, T \right\rangle + \left\langle UU^\top \widehat{Z}VV^\top, T\right\rangle$. 
The core i.i.d. part that contributes to asymptotic normality is the combination of the first order term of representation formula of the empirical spectral projectors \citep{xia2021normal}, and $\left\langle UU^\top \widehat{Z}_1VV^\top, T\right\rangle$. Define the $(d_1+d_2)\times (d_1+d_2)$ matrix $A$ with the corresponding perturbation $\widehat{E}$ as 

\begin{equation*}
    \begin{aligned}
        A = \left( \begin{array}{cc}
            0 & M \\ M^\top & 0 
        \end{array} 
        \right),\quad \widehat{E} = \left( \begin{array}{cc}
            0 & \widehat{Z} \\ \widehat{Z}^\top & 0 
        \end{array}
        \right).
    \end{aligned}
\end{equation*}
Also, define the singular subspace with its estimate as 

$$
\Theta=\left(\begin{array}{cc}
U & 0 \\
0 & V
\end{array}\right),  \quad \widehat{\Theta}=\left(\begin{array}{cc}
\widehat{U} & 0 \\
0 & \widehat{V}
\end{array}\right).
$$
By the representation formula \citep{xia2021normal}, when $\lambda_{\min} \ge 2 \norm{\widehat{Z} } $, it follows that:

\begin{equation}\label{eq:represent}
    \begin{aligned}
    \widehat{\Theta} \widehat{\Theta}^\top - \Theta\Theta^\top = \sum^{\infty}_{k=1} \underbrace{\sum_{\Sigma: s_1+\dots+ s_{k+1}=k } (-1)^{1+\tau(\Sigma) } \cdot  \mathfrak{P}^{-s_1} \widehat{E} \mathfrak{P}^{-s_2}\cdots \mathfrak{P}^{-s_k} \widehat{E} \mathfrak{P}^{-s_{k+1} } }_{ \mathcal{S}_{A,k}(\widehat{E}) },
    \end{aligned}
\end{equation}
where $s_1, \cdots, s_{k+1} \geq 0$ are integers and $\tau(\Sigma)=\sum_{i=1}^{k+1} \mathbf{1}\left(s_i>0\right)$. And $ \mathfrak{P}^{-s}$ is defined by
$$
\mathfrak{P}^{-s}=\left\{\begin{array}{cc}
\left(\begin{array}{cc}
U \Lambda^{-s} U^{\top} & 0 \\
0 & V \Lambda^{-s} V^{\top}
\end{array}\right), & \text { if } s \text { is even; } \\
\left(\begin{array}{cc}
0 & U \Lambda^{-s} V^{\top} \\
V \Lambda^{-s} U^{\top} & 0
\end{array}\right), & \text { if } s \text { is odd. }
\end{array}\right.
$$
with the order $0$ term: 
$$
\mathfrak{P}^0=\mathfrak{P}^{\perp}=\left(\begin{array}{cc}
U_{\perp} U_{\perp}^{\top} & 0 \\
0 & V_{\perp} V_{\perp}^{\top}
\end{array}\right).
$$
By \ref{eq:represent}, we have
$$
\begin{aligned}
& \widehat{\Theta} \widehat{\Theta}^{\top} A \widehat{\Theta} \widehat{\Theta}^{\top}-\Theta \Theta^{\top} A \Theta \Theta^{\top} \\
& =\left(\mathcal{S}_{A, 1}\left(\widehat{E}\right) A \Theta \Theta^{\top}+\Theta \Theta^{\top} A \mathcal{S}_{A, 1}\left(\widehat{E}\right)\right)+\sum_{k=2}^{\infty}\left(\mathcal{S}_{A, k}\left(\widehat{E}\right) A \Theta \Theta^{\top}+\Theta \Theta^{\top} A \mathcal{S}_{A, k}\left(\widehat{E}\right)\right) \\
& +\left(\widehat{\Theta} \widehat{\Theta}^{\top}-\Theta \Theta^{\top}\right) A\left(\widehat{\Theta} \widehat{\Theta}^{\top}-\Theta \Theta^{\top}\right).
\end{aligned}
$$
Define $\widetilde{T}= \left( \begin{array}{cc}
            0 & T \\ 0 & 0 
        \end{array} \right)$. We can then write 
$$
\begin{aligned}
    \left\langle \widehat{U}\widehat{U}^\top M \widehat{V}\widehat{V}^\top - M, T \right\rangle & =  \left\langle \widehat{\Theta} \widehat{\Theta}^{\top} A \widehat{\Theta} \widehat{\Theta}^{\top}-\Theta \Theta^{\top} A \Theta \Theta^{\top} , \widetilde{T} \right\rangle  \\
     = & \left\langle U U^\top \widehat{Z}_1 V_{\perp} V_{\perp}^\top + U_{\perp} U^\top_{\perp} \widehat{Z}_1 VV^\top, T \right\rangle +\left\langle U U^\top \widehat{Z}_2 V_{\perp} V_{\perp}^\top + U_{\perp} U^\top_{\perp} \widehat{Z}_2 VV^\top, T \right\rangle \\
     & + \sum_{k=2}^{\infty} \left\langle \mathcal{S}_{A, k}\left(\widehat{E}\right) A \Theta \Theta^{\top}+\Theta \Theta^{\top} A \mathcal{S}_{A, k}\left(\widehat{E}\right) , \widetilde{T} \right\rangle \\
    & + \left\langle \left(\widehat{\Theta} \widehat{\Theta}^{\top}-\Theta \Theta^{\top}\right) A\left(\widehat{\Theta} \widehat{\Theta}^{\top}-\Theta \Theta^{\top}\right), \widetilde{T} \right\rangle.
\end{aligned}
$$
Combined with $ \left\langle UU^\top \widehat{Z}VV^\top, T\right\rangle$, we have the decomposition
\begin{equation}\label{eq:asymp-decomp}
  \begin{aligned}
    &\left\langle \widehat{U}\widehat{U}^\top M \widehat{V}\widehat{V}^\top - M, T \right\rangle + \left\langle UU^\top \widehat{Z}VV^\top, T\right\rangle \\
   = &  \left\langle U U^\top \widehat{Z}_1 V_{\perp} V_{\perp}^\top + U_{\perp} U^\top_{\perp} \widehat{Z}_1 VV^\top, T \right\rangle + \left\langle UU^\top \widehat{Z}_1VV^\top, T\right\rangle \\
      & +  \left\langle U U^\top \widehat{Z}_2 V_{\perp} V_{\perp}^\top + U_{\perp} U^\top_{\perp} \widehat{Z}_2 VV^\top, T \right\rangle +\left\langle UU^\top \widehat{Z}_2VV^\top, T\right\rangle  \\
     & + \sum_{k=2}^{\infty} \left\langle \mathcal{S}_{A, k}\left(\widehat{E}\right) A \Theta \Theta^{\top}+\Theta \Theta^{\top} A \mathcal{S}_{A, k}\left(\widehat{E}\right) , \widetilde{T} \right\rangle \\
    & + \left\langle \left(\widehat{\Theta} \widehat{\Theta}^{\top}-\Theta \Theta^{\top}\right) A\left(\widehat{\Theta} \widehat{\Theta}^{\top}-\Theta \Theta^{\top}\right), \widetilde{T} \right\rangle.
\end{aligned}  
\end{equation}
Denote the sum of i.i.d. part as $\Bar{Z}=\left\langle U U^\top \widehat{Z}_1 V_{\perp} V_{\perp}^\top + U_{\perp} U^\top_{\perp} \widehat{Z}_1 VV^\top, T \right\rangle + \left\langle UU^\top \widehat{Z}_1VV^\top, T\right\rangle$. Compute the second-order moment and third-order moment, respectively:
\begin{equation*}
    \begin{aligned}
        \E \Bar{Z}^2 = & \frac{d_1^2d_2^2}{n}\E \xi^2 \left(\left\langle U_{\perp} U_{\perp}^{\top} X V V^{\top}, T\right\rangle+\left\langle U U X V_{\perp} V_{\perp}^{\top}, T\right\rangle + \left\langle UU^{\top} X V V^{\top}, T\right\rangle \right)^2 \\
        = & \frac{d_1^2d_2^2}{n}\sigma_{\xi}^2 \E  \left[\left\langle UU^{\top} X, T\right\rangle^2 +\left\langle U_{\perp} U_{\perp}^{\top} X VV^\top, T\right\rangle^2 + 2 \left\langle UU^{\top} X, T\right\rangle \left\langle U_{\perp} U_{\perp}^{\top} X VV^\top, T\right\rangle\right]\\
        = & \frac{d_1d_2}{n}\sigma_{\xi}^2 \sum_{i_1\in [d_1] }\sum_{i_2\in [d_2] } (e_{i_1}^\top U U^\top T e_{i_2} )^2 + (e_{i_1}^\top U_{\perp} U^\top_{\perp} TVV^\top e_{i_2} )^2 + 2 e_{i_2}^\top VV^\top T^\top U_{\perp} U^\top_{\perp}e_{i_1}  e_{i_1}^\top U U^\top T e_{i_2} \\
        = & \frac{d_1d_2}{n}\sigma_{\xi}^2 \left(\norm{U^\top T}_{\tF}^2 + \norm{U^\top_{\perp} TV }^2_{\tF} \right) = \frac{d_1d_2}{n}\sigma_{\xi}^2 \norm{\cP_M(T) }_\tF^2.
    \end{aligned}
\end{equation*}

\begin{equation*}
    \begin{aligned}
        \E \Bar{Z}^3 \le & C \frac{d_1^{3} d_2^3}{n^2 } \sigma_{\xi}^3 \E \left(\left\langle U_{\perp} U_{\perp}^{\top} X V V^{\top}, T\right\rangle+\left\langle U U X V_{\perp} V_{\perp}^{\top}, T\right\rangle + \left\langle UU^{\top} X V V^{\top}, T\right\rangle \right)^3 \\
        \le & C\frac{d_1^{3} d_2^3}{n^2 } \sigma_{\xi}^3 \cdot 4 \E  \left[\left\langle UU^{\top} X, T\right\rangle^3 +\left\langle U_{\perp} U_{\perp}^{\top} X VV^\top, T\right\rangle^3 \right]\\
        \le & C\frac{d_1^{3} d_2^3}{n^2} \sigma_{\xi}^3  \E \left[\left\langle UU^{\top} X, T\right\rangle^2\abs{\left\langle UU^{\top} X, T\right\rangle } + \left\langle U_{\perp} U_{\perp}^{\top} X VV^\top, T\right\rangle^2 \abs{\left\langle U_{\perp} U_{\perp}^{\top} X VV^\top, T\right\rangle}\right]. \\
    \end{aligned}
\end{equation*}
Since, by incoherence assumption, 
\begin{equation*}
    \begin{aligned}
        \abs{\left\langle UU^{\top} X, T\right\rangle } &= \abs{\left\langle U^{\top} e_{i_1} e_{i_2}^\top, U^\top T\right\rangle } \le \norm{U^\top T}_{\tF} \mu \sqrt{ \frac{r}{d_1}}, \\
        \abs{\left\langle U_{\perp} U_{\perp}^{\top} X VV^\top, T\right\rangle} & = \abs{\left\langle  U_{\perp}^{\top} e_{i_1} e_{i_2}^\top V, U_{\perp}^{\top}T V \right\rangle} \le \norm{U_{\perp}^{\top}T V}_{\tF} \mu \sqrt{ \frac{r}{d_2}},
    \end{aligned}
\end{equation*}
the third-order moment is thus bounded by:
\begin{equation*}
    \begin{aligned}
    \E \Bar{Z}^3 \le & C\frac{d_1^{2} d_2^{1.5} \mu \sqrt{r} }{n^2 } \sigma_{\xi}^3  \left(\norm{U^\top T}_{\tF}^3  + \norm{U^\top_{\perp} TV }^3_{\tF} \right) \le  C\frac{d_1^{2} d_2^{1.5} \mu \sqrt{r} }{n^2 } \sigma_{\xi}^3 \norm{\cP_M(T) }_{\tF} ^{3}. \\
    \end{aligned}
\end{equation*}
By invoking the Berry-Essen theorem to get the error bound of normal approximation, we have

\begin{equation}\label{eq:B-E-bound}
    \begin{aligned}
       \sup_{t\in \R} \abs{ \PP \left(\frac{\left\langle U U^\top \widehat{Z}_1 + U_{\perp} U^\top_{\perp} \widehat{Z}_1 VV^\top, T \right\rangle}{\sigma_{\xi} \norm{\cP_M(T) }_{\rm F}  \sqrt{d_1 d_2/n}  } \le t  \right) - \Phi(t)}\le C \mu \sqrt{\frac{r d_1}{n}}.
    \end{aligned}
\end{equation}
%\left(\norm{U^\top T}_{\tF}^2 + \norm{U^\top_{\perp} TV }^2_{\tF} \right)^{\frac{1}{2}}
We now return to \eqref{eq:asymp-decomp} to control other terms. We show that, apart from $\Bar{Z}$, all the other terms in \eqref{eq:asymp-decomp} will vanish uniformly for all $T\in\cH_0$ after scaling.

\begin{Lemma}\label{lemma:asymp-vanishing-part-1}
        Under the assumptions of incoherence and sufficient signal strength, there exists an absolute constant $C>0$ such that conditional on $E_0$, if $n \geq C d_1 \log d_1$, then with probability at least $1-2 \cdot d_1^{-\tau}$, the inequality 
$$
\begin{aligned}
     \abs{\left\langle U U^\top \widehat{Z}_2 V_{\perp} V_{\perp}^\top + U_{\perp} U^\top_{\perp} \widehat{Z}_2 VV^\top, T \right\rangle +\left\langle UU^\top \widehat{Z}_2VV^\top, T\right\rangle}\\  \leq C \sqrt{\tau} \sigma_{\xi} \gamma_{n} \norm{\cP_M(T) }_\tF  \sqrt{\frac{ d_1 d_2 \log d_1}{n}}
\end{aligned}
$$
uniformly holds for every $T$.
%\ref{asp:init}-\ref{asp:signals}
\end{Lemma}

\begin{Lemma}[\cite{xia2021statistical}]\label{lemma:asymp-vanishing-part-2}
        Under the assumptions of incoherence and sufficient signal strength, there exists an absolute constant $C>0$ such that conditional on $E_0$, if $n \geq C d_1 \log d_1$, then with probability at least $1-6\log d_1 \cdot d_1^{-\tau}$, inequalities 
$$
\begin{aligned}
    &\abs{\sum_{k=2}^{\infty} \left\langle \mathcal{S}_{A, k}\left(\widehat{E}\right) A \Theta \Theta^{\top}+\Theta \Theta^{\top} A \mathcal{S}_{A, k}\left(\widehat{E}\right) , \widetilde{T} \right\rangle} \le C \tau\|T\|_{\ell_1} \mu_{ }^2 \sigma_{\xi} \sqrt{\frac{r d_1 \log d_1}{n}} \cdot\left(\frac{\sigma_{\xi}}{\lambda_{\min}}  \sqrt{\frac{r d_1^2 d_2 \log d_1}{n}}\right), \\
    &  \abs{\left\langle \left(\widehat{\Theta} \widehat{\Theta}^{\top}-\Theta \Theta^{\top}\right) A\left(\widehat{\Theta} \widehat{\Theta}^{\top}-\Theta \Theta^{\top}\right), \widetilde{T} \right\rangle} \le C\tau\kappa_0 \|T\|_{\ell_1} \mu_{ }^2 \sigma_{\xi} \sqrt{\frac{r d_1 \log d_1}{n}} \cdot\left(\frac{\sigma_{\xi}}{\lambda_{\min}}  \sqrt{\frac{r d_1^2 d_2 \log d_1}{n}}\right) .
\end{aligned}
$$
uniformly hold for every $T$.

\end{Lemma}

Now combine \eqref{eq:B-E-bound} with Lemma \ref{lemma:vanishing-part}, \ref{lemma:asymp-vanishing-part-1}, and \ref{lemma:asymp-vanishing-part-2}. By Lipschitz property of $\Phi (t)$, we conclude that
%Assumption \ref{asp:alignment}, we conclude the that 

    \begin{equation*}
    \begin{aligned}
        &\sup _{t \in \mathbb{R}}\left|\mathbb{P}\left(\frac{\widehat{M}_T-M_T}{\sigma_{\xi}\|\cP_M(T) \|_\tF \cdot \sqrt{d_1 d_2 / n}} \leq t\right)-\Phi(t)\right| 
        \le C_2  \log d_1 \cdot d_1^{-\tau}+ C_3\mu \sqrt{\frac{r d_1}{n}} \\
        & + C_4\tau\kappa_0 \mu^2 \frac{\sigma_{\xi}}{ \lambda_{\min} }  \frac{\|T\|_{\ell_1}}{\| \cP_M(T)\|_\tF}\sqrt{\frac{\alpha_d r^2 d_1 d_2 \log ^2 d_1}{n}}+ C_5\tau\gamma_{n} \sqrt{\log d_1},
    \end{aligned}
    \end{equation*}
which proves the first statement.

%     \begin{equation*}
%     \begin{aligned}
%         &\sup _{t \in \mathbb{R}}\left|\mathbb{P}\left(\frac{\widehat{M}_T-M_T}{\sigma_{\xi}\|\cP_M(T) \|_\tF \cdot \sqrt{d_1 d_2 / n}} \leq t\right)-\Phi(t)\right| 
%         \le C_2  \log d_1 \cdot d_1^{-\tau}+ C_3\mu \sqrt{\frac{r d_1}{n}} \\
%         & + C_4\tau\kappa_0 \mu^2 \frac{\sigma_{\xi}}{\beta \lambda_{\min} } \max_{T\in \cH} \left\{\frac{\|T\|_{\ell_1}}{\|T\|_\tF}\right\}  \sqrt{\frac{\alpha_d r d_1^2 d_2 \log ^2 d_1}{n}}+ C_5\tau\gamma_{}(n, d_1, d_2,\tau) \sqrt{\log d_1},
%     \end{aligned}
%     \end{equation*}

Since 
\begin{equation}\label{eq:decomp-var-est}
    \frac{\widehat{M}_T-M_T}{\widehat{\sigma}_{\xi}  \widehat{s}_T \cdot \sqrt{d_1 d_2 / n}} - \frac{\widehat{M}_T-M_T}{\sigma_{\xi}\|\cP_M(T) \|_\tF \cdot \sqrt{d_1 d_2 / n}} = \frac{\widehat{M}_T-M_T}{\sigma_{\xi}\|\cP_M(T) \|_\tF \cdot \sqrt{d_1 d_2 / n}}\left( \frac{\sigma_\xi \norm{\cP_M(T)}_\tF }{\widehat{\sigma}_{\xi}  \widehat{s}_T} -1\right), 
\end{equation}
and 
    \begin{equation*}
    \begin{aligned}
\abs{\frac{\widehat{M}_T-M_T}{\sigma_{\xi}\|\cP_M(T) \|_\tF \cdot \sqrt{d_1 d_2 / n}}}\le C \sqrt{\tau \log d_1},
    \end{aligned}
    \end{equation*}
with probability at least $1-C \log d_1 \cdot d_1^{-\tau}-C\tau h_n $, we focus on the term $\frac{\sigma_\xi \norm{\cP_M(T)}_\tF }{\widehat{\sigma}_{\xi}  \widehat{s}_T} -1$ and discuss the estimation accuracy of $\sigma_\xi$ and $s_{T}$ respectively.

The estimation of $\sigma_\xi$ shares the accuracy 
$$
\left|1-\frac{\widehat{\sigma}_{\xi}}{\sigma_{\xi}}\right| \leq\left|1-\frac{\widehat{\sigma}_{\xi}^2}{\sigma_{\xi}^2}\right| \leq \frac{C_1 \tau \log d_1}{\sqrt{n}}+C_2 \gamma_n^2,
$$
with probability at least $1-2 d^{-\tau}$ by Bernstein inequality; under the event that Lemma \ref{lemma:vanishing-part} holds, it is also clear that
$$
\left| 1- \frac{\widehat{s}_T }{\norm{\cP_M(T)}_\tF}\right| \le \abs{1- \frac{\widehat{s}_T^2 }{\norm{\cP_M(T)}_\tF^2}} \le \abs{\frac{\norm{U^\top T }_{\tF}^2 -\norm{\widehat{U}^\top T }_{\tF}^2 + \norm{ U_{\perp}^\top T V }^2_{\tF}- \norm{ \widehat U_{\perp}^\top T \widehat V }^2_{\tF} }{\norm{\cP_M(T)}_\tF^2} },
$$
and 
$$
\begin{aligned}
\abs{\norm{U^\top T }_{\tF}^2 -\norm{\widehat{U}^\top T }_{\tF}^2} \leq C_1 \mu^2\tau \frac{\|T\|_{\ell_1}^2}{d_1} \cdot\left(\frac{\sigma_{\xi}^2}{\lambda_{\min}^2}\right) \frac{r d_1^2 d_2 \log d_1}{n}+C_2\| U^\top T\|_{\mathrm{F}}\|T\|_{\ell_1} \mu \cdot \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{r\tau d_1 d_2 \log d_1}{n}}.
\end{aligned}
$$
We compute the difference of $T$'s projections onto both singular subspaces and their orthogonal complements: 
$$
\abs{\norm{ U_{\perp}^\top T V }^2_{\tF}- \norm{ \widehat U_{\perp}^\top T \widehat V }^2_{\tF}} \le \abs{\norm{ U_{\perp}^\top T V }^2_{\tF}- \norm{  U_{\perp}^\top T \widehat V }^2_{\tF}} +\abs{\norm{ \widehat U_{\perp}^\top T V }^2_{\tF}- \norm{ \widehat U_{\perp}^\top T \widehat V }^2_{\tF}}.
$$
It follows that 

$$
\begin{aligned}
     &\abs{\norm{ U_{\perp}^\top T V }^2_{\tF}- \norm{  U_{\perp}^\top T \widehat V }^2_{\tF}} \le \norm{U^\top_{\perp} T (VV^\top- \widehat V \widehat V^\top) }_{\tF}^2 + 2\abs{\langle U^\top_{\perp} T (VV^\top- \widehat V \widehat V^\top), U^\top_{\perp} T V  \rangle} \\
& \leq C_1 \mu^2\tau \frac{\|T\|_{\ell_1}^2}{d_2} \cdot\left(\frac{\sigma_{\xi}^2}{\lambda_{\min}^2}\right) \frac{r d_1^2 d_2 \log d_1}{n}+C_2\| U^\top_\perp T {V} \|_{\mathrm{F}}\|T\|_{\ell_1} \mu \cdot \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{r\tau d_1^2 \log d_1}{n}},
\end{aligned}
$$
and
$$
\begin{aligned}
     &\abs{\norm{  U_{\perp}^\top T \widehat V }^2_{\tF}- \norm{ \widehat U_{\perp}^\top T \widehat V }^2_{\tF}} \le \norm{ (\widehat U \widehat U^\top- UU^\top) T \widehat V \widehat V^\top }_{\tF}^2 + 2\abs{\langle U^\top_{\perp} T (VV^\top- \widehat V \widehat V^\top), U^\top_{\perp} T \widehat V  \rangle} \\
     & \leq C_1 \mu^2\tau \frac{\|T\|_{\ell_1}^2}{d_1} \cdot\left(\frac{\sigma_{\xi}^2}{\lambda_{\min}^2}\right) \frac{r d_1^2 d_2 \log d_1}{n}+C_2 (\| U^\top_\perp T  V \|_{\mathrm{F}} \\
     & \ +{\| U^\top_\perp T (\widehat V \widehat V^\top -VV^\top ) \|_{\mathrm{F}}} )\|T\|_{\ell_1} \mu \cdot \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{r\tau d_1^2 \log d_1}{n}} \\
      & \leq C_1 \mu^2\tau \frac{\|T\|_{\ell_1}^2}{d_1} \cdot\left(\frac{\sigma_{\xi}^2}{\lambda_{\min}^2}\right) \frac{r d_1^2 d_2 \log d_1}{n}+ C_2\| U^\top_\perp T {V} \|_{\mathrm{F}}\|T\|_{\ell_1} \mu \cdot \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{r\tau d_1^2 \log d_1}{n}} \\
      & + C_3 \mu^2\tau \frac{\|T\|_{\ell_1}^2}{d_2} \cdot\left(\frac{\sigma_{\xi}^2}{\lambda_{\min}^2}\right) \frac{r d_1^2 d_2 \log d_1}{n}.
\end{aligned}
$$

%By Assumption \ref{asp:alignment}, $\norm{\cP_M(T)}_{\tF}\ge \beta_0 \norm{T}_\tF \sqrt{\frac{r}{d_1}}  $,
We then can show that 
\begin{equation}\label{eq:sT-diff}
    \begin{aligned}\left| 1- \frac{\widehat{s}_T }{\norm{\cP_M(T)}_\tF}\right| \le \abs{1- \frac{\widehat{s}_T^2 }{\norm{\cP_M(T)}_\tF^2}} \le 
         C_2 \mu \frac{\|T\|_{\ell_1}}{\norm{\cP_M(T)}_{\tF} } \cdot \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{\tau r \alpha_d d_1 d_2 \log d_1}{n}},
    \end{aligned}
\end{equation}
which indicates the fact that, with probability at least $1-C\log d_1 d_1^{-\tau} $,
\begin{equation*}
    \begin{aligned}
         \abs{\frac{ \widehat{\sigma}_{\xi}  \widehat{s}_T  }{\sigma_\xi \norm{\cP_M(T)}_\tF} -1}&\le \abs{\left(\frac{ \widehat{\sigma}_{\xi}    }{\sigma_\xi }- 1\right) \left(\frac{ \widehat{s}_T  }{ \norm{\cP_M(T)}_\tF}-1\right) } + \abs{\frac{ \widehat{\sigma}_{\xi}    }{\sigma_\xi }- 1}+ \abs{\frac{ \widehat{s}_T  }{ \norm{\cP_M(T)}_\tF}-1} \\
         &\le \frac{C_1 \tau \log d_1}{\sqrt{n}}+C_2 \gamma_n^2 + C_3 \mu \frac{\|T\|_{\ell_1}}{\norm{\cP_M(T)}_{\tF} } \cdot \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{\tau r \alpha_d d_1 d_2 \log d_1}{n}}.
    \end{aligned}
\end{equation*}
Applying again the Lipschitz property of $\Phi (t)$ to the decomposition \eqref{eq:decomp-var-est}, we can obtain the second statement.


\end{proof}

\subsection{Proof of Theorem \ref{thm:asymp-two-var}}
\begin{proof}
Since in Lemma \ref{lemma:vanishing-part}, \ref{lemma:asymp-vanishing-part-1}, and \ref{lemma:asymp-vanishing-part-2}, the inequalities uniformly hold for all $T$, we can write the couple $(W_{T_1},W_{T_2})$ as 
$$
(W_{T_1},W_{T_2}) = \left(\frac{\left\langle  \widehat{Z}_1 , \cP_M(T_1) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T_1) }_\tF  \sqrt{d_1 d_2/n}}+ \Delta_{T_1}, \frac{\left\langle  \widehat{Z}_1 , \cP_M(T_2) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T_2) }_\tF  \sqrt{d_1 d_2/n}}+ \Delta_{T_2} \right),
$$
where with probability at least $1-C\log d_1 d_1^{-\tau}  $,
\begin{equation}\label{eq:2-d-remainder}
\begin{aligned}
     \max\left\{\abs{\Delta_{T_1}}, \abs{\Delta_{T_2}}\right\} \le & C_4\tau\kappa_0 \mu^2 \frac{\sigma_{\xi} }{ \lambda_{\min} }\left(\frac{\|T_1\|_{\ell_1}}{\| \cP_M(T_1)\|_\tF}+\frac{\|T_2\|_{\ell_1}}{\| \cP_M(T_2)\|_\tF}\right)   \sqrt{\frac{\alpha_d r d_1 d_2 \log ^2 d_1}{n}}\\
     & + C_5\tau\gamma_{n}\sqrt{\log d_1},
\end{aligned}
\end{equation}
by the same argument in the proof of Theorem \ref{thm:asymp-normal}. Notice that $\langle\widehat{Z}_1,\cP_M(T_i) \rangle$, $i=1,2$ are the sum of i.i.d. random variables, with covariance:

\begin{equation*}
\begin{aligned}
         \E \langle\widehat{Z}_1,\cP_M(T_1) \rangle \langle\widehat{Z}_1,\cP_M(T_2) \rangle &= \frac{ d_1^2 d_2^2}{n^2} \sum_{i\in I_2} \xi_i^2 \left\langle  X_i, \cP_M(T_1) \right\rangle\left\langle X_i, \cP_M(T_2) \right\rangle \\
         & = \frac{ d_1 d_2}{n} \sigma_{\xi}^2 \sum_{i\in [d_1]}\sum_{j\in [d_2]} e_i^\top \cP_M(T_1)e_j e_i^\top \cP_M(T_2)e_j \\
         & = \frac{ d_1 d_2}{n} \sigma_{\xi}^2\left\langle  \cP_M(T_1), \cP_M(T_2) \right\rangle.
\end{aligned}
\end{equation*}
Then we have 
\begin{equation*}
    \operatorname{cov}\left(\frac{\left\langle  \widehat{Z}_1 , \cP_M(T_1) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T_1) }_\tF  \sqrt{d_1 d_2/n}}, \frac{\left\langle  \widehat{Z}_1 , \cP_M(T_2) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T_2) }_\tF  \sqrt{d_1 d_2/n}} \right) =:\rho_{T_1, T_2}.
\end{equation*}
We jointly control the c.d.f. of them by multivariate Berry-Essen theorem \citep{stein1972bound,raivc2019multivariate}

\begin{equation}
    \begin{aligned}
       \sup_{t_1,t_2\in \R} \abs{ \PP \left( \frac{\left\langle  \widehat{Z}_1 , \cP_M(T_1) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T_1) }_\tF  \sqrt{d_1 d_2/n}}\le t_1, \frac{\left\langle  \widehat{Z}_1 , \cP_M(T_2) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T_2) }_\tF  \sqrt{d_1 d_2/n}}  \le t_2  \right) - \Phi_{\rho_{T_1,T_2}}(t_1,t_2)}\le C \mu \sqrt{\frac{r d_1}{n}}.
    \end{aligned}
\end{equation}
The gradient bound  $\norm{\nabla \Phi_{\rho} (t_1,t_2)}\le C$ indicates the Lipschitz property of $\Phi_{\rho} (t_1,t_2)$, which suggests the desired probability bound. 
\end{proof}


\subsection{Proof of Theorem \ref{thm:weak-cor-fdr}}
We proceed to prove Theorem \ref{thm:weak-cor-fdr} in the sequel by three steps: we first show that $\bbI(W_{T}^{(i)}>t) $ follows weak dependency and asymptotic symmetricity for $T\in\cH_0$ when $t$ is in a certain region $[0,L_n]$; then we show that with high probability, the data-driven threshold is in the region $[0,L_n]$; finally we control the power when strong signals dominate signals in the non-null set. Since $n=O(d_1 d_2)$ in general, we take $h_n=\Omega({\frac{\sqrt{\log d_2} }{d_2 } }\vee (rd_1/n)^{1/4} ) $ in the proof for simplicity. Here $h_n$ can be smaller as long as $n$ is large. Denote $E_0\in \sigma(\cD_0)$ as the event that we initialize our algorithm with the required accuracy using $\cD_0$. We first start with the asymptotic property of ${W}^{(i)}_{T}$.
\subsubsection{Weak dependence and symmetricity}
From the proof of Theorem \ref{thm:asymp-normal} and definition of $h_n$, we have the following claim of the asymptotic normality of ${W}^{(1)}_{T}$.
\begin{Proposition}\label{prop:asy-w1}
Conditional on $E_0$, there exits a constant $C_2$ such that $W^{(1)}_T$ follows the asymptotic normality rate:

\begin{equation}\label{eq:asy-w1}
    \begin{aligned}
    \sup_{t\in\R}\abs{\PP(W^{(1)}_T>t|E_0 )-\Phi(-t)}\le C_2 h_n,
    \end{aligned}
\end{equation}
for any $T\in \cH_0$.
\end{Proposition}
This proposition implies the asymptotic symmetricity of ${W}^{(1)}_{T}$, ${W}^{(2)}_{T}$ conditioned on $E_0$, which is crucial for the following analysis. Since ${W}^{(1)}_{T}$, ${W}^{(2)}_{T}$ are asymptotically normal, the c.d.f. of their product $W_T^\mathsf{Rank}$ will converge to an asymptotically conditional symmetric random variable. We will show that, conditional on the splits $\cD_0$ and $\cD_1$, $W_T^\mathsf{Rank}$ is asymptotically symmetric for $T\in\cH_0$. Define 
\begin{equation*}
    G(t)= \frac{\sum_{T\in\cH_0 } \PP( {W}^{(1)}_{T} Z > t |\cD_0,\cD_1 )   }{q_0},
\end{equation*}
where $Z$ is a standard Gaussian variable. Here since ${W}^{(1)}_{T}\in\sigma(\cD_0,\cD_1)$, ${W}^{(1)}_{T}$ is fixed conditional on $\cD_0,\cD_1$.
% , which is the c.d.f. of the product of two independent standard normal variables. 
% Denote the p.d.f. of $\Psi(t)$ by $\psi(t)$. We have $\psi(t)= \frac{K_0(t)}{\pi}$, where $K_0(t)$ is the modified Bessel function of the second kind. By \cite{yang2017approximating}, we have the following order of $K_0(t)$ for $t>0$:

% \begin{equation}\label{eq:bessel-2}
%   \frac{\sqrt{\pi} e^{-t}}{\sqrt{2(t+\frac{1}{4})}}<K_0(t)<\frac{\sqrt{\pi} e^{-t}}{\sqrt{2t}}. 
% \end{equation}

% Therefore, it holds that

% \begin{equation}\label{eq:psi-order}
%  \sqrt{2} e^{\frac{1}{4}} \Phi (-\sqrt{2t+\frac{1}{2}})  <\Psi(-t)< \sqrt{2}\Phi(-\sqrt{2t}) . 
% \end{equation}
%$L_n= (1-\frac{\nu}{2})\log \frac{1}{h_n} $
Denote $L_n= G^{-1}( \frac{\epsilon_n \eta_n}{q_0})= \inf\left\{t:G(t)\le \frac{\epsilon_n \eta_n}{q_0}\right\}$, where $\epsilon_n$ is a rate to be specified later. We can exploit the following asymptotic symmetric property of ${W}^{(1)}_{T}$ to investigate the population version of the following $Ratio$:
\begin{equation*}
    Ratio=\frac{\sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank}} >L ) }{ \sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank}} <-L )}.
\end{equation*}
Here, we introduce a weaker characterization of strong signals, that is 
 \begin{equation}\label{eq:snr-strong}
        \cS = \left\{ T\in \cH: \frac{  \sqrt{n} \abs{M_T-\theta_T } }{ {\sigma_\xi \sqrt{d_1 d_2}\norm{\cP_M(T) }_{\tF} }  \sqrt{(\log q+\log d) } } \ge C_{\mathsf{gap} } \right\},
    \end{equation}   
    with $\eta_n=\abs{\cS}$ for some large constant $C_{\mathsf{gap} }$. In the following proof, we will actually focus on this definition of strong signals. This condition is actually weaker than in our main text, because $\norm{\cP_M(T)}_{\tF} \le \norm{T}_{\ell_1} \max_{i,j}\norm{ \cP_M(e_i e_j^\top) }_{\tF} \le 3 {\mu} \norm{T}_{\ell_1} \sqrt{\frac{r}{d_2} }$. Thus, all the signals that satisfy condition \eqref{eq:strong-T} can also satisfy condition \eqref{eq:snr-strong}, meaning that the $\eta_n$ defined here is always larger than that defined in \eqref{eq:strong-T}.

\begin{Lemma}\label{lemma:pop-wrank}  Conditional on $E_0$ and $\cD_1$, we have
% \begin{equation*}
%     \PP\left( \sup\limits_{0\le t \le L_n} \abs{\frac{ \sum_{T\in\cH_0}\PP(W_T>t ) }{q_0 (-t) } -1 }\ge \varepsilon \right) \le C_3 h_n^{\frac{\nu}{4}}
% \end{equation*}
\begin{equation*}
    \sup\limits_{0\le t \le L_n} \abs{\frac{ \sum_{T\in\cH_0}\PP(W_T^\mathsf{Rank} >t ) }{q_0 G(t) } -1 }\le C_3 \frac{h_n q_0}{\epsilon_n \eta_n }.
\end{equation*}
\end{Lemma}


\begin{proof}
% By \eqref{eq:psi-order} and Mill's ratio, $\Psi(-t)$ can be bounded by 
% $$\Psi(-L_n)\ge \sqrt{2} e^{\frac{1}{4}} \Phi (-\sqrt{2(1-\frac{\nu}{3})\log\frac{1}{h_n} })\ge \sqrt{2} e^{\frac{1}{4}}  \frac{h_n^{1-\frac{\nu}{3}} }{\sqrt{2(1-\frac{\nu}{3})\log\frac{1}{h_n} } +1}\ge\sqrt{2} e^{\frac{1}{4}} H^{1-\frac{\nu}{4}}_n,  $$
We only focus on small $h_n$. For each $T\in \cH_0$, conditional on $E_0$ and $\cD_1$, Proposition \ref{prop:asy-w1} implies that 
\begin{equation*}
    \begin{aligned}
    &\abs{\PP(W_T^\mathsf{Rank}>t )-\PP( {W}^{(1)}_{T} Z > t |E_0,\cD_1 )} \le C_2 h_n.
    \end{aligned}
\end{equation*}
% where $Z_1, Z_2$ are two i.i.d. Gaussian variables that are independent with other variables. Here we use the fact $\Psi(-t)=\PP(Z_1Z_2>t)$. 
The definition of $L_n$ also implies $G(t)\ge \frac{\epsilon_n \eta_n}{q_0}$.
Then, we can derive the following uniform bound of convergence:

\begin{equation*}
    \sup\limits_{0\le t \le L_n} \abs{\frac{ \sum_{T\in\cH_0}\PP(W_T^\mathsf{Rank}>t ) }{q_0 G(t) } -1 }\le \sup\limits_{0\le t \le L_n} \abs{\frac{ \sum_{T\in\cH_0}\PP(W_T^\mathsf{Rank}>t )-G(t) }{q_0 G(L_n) } } \le C_3 \frac{h_n q_0}{\epsilon_n \eta_n }.
\end{equation*}
\end{proof}

Then, we explore the weak dependency of linear forms under signals and correlations assumptions. We will show that, with high probability, the $Ratio$ can be very close to its population version described in Lemma \ref{lemma:pop-wrank}. Although we already have the intuition of dependency between different $W_T^{(1)}$ by Theorem \ref{thm:asymp-two-var}, the rate provided is not enough for FDR control based on $W_T^{\mathsf{Rank} }$. Here, we study the correlation of $W_T^{\mathsf{Rank} }$ between different $T$ with a more delicate analysis. Let $T_1$, and $T_2$ be two different indexing matrices in $\cH_0$. To this end, we introduce the following Lemma:

\begin{Lemma}[Weak dependency of null statistics]\label{lemma:weak-cov}
        Conditional on $E_0$, $\cD_1$, 
\begin{equation}
\begin{aligned}
          \sup_{0\le t\le L_n } \frac{\sum_{(T_i,T_j)\in \cH_{0,\text{weak} }^2 } \abs{\operatorname{cov}(\bbI(W_{T_i}^{\mathsf{Rank} }>t),\bbI(W_{T_j}^{\mathsf{Rank} }>t)) }}{ q_0^2 G^2(t)}\le  C_1 \frac{h_n q_0}{\epsilon_n \eta_n }+C_2 \frac{1}{ \left(\epsilon_n\eta_n q_0\right)^{\nu/2} },
\end{aligned}
\end{equation}
where $\nu$ is the weak correlation parameter defined in \eqref{eq:weak-corr-1}.
\end{Lemma}
\begin{proof}
Suppose we have a pair $(T_1,T_2)\in \cH_{0,\text{weak} }^2$. Here we adopt the notation in the proof of Theorem \ref{thm:asymp-normal-varest}: denote 
$$
\begin{aligned}
     (W_{T_1}^{(i)},W_{T_2}^{(i)}) &= \left(\frac{\left\langle  \widehat{Z}_1^{(i)} , \cP_M(T_1) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T_1) }_\tF  \sqrt{d_1 d_2/n}}+ \Delta_{T_1}^{(i)}, \frac{\left\langle  \widehat{Z}_1^{(i)} , \cP_M(T_2) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T_2) }_\tF  \sqrt{d_1 d_2/n}}+ \Delta_{T_2}^{(i)} \right) \\
     &:= \left(\widetilde{W}^{(i)}_{T_1}+\Delta_{T_1}^{(i)}  , \widetilde{W}^{(i)}_{T_2}+\Delta_{T_2}^{(i)} \right),\ i=1,2,
\end{aligned}
$$
where $\widehat{Z}_1^{(i)} $, $\Delta^{(i)}_T$ are defined analogously as in the proof of Theorem \ref{thm:asymp-normal-varest}. We have $\E (\widetilde{W}^{(1)}_T)^2=\E (\widetilde{W}^{(2)}_T)^2=1 $. Here $\widetilde{W}^{(1)}_T$ and $\widetilde{W}^{(2)}_T$ are standardized averages of $n$ i.i.d. samples and can be regarded as the cores which lead to asymptotic normality of $W^{(1)}_T$, $W^{(2)}_T$. By the proof of Theorem \ref{thm:asymp-normal}, the remainder term $ \Delta^{(i)}_T$ is controlled by:
\begin{equation}\label{eq:remainder}
    \PP\left(\abs{\Delta^{(i)}_T}>c_1 h_n | E_0\right)\le C_2 \frac{\log d_1}{d_1^2}.
\end{equation}
% $\widetilde{W}^{(1)}_{T_1}$ and $\widetilde{W}^{(1)}_{T_2}$
For $i=1$, as is shown in the proof of Theorem \ref{thm:asymp-normal-varest},  by multivariate Berry--Esseen theorem,   $(\widetilde{W}^{(2)}_{T_1},\widetilde{W}^{(2)}_{T_2})$  converges to normal variable $\omega_1 \sim \cN(0,R)$ conditional on $E_0$ where $R_{11}=R_{22}=1$ and $R_{12}=R_{21}=\operatorname{cov}(\widetilde{W}^1_{T_1},\widetilde{W}^1_{T_2})=\rho_{T_1,T_2}$ with the error bound:

\begin{equation}\label{eq:multi-be}
    \begin{aligned}
    \abs{\PP\left((\widetilde{W}^{(2)}_{T_1},\widetilde{W}^{(2)}_{T_2})\in {A} \middle| E_0 \right)-\PP(\omega_1\in{A})}\le C \mu \sqrt{\frac{r d_1}{n}},
    \end{aligned}
\end{equation}
for any convex set ${A} \subseteq \R^2$. Here the $\rho:=\rho_{T_1,T_2}$ is the correlation between $W^{(1)}_T$, $W^{(2)}_T$ defined in \eqref{eq:corr}. By the following calculation of the covariance between $\bbI(W_{T_1}^{\mathsf{Rank} }>t)$ and $\bbI(W_{T_2}^{\mathsf{Rank} }>t)$ conditional on $E_0$ and $\cD_1$, we have:
\begin{equation*}
\begin{aligned}
     &\abs{\operatorname{cov}(\bbI(W_{T_1}^{\mathsf{Rank} }>t),\bbI(W_{T_2}^{\mathsf{Rank} }>t)) } \\
     & = \abs{\PP(W^{(1)}_{T_1}W^{(2)}_{T_1} >t, W^{(1)}_{T_2}W^{(2)}_{T_2}>t )- \PP(W^{(1)}_{T_1}W^{(2)}_{T_1} >t )\PP(W^{(1)}_{T_2}W^{(2)}_{T_2} >t ) } \\
     & \le \abs{\PP(W^{(1)}_{T_1}W^{(2)}_{T_1} >t, W^{(1)}_{T_2}W^{(2)}_{T_2}>t )- \PP( {W}^{(1)}_{T_1} w_{11} > t, {W}^{(1)}_{T_2}w_{12} > t  )} \text{ (1)}\\ &+\abs{\PP(W^{(1)}_{T_1}W^{(2)}_{T_1} >t )\PP(W^{(1)}_{T_2}W^{(2)}_{T_2} >t )- \PP( {W}^{(1)}_{T_1} w_{11} > t  )\PP( {W}^{(1)}_{T_2} w_{12} > t  ) } \text{ (2)}\\
     &+\abs{\PP( {W}^{(1)}_{T_1} w_{11} > t, {W}^{(1)}_{T_2}w_{12} > t  )- \PP( {W}^{(1)}_{T_1} w_{11} > t  )\PP( {W}^{(1)}_{T_2} w_{12} > t  ) } \text{ (3)}.\\
    %  &\le \abs{\PP(W^{(1)}_{T_1}W^{(2)}_{T_1} >t, W^{(1)}_{T_2}W^{(2)}_{T_2}>t )- \Psi(-t)^2} + C\Psi(-t)h_n+ Ch_n^2,\\
\end{aligned}
\end{equation*}
Term (1), (2), (3) can be controlled separately. For (1), conditional on $E_0$ and $\cD_1$, we invoke multivariate Berry--Esseen theorem \eqref{eq:multi-be} to bound the joint c.d.f. of $(W^{(2)}_{T_1}, W^{(2)}_{T_2})$ by  
%\widetilde{W}^1_{T_1} >t_1, \widetilde{W}^1_{T_2}>t_2|E_0
\begin{equation*}
    \begin{aligned}
    &\PP(W^{(2)}_{T_1} >t_1, W^{(2)}_{T_2}>t_2 ) \\
    &\le  \PP(W^{(2)}_{T_1} >t_1, W^{(2)}_{T_2}>t_2,\abs{\Delta^{(2)}_{T_1}}\le c_1 h_n,\abs{\Delta^{(2)}_{T_2}}\le c_1 h_n  ) +\frac{2c_2 \log d_1}{d_1^2}\\ %+h_n^2
    &\le \PP(\widetilde{W}^{(2)}_{T_1} >t_1-c_1 h_n, \widetilde{W}^{(2)}_{T_2}>t_2-c_1 h_n) +\frac{2c_2 \log d_1}{d_1^2} \\
    &\le \PP(\omega_{11}>t_1,\omega_{12}>t_2 ) +c_1\left[\phi(t_1)\PP(\omega_{12}>t_2|\omega_{11}=t_1) +\phi(t_2)\PP(\omega_{11}>t_1|\omega_{12}=t_2)\right]h_n +C_2 h_n^2,
    \end{aligned}
\end{equation*}
where we apply Taylor expansion to the c.d.f. of normal distribution $\omega_1 \sim \cN(0,R)$ and apply the upper bound $\log d_1 \cdot d_1^{-2} \le h_n^2$. Analogously, it also holds that
\begin{equation*}
    \begin{aligned}
    &\PP(W^{(2)}_{T_1} >t_1, W^{(2)}_{T_2}>t_2)\ge \PP(\widetilde{W}^{(2)}_{T_1} >t_1+c_1 h_n, \widetilde{W}^{(2)}_{T_2}>t_2+c_1 h_n) -\frac{2c_2 \log d_1}{d_1^2} \\
    &\ge \PP(\omega_{11}>t_1,\omega_{12}>t_2 ) -c_1\left[\phi(t_1)\PP(\omega_{12}>t_2|\omega_{11}=t_1) +\phi(t_2)\PP(\omega_{11}>t_1|\omega_{12}=t_2)\right]h_n -C_2 h_n^2.
    \end{aligned}
\end{equation*}

We conclude that, conditional on $E_0$ and $\cD_1$
\begin{equation*}
\begin{aligned}
&\abs{\PP(W^{(2)}_{T_1} >t_1, W^{(2)}_{T_2}>t_2 )-\PP(\omega_{11}>t_1,\omega_{12}>t_2 ) }\\
& \le c_1\left[\phi(t_1)\PP(\omega_{12}>t_2|\omega_{11}=t_1) +\phi(t_2)\PP(\omega_{11}>t_1|\omega_{12}=t_2)\right]h_n + C_2 h_n^2.
\end{aligned}
\end{equation*}
% Following a similar argument in the proof of Lemma \ref{lemma:pop-wrank}, it is clear that
% \begin{equation*}
%     \begin{aligned}
%     &\abs{\PP\left(W^{(1)}_{T_1}W^{(2)}_{T_1} >t, W^{(1)}_{T_2}W^{(2)}_{T_2}>t |E_0\right)- \PP\left(\omega_{11}W^{(2)}_{T_1}>t, \omega_{12}W^{(2)}_{T_2}>t |E_0\right)} \\
%     &\le  c_1\left[\PP(\omega_{11}W^{(2)}_{T_1}>t, \omega_{12}W^{(2)}_{T_2}=t|E_0 )+\PP(\omega_{12}W^{(2)}_{T_2}>t, \omega_{11}W^{(2)}_{T_1}=t|E_0 )\right]\cdot h_n+C_2 h_n^2\\
%     &\le c_1\left[\PP(\omega_{11}W^{(2)}_{T_1}>t|E_0 )+\PP(\omega_{12}W^{(2)}_{T_2}>t |E_0)\right]\cdot h_n+C_2 h_n^2.
%     \end{aligned}
% \end{equation*}


% Note that $(\widetilde{W}^{(2)}_{T_1},\widetilde{W}^{(2)}_{T_2})$ conditional on $E_0$ also converges to normal variable, denoted by $\omega_2 \sim \cN(0,\Sigma)$. We then have
% \begin{equation*}
%     \begin{aligned}
%     \PP(\omega_{11}W^{(2)}_{T_1}>t|E_0 )+\PP(\omega_{12}W^{(2)}_{T_2}>t |E_0)\le 2\PP(\omega_{11}\omega_{21}>t )+C h_n = 2\Psi(-t)+C h_n,
%     \end{aligned}
% \end{equation*}
Using the Lipschitz property of $\Phi(t)$, we have 
\begin{equation}\label{eq:bound-w-omega}
\begin{aligned}
         &\abs{\PP(W^{(1)}_{T_1}W^{(2)}_{T_1} >t, W^{(1)}_{T_2}W^{(2)}_{T_2}>t )- \PP(W^{(1)}_{T_1}\omega_{11}>t, W^{(1)}_{T_2}\omega_{12}>t )} \\
         &\le 2c_1 h_n\left(\PP(W^{(1)}_{T_1}\omega_{11}>t) +\PP(W^{(1)}_{T_2}\omega_{12}>t)  \right) +Ch_n^2.
\end{aligned}
\end{equation}
For (2), the proof of Lemma \ref{lemma:pop-wrank} also implies the following bound 
\begin{equation}\label{eq:bound-w-omega-2}
\begin{aligned}
        &\abs{\PP(W^{(1)}_{T_1}W^{(2)}_{T_1} >t )\PP(W^{(1)}_{T_2}W^{(2)}_{T_2} >t )- \PP( {W}^{(1)}_{T_1} w_{11} > t  )\PP( {W}^{(1)}_{T_2} w_{12} > t  ) } \\
        &\le 2c_1 h_n\left(\PP(W^{(1)}_{T_1}\omega_{11}>t) +\PP(W^{(1)}_{T_2}\omega_{12}>t)  \right) +Ch_n^2, 
\end{aligned}
\end{equation}
by the same argument conditional on $E_0$ and $\cD_1$. Our next step is to compare the c.d.f. of $(\omega_1,\omega_2)$ with standard Gaussian $(Z_1,Z_2)$ to control the term (3), the difference between $\PP(\omega_{11} >t_1, \omega_{12}>t_2)$ and $\Phi(-t_1)\Phi(-t_2) $. Since $(T_1,T_2)\in \cH_{0,\text{weak} }^2$, the covariance between $w_{11},w_{12}$ is thus bounded by: $\abs{\rho}\le c q_0^{-\nu}$.
% $$
% \begin{aligned}
% \operatorname{cov}(\widetilde{W}^1_{T_1},\widetilde{W}^1_{T_2}) 
% & =\E\widetilde{W}^1_{T_1}\widetilde{W}^1_{T_2}=\E\frac{d_1 d_2 \sum_{i\in I_2} \xi_i^2 \prod\limits_{k=1}^{2} \left(\left\langle U_{\perp} U_{\perp}^{\top} X_i V V^{\top}, T_k\right\rangle+\left\langle U U^{\top} X_i V_{\perp} V_{\perp}^{\top}, T_k\right\rangle\right)  }{\delta_{T_1}\delta_{T_2}   n} \\
% & = \frac{\sum_{i\in [d_1]}\sum_{j\in [d_2]} \prod\limits_{k=1}^{2}\left\langle U_{\perp} U_{\perp}^{\top} e_i e_j^\top V V^{\top}, T_k\right\rangle+ \prod\limits_{k=1}^{2} \left\langle U U^{\top} e_i e_j^\top V_{\perp} V_{\perp}^{\top}, T_k\right\rangle  }{\left\|V^\top T^\top U_{\perp}\right \|_{\mathrm{F}}^2+\left\|U^{\top} TV_{\perp}\right\|_{\mathrm{F}}^2}
% \end{aligned}
% $$

% Since 
% \begin{equation*}
%      \left\langle U_{\perp} U_{\perp}^{\top} e_i e_j^\top V V^{\top}, T_1\right\rangle \left\langle U_{\perp} U_{\perp}^{\top} e_i e_j^\top V V^{\top}, T_2\right\rangle =  e_j^\top V V^{\top} T_1^\top U_{\perp}  U_{\perp}^{\top} e_i \cdot e_i\top U_{\perp}  U_{\perp}^{\top} T_1 V V^{\top}e_j,
% \end{equation*}
% By summing from $i\in[d_1]$ and $j\in [d_2]$, we have

% \begin{equation*}
% \begin{aligned}
%      \sum_{i\in [d_1]}\sum_{j\in [d_2]} \left\langle U_{\perp} U_{\perp}^{\top} e_i e_j^\top V V^{\top}, T_1\right\rangle \left\langle U_{\perp} U_{\perp}^{\top} e_i e_j^\top V V^{\top}, T_2\right\rangle &=  \tr( V V^{\top} T_1^\top U_{\perp}  U_{\perp}^{\top} T_2 V V^{\top})\\
%      & = -\tr( V V^{\top} T_1^\top U  U^{\top} T_2 V V^{\top}) \\
%      & = -\left\langle U^{\top} T_1 V  , U^{\top} T_2 V \right\rangle.
% \end{aligned}
% \end{equation*}
% And also
% \begin{equation*}
% \begin{aligned}
%      \sum_{i\in [d_1]}\sum_{j\in [d_2]} \left\langle U U^{\top} e_i e_j^\top V_{\perp} V_{\perp}^{\top}, T_1\right\rangle \left\langle U U^{\top} e_i e_j^\top V_{\perp} V_{\perp}^{\top}, T_2\right\rangle = -\left\langle U^{\top} T_1 V  , U^{\top} T_2 V \right\rangle
% \end{aligned}
% \end{equation*}
% analogously. By Assumption \ref{asp:incoherence} and \ref{asp:alignment}, the following inequality holds:
% $$\begin{aligned}
% \frac{\left\|V^{\top} T^{\top} U\right\|_\tF^2+\left\|U^{\top} T V\right\|_\tF^2}{\|T V\|_\tF^2+\left\|U^{\top} T\right\|_{\mathrm{F}}^2} \leq \frac{\mu^4\|T\|_{\ell_1}^2}{\beta^2\|T\|_{\mathrm{F}}^2} \cdot \frac{r}{d_2}.
% \end{aligned}$$
% \begin{equation*}
%   \abs{\rho}=\abs{\operatorname{cov}(\widetilde{W}^1_{T_1},\widetilde{W}^1_{T_2}) } \le C\frac{\mu^2 r^2 }{d_2}
% \end{equation*}
% Applying Mehler's identity \citep{kotz2000bivariate}:
% \begin{equation*}
% \Phi_\omega\left(t_1, t_2\right)=\Phi\left(t_1\right) \Phi\left(t_2\right)+\sum_{n=1}^{\infty} \frac{\rho^n}{n !} \phi^{(n-1)}\left(t_1\right) \phi^{(n-1)}\left(t_2\right),
% \end{equation*}
% and Lemma 1 in \cite{azriel2015empirical}:
% $$\sum_{n=1}^{\infty} \frac{\left[\sup _{t \in \mathbb{R}} \phi^{(n-1)}(t)\right]^2}{n !}<\infty,$$
% it holds that

% $$\sup_{t_1,t_2} \abs{\PP(\omega_{11}>t_1,\omega_{12}>t_2)-\PP(Z_{11}>t_1,Z_{12}>t_2)}\le C \rho/(1-\rho),$$
% which leads to the following decoupling:
% \begin{equation}\label{eq:omega-to-sd}
% \begin{aligned}
%           &\abs{\PP(\omega_{11}\omega_{21} >t, \omega_{12}\omega_{22}>t) -\Psi^2(-t)} \\
%           &\le \abs{\PP(\omega_{11}\omega_{21} >t, \omega_{12}\omega_{22}>t) -\PP(Z_{11}\omega_{21} >t, Z_{12}\omega_{22}>t)}+\abs{\PP(Z_{11}\omega_{21} >t, Z_{12}\omega_{22}>t) -\Psi^2(-t)}\\
%           &\le C\rho/(1-\rho) \le C h_n^2.
% \end{aligned}
% \end{equation}
% Here  $(Z_1,Z_2)$ are standard Gaussian variables both from $\R^2$.

We invoke the property of bivariate Gaussian copula \citep{meyer2013bivariate}:
\begin{equation*}
    \begin{aligned}
         \abs{\PP(\omega_{11} >t_1, \omega_{12}>t_2)-\Phi(-t_1)\Phi(-t_2) }=\abs{\int_{0}^{\rho} \phi_2(-t_1,-t_2,z ) dz},
    \end{aligned}
\end{equation*}
where $ \phi_2(x,y,z )$ is the p.d.f of bivariate normal distribution with correlation coefficient $z$. Without loss of generality, assume $t_1,t_2>0$ are away from $0$. Thus, it is clear that
\begin{equation*}
    \begin{aligned}
         \abs{\PP(\omega_{11} >t_1, \omega_{12}>t_2)-\Phi(-t_1)\Phi(-t_2) } &\le \int_{0}^{\rho } \phi_2(-t_1,-t_2,z ) dz, \\
         & \le \frac{\rho}{2 \pi \sqrt{1-\rho^2}} \exp \left(-\frac{t_1^2+t_2^2}{2}+\frac{\rho t_1 t_2}{\left(1-\rho^2\right)}\right) \\
         & \le \frac{2\rho}{2 \pi }  \exp \left(-\frac{t_1^2+t_2^2}{2}(1-c\rho)\right) \\
         & = 2\rho \left[\phi(-t_1)\phi(-t_2)\right]^{1-c\rho}.
    \end{aligned}
\end{equation*}
For any $\nu>0$, there exist $C_\nu>0$ such that $\Phi(-t)^\nu \le C_\nu/t $ for all $t>0$. Because by Mill's ratio, we have:
\begin{equation*}
    \Phi(-t)^\nu \le \frac{\phi (-t)^\nu}{t^\nu}\le C_\nu \frac{1}{t^{1-\nu}}\frac{1}{t^\nu}=C_\nu \frac{1}{t},
\end{equation*}
where we use the fact that $\phi (-t)^\nu\le C_\nu t^{-(1-\nu)} $. Now combine this with the upper bound of $\phi (-t)$: $\phi (-t)\le C (t+1)\Phi (-t) $ , we have:
\begin{equation}\label{eq:bound-w-omega-3}
    \begin{aligned}
         \abs{\PP(\omega_{11} >t_1, \omega_{12}>t_2)-\Phi(-t_1)\Phi(-t_2) } &\le 2\rho \left[\phi(-t_1)\phi(-t_2)\right]^{1-c\rho} \\
         & \le 2\rho\left[ C (\Phi(-t_1)^{-\nu}+1 )\Phi(-t_1) (\Phi(-t_2)^{-\nu}+1 )\Phi(-t_2) \right]^{1-c\rho} \\
         & \le C\rho \left[\Phi(-t_1)\Phi(-t_2)\right]^{(1-\nu)(1-c\rho) },
    \end{aligned}
\end{equation}
for the term (3). Together with \eqref{eq:bound-w-omega}, \eqref{eq:bound-w-omega-2}, we can show that
\begin{equation*}
\begin{aligned}
       &\sup_{0\le t\le L_n } \sum_{(T_i,T_j)\in \cH_{0,\text{weak} }^2 }\frac{ \abs{\operatorname{cov}(\bbI(W_{T_i}^{\mathsf{Rank} }>t),\bbI(W_{T_j}^{\mathsf{Rank} }>t)) }}{q_0^2 G^2(t)}\le  \frac{8c_1 h_n q_0 G(t)  }{q_0^2 G(t)^2} \\
       & +\sup_{0\le t\le L_n }\frac{ \sum_{(T_i,T_j)\in\cH_{0,\text{weak}} ^2 } C\rho \left[\PP({W}^{(1)}_{T_i} Z >t )\PP({W}^{(1)}_{T_j} Z>t)\right]^{(1-\nu)(1-c\rho) }  }{q_0^2 G(t)^2}\\
       & \le  C \frac{h_n q_0}{\epsilon_n \eta_n } + \sup_{0\le t\le L_n }\frac{ C\rho \left(\sum_{T\in \cH_0 }\PP({W}^{(1)}_{T} Z >t )^{(1-\nu)(1-c\rho) }\right)^2  }{q_0^2 G(t)^2} \\ 
       & \le  C \frac{h_n q_0}{\epsilon_n \eta_n } + \sup_{0\le t\le L_n }C\rho \frac{ \left( G(t) \right)^{2(1-\nu)(1-c\rho) }  }{ G(t)^2} \\
       & \le  C \frac{h_n q_0}{\epsilon_n \eta_n } +C {\rho}( \frac{q_0}{\epsilon_n\eta_n})^{3\nu}.
\end{aligned}
\end{equation*}
The argument above is valid for any $\nu$, thus, we choose $3\nu$ to be the $\frac{\nu}{2}$, where $\nu$ is defined in \eqref{eq:weak-corr-1}. It thus finishes the proof.
\end{proof}
We now apply the weak dependency yielded in Lemma \ref{lemma:weak-cov} to derive a uniform bound between $R$ and its population version: 

\begin{Lemma}\label{lemma:conv-prob}
For any $\varepsilon>0$, conditional on $E_0$ and $\cD_1$, it holds that
\begin{equation*}
   \PP\left(\sup\limits_{0\le t\le L_n}\abs{\frac{ \sum_{T\in\cH_0}\bbI(W_T^{\mathsf{Rank} }>t ) }{q_0 G(t) } -1} \ge \varepsilon\right) \le \frac{C}{\varepsilon^2} \log(\frac{q_0 }{\epsilon_n \eta_n}) \left( \left(\frac{\beta_{\mathsf{s}} q_0^2 }{ \epsilon_n^2\eta_n^2}\right)^{\frac{1}{2}} + \left(\frac{h_n q_0}{\epsilon_n \eta_n}+ \frac{1}{ \left(\epsilon_n\eta_n q_0\right)^{v/2} } \right)^{\frac{1}{2}}\right) .
\end{equation*}
\end{Lemma}

\begin{proof}
To prove the uniform convergence in probability, we define a grid on $[0,L_n]$:
%with the same spirit as \cite{du2021false}: 
$$\left\{ t_k= G^{-1}\left( \frac{1}{2} (2G(L_n))^{\frac{k}{K}} \right) \right\}_{k=0}^{K},  $$
which equates each $G(t_k)$ with $\frac{1}{2} (2G(L_n))^{\frac{k}{K}}$. Then for each $t\in [t_{k-1},t_{k})$, the ratio can be bounded by:
\begin{equation*}
    \frac{ \sum_{T\in\cH_0}\bbI(W_T^{\mathsf{Rank} }>t_{k} ) }{q_0 G(t_{k-1}) } \le \frac{ \sum_{T\in\cH_0}\bbI(W_T^{\mathsf{Rank} }>t ) }{q_0 G(t) } \le  \frac{ \sum_{T\in\cH_0}\bbI(W_T^{\mathsf{Rank} }>t_{k-1} ) }{q_0 G(t_{k}) }.
\end{equation*}
Define $(2G(L_n))^{\frac{1}{K}}=r_K$, we have $G(t_k)/G(t_{k-1})=r_K$, and 
\begin{equation*}
    \abs{\frac{ \sum_{T\in\cH_0}\bbI(W_T^{\mathsf{Rank} }>t ) }{q_0 G(t) } -1}\le \sup_{i=k-1,k}\frac{1}{r_K} \abs{\frac{ \sum_{T\in\cH_0}\bbI(W_T^{\mathsf{Rank} }>t_i ) }{q_0 G(t_{i}) }-1} +\abs{r_K-1}\vee\abs{\frac{1}{r_K}-1 },
\end{equation*}
for each $t\in [t_{k-1},t_{k})$. Then for any $t\in [0,L_n]$, it suffices to control the quantities 
\begin{equation*}
    \sup_{k=0,\dots, K}\frac{1}{r_K} \abs{\frac{ \sum_{T\in\cH_0}\bbI(W_T^{\mathsf{Rank} }>t_k ) }{q_0 G(t_{k}) }-1}\le  \sup_{k=0,\dots, K}\frac{1}{r_K} \abs{\frac{ \sum_{T\in\cH_0}\bbI(W_T^{\mathsf{Rank} }>t_k )-\PP(W_T^{\mathsf{Rank} }>t_k ) }{q_0 G(t_{k}) }} + C_3 \frac{h_n q_0}{\epsilon_n \eta_n }
\end{equation*}
and $\abs{r_K-1}\vee\abs{\frac{1}{r_K}-1 }$ by Proposition \ref{prop:asy-w1}.  Denote 
$$D_n=\sup\limits_{k=0,\dots, K}\frac{1}{r_K} \abs{\frac{ \sum_{T\in\cH_0}\bbI(W_T^{\mathsf{Rank} }>t_k )-\PP(W_T^{\mathsf{Rank} }>t_k ) }{q_0 G(t_{k}) }}.$$ 
It follows that
\begin{equation}
    \begin{aligned}
              &\E D_n^2\le \frac{K}{r_K^2}\E \abs{\frac{ \sum_{T\in\cH_0}\bbI(W_T^{\mathsf{Rank} }>t_{k} )-\PP(W_T^{\mathsf{Rank} }>t_{k} ) }{q_0 G(t_{k}) }}^2\\
              &\le  \frac{K}{r_K^2} \frac{\sum\limits_{(T_1,T_2)\in\cH^2_{0,\text{weak}}} \abs{\operatorname{cov}(\bbI(W_{T_1}^{\mathsf{Rank} }>t),\bbI(W_{T_2}^{\mathsf{Rank} }>t)) } +\sum\limits_{T_1,T_2\in\cH^2_{0,\text{strong}} } \abs{\operatorname{cov}(\bbI(W_{T_1}^{\mathsf{Rank} }>t),\bbI(W_{T_2}^{\mathsf{Rank} }>t)) } }{q_0^2 G^2(t) },
    \end{aligned}
\end{equation}
for any $t\in \{t_k\}$. Since the number of strong dependency pairs $\abs{\cH^2_{0,\text{strong}}}\le \beta_{\mathsf{s}}q_0^2 $, we have
\begin{equation*}
    \frac{\sum\limits_{T_1,T_2\in\cH^2_{0,\text{strong}} } \abs{\operatorname{cov}(\bbI(W_{T_1}^{\mathsf{Rank} }>t),\bbI(W_{T_2}^{\mathsf{Rank} }>t)) }}{q_0^2 G^2(t) }\le \frac{\beta_{\mathsf{s}}q_0^2 }{ \epsilon_n^2\eta_n^2},
\end{equation*}
for any $t\in [0,L_n]$. For the weak dependency pair, 

\begin{equation*}
    \frac{\sum\limits_{T_1,T_2\in\cH^2_{0,\text{weak}} } \abs{\operatorname{cov}(\bbI(W_{T_1}^{\mathsf{Rank} }>t),\bbI(W_{T_2}^{\mathsf{Rank} }>t)) }}{q_0^2 G^2(t)}\le C_1 \frac{h_n q_0}{\epsilon_n \eta_n }+C_2 \frac{1}{ \left(\eta_n q_0\right)^{v/2} },
\end{equation*}
 where we apply our previous results in Lemma \ref{lemma:weak-cov}. What remains for us is to specify the density of grid $K$. Choose a constant $\varsigma$ and we set
 $$K= \log(\frac{q_0 }{\epsilon_n \eta_n}) \min \left\{ \left(\frac{ q_0^2 \beta_{\mathsf{s}} }{ \eta_n^2 \epsilon_n  }\right)^{-\varsigma} , \left( \frac{q_0 h_n}{\eta_n\epsilon_n}+\frac{1}{ \left(\epsilon_n\eta_n q_0\right)^{v/2} } \right)^{-\varsigma} \right\}  ,$$ 
 then it is clear that $1\le\frac{1}{r_k}\le \left[ \frac{q_0}{\epsilon_n \eta_n} \right]^{1/K}\to 1$, and $ K (\frac{\beta_{\mathsf{s}}q_0^2 }{ \epsilon_n^2\eta_n^2} + \frac{h_n q_0}{\epsilon_n \eta_n })\to 0$. Therefore 
 \begin{equation*}
\begin{aligned}
      &\abs{r_K-1}\vee\abs{\frac{1}{r_K}-1 } \le C \frac{1}{K}\log(\frac{q_0 }{\epsilon_n \eta_n}) \le  \left( \left(\frac{\beta_{\mathsf{s}} q_0^2 }{ \epsilon_n^2\eta_n^2}\right)^{\varsigma} + \left(\frac{h_n q_0}{\epsilon_n \eta_n} +\frac{1}{ \left(\epsilon_n\eta_n q_0\right)^{v/2} }\right)^{\varsigma}\right) \\
    &\E D^2_n  \le C K \left(\frac{\beta_{\mathsf{s}}q_0^2 }{ \eta_n^2} + \frac{h_n q_0}{\epsilon_n \eta_n} \right)\le C \log(\frac{q_0 }{\epsilon_n \eta_n}) \left( \left(\frac{\beta_{\mathsf{s}} q_0^2 }{ \epsilon_n^2\eta_n^2}+\frac{1}{ \left(\epsilon_n\eta_n q_0\right)^{v/2} }\right)^{1-\varsigma} + \left(\frac{h_n q_0}{\epsilon_n \eta_n} \right)^{1-\varsigma}\right).
\end{aligned}
 \end{equation*}
We can finish the proof of uniform convergence by using Markov's inequality with $\varsigma=\frac{1}{2}$.
\end{proof}

Recall the main theorem. For the ratio $\frac{\sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank} } >L ) }{ \sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank} } <-L )}$, we have

\begin{equation*}
    Ratio=\frac{\sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank} } >L ) }{ \sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank} } <-L )} =  \frac{\sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank} } >L ) }{ q_0 G(t) } \cdot \frac{q_0 G(t) }{  \sum_{T\in \cH_0  } \bbI(W_T^{\mathsf{Rank} } <-L ) }.
\end{equation*}
Then, it's clear that, under the event that $L\le L_n$, if Lemma \ref{lemma:conv-prob} holds for a $\varepsilon$, then we have

\begin{equation*}
     Ratio\le \frac{1+\varepsilon}{1-\varepsilon}\le 1+\frac{2\varepsilon}{1-\varepsilon}\le 1+3\varepsilon,
\end{equation*}
with probability at least $1-\frac{C}{\varepsilon^2} \log(\frac{q_0 }{\epsilon_n \eta_n}) \left( \left(\frac{\beta_{\mathsf{s}} q_0^2 }{ \epsilon_n^2\eta_n^2}\right)^{\frac{1}{2}} + \left(\frac{h_n q_0}{\epsilon_n \eta_n}+(\epsilon_n\eta_n q_0)^{-\nu/2} \right)^{\frac{1}{2}}\right) $. 
By Lemma \ref{lemma:conv-prob}, we now successfully reduce our problem to proving our data-driven threshold $L\le L_n$ with high probability.

\subsubsection{ Threshold control}\label{sec:prof-thres}
 The gist of asymptotic threshold control is that when we choose $L_n$ as the threshold and $d_1,d_2,n$ go large, entries with strong signals in $\cS$ can always pass the test, and other entries with weak signals or no signal can pass the test will little possibility. We first focus on the entries with strong signals. Denote the standardized signal $\delta_T=(M_T-\theta_T)/({\sigma}_{\xi}\|\calP_M(T)\|_{\rm F}\sqrt{d_1d_2/n} )$, and $\widehat{W}^1_T= W^{(1)}_T- \delta_T$, $\widehat{W}^2_T= W^{(2)}_T- \delta_T$. Given any $T\in\cH_1$, following the argument that is similar to the proof in Lemma \ref{lemma:pop-wrank}, conditional on $E_0$, we have 
 \begin{equation*}
    \sup_{t\in \R}\abs{ \PP(W_T^\mathsf{Rank}>t )- \PP\left( (Z_1+\delta_T)(Z_2+\delta_T)>t \right) }\le C h_n.
 \end{equation*}
% For any $t>0$  and $\delta_T\in\R$, it is clear that $\PP\left( (Z_1+\delta_T)(Z_2+\delta_T)<-t \right)\le \PP\left( Z_1 Z_2<-t \right)$. 
Here $(Z_1,Z_2)$ is standard Gaussian. Without loss of generality, assume $M_T-\theta_T>0$. Then, 
 \begin{equation*}
\begin{aligned}
\PP(W_T^\mathsf{Rank}< L_n ) \le &\PP\left( (Z_1+\delta_T)(Z_2+\delta_T)< L_n \right) +C h_n \\
\le & 1- \PP\left( (Z_1+\delta_T)(Z_2+\delta_T) \ge L_n \right) +C h_n \\
\le & 1- \PP\left( Z_1\ge -\delta_T+\sqrt{L_n} \right)^2 +C h_n.
% \PP(W_T< L_n)&\le \PP(W^{(1)}_T<  \sqrt{L_n})+\PP(W^{(2)}_T<  \sqrt{L_n})\le 2\PP(W_T^1< \sqrt{L_n}|E_0) + Ch_n \\
% & =  2\PP(\widehat{W}_T^1+\delta^1_T< \sqrt{L_n}|E_0) + Ch_n
\end{aligned}
 \end{equation*}
Here, we use the fact that
\begin{equation*}
    \left\{ Z_1\ge -\delta_T+\sqrt{L_n} \right\} \cap \left\{ Z_2\ge -\delta_T+\sqrt{L_n} \right\} \subseteq \left\{(Z_1+\delta_T)(Z_2+\delta_T) \ge L_n \right\}.
\end{equation*}
%  From the proof of Theorem \ref{thm:asymp-normal}, $\PP\left(\abs{\frac{\sigma_T}{\widehat{\sigma}_T} -1}\ge c h_n |E_0\right)\le \frac{c\log d_1}{d_1^2}$. Thus, applying Assumption \ref{asp:signals-fdr} we can ensure that $$\frac{\delta^1_T}{\sqrt{L_n}}=\frac{M_T-\theta_T}{\sigma^1_T \sqrt{L_n}}\cdot\frac{\sigma^1_T}{\widehat{\sigma}_T}\ge 1+ \sqrt{2/(1-\frac{\nu}{2})} $$
%  happens with probability at least $1-\frac{c\log d_1}{d_1^2}$. We continue to control $\PP(W_T< L_n)$ by
 
%   \begin{equation*}
% \begin{aligned}
% \PP(W_T< L_n)&\le  2\PP(\widehat{W}_T^1< - \sqrt{2/(1-\frac{\nu}{2})}\sqrt{L_n}|E_0) + Ch_n \\
% & \le 2\Phi(-\sqrt{2 \log \frac{1}{h_n}} ) +Ch_n \le Ch_n
% \end{aligned}
%  \end{equation*}
An upper bound of $G(t)$ is given by 
\begin{equation*}
    G(t)= \frac{\sum_{T\in\cH_0 } \PP( {W}^{(1)}_{T} Z > t |\cD_0,\cD_1 )   }{q_0}\le \frac{\sqrt{2}}{\sqrt{\pi}} \exp{\left( -\frac{t^2}{ 2\max_{T\in\cH_0} \abs{{W}^{(1)}_{T} }^2 } \right)}.
\end{equation*}
From Theorem \ref{thm:asymp-normal}, an uniform upper bound of $\abs{{W}^{(1)}_{T} }$ is given by:
\begin{equation*}
    \PP\left(\max_{T\in\cH_0}\abs{{W}^{(1)}_{T} } \ge C(h_n +\sqrt{\log d_1 +\log q}) \middle|\cD_0  \right)\le \frac{1}{d_1^2}.
\end{equation*}
If $T\in \cS$, then $\delta_T\ge C_{\mathsf{gap}} \sqrt{\log d_1 +\log q} $ by the definition of $\cS$. The definition of $L_n$ implies that $L_n  \le C \sqrt{\log(\frac{q_0}{\epsilon_n \eta_n})}\cdot  \sqrt{\log d_1 +\log q} \ll \log(\frac{1}{h_n})\vee (\log d_1 +\log q) $. Generally, we have $d^{-10}\le h_n$, thus the term $\log (\frac{1}{h_n})$ can be omitted. Assume $C_{\mathsf{gap}}$ is large. It is clear that 
\begin{equation*}
    \PP\left( Z_1\ge -\delta_T+\sqrt{L_n} \right)^2\ge  \PP\left( Z_1\ge -C\sqrt{ (\log d_1 +\log q) }  \right)^2\ge (1- c h_n)^2,
\end{equation*}
i.e., $\PP(W_T^\mathsf{Rank}< L_n ) \le C h_n$.  For any $\varepsilon>0$, we compute the probability that $\sum\limits_{T\in\cS} \bbI(W_T^\mathsf{Rank}> L_n)=\eta_n$ by finding its complement:
 \begin{equation*}
\begin{aligned}
               \PP(\sum\limits_{T\in\cS} \bbI(W_T^\mathsf{Rank}> L_n)\le (1-\varepsilon)\eta_n )& =\PP(\sum\limits_{T\in\cS} \bbI(W_T^\mathsf{Rank}< L_n)> \varepsilon\eta_n )  \le \frac{\sum\limits_{T\in\cS} \PP(W_T^\mathsf{Rank}< L_n)}{\varepsilon\eta_n}\le C h_n/\varepsilon,
\end{aligned}
 \end{equation*}
 i.e., $ \PP(\sum\limits_{T\in\cS} \bbI(W_T^\mathsf{Rank}> L_n)\le (1-\varepsilon)\eta_n )\to 0$, $ \PP(\sum\limits_{T\in\cS} \bbI(W_T^\mathsf{Rank}> L_n)\ge \eta_n )\to 1$. This indicates that, all the signals in $\cS$ can pass our test. For our data-driven threshold \eqref{eq:dd-threshold}, we have 
 
  \begin{equation}\label{eq:dd-thre-lb}
\begin{aligned}
             \sum\limits_{T\in\cH} \bbI(W_T^\mathsf{Rank}> L_n)\ge   \sum\limits_{T\in\cS} \bbI(W_T^\mathsf{Rank}> L_n)\ge \frac{3}{4}\eta_n,
\end{aligned}
 \end{equation}
 with probability at least $1-C h_n$
% asymptotically. For our non-strong signals, define the upper bound of signal strength $$h=\sup\limits_{T\in \cS^c}\left\{  \frac{\abs{M_T-\theta_T}}{\sigma_\xi \norm{\cP_M(T)  }_\tF \sqrt{\frac{d_1d_2}{n}}\sqrt{\log\frac{1}{h_n} } }  \right\} $$
% Without loss of generality, we assume $h$ can be any small (because in reality, the signal phases can hardly be of the exact order $\sigma_{\xi} s_T \sqrt{\frac{d_1d_2}{n}\log\frac{1}{h_n} }$). Thus it is reasonable to take $h<\nu/6\sqrt{2}$ . 

% For the first probability, since 
% $\PP(\sum\limits_{T\in\cS} \bbI(W_T> L_n)\ge \eta_n )\to 1$, we only investigate the non-strong signals:
%   \begin{equation*}
% \begin{aligned}
%             \PP(\sum\limits_{T\in\cH_1/\cS} \bbI(W_T<- L_n)\ge \frac{\alpha}{2}\eta_n )\le \frac{2 \sum\limits_{T\in\cH_1/\cS} \PP(W_T<- L_n)}{\alpha \eta_n}.
% \end{aligned}
%  \end{equation*}
% For each $T\in\cH_1/\cS$, it is suggested that $\PP(\abs{\delta^1_T/\sqrt{\log(\frac{1}{h_n})}}\ge \frac{3}{2} h )\le \frac{c\log d_1}{d_1^2} $. Then,
%   \begin{equation*}
% \begin{aligned}
%             &\PP(W_T<- L_n) = \PP( (\widehat{W}^1_T+\delta_T^1)(\widehat{W}^2_T+\delta_T^2)<-L_n )\\
%             &\le \PP(\abs{\widehat{W}^1_T\widehat{W}^1_T}>(1-\frac{\nu}{2})L_n )+ 2\PP(\abs{\delta_T^1\widehat{W}^2_T}>\frac{\nu}{4}L_n ) +\frac{c\log d_1}{d_1^2}\\
%             &\le \Psi(-(1-\nu)\log(\frac{1}{h_n}) )+ 2\PP(\widehat{W}^2_T > \frac{\nu}{6h}L_n ) +Ch_n\\
%             &\le h_n^{1-\nu}+ 2\Phi(-\sqrt{2(1-\frac{\nu}{2})L_n} )+Ch_n\\
%             &\le 2h_n^{1-\nu}+Ch_n\le Ch_n^{1-\nu}.
% \end{aligned}
%  \end{equation*}

% Thus, by Assumption \ref{asp:signals},

%   \begin{equation*}
% \begin{aligned}
%             \PP(\sum\limits_{T\in\cH_1/\cS} \bbI(W_T<- L_n)\ge \frac{\alpha}{2}\eta_n )\le \frac{  Cq h_n^{1-\nu} }{\alpha \eta_n}\to 0,
% \end{aligned}
%  \end{equation*}

Consider the probability $ \PP(\sum\limits_{T\in\cH_0} \bbI(W_T^\mathsf{Rank}<- L_n)\ge \frac{\alpha}{4}\eta_n )$ for the no-signal linear forms $T\in\cH_0$. As we have shown in the proof of Lemma \ref{lemma:conv-prob}, we have 
  \begin{equation*}
\begin{aligned}
            \PP(\sum\limits_{T\in\cH_0} \bbI(W_T^\mathsf{Rank}<- L_n)\ge 2\epsilon_n\eta_n  )\le \log(\frac{q_0 }{\epsilon_n \eta_n}) \left( \left(\frac{\beta_{\mathsf{s}} q_0^2 }{ \epsilon_n^2\eta_n^2}\right)^{\frac{1}{2}} + \left(\frac{h_n q_0}{\epsilon_n \eta_n} +(\epsilon_n\eta_n q_0)^{-\nu/2}\right)^{\frac{1}{2}}\right) \to 0,
\end{aligned}
 \end{equation*}
and consequently, by taking $\epsilon_n=\alpha/8$,
  \begin{equation}\label{eq:dd-thre-up}
\begin{aligned}
            \PP(\sum\limits_{T\in\cH} \bbI(W_T^\mathsf{Rank}<- L_n)\ge\frac{3}{4}\alpha\eta_n )& \le \PP(2\sum\limits_{T\in\cH_0} \bbI(W_T^\mathsf{Rank}<- L_n)\ge \frac{\alpha}{2}\eta_n )+\PP(\sum\limits_{T\in\cS} \bbI(W_T^\mathsf{Rank}<- L_n)\ge \frac{\alpha}{4}\eta_n )\\
            &\le \log(\frac{q_0 }{\alpha \eta_n})\left( \left(\frac{\beta_{\mathsf{s}} q_0^2 }{ \alpha^2\eta_n^2}\right)^{\frac{1}{2}} + \left(\frac{h_n q_0}{\alpha \eta_n} +(\epsilon_n\eta_n q_0)^{-\nu/2}\right)^{\frac{1}{2}}\right)+ C h_n.
\end{aligned}
 \end{equation}
Combining \eqref{eq:dd-thre-lb} and \eqref{eq:dd-thre-up}, it is sufficient to conclude that 
$$
\PP\left(\frac{\sum\limits_{T\in\cH}\bbI\left(T: W_T^\mathsf{Rank}<-L_n\right)}{\left(\sum\limits_{T\in\cH}\bbI\left(T: W_T^\mathsf{Rank}>L_n\right)\right)\vee 1 } \ge \alpha \right)\le \log(\frac{q_0 }{\alpha \eta_n})\left( \left(\frac{\beta_{\mathsf{s}} q_0^2 }{ \alpha^2\eta_n^2}\right)^{\frac{1}{2}} + \left(\frac{h_n q_0}{\alpha \eta_n}+(\alpha\eta_n q_0)^{-\nu/2} \right)^{\frac{1}{2}}\right)+ C h_n,
$$
i.e., $\PP(L\le L_n)\to 1$.

\subsubsection{Power control}
From the discussion on the threshold control, it is clear that for any $\varepsilon$,

\begin{equation*}
    \PP(\sum\limits_{T\in\cS} \bbI(W_T^\mathsf{Rank}> L_n)\le (1-\varepsilon)\eta_n ) \le \frac{\sum\limits_{T\in\cS} \PP(W_T^\mathsf{Rank}< L_n)}{\varepsilon\eta_n}\le Ch_n/\varepsilon.
\end{equation*}
Under the event that $L\le L_n$, this also implies that with probability at least $1-C h_n/\varepsilon$, 

\begin{equation*}
  (1-\varepsilon)\eta_n \le   \sum\limits_{T\in\cS} \bbI(W_T^\mathsf{Rank}> L_n) \le \sum\limits_{T\in\cS} \bbI(W_T^\mathsf{Rank}> L).
\end{equation*}
The probability of $\{L\le L_n\}$ is lower bounded in Section \ref{sec:prof-thres}. We can, therefore, get the power:
\begin{equation*}
    \text{POWER}= \frac{\sum\limits_{T\in\cH_1} \bbI(W_T^\mathsf{Rank}> L)}{q_1}\ge \frac{\sum\limits_{T\in\cS} \bbI(W_T^\mathsf{Rank}> L)}{\eta_n} \cdot \frac{\eta_n}{q_1}\ge (1-\varepsilon)\frac{\eta_n}{q_1},
\end{equation*}
with probability at least:
\begin{equation*}
      1-C \log(\frac{q_0 }{\alpha \eta_n})\left( \left(\frac{\beta_{\mathsf{s}} q_0^2 }{ \alpha^2\eta_n^2}\right)^{\frac{1}{2}} + \left(\frac{h_n q_0}{\alpha \eta_n}+(\alpha\eta_n q_0)^{-\nu/2} \right)^{\frac{1}{2}}\right)- C \varepsilon^{-1} h_n.
\end{equation*}



\subsection{Proof of Proposition \ref{prop:weak-cor-aftscr}}\label{sec:proof-aftscr}
\begin{proof}
By definition, we can equally use the covariance matrix $\mathbf{Q}^* = (\mathbf{X}_{\cA}^{*\top}\mathbf{X}_{\cA}^{*} )^{-1}= \left( \Sigma_{\calA}^{ -\frac{1}{2}  \top} \Sigma_{\calA}^{-\frac{1}{2} }\right)^{-1}$ to derive the correlation coefficient matrix. Here in the proof, we use bold symbols like $\mathbf{Q}$ to distinguish our analysis from the $Q$ in the Algorithm \ref{alg:matrix-sda} of the main text, although they lead to the same correlation structure. We will show that, if two linear forms indexed by $T_i$, $T_j$ are weakly correlated in $ \mathbf{Q}^*$, i.e., 
$$ \abs{\frac{\mathbf{Q}^*_{ij} }{\sqrt{\mathbf{Q}^*_{ii}\mathbf{Q}^*_{jj}}} }= \frac{\abs{e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\right)^{-1}e_j} }{\sqrt{\mathbf{Q}^*_{ii}\mathbf{Q}^*_{jj}}} \le C_1 q_n^{-\nu}, $$ 
then, in the data-driven covariance matrix $\mathbf{Q}$, they are also weakly correlated:
\begin{equation*}
    \abs{\frac{\mathbf{Q}_{ij} }{\sqrt{\mathbf{Q}_{ii}\mathbf{Q}_{jj}}} } \le C_2 q_n^{-\nu},
\end{equation*}
with probability at least $1- Cd_1^{-2}\log d_1 $.
By definition, the covariance matrix of $\wt\sfw^{(2)}$ without normalization is 
$$
\begin{aligned}
     \mathbf{Q}=&\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X} \Sigma  \mathbf{X}^{\top} \mathbf{X}_{\mathcal{A}}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \\
     =& \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}+\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}\mathbf{X}_{\mathcal{A}}^{\top} \Delta \Sigma \mathbf{X}_{\mathcal{A}}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1},
\end{aligned}
$$
where we define $\Delta \Sigma= \mathbf{X} \Sigma  \mathbf{X}^{\top}- I = \widehat{\Sigma}^{-\frac{1}{2}}(\Sigma-\widehat{\Sigma})\widehat{\Sigma}^{-\frac{1}{2}} $.
The following Lemma characterizes the precision of our covariance estimation:
\begin{Lemma}\label{lemma:cov-est-prec}
    Suppose that we use $\widehat{U}$, $\widehat{V}$ obtained from $\cD_0$, $\cD_1$ to estimate $\Sigma$: 
    $$\widehat{\Sigma}= T_{\calH}(I_{d_1 d_2} - \widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top  ) T_{\calH}^\top. $$ 
    Then with probability at least $1-Cd_1^{-2}\log d_1 $, we have 
    \begin{equation}\label{eq:cov-est-prec-1}
        \norm{\Sigma^{-\frac{1}{2} }(\Sigma-\widehat{\Sigma})\Sigma^{-\frac{1}{2} } }\le C\frac{\kappa_T \sigma_\xi }{\lambda_{\min} }\left( \frac{\operatorname{supp}(T_{\calH} )}{\sqrt{d_2}}\wedge 1 \right)  \sqrt{\frac{ \kappa_1  d_1^2 d_2 \log d_1 }{n}}.
    \end{equation}
\end{Lemma}
For simplicity, we denote $\kappa_T' = \kappa_T\left( \frac{\operatorname{supp}(T_{\calH} )}{\sqrt{d_2}}\wedge 1 \right)  $. Lemma \ref{lemma:cov-est-prec} implies the bound of eigenvalue : $\abs{\lambda_i(\Sigma^{-\frac{1}{2} }\widehat{\Sigma} \Sigma^{-\frac{1}{2} } )-1}=o_p(1)$ for all eigenvalues. Thus, the  eigenvalues of its inverse can also be bounded by the rate in \eqref{eq:cov-est-prec-1}, i.e.,
\begin{equation*}
    \norm{\Delta \Sigma}\le C\frac{\kappa_T' \sigma_\xi }{\lambda_{\min} }\sqrt{\frac{ \kappa_1  d_1^2 d_2 \log d_1 }{n}}.
\end{equation*}
We then have
\begin{equation}\label{eq:Qij-decomp}
    \abs{\mathbf{Q}_{ij}- \mathbf{Q}_{ij}^*} \le \abs{e_i^\top \left( \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}-\left(\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\right)^{-1}\right)e_j} + \abs{e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}\mathbf{X}_{\mathcal{A}}^{\top} \Delta \Sigma \mathbf{X}_{\mathcal{A}}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}e_j}.
\end{equation}
Denote $\mathbf{Q}'= \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} $. The first term in \eqref{eq:Qij-decomp} can be controlled by:
\begin{equation}\label{eq:}
   \begin{aligned}
      &\abs{e_i^\top \left( \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}-\left(\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\right)^{-1}\right)e_j} \\
    &= \abs{e_i^\top  \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}\left( \mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^* - \mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)\left(\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\right)^{-1}e_j} \\
    & \le C\frac{\kappa_1^{1.5} \kappa_T' \sigma_{\xi} }{ \lambda_{\min}  }\cdot \sqrt{\frac{ d_1^2 d_2 \log d_1}{n}}\sqrt{\mathbf{Q}_{ii}'\mathbf{Q}_{jj}^* },
   \end{aligned}
\end{equation}
where we use the fact that 
\begin{equation*}
\begin{aligned}
      \norm{\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^* - \mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}}&\le \norm{\widehat{\Sigma}^{-1}-\Sigma^{-1} }\le \norm{\Sigma^{-1}(\widehat{\Sigma}-\Sigma)\Sigma^{-1}  } + O\left(\norm{\widehat{\Sigma}-\Sigma }^2 \right) \\
      & \le \frac{1}{\lambda_{\min}(\Sigma) } \norm{\Sigma^{-\frac{1}{2} }(\Sigma-\widehat{\Sigma})\Sigma^{-\frac{1}{2} }} + o\left(\norm{\widehat{\Sigma}-\Sigma } \right) \\
      & \le C\frac{\kappa_T' \sigma_\xi }{\lambda_{\min}(\Sigma)\lambda_{\min} }\sqrt{\frac{ \kappa_1  d_1^2 d_2 \log d_1 }{n}},
\end{aligned}
\end{equation*}
by Fréchet derivative \citep{higham2008functions,al2009computing} and Lemma \ref{lemma:cov-est-prec}, and also we have
\begin{equation*}
\begin{aligned}
      \norm{\left(\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\right)^{-1}e_j}^2 & \le \frac{1}{\lambda_{\min}\left(\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\right)  } \norm{ e^\top_j \left(\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\right)^{-1}\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\left(\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\right)^{-1}e_j } \\
      & \le \lambda_{\max}(\Sigma) \mathbf{Q}_{jj}^*,
\end{aligned}
\end{equation*}

\begin{equation*}
\begin{aligned}
      \norm{\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}e_i}^2 & \le \frac{1}{\lambda_{\min}\left(\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\right)  } \big| e^\top_i \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}e_i \big|  \\
      & + \frac{1}{\lambda_{\min}\left(\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\right)  } \big| e^\top_i \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}\left( \mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^* - \mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right) \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}e_i \big| \\
      & \le \lambda_{\max}(\Sigma) \mathbf{Q}_{ii}'+ C\frac{\kappa_1^{1.5} \kappa_T'  \sigma_{\xi} }{ \lambda_{\min}  }\cdot \sqrt{\frac{ d_1^2 d_2 \log d_1}{n}}\norm{\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}e_i}^2,
\end{aligned}
\end{equation*}
which is equivalent to 
\begin{equation*}
    \begin{aligned}
       \norm{\left(\mathbf{X}_{\mathcal{A}}^{*\top} \mathbf{X}_{\mathcal{A}}^*\right)^{-1}e_j} &\le \sqrt{\lambda_{\max}(\Sigma) \mathbf{Q}_{jj}^*}\\
      \norm{\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}e_i} &\le (1+c)\sqrt{\lambda_{\max}(\Sigma) \mathbf{Q}_{ii}'}.
    \end{aligned}
\end{equation*}
The second term in \eqref{eq:Qij-decomp} can be bounded given that
\begin{equation*}
    \begin{aligned}
      \abs{e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}\mathbf{X}_{\mathcal{A}}^{\top} \Delta \Sigma \mathbf{X}_{\mathcal{A}}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}e_j} &\le \norm{\mathbf{X}_{\mathcal{A}}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}e_j} \norm{\mathbf{X}_{\mathcal{A}}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1}e_i} \norm{\Delta \Sigma} \\
      & = \sqrt{\mathbf{Q}_{ii}'\mathbf{Q}_{jj}'}\norm{\Delta \Sigma}  \\
      & \le C\frac{\kappa_1^{1.5} \kappa_T'  \sigma_{\xi} }{ \lambda_{\min}  }\cdot \sqrt{\frac{ d_1^2 d_2 \log d_1}{n}} \sqrt{\mathbf{Q}_{ii}'\mathbf{Q}_{jj}'}.
    \end{aligned}
\end{equation*}
However, notice that, 
\begin{equation*}
    \abs{\frac{\mathbf{Q}_{ii}- \mathbf{Q}_{ii}'}{\mathbf{Q}_{ii}' } }\le C\frac{\kappa_1^{1.5} \kappa_T'  \sigma_{\xi} }{ \lambda_{\min}  }\cdot \sqrt{\frac{ d_1^2 d_2 \log d_1}{n}}.
\end{equation*}
We can conclude that 
\begin{equation*}
    \abs{\mathbf{Q}_{ij}-\mathbf{Q}_{ij}^*} \le C\frac{\kappa_1^{1.5} \kappa_T'  \sigma_{\xi} }{ \lambda_{\min}  }\cdot \sqrt{\frac{ d_1^2 d_2 \log d_1}{n}} \left(\sqrt{\mathbf{Q}_{ii}\mathbf{Q}_{jj}^* }+ \sqrt{\mathbf{Q}_{ii}\mathbf{Q}_{jj} } \right).
\end{equation*}
Setting $i=j$, we also have
\begin{equation*}
    \frac{\abs{\mathbf{Q}_{jj}-\mathbf{Q}_{ij}^*}}{ \mathbf{Q}_{jj}} \le C\frac{\kappa_1^{1.5} \kappa_T'  \sigma_{\xi} }{ \lambda_{\min}  }\cdot \sqrt{\frac{ d_1^2 d_2 \log d_1}{n}}\left(\sqrt{1+{\frac{\abs{\mathbf{Q}_{jj}-\mathbf{Q}_{jj}^*}}{ \mathbf{Q}_{jj}}} }+ 1 \right),
\end{equation*}
i.e.,
\begin{equation*}
      \frac{\abs{\mathbf{Q}_{jj}-\mathbf{Q}_{jj}^*}}{ \mathbf{Q}_{jj}} \le C\frac{\kappa_1^{1.5} \kappa_T'  \sigma_{\xi} }{ \lambda_{\min}  }\cdot \sqrt{\frac{ d_1^2 d_2 \log d_1}{n}}.
\end{equation*}
We now compare the difference of correlation coefficients:
\begin{equation*}
\begin{aligned}
       \abs{\frac{\mathbf{Q}_{ij} }{\sqrt{\mathbf{Q}_{ii}\mathbf{Q}_{jj}}} - {\frac{\mathbf{Q}_{ij}^* }{\sqrt{\mathbf{Q}_{ii}^*\mathbf{Q}_{jj}^*}} } } &\le \frac{  \abs{\mathbf{Q}_{ij}-\mathbf{Q}_{ij}^*}  }{ \sqrt{\mathbf{Q}_{ii}\mathbf{Q}_{jj}} }+ \abs{\mathbf{Q}_{ij}^*}\frac{\abs{\sqrt{\mathbf{Q}_{ii}\mathbf{Q}_{jj}}-\sqrt{\mathbf{Q}_{ii}^*\mathbf{Q}_{jj}^*}}}{\sqrt{\mathbf{Q}_{ii}\mathbf{Q}_{jj}} \sqrt{\mathbf{Q}_{ii}^*\mathbf{Q}_{jj}^*}  }\\
    & + \abs{\mathbf{Q}_{ij}-\mathbf{Q}_{ij}^*} \frac{\abs{\sqrt{\mathbf{Q}_{ii}\mathbf{Q}_{jj}}-\sqrt{\mathbf{Q}_{ii}^*\mathbf{Q}_{jj}^*}}}{\sqrt{\mathbf{Q}_{ii}\mathbf{Q}_{jj}} \sqrt{\mathbf{Q}_{ii}^*\mathbf{Q}_{jj}^*}  } \\
    &\le  C\frac{\kappa_1^{1.5} \kappa_T'  \sigma_{\xi} }{ \lambda_{\min}  }\cdot \sqrt{\frac{ d_1^2 d_2 \log d_1}{n}}.
\end{aligned}
\end{equation*}
If the assumption on the signal strength, i.e.,
\begin{equation*}
     \frac{\kappa_1^{1.5} \kappa_T'  \sigma_{\xi} }{ \lambda_{\min}  }\cdot \sqrt{\frac{ d_1^2 d_2 \log d_1}{n}} \lesssim \frac{1}{q^{\nu}},
\end{equation*}
is satisfied, we also have $ { \abs{\mathbf{Q}_{ij}} }/{\sqrt{\mathbf{Q}_{ii}\mathbf{Q}_{jj}}}\lesssim q^{-\nu} $, which indicates that these two linear forms are also weakly correlated in data-driven covariance matrix $\mathbf{Q}$.
\end{proof}



\subsection{Proof of Proposition \ref{prop:lasso-scr}}
\begin{proof}
We start with the decomposition of LASSO response $\mathbf{y}_1 = \mathbf{X}\mathbf{W}^{(1)}  $:
\begin{equation*}
   \mathbf{y}_1= \widehat{\Sigma}^{-\frac{1}{2}}\widehat{D}\widehat{\sfw} + \widehat{\Sigma}^{-\frac{1}{2}}\widehat{D}\widetilde{\mathbf{W}},
\end{equation*}
where $\widehat{\sfw}_i=  \frac{M_{T_i}-\theta_{T_i}}{\widehat\sigma_\xi^{(1)} \sqrt{d_1 d_2}\widehat{s}_{T_i}^{(1)} } \sqrt{n}$ is the standardized signals with variance estimation with respect to $T_i$, $\widetilde{\mathbf{W}}_i=\mathbf{W}_i^{(1)}/\widehat s_{T_i}^{(1)} -\sfw_i $ is the asymptotic normal noise. Here recall that $M_{T_i}:=\langle M, T_i\rangle$ and $\wt s_{T_i}^{(1)}=\big\|\calP_{\wt M^{(1)}}(T_i)\big\|_{\rm F}$. 


Our loading matrix is $\widehat{\Sigma}^{-\frac{1}{2}}\widehat{D}$, with
\begin{equation*}
\lambda_{\min}(\widehat{\Sigma}^{-\frac{1}{2}}\widehat{D})   = \frac{1}{\norm{\widehat{\Sigma}^{\frac{1}{2}}\widehat{D}^{-1} } }=\frac{1}{\sqrt{\norm{\widehat{D}^{-1}\widehat{\Sigma}^{}\widehat{D}^{-1} } }}\ge \frac{1}{\sqrt{\norm{\widehat{D}^{-1}{\Sigma}^{}\widehat{D}^{-1} } + \norm{\widehat{D}^{-1}\left({\Sigma}^{} -\widehat{\Sigma}^{} \right) \widehat{D}^{-1} } }}
\end{equation*}
By \eqref{eq:sT-diff} in the proof of Theorem \ref{thm:asymp-normal}, we have $\abs{1- \widehat{s}_T^{(1)}/s_T}\le  C_2  \frac{\mu\beta_T }{\beta_0} \cdot \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{\alpha_d d_1^2 d_2 \log d_1}{n}} $ with probability at least $1-Cd_1^{-2}\log d_1 $. Here $D:={\rm diag}(s_{T_1},\cdots, s_{T_q})$. 
Thus, the absolute value of the diagonal matrix can be controlled by:
\begin{equation}
   \abs{ D^{-1}-\widehat{D}^{-1}}\preceq C_2  \frac{\mu\beta_T }{\beta_0} \cdot \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{\alpha_d d_1^2 d_2 \log d_1}{n}} D^{-1}.
\end{equation}
This indicates that 
\begin{equation*}
   \norm{\widehat{D}^{-1}{\Sigma}^{}\widehat{D}^{-1} }\le (1+c)\norm{D^{-1}\Sigma D^{-1} }\le \frac{3}{2}\kappa_1,
\end{equation*}
for a small $c$ as long as $  \frac{\mu\beta_T }{\beta_0} \cdot \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{\alpha_d d_1^2 d_2 \log d_1}{n}} \to 0$; and also
\begin{equation*}
    \norm{\widehat{D}^{-1}\left({\Sigma}^{} -\widehat{\Sigma}^{} \right) \widehat{D}^{-1} } \le (1+c)  \norm{{D}^{-1}\left({\Sigma}^{} -\widehat{\Sigma}^{} \right) {D}^{-1} }\le C\frac{\beta_T \mu \sigma_\xi }{\beta_0 \lambda_{\min} }\sqrt{\frac{\alpha_d \kappa_1 q d_1^2 d_2 \log d_1 }{n}},
\end{equation*}
which can be derived following the same steps as Lemma \ref{lemma:cov-est-prec}. It thus gives the well-conditioning of our loading matrix in LASSO:
\begin{equation*}
    \lambda_{\min} \left(\widehat{\Sigma}^{-\frac{1}{2}}\widehat{D}\right)\ge \frac{1}{\sqrt{2\kappa_1}}.
\end{equation*}
Following a classic argument on the LASSO precision analysis \citep{van2009conditions,buhlmann2011statistics}, we have

\begin{equation*}
\begin{aligned}
  \norm{\widehat{\Sigma}^{-\frac{1}{2}}\widehat{D}\left(\wt\sfw^{(1)}  - \widehat{\sfw} \right) }^2\le & 2\left\langle \widehat{D} \widehat{\Sigma}^{-1}\widehat{D} \widetilde{\mathbf{W}},\wt\sfw^{(1)}-\widehat{ \sfw}  \right\rangle   +2\lambda \left(\norm{\widehat{ \sfw} }_{\ell_1} -\norm{\wt\sfw^{(1)}}_{\ell_1} \right) \\
  \le & \mathcal{\lambda} \norm{\wt\sfw^{(1)}-\widehat{ \sfw} }_{\ell_1} +2\lambda \left(\norm{\widehat{ \sfw} }_{\ell_1} -\norm{\wt\sfw^{(1)}}_{\ell_1} \right),
\end{aligned}
\end{equation*}
where we define $\lambda$ as the value that $\PP\left(2\norm{\widehat{D} \widehat{\Sigma}^{-1}\widehat{D} \widetilde{\mathbf{W}}}_{\infty} \ge \lambda \right)\le d_1^{-2}$. It is thus clear that 
\begin{equation*}
\begin{aligned}
  \norm{\widehat{\Sigma}^{-\frac{1}{2}}\widehat{D}\left(\wt\sfw^{(1)}  - \widehat{\sfw} \right) }^2 \le & 3\lambda \norm{\wt\sfw^{(1)}_{s}-\widehat{ \sfw}_{s} }_{\ell_1}  \le 3\lambda\sqrt{q_1} \norm{\wt\sfw^{(1)}-\widehat{ \sfw} }.
\end{aligned}
\end{equation*}
Here, we use the subscript $s$ to denote the support set of $\sfw$. Combined with the well-conditioning property of $\widehat{\Sigma}^{-\frac{1}{2}}\widehat{D}$, we have
\begin{equation*}
    \frac{1}{2\kappa_1 }\norm{\wt\sfw^{(1)}  - \widehat{\sfw} }^2\le   \norm{\widehat{\Sigma}^{-\frac{1}{2}}\widehat{D}\left(\wt\sfw^{(1)}  - \widehat{\sfw} \right) }^2 \le 3\lambda\sqrt{q_1} \norm{\wt\sfw^{(1)}-\widehat{ \sfw} },
\end{equation*}
i.e., $\norm{\wt\sfw^{(1)}  - \widehat{\sfw} }\le 6\lambda \kappa_1 \sqrt{q_1}$. Then, it amounts to determining the regularization level $\lambda$. Notice that $\widehat{D} \widetilde{\mathbf{W}}={D} \widehat{\mathbf{W}}$, where $\widehat{\mathbf{W}}_i=\mathbf{W}_i^{(1)}/s_{T_i}-  \frac{M_{T_i}-\theta_{T_i}}{\widehat \sigma_\xi s_{T_i} \sqrt{d_1 d_2} } \sqrt{n} $. Here $\widehat{\mathbf{W}}_i$ and $\widetilde{\mathbf{W}}_i$ only differ  in the sampling variance $s_{T_i}$.
We adopt the notation in the proof of Theorem \ref{thm:asymp-normal}: we define an average of i.i.d. matrix as $\widehat{Z}_1= \frac{d_1 d_2}{n} \sum_{i\in I_2 } \xi_i X_i $, and split the noise $\widehat{\mathbf{W}}=\widehat{\mathbf{W}}_1+\widehat{\mathbf{W}}_2$, where 
\begin{equation}\label{eq:noise-W-decomp}
    \widehat{\mathbf{W}}_{1i}= \frac{\left\langle \widehat{Z}_1, \cP_M(T_i) \right\rangle }{\sigma_\xi s_{T_i} \sqrt{d_1 d_2/n} }, \text{~for~each~} i\in[q].
\end{equation}
By Theorem \ref{thm:asymp-normal}, we have $\norm{\widehat{\mathbf{W}}_2}_\infty \le C h_n$, with probability at least $1-C d_1^2$. Therefore,
\begin{equation*}
    \begin{aligned}
      2\norm{\widehat{D} \widehat{\Sigma}^{-1}\widehat{D} \widetilde{\mathbf{W}}}_{\infty} & =2\norm{\widehat{D} \widehat{\Sigma}^{-1}{D} \widehat{\mathbf{W}}}_{\infty} \le 2(1+ch_n)\norm{{D} \widehat{\Sigma}^{-1}{D} \widehat{\mathbf{W}}}_{\infty} \\
      & \le 3 \left(\norm{{D} {\Sigma}^{-1}{D} \widehat{\mathbf{W}}}_{\infty} +\norm{{D} (\widehat{\Sigma}^{-1}-{\Sigma}^{-1}) {D} \widehat{\mathbf{W}}}_{\infty} \right)\\
      & \le 3\left( \norm{{D} {\Sigma}^{-1}{D} \widehat{\mathbf{W}}_1}_{\infty}+\norm{{D} {\Sigma}^{-1}{D} \widehat{\mathbf{W}}_2}_{\infty}+\norm{{D} (\widehat{\Sigma}^{-1}-{\Sigma}^{-1}) {D} \widehat{\mathbf{W}}}_{\infty} \right).
    \end{aligned}
\end{equation*}
For any $i$, it is clear that 
\begin{equation*}
    \begin{aligned}
      e_i^\top {D} {\Sigma}^{-1}{D} \widehat{\mathbf{W}}_1 = \frac{\left\langle \operatorname{Vec}(\widehat{Z}_1)^\top, e_i^\top {D} {\Sigma}^{-1}T_{\calH} (I_{d_1 d_2} - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top ) \right\rangle }{\sigma_\xi \sqrt{d_1 d_2/n} },
    \end{aligned}
\end{equation*}
with 
\begin{equation*}
    \begin{aligned}
      \E \left(e_i^\top {D} {\Sigma}^{-1}{D} \widehat{\mathbf{W}}_1 \right)^2 &= e_i^\top D \Sigma^{-1} D e_i\le \kappa_1. \\
    \end{aligned}
\end{equation*}
According to Bernstein inequality, we have

\begin{equation*}
    \frac{1}{\sqrt{n}} \abs{\frac{  e_i^\top {D} {\Sigma}^{-1}{D} \widehat{\mathbf{W}}_1}{\left(e_i^\top D \Sigma^{-1} D e_i\right)^{\frac{1}{2}}} } \le C_1 \sqrt{\frac{(\log d_1 +\log q) }{n}}+ C_2\frac{\sqrt{r d_1}(\log d_1 +\log q) }{n},
\end{equation*}
with probability at least $1-q^{-1}d_1^{-2}$.
% According to Berry-Essen theorem,
% \begin{equation*}
%     \abs{\PP\left(\frac{  e_i^\top {D} {\Sigma}^{-1}{D} \widehat{\mathbf{W}}_1}{\left(e_i^\top D \Sigma^{-1} D e_i\right)^{\frac{1}{2}}} \le t \right) -\Phi(t) }\le C \sqrt{\frac{d_1}{n}}.
% \end{equation*}
% By Lemma \ref{lemma:subspace-perturb}, we have with probability at least $1-q^{-1}d_1^{-2}$,
% $$
% \norm{\widehat{Z}_1}_2\le C_2 \sigma_{\xi}\sqrt{\frac{d_1^2 d_2 \log q d_1}{n}}  
% $$
This indicates that 
\begin{equation*}
    \PP\left(\norm{{D} {\Sigma}^{-1}{D} \widehat{\mathbf{W}}_1}_{\infty}\ge C\sqrt{\kappa_1(\log d_1 +\log q)}  \right) \le d_1^{-2}.
\end{equation*}
If we use $\widehat U$, $\widehat V$ to estimate $\Sigma$, then a corresponding accuracy in $\|\cdot\|_{\infty}$-norm is given by:
\begin{Lemma}\label{lemma:pres-DSD} If we use $\widehat{\Sigma}$ to approximate $\Sigma$, then
\begin{equation*}
            \norm{{D} (\widehat{\Sigma}^{-1}-{\Sigma}^{-1}) {D}}_{\infty} \le C\left(\kappa_\infty\sqrt{\kappa_1} +\kappa_1^{1.5}\kappa_T \left( \frac{\operatorname{supp}(T_{\calH} )}{\sqrt{d_2}}\wedge 1 \right)  \right)\frac{\beta_T \mu \sigma_\xi }{\beta_0 \lambda_{\min} }\sqrt{\frac{\alpha_d q d_1^2 d_2 \log d_1 }{n}}.
\end{equation*}
Here $\|M\|_{\infty}:=\max_i\|e_i^{\top}M\|_{\ell_1}$ and $\kappa_{\infty}:=\|R^{-1}\|_{\infty}$ where $R=D^{-1}\Sigma D^{-1}$. 
\end{Lemma}
Notice that, since $\widehat{\mathbf{W}}_{1i}$ is standardized, Bernstein inequality also gives the bound:
\begin{equation*}
    \norm{\widehat{\mathbf{W}}_1  }_\infty \le C\sqrt{\log d_1 +\log q },
\end{equation*}
with probability at least $1-d_1^{-2}$. This indicates that, with probability at least $1-C d^{-2}_1$, we have
\begin{equation*}
      2\norm{\widehat{D} \widehat{\Sigma}^{-1}\widehat{D} \widetilde{\mathbf{W}}}_{\infty} \le C \left( \sqrt{\kappa_1(\log d_1 +\log q)} +\kappa_\infty h_n \right) \le C \sqrt{\kappa_1(\log d_1 +\log q)},
\end{equation*}
as long as $\left(\kappa_\infty h_n \right)\vee \left(\left(\kappa_\infty\sqrt{\kappa_1} +\kappa_1^{1.5}\kappa_T \left( \frac{\operatorname{supp}(T_{\calH} )}{\sqrt{d_2}}\wedge 1 \right)  \right)\frac{\beta_T \mu \sigma_\xi }{\beta_0 \lambda_{\min} }\sqrt{\frac{\alpha_d q d_1^2 d_2 \log d_1 }{n}}\right)\le c \sqrt{\kappa_1}$ for some small constant $c$. Here we use the fact $\norm{{D} {\Sigma}^{-1}{D} \widehat{\mathbf{W}}_2}_{\infty} \le C\kappa_\infty h_n$.This leads to the error bound of $\wt\sfw^{(1)}$:

\begin{equation*}
   \norm{\wt\sfw^{(1)}  - \widehat{\sfw} }_\infty \le \norm{\wt\sfw^{(1)}  - \widehat{\sfw} }\le 6\lambda \kappa_1 \sqrt{q_1}\le C \kappa_1^{1.5}\sqrt{q_1(\log d_1 +\log q )}.
\end{equation*}
Since for each $i$, 
$$\abs{\widehat{\sfw}_i-\sfw_i}\le \left(\frac{C_1 \tau \log d_1}{\sqrt{n}}+C_2 \gamma_n^2 +C_3 \mu \frac{\|T\|_{\ell_1}}{\|T\|_\tF \beta_0} \cdot \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{\tau \alpha_d d_1^2 d_2 \log d_1}{n}}\right)\abs{\sfw_i} \le C h_n \abs{\sfw_i}, $$
we finish the proof.
\end{proof}

\subsection{Proof of Proposition \ref{prop:OLS-normal}}
\begin{proof}

We proceed to discuss the asymptotic normality of each $e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top} \mathbf{y}_2$: since $\mathbf{y}_2 = \mathbf{X}\mathbf{W}^{(2)}  $, with
\begin{equation*}
   \mathbf{y}_2= \widehat{\Sigma}^{-\frac{1}{2}}{D}\widehat{\sfw} + \widehat{\Sigma}^{-\frac{1}{2}}{D}\widehat{\mathbf{W}},
\end{equation*}
where, with a slight abuse of notation, we define $\widehat{\sfw}_i=  \frac{M_T-\theta_T}{\widehat\sigma_\xi \sqrt{d_1 d_2}{s}_T } \sqrt{n}$ is the standardized signals with variance estimation, $\widehat{\mathbf{W}}_i=\mathbf{W}_i/s_{T_i} -\widehat\sfw_i $ is the asymptotic normal noise. From the proof of Theorem \ref{thm:asymp-normal}, it is clear that $\widehat{\sfw}_i$ is close enough to ${\sfw}_i$.
% Here $\widehat\sfw$ and $\mathbf{W}$ are constructed from data  $\cD_0$ and $\cD_2$, and are defined analogously as in the proof of Proposition \ref{prop:lasso-scr}. 
Notice that, here, we do not assume $\cH_1\subseteq \cA$. For any $i\in\cA$ , we have

$$
\begin{aligned}
 & e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top} \mathbf{y}_2  = e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top} \left[ \mathbf{X}_{\mathcal{A}}, \mathbf{X}_{\mathcal{A}^c} \right] D (\widehat{\sfw}+ \widehat{\mathbf{W}}) \\
 &= s_{T_i} \widehat\sfw_i + e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} D\widehat{\mathbf{W}} + e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top}\mathbf{X}_{\mathcal{A}^c}D_{\cA^c}\widehat\sfw_{\cA^c} \ \\
 &= s_{T_i}\widehat\sfw_i+ e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} D\left(\widehat{\mathbf{W}}_1+\widehat{\mathbf{W}}_2\right)+e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top}\mathbf{X}_{\mathcal{A}^c}D_{\cA^c}\widehat \sfw_{\cA^c},
\end{aligned}
$$
where the noise decomposition $\widehat{\mathbf{W}}=\widehat{\mathbf{W}}_1+\widehat{\mathbf{W}}_2$ is the same as \eqref{eq:noise-W-decomp}. 
If $T_i\in \cA \cap \cH_0$, we have $\sfw_i =0$, thus $e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top} \mathbf{y}_2 = e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} D\left(\widehat{\mathbf{W}}_1+\widehat{\mathbf{W}}_2\right)$. We investigate the following terms: (i) the asymptotic normality of $e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} D\widehat{\mathbf{W}}_1$, (ii) the vanishing of $e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} D\widehat{\mathbf{W}}_2$, and (iii) the bias introduced by inconsistent screening $e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top}\mathbf{X}_{\mathcal{A}^c}D_{\cA^c}\widehat\sfw_{\cA^c}$.

\noindent \textbf{(i)} the asymptotic normality of $\widehat{\beta}_i:=e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} D\widehat{\mathbf{W}}_1$.
Conditional on $\cD_0$ and $\cD_1$, $\widehat{\beta}_i$ can be viewed as sum of i.i.d. independent random variables:

\begin{equation}
   \widehat{\beta}_i= \frac{\left\langle \Vect(\widehat{Z}_1),  e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} T_{\calH} \left(I_{d_1 d_2} -  U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \right\rangle }{\sigma_\xi  \sqrt{d_1 d_2/n} }.
\end{equation}
The variance of $\widehat{\beta}_i$ is given by 
\begin{equation*}
\begin{aligned}
     \E  \widehat{\beta}_i^2& = \norm{e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} T_{\calH} \left(I_{d_1 d_2} -  U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)}^2 \\
    &=  e_i^\top\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X} \Sigma  \mathbf{X}^{\top} \mathbf{X}_{\mathcal{A}}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} e_i=\mathbf{Q}_{ii}.
\end{aligned}
\end{equation*}
The third-order moment of each component is also derived by 

\begin{equation*}
\begin{aligned}
     \E & \abs{\sqrt{d_1 d_2/n}\frac{\left\langle \Vect(\xi_i X_i),  e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} T_{\calH} \left(I_{d_1 d_2} -  U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \right\rangle }{\sigma_\xi \mathbf{Q}_{ii}^{\frac{1}{2}}  }}^3 \\
     & \le C\frac{\sqrt{d_1 d_2}}{n^{1.5} }\frac{ \abs{\left\langle \Vect( X_i),  e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} T_{\calH} \left(I_{d_1 d_2} -  U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \right\rangle}  }{\mathbf{Q}_{ii}^{\frac{1}{2}} } \\
     & = C\frac{\sqrt{d_1 d_2}}{n^{1.5} }\frac{ \abs{\left\langle \Vect( X_i)\left(I_{d_1 d_2} -  U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) ,  e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} T_{\calH} \left(I_{d_1 d_2} -  U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \right\rangle}  }{\mathbf{Q}_{ii}^{\frac{1}{2}} } \\
     &\le C\frac{\sqrt{d_1 d_2}}{n^{1.5}} \norm{\Vect( X_i)\left(I_{d_1 d_2} -  U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)  }_\tF \\
    &\le  C \frac{\mu \sqrt{r d_1}}{n^{1.5}},
\end{aligned}
\end{equation*}
where we use the incoherence condition in the last inequality. It is thus suggested that:
\begin{equation}
    \abs{ \PP\left(\frac{\widehat{\beta}_i}{\sqrt{\mathbf{Q}_{ii}}  } \le t \middle| \cD_0,\cD_1\right) - \Phi(t) } \le C\mu \sqrt{\frac{r d_1}{n}}.
\end{equation}

\noindent \textbf{(ii)} the vanishing of $\Delta{\beta}_i:= e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} D\widehat{\mathbf{W}}_2$. By the proof of Theorem \ref{thm:asymp-normal}, we have $\norm{\widehat{\mathbf{W}}_2}_\infty \le C h_n$, with probability at least $1-Cd_1^{-2}\log d_1 $. Thus, by writing $\mathbf{X}=[\mathbf{X}_{\mathcal{A}},\mathbf{X}_{\mathcal{\cA}^c}]$, we have
\begin{equation*}
    \frac{\abs{\Delta{\beta}_i }}{\sqrt{\mathbf{Q}_{ii}} } = \frac{\abs{e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X} D\widehat{\mathbf{W}}_2}}{\sqrt{\mathbf{Q}_{ii}} } \le \frac{\abs{s_{T_i }\widehat{\mathbf{W}}_{2i}} + \abs{e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top}\mathbf{X}_{\mathcal{A}^c}D_{\cA^c}\widehat{\mathbf{W}}_{2,\cA^c} }  }{\sqrt{\mathbf{Q}_{ii}}}. \\
\end{equation*}
Using the definition of $C_\infty$, it follows that
\begin{equation*}
    \frac{\abs{\Delta{\beta}_i }}{ \sqrt{\mathbf{Q}_{ii}} } \le C C_\infty h_n,
\end{equation*}
uniformly for all $i$ with probability at least $1-C\log d_1 d_1^{-2}$. 

\noindent \textbf{(iii)} the bias $e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top}\mathbf{X}_{\mathcal{A}^c}D_{\cA^c}\widehat\sfw_{\cA^c}$ can be surely controlled by 
\begin{equation*}
    \frac{\abs{e_i^\top \left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top}\mathbf{X}_{\mathcal{A}^c}D_{\cA^c}\widehat\sfw_{\cA^c} } }{ \sqrt{\mathbf{Q}_{ii}} } \le C\cdot C_\infty (h_n+\norm{\sfw_{\cA^c} }_{\infty}).
\end{equation*}
Then, combing (i), (ii), and (iii) by the Lipschiz property of $\Phi(t)$, we have
\begin{equation*}
       \abs{ \PP\left( \frac{\wt\sfw^{(2)}_i}{\sqrt{\mathbf{Q}_{ii}} }\le t \middle| \cD_0, \cD_1  \right)-\Phi (t)}\le C \cdot C_\infty ( h_n+\norm{\sfw_{\cA^c} }_{\infty}) + C\mu \sqrt{\frac{r d_1}{n}} \le C \cdot C_\infty( h_n+\norm{\sfw_{\cA^c} }_{\infty}).
\end{equation*}


\end{proof}

\subsection{Proof of Theorem \ref{thm:matrix-fdr-strong}} \label{sec:proof-fdr-strong}
\begin{proof}
In the following proof, we write $h_n+\norm{\sfw_{\cA^c} }_{\infty}$ as $h_n$ for notational simplicity.  The proof essentially follows the proof of Theorem \ref{thm:weak-cor-fdr}. Define the expected false rejection:
\begin{equation*}
    \widetilde{G}(t)= \frac{\sum_{{T_i}\in\cH_0 \cap \cA } \PP( {\wt\sfw_i^{(1)}\frac{\sqrt{\mathbf{Q}_{ii}}}{ \widehat{\sigma}_{w i} } } Z > t |\cD_0,\cD_1 )   }{q_{0n}},
\end{equation*}
where $\wt\sigma_{wi}^2=e_i^{\top}\big(\mathbf{X}_{\calA}^{\top}\mathbf{X}_{\calA}\big)^{-1}e_i$ is defined in Algorithm~\ref{alg:matrix-sda-practical}. 
Denote $$L_n'= \widetilde{G}^{-1}\left( \frac{\epsilon_n \eta_n'}{q_{0n} }\right)= \inf\left\{t:\widetilde{G}(t)\le \frac{\epsilon_n \eta_n'}{q_{0n} }\right\},$$ where $\epsilon_n$ is a rate to be specified later, and $q_{0n}=\abs{\cA \cap \cH_0 }$. We can rewrite Lemma \ref{lemma:pop-wrank}, \ref{lemma:weak-cov}, and \ref{lemma:conv-prob} as:

\begin{Lemma}\label{lemma:pop-wrank-sda} Conditional on $E_0$ and $\cD_1$, we have
% \begin{equation*}
%     \PP\left( \sup\limits_{0\le t \le L_n} \abs{\frac{ \sum_{T\in\cH_0}\PP(W_T>t ) }{q_0 (-t) } -1 }\ge \varepsilon \right) \le C_3 h_n^{\frac{\nu}{4}}
% \end{equation*}
\begin{equation*}
    \sup\limits_{0\le t \le L_n} \abs{\frac{ \sum_{{T_i}\in\cH_0 \cap \cA }\PP(\wt\sfw_i^\mathsf{Rank} >t ) }{q_{0n} \widetilde{G} (t) } -1 }\le C_3 \frac{C_\infty h_n q_{0n} }{\epsilon_n \eta_n' }.
\end{equation*}
\end{Lemma}
Here we use $\wt\sfw_i^\mathsf{Rank}$ to indicate the combined statistics $\wt\sfw_{T_i}^\mathsf{Rank}$
\begin{Lemma}[Weak dependency of null features]\label{lemma:weak-cov-sda}
        Conditional on $E_0$, $\cD_1$, 
\begin{equation}
\begin{aligned}
          \sup_{0\le t\le L_n' } \frac{\sum_{(T_i,T_j)\in \cH_{0\cA,\text{weak} }^2 } \abs{\operatorname{cov}(\bbI(\wt\sfw_i^\mathsf{Rank} >t),\bbI(\wt\sfw_i^\mathsf{Rank}  >t)) }}{ q_{0n}^2 \widetilde G^2(t)}\le  C_1 \frac{C_\infty h_n q_{0n} }{\epsilon_n \eta_n' }+C_2 \frac{1}{ \left(\epsilon_n\eta_n' q_{0n}\right)^{v/2} }.
\end{aligned}
\end{equation}
\end{Lemma}

\begin{Lemma}\label{lemma:conv-prob-sda }
For any $\varepsilon>0$, conditional on $E_0$ and $\cD_1$, it holds that
\begin{equation*}
\begin{aligned}
    &\PP\left(\sup\limits_{0\le t\le L_n'}\abs{\frac{ \sum_{T_i\in\cH_0 \cap \cA }\bbI(\wt\sfw_i^\mathsf{Rank}  >t))}{q_{0n} \widetilde G(t) } -1} \ge \varepsilon\right) \\
    &\le \frac{C}{\varepsilon^2} \log(\frac{q_{0n} }{\epsilon_n \eta_n'}) \left( \left(\frac{\beta_{\mathsf{s}}' q_{0n}^2 }{ \epsilon_n^2\eta_n^{'2}}\right)^{\frac{1}{2}} + \left(\frac{ C_\infty h_n q_{0n} }{\epsilon_n \eta_n'}+ \frac{1}{ \left(\epsilon_n\eta_n' q_{0n}\right)^{v/2} } \right)^{\frac{1}{2}}\right) .
\end{aligned}
\end{equation*}
\end{Lemma}
The proof of Lemma \ref{lemma:pop-wrank-sda}, \ref{lemma:weak-cov-sda}, and \ref{lemma:conv-prob-sda } is same as that in Lemma \ref{lemma:pop-wrank}, \ref{lemma:weak-cov}, and \ref{lemma:conv-prob}, and thus omitted.
These lemmas imply that, if $L\le L_n'$, then we have $Ratio\le 1+3\varepsilon$ with probability at least 
\begin{equation*}
    1-\frac{C}{\varepsilon^2} \log(\frac{q_{0n} }{\epsilon_n \eta_n'}) \left( \left(\frac{\beta_{\mathsf{s}}' q_{0n}^2 }{ \epsilon_n^2\eta_n^2}\right)^{\frac{1}{2}} + \left(\frac{ C_\infty h_n q_{0n} }{\epsilon_n \eta_n'}+ \frac{1}{ \left(\epsilon_n\eta_n' q_{0n}\right)^{v/2} } \right)^{\frac{1}{2}}\right).
\end{equation*}

We then prove the probability of $\PP(L\le L_n')$ can be very large. A matching upper bound of $\widetilde{G}(t)$ is given by, similarly as in the proof Theorem \ref{thm:weak-cor-fdr},
 \begin{equation*}
    \widetilde{G}(t)= \frac{\sum_{{T_i}\in\cH_0 \cap \cA } \PP( {\wt\sfw_i^{(1)}\frac{\sqrt{\mathbf{Q}_{ii}}}{ \widehat{\sigma}_{w i} } } Z > t |\cD_0,\cD_1 )   }{q_{0n}} \le \frac{\sqrt{2}}{\sqrt{\pi}} \exp{\left( -\frac{t^2}{2 \max_{T\in\cH_0 \cap \cA} \abs{\wt\sfw_i^{(1)}\frac{\sqrt{\mathbf{Q}_{ii}}}{ \widehat{\sigma}_{ w i}  }}^2 } \right)}.
\end{equation*}

The LASSO results presented in Proposition \ref{prop:lasso-scr} show that, the $\abs{\wt\sfw_i^{(1)}}$ can be uniformly bounded by:
\begin{equation*}
\begin{aligned}
     \max_{T\in\cH_0 \cap \cA} \abs{\wt\sfw_i^{(1)}\frac{\sqrt{\mathbf{Q}_{ii}}}{ \widehat{\sigma}_{w i}   }} & \le  \max_{T\in\cH_0 \cap \cA}\abs{\wt\sfw_i^{(1)}} \frac{ \sqrt{e_i^\top\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X} \Sigma  \mathbf{X}^{\top} \mathbf{X}_{\mathcal{A}}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} e_i}  }{\sqrt{e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} e_i} } \\
     & \le \max_{T\in\cH_0 \cap \cA}\abs{\wt\sfw_i^{(1)}} \left(1+ \frac{\norm{ e_i^\top\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} \mathbf{X}_{\mathcal{A}}^{\top}  } \sqrt{\norm{\mathbf{X}\Sigma \mathbf{X}^{\top} -I } } }{\sqrt{e_i^{\top}\left(\mathbf{X}_{\mathcal{A}}^{\top} \mathbf{X}_{\mathcal{A}}\right)^{-1} e_i}}  \right) \\
     & \le (1+c)\max_{T\in\cH_0 \cap \cA}\abs{\wt\sfw_i^{(1)}} \\
     & \le C \kappa_1^{1.5}\sqrt{q_1(\log d_1 +\log q )}, \\
\end{aligned}
\end{equation*}
with probability at least $1-C d_1^{-2}$. Here we use the fact that $\norm{\mathbf{X}\Sigma \mathbf{X}^{\top} -I }\le \frac{1}{1-c}$ if we have its inverse $\norm{\Sigma^{-\frac{1}{2}} \widehat{\Sigma} \Sigma^{-\frac{1}{2}}-I }\le c$. The definition of $L_n'$ implies that 
$$L_n'  \le C \sqrt{\log(\frac{q_{0n} }{\epsilon_n \eta_n'})}\cdot  C \kappa_1^{1.5}\sqrt{q_1(\log d_1 +\log q )} \ll \sqrt{\log(\frac{1}{h_n})}\cdot \kappa_1^{1.5}\sqrt{q_1(\log d_1 +\log q )}. $$  
If $T_i\in \cS$, then $\abs{\delta_{T_i} }\ge C_{\mathsf{gap}}\sqrt{\log \frac{1}{h_n} }\vee \kappa_1^{1.5}\sqrt{q_1(\log d_1 +\log q )} $ by the definition of $\cS$, and also the LASSO estimation: 
$$  \abs{\wt\sfw_i^{(1)}}\ge C \kappa_1^{1.5}\sqrt{q_1(\log d_1 +\log q )},$$
by our assumption. Assume $C_{\mathsf{gap}}$ is large enough, and $\delta_{T_i}>0$. Then we have 
 \begin{equation*}
\begin{aligned}
\PP(\wt\sfw_i^\mathsf{Rank}< L_n' ) \le &\PP\left(\wt\sfw_i^{(1)} (Z_2+\delta_{T_i} \frac{s_{T_i} }{\sqrt{\mathbf{Q}_{ii} } } )< L_n' \right) +C_\infty h_n \\
\le & 1- \PP\left( (Z_2+\delta_{T_i}\frac{s_{T_i} }{\sqrt{\mathbf{Q}_{ii} } }) \ge L_n'/\wt\sfw_i^{(1)} \right) +C_\infty h_n \\
\le & 1- \PP\left( Z_2\ge -\delta_{T_i}+\sqrt{\log \frac{1}{h_n} } \right) +C_\infty h_n \\
\le & \PP\left(Z_2\le -2\sqrt{\log \frac{1}{h_n} } \right) +C_\infty h_n  \\
\le & 2C_\infty h_n.
% \PP(W_T< L_n)&\le \PP(W^{(1)}_T<  \sqrt{L_n})+\PP(W^{(2)}_T<  \sqrt{L_n})\le 2\PP(W_T^1< \sqrt{L_n}|E_0) + Ch_n \\
% & =  2\PP(\widehat{W}_T^1+\delta^1_T< \sqrt{L_n}|E_0) + Ch_n
\end{aligned}
 \end{equation*}
 We compute the probability:
  \begin{equation*}
\begin{aligned}
               \PP(\sum\limits_{T\in\cS} \bbI(\wt\sfw_i^\mathsf{Rank}> L_n')\le (1-\varepsilon)\eta_n' )& =\PP(\sum\limits_{T\in\cS} \bbI(\wt\sfw_i^\mathsf{Rank}< L_n)> \varepsilon\eta_n' )  & \\
               \le \frac{\sum\limits_{T\in\cS} \PP(\wt\sfw_i^\mathsf{Rank}< L_n')}{\varepsilon\eta_n'}\le C C_\infty h_n/\varepsilon,
\end{aligned}
 \end{equation*}
 i.e., $ \PP(\sum\limits_{T\in\cS} \bbI(\wt\sfw_T^\mathsf{Rank}> L_n')\le (1-\varepsilon)\eta_n' )\to 0$, $ \PP(\sum\limits_{T\in\cS} \bbI(\wt\sfw_T^\mathsf{Rank}> L_n')\ge \eta_n' )\to 1$. By taking $\epsilon_n=\alpha/8$, other steps essentially follow the proof of Theorem \ref{thm:weak-cor-fdr}.

\end{proof}


% \begin{Lemma}
%         When the signals satisfies , we have the following precision for $\widehat{S}$
% \begin{equation}\label{eq:cov-precision-lasso}
%     \max\left\{ \frac{\norm{D S^{-1}\widehat{S}D^{-1} -I  }_\infty }{\sqrt{\kappa_1\log d_1 \vee \log q } }, \frac{\norm{ (D S^{-1}\widehat{S}D^{-1} -I)\Sigma^{-\frac{1}{2}}  }_{2,\max} }{\sqrt{\kappa_1 } } , \norm{S-\widehat{S}}_\infty \right\} \le c
% \end{equation}
% for a uniform constant $c$ with probability exceeding $1-C d^{-2} $
% \end{Lemma}



\section{Proofs of Auxiliary Results}
\subsection{Verification of \eqref{eq:W-kth-moment}}
It has been shown in the proof of Theorem \ref{thm:asymp-normal} that the test statistics $W_T$ can be decomposed as 
\begin{equation*}
	W_{T} = \frac{\left\langle  \widehat{Z}_1 , \cP_M(T) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T) }_\tF  \sqrt{d_1 d_2/n}}+ \Delta_{T},
\end{equation*}
where $\Delta_T$ is a vanishing term with the rate of convergence described in \eqref{eq:2-d-remainder}. Suppose also the distribution of $\xi$ is symmetric. Denote $I_1$ the index set of observations in sample $\calD_1$. 
Therefore, for any integer $k\ge 2$, we have
\begin{equation*}
	\begin{aligned}
		&\E \abs{W_{T} }^{2k} \gtrsim \E \abs{\frac{\left\langle  \widehat{Z}_1 , \cP_M(T) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T) }_\tF  \sqrt{d_1 d_2/n}}}^{2k} = \E \abs{\frac{\sqrt{d_1 d_2/n} \sum_{i\in I_1} \xi_i \left\langle  X_i , \cP_M(T) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T) }_\tF }}^{2k} \\
		& \ge \left(\E \abs{\frac{\sqrt{d_1 d_2/n} \sum_{i\in I_1} \xi_i \left\langle  X_i , \cP_M(T) \right\rangle }{\sigma_{\xi} \norm{\cP_M(T) }_\tF }}^{4}  \right)^{k/2} \\
		& \ge \left(  \frac{d_1^2 d_2^2 \left( \sum_{i\in I_1} \E\xi_i^4 \left\langle  X_i , \cP_M(T) \right\rangle^4 + \sum_{i,j\in I_1, i\neq j}\E\xi_i^2 \left\langle  X_i , \cP_M(T) \right\rangle^2\xi_j^2 \left\langle  X_j , \cP_M(T) \right\rangle^2 \right) }{n^2\sigma_{\xi}^4 \norm{\cP_M(T) }^4_\tF }  \right)^{k/2} \\
		& \gtrsim \left(  \frac{d_1^2 d_2^2 \E\left\langle  X_i , \cP_M(T) \right\rangle^4 }{n \norm{\cP_M(T) }^4_\tF } +1 \right)^{k/2} \\
		& = \left(  \frac{d_1 d_2 \sum_{i\in [d_1],j\in[d_2]} \cP_M(T)_{i,j}^4 }{n \norm{\cP_M(T) }^4_\tF } +1 \right)^{k/2}. 
	\end{aligned}
\end{equation*}
If the energy of $\cP_M(T)$ is concentrated in a few entries, e.g., there exists an index set $J$ such that the entries in $J$ can dominate other entries, i.e., 
\begin{equation*}
	\sum_{(i,j)\in J} \cP_M(T)_{i,j}^2 \ge  \sum_{(i,j)\notin J} \cP_M(T)_{i,j}^2,
\end{equation*}
with  $s_0:=\abs{J}=O(1)$, then we have 
\begin{equation*}
	\begin{aligned}
		\frac{\sum_{i\in [d_1],j\in[d_2]} \cP_M(T)_{i,j}^4}{\norm{\cP_M(T) }^4_\tF } \ge \frac{\sum_{(i,j)\in J} \cP_M(T)_{i,j}^4}{  4 \left( \sum_{(i,j)\in J} \cP_M(T)_{i,j}^2 \right)^2 } \ge \frac{1}{4 s_0} \ge \Omega(1),
	\end{aligned}
\end{equation*}
and thus, we have
\begin{equation*}
	\begin{aligned}
		&\sqrt[2k]{\E \abs{W_{T} }^{2k}} \gtrsim \left( \frac{d_1 d_2}{n}\right)^{1/4}. 
	\end{aligned}
\end{equation*}

\subsection{Proof of Theorem \ref{thm:power-comparison}}
\begin{proof}
	Define the c.d.f. of the product of two standard normal random variables as $\Psi(t)$, also $\Tilde \Psi(t) :=1-\Psi(t)$. The c.d.f. of standard normal distribution is denoted by $\Phi(t)$ by convention, with $\Tilde \Phi(t) :=1-\Phi(t)$. For $j=1$, we have 
	\begin{equation*}
		\begin{aligned}
			\PP(Y_1>t|H_0) &= \Psi(-t) = \Tilde{\Psi}(t)\\
			F_1(z,t):= \PP(Y_1>t|z,H_1) &=  \PP\left( \frac{(\xi_1+\xi_2+2\delta )^2}{4} -\frac{(\xi_1-\xi_2)^2}{4} >t \right)= \PP \left( \frac{(Z_1+\sqrt{2}\delta )^2}{2}-\frac{Z_2^2}{2} >t \right) \\
			& = \int_{\R} \left[ \Tilde{\Phi}(\sqrt{2t + y_2^2}-\sqrt{\frac{2}{p}} z ) + \Tilde{\Phi}(\sqrt{2t + y_2^2}+\sqrt{\frac{2}{p}}z ) \right]d y_2.
		\end{aligned}
	\end{equation*}
	Here $\xi_1$, $\xi_2$, and $Z_1$, $Z_2$ are all standard normal random variables. Thus $ L_{p 1} = \Tilde{\Psi}^{-1}(p)$. Calculate the first order and second order derivative of $F_1(z,t)$ with respect to $z$ when $t= L_{p 1}$ :
	\begin{equation*}
		\begin{aligned}
			\partial_z   F_1(0,L_{p 1}) & = 0 \\
			\partial_z^2   F_1(0,L_{p 1}) & = \frac{8}{q} \int_{0}^{+\infty} -  f'(\sqrt{2L_{p 1} + y_2^2} )  d y_2= \frac{8}{p} f(-\sqrt{2L_{p 1}}).
		\end{aligned}
	\end{equation*}
	Since $\Tilde{\Psi}(t)<\sqrt{2}\Tilde{\Phi} (\sqrt{2t})$, we have $L_{p 1}< \frac{1}{2} \Tilde{\Phi}^{-1}(p/\sqrt{2})^2 $. When $x\to 0$, we have 
	\begin{equation*}
		\sqrt{2(\log(\frac{1-r_1}{x}) -\frac{1}{2}\log\log (\frac{1-r_1}{x})  ) } \le\Tilde{\Phi}^{-1}(x)\le \sqrt{2(\log(\frac{1}{x}) -\frac{1}{2+r_2}\log\log (\frac{1}{x})  ) }.
	\end{equation*}
	for any small $r_1,r_2>0$. Thus we have $L_{p 1}< \frac{1}{2} \Tilde{\Phi}^{-1}(p/\sqrt{2})^2 \le \log(\frac{\sqrt{2}}{p}) - \frac{1}{2+r_2}\log\log (\frac{\sqrt{2}}{p})$, and the second order derivative 
	\begin{equation*}
		\begin{aligned}
			\partial_z^2   F_1(0,L_{p 1}) & = \frac{8}{p} f(-\sqrt{2L_{p 1}}) \ge c (\log(\frac{\sqrt{2}}{p}) )^{1/(2+r_2)},
		\end{aligned}
	\end{equation*}
	is non-vanishing.
	
	For $j=2$,  we have
	\begin{equation*}
		\begin{aligned}
			\PP(Y_2>t|H_0) &= \PP(\xi_1>t,\xi_2>t )+\PP(\xi_1<-t,\xi_2<-t ) = 2 \Tilde{\Phi}^2(t) \\
			F_2(z,t):= \PP(Y_2>t|z,H_1) &=  \PP(\xi_1 +\mu>t,\xi_2+\mu >t ) +\PP(\xi_1 +\mu<-t,\xi_2+\mu <-t )\\ &=\Tilde{\Phi}^2(t+\sqrt{\frac{1}{p}}z ) +\Tilde{\Phi}^2(t-\sqrt{\frac{1}{p}}z ).
		\end{aligned}
	\end{equation*}
	In this case, the threshold $L_{p 2}= \Tilde{\Phi}^{-1}(\sqrt{\frac{p}{2}} ) $. Compute the derivatives of $F_2$:
	\begin{equation*}
		\begin{aligned}
			\partial_z   F_2(0,L_{p 2}) & = 0 \\
			\partial_z^2   F_2(0,L_{p 2}) & = \frac{4 }{p} (f^2(-L_{p 2}) +\Tilde{\Phi}(L_{p 2})f'(-L_{p 2})  )\ge c ((\log(\sqrt{\frac{2}{p}}) )^{1/(2+r_2) }  +1 ),
		\end{aligned}
	\end{equation*}
	which also has a non-vanishing second-order derivative.
	
	For $j=3$, we have
	\begin{equation*}
		\begin{aligned}
			\PP(Y_3>t|H_0) &= \PP(\xi_1+\xi_2>t, \xi_1>0, \xi_2>0 )+\PP(\xi_1+\xi_2<-t, \xi_1<0, \xi_2<0 ) \\
			&= 2 \PP(Y_1 >\frac{t}{\sqrt{2}}, -Y_1<Y_2<Y_1)=  2\Tilde{\Phi}(\frac{t}{\sqrt{2}})(1-\Tilde{\Phi}(\frac{t}{\sqrt{2}}))  \\
			\PP(Y_3>t|z,H_1) &=  \PP(Z_1 >\frac{t-2\mu}{\sqrt{2}}, -Z_1-\sqrt{2}\mu<Z_2<Z_1+\sqrt{2}\mu) \\
			& + \PP(Z_1 <\frac{-t-2\mu}{\sqrt{2}}, Z_1+\sqrt{2}\mu<Z_2<-Z_1-\sqrt{2}\mu)\\ 
			&\le \Tilde{\Phi}(\frac{t}{\sqrt{2}} )\left(\phi(\frac{t}{\sqrt{2}}+\sqrt{2}\mu)+\phi(\frac{t}{\sqrt{2}}-\sqrt{2}\mu) \right)  \\
			F_3(z,t):= & \Tilde{\Phi}(\frac{t}{\sqrt{2}} )\left(\phi(\frac{t}{\sqrt{2}}+\sqrt{\frac{2}{p}}z)+\phi(\frac{t}{\sqrt{2}}-\sqrt{\frac{2}{p}}z) \right).
		\end{aligned}
	\end{equation*}
	Compute the derivatives of $F_3$:
	
	\begin{equation*}
		\begin{aligned}
			\partial_z   F_3(0,L_{p 3}) & = 0 \\
			\partial_z^2   F_3(0,L_{p 3}) & = \frac{4 }{p} \Tilde{\Phi}(\frac{L_{p 3}}{\sqrt{2}} )f'(\frac{L_{p 3}}{\sqrt{2}})\le 0.
		\end{aligned}
	\end{equation*}
	If $\delta_0=o(\sqrt{\frac{1}{\pi} }) $, we have $z=\sqrt{p}\delta\to 0$. By Taylor's theorem, we have
	$$\operatorname{Power}_{W_j}(L_{p j} ) = p + \E_{\mathbf{\Theta}} \partial_z   F_j(0,L_{p j})z +  \E_{\mathbf{\Theta}} \frac{1}{2}\partial_z^2   F_j(0,L_{p j})z^2 +o( \E_{\mathbf{\Theta}} z^2),$$
	(or $\le$ for $j=3$ ). Plugging in the derivatives of $j=1,2,3$, clearly we have $\operatorname{Power}_{W_1}(L_{p 1} )\ge \operatorname{Power}_{W_3}(L_{p 3} )$, and $\operatorname{Power}_{W_2}(L_{p 2} )\ge \operatorname{Power}_{W_3}(L_{p 3} )$; for the second order derivative of $F_1$ and $F_2$, we also have 
	\begin{equation*}
		\begin{aligned}
			\partial_z^2   F_1(0,L_{p 1})- \partial_z^2   F_2(0,L_{p 2}) 
			& = \frac{4 }{p} (2f(-\sqrt{2L_{p 1}}) -f^2(-L_{p 2}) -\Tilde{\Phi}(L_{p 2})f'(-L_{p 2})  ) \\
			&\ge c \frac{1}{p}\exp\left(- \frac{1}{2} \Tilde{\Phi}^{-1}(p/\sqrt{2})^2 \right)\left(1 - \exp\left(\frac{1}{2} \Tilde{\Phi}^{-1}(p/\sqrt{2})^2  - \Tilde{\Phi}^{-1}(\sqrt{\frac{p}{2} })^2 \right) \right).
		\end{aligned}
	\end{equation*}
	
	Since
	\begin{equation*}
		\begin{aligned}
			&\frac{1}{2} \Tilde{\Phi}^{-1}(p/\sqrt{2})^2  - \Tilde{\Phi}^{-1}(\sqrt{\frac{p}{2} })^2= (\frac{1}{\sqrt{2}} \Tilde{\Phi}^{-1}(p/\sqrt{2}) + \Tilde{\Phi}^{-1}(\sqrt{\frac{p}{2} }) )(\frac{1}{\sqrt{2}} \Tilde{\Phi}^{-1}(p/\sqrt{2}) - \Tilde{\Phi}^{-1}(\sqrt{\frac{p}{2} }) ) \\
			&\le (\frac{1}{\sqrt{2}} \Tilde{\Phi}^{-1}(p/\sqrt{2}) + \Tilde{\Phi}^{-1}(\sqrt{\frac{p}{2} }) ) \\
			& \cdot ( \sqrt{\log(\frac{\sqrt{2}}{p})  - \frac{1}{1+r_2}\log \log(\frac{\sqrt{2}}{p}) } - \sqrt{\log (\frac{2(1-r_1)^2}{p} ) -\log\log( (1-r_1)\sqrt{\frac{2}{p}} )  } ) \\
			&\to -\infty,
		\end{aligned}
	\end{equation*}
	we have $\partial_z^2   F_1(0,L_{p 1})- \partial_z^2   F_2(0,L_{p 2}) \ge 0$, thus $\operatorname{Power}_{W_1}(L_{p 1} )\ge \operatorname{Power}_{W_2}(L_{p 2} )$. Translating the $\operatorname{Power}_{W_j}(L_{p j} )$ to $\operatorname{Power}_{W_j}(L_{\alpha j} )$, we finish our proof. 
\end{proof}

\subsection{Proof of Lemma \ref{lemma:vanishing-part}}
\begin{proof}
    To show this, we first state the perturbation of singular subspaces with respect to different norms:
\begin{Lemma}[\cite{xia2021statistical}, Lemma 2]\label{lemma:subspace-perturb}
        Under good initialization and signal requirements, there exists an absolute constant $C>0$ such that conditional on $E_0$, if $n \geq C d_1 \log d_1$, then with probability at least $1- d_1^{-\tau}$,

$$
\norm{\widehat{Z}}\le C_2 \sqrt{\tau}(1+\gamma_{n})\sigma_{\xi}\sqrt{\frac{d_1^2 d_2 \log d_1}{n}},  
$$
and 
$$
\max \left\{ \norm{UU^\top-\widehat{U}\widehat{U}^\top } ,\norm{VV^\top-\widehat{V}\widehat{V}^\top } \right\} \leq C_2\frac{\sqrt{\tau}\left(1+\gamma_{n}\right) \sigma_{\xi}}{\lambda_{\min}} \cdot \sqrt{\frac{d_1^2 d_2 \log d_1}{n}}.  
$$
    \end{Lemma}

\begin{Lemma}[\cite{xia2021statistical}, Theorem 4]\label{lemma:subspace-perturb-max}
         Under good initialization and signal requirements, there exists an absolute constant $C>0$ such that conditional on $E_0$, if $n \geq C \mu^2 r d_1 \log d_1$, then with probability at least $1- 5\log d_1 \cdot d_1^{-\tau}$,
$$
\norm{UU^\top-\widehat{U}\widehat{U}^\top }_{2,\max}  \leq C_2 \mu\frac{\sqrt{\tau}\left(1+\gamma_{n}\right) \sigma_{\xi}}{\lambda_{\min}} \cdot \sqrt{\frac{rd_1 d_2 \log d_1}{n}},  
$$
and 
$$
\norm{VV^\top-\widehat{V}\widehat{V}^\top }_{2,\max}  \leq C_2\mu\frac{\sqrt{\tau}\left(1+\gamma_{n}\right) \sigma_{\xi}}{\lambda_{\min}} \cdot \sqrt{\frac{rd_1^2\log d_1}{n}}.  
$$
    \end{Lemma}
Then under the event when Lemma \ref{lemma:subspace-perturb} and Lemma \ref{lemma:subspace-perturb-max} both hold, it follows that

$$
\begin{aligned}
    \abs{\left\langle \widehat{U}\widehat{U}^\top \widehat{Z} \widehat{V}\widehat{V}^\top - UU^\top \widehat{Z}VV^\top, T \right\rangle} \le \norm{T}_{\ell_1} \norm{\widehat{U}\widehat{U}^\top \widehat{Z} \widehat{V}\widehat{V}^\top - UU^\top \widehat{Z}VV^\top}_{\max},
\end{aligned}
$$
and by triangular inequality,
$$
\begin{aligned}
    \norm{\widehat{U}\widehat{U}^\top \widehat{Z} \widehat{V}\widehat{V}^\top - UU^\top \widehat{Z}VV^\top}_{\max} \le  &   \norm{ (\widehat{U}\widehat{U}^\top-UU^\top) \widehat{Z}VV^\top}_{\max} + \norm{ UU^\top \widehat{Z}( \widehat{V}\widehat{V}^\top -VV^\top)}_{\max} \\
    &+  \norm{ (\widehat{U}\widehat{U}^\top-UU^\top)  \widehat{Z} ( \widehat{V}\widehat{V}^\top- VV^\top)}_{\max}\\
    \le & \left\|\widehat{Z}\right\| \left(\left\|\widehat{U} \widehat{U}^{\top}-U U^{\top}\right\|_{2,  { \max }}\|V\|_{2, { \max }} +\norm{\widehat{V}\widehat{V}^\top- VV^\top}_{2,\max} \norm{U}_{2, { \max }} \right) \\
    & + \left\|\widehat{Z}\right\| \left\|\widehat{U} \widehat{U}^{\top}-U U^{\top}\right\|_{2,  { \max }}\norm{\widehat{V}\widehat{V}^\top- VV^\top}_{2,\max} \\
    \le & C_2 \tau\mu^2 \frac{\sigma_{\xi}}{\lambda_{\min}} \sqrt{\frac{r d_1^2 d_2 \log d_1}{n}} \cdot \sigma_{\xi} \sqrt{\frac{r d_1 \log d_1}{n}}.
\end{aligned}
$$
Thus, we conclude that under the event with probability larger than $1-d^{-\tau}-5\log d_1 \cdot d_1^{-\tau}\ge 1-6 d_1^{-\tau}\log d_1$, the desired bound holds.
\end{proof}

\subsection{Proof of Lemma \ref{lemma:asymp-vanishing-part-1}}
\begin{proof}
% By Lemma \ref{lemma:subspace-perturb}, it is suggested that with probability at least $1- 2d_1^{-\tau}$, $\norm{\widehat{Z}_2}\le C\gamma_{n}\sigma_\xi \sqrt{\frac{d_1^2 d_2 \log d_1}{n}} $. Under this event, we have
Denote $I_1$ the index set of observations in the sample $\calD_1$. 
Since 
$$
\begin{aligned}
     &\left\langle U U^\top \widehat{Z}_2 V_{\perp} V_{\perp}^\top + U_{\perp} U^\top_{\perp} \widehat{Z}_2 VV^\top, T \right\rangle +\left\langle UU^\top \widehat{Z}_2VV^\top, T\right\rangle = \left\langle \widehat{Z}_2, \cP_M(T)  \right\rangle\\ 
     &= \frac{d_1d_2}{n}\sum_{i\in I_1 }\left\langle \widehat{\Delta}, X_i \right\rangle \left\langle X_i, \cP_M(T) \right\rangle - \left\langle \widehat{\Delta}, \cP_M(T) \right\rangle,
\end{aligned}
$$
and 
$$
\begin{aligned}
     &\abs{\frac{d_1d_2}{n}\left\langle \widehat{\Delta}, X_i \right\rangle \left\langle X_i, \cP_M(T) \right\rangle - \frac{1}{n}\left\langle \widehat{\Delta}, \cP_M(T) \right\rangle} \\
     & \le\sqrt{3} \frac{\norm{\widehat{\Delta}}_{\max} \mu \sqrt{rd_1^2 d_2} \norm{\cP_M(T)}_\tF +\sqrt{d_1 d_2}\norm{\widehat{\Delta}}_{\max}\norm{\cP_M(T)}_\tF  }{n}, \\
     &\E \abs{\frac{d_1d_2}{n}\left\langle \widehat{\Delta}, X_i \right\rangle \left\langle X_i, \cP_M(T) \right\rangle - \frac{1}{n}\left\langle \widehat{\Delta}, \cP_M(T) \right\rangle}^2 \le \frac{d_1 d_2 \norm{\widehat{\Delta}}_{\max}^2 \norm{\cP_M(T)}_\tF^2 }{n^2},
\end{aligned}
$$
combined with initialization assumption and, by Bernstein inequality, we have 
$$
\begin{aligned}
     &\abs{\frac{d_1d_2}{n}\sum_{i\in I_1 }\left\langle \widehat{\Delta}, X_i \right\rangle \left\langle X_i, \cP_M(T) \right\rangle - \left\langle \widehat{\Delta}, \cP_M(T) \right\rangle}\le C \gamma_{n}\sigma_\xi\norm{\cP_M(T)}_\tF \sqrt{\frac{\tau d_1 d_2\log d_1 }{n}}, 
\end{aligned}
$$
with probability at least $1- 2 d^{-\tau}$. Here $n \geq C \mu_{ }^2 r d_1 \log d_1$.
\end{proof}

\subsection{Proof of Lemma \ref{lemma:cov-est-prec}}
\begin{proof}
Notice that both $\left(I_{d_1 d_2} - \widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top\right)$ and $\left(I_{d_1 d_2} -  U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)$ are projection matrices with $P=P^2$. We thus have
\begin{equation}\label{eq:decomp-detS}
   \begin{aligned}
     \widehat{\Sigma}-\Sigma&=T_{\calH}\left((I_{d_1 d_2} - \widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top  )-(I_{d_1 d_2} -  U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top ) \right) T_{\calH}^\top \\
     & = T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \left(I-U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) T_{\calH}^\top\\
     & + T_{\calH}\left(I-U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top \right) T_{\calH}^\top \\
     & + T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) T_{\calH}^\top.\\
    % & = T_{\calH}\left(\right) \left(\right) T_{\calH}^\top
\end{aligned} 
\end{equation}
We apply \eqref{eq:decomp-detS} to the error $\Sigma^{-\frac{1}{2}}(\widehat{\Sigma}-\Sigma)\Sigma^{-\frac{1}{2}}$:
\begin{equation*}
    \begin{aligned}
      &\norm{\Sigma^{-\frac{1}{2}}(\widehat{\Sigma}-\Sigma)\Sigma^{-\frac{1}{2}} }\le 2\norm{\Sigma^{-\frac{1}{2}}T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \left(I-U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) T_{\calH}^\top \Sigma^{-\frac{1}{2}} }\\
      &+  \norm{\Sigma^{-\frac{1}{2}}T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) T_{\calH}^\top \Sigma^{-\frac{1}{2}}}.
    \end{aligned}
\end{equation*}
Notice that $\norm{\left(I-U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) T_{\calH}^\top \Sigma^{-\frac{1}{2}}}\le 1$. We only need to focus on the term
\begin{equation*}
    \begin{aligned}
      &\norm{\Sigma^{-\frac{1}{2}}T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)}  \\
      & \le  \norm{\Sigma^{-\frac{1}{2}} T_{\calH} }\norm{\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)} \\
      & \le \sqrt{\kappa_1}\kappa_T\left( \norm{ \left(\widehat U_\perp \widehat U_\perp^\top -U_\perp U_\perp^\top \right)\otimes V_\perp V_\perp^\top} +\norm{U_\perp U_\perp^\top \otimes \left( \widehat V_\perp \widehat V_\perp^\top-V_\perp V_\perp^\top\right) }\right. \\
      & + \left.\norm{ \left(\widehat U_\perp \widehat U_\perp^\top -U_\perp U_\perp^\top \right)\otimes \left( \widehat V_\perp \widehat V_\perp^\top-V_\perp V_\perp^\top\right)}\right)\\
      & \le C_2 \sqrt{\kappa_1}\kappa_T \frac{\sqrt{\tau}\left(1+\gamma_{n}\right) \sigma_{\xi}}{\lambda_{\min}} \cdot \sqrt{\frac{d_1^2 d_2 \log d_1}{n}},  
    \end{aligned}
\end{equation*}
where we use the definition of $\kappa_T$ and the perturbation of singular subspaces in Lemma \ref{lemma:subspace-perturb}. Moreover, when $T_{\calH}$ is sparse, we use $e_{T, k}\in \R^{d_1\times d_2} $, $k\in[\operatorname{supp}(T_{\calH} ) ]$ to indicate the collective supports of  all the $\operatorname{vec}(T_i)$. We then have
\begin{equation}\label{eq:sparse-T}
    \begin{aligned}
      &\norm{\Sigma^{-\frac{1}{2}}T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)}  \\
          & = \norm{\Sigma^{-\frac{1}{2}}T_{\calH} \sum_{k=1}^{ \operatorname{supp}(T_{\calH} )  } e_{T, k}e_{T,k}^\top  \left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)} \\
          & \le \sqrt{\kappa_1}\kappa_T \operatorname{supp}(T_{\calH} ) \max_{k}\norm{e_{T,k}^\top  \left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)} \\
          & \le \sqrt{\kappa_1}\kappa_T\operatorname{supp}(T_{\calH} )  \max_{k} \left( \norm{e_{T,k}^\top  \left(\widehat U_\perp \widehat U_\perp^\top -U_\perp U_\perp^\top \right)\otimes V_\perp V_\perp^\top}\right. \\
          &  +\norm{e_{T,k}^\top  U_\perp U_\perp^\top \otimes \left( \widehat V_\perp \widehat V_\perp^\top-V_\perp V_\perp^\top\right) }+ \left.\norm{ e_{T,k}^\top \left(\widehat U_\perp \widehat U_\perp^\top -U_\perp U_\perp^\top \right)\otimes \left( \widehat V_\perp \widehat V_\perp^\top-V_\perp V_\perp^\top\right)}\right).\\
    \end{aligned}
\end{equation}
Since each $e_{T, k}$ can also be represented as $e_{T, k} = e_{T, k}^1 \otimes  e_{T, k}^2$, where $e_{T, k}^1\in\R^{d_1}$ and  $e_{T, k}^2\in\R^{d_2}$ are also canonical bases, we then have 
\begin{equation*}
    \begin{aligned}
      &\norm{\Sigma^{-\frac{1}{2}}T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)} \\
          &  \lesssim \sqrt{\kappa_1}\kappa_T\operatorname{supp}(T_{\calH} )  \left(\norm{UU^\top-\widehat{U}\widehat{U}^\top }_{2,\max} + \norm{VV^\top-\widehat{V}\widehat{V}^\top }_{2,\max} \right)  \\
          & \le C \sqrt{\kappa_1}\kappa_T\frac{\operatorname{supp}(T_{\calH} )}{\sqrt{d_2}} \frac{\sqrt{\tau}\left(1+\gamma_{n}\right) \sigma_{\xi}}{\lambda_{\min}} \cdot \sqrt{\frac{d_1^2 d_2 \log d_1}{n}},
    \end{aligned}
\end{equation*}
because the higher-order error can be dominated.  The rate $\gamma_{n}$ converges to 0, which means that the whole error can be controlled by:
\begin{equation*}
   \norm{ \Sigma^{-\frac{1}{2}}(\widehat{\Sigma}-\Sigma)\Sigma^{-\frac{1}{2}}}\le C\frac{\kappa_T \sigma_\xi }{\lambda_{\min} } \cdot \left(\frac{\operatorname{supp}(T_{\calH} )}{\sqrt{d_2}}\wedge 1\right) \sqrt{\frac{ \kappa_1  d_1^2 d_2 \log d_1 }{n}}.
\end{equation*}
\end{proof}



\subsection{Proof of Lemma \ref{lemma:pres-DSD} }
\begin{proof}
Denote $E=\Sigma-\widehat{\Sigma}$. By Fréchet derivative, as long as $\norm{E}=\norm{\Sigma-\widehat{\Sigma}}$ is small for any operator norm,  $\widehat{\Sigma}^{-1}-{\Sigma}^{-1}$ can be dominated by its Fréchet derivative ${\Sigma}^{-1}E{\Sigma}^{-1}$. Therefore, We have
\begin{equation*}
            \norm{{D} (\widehat{\Sigma}^{-1}-{\Sigma}^{-1}) {D}}_{\infty}\le \norm{{D}{\Sigma}^{-1}E{\Sigma}^{-1}{D}}_\infty+o(\norm{E}_\infty).
\end{equation*}
We only need to study the convergence rate of $\norm{{D}{\Sigma}^{-1}E{\Sigma}^{-1}{D}}_\infty$ as $E$ is small. This term, however, can be decomposed following \eqref{eq:decomp-detS}, i.e.,
\begin{equation*}
    \begin{aligned}
     & \norm{{D}{\Sigma}^{-1}E{\Sigma}^{-1}{D}}_\infty \\
       & \le  \norm{{D}{\Sigma}^{-1}D}_\infty \norm{D^{-1}
       T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \left(I-U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) T_{\calH}^\top {\Sigma}^{-1}{D}
       }_{\infty}\\
     & + 
     \norm{{D}{\Sigma}^{-1} T_{\calH}\left(I-U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top \right) T_{\calH}^\top {\Sigma}^{-1}{D}
     }_\infty
     \\
     & + \norm{{D}{\Sigma}^{-1}T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) T_{\calH}^\top{\Sigma}^{-1}{D}}_\infty \\
     & \le \kappa_\infty \sqrt{q} \norm{ D^{-1}T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) }_{2,\max} \sqrt{\kappa_1} \\
     & + \sqrt{\kappa_1}\sqrt{q}\norm{ \left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top \right) T_{\calH}^\top {\Sigma}^{-1}{D}
     }\\
     & + q \kappa_1^2 \norm{ D^{-1}T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) }_{2,\max}^2 \\
     & \le C\left(\kappa_\infty\sqrt{\kappa_1} +\norm{T_{\calH}}_2\kappa_1/\sqrt{\lambda_{\min}(\Sigma)} \right)\frac{\beta_T \mu \sigma_\xi }{\beta_0 \lambda_{\min} }\sqrt{\frac{\alpha_d q d_1^2 d_2 \log d_1 }{n}} \\
     & \le C\left(\kappa_\infty\sqrt{\kappa_1} +\kappa_1^{1.5}\kappa_T \right)\frac{\beta_T \mu \sigma_\xi }{\beta_0 \lambda_{\min} }\sqrt{\frac{\alpha_d q d_1^2 d_2 \log d_1 }{n}}, \\
    \end{aligned} 
\end{equation*}
where the 2-max norm here can be bounded by:
\begin{equation}
    \begin{aligned}
      &\norm{ D^{-1}T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) }_{2,\max} \\
      &\le \max_{T_i\in \cH }\frac{ \sum_{j\in[d_1] } \sum_{k\in [d_2] } \abs{T_i(j,k) \left[\left(U U^\top-\widehat U \widehat U^\top\right)\otimes V_{\perp}V_\perp^\top +  U_\perp U_\perp^\top \otimes \left(\widehat V \widehat V^\top -V V^\top \right) \right]\cdot e_j \otimes e_k} }{s_{T_i}  }\\
      & \quad + \max_{T_i\in \cH }\frac{ \sum_{j\in[d_1] } \sum_{k\in [d_2] } \abs{T_i(j,k) \left(U U^\top-\widehat U \widehat U^\top\right)\otimes \left(\widehat V \widehat V^\top -V V^\top \right) \cdot e_j \otimes e_k} }{s_{T_i}  }\\
      & \le C\max_{T_i\in \cH } \frac{\norm{T}_{\ell_1 } }{ \norm{T}_\tF \beta_0 \sqrt{r/d_1} } \frac{ \mu\left(1+\gamma_{n}\right) \sigma_{\xi}}{\lambda_{\min}} \cdot \sqrt{\frac{rd_1^2 \log d_1}{n}}   \\
      & \le C\frac{\beta_T \mu \sigma_\xi }{\beta_0 \lambda_{\min} }\sqrt{\frac{\alpha_d d_1^2 d_2 \log d_1 }{n}}.
    \end{aligned}
\end{equation}
Here, we use the 2-max norm bound in Lemma \ref{lemma:subspace-perturb-max}, the alignment assumption, and the definition of $\kappa_T$. Moreover, the norm $\norm{ \left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top \right) T_{\calH}^\top {\Sigma}^{-1}{D}
     }$ can also bounded by 

\begin{equation*}
    \begin{aligned}
      &\norm{D\Sigma^{-1}T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right) \left(I-U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)}  \\
      & \le \norm{D\Sigma^{-\frac{1}{2} }\cdot \Sigma^{-\frac{1}{2} }T_{\calH}\left(\widehat U_\perp \widehat U_\perp^\top \otimes \widehat V_\perp \widehat V_\perp^\top - U_\perp U_\perp^\top \otimes V_\perp V_\perp^\top\right)}\\
          &  \lesssim {\kappa_1}\kappa_T\operatorname{supp}(T_{\calH} )  \left(\norm{UU^\top-\widehat{U}\widehat{U}^\top }_{2,\max} + \norm{VV^\top-\widehat{V}\widehat{V}^\top }_{2,\max} \right)  \\
          & \le C \kappa_1\kappa_T\frac{\operatorname{supp}(T_{\calH} )}{\sqrt{d_2}} \frac{\sqrt{\tau}\left(1+\gamma_{n}\right) \sigma_{\xi}}{\lambda_{\min}} \cdot \sqrt{\frac{d_1^2 d_2 \log d_1}{n}},
    \end{aligned}
\end{equation*}
where we use the sparsity of $T_{\calH}$ following \eqref{eq:sparse-T}. This gives the desired bound 
\begin{equation*}
    \norm{{D}{\Sigma}^{-1}E{\Sigma}^{-1}{D}}_\infty \le C\left(\kappa_\infty\sqrt{\kappa_1} +\kappa_1^{1.5}\kappa_T\left( \frac{\operatorname{supp}(T_{\calH} )}{\sqrt{d_2}}\wedge 1 \right)  \right)\frac{\beta_T \mu \sigma_\xi }{\beta_0 \lambda_{\min} }\sqrt{\frac{\alpha_d q d_1^2 d_2 \log d_1 }{n}}.
\end{equation*}

\end{proof}

%\section{Additional Numerical Details}
%\subsection{Motivating Example}
%%%%%%%%%% to be updated
%In the motivating example, we deal with Book-Crossing dataset for the recommendation. The raw data consists of tons of implicit ratings (i.e., 0-rating). In the matrix completion, we remove the implicit rating data. In the implementation, we treat $r=5$. For the tests, since many of the entries are unobserved, we treat each $M_{ij}$ as the observation value if it is observed, and $\widehat{M}_{\mathsf{init} }$ (obtained by Riemannian GD of full data) if it is not observed. In the multiple testing, we choose $p=0.08$ as the proportion of non-null tests and positive signals with average strength $M_{ij}-m_{ij}=2$. By randomly assigning non-null signals with varied signal strength, the non-null set is of size $q_1=37$. Since the matrix is sparse, in practice, to maintain good completion, we choose the 500 most popular books and the 500 users who rate the most. By selection, we have the data of size $N=10558$. In this example, Our Method means the Algorithm \ref{alg:matrix-fdr}. To keep the consistency of methods, here we actually use the un-scaled version of test statistics with no factor $d_1 d_2/n$ in the debiasing step to keep better symmetricity.
%
%\subsection{MovieLens}
%In the numerical experiments dealing with MovieLens data, we use fast  Riemannian optimization for low-rank matrix recovery \citep{wei2016guarantees} to find a good matrix in the low-rank space as a proxy of the observed matrix. We initialize the Riemannian GD by the spectral method and then perform Riemannian GD with geometrically decaying stepsize. The Riemannian GD generally converges fast within $30$ iterations. Figure \ref{fig:supp-riemannian} shows the convergence of Riemannian GD and the entrywise recovery. Here the entrywise recovery is measured by the proportion of recovered entries for the observed data given certain tolerance $\delta$:
%
%\begin{equation*}
%    \frac{ \sum_{i} \mathbb{I} \left( \abs{Y_i-\left\langle\widehat{M}_{\mathsf{init}} , X_i \right\rangle} \le \delta\right)   }{N}
%\end{equation*}
%
%
%\begin{figure}
%\centering
%\begin{subfigure}{0.4\textwidth}
%     \includegraphics[width=\textwidth]{figures/conv_riemannian.png}
%     \caption{Convergence of Riemannian GD }
%     \label{fig:riemannian-convergence}
% \end{subfigure}
% \begin{subfigure}{0.4\textwidth}
%     \includegraphics[width=\textwidth]{figures/recover_riemannian.png}
%     \caption{Entrywise recovery of Riemannian GD }
%     \label{fig:riemannian-recovery}
% \end{subfigure}
% \caption{Performance of Riemannian Gradient Descent on the MovieLens dataset. The matrix recovery is good enough so that most of the entries can be recovered with tolerance $\delta=1$.}
% \label{fig:supp-riemannian}
%\end{figure}
%To ensure enough signals, in the selection of entry pairs, we choose the observed pairs with $M(i,j)-M(i,j+1)\ge 2$.
%
%\subsection{Rossmann sales dataset}
%Since in practice, the full dataset may violate the low-rank assumption and, together with the intricate observation errors, we use the following data processing approach: we first centralize all the  observations and rescale them by $10^3$; then we project the original observation matrix into a medium-rank structure with $r=100$ which contains most of the large singular values of the original data matrix. In the matrix recovery, we choose a small rank with $r=30$ to compress information.
%
%In this case, we also use fast  Riemannian optimization to initialize or algorithm. The initialization behavior is given in Figure \ref{fig:supp-riemannian-Ross}.
%\begin{figure}
%\centering
%\begin{subfigure}{0.4\textwidth}
%     \includegraphics[width=\textwidth]{figures/conv_riemannian-Ross.png}
%     \caption{Convergence of Riemannian GD }
%     \label{fig:riemannian-convergence-Ross}
% \end{subfigure}
% \begin{subfigure}{0.4\textwidth}
%     \includegraphics[width=\textwidth]{figures/recover_riemannian-Ross.png}
%     \caption{Entrywise recovery of Riemannian GD }
%     \label{fig:riemannian-recovery-Ross}
% \end{subfigure}
% \caption{Performance of Riemannian Gradient Descent on Rossmann sales dataset with random masking. The matrix recovery is good enough so that most of the entries can be recovered with tolerance $\delta=0.5$. }
% \label{fig:supp-riemannian-Ross}
%\end{figure}


\end{document}