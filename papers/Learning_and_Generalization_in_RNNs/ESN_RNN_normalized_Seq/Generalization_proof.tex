\section{Optimization and Generalization: proofs}\label{sec:optim_general_proofs}
\subsection{Proof of lemma~\ref{lem:trainloss}}
\begin{lemma}[Restating lemma~\ref{lem:trainloss}]\label{lem:trainloss_proof}
	For a constant $\varepsilon_x = \frac{1}{\operatorname{poly}(\rho)}$ and for every constant $\varepsilon \in \left(0, \frac{1}{p \cdot \operatorname{poly}(\rho) \cdot \mathfrak{C}_{\mathfrak{s}}(\Phi, \mathcal{O}(\varepsilon_x^{-1}))}\right),$ there exists $C^{\prime}=\mathfrak{C}_{\varepsilon}(\Phi, \mathcal{O}(\varepsilon_x^{-1}))$ and a parameter $\lambda=\Theta\left(\frac{\varepsilon}{L \rho}\right)$
	so that, as long as $m \geq \operatorname{poly}\left(C', p, L, \dout , \varepsilon^{-1}\right)$ and $N \geq \Omega\left(\frac{\rho^{3} p C_{\Phi}^2}{\varepsilon^2}\right),$ setting learning rate $\eta=\Theta\left(\frac{1}{\varepsilon \rho^{2} m}\right)$ and
	$T=\Theta\left(\frac{p^{2}  C'^2 \mathrm{poly}(\rho)}{\varepsilon^{2}}\right),$ we have
	\begin{align*}
		\underset{\mathrm{sgd}}{\mathbb{E}}\Big[\frac{1}{T} \sum_{t=0}^{T-1}  \underset{(\obx, \mathbf{y}^{\ast}) \sim \mathcal{Z}}{\mathbb{E}} \mathrm{Obj}(\obx, \mathbf{y}^{\ast};  \mathbf{W}+\mathbf{W}_{t}, \mathbf{A} + \mathbf{A}_t) \Big] \leq \mathrm{OPT} + \frac{\varepsilon}{2} + \frac{1}{\mathrm{poly}(\rho)},
	\end{align*}
	and $\left\|W_{t}\right\|_{F} \leq \frac{\Delta}{\sqrt{m}}$ for $\Delta=\frac{C'^{2} p^{2} \mathrm { poly }(\rho)}{\varepsilon^{2}}$.
\end{lemma}

\begin{proof}
	The proof will follow exactly the same routine as lemma 7.1 in \cite{allen2019can}. We allow $\mathbf{A}$ to change, which leads to changes in the proof. We outline the major differences here for completeness. For simplicity, we outline the proof for Gradient Descent.
	
	The training objective is given by
	\begin{align*}
		\mathrm{Obj}( \mathbf{W}_{t},  \mathbf{A}_t) &= \underset{\left(\obx, y^{\ast}\right) \sim \mathcal{Z}}{\mathbb{E}} \mathrm{Obj}(\obx, y^{\ast};  \mathbf{W}_{t},  \mathbf{A}_t),\text{ where } \\
		\mathrm{Obj}(\obx, y^{\ast};  \mathbf{W}_{t},  \mathbf{A}_t) &= G(\lambda F^{(\ell)}_{\mathrm{rnn}}(\bx, y^{\ast};  \mathbf{W} + \mathbf{W}_{t},  \mathbf{A} + \mathbf{A}_t)).
	\end{align*}
	Let $\obx$ be a true sequence and $\bx$ be its normalized version. Let's consider the matrices $\mathbf{W}+\mathbf{W}_t$, $\mathbf{A}+\mathbf{A}_t$ after SGD iteration $t$. Let
	\begin{itemize}
		\item  at RNN cell $i$,  $\mathbf{h}^{(i)}$,  $\mathbf{Back}_{i \to L}$ and  $\mathbf{D}^{(i)}$ are defined w.r.t. $\mathbf{A}, \mathbf{W}, \mathbf{B}, \bx$.
		\item at RNN cell $i$,  $\mathbf{h}^{(i)} + \mathbf{h}^{(i)}_t$,  $\mathbf{Back}_{i \to L} + \mathbf{Back}_{i \to L, t}$ and  $\mathbf{D}^{(i)} + \mathbf{D}^{(i)}_t$ are defined w.r.t. $\mathbf{A} + \mathbf{A}_t, \mathbf{W} + \mathbf{W}_t, \mathbf{B}, \bx$.
	\end{itemize}
	
	Define the following regularization term:
	\begin{align*}
		R(\bx; \mathbf{W}', \mathbf{A}') &= \sum_{i = 2}^{L} (\mathbf{Back}_{i \to L} + \mathbf{Back}_{i \to L, t}) (\mathbf{D}^{(i)} + \mathbf{D}^{(i)}_t) (\mathbf{W}'(\mathbf{h}^{(i-1)} +  \mathbf{h}^{(i-1)}_t)  + \mathbf{A}' \bx^{(i)}), \\&
		= \sum_{i = 2}^{L} (\mathbf{Back}_{i \to L} + \mathbf{Back}_{i \to L, t}) (\mathbf{D}^{(i)} + \mathbf{D}^{(i)}_t) \left(\left[\mathbf{W}', \mathbf{A}'\right]_r \left[\mathbf{h}^{(i-1)} +  \mathbf{h}^{(i-1)}_t, \bx^{(i)}\right]\right)
	\end{align*}
	which is a linear function over $[\mathbf{W}', \mathbf{A}']_r$. Define the following regularized loss function:
	\begin{align*}
		\widetilde{G}(\mathbf{W}', \mathbf{A}') &= \underset{\left(\obx, y^{\ast}\right) \sim \mathcal{Z}}{\mathbb{E}} \widetilde{G}(\obx, y^{\ast};  \mathbf{W}',  \mathbf{A}'),\text{ where } \\ 
		\widetilde{G}(\obx, y^{\ast};  \mathbf{W}',  \mathbf{A}') &= G(\lambda F^{(L)}_{\mathrm{rnn}}(\obx;  \mathbf{W} + \mathbf{W}_t,  \mathbf{A} + \mathbf{A}_t) + \lambda  R(\bx; \mathbf{W}', \mathbf{A}'))
	\end{align*}
	Note that,
	$ \widetilde{G}(\mathbf{0}, \mathbf{0}) = \mathrm{Obj}(\mathbf{W}_t, \mathbf{A}_t)$ and $\nabla_{[\mathbf{W}', \mathbf{A}']} \widetilde{G}(\mathbf{0}, \mathbf{0}) = \nabla_{[\mathbf{W}_t, \mathbf{A}_t]} \mathrm{Obj}(\mathbf{W}_t, \mathbf{A}_t)$. First of all, %using lemma~\ref{lemma:perturb_NTK_small_output} and lemma~\ref{lemma:perturb_NTK_small}, 
	we have
	\begin{align*}
		&F^{(\ell)}_{\mathrm{rnn}}(\bx;  \mathbf{W} + \mathbf{W}_t,  \mathbf{A} + \mathbf{A}_t) - F^{(\ell)}_{\mathrm{rnn}}(\bx;  \mathbf{W},  \mathbf{A}) \tag{difference between RNN output}\\&=  \sum_{\ell \in [L]} \mathbf{Back}_{\ell \rightarrow L}  \mathbf{D}^{(\ell)} \left(\mathbf{W}_t \mathbf{h}^{(\ell-1)} + \mathbf{A}_t \mathbf{x}^{(\ell)} \right) + \varepsilon' \tag{using lemma~\ref{lemma:perturb_NTK_small_output}}\\&
		= \sum_{\ell \in [L]}\left(\mathbf{Back}_{\ell \rightarrow L}+\mathbf{Back}_{\ell \rightarrow L, t}\right)\left(\mathbf{D}^{(\ell)}+\mathbf{D}^{(\ell)}_t\right) \left(\mathbf{W}_t\left(\mathbf{h}^{(\ell-1)}+h^{(\ell-1)}_t\right) + \mathbf{A}_t \mathbf{x}^{(\ell)} \right) + \varepsilon' + \varepsilon'' \tag{using lemma~\ref{lemma:perturb_NTK_small}}\\&
		=  R(\bx; \mathbf{W}_t, \mathbf{A}_t) + \varepsilon' + \varepsilon'' \tag{using the definition of $R$},
	\end{align*}
	where $0 \le \varepsilon', \varepsilon'' \le \mathcal{O}(\rho^7 \Delta^{4/3} m^{-1/6})$. Also, from lemma~\ref{lemma:perturb_small_target}, we have
	\begin{align*}
	    R(\bx; \mathbf{W}^{\ast}, \mathbf{A}^{\ast}) = F^{\ast}(\obx) \pm \varepsilon''', \label{eq:RFast}
	\end{align*}
	with $\varepsilon''' \le \varepsilon/2 + poly(\rho)^{-1}$, when $\varepsilon_x \le poly(\rho)^{-1}$, $\varepsilon < (p \cdot poly(\rho) \cdot \mathfrak{C}_s(\Phi, \mathcal{O}(\varepsilon_x^{-1})))^{-1}$ and $m \ge poly(\varrho)$.
	Hence, 
	\begin{align}
		& \widetilde{G}\left(\frac{1}{\lambda} \mathbf{W}^{\ast} - \mathbf{W}_t, \frac{1}{\lambda}\mathbf{A}^{\ast} - \mathbf{A}_t\right) \nonumber\\&= G\left(\lambda F_{\mathrm{rnn}}^{(L)}\left(\bx;  \mathbf{W} + \mathbf{W}_t,  \mathbf{A} + \mathbf{A}_t\right) + \lambda  R\left(\bx; \frac{1}{\lambda} \mathbf{W}^{\ast} - \mathbf{W}_t, \frac{1}{\lambda}\mathbf{A}^{\ast} - \mathbf{A}_t \right)\right) \nonumber\\&
		= G\left(\lambda F_{\mathrm{rnn}}^{(L)}\left(\bx;  \mathbf{W},  \mathbf{A}\right) + \lambda R\left(\bx;  \mathbf{W}_t, \mathbf{A}_t \right) + \lambda  R\left(\bx; \frac{1}{\lambda} \mathbf{W}^{\ast} - \mathbf{W}_t, \frac{1}{\lambda}\mathbf{A}^{\ast} - \mathbf{A}_t \right)\right) \pm \varepsilon' \pm \varepsilon'' \tag{using the difference of  $F_{\mathrm{rnn}}^{(L)}$ derived above} \nonumber\\&
		=  G\left(\lambda F_{\mathrm{rnn}}^{(L)}\left(\obx;  \mathbf{W},  \mathbf{A}\right) +  R\left(\bx;  \mathbf{W}^{\ast}, \mathbf{A}^{\ast} \right)\right) \pm \varepsilon' \pm \varepsilon'' \nonumber\\&
		= G\left(R\left(\bx;  \mathbf{W}^{\ast}, \mathbf{A}^{\ast} \right)\right) \pm \varepsilon' \pm \varepsilon'' \pm \varepsilon \label{eq:optimality_step3}
		\\&= G(F^{\ast}(\bx)) \pm \varepsilon' \pm \varepsilon'' \pm \varepsilon \pm \varepsilon''' \tag{using eq.~\ref{eq:RFast}}\\&= OPT + \mathcal{O}(\varepsilon) + \frac{1}{poly(\rho)},\label{eq:optimality}
	\end{align}
	after setting everything properly. Here $\lambda$ is chosen above such that with high probability $\abs{\lambda F_{\mathrm{rnn}}^{(L)}\left(\obx;  \mathbf{W},  \mathbf{A}\right)} \le \varepsilon$ to get eq.~\ref{eq:optimality_step3}.
	
	Now, at each step of gradient descent, we have
	\begin{align*}
		[\mathbf{W}_{t+1}, \mathbf{A}_{t+1}] &= [\mathbf{W}_{t}, \mathbf{A}_{t}] - \eta \nabla_{[\mathbf{W}_{t}, \mathbf{A}_{t}]} \mathrm{Obj}(\mathbf{W}_{t}, \mathbf{A}_{t}) \\&
		= [\mathbf{W}_{t}, \mathbf{A}_{t}] - \eta \nabla_{[\mathbf{W}', \mathbf{A}']} \widetilde{G}(\mathbf{0}, \mathbf{0}) \tag{using the equivalence between gradient derived above}. 
	\end{align*}
	Hence, we have
	\begin{align*}
		\norm{[\mathbf{W}_{t+1}, \mathbf{A}_{t+1}] - \frac{1}{\lambda}[\mathbf{W}^{\ast}, \mathbf{A}^{\ast}]}^2 &= \norm{[\mathbf{W}_{t}, \mathbf{A}_{t}] - \frac{1}{\lambda}[\mathbf{W}^{\ast}, \mathbf{A}^{\ast}]}^2 + \norm{[\mathbf{W}_{t+1}, \mathbf{A}_{t+1}] - [\mathbf{W}_t, \mathbf{A}_t]}^2 \\&+ 2 \left\langle [\mathbf{W}_{t+1}, \mathbf{A}_{t+1}] - [\mathbf{W}_t, \mathbf{A}_t],  [\mathbf{W}_{t}, \mathbf{A}_{t}] - \frac{1}{\lambda}[\mathbf{W}^{\ast}, \mathbf{A}^{\ast}] \right\rangle \\&
		=  \norm{[\mathbf{W}_{t}, \mathbf{A}_{t}] - \frac{1}{\lambda}[\mathbf{W}^{\ast}, \mathbf{A}^{\ast}]}^2 + \eta^2 \norm{\nabla_{[\mathbf{W}', \mathbf{A}']} \widetilde{G}(\mathbf{0}, \mathbf{0})}^2 \tag{from descent update}
		\\& - 2\eta \left\langle\nabla_{[\mathbf{W}', \mathbf{A}']} \widetilde{G}(\mathbf{0}, \mathbf{0}),  [\mathbf{W}_{t}, \mathbf{A}_{t}] - \frac{1}{\lambda}[\mathbf{W}^{\ast}, \mathbf{A}^{\ast}] \right\rangle \\&
		\ge \norm{[\mathbf{W}_{t}, \mathbf{A}_{t}] - \frac{1}{\lambda}[\mathbf{W}^{\ast}, \mathbf{A}^{\ast}]}^2 + \eta^2 \norm{\nabla_{[\mathbf{W}', \mathbf{A}']} \widetilde{G}(\mathbf{0}, \mathbf{0})}^2 \\& - 2\eta \left(\widetilde{G}(\mathbf{W}_t - \frac{1}{\lambda} \mathbf{W}^{\ast}, \mathbf{A}_t - \frac{1}{\lambda} \mathbf{A}^{\ast}) - \widetilde{G}(0, 0)\right) 
		\tag{using the convexity of $\widetilde{G}$}\\&
		= \norm{[\mathbf{W}_{t}, \mathbf{A}_{t}] - \frac{1}{\lambda}[\mathbf{W}^{\ast}, \mathbf{A}^{\ast}]}^2 + \eta^2 \norm{\nabla_{[\mathbf{W}_t, \mathbf{A}_t]} \mathrm{Obj}(\mathbf{W}_t, \mathbf{A}_t)}^2 \\& - 2\eta \left(OPT +\mathcal{O}(\varepsilon) + \frac{1}{poly(\rho)} - \mathrm{Obj}(\mathbf{W}_t, \mathbf{A}_t)\right) \tag{using eq.~\ref{eq:optimality}}.
	\end{align*}
 Thus, 
	\begin{align*}
		\frac{1}{T} \sum_{t \in [T]} \mathrm{Obj}(\mathbf{W}_t, \mathbf{A}_t) &\le  \frac{1}{2\eta T}  \left(\norm{[\mathbf{W}_T, \mathbf{A}_T] - \frac{1}{\lambda}[\mathbf{W}^{\ast}, \mathbf{A}^{\ast}]}^2 - \norm{ \frac{1}{\lambda}[\mathbf{W}^{\ast}, \mathbf{A}^{\ast}]}^2 \right)  \\& + \frac{\eta}{2T} \sum_{t \in [T]} \norm{\nabla_{[\mathbf{W}_t, \mathbf{A}_t]} \mathrm{Obj}(\mathbf{W}_t, \mathbf{A}_t)}^2 + OPT + \frac{1}{poly(\rho)} +  \mathcal{O}(\varepsilon).
	\end{align*}
	We can then finish the proof by bounding $ \norm[1]{\nabla_{[\mathbf{W}_t, \mathbf{A}_t]} \mathrm{Obj}(\mathbf{W}_t, \mathbf{A}_t)} \approx \norm[1]{\nabla \mathrm{Obj}(\mathbf{0}, \mathbf{0})} \le \mathcal{O}(\lambda \rho^2 \sqrt{m})$ for $\mathbf{W}_t, \mathbf{A}_t \le \frac{\Delta}{\sqrt{m}}$. We then show that $\Delta =  C'^2 poly(\rho) \varepsilon^{-2}$, since it can be bounded by the term $\eta T \sqrt{m} \sup_{t \in [T]} \cdot \norm[1]{\nabla_{[\mathbf{W}_t, \mathbf{A}_t]} \mathrm{Obj}(\mathbf{W}_t, \mathbf{A}_t)}$.
	%Since, $\widetilde{\mathbf{G}}$ is a linear function in $[\mathbf{W}', \mathbf{A}']$
\end{proof}




\subsection{Helping lemmas}

\begin{lemma} \label{lemma:perturb_NTK_small} [first order coupling]
	Let $\mathbf{W}, \mathbf{A}, \mathbf{B}$ be at random initialization, $\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(L)}$ be a fixed normalized input sequence, and $\Delta \in\left[\varrho^{-100}, \varrho^{100}\right] .$ With probability at least $1-e^{-\Omega(\rho)}$ over $\mathbf{W}, \mathbf{A}, \mathbf{B}$ the following holds. Given any matrices $W^{\prime}$ with $\left\|\mathbf{W}^{\prime}\right\|_{2} \leq \frac{\Delta}{\sqrt{m}},$ $\mathbf{A}^{\prime}$ with $\left\|\mathbf{A}^{\prime}\right\|_{2} \leq \frac{\Delta}{\sqrt{m}},$ and any $\widetilde{\mathbf{W}}$ with $\|\widetilde{\mathbf{W}}\|_{2} \leq \frac{\omega}{\sqrt{m}},$ $\widetilde{\mathbf{A}}$ with $\|\widetilde{\mathbf{A}}\|_{2} \leq \frac{\omega}{\sqrt{m}},$ letting
	$\mathbf{h}^{(\ell)}, \mathbf{D}^{(\ell)},$  $\mathbf{Back}_{i \rightarrow L}$ be defined with respect to $\mathbf{W}, \mathbf{A}, \mathbf{B}, \obx,$ and $\mathbf{h}^{(\ell)} + \mathbf{h}^{(\ell)\prime}, \mathbf{D}^{(\ell)} + \mathbf{D}^{(\ell)\prime},$  $\mathbf{Back}_{i \rightarrow L} + \mathbf{Back}^{\prime}_{\ell \rightarrow j}$ be defined with respect to $\mathbf{W}+\mathbf{W}^{\prime}, \mathbf{A} + \mathbf{A}',  \mathbf{B}, \obx,$
	then
	$$
	\begin{array}{l}
		\| \sum_{\ell \in [L]}\left(\mathbf{Back}_{\ell \rightarrow L}+\mathbf{Back}_{\ell \rightarrow L}^{\prime}\right)\left(\mathbf{D}^{(\ell)}+\mathbf{D}^{(\ell)\prime}\right) \left(\widetilde{\mathbf{W}}\left(\mathbf{h}^{(\ell-1)}+h^{(\ell-1)\prime}\right) + \widetilde{\mathbf{A}} \mathbf{x}^{(\ell)} \right) \\
		\quad\quad\quad\quad -\sum_{\ell \in [L]} \mathbf{Back}_{\ell \rightarrow L}  \mathbf{D}^{(\ell)} \left(\widetilde{\mathbf{W}} \mathbf{h}^{(\ell-1)} + \widetilde{\mathbf{A}} \mathbf{x}^{(\ell)} \right) \| \leq O\left(\frac{\omega \rho^{6} \Delta^{1 / 3}}{m^{1 / 6}}\right).
	\end{array}
	$$
\end{lemma}

\begin{proof}
	The proof will follow the same technique as has been used in Lemma 6.2 in \cite{allen2019can}. We give a brief overview here. 
	
	We allow a change in $\mathbf{A}$ by $\mathbf{A}^{\prime}$, which wasn't allowed in their lemma. However, we show now that the primary 3 properties (specified in Lemma F.1 in \cite{allen2019can}) used to prove the lemma change only by a constant factor, with the introduction of perturbation in $\mathbf{A}$. With probability at least $1-e^{-\Omega(\rho^2)}$,
	\begin{enumerate}
		\item $\norm{\mathbf{h}^{(\ell)\prime}}_2 \leq \mathcal{O}\left(\rho^{6} \Delta / \sqrt{m}\right).$ 
		\item $\norm{\mathbf{D}^{(\ell)\prime}}_{0} \leq \mathcal{O}\left(\rho^{4} \Delta^{2 / 3} m^{2 / 3}\right).$
		\item $\norm{ \mathbf{Back}_{\ell \rightarrow L}^{\prime} }_{2} \leq \mathcal{O}\left(\Delta^{1 / 3} \rho^{6} m^{1 / 3}\right).$
	\end{enumerate}
	Property 1 and property 2 will follow from Claim C.2 and property 3 will follow from Claim C.9 in \cite{allen2019convergence_rnn} with the following change. Due to the introduction of perturbation in $\mathbf{A}$, eq. C.2 in \cite{allen2019convergence_rnn} changes to
	\begin{align*}
		\mathbf{g}^{(\ell)\prime} = \mathbf{W}^{\prime} \mathbf{D}^{(\ell)} \mathbf{g}^{(\ell)} + (\mathbf{W} + \mathbf{W}^{\prime})\cdot \mathbf{D}^{(\ell)\prime}\cdot \mathbf{g}^{(\ell)} + (\mathbf{W} + \mathbf{W}^{\prime}) \cdot  (\mathbf{D}^{(\ell)} + \mathbf{D}^{(\ell)\prime}) \cdot  \mathbf{g}^{(\ell)\prime} + \mathbf{A}^{\prime} \mathbf{x}^{(\ell)},
	\end{align*}
	where we introduce an extra last term. Thus, since $\norm{\mathbf{A}^{\prime} \mathbf{x}^{(\ell)}} \le \norm{\mathbf{A}^{\prime}} \norm{\mathbf{x}^{(\ell)}} \le \mathcal{O}(\frac{\Delta}{\sqrt{m}})$, $\mathbf{g}^{(\ell)\prime}$ can be similarly written as $\mathbf{g}^{(\ell)\prime}_1 + \mathbf{g}^{(\ell)\prime}_2$, where $\norm{\mathbf{g}^{(\ell)\prime}_1} \le \tau_1$ and $\norm{\mathbf{g}^{(\ell)\prime}_2}_0 \le \tau_2$, with $\tau_1$ just changing by a factor 2 in eq. C.1\cite{allen2019convergence_rnn}. This minor change percolates to minor changes in the constant factors in $\norm{\mathbf{h}^{(\ell)\prime}}$, $\norm{\mathbf{D}^{(\ell)\prime}}_{0}$ and $\norm{ \mathbf{Back}_{\ell \rightarrow L}^{\prime} }_{2}$.
	
	Now, the proof follows from the following set of equations.
	\begin{align*}
		&\| \sum_{\ell \in [L]}\left(\mathbf{Back}_{\ell \rightarrow L}+\mathbf{Back}_{\ell \rightarrow L}^{\prime}\right)\left(\mathbf{D}^{(\ell)}+\mathbf{D}^{(\ell)\prime}\right) \left(\widetilde{\mathbf{W}}\left(\mathbf{h}^{(\ell-1)}+\mathbf{h}^{(\ell-1)\prime}\right) + \widetilde{\mathbf{A}} \mathbf{x}^{(\ell)} \right) \\
		&\quad\quad\quad\quad -\sum_{\ell \in [L]} \mathbf{Back}_{\ell \rightarrow L}  \mathbf{D}^{(\ell)} \left(\widetilde{\mathbf{W}} \mathbf{h}^{(\ell-1)} + \widetilde{\mathbf{A}} \mathbf{x}^{(\ell)} \right) \| \\&
		\le \| \sum_{\ell \in [L]}\mathbf{Back}_{\ell \rightarrow L}^{\prime} \left(\mathbf{D}^{(\ell)}+\mathbf{D}^{(\ell)\prime}\right) \left(\widetilde{\mathbf{W}}\left(\mathbf{h}^{(\ell-1)}+\mathbf{h}^{(\ell-1)\prime}\right) + \widetilde{\mathbf{A}} \mathbf{x}^{(\ell)} \right) \| \\&
		\le \underbrace{\|\sum_{\ell \in [L]} \mathbf{Back}_{\ell \rightarrow L}^{\prime} \left(\mathbf{D}^{(\ell)}+\mathbf{D}^{(\ell)\prime}\right) \widetilde{\mathbf{W}} \left(\mathbf{h}^{(\ell-1)}+\mathbf{h}^{(\ell-1)\prime}\right)\|}_{\text{Term 1}} \\& + \underbrace{ \|\sum_{\ell \in [L]} \mathbf{Back}_{\ell \rightarrow L} \left(\mathbf{D}^{(\ell)}+\mathbf{D}^{(\ell)\prime}\right) \widetilde{\mathbf{W}} \mathbf{h}^{(\ell-1)\prime}\|}_{\text{Term 2}}  + 
		\underbrace{\|\sum_{\ell \in [L]} \mathbf{Back}_{\ell \rightarrow L} \mathbf{D}^{(\ell)\prime} \widetilde{\mathbf{W}} \mathbf{h}^{(\ell-1)}\|}_{\text{Term 3}} \\&
		+ \underbrace{\|\sum_{\ell \in [L]} \mathbf{Back}_{\ell \rightarrow L}^{\prime} \left(\mathbf{D}^{(\ell)} +\mathbf{D}^{(\ell)\prime}\right) \widetilde{\mathbf{A}} \mathbf{x}^{(\ell)}\|}_{\text{Term 4}} + \underbrace{\|\sum_{\ell \in [L]} \mathbf{Back}_{\ell \rightarrow L} \mathbf{D}^{(\ell)\prime} \widetilde{\mathbf{A}} \mathbf{x}^{(\ell)}\|}_{\text{Term 5}} 
	\end{align*}
	Term 1, 2 and 3 appear in the proof of Claim 6.2\cite{allen2019can}. Terms 4 and 5 can be bounded using similar technique by using the bound on $\norm{\mathbf{Back}^{\prime}_{\ell \to L}}$ and $\norm{\mathbf{D}^{(\ell)\prime}}_0$ respectively.
\end{proof}





\begin{lemma} \label{lemma:perturb_NTK_small_output} [first order approximation]
	Let $\mathbf{W}, \mathbf{A}, \mathbf{B}$ be at random initialization, $\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(L)}$ be a fixed normalized input sequence, and $\Delta \in\left[\varrho^{-100}, \varrho^{100}\right] .$ With probability at least $1-e^{-\Omega(\rho)}$ over $\mathbf{W}, \mathbf{A}, \mathbf{B}$ the following holds. Given any matrices $W^{\prime}$ with $\left\|\mathbf{W}^{\prime}\right\|_{2} \leq \frac{\Delta}{\sqrt{m}},$ $\mathbf{A}^{\prime}$ with $\left\|\mathbf{A}^{\prime}\right\|_{2} \leq \frac{\Delta}{\sqrt{m}},$, letting
	$\mathbf{h}^{(\ell)}, \mathbf{D}^{(\ell)},$  $\mathbf{Back}_{i \rightarrow L}$ be defined with respect to $\mathbf{W}, \mathbf{A}, \mathbf{B}, \obx,$ and $\mathbf{h}^{(\ell)} + \mathbf{h}^{(\ell)\prime}, \mathbf{D}^{(\ell)} + \mathbf{D}^{(\ell)\prime},$  $\mathbf{Back}_{i \rightarrow L} + \mathbf{Back}^{\prime}_{\ell \rightarrow j}$ be defined with respect to $\mathbf{W}+\mathbf{W}^{\prime}, \mathbf{A} + \mathbf{A}',  \mathbf{B}, \obx,$
	then
	\begin{align*}
	    &\norm{F^{(L)}_{\mathrm{rnn}}(\bx; \mathbf{W} + \mathbf{W}', \mathbf{A} + \mathbf{A}') - F^{(L)}_{\mathrm{rnn}}(\bx; \mathbf{W}, \mathbf{A}) - F^{(L)}(\mathbf{x}, \mathbf{W}', \mathbf{A}')} \\&
		= \norm{\mathbf{B} \mathbf{h}^{(L)\prime} - \sum_{\ell \in [L]} \mathbf{Back}_{\ell \rightarrow L}  \mathbf{D}^{(\ell)} \left(\mathbf{W}' \mathbf{h}^{(\ell-1)} + \mathbf{A}' \mathbf{x}^{(\ell)} \right)} \le \mathcal{O}(\frac{\rho^7 \Delta^{4/3}}{m^{1/6}}).
	\end{align*}
\end{lemma}

\begin{proof}
	The proof will follow the same technique as has been used in Lemma 6.1 in \cite{allen2019can}. We give a brief outline here. 
	
	We allow a change in $\mathbf{A}$ by $\mathbf{A}^{\prime}$, which wasn't allowed in their lemma. This leads to an introduction of an additional term in eq. H.1 in \cite{allen2019can}. That is, there exist diagonal matrices $\mathbf{D}^{(\ell)\prime\prime}$, where $d^{(\ell)\prime\prime}_{rr} \in [-1, 1]$ and is non zero only when $d^{(\ell)\prime}_{rr} \ne d^{(\ell)}_{rr}$, 
	\begin{align*}
		\mathbf{B} (\mathbf{h}^{(L)} + \mathbf{h}^{(L)\prime}) -  \mathbf{B} \mathbf{h}^{(L)} &= \underbrace{\sum_{i=1}^{L-1} \mathbf{B} (\mathbf{D}^{(L)} + \mathbf{D}^{(L)\prime\prime}) \mathbf{W} \cdots \mathbf{W} (\mathbf{D}^{(i+1)} + \mathbf{D}^{(i+1)\prime\prime}) \mathbf{W}' (\mathbf{h}^{(i)} + \mathbf{h}^{(i)\prime})}_{\text{Term 1}} \\&
		+ \underbrace{\sum_{i=1}^{L-1} \mathbf{B} (\mathbf{D}^{(L)} + \mathbf{D}^{(L)\prime\prime}) \mathbf{W} \cdots \mathbf{W} (\mathbf{D}^{(i+1)} + \mathbf{D}^{(i+1)\prime\prime}) \mathbf{A}' \bx^{(i)}}_{\text{Term 2}}.
	\end{align*}
	In lemma 6.2 of \cite{allen2019can}, Term 1 was shown to be close to \\ $\sum_{i=1}^{L-1} \mathbf{B} \mathbf{D}^{(L)} \mathbf{W} \cdots \mathbf{W} \mathbf{D}^{(i+1)} \mathbf{W}' \mathbf{h}^{(i)}$  by $\mathcal{O}(\frac{\rho^7 \Delta^{4/3}}{m^{1/6}})$. The bound will stay the same, since we have shown similar bounds for $\norm{\mathbf{D}^{(\ell)\prime}}_0$ and $\norm{\mathbf{h}^{(\ell)\prime}}_2$ in the proof of lemma~\ref{lemma:perturb_NTK_small}. 
	
	Using the same technique, we can show that Term 2 is close to $\sum_{i=1}^{L-1} \mathbf{B} \mathbf{D}^{(L)}  \mathbf{W} \cdots \mathbf{W} \mathbf{D}^{(i+1)} \mathbf{A}' \bx^{(i)}$, since Term 2 can be similarly broken down into at most $2^L$ terms of the form $$(\mathbf{B}\mathbf{D}\mathbf{W}\cdots\mathbf{D}\mathbf{W})\mathbf{D}'' (\mathbf{W}\cdots\mathbf{D}\mathbf{W})\mathbf{D}'' \cdots \mathbf{D}'' (\mathbf{W}\cdots\mathbf{D}\mathbf{W}) \mathbf{A}'\mathbf{x}^{(i)}$$ and each term can then be similarly bounded to give an extra error bound $\mathcal{O}(\frac{\rho^7 \Delta^{4/3}}{m^{1/6}})$.
\end{proof}



%$\begin{array}{ll}\text { (a) }\left\|h_{i}^{\prime}\right\| \leq O\left(\rho^{6} \Delta / \sqrt{m}\right) \text { for every } i \in[L] & \text { (forward stability) }\end{array}$
%(b) $\left\|D_{i}^{\prime}\right\|_{0} \leq O\left(\rho^{4} \Delta^{2 / 3} m^{2 / 3}\right)$ for every $i \in[L] \quad$ (sign change)
%(c) $\|$ Back $_{i \rightarrow j}^{\prime} \|_{2} \leq O\left(\Delta^{1 / 3} \rho^{6} m^{1 / 3}\right)$ for every $1 \leq i \leq j \leq L$
%(backward stability)

\begin{lemma}\label{lemma:perturb_small_target}
	Let $\mathbf{W}^{\ast}$ and $\mathbf{A}^{\ast}$ be as defined in def. \ref{def:existence}. Let $\mathbf{W}, \mathbf{A}, \mathbf{B}$ be at random initialization, $\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(L)}$ be a fixed normalized input sequence, and $\Delta \in\left[\varrho^{-100}, \varrho^{100}\right] .$ With probability at least $1-e^{-\Omega(\rho)}$ over $\mathbf{W}, \mathbf{A}, \mathbf{B}$ the following holds. Given any matrices $W^{\prime}$ with $\left\|\mathbf{W}^{\prime}\right\|_{2} \leq \frac{\Delta}{\sqrt{m}},$ $\mathbf{A}^{\prime}$ with $\left\|\mathbf{A}^{\prime}\right\|_{2} \leq \frac{\Delta}{\sqrt{m}},$. Letting
	$\mathbf{h}^{(\ell)}, \mathbf{D}^{(\ell)},$  $\mathbf{Back}_{i \rightarrow L}$ be defined with respect to $\mathbf{W}, \mathbf{A}, \mathbf{B}, \obx,$ and $\mathbf{h}^{(\ell)} + \mathbf{h}^{(\ell)\prime}, \mathbf{D}^{(\ell)} + \mathbf{D}^{(\ell)\prime},$  $\mathbf{Back}_{i \rightarrow L} + \mathbf{Back}^{\prime}_{\ell \rightarrow j}$ be defined with respect to $\mathbf{W}+\mathbf{W}^{\prime}, \mathbf{A} + \mathbf{A}',  \mathbf{B}, \obx,$
	then for all $s \in [k]$
	\begin{align*}
		& \sum_{\ell \in [L]} \mathbf{e}_s^{\top} \left(\mathbf{Back}_{\ell \rightarrow L}+\mathbf{Back}_{\ell \rightarrow L}^{\prime}\right)\left(\mathbf{D}^{(\ell)}+\mathbf{D}^{(\ell)\prime}\right) \left(\mathbf{W}^{\ast}\left(\mathbf{h}^{(\ell-1)}+h^{(\ell-1)\prime}\right) + \mathbf{A}^{\ast} \mathbf{x}^{(\ell)} \right) \\& 
		= \sum_{r' \in [p]}  b_{r', s}^{\dagger} \Phi_{r', s} \left(\left\langle \mathbf{w}_{r', s}^{\dagger}, [\overline{\mathbf{x}}^{(2)}, \cdots, \overline{\mathbf{x}}^{(L-1)}]\right\rangle\right)  \\& \pm \mathcal{O}(\dout Lp\rho^2 \varepsilon + \dout L^{7/3} p \rho^2 L_{\Phi} \epsilon_x^{2/3} + \dout  L^5 p \rho^{11} L_{\Phi} C_{\Phi}  C_{\varepsilon}(\Phi, \mathcal{O}(\epsilon_x^{-1}))  m^{-1/30} ) \\& \pm  O\left(\frac{ C_{\varepsilon}(\Phi, \mathcal{O}(\epsilon_x^{-1})) \dout ^{1/2} \rho^{8} \Delta^{1 / 3}}{m^{1 / 6}}\right).
	\end{align*}
\end{lemma}

\begin{proof}
	The proof follows from Lemma~\ref{lemma:perturb_NTK_small}, using the bound on $\norm{\mathbf{W}^{\ast}}$ and $\norm{\mathbf{A}^{\ast}}$ from lemma~\ref{lemma:norm_WA}.
\end{proof}



