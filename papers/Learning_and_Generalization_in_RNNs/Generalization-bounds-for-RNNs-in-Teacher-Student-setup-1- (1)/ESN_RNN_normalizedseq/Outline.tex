\section{Proof Sketch}
While the full proof is highly technical, in this section we will sketch the proof focusing on the conceptual aspects while minimizing the technical aspects to the essentials; full proofs are in the appendix. The high-level outline of our proof is as follows.
%\vspace{-3\topsep}
\begin{enumerate}[leftmargin=*, itemsep=0.1pt, topsep=0pt]
	\item \emph{Overparamtrization simplifies the neural network behavior.} The function $F_{\mathrm{rnn}}^{(L)}(\bx; \mathbf{W}+\mathbf{W}', \mathbf{A} + \mathbf{A}')$ computed by the RNN is a function of the parameters $\mathbf{W}', \mathbf{A}'$ as well as of the input $\overline{\mathbf{x}}$. It is a highly non-linear and non-convex function in both the parameters and in the input. The objective function inherits these properties and its direct analysis is difficult. However, it has been realized in the last few years---predominantly for the FFN setting---that when the network is overparametrized (i.e., as the number of neurons $m$ becomes large compared to other paramters of the problem such as the complexity of the concept class), the network behavior simplifies in a certain sense. The general idea carries over to RNNs as well:
	in \eqref{eqn:linear-approximation} below we write the first-order Taylor approximation of $F_{\mathrm{rnn}}^{(L)}(\bx; \mathbf{W}+\mathbf{W}', \mathbf{A} + \mathbf{A}')$
	at $\mathbf{W}$ and $\mathbf{A}$ as a linear function of  $\mathbf{W}'$ and $\mathbf{A}'$; it is still a non-linear function of the input sequence. As in \cite{allen2019can} we call this function \emph{pseudo-network}, though our notion is more general as we vary both the parameters $\mathbf{W}'$ and $\mathbf{A}'$. Pseudo-network is a good approximation of the target network as a function of $\overline{\mathbf{x}}$ for all $\overline{\mathbf{x}}$. \todo{This statement needs more work and also pointers to the formal statements.}
	\item \emph{Existence of a good RNN.} In order to show that the RNN training successfully learns, we first show that there are parameters values for RNN so that as a function of $\overline{\mathbf{x}}$ it is a good approximation of $F^\ast$. Instead of doing this directly, we show that the pseudo-network can approximate $F^\ast$; this suffices as we know that the RNN and the pseudo-network remain close. This is done by constructing paramters $\mathbf{W}^{\ast}$ and $\mathbf{A}^{\ast}$ so that the resulting pseudo-network approximates the target function in the concept class (Section~\ref{sec:fitting-target}) for all $\overline{\mathbf{x}}$. 
	\item \emph{Optimization.} SGD makes progress because the loss function is convex in terms of the pseudo-network which stays close to the RNN as a function of $\mathbf{x}$. Thus, SGD finds parameters with training loss close to that achieved by $\mathbf{W}^{\ast}, \mathbf{A}^{\ast}$.
	\item \emph{Generalization.} Apply a Rademacher complexity-based argument to show that SGD has low population loss.
\end{enumerate}
%\smallskip\smallskip
%\vspace{-2\topsep}
Step 2 is the main novel contribution of our paper and we will give more details of this step in the rest of this section.\footnote{The above outline is similar to prior work, e.g., \cite{allen2019can}. Details can be quite different though, e.g., they only train $\mathbf{W}$ and keep $\mathbf{A}$ fixed to its initial value. 
Their contribution was also mainly in Step 2 and the other steps were similar to prior work.} 
\subsection{Pseudo-network}\label{sec:pseudo-network}
%As mentioned, the most important step is to show the existence of a pseudo-network that approximates the target function for all inputs. 
We define the pseudo-network here. Suppose $\mathbf{W}, \mathbf{A}, \mathbf{B}$ are at random initialization.
 The linear term in the first-order Taylor approximation is given by the pseudo-network
%\todo{Give a reference in the AL paper for this. It's not explained what the function on the LHS is, nor are its arguments explained. Maybe remark on how this formula is obtained.}
\begin{align} 
	F^{(L)}(\bx; \mathbf{W}', \mathbf{A}') 
	&:= \sum_{i=1}^{L}  \mathbf{Back}_{i \rightarrow L} \mathbf{D}^{(i)} \left(\mathbf{W'} \mathbf{h}^{(i-1)} + \mathbf{A'} \mathbf{x}^{(i)}\right) \label{eqn:linear-approximation}\\&
	\approx F_{\mathrm{rnn}}^{(L)}(\bx; \mathbf{W}+\mathbf{W}', \mathbf{A}+\mathbf{A}') - F_{\mathrm{rnn}}^{(L)}(\bx; \mathbf{W}, \mathbf{A}) \tag{using Lemma~\ref{lemma:perturb_NTK_small_output}}.
\end{align}
 This function approximates the change in the output of the RNN, when $(\mathbf{W}, \mathbf{A})$ changes to $(\mathbf{W}+\mathbf{W}^{'}, \mathbf{A}+\mathbf{A}^{'})$. 
 The parameter $\lambda$, that we defined in the objective function, will be used to make the contribution of  $F_{\mathrm{rnn}}^{(L)}$ at initialization small thus making pseudo-network a good approximation of RNN. Hence, we can observe that the pseudo network is a good approximation of the RNN, provided the weights stay close to the initialization.
 
 To complete the above definition of pseudo-network we define the two new notations in the above formula. For each $\ell \in [L]$, define $\mathbf{D}^{(\ell)} \in \mathbb{R}^{m \times m}$ as a diagonal matrix, with diagonal entries %\todo{why do we need to use $d_{rr}^{(\ell)}$ why not $\mathbf{D}^{(\ell)}_{rr}$}
\begin{align}\label{eqn:diagonal_main}
	d_{rr}^{(\ell)} := \mathbb{I}[\mathbf{w}_r^{\top} \mathbf{h}^{(\ell - 1)} + \mathbf{a}_r^{\top} \mathbf{x}^{(\ell)} \ge 0], \quad \forall r \in [m]. 
\end{align}
In words, the diagonal of matrix $\mathbf{D}^{(\ell)}$ represents the activation pattern for the RNN cell at step $\ell$ at initialization.

Define $\mathbf{Back}_{i \to j} \in \mathbb{R}^{\dout  \times m}$ for each $1 \le i \le j \le L$ by
\begin{align*}
	\mathbf{Back}_{i \to j} := \mathbf{B} \mathbf{D}^{(j)} \mathbf{W} \ldots \mathbf{D}^{(i+1)} \mathbf{W}, 
\end{align*}
with $\mathbf{Back}_{i \to i} := \mathbf{B}$ for each $i \in [L]$. Matrices $\mathbf{Back}_{i \to j}$ in Eq. \eqref{eqn:linear-approximation} arise naturally in the 
computation of the first-order Taylor approximation (equivalently, gradients w.r.t. the parameters) using standard matrix calculus.% in terms of the parameters $\mathbf{W}, \mathbf{A}$ of the function $F_{\mathrm{rnn}}^{(\ell)}(\bx; \mathbf{W}, \mathbf{A})$. 
Very roughly, one can think of $\mathbf{Back}_{i \to j}$ as related to the backpropagation signal from the output at step $j$ to the parameters at step $i$. 




\subsection{Existence of good pseudo-network}\label{sec:fitting-target}
Our goal is to construct $\mathbf{W}^{\ast}$ and $\mathbf{A}^{\ast}$ such that for any true input sequence $\overline{\mathbf{x}} = (\overline{\mathbf{x}}^{(2)}, \ldots, \overline{\mathbf{x}}^{(L-1)})$, if we define the normalized sequence $\mathbf{x} = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(L)})$, then with high probability we have
\begin{align}\label{eq:simplifiedFstar}
    F^{(L)}(\bx; \mathbf{W}^{\ast}, \mathbf{A}^{\ast}) \approx   F^{\ast}(\overline{\mathbf{x}}).
\end{align}
%\sum_{r \in [p]} b^{\dagger}_{r, s} \Phi_{r, s} (\langle \mathbf{w}^{\dagger}_{r, s}, [\overline{\mathbf{x}}^{(2)}, \ldots, \overline{\mathbf{x}}^{(L-1)}]\rangle). 
%\end{align*}
\iffalse
The major challenge in constructing $\mathbf{W}^{\ast}$ and $\mathbf{A}^{\ast}$ is to use the information contained in $\mathbf{h}^{(\ell)}$ either explicitly in $\mathbf{h}^{(\ell)}$ or implicitly in $\mathbf{D}^{(i)}$ (recall \eqref{eqn:diagonal_main}). The construction of $\mathbf{W}^\ast$ 
in \cite{allen2019can} is not able to use this information and ignores it by treating it as noise (which is also non-trivial). Conceptually,
the idea underlying our construction is that $\mathbf{h}^{(\ell)}$ contains information about all the inputs $\bx^{(1)}, \ldots, \bx^{(\ell)}$ up until step $\ell$. Furthermore, this information can be recovered approximately by a linear transformation (Theorem~\ref{thm:Invertibility_ESN_outline}). Our construction sets $\mathbf{W}^{\ast} = \mathbf{0}$. This might suggest that we are  ignoring $\mathbf{h}^{(\ell)}$ as the RHS of \eqref{eqn:linear-approximation} becomes
\begin{align*} 
	F^{(L)}&(\bx; \mathbf{W}^\ast, \mathbf{A}^\ast) = \sum_{i=1}^{L}  \mathbf{Back}_{i \rightarrow L} \mathbf{D}^{(i)}  \mathbf{A'} \mathbf{x}^{(i)}.
\end{align*}
Moreover, this may appear to be a linear function in the $\mathbf{x}^{(i)}$. However, as just noted, $\mathbf{D}^{(i)}$ also implicitly has information about $\mathbf{h}^{(\ell)}$, and this is exploited in our construction of $\mathbf{A}^{\ast}$ to allow us to approximate general nonlinear functions of the $\mathbf{x}^{(i)}$.
\fi
To simplify the presentation, in this sketch we will assume that $p$, the number of neurons in the concept class, and the output dimension $\dout $ are both equal to $1$. 
Also, let the output weight $b^{\dagger}:=1$. 
These assumptions retain the main proof ideas while simplifying equations. %In the appendix full proofs are given.  
Overall, we assume that the target function $F^{\ast} : \Reals^{(L-2) \cdot (d-1)} \to \Reals$ on a given sequence is given by
\begin{align}\label{eqn:Fstar-Phi}
    F^{\ast}(\overline{\mathbf{x}}) = \Phi^{\ast}(\langle \mathbf{w}^{\dagger}, [\overline{\mathbf{x}}^{(2)}, \cdots, \overline{\mathbf{x}}^{(L-1)}]\rangle),
\end{align}
where $\Phi^{\ast}: \Reals \to \Reals$ is a smooth function and $\mathbf{w}^{\dagger} \in \mathbb{S}^{(L-2) \cdot (d-1) - 1}$. 

%In the following theorem, we will show that there exist some pseudo network that can approximately express this target function. 




First, we state Lemma 6.2 in \cite{allen2019learning}, which is useful for our construction of the matrices $\mathbf{W}^{\ast}$ and $\mathbf{A}^{\ast}$.
Consider a smooth function $\phi: [-1, 1] \to \Reals$. It can be approximated as a linear combination of step functions (derivatives of ReLU) for all $u \in (-1, 1)$, i.e., there exists a ``weight function'' $H: \Reals^2 \to \Reals$ such that
$	 \phi\left(u\right) \approx \mathbb{E}_{\alpha_1, \beta_1, b_0} [{H\left(\alpha_{1}, b_0\right)} \, \mathbb{I}_{\alpha_{1} u + \beta_{1} \sqrt{1 - u^{2}} + b_0   \geq 0}]$
where $\alpha_1, \beta_1 \sim \mathcal{N}\left(0, 1\right) \text{ and } b_0 \sim \mathcal{N}\left(0, 1\right)$ are independent random variables (we omitted some technical details).

The above statement can be straightforwardly extended to the following slightly more general version: 
\begin{lemma}\label{lemma:Expressfunction_fNN}
    For every smooth function $\phi$, any $\overline{\mathbf{w}} \in \mathbb{S}^{d-1}$, and any $\varepsilon \in \left(0, \frac{1}{\mathfrak{C}_{s}\left(\phi, 1 \right)}\right)$ there exists a $H:\mathbb{R}^2 \to \left(- 
	\mathfrak{C}_\varepsilon\left(\phi,  1\right),  \mathfrak{C}_\varepsilon\left(\phi,  1\right)\right)$, which is $ \mathfrak{C}_\varepsilon\left(\phi, 1\right)$-Lipschitz continuous and for all $\mathbf{u} \in  \mathbb{S}^{d-1}$, we have
	$$
	\begin{array}{l}
		\Big|\phi(\overline{\mathbf{w}}^{\top} \mathbf{u}) - \mathbb{E}_{\mathbf{w} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), b_0 \sim \mathcal{N}(0, 1)} [{H(\mathbf{w}^{\top} \overline{\mathbf{w}}, b_0)} \,\mathbb{I}_{\mathbf{w}^{\top} \mathbf{u} + b_0 \geq 0} ]  \Big| \leq \varepsilon.
	\end{array}
	$$
	%\end{corollary}
\end{lemma} 
Very informally, this lemma states that the activation pattern of a one-layer $\relu$ network (given by $\mathbb{I}_{\mathbf{w}^{\top} \mathbf{u} \geq 0}$) at initialization can be used to express a smooth function of the dot product of the input vector with a fixed vector. While the above statement involves an expectation, one can easily replace it by an empirical average with slight increase in error. 
This statement formed the basis for FFN and RNN results in \cite{allen2019learning, allen2019can}. 
%For completeness of the proof, we reproduce the argument here.
Can we use it for RNNs for our general concept class? An attempt to do so is the following lemma showing that the pseudo-network can express any smooth function of the hidden state $\mathbf{h}^{(L-1)}$ and $\mathbf{x}^{(L)}$. 

\begin{lemma}[Informal]\label{lemma:Expressfunction_RNN}
	%\begin{corollary}\label{Cor:Smooth_H}
	For a given smooth function $\phi$, a vector $\overline{\mathbf{w}} \in \mathbb{S}^{(m+d-1)}$, and any $\varepsilon \in \left(0, \frac{1}{\mathfrak{C}_{s}\left(\phi, 1 \right)}\right)$, there exist matrices $\mathbf{W}^{\ast}$ and $\mathbf{A}^{\ast}$ such that for every normalized input sequence $\bx = (\bx^{(1)}, \ldots, \bx^{(L)})$ formed from a sequence $\obx$, we have with high probability,
	\begin{align*}
	    \abs{F^{(L)}(\bx; \mathbf{W}^{\ast}, \mathbf{A}^{\ast}) - \phi({\langle  \overline{\mathbf{w}}, [\mathbf{h}^{(L-1)}, \bx^{(L)}_{:d-1}]\rangle})} \le \varepsilon,
	\end{align*}
	provided $m = \mathrm{poly}(\frac{1}{\varepsilon}, L, \mathfrak{C}_{\epsilon}(\phi, \mathcal{O}(1)))$. Vector $\bx^{(L)}_{:d-1}$ is 
	$\bx^{(L)}$ without the last coordinate, the bias term appended to each input.
	%\end{corollary}
\end{lemma} 
The reason $\mathbf{h}^{(L-1)}$ and $\mathbf{x}^{(L)}$ come up is because they serve as inputs to the RNN cell when processing the $L$-th input. The proof sketched below uses the fact that RNNs are one-layer FFNs unrolled over time. Hence, we could try to apply the result of Lemma~\ref{lemma:Expressfunction_fNN} to the RNN cell at step $L$. However, a difficulty arises in carrying out this plan because the contributions of previous times steps also come up (as seen in the equations below) and it can be difficult to disentangle the contribution of step $L$. This is addressed in the proof: 

\begin{prf}%[of Lemma~\ref{lemma:Expressfunction_RNN}]
    \input{ESN_RNN_normalizedseq/Proof_outline_2}
\end{prf}
More generally, with much more technical work, it might be possible to prove an extension of the above lemma asserting the existence of a pseudo-network approximating a sum of functions of type $\sum_{i \in [L]}\phi_i({\langle  \overline{\mathbf{w}}_i, [\mathbf{h}^{(i-1)}, \bx^{(i)}]\rangle})$. However, even so it is not at all clear what class of functions of $\overline{\mathbf{x}}$ this represents because of the presence of the hidden state vectors. 


Thus, the major challenge in constructing $\mathbf{W}^{\ast}$ and $\mathbf{A}^{\ast}$ to express the functions from the desired concept class is to use the information contained in $\mathbf{h}^{(\ell)}$. The construction of $\mathbf{W}^\ast$ in \cite{allen2019can} is not able to use this information and ignores it by treating it as noise (which is also non-trivial). The idea underlying our construction is that $\mathbf{h}^{(\ell)}$ in fact contains information about all the inputs $\bx^{(1)}, \ldots, \bx^{(\ell)}$ up until step $\ell$. Furthermore and crucially, this information can be recovered approximately by a linear transformation (Theorem~\ref{thm:Invertibility_ESN_outline} below). 
This enables us to show: \todo{Quantify }
\begin{theorem}[Existence of pseudo-network approximation for target function; abridged statement of Theorem~\ref{thm:existence_pseudo} in the appendix] \label{thm:existence_pseudo_outline}
	For every target function $F^{\ast}$ of the form Eq.~\eqref{eqn:Fstar-Phi}, there exist matrices $\mathbf{W}^{*}$ and $\mathbf{A}^{\ast}$ such that with probability at least $1-e^{-\Omega\left(\rho^{2}\right)}$ over $\mathbf{W}, \mathbf{A}, \mathbf{B}$, we have for every normalized input sequence $\mathbf{x} = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(L)})$ formed from a true sequence $\overline{\bx}$,  
	\begin{align*}
		\abs{F^{(L)}(\bx; \mathbf{W}^{\ast}, \mathbf{A}^{\ast}) - \Phi^{\ast} \left(\langle  \mathbf{w}^{\dagger}, [\overline{\mathbf{x}}^{(2)}, \ldots, \overline{\mathbf{x}}^{(L-2)}]\rangle\right)}  \le \varepsilon + \frac{1}{\mathrm{poly}(\rho)},   
	\end{align*}
	provided $m \ge \mathrm{poly}(\rho, L, \varepsilon^{-1}, \mathfrak{C}_{\varepsilon}(\Phi, \mathcal{O}(\varepsilon_x^{-1})))$ and $\epsilon_x \le \frac{1}{\mathrm{poly}(\rho)}$.
\end{theorem}

\begin{prf}[Proof sketch]
    \input{ESN_RNN_normalizedseq/Proof_outline_3}
\end{prf}

\emph{Re-randomization.} In the proof sketches of Lemmas~\ref{lemma:Expressfunction_RNN} and Theorem~\ref{thm:existence_pseudo_outline} above we swept a technical but critical consideration under the rug: the random variables $\{\mathbf{w}_r, \mathbf{a}_r\}_{r \in [m]}$, $\obW^{(L)}$, $\{\mathbf{Back}_{i \to L}\}_{i \in [L]}$ and $\{\mathbf{h}^{(i)}\}_{i \in [L]}$ are not independent. This invalidates application of standard concentration inequalities w.r.t. the randomness of $\mathbf{W}$ and $\mathbf{A}$---this application is required in the proofs. Here our new variation of the re-randomization technique from \cite{allen2019can} comes in handy. The basic idea is the following: whenever we want to apply concentration bounds w.r.t. the randomness of $\mathbf{W}$ and $\mathbf{A}$, we divide the set of rows into disjoint sets of equal sizes. For each set, we will re-randomize the rows of the matrix $[\mathbf{W}, \mathbf{A}]_r$, show that the matrices $\obW^{[L]}$, $\{\mathbf{Back}_{i \to L}\}_{i \in [L]}$ and $\{\mathbf{h}^{(i)}\}_{i \in [L]}$ don't change a lot and then apply concentration bounds w.r.t. the new weights in the set. Finally, we account for the error from each set.

%Hence, we need to follow the re-randomization technique again: divide the neurons into disjoint sets of equal size. For each set, we re-randomize the rows of the matrix $[\mathbf{W}, \mathbf{A}]_r$, show that $\obW^{[L]}$, $\{\mathbf{Back}_{i \to L}\}_{i \in [L]}$ and $\{\mathbf{h}^{(i)}\}_{i \in [L]}$ don't change a lot and then apply concentration w.r.t. $\{\mathbf{w}_r, \mathbf{a}_r\}_{r \in [m]}$. Finally, we accumulate the error from each set.
\subsection{The rest of the proof}
Having shown that there exists a pseudo-network approximation of the RNN that can also approximate the concept class,
%and overparametrized RNNs behave like linear networks close to initialization, 
we will complete the proof by showing that SGD can find matrices with performance similar to $\mathbf{W}^{\ast}$ and $\mathbf{A}^{\ast}$ on the population objective $\mathrm{Obj(\cdot)}$. Lemma~\ref{lem:trainloss}  shows that the training loss decreases with time. The basic idea is to use the fact that within small radius of perturbation, overparametrized RNNs behave as a linear network and hence the training can be analyzed via convex optimization. Then, we show using Lemma~\ref{lem:radcomp} that the Rademacher complexity for overparametrized RNNs is bounded. Again, the main idea here is that overparametrized  RNNs behave as pseudo-networks in our overparametrized regime and hence their Rademacher complexity can be approximated by the Rademacher complexity of pseudo-networks. Finally, using generalization bounds on the Rademacher complexity, we get the final population-level objective in  Theorem~\ref{thm:main_theorem}.



%Now, the optimization theorem in \cite{allen2019can} can be modified to show that SGD can find matrices with properties comparable to $\mathbf{W}^{\ast}$ and $\mathbf{A}^{\ast}$.

\subsection{Invertibility of RNNs at initialization}\label{sec:invertibility}
In this section, we describe how to get back $\bx^{(1)}, \ldots, \bx^{(L)}$ from the hidden state $\mathbf{h}^{(L)}$. %The following lemma states that any linear function can be represented by an infinitely sized neural network with activation function $\relu$ with the outer weights taking a very simple form:
%\begin{lemma}\label{lem:normal_linearestimate_outline}[Lemma~\ref{lem:normal_linearestimate} in the appendix]
%	For any $\bv \in \Reals^d$, the linear function taking $\bx$ to $\bv^\top \bx$ for $\bx \in \Reals^d$, can be represented as
%	\begin{equation}\label{eqn:single_layer_inversion_exact_outline}
%		\bv^\top \bx = \int_{\Reals^d} p(\bw) \, \sigma(\mathbf{w}^{\top} \bx ) \,\deriv \mu_d (\mathbf{w}),
%	\end{equation}
%	with 
%	$p\left(\mathbf{w}\right) \;=\;  2 \, \mathbf{w}^{\top} \mathbf{v}.$ 
%\end{lemma}
%The fact that $p(\mathbf{w})$ is a linear function is key to our application of this lemma.
%The proof follows from the second order moment of the normal distributed variables $\mathbf{w}$. 
%We can show using concentration properties of normal distributed variables that the above relation holds true for a highly overparametrized neural network, with a small approximation error of the order $\frac{1}{\sqrt{m}}$, i.e.
The following lemma states that any linear function can be represented by a one-hidden layer FFN with activation function $\relu$,with a small approximation error of the order $\frac{1}{\sqrt{m}}$:
\begin{lemma}\label{lem:normal_linearestimate_appr_outline} [a simpler continuous version can be found in Lemma~\ref{lem:normal_linearestimate} in the appendix]
	For any $\bv \in \Reals^d$, the linear function taking $\bx$ to $\bv^\top \bx$ for $\bx \in \Reals^d$, can be represented as
	\begin{equation}
		\abs{\bv^\top \bx -  \mathbf{p}^{\top} \sigma(\mathbf{T} \mathbf{x})} \le \frac{\norm{\bv} \cdot \norm{\bx}}{\sqrt{m}},
	\end{equation}
	with 
	$\mathbf{p} \;=\;  2 \, \mathbf{T} \mathbf{v},$ where $\mathbf{T} \in \mathbb{R}^{m \times d}$ is a matrix with elements i.i.d. sampled from $\mathcal{N}(0, 1)$. 
\end{lemma}
%The fact that $\mathbf{p}$   is key to our application of this lemma.
Using the above lemma,\footnote{This lemma is from a paper that will appear soon; apart from the above lemma, 
	this work is very different from the present paper. We have reproduced the proof in full in the appendix.} we will show that the hidden state $\mathbf{h}^{(L)}$ can be inverted using a matrix $\obW^{[L]}$ to get back the input sequence $\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(L)}$. 


%First, define an alternate fixed sequence as follows: $\bx_{(0)} := (\mathbf{x}^{(1)}_{(0)}, \ldots, \mathbf{x}^{(L)}_{(0)})$, where
%	\begin{align*}
%		\mathbf{x}^{(1)}_{(0)} = (\mathbf{0}^{d-1}, 1), \;\;\;
%		\mathbf{x}^{(\ell)}_{(0)} = (\mathbf{0}^{d-1}, \varepsilon_x), \quad \forall \ell \in [2, L-1], \;\;\;
%		\mathbf{x}^{(L)}_{(0)} = (\mathbf{0}^{d-1}, 1).
%	\end{align*}
%Denote the activation patterns for this fixed sequence as the diagonal matrices: $\{ \mathbf{D}_{(0)}^{(\ell)} \}_{\ell \in [L]}$. Then, we have

\todo{We need a bound on the norm of the $x$ or it should show up in the RHS}
\begin{theorem}\label{thm:Invertibility_ESN_outline}[informal version of Theorem~\ref{thm:Invertibility_ESN}]
	%Define $\obW^{[\ell]}$ inductively by $\obW^{[1]} = \mathbf{D}^{(1)}_{(0)} \mathbf{A} $ and $ \obW^{[\ell]}  = [\mathbf{D}^{(\ell)}_{(0)} \mathbf{W} \obW^{[\ell-1]}, \mathbf{D}_{(0)}^{(\ell)}\mathbf{A}]_r \quad \text{ for }\, 2 \le \ell \le L .$ 
	There exists a set of matrices $\{ \obW^{[\ell]} \}_{\ell \in [L]}$, which can possibly depend on $\mathbf{W}$ and $\mathbf{A}$, such that for any $\varepsilon_x < \frac{1}{L}$ and any given normalized sequence $\bx^{(1)}, \ldots, \bx^{(L)}$, 
	with probability at least $1 - e^{-\Omega(\rho^2)}$ we have
	$$
	\begin{array}{l}
		\norm[1]{[\bx^{(1)}, \ldots, \bx^{(L)}] - \obW^{[L]\top} \mathbf{h}^{(L)}}_\infty  \leq \mathrm{poly}(L, \rho, m^{-1}, \varepsilon_x).
	\end{array}
	$$
\end{theorem}
%\iffalse
%Informally, the proof follows from the simple observation that RNNs are one-layer FFNs unrolled over time. 
Very roughly, the above result is obtained by repeated application of Lemma~\ref{lem:normal_linearestimate_appr_outline} to go from $\mathbf{h}^{(\ell)}$ to $(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)})$ starting with $\ell = L$. This uses the fact that the RNN cell is a one-hidden layer neural network and hence Lemma~\ref{lem:normal_linearestimate_appr_outline} is applicable. Several difficulties need to be overcome to carry out this plan. One difficulty is that a naive application of Lemma~\ref{lem:normal_linearestimate_appr_outline}
results in exponential blowup of error with $L$. We defer the technical details of this resolution to the full proof in the appendix.
%We solve this issue using normalization. We observed that the factor $2$ in $\mathbf{p}$ from Lemma~\ref{lem:normal_linearestimate_appr_outline} can be removed by using the activation pattern of the input (given by $\sigma'(\mathbf{T} \bx)$) explicitly in $\mathbf{p}$.
%We estimate the activation pattern of each sequence, using the activation pattern of some fixed alternative sequence. That allows us to invert the RNN network, as if we were inverting a linear RNN. 
Secondly, we apply re-randomization to tackle the dependence between $\mathbf{W}$, $\mathbf{A}$, $\{\mathbf{h}^{(\ell)}\}_{\ell \in [L]}$ and $\{\obW^{[\ell]}\}_{\ell \in [L]}$.
%\fi
We performed few toy experiments on the ability of invertibility for  RNNs at initialization (Sec.~\ref{sec:expts}). We observed, as predicted by our theorem above, that the error involved in inversion decreases with the number of neurons and increases with the length of the sequence (Fig.~\ref{fig:RNN_inver}). 



%\begin{proof}[Proof of theorem~\ref{thm:Invertibility_ESN_outline}]
%    \input{../ESN_RNN_normalizedseq/Proof_outline_4}
%\end{proof}

%\subsection{Informal Proofs of the lemmas and theorems}

%\begin{proof}[Proof of lemma~\ref{lemma:Expressfunction_fNN}]
%    \input{../ESN_RNN_normalizedseq/Proof_outline_1}
%\end{proof}

%\begin{proof}[Proof of lemma~\ref{lemma:Expressfunction_RNN}]
%    \input{ESN_RNN_normalizedseq/Proof_outline_2}
%\end{proof}

%\begin{proof}[Proof of theorem~\ref{thm:existence_pseudo_outline}]
%    \input{ESN_RNN_normalizedseq/Proof_outline_3}
%\end{proof}



\section{On concept classes}

It is apparent that our concept class is very general as it allows arbitrary dependencies across tokens. To concretely illustrate the generality of our concept class, and to compare with previous work, we show that our result implies that  RNNs can recognize a simple formal language $D_{L_1}$. Here we are working in the discrete setting where each input token comes from $\{0, 1\}$ possibly represented as a vector when fed to the RNN. For a sequence $\mathbf{z} \in \{0, 1\}^L$, we define $D_{L_1}(\mathbf{z})$ to be $1$ if the number of $1$'s in $\mathbf{z}$ is exactly $1$ and define it to be $0$ otherwise. We can show that $D_{L_1}$ is not representable in the concept class of \cite{allen2019can} (see Theorem~\ref{thm:allencantdl1} in the appendix).  However, we can show that the language $D_{L_1}$ can be recognized with a one-layer FFN with one neuron and quadratic activation. The idea is that we can simply calculate the number of $1$'s in the input string, which is doable using a single neuron. This implies that our concept class can represent language $D_{L_1}$ with low complexity. More generally, we can show that our concept class can efficiently represent pattern matching problems, where strings belong to a language only if they contain given strings as substrings. In general, we can show that our concept class can express general regular languages. However, the complexity of the concept class may depend super-polynomially on the length of the input sequence, depending on the regular language (more discussion in sec.~\ref{sec:diff}). Some regular languages allow special treatment though. For example, consider the language $\mathsf{PARITY}$. $\mathsf{PARITY}$ is the language over alphabet $\{0, 1\}$ with a string $w = (w_1, \ldots, w_j) \in \mathsf{PARITY}$ iff
$w_1+\ldots +w_j = 1 \,\mathrm{mod}\, 2$, for $j \geq 1$. We can show in sec.~\ref{sec:diff} that $\mathsf{PARITY}$ is easily expressible by our concept class with small complexity. RNNs perform well on regular language recognition task in our experiments in Sec.~\ref{sec:expts}. Figuring out which regular languages can be efficiently expressed by our concept class remains an interesting open problem.


%which contains those strings that contain a particular substring. This is generally called the pattern matching problem. 

%a formal language $\mathsf{PARITY}$.  
%Here we are working in the discrete setting where each input token comes from $\{0, 1\}$ possibly represented as a vector when fed to the RNN. For a sequence $\mathbf{z} \in \{0, 1\}^L$, we define $\mathsf{PARITY}(\mathbf{z})$ to be $1$ if the number of $1$'s in $\mathbf{z}$ is odd and define it to be $0$ otherwise. 
%$\mathsf{PARITY}$ is known to be expressible by small one-hidden layer networks with $\relu$ activation; e.g. \cite[Lemma 5]{shalev2017failures}. The $\relu$ can be approximated by polynomials~\cite{lorentz}, implying that our concept class contains functions of small complexity that can approximate $\mathsf{PARITY}$. In Sec.~\ref{sec:diff} we show that  
%RNNs can recognize the language $\mathsf{PARITY}$---but 
%$\mathsf{PARITY}$ is not representable in the concept class of \cite{allen2019can}. 
%As mentioned before, RNNs are closely related to finite automata which accept regular languages.  $\mathsf{PARITY}$ is a regular language and the above result can be extended to many other regular languages. 







