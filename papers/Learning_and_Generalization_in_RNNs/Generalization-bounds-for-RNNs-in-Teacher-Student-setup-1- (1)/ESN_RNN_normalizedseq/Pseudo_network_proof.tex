We first define a pseudo RNN model, which is shown later to stay close to the RNN model during the gradient descent dynamics. 

\begin{definition}[Pseudo Recurrent Neural Network]
	Given two matrices $\mathbf{W}^{\ast} \in \mathbb{R}^{m \times m}$ and $\mathbf{A}^{\ast} \in \mathbb{R}^{m \times d}$, the output for a pseudo RNN with activation function $\relu$ for a given sequence $\bx$ are given by
	\begin{equation*}
		%\bar{\mathbf{h}}_{(t)} &=  \mathbb{I}_{\mathbf{A}^{(0)T} \mathbf{x}_{(t)} + \mathbf{W}^{(0)T} \bar{\mathbf{h}}_{(t-1)} \ge 0} \mathbf{A}^{T} \mathbf{x}_{(t)} + \mathbf{W}^{T} \bar{\mathbf{h}}_{(t-1)}  \\
		F_s^{(\ell)}(\bx; \mathbf{W}^{\ast}, \mathbf{A}^{\ast})   =  \sum_{i \le \ell} \mathbf{Back}_{i \to \ell, s} \mathbf{D}^{(i)} \left(\mathbf{W}^{\ast} \mathbf{h}^{(i-1)} + \mathbf{A}^{\ast} \mathbf{x}^{(i)}\right) \quad \forall 1 \le \ell \le L,  s \in [\dout ],
		%\sum_{r \in [m]} b_{r, s} \bar{h}_{(t), r} \quad \forall t \ge 1,  s \in [\dout ],
	\end{equation*}
	where $\mathbf{Back}_{i \to \ell, s} = \mathbf{b}_s^{\top} \mathbf{D}^{(\ell)} \mathbf{W} \cdots \mathbf{D}^{(i+1)} \mathbf{W}$. For typographical simplicity, we will denote $F_s^{(\ell)}(\bx; \mathbf{W}^{\ast}, \mathbf{A}^{\ast})$ as $F_s^{(\ell)}$.
	%$\mathbf{D}^{(i)}$ denotes the matrix of activation pattern at RNN unit $i$ at initialization, whose elements are given by
	%\begin{align*}
	%    d^{(i)}_{jk} &= \mathbb{I}_{\mathbf{w}_{k}^{\top} \mathbf{h}^{(i-1)} + \mathbf{a}_{k}^{\top} \mathbf{x}^{(i)} \ge 0} \quad \text{if } k = j\\&= 0; \quad\quad\quad \text{otherwise}, \forall k, j \in [m].
	%\end{align*}
\end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Now, we show that there exist two matrices $\mathbf{W}^{\ast}$ and $\mathbf{A}^{\ast}$, defined below, such that the pseudo network is close to the concept class under consideration.
\begin{definition}\label{def:existence}
	Define $\mathbf{W}^{\ast}$ and $\mathbf{A}^{\ast}$ as follows.
	\begin{align*}
		\mathbf{W}^{\ast} &= 0 \\
		\mathbf{a}^{*}_{r} &= \frac{\dout }{m} \sum_{s \in [\dout ]} \sum_{r' \in [p]} b_{r, s} b_{r', s}^{\dagger} H_{r', s} \left(\theta_{r', s} \left(\langle \mathbf{w}_{r}, \overline{\mathbf{W}}^{[L]} \mathbf{w}_{r', s}^{\dagger}\rangle\right), \sqrt{m/2} a_{r, d}\right) \mathbf{e}_d, \quad \forall r \in [m],
	\end{align*}
	where
	\begin{equation*} 
		\theta_{r', s} = \frac{\sqrt{m/2}}{\norm[1]{ \overline{\mathbf{W}}^{[L]} \mathbf{w}_{r', s}^{\dagger}}},
	\end{equation*}
	and $\obW^{[L]} = [\mathbf{W}^{(L, 3)} \mathbf{D}_{(0)}^{(2)}\mathbf{A}_{[d-1]}, \mathbf{W}^{(L, 3)} \mathbf{D}_{(0)}^{(2)}\mathbf{A}_{[d-1]}, \cdots, \mathbf{W}^{(L, L)} 
	\mathbf{D}_{(0)}^{(L-1)}\mathbf{A}_{[d-1]}]_r$, where $\mathbf{W}^{(k_b, k_e)} = \prod_{k_b \ge \ell > k_e} \mathbf{D}_{(0)}^{(\ell)} \mathbf{W}$.
	%Here $\mathbf{P}$ denotes a diagonal matrix that projects a vector onto its top $L(d-1)$ coordinates and  $\mathbf{P}^{\perp} = \mathbf{I} - \mathbf{P}$.
\end{definition}
%\todo{There is a dimension mismatch. Pls correct later on.}


In the following theorem, we show that the pseudo RNN can approximate the target concept class, using the weight $\mathbf{W}^{*}$ and $\mathbf{A}^{\ast}$ define above.
\begin{theorem}[Existence of Good Pseudo Network]\label{thm:existence_pseudo}
	The construction of $\mathbf{W}^{*}$ and $\mathbf{A}^{\ast}$ in Definition~\ref{def:existence} satisfies the following. For every normalized input sequence $\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(L)}$, we have with probability at least $1-e^{-\Omega\left(\rho^{2}\right)}$ over $\mathbf{W}, \mathbf{A}, \mathbf{B},$ it holds for every $s \in [\dout ]$.
	$$
	\begin{array}{l}
		F_{s}^{(L)} \stackrel{\text { def }}{=} \sum_{i=1}^{L} \mathbf{e}_{s}^{\top} \mathbf{Back}_{i \rightarrow L} D^{(i)} \left(\mathbf{W}^{\ast} \mathbf{h}^{(i-1)} + \mathbf{A}^{\ast} \mathbf{x}^{(i)}\right) \\
		= \sum_{r \in[p]} b_{r, s}^{\dagger} \Phi_{r, s} \left(\left\langle  \mathbf{w}_{r, s}^{\dagger}, [\overline{\mathbf{x}}^{(2)}, \cdots, \overline{\mathbf{x}}^{(L-2)}]\right\rangle\right)\\ \pm \mathcal{O}(\dout Lp\rho^2 \varepsilon + \dout L^{17/6} p \rho^4 L_{\Phi} \varepsilon_x^{2/3} + \dout ^{3/2}L^5 p \rho^{11} L_{\Phi} C_{\Phi}  \mathfrak{C}_{\varepsilon}(\Phi, \mathcal{O}(\varepsilon_x^{-1}))  m^{-1/30} ).
		%\left(p \rho^{11} \cdot O\left(\varepsilon_{e}+\mathfrak{C}_{s}(\Phi, 1) \varepsilon_{x}^{1 / 3}+C m^{-0.05}\right)\right)
	\end{array}
	$$
	%We partition the set of $m$ input neurons into $p$ non-overlapping sets.
	%hen, define the weights $\left\{\mathbf{w}^{*}_{k + \frac{lm}{p}}\right\}_{k \in \left[\frac{m}{p}\right], l \in [p]}$ as follows.
	%Construct a matrix $\mathbf{C} \in \mathbb{R}^{p \times m}$ as follows. For each of the functions $f_i(\mathbf{x}) = \phi(\widetilde{\mathbf{a}}_i^{\top} \mathbf{x})$, we can construct vectors $\widetilde{\mathbf{c}}_i$ such that
	%\begin{equation*}
	%   \sup_{\mathbf{x \in \mathcal{X}}} \abs{f_i\left(\mathbf{x}\right) - \sum_{r = i  \frac{m}{2p} }^{(i + 1)  \frac{m}{2p}} \widetilde{c}_{i, r} \sigma\left(\mathbf{a}_r^{(0)\top} \mathbf{x}\right)} \le \varepsilon_i
	%\end{equation*} 
	%holds true with probability at-least $1 - \delta_i$, using \autoref{Thm:Rep_power}. Fix the rows of $\mathbf{C}$ as
	%\begin{align*}
	%   c_{ij} &= \widetilde{c}_{ij}, \quad \text{ if } i  \frac{m}{2p} \le j <  (i + 1)  \frac{m}{2p} \\ &=0 \quad \quad \text{otherwise}, \quad \forall i \in [p].
	%\end{align*}
	%Now,  define the weights $\left\{\mathbf{a}^{*}_{r}\right\}_{r \in [\frac{m}{2}, m-1]}$ as follows.
	%\begin{equation*}
	%\mathbf{a}^{*}_{r} = \frac{2}{m} \sum_{s \in [\dout ]} \sum_{r' \in [p]} b_{r, s} b_{r', s}^{\dagger} H\left(\theta_{r'} \left(\langle \mathbf{w}_{r}, \mathbf{C}^T \mathbf{\widetilde{w}}_{r'}\rangle + \langle \mathbf{a}_{r}^{[d-1]},  \mathbf{\widetilde{a}}_{r'}  \rangle\right), \theta_{r'}  b_{r, s}\right) \mathbf{e}_d,
	%\end{equation*}
	%where $\theta_{r'}$ is given by
	%\begin{equation*} 
	%\theta_{r'} = \frac{\sqrt{m}}{\sqrt{ \norm{\mathbf{a}_{r'}}^2 + \norm{\mathbf{C}^T \mathbf{\widetilde{w}}_{r'}}^2 }}.
	%\end{equation*}
	%Set $\left\{\mathbf{w}_{r}^*\right\}_{r \in [m]}$ and $\left\{\mathbf{a}^{*}_r\right\}_{r \in [\frac{m}{2} - 1]}$ as zero vectors.
	%For each $l \in [p]$, $\mathbf{\widetilde{a'}}_l\in \mathbb{R}^{m}$ is defined by 
	%\begin{equation*}
	%\widetilde{a'}_{l, i + \gamma \frac{m}{p}} =  \sqrt{\frac{\dout }{m}}\widetilde{a}_{l, \gamma}, \quad \forall \gamma \in [p], i \in \left[\frac{m}{p}\right].
	%\end{equation*}  
\end{theorem}

\begin{proof}
	The theorem has been restated and proven in theorem~\ref{thm:existence_pseudo_proof}.
\end{proof}
