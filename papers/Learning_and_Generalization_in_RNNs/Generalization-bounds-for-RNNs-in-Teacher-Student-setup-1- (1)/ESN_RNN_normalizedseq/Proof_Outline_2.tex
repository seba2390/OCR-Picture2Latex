Recall that $\mathbf{W}, \mathbf{A} \sim \mathcal{N}(\mathbf{0}, \frac{2}{m}\mathbf{I})$. Also, recall that we have assumed for simplicity $\dout  = 1$. Hence, $\mathbf{B}$ and $\mathbf{Back}_{i \to L}$ are row and column vectors respectively. For typographical simplicty, denote by $b_r$ and $\mathbf{Back}_{i \to L, r}$ the respective $r$-th components of these vectors.



We set $\mathbf{W}^{\ast} := \mathbf{0}$ and for every $r \in [m]$, $\mathbf{a}_r^{\ast} :=\frac{1}{m} b_r H( \sqrt{m/2} (\langle [\mathbf{w}_{r}, \mathbf{a}_{r, :d-1}],  \overline{\mathbf{w}} \rangle), \sqrt{m/2} a_{r, d}) \mathbf{e}_d$, for a function $H$ that we will describe below. With these choices we have
    \begin{align*}
        &F^{(L)}(\bx; \mathbf{W}^{\ast}, \mathbf{A}^{\ast})  = \sum_{i=1}^{L}  \mathbf{Back}_{i \rightarrow L} \mathbf{D}^{(i)} \left(\mathbf{W}^{\ast} \mathbf{h}^{(i-1)} + \mathbf{A}^{\ast} \mathbf{x}^{(i)}\right) 
        \\&= \frac{1}{m} \sum_{i=1}^{L}   \sum_{r \in [m]}  b_{r}  \mathbf{Back}_{i \to L, r}  H( \sqrt{m/2} (\langle [\mathbf{w}_r, \mathbf{a}_{r, :d-1}],  \overline{\mathbf{w}} \rangle), \sqrt{m/2} a_{r, d})    \cdot \mathbb{I}_{\mathbf{w}_r^{\top} \mathbf{h}^{(i-1)} + \mathbf{a}_r^{\top} \mathbf{x}^{(i)} \ge 0}. %\label{eq:defineF_outline}
	\end{align*}
	In the last step, we have simplified the formula using sum over neurons.  
	The first $L-1$ summands in the outer sum above nearly vanish due to small correlation between $\mathbf{B}$ and $\mathbf{Back}_{i \to L}$ for $i < L$ (see  Lemma~\ref{lemma:backward_correlation}). Recall that $\mathbf{Back}_{L \to L} = \mathbf{B}$ and thus the correlation is not small for $i=L$. This  gives
	%Now, we show the following:
	%\begin{itemize}
		%\item 
	%	The summands in \eqref{eq:defineF_outline} for $i \ne L$ are small due to small correlation between $\mathbf{B}$ and $\mathbf{Back}_{i \to L}$ (see  Lemma~\ref{lemma:backward_correlation} in the appendix).
	%\end{itemize}
	\begin{align*}
		F^{(L)}(\bx; \mathbf{W}^{\ast}, \mathbf{A}^{\ast}) &\approx   \frac{1}{m} \sum_{r \in [m]}  b_{r}^2 H( \sqrt{m/2} (\langle [\mathbf{w}_r, \mathbf{a}_{r, :d-1}],  \overline{\mathbf{w}} \rangle), \sqrt{m/2} a_{r, d})  \cdot \mathbb{I}_{\mathbf{w}_r^{\top} \mathbf{h}^{(i-1)} + \mathbf{a}_r^{\top} \mathbf{x}^{(i)} \ge 0}, %\\&
		%=  \sum_{r' \in [p]} b_{r', s}^{\dagger} \cdot  \Big(\frac{1}{m} \sum_{r \in [m]}  b_{r, s}^2 \cdot \mathbb{I}_{\mathbf{w}_r^{\top} \mathbf{h}^{(L-1)} + \mathbf{a}_{r, d} \ge 0}\\& \quad \quad \cdot H_{r', s}\Big(\theta_{r', s} \langle \mathbf{w}_{r}, \overline{\mathbf{W}}^{[L]} \mathbf{w}_{r', s}^{\dagger}\rangle , \sqrt{m/2} a_{r, d}\Big)  \Big).
	\end{align*}
    Now, this resembles a discretized version of Lemma~\ref{lemma:Expressfunction_fNN}. We can substitute $\mathbf{u}$ as $[\mathbf{h}^{(L-1)}, \mathbf{x}^{(L)}]$ in Lemma~\ref{lemma:Expressfunction_fNN} and use concentration bounds with respect to the randomness of weights $\mathbf{W}$ and $\mathbf{A}$ to complete the proof. 
    %Using the definition of $\mathbf{H}_{r', s}$, we can then show that
	%\begin{align*}
	%	F_{s}^{(L)} &\approx \sum_{r' \in [p]} b_{r', s}^{\dagger} \cdot \Phi_{r', s} \left( \varepsilon_x^{-1} \langle     \mathbf{w}_{r', s}^{\dagger}, \overline{\mathbf{W}}^{[L]\top} \mathbf{h}^{(L-1)} \rangle \right) \\&
	%	\approx \sum_{r' \in [p]} b_{r', s}^{\dagger} \cdot \Phi_{r', s} \left(\varepsilon_x^{-1} \langle     \mathbf{w}_{r', s}^{\dagger}, [\mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(L-1)}] \rangle  \right) \\&
	%	= \sum_{r' \in [p]} b_{r', s}^{\dagger} \cdot \Phi_{r', s} \left( \langle     \mathbf{w}_{r', s}^{\dagger}, [\overline{\mathbf{x}}^{(2)}, \ldots, \overline{\mathbf{x}}^{(L-1)}] \rangle \right),
	%\end{align*}
	%where in the second step, we used the invertibility property of $\overline{\mathbf{W}}^{[L]}$ from Theorem~\ref{thm:Invertibility_ESN_outline} and in the final step, we used the structure of the modified input sequence.