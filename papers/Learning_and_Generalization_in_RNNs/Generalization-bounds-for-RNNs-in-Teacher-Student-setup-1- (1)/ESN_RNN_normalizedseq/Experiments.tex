\section{Experiments}\label{sec:expts}

\iffalse
\begin{center}
 \begin{tabular}{|| c | c | c ||} 
 \hline
 Task & RNN(Relu) & RNN(Tanh)  \\ [0.5ex] 
 \hline\hline
  Counter-2 & 0.15 & 0.1\\
  Counter-3 & 0.02 & 0.02\\
  Counter-4 & 0.06 & 0.02 \\
  Boolean-3 & 0.99 & 0.756 \\
  Boolean-5 & 0.0 & 0.766\\
  Shuffle-2 & 0.966 & 0.60\\
  Shuffle-4 & 0.599 & 0.202\\
  Shuffle-6 & 0.348 & 0.338\\ [1ex]
 \hline
\end{tabular}
\end{center}
\fi 


%\iffalse
\begin{figure}[!ht]
\centering
\iffalse
\includegraphics[width=.45\textwidth]{../ESN_RNN_normalizedseq/RNN_2.png}
\includegraphics[width=.45\textwidth]{../ESN_RNN_normalizedseq/RNN_4.png}
\includegraphics[width=.45\textwidth]{../ESN_RNN_normalizedseq/RNN_8.png}
\fi
%\iffalse
\begin{subfigure}
  \centering
  \includegraphics[width=0.5\linewidth]{ESN_RNN_normalizedseq/RNN_2.png}
%          \vspace{-2\baselineskip}
  \caption{Data dimension: 2}
\end{subfigure}
\begin{subfigure}
  \centering
  \includegraphics[width=0.5\linewidth]{ESN_RNN_normalizedseq/RNN_4.png}
%\vspace{-2\baselineskip}
  \caption{Data dimension: 4}
\end{subfigure}%\hfill
\begin{subfigure}
  \centering
  \includegraphics[width=0.5\linewidth]{ESN_RNN_normalizedseq/RNN_8.png}
%\vspace{-2\baselineskip}
  \caption{Data dimension: 8}
\end{subfigure}%
%\fi
\caption{Invertibitiliy of RNNs at random initialization: Checking behavior of inversion error with number of neurons and the sequence length at different data dimensions.}
\label{fig:RNN_inver}
\end{figure}
%\fi 
\textbf{RNN inversion at random initialization.} We consider a randomly initialized RNN, with the entries of the weights $\mathbf{W}$ and $\mathbf{A}$ randomly picked from the distribution $\mathcal{N}(0, 1)$. Sequences are generated i.i.d. from normal distribution i.e. for each sequence, $\bx^{(i)} \sim N(0, \mathbf{I})$ for each $i \in [L]$. We use SGD with batch size 128, momentum $0.9$ and learning rate $0.1$ to compute the linear matrix $\obW^{[L]}$ so that $\norm[0]{\obW^{[L]} \mathbf{h}^{(L)} - [\bx^{(1)}, \ldots, \bx^{(L)}]}^2$ is minimized. We compute the following two quantities on the test dataset, containing $1000$ sequences: average $L_2$ error given by $\mathbb{E}_{\bx} \frac{\norm[0]{\obW^{[L]} \mathbf{h}^{(L)} - [\bx^{(1)}, \ldots, \bx^{(L)}]}}{\norm[0]{[\bx^{(1)}, \ldots, \bx^{(L)}]}}$ and average $L_\infty$ error given by $\mathbb{E}_{\bx} \norm[0]{\obW^{[L]} \mathbf{h}^{(L)} - [\bx^{(1)}, \ldots, \bx^{(L)}]}_{\infty}$. We plot both the quantities for different settings of data dimension $d$, sequence length $L$ and the number of neurons $m$. $L$ takes values from the set $\{2, 4, 6\}$, $d$ takes from $\{2, 4, 8\}$ and $m$ takes from $\{500, 1000, 2000, 5000, 10000\}$ (Figure~\ref{fig:RNN_inver}). The trends support our bounds in Theorem~\ref{thm:Invertibility_ESN_outline}, i.e. the error increases with increasing $L$ and decreases with increasing $m$. Note that the data distribution is different from the one assumed in normalized sequence Def.~\ref{def:normalized_seq}. It was easier to conduct experiments in the current data setting and a similar statement as Thm.~\ref{thm:Invertibility_ESN_outline} can be given.



\textbf{Performance of RNNs on different regular languages. } We check the performance of RNNs on the formal language recognition task for a wide variety of regular languages. We follow the set-up in \cite{BhattamishraAG20} who conducted experiments on LSTMs etc. but not on RNNs.

\input{ESN_RNN_normalizedseq/Regularlanguage}
%The test languages consist of Tomita grammars which constitute a popular benchmark, and a number of other languages including $\mathsf{PARITY}$ and cover a variety of capabilities needed to recognize regular languages.
%The details of the regular languages above can be found in \cite{BhattamishraAG20}; we only note that Tomita languages constitute a popular benchmark. 
We vary $m$, the dimension of the hidden state, in the range $[3, 32]$, used RMSProp optimizer~\cite{hinton2014coursera} with the smoothing constant $\alpha = 0.99$ and varied the learning rate in the range $[10^{-2}, 10^{-3}]$. For each language
we train models corresponding to each language
for $100$ epochs and a batch size of $32$. We experimented with two different activations $\relu$ and $\tanh$. 
%The best test accuracies achieved on different languages are given in table~\ref{table:regular} and these were all achieved for $m=32$.
In all but one case (Tomita 7 with ReLU) the test accuracies with near-perfect. This was the case across runs. Tomita 7 results could perhaps be improved by more extensive hyperparameter tuning. 
We train and test on strings of length up to 50, and in a few cases strings of larger lengths (when the number of strings in the language is small). 
%Details are in Appendix~\ref{sec:regular}.




\iffalse
\begin{figure*}

\centering
\includegraphics[width=.45\textwidth]{../ESN_RNN_normalizedseq/RNN_2.png}
\includegraphics[width=.45\textwidth]{../ESN_RNN_normalizedseq/RNN_4.png}
\includegraphics[width=.5\textwidth]{../ESN_RNN_normalizedseq/RNN_8.png}
\caption{Invertibility of RNNs at initialization}
%\label{fig:figure3}
\end{figure*}
%\FloatBarrier


\begin{center}
\begin{table}[!ht]
\centering
\begin{tabular}{|| c | c | c ||} 
 \hline
 Task & RNN(Relu) & RNN(Tanh)  \\ [0.5ex] 
 \hline\hline
 Tomita 1 & 1.0 & 1.0 \\
 Tomita 2 & 1.0 & 1.0 \\
 Tomita 3 & 1.0 & 1.0 \\
 Tomita 4 & 1.0 & 1.0 \\
 Tomita 5 & 1.0 & 1.0\\
 Tomita 6 & 1.0 & 1.0\\
 Tomita 7 & 0.259 & 0.99\\
 Parity & 1.0 & 1.0\\
 $\mathcal{D}_1$ & 1.0 & 1.0 \\
 $\mathcal{D}_2$ & 0.99 & 1.0\\
 $\mathcal{D}_4$ & 1.0 & 0.99\\
 $(aa)^{\ast}$ & 1.0 & 1.0\\
 $(abab)^{\ast}$ & 0.99 & 1.0 \\
 $(aa)^{\ast}(bb)^{\star}$ & 0.99 & 1.0 
 %\\Dyck-1 & 0.99 & \textbf{0.91} 
 \\[1ex]
 \hline
\end{tabular}
\caption{Performance of RNNs on different regular languages.}
\label{table:regular}
\end{table}
\end{center}
\fi
