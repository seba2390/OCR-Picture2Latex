\section{Generalization bounds of Recurrent neural networks}\label{sec:maingensec}
The proof has been structured as follows: In section~\ref{sec:Invert_RNN}, we prove thm.~\ref{thm:Invertibility_ESN} where we show that a linear transformation of $\mathbf{h}^{(L)}$ can give back $[\bx^{(1)}, \ldots, \bx^{(L)}]$. The proof follows from a direct application of lemma~\ref{lemma:singlecell_ESN}. Claim~\ref{clam:stabizable_V} shows that the linear matrix at each induction step satisfies a property of stability necessary for the inductive application of lemma~\ref{lemma:singlecell_ESN}.  

In section~\ref{sec:existence}, we first define a pseudo recurrent neural network that stays close to the over parameterized RNN at initialization throughout SGD. We then show in thm.~\ref{thm:existence_pseudo} that there exists a pseudo network which can approximate the target function in concept class. The proof involves breaking correlations among the hidden states and the weight matrices and then we show that the pseudo network concentrates on the desired signal. The above two steps have been divided among the four intermediate claims: ~\ref{claim:simplifybig}, ~\ref{claim:diffftildef}, ~\ref{claim:difftildefphi} and ~\ref{claim:fbacktildeback}.

In section~\ref{sec:optim_general}, we prove theorem~\ref{thm:main_theorem} which shows that RNNs can attain a population risk similar to the target function in the concept class using SGD. First, we show that the pseudo neural network stays close to RNN with small perturbation around initialization in lemmas~\ref{lemma:perturb_NTK_small_output} and~\ref{lemma:perturb_NTK_small}. We then show that there exists a RNN close to random RNN that can approximate the target function in lemma~\ref{lemma:perturb_small_target}. We complete the argument by showing that the SGD can find matrices with training loss close to the optimal in lemma~\ref{lem:trainloss} and then bounding the Rademacher complexity of RNNs with bounds on the movement in the weight matrices in lemma~\ref{lem:radcomp}.


    %First, we give a few definitions about a recurrent neural network (RNN) and the structure of the input sequence to the RNN
    

    \subsection{Invertibility of RNNs at initialization}\label{sec:Invert_RNN}    
    \input{ESN_RNN_normalizedseq/ESN_Invertibility_normalizedseq}
    
   
    \subsection{Existence of good pseudo network}\label{sec:existence}
    \input{ESN_RNN_normalizedseq/Pseudo_network_proof}


