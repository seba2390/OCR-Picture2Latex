\section{Limitations and Conclusions} \label{sec:conclusion}

We proved the first result on the training and generalization of RNNs when the functions in the concept class are allowed to be essentially arbitrary continuous functions of the token sequence. Conceptually the main new idea was to show that the hidden state of the RNN contains information about the whole input sequence and this can be recovered via a linear transformation. We believe our techniques can be used to prove similar results for echo state networks.  

Two main limitations of the present work are: (1) Our overparametrized setting requires the number of neurons to be large in terms of the problem parameters including the sequence length---and it is often qualitatively different from the practical setting. Theoretical analysis of practical parameter setting remains an outstanding challenge---even for one-hidden layer FFNs. 
%We also made use of sequence normalization though this involved very simple preprocessing and our techniques work without processing though the error would then have exponential dependence on the sequence length.
(2) We did not consider generalization to sequences longer than those in the training data. Such a result would be very interesting but it appears that it would require stronger assumptions than our very general assumptions about the data distribution. Our techniques might be a useful starting point to that end: for example, if we knew that the distributions of the hidden states are similar at different times steps and the output is the same as the hidden state (i.e. $\mathbf{B}$ is the identity) then our results might easily generalize to higher lengths. We note that to our knowledge the limitation noted here holds for all works dealing with generalization for RNNs.
(3) Understanding LSTMs remains open.




