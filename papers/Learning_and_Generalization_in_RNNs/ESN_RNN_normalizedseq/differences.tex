\section{On Concept Classes}\label{sec:diff}
    The concept class in \cite{allen2019can} matched the output to a true label at each step using loss function $G$, i.e. $F^{\ast}$ belongs to $\mathbb{R}^{ L \times d} \to \Reals^{\dout}$, given by
    \begin{equation} \label{eqn:AL_concept_class}
    F^{\ast (j)}_s\left(\mathbf{x}\right) = \sum_{i : i < j} \sum_{r \in [p]} \phi_{i \to j, r, s} (\mathbf{w}_{i \to j, r, s}^T \mathbf{x}^{(i)}), 
    \end{equation}
    for all $j \in [2, L]$ and $s \in [\dout ]$. Here $\phi_{i \to j, r, s}: \Reals \to \Reals$ are smooth functions and $\mathbf{w}_{i \to j, r, s}$ unit vectors. We can rewrite \eqref{eqn:AL_concept_class} in a more compact vector form
    \begin{equation} \label{eqn:AL_concept_class_simplified}
        F^{\ast (j)}\left(\mathbf{x}\right) =  \sum_{i : i < j} \psi_{i \to j}(\mathbf{x}^{(i)}),
    \end{equation}
where $\psi_{i \to j}(\mathbf{x}^{(i)})$ is defined in the obvious way: it is the vector of inner sums in \eqref{eqn:AL_concept_class}.
 %for $s \in [\dout ]$. 
 From the previous equation it is clear that $F^{\ast (j)}(\mathbf{x}^{(i)})$ is a \emph{sum} of 
functions of individual tokens $\mathbf{x}^{(i)}$. This suggests that this concept class can represent only a limited set of concepts. A clean framework for illustrating these issues is afforded by the task of recognizing membership in a given formal language. Fix a finite alphabet $\Sigma$ with each letter also encoded by a vector so that it can be processed by RNNs. We say that the RNN recognizes a language $\Lambda$ over $\Sigma$ if after processing the sequences $(\bx^{(1)}, \ldots, \bx^{(j)})$ encoding a string $w = (w_1, \ldots, w_j) \in \Sigma^j$, the output $\mathbf{y}^{(j)}$ satisfies $\abs[0]{\mathbf{y}^{(j)}-1}<1/3$, if $w \in \Lambda$ and $\abs[0]{\mathbf{y}^{(j)}}<1/3$, otherwise. For simplicity, in the following, we will require the more stringent conditions $\mathbf{y}^{(j)}=1$ and  $\mathbf{y}^{(j)}=0$; these can be easily relaxed with some extra work. Since our output is binary, the output dimension $\dout $ is set to $1$; this is the setting in which our experiments are also done.

    Below, we give examples of some simple regular languages that the above concept class can't recognize but can be recognized by functions in our concept class with small complexity. 
  \iffalse
        \begin{theorem}
        $\mathsf{PARITY}$ cannot be represented by the concept class in \eqref{eqn:AL_concept_class}.
    \end{theorem}
\begin{proof} \emph{(sketch.)} For strings in $w \in \{0, 1\}^j$, we can rewrite \eqref{eqn:AL_concept_class_simplified} as 
        \begin{equation*}
        F^{\ast (j)}\left(\mathbf{x}\right)(w) =  \sum_{i : i < j} \alpha_i(w_i),
    \end{equation*}
where each $(\alpha_i(0), \alpha_i(1)) \in \Reals^2$ is any two-dimensional vector. Thus, $F^{\ast (j)}\left(\mathbf{x}\right)(w)$
can be written as a \emph{linear} function of $w$. It is easy to show and well-known that linear functions have correlation $0$ with $\mathsf{PARITY}$ \cite{odonnell} (this requires a standard linear change in parametrization to functions over $\{-1,1\}^j$ with output also in $\{-1,1\}$). 
\end{proof}
\fi

We first consider a simple regular language $L_1$ over the alphabet $\{0,1\}$ given by the regular expression $0^\ast 10^\ast$. In words, a string is in $L_1$ iff it contains a single $1$. This language can be thought of as modeling the occurrence of an event (a single blip) in a time series.

Consider the set $S$ of strings $\{0^q 00 0^{L-q-2}, 0^q 11 0^{L-q-2}, 0^q 01 0^{L-q-2}, 0^q 10 0^{L-q-2}\}$ where $0 \leq q \leq L-2$. 
Clearly, $0^q 00 0^{L-q-2} \notin L_1$ and $0^q 11 0^{L-q-2} \notin L_1$ whereas $0^q 01 0^{L-q-2} \in L_1$ and $0^q 10 0^{L-q-2} \in L_1$.
We choose uniform distribution on $S$ as the data distribution $D_{L_1}$. 
\begin{theorem}\label{thm:allencantdl1}
Any concept class of type \eqref{eqn:AL_concept_class} must err with probability at least $1/4$ on $D_{L_1}$.
\end{theorem}
\begin{proof}\emph{(sketch)} 
Fix a $q$. Let $w=0^q a b 0^{L-q-2}$ where $a, b \in \{0, 1\}$, we can rewrite \eqref{eqn:AL_concept_class_simplified} as 
\begin{align*}
    F^{\ast (L)}(w) &= 
    \sum_{i : i \leq q } \alpha_i(0) +  \alpha_{q+1}(a) + \alpha_{q+2}(b) + \sum_{i : q+3 \leq i \leq L } \alpha_i(0) \\
    &= A + \alpha_{q+1}(a) + \alpha_{q+2}(b),
\end{align*}
where each $(\alpha_i(0), \alpha_i(1)) \in \Reals^2$ is any two-dimensional vector.
Now, we must have $A + \alpha_{q+1}(1) + \alpha_{q+2}(0) = 1$ and $A + \alpha_{q+1}(0) + \alpha_{q+2}(1) = 1$. And also,
$A + \alpha_{q+1}(0) + \alpha_{q+2}(0) = 0$ and $A + \alpha_{q+1}(1) + \alpha_{q+2}(1) = 0$. Summing the first two equations gives
$2A +  \alpha_{q+1}(0)  + \alpha_{q+1}(1) + \alpha_{q+2}(0) + \alpha_{q+2}(1) = 2$ and summing the next two equations gives
$2A +  \alpha_{q+1}(0)  + \alpha_{q+1}(1) + \alpha_{q+2}(0) + \alpha_{q+2}(1) = 0$. Thus at least one of the four equations above must fail.
The concept class thus incurs an error with probability at least 1/4.
\end{proof}


We can show that our concept class (Eq.~\eqref{eq:concept_class}) can recognize the language $D_{L_1}$. Assume that we get an length-$L$ string as a length-$L$ input sequence $\bx$, with `$0$' represented by one-dimensional vector $0$ and `$1$' represented by one-dimensional vector $1$. E.g. `$0010$' will be represented as a sequence $0,0,1,0$. Then, one can count the number of $1$'s in the given input and claim that if the number of $1$'s is exactly $1$, the string belongs to the language $D_{L_1}$. The required condition can be checked using a single neuron with activation $\phi(x) = 2x - x^2$, which is a quadratic activation, and weight vector containing all ones ($\mathbf{1}$).  Hence, one can show that acceptance condition is satisfied iff $\phi(\langle\mathbf{1}, \bx\rangle + 1/2)$ is positive. Thus overall, we have shown that the language $D_{L_1}$ can be computed by a one-hidden layer neural network with a quadratic activation and $1$ neuron, implying that our concept class can approximate the language $D_{L_1}$.

\paragraph{Other pattern matching languages.} $D_{L_1}$ can be thought of as a very simple pattern matching problem.  In fact, we can show a more general class of languages that can be learned by our concept class efficiently. Consider the following language: a string (of length at most $L$) belongs to the language iff it contains a particular substring (of some constant length $k$). We will denote this substring by $\bar{s}$. Assume that we get an $L$-length string as $L$-dimensional input $\bx$, with `$0$' represented by a one-dimensional vector $-1$ and `$1$' represented by one-dimensional vector $1$. E.g. `$0010$' will be represented by the sequence $-1,-1,1,-1$. Let $\mathbf{v}_{\bar{s}}$ denote the vector representation of the sequence for the substring $\bar{s}$. Then, we can enumerate all the consecutive substrings in the input and check if the required substring occurs in at least one of them. Mathematically, this translates to creating a one layer neural network with $(L-k+1)$ neurons and activation function $\phi(t) = e^{ct}$, for some constant $c = \Omega(\log L)$. The $i$-th neuron will contain the weight vector $\mathbf{v}_i$, where the substring between position $i$ and $i+k-1$ contains $\mathbf{v}_{\bar{s}}$ and the rest of the positions contain $0$. One can check that if the input string contains the desired substring $\bar{s}$, then
$\sum_{i=1}^{L} \phi(\langle \mathbf{v}_i, \bx\rangle - k) \ge 1$, otherwise it is less than $\frac{1}{L}$. Thus, overall we have shown that the language can be recognized by a one-layer network with exponential activations. Since, we have discussed before that exponential activations have $O(1)$ complexity (see Def.~\ref{def:complexity}), we have shown that our concept class can efficiently solve the pattern matching problem.

We can generalize the above ideas to address some other related problems where we need to find multiple substrings, problems where we need to make sure that the number of times a particular substring occurs is at most a certain limit, etc.


\paragraph{General regular languages.} More generally, our concept class can express all regular languages. However, the complexity of the concept class can be super-polynomial in the sequence length $L$ depending on the regular language. Here is a sketch of a general construction.
As previously mentioned, RNNs with ReLU activations and finite precision are known to be equivalent to deterministic finite automata (DFA) and thus capture regular languages \cite{korsky2019computational}. 
    The ReLU can be approximated by polynomials \cite{lorentz} so that the resulting RNN still approximates the DFA up to some required length (the larger the length, the better the approximation needs to be---and the higher the degree of the approximating polynomial). In turn, such an RNN using polynomial activations can be easily represented by our concept class. The complexity (Def.~\ref{def:complexity}) of the concept class is small as polynomials have small complexity. We omit the routine but technical details of this construction. 
    
    Many regular languages allow special treatment though. For example, consider the language $\mathsf{PARITY}$. $\mathsf{PARITY}$ is the language over alphabet $\{0, 1\}$ with a string $w = (w_1, \ldots, w_j) \in \mathsf{PARITY}$ iff
$w_1+\ldots +w_j = 1 \,\mathrm{mod}\, 2$, for $j \geq 1$. We can show that $\mathsf{PARITY}$ is hard for the above concept class for the uniform distribution on $\{0,1\}^L$. A simple proof of this can be obtained via Boolean Fourier analysis (e.g., \cite{odonnell}) which we now sketch. In this setting, 
we note that $\mathsf{PARITY}$ of $L$ bits corresponds to a degree-$L$ polynomial $(2 w_1 -1 )(2w_2 -1)\ldots (2w_L-1)$; the output now takes values in $\{-1, 1\}$ instead of $\{0, 1\}$. On the other hand, the functions in \eqref{eqn:AL_concept_class} with $\dout=1$ correspond to linear functions of the form $\sum_i \alpha_i w_i + \beta_i$ for some constants $\alpha_i, \beta_i \in \Reals$ for all $i$. Using these facts, the correlation between the two can be easily shown to be $0$ via the Plancherel--Parseval theorem, which implies that all functions of type \eqref{eqn:AL_concept_class} make significant error on $\mathsf{PARITY}$. 

However, we can show that $\mathsf{PARITY}$ is easily expressible by our concept class with small complexity. Assume that we get an length-$L$ string as a length-$L$ input sequence $\bx$, with `$0$' represented by one-dimensional vector $0$ and `$1$' represented by one-dimensional vector $1$. E.g. `$0010$' will be represented as a sequence $0,0,1,0$. Then, one can count the number of $1$'s in the given input and claim that if the number of $1$'s is even, the string belongs to the language $\mathsf{PARITY}$. The required condition can be checked using a single neuron with activation $\phi(x) = \cos(\pi x)$ and weight vector containing all ones ($\mathbf{1}$). Hence, one can show that acceptance condition is satisfied iff $\phi( \langle\mathbf{1}, \bx\rangle - 1)$ is positive. Thus overall, we have shown that the language $\mathsf{PARITY}$ can be computed by a one-hidden layer neural network with a $\cos$ activation and $1$ neuron. Since, we have discussed before that $\cos$ activations have $O(1)$ complexity (see Def.~\ref{def:complexity}), we have shown that our concept class can efficiently recognize $\mathsf{PARITY}$.


%However, $\mathsf{PARITY}$ is known to be expressible by small one-hidden layer networks with $\relu$ activation; e.g. \cite[Lemma 5]{shalev2017failures}. The $\relu$ can be approximated by polynomials~\cite{lorentz}, implying that our concept class contains functions that can approximate $\mathsf{PARITY}$. However, we note here that to approximate $\relu$ within error $\varepsilon$ one needs a degree-$\mathrm{poly}(1/\varepsilon)$ polynomial and this also translates into high complexity of the concept class. %That implies, our concept class can have complexity exponential w.r.t. error $1/\epsilon$. 
%There might be a more efficient construcion of a concept class that learns the language $\mathsf{PARITY}$. This remains an interesting open problem.



We performed experiments on the ability of RNNs to learn various regular languages (see sec.~\ref{sec:expts} for details). In almost all of the regular languages that we tested on, RNNs can achieve near perfect test accuracies (table~\ref{table:regular}).

 \iffalse    
    \begin{theorem}
         Assume that the alphabet is $\{a, b\}$ and the sequences in $\mathcal{D}$ are generated from the language $a^{\ast}b^{\ast}a^{\ast}$ with positive label for strings of the form $a^{\ast} b a^{\ast}$ and a negative label for the rest. Also, assume that $\dout =2$ with an "accept" logit and a "reject" logit and the loss function $G$ is a classification loss (cross entropy, hinge loss, etc.) Then, any function in the above concept class will make an error on at least $\Omega(\frac{1}{L^2})$ fraction of the samples in $\mathcal{D}$.
         %The concept class can't capture the regular language $a^{*}ba^{*}$.
    \end{theorem}
    
    \begin{proof}
        We give a proof outline here. Consider the following strings: $a^{q-2}ba$, $a^{q-2}bb$, $a^{q-2}ab$ and $a^{q-2}aa$, for any $q \in [L]$. By the definition of the distribution $\mathcal{D}$, $a^{q-2}ba$ and $a^{q-2}ab$ have positive label, while $a^{q-2}aa$ and $a^{q-2}bb$ have negative label. Assume that a function $F^{\ast}$ classifies these strings correctly. We will assume that the vector representations of the letters $a$ and $b$ are given by $\bx(a)$ and $\bx(b)$ respectively. $F_{"accept"}^{(\ell)}$ denotes the "accept" logit score at cell $\ell$. Similarly define $F_{"reject"}^{(\ell)}$.   
        
        Given that the strings $a^{q-2}ab$ and $a^{q-2}aa$ must be accepted and rejected respectively, we can show that for any $t \ge q + 1$,
        \begin{align*}
        &\sum_{r \in [p]} ( \phi_{q \to t, r, "accept"} (\mathbf{w}_{q \to t, r, "accept"}^T \mathbf{x}(b)) \\& \quad\quad - \phi_{q \to t, r, "reject"} (\mathbf{w}_{q \to t, r, "reject"}^T \mathbf{x}(b))) \\& < \sum_{r \in [p]} ( \phi_{q \to t, r, "accept"} (\mathbf{w}_{q \to t, r, "accept"}^T \mathbf{x}(a)) \\& \quad\quad- \phi_{q \to t, r, "reject"} (\mathbf{w}_{q \to t, r, "reject"}^T \mathbf{x}(a) )).
        \end{align*}
        
        Similarly, given that the strings $a^{q-2}ba$ and $a^{q-2}bb$ must be accepted and rejected respectively, we can show that for any $t \ge q + 1$,       
        \begin{align*}
        &\sum_{r \in [p]}  ( \phi_{q \to t, r, "accept"} (\mathbf{w}_{q \to t, r, "accept"}^T \mathbf{x}(b)) \\& \quad\quad- \phi_{q \to t, r, "reject"} (\mathbf{w}_{q \to t, r, "reject"}^T \mathbf{x}(b)))  \\ & > \sum_{r \in [p]} ( \phi_{q \to t, r, "accept"} (\mathbf{w}_{q \to t, r, "accept"}^T \mathbf{x}(a)) \\& \quad\quad- \phi_{q \to t, r, "reject"} (\mathbf{w}_{q \to t, r, "reject"}^T \mathbf{x}(a))).
        \end{align*}
        However, both the equations contradict each other. Hence, any function $F^{\ast}$ must make a mistake at at least one of these strings. Counting the number of sets of such strings gives the final result.
    \end{proof}
\fi