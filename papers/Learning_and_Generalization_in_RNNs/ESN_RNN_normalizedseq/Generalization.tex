\subsection{Optimization and Generalization}\label{sec:optim_general}

First, we show that the training loss decreases with gradient descent. The main component of the proof is to show that overparametrized RNN stays close to its pseudo network throughout training. Since, the pseudo network is a linear network, it is easier to check the trajectory of the pseudo network during gradient descent. Since we have already shown that there exists a pseudo network that can approximate the true function, we can show that gradient descent can find some pseudo network that performs as well as the constructed pseudo network. 
\begin{lemma}[Decrease in training loss]\label{lem:trainloss}
   For a constant $\varepsilon_x = \frac{1}{\operatorname{poly}(\rho)}$ and for every constant $\varepsilon \in \left(0, \frac{1}{p \cdot \operatorname{poly}(\rho) \cdot \mathfrak{C}_{\mathfrak{s}}(\Phi, \mathcal{O}(\varepsilon_x^{-1}))}\right),$ there exists $C^{\prime}=\mathfrak{C}_{\varepsilon}(\Phi, \mathcal{O}(\varepsilon_x^{-1}))$, $C_{\Phi} = \mathfrak{C}_{s}(\Phi, \sqrt{2L})$, and a parameter $\lambda=\Theta\left(\frac{\varepsilon}{L \rho}\right)$
so that, as long as $m \geq \operatorname{poly}\left(C', p, L, \dout , \varepsilon^{-1}\right)$ and $N \geq \Omega\left(\frac{\rho^{3} p C_{\Phi}^2}{\varepsilon^2}\right),$ setting learning rate $\eta=\Theta\left(\frac{1}{\varepsilon \rho^{2} m}\right)$ and
$T=\Theta\left(\frac{p^{2}  C'^2 \mathrm{poly}(\rho)}{\varepsilon^{2}}\right),$ we have
\begin{align*}
\underset{\mathrm{sgd}}{\mathbb{E}}\Big[\frac{1}{T} \sum_{t=0}^{T-1}  \underset{(\obx, \mathbf{y}^{\ast}) \sim \mathcal{Z}}{\mathbb{E}} \mathrm{Obj}(\obx, \mathbf{y}^{\ast};  \mathbf{W}+\mathbf{W}_{t}, \mathbf{A} + \mathbf{A}_t) \Big] \leq \mathrm{OPT} + \frac{\varepsilon}{2} + \frac{1}{\mathrm{poly}(\rho)},
\end{align*}
and $\left\|W_{t}\right\|_{F} \leq \frac{\Delta}{\sqrt{m}}$ for $\Delta=\frac{C'^{2} p^{2} \mathrm { poly }(\rho)}{\varepsilon^{2}}$.
\end{lemma}

\begin{proof}
	The lemma has been restated and proven in lemma~\ref{lem:trainloss_proof}.
\end{proof}

Now, we bound the rademacher complexity of overparametrized RNNs.  The main component of the proof is to use the fact that overparametrized RNN stays close to its pseudo network throughout training. Since, the pseudo network is a linear network, it is easier to compute the rademacher complexity of pseudo network.
\begin{lemma}[Rademacher Complexity of RNNs]\label{lem:radcomp}
   For every $s \in [\dout ]$, we have 
   \begin{align*}
      \underset{\zeta \in \{\pm 1\}^{N}}{\mathbb{E}} &\Big[\sup_{\norm{\mathbf{W}'}_F, \norm{\mathbf{A}'}_F \le \frac{\Delta}{\sqrt{m}}} \frac{1}{N} \sum_{q=1}^{N}  \zeta_q F^{(L)}_{\mathrm{rnn}, s} (\obx_q; \mathbf{W}+\mathbf{W}', \mathbf{A} + \mathbf{A}' ) \Big]  \le \mathcal{O}(\frac{\rho^7 \Delta^{4/3}}{m^{1/6}} + \frac{\rho^2 \Delta}{\sqrt{N}}),
   \end{align*}
   where $\overline{\bx}_1, \ldots, \overline{\bx}_N$ denote the training samples in $\mathcal{D}$.
\end{lemma}
\begin{proof}
    The proof follows the same outline as lemma 8.1 in \cite{allen2019can}. We give the outline here for completeness. From lemma~\ref{lemma:perturb_NTK_small_output}, we have w.p. at least $1 - e^{-\Omega(\rho^2)}$ for all $q \in [N]$, $s \in [\dout ]$ and for any $\mathbf{W}', \mathbf{A}'$ with $\abs{\mathbf{W}'}, \abs{\mathbf{A}'} \le \frac{\Delta}{\sqrt{m}}$,
    \begin{align*}
        \abs{F^{(L)}_{\mathrm{rnn}, s} (\obx_q; \mathbf{W}+\mathbf{W}', \mathbf{A} + \mathbf{A}') -  F^{(L)}_{s} (\obx_q; \mathbf{W}', \mathbf{A}') } \le \mathcal{O}(\rho^7 \Delta^{4/3} m^{-1/6}).
    \end{align*}
    Hence, $F_{\mathrm{rnn}}^{(L)}$ is close to $F^{(L)}$ and thus, its rademacher complexity will be close to that of $F^{(L)}$.  Since, $F^{(L)}$ is a linear network, we can apply the rademacher complexity for linear networks (fact~\ref{fact:radcomp_linear}) to get the final bound.
\end{proof}

Now, we can combine both the theorems above to get the following theorem.
\begin{theorem}\label{thm:main_theorem}
   For a constant $\epsilon_x = \frac{1}{\operatorname{poly}(\rho)}$ and for every constant $\varepsilon \in \left(0, \frac{1}{p \cdot \operatorname{poly}(\rho) \cdot \mathfrak{C}_{\mathfrak{s}}(\Phi, \mathcal{O}(\epsilon_x^{-1}))}\right),$ define complexity $C=\mathfrak{C}_{\varepsilon}(\Phi, \mathcal{O}(\epsilon_x^{-1}))$
and $\lambda=\frac{\varepsilon}{10 L \rho},$ if the number of neurons $m \geq \operatorname{poly}\left(C, p, L, \dout , \varepsilon^{-1}\right)$ and the number of samples is $N \geq \operatorname{poly}\left(C, p, L, \dout , \varepsilon^{-1}\right),$ then $S G D$ with $\eta=\Theta\left(\frac{1}{\varepsilon \rho^{2} m}\right)$ and $T=\Theta(p^{2} C^{2} \operatorname{poly}(\rho)\varepsilon^{-2})$
satisfies that, with probability at least $1-e^{-\Omega\left(\rho^{2}\right)}$ over the random initialization
\begin{align*} 
\underset{\mathrm { sgd }}{\mathbb{E}}\Big[\frac{1}{T} &\sum_{t=0}^{T-1} \underset{\left(\obx, y^{\ast}\right) \sim \mathcal{D}}{\mathbb{E}}\Big[ \mathrm{Obj}\Big(\obx, y^{\ast};  \mathbf{W}_{t},  \mathbf{A}_t\Big)\Big]\Big] \leq \mathrm{OPT}+\varepsilon+\frac{1}{\operatorname{poly}(\rho)}.
\end{align*}
\end{theorem}

\begin{proof}
    The proof follows from Fact~\ref{fact:genRad} using Lemma~\ref{lem:trainloss} and Lemma~\ref{lem:radcomp}.
\end{proof}