\section{Invertibility at a single step}\label{sec:singlecell_RNN}
The section has been structured as follows: we first prove that a linear transformation of a random $\relu$ network can give back a linear function of the input in lemma~\ref{lem:normal_linearestimate}. We then explain why a simple application of the above lemma doesn't give a similar lemma for random RNNs which is, we need to make sure we break the correlations among input, the output vector and the weight matrices. We show that such correlations can be broken using the arguments in Claims \ref{claim:ff}, \ref{claim:fg}, \ref{claim:ffv} and \ref{claim:gg}. This then helps us to prove lemma~\ref{lemma:singlecell_ESN} using an application of lemma~\ref{lem:normal_linearestimate}.



\subsection{Invertibility of one layer $\relu$ networks}
The following lemma is from a companion paper; we reproduce its proof here for completeness. 
\begin{lemma}\label{lem:normal_linearestimate}
	For any $\bv \in \Reals^d$, the linear function taking $\bx$ to $\bv^\top \bx$ for $\bx \in \Reals^d$, can be represented as
	\begin{equation}\label{eqn:single_layer_inversion_exact}
		\bv^\top \bx = \int_{\Reals^d} p(\bw) \, \sigma(\mathbf{w}^{\top} \bx ) \,\deriv \mu_d (\mathbf{w}),
	\end{equation}
	with 
	$$p\left(\mathbf{w}\right) \;=\;  2 \, \mathbf{w}^{\top} \mathbf{v}.$$ 
\end{lemma}
\begin{remark}
	A similar statement can be gleaned from the proof of Proposition 4 of \cite{bach2017breaking} which gives a similar representation except that 
	$\bw$ is uniformly distributed on $\Sphere^{d-1}$ instead of being Gaussian. The proof there makes use of spherical harmonics and does not seem to immediately apply to the Gaussian case. The proof below is elementary and can be easily adapted to any spherically-symmetric distribution. 
\end{remark}
%It will be convenient to set 
%\begin{align} \label{eqn:Cd}
%2 := 2,
%\end{align}
%so that $p(\mathbf{w}) = 2 \,\mathbf{w}^{\top} \mathbf{v}$.

\begin{proof}
	\iffalse
	First, let's consider $\sigma$ as the threshold activation.
	For $p(\mathbf{w}) = \mathbf{v}^{\top} \mathbf{w}$, we get
	\begin{align*}
		\int_{\mathbf{w} \in \mathbb{R}^{d}}  \mathbf{v}^{\top} \mathbf{w} \mathbb{I}_{\left(\mathbf{x}^{\top} \mathbf{w} \right) \ge 0} d\mu_d \left(\mathbf{w}\right) 
		&=  \mathbf{v}^{\top} \left(\int_{\mathbf{w} \in \mathbb{R}^{d}} \mathbf{w} \mathbb{I}_{\left(\mathbf{x}^{\top} \mathbf{w} \right) \ge 0} d\mu_d \left(\mathbf{w}\right) \right)
		\\& = \mathbf{v}^{\top} \left( \frac{1}{(\sqrt{2\pi})^{d}} \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \int_{r=0}^{\infty}  r \overline{\mathbf{w}} \mathbb{I}_{\left(r \mathbf{x}^{\top} \overline{\mathbf{w}} \right) \ge 0} r^{d-1}e^{-r^2/2} dr d\overline{\mathbf{w}} \right)
		\\& = \mathbf{v}^{\top} \left( \frac{1}{(\sqrt{2\pi})^{d}} \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \int_{r=0}^{\infty}  r \overline{\mathbf{w}} \mathbb{I}_{\left(r \mathbf{x}^{\top} \overline{\mathbf{w}} \right) \ge 0} r^{d-1}e^{-r^2/2} dr d\overline{\mathbf{w}} \right)
		\\& = \left(\frac{1}{(\sqrt{2\pi})^{d}} \int_{r=0}^{\infty} r^d e^{-r^2/2} dr \right)  \mathbf{v}^{\top} \left(   \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{\mathbf{w}} \mathbb{I}_{\left(\mathbf{x}^{\top} \overline{\mathbf{w}} \right) \ge 0}  d\overline{\mathbf{w}} \right)
		\\&=  \left(\frac{1}{\sqrt{2}} \frac{\Gamma\left((d+1)/2\right)}{\Gamma\left(d/2\right)} \right) \frac{1}{\omega_d} \mathbf{v}^{\top} \left(   \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{\mathbf{w}} \mathbb{I}_{\left(\mathbf{x}^{\top} \overline{\mathbf{w}} \right) \ge 0}  d\overline{\mathbf{w}} \right)
		\\& =  \left(\frac{1}{\sqrt{2}} \frac{\Gamma\left((d+1)/2\right)}{\Gamma\left(d/2\right)} \right) \frac{V_{d-1}}{\omega_d} \mathbf{v}^{\top} \mathbf{x}  \quad \text{(from \autoref{thm:coordinate})},  
	\end{align*}
	where we re-parametrize $\mathbf{w}$ as $r \overline{\mathbf{w}}$ for some $r \ge 0, \overline{\mathbf{w}} \in \mathbb{S}^{d-1}$. Thus, the desired function is
	
	\begin{equation*}
		p\left(\mathbf{w}\right) =  \frac{\mathbf{w}^{\top} \mathbf{v}}{\sqrt{2\pi}} \frac{ \Gamma\left(d/2\right)}{\Gamma\left((d+1)/2\right)}\frac{\Gamma\left((d+1)/2\right)}{ \Gamma\left(d/2\right)} = \frac{\mathbf{w}^{\top} \mathbf{v}}{\sqrt{2\pi}}. 
	\end{equation*}
	
	Now, let's consider $\sigma$ as the ReLU activation.
	For $p(\mathbf{w}) = \mathbf{v}^{\top} \mathbf{w}$, we get
	\fi     
	In the following, we re-parametrize $\mathbf{w}$ as $r \overline{\mathbf{w}}$ for some $r \ge 0, \overline{\mathbf{w}} \in \mathbb{S}^{d-1}$. 
	\begingroup \allowdisplaybreaks
	\begin{align}
		\frac{1}{2}
		\int_{\Reals^d} p(\bw) \,\sigma(\mathbf{w}^{\top} \bx ) \deriv \mu_d (\mathbf{w}) 
		&= \int_{\mathbf{w} \in \mathbb{R}^{d}}  \mathbf{v}^{\top} \mathbf{w}  \left(\mathbf{w}^{\top} \mathbf{x} \right) 
		\mathbb{I}_{\left(\mathbf{w}^{\top} \mathbf{x} \right) \ge 0} \deriv \mu \left(\mathbf{w}\right) \nonumber
		\\&=  \mathbf{v}^{\top} \left(\int_{\mathbf{w} \in \mathbb{R}^{d}} \mathbf{w} \left(\mathbf{w}^{\top} \mathbf{x} \right) \mathbb{I}_{\left(\mathbf{w}^{\top} \mathbf{x} \right) \ge 0} \deriv\mu \left(\mathbf{w}\right) \right) \nonumber
		\\& = \mathbf{v}^{\top} \left( \frac{1}{(\sqrt{2\pi})^{d}} \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \int_{r=0}^{\infty}  r \overline{\mathbf{w}} \left(r  \overline{\mathbf{w}}^{\top} \mathbf{x} \right) \mathbb{I}_{\left(r \overline{\mathbf{w}}^{\top} \mathbf{x} \right) \ge 0} r^{d-1}e^{-r^2/2} \deriv r \deriv \overline{\mathbf{w}} \right) \nonumber
		\\& = \left(\frac{1}{(\sqrt{2\pi})^{d}} \int_{r=0}^{\infty} r^{d+1} e^{-r^2/2} \deriv r \right)  \mathbf{v}^{\top} 
		\left(   \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}}  \overline{\mathbf{w}} \left(\overline{\mathbf{w}}^{\top} \mathbf{x} \right)  \mathbb{I}_{\left(\overline{\mathbf{w}}^{\top} \mathbf{x} \right) \ge 0}  \deriv \overline{\mathbf{w}} \right) \nonumber
		\\& = \left(\frac{1}{(\sqrt{2\pi})^{d}} 2^{d/2}\, \Gamma(d/2 +1) \right)  \mathbf{v}^{\top} 
		\left(   \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}}  \overline{\mathbf{w}} \left(\overline{\mathbf{w}}^{\top} \mathbf{x} \right)  \mathbb{I}_{\left(\overline{\mathbf{w}}^{\top} \mathbf{x} \right) \ge 0}  \deriv \overline{\mathbf{w}} \right) \nonumber
		\\&=  \frac{d}{\abs[0]{\Sphere^{d-1}}} \mathbf{v}^{\top} \left(   \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{\mathbf{w}} \overline{\mathbf{w}}^{\top} \mathbb{I}_{\left(\overline{\mathbf{w}}^{\top} \mathbf{x} \right) \ge 0}  \deriv \overline{\mathbf{w}} \right) \mathbf{x} \nonumber
		\\& =  \frac{d}{\abs[0]{\Sphere^{d-1}}}  \mathbf{v}^{\top} \mathbf{C}_\bx \mathbf{x},   \label{eqn:pw_computation}
		%           \\& =  \frac{d}{2} \frac{\omega_{d-1}}{2\omega_{d}} \frac{\Gamma\left(3/2\right) \Gamma\left(d/2\right)}{\Gamma\left((d+3)/2\right)} \mathbf{v}^{\top} \mathbf{x}  \quad \text{ (from \autoref{thm:coordinate})}.  \nonumber
	\end{align}
	\endgroup
	where $\mathbf{C}_\bx :=  \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{\mathbf{w}} \overline{\mathbf{w}}^{\top} \mathbb{I}_{\left(\overline{\mathbf{w}}^{\top} \mathbf{x} \right) \ge 0}  \deriv \overline{\mathbf{w}}$.
	Let the orthogonal matrix $\mathbf{U}_\bx$ be such that $\mathbf{U}_\bx^\top \mathbf{x} = \mathbf{e}_1$ (the choice is not unique; we choose one arbitrarily).
	Then 
	\begin{align}
		\mathbf{C}_\bx 
		&=  \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{\mathbf{w}} \overline{\mathbf{w}}^{\top} \mathbb{I}_{\left(\overline{\mathbf{w}}^{\top} \mathbf{x} \right) \ge 0}  \deriv \overline{\mathbf{w}} \nonumber
		\\&= \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \mathbf{U}_\bx \overline{\mathbf{w}} (\mathbf{U}_\bx\overline{\mathbf{w}})^\top \mathbb{I}_{(\mathbf{U}_\bx \overline{\mathbf{w}})^{\top} \mathbf{x}  \ge 0} \deriv \overline{\mathbf{w}} \nonumber
		\\&= \mathbf{U}_\bx \left( \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{\mathbf{w}} \overline{\mathbf{w}}^\top \mathbb{I}_{\overline{\mathbf{w}}^{\top} (\mathbf{U}_\bx^\top \mathbf{x}) \ge 0}  \deriv \overline{\mathbf{w}} \right) \mathbf{U}_\bx^\top \nonumber
		\\&= \mathbf{U}_\bx \left( \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{\mathbf{w}} \overline{\mathbf{w}}^\top 
		\mathbb{I}_{\overline{\mathbf{w}}_1 \ge 0}  \deriv \overline{\mathbf{w}} \right) \mathbf{U}_\bx^\top \nonumber
		\\&= \mathbf{U}_\bx \mathbf{C}_{\mathbf{e}_1} \mathbf{U}_\bx^\top.  \label{eqn:Cx}
	\end{align}
	Using the symmetry of $\Sphere^{d-1}$ we claim
	\begin{claim}\label{claim:Kd}
		We have $\mathbf{C}_{\mathbf{e}_1} = K_d \mathbf{I}$, for a constant $K_d$ (evaluated below). 
	\end{claim}
	\begin{proof}
		Let's first restate the claim:      
		\begin{align*}
			[\mathbf{C}_{\mathbf{e}_1}]_{i,j} = \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_i \overline{w}_j
			\,\mathbb{I}_{\overline{w}_1 \ge 0}  \deriv \overline{\mathbf{w}} 
			= \begin{cases} 
				0, & \text{if } i \neq j, \\
				K_d, & \text{otherwise.}
			\end{cases}
		\end{align*}
		To prove this, note that 
		\begin{align*}
			\int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_1^2 \,\mathbb{I}_{\overline{w}_1 \ge 0}  \deriv \overline{\mathbf{w}}
			= \frac{1}{2}\int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_1^2   \deriv \overline{\mathbf{w}},
		\end{align*}
		because $\overline{w}_1^2$ takes on the same value on $(\overline{w}_1, \overline{w}_2, \overline{w}_3, \ldots)$ and on $(-\overline{w}_1, \overline{w}_2, \overline{w}_3, \ldots)$.
		Similarly
		\begin{align*}
			\int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_2^2 \,\mathbb{I}_{\overline{w}_1 \ge 0}  \deriv \overline{\mathbf{w}}
			= \frac{1}{2}\int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_2^2   \deriv \overline{\mathbf{w}},
		\end{align*}
		because $\overline{w}_2^2$ takes on the same value on $(\overline{w}_1, \overline{w}_2, \overline{w}_3, \ldots)$ and on $(-\overline{w}_1, \overline{w}_2, \overline{w}_3, \ldots)$.
		Now clearly 
		\begin{align*}
			\int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_1^2   \deriv \overline{\mathbf{w}}= 
			\int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_2^2   \deriv \overline{\mathbf{w}}.
		\end{align*}
		Thus we have shown that $\int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_i^2 \,\mathbb{I}_{\overline{w}_1 \ge 0}  \deriv \overline{\mathbf{w}}$ does not depend on $i$. 
		Now notice that  
		\begin{align*}
			\int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_i \overline{w}_j \,\mathbb{I}_{\overline{w}_1 \ge 0}  \deriv \overline{\mathbf{w}} = 0
		\end{align*}
		because for each point $(\overline{w}_1, \overline{w}_2, \overline{w}_3, \ldots)$ there's a corresponding point $(\overline{w}_1, -\overline{w}_2, \overline{w}_3, \ldots)$ 
		with the integrands taking on opposite values (or both are $0$). A similar argument shows more generally that for all $i \neq j$ we have $\int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_i \overline{w}_j
		\,\mathbb{I}_{\overline{w}_1 \ge 0}  \deriv \overline{\mathbf{w}} = 0$. 
	\end{proof}
	
	Now we evaluate $K_d$. Using Claim~\ref{claim:Kd}, we can write
	\begin{align*}
		K_d = \frac{1}{d} \tr \mathbf{C}_{\mathbf{e}_1}
		= \frac{1}{d} \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} (\tr \overline{\mathbf{w}} \overline{\mathbf{w}}^{\top})\, \mathbb{I}_{\overline{w}_1 \ge 0}  \deriv \overline{\mathbf{w}} 
		= \frac{1}{d} \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \mathbb{I}_{\overline{w}_1 \ge 0}  \deriv \overline{\mathbf{w}} 
		= \frac{\abs[0]{\Sphere^{d-1}}}{2d}. 
	\end{align*}
	%      \todo{Need to remove the following set of equations. Also need to add some more explanations about the use of symmetry and possibly other things.}
	%      \begin{align*}
	%      K_d &= \int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_1^2 \,\mathbb{I}_{\overline{w}_1 \ge 0} \deriv \overline{\mathbf{w}} 
	%      \\&= \frac{1}{2}\int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}} \overline{w}_1^2  \deriv \overline{\mathbf{w}} 
	%      \\&= \frac{1}{2}\int_{-1}^{1} w_1^2 \left(\int_{\overline{\mathbf{w}} \in \mathbb{S}^{d-1}, \overline{w}_1 = w_1} \,\deriv \overline{\mathbf{w}}\right) \deriv w_1 
	%      \\&= \frac{1}{2}\int_{-1}^{1} w_1^2 \, \omega_{d-2}\, (1-w_1^2)^{\frac{d-3}{2}} \deriv w_1
	%      \\&= \frac{\omega_{d-2}}{2} \int_{0}^{1} t^{1/2}(1-t)^{\frac{d-3}{2}} \deriv t
	%      \\&=  \frac{\omega_{d-2}}{2} \frac{\Gamma({\frac{3}{2}})\Gamma({\frac{d-1}{2}})}{\Gamma({\frac{d+2}{2}})}. 
	%      \end{align*} 
	Using the orthogonality of $\mathbf{U}_\bx$ and Claim~\ref{claim:Kd} in \eqref{eqn:Cx} it follows that 
	\begin{align*}
		\mathbf{C}_\bx = \mathbf{U}_\bx \mathbf{C}_{\mathbf{e}_1} \mathbf{U}_\bx^\top = K_d \mathbf{U}_\bx \mathbf{I} \mathbf{U}_\bx^\top = K_d \mathbf{I}. 
	\end{align*}
	Continuing from where we left off in \eqref{eqn:pw_computation} we have
	\begin{align*}
		\frac{1}{2} \int_{\Reals^d} p(\bw) \,\sigma(\mathbf{w}^{\top} \bx ) \deriv \mu_d (\mathbf{w}) 
		= \frac{d}{\abs{\Sphere^{d-1}}}  \mathbf{v}^{\top} \mathbf{C}_\bx \mathbf{x} 
		=  \frac{d K_d}{\abs{\Sphere^{d-1}}} \mathbf{v}^{\top} \mathbf{x} 
		= \frac{1}{2} \mathbf{v}^{\top} \mathbf{x}.
	\end{align*}
	Thus, 
	\begin{equation*}
		p\left(\mathbf{w}\right) = 2 \,\mathbf{w}^{\top} \mathbf{v}. 
	\end{equation*}
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
Lemma~\ref{lem:normal_linearestimate} can be extended to gaussian distribution over $\mathbf{w}$ with variance $\beta \mathbf{I}$, for any $\beta > 0$.
\begin{corollary}\label{cor:linear_estimate}
	For any $\bv \in \Reals^d$, the linear function taking $\bx$ to $\bv^\top \bx$ for $\bx \in \Reals^d$, can be represented as
	\begin{equation}\label{eqn:single_layer_inversion_exact_cor}
		\frac{\beta^2}{2} \, \bv^\top \bx = \int_{\Reals^d} p(\bw) \, \sigma(\mathbf{w}^{\top} \bx ) \,\deriv \mu_d^{\beta} (\mathbf{w}),
	\end{equation}
	with 
	$$p\left(\mathbf{w}\right) \;=\;   \mathbf{w}^{\top} \mathbf{v}.$$ 
\end{corollary}


Lemma~\ref{lem:normal_linearestimate} can be discretized so that instead of the integral in \eqref{eqn:single_layer_inversion_exact}, we use an empirical average.
This comes at the expense of making the resulting version of \eqref{eqn:single_layer_inversion_exact} approximate. Furthermore, we can generalize the lemma so that instead of taking us from $\mathbf{h}^{(1)} = \sigma(\mathbf{W}\mathbf{x}^{(1)})$ to $\bv^\top \bx^{(1)}$ it takes us from $\mathbf{h}^{(\ell)}$ to  $\bv^\top [\mathbf{h}^{(\ell-1)}, \bx^{(\ell)}]$ for every $\ell \in [L]$. The following lemma does both of these.

%For any $\mathbf{v} \in \mathbb{R}^{m_{\ell}},$ let $g: \mathbb{R}^{m_{\ell}} \rightarrow \mathbb{R}$ be the linear function $g(\mathbf{h}):=\mathbf{v}^{\top} \mathbf{h} .$ Define





\subsection{Invertibility at a single step of RNN}
\begin{lemma}\label{lemma:singlecell_ESN}
	%From lemma~\ref{cor:normal_linearestimate}, we have that for a fresh
	We have an RNN at random initialization as defined in Def.~\ref{def:RNN}. Fix any $\ell \in\{0,1, \ldots, L-$ 1\} and $\zeta \in(0,1)$. Let $g: \mathbb{R}^{m+d } \times \mathbb{R}^{m+d}  \rightarrow \mathbb{R}$ be given by $g(\mathbf{v}, [\mathbf{h}, \mathbf{x}]) = \mathbf{v}^{\top} [\mathbf{h}, \mathbf{x}]$. Consider a vector $\mathbf{v} \in \mathbb{R}^{m+d}$ which is stable against re-randomization, as specified later in Assumption~\ref{ass:variant_v} with constants $(\kappa, \zeta)$.
	%There exists $\mathbf{u} \in \mathbb{R}^{m}$ s.t.
	Let $f\left(\mathbf{v}, \mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}\right) = \sum_{i=1}^{m} u_i \sigma\left(\mathbf{w}_i^{\top} \mathbf{h}^{(\ell-1)} +  \mathbf{a}_i^{\top} \mathbf{x}^{(\ell)} \right),$ where 
	$$ u_i = [\mathbf{w}_i, \mathbf{a}_i]^{\top} \mathbf{v}.$$
	
	Then for a given normalized sequence $\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(L)}$, with $\mathbf{x}^{(\ell)} \in \mathbb{R}^{d}$ for each $\ell \in [L]$, and for any constant $\rho > 0$, we have 
	\begin{equation*} \abs[0]{g({\mathbf{v}}, [\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}]) - f(\mathbf{v}, \mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)})} \le \mathcal{O}\left(\rho^{5 + \kappa} m^{-1/12} + \rho^{1+\kappa} m^{-\zeta/2} + \rho^{1+\kappa} m^{-1/4} + \rho^{5+\kappa} m^{-1/4}  \right) \cdot \norm{\mathbf{v}},
	\end{equation*}
	with probability at least $1 -  e^{-\Omega(\rho^2)}$.% w.r.t. $\mathbf{W}$ and $\mathbf{A}$.
	
	%satisfies for any constant $\rho>0$, with probability at least $1 - e^{-\Omega\left(\rho^{2}\right)}$,
	%\begin{equation*}
	%     \sup_{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(\ell)} \in \mathbb{S}^{d-1}} \abs{g\left([\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}]\right) - f\left(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}\right)} \le \mathcal{O}(\epsilon),
	%\end{equation*}
	%provided $m \ge \Omega\left((L^2 \rho^2 d \ln (1/\epsilon))^{23} L^{4/3} \sqrt{\frac{l + 1 + \norm{\mathbf{v}}^2}{\norm{\mathbf{v}}^2}}\right)$ and $\epsilon \ge \sqrt{l + 1 + \norm{\mathbf{v}}^2} \rho^{19/7} m^{-1/14}$. $c$ is equal to $\frac{1}{d}\frac{\Gamma\left((d-1)/2\right)\Gamma\left((d+3)/2\right)}{\Gamma\left(d/2\right)\Gamma\left(d/2\right)}$, which is a $\mathcal{O}\left(1\right)$ constant.
	%$$p\left(\mathbf{w}\right) = \frac{\mathbf{w}^{\top} \mathbf{v}}{\sqrt{2\pi}} = \mathcal{O}\left(1 \right) \mathbf{w}^{\top} \mathbf{v},    $$ 
	%if $\sigma$ is threshold activation.
\end{lemma}

\begin{proof}
	%Note that, when  $\sigma$ is $\relu$ activation, for a given $l$-length sequence  $\mathbf{x}^{(1)}, ..., \mathbf{x}^{(\ell)} \in \mathbb{S}^{d-1}$
	%\begin{align*}
	%    &f\left(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}\right)  - \mathbb{E} f\left(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}\right)  \\&= c \mathbf{v}^{\top} m \left(\frac{1}{m}\sum_{i=1}^{m} [\mathbf{w}_i, \mathbf{a}_i] [\mathbf{w}_i, \mathbf{a}_i]^{\top} \mathbb{I}_{\mathbf{w}_i^{\top} \mathbf{h}^{(\ell-1)} + \mathbf{a}_i^{\top} \mathbf{x}^{(\ell)} \ge 0} - \mathbb{E}_{\mathbf{w} \sim \mathcal{N}\left(0, \frac{2}{m}\mathbf{I}_{m+d}\right)}  \mathbf{w}\mathbf{w}^{\top} \mathbb{I}_{\mathbf{w}^{\top} [\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}] \ge 0}\right) [\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}].  
	%\end{align*}
	The major issue in using a discrete version of lemma~\ref{lem:normal_linearestimate} directly for input $[\mathbf{h}^{(\ell-1)}, \mathbf{x}]$ is that there is a coupling between the randomness of the weights $\mathbf{W}, \mathbf{A}$ and the hidden vector $\mathbf{h}^{(\ell-1)}$. This coupling can be understood as the dependence of $\mathbf{h}^{(\ell-1)}$ on the choice of weight vectors in $\mathbf{W}$ and $\mathbf{A}$. There may also be a coupling between  the randomness of $\mathbf{v}$ and  the weights $\mathbf{W}, \mathbf{A}$, for which we take some assumption later. To decouple this randomness, we use the fact that ESNs are stable to re-randomization of few rows of the weight matrices and follow the proof technique of Lemma G.3 \cite{allen2019can}. 
	
	
	
	Choose a random subset $\mathcal{K} \subset[m]$ of size $|\mathcal{K}|=N$. Define the function $f_{\mathcal{K}}$ as
	\begin{equation*}
		f_{\mathcal{K}}(\mathbf{v}, \mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}) = \sum_{k \in \mathcal{K}} u_{k} \sigma(\mathbf{w}_k^{\top} \mathbf{h}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)}).  
	\end{equation*}
	Replace the rows $\left\{\mathbf{w}_{k}, \mathbf{a}_{k}\right\}_{k \in \mathcal{K}}$ of $\mathbf{W}$ and $\mathbf{A}$ with freshly new i.i.d. samples $\widetilde{\mathbf{w}}_{k}, \widetilde{\mathbf{a}}_{k} \sim \mathcal{N}\left(0, \frac{2}{m} \mathbf{I}\right).$ to form new matrices $\widetilde{\mathbf{W}}$ and $\widetilde{\mathbf{A}}$. For the given sequence, we follow the notation of Lemma~\ref{lemma:rerandESN} to denote the hidden states corresponding to the old and the new weight matrices. We will assume one property for $\mathbf{v}$. Let say $\mathbf{v}$ depends on the matrices $\mathbf{W}$ and $\mathbf{A}$ and becomes $\widetilde{\mathbf{v}}$, with the new matrices $\widetilde{\mathbf{W}}$ and $\widetilde{\mathbf{A}}$. Then, we assume that the norm difference of $\mathbf{v}$ and $\widetilde{\mathbf{v}}$ is small with high probability.
	\begin{assumption}\label{ass:variant_v}
		With probability at least $1-e^{-\Omega(\rho^2)}$, \todo{clarify which random variables are used for this probability} there exists constants $\kappa \ge 0$ and $\zeta < 1$ such that
		\begin{align*}
			&\norm[2]{\mathbf{v} - \widetilde{\mathbf{v}}} \le \mathcal{O}(\rho^{\kappa} (N/m)^{\zeta} \norm{\mathbf{v}}) \\&
			\norm{[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r \left(\mathbf{v} - \widetilde{\mathbf{v}}\right)} \le \mathcal{O}(\rho^{\kappa} (N/m)^{0.5 + \zeta} \norm{\mathbf{v}}), \quad \forall k \in [m].
		\end{align*}
	\end{assumption}
	We will show later that the vector $\mathbf{v}$ that we need for inversion satisfies the above assumption with constants $(\kappa, \zeta) = (6, 1/6)$. Also, if $\mathbf{v}$ is independent of  $\mathbf{W}$ and $\mathbf{A}$, then the constants needed in the assumption are $(\kappa, \zeta) = (0, 0)$.
	
	
	The following claim shows that under the assumption~\ref{ass:variant_v}, function $f_{\mathcal{K}}$ and $\frac{N}{m}g$ are close to each other with high probability.
	
	\begin{claim}\label{claim:singlesubset}
		For the given sequence $\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(L)}$,
		\begin{align*}
			&\abs{f_{\mathcal{K}}(\mathbf{v}, \mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - \frac{N}{m}g(\mathbf{v}, [\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}])} \\&\le  \mathcal{O}\left(\rho^{5 + \kappa} N^{7/6} m^{-7/6} + \rho^{1 + \kappa} (N/m)^{1+\zeta} + \rho^{1+\kappa} N^{1/2} m^{-1}  + \rho^{5 + \kappa} (N/m)^{3/2}\right) \cdot \norm{\mathbf{v}},
		\end{align*}
		with probability exceeding $1 - e^{-\Omega(\rho^2)}$.
	\end{claim}
	The above claim has been restated and proven in claim~\ref{claim:singlesubset_proof}.
	
	To complete the proof, we divide the set of neurons into $m/N$ disjoint sets $\mathcal{K}_1, \cdots, \mathcal{K}_{m/N}$, each set is of size $N$. We apply the Claim~\ref{claim:singlesubset} to each subset $\mathcal{K}_i$ and then add up the errors from each subset. That is, with probability at least $1 - \frac{m}{N}e^{-\Omega(\rho^2)}$,
	\begin{align*}
		f(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}) &=  \sum_{i=1}^{m/N}  f_{\mathcal{K}_i}(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)})\\
		&= \sum_{i=1}^{m/N} \frac{N}{m} g([\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}]) + error_{\mathcal{K}_i}\\
		&= g([\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}]) + \sum_{i=1}^{m/N} error_{\mathcal{K}_i}, 
	\end{align*}
	where by Claim~\ref{claim:singlesubset},
	\begin{align*}
		\abs{error_{\mathcal{K}_i}} \le \mathcal{O}\left(\rho^{5 + \kappa} N^{7/6} m^{-7/6} + \rho^{1 + \kappa} (N/m)^{1+\zeta} + \rho^{1+\kappa} N^{1/2} m^{-1}  + \rho^{5 + \kappa} (N/m)^{3/2}\right) \cdot \norm{\mathbf{v}}.
	\end{align*}
	
	Thus,
	\begin{equation*}
		\abs{ f(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - g([\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}])} \le \mathcal{O}\left(\rho^{6 + \kappa} N^{1/6} m^{-1/6} + \rho^{2 + \kappa} (N/m)^{\zeta} + \rho^{1+\kappa} N^{-1/2} + \rho^{5 + \kappa} (N/m)^{1/2}\right) \cdot \norm{\mathbf{v}},
	\end{equation*}
	with probability at least $1 - \frac{m}{N}e^{-\Omega(\rho^2)}$.
	
	Choosing $N = m^{1/2}$, we have
	\begin{equation*}
		\abs{ f(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - g([\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}])} \le \mathcal{O}\left(\rho^{5 + \kappa} m^{-1/12} + \rho^{1+\kappa} m^{-\zeta/2} + \rho^{1+\kappa} m^{-1/4} + \rho^{5+\kappa} m^{-1/4}  \right) \cdot \norm{\mathbf{v}},
	\end{equation*}
	with probability at least $1 -  \sqrt{m} e^{-\Omega(\rho^2)} \ge 1 - e^{-\Omega(\rho^2)}$.
	For Lemma~\ref{lemma:rerandESN} to hold true, we need $N \le \frac{m}{\rho^{23}}$. Hence, we require $\sqrt{m} \le \frac{m}{\rho^{23}}$, which translates to $m \ge \rho^{46}$.
	
	
	%However, Claim~\ref{claim:singlesubset} requires $N \le \mathcal{O}(m L^{-1/2} \rho^{-23})$. Hence, the required number of neurons is
	%\begin{equation*}
	%    m \ge \Omega(L^{2/3} \rho^{20}).
	%\end{equation*}
	
	
	%where we use $N = \mathcal{O}\left(m^{1/7} \rho^{-24/7} \left(\frac{l + 1 + \norm{\mathbf{v}}^2}{\norm{\mathbf{v}}^2}\right)^{2/7}\right)$ in the final step. Here, $\epsilon$ is a constant that satisfies $\epsilon \ge  \Omega(\sqrt{l + 1 + \norm{\mathbf{v}}^2} \rho^{19/7} m^{-1/14})$. Since $N \le \mathcal{O}\left(m L^{-1} \rho^{-23}\right)$ to satisfy the conditions in \autoref{lemma:rerandESN}, we must have
	%\begin{equation*}
	%    m \ge \Omega\left(\rho^{23} L^{4/3} \sqrt{\frac{l + 1 + \norm{\mathbf{v}}^2}{\norm{\mathbf{v}}^2}}\right). 
	%\end{equation*}
	
	%Now, let's unfix $\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, ..., \mathbf{x}^{(L)}$. We take an $\delta$-net $S_{\delta}$, with $\delta = 2^{-1.5L}\epsilon$,  across $\left(\mathbb{S}^{(d-1)}\right)^L$ to get that for all sequences that belong to $S_{\delta}$, the bounds hold true with probability at least $1 - \delta^{-Ld} e^{-\Omega\left(\rho^2\right)}.$
	
	%We use \autoref{lemma:perturbation_ESN} in each $\delta$-cell, to get for all sequences $\mathbf{x}^{(1)}, ..., \mathbf{x}^{(L)} \in \left(\mathbb{S}^{d-1}\right)^{L}$,
	
	\iffalse
	\begin{align*}
		\abs{f\left(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}\right) - g\left([\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}]\right)} &\le 
		\inf_{\overline{\mathbf{x}}^{(1)}, ..., \overline{\mathbf{x}}^{(L)} \in \mathbf{S}_{\delta}} 
		\abs{f\left(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}\right) - f\left(\overline{\mathbf{h}}^{(\ell)}, \overline{\mathbf{x}}^{(\ell)}\right)}
		\\&\quad \quad \quad+ \abs{f\left(\overline{\mathbf{h}}^{(\ell)}, \overline{\mathbf{x}}^{(\ell)}\right) - g\left([\overline{\mathbf{h}}^{(\ell)}, \overline{\mathbf{x}}^{(\ell)}]\right)}
		+ \abs{g\left([\overline{\mathbf{h}}^{(\ell)}, \overline{\mathbf{x}}^{(\ell)}]\right) - g\left([\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}]\right)}
		\\&\le \inf_{\overline{\mathbf{x}}^{(1)}, ..., \overline{\mathbf{x}}^{(\ell)} \in \mathbf{S}_{\delta}}  \abs{ \mathbf{u}^{\top} (\mathbf{h}^{(\ell-1)} - \overline{\mathbf{h}}^{(\ell)}) } + \mathcal{O}(\epsilon) + \abs{\mathbf{v}^T ([\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}] - [\overline{\mathbf{h}}^{(\ell)}, \overline{\mathbf{x}}^{(\ell)}])} 
		\\&\le \inf_{\overline{\mathbf{x}}^{(1)}, ..., \overline{\mathbf{x}}^{(L)} \in \mathbf{S}_{\delta}} \norm{\mathbf{u}} \norm{\mathbf{h}^{(\ell-1)} - \overline{\mathbf{h}^{(\ell-1)}}}  + \mathcal{O}(\epsilon) + \norm{\mathbf{v}} \norm{[\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}] - [\overline{\mathbf{h}}^{(\ell)}, \overline{\mathbf{x}}^{(\ell)}]} 
		\\& \le \mathcal{O}\left(2^{1.5l} \delta \left(1 + \frac{\rho L}{\sqrt{m}}\right)\right) \norm{\mathbf{v}} + \mathcal{O}(\epsilon) +  \mathcal{O}\left(2^{1.5l} \delta \left(1 + \frac{\rho L}{\sqrt{m}}\right)\right) \norm{\mathbf{v}}
		%\\&\le  \inf_{\overline{\mathbf{x}} \in \mathbf{S}_{\epsilon}}  \norm{\mathbf{u}}  \left(\mathbf{x} - \overline{\mathbf{x}}^{(\ell)}\right)} + \zeta + \norm{\mathbf{v}} \norm{\mathbf{x}^{(\ell)} - \overline{\mathbf{x}}^{(\ell)}}
		%\\&\le \inf_{\overline{\mathbf{x}} \in \mathbf{S}_{\epsilon}}  \norm{\mathbf{u}} \sqrt{\sum_{i=1}^{m_l} \left(\sigma\left(\mathbf{w}_i^{\top} \mathbf{x}^{(\ell)}\right) - \sigma\left(\mathbf{w}_i^{\top} \overline{\mathbf{x}}^{(\ell)}\right)\right)^2}  + \zeta + \norm{\mathbf{v}} \norm{\mathbf{x}^{(\ell)} - \overline{\mathbf{x}}^{(\ell)}}
		%\\&\le \inf_{\overline{\mathbf{x}} \in \mathbf{S}_{\epsilon}}  \norm{\mathbf{u}} \sqrt{\sum_{i=1}^{m_l} \left(\left(\mathbf{w}_i^{\top} \mathbf{x}^{(\ell)}\right) - \left(\mathbf{w}_i^{\top} \overline{\mathbf{x}}^{(\ell)}\right)\right)^2}  + \zeta + \mathcal{O}\left(\epsilon\right)
		%\\&= \inf_{\overline{\mathbf{x}} \in \mathbf{S}_{\epsilon}}  \norm{\mathbf{u}} \sqrt{\sum_{i=1}^{m_l} \left(\mathbf{w}_i^{\top} \left(\mathbf{x}^{(\ell)} -  \overline{\mathbf{x}}^{(\ell)}\right)\right)^2}  + \zeta + \mathcal{O}\left(\epsilon\right)
		%\\&\le \mathcal{O}\left(\frac{1}{\sqrt{m_l}}\right) \mathcal{O}\left(\sqrt{m_l} \epsilon\right)  + \zeta + \mathcal{O}\left(\epsilon\right)
		%\\&= \mathcal{O}\left(\zeta + \epsilon\right),
	\end{align*}
	with probability at least $1 - 2\delta^{-Ld} e^{-\Omega(\rho^2)}$ for any $\rho, \epsilon > 0$.  Setting $\delta = 2^{-1.5L}\epsilon$, we get
	\begin{equation*}
		\sup_{\mathbf{x}^{(1)}, ..., \mathbf{x}^{(\ell)} \in \mathbb{S}^{d-1}} \abs{f\left(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}\right) - g\left([\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}]\right)}  \le \mathcal{O}\left(\epsilon\right),
	\end{equation*}
	with probability at least $1 -  e^{-\Omega\left(\rho^2\right)}$ for any $\rho > 0$, provided $m \ge \Omega\left((L^2 \rho^2 d \ln (1/\epsilon))^{23} L^{4/3} \sqrt{\frac{l + 1 + \norm{\mathbf{v}}^2}{\norm{\mathbf{v}}^2}}\right)$, where $\epsilon \ge \sqrt{l + 1 + \norm{\mathbf{v}}^2} \rho^{19/7} m^{-1/14}$. In the above inequality, we use the form of $\mathbf{u}$, using the inequality of chi-square distribution, to get that
	\begin{equation*}
		\norm{\mathbf{u}} \le \mathcal{O}\left(\norm{\mathbf{v}}\right).
	\end{equation*}
	\fi
\end{proof}

\iffalse
\begin{corollary}\label{cor:singlecell_ESN}
	%From lemma~\ref{cor:normal_linearestimate}, we have that for a fresh
	We have an recurrent neural network as defined in 3.1, as defined in \autoref{def:ESN}. Fix any $\ell \in\{0,1, \ldots, L-$ 1\} and $\zeta \in(0,1)$. For any $\mathbf{v} \in \mathbb{R}^{m + d},$ let $g: \mathbb{R}^{m + d} \rightarrow \mathbb{R}$ is such $g(\mathbf{h}) = \mathbf{v}^{\top} \mathbf{h}$ for a certain $\mathbf{v} \in \mathbb{R}^{m + d}$, $\sigma$ is the $\relu$ activation function.
	%There exists $\mathbf{u} \in \mathbb{R}^{m}$ s.t.
	Define $f\left(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}\right) = \sum_{i=1}^{m} u_i \sigma\left(\mathbf{w}_i^{\top} \mathbf{h}^{(\ell-1)} +  \mathbf{a}_i^{\top} \mathbf{x}^{(\ell)} \right),$ where 
	$$ u_i = [\mathbf{w}_i, \mathbf{a}_i]^{\top} \mathbf{v}.$$
	
	Then for a given sequence $\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(L)}$, with $\mathbf{x}^{(\ell)} \in \mathbb{S}^{d-1}$ for each $\ell \in [L]$, and for any constant $\rho > 0$, we have 
	\begin{equation*} \abs{g\left([\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}]\right) - f\left(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}\right)} \le \mathcal{O}\left( \rho^5 m^{-1/8} \right) \cdot \norm{\mathbf{v}},
	\end{equation*}
	with probability at least $1 - m^{3/4} \rho^6 e^{-\Omega(\rho^2)}$, provided $m \ge \Omega(\rho^{22})$.
	
	%satisfies for any constant $\rho>0$, with probability at least $1 - e^{-\Omega\left(\rho^{2}\right)}$,
	%\begin{equation*}
	%     \sup_{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(\ell)} \in \mathbb{S}^{d-1}} \abs{g\left([\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}]\right) - f\left(\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}\right)} \le \mathcal{O}(\epsilon),
	%\end{equation*}
	%provided $m \ge \Omega\left((L^2 \rho^2 d \ln (1/\epsilon))^{23} L^{4/3} \sqrt{\frac{l + 1 + \norm{\mathbf{v}}^2}{\norm{\mathbf{v}}^2}}\right)$ and $\epsilon \ge \sqrt{l + 1 + \norm{\mathbf{v}}^2} \rho^{19/7} m^{-1/14}$. $c$ is equal to $\frac{1}{d}\frac{\Gamma\left((d-1)/2\right)\Gamma\left((d+3)/2\right)}{\Gamma\left(d/2\right)\Gamma\left(d/2\right)}$, which is a $\mathcal{O}\left(1\right)$ constant.
	%$$p\left(\mathbf{w}\right) = \frac{\mathbf{w}^{\top} \mathbf{v}}{\sqrt{2\pi}} = \mathcal{O}\left(1 \right) \mathbf{w}^{\top} \mathbf{v},    $$ 
	%if $\sigma$ is threshold activation.
\end{corollary}

\begin{proof}
	The proof follows from the proof of Lemma~\ref{lemma:singlecell_ESN}, with the value of $N = m^{1/4} \rho^{-6}$. The requirement on $m$ comes from the requirement of $N \le m/\rho^{23}$ in Lemma~\ref{lemma:rerandESN}.
\end{proof}
\fi




\subsection{Proof of Claim~\ref{claim:singlesubset}}
\begin{claim}[Restating claim~\ref{claim:singlesubset}]\label{claim:singlesubset_proof}
	For the given sequence $\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(L)}$,
	\begin{align*}
		&\abs{f_{\mathcal{K}}(\mathbf{v}, \mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - \frac{N}{m}g(\mathbf{v}, [\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}])} \\&\le  \mathcal{O}\left(\rho^{5 + \kappa} N^{7/6} m^{-7/6} + \rho^{1 + \kappa} (N/m)^{1+\zeta} + \rho^{1+\kappa} N^{1/2} m^{-1}  + \rho^{5 + \kappa} (N/m)^{3/2}\right) \cdot \norm{\mathbf{v}},
	\end{align*}
	with probability exceeding $1 - e^{-\Omega(\rho^2)}$.
\end{claim}

\begin{proof}
	We will need $\left\{\mathbf{w}_{k}\right\}_{k \in \mathcal{K}}$ to satisfy several conditions. We will lower-bound the probability of each of these events and finally lower bound the probability of their intersection via the union bound. For the sake of clarity we will explicitly label these events $E_{1}, E_{2}, E_{3}, \text{ and } E_{4}$.
	
	
	
	The following claim shows that since $\mathbf{h}^{(\ell - 1)}$ doesn't change much with re-randomization (from lemma~\ref{lemma:rerandESN}), the function $f$ doesn't change much if we change the argument from $\mathbf{h}^{(\ell-1)}$ to $\widetilde{\mathbf{h}}^{(\ell - 1)}$. 
	%Proof. In the proof we will need $\mathbf{W}$ to satisfy several conditions. We will lower-bound the probability of each of these events and finally lower bound the probability of their intersection via the union bound. For the sake of clarity we will explicitly label these events $E_{1}, E_{2}, E_{3}$ and $E_{4} .$ For typographical
	
	\begin{claim}\label{claim:ff}
		\begin{equation*}
			\abs{f_{\mathcal{K}}(\mathbf{v}, \mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - f_{\mathcal{K}}(\mathbf{v}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) } \le \mathcal{O}\left(\rho^{5 + \kappa} N^{7/6} m^{-7/6} \norm{\mathbf{v}} \right),
		\end{equation*}
		with probability at least $1 - e^{-\Omega(\rho^2)}$.
	\end{claim}
	
	\begin{proof}
		
		
		We have,
		\begingroup \allowdisplaybreaks
		\begin{align*}
			\abs{f_{\mathcal{K}}(\mathbf{v}, \mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - f_{\mathcal{K}}(\mathbf{v}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) }  
			&=  \abs{\sum_{k \in \mathcal{K}} u_k \left(\sigma(\mathbf{w}_k^{\top} \mathbf{h}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)}) - \sigma(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)}) \right)} \\  
			&=  \abs{\sum_{k \in \mathcal{K}}  u_k \left(\sigma(\mathbf{w}_k^{\top} \mathbf{h}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)}) - \sigma(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)}) \right) } \\
			&\le \norm{\mathbf{u}_{\mathcal{K}}} \sqrt{\sum_{k \in \mathcal{K}} \left(\sigma(\mathbf{w}_k^{\top} \mathbf{h}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)}) - \sigma(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)}) \right)^2} \\
			&\le  \norm{\mathbf{u}_{\mathcal{K}}} \sqrt{\sum_{k \in \mathcal{K}} \left(\left(\mathbf{w}_k^{\top} \mathbf{h}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)}\right) - \left(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} - \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)}\right) \right)^2} \\
			&=  \norm{\mathbf{u}_{\mathcal{K}}} \sqrt{\sum_{k \in \mathcal{K}} (\mathbf{w}_k^{\top} (\mathbf{h}^{(\ell-1)} - \widetilde{\mathbf{h}}^{(\ell-1)}))^2}, 
			%\\&\le \rho^5 N^{5/3} m^{-7/6} \norm{u_k} \\
			%&\le 2\rho^5 N^{5/3} m^{-7/6}.
		\end{align*}
		\endgroup
		where we use cauchy schwartz inequality in the third step and $1$-lipschitzness of the activation function $\relu$ in the pre-final step.
		We will bound the two factors above separately: %The concentration inequality for chi-squared distributions (Fact~\ref{lem:chi-squared}) gives 
		
		We have,
		\begin{align}
			\norm{\mathbf{u}_{\mathcal{K}}} = \norm{[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r \mathbf{v}} = \norm{[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r \widetilde{\mathbf{v}} + [\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r \left(\mathbf{v} - \widetilde{\mathbf{v}}\right)} \le \norm{[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r \widetilde{\mathbf{v}}} + \norm{[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r. \left(\mathbf{v} - \widetilde{\mathbf{v}}\right)} .\label{eq:uK}
			%\leq \sqrt{\frac{2N}{m} + \frac{2\rho \sqrt{8N}}{m}} \norm{\mathbf{v}},
		\end{align}
		We break $\mathbf{u}_{\mathcal{K}}$ into two terms, since we need to handle the correlation between $[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r$ and $\mathbf{v}$ and we will do that using assumption~\ref{ass:variant_v}.
		
		Since, $[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r$ and $\widetilde{\mathbf{v}}$ are independent, we can use the concentration inequality for chi-squared distributions (Fact~\ref{lem:chi-squared}) to get
		\begin{align*}
			\norm{[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r \widetilde{\mathbf{v}}} \leq \sqrt{\frac{2N}{m} + \frac{2\rho \sqrt{8N}}{m}} \norm{\widetilde{\mathbf{v}}},
		\end{align*}
		with probability at least $1-2e^{-\rho^2}$. It can be further simplified into 
		\begin{equation*}
			\norm{[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r \widetilde{\mathbf{v}}} \leq \sqrt{\frac{2N}{m}} \norm{\widetilde{\mathbf{v}}} + \frac{2\rho \sqrt{2N}}{m} \norm{\widetilde{\mathbf{v}}},
		\end{equation*}
		using the fact that $\sqrt{1 + y} \le 1 + y/2$ for any variable $y > 0$. 
		Also, from assumption~\ref{ass:variant_v}, we have with probability $1-e^{-\Omega(\rho^2)}$,
		\begin{align*}
			\norm{[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r \left(\mathbf{v} - \widetilde{\mathbf{v}}\right)} \le \mathcal{O}(\rho^{\kappa} (N/m)^{0.5 + \zeta} \norm{\mathbf{v}}).
		\end{align*}
		Hence, with probability $1 - e^{-\Omega(\rho^2)}$,
		\begin{align*}
			\norm{[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r \mathbf{v}} \le \sqrt{\frac{2N}{m}} \norm{\widetilde{\mathbf{v}}} + \frac{2\rho \sqrt{2N}}{m} \norm{\widetilde{\mathbf{v}}} + \mathcal{O}(\rho^{ \kappa} (N/m)^{0.5 + \zeta} \norm{v}).
		\end{align*}
		
		%Again using assumption~\ref{ass:variant_v}, we have
		Finally going back to eq.~\ref{eq:uK}, with repeated utilization of assumption~\ref{ass:variant_v}, we have
		\begingroup\allowdisplaybreaks
		\begin{align*}
			\norm{[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r \mathbf{v}} &\le \sqrt{\frac{2N}{m}} \norm{\widetilde{\mathbf{v}}} + \frac{2\rho \sqrt{2N}}{m} \norm{\widetilde{\mathbf{v}}} + \mathcal{O}(\rho^{\kappa} (N/m)^{0.5 + \zeta} \norm{\mathbf{v}}) \\&
			\le \sqrt{\frac{2N}{m}} \norm{\mathbf{v}} + \frac{2\rho \sqrt{2N}}{m} \norm{\mathbf{v}} + \sqrt{\frac{2N}{m}} \norm{\widetilde{\mathbf{v}} - \mathbf{v}} + \frac{2\rho \sqrt{2N}}{m} \norm{\widetilde{\mathbf{v}} - \mathbf{v}} +  \mathcal{O}(\rho^{\kappa} (N/m)^{0.5 + \zeta} \norm{\mathbf{v}}) \\&
			\le \sqrt{\frac{2N}{m}} \norm{\mathbf{v}} + \frac{2\rho \sqrt{2N}}{m} \norm{\mathbf{v}} + \sqrt{\frac{2N}{m}} \norm{\widetilde{\mathbf{v}} - \mathbf{v}} + \frac{2\rho \sqrt{2N}}{m} \norm{\widetilde{\mathbf{v}} - \mathbf{v}} +  \mathcal{O}(\rho^{\kappa} (N/m)^{0.5 + \zeta} \norm{\mathbf{v}}) \\&
			\le \sqrt{\frac{2N}{m}} \norm{\mathbf{v}} + \frac{2\rho \sqrt{2N}}{m} \norm{\mathbf{v}} + \sqrt{\frac{2N}{m}} \norm{\widetilde{\mathbf{v}} - \mathbf{v}} + \frac{2\rho \sqrt{2N}}{m} \norm{\widetilde{\mathbf{v}} - \mathbf{v}} +  \mathcal{O}(\rho^{\kappa} (N/m)^{0.5 + \zeta} \norm{\mathbf{v}})
			\\&
			\le \sqrt{\frac{2N}{m}} \norm{\mathbf{v}} + \frac{2\rho \sqrt{2N}}{m} \norm{\mathbf{v}} + \sqrt{\frac{2N}{m}} \cdot \mathcal{O}(\rho^{\kappa} (N/m)^{\zeta} \norm{\mathbf{v}}) \\& \quad \quad \quad \quad + \frac{2\rho \sqrt{2N}}{m} \cdot \mathcal{O}(\rho^{\kappa} (N/m)^{\zeta} \norm{\mathbf{v}}) +  \mathcal{O}(\rho^ {\kappa} (N/m)^{0.5 + \zeta} \norm{\mathbf{v}}) \\&
			\le \mathcal{O}\left( \rho^{\kappa} \sqrt{N/m} \cdot \norm{\mathbf{v}}\right),
		\end{align*}
		\endgroup
		giving us a bound on norm of $\mathbf{u}_{\mathcal{K}}$. Let us call this event $E_1$.
		\todo{mabe give some more detail here.}
		
		Lemma~\ref{lemma:rerandESN} shows that with probability at least $1 - e^{-\Omega(\rho^2)}$,
		\begin{align}
			%\norm{\mathbf{h}^{(\ell-1)} - \widetilde{h}}
			& \norm{\mathbf{h}^{(\ell-1)} - \widetilde{\mathbf{h}}^{(\ell-1)}} \le \mathcal{O}\left(\rho^{5} N^{1/2} m^{-1/2}\right)\\&
			\norm{ \mathbf{W}_{\mathcal{K}} \left(\mathbf{h}^{(\ell-1)} - \widetilde{\mathbf{h}}^{(\ell-1)}\right) } \leq \mathcal{O}\left(\rho^{5} N^{2 / 3} m^{-2 / 3}\right), \quad \forall k \in [m]
		\end{align}
		Let us call this event $E_2$.
		
		Combining the two bounds we get 
		\begin{align*}
			\abs{f_{\mathcal{K}}(\mathbf{v}, \mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - f_{\mathcal{K}}(\mathbf{v}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) }  &\le \norm{\mathbf{u}_{\mathcal{K}}} \sqrt{\sum_{k \in \mathcal{K}} (\mathbf{w}_k^{\top} (\mathbf{h}^{(\ell-1)} - \widetilde{\mathbf{h}}^{(\ell-1)}))^2}
			\\& \leq \left(\rho^{\kappa} \sqrt{N/m} \right) \norm{\mathbf{v}}  \cdot
			\sqrt{\sum_{k \in \mathcal{K}} (\mathbf{w}_k^{\top} (\mathbf{h}^{(\ell-1)} - \widetilde{\mathbf{h}}^{(\ell-1)}))^2} \\&
			= \left(\rho^{\kappa} \sqrt{N/m} \right) \norm{\mathbf{v}}  \cdot
			\norm{ \mathbf{W}_{\mathcal{K}} \left(\mathbf{h}^{(\ell-1)} - \widetilde{\mathbf{h}}^{(\ell-1)}\right) }
			\\& \leq \left(\rho^{\kappa} \sqrt{N/m} \right) \norm{\mathbf{v}} \cdot  \mathcal{O}\left(\rho^{5} N^{2 / 3} m^{-2 / 3}\right) \\&\le \mathcal{O}\left(\rho^{5 + \kappa} N^{7/6} m^{-7/6} \norm{\mathbf{v}} \right),
		\end{align*}
		with probability at least $\Pr[E_1 \cap E_2] \geq 1 - e^{-\Omega(\rho^2)} - 2e^{-\rho^2} \ge 1 - e^{-\Omega(\rho^2)}$.
	\end{proof}
	
	%Note that $\mathbf{w}_i$ comes from $\mathcal{N}\left(0, \mathbf{I}\right)$ and hence, $\mathbf{w}_i \mathbb{I}_{\mathbf{w}_i^{\top} \mathbf{x}^{(\ell)} \ge 0}$ follows a sub-gaussian distribution (for any $\mathbf{s} \in \mathbb{R}^{m_{l-1}}$, $\mathbb{P}(|\langle \mathbf{s}, \mathbf{w}_i \mathbb{I}_{\mathbf{w}_i^{\top} \mathbf{x}^{(\ell)} \ge 0}\rangle|>t) = \mathbb{P}(|\langle \mathbf{s}, \mathbf{w}_i \rangle \mathbb{I}_{\mathbf{w}_i^{\top} \mathbf{x}^{(\ell)} \ge 0}|>t) \le \mathbb{P}(|\langle \mathbf{s}, \mathbf{w}_i \rangle |>t) \mathbb{P}(\mathbb{I}_{\mathbf{w}_i^{\top} \mathbf{x}^{(\ell)} \ge 0} = 1) \le e^{-t^2}$).
	%In the first step, we use cauchy-schwartz inequality and in the second step, we use 1-lipschitzness of $\relu$.
	%In the final step, we use the following.
	%\begin{itemize}
	%    \item $\mathbf{w}_k^{\top} \mathbf{v} \sim \mathcal{N}\left(0, \frac{2}{m} \norm{\mathbf{v}}^2\right)$. Using chi-squared inequality, we have $1 - e^{-\Omega(\rho^2)}$,
	%    \begin{equation*}
	%        \sqrt{\sum_{k \in \mathcal{K}} ([\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} \mathbf{v})^2} \le \sqrt{2} \sqrt{\frac{N}{m}} \norm{\mathbf{v}} \left(1 + \frac{\rho}{\sqrt{N}}\right).
	%    \end{equation*}
	%    \item $\left|\left\langle \mathbf{w}_{k}, \mathbf{h}^{(\ell)\prime}\right\rangle\right| \leq \mathcal{O}\left(\rho^{5} N^{2 / 3} m^{-2 / 3}\right) \quad \text { for every } k \in [m].$
	%    \item $c \sim \mathcal{O}\left(1\right)$.
	%\end{itemize}
	The following claim shows that since $\mathbf{v}$ doesn't change much with re-randomization (from assumption~\ref{ass:variant_v}), the function $f$ doesn't change much if we change the argument from $\mathbf{v}$ to $\widetilde{\mathbf{v}}$.  
	\begin{claim}\label{claim:ffv}
		\begin{equation*}
			\abs{f_{\mathcal{K}}(\mathbf{v}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - f_{\mathcal{K}}(\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) } \le \mathcal{O}(\rho^{1 + \kappa} (N/m)^{1+\zeta} \norm{\mathbf{v}}),
		\end{equation*}
		with probability at least $1 - e^{-\Omega(\rho^2)}$.
	\end{claim}
	
	\begin{proof}
		Let $\mathbf{u} = [\mathbf{W}, \mathbf{A}]_r \widetilde{\mathbf{v}}$.  We have,
		\begin{align*}
			\abs{f_{\mathcal{K}}(\mathbf{v}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - f_{\mathcal{K}}(\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)})}
			&=  \abs{\sum_{k \in \mathcal{K}} \left(u_k - \widetilde{u}_k\right)  \sigma(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)})} \\  
			&\le \sqrt{\sum_{k \in \mathcal{K}}\sigma(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)})^2 }  \sqrt{\sum_{k \in \mathcal{K}} \left( u_k - \widetilde{u}_k \right)^2} \\&
			\le \sqrt{\sum_{k \in \mathcal{K}} (\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)})^2 }  \sqrt{\sum_{k \in \mathcal{K}} \left( u_k - \widetilde{u}_k \right)^2}\\
			&= \sqrt{\sum_{k \in \mathcal{K}}(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)})^2} \sqrt{\sum_{k \in \mathcal{K}} \langle [\mathbf{w}_k, \mathbf{a}_k], (\mathbf{v} - \widetilde{\mathbf{v}}) \rangle^2},
		\end{align*}
		where we use cauchy schwartz inequality in the second step and $1$-lipschitzness of $\relu$ in the pre-final step.
		
		We will bound the two factors above separately: %The concentration inequality for chi-squared distributions (Fact~\ref{lem:chi-squared}) gives 
		Since, $[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r$ and $\widetilde{\mathbf{h}}$ are independent, we can use the concentration inequality for chi-squared distributions (Fact~\ref{lem:chi-squared}) to get
		\begin{align*}
			\sqrt{\sum_{k \in \mathcal{K}}(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)})^2} \leq \sqrt{\frac{2N}{m} + \frac{2\rho \sqrt{8N}}{m}} \norm{[\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}]},
		\end{align*}
		with probability at least $1-2e^{-\rho^2}$. It can be further simplified into 
		\begin{equation*}
			\sqrt{\sum_{k \in \mathcal{K}}(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)})^2} \leq \sqrt{\frac{2N}{m}} \norm{[\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}]} + \frac{2\rho \sqrt{2N}}{m} \norm{[\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}]},
		\end{equation*}
		using the fact that $\sqrt{1 + y} \le 1 + y/2$ for any variable $y > 0$. Let's call this event $E_3$.
		
		We assume that our deep random neural network satisfies $\norm[0]{\widetilde{\mathbf{h}}^{(\ell-1)}} \in (\sqrt{2 + (\ell - 2)\epsilon_x^2} - \frac{\rho^2}{\sqrt{m}},$\\$ \sqrt{2 + (\ell - 2)\epsilon_x^2} + 
		\frac{\rho^2}{\sqrt{m}})$ for all $\ell \in [L]$. This happens with probability at least $1-e^{-\Omega(\rho^2)}$ w.r.t. the matrices $\widetilde{\mathbf{W}}$ and $\widetilde{\mathbf{A}}$ from Lemma~\ref{lemma:norm_ESN}. Thus, provided $m \ge \Omega(\rho^4)$ and $\epsilon_x \le \frac{1}{L}$, $\norm[0]{\widetilde{\mathbf{h}}^{(\ell-1)}} \in \left( \sqrt{2}, \sqrt{3}\right)$. Let's call this event $E_4$. Also, since the sequence is assumed to be input normalized, $\norm{\mathbf{x}^{(\ell)}} \le 1$. Hence,
		\begin{equation*}
			\sqrt{\sum_{k \in \mathcal{K}}(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)})^2} \leq 2\sqrt{\frac{2N}{m}}  + \frac{4\rho \sqrt{2N}}{m},
		\end{equation*}
		Again, from assumption~\ref{ass:variant_v}, we have with probability $1-e^{-\Omega(\rho^2)}$,
		\begin{align*}
			\norm{[\mathbf{W}_{\mathcal{K}}, \mathbf{A}_{\mathcal{K}}]_r \left(\mathbf{v} - \widetilde{\mathbf{v}}\right)} \le \mathcal{O}(\rho^{\kappa} (N/m)^{0.5 + \zeta} \norm{\mathbf{v}}).
		\end{align*}
		Let us call this event $E_5$.
		
		Combining the two bounds we get 
		\begin{align*}
			\abs{f_{\mathcal{K}}(\mathbf{v}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - f_{\mathcal{K}}(\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)})}
			&\le \sqrt{\sum_{k \in \mathcal{K}}(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)})^2} \sqrt{\sum_{k \in \mathcal{K}} \langle [\mathbf{w}_k, \mathbf{a}_k], (\mathbf{v} - \widetilde{\mathbf{v}}) \rangle^2} \\&
			\le \left(2\sqrt{\frac{2N}{m}}  + \frac{4\rho \sqrt{2N}}{m}\right) \cdot \mathcal{O}(\rho^{ \kappa} (N/m)^{0.5 + \zeta} \norm{\mathbf{v}}), \\&
			\le \mathcal{O}(\rho^{1 + \kappa} (N/m)^{1+\zeta} \norm{\mathbf{v}}),
		\end{align*}
		with probability at least $\Pr[E_3 \cap E_4 \cap E_5] \geq 1 - 3e^{-\Omega(\rho^2)} \ge 1 - e^{-\Omega(\rho^2)}$.
	\end{proof}
	
	
	The next claim shows that the functions $\frac{N}{m} g$ and $f$ are close to each other, using concentration bounds w.r.t. $\{\bw_r\}_{r \in \mathcal{K}}$ and $\{\mathbf{a}_r\}_{r \in \mathcal{K}}$.
	
	\begin{claim}\label{claim:fg}
		\begin{equation*}
			\abs{f_{\mathcal{K}}(\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - \frac{N}{m} g(\widetilde{\mathbf{v}}, [\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}])} \le \mathcal{O}( \rho^{1+\kappa} N^{1/2} m^{-1} \norm{\mathbf{v}}),  
		\end{equation*}
		with probability at least $1 - e^{-\Omega(\rho^2)}$.    
	\end{claim}
	
	\begin{proof}
		Since $\widetilde{\mathbf{v}}$ and $\widetilde{\mathbf{h}}^{(\ell-1)}$ doesn't depend on $\left\{\mathbf{w}_k\right\}_{k \in \mathcal{K}}$, we can use corollary~\ref{cor:linear_estimate} directly to get 
		\begin{equation*}
			\mathbb{E}_{\left\{\mathbf{w}_k\right\}_{k \in \mathcal{K}}} f_{\mathcal{K}}(\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) = \frac{1}{2} \cdot \frac{2}{m} \cdot g([\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}]) = \frac{1}{m}g([\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}]) .
		\end{equation*}
		Let $\overline{\widetilde{\mathbf{v}}} := \widetilde{\mathbf{v}}/\norm{\widetilde{\mathbf{v}}}$ and $\overline{[\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}]} = \frac{[\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}]}{\norm{[\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}]}}$.
		
		
		
		
		For the given sequence $\mathbf{x}^{(1)}, \cdots, \mathbf{x}^{(\ell)}$, we have
		
		\begingroup
		\allowdisplaybreaks
		\begin{align}
			& \abs{\frac{1}{N}f_{\mathcal{K}}(\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - \frac{1}{m}g(\widetilde{\mathbf{v}}, [\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}])} \nonumber
			\\& = \abs{\frac{1}{N}  \sum_{k \in \mathcal{K}} [\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top}  \widetilde{\mathbf{v}} \sigma(\mathbf{w}_k^{\top} \widetilde{\mathbf{h}}^{(\ell-1)} + \mathbf{a}_k^{\top} \mathbf{x}^{(\ell)}) -   \mathbb{E}_{\mathbf{w} \sim \mathcal{N}\left(0, \frac{2}{m}\mathbf{I}_{m+d}\right)} \mathbf{w}^{\top} \widetilde{\mathbf{v}} \sigma(\mathbf{w}^{\top} [\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}])} \nonumber\\
			&= \abs{\widetilde{\mathbf{v}}^{\top} \left(\frac{1}{N} \sum_{k \in \mathcal{K}} [\mathbf{w}_{k}, \mathbf{a}_{k}] [\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top}  \mathbb{I}_{[\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} [\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}] \ge 0} - \mathbb{E}_{\mathbf{w} \sim \mathcal{N}\left(0, \frac{2}{m}\mathbf{I}_{m+d}\right)}  \mathbf{w}\mathbf{w}^{\top}  \mathbb{I}_{\mathbf{w}^{\top} [\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}] \ge 0}\right) [\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}]}  \nonumber\\
			&= \norm{\widetilde{\mathbf{v}}} \norm[0]{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}\Big|\overline{\widetilde{\mathbf{v}}}^{\top} \Big(\frac{1}{N} \sum_{k \in \mathcal{K}} [\mathbf{w}_{k}, \mathbf{a}_{k}] [\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top}  \mathbb{I}_{[\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0} 
			\nonumber\\ & \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
			- \mathbb{E}_{\mathbf{w} \sim \mathcal{N}\left(0, \frac{2}{m}\mathbf{I}_{m+d}\right)}  \mathbf{w}\mathbf{w}^{\top}  \mathbb{I}_{\mathbf{w}^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0}\Big) \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}\Big|  \nonumber\\
			&\le  \frac{\norm{\widetilde{\mathbf{v}}} \norm[0]{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}}{2}  \Big|\frac{1}{N}\sum_{k \in \mathcal{K}} \left(\left(\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \overline{\widetilde{\mathbf{v}}}\right)^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] \mathbb{I}_{[\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0} \right)^2 \nonumber\\
			&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad - \mathbb{E}_{\mathbf{w} \sim \mathcal{N}\left(0, \frac{2}{m}\mathbf{I}\right)} \left(\left(\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \overline{\widetilde{\mathbf{v}}}\right)^{\top} \mathbf{w} \mathbb{I}_{\mathbf{w}^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0} \right)^2\Big| \label{eqn:bernstein_xl_v}
			\\&  + \frac{\norm{\widetilde{\mathbf{v}}} \norm[0]{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}}{2}\Big| \frac{1}{N}\sum_{k \in \mathcal{K}} \left(\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] \mathbb{I}_{[\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0} \right)^2  \nonumber \\&\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad - \mathbb{E}_{\mathbf{w} \sim \mathcal{N}\left(0, \frac{2}{m}\mathbf{I}\right)} \left(\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}^{\top} \mathbf{w} \mathbb{I}_{\mathbf{w}^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0} \right)^2\Big| \label{eqn:bernstein_xl}\\
			&  + \frac{\norm{\widetilde{\mathbf{v}}} \norm[0]{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}}{2} \abs{\frac{1}{N}\sum_{k \in \mathcal{K}}  \left(\overline{\widetilde{\mathbf{v}}}^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] \mathbb{I}_{[\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0} \right)^2 - \mathbb{E}_{\mathbf{w} \sim \mathcal{N}\left(0, \frac{2}{m}\mathbf{I}\right)} \left(\overline{\widetilde{\mathbf{v}}}^{\top} \mathbf{w} \mathbb{I}_{\mathbf{w}^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0} \right)^2} \label{eqn:bernstein_v}
		\end{align}
		\endgroup
		
		
		The three terms above in \eqref{eqn:bernstein_xl_v}, \eqref{eqn:bernstein_xl} and \eqref{eqn:bernstein_v} correspond to the large deviation bounds for random variables $\left(\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \overline{\widetilde{\mathbf{v}}}\right)^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] \mathbb{I}_{[\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0}$, $\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] \mathbb{I}_{[\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0}$ and $\overline{\widetilde{\mathbf{v}}}^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] \mathbb{I}_{[\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0}$ respectively. Each of these random variables is
		sub-exponential as it is bounded above by a squared Gaussian random variable using the fact that $\mathbf{w} \sim \mathcal{N}\left(0, \frac{2}{m}\mathbf{I}\right)$:
		\todo{We need to explain that our r.v.s are dominated by the corresponding Gaussians.}
		\begin{itemize}
			\item $((\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \overline{\widetilde{\mathbf{v}}})^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] \mathbb{I}_{[\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0} )^2 \leq
			((\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \overline{\widetilde{\mathbf{v}}})^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] )^2$ 
			and $(\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \overline{\widetilde{\mathbf{v}}})^{\top} \mathbf{w} \sim N(0, \frac{2}{m}\norm[0]{\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \overline{\widetilde{\mathbf{v}}}}^2)$,      
			
			
			
			\item $((\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]})^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] \mathbb{I}_{[\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0} )^2 \leq
			((\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]})^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] )^2$ 
			and $(\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]})^{\top} \mathbf{w} \sim N(0, \frac{2}{m}\norm[0]{\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}}^2)$
			
			\item $((\overline{\widetilde{\mathbf{v}}})^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] \mathbb{I}_{[\mathbf{w}_{k}, \mathbf{a}_{k}]^{\top} \overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \ge 0} )^2 \leq
			((\overline{\widetilde{\mathbf{v}}})^{\top} [\mathbf{w}_{k}, \mathbf{a}_{k}] )^2$ 
			and $(\overline{\widetilde{\mathbf{v}}})^{\top} \mathbf{w} \sim N(0, \frac{2}{m}\norm[0]{\overline{\widetilde{\mathbf{v}}}}^2)$.
		\end{itemize}
		
		
		For typographical convenience, denote the expressions in \eqref{eqn:bernstein_xl_v}, \eqref{eqn:bernstein_xl} and \eqref{eqn:bernstein_v} by 
		$P(\{\bx^{(\ell)}\}), Q(\{\bx^{(\ell)}\})$ and $R(\{\bx^{(\ell)}\})$, respectively. We assume that our deep random neural network satisfies $\norm[0]{\widetilde{\mathbf{h}}^{(\ell-1)}} \in \left(\sqrt{2 + (\ell - 2)\epsilon_x^2} - \frac{\rho^2}{\sqrt{m}}, \sqrt{2 + (\ell - 2)\epsilon_x^2} + 
		\frac{\rho^2}{\sqrt{m}}\right)$ for all $\ell \in [L]$, with probability at least $1-e^{-\Omega(\rho^2)}$. Thus, provided $m \ge \Omega(\rho^4)$ and $\epsilon_x \le \frac{1}{L}$, $\norm[0]{\widetilde{\mathbf{h}}^{(\ell-1)}} \in \left(\sqrt{2}, \sqrt{3}\right)$.  %By Lemma~\ref{lemma:norm_ESN} this holds with probability at least 
		%with probability at least $1 - e^{-\Omega(\rho^2)}$
		%, provided $m = \Omega(\rho^2 L^2)$. Allowing more neurons i.e. assuming $m = \Omega(\sqrt{2} \rho^{2} L^{5})$, the bounds can be further simplified as $\norm[0]{\mathbf{h}^{(\ell-1)}} \in \left(0.5\sqrt{\ell}, 1.5\sqrt{\ell} \right)$ for all $\ell \in [L]$. 
		This event was taken care before in event $E_4$. Using this and concentration of chi-squared random variables
		(Fact~\ref{lem:chi-squared}) we get 
		\begin{itemize}
			\item $\Pr\left[\frac{2P(\{\bx^{(\ell)}\})}{\norm{\widetilde{\mathbf{v}}} \norm{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}} \leq \frac{2\sqrt{2} \rho}{ \sqrt{N} } \cdot \frac{2}{m}\norm[0]{\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \overline{\widetilde{\mathbf{v}}}}^2\right] \leq e^{-\rho^2}$,
			\item $\Pr\left[\frac{2Q(\{\bx^{(\ell)}\})}{\norm{\widetilde{\mathbf{v}}} \norm{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}} \leq \frac{2\sqrt{2} \rho}{ \sqrt{N} } \cdot \frac{2}{m}\norm[0]{\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}}^2\right] \leq e^{-\rho^2}$,
			\item $\Pr\left[\frac{2R(\{\bx^{(\ell)}\})}{\norm{\widetilde{\mathbf{v}}} \norm{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}} \leq \frac{2\sqrt{2} \rho}{ \sqrt{N} } \cdot \frac{2}{m}\norm[0]{\overline{\widetilde{\mathbf{v}}}}^2\right] \leq e^{-\rho^2}$.
		\end{itemize}
		
		%\iffalse 
		Define the following event 
		
		\begingroup\allowdisplaybreaks
		\begin{align*}
			E_6(\{\bx^{(\ell)}\}) := &\left(\frac{2P(\{\bx^{(\ell)}\})}{\norm{\widetilde{\mathbf{v}}} \norm{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}} \leq \frac{2\sqrt{2} \rho}{ \sqrt{N} } \cdot \frac{2}{m}\norm[0]{\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \overline{\widetilde{\mathbf{v}}}}^2 \right) \\&
			\cap \left(\frac{2Q(\{\bx^{(\ell)}\})}{\norm{\widetilde{\mathbf{v}}} \norm{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}} \leq \frac{2\sqrt{2} \rho}{ \sqrt{N} } \cdot  \frac{2}{m}\norm[0]{\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}}^2 \right)  \\&
			\cap \left(\frac{2R(\{\bx^{(\ell)}\})}{\norm{\widetilde{\mathbf{v}}} \norm{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}} \leq \frac{2\sqrt{2} \rho}{ \sqrt{N} } \cdot \frac{2}{m}\norm[0]{\overline{\widetilde{\mathbf{v}}}}^2 \right)          \end{align*}
		\endgroup
		%\fi
		We have thus shown that for the given sequence $\bx^{(1)}, \bx^{(2)}, \cdots, \bx^{(L)}$ with probability at least $1 - 3e^{-\rho^2} - e^{-\Omega(\rho^2)}$ the event $E_6(\bx) \cap E_3$ occurs, which implies
		\begin{align}\label{eqn:fg-enet}
			&\abs[0]{\frac{1}{N}f_{\mathcal{K}}(\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - \frac{1}{m}g([\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}])} \nonumber\\& \quad\quad\quad\quad\leq \frac{\norm{\widetilde{\mathbf{v}}} \norm{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}}{2} \cdot \frac{2\sqrt{2} \rho}{ \sqrt{N} } \cdot \frac{2}{m} (\norm[0]{\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \overline{\widetilde{\mathbf{v}}}}^2 + \norm[0]{\overline{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]}}^2 + \norm[0]{\overline{\widetilde{\mathbf{v}}}}^2) \\ \nonumber
			& \quad\quad\quad\quad\le \frac{16\sqrt{2} \rho}{ \sqrt{N} m} \norm{\widetilde{\mathbf{v}}} \norm{[\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \le \frac{32\sqrt{2} \rho}{ \sqrt{N} m} \norm{\widetilde{\mathbf{v}}}.
		\end{align} 
		\todo{Might need to add some more information above.}
		Hence with probabilty $1-e^{-\Omega(\rho^2)}$,
		\begin{equation*}
			\abs[0]{f_{\mathcal{K}}(\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - \frac{N}{m}g(\widetilde{\mathbf{v}}, [\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}])} \le \frac{32\sqrt{2N} \rho}{ m} \norm{\widetilde{\mathbf{v}}}.
		\end{equation*}
		We further use assumption~\ref{ass:variant_v} to get
		\begingroup \allowdisplaybreaks
		\begin{align*}
			\abs[0]{f_{\mathcal{K}}(\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - \frac{N}{m}g(\widetilde{\mathbf{v}}, [\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}])} &\le \frac{32\sqrt{2} \rho}{ \sqrt{N} m} \norm{\widetilde{\mathbf{v}}} \\&
			\le \frac{32\sqrt{2} \rho \sqrt{N}}{  m} \left(\norm{\mathbf{v}} + \norm{\mathbf{v} - \widetilde{\mathbf{v}}} \right)\\&
			\le \frac{32\sqrt{2} \sqrt{N}\rho}{  m} \left(\norm{\mathbf{v}} + \mathcal{O}\left(\rho^{\kappa} (N/m)^{\zeta}\right)\norm{\mathbf{v}}\right) \\&\le \mathcal{O}( \rho^{1+\kappa} N^{1/2} m^{-1} \norm{\mathbf{v}}),
		\end{align*}
		\endgroup
		with probability exceeding $1 - e^{-\Omega(\rho^2)}$.
	\end{proof}
	
	
	The following claim again uses the property that $\mathbf{v}$ and $\mathbf{h}^{(\ell - 1)}$ doesn't change much with re-randomization to show that function $g$ is also stable to re-randomization.
	\begin{claim}\label{claim:gg}
		\begin{equation*}
			\abs{\frac{N}{m}g([\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]) - \frac{N}{m}g(\mathbf{v}, [\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}])} \le \mathcal{O}(\rho^{5 + \kappa} (N/m)^{3/2}) \cdot \norm{\mathbf{v}},
		\end{equation*}
		with probability exceeding $1 - e^{-\Omega(\rho^2)}$.
	\end{claim}
	
	\begin{proof}
		\begingroup \allowdisplaybreaks
		\begin{align*}
			&\abs{g(\widetilde{\mathbf{v}}, [\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]) - g(\mathbf{v}, [\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}])}\\ &= \abs{\mathbf{v}^{\top} [\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}] - \widetilde{\mathbf{v}}^{\top} [\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \\&
			= \abs{\mathbf{v}^{\top} [\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}] - \widetilde{\mathbf{v}}^{\top} [\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}] + \widetilde{\mathbf{v}}^{\top} [\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}] - \widetilde{\mathbf{v}}^{\top} [\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} \\&
			\le\abs{\left(\mathbf{v} - \widetilde{\mathbf{v}}\right)^{\top} [\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}] - \widetilde{\mathbf{v}}^{\top} [\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}} + \abs{\widetilde{\mathbf{v}}^{\top} [\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}] - \widetilde{\mathbf{v}}^{\top} [\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} 
			\\&\le \norm{\mathbf{v} - \widetilde{\mathbf{v}}} \norm{[\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \norm{\widetilde{\mathbf{v}}} \norm{\widetilde{\mathbf{h}}^{(\ell-1)} - \mathbf{h}^{(\ell-1)}}
			\\&
			\le \norm{\mathbf{v} - \widetilde{\mathbf{v}}} \norm{[\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \norm{\widetilde{\mathbf{v}} - \mathbf{v}} \norm{\widetilde{\mathbf{h}}^{(\ell-1)} - \mathbf{h}^{(\ell-1)}} + \norm{\mathbf{v}}\norm{\widetilde{\mathbf{h}}^{(\ell-1)} - \mathbf{h}^{(\ell-1)}}.
			%\\&\le \norm{\mathbf{v}} \mathcal{O}(\rho^5 N^{1/2} m^{-1/2}),
		\end{align*}
		\endgroup
		
		
		We assume that our deep random neural network satisfies $\norm[0]{\mathbf{h}^{(\ell-1)}} \in (\sqrt{2 + (\ell - 2)\epsilon_x^2} - \frac{\rho^2}{\sqrt{m}},$\\$ \sqrt{2 + (\ell - 2)\epsilon_x^2} + 
		\frac{\rho^2}{\sqrt{m}})$ for all $\ell \in [L]$. This happens with probability at least $1-e^{-\Omega(\rho^2)}$ w.r.t. the matrices $\mathbf{W}$ and $\mathbf{A}$ from Lemma~\ref{lemma:norm_ESN}. Thus, provided $m \ge \Omega(\rho^4)$ and $\epsilon_x \le \frac{1}{L}$, $\norm[0]{\mathbf{h}^{(\ell-1)}} \in \left(\sqrt{2}, \sqrt{3}\right)$. Let's call this event $E_7$.
		Using assumption~\ref{ass:variant_v}, event $E_2$ and event $E_7$, we have with probability $1-e^{-\Omega(\rho^2)}$,
		\begin{align*}
			&\abs{g(\widetilde{\mathbf{v}}, [\widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]) - g(\mathbf{v}, [\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}])}\\ &\le 
			\norm{\mathbf{v} - \widetilde{\mathbf{v}}} \norm{[\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}]} + \norm{\widetilde{\mathbf{v}} - \mathbf{v}} \norm{\widetilde{\mathbf{h}}^{(\ell-1)} - \mathbf{h}^{(\ell-1)}} + \norm{\mathbf{v}}\norm{\widetilde{\mathbf{h}}^{(\ell-1)} - \mathbf{h}^{(\ell-1)}} \\&
			\le \mathcal{O}(\rho^\kappa (N/m)^{\zeta} (2 + \mathcal{O}(\rho^5 (N/m)^{1/2}))) \cdot \norm{\mathbf{v}} + \mathcal{O}(\rho^5 (N/m)^{1/2}) \cdot \norm{\mathbf{v}} \\&
			\le \mathcal{O}(\rho^{5 + \kappa} (N/m)^{1/2}) \cdot \norm{\mathbf{v}}.
		\end{align*}
		%using Lemma~\ref{lemma:rerandESN}. 
	\end{proof}
	
	Claims \ref{claim:ff}, \ref{claim:fg}, \ref{claim:ffv} and \ref{claim:gg} hold if the event $E_1 \cap E_2 \cap E_3 \cap E_4 \cap E_5 \cap E_6 \cap E_7$ occurs. This has probability at least $1 - e^{-\Omega(\rho^2)}$. Thus, we have
	\begin{align*}
		&\abs{f_{\mathcal{K}}(\mathbf{v}, \mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - \frac{N}{m}g(\mathbf{v}, [\mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}])} \\
		&\le  \abs{f_{\mathcal{K}}(\mathbf{v}, \mathbf{h}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - f_{\mathcal{K}}(\mathbf{v}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) } + \abs{f_{\mathcal{K}}(\mathbf{v}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - f_{\mathcal{K}}(\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) }
		\\& \quad  + \abs{f_{\mathcal{K}}(\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}) - \frac{N}{m}g(\widetilde{\mathbf{v}}, [\widetilde{\mathbf{h}}^{(\ell-1)}, \mathbf{x}^{(\ell)}])}  + \abs{\frac{N}{m}g([\widetilde{\mathbf{v}}, \widetilde{\mathbf{h}}^{(\ell-1)},  \mathbf{x}^{(\ell)}]) - \frac{N}{m}g(\mathbf{v}, [\mathbf{h}^{(\ell-1)},  \mathbf{x}^{(\ell)}])}\\
		&\le  \mathcal{O}\left(\rho^{5 + \kappa} N^{5/3} m^{-7/6} + \rho^{1 + \kappa} (N/m)^{1+\zeta} + \rho^{1+\kappa} N^{1/2} m^{-1}  + \rho^{5 + \kappa} (N/m)^{3/2}\right) \cdot \norm{\mathbf{v}}.
	\end{align*}
\end{proof}
