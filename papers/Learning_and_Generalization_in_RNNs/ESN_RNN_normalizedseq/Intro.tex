\section{Introduction}
Simple Recurrent Neural Networks \cite{Elman} also known as Elman RNNs or vanilla RNNs (just RNNs henceforth) along with their more advanced versions such as LSTMs \cite{LSTM} and GRU ~\cite{cho-etal-2014-learning} are among the most successful models for processing sequential data, finding wide-ranging applications including natural language processing, audio processing \cite{jurafsky_martin} and time series classification \cite{lim2020time}. Feedforward networks (FFNs) model functions on inputs of fixed length, such as vectors in $\Reals^d$. In contrast, RNNs model functions whose input consists of sequences of tokens $\bx^{(1)}, \bx^{(2)}, \ldots$, where $\bx^{(i)} \in \Reals^d$ for each $i$. 
%RNNs process these tokens sequentially and one can consider their output at each step or at the final step.
% The latter does not lead to any loss of generality for our purposes and we will work with it for simplicity. 
 RNNs have a notion of memory; formally it is given by the hidden state vector which is denoted by $\mathbf{h}^{(t)}$ after processing the $t$-th token. RNNs apply a fixed function to $\mathbf{h}^{(t)}$ and $\bx^{(t+1)}$ to compute $\mathbf{h}^{(t+1)}$ and the output. This fixed function is modeled by a neural networks with one hidden-layer. Compared to FFNs, new challenges arise in the analysis of RNNs: for example, the use of memory and the same function at each step introduces dependencies across time and RNN training suffers from vanishing and exploding gradients \cite{pascanu13}. 

Studies aimed at understanding the effectiveness of RNNs have been conducted since their introduction; for some of the early work, see, e.g., \cite{SiegelmannS95, kolen2001field}. These works take the form of experimental probing of the inner workings of these models as well as theoretical studies. 
The theoretical studies are often focused on expressibility, training and generalization questions in isolation rather than all together---the latter needs to be addressed to approach full understanding of RNNs and appears to be far more challenging. While experimental probing has continued apace, e.g., \cite{WeissGY18, BhattamishraAG20}, progress on theoretical front has been slow. It is only recently that training and generalization are starting to be addressed in the wake of progress on the relatively easier case of FFNs as discussed next.

RNNs are closely related to deterministic finite automata \cite{korsky2019computational, WeissGY18} as well as to dynamical systems. With finite precision and ReLU activation, they are equivalent to finite automata \cite{korsky2019computational} in computational power.
In the last few years progress was made on theoretical analysis of overparamterized FFNs with one-hidden-layer, e.g., \cite{JacotNTK, LiLiang2018, du2018gradient, allen2019learning, quanquan, Arora_Generalization, ghorbani2021linearized}. Building upon these techniques,
\cite{allen2019convergence_rnn} proved that RNNs trained with SGD (stochastic gradient descent) achieve small training loss if the number of neurons is sufficiently large polynomial in the number of training datapoints and the maximum sequence length. 

But the gap between our understanding of RNNs and FFNs remains large. \cite{Tao_generalization_RNN, generalization_zhao} provide generalization bounds on RNNs in terms of certain norms of the parameters. While interesting, these bounds shed light on only a part of the picture as they do not consider the training of the networks nor do not preclude the possibility that the norms of the parameters for the trained networks are large leading to poor generalization guarantees.
RNNs can be viewed as dynamical systems and many works have used this viewpoint to study RNNs, e.g., \cite{HardtMR16, MillerH19, pmlr-v99-oymak19a, maheswaranathan2019reverse}. Other related work includes relation to kernel methods, e.g., \cite{Yang19, RNTK, alemohammad2020scalable}, linear RNNs \cite{emami2021implicit}, saturated RNNs~\cite{merrill2019sequential, merrill2020formal, merrill2021formal}, and echo state networks~\cite{grigoryeva2018echo, ozturk2007analysis}. Several other works talk about the expressive power of the novel sequence to sequence models  Transformers~\cite{yun2019transformers, yun2020n}. Due to a large number of works in this area it is not possible to be exhaustive: apart from the references directly relevant to our work we have only been able to include a small subset. 
 %However, these results also only address parts of the problem of understanding RNNs.  


\cite{allen2019can} gave the first ``end-to-end'' result for RNNs. Very informally, their result is: if the concept class consists of functions that are sums of functions of tokens then overparametrized RNNs trained using SGD with sufficiently small learning rate can learn such a concept class. They introduce new technical ideas, most notably what they call re-randomization which allows one to tackle the dependencies that arise because the same weights are used in RNN across time. %Our paper makes use of their framework. 
However, an important shortcoming of their result is limited expressive power of their concept class: %: it consists of functions that can be represented as the sum of functions of tokens. 
while this class can be surprisingly useful as noted there, 
it cannot capture problems where the RNN needs to make use of the information in the past tokens when processing a token (in their terminolgy, their concept class can \emph{adapt} to time but not to tokens). Indeed, a key step in their proof shows that RNNs can learn to ignore the hidden state $\mathbf{h}^{(t)}$. (The above concept class comes up because it can be learnt even if $\mathbf{h}^{(t)}$ is ignored.) But the hidden state $\mathbf{h}^{(t)}$ is the hallmark of RNNs and 
is the source of information about the past tokens---in general, not something to be ignored. 
Thus, it is an important question to theoretically analyze RNNs' performance on general concept classes and it was also raised in \cite{allen2019can}. This question is addressed in the present paper. As in previous work, we work with sequences of bounded length $L$. Without loss of generality, we work with token sequences $\bx^{(1)}, \ldots, \bx^{(L)}$ of fixed length as opposed to sequences of length up to $L$.
Informally, our result is:
\vspace{-5pt}
\begin{center}
    \fbox{\begin{minipage}{\columnwidth}
Overparametrized RNNs can efficiently learn concept classes consisting of one-hidden-layer neural networks that take the entire sequence of tokens as input. The training algorithm used is SGD with sufficiently small step size. 
\end{minipage}}
\end{center}
 By the universality theorem for one-hidden-layer networks, 
  such RNNs can approximate all continuous functions of $\bx^{(1)}, \ldots, \bx^{(L)}$---though naturally the more complex the functions in the class the larger the network size required. We note that the above result applies to all three aspects mentioned above: expressive power, training and generalization. %To our knowledge, previously such a result was not known even for expressive power of RNNs. 
We illustrate the power of our result by showing that some regular languages such as PARITY can be recognized efficiently by RNNs.


