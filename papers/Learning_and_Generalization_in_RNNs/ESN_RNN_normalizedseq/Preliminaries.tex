\section{Preliminaries}
\todo{Only keep here notation that's used in the main paper}
Let
$\Sphere^{d-1} := \{\bx \in \Reals^d \mid \norm{\bx}_2 = 1\}$ be the unit sphere in $\Reals^d$.
For positive integer $n$ define $[n]:=\{1, 2, \ldots, n\}$. 
%For a matrix $\bM \in \Reals^{m\times n}$, set $\norm{\bM}_{2, \infty} := \norm{(\norm{\mathbf{m}_1}_2, \ldots \norm{\mathbf{m}_m}_2)}_\infty$, where $\mathbf{m}_1^T, \mathbf{m}_2^T, \ldots$ are the rows of $\bM$. 
Given a vector $\mathbf{v}$, by $v_i$ we denote its $i$-th component.
Given two vectors $\mathbf{a} \in \mathbb{R}^{d_1}$ and $\mathbf{b} \in \mathbb{R}^{d_2}$, $[\mathbf{a}, \mathbf{b}] \in \mathbb{R}^{d_1 + d_2}$ denotes the concatenation of the two vectors. $\langle \cdot, \cdot \rangle$ denotes the standard dot product. 
Given a matrix $\mathbf{M}$, we will denote its $i$-th row as $\mathbf{m}_i$ and the element in row $i$ and column $j$ as $m_{ij}$. Given two matrices $\mathbf{A} \in \mathbb{R}^{a_1 \times a_2}$ and $\mathbf{B} \in \mathbb{R}^{b_1 \times b_2}$ with $a_1 = b_1$ let $[\mathbf{A}, \mathbf{B}]_r \in \mathbb{R}^{a_1 \times (a_2 + b_2)}$ denote the matrix whose rows are obtained by concatenating the respective rows of $\mathbf{A}$ and $\mathbf{B}$. Similarly, $[\mathbf{A}, \mathbf{B}]_c \in \mathbb{R}^{(a_1 + b_1) \times a_2 }$ (assuming $a_2 = b_2$) denotes the matrix whose columns are obtained by concatenating the columns of $\mathbf{A}$ and $\mathbf{B}$. 
%We set $[\mathbf{A}, \mathbf{B}] := [\mathbf{A}, \mathbf{B}]_r$. 

$O(\cdot)$ and $\Omega(\cdot)$ hide absolute constants. Similarly, $\mathrm{poly}(\cdot)$ denotes a polynomial in its arguments with degree and coefficients bounded by absolute constants; different instances of $\mathrm{poly}(\cdot)$ may refer to different polynomials. Writing out explicit constants would lead to unwieldy formulas without any new insights.

%Let $\mu_d^{\beta}$ denote the Gaussian measure on $\Reals^d$ associated with the Gaussian probability distribution $\mathcal{N}(\mathbf{0},  \beta^2\mathbf{I})$. Let $\mu_d := \mu_d^{1}$ denote the standard Gaussian measure on $\Reals^d$.

Let $\sigma: \Reals \to \Reals$, given by $\sigma(x) := \max\{x, 0\} = x \,\mathbb{I}_{x \ge 0}$, be {\relu} activation function. 
$\relu$ can be extended to act on vectors by coordinate-wise application: $\sigma((x_1, \ldots, x_d)) := (\sigma(x_1), \ldots, \sigma(x_d))$.
Note that 
$\relu$ is a positive homogenous function of degree $1$, that is to say $\sigma(\lambda x) = \lambda \, \sigma(x)$ for all $x$ and all $\lambda \geq 0$. 




To be learnable efficiently, the functions in the concept class need to be not too complex. We will quantify this with the following two complexity measures which are weighted norms of the Taylor expansion and intuitively can be thought of as quantifying network size and sample complexities, resp., needed to learn $\phi$ up to error $\epsilon$. 

\begin{definition}[Function complexity \cite{allen2019learning}]\label{def:complexity}
	%The following notions from  measure the complexity of any infinite order smooth function $\phi: \mathbb{R} \rightarrow \mathbb{R}$.
	Suppose that function $\phi: \mathbb{R} \rightarrow \mathbb{R}$ has Taylor expansion $\phi(z)=\sum_{i=0}^{\infty} c_{i} z^{i}$. For $R, \epsilon>0$, define
	$$
	\begin{array}{l}
		\mathfrak{C}_{\varepsilon}(\phi, R) := \sum_{i=0}^{\infty}\left(\left(C^{*} R\right)^{i}+\left(\frac{\sqrt{\log (1 / \varepsilon)}}{\sqrt{i}} C^{*} R\right)^{i}\right)\left|c_{i}\right|, \\
		\mathfrak{C}_{\mathfrak{s}}(\phi, R) := C^{*} \sum_{i=0}^{\infty}(i+1)^{1.75} R^{i}\left|c_{i}\right|,
	\end{array}
	$$ 
	where $C^{*}=10^{4}$. As an example, if $\phi(z) = z^d$ for positive integer $d$, then $\mathfrak{C}_{\mathfrak{s}}(\phi, R) = O(R^d)$ and 
	$\mathfrak{C}_{\varepsilon}(\phi, R)  = O(R^d \log^{d/2}(\frac{1}{\varepsilon}))$. For $\phi(z) = \sin z, \cos z, e^{z}$, we have     
	$\mathfrak{C}_{\mathfrak{s}}(\phi, R) = O(1)$ and $\mathfrak{C}_{\varepsilon}(\phi, R) = \mathrm{poly}(1/\varepsilon)$. 
	We have $\mathfrak{C}_{{s}}(\phi, R) \leq \mathfrak{C}_{\varepsilon}(\phi, R) \leq \mathfrak{C}_{{s}}(\phi, O(R)) \times \mathrm{poly}(1 / \varepsilon)$ for all $\phi$ and for $\phi(z) = \sin z, e^{z}$ or constant degree polynomials, they only differ by $o(1 / \varepsilon)$. 
	See \cite{allen2019learning} for details.
	Note that $\phi$ itself is not a member of our concept class but functions like it will be used to construct members of our concept class. 
\end{definition}



\section{Problem Formulation}    
In our set-up, RNNs output a label after processing the whole input sequence.\footnote{While our set-up has similarity to previous work \cite{allen2019can}, there are also important differences.}
The data are generated from an unknown distribution $\mathcal{D}$ over $ ((\overline{\mathbf{x}}^{(2)}, \ldots, \overline{\mathbf{x}}^{(L-1)}), \mathbf{y}^{\ast}) \in ((\mathbb{S}^{d-2})^{L-2}, \mathcal{Y})$, for some label set $\mathcal{Y} \subset \mathbb{R}^{\dout }$ for some positive integer $\dout $. 
%One can think of $\dout $ as the number of classes in the prediction set. 
We call $\overline{\bx} := (\overline{\mathbf{x}}^{(2)}, \ldots, \overline{\mathbf{x}}^{(L-1)})$ the \emph{true sequence} and $\mathbf{y}^{\ast}$ the \emph{true label}. Denote by $\mathcal{Z}$ the training dataset containing $N$ i.i.d. samples from $\mathcal{D}$. We preprocess the true sequence to \emph{normalize} it:
\begin{definition}[Normalized Input sequence]\label{def:normalized_seq}
	Let $\overline{\bx} = (\overline{\mathbf{x}}^{(2)}, \ldots, \overline{\mathbf{x}}^{(L-1)})$ be a given true input sequence of length $L-2$, s.t. $\overline{\mathbf{x}}^{(i)} \in  \mathbb{S}^{d-2}$ and $\overline{x}^{(i)}_{d-1} = \frac{1}{2}$, for all $i \in [2, L-1]$. 
	%Define a modified sequence $\tilde{\mathbf{x}}^{(2)}, \ldots, \tilde{\mathbf{x}}^{(L)}$, where $\tilde{\mathbf{x}^{(i)}} = [\overline{\mathbf{x}}^{(i)}, \frac{1}{\sqrt{2}}]$, for all $i \in [1, L]$. 
	The normalized input sequence $\bx := (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(L)})$ is given by
	\begin{align*}
		\mathbf{x}^{(1)} := (\mathbf{0}^{d-1}, 1), \;\;\;
		\mathbf{x}^{(\ell)} := (\varepsilon_x \overline{\mathbf{x}}^{(\ell)}, 0), \quad \forall \ell \in [2, L-1], \;\;\;
		\mathbf{x}^{(L)} := (\mathbf{0}^{d-1}, 1), 
		%\mathbf{x}^{(L)} = (\mathbf{0}^{d-1}, \varepsilon_x), 
	\end{align*}
	where we set $\varepsilon_x > 0$ later in Theorem~\ref{thm:main_theorem_paper}. %We call this sequence a \emph{normalized sequence}. 
\end{definition}	
	We use normalized sequence in place of the true sequence as input to RNNs, as it helps in proofs, e.g., with bounds on the changes in the activation patterns at each RNN cell, when the input sequences change and also with inversion of RNNs (defined later). Our method can be applied without normalization too, but in that case our error bound has exponential dependence on the length of the input sequence. The extra dimension in the normalized sequence serves as bias which we do not use explicitly to simplify notation. 	
	% \todo{Explain if possible why we make this change. Also review the explanation that's presently there.}
	%We can define the hidden states of the ESN network as $\mathbf{h}^{(\ell)}_{(0)}$ and the diagonal matrices $\mathbf{D}^{(\ell)}_{(0)}$ for $\ell \le L$ for the alternate sequence.
	%\begin{align*}
	%    d_{(0), rr}^{(\ell)} = \mathbb{I}[\mathbf{w}_r^{\top} \mathbf{h}^{(\ell - 1)}_{(0)} + \mathbf{a}_r^{\top} \mathbf{x}^{(\ell)}_{(0)} \ge 0], \quad \forall r \in [m]. 
	%\end{align*}

%We will heavily use the alternate sequence to build the target matrices in the theorems later. 
%The choice $c_{\sigma}^{(\ell)}:=\sqrt{\frac{2}{m_{l}}}$ ensures that in expectation the $L_2$-norm of the output is the same as that of the input:
%\begin{align*}
%\Ex_{\bW^{(\ell)}} \norm{\bx^{(\ell)}}_2 = \norm{\bx^{(\ell-1)}}_2 \quad \text{for } \ell \in [L], 
%\end{align*} 
%which in turn implies
%\begin{align*}
%\Ex_{\bW} \norm{\bx^{(L)}}_2 = \norm{\bx}_2. 
%\end{align*} 
%\todo{Need more details in the above definition}

\iffalse
\begin{definition}[Echo state network (ESN) (RNNs at random initialization)]\label{def:ESN}
	The matrices $\mathbf{W} \in \mathbb{R}^{m \times m}$ and $\mathbf{A} \in \mathbb{R}^{m \times d}$ have been picked by sampling entries i.i.d. from $N(0, \frac{2}{m})$. We assume that the input sequences are of length $L$ for some given $L>0$ and are of the form 
	$\bx^{(1)}, \ldots, \bx^{(L)}$ with
	$\bx^{(\ell)} \in \mathbb{R}^{d}$ for all $\ell \in [L]$. Let $\mathbf{h}^{(0)} = 0$ and 
	%Then, the computation at step $l$ is given by
	\begin{align*}
		\mathbf{h}^{(\ell)} &:=  \sigma\left(\mathbf{A} \mathbf{x}^{(\ell)} + \mathbf{W} \mathbf{h}^{(\ell-1)} \right) \quad \text{for } \ell \in [L].
		%\\u^{(l)}_{s}         &= \sum_{r \in [m]} b_{s, r} h^{(l)}_{r} \quad \forall l \ge 1,  s \in [k],
	\end{align*}
	
	
	%Also, define the diagonal matrices $\mathbf{D}^{(\ell)}$ for $\ell \le L$ as follows. \begin{align*}
	%     d_{rr}^{(\ell)} = \mathbb{I}[\mathbf{w}_r^{\top} \mathbf{h}^{(\ell - 1)} + \mathbf{a}_r^{\top} \mathbf{x}^{(\ell)} \ge 0], \quad \forall r \in [m]. 
	%\end{align*}
	%where $\mathbf{x}^{(l)} \in \mathbb{S}^{d-1}$. Here  $\mathbf{W} \in \mathbb{R}^{m \times m}$, $\mathbf{A} \in \mathbb{R}^{m \times d}$, $\mathbf{h}^{(0)} = 0$ and $b_{r, s} \in \mathbb{R}$ for all $r \in [m], s \in [k]$. Set $\mathbf{w}_i \sim \mathcal{N}\left(0, \frac{2}{m} \mathbf{I}_{m \times m}\right)$ and $\mathbf{a}_i \sim \mathcal{N}\left(0, \frac{2}{m} \mathbf{I}_{d \times d}\right)$ for all $i \in [m]$.
\end{definition}
\fi    



\subsection{RNNs}
\begin{definition}[Recurrent Neural Networks]\label{def:RNN}
	We assume that the input sequences are of length $L$ for some given $L >0$ and are of the form $\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(L)}$ with $\mathbf{x}^{(\ell)} \in \mathbb{R}^{d}$ for all $\ell \in [L]$. An RNN is specified by three matrices $\mathbf{W}_{\mathrm{rnn}} \in \mathbb{R}^{m \times m}$, $\mathbf{A}_{\mathrm{rnn}} \in \mathbb{R}^{m \times d}$ and $\mathbf{B}_{\mathrm{rnn}}  \in \mathbb{R}^{\dout  \times m}$, where $m$ is the dimension of the hidden state and $\dout $ is the dimension of the output. The hidden states of the RNN are given by $\mathbf{h}^{(0)}_{\mathrm{rnn}} = \mathbf{0} \in \Reals^m$ and 
	%Then, the computation at step $l$ is given by
	\begin{align} \label{eqn:RNN_def}
		\mathbf{h}^{(\ell)}_{\mathrm{rnn}} &:=  \sigma(\mathbf{A}_{\mathrm{rnn}} \mathbf{x}^{(\ell)} + \mathbf{W}_{\mathrm{rnn}} \mathbf{h}^{(\ell-1)}_{\mathrm{rnn}} ) \quad \text{for } \ell \in [L].
		%\\u^{(l)}_{s}         &= \sum_{r \in [m]} b_{s, r} h^{(l)}_{r} \quad \forall l \ge 1,  s \in [\dout ],
	\end{align}
	The output at each step $\ell \in [L]$ is given by 
	%   \begin{align*}
	$ \mathbf{y}^{(\ell)}_{\mathrm{rnn}} = \mathbf{B}_{\mathrm{rnn}} \mathbf{h}^{(\ell)}_{\mathrm{rnn}}$. By \emph{RNN cell} we mean the underlying FFN in \eqref{eqn:RNN_def}.
	The $m$ rows of $\mathbf{W}_{\mathrm{rnn}}$ and $\mathbf{A}_{\mathrm{rnn}}$ correspond to the $m$ neurons in the RNN.
%	that processes $\mathbf{x}^{(\ell)},  \mathbf{h}^{(\ell-1)}_{\mathrm{rnn}} $ at each step to produce $\mathbf{h}^{(\ell)}_{\mathrm{rnn}}$.
	%   \end{align*}
	
	
	Pick the matrices $ \mathbf{W} \in \mathbb{R}^{m \times m}$ and $\mathbf{A} \in \mathbb{R}^{m \times d}$ by sampling entries i.i.d. from $N(0, \frac{2}{m})$, and pick $\mathbf{B}$ by sampling entries i.i.d. from $N(0, \frac{2}{\dout })$. When $\mathbf{W}_{\mathrm{rnn}} = \mathbf{W} $ and $\mathbf{A}_{\mathrm{rnn}} = \mathbf{A} $, the RNN is said to be at random initialization. %Such RNNs are also called Echo State Networks (ESNs). 
	We will denote the parameters of an RNN at initialization by dropping the subscript ``rnn'', thus the hidden states are $\{\mathbf{h}^{(\ell)}\}_{\ell \in [L]}$. 
	%\todo{N: say something like: "for the most part, we will be dealing with the hidden states of the RNN at initialization"} 
	In the following theorems, we will keep $\mathbf{B}_{\mathrm{rnn}}$ at initialization $\mathbf{B}$ and train only $\mathbf{A}_{\mathrm{rnn}}$ and $\mathbf{W}_{\mathrm{rnn}}$. %Denote by $\mathbf{A}_{[d-1]}  \in \mathbb{R}^{m \times (d-1)}$ the matrix containing the first $d-1$ columns of the matrix $\mathbf{A}$. 
	
	
	We write $F_{\mathrm{rnn}}^{(\ell)}(\bx; \mathbf{W}_{\mathrm{rnn}}, \mathbf{A}_{\mathrm{rnn}})=\mathbf{y}^{(\ell)}_{\mathrm{rnn}}=\mathbf{B} \mathbf{h}^{(\ell)}_{\mathrm{rnn}}$ for the output of the $\ell$-th step. Our goal is to use $\mathbf{y}^{(L)}_{\mathrm{rnn}} \in \mathbb{R}^{\dout }$ to fit the true label $\mathbf{y}^{\ast} \in \mathcal{Y}$ using some loss function $G: \mathbb{R}^{\dout } \times \mathcal{Y} \rightarrow \mathbb{R}$. We assume that for every $\mathbf{y}^{\ast} \in \mathcal{Y}, G\left(0^{k}, \mathbf{y}^{\ast}\right) \in[-1,1]$ is bounded, and
	$G\left(\cdot, \mathbf{y}^{\ast}\right)$ is convex and 1-Lipschitz continuous in its first variable. This includes, for instance, the cross-entropy loss and $\ell_{2}$ -regression loss (for bounded arguments).%. We assume that the input sequences are of length $L$ for some given $L>0$ and are of the form 
	%$\bx^{(1)}, \ldots, \bx^{(L)}$ with
	%$%\norm[0]{\bx^{(\ell)}} = 1$ for all $\ell \in [L]$. We call such sequences \emph{unit-normalized}.
	%where $\mathbf{x}^{(l)} \in \mathbb{S}^{d-1}$. Here  $\mathbf{W} \in \mathbb{R}^{m \times m}$, $\mathbf{A} \in \mathbb{R}^{m \times d}$, $\mathbf{h}^{(0)} = 0$ and $b_{r, s} \in \mathbb{R}$ for all $r \in [m], s \in [\dout ]$. Set $\mathbf{w}_i \sim \mathcal{N}\left(0, \frac{2}{m} \mathbf{I}_{m \times m}\right)$ and $\mathbf{a}_i \sim \mathcal{N}\left(0, \frac{2}{m} \mathbf{I}_{d \times d}\right)$ for all $i \in [m]$.
	
	
	\iffalse
	We introduce two more notations here. For each $\ell \in [L]$, define $\mathbf{D}^{(\ell)} \in \mathbb{R}^{m \times m}$ as a diagonal matrix, with diagonal entries \todo{why do we need to use $d_{rr}^{(\ell)}$ why not $\mathbf{D}^{(\ell)}_{rr}$}
	\begin{align}\label{eqn:diagonal}
		d_{rr}^{(\ell)} := \mathbb{I}[\mathbf{w}_r^{\top} \mathbf{h}^{(\ell - 1)} + \mathbf{a}_r^{\top} \mathbf{x}^{(\ell)} \ge 0], \quad \forall r \in [m]. 
	\end{align}
	Hence, $\mathbf{h}^{(\ell)} = \mathbf{D}^{(\ell)} (\mathbf{W} \mathbf{h}^{(\ell-1)} + \mathbf{A} \mathbf{x}^{(\ell)})$. Also, define $\mathbf{Back}_{i \to j} \in \mathbb{R}^{\dout  \times m}$ for each $1 \le i \le j \le L$ by
	\begin{align*}
		\mathbf{Back}_{i \to j} := \mathbf{B} \mathbf{D}^{(j)} \mathbf{W} \ldots \mathbf{D}^{(i+1)} \mathbf{W}, 
	\end{align*}
	with $\mathbf{Back}_{i \to i} := \mathbf{B}$. Matrices $\mathbf{Back}_{i \to j}$ arise naturally in Eq. \eqref{eqn:linear-approximation} in the first-order Taylor approximation in terms of the parameters of the function $F_{\mathrm{rnn}}^{(\ell)}(\bx; \mathbf{W}, \mathbf{A})$. Very roughly, one can interpret $\mathbf{Back}_{i \to j}$ as related to the backpropagation signal from the output at step $j$ to the parameters at step $i$. 
	
	For the fixed base sequence $\mathbf{x}^{(1)}_{(0)}, \ldots, \mathbf{x}^{(L)}_{(0)}$, we will denote the hidden states by $\mathbf{h}^{(\ell)}_{(0)}$ and the diagonal matrices by $\mathbf{D}^{(\ell)}_{(0)}$ for $\ell \le L$.
	\fi 
	%\begin{align*}
	%    d_{(0), rr}^{(\ell)} = \mathbb{I}[\mathbf{w}_r^{\top} \mathbf{h}^{(\ell - 1)}_{(0)} + \mathbf{a}_r^{\top} \mathbf{x}^{(\ell)}_{(0)} \ge 0], \quad \forall r \in [m]. 
	%\end{align*}
\end{definition}


\subsection{Concept Class}
We now define our target concept class, which we will show to be learnable by RNNs using SGD. 

\begin{definition}[Concept Class]\label{def:concept_class}
	Our concept class consists of functions $F: \mathbb{R}^{(L-2) \cdot (d-1)} \rightarrow \mathbb{R}^{\dout }$ defined as follows.
	Let $\Phi$ denote a set of smooth functions with Taylor expansions with finite complexity as in Def.~\ref{def:complexity}. To define a function $F$, we choose a subset $\{\Phi_{r, s}: \mathbb{R} \rightarrow \mathbb{R}\}_{r \in [p], s \in [\dout ]}$ from $\Phi$,  $\{\mathbf{w}_{r, s}^{\dagger} \in \mathbb{S}^{(L-2)(d-1)-1}\}_{r \in[p], s \in[\dout ]}$, a set of weight vectors, and $\{b_{r, s}^{\dagger} \in \mathbb{R}\}_{r \in[p], s \in[\dout ]}$, a set of output coefficients with $\abs[0]{b_{r, s}^\dagger} \leq 1$. Then, we define $F: \mathbb{R}^{(L-2) \cdot (d-1)} \rightarrow \mathbb{R}^{\dout }$, where for each output dimension $s \in [\dout ]$ we define the $s$-th coordinate $F_s$ of $F=(F_1, \ldots, F_{\dout })$ by
	\begin{equation}\label{eq:concept_class}
		F_s(\overline{\mathbf{x}}) := \sum_{r \in [p]} b_{r, s}^{\dagger} \Phi_{r, s} \left(\langle  \mathbf{w}_{r, s}^{\dagger}, [\overline{\mathbf{x}}^{(2)}, \ldots, \overline{\mathbf{x}}^{(L-1)}]\rangle\right).
	\end{equation}
	To simplify formulas, we assume $\Phi_{r, s}(0)=0$ for all $r$ and $s$. 
	We denote the complexity of the concept class by
	\begin{align*}
		\mathfrak{C}_{\varepsilon}(\Phi, R):=\max_{\phi \in \Phi}\{\mathfrak{C}_{\varepsilon}(\phi, R)\}, \;\;
		\mathfrak{C}_{\mathfrak{s}}(\Phi, R):=\max _{\phi \in \Phi}\{\mathfrak{C}_{\mathfrak{s}}(\phi, R)\}.
	\end{align*}
\end{definition}
\vspace{-2mm}
	Let $F^{\ast}$ be a function in the concept class with smallest possible population loss which we denote by $\mathrm{OPT}$. Hence, we are in an agnostic learning setting where our aim is to learn a function with population objective $\mathrm{OPT} + \varepsilon$. 
%The true underlying function gives output of dimension $\dout $. For each output dimension $s \in [\dout ]$, the function is composed of $p$ functions $\{ \Phi_{r, s} \}_{r \in [p]}$. 
As one can observe, functions in the concept class are given by a one hidden layer network with $p$ neurons and smooth activations. We will show that the complexity of the functions $\Phi_{r, s}$ determines the number of neurons and the number of training samples necessary to train the recurrent neural network that has $\mathrm{OPT} + \varepsilon$ population loss. 

While we have defined $F^*$ as a function of $\overline{\mathbf{x}}$, since there's a one-to-one correspondence between $\overline{\mathbf{x}}$ and $\mathbf{x}$, it will occasionally be convenient to talk about $F^*$ as being a function of $\mathbf{x}$---and this should cause no confusion. And similarly for other functions like $F_{\mathrm{rnn}}^{(\ell)}(\bx; \mathbf{W}, \mathbf{A})$.

\subsection{Objective Function and the Learning Algorithm}
We assume that there exists a function $F^{\ast}$ in the concept class that can achieve a population loss $\mathrm{OPT}$, i.e. $\underset{\left(\obx, \mathbf{y}^{\ast}\right) \sim \mathcal{D}}{\mathbb{E}} G(F^{\ast}(\obx), \mathbf{y}^{\ast}) \le \mathrm{OPT}$. The following loss function is used for gradient descent: 
\begin{align*}
    &\mathrm{Obj}(\mathbf{W}', \mathbf{A}') = \underset{\left(\obx, \mathbf{y}^{\ast}\right) \sim \mathcal{Z}}{\mathbb{E}} \mathrm{Obj}(\obx, \mathbf{y}^{\ast};  \mathbf{W}',  \mathbf{A}'), \text{  where} \\&
    \mathrm{Obj}(\obx, \mathbf{y}^{\ast};  \mathbf{W}',  \mathbf{A}') = G(\lambda F_{\mathrm{rnn}}^{(L)}(\bx;  \mathbf{W}+\mathbf{W}', \mathbf{A} + \mathbf{A}'), \mathbf{y}^{\ast} ).
\end{align*}
Parameter $\lambda$ whose value is set in the main Theorem~\ref{thm:main_theorem_paper} is a scaling factor needed for technical reasons discussed later. We consider vanilla stochastic gradient updates with $\mathbf{W}_t, \mathbf{A}_t$ denoting the matrices after $t$-steps of sgd. $\mathbf{W}_t$ and $\mathbf{A}_t$ are given by
\begin{align*}
	& \mathbf{W}_t = \mathbf{W}_{t-1} - \eta \, \nabla_{\mathbf{W}_{t-1}} \mathrm{Obj}(\obx, \mathbf{y}^{\ast};  \mathbf{W}_{t-1},  \mathbf{A}_{t-1}),
	\\&\mathbf{A}_t = \mathbf{A}_{t-1} - \eta \, \nabla_{\mathbf{A}_{t-1}} \mathrm{Obj}(\obx, \mathbf{y}^{\ast};  \mathbf{W}_{t-1},  \mathbf{A}_{t-1}),
\end{align*} \todo{try to save space}
where $(\obx, \mathbf{y}^{\ast})$ is a random sample from $\mathcal{Z}$ and $\bx$ is its normalized form. It should be noted that \cite{allen2019can} train only $\mathbf{W}$.



\emph{Remark.} We made two assumptions in our set-up: (1) input sequences are of fixed length, and (2) the output is only considered at the last step. These assumptions are without loss of generality and allow us to keep already quite complex formulas manageable without affecting the essential ideas. The main change needed to drop these assumptions is a change in the objective function, which will now include terms not just for how well the output fits the target at step $L$ but also for the earlier steps. The objective function for each step behaves in the same way as that for step $L$, and so the sum can be analyzed similarly. Intuitively speaking, considering the output at the end is the ``hardest'' training regime for RNNs as it uses the ``minimal'' amount of label information.
%All previous work dealing with either training or generalization of RNNs makes similar assumptions. 

\subsection{RNNs learn the concept class}
We are now ready to state our main theorem. We use $    \rho :=100 L \dout  \log m$ in the following.  Recall that a set $\Phi$ of smooth functions induces a concept class as in Def.~\ref{def:concept_class}.
\begin{theorem}[Main, restated in the appendix as Theorem~\ref{thm:main_theorem}]\label{thm:main_theorem_paper}
	Let $\Phi$ be a set of smooth functions.
	  For  $\epsilon_x := \frac{1}{\operatorname{poly}(\rho)}$ and $\varepsilon \in \left(0, \frac{1}{p \cdot \operatorname{poly}(\rho) \cdot \mathfrak{C}_{\mathfrak{s}}(\Phi, \mathcal{O}(\epsilon_x^{-1}))}\right)$, define complexity $C:=\mathfrak{C}_{\varepsilon}(\Phi, \mathcal{O}(\epsilon_x^{-1}))$
	and $\lambda:=\frac{\varepsilon}{10 L \rho}$. Assume that the number of neurons $m \geq \operatorname{poly}\left(C, p, L, \dout , \varepsilon^{-1}\right)$ and the number of samples $N \geq \operatorname{poly}\left(C, p, L, \dout , \varepsilon^{-1}\right)$. 
	Then with parameter choices $\eta:=\Theta\left(\frac{1}{\varepsilon \rho^{2} m}\right)$ and $T:=\Theta(p^{2} C^{2} \operatorname{poly}(\rho)\varepsilon^{-2})$
	with probability at least $1-e^{-\Omega\left(\rho^{2}\right)}$ over the random initialization, SGD satisfies
	\vspace{-2mm}
	\begin{align} \label{eqn:main-thm}
		\underset{\mathrm { sgd }}{\mathbb{E}}\Big[\frac{1}{T} \sum_{t=0}^{T-1} \underset{\left(\obx, \mathbf{y}^{\ast}\right) \sim \mathcal{D}}{\mathbb{E}} \mathrm{Obj}\Big(\obx, \mathbf{y}^{\ast};  \mathbf{W}_{t}, \mathbf{A}_t\Big)\Big] 
		\leq  \mathrm{OPT}+\varepsilon+ 1/\operatorname{poly}(\rho).
	\end{align}
\end{theorem}
\vspace{-2mm}
Informally, the above theorem states that by SGD training of overparametrized RNNs with sufficiently small learning rate and appropriate preprocessing of the input sequence, we can efficiently find an RNN that has population objective nearly as small as $\mathrm{OPT}$ as $\varepsilon+ 1/\operatorname{poly}(\rho)$ is small.
The required number of neurons and the number of training samples have polynomial dependence on the function complexity of the concept class, %functions involved in the true underlying function $F^{\ast}$, the number of functions composing to get $F^{\ast}$, 
the length of the input sequence, the output dimension, and the additional prediction error $\varepsilon$.  
%The LHS in \eqref{eqn:main-thm} is the value of the population objective function averaged over all the steps of the algorithm. The RHS in \eqref{eqn:main-thm} is close
%to $\mathrm{OPT}$ as $\varepsilon+ 1/\operatorname{poly}(\rho)$ is small. This shows that at large fraction of time steps the population objective is likely to be small---which in turn means that the function computed by the RNN is nearly as good as $F^{\ast}$, the best function in the concept class for minimizing the population objective.
\todo{can this be shortened a bit?}



