\section{Further Preliminaries}\label{sec:further_prelims}
\subsection{Notations}
Let $\Ball^d := \{\bx \in \Reals^d \mid \norm{\bx}_2 \leq 1\}$ be the unit $L_2$-ball in $\Reals^d$, and let
$\Sphere^{d-1} := \{\bx \in \Reals^d \mid \norm{\bx}_2 = 1\}$ be the unit $L_2$-sphere in $\Reals^d$. Let $V_d := \frac{\pi^{d/2}}{\Gamma((d+1)/2)}$ 
be the $d$-dimensional volume of $\Ball^d$ and let $\omega_{d-1} := \frac{2\pi^{d/2}}{\Gamma(d/2)}$ be the surface area (i.e. the $(d-1)$-dimensional volume) of $\Sphere^{d-1}$. Given a matrix $\mathbf{T} \in \mathbb{R}^{d_1 \times d_2}$ and a set $S \subset [d_1]$, we denote $\mathbf{T}_{S}$ as the matrix $\mathbb{R}^{\abs{S} \times d_2}$ that contains the rows of $\mathbf{T}$ whose indices are in the set $S$. We will denote a diagonal matrix $\mathbf{D}_{S}$ for a given set $S \subset [n]$ as $d_{ii} = 1$ for $i \in S$ and is $0$ elsewhere.


\todo{Repetitive with prelims in the main paper}
For positive integer $n$ define $[n]:=\{1, 2, \ldots, n\}$. For a matrix $\bM \in \Reals^{m\times n}$, set
$\norm{\bM}_{2, \infty} := \norm{(\norm{\mathbf{m}_1}_2, \ldots \norm{\mathbf{m}_m}_2)}_\infty$, where $\mathbf{m}_1^T, \mathbf{m}_2^T, \ldots$ are the rows of $\bM$. 
%Given two vectors $\mathbf{a} \in \mathbb{R}^{d_1}$ and $\mathbf{b} \in \mathbb{R}^{d_2}$, $[\mathbf{a}, \mathbf{b}] \in \mathbb{R}^{d_1 + d_2}$ denotes the concatenation of the two vectors. Given two matrices $\mathbf{A} \in \mathbb{R}^{a_1 \times a_2}$ and $\mathbf{B} \in \mathbb{R}^{b_1 \times b_2}$ with $a_1 = b_1$ let $[\mathbf{A}, \mathbf{B}]_r \in \mathbb{R}^{a_1 \times (a_2 + b_2)}$ denote the matrix whose rows are obtained by concatenating the respective rows of $\mathbf{A}$ and $\mathbf{B}$. Similarly, $[\mathbf{A}, \mathbf{B}]_c \in \mathbb{R}^{(a_1 + b_1) \times a_2 }$ (assuming $a_2 = b_2$) denotes the matrix whose columns obtained by concatenating the columns of $\mathbf{A}$ and $\mathbf{B}$. We set $[\mathbf{A}, \mathbf{B}] := [\mathbf{A}, \mathbf{B}]_r$. \todo{Better notation here would be to use subscripts c and r (for columns and rows) instead of 1 and 2}
Let $\mu_d^{\beta}$ denote the Gaussian measure on $\Reals^d$ associated with the Gaussian probability distribution $\mathcal{N}(\mathbf{0},  \beta^2\mathbf{I})$.
Let $\mu_d := \mu_d^{1}$ denote the standard Gaussian measure on $\Reals^d$.

%Let $\sigma: \Reals \to \Reals$, given by $\sigma(x) := \max\{x, 0\} = x \,\mathbb{I}_{x \ge 0}$, be the {\relu} activation function. $\relu$ can be extended to act on vectors by coordinate-wise application: $\sigma((x_1, \ldots, x_d)) := (\sigma(x_1), \ldots, \sigma(x_d))$.Note that $\relu$ is a positive homogenous function of degree $1$, that is to say $\sigma(\lambda x) = \lambda \, \sigma(x)$ for all $x$ and all $\lambda \geq 0$.


\todo{Specify what $O(\cdot)$ and $\Omega(\cdot)$ mean: do they hide absolute constants or do they hide constants depending on other parameters. Similarly, clarify the role of poly.}


For simplicity of notation, we will use
\begin{align*}
    &\varrho :=\frac{100 L \dout  p \cdot \mathfrak{C}_{\varepsilon}(\Phi, \mathcal{O}(\varepsilon_x^{-1})) \cdot \log m}{\varepsilon}. \\&
    \rho :=100 L \dout  \log m.
\end{align*}

%$$

\subsection{Extra set of Notations for RNNs}
We denote by $\mathbf{A}_{[d-1]}  \in \mathbb{R}^{m \times (d-1)}$ the matrix containing the first $d-1$ columns of the matrix $\mathbf{A}$. Then, we define an alternate fixed sequence as follows: $\bx_{(0)} := (\mathbf{x}^{(1)}_{(0)}, \ldots, \mathbf{x}^{(L)}_{(0)})$, where
	\begin{align*}
		\mathbf{x}^{(1)}_{(0)} = (\mathbf{0}^{d-1}, 1), \;\;\;
		\mathbf{x}^{(\ell)}_{(0)} = (\mathbf{0}^{d-1}, \varepsilon_x), \quad \forall \ell \in [2, L-1], \;\;\;
		\mathbf{x}^{(L)}_{(0)} = (\mathbf{0}^{d-1}, 1).
	\end{align*}
	We will heavily use this fixed sequence to build our model later on. \todo{Can you give better motivation for defining this? Is this definition necessary in the main paper?} There is a small difference in our definition of normalized sequence and the definition in \cite{allen2019can}. The difference is in the definition of $\bx^{(L)}$; our choice gives a better and simpler error bound. This difference leads to only minor changes in the theorems that we take from \cite{allen2019can} and we will account for those changes.

We re-introduce two more notations here for RNNs in def.~\ref{def:RNN}. For each $\ell \in [L]$, define $\mathbf{D}^{(\ell)} \in \mathbb{R}^{m \times m}$ as a diagonal matrix, with diagonal entries \todo{why do we need to use $d_{rr}^{(\ell)}$ why not $\mathbf{D}^{(\ell)}_{rr}$}
\begin{align}\label{eqn:diagonal}
	d_{rr}^{(\ell)} := \mathbb{I}[\mathbf{w}_r^{\top} \mathbf{h}^{(\ell - 1)} + \mathbf{a}_r^{\top} \mathbf{x}^{(\ell)} \ge 0], \quad \forall r \in [m]. 
\end{align}
Hence, $\mathbf{h}^{(\ell)} = \mathbf{D}^{(\ell)} (\mathbf{W} \mathbf{h}^{(\ell-1)} + \mathbf{A} \mathbf{x}^{(\ell)})$. Also, define $\mathbf{Back}_{i \to j} \in \mathbb{R}^{\dout  \times m}$ for each $1 \le i \le j \le L$ by
\begin{align*}
	\mathbf{Back}_{i \to j} := \mathbf{B} \mathbf{D}^{(j)} \mathbf{W} \ldots \mathbf{D}^{(i+1)} \mathbf{W}, 
\end{align*}
with $\mathbf{Back}_{i \to i} := \mathbf{B}$. Matrices $\mathbf{Back}_{i \to j}$ arise naturally in Eq. \eqref{eqn:linear-approximation} in the first-order Taylor approximation in terms of the parameters of the function $F_{\mathrm{rnn}}^{(\ell)}(\overline{\mathbf{x}} ; \mathbf{W}, \mathbf{A})$. Very roughly, one can interpret $\mathbf{Back}_{i \to j}$ as related to the backpropagation signal from the output at step $j$ to the parameters at step $i$. 

For the fixed base sequence $\mathbf{x}^{(1)}_{(0)}, \ldots, \mathbf{x}^{(L)}_{(0)}$, we will denote the hidden states by $\mathbf{h}^{(\ell)}_{(0)}$ and the diagonal matrices by $\mathbf{D}^{(\ell)}_{(0)}$ for $\ell \le L$.


%\fi 
%\begin{align*}
%    d_{(0), rr}^{(\ell)} = \mathbb{I}[\mathbf{w}_r^{\top} \mathbf{h}^{(\ell - 1)}_{(0)} + \mathbf{a}_r^{\top} \mathbf{x}^{(\ell)}_{(0)} \ge 0], \quad \forall r \in [m]. 
%\end{align*}
%\end{definition}

\subsection{Redefine Concept Class}
In this section, we re-define the concept class introduced in the main paper. We introduce additional symbols related to the lipschitz constant and the absolute bounds over the functions, that are necessary in the proof of the main theorem.
\begin{definition}[Concept Class]
		Our concept class consists of functions $F: \mathbb{R}^{(L-2) \cdot (d-1)} \rightarrow \mathbb{R}^{\dout }$ defined as follows.
	Let $\Phi$ denote a set of smooth functions with Taylor expansions with finite complexity as in Def.~\ref{def:complexity}. To define a function $F$, we choose a subset $\{\Phi_{r, s}: \mathbb{R} \rightarrow \mathbb{R}\}_{r \in [p], s \in [\dout ]}$ from $\Phi$,  $\{\mathbf{w}_{r, s}^{\dagger} \in \mathbb{S}^{(L-2)(d-1)-1}\}_{r \in[p], s \in[\dout ]}$, a set of weight vectors, and $\{b_{r, s}^{\dagger} \in \mathbb{R}\}_{r \in[p], s \in[\dout ]}$, a set of output coefficients with $\abs[0]{b_{r, s}^\dagger} \leq 1$. Then, we define $F: \mathbb{R}^{(L-2) \cdot (d-1)} \rightarrow \mathbb{R}^{\dout }$, where for each output dimension $s \in [\dout ]$ we define the $s$-th coordinate $F_s$ of $F=(F_1, \ldots, F_{\dout })$ by
	\begin{equation}\label{eq:concept_class_appendix}
		F_s(\overline{\mathbf{x}}) := \sum_{r \in [p]} b_{r, s}^{\dagger} \Phi_{r, s} \left(\langle  \mathbf{w}_{r, s}^{\dagger}, [\overline{\mathbf{x}}^{(2)}, \ldots, \overline{\mathbf{x}}^{(L-1)}]\rangle\right).
	\end{equation}
	To simplify formulas, we assume $\Phi_{r, s}(0)=0$ for all $r$ and $s$. 
	We denote the complexity of the concept class by
	\begin{align*}
		\mathfrak{C}_{\varepsilon}(\Phi, R):=\max_{\phi \in \Phi}\{\mathfrak{C}_{\varepsilon}(\phi, R)\}, \;\;
		\mathfrak{C}_{\mathfrak{s}}(\Phi, R):=\max _{\phi \in \Phi}\{\mathfrak{C}_{\mathfrak{s}}(\phi, R)\}.
	\end{align*}
	 Let $L_{\phi}$ denote the Lipschitz constant of function $\phi$ in the range $(-\sqrt{L}, \sqrt{L})$ and let $L_{\Phi} := \max_{\phi \in \Phi} L_{\phi}$. Also, $C_{\phi}$ denote the upper bound on the absolute value of $\phi$ in the range $(-\sqrt{L}, \sqrt{L})$ and let $C_{\Phi} := \max_{\phi \in \Phi} C_{\phi}$. We only focus on the properties of the functions in the above range, since the argument to the functions $\langle  \mathbf{w}_{r, s}^{\dagger}, [\overline{\mathbf{x}}^{(2)}, \ldots, \overline{\mathbf{x}}^{(L-1)}]\rangle$ can be shown to lie in the above range. Using the definition of $\mathfrak{C}$ from def.~\ref{def:complexity}, one can show that
	$    C_{\Phi}, L_{\Phi} \le \mathfrak{C}_s(\Phi, \sqrt{2L})$.
	We assume that there exists some function $F^{\ast}$ in the concept class that achieves population loss $\mathrm{OPT}$. Hence, our aim is to learn a function with population loss $\mathrm{OPT} + \varepsilon$. 
\end{definition}





\subsection{Important facts}

We will need the following well-known results. 
\begin{fact}[e.g. Cor.~5.35 in \cite{vershynin2010introduction}]\label{thm:norm_W}
	Let $\mathbf{A}$ be an $N \times n$ matrix whose entries are independent standard normal random variables. Then for every $t \geq 0,$ with probability at least $1-2 \exp \left(-t^{2} / 2\right)$ one has
	\[
	\sqrt{N}-\sqrt{n}-t \leq s_{\min }(\mathbf{A}) \leq s_{\max }(\mathbf{A}) \leq \sqrt{N}+\sqrt{n}+t.
	\]
\end{fact}


\begin{fact}[e.g. Thm.~1.1 in \cite{vershynin2011spectral}]\label{thm:norm_BA}
	Let $\varepsilon \in(0,1)$ and let $m, n, N$ be positive integers. Consider a random $m \times n$ matrix $\mathbf{W}=\mathbf{B} \mathbf{A},$ where $\mathbf{A}$ is an $N \times n$ random matrix whose
	entries are independent random variables with mean zero and $(4+\varepsilon)$-th moment bounded by 1, and $\mathbf{B}$ is an $m \times N$ non-random matrix. Then w.p. exceeding $1 - 2 \exp{(-t^2/2)}$
	$$ 
	\|\mathbf{W}\| \leq C(\varepsilon) \norm{\mathbf{B}}(\sqrt{n}+\sqrt{m} + t), \mbox{ and }
	$$
	$$ 
	s_{\min}(\mathbf{W}) \geq C(\varepsilon) \norm{\mathbf{B}}(\sqrt{m} - \sqrt{n-1} - t),
	$$
	where $C(\varepsilon)$ is a constant that depends only on $\varepsilon$. 
\end{fact}
\todo{I didn't see the lower bound result in Vershynin's paper cited above, certainly not in the statement of Theorem 1.}


\begin{fact}[Example 2.11 in \cite{wainwright_2019_book}]\label{lem:chi-squared}
	Let $Z_1, Z_2, \ldots$ be i.i.d. one-dimensional standard Gaussian random variables. Then
	\begin{align*}
		\Pr\left[\abs{\frac{1}{n}\sum_{i=1}^n Z_i^2 - 1} \geq t\right] \leq 2 e^{-nt^2/8}, \quad \text{for all } t \in (0, 1).
	\end{align*}
\end{fact}


\begin{fact}[Maximum of Gaussians, see e.g. \cite{boucheron2013concentration}.]\label{fact:max_gauss} 
	Let $x_{1}, x_{2}, \ldots, x_{n}$ be n Gaussians following $\mathcal{N}\left(0, \sigma^{2}\right) .$ Then
	for any $\rho>0$ 
	$$
	\operatorname{Pr}\left\{\max _{i \in[n]}\left|x_{i}\right| \leq \sqrt{2}\rho \sigma \right\} \geq 1-2ne^{-\rho^2}.
	$$
\end{fact}

\begin{fact}[Hoeffding's inequality]\label{fact:hoeffding}
	Let $x_1, \cdots, x_n$ be $n$ independent random variables, with each $x_i$ strictly bounded in the interval $[a_i, b_i]$. Let $\overline{x} = \frac{1}{n} \sum_i x_i$. Then for any $\rho > 0$,
	\begin{align*}
		\Pr\left[ \abs{\overline{x} - \mathbb{E}_x \overline{x}} \ge \frac{\rho}{n} \sqrt{\sum_i (a_i - b_i)^2}\right] \le e^{-2\rho^2},
	\end{align*}
\end{fact}


\begin{definition}[$\epsilon$-net on the sphere] A set $\mathcal{N} \subset \Sphere^{d-1}$ is called an $\epsilon$-net of $\Sphere^{d-1}$ if every point in 
	$\Sphere^{d-1}$ is within Euclidean distance $\epsilon$ of some point in $\mathcal{N}$. In other words, for every $\bx \in \Sphere^{d-1}$ there is a point 
	$\tilde{\bx} \in \mathcal{N}$ such that $\norm{\bx - \tilde{\bx}} \leq \epsilon$.
\end{definition}


\begin{fact}[see the proof of Cor. 4.2.13 in \cite{vershynin_2018_book}]\label{fact:eps-net}
	$\Sphere^{d-1}$ has an $\epsilon$-net of size at most $(3/\epsilon)^d$. 
\end{fact}

Let $\mathcal{F}:\mathbb{R}^{d} \to \mathbb{R}$ be a class of function and $\mathcal{Z} = (\bx_1, \ldots, \bx_N)$ be a set of training examples in $\mathbb{R}^{d}$. The empirical rademacher complexity is given by
\begin{align*}
	\hat{R}(\mathcal{F}, \mathcal{Z}) := \sup_{f \in \mathcal{F}} \mathbb{E}_{\zeta \in \{\pm 1\}^N} \left[\frac{1}{N} \sum_{q \in [N]} \zeta_q f(\bx_q) \right]
\end{align*}

\begin{fact}[Generalization through rademacher complexity, \cite{shalev2014understanding}]\label{fact:genRad}
	If for every function $f \in \mathcal{F}$, $\abs{f} \le b$, then with probability at least $1 - \delta$ for any $\delta \ge 0$,
	\begin{align*}
		\sup_{f \in \mathcal{F}} \left[\mathbb{E}_{\bx \in \mathcal{D}} f(\bx) - \mathbb{E}_{\bx \in \mathcal{Z}} f(\bx) \right] \le 2 \hat{R}(\mathcal{F}, \mathcal{Z})  + \mathcal{O}(\frac{b \sqrt{\log (1/\delta)}}{\sqrt{N}}).
	\end{align*}
\end{fact}

\begin{fact}[Rademacher complexity of linear networks, \cite{shalev2014understanding}]\label{fact:radcomp_linear}
	Suppose $\norm[0]{\bx}_2 = 1$ for all $\bx \in \mathcal{X}$. The class $\mathcal{F}=\left\{\bx \mapsto\langle \bw, \bx\rangle \mid\|\bw\|_{2}<B\right\}$ has rademacher complexity
	$$
	\widehat{R}(\mathcal{F}, \mathcal{Z}) \leq O\left(\frac{B}{\sqrt{\mathrm{N}}}\right).
	$$
\end{fact}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\subsection{Differences of Concept from Previous literature (Ctd from main paper)}

\subsection{Concept Class}
We now define the target concept class, which we aim to show that RNNs can learn using gradient descent. 

\begin{definition}[Concept Class]
	Let $\left\{\Phi_{r, s}: \mathbb{R} \rightarrow \mathbb{R}\right\}_{r \in [p], s \in [\dout ]}$ be infinite-order differentiable functions, and $\left\{\mathbf{w}_{r, s}^{\dagger} \in\right.$
	$\left. \mathbb{S}^{(L-2)(d-1)-1}\right\}_{r \in[p], s \in[\dout ]}$ be weight vectors and $\left\{b_{r, s}^{\dagger} \in \mathbb{R}\right\}_{r \in[p], s \in[\dout ]}$ be output vectors with values bounded by 1. Then, we consider target functions $F^{\ast}: \mathbb{R}^{L(d-1)} \rightarrow \mathbb{R}^{\dout }$ can be written as
	$$
	F_s^{\ast} = \sum_{r \in [p]} b_{r, s}^{\dagger} \Phi_{r, s} \left(\left\langle  \mathbf{w}_{r, s}^{\dagger}, [\overline{\mathbf{x}}^{(2)}, \cdots, \overline{\mathbf{x}}^{(L-1)}]\right\rangle\right) , \quad \forall s \in [\dout ].
	$$
	For proof simplicity, we assume $\Phi_{r, s}(0)=0 .$ We also use
	\begin{align*}
		\mathfrak{C}_{\varepsilon}(\Phi, R)&=\max_{r, s}\left\{\mathfrak{C}_{\varepsilon}\left(\Phi_{r, s}, R\right)\right\} \\  \mathfrak{C}_{\mathfrak{s}}(\Phi, R)&=\max _{r, s}\left\{\mathfrak{C}_{\mathfrak{s}}\left(\Phi_{ r, s}, R\right)\right\}
	\end{align*}
	to denote the complexity of $F^{*}$. Let $L_{\Phi_{r, s}}$ denote the lipschitz-constant of $\Phi_{r, s}$ in the range $(-\sqrt{L}, \sqrt{L})$ and let $L_{\Phi} = \max_{r, s} L_{\Phi_{r, s}}$. Also, denote $C_{\Phi_{r, s}}$ denote the absolute upper bound of $\Phi_{r, s}$ in the range $(-\sqrt{L}, \sqrt{L})$ and let $C_{\Phi} = \max_{r, s} C_{\Phi_{r, s}}$. We only focus on the properties of the functions in the above range, since the argument to the functions $\left\langle  \mathbf{w}_{r, s}^{\dagger}, [\overline{\mathbf{x}}^{(2)}, \cdots, \overline{\mathbf{x}}^{(L-1)}]\right\rangle$ can be shown to lie in the above range. Using the definition of $\mathfrak{C}$, one can show that
	\begin{align*}
		C_{\Phi}, L_{\Phi} \le \mathfrak{C}_s(\Phi, \sqrt{2L}).
	\end{align*}
\end{definition}

\subsection{Differences from previous literature}
The concept class in \cite{allen2019can} matched the output to a true label at each step, i.e. $F^{\ast}$ belonged to $\mathbb{R}^{\dout  \times (L-2)}$, given by
\begin{equation*}
	F^{\ast (\ell)}_s\left(\mathbf{x}\right) = \sum_{i < j} \sum_{r \in [p]} \phi_{i \to j, r, s} \left(w_{i \to j, r, s}^T \mathbf{x}^{(i)}\right) \quad \forall \ell \in [2, L], s \in [\dout ]
\end{equation*}
However, we show that the above concept class can't capture simple regular languages.
\begin{theorem}
	We assume that the alphabet set is $\{a, b\}$ and the sequences in $\mathcal{D}$ are generated from the language $a^{\star}b^{\star}a^{\star}$ with positive label for strings of the form $a^{\star} b a^{\star}$ and a negative label for the rest. Also, assume that $\dout =2$ with an "accept" logit and a "reject" logit and the loss function $G$ is a classification loss (cross entropy, hinge loss, etc.) Then, any function in the above concept class will make an error in at least $\Omega(\frac{1}{L^2})$ fraction of the samples in $\mathcal{D}$.
	%The concept class can't capture the regular language $a^{*}ba^{*}$.
\end{theorem}
The above theorem can be easily extended to the setting where we use a mean squared loss $G$ in place of a classification loss, with 2 specific vectors in $\mathbb{R}^{\dout }$ assigned to "accept" and "reject" decisions. There, one can show that any function in the concept class will have an expected error of at least $\norm{\mathbf{y}_{"accept"} - \mathbf{y}_{"reject"}}^2$.


\begin{proof}
	Let say, there exists a function in the concept class that can capture the language $a^{*}ba^{*}$.
	Pick $q \in \mathbb{N}$ such that $2 \le q \le L - 2$. Any network that captures the language $a^{*}ba^{*}$ should "accept" the string $a^{q-2}ab$ and $a^{q-2}ba$. Also,  
	strings $a^{q-2}aa$ and $a^{q-2}bb$ must be "rejected" by the function concerned. Let say $\dout  = 2$ i.e. the network computes an "accept" score and a "reject"
	score at each time step and then, takes a decision by comparing the two scores at all the time steps (can also be relaxed to comparing the scores only at time step $L$).  Let's denote the representation of characters $a$ and $b$ used by the network by $\mathbf{x}(a)$ 
	and $\mathbf{x}(b)$ respectively. 
	
	Given that the strings $a^{q-2}ab$ and $a^{q-2}aa$ must be accepted and rejected respectively, we can show that for any $t \ge q + 1$,
	\begin{align*}
		\sum_{r \in [p]} &\left( \phi_{q \to t, r, "accept"} \left(w_{q \to t, r, "accept"}^T \mathbf{x}(b)\right) - \phi_{q \to t, r, "reject"} \left(w_{q \to t, r, "reject"}^T \mathbf{x}(b)\right)\right) < \\ & \sum_{r \in [p]} \left( \phi_{q \to t, r, "accept"} \left(w_{q \to t, r, "accept"}^T \mathbf{x}(a)\right) - \phi_{q \to t, r, "reject"} \left(w_{q \to t, r, "reject"}^T \mathbf{x}(a)\right)\right)
	\end{align*}
	
	Similarly, given that the strings $a^{q-2}ba$ and $a^{q-2}bb$ must be accepted and rejected respectively, we can show that for any $t \ge q + 1$,       
	\begin{align*}
		\sum_{r \in [p]} &\left( \phi_{q \to t, r, "accept"} \left(w_{q \to t, r, "accept"}^T \mathbf{x}(b)\right) - \phi_{q \to t, r, "reject"} \left(w_{q \to t, r, "reject"}^T \mathbf{x}(b)\right)\right) > \\ & \sum_{r \in [p]} \left( \phi_{q \to t, r, "accept"} \left(w_{q \to t, r, "accept"}^T \mathbf{x}(a)\right) - \phi_{q \to t, r, "reject"} \left(w_{q \to t, r, "reject"}^T \mathbf{x}(a)\right)\right)
	\end{align*}
	However, both the equations contradict each other. Hence, there doesn't exist a function in the concept class that captures $a^{*}ba^{*}$.
\end{proof}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Some basic properties of recurrent neural networks at initialization}\label{sec:basic_prop}
The following lemma shows some basic properties of the recurrent neural network at initialization. They are a result of the concentration bounds that can be applied for gaussian weight matrices $\bW$ and $\mathbf{A}$.
\begin{lemma}\label{lemma:norm_ESN}%[Lemma B.1 and Lemma D.1 in \cite{allen2019can}]
	For any $\epsilon_x \in (0, \frac{1}{L})$ and any normalized input sequence $(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, ..., \mathbf{x}^{(L)})$, for all $\ell \in [L]$ with probability at least $1-e^{-\Omega\left(\rho^2\right)}$ we have
	\todo{Specify the value of $\rho$.}
	\begin{enumerate}
		\item $\abs{\norm[0]{\mathbf{h}^{(\ell)}} - \sqrt{2 + (\ell - 2)\epsilon_x^2}}  \le  \frac{\rho^2}{\sqrt{m}}$.
		\item $\norm{\mathbf{W}\mathbf{h}^{(\ell)}}_{\infty}, \norm{\mathbf{A}\mathbf{x}^{(\ell)}}_{\infty} \le \mathcal{O}(\frac{\rho}{\sqrt{m}})$, for all $1 \le 
		\ell \le L$.
		\item $\norm{\mathbf{h}^{(\ell)} - \mathbf{h}^{(\ell)}_{(0)}} \le \sqrt{L} \epsilon_x$, for all $1 \le 
		\ell \le L$.
		\item $\norm{\mathbf{\mathbf{W} (\mathbf{h}^{(\ell)} - \mathbf{h}_{(0)}^{(\ell)})}}_{\infty}, \norm{\mathbf{\mathbf{A} (\mathbf{x}^{(\ell)} - \mathbf{x}_{(0)}^{(\ell)})}}_{\infty} \le \mathcal{O}(\frac{\rho \sqrt{L} \epsilon_x}{\sqrt{m}})$, for all $1 \le 
		\ell \le L$. 
		\item $(1 - \frac{1}{100L})^{j-i+1} \norm{\mathbf{u}} \le \norm{\mathbf{D}^{(j)} \mathbf{W} \mathbf{D}^{(j-1)} \mathbf{W} \cdots \mathbf{D}^{(i+1)} \mathbf{W} \mathbf{D}^{(i)} \mathbf{W} \mathbf{u}} \le (1 + \frac{1}{100L})^{j-i+1} \norm{\mathbf{u}}$ for all $1 \le i \le j \le L$ and any fixed vector $\mathbf{u}$.
		\item $(1 - \frac{1}{100L})^{j-i+1} \norm{\mathbf{u}} \le \norm{\mathbf{D}^{(j)} \mathbf{W} \mathbf{D}^{(j-1)} \mathbf{W} \cdots \mathbf{D}^{(i+2)} \mathbf{W} \mathbf{D}^{(i+1)} \mathbf{W} \mathbf{D}^{(i)} \mathbf{A} \mathbf{u} } \le (1 + \frac{1}{100L})^{j-i+1} \norm{\mathbf{u}}$ for all $1 \le i \le j \le L$ and all vectors $\mathbf{u} \in \mathbb{R}^{d}$.
		\item $\norm{\mathbf{D}^{(j)} \mathbf{W} \mathbf{D}^{(j-1)} \mathbf{W} \cdots \mathbf{D}^{(i+1)} \mathbf{W} \mathbf{D}^{(i)} \mathbf{W}} \le \mathcal{O}(L^3)$ for all $1 \le i \le j \le L$.
		\item $\norm{\mathbf{D}^{(\ell)} - \mathbf{D}_{(0)}^{(\ell)}}_0 \le \mathcal{O}(L^{1/3} \epsilon_x^{2/3}m)$ for all $1 \le 
		\ell \le L$.
		\item $\abs{\mathbf{u}^{\top} \mathbf{W} \mathbf{D}^{(j)} \mathbf{W} \mathbf{D}^{(j-1)} \mathbf{W} \cdots \mathbf{D}^{(i+1)} \mathbf{W} \mathbf{D}^{(i)} \mathbf{W} \mathbf{v}} \le \mathcal{O}(\frac{\sqrt{s_1} \rho}{\sqrt{m}}) \cdot \norm{\mathbf{u}}\norm{\mathbf{v}}$, for all $1 \le i \le j \le L$ and for all $s_1$-sparse vectors $\mathbf{u}$ and $s_2$-sparse vectors $\mathbf{v}$ with $s_1, s_2 \le \frac{m}{\rho^3}$.
		\item  $\abs{\mathbf{u}^{\top} \mathbf{W} \mathbf{D}^{(j)} \mathbf{W} \mathbf{D}^{(j-1)} \mathbf{W} \cdots \mathbf{D}^{(i+1)} \mathbf{W} \mathbf{D}^{(i)} \mathbf{W} \mathbf{v}} \le \mathcal{O}(\frac{\sqrt{s_1} \rho}{\sqrt{m}}) \cdot \norm{\mathbf{u}}\norm{\mathbf{v}}$, for all $1 \le i \le j \le L$ and for all $s_1$-sparse vectors $\mathbf{u}$,  with $s_1 \le \frac{m}{\rho^3}$, and a fixed vector $\mathbf{v}$.
		\item $\norm{\mathbf{W} \mathbf{D}^{(j)} \mathbf{W} \mathbf{D}^{(j-1)} \mathbf{W} \cdots \mathbf{D}^{(i+1)} \mathbf{W}  \mathbf{D}^{(i)} \mathbf{A}}_{\infty, \infty} \le \mathcal{O}(\frac{\rho}{\sqrt{m}})$, for all $1 \le i \le j \le L$.
		%\item 
	\end{enumerate}
	%\begin{equation*}
	%    \abs{\norm[0]{\mathbf{h}^{(\ell)}} - \sqrt{1 + (\ell - 1)\epsilon_x^2}}  \le  \frac{\rho^2}{\sqrt{m}}.
	%\end{equation*}
\end{lemma}
%\todo{The third, last and second last items are different. Please mention few more details.}
\begin{proof}
	All of the properties except 4, 6, 9, 10 and 11 have been taken directly from Lemma B.1 and Lemma D.1 in \cite{allen2019can}. 
	\begin{enumerate}
		\item[4] The proof will follow from the proof of property 3. We outline the proof here. We have $\mathbf{W} (\mathbf{h}^{(\ell)} - \mathbf{h}^{(\ell)}_{(0)}) = \mathbf{W} \mathbf{U} \mathbf{U}^{\top} (\mathbf{h}^{(\ell)} - \mathbf{h}^{(\ell)}_{(0)})$ where $\mathbf{U}=G S\left(\mathbf{h}^{(1)}, \cdots, \mathbf{h}^{(L)}, \mathbf{h}^{(1)}_{(0)}, \cdots, \mathbf{h}^{(L)}_{(0)}\right) .$ Each entry of $\mathbf{W} \mathbf{U}$ is i.i.d. from $\mathcal{N}\left(0, \frac{2}{m}\right) .$ For any fixed $\mathbf{z}$ we have $\|\mathbf{W} \mathbf{U} \mathbf{z}\|_{\infty} \leq O(\sqrt{\rho} / \sqrt{m})$ with probability at least $1-e^{-\Omega\left(\rho^{2}\right)}$
		Taking $\epsilon$ -net over $\mathbf{z}$ and using $\left\|\mathbf{h}^{(\ell)} - \mathbf{h}^{(\ell)}_{(0)}\right\| \leq \sqrt{L}\epsilon_x$ from property 3 gives the desired bound. \footnote{GS denotes Gram-schmidt orthonormalization.}
		\item[6] The proof will follow from property 5. We will give the brief outline here. For a fixed vector $\mathbf{u}$, property 5 shows that
		\begin{equation*}
			(1 - \frac{1}{100L})^{j-i} \norm{\mathbf{D}^{(i)} \mathbf{A} \mathbf{u}} \le \norm{\mathbf{D}^{(j)} \mathbf{W} \mathbf{D}^{(j-1)} \mathbf{W} \cdots \mathbf{D}^{(i+2)} \mathbf{W} \mathbf{D}^{(i+1)} \mathbf{W} \mathbf{D}^{(i)} \mathbf{A} \mathbf{u} } \le (1 + \frac{1}{100L})^{j-i} \norm{\mathbf{D}^{(i)} \mathbf{A} \mathbf{u}}.
		\end{equation*}
		Following the proof technique of Lemma 7.1 in \cite{allen2018convergence}, we can show that with probability $1-e^{-\Omega(\rho^2)}$,
		\begin{align*}
			(1 - \mathcal{O}(\frac{\rho}{\sqrt{m}})) \norm{\mathbf{u}} \le \norm{\mathbf{D}^{(i)} \mathbf{A} \mathbf{u}} \le (1 + \mathcal{O}(\frac{\rho}{\sqrt{m}})) \norm{\mathbf{u}}. 
		\end{align*}
		Thus, assuming $m \ge \mathcal{O}(\rho^2 L^2)$ so that $\frac{\rho}{\sqrt{m}} = \mathcal{O}(\frac{1}{L})$,
		\begin{equation*}
			(1 - \frac{1}{100L})^{j-i + 1} \norm{\mathbf{u}} \le \norm{\mathbf{D}^{(j)} \mathbf{W} \mathbf{D}^{(j-1)} \mathbf{W} \cdots \mathbf{D}^{(i+2)} \mathbf{W} \mathbf{D}^{(i+1)} \mathbf{W} \mathbf{D}^{(i)} \mathbf{A} \mathbf{u}} \le (1 + \frac{1}{100L})^{j-i + 1} \norm{\mathbf{u}}.
		\end{equation*}
		The proof will follow from using an $\epsilon$-net over $\mathbb{R}^{d}$ to quantify for all vectors $\mathbf{u}$.
		\item[9] The proof will follow from Lemma B.14 in \cite{allen2019convergence_rnn}. We will give a brief overview here. Let $\mathbf{v}$ be a fixed $s_2$-sparse vector in $\mathbb{R}^{m}$.  Then, letting $\mathbf{z} =  \mathbf{D}^{(j)} \mathbf{W}  \cdots  \mathbf{D}^{(i)} \mathbf{W} \mathbf{v}$, we have w.p. $1-e^{-\Omega(m/L^2)}$ from Lemma B.12 of \cite{allen2019convergence_rnn}, $(1 - \frac{1}{100L})^{j-i + 1} \norm{\mathbf{v}} \le \norm{\mathbf{z}} \le (1 + \frac{1}{100L})^{j-i + 1} \norm{\mathbf{v}}$. 
		%and $\mathbf{U}=G S\left(\mathbf{h}^{(1)}, \cdots, \mathbf{h}^{(L)}\right).$
		
		Let $\mathbf{z}^{(\ell)} =  \mathbf{D}^{(\ell)} \mathbf{W}  \cdots  \mathbf{D}^{(i)} \mathbf{W} \mathbf{v}$ for $i \le \ell \le j$. Also let $\mathbf{U}=G S\left(\mathbf{h}^{(1)}, \cdots, \mathbf{h}^{(L)}, \mathbf{z}^{(1)}, \cdots, \mathbf{z}^{(L)}\right)$.
		Each entry of $\mathbf{W} \mathbf{U}$ is i.i.d. from $\mathcal{N}\left(0, \frac{2}{m}\right).$ The dimension of $\mathbf{W}\mathbf{U}$ is $(m, j - i + 1 + L)$. Using Fact~\ref{lem:chi-squared}, we can show that for a $s_1$-sparse fixed vector $\mathbf{u}$, w.p. at least $1 - e^{-\Omega(Lt^2)}$,
		\begin{align*}
			\norm{\left(\mathbf{W}\mathbf{U}\right)^{\top} \mathbf{u}} \le \mathcal{O}(\frac{\sqrt{L}t}{\sqrt{m}} )\norm{\mathbf{u}}.
		\end{align*}
		Hence,
		\begin{align*}
			\abs{\mathbf{u}^{\top} \mathbf{W} \mathbf{D}^{(j)} \mathbf{W} \mathbf{D}^{(j-1)} \mathbf{W} \cdots \mathbf{D}^{(i+1)} \mathbf{W} \mathbf{D}^{(i)} \mathbf{W} \mathbf{v}} 
			&=  \abs{\mathbf{u}^{\top} \mathbf{W} \mathbf{U} \mathbf{U}^{\top} \mathbf{D}^{(j)} \mathbf{W} \mathbf{D}^{(j-1)} \mathbf{W} \cdots \mathbf{D}^{(i+1)} \mathbf{W} \mathbf{D}^{(i)} \mathbf{W} \mathbf{v}} 
			\\&\le \norm{\left(\mathbf{W}\mathbf{U}\right)^{\top} \mathbf{u}} \norm{\mathbf{U}^{\top} \mathbf{D}^{(j)} \mathbf{W}  \cdots  \mathbf{D}^{(i)} \mathbf{W} \mathbf{v}} \\&
			= \norm{\left(\mathbf{W}\mathbf{U}\right)^{\top} \mathbf{u}} \norm{ \mathbf{D}^{(j)} \mathbf{W}  \cdots  \mathbf{D}^{(i)} \mathbf{W} \mathbf{v}}\\&
			\le \mathcal{O}(\frac{\sqrt{L}t}{\sqrt{m}}) \norm{\mathbf{u}} \norm{\mathbf{v}}.
		\end{align*}
		
		%We will set $t = \rho \sqrt{s_1}$. 
		The proof follows from setting $t = \rho \sqrt{s_1}$ and taking an $\epsilon$-net bound over all $s_2$-sparse vectors $\mathbf{v}$ and $s_1$-sparse vectors $\mathbf{u}$, that amounts to an error probability at least $1-e^{\Omega(s_2 \log m)}e^{-\Omega(m/L^2)}- e^{\Omega(s_1 \log m)}e^{-\Omega(s_1 \rho^2)}$ , which simplifies to atleast $1-e^{-\Omega(\rho^2)}$, since $s_1, s_2 \le \frac{m}{\rho^3}$.
		\item[10] The proof will follow the same technique used for property 9. The only difference is that $\mathbf{v}$ will be fixed and hence, no $\epsilon$-net is necessary for the vector $\mathbf{v}$.
		\item[11] The proof will follow the same technique used for property 9. $\mathbf{u}$ will be chosen from $\mathbf{e}_1, \cdots, \mathbf{e}_m$ and $\mathbf{v}$ will be chosen from the set of vectors $\left\{ \mathbf{D}^{(i)} \mathbf{A}\mathbf{e}_1, \cdots, \mathbf{D}^{(i)} \mathbf{A}\mathbf{e}_d \right\}$. Thus, the union bound over $\mathbf{u}$ and $\mathbf{v}$ needs to consider only $m$ and $d$ vectors respectively, in place of the $\epsilon$-net over $s_1$ and $s_2$ sparse vectors.  
	\end{enumerate}
	
	%Property 4 will follow from the proof of property 3, by modifying the proof of property 2 to focus on an $\epsilon$-net of the subspace that contains $\mathbf{h}^{(1)}, \cdots, \mathbf{h}^{(L)}, \mathbf{h}^{(1)}_{(0)}, \cdots, \mathbf{h}^{(L)}_{(0)}$ and using property 3 to bound the norm difference of $\mathbf{h}^{(\ell)}$.    
	%\begin{enumerate}
	%    \item 
	%\end{enumerate}
\end{proof}
The following lemma shows that the hidden states at initialization are resilient to re-randomization of few rows of the gaussian matrices $\bW$ and $\mathbf{A}$. The proof again follows from applying concentration bounds w.r.t. the new set of weights. This lemma is used multiple times later to break the correlations among different functions of  $\bW$ and $\mathbf{A}$.

\begin{lemma}[Stability after re-randomization, Lemma E.1 in \cite{allen2019can} ]\label{lemma:rerandESN}
	Consider a fixed set $\mathcal{K} \subseteq[m]$ with cardinality $N=\abs{\mathcal{K}}$. Consider the following matrices.
	\begin{itemize}
		\item $\widetilde{\mathbf{W}} \in \mathbb{R}^{m \times m} \text { where } \widetilde{\mathbf{w}}_{k}=\mathbf{w}_{k} \text { for } k \in [m] \backslash \mathcal{K} \text { but } \widetilde{\mathbf{w}}_{k} \sim \mathcal{N}\left(0, \frac{2 \mathbf{I}}{m}\right) \text { is i.i.d. for } k \in \mathcal{K}$
		\item $\widetilde{\mathbf{A}} \in \mathbb{R}^{m \times d} \text { where } \widetilde{\mathbf{a}}_{k}=\mathbf{a}_{k} \text { for } k \in[m] \backslash \mathcal{K} \text { but } \widetilde{\mathbf{a}}_{k} \sim \mathcal{N}\left(0, \frac{2 \mathbf{I}}{m}\right) \text { is i.i.d. for } k \in \mathcal{K}$
	\end{itemize}
	
	For any \todo{ALL use "normalized" sequences which are different. We need to clearly point out the differences in the statement and proof because of this.} normalized input sequence $\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(L)} \in \mathbb{S}^{d-1},$ we consider the following two executions of ESNs under $\mathbf{W}$ and $\widetilde{\mathbf{W}}$ respectively:
	\[
	\begin{array}{lrl}
		\mathbf{g}^{(0)}=\mathbf{h}^{(0)}=0 & \mathbf{g}^{(0)\prime}=\mathbf{h}^{(0)\prime}=0 \\
		\mathbf{g}^{(\ell)}=\mathbf{W} \mathbf{h}^{(\ell-1)}+\mathbf{A} \mathbf{x}^{(\ell)} & \tilde{\mathbf{g}}^{(\ell)} = \mathbf{g}^{(\ell)}+\mathbf{g}^{(\ell)\prime}=\widetilde{\mathbf{W}}\left(\mathbf{h}^{(\ell-1)}+\mathbf{h}^{(\ell-1)\prime}\right)+\widetilde{\mathbf{A}} \mathbf{x}^{(\ell)} \\
		\mathbf{h}^{(\ell)}=\sigma\left(\mathbf{W} \mathbf{h}^{(\ell-1)}+\mathbf{A} \mathbf{x}^{(\ell)}\right) & \tilde{\mathbf{h}}^{(\ell)} = \mathbf{h}^{(\ell)}+\mathbf{h}^{(\ell)\prime}=\sigma\left(\widetilde{\mathbf{W}}\left(\mathbf{h}^{(\ell-1)}+\mathbf{h}^{(\ell-1)\prime}\right)+\widetilde{\mathbf{A}} \mathbf{x}^{(\ell)}\right) & \text { for } \ell \in [L]
	\end{array}
	\]
	and define diagonal sign matrices $\mathbf{D}^{(\ell)} \in\{0,1\}^{m \times m}$ and $\widetilde{\mathbf{D}}^{(\ell)} = \mathbf{D}^{(\ell)}+\mathbf{D}^{(\ell)\prime} \in\{0,1\}^{m \times m}$ by letting
	$$
	d^{(\ell)}_{k, k}=\mathbb{I}_{g^{(\ell)}_{k} \geq 0} \text { and }\widetilde{d}^{(\ell)}_{k, k}=\mathbb{I}_{\widetilde{g}^{(\ell)}_{k} \geq 0}
	$$
	%Define diagonal sign matrices $\mathbf{D}^{(\ell)} \in\{0,1\}^{m \times m}$ and $\mathbf{D}^{(\ell)}+\mathbf{D}^{(\ell)\prime} \in\{0,1\}^{m \times m}$ by letting
	%$$
	%d^{(\ell)}_{k, k}=\mathbb{I}_{g^{(\ell)}_{k} \geq 0} \text { and }d^{(\ell)}_{k, k}+d^{(\ell)\prime}_{k, k}=\mathbb{I}_{g_k^{(\ell)}+g_k^{(\ell)\prime} \geq 0}
	%$$
	
	Let $N=\abs{\mathcal{K}} \leq m / \rho^{23}$. Fix any normalized input sequence $\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(L)}$. We
	have, with probability at least $1-e^{-\Omega\left(\rho^{2}\right)}$ over the randomness of $\mathbf{W}$, $\widetilde{\mathbf{W}}$, $\mathbf{A}$, $\widetilde{\mathbf{A}}$,
	\begin{enumerate}
		\item $\left\|\mathbf{g}^{(\ell)\prime}\right\|,\left\|\mathbf{h}^{(\ell)\prime}\right\| \leq \mathcal{O}\left(\rho^{5} \sqrt{N / m}\right) \quad$ for every $\ell \in[L].$
		\item $\left|\left\langle \mathbf{w}_{k}, \mathbf{h}^{(\ell)\prime}\right\rangle\right| \leq \mathcal{O}\left(\rho^{5} N^{2 / 3} m^{-2 / 3}\right) \quad \text { for every } k \in [m], \ell \in[L].$
		\item $\norm{ \mathbf{W}_{\mathcal{K}} \mathbf{h}^{(\ell)\prime}}\leq \mathcal{O}\left(\rho^{5} N^{2 / 3} m^{-2 / 3}\right) \quad \text { for every }  \ell \in[L].$
		\item $\norm{\mathbf{D}^{(\ell)} - \widetilde{\mathbf{D}}^{(\ell)}}_0 \le \mathcal{O}(\rho^4 N^{1/3} m^{2/3})$, for all $\ell \in [L]$.
		\item $\norm{ \left(\prod_{i \le \ell' \le j} \widetilde{\mathbf{D}}^{(k - \ell' + 1)} \widetilde{\mathbf{W}}  - \prod_{i \le \ell' \le j} \mathbf{D}^{(k - \ell' + 1)} \mathbf{W} \right) \mathbf{v}}_{2} \le  \mathcal{O}(\rho^5 (N/m)^{1/6}) \norm{\mathbf{v}}$, for all $1 \le i \le j \le L$ and for a fixed vector $\mathbf{v}$.
		\item $\norm{\mathbf{W}_{\mathcal{K}} \left(\prod_{i \le \ell' \le j} \widetilde{\mathbf{D}}^{(k - \ell' + 1)} \widetilde{\mathbf{W}}  - \prod_{i \le \ell' \le j} \mathbf{D}^{(k - \ell' + 1)} \mathbf{W}\right) \mathbf{v}}_2 \le  \mathcal{O}(\rho^6 (N/m)^{2/3})$, for all $1 \le i \le j \le L$  and for a fixed vector $\mathbf{v}$.
		%\item $\norm{ \mathbf{D}^{(\ell)\prime}}_0 \leq \mathcal{O}\left(\rho^{4} N^{1 / 3} m^{2 / 3} \right) \quad \text { for every }  \ell \in[L].$
	\end{enumerate}
\end{lemma}

%\todo{Bit 3 and bit 6 aren't there in their theorems. Add some details here.}
%\todo{Comment on the differences in the statements (extra $L$ factor in our case) and why they arise (e.g. because the input is normalized differently in the two cases? The lemmas don't hold for all $\rho$.). Bit 3 isn't there in the proof. Say that it follows directly from their proof.}

\begin{proof}
	All the properties except 3, 5 and 6 follow from Lemma E.1 in \cite{allen2019can}. 
	\begin{enumerate}
		\item[3] The proof will follow the same technique as used for property 2. We give a brief overview here. We follow the same technique to expand the desired term
		\begin{align*}
			\mathbf{W}_{\mathcal{K}} \mathbf{h}^{(\ell)\prime} = \mathbf{D}_{\mathcal{K}} \mathbf{W} \mathbf{D}^{(\ell)\prime} (\mathbf{g}^{(\ell)} + \mathbf{g}^{(\ell)\prime}) + \mathbf{D}_{\mathcal{K}} \mathbf{W} \mathbf{D}^{(\ell)}  \mathbf{g}^{(\ell)\prime}
		\end{align*}
		We bound both the terms using the same technique with the following difference: 
		we use property 9 of Lemma~\ref{lemma:norm_ESN} to bound the terms $\norm{\mathbf{D}_{\mathcal{K}} \mathbf{W} \mathbf{D}^{(\ell)\prime}}$ and $\norm{\mathbf{D}_{\mathcal{K}} \mathbf{W} \mathbf{D}^{(\ell)}  \mathbf{g}^{(\ell)\prime}}$.
		%, in place of sparsity $s_1 = 1$ used for property 2.
		
		\item[5] The proof follows from the proof of Lemma E.1(4) in \cite{allen2019can}. In the proof of lemma E.1(4), the term that has been bounded is
		\begin{align*}
			\norm{ \left(\prod_{i \le \ell' \le j} \widetilde{\mathbf{D}}^{(k - \ell' + 1)} \widetilde{\mathbf{W}}  - \prod_{i \le \ell' \le j} \mathbf{D}^{(k - \ell' + 1)} \mathbf{W} \right) \mathbf{e}_k}_{2}, \text{where } k \in [m].
		\end{align*}
		The important property of the vectors $\mathbf{e}_k$ that is used to bound the term above is the $1$-sparsity of the vectors, which is necessary for using a property similar to property 9 of lemma~\ref{lemma:norm_ESN}.
		However, we can show that the same bound holds for a fixed vector $\mathbf{v}$ by bounding the terms that contain $\mathbf{v}$ using property 10 of lemma~\ref{lemma:norm_ESN}.
		
		
		\item[6] The proof will follow the same technique as used for property 5. We give a brief overview here. 
		
		For property 5, the term under consideration, $\left(\prod_{i \le \ell' \le j} \widetilde{\mathbf{D}}^{(k - \ell' + 1)} \widetilde{\mathbf{W}}  - \prod_{i \le \ell' \le j} \mathbf{D}^{(k - \ell' + 1)} \mathbf{W} \right) \mathbf{v}$, was expanded into all the (exponentially many) difference
		terms, which were bounded separately. Denote the difference terms as $\mathbf{T}_1, \mathbf{T}_2, \cdots$. 
		
		For each term $\mathbf{T}_i$, the product $\mathbf{W}_{\mathcal{K}} \mathbf{T}_i$ can be written as a product of $\mathbf{D}_{\mathcal{K}} \mathbf{W} (\prod_{\ell_1 \le \ell \le \ell_2 } \mathbf{D}^{(\ell)} \mathbf{W}) \overline{\mathbf{D}}$ and a term $\overline{\mathbf{T}}_i$, for some $i \le \ell_1, \ell_2 \le j$ and $\overline{\mathbf{D}}$ is either $\mathbf{D}_{\mathcal{K}}$ or $\mathbf{D}^{(\ell)} - \mathbf{D}^{(\ell)}_{(0)}$. The term $\overline{\mathbf{T}}_i$ will be bounded in a similar manner as has been done in the proof of property 5. However, the extra term that appears will be the bound of the norm of $\mathbf{D}_{\mathcal{K}} \mathbf{W} (\prod_{\ell_1 \le \ell \le \ell_2 } \mathbf{D}^{(\ell)} \mathbf{W}) \overline{\mathbf{D}}$, which is bounded by $\mathcal{O}(\rho \sqrt{N/m})$ using property 9 of lemma~\ref{lemma:norm_ESN}. 
		
		We will give an example for a term $\mathbf{T}_i$. Few terms will be of the form \begin{equation*}
			(\prod_{\ell_1 \le \ell \le \ell_2 } \mathbf{D}^{(\ell)} \mathbf{W}) \cdot \mathbf{D}^{(\ell_2)\prime} \mathbf{W}  \cdot (\prod_{\ell_2 < \ell \le \ell_3 } \mathbf{D}^{(\ell)} \mathbf{W}) \mathbf{v}, 
		\end{equation*}
		for some $\ell_1, \ell_2, \ell_3$. We break its product with $\mathbf{W}_{\mathcal{K}}$ as
		\begin{align*}
			&\mathbf{W}_{\mathcal{K}} \cdot (\prod_{\ell_1 \le \ell \le \ell_2 } \mathbf{D}^{(\ell)} \mathbf{W}) \cdot \mathbf{D}^{(\ell_2)\prime} \mathbf{W}  \cdot (\prod_{\ell_2 < \ell \le \ell_3 } \mathbf{D}^{(\ell)} \mathbf{W}) \mathbf{v} \\&=
			\underbrace{\left(\mathcal{D}_{\mathcal{K}} \mathbf{W} \cdot (\prod_{\ell_1 \le \ell \le \ell_2 } \mathbf{D}^{(\ell)} \mathbf{W}) \cdot \mathbf{D}^{(\ell_2)\prime} \right)}_{\text{Term 1}} \cdot \underbrace{\left(\mathbf{D}^{(\ell_2)\prime} \cdot (\prod_{\ell_2 < \ell \le \ell_3 } \mathbf{D}^{(\ell)} \mathbf{W}) \mathbf{v}\right)}_{\text{Term 2}}. 
		\end{align*}
		Term 2 appears in the proof of property 5. Term 1 is the extra term that needs to be bounded and we can use property 9 of lemma~\ref{lemma:norm_ESN} to bound its norm by $\mathcal{O}(\rho \sqrt{N/m})$. 
	\end{enumerate}
\end{proof}

