  The aim of the present article is to develop a theory of tropical
  probability spaces, which are asymptotic classes of finite
    probability spaces.  Together with the accompanying techniques,
  we expect them to be relevant to problems arising in information
  theory, causal inference, artificial intelligence and neuroscience.

  As a matter of introduction and motivation of the research presented
  in the article, we start by considering a few simple examples.

\subsection{Single probability spaces}\label{s:intro-single}
  We consider a {finite probability space} $X = (S, p)$, where $S$
  is a finite set, and $p$ is a probability measure on $S$. For
  simplicity, assume for now that the measure $p$ has full
  support. Next, we consider the, so-called, Bernoulli sequence of
  {probability spaces}
  \[
  X^{\otimes n} = ( S^n, p^{\otimes n})
  \] 
  where $S^n$ denotes the $n$-fold Cartesian product of $S$, and
  $p^{\otimes n}$ is the $n$-fold product measure. 
  
  This situation arises in several contexts.  For example, in physics,
  $X^{\otimes n}$ would encode the state of the system comprised of many
  identical non-interacting (weakly interacting) subsystems with state
  space $X$.  In information theory, $X^{\otimes n}$ would describe the
  output of an i.i.d.~random source.  In dynamical systems or stochastic
  processes the setting corresponds to Bernoulli shifts and Bernoulli
  processes.
  
  The \emph{entropy} of $X$ is the exponential growth rate of the
  \emph{observable} cardinality of tensor powers of $X$.  The
  observable cardinality, loosely speaking, is the cardinality of the
  set $X^{\otimes n}$ after (the biggest possible) set of small measure
  of elements, each with negligible measure, has been removed.  It turns
  out that the observable cardinality of $X^{\otimes n}$ might be much
  smaller than $|S|^{n}$, the cardinality of the whole of $X^{\otimes
    n}$, in the following sense.

  The \emph{Asymptotic Equipartition Property} states that for every
  $\epsilon>0$ and sufficiently large $n$ one can find a, so-called,
  \emph{typical subset} $A^{\c n}_{\epsilon}\subset S^{n}$, such that
  it takes up almost all of the mass of $X^{\otimes n}$ and the
  probability distribution on $A^{\c n}_{\epsilon}$ is almost uniform
  on the normalized logarithmic scale,
  
  \begin{enumerate}
  \item\label{intro-aep1}
    $p^{\otimes n}(A^{\c n}_{\epsilon})\geq 1-\epsilon$
  \item\label{intro-aep2}
    For any $a,a'\in A^{\c n}_{\epsilon}$ holds 
    $\left|\frac1n \ln p(a)-\frac1n\ln p(a')\right|\leq\epsilon$
  \end{enumerate}

  The cardinality $|A^{\c n}_{\epsilon}|$ may be much smaller than
  $|S|^{n}$, but it will still grow exponentially with $n$.  Even
  though there are many choices for such a set $A^{\c n}_\epsilon$,
  the exponential growth rate with respect to $n$ is well-defined upto
  $2\epsilon$. In fact, there exists a number $h_{X}$ such that for
  any choice of the typical subset $A^{(n)}_{\epsilon}$ holds
\[
\ebf^{n \cdot h_X - \epsilon} \leq |A_\epsilon^{(n)}| \leq \ebf^{n \cdot h_X + \epsilon}
\]

The limit of the
growth rate as $\epsilon\to0+$ is called the entropy of $X$, as
explained in more detail in Section~\ref{s:category-entropy}
\[
\Ent(X) 
:= 
\lim_{\epsilon \downarrow 0} \lim_{n \to \infty} \frac{1}{n} \ln |A_\epsilon^{(n)}|
\]
By the law of large numbers
\[
\ent(X) 
= 
h_X 
= 
- \sum_{x \in S} p(x) \ln p(x)
\]
which is the formula by which the Shannon entropy is usually introduced. 

Entropy is especially easy to evaluate if the space is uniform, since
for any {finite probability space} with the uniform distribution holds
\[\tageq{entropy-uniform}
  \ent(X)=\ln|X|      
\]  

This point of view on entropy goes back to the original idea of Boltzmann,
according to which entropy is the logarithm of the number of
equiprobable states, that a system, comprised of many identical weakly
interacting subsystems, may take on.

\subsubsection{Asymptotic equivalence}\label{s:intro-single-ae}
The Asymptotic Equipartion Property implies that the sequence
$X^{\otimes n}$ is asymptotically equivalent to a sequence of uniform spaces in
the following sense. Let us denote by $p_U$ the probability
distribution that is supported on $A_\epsilon^{(n)}$, and is uniform
on its support. Then a sequence from independent samples according to $p_U$
is very hard to discriminate from a sequence of independent samples
from $p_X^{\otimes n}$.

Similarly, when $X$ and $Y$ are probability spaces with the same entropy, 
the sets $X$ and $Y$ are asymptotically equivalent in the sense that there is 
a bijection between the typical sets, which can be seen as a change of code. 
This is essentially the content of Shannon's source coding theorem.

In \cite{Gromov-Search-2012}, Gromov proposed this existence of an 
``almost-bijection'' as a basis of an
asymptotic equivalence relation on sequences of probability spaces.
Even though we were greatly influenced by ideas in
\cite{Gromov-Search-2012}, we found that Gromov's definition does not
extend easily to \textbf{configurations} of probability spaces. 

By a configuration of probability spaces we mean a collection of probability spaces with 
measure-preserving maps between some of them. We will give a precise definition in Section \ref{s:category-config}, but will consider some particular examples below.
Formalizing
and studying a notion of asymptotic equivalence for configurations of
probability spaces is the main topic of the present article.


\subsection{Configurations of probability spaces}
\label{s:intro-2fan}
Suppose that now instead of a single probability space, we consider
a pair of probability spaces $X=(\un X,p_{X})$ and $Y=(\un Y,p_{Y})$ with a
joint distribution, that is a probability measure on $\un X\times \un Y$ that
pushes forward to $p_{X}$ and $p_{Y}$ under coordinate projections.
In other words, we consider a triple of probability spaces $X$, $Y$
and $U$ with a pair of measure-preserving maps $U\to X$ and $U\to
Y$. This is what we later call a minimal two-fan of probability spaces
\[  
\begin{tikzcd}[row sep=small,column sep=tiny,ampersand replacement=\&]
  \mbox{}
  \&
  U
  \arrow{dl}{}
  \arrow{dr}{}
  \&
  \\
  X
  \&
  \&
  Y
\end{tikzcd}
\]  
and is a particular instance of a configuration of probability spaces.
 
\begin{figure}
  \begin{lpic}[l(6mm),b(5mm),draft,clean]{squares-snakes(0.45)}
    \lbl[r]{0,40; $Y_{1}$}
    \lbl[t]{41,-1; $X_{1}$}
    \lbl{25,55;$U_{1}$}
    \lbl[r]{110,40; $Y_{2}$}
    \lbl[t]{151,-1; $X_{2}$}
    \lbl{135,55;$U_{2}$}
    \lbl[r]{220,40; $Y_{3}$}
    \lbl[t]{261,-1; $X_{3}$}
    \lbl{250,50;$U_{3}$}
  \end{lpic}
  \caption{Examples of pairs of probability spaces together with a
    joint distribution.}
  \label{f:squares-snakes}
\end{figure}

\subsubsection{Three examples}
Three examples of such an object are shown on Figure
\ref{f:squares-snakes}, which is to be interpreted in the following
way. Each of the spaces $X_{i}$ and $Y_{i}$, $i=1,2,3$, have
cardinality six and a uniform distribution, where the weight of each
atom is $\frac16$.  The spaces $U_{i}$, $i=1,2,3$, have cardinality
$12$ and the distribution is also uniform with all weights being
$\frac{1}{12}$. The support of the measure on $U_{i}$'s is colored
grey on the pictures.  The maps from $U_{i}$ to $X_{i}$ and $Y_{i}$
are coordinate projections.

In view of equation (\ref{eq:entropy-uniform}) we have for each
$i=1,2,3$,
\begin{align*}
  \ent(X_{i})&=\ln 6\\
  \ent(Y_{i})&=\ln 6\\
  \ent(U_{i})&=\ln 12
\end{align*}

Now we would like to ask the following.\\
\textbf{Question}. 
\begin{quote}\em
  Is it possible to find an almost-bijection between sufficiently high
  powers of $(X_{i}^{\otimes n}\ot Z_{i}^{\otimes n}\to Y_{i}^{\otimes
    n})$ by $(X_{j}^{\otimes n}\ot Z_{j}^{\otimes n}\to Y_{j}^{\otimes
    n})$ for $i\neq j$ with an arbitrary given precision as in
  Shannon's coding theorem as described at the end of the previous
  subsection~\ref{s:intro-single}?  More generally, what is the proper
  generalization of an asymptotic equivalence relation as discussed in
  the previous subsection to sequences of tensor powers of two-fans?
\end{quote}

  We would like to argue that even though the entropies of the
  constituent spaces are all (pairwise) the same, all three examples
  above should be pairwise asymptotically different.

  To establish that the examples in Figure~\ref{f:squares-snakes} are
  different, that is, not isomorphic (see also Section
  \ref{s:category-config}) is relatively easy, since they have
  non-isomorphic symmetry groups.  However, we present a different
  argument, that lends itself for generalization to prove that the
  examples at hand are not \emph{asymptotically} equivalent and
  that also gives a quantitative difference between them.

  To distinguish Example 1 from both 2 and 3, one could argue along
  the following lines. We could try to add a third space $Z=(\un
  Z,p_{Z})$ to the pair $X$ and $Y$ and provide a \emph{joint
    distribution} $p_{Q}$ on
  \[
  Q=(\un X\times\un Y\times\un Z,p_{Q})
  \]
  such that the projection of $p_{Q}$ on the first two factors is
  $p_{U}$ and on the third factor is $p_{Z}$.
  
  Once we do that, we could evaluate entropies of various push-forwards
  of $p_{Q}$. Denote by $V=(\un X\times \un Z,p_{V})$ and $W=(\un Y\times
  \un Z,p_{W})$, where $p_{V}$ and $p_{W}$ are push-forwards of $p_{Q}$
  under corresponding coordinate projections. All the probability spaces
  now fit into a commutative diagram
  \[
  \begin{tikzcd}[row sep=small,column sep=small,ampersand replacement=\&]
    \&
    Q
    \arrow{dl}{}
    \arrow{d}{}
    \arrow{dr}{}
    \&
    \\
    \blue{U}
    \arrow[color=blue]{d}{}
    \arrow[color=blue]{dr}{}
    \&
    V
    \&
    W
    \arrow{dl}{}
    \arrow{d}{}
    \\
    \blue{X}
    \arrow[leftarrow, crossing over]{ur}{}
    \&
    \blue{Y}
    \&
    Z
    \arrow[leftarrow, crossing over]{ul}{}
    \\
  \end{tikzcd}
  \]
  where each arrow is a reduction, which is simply a measure-preserving
  map between probability spaces.

We consider the set of all possible extensions of the above form
and denote it by $\Ext(X,Y,U)$.
For any extension $\Ebf=(X,Y,Z,U,V,W,Q)$ in $\Ext(X,Y,U)$ we have
four ``new'' entropies
\[\tageq{extension-entropies}
\ent(Q),\quad\ent(V),\quad\ent(W),\quad\ent(Z)
\]
in addition to the ``known'' entropies of $X$, $Y$ and $U$.
The vector 
\[
\ent_{*}(\Ebf):=\big(\ent(X),\ent(Y),\ent(Z),\ent(U),\ent(V),\ent(W),\ent(Q)\big)
\]
is the entropy vector of the extension $\Ebf$.

The set of all possible values of the entropy vector for all extensions of
$(X,Y,U)$
\begin{align*}
&\res(X,Y,U)
:=
\\
&\set{\ent_{*}(\Ebf)\in\Rbb^{7}\st\text{$\Ebf$ is an extension of $(X,Y,U)$}}
\subset
\Rbb^{7}
\end{align*}
is what we call the unstabilized relative entropic set of the
two-fan $(X\ot U\to Y)$. 

\subsubsection{The unstabilized relative entropic sets for the examples}
\label{s:unstabilizedexamples}
It turns out that these \emph{unstabilized} relative entropic sets of $(X_1 \ot U_1 \to Y_1)$ and $(X_2 \ot U_2 \to Y_2)$ are different
\[
\res(X_{1},Y_{1},U_{1})\neq\res(X_{2},Y_{2},U_{2})
\]

To see this, let us calculate some particular points in the unstabilized relative entropic sets of the Examples 1--3. We consider the constrained Information-Optimization problem, of finding an extension $\Ebf = (X, Y, Z, U, V, W, Q)$ of $(X, Y, U)$ such that
\begin{enumerate}
\item \label{i:low}
the space $Z$ is a reduction of $U$, that is
\[
\Ent(Q) = \Ent(U)
\]
\item \label{i:indep}
the spaces $X$ and $Y$ are independent conditioned on $Z$, 
\[
\Ent(Q) + \Ent(Z) = \Ent(V) + \Ent(W)
\]
\item\label{i:max}
the sum
\[
\Ent(X \rel Z) + \Ent(Y \rel Z)
\]
is maximal, subject to conditions (\ref{i:low}) and (\ref{i:indep}).
\end{enumerate}

  It is very easy to read the solutions $\hat\Ebf_{1}$, $\hat\Ebf_{2}$
  and $\hat\Ebf_{3}$ of this optimization problem for Examples 1, 2
  and 3 right from the pictures in Figure~\ref{f:squares-snakes}.
  Indeed, condition~(\ref{i:low}) says that $Z_i$ must be a partition
  of $U_i$. Condition~(\ref{i:indep}) says that each set in the
  partition must be ``rectangular'', that is it must be a Cartesian
  product of a subset of $\un X_{i}$ and a subset of $\un Y_{i}$. The
  quantity to be maximized is the average $\log$-area of the sets in
  the partition.

  Optima are very easy to find ``by hand''. For Example 1 it is a
  partition of $U_{1}$ into three $2\times2$ squares. In
  Examples 2 and 3, one of the solutions is the partition of $U_{2}$,
  (resp. $U_{3}$) into $1\times2$ rectangles. Thus, the optimal values
  are $(2\ln2)$ for Example 1, and $(\ln2)$ for Examples
  2 and 3.

\subsubsection{The stabilized relative entropic set}
  We have just seen that Examples 1 and 2 can be told apart by
  determining the unstabilized relative entropic set.  However, this
  is not really what we are interested in.  Rather, we wonder whether
  high tensor powers can be distinguished this way.

  This relates to why we used the adjective ``unstabilized'': because
  the relative entropic set usually grows (is not stable) under taking
  tensor powers. That is, for every $n,k \in \Nbb$ it holds that
  \[\tageq{inclusionrelset}
    k\cdot \res (X^{\otimes n}, Y^{\otimes n}, U^{\otimes n})
    \subset 
    \res (X^{\otimes (k\cdot n)}, Y^{\otimes (k\cdot n)},
    U^{\otimes (k\cdot n)})
  \]
  but in general the set on the right-hand side can be strictly larger
  than the set on the left.

  In view of the inclusion (\ref{eq:inclusionrelset}) we may define the
  \emph{stabilized} relative entropic set
  \[
  \sres(X, Y, U) 
  =
  \closure\left( \lim_{n \to \infty}
  \frac{1}{n}\res(X^{\otimes n}, Y^{\otimes n}, U^{\otimes
    n})\right)
  \]
This set turns out to be convex. 

\subsubsection{The stabilized relative entropic set for the examples}

  In fact, the \emph{stabilized} relative entropic set also
  differentiates between Examples 1 and 2
  \[
    \sres(X_{1},Y_{1},U_{1})\neq\sres(X_{2},Y_{2},U_{2})
  \]
  The proof of this fact follows the same lines as in Section
  \ref{s:unstabilizedexamples}, but the stabilization makes the argument
  much more technical.


We expect that the stabilized relative entropic set cannot
differentiate between Examples 2 and 3. However, there are other types of relative
entropic sets, and other Information-Optimization problems that
\emph{can} differentiate between Examples 2 and 3.

The relative entropic sets are discussed in Section \ref{s:extensions}.

\subsection{Information-Optimization problems and relative entropic sets}

In Section \ref{s:unstabilizedexamples} we used an
Information-Optimization problem to find particular points in the
(unstabilized) relative entropic set.  This is no coincidence, and the
link between stabilized Information-Optimization problems and the
stable relative entropic set can be made very explicit.  Because the
stable relative entropic set is convex, it can be completely
characterized by Information-Optimization problems and vice versa.

Such Information-Optimization problems play a very important role in information theory \cite{Yeung-First-2012}, causal inference \cite{Steudel-Information-2015}, artificial intelligence
\cite{Dijk-Informational-2013}, information decomposition \cite{Bertschinger-Quantifying-2014}, robotics \cite{Ay-Predictive-2008},
and neuroscience \cite{Friston-Free-2009}.
The techniques developed in the article allow one to address this type of problems easily and efficiently.

\subsection{The intrinsic Kolmogorov-Sinai distance}

\skippar In \cite{Gromov-Search-2012}, Gromov proposed this as a basis of an asymptotic equivalence relation on 
sequences of probability spaces. 
Even though we were greatly influenced by ideas in \cite{Gromov-Search-2012}, we found that Gromov's definition does not extend easily to configurations of probability spaces. 
Formalizing and studying a notion of asymptotic equivalence for configurations of probability spaces is the main topic of the present article.

As we mentioned at the end of Section \ref{s:intro-single}, one is
tempted to define asymptotically equivalent configurations along the
lines of Shannon's source coding theorem following
\cite{Gromov-Search-2012}.  Two configurations would be asymptotically
equivalent if there is an almost measure-preserving bijection between
subspaces of almost full measure in their high tensor powers.

However, we found this approach inconvenient.  Instead of finding an
almost measure-preserving bijection between large parts of the two spaces,
 we consider a stochastic coupling
(transportation plan, joint distribution) between a pair of spaces and
measure its deviation from being an isomorphism of probability spaces,
that is a measure-preserving bijection.  Such a measure of deviation
from being an isomorphism then leads to the notion of intrinsic
Kolmogorov-Sinai distance, and its stable version -- the asymptotic
Kolmogorov-Sinai distance, as explained in Section \ref{s:kolmogorov}.

In the case of single probability spaces we define the \emph{intrinsic
  Kolmogorov-Sinai distance} between two probability spaces $X=(\un
X,p_{X})$ and $Y=(\un Y,p_{Y})$ by
\[
\ikd(X, Y) := \inf \set{\big[\ent(Z)-\ent(X)\big] +
\big[\ent(Z)-\ent(Y)\big]}
\]
where the infimum is taken over all choices of the joint distribution
$Z=(\un X\times\un Y,p_{Z})$. Note that each of the summands is
nonnegative and vanishes if and only if the corresponding
marginalization $Z\to X$ or $Z\to Y$ is an isomorphism of probability
spaces.  In this sense the distance measures the deviation from the
existence of a measure-preserving bijection between $X$ and $Y$.

Furthermore, we define the \emph{asymptotic Kolmogorov-Sinai distance}
between two probability spaces $X$ and $Y$ by
\[
\aikd(X, Y) = \lim_{n \to \infty} \frac{1}{n} \ikd (X^{\otimes n }, Y^{\otimes n}).
\]

This definition could be generalized to configurations of probability
spaces and we will say that two configurations are asymptotically
equivalent if the asymptotic Kolmogorov-Sinai distance between them
vanishes.

\subsection{Asymptotic Equipartition Property}\label{s:intro-aep}  
Examples 1, 2, and 3 above have the property that the symmetry group
acts transitively on the support of the measure on $U_{i}$ and they
are particular instances of what we call homogeneous configurations.

In Section \ref{s:ac-aep-conf}, we show an \emph{Asymptotic
  Equipartion Property for configurations}: Theorem
\ref{p:aep-complete} states that every sequence of tensor powers of a
configuration can be approximated in the asymptotic Kolmogorov-Sinai
distance by a sequence of homogeneous configurations.

This Asymptotic Equipartition Property allows one to substitute
configurations of probability spaces by homogeneous approximations.
Homogeneous probability spaces are just uniform probability spaces,
and as a first simple consequence of the Asymptotic Equipartition
Property, the asymptotic Kolmogorov-Sinai distance between probability
spaces $X$ and $Y$ can be computed and equals
\[
\aikd(X , Y) = |\ent(X) - \ent(Y)|.
\]

Homogeneous \emph{configurations} are, unlike homogeneous probability
spaces, rather complex objects.  Nonetheless, they seem to be simpler
than arbitrary configurations of probability spaces for the types of
problems that we would like to address.

More specifically, we show in Section \ref{s:extensions} that the
optimal values in (stabilized) Information-Optimization problems only
depend on the asymptotic class of a configuration and that they are
continuous with respect to the asymptotic Kolmogorov-Sinai distance;
in many cases, the optimizers are continuous as well.  The Asymptotic
Equipartition Property implies that for the purposes of calculating
optimal values and approximate optimizers, one only needs to consider
homogeneous configurations and this can greatly simplify computations.

Summarizing, the Asymptotic Equipartition Property and the continuity
of Information-Optimization problems are important justifications for
the choice of asymptotic equivalence relation and the introduction of
the intrinsic and asymptotic Kolmogorov-Sinai distances.

\subsection{The article}
The article has the following structure. Section~\ref{s:category} is
devoted to the basic setup used throughout the text.  In Section
\ref{s:config} we explain what we mean by configurations of
probability spaces, give examples, describe simple properties and
operations. Further, in Section~\ref{s:disttypes} we generalize the
notion of probability distribution to that on configurations and
discuss the theory of types for configurations.  In
Section~\ref{s:kolmogorov} the intrinsic Kolmogorov-Sinai distance and
the asymptotic Kolmogorov-Sinai distance are introduced and some
technical tools for the estimation of Kolmogorov distance are
developed.  Section~\ref{s:lagging} contains estimates on the
distances between types.  We use these estimates in the proof of the
Asymptotic Equipartition Property for configurations in
Section~\ref{s:ac-aep-conf}.  Section \ref{s:extensions} deals with
extensions of configurations. We prove there the Extension Lemma,
which is used to show continuity of extensions and implies, in
particular, that solutions of the constrained optimization problem for
the entropies of extensions are Lipschitz-continuous with respect to
the asymptotic Kolmogorov-Sinai distance, thus they only depend on the
asymptotic classes of configurations.  In Section~\ref{s:mixtures} we
briefly discuss a special type of configurations called mixtures,
which will play an important role in the construction of tropical
probability spaces.  Finally, in Section~\ref{s:tropical} we introduce
the notion of tropical probability spaces and configurations thereof,
and list some of their properties. We will continue our study of
tropical probability spaces and configurations in subsequent articles.

Some technical and not very illuminating proofs are deferred to
Section~\ref{s:technical} Technical Proofs.  In the electronic version
one can move between the proof in the technical section and the
statement in the main text by following the link (arrow up or down).
