  In the introduction we have already emphasized the close
  relationship between relative entropic sets and
  Information-Optimization problems.  There, our definitions were
  restricted to extensions of two-fans to full configurations
  corresponding to three random variables.  We will now generalize
  these definitions, and make the relationship between relative
  en\-tro\-pic sets and Information-Optimization problems explicit.

  Further we will prove the Extension Lemma and use it to show that
  the relative entropic set associated to a full configuration depends
  continuously on the configuration.

\subsection{Information-Optimization and the relative entropic set}

  In Section \ref{s:config-restrictions-fullfull} we introduced (for
  $k \leq l$) the restriction operator 
  \[
  R_{k,l}^*: \prob\<\Lambdabf_l> 
  \to 
  \prob\<\Lambdabf_k>
  \]
  as follows.  For a minimal full configuration
  $\Ycal=\langle Y_{i}\rangle_{i=1}^{l}$ we denote by
  \[
    R_{k,l}^*\Ycal
    =
    \langle Y_{i}\rangle_{i\in \{1, \ldots, k\}}
  \]
  the restriction of $\Ycal$ to a minimal full configuration generated
  by $Y_{i}$, $i\in \{1, \dots, k\}$.

  We call a minimal configuration $\Ycal \in \Prob\<\Lambda_k>$ an
  $l$-extension of a configuration $\Xcal$ if
  \[
    R_{k,l}^* \Ycal = \Xcal
  \]
  and we denote the class of all $l$-extensions by $\Ext_l(\Xcal)$.

  Recall that for a full configuration $\Ycal \in \Prob\<\Lambda_l>$,
  we record the entropies of all its probability spaces in a vector in
  $\Rbb^{2^{\set{1,\dots,l}}\setminus\set{\emptyset}}$ that we denote by
  \[
  \Ent_*(\Ycal) := \big(\Ent(Y_I)\big)_{I \in 2^{\set{1, \dots,
        l}}\setminus\set{\emptyset}}
  \]
  The entries in this vector are all nonnegative. To simplify
  notations we set 
  \[
  \Ebb_{l}:=
  (\Rbb^{2^{\set{1,\ldots,l}}\setminus\set{\emptyset}},|\,\cdot\,|_{1})
  \]
   and denote by $\Ebb_{l}^{*}$ its dual vector-space.

  As in the introduction, we introduce the \term{unstabilized relative
    entropic set}
  \[
  \res_l(\Xcal) 
  := 
  \set{\Ent_*(\Ycal) \st \Ycal \in \Ext_l(\Xcal)}
  \]
  By the additivity property of the entropy with respect to tensor
  powers, there is the inclusion
  \[\tageq{superadditivity-res}
  \res_l(\Xcal^{\otimes m}) 
  + 
  \res_l(\Xcal^{\otimes n}) 
  \subset 
  \res_l(\Xcal^{\otimes (m + n)})
  \]
  where the sum on the left hand side is the Minkowski sum. 
  This
  allows us to define the limit
  \[
  \lim_{n\to \infty} \frac{1}{n} \res_l(\Xcal^{\otimes n}) 
  := \bigcup_{n \in \Nbb} \frac{1}{n} \res_l (\Xcal^{\otimes n})
  \]
  and we define the \term{stabilized relative entropic set} by
  \[
  \sres_l(\Xcal) 
  := 
  \closure 
    \left( 
      \lim_{n \to \infty} 
        \frac{1}{n} \res_l(\Xcal^{\otimes n}) 
    \right)
  \]
    which is a closed convex subset of $\Ebb_{l}$ by
  property~(\ref{eq:superadditivity-res}).

  For a vector $c \in \Ebb_{l}^*$, we define the
  \term{Information-Optimization problem}
  \[
  \IO_{c}(\Xcal) 
  := 
  \inf_{\Ycal \in \Ext_l(\Xcal)} 
    \big\langle\, c\, ,\, \Ent_*(\Ycal)\,\big\rangle
  = 
  \inf_{\Ycal \in \Ext_l(\Xcal)} 
    \sum_{I \subset \{1, \dots, l\}} c_I \cdot\Ent (Y_I)
  \]
  where $c_{I}$'s are the coordinates of the vector $c$ with respect
  to the basis in $\Ebb^{*}_{l}$ dual to the standard basis in
  $\Ebb_{l}$.  Note that, equation~(\ref{eq:superadditivity-res})
  implies that the sequence
  \[
  n \mapsto \IO_{c}{(\Xcal^{\otimes n})}
  \]
  is subadditive.  Hence, the limit
  \[
  \lim_{n \to \infty} \frac{1}{n} \IO_{c}(\Xcal^{\otimes n})
  \]
  always exists (but may be equal to $-\infty$). If for all $n \in \Nbb$, 
  \[
  \frac{1}{n} \IO_{c}(\Xcal^{\otimes n}) = \IO_{c}(\Xcal)
  \]
  we call the optimization problem associated
  to $c$ \term[stable optimization problem]{stable}.  In general, we
  define the stabilized optimization problem
  \[
  \IO^s_{c}(\Xcal) := 
  \lim_{n \to \infty} \frac{1}{n} \IO_{c}(\Xcal_n^{\otimes n}).
  \]
  
  As the stabilized relative entropic set is convex, it is the
  intersection of half-spaces that are defined by linear inequalities
  on entropies
  \[
  \sres_k(\Xcal) 
  = 
  \bigcap_{c \in \Ebb_{l}^{*}} 
  \set{ x \in \Ebb_{l} \, 
    \st 
    \, \langle c,x\rangle \geq \IO^s_{c}(\Xcal) }
  \]

  In other words, the stabilized information optimization problems,
  that occur so often in practice, identify supporting hyper-planes of
  the convex set.  The solution of all such linear problems determine
  the shape of the relative entropic set and vice versa.


\subsection{The entropic set and the entropic cone}
  The definitions of relative entropic sets are motivated by the more
  classical notion of the entropic
  cone, which we will briefly discuss now.  For $l \in \Nbb$, the
  entropic set is defined as
  \[
  \res_l 
  := 
  \set{ \ent_*(\Ycal) 
    \st 
    \Ycal \in \prob\<\Lambdabf_l>,\,\Ycal \text{ is minimal} 
  }
  \]
  Its closure is usually referred to as the entropic cone
  \[
  \sres_l 
  := 
  \closure(\res_l)
  \]
  Indeed, the entropic cone $\sres_l$ is a closed, convex cone in
  $\Rbb^{2^l-1}$ \cite{Yeung-First-2012}. For $l \leq 3$, the entropic
  cone $\sres_l$ is polyhedral and completely described by Shannon
  inequalities. However, for $l \geq 4$, the situation is much more
  complicated. It is known that $\sres_l$ is not polyhedral for $l
  \geq 4$ \cite{Matus-Infinitely-2007}. The shape of the entropic cone
  is not known as of the time of writing this article. It is an
  important open problem in information theory to find tight bounds on
  the entropic cone for $l \geq 4$. We hope that the techniques
  developed in this article will eventually lend itself to finding a
  useful characterization.

  In fact, the entropic cone can be considered as the relative
  entropic set of an empty configuration     
  $\emptyconfig\in\prob\<\emptycat>$, that corresponds to the
  empty diagram category $\emptycat=\Lambdabf_{0}$
  \[
  \sres_{l}=\sres_{l}(\emptyconfig)
  \]

  For a diagram category $\Gbf$ let us denote by
  $\set{\bullet}=\set{\bullet}^{\Gbf}$ the constant
  $\Gbf$-configuration of one-point spaces.  Given an $l$-extension
  $\Ycal\in\prob\<\Lambdabf_{l}>$ of $\set{\bullet}^{\Lambdabf_{k}}$ the restriction to
  the last $l-k$ terminal spaces induces a linear isomorphism
  \[\tageq{entcone-and-entset}
  \sres_{l}(\set{\bullet}^{\Lambdabf_{k}})
  \cong
  \sres_{l-k}
  \]

\subsection{Extension lemma}

  The Lipschitz continuity of relative entropic sets will follow from
  the following important proposition, which we will refer to as the
  Extension Lemma.

\begin{proposition}{p:extensionlemma}{\rm(Extension Lemma)}
  Let $k,l\in\Nbb$, $k\leq l$ and let $\Xcal, \Xcal' \in
  \prob\<\Lambdabf_k>$ be minimal full configurations.  For
  every $\Ycal \in \Ext_l \Xcal$ there exists a $\Ycal' \in \Ext_l
  \Xcal'$ such that
  \[
  \ikd(\Ycal', \Ycal ) 
  \leq 2^{l-k} 
  \ikd (\Xcal', \Xcal)
  \]
\end{proposition}

The key behind the proof of the Extension Lemma, is that there is a full configuration $\Zcal$ that extends both $\Ycal$ and the optimal coupling between $\Xcal$ and $\Xcal'$. The configuration $\Ycal'$ can be chosen to be the restriction of $\Zcal$ to the full configuration generated by $\Xcal'$ and the terminal spaces in $\Ycal$ which are not in $\Xcal$. The estimate directly follows from Shannon inequalities.  
We present details at page \pageref{p:extensionlemma.rep}.

It follows immediately from the Extension Lemma and the Lipschitz
property of the entropy function $\Ent_*$ that asymptotically
equivalent configurations have the same solutions to all
  Information-Optimization problems and, consequently, they have the
same stabilized relative entropic set.

  In fact, we have a much stronger statement.  Both the unstabilized
  and stabilized relative entropic sets have a Lipschitz dependence on
  the configuration, if the distance between sets is measured by the
  Hausdorff distance.

  Let us endow the collection of subsets of $\Ebb_{l}$ with the
  Hausdorff metric with respect to the $\ell_1$-distance. For two
  subsets $S_1, S_2$ of $\Ebb_{l}$, define the Hausdorff distance
  between them by
  \[
  \d_H \left( S_1, S_2 \right) 
  = 
  \inf\set{ \epsilon > 0 
            \st 
            S_1 \subset S_2 + B_\epsilon \text{ and } S_2 \subset
            S_1 + B_\epsilon 
          }
  \]
  where $B_\epsilon$ is the $\ell_1$-ball of size $\epsilon$ around
  the origin in $\Ebb_l$.

  In fact, at this point the Hausdorff distance is only an extended
  pseudo-metric, in the sense, that it may take infinite values and it
  may vanish on pairs of non-identical points.

  Suppose now that we are given two minimal full configurations
  $\Xcal, \Xcal' \in \prob\<\Lambdabf_k>$, and suppose a point $y \in
  \Ebb_l$ lies in the unstabilized relative
  entropic set of $\Xcal$, that is
  \[
  y \in \res_l(\Xcal )
  \]
  This means that there is an extension $\Ycal \in \Ext_l(\Xcal)$ such that 
  \[
  \ent_*(\Ycal) = y
  \]
  By the Extension Lemma, there exists a configuration $\Ycal' \in
  \Ext_l(\Xcal')$ such that
  \[
  \ikd(\Ycal, \Ycal') 
  \leq 
  2^{l-k} \ikd (\Xcal, \Xcal')
  \]
  and by the $1$-Lipschitz property of the entropy function the point
  $y' := \ent_*(\Ycal')$ is close to the point $y$, that is
  \[
  | y - y' |_{1} 
  = 
  | \ent_*(\Ycal) - \ent_*(\Ycal')|_{1} \leq 2^{l-k} \ikd(\Xcal, \Xcal')
  \]
  We have thus obtained the following corollary to the Extension Lemma.

  \begin{corollary}{p:unstabilized-rel-ent-set-Lipschitz}
    Let $k \in \Nbb$ and $\Xcal, \Xcal' \in \prob\<\Lambdabf_k>$.
    Then the Hausdorff distance between their unstabilized relative
    entropic sets satisfies the following Lipschitz estimate
    \[
    \d_H\big( \res_{l}(\Xcal), \res_l(\Xcal') \big) 
    \leq 2^{l-k} \ikd(\Xcal, \Xcal')
    \]
  \end{corollary}


  Note that in particular, the distance between unstabilized relative
  entropic sets is always finite and
  \[
  \d_H\big( 
        \res_l(\Xcal), 
        \res_l(\set{\bullet}^{\Lambdabf_{k}} )
      \big) 
  \leq 
  2^{l-k} \ikd(\Xcal, \set{\bullet}^{\Lambdabf_{k}}) 
  = 
  2^{l-k}|\ent_*(\Xcal)|_1.
  \]
  Let us denote by $\convconend$ the metric space of closed convex
  sets $K$ in $\Ebb_{l}$ such that
  \[
  d_H( K, \sres_l(\set{\bullet}^{\Lambdabf_{k}})  ) < \infty
  \]
  endowed with the Hausdorff distance.

  \begin{theorem}{p:stabilized-rel-ent-set-Lipschitz}
    Let $k \in \Nbb$ and $\Xcal, \Xcal' \in \prob\<\Lambdabf_k>$.
    Then for all $l \in \Nbb$, the Hausdorff distance between their
    stabilized relative entropic sets satisfies the Lipschitz estimate
    \[
    \d_H\big( \sres_l(\Xcal), \sres_l(\Xcal') \big) 
    \leq 
    2^{l-k} \aikd(\Xcal, \Xcal')
    \]
    In other words, the map $\sres_l$ from minimal full
    configurations in $\prob\<\Lambdabf_k>$ to $\convconend$ is
    $2^{l-k}$-Lipschitz.
\end{theorem}

  Finally, as a primer to Section \ref{s:tropical}, note that for any
  set $K \in \convconend$ the sequence
  \[
  n \mapsto \frac{1}{n} K
  \]
  converges in the Hausdorff distance to
  $\sres_l(\set{\bullet}^{\Lambdabf_{k}})$. The set $K\subset
  \Ebb_{l}$ can be viewed as a metric space itself, by just restricting
  the $\ell^1$-metric to it. The above convergence can then be
  expressed by saying that the asymptotic cone of $K$ equals
  $\sres_l(\set{\bullet}^{\Lambdabf_{k}})$ and is isomorphic to $\sres_{l-k}$.

