% This file is iccc.tex.  It contains the formatting instructions for and acts as a template for submissions to ICCC.  It borrows liberally from the AAAI and IJCAI formats and instructions.  It uses the files iccc.sty, iccc.bst and iccc.bib, the first two of which also borrow liberally from the same sources. The format has been updated for the ICCC2022 to include a new, mandatory section to be included in camera-ready manuscripts.


\documentclass[letterpaper]{article}
\usepackage{iccc}


\usepackage{times}
\usepackage{helvet}
\usepackage{courier}

% packages I added
\usepackage{graphicx}
\usepackage{hyperref}

\pdfinfo{
/Title (Transfer learning for Underrepresented Music Generation)
/Author (A. Doosti, M. Guzdial)}
% The file iccc.sty is the style file for ICCC proceedings.
%
\title{Transfer learning for Underrepresented Music Generation}

\author{Anahita Doosti \and Matthew Guzdial\\
Department of Computing Science, Alberta Machine Intelligence Institute (Amii)\\
University of Alberta\\
Edmonton, AB, Canada\\
\{doostisa, guzdial\}@ualberta.ca\\
}

\setcounter{secnumdepth}{0}

\begin{document} 
\maketitle
\begin{abstract}
\begin{quote}
%full text
% The increasing popularity of Deep Neural Networks (DNN) has led to their application to many domains, including Music Generation. However, these large DNN-based models are heavily dependent on their training dataset, which means they perform poorly on musical genres that are out-of-distribution (OOD) of that dataset. This heavily limits these systems' practical use and essentially requires the model to be retrained on a large dataset containing a musical genre in order to recreate it. In many domains, transfer learning has been effective at adapting an existing model to a new target dataset of much smaller size by training for a much shorter period. However, such an approach remains underexplored in the domain of music generation. To investigate the viability of this approach, we exploredd different genres that might represent OOD genres for a DNN-based music generator. Consequently, we identified Iranian folk music as an example of such a genre of music. This was in line with the fact that this genre has a different melodic structure from music based on western music theory principles. We then proceeded to collect a dataset of Iranian folk music and utilize it to explore different methods of transfer learning to improve the performance of MusicVAE, a large generative music model with a DNN architecture. We identify a transfer learning approach that allows us to efficiently adapt MusicVAE to the Iranian folk music dataset, which indicates a potential for the future generation of underrepresented music genres.
This paper investigates a combinational creativity approach to transfer learning to improve the performance of deep neural network-based models for music generation on out-of-distribution (OOD) genres. We identify Iranian folk music as an example of such an OOD genre for MusicVAE, a large generative music model. We find that a combinational creativity transfer learning approach can efficiently adapt MusicVAE to an Iranian folk music dataset, indicating potential for generating underrepresented music genres in the future.
\end{quote}
\end{abstract}

\section{Introduction}

%General CS audience

%Paragraph 1
% (1) Music generation is a thing (cite some kind of survey)
% (2) Traditionally, this is done with non-learning artificial intelligence methods like rules, and search (cite examples of the things you're saying)
% (3) More recently, there has been interest in large general machine learning models for music generation
% (4) These models are trained on a wide set of music, but notably this is not an exhaustive set. 
% (5) Identify a lack or problem in this area: These big models don't do a good job on music genres outside their training data
%The automatic generation of music by means of computer systems can be traced back to 1957, when Newman Guttman generated a 17 second long melody called ``The Silver Scale" at the Bell Laboratories. The first score was composed by a computer in the same year, called "Illiac Suite" at the University of Illinois Urbana-Champaign (UIUC) \cite{briot2017deepSurvey}. Most of the traditional automatic music generation systems were algorithmic systems that ultimately relied heavily on expert musical knowledge, rule/grammar-based \cite{grammar_ex1} algorithms being an example \cite{herremans2017functional}. Stochastic models like Markov chains \cite{mccormack1996grammar} are also a traditionally common method. With the reemergence Neural Networks (NNs) in recent years, there has been much interest in revisiting the task of music generation using such large machine learning models. The hope is that their superior computational power can learn long-term and high-order dependencies and also eliminate the need for expert domain knowledge \cite{briot2017deepSurvey}. Characteristically, these models are trained on a very large amount of data. Yet realistically no matter how large, these datasets cannot be exhaustive. Datasets are usually limited to the types of music that are more popular. Particularly in case of working with symbolic music data, musical performances need to be transcribed in a particular format. Therefore any large symbolic dataset is usually gathered from online sources, which naturally results in popular music genres being more represented. Moreover, new music is constantly being created and new genres of music forming. Consequently this leads to a key weak point; these models are not as successful in generating music for genres outside their learned distribution and this distribution is closely based on their training datasets.

Automated music generation has a long history \cite{briot2017deepSurvey}. In recent years, large-scale neural network models for music generation have arisen, trained on massive datasets and requiring significant computation \cite{CIVIT2022118190}. 
While these approaches have proven successful at replicating genres of music like those in their training sets, due to the nature of large-scale neural network models we expect this may not prove true for dissimilar genres. 
Specifically, we hypothesize that these large scale models will perform poorly for out-of-distribution (OOD) genres of music, those representing underrepresented or less globally popular types of music. 
We therefore conducted a study on one such large-scale neural network model to understand (1) how it performed on OOD music genres, and (2) how we might best adapt the model to an OOD music genre.

%Paragraph 2 - Overview of the field
% (1) In the field of music generation, there are a few high-level strategies that current systems employ.
% (2-4) One sentence each mentioning something
% (5) But-- these approaches don't look into X: Transfer learning to adapt these models to uncovered types of music/music genre. 
%In recent years, there are a few prominent approaches to music generation, most of which involve the use of neural networks \cite{CIVIT2022118190}. For instance, sequence-based approaches are popular in this field due to their ability to learn long-term dependencies in musical pieces. Multiple studies have combined sequence-based models such as Long short-term memory (LSTM) Recurrent Neural Networks with autoencoders and achieved good results. Alternatively, Generative Adversarial Networks (GAN) have been employed to generate novel music. These approaches have mainly attempted to create a general generative model, which is not customizable and is highly dependent on the composition of the traininng dataset. These models typically are also trained from scratch. However, given  the data imbalance across different genres, approaches like transfer learning that can adapt knowledge from one domain to another might be useful. Despite this, few examples of prior work explore this approach.

% Paragraph 3 - Your research (VERY high level)
% (1) Studied this particular model
% (2) Found that did not cover specific music genres, particularly Persian music and video game music
% (3) Studied the impact of several different transfer learning methods/approaches to attempt to solve this problem
% (4) What worked/what didn't
For this paper, we focus on Google Magenta's MusicVAE model \cite{musicvae2018}. We lack the space for a full discussion of the model, but direct interested readers to the original paper. This hierarchical variational autoencoder is trained on an enormous dataset of roughly 1.5 million unique MIDI files collected from the web. While its exact dataset was not made public, online repositories of MIDI files are typically made up of fan-made annotations of popular songs. Thus, automatic and indiscriminate data collection would result in an unbalanced dataset in terms of genre diversity. This is due to the fact that popular chart-topping songs are much more likely to be annotated in the MIDI format. 
The training requirements for MusicVAE are a problem when it comes to generating underrepresented music, like experimental music or music from particular cultures with distinct musical traditions.
These genres of music are unlikely to have the massive datasets needed to train models like MusicVAE. 
Even if such datasets existed, new musical genres are constantly being invented, meaning we could never use this approach to generate all underrepresented genres of music.
%Therefore, we could assume that MusicVAE will not be good at generating more specific and less mainstream genres of music. Exploring several different genres we discovered that this is indeed true for Iranian folk music which was picked due to it being very different than popular western music in structure and due to the author's familiarity with it. Subsequently we surveyed several different methods of transfer learning to adapt MusicVAE to this particular genre using a minimal amount of new data, such as fine-tuning and Conceptual Expansion Monte Carlo Tree Search(CE-MCTS). \cite{Mahajan2023} Based on the accuracy of the model on reconstructing musical samples, we observed that CE-MCTS offers an improvement in quality especially in test accuracy. Finetuning all layers, performed only slightly better than MusicVAE itself and other methods lead to a poor performance.

% 
If we want to be able to generate underrepresented music, one option outside of training MusicVAE from scratch is transfer learning \cite{tan2018survey}. Transfer learning refers to the collection of approaches that can adapt knowledge from a model pre-trained on some source dataset (i.e., popular MIDI files) to a target domain with limited data (i.e., underrepresented music MIDI files). 
However, these approaches tend to require significant similarity between the source and target domains, which may not hold true for popular and underrepresented music genres \cite{marchetti2021convolutional}. 
Combinational creativity, also sometimes combinatorial creativity, is a type of creative problem solving in which two conceptual spaces are combined to represent a third or new conceptual space \cite{boden2009computer}. 
While different musical genres may vary in terms of their local features (e.g., melodies), they are all still music. 
As such, we hypothesized that a combinational creativity-inspired transfer learning approach may be able to outperform traditional transfer learning approaches at the task of adapting MusicVAE to an underrepresented genre of music \cite{Mahajan2023}.


% Paragraph 4 - Research Questions/Hypotheses
%Break down your research 1-3 RQs 
%Throughout the span of this thesis, pursuing our aforementioned motivations, we were interested in examining the following research questions:
%\begin{enumerate}
%    \item How good are large Deep Learning music generation models at reconstructing music of different genres?
%    \item Is it possible to specialize such a model for a specific out of distribution (OOD) genre?
%    \item More specifically, is it possible to improve MusicVAE's performance on Iranian music through transfer learning methods using a small amount of data?
%    \item What is the effects of the melody extraction algorithm used on the model's performance?
%\end{enumerate}

%Paragraph 5 - Contributions
% What answers do you have for these research questions
In this paper, we explore the application of CE-MCTS \cite{Mahajan2023}, a combinational creativity-based transfer learning approach to adapt MusicVAE to an underrepresented music genre.
% While there are many deep neural network (DNN) models like MusicVAE for music generation, to the best of our knowledge we are the first to apply transfer learning to a DNN model for music generation, using an approach other than standard finetuning.
While there are many deep neural network (DNN) models like MusicVAE for music generation, applying transfer learning to a DNN model for music generation remains under-explored \cite{svegliato2016deep,marchetti2021convolutional}.
In addition, while combinational-creativity-based transfer learning approaches have been applied to many domains including image classification \cite{banerjee2021combinets} and financial health prediction \cite{Mahajan2023}, they have never been applied to the music generation domain.
In the remainder of this paper, we first demonstrate an experiment to identify an out-of-distribution (OOD) music genre for MusicVAE. 
We then introduce CE-MCTS and a number of more standard transfer learning baselines. 
Finally, we demonstrate their performance in terms of reconstruction accuracy for an OOD music dataset and present a short discussion on their music generation performance. 

\subsection{Related Work}

There have been many recent applications of deep neural networks (DNN) to music generation \cite{CIVIT2022118190}. For instance, sequence-based approaches are popular in this field due to their ability to learn long-term dependencies in musical pieces. Multiple studies have combined sequence-based models such as Long Short-term Memory (LSTM) Recurrent Neural Networks (RNN) with autoencoders and achieved good results \cite{perfrnn2017}. Alternatively, Generative Adversarial Networks (GAN) have been employed to generate novel music \cite{yang2017midinet}. 
These models typically are also trained from scratch. However, given  the data imbalance across different genres, approaches like transfer learning that can adapt knowledge from one domain to another might be useful. One such example attempts to test a pre-trained Generative Adversarial Networks (BinaryMuseGAN) with traditional Scottish music and improves its performance using finetuning \cite{marchetti2021convolutional}. To the best of our knowledge, finetuning is the only transfer learning approach that has been applied to DNN music generation \cite{svegliato2016deep}. 
We use it as a baseline.

In regards to Iranian (Persian) traditional or folk music, the prior work focuses on generation of music via traditional non-machine learning approaches and/or training from scratch. In \cite{SaharPhD}, the author uses a combination of evolutionary algorithms, Boltzmann machine models and cellular automata to generate music. They evaluate their work by the use of surveys targeted to both general and professional audiences. Alternatively, researchers have employed RNNs trained on a dataset of traditional Iranian music to generate music \cite{rnn-persian}. We note our goal is not to generate Iranian music specifically, but to explore the best ways to adapt large DNN music generation models to OOD genres like Iranian music. 

\subsection{Genre Analysis}
MusicVAE as a music generation model boasts a very impressive performance. Specifically, it achieves 95.1\% over its test dataset. However, our hypothesis was that MusicVAE would do poorly for OOD music. 

To examine this question, we collected four experimental datasets of 10 songs each. These were small in size as we only required a general approximation of whether the genre was out-of-distribution for MusicVAE.
We selected these songs according to two criteria: (1) if they were published after MusicVAE or were otherwise unlikely to be included in the original dataset \cite{musicvae2018} and (2) if their genres were distinct from a melodic standpoint. Melodies can differ in many ways such as contour, range and scale and these characteristics are different across different genres \cite{melodytheory}.

Our four datasets are as follows:
\begin{itemize}
    \item \textbf{Synth pop}, songs from a 2021 Netflix special, Inside by Bo Burnham, which musically fall into the synth pop category. This dataset serves as a comparison point, since we expected MusicVAE to perform well on this genre.
    \item \textbf{Iranian folk}, arose from a region with a long-standing history of composing music with independent roots from western music. As such, we anticipated this would be the most challenging for MusicVAE. 
    \item \textbf{Video game}, consists of Nintendo Entertainment System video game music. These songs have limited polyphony as only 3 notes can be played on the NES at once. 
    \item \textbf{Horror scores}, were designed to build suspense and create a sense of foreboding. Musically this genre frequently uses dissonant notes or chords, atonality (not having a clear scale), sudden changes of tempo, and other effects to induce a sense of eeriness and dread. 
\end{itemize}

\begin{table}[tb]
    \centering
    \begin{tabular}{|c|c|}
    \hline
    Dataset & Accuracy(\%) \\
    \hline\hline
    Synth pop & 95.83 \\
    \hline
    Iranian folk & 43.75 \\
    \hline
    Video game & 84.38 \\
    \hline
    Horror score & 87.92 \\
    \hline
    \end{tabular}
    \caption{MusicVAE accuracy on 4 datasets of different genres}
    \label{table:4Genre}
\end{table}

In these experiments, we fed melody sequences extracted from the songs into the pre-trained MusicVAE and report the reconstruction accuracy.
We include the results of our analysis in Table \ref{table:4Genre}.
As we expected, the model performed best on the first dataset.
The 95.83\% accuracy is in line with what was reported for the test accuracy on the original MusicVAE dataset. 
Predictably, the accuracy is noticeably lower for the other three datasets, with the Iranian dataset standing at a mere 43\%, a major drop in performance compared to the rest. 
As such, we focused on Iranian folk music for the remainder of our study.
%On the other hand, when it came to the other three datasets, the model often would only produce the very first note of a melodic sequence followed by a lengthy silence. Incorrect notes were also a frequent observation. These shortcomings were very pronounced with the Iranian music dataset.  

\section{Iranian Folk Music Dataset} \label{data}   
We gathered a new dataset of Iranian folk MIDI files in order to evaluate the possibility of adapting MusicVAE to this out-of-distribution (OOD) genre.
This dataset consists of 100 MIDI files from both Farsi-speaking websites and from \url{musescore.com} which is a free sheet music sharing website. These files contain different instruments and varying levels of polyphony. 
We collected 100 MIDI files as this is in line with the target genre dataset size for prior finetuning-based transfer learning approaches with music generation models \cite{svegliato2016deep,marchetti2021convolutional}. 
However, we anticipate that CE-MCTS could perform well with fewer samples \cite{Mahajan2023}.
During our experiments we used a five-fold cross validation, meaning we split the data into five train-test splits (80 songs for training, 20 for testing), which helped ensure that we did not just get a ``lucky'' train-test split.

\section{Conceptual Expansion Monte Carlo Tree Search (CE-MCTS)}

We hypothesized that a combinational creativity-based transfer learning approach could most effectively adapt MusicVAE to an OOD genre. 
While there have been several prior examples of combinational creativity-based transfer learning approaches \cite{banerjee2021combinets,singamsetti2021conceptual}, we chose Conceptual Expansion Monte Carlo Tree Search (CE-MCTS)~\cite{Mahajan2023}. 
None of these combinational creativity-based transfer learning approaches have been applied to music data, but CE-MCTS demonstrated the ability to adapt to the behaviours of distinct groups of humans across problem domains. 
As such, we anticipated it would be the best for adapting to other types of human expression, like music.

Here we briefly describe CE-MCTS and how we adapted it in this work. However, for a full description of the approach we direct interested readers to \cite{Mahajan2023}.
The ``Conceptual Expansion'' (CE) in the name refers to the fact that we are searching over combinations \cite{banerjee2021combinets}. 
In this case, different combinations of the learned features from the original MusicVAE model. 
While it may seem unintuitive to combine features from the same model, this can allow us to approximate unseen features.
This is equivalent to combining different musical patterns in the source domain (i.e., popular MIDI music) to approximate patterns in the target domain (i.e., Iranian folk music).
We then employ Monte Carlo Tree Search (MCTS) to search over the space of these possible combinations. 
As is typical with MCTS, we build up a tree to search through this space. 
The root node represents the original trained MusicVAE model (no combinations) and every subsequent node represents a different set of feature combinations, with closer nodes representing more similar combinations. 

%As with standard MCTS, the algorithm starts by traversing the tree based on a given policy. This first step is called Selection. In our case, the algorithm uses an $\epsilon$-greedy policy. This means at each step a random number in the range $[0, 1]$ is generated. If this number is greater that $\epsilon$, the algorithm traverses to the best child node (e.g. the one with the highest fitness), otherwise it chooses to explore the space by randomly creating and adding a new node. The number of new nodes we explore is adjusted by a parameter called the rollout length. Adding a new node to the tree is a step called Expansion. After reaching the end of a rollout a portion of each new child's fitness is backpropagated to its parent using a discount factor. This process is repeated for a number of iterations and and each rollout starts from the root node.

%To create a child node, Mahajan and Guzdial employed four functions that create new models by manipulating the $f$ and $\alpha$ values of the parent node. The functions are as follows:
%\begin{itemize}
%    \item Function 1: Multiply a randomly selected index of a randomly selected $\alpha$ by a random scalar in the range $[-2, 2]$.
%    \item Function 2: Multiply a randomly selected $\alpha$ by a random scalar in the range $[-2, 2]$.
%    \item Function 3: Swap the values of two randomly selected $\alpha$ and $f$.
%    \item Function 4: Add a randomly selected $\alpha$ and $f$ values to the CE approximation.
%\end{itemize}

For our implementation of CE-MCTS we largely employed the same setup from the original paper \cite{Mahajan2023}. 
However, we made a number of changes for this domain.
For the fitness, we used the reconstruction accuracy of the training split (80 songs from Iranian folk music dataset).
We ran 10 iterations, each with 10 rollouts ($L=5$) and $\epsilon=0.5$.
We did this to bias the search towards exploration near the original MusicVAE model as we did not want to risk catastrophic forgetting, in which a model loses useful features.
Ultimately, we output the three best performing models according to the fitness and report their average performance over the test split (20 songs). 
The strategy for the final model selection varies based by domain.


%This method was introduce to specialize a model trained on general data for an individual's behavior and its results surpassed other transfer learning methods on two different tasks. In this method the search space is represented using Conceptual Expansion (CE), and searched through using a Monte Carlo tree search, which results in two properties; (1) The search space is unbounded and (2) we can take larger steps while exploring the space, unlike normal finetuning. This is ideal for our task and working on discrete data. Also, this approach relies on a smaller amount of data on the secondary task than is usually common for transfer learning because the researchers developed it to specialize to one individual at a time. This parallels our problem, given the size of our Iranian music dataset is proportionally very small in comparison to MusicVAE's original training data.

%To apply this approach to our task, we used the same four default neighbor functions. Each node in the Monte Carlo Tree search is a MusicVAE model, with the root being loaded with pre-trained weights and each subsequent child being a copy with its weights altered randomly by one of the neighbor functions. At each iteration there are 10 rollouts and at each rollout there is an $\epsilon=0.5$ chance of choosing either exploration or exploitation. If exploration is chosen the rollout has a length of 5 nodes. The fitness used for each node is accuracy and at the end of each exploration each child node's fitness is added to its parent's accuracy by a discount factor of 0.3. Since each node in the tree is a distinct instance of a MusicVAE network, this approach uses up a lot of memory as it builds out the tree. Due to our limitations, we set the root to the best node found through exploitation and pruned the tree, keeping only the subtree of the new node. At the end, we kept the top three nodes with the highest fitness/accuracy.

\section{Transfer Learning Baselines} \label{tlmethod}

We hypothesized that CE-MCTS would outperform standard transfer learning approaches at adapting MusicVAE to our Iranian folk music dataset. Here we introduce the transfer learning baselines we used for comparison purposes. Other transfer learning approaches were not appropriate as we lacked access to MusicVAE's original training dataset.

\begin{itemize}
    \item \textbf{Non-transfer}, where we train a randomly initialized MusicVAE on the Iranian music dataset alone. This represents the standard approach to this problem without transfer learning. We did not expect this to work given the limited amount of training data.
    \item \textbf{Zero-shot}, which uses the pre-trained weights of MusicVAE with no additional training on the Iranian music dataset. We know MusicVAE does poorly when reconstructing 10 Iranian folk songs, but this won't necessarily hold for our larger 100 song dataset.
    \item \textbf{Finetuning (all)}, in which we use finetuning, a traditional network-based transfer learning approach that has been applied in prior music generation DNN transfer learning work \cite{tan2018survey,svegliato2016deep,marchetti2021convolutional}. In this baseline, we applied finetuning by continuing to train MusicVAE on a train split of our dataset until convergence. The (all) indicates retraining all of MusicVAE's layers. This is unusual as it can lead to catastrophic forgetting, where useful features are lost in the adaptation process.
    \item \textbf{Finetuning (last)}, which is the same as Finetuning (all) except that we freeze the weights of all but the last layer. This is the more typical approach when applying finetuning as it assumes that the earlier layers contain useful features (e.g., musical patterns) and we can just adapt the last layer to apply these patterns more appropriately for our target dataset.
\end{itemize}

We also developed a knowledge distillation approach called student-teacher learning \cite{student-teacher}. In this approach we trained a MusicVAE (student network) on Iranian music, through a combination of its loss and the loss of another MusicVAE with pre-trained weights (teacher network). This model proved worse than all other baselines, thus we do not include its results.

%In the following sections, we go over each method in more detail. These methods were chosen because they allow us to use available knowledge in the form of MusicVAE pre-trained weights and adapt it to the domain of Iranian folk music by using a small dataset, namely the one discussed in the previous section. 

%There are many types of transfer learning, such as instance-based, mapping-based, network-based and adversarial-based. Instance-based learning would require re-weighting samples from the source domain to fit the target domain and mapping based learning would require learning a mapping from both the domains to a third latent space. However, we do not have access to the source domain data and even if we did, these methods require a similar amount of target and source data. For many genres of music collecting the require amount of data in itself would be very challenging. Moreover these methods do not decrease the training time nor the resources needed for training. Similarly, training adversarially would require a significantly large amount of data, which we lack \cite{tan2018survey}. 


%Alternatively, we focus on network-based methods.  These methods are predicated on the assumption that much like the human brain, the initial layers of a deep neural network act as feature extractors of increasing complexity and that these features are versatile and not limited to a source domain. This would mean by altering the last layers of a network we can adapt it to a target domain \cite{tan2018survey}. In our research, this would suggest that MusicVAE originally extracted universal features that appear in any genre of music during the initial training process. 

% mention student-teacher
%We also employed a knowledge distillation approach called student-teacher learning \cite{student-teacher}. In this approach we trained a MusicVAE (student network) on Iranian music, by using a combination of its loss and the loss of another MusicVAE with pre-trained weights (teacher network). Training this model proved very time consuming and we did not achieve desirable outcomes through it. Thus we do not include its results.




%\subsection{Finetuning All Layers}
%Finetuning is a network-based transfer learning method. This means instead of training MusicVAE from its initial (random) weights, we used the pre-trained MusicVAE weights provided for the 2-bar melody setup and then proceeded to train the full network using the Iranian music dataset. This could allow us to make use of previously learned knowledge in form of pre-trained weights and alter them through transfer learning to fit our new target domain/genre using minimal data and training time. In this baseline, we chose to not freeze any of the model's weights. However, we predicted that this would likely result in ``catastrophic forgetting", meaning that by allowing the entire model to be finetuned, the model loses or forgets essential knowledge that it had previously learned. In this case, it would likely lose the ``universal features'' that it learned to reconstruct the original training dataset.

%\subsection{Finetuning the Last Layer}
%In this baseline, we again finetuned the network using the pre-trained MusicVAE weights except this time we froze all weights except the last layer. The intuition behind this is that we would like to keep the low level, ``universal''feature extraction in the initial layers intact, while changing the last layer which is responsible for producing the final output based on these extracted features. We hoped that this baseline would be less susceptible to ``catastrophic forgetting". This approach to finetuning is more commonplace such as in \cite{kunze2017transfer}.

%There could be an argument to the contrary, if the datasets are so drastically different that the features extracted from the initial dataset are not useful for reconstructing the target data. In our case, this question boiled down to the musical similarity of Iranian and western music and which we cannot answer without expert knowledge. We anticipated that our experimental results could help answer this question empirically. 

% \subsection{Student-Teacher}

%\subsection{Non-transfer Baseline}
%In this baseline, we trained the MusicVAE model without using the pre-trained weights. This was done without any transfer learning methods and explores the possibility of building the music generation model entirely based on our OOD dataset. However, we expected that this method would prove to be ineffective due to our significantly smaller dataset of 100 MIDI files, as opposed to the original $\approx$ 1.5 million files used to train MusicVAE.
\section{Evaluation}

For our evaluation, we compared the reconstruction accuracy of our approach and baselines. 
Each model was fed our test set and we measured the percentage of correctly reconstructed notes.
Clearly, this method of evaluation does not assess the ability of the model to actually generate new musical sequences, which is the main objective of a music generation model. 
However, this approach allows us to run an initial quantitative evaluation. 
While there are objective metrics employed by other researchers, they are not consistently defined, making it difficult to compare outputs across different generation systems. Furthermore, there is no correlation between qualitative and quantitative metrics of evaluation, making it difficult to draw implications. We expect this to be even more difficult for a genre like Iranian music or other regional genres. Therefore, correctly evaluating the quality of generation would require a human subject study with experts in the target genre (i.e. Iranian folk music).~\cite{ji2020comprehensive} 
We leave this to future work, but include a small case study below.

\section{Results}

\begin{table}[tb]
    \centering
    \scalebox{0.75}{
    \begin{tabular}{|c||c|c|c|c|c|c|}
        \hline
        Approach & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Average  \\
        \hline\hline
        Non-transfer & 68.75 & 68.75 & 68.75 & 68.75 & 68.75 & 68.75 \\
        
        \hline
        Zero-shot & 93.75 & 90.62 & 84.37 & 87.50 & 87.50 & 88.75 \\
        \hline
        Finetune (all) & 87.50 & 84.37 & 78.12 & 78.12 & 75.00 & 80.62  \\
        \hline
        Finetune (last) & 96.87 & 90.62 & 93.75 & \textbf{100} & \textbf{100} & 96.25 \\
        \hline
        % Student-teacher & f1 & f2 & f3 & f4 & f5 & avg \\
        % \hline
        CE-MCTS & \textbf{98.96} & \textbf{94.80} & \textbf{98.97} & 94.84 & \textbf{100} & \textbf{97.52} \\
        \hline
    \end{tabular}
    }
    \caption{Training reconstruction accuracy of each approach on each fold}
    \label{table:train_res}
\end{table}

\begin{table}[h]
    \centering
    \scalebox{0.75}{
    \begin{tabular}{|c||c|c|c|c|c|c|}
        \hline
        Approach & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Average  \\
        \hline\hline
        Non-transfer & 37.50 & 37.00 & 37.50 & 6.25 & 53.12 & 34.27 \\
        \hline
        Zero-shot & 90.62 & 87.50 & 75.00 & 37.50 & 90.62 & 76.24 \\
        \hline
        Finetune (all) & 81.25 & 68.75 & 25.00 & 34.37 & 65.62 & 55.00 \\
        \hline
        Finetune (last) & \textbf{96.87} & 96.87 & 65.62 & 40.62 & 93.75 & 78.75\\
        \hline
        % Student-teacher & f1 & f2 & f3 & f4 & f5 & avg \\
        % \hline
        CE-MCTS & 93.75 & \textbf{97.9} & \textbf{83.34} & \textbf{51.07} & \textbf{93.77} & \textbf{83.97}\\
        \hline
    \end{tabular}
    }
    \caption{Test reconstruction accuracy of each approach on each fold}
    \label{table:test_res}
\end{table}

Tables \ref{table:train_res} and \ref{table:test_res} contain our training and test reconstruction accuracy results, respectively. 
Overall we can observe that CE-MCTS consistently outperforms other methods in both training and test accuracy. Although Finetuning (last) performs similarly on average during training, CE-MCTS is better at reconstructing the test data. 

As we expected, Finetuning (all) produces inferior results to Finetuning (last). In fact, it seems that the original pre-trained MusicVAE (Zero-shot) outperforms this method. This is likely due to catastrophic forgetting. As for the non-transfer method, it is not surprising that the small quantity of data available is unable to effectively train the network. The initial dataset size used to train MusicVAE is roughly 15,000 times larger than our dataset.

CE-MCTS outperforms both the pre-trained MusicVAE and last layer finetuning on test accuracy. Therefore we can deduce that by recombining the features in the earlier layers, CE-MCTS is able to create better features for the target dataset. Based on the performance of the Zero-shot and Finetuning (last) baselines, we can infer that the features present in original model are not sufficient to represent Iranian folk music.
This suggests we can usefully approximate Iranian folk music features via a combination of features from popular western music.

\section{Case Study} \label{qualitative}
In this section, we provide a brief qualitative analysis of some of the models in terms of music generation instead of reconstruction. We include figures with representative outputs from the three best performing approaches. These examples were chosen by the first author, who has expertise in Iranian music, to be generally representative of the characteristics of the outputs from these approaches. This is obviously highly subjective and susceptible to confirmation bias. A study with expert participants is needed to make reliable assertions about the quality of generation, but we leave this to future work.

In each corresponding figure, the x-axis represents time in seconds, limited to 4 seconds which is the length of all 2-bar outputs by MusicVAE. The y-axis represent the pitch for the notes in the MIDI format which ranges between 0 and 127. Each red rectangle in the figure represents a continuous note.

Figure \ref{fig:musicvae} represents a typical melody generated by the Zero-shot pre-trained MusicVAE. The notes in this melody sound harmonious and follow a somewhat cohesive progression. They also gradually move from a higher pitch to a lower one, spanning somewhat evenly across the melodic range (distance between the highest and the lowest pitch). 
Figures \ref{fig:finetunelast} and \ref{fig:cemcts}, were generated using the Finetuning (last) and CE-MCTS approaches, respectively. In the first author's subjective opinion, samples generated by these two models sound more similar to, and evocative of, the type of melodies present in Iranian folk music. This is hard to qualify but here we point out a number of characteristics commonly seen in traditional and folk Iranian (Persian) music according to \cite{farhat2004dastgah} and \cite{iranian_classical_music}. 
\begin{itemize}
    \item Melodies have a narrow register (pitch range).
    \item Melodic movement is often achieved with conjunct steps.
    \item There is an emphasis on cadence, symmetry, and repetition of musical motifs at varying pitches.
    \item Rhythmic patterns are generally kept uncomplicated and rhythmic changes are infrequent.
    \item The tempo is often fast, with dense ornamentation. Similar to this, it is common to see repetitive and rapid use of the same note/pitch.
\end{itemize}
As shown in both figures, the register is more limited locally and patterns that repeat the same note appear. 
\begin{figure}[h]
    \centering
    \includegraphics[keepaspectratio=true, width=0.4\textwidth]{img/melody_plot/musicvae2.png}
    % \caption[A supporting figure] 
    \caption{Visualization of a melody generated by the pre-trained MusicVAE model}
    \label{fig:musicvae}
\end{figure}

\begin{figure}[tbh]
    \centering
    \includegraphics[keepaspectratio=true, width=0.4\textwidth]{img/melody_plot/finetune_last.png}
    \caption{Visualization of a melody generated by the model finetuning the last layer}
    \label{fig:finetunelast}
\end{figure}

\begin{figure}[tbh]
    \centering
    \includegraphics[keepaspectratio=true, width=0.4\textwidth]{img/melody_plot/mcts.png}
    \caption{Visualization of a melody generated by the CE-MCTS model}
    \label{fig:cemcts}
\end{figure}

\section{Conclusions}
In this paper, we investigated how MusicVAE, a music generation model, can be adapted to OOD music.
We identified that MusicVAE in particular struggles with Iranian folk music. We then explored different transfer learning methods in order to improve MusicVAE's performance on a newly collected Iranian folk music dataset. Based on our results, we observed that CE-MCTS, a combinational creativity-based transfer learning approach, is better able to produce reconstructions of this genre of music. This suggests that we can successfully adapt these large music generation models for underrepresented genres of music, and that combinational creativity can be an especially helpful tool in this task. 


%\appendix{\LaTeX{} and Word Style Files}\label{stylefiles}

%The \LaTeX{} and Word style files are available on the ICCC-13
%website, {\tt http://computationalcreativity.net/iccc2013/}.
%These style files implement the formatting instructions in this
%document.

%The \LaTeX{} files are {\tt iccc.sty} and {\tt iccc.tex}, and
%the Bib\TeX{} files are {\tt iccc.bst} and {\tt iccc.bib}. The
%\LaTeX{} style file is for version 2e of \LaTeX{}, and the Bib\TeX{}
%style file is for version 0.99c of Bib\TeX{} ({\em not} version
%0.98i).

%The Microsoft Word style file consists of a single template file, {\tt
%iccc.dot}. 

%These Microsoft Word and \LaTeX{} files contain the source of the
%present document and may serve as a formatting sample.  


\bibliographystyle{iccc}
\bibliography{iccc}


\end{document}
