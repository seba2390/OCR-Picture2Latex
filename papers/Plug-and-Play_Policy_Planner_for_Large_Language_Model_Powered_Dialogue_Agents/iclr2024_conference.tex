
\documentclass{article} % For LaTeX2e
\usepackage{iclr2024_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage[colorlinks=true,linkcolor=black,anchorcolor=black,citecolor=black,filecolor=black,menucolor=black,runcolor=black,urlcolor=black]{hyperref}
\usepackage{url}
\usepackage{booktabs,multirow}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{xcolor}
\newcommand{\cmark}{\color{green}\ding{51}}%
\newcommand{\xmark}{\color{red}\ding{55}}%


\usepackage{pgfplots}
\usetikzlibrary{pgfplots.groupplots,intersections,pgfplots.fillbetween}
\pgfplotsset{compat=1.3}
\usepackage{scalefnt} 
\definecolor{color1}{RGB}{145,30,180}
\definecolor{color2}{RGB}{245,130,48}
\definecolor{color3}{RGB}{230,25,75}

\title{Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents}

\iclrfinalcopy
% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Yang Deng$^1$ \quad Wenxuan Zhang$^2$ \quad Wai Lam$^3$ \quad See-Kiong Ng$^1$ \quad Tat-Seng Chua$^1$\\
$^1$National University of Singapore \quad $^2$DAMO Academy, Alibaba Group \\
$^3$ The Chinese University of Hong Kong\\
\texttt{dengyang17dydy@gmail.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\isak}[1]{[\textcolor{orange}{Isaac: #1}]}
\newcommand{\dy}[1]{[\textcolor{red}{dy: #1}]}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. 
Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. 
In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. 
Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. 
In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. 
In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. 
Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues. 
\end{abstract}


\section{Introduction}

Large language models (LLMs) powered dialogue agents (\textit{e.g.}, ChatGPT~\citep{instructgpt}, Vicuna~\citep{vicuna2023}, LLaMA2-Chat~\citep{llama2}, etc) have demonstrated exceptional proficiency in context understanding and response generation in various dialogue problems \citep{mmmeval-chatgpt,zhang2023sgptod,chatgpt-emo-dial}.  
However, as LLMs are trained to passively follow users' instructions, dialogue agents built upon them typically prioritize accommodating users' intention. 
Therefore, LLM-powered dialogue agents often face challenges in handling proactive dialogue problems that require the dialogue agent to strategically take the initiative to steer the conversation towards an anticipated goal~\citep{proactive-survey}, such as negotiation~\citep{negotiate-survey}, emotional support~\citep{esconv}, and tutoring~\citep{eacl23-tutor}. 

In such scenarios, the key is to improve the capability of LLM-powered dialogue agents in dialogue policy planning, which refers to the process of deciding what actions the dialogue agent should take to effectively achieve specific goals during the dynamic interactions with the user. 
In the pre-LLM era, researchers mainly employ corpus-based learning approaches to conduct the dialogue policy planning via dialogue act prediction~\citep{iclr20-noncollab,iclr21-negotiate-strategy,emnlp22-esc,eacl23-strategy-tutor}. However, such approaches rely heavily on static human-annotated dialogues and fail to optimize the long-term goal of the conversation. 
With the advent of LLMs, it further becomes unrealistic and costly to fine-tune the whole dialogue systems for every specific application. 
To this end, as shown in Figure \ref{fig:review}(a), recent works investigate prompt-based policy planning methods that prompt a frozen actor LLM to either conduct self-thinking of strategy planning for each turn \citep{acl23-askanexpert,llm-proactive} or generate AI feedback given the whole dialogue history to iteratively improve the dialogue policy planning for a certain case \citep{negotiate-selfplay,gdpzero}. 




Despite their effectiveness in improving the dialogue policy planning, there are several challenges that remain to be tackled. 
1) LLMs fall short of planning effective dialogue policy with zero-shot or few-shot prompting schemes \citep{llm-proactive}. Therefore, the improvement of accomplishing the goal will be limited by the planning capability of the frozen actor LLM. 
2) Existing approaches based on iterative refinement \citep{negotiate-selfplay,gdpzero} lack of transferability, as multiple rounds of self-play dialogue simulations are required for every new-coming case to plan a satisfactory strategy for it, which is impractical in real-world applications. 
3) Existing studies typically evaluate the performance of  dialogue agents in terms of turn-level response quality measurements based on fixed reference responses. However, these evaluation protocols fail to automatically assess the policy planning capability of the dialogue agent, which is determined by the effectiveness and efficiency of the goal achievement in multi-turn conversations. 


\begin{wrapfigure}{r}{6cm}
\setlength{\abovecaptionskip}{0pt}   
%\setlength{\belowcaptionskip}{2pt}
\vspace{-0.8cm}
\centering
\includegraphics[width=0.4\textwidth]{fig/review.pdf}
\caption{The architectures of two types of LLM-based proactive dialogue systems.} 
\vspace{-0.3cm}
\label{fig:review}
\end{wrapfigure}


To tackle these challenges, we introduce a novel dialogue policy planning paradigm to strategize LLMs with a tunable language model plug-in, named Plug-and-Play Dialogue Policy Planner (PPDPP). 
As shown in Figure \ref{fig:review}(b), PPDPP acts as the policy agent to predict the dialogue strategy at the next turn for the dialogue agent, which can be first supervisedly fine-tuned with available human-annotated corpora. 
Then, we employ the self-play paradigm to prompt two LLMs (an assistant and a user) with various case background information to perform the role-playing conversation that simulates the dynamic environment of multi-turn interactions between the dialogue agent and the real user. 
For each case, these two LLMs are each tasked with distinct, often competing goals (\textit{e.g.}, in negotiation dialogues, the buyer seeks to attain a more favorable price, whereas the seller endeavors to secure a higher price). 
Meanwhile, a third LLM acts as the reward model to provide goal-oriented verbal feedback, indicating the goal achievement, which is transformed to scalar rewards used for reinforcement learning (RL). 
When reaching the goal or the maximum conversation turn, we leverage RL algorithm to further tune the policy agent with the collected interaction data and the goal-oriented AI feedback.
In this way, the LLM-powered dialogue agent can not only exhibit adaptability to various new cases but also find utility across diverse applications simply by shifting the tuned plug-in.


To overcome the limitation of traditional turn-level response evaluation metrics, we further propose an LLM-based interactive evaluation approach that harnesses LLM-based user simulators and reward models as introduced above. This approach enables the simulation of diverse user-assistant interactions to assess both the success rate and the average number of turns of achieving designated goals. 
We conduct extensive experiments on three different proactive dialogue problems, including negotiation, emotional support, and tutoring dialogues. 
Experimental results demonstrate the superiority of the proposed PPDPP framework over existing LLM-based dialogue systems, showing that PPDPP can effectively and efficiently lead the conversations to achieve the designated goal. The code will be released via \url{https://github.com/dengyang17/PPDPP}.




\section{Related Works}

\paragraph{Dialogue Policy Planning}
Dialogue policy planning has been widely-studied in task-oriented dialogues~\citep{iclr22-gptcritic,iclr23-rewardtod}  and conversational recommendation~\citep{crs-survey}, where the interaction process can be easily abstracted into a sequence of slots and values (\textit{e.g.}, location, price, etc). Meanwhile, the success of planning is objective, such as whether the system provides an appropriate entity/item. However, in proactive dialogues \citep{proactive-survey,wsdm23-proactive}, there is no pre-defined agenda or schema for simplifying the multi-turn interaction. Instead, the natural language interaction requires more complex reasoning and certain domain knowledge (\textit{e.g.}, psychological or pedagogical skills). Moreover, the planning outcome is rather subjective, such as learning gain during tutoring, emotional intensity relaxation during counselling. Therefore, it imposes more difficulties in planning optimal dialogue policy in proactive dialogues. 
In order to mimic the behaviors of human experts, corpus-based fine-tuning approaches are typically adopted for predicting the dialogue  strategies~\citep{iclr21-negotiate-strategy,emnlp22-esc,eacl23-strategy-tutor}.  
As summarized in Table \ref{tab:comparison}, we differentiate our method from the recent LLM-based policy planning methods in terms of seven perspectives. 
General policy planning methods typically optimize towards an objective goal in a single-turn interaction, such as ROUGE score in summarization \citep{dsp} or accuracy in QA \citep{reflexion,yao2023retroformer}. 
As for dialogue policy planning methods, \citet{acl23-mixed} validate the effectiveness of mixed-initiative strategy-based prompting in proactive dialogue problems. 
\citet{llm-proactive} and \citet{acl23-askanexpert} prompt LLMs to conduct self-thinking of policy planning for the next turn, ignoring the long-term conversation goals. 
\citet{negotiate-selfplay} conduct self-play simulation to iteratively refine the policy planning with long-term feedback. 
However, this type of iterative refinement is exclusive to each individual case, but not transferable to new situations. 
Moreover, the policy planning capability in LLM-powered dialogue agents cannot be improved by these methods, as all parameters are frozen and not learnable. 


\begin{table}[]
    \vspace{-0.2cm}
    \caption{Overview of LLM-based general (upper) and dialogue (lower) policy planning methods. }
    \vspace{5pt}
    \label{tab:comparison}
    \centering
    \begin{adjustbox}{width=\textwidth}
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{lccccccc}
    \toprule
     & Subjective & Multi-turn & Decision & Explicit  & Long-term & Gradient & Transfer-\\
     & Goal & Interaction & Making & Strategy & Optimization & Learning & ability\\
    \midrule
    DSP \citep{dsp}& \xmark& \xmark& \xmark & \xmark & \cmark & \cmark & \cmark  \\
    RAP \citep{rap} & \xmark& \cmark& \cmark & \cmark & \cmark  & \xmark & \xmark   \\
    Reflexion \citep{reflexion} & \xmark& \xmark& \cmark& \xmark& \cmark & \xmark& \xmark\\
    Retroformer \citep{yao2023retroformer} & \xmark & \xmark& \cmark & \xmark  & \cmark & \cmark & \cmark\\
    \midrule
    MI-Prompt \citep{acl23-mixed} & \cmark& \xmark& \xmark &  \cmark& \xmark&   \xmark& \xmark\\
    Ask-an-Expert \citep{acl23-askanexpert} & \cmark & \xmark& \cmark & \xmark &   \xmark & \xmark&  \cmark\\
    ProCoT \citep{llm-proactive} & \cmark& \xmark &  \cmark &  \cmark & \xmark & \xmark&  \cmark\\
    ICL-AIF \citep{negotiate-selfplay}  & \cmark & \cmark  & \xmark & \xmark &  \cmark&\xmark& \xmark\\
    \midrule
    \textbf{PPDPP} & \cmark & \cmark & \cmark &  \cmark& \cmark & \cmark& \cmark\\
    \bottomrule
    \end{tabular}}
    \end{adjustbox}
\vspace{-0.4cm}
\end{table}

\paragraph{Learnable Plug-ins for Large Language Models}
Due to the black-box nature of commercial LLMs and the high expenses of fine-tuning the whole open-source LLMs, a recent trend in improving certain capabilities of LLMs is to investigate the utility of external plug-ins, such as APIs \citep{toolformer}, vision models \citep{visual-chatgpt}, or functional models from Huggingface \citep{hugginggpt}. 
However, these plug-ins fail to learn from valuable feedback to iteratively enhance the their capabilities, resulting in performances that are solely dependent on the quality of fixed plug-ins. 
Some latest studies demonstrate that small language models are also valuable plug-ins for LLMs in various applications, such as text classification \citep{small-plugin},  summarization \citep{dsp}, question answering \citep{yao2023retroformer}, or to endow LLMs with specific capabilities, such as mental state reasoning \citep{acl23-tom-plug}. 
These learnable plug-ins can benefit from supervised fine-tuning with available resources or reinforcement learning with the environment. 



\paragraph{Reinforcement Learning from AI Feedback}
As LLMs become more powerful to be capable of supervising other models, \citet{constitutionalai} propose the idea of ``RL from AI Feedback" (RLAIF) to train a harmless and detoxified LLM through self-improvement, without human labels. 
Since the feedback from LLMs is typically in the form of natural language, most of existing studies \citep{reflexion,negotiate-selfplay,self-refine,rap} directly leverage the generated natural language feedback from LLMs to self-refine the task instruction prompt, instead of obtaining a scalar reward for training the model. 
In this work, we propose to transform the goal-oriented AI feedback into scalar rewards for facilitating RLAIF under the context of dialogue systems. 


\section{Method}\label{sec:method}
\paragraph{MDP Environment}
We formulate the dialogue process as a Markov Decision Process (MDP). 
At each turns $t$, according to the observation on the dialogue history, the dialogue system selects an action $a_t\in\mathcal{A}$, where $\mathcal{A}$ is a set of candidate strategies pre-defined by domain experts. In return, the user player responds to the action. This process repeats until the conversational goal is achieved or the maximum number of turns $T$ is reached. The objective is to learn a policy $\pi$ maximizing the expected cumulative rewards over the observed dialogue episodes as: 
\begin{equation}
    \pi^* = \arg \max\nolimits_{\pi\in\Pi} \left[\sum\nolimits_{t=0}^T r(s_t,a_t)\right], 
\end{equation}
where $s_t$ is the state representing the dialogue history. $r(\cdot)$ is the intermediate reward, denoted as $r_t$. 

\paragraph{Plug-and-Play Dialogue Policy Planner}
As shown in Figure \ref{fig:review}(b), a smaller model is adopted to act as a plug-in for controlling the dialogue policy planning in the LLM-powered dialogue agent. 
We leverage a tunable pre-trained language model, \textit{e.g.}, RoBERTa~\citep{roberta}, as the dialogue policy planner to predict the action $a_t$ based on the dialogue history $\{u_1^\text{sys}, u_1^\text{usr}, ..., u_{t-1}^\text{sys}, u_{t-1}^\text{usr}\}$ as the current state $s_t$. 
Before performing interactive online learning, PPDPP can be initialized by fine-tuning on available dialogue corpus $\mathcal{D}$: 
\begin{align}
    a_t &= \mathbf{PPDPP}(u_1^\text{sys}, u_1^\text{usr}, ..., u_{t-1}^\text{sys}, u_{t-1}^\text{usr})\\
    \mathcal{L}_c &= -\frac{1}{|\mathcal{D}|}\sum\nolimits_{d\in\mathcal{D}}\frac{1}{T_d}\sum\nolimits_{t=1}^{T_d} a_t\log y_t
\end{align}
where $T_d$ denotes the number of turns of the dialogue. Although corpus-based learning typically leads to sub-optimal policy, such initialization is supposed to accelerate the convergence process of interactive online training. 


\paragraph{Self-play Interaction}
During the interactive online learning, we prompt two LLMs as the user and the assistant to perform self-play conversations that simulate the dynamic user-assistant interaction. The descriptions of the role and the instructions of their corresponding conversational goals are delivered to each LLM. 
For example, in emotional support dialogues, the patient (user) player will receive the situation description about the causes of the emotional problem while the therapist (assistant) player will receive the task description to reduce users' emotional distress and help them work through the challenges. 
In tutoring dialogues, the student (user) player will receive the descriptions about their knowledge state, while the teacher (assistant) player will receive the task descriptions to teach users to master a certain exercise. 
When it comes to the assistant's turn, PPDPP first predict the next action $a_t$ based on the interaction history. The predicted action is mapped to a pre-defined natural language instruction $\mathcal{M}_a(a_t)$. Then the assistant player generates the strategic response based on the dialogue history and the natural language action instruction:
\begin{equation}
    u_t^{sys} = \mathbf{LLM}_\text{sys}(p_\text{sys};\mathcal{M}_a(a_t);u_1^\text{sys}, u_1^\text{usr}, ..., u_{t-1}^\text{sys}, u_{t-1}^\text{usr})
\end{equation}
Then the user player generates the response based on the updated dialogue history with $u_t^{sys}$: 
\begin{equation}
    u_t^{usr} = \mathbf{LLM}_\text{usr}(p_\text{usr};u_1^\text{sys}, u_1^\text{usr}, ..., u_{t-1}^\text{sys}, u_{t-1}^\text{usr},u_t^{sys})
\end{equation}
where $p_\text{sys}$ and $p_\text{usr}$ are the corresponding prompts. This process is repeated until a terminal state is reached. 
Overall, there are three types of states in the self-play interaction: 1) ON-GOING: the dialogue between the two players is still ongoing as the goal has not been achieved; 2) GOAL-COMPLETED: the designated conversational goal is completed, such as solving the seeker's emotional problem or the student mastering the exercise; 3) GOAL-FAILED: the conversational goal is considered as failure when the conversation reaches a maximum turn without completing the goal.  



\paragraph{LLM as Reward Model}
We prompt a third LLM to be the reward model, named $\mathbf{LLM}_\text{rwd}$, which has two functions: (1) to determine the goal completion during the conversation; (2) to evaluate the policy outcome with scalar rewards.  
\begin{itemize}[leftmargin=*]
    \item In emotional support dialogues, to assess whether the patient' emotional issue has been solved, we prompt the reward model to answer a multi-choice question ``Has the patient's issue been solved?", and then generate the goal-oriented AI feedback at temperature $\tau>0$ to sample the  responses for $l$ times. We define a mapping  $\mathcal{M}_r(\cdot)$ to transform verbal feedback to scalar rewards, such as ``feel worse", ``feel the same", ``feel better", ``solved" as -1.0, -0.5, 0.5, and 1.0, respectively. 
    \item In tutoring dialogues, to assess the student's mastery degree of the exercise, we prompt the reward model to sample the response for the multi-choice question ``Did the Student correctly translated the whole sentence into Italian?".  We define the rewards for ``incorrect answer", ``no answer", ``partially correct answer" and ``correct answer" as -1.0, -0.5, 0.5, and 1.0, respectively. 
    \item In negotiation dialogues, it is required to first assess whether the buyer and the seller have reached a deal. We prompt the reward model to sample the response for the binary question ``Have they reached a deal?", and define the scores for ``no deal" and ``deal" as -1.0 and 1.0 respectively, which are adopted for determining whether the goal is completed. Since the objective of negotiation dialogues is to maximize the gains from the assistant side, we also employ the reward model to extract the deal price when reaching a deal. The reward is defined by the Sale-to-List Ratio \citep{sigdial19-negotiate}, which is formulated as $(\texttt{deal price} - \texttt{seller target price})/(\texttt{buyer target price} - \texttt{seller target price})$. 
\end{itemize}

In general, we obtain a scalar value $v_t$ by sampling the goal-oriented AI feedback for $l$ times and converting them into a scalar value through averaging: 
\begin{equation}
    v_t = \frac{1}{l}\sum\nolimits^l_{i=1}\mathcal{M}_r(\mathbf{LLM}_\text{rwd}(p_\text{rwd};u_1^\text{sys}, u_1^\text{usr}, ..., u_{t-1}^\text{sys}, u_{t-1}^\text{usr},u_t^{sys},u_t^{usr};\tau))
\end{equation} 
where $p_\text{rwd}$ is the prompt. We first use $v_t$ to determine the state of the self-play interaction. If $v_t$ is not less than a certain threshold $\epsilon$, we regard the state as GOAL-COMPLETED. 
If the conversation reach a terminal state, including GOAL-COMPLETED and GOAL-FAILED, we have the reward $r_t=v_t$. If not, we assign a small negative reward, \textit{e.g.}, $r_t=-0.1$, to penalize the lengthy conversation for promoting efficient goal completion. 


\paragraph{Reinforcement Learning}
When reaching the goal or the maximum conversation turn, we obtain goal-oriented reward $r_t$. 
We denote the policy agent as $\pi(a_t|s_t)$, which returns the probability of taking action $a_t$ given the state $s_t$. To optimize the policy agent, we simply use the vanilla policy gradient method~\citep{nips99-pg}, formulated as follows:
\begin{equation}
    \theta \leftarrow \theta - \alpha \nabla \log \pi_\theta(a_t|s_t)R_t
\end{equation}
where $\theta$ denotes the parameter of the policy network, $\alpha$ denotes the learning rate of the policy network, and $R_t$ is the total reward accumulating from turn $t$ to the final turn $T$: $R_t=\sum^T_{t'=t}\gamma^{T-t'}r_{t'}$, where $\gamma$ is a discount factor which discounts future rewards over immediate reward. 
In the inference, the LLM-powered dialogue agent with the tuned PPDPP can be directly applied to diverse new situations without the necessity of performing multiple iterations of simulation again. 

\section{Experimental Setups}
\subsection{Datasets}

We evaluate the proposed framework in three different applications of proactive dialogues, including negotiation dialogues, emotional support dialogues, and tutoring dialogues. The statistics of adopted datasets are presented in Table \ref{tab:dataset}. In specific, the human-annotated dialogues in the train set are used for the supervised fine-tuning of the dialogue policy planner, while only the case background information in the dataset is adopted for the reinforcement learning process. 

\begin{wraptable}{r}{7cm}
\vspace{-0.7cm}
\caption{The statistics of datasets (train/dev/test).}\label{tab:dataset}
\vspace{-0.3cm}
\begin{tabular}{lrr}\\\toprule  
Dataset &\# Case   & \# Act \\\midrule
CraisglistBargain & 3,090/188/188&11\\  
ESConv &1,040/130/130 & 8\\  
CIMA &909/113/113 & 5\\  \bottomrule
\end{tabular}
\vspace{-0.4cm}
\end{wraptable} 


\noindent \textbf{CraisglistBargain}
\citep{emnlp18-negotiate} is created under the bargain negotiation setting where the buyer and the seller are negotiating the price of an item on sale. \citet{acl21-negotiate-tom} design 15 dialogue acts for labeling this dataset, including 11 negotiation strategies and 4 terminal acts. In our experiment, we only consider the 11 negotiation strategies and split the development set into two parts as a new development set and a test set. Each case is associated with an item category, an item description, a buyer target price, and a seller target price, which are adopted as the instruction information. 


\noindent \textbf{ESConv} 
\citep{esconv} is an emotional support conversation dataset, contains 1,300 cases with 8 types of support strategies. We adopt the original train/dev/test split. Each case is accompanied with a problem type, an emotion type, and a situation description. 



\noindent \textbf{CIMA} \citep{cima} is a crowd-sourced dataset, where annotators were asked to role-play students and teachers by working through an exercise with 5 pedagogical strategies on translating a prepositional sentence from English to Italian. We regard each exercise as a case in our experiment and randomly split the dataset into train/dev/test sets by 8:1:1. 


\subsection{Evaluation Protocols}
Previous studies \citep{iclr21-negotiate-strategy,emnlp22-esc,eacl23-strategy-tutor} on emotional support, negotiation, and tutoring dialogues typically evaluate the turn-level performance, based on the fixed reference responses. 
Differently, when it comes to the evaluation of dialogue policy planning, it would be more appropriate to focus on the dialogue-level interactive evaluation \citep{survey-dpl,crs-survey}. 
Goal completion is the key to evaluating proactive dialogue systems \citep{proactive-survey}. 
To this end, we adopt the average turn (AT) and the success rate at turn $t$ (SR@$t$) as the automatic evaluation metrics. AT measures the efficiency of the goal completion by calculating the average number of turns to achieve the goal while SR measures the effectiveness of the goal completion by computing the success rate of achieving the goal within a pre-defined maximum turn. 
We set the maximum turn of the conversation as 8 in our experiment. 
In particular, as for CraisglistBargain, since reaching a deal cannot be regarded as a success, we adopt the Sale-to-List Ratio (SL\%) \citep{sigdial19-negotiate} as the metrics to determine the effectiveness of goal completion during the negotiation dialogue. A higher SL\% represents the buyer gets more benefits from the deal. If failing to reach a deal at the end, we assign SL\% as 0 for this case. 
The evaluation is conducted by interacting with the LLM-based user simulator, while the goal completion (\textit{i.e.}, GOAL-COMPLETED or GOAL-FAILED) is determined by the LLM-based reward model, as introduced in Section \ref{sec:method}. 
To validate the reliability of this evaluation framework, a preliminary analysis of adopting LLMs as reward models and user simulators is presented in Appendix \ref{app:preliminary}. 

\subsection{Implementation Details}
We adopt RoBERTa (\texttt{roberta-large}) as the plug-and-play dialogue policy planner for evaluation.  
The details of training process are provided in Appendix \ref{app:implement}.
In the main results of evaluating the dialogue policy planning methods, we use ChatGPT (\texttt{gpt-3.5-turbo-0613}) as the frozen LLM for both the role-playing LLMs ($\mathbf{LLM}_\text{sys}$ and $\mathbf{LLM}_\text{usr}$) and the reward model ($\mathbf{LLM}_\text{rwd}$). As for the role-playing LLMs, we set the temperature $\tau=0$ to generate the deterministic outputs with the same inputs. While we set $\tau=1.1$ and the sample times $l=10$ for the reward model to integrate the scalar rewards. 
The role-playing prompts for the assistant and the user players ($p_\text{sys}$ and $p_\text{usr}$) are presented in Appendix \ref{app:resp_prompt} and \ref{app:user_prompt}, respectively. 
The mapping of natural language instructions, $\mathcal{M}_a(\cdot)$, for dialogue actions is presented in Appendix \ref{app:strategy}. 
The prompts for the reward model, $p_\text{rwd}$, are presented in Appendix \ref{app:reward_prompt}. 
In order to compare with different LLMs, we also adopt two popular open-source LLM-based dialogue systems, including \texttt{Vicuna-13B-delta-v1.1} and \texttt{LLaMA-2-13B-Chat} with the same hyper-parameter setting as ChatGPT. 


\subsection{Baselines}
We first compare to a general fine-tuned dialogue models, DialoGPT \citep{dialogpt}. Furthermore, we adopt several latest LLM-based dialogue policy planning methods for comparisons as follows.  Implementation details for these baselines are presented in Appendix~\ref{app:baseline_prompt}.  

\noindent \textbf{Standard} simply prompts two LLMs to conduct self-play conversations using task instructions without considering any dialogue strategy. 

\noindent \textbf{Proactive} \citet{llm-proactive} first prompts the LLM-based dialogue system to select the most appropriate strategy for the next turn, and then based on the selected strategy to generate the response. Since the predicted strategy label is not verbal description for instructing LLMs, we map the strategy label into the mixed-initiative strategy prompt (\textbf{MI-Prompt}) as \citet{acl23-mixed}. 

\noindent \textbf{ProCoT} \citet{llm-proactive} further improve Proactive by first prompting the LLM-based dialogue system to generate a chain-of-thought descriptive analysis for planning the strategy for the next turn. MI-Prompt is also incorporated into ProCoT. 

\noindent \textbf{Ask-an-Expert (AnE)} \citep{acl23-askanexpert} prompts another LLM as the strategic expert with $M$-part questions for reasoning about the next dialogue strategy. The dialogue strategy is a verbal description instead of selecting from a pre-defined strategy taxonomy. 

\noindent \textbf{ICL-AIF} \citep{negotiate-selfplay} prompts another LLM to provide feedback to a player to improve their dialogue strategies, which is a verbal feedback instead of explicit strategies. Different from AnE, ICL-AIF employs dialogue-level feedback for strategy improvement with $N$ times of iteration.  



 


\section{Experiments}




\subsection{Overall Evaluation}\label{exp:result}


Table~\ref{tab:overall_res} summarizes the experimental results of method comparisons on three datasets. Overall, the proposed method, PPDPP, consistently outperforms all the baselines with a noticeable margin across three different proactive dialogue problems. The results show that PPDPP can not only efficiently achieve the conversational goal (less Average Turn), but also effectively accomplish more tasks (higher Success Rate or higher Sale-to-List Ratio). 
Besides, compared with AnE and ICL-AIF, PPDPP requires much fewer number of tokens for calling the API of black-box LLMs to handle each new conversation. 
Furthermore, the supervised fine-tuning on human-annotated dialogues contribute little to the final performance, as compared to reinforcement learning with the dynamic environment. 
In the following, we discuss the task-specific observations for each application:

\begin{table}[]
    \vspace{-0.2cm}
    \caption{Experimental results. \#Tokens denotes the approximate tokens required for prompting LLMs to simulate a new conversation, where $L$ denotes the standard prompt length for one episode of conversation. 
    $M$ and $N$ are respectively set to 3 and 5 by default as set by corresponding works.}
    \vspace{5pt}
    \centering
    \begin{adjustbox}{width=\textwidth}
    \setlength{\tabcolsep}{2mm}{
    \begin{tabular}{llrccccccc}
    \toprule
    &  & \multicolumn{3}{c}{CraisglistBargain}  &  \multicolumn{2}{c}{ESConv} & \multicolumn{2}{c}{CIMA}\\
    \cmidrule(lr){3-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
    Method & \#Tokens & AT$\downarrow$ & SR$\uparrow$  & SL\%$\uparrow$  & AT$\downarrow$ & SR$\uparrow$  & AT$\downarrow$ & SR$\uparrow$  \\
    \midrule
    DialoGPT & - & 6.73 & 0.3245 & 0.2012 & 5.31 & 0.7538 & 5.43 & 0.4956 \\
    \midrule
      Standard  & $\mathcal{O}(L)$ &6.47 & 0.3830 &0.1588 & 5.10 & 0.7692 & 3.89 & \underline{0.6903} \\
      AnE \citep{acl23-askanexpert} & $\mathcal{O}((M+1)L)$ & 5.91 & 0.4521 & 0.2608& 4.76 & 0.8000 &\underline{3.86} & 0.6549 \\
      Proactive \citep{llm-proactive}   & $\mathcal{O}(L)$ & 5.80 & 0.5638 & 0.2489& 5.08 & 0.7538 & 4.84 & 0.5310 \\
      ~ + MI-Prompt \citep{acl23-mixed} & $\mathcal{O}(2L)$& \underline{5.74} & \underline{0.5691} & 0.2680&4.78&0.7846& 4.70&0.5664 \\
      ProCoT \citep{llm-proactive}   & $\mathcal{O}(L)$ & 6.22 & 0.5319 & 0.2486& 4.75 & 0.7923 & 4.58 & 0.5487 \\
      ~ + MI-Prompt \citep{acl23-mixed} &$\mathcal{O}(2L)$& 6.12 & 0.5532 & \underline{0.3059} & 4.83 & 0.7769 & 4.72 & 0.5221  \\
      ICL-AIF \citep{negotiate-selfplay} & $\mathcal{O}((N+1)L)$ & 6.53 & 0.3617 & 0.1881& \underline{4.69} & \underline{0.8079} & 4.19 & 0.6106 \\
      \midrule
      \midrule
      \textbf{PPDPP} &$\mathcal{O}(L)$& 5.62 & 0.6117 & \textbf{0.3376} & \textbf{4.56} & \textbf{0.8462} & \textbf{3.03} & \textbf{0.8407} \\
      
      ~ - w/o SFT&$\mathcal{O}(L)$ & 5.71 & 0.6223 & 0.3354 & 4.68 & 0.8384 & 3.18 & 0.8230 \\
      ~ - w/o RL &$\mathcal{O}(L)$& \textbf{5.57} & \textbf{0.6649} & 0.2280& 5.24 & 0.7308 & 3.41 & 0.7965 \\
      \bottomrule
    \end{tabular}}
    \end{adjustbox}
\vspace{-0.5cm}
    \label{tab:overall_res}
\end{table}

\noindent \textbf{Negotiation Dialogues (CraisglistBargain)} ~
Among the baselines, all three turn-level policy planning methods (\textit{incl.} AnE, Proactive, and ProCoT) substantially improve the deal success rate (SR) and the deal benefit (SL\%). Unexpectedly, ICL-AIF only casts a trivial improvement on the deal benefit while even negatively affecting the deal success rate. This result indicates that the dialogue-level AI feedback fails to dynamically adjust the dialogue strategy along with the dialogue state transition to reach the consensus with the users in negotiation dialogues.    
As for the proposed method, we observe that PPDPP (- w/o RL) can largely improve the deal success rate with supervised fine-tuning on the human-annotated corpus, but bring not much benefit to the system side. 
RL with simulated interactions further increases the SL\% with a substantial margin (0.2280 $\rightarrow$ 0.3476) by optimizing the policy towards a higher negotiation outcome. 
However, this optimization objective  negatively affects the deal success rate (0.6649 $\rightarrow$ 0.6117), which is as expected because it inevitably downgrades the deal willingness of the seller when you are trying to maximize your own benefit. 


\noindent \textbf{Emotional Support Dialogues (ESConv)} ~
ChatGPT with Standard prompting has already achieved a quite good performance in this problem, \textit{e.g.}, reaching a success rate of 76.92\%. 
Among the baselines, those methods of learning from AI feedback (\textit{incl.} AnE and ICL-AIF) perform slightly better than explicit strategy prediction methods (\textit{incl.} Proactive and ProCoT). 
Compared with these zero-shot approaches, simply supervised fine-tuning on human-annotated corpus (PPDPP - w/o RL) performs even worse than Standard prompting. 
This result shows that corpus-based learning is far from satisfactory for handling emotional support dialogues. 
After learning from dynamic interactions, PPDPP effectively improves the success rate (0.7308 $\rightarrow$ 0.8462) as well as outperforms all other baselines with a noticeable margin. 


\noindent \textbf{Tutoring Dialogues (CIMA)} ~
All the baseline methods fail to defeat the Standard prompting scheme in tutoring dialogues, indicating that the ChatGPT itself has already been powerful at teaching others in translating English into Italian. 
However, different from emotional support dialogues, the corpus-based learning is shown to be useful in tutoring dialogues, substantially outperforming all the baselines. 
We attribute this difference to the diversity of cases in these two applications: In ESConv, there are various emotional issues that require different chains of strategies to handle. While in CIMA, all the cases are concerned on the same type of exercises, \textit{i.e.}, translating an English sentence with the same grammatical structure into Italian. Corpus-based learning tends to perform well in those testing cases that are similar to the training data. 
Nevertheless, RL can still further improve the performance to a great extent (0.7965 $\rightarrow$ 0.8407). 




\subsection{In-depth Analysis}






\input{fig/turn_performance}

\paragraph{Performance w.r.t Turns}
Besides SR@8, we also evaluate the  success rate at each turn (SR@$t$) in Figure \ref{fig:exp-turn}. To better compare different methods, we report the relative success rate compared with the Standard prompting schemes. For example, the line of $y = 0$ represents the curve of SR@$t$ for Standard against itself. 
As for the negotiation dialogues (CraisglistBargain), we also present the relative Sale-to-List Ratio against Standard in terms of different conversation turns (SL\%@$t$) in Figure \ref{fig:exp-turn-slr}.  
PPDPP outperforms these baselines across all the datasets and almost each turn in the conversation session. 
The exception is that AnE has a relatively strong performance at the first few turns. 
By obtaining the detailed feedback regarding the simple situation within a short dialogue context, AnE successfully accomplishes the conversation goal at the early stage of the conversation. However, the performance falls quickly as the conversation turn increases, indicating that AnE fails to achieve the long-term goals of complicated situations. 
Moreover, as for the tutoring dialogues (CIMA), all the baselines perform even worse than the Standard prompting after three turns of conversations, indicating that they fall short in adjusting their policy for reaching the long-term goal after getting stuck in a wrong decision. 


\vspace{-0.2cm}
\paragraph{Comparisons with Different LLMs}
We compare the performance of PPDPP in terms of using different LLMs as the backbone LLM for response generation, including two popular open-source LLMs, Vicuna and LLaMA2-Chat. 
Note that the user simulator and the reward model are still built upon ChatGPT. 
Figure \ref{fig:curve} shows the test performance curves along with training episodes.  
Overall, the RL training of PPDPP effectively enhances the performance of all these LLM-powered dialogue agents on each dialogue problem, where the optimization objective of PPDPP generally increases along with training episodes, \textit{i.e.}, SL\% for CraisglistBargain and SR for ESconv and CIMA. 
However, ChatGPT is not necessarily outperform them in all these proactive dialogue problems.  For example, in negotiation dialogues, both Vicuna and LLaMA2-Chat achieve higher benefits (\textit{i.e.}, SL\%) but with lower success rate of reaching a deal that ChatGPT, which indicates that ChatGPT is more likely to make compromises with users. 
This tendency could be attributed to ChatGPT's enhanced response-ability, as the negotiation strategy doesn't prescribe a specific bargaining price, allowing it to favor prices that align closely with the context of the ongoing dialogue.
In emotional support dialogues, Vicuna achieves competitive performance as ChatGPT. 
In tutoring dialogues for Italian translation, ChatGPT substantially outperforms others due to its remarkable multilingual capabilities. 
These results imply that LLM-powered dialogue agents have some inherent strengths in different dialogue problems from the black-box training process of their backbone LLMs. 



\input{fig/training_curve}


\subsection{Human Evaluation}
Following previous studies \citep{esconv,iclr21-negotiate-strategy}, we conduct human evaluation on 100 randomly sampled dialogues from ESConv and CraisglistBargain. Three annotators are asked to pair the generated responses by PPDPP with those by other methods, including AnE, ProCoT, and ICL-AIF. 
As for emotional support dialogues (ESConv), we measure three main perspectives of the responses, including \textbf{Ide}ntification, \textbf{Com}forting, and \textbf{Sug}gestion. 
As for negotiation dialogues (CraisglistBargain), we also measure three main perspectives of the responses, including \textbf{Per}suasive, \textbf{Coh}erent, and \textbf{Nat}ural. The instructions for annotators are presented in Appendix \ref{app:human}. 
\begin{table}[t]
\vspace{-0.2cm}
    \caption{Human evaluation results.}
    \vspace{5pt}
    \centering
    \begin{adjustbox}{width=\textwidth}
    \setlength{\tabcolsep}{1mm}{
    \begin{tabular}{lcccccccccccccccc}
    \toprule
    & \multicolumn{8}{c}{ESConv} &  \multicolumn{8}{c}{CraisglistBargain} \\
    \cmidrule(lr){2-9} \cmidrule(lr){10-17} 
    PPDPP  & \multicolumn{2}{c}{Ide.} &  \multicolumn{2}{c}{Com.} &  \multicolumn{2}{c}{Sug.} & \multicolumn{2}{c}{Ove.} &  \multicolumn{2}{c}{Per.} &  \multicolumn{2}{c}{Coh.} &  \multicolumn{2}{c}{Nat.} & \multicolumn{2}{c}{Ove.} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11} \cmidrule(lr){12-13} \cmidrule(lr){14-15} \cmidrule(lr){16-17}
    \multicolumn{1}{c}{vs.}& Win & Lose & Win & Lose& Win & Lose& Win & Lose& Win & Lose& Win & Lose & Win & Lose& Win & Lose\\
    \midrule
      AnE & \textbf{31\%} & 15\% & 14\% & \textbf{27\%} & \textbf{52\%} & 12\% & \textbf{34\%} & 24\% & \textbf{40\%} & 23\% & \textbf{22\%} & 12\% & \textbf{14\%} & 7\% & \textbf{31\%} & 18\% \\
      ProCoT  & \textbf{27\%} & 21\% & \textbf{34\%} & 20\% & \textbf{38\%} & 15\% & \textbf{30\%} & 11\% & \textbf{24\%} & 21\% & \textbf{17\%} & 15\% & \textbf{9\%} & 6\% & \textbf{27\%} & 21\%\\
      ICL-AIF & \textbf{35\%} & 12\% & \textbf{32\%} & 28\% & \textbf{33\%} & 29\% & \textbf{29\%} & 22\% & \textbf{55\%} & 11\% & \textbf{39\%} & 12\% & \textbf{25\%} & 3\% & \textbf{62\%} & 4\% \\
      \bottomrule
    \end{tabular}}
    \end{adjustbox}
    \label{tab:human_eval}
    \vspace{-0.5cm}
\end{table}

As presented in Table \ref{tab:human_eval}, PPDPP outperforms other baselines in almost all perspectives of the human evaluation as well as the overall (Ove.) evaluation, except that AnE achieves a higher win rate in Comforting for emotional support dialogues.  
Details of qualitative case study for different methods are presented in Appendix \ref{app:case}. 
We observe that AnE can provide detailed instructions for emotional support strategies, and most of them are empathetic strategies, which contributes to the strong comforting capability of AnE. However, the dialogue system is further expected to take the initiative to explore and solve the patient's emotional issue, not just conveying empathy. 


\section{Conclusions}
In this work, we introduce a new paradigm for strategizing LLM-powered dialogue agents with a plug-and-play dialogue policy planner, namely PPDPP. 
Furthermore, we develop a novel training framework to facilitate supervised fine-tuning with available human-annotated corpus as well as reinforcement learning from goal-oriented AI feedback to enhance their policy planning capability.  
In this manner, the LLM-powered dialogue agents can not only be generalized to different cases, but also exhibits versatility across diverse applications by simply substituting the learned plug-in without affecting the response-ability of LLMs. 
In addition, this framework can serve as an interactive evaluation protocol, enabling the measurement of dialogue-level effectiveness and efficiency in multi-turn conversations. 
Experimental results on three different proactive dialogue problems show the superiority of PPDPP, including negotiation, emotional support, and tutoring dialogues. 

Our findings carry significant implications for the field of conversational AI research: (1) They highlight the potential of tunable plug-ins to address specific shortcomings in LLMs, which can be extended to various applications and integrated with multiple plug-ins to tackle more complex dialogue challenges. (2) They indicate that dialogue agents powered by different LLMs inhere the strengths in diverse problems from their respective training processes. Recognizing the resource-intensive nature of training specialized LLMs, this insight implies the potential value in employing the ensemble of multiple  agents collaboratively to address a wide range of dialogue problems. 


\clearpage
\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}


\clearpage

\appendix

\section{Reliability Analysis of LLMs as Reward Models and User Simulators}\label{app:preliminary}

Before conducting the self-play evaluation, we first validate the reliability of adopting LLMs as reward model and user simulator. We sample 50 self-play dialogues from each dataset.  

\begin{wrapfigure}{r}{7cm}
 %\vspace{-0.5cm}
 \centering
 \begin{tikzpicture} 
 \begin{axis}[
 enlargelimits=0.3,
 legend style={at={(0.5,-0.25)},
  anchor=north,legend columns=-1},
 symbolic x coords={CraisglistBargain, ESConv, CIMA },
 xtick=data,
 ybar=3pt,% configures `bar shift'
 bar width=10pt,
 width=7cm, height=4cm,
 nodes near coords,
 nodes near coords align={vertical},
 nodes near coords style={font=\tiny},
 font=\small,
 grid=both,
 grid style={dashed, gray!50},
 ]
 \addplot[fill=brown!40!white,draw=brown] coordinates {
  (CraisglistBargain, 90.3)
  (ESConv, 83.6)
  (CIMA, 73.6)
 };
 \addplot[fill=yellow!40!white,draw=yellow] coordinates {
  (CraisglistBargain, 92.1)
  (ESConv, 82.2)
  (CIMA, 71.1)
 };
 \addplot [fill=black!40!white,draw=black] coordinates {
  (CraisglistBargain, 97.3)
  (ESConv, 93.4)
  (CIMA, 94.6)
 };
 \legend{Vicuna, LLaMA2-Chat, ChatGPT}
 \end{axis}
 \end{tikzpicture}
 \caption{Analysis of LLMs as reward model.}
 \label{fig:reward_model}
 %\vspace{-0.2cm}
\end{wrapfigure} 

\noindent \textbf{LLMs as Reward Model} ~ Since the reward model is prompted to select one of the situation that is matched the most with the current user state, we compute the F1 score of the prediction versus the human-annotated labels. 
We analyze the validity of Vicuna-13B \citep{vicuna2023}, LLaMA2-Chat-13B \citep{llama2}, and ChatGPT. 
As presented in Figure~\ref{fig:reward_model}, all these three LLMs can perform quite well in serving the user simulator for CraisglishBargain and ESConv. However, since Vicuna and LLaMA2 have not trained on large-scale Italian data, they fail to evaluate the correctness of students' Italian translation for CIMA. 
According to the analysis, ChatGPT is qualified to serve as the reward model for all these three problems.

\begin{wraptable}{l}{7cm}
%\vspace{-0.3cm}
\begin{adjustbox}{width=6.8cm}
\begin{tabular}{lcccc}
\toprule  
&\multicolumn{2}{c}{Single-turn}&\multicolumn{2}{c}{Multi-turn}\\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
Setting &Natural   & Useful &Natural   & Useful\\
\midrule
DialoGPT & 8\% & 4\% & 2\% & 5\% \\  
ChatGPT & \textbf{63\%} & \textbf{72\%} & \textbf{78\%} & \textbf{74\%}\\  
Tie & 29\% & 24\% & 20\% & 21\%\\  
\midrule
Human & 14\% & 22\% & 18\% & 27\%\\  
ChatGPT & \textbf{49\%} & \textbf{42\%} & 36\% & 33\%\\  
Tie & 37\% & 36\% & \textbf{46\%} &\textbf{41\%}\\  
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Comparison on user simulators. The Cohen’s Kappa between annotators is 0.72. }\label{tab:user_sim}
%\vspace{-0.2cm}
\end{wraptable} 

\noindent \textbf{LLMs as User Simulator}~ 
The simulated users are expected to play their assigned role under the specific context, \textit{e.g.}, being a patient with specific emotional issues caused by certain real-world scenarios in the emotional support dialogues. 
Following previous studies on leveraging language models as user simulator \citep{wsdm22-user-sim,llm-crs-eval},  we assess the quality based on the \textit{naturalness} and \textit{usefulness} of the generated utterances in the settings of single-turn and multi-turn free-form conversations. \textit{Naturalness} refers to that the utterances are fluent and likely to be generated by humans, and \textit{usefulness} means that the utterances are consistent with the role descriptions. We compare the prompted user simulator based on LLMs with a fine-tuned version of DialoGPT \citep{dialogpt} and the original conversations on the each dataset. 
Two annotators are employed to make pairwise evaluations by rating ``Win/Tie/Lose" between two samples. 
As presented in Table \ref{tab:user_sim}, the ChatGPT-based simulator exhibits a notably superior performance compared to DialoGPT, particularly when it comes to the naturalness of responses in multi-turn conversations, which showcases the impressive language generation capabilities inherent in LLMs. 
Furthermore, even compared with human-annotated dialogues, the ChatGPT-based simulator shows competitive performance. These results validate the reliability of adopting ChatGPT as the user simulator. 


\section{Training Details}\label{app:implement}
The training process includes two phases: supervised fine-tuning (SFT) and reinforcement learning (RL). 
During SFT, we fine-tune PPDPP on the training set and save the checkpoint based on the best performance at the validation set. 
During RL, we randomly sample cases in the training set for online training. 
The hyper-parameters used in our experiments are detailed in Table \ref{tab:hyperparam}.
All the experiments are run on a server equipped with 8 Tesla V100 GPUs. 


\begin{table}[h]
    \centering
    \begin{tabular}{lll}
    \toprule
    Training Phase   &  Hyper-parameter & Value\\
    \midrule
     \multirow{6}{*}{SFT}  & Batch Size & 16\\
     & Training Epochs & 10 \\
     & Learning Rate & 6e-6\\
     & Max Sequence Length & 512 \\
     & Learning Scheduler & Linear\\
    & Weight Decay & 0.01\\
    \midrule
    \multirow{6}{*}{RL} & Training Episodes & 1,000\\
    & Learning Rate & 1e-6\\
    & Max Conversation Turn & 8 \\
    & Discount Factor $\gamma$ & 0.999 \\
    & Max New Tokens & 32 \\
    & Threshold for Goal Completion $\epsilon$ & 1.0 \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameter settings in two training phases.}
    \label{tab:hyperparam}
\end{table}

\section{Human Evaluation Instructions}\label{app:human}
As for emotional support dialogues (ESConv), we measure three main perspectives of the responses as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Identification}:  Which assistant is more helpful in exploring and identifying the problem?
    \item \textbf{Comforting}:  Which assistant is more skillful in comforting you?
    \item \textbf{Suggestion}: Which assistant provides more helpful suggestions for solving the problem?
\end{itemize}


As for negotiation dialogues (CraisglistBargain), we also measure three main perspectives of the responses as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Persuasive}:  Which assistant is more persuasive in the negotiation?
    \item \textbf{Coherent}: Which assistant is more on topic and in accordance with the conversation history?
    \item \textbf{Natural}: Which assistant is more human-like?
\end{itemize}

\section{Prompting Details}
In this section, we present the prompting details in our implementation. 

\subsection{Response Generation}\label{app:resp_prompt}
We first describe the details of role-playing prompts for the dialogue systems to generate responses, which involves the dialogue strategy prompt, \textit{i.e.}, \texttt{[action]}, for instructing the action at the next turn. 

\noindent \textbf{Negotiation Dialogue} ~ 
In the negotiation dialogues, the assistant is assigned the role of buyer to bargain with the seller for a lower item price. 
In each case, there is an item name \texttt{[item\_name]} and an item description \texttt{[item\_description]} to describe the negotiation background. 
The buyer is assigned with a target price to achieve, \textit{i.e.}, \texttt{[buyer\_target\_price]}. 
The negotiation begins with the listed item price, \textit{i.e.}, \texttt{[seller\_target\_price]}. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11.5cm}}
    \toprule
    System   &  Now enter the role-playing mode. In the following conversation, you will play as a buyer in a price bargaining game. \\
    \midrule
    User    &  You are the buyer who is trying to buy the \texttt{[item\_name]} with the price of \texttt{[buyer\_target\_price]}. Product description: \texttt{[item\_description]}
    Please reply with only one short and succinct sentence. \texttt{[action]} Now start the game.\\
    \midrule
    Assistant & Hi, how much is the \texttt{[item\_name]}? \\
    \midrule
    User & Hi, this is a good \texttt{[item\_name]} and its price is \texttt{[seller\_target\_price]}. \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for response generation in negotiation dialogues.}
    \label{tab:cb_response}
\end{table}


\noindent \textbf{Emotional Support Dialogue} ~ 
In the emotional support dialogues, the assistant is assigned the role of therapist to help the patient reduce their emotional distress and help them understand and work through the challenges. 
In each case, the conversation begins with the user describing their problems, \textit{i.e.}, \texttt{[situation]}, which serves as the distinct dialogue background.


\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
    \toprule
    System   &  Now enter the role-playing mode. In the following conversation, you will play as a therapist in a counselling conversation with a patient. \\
    \midrule
    User    &  You are the therapist who is trying to help the patient reduce their emotional distress and help them understand and work through the challenges. Please reply with only one short and succinct sentence. \texttt{[action]} Are you ready to play the game?\\
    \midrule
    Assistant & Yes, I'm ready to play the game! \\
    \midrule
    User & \texttt{[situation]} \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for response generation in emotional support dialogues.}
    \label{tab:esc_response}
\end{table}


\noindent \textbf{Tutoring Dialogue} ~ 
In the tutoring dialogues, the assistant is assigned the role of teacher to teach the student to translate an English sentence into Italian. 
In each case, the conversation begins with the translation exercise, \textit{i.e.}, \texttt{[exercise]}, and the student's individual problem on the exercise, \textit{i.e.}, \texttt{[situation]}, which serve as the distinct dialogue background. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
    \toprule
    System   &  Now enter the role-playing mode. In the following conversation, you will play as a teacher in a tutoring conversation with a student. \\
    \midrule
    User    &  You are the teacher who is trying to teach the student to translate ``\texttt{[exercise]}" into Italian. Please reply with only one short and succinct sentence. Please do not tell the student the answer or ask the student about other exercises. \texttt{[action]} Now ask me an exercise.\\
    \midrule
    Assistant & Please translate ``\texttt{[exercise]}" into Italian. \\
    \midrule
    User & \texttt{[situation]} \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for response generation in tutoring dialogues.}
    \label{tab:cima_response}
\end{table}

\clearpage
\subsection{User Simulator}\label{app:user_prompt}
Next, we describe the role-playing prompt for instructing LLMs to simulate users, which excludes the dialogue strategy prompts so that the simulated users will only be responsive to the dialogue history without taking specific actions. 

\noindent \textbf{Negotiation Dialogue} ~ 
In the negotiation dialogues, the assistant is assigned the role of seller to bargain with the buyer for a higher item price. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11.5cm}}
    \toprule
    System   &  Now enter the role-playing mode. In the following conversation, you will play as a seller in a price bargaining game. \\
    \midrule
    User    &  You are the seller who is trying to sell the \texttt{[item\_name]} with the price of \texttt{[seller\_target\_price]}. Product description: \texttt{[item\_description]}
    Please reply with only one short and succinct sentence. Are you ready to play the game?\\
    \midrule
    Assistant & Yes, I'm ready to play the game! \\
    \midrule
    User & Hi, how much is the \texttt{[item\_name]}? \\
    \midrule
    Assistant & Hi, this is a good \texttt{[item\_name]} and its price is \texttt{[seller\_target\_price]}. \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for user simulator in negotiation dialogues.}
    \label{tab:cb_user}
\end{table}


\noindent \textbf{Emotional Support Dialogue} ~ 
In the emotional support dialogues, the assistant is assigned the role of patient to look for the help form the therapist. For better simulating the user, the emotion type \texttt{[emotion\_type]} and the problem type \texttt{[problem\_type]} are also provided in the prompt. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
    \toprule
    System   &  Now enter the role-playing mode. In the following conversation, you will play as a patient in a counselling conversation with a therapist. \\
    \midrule
    User    &  You are the patient who is looking for the help from the therapist, because you have the emotional issue about \texttt{[emotion\_type]} regarding \texttt{[problem\_type]}. Please reply with only one short and succinct sentence. Now tell me your issue.\\
    \midrule
    Assistant & \texttt{[situation]} \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for user simulator in emotional support dialogues.}
    \label{tab:esc_user}
\end{table}


\noindent \textbf{Tutoring Dialogue} ~ 
In the tutoring dialogues, the assistant is assigned the role of student to learn to translate an English sentence into Italian. 
Since LLMs have strong capabilities of translation, we further instruct them to forget the translation of the discussed exercise. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
    \toprule
    System   &  Now enter the role-playing mode. In the following conversation, you will play as a student who does not know Italian in a tutoring conversation with a teacher. \\
    \midrule
    User    &  You are the student who is trying to translate an English sentence into Italian. You don't know the translation of ``\texttt{[exercise]}" in Italian. Please reply with only one short and succinct sentence. Are you ready to play the game?\\
    \midrule
    Assistant & Yes, I'm ready to play the game! \\
    \midrule
    User & Please translate ``\texttt{[exercise]}” into Italian. \\
    \midrule
    Assistant & \texttt{[situation]}\\
    \bottomrule
    \end{tabular}
    \caption{Prompts for user simulator in tutoring dialogues.}
    \label{tab:cima_user}
\end{table}

\clearpage
\subsection{Strategy Prompting}\label{app:strategy}
Here we introduce the mapping of dialogue strategies to their natural language prompts, which is used as \texttt{[action]} for instructing the dialogue system to take the action.

\noindent \textbf{Negotiation Dialogue} ~ \citet{acl21-negotiate-tom} annotated 11 negotiation strategies for the CraisglistBargain dataset \citep{emnlp18-negotiate}. We present these strategies and their natural language prompts for LLMs in Table \ref{tab:negotiate_strat}. 


\begin{table}[h]
    \centering
    \begin{tabular}{l|p{9.5cm}}
        Dialogue Strategy & Natural Language Form \\
        \hline
        Greetings & Please say hello or chat randomly.\\
        Ask a question & Please ask any question about product, year, price, usage, etc.\\
        Answer a question & Please provide information about the product, year, usage, etc.\\
        Propose the first price & Please initiate a price or a price range for the product.\\
        Propose a counter price & Please propose a new price or a new price range.\\
        Use comparatives & Please propose a vague price by using comparatives with existing price. \\
        Confirm information & Please ask a question about the information to be confirmed.\\
        Affirm confirmation & Please give an affirmative response to a confirm.\\
        Deny confirmation & Please give a negative response to a confirm.\\
        Agree with the proposal & Please agree with the proposed price.\\
        Disagree with a proposal & Please disagree with the proposed price.\\
    \end{tabular}
    \caption{Mapping of negotiation strategies to natural language prompting.}
    \label{tab:negotiate_strat}
\end{table}

\noindent \textbf{Emotional Support Dialogue} ~ The ESConv dataset \citep{esconv} is annotated with 8 emotional support strategies. We present these strategies and their natural language prompts for LLMs in Table \ref{tab:esc_strat}. 

\begin{table}[h]
    \centering
    \begin{tabular}{l|p{9cm}}
        Dialogue Strategy & Natural Language Form \\
        \hline
        Question & The Therapist asks the Patient to elaborate on the situation they just described. \\
        Self-disclosure & The Therapist provides a statement relating to the Patient about the situation they just described. \\
        Affirmation and Reassurance & The Therapist provides affirmation and reassurance to the Patient on the situation they just described. \\
        Providing Suggestions & The Therapist provides suggestions to the Patient on the situation they just described. \\
        Reflection of feelings & The Therapist acknowledges the Patient’s feelings about the situation they described. \\
        Information & The Therapist provides factual information to help the Patient with their situation. \\
        Restatement or Paraphrasing & The Therapist acknowledges the Patient’s feelings by paraphrasing their situation. \\
        Others & The Therapist chats with the Patient. \\
    \end{tabular}
    \caption{Mapping of emotional support strategies to natural language prompting.}
    \label{tab:esc_strat}
\end{table}

\noindent \textbf{Tutoring Dialogue} ~ The CIMA dataset \citep{esconv} is annotated with 5 tutoring strategies. We present these strategies and their natural language prompts for LLMs in Table \ref{tab:tutor_strat}. 

\begin{table}[h]
    \centering
    \begin{tabular}{l|p{10cm}}
        Dialogue Strategy & Natural Language Form \\
        \hline
        Hint & The Teacher provides knowledge to the Student via a hint.\\
        Open-ended Question & The Teacher asks a question to the Student to determine the Student’s understanding or continue the conversation. \\
        Correction & The Teacher corrects a mistake or addresses a misconception the Student has. \\
        Confirmation & The Teacher confirms the Student’s answer or understanding is correct. \\
        Others & The Teacher chats with the Student. \\
    \end{tabular}
    \caption{Mapping of pedagogical strategies to natural language prompting.}
    \label{tab:tutor_strat}
\end{table}


\subsection{Reward Model}\label{app:reward_prompt}

In terms of different conversational goals, the prompts for the reward model are designed to assess the degree of goal completeness. 


\noindent \textbf{Negotiation Dialogue} ~ 
As the goal of the negotiation dialogues is to reach a deal and maximize the benefit on the assistant side, the reward model need to first assess whether the user and the assistant has reached a deal, and then extract the final deal price to measure the benefit. 
As shown in Table \ref{tab:cb_reward}, there are two options for the reward model, deal or no deal. If reached a deal, then the reward model will further extract the deal price. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
    \toprule
    System   &  Given a conversation between a Buyer and a Seller, please decide whether the Buyer and the Seller have reached a deal at the end of the conversation. \\
    \midrule
    User    &  Please decide whether the Buyer and the Seller have reached a deal at the end of the conversation. If they have reached a deal, please extract the deal price as [price]. You can only reply with one of the following sentences: They have reached a deal at [price]. They have not reached a deal. \\
    \\
    & The following is the conversation: Buyer: Can we meet in the middle at \$15? Seller: Sure, let's meet at \$15 for this high-quality balloon.\\
    & Question: Have they reached a deal? Answer: They have reached a deal at \$15.\\
    \\
    &The following is the conversation: Buyer: That's still a bit high, can you go any lower? Seller: Alright, I can sell it to you for \$15.\\
    & Question: Have they reached a deal? Answer: They have not reached a deal.\\
    \\
    &The following is the conversation: \texttt{[conversation]}\\
    & Question: Have they reached a deal? Answer: \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for reward model in negotiation dialogues.}
    \label{tab:cb_reward}
\end{table}

\noindent \textbf{Emotional Support Dialogue} ~ 
As the ultimate goal of the emotional support dialogues is to solve the patient's emotional issue, we design four levels of rewards to assess the progress of the emotional support dialogue, as presented in Table \ref{tab:esc_reward}. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
    \toprule
    System   &  Given a conversation between a Therapist and a Patient, please assess whether the Patient' emotional issue has been solved after the conversation. \\
    \midrule
    User    &  You can only reply with one of the following sentences: \\
    & No, the Patient feels worse. \\
    & No, the Patient feels the same. \\
    & No, but the Patient feels better. \\
    & Yes, the Patient's issue has been solved.\\
    &\\
    & The following is a conversation about \texttt{[emotion\_type]} regarding \texttt{[problem\_type]}: \texttt{[conversation]}\\
    & Quetion: Has the Patient's issue been solved? Answer: \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for reward model in emotional support dialogues.}
    \label{tab:esc_reward}
\end{table}

\noindent \textbf{Tutoring Dialogue} ~ 
As the goal of the tutoring dialogues is to teach the student to correctly answer the exercise, we design four levels of rewards to assess the progress of the tutoring dialogue, as presented in Table \ref{tab:cima_reward}. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
    \toprule
    System   &  Given a conversation between a Teacher and a Student, please assess whether the Student correctly translate the English sentence into Italian in the conversation. \\
    \midrule
    User    &  Please assess whether the Student correctly translated the whole sentence of ``\texttt{[exercise]}" into Italian in the conversation. You can only reply with one of the following sentences: \\
    & No, the Student made an incorrect translation. \\
    & No, the Student did not try to translate. \\
    & No, the Student only correctly translated a part of ``\texttt{[exercise]}". \\
    & Yes, the Student correctly translated the whole sentence of ``\texttt{[exercise]}".\\
    \\
    & The following is the conversation: \texttt{[conversation]}\\
    &Question: Did the Student correctly translate the whole sentence of ``\texttt{[exercise]}" into Italian? Answer: \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for reward model in tutoring dialogues.}
    \label{tab:cima_reward}
\end{table}


\subsection{Baselines}\label{app:baseline_prompt}

As for the baseline LLM-based dialogue systems, following their original designs, we adapt these systems into the applications studied in our experiments. 

\noindent \textbf{Ask-an-Expert} \citep{acl23-askanexpert} ~ 
This method is originally proposed for emotional support dialogues, whose prompt is directly adopted in our experiments. 
In order to further accommodate the other two applications, we simply change the role in the pre-defined questions for asking the expert LLM, as shown in Table \ref{tab:ane_prompt}. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
    \toprule
    \multicolumn{2}{c}{\textbf{Negotiation Dialogues}}\\
    \midrule
    System   &  Assume you are the bargain expert to reach a better deal with the seller. Given the conversation history, answer the question. Please answer with only one short and succinct sentence.\\
    \midrule
    User    &  The following is the conversation history: [conversation]\\
    & Question: How did the seller feel? Answer:  \\
    \midrule
    Assistant& [answer1] \\
    \midrule
    User    &  The following is the conversation history: [conversation]\\
    & Question: Why did the seller feel that way? Answer:  \\
    \midrule
    Assistant & [answer2] \\
    \midrule
    User    &  The following is the conversation history: [conversation]\\
    & Question: What should the buyer do? Answer:   \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textbf{Emotional Support Dialogues}}\\
    \midrule
    System   &  Assume you are a therapist expert to help the patient reduce their emotional distress and help them understand and work through the challenges. Given the conversation history, answer the question. Please answer with only one short and succinct sentence. \\
    \midrule
    User    &  The following is the conversation history: [conversation]\\
    & Question: How did the patient feel? Answer:  \\
    \midrule
    Assistant& [answer1] \\
    \midrule
    User    &  The following is the conversation history: [conversation]\\
    & Question: Why did the patient feel that way? Answer:  \\
    \midrule
    Assistant & [answer2] \\
    \midrule
    User    &  The following is the conversation history: [conversation]\\
    & Question: What should the therapist do? Answer:   \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textbf{Tutoring Dialogues}}\\
    \midrule
    System   &  Assume you are the teaching expert to teach the student to translate the English sentence into Italian. Given the conversation history, answer the question. Please answer with only one short and succinct sentence. \\
    \midrule
    User    &  The following is the conversation history: [conversation]\\
    & Question: How did the student feel? Answer:  \\
    \midrule
    Assistant& [answer1] \\
    \midrule
    User    &  The following is the conversation history: [conversation]\\
    & Question: Why did the student feel that way? Answer:  \\
    \midrule
    Assistant & [answer2] \\
    \midrule
    User    &  The following is the conversation history: [conversation]\\
    & Question: What should the teacher do? Answer:   \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for implementing Ask-an-Expert \citep{acl23-askanexpert}.}
    \label{tab:ane_prompt}
\end{table}

\noindent \textbf{Proactive} \citep{llm-proactive} ~ 
This method is originally proposed for negotiation dialogues, whose prompt\footnote{\url{https://github.com/dengyang17/LLM-Proactive}} is directly adopted in our experiments. 
In order to further accommodate the other two applications, we just need to modify the task instruction and the candidate set of dialogue strategies, as presented in Table \ref{tab:proactive_prompt}. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
    \toprule
    \multicolumn{2}{c}{\textbf{Negotiation Dialogues}}\\
    \midrule
    System   &  Assume you are the buyer. Given the conversation history, in order to reach a better deal with the seller, please select the most appropriate dialogue strategy.\\
    \midrule
    User    &  You can only reply by selecting one of the following dialogue strategy to reach the goal: Greetings. Ask a question. Answer a question. Propose the first price. Propose a counter price. Use comparatives. Confirm information. Affirm confirmation. Deny confirmation. Agree with the proposal. Disagree with a proposal. \\
    & The following is the conversation history: [conversation]\\
    & Question: Which one is the most appropriate dialogue strategy? Answer: \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textbf{Emotional Support Dialogues}}\\
    \midrule
    System   &  Assume you are the therapist. Given the conversation history, in order to help the patient reduce their emotional distress and help them understand and work through the challenges, please select the most appropriate dialogue strategy.\\
    \midrule
    User    &  You can only reply by selecting one of the following dialogue strategy to reach the goal: Question. Self-disclosure. Affirmation and Reassurance. Providing Suggestions. Reflection of feelings. Information. Restatement or Paraphrasing.\\
    & The following is the conversation history: [conversation]\\
    & Question: Which one is the most appropriate dialogue strategy? Answer: \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textbf{Tutoring Dialogues}}\\
    \midrule
    System   &  Assume you are the teacher. Given the conversation history, in order to teach the student to translate the English sentence into Italian, please select the most appropriate dialogue strategy. \\
    \midrule
    User    &  You can only reply by selecting one of the following dialogue strategy to reach the goal: Hint. Open-ended Question. Correction. Confirmation. Others.\\
    & The following is the conversation history: [conversation]\\
    & Question: Which one is the most appropriate dialogue strategy? Answer: \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for implementing Proactive prompting schemes \citep{llm-proactive}.}
    \label{tab:proactive_prompt}
\end{table}


\noindent \textbf{ProCoT} \citep{llm-proactive} ~ 
Similar to Proactive prompting scheme, this method is originally proposed for negotiation dialogues, whose prompt\footnote{\url{https://github.com/dengyang17/LLM-Proactive}} is directly adopted in our experiments. 
In order to further accommodate the other two applications, we just need to modify the task instruction and the candidate set of dialogue strategies, as presented in Table \ref{tab:procot_prompt}. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
    \toprule
    \multicolumn{2}{c}{\textbf{Negotiation Dialogues}}\\
    \midrule
    System   &  Assume you are the buyer. Given the conversation history, in order to reach a better deal with the seller, please first analyse the current bargain progress and the buyer's target price in a concise summary, then select one of the following dialogue strategy: Greetings. Ask a question. Answer a question. Propose the first price. Propose a counter price. Use comparatives. Confirm information. Affirm confirmation. Deny confirmation. Agree with the proposal. Disagree with a proposal.  \\
    \midrule
    User    &  The answer should start with a concise analysis of the current bargain progress and the buyer's target price, and then follow by ``To reach this goal, the most appropriate strategy is []".\\
    & The following is the conversation history: [conversation]\\
    & Question: How is the current bargain progress and the buyer's target price, and which one is the most appropriate dialogue strategy? Answer:  \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textbf{Emotional Support Dialogues}}\\
    \midrule
    System   &  Assume you are the therapist. Given the conversation history, in order to help the patient reduce their emotional distress and help them understand and work through the challenges, please first analyse the current therapy progress and the patient's emotional state in a concise summary, then select one of the following dialogue strategy: Question. Self-disclosure. Affirmation and Reassurance. Providing Suggestions. Reflection of feelings. Information. Restatement or Paraphrasing.\\
    \midrule
    User    &  The answer should start with a concise analysis of the current therapy progress and the patient's emotional state, and then follow by ``To reach this goal, the most appropriate strategy is []".\\
    &The following is the conversation history: [conversation] \\
    & Question: How is the current therapy progress and the patient's emotional state, and which one is the most appropriate dialogue strategy? Answer: \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textbf{Tutoring Dialogues}}\\
    \midrule
    System   &  Assume you are the teacher. Given the conversation history, in order to teach the student to translate the English sentence into Italian, please first analyse the current tutoring progress and the student's knowledge state in a concise summary, then select one of the following dialogue strategy from Hint. Open-ended Question. Correction. Confirmation. Others.\\
    \midrule
    User    &  The answer should start with a concise analysis of the current tutoring progress and the student's knowledge state, and then follow by ``To reach this goal, the most appropriate strategy is []".\\
    & The following is the conversation history: [conversation]\\
    & Question: How is the current tutoring progress and the student's knowledge state, and which one is the most appropriate dialogue strategy? Answer: \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for implementing ProCoT prompting schemes \citep{llm-proactive}.}
    \label{tab:procot_prompt}
\end{table}

\noindent \textbf{ICL-AIF} \citep{negotiate-selfplay} ~ 
This method is originally proposed for negotiation dialogues, whose prompt\footnote{\url{https://github.com/FranxYao/GPT-Bargaining}} is directly adopted in our experiments. 
In order to further accommodate the other two applications, we just need to modify the task instruction and the role-playing description, as presented in Table \ref{tab:iclaif_prompt}. 

\begin{table}[h]
    \centering
    \begin{tabular}{lp{11cm}}
    \toprule
    \multicolumn{2}{c}{\textbf{Negotiation Dialogues}}\\
    \midrule
    System   &  Now enter the role-playing mode. In the following conversation, you will play as a coach in a bargain game. There will be a buyer and a seller bargaining about a product price. Your task is to read the conversation between the buyer and the seller, then provide suggestions to the buyer about how to buy the product with a lower price. \\
    \midrule
    User    &  Read the following conversation between the buyer and the seller, then give three suggestions to the buyer about how to buy the product with a lower price. Each suggestion should be only one short and succinct sentence.\\
    &The following is the conversation: [conversation] \\
    &Question: What are your suggestions? Answer:  \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textbf{Emotional Support Dialogues}}\\
    \midrule
    System   &  Now enter the role-playing mode. In the following conversation, you will play as a coach in a counselling game. There will be a therapist and a patient talking about some emotional issues. Your task is to read the conversation between the therapist and the patient, then provide suggestions to the therapist about how to help the patient reduce their emotional distress and help them understand and work through the challenges.\\
    \midrule
    User    &  Read the following conversation between the therapist and the patient, then give three suggestions to the therapist about how to help the patient reduce their emotional distress and help them understand and work through the challenges. Each suggestion should be only one short and succinct sentence.\\
    &The following is the conversation: [conversation] \\
    & Question: What are your suggestions? Answer: \\
    \midrule
    \midrule
    \multicolumn{2}{c}{\textbf{Tutoring Dialogues}}\\
    \midrule
    System   &  Now enter the role-playing mode. In the following conversation, you will play as a coach in a tutoring game. There will be a teacher and a student in an Italian class. Your task is to read the conversation between the teacher and the student, then provide suggestions to the teacher about how to teach the student to translate the English sentence into Italian.\\
    \midrule
    User    &  Read the following conversation between the teacher and the student, then give three suggestions to the teacher about how to teach the student to translate the English sentence into Italian. Each suggestion should be only one short and succinct sentence.\\
    &The following is the conversation: [conversation]\\ 
    &Question: What are your suggestions? Answer: \\
    \bottomrule
    \end{tabular}
    \caption{Prompts for implementing ICL-AIF \citep{negotiate-selfplay}.}
    \label{tab:iclaif_prompt}
\end{table}





\clearpage
\section{Example Conversations}\label{app:case}
We present example conversations produced by different dialogue systems interacting with the same user simulator. 

Tables \ref{tab:cb_case_1}, \ref{tab:cb_case_2}, and \ref{tab:cb_case_3} show the example negotiation conversations. 
In this case, the buyer and the seller are bargaining at the price of a furniture. The listed price is \$150, while the target price of the buyer is \$135, which requires to be reached as closed as possible by the dialogue system. 
As presented in the tables, there are several observations as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Standard} (Table \ref{tab:cb_case_1}). The Standard prompting scheme directly reveals the buyer's budget at the beginning and steadfastly adheres to it without using any negotiation strategies, which leads to no deal in this conversation. 
    \item \textbf{Ask-an-Expert} (Table \ref{tab:cb_case_1}). The Ask-an-Expert methods employ effective actions to efficiently reach a deal with the seller. However, the expert LLM's suggestions let the buyer make a large compromise in the deal. 
    \item \textbf{ProCoT} (Table \ref{tab:cb_case_2}). The ProCoT prompting scheme adopts effective negotiation strategies to reach a much better deal with the seller. 
    \item \textbf{ICL-AIF} (Table \ref{tab:cb_case_2}). The ICL-AIF method only provides dialogue-level strategy suggestions. Although all these strategies are adopted in the conversation, it fails to mitigate the dynamic user interactions, which leads to no deal at the end. 
    \item \textbf{PPDPP} (Table \ref{tab:cb_case_3}). The proposed PPDPP, similar to ProCoT, can employ effective negotiation strategies to reach a much better deal. Furthermore, when the seller expresses his/her willingness to make a compromise, the buyer tries to maximize his/her benefit in the deal. 
\end{itemize}


\input{case/cb_case}



Tables \ref{tab:esc_case_1}, \ref{tab:esc_case_2},  \ref{tab:esc_case_3}, and \ref{tab:esc_case_4} show the example emotional support conversations. 
In this case, the patient encounters the job crisis issue and feels fear, which requires to be solved by the the dialogue system. 
As presented in the tables, there are several observations as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Standard} (Table \ref{tab:esc_case_1}). The Standard prompting scheme consistently convey empathy to the patient for multiple turns, which becomes less useful when the patient's emotional intensity has been reduced. 
    \item \textbf{Ask-an-Expert} (Table \ref{tab:esc_case_2}). The Ask-an-Expert method employs detailed actions to produce an engaging conversation with the patient, but it shares the same drawback of Standard prompting. 
    \item \textbf{ProCoT} (Table \ref{tab:esc_case_3}). The ProCoT prompting scheme adopts effective emotional support strategies to efficiently solve the patient's issue by providing effective suggestions. 
    \item \textbf{ICL-AIF} (Table \ref{tab:esc_case_4}). Similar to negotiation dialogues, the ICL-AIF method only provides dialogue-level strategy suggestions. 
    However, in emotional support dialogues \citep{esconv}, \citet{helping-skill} state that there are three stages that can be flexibly adjusted to meet the help-seeker’s needs. 
    We observe that the three suggestions given by the coach LLM typically contain the strategies in these three stages. 
    Therefore, ICL-AIF effectively interacts with the patient by going through these three stages of emotional support dialogues. 
    \item \textbf{PPDPP} (Table \ref{tab:esc_case_4}). The proposed PPDPP method optimizes the policy planner to efficiently achieve the goal, which leads to a fewer turns of conversations in emotional support dialogues. 
\end{itemize}

\input{case/esc_case}
\end{document}