\section{METHODOLOGY}
\label{sect:method}

% Map prediction exhibits the same form as the matrix completion if the map is represented by a matrix. Typical forms of map encountered in environmental monitoring include occupancy map, elevation map~(terrain map), and certain interested value 2D distribution map~(e.g., temperature distribution, $CO_2$ distribution, etc.). If the values are discretely distributed in a 2D plane, then the map will become a matrix. In this paper, we call those maps as feature maps and address the real-time prediction of a partially observed urban buildings layout map, in which the places identified~(for example, by an aerial vehicle from the air) as having buildings are treated as featured spaces, while other places as non-featured spaces. 

% In this section we first describe the definition of the general LRMC problem. Then we list two important assumptions required by LRMC model for the underlying feature map and we illustrate that maze-like environments satisfy those assumptions. Next, we give a reformulation of LRMC problem and lastly, we consider the incremental sampling process in environmental mapping scenarios and convert the map sampling problem into a way-points sampling. We also describe our proposed non-myopic planning method that is used to find an efficient path among the sampled way points.

\subsection{Problem Description}
Map prediction exhibits the same form as the matrix completion \cite{recht2009simpler} while a 2D map can be treated as a matrix. We propose to use LRMC as the map prediction method. LRMC is defined as completing a partially observed matrix $\tilde{\mathbf{M}}\in\mathbb{R}^{n_1\times n_2}$ whose part of entries are missing and the corresponding ground-truth matrix $\mathbf{M}$ has a low-rank and incoherent structures. Since the ground-truth rank is usually unknown, a typical way to complete $\tilde{\mathbf{M}}$ is to find a minimal rank matrix $\mathbf{X}^*$ that is consistent with $\tilde{\mathbf{M}}$ on the observed entries. A formal mathematical definition is:
\begin{equation}
    \label{eq:mc_1}
    \begin{aligned}
        \mathbf{X}^* = ~&\underset{\mathbf{X}}{\text{argmin}} 
               &&rank(\mathbf{X})\\
               &\text{subject to}
               && X_{ij} = M_{ij},~~\left ( i, j \right )\in \Omega,
    \end{aligned}
\end{equation}
where $\Omega \subset \left \{ 1, \cdots, n_1 \right \}\times \left \{ 1, \dots, n_2 \right \} $ is an index set of locations corresponding to the observed entries~($\left ( i, j \right )\in \Omega$ if $M_{ij}$ is observed). 

\subsection{Low-Rank and Incoherent Structures}
Two important assumptions for the underlying matrix $\mathbf{M}$ must be satisfied to guarantee the matrix completion results:
\begin{itemize}
    \item 
    $\mathbf{M}$ should be a low-rank matrix. A low-rank matrix implies its rank value is much smaller than its dimension and this structure enables the possibility to leverage linear dependencies among columns and/or rows of a matrix to predict missing entries~\cite{ongie2018tensor}. 
    \item
    $\mathbf{M}$ should be an incoherent matrix, which means the coherence of $\mathbf{M}$ is low. Suppose that a \textit{compact} SVD of a rank-r matrix $\mathbf{M}\in \mathbb{R}^{n_1\times n_2}$ is $\mathbf{M} = \mathbf{U}\Sigma\mathbf{V}^T$, where $\mathbf{U} \in \mathbb{R}^{n_1\times r}$ and $\mathbf{V}\in \mathbb{R}^{n_2\times r}$ are stacked left and right singular vectors of $\mathbf{M}$, while $\Sigma \in \mathbb{R}^{r\times r}$ is the diagonal matrix formed by a number of $r$ singular values of $\mathbf{M}$. The statistical leverage scores of $\mathbf{U}$ and $\mathbf{V}$ are defined by $\gamma_{U} = \underset{i\in \left \{ 1, \cdots, n_1 \right \}}{\text{max}}\left \| \mathbf{U}_{i,~:} \right \|^2_2$ and $\gamma_{V} = \underset{j\in \left \{ 1, \cdots, n_2 \right \}}{\text{max}}\left \| \mathbf{V}_{j,~ :} \right \|^2_2$, respectively, where $\mathbf{U}_{i,~:}$ and $\mathbf{V}_{j,~:}$ represent the $i^{th}$ and $j^{th}$ row of $\mathbf{U}$ and $\mathbf{V}$. Then the coherence of $\mathbf{M}$ is defined as:
    \begin{equation}
        \label{eq:coherence}
        \gamma_{M} = \max \left \{ \gamma_{U}, \gamma_{V} \right \}.
    \end{equation}
    The value of $\gamma_M$ is within $\left [ 0, 1 \right ]$ and an incoherent structure means the value of $\gamma_M$ is near 0. A small coherence value implies that the mass of values in the matrix will not concentrate on small portions, but spread out through the whole matrix.
\end{itemize}

Under the two conditions, the matrix $\tilde{\mathbf{M}}$ has been shown to be provably recoverable~\cite{candes2009exact}. Fig.~\ref{fig:mazes_1} and Table~\ref{tb:maze_prop} illustrate the trends of ranks and coherences of different structured environments which are similar to complex versions of either urban road networks or building layouts.
%urban layouts (outdoor) or building floor plans (indoor).   
We can observe that the typical structured environments have ranks below 100, which are significantly smaller compared to the original matrix dimensions which could be multiple magnitudes higher (e.g., $10^4$ or above). 
The trend of ranks also reflects the complexity of the environment, and the higher the rank is, the more complex the environment is.
Along with the rank values, we also list the coherence values computed by using Eq.~(\ref{eq:coherence}).
%The incoherence prevents the information of the row and column spaces of the matrix from being too concentrated in a few rows or columns.
Although the coherence increases proportionally to the rank, they all stay at a low level indicating high incoherence. %where we can be convinced they all are incoherent. 

\begin{figure} \vspace{-3pt}
  \centering
%   \subfigure[]
  	{\label{fig:mazes_1}\includegraphics[width=0.99\linewidth]{figs/mazes_with_ranks.png}}
  \caption{\small Maze environments with different ranks. %The value of 1.0 represents the featured space while 0.2 is non-featured space. Here we use 0 to represent missing parts, the color-map range from 0.2 (non-featured space) to 1.0 (featured space).
  } \vspace{-10pt}
\label{fig:mazes_1}  
\end{figure}

\begin{table}[h]

\caption{Matrix properties of maze environments} %title of the table
\centering % centering table
\begin{tabularx}{\linewidth}{YYYYYYY}%{c rrrrrrr} % creating eight columns
\hline\hline %inserting double-line
% Properties&\multicolumn{7}{c}{Sum of Extracted Bits} \\ [0.5ex]
   & (a) & (b) & (c) & (d) & (e) & (f)\\
\hline % inserts single-line
Rank & 2 & 7 & 11 & 14 & 19 & 22\\ % Entering row contents
Coherence & 0.0200 & 0.0400 & 0.0625 & 0.0833 & 0.1000 & 0.1250\\ % [1ex] adds vertical space
\hline % inserts single-line
\end{tabularx}
\label{tb:maze_prop}
\end{table}


\subsection{Solving LRMC in Map Prediction}
\label{sect:static}

%We model the map prediction problem as a LRMC problem. 
To solve the LRMC problem, %Eq.~\eqref{eq:mc_1} gives a formal mathematical formulation for LRMC. 
the rank minimization problem in Eq.~(\ref{eq:mc_1}) is non-convex and NP-hard unfortunately, and hence of little practical use. An alternative way for Eq.~(\ref{eq:mc_1}) is to replace $rank(\mathbf{X})$ with $\left \| \mathbf{X} \right \|_*$, where $\left \| \mathbf{X} \right \|_*$ is the nuclear norm of $\mathbf{X}$, that is, the sum of sigular values of $\mathbf{X}$: $\left \| \mathbf{X} \right \|_* = \sum_{k=1}^{\min \left \{ n_1, n_2 \right \}}\sigma_k(\mathbf{X})$, where $\sigma_k(\mathbf{X})$ is the $k^{th}$ largest singular values of $\mathbf{X}$. The new formulation is:
\begin{equation}
    \label{eq:mc_2}
    \begin{aligned}
        \mathbf{X}^* = ~&\underset{\mathbf{X}}{\text{argmin}} 
               &&\left \| \mathbf{X} \right \|_*\\
               &\text{subject to}
               && X_{ij} = M_{ij},~~\left ( i, j \right )\in \Omega.
    \end{aligned}
\end{equation}
The nuclear norm $\left \| \mathbf{X} \right \|_*$ is an effective convex relaxation to the rank objective and the optimization in Eq.~(\ref{eq:mc_2}) could be solved by semi-definite programming~\cite{laurent2014positive}.

Nevertheless, the optimizations in Eq.~(\ref{eq:mc_1}) and Eq.~(\ref{eq:mc_2}) aim to exactly recover the partially observed matrix, and this way was claimed as too rigid and may result in over-fitting~\cite{mazumder2010spectral}. A more robust way is to add a regularization parameter in Eq.~(\ref{eq:mc_2}):
\begin{equation}
    \label{eq:mc_3}
    \begin{aligned}
        \mathbf{X}^* = ~&\underset{\mathbf{X}}{\text{argmin}} 
               &&\left \| \mathbf{X} \right \|_*\\
               &\text{subject to}
               && \sum_{(i, j)\in \Omega}(X_{ij} - M_{ij})^2\leq \delta,
    \end{aligned}
\end{equation}
where $\delta \geq 0$ is a parameter regulating the training error tolerance. Equivalently Eq.~(\ref{eq:mc_3}) could be reformulated in a Lagrange form:
\begin{equation}
    \label{eq:mc_4}
    \begin{aligned}
        \mathbf{X}^* = \underset{\mathbf{X}}{\text{argmin}} \sum_{(i, j)\in \Omega} (X_{ij} - M_{ij})^2 + \lambda \left \|\mathbf{X} \right \|_{*},
    \end{aligned}
\end{equation}
where $\lambda$ is a regularization parameter controlling the nuclear norm. A SVD based iterative method proposed in~\cite{mazumder2010spectral} can be used to solve the Eq.~(\ref{eq:mc_4}).


\subsection{Mapping with Non-myopic Planner}
\label{sect:plan}

We are intrigued if the proposed map prediction can facilitate and improve existing environmental mapping and coverage methods. Popular planning paradigms include lawnmower mapping~\cite{li2019coverage, song2018varepsilon}, myopic (short-horizon) planning~\cite{khan2014greedy}, as well as non-myopic (long-horizon) planning~\cite{ kantaros2019asymptotically}. Lawnmower planning pursues complete spatial coverage, and myopic methods are oftentimes greedy schemes. The non-myopic approaches have been favored by many researchers for environmental mapping and monitoring~\cite{ma2016information, chen2019pareto}. Since non-myopic planners optimize paths over some horizon, we adopt and adapt the standard  \textit{Traveling Salesman Problem}~(TSP), which will be used in our evaluation in the experimental section. 

%With the predicted map, a non-myopic planer may be available to the mapping problem.
%The observation set $\Omega$ in LRMC typically is given at one-time. However, 
% In robotic environmental mapping scenarios, the observed/measured samples $\Omega$ are streamed along the robot traversal. 
% %cannot be given at one-time and the construction of $\Omega$ relies on the robot traversing the whole mapping space---this belongs to a coverage planning problem. 
% Therefore, we need to first determine a set of way points for future navigation guidance. 
%that need to be reached incrementally by the robot. A well-known efficient non-myopic planning method to find a path among a set of known locations is to solve the \textit{Traveling Salesman Problem}~(TSP). 
A TSP %definition in coverage planning 
can be defined as that, given the positions of a collection of way points, what is the shortest possible path of reaching each way point only once and eventually returning to the starting point. 
To efficiently solve the TSP, we extend our recently developed adaptive \textit{k}-opt method~\cite{ma2016adaptive} which exploits and modifies the adaptive $k$-swap mechanism originally designed for multi-agent
task allocation. The core idea is to approximate the NP-hard problem with an opportunistic and localized tour improvement heuristic by taking account of tour
constraints adaptively and flexibly, so that real-time solution can be obtained with small sacrifice on solution accuracy. 
%Different from existing \textit{k}-opt methods, the parameter $k$ in the adaptive \textit{k}-opt method is adjusted adaptively as the tour improvement proceeds. 

To determine the number of the way points for navigation guidance in TSP, previous work~\cite{candes2009exact} has theoretically proven that a partially observed low-rank matrix could be perfectly recovered if the number of sampled entries $m$ obeys:
\begin{equation}
    \label{eq:sampled_cond}
        m\geq Cn^{1.2}r\log n,
\end{equation}
where $C$ is a positive numerical coefficient, $n = \max\left \{ n_1, n_2 \right \}$, and $r$ is the rank of the underlying matrix $\mathbf{M}$~\cite{candes2009exact}. Based on Eq.~(\ref{eq:sampled_cond}),
%we consider a static environmental modeling scenario, i.e., static sensor placement problem, where the number of locations for placing sensors 
the placements of way points
can be defined %based on Eq.~(\ref{eq:sampled_cond}) 
as:
\begin{equation}
    \label{eq:sample_num}
    N^{'}_{s} = \left \lfloor \frac{C\cdot n^{1.2}r\log n}{N} \right \rfloor
\end{equation}
where $N^{'}_{s}$ represents the number of way points and $N$ is the number of observed map samples within each way point's local sensing range. 
Note that, in coverage planning scenarios, the robot can take observations in between any pair of successive sampling way points and may introduce additional observations. This implies we may want to reduce the number of sampled way points, %as suggested by Eq.~(\ref{eq:sample_num}) 
and we do this by using a scale factor $\epsilon$, so that $N_{s} = \left \lfloor \epsilon N^{'}_{s} \right \rfloor$, 
% \begin{equation}
%     \label{eq:new_sample_num}
%     N_{s} = \left \lfloor \epsilon N^{'}_{s} \right \rfloor = \left \lfloor \epsilon \frac{C\cdot n^{1.2}r\log n}{N} \right \rfloor
% \end{equation}
where $N_{s}$ is the number of sampled way points in TSP planning. 
We name this TSP based planning with different $\epsilon$ values as $TSP\_\epsilon$. 
Once a number of $N_{s}$ points are sampled, a fast and  near-shortest path will be formed using the adaptive \textit{k}-opt TSP solver and thereafter our robot could start the coverage mission by following the path. 
%where the $\epsilon$ is the scale factor in Eq.~(\ref{eq:new_sample_num}).


