\section{PRELIMINARIES}

\subsection{Low-Rank Matrix Completion}
\textit{Low-Rank Matrix Completion} is defined as completing a partially observed matrix $\tilde{\mathbf{M}}\in\mathbb{R}^{n_1\times n_2}$ whose part of entries are missing and the corresponding ground-truth matrix $\mathbf{M}$ has a low-rank structure, which indicates $rank(\mathbf{M})\ll \min \left \{ n_1, n_2 \right \}$. Since the ground-truth rank is unknown at most times, a typical way to complete $\tilde{\mathbf{M}}$ is to find a minimal rank matrix $\mathbf{X}^*$ that is consistent with $\tilde{\mathbf{M}}$ on the observed entries. A formal mathematical definition is:
\begin{equation}
    \label{eq:mc_1}
    \begin{aligned}
        \mathbf{X}^* = ~&\underset{\mathbf{X}}{\text{argmin}} 
               &&rank(\mathbf{X})\\
               &\text{subject to}
               && X_{ij} = M_{ij},~~\left ( i, j \right )\in \Omega,
    \end{aligned}
\end{equation}
where $\Omega \subset \left \{ 1, \cdots, n_1 \right \}\times \left \{ 1, \dots, n_2 \right \} $ is an index set of locations corresponding to the observed entries~($\left ( i, j \right )\in \Omega$ if $M_{ij}$ is observed). Unfortunately, the rank minimization problem in Eq.~(\ref{eq:mc_1}) is non-convex and NP-hard, and hence of little of practical use. An alternative way for Eq.~(\ref{eq:mc_1}) is to replace $rank(\mathbf{X})$ with $\left \| \mathbf{X} \right \|_*$, where $\left \| \mathbf{X} \right \|_*$ is the nuclear norm of $\mathbf{X}$, that is, the sum of sigular values of $\mathbf{X}$: $\left \| \mathbf{X} \right \|_* = \sum_{k=1}^{\min \left \{ n_1, n_2 \right \}}\sigma_k(\mathbf{X})$, where $\sigma_k(\mathbf{X})$ is the $k^{th}$ largest singular values of $\mathbf{X}$. The new formulation is:
\begin{equation}
    \label{eq:mc_2}
    \begin{aligned}
        \mathbf{X}^* = ~&\underset{\mathbf{X}}{\text{argmin}} 
               &&\left \| \mathbf{X} \right \|_*\\
               &\text{subject to}
               && X_{ij} = M_{ij},~~\left ( i, j \right )\in \Omega.
    \end{aligned}
\end{equation}
The nuclear norm $\left \| \mathbf{X} \right \|_*$ is an effective convex relaxation to the rank objective and the optimization in Eq.~(\ref{eq:mc_2}) could be solved in semidefinite programming.

However, the optimizations in Eq.~(\ref{eq:mc_1}) and Eq.~(\ref{eq:mc_2}) aim to exactly recover the partially observed matrix, and this way was claimed as too rigid and may result in over-fitting~\cite{mazumder2010spectral}. A more robust way is to add a regularization parameter in Eq.~(\ref{eq:mc_2}):
\begin{equation}
    \label{eq:mc_3}
    \begin{aligned}
        \mathbf{X}^* = ~&\underset{\mathbf{X}}{\text{argmin}} 
               &&\left \| \mathbf{X} \right \|_*\\
               &\text{subject to}
               && \sum_{(i, j)\in \Omega}(X_{ij} - M_{ij})^2\leq \delta,
    \end{aligned}
\end{equation}
where $\delta \geq 0$ is a parameter regulating the training error tolerance. Equivalently Eq.~(\ref{eq:mc_3}) could be reformulated in a Lagrange form:
\begin{equation}
    \label{eq:mc_4}
    \begin{aligned}
        \mathbf{X}^* = \underset{\mathbf{X}}{\text{argmin}} \sum_{(i, j)\in \Omega} (X_{ij} - M_{ij})^2 + \lambda \left \|\mathbf{X} \right \|_{*},
    \end{aligned}
\end{equation}
where $\lambda$ is a regularization parameter controlling the nuclear norm.
\subsection{Assumptions for Completion Guarantee}
Three important assumptions for the underlying matrix $\mathbf{M}$ and the observation set $\Omega$ must be satisfied to guarantee the matrix completion results according to~\cite{candes2009exact}:
\begin{itemize}
    \item 
    $\mathbf{M}$ should be a low-rank matrix. A low-rank matrix implies its rank value is much smaller than its dimension and this structure enables the possibility to leverage linear dependencies among columns and/or rows of a matrix to impute missing entries~\cite{ongie2018tensor}. 
    \item
    $\mathbf{M}$ should be an incoherent matrix, which means the coherence of $\mathbf{M}$ is low. Suppose that a \textit{compact} SVD of a rank-r matrix $\mathbf{M}\in \mathbb{R}^{n_1\times n_2}$ is $\mathbf{M} = \mathbf{U}\Sigma\mathbf{V}^T$, where $\mathbf{U} \in \mathbb{R}^{n_1\times r}$ and $\mathbf{V}\in \mathbb{R}^{n_2\times r}$ are stacked left and right singular vectors of $\mathbf{M}$, while $\Sigma \in \mathbb{R}^{r\times r}$ is the diagonal matrix formed by the $r$ singular values of $\mathbf{M}$. The statistical leverage scores of $\mathbf{U}$ and $\mathbf{V}$ are defined by $\gamma_{U} = \underset{i\in \left \{ 1, \cdots, n_1 \right \}}{\text{max}}\left \| \mathbf{U}_{i,~:} \right \|^2_2$ and $\gamma_{V} = \underset{j\in \left \{ 1, \cdots, n_2 \right \}}{\text{max}}\left \| \mathbf{V}_{j,~ :} \right \|^2_2$, respectively, where $\mathbf{U}_{i,~:}$ and $\mathbf{V}_{j,~:}$ represent the $i^{th}$ and $j^{th}$ row of $\mathbf{U}$ and $\mathbf{V}$. Then the coherence of $\mathbf{M}$ is defined as:
    \begin{equation}
        \label{eq:coherence}
        \gamma_{M} = \max \left \{ \gamma_{U}, \gamma_{V} \right \}.
    \end{equation}
    \item
    The observation set $\Omega$ should be randomly uniformly sampled and the number of the sampled entries $m$ should obey: 
    \begin{equation}
        \label{eq:sampled_cond}
            m\geq Cn^{1.2}r\log n,
    \end{equation}
    where $C$ is some positive numerical coefficient, $n = \max\left \{ n_1, n_2 \right \}$, and $r$ is the rank of the underlying matrix $\mathbf{M}$~\cite{candes2009exact}.
\end{itemize}


