\documentclass{article}

\usepackage[left=3cm,right=3cm,top=4cm,bottom=4cm]{geometry}

\usepackage{tikz}
\usepackage{amsmath}
\input{math_commands.tex}

\usepackage{todonotes}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %


\usepackage{booktabs}
\usepackage{soul}
\usepackage[numbers]{natbib}
\usepackage{comment}

\usepackage[justification=centering]{subfig}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{fancyhdr}

\usepackage{enumitem}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}




\newcommand{\anvith}[1]{\textcolor{brown}{2D: #1}}




\begin{document}

\date{}

\title{\Large \bf Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD}

\author{Anvith Thudi\\
  University of Toronto and Vector Institute\\
  Hengrui Jia \\
  University of Toronto and Vector Institute\\
  Casey Meehan\\
  University of California, San Diego\\
  Ilia Shumailov\\
  University of Oxford\\
  Nicolas Papernot\\
  University of Toronto and Vector Institute\\
}

\maketitle

\begin{abstract}
Differentially private stochastic gradient descent (DP-SGD) is the canonical approach to private deep learning. While the current privacy analysis of DP-SGD is known to be tight in some settings, several empirical results suggest that models trained on common benchmark datasets leak significantly less privacy for many datapoints. Yet, despite past attempts, a rigorous explanation for why this is the case has not been reached. Is it because there exist tighter privacy upper bounds when restricted to these dataset settings, or are our attacks not strong enough for certain datapoints? In this paper, we provide the first per-instance (i.e., ``data-dependent") DP analysis of DP-SGD. Our analysis captures the intuition that points with similar neighbors in the dataset enjoy better data-dependent privacy than outliers. Formally, this is done by modifying the per-step privacy analysis of DP-SGD to introduce a dependence on the distribution of model updates computed from a training dataset. We further develop a new composition theorem to effectively use this new per-step analysis to reason about an entire training run. Put all together, our evaluation shows that this novel DP-SGD analysis allows us to now \emph{formally} show that DP-SGD leaks significantly less privacy for many datapoints (when trained on common benchmarks) than the current data-independent guarantee. This implies privacy attacks will necessarily fail against many datapoints if the adversary does not have sufficient control over the possible training datasets. 
\end{abstract}


\input{Sections/Introduction}
\input{Sections/Background}
\input{Sections/Analysis_V2}
\input{Sections/Main_Body_Empirical_Results}
\input{Sections/Discussion}
\input{Sections/Conclusion}

\section*{Acknowledgements}
We would like to acknowledge our sponsors, who support our research with financial and in-kind contributions: Amazon, Apple, CIFAR through the Canada CIFAR AI Chair, DARPA through the GARD project, Intel, Meta, NSERC through the COHESA Strategic Alliance and a Discovery Grant, Ontario through an Early Researcher Award, and the Sloan Foundation. Anvith Thudi is supported by a Vanier Fellowship from the Natural Sciences and Engineering Research Council of Canada.  Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute. We would further like to thank Relu Patrascu at the University of Toronto for providing the compute infrastructure needed to perform the experimentation outlined in this work. We would also like to thank members of the CleverHans lab and Mahdi Haghifam for their feedback on drafts of the manuscript.


\bibliographystyle{abbrvnat}
\bibliography{references}

\appendix
\newpage

\include{Sections/Appendix}

\end{document}