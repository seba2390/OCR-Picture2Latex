\section{Detailed Empirical Results}
\label{sec:detailed_emp_res}





In this section we present evaluation for the three different per-step guarantees presented in Section~\ref{sec:analysis}. In evaluating the guarantees they give for DP-SGD, we also consider two update rules $U$: the "sum" $U(X_B) =$ \\ $ \frac{1}{L} \sum_{x \in X_B} \nabla_{\theta}L(\theta,x)/ \max(1,\frac{||\nabla_{\theta}L(\theta,x)||_2}{C})$, and the "mean" $U(X_B) =$ \\ $\frac{1}{|X_B|}\sum_{x \in X_B} \nabla_{\theta}L(\theta,x)/ \max(1,\frac{||\nabla_{\theta}L(\theta,x)||_2}{C})$. Note the subtle difference between dividing by a fixed constant $L$ (typically the expected mini-batch size when Poisson sampling datapoints) and by the mini-batch size $|X_B|$. This means for the sum the upper-bound on sensitivity is $\frac{C}{L}$, while for the mean the upper-bound on sensitivity is only $C$ (consider neighbouring mini-batches of size 1 and 2). In practice, this means using the mean update rule requires far more noise and hence is not practical to use. We will highlight how our analysis by sensitivity distributions allows better analysis of the mean update rule.


\paragraph{Experimental Setup.} In the subsequent experiments, we empirically verify our claimed guarantees on MNIST~\citep{lecun1998mnist} and CIFAR-10~\citep{krizhevsky2009learning}. Unless otherwise specified, LeNet-5~\citep{lecun1989backpropagation} and ResNet-20~\citep{he2016deep} were trained on the two datasets for 10 and 200 epochs respectively, with a mini-batch size equal to 128, clipping norm equal to 1, $\epsilon=10$, and $\delta = 10^{-5}$ (or $\alpha = 8$ in cases of Renyi-DP) 
. 
Regarding hardware, we used NVIDIA T4 to accelerate our experiments. 


\subsection{Studying the $(\epsilon,\delta)$-DP Case}
\label{ssec:eps_delta_experiments}

Here we compute one of the inequalities needed for $(\epsilon,\delta)$-DP given by Corollary~\ref{cor:eps_delta_sens}.
We set $\delta' = 0.5 \times 10^{-5}$ and $\delta'' = 10^{-5}$, and $p = 10^{4}$. For CIFAR-10, we set $\sigma =  0.488, 0.508, 0.566, 0.625, 0.703$ for mini-batch size $16, 32, 64, 128, 256$ respectively; and for MNIST, we set $\sigma =  0.410, 0.430, 0.449, 0.469, 0.508, 0.566$ for mini-batch size $16, 32, 64, 128, 256, 512$ respectively.
In the proceeding figures we plot results with respect to models at three stages of training: before training starts where the model is just randomly initialized, in the middle of the training where the model has been trained for half the number of epochs, and after the training. We further plot distributions of the guarantee over 500 training points, and indicate whether the data points are correctly or incorrectly classified. Note that we conduct the experiments on both mean and sum update rules. For the case of mean update, which requires computing an expectation of the sensitivity distribution $\Delta_{U,x^*}(X_B)$ to some power, we use 100 samples.

To the best of our knowledge, the standard solely $(\epsilon,\delta)$-DP analysis of the sample Gaussian mechanism would give the guarantee of $\epsilon' = \ln( \mathbb{P}_{x^*}(1) e^{C_{\delta,\sigma} \Delta_{U}} + \mathbb{P}_{x^*}(0))$ and $\delta'' = \mathbb{P}_{x^*}(1) \delta$. We use this as our baseline in this subsection.





\paragraph{Comparison to Baseline.} We see in Figures~\ref{fig:ed_eps_distrib_bs_mnist_mean},\ref{fig:ed_eps_distrib_bs_mnist_sum},\ref{fig:ed_eps_distrib_bs_cifar_mean},\ref{fig:ed_eps_distrib_bs_cifar_sum} that our analysis applied to both the mean and sum beat the baseline analysis of the $(\epsilon,\delta)$-DP guarantee. In particular, we see that for checkpoints in the middle and end of the training, we give a privacy guarantee more than a magnitude better than the baseline for the sum or mean. Furthermore, both our and the baseline's guarantee increases for the sum update rule as we increase the expected mini-batch size, however, for the mean update rule our guarantee decreases (concentrating at smaller values for more datapoints) where as the baseline still increases. 







\paragraph{Disparity between Correct and Incorrect.} Shown in Figures~\ref{fig:ed_eps_curve_vary_arch_mnist},\ref{fig:ed_eps_curve_vary_arch_cifar} we see that on average correctly classified data points have better per-step privacy guarantees than incorrectly classified data points across training with different architectures, and this holds most strongly towards the end of training. 


\paragraph{Changing DP strength used to get checkpoints.} Now we re-conduct the previous experiments but with varying epsilon, $\epsilon = 1, 3, 10, 30, 100$ shown in Figures~\ref{fig:ed_eps_curve_vary_eps_mnist},\ref{fig:ed_eps_curve_vary_eps_cifar}. We see that changing the strength of DP used to get the checkpoints increases our guarantees, but less so for correctly classified datapoints.










\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_16_mean.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_32_mean.pdf}
}
\\\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_64_mean.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_128_mean.pdf}
}
\\\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_256_mean.pdf}
}
\subfloat[Mini-batch size = 512]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_512_mean.pdf}
}
\caption{Distribution plots of per-step guarantee given by Corollary~\ref{cor:eps_delta_sens} computed on LeNet-5 trained on MNIST with mean update rule and varying mini-batch sizes of $16, 32, 64, 128, 256, 512$. As specified by the legend labels, we group the plotted guarantees by (a) whether the model is at the initial, middle, or final stage of the training, and (b) whether the point on which the data-dependent guarantee is computed is classified correctly or not by the model. It can be seen that in all settings our guarantee is better than the baseline, which is represented by the dashed purple line, by orders of magnitudes. However, the guarantee distributions of incorrectly classified points and points at the initial stage of training are closer to the baseline compared to the other settings.
}
\label{fig:ed_eps_distrib_bs_mnist_mean}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_CIFAR10_resnet20_16_mean.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_CIFAR10_resnet20_32_mean.pdf}
}

\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_CIFAR10_resnet20_64_mean.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_CIFAR10_resnet20_128_mean.pdf}
}

\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_CIFAR10_resnet20_256_mean.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:ed_eps_distrib_bs_mnist_mean} except we now use ResNet-20 models trained on CIFAR-10. Similar results are observed so we conclude that our guarantee is effective across datasets.
}
\label{fig:ed_eps_distrib_bs_cifar_mean}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_16_sum.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_32_sum.pdf}
}
\\\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_64_sum.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_128_sum.pdf}
}
\\\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_256_sum.pdf}
}
\subfloat[Mini-batch size = 512]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_MNIST_lenet_512_sum.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:ed_eps_distrib_bs_mnist_mean} except we now use a sum update rule. Unlike the case of using the mean update rule, we now see that our guarantees are more similar to the baseline. Note that the magnitude of our guarantees here does not significantly differ from the case of the mean update rule. The reason for this observation is that the baseline guarantee being much tighter/smaller as explained in Section~\ref{ssec:exp_hard_renyi}.
}
\label{fig:ed_eps_distrib_bs_mnist_sum}
\end{figure}


\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_CIFAR10_resnet20_16_sum.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_CIFAR10_resnet20_32_sum.pdf}
}
\\\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_CIFAR10_resnet20_64_sum.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_CIFAR10_resnet20_128_sum.pdf}
}
\\\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_hist_CIFAR10_resnet20_256_sum.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:ed_eps_distrib_bs_mnist_sum} except we now use ResNet-20 models trained on CIFAR-10. More of our guarantees concentrate near the baseline guarantee, which may be due to the fact that the CIFAR-10 model has worse accuracy than the MNIST model and we have shown in Section~\ref{ssec:exp_better_privacy} that more accurate points tend to have better data-dependency guarantee.
}
\label{fig:ed_eps_distrib_bs_cifar_sum}
\end{figure}


\begin{figure}[t]
\centering
\subfloat[Update-rule: mean]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_curve_MNIST_lenet_eps_mean.pdf}
}
\subfloat[Update-rule: sum]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_curve_MNIST_lenet_eps_sum.pdf}
}
\caption{
Per-step guarantee given by Corollary~\ref{cor:eps_delta_sens} with respect to the overall DPSGD privacy guarantee used to obtain the models on MNIST. Each curve corresponds to a certain training stage and whether the data points are correctly classified or not. We include both mean and sum update rules and do not observe a significant difference between them.
}
\label{fig:ed_eps_curve_vary_eps_mnist}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Update-rule: mean]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_curve_CIFAR10_resnet20_eps_mean.pdf}
}
\subfloat[Update-rule: sum]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_curve_CIFAR10_resnet20_eps_sum.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:ed_eps_curve_vary_eps_mnist} except we now use ResNet-20 models trained on CIFAR-10. Similar results are observed.
}
\label{fig:ed_eps_curve_vary_eps_cifar}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Update-rule: mean]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_curve_MNIST_arch_mean.pdf}
}
\subfloat[Update-rule: sum]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_curve_MNIST_arch_sum.pdf}
}
\caption{Per-step guarantee given by Corollary~\ref{cor:eps_delta_sens} computed by using models trained on MNIST with different architecture, divided by the per-step DPSGD guarantee (baseline). Each curve corresponds to a certain training stage and whether the data points are correctly classified or not. We include both mean and sum update rules and do not observe a significant difference between them. Within each figure, we can see that our guarantee does not vary significantly across different architectures so we may claim to be independent of datasets. Besides, the disparity of these curves also supports our claim that more accurate points have better privacy guarantees.
}
\label{fig:ed_eps_curve_vary_arch_mnist}
\end{figure}


\begin{figure}[t]
\centering
\subfloat[Update-rule: mean]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_curve_CIFAR10_arch_mean.pdf}
}
\subfloat[Update-rule: sum]
{
\includegraphics[width=0.48\linewidth]{figures/ed_eps_curve_CIFAR10_arch_sum.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:ed_eps_curve_vary_arch_mnist} except we now use ResNet-20 models trained on CIFAR-10. Similar results can be seen so we believe our guarantee given by Corollary~\ref{cor:eps_delta_sens} is independent of the model architecture and the dataset. 
}
\label{fig:ed_eps_curve_vary_arch_cifar}
\end{figure}


\subsection{Studying the Theorem~\ref{thm:easy_renyi_dp} Results}
\label{ssec:eval_easy_renyi}

Here we compute the $(\alpha,\epsilon)$-R\'enyi-DP guarantee given by Theorem~\ref{thm:easy_renyi_dp} and plot results analogous to Section~\ref{ssec:eps_delta_experiments}. We only evaluate the sum using the guarantee of Theorem~\ref{thm:easy_renyi_dp}, as in this case $\Delta_{U,x^*} = \Delta_{U,x^*}(X_B)$ is a constant. For the mean update rule, it is not clear how to get reliable estimates of $\Delta_{U,x^*}$, which is a supremum over mini-batches.






\paragraph{Comparison to Baseline.} We see in Figures~\ref{fig:renyi_simple_eps_distrib_bs_mnist_sum}, \ref{fig:renyi_simple_eps_distrib_bs_cifar_sum} that our analysis applied to sum beats the baseline analysis of the R\'enyi-DP guarantee. In particular, we see that for checkpoints in the middle and end of training, we give a privacy guarante several magnitudes better than the baseline for most datapoints. Considering the effect of the expected mini-batch size, we see both our and the baseline's guarantee increases. However, we see that the datapoints with the most privacy lose several magnitudes of privacy as we increase the expected batch size; that is, all the points get concentrated at more similar privacy guarantees (still magnitudes below the baseline). Considering how our guarantee scales with $\alpha$,  in Figure~\ref{fig:renyi_simple_eps_curve_alpha_mnist_sum},\ref{fig:renyi_simple_eps_curve_alpha_cifar_sum} we see both our and the baseline's guarantees increase. Comparing the relative change, in Figure~\ref{fig:renyi_simple_fraction_eps_curve_alpha_mnist_sum},\ref{fig:renyi_simple_fraction_eps_curve_alpha_cifar_sum} we see we scale proportionally to the baseline with varying $\alpha$.




\paragraph{Disparity between Correct and Incorrect.}  Shown in Figures~\ref{fig:renyi_simple_fraction_curve_vary_arch_mnist} we see that correctly classified data points on average have better per-step privacy guarantees across different architectures.






\paragraph{Expected Guarantees for Composition.} In Figure~\ref{fig:renyi_simple_composition_mnist_sum} we plot the expected guarantee over 10 trials at different steps of training (starting from the same checkpoint for each trial) where training was done with the full training dataset. We are evaluating the guarantee for 100 test points, hence computing the expectations needed to bound $D_{\alpha}(X||X\cup \{x_{\text{test}}\})$ according to Theorem~\ref{thm:better_composition}. We find that our expected guarantee decreases with respect to the baseline guarantee as we progress through training, and this persists regardless of the epsilons used during training (also shown in Figure~\ref{fig:renyi_simple_fraction_eps_curve_vary_eps_mnist_sum}). 

In Figure~\ref{fig:remove_10points} we plot the expected guarantees when generating checkpoints without training on the given datapoint: the max of this quantity and the quantity in Figure~\ref{fig:renyi_simple_composition_mnist_sum} bounds the data-dependent R\'enyi-DP guarantee by Theorem~\ref{thm:better_composition}. 








\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_hist_MNIST_lenet_16_8_sum.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_hist_MNIST_lenet_32_8_sum.pdf}
}
\\\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_hist_MNIST_lenet_64_8_sum.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_hist_MNIST_lenet_128_8_sum.pdf}
}
\\\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_hist_MNIST_lenet_256_8_sum.pdf}
}
\subfloat[Mini-batch size = 512]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_hist_MNIST_lenet_512_8_sum.pdf}
}
\caption{Distribution plots of per-step R\'enyi-DP guarantee given by Theorem~\ref{thm:easy_renyi_dp} computed on LeNet-5 trained on MNIST with sum update rule and varying mini-batch sizes of $16, 32, 64, 128, 256, 512$ at three different training stages: at initialization,  in the middle of training, and after training is finalized. The purple dashed line represents the baseline per-step DP-SGD guarantee (Section 3.3 in~\citet{mironov2019r}). We can see our guarantees computed at just initialized models are mostly at the baseline, whereas as training proceeds, more data points obtain guarantees that are better than the baseline by orders of magnitudes. This impact of training stage holds across different mini-batch sizes, while at the same time, the magnitudes of the guarantees increase as the mini-batch size increases--this may be caused by the increasing sampling rate.
}
\label{fig:renyi_simple_eps_distrib_bs_mnist_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_hist_CIFAR10_resnet20_16_8_sum.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_hist_CIFAR10_resnet20_32_8_sum.pdf}
}
\\\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_hist_CIFAR10_resnet20_64_8_sum.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_hist_CIFAR10_resnet20_128_8_sum.pdf}
}
\\\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_hist_CIFAR10_resnet20_256_8_sum.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:renyi_simple_eps_distrib_bs_mnist_sum} except we now use ResNet-20 models trained on CIFAR-10. We continue to see that some data points are able to obtain significantly better guarantees than the baseline at later stages of training. However, it seems varying mini-batch size does not lead to worse guarantees (and even improves the guarantees for some points). We hypothesize that this may be due to the fact that increasing the mini-batch size for training on CIFAR-10 has a greater impact on model accuracy, which cancels out or even dominates the impact of increasing sampling rate.
}
\label{fig:renyi_simple_eps_distrib_bs_cifar_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_curve_MNIST_lenet_16_sum.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_curve_MNIST_lenet_32_sum.pdf}
}
\\\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_curve_MNIST_lenet_64_sum.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_curve_MNIST_lenet_128_sum.pdf}
}
\\\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_curve_MNIST_lenet_256_sum.pdf}
}
\subfloat[Mini-batch size = 512]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_curve_MNIST_lenet_512_sum.pdf}
}
\caption{Per-step R\'enyi-DP guarantee given by Theorem~\ref{thm:easy_renyi_dp} as a function of $\alpha$, plotted at 3 stages of training and varying mini-batch sizes. It is shown that for all mini-batch sizes, the guarantees at the initial stage overlap with the baseline, whereas the guarantees at the middle and final stages increase slower as $\alpha$ increases. Also see Figure~\ref{fig:renyi_simple_fraction_eps_curve_alpha_mnist_sum} which plots the ratio between our guarantee and the baseline.
}
\label{fig:renyi_simple_eps_curve_alpha_mnist_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_curve_CIFAR10_resnet20_16_sum.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_curve_CIFAR10_resnet20_32_sum.pdf}
}

\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_curve_CIFAR10_resnet20_64_sum.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_curve_CIFAR10_resnet20_128_sum.pdf}
}

\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_curve_CIFAR10_resnet20_256_sum.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:renyi_simple_eps_curve_alpha_mnist_sum} except we now use ResNet-20 models trained on CIFAR-10, and similar results are observed. However, the curves are too close to each other, so for ease of visualization, we plot the ratio between our guarantee and the baseline in Figure~\ref{fig:renyi_simple_fraction_eps_curve_alpha_cifar_sum}.
}
\label{fig:renyi_simple_eps_curve_alpha_cifar_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_fraction_curve_MNIST_lenet_16_sum.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_fraction_curve_MNIST_lenet_32_sum.pdf}
}
\\\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_fraction_curve_MNIST_lenet_64_sum.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_fraction_curve_MNIST_lenet_128_sum.pdf}
}
\\\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_fraction_curve_MNIST_lenet_128_sum.pdf}
}
\subfloat[Mini-batch size = 512]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_fraction_curve_MNIST_lenet_512_sum.pdf}
}
\caption{This is the exact reproduction of Figure~\ref{fig:renyi_simple_eps_curve_alpha_mnist_sum} except we normalize the y-axis by the baseline guarantee. Besides the takeaway mentioned in Figure~\ref{fig:renyi_simple_eps_curve_alpha_mnist_sum}, we can also see that mini-batch size does not have a significant impact  on how our guarantee changes with respect to $\alpha$ except for mini-batch size $=512$. This exception may be because the model with mini-batch size $512$ was not trained to convergence since the number of training epochs was set to $10$ for all MNIST models.
}
\label{fig:renyi_simple_fraction_eps_curve_alpha_mnist_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_fraction_curve_CIFAR10_resnet20_16_sum.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_fraction_curve_CIFAR10_resnet20_32_sum.pdf}
}

\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_fraction_curve_CIFAR10_resnet20_64_sum.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_fraction_curve_CIFAR10_resnet20_128_sum.pdf}
}

\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_alpha_fraction_curve_CIFAR10_resnet20_128_sum.pdf}
}
\caption{This is the exact reproduction of Figure~\ref{fig:renyi_simple_eps_curve_alpha_cifar_sum} except we normalize the y-axis by the baseline guarantee. It can be seen now that at later stages of training, our guarantees increases slower comparing to the baseline guarantee.
}
\label{fig:renyi_simple_fraction_eps_curve_alpha_cifar_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[our guarantee]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_vary_eps_curve_MNIST_lenet_128_sum.pdf}
}
\subfloat[our guarantee divided by baseline]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_vary_eps_fraction_curve_MNIST_lenet_128_sum.pdf}
}

\caption{Per-step R\'enyi-DP guarantee given by Theorem~\ref{thm:easy_renyi_dp} as a function of $\epsilon$, plotted at 3 stages of training and varying mini-batch sizes. The baseline is either (a) plotted in the figure, or (b) used to normalize the plotted guarantees. We can see that our guarantees increase significantly slower than the baseline as  $\epsilon$ increases.
}
\label{fig:renyi_simple_fraction_eps_curve_vary_eps_mnist_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[our guarantee]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_vary_eps_curve_CIFAR10_resnet20_128_sum.pdf}
}
\subfloat[our guarantee divided by baseline]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_vary_eps_fraction_curve_CIFAR10_resnet20_128_sum.pdf}
}

\caption{This is the reproduction of Figure~\ref{fig:renyi_simple_fraction_eps_curve_vary_eps_mnist_sum} except we now use ResNet-20 models trained on CIFAR-10. Unlike the results of MNIST, we now observe that the ratio between our guarantee and the baseline decreases as $\epsilon$ increases. We hypothesize that this may be because increasing  $\epsilon$ has a much larger impact on accuracy of CIFAR-10 models.
}
\label{fig:renyi_simple_fraction_eps_curve_vary_eps_cifar_sum}
\end{figure}



\begin{figure}[t]
\centering
\subfloat[MNIST]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_vary_arch_fraction_curve_MNIST_sum.pdf}
}
\subfloat[CIFAR10]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_simple_eps_vary_arch_fraction_curve_CIFAR10_sum.pdf}
}
\caption{Per-step R\'enyi-DP guarantee given by Theorem~\ref{thm:easy_renyi_dp} (divided by the baseline guarantee) plotted with respect to different model architectures trained on MNIST and CIFAR-10, at 3 stages of training. Consistently across different architectures and datasets, data points at later stages of training that are correctly classified have significantly better privacy guarantees than the baseline.
}
\label{fig:renyi_simple_fraction_curve_vary_arch_mnist}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[varying batch size]
{
\includegraphics[width=0.48\linewidth]{figures/compo_simple_eps_compo_vary_eps_fraction_curve_MNIST_batch_size.pdf}
}
\subfloat[varying architecture]
{
\includegraphics[width=0.48\linewidth]{figures/compo_simple_eps_compo_vary_eps_fraction_curve_MNIST_model.pdf}
}
\\\subfloat[varying epsilon]
{
\includegraphics[width=0.48\linewidth]{figures/compo_simple_eps_compo_vary_eps_fraction_curve_MNIST_eps.pdf}
}
\subfloat[varying batch size\\$\text{~~~~~}$($10^{th}$ percentile)]
{
\includegraphics[width=0.48\linewidth]{figures/compo_simple_eps_compo_vary_eps_fraction_curve_MNIST_batch_sizepercentile10.pdf}
}
\\\subfloat[varying architecture\\$\text{~~~~~}$($10^{th}$ percentile)]
{
\includegraphics[width=0.48\linewidth]{figures/compo_simple_eps_compo_vary_eps_fraction_curve_MNIST_modelpercentile10.pdf}
}
\subfloat[varying epsilon\\$\text{~~~~~}$($10^{th}$ percentile)]
{
\includegraphics[width=0.48\linewidth]{figures/compo_simple_eps_compo_vary_eps_fraction_curve_MNIST_epspercentile10.pdf}
}
\caption{Expected privacy guarantees from Theorem~\ref{thm:easy_renyi_dp} plotted as a fraction of the per-step DP-SGD guarantee over training. One can see that the ratio between our guarantee and the per-step DP-SGD guarantee (the baseline) decreases as training approaches the end, and this is consistent across different strengths of DP (i.e., $\epsilon$ set for the entire training), varying mini-batch size, and different model architectures.
}
\label{fig:renyi_simple_composition_mnist_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[average (with confidence interval)]
{
\includegraphics[width=0.48\linewidth]{figures/remove_simple_eps_compo_vary_eps_fraction_curve_MNIST_exp.pdf}
}
\subfloat[$10^{th}$ percentile]
{
\includegraphics[width=0.48\linewidth]{figures/remove_simple_eps_compo_vary_eps_fraction_curve_MNIST_exppercentile10.pdf}
}

\caption{This is a reproduction of Figure~\ref{fig:renyi_simple_composition_mnist_sum} except the expected guarantee is computed for $D_{\alpha}(M(X')||M(X))$ instead of $D_{\alpha}(M(X)||M(X'))$. However, a similar trend can be observed.
}
\label{fig:remove_10points}
\end{figure}



\subsection{Studying the Theorem~\ref{thm:renyi_dp_sens} Results}


Here we compute the $(\alpha,\epsilon)$-R\'enyi-DP guarantee given by Theorem~\ref{thm:renyi_dp_sens} (in particular just the $D_{\alpha}(M(X')||M(X))$ upper-bound unless otherwise stated) and plot results analogous to the previous subsections. In computing Theorem~\ref{thm:renyi_dp_sens} guarantees we use 20 samples for both the inner and outer expectations.



We re-use the standard R\'enyi-DP analysis as our baseline, as also used in Section~\ref{ssec:eval_easy_renyi}. In the case of the mean update rule, this means taking our sensitivity norm to be the clipping norm $C = 1.0$ as used in training. 







\paragraph{Comparison to Baseline.} We see in Figures~\ref{fig:renyi_hard_eps_distrib_bs_mnist_mean},\ref{fig:renyi_hard_eps_distrib_bs_cifar_mean} that our analysis for the mean beats the baseline analysis of the R\'enyi-DP guarantee. In particular, we see that for checkpoints in the middle and end of training, we give a privacy guarantee roughly a magnitude better than the baseline. In the case of the sum we see we do significantly worse than the baseline with this analysis as shown in Figures~\ref{fig:renyi_hard_eps_distrib_bs_mnist_sum},\ref{fig:renyi_hard_eps_distrib_bs_cifar_sum}.


\paragraph{Varying Expected Batch Size.} We see in Figures~\ref{fig:renyi_hard_eps_distrib_bs_mnist_sum},\ref{fig:renyi_hard_eps_distrib_bs_cifar_sum} for the sum update rule, both our and the baseline's guarantee increases (with our guarantee continuing to be worse than the baseline). However, for the mean update rule our guarantee decreases where as the baseline still increases, as show in Figures~\ref{fig:renyi_hard_eps_distrib_bs_mnist_mean},\ref{fig:renyi_hard_eps_distrib_bs_cifar_mean}. 

\paragraph{Varying Alphas.} In Figure~\ref{fig:renyi_hard_eps_distrib_alpha_mnist_mean},\ref{fig:renyi_hard_eps_distrib_alpha_cifar_mean} we see that our guarantee for the mean update rule does better than the baseline for sufficiently large alpha, but for small alpha (e.g., $\alpha = 2,4$) does worse. Varying $\alpha$ does not make the guarantee given by Theorem~\ref{thm:renyi_dp_sens} for the sum better than the baseline (Figures~\ref{fig:renyi_hard_eps_distrib_alpha_mnist_sum},\ref{fig:renyi_hard_eps_distrib_alpha_cifar_sum}).






\paragraph{Changing DP strength used to get checkpoints.} In Figure~\ref{fig:ed_eps_curve_vary_eps_mnist} we see that changing the strength of DP used to get the checkpoints also increases our guarantees.

\paragraph{The Reverse Divergence.}
In Figures~\ref{fig:reverse_renyi} and~\ref{fig:reverse_renyi_cifar} we plot the upper-bound for $D_{\alpha}(M(X)||M(X'))$ from Theorem~\ref{thm:renyi_dp_sens} for models trained on MNIST and CIFAR-10 respectively, and conclude we once again do better than the baseline for the mean update rule. Hence, as we do better than the baseline for both divergences, we conclude the analysis of Theorem~\ref{thm:renyi_dp_sens} gives tighter guarantees for the mean update rule.




\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_16_8_mean.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_32_8_mean.pdf}
}
\\\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_64_8_mean.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_8_mean.pdf}
}
\\\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_256_8_mean.pdf}
}
\subfloat[Mini-batch size = 512]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_512_8_mean.pdf}
}

\caption{Distribution plots (log scale) of per-step guarantees given by Theorem~\ref{thm:renyi_dp_sens} computed on LeNet-5 trained on MNIST with mean update rule and varying mini-batch sizes of $16, 32, 64, 128, 256, 512$. As specified by the legend labels, we group the plotted guarantees by whether the model is at the initial, middle, or final stage of the training. It can be seen that in all settings our guarantee is better than the baseline, which is represented by the dashed purple line. However, the guarantee distributions of points at the initial stage of training are closer to the baseline compared to the other distributions.
Additionally, since a mean update rule is used, the bounds depend on the mini-batch size, and better bounds are achieved when the mini-batch size is large. 
}
\label{fig:renyi_hard_eps_distrib_bs_mnist_mean}
\end{figure}


\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_16.0_8_mean.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_32.0_8_mean.pdf}
}

\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_64.0_8_mean.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_8_mean.pdf}
}

\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_256.0_8_mean.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:renyi_hard_eps_distrib_bs_mnist_mean} except we now use ResNet-20 models trained on CIFAR-10. Similar results are observed so we conclude that the guarantee given by Theorem~\ref{thm:renyi_dp_sens} is effective across datasets.
}
\label{fig:renyi_hard_eps_distrib_bs_cifar_mean}
\end{figure}


\begin{figure}[t]
\centering
\subfloat[Alpha = 2]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_2_mean.pdf}
}
\subfloat[Alpha = 4]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_4_mean.pdf}
}
\\\subfloat[Alpha = 8]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_8_mean.pdf}
}
\subfloat[Alpha = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_16_mean.pdf}
}
\\\subfloat[Alpha = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_32_mean.pdf}
}
\subfloat[Alpha = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_64_mean.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:renyi_hard_eps_distrib_bs_mnist_mean} except we now fix the batch size to be 128 and vary the value of $\alpha$. It can be seen that our guarantee is worse than the baseline when $\alpha=2$ and outperforms the latter in all other settings of $\alpha$. This suggests our guarantee is favored when $\alpha$ is large.
}
\label{fig:renyi_hard_eps_distrib_alpha_mnist_mean}
\end{figure}


\begin{figure}[t]
\centering
\subfloat[Alpha = 2]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_2_mean.pdf}
}
\subfloat[Alpha = 4]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_4_mean.pdf}
}
\\\subfloat[Alpha = 8]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_8_mean.pdf}
}
\subfloat[Alpha = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_16_mean.pdf}
}
\\\subfloat[Alpha = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_32_mean.pdf}
}
\subfloat[Alpha = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_64_mean.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:renyi_hard_eps_distrib_alpha_mnist_mean} except we now use ResNet-20 models trained on CIFAR-10. Whereas our guarantees are worse than the baseline for $\alpha=2,4$, the conclusion that our guarantee is favored when $\alpha$ is large still holds.
}
\label{fig:renyi_hard_eps_distrib_alpha_cifar_mean}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_16_8_sum.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_32_8_sum.pdf}
}
\\\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_64_8_sum.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_8_sum.pdf}
}
\\\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_256_8_sum.pdf}
}
\subfloat[Mini-batch size = 512]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_512_8_sum.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:renyi_hard_eps_distrib_bs_mnist_mean} except we now use a sum update rule. Unlike the mean update rule, we observe that the guarantees given by Theorem~\ref{thm:renyi_dp_sens} are significantly larger than the baseline guarantees. We suspect that this is because Theorem~\ref{thm:renyi_dp_sens} essentially computes how similar the gradient of any batch from $X$ is to any other batch from $X'$ in Euclidean space. In the absence of the factor that scale down the gradients by the mini-batch size, we do not expect these gradients concentrate at similar values.
}
\label{fig:renyi_hard_eps_distrib_bs_mnist_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Mini-batch size = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_16.0_8_sum.pdf}
}
\subfloat[Mini-batch size = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_32.0_8_sum.pdf}
}

\subfloat[Mini-batch size = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_64.0_8_sum.pdf}
}
\subfloat[Mini-batch size = 128]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_8_sum.pdf}
}

\subfloat[Mini-batch size = 256]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_256.0_8_sum.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:renyi_hard_eps_distrib_bs_mnist_sum} except we now use ResNet-20 models trained on CIFAR-10. The results are similar.
}
\label{fig:renyi_hard_eps_distrib_bs_cifar_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Alpha = 2]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_2_sum.pdf}
}
\subfloat[Alpha = 4]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_4_sum.pdf}
}
\\\subfloat[Alpha = 8]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_8_sum.pdf}
}
\subfloat[Alpha = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_16_sum.pdf}
}
\\\subfloat[Alpha = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_32_sum.pdf}
}
\subfloat[Alpha = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_MNIST_lenet_128_64_sum.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:renyi_hard_eps_distrib_bs_mnist_sum} except we now fix the batch size to be 128 and vary the value of $\alpha$. It can be seen that increasing alpha does not bring our guarantee and the baseline guarantee closer.
}
\label{fig:renyi_hard_eps_distrib_alpha_mnist_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Alpha = 2]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_2_sum.pdf}
}
\subfloat[Alpha = 4]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_4_sum.pdf}
}
\\\subfloat[Alpha = 8]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_8_sum.pdf}
}
\subfloat[Alpha = 16]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_16_sum.pdf}
}
\\\subfloat[Alpha = 32]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_32_sum.pdf}
}
\subfloat[Alpha = 64]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_64_sum.pdf}
}
\caption{This is the reproduction of Figure~\ref{fig:renyi_hard_eps_distrib_alpha_mnist_sum} except we now use ResNet-20 models trained on CIFAR-10. The results are similar.
}
\label{fig:renyi_hard_eps_distrib_alpha_cifar_sum}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Update-rule: mean]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_curve_vary_MNIST_mean.pdf}
}
\subfloat[Update-rule: sum]
{
\includegraphics[width=0.48\linewidth]{figures/renyi_hard_eps_curve_vary_MNIST_sum.pdf}
}
\caption{Per-step R\'enyi-DP guarantee given by Theorem~\ref{thm:renyi_dp_sens} computed on LeNet-5 trained on MNIST as a function of $\epsilon$, plotted at 3 stages of training and varying mini-batch sizes with 2 different update rules. We can see that as training proceeds, our guarantees increase slower while $\epsilon$ increases.
}
\label{fig:renyi_hard_eps_curve_vary_eps_mnist}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Update-rule: mean]
{
\includegraphics[width=0.48\linewidth]{figures/reverse_renyi_hard_eps_hist_MNIST_lenet_128_8_mean.pdf}
}
\subfloat[Update-rule: sum]
{
\includegraphics[width=0.48\linewidth]{figures/reverse_renyi_hard_eps_hist_MNIST_lenet_128_8_sum.pdf}
}
\caption{This is the reproduction of the mini-batch size 128 case of Figures~\ref{fig:renyi_hard_eps_distrib_bs_mnist_mean} and~\ref{fig:renyi_hard_eps_distrib_bs_mnist_sum} where $X$ and $X'$ are swapped to show that our bounds for both divergences are tighter than the baseline for mean update rule. Thus our guarantees are better than the baseline.
}
\label{fig:reverse_renyi}
\end{figure}

\begin{figure}[t]
\centering
\subfloat[Update-rule: mean]
{
\includegraphics[width=0.48\linewidth]{figures/reverse_renyi_hard_eps_hist_CIFAR10_resnet20_128_8_mean.pdf}
}
\subfloat[Update-rule: sum]
{
\includegraphics[width=0.48\linewidth]{figures/reverse_renyi_hard_eps_hist_CIFAR10_resnet20_128_8_sum.pdf}
}
\caption{This is the reproduction of Figures~\ref{fig:reverse_renyi} except 
we now use ResNet-20 models trained on CIFAR-10. The results are consistent with MNIST.}
\label{fig:reverse_renyi_cifar}
\end{figure}




