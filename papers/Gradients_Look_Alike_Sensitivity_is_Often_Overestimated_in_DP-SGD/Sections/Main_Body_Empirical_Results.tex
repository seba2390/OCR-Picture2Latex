\section{Empirical Results}
\label{sec:main_body_emp_results}


In Section~\ref{sec:analysis} we provided the first framework to analyze DP-SGD's per-instance privacy guarantees. This followed by providing new per-step analyses (Theorem~\ref{thm:easy_renyi_dp} and~\ref{thm:renyi_dp_sens}), and a new composition theorem that relies on summing ``expected" per-step guarantees (Theorem~\ref{thm:better_composition}). 
We now highlight several conclusions our framework allows us to make about per-instance privacy when using DP-SGD. For conciseness, we defer a subset of the experimental results to Appendix~\ref{sec:detailed_emp_res}. %





\paragraph{Experimental Setup.} In the subsequent experiments, we apply our analysis on MNIST~\citep{lecun1998mnist} and CIFAR-10~\citep{krizhevsky2009learning}. Unless otherwise specified, LeNet-5~\citep{lecun1989backpropagation} and ResNet-20~\citep{he2016deep} were trained on the two datasets for 10 and 200 epochs respectively using DP-SGD, with a mini-batch size equal to 128, $\epsilon=10$, $\delta = 10^{-5}$, $\alpha = 8$ (in cases of Renyi DP), and clipping norm $C = 1.0$. All the experiments are repeated 100 times by sampling 100 data points to obtain a distribution/confidence interval if not otherwise stated.
Regarding hardware, we used NVIDIA T4 to accelerate our experiments. 


\begin{figure}[!t]

\centering
\subfloat[Training with the datapoint ($X \cup \{x^*\}$) \label{fig:compo_1_more}]
{
\includegraphics[width=0.33\linewidth]{figures/compo_simple_eps_compo_vary_eps_fraction_curve_MNIST_eps.pdf}
}
\subfloat[Training with the datapoint ($X \cup \{x^*\}$)\\$\text{~~~~~}$($10^{th}$ percentile) \label{fig:compo_1_more_10per}]
{
\includegraphics[width=0.33\linewidth]{figures/compo_simple_eps_compo_vary_eps_fraction_curve_MNIST_epspercentile10.pdf}
}
\subfloat[Training with and without the datapont \\ ($X \cup \{x^*\}$ and $X$) \label{fig:compo_1_less}]
{
\includegraphics[width=0.33\linewidth]{figures/remove_simple_eps_compo_vary_eps_fraction_curve_MNIST_exp.pdf}
}
\caption{Per-step privacy contribution from our composition theorem (Theorem~\ref{thm:better_composition}) using the per-step gurantee for the sum update rule (Theorem~\ref{thm:easy_renyi_dp}) as needed for DP-SGD, plotted as a fraction of the baseline data-independent per-step DP-SGD guarantee (Section 3.3 in~\citet{mironov2019r}). %
The expectations for Theorem~\ref{thm:better_composition} are computed over 10 trials. Figure~\ref{fig:compo_1_more} plots the average relative per-step contributon of 100 random points in MNIST for different strengths of the DP guarantee (i.e., different upper bounds $\varepsilon$) used when training on $X' = X \cup \{x^*\}$. The $10^{th} percentile$ is plotted in Figure~\ref{fig:compo_1_more_10per}. Figure~\ref{fig:compo_1_less} plots expectation when training on $X'$ and $X$ for 10 random points in MNIST. We see from both subfigures our per-step contribution decreases relative to the baseline as training progresses: using Theorem~\ref{thm:better_composition} one can conclude that many datapoints have better overall data-dependent privacy guarantees than expected by classical analysis.
}


\label{fig:composition}
\end{figure}

\subsection{Many Datapoints have Better Privacy}
\label{ssec:exp_better_privacy}




Here we describe how our per-instance RDP analysis of DP-SGD, using Theorem~\ref{thm:easy_renyi_dp} for the per-step analysis (with the update rule being the sum of gradients as is typically used) and Theorem~\ref{thm:better_composition} for the composition analysis, allows us to explain why per-instance privacy attacks will fail for many datapoints: many points have better per-instance privacy than the data-independent analysis. We further investigate the distribution of the per-instance privacy guarantees, and which points exhibit better per-instance privacy with our analysis.

\paragraph{Improved Per-Instance Analysis for Most Points} We compare the guarantees given by Theorem~\ref{thm:easy_renyi_dp} for the per-step guarantee in DP-SGD to the guarantee given by the data-independent analysis (see Section 3.3 in \citet{mironov2019r}), and plot per-step contribution coming from our composition theorem. In particular, we take $X$ to be the full MNIST training set, and randomly sample a data point $x^*$ from the test set to create $X' = X \cup x^*$ (as mentioned earlier, we repeat the sampling of $x^*$ 100 times to obtain a confidence interval). We train 10 different models on $X$ with the same initialization and compute the per-step contribution from Theorem~\ref{thm:better_composition} between $X$ and $X'$ (using Theorem~\ref{thm:easy_renyi_dp} to analyze the per-step guarantee from a given model) over the training run, shown in Figure~\ref{fig:compo_1_more}.
We can see that our analysis of the per-step contribution decreases with respect to the baseline as we progress through training. This persists regardless of the expected mini-batch size, the strength of DP used during training, and model architectures; see Figure~\ref{fig:renyi_simple_composition_mnist_sum} in Appendix~\ref{sec:detailed_emp_res}.
By Theorem~\ref{thm:better_composition} we conclude that $D_{\alpha}(Train_{DP-SGD}(X) || Train_{DP-SGD}(X'))$ is significantly less than the baseline for many data points. %



To see our improvement over the max of $D_{\alpha}(Train_{DP-SGD}(X) || Train_{DP-SGD}(X'))$ and \\ $D_{\alpha}(Train_{DP-SGD}(X') || Train_{DP-SGD}(X))$, i.e., the R\'enyi-DP guarantee, we computed the expectation when training on $X$ and $X'= X \cup \{x^*\}$  for $10$ training points $x^*$ where $X$ is now the training set of MNIST with one point removed and $X'$ is the full training set. Our results are shown in Figure~\ref{fig:compo_1_less} where we see a similar decreasing trend relative to the baseline over training: we conclude by Theorem~\ref{thm:better_composition} that many datapoints have better per-instance R\'enyi DP than the baseline. In other words, we conclude many datapoints have stronger per-instance RDP guarantees than can be demonstrated through the classical data-independent analysis.


\begin{figure}[!t]

\centering
\subfloat[Mini-batch Size = 128 \label{fig:simple_renyi_training_stage}]
{
\includegraphics[width=0.4\linewidth]{figures/renyi_simple_eps_hist_CIFAR10_resnet20_128_8_sum.pdf}
}
\subfloat[Varying Mini-batch Size \label{fig:simple_renyi_vary_bs}]
{
\includegraphics[width=0.4\linewidth]{figures/renyi_simple_eps_hist_vary_bs_CIFAR10_resnet20_sum.pdf}
}

\caption{Distribution plots of the per-step guarantees given by Theorem~\ref{thm:easy_renyi_dp} for $500$ datapoints in CIFAR10 with respect to: (a) different stages of training, and (b) varying mini-batch size. The purple dashed line 
represents the data-independent baseline. We observe a long tail of datapoints with magnitudes better privacy than expected in both plots.
}
\label{fig:simple_renyi}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=0.4\linewidth]{figures/renyi_simple_eps_resnet20_fraction_curve_CIFAR10_sum.pdf}
\caption{Per-step guarantees given by Theorem~\ref{thm:easy_renyi_dp} for $500$ datapoints in CIFAR10 across training stages with respect to correct or incorrect classifications. It can be seen that correctly classified datapoints are on average more private than incorrectly classified ones.}
\label{fig:correct_incorrect}
\end{figure}



\paragraph{Long-Tail of Better Per-Instance Privacy.} However, the previous figures only show the average effect over datapoints. In Figures~\ref{fig:simple_renyi_training_stage} and~\ref{fig:simple_renyi_vary_bs} we plot the distribution of per-step guarantees over $500$ data points in CIFAR10. The key observations are (1) there exists a long tail of data points with significantly better per-instance privacy than the baseline,  (2) such improvements mostly exist in the middle and end of the training process, and (3) such improvements are mostly independent of mini-batch size.










\paragraph{Correct Points are More Per-Instance Private.} Next, we turn to understanding what datapoints are experiencing better privacy when using DP-SGD. In Figure~\ref{fig:correct_incorrect}, we plot the per-step guarantees given by Theorem~\ref{thm:easy_renyi_dp} for correctly and incorrectly classified data points at the beginning, middle, and end of training
on CIFAR10 (and for MNIST in Figure~\ref{fig:renyi_simple_fraction_curve_vary_arch_mnist} in Appendix~\ref{sec:detailed_emp_res}). We see that, on average, correctly classified data points have better per-step privacy guarantees than incorrectly classified data points across training. This disparity holds most strongly towards the end of training.










\subsection{Higher Sampling Rates can give Better Privacy}
\label{ssec:exp_hard_renyi}

We now highlight how our analysis, if it uses Theorem~\ref{thm:renyi_dp_sens} for the per-step analysis, allows us to better analyze DP-SGD with other update rules (not the sum of gradients which is what the current implementation of DP-SGD uses and Section~\ref{ssec:exp_better_privacy} analyzed). In particular, we will analyze the mean update rule and show how it has a privacy trade-off with sampling rate that is opposite to the trade-off for the sum update rule.

In normal SGD (with gradient clipping), one computes a mean for the per-step update 
$U(X_B) = \frac{1}{|X_B|}\sum_{x \in X_B} \nabla_{\theta}\mathcal{L}(\theta,x)/ \max(1,\frac{||\nabla_{\theta}\mathcal{L}(\theta,x)||_2}{C})$. 
However, DP-SGD computes a weighted sum $U(X_B) = \frac{1}{L} \sum_{x \in X_B} \nabla_{\theta}\mathcal{L}(\theta,x)/ \max(1,\frac{||\nabla_{\theta}\mathcal{L}(\theta,x)||_2}{C})$. Note the subtle difference between dividing by a fixed constant $L$ (typically the expected mini-batch size when Poisson sampling datapoints) and by the mini-batch size $|X_B|$. This means for the sum the upper-bound on sensitivity is $\frac{C}{L}$, while for the mean the upper-bound on sensitivity is only $C$ (consider neighbouring mini-batches of size 1 and 2). Hence using the mean update rule requires far more noise and so is not practical to use. We highlight how our per-instance analysis by sensitivity distributions provides better guarantees for the mean update rule.


\begin{figure}[!t]
\centering
\subfloat[$D_{\alpha}(M(X')||M(X))$\\$\text{~~~~~}$Mini-batch Size = $128$ \label{fig:hard_renyi_training_stage}]
{
\includegraphics[width=0.33\linewidth]{figures/renyi_hard_eps_hist_CIFAR10_resnet20_128.0_8_mean.pdf}
}
\subfloat[$D_{\alpha}(M(X)||M(X'))$ \\$\text{~~~~~}$Mini-batch Size = $128$
\label{fig:hard_renyi_reverse}]
{
\includegraphics[width=0.33\linewidth]{figures/reverse_renyi_hard_eps_hist_CIFAR10_resnet20_128_8_mean.pdf}
}
\subfloat[$D_{\alpha}(M(X')||M(X))$\\$\text{~~~~~}$Varying Mini-batch Size \label{fig:hard_renyi_vary_bs}]
{
\includegraphics[width=0.33\linewidth]{figures/renyi_hard_eps_hist_vary_bs_CIFAR10_resnet20_mean_unnormalized.pdf}
}

\caption{ Distribution plots (log scale) of per-step guarantees from Theorem~\ref{thm:renyi_dp_sens} for $500$ datapoints in CIFAR10 with respect to different training stages and mini-batch sizes. Bounds on both $D_{\alpha}(M(X)||M(X'))$ and $D_{\alpha}(M(X')||M(X))$ are shown for an expected mini-batch size of 128.  From Figures~\ref{fig:hard_renyi_training_stage},\ref{fig:hard_renyi_reverse}, we conclude Theorem~\ref{thm:renyi_dp_sens} gives better data-dependent guarantees for the mean update rule than classicial analysis, and from Figure~\ref{fig:hard_renyi_vary_bs} that increasing the expected mini-batch size decreases our bound for this update rule (counter-intuitive to privacy amplification by subsampling).
}
\label{fig:hard_renyi}
\end{figure}


\paragraph{Better Analysis of the Mean Update Rule. } Letting $M$ now be the sampled Gaussian mechanism with the mean update rule, we compute the bound on $D_{\alpha}(M(X')||M(X))$ and $D_{\alpha}(M(X)||M(X'))$ given by Theorem~\ref{thm:renyi_dp_sens}, where we estimated the inner and outer expectation using $20$ samples, i.e., $20$ random $X_B'^{\alpha}$ (or $X_B^{\alpha}$) for each of the $20$ random $X_B$ (or $X_B'$). We obtain Figure~\ref{fig:hard_renyi_training_stage} and~\ref{fig:hard_renyi_vary_bs} by repeating this for $500$ data points in CIFAR10 while varying the training stage. We observe that for both divergences, we beat the baseline analysis by more than a magnitude at the middle and end of training. We conclude Theorem~\ref{thm:renyi_dp_sens} gives us better per-instance R\'enyi DP guarantees for the mean update rule.








\paragraph{Per-Instance Privacy Improves with Higher Sampling Rate.} Furthermore, counter-intuitively to typical subsample privacy amplification, in Figure~\ref{fig:hard_renyi_vary_bs} we see that our bound decreases with increasing expected mini-batch size: 
we attribute this to the law of large numbers, whereby increasing the expected mini-batch size leads to sampled mini-batches having similar updates more often and hence the sensitivity distribution concentrates at smaller values. An analogous result is shown for MNIST in Figure~\ref{fig:renyi_hard_eps_distrib_bs_mnist_mean} (in Appendix~\ref{sec:detailed_emp_res}).



