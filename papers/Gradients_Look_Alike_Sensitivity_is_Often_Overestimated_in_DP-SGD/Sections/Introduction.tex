\section{Introduction}








Differential Privacy (DP) is the standard framework for private data analysis~\citep{dwork2006calibrating}. Making an algorithm differentially private limits the success any attack can have in knowing whether any datapoint was or was not an input to the algorithm given just the outputs of the algorithm. %
To obtain this notion of indistinguishability the algorithm needs to perform a noisy analysis of the data. %
In the case of deep learning, the canonical private training algorithm is DP-SGD~\citep{abadi2016deep}, where Gaussian noise is added to the gradients computed on training examples. Much work has gone into improving the privacy analysis of DP-SGD for a given amount of noise \citep{mironov2017renyi,mironov2019r,gopi2021numerical} in an effort to minimize the impact of noise on performance.



To reiterate, this current privacy analysis for DP-SGD  is \textit{data independent}: it assumes an upper-bound on how much \emph{any} individual datapoint from \textit{any} dataset can have their privacy leaked. It is furthermore now known to be tight; there exist specific pairs of datasets and models for which a privacy attack can match the upper-bound of DP-SGD~\citep{nasr2021adversary}. Yet, when training on common benchmark datasets like CIFAR10, \citet{carlini2022privacy} empirically saw that even strong privacy attacks perform significantly worse for many datapoints than the guarantees associated with DP-SGD. %
That is, when training on real-world data, there is a gap between what our strongest attacks can achieve and what our current data-independent privacy analysis of DP-SGD can tell us. Hence the question, why is there a gap? Is it because our attacks are still too weak for many datapoints, or is it because there exist tighter privacy upper bounds when restricted to these dataset settings? Past work attempted to answer this by analyzing specific privacy attacks \citep{mahloujifar2022optimal, thudi2022bounding, guo2022bounding}, or studying a weakened notion of DP~\citep{yu2022individual}. But all past work either have bounds limited in scope or with unproven assumptions. No work has yet derived tighter indistinguishability guarantees that are specific to the data being analyzed, analogous to how DP gives indistinguishability guarantees to prevent privacy attacks (for all possible datasets).








Our work provides the first per-instance DP analysis of DP-SGD, i.e., bounds on the distinguishability of outputted models that are specific to training on a given dataset or the dataset plus a point. This analysis bridges the theoretical gap between the tight data-independent analysis of DP-SGD and what is achievable when training on common deep-learning datasets. %
These new guarantees follow from an exploration of the role of \textit{sensivity} in privacy analysis. Currently, to obtain data-independent privacy guarantees, a model trainer needs to bound how much \emph{any} individual datapoint from \textit{any} dataset can contribute to a gradient update---a quantity known as the algorithm's sensitivity. This is currently done by setting an upper-bound ahead of time which is enforced during training by clipping the gradient computed on each datapoint %
to a norm below this preset sensitivity value. However, we highlight that this overestimates the sensitivity of DP-SGD to a \emph{specific} datapoint in a \textit{given} dataset when that datapoint's update is similar to the update given by many other datapoints in this dataset. In deep learning, many mini-batches in a dataset do produce similar gradients~\citep{shumailov2021dataorder, thudi2022necessity, kong2022forgeability}, hence such a case of overestimating sensitivity is common. Our per-instance (i.e., ``data-dependent") DP analysis of DP-SGD leverages this phenomenon.














Let us first focus on a single update of DP-SGD. Intuitively, if many of the datapoints produced almost the same gradient, then with high probability we would have obtained the same updated model with or without one of these datapoints. Making this intuition rigorous, we introduce a class of distributions we call \textit{sensitivity distributions}: broadly they capture the difference between updates computed from a given mini-batch to sampling another mini-batch. From this, we derive new bounds on the privacy leakage of a single DP-SGD update that incorporates how concentrated these distributions are at small values, i.e., have many mini-batches that produce almost the same gradient. Using this bound we can show that for many datapoints in common benchmark datasets, the individual per-step guarantee for that point can be magnitudes lower than the data-independent guarantee. 




Building on our analysis for a single DP-SGD update, we give a per-instance bound on the \emph{overall} privacy leakage of a \emph{full DP-SGD run}. The current analysis considers the model that leaks the most privacy at every step (the worst-case model) and notes that summing the maximum per-step leakages bounds the overall privacy leakage of a DP-SGD run. %
Yet, the sensitivity distributions that we introduce are heavily dependent on the model being updated:  e.g., there is a difference between gradients computed using a partially-trained model and a randomly-initialized model. 
Towards not relying on analyzing worst-case models, intuitively it should not matter what the privacy leakage of the worst-case models is if they are  unlikely to be reached. %
More rigorously, we 
develop a new composition theorem
which allows us to upper-bound the overall per-instance privacy leakage of using DP-SGD by the expected privacy leakage at each step during training. 











These analytic results give %
a new framework to understand the privacy guarantees of DP-SGD for individual datapoints. However, it remains to verify whether this analysis is tight enough to show that many datapoints have better privacy when training on benchmark datasets.
We thus turn to experimentation. 
The crux of implementing our results is to repeat training several times to compute the expected per-step privacy leakage. %
Because this one-dimensional statistic is bounded by the existing worst-case privacy analysis, one  achieves non-trivial estimates with few samples.  Doing this:

\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt]
    \item We show that when training on common benchmark datasets, many data points have better per-instance privacy than what the current data-independent guarantee associated with DP-SGD tells us.
    For some datapoints, we observe more than a magnitude improvement in the privacy guarantee  $\varepsilon$. %
    This explains the prior results we motivated our work with: for many datapoints, attacks that can only observe the outputs from training with or without the datapoint will fail.
    
    \item  In our framework, we observe a disparity where correctly-classified points obtain better privacy guarantees than misclassified points. In other words, %
    training algorithms that lead to high-performing models quantifiably leak less per-instance privacy for many points. This is as they reach states that have similar updates for large clusters of datapoints. We hypothesize that %
    designing model architectures to be more performative may also make them more private.
    \item In classical privacy analysis, training with higher mini-batch sampling rates leaks more privacy. However we find that for certain update rules, training with higher sampling rates can give better per-instance privacy because mini-batch updates concentrate on the dataset mean; this leads to many mini-batches with similar updates. 
    
\end{enumerate}
 
 
 The consequences of our work are far reaching: having better per-instance DP guarantees has implications for unlearning, generalization, and memorization because of how DP formulates privacy by preventing distinguishability between the models trained with or without a datapoint. %
 For unlearning, a strong per-instance DP guarantee implies that the models coming from training with a datapoint are indistinguishable to the models trained without it. For generalization and memorization, a strong per-instance DP guarantee implies that the models coming from training without a datapoint perform similarly to those that had trained with it. With our framework, one can now say a specific datapoint does not need to be unlearned, or that a datapoint will not be memorized. 
 
 




















 
