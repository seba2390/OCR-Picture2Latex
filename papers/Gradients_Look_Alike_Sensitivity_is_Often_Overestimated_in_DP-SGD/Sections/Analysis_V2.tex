\section{A Per-Instance Analysis of DP-SGD}
\label{sec:analysis}





We now present our new analysis of DP-SGD which removes the data-independent nature of the per-step and composition analyses currently used for DP-SGD. The impact of this new analysis is presented in Section~\ref{sec:main_body_emp_results}, where we show that many datapoints have much better privacy than suggested by the current analysis of DP-SGD, explaining the failure of many privacy attacks in practice.

The technical contributions that led to this are two-fold. At the per-step level, we generalize the notion of sensitivity to what we term \emph{sensitivity distributions}; given two datasets, sensitivity distributions capture how similar the updates between mini-batches from either dataset are. At the composition step, we generalize RDP composition to do accounting by the ``expected" intermediate privacy losses during training as opposed to the largest possible intermediate privacy losses. Together, we can now study the data-dependent behaviour of DP-SGD.





\subsection{Sensitivity Distribution Generalize the $(\epsilon,\delta)$-DP Analysis}
\label{ssec:eps_delta_case}




We first turn to $(\epsilon,\delta)$-DP, which is not used to analyze DP-SGD for composition reasons, but allows for simpler expressions to demonstrate the improvements afforded by particular data-dependent random variables we call \textit{sensitivity distributions}. In particular, in this section we will first consider the classical data-independent $(\epsilon,\delta)$-DP analysis of the sampled Gaussian mechanism $M$ and show how one can generalize this analysis and obtain tighter per-instance $(\epsilon,\delta)$-DP guarantees.


Recall that for an update rule $U$, the Gaussian mechanism is defined as $A(X) = U(X) + N(0,\sigma)$. The sampled Gaussian mechanism is then defined as $M(X) = A(\mathbf{X_B})$ where $\mathbf{X_B}$ is a mini-batch constructed from a dataset $X$ by sampling each datapoint $x \in X$ independently with probability $\mathbb{P}_{x}(1)$ (unless otherwise stated we think of $X_B$, not bold-face, as a specific mini-batch). Note, one assumes the sampling probability $\mathbb{P}_{x}(1)$ is only a function of $x$ and not the full dataset $X$, e.g., some fixed constant. The classical data-independent $(\epsilon,\delta)$-DP analysis of the sampled Gaussian mechanism follows two steps. First, we derive the guarantee for just the Gaussian mechanism. To do so, one first assumes a data-independent sensitivity bound $C_U$ on $U$: for all $X,X' = X \cup \{x^*\}$ we have $||U(X) - U(X')||_{2} \leq C_{U}$. This can be achieved by clipping the output values of $U$ to have a small norm. With this constant $C_U$ one has that the Gaussian mechanism $A$ gives the $(\epsilon,\delta)$-DP guarantee $\epsilon = C_{\delta,\sigma} C_U$ for some constant $C_{\delta,\sigma}$ depending on $\delta$ and $\sigma$ where $\sigma$ is the standard deviation of the added Gaussian noise~\footnote{For example, one can take $C_{\delta, \sigma} = \frac{\sqrt{2 \ln (1.25/\delta)}}{\sigma}$~\citep{dwork2014algorithmic}.}. To then analyze the sampled Gaussian mechanism one would incorporate the privacy gain from not sampling $x^*$ sometimes~\citep{beimel2014bounds}\citep{kasiviswanathan2011can} to get the privacy guarantees of $M$ %
as $(\epsilon',\delta')$-DP where $\epsilon' = \ln( \mathbb{P}_{x^*}(1) e^{C_{\delta,\sigma}~C_U} + \mathbb{P}_{x^*}(0))$ and $\delta' = \mathbb{P}_{x^*}(1) \delta$. Here $\mathbb{P}_{x^*}(0)=1-\mathbb{P}_{x^*}(1)$, and this gain in privacy by sometimes not using the datapoint is called privacy amplification by sampling.






Towards tightening this analysis into a per-instance analysis, let $$\Delta_{U,x^*}(X_B) \coloneqq ||U(X_B) - U(X_B \cup \{x^*\})||_2$$
then $\Delta_{U,x^*}(\mathbf{X_B})$ is a data-dependent random variable which we will call a \emph{sensitivity distribution}: it captures the change in the distribution of mini-batches updates caused by adding a point $x^*$ to the mini-batch. The classical data-independent analysis only (implicitly) uses sensitivity distributions via the data-independent bound $|\Delta_{U,x^*}(X_B)| \leq C_{U}~\forall X_B$. Instead, we will show how to directly use the $L_p$ norms $||\Delta_{U,x^*}(\mathbf{X_B})||_{p} = (\mathbb{E}_{X_B} (\Delta_{U,x^*}(X_B)^p))^{1/p}$ (or generally the $L_p$ norm of some monotonic transformation of $\Delta_{U,x^*}(\mathbf{X_B})$) to obtain tighter per-instance privacy guarantees. Furthermore, when using $p < \infty$, this analysis will be able to translate the phenomenon that many mini-batches produce similar updates into better privacy guarantees (as the sensitivity distribution concentrates at smaller values and hence has smaller $p$-norms). To emphasize this ability, past work that studied sampling relied mainly on the intuition that by sampling a datapoint with low probability, we have any given step often does not leak privacy for that point as it was not used. This translates to better privacy guarantes. By using the $L_p$ norms of sensitivity distributions with $p< \infty$ we make an additional observation, which is that if many of the other mini-batches produce the same update, then effectively we have an even lower probability of an attacker observing a noticeable shift due solely to that point.  %


In particular, recall that to prove per-instance $(\epsilon,\delta)$-DP for a pair of datasets $X,X'= X \cup \{x^*\}$ we need to bound $\mathbb{P}(M(X') \in S) \leq e^{\epsilon} \mathbb{P}(M(X) \in S) + \delta$ and $\mathbb{P}(M(X) \in S) \leq e^{\epsilon} \mathbb{P}(M(X') \in S) + \delta$. As a proof-of-concept on the role of sensitivity distributions, we present an analysis for the first inequality in Corollary~\ref{cor:eps_delta_sens} \footnote{We will later turn to R\'enyi-DP which provides both inequalities.}. Inspecting Corollary~\ref{cor:eps_delta_sens}, we see that it approximately follows the formula given by the classical analysis except the role of $C_U$ is replaced with a dependency on how concentrated $\Delta_{U,x^*}(X_B)$ is at small values (the $L_p$ norm of an exponential applied to $\Delta_{U,x^*}(X_B)$). When enough mini-batches provide updates more similar than the upper-bound $C_U$, the per-instance guarantee of Corollary~\ref{cor:eps_delta_sens} will significantly beat the classical data-independent analysis, as demonstrated for MNIST and CIFAR10 in Appendix~\ref{ssec:eps_delta_experiments}.




\begin{corollary}
\label{cor:eps_delta_sens}
For $p \in (1,\infty)$, let $a_p = \mathbb{P}_{x^*}(1) (\mathbb{E}_{x_{B}}(e^{C_{\delta,\sigma} \Delta_{U,x^*}(X_B)p}))^{1/p}$, $\epsilon' = \ln(a_p^{\frac{1}{1-1/p}}\delta'^{\frac{-1}{p-1}} + \mathbb{P}_{x^*}(0)) $ and $\delta'' = \mathbb{P}_{x^*}(1)\delta + \delta'$. Then, for $X' = X \cup \{x^*\}$ $$\mathbb{P}(M(X') \in S) \leq e^{\epsilon'} \mathbb{P}(M(X) \in S) + \delta''$$

\end{corollary}


\emph{Proof Sketch:} The proof of Corollary~\ref{cor:eps_delta_sens} follows two stages. First by expanding mini-batch sampling and applying Holder's inequality, we can show

\begin{multline}
    \mathbb{P}(M(X') \in S) \leq \mathbb{P}_{x^*}(1) \mathbb{E}_{X_B}(e^{C_{\delta,\sigma} \Delta_{U,x^*}(X_B)p})^{1/p} \mathbb{P}(M(X) \in S)^{1-1/p} \\ + \mathbb{P}_{x^*}(1)\delta + \mathbb{P}_{x^*}(0) \mathbb{P}(M(X) \in S)
\end{multline}

This is stated as Lemma~\ref{lem:holder_approach}. One then follows the proof strategy of Proposition 3 in \citet{mironov2017renyi} to convert an inequality bounding $\mathbb{P}(M(X') \in S)$ with a power of $\mathbb{P}(M(X) \in S)$ into an $(\epsilon,\delta)$-DP inequality. The full proof of Corollary~\ref{cor:eps_delta_sens} is in Appendix~\ref{proof:eps_delta_sens}.





\subsection{Per-Instance R\'enyi-DP Analysis for DP-SGD}

With now an understanding of the power of incorporating $L_p$ norms of sensitivity distributions (upto some transformations) into DP analyses, we turn to analyzing the R\'enyi-DP guarantees of DP-SGD. R\'enyi-DP is more suited to compose the guarantees of each step of DP-SGD to obtain the guarantees for an entire training run. We first present per-step analyses for the sampled Gaussian mechanism, and then a new composition theorem to reason about the entire training run. We then discuss how to analyze DP-SGD for general update rules, i.e., not just the sum of gradients.

Our per-step analyses will focus on integer values of $\alpha$ for R\'enyi-DP. This is for simplicity, as R\'enyi divergences $D_{\alpha}(P||Q) \coloneqq \frac{1}{\alpha -1} \ln \mathbb{E}_{x \sim Q} (\frac{P}{Q})^{\alpha}$ are increasing in their order $\alpha$, hence we can bound the guarantee for any $\alpha$ by the guarantee for $\lceil \alpha \rceil$. In terms of notation, we will use ${X_B}^{\tilde \alpha} = ({X_B}^1,\cdots,{X_B}^{\alpha})$ to denote $\alpha$ mini-batches from $X$ (sampled independently if random). Analogously we use ${X'_B}^{\tilde \alpha}$ and $X'_B$ for $X'$.


\subsubsection{Per-Instance R\'enyi DP for the Sum Update Rule}
\label{ssec:sum_update}

In Section~\ref{ssec:eps_delta_case} we introduced the sensitivity distribution $\Delta_{U,x^*}(\mathbf{X_B}) = ||U(\mathbf{X_B}) - U(\mathbf{X_B \cup \{x^*\}})||_2$ and showed how directly leveraging its $L_p$ norms gives better per-instance DP analysis. In particular, how $p < \infty$ allows one to take advantage of expected sensitivity over mini-batches. However, for update rules of the form $U(X_B) = \sum_{x_i \in X_B} g(x_i)$ (i.e., the sum update rule typically used in DP-SGD) we have $\Delta_{U,x^*}(\mathbf{X_B})$ is always a constant: $\Delta_{U,x^*}(\mathbf{X_B}) = ||g(x^*)||_2$. Hence an analysis of the sampled Gaussian mechanism that used $\Delta_{U,x^*} \coloneqq \sup_{X_B \sim X} \Delta_{U,x^*}(X_B)$ would effectively capture all $L_p$ norms of the sensitivity distribution $\Delta_{U,x^*}(X_B)$ for the sum update rule. We state such a per-instance version of the classical RDP analysis of the sampled Gaussian mechanism below.



\begin{theorem}
\label{thm:easy_renyi_dp}
    For integer $\alpha > 1$, the sampled Gaussian mechanism with noise $\sigma$ and sampling probability $\mathbb{P}_{x^*}(1)$ for $x^*$ is $(\alpha,\epsilon)$-R\'enyi DP for:

    
    $$\epsilon = \frac{1}{\alpha -1} \ln(\sum_{k=0}^{\alpha} {\alpha \choose k} (1 - \mathbb{P}_{x^*}(1))^{\alpha -k} \mathbb{P}_{x^*}(1)^k \exp(\frac{\Delta_{U,x^*}^2(k^2 - k)}{2 \sigma^2}))$$

        
\end{theorem}

Note that some key variables in Theorem~\ref{thm:easy_renyi_dp} are the sampling rate $\mathbb{P}_{x^*}(1)$ (increasing it typically increases the bound), the standard deviation of noise $\sigma$ (increasing it typically decreases the bound), and the sensitivity upper-bound over minibatches $\Delta_{U,x^*}$ (increasing it typically increases the bound). The proof strategy is analogous to \citet{mironov2019r} and replaces their sensitivity upper-bound with the per-instance bound $\Delta_{U,x^*}$ on the mini-batches. 


\begin{proof}
    Following the calculation of Theorem 4 in \citet{mironov2019r} we have 

    
    $$D(M(X')| M(X)) \\ \leq D_{\alpha}((1-\mathbb{P}_{x^*}(1))N(0,\sigma^2) + \mathbb{P}_{x^*}(1)N(\Delta_{U,x^*},\sigma^2)|| N(0,\sigma^2))$$
    
    where instead of using $||U(X_B) - U(X_B \cup x^*)||_2 \leq 1$ for $X_B$ batches from $X$ as in the proof of the theorem we used $||U(X_B) - U(X_B \cup x^*)||_2 \leq \Delta_{U,x^*}$ by the definition of $\Delta_{U,x^*}$. Similarly, we have $D(M(X)| M(X')) \leq D_{\alpha}( N(0,\sigma^2) || (1-\mathbb{P}_{x^*}(1))N(0,\sigma^2) + \mathbb{P}_{x^*}(1)N(\Delta_{U,x^*},\sigma^2))$.

    Analogous to Corollary 7 in \citet{mironov2019r} we have

    \begin{multline}
        D_{\alpha}((1-\mathbb{P}_{x^*}(1))N(0,\sigma^2) + \mathbb{P}_{x^*}(1)N(\Delta_{U,x^*},\sigma^2)|| N(0,\sigma^2)) \\ \geq D_{\alpha}( N(0,\sigma^2) || (1-\mathbb{P}_{x^*}(1))N(0,\sigma^2) + \mathbb{P}_{x^*}(1)N(\Delta_{U,x^*},\sigma^2))
    \end{multline}
    
    where instead of using $\nu(x) = 1 -x$ we use $\nu(x) = \Delta_{U,x^*} - x $ which still satisfies $\nu = \nu^{-1}$

    Now one follows the integer $\alpha$ calculations in Section 3.3 of \citet{mironov2019r}, to conclude our theorem statement. The only change is that instead of computing $\mathbb{E}_{z \sim N(0,\sigma^2)}(\frac{N(1,\sigma^2)}{N(0,\sigma^2)})^k$ one computes $\mathbb{E}_{z \sim N(0,\sigma^2)}(\frac{N(\Delta_{U,x^*},\sigma^2)}{N(0,\sigma^2)})^k$ and following analogous calculation get $\leq \exp(\frac{\Delta_{U,x^*}^2(k^2 - k)}{2 \sigma^2})$.

    
\end{proof}






\subsubsection{A Generalized R\'enyi-DP Composition}
\label{ssec:comp}




With now an analysis for the per-step guarantees from DP-SGD (which as currently implemented uses the sum update rule), we now resolve how to obtain a per-instance RDP bound for a full training run with DP-SGD without the limitations of past composition theorem (see Section~\ref{ssec:back_full_comp} for a discussion on past composition bounds). In particular, we provide a composition theorem that bounds the overall per-instance privacy leakage by the ``expected" per-instance privacy guarantee at each step when training on a given dataset. This is presented in Theorem~\ref{thm:better_composition}.


More technically, we once again generalize the classical analysis to look at arbitrary $L_p$ norms, but now for the composition step. The classical R\'enyi DP composition theorem implicity uses the $L_\infty$ norm of the distribution of per-step guarantees at each step (coming from the distribution of possible models at each step as training is random), and Theorem~\ref{thm:better_composition} generalizes this to arbitrary $L_p$ norms of the exponential of the per-step guarantees (with some constants to scale). By using $L_p$ norms with $p < \infty$ we take advantage of cases where many models have better privacy guarantees than the worst model. 




\begin{theorem}
\label{thm:better_composition}

    Let $p \in (1,\infty)$ and consider a sequence of functions $X_1(x_1),$ $X_2(x_1,x_2),\cdots X_n(x_{n-1},x_n)$ where $X_{i}$ is a density function in the second argument for any fixed value of the first argument, except $X_1$ which is a density function in $x_1$. Consider an analogous sequence $Y_1(x_1),\cdots, Y_n(x_{n-1}, x_n)$. Then letting $X = \prod_{j=1}^{n} X_j$ be the density function for a sequence $x_1,\cdots,x_n$ generated according to the Markov chain defined by $X_i$, and similarly $Y$, we have 
    
    
    \begin{multline}
        D_{\alpha}(X || Y)  \leq \frac{1}{\alpha -1} (\sum_{i=0}^{n-2} \frac{(p-1)^i}{p^{i+1}} \ln (\mathbb{E}_{X_1,\cdots X_{n-(i+1)}}  (e^{(g_p^{i}(\alpha) -1)D_{g_p^{i}(\alpha)}(X_{n-i}|| Y_{n-i})p}))) \\ + \frac{1}{\alpha -1} (\frac{p-1}{p})^{n-1} (g_p^{n-1}(\alpha) -1)D_{g_p^{n-1}(\alpha)}(X_{1}|| Y_{1}) 
    \end{multline}


    where $g_p(\alpha) = \frac{p}{p-1} \alpha - \frac{1}{p}$ and $g_p^{i}$ is $g_p$ composed $i$ times, where we defined $g_p^{0}(\alpha) = \alpha$.
\end{theorem}

Note some key variables in Theorem~\ref{thm:better_composition} are a flexible parameter $p$ (which we'll soon describe leads to blow-up as it gets smaller), and the distribution of per-step guarantees $D_{g_p^{i}(\alpha)}(X_{n-i}|| Y_{n-i})$ (the more concentrated at $0$ they are, the smaller the upper-bound). The proof relies on using an induction argument to continually break up the composition and is presented below.

\begin{proof}


The proof follows by repeating a similar reduction as Theorem~\ref{thm:composition}. First note 
    
\begin{multline}
    \int (X_1 \cdots X_n)^{\alpha} (Y_1 \cdots Y_n)^{1 - \alpha} dx_1 \cdots dx_n \\  = \int (X_1 \cdots X_{n-1})^{\alpha - 1/p} (Y_1 \cdots Y_{n-1})^{1 - \alpha}  \\ (\int X_n^{\alpha} Y_n^{1- \alpha} dx_n) (X_1 \cdots X_{n-1})^{1/p} dx_1 \cdots dx_{n-1}
    \\ \leq ( \int (X_1 \cdots X_n)^{\frac{p}{p-1}\alpha - \frac{1}{p-1}} (Y_1 \cdots Y_n)^{ \frac{p}{p-1}(1 - \alpha)}  dx_1 \cdots dx_{n-1})^{\frac{p-1}{p}} \\ (\int (\int X_n^{\alpha} Y_n^{1- \alpha} dx_n)^p (X_1 \cdots X_{n-1}) dx_1 \cdots dx_{n-1})^{1/p} 
\end{multline}

where the first equality was from using the markov property, and the last inequality was from Holder's inequality with Holder constant $p$. Do note that, defining $g_p(\alpha) = \frac{p}{p-1}\alpha - \frac{1}{p-1}$, we have $\frac{p}{p-1}(1 - \alpha) = 1 - g_p(\alpha)$. So now looking at the first term of the upper-bound we got, we are back to the original expression but with $\alpha \rightarrow g_p(\alpha)$ and $n \rightarrow n-1$, and an exponent to $\frac{p-1}{p}$. Note the second term is an expectation over the $n-1$ model state of the Markov chain. Do note $\int X_n^{\alpha} Y_n^{1- \alpha} dx_n$ is $e^{(\alpha -1)D_{\alpha}(X_{n-i}|| Y_{n-i})}$ for a fixed $n-1$ model state (i.e., fixed $x_{n-1}$ ). So repeating this step on the first term until we are left only with an integral over $x_1$ we have

\begin{multline}
    \int (X_1 \cdots X_n)^{\alpha} (Y_1 \cdots Y_n)^{1 - \alpha} dx_1 \cdots dx_n \\  
    \leq (\prod_{i=0}^{n-2} (\mathbb{E}_{X_1,\cdots X_{n-(i+1)}}  ((e^{(g_p^{i}(\alpha) -1)D_{g_p^{i}(\alpha)}(X_{n-i}|| Y_{n-i})})^p))^{\frac{(p-1)^i}{p^{i+1}}}) \\ ( (e^{(g_p^{n-1}(\alpha) -1)D_{g_p^{n-1}(\alpha)}(X_{1}|| Y_{1})})^p)^{\frac{(p-1)^{n-1}}{p^n}}
\end{multline}

So now noting $$D_{\alpha}(X || Y) = \frac{1}{\alpha -1} \ln(\int (X_1 \cdots X_n)^{\alpha} (Y_1 \cdots Y_n)^{1 - \alpha} dx_1 \cdots dx_n)$$

we conclude by the previous expression that 

\begin{multline}
        D_{\alpha}(X || Y) \leq \frac{1}{\alpha -1} (\sum_{i=0}^{n-2} \frac{(p-1)^i}{p^{i+1}}  \ln (\mathbb{E}_{X_1,\cdots X_{n-(i+1)}}  ((e^{(g_p^{i}(\alpha) -1)D_{g_p^{i}(\alpha)}(X_{n-i}|| Y_{n-i})p}))) \\ + \frac{1}{\alpha -1} ((\frac{(p-1)^{n-1}}{p^n}) \ln ((e^{(g_p^{n-1}(\alpha) -1)D_{g_p^{n-1}(\alpha)}(X_{1}|| Y_{1})})^p)) 
    \end{multline}

which completes the proof as the last term simplifies to the term stated in the theorem.
\end{proof}


\paragraph{Applying to DP-SGD.} To interpret Theorem~\ref{thm:better_composition} in the context of DP-SGD, we can let $X_i$ be the distribution of the $i'th$ model update (for a fixed $(i-1)'th$ model) when training on one dataset $D$, and similarly $Y_i$ when training on a neighbouring dataset $D'$. Letting $Train_{DP-SGD}$ denote the Markov chain of the intermediate model updates when using DP-SGD, we have the maximum over the bound given by Theorem~\ref{thm:better_composition} on $D_{\alpha}(Train_{DP-SGD}(D)||Train_{DP-SGD}(D'))$ and $D_{\alpha}(Train_{DP-SGD}(D')||Train_{DP-SGD}(D))$ provides our per-instance RDP guarantee for DP-SGD.




\paragraph{Balancing the value of $p$.}To understand the dependence on $p$ in Theorem~\ref{thm:better_composition}, consider for a moment $p =2$. In this case, we observe that at the $i$'th step, we need to compute a R\'enyi divergence of order $\sim 2^{i} \alpha$. It is known that the R\'enyi divergence $D_{c}(P||Q)$ grows with $c$ \citep{van2014renyi}, and in the case of the Gaussian mechanism, this growth is linear with $c$~\citep{mironov2017renyi}. Hence this exponential growth in the R\'enyi divergence order can prove impractical as a useful tool to analyze DP-SGD. However, as $p \rightarrow \infty$ we see that the growth on the order of the divergence shrinks.

Yet, by taking larger $p$ values we are effectively taking larger $L_{p}$-norms of the per-step guarantees seen in training and so effectively turn to worst-case per-step analysis as $p \rightarrow \infty$. Hence it is desirable to choose $p$ just sufficient for there to not be a significant blow-up in the order of the divergences for a given $n$. This can be done by analyzing how $g_{p}^{i}(\alpha)$ grows.

\begin{fact}\label{fact:p_control}
    If $p = O(n)$ then $g_{p}^i(\alpha) \leq 2 \alpha~\forall i \leq n$. In particular, $p = 3n$ works for sufficiently large $n$.
\end{fact}

The proof follows from direct calculations with the formula for $g_{p}(\alpha)$. 

\begin{proof}

Note that $g_{p}(\alpha) \leq \frac{p}{p-1}\alpha$ hence $g_{p}^{i}(\alpha) \leq (\frac{p}{p-1})^{i}\alpha$. From this we see showing $\frac{p}{p-1}^{n} \leq 2$ for $p = O(n)$ will imply $g_p^{i}(\alpha) \leq 2 \alpha~\forall 1 \leq n$.

Note we can equivalently show $ln(\frac{p}{p-1}) = \ln(p) - \ln(p-1) \leq \frac{\ln (2)}{n}$. But if we take $p = 3n$ note $\ln(3n) - \ln(3n-1) \leq \frac{1}{3n-1}$ by the derivative of $\ln(x) \leq \frac{1}{3n-1}$ for $x \geq 3n-1$. So it suffices to show $\frac{1}{3n-1} \leq \frac{\ln(2)}{n}$, but this is true for sufficiently large $n$.

    
\end{proof}


\paragraph{Estimating Theorem~\ref{thm:better_composition}}


In cases where one does not know the expectations used in Theorem~\ref{thm:better_composition} analytically, as is the case with DP-SGD when it is applied to deep learning, one can resort to empirically estimating the means. Our goal is to understand how much better our data-dependent guarantees are than the data-independent baseline for DP-SGD on common datasets. Hence, we wish to estimate the expression of Theorem~\ref{thm:better_composition} (or specifically the per-step contributions) with an error
$c \epsilon$ for $c < 1$ where $\epsilon$ is the data-independent guarantee (per-step). 


The following fact focuses on estimating the $i'th$ per-step guarantee with an error relative to the worst-case per-step guarantee when $p = 3n$ as is used in our experiments. In particular, we have the $i'th$ per-step guarantee 

\begin{multline*}
    \frac{1}{\alpha-1} \frac{(p-1)^i}{p^{i+1}} \ln (\mathbb{E}_{X_1, \cdots, X_{n-(i+1)}} f) \\ \coloneqq \frac{1}{\alpha-1} \frac{(p-1)^i}{p^{i+1}} \ln (\mathbb{E}_{X_1,\cdots X_{n-(i+1)}}  ((e^{(g_p^{i}(\alpha) -1)D_{g_p^{i}(\alpha)}(X_{n-i}|| Y_{n-i})})^p))
\end{multline*}

is less than the data-independent per-step privacy guarantee $\epsilon/n$ if $\mathbb{E}_{X_1,\cdots X_{n-(i+1)}} f \leq e^{(\alpha-1) 3 \epsilon}$ for $p = 3n$. Hence we describe the number of samples needed to estimate $\mathbb{E} f$ with precision relative to $e^{(\alpha-1) 3 \epsilon}$ (with high probability), which can be done in a constant number of samples relative to the data-independent bound.

\begin{fact}\label{fact:estimating}
    Let $\epsilon/n$ be the classical $\alpha$-R\'enyi DP guarantee for the $i'th$ step, and $\epsilon'/n$ be the analogous $2\alpha$-R\'enyi DP guarantee for the $i'th$ step. Then for $l \geq \frac{- \ln(J)}{c^2} e^{6(\alpha-1)\epsilon - 3(2\alpha -1) \epsilon'}$ and $p = 3n$ with $n$ s.t $g_p^{n-1} \leq 2\alpha$, we have $\mathbb{P}(|\mathbb{E}^{l} f - \mathbb{E}f| \geq c e^{(\alpha-1) 3 \epsilon}) \leq J$. Here $\mathbb{E}^l$ denotes the empirical mean over $l$ samples.
\end{fact}

The proof follows from Hoeffding's inequality.

\begin{proof}
    

For the given choice of $p$ and $\alpha$ we have $g_{p}^{i} \leq 2\alpha$ hence $D_{g_p^{i}(\alpha)}(X_{n-i} || Y_{n-i}) \leq D_{2 \alpha}(X_{n-i} || Y_{n-i}) \leq \epsilon'/n$ where $\epsilon'$ is determined by $\epsilon$ (when accounting for the increase due to the $\alpha$-order). Hence we have that $f \leq e^{3 (2\alpha -1) \epsilon'}$.

By Hoeffding's inequality we can hence conclude $\mathbb{P}(|\mathbb{E}^l f - \mathbb{E}f| \geq c e^{3(\alpha-1)\epsilon}) \leq e^{-\frac{e^{6(\alpha-1)\epsilon}c^2 l}{e^{3(2\alpha - 1) \epsilon'}}}$. Now upper-bounding the right-hand side by $J$ and rearranging to isolate for $l$, we can conclude the stated condition on $l$.

\end{proof}


\subsubsection{Per-Instance R\'enyi DP for General Updates}



The results of Section~\ref{ssec:sum_update} and Section~\ref{ssec:comp} provide a complete per-instance RDP analysis of the current implementation of DP-SGD. In particular, with the per-step update rule being the sum of gradients. In this section we ask, how should we analyze per-step guarantees (and hence DP-SGD given our composition theorem) if the update rule is not the sum? In general, the worst-case sensitivity over mini-batches may be far higher than the expected sensitivity over mini-batches (unlike the sum update rule), meaning the analysis from Theorem~\ref{thm:easy_renyi_dp} may be as bad as a data-independent analysis. For example, the typical update rule used in normal SGD is the mean update rule. However, $\Delta_{U,x^*}(X_B)$ for the mean update rule is the difference between the update for the datapoint $x^*$ and the mean of the updates on $X_B$; this difference is not the same for all minibatches $X_B$ and hence would be overestimated with the analysis of Theorem~\ref{thm:easy_renyi_dp}. One could resolve this issue of overestimating sensitivity by using the $L_p$ norms $||\Delta_{U,x^*}(\mathbf{X_B})||_{p} = (\mathbb{E}_{X_B} (\Delta_{U,x^*}(X_B)^p))^{1/p}$ with $p < \infty$ in the RDP analysis of the sampled Gaussian mechanism, as was done in the $(\epsilon,\delta)$-DP case. %
However, we are not aware of an approach to do this for R\'enyi DP.




Instead, we show how a new sensitivity distribution comparing all mini-batches $X_B$ in $X$ to all mini-batches $X'_B$ in $X' = X \cup \{x^*\}$, as opposed to just a single point $x^*$ as done with $\Delta_{U,x^*}(X_B)$, is amenable to a R\'enyi-DP analysis of the sampled Gaussian mechanism that does not look at the maximum privacy leakage over mini-batches. %
If the distribution of all updates given by $X$ is similar to the distribution of all updates given by $X'$, then analysis with this new sensitivity distribution can be expected to beat the current data-independent analysis.


Specifically, given $\alpha$ minibatches sampled from $X$, ${X_{B}}^{\tilde \alpha} \sim X$, and a particular minibatch sampled from $X'$, $X'_B \sim X'$, we define a new sensitivity distribution for $\alpha$-R\'enyi DP as follows: 

\begin{multline*}
\Delta_{U,\alpha}({X_{B}}^{\tilde \alpha}, X'_B) \coloneqq \sum_{i} ||U({X_B}^i)||_2^2 - (\alpha-1) ||U(X'_B)||_2^2 - ||\Delta_{\alpha}({X_B}^{\tilde \alpha},X'_B)||_2^2
\end{multline*}


where $\Delta_{\alpha}({X_B}^{\tilde \alpha},X'_B) = (\sum_{i} U({X_B}^i)) - (\alpha - 1) U(X'_B)$. When letting ${X_{B}}^{\tilde \alpha}$ and $X'_B$ be random variables, $\Delta_{U,\alpha}$ effectively compares all the mini-batches in $X'$ to all the $\alpha$-tuples of mini-batches in $X$. The $\alpha$-tuples appear here due to their equivalence with an expectation over mini-batches to the power of $\alpha$ which appears when analyzing $\alpha$-R\'enyi divergences. As described earlier, comparing this to the previous sensitivity distribution $\Delta_{U,x^*}(X_B)$, we see that this new sensitivity will compare all mini-batches in $X$ to all mini-batches in $X'$ (and not just to a point $x^*$) and hence captures more ``global" changes in updates due a datapoint $x^*$.




Theorem~\ref{thm:renyi_dp_sens} states the R\'enyi diveregence of the sampled Gaussian mechanism $M$ between two arbitrary datasets using $\Delta_{U, \alpha}$ through applying a transformation on its fixed $X'_B$ marginal values and taking its expectation over $X'_B$. Taking the maximum of the bounds for $D_{\alpha}(M(X)||M(X'))$ and $D_{\alpha}(M(X')||M(X))$ from Theorem~\ref{thm:renyi_dp_sens} where $X' = X \cup \{x^*\}$ gives a per-instance guarantee of $M$ for $X,X'$.




\begin{theorem}
\label{thm:renyi_dp_sens}
Let $\alpha > 1$ be an integer. Given two arbitrary datasets $X,X'$, the sampled Gaussian mechanism $M$ with noise $\sigma$ satisfies: 

$$D_{\alpha}(M(X')||M(X)) \leq \frac{1}{(\alpha-1)} \mathbb{E}_{X_B} (\ln (\mathbb{E}_{{X'_{B}}^{\tilde \alpha}}(e^{\frac{-1}{2\sigma^2}\Delta_{U,\alpha}({X'_{B}}^{\tilde \alpha}, X_B)})))$$



\end{theorem}



Some key variables in Theorem~\ref{thm:renyi_dp_sens} is the standard deviation of noise $\sigma$ (increasing it decreases the upper-bound) and the sensitivity distribution $\Delta_{U,\alpha}({X_{B}}^{\tilde \alpha}, X'_B)$ (the more concentrated at $0$ it is, the smaller the upper-bound). The proof relies on convexity, which is always true for the second argument of the R\'enyi divergence $D_{\alpha}(A||B)$, and then direct calculations involving Gaussians. %

\begin{proof}

For simpler notation, we use $\mu_X = U(X)$. We proceed by taking $\alpha$ to be an integer (to use an expansion similar to Section 3.3 in \citet{mironov2019r}) and utilizing Theorem 12 in~\citet{van2014renyi}. We will let $N_{X_B} = N(\mu_{X_B},\sigma^2)$ where $\mu_{X_B} = U(X_B)$ as stated earlier.


We proceed to bound $D_{\alpha}(M(X') || M(X))$ for arbitrary $X',X$. Hence a completely analogous argument will allow us to also bound $D_{\alpha}(M(X) || M(X'))$ when $X'$ is specifically $X \cup \{x^*\}$. First note


\begin{multline}
    D_{\alpha}(M(X') || M(X)) = D_{\alpha}(\sum_{X'_B} \mathbb{P}(X'_B) N_{X'_B} || \sum_{X_B} \mathbb{P}(X_B) N_{X_B}) \\ \leq \sum_{X_B} \mathbb{P}(X_B) D_{\alpha}(\sum_{X'_B} \mathbb{P}(X'_B) N_{X'_B} || N_{X_B})
\end{multline}


where the last inequality is from the fact the divergence is convex in the second argument (Theorem 12 in~\citet{van2014renyi}). 

Now note 
\begin{multline}
    e^{(\alpha-1)D_{\alpha}(\sum_{X'_B} \mathbb{P}(X'_B) N_{X'_B} || N_{X_B})} \\ = \int (\sum_{X'_B}\mathbb{P}(X'_B) \frac{1}{(\sigma \sqrt{2\pi})^d} e^{\frac{-1}{2\sigma^2} |x - \mu_{X'_B}|^2})^{\alpha} (\frac{1}{(\sigma \sqrt{2\pi})^d} e^{\frac{-1}{2\sigma^2}|x - \mu_{X_B}|^2})^{1- \alpha} dx \\ = \sum_{{X'_B}^{\tilde \alpha}} \mathbb{P}({X'_B}^{\tilde \alpha}) \frac{1}{(\sigma \sqrt{2\pi})^d} \int e^{\frac{-1}{2\sigma^2} ( (\sum_{{X'_B}^i}|x- \mu_{{X'_B}^i}|^2) - (\alpha - 1)|x - \mu_{X_B}|^2)}
\end{multline}


where we expanded $(\sum_{X'_B}\mathbb{P}(X'_B) \frac{1}{(\sigma \sqrt{2\pi})^d} e^{\frac{-1}{2\sigma^2} |x - \mu_{X'_B}|^2})^{\alpha}$ by noting each term in the product is just iterating through all $\alpha$ tuples of mini-batches from $X'$.

Now note we can for now consider the integral in each dimension, as the overall integral is the product of each dimension. Also recall from the theorem statement that we define $$\Delta_{\alpha}({X'_B}^{\tilde \alpha},X_B) = (\sum_{i} \mu_{{X'_B}^i}) - (\alpha - 1) \mu_{X_B}$$ Hence (letting everything be one dimensional for now) we have

\begin{multline}
    (\sum_{{X'_B}^i}|x- \mu_{{X'_B}^i}|^2) - (\alpha - 1)|x - \mu_{X_B}|^2 \\ = x^2 - 2 \Delta_{\alpha}({X'_B}^{\tilde \alpha},X_B)x + \sum_{i} \mu_{{X'_B}^i}^2 - (\alpha-1) \mu_{X_B}^2 \\ = (x - \Delta_{\alpha}({X'_B}^{\tilde \alpha},X_B))^2 + \sum_{i} \mu_{{X'_B}^i}^2 - (\alpha-1) \mu_{X_B}^2 - \Delta_{\alpha}({X'_B}^{\tilde \alpha},X_B)^2
\end{multline}


Hence, we have 

\begin{multline}
    \int e^{\frac{-1}{2\sigma^2} ( (\sum_{{X'_B}^i}|x- \mu_{{X'_B}^i}|^2) - (\alpha - 1)|x - \mu_{X_B}|^2)} \\ = e^{\frac{-1}{2\sigma^2}(\sum_{i} {\mu_{{X'_B}^i}}^2 - (\alpha-1) \mu_{X_B}^2 - \Delta_{\alpha}({X'_B}^{\tilde \alpha},X_B)^2)} \int e^{\frac{-1}{2\sigma^2} (x - \Delta_{\alpha}({X'_B}^{\tilde \alpha},X_B))^2} \\ = \sigma \sqrt{2 \pi} e^{\frac{-1}{2\sigma^2}(\sum_{i} \mu_{{X'_B}^i}^2 - (\alpha-1) \mu_{X_B}^2 - \Delta_{\alpha}({X'_B}^{\tilde \alpha},X_B)^2)}
\end{multline}


Note going back to the integral over all dimensions we get $$= (\sigma \sqrt{2 \pi})^{d} e^{\frac{-1}{2\sigma^2}(\sum_{i} ||{\mu_{{X'_B}^i}||_2}^2 - (\alpha-1) ||\mu_{X_B}||_2^2 - ||\Delta_{\alpha}({X'_B}^{\tilde \alpha},X_B)||_2^2)}$$

Thus to conclude we get 

\begin{multline}
    D_{\alpha}(M(X') || M(X))  \leq \sum_{X_B} \mathbb{P}(X_B) D_{\alpha}(\sum_{X'_B} \mathbb{P}(X'_B) N_{X'_B} || N_{X_B}) \\ = \sum_{X_B} \mathbb{P}(X_B) \frac{1}{(\alpha-1)} \\ \ln (\sum_{{X'_B}^{\tilde \alpha}} \mathbb{P}({X'_B}^{\tilde \alpha}) e^{\frac{-1}{2\sigma^2}(\sum_{i} ||\mu_{{X'_B}^i}||_2^2 - (\alpha-1) ||\mu_{X_B}||_2^2 - ||\Delta_{\alpha}({X'_B}^{\tilde \alpha},X_B)||_2^2)})
\end{multline}

A completely analogous calculation gives the same bound with just $X_B$ replaced with $X_B'$ (and vice-versa) for $D_{\alpha}(M(X)||M(X'))$. Taking the max over both these divergences gives a bound on the per-step per-instance R\'enyi-DP guarantee.

\end{proof}


Hence we now have a per-step RDP analysis for DP-SGD that takes advantage of when expected minibatch sensitivity to $x^*$ is much better than the worst cast minibatch sensitivity. While this phenomenon is not useful for studying the sum update rule (what is currently used for DP-SGD) as every mini-batch has the same sensitivity to $x^*$, in Section~\ref{ssec:exp_hard_renyi} we show this analysis allows us to provide a tighter analysis of the mean update rule. Hence, this opens the possibility of future work deploying DP-SGD with different update rules.
