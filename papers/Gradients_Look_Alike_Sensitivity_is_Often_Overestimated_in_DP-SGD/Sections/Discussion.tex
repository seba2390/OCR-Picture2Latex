\section{Discussion}


Here we first discuss past work on composition theorems (Section~\ref{ssec:back_full_comp}) and the current computational trade-offs of our analysis which future work could improve (Section~\ref{ssec:comp_lims}). We then discuss some theoretical questions based on observations from our analysis (Section~\ref{ssec:theoretical_questions}). Lastly, we describe several applications of our analysis (Section~\ref{ssec:apps}).

\subsection{Fully Adaptive Composition Theorems}
\label{ssec:back_full_comp}


One of the main technical contributions of this paper is generalizing the normal R\'enyi DP composition theorem (Proposition 1 in \citet{mironov2017renyi}), which sums worst-case per-step guarantees, to allow for better per-instance analysis. Other work have also generalized the composition theorem to have better per-instance analysis~\citep{feldman2021individual, koskela2022individual}, and called these new theorems Fully Adaptive Composition. For R\'enyi DP, \citet{feldman2021individual} showed that composition can be done by considering the worst-case sum of the per-step guarantees from a DP-SGD training run (Theorem~3.1 in \citet{feldman2021individual}), as opposed to summing the worst-case guarantee at each step. However, for DP-SGD, the degree of improvement provided by the worse-case sum compared to the normal composition is not clear. It could be that the worst-case sum is equal to the sum of the worst-case per-step guarantees if the training run goes to bad states with non-zero probability at each step. Furthermore, it is hard to measure the worst-case sum to show this is not the case. Instead, our composition theorem resolves this issue by showing we only need ``expected" per-step guarantees which can be reliably estimated.






\subsection{Computational Limitations and Future Improvements}
\label{ssec:comp_lims}





As explained in Section~\ref{ssec:comp}, there is a tension between preventing blow-up in our composition theorem (Theorem~\ref{thm:better_composition}) and estimating the per-step contributions with few samples: both require manipulating a parameter $p$, with the former requiring large $p$ and the latter requiring small $p$. We showed in Section~\ref{ssec:comp} how the value of $p$ we chose for our experiments strikes a balance where we can limit blow-up while still estimating the per-step contribution to the composition with few samples. This balance is further backed up by the confidence intervals for our estimates of the per-step contributions (see Figure~\ref{fig:confid_10points}). Nevertheless, we still require several training runs to compute the per-instance guarantee for a specific point.

\begin{figure}
    \centering
    \includegraphics[width=0.4\linewidth]{figures/confid_10points_simple_eps_compo_vary_eps_fraction_curve_MNIST_point.pdf}
    \caption{The expected per-step guarantees needed for Theorem~\ref{thm:better_composition} (using Theorem~\ref{thm:easy_renyi_dp} for the per-step guarantee) for 10 different points in MNIST computed once each epoch when training with the datapoint (i.e., on $X' = X \cup \{x^*\}$). The shaded region is the $95\%$ confidence interval over 10 trials.}
    \label{fig:confid_10points}
\end{figure}

Future work may be able to improve (analytically) the trade-off between blow-up and sample complexity for Theorem~\ref{thm:better_composition}, and hence make it cheaper to compute the per-instance DP guarantees. Future work may also be able to derive composition theorems analogous to Theorem~\ref{thm:better_composition} which are easier to estimate. Similarly, Theorem~\ref{thm:renyi_dp_sens} is computationally expensive to compute; we require computing several mini-batch updates at every step to estimate the expectations. Future work may be able to derive alternative per-step guarantees for general update rules that are easier to empirically estimate.


\subsection{Theoretical Questions}
\label{ssec:theoretical_questions}

In this paper we applied our analysis of DP-SGD to deep learning. However, one can still apply our analysis of DP-SGD to classical machine learning. For example, theoretical work has shown improved data-independent privacy guarantees for convex losses \cite{altschuler2022privacy}; the proof relied on the update rule being a contraction for convex losses. In contrast to this approach, our analysis can directly translate smaller gradient norms (in expectation) during training to better privacy guarantees. Hence, we believe an interesting future direction is applying our analysis to learning settings that permit direct calculations of sensitivity distributions.

In this direction, a particular phenomenon we wish to highlight is that by training with less noise we sometimes see a disproportional decrease in per-instance privacy consumption. This is shown in Figure~\ref{fig:compo_1_more_10per} where training with the noise level for $\epsilon=10$ (data-independent bound) resulted in disproportionately smaller per-step privacy consumption than training with the higher noise level for $\epsilon=1$ (data-independent bound). Stated another way, our analysis shows more noise is not always better for privacy. In settings where sensitivity distributions can be explicitly calculated, one may be able to compute what this noise vs. per-instance privacy trade-off is.




\subsection{Applications}
\label{ssec:apps}


In this paper, we focused on explaining how privacy attacks will fail for many datapoints if the adversary only observes typical datasets common to deep learning. This was done by providing a new per-instance DP analysis for DP-SGD. We now highlight other applications of our analysis.



\paragraph{Estimating Privacy} A growing theme in private deep learning is empirically ``estimating" privacy in different data settings, and is broadly encapsulated by privacy auditing~\cite{nasr2021adversary,Nasr:2023, Zanella-Beguelin:2022, Jagielski:2020}. However, here estimating means obtaining lower bounds on privacy leakage (e.g., the parameter $\epsilon$ used in differential privacy). Our work presents a shift in how we can go about estimating privacy. Our analysis provides \emph{upper-bounds}, and when direct calculations of expected sensitivity distributions are not possible, one can still estimate our upper-bounds by repeating training. Future work can hence use our analysis to provide potentially matching upper-bounds to lower-bounds obtained with specific privacy attacks, and hence be able to conclude that these privacy attacks are optimal in more settings.

\paragraph{Estimating Memorization} Related to estimating privacy, a broad literature is concerned with measuring memorization~\cite{carlini2019secret,zhang2019identity,feldman2020neural,carlini2022quantifying,tirumala2022memorization}. The methodology for estimating memorization varies, but includes privacy attacks~\cite{carlini2022quantifying}, or approximations of influence~\cite{carlini2019secret,feldman2020neural}. Our work provides, to the best of our knowledge, the first approach to estimating memorization via upper bounds. Hence, our work may provide a complementary tool to past work on memorization. 

To elaborate on this connection to memorization, our theory shows that points whose gradients converge to $0$ quickly when training with or without them are less likely to be memorized. In other words, points that are optimized quickly are generalized too when using DP-SGD. This theory complements current state-of-the-art methods for predicting the correctness of the predictions on test points, which use how quickly predictions on the point converged (i.e., the point was optimized quickly) to predict which test inputs are correctly classified when using private (and not private) deep learning ~\cite{rabanser2022selective, rabanser2023training}. Future work may be able to use our theory to explain the performance of these methods.



\paragraph{Estimating Unlearning} Unlearning a datapoint $x^*$ is to obtain the model (distribution) coming from training on the dataset $D\setminus x^*$ given a model trained on the dataset $D$ \cite{cao2015towards}. The only known methods to do this exactly for deep learning are variations of naively retraining on $D \setminus x^*$ \cite{bourtoule2021machine}. Given the general intractability of exact unlearning, significant work has looked into \emph{approximate unlearning}; approximate unlearning is to obtain the same model (distribution) as training with $D \setminus x^*$ up to some error in a predefined metric. A popular measure of approximate unlearning has been using per-instance DP guarantees \cite{guo2019certified}, or only one of the per-instance DP inequalities~\cite{gupta2021adaptive}\cite{ginart2019making} (which is implied by the former). However, the only known methods (to the best of our knowledge) to achieve this kind of guarantee for deep learning is to train with DP-SGD and use the data-independent DP bound as the unlearning guarantee. Our analysis allows for unlearning guarantees that are specific to individual points. While DP-SGD does not explicitly target specific points to have better unlearning guarantees, future work may be able to use our analysis to derive a modified version of DP-SGD that explicitly unlearns a subpopulation of the training set (hence future deletion requests for that subpopulation are already handled). 




\paragraph{An Alternative Framework for Forgeability} However, underlying machine unlearning (as a legal requirement, e.g., the EU GDPR~\cite{mantelero2013eu}) is the problem of whether an auditor can ever claim an entity did not unlearn a point. That is, can a model trainer claim to have trained without a point even if they in fact did? Forgeability~\cite{thudi2022necessity} is a framework under which a model trainer can claim to have obtained their model by training on a dataset they did not in fact train on. To make a claim of training on a given dataset, forgeability relies on providing a valid Proof-of-Learning (PoL)~\cite{jia2021proof} that uses the claimed dataset (different from what the model trainer originally used). Recall PoL is a sequence of checkpoints and minibatches for which the update from $i'th$ minibatch given the $i'th$ checkpoint leads to the $i+1'th$ checkpoint upto some error $\delta$ in a metric $d$. However, it is currently not known how to properly pick the threshold $\delta$ and metric $d$ to define a ``valid" update for a PoL (due to a lack of models for the backend noise during training), or how to make PoL efficient to verify without introducing additional security risks~\cite{fang2023proof}. Hence the current framework for forgeability may not be robust until PoL is better understood.

As an alternative to using PoL, a per-instance DP guarantee tells us that it is very likely we would have obtained the same sequence of checkpoints with either of the two datasets. Hence, when an auditor claims a model trainer trained on a point (and the point has strong per-instance DP guarantees), the trainer can refute by submitting a dataset without the point and their original sequence of checkpoints and the details of their DP training implementation. Given this, an 
auditor that only has the information provided in the submitted proof can no longer distinguish between whether a trainer had or had not trained on the point. 






