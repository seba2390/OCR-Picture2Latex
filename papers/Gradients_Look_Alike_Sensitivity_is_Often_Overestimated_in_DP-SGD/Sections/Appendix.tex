

\section{Proofs}

\subsection{Proof of Lemma~\ref{lem:holder_approach}}
\label{proof:holder_approach}

\begin{lemma}
\label{lem:holder_approach}
With the above notation, and $X' = X \cup \{x^*\}$, we have 


\begin{multline}
    \mathbb{P}(M(X') \in S) \\ \leq \mathbb{P}_{x^*}(1) \mathbb{E}_{X_B}(e^{C_{\delta,\sigma} \Delta_{U,x^*}(X_B)p})^{1/p} \mathbb{P}(M(X) \in S)^{1-1/p} \\ + \mathbb{P}_{x^*}(1)\delta + \mathbb{P}_{x^*}(0) \mathbb{P}(M(X) \in S)
\end{multline}

\end{lemma}

\begin{proof}
    
First note sampling mini-batches from $X'$ is equivalent to sampling a mini-batch $X_B$ from $X$, then sampling $x^*$ with probability $\mathbb{P}_{x^*}(1)$. Hence we have  

\begin{multline*}
    \mathbb{P}(M(X') \in S) = \sum_{x_B} (\mathbb{P}_{x^*}(1) \mathbb{P}(A(X_B \cup x^*) \in S) + \mathbb{P}_{x^*}(0) \mathbb{P}(A(X_B) \in S))\mathbb{P}(X_B)
\end{multline*}

Now note we have $\mathbb{P}(A(X_B \cup x^*) \in S) \leq e^{C_{\delta,\sigma}\Delta_{U,x^*}(X_B)} \mathbb{P}(A(X_B) \in S) + \delta$ by the $(\epsilon,\delta)$-DP guarantee of the Gaussian mechanism. So considering summing that over $X_B$ we have $\sum_{X_B} \mathbb{P}(A(X_B \cup x^*) \in S) \mathbb{P}(X_B) \leq \sum_{X_B} e^{C_{\delta,\sigma} \Delta_{U,x^*}(X_B)} \mathbb{P}(A(X_B) \in S) \mathbb{P}(X_B) + \delta$. Now we apply Holder's inequality to get $\sum_{X_B} e^{C_{\delta,\sigma} \Delta_{U,x^*}(X_B)} \mathbb{P}(A(X_B) \in S) \mathbb{P}(X_B) \leq \mathbb{E}_{X_B}((e^{C_{\delta,\sigma} \Delta_{U,x^*}(X_B)})^{p})^{1/p} \mathbb{E}_{x_B}(\mathbb{P}(A(x_B) \in S)^{q})^{1/q}$. Note that $\mathbb{P}(A(x_B) \in S)^{q} \leq \mathbb{P}(A(x_B) \in S)$ for $q \geq 1$ as $\mathbb{P}(A(x_B) \in S) \leq 1$. 


So we have 

\begin{multline}
\sum_{X_B} \mathbb{P}(A(X_B \cup x^*) \in S) \leq \mathbb{E}_{x_B}((e^{C_{\delta,\sigma} \Delta_{U,x^*}(X_B)})^{p})^{1/p} \mathbb{E}_{x_B}(\mathbb{P}(A(x_B) \in S))^{1/q} + \delta
\end{multline}

So to conclude we have

\begin{multline}
    \mathbb{P}(M(X') \in S) \mathbb{P}(X_B) \\ 
    \leq \mathbb{P}_{x^*}(1) \mathbb{E}_{x_B}((e^{C_{\delta,\sigma} \Delta_{U,x^*}(X_B)})^{p})^{1/p} \mathbb{E}_{x_B}(\mathbb{P}(A(x_B) \in S))^{1-1/p} \\ + \delta + \mathbb{P}_{x^*}(0) \mathbb{E}_{X_B}(\mathbb{P}(A(X_B) \in S))
\end{multline}

Note $\mathbb{E}_{X_B}(\mathbb{P}(A(X_B) \in S)) = \mathbb{P}(M(X) \in S)$ which completes the proof.


\end{proof}

\subsection{Proof of Corollary~\ref{cor:eps_delta_sens}}
\label{proof:eps_delta_sens}

\begin{proof}

The following proof is analogous to Proposition 3 in \cite{mironov2017renyi}.

Let $Q = \mathbb{P}(M(X) \in S)$ and note the first term in Lemma~\ref{lem:holder_approach} is then $(a_p^{\frac{1}{1-1/p}}Q)^{1- 1/p}$. Now consider two cases: if $a_p^{\frac{1}{1-1/p}}Q > \delta'^{\frac{p}{p-1}}$ we have $(a_p^{\frac{1}{1-1/p}}Q)^{1- 1/p} \leq a_p^{\frac{1}{1-1/p}}Q \cdot \delta'^{\frac{-1}{p-1}}$, and so the overall expression in Lemma~\ref{lem:holder_approach} is $\leq (a_p^{\frac{1}{1-1/p}} \delta'^{\frac{-1}{p-1}} + \mathbb{P}(0))Q + \mathbb{P}_{x^*}(1)\delta$.

Now else we have the first term is $\leq \delta'$ and the overall expression is $\leq \mathbb{P}_{\mathbf{x}^*}(0) Q + \mathbb{P}_{x^*}(1)\delta + \delta'$. Combining the two scenarios we see we always have $\mathbb{P}(M(X') \in S) \leq (a_p^{\frac{1}{1-1/p}}\delta'^{\frac{-1}{p-1}} + \mathbb{P}_{x^*}(0)) \mathbb{P}(M(X) \in S) + \mathbb{P}_{x^*}(1)\delta + \delta'$, giving the stated condition.

\end{proof}



\subsection{Proof of Theorem~\ref{thm:composition}}
\label{proof:composition}

We first present a version of Theorem~\ref{thm:better_composition} that uses Cauchy-Schwarz, i.e., Holder's inequality with Holder constant $p=2$. This we believe is easier to follow, and makes clearer the specific role of the Holder's constants in the proof of Theorem~\ref{thm:better_composition}


\begin{theorem}
\label{thm:composition}

    Consider a sequence of functions $X_1(x_1), X_2(x_1,x_2),$ \\ $ \cdots X_n(x_{n-1},x_n)$ where $X_{i}$ is a density function in the second arugment for any fixed value of the first argument, except $X_1$ which is a densitiy function in $x_1$. Consider an analogous sequence $Y_1(x_1),\cdots, Y_n(x_{n-1})$. Then letting $X = \prod_{j=1}^{n} X_j$ be the density function for a sequence $x_1,\cdots,x_n$ generated according to the Markov chain defined by $X_i$, and similarly $Y$, we have
    


    \begin{multline}
        D_{\alpha}(X || Y) \\ \leq \frac{1}{\alpha -1} (\sum_{i=0}^{n-2} \frac{1}{2^{i+1}} \ln (\mathbb{E}_{X_1,\cdots X_{n-(i+1)}}  ((e^{(g^{i}(\alpha) -1)D_{g^{i}(\alpha)}(X_{n-i}|| Y_{n-i})})^2))) \\ + \frac{1}{\alpha -1} ((\frac{1}{2})^{n} \ln ((e^{(g^{n-1}(\alpha) -1)D_{g^{n-1}(\alpha)}(X_{1}|| Y_{1})})^2)) 
    \end{multline}
    

    where $g(\alpha) = 2\alpha -1$ and $g^i$ means $g$ composed $i$ times, where $g^{0}(\alpha) = \alpha$
\end{theorem}

\begin{proof}


The proof relies on repeating the same reduction on the number of steps being considered. First note 
    
\begin{multline}
    \int (X_1 \cdots X_n)^{\alpha} (Y_1 \cdots Y_n)^{1 - \alpha} dx_1 \cdots dx_n \\  = \int (X_1 \cdots X_{n-1})^{\alpha - 1/2} (Y_1 \cdots Y_{n-1})^{1 - \alpha}  \\ (\int X_n^{\alpha} Y_n^{1- \alpha} dx_n) (X_1 \cdots X_{n-1})^{1/2} dx_1 \cdots dx_{n-1}
    \\ \leq ( \int (X_1 \cdots X_n)^{2\alpha -1} (Y_1 \cdots Y_n)^{1 - (2\alpha-1)}  dx_1 \cdots dx_{n-1})^{1/2} \\ (\int (\int X_n^{\alpha} Y_n^{1- \alpha} dx_n)^2 (X_1 \cdots X_{n-1}) dx_1 \cdots dx_{n-1})^{1/2}
\end{multline}

where the first equality was from using the markov property, and the last inequality was from Cauchy-Schwarz. So now looking at the first term, we are back to the original expression but with $\alpha \rightarrow g(a) = 2\alpha -1$ and $n \rightarrow n-1$, and an exponent to $1/2$. Note the second term is an expectation over the $n-1$ model state of the Markov chain. Do note $\int X_n^{\alpha} Y_n^{1- \alpha} dx_n$ is $e^{(\alpha -1)D_{\alpha}(X_{n-i}|| Y_{n-i})}$ for a fixed $n-1$ model state (i.e., fixed $x_{n-1}$ ). So repeating this step on the first term until we are left only with an integral over $x_1$ we have

\begin{multline}
    \int (X_1 \cdots X_n)^{\alpha} (Y_1 \cdots Y_n)^{1 - \alpha} dx_1 \cdots dx_n \\  
    \leq (\prod_{i=0}^{n-2} (\mathbb{E}_{X_1,\cdots X_{n-(i+1)}}  ((e^{(g^{i}(\alpha) -1)D_{g^{i}(\alpha)}(X_{n-i}|| Y_{n-i})})^2))^{(\frac{1}{2})^{i+1}}) \\ ( (e^{(g^{n-1}(\alpha) -1)D_{g^{n-1}(\alpha)}(X_{1}|| Y_{1})})^2)^{(\frac{1}{2})^{n}}
\end{multline}

So now noting $$D_{\alpha}(X || Y) = \frac{1}{\alpha -1} \ln(\int (X_1 \cdots X_n)^{\alpha} (Y_1 \cdots Y_n)^{1 - \alpha} dx_1 \cdots dx_n)$$

we conclude by the previous expression that 

\begin{multline}
    D_{\alpha}(X || Y) \\ \leq \frac{1}{\alpha -1} (\sum_{i=0}^{n-2} \frac{1}{2^{i+1}} \ln (\mathbb{E}_{X_1,\cdots X_{n-(i+1)}}  ((e^{(g^{i}(\alpha) -1)D_{g^{i}(\alpha)}(X_{n-i}|| Y_{n-i})})^2))) \\ + \frac{1}{\alpha -1} ((\frac{1}{2})^{n} \ln ((e^{(g^{n-1}(\alpha) -1)D_{g^{n-1}(\alpha)}(X_{1}|| Y_{1})})^2)) 
\end{multline}

which completes the proof.

\end{proof}









\input{Sections/Empirical_Results}


