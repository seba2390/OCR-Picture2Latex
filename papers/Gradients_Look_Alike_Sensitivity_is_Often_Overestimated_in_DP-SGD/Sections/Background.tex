\section{Background}

Here we describe the current data-independent privacy analysis of DP-SGD (Section~\ref{ssec:DP-SGD_back}) and the relevance of per-instance DP in explaining empirical privacy attacks in contrast to past approaches (Section~\ref{ssec:per-instance_back}). We also discuss the implication of per-instance DP for unlearning and memorization in Section~\ref{ssec:per-instance_back}. Later, in Section~\ref{ssec:back_full_comp}, we describe past work on generalizing composition theorems and how they are not applicable for a better per-instance analysis of DP-SGD.


\paragraph{Machine Learning Notation}
We consider a learning setup where we have a dataset $X = \{x_1,\cdots x_n\}$ with datapoints from some space $\mathfrak{X}$ (e.g., images and their labels). Given a loss function $\mathcal{L}: \mathbb{R}^d \times \mathfrak{X} \rightarrow \mathbb{R}$, our objective is to minimize the loss $\frac{1}{n}\sum_{x_i \in X} \mathcal{L}(\theta,x_i)$ with respect to the parameters $\theta \in \mathbb{R}^d$ of some model. The canonical approach to do this for deep learning models is to use stochastic gradient descent (SGD). However, we consider having an additional requirement that the models we obtain should not leak the ``privacy" of individual datapoints

\subsection{DP-SGD Analysis}
\label{ssec:DP-SGD_back}






DP~\citep{dwork2006calibrating} is the de-facto definition of privacy used in ML. The typical definition used in machine learning is given below, where one thinks of $M$ as the training algorithm:

\begin{definition}[$(\epsilon,\delta)$-DP]
    An algorithm $M$ is said to be $(\epsilon,\delta)$-DP if for all neighbouring datasets $X, X'$ (i.e. Hamming distance $1$ apart), we have that $$\mathbb{P}(M(X) \in S) \leq e^{\epsilon} \mathbb{P}(M(X') \in S) + \delta$$
\end{definition}

DP-SGD~\citep{abadi2016deep,bassily2014private, song2013stochastic} is an $(\epsilon,\delta)$-DP version of stochastic gradient descent (SGD) which clips the individual gradients and adds Gaussian noise to the mini-batch update. Formally, given a dataset $X$, DP-SGD repeatedly computes the following deterministic update rule $U(X_{B} = \{x: x \sim X~with~probability~\frac{L}{|X|}\}) = \sum_{x \in X_B} \nabla_{\theta}\mathcal{L}(\theta,x)/ \max(1,\frac{||\nabla_{\theta}\mathcal{L}(\theta,x)||_2}{C})$ and then updates $\theta  \rightarrow \theta - \eta \frac{1}{L}(U(X_B) + N(0,\sigma^2 C^2))$.

The current tightest privacy analysis of DP-SGD uses R\'enyi-DP (RDP)~\citep{mironov2017renyi} which implies $(\epsilon,\delta)$-DP; the merits of first working with RDP is that it provides a tighter privacy analysis for releasing the composition of multiple steps in DP-SGD -- where each step is an update computed on a different mini-batch $X_B$. An algorithm $M$ is $(\alpha,\epsilon)$-R\'enyi DP if for all neighbouring datasets $X, X'$ we have
$D_{\alpha}(M(X)|| M(X')) \leq \epsilon$
where for two probability distributions $P,Q$ we define the $\alpha$-R\'enyi divergence as $$D_{\alpha}(P||Q) \coloneqq \frac{1}{\alpha -1} \ln \mathbb{E}_{x \sim Q} (\frac{P}{Q})^{\alpha}$$


The RDP analysis follows two steps:

\begin{enumerate}[leftmargin=*,noitemsep,topsep=0pt]
    \item Per Step: Analyzing the privacy guarantee of each training step $\eta \frac{1}{L}(U(X_B) + N(0,\sigma^2 C^2))$, which is the same as $U(X_B) + N(0,\sigma^2 C^2)$ by the post-processing property of RDP (the output of a DP algorithm can be post-processed without degrading the DP guarantee provided). 
    \item Composition: Understanding the accumulated RDP guarantee of releasing all the updates. 
\end{enumerate}


The first part was analytically studied in \citet{mironov2019r} and is called the sampled Gaussian mechanism. The accumulation step follows from the composition theorem for RDP~\citep{mironov2017renyi}. In this paper, we provide new per-step and composition privacy analyses for DP-SGD that are \emph{specific} to a pair of neighbouring datasets $X,X'$.



\subsection{Motivation for Studying Per-Instance DP} 
\label{ssec:per-instance_back}



In contrast to the classical analysis of DP-SGD, we will analyze its per-instance R\'enyi DP guarantees~\citep{wang2019per} -- also known as "Individual R\'enyi DP"~\cite{feldman2021individual}. That is, the RDP guarantee specific to a \textit{given} pair of neighbouring datasets.




\begin{definition}[Per-Instance R\'enyi DP]
\label{def:per-instance-DP}
    We say an algorithm $M$ is $(\alpha, \epsilon)$ per-instance R\'enyi DP for a pair of datasets $X,X' = X \cup x^*$ if $$\max \{D_{\alpha}(M(X)||M(X')), D_{\alpha}(M(X')||M(X)) \}  \leq \epsilon$$
\end{definition}

Colloquially, when $X$ is understood from context, we will specify the per-instance guarantee by saying an algorithm is $(\alpha,\epsilon)$-R\'enyi DP for a point $x^*$ (which determines $X'$). 





Per-instance DP guarantees provide a privacy upper bound for an adversary trying to distinguish between a specific pair of datasets $X,X'$ given the ouput of $M$ on one of them, %
and not a bound for all neighbouring datasets like classical DP. However, this granularity allows for tighter analysis of each $X,X'$ case. The tighter per-instance DP bounds we derive will allow us to say that for specific pairs of datasets $X,X' = X \cup x^*$, privacy attacks against $x^*$ will fail when the adversary can only observe the outputs from $X$ or $X'$. More generally, if there are strong per-instance guarantees for all the neighbouring datasets the adversary can observe, then they will still fail. Instantiating our analysis, we will show that on common benchmark datasets, an adversary trying to distinguish if a specific point was added or not will fail for many points. 














Our work on upper bounding per-instance DP guarantees is contrasted with past work on rigorously explaining when privacy attacks against DP-SGD will perform worse than what is implied by the current (tight) data-independent analysis. One line of work has been to upper-bound the performance of specific attacks. Putting aside the limitation in only upper-bounding specific attacks, this line of work either lacks an individualized guarantee to explain the difficulty for individual points~\citep{mahloujifar2022optimal} or relies on a particular threat model to explain better privacy~\citep{thudi2022bounding}. In the case improved individual upper-bounds were achieved~\cite{guo2019certified}, this was with bounds that can fail due to assumptions. In short, this line of work lacks the generality/strength of Definition~\ref{def:per-instance-DP} in explaining why \textit{any} empirical privacy attack will perform worse in some data settings.

A more recent line of work has been to attempt to do individual (i.e., per-instance) DP accounting for DP-SGD~\citep{yu2022individual}. However, \citet{yu2022individual} could not analyze the per-instance guarantees of DP-SGD and instead relied on a weaker guarantee that holds if intermediate models were not random (which is not true for DP-SGD)%
. The main technical bottleneck to extend their approach to analyze DP-SGD, as also noted by \citet{yu2022individual}, was how to effectively analyze composition when the intermediate models are random variables. Our work provides a new composition theorem to handle this technical issue, and in doing so provides proper per-instance DP guarantees without the assumptions present in \citet{yu2022individual}.


Per-instance DP guarantees are also important beyond privacy. Memorization~\citep{feldman2020does} is a per-instance quantity (only reasoning about a particular pair $X,X'$), and hence is bounded by Definition~\ref{def:per-instance-DP}. Similarly, unlearning is a per-instance quantity, and a growing section of the literature uses per-instance DP guarantees to quantify unlearning~\citep{guo2019certified}. Hence in providing per-instance DP bounds for DP-SGD, we have also quantified a set of points that will not be memorized nor need to be unlearned (as they are already unlearnt). We refer the reader to \citet{kulynych2022you} for a more general discussion on the utility of DP inequalities in studying properties of deep learning.


