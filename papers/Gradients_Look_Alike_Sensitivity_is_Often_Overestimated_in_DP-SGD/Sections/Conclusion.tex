\section{Conclusion}

Our work can be viewed as the first existential result showing better per-instance DP guarantees for deep learning when using DP-SGD. In doing so, we provided one resolution to an open problem in the field of privacy attacks against deep learning: why many privacy attacks fail in practice. However, further work is needed to convert our analysis into a fast algorithm to do privacy accounting. Our composition theorem requires computing several training runs, and the per-step analysis of Theorem~\ref{thm:renyi_dp_sens} (which allows better analysis of the mean update rule) requires computing updates for many mini-batches at each step. Future work may be able to significantly reduce the cost associated with using these theorems, or propose alternative theorems that are more efficient to implement. Future work can also likely design algorithms that explicitly take advantage of sensitivity distributions, which we showed are implicit in DP-SGD in explaining its better per-instance privacy guarantees.



