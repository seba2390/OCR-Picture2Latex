\documentclass{article}
\usepackage[margin=1.5in]{geometry}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{multirow,makecell}
\usepackage{url}
\usepackage{graphicx}
\usepackage{authblk}
\usepackage{listings}
\usepackage{tablefootnote}
\usepackage{threeparttable}

\title{Large language models can rate news outlet credibility}
\author{Kai-Cheng Yang\thanks{yangkc@iu.edu} ~and Filippo Menczer}
\affil{Observatory on Social Media, Indiana University Bloomington, USA}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Although large language models (LLMs) have shown exceptional performance in various natural language processing tasks, they are prone to hallucinations.
State-of-the-art chatbots, such as the new Bing, attempt to mitigate this issue by gathering information directly from the internet to ground their answers.
In this setting, the capacity to distinguish trustworthy sources is critical for providing appropriate accuracy contexts to users.
Here we assess whether ChatGPT, a prominent LLM, can evaluate the credibility of news outlets.
With appropriate instructions, ChatGPT can provide ratings for a diverse set of news outlets, including those in non-English languages and satirical sources, along with contextual explanations.
Our results show that these ratings correlate with those from human experts (Spearmam's $\rho=0.54, p<0.001$).
These findings suggest that LLMs could be an affordable reference for credibility ratings in fact-checking applications.
Future LLMs should enhance their alignment with human expert judgments of source credibility to improve information accuracy.
\end{abstract}

\section{Introduction}

Large language models (LLMs), such as ChatGPT, have demonstrated outstanding capabilities in numerous natural language processing tasks, including text summarization, named entity recognition, and sentiment analysis~\cite{ye2023comprehensive,qin2023chatgpt}.
They also show great potential in assisting research by performing text annotation~\cite{gilardi2023chatgpt}, simulating survey responses from human samples~\cite{argyle2023out,brand2023using}, and aiding in paper writing and data analysis~\cite{korinek2023language}, among other tasks.

However, LLMs tend to generate content that lacks factual basis, which is often referred to as hallucination~\cite{ji2023survey}.
Given that LLMs can produce convincing statements~\cite{jakesch2023human} and even alter people's beliefs~\cite{jakesch2023co}, hallucinations can dangerously mislead users. 
This raises the concern that language models could become a new source of misinformation and disinformation, further polluting our communication channels~\cite{kreps2022news,goldstein2023generative,spitale2023ai}.

A major root of hallucination is the lack of ground truth and up-to-date data~\cite{ji2023survey}.
Due to the high cost of training and updating LLMs, these models are typically fixed after the training phase.
For example, ChatGPT-3.5 was only trained on data collected before September 2021, making it unaware of events that occurred after that date.
A solution to address this issue is to let LLMs directly retrieve information from the internet and process it in real-time~\cite{peng2023check}. 
This approach has been implemented by LLMs such as Microsoft's new Bing\footnote{blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web} and Google's Bard.\footnote{blog.google/technology/ai/bard-google-ai-search-updates}
In this setting, the ability of language models to determine the credibility of information sources is crucial for providing helpful accuracy context to users.

In this study, we assess if ChatGPT can rate the credibility of news outlets, which are major information sources on the internet.
We prompt ChatGPT to provide credibility ratings for over 7,000 news domains.
The process only takes about two hours and costs \$3.
The results suggest that, with appropriate instructions, ChatGPT can produce credibility ratings for a wide range of news outlets, including those in non-English languages and satirical sources, along with contextual explanations.
These ratings have a moderate correlation with those from human experts (Spearmam's $\rho=0.54, p<0.001$).
Furthermore, ChatGPT achieves acceptable accuracy in a binary classification scenario (AUC=0.89).

Evaluating the credibility of news sources is crucial for studying and curbing the spread of misinformation online~\cite{lazer2018science}.
With credibility ratings, researchers can track the diffusion of misinformation and estimate their prevalence online~\cite{shao_spread_2018,grinberg2019fake,guess2020exposure,yang2021covid}.
Social media platforms can reduce user exposure to content from low-credibility sources\footnote{about.fb.com/news/2019/10/update-on-election-integrity-efforts} and highlight the credibility of information sources to improve user discernment~\cite{nadarevic2020perceived,kim2019combating}.
However, expert ratings of domain credibility can be expensive and time-consuming~\cite{pennycook2019fighting}.
The findings here suggest that LLMs could be an affordable and convenient reference for credibility ratings and serve as media literacy tools to help users investigate news sources~\cite{kozyreva2022toolbox}.

\section{Results}
\subsection{ChatGPT can rate the credibility of news outlets}

We start with a comprehensive list of 11,520 news domains compiled by Lin et al. from multiple sources~\cite{lin2022high}.
We focus on a subset of 7,523 domains indexed in the Tranco list~\cite{pochat2018tranco}, which ranks the top 5.5 million websites worldwide based on their popularity.
Websites not included in the list have very limited viewership.
More details on data processing can be found in the Methods section.

We ask ChatGPT to rate the credibility of each domain.
Take \url{reuters.com} as an example; we use the following prompt:

\begin{quote}
Rate the website's credibility: reuters.com on a scale between 0 and 1, where 0 means very low credibility and 1 means very high credibility.
Assistant returns the rating -1 when the assistant has no knowledge of the website, otherwise assistant should provide the best estimation.
\end{quote}
Here, ``assistant'' refers to the ChatGPT model.
In the experiment, we add extra instructions regarding the answer format to facilitate downstream analysis.
The details can be found in the Methods section.
The prompt leads to the following answer:

\begin{quote}
Reuters is a well-known and respected news organization with a long history of producing accurate and reliable news content, so the rating will be 0.9. 
\end{quote}
When queried about \url{childrenshealthdefense.org}, ChatGPT responds:
\begin{quote}
The website has been flagged for spreading misinformation and conspiracy theories about vaccines and COVID-19, so the rating will be 0.2.
\end{quote}
We can see that ChatGPT is capable of identifying the news sources based on their website domains, assessing their credibility, and providing justifications for their ratings.
The experiment is performed in a zero-shot setting since no extra information regarding the domain of interest is provided to the language model.
Therefore, ChatGPT has to rely on the knowledge embedded within its architecture to perform the evaluation.

We use the ChatGPT API\footnote{openai.com/blog/introducing-chatgpt-and-whisper-apis} to perform the experiment programmatically.
Using five concurrent processes on a single desktop, we evaluated all 7,523 domains in approximately two hours, with a cost of around \$3.
ChatGPT successfully rated 7,282 of them but reported errors for the remaining 241 due to a lack of information regarding them.
These 241 domains are significantly less popular ($p<0.001$ according to Mann Whitney U Test) based on the Tranco ranking (see Methods).

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/chatgpt_replies.pdf}
    \caption{
    Distribution of the domain ratings generated by ChatGPT.
    A higher rating indicates higher credibility for a domain.
    }
    \label{fig:chatgpt_replies}
\end{figure}

By analyzing the answers returned by ChatGPT, we find that it can follow the instructions and yield ratings in the range between 0 and 1 with one decimal point of precision.
The rating distribution in Figure~\ref{fig:chatgpt_replies} is bimodal and indicates that ChatGPT rarely assigns scores around 0, 0.5, and 1.
These findings suggest that ChatGPT can efficiently and cost-effectively assess the reliability of many news domains.

\subsection{ChatGPT ratings correlate with human expert judgments}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/corr_heat.pdf}
    \caption{
    Joint distributions of ChatGPT ratings against (a) the aggregate ratings produced by Lin et al.~\cite{lin2022high}, (b) Media Bias/Fact Check (MBFC) ratings, and (c) NewsGuard ratings.
    For each pair-wise comparison, we report the number $n$ of sources present in both ratings, the Spearman correlation coefficient $\rho$, and the corresponding $p$-value.
    }
    \label{fig:corr_heat}
\end{figure}

Let us measure how well the ChatGPT ratings align with those of human experts.
We utilize the aggregate human ratings produced by Lin et al.~\cite{lin2022high}, which are derived from multiple sources.
Additionally, we take into account the ratings assigned by Media Bias/Fact Check (MBFC)\footnote{mediabiasfactcheck.com} and NewsGuard,\footnote{newsguardtech.com} both of which are frequently employed in research projects and industrial products. 
All ratings are rescaled to the range 0--1 for easy comparison.
Further information on these human expert ratings and the associated data processing methods can be found in the Methods section. 
Figure~\ref{fig:corr_heat} plots the joint distributions of ChatGPT ratings versus different human expert ratings.
The moderate positive correlation in all cases suggests that the judgment of ChatGPT is consistent with that of human experts.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/auc_f1.pdf}
    \caption{
    (a) The receiver operating characteristic (ROC) curves of ChatGPT's ratings when compared to the binary labels yielded by NewsGuard and MBFC.
    We also report the AUC (Area Under the Curve) values.
    (b) The F1 score of ChatGPT's ratings as a function of the threshold.
    }
    \label{fig:auc_f1}
\end{figure}

In many cases, it is useful to classify domains as either low- or high-credibility~\cite{grinberg2019fake,guess2020exposure}.
Using the NewsGuard ratings, one can follow the official recommendation and treat domains with scores lower than 60 (out of 100) as low-credibility.
Using the MBFC ratings, one may consider domains with ``very low'' or ``low'' factual reporting levels as low-credibility.
By comparing the ChatGPT ratings with these ground-truth labels, we can gauge the model's efficacy as a classifier for identifying low-credibility domains.
Since there are significantly more high-credibility domains, we use AUC (Area Under the Receiver Operating Characteristic Curve) score to quantify the performance of ChatGPT; this measure is robust to class imbalance.
The results, shown in Figure~\ref{fig:auc_f1}(a), indicate that ChatGPT achieves an AUC score of 0.89 when compared to both the NewsGuard and MBFC ratings.

What if one hopes to choose a threshold to dichotomize the ChatGPT ratings?
To identify an optimal value, we vary the threshold for ChatGPT ratings and calculate the corresponding F1 scores against the labels from NewsGuard and MBFC.
The results are shown in Figure~\ref{fig:auc_f1}(b).
The F1 score balances the trade-off between precision and recall, so an ideal threshold should maximize it.
We observe that the F1 score is highest when the threshold is in the vicinity of 0.5 in both cases.
Since ChatGPT ratings exhibit a bimodal distribution (see Figure~\ref{fig:chatgpt_replies}), choosing 0.5 as the threshold is conceptually straightforward and can maximize practical accuracy.
With this threshold, ChatGPT achieves F1 scores of 0.73 and 0.63 when compared with the NewsGuard and MBFC labels, respectively.

\subsection{Non-English and satirical domains}

\begin{table}
\centering
\caption{
Correlation between ChatGPT ratings and NewsGuard/MBFC ratings on news outlets in different languages.
From left to right, we report the outlet language, the number and percentage of sources in that language, and the Spearman correlation coefficient $\rho$.
All coefficients are significant at the $p<0.001$ level.
}
\label{table:language}
%\resizebox{\linewidth}{!}{%
\begin{tabular}{l|lrrr}
\hline
Rating & Language & Sources & \% & $\rho$  \\
\hline
\multirow{5}{5em}{NewsGuard} & English & 4,574 & 84.1 & 0.51 \\
& Italian & 306 & 5.6  & 0.38 \\
& French & 294 & 5.4 & 0.53 \\
& German & 267 & 4.9 & 0.51 \\
& Total & 5,441 & 100.0 & 0.51 \\
\hline
\multirow{3}{5em}{MBFC} & English & 2,984 & 89.4 & 0.62 \\
& Non-English & 354 & 10.6 & 0.65 \\
& Total & 3,338 & 100.0 & 0.60\\
\hline
\end{tabular}%
%}
\end{table}

A significant challenge with current domain credibility ratings is their focus on English-language outlets.
Among the sources rated by both ChatGPT and NewsGuard, 84.1\% are in English, and the figure is 89.4\% for MBFC.
More detailed statistics can be found in Table~\ref{table:language}.
This bias makes it difficult for non-English-speaking users to assess the credibility of information sources.

Given ChatGPT's multilingual capabilities, let us investigate its performance on non-English sources.
We categorize the source list based on their languages and measure the correlation between ChatGPT ratings and NewsGuard/MBFC ratings in each sub-group.
The results presented in Table~\ref{table:language} suggest that ChatGPT's performance on non-English sources is consistent with its performance on English sources.
Italian-language sources are the exception with a lower correlation.

Working with source credibility also involves handling satire websites, such as \url{theonion.com} and \url{babylonbee.com} which publish humorous articles containing non-factual and misleading content.
Many of these websites are not intended to deceive; they explicitly label their content as satire, making it challenging to handle them in research and practice.
Some researchers classify them as low-credibility~\cite{shao_spread_2018}, while some treat them separately in their analysis~\cite{grinberg2019fake}.

To test if ChatGPT can identify satirical sources, we employ the MBFC list of satire websites, 53 of which are in the set evaluated in our experiment.
We manually review the answers provided by ChatGPT and find that it recognizes the satirical nature of 41 sources  (77.4\%).
For example, ChatGPT states that ``The Onion is a well-known satirical news website'' in the response.
Among the remaining 12 satire sources, seven are identified as posting misleading or fake news by ChatGPT, two are mislabeled as highly credible, and three yield errors due to a lack of information.

\section{Discussion}

In this paper, we show that ChatGPT, a prominent LLM, can rate the credibility of various news sources in a zero-shot setting.
The ratings correlate moderately with those from human experts.
Furthermore, ChatGPT shows consistent accuracy on sources using different languages and can identify satirical websites.

Our study has some limitations.
Due to the flexibility of the prompt provided to ChatGPT, there are different ways to rate source credibility.
For instance, one could employ a binary classification approach or ask ChatGPT to compare two sources at a time, which might result in different outcomes.
However, we were unable to test all of these approaches.
Additionally, our study exclusively focuses on ChatGPT; there are currently many other LLMs available and new ones emerging soon. 
Our findings may not generalize to all models.

Despite these limitations, our findings suggest that ChatGPT has encoded a set of evaluations aligned with human experts regarding news sources.
It is not entirely clear how the language model acquires such a capacity.
Based on the justifications provided by ChatGPT alongside the ratings, our guess is that the model summarizes descriptions obtained from various sources and bases its outputs on them.
For high-credibility sources, ChatGPT typically states that they are well-established and reputable news websites.
On the other hand, ChatGPT typically claims that low-credibility sources are known for posting misleading or unverified content.
In a handful of cases, ChatGPT refers to the information from MBFC in the justifications.

Nevertheless, such a capacity may help the model ground its responses.
For instance, we asked the new Bing (backed by ChatGPT) to check the veracity of a satirical story claiming that Trump sued his grandchildren for violating NDAs he made them sign as infants.\footnote{halfwaypost.com/2021/05/05/trump-is-suing-his-grandchildren-for-violating-ndas-he-made-them-sign-as-infants}
The tool identified the source and accurately noted its satirical nature in the response.

Alignment is a critical component of LLM training to ensure the models behave as desired~\cite{ouyang2022training}. 
Future models might consider enhancing their alignment with human judgments of source credibility by explicitly integrating such information into both the training and inference phases.
Researchers could assign more weight to information from credible sources in the training data.
In the inference phase, models could be configured to give greater attention to messages from credible sources when browsing the internet for information.

The results of this study also suggest that LLMs have the potential to serve as an affordable and accessible reference for domain credibility ratings.
Such ratings are vital for researchers to study the dissemination and prevalence of misinformation online.
The low cost of LLMs makes them an appealing alternative to services with higher costs such as NewsGuard, or those with a narrower focus such as MBFC.
With the ability to provide contextual information and actively answer user questions, LLMs might be adopted by the general public as media literacy tools to investigate the credibility of news sources and perform fact-checking.
However, our results indicate that the accuracy of LLMs is far from perfect, calling for further analysis among other misinformation intervention methods~\cite{kozyreva2022toolbox}.

\section{Methods}

\subsection{Querying the ChatGPT API}

We use the \texttt{openai-python} package\footnote{github.com/openai/openai-python} to query the API endpoint.
Specifically, we choose the \texttt{gpt-3.5-turbo-0301} model, i.e., the ChatGPT-3.5 model released on March 1, 2023.
We set the temperature parameter to 0 to minimize the randomness of the output generation.
Below, we provide the complete prompt used in our experiment:

\begin{quote}
Rate the website's credibility: \{domain\} on a scale between 0 and 1, where 0 means very low credibility and 1 means very high credibility.
Assistant returns the rating -1 when the assistant has no knowledge of the website, otherwise assistant should provide the best estimation.

Assistant should only provide the result in JSON format, do not provide anything else.
Here is an example:

\{`url': `example.com', `rating': 0.5, `reason': `The example website is known to post credible content.'\}
\end{quote}
In each query, \{domain\} is replaced with the domain of interest.

ChatGPT is configured to answer questions in human-readable form by default.
So we add extra instructions to force ChatGPT to return the results in JSON (JavaScript Object Notation) format, which can be easily processed by machine.
Below we show a typical answer generated by ChatGPT:

\begin{quote}
\{`url': `foxnews.com', `rating': 0.6, `reason': `Fox News has a reputation for having a conservative bias and has been known to publish misleading or inaccurate information in the past. However, they also have a large audience and employ many experienced journalists, so some of their content can be considered credible.'\}
\end{quote}
Despite our instructions, ChatGPT responds with invalid JSON objects from time to time.
We manually code these answers to extract the relevant information.

\subsection{Human expert ratings}

In this study, we adopt human expert ratings from three sources: the aggregate list compiled by Lin et al.~\cite{lin2022high}, NewsGuard, and MBFC.

\subsubsection{Aggregate list form Lin et al.}

Lin et al.~\cite{lin2022high} analyze the news domain ratings from six sources, including NewsGuard, Ad Fontes Media,\footnote{adfontesmedia.com} Iffy index of unreliable sources,\footnote{iffy.news} MBFC, and two lists compiled by professional fact-checkers and researchers~\cite{pennycook2019fighting,lasser2022social}.
The comparison of these ratings revealed a high level of consistency.
Using an ensemble method, they generate an aggregate list that contains credibility ratings for 11,520 domains. This list is publicly accessible on  GitHub.\footnote{github.com/hauselin/domain-quality-ratings}
Our analysis starts with this list.

\subsubsection{NewsGuard}

NewsGuard is a journalistic organization that routinely assesses the reliability of news websites based on various criteria.
They assign news outlets a trust score on a scale of 0 to 100.\footnote{newsguardtech.com/ratings/rating-process-criteria}
When dichotomizing the scores, we use the threshold 60 following the official recommendation.
We rescale the scores to numbers between 0 and 1 in our analysis for easy comparison across different rating systems.
NewsGuard updates its database routinely, and for this study, we use the snapshot from January 2022.
NewsGuard ratings are available in English, French, German, and Italian.
The English outlets include those from the US, Canada, UK, and Australia.

\subsubsection{MBFC}

Media Bias/Fact Check (MBFC) is an independent organization that reviews and rates the reliability of news sources.
More information about MBFC ratings can be found on their methodology page.\footnote{mediabiasfactcheck.com/methodology}
For this study, we obtained the ratings of 4,275 domains from the MBFC website in October 2022.
We focus on the factual reporting levels of the domains, which are categorized into ``very low,'' ``low,'' ``mixed,'' ``mostly factual,'' ``high,'' and ``very high'' labels.
We respectively assign numerical values of 0, 0.2, 0.4, 0.6, 0.8, and 1.0 to these categories for our analysis.
When dichotomizing the scores, we consider domains with ``very low'' and ``low'' factual reporting levels as low-credibility sources.

MBFC only provides the country information for a subset of the domains; the language information is not available.
For simplicity, we consider all domains from the US, UK, Canada, and Australia as English outlets.
These four countries also have the most domains in the MBFC dataset.
The remaining domains are treated as non-English outlets.
In addition, MBFC maintains a list of satirical websites, which is used in our analysis.

\subsection{Website popularity ranking}

We adopt the Tranco list to measure the popularity of websites~\cite{pochat2018tranco}.
This list combines the website ranking information from multiple sources, including Alexa\footnote{alexa.com/topsites} and Cisco Umbrella.\footnote{umbrella-static.s3-us-west-1.amazonaws.com/index.html}
The Tranco list is updated on a routine basis, and for this study, we use the snapshot from September 2021.
This snapshot contains the ranks of the top 5.5 million websites worldwide.
Websites not on the list typically have limited viewership or have been deactivated.

\subsection{Data availability}

The code and data used in this study are shared in the GitHub repository (\url{github.com/osome-iu/ChatGPT_domain_rating}).
We are unable to share the NewsGuard data due to their policy.

\bibliographystyle{unsrt}
\bibliography{ref}

\end{document}
