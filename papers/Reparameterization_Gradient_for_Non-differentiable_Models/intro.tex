%auto-ignore

\section{Introduction}
Stochastic variational inference (SVI) is a popular choice for performing posterior inference in Bayesian machine learning. It picks a family of variational distributions, and formulates posterior inference as a problem of finding a member of this family that is closest to the target posterior. SVI, then, solves this optimization problem approximately using stochastic gradient ascent. One major challenge in developing an effective SVI algorithm is the difficulty of designing a low-variance estimator for the gradient of the optimization objective. Addressing this challenge has been the driver of recent advances for SVI, such as reparameterization trick~\cite{KingmaICLR14,RezendeICML14,RuizNIPS16,NaessethAISTATS17,KucukelbirJMLR2017}, clever control variate~\cite{RanganathAISTATS14,GuICLR16,GuICLR17,TuckerNIPS17,GrathwohlICLR18,MillerReparam2017}, and continuous relaxation of discrete distributions~\cite{MaddisonICLR17,JangICLR17}. 

%\hs{Double-check whether we don't want to mention REBAR, Monte-Carlo Objective, SMC-based algorithms, Stein something. They might not be directly related to variance reduction. Just tighter bound. Also, general citation on stochastic variational inference may be useful.}

Our goal is to tackle the challenge for models with non-differentiable densities. Such a model naturally arises when one starts to use both discrete and continuous random variables or specifies a model using programming constructs, such as if statement, as in probabilistic programming~\cite{GoodmanUAI08,Veture14,WoodAISTATS14,GordonICSE14}. The high variance of a gradient estimate is a more serious issue for these models than for those with differentiable densities. Key techniques for addressing it simply do not apply in the absence of differentiability. For instance, a prerequisite for the so called reparameterization trick is the differentiability of a model's density function. 

In the paper, we present a new gradient estimator for non-differentiable models. Our estimator splits the space of latent variables into regions where the joint density of the variables is differentiable, and their boundaries where the density may fail to be differentiable. For each differentiable region, the estimator applies the standard reparameterization trick and estimates the gradient restricted to the region. For each potentially non-differentiable boundary, it uses a form of manifold sampling, and computes the direction for variational parameters that, if followed, would increase the boundary's contribution to the variational objective. This manifold sampling step cannot be skipped if we want to get an unbiased estimator, and it only adds a linear overhead to the overall estimation time for a large class of non-differentiable models. The result of our gradient estimator is the sum of all the estimated values for regions and boundaries.

Our estimator generalizes the estimator based on the reparameterization trick. When a model has a differentiable density, these two estimators coincide. But even when a model's density is not differentiable and so the reparameterization estimator is not applicable, ours still applies; it continues to be an unbiased estimator, and enjoys variance reduction from reparameterization. The unbiasedness of our estimator is not trivial, and follows from an existing yet less well-known theorem on exchanging integration and differentiation under moving domain~\cite{FlandersAMM1973} and the divergence theorem. We have implemented a prototype of an SVI algorithm that uses our gradient estimator and works for models written in a simple first-order loop-free probabilistic programming language. The experiments with this prototype confirm the strength of our estimator in terms of variance reduction.

%The rest of the paper is organized as follows. In Section~\ref{sec:background}, we review stochastic variational inference and the reparameterization trick. Our estimator and its theoretical justifications are described in Section~\ref{sec:result}. The experiments with our prototype implementation are discussed in Section~\ref{sec:experiments}. The relationship with a large body of existing work and the concluding remarks appear in Sections~\ref{sec:related} and \ref{sec:conclusion}. Throughout the paper, we give examples that show why we made certain decisions about our estimator and what would go wrong if we did otherwise. Proofs of theorems and some counterexamples can be found in the supplementary materials accompanying this paper.

%\hs{Some introduction. Come back and revise. Typically SVI refers to stochastic gradient and subsampling. Be aware of this. Also, the normalizing flow paper says that the key problem in variational inference is to design a family of expressive yet tractable approximating distributions. So use careful wording when we emphasize the high-variance problem. We may decide to skip the overview part. It does not add much information.}
