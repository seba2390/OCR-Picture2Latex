%auto-ignore

\section{Variational Inference and Reparameterization Gradient}
\label{sec:background}

Before presenting our results, we review the basics of stochastic variational inference. 

Let $\bm{x}$ and $\bm{z}$ be, respectively, observed and latent variables living in $\R^m$ and $\R^n$, and $p(\bm{x},\bm{z})$ a density that specifies a probabilistic model about $\bm{x}$ and $\bm{z}$. 
We are interested in inferring information about the posterior density $p(\bm{z} | \bm{x}^0)$
for a given value $\bm{x}^{0}$ of $\bm{x}$.

Variational inference approaches this posterior-inference problem from the optimization angle. It recasts posterior inference as a problem of finding a best approximation to the posterior among a collection of pre-selected distributions $\{q_\theta(\bm{z})\}_{\tht\in \R^d}$, called \emph{variational distributions}, which all have easy-to-compute and easy-to-differentiate densities and permit efficient sampling. A standard objective for this optimization is to maximize a lower bound of
$\log p(\bm{x}^0)$ called \emph{evidence lower bound} or simply $\ELBO{}$:
\begin{equation}
\label{eqn:vi-objective}
\mathrm{argmax}_\theta \Big(\ELBO{\theta}\Big),\quad
\mbox{where}\ \, \ELBO{\theta} \defeq \E{q_{\tht}(\bm{z})}{\log \frac{p(\bm{x}^0,\bm{z})}{q_\tht(\bm{z})}}.
\end{equation}
It is equivalent to the objective of minimizing the KL divergence from $q_{\tht}(\bm{z})$ to the posterior
$p(\bm{z} | \bm{x}^0)$.

Most of recent variational-inference algorithms solve the optimization problem \eqref{eqn:vi-objective} by stochastic gradient ascent. They repeatedly estimate the gradient of $\ELBO{\tht}$ and move $\tht$ towards the direction of this estimate:
\begin{equation*}
%\label{eqn:gradient-update}
\tht \leftarrow \tht + \eta \cdot \reallywidehat{\grad{\tht}{\ELBO{\tht}}}
\end{equation*}
The success of this iterative scheme crucially depends on whether it can estimate the gradient well in terms of computation time and variance. As a result, a large part of research efforts on stochastic variational inference has been devoted to constructing low-variance gradient estimators or reducing the variance of existing estimators.

The reparameterization trick~\cite{KingmaICLR14,RezendeICML14} is the technique of choice for constructing a low-variance gradient estimator for models with differentiable densities. It can be applied in our case if the joint $p(\bm{x},\bm{z})$ is differentiable with respect to the latent variable $\bm{z}$. The trick is a two-step recipe for building a gradient estimator. First, it tells us to find a distribution $q(\bm{\eps})$ on $\R^n$ and a smooth function $f : \R^d \times \R^n \to \R^n$ such that $f_\tht(\bm{\eps})$ for $\bm{\eps} \sim q(\bm{\eps})$ has the distribution $q_\tht$. Next, the reparameterization trick suggests us to use the following estimator:
\begin{equation}
\label{eqn:reparam-gradient}
\reallywidehat{\grad{\tht}{\ELBO{\tht}}}
\;\defeq\;
	\frac{1}{N} \sum_{i=1}^N \grad{\tht}{\log \frac{r(f_\tht(\bm{\eps}^{i}))}{q_\tht(f_\tht(\bm{\eps}^{i}))}},
	\quad
	\mbox{where}\ \,
	r(\bm{z}) \defeq p(\bm{x}^0,\bm{z})\ \,
	\mbox{and}\ \,
	\bm{\eps}^{1},\ldots,\bm{\eps}^{N} \sim q(\bm{\eps}). 
\end{equation}

The reparameterization gradient in \eqref{eqn:reparam-gradient} is unbiased, and has variance significantly lower than the so called score estimator (or REINFORCE)~\cite{WilliamsMLJ1992,PaisleyICML12,WingateBBVI13,RanganathAISTATS14}, which does not exploit differentiability. But so far its use has been limited to differentiable models. We will next explain how to lift this limitation.
