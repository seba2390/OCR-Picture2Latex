%auto-ignore

\section{Reparameterization for Non-differentiable Models}
\label{sec:result}

Our main result is a new unbiased gradient estimator for a class of non-differentiable models, which can use the reparameterization trick despite the non-differentiability. 

Recall the notations from the previous section: $\bm{x} \in \R^m$ and $\bm{z}\in \R^n$ for observed and latent variables, $p(\bm{x},\bm{z})$ for their joint density, $\bm{x}^0$ for an observed value, and $q_\tht(\bm{z})$ for a variational distribution parameterized by $\tht \in \R^d$.

Our result makes two assumptions. First, the variational distribution $q_\tht(\bm{z})$ satisfies the conditions of the reparameterization gradient. Namely, $q_\tht(\bm{z})$ is continuously differentiable with respect to $\tht \in \R^d$, and is the distribution of $f_\tht(\bm{\eps})$ for a smooth function $f : \R^d \times \R^n \to \R^n$ and a random variable $\bm{\eps} \in \R^n$ distributed by $q(\bm{\eps})$. Also, the function $f_\tht$ on $\R^n$ is bijective for every $\tht \in \R^d$. Second, the joint density $r(\bm{z}) = p(\bm{x}^0,\bm{z})$ at $\bm{x} = \bm{x}^0$ has the following form:
\begin{equation}
\label{eqn:nondiff-density}
r(\bm{z}) = \sum_{k = 1}^K \indc{\bm{z} \in R_k} \cdot r_k(\bm{z})
\end{equation}
where $r_k$ is a non-negative continuously-differentiable function $\R^n \to \R$, 
$R_k$ is a (measurable) subset of $\R^n$ with measurable boundary $\partial R_k$ such that $\int_{\partial R_k} d\bm{z} = 0$, and $\{R_k\}_{1 \leq k \leq K}$ is a partition of $\R^n$.
Note that $r(\bm{z})$ is an unnormalized posterior under the observation $\bm{x} = \bm{x}^0$. The assumption indicates that the posterior $r$ may be non-differentiable at some $\bm{z}$'s, but all the non-differentiabilities occur only at the boundaries $\partial R_k$ of regions $R_k$. Also, it ensures that when considered under the usual Lebesgue measure on $\R^n$, these non-differentiable points are negligible (i.e., they are included in a null set of the measure). As we illustrate in our experiments section, models satisfying our assumption naturally arise when one starts to use both discrete and continuous random variables or specifies models using programming constructs, such as if statement, as in probabilistic programming~\cite{GoodmanUAI08,Veture14,WoodAISTATS14,GordonICSE14}.


%\hs{Should convince the reader that this kind of models are interesting. Also think about using a running example throughout the paper. Also, I don't quite know what conditions we should impose on $R_k$. At least, we should be aware of them, even if we decide not to spell them out explicitly.}

Our estimator is derived from the following theorem:
\begin{theorem}\label{thm:grad-eq-main}
Let
\[
h_k(\bm{\eps},\tht) \defeq
\log\frac{r_k(f_\tht(\bm{\eps}))}{q_\tht(f_\tht(\bm{\eps}))},
\qquad
\bm{V}(\bm{\eps},\tht) \in \R^{d \times n},
\qquad
\bm{V}(\bm{\eps},\tht)_{ij} \defeq \Bigg(\pdv{\tht_i} \Big(f^{-1}_\tht(\bm{z})\Big)\eval{\bm{z}=f_{\tht}(\bm{\eps})}\Bigg)_j.
\]
Then,
\[
\grad{\tht}{\ELBO{\tht}}
=
\underbrace{\E{q(\bm{\eps})}{\sum_{k=1}^K \indc{f_\tht(\eps) \,{\in}\, R_k} \cdot \grad{\tht}{h_k(\bm{\eps},\tht)}}}_{\ReparamGrad_\tht}
+
\underbrace{\sum_{k=1}^K \int_{f_\tht^{-1}(\partial R_{k})} \big(q(\bm{\eps}) h_k(\bm{\eps},\tht) \bm{V}(\bm{\eps},\tht)\big) \bullet d\bm{\Sigma}}_{\RegionChange_\tht}
\]
where the RHS of the plus uses the surface integral of $q(\bm{\eps}) h_k(\bm{\eps},\tht) \bm{V}(\bm{\eps},\tht)$
over the boundary $f_\tht^{-1}(\partial R_k)$ expressed in terms of $\bm{\eps}$,
the $d\bm{\Sigma}$ is the normal vector of this boundary
that is outward pointing with respect to $f_\tht^{-1}(R_k)$,
and the $\bullet$ operation denotes the matrix-vector multiplication.
\end{theorem}
The theorem says that the gradient of $\ELBO{\tht}$ comes from two sources. The first is the usual reparameterized
gradient of each $h_k$ but restricted to its region $R_k$. The second source is the sum of the surface integrals over the region boundaries $\partial R_k$. Intuitively, the surface integral for $k$ computes the direction to move $\tht$ in order to increase the contribution of the boundary $\partial R_k$ to $\ELBO{\tht}$.
%, in particular, the part of integrating $q(\bm{\eps})h_k(\bm{\eps},\tht)$. 
Note that the integrand of the surface integral has the additional $\bm{V}$ term.
This term is a by-product of rephrasing the original integration over $\bm{z}$
in terms of the reparameterization variable $\bm{\eps}$.
We write $\ReparamGrad_\tht$ for the contribution from the first source,
and $\RegionChange_\tht$ for that from the second source. The proof of the theorem uses
an existing but less known theorem about interchanging integration and differentiation under moving
domain~\cite{FlandersAMM1973}, together with the divergence theorem. It appears in the supplementary material of this paper.

At this point, some readers may feel uneasy with the $\RegionChange_\tht$ term in our theorem.
They may reason like this. Every boundary $\partial R_k$ is a measure-zero set in $\R^n$, and non-differentiabilities occur only at these $\partial R_k$'s. So, why do we need more than $\ReparamGrad_\tht$, the case-split version of the usual reparameterization? Unfortunately, this heuristic reasoning is incorrect, as indicated by the following proposition:
\begin{proposition}
There are models satisfying this section's conditions s.t. $\grad{\tht}{\ELBO{\tht}} \neq \ReparamGrad_\tht$.
\end{proposition}
\begin{proof}
  Consider the model $p(x,z) = \Dnormal(z|0,1) \big(\indc{z > 0}\Dnormal(x|5,1) + \indc{z \leq 0}\Dnormal(x|{-}2,1)\big)$ for $x \in \R$ and $z \in \R$,
  the variational distribution $q_\tht(z) = \Dnormal(z|\tht,1)$ for $\tht \in \R$,
  and its reparameterization $f_\tht(\eps) = \eps + \tht$
  and $q(\eps) = \Dnormal(\eps|0,1)$ for $\eps \in \R$.
  For an observed value $x^0 = 0$, the joint density $p(x^0,z)$ becomes
  $r(z) = \indc{z > 0}\cdot c_1 \Dnormal(z|0,1) + \indc{z \leq 0} \cdot c_2 \Dnormal(z|0,1)$,
  where $c_1 = \Dnormal(0|5,1)$ and $c_2 = \Dnormal(0|{-}2,1)$.
  Notice that $r$ is non-differentiable only at $z=0$
  and $\{0\}$ is a null set in $\R$.

  For any $\tht$,
  $\grad{\tht}\ELBO{\tht}$ is computed as follows:
  Since
  $\log({r(z)}/{q_\tht(z)}) =
  \indc{z>    0} \cdot ( \tht^2/2 - z\tht + \log c_1 ) +
  \indc{z\leq 0} \cdot ( \tht^2/2 - z\tht + \log c_2 )$,
  we have\footnote{The error function $\erf$ is defined by
  $\erf(x) = 2 \int_0^x \exp(-t^2)\,dt / \sqrt{\pi}$.}
  $\ELBO{\tht}
  = \frac{1}{2}[-\tht^2
  + \erf({\tht}/{\sqrt{2}})\log({c_1/c_2})
  + \log(c_1c_2)]$
  and thus obtain
  $\grad{\tht} \ELBO{\tht}
  = -\tht + \log({c_1}/{c_2}) \exp(-{\tht^2}/{2}) /\sqrt{2\pi}$.

  On the other hand, $\ReparamGrad_\tht$ is computed as follows:
  After reparameterizing $z$ into $\eps$, we have
  $\log\big({r(f_\tht(\eps))} / {q_\tht(f_\tht(\eps))}\big) =
  \indc{\eps+\tht>    0} \cdot ( -\tht^2/2 - \eps\tht + \log c_1 ) +
  \indc{\eps+\tht\leq 0} \cdot ( -\tht^2/2 - \eps\tht + \log c_2 )$,
  so the term inside the expectation of $\ReparamGrad_\tht$ is
  $ \indc{\eps+\tht>    0} \cdot ( -\tht - \eps ) +
  \indc{\eps+\tht\leq 0} \cdot ( -\tht - \eps )$
  and we obtain $\ReparamGrad_\tht = -\tht$.

  Note that $\grad{\tht}\ELBO{\tht} \neq \ReparamGrad_\tht$ for any $\tht$.
  The difference between the two quantities is $\RegionChange_\tht$ in Theorem~\ref{thm:grad-eq-main}.
  The main culprit here is that
  interchanging differentiation and integration is sometimes invalid:
  for $D_1, D_2(\tht) \subset \R^n$ and $\alpha_1, \alpha_2 : \R^n \times \R^d \to \R$,
  the below equations {\em do not} hold in general
  if $\alpha_1$ is not differentiable in $\tht$,
  and if $D_2(\cdot)$ is not constant (even when $\alpha_2$ is differentiable in $\tht$).
  \[ 
  \grad{\tht}\int_{D_1} \alpha_1(\bm{\eps},\tht)d\bm{\eps} =
  \int_{D_1}\grad{\tht} \alpha_1(\bm{\eps},\tht)d\bm{\eps}
  \quad\tand\quad
  \grad{\tht}\int_{D_2(\tht)} \alpha_2(\bm{\eps},\tht)d\bm{\eps} =
  \int_{D_2(\tht)}\grad{\tht} \alpha_2(\bm{\eps},\tht)d\bm{\eps}.
  \]
\end{proof}

The $\ReparamGrad_\tht$ term in Theorem~\ref{thm:grad-eq-main} can be easily estimated by the standard
Monte Carlo:
\begin{equation*}
%\label{eqn:reparam-estimator}
\ReparamGrad_\tht \;\approx\;
\frac{1}{N} \sum_{i=1}^N\Bigg(\sum_{k=1}^K \indc{f_\tht(\bm{\eps}^i) \,{\in}\, R_k} \cdot \grad{\tht}{h_k(\bm{\eps}^i,\tht)}\Bigg)
\quad
\mbox{for i.i.d.\ $\bm{\eps}^1,\ldots,\bm{\eps}^N \sim q(\bm{\eps})$}. 
\end{equation*}
We write $\reallywidehat{\ReparamGrad_\tht}$ for this estimate.

However, estimating the other $\RegionChange_\tht$ term is not that easy, because of the difficulties in estimating surface integrals in the term. In general, to approximate a surface integral well, we need a parameterization of the surface, and a scheme for generating samples from it~\cite{DiaconisMainfold13}; this general methodology and a known theorem related to our case are reviewed in the supplementary material. 

In this paper, we focus on a class of models that use relatively simple (reparameterized) boundaries $f^{-1}_\tht(\partial R_k)$ and permit, as a result, an efficient method for estimating surface integrals in $\RegionChange_\tht$. 

A good way to understand our simple-boundary condition is to start with something even simpler, namely the condition that $f^{-1}_\tht(\partial R_k)$ is an $(n{-}1)$-dimensional hyperplane $\{\bm{\eps} \,\mid\, \bm{a} \cdot \bm{\eps} = c\}$.
Here the operation $\cdot$ denotes the dot-product.
A surface integral over such a hyperplane can be estimated using  % estimated easily
the following theorem:
\begin{theorem}\label{thm:est-surface-integral}
Let $q(\bm{\eps}) = \prod_{i=1}^n q(\bm{\eps}_i)$ and $S$ a measurable subset of $\R^n$.
Assume that $S = \{\bm{\eps} \,\mid\, \bm{a} \cdot \bm{\eps} > c\}$ or 
$S = \{\bm{\eps} \,\mid\, \bm{a} \cdot \bm{\eps} \geq c\}$ for some $\bm{a} \in \R^n$ and $c \in \R$,
and that $\bm{a}_j \neq 0$ for some $j$. Then,
\[
\int_{\partial S} \big(q(\bm{\eps}) \bm{F}(\bm{\eps})\big) \bullet d\bm{\Sigma}
\;= \;
\E{q(\bm{\zeta})}{\bm{G}(g(\bm{\zeta})) \bullet \bm{n}}
\quad\mbox{for all measurable $\bm{F} : \R^n \to \R^{d\times n}$}. 
%% \sqrt{\sum_{i\neq j} \Big(\frac{\bm{a}_i}{\bm{a}_j}\Big)^2}
%% \cdot \prod_{i\neq j} \bm{a}_i
%% \cdot \E{q(\bm{\zeta})}{\bm{W}(g(\bm{\zeta})) \bullet \bm{n}}
\]
Here $d\bm{\Sigma}$ is the normal vector pointing outward with respect to
$S$,
%% $\{\bm{\eps} \,\mid\, \bm{a} \cdot \bm{\eps} > c\}$,
$\bm{\zeta}$ ranges over $\R^{n-1}$, its density $q(\bm{\zeta})$ is the product
of the densities for its components, and this component density $q(\bm{\zeta}_i)$
is the same as the density $q(\bm{\eps}_{i'})$ for the $i'$-th component of $\bm{\eps}$,
where $i'=i + \indc{i \geq j}$.
Also,
\begin{align*}
\bm{G}(\bm{\eps}) & \defeq
q(\bm{\eps}_j) \bm{F}(\bm{\eps}),
&
g(\bm{\zeta}) & \defeq
\Big(\bm{\zeta}_1,\ldots,\bm{\zeta}_{j-1},
\frac{1}{\bm{a}_j}(c-\bm{a}_{-j}\cdot\bm{\zeta}),
%% \Big(\sum_{i=1}^{j-1} \frac{\bm{a}_i\bm{\zeta}_i}{\bm{a}_j}+ \sum_{i=j}^{n-1} \frac{\bm{a}_{i+1}\bm{\zeta}_i}{\bm{a}_j}  - \frac{c}{\bm{a}_j}\Big),
\bm{\zeta}_{j},\ldots,\bm{\zeta}_{n-1}\Big)^\intercal,
\\
\bm{a}_{-j} & \defeq (\bm{a}_1, \ldots, \bm{a}_{j-1}, \bm{a}_{j+1}, \ldots, \bm{a}_n),
&
\bm{n} & \defeq
\sgn{-\bm{a}_j} \Big(\frac{\bm{a}_1}{\bm{a}_j},\ldots,\frac{\bm{a}_{j-1}}{\bm{a}_j},1,\frac{\bm{a}_{j+1}}{\bm{a}_j},\ldots,\frac{\bm{a}_n}{\bm{a}_j}\Big)^\intercal.
%% \Big(-\frac{\bm{a}_i}{\bm{a}_j} \Big)_{1 \leq i \leq n}^\intercal,
%% \Big(\frac{\bm{a}_i}{\bm{a}_j},\ldots,\frac{\bm{a}_{j-1}}{\bm{a}_j},-1,\frac{\bm{a}_{j+1}}{\bm{a}_j},\ldots,\frac{\bm{a}_n}{\bm{a}_j}\Big)^\intercal
%% \Bigg/ \sqrt{\sum_{i\neq j} \Big(\frac{\bm{a}_1}{\bm{a}_j}\Big)^2},
\end{align*}
\end{theorem}
The theorem says that if the boundary $\partial S$ is an $(n{-}1)$-dimensional hyperplane
$\{\bm{\eps} \,\mid\, \bm{a} \cdot \bm{\eps} = c\}$,
we can parameterize the surface by a linear map $g : \R^{n-1} \to \R^n$ and express
the surface integral as an expectation over $q(\bm{\zeta})$. This distribution
for $\bm{\zeta}$ is the marginalization of $q(\bm{\eps})$ over the $j$-th component. Inside the
expectation, we have the product of the matrix $\bm{G}$ and the vector
$\bm{n}$. The matrix comes from the integrand of the surface integral, and
the vector is the direction of the surface. Note that $\bm{G}(\bm{\eps})$ has $q(\bm{\eps}_j)$ instead
of $q(\bm{\eps})$; the missing part of $q(\bm{\eps})$ has been converted to the distribution $q(\bm{\zeta})$.

When every  $f^{-1}_\tht(\partial R_k)$ is an $(n{-}1)$-dimensional hyperplane $\{\bm{\eps} \,\mid\, \bm{a} \cdot \bm{\eps} = c\}$ for $\bm{a} \in \R^n$ and $c \in \R$ with $\bm{a}_{j_k} \neq 0$, we can use Theorem~\ref{thm:est-surface-integral} and estimate the surface integrals in $\RegionChange_\tht$ as follows:
\[
\int_{f_\tht^{-1}(\partial R_{k})} \big(q(\bm{\eps}) h_k(\bm{\eps},\tht) \bm{V}(\bm{\eps},\tht)\big) \bullet d\bm{\Sigma}
\;\approx\;
%% \sqrt{\sum_{i\neq j} \Big(\frac{\bm{a}_i}{\bm{a}_j}\Big)^2}
%% \cdot \prod_{i\neq j} \bm{a}_i
%% \cdot
\frac{1}{M}\sum_{i=1}^M \bm{W}(g(\bm{\zeta}^i)) \bullet \bm{n}
\quad
\mbox{for i.i.d.\ $\bm{\zeta}^1, \ldots, \bm{\zeta}^ M \sim q(\bm{\zeta})$},
\]
where $\bm{W}(\bm{\eps}) = q(\bm{\eps}_{j_k})h_k(\bm{\eps},\tht)\bm{V}(\bm{\eps},\tht)$.
Let $\reallywidehat{\RegionChange_{(\tht,k)}}$ be this estimate. Then, our estimator
for the gradient of $\ELBO{\tht}$ in this case computes:
\[
\reallywidehat{\grad{\tht}\ELBO{\tht}} \;\defeq\;
\reallywidehat{\ReparamGrad_\tht}
+
\sum_{k=1}^K \reallywidehat{\RegionChange_{(\tht,k)}}.
\]
The estimator is unbiased because of Theorems~\ref{thm:grad-eq-main} and \ref{thm:est-surface-integral}:
\begin{corollary}
$\E{}{\reallywidehat{\grad{\tht}\ELBO{\tht}}} = \grad{\tht}\ELBO{\tht}$.
\end{corollary}

We now relax the condition that each boundary is a hyperplane, and consider a more liberal \emph{simple-boundary condition}, which is often satisfied by non-differentiable models from a first-order loop-free probabilistic programming language. This new condition and the estimator under this condition are 
what we have used in our implementation. The relaxed condition is that
%% Consider the case that
the regions $\{f_\tht^{-1}(R_k)\}_{1 \leq k \leq K}$ are obtained
by partitioning $\R^n$ with $L$ $(n{-}1)$-dimensional hyperplanes. That is, there are 
affine maps $\Phi_1,\ldots,\Phi_L : \R^n \to \R$ such that for all $1 \leq k \leq K$,
\[
	f_\tht^{-1}(R_k) = \bigcap_{l=1}^L S_{l, (\sigma_k)_l}
\qquad \mbox{for some $\sigma_k \in \{-1,1\}^L$}
\]
where
$S_{l,1} = \{\bm{\eps} \,\mid\, \Phi_l (\bm{\eps}) > 0 \}$ and
$S_{l,-1} = \{\bm{\eps} \,\mid\, \Phi_l (\bm{\eps}) \leq 0 \}$.
%%\{\bm{\eps} \,\mid\,  \sigma^k_l=1 \Rightarrow \Phi_l (\bm{\eps}) > 0 \}
%% f_\tht^{-1}(R_k) = \{\bm \eps: [\sigma_k]_j \Phi_j(\bm \eps) > 0 \text{ for all }1 \leq j \leq  m\}
Each affine map $\Phi_l$ defines an $(n{-}1)$-dimensional hyperplane $\partial S_{l,1}$,
and $(\sigma_k)_l$ specifies on which side the region $f_\tht^{-1}(R_k)$ lies
with respect to the hyperplane $\partial S_{l,1}$.
Every probabilistic model written in a first-order probabilistic programming language
satisfies the relaxed condition,
if the model does not contain a loop and uses only a fixed finite number of random
variables and the branch condition of each if statement in the model
is linear in the latent variable $\bm{z}$;
in such a case, $L$ is the number of if statements in the model.

Under the new condition, how can we estimate $\RegionChange_\tht$?
A naive approach %% to estimate $\RegionChange_\tht$ in this case
is to estimate the $k$-th surface integral for each $k$ (in some way) and sum them up.
However, with $L$ hyperplanes, the number $K$ of regions can grow as fast as $\bigO{L^n}$,
implying that the naive approach is slow. Even worse the boundaries 
$f_\tht^{-1}(\partial R_k)$ do not satisfy the condition of Theorem~\ref{thm:est-surface-integral},
and just estimating the surface integral over each $f_\tht^{-1}(\partial R_k)$ 
may be difficult.
%% ref: https://mathoverflow.net/a/38287

A solution is to transform the original formulation of $\RegionChange_\tht$ such
that it can be expressed as the sum of surface integrals over $\partial S_{l,1}$'s.
The transformation is based on the following derivation:
\begin{align}
  \RegionChange_\tht
  &=\sum_{k=1}^K
  \int_{f_\tht^{-1}(\partial R_{k})}
  \big(q(\bm{\eps}) h_k(\bm{\eps},\tht) \bm{V}(\bm{\eps},\tht)\big) \bullet d\bm{\Sigma} \nonumber \\
  &=
  \sum_{l=1}^L \int_{\partial S_{l,1}} %%{\{\bm{\eps} \,\mid\, \Phi_l (\bm{\eps})=0\}}
  \Big( q(\bm{\eps}) \bm{V}(\bm{\eps},\tht)
  \sum_{k=1}^K \indc{ \bm \eps \in \bar{ f_\tht^{-1}(R_k)}} (\sigma_k)_l h_k(\bm{\eps},\tht) 
  \Big) \bullet d\bm{\Sigma} \label{eqn:surface-clever}
\end{align}
where $\bar{T}$ denotes the closure of $T \subset \R^n$,
and $d\bm{\Sigma}$ in \eqref{eqn:surface-clever} is the normal vector pointing
outward with respect to $S_{l,1}$.
Since $\{f_\tht^{-1}(R_k)\}_k$ are obtained
by partitioning $\R^n$ with $\{\partial S_{l,1}\}_l$,
we can rearrange the sum of $K$ surface integrals over complicated boundaries
$f_\tht^{-1}(\partial R_k)$,
into the sum of $L$ surface integrals over the hyperplanes $\partial S_{l,1}$ as above.
Although the expression inside the summation over $k$ in \eqref{eqn:surface-clever} looks complicated,
for almost all $\bm{\eps}$,
the indicator function is nonzero for exactly two $k$'s:
$k_1$ with $(\sigma_{k_1})_l=1$  and $k_{-1}$ with $(\sigma_{k_{-1}})_l=-1$.
So, we can efficiently estimate the $l$-th surface integral in \eqref{eqn:surface-clever}
using Theorem~\ref{thm:est-surface-integral},
and call this estimate $\reallywidehat{\RegionChange_{(\tht,l)}}{}'$.
%% We use Monte Carlo samples linear in $L$ (not $K$) to estimate $\RegionChange_\tht$.
%% \begin{corollary}
%% The gradient estimator derived from \eqref{eqn:reparam-estimator} and \eqref{eqn:surface-clever}
%% is unbiased.
%% \end{corollary}
Then, our estimator for the gradient of $\ELBO{\tht}$ in this more general case computes:
\begin{equation}
  \label{eqn:our-estimator}
  \reallywidehat{\grad{\tht}\ELBO{\tht}}{}' \;\defeq\;
  \reallywidehat{\ReparamGrad_\tht}
  +
  \sum_{l=1}^L \reallywidehat{\RegionChange_{(\tht,l)}}{}'.
\end{equation}
The estimator is unbiased because of
Theorems~\ref{thm:grad-eq-main} and \ref{thm:est-surface-integral} and
Equation~\ref{eqn:surface-clever}:
\begin{corollary}
$\E{}{\reallywidehat{\grad{\tht}\ELBO{\tht}}{}'} = \grad{\tht}\ELBO{\tht}$.
\end{corollary}
