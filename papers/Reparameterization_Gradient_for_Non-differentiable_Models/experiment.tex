%auto-ignore

\section{Experimental Evaluation}
\label{sec:experiments}

We experimentally compare our gradient estimator ($\ours$) to 
the score estimator ($\score$),
an unbiased gradient estimator that is applicable to non-differentiable models,
and the reparameterization estimator ($\repar$),
a biased gradient estimator that computes only $\reallywidehat{\ReparamGrad_\tht}$
(discussed in Section~\ref{sec:result}). $\repar$
is biased in our experiments because it is applied to non-differentiable models.


We implemented a black-box variational inference engine
that accepts a probabilistic model written in
a simple probabilistic programming language
(which supports basic constructs such as {\tt sample}, {\tt observe}, and {\tt if} statements)
and performs variational inference using one of the three aforementioned gradient estimators.
Our implementation\footnote{
  Code is available at \url{https://github.com/wonyeol/reparam-nondiff}.
}
is written in Python
and uses {\tt autograd}~\cite{Maclaurin-thesis},
an automatic differentiation package for Python,
to automatically compute the gradient term in $\reallywidehat{\ReparamGrad_\tht}$ 
for an arbitrary probabilistic model.

\paragraph{Benchmarks.}
We evaluate our estimator on three models for small sequential data:
%one of which uses data generated synthetically:
\begin{itemize}
\item $\tcl$~\cite{SoudjaniQUEST17} models the random dynamics of a controller that attempts to keep the temperature of a room within specified bounds. The controller's state has a continuous part for the room temperature and a discrete part that records the on or off of an air conditioner. At each time step, the value of this discrete part decides which of two different random state updates is employed, and incurs the non-differentiability of the model's density. We use a synthetically-generated sequence of $21$ noisy measurements of temperatures, and perform posterior inference on the sequence of the controller's states given these noisy measurements.
  This model consists of a 41-dimensional latent variable and 80 if statements.
\item $\sns$~\cite{PilonBook} is a model for the numbers of per-day SNS messages over the period of 74 days (skipping every other day). It allows the SNS-usage pattern to change over the period, and this change causes non-differentiability. Finding the posterior distribution over this change is the goal of the inference problem in this case. We use the data from \cite{PilonBook}.
  This model consists of a 3-dimensional latent variable and 37 if statements.
\item $\infl$~\cite{ShumwayBook} is a model for the US influenza mortality data in 1969. The mortality rate in each month depends on whether the dominant influenza virus is of type $1$ or $2$, and finding this type information from a sequence of observed mortality rates is the goal of the inference. The virus type is the cause of non-differentiability in this example.
  This model consists of a 37-dimensional latent variable and 24 if statements.
\end{itemize}

\paragraph{Experimental setup.}
We optimize the $\ELBO{}$ objective using Adam~\cite{KingmaICLR15}
with two stepsizes: 0.001 and 0.01.
We run Adam for 10000 iterations and at each iteration,
we compute each estimator using $N \in \{1, 8, 16\}$ Monte Carlo samples.
For $\ours$,
we use a single subsample $l$ (drawn uniformly at random from $\{1,\cdots,L\}$)
to estimate the summation in~\eqref{eqn:our-estimator},
and use $N$ Monte Carlo samples to compute 
$\reallywidehat{\RegionChange_{(\tht,l)}}{}'$.
%% we use $N=M$, i.e., we use the same number of Monte Carlo samples
%% to compute $\reallywidehat{\ReparamGrad_{\tht}}$ in \eqref{eqn:reparam-estimator}
%% and to compute $\reallywidehat{\RegionChange_{(\tht,l)}}$ for $1\leq l \leq L$ derived from
%% \eqref{eqn:surface-clever}.
%
While maximizing $\ELBO{}$, we measure two things:
the variance of estimated gradients of $\ELBO{}$, and $\ELBO{}$ itself.
Since each gradient is not scalar, we measure two kinds of variance of the gradient,
as in \cite{MillerReparam2017}:
$\varcmp$, the average variance of each of its components,
and $\varnrm$, the variance of its $l^2$-norm.
To estimate the variances and the $\ELBO{}$ objective,
we use 16 and 1000 Monte Carlo samples, respectively.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% tables & graphs
\newcommand{\FIGSCALE}{.39}

{\def\arraystretch{1.4}
\begin{table}[t]
  \begin{subtable}{\textwidth}
    \center
    \begin{tabular}{cc|ccc}
      \hline
      Estimator & Type of Variance & $\tcl$ & $\sns$ & $\infl$ \\
      \hline
      $\repar$
      & $\varcmp$ & $\bm{4.45 \times 10^{-9}}$ & $2.91 \times 10^{-2}$ & $\bm{4.38 \times 10^{-3}}$ \\
      & $\varnrm$ & $\bm{2.45 \times 10^{-8}}$ & $2.92 \times 10^{-2}$ & $\bm{2.12 \times 10^{-3}}$ \\
      \hline
      $\ours$
      & $\varcmp$ & $1.85 \times 10^{-6}$ & $\bm{2.77 \times 10^{-2}}$ & $4.89 \times 10^{-3}$ \\
      & $\varnrm$ & $7.59 \times 10^{-5}$ & $\bm{2.46 \times 10^{-2}}$ & $2.36 \times 10^{-3}$ \\ \hline
    \end{tabular}
    \caption{$\text{stepsize}=0.001$}
  \end{subtable}

  \begin{subtable}{\textwidth}
    \center
    \begin{tabular}{cc|ccc}
      \hline
      Estimator & Type of Variance & $\tcl$ & $\sns$ & $\infl$ \\
      \hline
      $\repar$
      & $\varcmp$ & $3.88 \times 10^{-11}$ & $\bm{5.03 \times 10^{-4}}$ & $\bm{2.46 \times 10^{-3}}$ \\
      & $\varnrm$ & $\bm{6.11 \times 10^{-11}}$ & $1.02 \times 10^{-3}$ & $\bm{1.26 \times 10^{-3}}$ \\
      \hline
      $\ours$
      & $\varcmp$ & $\bm{1.24 \times 10^{-11}}$ & $5.07 \times 10^{-4}$ & $2.80 \times 10^{-3}$ \\
      & $\varnrm$ & $8.05 \times 10^{-11}$ & $\bm{8.12 \times 10^{-4}}$ & $1.40 \times 10^{-3}$ \\ \hline
    \end{tabular}
    \caption{$\text{stepsize}=0.01$}
  \end{subtable}

  %\vspace{5pt}
  \caption{
    Ratio of $\{\repar,\ours\}$'s average variance to $\score$'s 
    for $N=1$. % $\text{stepsize}=0.01$ and
    The values for $\score$ are all $1$, so omitted.
    The optimization trajectories used to compute the above variances
    are shown in Figure~\ref{fig:elbo}. % (b,d,f).
  }
  \label{tab:variance}
\end{table}
}

\begin{figure}[t]
  \center
  \begin{subfigure}{\FIGSCALE\textwidth}
    \includegraphics[width=\textwidth]{{fig/bm-tcl.py_iter=10000_lr=0.001_sample=1,8,16_samplevar=0_sample=1000_step=100_pretty}.pdf}
    \caption{$\tcl$ ($\text{stepsize}=0.001$)}
    \label{fig:tcl-0.001}
  \end{subfigure}
  \begin{subfigure}{\FIGSCALE\textwidth}
    \includegraphics[width=\textwidth]{{fig/bm-tcl.py_iter=10000_lr=0.01_sample=1,8,16_samplevar=0_sample=1000_step=100_pretty}.pdf}
    \caption{$\tcl$ ($\text{stepsize}=0.01$)}
    \label{fig:tcl-0.01}
  \end{subfigure}
  \\
  \begin{subfigure}{\FIGSCALE\textwidth}
    \includegraphics[width=\textwidth]{{fig/bm-sns.py_iter=10000_lr=0.001_sample=1,8,16_samplevar=0_sample=1000_step=100_pretty}.pdf}
    \caption{$\sns$ ($\text{stepsize}=0.001$)}
    \label{fig:sns-0.001}
  \end{subfigure}
  \begin{subfigure}{\FIGSCALE\textwidth}
    \includegraphics[width=\textwidth]{{fig/bm-sns.py_iter=10000_lr=0.01_sample=1,8,16_samplevar=0_sample=1000_step=100_pretty}.pdf}
    \caption{$\sns$ ($\text{stepsize}=0.01$)}
    \label{fig:sns-0.01}
  \end{subfigure}
  \\
  \begin{subfigure}{\FIGSCALE\textwidth}
    \includegraphics[width=\textwidth]{{fig/bm-time.py_iter=10000_lr=0.001_sample=1,8,16_samplevar=0_sample=1000_step=100_pretty}.pdf}
    \caption{$\infl$ ($\text{stepsize}=0.001$)}
    \label{fig:time-0.001}
  \end{subfigure}
  \begin{subfigure}{\FIGSCALE\textwidth}
    \includegraphics[width=\textwidth]{{fig/bm-time.py_iter=10000_lr=0.01_sample=1,8,16_samplevar=0_sample=1000_step=100_pretty}.pdf}
    \caption{$\infl$ ($\text{stepsize}=0.01$)}
    \label{fig:time-0.01}
  \end{subfigure}
  \caption{
    The $\ELBO{}$ objective as a function of the iteration number.
    \{dotted, dashed, solid\} lines represent $\{N=1, N=8, N=16\}$.
  }
  \label{fig:elbo}
\end{figure}

{\def\arraystretch{1.4}
\begin{table}[t]
  \center
  \begin{tabular}{c|ccc}
    \hline
    Estimator & $\tcl$ & $\sns$ & $\infl$ \\ \hline
    $\score$ & $21.7$ & $4.9$ & $18.7$ \\
    $\repar$ & $46.1$ & $15.4$ & $251.4$ \\
    $\ours$  & $79.2$ & $24.9$ & $269.8$ \\ \hline
  \end{tabular}
  \vspace{5pt}
  \caption{Computation time (in ms) per iteration for $N=1$.}
  \label{tab:time}
\end{table}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\paragraph{Results.}
Table~\ref{tab:variance} compares the average variance of each estimator
for $N=1$, % $\text{stepsize}=0.01$
where the average is taken over a single optimization trajectory.
The table clearly shows that during the optimization process,
$\ours$ has several orders of magnitude (sometimes $<10^{-10}$ times) smaller variances
than $\score$.
Since $\ours$ computes additional terms when compared with $\repar$,
we expect that $\ours$ would have larger variances than $\repar$,
and this is confirmed by the table.
It is noteworthy, however, that for most benchmarks,
the averaged variances of $\ours$ are very close to those of $\repar$. % and sometimes smaller than
This suggests that the additional term $\RegionChange_\tht$ in our estimator
often introduces much smaller variances than the reparameterization term $\ReparamGrad_\tht$.
%
%% Some might think that such a result is counter-intuitive
%% because $\ours$ computes additional terms, compared with $\repar$,
%% using several more Monte Carlo samples.
%% We observe that
%% the variance of $\repar$ (and the $\ReparamGrad_\tht$ term in $\ours$)
%% is proportional to $-\ELBO{}$,
%% and the variance of the $\RegionChange_\tht$ term in $\ours$ is often much smaller
%% than that of $\ReparamGrad_\tht$.
%% For instance, for $\sns$ with $\text{stepsize}=0.01$ and $N=1$,
%% $\varcmp$ and $\varnrm$ of $\RegionChange_\tht$
%% are 16 and 5 times smaller on average than those of $\ReparamGrad_\tht$, respectively.
%% Since $\ours$ follows a trajectory mostly of a higher $\ELBO{}$ than $\repar$
%% (see Figure~\ref{fig:elbo}),
%% the averaged variances of $\ours$ can be smaller than those of $\repar$
%% by the above observations.

Figure~\ref{fig:elbo} shows the $\ELBO{}$ objective,
for different estimators with different $N$'s,
as a function of the iteration number. As expected, 
using a larger $N$ makes all estimators converge faster in a more stable manner. 
In all three benchmarks, $\ours$ outperforms (or performs similarly to) the other two and converges stably,
and $\repar$ beats $\score$. Increasing the stepsize to $0.01$ makes
$\score$ unstable in $\tcl$ and $\sns$. It is also worth noting that
$\repar$ converges to sub-optimal values in $\tcl$ (possibly because $\repar$ is biased). 

Table~\ref{tab:time} shows the computation time per iteration of each approach for $N=1$.
% to complete a single optimization trajectory for $N=1$.
% All of our experiments are run with a single thread.
Our implementation performs the worst in this wall-time comparison,
but the gap between $\ours$ and $\repar$ is not huge:
the computation time of $\ours$ is less than 1.72 times that of $\repar$ in all benchmarks.
Furthermore, we want to point out that our implementation is an early unoptimized prototype, and there are several rooms to improve in the implementation.
%% For instance, the implementation may subsample summands in \eqref{eqn:surface-clever} and avoid computing all $L$ terms when estimating the $\RegionChange_\tht$ term.
For instance, it currently constructs Python functions dynamically, and computes the gradients of these functions using {\tt autograd}. But this dynamic approach is costly because {\tt autograd} is not optimized for such dynamically constructed functions; this can also be observed in the bad performance of $\repar$, particularly in $\infl$, that employs the same strategy of dynamically constructing functions
and taking their gradients.
So one possible optimization is to avoid this gradient computation of dynamically
constructed functions by building the functions statically during compilation.
