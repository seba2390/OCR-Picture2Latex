\section{Related Work}
\label{related}

While we have employed fundamental algorithms for BFS and SSSP, various optimized algorithms and implementations for these and other graph applications have been developed on a variety of architectures, including distributed and shared-memory supecomputers, and multi-core machines~\cite{madduri-fasterparallelbc-ipdps2009, kulkarni-lonestar-ispass2009, agarwal-scalablegraphexploration-sc2010, buluc-parallelBFS-sc2011, ediger-graphct-tpds2013,yoo05, bader06a, kulkarni07}. BFS has received significant attention over the years~\cite{Luo:2010:EGI:1837274.1837289,hongBFS,merrill-scalablegputraversal-ppopp2012,gharaibeh12}. The work by Merrill et al.~\cite{merrill-scalablegputraversal-ppopp2012} has developed a work-efficient queue-based algorithm for BFS. $\Delta-$stepping or the derivations of it \cite{meyer-stepping-ja2003, venkatesan-scalablesssp-ipdps2014} are commonly used for SSSP.  Harish and Narayanan~\cite{harish07} describe CUDA implementations of graph algorithms such as BFS and single-source shortest paths computation.  Vineet et al.~\cite{vineet09} and Nobari et al.~\cite{nobari12} propose computing the minimum spanning tree and forest. The primary objective of our work is to propose and evaluate load balancing strategies within a common framework. While we have used the LoneStar-GPU framework and algorithms, our strategies are equally applicable to the above mentioned optimized algorithms as well.

The work by Merrill et al.~\cite{merrill-scalablegputraversal-ppopp2012} has implemented BFS traversal on GPUs using prefix sum computations. The work introduces techniques for local duplication detection to avoid race condition, gathering neighbors for edges, concurrent discovery of duplicates, and strategies for graph contraction and expansion.
Nasre et al. have implemented topology and data-driven versions of several graph applications and have quantitatively compared the two versions~\cite{nasre-datavstoplogy-ipdps2013}. In the topology-driven algorithms, GPU threads are spawned for all nodes in a graph, while in the data-driven algorithms, worklists are built dynamically and threads are spawned corresponding to only active elements/nodes in a time step. In another work, Nasre et al. have developed execution strategies to address challenges related to {\em morph graphs} in which the structure of the graph changes during execution~\cite{nasre-morphgpus-ppopp2013}. They propose optimizations for concurrent node addition, deletion and refinement including reorganizing memory layout, conflict resolution, adaptive parallelism and reducing warp divergence.
The work by Gharaibeh et al.~\cite{gharaibeh-graphsgpus-ipdps2013}  proposed hybrid executions of graph applications utilizing both CPU and GPU cores. The work devises and compares different strategies for partitioning the graph nodes among the CPUs and GPUs.

All these efforts assign the GPU threads to the nodes of the graph, thus performing node-based parallelism. None of these strategies addresses the resulting load imbalance due to node-level parallelism. Sariy{\"u}ce et al.~\cite{sariyuce-bc-gpgpu2013} evaluate both node-based and edge-based parallelism for the betweenness centrality problem. They identify the load imbalance in the node-based parallelism and show that the edge-based parallelism results in good load balance. They also proposed the concept of virtual nodes in which duplicate nodes are created for the actual nodes with high out-degrees. One of our strategies, namely, {\em the node splitting} approach, is similar to the virtual nodes concept. However, in our method, the node-splitting level or the number of virtual nodes is determined automatically using a novel heuristic. Our work is also more comprehensive since it considers multiple load balancing strategies and multiple graph applications. Our work also proposes a novel hierarchical processing method for load balancing. While for the betweenness centrality problem, the authors show that the virtual node strategy performs uniformly better than the edge-based parallelism, we show cases in which the edge-based parallelism gives the best results and analyze the reasons.
We also show that different application scenarios demand different strategies and there is no \textit{one-size-fits-all} solution.
