\section{Experiments and Results}
\label{exp_res}

To assess the effectiveness of our proposed techniques, we embed them in the implementation of two graph algorithms: breadth-first search computation (BFS) and single-source shortest paths computation (SSSP).
Both these algorithms are fundamental to several domains and form the building block for several interesting applications.
We compare our implementations against the LonestarGPU benchmark implementations~\cite{lonestargpu-web},
which use a node-based task-distribution.
%and integrated our load balancing strategies into the baseline implementations. 
We used LonestarGPU-1.02 version, an older version available at the time of our work. 
For our experiments, we use both synthetically generated graphs as well as the real-world graphs.
The synthetic ones are the RMAT graphs based on the recursive matrix model~\cite{gtgraph} and random graphs based on the Erd\H{o}s-R\'enyi model (denoted as ER*). 
Both are generated using GTgraph~\cite{gtgraph}.
For real-world graphs, we use the USA road networks (for West, Florida and overall USA).
To assess scalability, we include three relatively larger graphs obtained using the graph generation tool available in the Graph500 benchmark~\cite{graph500-web}. 
%For all three graphs, the number of nodes is 16.78 million and the number of edges is 335 million, with the average degree of the nodes being 20.  
The tool accepts three parameters: number of nodes, number of edges and a seed value for random number generation, and generates a corresponding graph.
Depending upon the seed value, the graph connectivity differs.
%For all the graphs, the minimum degree is 0, maximum degree is about 924000, and standard deviation of degrees is about 20900.
The properties of all the graphs are given in Table~\ref{graph_properties}. 
The last column of the table represents the amount of load imbalance in the form of the skewness in the outdegree distribution of the nodes.
This is indicated in terms of the maximum, average and standard deviations ($\sigma$) of their outdegrees.

We observe in Table~\ref{graph_properties} that Graph500 and RMAT graphs have a high maximum degree as well as a lot of variance in the number of outdegrees.
RMAT graphs are also characterized by \textit{small-world property} due to their low diameter.
In contrast, the road networks have very small maximum degree and little variation in the outdegree distribution. 
They have large diameters (not shown) in comparison to the RMAT and ER graphs.
ER graphs have a random distribution of edges in the graph and have a higher max-degree as well as the standard deviation than road networks.
However, they do not have large diameters as in the case of road networks, nor do they exhibit the small-world property.
It should be noted that despite this variance, the average degrees of all the graphs remain comparable.
Together these graphs test various aspects of our strategies.
%Each graph has the minimum outdegree as $0$ or $1$.

\begin{table}
\centering
\caption{Graphs Used in our Experiments}
\footnotesize
\begin{tabular}{|r|r|r|rrr|}
%\begin{tabular}{|p{0.9in}|p{0.45in}|p{0.45in}|p{0.9in}|}
\hline%\hline
{\em Graph} 	& {\em Nodes} 		& {\em Edges} 		& \multicolumn{3}{c|}{\em Outdegrees}	\\
 		& {\em (Million)}	& {\em (Million)}	& {\em Max}	& {\em Avg}	& $\sigma$ \\
\hline\hline
rmat20 & 1.05 & 8.26 & 1,181 & 8 & 177.40 \\
\hline
road-FLA & 1.07 & 2.71 & 8 & 3 & 2.45 \\
road-W & 6.26 & 15.12 & 9 & 4 & 2.74 \\
road-USA & 23.95 & 57.71 & 9 & 3 & 2.74 \\
\hline
ER20 & 1.05 & 4.19 & 15 & 4 & 4.47 \\
ER23 & 8.39 & 33.55 & 10 & 3 & 4.46 \\
\hline
Graph500 & 16.78 & 335.00 & 924,000 & 20 & 20,900 \\
(three graphs) &  &  &  &  &  \\
\hline%\hline
\end{tabular}
\label{graph_properties}
\end{table}

We implemented all our proposed strategies using CUDA.
Our experiments were performed on a Kepler-based GPU system. 
The GPU has a Tesla K20c architecture with 13 SMXs each having 192 CUDA cores (2,496 CUDA cores totally) with 4.66 GB of main memory, 1 MB of L2 cache and 64 KB of registers per SM.  
It has a configurable 64 KB of fast memory per SMX that is split between the L1 data cache and shared memory. 
The programs have been compiled using \textit{nvcc} version 5.0 with \textit{-O3 -arch=sm\_35} flags. 
The CPU is a hex-core Intel Xeon E5-2620 2.0 GHz workstation with CentOS 6.4, 16 GB RAM and 500 GB hard disk.

\newcommand{\figurewidth}{0.25}

\begin {figure*}
\centering
\subfigure[Low Diameter Graphs]{
  \includegraphics[width=\figurewidth\linewidth]{images/sssp-synthetic}
  %\includegraphics[scale=0.2]{images/AllRGraphs_sssp.pdf}
  \label{rgraphs_sssp}
}
\subfigure[Road Networks]{
  \includegraphics[width=\figurewidth\linewidth]{images/sssp-road}
  \label{USA-road-d_FLA_sssp}
  \label{USA-road-d_W_sssp}
  \label{USA-road-d_USA_sssp}
}
\subfigure[Graph500 Graphs]{
  \includegraphics[width=\figurewidth\linewidth]{images/sssp-graph500}
  \label{very_large_sssp}
}
\caption{Comparison of Load Balancing Strategies for \textbf{SSSP}} %\todo{explanation of baseline kernel time being high}}
\label{overall-comparisons-sssp}
\end{figure*}

\begin {figure*}
\centering
\subfigure[Low Diameter Graphs]{
  \includegraphics[width=\figurewidth\linewidth]{images/bfs-synthetic}
  \label{rgraphs_bfs}
}
\subfigure[Road Networks]{
  \includegraphics[width=\figurewidth\linewidth]{images/bfs-road}
  \label{USA-road-d_FLA_bfs}
  \label{USA-road-d_W_bfs}
  \label{USA-road-d_USA_bfs}
}
\subfigure[Graph500 Graphs]{
  \includegraphics[width=\figurewidth\linewidth]{images/bfs-graph500}
  \label{very_large_bfs}
}
\caption{Comparison of Load Balancing Strategies for \textbf{BFS}}
\label{overall-comparisons-bfs}
\label{very_large_graph_results}
\end{figure*}

\REM {
\begin {figure}
\centering
\subfigure[Synthetic Graphs]{
  \includegraphics[scale=0.2]{images/AllRGraphs_bfs.pdf}
  \label{rgraphs_bfs}
}
\subfigure[road-FLA]{
  \includegraphics[scale=0.2]{images/USA-road-d_FLA_bfs.pdf}
  \label{USA-road-d_FLA_bfs}
}
\subfigure[road-W]{
  \includegraphics[scale=0.2]{images/USA-road-d_W_bfs.pdf}
  \label{USA-road-d_W_bfs}
}
\subfigure[road-USA]{
  \includegraphics[scale=0.2]{images/USA-road-d_USA_bfs.pdf}
  \label{USA-road-d_USA_bfs}
}
\caption{Comparison of Load Balancing Strategies - BFS}
\label{overall-comparisons-bfs}
\end{figure}
}

\subsection{Performance Comparison of Strategies}\label{expt:exectime}

In this section, we compare the various strategies in terms of performance or execution times. The strategies are denoted as BS for LoneStar GPU baseline version that implements node-based task-distribution, EP for edge-based distribution, WD for workload decomposition, NS for node-splitting, and HP for hierarchical processing. We split the overall execution time into useful kernel time and the overhead associated with implementing a strategy. The overheads encompass all the corresponding initializations, extra kernel invocations and bookkeeping. Note that BS also has an overhead component.

Figure \ref{overall-comparisons-sssp} shows the comparison results for SSSP. We find that all our strategies perform significantly better than the baseline (BS) method in almost all cases, for graphs with small as well as large diameters. This is because in SSSP, especially for the graphs with large diameters, the kernel times dominate the overheads (unlike in BFS, discussed below).  This shows that our load balancing strategies in particular, and load balancing in general, are fruitful for applications that perform even a reasonable amount of computations. We believe the techniques would be more useful for computation-intensive irregular applications. The edge-based parallelism (EP) method performs the best, giving 60--80\% smaller execution times than the baseline. Unfortunately, due to its high storage requirement, EP is unable to run on larger graphs such as Graph500. Among the node-based strategies, workload decomposition (WD) method performs the best for graphs with highly skewed or random degree distribution. For such graphs (RMAT and ER), the node splitting (NS) performs the worst since its node creation overhead coupled with highly skewed degree distribution dominates the kernel times. However, when the deviation in the size of the neighborhood is less, the NS method performs the best among the node-based strategies since its node creation overhead is a one-time cost and is amortized by relatively large total kernel execution times.

Hierarchical processing (HP) performs in between the WD and NS methods for smaller graphs. However, the main advantage of HP is seen in dealing with larger graphs such as Graph500. At the time of writing this paper, we were able to execute only the HP strategy of the three load balancing strategies (WD,NS and HP) for these large graphs.
% Apart from BS, only HP is able to complete execution on Graph500 graphs without running out of memory.
As mentioned, the edge-based parallelism (EP) has a high storage complexity related to storing the edges and hence cannot be executed for these large graphs.
% The workload decomposition (WD) strategy could not be executed since its additional memory requirements related to node and edge offsets, and prefix sum arrays could not be accommodated for these graphs. The node splitting (NS) strategy's additional memory requirements for storing the additional child nodes could not be accommodated as well.
We find that the HP method gives large improvements resulting in 48-75\% reduction in execution times for these large graphs. Thus the HP method will have larger importance as we explore real-world BigData graphs.


\REM {
\begin {figure}
\centering
\subfigure[BFS]{
  \includegraphics[scale=0.2]{images/very_large_bfs.pdf}
  \label{very_large_bfs}
}
\subfigure[SSSP]{
  \includegraphics[scale=0.2]{images/very_large_sssp.pdf}
  \label{very_large_sssp}
}
\caption{Results for Very Large Graphs}
\label{very_large_graph_results}
\end{figure}
}

\REM {
Following are the overheads reported for the different strategies: \\
1. Baseline (BS): initialization of worklist, and total time for the main loop excluding GPU kernel executions. \\
2. Edge-based parallelism (EP): initialization of worklist, and total time for the main loop excluding GPU kernel executions. \\
3. Workload Decomposition (WD): initialization of worklist, CUDA kernel for obtaining the offsets before the main loop, and total time for the main loop excluding GPU kernel executions. \\
4. Node Splitting (NS): finding node-splitting level using histogram approach and 10 bins, initialization of worklist, and total time for the main loop excluding GPU kernel executions. \\
5. Hierarchical processing (HP): finding node-splitting level using histogram approach and 10 bins, creating space for extra worklists, initialization of worklist, creating GPU memory for allocating worklists and total time for the main loop excluding GPU kernel executions.
}

Figure \ref{overall-comparisons-bfs} shows the comparison results for BFS. It is noteworthy that BFS is a memory-bound kernel, and it performs only a little computation. Therefore, we observe the associated overheads are large in general, unlike in SSSP where the overheads were lesser than the computations. Only when the graphs get sufficiently bigger, do the overheads amortize. The EP method, similar to SSSP, consistently delivers better performance than BS. However, its high storage requirements could not be accommodated for the large-sized Graph500 graphs. For the graphs with small diameters, namely the RMAT and ER graphs, the execution time with EP is 48--68\% lesser than that of BS (0.17 MTEPS (BS) vs. 0.54 MTEPS (EP) for RMAT20). For the graphs with large diameters, namely, the road networks, the maximum performance gain with EP over BS is about 10\%.  

Similar to SSSP results, the WD method performs the best among the node-based approaches for graphs with small diameters for the BFS application. The NS method involves considerable overhead for these graphs. For graphs with large diameters, the NS method performs the best since it incurs lesser one-time overheads. In case of relatively larger graphs such as Graph500, HP performs considerably ($>$2$\times$) better than BS, while the EP method fails
% other approaches (EP, NS and WD) fail
to complete execution due to insufficient memory.

% Though the EP and WD methods both provide equal load to all threads, the WD method requires many more global memory accesses that are uncoalesced due to which the kernel execution times are larger with the WD method. The overheads associated with WD method, namely, inclusive scan and finding offsets are not present in the EP method. The kernel execution times of the HP method are consistently more (about 5--27\% more) than in the NS method due to extra kernel invocations which leads to more global memory accesses and atomic operations to maintain the sub iteration worklist. However, for graphs with smaller diameters, the large overheads due to the formation of extra nodes in the NS method results in the HP method giving significantly smaller (about 65\% smaller) total execution times when compared to the NS method. For graphs with large diameters, the overheads in the HP method are also greater than in the NS method due to the prominent number of memory copies in the HP method due to many kernel invocations while the NS method incurs only a one-time overhead for the formation of the nodes. Hence the NS methods gives 30--38\% smaller total execution times than the HP method for these graphs. For road networks, though the kernel execution times in HP are 7--12\% smaller than in WD, the total execution times are 22--31\% greater due to larger overhead of HP.

\subsection{Performance, Space Complexity and Implementation Tradeoffs}

\begin{figure}
\centering
\includegraphics[width=0.5\linewidth]{images/all-by-dimension-tight.pdf}
\caption{Overall comparison of strategies. Each axis represents a ranked order. A strategy closer to the origin (at the center) is ranked higher.}
\label{expt:overall}
\end{figure}

While the previous section focused on the performance aspects of the strategies, in this section we compare the strategies in terms of three axes of comparison: (i) execution time, (ii) memory requirement, and (iii) implementation complexity. The first two are quantitative, while the last one is a qualitative assessment out of our experience. Figure~\ref{expt:overall} shows the relative rankings of the strategies in terms of the three aspects. In each axis, a strategy that is closer to the origin is superior in terms of the corresponding factor.

Overall, it is clear that no one technique fares in all aspects. This suggests that we may have to use different load balancing strategies depending upon the performance requirements, amount of GPU memory and personnel expertise available. Despite the lack of a clear winner, Edge-based Processing (EP) ranks better on two axes (Execution Time and Implementation Complexity). This makes it a more desirable option when the amount of memory is not an issue. 
%We believe our study explains why recent implementations focus on EP~\cite{sariyuce-bc-gpgpu2013}.
Node-based processing (which we call as baseline BS) is also easy to implement, and has a low memory requirement (due to CSR representation), but in our experience, performs the worst. Another useful choice could be Hierarchical Processing (HP). It incurs lesser memory penalty and has a moderate implementation complexity. HP does not fare well in terms of performance for small graphs but performs well for large graphs.
% (while the other methods could not be executed for these large graphs due to high memory complexity).
Workload Decomposition (WD) and Node Splitting (NS) could be the methods of choice when performance is more important but memory is insufficient to execute EP. NS (implemented as a static phase) is likely to perform better than WD despite graph modification, but incurs larger memory overhead.

\subsection{Degree Distribution due to Node Splitting}
%The NS method computes the threshold for node-splitting (i.e., maximum degree threshold MDT) using histogram-based approach.
%In our node-splitting strategy, our histogram-based method automatically determines the maximum degree threshold (MDT) or correct level of node splitting. 
The NS method modifies the graph by creating child-nodes to distribute the outdegrees.
Therefore, the degree distribution in the modified graph differs from the original.
Figure~\ref{ns-degreedist} shows the distribution of out-degrees of the nodes before (red curve) and after (green curve) node splitting for two synthetic graphs.
The maximum degree thresholds (MDTs) determined using the histogram approach are also shown. 
It is evident from the figure that NS achieves a better load balancing by confining all the nodes to outdegrees within a small range (represented by green curves). We obtained similar results for the other graphs.
%We find that our histogram-based approach effectively leads to load balancing using node splitting by confining all the nodes to degree distribution within a small range (represented by the green curves).
It should also be noted that by exploiting histogramming, the MDT does not get biased to a range based on the graph size. %or degree distribution.
For instance, for road networks and random graphs, MDT is 2--4 whereas for RMAT graph, it gets rightly computed as 118.

\begin {figure}
\centering
\subfigure[rmat20. MDT=118]{
  \includegraphics[scale=0.2]{images/rmat20_degNS.pdf}
  \label{rmat20_degNS}
}
\subfigure[ER23. MDT=3]{
  \includegraphics[scale=0.2]{images/r4-2e23_degNS.pdf}
  \label{r42e23_degNS}
}
\caption{Degree Distributions of Graphs Before and After Node Splitting}
\label{ns-degreedist}
\end{figure}

%\subsection{Very Large Graphs}
%\todo{merge with earlier subsections}
%Although the edge-based parallelism (EP) method gave the best results in all cases, as mentioned earlier, one of the disadvantages of this method is that it cannot accommodate very large-size graphs since it follows the COO representation for edge processing. We experimented with three such large graphs. These graphs were obtained using the graph generation tool available in the Graph500 benchmark. For all three graphs, the number of nodes is 16.78 million and the number of edges is 335 million, with the average degree of the nodes being 20.  The graphs are generated using a random number generator provided in the Graph500 Benchmark.  A seed is provided to the generator that generates a graph with the mentioned number of nodes and edges with different connectivity. For all the graphs, the minimum degree is 0, maximum degree is about 924000, and standard deviation of degrees is about 20900. 

\subsection{Work Chunking Optimization for Edge-based Parallelism}
While using atomic operations to add edges to the worklist in the GPU kernel, we use work chunking in which we collect all edges of a node and add them together using a single atomic operation. 
We compare this strategy with the default strategy of using an atomic operation for adding every edge. 
Figure~\ref{work-chunk-table} shows the speedups obtained due to work chunking EP method over the the default EP method.
We find that work chunking results in 1.11--3.125, with an average of 1.82, speedups over the default method.

\REM {
\begin{table}
\small
\centering
\caption{Benefits of Work Chunking for Reducing Atomic Operations}
\begin{tabular}{|>{\hfill}p{0.55in}|>{\hfill}p{0.45in}|>{\hfill}p{0.5in}|>{\hfill}p{0.55in}|>{\hfill}p{0.55in}|}
\hline %\hline
{\em Graph} & \multicolumn{2}{|c|}{\em BFS} & \multicolumn{2}{|c|}{\em SSSP} \\
\cline{2-5}
& EP (msecs) & EP$_{\mbox{chunk}}$  (msecs) &  EP (msecs) & EP$_{\mbox{chunk}}$ (msecs) \\
\hline \hline
rmat20 & 36.10 & 14.84 & 297.75 & 95.23 \\
\hline
road-FLA & 592.08 & 597.24 & 3535.33 & 2479.11 \\
road-W & 993.44 & 990.82 & 18421.72 & 11790.62 \\
road-USA & 2116.23 & 1865.22 & 228329.00 & 125358.55 \\
\hline
ER20 & 20.47 & 12.62 & 61.94 & 37.19 \\
ER23 & 162.57 & 84.93 & 674.17 & 292.9 \\
%\hline
%\todo{Graph500} &  &  & & \\
\hline %\hline
\end{tabular}
\label{work-chunk-table}
\end{table}
}
\begin{figure}
\centering
  \includegraphics[width=0.70\linewidth]{images/sssp-bfs-chunking}
  \caption{Benefits of Work Chunking in Edge-based Processing}
  %\caption{Benefits of Work Chunking due to Reduced Atomic Operations}
  \label{work-chunk-table}
\end{figure}
