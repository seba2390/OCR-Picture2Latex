\section{Introduction}
\label{intro}
Large graphs are becoming ubiquitous due to the rise of large connected real life networks including social, transportation, web and gene expression networks, which can be abstracted and modeled as graphs. There is large interest in the acceleration of different graph processing algorithms and applications including breadth first search (BFS), single-source shortest path (SSSP), minimum spanning tree (MST) and betweenness centrality algorithms on GPUs~\cite{nasre-morphgpus-ppopp2013, merrill-scalablegputraversal-ppopp2012, sariyuce-bc-gpgpu2013, gharaibeh-graphsgpus-ipdps2013, ediger-graphct-tpds2013, buluc-parallelBFS-sc2011,mclaughlin-bc-sc2014} due to both the large processing needs of the applications and also due to the benchmark efforts like Graph500~\cite{graph500-web}.

Traditionally, graph applications are considered to be difficult to analyze and parallelize. This difficulty stems from the unpredictable data-access and control-flow patterns, termed as \textit{irregularity}, which is inherent in graph applications. Thus, it is quite challenging to statically predict the access pattern without any knowledge about the input graph. Therefore, most of the effective techniques towards the analysis and the parallelization of graph algorithms are dynamic in nature.

One of the key challenges while dealing with irregular graph algorithms is maintaining load-balance across GPU threads. Depending upon the graph, the task-distribution can get skewed. As an example,
many of the GPU-based implementations~\cite{nasre-morphgpus-ppopp2013, nasre-datavstoplogy-ipdps2013, merrill-scalablegputraversal-ppopp2012, sariyuce-bc-gpgpu2013, gharaibeh-graphsgpus-ipdps2013} employ a node-based task-distribution, i.e., they assign a thread to a set of graph nodes. 
Such a node-based distribution works well with the compressed sparse-row (CSR) storage format often used to store graphs compactly
(CSR format represents a graph in adjacency list format, but the adjacencies of each node are concatenated together to form a monolithic adjacency list 
of size equal to the number of edges. 
Each node's adjacency list then starts at various offsets in this monolithic list).
However, this node-based distribution is unsuitable for graphs with wide variance in the node degrees, such as the social networks. 
This is because in many graph applications, the activity at each node deals with propagating some information to its neighbors or collecting it from them.
Therefore, the work done is proportional to the degree of a node.
Thus, the node-based task-distribution can lead to high load-imbalance for skewed degree graphs. 

To address the load-balancing problem, previous research has explored edge-based task-distribution~\cite{sariyuce-bc-gpgpu2013}.
In this method, a thread is assigned a set of edges (instead of nodes as in the node-based method above).
This leads to near-perfect load balancing across threads, since each thread processes (almost) the same number of edges.
However, there are a couple of issues with edge-based processing.
First, it may not always be feasible to convert a node-based processing algorithm into an edge-based processing algorithm.
Theoretically, such a conversion requires the node activity to be \textit{distributive} which need not necessarily hold.
Second, it poses restrictions on the graph format: to assign an edge to a thread, the graph should either be in a coordinate list (COO) format or it should be converted to such a format
(COO format represents a graph as a sequence of edges with each edge as a tuple $<src, dst, wt>$).
The former consumes more memory while the latter has conversion overheads.
The memory requirement is a key factor for GPUs as they continue to have low memories (upto 12 GB).
In fact, in our experiments with large graphs, we found that the edge-based methods which rely on COO format cannot be executed due to insufficient memory.
These restrictions make edge-based task-distribution unsuitable as a general solution.

Despite the aforementioned issues, node-based and edge-based task-distribution methods are attractive because of their simplicity.
The simplicity stems from the one-time assignment of graph elements to threads.
Thus, existing methods are \textit{static} in nature.
Unfortunately, the characteristics of irregular graph algorithms often change dynamically.
The processing workload at different parts of the graph varies as the algorithm progresses.
Therefore, application of static load-balancing techniques is often inadequate, and
we need dynamic load balancing mechanisms while dealing with graph algorithms.

In this paper, we propose three techniques to address the issues with the existing methods.
The first method, which combines node-based and edge-based methods is \textit{workload decomposition}. 
It assigns a set of edges to a thread similar to the edge-based task-distribution.
However, the considered edges belong only to the set of \textit{active} nodes (i.e., nodes in the worklist where there is work to be done) resembling a node-based task-distribution.
The second method, which we call as \textit{node splitting}, avoids load-imbalance by changing the underlying graph structure.
As the name suggests, it splits a high-degree node into several small-degree nodes -- thereby reducing the load-imbalance.
The third method uses a \textit{hierarchical processing} and employs a hierarchy of worklists.
The imbalance in the task-distribution from the main (super) worklist is handled by creating several sub-worklists distributed evenly across threads and changing the number of threads proportional to the size of the sub-worklist.


%In this paper, we compare four different load balancing strategies, namely, {\em edge-based assignment}, {\em workload decomposition}, {\em node splitting}, and {\em hierarchical processing}. The edge-based assignment follows edge-based parallelism while the other three strategies follow node-based parallelism. The edge-based assignment approach balances load by assigning the edges to the GPU threads. The workload decomposition strategy assigns the threads to a set of nodes and makes a thread process a subset of edges of the nodes that are assigned to it. We also propose a load balancing strategy based on the concept of {\em node splitting}. In this approach, nodes which have an out degree greater than a threshold are split into lesser out degree nodes, thereby ensuring load balancing. The performance of node splitting approach is based on the node splitting level. We also propose a novel histogram based heuristic approach which automatically selects the node splitting level. Our last strategy is a novel {\em hierarchical processing} method that decomposes a stage of GPU execution into sub-stages using kernel unrolling such that the variation in out-degrees of the nodes processed at a sub-stage is within a threshold.

%We applied our load balancing strategies to two applications, namely, Breadth First Search (BFS) and Single Source Shortest Path (SSSP) on different graphs, and analyze the merits of the algorithms.  We found that the edge-based processing method performs the best giving about 10\% better performance than the baseline for BFS, and about 60-80\% better performance than the baseline for SSSP. Compared to BFS, all our load balancing strategies give significantly better results (at least 20\% better) than the baseline for SSSP. This shows that load balancing becomes very essential for computationally-intensive graph applications, especially for large graphs.
%For very large graphs in which some of our load balancing strategies cannot be executed due to memory constraints, our novel hierarchical processing method proposed in this work gives 48-75\% reduction in execution time compared to the baseline.

Following are the primary contributions of our paper.
\begin{enumerate}
\item We identify the limitations of the currently prevalent node and edge-based approaches towards load-balancing graph algorithms on GPUs.
\item We propose three methods to address the above limitations: \textit{workload decomposition}, \textit{node splitting} and \textit{hierarchical processing}. We discuss various trade-offs associated with these methods; a method needs to be chosen depending upon the application requirements and the nature of input graphs. We argue that a single load-balancing strategy is unlikely to be suitable in all scenarios.
\item We perform a comprehensive evaluation of the existing and the proposed methods using a range of real-world and synthetic graphs and two graph algorithms: breadth first search processing and single source shortest paths computation. We illustrate the utility of each method and quantitatively show the trade-offs involved.
\end{enumerate}

The rest of the paper is organized as follows.
In Section~\ref{motivation} we explain existing node-based and edge-based task-distributions and discuss their advantages and limitations.
In Section~\ref{strategies} we propose our load balancing strategies and discuss the trade-offs involved.
In Section~\ref{exp_res} we illustrate the effectiveness of our proposed techniques using two graph algorithms.
In Section~\ref{related} we compare our work with and contrast it against other proposed methods dealing with graph applications on GPUs. 
Finally, in Section~\ref{con_fut} we conclude and mention our plans for future work.


