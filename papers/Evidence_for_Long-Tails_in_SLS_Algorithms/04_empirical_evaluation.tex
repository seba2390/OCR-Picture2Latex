


\subsection{Experimental Setup, Instance Types, and Solvers Used}





Hoos and Stützle~\cite{HS98EvaluatingLasVegas} introduced the concept of \emph{runtime distribition} to characterize the cdf of Las Vegas algorithms, where the runtime can vary from one execution to another, even with the same input.
To obtain enough data for a fitting of such a distribution, for each base instance $F$ we created 5000 modified instances $F^{(1)}, \dots, F^{(5000)}$ by generating resolvent sets $\Clauses^{(1)}, \dots, \Clauses^{(5000)}$ %
each by using \refAlg{algo:res} with $w=4$ and a value of $p$ such that the expected number of resolvents being added was $\frac{1}{10} |F|$.
Note that we also conducted a series of experiments to rule out the influence of $p$ on our results.
Each of these modified instances was solved 100 times, each time using a different seed.
For $i = 1, \dots, 5000$ and $j = 1, \dots, 100$ we thus obtained the values $\mathsf{flips}_{S}(F^{(i)}, s_j)$
indicating how many flips were used to solve the modified instance~$F^{(i)}$ with solver~$S$ when using the seed~$s_j$.
Next, we calculated the mean number of flips $\mathsf{mean}_S(F^{(i)}) := \frac{1}{100} \sum_{j=1}^{100} \mathsf{flips}_{S}(F^{(i)}, s_j)$ required to solve $F^{(i)}$ with solver~$S$ whose hardness distribution we are going to analyze.


All experiments were performed on bwUniCluster 2.0 and 
three 
local servers.
Sputnik~\cite{VLSKK15Sputnik} was used to distribute the computation and to parallelize 
the 
trials.
Due to the heterogeneity of the 
computer 
setup, measured runtimes are not directly comparable to each other.
Consequently, we instead measured the number of variable flips performed by the SLS solver.
This is a hardware-independent performance measure with the benefit that it can also be analyzed 
theoretically.
To give an indication of how flips relate to wall-clock time, 
one million flips take about one second of computing time on one of our servers.
To give an idea of the computational effort involved, obtaining
the data for the ecdf of a 100 variable base instance with \SRWA{} took an average of 17,193,517 seconds ($\approx 199$~days) when unparallelized.
This clearly prohibited examining instances having a number of variables currently being routinely solved by the state-of-the-art SLS algorithms.
For the experiments the following instance types were used:

\begin{enumerate}
	\item \textbf{\textcolor{lipicsGray}{Hidden Solution:}}		
	We implemented the CDC algorithm~\cite{BC18UsingAlgorithmConfigurationTools, BHLRTWZ02Hiding} in~\cite{LW21SourceCodeOfConcealSATgen}
	to generate instances with a hidden solution. 
	For this, 
	at the beginning,
	a complete assignment~$\alpha$ is specified to ensure the generated formula's satisfiability.
	Then, repeatedly a randomly generated clause~$C$ is added to the formula with a weighted probability $p_i$ depending on the number~$i$ of correct literals in $C$ with respect to $\alpha$. 
	We included this type of instances because SLS solvers struggle to solve such instances. Experiments like these might be beneficial to find theoretical reasons for this behavior.	
	
	\item \textbf{\textcolor{lipicsGray}{Hidden Solution With Different Chances:}}			
	We also created sets of formulas with different underlying $p_i$ values to rule out the influence of these.
	
	
	\item \textbf{\textcolor{lipicsGray}{Uniform Random:}}	
	To generate
	uniform, random $k$-SAT instances with $n$ variables and $m$ clauses, each
	clause is generated by sampling $k$~literals uniformly and independently. 
	Using Gableske's kcnfgen~\cite{kcnfgen},
	we generated formulas with $n \in \set{50,60,70,80,90}$ variables and
	a clause-to-variable ratio~$r$ close to the \emph{satisfiability threshold}~\cite{MMZ06ThresholdValues} of $r \approx 4.267$.
	We checked each instance 
	with \texttt{Glucose3}~\cite{AS09Glucose,ES03MiniSat}
	for satisfiability
	until we had 5 formulas of each size.		
	
	
	\item \textbf{\textcolor{lipicsGray}{Factoring:}} 
	These formulas encode the factoring problem in the interval $\set{128, \dots, 256}$ and were generated with~\cite{Diemer21GenFactorSat}.
	
	
	\item \textbf{\textcolor{lipicsGray}{Coloring:}}
	These formulas assert that a 
	graph 
	is colorable with 
	3 colors.
	We generated 
	these formulas, 
	using~\cite{LENV17CNFgen}, over random graphs with $n$ vertices and $m = 2.254n$ edges in expectation,
	which is slightly below the \introduceterm{non-colorability threshold}~\cite{KaporisKS00}.
	We
	obtained 32~satisfiable instances in 150 variables.
	
	
\end{enumerate}



Our experiments investigated leading SLS solvers where the dominating component is based on the random walk procedure proposed in~\cite{Schoening02AProbabilisticAlgorithm}.
In this paper, Schöning's Random Walk Algorithm \SRWA{} was introduced, which is one of the solvers we used.
The \probSAT{} solver family~\cite{BalintImplementationOfProbSAT} is based on this approach.
One of these solvers won the random track of the SAT competition 2013~\cite{SAT13}.
Another advancement of \algoformat{SWRA} was implemented as \YalSAT{}~\cite{YalSAT}, which won the random track of the SAT competition 2017~\cite{SAT17}.
These performances and similarities were reasons for choosing \SRWA{}, \probSAT{}, and \YalSAT{} as SLS solvers for this paper. The connection to \GapSAT{}~\cite{LW20OnTheEffectOfLearnedClauses} is another case in point.

We excluded the solvers \algoformat{DCCAlm}~\cite{LCS16DCCAlm} and \algoformat{CSCCSat}~\cite{LCWS16CSCCSat} (combining \algoformat{FrwCB}~\cite{LCWS13FRW} and \algoformat{DCCASat}~\cite{LCWS14DoubleConfiguration}) as all of these 
depend on 
heuristics (like \textsf{CC}, \textsf{BM}, \textsf{CSDvars}, \textsf{NVDvars}, \textsf{SDvars}) that ultimately reduce the probabilistic nature when choosing the next variable to flip.

For \SRWA{} we conducted most of our experiments: All instance types were tested, including different change values for the generation of the hidden solution.
For \probSAT{}, 55 hidden solution instances with $n \in \set{50,100,150,200,300,800}$ were used.
Since \YAL{} can be regarded as a \probSAT{} derivate, 
we tested \YAL{} with 10 hidden solution instances with 300 variables each. 
















\subsection{Experimental Results and Statistical Evaluation}

The goal of this section is to explore the scenarios described above in more detail. We are particularly interested in how the hardness of an instance changes when logically equivalent clauses are added in the manner described above. To characterize this effect as accurately as possible, studying the ecdf is the most suitable method for this purpose. In turn, the ecdf can be described using well-known distribution types such as \eg the normal distribution. In the following, we shall demonstrate that the three-parameter lognormal distribution, in particular, provides an exceptionally accurate description of the runtime behavior, and this is true for all considered problem domains and all solvers. The results are so compelling that we ultimately conjecture that the runtimes of \Alfa{}-type algorithms all follow a lognormal distribution, regardless of the considered problem domain. 

To illustrate this point, we first demonstrate our approach using two base instances. The first one is a factorization instance that was solved by \SRWA{}. The second instance has a hidden solution and was solved by \probSAT{}. For later reference, we refer to the first instance as~$A$ and to the second instance as~$B$. As described above, we obtain 5000~samples for each base instance. Using these data points, we estimate the lognormal distribution's three parameters by applying the maximum likelihood method (see~\cite{AllData}). After that, one can visually evaluate the suitability of the fitted lognormal distribution for describing the data. A useful method of visualizing the suitability is to plot the ecdf and the fitted cdf on the same graph.


\begin{figure}[htb]
	\centering
	\begin{subfigure}[b]{.465\linewidth}
		\centering
		\input{tikz/cdf}
	\end{subfigure}%
	\hfill%
	\begin{subfigure}[b]{.465\linewidth}
		\centering
		\input{tikz/prob_cdf}
	\end{subfigure}%
	\caption{%
		The ecdf and fitted cdf of the hardness distribution %
		of instance $A$ (left) and $B$ (right).
	}
	\label{fig:cdf-ecdf}
\end{figure}
Such a comparison is illustrated in \refFig{fig:cdf-ecdf} for the two instances~$A$ and~$B$. In both cases, no difference between the empirical data of the ecdf and the fitted distribution can be detected visually. In other words, the absolute error between the predicted probabilities from the fitted cdf versus the empirical probabilities from the ecdf is minuscule. Even though these are only two examples, it should be noted that these two instances are representative of the behavior of the investigated algorithms. Hardly any deviation could be observed in this plot type for all instances and all algorithms. All data is published under~\cite{AllData}.

For the analysis, however, one should not confine oneself to this plot type. Although absolute errors can be observed easily, relative errors are more difficult to detect. Such a relative error may have a significant impact when used for decisions such as restarts. To illustrate this point, suppose that the true probability of a run of length $\ell$ is $0.0001$. In contrast, the probability estimated based on a fit is $0.001$. As can be seen, the absolute error of $0.0009$ is small, whereas the relative error of $10$ is large. If one were to perform restarts after $\ell$ steps, the actual expected runtime would be ten times greater than the estimated expected runtime. Thus, the erroneous estimate of that probability would have translated into an unfavorable runtime. This example should illustrate the importance of checking the tails of a distribution for errors as well. 

The left tail, \ie the probabilities for 
very
small values, can be checked visually by plotting the ecdf and 
fitted cdf with both axes logarithmically scaled.
Thereby, the probabilities for extreme events (in this case, especially easy instances) can be measured accurately. 
\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.465\linewidth}
		\centering
		\input{tikz/log_cdf}
	\end{subfigure}%
	\hfill%
	\begin{subfigure}[t]{.465\linewidth}
		\centering
		\input{tikz/prob_log_cdf}
	\end{subfigure}%
	\caption{%
		Logarithmically scaled ecdf and fitted cdf of instances $A$ (left) and $B$ (right).
	}
	\label{fig:log_cdf-log_ecdf}
\end{figure}

The two instances~$A$ and~$B$ are being examined in this manner in \refFig{fig:log_cdf-log_ecdf}. As can be observed, the lognormal fit accurately predicts the probabilities associated with very short runs. For the other instances, lognormal distributions were mostly also able to accurately describe the probabilities for short runs. However, the behavior of the ecdf and the fitted lognormal distribution differed very slightly in a few instances. 

Lastly, the probabilities for particularly hard instances should also be checked.  Any mistakes in this area could lead to underestimating the likelihood of encountering an exceptionally hard instance. For analyses of this type, the survival function $S$ is a useful tool; if $F$ is the cdf, $S(x):=1-F(x)$. Therefore, the survival function's value $S(x)$ represents the probability that an instance is (on average) harder than $x$ in our case. If we plot the empirical survival function, \ie $\hat{S}_n(x) := 1-\hat{F}_n(x)$, and the fitted survival function together on a graph with logarithmically scaled axes, we can easily detect errors in the right tail.

\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.465\linewidth}
		\centering
		\input{tikz/log_sf}
	\end{subfigure}%
	\hfill%
	\begin{subfigure}[t]{.465\linewidth}
		\centering
		\input{tikz/prob_log_sf}
	\end{subfigure}%
	\caption{%
		Logarithmically scaled empirical survial function and fitted survival function of instances~$A$ (left) and $B$ (right).
	}
	\label{fig:sf-esf}
\end{figure}

\refFig{fig:sf-esf} illustrates this type of plot for the instances~$A$ and~$B$. Here, there is a discernible deviation between $A$ and $B$. While for $A$, the lognormal fit provides an accurate description of the probabilities for long runs, in the case of $B$, the empirical survival function seems to approach $0$ somewhat slower than the lognormal estimate. In the vast majority of cases, these extreme value probabilities are accurately reflected by the lognormal fit. In most other cases, the empirical survival function approaches 0 more slowly than the lognormal fit. Thus, in these cases, the likelihood of encountering an exceptionally hard instance is underestimated.

So far, we discussed the behavior of lognormal fits based on this visual inspection. Altogether, we concluded that lognormal distributions seem to be well suited for describing the data. Next, we shall concretize this through a statistical test. To be more precise, we apply the $\chi^2$-test as a goodness-of-fit test for each base instance.
For each such instance, the fitted lognormal distribution used the $5000$ data points, and afterwards, the $\chi^2$-test statistic is computed. Subsequently, the probability that such a value of the test statistic occurs under the assumption of the so-called null hypothesis is determined. We will refer to this probability as the $p$-value. In our case, the null hypothesis is the assumption that the data follow a lognormal distribution. If the fit is poor, then a small $p$-value will occur. If $p$ is sufficiently small, the null hypothesis is rejected. We reject the null hypothesis if \mbox{$p<0.05$}. 

Two more remarks are due on this matter. First, from a high $p$-value, one cannot 
prove
that the assumption that the data are lognormally distributed is correct.
However, we use a sufficiently high $p$-value as a heuristic whether this assumption is reasonable.

Secondly, there is an obstacle that complicates statistical analysis by this method. As described,
each of the $5000$~data points is obtained by first sampling $100$~runtimes of the corresponding instance and then calculating the mean. 
This means that we do not work with the actual expected values but only estimates. In other words, this implies that our data is noisy. The greater the variance in the respective instance, the greater the corresponding noise. If one were to apply the $\chi^2$-test to this noisy data, some cases would be incorrectly rejected, especially if the variance is large. To overcome this limitation, we additionally use a bootstrap-test, which is based on Cheng~\cite{cheng2017non}. This test is presented %
in \refAlg{algo:bootstrap}.

\begin{algorithm}[htb]
	\textbf{Input:} (noisy) random sample $\mathbf{y}=(y_1, y_2, \dots, y_n)$, integer $N$, significance $\alpha\in (0,1)$\\
	\BlankLine
	
	$\hat{\theta}\leftarrow \textnormal{MLE}(\mathbf{y}, F)$, lognormal maximum likelihood estimation, $F$ is the lognormal cdf\\
	$X^2 \leftarrow \textnormal{ChiSquare}(\mathbf{y}, \hat{\theta})$, Chi-squared goodness of fit test statistic\\
	
	\SetKw{KwWithProb}{with probability}
	\SetKw{KwDo}{do}
	
	\SetKwFunction{Shuffle}{Shuffle}
	
	\For{$j=1$ to $N$}{
		$\mathbf{y}' \leftarrow (y'_1, \dots, y'_n)$, where all $y'_i$ are i.\,i.\,d.\ samples from the fitted lognormal\\
		 \phantom{$\mathbf{y}' \leftarrow (y'_1, \dots, y'_n)$,} distribution with parameters $\hat{\theta}$\\
		$\mathbf{y}' \leftarrow \mathbf{y}' + \textnormal{noise}$, where noise is sampled from an $n$-dimensional normal distribution\\
		$\hat{\theta}'\leftarrow \textnormal{MLE}(\mathbf{y}', F)$\\
		$X^2_j \leftarrow \textnormal{ChiSquare}(\mathbf{y}', \hat{\theta}')$\\
	}
	Let $X^2_{(1)}\leq X^2_{(2)}\leq \dots \leq X^2_{(N)}$ be the sorted test statistics.\\
	\lIf{$X^2_{( \lfloor (1-\alpha)\cdot N \rfloor )}<X^2$}{reject \textbf{else} accept}
	
	
	\caption{Bootstrap-test for noisy data}
	\label{algo:bootstrap}
\end{algorithm}

Briefly summarized, this test simulates how our data points were generated, assuming the null hypothesis. For this purpose, particular attention should be paid to how the test sample is rendered noisy. Owing to the central limit theorem, it is reasonable to assume that the initial data's sample mean originates from a normal distribution around the true expected value.  We use this assumption in the bootstrap-test using a noise signal drawn from a normal distribution with expected value~$0$. The variance of this normal distribution is determined from the initial data and divided by $100$ (cf.\ central limit theorem). %

If there is a large difference between the respective $p$-values of the $\chi^2$- and bootstrap-tests, this suggests that the variance in the initial data is too high and that the number of samples used to calculate the sample mean should be increased. However, this case has only occurred twice, and we explicitly indicate it later. In all other cases, the $p$-values of the $\chi^2$- and the bootstrap-tests were similar.
To illustrate this point, let us consider the $p$-values of the two instances~$A$ and~$B$. The excellent representation of the runtime behavior over the entire support on instance~$A$ is also reflected in the two $p$-values. Using the $\chi^2$-test, one obtains a $p$-value of approximately $0.783$, and using the bootstrap-test, one obtains a $p$-value of $0.76$. Since these two $p$-values are both above $0.05$, we conclude that the assumption that lognormal distributions describe the runtimes is reasonable.

In contrast, for instance $B$, we observed that while a lognormal distribution accurately describes the main part %
of the distribution,
the probabilities for extremely long runs are inadequately represented. This observation is again reflected in the respective $p$-values. The $\chi^2$-test yields a $p$-value of 
$\approx 0.008$, and the bootstrap-test yields a $p$-value of $0.013$. Since both $p$-values are
below 
$0.05$, the assumption that these data originate from a lognormal distribution is rejected.

Overall, this example is intended to demonstrate that these two statistical tests can show an inadequate fit, even if the problems only arise for extreme values. Second, it should demonstrate that the $p$-values of the two tests are generally similar; if the $p$-values differ significantly, then more samples should be used to calculate the sample mean.

We now proceed by considering the adequacy of lognormal distributions for describing \SRWA{} runtimes.  The results of the statistical analysis are reported in \refTab{tab:stat} and can be found in~\cite{AllData}.
\begin{table}[htb]
	\begin{tabular}{l| c c c c c || c}
		& hidden & different chances & uniform & factoring & coloring & total\\ \hline
		rejected & 0 & 2 & 1 & 2 & 0 & 5\\
		$\#$ of instances & 20 & 120 & 25 & 33 & 32 & 230
	\end{tabular}
	\caption{Statistical goodness-of-fit results for \Alfa{}+\SRWA{} runtimes over various problem domains. The \emph{rejected} row contains the number of instances where the lognormal distribution is not a good fit according to the $\chi^2$-test at a significance level of $0.05$. To put these results into perspective, the second row contains the total number of instances of each domain. Out o a total of 230 instances, 5 got rejected.}
	\label{tab:stat}
\end{table}

The first line in the table represents for how many instances the statistical tests rejected the lognormal distribution hypothesis. The second line indicates how many instances have been checked in total. 
It should be noted that the same number of instances was rejected by the $\chi^2$-test as was by the bootstrap-test.
Thus,
there is no need to distinguish between the tests here. It should also be mentioned that in statistical tests, there is always a possibility that a hypothesis will be rejected even though the null hypothesis holds (type 1 error).
At a significance level of \mbox{$0.05$}, this probability is at the most $5\%$.
Accordingly, the total of $5$~rejected instances may be attributed to 
so-called
\mbox{type~1 errors}. This statement is also supported by the fact that no exceptionally low $p$-value was observed, \ie no $p$-value that is unusual for a total of $230$~samples.

For \probSAT{}, on the other hand, the situation appears to be different. The results are summarized in \refTab{tab:stat_probSAT} and~\cite{AllData}.
\begin{table}[htb]
	\begin{tabular}{l| c c c c c c || c}
		number of variables & 50 & 100 & 150 & 200 & 300 & 800 & total \\\hline
		rejected & 2 & 2 & 1 & 0 & 2 & 0 & 7\\
		$\#$ of instances & 10 & 10 & 10 & 10 & 10 & 5 & 55
	\end{tabular}
	\caption{Goodness-of-fit results for \Alfa{}+\probSAT{} over various hidden solution instance sizes.
	The \emph{rejected} row contains the number of instances where the lognormal distribution is not a good fit according to the $\chi^2$-test at a significance level of $0.05$. To put these results into perspective, the second row contains the total number of instances of each instance size.}
	\label{tab:stat_probSAT}
\end{table}
The columns refer to the number of variables in the corresponding SAT instances.
The number of rejected instances is again identical regardless of whether the $\chi^2$- or bootstrap-test is applied. %
As can be seen, the lognormal distribution hypothesis was rejected for $7$ of the $55$~instances.
This number can no longer be accounted for by type 1 errors at a significance level of~$0.05$.
However, one can observe that the majority of rejected instances occur for a small number of variables.
If one were to consider only the instances from $150$~variables onwards, then the remaining rejected instances may be attributed to type 1 errors.
This raises the suspicion that there may be a limiting process, \ie that the lognormal distribution hypothesis is only valid for $n\rightarrow \infty$.

Lastly, a difference between the two static tests emerges for \YAL{}. According to the $\chi^2$-test, $2$ of the total $10$~instances are rejected. However, using the bootstrap-test, the lognormal distribution hypothesis is not rejected for any instance. Therefore, one cannot rule out the possibility that lognormal distributions are the natural model to describe the instances, but more experiments are required to make a more precise statement.

In summary, the presumption that lognormal distributions are the appropriate choice for describing runtimes has been reinforced for \SRWA{}. For \probSAT{}, this appears plausible at least above a certain instance size. Likewise, the choice of lognormal distributions also seems reasonable for \YAL{}. These observations lead us to the following conjecture.

\begin{conjecture}[Strong Conjecture]
	\label{conj:strong}
	The runtime of $\Alfa$ with $\algoformat{SLS} \in \set{\texttt{SRWA}, \texttt{probSAT}, \texttt{YalSAT}}$ follows a lognormal distribution.
\end{conjecture}

If this statement is true, then it would be intriguing in that one can infer how modifying the base instance affects the hardness of instances.
This effect is likely to be the result of generating models for lognormal distributions.
Just as the normal distribution is a natural model for the sum of \iid random variables, the lognormal distribution is a natural model for the multiplication of \iid random variables. Thus, one can hypothesize that each added clause exerts a small multiplicative effect on the instance's hardness. 

Simultaneously, the three parameters of the lognormal distribution also provide insight into how the hardness of the instance changes. For example, the location parameter $\gamma$ implies an inherent problem hardness that cannot be decreased regardless of the added clauses' choice. 
At the same time, $\gamma$ also serves as a numerical description for the value of this intrinsic hardness.
Using Bayesian statistics, it is possible to infer the parameters while the solver is running.
These estimated parameters can, for example, be used to schedule restarts.
This would lead to a scenario similar to that discussed in~\cite{RHK02RestartPolicies}.

Conjecture~\ref{conj:strong} is a strong statement.
However, %
a small deviation of the probabilities, for example, at the left tail, would render the strong conjecture invalid from a 
strict
mathematical point of view. Particularly, 
visual analyses revealed that the left tail's behavior, \ie for extremely short runs, is occasionally not accurately reflected by lognormal distributions. Conversely, the right tail, \ie the probabilities for particularly long runs, are usually either correctly represented by lognormal distributions or, occasionally, the corresponding probability approaches $0$~even more slowly. We, therefore, rephrase our conjecture in a %
weakened form. Our observations fit a class of distributions known as long-tail distributions defined purely in terms of their behavior at the right tail.

\begin{definition}[\cite{foss2011introduction}]
	\label{def:long_tail}
	A positive, real-valued random variable $X$ is \introduceterm{long-tailed}, if and only if
	\begin{align*}
		\forall x\in \Rpos:\, \prob{X>x} > 0
		\quad \quad \textnormal{and} \quad \quad
		\forall y\in \Rpos: \lim\limits_{x\rightarrow \infty} \frac{\prob{X > x+y}}{\prob{X>x}} = 1.
	\end{align*}
\end{definition}
\begin{conjecture}[Weak Conjecture]
	\label{conj:weak}
	The runtime of \Alfa{} with $\algoformat{SLS} \in \set{\texttt{SRWA}, \texttt{probSAT}, \texttt{YalSAT}}$ follows a long-tailed distribution.
\end{conjecture}


It should be noted that lognormal distributions have the long-tail property~\cite{foss2011introduction,nair2020fundamentals}. That is, if the Strong Conjecture holds, the Weak Conjecture is implied. The reverse is, however, not true. In the next section, we show an important consequence in case the Weak Conjecture holds.









