
\section{Formal analysis of \logdense and \loglogdense}


\subsection{Proof of Proposition~\ref{them:log-dense-log-dist}}
\label{sec:logdense_proof}
\begin{myproof}
We call \bd$(x_i, x_j)$ the back-propagation distance from $x_i$, $x_j$, which is the distance between the two 
nodes $x_i, x_j$ on the graph constructed for defining \pbd in Sec.~\ref{sec:pbd}.
The scaling transition happens only once for each scale during backpropagation; i.e., there 
are at most $n_{block}-1$ number of transitions between any two layers $x_i$, $x_j$.  

Since the transition between each two scales happens at most once, in between two layers $x_i$, $x_j$, we first consider $n_{block}=1$, and add $n_{block}-1$ to the final distance bound to account for multiple blocks. 

We now prove the proposition for $n_{block}=1$ by induction on $|i-j|$. 
Without loss of generality we assume $i>j$. 
The base case: for all $i> j$ such that $i = j+1$, we have \bd$(x_i, x_j)$ = 1.
Now we assume the induction hypothesis that for some $t \geq 0$ and $t\in \mathbb{N}$, such that 
for all $i >j$ and $i - j \leq 2^t$, we have \bd$(x_i, x_j) \leq t +1$.  
Then for any two layers $i >j$ such that $i -j \leq 2^{t+1}$, if 
$i - j \leq 2^t$, then by the induction hypothesis, \bd$(x_i, x_j) \leq t +1 < t+2$.
If $2^{t+1} \geq i - j > 2^t$, then 
we have $k := i - 2^t > j$, and $k= i - 2^t \leq 2^{t+1} -2^t + j = j+ 2^t$. So that 
$k - j \in (0, 2^t]$. Next by the induction hypothesis, 
\bd$(x_k, x_j) \leq t+1$. 
Furthermore, by the connections of \logdense, $x_i$ takes input directly from 
$x_{i-2^t}$, so that \bd($x_i$, $x_k$) = 1. Hence, by the triangle inequality of 
distances in graphs, we have \bd$(x_i, x_j) \leq$ \bd$(x_i, x_k)$ + \bd$(x_k, x_j) \leq 1 + (t+1)$. 
This proves the induction hypothesis for $t+1$, so that the proposition follows, i.e., 
for any $i \neq j$, \bd$(x_i, x_j) \leq \log |i-j| + 1$. 

\end{myproof}

%\allienote{Here's my version}
%We call $k_l, k_{l-1}, ..., k_0$ the indices of nodes the backpropagation signal moves through with direct connectivity from node $i=k_l$ to node $j=k_0$.  As long as we choose these nodes such that each is directly connected to the next in the sequence, the total \pdb is $\Sigma_{t=1}^{t=l}\mathpbd(k_{t-1}, k_t) = l$.  The problem reduces to counting the number of nodes $l$ we use to traverse from $i$ to $j$.
%We will choose the nodes in a greedy fashion working backward from $i$, finding the node $k_{t-1}$ by moving as far as possible from $k_t$ without overshooting $j$.  Formally, $k_{l-1} = i - 2^{\lfloor \log(i-j) \rfloor}$, or more generally, $k_{t-1} = k_t - 2^{\lfloor \log(k_{t} - j) \rfloor}$.
%When we choose the nodes in this fashion, the most nodes we could need are  $l = 1 + m = 1 + \lfloor(\log(j - i)\rfloor$: $k_l,...,k_0 = i, i-2^m, i-2^{m-1}, i-2^{m-2}, ..., i-2^0$.


\subsection{Proof of Proposition~\ref{them:loglog-dense-loglog-dist}}
\label{sec:loglogdense_proof}

\begin{myproof}
Since the transition between each two scales happens at most once, in between two layers $x_i$, $x_j$, we again first consider $n_{block}=1$, and add $n_{block}-1$ to the final distance bound to account for multiple blocks. 

(\textbf{Number of connections.})
We first analyze the recursion tree of \verb=lglg_conn=($0,L$). 
In each \verb=lglg_conn=$(s,t)$ call, let $n = t-s+1$ be the number of layers on the segment $(s,t)$. 
Then the interval of key locations $\delta = \lfloor \sqrt{t-s+1} \rfloor = \lfloor \sqrt{n} \rfloor$, and 
the key location set $K=\{s\} \cup \{ t-   k \delta  : t- k \delta  \geq s \text{ and }  k =0,1,2,..., \} $ has 
a cardinality of $|K| = 1 + \lceil \frac{n-1}{\lfloor \sqrt{n} \rfloor} \rceil \in (\sqrt{n},  2.5 + \sqrt{n})$. 
Hence the step (a) of \verb=lglg_conn=$(s,t)$ in Sec.~\ref{sec:loglogdense} adds $1+2+3+... + (|K|-1) = 0.5|K|(|K|-1)$.
Step (b) of \verb=lglg_conn=$(s,t)$ then creates $(n - |K|)$ new connections, since the ones among $x_s,..., x_t$ that are not given new connections are exactly $x_i$ in $K$. Hence, \verb=lglg_conn=$(s,t)$ using step (a),(b) increases the total connections by 
\begin{align}
   c(n) = n  + 0.5 |K|^2 - 1.5|K| < 1.5n + \sqrt{n} + 3.125.
\end{align}
Step (c) instantiate $|K|-1 \leq \sqrt{n} + 1.5$ calls of \verb=lglg_conn=, each of which has an input segment of length at most $\delta \leq \sqrt{n}  + 1$.
Hence, let C(n) be the number of connections made by the recursive call \verb=lglg_conn=$(s,t)$ for $n = t-s+1$, then we have the recursion
\begin{align}
    C(n) \leq (\sqrt{n} + 1.5) C(\sqrt{n}+1) + c(n).
\end{align}
Hence, the input segment length takes a square root in each depth until the base case at length 2. 
The depth of the recursion tree of \verb=lglg_conn=$(s,t)$ is then $1 + \log \log (t-s+1)$. Furthermore, the connections made on each depth $i$ of the tree is $1.5n + o(n)$, because at each depth $i=0,1...$, 
$c(n^{2^{-i}} + o(n^{2^{-i}})) = 1.5n^{2^{-i}} + o(n^{2^{-i}})$ connectons are made in each \verb=lglg_conn=, and the number of calls is 1 for $i=0$, and 
$\Pi _{j=0}^{i} (n^{2^{-j}}+1.5) = n^{1 - 2^{-i}} + o(n^{1 - 2^{-i}})$. Hence the total connections in \loglogdense is $C(L+1) = 1.5L\log\log L + o(L\log\log L)$. 

(\textbf{Back-propogation distance.})
First, each $x_i$ for $i \in [s,t]$ in the key location set $K$ of \verb=lglg_conn=($s,t$) is in the input set of $x_t$. 
Second, for every $x_i$, and for every call \verb=lglg_conn=($s,t$) in the recursion tree such that $s < i \leq t$ and $(s,t) \neq (0,L)$, we know step (b) adds $x_s$ to the input set of $x_i$. Hence, we can form a back-propagation path from any $x_i$ to $x_j$ $(i  > j)$ by first using a connection from step (b) to go to a key location of the \verb=lglg_conn=($s,t$) call such that $[s,t]$ is the smallest interval in the recursion tree such that $i,j \in [s,t]$. Then we can continue the path to $x_j$ by following the recursion calls whose input segments include $j$ until $j$ is in a key location set. The longest path is then the depth of the recursion tree plus one initial jump, i.e., $2 + \log\log L$.



\end{myproof}

%% The following is wrong XD
%We also note that we can further tighten the above bound on $o(L\log\log L)$ to $o(L)$, because for $i \geq 1$, $\Pi _{j=0}^{i} (n^{2^{-j}}+1.5)$ is a polynomial of $n$ whose highest two powers are $n^{1 - 2^{-i}}$ and $n^{1 - 2^{-i+1}}$.
%Hence, $\Pi _{j=0}^{i} (n^{2^{-j}}+1.5) = n^{1 - 2^{-i}}  + O(n^{1 - 2^{-i+1}})$, so that the number of connections from each depth $i$ is then $1.5n + O(n^{1-2^{-i}})$ instead of $1.5n + o(n)$. This value increases with $i$, so setting $i = \log\log L$, we have per depth cost to be at most $1.5n + O(n^{1 - (1/ \log (n))})$


\subsection{\loglogdense Layers on Average Has Five Connections in Practice}
Figure~\ref{fig:loglog_avg_connections} shows the average number of input layers for each feature layer in \loglogdense. Without augmentations, \verb=lglg_conn= on average has 3 to 4 connections per layer. With augmentations using \logdense, we desire each layer to have four inputs if possible. On average, this increases the number of inputs by 1 to 1.5 for $L \in (10, 2000)$.

\begin{figure}
    \centering
    \subfloat[Average number of inputs per Layer in \loglogdense]{
        \includegraphics[width=0.5\textwidth]{loglog_averge_conn.png}
        \label{fig:loglog_avg_connections}
    }
    ~
    \subfloat[Blocks versus Computational Cost in FCNs]{
        \includegraphics[width=0.5\textwidth]{fc_flops_vs_blocks.png}
        \label{fig:fc_computation}
    }

    \cprotect\caption{\textbf{(a)} In \verb=lglg_conn=$(0,L)$, i.e., min inputs = 1, each layer on average takes input from 3 to 4 layers. If we force input size to be four when possible using \logdense connection pattern, i.e., min inputs = 4, we increase the average input size by 1 to 1.5. \textbf{(b)} Computational cost (in FLOPS) distribution through the 11 blocks in FC-DenseNet and FC-\logdense. Half of the computations are from the final two blocks due to the high final resolutions. We compute the FLOPS assuming the input is a single 224x224 image.}
    \label{fig:my_label}
\end{figure}
