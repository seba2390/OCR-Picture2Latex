\documentclass{article}

% use Times

\usepackage{iclr2018_conference, times} 
% For figures

\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfig}
\usepackage{xspace}
\usepackage{caption}
\usepackage{color}
\usepackage{cprotect}



% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2017} with
% \usepackage[nohyperref]{icml2017} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''

\usepackage{verbatim}		% allows begin/end comment

\usepackage{url}		% allows sensible printing of URLs
\usepackage{fancyhdr}		% allows headers and footers
\usepackage{datetime}		% provides date & time of compilation
\usepackage{lastpage}		% provides page number of last page

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xspace}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newenvironment{myproof}[1][\proofname]{\proof[#1]\mbox{}}{\endproof}


\newcommand{\concat}{\text{concat}}
\newcommand{\logdensename}{Log-Dense Network\xspace}
\newcommand{\logdense}{Log-DenseNet\xspace}
\newcommand{\logdenses}{Log-DenseNets\xspace}
\newcommand{\logdensenp}{Log-DenseNet}
\newcommand{\loglogdensename}{LogLog-Dense Network\xspace}
\newcommand{\loglogdense}{LogLog-DenseNet\xspace}
\newcommand{\loglogdenses}{LogLog-DenseNets\xspace}
\newcommand{\loglogdensenp}{LogLog-DenseNet}
\newcommand{\nearest}{NEAREST\xspace}
\newcommand{\evenspace}{EVENLY-SPACED\xspace}
\newcommand{\pbdfull}{maximum backpropagation distance\xspace}
\newcommand{\pbd}{MBD\xspace}
\newcommand{\bd}{\text{BD}\xspace}
\newcommand{\bdnp}{\text{BD}}
\newcommand{\mathpbd}{\text{MBD}\xspace}
\newcommand{\round}[1]{\lfloor #1 \rceil}

\newcommand{\annname}{Anytime Neural Network\xspace}
\newcommand{\ann}{ANN\xspace}
\newcommand{\annnp}{ANN} 

\newcommand{\allienote}[1]{\textcolor{blue}{#1}}
\newcommand{\naive}{na\"{\i}ve\xspace}
\newcommand{\naiive}{\naive}
\usepackage{tablefootnote}
\usepackage{scrextend}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ZEYUAN: space hacking BEGINS here. Can remove them for the final submission.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\ifshort
%\renewcommand{\parhead}[1]{\medskip \noindent {\bfseries\boldmath\ignorespaces #1.}\hskip 0.9em plus 0.3em minus 0.3em}
\usepackage{enumitem} % control layout of itemize, enumerate, description
\setlength{\footskip}{1\baselineskip}
%\setlength{\parskip}{0.3\baselineskip}
\setlength\textfloatsep{1\baselineskip plus 2pt minus 2pt}
%\AtBeginDocument{%
% \abovedisplayskip=4pt minus 1pt
% \abovedisplayshortskip=2pt plus 1pt
% \belowdisplayskip=4pt minus 1pt
% \belowdisplayshortskip=2pt plus 1pt
%}
\setitemize{itemsep=0mm, leftmargin=5mm, topsep=0mm}
\setenumerate{itemsep=0mm, leftmargin=5mm, topsep=0mm}
\usepackage[noindentafter]{titlesec}
\titlespacing\section{0pt}{4pt plus 0pt minus 2pt}{2pt plus 0pt minus 1pt}
\titlespacing\subsection{0pt}{4pt plus 0pt minus 1pt}{1pt plus 1pt minus 1pt}
\titlespacing\subsubsection{0pt}{4pt plus 0pt minus 1pt}{1pt plus 1pt minus 1pt}

%\newtheoremstyle{slplain}% name
%  {.4\baselineskip\@plus.1\baselineskip\@minus.1\baselineskip}% Space above
%  {.3\baselineskip\@plus.1\baselineskip\@minus.1\baselineskip}% Space below
%  {\itshape}% Body font
%  {}%Indent amount (empty = no indent, \parindent = para indent)
%  {\bfseries}%  Thm head font
%  {.\xspace}%       Punctuation after thm head
%  { }%      Space after thm head: " " = normal interword space;
%       %       \newline = linebreak
%  {}%       Thm head spec
%\theoremstyle{slplain} % italics

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ZEYUAN: space hacking ENDS here. Can remove them for the final submission.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{\logdense: How to Sparsify a DenseNet}

\author{Hanzhang Hu$^1$, Debadeepta Dey$^2$, Allison Del Giorno$^1$, Martial Hebert$^1$ \& J. Andrew Bagnell$^1$ \\
\resizebox{\textwidth}{!}{
\begin{tabular}[h]{@{}l}
    $^1$ Carnegie Mellon University\\
    Pittsburgh, PA, USA \\
    \texttt{\{hanzhang,adelgior,hebert,dbagnell\}@cs.cmu.edu}
\end{tabular}
\hfill
\begin{tabular}[h]{l@{}}
    $^2$ Microsoft Research \\
    Redmond, WA, USA \\
    \texttt{dedey@microsoft.com}
\end{tabular}
}
}

\iclrfinalcopy
\begin{document}
\maketitle
%\twocolumn[
%\icmltitle{Anytime Neural Network via Joint Optimization of Auxiliary Losses}

%\begin{icmlauthorlist}
%\icmlauthor{Hanzhang Hu}{cmu}
%\icmlauthor{J. Andrew Bagnell}{cmu}
%\icmlauthor{Martial Hebert}{cmu}
%\end{icmlauthorlist}
%\icmlaffiliation{cmu}{Carnegie Mellon University, Pittsburgh, USA}
% \icmlkeywords{Anytime prediction}

%\vskip 0.3in
%]

\begin{abstract}
Skip connections are increasingly utilized by deep neural networks to improve accuracy and cost-efficiency. 
In particular, the recent DenseNet is efficient in computation and parameters, and achieves state-of-the-art predictions by directly connecting each feature layer to all previous ones. However, DenseNet's extreme connectivity pattern may hinder its scalability to high depths, and in applications like fully convolutional networks, full DenseNet connections are prohibitively expensive. 
This work first experimentally shows that one key advantage of skip connections is to have short distances among feature layers during backpropagation. Specifically, using a fixed number of skip connections, the connection patterns with shorter backpropagation distance among layers have more accurate predictions. Following this insight, we propose a connection template, \logdense, which, in comparison to DenseNet,  only slightly increases the backpropagation distances among layers from 1 to  ($1 + \log_2 L$), but uses only $L\log_2 L$ total connections instead of $O(L^2)$. Hence, \logdenses are easier than DenseNets to implement and to scale. We demonstrate the effectiveness of our design principle by showing better performance than DenseNets on \textit{tabula rasa} semantic segmentation, and competitive results on visual recognition.


%In particular, we show that in semantic segmentation with fully convolutional networks, where full DenseNet connections are too expensive, \logdenses achieve better performance with half of parameters thanks to the proposed design principle. We also demonstrate competitive results on visual recognition. 

%We demonstrate the effectiveness of our proposed design principle by showing better performance than  DenseNet on \textit{tabula rasa} semantic segmentation, and competitive results on visual recognition.
%In addition, \logdense can be {\naiive}ly implemented for deep configurations using standard deep learning libraries without the custom GPU memory management needed for increasing depths of DenseNet. 
%We combine this insight with the knowledge that deeper feature compositions have more representational power, and propose a new design principle for skip connections. This principle leads to a new architecture: \logdense. The proposed network only increases the max distances slightly in practice ($1 + \log_2 L$), and has much smaller complexity than DenseNets of the same depth ($L\log_2 L$ total connections).  

\end{abstract}

\section{Introduction}

Deep neural networks have been improving performance for many machine learning tasks, scaling from networks like AlexNet~\citep{alexnet} to increasingly more complex and expensive networks, like VGG~\citep{vggnet}, ResNet~\citep{resnet} and Inception~\citep{inception_v4}.  Continued hardware and software advances will enable us to build deeper neural networks, which have higher representation power than shallower ones. %~\citep{deepnet_info_theory}. 
However, the payoff from increasing the depth of the networks only holds in practice if the networks can be trained effectively. 
It has been shown that {\naive}ly scaling up the depth of networks actually decreases the performance \citep{resnet}, partially because of vanishing/exploding gradients in very deep networks. 
Furthermore, in certain tasks such as semantic segmentation, it is common to take a pre-trained network and fine-tune, because training from scratch is difficult in terms of both computational cost and reaching good solutions. Overcoming the vanishing gradient problem and being able to train from scratch are two active areas of research.
%This works well for overlapping tasks (image classification with similar classes), but does not scale well to training networks for entirely new tasks, such as surface normal estimation or cancer detection, and should theoretically limit performance of networks compared to effective training from scratch on the desired task.  
% because gradients tend to vanish or explode through long chains of backpropagation.
%More recently, a wide array of works found that adding shortcuts in connections of layers can effectively combat degradation of gradients.  

Recent works attempt to overcome these training difficulties in deeper networks by introducing skip, or shortcut, connections~\citep{fcn, hypercolumn, highwaynet, resnet, fractalnet, densenet} so the gradient reaches earlier layers and compositions of features at varying depth can be combined for better performance. In particular, DenseNet~\citep{densenet} is the extreme example of this, concatenating all previous layers to form the input of each layer, i.e., connecting each layer to all previous ones. However, this incurs an $O(L^2)$ run-time complexity for a depth $L$ network, and may hinder the scaling of networks. Specifically, in fully convolutional networks (FCNs), where the final feature maps have high resolution so that full DenseNet connections are prohibitively expensive, \cite{fcdense} propose to cut most of connections from the mid-depth. To combat the scaling issue, \cite{densenet} propose to halve the total channel size a number of times. Futhermore, \cite{slim_nn} cut 40\% of the channels in DenseNets while maintaining the accuracy, suggesting that much of the $O(L^2)$ computation is redundant. Therefore, it is both necessary and natural to consider a more efficient design principle for placing shortcut connections in deep neural networks.

%However, increasing the number of skip connections arbitrarily compromises the computational and memory efficiency of the network.  It is likely that not all permutations of inputs are needed to effectively learn a good representation, and generating $O(L^2)$ connections in a depth $L$ network likely wastes computation that could be reallocated to form deeper feature compositions.  Therefore, we ask whether there is a more efficient design principle to consider as we continue to scale up the size of deep neural networks.

In this work, we address the scaling issue of skip connections by answering the question: if we can only afford the computation of a limited number of skip connections and we believe the network needs to have at least a certain depth, where should the skip connections be placed?
We design experiments to show that with the same number of skip connections at each layer, the networks can have drastically different performance based on where the skip connections are. In particular, we summarize this result as the following design principle, which we formalize in Sec.~\ref{sec:pbd}:
\textbf{given a fixed number of shortcut connections to each feature layer, we should choose these shortcut connections to minimize the distance among layers during backpropagation.}

%the networks whose layers have shorter between-layer distances during backpropagation clearly outperform the networks that have longer distances. 

%In this work, we design experiments to show that the short distance among layers during backpropagation is indeed a key design principle to follow.   In particular, we show that with the same number of connections to previous layers at each depth of the network, connecting networks such that layers are closer to each other leads to better predictions. With this confirmed, our paper proposes a new design principle that aims to not only reduce the maximum \pbdfull (\pbd) in networks, but also considers the most efficient use of memory and computation given that limited resources create a trade-off between depth and density.  We summarize this design principle as follows:
%\textbf{If each feature layer takes input from a fixed number of previous layers, we should choose these shortcut connections to minimize the distance among layers during backpropagation.}
%Network architectures should ensure that the distance between any two layers during backpropagation is as small as possible while allowing large depths on a fixed computational and memory budgets by minimizing the number of skip connections.}


%. We maintain short distances within the network through carefully chosen skip connections, but reduce the parameter and \naive memory consumption while preserving most of its predictive power. 
%The network has $O(L\log L)$ run-time complexity and costs $O(L\log L)$ memory in naive implementations; it is also capable of scaling much deeper and wider without the custom GPU memory sharing and management required by DenseNet.  Furthermore, using the same number of computations and fewer parameters, the proposed network is able to achieve performance comparable to DenseNet in visual recognition on ILSVRC2012~\citep{ilsvrc} and \textit{tabula rasa} semantic segmentation on CamVid~\citep{camvid-dataset}. We summarize our contributions as follows:

Following this principle, we design a network template, \logdense. In comparison to DenseNets at depth $L$, \logdenses cost only $L\log L$, instead of $O(L^2)$ run-time complexity. Furthermore, \logdenses only slightly increase the short distances among layers during backpropagation from 1 to $1 + \log L$. Hence, \logdenses can scale to deeper and wider networks, even without custom GPU memory managements that DenseNets require. In particular, we show that \logdenses outperform DenseNets on \textit{tabula rasa} semantic segmentation on CamVid~\citep{camvid-dataset}, while using only half of the parameters, and similar computation. \logdenses also achieve comparable performance to DenseNet with the same computations on visual recognition data-sets, including ILSVRC2012~\citep{ilsvrc}. In short, our contributions are as follows:

\begin{itemize}
    \item We experimentally support the design principle that with a fixed number of skip connections per layer, we should place them to minimize the distance among layers during backpropagation. 
    \item The proposed \logdenses achieve small $1 + \log_2 L$ between-layer distances using few connections ($L\log_2 L$), and hence, are scalable for deep networks and applications like FCNs.
    \item The proposed network outperforms DenseNet on CamVid for \textit{tabula rasa}  semantic segmentation, and achieves comparable performance on ILSVRC2012 for recognition.
\end{itemize}

\section{Background and Related Works}
\label{sec:background}
%Many recent works attempt to handle the vanishing gradient problem as well as computational and memory constraints that come with recent deeper architectures.  The major two threads of research involve building skip connections into the architecture and compressing networks during or after training.

\textbf{Skip connections.}
The most popular approach to creating shortcuts is to directly add features from different layers together, with or without weights. Residual and Highway Networks~\citep{resnet, highwaynet} propose to sum the new feature map at each depth with the ones from skip connections, so that new features can be understood as fitting residual features of the earlier ones. FractalNet~\citep{fractalnet} explicitly constructs shortcut networks recursively and averages the outputs from the shortcuts. Such structures prevent deep networks from degrading from the shallow shortcuts via ``teacher-student" effects. \citep{dropoutnet} implicitly constructs skip connections by allowing entire layers to be dropout during training. 
%DenseNet~\citep{densenet} takes the extreme by concatenating all previous features together as the input of each new feature layer, forming $\frac{L(L+1)}{2}$ total connections in a depth $L$ network.
DualPathNet~\citep{dualpathnet} combines the insights of DenseNet~\citep{densenet} and ResNet~\citep{resnet}, and utilizes both concatenation and summation of previous features. 

%Recently, a wide array of works found that adding shortcuts (skip connections) between layers can effectively combat degradation of gradients. ResNet \citep{resnet} adds the input of a feature section to the end product to enable additive skips, and this is a special case of highway networks~\citep{highwaynet}, which learn the relative weighting of the input as well. FractalNet and DenseNet both use concatenation of early features instead of additions. Dual path networks~\citep{dualpathnet} combine DenseNets and ResNets and draw a parallel between higher order recurrent networks and feed-forward networks with skip connections. Skip connections are also used by fully convolutional networks~\citep{fcn} for combining high resolution low level features with low resolution global features.

%However, we note that summing features is a special case of concatenating these features. In particular, DenseNet~\citep{densenet} addresses this distinction, and each layer $i$ in a DenseNet takes input from the concatenation of all previous $i-1$ layers. Hence, the number of parameters and computational cost of DenseNet grows on the order of $\frac{L(L+1)}{2}$, where $L$ is the total number of layers.

\textbf{Run-time Complexity and Memory of DenseNets.}
DenseNet~\citep{densenet} emphasizes the importance of compositional skip connections and it is computationally efficient (in accuracy per FLOP) compared to many of its predecessors.  One intuitive argument for the cost-efficiency of DenseNet is that the layers within DenseNet are directly connected to each other, so that all layers can pick up training signals easily, and adjust accordingly.  However, the quadratic complexity may prevent DenseNet from scale to deep and wide models. In fact at each downsampling, DenseNet applies block compression, which halves the number of channels in the concatenation of previous layers. DenseNet also opts not to double the output channel size of conv layers after downsampling, which divides the computational cost of each skip connection. 
These design choices enable DenseNets to be deep for image classification where final layers have low resolutions. 
However, final layers in FCNs for semantic segmentation have higher resolution than in classification. Hence, to fit models in the limited GPU memory, FC-DenseNets~\citep{fcdense} have to cut most of their skip connections from mid-depth layers. 
Furthermore, a \naive implementation of DenseNet requires $O(L^2)$ memory, because the inputs of the $L$ convolutions are individually stored, and they cost $O(L^2)$ memory in total.   
Though there exist $O(L)$ implementations via memory sharing among layers~\citep{densenet_torch}, they require custom GPU memory management, which is not supported in many existing packages. Hence, one may have to use custom implementations and recompile packages like Tensorflow and CNTK for memory efficient Densenets, e.g., it costs a thousand lines of C++ on Caffe~\citep{densenet_caffe}. 
Our work recognizes the contributions of DenseNet's architecture to utilize skip connections, and advocates for the efficient use of compositional skip connections to shorten the distances among feature layers during backpropagation. Our design principle can especially help applications like FC-DenseNet~\citep{fcdense} where the network is desired to be at least a certain depth, but only a limited number of shortcut connections can be formed. 
%However, we emphasize that there is an inherent computation and memory trade-off that should not be overlooked.

\textbf{Network Compression.} 
A wide array of works have proposed methods to compress networks by reducing redundancy and computational costs. \citep{linear_structure_in_cnn, compress4mobile, deep_roots} decompose the computation of convolutions at spatial and channel levels to reduce convolution complexity. \citep{distillation, deepreally} propose to train networks with smaller costs to mimic expensive ones. \citep{slim_nn} uses $L1$ regularization to cut 40\% of channels in DenseNet without losing accuracy. These methods, however, cannot help in applications that cannot fit the complex networks in GPUs in the first place. This work, instead of cutting connections arbitrarily or post-design, advocates a network design principle to place skip connections intelligently to minimize between-layer distances.


\section{From DenseNet to \logdense} % and \loglogdense
\subsection{Preliminary on DenseNets}
\label{sec:densenet-preliminary}
Formally, we call the feature layers in a feed-forward convolutional network as $x_0, x_1,..., x_L$, where $x_0$ is the feature map from the initial convolution on the input image $x$, and each of the subsequent $x_i$ is from a transformation $f_i$ with parameter $\theta_i$ that takes input from a subset of $x_0,...,x_{i-1}$. In particular, the traditional feed-forward networks have $x_i = f_i(x_{i-1}; \theta_i)$, and the skip connections allow $f_i$ to utilize more than just $x_{i-1}$ for computing $x_i$. Following the trend of utilizing skip connections to previous layers \citep{resnet, highwaynet, fractalnet}, DenseNet \citep{densenet} proposes to form each feature layer $x_i$ using all previous features layers, i.e., 
\begin{align}
   x_i = f_i( \concat(\{ x_j : j = 0,..., i-1\} )\;; \; \theta_i),
\end{align}
where $\concat(\bullet)$ concatenates all features in its input collection along the feature channel dimension. Each $f_i$ is a bottleneck structure~\citep{densenet}, BN-ReLU-1x1conv-BN-ReLU-3x3conv, where the final conv produces $g$, the growth rate, number of channels, and the bottleneck 1x1 conv produces $4g$ channels of features out of the merged input features. DenseNet also organizes layers into $n_{block}$ number of blocks. Between two contiguous blocks, there is a 1x1conv-BN-ReLU, followed by an average pooling, to transform and downsample all previous features maps together to a coarser resolution. In practice, $n_{block} \leq 4$ in almost all state-of-the-art visual recognition architectures~\citep{resnet, inception_v4, densenet}. 
The direct connections among layers in DenseNet are argued to be the key reason why DenseNets enjoy high efficiency in parameter and computation to achieve the state-of-the-art predictions: the direct connections introduce implicit deep supervision~\citep{supervisednet} in intermediate layers, and reduce the vanishing/exploding gradient problem by enabling direct influence between any two feature layers.

%However, such dense direct connections cost DenseNet $O(L^2)$ run-time complexity, and often $O(L^2)$ memory, if one has no custom complied packages. This causes various temporary solutions to cut connections or to compress channels in DenseNet.

%%% OLD for reference
%The transformation function $f_i$ can be a composition of multiple transformations, including convolutions (conv) as well as pooling, batch-normalization (BN)\citep{batchnorm} and rectified linear layer (ReLU)\citep{relu}. We call $x_i$ the output feature map of $f_i$.  
%In traditional feed-forward networks, layer $i$ takes input from its predecessor layer $i-1$, so that $x_i = f_i(x_{i-1}; \theta_i)$. 
%Some works utilize early features in the network construction. The best known approach is ResNet\citep{resnet}, which proposes to use identity shortcut connections to enforce feature layers to be at least as competitive as identity functions. 
%More specifically in ResNet, $x_i = f_i(x_{i-1}; \theta_i) + x_{i-1}$, and $f_i$  contains multiple convolutions, batch-normalization and ReLU. \allienote{This ResNet reference should probably be in the background.}
%DenseNets \citep{densenet} instead leverage early features, and each layer $i$ in a DenseNet takes input from all previous feature layers, i.e., 
%\begin{align}
%   x_i = f_i( \concat(\{ x_j : j = 0,..., i-1\} )\;; \; \theta_i),
%\end{align}


\subsection{\pbdfull}
\label{sec:pbd}
We now formally define the proposed design principle that with the same number of connections, the distance between any two layers during backpropagation should be as small as possible. We consider each $x_i$ as a node in a graph, and the directed edge $(x_i, x_j)$ exists if $x_i$ takes direct input from $x_j$. The \textit{backpropagation distance} (\bd) from $x_i$ to $x_j$ $(i > j)$ is then the length of the shortest path from $x_i$ to $x_j$ on the graph. Then we define the \textit{\pbdfull} (\pbd) as the maximum \bd among all pairs $i > j$. Then DenseNet has a \pbd of 1, if we disregard transition layers. To reduce the $O(L^2)$ computation and memory footprint of DenseNet, we propose \logdense which increase \pbd slightly to $1+\log_2 L$ while using only $O(L \log L)$ connections and run-time complexity. Since the current practical networks have less than 2000 depths, the proposed method has a \pbd of at most 7 to 11.


\subsection{\logdense}
\label{sec:logdense}

\begin{figure}
    \centering
    \subfloat[DenseNet]{
        \includegraphics[width=0.22\textwidth]{Dense_connections.png}
        \label{fig:dense_connections}
    }
    ~
    \subfloat[\logdense V1]{
        \includegraphics[width=0.22\textwidth]{Log_connections.png}
        \label{fig:logv1_connections}
    }
    ~
    \subfloat[\logdense V2]{
        \includegraphics[width=0.22\textwidth]{LogV2_connections.png}
        \label{fig:logv2_connections}
    }
    ~
    \subfloat[\loglogdense]{
        \includegraphics[width=0.22\textwidth]{LogLog_connections.png}
        \label{fig:loglog_connections}
    }
   
    
    \cprotect\caption{Connection illustration for $L=24$. Layer 0 is the initial convolution.
    $(i,j)$ is black means $x_j$ takes input from $x_i$; it is white if otherwise. We assume there is a block transition at depth 12 for \logdense V2. \loglogdense is a connection strategy that has 2+$\log\log L$ \pbd.}
    \label{fig:connections}
\end{figure}

For simplicity, we let $\log(\bullet)$ denote $\log_2(\bullet)$. 
In a proposed \logdensename, each layer $i$ takes direct input from at most $\log(i)+1$ number of previous layers, and these input layers are exponentially apart from depth $i$ with base $2$, i.e.,
\begin{align}
    \label{eq:logdense_layer}
    x_i = f_i(\concat(
    \{ x_{i - \lfloor 2^k \rceil} : k = 0, ..., \lfloor \log( i) \rfloor  \}
    )\;; \;\theta_i),
\end{align}
where $\lfloor \bullet \rceil$ is the nearest integer function and $\lfloor \bullet \rfloor$ is the floor function. For example, the input features for layer $i$ are layer $i-1, i-2, i-4,...$. We define the input index set at layer $i$ to be 
\mbox{$ \{ {i - \lfloor 2^k \rceil} : k = 0, ..., \lfloor \log( i) \rfloor  \}$}. We illustrate the connection in Fig.~\ref{fig:logv1_connections}. 
Since the complexity of layer $i$ is $\log(i)+1$, the overall complexity of a \logdense is $\sum _{i=1}^L (\log(i) + 1)\leq L + L\log L = \Theta( L \log L)$, which is significantly smaller than the quadratic complexity, $\Theta(L^2)$, of a DenseNet. 


%As we will show in Sec.~\ref{sec:logdense_exp}, this reduction in complexity is especially important in the state-of-the-art networks, where networks models and intermediate features can barely fit into GPU memory. 



\textbf{\logdense V1: independent transition.}
%Pooling is an important step in convolutional networks to shrink resolutions of feature maps to produce high level features for classifications. 
Following \cite{densenet}, we organize layers into blocks. Layers in the same block have the same resolution; the feature map side is halved after each block. In between two consecutive blocks, a transition layer will shrink all previous layers so that future layers can use them in Eq~\ref{eq:logdense_layer}. We define a \textit{pooling transition} as a 1x1 conv followed by a 2x2 average pooling, where the output channel size of the conv is the same as the input one. 
We refer to $x_i$ after $t$ number of pooling transition as $x_i^{(t)}$. In particular, $x_i^{(0)} = x_i$. Then at each transition layer, for each $x_i$, we find the latest $x_i^{(t)}$, i.e., $t = max \{ s \geq 0 : x_i^{(s)} \text{exists}\}$, and compute $x_i^{(t+1)}$. 
We abuse the notation $x_i$ when it is used as an input of a feature layer to mean the appropriate $x_i^{(t)}$ so that the output and input resolutions match.
Unlike DenseNet, we independently process each early layer instead of using a pooling transition on the concatenated early features, because the latter option results in $O(L^2)$ complexity per transition layer, if at least $O(L)$ layers are to be processed. Since \logdense costs $O(L)$ computation for each transition, the total transition cost is $O(L\log L)$ as long as we have $O(\log L)$ transitions.


\textbf{\logdense V2: block compression.}
Unfortunately, many neural network packages, such as TensorFlow, cannot compute the $O(L)$ 1x1 conv for transition efficiently: in practice, this $O(L)$ operation costs about the same wall-clock time as the $O(L^2)$-cost 1x1 conv on the concatenation of the $O(L)$ layers.
To speed up transition and to further reduce \pbd, we propose a block compression for \logdense similar to the block compression in DenseNet~\citep{densenet}. At each transition, the newly finished block of feature layers are concatenated and compressed into $g \log L$ channels using 1x1 conv. The other previous compressed features are concatenated, followed by a 1x1 conv that keep the number of channels unchanged. These two blocks of compressed features then go through 2x2 average pooling to downsample, and are then concatenated together. Fig.~\ref{fig:logv2_connections} illustrates how the compressed features are used when $n_{block}=3$, where $x_0$, the initial conv layer of channel size $2g$, is considered the initial compressed block. The total connections and run-time complexity are still $O(L\log L)$, at any depth the total channel from the compressed feature is at most $(n_{block} -1) g \log L + 2g$, and we assume $n_{block} \leq 4$ is a constant. Furthermore, these transitions cost $O(L\log L)$ connections and computation in total, since compressing of the latest block costs $O(L\log L)$ and transforming the older blocks costs $O(\log^2 L)$. 


\textbf{\logdense \pbd.} 
The reduction in complexity from $O(L^2)$ in DenseNet to $O(L\log L)$ in \logdense only increases the \pbd among layers to $1+\log L$. This result is summarized as follows.

\begin{proposition}
\label{them:log-dense-log-dist}
For any two feature layers $x_i \neq x_j$ in \logdense that has $n_{block}$ number of blocks, the \pbdfull between $x_i$ and $x_j$ is at most \mbox{ $ \log |j-i| + n_{block}$ }.
\end{proposition}

This proposition argues that if we ignore pooling layers, or in the case of \logdense V1, consider the transition layers as part of each feature layer, then any two layers $x_i$ and $x_j$ are only $\log |j-i| +1$ away from each other during backpropagation, so that layers can still easily affect each other to fit the training signals. 
Sec.~\ref{sec:exp-low-backprop-distance} experimentally shows that with the same amount the connections, the connection strategy with smaller \pbd leads to better accuracy. We defer the proof to the appendix. In comparison to \logdense V1, V2 reduces the \bd between any two layers from different blocks to be at most $n_{block}$, where the shortest paths go through the  compressed blocks.

It is also possible to provably achieve $2+\log \log L$ \pbd using only $1.5L\log\log L + o(L\log\log L)$ shortcut connections (Fig.~\ref{fig:loglog_connections}), but we defer this design to the appendix, because it has a complex construction and involves other factors that affect the prediction accuracy.


\textbf{Deep supervision.}
Since we cut the majority of the connections in DenseNet when forming \logdense, we found that having additional training signals at the intermediate layers using deep supervision \citep{supervisednet}  for the early layers helps the convergence of the network, even though the original DenseNet does not see performance impact from deep supervision. For simplicity, we place the auxiliary predictions at the end of each block. Let $x_i$ be a feature layer at the end of a block. Then the auxiliary prediction at $x_i$ takes as input $x_i$ along with $x_i$'s input features. Following \citep{hu:ann}, we put half of the total weighting in the final prediction and spread the other half evenly. After convergence, we take one extra epoch of training optimizing only the final prediction. We found this results in the lower validation error rate than always optimizing the final loss alone.

%We also found that one extra epoch of training without auxiliary losses in the end results in higher validation accuracy than training directly for the final prediction alone. 

%we first walk through the root of the tree of recursive calls manually as an example. 
%The root function call takes the input pair $(1, L)$, which represent the start and the end (inclusive) of a sequence of contiguous layer indices, and in this case, all the indices. Then we let the set $K$ be $\{ \round{ k\sqrt{L} } : k\sqrt{L} \leq L \wedge  k =0,1,2,..., \} \cup \{ L \}$. Next we form dense connections among the layers indexed in $K$, i.e., if $i, j \in K$ and $i > j$, then $x_i$ takes input from $x_j$ directly. We now have $|K|-1$ number of smaller contiguous index segments: $(1, \round{\sqrt{L}}), (\round{\sqrt{L}}, \round{2\sqrt{L}}), ...$, and we call the recursive function on each of these sequence. Since $|K| \leq \sqrt{L}+1$, the connections formed at the root call is $0.5L + \Theta(\sqrt{L})$, and we create at most $\sqrt{L}$ recursive calls whose input segments have length $\sqrt{L}$. 


%Hence the recursion tree has depth of $\log\log L$, ending at tree leaves that have segments of length 1 or 2. At each depth $d$ (root is at depth $0$), the segment length of each call is $L^{2^{-d}}$, and there are $L^{1 - 2^{-d}}$ recursive calls on that depth. So that the connections formed on that depth is $0.5L + \Theta(\sqrt{L})$. 





%\subsection{Short Connections in Dense Structures}
%\label{sec:short-distance}

%One key common argument for DenseNet is that the early features can affect late layers through shortcut connections. In fact, each feature in DenseNet directly contributes to each future feature, if we ignore the constant number of pooling transitions. Such short backpropagation paths are important because a number of other works \cite{fractalnet, resnet_is_ensemble} have found it helpful for avoiding vanishing/exploding gradients and improving performance of deep networks. 

%Formally, we define a \textit{backpropagation path} from layer $x_i$ to layer $x_j$ to be a sequence of the feature layers $x_{k_0}, x_{k_1}, ..., x_{k_p}$, such that $k_0 =i$, $k_p = j$, $k_t \in \{0,..., L\}$, and for each $t < p$, ${k_{t+1}} \in \Lambda _b(k_t)$. Since we assume there are $n_{block}$ number of blocks, the number of conv on a backpropagation path, $x_{k_0}, ..., x_{k_p}$, is at most $p + n_{block} -1$. Then in DenseNet, if we ignore pooling layers, the shortest backpropagation path between any two layers is 1 due to direct connections. We can classify the shortest backpropagation paths for $b \leq 2$ as the following proposition. 


%The above proposition states that the backpropagation distance between layers in a \logdense grows logarithmically, so that early layers are effectively much closer to the late layers than in a regular feed-forward network, where the distance grows linearly. 
%We design experiments that provide evidence that it is important to have low backpropagation distances in Sec~\ref{sec:logdense_exp}, where we compare the proposed \logdense against networks that also have $O(\log L)$ inputs per feature layers but have higher shortest backpropagation distances. 
%We defer the proof, which is an induction on $|i-j|$, to the appendix. 




\section{Experiments}
\label{sec:exp-logense-main}

For visual recognition, we experiment on CIFAR10, CIFAR100 \citep{cifar}, SVHN \citep{svhn}, and ILSVRC2012~\citep{ilsvrc}.\footnote{CIFAR10 and CIFAR100 have 10 and 100 classes, and each have 50,000 training and 10,000 testing 32x32 color images. We adopt the standard augmentation to randomly flip left to right and crop 28x28 for training. SVHN contains around 600,000 training and around 26,000 testing 32x32 color images of numeric digits from the Google Street Views. We adopt the same pad-and-crop augmentations, and also apply Gaussian blurs. ILSVRC consists of 1.2 million training and 50,000 validation images from 1000 classes. We apply the same data augmentation for training as \citep{resnet, densenet}, and we report validation-set error rate from a single-crop of size 224x224 at test time. }
We follow \citep{resnet, densenet} for the training procedure and parameter choices. Specifically, we optimize using stochastic gradient descent with a moment of 0.9 and a batch size of 64 on CIFAR and SVHN. The learning rate starts at 0.1 and is divided by 10 after 1/2 and 3/4 of the total iterations are done. We train 250 epochs on CIFAR, 60 on SVHN, and 90 on ILSVRC. For CIFAR and SVHN, we specify a network by a pair $(n,g)$, where $n$ is the number of dense layers in each of the three dense blocks, and $g$, the growth rate, is the number of channels in each new layer.


%For \logdense V1, we found deep supervision leads to 3.51\% relative reduction in error rates on CIFAR and SVHN, which we defer to the appendix. 
 

\subsection{It matters where shortcut connections are}
\input{table_dsm_maintext.tex}


\label{sec:exp-low-backprop-distance}
This section verifies that short \pbd is an important design principle by comparing the proposed \logdense V1 against two other intuitive connection strategies that also connects each layer $i$ to  $1 + \log(i)$ previous layers. The first strategy, called \textbf{\nearest} connects layer $i$ to its previous $\log(i)$ depths, i.e., 
\mbox{$x_i = f_i(\concat(
    \{ x_{i - k} : k = 1, ..., \lfloor \log_b( i) \rfloor  \}
    )\;; \;\theta_i).
$}
The second strategy, called \textbf{\evenspace} connects layer $i$ to $log(i)$ previous depths that are evenly spaced; i.e., 
\mbox{$x_i = f_i(\concat(
    \{ x_{ \lfloor i - 1 - k \delta \rceil} : 
    \delta = \frac{i}{log(i)} \text{ and }
    k =0,1,2,... \text{ and } k\delta \leq i-1 \}
    )\;; \;\theta_i).
$}
Both methods above are intuitive. However, each of them has a \pbd that is on the order of $O(\frac{L}{log(L)})$, which is much higher than the $O(log(L))$ \pbd of the proposed \logdense V1. 
We experiment with networks whose $(n,g)$ are in $\{12, 32, 52\} \times \{16, 24, 32\}$, and show in Table~\ref{tab:dsm_maintext} that \logdense almost always outperforms the other two strategies. Furthermore, the average relative increase of top-1 error rate using \nearest and \evenspace from using \logdense is $12.2\%$ and $8.5\%$, which is significant: for instance, (52,32) achieves 23.10\% error rate using \evenspace, which is about 10\% relatively worse than the 20.58\% from (52,32) using \logdense, but (52,16) using \logdense already has 23.45\% error rate using a quarter of the computation of (52,32). 

We also showcase the advantage of small \pbd when each layer $x_i$ is connects to $\approx \frac{i}{2}$ number of previous layers. With this many connections, \nearest has a \pbd of $\log L$, because we can halve $i$ (assuming $i > j$) until $j > i /2$ so that $i$ and $j$ are directly connected. \evenspace has a \pbd of $2$, because each $x_i$ takes input from every other previous layer.  Table~\ref{tab:dsm678_maintext} shows that \evenspace significantly outperform \nearest on CIFAR10 and CIFAR100.  We also show that \nearest is not under-performing simply because connecting to the most recent layers are ineffective. Starting with the \nearest scheme, we make $x_i$ also take input from $x_{\round{i/4}}, x_{\round{i/8}}, x_{\round{i/16}},...$. We call this scheme NearestHalfAndLog, and it has a \pbd of $2$, because any $j < i$ is either directly connected to $i$, if $j > i / 2$, or $j$ is connected to some $i / \round{i / 2^k}$ for some $k$, which is connected to $i$ directly. 
Fig.~\ref{fig:nearest_half_and_log} illustrates the connections of this scheme.
We observe in Table~\ref{tab:dsm678_maintext} that with this few $\log_i -1$ additional connections to the existing $\lceil i / 2 \rceil$ ones, we drastically reduce the error rates to the level of \evenspace, which has the same \pbd of 2. 
These comparisons support our design principle: with the same number of connections at each depth $i$, the connection strategies with low \pbd outperform the ones with high \pbd.


%%%%%%%%%%%%%%%%%%%%%%%%%%%% Scene parsing %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\logdense for Training Semantic Segmentation from Scratch}
\label{sec:exp-fcdense}
\input{table_fclogdense}
% Background of what scene parsing is, and what the difficulty it has. 
Semantic segmentation assigns every pixel of input images with a label class, and it is an important step for understanding image scenes for robotics such as autonomous driving. The state-of-the-art training procedure~\citep{pspnet, deeplab} typically requires training a fully-convolutional network (FCN)~\citep{fcn} and starting with a recognition network that is trained on large data-sets such as ILSVRC or COCO, because training FCNs from scratch is prone to overfitting and is difficult to converge.
\cite{fcdense} shows that DenseNets are promising for enabling FCNs to be trained from scratch. In fact, fully convolutional DenseNets (FC-DenseNets) are shown to be able to achieve the state-of-the-art predictions training from scratch without additional data on CamVid \citep{camvid-dataset} and GATech \citep{gatech-dataset}. However, the drawbacks of DenseNet are already manifested in applications on even relatively small images (360x480 resolution from CamVid). In particular, to fit FC-DenseNet into memory and to run it in reasonable speed, \cite{fcdense} proposes to cut many mid-connections: during upsampling, each layer is only directly connected to layers in its current block and its immediately previous block. Such connection strategy is similar to the \nearest strategy in Sec.~\ref{sec:exp-low-backprop-distance}, which has already been shown to be less effective than the proposed \logdense in classification tasks. We now experimentally show that fully-convolutional \logdense (FC-\logdense) outperforms FC-DenseNet.

% Our design of FC-\logdense
\textbf{FC-\logdense 103.} Following \citep{fcdense}, we form FC-\logdense V1-103 with 11 \logdense V1 blocks, where the number of feature layers in the blocks are $4, 5, 7, 10, 12, 15, 12, 10, 7, 5, 4$. After each of the first five blocks, there is a transition that transforms and downsamples previous layers independently. After each of the next five blocks, there is a transition that applies a transposed convolution to upsample each previous layer. Both down and up sampling are only done when needed, so that if a layer is not used directly in the future, no transition is applied to it. Each feature layer takes input using the \logdense connection strategy. Since \logdense connections are sparse to early layers, which contain important high resolution features for high resolution semantic segmentation, we add feature layer $x_4$, which is the last layer of the first block, to the input set of all subsequent layers. This adds only one extra connection for each layer after the first block, so the overall complexity remains roughly the same. 
We do not form any other skip connections, since \logdense  already provides sparse connections to past layers. 
 
% Well techniquely we connect to the scaled version of the last layer of the first dense-block.
\textbf{Training details.} Our training procedure and parameters follow  from those of FC-DenseNet~\citep{fcdense}, except that we set the growth rate to 24 instead of 16, in order to have around the same computational cost as FC-DenseNet. 
We defer the details to the appendix. However, we also found auxiliary predictions at the end of each dense block reduce overfitting and produce interesting progression of the predictions, as shown in Fig.~\ref{fig:scene_parsing}. Specifically, these auxiliary predictions produces semantic segmentation at the scale of their features using 1x1 conv layers. The inputs of the predictions and the weighting of the losses are the same as in classification, as specified in Sec.~\ref{sec:logdense}.


\textbf{Performance analysis.}
We note that the final two blocks of FC-DenseNet and FC-\logdense cost half of their total computation. This is because the final blocks have fine resolutions, which also make the full DenseNet connection in the final two blocks prohibitively expensive. This is also why FC-DenseNets~\citep{fcdense} have to forgo all the mid-depth the shortcut connections in its upsampling blocks. 
Table~\ref{tab:camvid_ious} lists the Intersection-over-Union ratios (IoUs) of the scene parsing results.
FC-\logdense achieves 67.3\% mean IoUs, which is slightly higher than the 66.9\% of FC-DenseNet. Among the 11 classes, FC-\logdense performs similarly to FC-DenseNet. Hence FC-\logdense achieves the same level of performance as FC-DenseNet with 50\% fewer parameters and similar computations in FLOPS. This supports our hypothesis that we should minimize \pbd when we have can only have a limited number of skip connections. FC-\logdense can potentially be improved if we reuse the shortcut connections in the final block to reduce the number of upsamplings.

\begin{figure}
    \centering
    \subfloat{
    \includegraphics[width=\linewidth]{camvid_cmap.png}
    }
    
    \subfloat{
    \includegraphics[width=\linewidth]{img_0.png}
    }
    
    \subfloat{
    \includegraphics[width=\linewidth]{img_120.png}
    }
    


    \caption{Each row: input image, ground truth labeling, and any scene parsing results at 1/4, 1/2, 3/4 and the final layer. The prediction at 1/2 is blurred, because it and its feature are at a low resolution.}
    \label{fig:scene_parsing}
    \vspace{-5pt}
\end{figure}



%%%%%%%%%%%%% Trade-off between computation and accuracy dense vs logdense vs resnet %%%%%%

\subsection{Computational Efficiency of Sparse and Dense Networks}
\label{sec:exp-trade-off}
\vspace{-5pt}

This section studies the trade-off between computational cost and the accuracy of networks on visual recognition. In particular, we address the question of whether sparser networks like \logdense perform better than DenseNet using the same computation. DenseNets can be very deep for image classification, because they have low resolution in the final block. In particular, a skip connection to the final block costs $1/64$ of one to the first block. 
Fig.~\ref{fig:flops_cifar100} illustrates the error rates on CIFAR100 of \logdense V1 and V2 and DenseNet. The \logdense variants have $g=32$, and $n=12,22,32,...,82$. DenseNets have $g=32$, and $n=12,22,32,42$. \logdense V2 has around the same performance as DenseNet on CIFAR100. This is partially explained by the fact that most pairs of $x_i, x_j$ in \logdense V2 are cross-block, so that they have the same \pbd as in Densenets thanks to the compressed early blocks. The within block distance is bounded by the logarithm of the block size, which is smaller than 7 here.
\logdense V1 has similar error rates as the other two, but is slightly worse, an expected result, because unlike V2, backpropagation distances between a pair $x_i,x_j$ in V1 is always $ \log |i -j|$, so on average V1 has a higher \pbd than V2 does. 
The performance gap between \logdense V1 and DenseNet also gradually widens with the depth of the network, possibly because the \pbd of \logdense has a logarithmic growth. We observe similar effects on CIFAR10 and SVHN, whose performance versus computational cost plots are deferred to the appendix.
These comparisons suggest that to reach the same accuracy, the sparse \logdense costs about the same computation as the DenseNet, but is capable of scaling to much higher depths. 
We also note that using \naive implementations, and a fixed batch size of 16 per GPU, DenseNets (52, 24) already have difficulties fitting in the 11GB RAM, but \logdense can fit models with $n>100$ with the same $g$. We defer the plots for number of parameters versus error rates to the appendix as they look almost the same as plots for FLOPS versus error rates.


On the more challenging ILSVRC2012~\citep{ilsvrc}, we observe that \logdense V2 can achieve comparable error rates to DenseNet. Specifically, \logdense V2 is more computationally efficient than ResNet~\citep{resnet} that do not use bottlenecks (ResNet18 and ResNet34): \logdense V2 can achieve lower prediction errors with the same computational cost. However, \logdense V2 is not as computationally efficient as ResNet with bottlenecks (ResNet 50 and ResNet101), or DenseNet. This implies there may be a trade-off between the shortcut connection density and the computation efficiency. For problems where shallow networks with dense connections can learn good predictors, there may be no need to scale to very deep networks with sparse connections. However, the proposed \logdense  provides a reasonable trade-off between  accuracy and scalability for tasks that require deep networks, as in Sec.~\ref{sec:exp-fcdense}. 

%\textcolor{red}{what is 369; what is ilsvrc; how we train ilsvrc}

%TODO \textcolor{red}{Mention: DenseNet suffers less here because you keep shrinking everything, so $L^2$ is not that bad.}

%\textcolor{red}{Hallucinations} 
%We mentioned that the step (b) of \verb=lglg_conn= in Sec.~\ref{sec:loglogdense} ensures the \pbd of \loglogdense to be $1 + \log \log L$ instead of $2\log \log L$. We verify that \loglogdense without the step (b), called 2LogLog, has a  worse cost-efficiency. Recall from Sec.~\ref{sec:loglogdense} that we use \logdense connections to ensure \loglogdense layers to have at least four input layers each if possible, so that the FLOPS cost of the two \loglogdense versions are around the same, because most layers have less than four input layers before the \logdense augmentation. In Fig.~\ref{fig:flops_loglog2}, we observe that the proposed \loglogdense is able to achieve lower errors with the same computation, because its \pbd is about half of that of a 2LogLog. 




\begin{figure}[t]
    \centering
    \subfloat[CIFAR100 Error versus FLOPS]{
        \includegraphics[width=0.45\textwidth]{logdense_flops_cifar100.png}
        \label{fig:flops_cifar100}
    }
    ~
    \subfloat[ILSVRC Error versus FLOPS]{
        \includegraphics[width=0.45\textwidth]{logdense_ilsvrc_err_vs_flops.png}
        \label{fig:flops_ilsvrc}
    }
    \caption{\textbf{(a)} Using the same FLOPS, \logdense V2 achieves about the same prediction accuracy as DenseNets on CIFAR100. The DenseNets have block compression and are trained with drop-outs. \textbf{(b)} On ILSVRC2012, \logdense169, 265 have the same block sizes as DenseNets169, 265. \logdensenp369 has block sizes $8,16, 80, 80$. }
    \vspace{-10pt}
\end{figure}




%%%%%%%%%%%% CONCLUSION and discussion %%%%%%%%%%
\vspace{-5pt}
\section{Conclusions and Discussions}
\vspace{-5pt}
\label{sec:conclusion}
We show that short backpropagation distances are important for networks that have shortcut connections: if each layer has a fixed number of shortcut inputs, they should be placed to minimize \pbd. Based on this principle, we design \logdense, which uses $O(L \log L)$ total shortcut connections on a depth-$L$ network to achieve $1 + \log L$ \pbd. We show that \logdenses improve the performance and scalability of \textit{tabula rasa} fully convolutional DenseNets on CamVid. \logdenses also achieve competitive results in visual recognition data-sets, offering a trade-off between accuracy and network depth. Our work provides insights for future network designs, especially those that cannot afford full dense shortcut connections and need high depths, like FCNs.




\bibliography{ann}
\bibliographystyle{iclr2018_conference}

\newpage
\appendix


\textbf{Appendix}

\input{loglog.tex}
\input{proof.tex}


\section{Additional Experimental Results}

\subsection{CamVid Training Details}
We follow \cite{fcdense} to optimize the network using 224x224 random cropped images with RMSprop. The learning rate is 1e-3 with a decay rate 0.995 for 700 epochs. We then fine-tune on full images with a learning rate of 5e-4 with the same decay for 300 epochs.  The batch size is set to 6 during training and 2 during fine-tuning. We train on two GTX 1080 GPUs. 
We use no pre-processing of the data, except left-right random flipping. Following \cite{segnet}, we use the median class weighting to balance the weights of classes, i.e., the weight of each class $C$ is the median of the class probabilities divided by the over the probability of $C$. 





\subsection{Computational Efficiency on CIFAR10 and SVHN}

Fig.~\ref{fig:flops_cifar10} and Fig.~\ref{fig:flops_svhn} illustrate the trade-off between computation and accuracy of \logdense and DenseNets on CIFAR10 and SVHN. \logdenses V2 and DenseNets have similar performances on these data-sets: on CIFAR10, the error rate difference at each budget is less than 0.2\% out of 3.6\% total error; on SVHN, the error rate difference is less than 0.05\% out of 1.5\%. Hence, in both cases, the error rates between \logdense V2 and DenseNets are around 5\%. 

\begin{figure}[t]
    \centering
    \subfloat[CIFAR10 Error versus FLOPS]{
        \includegraphics[width=0.42\textwidth]{logdense_flops_cifar10.png}
        \label{fig:flops_cifar10}
    }
    ~
    \subfloat[SVHN Error versus FLOPS]{
        \includegraphics[width=0.42\textwidth]{logdense_flops_svhn.png}
        \label{fig:flops_svhn}
    }
    \caption{On CIFAR10 and SVHN, \logdense V2 and DenseNets have very close error rates ($<5\%$ relatively difference) at each budget.}
\end{figure}


\subsection{Number of Parameter  versus Error Rates. }
Figure~\ref{fig:params_all_data} plots the number of parameters used by \logdense V2, DenseNet, and ResNet versus the error rates on the image classification data-sets, CIFAR10, CIFAR100, SVHN, ILSVRC. We assume that DenseNet and \logdense use \naive implementations. 

\begin{figure}
    \centering
    \subfloat[CIFAR10 Number of Parameters versus Error Rates]{
        \includegraphics[width=0.42\textwidth]{logdense_params_cifar10.png}
        \label{fig:params_cifar10}
    }
    ~
    \subfloat[CIFAR100 Number of Parameters versus Error Rates]{
        \includegraphics[width=0.42\textwidth]{logdense_params_cifar100.png}
        \label{fig:params_cifar100}
    }
    
    \subfloat[SVHN Number of Parameters versus Error Rates]{
        \includegraphics[width=0.42\textwidth]{logdense_params_svhn.png}
        \label{fig:params_svhn}
    }
    ~
    \subfloat[ILSVRC Number of Parameters versus Error Rates]{
        \includegraphics[width=0.42\textwidth]{logdense_ilsvrc_err_vs_params.png}
        \label{fig:params_ilsvrc}
    }
    \caption{The number of parameter used in the \naive implementation versus the error rates on various data-sets.}
    \label{fig:params_all_data}
\end{figure}


\subsection{\loglogdense Experiments and More Principles than \pbd}
This section experiments with \loglogdense and show that there are more that just \pbd that affects the performance of networks. Ideally, since \loglogdense have very small \pbd, its performance should be very close to DenseNet, if \pbd is the sole decider of the performance of networks. However, we observe in Fig.~\ref{fig:loglog_llm1} that \loglogdense is not only much worse than \logdense and DenseNet in terms accuracy at each given computational cost (in FLOPS), it is also widening the performance gap to the extent that the test error rate actually increases with the depth of the network. This suggests there are more factors at play than just \pbd, and in deep \loglogdense, these factors inhibit the networks from converging well. 

One key difference between \loglogdense's connection pattern to \logdense's is that the layers are not symmetric, in the sense that layers have drastically different shortcut connection inputs. In particular, while the average input connections per layer is five (as shown in Fig.~\ref{fig:loglog_avg_connections}), some nodes, such as the nodes that are multiples of $L^{\frac{1}{2}}$, have very large in-degrees and out-degrees (i.e., the number of input and output connections). These nodes are given the same number of channels as any other nodes, which means there must be some information loss passing through such ``hub" layers, which we define as layers that are densely connected on the depth zero of \verb=lglg_conn= call. 
Hence a natural remedy is to increase the channel size of the hub nodes. In fact, Fig.~\ref{fig:loglog_llm3} shows that by giving the hub layers three times as many channels, we greatly improve the performance of \loglogdense to the level of \logdense. This experiment also suggests that the layers in networks with shortcut connections should ensure that high degree layers have enough capacity (channels) to support the amount of information passing. 


\begin{figure}
    \centering
    \subfloat[\loglogdense Hub Multiplier=1]{
    \includegraphics[width=0.42\linewidth]{loglog_flops_vs_perfs_cifar100_llm1.png}
    \label{fig:loglog_llm1}
    }
    ~
    \subfloat[\loglogdense Hub Multiplier=3]{
    \includegraphics[width=0.42\linewidth]{loglog_flops_vs_perfs_cifar100_llm3.png}
    \label{fig:loglog_llm3}
    }

\caption{Performance of \loglogdense (red) with different hub multiplier (1 and 3). Larger hubs allow more information to be passed by the hub layers, so the predictions are more accurate.}
\end{figure}


%\subsection{Deep supervision in \logdense V1} 
%We train \logdense V1 without deep supervision, i.e., the only loss function is at the final layer from the final prediction. We found this on nine model configurations and three data-sets as mentioned in Sec.~\ref{sec:exp-low-backprop-distance}, that deep supervision has better performance on 21 out of the 27 settings; on average in these 27 settings, deep supervision leads to a 3.51\% relative reduction in top-1 error rates. Hence we apply deep supervision in our experiments. The locations of deep supervisions are evenly spaced in the layers, and we have eight outputs including the final prediction.


%\subsection{Table for comparing different connection strategies} 
%\input{table_logdense_wrapper.tex}
%Table~\ref{tab:table_dsm_full} list the top-1 error rates of three connection strategies on the 27 settings (nine model settings and three data-sets). We also display the performance at 1/4, 1/2, 3/4 of the FLOPS in additional to the final performance from the deep supervision for reference. 



\subsection{Additional Semantic Segmentation Results}
\label{sec:additional_scene_parsing}
We show additional semantic segmentation results in Figure~\ref{fig:scene_parsing_appendix}. We also note in Figure~\ref{fig:fc_computation} how the computation is distributed through the 11 blocks in FC-DenseNets and FC-\logdenses. In particular, more than half of the computation is from the final two blocks because the final blocks have high resolutions, making them exponentially more expensive than layers in the mid depths and final layers of image classification networks. 


\begin{figure}
    \centering
    \subfloat{
    \includegraphics[width=\linewidth]{camvid_cmap.png}
    }
    
    \subfloat{
    \includegraphics[width=\linewidth]{img_0.png}
    }
    
    \subfloat{
    \includegraphics[width=\linewidth]{img_30.png}
    }
    
    \subfloat{
    \includegraphics[width=\linewidth]{img_30.png}
    }
    
    \subfloat{
    \includegraphics[width=\linewidth]{img_60.png}
    }
    
    \subfloat{
    \includegraphics[width=\linewidth]{img_90.png}
    }
    
    \subfloat{
    \includegraphics[width=\linewidth]{img_120.png}
    }
    
    \subfloat{
    \includegraphics[width=\linewidth]{img_150.png}
    }
    
    \subfloat{
    \includegraphics[width=\linewidth]{img_180.png}
    }
    
    \subfloat{
    \includegraphics[width=\linewidth]{img_210.png}
    }

    \caption{Each row: input image, ground truth labeling, and any scene parsing results at 1/4, 1/2, 3/4 and the final layer.}
    \label{fig:scene_parsing_appendix}
\end{figure}


\end{document}
