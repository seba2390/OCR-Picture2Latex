
\section{\loglogdense}
\label{sec:loglogdense}


\begin{figure}
    \centering
    \subfloat[Illustration of lglg\_conn recursion]{
        \includegraphics[width=0.45\textwidth]{loglog_recursion.png}
    }
    ~
    \subfloat[lglg\_conn($0,L$)]{
        \includegraphics[width=0.22\textwidth]{LogLog_connections.png}
    }
    ~
    \subfloat[\loglogdense]{
        \includegraphics[width=0.22\textwidth]{LogLog+_connections.png}
    }
    \cprotect\caption{\textbf{(a)}The tree of recursive calls of \verb=lglg_conn=. \textbf{(b)} \loglogdense augments each $x_i$ of \verb=lglg_conn=($0,L$) with \logdense connections until $x_i$ has at least four inputs.
    }
    \label{fig:loglog_recursion}
\end{figure}
Following the short \pbd design principle, we further propose \loglogdense, which uses $O(L\log \log L)$ connections to achieve $1+\log\log L$ \pbd. 
For the clarity of construction, we assume there is a single block for now, i.e., $n_{block}=1$.
We add connections in \loglogdense recursively, after we initialize each depth $i=1,...,L$ to take input from $i-1$, where layer $i=0$ is the initial convolution that transform the image to a $2g$-channel feature map. Fig.~\ref{fig:loglog_recursion} illustrates the recursive calls. Formally, the recursive connection-adding function is called \verb=lglg_conn=$(s,t)$, where the inputs $s$ and $t$ represent the start and the end indices of a segment of contiguous layers in $0,...,L$. For instance, the root of the recursive call is \verb=lglg_conn=($0,L$), where $(0,L)$ represents the segment of all the layers $0,...,L$.
\verb=lglg_conn=$(s,t)$ exits immediately if $t-s \leq 1$. 
If otherwise, we let $\delta = \lfloor \sqrt{t-s+1} \rfloor$, $K=\{s \} \cup \{ t- k \delta : t - k \delta  \geq s \text{ and }  k =0,1,2,..., \}$, and let $a_1,...,a_{|K|}$ be the sorted elements of $K$.
\textbf{(a)} Then we add dense connections among layers whose indices are in $K$, i.e., if $i, j \in K$ and $i > j$, then we add $x_j$ to the input set of $x_i$. \textbf{(b)} Next for each $k=1,..., |K|-1$, we add $a_{k}$ to the input set of $x_j$ for each $j=a_{k}+1,..., a_{k+1}$. \textbf{(c)} Finally, we form $|K|-1$ number of recursive \verb=lglg_conn= calls, whose inputs are
$(s_k, s_{k+1})$ for each $k=1,...,|K|-1$. 

If $n_{block} > 1$, we reuse \logdense V1 transition to scale each layer independently, so that when we add $x_j$ to the input set of $x_i$, the appropriately scaled version of $x_j$ is used instead of the original $x_j$. 
We defer to the appendix the formal analysis on the recursion tree rooting at \verb=lglg_conn=($0,L$), which forms the connection in \loglogdense, and summarize the result as follows.
\begin{proposition}
\label{them:loglog-dense-loglog-dist}
\loglogdense of $L$ feature layers has at most $1.5L \log\log L + o(L \log\log L)$ connections, and a \pbd at most \mbox{ $ \log\log L + n_{block}$ +1 }.
\end{proposition}

Hence, if we ignore the independent transitions and think them as part of each $x_i$ computation, the \pbd between any two layers $x_i, x_j$ in \loglogdense is at most $2 + \log\log L$, which effectively equals 5, because  $\log\log L < 3.5$ for $L < 2545$. Furthermore, such short \pbd is very cheap: on average, each layer takes input from 3 to 4 layers for $L < 1700$, which we verify in~\ref{sec:loglogdense_proof}. We also note that without step (b) in the \verb=lglg_conn=, the \pbd is $2+ 2\log \log L$ instead of $2 + \log \log L$.


\textbf{Bottlenecks.} 
In DenseNet, since each layer takes input from the concatenation of all previous layers, it is necessary to have bottleneck structures \citep{resnet, densenet}, which uses 1x1 conv to shrink the channel size to $4g$ first, before using 3x3 conv to generate the $g$ channel of features in each $x_i$. In \logdense and \loglogdense, however, the number of input layers is so small that bottlenecks actually increase the computation, e.g., most of \loglogdense layers do not even have $4g$ input channels. However, we found that bottlenecks are cost effective for increasing the network depth and accuracy. Hence, to reduce the variation of structures, we use bottlenecks and fix the bottleneck width to be $4g$. For \loglogdense, we also add \logdense connections from nearest to farthest for each $x_i$ until either $x_i$ has four inputs, or there are no available layers. For $L<1700$, this increases average input sizes only to $4.5 \sim 5$, which we detail in the next section. 
