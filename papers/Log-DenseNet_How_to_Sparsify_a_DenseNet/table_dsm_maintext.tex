
\begin{table}
    \centering
    \resizebox{0.75\textwidth}{!}{
    \begin{tabular}{c|ccc| ccc| ccc}
   & \multicolumn{3}{c|}{CIFAR10} & \multicolumn{3}{c|}{CIFAR100}  & \multicolumn{3}{c}{SVHN}\\
\hline
 (n,g)   & L & N & E & L & N & E & L & N & E \\
\hline
(12,16)
	&  \textbf{7.23} &  7.59 &  7.45 & \textbf{29.14} & 30.59 & 30.72 &  \textbf{2.03} &  2.11 &  2.27 \\
(12,24)
	&  \textbf{5.98} &  6.46 &  6.56 & \textbf{26.36} & 26.96 & 27.80 &  \textbf{1.94} &  2.10 &  2.05 \\
(12,32)
	&  \textbf{5.48} &  6.00 &  6.15 & \textbf{24.21} & 24.70 & 25.57 &  \textbf{1.85} &  1.92 &  1.90 \\
\hline
(32,16)
	&  \textbf{5.96} &  6.45 &  6.21 & \textbf{25.32} & 27.48 & 26.81 &  1.97 &  \textbf{1.94} &  1.96 \\
(32,24)
	&  \textbf{5.03} &  5.74 &  5.43 & \textbf{22.73} & 25.08 & 24.80 &  \textbf{1.77} &  1.82 &  1.95 \\
(32,32)
	&  \textbf{4.81} &  5.65 &  4.94 & \textbf{21.77} & 23.79 & 23.87 &  \textbf{1.76} &  1.82 &  1.95 \\
\hline
(52,16)
	&  \textbf{5.13} &  6.80 &  6.09 & \textbf{23.45} & 27.99 & 26.58 &  \textbf{1.66} &  1.98 &  1.85 \\
(52,24)
	&  \textbf{4.34} &  5.83 &  5.03 & \textbf{20.99} & 26.07 & 24.19 &  \textbf{1.64} &  1.90 &  1.80 \\
(52,32)
	&  \textbf{4.56} &  6.10 &  4.98 & \textbf{20.58} & 24.79 & 23.10 &  \textbf{1.72} &  1.89 &  1.78 \\
\hline
    \end{tabular}}
    \caption{Error rates of \logdense V1(L), \nearest(N) and \evenspace(E), in each of which layer $x_i$ has $\log i$ 
    previous layers as input. (L) has a \pbd of $1+\log L$, and the other two have $\frac{L}{\log L}$.
    (L) outperforms the other two clearly. These networks do not have bottlenecks. }
    \label{tab:dsm_maintext}
\vspace{-8pt}
\end{table}


\begin{table}
\centering

    \subfloat[Error Rates with $i/2$ shotcuts to $x_i$]{
    \resizebox{0.6\textwidth}{!}{
    \begin{tabular}{c|ccc | ccc}
& \multicolumn{3}{c|}{CIFAR10} & \multicolumn{3}{c}{CIFAR100}  \\
\hline
 (n,g)   & N & E & N+L & N & E & N+L \\
\hline
(12,16)
	&  9.45 &  6.42 &  \textbf{5.77} & 35.97 & 29.65 & \textbf{25.49} \\
(12,24)
	&  6.49 &  5.18 &  \textbf{5.12} & 29.11 & 24.61 & \textbf{22.87} \\
(12,32)
	&  5.01 &  4.84 &  \textbf{4.70} & 25.04 & 23.70 & \textbf{21.96} \\
\hline
(32,12)
	&  7.16 &  4.90 &  \textbf{4.80} & 33.64 & 24.03 & \textbf{22.70} \\
(32,24)
	&  4.69 &  \textbf{4.16} &  4.36 & 24.58 & \textbf{21.00} & 21.27   \\
(32,32)
	&  4.30 &  4.24 &  \textbf{4.03} & 22.84 & \textbf{21.28} & 21.72  \\
\hline
(52,16)
	&  5.68 &  4.72 &  \textbf{4.34} & 28.44 & 21.73 & \textbf{20.68} \\
%(52,24)
%	&  4.41 &   -   &   -   &   -   &   -   &   -   \\
%(52,32)
%	&   -   &  3.95 &  4.18 &   -   & 20.66 &   -   \\
\hline 
    \end{tabular}}}
     ~
    \subfloat[NearestHalfAndLog]{\raisebox{-47pt}{
        \includegraphics[width=0.22\textwidth]{NearestHalfAndLog_connections.png}}
        \label{fig:nearest_half_and_log}
    }
    \caption{\textbf{(a)} \nearest (N), \evenspace (E), and NearestHalfAndLog (N+L) each connects to about $i/2$ previous layers at $x_i$, and have \pbd $\log L$, $2$ and $2$. N+L and E clearly outperform N. 
    \textbf{(b)} Connection illustration of N+L: each layer $i$ receives $\frac{i}{2} + \log(i)$ shortcut connections.}
    \label{tab:dsm678_maintext}

\vspace{-8pt}
\end{table}
