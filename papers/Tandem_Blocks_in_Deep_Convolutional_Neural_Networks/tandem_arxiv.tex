\documentclass{article} % For LaTeX2e
\usepackage{arxiv,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath, amsthm, amssymb, mathtools, tikz, pgfplots, caption, subcaption, graphicx}
\usepackage{placeins}
\usetikzlibrary{arrows.meta}

\renewcommand{\labelenumi}{\textnormal{(}{\roman{enumi}\textnormal{)}}}

%\title{Linear Connections in Small Convolutional Neural Networks}
\title{Tandem Blocks in Deep Convolutional\\ Neural Networks}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Chris Hettinger, Tanner Christensen, Jeffrey Humpherys, \& Tyler J. Jarvis \\
Department of Mathematics\\
Brigham Young University\\
Provo, UT 84602, USA \\
\texttt{hettinger@math.byu.edu, tkchristensen@byu.edu,} \\ 
\texttt{jeffh@math.byu.edu, jarvis@math.byu.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\todo}{\marginpar{TODO}} 

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
Due to the success of residual networks (resnets) and related architectures, shortcut connections have quickly become standard tools for building convolutional neural networks. The explanations in the literature for the apparent effectiveness of shortcuts are varied and often contradictory.  We hypothesize that shortcuts work primarily because they act as linear counterparts to nonlinear layers.  We test this hypothesis by using several variations on the standard residual block, with different types of linear connections, to build small (100k--1.2M parameter) image classification networks. Our experiments show that other kinds of linear connections can be even more effective than the identity shortcuts. Our results also suggest that the best type of linear connection for a given application may depend on both network width and depth.
\end{abstract}

\section{Introduction}
\label{introduction}
%Due to the success of residual networks and related architectures, shortcut connections (a.k.a. skip-layer connections) have recently become standard tools for building convolutional neural networks. 
%\subsection{Residual Learning}

Deep convolutional neural networks have become the dominant force for many image classification tasks; see \cite{CNN,VeryDeep,Inception}.  Their ability to assimilate low-, medium-, and high-level features in an end-to-end multi-layer fashion has led to myriad groundbreaking advances in the field.  In recent years, residual networks (resnets) have emerged as one of the best performing neural network archetypes in the literature; see \cite{ResNets}.  Through the use of identity shortcut connections, resnets have overcome the challenging technical obstacles of vanishing gradients and the apparent degradation that otherwise comes with training very deep networks. Resnets have achieved state-of-the-art performance on several image classification datasets using very deep neural networks, sometimes with over 1000 layers.

Although shortcut connections appeared in the early neural network literature, e.g., \cite{Bishop,Ripley,Centering}, their importance became more clear in 2015 with the emergence of the HighwayNets of \cite{HighwayNets} and resnets.  The former involved gated shortcut connections that regulate the flow of information across the network, while the latter used identity shortcut connections, which are parameterless.  Resnets are also presumed to be easier to train and seem to perform better in practice. 
In their first resnet paper, He et al.~argued that identity maps let gradients flow back, enabling the training of very deep networks, and that it's easier for a layer to learn when initialized near an identity map than near a zero map (with small random weights); see also \cite{IdentityMappings}. %from a starting point of keeping things the same (the identity map) than from the zero map

However, in a flurry of recent activity, most notably from \cite{Wideresnets, UnrolledIterativeEstimation, EnsemblesofRelativelyShallow, Demystifying} and \cite {WiderOrDeeper}, arguments have emerged that the effectiveness of resnets is not due to their depth, where practitioners were training networks of hundreds or thousands of layers, but rather that deep resnets are effectively creating ensembles of shallower networks, and the layers are more likely to refine and reinforce existing features than engineer new ones.  These arguments assert that the achievement of resnets is less about extreme depth and more about their ability to ease backpropagation with moderate depth.  Indeed, in many cases wider residual networks that were only 10--50 layers deep were shown to perform better and train in less time than very deep ones (over 100 layers). See \cite{Wideresnets}.

More recently still, others have presented many clever and creative ways to train very deep networks using variations on the shortcut theme; see for example \cite{DenselyConnected, FractalNet, Multilevel, Pyramidal, Multiresidual, DualPath, Polynet, ResNets, WideResidualInception, Aggregated, ResidualGates, Inception,RethinkingInception}, and \cite{InceptionResNet}.   
In summary, shortcut connections clearly help in practice, but there are many different, and sometimes conflicting hypotheses as to why.

In this paper we investigate a new hypothesis about shortcut connections, namely, that their power lies not in the identity mapping itself, but rather just in combining linear and nonlinear functions at each layer.   The tests where identity shortcuts were observed to perform better than general linear connections were all done in very deep (100 or more layers) networks. The recent evidence that wider, shallower, resnet networks can outperform deeper ones suggests that it is worth investigating whether identity connections are better than general linear connections in such networks. 

We first describe some of the intuition about why this might be the case.  We then investigate this idea with careful experiments using relatively small networks constructed of five different types of  blocks.  These blocks are all variations on the idea of residual blocks (resblocks), but where the identity shortcut is replaced with a more general linear function. We call these blocks, consisting of both a linear and a nonlinear part, \emph{tandem blocks} and the resulting networks \emph{tandem networks}. Residual networks and several similar architectures are special cases of tandem networks.

The networks we use in our experiments are relatively small  (100k--1.2M parameter) image classification networks constructed from these various tandem blocks.  The small networks are appropriate because the goal of the experiments is not to challenge state-of-the-art results produced by much larger models, but rather to compare the five architectures in a variety of settings in order to gain insight into their relative strengths and weaknesses. Whereas many other authors pursue extreme network depth as a goal in itself, here we limit our focus to comparing performance (in this case, classification accuracy) of different architectures. 

Our experiments suggest that general linear layers, which have learnable parameters, perform at least as well as the identity shortcut of resnets. This is true even when some width is sacrificed to keep the total number of parameters the same. Our results further suggest that the best specific type of linear connection to use in the blocks of a tandem network depends on several factors, including both network width and depth.

% TODO: Results section
% TODO: captions
% TODO: tandem description
% TODO: Add Xie stuff
% TODO: captions and section 2, B_{id} = resnet
% ADAM

\section{Tandem Blocks}
\label{linear}

The basic building block we use is the \emph{tandem block}, which consists of a linear and a nonlinear part in parallel (see Figure \ref{fig:blocks}). In the case of resnet blocks, the linear part is simply an identity shortcut. But a tandem block generalizes this to allow any linear map. The outputs of the two parts are summed and then passed to subsequent blocks.  

Note that, unlike in the case of the original resnet paper, we do not pass the resulting sum to another nonlinear activation function.  This is because recent work of \cite{EraseRelu} has shown that removing the activation function after the sum in resnets improves performance.  

%\begin{figure}[htb]
%\centering
%\resizebox{3cm}{!}{
%\begin{tikzpicture}
%\tikzset{>=Latex}
%\node[rectangle,draw,align=center,font=\Large] (rc1) at ( 2,-3.5)   {Nonlinear\\activation};
%\node[rectangle,draw,align=center,font=\Large] (lc1) at (-2,-3.5) {Linear\\function};
%\node[rectangle,draw,align=center,font=\Large] (add) at ( 0,-6)   {Add};
%\draw[thick,- ] (0,0)  -- (0,-1);
%\draw[thick,->] (0,-1) -- (2,-2)  -- (rc1);
%\draw[thick,->] (rc1)  -- (2,-5) -- (add);
%\draw[thick,->] (0,-1) -- (-2,-2) -- (lc1);
%\draw[thick,->] (lc1)  -- (-2,-5) -- (add);
%\draw[thick,->] (add)  -- (0,-7);
%\end{tikzpicture}}
%\caption{Illustrating the basic idea of a tandem block, replacing the identity in resnets with a linear transformation, and adding the two activation functions together.  Note that, unlike \cite{ResNets}, we do not use an activation function after the final sum; this is motivated by the results of \cite{EraseRelu}.}
%\label{fig:tandemBlock}
%\end{figure}

%Of course, in theory any results achievable with shortcut connections can also be achieved without them provided they are given suitable choice of weights, but in practice, networks without shortcut connections do not usually seem to be very good at learning those weights. \cite{AllYouNeed} showed that in some cases these weights can be learned, and shortcuts may not be needed at all, if things are set up carefully with normally-connected convolutions.  But this requires very careful design and is highly sensitive to initialization.

Many authors assert that identity shortcuts are superior to other linear maps in such a configuration. See \cite{IdentityMappings,InceptionResNet,ResNets, ResidualGates}, and \cite{Demystifying}.  The reasons given for this assertion vary and are usually heuristic in nature. 
We chose the architectures we did in order to test whether the following properties of identity shortcuts were important: having no parameters, maintaining feature size (as opposed to creating higher-level features with $3 \times 3$ filters), and bypassing multiple nonlinear convolutional layers.  


%There are at least three reasons to hypothesize that network blocks with both a linear and a nonlinear component running parallel to each other (like resblocks), should perform well in deep networks.  
%\begin{enumerate}
%\item Linear functions behave very well under backpropagation, not only are they simple to calculate, but they also do not cause gradients to vanish, unless they learn weights that badly disproportionate (but this can be handled by appropriate initialization).  Therefore, removing some nonlinear activation functions to make parts of a block purely linear should facilitate training deep networks. 
%\item The tandem setup, with both linear and nonlinear parts in each block, gives paths through the network with varying degrees of nonlinearity.  This should give a network the ability to behave somewhat like an ensemble of regular (fully nonlinear) subnetworks of many depths.  Going deeper should have little or no cost because it is essentially equivalent to adding some additional subnetworks to the ensemble. In fact, it should actually improve performance because the portions corresponding to deeper subnetworks can handle more nuanced patterns while the parts corresponding to shallower subnetworks paint the broad strokes. Thus, the tandem architecture should allow networks to learn optimal levels of complexity easily.  
%\item In their wonderful paper, \cite{EraseRelu} show that many network architectures have far too many activation functions, and that performance can be improved by removing activations that can't be avoided via linear paths. Intuitively, this should be more compelling the deeper you go.  
%\end{enumerate}

%Despite these heuristic reasons to expect better deep-network performance from our tandem architectures, they do still risk overfitting; but, as usual, this can be partially handled with standard regularization techniques, including dropout.

%Of course, one would expect these effects to play out differently in narrow, deep networks versus wide, shallow ones.  The rest of this paper is focused on some specific careful experiments to explore and test these hypotheses.

\subsection{Building Blocks}

We consider five different tandem blocks, each of which can be viewed as a variant on the standard resblock. The nonlinear part of each block (depicted on the right side of each block in Figure \ref{fig:blocks}) consists of either one or two layers of activated (nonlinear) convolutions. The linear part of each block (depicted on the left in Figure \ref{fig:blocks}) consists of either (i) identity maps, corresponding to standard resblocks  or (ii) convolutions of size $1\times 1$ or $3\times 3$, corresponding to more general tandem blocks.  Note that in some (usually rare) cases identity maps are impossible to use because of mismatches in layer width.  In those cases, as is typical with resnets, we use $1\times 1$ convolutions to project the identity to have the necessary width.  In all cases the outputs of the linear and nonlinear parts are added together at the end.

Specifically, the block variants we defined are as follows:
\begin{itemize}
  \item $B_{\operatorname{id}}(2,w)$ is the standard residual block, with two activated convolutional layers and an identity connection from start to finish. As is standard for resnets, in the relatively rare case that layers of different widths must be connected (and hence the identity cannot be used), the identity connection is replaced with a more flexible $1\times 1$ convolution.
   \item $B_{\operatorname{id}}(1,w)$ is the same as $B_{\operatorname{id}}(2,w)$, but with only one activated convolution instead of two.  This is another form of resblock.
   \item $B_{1\times 1}(2,w)$ is a tandem block like $B_{\operatorname{id}}(2,w)$ except that it always uses $1\times 1$ convolutions instead of identity connections, even when connecting layers of the same width.
  \item $B_{1\times 1}(1,w)$ is a tandem block like  $B_{1\times 1}(2,w)$, but with only one activated convolution instead of two.
  \item $B_{3\times 3}(1,w)$ has $3\times 3$ convolutions on both  sides, but on the nonlinear side it is followed by a nonlinear activation function while on the linear side it is not. 
\end{itemize}
These are all shown in Figure \ref{fig:blocks}. The first four blocks have all been used in previous publications, such as \cite{IdentityMappings}. We believe that the use of $3 \times 3$ filters for linear connections is unique. However, our primary goal is not to introduce a novel architecture. Rather, it is to investigate \emph{why} tandem blocks, including residual blocks, work. We chose this selection of blocks in order to determine which properties of shortcut connections (identity maps, fixed weights, linearity, etc) were necessary and which were unnecessary or even suboptimal. In particular, these blocks were ideal for determining whether learnable convolutions are just a necessary evil for accommodating occasional changes in layer width or whether they are a viable, or even superior, alternative to identity shortcuts. 

% The Compound One
\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.25\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
			\tikzset{>=Latex}
			\path (-4,0) -- (4,0);			
			
			\node[rectangle,draw,align=center,font=\Large] (rc1) at ( 2,-3) {$3\times 3\times w$ Conv};
			\node[rectangle,draw,align=center,font=\Large] (rr1) at ( 2,-4.25) {Relu};
			\node[rectangle,draw,align=center,font=\Large] (rc2) at ( 2,-6) {$3\times 3\times w$ Conv};
			\node[rectangle,draw,align=center,font=\Large] (rr2) at ( 2,-7.25) {Relu};
			\node[rectangle,draw,align=center,font=\Large] (lc1) at (-2,-5) {Identity};% or\\$1\times 1\times w$ Conv
			\node[rectangle,draw,align=center,font=\Large] (add) at ( 0,-9) {Add};
			\draw[thick,- ] (0,0)  -- (0,-1);
			\draw[thick,->] (0,-1) -- (2,-2)  -- (rc1);
			\draw[thick,->] (rc1)  -- (rr1);
			\draw[thick,->] (rr1)  -- (rc2);
			\draw[thick,->] (rc2)  -- (rr2);
			\draw[thick,->] (rr2)  -- (2,-8)  -- (add);
			\draw[thick,->] (0,-1) -- (-2,-2) -- (lc1);
			\draw[thick,->] (lc1)  -- (-2,-8) -- (add);
			\draw[thick,->] (add)  -- (0,-10);
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{$B_{\operatorname{id}}(2,w)$}%\label{fig:bi2}
	\end{subfigure}
	\hspace{.025\linewidth}
	\begin{subfigure}[t]{.25\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
			\tikzset{>=Latex}
			\path (-4,0) -- (4,0);			
			
			\node[rectangle,draw,align=center,font=\Large] (rc1) at ( 2,-3) {$3\times 3\times w$ Conv};
			\node[rectangle,draw,align=center,font=\Large] (rr1) at ( 2,-4.25) {Relu};
			\node[rectangle,draw,align=center,font=\Large] (rc2) at ( 2,-6) {$3\times 3\times w$ Conv};
			\node[rectangle,draw,align=center,font=\Large] (rr2) at ( 2,-7.25) {Relu};
			\node[rectangle,draw,align=center,font=\Large] (lc1) at (-2,-5) {$1\times 1\times w$ Conv};
			\node[rectangle,draw,align=center,font=\Large] (add) at ( 0,-9) {Add};
			\draw[thick,- ] (0,0)  -- (0,-1);
			\draw[thick,->] (0,-1) -- (2,-2)  -- (rc1);
			\draw[thick,->] (rc1)  -- (rr1);
			\draw[thick,->] (rr1)  -- (rc2);
			\draw[thick,->] (rc2)  -- (rr2);
			\draw[thick,->] (rr2)  -- (2,-8)  -- (add);
			\draw[thick,->] (0,-1) -- (-2,-2) -- (lc1);
			\draw[thick,->] (lc1)  -- (-2,-8) -- (add);
			\draw[thick,->] (add)  -- (0,-10);
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{$B_{1\times 1}(2,w)$}%\label{fig:b12}
	\end{subfigure}
	
	\vspace{.025\linewidth}
	\begin{subfigure}[t]{.25\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
			\tikzset{>=Latex}
			\path (-4,0) -- (4,0);			
			
			\node[rectangle,draw,align=center,font=\Large] (rc1) at ( 2,-3)   {$3\times 3\times w$ Conv};
			\node[rectangle,draw,align=center,font=\Large] (rr1) at ( 2,-4.25)   {Relu};
			\node[rectangle,draw,align=center,font=\Large] (lc1) at (-2,-3.5) {Identity};% or\\$1\times 1\times w$ Conv
			\node[rectangle,draw,align=center,font=\Large] (add) at ( 0,-6)   {Add};
			\draw[thick,- ] (0,0)  -- (0,-1);
			\draw[thick,->] (0,-1) -- (2,-2)  -- (rc1);
			\draw[thick,->] (rc1)  -- (rr1);
			\draw[thick,->] (rr1)  -- (2,-5)  -- (add);
			\draw[thick,->] (0,-1) -- (-2,-2) -- (lc1);
			\draw[thick,->] (lc1)  -- (-2,-5) -- (add);
			\draw[thick,->] (add)  -- (0,-7);
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{$B_{\operatorname{id}}(1,w)$}%\label{fig:bi1}
	\end{subfigure}
	\hspace{.025\linewidth}
	\begin{subfigure}[t]{.25\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
			\tikzset{>=Latex}
			\path (-4,0) -- (4,0);			
			
			\node[rectangle,draw,align=center,font=\Large] (rc1) at ( 2,-3)   {$3\times 3\times w$ Conv};
			\node[rectangle,draw,align=center,font=\Large] (rr1) at ( 2,-4.25)   {Relu};
			\node[rectangle,draw,align=center,font=\Large] (lc1) at (-2,-3.5) {$1\times 1\times w$ Conv};
			\node[rectangle,draw,align=center,font=\Large] (add) at ( 0,-6)   {Add};
			\draw[thick,- ] (0,0)  -- (0,-1);
			\draw[thick,->] (0,-1) -- (2,-2)  -- (rc1);
			\draw[thick,->] (rc1)  -- (rr1);
			\draw[thick,->] (rr1)  -- (2,-5)  -- (add);
			\draw[thick,->] (0,-1) -- (-2,-2) -- (lc1);
			\draw[thick,->] (lc1)  -- (-2,-5) -- (add);
			\draw[thick,->] (add)  -- (0,-7);
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{$B_{1\times 1}(1,w)$}%\label{fig:b11}
	\end{subfigure}
	\hspace{.025\linewidth}
	\begin{subfigure}[t]{.25\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
			\tikzset{>=Latex}
			\path (-4,0) -- (4,0);			
			
			\node[rectangle,draw,align=center,font=\Large] (rc1) at ( 2,-3)   {$3\times 3\times w$ Conv};
			\node[rectangle,draw,align=center,font=\Large] (rr1) at ( 2,-4.25)   {Relu};
			\node[rectangle,draw,align=center,font=\Large] (lc1) at (-2,-3.5) {$3\times 3\times w$ Conv};
			\node[rectangle,draw,align=center,font=\Large] (add) at ( 0,-6)   {Add};
			\draw[thick,- ] (0,0)  -- (0,-1);
			\draw[thick,->] (0,-1) -- (2,-2)  -- (rc1);
			\draw[thick,->] (rc1)  -- (rr1);
			\draw[thick,->] (rr1)  -- (2,-5)  -- (add);
			\draw[thick,->] (0,-1) -- (-2,-2) -- (lc1);
			\draw[thick,->] (lc1)  -- (-2,-5) -- (add);
			\draw[thick,->] (add)  -- (0,-7);
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{$B_{3\times 3}(1,w)$}%\label{fig:b31}
	\end{subfigure}
	
	\caption{The five tandem blocks used in all of our experiments.  The two left-most blocks $B_{\operatorname{id}}(2,w)$ and $B_{\operatorname{id}}(1,w)$ correspond to traditional resnets and the others to more general tandem nets.}\label{fig:blocks}
\end{figure}

Reasonable heuristic arguments could be made to justify all sorts of expectations for the different blocks.
For example, one might expect the identity blocks to perform well in deeper networks because they avoid changes in gradient magnitude, but they might also be less effective than blocks that can use $1\times 1$ convolutions to recombine features in potentially more useful ways. Using $3\times 3$ convolutions for the linear connections and letting both sides engineer new filters could create more robust networks that need less depth, but it could also be an inefficient use of parameters. Our experiments were designed to find out which of these intuitions are correct.

\section{Experiment Design}

For our experiments, we built networks of varying widths and depths from our five chosen tandem blocks and tested them on several popular image recognition datasets.

%We experiment with several different types of basic building blocks and assemble these into several network architectures.

\subsection{Network Architectures}
We focused on small architectures ranging from 8 to 26 layers and from 100k to 1.2M parameters. This was appropriate because the goal of these experiments was not to challenge state-of-the-art results produced by much larger models, but rather to compare the five block types in a variety of settings, to gain insight into their relative strengths and weaknesses.

Each experiment features five models, corresponding to the five types of block.  To ensure fair comparisons, each model has the same number of layers and nearly the same number of learnable parameters. To achieve the latter, each different type of block must have a different width $w$. In particular, $B_{3\times 3}$ requires significantly smaller values of $w$ than the other blocks because its linear convolutions have many more parameters.

%We also explored different values of hyperparameters for different architectures (through a grid search, on validation data) to make sure that each was achieving its best performance.

\subsection{Network Shape}

For all models we used a simple architecture with three shape hyperparameters. The block layer parameter $l$ sets the number of layers in each block, as in Figure \ref{fig:blocks}. The depth parameter $d$ controls the depth in the network by determining how many times to repeat each block. The width parameter $w$ sets the width of each block and is used to control the number of parameters in each model. The architecture is illustrated in Table \ref{table:arch}. In every case, the resulting network had $6d+2$ layers.   

\begin{table}[htb]
\centering
\begin{tabular}{l|l}
\multicolumn{1}{c}{\bf Component}  &\multicolumn{1}{c}{\bf Repetitions}
\\ \hline
Input         &$1$ \\
$3\times 3\times w$ Conv  &  $1$       \\
$B(l,w)$                  &  $2d/l$    \\
$B(l,2w)$ with Stride 2   &  $1$       \\
$B(l,2w)$                 &  $2d/l-1$  \\
$B(l,4w)$ with Stride 2   &  $1$       \\
$B(l,4w)$                 &  $2d/l-1$  \\
Global Average Pooling      &  $1$       \\
Softmax Output Layer          &  $1$       \\
\end{tabular}

\vspace{.025\linewidth}
\captionsetup{width=\linewidth}
\caption{All of the networks used in our experiments were instances of this meta-architecture.  The parameters $w$, $d$ and $l$ are chosen so that the total depth of each network is the same and the total number of parameters is comparable.}\label{table:arch}
\end{table}

\subsection{Regularization}

In each model, we used both dropout and $L^2$ regularization (weight decay). In all blocks, we applied dropout immediately after the linear and nonlinear sides were added together. In blocks with two nonlinear layers, we also applied dropout after the first nonlinear layer. We applied $L^2$ regularization to the weights of every convolution, both linear and nonlinear, but not to the biases.

We determined the weight decay and dropout rates for each architecture separately for each architecture through a series of grid searches. While different values of these hyperparameters proved optimal for different networks (as measured on a validation set), they were always modest (with dropout values from $0.1$ to $0.2$ and weight decay values from $0.0001$ to $0.0003$) and did not significantly change the relative performances of different architectures. 

We also used a simple augmentation scheme for the training data: shifting images both horizontally and vertically by a factor of no more than $10\%$ and flipping the images horizontally.

\subsection{Batch Normalization}

We tried inserting batch normalization layers in several places in each block---before activations, after activations, before convolutions, after the addition---and were quite surprised to find that none of these approaches was helpful. Contrary to the claims usually associated with this method, we observed that networks with batch normalization achieved about the same performance on average, but were less stable and more sensitive to learning rates. It also took much longer to train networks with batch normalization. Accordingly, we left it out of all of our experiments.

\subsection{Initialization}

We initialized all of the weights, but not the biases, by sampling from a truncated normal distribution (cut off at two standard deviations) with zero mean. Standard deviations were scaled from a base value by fan-in, as in \cite{ResNets}. The appropriate base standard deviation varied considerably from network to network. Some were as high as $1.2$ while others were as low as $0.3$. Aside from this variation, the responses to different base standard deviations were consistent. Low values produced networks that didn't learn, while high values produced networks that diverged. For a separate experiment described in Section \ref{nonid}, we tried some non-random initialization schemes to see if networks would learn identity maps on their own.

%This behavior suggests that all of these networks might all have loss functions with similar broad characteristics, including perhaps a nonoptimal neighborhood near zero with small gradient, thus gradient descent makes little or no progress in this region. Farther from the origin, the loss functions may have many local optima and a gradient of high variance, causing gradient descent to bounce around, unproductively, even with a very low learning rate. 

\subsection{Learning Rate and Descent Method}

We used the Adam method of gradient descent for all experiments, with the authors' recommended hyperparameters; see \cite{Adam}. Even with adaptive learning rates, we found that a learning rate schedule significantly improved results. For CIFAR-10 and CIFAR-100, we used a learning rate of $0.001$ through epoch 90, then $0.0002$ through epoch 120, and then $0.00004$ until epoch 150. For SVHN and Fashion-MNIST, we scaled this schedule to 100 epoch runs.

%%%%%%%%%%%%%%%%

\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=5, xmax=150, ymin=80, ymax=95, xtick = {40,80,...,120},
             ytick = {81,82,...,95}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/c10_deep2.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/c10_deep2.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/c10_deep2.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/c10_deep2.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/c10_deep2.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{14 Layers,\\ 300k Parameters}%\label{fig:b02}
	\end{subfigure}
	\hspace{.01\linewidth}
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=5, xmax=150, ymin=80, ymax=95, xtick = {40,80,...,120},
             ytick = {81,82,...,95}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/c10_deep2_wide.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/c10_deep2_wide.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/c10_deep2_wide.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/c10_deep2_wide.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/c10_deep2_wide.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{14 Layers,\\ 1.2m Parameters}%\label{fig:b12}
	\end{subfigure}
	\hspace{.01\linewidth}	
	%
	%\vspace{.05\linewidth}
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=5, xmax=150, ymin=80, ymax=95, xtick = {40,80,...,120},
             ytick = {81,82,...,95}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/c10_deep3.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/c10_deep3.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/c10_deep3.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/c10_deep3.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/c10_deep3.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{20 Layers,\\ 475k Parameters}%\label{fig:b01}
	\end{subfigure}
	\hspace{.01\linewidth}
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=5, xmax=150, ymin=80, ymax=95, xtick = {40,80,120},
             ytick = {81,82,...,95}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/c10_deep4.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/c10_deep4.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/c10_deep4.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/c10_deep4.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/c10_deep4.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{26 Layers,\\ 640k Parameters}%\label{fig:b11}
	\end{subfigure}
\caption{Plots of the test accuracy by epoch from the third-best of each architecture's five runs for all CIFAR-10 experiments.  
In each case, the tandem model $B_{1\times 1}(1,w)$ (light blue) performed best, beating both the resnet models $B_{\operatorname{id}}(1,w)$ and $B_{\operatorname{id}}(2,w)$.  The model $B_{1\times 1}(1,w)$  was consistently the best in the different runs.  Average final accuracy for the five runs is listed in Table~\ref{table:results}.}\label{fig:cifar10}
\end{figure}

%%%%%%%%%%

\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=5, xmax=150, ymin=42, ymax=74, xtick = {40,80, 120},
             ytick = {44,46,...,74}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/c100_deep2.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/c100_deep2.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/c100_deep2.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/c100_deep2.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/c100_deep2.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{14 Layers,\\ 300k Parameters}%\label{fig:b02}
	\end{subfigure}
	\hspace{.01\linewidth}
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=5, xmax=150, ymin=42, ymax=74, xtick = {40,80,120},
             ytick = {44,46,...,74}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/c100_deep2_wide.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/c100_deep2_wide.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/c100_deep2_wide.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/c100_deep2_wide.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/c100_deep2_wide.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{14 Layers,\\ 1.2m Parameters}%\label{fig:b12}
	\end{subfigure}
	\hspace{.01\linewidth}	
	%
	%\vspace{.05\linewidth}
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=5, xmax=150, ymin=42, ymax=74, xtick = {40,80,120},
             ytick = {44,46,...,74}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/c100_deep3.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/c100_deep3.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/c100_deep3.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/c100_deep3.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/c100_deep3.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{20 Layers,\\ 475k Parameters}%\label{fig:b01}
	\end{subfigure}
	\hspace{.01\linewidth}
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=5, xmax=150, ymin=42, ymax=74, xtick = {40,80,120},
             ytick = {44,46,...,74}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/c100_deep4.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/c100_deep4.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/c100_deep4.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/c100_deep4.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/c100_deep4.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{26 Layers,\\ 640k Parameters}%\label{fig:b11}
	\end{subfigure}
\caption{Plots of the test accuracy by epoch from the third-best of each architecture's five runs for all CIFAR-100 experiments.  In all cases the tandem models were clear winners, with $B_{1\times 1}(1,w)$ (light blue) performing best or near best each time, and both resnet models $B_{\operatorname{id}}(1,w)$ and $B_{\operatorname{id}}(2,w)$ at or near the bottom.  The tandem model $B_{3\times 3}(1,w)$ performed better than the model $B_{1\times 1}(2,w)$ in the shallower networks (14 and 20 layers), but $B_{1\times 1}(2,w)$ did better in the deeper (26-layer) network. Average final accuracy for the five runs is listed in Table~\ref{table:results}.}
\label{fig:cifar100}
\end{figure}



%%%%%%%%%%%%%%%%%


\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=2, xmax=100, ymin=88, ymax=97, xtick = {30,60,90},
             ytick = {89,90,...,97}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/svhn_deep1.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/svhn_deep1.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/svhn_deep1.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/svhn_deep1.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/svhn_deep1.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{8 Layers,\\ 130k Parameters}%\label{fig:b02}
	\end{subfigure}
	\hspace{.01\linewidth}
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=2, xmax=100, ymin=88, ymax=97, xtick = {30,60,90},
             ytick = {89,90,...,97}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/svhn_deep2.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/svhn_deep2.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/svhn_deep2.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/svhn_deep2.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/svhn_deep2.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{14 Layers,\\ 300k Parameters}%\label{fig:b12}
	\end{subfigure}
%	\vspace{.05\linewidth}
	\hspace{.01\linewidth}
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=2, xmax=100, ymin=88, ymax=97, xtick = {30,60,90},
             ytick = {89,90,...,97}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/svhn_deep3.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/svhn_deep3.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/svhn_deep3.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/svhn_deep3.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/svhn_deep3.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{20 Layers,\\ 475k Parameters}%\label{fig:b01}
	\end{subfigure}
\caption{Plots of the test accuracy by epoch from the third-best of each architecture's five runs for all SVHN experiments.  Again, in all three cases the tandem models $B_{1\times 1}(1,w)$ performed best or near best, and outperformed the resnet models $B_{\operatorname{id}}(1,w)$ and $B_{\operatorname{id}}(2,w)$, although in the last experiment, all the models---both resnet and more general tandem---have similar performance. Average final accuracy for the five runs is listed in Table~\ref{table:results}.}\label{fig:svhn}
\end{figure}

\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=2, xmax=100, ymin=80, ymax=94, xtick = {30,60,90},
             ytick = {81,82,...,94}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/fashion_deep1.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/fashion_deep1.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/fashion_deep1.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/fashion_deep1.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/fashion_deep1.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{8 Layers,\\ 130k Parameters}%\label{fig:b02}
	\end{subfigure}
	\hspace{.01\linewidth}
	\begin{subfigure}[t]{.23\linewidth}
		\centering
		\resizebox{\linewidth}{!}{
		\begin{tikzpicture}
\begin{axis}[xmin=2, xmax=100, ymin=80, ymax=94, xtick = {30,60,90},
             ytick = {81,82,...,94}, ymajorgrids=true, tick style={draw=none},
             tick label style={}, axis lines*=center,
             legend style={at={(0.4,0.15)},anchor=west},
             xlabel={}, ylabel={}, legend cell align=left, width=6cm,height=10cm]
\addplot[red]    table [x=epochs, y=a, col sep=comma, mark=none] {Data/fashion_deep2.csv};
\addplot[orange]   table [x=epochs, y=b, col sep=comma, mark=none] {Data/fashion_deep2.csv};
\addplot[violet]  table [x=epochs, y=e, col sep=comma, mark=none] {Data/fashion_deep2.csv};
\addplot[cyan] table [x=epochs, y=c, col sep=comma, mark=none] {Data/fashion_deep2.csv};
\addplot[black] table [x=epochs, y=d, col sep=comma, mark=none] {Data/fashion_deep2.csv};
\addlegendentry{$B_{\operatorname{id}}(2,w)$};
\addlegendentry{$B_{\operatorname{id}}(1,w)$};
\addlegendentry{$B_{1\times 1}(2,w)$};
\addlegendentry{$B_{1\times 1}(1,w)$};
\addlegendentry{$B_{3\times 3}(1,w)$};
\end{axis}
		\end{tikzpicture}}
		\captionsetup{width=.8\linewidth}
		\caption*{14 Layers,\\ 300k Parameters}%\label{fig:b12}
	\end{subfigure}
	\caption{Plots of the test accuracy by epoch from the third-best of each architecture's five runs for all  Fashion-MNIST experiments. Again the tandem model $B_{1\times 1}(1,w)$ performed at or near best, although the tandem model $B_{3\times 3}(1,w)$ did best in the deeper (14-layer) experiment.  The models with two nonlinear layers $B_{id}(2,w)$ and $B_{1\times 1}(2,w)$ performed worst. Average final accuracy for the five runs is listed in Table~\ref{table:results}.}\label{fig:fashion}
\end{figure}


\begin{table}[htb]
\centering
\begin{tabular}{|l l l|l l l l l|}
\hline
Dataset       & Layers & Params & $B_{\operatorname{id}}(2)$ & $B_{\operatorname{id}}(1)$ & $B_{1\times 1}(2)$ & $B_{1\times 1}(1)$ & $B_{3\times 3}(1)$ \\
\hline
CIFAR-10      & 14 & 300k & 90.91 & 91.30 & 91.43 & \textbf{\color{green!70!black} 92.14} & 91.44 \\
\hline
CIFAR-10      & 14 & 1.2m & 92.48 & 93.12 & 92.18 & \textbf{\color{green!70!black} 93.83} & 93.24 \\
\hline
CIFAR-10      & 20 & 470k & 91.91 & 91.51 & 91.99 & \textbf{\color{green!70!black} 92.82} & 92.07 \\
\hline
CIFAR-10      & 26 & 640k & 92.19 & 91.81 & 92.45 & \textbf{\color{green!70!black} 92.84} & 91.98 \\
\hline
CIFAR-100     & 14 & 300k & 64.28 & 64.66 & 65.31 & \textbf{\color{green!70!black} 67.74} & 67.25 \\
\hline
CIFAR-100     & 14 & 1.2m & 67.31 & 68.87 & 68.13 & 72.34 & \textbf{\color{green!70!black} 72.69} \\
\hline
CIFAR-100     & 20 & 470k & 64.73 & 64.54 & 66.75 & \textbf{\color{green!70!black} 69.17} & 68.02 \\
\hline
CIFAR-100     & 26 & 640k & 65.52 & 63.40 & 67.20 & \textbf{\color{green!70!black} 68.42} & 65.89 \\
\hline
SVHN          & 8  & 130k & 94.86 & 94.82 & 95.07 & \textbf{\color{green!70!black} 95.37} & 94.81 \\
\hline
SVHN          & 14 & 300k & 96.18 & 96.31 & 96.12 & \textbf{\color{green!70!black} 96.51} & 96.33 \\
\hline
SVHN          & 20 & 470k & 96.50 & 96.24 & 96.60 & 96.67 & \textbf{\color{green!70!black} 96.74} \\
\hline
Fashion-MNIST & 8  & 130k & 91.08 & 91.52 & 90.85 & \textbf{\color{green!70!black} 91.62} & 91.42 \\
\hline
Fashion-MNIST & 14 & 300k & 92.74 & 93.23 & 93.16 & \textbf{\color{green!70!black} 93.71} & 93.65 \\
\hline
\end{tabular}
%\vspace{.025\linewidth}
\captionsetup{width=\linewidth}
\caption{The average test accuracy achieved with each tandem block in five repetitions of each experiment. In every case both standard residual blocks $B_{\operatorname{id}}(2,w)$ and $B_{\operatorname{id}}(1,w)$ were outperformed by at least one of the more general tandem blocks. In all but one case (FAshion-MNIST, 8 layers) both of the standard residual blocks were outperformed by at least two of the more general tandem networks.}\label{table:results}
\end{table}



%%%%%%%%%%%%%%%%%



\section{Results}

To evaluate the five blocks, we used each to build several different networks and then tested them on four popular image recognition problems: CIFAR-10, CIFAR-100, SVHN, and Fashion-MNIST. In total, we made the five blocks compete in thirteen challenges. Each challenge was run five times. Table \ref{table:results} summarizes the results of all of our experiments. To make the graphs in Figures \ref{fig:cifar10}, \ref{fig:cifar100}, \ref{fig:svhn}, and \ref{fig:fashion} represent what these runs actually look like, we plotted the test accuracy from the third-best of each architecture's five runs.  %%TJ moved next here from caption--ask why it was in caption.
The results for standard resblocks were very close to those published by \cite{ResNets} and \cite{Wideresnets}.

 %The best blocks in each challenge were competitive with the best published results for their numbers of parameters; see Table \ref{table:results} for the breakdown.

In all four challenges on CIFAR-10 (Figure \ref{fig:cifar10}), the $B_{1\times 1}(1,w)$ variant (which uses learnable projections instead of identity maps) excelled while the $B_{\operatorname{id}}$ variants, with their more standard identity connections, lagged behind.

The results for CIFAR-100 (Figure \ref{fig:cifar100}) were similar to those for CIFAR-10 in that the tandem block $B_{1\times 1}(1,w)$ performed at or near the top and consistently outperformed all the traditional resnet blocks.  The two-layer blocks improved their relative performance as depth increased and $B_{3\times 3}$ actually got worse. However, $B_{3\times 3}(1,w)$ shined with the mid-depth  architectures, particularly on the wider one with 1.2m parameters. This suggests that $B_{3\times 3}$ might be even better suited to building relatively wide networks than the resblocks used to achieve state-of-the-art results by \cite{Wideresnets}.

On SVHN (Figure \ref{fig:svhn}), the tandem net $B_{1\times 1}(1,w)$ once again excelled while the traditional resblocks $B_{\operatorname{id}}(2,w)$ and $B_{\operatorname{id}}(1,w)$ stayed behind. Interestingly, $B_{3\times 3}(1,w)$ improved as the networks became deeper, even beating $B_{1\times 1}(1,w)$ in the 20-layer network. 

On the Fashion-MNIST dataset (Figure \ref{fig:fashion}), $B_{1\times 1}(1,w)$ again had the strongest performance, but the resnet $B_{\operatorname{id}}(1,w)$ was not terribly far behind.
The tandem block $B_{3\times 3}(1,w)$ did better on the deeper network.  
In these tests the blocks with two nonlinear layers  $B_{\operatorname{id}}(2,w)$ and $B_{1\times 1}(2,w)$ were significantly behind.

\section{Non-Identity Maps}\label{nonid}

The use of learnable parameters for the linear convolutions in tandem blocks naturally invites several questions. Will linear convolutions with randomly initialized weights learn something similar to an identity map? Will linear convolutions initialized to an identity map stay there, or learn something else? If not identity maps, what do the optimal linear transformations for tandem blocks look like? To explore these questions, we looked at the singular value decompositions of the weight matrices of blocks with $1\times 1$ linear convolutions. Figure \ref{fig:svd} shows that the linear convolutions did not learn identity maps, regardless of initialization scheme.

\begin{figure}[htb]
	\centering
	\begin{subfigure}[t]{.3\linewidth}
		\centering
		\includegraphics[width=1.15\linewidth]{Figures/identity_svd.pdf}
		\captionsetup{width=.8\linewidth}
		\caption*{Identity}
	\end{subfigure}
	\hspace{.025\linewidth}
	\begin{subfigure}[t]{.3\linewidth}
		\centering
		\includegraphics[width=1.15\linewidth]{Figures/zeros_svd.pdf}
		\captionsetup{width=.8\linewidth}
		\caption*{Zero}
	\end{subfigure}
	\hspace{.025\linewidth}
	\begin{subfigure}[t]{.3\linewidth}
		\centering
		\includegraphics[width=1.15\linewidth]{Figures/small_gaussian_svd.pdf}
		\captionsetup{width=.8\linewidth}
		\caption*{Gaussian}
	\end{subfigure}
\caption{These plots show the singular values of the weight matrix of the linear $1\times 1$ convolution in a $B_{1\times 1}(1,w)$ block. 
We tried initializing these weights with identity matrices, zero matrices, and random matrices. All the singular values of the identity matrix are equal to $1$, as seen in the initial epoch of the far left panel. Similarly all the singular values for the zero matrix are $0$, as seen in the initial epoch of the middle panel.  
In each case, the network learned a weight matrix that was quite different from any of the initializations. In particular, this shows that identity maps are not even locally optimal in these applications.}\label{fig:svd}
\end{figure}



\section{Conclusions}

We generalized residual blocks (which use identity shortcut connections) to tandem blocks (which can learn any linear connection, not just the identity). We found that general linear connections with learnable weights, have the same benefits as the identity maps in residual blocks, and they actually increase performance compared to identity maps. We also showed that linear connections do not learn identity maps, even when initialized with identity weight matrices. These results seem to confirm that the success of residual networks and related architectures is not due to special properties of  identity maps, but rather is simply a result of using linear maps to complement nonlinear ones. 

The additional flexibility gained by replacing identity maps with  convolutions led to better results in every one of our experiments. This was not due to extra parameters, as we adjusted layer widths to keep parameter counts as close to equal as possible. Instead, general linear convolutions appear to do a better job than identity maps of working together with nonlinear convolutions.

Our results further suggest that tandem blocks with a single nonlinear convolution tend to outperform those with two, but blocks that use $3\times 3$ convolutions for their linear connections may be better in wide networks than those with $1\times 1$s.

Finally, we note that there are many more possible types of tandem block than those we have considered here, and many more applications in which to test them. 

\subsubsection*{Acknowledgments}

This work was supported in part by the National Science Foundation, grant numbers 1323785 and 1564502, and the Defense Threat Reduction Agency, grant number HDRTA1-15-0049.

%Removed for review.

\FloatBarrier
%\newpage

\bibliography{tandem_arxiv}
\bibliographystyle{arxiv}

\end{document}
