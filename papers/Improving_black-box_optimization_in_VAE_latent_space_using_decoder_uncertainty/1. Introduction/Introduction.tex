\section{Introduction}
\label{Sec1_Intro}

We consider the task of optimizing an expensive black-box objective function taking inputs in a \emph{high-dimensional discrete} space. This could be for example finding new molecules for drug design, or automatically generating a computer program that matches a desired output. Solving this task directly in the original space (e.g., with discrete local search methods such as genetic algorithms) may be challenging given the complex structure and high dimensionality of the data.
Recently, Variational autoencoders (VAEs) \citep{kingma2014autoencoding,rezende2014stochastic} have been successfully leveraged to model a wide range of discrete data modalities --- from natural language \citep{bowman2016generating}, to arithmetic expressions \citep{kusner2017grammar}, computer programs \citep{dai2018syntaxdirected} or molecules \citep{Gomez_Bombarelli_2018}. By learning a lower-dimensional continuous representation of objects in their latent space, VAEs allow to transform the original discrete optimization problem into a simpler \emph{continuous} optimization one in latent space. For example, this can be achieved via Bayesian Optimization in the latent space, or via gradient ascent with a jointly-trained neural network predicting the black box property from the latent space representation \citep{Gomez_Bombarelli_2018, bradshaw2019model}.
Initial methods in this area have suffered from the fact that the search in latent space may explore areas for which no data was available at train time, and therefore where the decoder network of the VAE will be unreliable \citep{janz2017actively}: seemingly good candidate points in latent space may be decoded into objects that are invalid, unrealistic or low quality.

\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{1. Introduction/Fig1_uncertainty_guided_optim.png}
    \caption{\textbf{Uncertainty-guided optimization in VAE latent space} The goal of black-box optimization in latent space is to attain regions with high values of the back-box objective after decoding, while avoiding the regions that lead to invalid decodings (left). Standard Bayesian Optimization in latent space may query these suboptimal areas (e.g., regions on left hand side, center). High decoder uncertainty regions overlap with regions leading to invalid decodings (right), so that censoring high uncertainty points helps guiding the optimization towards the most promising latent points.}
    \label{Sec1_Fig1}
\end{figure*}

While several methods have been introduced to promote validity of decoded objects (\S\ref{Sec2_Background_Optim}), they either focus on modifying the generative model learning procedure or adapting the decoder architecture to satisfy the syntactic requirements of the data modality of interest. We propose instead to quantify and leverage the uncertainty of the decoder network to guide the optimization process in latent space (Fig.~\ref{Sec1_Fig1}). This approach does not require any change to the model training nor architecture, and can easily be integrated within several optimization frameworks. It results in a better trade-off between the values of the black-box objective and the validity of the newly generated objects, sometimes improving both simultaneously.

To be effective, this method requires robust estimates of model uncertainty for high dimensional structured data. Existing methods for uncertainty estimation in this domain often rely on heuristics or make independence assumptions to make computations tractable (\S\ref{Sec2_Background_Uncertainty}). We demonstrate that such assumptions are not appropriate in our setting, and propose new methods for uncertainty estimation in high dimensional structured data instead. 

Our contributions are as follows:
\begin{itemize}
\item We introduce an algorithm to quantify the uncertainty of high-dimensional discrete data, and use it to estimate the uncertainty of the decoder (\S\ref{Sec3_Estimator});
\item We show how the uncertainty of the decoder can be incorporated across several optimization frameworks, including gradient ascent and Bayesian optimization (\S\ref{Sec4_Uncertainty_guided_Optimization});
\item We illustrate our approach in a digit generation setting --- a simple setup to provide intuition for the method --- then quantify its benefits in the more complex tasks of arithmetic expressions approximation and molecule generation, covering a diverse set of decoder architectures across experiments (Convolutional, Recurrent and Graph Neural Networks) (\S\ref{Sec5_Experiments}).
\end{itemize}