\section{Experimental results}
\label{Sec5_Experiments}

After describing the common experimental setting across applications, we demonstrate the effectiveness of using the uncertainty of the decoder to guide the optimization for constrained digit generation. Our objective is to illustrate the concepts introduced above in a simple and intuitive example. We then move on to quantify the benefits of our approach in the more complex cases of arithmetic expression approximation and molecular generation for drug design. 

\textbf{Uncertainty estimators and baselines} Across all experiments, we quantify the Mutual Information between outputs and decoder parameters with both the Importance sampling estimator (IS-MI) described in \S\ref{Sec3_Estimator} and based on the token independence approximation (TI-MI) described in \S\ref{Sec2_Background_Uncertainty}. Sampling from model parameters is achieved via Monte Carlo dropout \citep{gal2016dropout}. We compare optimization results with two baselines: the standard approach that fully ignores decoder uncertainty, and an approach in which we censor proposal points with low probability under the prior distribution (standard normal) of the VAEs in latent space (referred to as `NLLP').

\textbf{Optimization} We perform uncertainty-guided optimization in latent space as per the two approaches described in \S\ref{Sec4_Uncertainty_guided_Optimization}. For Bayesian Optimization, we train a single task Gaussian Process as our surrogate model based on a random subset of training points embedded in latent space and their corresponding black-box objective values. We then perform several iterations of batch Bayesian Optimization using the Expected Improvement heuristic as our acquisition function. At each iteration we select a batch of 20 latent vectors by sequentially maximizing the acquisition function. We select the point with the highest predicted target value for which the decoder uncertainty is below a predefined threshold (e.g., $99^{th}$ percentile of decoder uncertainty values observed on the training set) or the one with lowest uncertainty if no point in the batch is below the threshold. We re-train the surrogate model with the newly generated point at each step.
For gradient ascent, we randomly sample points from the training set, embed them in latent space, and use these as our starting positions. We then compute the gradient of the auxiliary property network with respect to latent positions and accept proposal moves along these directions if the decoder uncertainty at the corresponding position in latent is below a predefined threshold (e.g., $99^{th}$ percentile of decoder uncertainty on the training set).
All optimization experiments reported below are carried out 10 times independently with different random seeds. %Hyperparameter values across experiments are detailed in Appendix~\ref{Appendix_B_Digit_generation}-\ref{Appendix_E_Molecule_JTVAE}.

\subsection{Illustrative example in digit generation}
\label{Sec5.1_MNIST}

\textbf{Setup} In this first setting, our objective is to generate \emph{valid} images of the digit 3 that are as \emph{thick} as possible. We train a VAE model generating images of the digit 3 jointly with an auxiliary network predicting their thickness. We use a `Conv-Deconv'\citep{Higgins2017} architecture for the VAE and a 3-layer feedforward network for property prediction. The underlying data consist of grayscale images of the digit 3  extracted from the MNIST dataset \citep{Lecun1998MNIST} that we discretize to form tensors of binary values. We use the sum of pixel intensities across a given image as a proxy for the thickness of the corresponding digit. An unconstrained optimization in the latent space would ultimately lead to the generation of \emph{invalid} white `blobs'. To avoid this failure mode and promote validity of the resulting candidate set, we leverage the uncertainty of the decoder network. In order to assess the validity of objects, we independently train a deep Convolutional Neural Network to classify images of the digit 3  (binary classification).

\textbf{Results} Latent points with high decoder uncertainty lead to a higher rate of invalid decoded digits (Appendix~\ref{Appendix_B.3_Digit_generation_Uncertainty_estimator}), which help avoid the aforementioned failure mode during optimization. 
For both Bayesian Optimization and gradient ascent, we observe that the decoder uncertainty constraints help ensuring the generated digits remain valid, while preserving the ability of the optimization algorithm to increase the thickness (Fig.\ref{Sec5_Fig_MNIST_Top_images}). 


\begin{figure}[ht]
\begin{minipage}[m]{0.48\linewidth}
\vspace{0pt}
\centering
    \includegraphics[scale=0.55]{5. Experimental Results/MNIST_images/MNIST_Optimization_Top-Digits.png}
    \caption{\textbf{Top 5 decoded digits after optimization} Leveraging decoder uncertainty helps preventing the generation of invalid digits.}
    \label{Sec5_Fig_MNIST_Top_images}
\end{minipage}
%\hspace{0.3cm}
\hfill
\begin{minipage}[m]{0.48\linewidth}
\vspace{0pt}\raggedright
\centering
\resizebox{.98\textwidth}{!}{
\input{5. Experimental Results/Arithmetic_expression_results}
}
\caption{\textbf{Arithmetic expressions approximation results} Mutual Information-based constraints during Bayesian optimization in latent space help promoting higher validity \% of decodings while increasing black-box objective values. Uncertainty threshold values used for censoring candidate points are based on decoder uncertainty values observed on the training data ($95^{th}$ percentile).}
\label{Sec5.2_Arithmetic_results}
\end{minipage}
\end{figure}

\subsection{Arithmetic expressions approximation}

\textbf{Setup} We follow an experimental design similar to \citet{kusner2017grammar}, in which we seek to optimize univariate arithmetic expressions generated by a formal grammar (rules and examples are provided in Appendix~\ref{Appendix_C_Arithmetic_expression}). The objective is to find an expression that minimizes the mean squared error (MSE) with respect to a predefined target expression ($1/3 * x * sin(x*x)$). More specifically, since the presence of exponentials in expressions may results in very large MSE values, the black-box objective is defined as $-log(1+MSE)$. We train a `Character VAE' (CVAE) \citep{kusner2017grammar} on $80,000$ expressions generated by the formal grammar, then perform optimization in the latent space.

\textbf{Results} We observe that methods leveraging the decoder uncertainty result in almost always valid decodings, and reach higher average values of the black box objective for valid decoded expressions compared to baselines (Fig.~\ref{Sec5.2_Arithmetic_results}; gradient ascent results in Appendix~\ref{Appendix_C.4_Arithmetic_expression_Optimization}). In particular, censoring candidate points based on their probability under the standard normal prior does not help promoting validity of decodings at all. In this setting with relatively short sequences (arithmetic expressions have at most 19 characters), leveraging the TI-MI or IS-MI estimators leads to comparable performance.

\subsection{Molecule generation for drug discovery}

Molecular generation for drug design seeks to identify new molecules satisfying desired chemical properties. Molecules are typically either represented as sequences of characters, using their SMILES representation \citep{weininger88smiles}, or as graphs of atoms \citep{duvenaud2015convolutional}. We demonstrate the effectiveness of the approach described in \S~\ref{Sec4_Uncertainty_guided_Optimization} for these two different representations: experiments with the `Character VAE' (CVAE) for molecules \citep{Gomez_Bombarelli_2018} leverage the SMILES representation, while experiments with the JT-VAE \citep{jin2019junction} are based on a graph representation of molecules. For both architectures, we trained our models on a set of 250k drug-like molecules from the ZINC dataset \citep{Irwin2012ZINCAF}.

\subsubsection{Character VAE (CVAE)}
\label{Sec5_CVAE}

\textbf{Setup} We jointly train a CVAE model, which learns to encode and decode molecules SMILES strings, along with an auxiliary property network that predicts a target property of these molecules. Following prior work \citep{kusner2017grammar, dai2018syntaxdirected, jin2019junction}, we define the black-box objective as the octanol-water partition coefficient penalized by the synthetic accessibility score and the number of long cycles, (Appendix~\ref{Appendix_D.1_Molecule_CVAE_Data}) and we refer to it as `Penalized logP' for brevity.
Since the SMILES representation of molecules follows a strict syntax that determines whether a given expression is valid or not, we are interested in generating molecules that simultaneously maximize the target property and represent valid SMILES expressions.

\begin{figure*}[h]
    \centering
    \includegraphics[width=5.5in,scale=1.0]{5. Experimental Results/CVAE_images/Latent_space_viz-01-01.png}
    \vspace{-1mm}
    \caption{\textbf{CVAE latent space visualization.} We apply Principal Component Analysis on the embedding of the full training data and keep the first 2 components. We then create a grid on the resulting 2D-space and measure the penalized logP (a), decoder uncertainty (b) and the proportion of valid decodings in that region (c) (a \& c averaged over 300 decodings; b measured via IS by sampling 100 times from the importance distribution, and averaging over 100 samples of model parameters.; for a, white squares correspond to regions where none of the 300 decodings are valid). We observe a strong overlap between decoder uncertainty (b) and validity of decodings (c).}
    \label{Fig_CVAE_Latent_space_viz}
\end{figure*}
\vspace{-1mm}

\begin{figure*}[h]
    \centering
    \includegraphics[width=5.5in,scale=1.0]{5. Experimental Results/CVAE_images/Plot_uncertainty_estimator.png}
    \caption{\textbf{Uncertainty estimator.} a) Distribution of decoder uncertainty values (IS-MI) for 1k samples for 4 distinct sets (train \& test set samples embedded in latent space; samples from the prior; samples far from the prior). b) Valid decodings (\%) as a function of the proportion of samples kept based on their uncertainty --- eliminating points with high uncertainty first (dataset comprised of 50\% samples from test set \& 50\% of samples far from the prior). The IS-MI estimator has superior ability to identify points leading to invalid decodings. c)  Valid decodings (\%) for samples from a normal distribution with increasing standard deviation. Samples with decoder uncertainty below a predefined threshold (maximum IS-MI value observed on training data) have a much higher rate of valid decodings. Points above the threshold are very likely to lead to invalid decodings.}
    \label{Fig_CVAE_Uncertainty_estimator}
\vspace{-2mm}
\end{figure*}

\textbf{Results} We first verify that our estimator is able to discriminate points in-distribution (low uncertainty) vs out-of-distribution (high uncertainty). We consider 4 distinct sets of points in latent: embeddings into latent space of a random sample from the train and test sets, random samples from the VAE prior (standard normal) and random samples ``far from the prior'' (we sample from an isotropic gaussian with standard deviation equal to 10). As can be seen on Fig.\ref{Fig_CVAE_Uncertainty_estimator}a, uncertainty estimates for the first 3 sets strongly overlap while being disjoint from the estimates corresponding to points far from the prior.  Furthermore, we observe a strong correlation between low decoder uncertainty and regions that lead to valid SMILES decodings (Fig.~\ref{Fig_CVAE_Latent_space_viz}). This is corroborated by the analysis described in Fig.~\ref{Fig_CVAE_Uncertainty_estimator}c: when considering latent points ``far from the prior'', points for which the decoder uncertainty is lower than a predefined threshold (e.g., maximum value observed on training data) will lead to a significantly higher proportion of valid decoded molecules compared to latent points with uncertainty above the threshold. This is critical as it allows to censor points that will likely lead to invalid decodings, even when we move \emph{far from the prior} in latent space.

For the Bayesian Optimization experiments, we investigate the impact of different bounds on the space we optimize within, as well as different uncertainty thresholds. As we increase the bounds, we typically reach higher optima, at the cost of a higher fraction of invalid decodings during search. We obtain higher validity \% and penalized logP values when leveraging the decoder uncertainty (Table \ref{Table_CVAE_BO_results}). In this setting, the token-level independence assumption (TI-MI) leads to poor performance compared to the importance sampling-based estimator (IS-MI). Results are also robust to the choice of decoder uncertainty thresholds (\ref{Appendix_D.4_Molecule_CVAE_Optimization}).

\input{5. Experimental Results/Table_CVAE_BO_May28.tex}

\subsubsection{Junction Tree VAE (JT-VAE)}
\label{Sec5_JTVAE}

\textbf{Setup} We train a Junction Tree VAE model (JT-VAE) \citep{jin2019junction} using the same dataset of 250k molecules (ZINC) and black-box objective (penalized logP) as for the CVAE experiments. 
All molecules generated by the JT-VAE are valid by design. However, not all generated molecules will be of high \emph{quality}, which we assess with the quality filters proposed by \citet{Brown_2019} that aim at ruling out ``compounds which are potentially unstable, reactive, laborious to synthesize, or simply unpleasant to the eye of medicinal chemists.''
We show that it is straightforward to attain state-of-the-art performance in terms of penalized logP values with the basic optimization approaches described in \S~\ref{Sec4_Uncertainty_guided_Optimization} by moving sufficiently `far away' in latent, but that in doing so we tend to generate molecules that never pass quality filters. Factoring in decoder uncertainty during optimization helps generate new molecules with both high penalized logP values and high quality.

Using notations from \S~\ref{Sec3_Estimator}, sampling a new object $\tilde{y}_s$ is achieved by successively decoding from the junction tree decoder and then the graph decoder. We then decompose $\log{p_{s,m}}$ -- the log probability of the sampled graph molecule -- as the sum of the log probabilities corresponding to the different predictions made by the junction tree decoder and graph decoder, namely the topology and node predictions in the junction tree decoder, and the subgraph prediction in the graph decoder. \\
We replicate the analysis described in \ref{Sec5_CVAE} with the 4 distinct datasets in latent space (i.e., train, test, prior and far from prior) and observe similar results: the histogram of decoder uncertainty values for points ``far from the prior'' is disjoint from the other three histograms (Appendix~\ref{Appendix_E.3_Molecule_JTVAE_Uncertainty_estimator}), confirming the ability of the estimator to identify out-of-distribution points.

\textbf{Results} Both gradient ascent (Table~\ref{Sec5_Table_JTVAE_GA_results}) and Bayesian Optimization (Appendix~\ref{Appendix_E.4_Molecule_JTVAE_Optimization}) allow to generate new molecules with state-of-the-art performance in terms of penalized logP (Table~\ref{Appendix_E_Table_Molecular_generation_top_performance}). However, the majority of these molecules do not pass quality filters. Leveraging decoder uncertainty leads to the generation of high logP and high quality molecules. Using likelihood under the prior (NLLP) to achieve the same is detrimental to optimization performance.

\input{5. Experimental Results/Table_JTVAE_GA_May28}