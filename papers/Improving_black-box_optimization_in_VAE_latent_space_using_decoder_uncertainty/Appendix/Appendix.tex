\section{Analysis of variance of uncertainty estimators}
\label{Appendix_Uncertainty_estimator}

We demonstrate the lower variance of the Importance sampling-based estimator compared to the naive Monte Carlo estimator, focusing on the Character VAE for molecular generation setting described in \S~\ref{Sec5_CVAE} and Appendix~\ref{Appendix_D_Molecule_CVAE}.

\textbf{Setup.} We sample 1,000 points at random in latent space from an isotropic Gaussian with fixed standard deviation $\sigma$ (we repeat the experiment for different values of the standard deviation). We assess at these points the mutual information between outputs (generated molecule SMILES) and decoder parameters with the Importance sampling-based estimator (IS-MI) described in \S~\ref{Sec3_Estimator}, and the naive Monte Carlo (MC-MI) equivalent. The MC-MI estimator is obtained directly from equation \ref{Equation_MI_pre_IS} by sampling output sequences $y_s$ uniformly at random from  $\mathcal{S}$, the space of all possible SMILES strings. As per the setting described in Appendix~\ref{Appendix_D_Molecule_CVAE}, we limit the length of molecular sequences to 120 characters from a vocabulary comprised of 34 elements (plus the padding character). Consequently, $\mathcal{S}$ is finite and we can uniformly sample from it by independently sampling characters at each position uniformly at random from the vocabulary.
Both estimators are computed by sampling a fixed number of decoder parameters using Monte Carlo dropout \cite{gal2016dropout} (we used 100 model samples in all experiments).

\textbf{Results.} We analyze the impact of the number of $y_s$ samples for each estimator on the variance of the corresponding estimators, measured over 10 independent runs. More specifically, we compute the standard deviation over the 10 runs, normalized by the estimator mean across runs. We observe that the IS-MI estimator has a normalized standard deviation 2-10x smaller than the MC-MI estimator across the different experiments (Fig.~\ref{Appendix_Fig_variance_analysis_results}).

\begin{figure}[ht]
\vspace{0pt}
\resizebox{\textwidth}{!}{
    \includegraphics[scale=1]{Appendix/Images/Variance/CVAE_variance_std1.png}
    \includegraphics[scale=1]{Appendix/Images/Variance/CVAE_variance_std5.png}
}
\hfill
\resizebox{\textwidth}{!}{
    \includegraphics[scale=1]{Appendix/Images/Variance/CVAE_variance_std3.png}
    \includegraphics[scale=1]{Appendix/Images/Variance/CVAE_variance_std10.png}
}
\caption{\textbf{Variance analysis for uncertainty estimators} Comparison of the normalized standard deviations based on the number of sampled output sequences for the IS-MI Vs MC-MI estimators. We vary the standard deviation of the isotropic Gaussian used to sample points in latent space, from 1 (top left) to 10 (bottom right).}
\label{Appendix_Fig_variance_analysis_results}
\end{figure}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Digit generation experiments}
\label{Appendix_B_Digit_generation}

\subsection{Data}
\label{Appendix_B.1_Digit_generation_Data}

\textbf{Data source.} The MNIST \citep{Lecun1998MNIST} dataset consists of grayscale images (28x28 pixels) representing handwritten digits. The training dataset is comprised of 60k images and the test dataset is comprised of 10k images.

\textbf{Pre-processing.} We use the same train/test split as from the source listed in Table~\ref{Appendix_Data_sources}, and filter the data to keep images of 3 digits only. We then binarize the data, using a low pixel intensity threshold value ($10^{th}$ percentile of pixel intensity values observed on the training data). No data augmentation is used at train time nor at inference. We use the sum of all pixel intensities in the image as a proxy of the thickness of the digits, which provides strong empirical results.

\subsection{Model details}
\label{Appendix_B.2_Digit_generation_Model}

\textbf{Architecture.} We jointly train a variational autoencoder with an auxiliary network (the ``Property network``) predicting digit thickness based on latent representation (see  Fig.~\ref{Appendix_Figure_Joint_training_architecture}). For the VAE, we use a ``Conv-Deconv'' architecture\citep{Higgins2017}. All model parameters are summarized in table~\ref{Appendix_B_Table_model}.

\begin{table}[h]
\begin{center}
\caption{\textbf{Digit generation - Model architecture details}}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Component}  & \textbf{Description} \\
\toprule
\textbf{Encoder} & $\bullet$ 4 consecutive convolutional layers with 28, 56, 56 and 224 filters respectively, \\ 
& \quad kernel sizes 4,4,3 and 4 respectively, with batch norm and RELU activations \\
& \quad after each convolutional layer \\ 
& $\bullet$ Continuous latent space of dimension 2 \\
\midrule
\textbf{Decoder} &  $\bullet$ 4 2D-transposed convolutional layers with 224, 56, 56 and 28 filters respectively, \\
& \quad kernel sizes 4,3,4 and 4 respectively, with dropout 0.2 and RELU activations \\
& \quad after each convolutional layer \\ 
\midrule
\textbf{Property} &  $\bullet$ 3-layer feedforward network with 100 units each \\
\textbf{network} & $\bullet$ RELU activations (after each layer except the final one) and dropout 0.1 \\
\bottomrule
\end{tabular}
}
\label{Appendix_B_Table_model}
\end{center}
\end{table}

We assess the validity of generated digits with a separately trained Convolutional Neural Network (CNN) that learns to classify images of 3 digits (binary classification). This network is comprised of two convolution layers (with 23 and 64 filters respectively), followed by a max pool layer and two fully-connected layers (with 9,216 and 128 units respectively). RELU activations are used throughout, except for the final layer where a sigmoid activation is used for the binary classification.

\textbf{Training.} We train the joint architecture by minimizing the sum of the VAE ELBO and the mean squared error (MSE) on the thickness prediction task.
We use the Adam algorithm \citep{kingma2017adam} with a learning rate of $10^{-3}$, batch size of $512$, weight decay $10^{-5}$ for 300 epochs. We anneal the KL divergence with a sigmoid schedule for the first 30 epochs to avoid potential posterior collapse.

The independent CNN classifier network is trained by minimizing the binary cross entropy loss. We use the Adam optimizer, with learning rate $10^{-4}$, batch size 512 and train for 120 epochs.

\subsection{Uncertainty estimator}
\label{Appendix_B.3_Digit_generation_Uncertainty_estimator}

We compute the Importance sampling-based estimator following Algorithm~\ref{Sec3_Algorithm_IS-MI}, with $100$ samples from model parameters and $100$ binary images $y$ sampled from the importance distribution. We use Monte Carlo dropout \cite{gal2016dropout} to sample decoder parameters. 

\textbf{Uncertainty histograms. } We evaluate the IS-MI estimator values in different regions of latent space. We consider 4 distinct sets of points:
\vspace{-2mm}
\begin{itemize}
\setlength\itemsep{0em}
    \item \textbf{`Train':} Embeddings into latent space of 5k images sampled randomly from the train set;
    \item \textbf{`Test':} Embeddings into latent space of 5k images sampled randomly from the test set;
    \item \textbf{`Prior':} 5k random samples from the VAE prior;
    \item \textbf{`Far from prior':} 5k random samples from an isotropic Gaussian with standard deviation equal to 20.
\end{itemize}

We observe an almost perfect overlap between the distributions of decoder uncertainty values for the `Train' and `Test' sets, and both have a strong overlap with the `Prior' set (Fig.~\ref{Appendix_Fig_Digit_generation_histogram}). These first 3 sets are all fairly disjoint from the last set (`far from prior'), confirming the ability of the uncertainty estimators to properly identify `out-of-distribution' points.

\begin{figure}[ht]
\vspace{0pt}
\begin{center}
\resizebox{0.5\textwidth}{!}{
    \includegraphics[scale=1]{Appendix/Images/MNIST/Uncertainty_decoder_histogram_Jun3_2_1.0_ConvDeconv_MI_Method1_Importance_Sampling_Fast_CNN_5_bernoulli.csv.png}
}
\end{center}
\caption{\textbf{Digit generation - Decoder Uncertainty distribution} Distribution of decoder uncertainty values based on the IS-MI estimator, evaluated on the 4 distinct datasets defined in Appendix~\ref{Appendix_B.3_Digit_generation_Uncertainty_estimator}}
\label{Appendix_Fig_Digit_generation_histogram}
\end{figure}

\subsection{Optimization details}
\label{Appendix_B.4_Digit_generation_Optimization}

We follow the gradient ascent and Bayesian Optimization approaches described at the beginning of \S~\ref{Sec5_Experiments}. We provide below all hyperparameter values used for each method.

\textbf{Gradient ascent.} We start from 300 point sampled at random from the training set and then embedded in latent space. We perform 10 gradient update steps with a value of alpha equal to 50 (using notations from \S~\ref{Sec4_Uncertainty_guided_Optimization}). When censoring points based on the IS-MI estimator we used a conservative decoder uncertainty threshold, defined as the $75^{th}$ percentile of the IS-MI estimator values observed on training data.

\textbf{Bayesian optimization.} The single task Gaussian Process is initially trained on 1,000 images sampled at random from the training set that we then embed in latent space. We use the Expected Improvement as our acquisition function and sequentially generate 10 new digits overall (re-training the Gaussian Process after each acquisition). When leveraging the IS-MI estimator to guide the optimization, we censor proposal points based on the same uncertainty threshold as for gradient ascent ($75^{th}$ percentile of the IS-MI estimator values on training data). 

Results for the gradient ascent and Bayesian Optimization experiments are presented on Figure~\ref{Sec5_Fig_MNIST_Top_images}. Using the uncertainty of the decoder during gradient ascent of Bayesian Optimization helps maximizing thickness of the generated digits while preventing the them to be invalid white `blobs'.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Arithmetic expression experiments}
\label{Appendix_C_Arithmetic_expression}

\subsection{Data}
\label{Appendix_C.1_Arithmetic_expression_Data}

\textbf{Data source.} We use the same dataset as in \citet{kusner2017grammar} which consists of univariate arithmetic expressions that are randomly generated from the following grammar:
\vspace{-4mm}
\begin{center}
\resizebox{0.6\textwidth}{!}{
\includegraphics[scale=1]{Appendix/Images/Arithmetic expressions/Grammar_rules.png}
}
\end{center}
\vspace{-4mm}
where S and T are non-terminals and the symbol `|' separates the possible production rules generated from each non-terminal. For instance, the following univariate arithmetic expressions can be generated from this grammar: $\sin(2)$, $x/(3+1)$, $2+x+\sin(1/2)$, and $x/2 * \exp(x)/\exp(2 * x)$. 

\textbf{Target.} The objective of the optimization in this setting is to find an expression that minimizes the mean squared error (MSE) with respect to a predefined target expression: $1/3 * x * \sin(x*x)$. Specifically, we measure the MSE between the target and proposal expressions over 1000 input  values $x$ that  are  linearly-spaced  between $-10$ and $10$.
Since the presence of exponentials in expressions may results in very large MSE values, the black-box objective to maximize is actually defined as $-\log(1+MSE)$.

\subsection{Model details}
\label{Appendix_C.2_Arithmetic_expression_Model}

\textbf{Architecture.} We jointly train a variational autoencoder and a property network which predicts the target defined above. For the VAE, we use an architecture identical to the CVAE in \citet{kusner2017grammar}, with a convolutional neural network (CNN) encoder and a Recurrent Neural Network (RNN) decoder. All model parameters are summarized in table~\ref{Appendix_C_Table_model}.

\begin{table}[h]
\begin{center}
\caption{\textbf{Arithmetic expressions approximation - Model architecture details}}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Component}  & \textbf{Description} \\
\toprule
\textbf{Encoder} & $\bullet$ 3 consecutive 1D convolutional layers with 2,3 and 4 filters respectively (kernel size 5),\\ 
& \quad with batch norm and RELU activations after each convolutional layer \\ 
& $\bullet$ Continuous latent space of dimension 25 \\
\midrule
\textbf{Decoder} &  $\bullet$ A stack of 3 Gated recurrent unit (GRU) layers \citep{cho2014properties} with hidden dimension 100, \\
& \quad dropout 0.2 (between layers) and RELU activations except for the last layer which \\ 
& \quad has a softmax activation over the arithmetic expressions vocabulary \\
\midrule
\textbf{Property} &  $\bullet$ 3-layer feedforward network with 200 units each \\
\textbf{network} & $\bullet$ RELU activations (after each layer except the final one) and dropout 0.2 \\
\bottomrule
\end{tabular}
}
\label{Appendix_C_Table_model}
\end{center}
\end{table}

\textbf{Training.} We train the joint architecture by minimizing the sum of the VAE ELBO and $\log(1+MSE)$ between an expression and the target one (as per Appendix~\ref{Appendix_C.1_Arithmetic_expression_Data}).
We minimize the loss with the Adam algorithm \citep{kingma2017adam} with a learning rate of $10^{-3}$ and batch size of $600$ for 80 epochs. We annealed the KL divergence with a sigmoid schedule for the first 10 epochs to avoid potential posterior collapse.

\subsection{Uncertainty estimators}
\label{Appendix_C.3_Arithmetic_expression_Uncertainty_estimator}

We replicate an analysis analogous to the one described in Appendix~\ref{Appendix_B.3_Digit_generation_Uncertainty_estimator}, sampling 1k points in latent space for each of the 4 datasets. The IS-MI estimator is computed with $100$ samples from model parameters via Monte Carlo dropout and $100$ expressions samples from the importance distribution. Similarly to what we observed before, the distribution of decoder uncertainty values for the `Train', `Test' and `Prior' sets have a strong overlap and are fairly disjoint from the `Far from prior' distribution (Fig.~\ref{Appendix_C_Fig_decoder_uncertainty_estimator}).

\begin{figure}[ht]
\vspace{0pt}
\resizebox{0.5\textwidth}{!}{
    \includegraphics[scale=1]{Appendix/Images/Arithmetic expressions/Uncertainty_decoder_ISMI.png}
}
\hfill
\resizebox{0.48\textwidth}{!}{
    \includegraphics[scale=1]{Appendix/Images/Arithmetic expressions/Uncertainty_decoder_TIMI.png}
}
\caption{\textbf{Arithmetic expressions approximation - Decoder Uncertainty distribution}. Analysis for the IS-MI estimator (left) and the TI-MI estimator (right). Both provide relatively good separation of `out-of-distribution' points in this setting.}
\label{Appendix_C_Fig_decoder_uncertainty_estimator}
\end{figure}

\subsection{Detailed optimization results}
\label{Appendix_C.4_Arithmetic_expression_Optimization}
We follow the gradient ascent and Bayesian Optimization approaches described at the beginning of \S~\ref{Sec5_Experiments}. We provide below all hyperparameter values used in each method.

\textbf{Gradient ascent.} We start from 500 point sampled at random from the training set and then embedded in latent space. We perform 10 gradient update steps with a value of alpha equal to 10 (using notations from \S~\ref{Sec4_Uncertainty_guided_Optimization}). When censoring points based on the IS-MI estimator we used a decoder uncertainty threshold defined as the $95^{th}$ percentile of the IS-MI estimator values observed on training data. Detailed results in Table~\ref{Appendix_C_Table_AE_GA_results}. 

\input{Appendix/Arithmetic_expressions_results_GA}

\textbf{Bayesian optimization.} The single task Gaussian Process is initially trained on 500 expressions sampled at random from the training set that we then embed in latent space. We sequentially generate 250 new arithmetic expressions (re-training the Gaussian Process after each acquisition). When using the IS-MI estimator to guide the optimization, we censor proposal points based on the same uncertainty threshold as for gradient ascent (ie., $95^{th}$ percentile of the IS-MI estimator values observed on training data). Detailed results in Table~\ref{Appendix_C_Table_AE_BO_results}.

\input{Appendix/Arithmetic_expressions_results_BO}

In both the gradient ascent and Bayesian Optimization experiments, we obtain higher values of the black-box objective as well as a higher proportion of valid generated expressions (nearing 100\% in both cases) when leveraging decoder uncertainty. We get comparable results with the IS-MI and TI-MI estimators in this setting with relatively short sequences.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Molecule generation experiments with CVAE}
\label{Appendix_D_Molecule_CVAE}

\subsection{Data}
\label{Appendix_D.1_Molecule_CVAE_Data}

\textbf{Data source.} We use a dataset of 250k drug-like molecules from the ZINC database \cite{Irwin2012ZINCAF}. Each molecule is represented via its SMILES representation \cite{weininger88smiles}, ie. as a sequence of characters (from a vocabulary of 34 elements, plus the padding character). Following \cite{Gomez_Bombarelli_2018}, molecule length is capped at 120, and shorter strings are space-padded to this length.

\textbf{Target.} The black-box objective in this set of experiments is the `penalized logP', defined as the octanol-water partition coefficient penalized by the synthetic accessibility score and the number of long cycles. We follow prior work \cite{kusner2017grammar, dai2018syntaxdirected, jin2019junction, liu2020chanceconstrained} and compute this metric as follows:
\begin{equation}
    \text{Penalized $\log$ P}(x) = \widehat{\log P(x)} - \widehat{SAS(x)} - \widehat{cycle(x)}
\end{equation}
where $\log P(x)$ is the octanol-water partition coefficient, $SAS(x)$ is the synthetic accessibility score, $cycle(x)$ counts the number of rings that have more than six atoms, and the\; $\widehat{    }$\; operator represents the standard normalization based on the raw training subset from ZINC (ie. subtracting the mean of the training set, and dividing by the standard deviation).

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Appendix/Joint_training_architecture.png}
    \caption{\textbf{Joint training architecture} A Variational autoencoder (VAE) is jointly trained with an auxiliary network predicting the value of the black-box objective from the latent space encoding. Optimization is then carried out in latent space via gradient ascent or Bayesian optimization}
    \label{Appendix_Figure_Joint_training_architecture}
\end{figure}

\subsection{Model details}
\label{Appendix_D.2_Molecule_CVAE_Model}

\textbf{Architecture.} We adopt a model architecture similar to \citet{Gomez_Bombarelli_2018}: the encoder is comprised of 3 convolutional layers, the decoder is composed of a stack of 3 GRU layers \citep{cho2014properties} and the property network has a simple feed forward architecture with 3 hidden layers. A detailed description is provided in Table~\ref{Appendix_D_Table_model}.

\begin{table}[h]
\begin{center}
\caption{\textbf{Molecular generation with CVAE - Model architecture details}}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Component}  & \textbf{Description} \\
\toprule
\textbf{Encoder} & $\bullet$ 3 consecutive 1D convolutional layers with 9,9 and 10 filters respectively,\\ 
& \quad kernel sizes 9,9 and 11 respectively, with batch norm and RELU activations \\
& \quad after each convolutional layer \\ 
& $\bullet$ Continuous latent space of dimension 56 \\
\midrule
\textbf{Decoder} &  $\bullet$ A stack of 3 Gated recurrent unit (GRU) layers \citep{cho2014properties} with hidden dimension 500, \\
& \quad with dropout 0.2 (between layers) and RELU activations except for the last \\ 
& \quad layer which has a softmax activation over the SMILES vocabulary.\\
& $\bullet$ At each step, the character generated at the previous step is concatenated \\ 
& \quad with the latent embedding and fed as input\\
\midrule
\textbf{Property} &  $\bullet$ 3-layer feedforward network with 1,000 units each \\
\textbf{network} & $\bullet$ RELU activations (after each layer except the final one) and dropout 0.2 \\
\bottomrule
\end{tabular}
}
\label{Appendix_D_Table_model}
\end{center}
\end{table}

\textbf{Training. } The total loss we minimize is the sum of the VAE ELBO and the MSE loss on the black-box property prediction task.
We train the network with the Adam algorithm \cite{kingma2017adam} with a learning rate of $5.10^{-4}$ (reduced by a factor 2 with a patience of 10 epochs) for 150 epochs total. We anneal the KL divergence with a sigmoid schedule for the first 30 epochs to avoid potential posterior collapse.
We also use teacher forcing on the character sampled at each time step in the decoder during training, and gradient clipping (upper bound set to 10) to avoid exploding gradients.

\subsection{Uncertainty estimators}
\label{Appendix_D.3_Molecule_CVAE_Uncertainty_estimator}
Similar to the previous two experimental settings, we first assess our uncertainty estimator by examining the distribution of its values on the 4 datasets defined in Appendix~\ref{Appendix_B.3_Digit_generation_Uncertainty_estimator}, using 1k samples for each dataset. We compute the IS-MI estimator based on Algorithm~\ref{Sec3_Algorithm_IS-MI}, use Monte Carlo dropout sampling to obtain 100 samples from decoder parameters, and sample 100 molecule SMILES from the importance distribution (Fig.~\ref{Appendix_D_Fig_decoder_uncertainty_estimator}). In this setting, the TI-MI estimator provides very poor uncertainty estimates as it assigns very low uncertainty values for a majority of `out-of-distribution' points that were sampled far from the prior (Fig.\ref{Appendix_D_Fig_decoder_uncertainty_estimator}).
This is consistent with what we see in Fig.\ref{Fig_CVAE_Uncertainty_estimator}b. In this experiment, we analyze the proportion of valid decodings when keeping the x\% points we are most certain about, based on the various estimators considered (ie. IS-MI, NLLP and TI-MI). If low uncertainty for a given estimator corresponds to high validity, then the \% of valid decodings should increase as we keep a narrower set of most confident points. This is what we observe for the IS-MI estimator, unlike the TI-MI estimator which incorrectly selects points leading to invalid decodings as the lowest uncertainty points.

\begin{figure}[ht]
\vspace{0pt}
\resizebox{0.5\textwidth}{!}{
    \includegraphics[scale=1]{Appendix/Images/CVAE/CVAE_uncertainty_decoder_ISMI.png}
}
\hfill
\resizebox{0.48\textwidth}{!}{
    \includegraphics[scale=1]{Appendix/Images/CVAE/CVAE_uncertainty_decoder_TIMI.png}
}
\caption{\textbf{Molecular generation with CVAE - Decoder uncertainty distribution}. Analysis for the IS-MI estimator (left) and the TI-MI estimator (right)}
\label{Appendix_D_Fig_decoder_uncertainty_estimator}
\end{figure}

\subsection{Detailed optimization results}
\label{Appendix_D.4_Molecule_CVAE_Optimization}
\textbf{Gradient ascent.} We start from 200 point sampled at random from the training set and embedded in latent space. We perform 10 gradient update steps with a value of alpha equal to 20. In the experiments where we impose a maximum threshold on the decoder uncertainty, we set that threshold as the $99^{th}$ percentile of corresponding estimator values observed on the training data. Leveraging the IS-MI estimator during optimization helps reaching higher values of the black-box objective (Table~\ref{Appendix_D_Table_CVAE_GA_results}). While constraints imposed with the NLLP baselines do help maintaining a higher \% of valid decodings, using this baseline is detrimental to optimization performance.

\input{Appendix/CVAE_results_GA}

\textbf{Bayesian Optimization.} We train a single task Gaussian Process (GP) on 500 points sampled at random from the training set and embedded in latent. We use the Expected Improvement as our acquisition function, sequentially generate 100 new molecules (and re-train the GP after each acquisition). Similar to our gradient ascent experiments, we set the uncertainty threshold to the $99^{th}$ percentile of corresponding estimator values observed on the training data. We observe that when using the IS-MI estimator, we not only increase the \% of valid decodings by 1.5-10x compared to baselines, but we also reach higher values of the `penalized logP' objective across all settings (Table.~\ref{Appendix_D_Table_CVAE_BO_results}). These results are robust to the choice of distribution decile used to define the uncertainty threshold, as shown in Table.\ref{Appendix_D_Table_CVAE_BO_threshold_analysis}.

\input{Appendix/CVAE_results_BO}

\input{Appendix/CVAE_threshold_analysis_BO}
\vspace{-3mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Molecule generation experiments with JTVAE}
\label{Appendix_E_Molecule_JTVAE}

\subsection{Data}
\label{Appendix_E.1_Molecule_JTVAE_Data}
We refer the reader to Appendix~\ref{Appendix_D.1_Molecule_CVAE_Data} as we used the same experimental setting as for the CVAE experiments, ie. we train our model on a subset of 250k drug-like molecules from the ZINC database, and seek to optimize the `penalized logP' metric.

\subsection{Model architecture}
\label{Appendix_E.2_Molecule_JTVAE_Model}

\textbf{Architecture.} We jointly train a Junction Tree VAE model (JT-VAE) with a property network predicting the `penalized logP' property based on latent representation, leveraging the same architecture design as in \citet{jin2019junction} for the constrained optimization task. The only difference we introduce is the incorporation of dropout layers in the junction tree decoder and graph decoder to allow sampling from the decoder parameters via Monte Carlo dropout (see Table~\ref{Appendix_E_Table_model}).

\begin{table}[h]
\begin{center}
\caption{\textbf{Molecular generation with JT-VAE - Model architecture details.} All components are identical to the JT-VAE architecture used for constrained optimization from \citet{jin2019junction}, except for the dropout layers detailed below}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Component}  & \textbf{Description} \\
\toprule
\textbf{Encoder} & $\bullet$ Junction tree encoder and molecular graph encoder  \\ 
& $\bullet$ Continuous latent space of dimension 56 \\
\midrule
\textbf{Decoder} &  $\bullet$ Junction tree decoder with dropout layer (0.2 drop rate) applied to the input and the output \\
& \quad of the GRU used for message passing, and dropout layer (0.2 rate) applied right before\\
& \quad  the final layer for both the topology prediction and node prediction networks \\ 
& $\bullet$ Molecular graph decoder with dropout layer (0.2 rate) applied right before the final layer \\
& \quad of the subgraph prediction network
\\
\midrule
\textbf{Property} &  $\bullet$ 2-layer feedforward network with 450 units and 1 unit resp., $\tanh$ activation for the first \\
\textbf{network} & \quad layer, no activation for the second, and dropout 0.2 before both layer\\
\bottomrule
\end{tabular}
}
\label{Appendix_E_Table_model}
\end{center}
\end{table}

\textbf{Training.} In line with the experiments discussed above, and following the same training procedure as per \citet{jin2019junction}, we minimize the sum of the VAE loss and the MSE on the black-box prediction task.

\subsection{Uncertainty estimators}
\label{Appendix_E.3_Molecule_JTVAE_Uncertainty_estimator}
We compute the IS-MI estimator based on Algorithm~\ref{Sec3_Algorithm_IS-MI}. Following notations from Algorithm~\ref{Sec3_Algorithm_IS-MI}, we sample a molecule $\tilde{y}_s$ by successively decoding from the junction tree decoder and then the graph decoder. We then decompose $\log{p_{s,m}}$ as the sum of the log probabilities corresponding to each prediction made by the junction tree decoder and graph decoder (for the graph outcome generated in the previous step), namely the topology and node predictions in the junction tree decoder, and the subgraph prediction in the graph decoder. 
Similar to the setting discussed in Appendix~\ref{Appendix_B.3_Digit_generation_Uncertainty_estimator}, we inspect the distribution of the IS-MI values obtained on the same 4 datasets (using 1k samples for each dataset), and estimate the mutual information with $100$ samples from decoder parameters and $100$ molecules sampled from the importance sampling distribution. We observe similar results as before: overlap between IS-MI distributions on the `Train', `Test' and `Prior' sets. This overlap is stronger between the first two sets as the embedding of the training data in latent does not necessarily follow a standard normal distribution after model training. The distribution of IS-MI values on the `Far from prior' set is disjoint from the first 3 sets, with the highest values obtained on this set.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.45\textwidth]{Appendix/Images/JTVAE/Uncertainty_decoder_histogram_50bins_Uncertainty_MI_Method1_Importance_Sampling_fast_Jun13_stddev_10.0_20_100_100_zdim_56_vf.png}
    \caption{\textbf{Molecular generation with JT-VAE - Decoder uncertainty distribution.}}
    \label{Appendix_Fig_JTVAE_Uncertainty_estimator}
\end{figure}

\subsection{Detailed optimization results}

All molecules generated by the JT-VAE are valid by design. However, not all generated molecules will be of high \emph{quality}, as measured for example by the quality filters from \citet{Brown_2019} discussed in \S~\ref{Sec5_JTVAE}. Since in this molecular generation setting, our objective is to generate new molecules with high penalized logP values that could be used as potential drugs, we want to ensure that the candidate drugs we shortlist for further investigation also pass these quality filters. In each optimization experiment described below, we prioritize a small number of candidate molecules (eg., top 10 molecules with highest logP values) and use the quality filters from \citet{Brown_2019} as a proxy for the subsequent (costly) verification of these candidates by medicinal chemists. If a generated molecule does not pass these quality filters, its penalized logP value is assigned to a default value (eg., average penalized logP value on the training set). In all optimization experiments, we estimate mutual information with $100$ samples from decoder parameters, and a single molecule sampled from the importance distribution.

\label{Appendix_E.4_Molecule_JTVAE_Optimization}
\textbf{Gradient ascent.} We start from 100 molecules sampled at random from the training set, that we then embed in latent space. We perform 100 gradient update steps with a large value of $\alpha$ (as per notations in \S~\ref{Sec4_Uncertainty_guided_Optimization}), eg., 100 or 200. This leads state-of-the-art performance in terms of penalized logP values (see Table~\ref{Appendix_E_Table_Molecular_generation_top_performance} and Fig.~\ref{Fig_Appendix_JTVAE_GA_Top_molecules_generated}). However, as we move `further away' in latent space, the quality of generated molecules tends to degrade.
By setting an upper bound on the uncertainty of the decoder (eg., $95^{th}$ percentile of IS-MI values observed on the training data) during optimization, we are able to generate molecules with both high penalized logP values and high quality (see Table~\ref{Appendix_E_Table_JTVAE_GA_results}). Selecting different uncertainty threshold values enable to reach different trade-offs between quality and black-box objective.
A similar approach with upper bounds in terms of NLLP values (eg., threshold defined as $95^{th}$ or $99^{th}$ percentiles of NLLP values on the training data) does help promoting high quality molecules but leads to much lower penalized logP values.

\input{Appendix/Molecular_generation_top_optim}
\input{Appendix/JTVAE_results_GA}

\begin{figure*}[h]
    \centering
    \includegraphics[width=5.5in,scale=1.0]{Appendix/Images/JTVAE/Uncertainty_guided_optim_top_molecules.png}
    \caption{\textbf{Top molecules generated via gradient ascent with a JT-VAE} ($\alpha$ = 200).}
    \label{Fig_Appendix_JTVAE_GA_Top_molecules_generated}
\end{figure*}

\textbf{Bayesian Optimization.} We train a single task Gaussian Process (GP) on 500 points sampled at random from the training set and embedded in latent. We use the Expected Improvement as our acquisition function, sequentially generate 500 new molecules (and re-train the GP after each acquisition). In experiments in which we impose an upper bound on decoder uncertainty, we set that bound as the $99^{th}$ percentile of decoder uncertainty values observed on the training data. Similar to what we observe in the gradient ascent experiments, leveraging the IS-MI estimator helps generating candidate molecules with both high `penalized logP' values and high quality (Table~\ref{Appendix_E_Table_JTVAE_BO_results}). The NLLP of proposal points at each step of the batch Bayesian Optimization tend to always be above the NLLP threshold ($99^{th}$ percentile of values on the training data). In these situations, we select the point with lowest NLLP value from the proposal batch as described in \S~\ref{Sec5_Experiments}. This explains why the results obtained with NLLP are closer to what we obtain without any constraint during optimization, and why we lose the ability to increase the quality of generated molecules in this setting.

\input{Appendix/JTVAE_results_BO}

\vspace{-2mm}
\section{Discussion}

\vspace{-1mm}
\textbf{Strengths of the approach. } Leveraging the uncertainty of the decoder is a \emph{simple} yet \emph{effective} approach to promote the validity or quality of objects generated while optimizing a given black-box property in VAE latent space, without compromising on the final objective. It is simple because it does not impose changes to the model architecture nor the training procedure to work. The only requirement is the ability to sample from decoder parameters -- which we have achieved in this work via Monte Carlo dropout given its practical simplicity.
As we have demonstrated, the value of the approach is not limited to a particular decoder architecture, and the proposed scheme delivered compelling results across convolutional, recurrent and graph neural network decoders.
In several of the experimental settings, using the decoder uncertainty also helped attaining substantially higher values of the black-box objective during optimization (\S~\ref{Sec5_CVAE} in particular), as the optimization procedure could use the decoder uncertainty to avoid exploring regions which are doomed to lead to invalid decodings.

\textbf{Limitations. } The main limitation of the approach is the computation overhead resulting from the estimation of the decoder uncertainty. While the computation of the IS-MI estimator is easily parallelizable, it nonetheless increases the time required to complete each optimization step (this overhead depends on a number of factors including number of samples for IS-MI evaluation, decoder architecture and hardware used). 
In the setting we have considered, the black-box objective may be very expensive to evaluate (eg., a costly wet lab experiment, a time-consuming simulation). Consequently, the ability to generate strong candidates that satisfy non-obvious validity or quality requirements takes precedence over the computation time needed to generate them. 
Furthermore, the computation overhead may also not be a primary concern in situations where the uncertainty of the decoder helps achieve a superior optimum compared to what could have been achieved without it (eg., \S~\ref{Sec5_CVAE}). 
However, in settings in which the decoder uncertainty does not help reaching superior optima and in which the cost of evaluating candidate objects is low, then the decision of whether to leverage the decoder uncertainty or not should be based on the relative trade-off between validity gains during optimization and computation overhead.

\textbf{What we thought would work, but did not work. } In \S~\ref{Sec4_Uncertainty_guided_Optimization} we presented two possible approaches for incorporating the decoder uncertainty within Bayesian Optimization in latent space: the uncertainty censoring approach and the uncertainty-aware surrogate model approach. While we have used the former extensively across experiments in \S~\ref{Sec5_Experiments}, the latter has not delivered strong results. A possible interpretation would be that the latter approach may heavily penalize optimization directions for which the gradient of the black-box objective is aligned with the gradient of decoder uncertainty, while the former would allow moving along these directions up until the uncertainty threshold is exceeded.

\textbf{Societal impact. } We introduce a general approach to improve the black-box optimization of complex high-dimensional discrete objects in VAE latent space. The potential applications of this field are very broad -- from more effective drug or protein design to improved automatic program synthesis. The estimator of decoder uncertainty we have introduced may also be leveraged in a wide range of other areas that would benefit from reliable uncertainty estimates in high dimensional settings (eg., active learning, outlier detection).

\vspace{-2mm}
\section{Reproducibility}
\vspace{-1mm}
\textbf{Code and dependencies. }
The codebase was fully developed in Pytorch (v1.7) \cite{NEURIPS2019_9015}. All Bayesian Optimization experiments were conducted with the BoTorch package \cite{balandat2020botorch}. For molecule generation experiments, we used the rdkit (https://github.com/rdkit/rdkit) and guacamol packages (https://github.com/BenevolentAI/guacamol). 
In addition, we have made use of the code repositories listed in Table~\ref{Appendix_Code_repos} when developping our approach and conducting experiments.

\begin{table}[h]
\setlength\belowcaptionskip{0.5pt}
\begin{center}
\caption{\textbf{Code repositories used}}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Setting} &  \textbf{Data source} \\
\toprule
Grammar VAE & \url{https://github.com/mkusner/grammarVAE} \\
JT-VAE (official repo) & \url{https://github.com/wengong-jin/icml18-jtnn} \\
JT-VAE (Python 3 implementation)& \url{https://github.com/Bibyutatsu/FastJTNNpy3} \\
Quality filters & \url{https://github.com/PatWalters/rd_filters} \\
\bottomrule
\end{tabular}
}
\label{Appendix_Code_repos}
\end{center}
\end{table}
\vspace{-2mm}
\textbf{Compute resources. } All experiments were carried out with a single GPU (Titan RTX). We summarize compute usage for the different experiments in Table~\ref{Appendix_Compute_resources}.

\begin{table}[h]
\begin{center}
\caption{\textbf{Compute usage per experiment summary}}
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
 &   \multicolumn{4}{c}{\textbf{Avg. compute time per iteration (GPU hrs)}} \\
\textbf{Experiment} & \textbf{Digit generation} & \textbf{Arithmetic expression} & \textbf{CVAE} & \textbf{JTVAE}\\
\toprule
Model training & <0.5 & 0.5 & 12 & 50 \\
Decoder uncertainty histograms & 2 & 2 & 3 & 40 \\
Bayesian optimization & <0.5 & <0.5 & 2 & 14 \\
Gradient ascent & <0.5 & <0.5 & 1 & 5 \\
\bottomrule
\end{tabular}
}
\label{Appendix_Compute_resources}
\end{center}
\end{table}

\vspace{-2mm}
\textbf{Data sources. } We list the raw data sources used for all experiments in Table~\ref{Appendix_Data_sources}.

\vspace{-1mm}
\begin{table}[h]
\setlength\belowcaptionskip{1pt}
\begin{center}
\caption{\textbf{Data sources summary}}
\resizebox{\textwidth}{!}{
\begin{tabular}{ll}
\toprule
\textbf{Setting} &  \textbf{Data source} \\
\toprule
Digit generation & \url{http://yann.lecun.com/exdb/mnist/} \\
Arithmetic expression & \url{https://github.com/mkusner/grammarVAE/tree/master/data} \\
Molecular generation & \url{https://github.com/aspuru-guzik-group/chemical_vae/tree/master/models/zinc} \\
\bottomrule
\end{tabular}
}
\label{Appendix_Data_sources}
\end{center}
\end{table}