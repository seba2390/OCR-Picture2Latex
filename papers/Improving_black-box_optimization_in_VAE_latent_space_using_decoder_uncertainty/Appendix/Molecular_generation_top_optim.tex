\begin{table}[h]
\begin{center}
\caption{\textbf{Molecular generation - Top optimization performance.} We achieve state-of-the-art performance on the molecular generation task using a JT-VAE model jointly trained with an auxiliary network predicting penalized logP from latent embeddings (as per \S 3.3 of \cite{jin2019junction}) and performing gradient ascent as described in \S~\ref{Sec4_Uncertainty_guided_Optimization}. Results were obtained by embedding in latent space 100 points selected at random from the test set and then performing 100 gradient updates with $\alpha = 200$. We report mean performance over 10 runs, as well as the best generated molecules across these 10 runs.}
\resizebox{\textwidth}{!}{
\begin{tabular}{llcccc}
\toprule
\textbf{Model} & \textbf{Optimization method} &  \multicolumn{3}{c}{\textbf{Penalized logP}} \\
&  & \textbf{Top 1} $\uparrow$ & \textbf{Top 2} $\uparrow$ & \textbf{Top 3} $\uparrow$\\
\toprule
JT-VAE \cite{jin2019junction} & Bayesian Optimization & 5.30 & 4.93 & 4.49 \\
MolDQN \cite{Zhou_2019MolDQN} & Reinforcement learning & 11.84 & 11.84 & 11.82 \\
GraphAF \cite{shi2020graphaf} & Reinforcement learning & 12.23 & 11.29 & 11.05 \\
CCGF \cite{liu2020chanceconstrained} & Chance-constrained optimization & 12.32 & 11.79 & 11.61 \\
ChemBO \cite{korovina2019chembo} & Bayesian Optimization & 18.39 & - & - \\
JT-VAE \cite{tripp2020sampleefficient} & Bayesian Optim. \& retraining (median of 5 runs) & 21.20 & 15.34 & 15.34 \\
JT-VAE \cite{tripp2020sampleefficient} & Bayesian Optim. \& retraining (best over 5 runs) & 27.84 & 27.59 & 27.21\\ 
\midrule
JT-VAE (ours) & Gradient ascent (mean of 10 runs) & 23.65	& 21.17	& 19.45\\
JT-VAE (ours) & Gradient ascent (best over 10 runs) & \textbf{30.81} & \textbf{30.00} & \textbf{29.82}\\
\bottomrule
\end{tabular}
\label{Appendix_E_Table_Molecular_generation_top_performance}
}
\end{center}
\end{table}