\section{Background}
\label{Sec2_Background}

\subsection{Optimization of high-dimensional discrete objects with generative models}
\label{Sec2_Background_Optim}

Focusing on molecular generation, \citet{Gomez_Bombarelli_2018} propose to train a VAE to learn a distribution over the so-called SMILES representation of molecules (ie. linear sequences of characters) \citep{weininger88smiles}, and subsequently perform the optimization in the latent space. 
Since the SMILES representation follows strict syntactic requirements that are not explicitly enforced by the generative model, promising points in latent space may be decoded into invalid molecules. 
To improve the validity of decoded sequences, \citet{kusner2017grammar} and \citet{dai2018syntaxdirected} develop task-specific grammar rules into the VAE decoder, focusing on use cases in molecular and computer program generation. However, crafting the corresponding rules requires domain-specific knowledge, needs to be designed from scratch for each new task, and may not be straightforward to elicit in the first place (e.g., digit generation example in \ref{Sec5.1_MNIST}). \citet{liu2020chanceconstrained} propose instead to formulate the problem as a chance-constrained optimization task to simultaneously optimize the target property as well as the probability to generate valid sequences. 
%While domain-agnostic, the approach changes the learning process of the model which may be costly (as several new hyperparameters are introduced) and may impact the initial optimization in non-obvious ways.
A different line of research has focused on representing high-dimensional structured objects as graphs instead \citep{duvenaud2015convolutional,li2018learning}. The Junction Tree VAE (JT-VAE) \citep{jin2019junction} generates systematically valid molecular graphs, by first generating a tree-structured scaffold over a finite set of molecular clusters, and then assembling these clusters back into molecules with a message passing network. The MolDQN \citep{Zhou_2019MolDQN} casts the optimization problem as a reinforcement learning task (double Q-learning), which allows in turn to more naturally extend to simultaneous optimization of different objectives. GraphAF \citep{shi2020graphaf} combines the strengths of autoregressive and flow-based approaches to efficiently generate realistic and valid molecular graphs.
Lastly, \citet{tripp2020sampleefficient} show that the black-box optimization performance can be further enhanced by iteratively retraining the generative model on the data points selected during optimization, with weights that directly depend on their objective function value.

Our approach deviates from all the above in that it is is representation-agnostic (works with sequences or graphs), does not change the model architecture nor the learning procedure and can be combined with several of these approaches to reach even stronger optimization performance.

\subsection{Quantifying model uncertainty}
\label{Sec2_Background_Uncertainty}
Adopting a Bayesian viewpoint, the overall uncertainty of a model in a given region of the input space can be broken down into two types of uncertainty \citep{kendall2017uncertainties}:
\begin{itemize}
    \item \textbf{Epistemic uncertainty:} Uncertainty due to lack of knowledge about that particular region of the input space --- the posterior predictive distribution is broad in that region due to lack of information that can be reduced by collecting more data;
    \item \textbf{Aleatoric uncertainty:} Uncertainty due to inherent stochasticity/noise in the observations in that region --- collecting additional data would not further reduce that uncertainty.
\end{itemize}

We denote input points as $x$, outputs as $y$ and the training data as $\mathcal{D}$. The total uncertainty $\mathcal{U}$ of a model at an input point $x$ is typically measured by the predictive entropy, ie. the entropy of the predictive posterior distribution $P(y|x,\mathcal{D})$:
\begin{equation}
%\setlength{\belowdisplayskip}{5pt}
\mathcal{U}(x) = \mathcal{H}(P(y|x,\mathcal{D})) = \sum_{y} - P(y|x,\mathcal{D}) \log P(y|x,\mathcal{D}) dy.
\end{equation}

Denoting $P(\theta|\mathcal{D})$ the posterior distribution over model parameters $\theta$, we can further decompose the predictive entropy $\mathcal{U}$ as the sum of two terms:
\begin{align}
\mathcal{U}(x) = \underbrace{(\mathcal{H}(P(y|x,\mathcal{D})) - \EX_{P(\theta|\mathcal{D})}(\mathcal{H}(P(y|x,\theta)))}_\text{Mutual Information $\mathcal{M}$} +   \underbrace{\EX_{P(\theta|\mathcal{D})}(\mathcal{H}(P(y|x,\theta)))}_\text{Expected entropy $\mathcal{E}$}
\end{align}
%nonumber
The first term --- the Mutual Information $\mathcal{M}$ between model parameters $\theta$ and the prediction $y$ --- is a measure of epistemic uncertainty, as it quantifies the magnitude of the change in model parameters that would result from observing $y$. If the model is uncertain about its prediction for $y$, the change in model coefficients from observing $y$ should be high. Conversely, if the model is confident about its prediction for $y$, model parameters will not vary from observing $y$:
\begin{equation}
\label{Sec2_MI_equation}
\mathcal{M}(x) = \mathcal{H}(P(y|x,\mathcal{D})) - \EX_{P(\theta|\mathcal{D})}(\mathcal{H}(P(y|x,\theta))).
\end{equation}
The second term --- the Expected Entropy $\mathcal{E}$ --- is a measure of the residual uncertainty, ie. the aleatoric uncertainty:
\begin{equation}
\mathcal{E}(x) = \EX_{P(\theta|\mathcal{D})}(\mathcal{H}(P(y|x,\theta))).
\end{equation}
In high dimensional settings, an exact estimation of these different quantities is not tractable, therefore, several approximations and heuristics have been introduced.
The softmax variance, i.e. the variance of predictions across model parameters, has been shown to approximate epistemic uncertainty well in certain settings \citep{carlini2016evaluating, feinman2017detecting,smith2018understanding}. 

In the context of sequential data, the inherent structure in the data generating process often introduces strong dependencies between the output dimensions, e.g., the tokens in a generated sentence. In cases where there exist weak correlations between tokens, quantifying the different types of uncertainties above can be made tractable by ignoring these dependencies \citep{malinin2020uncertainty}, in which case the predictive entropy for a sequence $y = (y_1, y_2, ..., y_L)$ may be approximated as the sum of token-level predictive entropies over the $L$ tokens:
%\vspace{-2mm}
\begin{equation} 
\mathcal{U}(x)
= \sum_{l=1}^L \mathbb{E}_{P(y|x,\mathcal{D})}[\log P(y_l|x,y_{k<l},\mathcal{D})]  \approx \sum_{l=1}^L \mathbb{E}_{P(y_l|x,y_{k<l},\mathcal{D})}[\log P(y_l|x,y_{k<l},\mathcal{D})] .
\label{Sec2_Equation_Token_level_MI}
\end{equation}

Unlike the standard expectation definition which integrates over all $y$, here only $y_l$ is integrated over, and we condition on $y_{k<l}$, which are obtained from a sample from $P(y|x,\mathcal{D})$. The process is repeated for several of these samples and an average is finally computed to reduce variance.

While the above has been shown to work well in certain experiments \citep{malinin2020uncertainty}, valuable information is being discarded when we ignore dependencies across tokens. These dependencies are likely to be informative in applications such as the ones considered in \S\ref{Sec5_CVAE}. Alternative approaches have been suggested which make use of domain-specific metrics in Natural Language Processing such as the BLEU score \citep{Xiao2019Wat}, but these are difficult to extend to other domains. Lastly, a naive Monte Carlo estimation is expected to perform poorly in high dimensions since the majority of samples will be in regions with negligible contribution to the sum (Appendix~\ref{Appendix_Uncertainty_estimator}).