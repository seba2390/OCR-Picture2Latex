\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
%\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
 %\usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[table,xcdraw]{xcolor}         % colors

%My packages
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{todonotes}
\usepackage{hyperref}
%\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=blue}
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\input{math_commands.tex}
\usepackage{url}
\usepackage{amsmath,amssymb,dsfont}
\DeclareMathOperator{\EX}{\mathbb{E}}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithmic}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{subfigure}
\usepackage{textcomp}
\usepackage{multirow}

\usepackage{times}

\title{Improving black-box optimization in VAE latent space using decoder uncertainty}

\author{%
  Pascal Notin$^{1}$\thanks{Correspondence to pascal.notin@cs.ox.ac.uk}
  \hfill
  José Miguel Hernández-Lobato$^{2}$
  \hfill
  Yarin Gal$^{1}$ \\[1.5em]
  $^1$ OATML, Department of Computer Science, University of Oxford \\
  $^2$ Department of Engineering, University of Cambridge \\
 
}

\begin{document}

\maketitle

\begin{abstract}
Optimization in the latent space of variational autoencoders is a promising approach to generate high-dimensional discrete objects that maximize an expensive black-box property (e.g., drug-likeness in molecular generation, function approximation with arithmetic expressions). However, existing methods lack robustness as they may decide to explore areas of the latent space for which no data was available during training and where the decoder can be unreliable, leading to the generation of unrealistic or invalid objects. We propose to leverage the epistemic uncertainty of the decoder to guide the optimization process. This is not trivial though, as a naive estimation of uncertainty in the high-dimensional and structured settings we consider would result in high estimator variance. To solve this problem, we introduce an importance sampling-based estimator that provides more robust estimates of epistemic uncertainty. Our uncertainty-guided optimization approach does not require modifications of the model architecture nor the training process. It produces samples with a better trade-off between black-box objective and validity of the generated samples, sometimes improving both simultaneously. We illustrate these advantages across several experimental settings in digit generation, arithmetic expression approximation and molecule generation for drug design.


\end{abstract}

\input{1. Introduction/Introduction}
\input{2. Background/Background}
\input{3. Importance sampling estimator/Importance_sampling_estimator}
\input{4. Uncertainty-guided optimization/Uncertainty_guided_search}
\input{5. Experimental Results/Experiments}
\input{6. Conclusion/Conclusion}


\newpage
\small
\bibliographystyle{unsrtnat}
\bibliography{bibs/references}

\newpage

\appendix

\clearpage
\section*{Appendix}
\input{Appendix/Appendix}

\end{document}
