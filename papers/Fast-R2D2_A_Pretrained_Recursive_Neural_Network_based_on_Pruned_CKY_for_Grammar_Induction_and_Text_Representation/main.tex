\documentclass[11pt,a4paper]{article}
\usepackage{emnlp2022}
\usepackage{times}
% \usepackage{todonotes}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{booktabs} % For formal tables
\usepackage[normalem]{ulem}
\usepackage{xcolor} %%xl: I need xcolour....
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{mathrsfs}
 \usepackage{amssymb}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage[font=rm]{caption}
% \usepackage{subcaption}
\DeclareCaptionType{copyrightbox}
\usepackage{shortvrb}
\usepackage{tabularx}
\usepackage{verbatim}
\usepackage{xspace}
\usepackage{listings}
\usepackage{tikz}
\usepackage{verbatim}
\usetikzlibrary{trees}
\usepackage{forest}
\forestset{
    nice empty nodes/.style={
        for tree={
            s sep=0.1em, 
            l sep=0.33em,
            inner ysep=0.4em, 
            inner xsep=0.05em,
            l=0,
            calign=midpoint,
            fit=tight,
            where n children=0{
               tier=word,
               minimum height=1.25em,
            }{},
            where n children=2{
               l-=1em,
            }{},
            parent anchor=south,
            child anchor=north,
            delay={if content={}{
                    inner sep=0pt,
                    edge path={\noexpand\path [\forestoption{edge}] 
                    			(!u.parent anchor) 
                               -- (.south)\forestoption{edge label};}
                }{}}
        },
    },
}
\lstset{basicstyle=\small\ttfamily,mathescape,columns=fullflexible,keepspaces=true}
\usepackage{fontawesome}
\usepackage[multiple]{footmisc}
\usepackage[all]{nowidow}
\usepackage{balance}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\newcommand{\hx}[1]{{\color{blue}{#1}}}

\usepackage{xspace}
\input{defs.tex}

\def\aclpaperid{149} %  Enter the acl Paper ID here

\setlength\titlebox{8.0cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\usepackage{gnuplottex}
\usepackage[medium,compact]{titlesec}
\usepackage{enumitem}

\DeclareMathOperator*{\pop}{pop}
\DeclareMathOperator*{\len}{len}
\DeclareMathOperator*{\push}{push}
\DeclareMathOperator*{\KL}{KL}
\DeclareMathOperator*{\softmax}{softmax}

\setlist{itemsep=0pt,parsep=0pt}

\input{localmacros}

\title{Fast-R2D2:\vspace{-0.2ex} \\
A Pretrained Recursive Neural Network based on Pruned CKY\vspace{-0.2ex} \\
for Grammar Induction and Text Representation\vspace{-0.85ex}}

\date{}

\author{Xiang Hu\footnotemark[2] \quad Haitao Mi\footnotemark[2]~\thanks{~Work done while at Ant Group. To contact Haitao, haitaomi@global.tencent.com} \quad Liang Li\footnotemark[3] \quad Gerard de Melo\footnotemark[4] \\
 Ant Group\footnotemark[2] \\
 \tt \{aaron.hx, haitao.mi\}@alibaba-inc.com\footnotemark[2] \\
 School of Cyber Science and Technology, Shandong University, China /\\
 Key Laboratory of Cryptologic Technology and \\
 Information Security of Ministry of Education, Shandong University /\\Quancheng Laboratory, China\footnotemark[3]\\ 
 \tt li.liang@sdu.edu.cn\footnotemark[3] \\
 Hasso Plattner Institute / University of Potsdam\footnotemark[4] \\
 \tt gdm@demelo.org\footnotemark[4]  \\}

\begin{document}
\maketitle
\begin{abstract}
Chart-based models have shown great potential in unsupervised grammar induction, running recursively and hierarchically, 
but requiring $O(n^3)$ time-complexity.
The Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pretraining
even with a complex tree encoder, by introducing a heuristic pruning method.
However, its rule-based pruning process suffers from local optima and slow inference. 
In this paper, we propose a unified R2D2 method that overcomes these issues. 
We use a top-down unsupervised parser as a model-guided pruning method, 
which also enables parallel encoding during inference.
Our parser casts parsing as a split point scoring task by first scoring all split points for a given sentence and then 
using the highest-scoring one to recursively split a span into two parts.
The reverse order of the splits is considered as the order 
of pruning in the encoder.
We optimize the unsupervised parser by minimizing the Kullbackâ€“Leibler distance between tree probabilities from the parser and the R2D2 model.
Our experiments show that 
our Fast-R2D2 significantly improves the grammar induction 
quality and achieves competitive results in downstream 
tasks.\footnote{The code is available at: \url{https://github.com/alipay/StructuredLM\_RTDT}}
\end{abstract}

\section{Introduction}
\input{intro.tex}
\input{background.tex}
\input{alg.tex}

\section{Experiments}
\label{sec:exps}
\input{exp.tex}

\section{Related Work}
\label{sec:related_works}
\input{related.tex}

\section{Conclusion}
In this paper, we have presented Fast-R2D2, which improves the performance and inference speed of R2D2 by introducing a fast top-down parser to guide the pruning of the R2D2 encoder.
Pretrained on the same corpus, Fast-R2D2 significantly outperforms sequential Transformers with a similar scale of parameters on classification tasks. 
Experimental results show that Fast-R2D2 is a promising and feasible way to learn hierarchical text representations, which is different from layer stacking models and can also generate interpretable intermediate representations.
As future work, we are investigating leveraging the intermediate representations in additional downstream tasks.


\section{Limitations}
Our approach has three major limitations. First, Fast-R2D2 has shortcomings with regard to cross-sentence tasks due to the lack of cross-attention between sentences. Second, Fast-R2D2 requires greater memory resources for pretraining compared to sequential Transformers.  At each invocation, the composition function takes four inputs and runs on $m$ candidates, which means the total number of calls to the MLP is $4mn$. Hence, the pretraining time of  Fast-R2D2 is about 3 to 4 times that of BERT with 12 layers. Finally, our model does not beat most of the baselines in grammar induction when trained on WSJ only. A side effect of the pruning strategy is that the chart-table actually is a sparse table, which means not all tokens are reconstructed based on complete context.  This issue can be alleviated by pre-training on a large corpus, which is what our method is designed for, and why we introduce the ability to parallelize the computation.

\section{Acknowledgement}
We would like to thank the Aliyun EFLOPS team for their substantial support in designing and providing a cutting-edge training platform to facilitate fast experimentation in this work. We would also like to thank the Zhixiaobao team for their support in applying our model to real applications.

\newpage
\bibliographystyle{acl_natbib} 
\bibliography{anthology}

\onecolumn
\input{appendix.tex}

\end{document}
