% First Paragraph: CYK-based encoder is promising ==> cubic time complexity ==> R2D2 
Compositional, hierarchical, and recursive processing are widely believed to be essential traits of human language across diverse  linguistic theories~\cite{DBLP:journals/tit/Chomsky56,chomsky2014aspects}.
% Discuss about the importance of structures, and different approaches like continuous latent structure and discrete explicit structure.
Chart-based models~\cite{DBLP:journals/corr/MaillardCY17,kim-etal-2019-compound,dblp:conf/naacl/drozdovvyim19,hu-etal-2021-r2d2} have made promising progress in both grammar induction and hierarchical encoding in recent years.
The differential CKY encoding architecture of \newcite{DBLP:journals/corr/MaillardCY17} simulates 
the hierarchical and recursive process explicitly by introducing an energy function to combine all possible derivations when constructing each cell representation.
However, this entails a cubic time complexity, which makes it impossible to scale to large 
language model training like BERT~\cite{devlin2018}. 
Simultaneously, its cubic memory cost also limits the tree encoder's ability to draw on huge parameter models as a backbone.

\newcite{hu-etal-2021-r2d2} introduced a heuristic pruning method, successfully reducing the time complexity 
%to a linear time. 
to a linear number of compositions.
The experiments show that chart-based models exhibit great potential for grammar induction and representation learning when applying a sophisticated tree encoder such as Transformers with large corpus pretraining, leading to a Recursive Transformer based on Differentiable Trees, or R2D2 for short.
However, R2D2's heuristic pruning approach is rule-based and only considers certain composition probabilities.
Thus, trees constructed in this way are not guaranteed to be globally optimal. Moreover, as each step during pruning is based on previous decisions, the entire encoding process is sequential and thus slow in the inference stage.
% TODO: where shall we fit URNNG model?

% \newcite{DBLP:journals/corr/MaillardCY17} propose a differential CKY encoding architecture. 
% They primarily introduce an energy function to combine all possible derivations when constructing each span representation. 
% However, the cubic complexity of the architecture results in that only shallow networks could be applied as the tree encoder and length of input texts is limited. Meanwhile, it relies on annotated downstream tasks to guide the model learn structures. Inspired by this work, R2D2~\cite{hu-etal-2021-r2d2} propose a pruned CKY encoding architecture. By merging cells with high composition probabilities, they can participate in subsequent encoding as one cell, then all cells split them apart could be eliminated. Therefore, the complexity of the CKY encoding architecture is reduced to $O(n)$, which make it possible to apply deep network as the backbone. Meanwhile, they also propose a novel unsupervised pretrain algorithm based on bi-directional language model task, which enables the model learn interpretable syntax structures unsupervisedly. But the disadvantage is that the merge decision is based on composition probabilities in a local window, which is not promised to the global optimal structure. URNNG~\cite{dblp:conf/naacl/kimrykdm19} design a structured language model based on another idea. They build a shift-reduce parser and a CKY parser with different parameter set. CKY parser is used to sample trees to optimize the shift-reduce parser and language model loss. Language model Loss is then used as the reward to the trees sampled by the CKY parser. As the CKY parser is optimized by reinforcement learning, the feedback is not straightforward. And the reward of reinforcement learning is a one-way language model, which the following information could not be leveraged.

% Second paragraph: Our intuitions and solution 

%In this work, we solve these issues in an unified method by introducing a global pruning strategy by using a light and fast top-down parser, where we cast parsing as split point scoring.

In this work, we resolve these issues by proposing a unified method with a new global pruning strategy based on a lightweight and fast top-down parser. 
We cast parsing as split point scoring, where we first encode the input sentence with a bi-directional LSTM, and score all split points in parallel.
Specifically, for a given sentence, the parser first scores each split point between words in parallel by looking at its left and right contexts, and then recursively splits a span (starting with the whole sentence) into two sub-spans by picking the highest-scoring split point among the current split candidates.
% During training, we incorporate sampling in the recursive splitting process, where, in each step, we sample a split point with respect to the score distribution in the current span and simplify the process as a sorting problem.
Subsequently, the reverse order of the sorted split points can serve as the merge order to guide the pruning of the CKY encoder, which enables the encoder to search for more reasonable trees.
As the gradient of the pretrained component cannot be back-propagated to the parser, inspired by URNNG~\cite{dblp:conf/naacl/kimrykdm19}, we optimize the parser by taking trees sampled from the CKY chart table generated by the encoder as ground truth. Thus, the parser and the chart-based encoder promote each other in this way back and force just like the strategy and value networks in AlphaZero~\cite{DBLP:journals/nature/SilverSSAHGHBLB17}.
Additionally, the pretrained tree encoder can compose sequences recursively in parallel according to the trees generated by the parser, which makes Fast-R2D2 a Recursive Neural Network~\cite{DBLP:journals/ai/Pollack90,DBLP:conf/emnlp/SocherPWCMNP13} variant.
% the parser recursively split a span into two sub-spans by 
% scoring each split point in current span and picking the next split point with the highest score.

%the parser first scores each split point between words in parallel by looking at its left and right contexts, 
%and then % recursively split the whole sentence span into small spans, in each step, we split a span into two sub-spans by picking the 
%what does this mean

% a split point  from the top-down parser,
% Then we use the corresponding binary tree to guide the pruning of CKY encoder. 

% optimize them together
% Besides the pre-train objectives of the top-down parser and the differential CKY encoder, 
% we also optimize them together by minimizing the KL distance between tree probabilities from two models.

In this paper, we make the following main contributions:
\begin{enumerate}
    % MARK: changed
    \item We propose an architecture to jointly pretrain parser and encoder of a recursive network in linear memory cost. Experiments show that our pretrained parser outperforms models custom-tailored for grammar induction.
%    \item We propose a general model-based pruned CKY encoding architecture by introducing a top-down parser and corresponding unsupervised training objective. Experiments show that our parser outperforms models custom-tailored for grammar induction.
    %specially designed for grammar induction.
    % \item We propose a model-based pruning method based on a top-down parser and corresponding unsupervised training objective. Experiments show that our parser outperforms models custom-tailored for grammar induction.
    %specially designed for grammar induction.
    %We propose an unsupervised model-based pruning method based on global information. Such pruned CKY encoding architecture is in linear complexity and theoretically applicable to all one-dimensional inputs. % WHAT'S one-dimensional inputs? like audio
    % and could be optimized by R2D2 pretrain objective.
    % \item We propose a training objective for downstream tasks that naturally supports multi-task objectives. % need more explanation here.
    \item By encoding in parallel following trees generated by the top-down parser, Fast-R2D2 significantly improves the inference speed 30 to 50 fold compared to R2D2.
    \item We pre-train Fast-R2D2 on a large corpus and evaluate it on downstream tasks. The experiments demonstrate that a pretrained recursive model based on an unsupervised parser significantly outperforms pretrained sequential Transformers~\cite{DBLP:conf/nips/VaswaniSPUJGKP17} with the same parameter size in single sentence classification tasks.
    %To our limited knowledge, it's the first pretrained Recursive Neural Network achieve SOTA results. % TODO: first hierarchical model better than BERT
\end{enumerate}

%wiki103, and perform experiment on grammar induction on English and Chinese, and classification tasks on GLUE. Experiments results show ...

% In this paper, we revisit these ideas, and propose a global and learn-able pruning strategy for R2D2 which is achieved by a top-down parser. Specifically, for a given sentence, the parser estimate a score for each split-table points. By splitting a span into two at the point with the highest score recursively from top to down, we sample a split sequence and the corresponding binary tree. The reverse order of the split sequence can be regarded as the merge order for the pruning process. Due to R2D2 iterate all possible combines in a window during encoding process, it will explore other trees near the sampled tree. With the bi-directional language model task as the pretrain loss, R2D2 will learn a better distribution over trees. Inspired by the idea of URNNG that optimize parser by sampling over trees estimated by CKY, we optimize the top-down parser by sampling over R2D2. By optimizing R2D2 and the parser alternately, trees parsed by parser will be more and more accurate, and R2D2 will be pruned based on a proper sequence to search for better trees. Compared with URNNG, our method is linear complexity, not rely on reinforcement learning, directly get feedback from bi-directional language model task loss. Compared to R2D2 based on rule based pruning, our pruning strategy is based on full text and model-based, which make the whole architecture more elegant and flexible.



% 核心motivation介绍: 本文最大的贡献是完善了一种通用的线性复杂度的无监督学习框架。复杂度之所以影响很大，因为传统O(n^3)复杂度的模型都无法使用深度网络，导致表征能力不够，把O(n^3)降到O(n)释放了该架构的可能性，使之可以使用非常复杂的网络作为树编码器，R2D2证明了其表征能力及结构均可达到SOTA，但其基于规则的局部剪枝无法保证找到全局最优结构，限制了该架构的能力。

% 人类对文本的理解并非囫囵吞枣，直接得到整句表征。我们认为，人类会通过自底向上的方式对文本进行构筑，并对中间结果建立索引，形成知识。比如"熊猫爱吃竹子"这个信息，我们会对熊猫或竹子构建索引，问到"熊猫喜欢吃什么?"或"什么动物喜欢吃竹子?", 人类会根据文本内容索引到熊猫或竹子，再索引到相关知识。这些现象说明人类理解句子的过程不仅仅是从字到词，词到短语具备结构性，同时合理的span拥有自己的表征，可以被我们的大脑构建索引。因此，一个理想的语言模型除了具备得到全句表征的能力外，应该存在显式的，离散的内部结构，并且内部结构每个节点均有自己的表征。同时，它需要具备能够无监督习得类人句法结构的能力。在满足上述要求的语言模型中，这些工作:Maillard, URNNG，DIORA, R2D2均满足上述理想语言建模的条件。

% Maillard提出一种可微的CKY编码架构，they primarily introduce an energy function to combine all possible derivations when constructing each span representation. 但该架构立方复杂度导致只能使用浅层网络，并且处理的文本长度有限。并依赖下游任务引导学习结构。
%R2D2在该工作基础上提出一种剪枝CKY编码，通过将底层的一些组合概率较高的span进行合并，使他们作为整体参与后继运算，可以将不必要的cell全部剔除，不参与计算。因此将CKY编码架构复杂度降到O(n), 也因此可以使用更复杂的树编码器，如文中提出的递归Transformer。但其带来的缺陷是合并决策基于规则和局部信息，未必能得到全局最优的切分。URNNG从另一个思路构造了一种基于shift reduce的结构语言模型。由于shift reduce的parser无法直接求所有子树的概率之和，作者通过CKY估算所有概率之和，通过CKY采样优化shift reduce parser，再通过shift reduce parser得到的单向语言模型loss，结合强化学习方法优化CKY。但缺陷是为了估算所有子树概率引入的CKY算法复杂度为O(n^3), 限制了可用性。以及架构本身不可导，需要通过强化学习来优化CKY，反馈并不如可导的架构直接。且强化学习的reward为单向语言模型，无法结合下文。

% 我们重新探访了R2D2的思想，设计了一种迭代优化的全局剪枝策略。具体而言，我们设计了一种自顶向下的parser，对于给定文本，该parser会对每一个切分点进行评估，并得到每一个切分点的分数。通过自顶向下递归的找每个span的最优切分点，我们可以得到一个切分序列S及对应的二叉树z。该切分序列的逆序即可视为剪枝过程选择合并的序列。R2D2基于全局parser得到的树结果，作为剪枝路径。由于R2D2编码过程中窗口的存在，R2D2会探索结构z附近的其他树结构，并由R2D2工作中提出的预训练算法，学到更符合预训练loss的树结构分布。受URNNG工作中通过CKY采样矫正parser结构的启发，我们采用类似的思想，通过采样使得 D_{kl}(p(z|x)||q(z|x)尽可能小，让parser来拟合R2D2学到的结果。如此不断循环，parser将得到越来越准确的全局parse tree，R2D2将在该树附近的结构，根据预训练任务的引导，寻找更优的树概率分布反过来提升parser。相比URNNG，该方法复杂度为线性，全局可微，结构的学习不依赖强化学习，且结合上下文信息。相比基于规则剪枝的R2D2,我们的方法能在全局的基础上进行剪枝，使整个架构更灵活，优雅。