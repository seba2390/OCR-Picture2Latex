\section{Preliminaries}

\subsection{R2D2 Architecture}\label{sec:r2d2}

\paragraph{Differentiable Trees.} 
R2D2 follows the work of \newcite{DBLP:journals/corr/MaillardCY17} in defining a CKY-style~\cite{10.5555/1097042,kasami1966efficient,younger1967recognition} encoder.
For a sentence $\mathbf{S} = \{s_{1}, s_{2},..., s_{n}\}$ with $n$ words or word-pieces, 
it defines a chart table as illustrated in Figure~\ref{fig:chart_data}. In the table, each cell $\mathcal{T}_{i, j}$ is a tuple $\langle e_{i, j}, p_{i, j}, \widetilde{p}_{i,j} \rangle$, where
$e_{i, j}$ is a vector representation, $p_{i, j}$ is the probability of a single composition step, 
and $\widetilde{p}_{i,j}$ is the probability of the subtree for the span $[i, j]$ over the sub-string $s_{i:j}$.
When $i$ equals $j$, the table has terminal nodes $\mathcal{T}_{i, i}$ with $e_{i, i}$ initialized with the embeddings of input tokens $s_{i}$, while
$p_{i, i}$ and $\widetilde{p}_{i,i}$ are set to one. 
When $j>i$, the representation $e_{i, j}$ is a weighted sum of intermediate combinations $c_{i, j}^{k}$, defined as: 
\begin{align}
\label{eq:tree_encoder}
&c_{i,j}^{k}, \  p_{i,j}^{k} = f(e_{i, k}, e_{k+1, j})\\
\label{eq:tree_prob}
&\widetilde{p}_{i,j}^{k} = p_{i,j}^{k} \,\, \widetilde{p}_{i,k} \,\, \widetilde{p}_{k+1,j}\\
&\boldsymbol{\alpha}_{i,j} = \text{\gumbel} (\log( \mathbf{\widetilde{p}}_{i,j}))\\
&e_{i,j} =  [c_{i,j}^{i}, c_{i,j}^{i+1}, ..., c_{i,j}^{j-1}]\,\boldsymbol{\alpha}_{i,j}\\
&[p_{i,j},\widetilde{p}_{i,j}] = \boldsymbol{\alpha}_{i,j}^\intercal [\boldsymbol{p}_{i,j}, \boldsymbol{\widetilde{p}}_{i,j}]
\end{align}
$k$ is a split point from $i$ to $j-1$, $f(\cdot)$ is an $n$-layer Transformer encoder,
$p_{i,j}^{k}$ and $\widetilde{p}_{i,j}^{k}$ denote the single step combination probability and the subtree probability, respectively, at split point $k$,
$\boldsymbol{p}_{i,j}$ and $\boldsymbol{\widetilde{p}}_{i,j}$ are the concatenation of all $p_{i,j}^{k}$ or $\widetilde{p}_{i,j}^{k}$ 
values, 
and \gumbel is the Straight-Through Gumbel-Softmax operation of \newcite{DBLP:conf/iclr/JangGP17} with temperature set to one. As \gumbel picks the optimal splitting point $k$ at each cell in practice,
it is straightforward to recover the complete derivation tree from the root node $\mathcal{T}_{1,n}$ in a top-down manner recursively.

\begin{figure}[htb!]
  \centering
  \includegraphics[width=0.45\textwidth]{data/chart_table.png}
  %\vspace{-2pt}
  \caption{Chart data structure. There are two alternative ways of generating $\mathcal{T}_{1,3}$:
     combining either ($\mathcal{T}_{1,2}$, $\mathcal{T}_{3,3}$) or ($\mathcal{T}_{1,1}$, $\mathcal{T}_{2,3}$).}
  \label{fig:chart_data}
\end{figure}

% \begin{figure*}[htb!]
%     \flushleft
%     \includegraphics[width=1\textwidth]{data/pruning_2.png}
%     \caption{A slice for the pruning and encoding process. In heuristic pruning, cells to merge in (b) and (e) we select according local composition probabilities. In model-based pruning, the cells are selected according to merge order estimated by a top-down parser.}
%     \label{fig:pruning}
% \end{figure*}

\begin{figure*}[htb!]
    \flushleft
    \includegraphics[width=1\textwidth]{data/pruning.png}
    \vspace{-3ex}
    \caption{Example of chart pruning and encoding process. With R2D2's original heuristic pruning, cells to merge are selected according to local composition probabilities. For better model-based pruning, we propose selecting cells according to the merge order estimated by a top-down parser.}\vspace{-1ex}
    \label{fig:pruning}
\end{figure*}

\paragraph{Heuristic pruning.} 
%
As shown in Figure~\ref{fig:pruning}, R2D2 starts to prune if all cells beneath height $m$ have been encoded. 
The heuristic rules work as follows:
\begin{enumerate}
    \item Recover the maximum sub-tree for each cell at the $m$-th level, and collect all cells at the $2$nd level that appear in any sub-tree.
    \item Rank candidates in Step 1 by the composition probability $p_{i, j}$, and pick the highest-scoring cell as a non-splittable span (e.g., $\mathcal{T}_{1,2}$).
    \item Remove any invalid cells that would break the now non-splittable span from Step 2, e.g., the dark cells in (c), and reorganize the chart table much like in the Tetris game as in (d).
    \item Encode the blank cells at the $m$-th level, e.g., the cell highlighted with stripes in (d), and go back to Step 1 until the root cell has been encoded.
\end{enumerate}

% During pruning, each cell at the $m_{th}$ layer is expanded to a binary tree and all non-terminals with two leaves are recalled. All recalled nodes are ranked according to the composition probability estimated by R2D2 and the highest one is select to merge as shown in Figure~\ref{fig:pruning}(b). Then all cells that split apart the merging cell is pruned. Rest cells are compacted to a smaller chart table just like the Tetris Game as shown in Figure~\ref{fig:pruning}(c). Then new cells beneath height $m$ is encoded and repeat the aforementioned process until the root is encoded.
% 如图x所示，R2D2设置一个threashold，使得当前chart table中已编码的cell的最大层数不超过m。基于召回排序的思想，每次将m层节点进行展开，召回由两个叶节点构成的span组合，在所有组合中对组合概率进行排序，找到最佳合并点，进行合并。之后对chart table进行剪枝，chart table如同俄罗斯方块一样从上往下，从右往左进行合并。循环往复，直到根节点完成编码。

\paragraph{Pretraining.}

% 为了可以在没有gold tree的情况下学习句法结构，R2D2提出了一种自监督的学习目标。与双向语言模型任务相似，通过每个位置的上下文信息揭露token i。

To learn meaningful structures without gold trees, \newcite{hu-etal-2021-r2d2} propose a self-supervised pretraining objective. Similar to the bidirectional masked language model task, R2D2 reconstructs a given token $s_i$ based on its context representation $e_{1,i-1}$ and $e_{i+1, n}$. The probability of each token is estimated by the tree encoder defined in R2D2. The final objective is:
\begin{equation}
\label{eq:bilm_loss}
\underset{\theta}{\mathrm{min}}\,\sum_{i=1}^{n} -\log\,p_{\theta}(s_{i} \mid e_{1:i-1}, e_{i+1:n})
\end{equation}