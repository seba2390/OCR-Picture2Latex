% \paragraph{Why Top-down Parser.}
% There are two necessities to choose a top-down parser instead of a transition-based parser:
% \begin{itemize}[leftmargin=*,noitemsep,nolistsep]
%     \item Different from URNNG, who estimates the total probabilities of all potential trees through CKY, 
%     the pruned CKY encoder does not explore all potential trees. 
%     Therefore, it is necessary for the parser to generate trees diverse enough in split points to prevent R2D2 from collapsing to a local optimal structure. 
%     In our framework, we observed that shift-reduce based parser collapses to a balanced binary tree by experiments. % TODO: re-phrase this sentence.
%     \item As the parser is optimized by trees sampled from CKY, shift-reduce parser has to evaluate the probability for each sample, 
%     which is time-consuming when samples are large. 
%     By contrast, the top-down parser only encodes sentences once, and supports hundreds of sampled trees.
% \end{itemize}


% 介绍结构预测领域一系列工作=>介绍和本文相关的工作: 有内部表征的显式离散建模
Many attempts have been done in grammar induction and hierarchical encoding. \citet{DBLP:conf/conll/Clark01} and \citet{DBLP:conf/acl/KleinM02}
provided some of the first successful statistical approaches to grammar induction. There have been multiple recent papers that focus on structure induction based on language modeling objectives~\cite{DBLP:conf/nips/ShenTHLSC19,DBLP:conf/iclr/ShenTSC19,DBLP:conf/acl/ShenTZBMC20,kim-etal-2019-compound}. \newcite{DBLP:journals/ai/Pollack90} proposed to use RvNN as a recursive architecture to encode text hierarchically, and \newcite{DBLP:conf/emnlp/SocherPWCMNP13} showed the effectiveness of RvNNs with gold trees for sentiment analysis. In this work, we focus on models that are capable of learning meaningful structures in an unsupervised way and encoding text over the induced tree hierarchically.

In the line of work on learning a sentence representation with structures, \newcite{DBLP:conf/iclr/YogatamaBDGL17} jointly train their shift-reduce parser and sentence embedding components without gold trees.
As their parser is not differentiable, they have to resort to reinforcement training, resulting in increased variance, which may easily collapse to trivial left or right branching trees. Gumbel-Tree-LSTMs~\cite{DBLP:conf/aaai/ChoiYL18} construct trees by recursively selecting two terminal nodes to merge and learning the composition probability via downstream tasks. CRvNN~\cite{DBLP:conf/icml/ChowdhuryC21} makes the entire process end-to-end differentiable and parallel by introducing a continuous relaxation.
%Compared to approaches building tree in a discrete actions, \newcite{}
% The work of URNNG~\cite{dblp:conf/naacl/kimrykdm19} applies variational inference over latent trees to perform unsupervised optimization of the RNNG \cite{dyer-etal-2016-recurrent}, an RNN model that estimates a joint distribution over sentences and trees based on shift-reduce operations. However, it is hard to induce them when trained from scratch.
URNNG~\cite{dblp:conf/naacl/kimrykdm19} propose the first architecture to jointly pretrain parser and encoder based on RNNG \cite{dyer-etal-2016-recurrent}. However, it has $O(n^3)$ complexity and remains unable to improve upon a right-branching baseline when punctuation is removed.
%relies on reinforcement learning which make it hard to induce reasonable trees when punctuation is removed.
%They propose a RNNG encoder based on the actions of a shift-reduce parser and a CKY parser to estimate the probability of all potential trees. The shift-reduce parser is optimized by sampling over CKY parser while the CKY parser is optimized by reinforcement learning with the language model loss of corresponding structure as reward.
%an RNN model that estimates a joint distribution over sentences and trees based on shift-reduce operations. 
\newcite{DBLP:journals/corr/MaillardCY17} propose an alternative approach, based on a differentiable CKY encoding. The algorithm is differentiable due to a soft-gating approach, which approximates discrete candidate selection by a probabilistic mixture of the constituents available in a given cell of the chart. While their work relies on annotated downstream tasks to learn structures, \newcite{dblp:conf/naacl/drozdovvyim19} propose a novel auto-encoder-like pretraining objective based on the inside--outside algorithm~\cite{Baker1979TrainableGF,DBLP:conf/icgi/Casacuberta94}. 
%As mentioned above, CKY-based models have cubic time complexity. 
%\newcite{hu-etal-2021-r2d2} propose a pruned differentiable CKY encoding architecture with a simple pretraining objective related to bi-directional language modeling. They reduce the time complexity to a linear number of composition steps and make it possible to apply sophisticated tree encoders and to scale to large corpus pretraining.
