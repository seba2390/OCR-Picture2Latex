\subsection{Unsupervised Grammar Induction}

\subsubsection{Setup}\label{sec:LM_setup}
\paragraph{Baselines and Evaluation.} 
For comparison, we include six recent strong models for unsupervised parsing with available open source implementations: StructFormer \cite{DBLP:conf/acl/ShenTZBMC20}, Ordered Neurons~\cite{DBLP:conf/iclr/ShenTSC19}, URNNG~\cite{dblp:conf/naacl/kimrykdm19}, DIORA~\cite{dblp:conf/naacl/drozdovvyim19}, C-PCFG~\cite{kim-etal-2019-compound}, and R2D2~\cite{hu-etal-2021-r2d2}. 
To observe the marginal gain from pretraining, we also include Fast-R2D2 without pretraining denoted as Fast-R2D2$_{\rm w/o}$.
Following~\newcite{htut-etal-2018-grammar}, we train all systems on a training set consisting only of raw text, and evaluate and report the results on an annotated test set. 
As an evaluation metric, we adopt sentence-level unlabeled $F_1$ computed using the script from \newcite{kim-etal-2019-compound}.
We compare against the non-binarized gold trees per convention.
The results of Fast-R2D2 are obtained from 3 runs of each model with different random seeds in pre-training.
The best checkpoint for each system is picked based on scores on the validation set. 
Fast-R2D2 is pretrained with span constraints for the word level but without span constraints for the word-piece level.
To support word-piece level evaluation, 
we convert gold trees to word-piece level trees 
by simply breaking each terminal node into a non-terminal node with its word-pieces as terminals, e.g., (NN discrepancy) into (NN (WP disc) (WP \#\#re) (WP \#\#pan) (WP \#\#cy)).

\paragraph{Environment.} EFLOPS~\cite{DBLP:conf/hpca/DongCZYWFZLSPGJ20} is a highly scalable distributed training system designed by Alibaba. With its optimized hardware architecture and co-designed supporting software tools, including ACCL~\cite{DBLP:journals/micro/DongWFCPTLLRGGL21} and KSpeed (the high-speed data-loading service), it could easily be extended to 10K nodes (GPUs) with linear scalability.

\paragraph{Hyperparameters.} The tree encoder of our model uses 4-layer Transformers with 768-dimensional embeddings, 
3,072-dimensional hidden layer representations, and 12 attention heads. 
The top-down parser of our model uses a 4-layer bidirectional LSTM with 128-dimensional embeddings and 256-dimensional hidden layer. The sampling number $K$ is set to be 256.
Training is conducted using Adam optimization with weight decay using a learning rate of $5 \times 10^{-5}$ for the tree encoder and $1 \times 10^{-2}$ for the top-down parser.
The batch size is set to 64 per GPU for $m$=$4$, though we also limit the maximum total length for each batch, such that excess sentences are moved to the next batch. The limit is set to 1,536. It takes about 120 hours for 60 epochs of training with $m$=$4$ on 8 A100 GPUs.

\paragraph{Data.}  For English, to fully leverage the scalability of Fast-R2D2, we pretrain Fast-R2D2 on WikiText103~\cite{DBLP:conf/iclr/MerityX0S17}
and then fine-tune the model on the Penn Treebank (PTB)~\cite{marcus-etal-1993-building}
for 10 epochs with the same objective.
WikiText103 is split at the sentence level, and sentences longer than 200 after tokenization are discarded (about 0.04‰ of the original data). 
The total number of sentences is 4,089,500, and the average sentence length is 26.97.
For Chinese, we use a subset of Chinese Wikipedia (Simplified Characters) for pretraining, specifically the first 10,000,000 sentences shorter than 150 characters and then fine-tune on Chinese Penn Treebank (CTB) 8.0~\cite{ctb8}.
We test our approach on PTB WSJ data with the standard splits (2--21 for training, 22 for validation, 23 for test) and the same preprocessing as in recent work \cite{kim-etal-2019-compound}, where we discard punctuation and lower-case all tokens. 
To explore the universality of the model across languages, we further evaluate using the CTB,
on which we also remove punctuation.
Note that in all settings, the training and fine-tuning is conducted entirely on raw unannotated text.

\subsubsection{Results and Discussion}

\begin{table}
\newcommand{\invzero}{\hphantom{0}}
\begin{center}
\setlength{\tabcolsep}{3.pt}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{@{}l|l|l|l|l@{}}
                    &  eval & mem. & \multicolumn{1}{c|}{WSJ}  & \multicolumn{1}{c}{CTB}  \\
Model               & gran. & cplx  &  $F_1(\mu)$ & $F_1(\mu)$\\ \hline \hline
Left Branching (W)  & WD & $O(n)$& \invzero 8.15  & 11.28 \\
Right Branching (W) & WD & $O(n)$& 39.62 & 27.53 \\
Random Trees (W)    & WD & $O(n)$ & 17.76 & 20.17 \\
\hline
URNNG (W)           & WD & $O(n^3)$& 45.4$^\dag$ & ~~--- \\
ON-LSTM (W)         & WD & $O(n)$  & 47.7$^\dag$ & 24.73 \\
DIORA (W)           & WD & $O(n^3)$& 51.4 & ~~---  \\
StructFormer (W)    & WD & $O(n^2)$& 54.0$^\ddagger$ & ~~--- \\
C-PCFG (W)          & WD & $O(n^3)$& 55.2$^\dag$ & 49.95 \\ \hline
R2D2 (WP)           & WD & $O(n)$ & 48.11 & 44.85  \\
Fast-R2D2$^*$(W)$_{\rm w/o}$ & WD & $O(n)$ & 48.24 & 45.24 \\
Fast-R2D2$^*$(WP)$_{\rm w/o}$ & WD & $O(n)$ & 48.89 & 45.26 \\
Fast-R2D2$^*$(WP)  & WD & $O(n)$ & \textbf{57.22} & \textbf{53.13} \\
\hline \hline
R2D2 (WP)           & WP & $O(n)$  & 52.28 & 63.94 \\ 
Fast-R2D2(WP)      & WP & $O(n)$ & 50.20 & \textbf{67.79} \\
Fast-R2D2$^*$(WP)  & WP & $O(n)$& \textbf{53.88} & 67.74 \\ \hline
\end{tabular}
}
\end{center}
\caption{Unsupervised parsing results with words (W) or word-pieces (WP) as input. ``eval gran." is short for evaluation granularity.
        Values marked with $^{\dag}$ are taken from \newcite{kim-etal-2019-compound}, while $^{\ddagger}$ denotes values taken from \newcite{DBLP:conf/acl/ShenTZBMC20}.
        The bottom three systems are all pre-trained or trained 
        at the word-piece level \textbf{without} span constraints and are measured against word-piece level golden trees. ${\rm w/o}$ means without pretraining.}
\label{tbl:constituency_parsing}
\end{table}


Table~\ref{tbl:constituency_parsing} shows the results of all systems with words (W) and word-pieces (WP) as input on the WSJ and CTB test sets. 
When we evaluate all systems on word-level golden trees, 
our Fast-R2D2 performs substantially better than R2D2 across both datasets.
We denote as Fast-R2D2 the method of using the parser to guide the pruning and selecting the best tree using the chart table and as Fast-R2D2$^*$ the system that uses the top-down parser for tree induction with subsequent R2D2 encoding.
Interestingly, the results suggest that Fast-R2D2$^*$ outperforms Fast-R2D2, especially on the WSJ test set.
Additionally, pretrained Fast-R2D2$^*$
outperforms the models specifically designed for grammar induction.

\begin{table}[!htb]
\small
\begin{center}
\setlength{\tabcolsep}{3.5pt}
\resizebox{0.48\textwidth}{!}{ %
\begin{tabular}{@{}ll| l l l l l l@{}}
 & Model  & WD & NNP & VP & SBAR\\\hline \hline
\multirow{5}{*}{\rotatebox[origin=c]{90}{WSJ}} & DIORA (WP)  & 94.63 & 77.83 & 17.30 & 22.16\\
& C-PCFG (W)                  & ~~--- & ~~--- & 41.7$^\dag$ & 56.1$^\dag$ \\
& C-PCFG (WP)                  & 87.35 & 66.44 & 23.63 & 40.40 \\
& R2D2 (WP)    & \textbf{99.76} & \textbf{86.76} & 24.74 & 39.81\\
& Fast-R2D2$^*$ (WP) & 97.67 & 83.44 & \textbf{63.80} & \textbf{65.68} \\ \hline \hline
\multirow{3}{*}{\rotatebox[origin=c]{90}{CTB}} & C-PCFG(WP) &89.34 & 46.74 & 39.53 & ~~---\\
 & R2D2 (WP) & 97.16 & 67.19 & 37.90 & ~~---\\
 & Fast-R2D2$^*$ (WP) & \textbf{97.80} & \textbf{68.57} & \textbf{46.59} & ~~---
 \\ \hline \hline
\end{tabular}
}
\end{center}
\caption{Recall of constituents and words. WD means word.  Values with $^{\dag}$ are taken from \newcite{kim-etal-2019-compound}.}
\label{tbl:unsupervised_chunking}
\end{table}

Following \newcite{dblp:conf/naacl/kimrykdm19} and \newcite{drozdov-etal-2020-unsupervised},
we also compute the recall of constituents when evaluating on word-piece level golden trees.
Besides standard constituents, we also compare the recall of word-piece chunks and proper noun chunks. 
Proper noun chunks are extracted by finding adjacent unary nodes with the same parent and tag NNP. 
Table~\ref{tbl:unsupervised_chunking} reports the recall scores for constituents and words on the WSJ and CTB test sets. 
Compared with the R2D2 baseline, 
our Fast-R2D2 performs slightly worse for small semantic units, 
but significantly better over larger semantic units (such as VP and SBAR) on the WSJ test set.
On the CTB test set, our Fast-R2D2 outperforms R2D2 on all constituents. 

From Tables~\ref{tbl:constituency_parsing}~and~\ref{tbl:unsupervised_chunking}, 
we conclude that Fast-R2D2 overall obtains better results than R2D2 on CTB, while faring slightly worse than R2D2 only for small semantic units on WSJ. We conjecture that this difference stems from differences in  tokenization between Chinese and English. 
Chinese is a character-based language without complex morphology, where collocations of characters are consistent with the language, making it easier for the top-down parser to learn them well. 
In contrast, word-pieces for English are built based on statistics, and individual word-pieces are not necessarily natural semantic units. Thus, there may not be sufficient semantic self-consistency, such that it is harder for a top-down parser with a small number of parameters to fit it well.

\subsection{Downstream Tasks}
We next consider the effectiveness of Fast-R2D2 in downstream tasks. This experiment is not intended to advance the state-of-the-art on the GLUE benchmark but rather to assess to what extent our approach performs respectably against the dominant inductive bias as in conventional sequential Transformers.

\subsubsection{Setup}
\paragraph{Data and Baseline.}
We fine-tune pretrained models on several datasets,
including SST-2, CoLA, QQP, and MNLI from the GLUE benchmark~\cite{wang2018glue}.
As sequential Transformers with their dominant inductive bias remain the norm for numerous NLP tasks, 
we mainly compare Fast-R2D2 with \bert~\cite{devlin2018} as a representative pretrained model based on a sequential Transformer. 
We did not include recursive models such as Gumbel-Tree-LSTMs~\cite{DBLP:conf/aaai/ChoiYL18} and CRvNN~\cite{DBLP:conf/icml/ChowdhuryC21} among our baselines, as they are not pretrained models.
In order to compare the two forms of inductive bias fairly and efficiently,
we pretrain \bert models with 4 layers and 12 layers as well as our Fast-R2D2 from scratch on the WikiText103 corpus following Section~\ref{sec:LM_setup}. 
Considering that longer inputs in the pre-training stage are helpful for BERT’s downstream task performance, we use the original corpus that is not split into sentences as inputs.
For simplicity, Fast-R2D2 is fine-tuned without span constraints.
Following the common settings, we add an MLP layer over the root representation of the R2D2 encoder for single-sentence classification. 
For cross-sentence tasks such as QQP and MNLI, we feed the root representations of the two sentences into the pretrained tree encoder of R2D2 as left and right inputs, 
and also add a new task ID as another input term to the R2D2 encoder. 
Then we feed the hidden output of the new task ID into another MLP layer to predict the final label.
We train all systems across the four datasets for 10 epochs 
with a learning rate of $5\times 10^{-5}$, batch size $64$, and maximum input length $200$.
We validate each model in each epoch and report the best results on development sets.

\begin{table}
\begin{center}
\setlength{\tabcolsep}{1.5pt}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{l|c|r r|r r}
\multirow{4}{*}{Model} & \multirow{4}{*}{Para.} & \multicolumn{2}{c|}{Single sent.} & \multicolumn{2}{c}{Cross sent.} \\
 &  & \begin{tabular}[c]{@{}l@{}}SST-2\\ (Acc.)\end{tabular} & \begin{tabular}[c]{@{}l@{}}CoLA\\ (Mcc.)\end{tabular} & \begin{tabular}[c]{@{}l@{}}QQP\\ (F1)\end{tabular} & \begin{tabular}[c]{@{}l@{}}MNLI\\m/mm\\ (Acc.)\end{tabular}            \\ \hline \hline
\bert (4L)  & 52M & 84.98 & 17.07 & 84.01 & 73.73/74.63 \\
\bert (12L) & 116M & 90.25 & 40.72 & 87.13 & 80.00/80.41 \\ \hline
R2D2        & 52M & 89.33 & 34.79 & 84.27 &  69.35/68.72 \\ \hline
Fast-R2D2$^\dag$& {\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\\52M/\\ 10M\end{tabular}}} & 87.50 & 8.67 & 83.97 & 69.53/69.50 \\
Fast-R2D2$^*\dag$& {} & 88.30 & 10.14 & 84.07 & 69.36/69.11 \\
Fast-R2D2  & {} & 90.25 & 38.45 & 84.35 & 69.36/68.80 \\ 
Fast-R2D2$^*$& {} & 90.71 & 40.11 & 84.32 & 69.64/69.57\\
\hline \hline
\end{tabular}
}
\end{center}
\caption{Downstream results. All systems are pretrained from scratch on WikiText103.
        Para.\ describes the number of parameters for each model. Fast-R2D2 contains the R2D2 encoder and top-down parser, two components with 52M and 10M parameters, respectively.
        Mcc.\ stands for Matthew's correlation coefficient.
        Fast-R2D2 with $\dag$ are models fine-tuned without $\mathcal{L}_\mathrm{bilm}$ for an ablation study.
    }\vspace{-10pt}
\label{tbl:classification}
\end{table}
\subsubsection{Results and Discussion}
Table~\ref{tbl:classification} shows the corresponding scores on SST-2, CoLA, QQPl, and MNLI. 
In terms of the parameter size, our Fast-R2D2 model has 52M and 10M parameters for the R2D2 encoder and top-down parser, respectively.
It is clear that 12-layer \bert is significantly better than 4-layer \bert.
As mentioned in Section~\ref{sec:downstream}, Fast-R2D2 has two options to construct the final tree and representation for a given input sentence:
Fast-R2D2$^*$ uses the output tree from the top-down parser, while Fast-R2D2 uses the best tree inferred by the R2D2 encoder.
Similar to the results for unsupervised parsing, Fast-R2D2$^*$ in classification tasks again outperforms Fast-R2D2.
We hypothesize that trees generated by the top-down parser without Gumbel noise are more stable and reasonable.
Fast-R2D2 significantly outperforms 4-layer \bert and achieves competitive results compared to 12-layer \bert in single sentence classification tasks such as SST-2 and CoLA, but still performs significantly worse in the cross-sentence tasks. 
We believe this is an expected result, as there is no cross-attention mechanism in the inductive bias of Fast-R2D2. 
However, the performance of Fast-R2D2 on classification tasks shows that the inductive bias of R2D2 has higher parameter utilization than sequentially applied Transformers.
Importantly, we demonstrate that a Recursive Neural Network variant with an unsupervised parser can achieve comparable results to pretrained sequential Transformers even with fewer parameters and interpretable intermediate results, 
Hence, our Fast-R2D2 framework provides an alternative for NLP tasks.

\subsection{Speed Evaluation}
To assess the time cost, we mainly compare sequential Transformers and Fast-R2D2 in forced encoding on various sequence length ranges. We randomly select 1,000 sentences for each range from WikiText103 and report the average time consumption on a single A100 GPU. \bert is based on the open source Transformers library\footnote{\url{https://github.com/huggingface/transformers}} and R2D2 is based on the official code in \newcite{hu-etal-2021-r2d2}.\footnote{\url{https://github.com/alipay/StructuredLM_RTDT/tree/r2d2}}

\begin{table}% [htb!]
\small
\begin{center}
\setlength{\tabcolsep}{3.pt}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{l|rrrr}
\multirow{2}{*}{Model} & \multicolumn{4}{c}{Sequence Length Ranges} \\\cline{2-5}
      & \multicolumn{1}{c|}{0--50} & \multicolumn{1}{l|}{50--100} & \multicolumn{1}{l|}{100--200} & 200--500 \\ 
\hline
\bert (12L) & \multicolumn{1}{r|}{1.36}     & \multicolumn{1}{r|}{1.46}       & \multicolumn{1}{r|}{1.62}        & 2.38 \\ \hline
R2D2  & \multicolumn{1}{r|}{38.06}     & \multicolumn{1}{r|}{173.74}       & \multicolumn{1}{r|}{555.95}        &    ---     \\
Fast-R2D2  & \multicolumn{1}{r|}{4.67} & \multicolumn{1}{r|}{14.91} & \multicolumn{1}{r|}{39.73} & 150.26 \\
Fast-R2D2* & \multicolumn{1}{r|}{1.28} & \multicolumn{1}{r|}{2.96}  & \multicolumn{1}{r|}{5.56}  & 10.70 \\ 
\hline \hline
\end{tabular}
}
\end{center}
\caption{Inference time in seconds for various systems to process 1,000 sentences with a batch size of 50.}
\label{tbl:speed_test}
\end{table}

Table~\ref{tbl:speed_test} shows the inference time in seconds for different systems to process 1,000 sentences with a batch size of 50.
Running R2D2 is time-consuming, since the heuristic pruning method involves substantial memory exchanges between GPU and CPU. 
In Fast-R2D2, we alleviate this problem by using model-guided pruning to accelerate the chart table processing,
in conjunction with a code implementation in CUDA, Fast-R2D2 reduces the inference time significantly. 
Fast-R2D2$^{*}$ further improves the inference speed by running forced encoding in parallel over the binary tree generated by the parser, which is about 30--50 times faster than R2D2 in various ranges. 
Although there is still a gap in speed compared to sequential Transformers, Fast-R2D2$^{*}$ is sufficiently fast for most NLP tasks while producing interpretable intermediate representations.
