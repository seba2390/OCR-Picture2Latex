\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{bbm}
%\usepackage{natbib}

%% my packages
\usepackage[font=footnotesize,labelfont=bf]{caption}

\usepackage{multirow}
\usepackage{tabularx}
\usepackage{dsfont}
\usepackage{url}
\usepackage[tight]{subfigure}
\usepackage{color}
%\usepackage[small]{caption}
\usepackage{subfigure}
\usepackage{amsthm}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[export]{adjustbox}
\usepackage{wrapfig}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
% \usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
% \setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Ring loss: Convex Feature Normalization for Face Recognition}

%\author{Yutong Zheng\\
%Carnegie Mellon University\\
%{\tt\small \{yutongzh\}@andrew.cmu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
%\and
%Dipan Pal\\
%Carnegie Mellon University\\
%{\tt\small dipanp@andrew.cmu.edu}
%}


\author{Yutong Zheng, Dipan K. Pal and Marios Savvides \\
  Department of Electrical and Computer Engineering\\
  Carnegie Mellon University\\
{\tt\small \{yutongzh, dipanp, marioss\}@andrew.cmu.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}



%%%%%%%%% ABSTRACT
\begin{abstract}
We motivate and present Ring loss, a simple and elegant feature normalization approach for deep networks designed to augment standard loss functions such as Softmax. We argue that deep feature normalization is an important aspect of supervised classification problems where we require the model to represent each class in a multi-class problem equally well. The direct approach to feature normalization through the hard normalization operation results in a non-convex formulation. Instead, Ring loss applies soft normalization, where it gradually learns to constrain the norm to the scaled unit circle while preserving convexity leading to more robust features. We apply Ring loss to large-scale face recognition problems and present results on LFW, the challenging protocols of IJB-A Janus, Janus CS3 (a superset of IJB-A Janus), Celebrity Frontal-Profile (CFP) and MegaFace with 1 million distractors. Ring loss outperforms strong baselines, matches state-of-the-art performance on IJB-A Janus and outperforms all other results on the challenging Janus CS3 thereby achieving state-of-the-art. We also outperform strong baselines in handling extremely low resolution face matching.
\end{abstract}

%%%%%%%%% BODY TEXT







\section{Introduction}



Deep learning has demonstrated impressive performance on a variety of tasks. Arguably the most important task, that of supervised classification, has led to many advancements. Notably, the use of deeper structures  \cite{simonyan2014very, szegedy2015going, he2016deep} and more powerful loss functions \cite{hadsell2006dimensionality, schroff2015facenet, wen2016discriminative, tadmor2016learning, liu2016large} have resulted in far more robust feature representations.  There has also been more attention on obtaining better-behaved gradients through normalization of batches or weights \cite{ioffe2015batch, ba2016layer, salimans2016weight}. 

%There have also been some attempts to modify the Softmax loss itself in order to increase discrimination in terms of larger angular margin \cite{}. 


One of the most important practical applications of deep networks with supervised classification is face recognition. Robust face recognition poses a huge challenge in the form of very large number of classes with relatively few samples per class for training with significant nuisance transformations. A good understanding of the challenges in this task results in a better understanding of the core problems in supervised classification, and in general representation learning. However, despite the impressive attention on face recognition tasks over the past few years, there are still many gaps towards such an understanding. Notably, the need and practice of feature normalization. Normalization of features has recently been discovered to provide significant improvement in performance which implicitly results in a cosine embedding \cite{ranjan2017l2, Wang2017NormFace}. However, direct normalization in deep networks explored in these works results in a non-convex formulation resulting in local minima generated by the loss function itself. It is important to preserve convexity in loss functions for more effective minimization of the loss given that the network optimization itself is non-convex.  In a separate thrust of work, cosine similarity has also been very recently explored for supervised classification \cite{liu2017learning, chunjie2017cosine}. Nonetheless, a concrete justification and principled motivation for the \textit{need} for normalizing the features itself is also lacking.

%On a separate note, since the most common approach to feature matching involves the dot-product, normalization of features directly results in the use of cosine similarity.



\begin{figure}%{r}{0.5\textwidth}
    \begin{center}
        \subfigure[Features trained using Softmax]{%
        \centering
            \includegraphics[width=0.45\columnwidth,height=0.4\columnwidth,valign=m]{Softmax.pdf}\label{fig_softmax}
        }%
         \subfigure[Features trained using Ring loss]{%
        \centering
        \includegraphics[width=0.5\columnwidth,height=0.4\columnwidth,valign=m]{RingLoss.pdf}\label{fig_ring_loss}
        }
    \end{center}
    \vspace{-0.5cm}
\caption{Sample MNIST features trained using (a) Softmax and (b) Ring loss on top of Softmax. Ring loss uses a convex norm constraint to gradually enforce normalization of features to a learned norm value $R$. This results in features of equal length while mitigating classification margin imbalance between classes. Softmax achieves 98.97 \% accuracy on MNIST, whereas Ring loss achieves 99.34 \% demonstrating the superior performance of the network learned normalized features. } 
\label{fig_sample_visualization}
\vspace{-0.5cm}
\end{figure}


\begin{figure*}%{r}{0.5\textwidth}



    \begin{center}
        \subfigure[A 2-class 2-sample case]{%
        \centering
            \includegraphics[width=0.7\columnwidth,valign=m]{RingLoss_sample_case.pdf}\label{fig_class_margin_setting}
        }%
         \subfigure[Classification margin]{%
        \centering
        \includegraphics[width=0.9\columnwidth,valign=m]{class_margin.eps}\label{fig_class_margin}
        }
    \end{center}
    \vspace{-0.7cm}
\caption{  (a) A simple case of binary classification.  The shaded regions (yellow, green)  denote the classification margin (for class 1 and 2).  (b)  Angular classification margin for $\theta_1$ for different $\delta=\cos \theta_2$. }
\label{fig_class_margin_visualization}
\vspace{-0.5cm}
\end{figure*}




\textbf{Contributions.} In this work, we propose Ring loss, a simple and elegant approach to normalize all sample features through a convex augmentation of the primary loss function (such as Softmax). The value of the target norm is also learnt during training. Thus, the only hyperparameter in Ring  loss is the loss weight w.r.t to the primary loss function. We provide an analytical justification illustrating the benefits of feature normalization and thereby cosine feature embeddings. Feature matching during testing in face recognition is typically done through cosine distance creating a gap between testing and training protocols which do not utilize normalization. The incorporation of Ring loss during training eliminates this gap. Ring loss is differentiable that allows for seamless and simple integration into deep architectures trained using gradient based methods. We find that Ring loss provides consistent improvements over a large range of its hyperparameter when compared to other baselines in normalization and indeed other losses proposed for face recognition in general. Interestingly, we also find that Ring loss helps in being robust to lower resolutions through the norm constraint.

%We achieve 99.28 \% accuracy on LFW while training the Face-ResNet \cite{wen2016discriminative} purely on CASIA-WebFace \cite{yi2014learning} on about 0.5 million images.







%need and prior work for normalization, batch ,layer, norm face, etc


%\textbf{Prior Art}




\section{Ring loss: Convex Feature Normalization}


\subsection{Intuition and Motivation. } 

%\textbf{The need for the cosine metric. } At test time, three kinds of metrics are usually used for matching on feature space, i.e. dot product, cosine similarity and Euclidean distance. The feature vector can be seen as the product of two independent components, the norm of the feature $\|\mathbf{x}\|$ and the direction vector $\mathbf{d}$, i.e. 
%\begin{align}
%\mathbf{x} = \|\mathbf{x}\| \mathbf{d}, 
%\end{align}
%The norm of the features can only encode information along one dimension (length of features), while the direction encode the information from the rest of the dimensions. Therefore, as the feature dimension increases, the direction encodes the information dominantly. In this way, one can still incorporate feature norm for testing, but only if it learns representative information during training. 

%Consider two feature vectors $\mathbf{x_1}, \mathbf{x_2}$, the dot product is
%\begin{align}
%\mathbf{x_1}\cdot\mathbf{x_2} = \|\mathbf{x_1}\|\|\mathbf{x_2}\|\cos\theta, 
%\end{align}
%where $\theta$ is the angle between the two vectors $\mathbf{x_1}$ and $\mathbf{x_2}$. Note that $\cos\theta$ is exactly the cosine similarity metric. 

%Cosine similarity has negative correlation with the angle $\theta$ when $\theta \in [0, \pi]$. The angle $\theta$ can be seen as orthodromic distance on a hypersphere, 
%\begin{align}
%D_{ortho} = \theta r,
%\end{align}
%where $r$ is the radius of the hypersphere. 

%lower or higher norm in features, actually different norm amongst features creates issues with softmax\\
%need to normalize features to fixed norm\\
%this constraint is non-convex, and current work does not take into account\\
%the relaxation to less than inequality can admit degenerate solution of features going to zero\\





\begin{figure*}[t]
\centering
\begin{tabular}{ccc}
\subfigure[$\lambda=0$  (Pure Softmax) ]{\includegraphics[scale=.3]{visual_00.pdf}} & 
\subfigure[$\lambda=1$ ]{\includegraphics[scale=.3]{visual_01.pdf}} &
\subfigure[$\lambda=10$ ]{\includegraphics[scale=.3]{visual_10.pdf}}% \\
%\subfigure[ $\lambda=0, \Delta \theta \in (20^\circ, 160^\circ ) $  ]{\includegraphics[scale=.3]{softmax_00_05.pdf}} & 
%\subfigure[ $\lambda=1, \Delta \theta \in (10^\circ, 60^\circ )$ ]{\includegraphics[scale=.3]{ring_1_05.pdf}} &
%\subfigure[$\lambda=10, \Delta \theta \in (5^\circ, 6^\circ )$]{\includegraphics[scale=.3]{ring_10_01.pdf}}

\end{tabular}
\vspace{-0.5cm}
\caption{\textbf{Ring loss Visualizations:} (a), (b) and (c) show the final convergence of the samples (for varying $\lambda$). The blue-green dots are the samples before the gradient update and the red dots are the same samples after the update. The dotted blue vector is the target class direction. $\lambda=0$ fails to converge and does not constrain the norm whereas $\lambda=10$ takes very small steps towards Softmax gradients. A good balance is achieved at  $\lambda=1$. In our large scale experiments, a large range of $\lambda$ achieves this balance. }
\label{fig_ring_loss_visual}
\vspace{-0.5cm}
\end{figure*}
% (d), (e) and (f)  show the angular updates obtained through Ring loss + Softmax gradients (for varying $\lambda$). 


 %The Softmax loss is arguably the most prevalent loss function for classification. Technically, it consists of  the last fully connected layer, the cross-entropy layer and a softmax layer. 
 %correctly observe that the Softmax function acts as a max operator invariant to the scaled version of the feature, provided it is the maximum feature. They 
 
 There have been recent studies on the use of norm constraints right before the Softmax loss \cite{Wang2017NormFace, ranjan2017l2}. However, the formulations investigated are non-convex in the feature representations leading to difficulties in optimization. Further, there is a need for better understanding of the benefits of normalization itself. Wang \emph{et.al.} \cite{Wang2017NormFace} argue that the `radial' nature of the Softmax features is not a useful property, thereby cosine similarity should be preferred leading to normalized features. A concrete reason was, however, not provided. Ranjan \emph{et.al.} \cite{ranjan2017l2} show that the Softmax loss encodes the quality of the data (images) into the norm thereby deviating from the ultimate objective of learning a good representation purely for classification.\footnote{We in fact, find in our pilot study that the Softmax features also encode the `difficulty' of the class.} Therefore for better classification, normalization forces the network to be invariant to such details. This is certainly not the entire story and in fact overlooks some key properties of feature normalization. We now motivate Ring loss with three arguments. 1) We show that the norm constraint is beneficial to maintain a balance between the angular classification margins of multiple classes. 2) It removes the disconnect between training and testing metrics. 3) It minimizes test errors due to angular variation due to low norm features.   
 % There still exists the need for a concrete justification for the norm constraint and its effects on classification. 
 %We find more evidence for our intuition in a few pilot experiments on MNIST.
 
%TO ADD: the reasoning is for any method that enforces the inequality

%Similarly, for class 2 classification, we need $w_2^T x_2 > w_2^Tx_1$.  We focus on investigating the classification margin for class 1 for now for the setting depicted in Fig.~\ref{fig_class_margin_setting}.

\textbf{The Angular Classification Margin Imbalance. } Consider a binary classification task with two feature vectors $x_1$ and $x_2$ from class 1 and 2 respectively, extracted using some model (possibly a deep network). Let the classification weight vector for class 1 and 2 be $w_1, w_2$ respectively (potentially Softmax). An example arrangement is shown in Fig.~\ref{fig_class_margin_setting}. Then in general, in order for the class 1 vector $w_1$ to pick $x_1$ \textit{and not} $x_2$ for correct classification, we require $w_1^T x_1 > w_1^Tx_2 \Rightarrow  \|x_1\|_2 \cos \theta_1  >  \|x_2\|_2\cos \theta_2$\footnote{Although, it is more common to in turn investigate competition between two weight vectors to classify a single sample, we find that this alternate perspective provide some novel and interesting insights.}. Here, $\theta_1$ and $\theta_2$ are the angles between the weight vector $w_1$ (class 1 vector only) and $x_1$, $x_2$ respectively\footnote{Note that this reasoning is applicable to any loss function trying to enforce this inequality in some form.}.  We call the feasible set (range for $\theta_1$) for this inequality to hold as the \textit{angular classification margin}. Note that it is also a function of $\theta_2$.  Setting $\frac{\|x_2\|_2}{\|x_1\|_2} = r$, we observe $r>0$ and that for correct classification, we need $\cos \theta_1 > r \cos \theta_2 \Rightarrow \theta_1 < \cos ^{-1}( r \cos \theta_2)$ since $\cos \theta$ is a decreasing function between $[-1, 1]$ for $\theta \in [0, \pi]$. This inequality needs to hold true for any $\theta_2$. Fixing $\cos \theta_2 = \delta$, we have $\theta_1 < \cos ^{-1} (r\delta)$. From the domain constraints of $\cos^{-1}$, we have $-1 \leq r\delta \leq 1 \Rightarrow \frac{-1}{\delta}  \leq r \leq \frac{1}{\delta }$. Combining this inequality with $r>0$, we have $ 0 < r \leq \frac{1}{|\delta|} \Rightarrow  \|x_2\|_2  \leq \frac{1}{\delta } \|x_1\|_2 ~~\forall  \delta \in (0 ~1]$. For our purposes it suffices to only look at the case $\delta > 0$ since the $\delta<0$ doesn't change the inequality $-1 \leq r\delta \leq 1 $ and is more interesting. 


%We thus have one upper bound on $\theta_1$ in terms of $r, \delta$, and the second one on $r$ in terms of $\delta$.

\textbf{Discussion on the angular classification margin. }  We plot the upper bound on $\theta_1$ (\emph{i.e.} $\cos ^{-1}( r \cos \theta_2)$) for a range of $\delta$ ([0.1, 1]) and the corresponding range of $r$. Fig.~\ref{fig_class_margin} showcases the plot. Consider $\delta=0.1$ which implies that the sample $x_2$ has a large angular distance from $w_1$ (about $85^\circ$). This case is favorable in general since one would expect a lower probability of $x_2$ being classified to class 1. However, we see that as $r$ increases (difference in norm of $x_1, x_2$), the classification margin for $x_1$ decreases from $90^\circ$ to eventually $0^\circ$. In other terms, as the norm of $x_2$ increases w.r.t $x_1$, the angular margin for $x_1$ to be classified correctly while rejecting $x_2$ by $w_1$, decreases. The difference in norm ($r>1$) therefore will have an adverse effect during training by effectively enforcing smaller angular classification margins for classes with smaller norm samples. This also leads to lop-sided classification margins for multiple classes due to the difference in class norms as can be seen in Fig.~\ref{fig_softmax}. This effect is only magnified as $\delta$ increases (or the sample $x_2$ comes closer to $w_1$). Fig.~\ref{fig_class_margin} shows that the angular classification margin decreases much more rapidly as $\delta$ increases. However, $r<1$ leads to a larger margin and seems to be beneficial for classifying class 1 (as compared to $r>1$). One might argue that this suggests that the $r<1$ should be enforced for better performance. However, note that the same reasoning applies correspondingly to class 2, where we want to classify $x_2$ to $w_2$ while rejecting $x_1$. This creates a trade off between performance on class 1 versus class 2 based on $r$ which also directly scales to multi-class problems. In typical recognition applications such as face recognition, this is not desirable. Ideally, we would want to represent all classes equally well. Setting $r=1$ or constraining the norms of the samples from both classes to be the same ensures this.
%This reasoning directly extends to the multi-class setting.

\textbf{Effects of Softmax on the norm of MNIST features.} We qualitatively observe the effects of vanilla Softmax on the norm of the features (and thereby classification margin) on MNIST in Fig.~\ref{fig_softmax}. We see that digits 3, 6 and 8 have large norm features which are typically the classes that are harder to distinguish between. Therefore, we observe $r<1$ for these three `difficult' classes (w.r.t to the other `easier' classes) thereby providing a larger angular classification margin to the three classes. On the other hand, digits 1, 9 and 7 have lower norm corresponding to $r>1$ w.r.t to the other classes, since the model can afford to decrease the margin for these `easy' classes as a trade off. We also observe that arguably most easily distinguishable class, digit 1, has the lowest norm thereby the highest $r$. On the other hand, Fig.~\ref{fig_ring_loss} showcases the features learned using Softmax augmented with our proposed Ring loss, which forces the network to learn feature normalization through a convex formulation thereby mitigating this imbalance in angular classification margins. 

%Note that since $\cos \theta \in (-1,1)$, the norm of the features has a much greater effect on the inequality.



%The only solution One solution could be to encode class information in the norm of the features. This would an effective solution in a two-class problem. However, for a multi-class problem, this approach fails. If the norm of the features of one class (say class 1) is higher, then in order for the rest of the classes to be classified correctly, the cosine distance between the other classes and class 1 needs to be even larger making the correct classification requirement harder. Multi-class classification therefore ideally requires the norm of the features  to be equal or close across all classes for effective classification.




 %different class norm in fig 1 left mnist denotes difficulty, for eg. 3, 6, 8 examples cloeses towards origin are harder to classify

%QUANTITATIVE RESULTS ON MNIST

%\textbf{Removing the disconnect between training and testing.} Evaluation using the cosine metric is currently ubiquitous in applications such as face recognition where the features are normalized beforehand in gallery (thereby requiring fewer FLOPs during large scale testing). However, during training, it is not the case and the norm is usually not constrained. This creates a disconnect between training and testing scenarios which hinders performance. Ring loss removes this disconnect in an elegant way.

% Having motivated the norm constraint for a multi-class classification task from an classification margin perspective, we look at typical testing scenarios.


%We propose a simple and elegant approach to the problem. We add the norm constraint as a convex augmentation during training thereby removing the disconnect between training and testing settings allowing for superior performance.  

%Without the norm constraint, the network might learn to encode some class information in the norm itself as explained before. This information would be ignored during testing using a cosine metric which leads to inefficient training. The cosine metric encodes the information in the angle of the feature vector $\theta$, which can be seen as the orthodromic distance on a hypersphere. 






%high norm features do not produce big gradient, low norm features produce much higher gradien

%make training and testing cases similar
\textbf{ Regularizing Softmax loss with the norm constraint.}
The ideal training scenario for a system testing under the cosine metric would be where all features pointing in the same \textit{direction} have the same loss. However, this is not true for the most commonly used loss function, Softmax and its variants (FC layer combined with the softmax function and the cross-entropy loss). Assuming that the weights are normalized, i.e. $\|w_k\|=1$, the Softmax loss for feature vector $\mathcal{F}(\mathbf{x}_i)$ can be expressed as (for the correct class $y_i$):
\begin{align}
L_{SM} &= -\log{
 \frac{
  \exp{w_k \mathcal{F}(\mathbf{x}_i)}}
  {
  \sum_{{k'}=1}^K 
   {\exp{w_{k'} \mathcal{F}(\mathbf{x}_i)}}
  }F
 }\\
 &= -\log{
 \frac{
  \exp{\|\mathcal{F}(\mathbf{x}_i)\|\cos{\theta_{ki}}}}
  {
  \sum_{{k'}=1}^K 
   {\exp{\|\mathcal{F}(\mathbf{x}_i)\|\cos{\theta_{k'i}}}}
  }
 }
\end{align}

%Note that the likelihood term depends on the feature norm. for features with different norms, even though their directions are the same, their losses are not the same in terms of angle.
Clearly, despite having the same direction, two features with different norms have different losses. From this perspective, the straightforward solution to regularize the loss and remove the influence of the norm is to normalize the features before Softmax as explored in $l_2$-constrained Softmax  \cite{ranjan2017l2}. However, this approach is effectively a projection method, i.e. it calculates the loss as if the features are normalized to the same scale, while the actual \textit{network} does not \textit{learn} to normalize features.
%&= -\log{P(y_i|\mathbf{x}_i)}\\
%Thereby, the network does not \textit{learn} to normalize features.  

\textbf{ The need for features normalization in feature space.} As an illustration, consider the training and testing set features trained by vanilla Softmax, of the digit 8 from MNIST in Fig.~\ref{fig_train_test}. Fig.~\ref{fig_softmax_train} shows that at the end of training, the features are well behaved with a large variation in the norm of the features with a few samples with low norm. However, Fig.~\ref{fig_softmax_test} shows that that the features for the test samples are much more erratic. There is a similar variation in norm but now most of the low norm features have huge variation in \textit{angle}. Indeed, variation in samples for lower norm features translates to a larger variation in angle than the same for higher norm samples features. This translates to higher errors in classification under the cosine metric (as is common in face recognition). This is yet another motivation to normalize features during training. Forcing the network to \textit{learn} to normalize the features helps to mitigate this problem during test wherein the network learns to work in the normalized feature space. A related motivation to feature normalization was proposed by Ranjan \emph{et.al.} \cite{ranjan2017l2} wherein it was argued that low resolution of an image results in a low norm feature leading to test errors. Their solution to project (not implicitly learn) the feature to the scaled unit hypersphere was also aimed at handling low resolution. We find in our large scale experiment with low resolution images (see Exp. 6 Fig.~\ref{fig_scale2}) that soft normalization by Ring loss achieves better results. In fact hard projection method by $l_2$-constrained Softmax \cite{ranjan2017l2} performs worse than Softmax for a downsampling factor of 64.


% (instead of directly projecting as in \cite{ranjan2017l2})


%Consider a data sample from class $k$, we can write $\mathbf{x}_i^{\left(k\right)}=\mu_k+s_i$, where $\mu_k$ is the mean of this class and $s_i$ is the shifting vector of sample $\mathbf{x}_i$ from $\mu_k$. Assume we split the data to training and testing with an unbiased sampler, we can write the distribution of training and testing split of the data as: 
%\begin{align*}
%X_{train}^{\left(k\right)}&=\mu_k+S_{train}\\
%X_{test}^{\left(k\right)}&=\mu_k+S_{test},
%\end{align*}
%where $S_{train},S_{test}$ are the random variables for shifting vector of training set and testing set respectively. The variances for the shifting vectors are: 
%\begin{align*}
%    \sigma_{train}^{\left(k\right)}&=\sigma_{share}^{\left(k\right)}+\tilde{\sigma}_{train}%^{\left(k\right)}\\
%    \sigma_{test}^{\left(k\right)}&=\sigma_{share}^{\left(k\right)}+\tilde{\sigma}_{test}^{%\left(k\right)}.
%\end{align*}
%During training, the network tries to bring the effect from training variance, $\sigma_{train}^{\left(k\right)}$, to zero. However, for testing set, it only eliminates effect from $\sigma_{share}^{\left(k\right)}$, leaving $\tilde{\sigma}_{test}^{\left(k\right)}$ independent from the network, i.e. the effect of $\tilde{\sigma}_{test}^{\left(k\right)}$ on the feature space is consistent regardless of the location. Therefore, its effect on angle depends on the feature norm (Fig. \ref{fig_train_test}). Consequently, low-norm features have larger variance in terms of angle and are more likely to be noisy. 


\begin{figure}%{r}{0.5\textwidth}
    \begin{center}
        \subfigure[Features for training set]{%
        \centering
            \includegraphics[width=0.49\columnwidth,height=0.4\columnwidth,valign=m]{deviation_train_1.pdf}\label{fig_softmax_train}
        }%
         \subfigure[Features for testing set]{%
        \centering
        \includegraphics[width=0.49\columnwidth,height=0.4\columnwidth,valign=m]{deviation_test_1.pdf}\label{fig_softmax_test}
        }
    \end{center}
    \vspace{-0.5cm}
\caption{MNIST features for digit 8 trained using vanilla Softmax loss. } 
\label{fig_train_test}
\vspace{-0.7cm}
\end{figure}
%The testing feature is more noisy in general, but noises mainly affect the low-norm area. $\sigma$ is the testing variance independent from training set. $\theta$ is the angle $\left<\mathbf{w}, \mathcal{F}(\mathbf{x}_i)\right>$ with the effect of $\sigma$ and clearly depends on the feature norm.


%This kind of noise results in false positive much more often than false negative when testing in large scale, e.g. face recognition, because negative pairs appear more often than positive pairs. Therefore, one can reduce the false positive rate if features are actually normalized in feature space. 

\textbf{ Incorporating the norm constraint as a convex problem.} Identifying the need to normalize the sample features from the network, we now formulate the problem. We define $L_S$ as the primary loss function (for instance Softmax loss). Assuming that $\mathcal{F}$ provides deep features for a sample $x$ as $\mathcal{F}(x)$, we would like to minimize the loss subject to the normalization constraint as follows,
\begin{align}
\min L_S(\mathcal{F}(x)) ~~s.t. ~~ \|\mathcal{F}(x)\|_2=R \label{eq_1}
\end{align}
Here, $R$ is the scale constant that we would like the features to be normalized to. This is the exact formulation recently studied and implemented by \cite{ranjan2017l2, Wang2017NormFace}. Note that this problem is non-convex in $\mathcal{F}(x)$ since the set of feasible solutions is itself non-convex due to the norm equality constraint. Approaches which use standard SGD while ignoring this critical point would not be providing feasible solutions to this problem thereby, the network $\mathcal{F}$ would not learn to output normalized features. Indeed, the features obtained using this straightforward approach are not normalized as was found in Fig. 3b in \cite{ranjan2017l2} compared to our approach (Fig.~\ref{fig_ring_loss}). One naive approach to get around this problem would be to relax the norm equality constraint to an inequality. This objective will now be convex, however it does not necessarily enforce equal norm features. In order to incorporate the formulation as a convex constraint, the following form is directly useful as we find below.


%ADD IN EXP WITH NO NORMALIZATION OF SOFTMAX WEIGHTS, AND NORMALIZING FEATURES





%This corresponding relaxed version of the optimization problem becomes,
%\begin{align}
%\min_\theta L_S(\mathcal{F}(x); \theta) ~~s.t. ~~ \|\mathcal{F}(x)\|_2\leq R
%\end{align}


%We now incorporate the norm constraint into the objective function as a convex constraint.


\subsection{Ring loss}
%add Lagrangian, normalization through weights and not explicit operation

\textbf{Ring loss Definition. } Ring loss $L_R$ is defined as
%We define the Ring loss $L_R$ for a mini-batch of size $m$ with samples $\mathbf{x}_i$ for a network $\mathcal{F}$ as the Lagrangian of Equation.~\ref{eq_1},
\begin{align} 
L_R = \frac{\lambda}{2m} \sum_{i=1}^m (\|\mathcal{F}(\mathbf{x}_i)\|_2  -  R)^2
\end{align}
where $\mathcal{F}(\mathbf{x}_i)$ is the deep network feature for the sample $\mathbf{x}_i$.  Here, $R$ is the target norm value which is also learned and $\lambda$ is the loss weight enforcing a trade-off between the primary loss function. $m$ is the batch-size. The square on the norm difference helps the network to take larger steps when the norm of a sample is too far off from $R$ leading to faster convergence. The corresponding gradients are as follows.
\begin{align}
\frac{\partial L_R}{\partial R } &= -\frac{\lambda}{m}\sum_{i=1}^m (\|\mathcal{F}(\mathbf{x}_i)\|_2  -  R) \\ \frac{\partial L_R}{\partial \mathcal{F}(\mathbf{x}_i) }&=  \frac{\lambda}{m}  \left(1  -  \frac{R}{\|\mathcal{F}(\mathbf{x}_i)\|_2}  \right) \mathcal{F}(\mathbf{x}_i) 
\end{align}

% &=   \frac{\lambda}{m}  \left(\|\mathcal{F}(\mathbf{x}_i)\|_2  -  R \right) \frac{\mathcal{F}(\mathbf{x}_i)}{\|\mathcal{F}(\mathbf{x}_i)\|_2} \\ 


Ring loss ($L_R$) can be used along with any other loss function such as Softmax or large-margin Softmax \cite{liu2017sphereface}. The loss encourages norm of samples being value $R$ (a learned parameter) rather than explicit enforcing through a hard normalization operation. This approach provides informed gradients towards a better minimum which helps the network to satisfy the normalization constraint. The network therefore, learns to normalize the features using \textit{model weights themselves} (rather than needing an explicit non-convex normalization operation as in \cite{ranjan2017l2}, or batch normalization \cite{ioffe2015batch}). In contrast and in connection, batch normalization \cite{ioffe2015batch} enforces the scaled normal distribution for each element in the feature independently. This does not constrain the overall norm of the feature to be equal across all samples and neither addresses the class imbalance problem. As shown in Fig. \ref{fig_accuracy}, Ring loss stabilizes the feature norm across all classes, and, in turn, rectifies the classification imbalance for Softmax to perform better overall. 

\begin{figure}%{r}{0.5\textwidth}
    \begin{center}
        \subfigure{%
        \centering
        \includegraphics[width=0.8\columnwidth,height=0.35\columnwidth,valign=m]{Accuracy.pdf}
        }
        \hspace{-0.85cm}
        \subfigure{%
        \centering
        \includegraphics[width=0.15\columnwidth,height=0.30\columnwidth,valign=m]{Legend.pdf}
        }
    \end{center}
    \vspace{-0.5cm}
\caption{Ring loss improves MNIST testing accuracy across all classes by reducing inter-class norm variance. Norm Ratio is the ratio between average class norm and average norm of all features.} 
\label{fig_accuracy}
\vspace{-0.7cm}
\end{figure}

\textbf{Ring loss Convergence Visualizations.} To illustrate the effect of the Softmax loss augmented with the enforced soft-normalization, we conduct some analytical simulations. We generate a 2D mesh of points from $( -1.5, 1.5 )$ in x,y-axis. We then compute the gradients of Ring loss ($R=1$) assuming the dottef blue vertical line (see Fig.~\ref{fig_ring_loss_visual}) as the target class and update each point with a fixed step size for 20 steps. We run the simulation for $\lambda = \{ 0, 1, 10 \}$. Note that $\lambda=0$ represents pure Softmax. Fig.~\ref{fig_ring_loss_visual} depicts the results of these simulations.  Sub-figures (a), (b) and (c)  in Fig.~\ref{fig_ring_loss_visual} show the initial points on the mesh grid (light green) and the final updated points (red). For pure Softmax ($\lambda=0$), we see that the updates increases norm of the samples and moreover they do not converge. For a reasonable loss weight of $\lambda=1$, Ring loss gradients can help the updated points converge much faster in the same number of iterations. For heavily weighted Ring loss with $\lambda=10$, we see that the gradients force the samples to a unit norm since $R$ was set to 1 while overpowering Softmax gradients. These figures suggest that there exists a trade off enforced by $\lambda$ between the Softmax loss $L_S$ and the normalization loss. We observe similar trade-offs in our experiments.\\
%\textbf{Angular update visualization. }  Sub-figures (d), (e) and (f) on the other hand illustrate the change in angle ($\Delta \theta$) for a single update for the same $\lambda$'s. The range for $\Delta \theta$ denoted is the min and max angle within each plot. We see that for pure Softmax $\lambda=0$, there is a significant variation in angle. Samples with higher norm result in a smaller change in angle. This is to be expected in general but is not an ideal situation. Ideally we would want samples from a single class to have similar angular changes irrespective of norm. This is what is achieved by the heavily weighted Ring loss with $\lambda=10$ (Fig.~\ref{fig_ring_loss_visual}(c)). However, the samples as shown in the figure do not converge near the correct class. Thereby, we need a good balance between Softmax and the norm loss through $\lambda$. Indeed as Fig.~\ref{fig_ring_loss_visual}(b) shows, Ring loss with $\lambda=1$ more uniform angular change for different norms than pure Softmax and while observing better convergence than heavily weighted Ring loss ($\lambda=10$).



%\section{Analysis of the benefits of feature normalization.}
%We now present some analytical insights into the benefits of feature normalization in deep networks. We assume a deep network $\mathcal{F}$ with a fully connected layer with weights $W\in \mathbb{R}^{d\times m}$ before the Softmax loss. The output of this layer is used as the final representation vector. We study the properties of the input space to this fully connected layer. We assume the eigen vectors of $W^TW$ to be $\{ e_1, e_2, .. e_m\}$ with the corresponding eigen values as $\{\lambda_i\}$. Let the inputs to this layer be of the form $x_1 = \sum_i^m \alpha_i e_i + \epsilon_1$. Consider another input $x_2$ with corresponding parameters $\beta_i$ and $\epsilon_2$. Here $\epsilon_1, \epsilon_2$ model any component of the input that the weight matrix does not span,  \emph{i.e.} it lies in the null space or $W^TW\epsilon=0$. If one assumes that the features of this layer were $\textit{learnt}$ to be normalized with norm $R_1, R_2$ respectively (rather than an explicit projection), then we have $\|Wx_1\|_2^2 = x_1 W^TWx_1 = R_1^2, \|Wx_2\|_2^2 = x_2 W^TWx_2 = R_2^2$. Putting in the expressions for the inputs in terms of $\alpha_i, \beta_i, e_i$ we have, 
%\begin{align}
%\sum_i \alpha_i^2\lambda_i = R_1^2\\
%\sum_i \beta_i^2\lambda_i  = R_2^2
%\end{align}
%This is because $\epsilon_1, \epsilon_2$ lie in the null space of $W^TW$. 

%\textbf{Case 1: $R_1=R_2$}
%In the case when $R_1=R_2$ or the norm of all samples are the same, we can subtract the two equations to obtain, $\sum_i (\alpha_i^2 - \beta_i^2)\lambda_i = 0  $

\section{Experimental Validation}
%benchmark against batch normalization right before softmax


%mention that you run center loss yourself with your exact setting using the published code on the actual lfw protocol. describe exactly what we did for more credibility.
%Face recognition is a challenging task in general due to a large number of classes, i.e. above 100,000 classes, with typically few samples per class for training (average 100 samples per subject for MS-Celeb 1M \cite{guo2016ms} compared to say 1000 for ImageNet).


We benchmark Ring loss on large scale face recognition tasks while augmenting two loss functions. The first one is the ubiquitous Softmax, and the second being a successful variant of Large-margin Softmax \cite{liu2016large} called SphereFace \cite{liu2017sphereface}.  We present results on five large-scale benchmarks of LFW \cite{LFWTech}, IARPA Janus Benchmark IJB-A   \cite{klare2015pushing}, Janus Challenge Set 3 (CS3) dataset (which is a super set of the IJB-A Janus dataset), Celebrities Frontal-Profile (CFP) \cite{sengupta2016frontal} and finally the MegaFace dataset \cite{kemelmacher2016megaface}. We also present results of Ring loss augmented Softmax features on low resolution images from Janus CS3 to showcase resolution robust face matching.

\begin{figure}%{r}{0.5\textwidth}
    %\begin{center}
        %\subfigure[CFP Frontal vs. Profile Verification]{%
        \centering
            \includegraphics[width=0.9\columnwidth,valign=m]{CFP.eps}
        %}%
        %\subfigure[Center loss augmentation]{%
        %\centering
        %    \includegraphics[width=0.3\columnwidth,valign=m]{janus_softmax.eps}\label{fig_janus_centerloss}
        %}%
         %\subfigure[Janus CS3 1:1 Verification]{%
        %\centering
        %\includegraphics[width=0.9\columnwidth,valign=m]{cs3_ringloss_cuvres.eps}\label{fig_low_res}
        %}
        
    %\end{center}
    \vspace{-0.2cm}
\caption{ ROC curves on the CFP Frontal vs. Profile verification protocol. For all Figures and Tables, SM denotes Softmax, SF denotes SphereFace \cite{liu2017sphereface},l2-Cons SM denotes \cite{ranjan2017l2}, + CL denotes Center Loss augmentation \cite{wen2016discriminative} and finally + R denotes Ring loss augmentation. Numbers in bracket denote value of hyperparameter (loss weight), \emph{i.e.} $\alpha$ for \cite{ranjan2017l2}, $\lambda$ for Center loss and Ring loss.}
\label{fig_cfp}
\vspace{-0.5cm}
\end{figure}


%$l2$-constrained Softmax \cite{ranjan2017l2} fails to constrain the norm.

% the Face-ResNet architecture from Wen \emph{et. al.} \cite{wen2016discriminative} and 
\textbf{Implementation Details.}
For all the experiments in this paper, we usethe ResNet 64 (Res64) layer architecture from Liu \emph{et. al.} \cite{liu2017sphereface}. For Center loss, we utilized the code repository online and used the best hyperparameter setting reported\footnote{see \url{https://github.com/ydwen/caffe-face.git}}. The $l_2$-constrained Softmax loss was implemented follwing \cite{ranjan2017l2}  by integrating a normalization and scaling layer\footnote{see \url{https://github.com/craftGBD/caffe-GBD}. In our experiments, for $\alpha=50$ the gradients exploded due the relatively deep Res64 architecture and learning $\alpha$ initialized at 30 did not converge.} before the last fully-connected layer. For experiments with L-softmax \cite{liu2016large} and SphereFace \cite{liu2017sphereface}, we used the publicly available Caffe implementation. The Resnet 64 layer (Res64) architecture results in a feature dimension of 512 (at the fc5 layer), which is used for  matching using the cosine distance. Ring loss and Center loss are both applied on this feature \emph{i.e.} to the output of the fc5 layer.  All models were trained on the MS-Celeb 1M dataset \cite{guo2016ms}. The dataset was cleaned to remove potential outliers within each class and also noisy classes before training. To clean the dataset we used a pretrained model to  extract features from the MS-Celeb 1M dataset. Then, classes that had variance in the MSE, between the sample features and the mean feature of that class, above a certain threshold were discarded. Following this, from the filtered classes, images that have their MSE error between their feature vector and the class mean feature vector higher than a threshold are discarded. After this procedure, we are left with about 31,000 identities and about 3.5 million images. The learning rate was initialized to 0.1 and then decreased by a factor of 10 at 80K and 150K iterations for a total of 165K iterations. All models evaluated were the 165K iteration model\footnote{For all Tables, results reported after double horizontal lines are from models trained during our study. The results above the lines reported directly from the paper as cited.}.



 

%The second fully-connected layer (fc6), along with the Softmax loss layer, forms the Softmax loss.
%first train a Softmax model on the CASIA-WebFace with 10,000 subjects and about 0.5 million images to obtain a pre-trained model tuned to our cropping. We then 


%The original dataset which contains 10 million images and 100K identities was cleaned to remove mislabellings and outliers.
%We use 5 point face alignment in this work. 

\textbf{Preprocessing.} All faces were detected and aligned using \cite{zhang2016joint} which provided landmarks for the two eye centers, nose and mouth corners (5 points). Since MS-Celeb1M, IJB-A Janus and Janus CS3 have harder faces we use a robust detector \emph{i.e.} CMS-RCNN \cite{zhu2017cms} to detect faces and a fast landmarker that is robust to pose \cite{bhagavatula2017faster}. The faces were then aligned using a similarity transformation and were cropped to $112\times 96$ in the RGB format. The pixel level activations were normalized by subtracting 127.5 and then dividing by 128. For failed detections, the training set images are ignored. In the case of testing, ground truth landmarks were used from the corresponding dataset.

%Our model, Face-ResNet, consists of 27 $3\times3$ convolution layers and two fully-connected (fc) layers (c1, c2, c3, c4, fc5, fc6). Convolution layers with residual connections are grouped in four convolution blocks (c1, c2, c3, c4), which contain 4, 5, 11 and 7 convolution layers respectively. The convolution blocks are connected by stride 2 max-pooling layers. Instead of global pooling, the first fully-connected layer (fc5), takes in all values from the last convolution feature map.


%LFW experiments follow protocol I and report the accuracy of the last iteration, i.e. 28K. IJB-A Janus and MegaFace experiments follow protocol II. The model at the best iteration is selected via cross-validation on LFW. Results are reported on that iteration. 






\begin{figure*}%{r}{0.5\textwidth}
    \begin{center}
        \subfigure[IJB-A Janus 1:1 Verification]{%
        \centering
            \includegraphics[width=0.9\columnwidth,valign=m]{janus_curves.eps}\label{fig_janus}
        }%
        %\subfigure[Center loss augmentation]{%
        %\centering
        %    \includegraphics[width=0.3\columnwidth,valign=m]{janus_softmax.eps}\label{fig_janus_centerloss}
        %}%
         \subfigure[Janus CS3 1:1 Verification]{%
        \centering
        \includegraphics[width=0.9\columnwidth,valign=m]{cs3_ringloss_cuvres.eps}\label{fig_cs3}
        }
        
    \end{center}
    \vspace{-0.5cm}
\caption{ ROC curves on the (a) IJB-A Janus 1:1 verification protocol and the (b) Janus CS3 1:1 verification protocol. For all Figures and Tables, SM denotes Softmax, SF denotes SphereFace \cite{liu2017sphereface},l2-Cons SM denotes \cite{ranjan2017l2}, + CL denotes Center Loss augmentation \cite{wen2016discriminative} and finally + R denotes Ring loss augmentation. Numbers in bracket denote value of hyperparameter (loss weight), \emph{i.e.} $\alpha$ for \cite{ranjan2017l2}, $\lambda$ for Center loss and Ring loss.}
\vspace{-0.5cm}
\end{figure*}




%\textbf{Training Data: CASIA-WebFace. } The CASIA-WebFace database contains about 10,575 subjects and 494,414 images. These were collected in a semi automated fashion. The dataset has a wide variety of faces with varying degrees of degradations such as illumination, color, in plane rotation and variation in age and pose. While most faces are unoccluded, there does exist a minimal amount of occlusion in some of the images covering upto 15 \% of the face due either shadows or objects. There is extreme pose variation in a few of the images (upto 90 degrees), whereas many of the images have pose variation between 25 degrees to 45 degrees. CASIA has more number of images per subject than most other databases such as LFW which makes it a good candidate to train large scale models. 



%Thus, we will train the CMU face matcher on the full CASIA database to incorporate as much training data as possible.



\textbf{Exp 1. Testing Benchmark: LFW.} The LFW \cite{LFWTech} database contains about 13,000 images for about 1680 subjects with a total of 6,000 defined matches. The primary nuisance transformations are illumination, pose, color jittering and age. As the field has progressed, LFW has been considered to be saturated and prone to spurious minor variances in performance (in the last \% of accuracy) owing to the small size of the protocol. Small differences in accuracy on this protocol do not accurately reflect the generalizing capabilities of a high performing model. Nonetheless, as a benchmark, we report performance on this dataset.

% For this dataset, we only use the model obtained at the very end of training (\emph{i.e.} the 165K iteration model for Res64).

\textbf{Results: LFW.}  Table~\ref{tab_lfw} showcases the results on the LFW protocol. We find that for Softmax (SM + R), Ring loss normalization seems to significantly improve performance (up from 98.47\% to \textbf{99.52\%} using Ring loss with $\lambda=0.01$). We find similar trends while using Ring loss with SphereFace. The LFW accuracy of SphereFace improves from 99.47\% to \textbf{99.50\%}. We note that since even our baselines are high performing, there is a lot of variance in the results owing to the small size of the LFW protocol (just 6000 matches compared to about 8 million matches in the Janus CS3 protocol which shows clearer trends). Indeed we find clearer trends with MegaFace, IJB-A and CS3 all of which are orders of magnitude larger protocols.
% Moreover, from Fig.~\ref{fig_lfw_softmax} we observe that Ring is able to provide a performance boost for a large range of $\lambda$. These results support the perception of the face recognition community that LFW has become saturated and prone to spurious variance in results near the high 99 percent and should not be used alone to evaluate robust face recognition systems.

%Further, Ring loss seems to provide performance benefits even with low values of $\lambda$ as seen from Fig.~\ref{fig_lfw_sphereface}. On a broader note, strong and harder loss functions such as SphereFace, which optimizes for a large angular margin seems to require softer normalization through Ring loss (lower $\lambda$). Indeed, a higher $\lambda$ seems to invoke the effect simulated in our Ring loss visualization in Fig.~\ref{fig_ring_loss_visual}. Here, the Ring loss gradients are directed more towards a scaled unit circle rather than the correct class direction. \footnote{Since Ring loss actively works to constrain the norm through the model weights themselves (rather than hard normalization), we found that it interferes with losses which try to implicitly constrain the norm of different classes to different values such as Center loss \cite{wen2016discriminative}.}

%%%%%%%%%%%%%%%%%%
%\begin{wrapfigure}{r}{0.40\textwidth}

%%\begin{figure}
%\centering
%\includegraphics[width=1.0\columnwidth,valign=m]{fig/MMIF_motivation.pdf}
%\includegraphics[scale=.5]{downscale_results.eps}


%\includegraphics[width=0.4\columnwidth,valign=m]{image/Table1.pdf}
%\centering 
%\caption{ downscale}
%\label{fig_downscale_LFW}
%%\end{figure}
%\end{wrapfigure}
%%%%%%%%%%%%%%%%%%%%

\textbf{Exp 2. Testing Benchmark: IJB-A Janus.} IJB-A \cite{klare2015pushing} is a challenging dataset which consists of 500 subjects with  extreme pose, expression and illumination with a total of 25,813 images. Each subject is described by a template instead of a single image. This allows for score fusion techniques to be developed. The setting is suited for applications which have multiple sources of images/video frames. We report results on the 1:1 template matching protocol containing 10 splits with about 12,000 pair-wise template matches each resulting in a total of 117,420 template matches. The template matching score for two templates $T_i, T_j$ is determined by using the following formula, $S(T_i, T_j) =  \sum_{\gamma=1}^K \frac{\sum_{t_a\in T_i, t_b\in T_j}  s(t_a, t_b)  \exp{ \gamma s(t_a, t_b) } }{  \sum_{t_a\in T_i, t_b\in T_j} \exp{ \gamma s(t_a, t_b) }   } $ where $s(t_a. t_b)$ is the cosine similarity score between images $t_a, t_b$ and $K=8$. 

%In this benchmark, we report results for the Res64 architecture models, we report results on the 165K iteration since it offers a large number of iterations at all learning rates for all models allowing the models to stabilize for fairer comparison.


\textbf{Results: IJB-A Janus.} Table.~\ref{tab_janus} and Fig.~\ref{fig_janus} present these results. We see that Softmax + Ring loss (0.001) outperforms Softmax by a large margin, particularly 60.52\% verification rate compared to \textbf{78.41\%} verification at $10^{-5}$ FAR. Further, it outperforms Center loss \cite{wen2016discriminative} (46.01\%) and $l_2$-constrained Softmax (73.29\%)  \cite{ranjan2017l2}. Although SphereFace performs better than Softmax + Ring loss, an augmentation by Ring loss boosts SphereFace's performance from 78.52\% to \textbf{82.41\%} verification rate for $\lambda=0.01$. This matches the state-of-the-art reported in \cite{ranjan2017l2} which uses a 101-layer ResNext architecture despite our system using a much shallower 64-layer ResNet architecture. The effect of high $\lambda$ akin to the effects simulated in Fig.~\ref{fig_ring_loss_visual} show in this setting for $\lambda=0.03$ for SphereFace augmentation. We observe this trade-off in Janus CS3, CFP and MegaFace results as well. Nonetheless, we notice that Ring loss augmentation provides consistent improvements over a large range of $\lambda$ for both Softmax and Sphereface. This is in sharp contrast with  $l_2$-constrained Softmax whose performance varies significantly with $\alpha$ rendering it difficult to optimize. In fact for $\alpha=10$, it performs worse than Softmax. 

%Fig.~\ref{fig_janus} shows the ROC curves obtained for multiple values of $\lambda$ for Ring loss augmenting both Softmax and SphereFace. 


%For Softmax, all values of $\lambda$ experimented with provide a significant boost in performance. Whereas for SphereFace, high $\lambda$ hurts. However, a lower $\lambda$ around 0.0001 provides considerable improvement.


\textbf{Exp 3. Testing Benchmark: Janus CS3.} The Janus CS3 dataset is a super set of the IARPA IJB-A dataset. It contains about 11,876 still images and 55,372 video frames from 7,094 videos. For the CS3 1:1 template verification protocol there are a total of 1,871 subjects and 12,590 templates. The CS3 template verification protocol has over 8 million template matches which amounts to an extremely large number of template verifications. There are about 1,870 templates in the gallery and about 10,700 templates in the probe. The CS3 protocol being a super set of IJB-A, has a large number of extremely challenging images and faces. The challenging conditions range from extreme illumination, extreme pose to significant occlusion. For sample images, please refer to Fig. 4 and Fig. 10 in \cite{lin2017proximity}, Fig. 1 and Fig. 6 in \cite{bodla2017deep}. Since the protocol is template matching, we utilize the same template score fusion technique we utilize in the IJB-A results with $K=2$.

\begin{figure*}%{r}{0.5\textwidth}
    \begin{center}
        \subfigure[Downsampling 4x]{%
        \centering
            \includegraphics[width=0.4\columnwidth,valign=m]{scale2.eps}
        }%
        %\subfigure[Center loss augmentation]{%
        %\centering
        %    \includegraphics[width=0.3\columnwidth,valign=m]{janus_softmax.eps}\label{fig_janus_centerloss}
        %}%
         \subfigure[Downsampling 16x]{%
        \centering
        \includegraphics[width=0.4\columnwidth,valign=m]{scale4.eps}
        }
         \subfigure[Downsampling 25x]{%
        \centering
        \includegraphics[width=0.4\columnwidth,valign=m]{scale5.eps}
        }
         \subfigure[Downsampling 36x]{%
        \centering
        \includegraphics[width=0.4\columnwidth,valign=m]{scale6.eps}
        }
         \subfigure[Downsampling 64x]{%
        \centering
        \includegraphics[width=0.4\columnwidth,valign=m]{scale8.eps}
        }
    \end{center}
    \vspace{-0.5cm}
\caption{ ROC curves for the downsampling experiment on Janus CS3. Ring loss (SM + R $\lambda=0.01$) learns the most robust features, whereas $l_2$-constrained Softmax (l2-Cons SM $\alpha=30$) \cite{ranjan2017l2} performs poorly (worse than the baseline Softmax) at very high downsampling factor of 64x. }
\label{fig_scale2}
\vspace{-0.5cm}
\end{figure*}

\textbf{Results: Janus CS3.} Table.~\ref{tab_cs3} and Fig.~\ref{fig_cs3} showcases our results on the CS3 dataset. We report verification rates (VR) at $10^{-3}$ through $10^{-6}$ FAR. We find that our Ring loss augmented Softmax model outperforms the previous best reported results on the CS3 dataset. Recall that the Softmax + Ring loss model (SM + R) was trained only on a subset of the MS-Celeb dataset and achieves a VR of 74.56\% at $10^{-4}$ FAR. This is in contrast to Lin \emph{et. al.} who train on MS-Celeb plus CASIA-WebFace (an additional 0.5 million images) and achieve 72.52 \%. Further, we find that even though our baseline Sphereface Res64 model outperforms the previous state-of-the-art, our Ring loss augmented Sphereface model outperforms all other models to achieve high a VR of \textbf{82.74 \%} at $10^{-4}$ FAR. At very low FAR of $10^{-6}$ our SF + R model achieves VR \textbf{35.18 \%} which to the best of our knowledge is the state-of-the-art on the challenging Janus CS3. In accordance with the results on IJB-A Janus, Ring loss provides consistent improvements over large ranges of $\lambda$ whereas $l_2$-constrained Softmax exhibits significant variation w.r.t. to its hyperparameter.




\textbf{Exp 4. Testing Benchmark: MegaFace. } The recently released MegaFace benchmark is extremely challenging which defines matching with a gallery of about 1 million distractors \cite{kemelmacher2016megaface}. The aspect of face recognition that this database test is discrimination in the presence of very large number of distractors. The testing database contains two sets of face images. The distractor set contains 1 million distractor subjects (images). The target set contain 100K images from 530 celebrities.

%Following the testing protocol, we extract features from both sets of face images and match against the large-scale gallery.





\textbf{Result: MegaFace.}  Table.~\ref{tab_megaface} showcases our results. Even at this extremely large scale evaluation (evaluating FaceScrub against 1 million), the addition of Ring loss provides significant improvement to the baseline approaches. The identification rate ($\%$) for Softmax upon the addition of Ring loss ($\lambda = 0.001$) improves from 56.36\% to a high \textbf{71.67\%} and for SphereFace it improves from 74.95\% to \textbf{75.22\%} for a \textit{single} patch model. This is higher than the single patch model reported in the orginal Sphereface paper (72.72\% \cite{liu2017sphereface}). We outperform Center loss  \cite{wen2016discriminative} augmenting both Softmax (67.24\%) and Sphereface (71.15\%). We find that though for MegaFace, $l_2$-constrained Softmax \cite{ranjan2017l2} for $\alpha=30$  achieves 72.22\%, there is yet again significant variation in performance that occurs due to a change in the hyper parameter $\alpha$ (66.20\% for $\alpha=10$ to 72.22\% for $\alpha=30$). Ring loss hyper parameter ($\lambda$), as we find again, is more easily tunable and manageable. This results in a smaller variance in performance for both Softmax and SphereFace augmentations.

%This provides further evidence that convex normalization optimization performs better than the straight forward hard normalization approach. 




%%%%%%%%%%%%%%%%%%
%OLD TABLE
\begin{table}%[!t]
\small
%\tiny
\caption{Accuracy (\%) on LFW. } % title of Table
\centering % used for centering table
\begin{tabular}{l c c } % centered columns (4 columns)

\hline\hline %inserts double horizontal lines


Method & Training Data   & Accuracy (\%)   \\
\hline % inserts single horizontal line
FaceNet \cite{schroff2015facenet}  & 200M private  & 99.65   \\ % inserting body of the table
%DeepFace \cite{taigman2014deepface}  &  4.4M private   &   97.35 \\
%DeepID \cite{taigman2014deepface}  & 88K   &   97.45 \\
%Masi \emph{et. al.} \cite{masi2016we}   &  WebFace+2.4M synth     & 98.06   \\
%Multi-batch\cite{tadmor2016learning} & 2.6M    &  98.20  \\
Deep-ID2+  \cite{sun2014deep}  &  CelebFace+	& 99.15  \\
Range loss \cite{ZhangFWL016}  &  WebFace &  99.52 \\
  &  +Celeb1M(1.5M) &  \\
Baidu \cite{liu2015targeting}  &  1.3M &  99.77 \\


%Wang \emph{et. al.}  \cite{WangOJ15} & WebFace   & 97.52 \\
%DCNN \cite{chen2016unconstrained} & WebFace  &  97.45  \\
%Yi \emph{et. al.} \cite{yi2014learning}  & WebFace &  97.73 \\
Norm Face \cite{Wang2017NormFace} & WebFace  &   99.19 \\

\hline
\hline

SM    &   MS-Celeb     &   98.47  \\
$l2$-Cons SM (30) \cite{ranjan2017l2}   &   MS-Celeb     &   99.55  \\ 
$l2$-Cons SM (20) \cite{ranjan2017l2}   &   MS-Celeb     &   99.47  \\ 
$l2$-Cons SM (10) \cite{ranjan2017l2}   &   MS-Celeb     &   99.45  \\ 
SM + CL \cite{wen2016discriminative}   &  MS-Celeb     &     99.17   \\ 
SF   \cite{liu2017sphereface}   &   MS-Celeb     &   99.47   \\
SF + CL \cite{wen2016discriminative, liu2017sphereface}  &  MS-Celeb     &   99.52   \\


\hline
%MS-Celeb 1M &    \\ 
%\hline
SM + R  ($0.01$)    &  MS-Celeb     & 99.52    \\
SM + R  ($0.001$)   &  MS-Celeb     &  99.50  \\
SM + R  ($0.0001$)  & MS-Celeb   &    99.28    \\ 
\hline
SF + R  ($0.03$)   &   MS-Celeb     &    99.48  \\ 
SF + R  ($0.01$)   &   MS-Celeb     &    99.43 \\ 
SF + R  ($0.001$)   &   MS-Celeb     &   99.42 \\ 
SF + R  ($0.0001$)   &  MS-Celeb     &  99.50   \\ 

\hline
%inserts single line
\end{tabular}
\label{tab_lfw} % is used to refer this table in the text
%\vspace -2mm
\end{table}
%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%
%\begin{table}%[!t]
%\small
%\caption{Results on IJB-A Janus} % title of Table
%\centering % used for centering table
%\begin{tabular}{l c  c} % centered columns (4 columns)
%\hline\hline %inserts double horizontal lines
%Method  & 0.001 FAR &  0.01 FAR \\
%\hline % inserts single horizontal line
% Wang \emph{et. al.} \cite{WangOJ15} & 51.4 & 73.3 \\ % inserting body of the table
%DCNN \cite{chen2016unconstrained}  & -  &   57.3 \\
%DCNN$_{ft}$ \cite{chen2016unconstrained}&  -  &  64.0  \\
%DCNN$_{ft + m}$ \cite{chen2016unconstrained} &  -  &   78.7 \\
%DCNN$_{ft + m + c}$ \cite{chen2016unconstrained} &  -  &   81.8 \\
%DCNN$_{fusion}$ \cite{chen2016unconstrained}  &  -  &   83.8 \\
%PAM \cite{masi2016pose} &   65.2  &  82.6  \\
%Chen \emph{et. al.} \cite{chen2015end}  &  95.0 &  84.4 \\
%\hline
%Softmax    &   55.01   &  74.20 \\ 
%$l2$-constrained Softmax &   59.95   &  78.94  \\ 
%Center loss &  59.59  &   85.32  \\ 
%Softmax + R ($\lambda=0.01$)(Ours)   &  \textbf{66.24}   &   \textbf{82.05}  \\ 
%\hline
%SphereFace   &   72.84  &  87.64 \\ 
%SphereFace + R ($\lambda=0.0001$)(Ours)  &  \textbf{76.90}  &  \textbf{88.68} \\ 
%\hline
%\end{tabular}
%\label{tab_sectionc} % is used to refer this table in the text
%\end{table}
%\begin{table}%[!t]
%\small
%\caption{Results on MegaFace. + R denotes Ring loss} % title of Table
%\centering % used for centering table
%\begin{tabular}{l c c} % centered columns (4 columns)
%\hline\hline %inserts double horizontal lines
%Method & Training Protocol & Accuracy \%  \\
%\hline
%Center loss  & small  &    59.71\% \\ 
%Softmax & small  &  52.64   \%\\ 
%L2-Constrained Softmax & small  &    66.41 \%\\ 
%Softmax + R  ($\lambda=0.01$)  & small &  67.01 \%\\ 
%\hline
%SphereFace & small  &    67.88  \%\\ 
%SphereFace + R ($\lambda = $)(Ours) &  small  &     \%\\ 
%\hline
%\end{tabular}
%\label{tab_sectionc} % is used to refer this table in the text
%\end{table}
%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
%\parbox{.5\linewidth}{
\centering
\begin{tabular}{l  c c } % centered columns (4 columns)

\hline\hline %inserts double horizontal lines
Method   & Acc \% (MegaFace)  &  $10^{-3}$ (CFP)  \\
\hline
\hline
%Center loss   &    59.71\% \\ 
%CASIA-WebFace  &    \\ 
%\hline
%Center loss* (FR)\cite{wen2016discriminative}  & Small  & 65.23\% \\ 
%SM (FR) & Small &  52.64   \%\\ 
%$l_2$-Cons SM (FR) \cite{ranjan2017l2} & Small   &    66.41 \%\\ 
%SM + R (FR) & Small  &  \textbf{67.01} \%\\ 
% inserts single horizontal line
%Center loss + Ring loss ($\lambda$= 0.1) & small  &     \% \\ 
%\hline
%SF  (FR)\cite{liu2017sphereface}& Small  &    67.88  \%\\ 
%SF + R (FR)  & Small  &    \textbf{68.62} \%\\ 


SM    &     56.36  &   55.86   \\
$l2$-Cons SM (30) \cite{ranjan2017l2}   &  72.22 &   82.14   \\ 
$l2$-Cons SM (20) \cite{ranjan2017l2}   &   70.29    &  83.69  \\ 
$l2$-Cons SM (10) \cite{ranjan2017l2}   &   66.20   &   76.77  \\ 
SM + CL \cite{wen2016discriminative}   &  67.24     & 78.94   \\ 
SF   \cite{liu2017sphereface}   &   74.95  &   89.94  \\
SF + CL \cite{wen2016discriminative, liu2017sphereface}  &  71.15  &   82.97 \\


\hline
%MS-Celeb 1M &    \\ 
%\hline
SM + R  ($0.01$)    &  71.10  &  87.43  \\
SM + R  ($0.001$)   &  71.67 &  81.29  \\
SM + R  ($0.0001$)  &   69.41  &  76.30  \\ 
\hline
SF + R  ($0.03$)   &    73.05 & 86.23    \\ 
SF + R  ($0.01$)   &     74.93  & \textbf{ 90.94}   \\ 
SF + R  ($0.001$)   &   \textbf{ 75.22 } &  87.69   \\ 
SF + R  ($0.0001$)   &   74.45  &   88.17 \\ 





\hline
%inserts single line
\end{tabular}
\caption{Identification rates on MegaFace with 1 million distractors (Accuracy \%) and Verification rates at $10^{-3}$ FAR for the CFP Frontal vs. Profile protocol.  }
%}
\label{tab_megaface}

\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}




\hfill
%\parbox{.5\linewidth}{
\centering
\begin{tabular}{l c c c} % centered columns (4 columns)

\hline\hline %inserts double horizontal lines


Method  & $10^{-5}$   & $10^{-4}$  &  $10^{-3}$  \\
\hline % inserts single horizontal line



% Wang \emph{et. al.} \cite{WangOJ15} & 51.4 & 73.3 \\ % inserting body of the table
%DCNN \cite{chen2016unconstrained}  & -  &   57.3 \\
%DCNN$_{ft}$ \cite{chen2016unconstrained}&  -  &  64.0  \\
%DCNN$_{ft + m}$ \cite{chen2016unconstrained} &  -  &   78.7 \\
%DCNN$_{ft + m + c}$ \cite{chen2016unconstrained} &  -  &   81.8 \\
%DCNN$_{fusion}$ \cite{chen2016unconstrained}  &  -  &   83.8 \\
%PAM \cite{masi2016pose} &   65.2  &  82.6  \\
%Chen \emph{et. al.} \cite{chen2015end}  &  95.0 &  84.4 \\
$l2$-Cons SM* (101) \cite{ranjan2017l2} &-     &    87.9  &  93.7  \\ 
$l2$-Cons SM* (101x) \cite{ranjan2017l2} & -& 88.3     & 93.8  \\ 




%DeepID3 \cite{sun2015deepid3}  & WebFace & Yes &  96.8  \\
\hline 
\hline

% old numbers
%SM (FR)  &   55.01   &  74.20 \\ 
%$l2$-Cons SM (FR) \cite{ranjan2017l2} &   59.95   &  78.94  \\ 
%Center loss (FR) \cite{wen2016discriminative} &  59.59  &   85.32  \\ 
%SM + R  (FR) ($\lambda=0.01$) &  \textbf{66.24}   &   82.05  \\ 

% new numbers
%SM (FR)  &     &  55.01 \\ 
%$l2$-Cons SM (FR) \cite{ranjan2017l2} &    &   59.95   \\ 
%Center loss (FR) \cite{wen2016discriminative} &   &   59.59   \\ 
%SM + R  (FR) ($\lambda=0.01$) &     &   \textbf{66.24}  \\ 



%\hline
%SF (FR)  \cite{liu2017sphereface} &    & 72.84  \\ 
%SF + R (FR) ($\lambda=0.001$)  &    & 76.90 \\ 



%SF (FR)  \cite{liu2017sphereface} &   72.84  &  87.64 \\ 
%SF + R (FR) ($\lambda=0.001$)  &  76.90  &  88.68 \\ 


%\hline
SM    &    60.52   &  69.69     & 83.10   \\ 

$l2$-Cons SM (30)  \cite{ranjan2017l2}&  73.29   & 80.65   &   90.72   \\ 
$l2$-Cons SM (20)  \cite{ranjan2017l2}&   67.63  &   76.88    & 89.89     \\ 
$l2$-Cons SM (10)  \cite{ranjan2017l2}&  53.74   & 68.58    &  83.42    \\ 
SM + CL \cite{wen2016discriminative} &  46.01  & 74.10  &   88.32  \\ 
SF   \cite{liu2017sphereface}& 78.52    &  88.0   &  \textbf{93.24}  \\
SF + CL \cite{wen2016discriminative, liu2017sphereface}  & 72.35   & 81.11  &  89.26 \\
\hline

SM + R ($0.01$) & 72.53  &  79.1  & 90.8     \\ 
SM + R ($0.001$) &   78.41  &  85.0  &  91.5      \\ 
SM + R  ($0.0001$)  &  69.23  &  82.30   &  89.20    \\ 

\hline
%SF + R (Res64) ($\lambda=0.03$)  &   91.40 &    96.02 \\ 
SF + R  ($0.03$) & 79.54   &    85.37  &  91.64    \\ 
SF + R  ($0.01$) &   \textbf{82.41}   & \textbf{ 88.5} & \textbf{93.22}     \\ 

SF + R  ($0.001$) & 79.74    &     87.71 &  92.62    \\
SF + R  ($0.0001$) &    80.13 &     86.34    & 92.57    \\
%SF + R (Res64) ($\lambda=0.001$) & 92.62  & 96.1   \\ 



%SF (Res64)  \cite{liu2017sphereface} &  93.24    &  96.23  \\
%SF + R (Res64) ($\lambda=0.03$)  &   91.40 &    96.02 \\ 
%SF + R (Res64) ($\lambda=0.01$)  &  93.22  &  96.25     \\ 
%SF + R (Res64) ($\lambda=0.001$) & 92.62  & 96.1   \\ 


%\hline

%CMU Biometrics Center Matcher  &  \textbf{87.65} &  \textbf{95.63}  \\

%SphereFace + Ring loss ($\lambda=0.001$)  &  76.78  &  88.10 \\ 
%SphereFace + Ring loss ($\lambda=0.005$)  &    67.74  &  84.43 \\ 
%SphereFace + Ring loss ($\lambda=0.01$) &  72.10  &  85.30  \\ 

\hline
%inserts single line
\end{tabular}
\caption{Verification \% on the IJB-A Janus 1:1 verification protocol.  $l2$-Cons SM* indicates the result reported in \cite{ranjan2017l2} which uses a 101 layer ResNet/ResNext architecture.}
%}
\label{tab_janus}
%\vspace{-0.5cm}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
\hfill
%\parbox{.5\linewidth}{
\centering
\begin{tabular}{l c c c c} % centered columns (4 columns)

\hline\hline %inserts double horizontal lines


Method  & $10^{-6}$  &  $10^{-5}$    &  $10^{-4} $   &  $10^{-3}$    \\
\hline % inserts single horizontal line

Bodla \emph{et. al.} Final1 \cite{bodla2017deep}   &  -  & -   &  69.81  & 82.89    \\
Bodla \emph{et. al.} Final2 \cite{bodla2017deep}   &  -   &  -   &  68.45  & 82.97   \\

Lin \emph{et. al.}  \cite{lin2017proximity}    &  -  &  -  & 72.52   &  83.55 \\

\hline
\hline
SM  &  6.16   & 42.03    & 64.52    &  80.86   \\ 
$l2$-Cons SM (30)  \cite{ranjan2017l2}&  24.47  & 52.32   & 73.36    &  87.46  \\ 
$l2$-Cons SM (20)  \cite{ranjan2017l2}&  21.14  & 48.82  &   68.84  &  85.34    \\ 
$l2$-Cons SM (10)  \cite{ranjan2017l2}& 13.28  & 36.08  & 57.80  & 78.36   \\ 
SM + CL \cite{wen2016discriminative} &  2.88  & 20.87   &  65.71    &  84.55   \\ 
SF  \cite{liu2017sphereface} &  28.51   &  63.92 &   82.29  &  90.58  \\
SF + CL \cite{wen2016discriminative, liu2017sphereface}  &  28.99  &    53.36    &  72.91  &    86.14 \\

\hline

SM + R ($0.01$)   & 25.17  & 52.60  & 73.56  &  87.50  \\
SM + R  ($0.001$)  &  26.62 &  54.13    & 74.56    &  87.93     \\ 
SM + R  ($0.0001$)  & 17.35 &  50.65   &  71.06  &  85.48   \\ 
\hline
%SF + R (Res64) ($\lambda=0.03$)  &   91.40 &    96.02 \\ 
%SF + R (Res64) ($\lambda=0.03$)  &    &   &   &      \\ 
SF + R  ($0.03$)  &   27.27  &  56.84    &  76.97   &  88.75   \\ 
SF + R  ($0.01$)  & \textbf{ 35.18 } &  \textbf{65.02}  &\textbf{ 82.74 } &   \textbf{90.99 }    \\ 
SF + R  ($0.001$)  & 32.19    &  63.13     &    81.62   &  90.17   \\
SF + R  ($0.0001$)  &   32.01  & 63.12    & 81.57   &  90.24  \\ 
%SF + R (Res64) ($\lambda=0.001$)  &    &   &   &      \\ 
%SF + R (Res64) ($\lambda=0.001$) & 92.62  & 96.1   \\ 

%\hline

%CMU Biometrics Center Matcher  &  \textbf{87.65} &  \textbf{95.63}  \\

%SphereFace + Ring loss ($\lambda=0.001$)  &  76.78  &  88.10 \\ 
%SphereFace + Ring loss ($\lambda=0.005$)  &    67.74  &  84.43 \\ 
%SphereFace + Ring loss ($\lambda=0.01$) &  72.10  &  85.30  \\ 

\hline
%inserts single line
\end{tabular}
\caption{Verification \% on the Janus CS3 1:1 verification protocol. }
%}
%\vspace{-0.5cm}
\label{tab_cs3}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\textbf{Exp 5. Testing Benchmark: CFP Frontal vs. Profile. } Recently the CFP (Celebrities Frontal-Profile) dataset was released to evaluate algorithms exclusively on frontal versus profile matches \cite{sengupta2016frontal}. This small dataset has about 7,000 pairs of matches defined with 3,500 same pairs and 3,500 not-same pairs for about 500 different subjects. For sample images please refer to Fig. 1 of \cite{sengupta2016frontal}. The dataset presents a challenge since each of the probe images is almost entirely profile thereby presenting extreme pose along with illumination and expression challenges. 

%We showcase results on this dataset to demonstrate purely off-angle matching capabilities of Ring loss augmented loss functions.

\textbf{Result: CFP Frontal vs. Profile.} Fig.~\ref{fig_cfp} showcases the ROC curves for this experiment whereas Table.~\ref{tab_megaface} shows the verification rates at $10^{-3}$ FAR.  Ring loss (\textbf{87.43\%}) provides consistent and significant boost in performance over Softmax (55.86\%). We find however, SphereFace required more careful tuning of $\lambda$ with $\lambda=0.01$ (\textbf{90.94\%}) outperforming the baseline. Further, Softmax and Ring loss with $\lambda=0.01$ significantly outperforms all runs for $l_2$-constrained Softmax \cite{ranjan2017l2} (83.69). Thus, Ring loss helps in providing higher verification rates while dealing with frontal to highly off-angle matches thereby explicitly demonstrating robustness to pose variation.





\textbf{Exp 6. Low Resolution Experiments on Janus CS3. } One of the main motivations for $l_2$-constrained Softmax was to handle images with varying resolution. Low resolution images were found to result in low norm features and vice versa. Ranjan \emph{et.al.}  \cite{ranjan2017l2} argued normalization (through $l_2$-constrained Softmax) would help deal with this issue. In order to test the efficacy of our alternate convex normalization formulation towards handling low resolution faces, we synthetically downsample Janus CS3 from an original size of ($112\times 96$) by a factor of 4x, 16x, 25x, 36x and 64x respectively (images were downsampled and resized back up using bicubic interpolation in order to fit the model). We run the Janus CS3 protocol and plot the ROC curves in Fig.~\ref{fig_scale2}. We find that the Ring loss helps Softmax features be more robust to resolution. Though $l_2$-constrained Softmax provides improvement over Softmax, it's performance is lower than Ring loss. Further, at extremely high downsampling of 64x, $l_2$-constrained Softmax in fact performs worse than Softmax, whereas Ring loss provides a clear improvement. Center loss fails early on at 16x. We therefore find that our simple convex soft normalization approach is more effective at arresting performance drop due to resolution in accordance with the motivation for as normalization presented in \cite{ranjan2017l2}.




\textbf{Conclusion.} We motivate feature normalization in a principled manner and develop an elegant, simple and straight forward to implement convex approach towards that goal. We find that Ring loss consistently provides significant improvements over a large range of the hyperparameter $\lambda$. Further, it helps the network itself to learn normalization thereby being robust to a large range of degradations.










{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}






\subsection{Error analysis.}
In order to analyze the error, we trained and tested some LeNet models for MNIST dataset (Table \ref{tab_mnist}). All models are trained for 80 epochs with same settings except from the loss function. 

\begin{table}
\centering
\begin{tabular}{l  c}
\hline\hline
Method   & Accuracy \%  \\
\hline
Softmax    &     98.96 \\
Softmax + CenterLoss ($0.1$)\cite{wen2016discriminative}   &  99.35    \\ 
Softmax + RingLoss  ($0.1$)    &  99.45 \\
$l2$-Constrained Softmax  \cite{ranjan2017l2}   &  99.54  \\ 
SphereFace   \cite{liu2017sphereface}   &   99.60  \\
SphereFace + RingLoss ($0.1$)  &  99.65  \\
\hline
\end{tabular}
\caption{Classification accuracy on MNIST testing set. Brackets indicate $\lambda$ values for the secondary loss. All models utilize the LeNet architecture. }
\label{tab_mnist}
\end{table}

\textbf{Error rate for different angular threshold.} During testing, we are more interested in decision boundaries in terms of angle. So let's define the angle of features deviated from $\mathbf{w}_{y_{i}}$: 
\begin{align*}
\theta_i=\arctan{\left(\frac{\psi_i}{\xi_i}\right)},
\end{align*}
where $\xi_i$ is the projection length (projected feature norm) and $\psi_i$ is the rejection length (distance from the feature to the projected feature):
\begin{align*}
\xi_i&=\left\|\mathcal{F}(\mathbf{x}_i)\frac{\mathbf{w}_{y_{i}}}{\|\mathbf{w}_{y_{i}}\|}\right\|\\
\psi_i&=\left\|\mathcal{F}(\mathbf{x}_i)-\mathcal{F}(\mathbf{x}_i)\frac{\mathbf{w}_{y_{i}}}{\|\mathbf{w}_{y_{i}}\|}\right\|.
\end{align*}

Suppose a classifier assigns samples that satisfy $\theta_i<\theta_{th}$ as positive, where $\theta_{th}$ is an angular threshold. As $\theta_{th}$ changes, the error rate (err) changes accordingly:
\begin{align*}
err(\theta_{th})=\frac{\#\left\{i:\theta_i\geq\theta_{th}\right\}}{\#\left\{i\right\}}
\end{align*}

\begin{figure}%{r}{0.5\textwidth}
    \begin{center}
        \includegraphics[width=1\columnwidth,height=0.7\columnwidth,valign=m]{err_theta.pdf}\label{fig_err_theta}
    \end{center}
    \vspace{-0.5cm}
\caption{Error-$\theta_{th}$ plot on MNIST testing set for the methods listed in Table \ref{tab_mnist}. Having most samples gathering tighter around $\mathbf{w}_{y_{i}}$ not necessarily leads to good classification accuracy. Hard samples are also crucial. }
\label{fig_train_test}
\end{figure}

\textbf{Relation between feature norm and angular error.} As we argued in the motivation, having norm constraint in feature space stabilizes the loss as well as reduces noise in low-norm features. To further illustrate this, an error measurement is established. Since we care about the feature angle fluctuation with respect to feature norm during test time, 
Further we define the variance of rejection length $\sigma_\psi(\xi)$ as a function of projection length: 
\begin{align*}
\sigma_\psi^2(\xi)=Var\left[\left\{\psi_j:\forall{j},\xi_j\in\left[\xi-\frac{h}{2},\xi+\frac{h}{2}\right]\right\}\right],
\end{align*}
where $h$ is the width of a sliding window. Thus the effect of fluctuation of the feature on the angle can be measured with a dummy angle:
\begin{align*}
\tilde{\theta}(\xi)=\arctan{\left[\sigma_\psi(\xi)\right]}.
\end{align*}
Plotting the dummy angle $\tilde{\theta}$ with respect to projection length $\xi$, we get the effect of fluctuation on the angle as the feature norm increases (See Fig. \ref{fig_error_visual}). 


\begin{figure*}[t]
\centering
\begin{tabular}{ccc}
\subfigure[Softmax]{\includegraphics[scale=.3]{softmax.pdf}} & 
\subfigure[l2-constraint Softmax]{\includegraphics[scale=.3]{l2-softmax.pdf}} &
\subfigure[RingLoss+Softmax]{\includegraphics[scale=.3]{r01.pdf}} \\

\end{tabular}

\caption{\textbf{MNIST Error analysis with feature dimension of 5:} plot of the dummy angle $\tilde{\theta}$ with respect to the projected feature norm $\xi$. Size of the circle denotes the number of samples for current sliding window, i.e. density of samples at norm $\xi$. Clearly RingLoss eliminates the low norm features, thus decreases angle fluctuation during testing. Best viewed in color. }
\label{fig_error_visual}
\end{figure*}