Humans are exceptional visual learners capable of generalizing their learned knowledge to novel domains and concepts and capable of learning from few examples. %Consider a child who is capable of recognizing an elephant during the first trip to the zoo based solely on watching a cartoon. Or perhaps can quickly learn to recognize zebras after already learning about horses. 
%These types of generalizations and displays of reusing knowledge to enable efficient learning of subsequent concepts are routinely performed by humans. 
In recent years, computational models based on end-to-end learnable convolutional networks have made significant improvements for visual recognition~\cite{he2016deep,krizhevsky2012alexnet,simonyan2014vgg} and have been shown to demonstrate some cross-task generalizations~\cite{donahue2014decaf,RazavianASC14} while enabling faster learning of subsequent tasks as most frequently evidenced through fine-tuning~\cite{girshick2014rcnn,long2015learning,ren2015faster}.

%\jh{Fei-Fei: what are your thoughts about starting in this way? Do you think it's good grounding or would be better to focus on machine learning / computer vision results and technical problems.}

However, most efforts focus on the supervised learning scenario where a closed world assumption is made at training time about both the domain of interest and the tasks to be learned. Thus, any generalization ability of these models is only an observed byproduct. There has been a large push in the research community to address generalizing and adapting deep models across different domains~\cite{tzeng2017adversarial,ganin2016domain, sun2016deep, long2016unsupervised}, to learn tasks in a data efficient way through few shot learning~\cite{koch2015siamese,vinyals2016matching,ravi2017optimization,finn2017model}, and to generically transfer information across tasks~\cite{aytar2011tabula,girshick2014rcnn,ren2015faster,long2015fully}. 

While most approaches consider each  scenarios in isolation we aim to directly tackle the joint problem of adapting to a novel domain which has new tasks and few annotations. Given a large labeled source dataset with annotations for a task set, A, we seek to transfer knowledge to a sparsely labeled target domain with a possibly wholly new task set, B. 
This setting is in line with our intuition that we should be able to learn reusable and general purpose representations which enable faster learning of future tasks requiring less human intervention. In addition, this setting matches closely to the most common practical approach for training deep models which is to use a large labeled source dataset (often ImageNet~\cite{imagenet_cvpr09,russakovsky2015imagenet}) to train an initial representation and then to continue supervised learning with a new set of data and often with new concepts.  

In our approach, we jointly adapt a source representation for use in a distinct target domain using a new multilayer unsupervised domain adversarial formulation while introducing a novel cross-domain and within domain class similarity objective. This new objective can be applied even when the target domain has non-overlapping classes to the source domain. 

We evaluate our approach in the challenging setting of joint transfer across domains and tasks and demonstrate our ability to successfully transfer, reducing the need for annotated data for the target domain and tasks. 
We present results transferring from a subset of Google Street View House Numbers (SVHN)~\cite{netzer2011reading} containing only digits 0-4 to  a subset of MNIST~\cite{lecun1998gradient} containing only digits 5-9.
% !!!
% We show that through transfer from street view we are able to recover full MNIST performance using only \jh{fill in - XX\%} of the data. 
Secondly, we present results on the challenging setting of adapting from ImageNet~\cite{imagenet_cvpr09} object-centric images to UCF-101~\cite{soomro2012ucf101} videos for action recognition. 
% !!!
%\jh{add some comment here based on our experiments.}

% !!!
%\jh{Should we consider data efficient or annotation/label efficient?}\\
% \jh{TODO: add a small figure which shows example data used for train/test to make the setup clear.}
