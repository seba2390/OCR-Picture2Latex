% We now evaluate our model on (the proposed problem), we also perform additional experiments for ablation study.
This section is structured as follows. In section 4.1, we show that our method outperform fine-tuning approach by a large margin, and all parts of our method are necessary. In section 4.2, we show that our method can be generalized to bigger datasets. In section 4.3, we show that our multi-layer domain adversarial method outperforms state-of-the-art domain adversarial approaches.

\textbf{Datasets} We perform adaptation experiments across two different paired data settings. First for adaptation across different digit domains we use MNIST~\cite{lecun1998gradient} and Google Street View House Numbers (SVHN)~\cite{netzer2011reading}. The MNIST handwritten digits database has a training set of 60,000 examples, and a test set of 10,000 examples. The digits have been size-normalized and centered in fixed-size images.
SVHN is a real-world image dataset for machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It has 73,257 digits for training, 26,032 digits for testing.
As our second experimental setup, we consider adaptation from object centric images in ImageNet~\cite{russakovsky2015imagenet} to action recognition in video using the UCF-101~\cite{soomro2012ucf101} dataset.
ImageNet is a large benchmark for the object classification task. We use the task 1 split from ILSVRC2012.
UCF-101 is an action recognition dataset collected on YouTube. With 13,320 videos from 101 action categories, UCF-101 provides a large diversity in terms of actions and with the presence of large variations in camera motion, object appearance and pose, object scale, viewpoint, cluttered background, illumination conditions, etc.

\textbf{Implementation details}
We pre-train the source domain embedding function with  cross-entropy loss. For domain adversarial loss, the discriminator takes the last three layer activations as input when the number of output classes are the same for source and target tasks, and takes the second last and third last layer activations when they are different. The similarity score is chosen as the dot product of the normalized support features and the unnormalized target feature. We use the temperature $\tau=2$ for source-target semantic transfer and $\tau=1$ for within target transfer as the label space is shared. We use $\alpha=0.1$ and $\beta=0.1$ in our objective function. The network is trained with Adam optimizer~\cite{kingma2014adam} and with learning rate $10^{-3}$. We conduct all the experiments with the PyTorch framework.

%\textbf{Omniglot~\cite{lake2011one}} Omniglot consists of 1623 characters from 50 different alphabets. Each of these was hand drawn by 20 different people. Following~\cite{santoro2016meta}, we augmented the data set with random rotations by multiples of 90 degrees and used 1200 characters for training, and the remaining character classes for evaluation.

%\textbf{mini-ImageNet~\cite{vinyals2016matching,ravi2017optimization}} The mini-ImageNet dataset, originally proposed by Vinyals
%et al.~\cite{vinyals2016matching}, is derived from the larger ILSVRC-12 dataset~\cite{russakovsky2015imagenet}. The splits used by Vinyals et al.~\cite{vinyals2016matching} consist of 60,000 color images of size 84$\times$84 divided into 100 classes with 600 examples each. We use the split proposed by \cite{ravi2017optimization}.

\subsection{SVHN 0-4 $\rightarrow$ MNIST 5-9}
\textbf{Experimental setting.} In this experiment, we define three datasets: (i) labeled data in source domain $\mathcal{D}_{1}$; (ii) few labeled data in target domain $\mathcal{D}_{2}$; (iii) unlabeled data in target domain $\mathcal{D}_{3}$. We take the training split of SVHN dataset as dataset $\mathcal{D}_{1}$. To fairly compare with traditional learning paradigm and episodic training, we subsample $k$ examples from each class to construct dataset $\mathcal{D}_{2}$ so that we can perform traditional training or episodic ($k-1$)-shot learning. We experiment with $k=2,3,4,5$, which corresponds to $10,15,20,25$ labeled examples, or $0.017\%,0.025\%,0.333\%,0.043\%$ of the total training data respectively. Since our approach involves using annotations from a small subset of the data, we randomly subsample $10$ different subsets $\{\mathcal{D}_{2}^{i}\}_{i=1}^{10}$ from the training split of MNIST dataset, and use the remaining data as $\{\mathcal{D}_{3}^{i}\}_{i=1}^{10}$ for each $k$. Note that source domain and target domain have non-overlapping classes: we only utilize digits $0$-$4$ in SVHN, and digits $5$-$9$ in MNIST. 
%\jh{mention subsampling, number of runs, we report average accuracy with error bars. (done)}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.7\linewidth]{svhn_mnist.png}
\caption{An illustration of our task. Our model effectively transfer the learned representation on SVHN digits 0-4  (left) to MNIST digits 5-9 (right).} 
\label{fig:svhn_mnist}
\end{figure*}

\textbf{Baselines and prior work.} We compare against six different methods: (i) \textit{Target only}: the model is trained on $\mathcal{D}_{2}$ from scratch; (ii) \textit{Fine-tune}: the model is pretrained on $\mathcal{D}_{1}$ and fine-tuned on $\mathcal{D}_{2}$; (iii) \textit{Matching networks}~\cite{vinyals2016matching}: we first pretrain the model on $\mathcal{D}_{3}$, then use $\mathcal{D}_{2}$ as the support set in the matching networks; (iv) \textit{Fine-tuned matching networks}: same as baseline iii, except that for each $k$ the model is fine-tuned on $\mathcal{D}_{2}$ with 5-way ($k-1$)-shot learning: $k-1$ examples in each class are randomly selected as the support set, and the last example in each class is used as the query set; (v) \textit{Fine-tune + adversarial}: in addition to baseline ii, the model is also trained on $\mathcal{D}_{1}$ and $\mathcal{D}_{3}$ with a domain adversarial loss; (vi.) \textit{Full model}: fine-tune the model with the proposed multi-layer domain adversarial loss.

\textbf{Results and analysis.} We calculate the mean and standard error of the accuracies across $10$ sets of data, which is shown in Table~\ref{tbl:mnist}. Due to domain shift, matching networks perform poorly without fine-tuning, and fine-tuning is only marginally better than training from scratch. Our method with multi-layer adversarial only improves the overall performance, but is more sensitive to the subsampled data. Our method achieves significant performance gain, especially when the number of labeled examples is small ($k=2$). For reference, fine-tuning on full target dataset gives an accuracy of $99.65\%$. 

\begin{table}[htbp]
\centering
\caption{The test accuracies of the baseline models and our method. Row 1 to row 6 correspond (in the same order) to the six methods proposed in section 4.2. Note that the accuracies of two matching net methods are calculated based on nearest neighbors in the support set. We report the mean and the standard error of each method across $10$ different subsampled data.}
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Method} & \textbf{k=2} & \textbf{k=3} & \textbf{k=4} & \textbf{k=5}\\
\hline
\hline
\hline
Target only & 0.642 $\pm$ 0.026 & 0.771 $\pm$ 0.015 & 0.801 $\pm$ 0.010 & 0.840 $\pm$ 0.013\\
\hline
Fine-tune & 0.612 $\pm$ 0.020 & 0.779 $\pm$ 0.018 & 0.802 $\pm$ 0.016 & 0.830 $\pm$ 0.011\\
\hline
\hline
Matching nets~\cite{vinyals2016matching} & 0.469 $\pm$ 0.019 & 0.455 $\pm$ 0.014 & 0.566 $\pm$ 0.013 & 0.513 $\pm$ 0.023\\
\hline
Fine-tuned matching nets & 0.645 $\pm$ 0.019 & 0.755 $\pm$ 0.024 & 0.793 $\pm$ 0.013 & 0.827 $\pm$ 0.011\\
\hline
\hline
Ours: fine-tune + adv. & 0.702 $\pm$ 0.020 & 0.800 $\pm$ 0.013 & 0.804 $\pm$ 0.014 & 0.831 $\pm$ 0.013\\
\hline
Ours: full model ($\gamma=0.1$) & \textbf{0.917 $\pm$ 0.007} & \textbf{0.936 $\pm$ 0.006} & \textbf{0.942 $\pm$ 0.006} & \textbf{0.950 $\pm$ 0.004}\\
\hline
\end{tabular}
}

\label{tbl:mnist}
\end{table}

% \jh{TODO: questions which should be answered in this section:
% \begin{enumerate}
% \item Do we beat standard approaches?
% \item What happens to the learned representation after transfer/adapt? Which mnist digits get mapped closed to which svhn digits?
% \item We have made learning with few examples more robust, have we also made learning more efficient? Ie does it take fewer examples to reach comparable performance on this subset of mnist?
% \end{enumerate}}

\newcommand{\currW}{0.16\linewidth}
\begin{figure}[htbp]
\centering
\subfloat[]{
\minipage{\currW}
  \includegraphics[width=\linewidth]{source.png}
  \label{fig:source}
\endminipage
}
\subfloat[]{
\minipage{\currW}
  \includegraphics[width=\linewidth]{source-target.png}
  \label{fig:source}
\endminipage
}
\subfloat[]{
\minipage{\currW}
  \includegraphics[width=\linewidth]{finetune.png}
  \label{fig:source}
\endminipage
}
%\hfill
\subfloat[]{
\minipage{\currW}
  \includegraphics[width=\linewidth]{ours.png}
  \label{fig:source}
\endminipage
}
\subfloat[]{
\minipage{\currW}
  \includegraphics[width=\linewidth]{source_finetune.png}
  \label{fig:source}
\endminipage
}
\subfloat[]{
\minipage{\currW}
  \includegraphics[width=\linewidth]{source_ours.png}
  \label{fig:source}
\endminipage
}
\caption{The t-SNE~\cite{van2012visualizing, van2014accelerating} visualization of different feature embeddings. (a) Source domain embedding. (b) Target domain embedding using encoder trained with source domain domain. (c) Target domain embedding using encoder fine-tuned with target domain data. (d) Target domain embedding using encoder trained with our method. (e) An overlap of a and c. (f) An overlap of a and d. (best viewed in color and with zoom)}
\label{fig:source}
\end{figure}


%%% !!! Video experiment
\subsection{Image object recognition $\rightarrow$ video action recognition}
%\jh{again, add a small figure to show the image setup.}

\textbf{Problem analysis.} Many recent works~\cite{tang2012shifting,kalogeiton2016analysing} study the domain shift between images and video in the object detection settings. Compared to still images, videos provide several advantages: (i) motion provides information for foreground vs background segmentation~\cite{pathak2016learning}; (ii) videos often show multiple views and thus provide 3D information. On the other hand, video frames usually suffer from: (i) motion blur; (ii) compression artifacts; (iii) objects out-of-focus or out-of-frame.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.75\linewidth]{imagenet_ucf101.png}
% \caption{An illustration of our task.} 
\label{fig:imagenet_ucf101}
\end{figure*}

\textbf{Experimental setting.} In this experiment, we focus on three dataset splits: (i) ImageNet training set as the labeled data in source domain $\mathcal{D}_{1}$; (ii) $k$ video clips per class randomly sampled from UCF-101 training as the few labeled data in target domain set $\mathcal{D}_{2}$; (iii) the remaining videos in UCF-101 training set as the unlabeled data in target domain $\mathcal{D}_{3}$. We experiment with $k=3,5,10$, which corresponds $303,505,1010$ video clips, or $2.27\%,3.79\%,7.59\%$ of the total training data respectively. Each experiment is run $3$ times on $\mathcal{D}_{1}$, $\{\mathcal{D}_{2}^{i}\}_{i=1}^{3}$, and $\{\mathcal{D}_{3}^{i}\}_{i=1}^{3}$.

\textbf{Baselines and prior work.} We compare our method with two baseline methods: (i) \textit{Target only}: the model is trained on $\mathcal{D}_{2}$ from scratch; (ii) \textit{Fine-tune}: the model is first pre-trained on $\mathcal{D}_{1}$, then fine-tuned on $\mathcal{D}_{2}$. For reference, we report the performance of a fully supervised method~\cite{simonyan2014two}.

\textbf{Results and analysis.} The accuracy of each model is shown in Table~\ref{ucf101}. We also fine-tune a model with all the labeled data for comparison. Per-frame performance (img) and average-across-frame performance (vid) are both reported. Note that we calculate the average-across-frame performance by averaging the \textit{softmax} score of each frame in a video. Our method achieves significant improvement on average-across-frame performance over standard fine-tuning for each value of $k$. Note that compared to fine-tuning, our method has a bigger gap between per-frame and per-video accuracy. We believe that this is due to the semantic transfer: our entropy loss encourages a sharper softmax variance among per-frame softmax scores per video (if the variance is zero, then per-frame accuracy = per-video accuracy). By making more confident predictions among key frames, our method achieves a more significant gain with respective to per-video performance, even when there is little change in the per-frame prediction.

\begin{table}[htbp]
\centering
\caption{Accuracy of UCF-101 action classification. The results of the two-stream spatial model are taken from~\cite{simonyan2014two} and vary depending on hyperparameters. We report the mean and the standard error of each method across 3 different subsampled data.}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Method} & \textbf{k=3} & \textbf{k=5} & \textbf{k=10} & \textbf{All}\\
\hline
\hline
\hline
Target only (img) & 0.098$\pm$0.003 & 0.126$\pm$0.022 & 0.100$\pm$0.035 & -\\
Target only (vid) & 0.105$\pm$0.003 & 0.133$\pm$0.024 & 0.106$\pm$0.038 & -\\
\hline
Fine-tune (img) & 0.380$\pm$0.013 & 0.486$\pm$0.012 & 0.529$\pm$0.039 & 0.672\\
Fine-tune (vid) & 0.406$\pm$0.015 & 0.523$\pm$0.010 & 0.568$\pm$0.042 & 0.714\\
\hline
\hline
Two-stream spatial~\cite{simonyan2014two} & - & - & - & 0.708 - 0.720\\
\hline
\hline
Ours (img) & 0.393$\pm$0.006 & 0.459$\pm$0.013 & 0.523$\pm$0.002 & -\\
Ours (vid) & \textbf{0.467$\pm$0.007} & \textbf{0.545$\pm$0.014} & \textbf{0.620$\pm$0.005} & -\\
\hline
\end{tabular}

\label{ucf101}
\end{table}


\subsection{Ablation: unsupervised domain adaptation}

To validate our multi-layer domain adversarial loss objective, we conduct an ablation experiment for unsupervised domain adaptation. We compare against multiple recent domain adversarial unsupervised adaptation methods.
In this experiment, we first pretrain a source embedding CNN on the training split SVHN~\cite{netzer2011reading} and then adapt the target embedding for MNIST by performing adversarial domain adaptation. We evaluate the classification performance on the test split of MNIST~\cite{lecun1998gradient}. We follow the same training strategy and model architecture for the embedding network as \cite{tzeng2017adversarial}.

All the models here have a two-step training strategy and share the first stage. ADDA~\cite{tzeng2017adversarial} optimizes encoder and classifier simultaneously. We also propose a similar method, but optimize encoder only. Only we try a model with no classifier in the last layer (i.e. perform domain adversarial training in feature space). We choose $\gamma=0.1$ as the decay factor for this model.

%
The accuracy of each model is shown in Table~\ref{ablation}. We find that our method achieve $6.5\%$ performance gain over the best competing domain adversarial approach indicating that our multilayer objective indeed contributes to our overall performance. In addition, in our experiments, we found that the multilayer approach improved overall optimization stability, as evidenced in our small standard error.

\begin{table}[htbp]
\centering
\caption{Experimental results on unsupervised domain adaptation from SVHN to MNIST. Results of Gradient reversal, Domain confusion, and ADDA are from~\cite{tzeng2017adversarial}, and the results of other methods are from experiments across 5 different subsampled data.}
\begin{tabular}{|c|c|}
\hline
\textbf{Method} & \textbf{Accuracy}\\
\hline
\hline
\hline
Source only & 0.601 $\pm$ 0.011\\
\hline
Gradient reversal~\cite{ganin2016domain} & 0.739\\
\hline
Domain confusion~\cite{tzeng2015simultaneous} & 0.681 $\pm$ 0.003\\
\hline
 ADDA~\cite{tzeng2017adversarial} & 0.760 $\pm$ 0.018\\
\hline
\hline
Ours & \textbf{0.810 $\pm$ 0.003}\\
\hline
\end{tabular}

\label{ablation}
\end{table}

% \textbf{Visualization.} To evaluate the effectiveness of the adaptation, we use the Barnes-Hut t-SNE~\cite{van2014accelerating} algorithm to perform dimension reduction for model B and visualize the 2-D feature space. 

% \begin{figure}[htbp]
% \minipage{0.2\textwidth}
%   \includegraphics[width=\linewidth]{ablation/source.png}
%   \label{fig:source}
% \endminipage\hfill
% \minipage{0.2\textwidth}
%   \includegraphics[width=\linewidth]{ablation/target.png}
%   \label{fig:target}
% \endminipage\hfill
% \minipage{0.2\textwidth}%
%   \includegraphics[width=\linewidth]{ablation/target_da.png}
%   \label{fig:target_da}
% \endminipage
% \hfill
% \minipage{0.2\textwidth}%
%   \includegraphics[width=\linewidth]{ablation/source_target.png}
%   \label{fig:source_target}
% \endminipage\hfill
% \minipage{0.2\textwidth}%
%   \includegraphics[width=\linewidth]{ablation/source_target_da.png}
%   \label{fig:source_target_da}
% \endminipage
% \caption{t-SNE visualization of model B. From left to right: a) source domain distribution; b) target domain distribution; c) target domain distribution after domain adaptation; d) distribution of source domain and target domain; e) distribution of source domain and adapted target domain.}
% \end{figure}

%\subsection{Ablation: Few-shot learning}
%\textbf{Experimental setting.} In order to evaluate the effectiveness of episodic training, we perform experiments on Omniglot~\cite{lake2011one} and mini-ImageNet dataset~\cite{ravi2017optimization}. We only evaluate the 5-way, 5-shot learning in this experiment.

%\textbf{Baselines.}

%\textbf{Results.}
%The performance comparison is shown as follows:

%\begin{table}[htbp]
%\centering
%\begin{tabular}{|c|c|c|}
%\hline
%& Omniglot & mini-ImageNet  \\
%\hline
%Siamese networks~\cite{koch2015siamese} & 98.40\% & N/A\\
%\hline
%Matching networks~\cite{vinyals2016matching} & 98.90\% & 60.00\%/55.31\% \\
%\hline
%MANN~\cite{santoro2016meta} & 94.90\% & N/A 
%\\
%\hline
%Prototypical networks~\cite{snell2017prototypical} & 99.70\% & 68.20\%
%\\
%\hline
%\cite{ravi2017optimization} & N/A & 60.60\%
%\\
%\hline
%ours & - & -
%\\
%\hline
%\end{tabular}
%\end{table}

% \begin{table}[htbp]
% \centering
% \begin{tabular}{|c|c|c|c|}
% \hline
% & Siamese networks~\cite{koch2015siamese} & Matching networks~\cite{vinyals2016matching} & MANN~\cite{santoro2016meta}  \\
% \hline
% Omniglot & 98.40\% & 98.90\% & 94.9\% \\
% \hline
% mini-ImageNet & N/A & 60.00\%/55.31\% & N/A \\
% \hline
% & Prototypical networks~\cite{snell2017prototypical} & \cite{ravi2017optimization} & Ours \\
% \hline
% Omniglot & 99.70\% & N/A & - \\
% \hline
% mini-ImageNet  & 68.20\% & 60.60\% & -\\
% \hline
% \end{tabular}
% \end{table}