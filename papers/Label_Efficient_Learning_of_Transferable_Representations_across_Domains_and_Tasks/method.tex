\newcommand{\domain}{\mathcal{D}}
\newcommand{\src}{\mathcal{S}}
\newcommand{\tgt}{\mathcal{T}}
\newcommand{\img}{\mathbf{x}}
\newcommand{\lbl}{\mathbf{y}}
\newcommand{\rep}{\mathbf{\phi}}
\newcommand{\lossT}{\mathcal{L}}
\newcommand{\lossSup}{\lossT_{\text{sup}}}
\newcommand{\lossDom}{\lossT_{\textit{DT}}}
\newcommand{\lossST}{\lossT_{\textit{ST}}}

\newcommand{\imgSrc}{\img^{s}}
\newcommand{\labelSrc}{\lbl^{s}}
\newcommand{\numSrc}{n^{s}}
\newcommand{\imgTgt}{\img^{t}}
\newcommand{\repSrc}{\rep^{s}}
\newcommand{\repTgt}{\rep^{t}}
\newcommand{\imgTgtU}{\tilde{\img}^{t}}
\newcommand{\labelTgt}{\lbl^{t}}
\newcommand{\numTgt}{n^{t}}
\newcommand{\numTgtU}{n^{t}}
\newcommand{\numTgtL}{m^{t}}



\newcommand{\imgset}{\mathcal{X}}
\newcommand{\imgsetSrc}{\imgset^{\src}}
\newcommand{\imgsetTgt}{\imgset^{\tgt}}
\newcommand{\imgsetTgtU}{\tilde{\imgsetTgt}}
\newcommand{\imgsetTgtL}{\imgsetTgt}


\newcommand{\labelset}{\mathcal{Y}}
\newcommand{\labelsetSrc}{\labelset^{\src}}
\newcommand{\labelsetTgt}{\labelset^{\tgt}}


We introduce a semi-supervised learning algorithm which transfers information from a large labeled source domain, $\src$, to a sparsely labeled target domain, $\tgt$. The goal being to learn a strong target classifier without requiring the large annotation overhead required for standard supervised learning approaches. 

In fact, this setting is very commonly explored for convolutional network (convnet) based recognition methods. When learning with convnets the usual learning procedure is to use a very large labeled dataset (e.g. ImageNet~\cite{imagenet_cvpr09,russakovsky2015imagenet}) for initial training of the network parameters (termed pre-training). The learned weights are then used as initialization for continued learning on new data and for new tasks, called fine-tuning.
Fine-tuning has been broadly applied to reduce the number of labeled examples needed for learning new tasks, such as recognizing new object categories after ImageNet pre-training~\cite{simonyan2014vgg,he2016deep}, or learning new label structures such as detection after classficiation pre-training~\cite{girshick2014rcnn,ren2015faster}. Here we focus on transfer in the case of a shared label structure (e.g. classification of different category sets).

We assume the source domain contains $\numSrc$ images, $\imgSrc \in \imgsetSrc$, with associated labels, $\labelSrc \in  \labelsetSrc$. Similarly, the target domain consists of $\numTgtU$ unlabeled images, $\imgTgtU \in \imgsetTgtU$, as well as $\numTgtL$ images, $\imgTgt \in \imgsetTgtL$, with associated labels, $\labelTgt \in \labelsetTgt$. We assume that the target domain is only sparsely labeled so that the number of image-label pairs is much smaller than the number of unlabeled images, $\numTgtL \ll \numTgtU$.  Additionally, the number of source labeled images is assumed to be much larger than the number of target labeled images, $\numTgtL \ll \numSrc$.

Unlike standard domain adaptation approaches which transfer knowledge from source to target domains assuming a marginal or conditional distribution shift under a shared label space ($\labelsetSrc = \labelsetTgt$), we tackle joint image or feature space adaptation as well as transfer across semantic spaces. Namely, we consider the case where the source and target label spaces are not equal, $\labelsetSrc \neq \labelsetTgt$, and even the most challenging case where the sets are non-overlapping, $\labelsetSrc \cap \labelsetTgt = \emptyset$.

\subsection{Joint domain and semantic transfer}
Our approach consists of unsupervised feature alignment between source and target as well as semantic transfer to the unlabeled target data from either the labeled target or the labeled source data. We introduce a new multi-layer domain discriminator which can be used for domain alignment following the recent domain adversarial learning approaches~\cite{ganin2016domain,tzeng2017adversarial}. We next introduce a new semantic transfer learning objective which uses cross category similarity and can be tuned to account for varying size of label set overlap. 

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{model_arch.pdf}
\caption{Our proposed learning framework for joint transfer across domains and semantic transfer across source and target and across target labeled to unlabeled data. We introduce a  domain discriminator which aligns source and target representations across multiple layers of the network through domain adversarial learning.  We enable semantic transfer through minimizing the entropy of the pairwise similarity between unlabeled and labeled target images and use the temperature of the softmax over the similarity vector to allow for non-overlapping label spaces.
}
\label{fig:model}
\end{figure*}

We depict our overall model in Figure~\ref{fig:model}. We take the $\numSrc$ source labeled examples, $\{\imgSrc, \labelSrc\}$, the $\numTgtL$ target labeled examples, $\{\imgTgt, \labelTgt\}$, and the $\numTgtU$ unlabeled target images, $\{\imgTgtU\}$ as input. We learn an initial layered source representation and classification network (depicted in blue in Figure~\ref{fig:model}) using standard supervised techniques. We then initialize the target model (depicted in green in Figure~\ref{fig:model}) with the source parameters and begin our adaptive transfer learning. 

Our model jointly optimizes over a target supervised loss, $\lossSup$, a domain transfer objective, $\lossDom$, and finally a semantic transfer objective, $\lossST$. Thus, our total objective can be written as follows:
\begin{eqnarray}
	\lossT(\imgsetSrc, \labelsetSrc, \imgsetTgtL, \labelsetTgt, \imgsetTgtU) =
    		\lossSup(\imgsetTgtL, \labelsetTgt) + \alpha \lossDom(\imgsetSrc, \imgsetTgtU)
            + \beta \lossST(\imgsetSrc, \imgsetTgtL, \imgsetTgtU)
\end{eqnarray}
where the hyperparameters $\alpha$ and $\beta$ determine the influence of the domain transfer loss and the semantic transfer loss, respectively. In the following sections we elaborate on our domain and semantic transfer objectives.

\subsection{Multi-layer domain adversarial loss}
We define a novel domain alignment objective function called \textit{multi-layer domain adversarial loss}.
Recent efforts in deep domain adaptation have shown strong performance using feature space domain adversarial objectives~\cite{ganin2016domain,tzeng2017adversarial}. These methods learn a target representation such that the target distribution viewed under this model is aligned with the source distribution viewed under the source representation. This alignment is accomplished through an adversarial minimization across domain, analogous to the prevalent generative adversarial approaches~\cite{goodfellow2014generative}. In particular, a domain discriminator, $D(\cdot)$, is trained to classify whether a particular data point arises from the source or the target domain. Simultaneously,
the target embedding function $E^{t}(\imgTgt)$ (defined as the application of layers of the network is trained to generate the target representation that cannot be distinguished from the source domain representation by the domain discriminator. Similar to ~\cite{Tzeng_ICCV2015,tzeng2017adversarial}, we consider a representation to be domain invariant if the domain discriminator can not distinguish examples from the two domains.  

Prior work considers alignment for a single layer of the embedding at a time and as such learns a domain discriminator which takes the output from the corresponding source and target layers as input. Separately, domain alignment methods which focus on first and second order statistics have shown improved performance through applying domain alignment independently at multiple layers of the network~\cite{long2015learning}. Rather than learning independent discriminators for each layer of the network we propose a simultaneous alignment of multiple layers through a multi-layer discriminator.

At each layer of our multi-layer domain discriminator, information is accumulated from both the output from the previous discriminator layer as well as the source and target activations from the corresponding layer in their respective embeddings. Thus, the output of each discriminator layer is defined as: 
\begin{equation}
\mathbf{d}_l = D_{l}(\sigma(\gamma \mathbf{d}_{l-1}\oplus E_{l}(\img)))
\end{equation}
where $l$ is the current layer, $\sigma(\cdot)$ is the activation function, $\gamma \le 1$ is the decay factor, $\oplus$ represents concatenation or element-wise summation, and $\img$ is taken either from source data $\imgSrc \in \imgsetSrc$, or target data $\imgTgtU \in \imgsetTgtU $. Notice that the intermediate discriminator layers share the same structure with their corresponding encoding layers to match the dimensions.

Thus, the following loss functions are proposed to optimize the multi-layer domain discriminator and the embeddings, respectively, according to our domain transfer objective:

\begin{align}
\lossDom^{D} &= -\mathbb{E}_{\imgSrc \sim \imgsetSrc}\left[\log \mathbf{d}_{l}^s \right]-\mathbb{E}_{\imgTgt\sim \imgsetTgt}\left[\log (1-\mathbf{d}_{l}^t)\right]
\\
\lossDom^{E^{t}} &= -\mathbb{E}_{\imgSrc\sim\imgsetSrc}\left[\log( 1- \mathbf{d}_{l}^s)\right]-\mathbb{E}_{\imgTgt \sim\imgsetTgt}\left[\log \mathbf{d}_{l}^t\right]
\end{align}
where $\mathbf{d}_{l}^s, \mathbf{d}_{l}^t$ are the outputs of the last layer of the source and target multi-layer domain discriminator. Note that these losses are placed after the final domain discriminator layer and the last embedding layer but then produce gradients which back-propagate throughout all relevant lower layer parameters. These two losses together comprise $L_{DT}$, and there is no iterative optimization procedure involved.

This multi-layer discriminator (shown in Figure~\ref{fig:model} - yellow) allows for deeper alignment of the source and target representations which we find empirically results in improved target classification performance as well as more stable adversarial learning.


\subsection{Cross category similarity for semantic transfer}
In the previous section, we introduced a method for transferring an embedding from the source to the target domain. However, this only enforces alignment of the global domain statistics with no class specific transfer. Here, we define a new semantic transfer objective, $\lossST$, which transfers information from a labeled set of data to an unlabeled set of data by minimizing the entropy of the softmax with temperature of the similarity vector between an unlabeled point and all labeled points. Thus, this loss may be applied either between the source and unlabeled target data or between the labeled and unlabeled target data. 

For each unlabeled target image, $\imgTgtU$, we compute the similarity, $\psi(\cdot)$, to each labeled example or to each prototypical example~\cite{snell2017prototypical} per class in the labeled set. For simplicity of presentation let us consider semantic transfer from the source to the target domain first. For each target unlabeled image we compute a similarity vector where the $i^{th}$ element is the similarity between this target image and the $i^{th}$ labeled source image: $[v_s(\imgTgtU)]_i = \psi(\imgTgtU, \imgSrc_i)$. Our semantic transfer loss can be defined as follows:

\begin{eqnarray} \label{eq:5}
\lossST(\imgsetTgtU, \imgsetSrc) &=& \sum_{\imgTgtU\in\imgsetTgtU} H(\sigma(v_s(\imgTgtU)/ \tau))
\end{eqnarray}
where, $H(\cdot)$ is the information entropy function, $\sigma(\cdot)$ is the softmax function and $\tau$ is the temperature of the softmax. Note that the temperature can be used to directly control the percentage of source examples we expect the target example to be similar to (see Figure~\ref{fig:ent_compare}). 

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{entropy.png}
\caption{We illustrate the purpose of temperature ($\tau$) for our pairwise similarity vector. Consider an example target unlabeled point and its similarity to four labeled source points (\emph{x-axis}). We show here, original unnormalized scores (\emph{leftmost}) as well as the same similarity scores after applying softmax with different temperatures, $\tau$. Notice that entropy values, $H(x)$, have higher variance for scores normalized with a small temperature softmax.}
\label{fig:ent_compare}
\end{figure}

Entropy minimization has been widely used for unsupervised~\cite{aaai99} and semi-supervised~\cite{grandvalet2004semi} learning by encouraging low density separation between clusters or classes.  Recently this principle of entropy minimization has be applied for unsupervised adaptation~\cite{long2016unsupervised}. Here, the source and target domains are assumed to share a label space and each unlabeled target example is passed through the initial source classifier and the entropy of the softmax output scores is minimized.

In contrast, we do not assume a shared label space between the source and target domains and as such can not assume that each target image maps to a single source label. Instead, we compute pairwise similarities between target points and the source points (or per class averages of source points~\cite{snell2017prototypical}) across the features spaces aligned by our multi-layer domain adversarial transfer. We then tune the softmax temperature based on the expected similarity between the source and target labeled set. For example, if the source and target label set overlap, then a small temperature will encourage each target point to be very similar to one source class, whereas a larger temperature will allow for target points to be similar to multiple source classes. 



%\textbf{Generalize to new task with similarity loss.} Softmax-based approaches distinguish between available classes and have shown state-of-the-art performance on various classification tasks~\cite{he2016deep}. However, the top layers of the network are task-specific and are not suitable for novel tasks and classes. 
%This philosophy motivates the class of distance metric learning approaches~\cite{bromley1993signature}, which learn a transformation to a representation space where distance is in correspondence with a notion of similarity. Metric learning offers a number of benefits: (i) unlike softmax-based approach, its output dimensionality is flexible, which enables graceful scaling to new tasks; (ii) it is not a byproduct of the classification network and can be optimized jointly with other loss functions to encourage general distance metric across multiple domains and label spaces; (iii) recent advances~\cite{hoffer2015deep,rippel2015metric} have shown comparable or even better results than softmax-based approach, especially when only a few labeled examples per class are available.

%To exploit these advantages, we propose to train the target embedding by learning a similarity function that predicts the nearest neighbor of the query example $Q$ given the support set $S$. Accordingly, assume that the support set $S=\{(\mathbf{x}_{i}^{t},y_{i}^{t})\}_{i=1}^{n}\sim\mathbf{X}^{t}\times Y^{t}$ and the query example $Q=(\mathbf{x}^{t},y^{t})\sim\mathbf{X}^{t}\times Y^{t}$, where $n$ is the number of classes in each training step and all the labels $\{y_{i}^{t}\}_{i=1}^{n}$ are mutually exclusive.
%, the function predicts an index $j$ of the support set such that $y_{j}^{t}=y^{t}$. 
%The loss function is defined as follow:

%\begin{equation}
%\mathcal{L}_{sim}=\sum_{i}^{n}\mathbbm{1}\{y_j==y_i\}\mathbf{d}(E^t(\mathbf{x}_i^t), E^t(\mathbf{x}_j^t))
%\end{equation} where $\mathbbm{1}(\cdot)$ is the indicator function and $\mathbf{d}(\cdot)$ is a distance function such as euclidean or cosine. Given the match index prediction $\hat{j}$, the class prediction of the query example $\hat{y}$ can be retrieve as $\hat{y}^{t}=y_{\hat{j}}^{t}$. Inspired by term-weighting approaches~\cite{salton1988term} in information retrieval, we only normalize the embeddings of the support set. \jh{TODO: make the text more general. We present this type of loss then present how we use it.}


%\textbf{Unsupervised learning with entropy minimization.} When are unlabeled examples informative? Studies show that the (asymptotic) information content of unlabeled examples decreases as classes overlap~\cite{castelli1996relative,o1978normal}, which can be measured with conditional entropy $H(Y|X)$. Thus, the assumption that classes are well separated is sensible if we expect to take advantage of unlabeled examples.

%We exploit the entropy minimization principle~\cite{grandvalet2004semi} in this section, which encourages the low-density separation between classes by minimizing the entropy of class-conditional distribution~\cite{long2016deep}. 

% !!!
%\jh{2. explain temperature softmax over the similarity scores. include intuition as to why we expect minimizing entropy to be a viable approach for both overlapping (within target) and non-overlapping classes (source-target). In addition, we can use the temperature as a hyperparameters to tune based on the relatedness of the label spaces.(explain this as well)}

For semantic transfer within the target domain, we utilize the metric-based cross entropy loss between labeled target examples to stabilize and improve the learning. For a labeled target example, in addition to the traditional cross entropy loss, we also calculate a metric-based cross entropy loss \footnote{We refer this as "metric-based" to cue the reader that this is not a cross entropy within the label space.}. Assume we have $k$ labeled examples from each class in the target domain. We compute the embedding for each example and then the centroid $c_i^{\mathcal{T}}$ of each class in the embedding space. Thus, we can compute the similarity vector for each labeled example, where the $i^{th}$ element is the similarity between this labeled example and the centroid of each class: $[v_t(\imgTgt)]_i = \psi(\imgTgt, c_i^{\mathcal{T}})$. We can then calculate the metric based cross entropy loss:
\begin{equation} \label{eq:6}
\mathcal{L}_{ST, \text{sup}}(\imgsetTgt) = -\sum_{\{\imgTgt, \labelTgt\} \in \imgsetTgt} \log \frac{\exp\left([v_t(\imgTgt)]_{\labelTgt}\right)}{\sum_{i=1}^{n}\exp\left([v_t(\imgTgt)]_{i}\right)}
\end{equation}

Similar to the source-to-target scenario, for target-to-target we also have the unsupervised part,
\begin{eqnarray} \label{eq:7}
\mathcal{L}_{ST, \text{unsup}}(\imgsetTgtU, \imgsetTgt) &=& \sum_{\imgTgtU\in\imgsetTgtU} H(\sigma(v_t(\imgTgtU)/ \tau))
\end{eqnarray}

With the metric-based cross entropy loss, we introduce the constraint that the target domain data should be similar in the embedding space. 
% This constraint is similar to the non-parametric nearest neighbor method, which reduces the risk of overfitting with limited labeled data. 
Also, we find that this loss can provide a guidance for the unsupervised semantic transfer to learn in a more stable way.
$\mathcal{L}_{ST}$ is the combination of $\mathcal{L}_{ST, \text{unsupervised}}$ from source-target (Equation \ref{eq:5}), $\mathcal{L}_{ST, \text{supervised}}$ from source-target (Equation \ref{eq:6}), and $\mathcal{L}_{ST, \text{unsupervised}}$ from target-target (Equation \ref{eq:7}), i.e.,
\begin{eqnarray}
\mathcal{L}_{ST}(\imgsetSrc, \imgsetTgt, \imgsetTgtU)=\mathcal{L}_{ST}(\imgsetTgtU, \imgsetSrc)+\mathcal{L}_{ST, \text{sup}}(\imgsetTgt)+\mathcal{L}_{ST, \text{unsup}}(\imgsetTgtU, \imgsetTgt)
\end{eqnarray}

% Using episodic training strategy (see~\cite{vinyals2016matching} for further details), 


%Temperature $\tau$ is an important hyper-parameter. In order to make use of the unlabeled data in \textit{target} domain, we propose a temperature-parameterized entropy loss. Instead feeding a softmax layer, we extract the activation values and re-normalize them~\cite{jang2016categorical} as follows:

%\begin{equation}
%p_{\mathbf{x}_i^*} = \text{Softmax} \left( E^*(\textbf{x}_i^t) / \tau^* \right)
%\end{equation}
%where $* \in \{ s, t \}$. Notice that $\textbf{x}_i^t$ always comes from the unlabeled examples in \textit{target} domain.


%When $\tau \rightarrow 0$, $p_{\mathbf{x}_i}$ tends to be a uniform distribution; when $\tau \rightarrow \infty$, $p_{\mathbf{x}_i}$ degrades to the \textit{argmax} function. Accordingly, we should use a high temperature when class label aligns well (overlaps totally); when class labels have large non-overlapping, $\tau$ should be carefully selected. For example, when we consider the unsupervised learning setting with labeled data and unlabeled in \textit{target} domain, since we know each unlabeled example must lie in one of the category, we can set a high temperature to leverage the unlabeled data. For the mapping between \textit{source} and \textit{target} domain, unlabeled example may be close to multiple (but not all) classes in \textit{source} domain. We still want to minimize the entropy, but we can relax the constraint by selecting a high temperature for it.

% \begin{equation}
% p_i^{temp} = \frac{\exp^{\log{p_i} / \tau}}{\sum_{j=1}^n \exp^{\log{p_j} / \tau}}
% \end{equation}

%\begin{align}
%\mathcal{L}_{ent} 
%&= \mathcal{L}_{ent}^s + \mathcal{L}_{ent}^t \\
%&= \frac{1}{N} \sum_{i=1}^N \left[ H(p_{\mathbf{x}_i^s}) +  H(p_{\mathbf{x}_i^t}) \right]
%\end{align}
%where $N$ is the number of unlabeled examples in \textit{target} domain, $H()$ is the entropy function.


