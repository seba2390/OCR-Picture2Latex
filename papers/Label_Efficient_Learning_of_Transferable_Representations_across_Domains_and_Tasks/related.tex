\textbf{Domain adaptation.}
Domain adaptation seeks to learn from related source domains a well performing model on target data distribution~\cite{csurka2017domain}. Existing work often assumes that both domains are defined on the same task and labeled data in target domain is sparse or non-existent~\cite{tzeng2017adversarial}. 
Several methods have tackled the problem with the Maximum Mean Discrepancy (MMD) loss~\cite{gretton2009covariate,long2015learning,long2016deep,long2016unsupervised,zhang2015deep} between the source and target domain. Weight sharing of CNN parameters~\cite{sun2016deep,hoffman2016cross,hoffman2016learning,castrejon2016learning} and minimizing the distribution discrepancy of network activations~\cite{rozantsev2016beyond,tzeng2014deep,li2016revisiting} have also shown convincing results. Adversarial generative models~\cite{liu2016coupled,liu2017unsupervised,bousmalis2016unsupervised,taigman2016unsupervised} aim at generating source-like data with target data by training a generator and a discriminator simultaneously, while adversarial discriminative models~\cite{tzeng2015simultaneous,tzeng2017adversarial,ganin2016domain,ganin2014unsupervised,hoffman2016fcns} focus on aligning embedding feature representations of target domain to source domain. Inspired by adversarial discriminative models, we propose a method that aligns domain features with multi-layer information.

% In addition to distribution discrepancy in a shared feature space, the \textit{source} and \textit{target} domain might also differ in feature space or modality, which is the scope of heterogeneous domain adaptation\cite{csurka2017domain}. With the assumption that low-level layers are domain/modality specific, mid-level layers are semantic specific, and high-level layers are task specific, Gupta et al.~\cite{gupta2016cross} tackle the problem by sharing parameters of mid-level layers.

\textbf{Transfer learning.}
Transfer learning aims to transfer
knowledge by leveraging the existing labeled data of some related task or domain~\cite{pan2010survey,weiss2016survey}. In computer vision, examples of transfer learning include~\cite{aytar2011tabula,lim2011transfer,tommasi2010safety} which try to overcome the deficit of training samples for some categories by adapting classifiers trained for other categories~\cite{oquab2014learning}. With the power of deep supervised learning and the ImageNet dataset~\cite{imagenet_cvpr09, russakovsky2015imagenet}, learned knowledge can even transfer to a totally different task (i.e. image classification $\rightarrow$ object detection~\cite{ren2015faster,redmon2016you,liu2016ssd}; image classification $\rightarrow$ semantic segmentation~\cite{long2015fully}) and then achieve state-of-the-art performance. In this paper, we focus on the setting where source and target domains have differing label spaces but the label spaces share the same structure. Namely adapting between classifying different category sets but not transferring from classification to a localization plus classification task.

\textbf{Few-shot learning.}
Few-shot learning seeks to learn new concepts with only a few annotated examples. Deep siamese networks~\cite{koch2015siamese} are trained to rank similarity between examples. Matching networks~\cite{vinyals2016matching} learns a network that maps a small labeled support set and an unlabeled example to its label. Aside from these metric learning-based methods, meta-learning has also served as a essential part. Ravi et al.~\cite{ravi2017optimization} propose to learn a LSTM meta-learner to learn the update rule of a learner. Finn et al.~\cite{finn2017model} tries to find a good initialization point that can be easily fine-tune with new examples from new tasks. When there exists a domain shift, the results of prior few-shot learning methods are often degraded.

\textbf{Unsupervised learning.}
Many unsupervised learning algorithms have focused on
modeling raw data using reconstruction objectives~\cite{hinton2006reducing,vincent2008extracting,kingma2013auto}. Other probabilistic models include restricted Boltzmann machines~\cite{hinton1986learning}, deep Boltzmann machines~\cite{salakhutdinov2009deep}, GANs~\cite{goodfellow2014generative,dumoulin2016adversarially,donahue2016adversarial}, and autoregressive models~\cite{oord2016pixel,van2016conditional} are also popular. 
An alternative approach, often terms ``self-supervised learning''~\cite{de1994learning}, defines a pretext task such as predicting patch ordering~\cite{doersch2015unsupervised}, frame ordering~\cite{misra2016shuffle}, motion dynamics~\cite{luo2017unsupervised}, or colorization~\cite{zhang2016colorful}, as a form of indirect supervision. Compared to these approaches, our unsupervised learning method does not rely on exploiting the spatial or temporal structure of the data, and is therefore more generic.

%\textbf{Image/video classification.}
%Object classification is a classic problem in computer vision. In this paper, we explore the joint transfer and domain adaption problem, from the perspective of the visual object classification.

