\section{Design and Architecture}

%We have extended our work on Cloud Kotta to bridge the gap between the interactive development style of
%the data science community and the batch submission model that we have supported so far. 

In an effort to address the requirements described in the previous section, we extended Cloud Kotta to integrate the Jupyter development environment and marry its interactive
execution model with the Cloud Kotta computation fabric. Our approach allows users to select
and analyze secure datasets while leveraging the virtually 
infinite compute and storage capacity of the cloud all within a familiar and intuitive environment. 
The architecture of our solution is depicted in \figurename~\ref{fig:arch}.

\begin{figure}
  \center
  \includegraphics[width=0.45\textwidth]{figures/arch.png}
  \caption{Architecture.}
  \label{fig:arch}
  \vspace{-1.5em}
\end{figure}

\subsection{Cloud Kotta}
Cloud Kotta is a cloud-based service for conducting collaborative research around protected datasets.
It is designed to be deployed on Amazon Web Services (AWS), leveraging highly reliable
platform services and scalable data storage and compute environments. 
Cloud Kotta leverages a suite of AWS service including 
Simple Storage Service (S3) for long term data storage, 
DynamoDB for storing fine grain job histories, 
Simple Queue Service (SQS) for rapid and reliable task distribution,
and Elastic Compute Cloud (EC2) for an elastically scalable computing environment.

Cloud Kotta stores and manages protected data in S3. This provides a
low cost model for storing large amounts of data while also providing
a fine grain access control model for individual objects. Moreover, Kotta offers
secure HTTPS interfaces for accessing data. 
When data is to be analyzed, it is moved and cached on cloud instance storage
using AWS Elastic Block Storage.
Cloud Kotta employs advanced data management policies that move data within storage 
tiers based on frequency of access. This model allows us to trade off
high availability for lower cost similar to least-recently-used caching. 
Combining these methods allows Cloud Kotta to reduce storage costs, significantly.

To satisfy bursty and potentially large-scale compute workloads
Cloud Kotta employs multiple elastic pools of compute resources. 
This allows the system to match incoming workloads so as to
provide seemingly infinite compute capacity at a fraction of the 
cost of maintaining a large persistent cluster in the cloud. 
That is, instances are only initiated when needed so that users are only charged for the resources needed for compute.
Where possible, Cloud Kotta leverages low cost \emph{spot} instances
and adopts a smart bidding system across four availability zones to ensure
that the resources provisioned are the cheapest available machines. % and enforce caps on compute costs.

To service the two broad job categories on the system, Cloud Kotta maintains a test queue that is geared
for development jobs and a separate production queue for jobs that require significant CPU and or memory resources.
The test queue is comprised of an elastic pool of \emph{t2.medium} instances, each with 2 vCPUs and 4GB of RAM. These instances
are \emph{on-demand} instances which can be provisioned in under two minutes. 
To ensure rapid response time, the test 
queue always has at least one machine on-line. The production queue, on the other hand, is usually populated with c3.8xlarge
(32vCPU, 60GB RAM) or i2.8xlarge (36vCPU, 244GB RAM) machines that are provisioned from the spot market.
These machines are slower to provision and are designed for jobs that are generally tolerant of delays.

In order to utilize these resources, Cloud Kotta provides a queue based job submission model that is accessible
through a Web GUI, REST API and a command line interface. The Web GUI is convenient for submitting single tasks
and tracking their progress, but become a hassle when dealing with a large number of jobs. The REST interface
is designed to support programmatic access for example from external applications of via scripts that aim to manage several hundreds of jobs. This
flexibility comes with the additional effort of fitting science workflows to this model. The command line
interface is useful for submitting a large number of jobs with minor variations. This is suited for bash
script based tasks. %and less so for workflows written in python which form the bulk of our workloads.

\subsection{Security}

Security of the hosted data is the primary concern for a secure data enclave. Cloud Kotta is architected with
this in mind, and so implements a rich, fine grain access control model. 
At the heart of the model users are assigned roles. All datasets define policies
with respect to these roles, for example defining who can access each dataset.
Policies also control what permissions individual services have. 
Every request that involves data is signed and logged such that 
access can be audited in the future.

Rather than implement a user management system, Cloud Kotta leverages 
Amazon's identity services. Cloud Kotta's web interface 
relies on Login with Amazon, an OAuth~2.0 provider for user authentication. 
This workflow allows users to authenticate with their Amazon identity. 
Cloud Kotta is given a short-term token that can be used to verify identity
and conduct operations on that user's behalf. While the authentication
tokens themselves are valid for only an hour, we create cookies to extend 
session validity to 6 hours. This approach works well for web-based sessions; 
however it is not suitable in a programmatic context. To simplify programmatic
authentication Cloud Kotta leverages refresh tokens issued through a one-time registration
process. These tokens can be refreshed by the application for as long as they
are valid. Users may revoke tokens at any point in the future. 

To support analysis of private data on elastic cloud instances Cloud
Kotta leverages a unique delegated access model. Each instance is 
created with a special role, this role provides a minimal set of 
privileges by default (e.g., to read from the queue). However, the role
allows the instance to inherent the role of the user that owns a submitted
job. In this case, the instance changes to the user's role which 
allows it to access protected data throughout the execution of the 
analysis task.  

Cloud Kotta applies best practices cloud security models to secure
the entire system. For example, S3 buckets are encrypted and 
accessible only from within a predefined virtual private cloud (VPC). 
The compute layer is isolated from the internet by a private subnet. 


%One concern with refresh tokens is that they do not expire like access tokens as per the definition in OAuth
%2.0. In order to address this, we plan to track issued tokens and blacklist tokens older than a configurable
%threshold. This threshold would be set per user based on the security guarantees required by the protected
%datasets they have access to.

\subsection{Jupyter Deployment}

Jupyter is an application that allows users to author living documents
that contain code (e.g., Python, R, Julia), rich descriptive text (e.g., documentation), 
and the results of running code (e.g., text, figures). Notebooks are organized into cells,
users can execute these cells in any order and state is shared between cells. 
Jupyter notebooks are authored and executed in a web browser, using a locally
deployed web server. 
One of the reasons Jupyter has become so popular is that it provides 
a flexible and interactive model for analysis, it is intuitive to use
via a web browser, and notebooks are self-describing. It is this combination
that makes it easy to share, reproduce, and extend analyses.
However, Jupyter notebooks are limited in that they are single-user
instances, they are not designed to exploit large scale computing
infrastructure, and they do not provide support for accessing 
data securely. 

JupyterHub~\cite{jupyterhub} is a multi-user server that manages user authentication and 
can spawn multiple single-user Jupyter notebooks. We base our integration
on JupyterHub for this reason. To do so, we have deployed JupyterHub
on an EC2 instance within the Cloud Kotta system. We use JupyterHub's PAM
authentication system to authenticate users onto the login node with access
to a persistent filesystem. This authentication system is currently separate
from the system used by Cloud Kotta and therefore requires a second user
account.  We are actively working to integrate the two models. 
JupyterHub users are provided with their own (isolated) space that
can be used for creating and maintaining stateful development environments
(e.g., notebooks and temporary data). We use a notebook spawner
to create new notebook instances for users on demand. This spawner is designed
to restrict the CPU and RAM available to each notebook such that
individual users will not be able to accidentally render the service unusable.
As we will describe later, our library extensions allow users to offload analysis
tasks that require resources beyond what is allocated to an individual notebook instance. 

%we provide methods to offload compute or memory heavy functions to larger machines which are
%better equipped to handle such workloads. This enable the user to have faster development cycles
%with the low latencies during development as well as the vast computational capacity of the cloud
%for production workloads.

Since availability and reliability is critical for the Jupyter environment, it is important that the
host EC2 instance capacity matches expected workload (i.e., number of users). To address this
concern, the Cloud Kotta system administration team monitors load and can scale-up or scale-down the
instance type that is provisioned for the service.


\subsection{Kotta Interactive Library}

The primary interface used to connect Jupyter notebooks with
Cloud Kotta is the \texttt{kotta} python library. This library enables the user to
markup python functions using a decorator. Functions decorated with the \texttt{@kottajob} decorator, when called
are executed transparently on the Cloud Kotta infrastructure. The library handles the serialization of the
arguments passed, canning of the decorated function, and deserialization of the results once they
have been computed. The decorator takes an authenticated connection object, the target Cloud Kotta queue and a walltime as
required arguments. 
The \texttt{kotta} library authenticates with Cloud Kotta using valid access tokens.
These tokens are, at present, stored in a file within the user's private file system. 
This sequence of steps taken by the \texttt{kotta} library is shown in \figurename~\ref{fig:kotta_lib}

\begin{figure}
  \center
  \includegraphics[width=0.45\textwidth]{figures/kotta_lib.png}
  \caption{Kotta Library sequence diagram}
  \label{fig:kotta_lib}
  \vspace{-1.5em}
\end{figure}

Here is a simple example of a decorated Python function
which simply computes the sum of a list of integers:

\begin{lstlisting}
@kottajob(konn, 'Test', 5)
def my_sum(items):
   return sum(items)

result = my_sum(range(0,100))
print(result)
\end{lstlisting}

The above code snippet, when executed with the \texttt{kotta} library, 
looks to the user like any other Python script, albeit with the additional latencies from
serialization, transmission and execution on a remote compute node. The function runs in blocking mode,
therefore \texttt{my\_sum()} does not return until its results are available. The \texttt{@kottajob}
decorator can also run in non-blocking mode and that makes it simple to exploit many-task parallelism
within analysis workflows.

\begin{lstlisting}
@kottajob(konn, 'Test', 5, block=False)
def my_sqrt(items):
   import math
   return [math.sqrt(x) for x in items]

jobs = []
# Split the problem space into 5 chunks of size 20
for item in range(0,100,20):
   jobs.extend([my_sum(range(item,item+20))])

# Wait for results
print([job.result for job in jobs])
\end{lstlisting}

Since the function \texttt{my\_sqrt()} is decorated with the option \texttt{block=False}, calls to
the function return immediately with a \emph{future}. A future is a common term used to describe
an object that acts as a proxy for which a value is not yet known. 
The advantage of this approach is that users may execute several such functions, all
of which are executed in parallel.

Decoratated functions also take special keywords \texttt{inputs} and \texttt{outputs} which are
used to indicate to Cloud Kotta that special (protected) datasets must be staged-in or staged-out.
In this case, Cloud Kotta uses its secure data enclave to locate the requested dataset. Cloud Kotta
then invokes its standard security mechanisms to authorize access to the data if the user
has privileges to access the dataset. 
Since each task is handled by Cloud Kotta there's an auditable
record of every compute task that touched protected datasets.

\begin{lstlisting}
@kottajob(konn, 'Test', 10)
def file_sum(inputs=[]):
   import os
   print(os.listdir('.'))
   lines = open(os.path.basename(inputs[0]), 'r').readlines()
   data = [int(line.strip()) for line in lines]
   total = sum(data)
   length = len(data)
   return total, length

returns = file_sum(inputs=['s3://klab-jobs/1m_shuffled.txt'])
print(returns)
\end{lstlisting}

The \texttt{kotta} library by default captures both STDOUT and STDERR streams. These are accessible through
the future returned from a non-blocking call, or when a blocking call fails with an exception.
This simplifies debugging during the development process. It is also important to note that Cloud Kotta
logs the entirety of the task allowing both retrospective and prospective provenance. For tasks running
longer than 5 minutes Cloud Kotta also captures continuous CPU and Memory utilization information that can be viewed
through the Web GUI. The \texttt{@kottajob} decorator also takes a \texttt{requirements} keyword argument that
accepts a specially formatted string. This allows for installing and setting up requisite python packages
before the decorated function is executed on the compute nodes.

Finally, the library also supports specifying execution of arbitrary applications in a batch model. This allows the user to easily
plug in existing applications written in any language and to subsequently manage these applications via Python. 
This ensures that complex workflows with unusual
requirements such as ensuring that no data is fetched to the Jupyter notebook are possible.
Here is a minimal example:

\begin{lstlisting}
job = KottaJob(jobtype="script",
               jobname="Hello",
               executable='/bin/bash myscript.sh',
               script_name='myscript.sh',
               script="""/bin/bash
               echo 'Hello world'
               """)

# Submit a job with the Kotta connection object
job.submit(konn)

# Track job status
job.status(konn)

# Print stdout
print(job.stdout)
\end{lstlisting}









