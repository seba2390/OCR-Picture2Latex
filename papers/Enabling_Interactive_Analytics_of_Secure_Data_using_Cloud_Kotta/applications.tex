\section{Applications}

Adding interactivity to Cloud Kotta for rapid ideation and iteration was motivated by a gap between the functionality of the computational environment and the type of interrogative, discovery science adopted by researchers. This gap was particularly noticeable in work being carried out by Knowledge Lab researchers on the Clarivate Analytics (formerly the IP \& Science business of Thomson Reuters) {\it Web of Science} and the ITHAKA {\it JSTOR} datasets. This work, situated in the Computational Social Sciences and the broader Science of Science, seeks to understand the institutions and dynamics of knowledge production by modeling the scientific system from publication data~\cite{evans2011metaknowledge}. In what follows, we outline three concrete use cases that initially motivated and then benefited from our extension of Cloud Kotta.

Cloud Kotta has been used to analyze the full {\it Web of Science} raw data and to compute two measures of the scientific impact of individual journals and individual papers -- Eigenfactor and the Article Influence metrics, respectively~\cite{bergstrom2008eigenfactor}. The general analysis pipeline is as follows. The {\it Web of Science} data files are converted from their raw XML format to a structured collection of lists and dictionaries for each year. From this intermediate form, the data are processed and
aggregated in batches corresponding to moving window years (e.g., as specified by the Eigenfactor computation). 
Of course, any large computational infrastructure would have proved efficacious for this job. However, because of the sensitive nature of the data and accompanying access agreements, the use of the Cloud Kotta secure data enclave and computational infrastructure was necessitated. The ability to collocate propriatery data and compute within the same virtual private and secure cloud was also convenient, due to the size of the dataset (~100 Gb), the compute time (several hours per job), and the susceptibility of the task to parallelization (the initial dataset is split into year-labeled files). The collection of jobs was submitted en masse through the Cloud Kotta Client Python interface described in this paper and outputs were fetched in a similar manner. It is worth noting that, while the selection of machine types and OS images are hidden from the user, the system is
flexible enough that, upon request by the user, instances with, for example, more memory or an image with task specific
pre-installed libraries, can be made available --which is what occurred in this case. For future bibliometric work, users will be in a position to make minor modifications to the setup and codebase used in the Eigenfactor computation thanks to Cloud Kotta's job publication features.

In addition to parsing tens of millions of files and calculating metrics from extracted metadata, the interactive Jupyer analysis environment is ideal for generating and then iteratively interrogating networks (e.g. graphs) comprised of billions of links. Much of the work that goes into the initial analysis of networks is rather exploratory. However, when a network is as massive as, for instance, the citation network that underlies the scientific enterprise, rapid ideation is stymied by secure but rigid computational environments with queue based job submission protocols. 

In order to get a better sense of the citation network in the {\it Web of Science}, researchers needed to parse tens of millions of raw XML files, generate an edge list of all source papers and their respective citations targets, and create an adjacency list. The goal was to make it easier to directly parse, interrogate, and derive summary statistics about citation counts for any specific paper in the {\it Web of Science}. The derived, proprietary edge list citation network is 20 GB uncompressed, making it immediately a memory and disk-intensive problem for local machines or small, single nodes. Moreover, because the data is proprietary and subject to data use agreements, the network cannot be replicated and processed locally. Given that we are working with billions of citations and parallelizable algorithms for assembling the adjacency list, it makes economic and legal sense to utilize the Kotta decorator and farm the problem out to large nodes. Moreover, once the graph has been generated, basic summary statistics can be fetched directly from the Jupyter notebook by running simple network analysis functions with the Cloud Kotta decorator. In this way, even a network with billions of links can be explored in the straightforward and intuitive way that researchers have come to expect in an area of discovery science.

Finally, users have found the new, interactive analysis capabilities of Cloud Kotta to be useful in conducting research using natural language processing and text mining on big, scholarly, data. For instance, in one project, researchers seek to analyze the rhetorical framing of Adam Smith and his political philosophy from {\it The Wealth of Nations}. In order to do this, users first conducted text mining to identify terms and concepts related to Adam Smith as well as the contexts in which they appear within the body of social science literature from 1890s to the 2010s. Cloud Kotta allowed researchers to actively and iteratively filter through candidate terms and concepts and relate them to similar concepts and phrases found in Adam Smith's work. This problem required leveraging 20 GB of protected OCR'd (optical character recognition) data from JSTOR, and performing relatively expensive analyses (e.g., tf-idf, word2vec, lda, etc.) which makes the problem just out of reach for many personal, local machines, both in terms of memory and disk, as well as data use agreements.

Of course, it was possible to carry out many of the tasks required of these projects in more traditional, queue based compute environments. Several of the tasks, however, could not have been performed on more traditional data enclaves, as these have tended to be local, relatively small, and isolated machines. Nevertheless, enabling researchers to rapidly ideate and iterate on code and analysis has greatly improved the research process in each of the applications and has, as a result, sped discovery.