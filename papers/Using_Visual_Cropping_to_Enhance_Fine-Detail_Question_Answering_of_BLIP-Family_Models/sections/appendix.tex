\section{Results using segmentation and object detection models}

In addition to the auto-cropping methods, we also try object detection and image segmentation methodologies to extract the relevant area of interest corresponding to a given question. To achieve this, we utilized two state-of-the-art models: Segment Anything Model (SAM)~\citep{kirillov2023segment} and YOLO-v8\footnote{https://github.com/ultralytics/ultralytics}~\citep{yolo}, which specialize in segmentation-based and object detection with localization approaches. 

\begin{table}[h!]
\centering
% \scriptsize
\small
\caption{Complete results including YOLO and SAM model.}%  \filip{highlight best results}\jiarui{fixed}}
\begin{tabular}{l|c|rr|rr|rr}
\toprule

\multicolumn{2}{c|}{\textbf{Dataset} \textit{(question count)}} &  \multicolumn{2}{c}{\bf VQA-text (500)} & \multicolumn{2}{c}{\bf VQA-hard (109)} & \multicolumn{2}{c}{\bf VQA-random (1001)} \\ \midrule
\multicolumn{1}{l}{} & \textbf{cropping} & \texttt{acc} & \texttt{str-simi}  & \texttt{acc} & \texttt{str-simi}  & \texttt{acc} & \texttt{str-simi}  \\\midrule
 & w/o cropping & 27.34 & 60.47 & 44.36 & 68.14 & 83.53 & 94.56 \\
 & \hc & 30.68 & 65.29 & 55.14 & 73.21 & - & - \\
 & \grad & \bf 24.80 & \bf 59.58 & 48.90 & 67.32 & 67.61 & 83.87 \\
 & \sac & 22.80 & 55.96 & \bf 49.63 & \bf 70.95 & \bf 79.48 & \bf 92.00 \\
 & \rac & 19.86 & 53.04 & 47.34 & 69.65 & 71.06 & 86.31 \\
 & \texttt{yolo}&17.04&49.04&42.94&65.21&68.18&83.45 \\
\multirow{-7}{*}{Fine-tuned} & \texttt{sam}&15.58&48.29&48.72&70.04&65.24&81.83 \\\midrule
 & w/o cropping & 27.76 & 57.52 & 33.60 & 55.55 & 60.98 & 79.06 \\
 & \hc & 57.26 & 83.04 & 41.83 & 64.59 & - & - \\
 & \hc$^{+}$ & 58.52 & 82.81 & 41.93 & 61.69 & - & - \\
 & \grad & 37.12 & 66.28 & 40.64 & 63.74 & 59.83 & 79.08 \\
 & \grad$^{+}$ & \bf 43.04 & \bf 70.39 & \bf 41.47 & \bf 63.32 & \bf 65.57 & \bf 82.70 \\
 & \sac & 27.62 & 56.32 & 35.41 & 57.29 & 60.59 & 78.57 \\
 & \sac$^{+}$ & 32.54 & 59.64 & 34.86 & 55.17 & 61.94 & 79.29 \\
 & \rac & 25.80 & 54.61 & 36.15 & 57.26 & 60.71 & 79.14 \\
 & \rac$^{+}$ & 32.70 & 61.13 & 35.96 & 57.67 & 63.44 & 80.29 \\
 & \texttt{yolo}&19.46&49.10&34.22&56.88&58.10&77.05 \\
 & \texttt{yolo}$^{+}$&29.58&58.74&35.87&58.17&63.70&80.50 \\
 & \texttt{sam}&22.96&53.11&38.17&58.07&57.33&76.38 \\
\multirow{-12}{*}{Zero-shot} & \texttt{sam}$^{+}$&35.14&63.24&36.33&54.04&64.27&80.90 \\
% \midrule
% Num. of images & - & \multicolumn{2}{c}{109} & \multicolumn{2}{|c}{951}\\ 
\bottomrule
\end{tabular}

\label{tab:sam yolo}
\end{table}




\subsection{Method description}
We initially fed the input image to the SAM model, generating multiple segmentation masks representing different regions within the image. Subsequently, the smallest bounding box encompassing each segmentation mask was individually passed through the CLIP model to identify the mask that exhibited the highest similarity with the provided question. The segmentation mask exhibiting the highest similarity was then forwarded to the subsequent BLIP model for further visual QA pipeline.
Regarding the YOLO methodology, we followed a similar procedure as with SAM. After inputting the image into the YOLO model, it provided a set of bounding boxes representing detected objects within the image. Each bounding box was subsequently processed through the CLIP model to determine the bounding box that demonstrated the highest similarity with the given question. The bounding box associated with the object exhibiting the highest similarity was then forwarded to the subsequent BLIP model for the visual QA task.

\subsection{Results}
Table \ref{tab:sam yolo} shows the overall comparison of all the cropping methods used in our experimentation. The proposed \grad~model demonstrates a remarkable similarity to the human cropping process, thereby surpassing alternative cropping methodologies in terms of performance and effectiveness. Also, as the SAM and YOLO methods generally performed worse, we left them out of the main paper.

Looking at concrete predictions, the SAM model provides a more accurate representation of the area of interest compared to YOLO. This distinction arises from the methodology employed by SAM, which generates segmentation masks covering the entirety of the image. Consequently, we can individually pass each image segment to the CLIP model, enabling CLIP to perceive the entire image comprehensively.

In contrast, YOLO selects objects solely from a predefined list of 80 classes. As a result, there are instances where the detected object may not be directly relevant to the given question. To address this limitation, we employ a strategy to select the bounding boxes that exhibit the closest proximity to the question. However, due to YOLO's class-based approach, it is not an optimal choice for accurately identifying the area of interest. Figure \ref{fig:sam_yolo_examples} shows the bounding box created by SAM and YOLO. In analyzing the bounding boxes, it is evident that SAM presents a superior approach for visual QA due to its ability to localize image regions that hold greater relevance to the posed question. Conversely, being constrained to a limited set of objects, YOLO fails to capture crucial areas within the image that may contain the answer, thus leading to potential information loss.



\begin{figure}
    \centering
    
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{appendix_figures/yolo_sam_1.png}
        \caption{}
        \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{appendix_figures/yolo_sam_2.png}
        \caption{}
        \label{fig:sub2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{appendix_figures/yolo_sam_3.png}
        \caption{}
        \label{fig:sub3}
    \end{subfigure}
    
    \vspace{0.1cm}
    
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{appendix_figures/yolo_sam_4.png}
        \caption{}
        \label{fig:sub4}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{appendix_figures/yolo_sam_5.png}
        \caption{}
        \label{fig:sub5}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{appendix_figures/yolo_sam_6.png}
        \caption{}
        \label{fig:sub6}
    \end{subfigure}
    
    \vspace{0.1cm}
    
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{appendix_figures/yolo_sam_7.png}
        \caption{}
        \label{fig:sub7}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{appendix_figures/yolo_sam_8.png}
        \caption{}
        \label{fig:sub8}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{appendix_figures/yolo_sam_9.png}
        \caption{}
        \label{fig:sub9}
    \end{subfigure}
    
    \caption{Examples of bounding box creation using SAM and YOLO.}
    \label{fig:sam_yolo_examples}
\end{figure}


\begin{table}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\small
\caption{Intersection over Union (IoU in \%) results between the bounding boxes generated by different cropping methods and the human-cropping which we consider as ground-truth.}
\label{tab:iou_results}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{VQA-hard} & \textbf{VQA-text} \\ \midrule
\grad   &  \textbf{25.84}  &  \textbf{16.15}    \\ 
\sac   & 23.31   & 5.86       \\ 
\rac &  22.57  &   5.76     \\ 
\texttt{SAM}     &  21.18  &   6.26     \\ 
\texttt{YOLO}    &  20.42  &     3.54   \\ \bottomrule
\end{tabular}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\linewidth}
\centering
\small
\caption{Average inference time (in seconds/image) for different cropping methods.}
\label{tab:inference_time}
\begin{tabular}{lc}
\toprule
\textbf{Model} & \textbf{Inference time (seconds per image)}  \\ \midrule
\grad   &  0.713      \\ 
\sac   & 5.151      \\ 
\rac &  1.320     \\ 
\texttt{SAM}     &  6.858    \\ 
\texttt{YOLO}    &  \textbf{0.461} \\ \bottomrule
\end{tabular}
\end{minipage}
\end{table}

\section{Comparison of bounding box and inference time of each model}

\subsection{Bounding box comparison}

In the results summarized in \autoref{tab:iou_results}, the \grad~model outperforms the others significantly on both the VQA-hard and VQA-text tasks. This model achieved an IoU of 25.84\% and 16.15\%, respectively. In the context of the VQA-text dataset, our proposed \grad~cropping technique exhibited remarkable proficiency in localizing the specific region within the image where the relevant text was located. In contrast, alternative cropping methodologies consistently faltered, leading to a significantly diminished IoU score across multiple instances.


\subsection{Inference time comparison}
In \autoref{tab:inference_time}, we present a comparative analysis of the inference time associated with various cropping methods. The inference time is calculated as the average time, in seconds, required to process each input image. This calculation is based on an evaluation of 1001 images from the VQA-random dataset, utilizing a single NVIDIA RTX A5000 GPU.

Our observations indicate that the YOLO model, employed for bounding box generation, is the most time-efficient. However, it is essential to note that this efficiency is somewhat compromised by the model's limited class range. On the other hand, the \grad~method, with an inference time of 0.713 seconds, offers a comparable speed to the YOLO model while maintaining superior performance. This balance between speed and performance makes the \grad~a compelling method for bounding box generation.


\section{Ablation study and external analysis}
In this section, we study the effects of our cropping methods' critical components and hyper-parameters by ablation study and external analysis. The experiments in this section provide a better understanding of the performance and properties of different cropping methods.




\subsection{Ablation study of \texttt{grad} components}

% Importance of High-pass Filtering and Gradient Value Highlighting in Gradient-based Cropping}





In~\autoref{tab:ablation grad}, we apply the ablation study on two critical elements that form the backbone of gradient-based cropping, namely, high-pass filtering (\autoref{binary mask}) and gradient value highlighting (\autoref{formula3} and \autoref{formula4}). Our findings underscore that both these components play pivotal roles in the success of our gradient-based cropping approach.
High-pass filtering comes into play as a crucial function that emphasizes the details in an image, compared to color and brightness. This allows the resulting mask to remove high gradient regions that contain little to no details since it is often such details that determine the precise location a model must focus on. Furthermore, the process of gradient value highlighting serves to temper the extreme values (removing outliers), thereby facilitating a more balanced normalization. It further accentuates regions possessing strong gradients.


\begin{table}[h!]
\centering
\small
\caption{Ablation studies to measure the importance of gradient value highlighting and high-pass filtering for the \grad~method.}
\begin{tabular}{l|c|rr|rr|rr}
\toprule

\multicolumn{2}{c|}{\textbf{Dataset}} &  \multicolumn{2}{c}{\bf VQA-text} & \multicolumn{2}{c}{\bf VQA-hard } & \multicolumn{2}{c}{\bf VQA-random} \\ \midrule
\multicolumn{1}{l}{} & \textbf{metric} & \texttt{acc} & \texttt{str-simi}  & \texttt{acc} & \texttt{str-simi}  & \texttt{acc} & \texttt{str-simi}
\\\midrule
 & w/o cropping & 27.76 & 57.52 & 33.60 & 55.55 & 60.98 & 79.06 \\
 & \grad$^{+}$ & \textbf{41.47} & \textbf{63.32} & \textbf{43.04} & \textbf{70.39} & \textbf{65.57} & \textbf{82.70 }\\
 & w/o grad value highlighting& 35.32 & 54.64 & 39.98 & 67.27 & 65.28 & 81.96 \\
 \multirow{-4}{*}{Zero-shot} & w/o high-pass filtering & 34.40 & 54.46 & 34.34 & 61.42 & 64.22 & 81.48\\

\bottomrule
\end{tabular}

\label{tab:ablation grad}
\end{table}


However, the influence of both high-pass filtering and gradient value highlighting seems to lessen on the VQA-random dataset. The accuracy and structural similarity (str-simi) values remain relatively steady despite the omission of either factor.



\begin{table}[h]
\small
\centering
\caption{Experimental results of two clip-based models on changing the granularity of string-image similarity.}
\label{tab:external study clip}
\begin{tabular}{ccc|ccc}
\toprule
\multicolumn{3}{c}{\sac} & \multicolumn{3}{c}{\rac} \\\midrule
iter-10, ratio-0.2 & \multicolumn{1}{c}{iter-20, ratio-0.1} & iter-40, ratio-0.05 & large-patch & \multicolumn{1}{l}{medium-patch} & \multicolumn{1}{l}{small-patch} \\
\multicolumn{1}{c}{34.59} & 34.86 & 36.33 & \multicolumn{1}{c}{35.32} & 35.96 & 36.24 \\
\multicolumn{1}{c}{54.86} & 55.17 & 56.7 & \multicolumn{1}{c}{55.55} & 57.67 & 56.45\\\bottomrule
\end{tabular}
\end{table}


\subsection{Impact of the granularity of similarity computation}


To study whether more granular image-text similarity computation improves the cropping performance, we adjust the hyperparameters of \rac~and \sac. Specifically, we modify the patch size and window size of the \sac~model, while ensuring that each window contains the same number of pixels. We introduce three variations for the patch size: 8, 16, and 32, with corresponding window sizes of 12, 6, and 3, respectively. For the \rac~model, we alter the number of iteration times to 10, 20, and 40, and adjust the cropping ratios to 0.2, 0.1, and 0.05, respectively. The adjustments are designed to maintain approximately the same final cropping size.

As presented in Table~\ref{tab:external study clip}, our observations suggest that increasing the granularity of the two clip-based cropping methods results in a modest performance enhancement. The most substantial increase we observe is 1.74\%. This finding indicates that the model's granularity does not play a decisive role in its performance.





\section{More examples}

In Figure \ref{fig: gradient}, we provide more examples of different models' successes and failures in predicting the correct answer for the given question. We also provide the bounding boxes created by each of the three models (\grad,~\sac,~ and \rac).

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\textwidth]{appendix_figures/success_failure_appendix.pdf}
    \caption{Success ($\checkmark$) and Failure ($\times$) of the used cropping techniques in predicting the correct answer to the question.}
    \label{fig: gradient}
\end{figure}
