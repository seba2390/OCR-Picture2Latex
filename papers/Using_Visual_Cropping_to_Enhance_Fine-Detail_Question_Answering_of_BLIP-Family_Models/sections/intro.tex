\section{Introduction}

% Introductory/background paragraph
% \filip{we are mixing VQA as a task and as a benchmark}
Visual Question Answering, or visual QA, is a challenging task that requires effective integration between perceptual, linguistic, and background knowledge~\citep{schwenk2022okvqa}. Achieving robust performance on visual QA tasks is essential to support downstream tasks requiring textual and visual understanding, including semantic scene search~\citep{vo2019reading}, summarization~\citep{rafiq2020scene}, and captioning~\citep{stefanini2022show}. This can benefit applications, such as intelligent traffic monitoring~\citep{xu2021sutd}, and medical diagnosis~\citep{ren2020cgmvqa}.
Given its relevance and challenging nature, visual QA has recently been addressed in open-ended zero- and few-shot settings. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/crop_fig.pdf}
    \caption{Effect of cropping on visual QA (left: zero-shot and right: fine-tuned) tasks using human cropping (\textit{x-axis} represents cropping size, left-end represents the original image and right-end represents the cropped area of interest). Note that lower loss indicates higher answer confidence.}% in the answer.}
    \label{fig:crop_fig}
\end{figure}

%% Problem statement paragraph
State-of-the-art models for open-ended visual QA, like BLIP2~\citep{li2023blip}, typically combine Large Language Models (LLMs) and foundational image embeddings~\citep{brown2020language}. Curiously, while object detection and visual localization have been prevalent tasks \citep{yolo, pascal}, models like BLIP2 work in an end-to-end fashion and do not explicitly include localization components, which raises the question of the mechanism by which they associate the question elements with objects in the image. For instance, given a question like \textit{Are there any street signs in the picture?} (\autoref{fig:crop_fig}), it is unclear whether BLIP-family models focus consistently on the area where the potential street sign is located and whether that focus is clearly leveraged in their QA procedure. Our initial analysis of BLIP2 predictions revealed a recurring theme of mistakes on questions about fine details in images, such as reading text (e.g., letters on the side of an airplane), recognizing whether an object with a specific attribute (e.g., a white dog) exists, or counting objects of a particular type in an image (e.g., slices of pizza). While BLIP-family models have an attention mechanism that, in theory, should localize the important image segment that can help answer the question, we hypothesize that the question-answering module is not consistently attending to the salient part of the image.




% \mahyar{we could also consider closed-ended visual question-answering methods if we had the time, basically anything from Table 4 of BLIP2.}

%% Approach

Inspired by these insights, we investigate the following question: \textit{Can visual cropping be employed to improve the performance of the BLIP-family visual QA models on fine-detail questions?} 
We illustrate our intuition behind visual cropping with two examples in \autoref{fig:crop_fig}.
While the original BLIP2 method is unable to detect traffic signs in Figure \ref{fig:crop_fig} (left) and identify the bird type in Figure \ref{fig:crop_fig} (right), precise cropping of the image gradually improves its ability to answer both of these questions correctly, switching to \textit{yes} for the first question and to \textit{egret} for the second. 
% Moreover, the confidence of the model (measured as an inverse loss of correct answer) gradually increases following our intuition.
To provide a systematic insight into our research question, we define three meaningful subsets from the popular VQA-v2 benchmark~\citep{goyal2017making}: \texttt{\textbf{1)}} \textit{VQA-text} contains reading questions, i.e., questions whose answers can be read out from a particular part of the image; \texttt{\textbf{2)}}  \textit{VQA-hard} is selected from errors made both by the BLIP2 zero-shot model and VQA-v2 fine-tuned BLIP model; and \texttt{\textbf{3)}}  \textit{VQA-random} is a random subset that enables us to study the impact of cropping on natural data distribution.
Besides human cropping (\hc), we experiment with novel automatic cropping methods based on the BLIP model gradient (\grad) and on multi-modal embedding by CLIP~\citep{clip} (\sac~and \rac). %cseveral cropping variants: manual/human cropping (\hc), recursive automatic cropping (\rac), sliding-window automatic cropping (\sac), segmentation-based cropping (\seg), and gradient-based cropping (\grad). 
Our experiments demonstrate that the performance of BLIP model variants can be significantly improved through human cropping and that automatic cropping methods can often produce similar benefits. We observe that zero-shot models display more significant improvements, but fine-tuned BLIP models also exhibit enhanced performance. These results indicate that fine-tuning can improve the overall in-domain performance but it does not fully address the issue of insufficient attention to image details. Our findings also indicate that cropping enhancement is robust as it yields an improvement over the baseline on the general VQA-random task.

In summary, our paper makes the following contributions: \texttt{1)} We address a fundamental weakness of visual QA models being unable to reason about non-salient visual elements, by \textbf{measuring the effect of human cropping} on zero-shot and fine-tuned models. \texttt{2)} We devise \textbf{two novel automatic cropping strategies} that leverage multi-modal encoders and vision QA model gradients to approximate human cropping. \texttt{3)} We investigate the value of manual and automatic cropping strategies \textbf{by controlled experiments with zero-shot and fine-tuned models on three new subsets of VQA-v2}. \texttt{4)} We perform \textbf{further analysis} to understand the cropping impact on data partitions, and \textbf{case studies} to connect quantitative differences with qualitative observations across question types and datasets.

% what impacts cropping performance, as well as gain insight into when and why cropping helps model performance.

% \filip{we need to introduce BLIP2 somewhere. This could happen in the intro (1 full BLIP par) or in section 3 as a new 3.1}


% \filip{i tried to integrate this par above, but ended up rewriting the story mostly. Feel free to update it, merge, or move}
% We propose the accuracy on the fine-detail VQA as a measure of attention to details for any VQA model. Next, we show that BLIP2 performs much lower on this dataset compared to the total VQA dataset. Then, we show that finetuning BLIP2, which is proposed as a way to improve the performance of BLIP2 on the specific task of visual question answer, similarly has a large performance gap between fine-detail VQA and the total VQA. This means that the finetuning while helping  the overall performance, cannot address the issue of lack of attention to fine details in the image. Next, we investigate the cause of this problem, and we show that the issue seems to be related to an information overload, that is, a simple cropping can significantly improve the performance of both zero-shot and finetuned BLIP2 models \mahyar{The "information overload" is very handwavy, we can make it more mathematically sound later}. Finally, we propose a method that directly utilizes cropping to improve the performance of both models without requiring any manual control.

% \filip{this par also seems outdated} 
% We summarize our contributions as follows: (1) We identify the limitations of current captioning models in capturing detailed information from complex images, then we 

% (2) We highlight the constraints of object detection models in detecting out-of-domain objects due to their limited class coverage. (3) We propose a novel pipeline that integrates automatic interactive image captioning and dynamic cropping, which significantly improves both captioning and object detection performance in various scenarios.