\section{Experiments}

% \subsection{Experimental Setup}
% \filip{Let's add here a few subsections: evaluation metrics, parameter values, computing environment details}

% \subsection{Evaluation and Metrics}

\textbf{Experimental Setup}~
% Our evaluation answers three main experimental questions: \texttt{1)} Is cropping effective for fine-detail VQA? \texttt{2)} What impacts cropping performance? \texttt{3)} Why does cropping (not) help?
Our analysis is carried out on the above-mentioned three datasets with zero-shot \textit{BLIP-2-FlanT5XL} and the fine-tuned \textit{BLIP-vqav2}.  We employ our three automatic cropping methods. For VQA-text and VQA-hard, we also utilize existing human croppings. %For each cropping method, we evaluate performance of fine-tuned VQA model and zero-shot VQA model on the cropped image. 
In the case of the zero-shot BLIP2 model, images are transformed into the same embedding space as the question text, facilitating easy concatenation prior to input into the language model. We exploit this feature by concatenating embeddings of both the cropped image and the original image to serve as a joint visual input for the language model. 


% \textbf{Image embedding concatenation}  This approach helps preserve comprehensive image information. 

\textbf{Metrics}~
We employ \textit{accuracy (\texttt{acc})} following VQA-v2,\footnote{ \scriptsize https://visualqa.org/evaluation.html} which is a robust metric considering inter-human variability in phrasing. After applying text processing steps to normalize the model answer, the accuracy is defined as:
$\texttt{acc(ans)} = \min(0.3\times n, 1)$, where $n$ denotes the times that the answer ($ans$) appears among the answers from 10 human annotations. In addition to accuracy, we introduce a metric based on \textit{string similarity (\texttt{str-simi})}, which is calculated using the longest common sub-sequence technique,\footnote{ \scriptsize We also tried other string similarity metrics, but they are highly correlated to each other, so we only show results for one of them.} which is able to distinguish similar answers from dissimilar ones. The \texttt{str-simi} metric is particularly relevant in the context of VQA-text, given its inherent nature as a string-recognition task.

\textbf{Implementation Details}~
For \texttt{grad}, we use a $k_{discard}$ of $1$, a kernel size of 5, and $\sigma$ as $0.3 * ((K_{size} - 1) * 0.5 - 1) + 0.8$, which is $1.1$ by default. 
For patch pooling, we use a patch size of $16\times16$ which is the size of ViT tokens. During pooling, we use a top pooling with a $n_{pool}$ of $5$. For \texttt{clip-w}, we use a window size of $6\times6$, a patch size of $16\times16$, a stride of $1$, and a final threshold $\theta$ of 0.5. For \texttt{clip-r}, we use a cropping ratio $r$ of 0.9, and we perform 20 recursive crops for each image.
We use \textit{python 3.8.16, salesforce-lavis 1.0.2, transformers 4.29.1 and torch 2.0.1} for all the BLIP model experiments. Our environment consists of an Intel(R) Xeon(R) Gold 5215 CPU @ 2.50GHz with 40 cores and 256 GB of RAM. Additionally, we utilize NVIDIA RTX A5000 GPUs for our experiments. 