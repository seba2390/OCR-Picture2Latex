\section{Related Work}
% \subsection{Image captioning and Question Answering}

% Vit, Swin.

% In the context of zero-shot vision question-answering, the Blip-2 model~\cite{li2023blip} enhances performance by fine-tuning a compact model that bridges the gap between the vision feature extraction model and large language models. Additionally, \citet{yang2022zero} employs captioning models to generate informative prompts for large language models, which are then used to execute prompt-based zero-shot visual question answering.

% \subsection{Large Language models}
% % GPT-3, chatgpt
% % \filip{Perhaps write it about LLM prompting or LM prompting?}

% haha we don;t need this anymore
% Prompting large language models has recently become critical, given that the quality of the output of LLMs strongly depends on the formulation of their inputs. The Self-talk method~\citep{shwartz2020unsupervised} shows that language models can answer questions better, by intermediate reasoning steps that are resolved by a separate model. In a multiple-choice task, the prompted model can be asked to evaluate each of the candidate answers~\citep{pinto} or reason about the goals of the speaker~\citep{sourati2023case}. The model can also \textit{ask itself}, by decomposing a more complex question into a chain of more manageable questions that can be answered more directly~\citep{chothr}. Such prompting where large LMs are asked to augment the input or produce intermediate reasoning steps is often seen as a knowledge distillation approach~\citep{west2021symbolic}. \filip{tell how we compare once we know}



% \subsection{Large scale Multi-modal pre-training}

% Pali~\cite{chen2022pali}, mPLUG~\cite{li2022mplug}


% \subsection{Vision Language pre-training (VLP)}


\textbf{Vision Language Pre-training (VLP)} models have recently shown promising results across vision-language generation and understanding tasks such as image captioning, visual question answering, and visual grounding \citep{hossain2019comprehensive}. \cite{huang2023language} proposed a multimodal language model that generates text in an auto-regressive manner, perceiving modalities, following instructions, learning in context, and producing outputs. \cite{nguyen2022grit} proposed a Transformer-based architecture for image captioning that integrates region and grid features using Swin Transformer \citep{liu2021swin} and DETR-based detector \citep{carion2020end}.
% providing richer context information and significant improvement in accuracy and speed.
\cite{zhou2020unified} developed a VLP model with a shared transformer network for encoding and decoding. 
% For image captioning, \cite{duan2022position} introduced the Position-Aware Transformer (PAT) model, which extracts both regional and static global visual features and integrates spatial information aligned with each visual feature to unify both regional and global aspects. 
% \cite{li2021scheduled} proposed a two-stream decoupled design of encoder-decoder structure for simultaneous vision-language understanding and generation pretraining, using a primary scheduled sampling strategy to mitigate the discrepancy in VLP. 
% \cite{khan2022single} proposed a single-stream architecture for aligning images and language at multiple levels using symmetric cross-modality reconstruction and pseudo-labeled keyword prediction tasks, improving fine-grained alignment and semantic grounding.
Our approach is orthogonal to these works by focusing on the problem of fine-detail questions in VQA-v2 and investigating the efficacy of visual cropping in improving the performance of state-of-the-art visual QA models. While we test our approach with BLIP-family models, it could be extended to VLP models in the future.
% \subsection{Vision Questions Answering (VQA)}

\textbf{Visual QA}
% Visual QA 
is an emerging research area at the intersection of computer vision and natural language processing, aimed at developing AI models that can answer questions about visual content. \cite{salaberria2023image} proposed a text-only approach for visio-linguistic tasks based on automatic captioning and pre-trained language models, demonstrating improved performance in knowledge-intensive tasks such as OK-VQA and outperforming comparable multimodal models. \cite{alberti2019fusion} introduced Bounding Boxes in Text Transformer (B2T2) architecture to improve visual QA. \cite{garderes2020conceptbert} devised a concept-aware algorithm to answer visual QA questions requiring common sense or basic factual knowledge from external structured content. 
%Given an image and a question in natural language, ConceptBert requires visual elements of the image and a Knowledge Graph to infer the correct answer. 
% \cite{selvaraju2019taking} proposed the Human Importance-aware Network Tuning (HINT) approach to improve visual grounding in vision and language models using human attention demonstrations, outperforming top approaches that penalize over-reliance on language priors (VQA-CP (\cite{agrawaldon}) and robust captioning). 
% \cite{wijmans2019embodied} extended the visual QA task to a 3D environment in which they examined the agent's ability to answer the question by perceiving its environment through point clouds.
Visual QA models achieve remarkable results but rely on abundant labeled training data. Zero-shot visual QA has gained attention, focusing on answering questions about unseen objects or scenes without task-specific fine-tuning. \cite{pfeiffer2022xgqa} presented the first benchmark for cross-lingual visual QA, with seven target languages. 
% They proposed adapter-based approaches for multilingual multimodal models and evaluated them in zero-shot and few-shot learning settings.
\citet{yang2022zero} introduced zero-shot VideoQA using frozen bidirectional language models, achieving state-of-the-art results on benchmark datasets. 
% The approach unifies a frozen pre-trained bidirectional language model with a robust visual encoder via a visual-to-text projection module and small adapter modules.
\cite{blip} proposed BLIP, a flexible and transferable model, addressing challenges in understanding and generation-based tasks faced by existing pre-trained models. In follow-up work,
\cite{li2023blip} proposed BLIP2, a generic and efficient pre-training strategy leveraging off-the-shelf frozen pre-trained image encoders and frozen large language models to bootstrap vision-language-language pre-training. 
We take BLIP and BLIP2 as state-of-the-art generalizable models, investigate their ability to answer diverse fine-detail questions, and propose to enhance their ability by novel cropping components.
% \subsection{Image Localization}

\textbf{Image Localization}
Accurately localizing objects in images has been a longstanding challenge, driving extensive research into new algorithms and techniques. \cite{clip_localization1} proposed leveraging CLIP for phrase localization without human annotations, showing improved performance over existing no-training methods in zero-shot phrase localization. \cite{kirillov2023segment} created Segment Anything, which performs image segmentation by developing a versatile and promptable model pre-trained on a diverse dataset. By leveraging prompt engineering, the model aims to tackle various segmentation challenges on novel data distributions. You only look once (YOLO) (\cite{yolo}) and Single shot detector (SSD) (\cite{ssd}) proposed by is an object detection algorithm that simultaneously predicts bounding boxes and class probabilities in real-time, enabling efficient and accurate object detection in images and videos. Some other image localization methods include Faster R-CNN (\cite{faster-rcnn}), 
% R-FCN (Region-based Fully Convolutional Networks \cite{r-fcn}), 
and RetinaNet (\cite{retinanet}). These methods also aim to accurately localize objects in images using different network architectures and techniques. Our approach leverages localization techniques based on multi-modal encoders and gradient mechanisms to improve visual QA via automatic cropping.\footnote{\scriptsize We also experimented with using YOLO and Segment Anything for cropping, but we do not report their results in this paper due to poor performance on the visual QA task.} %, by leveraging 

% are i%nspired by localizaleverage multi-modal encoders, segmentation algorithms, and gradient-based mechanisms to approximate the performance of manual cropping.
