

\begin{table}[t!]
\centering
% \scriptsize
\small
\caption{Human cropping and automatic result on our VQA-text, VQA-hard, and VQA-random dataset. We experiment with fine-tuned and zero-shot VQA models. $^+$ denotes the concatenation result of the original image and cropped image. The performance of the best auto-cropping strategy for a model and dataset is marked in bold.}%  \filip{highlight best results}\jiarui{fixed}}
\begin{tabular}{l|c|rr|rr|rr}
\toprule

\multicolumn{2}{c|}{\textbf{Dataset} \textit{(question count)}} &  \multicolumn{2}{c}{\bf VQA-text (500)} & \multicolumn{2}{c}{\bf VQA-hard (109)} & \multicolumn{2}{c}{\bf VQA-random (1001)} \\ \midrule
\multicolumn{1}{l}{} & \textbf{cropping} & \texttt{acc} & \texttt{str-simi}  & \texttt{acc} & \texttt{str-simi}  & \texttt{acc} & \texttt{str-simi}  \\\midrule
 & w/o cropping & 27.34 & 60.47 & 44.36 & 68.14 & 83.53 & 94.56 \\
 & \hc & 30.68 & 65.29 & 55.14 & 73.21 & - & - \\
 & \grad & \bf 24.80 & \bf 59.58 & 48.90 & 67.32 & 67.61 & 83.87 \\
 & \sac & 22.80 & 55.96 & \bf 49.63 & \bf 70.95 & \bf 79.48 & \bf 92.00 \\
\multirow{-5}{*}{Fine-tuned} & \rac & 19.86 & 53.04 & 47.34 & 69.65 & 71.06 & 86.31 \\\midrule
 & w/o cropping & 27.76 & 57.52 & 33.60 & 55.55 & 60.98 & 79.06 \\
 & \hc & 57.26 & 83.04 & 41.83 & 64.59 & - & - \\
 & \hc$^{+}$ & 58.52 & 82.81 & 41.93 & 61.69 & - & - \\
 & \grad & 37.12 & 66.28 & 40.64 & 63.74 & 59.83 & 79.08 \\
 & \grad$^{+}$ & \bf 43.04 & \bf 70.39 & \bf 41.47 & \bf 63.32 & \bf 65.57 & \bf 82.70 \\
 & \sac & 27.62 & 56.32 & 35.41 & 57.29 & 60.59 & 78.57 \\
 & \sac$^{+}$ & 32.54 & 59.64 & 34.86 & 55.17 & 61.94 & 79.29 \\
 & \rac & 25.80 & 54.61 & 36.15 & 57.26 & 60.71 & 79.14 \\
\multirow{-9}{*}{Zero-shot} & \rac$^{+}$ & 32.70 & 61.13 & 35.96 & 57.67 & 63.44 & 80.29 \\
% \midrule
% Num. of images & - & \multicolumn{2}{c}{109} & \multicolumn{2}{|c}{951}\\ 
\bottomrule
\end{tabular}

\label{tab:main_result}
\end{table}

\section{Results}

\subsection{Effectiveness of Cropping Strategies}

\textbf{Human cropping helps VQA performance, especially for zero-shot models.} \autoref{tab:main_result} shows the performance of the BLIP fine-tuned and the BLIP2 zero-shot model with different cropping methods compared to the original baseline. We note that providing a human-cropped bounding box enhances the model performance significantly for both VQA-text and VQA-hard. On the VQA-text dataset, the performance increases by 3 absolute points for the fine-tuned model and by 30\% (206\% in relative terms) for the zero-shot model. Curiously, with this boost in its performance, the zero-shot model's performance on reading text from images becomes superior to that of the fine-tuned model. We attribute this to the overfitting of the fine-tuned model to the VQA dataset, making it unable to generalize to cropped images that are comparatively easier but uncommon in the training data. Cropping is also highly beneficial for the VQA-hard subset, where the models' performance with human cropping grows by 11\% for both the fine-tuned model and the zero-shot model. We do not have human cropping results on the VQA-random set.




\textbf{Automatic cropping can partially approximate human cropping.} As human cropping cannot be expected to be realistically available for novel images, we investigate to what extent our automatic methods can approximate human cropping results. Our results (\autoref{tab:main_result}) show that automatic cropping is able to approximate human cropping on the VQA-hard dataset. Curiously, automatic cropping is less effective on the VQA-text data, where it harms performance for the fine-tuned model and performs between the baseline and human cropping on the zero-shot model. This shows that detecting the correct positioning of text is more challenging for automatic cropping methods than the wide set of questions in the VQA-hard set. 
We observe analogous results on the VQA-random dataset, where again automatic cropping methods help the zero-shot performance but harm the fine-tuned model performance.
We conclude that automatic cropping methods can often mimic human cropping performance, yet, certain question types may require specialized cropping methods.

% \textbf{Automatic cropping generalizes well to the overall VQA task} \filip{todo once we have the results}

% Phase 1 result: cropping is useful!
% Phase 3 result: is this work useful for general VQA? Generalization
% FT vs. ZS (not sure if we should include this as a separate paragraph or merge this into previous paragraphs.





\subsection{Factors Impacting Cropping Performance}
\label{5.2}
% Phase 2 result: whatâ€™s the advantages and disadvantages of automatic cropping methods?
% Granular performance regarding bounding boxes sizes: smaller bbox more useful?

% \textbf{Gradient}

\textbf{Gradient cropping works better with the zero-shot model, concatenation provides a further boost.} The results in \autoref{tab:main_result} show that the \texttt{grad} method performs better on average than the \texttt{clip} models. 
This performance difference is especially clear for the zero-shot model, where \texttt{grad} performs better across all three datasets with a substantial margin. The performance difference for the fine-tuned model is less pronounced, with the \texttt{clip-w} model performing slightly better on average, and performing especially well on the VQA-random set. Among the two \texttt{clip} variants, the \texttt{clip-w} method performs consistently better on the fine-tuned model, and the \texttt{clip-r} model performs better on the zero-shot model. We hypothesize that this is because using a static sliding window aligns better with the VQA-v2 data images on which BLIP has been fine-tuned, whereas the zero-shot BLIP2 model is more flexible and able to benefit from dynamic window sizes as it overfits less to the VQA-v2 images. Finally, we note that the impact of the concatenated model (denoted by `+' in the table) is consistently positive across different datasets and cropping methods. Notably, the integration of the gradient-cropped image and the original image improves the zero-shot performance on the VQA-random set by 4.59 percentage points, which indicates that the zero-shot model is able to flexibly weigh the fine detail and the original image at inference time.


\begin{table}[t!]
\centering
\small
\caption{Granular performance of different sizes of bounding boxes, where the ratio is the bounding box's area divided by the whole image's area. We mark the best result per model and dataset in bold. }% \filip{highlight best results}\jiarui{highlighted}}
\begin{tabular}{l|c|rr|rr|rr}
\toprule
\multicolumn{2}{c}{\textbf{bbox\_ratio}} \textit{(question count)} & \multicolumn{2}{|c}{\textbf{$<$ 0.005} \textit{(90)}}&\multicolumn{2}{|c}{\textbf{$<$ 0.05} \textit{(410)}}  &\multicolumn{2}{|c}{\textbf{$>$ 0.05} \textit{(90)}} \\ \midrule
\textbf{Model} & \textbf{Cropping Method} & \texttt{acc} & \texttt{str-simi} & \texttt{acc} & \texttt{str-simi}  & \texttt{acc} & \texttt{str-simi} \\ \midrule
\multirow{2}{*}{Fine-tuned} 
& w/o cropping & 22.11 & 56.30 & 26.17 & 59.92 & \bf 34.22 & 65.81 \\ 
& \hc  & \bf 28.44 & \bf 61.17 & \bf 29.98 & \bf 64.92  & 33.89 & \bf 66.99\\
\midrule
\multirow{3}{*}{Zero-shot}  
& w/o cropping & 11.33 & 45.69  & 22.78 & 52.58 & 56.22 & 84.32 \\ 
& \hc & 64.22 & \bf 85.21 & 57.78 & \bf 82.93 & 54.89 & 83.56 \\ 
& \hc$^{+}$ & \bf 66.11 &  84.46 & \bf 58.12 & 82.13 & \bf 60.33 & \bf 85.93 \\ 
% \midrule
% Num. of images & - & \multicolumn{2}{c}{109} & \multicolumn{2}{|c}{951}\\ 
\bottomrule
\end{tabular}

\label{tab:bbox_size}
\end{table}

\textbf{Cropping helps more when bounding boxes are smaller.} 
We investigate the impact of varying cropping result sizes on our VQA-text dataset. % to , with the aim of ensuring accurate bounding box delineation in larger dimensions.
\autoref{tab:bbox_size} shows that human cropping helps for different bounding box sizes, yet, its impact is most significant for questions with focused bounding boxes. This method improves the baseline zero-shot model performance by nearly 600\% on the smallest bounding box size questions. The questions which have larger bounding boxes resemble the original image closer, which makes the impact of cropping intuitively less significant, and sometimes even negative, on these questions. Meanwhile, concatenating the cropped and the original image has a positive impact across all three datasets, which shows that the BLIP2 model can consistently combine images with different granularities.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{figures/crop_comparison.pdf}
    \caption{Three cases of bounding boxes given by human and automatic cropping methods.}
    \label{fig:case_study}
\end{figure}



\subsection{Case Study}

We study the predictions of the two automatic cropping methods that use a strategy of picking a bounding box from a patch map: \grad~and \sac, together with human cropping. %~(as a representation of the CLIP-based model since \rac~has a fixed ratio of the bounding box). We are making a fair comparison because both of them use a strategy of picking a bounding box from a patch map.
\autoref{fig:case_study} shows three examples. In the left image, humans and \texttt{grad} answer correctly by selecting the correct region (\textit{NY}) precisely. Interestingly, here the CLIP model crops the image around the man, which is the only noun in the question (\textit{what does the man love}). In the middle image, the cropping result of \texttt{clip-w} is close to the one of the human. The image on the right has wrong cropping by both automatic methods, indicating a more complex localization task.


% Specifically, in the first example, instead of picking \textit{what does the man love}, the CLIP model picks the man, which is the only noun in the question. On the other hand, the \grad-based model accurately detected the 

Overall, we observe that \texttt{clip-w} consistently crops a larger area of the image, which is primarily driven by the question keywords. The gradient-based model aims to capture the area of interest more precisely. While \texttt{grad} is able to benefit the visual QA model more, especially in scenarios where the answer was more straightforward and easily captured, we find that the more flexible area selection by CLIP-based models proved advantageous in cases where the answer was distributed across a broader region in the image. These observations are intuitive, as the gradients from the visual QA model are more granular between the pixels while the CLIP scores are computed in a more coarse manner. These insights highlight the varying strengths and considerations associated with automated cropping models in different contexts.
% Conversely, the gradient-based model demonstrated effectiveness in scenarios where the answer was more straightforward and easily captured. 

 % In this case, the CLIP-based model's bounding box tends more to include the object while the gradient-based model prefers to include only the areas of interest. .



% we explored the performance of three cropping techniques: gradient, clip-based, and manual/human. The clip-based technique selects a larger area of the image, primarily driven by the question. On the other hand, the gradient-based model aims to capture the area of interest precisely. 
  

% \textbf{Two main failure types on fine-detail questions} stem out of our deeper analysis of the model's performance. The first type, \texttt{a)} is characterized by the VQA model's inability to locate the correct region, leading it to base its answers on irrelevant areas in an image or on no specific region. \filip{let's deepen this discussion - show/describe an example, explain what is happening, in which cases type a) occurs} The second type, \texttt{b)} occurs when the VQA model correctly identifies the target location but fails to scrutinize it in adequate detail. \filip{again here we need to describe what causes type b, when it happens in practice, etc.}


% \textbf{Cropping makes the model gradually more confident in the correct answer.}~
% In Figure~\ref{fig:crop_fig}, we observe a gradual increase in the model's confidence as it zooms into the region of interest within the image. In the first case of the street sign, the loss incurred when the model affirms the presence of an object ('yes') is significantly higher than when it negates ('no'). This disparity shrinks as the model takes a closer look at the street sign. Similarly, in the second case, the model's certainty in classifying the precise category of bird present in the image improves. These observations underscore the effectiveness of cropping as a strategy to bolster the model's confidence, both in affirming the presence of an object and in enhancing the accuracy of fine-grained classifications. \filip{this is fine, but we don't say what cropping this is. Also, we already discuss these examples in section 1, but if the next paragraph is novel then we may be ok :(}

% \textbf{Visual QA model's ability suffers from its inability to focus on image details rather than the inability to localize}~
% As is shown in~\ref{5.2}, gradient-based cropping notably enhances the visual QA performance, which demonstrates that model can enhance its performance by focusing on areas identified by its own attention mechanism. 
% For example, in an image with both large white texts and smaller texts (Figure~\ref{fig: gradient}), \texttt{grad} helps the Visual QA model to successfully locate the small white text according to the question \textit{What does the small white text spell?} This shows visual QA model's strong capability in accurately localizing pertinent area when answering a question, thereby suggesting that the inability to focus in detail is a more critical shortcoming of the visual QA models. This observation also underscores potential of cropping as an effective strategy to address such a problem.


% The compelling performance of gradient cropping, coupled with the case study's revelations, underscores that the key issue with fine-detailed questions is not the VQA model's ability to identify the area of focus, but its capacity to hone in on the details. By implementing our cropping approach, we effectively diminish information redundancy and enable the model to pay closer attention to the pertinent details. Thus, our cropping methodology emerges as a highly promising solution for enhancing VQA performance on fine-detail-related questions.

% \textbf{Cropping makes the model gradually more confident in the correct answer} As is shown by~\ref{fig:crop_fig}, when we focus more on the 

% \filip{todo once we have a plot}

% %Attention maps, confidence plots, 


% \textbf{Cropping helps the model behave in intuitive ways} \filip{case study here}
