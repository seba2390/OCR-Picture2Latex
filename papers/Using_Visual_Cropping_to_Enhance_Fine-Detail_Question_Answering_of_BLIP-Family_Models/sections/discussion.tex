\section{Conclusions and Outlook}

% \filip{This section would be nice to have but depending on space and time we can also drop it. Some limitations + future work are: more benchmarks (we only focus on one), more models (we only focus on one), and more sophisticated ways to combine cropped+original image predictions.}
% In this study, we highlight significant limitations in the state-of-the-art BLIP-family VQA models and propose to address these issues via cropping methods. 

This paper focused on improving the accuracy of state-of-the-art visual QA models on fine-detail questions using visual cropping. We defined three controlled benchmarks and experimented with novel cropping approaches based on the visual QA model's gradient and multi-modal encoding. Our experiments showed that the family of the BLIP model benefits significantly from human cropping, and automated cropping approaches can largely approximate these benefits. A deeper dive into our findings indicates that the performance enhancement is more pronounced in zero-shot models than in fine-tuned models and more salient with smaller bounding boxes than larger ones. While cropping enhancement helps with fine-detail questions, it is also robust on the overall VQA-v2 task. These insights provide a promising direction for improving visual QA models, especially on fine-detail questions, and highlight the importance of further research on this topic. 
% In the future, we plan to explore more sophisticated methods for combining fine- and coarse-detail images as input to the model, to experiment with a larger set of benchmarks and question types, and to investigate the generalization of our methods to other VQA model families.

Despite our advancements, our experiments exhibit several limitations that open up opportunities for future research.
(1) The cropping process may compromise visual QA performance by eliminating valuable context information from the full image. We attempted to address this issue by concatenating the original and cropped images for visual input. However, this approach deviates from the pre-training regimen of the visual QA model, which might cause mismatches between the learning objectives. %\filip{in the learning objectives?}\jiarui{fixed} 
Future work could explore more sophisticated methods for combining predictions from the cropped and original images, such as training a decision-making model or leveraging large language models to ensemble multiple sources of information~\cite{hugginggpt}.
(2) Our approach is time-consuming. We refrained from testing our method on the entire VQA dataset and chose a random subset instead due to the expensive cropping procedures. For instance, applying a single CLIP sliding window auto-cropping method to our VQA-random dataset with 1001 questions takes approximately 1.5 hours. The recursive strategy is twice as fast as the sliding window approach, yet, it performs relatively worse on average. Therefore, future steps should focus on enhancing the speed and precision of cropping techniques. 
%Exploring CLIP encoders or leveraging more efficient localization techniques could provide valuable insights for further improvement.
(3) As all our test datasets are subsets of VQA-v2, we see extending our experiments to additional benchmarks as crucial. Real-world images can have a variety of complex factors, e.g., they may be blurred, with low resolution, have less brightness, or have low contrast. To enable our cropping methods to benefit these use cases, we plan to enrich them with image super-resolution and image enhancement methods.% in our cropping methods. 
% \filip{how are these two different?}