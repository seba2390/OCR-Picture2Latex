\documentclass{article}



\usepackage{subcaption}
\usepackage{graphicx}
% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2023}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{amsmath}


\newcommand{\grad}{\texttt{grad}}
\newcommand{\seg}{\texttt{seg}}
\newcommand{\sac}{\texttt{clip-w}}
\newcommand{\rac}{\texttt{clip-r}}
\newcommand{\hc}{\texttt{human-crop}}

%%% OUR COMMANDS
\usepackage[sectionbib]{bibunits}
\defaultbibliographystyle{nips}
\defaultbibliography{cite}
\input{commands}

\title{On the Difficulty of Answering Fine-Detail Questions in Open-Ended Visual Question Answering}
\title{Capturing Fine Details in Open-Ended Visual Question Answering via Cropping}
\title{Answering Fine-Detail Questions via Visual Cropping}
\title{Enhancing Vision Question Answering of BLIP-Family Models via Visual Cropping}
\title{Enhancing Fine-Detail Question Answering of BLIP-Family Models via Visual Cropping}
\title{Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models}


% \title{Curiosity-Guided Captioning: Harnessing Interactive Question-Answering for In-Depth Visual Narratives}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Jiarui Zhang \\
  % \thanks{Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.} \\
  Information Sciences Institute\\
  University of Southern California, USA \\
  \texttt{jrzhang@isi.edu} \\
  % examples of more authors
  \And
  Mahyar Khayatkhoei \\
  Information Sciences Institute \\
  University of Southern California, USA \\
  \texttt{mkhayat@isi.edu} \\
  \AND
  Prateek Chhikara \\
  Information Sciences Institute \\
  University of Southern California, USA \\
  \texttt{pchhikar@isi.edu} \\
  \And
  Filip Ilievski \\
  Information Sciences Institute \\
  University of Southern California, USA \\
  \texttt{ilievski@isi.edu} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}
% \begin{bibunit}

\maketitle


\begin{abstract}
Visual Question Answering is a challenging task, as it requires seamless interaction between perceptual, linguistic, and background knowledge systems. While the recent progress of visual and natural language models like BLIP has led to improved performance on this task, we lack understanding of the ability of such models to perform on different kinds of questions and reasoning types. As our initial analysis of BLIP-family models revealed difficulty with answering fine-detail questions, we investigate the following question: \textit{Can visual cropping be employed to improve the performance of state-of-the-art visual question answering models on fine-detail questions?}
Given the recent success of the BLIP-family models, we study a zero-shot and a fine-tuned BLIP model. We define three controlled subsets of the popular VQA-v2 benchmark to measure whether cropping can help model performance. Besides human cropping, we devise two automatic cropping strategies based on multi-modal embedding by CLIP and BLIP visual QA model gradients.
Our experiments demonstrate that the performance of BLIP model variants can be significantly improved through human cropping, and automatic cropping methods can produce comparable benefits. A deeper dive into our findings indicates that the performance enhancement is more pronounced in zero-shot models than in fine-tuned models and more salient with smaller bounding boxes than larger ones.
We perform case studies to connect quantitative differences with qualitative observations across question types and datasets.
Finally, we see that the cropping enhancement is robust, as we gain an improvement of 4.59\% (absolute) in the general VQA-random task by simply inputting a concatenation of the original and gradient-based cropped images.
We make our code available to facilitate further innovation on visual cropping methods for question answering.

% We also closely examine the improvements by gradient-based cropping alongside specific case studies, which led us to conclude that visual QA models exhibit a commendable proficiency in localizing the target area. However, these models are inherently limited by their inability to adjust their inputs based on this recognition, thereby bringing to light the crucial role and effectiveness of our cropping methodology. 
\end{abstract}


\input{sections/intro}
\input{sections/relatedwork}
\input{sections/methods}
\input{sections/experiments}
\input{sections/result}
\input{sections/discussion}
\input{sections/conclusion}


% \clearpage

\bibliography{cite}
\bibliographystyle{nips}

% \putbib

% \end{bibunit}

% \begin{bibunit}
\newpage
\appendix
\onecolumn
\input{sections/appendix}
% \putbib
% \end{bibunit}

\end{document}
