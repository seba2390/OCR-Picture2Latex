\section{Evaluation Setup}
Next, we build machine learning-based binary classifiers for each of the target variables (i.e. personality, boredom proneness, demographics, and shopping interests) and for each of the data types from scenarios 1 through 4. In order to be able to carry out binary classification, we split participants into two sets (low vs. high) using the median value of each target variable. This approach has been used frequently in the literature \cite{chittaranjan2013mining, de2013predicting, staiano2014money}. As multiple participants can have the median value and the median based split does not evenly divide the participant pool accordingly, the baseline accuracies are sometimes not exactly 50\%. 

We perform a quantitative comparison of the quality of the user models built in each scenario and derive insights about the relationship between mobile online activity and user traits/interests. Since several related works have used a similar approach \cite{chittaranjan2013mining, de2013predicting, staiano2014money}, we can also interpret the quality of our results by comparing them to those reported in the literature. 

As mentioned earlier, the features computed in each scenario are evaluated separately. Recall that having access to the hostname and/or the content of the HTTP(S) message (S3 and S4) does not necessarily imply being able to access the data considered in S1 and S2. For example, a web browser client can access data from S3 and S4 but typically lacks the ability to access the overall mobile-phone traffic (such as including app-generated traffic) in order to compute features corresponding to S1 and S2.

\subsection{Ground-truth}
Next, we report the statistics of the ground truth variables obtained from our participants' answers to the questionnaires.

% Head 3
\subsubsection{Demographics}
Table~\ref{tbl:two} shows the demographic distribution of the participants. As seen in the Table, our participants are diverse in gender, age and education levels, which supports a general interpretation of our results.

\subsubsection{Personality}
The answers to the IPIP Big-5 Personality Test showed high internal consistency, with Cronbach's alpha values of .78 (openness), .81 (extroversion), .80 (conscientiousness), .87 (agreeableness), and .86 (neuroticism). The scores were also near-normally distributed. Though we were not able to verify if the statistics (mean and st. dev) of our sample (shown in Table~\ref{tbl:three}) match with those of a larger Spanish population due to the lack of relevant literature, we believe that our results are reliable as the internal consistency matches that of prior Big-5 tests. The mean internal consistency of the IPIP Big-5 test \cite{goldberg2006international} is .84. In addition, Del Barrio et al. \cite{del2003personalidad} also reported the results of a Spanish language version, with Cronbach's alpha between .78 and .88 and temporal stability (r between .71 and .84). 
The median value-based division for the binary classification task gave two balanced sets for all the five personality dimensions: 31 participants (\textit{high}) vs. 30 participants (\textit{low}) for Extraversion, Agreeableness, and Neuroticism, and 33 participants (\textit{high}) vs. 28 participants (\textit{low}) for Conscientiousness and Openness.  

\begin{table}
  \includegraphics[scale=0.5]{figures/personality.png}
  \caption{Big-5 statistics of our participants}
  \label{tbl:three}
\end{table}

\subsubsection{Boredom Proneness}
The answers to the Boredom Proneness Scale also demonstrated a high internal consistency with an alpha value of .86 --which is in accordance to previous work \cite{farmer1986boredom} and demonstrates satisfactory levels of internal consistency (alpha = .79) as well as of test-retest reliability (r = .83). The median value-based division for the binary classification task also gave two balanced sets with 31 and 30 participants in the high and low boredom proneness classes respectively. 

\subsubsection{	Shopping Variables}
While we inquired the frequency of purchasing for 14 product categories, we excluded five of them from the evaluation since the responses were extremely skewed to the selection "never purchase it". For the other 9 categories, we segment participants into two sets using the median value per category. Table~\ref{tbl:four} depicts the statistics for the four example product categories for which the classification accuracy in our experiments was sufficiently high.

\subsection{Model Selection}
We tested a number of machine learning-based classifiers and chose Gradient Boosting Machines (GBMs) in all reported results. Three different classes of algorithms were tested: first, Support Vector Machines (SVMs) (with RBF and linear kernels) given their high performance in related tasks \cite{chittaranjan2013mining, matic2015boredom, de2013predicting}; second, decision tree-based methods (namely, Random Forests and GBMs) since they satisfy the max-margin property, yield state-of-the-art results and do not require feature space specification \cite{breiman2001random}; third, probabilistic methods (Na{\" i}ve Bayes). 

\begin{table}
  \includegraphics[scale=0.5]{figures/shopping.png}
  \caption{Selected Shopping Interest - Statistics}
  \label{tbl:four}
\end{table}

This testing process helped us approach the task from different angles. GBMs and SVMs with RBFs kernel outperformed others, and occasionally SVMs were more accurate than GBMs. However, we chose GBMs as they showed more stable performance across the target variables and they do not require feature space specification. Thus, they are not affected in their performance by feature selection. 

Our implementation of GBMs is based on the R library \textit{XGBoost}
\footnote{https://xgboost.readthedocs.org/en/latest/}. 
We tried a set of parameter combinations to prevent overfitting: \textit{eta} that determines the learning rate, \textit{gamma} regulating the sensitiveness to training examples, and the number of iterations. The set of combinations we tried includes the default values defined by the library, 0.3 for \textit{eta} and 0 for \textit{gamma}, and the ones with lower \textit{eta} (0.1 or 0.2) and higher \textit{gamma} (2 or 3) which can help avoiding overfitting. For the combinations with lower \textit{eta} we doubled the number of iterations as the learning rate is slower. For each data type, we applied the same set of combinations and chose the one with the best performance across all target variables. Considering the number of instances (61) we measured performance through a leave-one-out approach, which sequentially selects one data point (i.e. in our case one participant), trains the model with the rest of the data points and tests the model with the selected data point. 



