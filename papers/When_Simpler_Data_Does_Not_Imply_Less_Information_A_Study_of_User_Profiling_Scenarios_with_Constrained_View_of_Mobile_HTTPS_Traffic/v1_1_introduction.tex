\section{Introduction}

The exponential growth in smartphone adoption, the availability of thousands of mobile apps connected to the Internet and the development of the Internet-of-Things with billions of connected devices are contributing to the generation of vast amounts of personal data streams. Part of this data is reflective of user activities and behaviors as people carry their mobile devices around-the-clock. The availability of this human-behavioral data combined with sophisticated data-driven machine learning techniques has enabled unprecedented user profiling possibilities. A broad spectrum of players is collecting and analyzing these data streams, such as mobile apps, shops offering free Wi-Fi Internet access to its customers, telecoms and major Internet companies. As useful as it might be for personalizing and improving services, the access to such data streams introduces serious concerns regarding user privacy. 

To date, much progress has been made on understanding the privacy implications of mobile and Internet services that have access to personal identifiable information (PII). However, even without the access to PII, studies have revealed that it is possible to fingerprint \cite{narayanan2008robust}, track \cite{de2013unique}, and carry out different kinds of discrimination \cite{hannak2014measuring} from the analysis of usage logs of specific services, such as browsing history, search logs and movie streaming systems. In consequence, specific legislation and regulation for the treatment of online personal data has been passed in several countries, including Canada and the European Union \cite{ECPPD} aiming to give users back control over their personal data, protect their privacy and simplify the regulatory environment for businesses. This regulation requires Internet services and mobile apps to collect explicit user consent when collecting and analyzing personal data.

Nevertheless, due to the high complexity of the personal data ecosystem and the diversity of entities surrounding the flow of personal data, threats to user privacy (such as data leakage or profiling sensitive information) may arise from entities different than those providing the services with which the users directly interact. For example, an entity positioned in between the user and the service provider might have access to parts of the user's data which --despite being a partial view-- could enable it to profile the user. As a result, users are subject to non-explicit profiling that is beyond the scope of their attention and consent, especially when the observing entities are not intuitively noticeable by the user and/or the data is not clearly subject to privacy policies and personal data laws. The recent change made to the broadband privacy regulation in the U.S. can increase the possibility of such scenarios. This change allows Internet service providers to share private data such as web browsing history without prior user consent \footnote{refer to https://www.nytimes.com/2017/04/03/technology/trump-repeal-online-privacy-protections.html}. 

Our work aims to foster a discussion about non-explicit profiling scenarios by exploring possible profiling approaches with real user data and analyzing their capabilities to profile a variety of potentially sensitive personal information. We refer to the profiling approaches as \textit{constrained profiling} due to the inherent constraints that a profiling entity has. The constraints can stem from multiple reasons, including inherent limitations in the provision of the service (e.g. in the case of HTTPS pages, web browsers can see the content of HTTPS pages while Wi-Fi access points only see up to the domain name of the address), economic limitations (e.g. costs or inability to store large amounts of data), compliance to personal data laws or internal company policies that limit storing certain data for liability reasons. Having awareness of such constraints in practice, we investigate the profiling capabilities of state-of-the-art algorithms when applied to constrained data, and derive implications for designers of user modeling services.

We focus our study on mobile HTTP(S) 
\footnote{Note that we refer to the traffic of HTTP protocol in the following ways: HTTP for unencrypted traffic, HTTPS for encrypted traffic (SSL-over-HTTP), and HTTP(S) for whole HTTP and HTTPS traffic} traffic, and develop possible profiling scenarios considering the potential profiling entities around the data and the constraints they would have. To collect the data, we carried out an in-the-wild user study with 61 participants who gave us access to their mobile HTTP(S) traffic for at least 30 days. The data set includes the traffic generated from any smartphone apps and browsers, which is a relevant data stream for various entities including mobile app providers, telecommunication operators, mobile advertising companies, etc. We apply different constraints to the raw HTTP(S) traffic data - such as having access to only the timestamp of the data, to the header, and ultimately to the full content for the HTTP pages. We consider four profiling scenarios that analyze each of the constrained datasets and also require different levels of technical sophistication, such as the ability to filter noise, categorize websites or analyze Web content. We build user models using state-of-the-art machine learning algorithms taking as input each of the four constraint datasets and compare their performance. In order to cover a wide set of profiling scenarios we consider a variety of personal attributes including personal traits (the Big-5 personality traits and boredom proneness), demographics, and product interests. 

Our study suggests that certain types of personal information inference are still possible even under the constraints described above. The results can be interpreted differently: from the users' perspective, as greater privacy risks from a larger and broader set of profiling entities, encouraging users to be more conservative in sharing data; on the other hand from the perspective of the business providers, as an opportunity to collect less personal data while still being able to provide personalized services. Instead of drawing a one sided interpretation, more fundamentally, our interpretation of the results is to acknowledge the potential value of the users' data and to have 'transparency' as a principle. Although transparency itself does not guarantee a solution\cite{nissenbaum2011contextual, acquisti2005privacy}, we believe that it still is an essential element that enables future discussion about more responsible ways of collecting and using personal data that different parties can agree on.

The main contribution of our study is in unfolding the profiling scenarios around mobile HTTP(S) traffic of people, and examining the capability for inferring various personal information. We discover multiple instances of unexpected profiling scenarios that can be implemented in practice with commonly known techniques, and extract key implications that can shape future discussions. On the other hand, the study is limited in a number of aspects. The data set does not cover the traffic generated in stationary situations, which limits the study having a complete picture of mobile phone usage. In addition, our findings made from the mobile HTTP(S) traffic also suggest more diverse directions for extension considering other diverse behavioral data streams such as location traces and app specific behaviors (\textit{e.g.}, multimedia playlists or photo-taking patterns). The results are also limited in terms of achieving significantly better profiling performance than state-of-the-art methods as our study focuses more on understanding the  user profiling capabilities under a variety of data availability scenarios. 