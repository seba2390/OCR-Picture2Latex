\section{Study Design}
We conducted an in-the-wild user study to collect: (1) mobile HTTP(S) traffic, and (2) self-reported traits and interests about our participants through a series of online questionnaires. The HTTP(S) traffic was collected over 6 months during which we logged an average of 77 $\pm$ 21 days per participant. We describe the participants and the data collection methodology below.

\subsection{Participants}
We recruited ~200 smartphone users in Spain. They were recruited through two channels: first, an email list of volunteers who signed up for participation in user experiments carried out by our research organization. Second, a recruitment agency for user studies. The participants did not receive fixed incentives, but we held monthly raffles during the data collection period (from August 2015 to January 2016) for a 100 Euro gift card of a large online store. Participants were enrolled via the study's website which guided them to go through three stages: (1) a consent form
\footnote{Due to the difference in the legal and organizational infrastructure of the country where the study took place, the study did not go through the Institutional Review Board review process. The Ethics Committee, which is a similar institution in the European Union, deals with clinical trials.}
, (2) several user profile questionnaires to collect ground truth, and (3) instructions on how to configure their mobile phones to use our mobile proxy. 

As the study collects potentially sensitive information, we elaborated on the data collection and our analyses throughout the consent form in a reader-friendly way while assuming a non-technical audience. The consent form described upfront that the study aims to explore the relationship between mobile browsing behaviors and personal characteristics. It further described that we would explore various data mining techniques applied on the collected data and that no human will read their logs. We also made a dedicated section to inform about the data management process. It described that the browsing history would be collected from their mobile device, and we would not collect any other personally identifiable information than the email address. We also clarified that the data would not be shared outside the research organization nor leave, and that the data would be deleted once the study is completed. The data was stored in a way that allowed access to the researchers involved in this study only via the intranet of the organization.

Only the applicants who successfully completed the three stages were enrolled to the study, namely the ones who consented, filled out the questionnaires and properly configured our mobile proxy. While 94 out of the 200 participants successfully completed these three stages, the amount of collected data per participant varied from 8 days to 170 days. Hence, we had a trade-off between maximizing the number of days of the collected logs per user and the number of available subjects who satisfied the threshold. We iteratively tested a range of the thresholds and observed that the accuracy of our models stabilizes with at least 30 days (61 subjects) of internet logs. Interestingly, the modeling performance remained similar as we were increasing the threshold of the number of days, however, sharply dropped when less than 40 subjects were involved in the analysis. Lowering the threshold below 30 days also negatively affected the performance as it adds noise, potentially suggesting that less than one month of logs does not suffice to extract features that quantify users' typical internet usage routines. Figure~\ref{fig:zero} shows the details about the trade-off. We thus present the results taken from the 61 participants who contributed their data for at least 30 days. Table~\ref{tbl:two} summarizes our participants' demographic information.

\begin{figure}
  \includegraphics[scale=0.6]{figures/trade_off.png}
  \caption{Trade-off between the subjects and the data availability: the bars show the number of subjects who satisfy the data availability requirement (x-axis), and the red line shows the number of target variables predicted with a certain level of balanced accuracy ($>$ 60\%).}
  \label{fig:zero}
\end{figure}

\begin{table}
  \includegraphics[scale=0.5]{figures/participants.png}
  \caption{Demographics of the participants in our study.}
  \label{tbl:two}
\end{table}

\subsection{Mobile Online Activity Data Collection}
The proxy-based approach that we followed for data collection has several advantages. First, it does not require participants to install any app and spend resources of their mobile device for the study. Second, we collect the HTTP(S) traces regardless of the app or browser that generated them. As many apps use the HTTP(S) protocol to communicate, our approach can capture information about the usage of apps. In addition, we log web page accesses from any app, for example, a page opened with a non-default browser by clicking a link in the Twitter app. Finally, it does not require to write and maintain several versions of the data collection software for different mobile operating systems. Note that our approach is limited when it comes to capturing the activities inside mobile apps, such as tweeting inside the Twitter app. This limitation is difficult to address since apps usually have their own internal protocol and data format. With respect to HTTPS traffic, we do not obtain the full URL of the destination but only the hostname. In addition, recall that we do not collect private pages that require tokens or authentication. 

\subsection{Questionnaires}
We considered various aspects that could be of interest of service providers for personalizing individual online experiences. In this regard, the participants filled multiple questionnaires to collect ground truth information about their characteristics and interests, which we aimed to model. We asked the participants to report their shopping interests (being an evident focus for online advertisers), demographics (that represents one of the basic categories of variables when it comes to using computers and internet), and finally personal traits (that have been shown to be relevant to UX personalization). Specifically, we collected the following information: 
\paragraph{\textbf{1) Demographics}} Participants provided us with their gender, age and education level as depicted in Table~\ref{tbl:two}.

\paragraph{\textbf{2) Personality}} We collected our participants' Big-5 personality traits using the widely validated 50-item IPIP questionnaire \cite{goldberg2006international}. This model is commonly used in Social Psychology to characterize personality using five dimensions (extraversion, neuroticism, agreeableness, conscientiousness and openness). Over the past decade the Big-5 has attracted the attention of the user modeling community due to the impact of an individuals' personality on their mobile phone usage \cite{butt2008personality}, online activities \cite{ramirez2010relationship}, satisfaction with technological services \cite{oliveira2013influence}, and online purchasing preferences \cite{huang2010relationship}.

\paragraph{\textbf{3) Boredom Proneness}} We quantified the tendency of individuals to experience boredom using the Boredom Proneness Scale \cite{matic2015boredom}, which has been used for this purpose for the last three decades. Boredom proneness is associated with susceptibleness to stimulation seeking behavior and mobile phones are often used as a stimulation source \cite{barabasi2005origin}.

\paragraph{\textbf{4) Shopping Interests}} We obtained self-reported information about product interests by asking our participants about basic purchase frequency of various products through questions on a 7-point Likert scale (1=never, 7=very often). We identified 14 product categories commonly listed in online stores: books, computers, software, mobile apps, music, videos, flowers, flights, tickets, clothes, travel, furniture, home appliances, and groceries. 

