\section{Feature Extraction}
We compute a rich set of features for each data type with the goal of capturing the users' mobile online activity routines. We elaborate on the features below, whose summary is shown in Table~\ref{tbl:one}. 

\begin{table}
  \includegraphics[scale=0.5]{figures/features.png}
  \caption{Overview of the developed features.}
  \label{tbl:one}
\end{table}

\subsection{Scenario 1 (S1): Dynamics of HTTP(S) Access}
The timestamp information of HTTP(S) traffic can be obtained without looking into the HTTP(S) message at all. The features at this level are designed to capture the temporal patterns in mobile online accesses; patterns derived from when an online access starts, how long it proceeds for, and how often such accesses take place. Note that S1 data includes not only user-initiated HTTP(S) requests but also automatically generated requests regardless of actual user interaction (\textit{e.g.}, background activity of apps or OS platform-level communication). As we do not look into any field of the HTTP(S) traffic in this scenario, we are not able to distinguish and filter them out. 

To compute features from the data of scenarios 1 and 2 (S2, explained below), we divide the day into \textit{four time periods} and compute features aggregated per time period: morning (5am to 12pm), afternoon (12pm to 6pm), evening (6pm to 10pm) and night (10pm to 5am).
We compute the following features:

\paragraph{1. First and last access time} We take the time of the first and the last HTTP(S) requests for each period of the day. 

\paragraph{2. Total number of requests} This feature counts the number of observed requests for each period of the day.

\paragraph{3. Total online duration} We approximate the total duration of mobile online activity per day and per period of the day. For this, we break down the day into 10 minute slots, and then count the slots when there is an HTTP(S) traffic. 

\paragraph{4. Estimation of sessions, their frequency and duration} Using a time window of 5 minutes, we estimate sessions by grouping the consecutive requests that are made within the time window. We then count the number of sessions and measure the total duration of the sessions, again for each of period of the day. 

\paragraph{5. Inter-session time} We measure the time between every two consecutive sessions of a day. We collect all the measurements over the whole study period, fit to Weibull and Power-Law distributions
\footnote{In addition to the parameters of the two distributions, we included also Kolmogorov-Smirnov scores as one of the features.}
 and compute the distribution parameters. This is in accordance with a number of studies to characterize burstiness in a variety of human behaviors \cite{barabasi2005origin} and particularly phone usage patterns follow one of the two distributions \cite{jiang2013calling}.

Once the features are extracted per each day, we compute descriptive statistics separately for weekends and weekdays
\footnote{More precisely working vs non-working days (the latter including weekends and bank-holidays).}
. Overall, we compute 135 features in this scenario. 

\subsection{Scenario 2 (S2): Header Analysis}
In this scenario, we assume that the data is constrained only to the header of the HTTP(S) messages - not the content. Among the fields in the header, we take into account the three fields that could be related to the activity of the users: request method (i.e., HTTP(S) verb), delivered data, and host and path which informs the destination URL. These fields inform where the users visit, the amount of content consumed (bytes), and possibly the type of interaction such as POST, GET, or CONNECT for HTTPS. Except the full path of the URLs, the fields can be observed also for HTTPS traffic
\footnote{As for HTTPS traffic, the field 'delivered data' is computed by taking the overall sum of bytes transferred via a TLS session.}
. The other fields mostly depend on the underlying infrastructure rather than on user actions. 

Another advantage over the first scenario is that it opens the possibility to identify and filter out automatically generated traffic. As mentioned, the automatic traffic could be generated by the background activity of apps, operating system, and also by web browsers to fetch objects for page rendering. Filtering such traffic helps the analyses to reflect the actual behavior of users more accurately and avoid possible biases due to certain apps or platforms. While the filtering task itself is a challenging research problem, we implemented a simple and practical solution using an HTTP(S) client. It filters out the HTTP(S) requests that do not return an web document with certain content (text, image, or video), and those made to page objects and resources (e.g., json, css, and Javascript files) since they are likely to be triggered by browsers automatically in the rendering process rather than by a user's request. 

We compute the following features from data in S2: 

\paragraph{1. Total amount of consumed content} This feature sums up the total number of bytes exchanged during each period of the day. This feature is calculated using all the messages regardless of whether they are generated through user interaction or not since we are interested in the total amount of generated traffic. 

\paragraph{2. Number of messages by method} For each HTTP(S) verb, we count the number of messages exchanged in each period of the day. In this case, we only consider the messages that are associated to user interaction.

\paragraph{3. Features in S1 after filtering} We compute all the features from S1 data but only with the messages that are associated with user interaction.  

Similar to the data in S1, we compute descriptive statistics separately for weekends and weekdays yielding a total of 180 (135 + 45) features.

\subsection{Scenario 3 (S3): Category of web pages}
As described, this scenario requires a method for identifying the topical category of a given URL. We use DMOZ
\footnote{www.dmoz.org}
, a commonly used open directory of websites, to annotate the destination hostnames with semantic tags. First, for each participant, we sort out the HTTP(S) messages that are associated with user interaction as we do for Scenario 2. Second, we query the DMOZ directory for all the messages in order to obtain their topical categories. The dictionary returns a category of multiple hierarchies, for example,  'Computers/News and Media' for the website 'techcruch.com' We take into account the categories of all the hierarchies for feature construction. 

We generate the features by creating a term vector for each participant, with as many dimensions as categories and a weight value assigned to each category. We tried two weighting schemes: term frequency (TF), which counts the frequency of categories from the history of hostnames for each participant; and TF-IDF, which counts the frequency of the category from the history of hostnames (the TF term), and then scales the frequency based on the commonality of the corresponding category across all participants (the IDF term). It emphasizes the categories that distinguish a user from the others. The IDF term decreases the weight of the categories that commonly appear across subjects. 

We additionally apply Latent Semantic Indexing (LSI) to reduce the dimensionality of the vectors due to the sparsity of the original term vectors. LSI is a popularly used technique especially in information retrieval, which reduces the dimensionality by analyzing the similarity between the original dimensions using singular vector decomposition. 

\subsection{Scenario 4 (S4): Topic of web pages}
The data analyzed in S4 assumes having access to the content of a limited set of web pages that are publicly accessible. 

We create a topic profile for each participant from the content of the visited web pages. In order to recognize the topic of the web pages, a topic model is built first using the content of all the web pages accessed by all the participants. The topic model identifies the major topics observed in the corpus, which serves as a categorization framework of web pages. Once the major topics are identified, we use the topic model to estimate the topic distribution of every web page over the identified major topics, and compute the topic profile of each participant by aggregating the topic distribution of the browsed pages. 

The corpus of HTTP pages goes through three pre-processing steps before the topic model construction: main text extraction, language recognition, and lemmatization. In order to filter noise and focus on the main content, we use Boilerpipe \cite{kohlschutter2010boilerplate}, a web page parsing library. As our corpus includes web pages of different languages, language identification is crucial for the subsequent analysis process. The language of each web page is identified through Libtextcat \cite{cavnar1994n}, and the pages of the top two most common languages (Spanish and English) are taken for analysis as they cover the majority of the corpus. Finally, we use FreeLing \cite{carreras2004freeling} for lemmatization of both Spanish and English text and perform stop-word removal. A topic model is created separately for the two languages. 

We use Latent Dirichlet Allocation (LDA) \cite{blei2003latent} to build the topic model. LDA is a fully Bayesian unsupervised framework for inferring latent topics of a given corpus. It views documents as mixtures of topics and topics as mixtures of words. During topic inference, both sets of mixtures (words and topics) are adjusted to maximize the likelihood of the input corpus. 

We implement LDA based on Collapsed Gibbs Sampling. The two hyper-parameters $\alpha$ and $\beta$ are tuned using the Digamma Recurrence Relation \cite{breiman2001random}. The number of topics, \textit{K}, is set to 20 (per language), as it provides a good trade-off between topic specificity and coverage in our application setting. After applying LDA, we manually label each topic from the keywords produced by LDA and by looking at sample web pages associated with the topic. 

