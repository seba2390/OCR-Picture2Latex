% \documentclass{article}

% % \usepackage{corl_2021} % Use this for the initial submission.
% %\usepackage[final]{corl_2021} % Uncomment for the camera-ready ``final'' version.
% \usepackage[preprint]{corl_2021} % Uncomment for pre-prints (e.g., arxiv); This is like ``final'', but will remove the CORL footnote.
% \usepackage{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
% \usepackage{graphicx}
% \usepackage{subcaption}
% \usepackage{booktabs}
% \usepackage{multirow}
% \usepackage{siunitx}
% \usepackage{amssymb}% http://ctan.org/pkg/amssymb
% \usepackage{pifont}% http://ctan.org/pkg/pifont
% \usepackage{pdfpages}
% \usepackage{caption}
% \usepackage{hyperref}
% \newcommand{\cmark}{\ding{52}}%
% \newcommand{\xmark}{\ding{56}}%
% % \newcommand{\datasetName}{TTO}
% \usepackage[table,xcdraw]{xcolor}
% \usepackage{enumitem}
% \usepackage[font=small,labelfont=bf]{caption}
% % \usepackage{cite}
% \usepackage{xspace}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % space tweaks 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \setlength{\abovecaptionskip}{0.5mm}
% \setlength{\belowcaptionskip}{0.5mm} 
% \setlength{\textfloatsep}{1.5mm}
% \setlength{\dbltextfloatsep}{1.5mm}

% \renewcommand{\thesection}{\Alph{section}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % document details
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % \newcommand{\algoName}{TranspareNet\xspace}
% \newcommand{\algoName}{TranspareNet\xspace}
% \newcommand{\dataName}{TODD\xspace}
% \DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}
% % \title{\algoName: Joint Point-Cloud and Depth Completion for Transparent Objects}
% \title{Seeing Glass: Joint Point Cloud and Depth Completion for Transparent Objects}
% % The \author macro works with any number of authors. There are two
% % commands used to separate the names and addresses of multiple
% % authors: \And and \AND.
% %
% % Using \And between authors leaves it to LaTeX to determine where to
% % break the lines. Using \AND forces a line break at that point. So,
% % if LaTeX puts 3 of 4 authors names on the first line, and the last
% % on the second line, try using \AND instead of \And before the third
% % author name.

% % NOTE: authors will be visible only in the camera-ready and preprint versions (i.e., when using the option 'final' or 'preprint'). 
% % 	For the initial submission the authors will be anonymized.
% \author{
% Haoping Xu$^{1}$, Yi Ru Wang$^{1}$, Sagi Eppel$^{1}$, Al\`{a}n Aspuru-Guzik$^{1}$, Florian Shkurti$^{1}$, Animesh Garg$^{1,2}$ \\
% $^{1}$University of Toronto, Vector Institute,  
% $^{2}$Nvidia \\
% % \texttt{\{\href{mailto:dturpin@cs.toronto.edu}{dylanturpin},
% % \href{mailto:liquan.wang@cs.toronto.edu}{liquan.wang},
% % \href{mailto:tsogkas@cs.toronto.edu}{tsogkas},
% % \href{mailto:sven@cs.toronto.edu}{sven}, \href{mailto:garg@cs.toronto.edu}{garg}\}@cs.toronto.edu}
% }


% \begin{document}


\section{Appendix}
\label{sec:appendix}

\subsection{Data Capture and Annotation Pipeline }
We use an eye-in-hand Franka Emika Panda with Intel RealSense RGBD D435i camera to collect the dataset. Intel D435i camera has a resolution of 1280x720 with field of view of 87 degrees horizontally and 58 degrees vertically \footnote{Intel RealSense D435i's specifications are listed in \url{https://ark.intel.com/content/www/us/en/ark/products/190004/intel-realsense-depth-camera-d435i.html}  }. All RGB and depth images are captured under 640x480 resolution . To fully automate the annotation, AprilTags in the 36h11 family, where each tag is a square with 40mm side length, is used. Objects are placed to a fixed location and pose with respect to the tags, which can be used later generate the ground truth with the object mesh. The Franka Panda robot eye-in-hand system is built, and controllers for the robot as well as the RealSense camera are programmed using FrankaPy \cite{frankapy}. In terms of automating annotation, we use the AprilTag2 \cite{apriltag2} approach, which has more convenient API support and enables easier 6DoF Pose calculations. The detailed dataset collection process is performed as follows:
\begin{itemize}
    \item An array of AprilTag2 are placed around a transparent object, each tag is from the same family with a different index. And the offset from the tag and object is known using a printed template.
    \item Multiple positions and viewing angles are selected. For every viewpoint, the robot aims the camera towards objects and captures the RGB-D images.
    \item  For automatic annotation, we first detect the AprilTag pose using the color input. The object pose can be extracted via the tag's pose. Using the 3D model of the object, the segmentation mask, ground truth depth map can be computed. 
    \item Every viewpoint in the dataset will consist of the color image, raw depth map, ground truth depth, segmentation mask as well as each object's 6DoF pose.
\end{itemize}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.35\textwidth]{datacapture/april_tag.pdf}
    \includegraphics[width=0.64\textwidth]{datacapture/frankasetup.png}
    \caption{\textbf{Dataset Collection Setup.} Left: Template with April Tags and alignment marker for object placement. The relative positions for markers and tags are fixed so transparent object pose can be calculated. Right: Franka robot and eye-in-hand RGB-D camera setup for dataset collection.}
    \label{fig:my_label}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{datacapture/backgorund.png}
    \caption{Four different color backgrounds used for April Tag templates in \dataName dataset }
    \label{fig:my_label}
\end{figure}




\subsection{Implementation Details}


The Point Cloud Completion module is trained with Adam optimizer\citep{kingma2017adam} with initial parameters $\alpha = 10^{-4}, \beta_1 = 0.9, \beta_2=0.999$. The learning rate decays by 2 after 50 epochs and training is stopped after 300 epochs. For training the decoder modulation depth completion module, we use the Adam optimizer \citep{kingma2017adam} with initial learning rate set to $10^{-4}$, $\beta_1 = 0.9$, $\beta_2 = 0.999$. We use the EfficientNet-b4 \citep{tan2020efficientnet} pretrained on ImageNet \citep{russakovsky2015imagenet} as the backbone. Experiments conducted on the ClearGrasp \citep{ClearGrasp} dataset are trained with a resolution of $240 \times 320$. Experiments conducted on our proposed dataset, \dataName, are trained with a resolution of $480 \times 640$. Training is performed end-to-end for 100 epochs, with early stopping.





\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{datacapture/objects.png}
    \caption{Six objects and corresponding 3D models used in \dataName, from left to right is Beaker 1, Beaker 2, Beaker 3, Flask 1, Flask 2 and Flask 3. From top to bottom is empty objects, objects filled with five different liquids and 3D models. }
    \label{fig:my_label}
\end{figure}
\subsection{Evaluating the Point Cloud Completion Module Separately}
In Table \ref{table:pcc}, we present an evaluation of the quality of our Point Cloud Completion model. $only-valid$ and $all$ refer to different considerations when computing the metrics. $only-valid$ refers to only calculating the metrics using points within the transparent object mask which are valid. This means that the non-valid depths as produced by the sparse Point Cloud Completion module is disregarded during $only-valid$ version of metrics calculation. $all$ refers to calculating the metrics using the entirety of the object mask, by considering non-valid depths of the Point Cloud Completion module as zero values. As a result, due to the sparsity of the Point Cloud, we can see that the $all$ version, which considers the invalid depths in the metric calculation, has significantly worse values than the $only-valid$ version. We can also see from the results of Table \ref{table:pcc} that the $only-valid$ version achieved relatively high scores across the board for the metrics shown. This means that the sparse depths that are output from the Point Cloud Completion module is indeed representative of the depth within the region of the transparent object, although relatively sparse. However, we note that due to the computation load required to increase the density of the point cloud output, we can only produce a relatively sparse point cloud from the Point Cloud Completion module, which cannot be directly used for downstream manipulation tasks. Hence, we add the Depth Completion (DC) module discussed in the main text.

\begin{table}[!t]
\centering
\caption{\textbf{Evaluation of PCC Module Performance.} Validation and Test set both contain non-overlapping novel scenarios, placements, and objects. $^*$ Metrics used are an adapted version that excludes empty depths. }
\label{table:pcc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \toprule
    \rowcolor[HTML]{CBCEFB}
% \begin{tabular}{|RMSE|MAE|$\delta_{1.05}$|$\delta_{1.10}$|$\delta_{1.25}$|$\delta_{1.25^2}$|$\delta_{1.25^3}$|SSIM|}
% 	\hline
	$\ $ & RMSE (\textdownarrow) & MAE (\textdownarrow) & $\delta_{1.05}$ (\textuparrow) & $\delta_{1.10}$ (\textuparrow) & $\delta_{1.25}$ (\textuparrow) & Rel (\textdownarrow)\\
% 	\hline \hline
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Validation} \\
	\midrule
% 	\rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp & 0.8715 & 0.3740 & 0.7126 & 0.7351 & 0.7701 & 0.1140\\
% 	\hline
	TanspareNet-PCC Only$^*_{\textrm{only-valid}}$ & 0.0115 & 0.0074 & 0.8949 & 0.9587 & 0.9969 & 0.0150\\
% 	\hline
	\rowcolor[HTML]{EFEFEF} 
	TanspareNet-PCC Only$_{\textrm{all}}$ & 0.2549 & 0.1820 & 0.4521 & 0.4785 & 0.4938 & 0.5146\\
% 	\hline
% 	TanspareNet-DC Only & 0.0184 & 0.0135 & 0.6951 & 0.8423 & 0.9594 & 0.0251 \\
% 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
% 	\algoName & \textbf{0.0174} & \textbf{0.0123} & \textbf{0.7714} & \textbf{0.9018} & \textbf{0.9018} & \textbf{0.0215}\\
% 	\hline
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Test} \\
	\midrule
% 	\rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp & 0.7734 & 0.3593 & 0.6884 & 0.7108 & 0.7466 & 0.1276\\
% 	\hline
	TanspareNet-PCC Only$^*_{\textrm{only-valid}}$ & 0.0118 & 0.0076 & 0.8874 & 0.9588 & 0.9953 & 0.0158\\
% 	\hline
	\rowcolor[HTML]{EFEFEF} 
	TanspareNet-PCC Only$_{\textrm{all}}$ & 0.2601 & 0.1887 & 0.4270 & 0.4547 & 0.4681 & 0.5400\\
% 	\hline
% 	TanspareNet-PCC Only & 0.0190 & 0.0137 & 0.6661 & 0.8956 & 0.9635 & 0.0314 \\
% 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
% 	\algoName & \textbf{0.0172} & \textbf{0.0122} & \textbf{0.7603} & \textbf{0.9047} & \textbf{0.9637} & \textbf{0.0251}\\
	\bottomrule
% 	$\ $ &  \multicolumn{6}{c}{Real-Known} \\
% 	\hline
% 	ClearGrasp & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	DMLRN & 0 & 0 & 0 & 0 & 0 & 0\\
% 	\hline
% 	$\ $ &  \multicolumn{6}{c}{Real-Unknown} \\
% 	\hline
% 	ClearGrasp & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	DMLRN & 0 & 0 & 0 & 0 & 0 & 0\\
% 	\hline
% 	\hline

\end{tabular}
}
\end{table}


\subsection{Additional Qualitative Results}

In Figure \ref{fig:transpare_success}, we present some additional qualitative examples of the performance of \algoName on \dataName. We note that \algoName is generally more capable at producing accurate depths along edges of glass vessels (Row 1). We can also see that in cases where the Depth Completion (DC) is inaccurate, along sides of vessels, \algoName is able to provide a more complete depth (Row 2). For cases where there are depth artifacts along depth discontinuities, \algoName is able to filter out the artifacts and generate a more realistic depth map where the outline of the respective vessels are artifact-free (Row 3, Row 6). 

In Figure \ref{fig:transpare_fail}, we show some failure cases of \algoName. Through analysis of the failure cases, we see that failures often occur when the point-cloud generated by the Point Cloud Completion (PCC) module is too sparse in nature, or if there is heavy occlusion of an object by another transparent object, such that the prominent edges of the object is non-visible.



\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{result_pic/good_cases.png} \\
\caption{\textbf{Visualization of performance on \dataName dataset.} From left to right, (a) RGB image (b) Raw depth from RGB-D sensor (c) PCC predicted depth (d) DC predicted depth (e) \algoName predicted depth (f) Ground Truth.
}
\label{fig:transpare_success}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{result_pic/bad_cases.png} \\
\caption{\textbf{Visualization of failure cases on \dataName dataset.} From left to right, (a) RGB image (b) Raw depth from RGB-D sensor (c) PCC predicted depth (d) DC predicted depth (e) \algoName predicted depth (f) Ground Truth.
}
\label{fig:transpare_fail}
\end{figure}

\subsection{Pose Estimation}
RGB-D information is vital for downstream robotic manipulation tasks, especially for 3D object 6DoF pose estimation. We demonstrate the importance of correct and complete depth information for pose estimation by comparing results on sensor raw depth, ground truth depth, and predicted depths from \algoName. For the comparison, MaskedFusion\citep{maskedfusion}, a state-of-the-art 6DoF pose estimation network, is trained on \dataName with instance segmentation masks, depth and RGB images. MaskedFusion is based on DenseFusion\citep{densefusion}, but includes additional mask branch in their PoseNet\citep{maskedfusion} and crops out corresponding image and depth based on object's mask. 


\begin{table}[!b]
    \centering
    \caption{\textbf{Pose Estimation}: Quantitative evaluation of MaskedFusion~\cite{maskedfusion} 6DoF pose using the ADD metric on the
\dataName dataset using raw, ground truth and \algoName predicted depth.}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{c|c|c|c|c|c|c|c|}
    \toprule
    \rowcolor[HTML]{CBCEFB}
         & Beaker 1 & Beaker 2 & Beaker 3 & Flask 1 & Flask 2 & Flask 3  & Average\\
    \midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{7}{c}{ADD (\textdownarrow)} \\
    \midrule 
    
    \rowcolor[HTML]{EFEFEF} 
        RGB + Raw Depth & 0.03251 & 0.01067 & 0.008682 & 0.01001 & 0.01016 & 0.007643 & 0.01578\\
        RGB + GT Depth & 0.01913 & 0.006411 & 0.004913 & 0.01097 & 0.009505 & 0.005148 & 0.01187 \\
    \rowcolor[HTML]{EFEFEF} 
        RGB + TranspareNet Depth & 0.01925 & 0.006553 & 0.005231 & 0.01091 & 0.009764 & 0.005406 & 0.01209 \\
    \midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{7}{c}{$<$2cm (\textuparrow)} \\
    \midrule
    \rowcolor[HTML]{EFEFEF} 
       RGB + Raw Depth & 51.79 & 96.65 & 98.93 & 95.61 & 97.66 & 99.16 & 85.09\\
      
       RGB + GT Depth & 74.74 & 98.61 & 98.91 & 91.67  &  98.31 & 99.08 & 88.24\\
    \rowcolor[HTML]{EFEFEF}  
       RGB + TranspareNet Depth & 74.78 & 98.71 &  99.24 & 91.61 & 98.02 & 99.75 & 88.14 \\
    \bottomrule
    \end{tabular}
    }
    
    \label{tab:masked_fusion}
\end{table}




 \textbf{Metrics: } Following MaskedFusion \citep{maskedfusion}, the Average Distance of Model Points (ADD) is used to evaluate distance between ground truth model and predicted model. We denote the ground truth rotation $R$ and translation $t$ and the estimated rotation $\hat{R}$ and translation $\hat{t}$. The average distance
is calculated using the mean of the pairwise distances between the 3D model points of the ground truth pose and the estimated pose. The percentage of objects whose ADD is less than 2cm is used as another metric. 
\begin{equation}
    ADD = \frac{1}{m}\sum_{x \in M} | (Rx+t) - (\hat{R}x +\hat{t})|
\end{equation}

We trained three versions of MaskedFusion on \dataName with raw, ground truth and \algoName depth respectively, each model is trained for 50 epochs and trained with Adam optimizer\citep{kingma2017adam} with initial parameters $\alpha = 10^{-4}, \beta_1 = 0.9, \beta_2=0.999$. Table \ref{tab:masked_fusion} demonstrates the effectiveness of \algoName's, with the performance of MaskedFusion trained on \algoName predicted depth closely matches with the version using ground truth depth as input. Comparing both models with the raw depth one, the importance of correct depth estimation is highlighted, as distorted and incomplete raw depth reported by RGB-D sensor significantly deteriorates MaskedFusion's accuracy. 


Our work is directly relevant to perception-based closed-loop manipulation. Most manipulation techniques use point clouds as the perceptual input which is useful for object pose estimation and sim-to-real transfer \citep{mousavian20196dof, qin2019s4g, zhao2021regnet, yan2019dataefficient}. Point clouds are deprojected from raw depth maps taken from sensors, so the quality of the depth map directly affects the input for manipulation. In previous works,  \citep{mahler2017dexnet} uses higher quality cameras and \citep{pas2017grasp} observer scenes from multi-viewpoints to deal with sensor noise and occlusion. Recently,  \citep{qin2019s4g, zhao2021regnet} take partial point clouds as input and train a multi-stage network on synthetic data. To simulate sensor noise, they employ various data-jittering methods on simulated point clouds. However, these methods assume objects in constructed scenes are opaque and overall geometrical information is preserved in point clouds. For transparent objects, ToF / stereo-based depth sensors cannot accurately capture the depth of transparent and specular objects. \autoref{fig:transparent_error} illustrates the quality of the depth estimation by an Intel Realsense Camera where the depth information of the glass beaker is mostly missed or incorrectly captured. 

According to ClearGrasp report, the effect of an inaccurate depth map on downstream manipulation tasks has been quantified to 12\% grasping success rate before any depth estimation methodologies for a parallel jaw gripper, and 64\% for suction. This shows the importance of accurate depth estimation for handling transparent objects. 
 
\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{depth_comparev3.png} \\
\caption{\textbf{Distortion in Transparent Object Depth.} From left to right, (a) RGB image, (b) Raw Depth and Ground Truth Depth, (c) Incorrect depth measurement due to reporting background depth as transparent object depth, (d) Predicted Depth from \algoName}
\label{fig:transparent_error}
\end{figure}

\subsection{Evaluation of Surface Normals}
In evaluating surface normals, we use the same metrics as that of ClearGrasp \citep{ClearGrasp}. We compute the mean and median errors of the predicted vectors, as compared to ground truth over all pixels within the image. We also report percentages of the predicted normals which are within $11.25^{\circ}$, $22.5^{\circ}$, and $30^{\circ}$ of that of the ground truth normals. Note that we mask the pixels which include transparent vessels for metric computation.

Because we do not directly predict normals within our model, we compute surface normals from depth estimations. We consider the depth image to be a function of $z(x,y)$, where $x$ is along the horizontal axis, $y$ is along the vertical axis, and $z$ denotes the depth (in metres).

The orthogonal vectors tangent to the plane parallel to the $x$ and $y$ axis can then be represented as $(1, 0, dz/dx)$ and $(0,1,dz/dy)$, respectively. Taking the cross product of these vectors, we arrive at the vector which represents the surface normal $(-dz/dx, -dz/dy, 1)$.

We present an evaluation of the quality of estimated normals based on angular difference with the estimated ground truth normals in Table \ref{tab:Normals}. We also provide a visualization of estimated normals in Figure \ref{fig:normals}.
% To compute an estimate of the surface normal, we make the assumption that the horizontal FoV is $0.5 m$. This means that for an image of $640 \times 416$, a single pixel occupies $0.003125 m$. The normal to the surface is then $(-dz)$
% \begin{equation}
    
% \end{equation}

\begin{table}
\centering
\caption{\textbf{Evaluation of Surface Normals.} We present an assessment of normals estimated from the predicted depth from TranspareNet.}
\label{t3}
\resizebox{0.75\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c}
    \toprule
    \rowcolor[HTML]{CBCEFB}
% \begin{tabular}{|RMSE|MAE|$\delta_{1.05}$|$\delta_{1.10}$|$\delta_{1.25}$|$\delta_{1.25^2}$|$\delta_{1.25^3}$|SSIM|}
% 	\hline
	$\ $ & Mean (\textdownarrow) & Median (\textdownarrow) & $\delta_{11.25 ^{\circ}}$ (\textuparrow) & $\delta_{22.5 ^{\circ}}$ (\textuparrow) & $\delta_{30 ^{\circ}}$ (\textuparrow) \\
% 	\hline \hline
	\midrule
% 	\rowcolor[HTML]{FBE0CB}
% 	$\ $ &  \multicolumn{5}{c}{Novel 1 Object Scene} \\
% 	\midrule
	\rowcolor[HTML]{EFEFEF}
% 	Raw &  31.72863124522087 &  27.843129016346257
%  & 19.04540539779886 & 45.08973853132256 & 58.12953424912863 \\
	
	Novel 1 Object Scene &  17.32 &  14.47
& 44.35 & 71.99 & 82.81 \\

    \midrule
% 	\rowcolor[HTML]{FBE0CB}
% 	$\ $ &  \multicolumn{5}{c}{Novel 2 Object Scene} \\
% 	\midrule
% 	\hline
% 	\rowcolor[HTML]{EFEFEF}
% 	Raw &  31.72863124522087 &  27.843129016346257
%  & 19.04540539779886 & 45.08973853132256 & 58.12953424912863 \\
 
	Novel 2 Object Scene & 17.01 & 13.92 & 46.14 & 72.12 & 82.62 \\
	\midrule
% 	\rowcolor[HTML]{FBE0CB}
% 	$\ $ &  \multicolumn{5}{c}{Novel 2 Object Scene} \\
% 	\midrule
% 	\rowcolor[HTML]{EFEFEF}
% 	Raw &  31.72863124522087 &  27.843129016346257
%  & 19.04540539779886 & 45.08973853132256 & 58.12953424912863 \\
 
    \rowcolor[HTML]{EFEFEF} 
    Novel 3 Object Scene & 19.45 & 15.75 & 39.14 & 65.74 & 77.81 \\
    \midrule
    Novel Objects Combined & 18.57 & 15.14 & 41.54 & 68.17 & 79.70 \\
	
    
% 	\algoName (ours) & \textbf{0.0166} & \textbf{0.0140} & \textbf{0.7133} & \textbf{0.9299} & \textbf{0.9945} & \textbf{0.0398}\\
	
	
% % 	\hline
% 	\midrule
% 	\rowcolor[HTML]{FBE0CB}
% 	$\ $ &  \multicolumn{6}{c}{Novel 2 Object Scene} \\
% 	\midrule
% 	\rowcolor[HTML]{EFEFEF} 

% 	ClearGrasp \citep{ClearGrasp} &  0.0534 & 0.0433 & 0.3458 & 0.5414 & 0.8412 & 0.1455\\
	
% 	TranspareNet-PCC Only & 0.2477 & 0.1817 & 0.4139 & 0.4434 & 0.4561 & 0.5516 \\
%     \rowcolor[HTML]{EFEFEF} 
% 	TranspareNet-DC Only & 0.0212 & 0.0168 & 0.5954 & 0.8256 & 0.9874 & 0.0564 \\
% % 	\hline
% % 	\rowcolor[HTML]{EFEFEF} 
% 	\algoName (ours) & \textbf{0.0194} & \textbf{0.0159} & \textbf{0.6475} & \textbf{0.8693} & \textbf{0.9876} & \textbf{0.0496}\\
% 	\midrule
% 	\rowcolor[HTML]{FBE0CB}
% 	$\ $ &  \multicolumn{6}{c}{Novel 3 Object Scene} \\
% 	\midrule
% 	\rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp \citep{ClearGrasp} & 0.0612 & 0.0493 & 0.2985 & 0.4954 & 0.8361 & 0.1536 \\
% % 	\hline
% 	TranspareNet-PCC Only & 0.2659 & 0.1922 & 0.4275 & 0.4564 & 0.4724 & 0.5362 \\
% % 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
% % 	\hline
% 	TranspareNet-DC Only & 0.0250 & \textbf{0.0189} & \textbf{0.5902} & 0.8305 & 0.9866 & 0.0555 \\
% % 	\hline
% % 	\rowcolor[HTML]{EFEFEF} 
% 	\algoName (ours) & \textbf{0.0232} & 0.0190 & 0.5817 & \textbf{0.8408} & \textbf{0.9904} & \textbf{0.0546}\\
% 	\midrule
% 	\rowcolor[HTML]{FBE0CB}
% 	$\ $ &  \multicolumn{6}{c}{Novel Combined} \\
% 	\midrule
% 	\rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp \citep{ClearGrasp} & 0.0563 & 0.0455 & 0.3262 & 0.5233 & 0.8476 & 0.1435\\
% % 	\hline
% 	TranspareNet-PCC Only & 0.2584 & 0.1864 & 0.4354 & 0.4627 & 0.4767 & 0.5314 \\
% % 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
% % 	\hline
% 	TranspareNet-DC Only & 0.0232 & 0.0180 & 0.6010 & 0.8380 & 0.9879 & 0.0543\\
% % 	\hline
% % 	\rowcolor[HTML]{EFEFEF} 
% 	\algoName (ours) & \textbf{0.0213} & \textbf{0.0175} & \textbf{0.6180} & \textbf{0.8619} & \textbf{0.9905} & \textbf{0.0510}\\
% 	\bottomrule
% % 	$\ $ &  \multicolumn{6}{c}{Real-Known} \\
% % 	\hline
% % 	ClearGrasp & 0 & 0 & 0 & 0 & 0 & 0 \\
% % 	\hline
% % 	DMLRN & 0 & 0 & 0 & 0 & 0 & 0\\
% % 	\hline
% % 	$\ $ &  \multicolumn{6}{c}{Real-Unknown} \\
% % 	\hline
% % 	ClearGrasp & 0 & 0 & 0 & 0 & 0 & 0 \\
% % 	\hline
% % 	DMLRN & 0 & 0 & 0 & 0 & 0 & 0\\
% % 	\hline
% % 	\hline
\end{tabular}
}
\label{tab:Normals}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{supplemental_materials/normals.png}\\
\caption{\textbf{Visualization of normals estimated from depth on \dataName dataset.} From left to right, (a) RGB image (b) Normal estimated from raw depth from RGB-D sensor (c) Depth predicted by \algoName (d) Normals estimated from predicted depth (e) Normals estimated from ground truth depth.
}
\label{fig:normals}
\end{figure}

\subsection{Inference using non-perfect object masks}
In a real life deployment scenario, it is often the case where there is no segmentation mask to select the objects of interest for depth completion. Therefore, we present an evaluation for the inference ability of our model when the mask is predicted using the pre-trained transparent object segmentation model from \citep{TransLab}. Using the pre-trained model, we find that the predicted masks have an IoU of $0.484225$ with the ground truth masks. Using the predicted masks, we evaluate the performance of (1) \algoName Depth Completion Only, and (2) \algoName which involves Point Cloud Completion and Depth Completion, results are presented in Table \ref{tab:trans10k_mask_results}. We also show a visualization of the differences in estimated depths (when the mask is predicted), as compared to when the mask is ground truth in Figure \ref{fig:trans10k_mask}.


Note that this model was only trained using Trans10k. There is no fine-tuning on our dataset. We anticipate that fine-tuning the pre-trained Trans10k \citep{TransLab} on our dataset will yield improved predicted segmentation IoU, and hence improved depth completion by \algoName.

\begin{table}[!t]
\centering
\caption{\textbf{Inference using predicted object masks.} We assess the performance of various models with Novel 1 Object images when the given mask input is predicted by the pre-trained model in \citep{TransLab}. The arrows beside the metrics denote whether lower or higher values are more desired. }
\label{t3}
\resizebox{0.75\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \rowcolor[HTML]{CBCEFB}
% \begin{tabular}{|RMSE|MAE|$\delta_{1.05}$|$\delta_{1.10}$|$\delta_{1.25}$|$\delta_{1.25^2}$|$\delta_{1.25^3}$|SSIM|}
% 	\hline
	$\ $ & RMSE (\textdownarrow) & MAE (\textdownarrow) & $\delta_{1.05}$ (\textuparrow) & $\delta_{1.10}$ (\textuparrow) & $\delta_{1.25}$ (\textuparrow) & REL (\textdownarrow)\\
% 	\hline \hline
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Novel 1 Object Scene} \\
	\midrule
% 	\rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp \citep{ClearGrasp} &  0.0425 &  0.0367 & 0.4023 & 0.6018 & 0.8990 & 0.1073 \\
% % 	\hline
	
% 	TranspareNet-PCC Only & 0.2443 & 0.1713 & 0.4927 & 0.5113 & 0.5196 & 0.4877 \\
	
    \rowcolor[HTML]{EFEFEF} 
    TranspareNet-DC Only & 0.0446 & 0.0362 & 0.4064 & 0.6056 & 0.8511 & 0.0914\\
    
	\algoName (ours) & \textbf{0.0341} & \textbf{0.0279} & \textbf{0.4740} & \textbf{0.7056} & \textbf{0.9102} & \textbf{0.0598}\\
\end{tabular}
}
\label{tab:trans10k_mask_results}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{supplemental_materials/trans10k_seg.png}\\
\caption{\textbf{Visualization of differences in predicted depth when the given mask is predicted vs. ground truth.} From left to right, (a) RGB image (b) Ground Truth Mask (c) Depth predicted by \algoName with GT Mask (d) Predicted Mask (e) Depth predicted by \algoName with Predicted Mask.
}
\label{fig:trans10k_mask}
\end{figure}

\subsection{Assessment of the Importance of April Tags in Model Performance}
Because our method automates object pose extraction using April Tags \citep{apriltag2}, we hope to identify whether April Tags play a role in the performance of our depth completion module. To evaluate whether such a connection exists, we mask out all April Tags from the RGB-input during inference, and compare with the \algoName performance when the tag is not masked.

We present an evaluation in Table \ref{tab:tags_removed}. We also show a visualization of the differences in predicted depths with and without Tags removed in Figure \ref{fig:tags_removed}, as can be seen the visualization, the predicted depths with and without April Tag masked are similar, which shows that our model did not overfit to the positioning of the April Tags, and is learning useful insight based on the geometry the vessels.

\begin{table}[!t]
\centering
\caption{\textbf{Depth completion results with and without April Tags on \dataName Dataset.} We assess the performance of various models with Novel 1 Object images, as well as novel cluttered images with 2 or 3 objects. The arrows beside the metrics denote whether lower or higher values are more desired. }
\label{t3}
\resizebox{0.75\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \rowcolor[HTML]{CBCEFB}
% \begin{tabular}{|RMSE|MAE|$\delta_{1.05}$|$\delta_{1.10}$|$\delta_{1.25}$|$\delta_{1.25^2}$|$\delta_{1.25^3}$|SSIM|}
% 	\hline
	$\ $ & RMSE (\textdownarrow) & MAE (\textdownarrow) & $\delta_{1.05}$ (\textuparrow) & $\delta_{1.10}$ (\textuparrow) & $\delta_{1.25}$ (\textuparrow) & REL (\textdownarrow)\\
% 	\hline \hline
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Novel 1 Object Scene} \\
	\midrule
% 	\rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp \citep{ClearGrasp} &  0.0425 &  0.0367 & 0.4023 & 0.6018 & 0.8990 & 0.1073 \\
% % 	\hline
	
% 	TranspareNet-PCC Only & 0.2443 & 0.1713 & 0.4927 & 0.5113 & 0.5196 & 0.4877 \\
	
    \rowcolor[HTML]{EFEFEF} 
    Tags Removed & 0.0260 & 0.0230 & 0.3735 & 0.7451 & 0.9855 & 0.0614\\
    
	Tags Not Removed & 0.0166 & 0.0140 & 0.7133 & 0.9299 & 0.9945 & 0.0398\\
	
	
% 	\hline
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Novel 2 Object Scene} \\
	\midrule
% 	\rowcolor[HTML]{EFEFEF} 

% 	ClearGrasp \citep{ClearGrasp} &  0.0534 & 0.0433 & 0.3458 & 0.5414 & 0.8412 & 0.1455\\
	
% 	TranspareNet-PCC Only & 0.2477 & 0.1817 & 0.4139 & 0.4434 & 0.4561 & 0.5516 \\
    \rowcolor[HTML]{EFEFEF} 
	Tags Removed & 0.0288 & 0.0233 & 0.3611 & 0.6649 & 0.9665 & 0.0687 \\
% 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
	Tags Not Removed & 0.0194 & 0.0159 & 0.6475 & 0.8693 & 0.9876 & 0.0496\\
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Novel 3 Object Scene} \\
% 	\midrule
% 	\rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp \citep{ClearGrasp} & 0.0612 & 0.0493 & 0.2985 & 0.4954 & 0.8361 & 0.1536 \\
% % 	\hline
% 	TranspareNet-PCC Only & 0.2659 & 0.1922 & 0.4275 & 0.4564 & 0.4724 & 0.5362 \\
% 	\hline
	\rowcolor[HTML]{EFEFEF} 
% 	\hline
% 	Tags Removed & 0.0250 & \textbf{0.0189} & \textbf{0.5902} & 0.8305 & 0.9866 & 0.0555 \\
% 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
	Tags Removed & 0.0329 & 0.0282 & 0.3189 & 0.6332 & 0.9704 & 0.0739\\
	
	Tags Not Removed & 0.0232 & 0.0190 & 0.5817 & 0.8408 & 0.9904 & 0.0546\\
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Novel Combined} \\
	\midrule
% 	\rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp \citep{ClearGrasp} & 0.0563 & 0.0455 & 0.3262 & 0.5233 & 0.8476 & 0.1435\\
% % 	\hline
% 	TranspareNet-PCC Only & 0.2584 & 0.1864 & 0.4354 & 0.4627 & 0.4767 & 0.5314 \\
% 	\hline
	\rowcolor[HTML]{EFEFEF} 
% 	\hline
	Tags Removed & 0.0309 & 0.0271 & 0.3293 & 0.6452 & 0.9706 & 0.0707\\
% 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
	Tags Not Removed & 0.0213 & 0.0175 & 0.6180 & 0.8619 & 0.9905 & 0.0510\\
	\bottomrule
% 	$\ $ &  \multicolumn{6}{c}{Real-Known} \\
% 	\hline
% 	ClearGrasp & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	DMLRN & 0 & 0 & 0 & 0 & 0 & 0\\
% 	\hline
% 	$\ $ &  \multicolumn{6}{c}{Real-Unknown} \\
% 	\hline
% 	ClearGrasp & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	DMLRN & 0 & 0 & 0 & 0 & 0 & 0\\
% 	\hline
% 	\hline
\end{tabular}
}
\label{tab:tags_removed}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{supplemental_materials/no_tag.png}\\
\caption{\textbf{Visualization of differences in predicted depth when the April Tags is Masked out vs. Not Masked out.} From left to right, (a) RGB image (With Tag) (b) April Tag Mask (c) RGB image (Tags Removed) (d) Depth predicted by \algoName with Tag in RGB (e) Depth predicted by \algoName without Tag in RGB.
}
\label{fig:tags_removed}
\end{figure}

\subsection{Extended Test Set}
\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{datacapture/new_objects.JPG}\\
\caption{New transparent objects (bowl and cup) with different shapes for \dataName extended test set.
}
\label{fig:new_objects}
\end{figure}
We introduce an extended test set, consisting of a glass bowl and cup, as shown in \autoref{fig:new_objects}. The test set consists of 4k images of these objects arranged in different settings. We provide an evaluation of \algoName on the extended test set. Results are shown in Table \ref{tab:new_test_results}. We further provide visualizations for a representative sample of the extended test set, and the quality of the predicted depth in Figure \ref{fig:visualization_pred_new_obj}. 



Additionally, our automated dataset creation pipeline allows users to customize the dataset for their intended application scenario by rapidly collecting and annotating images for the transparent objects in the scene. For example, the additional images only took 8 hours to collect and annotate automatically, in contrast with ClearGrasp \citep{ClearGrasp}, which involved manual placement, capture, and annotation of the small (~200) real-life test set. Although the existing TODD dataset is limited in object types and scenes, we think the pipeline provides a feasible approach to tailor the dataset for specific applications.


\begin{table}[!t]
\centering
\caption{\textbf{Inference on Extended Test Set.} We assess the performance of various models using the extended Test set. The arrows beside the metrics denote whether lower or higher values are more desired. }
\label{t3}
\resizebox{0.75\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \rowcolor[HTML]{CBCEFB}
% \begin{tabular}{|RMSE|MAE|$\delta_{1.05}$|$\delta_{1.10}$|$\delta_{1.25}$|$\delta_{1.25^2}$|$\delta_{1.25^3}$|SSIM|}
% 	\hline
	$\ $ & RMSE (\textdownarrow) & MAE (\textdownarrow) & $\delta_{1.05}$ (\textuparrow) & $\delta_{1.10}$ (\textuparrow) & $\delta_{1.25}$ (\textuparrow) & REL (\textdownarrow)\\
% 	\hline \hline
	\midrule
% 	\rowcolor[HTML]{FBE0CB}
% 	$\ $ &  \multicolumn{6}{c}{Novel 1 Object Scene} \\
% 	\midrule
% 	\rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp \citep{ClearGrasp} &  0.0425 &  0.0367 & 0.4023 & 0.6018 & 0.8990 & 0.1073 \\
% % 	\hline
	
% 	TranspareNet-PCC Only & 0.2443 & 0.1713 & 0.4927 & 0.5113 & 0.5196 & 0.4877 \\
	
    \rowcolor[HTML]{EFEFEF} 
    TranspareNet-DC Only & \textbf{0.0349} & \textbf{0.0308} & 0.3428 & 0.5580 & \textbf{0.9228} & \textbf{0.0808}\\
    
	\algoName (ours) & 0.0363 & 0.0335 & \textbf{0.4236} & \textbf{0.5702} & 0.8417 & 0.0868 \\
\end{tabular}
}
\label{tab:new_test_results}
\end{table}


\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{supplemental_materials/new_val.png}\\
\caption{\textbf{Visualization of Extended Test Set.} The extended test set contains new vessels, the shapes of which have not been exposed during the training process. From left to right, (a) RGB image (b) Raw Depth (c) Depth predicted by \algoName-DC Only (d) Depth predicted by \algoName (e) Ground-truth Depth
}
\label{fig:visualization_pred_new_obj}
\end{figure}

\clearpage
% \bibliography{example} 
% \end{document}`

% \textbf{Visualization of prediction results on \dataName dataset.} From left to right, (a) RGB image (b) Raw depth from RGB-D sensor (c) PCC predicted depth (d) DC predicted depth (e) \algoName predicted depth (f) Ground Truth. We mark the major difference between DC and \algoName predictions with bounding boxes.