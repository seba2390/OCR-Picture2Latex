\documentclass{article}

%  \usepackage{corl_2021} % Use this for the initial submission.
\usepackage[final]{corl_2021} % Uncomment for the camera-ready ``final'' version.
%\usepackage[preprint]{corl_2021} % Uncomment for pre-prints (e.g., arxiv); This is like ``final'', but will remove the CORL footnote.

\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{pdfpages}
\usepackage{caption}
\usepackage{hyperref}
\newcommand{\cmark}{\ding{52}}%
\newcommand{\xmark}{\ding{56}}%
% \newcommand{\datasetName}{TTO}
\usepackage[table,xcdraw]{xcolor}
\usepackage{enumitem}
\usepackage[font=small,labelfont=bf]{caption}
% \usepackage{cite}
\usepackage{xspace}
\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% space tweaks 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\abovecaptionskip}{0.5mm}
\setlength{\belowcaptionskip}{0.5mm} 
\setlength{\textfloatsep}{1.5mm}
\setlength{\dbltextfloatsep}{1.5mm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% document details
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newcommand{\algoName}{TranspareNet\xspace}
\newcommand{\algoName}{TranspareNet\xspace}
% \newcommand{\dataName}{T\textsuperscript{2}OAD\xspace}
\newcommand{\dataName}{TODD}
\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}
% \title{\algoName: Joint Point-Cloud and Depth Completion for Transparent Objects}
\title{Seeing Glass: Joint Point Cloud and Depth Completion for Transparent Objects}
% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

% NOTE: authors will be visible only in the camera-ready and preprint versions (i.e., when using the option 'final' or 'preprint'). 
% 	For the initial submission the authors will be anonymized.

\author{
Haoping Xu$^{1,2}$\thanks{Authors contributed equally} , %
Yi Ru Wang$^{1,2}$\footnotemark[1] , %
Sagi Eppel$^{1}$, %
Al\`{a}n Aspuru-Guzik$^{1,2}$, %
\\
\textbf{Florian Shkurti$^{1,2}$, %
Animesh Garg$^{1,2,3}$} \\
$^{1}$University of Toronto, 
$^{2}$Vector Institute,  
$^{3}$Nvidia\\
\texttt{\{\href{mailto:haoping.xu@mail.utoronto.ca}{haoping.xu},
\href{mailto:yiruhelen.wang@mail.utoronto.ca}{yiruhelen.wang}\}@mail.utoronto.ca} \\
\texttt{\href{mailto:sagieppel@gmail.com}{sagieppel}@gmail.com, \href{mailto:aspuru@utoronto.ca}{aspuru}@utoronto.ca},
\texttt{
\{\href{mailto:florian@cs.toronto.edu}{florian}, \href{mailto:garg@cs.toronto.edu}{garg}\}@cs.toronto.edu}}


\begin{document}
\maketitle

%===============================================================================

\begin{abstract}
    The basis of many object manipulation algorithms is RGB-D input. Yet, commodity RGB-D sensors can only provide distorted depth maps for a wide range of transparent objects due light refraction and absorption. To tackle the perception challenges posed by transparent objects, we propose TranspareNet, a joint point cloud and depth completion method, with the ability to complete the depth of transparent objects in cluttered and complex scenes, even with partially filled fluid contents within the vessels. To address the shortcomings of existing transparent object data collection schemes in literature, we also propose an automated dataset creation workflow that consists of robot-controlled image collection and vision-based automatic annotation. Through this automated workflow, we created Toronto Transparent Objects Depth Dataset (\dataName), which consists of nearly 15000 RGB-D images. Our experimental evaluation demonstrates that TranspareNet outperforms existing state-of-the-art depth completion methods on multiple datasets, including ClearGrasp, and that it also handles cluttered scenes when trained on \dataName. Code and dataset will be released at \url{https://www.pair.toronto.edu/TranspareNet/}
    
% \blfootnote{\{\href{mailto:haoping.xu@mail.utoronto.ca}{haoping.xu},
% \href{mailto:yiruhelen.wang@mail.utoronto.ca}{yiruhelen.wang}\}@mail.utoronto.ca, \{\href{mailto:florian@cs.toronto.edu}{florian}, \href{mailto:garg@cs.toronto.edu}{garg}\}@cs.toronto.edu}
    
    % \href{https://anonymous.4open.science/r/TranspareNet-567C/README.md}{Anonymous Github repo}
    
    % with minimal manual intervention. 

%   The basis of many object manipulation algorithms is RGB-D input. Yet, transparent objects possess unique visual properties that distort the depths captured by commodity RGB-D sensors. To tackle the perception challenges posed by transparent objects, we propose TranspareNet, a joint point cloud and depth completion method, with the ability to complete the depth of transparent objects in cluttered and complex scenes, with and without fluid contents within the vessels. To address the shortcomings of existing transparent object data collection schemes in literature, we also propose an automated dataset creation workflow that consists of robot-controlled image collection and vision-based automatic annotation. Through this automated workflow, we created Transparent Object Depth Dataset (\dataName), which consists of nearly 15000 RGB-D images. Experiments demonstrate that our method outperforms existing state-of-the-art depth completion methods, namely on the ClearGrasp dataset, and that it handles cluttered scenes when trained on \dataName.
    
    
    
\end{abstract}

% Two or three meaningful keywords should be added here
\keywords{Transparent Objects, Depth Completion, 3D Perception, Dataset} 

%===============================================================================

\section{Introduction}

RGB-D sensors have been instrumental in 3D perception for robotics. While reliable for opaque objects, commodity-level RGB-D sensors often fail when capturing the depth of transparent objects, made of materials such as glass or clear plastic. This is because transparent objects possess unique visual properties that distort the captured depth. Specular surface reflections introduce gaps in the depth map, while depth projection to surfaces behind the transparent object, as shown in \autoref{fig:transparent_error}, introduce inaccurate depth estimates. Nonetheless, transparent objects are common in our homes and daily lives, from kitchens and dining rooms to laboratory settings. Perception of these objects is under-explored, yet vital for robotic systems that operate in unstructured human-made environments.

% as in the case of specular surface reflections (Type I Error) \citep{ClearGrasp}, or by introducing noise with inaccurate depths, caused by projection to surfaces behind the transparent object of interest (Type II Error) \citep{ClearGrasp}, as shown in Fig.\ref{fig:transparent_error}. 

In this work, we present a method to leverage real complex transparent object depth to estimate accurate 3D geometries of transparent objects. The design is motivated by three main ideas.
First, although the depth information for transparent objects captured by RGB-D sensors is noisy by nature, there remains useful components that can be leveraged by downstream depth-completion tasks. The previous state-of-the-art method and dataset, ClearGrasp \citep{ClearGrasp}, masks out all depths at the location of the transparent object, renders them invalid, and conducts global optimization with surface normal and boundaries to reconstruct its geometry. This is sub-optimal, as it ignores all depth information at the transparent object's location. Ours leverages the unique depth distortion at the location of transparent objects to generate a point cloud distribution using the Point Cloud Completion module, which is then fed through a Depth Completion module to generate the complete depth map. 
Second, existing large-scale datasets for transparent objects are either synthetic \citep{ClearGrasp}, too simple and without clutter \citep{Keypose}, or lack depth information \citep{TransLab}. There remains a lack of datasets for transparent objects in a real-world setting with clutter and content within. Therefore, we introduce the novel Toronto Transparent Objects Depth Dataset (\dataName) which is comparable in scale to existing transparent object datasets, and captures the realistic state that transparent objects are found in everyday life, including clutter, which is absent from previous datasets. 
Finally, collecting a dataset of transparent objects has either involved synthetic generation \citep{ClearGrasp}, which has a significant and often insurmountable domain gap with real world data, or manual placement and capture, which is too difficult to scale up in size. Therefore, we design an automated pipeline for dataset collection, which can capture and annotate RGB-D data, including the corrected depth for transparent objects.
    
    % Training of such a model would require access to ground truth surface normal and boundary information, which is not trivial to obtain for real images. Instead, our method leverages the valid depths at the location of the Transparent object and estimates a sparse point cloud using the mask and raw depth. The point cloud estimated contains a sparse distribution of the transparent object depth, which is then fed through a depth completion module to generate the complete depth-map. 
    % Smth about importance of leveraging existing depth and not mask it out completely
    % \item Smth about importance of real world data 
    % \item Smth about how collection of data is so difficult therefore we want to automate it


Our primary contributions are threefold: 
\begin{enumerate}[noitemsep,labelwidth=!,labelindent=0pt]
% \begin{enumerate}[noitemsep, wide, labelwidth=!, labelindent=0.pt]
    \item We introduce the Toronto Transparent Objects Depth Dataset (\dataName), a large-scale real transparent object dataset with around 15,000 RGB-D observations that contain RGB, raw depths paired with ground truth depth (in which the depth at the transparent object is complete), instance segmentation, and object pose. To our knowledge, our dataset is the first large-scale transparent object dataset that contains complex scenes with clutter, and contains transparent objects in a realistic setting with fluid content within the vessels.
    \item We introduce a scalable automatic dataset collection and annotation scheme for the collection of RGB-D images of transparent objects, with fully automatic labeling of ground truth depth, 6DoF pose, as well as instance segmentation.
    \item We introduce a novel depth completion method, \algoName, that achieves state-of-the-art performance on the existing ClearGrasp dataset \citep{ClearGrasp}, and benchmark its performance on our dataset, \dataName. \algoName is a joint point cloud and depth completion method that leverages RGB and depth signals of transparent objects.  
\end{enumerate}
\begin{figure}[t]
\centering
\includegraphics[width=0.9\textwidth]{depth_comparev3.png} \\
\caption{\textbf{Distortion in Transparent Object Depth.} From left to right, (a) RGB image, (b) Raw Depth and Ground Truth Depth, (c) Incorrect depth measurement due to reporting background depth as transparent object depth, (d) Predicted Depth from \algoName}
\label{fig:transparent_error}
\end{figure}
% Motivated by the absence of automated annotation pipelines 


% Things to cover in intro:
% - Importance of the task: Transparent objects are very common; needed for tasks like dish washing or sorting/cleaning plastic containers.
% - Visual properties of transparent objects: difficult to perceive by commodity RGBD sensors
%         - Go into detail on what are the visual properties: Refractive, specular, link to figure 1
% - Ideas that drives design of network:
% - Primary contributions:
%     - We introduce TODs, a large-scale transparent objects dataset that contains RGB, raw depths paired with ground truth depth, instance segmentation, and object pose. To our knowledge, our dataset is the first large-scale transparent object dataset that contains complex setups with clutter in a realistic setting (with and without different liquids within). 
%     - We introduce a novel automatic dataset collection and annotation pipeline for manipulation in a table-top setting.
%     - We introduce a novel depth completion method, TranspareNet, that achieves SOTA performance on existing transparent object completion datasets, and benchmark its performance on TODs.

% Problem Statement \\
% Object manipulation typically requires reliable depth estimations to enable successful grasp generations. However, for some objects which are transparent and specular in material, commercial RGB-D sensors are unable to capture reliable depths for the objects. Therefore, depth estimation is a crucial component of transparent object manipulation. Existing datasets are either synthetically generated, or do not contain ground truth depth information, as summarized in Table X. There lacks datasets which provide real life depth + ground truth depth on glassware objects viewed at different angles and settings to enable reliable real-world depth completion.




% Motivation and Impact \\
% Reliable depth completion in a real-world setting needs real-world data from a similar domain to minimize the domain gap between training and test scenarios. Real-life dataset curation for transparent objects remains a relatively unexplored area, due to the difficulty in associating ground truth depth data with the raw sensor depth data. A large-scale real-life transparent-objects dataset would accelerate research geared towards depth completion and automating dataset collection would enable researchers to produce data that is tailored to their own table-top object manipulation setting. \\

% Our contributions can be summarized as follows:
% \begin{itemize}
%     \item We propose an automated data collection and annotation pipeline that enables for real-life dataset generation in a table top setting, which pairs raw sensor depth with corrected ground truth depth of objects.
%     \item We propose a dataset, \textbf{INSERT DATASET NAME HERE}, which consists of 15000 RGBD images of transparent objects, with matching ground truth depth and segmentation. 
%     % To our knowledge, this is the first large-scale real-life transparent object dataset with depth. 
%     \item We demonstrate the importance of our dataset for real-life manipulation of transparent objects by benchmarking our dataset with the SOTA depth completion models.
% \end{itemize}

%===============================================================================
\section{Related Work}
\textbf{Transparent Object Segmentation.} Transparent objects' refractive and reflective nature, as well as changing appearance in various scenes due to background, makes detecting them a challenging task in computer vision. There have been several works that applies state-of-the-art segmentation methods \citep{he2018mask,chen2017rethinking,carion2020endtoend} to transparent object segmentation \citep{LabPic, ClearGrasp, xie2021segmenting}. However, due to the significant domain gap between opaque and transparent objects, all of these aforementioned methods were trained on custom transparent object datasets. Due to the cost associated with image collection and annotation, many synthetic datasets have been proposed, including TOM-Net \citep{chen2018tomnet}, ClearGrasp \citep{ClearGrasp}, and Omni \citep{zhu2021rgbd}\footnote{Concurrent work, dataset is not released}. While real world transparent object datasets, like Trans10K \citep{xie2021segmenting} and LabPic \citep{LabPic}, avoid the potential domain gap between rendered and real images, they are time-consuming and expensive to collect and annotate. LIT \citep{zhou2020lit} utilizes a special light-field sensor to detect transparent objects and estimate their poses. Our proposed automated dataset collection pipeline enables large scale, automatic annotation of transparent objects using common RGB-D sensor. 


% Advances in machine learning research provide candidate models to predict transparent objects' bounding boxes and masks. Networks with outstanding performance in regular object detection tasks, like Mask RCNN \citep{he2018mask}, Deeplab V3 \citep{chen2017rethinking} and Transformer\citep{carion2020endtoend}, have been used to handle transparent objects \citep{LabPic, ClearGrasp, xie2021segmenting}. However, we note that due to the significant domain gap between opaque and transparent objects, the aforementioned models were 

% However, the significant domain gap between opaque and transparent objects meant that the above mentioned models need to be retrained on datasets dedicated to glass-like objects.

%  constrained by the time and finance requirement and are still not on par with modern image datasets like COCO \citep{lin2015microsoft} and ImageNet \citep{russakovsky2015imagenet}.

\begin{figure}[t!]
\centering
% \includegraphics[width=0.9\textwidth]{fig1 v3.pdf}
\includegraphics[width=0.9\textwidth]{fig1_v4.pdf}
\caption{\textbf{Overview of the proposed \algoName.} Transparent object depth is de-projected to a point cloud, and put through Point Cloud Completion module to get the final point cloud. The final point cloud is then projected to the depth domain and replaces the original depth of the transparent object within the mask. An encoder-decoder based Depth Completion module takes combined depth and RGB signal as input, and the decoder modulation branch takes the object mask as input for modulation. Finally, the decoder outputs the predicted completed depth.}
\label{fig:architecture}
\end{figure}

\textbf{Depth Completion.} Depth completion refers to the task of generating a dense depth map from an incomplete depth map due to sparse measurements, noise, or sensor limitations. In the context of our work, we use depth completion to describe the task of completing and refining noisy depths captured by RGB-D sensors for transparent objects. Works in this area generally fall in four main schemes: constraints driven, monocular depth estimation, depth completion from sparse point cloud, and depth completion given noisy RGB-D. Our work falls in the last scheme. Constraint driven approaches assume a specific setup method with fixed viewpoint(s) and capturing procedure \cite{7299026,7780842,s20236790,Albrecht2013SeeingTU,fusingdepth}, sensor type \cite{song2017,song2018depth,zhou2020lit}, or known object models \citep{6630571,Phillips2016SeeingGF,Klank}. Our proposed depth completion method does not apply any assumptions. Monocular depth estimation refers to the direct regression of depth from RGB input \cite{eigen2014depth,ranftl2020robust,laina2016deeper,chen2017singleimage,garg2016unsupervised,godard2017unsupervised,xu2017multiscale,hao2018preserving,xu2018structured,fu2018deep,hu2018revisiting,huynh2020guiding,lee2020big,alhashim2019high}. This family of works generally require access to large-scale RGB-D datasets, for which the depth reflects objects depicted in the RGB image. Historically, this area has not been explored for transparent object handling, due to the absence of large scale transparent data paired with noise-free depth. Our proposed dataset and data collection and annotation approach can now facilitate the generation of sufficient data to support research in this direction. The third scheme, depth completion from sparse depth data, refers to the conversion of a sparse point cloud to a dense depth map via deep learning methods. Works in this area concentrate on applications involving LiDAR for outdoor environments \cite{cheng2019cspn, uhrig2017sparsity,tang2019learning,cheng2020s3cnet}, and are inherently different from the semi-sparse and noisy signals from RGB-D sensors for transparent objects. There are two main lines of work in depth refinement of noisy RGB-D data of transparent objects. ClearGrasp \citep{ClearGrasp} takes an optimization approach and estimates surface normals, occlusion boundary, and segmentation to perform a global optimization for depth estimation. More recently, \citep{zhu2021rgbd} proposed a voxel based local implicit neural function for depth estimation. We show that our method outperforms both of these methods. The key insight that contributed to the success of our method is that we leverage the unique distortion in the depth caused by transparency to generate a coarse estimation of object depth through point cloud completion, and conduct depth completion for refinement of the sparse point cloud. Previous methods like ClearGrasp \citep{ClearGrasp} discard this information.

\textbf{Prior Transparent Dataset Collection and Annotation}. Trans10K\citep{xie2021segmenting} is a 2D transparent object segmentation dataset that consists of real images of transparent objects and their semantic segmentation masks. All of its images and labels are captured and annotated manually. \citet{doi:10.1177/0278364917734052} proposed a dataset of transparent liquid in opaque containers, and consists of rendered and real-world images. The captured images' segmentation masks are annotated via thermal cameras in combination with heated liquid. For transparent object datasets with depth information, ClearGrasp \citep{ClearGrasp} uses a synthetic dataset generated by Blender as the training set, and collects the small scale real-world validation and testing dataset using matched transparent and opaque objects. This process requires manual placement and matching of transparent and opaque pairs for each generated image. Another 3D dataset, Keypose \citep{Keypose}, collects its dataset using a eye-in-hand robot arm and tracking system based on AprilTag 2 \citep{apriltag2}. The camera path is calculated using AprilTag, and along the scan trajectory, several frames' keypoints are manually labelled, where the rest are labeled based on the trajectory. The depth info is collected using a similar replacing method as ClearGrasp \citep{ClearGrasp}. Such replacing method needs human intervention to align the opaque and transparent twins through image overlay, which can be hard and inaccurate especially in complex scenes.  Furthermore, since each image in ClearGrasp \citep{ClearGrasp} is manually captured, opaque swapped and annotated, it cannot scale to a larger dataset size due to time and labour constraints. Our method overcomes the manual effort that previous methods used for dataset collection with an automated pipeline for dataset creation and annotation.
\begin{figure}[t!]
\centering
\includegraphics[width=0.9\textwidth]{franka_2_v3.png} \\

\caption{\textbf{Dataset creation pipeline.} A commodity RGB-D sensor is mounted to the robot arm's end effector. The scene with the transparent object is scanned from multiple viewing angles to collect the raw depth (left). AprilTags on the base template are detected for each image, and based on their 6DoF poses and the known translation between tags and objects, we can fit the 3D model of object(s) to their respective locations (middle). The result of this automatic collection and annotation process is the RGB image, instance object segmentation, raw depth, ground truth depth (right).}
\label{fig:dataset_creation}
\end{figure}

%===============================================================================
\section{Toronto Transparent Objects Depth Dataset (\dataName)}

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sample_picsv2/image.jpg}\\
    % \caption{}
\end{subfigure}%
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sample_picsv2/mask_1.png} \\
    % \caption{}
\end{subfigure}%
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sample_picsv2/depth_1.jpg} \\
    % \caption{}
\end{subfigure}%
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sample_picsv2/detph_GroundTruth_1.jpg} \\
    % \caption{}
\end{subfigure}%
% \medskip

\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sample_picsv2/image2.jpg}\\
    % \caption{}
\end{subfigure}%
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sample_picsv2/mask2.png} \\
    % \caption{}
\end{subfigure}%
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sample_picsv2/depth2.jpg} \\
    % \caption{}
\end{subfigure}%
\begin{subfigure}[b]{0.21\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sample_picsv2/detph_GroundTruth2.jpg} \\
    % \caption{}
\end{subfigure}%


\caption{\textbf{Samples from the proposed dataset.} (a) RBG image (b) Instance Segmentation (c) Raw depth from RGB-D sensor (d) Ground truth depth obtained through automatic depth annotation. The dataset also includes 6DoF object pose information, which is not depicted in the figure.}
\label{fig:dataset_inference}
\end{figure}
We use an eye-in-hand Franka Emika Panda controlled by FrankaPy\citep{frankapy} with Intel Realsense RGB-D camera to collect the dataset. As shown in \autoref{fig:dataset_creation}, the pipeline consists of multiple steps. First, the end-effector of the robot arm moves the camera to multiple positions around the transparent object(s), while ensuring focus on the transparent object. Templates with AprilTags \citep{apriltag2} and alignment marks are used to maintain the known translation between objects and tags. Then for every viewpoint captured, AprilTags can be recognized together with their 6DoF pose in camera coordinates. Combining this with the known shift between tags and transparent objects, the corresponding CAD model of each object can be overlaid at the appropriate locations of transparent objects within the image. The resultant 3D meshes are sufficient to automate the succeeding dataset annotation tasks. Namely, projection of meshes to the image space provides the instance segmentation mask as well as the ground truth depth as shown in \autoref{fig:dataset_inference}. With our proposed dataset creation pipeline, we can collect and annotate 300 RGB-D images for one scene in 30 minutes with minimal human intervention. Compared to methods used by \cite{ClearGrasp, TransLab, Keypose}, our proposed pipeline only needs placing the transparent object once per sequence and annotation is fully automated, which guarantees the accuracy of annotated labels.

In total, \dataName has 14,659 images of scenes which contains six glass beakers and flasks in five different backgrounds. Four objects are used in training set and the other two novel objects form the novel validation and test set. The training set has 10,302 images where validation and testing set combined has 4357 images. When comparing with existing datasets in \autoref{fig:dataset_compare}, \dataName has the following advantages. Every scene consists of up to three transparent objects with occlusion, which introduces additional complexity to the dataset compared to KeyPose's \citep{Keypose} single object scenes. The objects and their placement are selected to mimic real-life transparent glassware, which can help to develop vision aware robots capable of manipulating transparent vessels. One particular direction of that is robotic chemists automating chemical research \citep{mobilechemist}. Additionally, the glass vessels in the dataset are empty or filled with 5 different coloured liquids to simulate real-life circumstances, which is common in 2D transparent datasets like Trans10K \citep{xie2021segmenting} and LabPic \citep{LabPic}, yet is missing in 3D datasets like ClearGrasp \citep{ClearGrasp} and KeyPose \citep{Keypose}. Besides the RGB and raw depth information, the dataset consists of ground truth depth, instance segmentation mask, as well as the objects' 6DoF poses. Our dataset's size is much bigger than ClearGrasp's \citep{ClearGrasp} 286 real world images, and is comparable with Trans10K (10K) \citep{xie2021segmenting} and KeyPose(48K)\citep{Keypose}, while having the potential to quickly scale to greater size with relative ease.

\begin{figure}
  \begin{minipage}[t]{0.4\linewidth}
    \vspace{0pt}
    \centering
    \includegraphics[width=\textwidth]{dataset_sample/dataset.png}\\
    \captionof{figure}{\textbf{Sample images} of ClearGrasp \citep{ClearGrasp} (top left), Ours (top right), Trans10K \citep{TransLab} (bottom left) and KeyPose \citep{Keypose} (bottom right).}
    \label{fig:dataset_compare}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.55\linewidth}
    \vspace{0pt}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{m{0.3\textwidth}|m{0.2\textwidth}|m{0.18\textwidth}|m{0.16\textwidth}|m{0.15\textwidth}}
        \toprule
        \rowcolor[HTML]{CBCEFB}
        $\ $ & \centering ClearGrasp \citep{ClearGrasp}& \centering Trans10K \citep{TransLab} & \centering KeyPose \citep{Keypose}  & \centering \textbf{Ours}  \cr 
        \midrule
        Real Samples & \centering $0.3\times10^3$ & \centering $10\times10^3$ & \centering $40\times10^3$ & \centering $15\times10^3$  \cr
        % \hline
        \rowcolor[HTML]{EFEFEF} 
        Auto Collection \& Annotation& \centering \xmark & \centering \xmark & \centering \xmark & \centering \cmark \cr
        % \hline
        Raw \& Ground Truth Depth & \centering \cmark & \centering \xmark & \centering \cmark & \centering \cmark\cr
        % \hline
        \rowcolor[HTML]{EFEFEF} 
        Instance Segmentation &\centering \xmark & \centering \xmark & \centering \cmark & \centering \cmark \cr
        % \hline 
        RGB &\centering \cmark & \centering \cmark & \centering \cmark & \centering \cmark \cr
        % \hline 
        \rowcolor[HTML]{EFEFEF} 
        Pose &\centering \xmark & \centering \xmark & \centering \cmark & \centering \cmark \cr
        % \hline
        Multi-Object Clutter &\centering \cmark & \centering \cmark & \centering \xmark & \centering \cmark \cr
        \bottomrule
    \end{tabular}
    }
    \captionof{table}{Comparison of \dataName (Ours) with ClearGrasp \citep{ClearGrasp}, Trans10K \citep{TransLab}, Keypose \citep{Keypose}. Ours is the only large-scale automatically collected 3D transparent object dataset with real glassware in cluttered settings.}
    
  \end{minipage}
  
\end{figure}


%===============================================================================
\section{Our Depth Completion Method}
Existing transparent object depth completion and pose estimation methods consider the distorted and incomplete depth of transparent objects as misguiding information and either only rely on color information \citep{Keypose}, or crop out the incorrect depth \citep{ClearGrasp}.  We propose a novel depth completion method to handle transparent objects called \algoName, whose general pipeline is shown in \autoref{fig:architecture}. Instead of discarding the transparent objects' depth, each item's depth is separated out and de-projected into a point cloud. With the distorted point cloud, a Point Cloud Completion network is used to recover the basic shape in the form of a predicted point cloud. However, such a point cloud is still too sparse and noisy to be considered as the final complete depth and used in downstream manipulation tasks. To further refine the depth, predicted point clouds are projected back to the depth channel and a depth completion module is used to fill the blank depth as well as correct the shifted depth. By combining both point cloud and depth completion tasks, our method avoids directly feeding incorrect transparent depth to the depth completion task, while capable to utilize the unique distortion depth caused by transparency and generate a coarse estimation of object depth via point cloud completion. Such intermediate depth improves the performance of the downstream depth completion subtask, as shown in Section~\ref{sec:Experiments}.

\subsection{Point Cloud Completion}
Transparent objects cause the RGB-D sensor to report invalid or inaccurate depth information. The Point Cloud Completion module is aimed at taking each object's depth as point cloud and estimating the correct depth by predicting the completed point cloud. Inspired by the point cloud processing modules proposed in GRNet \citep{GRNet}, we first feed the incomplete point cloud from transparent object depth de-projection through a Gridding layer \citep{GRNet}, which computes weighted vertices of 3D grid cells that points lie in. Then we use a 3D CNN encoder-decoder with U-Net connections \citep{ronneberger2015unet} to learn the features that are necessary for point cloud completion. The succeeding component is the Gridding reverse layer \citep{GRNet}, which back projects each 3D grid cell to a point in the coarse complete cloud whose coordinate is the weighted sum of the eight vertices. This is then forward propagated through a Multi-Layer Perceptron that creates the final completed point cloud. For each point, its corresponding grid cell's features from the CNN decoder are concatenated and combined with it. The MLP also refines the coarse to final point cloud with the aim of recovering details of the target object. However, the 3D CNN based network's limited resolution leads to sparse and noisy prediction, thus an additional depth completion module is needed. 

%This point cloud completion pipeline is a variant of GRNet~\citep{GRNet}.

% \textbf{Loss Function} Instead of directly evaluating loss between point clouds, comparing 3D grid bypass the unorderedness of point clouds and are suitable for applying L1/L2 losses. The Gridding Loss \citep{GRNet} is a L1 distance between predicted $\mathcal{G}_{pred} = <V^{pred}, W^{pred}>$ and ground truth $\mathcal{G}_{gt} = <V^{gt}, W^{gt}>$ 3D grids in $N_G$ resolution. The Gridding Loss can be defined as 
 
% $$ \mathcal{L}_{Gridding}( W^{pred},  W^{gt}) = \frac{1}{N^3_G} \sum| W^{pred} -  W^{gt}|$$

\subsection{Depth Completion}

The output from the depth completion module consists of a sparse distribution around the location of the transparent vessel(s) shown in \autoref{fig:architecture}. We fuse this with the RGB signal to generate a 4D input to our encoder-decoder structure. The encoder-decoder based depth-completion module processes the scene's depth to fill in the sparse depth and correct the noisy depth present. The decoder of the Depth Completion module consists of spatially-adaptive denormalization (SPADE) blocks, first introduced in \citep{park2019semantic}. Our usage of SPADE in the encoder-decoder Depth Completion module is a variant of \citep{dmidc2020}. This module enables us to learn spatially-dependent scale and bias for decoder feature maps, which helps reduce the domain shift between RGB and depth, as introduced by the empty depths on the depth map. Let $\textbf{m}$ denote the object mask. Given a batch of N inputs with dimension $C^i \times H^i \times W^i$, the output of the SPADE block at site ($n \in N$, $c \in C^i$, $y \in H^i$, $x \in W^i$) is then 
\begin{equation}
    \resizebox{0.9\textwidth}{!}{
    $\gamma_{c,y,x}^i(\textrm{\textbf{m}}) = \frac{h^i_{n,c,y,x} - \mu_c^i}{\sigma_c^i} + \beta^i_{c,y,x}(\textrm{\textbf{m}})  \quad \textrm{where } \mu_c^i = \frac{\sum_{n,y,x} h^i_{n,c,y,x} }{N H^i W^i} \quad \sigma_c^i = \sqrt{\frac{\sum_{n,y,x} (h^i_{n,c,y,x})^2 - (\mu_c^i)^2}{N H^i W^i} }
    $}
\end{equation}

$h^i_{n,c,y,x}$ is the activation at the site prior to normalization, $\mu_c^i$ and $\sigma_c^i$ are the mean and standard deviation of the activations within channel $c$. $\gamma_{c,y,x}^i(\textbf{m})$ and $\beta^i_{c,y,x}(\textbf{m})$ represent the modulation parameters. They represent the scaling and bias values for the $i$-th activation map at site $(c,y,x)$, respectively.

% \textbf{Loss Function:} The network is trained using logarithmic $L_1$ pair-wise loss. Pairwise loss computes error using pairs of pixels, $i$, $j$  in the output, which forces the pairs of pixels in the predicted depth to regress to similar values as the corresponding pairs in the ground truth depth. The loss function express as following where $\mathcal{G}$ describes the set of pixels where the ground truth depth is non-zero, $i$ and $j$ are the pixel pairs, and $y$ and $y*$ denote the ground truth and predicted depths, respectively. 

% $$\mathcal{L}(y_i,y_i^*) = \frac{1}{| \mathcal{G}^2 |} \sum_{i,j \in \mathcal{G}} \mid log \frac{y_i}{y_j} - log \frac{y_i^*}{y_j^*} \mid $$

\subsection{Loss Function}
The point cloud completion network is trained with Gridding Loss \citep{GRNet}, which is a L1 distance between predicted $\mathcal{G}_{p} = <V^{p}, W^{p}>$ and ground truth $\mathcal{G}_{gt} = <V^{gt}, W^{gt}>$ 3D grids in $N_G$ resolution. $V=\{v_i\}^{N_G^3}_{i=1}$ is collection of all vertices in 3D grid and $W=\{w_i\}^{N_G^3}_{i=1}$ is the weights corresponding to each vertex. Gridding Loss bypasses the un-orderedness of point clouds and is evaluated on the 3D grid. The depth completion network is trained using log $L_1$ pair-wise loss which forces the pairs of pixels in the predicted depth to regress to similar values as the corresponding pairs in the ground truth depth \citep{dmidc2020}. Let $\mathcal{G}$ describe the set of pixels where the ground truth depth is non-zero, $i$ and $j$ are the pixel pairs, and $y$ and $y*$ denote the ground truth and predicted depths, respectively.  We express these two loss functions as:
\begin{equation}
\resizebox{0.9\textwidth}{!}{$
\textrm{Gridding:} \mathcal{L}( W^{p},  W^{gt}) = \frac{1}{N^3_G} \sum| W^{p} -  W^{gt}|,  \quad \textrm{log } L_1 \textrm{: }\mathcal{L}(y_i,y_i^*) = \frac{1}{| \mathcal{G}^2 |} \sum_{i,j \in \mathcal{G}} \mid \text{log} \frac{y_i}{y_j} - \text{log} \frac{y_i^*}{y_j^*} \mid $
}
\end{equation}

\begin{figure}[!t]
% \centering
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/image.jpg}\\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/depth.jpg} \\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/depth_pred.jpg} \\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/detph_dmlrn_box.jpg} \\
%     % \caption{}
% \end{subfigure}%
% % \medskip
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/transpareNet_depth_box.jpg}\\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/detph_GroundTruth.jpg} \\
%     % \caption{}
% \end{subfigure}%

% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/image1.jpg}\\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/depth1.jpg} \\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/depth_pred1.jpg} \\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/detph_dmlrn1_box.jpg} \\
%     % \caption{}
% \end{subfigure}%
% % \medskip
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/transpareNet_depth1_box.jpg}\\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/detph_GroundTruth1.jpg} \\
%     % \caption{}
% \end{subfigure}%

% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/image2.jpg}\\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth ]{result_pic/depth2.jpg} \\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/depth_pred2.jpg} \\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/detph_dmlrn2_box.png} \\
%     % \caption{}
% \end{subfigure}%
% % \medskip
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/transpareNet_depth2_box.jpg}\\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.16\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{result_pic/detph_GroundTruth2.jpg} \\
%     % \caption{}
% \end{subfigure}%

\centering
\includegraphics[width=\textwidth]{result_pic/results.JPG}

\caption{\textbf{Visualization of prediction results on \dataName dataset.} From left to right, (a) RGB image (b) Raw depth from RGB-D sensor (c) PCC predicted depth (d) DC predicted depth (e) \algoName predicted depth (f) Ground Truth. We mark the major difference between DC and \algoName predictions with bounding boxes.}
\label{fig:result_compare}
\end{figure}







% completes the sparsity at the object location output from GRNet,  object mask, we perform depth refinement to fill in the blank depth and correct the shifted depth. We apply a standard encoder-decoder structure, with the addition of a custom decoder modulation procedure by spatially-adaptive denormalization (SPADE) \citep{dmidc2020} \citep{park2019semantic}. 
% \textbf{Architecture:} The decoder modulation branch follows the standard encoder-decoder structure, with the addition of the custom decoder modulation procedure by spatially-adaptive denormalization (SPADE) \citep{dmidc2020} \citep{park2019semantic}. The EfficientNet-b4 backbone architecture from the EfficientNet family \citep{tan2020efficientnet} generates feature maps from the 4D RGBD signal, which is passed to the lightweight RefineNet decoder \citep{nekrasov2018lightweight}. The 4D signal of of transparent objects is an inherently in-homogeneous distribution, due to the presence of missing and noisy depths. Therefore, the SPADE block in the decoder reduces the domain gap between RGB and RGBD signals at locations of missing depths through spatially-dependent scale and bias for normalization of feature maps \citep{park2019semantic}. The input signal to the depth completion network consists of the RGB image, depth, and mask of areas requiring depth completion, no additional modalities are needed.  


% When the 4D signal is passed into the network, The SPADE block helps reduce domain gap 

% - inhomogeneous spacial distribution --> parts of the depth data is missing 
% - signal compression in backbone smoothes inhomogeneity --> works well for small depth gaps
% - if gaps are too large, incorrect activations by convolutions due to large domain shift b/w rgb & rgbd signals
% - reduce domain gap by Spatially-dependent scale and bias for normalized feature maps --> decoder
% - preserves semantic information against common normalization layers






% There are two main streams of losses used in literature for depth completion: pixel-wise and pairwise losses. 
% - Different types of loss functions
% - pixel-wise and pairwise
% - pixelsize measure mean per-pixel distance between prediction and target
% pairwise express error by comparing relationships between pairs of pixels in output --> force the relationship between each pair of pixels in the prediction to be similar to that of the corresponding pair in the ground truth.


\begin{table}[!t]
\centering
\caption{\textbf{Depth completion results} on Known and Novel Objects within the ClearGrasp \citep{ClearGrasp} Dataset. Metrics are defined in Section \ref{sec:metrics}. Arrows beside the metrics denote whether higher or lower values are desired.}
\label{t2}
\resizebox{0.75\textwidth}{!}{%
% \begin{tabular}{c|c|c|c|c|c|c|c|c}
\begin{tabular}{c|c|c|c|c|c|c}
% \begin{tabular}{|RMSE|MAE|$\delta_{1.05}$|$\delta_{1.10}$|$\delta_{1.25}$|$\delta_{1.25^2}$|$\delta_{1.25^3}$|SSIM|}
    \toprule
    \rowcolor[HTML]{CBCEFB}
	$\ $ & RMSE (\textdownarrow) & MAE (\textdownarrow) & $\delta_{1.05}$ (\textuparrow) & $\delta_{1.10}$ (\textuparrow) & $\delta_{1.25}$ (\textuparrow) & REL (\textdownarrow)\\
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Seen} \\
	\midrule
% 	\hline
	NLSPN \citep{park2020nonlocal} & 0.136 & 0.113 & 0.1902 & 0.3595 & 0.7043 & 0.231 \\
% 	\hline
    \rowcolor[HTML]{EFEFEF} 
	ClearGrasp \citep{ClearGrasp} & 0.041 & 0.031 & 0.6943 & 0.8917 & 0.9674 & 0.055\\
% 	\hline
	LocalImplicit \citep{zhu2021rgbd} & 0.023 & 0.017 & 0.8356 & 0.9504 & 0.9923 & 0.031\\
% 	\hline
    \rowcolor[HTML]{EFEFEF} 
    TranspareNet-DC Only & \textbf{0.011} & \textbf{0.008} & \textbf{0.9354} & \textbf{0.9831} & \textbf{0.9985} & \textbf{0.010}\\
% 	DMLRN & \textbf{0.0112} & \textbf{0.0081} & \textbf{93.54} & \textbf{98.31} & \textbf{99.85} & \textbf{0.0099}\\
	\bottomrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Unseen} \\
	\midrule
	NLSPN \citep{park2020nonlocal} & 0.132 & 0.106 & 0.1625 & 0.3213 & 0.6478 & 0.239 \\
% 	\hline
    \rowcolor[HTML]{EFEFEF} 
	ClearGrasp \citep{ClearGrasp} & 0.044 & 0.038 & 0.4137 & 0.7920 & 0.9729 & 0.074 \\
% 	\hline
	LocalImplicit \citep{zhu2021rgbd} & 0.041 & 0.034 & 0.5269 & 0.7942 & 0.9805 & 0.063\\
% 	\hline
    \rowcolor[HTML]{EFEFEF} 
    TranspareNet-DC Only & \textbf{0.032} & \textbf{0.027} & \textbf{0.6080} & \textbf{0.8053} & \textbf{0.9821} & \textbf{0.052}\\
	
% 	DMLRN & \textbf{0.0316} & \textbf{0.0274} & \textbf{60.80} & \textbf{80.53} & \textbf{98.21} & \textbf{0.0523}\\
	\bottomrule
% 	\rowcolor[HTML]{FBE0CB}
% 	$\ $ &  \multicolumn{6}{c}{Real-Known} \\
% 	\midrule
% 	NLSPN & 0.149 & 0.127 & 14.04 & 26.67 & 54.32 & 0.228 \\
% % 	\hline
%     \rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp & \textbf{0.039} & \textbf{0.029} & \textbf{72.62} & \textbf{86.96} & 95.58 & 0.051\\
% % 	\hline
% 	LocalImplicit & 0.040 & 0.031 & 65.71 & 84.27 & \textbf{96.60} & 0.053\\
% % 	\hline
%     \rowcolor[HTML]{EFEFEF} 
% 	DMLRN & 0.0782 & 0.04003 & 58.85 & 82.48 & 94.79 & \textbf{0.0452}\\
% 	\bottomrule
% 	\rowcolor[HTML]{FBE0CB}
% 	$\ $ &  \multicolumn{6}{c}{Real-Novel} \\
% 	\midrule
% 	NLSPN & 0.145 & 0.123 & 13.77 & 25.81 & 51.59 & 0.240 \\
% % 	\hline
%     \rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp & 0.034 & 0.025 & 76.72 & 91.00 & 97.63 & 0.045\\
% % 	\hline
% 	LocalImplicit & 0 & 0 & 0 & 0 & 0 & 0 \\
% % 	\hline
%     \rowcolor[HTML]{EFEFEF} 
% 	DMLRN & 0.0372 & 0.0336 & 44.44 & 70.46 & \textbf{99.27} & 0.0618 \\
% 	\bottomrule
% 	\hline
\end{tabular}
}
\end{table}


\section{Experimental Results}
\label{sec:Experiments}
We conduct experiments to test our \algoName model along with SOTA methods like NLSPN \citep{park2020nonlocal} and ClearGrasp\citep{ClearGrasp} as well as superior con-current method LocalImplicit \citep{zhu2021rgbd} on the ClearGrasp \citep{ClearGrasp} dataset to demonstrate our method's strong performance. Additionally, we test ClearGrasp, \algoName, as well as its point cloud and depth completion modules on our \dataName dataset. This allows us to study the effects of each individual module of \algoName and advantages of the proposed joint pipeline over existing methods for depth and point cloud completion. Importantly, our results demonstrate that distorted transparent object depth can be converted into sparse depth estimate via point cloud completion and used to improve downstream depth completion quality.

\label{sec:result}
\subsection{Metrics}
\label{sec:metrics}
For depth completion, the standard metrics as described in \citep{ClearGrasp} are followed. The prediction and ground truth arrays are first resized to $144 \times 256$ resolution prior to evaluation. Errors are computed using the following metrics, Root Mean Squared Error (RMSE), Absolute Relative Difference (REL), Mean Absolute Error (MAE) and Threshold ($\delta$). 

\begin{equation}
\resizebox{0.9\textwidth}{!}{$
\textrm{RMSE: } \sqrt{\frac{1}{| \hat{D}|}\sum_{d_i \in \hat{D}} || d_i-d_i^*||^2}  \quad \quad \textrm{REL: } \frac{1}{| \hat{D}|}\sum_{d_i \in \hat{D}} | d_i-d_i^*|/d_i^* \quad \quad \textrm{MAE: } \frac{1}{| \hat{D}|}\sum_{d_i \in \hat{D}} | d_i-d_i^*|$
}
\end{equation}

Threshold is \% of $d_i \in \hat{D}$ satisfying max($\frac{d_i}{d_i^*}$, $\frac{d_i^*}{d_i}$) $<$ $\delta$. Here, $\hat{D}$ denotes the set of pixels $\mathcal{G} \cap D$, which has a valid corresponding ground-truth depth, and falls within the mask. $d_i \in \hat{D}$ is the predicted depth, and $d^*_i$ is the corresponding ground-truth depth. Note that RMSE, REL and MAE metrics are computed in meters. For the threshold metric, $\delta$ is set to 1.05, 1.10 and 1.25.

\subsection{Depth Completion Comparison with State-of-the-Art Methods}

The ability of our method to estimate depth for transparent objects is shown \autoref{t2}. All models were trained using the ClearGrasp \citep{ClearGrasp} dataset. \textit{Seen} denotes synthetic objects which have appeared in training set, under a different setting. \textit{Unseen} denotes synthetic objects that have not appeared in the training set. We compare with several state-of-the-art methods. NLSPN \citep{park2020nonlocal} is a state-of-the-art method for depth completion on the NYUV2 \citep{Silberman:ECCV12} and KITTI \citep{uhrig2017sparsity} datasets. ClearGrasp \citep{ClearGrasp} is the state-of-the art of transparent object depth completion using global optimization. LocalImplicit \citep{zhu2021rgbd} is a new con-current method that achieved superior performance than ClearGrasp under several metrics. The Point Cloud Completion (PCC) module cannot be trained using the ClearGrasp synthetic dataset, since it lacks raw sensor depth. Therefore, we only show the performance of the Depth Completion (DC) component of our proposed method and demonstrate that it already outperforms all depth completion methods for the metrics presented, by a large margin.

\subsection{Performance on Our Dataset}
We evaluate each component of our model on our proposed dataset as well as compare the results with performance of the ClearGrasp algorithm \citep{ClearGrasp}. Evaluation of ClearGrasp uses the pre-pretrained model, since we cannot re-train ClearGrasp \cite{ClearGrasp} on our data, as ground truth surface normal and occlusion boundaries are non-trivial to obtain for our real-object dataset \dataName. For evaluation of the pretrained ClearGrasp model, we use the ground truth depth and mask \footnote{We acknowledge the transparent object depth completion method \citep{zhu2021rgbd}, which is concurrent with our work. However, because neither the code nor their dataset is released, we can not apply our dataset to evaluate the model's performance.}.  When we evaluate the direct output from our Point Cloud Completion (PCC) module, we see that it suffers due to the sparsity of depths in the transparent object region, empty depth values of which are interpreted as zero to ensure numerical stability. When computing metrics using only regions within the object mask where the depth is non-empty, we see improved metric values (See Appendix), this means that the Point Cloud Completion produces meaningful depths that is valuable for the downstream depth completion task. We cannot directly use the output from the Point Cloud Completion module, since the point cloud is too sparse. When only the depth completion (DC) module is used without point cloud completion, we see inferior results as compared to our joint point cloud and depth completion network, \algoName, shown in Table \ref{tab:TODs_Metrics}. We can see that \algoName outperforms ClearGrasp \citep{ClearGrasp}, PCC, and DC. This means that our joint approach, which involves leveraging the unique depth distortion around transparent objects, generating a course estimation, and conducting depth completion using the point cloud distribution is indeed effective. A qualitative comparison of different stages of our model is shown in \autoref{fig:result_compare}. We see that for single and multi-object cluttered scenes, our method achieves SOTA results. 

\begin{table}[!t]
\centering
\caption{\textbf{Depth completion results on the \dataName Dataset.} We assess the performance of various models with Novel 1 Object images, as well as novel cluttered images with 2 or 3 objects. Our method outperforms all previous methods. Metrics are defined in Section \ref{sec:metrics}. The arrows beside the metrics denote whether lower or higher values are more desired. }
\label{t3}
\resizebox{0.75\textwidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c}
    \toprule
    \rowcolor[HTML]{CBCEFB}
% \begin{tabular}{|RMSE|MAE|$\delta_{1.05}$|$\delta_{1.10}$|$\delta_{1.25}$|$\delta_{1.25^2}$|$\delta_{1.25^3}$|SSIM|}
% 	\hline
	$\ $ & RMSE (\textdownarrow) & MAE (\textdownarrow) & $\delta_{1.05}$ (\textuparrow) & $\delta_{1.10}$ (\textuparrow) & $\delta_{1.25}$ (\textuparrow) & REL (\textdownarrow)\\
% 	\hline \hline
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Novel 1 Object Scene} \\
	\midrule
	\rowcolor[HTML]{EFEFEF} 
	ClearGrasp \citep{ClearGrasp} &  0.0425 &  0.0367 & 0.4023 & 0.6018 & 0.8990 & 0.1073 \\
% 	\hline
	
	TranspareNet-PCC Only & 0.2443 & 0.1713 & 0.4927 & 0.5113 & 0.5196 & 0.4877 \\
	
    \rowcolor[HTML]{EFEFEF} 
    TranspareNet-DC Only & 0.0188 & 0.0159 & 0.6483 & 0.8819 & 0.9931 & 0.0473\\
    
	\algoName (ours) & \textbf{0.0166} & \textbf{0.0140} & \textbf{0.7133} & \textbf{0.9299} & \textbf{0.9945} & \textbf{0.0398}\\
	
	
% 	\hline
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Novel 2 Object Scene} \\
	\midrule
	\rowcolor[HTML]{EFEFEF} 

	ClearGrasp \citep{ClearGrasp} &  0.0534 & 0.0433 & 0.3458 & 0.5414 & 0.8412 & 0.1455\\
	
	TranspareNet-PCC Only & 0.2477 & 0.1817 & 0.4139 & 0.4434 & 0.4561 & 0.5516 \\
    \rowcolor[HTML]{EFEFEF} 
	TranspareNet-DC Only & 0.0212 & 0.0168 & 0.5954 & 0.8256 & 0.9874 & 0.0564 \\
% 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
	\algoName (ours) & \textbf{0.0194} & \textbf{0.0159} & \textbf{0.6475} & \textbf{0.8693} & \textbf{0.9876} & \textbf{0.0496}\\
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Novel 3 Object Scene} \\
	\midrule
	\rowcolor[HTML]{EFEFEF} 
	ClearGrasp \citep{ClearGrasp} & 0.0612 & 0.0493 & 0.2985 & 0.4954 & 0.8361 & 0.1536 \\
% 	\hline
	TranspareNet-PCC Only & 0.2659 & 0.1922 & 0.4275 & 0.4564 & 0.4724 & 0.5362 \\
% 	\hline
	\rowcolor[HTML]{EFEFEF} 
% 	\hline
	TranspareNet-DC Only & 0.0250 & \textbf{0.0189} & \textbf{0.5902} & 0.8305 & 0.9866 & 0.0555 \\
% 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
	\algoName (ours) & \textbf{0.0232} & 0.0190 & 0.5817 & \textbf{0.8408} & \textbf{0.9904} & \textbf{0.0546}\\
	\midrule
	\rowcolor[HTML]{FBE0CB}
	$\ $ &  \multicolumn{6}{c}{Novel Combined} \\
	\midrule
	\rowcolor[HTML]{EFEFEF} 
	ClearGrasp \citep{ClearGrasp} & 0.0563 & 0.0455 & 0.3262 & 0.5233 & 0.8476 & 0.1435\\
% 	\hline
	TranspareNet-PCC Only & 0.2584 & 0.1864 & 0.4354 & 0.4627 & 0.4767 & 0.5314 \\
% 	\hline
	\rowcolor[HTML]{EFEFEF} 
% 	\hline
	TranspareNet-DC Only & 0.0232 & 0.0180 & 0.6010 & 0.8380 & 0.9879 & 0.0543\\
% 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
	\algoName (ours) & \textbf{0.0213} & \textbf{0.0175} & \textbf{0.6180} & \textbf{0.8619} & \textbf{0.9905} & \textbf{0.0510}\\
	\bottomrule
% 	$\ $ &  \multicolumn{6}{c}{Real-Known} \\
% 	\hline
% 	ClearGrasp & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	DMLRN & 0 & 0 & 0 & 0 & 0 & 0\\
% 	\hline
% 	$\ $ &  \multicolumn{6}{c}{Real-Unknown} \\
% 	\hline
% 	ClearGrasp & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	DMLRN & 0 & 0 & 0 & 0 & 0 & 0\\
% 	\hline
% 	\hline
\end{tabular}
}
\label{tab:TODs_Metrics}
\end{table}
% \begin{table}[!t]
%     \centering
%     \caption{Depth completion results on the \dataName Dataset. We assess the performance of various models with unseen single object images, as well as unseen cluttered images with 2 or 3 objects. Our method out-performs all previous methods. The arrows beside the metrics denote whether lower or higher values are more desired. CG is ClearGrasp, PCC is \algoName point cloud completion module only, DE is \algoName depth completion module only and Ours is the complete \algoName method.}
%     \resizebox{1\textwidth}{!}{%
%     \begin{tabular}{c|c c c c|c c c c|c c c c|c c c c}
%         \toprule
%          \cellcolor[HTML]{CBCEFB}$\ $ & \multicolumn{4}{c}{\cellcolor[HTML]{FBE0CB} Unseen 1 Object Scene}  & \multicolumn{4}{c}{ \cellcolor[HTML]{cbe9fb}Unseen 2 Object Scene} & \multicolumn{4}{c}{ \cellcolor[HTML]{f6fbcb}Unseen 3 Object Scene} & \multicolumn{4}{c}{ \cellcolor[HTML]{fbd7cb}Unseen Combined}\\
%          \midrule
%          \cellcolor[HTML]{CBCEFB}$\ $ & \cellcolor[HTML]{FBE0CB}CG & \cellcolor[HTML]{FBE0CB}PCC & \cellcolor[HTML]{FBE0CB}DE & \cellcolor[HTML]{FBE0CB}Ours & \cellcolor[HTML]{cbe9fb}CG & \cellcolor[HTML]{cbe9fb}PCC & \cellcolor[HTML]{cbe9fb}DE & \cellcolor[HTML]{cbe9fb}Ours & \cellcolor[HTML]{f6fbcb}CG & \cellcolor[HTML]{f6fbcb}PCC & \cellcolor[HTML]{f6fbcb}DE & \cellcolor[HTML]{f6fbcb}Ours & \cellcolor[HTML]{fbd7cb}CG & \cellcolor[HTML]{fbd7cb}PCC &\cellcolor[HTML]{fbd7cb} DE & \cellcolor[HTML]{fbd7cb}Ours \\
%          \midrule
% 	     \rowcolor[HTML]{EFEFEF} 
%          \cellcolor[HTML]{CBCEFB}RMSE (\textdownarrow)          & 0.9276 & 0.2443 & 0.0188 & \textbf{0.0166} & 0.8641 & 0.2477 & 0.0212 & \textbf{0.0166} &  0.7972 &  0.2659 & 0.0250 & \textbf{0.0232} & 0.7904 & 0.2584 & 0.0232 & \textbf{0.0213}  \\
%          \cellcolor[HTML]{CBCEFB}MAE (\textdownarrow)           & 0.4027 & 0.1713 & 0.0159 & \textbf{0.0140} & 0.4436 & 0.1817 & 0.0168 & \textbf{0.0140} &  0.3639 & 0.1922 & \textbf{0.0189} &  0.0190 & 0.3639 & 0.1864 &  0.0180 & \textbf{0.0175}  \\
%          \rowcolor[HTML]{EFEFEF} 
%          \cellcolor[HTML]{CBCEFB}$\delta_{1.05}$ (\textuparrow) & \textbf{0.7596} & 0.4927 & 0.6483 & 0.7133 & 0.6910 & 0.4139 & 0.5954 & \textbf{0.7133} & \textbf{0.6976} & 0.4275 & 0.5902 & 0.5817 & \textbf{0.6976} & 0.4354 & 0.6010 & 0.6180 \\
%          \cellcolor[HTML]{CBCEFB}$\delta_{1.10}$ (\textuparrow) & 0.7746 & 0.5113 & 0.8819 & \textbf{0.9299} & 0.7102 & 0.4434 & 0.8256 & \textbf{0.9299} & 0.7198 & 0.4564 & 0.8305 & \textbf{0.8408} & 0.7198 & 0.4627 & 0.8380 & \textbf{0.8619}  \\
%          \rowcolor[HTML]{EFEFEF} 
%          \cellcolor[HTML]{CBCEFB}$\delta_{1.25}$ (\textuparrow) & 0.7911 & 0.5196 & 0.9931 & \textbf{0.9945} & 0.7371 & 0.4561 & 0.9874 & \textbf{0.9945} &  0.7558 & 0.4724 & 0.9866 & \textbf{0.9904} & 0.7558 & 0.4767 & 0.9879 & \textbf{0.9905}  \\
%          \cellcolor[HTML]{CBCEFB}Rel (\textdownarrow)           & 0.1051 & 0.4877 & 0.0473 & \textbf{0.0398} & 0.1385 & 0.5516 & 0.0564 & \textbf{0.0398} & 0.1233 & 0.5362 & 0.0555 & \textbf{0.0546} & 0.1231 & 0.5314 & 0.0543 & \textbf{0.0510} \\
%         \bottomrule
%     \end{tabular}
%     }
%     \label{tab:TODs_Metrics}
% \end{table}

\section{Conclusion}
\label{sec:conclusion}
We introduced \algoName, a novel method that achieves state-of-the-art performance on synthetic transparent object data from ClearGrasp \citep{ClearGrasp}. Our method leverages existing depth information at the location of transparent objects to estimate the complete depth for transparent objects. We also introduced a novel real dataset, \dataName, which has complex scenes with numerous vessels under occlusion. The transparent vessels in our dataset mimics vessels found in household settings in shape and appearance. To our knowledge, our dataset is the first to contain depth information of transparent vessels with partially filled liquids. We benchmark our dataset using the proposed depth completion method, \algoName, and achieve SOTA results. We also introduced an automatic data capture and annotation workflow that consists of robot-controlled image collection and vision-based automatic annotation. We hope that this work can accelerate future research in the field of household robotics and handling of glassware.


% . Note that the empty depths from the point cloud distribution heavy skew the metrics, hence the poor performance. GRNet$_{non-empty}$ denotes the calculation using only regions within the mask where the depth is non-empty. This is more reflective of the quality of the point cloud generated by GRNet.

% Create figures: Raw input, non-empty version (black), empty version (gray zone), our model --> show non-empty depth is not usable

% If you are only considering: see qualitative results 
% drop non-empty and empty
% replace last row with algo name





% \begin{figure}[h]
% \centering
% \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{sample_pics/image.jpg}\\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{sample_pics/instance_segment.png} \\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{sample_pics/raw_depth.png} \\
%     % \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{sample_pics/gt_depth.png} \\
%     % \caption{}
% \end{subfigure}%
% % \medskip

% \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{sample_pics/image_1.jpg} \\
%     \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{sample_pics/instance_segment_1.png} \\
%     \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{sample_pics/raw_depth_1.png} \\
%     \caption{}
% \end{subfigure}%
% \begin{subfigure}[b]{0.24\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{sample_pics/gt_depth_1.png} \\
%     \caption{}
% \end{subfigure}%

% \caption{A selection of the proposed dataset: (a) RBG image (b) Instance Segmentation (c) Raw depth from RGB-D sensor (d) Ground truth depth obtained through automatic depth annotation. The dataset also includes object pose information, which is not depicted in the figure.}
% \label{fig:dataset_inference}
% \end{figure}




% \section{Ablative Studies}
% \begin{table}[!h]
% \centering
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
% % \begin{tabular}{|RMSE|MAE|$\delta_{1.05}$|$\delta_{1.10}$|$\delta_{1.25}$|$\delta_{1.25^2}$|$\delta_{1.25^3}$|SSIM|}
% 	\hline
% 	$\ $ & RMSE & MAE & $\delta_{1.05}$ & $\delta_{1.10}$ & $\delta_{1.25}$ & rel\\
% 	\hline \hline
% 	$\ $ &  \multicolumn{6}{c}{Cup 0} \\
% 	\hline
% 	Cleargrasp$_{CG}$ & 0.11 & 0.09 & 33.31 & 52.77 & 86.02 & 0.123 \\
% 	\hline
% 	DMLRN$_{CG}$ & 0.1628 & 0.0933 & 54.48 & 73.07 & 85.28 & 0.0762 \\
% 	\hline
% 	DMLRN$_{Ours}$ & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	GRNet + DMLRN$_{Ours}$ & 0 & 0 & 0 & 0 & 0 & 0 \\
	
% 	% 2021-06-16 11:56:54 - saic-dc: Inference: subset -- cup_0 | mse: 0.0021 (0.0345) # mae: 0.0300 (0.0933) # rmse: 0.0459 (0.1628) # d105: 0.5085 (0.5448) # d110: 0.9644 (0.7307) # d125_1: 0.9867 (0.8528) # d125_2: 0.9935 (0.9409) # d125_3: 0.9966 (0.9523) # rel_med: 0.0463 (0.0762) # rel_avg: 0.0569 (0.7105)
	
%     % 	Inference: subset -- cup_1 | mse: 0.0169 (0.0481) # mae: 0.0662 (0.1240) # rmse: 0.1300 (0.2054) # d105: 0.4903 (0.3809) # d110: 0.7262 (0.5682) # d125_1: 0.9222 (0.8070) # d125_2: 0.9362 (0.9145) # d125_3: 0.9530 (0.9288) # rel_med: 0.0496 (0.1030) # rel_avg: 0.4837 (1.0075)
    
%     % Inference: subset -- bottle_0 | mse: 0.0101 (0.0489) # mae: 0.0408 (0.1149) # rmse: 0.1007 (0.1983) # d105: 0.8407 (0.4749) # d110: 0.9451 (0.6838) # d125_1: 0.9468 (0.8274) # d125_2: 0.9583 (0.9077) # d125_3: 0.9646 (0.9234) # rel_med: 0.0281 (0.0835) # rel_avg: 0.2410 (1.1432)
    
%     % Inference: subset -- bottle_1 | mse: 0.0316 (0.0494) # mae: 0.1074 (0.1304) # rmse: 0.1777 (0.2059) # d105: 0.3253 (0.3427) # d110: 0.5584 (0.5254) # d125_1: 0.8855 (0.7856) # d125_2: 0.9247 (0.9163) # d125_3: 0.9366 (0.9313) # rel_med: 0.0813 (0.1103) # rel_avg: 0.6608 (0.9841)
    
%     % Inference: subset -- bottle_2 | mse: 0.0110 (0.0443) # mae: 0.0412 (0.1179) # rmse: 0.1049 (0.1950) # d105: 0.7178 (0.4142) # d110: 0.9197 (0.6031) # d125_1: 0.9441 (0.8005) # d125_2: 0.9510 (0.9259) # d125_3: 0.9646 (0.9385) # rel_med: 0.0311 (0.0964) # rel_avg: 0.3377 (0.9346)
	
% 	\hline
% % 	DMLRN_{ours} & 0.023 & 0.017 & 83.56 & 95.04 & 99.23 & 0.031\\
% % 	\hline
% % 	DMLRN & \textbf{0.0112} & \textbf{0.0081} & \textbf{93.54} & \textbf{98.31} & \textbf{99.85} & \textbf{0.0099}\\
% % 	\hline
% 	$\ $ &  \multicolumn{6}{c}{Cup 1} \\
% 	\hline

% 	Cleargrasp$_{CG}$ & 0.06 & 0.049 & 50.90 & 76.17 & 98.87 & 0.064 \\
% 	\hline
% 	DMLRN$_{CG}$ & 0.2054 & 0.1240 & 38.09 & 56.82 & 80.70 & 0.1030 \\
% 	\hline
% 	DMLRN$_{Ours}$ & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	GRNet + DMLRN$_{Ours}$ & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	$\ $ &  \multicolumn{6}{c}{Bottle 0} \\
% 	\hline

% 	Cleargrasp$_{CG}$ & 0.063 & 0052 & 49.62 & 75.14 & 97.93 & 0.068 \\
% 	\hline
% 	DMLRN$_{CG}$ & 0.1983 & 0.1149 & 0.4749 & 0.6838 & 0.8274 & 0.0835 \\
% 	\hline
% 	DMLRN$_{Ours}$ & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	GRNet + DMLRN$_{Ours}$ & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	$\ $ &  \multicolumn{6}{c}{Bottle 1} \\
% 	\hline
% 	Cleargrasp$_{CG}$ & 0.105 & 0.083 & 41.62 & 58.56 & 85.81 & 0.115 \\
% 	\hline
% 	DMLRN$_{CG}$ & 0.2059 & 0.1304 & 0.3427 & 0.5254 & 0.7856 & 0.1103 \\
% 	\hline
% 	DMLRN$_{Ours}$ & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	GRNet + DMLRN$_{Ours}$ & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	$\ $ &  \multicolumn{6}{c}{Bottle 2} \\
% 	\hline
% 	Cleargrasp$_{CG}$ & 0.064 & 0.051 & 59.77 & 79.46 & 94.66 & 0.068 \\
% 	\hline
% 	DMLRN$_{CG}$ & 0.1950 & 0.1179 & 0.4142 & 0.6031 & 0.8005 & 0.0964 \\
% 	\hline
% 	DMLRN$_{Ours}$ & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% 	GRNet + DMLRN$_{Ours}$ & 0 & 0 & 0 & 0 & 0 & 0 \\
% 	\hline
% % 	\hline
% \end{tabular}
% \caption{Depth completion results on Test Set.}
% \label{keypose_eval}
% \end{table}
% \subsection{Object Manipulation}

	

%===============================================================================



% \section{Appendix}
% \label{sec:appendix}

% \subsection{Implementation Details}
% The Point Cloud Completion module is trained with Adam optimizer\citep{kingma2017adam} with initial parameters $\alpha = 10^{-4}, \beta_1 = 0.9, \beta_2=0.999$. The learning rate decays by 2 after 50 epochs and training is stopped after 300 epochs. For training the decoder modulation depth completion module, we use the Adam optimizer \citep{kingma2017adam} with initial learning rate set to $10^{-4}$, $\beta_1 = 0.9$, $\beta_2 = 0.999$ and without weight decay. We use the EfficientNet-b4 \citep{tan2020efficientnet} pretrained on ImageNet \citep{russakovsky2015imagenet} as the backbone. Experiments conducted on the ClearGrasp \citep{ClearGrasp} dataset are trained with a resolution of $240 \times 320$. Experiments conducted on the our proposed dataset, \datasetName, are trained with a resolution of $480 \times 640$. Training is performed end-to-end for 100 epochs, with early stopping.

% \begin{table}[!h]
% \centering
% \caption{Depth completion results on the proposed dataset. Validation and Test set both contain non-overlapping novel scenarios, placements, and objects. $^*$ Metrics used are an adapted version that excludes empty depths, we include GRNet for comparison reasons, but cannot directly use the prediction from GRNet in downstream tasks due to sparsity. }
% \label{t3}
% \resizebox{0.7\textwidth}{!}{%
% \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
%     \toprule
%     \rowcolor[HTML]{CBCEFB}
% % \begin{tabular}{|RMSE|MAE|$\delta_{1.05}$|$\delta_{1.10}$|$\delta_{1.25}$|$\delta_{1.25^2}$|$\delta_{1.25^3}$|SSIM|}
% % 	\hline
% 	$\ $ & RMSE (\textdownarrow) & MAE (\textdownarrow) & $\delta_{1.05}$ (\textuparrow) & $\delta_{1.10}$ (\textuparrow) & $\delta_{1.25}$ (\textuparrow) & Rel (\textdownarrow)\\
% % 	\hline \hline
% 	\midrule
% 	\rowcolor[HTML]{FBE0CB}
% 	$\ $ &  \multicolumn{6}{c}{Validation (Novel)} \\
% 	\midrule
% 	\rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp & 0.8715 & 0.3740 & 0.7126 & 0.7351 & 0.7701 & 0.1140\\
% % 	\hline
% 	GRNET^*$_{non-empty}$ & 0.0115 & 0.0074 & 0.8949 & 0.9587 & 0.9969 & 0.0150\\
% % 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
% 	GRNET$_{empty}$ & 0.2549 & 0.1820 & 0.4521 & 0.4785 & 0.4938 & 0.5146\\
% % 	\hline
% 	DMLRN & 0.0184 & 0.0135 & 0.6951 & 0.8423 & 0.9594 & 0.0251 \\
% % 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
% 	\algoName & \textbf{0.0174} & \textbf{0.0123} & \textbf{0.7714} & \textbf{0.9018} & \textbf{0.9018} & \textbf{0.0215}\\
% % 	\hline
% 	\midrule
% 	\rowcolor[HTML]{FBE0CB}
% 	$\ $ &  \multicolumn{6}{c}{Test (Novel)} \\
% 	\midrule
% 	\rowcolor[HTML]{EFEFEF} 
% 	ClearGrasp & 0.7734 & 0.3593 & 0.6884 & 0.7108 & 0.7466 & 0.1276\\
% % 	\hline
% 	GRNET$^*_{non-empty}$ & 0.0118 & 0.0076 & 0.8874 & 0.9588 & 0.9953 & 0.0158\\
% % 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
% 	GRNET$_{empty}$ & 0.2601 & 0.1887 & 0.4270 & 0.4547 & 0.4681 & 0.5400\\
% % 	\hline
% 	DMLRN & 0.0190 & 0.0137 & 0.6661 & 0.8956 & 0.9635 & 0.0314 \\
% % 	\hline
% 	\rowcolor[HTML]{EFEFEF} 
% 	\algoName & \textbf{0.0172} & \textbf{0.0122} & \textbf{0.7603} & \textbf{0.9047} & \textbf{0.9637} & \textbf{0.0251}\\
% 	\bottomrule
% % 	$\ $ &  \multicolumn{6}{c}{Real-Known} \\
% % 	\hline
% % 	ClearGrasp & 0 & 0 & 0 & 0 & 0 & 0 \\
% % 	\hline
% % 	DMLRN & 0 & 0 & 0 & 0 & 0 & 0\\
% % 	\hline
% % 	$\ $ &  \multicolumn{6}{c}{Real-Unknown} \\
% % 	\hline
% % 	ClearGrasp & 0 & 0 & 0 & 0 & 0 & 0 \\
% % 	\hline
% % 	DMLRN & 0 & 0 & 0 & 0 & 0 & 0\\
% % 	\hline
% % 	\hline
% \end{tabular}
% }
% \end{table}

%===============================================================================
%===============================================================================

% The maximum paper length is 8 pages excluding references and acknowledgements, and 10 pages including references and acknowledgements

\clearpage
% The acknowledgments are automatically included only in the final and preprint versions of the paper.
\acknowledgments{Animesh Garg is a CIFAR AI Chair. Al\`{a}n Aspuru-Guzik is a CIFAR AI Chair and CIFAR Lebovic Fellow. Animesh Garg and Florian Shkurti are also supported in part through the NSERC Discovery Grants Program. The authors would like to acknowledge Vector Institute and ComputeCanada for computing services. Al\`{a}n Aspuru-Guzik and Haoping Xu thank the Canada 150 Research Chair funding from NSERC, Canada. Al\`{a}n Aspuru-Guzik is thankful for the generous support of Dr. Anders G. Fr\o seth. The authors would like to thank Yuchi Zhao for constructive feedback and discussions on the manuscript.
}

%===============================================================================

% no \bibliographystyle is required, since the corl style is automatically used.
\bibliography{example}  % .bib

\include{00-appendix_camera_ready}

\end{document}
