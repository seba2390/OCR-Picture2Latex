

\section{Implementation Details for Image Classification}
We evaluate our doubly robust self-training method on the ImageNet100 and mini-ImageNet100 datasets, which are subsets of ImageNet-1k from ImageNet Large Scale Visual Recognition Challenge 2012~\citep{russakovsky2015imagenet}.
%
Two models are evaluated: (1) DaViT-T~\citep{ding2022davit}, a state-of-the-art 12-layer vision transformer architecture with a patch size of 4, a window size of 7, and an embedding dim of 768, and (2) ResNet50~\citep{he2016deep}, a classic and powerful convolutional network with 50 layers and embedding dim 2048.
%
We evaluate all the models on the same ImageNet100 validation set (50 samples per class). 
%
For the training, we use the same data augmentation and regularization strategies following the common practice in~\cite{liu2021swin,lin2017focal,ding2022davit}. We train all the models with a batch size of $1024$ on 8 Tesla V100 GPUs (the batch size is reduced to 64 if the number of training data is less than 1000). We use AdamW~\citep{loshchilov2017decoupled} optimizer and a simple triangular learning rate schedule~\citep{smith2019super}. The weight decay is set to $0.05$ and the maximal gradient norm is clipped to $1.0$. The stochastic depth drop rates are set to $0.1$ for all models. During training, we crop images randomly to $224 \times 224$, while a center crop is used during evaluation on the validation set. We use a curriculum setting where the $\alpha_t$ grows linearly or quadratically from 0 to 1 throughout the training.
%
To show the effectiveness of our method, we also compare model training with different curriculum learning settings and varying numbers of epochs.


% {\color{red} TODO: add some details about the architectures}


\section{Additional Experiments in Image Classification}

\begin{table}[!htbp]
\footnotesize
\centering
\caption{Ablation study on different curriculum settings on ImageNet-100. All models are trained in 20 epochs.}
\setlength{\tabcolsep}{8pt}
\renewcommand\arraystretch{1.2}
\resizebox{1\linewidth}{!}{
    \begin{tabular}{l|cc|cc|cc|cc}
    \shline
     \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{30\% GTs} & \multicolumn{2}{c|}{50\% GTs} & \multicolumn{2}{c|}{70\% GTs} & \multicolumn{2}{c}{90\% GTs} \\
     & top1 & top5 & top1 & top5 & top1 & top5 & top1 & top5 \\
    \shline
    Naive Labeled + Pseudo  & 28.01 & 54.63 & 37.6 & 66.72 & 43.76 & 73.42 & 47.74 & 77.15 \\
    % \hline
    % Prob  & 28.69 & 55.23 & 38.7 & 68.32 & 44.28 & 74.24 & 47.88 & 77.05\\
    % Prob + Curriculum (linear) & 29.31	& 55.91	&	39.16	&68.01	&	44.4&	73.4	&	47.44&	76.03 \\
    % \hline
    doubly robust, $\alpha_t=1$  &28.43&	56.65	&	38.06	&67.18	&	43.22&	73.18	&	48.52	&77.21\\
    doubly robust, $\alpha_t=t/T$ (linear) & 30.87	& 60.98 & 40.18 &	71.06  &\textbf{46.60}&	\textbf{75.80}& \textbf{50.44}	&\textbf{78.88} \\
    doubly robust, $\alpha_t=(t/T)^2$ (quadratic) & \textbf{31.15} &	\textbf{61.29}	&	\textbf{40.86}	&\textbf{71.14}	&	45.50	&75.11		&49.64&	77.77\\
    \shline
    \end{tabular}}
    \label{tab:classification_ablation}
    % \vspace{-8pt}
\end{table}


\noindent \textbf{Ablation study on curriculum settings.}
There are three options for the curriculum setting: 
1) $\alpha_t=1$ throughout the whole training,
2) grows linearly with training iterations $\alpha_t=t/T$,
3) grows quadratically with training iterations $\alpha_t=(t/T)^2$.
From results in Table~\ref{tab:classification_ablation}, we see:
the first option achieves comparable performance with the `Naive Labeled + Pseudo' baseline.
Both the linear and quadratic strategies show significant performance improvements: the linear one works better when more labeled data is available, e.g., 70\% and 90\%, while the quadratic one prefers less labeled data, e.g. 30\% and 50\%.


\begin{table}[!htbp]
\footnotesize
\centering
\caption{Ablation study on the number of epochs. All models are trained using 10\% labeled data on ImageNet-100.}
\setlength{\tabcolsep}{10pt}
\renewcommand\arraystretch{1.2}
\resizebox{1\linewidth}{!}{
    \begin{tabular}{c|cc|cc|cc|cc}
    \shline
     \multirow{2}{*}{Training epochs} & \multicolumn{2}{c|}{Labeled Only} & \multicolumn{2}{c|}{Pseudo Only} & \multicolumn{2}{c|}{Labeled + Pseudo} & \multicolumn{2}{c}{doubly robust Loss} \\
     & top1 & top5 & top1 & top5 & top1 & top5 & top1 & top5 \\
    \shline
20	&16.02	&39.68	&	17.02	&38.64	&	19.38&	41.96		& \textbf{21.88}	& \textbf{47.18} \\
50	& 25.00	& 51.21	&	28.90	&53.74	&	30.36	& 57.04		& \textbf{36.65}	& \textbf{65.68} \\
100 	& 35.57& 	64.66	&	44.43	&\textbf{71.56}	&	42.44	& 68.94	& \textbf{45.98} 
& {70.66} \\
    \shline
    \end{tabular}}
    \label{tab:classification_epoch}
    % \vspace{-8pt}
\end{table}


\noindent \textbf{Ablation Study on the Number of Epochs.}
We conduct experiments on different training epochs. The results are shown in Table~\ref{tab:classification_epoch}. Our model is consistently superior to the baselines. And we can observe the gain is larger when the number of training epochs is relatively small, e.g. 20 and 50.

\noindent \textbf{Fully trained results (300 epochs) on ImageNet-100.}
In our original experiments, we mostly focus on a teacher model that is not super accurate, since our method reduces to the original pseudo-labeling when the teacher model is completely correct for all labels. In this experiment, we fully train the teacher model with 300 epochs on ImageNet-100, leading to the accuracy of the teacher model as high as 88.0\%. From Figure~\ref{fig:rebuttal}, we show that even in this case, our method outperforms the original pseudo-labeling baseline.

\begin{figure}[!htbp]
  \centering
  \caption{Results on ImageNet-100 using fully trained (300 epochs) DaViT-T with different data fractions.}
  \includegraphics[width=0.6\textwidth]{figures/rebuttal.pdf}
  \label{fig:rebuttal}
\end{figure}


\noindent \textbf{Comparisons with previous SOTAs on CIFAR-10 and CiFAR-100.}
We compare with another 11 baselines in terms of error rate on CIFAR-10-4K and CIFAR-100-10K under the same settings (i.e., Wide ResNet-28-2 for CIFAR-10 and WRN-28-8 for CIFAR-100). We show that our method is only 0.04 inferior to the best method Meta Pseudo Labels for CIFAR-10-4K, and achieves the best performance for CIFAR-100-10K.

\begin{table}[!htbp]
    \centering
    \caption{Comparisons with previous SOTAs on CiFAR-10 and CIFAR-100.}
    \setlength{\tabcolsep}{12pt}
    \renewcommand\arraystretch{1.2}
    \begin{tabular}{c|c|c}
    \shline
Method    &   CIFAR-10-4K  (error rate, \%) &   CIFAR-100-10K  (error rate, \%) \\
\shline
 Pseudo-Labeling    & 16.09               & 36.21                 \\
 LGA + VAT          & 12.06               & --                    \\
 Mean Teacher       & 9.19                & 35.83                 \\
 ICT                & 7.66                & --                    \\
 SWA                & 5.00                & 28.80                 \\
 MixMatch           & 4.95                & 25.88                 \\
 ReMixMatch         & 4.72                & 23.03                 \\
 EnAET              & 5.35            & --                    \\
 UDA                & 4.32                & 24.50                 \\
 FixMatch           & 4.31                & 23.18                 \\
 Meta Pesudo Labels & \textbf{3.89}            & --                    \\
 \hline
\textbf{Ours}           & 3.93                & \textbf{22.30}             \\
   \shline
    \end{tabular}
    \label{tab:exp2}
\end{table}


\section{Implementation Details of 3D Object Detection}
Our experiments follow the standard approach for semi-supervised detection: we first initialize two detectors, the teacher (i.e., labeler) and the student. First, a random split of varying sizes is selected from the nuScenes training set. We pre-train the teacher network using the ground-truth annotations in this split. Following this, we freeze the weights in the teacher model and then use it to generate pseudo-labels on the entire training set. The student network is then trained on a combination of the pseudo-labels and ground-truth labels originating from the original split. In all of our semi-supervised experiments, we use CenterPoint with a PointPillars backbone as our 3D detection model \citep{yin2021center,Lang2019PointPillarsFE}. The teacher pre-training and student training are both conducted for 10 epochs on 3 NVIDIA RTX A6000 GPUs. We follow the standard nuScenes training setting outlined in \cite{zhu2019class}, with the exception of disabling ground-truth paste augmentation during training to prevent data leakage from the labeled split. To select the pseudo-labels to be used in training the student, we simply filter the teacher predictions by detection confidence, using all detections above a chosen threshold. We use a threshold of 0.3 for all classes, as in \cite{park2022detmatch}. In order to conduct training in a batch-wise manner, we compute the loss over only the samples contained within the batch. We construct each batch to have a consistent ratio of labeled/unlabeled samples to ensure the loss is well-defined for the batch. 

\section{Additional Experiments in 3D Object Detection}

\textbf{Comparison with Semi-Supervised Baseline} We compare our approach to another semi-supervised baseline on the 3D object detection task, Pseudo-labeling and confirmation bias \cite{pseudoLabel2019}. We shows that in multiple settings, our approach surpasses the baseline performance.\\
\textbf{Ablation on Pseudo-label Confidence Threshold} To demonstrate that appropriate quality pseudo-labels are used to train the student detector, we performance ablation experiments varying the detection threshold used to extract pseudo-labels from the teacher model predictions. We show that training with a  threshold of $\tau=0.3$ outperforms training with a more stringent threshold, and is the appropriate experimental setting for our main experiments. 

\begin{table}[!htbp]
% \footnotesize
% \setlength{\tabcolsep}{13pt}
% \renewcommand\arraystretch{1.05}
\centering
\caption{Performance comparison with pseudo-labeling baseline on nuScenes \textit{val} set.}
% \resizebox{1\linewidth}{!}{
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{c |c  c | c  c | c  c | c  c }
    \hline
    \multirow{2}{*}{Labeled  Fraction} & \multicolumn{2}{c|}{Labeled Only} & \multicolumn{2}{c|}{Labeled + Pseudo} & \multicolumn{2}{c|}{Doubly robust Loss}
    & \multicolumn{2}{c}{Pseudo-Labeling + Confirmation Bias}\\
     & mAP$\uparrow$ & NDS$\uparrow$ & mAP$\uparrow$ & NDS$\uparrow$ & mAP$\uparrow$ & NDS$\uparrow$ & mAP$\uparrow$ & NDS$\uparrow$\\
     \hline
     1/24 & 7.56 & 18.01 & 7.60 & 17.32 & \textbf{8.18} & \textbf{18.33} & 7.80 & 16.86\\
     1/16 & 11.15 & 20.55 & 11.60 & 21.03 & \textbf{12.30} & {22.10} & 12.15 & \textbf{22.89}\\
     \hline
    \end{tabular}
    \end{adjustbox}
\vspace{-6pt}
\label{tab:confirmation_bias}
\end{table}
\begin{table}[!htbp]
% \footnotesize
% \setlength{\tabcolsep}{13pt}
% \renewcommand\arraystretch{1.05}
\centering
\caption{Doubly Robust Loss performance comparison with differing detection thresholds for pseudo-labels.}
%\resizebox{1\linewidth}{!}{
    \begin{adjustbox}{width=\textwidth}
    \begin{tabular}{c |c  c  c  c | c  c  c  c}
    \hline
    \multirow{3}{*}{Labeled Data Fraction} & \multicolumn{4}{c|}{$\tau = 0.3$} & \multicolumn{4}{c}{$\tau = 0.5$}\\
     & \multicolumn{2}{c}{Labeled+Pseudo} & \multicolumn{2}{c|}{Doubly Robust Loss} & \multicolumn{2}{c}{Labeled+Pseudo} & \multicolumn{2}{c}{Doubly Robust Loss} \\   
     & mAP$\uparrow$ & NDS$\uparrow$ & mAP$\uparrow$ & NDS$\uparrow$ & mAP$\uparrow$ & NDS$\uparrow$ & mAP$\uparrow$ & NDS$\uparrow$ \\
     \hline
     1/24 & 7.56 & 18.01 & \textbf{8.18} & \textbf{18.33} & 7.15 & 15.82 & 4.37 & 13.17\\
     1/16 & 11.15 & 20.55 & \textbf{12.30} & \textbf{22.10} & 11.05 & 21.22 & 8.09 & 19.70 \\
     \hline
    \end{tabular}
    \end{adjustbox}
\vspace{-6pt}
\end{table}


\section{Considerations when $\hat f$ is trained from labeled data}\label{app:split}

In Theorem~\ref{thm:general}, we analyzed the double robustness of the proposed loss function when the predictor $\hat f$ is pre-existing and not trained from the labeled dataset. In practice, one may only have access to the labeled and unlabeled datasets without a pre-existing teacher model. In this case, one may choose to split the labeled samples $\mathcal{D}_2$ into two parts. The last  $n/2$ samples are used to  train $\hat f$, and the first $n/2$ samples are used in the doubly robust loss:
\begin{align*}
\mathcal{L}^{\mathsf{DR2}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m}  \sum_{i=1}^{m} \ell_\theta(X_i, \hat f(X_i)) -  \frac{2}{n} \sum_{i=m+1}^{m+n/2} \frac{1}{\pi(X_i)}\ell_\theta(X_i, \hat f(X_i))  + \frac{2}{n} \sum_{i=m+1}^{m+n/2} \frac{1}{\pi(X_i)}\ell_\theta(X_i, Y_i). 
\end{align*}
Since $\hat f$ is independent of all samples used in the above loss, the result in Theorem~\ref{thm:general} continues to hold. Asymptotically, such a doubly robust estimator is never worse than the estimator trained  only on the labeled data.
\section{Proof of Proposition~\ref{prop:mean_upper}}\label{proof:mean_upper}
For the labeled-only estimator $\hat \theta_{\mathsf{TL}}$, we have
 \begin{align*}
    \mathbb{E}[(\theta^\star -  \hat \theta_{\mathsf{TL}})^2] & = \mathbb{E}\left[\left(\mathbb{E}[Y] - \frac{1}{n} \sum_{i=m+1}^{m+n} Y\right)^2\right] = \frac{1}{n} \mathsf{Var}[Y].
\end{align*}
For the self-training loss, we have
  \begin{align*} 
      \mathbb{E}[(\theta^\star -  \hat \theta_{\mathsf{SL}})^2] &  =  \mathbb{E}\left[\left(\mathbb{E}[Y] - \frac{1}{m+n}\left(  \sum_{i=1}^m \hat f(X_i)+ \sum_{i=m+1}^{m+n} Y_i\right)\right)^2\right] \\
      & \leq 2 \left(\mathbb{E}\left[\left(\frac{m}{m+n}\left(\mathbb{E}[Y] - \frac{1}{m} \sum_{i=1}^m \hat f(X_i)\right)\right)^2\right] +\mathbb{E}\left[\left(\frac{n}{m+n}\left(\mathbb{E}[Y] - \frac{1}{n} \sum_{i=m+1}^{m+n} Y_i \right) \right)^2\right] \right) \\
      & \leq \frac{2m^2}{(m+n)^2} \mathbb{E}[(\hat f(X)-Y)]^2 + \frac{2m}{(m+n)^2}\Var[{\hat f(X)-Y}]  + \frac{2n}{(m+n)^2} \mathsf{Var}[Y].  
\end{align*} 
For the doubly robust loss, on one hand, we have
\begin{align*}
       \mathbb{E}[(\theta^\star -  \hat \theta_{\mathsf{DR}})^2] & = 
 \mathbb{E}\left[\left(\mathbb{E}[Y] -   \frac{1}{m+n}  \sum_{i=1}^{m+n} \hat f(X_i) + \frac{1}{n}  \sum_{i=m+1}^{m+n} ( \hat f(X_i)-Y_i)\right)^2\right] \\ 
 & \leq 2  \mathbb{E}\left[\left(\mathbb{E}[Y] - \frac{1}{n} \sum_{i=m+1}^{m+n}   Y_i\right)^2\right] + 2  \mathbb{E}\left[\left(\mathbb{E}[\hat f(X)] - \frac{1}{n} \sum_{i=m+1}^{m+n}   \hat f(X_i)\right)^2\right]\\ 
 & \quad  +  2  \mathbb{E}\left[\left(\mathbb{E}[\hat f(X)] - \frac{1}{m+n} \sum_{i=1}^{m+n}   \hat f(X_i)\right)^2\right] \\
 & = \frac{2}{n} \mathsf{Var}[Y]+ \left(\frac{2}{m+n } + \frac{2}{n }\right)\mathsf{Var}[\hat f(X)].
\end{align*}
On the other hand, we have
\begin{align*}
       \mathbb{E}[(\theta^\star -  \hat \theta_{\mathsf{DR}})^2] & = 
 \mathbb{E}\left[\left(\mathbb{E}[Y] -   \frac{1}{m+n}  \sum_{i=1}^{m+n} \hat f(X_i) + \frac{1}{n}  \sum_{i=m+1}^{m+n} ( \hat f(X_i)-Y_i)\right)^2\right] \\ 
 & \leq 2  \mathbb{E}\left[\left(\mathbb{E}[Y] - \frac{1}{m+n} \sum_{i=1}^{m+n}   Y_i\right)^2\right] + 2  \mathbb{E}\left[\left(\mathbb{E}[\hat f(X) - Y] - \frac{1}{n} \sum_{i=m+1}^{m+n}  ( \hat f(X_i)-Y_i)\right)^2\right]\\ 
 & \quad  +  2  \mathbb{E}\left[\left(\mathbb{E}[\hat f(X)-Y] - \frac{1}{m+n} \sum_{i=1}^{m+n}   (\hat f(X_i)-Y_i)\right)^2\right] \\
 & = \left(\frac{2}{m+n}+ \frac{2}{n}\right)\Var[{\hat f(X)-Y}]  + \frac{2}{m+n} \mathsf{Var}[Y] .
\end{align*}
The proof is done by taking the minimum of the two upper bounds.
\section{Proof of Theorem~\ref{thm:general}}\label{proof:general_guarantee}


\begin{proof}
We know that 
\begin{align*}
& \|    \nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) - \mathbb{E}[
    \nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star)] \|_2 \\ 
      = &\Big\| \frac{1}{m+n}  \sum_{i=1}^{m+n} (\nabla_\theta \ell_{\theta^\star}(X_i, \hat f(X_i)) -  \mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, \hat f(X))] )+  \frac{1}{n} \sum_{i=m+1}^{m+n}\Big (\nabla_\theta \ell_{\theta^\star}(X_i, Y_i)-\nabla_\theta \ell_{\theta^\star}(X_i, \hat f(X_i))   \\ 
      & - \mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, Y)-\nabla_\theta \ell_{\theta^\star}(X, \hat f(X)) ]\Big) \Big\|_2 \\ 
    \leq    &\Big\| \frac{1}{m+n}  \sum_{i=1}^{m+n} (\nabla_\theta \ell_{\theta^\star}(X_i, \hat f(X_i)) -  \mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, \hat f(X))] )  \Big\|_2+   \Big\|\frac{1}{n} \sum_{i=m+1}^{m+n}\Big (\nabla_\theta \ell_{\theta^\star}(X_i, Y_i)-\nabla_\theta \ell_{\theta^\star}(X_i, \hat f(X_i))   \\ 
      & - \mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, Y)-\nabla_\theta \ell_{\theta^\star}(X, \hat f(X)) ]\Big) \Big\|_2.
\end{align*}

From the multi-dimensional Chebyshev inequality~\citep{bibby1979multivariate, marshall1960multivariate}, we have that with probability at least $1-\delta/2$, for some universal constant $C$, 
\begin{align*}
    \Big\| \frac{1}{m+n}  \sum_{i=1}^{m+n} (\nabla_\theta \ell_{\theta^\star}(X_i, \hat f(X_i)) -  \mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, \hat f(X))] )  \Big\|_2 \leq C \|\Sigma_{\theta^\star}^{\hat f}\|_2\sqrt{\frac{d}{(m+n) \delta}}. 
\end{align*}
Similarly, we also have that with probability at least $1-\delta/2$, 
\begin{align*}
   & \Big\|\frac{1}{n} \sum_{i=m+1}^{m+n}\Big (\nabla_\theta \ell_{\theta^\star}(X_i, Y_i)-\nabla_\theta \ell_{\theta^\star}(X_i, \hat f(X_i))    - \mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, Y)-\nabla_\theta \ell_{\theta^\star}(X, \hat f(X)) ]\Big) \Big\|_2  
   \leq  C \|\Sigma_{\theta^\star}^{Y-\hat f}\|_2\sqrt{\frac{d}{n \delta}}.
\end{align*}
Furthermore, note that 
\begin{align*}
  \mathbb{E}[
    \nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star)]= \mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, Y)] = \nabla_\theta \mathbb{E}[\ell_{\theta^\star}(X, Y)] = 0.
\end{align*}
Here we use Assumption~\ref{ass:diff} and Assumption~\ref{ass:mom} to ensure that the expectation and differentiation are interchangeable. Thus we have that with probability at least $1-\delta$, 
\begin{align*}
  \| \nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) \|_2 \leq C   \left(\|\Sigma_{\theta^\star}^{\hat f}\|_2\sqrt{\frac{d}{(m+n) \delta}} + \|\Sigma_{\theta^\star}^{Y-\hat f}\|_2\sqrt{\frac{d}{n \delta}}\right).
\end{align*}

On the other hand, we can also write the difference as
\begin{align*}
& \|    \nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) - \mathbb{E}[
    \nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star)] \|_2 \\ 
      = &\Big\| \frac{1}{m+n}  \sum_{i=1}^{m+n} (\nabla_\theta \ell_{\theta^\star}(X_i, \hat f(X_i)) -  \mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, \hat f(X))] )+  \frac{1}{n} \sum_{i=m+1}^{m+n}\Big (\nabla_\theta \ell_{\theta^\star}(X_i, Y_i)-\mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, Y)]\Big)   \\ 
      & -   \frac{1}{n} \sum_{i=m+1}^{m+n}\Big (\nabla_\theta \ell_{\theta^\star}(X_i, Y_i) - \mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, \hat f(X)) ]\Big) \Big\|_2 \\ 
      \leq  &\Big\| \frac{1}{m+n}  \sum_{i=1}^{m+n} (\nabla_\theta \ell_{\theta^\star}(X_i, \hat f(X_i)) -  \mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, \hat f(X))] ) \Big\|_2+ \Big\| \frac{1}{n} \sum_{i=m+1}^{m+n}\Big (\nabla_\theta \ell_{\theta^\star}(X_i, Y_i)-\mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, Y)]\Big)  \Big\|_2 \\ 
      & + \Big\| \frac{1}{n} \sum_{i=m+1}^{m+n}\Big (\nabla_\theta \ell_{\theta^\star}(X_i, Y_i) - \mathbb{E}[\nabla_\theta \ell_{\theta^\star}(X, \hat f(X)) ]\Big) \Big\|_2  \\
      \leq &  C \left(\|\Sigma_{\theta^\star}^{\hat f}\|_2\left(\sqrt{\frac{d}{(m+n) \delta}} + \sqrt{\frac{d}{n\delta}} \right) + \|\Sigma_{\theta^\star}^{Y}\|_2\sqrt{\frac{d}{n \delta}}\right).
\end{align*}
Here the last inequality uses the multi-dimensional Chebyshev inequality and it holds with probability at least $1-\delta$. This finishes the proof.
\end{proof}

\section{Proof of Proposition~\ref{prop:dr_mis}}\label{proof:dr_mis}
\begin{proof}
We have
\begin{align*}
\mathbb{E}[\mathcal{L}^{\mathsf{DR2}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) ]
& = \frac{1}{m}  \sum_{i=1}^{m} \mathbb{E}_{X_i\sim\mathbb{P}_X}[\ell_\theta(X_i, \hat f(X_i))] -  \frac{1}{n} \sum_{i=m+1}^{m+n}  \mathbb{E}_{X_i\sim\mathbb{Q}_X}\left[\frac{1}{\pi(X_i)}\ell_\theta(X_i, \hat f(X_i))\right] \\
& \quad  + \frac{1}{n}  \sum_{i=m+1}^{m+n} \mathbb{E}_{X_i\sim\mathbb{Q}_X, Y_i\sim \mathbb{P}_{Y|X_i}}\left[\frac{1}{\pi(X_i)}\ell_\theta(X_i, Y_i)\right] \\
& = \mathbb{E}_{X\sim\mathbb{P}_X}[\ell_\theta(X, \hat f(X))] - \mathbb{E}_{X\sim\mathbb{Q}_X}\left[\frac{1}{\pi(X)}\ell_\theta(X, \hat f(X))\right] \\
& \quad  +  \mathbb{E}_{X\sim\mathbb{Q}_X, Y\sim \mathbb{P}_{Y|X}}\left[\frac{1}{\pi(X)}\ell_\theta(X, Y)\right].
\end{align*}
In the first case when $\pi(x) \equiv \frac{\mathbb{P}_X(x)}{\mathbb{Q}_X(x)}$, we have
\begin{align*}
\mathbb{E}[\mathcal{L}^{\mathsf{DR2}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) ]
& = \mathbb{E}_{X\sim\mathbb{P}_X}[\ell_\theta(X, \hat f(X))] - \mathbb{E}_{X\sim\mathbb{Q}_X}\left[\frac{\mathbb{P}_X(X)}{\mathbb{Q}_X(X)}\ell_\theta(X, \hat f(X))\right] \\
& \quad  +  \mathbb{E}_{X\sim\mathbb{Q}_X, Y\sim \mathbb{P}_{Y|X}}\left[\frac{\mathbb{P}_X(X)}{\mathbb{Q}_X(X)}\ell_\theta(X, Y)\right] \\
& = \mathbb{E}_{X\sim\mathbb{P}_X}[\ell_\theta(X, \hat f(X))] - \mathbb{E}_{X\sim\mathbb{P}_X}\left[\ell_\theta(X, \hat f(X))\right] \\
& \quad  +  \mathbb{E}_{X\sim\mathbb{P}_X, Y\sim \mathbb{P}_{Y|X}}\left[\ell_\theta(X, Y)\right] \\
& = \mathbb{E}_{X, Y\sim\mathbb{P}_{X, Y}}\left[\ell_\theta(X, Y)\right].
\end{align*}
In the second case when $\ell_\theta(x, \hat f(x))  = \mathbb{E}_{ Y\sim \mathbb{P}_{Y\mid X=x}}[\ell_\theta(x, Y)]$, we have
\begin{align*}
\mathbb{E}[\mathcal{L}^{\mathsf{DR2}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) ]
& = \mathbb{E}_{X\sim\mathbb{P}_X}[\ell_\theta(X, \hat f(X))] - \mathbb{E}_{X\sim\mathbb{Q}_X}\left[\frac{1}{\pi(X)}\ell_\theta(X, \hat f(X))\right] \\
& \quad  +  \mathbb{E}_{X\sim\mathbb{Q}_X}\mathbb{E}_{Y\sim \mathbb{P}_{Y|X}}\left[\frac{1}{\pi(X)}\ell_\theta(X, Y) \mid X\right] \\
& = \mathbb{E}_{X\sim\mathbb{P}_X}[\ell_\theta(X, \hat f(X))] - \mathbb{E}_{X\sim\mathbb{Q}_X}\left[\frac{1}{\pi(X)}\ell_\theta(X, \hat f(X))\right] \\
& \quad  +  \mathbb{E}_{X\sim\mathbb{Q}_X}\left[\frac{1}{\pi(X)}\ell_\theta(X, \hat f(X))  \right] \\
& = \mathbb{E}_{X\sim\mathbb{P}_X}[\ell_\theta(X, \hat f(X))] \\ 
& = \mathbb{E}_{X, Y\sim\mathbb{P}_{X, Y}}\left[\ell_\theta(X, Y)\right].
\end{align*}

This finishes the proof.
\end{proof}