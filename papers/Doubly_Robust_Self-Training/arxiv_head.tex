\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{babel}
\usepackage{verbatim}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{adjustbox}

\usepackage{bm}
\usepackage{enumitem}
 
\usepackage{babel}
\usepackage{subcaption}
\usepackage{algorithmic}
 
\usepackage{amsthm}
\usepackage{bbm}

\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xspace}
\newlength\savewidth
\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\my}[1]{\textcolor{red}{[Mingyu: #1]}}
\newcommand{\banghua}[1]{\textcolor{red}{[Banghua: #1]}}

\definecolor{citecolor}{HTML}{0071BC}
\hypersetup{colorlinks,linkcolor={red},citecolor={citecolor}}

\newtheorem{lemma}{\textbf{Lemma}}
\newtheorem{theorem}{\textbf{Theorem}}\setcounter{theorem}{0}
\newtheorem{corollary}{\textbf{Corollary}}
\newtheorem{assumption}{\textbf{Assumption}}
\newtheorem{example}{\textbf{Example}}
\newtheorem{definition}{\textbf{Definition}}
\newtheorem{fact}{\textbf{Fact}}
\newtheorem{proposition}{\textbf{Proposition}}\setcounter{theorem}{0}

\theoremstyle{definition}
\newtheorem{remark}{\textbf{Remark}}
\newtheorem{condition}{Condition}
\newtheorem{claim}{\textbf{Claim}}
\usepackage{natbib} 
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}} %Citation-related commands
\makeatother

\input{macro.tex}
\title{Doubly Robust  Self-Training}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{Banghua Zhu, Mingyu Ding, Philip Jacobson, Ming Wu, \\ 
Wei Zhan, Michael I. Jordan, Jiantao Jiao\thanks{Banghua Zhu, Mingyu Ding, Philip Jacobson, Ming Wu,
Wei Zhan, Michael I. Jordan, Jiantao Jiao are with the Department of Electrical Engineering and Computer Sciences, University of California, Berkeley. Email: \{banghua, myding, philip\_jacobson, mingwu, wzhan, jordan, jiantao\}@berkeley.edu.}}


\begin{document}


\maketitle


\begin{abstract}
Self-training is an important technique for solving semi-supervised learning problems.  It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
\end{abstract}


\input{self_labeling}
\newpage
\bibliography{ref}
\newpage 
\appendix
\input{appendix}
\end{document}