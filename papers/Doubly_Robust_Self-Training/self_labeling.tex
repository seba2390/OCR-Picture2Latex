\section{Introduction}

Semi-supervised learning considers the problem of learning based on a small labeled dataset together with a large unlabeled dataset. This general framework plays an important role in many problems in machine learning, including model fine-tuning, model distillation, self-training, transfer learning and continual learning~\citep{zhu2005semi,pan2010survey, weiss2016survey, gou2021knowledge, de2021continual}. Many of these problems also involve some form of distribute shift, and accordingly, to best utilize the unlabeled data, an additional assumption is that one has access to a teacher model obtained from prior training.  It is important to study the relationships among the datasets and the teacher model.  In this paper, we ask the following question:
\begin{quote}
    \emph{Given a teacher model, a large unlabeled dataset and a small labeled dataset, how can we design a principled learning process that ensures consistent and sample-efficient learning of the true model?}
\end{quote}

Self-training is one widely adopted and popular approach in computer vision and autonomous driving for leveraging information from all three components~\citep{pseudolabel2013,berthelot2019mixmatch,berthelot2019remixmatch,sohn2020fixmatch, xie2020self, jiang2022improving, qi2021offboard}. This approach involves using a teacher model to generate pseudo-labels for all unlabeled data, and then training a new model on a mixture of both pseudo-labeled and labeled data. However, this method can lead to overreliance on the teacher model and can miss important information provided by the labeled data. As a consequence, the self-training approach becomes highly sensitive to the accuracy of the teacher model. Our study demonstrates that even in the simplest scenario of mean estimation, this method can yield significant failures when the teacher model lacks accuracy.



% In the field of computer vision and driving, self-training is a popular and straightforward solution for the above question.  % proposes a framework that combines active learning with auto-labeling to identify rare examples  for the task of  3D object detection.    During the stage of active learning, they aim at identifying rare examples rather than hard examples, since training on hard examples will not improve the model performance. They propose an estimator  of the rareness of the new example based on the flow model trained from existing dataset.    During the stage of auto-labeling, 
 

To overcome this issue, we propose an alternative method that is \emph{doubly robust}---when the covariate distribution of the unlabeled dataset and the labeled dataset matches, the estimator is always  consistent  no matter whether  the  teacher model is accurate or not. On the other hand, when the teacher model is an accurate predictor, the estimator makes full use of the pseudo-labeled dataset and greatly increases the effective sample size. The idea is inspired by and directly related to missing-data inference and causal inference~\citep{rubin1976inference, kang2007demystifying, birhanu2011doubly, ding2018causal},  to semiparametric mean estimation~\citep{zhang2019semi}, and to recent work on prediction-powered inference~\citep{angelopoulos2023prediction}. 

\subsection{Main results}
The proposed algorithm is based on a simple modification of the standard loss for self-training.   Assume that we are given a set of unlabeled samples, $\mathcal{D}_1 = \{X_1,X_2,\cdots, X_m\}$, drawn from a fixed distribution $\mathbb{P}_X$, a set of labeled samples $\mathcal{D}_2 =\{(X_{m+1}, Y_{m+1}), (X_{m+2}, Y_{m+2}), \cdots, (X_{m+n}, Y_{m+n})\}$, drawn from some joint distribution $\mathbb{P}_X\times \mathbb{P}_{Y|X}$, and a teacher model  $\hat f$. Let $\ell_\theta(x, y)$ be a pre-specified loss function that characterizes the prediction error of the estimator with parameter $\theta$ on the given sample $(X, Y)$. Traditional self-training aims at minimizing the combined loss for both labeled and unlabeled samples, where the pseudo-labels for unlabeled samples are generated using $\hat f$:
\begin{align*}
\mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \left(\sum_{i=1}^m \ell_\theta(X_i, \hat f(X_i)) + \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i)\right). %\\
% & = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  + \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i). 
\end{align*}
Note that this can also be viewed as first using $\hat f$ to predict all the data, and then replacing the originally labeled points with the known labels: 
\begin{align*}
\mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
 & = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  + \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i). 
\end{align*}
Our proposed doubly robust loss instead replaces the coefficient $1/(m+n)$ with $1/n$ in the last two terms:
\begin{align*}
\mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  + \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i). 
\end{align*}
This seemingly minor change has a major beneficial effect---the estimator becomes consistent and doubly robust. 
\begin{theorem}[Informal]
    Let $\theta^\star$ be defined as the minimizer  $\theta^\star = \argmin_{\theta} \mathbb{E}_{(X, Y)\sim \mathbb{P}_{X}\times \mathbb{P}_{Y|X}}[\ell_\theta(X, Y)]$. Under certain regularity conditions, we have 
    \begin{align*}
  \| \nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) \|_2 \lesssim  
  \begin{cases}
  \sqrt{\frac{d}{m+n}}, & \text{ when } Y \equiv \hat f(X), \\
  \sqrt{\frac{d}{n}}, & \text{otherwise}. 
  \end{cases}
\end{align*}  
On the other hand, there exists instances such that $  \| \nabla_\theta \mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) \|_2\geq C$ always holds true no matter how large $m, n$ are. 
\end{theorem} 
The result shows that the true parameter $\theta^\star$ is also a local minimum of the doubly robust loss, but not a local minimum of the original self-training loss.  We flesh out this comparison for the special example of mean estimation in Section~\ref{sec:mean}, and present empirical results on image and driving datasets in  Section~\ref{sec:empirical}.


\subsection{Related work}

\textbf{Missing-data inference and causal inference.} 
The general problem of causal inference can be formulated as a missing-data inference problem as follows. For each unit in an experiment, at most one of the potential outcomes---the one corresponding to the treatment to
which the unit is exposed---is observed, and the other
potential outcomes are viewed as missing~\citep{holland1986statistics, ding2018causal}. Two of the standard methods for solving this problem are data imputation~\cite{rubin1979using} and propensity
score weighting~\cite{rosenbaum1983central}. A doubly robust causal inference estimator combines
the virtues of these two methods. The estimator is referred to as ``doubly robust'' due to the following property: if the model for imputation is correctly specified then it is  consistent  no matter whether the propensity score model is correctly specified; on the other hand, if the model propensity score model is correctly specified, then it is consistent no matter whether the model for imputation is correctly specified~\citep{scharfstein1999adjusting,  bang2005doubly, birhanu2011doubly, ding2018causal}. 

We note in passing that double machine learning is another methodology that is inspired by the doubly robust paradigm in causal inference~\citep{semenova2017estimation, chernozhukov2018double, chernozhukov2018biased, foster2019orthogonal}. The  problem in double machine learning is related to the classic semiparametric problem of inference for a low-dimensional parameter in the presence of high-dimensional nuisance parameters, which is different goal than the predictive goal characterizing semi-supervised learning. 

The recent work of prediction-powered inference~\citep{angelopoulos2023prediction} focuses on confidence estimation when there are both unlabeled data, labeled data, along with a teacher model. Their focus is the inferential problem of obtaining a confidence set, while ours is the doubly robust property of a point estimator.  Since they focus on confidence estimation, an important, strong, yet biased baseline point-estimate algorithm that directly combines the ground-truth labels and pseudo-labels is not considered in their case. In our paper, we show with both theory and experiments that the proposed doubly-robust estimator achieves better performance than the naive combination of ground-truth labels and pseudo-labels. 

\vspace{0.1in}

\textbf{Self-training.}
Self-training is a popular semi-supervised learning paradigm in which machine-generated pseudo-labels are used for training with unlabeled data \citep{pseudolabel2013,berthelot2019mixmatch,berthelot2019remixmatch,sohn2020fixmatch, zhao2023towards}. To generate these pseudo-labels, a teacher model is pre-trained on a set of labeled data, and its predictions on the unlabeled data are extracted as pseudo-labels. Previous work seeks to address the noisy quality of pseudo-labels in various ways. MixMatch \citep{berthelot2019mixmatch} ensembles pseudo-labels across several augmented views of the input data. ReMixMatch \citep{berthelot2019remixmatch} extends this by weakly augmenting the teacher inputs and strongly augmenting the student inputs. FixMatch \citep{sohn2020fixmatch} uses confidence thresholding to select only high-quality pseudo-labels for student training.

Self-training has  been applied in  both 2D computer vision problems~\citep{liu2021unbiased,NEURIPS2019_d0f4dae8,Tang2021HumbleTT,sohn2020detection,zhou2022} and 3D problems~\citep{park2022detmatch,wang20213dioumatch,li2023dds3d,liu2023hierarchical} object detection. STAC \citep{sohn2020detection} enforces consistency between strongly augmented versions of confidence-filtered pseudo-labels. Unbiased teacher \citep{liu2021unbiased} updates the teacher during training with an exponential moving average (EMA) of the student network weights. Dense Pseudo-Label \citep{zhou2022} replaces box pseudo-labels with the raw output features of the detector to allow the student to learn richer context. In the 3D domain, 3DIoUMatch \citep{wang20213dioumatch} thresholds pseudo-labels using a model-predicted Intersection-over-Union (IoU). DetMatch \citep{park2022detmatch} performs detection in both the 2D and 3D domains and filters pseudo-labels based on 2D-3D correspondence. HSSDA \citep{liu2023hierarchical} extends strong augmentation during training with a patch-based point cloud shuffling augmentation. Offboard3D \citep{qi2021offboard} utilizes multiple frames of temporal context to improve pseudo-label quality.

There has been a limited amount of theoretical analysis of these methods, focusing on semi-supervised methods for mean estimation and linear regression~\citep{zhang2019semi, azriel2022semi}. Our analysis bridges the gap between these analyses and the doubly robust estimators in the causal inference literature. 

 

\section{Doubly Robust Self-Training}

We begin with the case where the marginal distributions for the covariates of the labeled and unlabeled datasets are the same. 
Assume that we are given a set of unlabeled samples, $\mathcal{D}_1 = \{X_1,X_2,\cdots, X_m\}$, drawn from a fixed distribution $\mathbb{P}_X$ supported on $\mathcal{X}$, a set of labeled samples $\mathcal{D}_2 =\{(X_{m+1}, Y_{m+1}), (X_{m+2}, Y_{m+2}), \cdots, (X_{m+n}, Y_{m+n})\}$, drawn from some joint distribution $\mathbb{P}_X\times \mathbb{P}_{Y|X}$ supported on $\mathcal{X}\times\mathcal{Y}$, and a pre-trained model,  $\hat f:\mathcal{X}\mapsto \mathcal{Y}$. Let $\ell_\theta(\cdot, \cdot):\mathcal{X}\times\mathcal{Y}\mapsto \mathbb{R}$ be a pre-specified loss function that characterizes the prediction error of the estimator with parameter $\theta$ on the given sample $(X, Y)$. 
Our target is to find some $\theta^\star\in\Theta$ that satisfies
\begin{align*}
    \theta^\star \in \argmin_{\theta\in\Theta} \mathbb{E}_{(X, Y)\sim \mathbb{P}_{X}\times \mathbb{P}_{Y|X}}[\ell_\theta(X,Y)].
\end{align*}
For a given loss $\ell_\theta(x, y)$, consider a naive estimator that ignores the predictor $\hat f$ and only trains on the labeled samples:
\begin{align*}
\mathcal{L}^{\mathsf{TL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{n}    \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i).  
\end{align*}
Although naive, this is a safe choice since it is an empirical risk minimizer. As $n\rightarrow \infty$, the loss converges to the population loss. However, it ignores all the information provided in $\hat f$ and the unlabeled dataset, which makes it inefficient when the predictor $\hat f$ is informative.

On the other hand, traditional self-training aims at minimizing the combined loss for both labeled and unlabeled samples, where the pseudo-labels for unlabeled samples are generated using $\hat f$:\footnote{There are several variants of the traditional self-training loss. For example, \citet{xie2020self} introduce an extra weight $(m+n)/n$ on the labeled samples, and add noise to the student model; \citet{sohn2020fixmatch} use confidence thresholding to filter unreliable pseudo-labels. However, both of these alternatives still suffer from the inconsistency issue. In this paper we focus on the simplest form $\mathcal{L}^{\mathsf{SL}}$.  }
\begin{align*}
\mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \left(\sum_{i=1}^m \ell_\theta(X_i, \hat f(X_i)) + \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i)\right) \\
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  + \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i). 
\end{align*}

As is shown by the last equality, the self-training loss can be viewed as first using $\hat f$ to predict all the samples (including the labeled samples) and computing the average loss, then replacing that part of the loss corresponding to the labeled samples  with the loss on the original labels. Although the loss uses the information arising from the unlabeled samples and $\hat f$, the performance can be poor when the predictor is not accurate.

We propose an alternative loss, which simply replaces the weight $1/(m+n)$ in the last two terms with $1/n$:
\begin{align}
\mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  + \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i).  \label{eq:dr}
\end{align}

As we will show later,  this is a doubly robust estimator. We provide an intuitive interpretation here:
\begin{itemize}[leftmargin=24pt, itemsep=4pt]
    \vspace{-4pt}
    \item In the case when the given predictor is perfectly accurate, i.e., $\hat f(X) \equiv Y$ always holds (which also means that $Y |X=x$ is a deterministic function of $x$), the last two terms cancel, and the loss minimizes the average loss,  $\frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i))$, on all of the provided data.  The effective sample size is $m+n$, compared with effective sample size $n$ for training only on a labeled dataset using $\mathcal{L}^{\mathsf{TL}}$. In this case, the loss $\mathcal{L}^{\mathsf{DR}}$ is much better than $\mathcal{L}^{\mathsf{TL}}$, and comparable to $\mathcal{L}^{\mathsf{SL}}$. 
    
    We may as well relax the assumption of $\hat f(X) = Y$  to $\mathbb{E}[\ell_\theta(X, \hat f(X))] = \mathbb{E}[\ell_\theta(X, Y)]$. As $n$ grows larger, the loss is  approximately minimizing the average loss  $\frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i))$.
    \item On the other hand, no matter how bad  the  given predictor is,  the difference between the first two terms vanishes as either of  $m, n$ goes to infinity since the 
    labeled samples $X_{m+1},\cdots, X_{m+n}$ arise from the same distribution as $X_1,\cdots, X_m$. Thus asymptotically the loss minimizes $ \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i)$, which discards the bad predictor $\hat f$  and focuses only on the labeled dataset. Thus, in this case the loss $\mathcal{L}^{\mathsf{DR}}$ is much better than $\mathcal{L}^{\mathsf{SL}}$, and comparable to $\mathcal{L}^{\mathsf{TL}}$.
\end{itemize}
This loss is appropriate only when the covariate distributions between labeled and unlabeled samples match. In the case where there is a distribution mismatch, we propose an alternative loss; see Section~\ref{sec:mismatch}. 


\subsection{Motivating example: Mean estimation}\label{sec:mean}
 As a concrete example, in the case of one-dimensional mean estimation  we take $\ell_\theta(X, Y) = (\theta-Y)^2$. Our target is to find some $\theta^\star$ that satisfies
\begin{align*}
    \theta^\star = \argmin_{\theta} \mathbb{E}_{(X, Y)\sim \mathbb{P}_{X}\times \mathbb{P}_{Y|X}}[(\theta-Y)^2].
\end{align*}
One can   see that $\theta^\star = \mathbb{E}[Y]$. In this case, the loss for training only on labeled data  becomes
\begin{align*}
\mathcal{L}^{\mathsf{TL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) & =   \frac{1}{n}    \sum_{i=m+1}^{m+n} (\theta- Y_i)^2.
\end{align*}
Moreover, the optimal parameter is $\hat \theta_{\mathsf{TL}} =  \frac{1}{n}\sum_{i=m+1}^{m+n} Y_i$, which is a simple empirical average over all observed $Y$'s.

For a given pre-existing predictor $\hat f$, the loss for  self-training  becomes
\begin{align*}
\mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) & =   \frac{1}{m+n}  \left(\sum_{i=1}^m (\theta-\hat f(X_i))^2 + \sum_{i=m+1}^{m+n} (\theta- Y_i)^2\right).
\end{align*}
It is straightforward to see that the minimizer of the loss is the unweighted average between the unlabeled predictors $\hat f(X_i)$'s and the labeled $Y_i$'s:  $$\theta^\star_{\mathsf{SL}} = \frac{1}{m+n}\left(  \sum_{i=1}^m \hat f(X_i)+ \sum_{i=m+1}^{m+n} Y_i\right). $$
In the case of $m\gg n$, the mean estimator is almost the same as the average of all the predicted values on the unlabeled dataset, which can be far from $\theta^\star$ when the predictor $\hat f$ is inaccurate.


On the other hand, for the proposed doubly robust estimator, we have
\begin{align*}
\mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} (\theta- \hat f(X_i))^2 -  \frac{1}{n} \sum_{i=m+1}^{m+n} (\theta- \hat f(X_i))^2  + \frac{1}{n} \sum_{i=m+1}^{m+n} (\theta-  Y_i)^2  \\
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} (\theta- \hat f(X_i))^2 + \frac{1}{n} \sum_{i=m+1}^{m+n} 2 (\hat f(X_i)-Y_i)\theta + Y_i^2 - \hat f(X_i)^2.
\end{align*}
Note that the loss is still convex, and we have
\begin{align*}  
\theta^\star_{\mathsf{DR}}= \frac{1}{m+n}  \sum_{i=1}^{m+n} \hat f(X_i) - \frac{1}{n}  \sum_{i=m+1}^{m+n} ( \hat f(X_i)-Y_i).
\end{align*}
This recovers the estimator in  prediction-powered inference~\citep{angelopoulos2023prediction}. Assume that $\hat f$ is independent of the labeled data. We can calculate the mean-squared error of the three estimators as follows.


% Need another prop suggesting real meaning of doubly robust.
\begin{proposition}\label{prop:mean_upper}
Let $\Var[{\hat f(X)-Y}] = \mathbb{E}[(\hat f(X)-Y)^2 - \mathbb{E}[(\hat f(X)-Y)]^2]$. We have
   \begin{align*}
    \mathbb{E}[(\theta^\star -  \hat \theta_{\mathsf{TL}})^2] & =  \frac{1}{n} \mathsf{Var}[Y], \\
      \mathbb{E}[(\theta^\star -  \hat \theta_{\mathsf{SL}})^2] &  \leq \frac{2m^2}{(m+n)^2} \mathbb{E}[(\hat f(X)-Y)]^2 + \frac{2m}{(m+n)^2}\Var[{\hat f(X)-Y}]  + \frac{2n}{(m+n)^2} \mathsf{Var}[Y], \\ 
   \mathbb{E}[(\theta^\star -  \hat \theta_{\mathsf{DR}})^2]  & \leq  2\min\Bigg(\frac{1}{n} \mathsf{Var}[Y]+ \frac{m+2n}{(m+n)n } \mathsf{Var}[\hat f(X)], \frac{m+2n}{(m+n)n }\Var[{\hat f(X)-Y}]  + \frac{1}{m+n} \mathsf{Var}[Y]\Bigg).
\end{align*} 
 \end{proposition}
The proof is deferred to Appendix~\ref{proof:mean_upper}. The proposition illustrates the double-robustness of $\hat \theta_{\mathsf{DR}}$---no matter how poor the estimator $\hat f(X)$ is, the rate is always upper bounded by $\frac{4}{n} (\mathsf{Var}[Y]+ \mathsf{Var}[\hat f(X)])$. On the other hand, when $\hat f(X)$ is an accurate estimator of $Y$ (i.e., $\Var[{\hat f(X)-Y}]$ is small), the rate can be improved to $ \frac{2}{m+n} \mathsf{Var}[Y]$. In contrast, the self-training loss always has a non-vanishing term, $\frac{2m^2}{(m+n)^2} \mathbb{E}[(\hat f(X)-Y)]^2$, when $m\gg n$, unless the predictor $\hat f$ is accurate. 

On the other hand, when $\hat f(x) = \hat \beta_{(-1)}^\top x + \hat\beta_{1}$ is a linear predictor trained on the labeled data with $\hat \beta = \argmin_{\beta=[\beta_1, \beta_{(-1)}]} \frac{1}{n}    \sum_{i=m+1}^{m+n} (\beta_{(-1)}^\top X_i +  \beta_{1}- Y_i)^2$, our estimator reduces to the semi-supervised mean estimator in~\citet{zhang2019semi}. Let $\tilde X = [1, X]$. In this case, we also know that the self-training reduces to training only on labeled data, since $\hat\theta_{\mathsf{TL}}$ is also the minimizer of the self-training loss. We have the following result  that reveals the superiority of the doubly robust estimator compared to the other two options. 

 \begin{proposition}[\citep{zhang2019semi}]\label{prop:mean_semi}
We establish the asymptotic behavior of various estimators when   $\hat f$  is a linear predictor trained on the labeled data:
\begin{itemize}
\item Training only on labeled data $\hat \theta_{\mathsf{TL}}$ is equivalent to self-training $\hat\theta_{\mathsf{SL}}$, which gives unbiased estimator but with larger variance:
\begin{align*}
    & \sqrt{n}(\hat \theta_{\mathsf{TL}} - \theta^\star) \rightarrow \mathcal{N}(0, \mathbb{E}[(Y-\beta^\top \tilde X)^2] + \beta_{(-1)}^\top \Sigma\beta_{(-1)}).
\end{align*}

\item Doubly Robust  $\hat \theta_{\mathsf{DR}}$ is unbiased with smaller variance:
\begin{align*}
    &\sqrt{n}(\hat \theta_{\mathsf{DR}} - \theta^\star) \rightarrow \mathcal{N}(0,  \mathbb{E}[(Y-\beta^\top \tilde X)^2] + \frac{n}{m+n} \beta_{(-1)}^\top \Sigma \beta_{(-1)}).
\end{align*}
\end{itemize}
Here $\beta = \argmin_{\beta} \mathbb{E}[(Y-\beta^\top \tilde X)^2]$ and  $\Sigma = \mathbb{E}[(X-\mathbb{E}[X])(X-\mathbb{E}[X])^\top]$.
\end{proposition}

\subsection{Guarantee for general loss} 
In the general case, 
we  show that the doubly robust loss function continues to exhibit desirable properties. In particular,  as $n,m$ goes to infinity, the global minimum of the original loss is also a critical point of the new doubly robust loss, no matter how inaccurate the predictor $\hat f$.  

Let $\theta^\star$ be the minimizer of $\mathbb{E}_{\mathbb{P}_{X, Y}}[\ell_\theta(X, Y)]$. Let $\hat f$ be a pre-existing model that does not depend on the datasets $\mathcal{D}_1, \mathcal{D}_2$. 
We also make the following regularity assumptions.
\begin{assumption}\label{ass:diff}
The loss $\ell_\theta(x, y)$ is  differentiable at $\theta^\star$ for any $x, y$.
\end{assumption}
\begin{assumption}\label{ass:mom}
The random variables $\nabla_\theta \ell_\theta(X, \hat f(X)) $ and  $\nabla_\theta \ell_\theta(X, Y)$ have bounded first and second moments.  
\end{assumption}
Given this assumption, we denote $\Sigma_{\theta}^{Y-\hat f} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, \hat f(X)) - \nabla_\theta \ell_\theta(X, Y)]$ and let $\Sigma_{\theta}^{\hat f} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, \hat f(X))]$, $\Sigma_{\theta}^{Y} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, Y)]$. 
 
% We the  a performance guarantee for the  loss function $\mathcal{L}^{\mathsf{DR1}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta)$ when $\ell_\theta$ is convex. 
\begin{theorem}\label{thm:general} Under Assumptions~\ref{ass:diff} and~\ref{ass:mom}, we have that with probability at least $1-\delta$,
% \my{the width of the Eq exceeds the limit} Banghua: fixed, thx!
\begin{align*}
  \| \nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) \|_2 & \leq C   \min\Bigg(\|\Sigma_{\theta^\star}^{\hat f}\|_2\sqrt{\frac{d}{(m+n) \delta}} + \|\Sigma_{\theta^\star}^{Y-\hat f}\|_2\sqrt{\frac{d}{n \delta}},  \\ 
  &  \qquad \qquad \|\Sigma_{\theta^\star}^{\hat f}\|_2\left(\sqrt{\frac{d}{(m+n) \delta}} + \sqrt{\frac{d}{n\delta}} \right) + \|\Sigma_{\theta^\star}^{Y}\|_2\sqrt{\frac{d}{n \delta}}\Bigg),
\end{align*} 
where $C$ is a universal constant, and $\mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}$ is defined in Equation (\ref{eq:dr}).
\end{theorem}

The proof is deferred to Appendix~\ref{proof:general_guarantee}. From the example of mean estimation we know that one can design instances such that $   \| \nabla_\theta \mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) \|_2  \geq C$ for some positive constant $C$. 

When the loss $\nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}$ is convex, the global minimum of $\nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}$ converges to $\theta^\star$ as both $m, n$ go to infinity. When the loss $\nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}$ is strongly convex, it also implies that  $\|\hat \theta-\theta^\star\|_2$ converges to  zero as both $m, n$ go to infinity, where $\hat \theta$ is the minimizer of $\nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}$.

When $\hat f$ is a perfect predictor with $\hat f(X) \equiv Y$ (and $Y|X=x$ is deterministic), one has  $ 
\mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, Y_i)$. The effective sample size is $m+n$ instead of $n$ in $\mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta)$.

When $\hat f$ is also trained from the labeled data, one may apply data splitting to achieve the same guarantee up to a constant factor. We provide further discussion in Appendix~\ref{app:split}.

\subsection{The case of distribution mismatch}\label{sec:mismatch}


We also consider the case in which the marginal distributions of the covariates for the labeled and unlabeled datasets are different.
Assume in particular that we are given a set of unlabeled samples, $\mathcal{D}_1 = \{X_1,X_2,\cdots, X_m\}$, drawn from a fixed distribution $\mathbb{P}_X$, a set of labeled samples, $\mathcal{D}_2 =\{(X_{m+1}, Y_{m+1}), (X_{m+2}, Y_{m+2}),$ $ \cdots, (X_{m+n}, Y_{m+n})\}$, drawn from some joint distribution $\mathbb{Q}_X\times \mathbb{P}_{Y|X}$, and a pre-trained model  $\hat f$. 
In the case when the labeled samples do not follow the same distribution as the unlabeled samples, we need to introduce an importance weight $\pi(x)$. This yields the following doubly robust estimator:
\begin{align*}
\mathcal{L}^{\mathsf{DR2}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m}  \sum_{i=1}^{m} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{n} \sum_{i=m+1}^{m+n} \frac{1}{\pi(X_i)}\ell_\theta(X_i, \hat f(X_i))  + \frac{1}{n} \sum_{i=m+1}^{m+n} \frac{1}{\pi(X_i)}\ell_\theta(X_i, Y_i). 
\end{align*}
Note that we not only introduce the importance weight $\pi$, but we also change the first term from the average of all the $m+n$ samples to the average of $n$ samples. 

\begin{proposition}\label{prop:dr_mis}
We have $ \mathbb{E}[\mathcal{L}^{\mathsf{DR2}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) ] = \mathbb{E}_{\mathbb{P}_{X, Y}}[\ell_\theta(X, Y)]$ as long as one of the following two assumptions hold:
        \begin{itemize}
            \item For any $x$, $\pi(x) = \frac{\mathbb{P}_X(x)}{\mathbb{Q}_X(x)}$.
            \item For any $x$, $\ell_\theta(x, \hat f(x))  = \mathbb{E}_{ Y\sim \mathbb{P}_{Y\mid X=x}}[\ell_\theta(x, Y)]$.
        \end{itemize}
\end{proposition}
The proof is deferred to Appendix~\ref{proof:dr_mis}. 
The proposition implies that  as long as either $\pi$ or 
$\hat f$ is accurate, the expectation of the loss is the same as that of the target loss. When the distributions for the unlabeled and labeled samples match each other, this reduces to the case in the previous sections. In this case, taking $\pi(x)=1$ guarantees that the expectation of the doubly robust loss is always the same as that of the target loss. 
% And similarly, we can provide non-asymptotic rates. Notably, we need to make the assumption that $\pi(x)$ is always bounded.
% \begin{assumption}\label{ass:mom2}
% Assume that $\pi(x)<\infty$, and the random variable $\nabla_\theta \ell_\theta(X, \hat f(X)) $ and  $\nabla_\theta \ell_\theta(X, Y)$ have bounded first and second moments under distribution $\mathbb{P}_{X, Y}$.
% \end{assumption}
% With this assumption, we denote $\Sigma_{\theta}^{Y-\hat f} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, \hat f(X)) - \nabla_\theta \ell_\theta(X, Y)]$, $\Sigma_{\theta}^{\hat f} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, \hat f(X))]$, $\Sigma_{\theta}^{Y} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, Y)]$. 
\section{Experiments}\label{sec:empirical}

% \subsection{US Census Dataset}

To employ the new doubly robust loss in practical applications, we need to specify an appropriate optimization procedure, in particular one that is based on (mini-batched) stochastic gradient descent so as to exploit modern scalable machine learning methods.  In preliminary experiments we observed that directly minimizing the doubly robust loss in Equation (\ref{eq:dr}) with stohastic gradient can lead to instability, and thus, we propose instead to minimize the  curriculum-based loss in each epoch:
\begin{align*}
\mathcal{L}^{\mathsf{DR}, t}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) - \alpha_t\cdot \left(  \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  - \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i)\right).  %\label{eq:dr_weighted}
\end{align*}
As we show in the experiments below, this choice yields a stable algorithm. We set $\alpha_t = t/T$, where $T$ is the total number of epochs. For the object detection experiments, we introduce the labeled samples only in the final epoch, setting $\alpha_t = 0$ for all epochs before setting $\alpha_t = 1$ in the final epoch. Intuitively, we start from the training with samples only from the pseudo-labels, and gradually introduce the labeled samples in the doubly robust loss for fine-tuning. 

We conduct experiments on both image classification task with ImageNet dataset~\citep{russakovsky2015imagenet} and 3D object detection task with  autonomous driving dataset nuScenes~\citep{nuscenes2019}. The code is available in \url{https://github.com/dingmyu/Doubly-Robust-Self-Training}.

\subsection{Image classification}

\textbf{Datasets and settings.}
We evaluate our doubly robust self-training method on the ImageNet100 dataset, which contains a random subset of 100 classes from ImageNet-1k~\citep{russakovsky2015imagenet}, with  120K training images (approximately 1,200 samples per class) and 5,000 validation images (50 samples per class).
%
To further test the effectiveness of our algorithm in a low-data scenario, we create a dataset that we refer to as mini-ImageNet100 by
randomly sampling 100 images per class from ImageNet100.
%
Two models were evaluated: (1) DaViT-T~\citep{ding2022davit}, a popular vision transformer architecture with state-of-the-art performance on ImageNet, and (2) ResNet50~\citep{he2016deep}, a classic convolutional network to verify the generality of our algorithm. 

 \textbf{Baselines.} 
To provide a comparative evaluation of doubly robust self-training, we establish three baselines: (1) `Labeled Only' for training on labeled data only (partial training set) with a loss $\mathcal{L}^{\mathsf{TL}}$, (2) `Pseudo Only' for training with pseudo labels generated for all training samples, and (3) `Labeled + Pseudo' for a mixture of pseudo-labels and labeled data, with the loss $\mathcal{L}^{\mathsf{SL}}$. 
% More implementation details and ablations are provided in Appendix.
See the Appendix for further implementation details and ablations.
% We also compare model training with different numbers of epochs and different proportions of labeled data. More ablation studies could be found in Appendix.
% We evaluate all the models on the same ImageNet-100 validation set.
% 

% \begin{table}[t]
% \footnotesize
% \centering
% \caption{Comparisons on ImageNet-100-1200, all models trained for 20 epochs.}
% \label{tab:classification_fraction}
% \setlength{\tabcolsep}{10pt}
% \renewcommand\arraystretch{1.2}
% \resizebox{1\linewidth}{!}{
%     \begin{tabular}{c|cc|cc|cc|cc}
%     \shline
%      \multirow{2}{*}{Labeled Data Percent} & \multicolumn{2}{c|}{Labeled Only} & \multicolumn{2}{c|}{Pseudo Only} & \multicolumn{2}{c|}{Labeled + Pseudo} & \multicolumn{2}{c}{Doubly robust Loss} \\
%      & top1 & top5 & top1 & top5 & top1 & top5 & top1 & top5 \\
%     \shline
%     1 & 3.09 & 10.59 & 3.57&12.76	&	3.59	&13.54 & \textbf{7.01}	&\textbf{21.88} \\
%     5 & 9.21 & 25.15	&	11.11&	26.82	&	10.95	&26.75&	\textbf{11.75}&	\textbf{28.09}\\
%     10 &  16.02	& 39.68		&17.02	&38.64		& 19.38	&41.96	&	\textbf{23.26}	&\textbf{50.45}\\
%     20 & 16.86	&39.98	&	18.24	&39.00	&	20.31	&44.52	&	\textbf{25.81}&	\textbf{53.97} \\
%     30 & 23.94&	50.04		&24.40	&48.94	&	28.01	&54.63		&\textbf{32.63}&	\textbf{62.09} \\
%     40 &  29.81	&58.01	&	29.73	&55.03	&	34.03	&62.74	&	\textbf{37.01}&	\textbf{67.90} \\
%     50 & 32.77&	62.58		&31.89&	58.05	&	37.60	&66.72	&	\textbf{39.70}&	\textbf{70.34} \\
%     60 &  36.47	&66.32	&	35.19	&62.43&		41.36	&71.82	&	\textbf{45.52}	&\textbf{74.56} \\
%     70 &  39.56&	69.86	&	37.33	&65.64&		43.76	&73.42&		\textbf{48.44}	&\textbf{77.86} \\
%     80 & 43.12&	72.60	&	39.72	&67.78	&	46.14 & 75.27		& \textbf{51.42}	&\textbf{79.44} \\
%     90 &  45.02&75.25	&	41.16	&69.50	&	47.74	& 77.15		&\textbf{51.52}	& \textbf{79.92} \\
%     100 & 47.84	&77.13&		42.82&	70.32	&	47.84 &	77.13	& \textbf{51.79} & \textbf{80.01} \\
%     \shline
%     \end{tabular}}
%     % \vspace{-8pt}
% \end{table}



\begin{figure}[t]
  % \centering
  \includegraphics[width=0.495\textwidth]{figures/davit-top1.pdf}\hfill
  \includegraphics[width=0.495\textwidth]{figures/davit-top5.pdf}
  
  \vspace{-3pt} \small{\hspace{70pt} (a) Top-1 on DaViT \hspace{125pt} (b) Top-5 on DaViT}
  
  \includegraphics[width=0.495\textwidth]{figures/resnet-top1.pdf}\hfill
  \includegraphics[width=0.495\textwidth]{figures/resnet-top5.pdf}

  \vspace{-3pt} \small{\hspace{63pt} (c) Top-1 on ResNet50 \hspace{115pt} (d) Top-5 on ResNet50}
  \vspace{-0.05in}
  \caption{Comparisons on ImageNet100 using two different network architectures. Both Top-1 and Top-5 accuracies are reported. All models are trained for 20 epochs.
  }
  \label{fig:classification_fraction}
  \vspace{-16pt}
\end{figure}


\textbf{Results on ImageNet100.} 
We first conduct experiments on ImageNet100 by training the model for 20 epochs using different fractions of labeled data from 1\% to 100\%.
From the results shown in Fig.~\ref{fig:classification_fraction}, we observe that: (1) Our model outperforms all baseline methods on both two networks by large margins.
For example, we achieve 5.5\% and 5.3\% gains (Top-1 Acc) on DaViT over the `Labeled + Pseudo' method for 20\% and 80\% labeled data, respectively.
(2) The `Labeled + Pseudo' method consistently beats the `Labeled Only' baseline.
(3) While `Pseudo Only' works for smaller fractions of the labeled data (less than 30\%) on DaViT, it is inferior to `Labeled Only' on ResNet50.

\textbf{Results on mini-ImageNet100.} We also perform comparisons on mini-ImageNet100 to demonstrate the performance when the total data volume is limited.
% where only 100 training samples per class, to show the case when the total amount of data is limited. We train all models 100 epochs.
%
From the results in Table~\ref{tab:classification_100sample}, we see our model generally outperforms all baselines. 
%
As the dataset size decreases and the number of training epochs increases, the gain of our algorithm becomes smaller. This is expected, as (1) the models are not adequately trained and thus have noise issues, and (2) there are an insufficient number of ground truth labels to compute the last term of our loss function. In extreme cases, there is only one labeled sample (1\%) per class.


% \subsubsection{Experiments}
% \noindent \textbf{Results on ImageNet-100-1200.} We conduct experiments on original ImageNet-100, which contains 1200 training samples for each class.
% We train the model for 20 epochs using different fractions of labeled data from 1\% to 100\%.
% From the results shown in Fig.~\ref{fig:classification_fraction}, we observe that: 1) Our model outperforms all baseline methods by large margins.
% For example, we achieve 5.5\% and 5.3\% gains on top1 accuracy over the `Labeled + Pseudo' method for 20\% and 80\% labeled data, respectively.
% 2) The `Labeled + Pseudo' method consistently beats the `Labeled Only' baseline, while `Pseudo Only' works for a smaller fraction of the labeled data, i.e., when it is less than 30\%.


% \noindent \textbf{Results on ImageNet-100-100.} We also perform comparisons on ImageNet-100-100, where only 100 training samples per class, to show the case when the total amount of data is limited. We train all models 100 epochs.
% %
% From the results in Table~\ref{tab:classification_100sample}, we see our model generally outperforms all baselines. 
% %
% As the dataset size gets smaller and the number of training epochs increases, the gain of our algorithm becomes smaller. This is expected, as 1) all models are not well trained so the noise issue exists, and 2) there are not enough ground truths to compute the last term of our loss. In extreme cases, there is only 1 labeled data (1\%) per class.


\begin{table}[t]
\footnotesize
\centering
\caption{Comparisons on mini-ImageNet100, all models trained for 100 epochs.}
\setlength{\tabcolsep}{10pt}
\renewcommand\arraystretch{1.05}
\resizebox{1\linewidth}{!}{
    \begin{tabular}{c|cc|cc|cc|cc}
    \shline
     \multirow{2}{*}{Labeled Data Percent} & \multicolumn{2}{c|}{Labeled Only} & \multicolumn{2}{c|}{Pseudo Only} & \multicolumn{2}{c|}{Labeled + Pseudo} & \multicolumn{2}{c}{Doubly robust Loss} \\
     & top1 & top5 & top1 & top5 & top1 & top5 & top1 & top5 \\
    \shline
    1 & 2.72	&9.18	&	\textbf{2.81}	&9.57	&	2.73&	9.55	&	2.75	&\textbf{9.73} \\
    5 & 3.92	&13.34	&	4.27&	13.66	&	4.27&	14.4	&	\textbf{4.89}	&\textbf{16.38}	\\
    10 & 6.76	&20.84		&7.27&	21.64	&	7.65&	22.48	&	\textbf{8.01}	&\textbf{21.90}  \\
    20 & 12.3&	31.3	&	13.46&	30.79	&	\textbf{13.94}&	\textbf{32.63}	&	13.50	&32.17 \\
    50 & 20.69	&46.86	&	20.92&	45.2	&	24.9	&50.77	&	\textbf{25.31}	&\textbf{51.61}\\
    80 &  27.37	& 55.57		&25.57	&50.85	&	30.63	&58.85	&	\textbf{30.75}	&\textbf{59.41} \\
    100  & 31.07&	60.62	&	28.95	&55.35	&	\textbf{34.33}&	62.78	&	34.01	&\textbf{63.04} \\
    \shline
    \end{tabular}}
    \label{tab:classification_100sample}
    \vspace{-6pt}
\end{table}


%




\subsection{3D object detection}
% \paragraph{
\textbf{Doubly robust object detection.}
Given a visual representation of a scene, 3D object detection aims to generate a set of 3D bounding box predictions $\{b_i\}_{i\in[m+n]}$ and a set of corresponding class predictions $\{c_i\}_{i\in[m+n]}$. Thus, each single ground-truth annotation $Y_i \in Y$ is a set $Y_i = (b_i, c_i)$ containing a box and a class. During training, the object detector is supervised with a sum of the box regression loss $\mathcal{L}_{loc}$ and the classification loss $\mathcal{L}_{cls}$, i.e. $\mathcal{L}_{obj} = \mathcal{L}_{loc} + \mathcal{L}_{cls}$.

In the self-training protocol for object detection, %we have a set of $m$ unlabeled scenes $\{X_i\}_{i=1}^m$, a set of $n$ labeled scenes $\{(X_i,Y_i)\}_{i=m}^{m+n}$, and a labeler $f$ pre-trained on the labeled scenes. 
pseudo-labels for a given scene $X_i$ are selected from the labeler predictions $f(X_i)$ based on some user-defined criteria (typically the model's detection confidence). 
%We denote the selected pseudo-labels as $f(X_i)^{(>\tau)}$, where $\tau$ is the criteria threshold below which detections are discarded. 
Unlike in standard classification or regression, $Y_i$ will contain a differing number of labels depending on the number of objects in the scene. Furthermore, the number of extracted pseudo-labels $f(X_i)$ will generally not be equal to the number of scene ground-truth labels $Y_i$ due to false positive/negative detections. Therefore it makes sense to express the doubly robust loss function in terms of the individual box labels as opposed to the scene-level labels. We define the doubly robust object detection loss as follows:
\begin{align*}
\mathcal{L}^{\mathsf{DR}}_{obj}(\theta) 
& = \frac{1}{M+N_{ps}}  \sum_{i=1}^{M+N_{ps}} \ell_\theta(X_i,  f(X_i)) -  \frac{1}{N_{ps}} \sum_{i=M+1}^{M+N_{ps}} \ell_\theta(X_i',  f(X_i'))  + \frac{1}{N} \sum_{i=M+1}^{M+N} \ell_\theta(X_i, Y_i),
\end{align*}
where $M$ is the total number of pseudo-label boxes from the unlabeled split, $N$ is the total number of labeled boxes, $X'_i$ is the scene with pseudo-label boxes from the \textit{labeled} split, and $N_{ps}$ is the total number of pseudo-label boxes from the \textit{labeled} split. We note that the last two terms now contain summations over a differing number of boxes, a consequence of the discrepancy between the number of manually labeled boxes and pseudo-labeled boxes. Both components of the object detection loss (localization/classification) adopt this form of  doubly robust loss.

% \paragraph{
\textbf{Dataset and setting.} 
To evaluate doubly robust self-training in the autonomous driving setting, we perform experiments on the large-scale 3D detection dataset nuScenes~\citep{nuscenes2019}. The nuScenes dataset is comprised of 1000 scenes (700 training, 150 validation and 150 test) with each frame containing sensor information from RGB camera, LiDAR, and radar scans. Box annotations are comprised of 10 classes, with the class instance distribution following a long-tailed distribution, allowing us to investigate our self-training approach for both common and rare classes. The main 3D detection metrics for nuScenes are mean Average Precision (mAP) and the nuScenes Detection Score (NDS), a dataset-specific metric consisting of a weighted average of mAP and five other true-positive metrics. For the sake of simplicity, we train object detection models using only LiDAR sensor information.


% \begin{figure}
%   \centering
%   \begin{subfigure}{.5\textwidth}
%   \centering
%   %\includegraphics[scale=0.45]{self_labeling_map.png}
%   \caption{}
%   \end{subfigure}%
%   \begin{subfigure}{.5\textwidth}
%   \centering
%   %\includegraphics[scale=0.45]{self_labeling_nds.png}
%   \caption{}
%   \end{subfigure}%
%   \caption{Self-training experiments on nuScenes \textit{val} set using varying fractions of ground-truth labels. In the low label regime, training with the doubly robust loss function significantly improves both mAP (a) and NDS (b) over the baseline training with the naive loss.}
%   \label{nuscenes}
% \end{figure}

\begin{table}[t]
\footnotesize
\setlength{\tabcolsep}{13pt}
\renewcommand\arraystretch{1.05}
\centering
\caption{Performance comparison on nuScenes \textit{val} set.}
\resizebox{1\linewidth}{!}{
    \begin{tabular}{c |c  c | c  c | c  c }
    \shline
    \multirow{2}{*}{Labeled Data Fraction} & \multicolumn{2}{c|}{Labeled Only} & \multicolumn{2}{c|}{Labeled + Pseudo} & \multicolumn{2}{c}{Doubly robust Loss}\\
     & mAP$\uparrow$ & NDS$\uparrow$ & mAP$\uparrow$ & NDS$\uparrow$ & mAP$\uparrow$ & NDS$\uparrow$\\
     \hline
     1/24 & 7.56 & 18.01 & 7.60 & 17.32 & \textbf{8.18} & \textbf{18.33}\\
     1/16 & 11.15 & 20.55 & 11.60 & 21.03 & \textbf{12.30} & \textbf{22.10}\\
     1/4 & 25.66 & 41.41 & \textbf{28.36} & \textbf{43.88} & 27.48 & 43.18 \\
     \shline
    \end{tabular}}
\label{nuscresults}
\vspace{-6pt}
\end{table}

\begin{table*}
\centering
\small
\setlength{\tabcolsep}{9pt}
\renewcommand\arraystretch{1.05}
\caption{Per-class mAP (\%) comparison on nuScenes \textit{val} set using 1/16 of total labels in training.}
\vspace{-0.08in}
\begin{tabular}{c||c | c | c | c | c | c | c}
\shline
 & Car & Ped & Truck & Bus & Trailer & Barrier & Traffic Cone\\
 \shline
 Labeled Only & 48.6 & 30.6 & 8.5 & 6.2 & 4.0 & 6.8 & 4.4\\
 \hline
 Labeled + Pseudo & 48.8 & 30.9 & 8.8 & 7.5 & 5.7 & 6.7 & 4.0\\
 Improvement & +0.2 & +0.3 & +0.3 & +1.3 & \textbf{+1.7} & -0.1 & -0.4\\
 \hline
 Doubly robust Loss & 51.5 & 32.9 &  9.6 & 8.2 & 5.2 & 7.2 & 4.5   \\
 Improvement & \textbf{+2.9} & \textbf{+2.3} & \textbf{+1.1}  & \textbf{+2.0} & +1.2 & \textbf{+0.4} &  \textbf{+0.1}  \\
 \shline
\end{tabular}
\label{nuscclass}
\vspace{-12pt}
\end{table*}

% \paragraph{
\textbf{Results.}
After semi-supervised training, we evaluate our student model performance on the nuScenes \textit{val} set. We compare three settings: training the student model with only the available labeled data (i.e., equivalent to teacher training), training the student model on the combination of labeled/teacher-labeled data using the naive self-training loss, and training the student model on the combination of labeled/teacher-labeled data using our proposed doubly robust loss. We report results for training with 1/24, 1/16, and 1/4 of the total labels in Table \ref{nuscresults}. We find that the doubly robust loss improves both mAP and NDS over using only labeled data and the naive baseline in the lower label regime, whereas performance is slightly degraded when more labels are available. Furthermore, we also show a per-class performance breakdown in Table \ref{nuscclass}. We find that the doubly robust loss consistently improves performance for both common (car, pedestrian) and rare classes. Notably, the doubly robust loss is even able to improve upon the teacher in classes for which pseudo-label training \textit{decreases} performance when using the naive training (e.g., barriers and traffic cones).


\section{Conclusions}

We have proposed a novel doubly robust loss for self-training. Theoretically, we analyzed the double-robustness property of the proposed loss, demonstrating its statistical efficiency when the pseudo-labels are accurate. Empirically, we showed that large improvements can be obtained in both image classification and 3D object detection.

As a direction for future work, it would be interesting to understand how the doubly robust loss might be applied to other domains that have a missing-data aspect, including model distillation, transfer learning, and continual learning. It is also important to find practical and efficient algorithms when the labeled and unlabeled data do not match in distribution. 


\section*{Acknowledgements}
Banghua Zhu and Jiantao 
Jiao are partially supported by NSF IIS-1901252, CIF-1909499 and CIF-2211209. Michael I. Jordan is partially supported by NSF IIS-1901252. Philip Jacobson is supported by the National Defense Science and Engineering Graduate (NDSEG) Fellowship. This work is also partially supported by Berkeley DeepDrive. 

% {\color{red} TODO: 
% Move exp details to Appendix. 

% If still no more space, 
% move Table 4 first, then Table 3.

% Make the table style consistent by changing the style of the 3D object detection table.

% Convert to linechart? 

% Go over the paper again. 

% Upload code. 
% }