\section{Introduction}

Semi-supervised learning considers the problem of machine learning given a large unlabeled dataset and a small labeled dataset. It plays an important role in the problem of model finetuning, model distillation, self-training, transfer learning and continual learning~\citep{zhu2005semi,pan2010survey, weiss2016survey, gou2021knowledge, de2021continual}. To best utilize the unlabeled data, one common assumption in distillation or self-training is that one has access to a teacher model obtained from prior training processes,  which may or may not be working well in the target task due to potential distribution shift. 
In this paper, we ask the following question:
\begin{quote}
    Given a teacher model, a large unlabeled dataset and a small labeled dataset, how can we design a principled learning process to learn the true model in a consistent and sample-efficient fashion?
\end{quote}

One widely adopted and popular approach in computer vision and autonomous driving for leveraging information from all three components is self-training~\citep{pseudolabel2013,berthelot2019mixmatch,berthelot2019remixmatch,sohn2020fixmatch, xie2020self, jiang2022improving, qi2021offboard}. This approach involves using a teacher model to generate pseudo-labels for all unlabeled data, and then training a new model on a mixture of both pseudo-labeled and labeled data. However, this method can lead to overreliance on the teacher model and can miss important information provided by the labeled data. As a consequence, the self-training approach becomes highly sensitive to the accuracy of the teacher model. Our study demonstrates that even in the simplest scenario of mean estimation, this method can yield significant failures when the teacher model lacks accuracy.



% In the field of computer vision and driving, self-training is a popular and straightforward solution for the above question.  % proposes a framework that combines active learning with auto-labeling to identify rare examples  for the task of  3D object detection.    During the stage of active learning, they aim at identifying rare examples rather than hard examples, since training on hard examples will not improve the model performance. They propose an estimator  of the rareness of the new example based on the flow model trained from existing dataset.    During the stage of auto-labeling, 
 

To overcome this issue, we propose an alternative method that is doubly robust. 
 When the covariate distribution of the unlabeled dataset and the labeled dataset matches, the estimator is always  consistent  no matter whether  the  teacher model is accurate or not. On the other hand, when the teacher model is an accurate predictor, the estimator makes full use of the pseudo-labeled dataset and greatly increases the effective sample size. The idea is inspired by the missing data inference in the literature of causal inference~\citep{rubin1976inference, kang2007demystifying, birhanu2011doubly, ding2018causal}, and the semi-parametric mean estimation~\citep{zhang2019semi}.

\subsection{Main Results}
The proposed algorithm is a simple modification of the original loss for self-training.   Assume that we are given a set of unlabeled samples $\mathcal{D}_1 = \{X_1,X_2,\cdots, X_m\}$, drawn from a fixed distribution $\mathbb{P}_X$, a set of labeled samples $\mathcal{D}_2 =\{(X_{m+1}, Y_{m+1}), (X_{m+2}, Y_{m+2}), \cdots, (X_{m+n}, Y_{m+n})\}$ drawn from some joint distribution $\mathbb{P}_X\times \mathbb{P}_{Y|X}$, and a teacher model  $\hat f$. Let $\ell_\theta(x, y)$ be a pre-specified loss function that characterizes the prediction error of the estimator with parameter $\theta$ on the given sample $(X, Y)$. The traditional self-training aims at minimizing the combined loss for both labeled and unlabeled samples, where the pseudo-labels for unlabeled samples are generated using $\hat f$:
\begin{align*}
\mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \left(\sum_{i=1}^m \ell_\theta(X_i, \hat f(X_i)) + \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i)\right). %\\
% & = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  + \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i). 
\end{align*}
Note that it can also be viewed as the first using $\hat f$ to predict all the data, and then replace the labeled ones with the known labels.  
\begin{align*}
\mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
 & = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  + \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i). 
\end{align*}
As an alternative, our proposed doubly robust loss simply replaces the coefficient $1/(m+n)$ with $1/n$ in the last two terms. 

\begin{align*}
\mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  + \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i). 
\end{align*}

With such a small change, we can show that the estimator becomes consistent and a doubly robust estimator. 
\begin{theorem}[Informal]
    Let $\theta^\star$ be the minimizer of the original loss  $\theta^\star = \argmin_{\theta} \mathbb{E}_{(X, Y)\sim \mathbb{P}_{X}\times \mathbb{P}_{Y|X}}[\ell_\theta(X, Y)]$. Under certain regularity conditions, we have 
    \begin{align*}
  \| \nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) \|_2 \lesssim  
  \begin{cases}
  \sqrt{\frac{d}{m+n}}, & \text{ when } Y \equiv \hat f(X), \\
  \sqrt{\frac{d}{n}}, & \text{otherwise}. 
  \end{cases}
\end{align*}  
On the other hand, there exists instances such that $  \| \nabla_\theta \mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) \|_2\geq C$ always holds true no matter how large $m, n$ are. 
\end{theorem} 
The result shows that the true parameter $\theta^\star$ is also a local minimum of the doubly-robust loss, but not a local minimum of the original self-training loss.  We also provide more detailed comparisons for the special example of mean estimation in Section~\ref{sec:mean}, and the empirical results on image and driving datasets are provided in  Section~\ref{sec:empirical}.


\subsection{Related Work}


% \paragraph
\noindent \textbf{Missing Data Inference and Causal Inference.} 
The problem of missing data inference has been a central and fundamental problem  in causal inference. For each unit in the experiment, at most one of the potential outcomes—the one corresponding to the treatment to
which the unit is exposed—is observed, and the other
potential outcomes are missing~\citep{holland1986statistics, ding2018causal}. The doubly robust method combines
the virtues of data imputation~\cite{rubin1979using} and propensity
score weighting~\cite{rosenbaum1983central}. The estimator is named doubly robust due to the following property: if the models for imputation are correctly specified then it is a consistent estimator no matter whether the propensity score model is correctly specified; on the other hand, if the model propensity score model is correctly specified, then it is consistent no matter whether the model for imputation is correctly specified~\citep{scharfstein1999adjusting,  bang2005doubly, birhanu2011doubly, ding2018causal}. 

Another line of work that is also inspired by the doubly robust estimator in causal inference is double machine learning~\citep{chernozhukov2018double}. The  problem in double machine learning is related to the classic semi-parametric problem of inference on a low-dimensional parameter in the presence of high-dimensional nuisance parameters, and thus is different from our question of semi-supervised learning. 


% \paragraph{
\noindent \textbf{Self-Labeling / Self-Training.}
Self-labeling is a popular semi-supervised learning paradigm in which machine-generated pseudo-labels are used for training with unlabeled data \citep{pseudolabel2013,berthelot2019mixmatch,berthelot2019remixmatch,sohn2020fixmatch}. To generate these pseudo-labels, a teacher model is pre-trained on a set of labeled data, and its predictions on the unlabeled data are extracted as pseudo-labels. Previous work seeks to address the noisy quality of pseudo-labels in various ways. MixMatch \cite{berthelot2019mixmatch} ensembles pseudo-labels across several augmented views of the input data. ReMixMatch \cite{berthelot2019remixmatch} extends this by weakly augmenting the teacher inputs and strongly augmenting the student inputs. FixMatch \cite{sohn2020fixmatch} uses confidence thresholding to select only high-quality pseudo-labels for student training.

Self-labeling has also been applied in the domains of both 2D \citep{liu2021unbiased,NEURIPS2019_d0f4dae8,Tang2021HumbleTT,sohn2020detection,zhou2022} and 3D \citep{park2022detmatch,wang20213dioumatch,li2023dds3d,liu2023hierarchical} object detection. STAC \cite{sohn2020detection} enforces consistency between strongly augmented versions of confidence-filtered pseudo-labels. Unbiased teacher \cite{liu2021unbiased} updates the teacher during training with an exponential moving average (EMA) of the student network weights. Dense Pseudo-Label \cite{zhou2022} replaces box pseudo-labels with the raw output features of the detector to allow the student to learn richer context. In the 3D domain, 3DIoUMatch \cite{wang20213dioumatch} thresholds pseudo-labels using a model-predicted Intersection-over-Union (IoU). DetMatch \cite{park2022detmatch} performs detection in both the 2D and 3D domains and filters pseudo-labels based on 2D-3D correspondence. HSSDA \cite{liu2023hierarchical} extends strong augmentation during training with a patch-based point cloud shuffling augmentation. Offboard3D \cite{qi2021offboard} utilizes multiple frames of temporal context to improve pseudo-label quality.

There have been some theoretical analyses for the case of semi-supervised inference for mean estimation and linear regression~\citep{zhang2019semi, azriel2022semi}. Our analysis bridges the gap between these approaches and the doubly-robust estimators in causal inference literature. Our proposed loss can be viewed as a generalization of these approaches, and can exactly reduce to the same estimator when considering mean estimation.

 

\section{Doubly-Robust Self-Labeling}
\subsection{Proposed Algorithm}

We begin with the case where the marginal distributions of the covariate of the labeled and unlabeled datasets are the same. 
Assume that we are given a set of unlabeled samples $\mathcal{D}_1 = \{X_1,X_2,\cdots, X_m\}$, drawn from a fixed distribution $\mathbb{P}_X$ supported on $\mathcal{X}$, a set of labeled samples $\mathcal{D}_2 =\{(X_{m+1}, Y_{m+1}), (X_{m+2}, Y_{m+2}), \cdots, (X_{m+n}, Y_{m+n})\}$ drawn from some joint distribution $\mathbb{P}_X\times \mathbb{P}_{Y|X}$ supported on $\mathcal{X}\times\mathcal{Y}$, and a pre-trained model  $\hat f:\mathcal{X}\mapsto \mathcal{Y}$. Let $\ell_\theta(\cdot, \cdot):\mathcal{X}\times\mathcal{Y}\mapsto \mathbb{R}$ be a pre-specified loss function that characterizes the prediction error of the estimator with parameter $\theta$ on the given sample $(X, Y)$. 
Our target is to find some $\theta^\star\in\Theta$ that satisfies
\begin{align*}
    \theta^\star \in \argmin_{\theta\in\Theta} \mathbb{E}_{(X, Y)\sim \mathbb{P}_{X}\times \mathbb{P}_{Y|X}}[\ell_\theta(X,Y)].
\end{align*}
For any loss $\ell_\theta(x, y)$, consider the first simple estimator which ignores the predictor $\hat f$ and only trains on the labeled samples:
\begin{align*}
\mathcal{L}^{\mathsf{TL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{n}    \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i).  
\end{align*}
This can be a safe choice since it's always an empirical risk minimizer. As $n\rightarrow \infty$, the loss converges to the population loss. However, it ignores all the information provided in $\hat f$ and the unlabeled dataset, which makes it less sample efficient when the predictor $\hat f$ is informative.

On the other hand, the traditional self-training aims at minimizing the combined loss for both labeled and unlabeled samples, where the pseudo-labels for unlabeled samples are generated using $\hat f$\footnote{There are several variants of the traditional self-training loss. For example, \citet{xie2020self} introduces an extra weight $(m+n)/n$ on the labeled samples, and adds noise to the student model; \citet{sohn2020fixmatch} uses confidence thresholding to filter unreliable pseudo-labels. However, both of the alternatives still suffer from the inconsistency issue. In this paper we focus on the simplest form $\mathcal{L}^{\mathsf{SL}}$.  }:
\begin{align*}
\mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \left(\sum_{i=1}^m \ell_\theta(X_i, \hat f(X_i)) + \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i)\right) \\
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  + \frac{1}{m+n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i). 
\end{align*}

As is shown by the last equality, the self-training loss can be viewed as first using $\hat f$ to predict all the samples (including the labeled samples) and computing the average loss, then replacing part of the loss for labeled samples  with the loss on provided labels. Although the loss uses the information of the unlabeled samples and $\hat f$, the performance can be bad when the predictor is not accurate.

On the other hand, we propose an alternative loss, which simply replaces the weight $1/(m+n)$ in the last two terms with $1/n$:
\begin{align}
\mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  + \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i).  \label{eq:dr}
\end{align}



As we will show later,  this is a doubly-robust estimator. We provide an intuitive interpretation here:
\begin{itemize}[leftmargin=24pt, itemsep=4pt]
    \vspace{-4pt}
    \item In the case when the given predictor is always accurate, i.e. $\hat f(X) \equiv Y$ always holds (which also means that $Y |X=x$ is a deterministic function of $x$), the last two terms cancel, and the loss is exactly minimizing the average loss  $\frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i))$ on all the data provided.  The effective sample size is $m+n$, compared with effective sample size $n$ for training only on labeled dataset $\mathcal{L}^{\mathsf{TL}}$. Thus in this case, the loss $\mathcal{L}^{\mathsf{DR}}$ is much better than $\mathcal{L}^{\mathsf{TL}}$, and comparable to $\mathcal{L}^{\mathsf{SL}}$. 
    
    We may as well relax the assumption of $\hat f(X) = Y$  to $\mathbb{E}[\ell_\theta(X, \hat f(X))] = \mathbb{E}[\ell_\theta(X, Y)]$. As $n$ grows larger, the loss is  approximately minimizing the average loss  $\frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i))$.
    \item On the other hand, no matter how bad  the  given predictor is,  the difference between the first two terms vanishes as either of  $m, n$ goes to infinity since the 
    labeled samples $X_{m+1},\cdots, X_{m+n}$ follow the same distribution as $X_1,\cdots, X_m$. Thus asymptotically the loss is minimizing $ \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i)$, which discards the bad predictor $\hat f$  and only focuses on the labeled dataset. Thus in this case, the loss $\mathcal{L}^{\mathsf{DR}}$ is much better than $\mathcal{L}^{\mathsf{SL}}$, and comparable to $\mathcal{L}^{\mathsf{TL}}$.
\end{itemize}
Note that this loss shall only be used when the covariate distributions between labeled and unlabeled samples match. In the case where there is a distribution mismatch, we propose an alternative loss in Section~\ref{sec:mismatch}. 

\subsection{Motivating example: mean estimation}\label{sec:mean}
 As a concrete example, in the case of one-dimensional mean estimation  we can take $\ell_\theta(X, Y) = (\theta-Y)^2$. Our target is to find some $\theta^\star$ that satisfies
\begin{align*}
    \theta^\star = \argmin_{\theta} \mathbb{E}_{(X, Y)\sim \mathbb{P}_{X}\times \mathbb{P}_{Y|X}}[(\theta-Y)^2].
\end{align*}
One can   see that $\theta^\star = \mathbb{E}[Y]$. In this case, the loss for training only on labeled data  becomes
\begin{align*}
\mathcal{L}^{\mathsf{TL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) & =   \frac{1}{n}    \sum_{i=m+1}^{m+n} (\theta- Y_i)^2.
\end{align*}
And the optimal parameter is $\hat \theta_{\mathsf{TL}} =  \frac{1}{n}\sum_{i=m+1}^{m+n} Y_i$, which is a simple empirical average over all observed $Y$'s.

The loss for  self-training  becomes
\begin{align*}
\mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) & =   \frac{1}{m+n}  \left(\sum_{i=1}^m (\theta-\hat f(X_i))^2 + \sum_{i=m+1}^{m+n} (\theta- Y_i)^2\right)
\end{align*}
It's straightforward to see that the minimizer of the loss is the unweighted average between the unlabeled predictors $\hat f(X_i)$'s and the labeled $Y_i$'s, i.e.  $$\theta^\star_{\mathsf{SL}} = \frac{1}{m+n}\left(  \sum_{i=1}^m \hat f(X_i)+ \sum_{i=m+1}^{m+n} Y_i\right). $$
In the case of $m\gg n$, the mean estimator is almost the same as the average of all the predicted value on the unlabeled dataset, which can be far from $\theta^\star$ when the predictor $\hat f$ is inaccurate.


On the other hand, for the proposed doubly robust estimator, we have
\begin{align*}
\mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} (\theta- \hat f(X_i))^2 -  \frac{1}{n} \sum_{i=m+1}^{m+n} (\theta- \hat f(X_i))^2  + \frac{1}{n} \sum_{i=m+1}^{m+n} (\theta-  Y_i)^2  \\
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} (\theta- \hat f(X_i))^2 + \frac{1}{n} \sum_{i=m+1}^{m+n} 2 (\hat f(X_i)-Y_i)\theta + Y_i^2 - \hat f(X_i)^2.
\end{align*}
Note that the loss is still convex, and we have
\begin{align*}  
\theta^\star_{\mathsf{DR}}= \frac{1}{m+n}  \sum_{i=1}^{m+n} \hat f(X_i) - \frac{1}{n}  \sum_{i=m+1}^{m+n} ( \hat f(X_i)-Y_i).
\end{align*}
This recovers the estimator in  prediction-powered inference~\citep{angelopoulos2023prediction}. Assume that $\hat f$ is independent of the labeled data. We can calculate the mean squared error of the three estimators as follows.


% Need another prop suggesting real meaning of doubly-robust.
\begin{proposition}\label{prop:mean_upper}
Let $\Var[{\hat f(X)-Y}] = \mathbb{E}[(\hat f(X)-Y)^2 - \mathbb{E}[(\hat f(X)-Y)]^2]$. We have
   \begin{align*}
    \mathbb{E}[(\theta^\star -  \hat \theta_{\mathsf{TL}})^2] & =  \frac{1}{n} \mathsf{Var}[Y], \\
      \mathbb{E}[(\theta^\star -  \hat \theta_{\mathsf{SL}})^2] &  \leq \frac{2m^2}{(m+n)^2} \mathbb{E}[(\hat f(X)-Y)]^2 + \frac{2m}{(m+n)^2}\Var[{\hat f(X)-Y}]  + \frac{2}{m+n} \mathsf{Var}[Y], \\ 
   \mathbb{E}[(\theta^\star -  \hat \theta_{\mathsf{DR}})^2]  & \leq  \min\left(\frac{1}{n} (\mathsf{Var}[Y]+\mathsf{Var}[\hat f(X)]), \left(\frac{2}{m+n}+ \frac{2}{n}\right)\Var[{\hat f(X)-Y}]  + \frac{2}{m+n} \mathsf{Var}[Y]\right).
\end{align*} 
 \end{proposition}
The proof is deferred to Appendix~\ref{proof:mean_upper}. From the proposition, we can see the double-robustness of $\hat \theta_{\mathsf{DR}}$: no matter how bad estimator $\hat f(X)$ is, the rate is always upper bounded by $\frac{1}{n} (\mathsf{Var}[Y]+\mathsf{Var}[\hat f(X)])$. On the other hand, when $\hat f(X)$ is accurate estimator of $Y$ (i.e. $\Var[{\hat f(X)-Y}]$ is small), the rate can be improved to $ \frac{2}{m+n} \mathsf{Var}[Y]$. 

On the other hand, when $\hat f(x) = \hat \beta_{(-1)}^\top x + \hat\beta_{1}$ is a linear predictor trained directly on the labeled data with $\hat \beta = \argmin_{\beta=[\beta_1, \beta_{(-1)}]} \frac{1}{n}    \sum_{i=m+1}^{m+n} (\beta_{(-1)}^\top X_i +  \beta_{1}- Y_i)^2$, our estimator reduces to the estimator in the semi-supervised mean estimator in~\citet{zhang2019semi}. we have the following result  that reveals the superiority of the doubly robust estimator compared to the other two options. 

 \begin{proposition}[\citep{zhang2019semi}]\label{prop:mean_semi}
We provide the asymptotic behavior  when   $\hat f$  is a linear predictor trained on the labeled data:
\begin{itemize}
    \item Self-labeling $\hat\theta_{\mathsf{SL}}$ is biased and thus inconsistent: \begin{align*}
  &\mathbb{E}[\hat \theta_{\mathsf{DR}} - \theta^\star] =  \frac{m}{m+n}\mathbb{E}[\beta^\top X-Y]
\end{align*}
\item Training only on labeled data $\hat \theta_{\mathsf{TL}}$ is unbiased but large variance:
\begin{align*}
    & \sqrt{n}(\hat \theta_{\mathsf{TL}} - \theta^\star) \rightarrow \mathcal{N}(0, \mathbb{E}[(Y-\beta^\top X)^2] + \beta_{(-1)}^\top \Sigma\beta_{(-1)})
\end{align*}

\item Doubly Robust  $\hat \theta_{\mathsf{DR}}$ is unbiased with smaller variance:
\begin{align*}
    &\sqrt{n}(\hat \theta_{\mathsf{DR}} - \theta^\star) \rightarrow \mathcal{N}(0,  \mathbb{E}[(Y-\beta^\top X)^2] + \frac{n}{m+n} \beta_{(-1)}^\top \Sigma \beta_{(-1)})
\end{align*}
\end{itemize}
Here $\beta = \argmin_{\beta=[\beta_1, \beta_{(-1)}]} \mathbb{E}[(Y-\beta_1 - \beta_{(-1)}^\top X)^2]$,  $\Sigma = \mathbb{E}[(X-\mu)(X-\mu)^\top]$.
\end{proposition}

\subsection{Guarantee for general loss} 
In the general case, 
we  show that the doubly robust loss function still provides a good landscape. In particular,  as $n,m$ goes to infinity, the global minimum of the original loss is also a critical point of the new doubly robust loss, no matter how bad the predictor $\hat f$ is.  

Let $\theta^\star$ be the minimizer of $\mathbb{E}_{\mathbb{P}_{X, Y}}[\ell_\theta(X, Y)]$.
We  make the following regularity assumptions.
\begin{assumption}\label{ass:diff}
The loss $\ell_\theta(x, y)$ is  differentiable at $\theta^\star$ for any $x, y$.
\end{assumption}
\begin{assumption}\label{ass:mom}
The random variables $\nabla_\theta \ell_\theta(X, \hat f(X)) $ and  $\nabla_\theta \ell_\theta(X, Y)$ have bounded first and second moments.  
\end{assumption}
With this assumption, we denote $\Sigma_{\theta}^{Y-\hat f} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, \hat f(X)) - \nabla_\theta \ell_\theta(X, Y)]$, $\Sigma_{\theta}^{\hat f} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, \hat f(X))]$, $\Sigma_{\theta}^{Y} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, Y)]$. 
 
% We the  a performance guarantee for the  loss function $\mathcal{L}^{\mathsf{DR1}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta)$ when $\ell_\theta$ is convex. 
\begin{theorem}\label{thm:general} Under Assumption~\ref{ass:diff} and~\ref{ass:mom}, we have that with probability at least $1-\delta$,
% \my{the width of the Eq exceeds the limit} Banghua: fixed, thx!
\begin{align*}
  \| \nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) \|_2 \leq & C   \min\Bigg(\|\Sigma_{\theta^\star}^{\hat f}\|_2\sqrt{\frac{d}{(m+n) \delta}} + \|\Sigma_{\theta^\star}^{Y-\hat f}\|_2\sqrt{\frac{d}{n \delta}},  \\ 
  &  \qquad \qquad \|\Sigma_{\theta^\star}^{\hat f}\|_2\left(\sqrt{\frac{d}{(m+n) \delta}} + \sqrt{\frac{d}{n\delta}} \right) + \|\Sigma_{\theta^\star}^{Y}\|_2\sqrt{\frac{d}{n \delta}}\Bigg).
\end{align*} 
Here $C$ is some universal constant, and $\mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}$ is defined in Equation (\ref{eq:dr}).
\end{theorem}

The proof is deferred to Appendix~\ref{proof:general_guarantee}. From the example of mean estimation we know that one can design instances such that $   \| \nabla_\theta \mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) \|_2  \geq C$ for some positive constant $C$. 

When the loss $\nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}$ is convex, it implies that the global minimum of $\nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}$ converges to $\theta^\star$ as both $m, n$ go to infinity. When the loss $\nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}$ is strongly convex, it also implies that  $\|\hat \theta-\theta^\star\|_2$ converges to  $0$  as both $m, n$ go to infinity, where $\hat \theta$ is the minimizer of $\nabla_\theta \mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}$.

On the other hand, when $\hat f$ is a perfect predictor with $\hat f(X) = Y$, one has 
\begin{align*}
\mathcal{L}^{\mathsf{DR}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta^\star) = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, Y_i),
\end{align*}
which is the ERM with $m+n$ samples instead of $n$ samples in $\mathcal{L}^{\mathsf{SL}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta)$.


\subsection{The case of distribution mismatch}\label{sec:mismatch}


We also consider the case where the marginal distributions of the covariate of the labeled and unlabeled datasets are different.
Assume that we are given a set of unlabeled samples $\mathcal{D}_1 = \{X_1,X_2,\cdots, X_m\}$, drawn from a fixed distribution $\mathbb{P}_X$, a set of labeled samples $\mathcal{D}_2 =\{(X_{m+1}, Y_{m+1}), (X_{m+2}, Y_{m+2}), \cdots, (X_{m+n}, Y_{m+n})\}$ drawn from some joint distribution $\mathbb{Q}_X\times \mathbb{P}_{Y|X}$, and a pre-trained model  $\hat f$. 
In the case when the labeled samples do not follow the same distribution as the unlabeled samples, we may need to introduce the importance weight $\pi(x)$. This introduces the following doubly robust estimator:
\begin{align*}
\mathcal{L}^{\mathsf{DR2}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m}  \sum_{i=1}^{m} \ell_\theta(X_i, \hat f(X_i)) -  \frac{1}{n} \sum_{i=m+1}^{m+n} \frac{1}{\pi(X_i)}\ell_\theta(X_i, \hat f(X_i))  + \frac{1}{n} \sum_{i=m+1}^{m+n} \frac{1}{\pi(X_i)}\ell_\theta(X_i, Y_i). 
\end{align*}
Note that we not only introduce the extra importance weight $\pi$, but also change the first term from the average of all the $m+n$ samples to the average of $n$ samples. 

\begin{proposition}\label{prop:dr_mis}
We have $ \mathbb{E}[\mathcal{L}^{\mathsf{DR2}}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) ] = \mathbb{E}_{\mathbb{P}_{X, Y}}[\ell_\theta(X, Y)]$ as long as one of the following two assumptions hold:
        \begin{itemize}
            \item For any $x$, $\pi(x) = \frac{\mathbb{P}_X(x)}{\mathbb{Q}_X(x)}$.
            \item For any $x$, $\ell_\theta(x, \hat f(x))  = \mathbb{E}_{ Y\sim \mathbb{P}_{Y\mid X=x}}[\ell_\theta(x, Y)]$.
        \end{itemize}
\end{proposition}
The proof is deferred to Appendix~\ref{proof:dr_mis}. 
The proposition implies that  as long as one of the $\pi$ or 
$\hat f$ is accurate, the expectation of the loss is the same as that of the target loss. When the distributions between unlabeled and labeled samples match each other, it reduces to the case in the previous sections. In this case, taking $\pi(x)=1$ guarantees that the expectation of the doubly-robust loss is always the same as that of the target loss. 
% And similarly, we can provide non-asymptotic rates. Notably, we need to make the assumption that $\pi(x)$ is always bounded.
% \begin{assumption}\label{ass:mom2}
% Assume that $\pi(x)<\infty$, and the random variable $\nabla_\theta \ell_\theta(X, \hat f(X)) $ and  $\nabla_\theta \ell_\theta(X, Y)$ have bounded first and second moments under distribution $\mathbb{P}_{X, Y}$.
% \end{assumption}
% With this assumption, we denote $\Sigma_{\theta}^{Y-\hat f} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, \hat f(X)) - \nabla_\theta \ell_\theta(X, Y)]$, $\Sigma_{\theta}^{\hat f} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, \hat f(X))]$, $\Sigma_{\theta}^{Y} = \mathsf{Cov}[\nabla_\theta \ell_\theta(X, Y)]$. 
\section{Experiments}\label{sec:empirical}

% \subsection{US Census Dataset}

\subsection{Optimization of the Doubly Robust Loss}

In practice, we train a neural network with mini-batched stochastic gradient descent. Although we have shown in Theorem~\ref{thm:general} that the true parameter remains a local minimum of the doubly robust loss, the optimization landscape might be completely different for the new doubly robust loss. We observed in the experiments that directly minimizing the doubly robust loss in Equation (\ref{eq:dr}) might lead to instability. Instead, we propose to minimize the following curriculum-based loss in each epoch:
\begin{align}
\mathcal{L}^{\mathsf{DR}, t}_{\mathcal{D}_1,\mathcal{D}_2}(\theta) 
& = \frac{1}{m+n}  \sum_{i=1}^{m+n} \ell_\theta(X_i, \hat f(X_i)) - \alpha_t\cdot \left(  \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, \hat f(X_i))  - \frac{1}{n} \sum_{i=m+1}^{m+n} \ell_\theta(X_i, Y_i)\right).  \label{eq:dr_weighted}
\end{align}
Here we set $\alpha_t = t/T$ or $(t/T)^2$, where $T$ is the total number of epochs. For object detection experiments, we introduce the labeled samples only in the final epoch, setting $\alpha_t = 0$ for all epochs before setting $\alpha_t = 1$ in the final epoch. Intuitively, we start from the training with samples only from the pseudo-labels, and gradually introduce the labeled samples in doubly robust loss for fine-tuning. We observe that this greatly stabilizes the training landscape in the experiments below.

\subsection{Image Classification}
\noindent \textbf{Datasets and Settings.}
We evaluate our doubly-robust self-training on the ImageNet100 dataset, which contains random 100 classes from ImageNet-1k~\citep{russakovsky2015imagenet}, with a number of 120K training images (approximately 1,200 samples per class) and 5,000 validation images (50 samples per class).
%
To further test the effectiveness of our algorithm in a low-data scenario, we create an additional dataset called mini-ImageNet100 by
randomly sampling 100 images per class from ImageNet100.
%
Two carefully-selected models are evaluated: \textbf{1)} DaViT~\citep{ding2022davit}, a popular vision transformer architecture with state-of-the-art performance on ImageNet, and \textbf{2)} ResNet50~\citep{he2016deep}, a classic and powerful convolutional network to verify the generality of our algorithm. 

\noindent \textbf{Baselines.} 
In addition to doubly-robust self-training, we establish 3 baselines: \textbf{1)} `Labeled Only' for training on labeled data only (partial training set) with a loss $\mathcal{L}^{\mathsf{TL}}$, \textbf{2)} `Pseudo Only' for training with pseudo labels generated for all training samples, \textbf{3)} `Labeled + Pseudo' for a mixture of pseudo-labels and labeled data, with the loss $\mathcal{L}^{\mathsf{SL}}$. 
% More implementation details and ablations are provided in Appendix.
See \textbf{Appendix} for more implementation details and ablations.
% We also compare model training with different numbers of epochs and different proportions of labeled data. More ablation studies could be found in Appendix.
% We evaluate all the models on the same ImageNet-100 validation set.
% 

% \begin{table}[t]
% \footnotesize
% \centering
% \caption{Comparisons on ImageNet-100-1200, all models trained for 20 epochs.}
% \label{tab:classification_fraction}
% \setlength{\tabcolsep}{10pt}
% \renewcommand\arraystretch{1.2}
% \resizebox{1\linewidth}{!}{
%     \begin{tabular}{c|cc|cc|cc|cc}
%     \shline
%      \multirow{2}{*}{Labeled Data Percent} & \multicolumn{2}{c|}{Labeled Only} & \multicolumn{2}{c|}{Pseudo Only} & \multicolumn{2}{c|}{Labeled + Pseudo} & \multicolumn{2}{c}{Doubly-Robust Loss} \\
%      & top1 & top5 & top1 & top5 & top1 & top5 & top1 & top5 \\
%     \shline
%     1 & 3.09 & 10.59 & 3.57&12.76	&	3.59	&13.54 & \textbf{7.01}	&\textbf{21.88} \\
%     5 & 9.21 & 25.15	&	11.11&	26.82	&	10.95	&26.75&	\textbf{11.75}&	\textbf{28.09}\\
%     10 &  16.02	& 39.68		&17.02	&38.64		& 19.38	&41.96	&	\textbf{23.26}	&\textbf{50.45}\\
%     20 & 16.86	&39.98	&	18.24	&39.00	&	20.31	&44.52	&	\textbf{25.81}&	\textbf{53.97} \\
%     30 & 23.94&	50.04		&24.40	&48.94	&	28.01	&54.63		&\textbf{32.63}&	\textbf{62.09} \\
%     40 &  29.81	&58.01	&	29.73	&55.03	&	34.03	&62.74	&	\textbf{37.01}&	\textbf{67.90} \\
%     50 & 32.77&	62.58		&31.89&	58.05	&	37.60	&66.72	&	\textbf{39.70}&	\textbf{70.34} \\
%     60 &  36.47	&66.32	&	35.19	&62.43&		41.36	&71.82	&	\textbf{45.52}	&\textbf{74.56} \\
%     70 &  39.56&	69.86	&	37.33	&65.64&		43.76	&73.42&		\textbf{48.44}	&\textbf{77.86} \\
%     80 & 43.12&	72.60	&	39.72	&67.78	&	46.14 & 75.27		& \textbf{51.42}	&\textbf{79.44} \\
%     90 &  45.02&75.25	&	41.16	&69.50	&	47.74	& 77.15		&\textbf{51.52}	& \textbf{79.92} \\
%     100 & 47.84	&77.13&		42.82&	70.32	&	47.84 &	77.13	& \textbf{51.79} & \textbf{80.01} \\
%     \shline
%     \end{tabular}}
%     % \vspace{-8pt}
% \end{table}



\begin{figure}[t]
  % \centering
  \includegraphics[width=0.495\textwidth]{davit-top1.pdf}\hfill
  \includegraphics[width=0.495\textwidth]{davit-top5.pdf}
  
  \vspace{-3pt} \small{\hspace{70pt} (a) Top-1 on DaViT \hspace{125pt} (b) Top-5 on DaViT}
  
  \includegraphics[width=0.495\textwidth]{resnet-top1.pdf}\hfill
  \includegraphics[width=0.495\textwidth]{resnet-top5.pdf}

  \vspace{-3pt} \small{\hspace{63pt} (c) Top-1 on ResNet50 \hspace{115pt} (d) Top-5 on ResNet50}
  \vspace{-0.05in}
  \caption{Comparisons on ImageNet100 using two different network architectures. Both Top-1 and Top-5 accuracies are reported. All models are trained for 20 epochs.
  }
  \label{fig:classification_fraction}
  \vspace{-12pt}
\end{figure}


\noindent \textbf{Results on ImageNet100.} 
We first conduct experiments on ImageNet100 by training the model for 20 epochs using different fractions of labeled data from 1\% to 100\%.
From the results shown in Fig.~\ref{fig:classification_fraction}, we observe that: \textbf{1)} Our model outperforms all baseline methods on both two networks by large margins.
For example, we achieve 5.5\% and 5.3\% gains (Top-1 Acc) on DaViT over the `Labeled + Pseudo' method for 20\% and 80\% labeled data, respectively.
\textbf{2)} The `Labeled + Pseudo' method consistently beats the `Labeled Only' baseline.
\textbf{3)} While `Pseudo Only' works for smaller fractions of the labeled data (less than 30\%) on DaViT, it is inferior to `Labeled Only' on ResNet50.


\noindent \textbf{Results on mini-ImageNet100.} We also perform comparisons on mini-ImageNet100 to demonstrate the performance when the total data volume is limited.
% where only 100 training samples per class, to show the case when the total amount of data is limited. We train all models 100 epochs.
%
From the results in Table~\ref{tab:classification_100sample}, we see our model generally outperforms all baselines. 
%
As the dataset size decreases and the number of training epochs increases, the gain of our algorithm becomes smaller. This is expected, as \textbf{1)} the models are not adequately trained and thus have noise issues, and \textbf{2)} there are insufficient ground truths to compute the last term of our loss function. In extreme cases, there is only 1 labeled sample (1\%) per class.


% \subsubsection{Experiments}
% \noindent \textbf{Results on ImageNet-100-1200.} We conduct experiments on original ImageNet-100, which contains 1200 training samples for each class.
% We train the model for 20 epochs using different fractions of labeled data from 1\% to 100\%.
% From the results shown in Fig.~\ref{fig:classification_fraction}, we observe that: 1) Our model outperforms all baseline methods by large margins.
% For example, we achieve 5.5\% and 5.3\% gains on top1 accuracy over the `Labeled + Pseudo' method for 20\% and 80\% labeled data, respectively.
% 2) The `Labeled + Pseudo' method consistently beats the `Labeled Only' baseline, while `Pseudo Only' works for a smaller fraction of the labeled data, i.e., when it is less than 30\%.


% \noindent \textbf{Results on ImageNet-100-100.} We also perform comparisons on ImageNet-100-100, where only 100 training samples per class, to show the case when the total amount of data is limited. We train all models 100 epochs.
% %
% From the results in Table~\ref{tab:classification_100sample}, we see our model generally outperforms all baselines. 
% %
% As the dataset size gets smaller and the number of training epochs increases, the gain of our algorithm becomes smaller. This is expected, as 1) all models are not well trained so the noise issue exists, and 2) there are not enough ground truths to compute the last term of our loss. In extreme cases, there is only 1 labeled data (1\%) per class.


\begin{table}[t]
\footnotesize
\centering
\caption{Comparisons on mini-ImageNet100, all models trained for 100 epochs.}
\setlength{\tabcolsep}{10pt}
\renewcommand\arraystretch{1.05}
\resizebox{1\linewidth}{!}{
    \begin{tabular}{c|cc|cc|cc|cc}
    \shline
     \multirow{2}{*}{Labeled Data Percent} & \multicolumn{2}{c|}{Labeled Only} & \multicolumn{2}{c|}{Pseudo Only} & \multicolumn{2}{c|}{Labeled + Pseudo} & \multicolumn{2}{c}{Doubly-Robust Loss} \\
     & top1 & top5 & top1 & top5 & top1 & top5 & top1 & top5 \\
    \shline
    1 & 2.72	&9.18	&	\textbf{2.81}	&9.57	&	2.73&	9.55	&	2.75	&\textbf{9.73} \\
    5 & 3.92	&13.34	&	4.27&	13.66	&	4.27&	14.4	&	\textbf{4.89}	&\textbf{16.38}	\\
    10 & 6.76	&20.84		&7.27&	21.64	&	7.65&	22.48	&	\textbf{8.01}	&\textbf{21.90}  \\
    20 & 12.3&	31.3	&	13.46&	30.79	&	\textbf{13.94}&	\textbf{32.63}	&	13.50	&32.17 \\
    50 & 20.69	&46.86	&	20.92&	45.2	&	24.9	&50.77	&	\textbf{25.31}	&\textbf{51.61}\\
    80 &  27.37	& 55.57		&25.57	&50.85	&	30.63	&58.85	&	\textbf{30.75}	&\textbf{59.41} \\
    100  & 31.07&	60.62	&	28.95	&55.35	&	\textbf{34.33}&	62.78	&	34.01	&\textbf{63.04} \\
    \shline
    \end{tabular}}
    \label{tab:classification_100sample}
    \vspace{-12pt}
\end{table}


%




\subsection{3D Object Detection}
% \paragraph{
\noindent \textbf{Doubly-Robust Object Detection.}
Given some visual representation of a scene, 3D object detection aims to generate a set of 3D bounding box predictions $\{b_i\}_{i\in[m+n]}$ and a set of corresponding class predictions $\{c_i\}_{i\in[m+n]}$. Thus, each single ground-truth annotation $Y_i \in Y$ is a set $Y_i = (b_i, c_i)$ containing a box and a class. During training, the object detector is supervised with a sum of the box regression loss $\mathcal{L}_{loc}$ and the classification loss $\mathcal{L}_{cls}$:
\begin{align*}
    \mathcal{L}_{obj} = \mathcal{L}_{loc} + \mathcal{L}_{cls}
\end{align*}
In the self-training  for object detection, we have a set of $m$ unlabeled scenes $\{X_i\}_{i=1}^m$, a set of $n$ labeled scenes $\{(X_i,Y_i)\}_{i=m}^{m+n}$, and a labeler $f$ pre-trained on the labeled scenes. Pseudo-labels for a given scene $X_i$ are selected from the labeler predictions $f(X_i)$ based on some user-defined criteria (typically the model's detection confidence). 
%We denote the selected pseudo-labels as $f(X_i)^{(>\tau)}$, where $\tau$ is the criteria threshold below which detections are discarded. 
Unlike in standard classification or regression, $Y_i$ will contain a differing number of labels depending on the number of objects in the scene. Furthermore, the number of extracted pseudo-labels $f(X_i)$ will generally not be equal to the number of scene ground-truth labels $Y_i$ due to false positive/negative detections. Therefore it makes sense to express the doubly-robust loss function in terms of the individual box labels as opposed to the scene-level labels. We define the doubly-robust object detection loss as follows:
\begin{align*}
\mathcal{L}^{\mathsf{DR}}_{obj}(\theta) 
& = \frac{1}{M+N_{ps}}  \sum_{i=1}^{M+N_{ps}} \ell_\theta(X_i,  f(X_i)) -  \frac{1}{N_{ps}} \sum_{i=M+1}^{M+N_{ps}} \ell_\theta(X_i',  f(X_i'))  + \frac{1}{N} \sum_{i=M+1}^{M+N} \ell_\theta(X_i, Y_i). 
\end{align*}
where $M$ is the total number of pseudo-label boxes from the unlabeled split, $N$ is the total number of labeled boxes, $X'_i$ is the scene with pseudo-label boxes from the \textit{labeled} split, and $N_{ps}$ is the total number of pseudo-label boxes from the \textit{labeled} split. We note that the last two terms now contain summations over a differing number of boxes, an upshot of the discrepancy between the number of manually-labeled boxes and pseudo-labeled boxes. Both components of the object detection loss (localization/classification) adopt this form of  doubly-robust loss.

% \paragraph{
\noindent \textbf{Dataset and Setting.} 
To evaluate doubly-robust self-training in the autonomous driving setting, we perform experiments on the large-scale 3D detection dataset nuScenes~\cite{nuscenes2019}. nuScenes is comprised of 1000 scenes (700 training, 150 validation and 150 test) with each frame containing sensor information from RGB camera, LiDAR, and radar scans. Box annotations are comprised of 10 classes, with the class instance distribution following a long-tailed distribution, allowing us to investigate our self-training approach for both common and rare classes. The main 3D detection metrics for nuScenes are mean Average Precision (mAP) and the nuScenes Detection Score (NDS), a dataset-specific metric consisting of a weighted average of mAP and five other true-positive metrics. For the sake of simplicity, we train object detection models using only LiDAR sensor information.


% \begin{figure}
%   \centering
%   \begin{subfigure}{.5\textwidth}
%   \centering
%   %\includegraphics[scale=0.45]{self_labeling_map.png}
%   \caption{}
%   \end{subfigure}%
%   \begin{subfigure}{.5\textwidth}
%   \centering
%   %\includegraphics[scale=0.45]{self_labeling_nds.png}
%   \caption{}
%   \end{subfigure}%
%   \caption{Self-labeling experiments on nuScenes \textit{val} set using varying fractions of ground-truth labels. In the low label regime, training with the doubly-robust loss function significantly improves both mAP (a) and NDS (b) over the baseline training with the naive loss.}
%   \label{nuscenes}
% \end{figure}

\begin{table}[t]
\footnotesize
\setlength{\tabcolsep}{13pt}
\renewcommand\arraystretch{1.05}
\centering
\caption{Performance comparison on nuScenes \textit{val} set.}
\resizebox{1\linewidth}{!}{
    \begin{tabular}{c |c  c | c  c | c  c }
    \shline
    \multirow{2}{*}{Labeled Data Fraction} & \multicolumn{2}{c|}{Labeled Only} & \multicolumn{2}{c|}{Labeled + Pseudo} & \multicolumn{2}{c}{Doubly-Robust Loss}\\
     & mAP$\uparrow$ & NDS$\uparrow$ & mAP$\uparrow$ & NDS$\uparrow$ & mAP$\uparrow$ & NDS$\uparrow$\\
     \hline
     1/24 & 7.56 & 18.01 & 7.60 & 17.32 & \textbf{8.18} & \textbf{18.33}\\
     1/16 & 11.15 & 20.55 & 11.60 & 21.03 & \textbf{12.30} & \textbf{22.10}\\
     1/4 & 25.66 & 41.41 & \textbf{28.36} & \textbf{43.88} & 27.48 & 43.18 \\
     \shline
    \end{tabular}}
\label{nuscresults}
\vspace{-6pt}
\end{table}

\begin{table*}
\centering
\small
\setlength{\tabcolsep}{8.5pt}
\renewcommand\arraystretch{1.05}
\caption{Per-class mAP (\%) comparison on nuScenes \textit{val} set using 1/16 of total labels in training.}
\vspace{-0.08in}
\begin{tabular}{c||c | c | c | c | c | c | c}
\shline
 & Car & Ped & Truck & Bus & Trailer & Barrier & Traffic Cone\\
 \shline
 Labeled Only & 48.6 & 30.6 & 8.5 & 6.2 & 4.0 & 6.8 & 4.4\\
 \hline
 Labeled + Pseudo & 48.8 & 30.9 & 8.8 & 7.5 & 5.7 & 6.7 & 4.0\\
 Improvement & +0.2 & +0.3 & +0.3 & +1.3 & \textbf{+1.7} & -0.1 & -0.4\\
 \hline
 Doubly-Robust Loss & 51.5 & 32.9 &  9.6 & 8.2 & 5.2 & 7.2 & 4.5   \\
 Improvement & \textbf{+2.9} & \textbf{+2.3} & \textbf{+1.1}  & \textbf{+2.0} & +1.2 & \textbf{+0.4} &  \textbf{+0.1}  \\
 \shline
\end{tabular}
\label{nuscclass}
\vspace{-12pt}
\end{table*}

% \paragraph{
\noindent \textbf{Results.}
After semi-supervised training, we evaluate our student model performance on the nuScenes \textit{val} set. We compare three settings: training the student model with only the available labeled data (i.e. equivalent to teacher training), training the student model on the combination of labeled/teacher-labeled data using the naive self-training loss, and training the student model on the combination of labeled/teacher-labeled data using our proposed doubly-robust loss. We report results for training with 1/24, 1/16, and 1/4 of the total labels in Table \ref{nuscresults}. We find that the doubly-robust loss improves both mAP and NDS over using only labeled data and the naive baseline in the lower label regime, whereas performance is slightly degraded when more labels are available. Furthermore, we also show a per-class performance breakdown in Table \ref{nuscclass}. We find that the doubly robust loss consistently improves performance for both common (car, pedestrian) and rare classes. Notably, the doubly-robust loss is even able to improve upon the teacher in classes for which pseudo-label training \textit{decreases} performance when using the naive training (e.g. barriers and traffic cones).


\section{Conclusion}

In this paper, we propose the new doubly-robust loss for self-training. Theoretically, we analyze the double-robustness property of the proposed loss and show its statistical efficiency when the pseudo-labels are accurate. Empirically, we see large improvements in both image classification and 3D object detection datasets.  As part of future work, it would be interesting to understand how the doubly robust loss can be applied to other domains of questions, including model distillation, transfer learning, and continual learning. It is also important to find practical and efficient algorithms when the labeled and unlabeled data do not match in distribution. 


% {\color{red} TODO: 
% Move exp details to Appendix. 

% If still no more space, 
% move Table 4 first, then Table 3.

% Make the table style consistent by changing the style of the 3D object detection table.

% Convert to linechart? 

% Go over the paper again. 

% Upload code. 
% }