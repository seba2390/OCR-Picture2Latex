\vspace{-0.5em}
\section{Evaluation}
\label{sec:eval}

\noindent
{\bf Objectives.} 
We evaluate \sysname on four aspects. 
First, we present analysis results on the instrumented code and its impact to show the correctness of \sysname (Section~\ref{subsec:instr}). 
Second, we run PoC exploits against a set of vulnerable programs and their \sysname instrumented versions, to show the effectiveness of \sysname in preventing command injection attacks (Section~\ref{subsec:eval_attack}).
Third, we measure the performance  overhead of \sysname (Section~\ref{subsec:eval_perf}).
Fourth, we present case studies to show the effectiveness of \sysname in advanced command injection attacks (Section~\ref{subsec:casestudy}).

\noindent
{\bf Implementation.}
%\sysname's static analysis supports diverse programming languages: C/C++, PHP, JavaScript and Lua. 
We implement our static analysis tool by leveraging LLVM~\cite{llvm} for C/C++, php-ast~\cite{php-ast} and Taint'em All~\cite{taintall} for PHP, Acorn~\cite{js-ast} for JavaScript and Lua SAST Tool~\cite{costin2017lua} for Lua.
\sysname uses \code{LD\_PRELOAD} that requires access to the shell. 
Hence, it does not support a web hosting service such as cPanel~\cite{cpanel}.

\noindent
{\bf Setup.} All the experiments were done on a machine with Intel Core i7-9700k 3.6Ghz, 16GB RAM, and 64-bit Linux Ubuntu 18.04.


\input{tex/table_main_eval}

\noindent
{\bf Program Selection.}
%To evaluate the effectiveness of \sysname in preventing \updated{command}{input} injection attacks (Section~\ref{subsec:eval_perf}), 
We search publicly known \updated{command}{input} injection vulnerabilities (including SQL and XXE\updated{(XML external entity)}{} injections) in recent five years. Among them, we reproduced 27 vulnerabilities and used the vulnerable programs as shown in \autoref{table:selectedprograms}. Note that the versions of the evaluated programs can be found in Appendix~\ref{appendix:versions} (\autoref{table:versions}).
%shows the programs including names, sizes, languages written in, and brief descriptions. 
The selected programs are diverse, including popular programs such as WordPress~\cite{wordpress} and OpenCV~\cite{opencv}.
They are also written in diverse programming languages such as PHP, C/C++, Lua, and JavaScript. 
%LuCI~\cite{LuCI} is an Embedded Configuration Interface widely used in embedded devices. 
The programs and vulnerabilities\updated{ used in the evaluation}{} are on \cite{csr-tool}\updated{ (\autoref{table:vulnanddescription})}{}.


%\label{subsec:eval_perf}
\noindent
{\bf Input Selection.}
%We use the above 27 programs in \autoref{table:selectedprograms} to measure the performance of \sysname (Section~\ref{subsec:eval_perf}). 
To obtain realistic test input (or test data) that can cover diverse aspects of the program, we leverage publicly available input data sources. %\mw{added methods for test}
For instance, Leptonica~\cite{Leptonica} provides 278 test cases with 192 images. 
Other programs also have developer provided test cases: NPM-opencv~\cite{opencv}, fs-git~\cite{fs-git}, PM2~\cite{PM2} and codecov~\cite{codecov}. For the programs with less than 100 test cases, we extend them on different inputs with around 100 cases. 
%XXX and XXX use Mocha~\cite{Mocha}, a test automation framework, to manually write unit test cases for them. 
%
For the programs that accept PDFs (e.g., pdfinfojs), videos (e.g., Avideo-Encoder), and patches (e.g., GNU-Patch), we crawl more than 100 samples for each type from public websites~\cite{tpn-pdf, ted-video}.
%Some programs have standard input like \code{pdfinfojs} for pdf files, \code{Cool Video Gallery} for videos, \code{Gnu-Patch} for patch, \code{Ruby Resolv Module} for host name. So we generate inputs that are various both in size and contents to test the programs. 
%For the cases that have limited execution options. We use Mocha~\cite{Mocha}, a test framework, to manually write unit test cases for them. 
%Overall, we obtain at least 100 test cases for all programs selected for the performance evaluation.
% \YK{add later...}
To run the programs for the performance evaluation (Section~\ref{subsec:eval_perf}), we leverage Apache Jmeter~\cite{apachejmeter} and Selenium~\cite{selenium} for web applications. We also use Selenium scripts provided by \cite{sec19-selenium} to simulate requests and interactions for web services such as WordPress. 
OLPTBench~\cite{difallah2013oltp}, which aims to conduct extensive tests on relational database systems, is used to test diverse SQL queries. 
In addition, a large publicly available XML data-set (1026 MB total)~\cite{xmldata} is used.
We also include popular web servers~\cite{apache,lighttpd,OpenLightSpeed,Cherokee}, SQL engines~\cite{sqlite,Mysql}, and XML libraries~\cite{libxml,simplexml,libxmljs,luaexpat} in our evaluation. % to show the performance of \sysname in various aspects.


\subsection{Instrumentation Results and Correctness}
\label{subsec:instr}
\autoref{table:selectedprograms} presents the results of our instrumentation in detail.

\noindent
{\bf Statistics\updated{ (\# Instrumentation)}{}.}
The ``Const.'' and ``Dynamic'' columns  represent the number of completely constant commands\updated{ (e.g., `\code{\$cmd = {\color{blue}"rm *.bak"}}')}{}  and the number of dynamically composed commands with other values\updated{ (e.g., `\code{\$cmd = {\color{red}\$base\_dir}.{\color{blue}"/ping"}}')}{} respectively. 
There are three groups based on the number of variables involved in creating a command or query dynamically. 
The first group includes cases where 1$\sim$5 variables are involved, that are trivial to verify that they do not break benign functionalities. Most cases belong to this group. 
The second and third groups indicate 6$\sim$10 and more than 11 variables are involved respectively.  
We checked them all that they do not break benign functionalities. Examples and details can be found in Appendix~\ref{appendix:manual_analysis_instr_detail}.
%
%They are more complex to analyze. 
%We find that the number of cases decreases as the number of variables involved increases.  
%in which only the path name is constant but the arguments are from the users' inputs. 
The ``Sinks'' column represents the number of sink functions identified by \sysname. 
%
Note that while there are applications that require many instrumentations (e.g., 462 for WordPress), most of them are constants or dynamic cases with only a few variables are involved. 
%In other words, they are simple cases that do not make the instrumentation complicated.
%There are three exceptions: {\tt s6} (Pie Register), {\tt s11} (LuCI) and {\tt s16} (PM2).
%
WordPress is a content management system stores/retrieves contents from databases, PHPSHE is a website builder, and Pie Register is a user registration form service. 
LuCI is a web interface for configuring OpenWrt~\cite{openwrt} that runs various commands in nature. 
These programs include many SQL queries, leading to a large number of instrumentations.
However, patterns of queries in those programs  are simple and similar to each other.
Further, we analyze the dynamically composed commands.
Most cases are appending file names to base folders to compose paths and  adding table names in queries. 
%
%
%
%\code{s17} (meta-git) is a JavaScript plugin for \code{meta} which is a meta project management system (i.e., supporting multiple repository systems). 
%\code{meta-git} includes various commands to execute for each repository system. 
%wraps all the common commands used by the \code{meta}. Users will use this plugin to run \code{meta}'s commands. So it has many statements to instrument. 
%For each feature of the \code{meta}, the programs define a corresponding command statements to run the \code{meta-git}. Though it has more statements, they have the similar path name. Foe example, they have \code{/bin/meta-git}, \code{/bin/meta-git-add}, \code{/bin/meta-git-diff} and so on. 
%\YK{@meng, add some explanations}.
%

%Specifically, \code{s7} (Lighttpd) and \code{s10} (Goahead) are both web servers.
%They support CGI (Common Gateway Interface), which will execute a binary program on the server-side through clients' requests. 
%Both servers compose a file path of a CGI program by concatenating the requested filename, and a base CGI file folder originated from its configuration. 
%They have cgi-handler for the incoming network requests. So the path name of the program will be selected by the outside user.
%

%In addition, we find out there are programs that directly feed the untrusted inputs (e.g., inputs from remote users) to the command executing functions (e.g., \code{system()}).
%\code{s3} (Cool Video Gallery) allows remote users to configure the binary file path of the video decoder, which it will use. \code{s4} (Advanced XML Reader) allows an untrusted XML file with an external entity to be processed. 

\noindent
{\bf Correctness.}
We run test cases and analyze all the instrumented code to show the instrumented programs' correctness. 

-- \textit{Testing Instrumented Programs:}
    To empirically show that our instrumentation does not break the original functionalities, we run test cases that can cover instrumented code and other parts of the program code affected by the instrumentation. 
    We leverage test cases provided by developers of the target applications. If there are no provided test cases or test cases are not sufficient, we manually extend test cases to cover those. All the test cases are presented in \autoref{table:selectedprograms_cov}. % where the number of test cases we additionally created is shown in the ``Added'' column.
    In total, we run 15,916 test cases for the 27 programs, achieving the average code coverage of 78.17\%. For the code that is not covered by the test cases, we manually checked that they are not affected by our instrumentation. %More details can be found in \cite{csr-tool}.
    %we use real inputs and unit test cases to run the instrumented programs. For those applications not having test cases including AVideo-Encode, Pepperminty-Wiki, PHPSHE, Pie-register, and LuCI, we create test cases to simulate the realistic tasks. We also added test cases to the applications having unit test cases in the source code to improve the code coverage. Multiple code test and code coverage report tools are used in the experiments. We leverage PHPUnit~\cite{PHPUnit} and Xdebug~\cite{Xdebug} for PHP applications, Mocha~\cite{Mocha}, Jest~\cite{Jest} and nyc~\cite{nyc} for JavaScript applications, LuaCov~\cite{LuaCov} for Lua applications and Gcov~\cite{Gcov} for C/C++ applications. In those applications, there are functions and modules not supported by our test environment. For example, Leptonica, kill-port and PM2 have code to support operating systems other than Linux. LuCI has functions that need hardware support like LTE network. These parts of code are removed. The average code coverage is 78.17\%. Although there is code not covered by our test cases, we checked that those code is independent from our instrumentation. The details of the results are shown in Appendix[XXXXX].

-- \textit{Manual Analysis of Instrumentation:} 
We analyze the impact of our instrumentations and categorize them into three types: instrumentations that affect (1) a single basic block (the BB$^1$ column), (2) a single function (the Fn$^2$ column), and (3) multiple functions (the Fns$^3$ column).
The first category only affects statements within its basic block. 
Mostly, they are the cases where a constant string is instrumented and directly passed to a sink function. For this case, it is trivial to prove that it does not impact the correctness of the program as the impact of the instrumentation is contained within the current basic block.
%In this case, the instrumentation does not break the program's benign functionality, as long as our command target randomization scheme is correct. 
%Note that we do not observe errors in our command target randomization scheme during our evaluation.
For the second category (i.e., single function), the instrumented values are stored into local variables, but it does not affect other functions (i.e., they are not returned or passed to other functions). Hence, the impact of instrumentation is limited within the function. 
The last category (i.e., multiple functions) means that the instrumentation affects multiple functions because the instrumented value is stored to a variable shared between functions (e.g., global variable) or passed to other functions as arguments. 
We verify all the cases in the three categories that they do not break the original functionalities of the target programs by tracing dependencies caused by our instrumentations.
%Note that there are four programs containing instrumentations affecting many functions: WordPress, PHPSHE, Pie Register, and LuCI. 
Details with example code for the three categories are in Appendix~\ref{appendix:manual_analysis_instr_detail}. 


\updated{{\it -- Affected Variables and Functions by Instrumentation:}}{}
We also inspect local and global variables and functions affected by the instrumentations.
In the next three columns, the average number of variables/functions affected by each instrumentation is presented, followed by the total number of variables/functions affected in the entire program.
The average number of variables and functions affected per instrumentation is not large: less than 12 local variables, 2 global variables, and 8 functions. 
%We use static analysis to trace and verify the complexity of the instrumentation and its impact automatically.
We verify all of them and \sysname does not break the benign functionality.
%Moreover, we additionally analyzed the number of data propagations from the instrumented variables to sinks (which we call data propagation depth). The average value is 3.85, meaning that each instrumented variable is propagated about four times on average. Details are in Appendix~\ref{appendix:data_propagation}.


\begin{comment}
\noindent
{\bf Modifying Insecure Applications.}
The above programs directly feed untrusted inputs to the sinks (i.e., command execution APIs). Such a practice is insecure in nature. Hence, if we apply our technique to those insecure programs, \sysname will prevent the command from executing functionality completely. While this is desirable for security, it breaks the benign functionality. We notice that those programs, in fact, intended to execute a few predefined programs, while their insecure implementation does not clearly express the intention. 

Hence, we modify the insecure programs slightly so that they do not directly use untrusted inputs. 
Specifically, for \code{s3} (Cool Video Gallery), we create a list of a few possible video decoders' binaries and allow the program can obtain the path from the list. For \code{s4} (Advanced XML Reader), XXE (XML External Entity) injection attacks are handled by randomizing trusted XML files at runtime (Details can be found in Section~\ref{subsubsec:case_xxe_prevention}). 
%We maintain a list of trusted XML files. 
\end{comment}





%There are still special cases: \code{s6} (Lighttpd), \code{s9} (Goahead), \code{s3} (Cool Video Gallery), \code{s11} (Nodejs-open) and \code{s28} (Ruby Rake Filelist). 
%\code{s3} needs the administrator of the site to define the path of the video decoder. \code{s11} provides a method to open a program so the input can be fully defined by the user. \code{s28} is caused by the grammar of the \code{Ruby} language. The developer mistakenly uses \code{open} instead of \code{file.open} make it be able to run executable binaries.\mw{added some explanations}
%\YK{i need why there are more numbers rather than what they are doing. re do}

%the number of instrumented statements. T
%the first three columns list the program, their source code's size, and their programming language. The last column presents the number of entries computed by our static analysis tools. From the table, we have the following observations. \sysname can support diverse programming languages\hl{,} including PHP, C/C++, Ruby, JavaScript\hl{,} and Lua. Meanwhile, it is able to support large and complex program such as WordPress and Luci. The number of sink APIs is small with respect to the program size. This supports our speculation that the exploitable space is small. 


%\input{tex/table_cve_list}

\vspace{-1em}
\subsection{Effectiveness}
\label{subsec:eval_attack}

\subsubsection{Against PoC (Proof of Concept) Exploits}
\label{subsec:poc_eval}
We reproduce 27 PoC exploits on \sysname instrumented programs, as shown in \autoref{table:selectedprograms}.
%shows the v we have reproduced on vanilla applications.
%Then, we apply \sysname to the applications and rerun the exploits to check whether \sysname successfully prevents the attacks.
%The first column shows the identifier (ID) of the programs. 
%\textsl{Vulnerability} column shows the reported vulnerability IDs or names in public (e.g., CVE and Hackerone). %and functions or elements (XML element for XXE attack in {\tt s4})  respectively.
%The fourth column shows the functions (or XML element for XXE attack in {\tt s4}) that are exploited. 
The ``Vulnerability'' column shows attack type (e.g., Command injection, XXE injection, and SQL injection) with citations.
%\textsl{Result} column shows the results when the PoC exploits are launched on the \sysname protected programs.
All the PoC (Proof of Concept) attacks are successful in the vanilla versions, while prevented in the \sysname protected programs.
%As shown in the last column, there are three different results: 
%{\it Command not found}, {\it File not found}, and {\it Time out}.
%The first two essentially mean that the exploit failed to execute the command as the command is already randomized. Time out happens because the program waits for the completion of the command while the command will never be completed as it fails to execute.
%to Meng, Also this should be fixed with superscript in table 2.
 %, meaning that it is essentially the command not found error.
%command execution is finished, which will not be satisfied if the program fails to execute the command. 
%Hence, it is essentially equivalent to the command not found error. 
%\YK{Why crash on exeSync?} \mw{I did not give the correct input for the program. There must be a process to kill, so the vulnerability can be triggered. But I used the wrong PID, so it crahsed} \YK{is this your fault? need to run exp again?}

%\vspace{-1em}
\subsubsection{Against Automated Vulnerability Discovery Tools} 
To see whether \sysname can prevent \emph{diverse malicious commands}, in addition to the tested CVEs in Section~\ref{subsec:poc_eval}, we leverage three automated vulnerability discovery tools, Commix~\cite{commix}, sqlmap~\cite{sqlmap}, and xcat~\cite{xcat}, to test a diverse set of known malicious commands.
%
We launch 102 command injection attacks, 97 SQL injection attacks, and 24 XXE injection attacks, leveraging the three tools.
They essentially brute-force the target programs' inputs using the known malicious commands. Then, they check whether it is vulnerable to command injection attacks. 
The result shows that \sysname successfully prevents all 223 tested attacks. Details are in Appendix~\ref{appendix:vulnerability_discovery_tools}.

%Then, we test the identified payloads to show the effectiveness of \sysname.
\revised{
\updated{}{
\subsubsection{Bidirectional Analysis Compared to Backward and Forward Analysis}

We apply forward and backward data flow analysis alone to the programs and presented the average length of dependency chain obtained from each analysis in the last two columns of \autoref{table:selectedprograms}. 
%
We observe that data-flow analysis accuracy, including forward and backward analyses, decreases as the data dependency chain's length becomes larger than 10 in general, causing false negatives. We find such cases in WordPress~\cite{wordpress}, Activity Monitor~\cite{PlainviewActivityMonitor}, and Pie Register~\cite{pieregister}, marked with $\alpha$, $\beta$, $\gamma$, and $\delta$ with the red cell background color. 
%When we use forward or backward analysis alone for them, we find false negative cases from the data flow analysis results. 
%Note that with our bidirectional analysis, we observe no false negative. 
We manually verified that all the results from our analysis are true-positives. In particular, we run other static/dynamic taint-analysis techniques~\cite{taintless,psalm,pecltaint} and manually verify that the dependencies identified by the existing techniques but not by ours are false-positives. 
Appendix~\ref{appendix:bidirectional_effectiveness} and \ref{appendix:bidir_accuracy} provide more details including examples and accuracy of the bidirectional analysis.
}
}

\vspace{-1em}
\subsection{Performance Evaluation}
\label{subsec:eval_perf}
\vspace{-0.5em}
\noindent
{\bf Runtime Overhead (Overhead: $\approx$5\%).} 
We measure the runtime overhead of \sysname on the 27 programs\updated{}{ in Table~\ref{table:selectedprograms} as shown in \autoref{fig:overhead}}.
\updated{}{Note that each application has 4 measures as we use 4 different randomization schemes mapping 1 byte to 1, 2, 4, and 8 bytes}.
\updated{}{In each bar, the bottom black portion represents the overhead caused by creating randomization tables, including those for rerandomization, while the top gray portion is the overhead from the computations for randomization.} 
%
For each program, we use 100 typical benign test inputs that cover instrumented statements. % to evaluate performance overhead.
For each input, we run ten times and take the average. 
%We modify the sample programs provided by the developer to make sure all the sink APIs are executed in the test. 
%\updated{To obtain realistic inputs,}{For data files used for inputs,} we use non-trivial size data for inputs. For example, we use a large XML file whose size is around 100MB obtained from a publicly available XML data-set maintained by the University of Washington~\cite{xmldata} and we obtain 100 large graphs from \code{Leptonica}~\cite{Leptonica}.
\updated{\autoref{fig:Perf_all} shows the result.}{} 
%
The average overhead is 3.64\%\updated{}{, 3.91\%, 4.28\% and 5.01\% for 1, 2, 4, and 8 bytes randomization schemes respectively}. 
%\CJ{10) [8] (\#E) Evaluation of the randomization approach in different applications (e.g., overhead from managing randomization tables).}
% s1 s10 s12 s23
%\updated{Among the programs,}{} \code{s11} (LuCI) has slightly higher overhead than others, i.e., 7\%$\sim$8\%. 
%Our manual inspection reveals that they execute more commands (e.g., 150 commands while a typical program executes 50 commands) during the evaluation. %\updated{}{Managing larger randomization table is the main contribution of the increasement of the run-time overhead in different kinds of schemes. Storage overhead for one single randomization table is 54 bytes, 106 bytes, 209 bytes and 417 bytes respectively when \sysname is configured to randomize one character to 1, 2, 4, or 8 characters.}


\begin{figure}[h]
    \centering
    %\vspace{-0.5em}
    \includegraphics[width=1\columnwidth]{fig/detailed_overhead.pdf}
    \vspace{-2.5em}
    \caption{\revised{Runtime Overhead}}
    \vspace{-1em}

    \label{fig:overhead}
\end{figure}

%resulting in a higher performance overhead in each test.
%This is because the execution of \code{s1} will invoke 3 instrumented statements, the execution of \code{s13} will invoke 4 instrumented statements and execution of \code{24} will invoke 6 instrumented statements. Unlike other programs that only invoke 1 or 2 instrumented statements for the tested task. 
%Moreover, the complexity of the target executable binary is less than others which make the overhead of \sysname more obvious.\mw{explainnation added}
%Fig.8 shows the results. The average overhead is less than 4\%. 
%For the programs that need data input. We provide it with large amounts of data. 


% \begin{figure}[h]
% \centering
% \vspace{-1.5em}
% \includegraphics[width=1\columnwidth]{fig/all_perf.pdf}
% \vspace{-3em}
% \caption{Runtime Overhead}
% \vspace{-0.5em}
% \label{fig:Perf_all}
% \end{figure}


%Specifically, we focus on measuring runtime overhead in realistic deployment scenarios. For instance, we measure a full system overhead on a web server equipped with a SQL database engine. In such a scenario, we instrument major software layers, including a web-server, a PHP engine, a PHP application, and a database engine (SQLite). Similarly, to evaluate runtime overhead on an embedded web-server running Lua applications, we apply \sysname to the embedded web-server, Lua language engine, and Lua applications. This provides us a conservative runtime overhead in realistic deployment scenarios.


%\noindent
%{\bf Overhead of Static Analysis and Instrumentation Techniques.}
%\sysname's static analysis and instrumentation are offline tasks. Hence, their performance overhead is a one-time cost and not significant for deployment.



%\begin{figure}[h]
%    \centering
 %   \includegraphics[width=0.7\columnwidth]{fig/server_perf.pdf}
%     \caption{Overhead on Webservers Running WordPress with SQLite}
 %    \label{fig:Perf_server}
%\end{figure}

\noindent
{\bf \updated{Overhead on}{Throughput of} Full Stack Web \updated{Services (Overhead: 3.69\%)}{Servers (WordPress)}.}
We measure the overhead on \updated{}{throughput of} full-stack web services to understand the performance overhead of realistic deployment of \sysname.
\updated{Specifically,}{}We applied \sysname to four different web servers: Apache 2.4.41, Lighttpd 1.4.55, Cherokee 1.2.102, and Openlightspeed 1.5.11. 
We also apply \sysname to SQLite 3.31.0, PHP 7.2, and WordPress 4.9.8, along with the listed vulnerable plugins. %We also applied \sysname to The instrumented database used in this part is Sqilte version 3.31.0. 
%
\updated{To test the web services, }{}Apache Jmeter~\cite{apachejmeter} is used to request 10,000 concurrent webpages, covering various functionalities of WordPress, including posting blogs, changing themes, and activating/deactivating/configuring plugins. 
%
\updated{}{The average overhead on throughput is 3.69\% (4.33\%, 3.76\%, 3.18\%, and 3.47\% overhead on Apache, Lighttpd, OpenLightSpeed, and Cherokee respectively).
}
%for the plugins that need extra inputs. For example, the input of the Advanced XML Reader is a 100 MB XML file, and the inputs of Cool Video Gallery are 100 different short videos. 

% \input{tex/table_server_overhead}

\noindent
{\bf Overhead on Database Engines and XML Parsers.} 
%To understand the performance overhead on database engines, 
We apply \sysname to SQLite and MySQL and run various SQL queries using data-sets from OLTP-Bench~\cite{difallah2013oltp}. % to run various SQL queries.
The result shows that the overhead with SQLite is 4.9\% and MySQL is 5.3\%. 
We also measure the overhead on four XML parsers~\cite{libxml, simplexml, libxmljs,luaexpat}. % using large datasets~\cite{xmldata}. % (i.e., 1GB in total). % on four XML parsers %Libxml~\cite{libxml}, SimpleXML~\cite{simplexml}, libxmljs~\cite{libxmljs} and LuaExpat~\cite{luaexpat}, 
%with instrumentations. 
The average overhead is 1.4\% (Details in Appendix~\ref{appendix:microbenchmarks_xml}).

%Table~\ref{table:OverheadDB} shows the result. Overall, the overhead is less than 5.5\%. SQLite is slightly faster than MySQL due to the complexity of MySQL that requires complicated instrumentations.

\revised{
\noindent
\updated{}{\textbf{Memory Overhead.} \sysname needs to maintain randomization tables on memory during execution. 
Memory overhead for one randomization table is 54 bytes, 106 bytes, 209 bytes and 417 bytes respectively when \sysname is configured to randomize 1 to 1, 2, 4, and 8 bytes. 
At runtime, the memory overhead is $\sim$1MB on large programs such as WordPress, with 8 bytes randomization scheme.
}
}
%\begin{figure}[h]
 %   \centering
 %   \includegraphics[width=0.7\columnwidth]{fig%/db_perf.pdf}
%     \caption{Overhead of \sysname on Database %Engines}
%     \label{fig:perf_db}
%\end{figure}


%\noindent
%{\bf Microbenchmarks.}
%We conduct additional microbenchmarks on each instrumented APIs in PHP, C, Lua, and JavaScript.
%We break down the APIs into three different types: Command, XML, and SQL APIs for APIs that execute commands, XML queries, and SQL queries, respectively.
%
%The results show that the overhead is reasonable (from 1.26\% to 5.71\%). %We present details in Appendix~\ref{appendix:microbenchmarks_apis}.
%In addition, we conduct microbenchmarks on the instrumented OpenWrt firmware's uHTTPd~\cite{uHTTPd} and LuCI web configuration interface~\cite{LuCI}. The average overhead is less than 6\%~\cite{csr-tool}. % (Details in Appendix~\ref{appendix:embedded_devices_perf}).  
%More details can be found on \cite{csr-tool}.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{-1em}
\subsection{Case Study}
\label{subsec:casestudy}
%\vspace{-0.5em}
\updated{We present two case studies in this section and two additional case studies in Appendix~\ref{appendix:case_study}.}{}

%\vspace{-1em}
\subsubsection{Advanced SQL Injections Exploiting Parsers}
\label{subsubsec:advanced_sql_injection}
We present a few sophisticated injection attacks exploiting flaws of 11 popular parsers~\cite{andialbrecht/sqlparse, xwb1989/sqlparser, client9/libinjection, greenlion/PHP-SQL-Parser, moz-sql-parser, hyrise/sql-parser, alibaba/nquery, phpmyadmin/sql-parser, flora-sql-parser, node-sql-parser, druid-sql-parser}, showing the weaknesses of the parser-based for randomization approaches~\cite{sqlrand, autorand, diglossia}.
All of the cases are successfully prevented by \sysname, demonstrating the effectiveness of \sysname.
%
%\autoref{fig:sqlinjection_examples} shows 3 types of SQL injection attacks (A1, A2, and A3 for Attack 1, 2, and 3 respectively) that existing parsers fail to recognize.
%Note that existing randomization techniques such as SQLRand~\cite{sqlrand} and AutoRand~\cite{autorand} use parsers to randomize and derandomize SQL queries (keywords). 
%Hence, if parsers fail to recognize certain structures, keywords in the structures are not randomized, and attackers leverage them to inject malicious queries.
%
%\sysname  \emph{handles (i.e., prevents) all of them}.


\begin{figure}[h]
    \centering
    %\vspace{-0.5em}
    \includegraphics[width=0.95\columnwidth]{fig/sqlexamples.pdf}
    \vspace{-1em}
     \caption{SQL Injections that Parsers Fail to Recognize (Yellow: keywords, Blue: strings)}
    \label{fig:sqlinjection_examples}
     %\vspace{-1em}
\end{figure}


\noindent
{\bf A1: Dialect SQL Grammar ((a), (b), and (c)).} 
The dialect SQL attack shows that using a parser is an insecure design choice of the existing techniques. 
For instance, MySQL supports a SQL dialect: if a query in a comment starts with ``\code{/*!}'', it can be executed, as shown in \autoref{fig:sqlinjection_examples}-(a).
However, many parsers do not support this dialect.
An attacker can inject a malicious payload inside the comment, exploiting parsers that cannot recognize queries in a comment.
We confirmed py-sqlparse~\cite{andialbrecht/sqlparse} and JS-parser~\cite{node-sql-parser} fail to recognize injected queries in a comment as shown in \autoref{fig:sqlinjection_examples}-(b).
In addition, as shown in \autoref{fig:sqlinjection_examples}-(c), PostgreSQL~\cite{PostgreSQL} considers the `\#' symbol as an XOR operator, while others typically consider it as a single line comment operator. 
An attacker can also inject a malicious query with `\#'. 
Note that some techniques may automatically remove queries after `\#', removing injected queries. However, this will break benign queries using \# is an XOR operator as shown in as shown in \autoref{fig:sqlinjection_examples}-(c): doing a simple XOR encryption on a password.


\noindent
{\bf A2: Sub-query Parsing Error ((d) and (e)).} 
Attackers can inject malicious queries as a subquery to exploit approaches relying on parsers that cannot parse sub-queries correctly.
For instance, \autoref{fig:sqlinjection_examples}-(d) shows a SQL statement including two queries where the second query is a sub-query. 
As shown in \autoref{fig:sqlinjection_examples}-(e), php-parse~\cite{phpmyadmin/sql-parser} parses the entire sub-query as a string. %Hence, techniques using php-parse cannot prevent attacks that inject a malicious query into the sub-query.
%Highlighted terms are the keywords (to be randomized and derandomized by a parser).
Note that we present a specific case study for this attack type in Section~\ref{subsec:comparison_existing}.


\noindent
{\bf A3: String Parsing Error ((f) and (g)).} 
Moz-sqlparser~\cite{moz-sql-parser} and py-sqlparse~\cite{andialbrecht/sqlparse} have a bug in parsing a string~\cite{sqlparsebug}, allowing injected queries to be considered as a string that is not a randomization target.
For example, \autoref{fig:sqlinjection_examples}-(f) shows two SQL queries where the Query 2 is an injected malicious query. 
\cite{moz-sql-parser, andialbrecht/sqlparse} mistakenly consider the entire second query as a part of a string (blue marked).
%Note that if previous techniques use such parsers, they will not be able to prevent the injected query.


%\autoref{fig:sqlinjection_examples}-(f) shows two SQL queries where the Query 2 is an injected malicious query. 
%Moz-sqlparser~\cite{moz-sql-parser} and py-sqlparse~\cite{andialbrecht/sqlparse} fail to recognize the second query, as shown in \autoref{fig:sqlinjection_examples}-(g).
%This is because of a bug in parsing a string~\cite{sqlparsebug}, and they consider the entire second query as a part of a string (as highlighted with blue). Note that if previous techniques use such parsers, they will not be able to prevent the injected query.

%\vspace{-1em}
\revised{
\subsubsection{Comparison with Existing Techniques}
\label{subsec:comparison_existing}
We compare \sysname with two state-of-the-art techniques~\cite{diglossia, sqlrand-llvm}.


\begin{figure}[h]
    \centering  
    %\vspace{-1em}
     \includegraphics[width=1.0\columnwidth]{fig/diglossia_failure2.pdf}
     \vspace{-2em}
     \caption{Diglossia with php-parse~\cite{phpmyadmin/sql-parser}}
     \vspace{-1em}
    \label{fig:diglossia_failure_2} 
\end{figure}

\noindent
\textbf{Diglossia~\cite{diglossia} vs \sysname.}
%Diglossia uses an additional shadow parser that uses a different language from the original parser. % to detect injected code.
Diglossia runs two parsers, the original parser and the shadow parser, together.
The shadow parser is created to use a different language than the original parser and its input is obtained by translating the original input into the other language. 
At runtime, it obtains two parse trees from the parsers.
Different nodes between the trees indicate the parts originated from untrusted sources. 
If identical nodes are representing keywords (not strings/numbers), it detects an injection attack.
In this experiment, we implement our own version of Diglossia using php-parse~\cite{phpmyadmin/sql-parser} and \sysname's randomization scheme for the translation, since Diglossia does not provide its source code.





\autoref{fig:diglossia_failure_2}-(a) shows a vulnerable PHP code.
Given the malicious input shown in \autoref{fig:diglossia_failure_2}-(b), the malicious query is injected as shown in \autoref{fig:diglossia_failure_2}-(c).
%
As explained in Section~\ref{subsubsec:advanced_sql_injection}, the parser failed to parse the subquery after the \code{IN} keyword, resulting in an incorrect tree as shown in \autoref{fig:diglossia_failure_2}-(d). 
The last two children nodes (with red borders) of \code{WHERE} are unknown type nodes. 
When the malicious input is injected, both parse trees have the injected query as unknown nodes, resulting in a broken trees. As a result, it failed to recognize injected query. 
%Note that `\code{X}' appears twice in \autoref{fig:diglossia_failure_2}-(d).
%In other words, 
Note that \autoref{fig:diglossia_failure_2}-(d) and (e) show that they have identical nodes, marked with red borders. However, they are considered as literal nodes, hence not considered as an injected code. 
Worse, while the parser fails to process the query, it does not show error messages but silently suppresses the errors, missing the opportunities to detect the attack.
%To this end, the injected query is allowed. 
On the other hand, \sysname successfully prevents the injected SQL query \code{DROP TABLE users} from being executed.
Note that the performance of \sysname (about 5\%) is slightly better than and Diglossia (7.54\%).


\noindent
\textbf{sqlrand-llvm~\cite{sqlrand-llvm} vs \sysname.}
sqlrand-llvm~\cite{sqlrand-llvm} is an implementation of SQLRand using LLVM. %While this aims to implement SQLRand, there are a few different design choices made by sqlrand-llvm. 
It hooks \code{mysql\_query()} to tokenize a randomized input query and compare each token with a list of randomized SQL keywords. % via a string operation. 
It then derandomizes the matched tokens and then pass the deranomized query to the SQL engine. 


\begin{figure}[ht]
    \centering
    %\vspace{-0.5em}
    \includegraphics[width=0.9\columnwidth]{fig/sqlrand_injected.pdf}
    \vspace{-1em}
     \caption{SQL Injection Example with sqlrand-llvm~\cite{sqlrand-llvm}}
     \vspace{-1em}
     \label{fig:sqlrand-injected}
\end{figure}


{\it 1) SQL Injection with New Keywords:} 
sqlrand-llvm maintains a list of known SQL keywords and another list of randomized keywords. 
If a query with a keyword that is not included in the list are injected, it cannot prevent.
For instance, as shown in \autoref{fig:sqlrand-injected}, SQL keywords in the original query are randomized by appending \code{123} to the keywords.
During the derandomization process, if it encounters a SQL keyword that is not randomized, it considers the keyword is injected. 
However, it does not support \code{exec} and \code{proc} keywords according to the sqlrand-llvm's source code~\cite{sqlrand-llvm}.
%However, for a keyword that is not included in the list of known SQL keywords, it allows the query.
As shown in \autoref{fig:sqlrand-injected}, \code{exec} and \code{proc} in the injected query (highlighted) are not detected.
Note that \code{exec proc} can execute a stored procedure called \code{proc}.
%
\sysname prevents the attack with a similar performance: \sysname (5\%) and sqlrand-llvm (4.13\%).
%Since \sysname does not leverage a list of known SQL keywords, all terms go through the dual randomization scheme, \updated{resulting in randomized injected SQL query (i.e., \code{\color{red} ; hshp joep})}{breaking injected SQL queries}.


{\it 2) Breaking Benign Queries:} 
sqlrand-llvm uses \code{strtok()} to randomize/derandomize keywords even if they are a part of a string.
This results in an error if a string contains a SQL keyword. 
Consider a query ``\code{select * from users where name=`\$name'}'', where the value of \code{\$name} is `\code{grant}'. Its randomized query is ``\code{select123 * from123 users where123 name=`grant'}''. 
The value \code{grant} is from the user at runtime, hence not randomized. Unfortunately, \code{grant} is one of the known SQL keywords used in sqlrand-llvm, meaning that it will detect an injection attack (false positive) because the \code{grant} is not randomized.
\sysname does not have this issue as it does not randomize string type values.
}


