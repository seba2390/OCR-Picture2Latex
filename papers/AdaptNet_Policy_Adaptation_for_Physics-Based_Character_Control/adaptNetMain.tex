\documentclass[acmtog]{acmart}

\acmSubmissionID{543}

\usepackage{booktabs} %

\citestyle{acmauthoryear}



\usepackage[ruled]{algorithm2e} %
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}

\acmJournal{TOG}
\acmVolume{42}
\acmNumber{6}
\acmArticle{112.1522}
\acmYear{2023}
\acmMonth{12}


\acmDOI{10.1145/3618375}

\usepackage{subcaption}
\usepackage{soul}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{array}


\begin{document}
\title{AdaptNet: Policy Adaptation for Physics-Based Character Control}

\author{Pei Xu}
\orcid{0000-0001-7851-3971}
\affiliation{%
 \institution{Clemson University}
 \country{USA}}
 \affiliation{
 \institution{Roblox}
 \country{USA}}
\email{peix@clemson.edu}


\author{Kaixiang Xie}
\orcid{0000-0002-5877-9374}
\affiliation{%
 \institution{McGill University}
 \country{Canada}}
\email{kaixiang.xie@mail.mcgill.ca}

\author{Sheldon Andrews}
\orcid{0000-0001-9776-117X}
\affiliation{%
 \institution{École de Technologie Supérieure}
 \country{Canada}}
 \affiliation{
 \institution{Roblox}
 \country{USA}}
\email{sheldon.andrews@gmail.com}

\author{Paul G. Kry}
\orcid{0000-0003-4176-6857}
\affiliation{%
 \institution{McGill University}
 \country{Canada}}
\email{kry@cs.mcgill.ca}

\author{Michael Neff}
\orcid{0000-0003-0226-2808}
\affiliation{%
 \institution{University of California, Davis}
 \country{USA}}
\email{mpneff@ucdavis.edu}


\author{Morgan McGuire}
\orcid{0000-0003-1074-0953}
 \affiliation{
 \institution{Roblox}
 \country{USA}}
\affiliation{%
 \institution{University of Waterloo}
 \country{Canada}}
\email{morgan@roblox.com}


\author{Ioannis Karamouzas}
\orcid{0009-0000-4315-6556}
\affiliation{%
 \institution{University of California, Riverside}
 \country{USA}} 
\email{ioannis@cs.ucr.edu}

\author{Victor Zordan}
\orcid{0000-0002-7309-7013}
\affiliation{
 \institution{Roblox}
 \country{USA}}
\affiliation{%
 \institution{Clemson University}
 \country{USA}}
\email{vbzordan@roblox.com}



\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010371.10010352</concept_id>
       <concept_desc>Computing methodologies~Animation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010371.10010352.10010379</concept_id>
       <concept_desc>Computing methodologies~Physical simulation</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010257.10010258.10010261</concept_id>
       <concept_desc>Computing methodologies~Reinforcement learning</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Animation}
\ccsdesc[300]{Computing methodologies~Physical simulation}
\ccsdesc[300]{Computing methodologies~Reinforcement learning}


\keywords{character animation, physics-based control, motion synthesis, reinforcement learning, motion style transfer, domain adaptation, GAN}

\renewcommand{\shortauthors}{P. Xu, K. Xie, S. Andrews, P. Kry, M. Neff, M. McGuire, I. Karamouzas, and V. Zordan}

\begin{abstract}
Motivated by humans' ability to adapt skills in the learning of new ones, this paper presents
AdaptNet, an approach 
for modifying 
the latent space of %
existing policies 
to allow  
new behaviors to be quickly learned from like tasks in comparison to learning from scratch.  
Building on top of a given reinforcement learning  controller, %
AdaptNet %
uses a two-tier hierarchy that augments the original state embedding to support modest changes in a behavior and further modifies the policy network %
layers to make more substantive changes.   
The technique is shown to be effective for adapting existing physics-based controllers to a wide range of new styles for locomotion, new task targets, changes in character morphology and extensive changes in environment. Furthermore, it exhibits
significant increase in learning efficiency, as indicated by greatly reduced training times when compared to training from scratch or using other approaches that modify existing policies.
Code is available at 
\href{https://motion-lab.github.io/AdaptNet}{\textit{https://motion-lab.github.io/AdaptNet}}.




\end{abstract}

\begin{teaserfigure}
\centering
    \includegraphics[width=\linewidth]{images/adaptnet_teaser2.png}\hfill
    \caption{
    Examples policy adaptation for locomotion.
    From left to right and top to bottom: 
    motion interpolation, local collision avoidance, 
    body-length changes, style transfer, morphology changes, 
    rough terrain adaptation.  
}
    \label{fig:teaser}
\end{teaserfigure}



\maketitle

\input{adaptNetBody}


\end{document}
