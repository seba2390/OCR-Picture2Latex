

\section{Experimental Setup}

To investigate the interpolation of sparse retrievers with dense retriever PRF approaches, we devise a number of empirical experiments aimed at investigating: 1) the impact of interpolation on different dense retriever PRF approaches; 2) the impact of interpolating sparse retrievers before/after/both the PRF; 3) the impact of interpolating on different sparse retrievers, unsupervised (BOWs) or learned.

\textbf{Datasets.} For all of our experiments, we use the TREC Deep Learning Track passage retrieval task 2019~\cite{craswell2020overview} (DL19) and 2020~\cite{craswell2021overview} (DL20). DL19 contains 43 judged queries, while DL20 contains 54 judged queries. The relevance judgement levels for both datasets  range from 0 (not relevant) to 3 (highly relevant). 
%However, among these four relevance labels, relevance label 1 needs to pay extra attention to, because 1 here indicates the passages are only on-topic but not really relevant. Therefore, 
We treat passages with relevance label 1 as not relevant when we compute the binary relevance metrics (i.e., MAP, Recall). The passage collection in our experiments is the MS MARCO Passage Ranking Dataset~\cite{nguyen2016ms}, which is a benchmark English dataset for ad-hoc retrieval that contains $\approx$8.8 million passages. The average judgements per query for DL19 and DL20 are 215.3 and 210.9, whereas the MS MARCO Passage Ranking Dataset only has $\approx$1 judgement per query.

%Because the average judgements for MS MARCO Passage Ranking Dataset queries is only $\approx$1, we do not evaluate our methods on this dataset, since it does not make sense to evaluate PRF methods with a dataset that only has 1 judgement per query.

%\subsection{Baselines}

\textbf{Baselines.} We include:
\begin{itemize}[leftmargin=*]
	\item \texttt{ANCE}: First stage dense retriever~\cite{xiong2020approximate}. We use the model implemented in Pyserini\footnote{\url{https://github.com/castorini/pyserini/blob/master/docs/experiments-ance.md}}~\cite{lin2021pyserini} for inference;
	\item \texttt{Vector-PRF (VPRF)}: A simple Rocchio PRF approach based on dense retrievers~\cite{li2021pseudo}. We use the model implemented in Pyserini\footnote{\label{vprf}\url{https://github.com/castorini/pyserini/blob/master/docs/experiments-vector-prf.md}}~\cite{lin2021pyserini};
	\item \texttt{TCT ColBERT V2 HN+ (TCTv2)}: A BERT-style distilled dense retriever learned from ColBERT with reduced query/passage embedding dimensions~\cite{lin2021batch}; %Instead of every token having its own embedding,  the output format from either query or passage encoder is the same as ANCE;
	\item \texttt{TCT ColBERT V2 HN+ VPRF (TCTv2+VPRF)}: The application of the Rocchio VPRF from~\citet{li2021pseudo} on top of TCT ColBERT V2 HN+ dense retriever. This model is also made available by the authors in Pyserini\footnoteref{vprf}~\cite{lin2021pyserini};
	\item \texttt{DistilBERT KD TASB (DBB)}: A DistilBERT-style dense retriever with balanced topic aware sampling training strategy~\cite{hofstatter2021efficiently}. We use the model implemented in Pyserini\footnote{\url{https://github.com/castorini/pyserini/blob/master/docs/experiments-distilbert_tasb.md}}~\cite{lin2021pyserini} by the original authors;
	\item \texttt{DistilBERT KD TASB + VPRF (DBB+VPRF)}: The application of the Rocchio VPRF from~\citet{li2021pseudo} on top of DistilBERT KD TASB dense retriever. This model is implemented by~\citet{li2021pseudo} and available to use in Pyserini\footnoteref{vprf}~\cite{lin2021pyserini}.
\end{itemize}

In our experiments, we use the parameters $\alpha=0.4$, $\beta=0.6$, and PRF depth = 3 for Rocchio VPRF, following the settings recommended by~\citet{li2021pseudo}. In terms of the interpolation parameter $\lambda$, we use $\lambda=0.5$ for all  experiments.  For generating the BM25 runs to be used for interpolation, we use the BM25 implementation provided by Pyserini~\cite{lin2021pyserini} and we use the default parameter values for $k_1$ and $b$ within Pyserini. For generating uniCOIL runs, we also use the pre-built uniCOIL index provided by Pyserini.

\textbf{Evaluation Measures.} We use the official evaluation metrics from DL19 and DL20: nDCG@10 and Recall@1000. We also report MAP as a complementary metric.

%To measure the similarity and diversity between the original PRF run and the interpolated PRF run, we also include the Jaccard Similarity (JS) metric. The JS measurement is evaluated by taking the ratio of intersection over a union of the two result lists.

%In order to measure the change of the number of relevant passages in the top 3 results before and after the interpolation without considering the rankings, we also report nCG@3, because PRF does not take the ranking of the candidate passages into account, therefore we remove the discount part from the nDCG@3.