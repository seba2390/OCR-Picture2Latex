\section{Methods}

Next, we introduce our method for interpolating sparse retriever and dense retrievers in the context of PRF, and in particular of the Vector-PRF approach~\cite{li2021pseudo}. In this paper, we adopt the same interpolation strategy used by~\citet{wang2021bert}: a linear interpolation between sparse retriever scores and dense retriever scores, as shown in equation~\ref{eq:inter}.

\begin{equation}
	\label{eq:inter}
	s(p) = \lambda\hat{s}_{Sparse}(p) + (1 - \lambda)s_{Dense}(p)
\end{equation}

\noindent that is, the score of a passage is the linear interpolation of the sparse retriever score of the passage and the score of the same passage from the dense model, modulated by a parameter $\lambda$, which controls the contribution of the sparse retriever score to the final score of the passage. In our experiments, \texttt{Sparse} refers to BM25 or uniCOIL, \texttt{Dense} refers to any dense retrieval model we use. This interpolation mechanism is a simple yet effective approach to ``help'' the dense retrievers capture the passages' weak relevance signals.


As mentioned, for PRF, we used the Rocchio Vector-PRF approach of \citet{li2021pseudo}.
%To investigate the research questions we have proposed, we use the PRF approaches proposed by \citet{li2021pseudo}. 
The way we do interpolation with PRF can be categorized into three different types; we discuss each type with detail in the following subsections.

\subsection{Pre-PRF Interpolation}

The PRF process often involves two rounds of retrieval~\cite{li2021pseudo}. The first retrieval round is to generate the initial results for preparing PRF feedback candidates. After getting the initial retrieval results, top-$k$ passages for each query from these results are selected as PRF feedback passages and are used to modify the original query representations. Therefore, the interpolation can be performed at either round of retrieval.

For Pre-PRF zero-shot interpolation, we perform the sparse retriever interpolation with the first round of dense retrieval results, then we apply PRF with the interpolated results to generate the new query representations for the second round of retrieval. After the interpolation, the ranking of the passages in the results are likely to be different, affecting the PRF's performance. 
%In this whole process, no training is involved, the models are not trained with the new interpolated results. We apply this approach to all the models we selected in our experiments.

\subsection{Post-PRF Interpolation}

Other than applying interpolation before the PRF to the initially retrieved results, we also apply interpolation to the results after the second round of retrieval with PRF query representations. In this approach, the initial retrieval results are directly used for generating PRF query representations, then the PRF queries are used to perform a second round of retrieval. After the second round of retrieval, the results are then interpolated with the sparse retriever's results to obtain the final results list. 
%During this process, no model training is involved as well, the models used in our experiments are zero-shot in this approach.

\subsection{Both-PRF interpolation}
Finally, Both-PRF performs interpolation before and after PRF. To perform Both-PRF, we firstly interpolate the sparse retriever's results with the dense retriever's results, then we apply PRF with the interpolated results to generate the new query representations for second round of retrieval. Then results from the second round of retrieval are again interpolated with the sparse retriever's results to generate the final results list.