\section{Related Work}

%Pseudo-Relevance Feedback approaches with bag-of-words models such as BM25 has been extensively studied in the past, with the aim to address the mismatch between query intent and query representation~\cite{clinchant2013a,wang2020pseudo}. A typical PRF setting is to use the top-ranked passages from a retrieval system as relevant signals from which to select query terms to add to the original query or to set the weights for the query terms. Traditional PRF approaches include KL expansion~\cite{lv2014revisiting,zhai2001model}, RM3~\cite{lv2009comparative}, relevance models~\cite{lavrenko2001relevance} and Rocchio~\cite{rocchio1971rocchio}. The use of PRF models on top of efficient bag-of-words retrieval models is common in information retrieval systems and it has been shown to be an effective strategy to modify the query representation for improving retrieval effectiveness~\cite{zhai2001model,lv2014revisiting,clinchant2013a}.

%Transformer-based deep language models~\cite{vaswani2017attention}, such as BERT~\cite{devlin2018bert}, T5~\cite{raffel2020exploring}, and RoBERTa~\cite{liu2019roberta}, have achieved superior performance compared to the existing state-of-the-art models in different information retrieval tasks. However, due to the expensive computational overhead of these models, most pipelines are only adopting them as second stage re-rankers to re-rank the initially retrieved results~\cite{nogueira2019passage,lin2020pretrained}. Recent research has integrated PRF with deep language models to further enhance the effectiveness of ranking. \citet{yu2021pgt} proposed a fully connected graph transformer model that uses graph nodes to contextualise query, candidate passages, and PRF feedback passages, with inter- and intra-sequence attention to capture the PRF signals between graph nodes. While the proposed model achieves better effectiveness than BERT re-ranker, it still consumes 88\% as many computations on each input compared to BERT-PRF~\cite{yu2021pgt}, which makes this model still expensive to be applied in practical applications.

%Dense retriever has attracted lots of attention in recent years. Unlike the deep language model re-rankers, i.e. BERT, which uses a cross-encoder to jointly encode both query and passage, the dense retrievers are using dual-encoders to encode query and passage separately~\cite{xiong2020approximate,lin2020distilling,lin2021batch,qu2021rocketqa,ren2021rocketqav2,hofstatter2021efficiently}. The passages are encoded at indexing time and stored offline. During query time, only queries are encoded "on-the-fly". By doing this, dense retrievers achieve a good balance between efficiency and effectiveness. On one hand, they have comparable efficiency with traditional bag-of-words retrieval models, but with superior effectiveness. On the other hand, they are less effective than deep language model re-rankers, but with superior efficiency.

There are two lines of research that are related to our work. The first line of research  investigates the integration of PRF with dense retrievers. \citet{li2021pseudo} proposed a simple PRF method called Vector-PRF which adapted the classic Rocchio PRF method~\cite{rocchio1971rocchio} used on bag-of-words representations, to dense retrievers in a zero-shot manner. Vector-PRF has been shown to improve effectiveness, at  additional minimal efficiency expense. We adopt this method in our paper. \citet{wang2021pseudo} proposed a more complex model that uses a clustering technique to model the PRF signals; this is in turn applied to the ColBERT dense retriever~\cite{khattab2020colbert}. However, the improvements achieved by this method come at the cost of efficiency. \citet{yu2021improving}, on the other hand, proposed the ANCE-PRF model that requires the training of a new query encoder based on the original ANCE~\cite{xiong2020approximate} query encoder. ANCE-PRF achieved significant improvements over ANCE. However, due to the input limit of the BERT-style model (512 tokens~\cite{devlin2018bert}), ANCE-PRF is limited in the amount of feedback it can consider: experimentally, $k=5$ is the maximum PRF depth for MS MARCO. % the same limitation (PRF cannot exceed a certain depth).
%On the other hand, Vector-PRF proposed by~\citet{li2021pseudo} does not suffer from this limitation.

The second line of research regards the interpolation of sparse retrieval models and deep language models to further boost effectiveness, especially in terms of recall. \citet{wang2021bert} investigated the interpolation of BM25 and dense retrievers. Their findings suggest that dense retrievers are highly effective in encoding strong relevance signals, but they are not as effective when dealing with weak relevance signals. The interpolation of BM25 and dense retrievers is able to make up for each other's weaknesses: this interpolation can significantly improve the effectiveness of dense retrievers. Furthermore, \citet{lin2021batch,lin2021few} and \citet{arabzadeh2021predicting} also investigated different approaches to combine learned sparse retrieval results with dense retrieval results to improve retrieval effectiveness, and significant improvements are recorded from their experiments.
Importantly, according to~\citet{wang2021bert}, dense retrievers are not so good at dealing with weak relevance signals. Therefore, PRF approaches based on dense retrievers might also inherit this limitation. To the best of our knowledge, there is no previous study that has examined the interpolation of sparse and dense retrievers within the PRF framework.

