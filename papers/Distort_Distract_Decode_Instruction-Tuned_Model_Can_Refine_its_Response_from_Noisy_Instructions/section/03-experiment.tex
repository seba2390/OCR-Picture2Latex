\vspace{-10pt}
\section{Experiments}
\label{sec3:experiment}
\vspace{-5pt}
\subsection{Experimental Setup}
\vspace{-5pt}
\paragraph{Datasets} 
For our experiments, two datasets are utilized: \textsc{SupNatInst}\,\citep{sni_dataset} and \textsc{UnNatInst}\,\citep{unnatural_ni_dataset}. Both datasets feature a diverse collection of crowd-sourced NLP tasks. In \textsc{SupNatInst}, each task is formatted as a `Definition' prompt that acts as the instruction. For zero-shot evaluations, only the `Definition' is utilized, whereas two positive demonstration examples are incorporated for few-shot evaluations. Our experiments focus solely on the English segment of the dataset, and 100 instances per tasks are used for evaluation following Wang et al. \citep{sni_dataset}. This subset comprises 119 evaluation tasks, grouped into 12 categories:
%
\small
\vspace{-1pt}
\begin{multicols}{3}
    \begin{itemize}[left=0pt, wide, labelsep=2.5pt]
        \item \textbf{AC:} Answerability Classification
        \item \textbf{CEC:} Cause-Effect Classification
        \item \textbf{CR:} Coherence Resolution
        \item \textbf{DT:} Data-to-Text
        \item \textbf{DAR:} Dialogue Act Recognition
        \item \textbf{GEC:} Grammar Error Correction
        \item \textbf{KT:} Keyword Tagging
        \item \textbf{OE:} Overlap Extraction
        \item \textbf{QR:} Question Rewriting
        \item \textbf{TE:} Textual Entailment
        \item \textbf{TG:} Title Generation
        \item \textbf{WA:} Word Analogy
    \end{itemize}
\end{multicols}
\normalsize
\vspace{-10pt}
%
The \textsc{UnNatInst} dataset features LM-generated instructions based on an initial set of 15 seed samples. From its 64,000 samples, we evaluate a subset of 10,000.

\vspace{-5pt}
\paragraph{Models} We use the T\textit{k}-instruct models\,\citep{sni_dataset}, instruction-tuned from T5-LM\,\citep{t5-lm}. These models are trained across 757 english tasks from the \textsc{SupNatInst} training split over 2 epochs, with each task comprising 100 samples.
Our evaluation primarily involves three sizes of T\textit{k}-Instruct models: Large (770M), XL (3B), and XXL (11B). While T\textit{k}-XL and T\textit{k}-XXL come from publicly available checkpoints, the 770M model is manually trained under the same settings as the other T\textit{k}-instruct models.
Additionally, T0 (3B), Alpaca (7B), and Open-instruct-SNI (OpenSNI) are also used for further evaluations. T0 model also fine-tunes T5-LM \,\citep{t5-lm} using task prompts sourced from PromptSource \,\citep{promptsource}. Alpaca \,\citep{alpaca} fine-tunes the LLaMA \,\citep{llama} based on a style outlined by Wang et al. \citep{self_instruct}, whereas OpenSNI \,\citep{tulu} is a fine-tuned version of LLaMA on \textsc{SupNatInst}, marking a distinct way of use from Alpaca.
% specifically aligned with and trained on \textsc{SupNatInst}, marking a distinct approach from Alpaca's utilization of the LLaMA. 
In our experiments, greedy decoding is primarily employed for these models.

\vspace{-5pt}
\paragraph{Evaluation Metrics} We examine the outputs of instruction-tuned LMs on \textit{unseen} tasks. Unless specified, all evaluations are conducted in a zero-shot setting, where the models perform tasks based solely on instructions, without any demonstration examples. Task performance is measured using the Rouge-L score\,\citep{rouge}, which measures the overlap between generated and reference sequences, and is often used for open-ended tasks as Wang et al. \,\citep{sni_dataset}. Adding to the Rouge-L score, classification tasks further use the Exact Match (EM) metric, which measures whether the response precisely matches a pre-defined label. 
To better evaluate pragmatics\,\citep{pragmatics} not captured by metrics like EM or Rouge-L, we introduce two additional metrics: \textit{Label Adherence} and \textit{Label Coherence}. These metrics offer insights into how closely the generated responses adhere to the provided instructions. Detailed explanations of our evaluation metrics are as follows:
%
\begin{itemize}[left=10pt]
    \item \textbf{Label Adherence (LA)}: LA checks if the response stays within the label space defined by the instruction, regardless of its agreement with the golden label. For example, if the instruction specifies answers as `True' or `False', any response within this set is deemed conforming.
    %
    \item \textbf{Label Coherence (LC)}: This metric evaluates the semantic alignment of the response with the gold label, allowing for near-equivalent answers. For example, responses like `Correct' may align with a gold label of `True'. We compare responses against an expanded set of gold labels with semantically equivalent expressions.
\end{itemize}

For a more comprehensive evaluation, LA and LC are primarily measured on classification tasks identifying 58 tasks among the 119 unseen tasks in \textsc{SupNatInst}, which contains the predefined labels. Although adherence and coherence are valuable for open-ended generation, focusing on classification ensures thorough evaluation. For clarity, an example illustrating the relationship between EM, LA, and LC is provided with further details on evaluation in \textcolor{red}{Appendix} \ref{metric_details}.

\vspace{-5pt}
\subsection{Performance on Unseen Task Generalization}
\vspace{-5pt}
\paragraph{Result Overview}
\input{materials/tables/table_sni_benchmark}

\autoref{tab:sni_benchmark} displays the results when applying Instructive Decoding (ID) to the T\textit{k}-Instruct models and OpenSNI-7B model. ID consistently outperforms the baseline model, which employs only the standard instruction, as indicated by higher overall Rouge-L scores. This performance advantage is evident across all types of noisy instructions. Notably, while larger models generally yield higher scores, the improvements are not uniformly distributed across task categories. For instance, the `OE (Overlap Extraction)' task shows a slight performance decline, which hints at possible architectural limitations for learning in this specific task  Nevertheless, the `opposite' variant consistently results in the most significant improvements in Rouge-L scores across all model sizes, thus affirming the robustness of our method.

\vspace{-5pt}
\paragraph{From Degradation to Enhancement: The Two-fold Impact of Noisy Instructions}

When used in a standard decoding process, noisy instructions lead to a significant decline in performance for generated responses. However, when integrated into ID, these instructions actually enhance performance. We attempt to unveil the relationship between such degradation and its enhancement with ID (\autoref{fig:task_results}\,\textcolor{red}{(a)}). When replacing the original instruction with a noisy variant during the decoding process, a noticeable drop in Rouge-L scores occurs, as shown on the x-axis labeled `degradation'. The y-axis displays the performance improvement gained through ID when using these noisy instructions. Interestingly, we find a strong positive correlation between the initial drop in performance and the subsequent improvement when using ID. This correlation is quantified using the Pearson Correlation Coefficient ($R$ in \autoref{fig:task_results}\,\textcolor{red}{(a)}; Cohen et al. \citep{cohen2009pearson}). The more substantial the initial drop caused by a noisy instruction, the greater the performance gain when it is integrated into ID. Notably, the `opposite' instruction, which causes the most significant initial decline, results in the largest performance boost when used with ID.

\vspace{-5pt}
\paragraph{Comparative Winning Rates of Base vs. Ours} 

\begin{figure}[b]
\vspace{-10pt}
    \begin{subfigure}{0.315\textwidth}
        \centering
        \includegraphics[width=\linewidth]{materials/figures/degrade_improve.pdf}
        \vspace{-5pt}
        \caption{Degradation vs. ID Boost}
        % \label{fig3_3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.657\textwidth}
        \centering
        \includegraphics[width=\linewidth]{materials/figures/base_vs_ours.pdf}
        \vspace{-15pt}
        \caption{Comparative winning rates of Base vs. Ours}
        % \label{fig3_1}
    \end{subfigure}
    \vspace{-5pt}
    \caption{(a) Correlation between performance degradation with noisy instructions and improvement with those used in ID; (b) comparative winning rates of Base vs. Ours. on held-out tasks. The \textcolor{blue}{blue} bars show base method wins, while the \textcolor{green!60!black}{green} bars indicate our method's dominance.}
    \label{fig:task_results}
    \vspace{-15pt}
\end{figure}

\autoref{fig:task_results} \textcolor{red}{(b)} illustrates tasks where ID outperforms the baseline, as measured by the Rouge-L score. This improvement is consistent across a range of tasks, regardless of model size. Although the overall Rouge-L score for T\textit{k}-XXL is on par with that of T\textit{k}-Large and T\textit{k}-XL, distinct improvements are observed across tasks when ID is used with larger models. This synergy appears to optimize the potential of the larger models.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{materials/figures/fig_em_la_lc.pdf}
\caption{Evaluation on three different scales of T\textit{k}-Instruct models (i,e., Large, XL, XXL) with different noisy instructions for instructive decoding over heldout dataset of \textsc{SupNatInst}. Each figure shows the performance changes from applying ID.}
\label{fig:em_ic_sc}
%\vspace{-10pt}
\end{figure}

\vspace{-10pt}
\paragraph{Granular Performance Analysis on the Classification Tasks}

We conduct an in-depth analysis of 58 classification tasks from \textsc{SupNatInst} to scrutinize the shifts in their response outcomes in detail (\autoref{fig:em_ic_sc}). The analysis is segmented into three metrics: EM, LA, and LC. A clear trend emerges: as the model size increases, EM scores also rise. However, when examining the LA and LC metrics based on baseline responses, the T\textit{k}-XL model outperforms the T\textit{k}-XXL model. This suggests that while larger models excel at strictly adhering to provided instructions, smaller models are more effective at generating semantically congruent outputs within the given instructional context. With the incorporation of ID, performance patterns remain largely consistent across different model sizes and noisy instructions. Specifically, as model sizes increase, the 'opposite' variant significantly improves the performances, particularly in the LC metrics for the T\textit{k}-XXL model. The random 'trunc-shuffle' variant exhibits a significant boost in LA scores as model size grows, highlighting the complex interplay between model sizes and their responsiveness to instructions.

\begin{figure}[h]
% \vspace{-5pt}
\centering
\begin{minipage}{0.37\textwidth}
%\small
\centering
\captionof{table}{Rouge-L scores across different models and datasets.}\label{tab:unni_t0}
\resizebox{\textwidth}{!}{%
\begin{tabular}{cccc}
\toprule
Dataset & \textsc{UnNatInst} & \multicolumn{2}{c}{\textsc{SupNatInst}} \\
\midrule
Model & T\textit{k}-Large & T0-3B & Alpaca-7B\\
\midrule\midrule
baseline & 43.25 & 26.58 & 23.61\\
null & \textbf{44.57} & 29.33 & 31.21\\
rand words & 44.44 & \textbf{29.49} & 30.93\\
opposite & 43.42 & 29.46 & \textbf{31.38}\\
\bottomrule
\end{tabular}
}
\end{minipage}%
\hspace{0.5cm}
\begin{minipage}{0.46\textwidth}
\centering
\captionof{table}{Rouge-L scores under a few-shot scenario across different models. We set $\epsilon$ to 0.2.}
\label{tab:few-shot}
\scriptsize
\begin{tabular}{cccc}
\toprule
Model & T\textit{k}-Large & T\textit{k}-XL & Alpaca-7B \\
\midrule\midrule
baseline & 47.63&	54.34&	37.06\\
null & 47.94&	54.78&	\textbf{38.75}\\
null$^*$ &46.95&	54.41&	38.07\\
opposite & \textbf{48.08}&	\textbf{54.80}&	37.79\\
opposite$^*$ & 47.01&	54.51&	 37.55\\
\bottomrule
\end{tabular}
\end{minipage}
\vspace{-5pt}
\end{figure}


\vspace{-5pt}

\subsection{Ablation Study}
\vspace{-5pt}
\paragraph{Generalization Capabilities of ID}

To further assess the adaptability and effectiveness of ID, we cross-evaluate models in the following way: models trained on \textsc{SupNatInst} are tested on \textsc{UnNatInst} and models not trained on \textsc{SupNatInst} are assessed using the \textsc{SupNatInst} test set. \Tableautoref{tab:unni_t0} shows the results, measured through the overall Rouge-L score. For the T\textit{k}-Large model evaluated on the \textsc{UnNatInst} training set, ID consistently outperforms the baseline, even if the `opposite' variant isn't the top performer. Models trained on other datasets, such as T0-3B and Alpaca-7B, also perform better with ID. Notably, there is a significant performance boost, especially for Alpaca-7B. This indicates that ID effectively addresses the shift between training and test distributions, highlighting its versatility and robustness as a broadly applicable solution.

\vspace{-5pt}
\paragraph{Sensitivity of Smoothing Coefficient}
\begin{wrapfigure}{r}{0.33\textwidth}
\vspace{-10pt}
\centering
\includegraphics[width=1.0\linewidth]{materials/figures/ablation.pdf}
\vspace{-20pt}
\caption{Overall Rouge-L scores across varying $\epsilon$ values with 'null' instruction in ID.}
\vspace{-10pt}
\label{fig:null_epsilon}
%\vspace{-10pt}
\end{wrapfigure}

\autoref{fig:null_epsilon} shows the influence of the hyperparameter $\epsilon$ on our method's performance. This parameter adjusts the smoothness of logits derived from noisy instructions. Although our typical choice for $\epsilon$ was 0.3, we evaluated ID-null across a range of $\epsilon$ values, spanning from -0.5 to 0.5 at 0.01 intervals. Performance tends to decline with positive $\epsilon$ values, as the model becomes increasingly biased toward the noisy instruction. Conversely, excessively negative values (below -0.4) lead to a deterioration in performance. Interestingly, the model's performance stabilizes between -0.1 and -0.4, indicating a certain level of robustness to variations in $\epsilon$ within this range.

\vspace{-5pt}
\paragraph{Few-Shot Generalization}

Here, we evaluate how ID performs in the presence of a few positive demonstration examples (i.e., few-shot evaluation).  The results are presented in \Tableautoref{tab:few-shot}. In this table, the terms null' and opposite' refer to the use of noisy instructions without examples, while null$^*$' and opposite$^*$' indicate the incorporation of two positive demonstration examples. The table shows that ID's performance gains are more modest in the few-shot context than in the zero-shot context. This is likely because the baseline performance is already improved by the inclusion of examples, thereby diminishing the benefits of perturbations from $\Tilde{z}$. Nevertheless, we find that the negative impact of noisy instructions is relatively minor, as the provided examples help to clarify the task's intent.
