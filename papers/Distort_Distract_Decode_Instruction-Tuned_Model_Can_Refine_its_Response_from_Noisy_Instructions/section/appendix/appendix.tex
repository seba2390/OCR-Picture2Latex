\clearpage
\appendix
\appendixpage

\startcontents[sections]
% \printcontents[sections]{l}{1}{\setcounter{tocdepth}{2}}

\section{Broader Impact}
\label{sec:broad}
In advancing the domain of instruction-adherence for Language Models (LLM), we introduce an innovative technique, Instructive Decoding (ID). Recognizing the potential paradigm shifts this method might instigate, we find it imperative to discuss its broader implications, especially concerning efficiency, scalability, accessibility, systematic impact, and potential misuse.

\paragraph{Efficiency and Scalability}
Optimizing instruction adherence at the decoding level, as underscored by ID, presents pronounced advantages in both efficiency and scalability. Traditional endeavors to fine-tune instructions often lean on exhaustive training, entailing considerable resource commitments. This not only poses challenges for real-world applicability, especially with behemoth models or voluminous datasets, but also limits scalability. Our decoding-centric method, on the other hand, augments instruction adherence without extensive retraining. This reduction in computational overhead paired with the method's adaptability to diverse tasks signifies a pivotal step towards ensuring future large language models are both instruction-responsive and deployment-efficient.


\paragraph{Accessibility}
ID inherently fosters increased accessibility of instruction-tuned models to a wider spectrum of users. A salient attribute of our methodology is its efficacy in amplifying instruction adherence, even for models with a more modest parameter count (up to 3B). This democratization is potent, especially when considering that our method eschews dependencies on vast datasets, high-end computational resources, or specialized engineering teams. In a machine learning landscape often characterized by escalating computational needs and intricacies, ID emerges as a beacon, rendering top-tier, instruction-adherent models accessible to a more expansive audience. This broad-based accessibility is poised to catalyze novel applications across sectors, enriching both the research community and the general populace.


\paragraph{Systematic Impact}
The introduction of our Instructive Decoding (ID) methodology offers a promising avenue for democratizing advanced instruction-following capabilities in contemporary language models. Independent of their operational scale, organizations and researchers can leverage the enhanced proficiency of LLMs without the typical burdens of exhaustive tuning. This democratization holds the potential to streamline and standardize AI implementations across multifarious industries. Nevertheless, with widespread adoption comes the imperative of rigorous monitoring to identify, mitigate, and rectify unforeseen biases or unintended consequences that may emerge.



\paragraph{Potential Misuses}
The amplification of instruction-adherence in models, while laudable, introduces vulnerabilities that may be exploited for malevolent purposes, such as disseminating misleading narratives or manipulating public discourse. It is our responsibility, as proponents of this technology, to instate robust safeguards, advocate for ethical deployment standards, and formulate stringent usage guidelines. Continuous emphasis should be placed on responsible application, vigilant oversight, and cultivating a user ecosystem that is cognizant of both the potential benefits and inherent risks of such advanced systems.





\section{Limitation \& Future Work}
\subsection{Limitation}
\paragraph{Generalization}
While our method has shown promising results in specific tasks and datasets, it remains uncertain how universally it can be applied across various instruction-following scenarios, languages, and cultures. Future research is essential to validate its effectiveness in diverse contexts and ensure it doesn't inadvertently introduce biases or inaccuracies in untested situations.


\paragraph{Robustness and Stability}
Our approach, though effective under the conditions tested, may exhibit sensitivity to slight variations in instructions or other input parameters. This sensitivity might manifest as inconsistent outputs or varied model performance, emphasizing the need for comprehensive testing across a range of inputs to ensure stable and robust operation.


\paragraph{Resources}
To produce a single output, our method necessitates two separate inferences. This inherent design, while facilitating the desired model behaviors, leads to increased computational overhead. As a consequence, there could be a tangible impact on speed, particularly in resource-constrained environments, and a potential increment in storage requirements due to the need to maintain intermediate representations or states.


\paragraph{Error Propagation}
Given our method's two-step inference process, there's an inherent risk of error propagation: inaccuracies or biases introduced in the initial inference might not only persist but could also be exacerbated in the subsequent inference. Addressing this challenge requires meticulous design and evaluation to ensure that initial errors don't compromise the quality of final outputs.


\subsection{Future Work}
\paragraph{Resource-Efficient ID}
As we explore deeper into the behaviors of instruction-tuned models and the efficacy of ID, a clear trajectory emerges for future exploration: enhancing the resource efficiency of the ID process. While our current methodology has showcased promising results, the computational overhead and time complexity associated with it remain areas of improvement. In future iterations, we aim to refine our algorithms to make the ID process not just effective but also leaner in terms of computational resources. This could involve optimizing the perturbation computations, streamlining the sampling process, or introducing lightweight heuristics to guide the decoding. Such enhancements would make our approach more amenable to real-world applications, where both accuracy and efficiency are paramount.


\paragraph{Robustness on More Diverse Tasks}
Another direction for future research lies in testing the robustness of instruction-tuned models, especially with ID, across a broader spectrum of tasks. While our initial investigations are mainly focused on the analysis of \textsc{SupNatInst} dataset, the potential of this approach could be unearthed by exposing the model to a gamut of diverse challenges â€“ from intricate sequence-to-sequence tasks to multi-modal problem settings. Such an expanded evaluation would provide deeper insights into the model's versatility and its adaptability to various task nuances. Furthermore, it would be intriguing to observe how the model, anchored by its initial instruction, fares in tasks that exhibit high levels of ambiguity or where the boundaries between classes are not starkly defined. Pushing the boundaries in this manner will not only test the model's resilience but also its capability to generalize from one context to another seamlessly.

\paragraph{ID for RLHF Enhanced-LLMs}
Instruction tuning in a supervised manner equips models to respond precisely to clear-cut tasks or queries, but its prowess diminishes when faced with ambiguous or vague questions. Herein lies the significance of Reinforcement Learning from Human Feedback (RLHF). By integrating human feedback into the model's learning process, RLHF ensures that models can interpret and respond to less defined queries in a manner that aligns closely with human intentions. Introducing ID into RLHF-enhanced LLMs emerges as an intriguing avenue to further enhance this capability. While RLHF provides the foundation for models to comprehend and align with human intent, ID can be instrumental in refining the model's adaptability to instructions and user preferences. The amalgamation of RLHF's continuous learning approach with ID's anchoring capabilities may lead to a more contextually adept and user-aligned model. In essence, this synergy could result in LLMs that not only grasp the intricacies of human intent but also consistently generate outputs that are both accurate and contextually relevant, regardless of the clarity or vagueness of the incoming query.


\paragraph{Theoretical Analysis for ID}
ID stands as a distinct mechanism that aligns responses more toward a goal-oriented direction without the need for additional training; it augments the provided instruction to elicit more pertinent outputs from the model. Yet, while its practical benefits are becoming increasingly evident, a deeper theoretical understanding remains a pressing requirement. Specifically, understanding the interplay between the input that's instruction-augmented and how it influences the model's prediction is of paramount importance. A rigorous analysis should explore the level of perturbation this augmented instruction introduces into the model's decision-making process. Furthermore, the inherent trade-offs between the exact match, Rouge-L scores, and semantic coherence in relation to these perturbations need to be delineated. Establishing such a theoretical foundation would provide invaluable insights into how ID effectively alters model behavior, paving the way for more predictable and controlled outcomes. Future research endeavors focusing on these aspects can unveil the precise mechanics at play, allowing for further refinement and optimization of the ID approach.

\section{Experimental Setup Details}
\label{sec:futher_exp}
\paragraph{T\textit{k}-Instruct}
T\textit{k}-Instruct is an instruction-tuned model trained using \textsc{SupNatInst} on the T5-LM within an encoder-decoder architecture. As previously mentioned, we employ the publicly available checkpoints (\href{https://huggingface.co/models?search=allenai/tk-instruct}{T\textit{k}-Instruct public checkpoints}) for T\textit{k}-Instruct, specifically models such as \textit{3b-def, 3b-def-pos}, and \textit{11b-def}, which -def models are zero-shot tuned model and -def-pos model is tuned with additional 2 positive demonstration examples for few-shot generalization. For model sizes not publicly disclosed, we adhere to the training setup provided by Wang et al.\,\citep{sni_dataset} to perform fine-tuning. Only the definition and input are used for training the T\textit{k}-Small (60M), Base (220M), Large models (770M), whereas the T\textit{k}-Large-def-pos is trained with both a definition and two positivie demonstration examples, each adapted from their corresponding T5-LM\,\citep{t5-lm}. The models are trained with a batch size of 16, for 2 epochs, using a learning rate of 5e-5. Due to the absence of an official validation task, training is conducted without splits and the last checkpoint is used for experiments. The number of training instances utilized is 67,825. For both training and evaluation, the combined maximum length of demonstrations and contexts is set to 1,024, while the maximum generation length is limited to 128.

\paragraph{OpenSNI, T0, and Alpaca}
OpenSNI represents a model trained on the \textsc{SupNatInst} for comparison among instruction datasets as depicted by Wang et al.\,\citep{tulu}, following the methods of Touvron et al. \,\citep{llama}. It has been fine-tuned with 96,913 training instances over 2 epochs using a learning rate of 2e-5. Two publicly available variants of this model exists: 7B and 13B, with our experiments using the 7B variant from \href{https://huggingface.co/allenai/open-instruct-sni-7b}{OpenSNI-7B public checkpoint}. We observe a superior performance in the 7B model compared to the 11B variant of T\textit{k}-Instruct (i.e., T\textit{k}-XXL). We attribute this not only to LLaMA's potent pre-trained capability but also the increased number of instances used in training. In the methodology proposed by Wang et al.\,\citep{tulu}, the fine-tuning is conducted with a fixed template for both input and output to facilitate comparisons across instruction datasets. Notably, this differs slightly from the template of \textsc{SupNatInst}. In our experiments, we employ the \textsc{SupNatInst} template with the OpenSNI model. As seen in \autoref{table:opensni_format}, there is a significant performance difference when using the \textsc{SupNatInst} template compared to the one used in training.
\begin{table}[h]
\centering
\caption{Rouge-L score of OpenSNI-7B on \textsc{SupNatInst} with different input format}
\label{table:opensni_format}
\begin{tabular}{c|c|c}
\toprule
Method \textbackslash Format & SupNatInst & Open-instruct \\
\midrule
baseline & 47.85 & 46.20 \\
null & 49.04 & 48.70 \\
opposite & 49.47 & 48.94 \\
\bottomrule
\end{tabular}
\end{table}

We also use the T0-3B \,\citep{t0} and Alpaca-7B \,\citep{alpaca} checkpoints from \href{https://huggingface.co/bigscience/T0_3B}{T0-3B public checkpoint}, and \href{https://huggingface.co/WeOpenML/Alpaca-7B-v1}{Reproduced Alpaca-7B public checkpoint} \,\citep{pandalm} in our experiments, repectively. We set maximum length of inputs and generation length to 1,024 and 128, respectively.

\section{Metric Details}
\label{metric_details}
\paragraph{Rouge-L} 
Rouge-L (Recall-Oriented Understudy for Gisting Evaluation with Longest Common Subsequence) is one of the metrics under the ROUGE framework \,\citep{rouge}, used predominantly for evaluating the quality of summaries by comparing them to reference summaries. Rouge-L specifically utilizes the Longest Common Subsequence (LCS) approach. LCS captures the longest co-occurring in-sequence n-grams, words, or bytes between the system-generated summary and a set of reference summaries. The advantage of Rouge-L is that it does not require predefined n-gram length like other ROUGE metrics (e.g., ROUGE-N), making it more adaptive to varying lengths of summaries and capturing fluent sequences more effectively. Given a candidate summary \( C \) and a reference summary \( R \), the precision \( P \) and recall \( R \) for Rouge-L are calculated as:
\[ P_{LCS} = \frac{LCS(C, R)}{|C|} \]
\[ R_{LCS} = \frac{LCS(C, R)}{|R|} \]

\noindent where \( |C| \) and \( |R| \) are the lengths of the candidate and reference summaries, respectively, and \( LCS(C, R) \) denotes the length of the longest common subsequence between the two summaries. The F1 score for Rouge-L is then computed as the harmonic mean of the precision and recall:
\[ F1_{LCS} = \frac{2 \times P_{LCS} \times R_{LCS}}{P_{LCS} + R_{LCS}} \]
Due to its measurement efficiency, we choose Rouge-L as our main metric for zero-shot instruction following ability. 


We opt for Rouge-L as our primary metric for zero-shot instruction following capability. Other studies \citep{mmlu,flip} have utilized methods such as ranking options by likelihood for possible labels to assess instruction following abilities. However, these methods not only fail to reflect the efficacy of our ID but, when considering a more practical instruction following scenarioâ€”specifically, open-ended text generation corresponding to the provided instruction and contextâ€”Rouge-L emerges as the more appropriate metric for representing the overall task performance.

While there exist frameworks, such as Alpaca Farm\,\citep{alpacafarm} and Chatbot Arena\,\citep{chatbot-arena}, that evaluate the generation capabilities of instruction-tuned models, they predominantly focus on assessing dialogue formats. As a result, they are not ideally suited for evaluating IDs that aim to improve zero-shot task generalization.
\paragraph{Label Adherence \& Label Coherence}
\input{materials/tables/table_lc_examples}
\begin{figure}
\centering
\vspace{-10pt}
\includegraphics[width=\textwidth]{materials/figures/appendix_fig_la_lc_example.pdf}
\vspace{-10pt}
\caption{The example of Adhernece and Coherence in \texttt{task290\_tellmewhy\_question\_answerability}. In classification tasks, the definition (i.e., instruction) contains not only semantic information about the task but also hints on how to respond. If an instruction-tuned model solely pursues adherence and conforms only to the label format (i.e., Only Adherence), it may produce incorrect answers. Conversely, if it tries to align only semantically (i.e., Only Coherence), it deviates from the predetermined format.}
\vspace{-12pt}
\label{fig:appendix_la_lc}
\end{figure}
For an in-depth analysis of ID, we measure LA and LC in addition to EM (Exact Match) across 58 classification tasks. The illustration of Label Adherence and Coherence is in \autoref{fig:appendix_la_lc}. To measure LA, we construct the space of all ground truth outputs for each task's instances and evaluate whether the generated answer resided within this space. Conversely, to comprehensively evaluate the LC of instruction-tuned LMs, we take a scrupulous approach. Rather than solely relying on the default labels provided in the \textsc{SupNatInst} dataset for classification tasks\,(\autoref{tab:sni_classification}), we go a step further. We manually select all classification tasks and deliberately extend their label space. By doing so, we aim to capture a broader range of potential responses the model generates, ensuring a more precise assessment of its semantic coherence.

\autoref{tab:lc_examples} presents an example of an extended label space. For tasks like entailment classification, we expanded the label space by collating responses from our experiments that semantically matched the ground truth labels such as `entailment', `neutral', and `contradiction'. Additionally, we underwent further processing, such as removing special characters like `.', `\textbackslash n', `?', `!', and conducting comparisons without upper case sensitivity, to ultimately create the extended label space used in the LC evaluation. This manual label space enhancement not only increases the quality of our evaluation but also provides deeper insights into how the model interprets and aligns its outputs with the intended semantics. \autoref{fig:appendix_la_lc} shows the example of adherence and coherence for the needs of LA and LC.


\label{appendix_experimental_setup}

\section{Additional Experiments}
\label{sec:additional}
\paragraph{Decode by Sampling}
\begin{table}[h]
\centering
\caption{Comparison between sampling-based decoding and greedy decoding. Top-k and temperature scaling are adopted. Mean and standard deviation of 3 seeds experiments are reported.}
\label{tab:sampling}
\begin{tabular}{l|c|c}
\toprule
Method & Top-k ($k=40$) \& Temp ($\tau=0.7$) & Greedy\\
\midrule \midrule
original instruction & \(43.17 \pm 0.26\) & 45.36\\
null & \(41.61 \pm 0.20\) & 46.35\\
rand words & \(41.60 \pm 0.26\) & 46.46\\
\bottomrule
\end{tabular}
\end{table}

We conduct experiments using greedy decoding. This is necessary because \textsc{SupNatInst} comprises 119 tasks, which encompass not only generation but also classification and question-answering tasks. Although sampling-based decoding aids in increasing diversity, it operates stochastically, which is not beneficial for classification or question-answering. Nevertheless, we examine whether ID has benefits from sampling, and the results are presented in \Autoref{tab:sampling}. From the outset, one can observe a performance degradation across all methods, including the baseline, with ID experiencing a particularly significant decline. As described in \Autoref{sec:qualitive_analysis}, we demonstrate that this outcome stems from the smoothing effect from the characteristics of ID. Because ID reduces the top1 probability by increasing the probabilities for other tokens, sub-optimal tokens can be easily sampled, leading generalization far worse than that of the greedy decoding.
% ID dramatically benefits from the smoothing effect of the sampling distribution, leading to a softer sampling distribution. 


\paragraph{CD ablations}
\begin{figure}
\centering
% \vspace{-10pt}
\includegraphics[width=\textwidth]{materials/figures/appendix_cd_ablation.pdf}
% \vspace{-10pt}
\caption{Performance results after applying CD to \textsc{SupNatInst} in comparison to the expert model's original score. We explore various values for \(\tau\), representing the temperature parameter $\tau$ for the amateur model, within the range [0.5,10.0]. The parameter \(\alpha\), which constrains the model's confidence, is set to 0.1 as in Li et al. \,\citep{contrastive_decoding}. For the ID-amateur approach, which introduces noisy instructions to the `amateur' model, we examine the optimal value for \(\epsilon\) among 0.1, 0.2, and 0.3.}
% \vspace{-12pt}
\label{fig:appendix_cd}
\end{figure}

In \Autoref{sec:CD}, we discuss the application of Contrastive Decoding (CD) to ID for unseen task generalization. The comprehensive results for the hyperparameters that demonstrates the highest performance during our experiments can be found in \autoref{fig:appendix_cd}. As previously mentioned, while CD experiences significant sensitivity concerning model selection, the simultaneous use of ID's opposite instruction with CD (i.e. ID-amatuer) reduces this sensitivity. Even when the expert model is smaller than the amateur model, it displays more robust results, and the degradation is considerably less when compared to the standard CD. This can be attributed to the fact that as the amateur model grows in size, it better understands the meaning of the opposite instruction, thereby producing significant noisy logits.




\paragraph{Ablations on the Number of Random Words for Noisy Instruction}



\begin{table}[h]
\centering
\caption{Performance degradation with increasing number of random words in the noisy instruction for T\textit{k}-Large and T\textit{k}-XL models. This table highlights the trade-offs when introducing randomness in instructions.}
\label{tab:ab_rand_word}
\begin{tabular}{@{}l|ccccccc@{}}
\toprule
                                   & \multicolumn{7}{c}{The number of random words}        \\\midrule
Model                              & 1     & 3     & 5     & 10    & 30    & 50    & 100   \\\midrule
T\textit{k}-Large & 41.77 & 41.74 & 41.73 & 41.54 & 41.40 & 41.36 & 41.35 \\
T\textit{k}-XL    & 46.46 & 46.39 & 46.34 & 46.30 & 46.29 & 46.25 & 46.11 \\ \bottomrule
\end{tabular}%
\end{table}

To understand the influence of the number of random words in the noisy instruction, we conduct ablation experiments varying their count. In \Autoref{tab:ab_rand_word}, performance metrics for T\textit{k}-Large and T\textit{k}-XL models across different random word counts are presented. As the number of random words increases, there is a marginal decline in performance for both models. This suggests a potential saturation point beyond which additional random words might not offer significant noise benefits. The results underscore the importance of adjusting the randomness level in the noisy instruction to achieve optimal performance.


\paragraph{Ablations on the Ration of Truc-Shuf}



\begin{table}[h]
\centering
\caption{Variation in performance with different truncation ratios in the Truc-Shuf approach for Tk-Large and Tk-XL models. The table showcases the resilience and adaptability of the models to varying degrees of truncation in the instructions}
\label{tab:ab_trunc-shuff}
\begin{tabular}{@{}l|ccccccccc@{}}
\toprule
         & \multicolumn{9}{c}{Trucation Ratio}                                   \\\midrule
Model    & 0.1   & 0.2   & 0.3   & 0.4   & 0.5   & 0.6   & 0.7   & 0.8   & 0.9   \\\midrule
T\textit{k}-Large & 41.68 & 41.67 & 41.57 & 41.87 & 41.60 & 41.70 & 41.61 & 41.73 & 41.66 \\
T\textit{k}-XL    & 46.37 & 46.31 & 46.39 & 46.60 & 46.21 & 46.45 & 46.30 & 46.26 & 46.67 \\\bottomrule
\end{tabular}%
\end{table}

To ascertain the impact of truncation on the model's performance, we perform ablation studies varying the truncation ratio. As illustrated in \autoref{tab:ab_trunc-shuff}, we report the performance for T\textit{k}-Large and T\textit{k}-XL models across different truncation ratios. The table indicates that the models exhibit varying sensitivity to the truncation level. Notably, neither extreme truncation nor minimal truncation consistently maximizes the performance, suggesting an intermediate truncation ratio might be optimal. The results underline the significance of adjusting the truncation ratio to optimize the balance between the retention of task-relevant information and the introduction of noise.


\begin{figure}
\centering
% \vspace{-10pt}
\includegraphics[width=\textwidth]{materials/figures/appendix_fig_null_example.pdf}
% \vspace{-10pt}
\caption{An example on logits correction by ID with T\textit{k}-XL model.}
% \vspace{-12pt}
\label{fig:null_example}
\end{figure}

\paragraph{Example on Logits Correction by ID}

The baseline response guided by the original instruction displays a slight ambiguity, predicting tokens like `True' and `Correct' at relatively high levels, but also showing minor confusion with `Answer', `Fal', and `Yes'. However, for the given task, the correct response should be `Correct'. When using the `null' with ID, the prediction scores across these tokens generally increase. By contrasting these outcomes, the model is further reinforced to adhere to the `Correct' response, underlining the strength of ID in guiding models towards the appropriate response.



\input{materials/tables/table_sni_classification_tasks}
