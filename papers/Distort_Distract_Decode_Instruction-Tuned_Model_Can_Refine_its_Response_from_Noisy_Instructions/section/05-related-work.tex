% \clearpage
\vspace{-8pt}
\section{Related Work}

\paragraph{Instruction-tuned Language Models}
% \vspace{-5pt}
Instruction-tuning is a method to fine-tune pre-trained LMs to better follow natural language instructions\,\citep{flan, t0}. This fine-tuning process has demonstrated consistent enhancements in the model's ability to generalize to unseen tasks, particularly in zero-shot scenarios\,\citep{alpaca, wizardlm, sni_dataset, instruction_tuning_with_gpt4, instruct_gpt, flan_t5}. Previous studies indicate that expanding the breadth, volume, and ingenuity of tasks for training improves instruction-tuning even further\,\citep{sni_dataset, self_instruct, how_far_can_camels_go}. While some efforts also use human feedback\,\citep{instruct_gpt, fingrained_human_feedback, finetuning_lms_human_preferences, pro_human_feedback}, this paper focuses on the instruction-tuned models that are trained on task datasets in a supervised manner.

% \vspace{-5pt}
\paragraph{Impact of Instructions on Generated Responses}
Understanding how generative LMs interpret instructions remains an active area of discussion. It has been suggested only the essential tokens directly related to the expected response influence the performance\,\citep{did_you_read_instructions}. However, instruction-tuned LMs are so heavily conditioned on pre-trained knowledge that it is difficult to override such conditioning through the prompted instructions\,\citep{instruction_verbalizer_manipulation, llm_counterfactual_tasks}. 
Recent research indicates that the success of instruction-tuning is contingent upon the familiarity of instructions LMs encounter during their training phase\,\citep{instructeval, formatting_consistency}. More specifically, LMs trained with certain instructions can exhibit improved generalization on \textit{unseen} tasks, even when presented with misleading instructions during evaluation\,\citep{evaluating_robustness_instruction_tuned, do_really_follows_instructions}. In zero-shot scenarios, this sensitivity to instruction variations becomes especially evident\,\citep{evaluating_robustness_instruction_tuned, robustness_task_instructions}. In this work, we suggest this sensitivity can be leveraged by contrasting responses generated from noisy instructions.

% \vspace{-5pt}
\paragraph{Contrast in Text Generation} The concept of using contrast to improve text generation in generative LMs has been studied in various ways\,\citep{contrastive_decoding, contrastive_input_decoding, dexperts, coherence_boosting}. For example, Contrastive Decoding\,\citep{contrastive_decoding} aims to maximize the output probability by contrasting a less proficient model with an expert-level model.  Meanwhile, Coherence Boosting enhances long-range contextual understanding by giving more weight to distant words\,\citep{coherence_boosting}. This contrastive approach has demonstrated its effectiveness in diverse areas through its variants, such as text detoxification\,\citep{dexperts}, resolving knowledge conflicts\,\citep{context_aware_decoding}, mitigating bias in input text\,\citep{contrastive_input_decoding} and boosting response truthfulness\,\citep{dola_contrastive_decoding}. Our study extends this line of work but places emphasis on the role of instructions in the input text. Also, unlike previous studies, we present findings that it is possible to utilize inputs that cause severe performance degradation, experiments show that contrasting predictions based on noisy instructions can significantly improve the generalization of instruction-tuned LMs on unseen tasks.