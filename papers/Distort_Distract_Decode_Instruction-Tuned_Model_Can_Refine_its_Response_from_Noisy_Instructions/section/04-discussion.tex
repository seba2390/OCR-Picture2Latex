
\vspace{-10pt}
\section{Discussion}
\label{sec4:discuss}

\vspace{-5pt}
\paragraph{Qualitative Analysis on ID with Opposite}
\label{sec:qualitive_analysis}

As \autoref{fig:discussion_fig}\,\textcolor{red}{(a)} demonstrates, the baseline shows strong label adherence but often settles on a single label. The introduction of the `Opposite' technique diversifies these responses, as evidenced by tasks previously biased toward `True' now yielding more balanced outcomes. Specifically, there is a marked increase in the prediction probabilities for tokens that are not the top-ranked predictions guided by the original instruction. This not only expands the instruction-guided output space but also emphasizes the increased likelihood for alternative tokens. This observation is evident when the data points in the figure gravitate closer to the origin. Intriguingly, combining the original instruction with the noisy instruction prompt does not lead to improved performance. Although there is a shift away from distinct `True' or `False' predictions—indicating a smoothing effect—this shift does not reverse the predictions. We conjecture that including the original instruction in the contrastive prediction may inadvertently anchor the model's responses, altering their magnitudes but not their directions.

%As \autoref{fig:discussion_fig}\,\textcolor{red}{(a)} demonstrates, the baseline shows strong Label Adherence but often settles on a single label. The introduction of the `Opposite' technique diversifies these responses, as evidenced by tasks previously biased toward `True' now yielding more balanced outcomes. Specifically, there is a marked elevation in the prediction probabilities for tokens that are not the top-1 prediction guided by the original instruction. This not only broadens the instruction-guided output space but also emphasizes the increased likelihood for alternative tokens. This observation is evident as the data points in the figure gravitate closer to the origin. Intriguingly, combining the original instruction with the noisy instruction prompt does not lead to improved performance. Although there is a shift away from distinct `True' or `False' predictions, signifying smoothing, it does not reverse the predictions. We conjecture that including the original instruction in the contrastive prediction might inadvertently anchor the responses, altering their magnitudes but not their directions.

% This shift suggests a label-smoothing effect\,\citep{label_smoothing}, increasing the likelihood of alternative tokens being selected. 

\begin{figure}[t]
\vspace{-10pt}
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=1.0\linewidth]{materials/figures/logits_scatter.pdf}
        \caption{Distribution shift in classification}
        \label{fig7_1} 
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=1.0\linewidth]{materials/figures/embedding_cluster.pdf}
        \caption{Visualization of input embeddings}
        % \label{fig:tsne}
        \label{fig7_2}
    \end{subfigure}
    \vspace{-5pt}
    \caption{(a) Shift in responses for binary classification tasks using T\textit{k}-XL, comparing baseline, ID with `Opposite', and ID combining `Opposite + Base Inst'.; (b) t-SNE visualization of embeddings for `Keyword Tagging (KT)' and `Word Analogy (WA)', extracted from the T\textit{k}-XXL encoder by concatenating the instruction and input.}
    \label{fig:discussion_fig}
    \vspace{-15pt}
\end{figure}

\vspace{-5pt}
\paragraph{Visualization of Embeddings: Evidence of Anchoring Effect}

\autoref{fig:discussion_fig}\,\textcolor{red}{(b)} provides a t-SNE\,\citep{t-SNE} visualization of input embeddings from category KT and WA, extracted from the T\textit{k}-XXL encoder. This visualization serves as empirical evidence for the impact of various noisy instruction variants. Notably, unique clusters form for each type of instruction embedding, indicating that the encoder interprets these noisy instructions differently, thereby exerting different anchoring effects—beneficial for ID. This phenomenon is clearly reflected in the WA category, consistent with the improvements by our method. In contrast, some embeddings in the KT category overlap, suggesting a limited distinction between the original and noisy instructions. This weakens the anchoring effect and results in a decline in Rouge-L scores for KT. This observation suggests that as the model gets better at understanding noisy instructions, the performance of ID usually improves as well. This is often the case when using higher-performing models.

\vspace{-6pt}
\paragraph{On the Utility of ID over Contrastive Decoding}
\label{sec:CD}
\begin{wrapfigure}{r}{0.4\textwidth}
    \vspace{-20pt}
    \includegraphics[width=1.0\linewidth]{materials/figures/cd_id_amateur.pdf}
    \vspace{-15pt}
    \caption{CD vs. ID-Amateur performances across T\textit{k}-instruct models.} 
    \label{fig:discussion_cd_id}
    \vspace{-20pt}
\end{wrapfigure}

We examine the synergistic effects of integrating ID with the use of amateur models (ID-amateur) for $\Tilde{z}$ across various T\textit{k}-Instruct model families in \autoref{fig:discussion_cd_id}. More precisely, we feed a smaller amateur model with the noisy `opposite' instruction in the ID-amateur method. This approach is compared with the standard Contrastive Decoding (CD, Li et al. \,\citep{contrastive_decoding}) with the original instruction for analysis, where \(\tau\) is temperature for amateur. Using T\textit{k}-small with T\textit{k}-XL in CD modestly surpasses ID-amateur due to smaller models' limited grasp of noisy instructions. As the `amateur' model size grows, CD's performance diminishes, highlighting its size sensitivity. Conversely, ID-amateur maintains consistent adaptability across diverse model sizes. To sum up, ID-amateur method maintains performance across model scales while mitigating issues inherent in standard contrastive decoding.
