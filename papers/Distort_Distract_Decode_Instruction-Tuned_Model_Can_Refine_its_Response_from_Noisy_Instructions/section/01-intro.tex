\section{Introduction}
\label{sec1:introduction}
\vspace{-5pt}

Language Models (LMs) have opened up a new era in Natural Language Processing (NLP) by leveraging extensive datasets and billions of parameters\,\citep{llm_survey, gpt4_report, scaling_law}. These LMs excel at In-Context Learning (ICL), generating responses based on a few demonstrations without needing further parameter adjustments\,\citep{emergent_abilities_llms, gpt3, icl_survey}. The rise of instruction-tuning has further enhanced this capability, optimizing LMs to align their outputs closely with human-specified instructions\,\citep{flan, t0, gpt3, lms_are_unsupervised_multitask_learners}. This approach has demonstrated a significant improvement in zero-shot scenarios, underscoring its importance for tackling diverse tasks.

However, instruction-tuned models often struggle with unfamiliar tasks due to limitations in their training datasets, whether the datasets are human-annotated\,\citep{ni_dataset, sni_dataset} or model-generated\,\citep{self_instruct, unnatural_ni_dataset}. Refining these datasets is essential but requires substantial effort and computational resources, highlighting the need for more efficient approaches\,\citep{flan_t5, lima}. Moreover, the depth of a model's understanding of and how they respond to instructions remains an area of active research. While recent studies have provided some insights\,\citep{do_really_follows_instructions, did_you_read_instructions}, many questions remain unanswered. Techniques such as prompt-engineering\,\citep{prompt_analysis} and utilizing diversified outputs\,\citep{self_consistency} aim to increase the quality of outputs. However, the effectiveness of these techniques often depends on the fortuitous alignment of prompts or initial conditions, making them labor-intensive since the tuning process must be tailored.

In pursuit of refining the behavior of LMs, some researchers have begun to explore the \textit{anchoring effect}\,\citep{kahneman1982judgment}—a well-known cognitive bias where initial information exerts disproportionate influence on subsequent judgments. Intriguingly, this cognitive principle has been demonstrated to extend to LMs. For example, through effective prompting, the outputs generated by LMs can be steered towards a specific intent\,\citep{jones2022capturing}. Similarly, emphasizing the first few sentences of a long context enhances the model's overall comprehension of the content\,\citep{coherence_boosting}. Given these observations on LMs—parallels that mirror human tendencies—and the influential role of initial prompts, we hypothesize that the strategic application of the anchoring effect could substantially improve LMs' fidelity to instructions.

In this work, we propose \textit{Instructive Decoding}\,(ID)\,(\autoref{fig:main}), a novel method that enhances the attention of instruction-tuned LMs towards provided instructions during the generation phase without any parameter updates. The core idea of ID is deploying \textit{noisy} variants of instructions, crafted to induce a clear \textit{anchoring effect} within the LMs, to adjust the output anchored by the original instruction. More precisely, this effect aims to steer the models toward particular, potentially sub-optimal predictions. Our range of variants spans from simple strategies such as instruction truncation and more aggressive alterations, the most extreme of which is the \textit{opposite} instruction. By intentionally introducing such deviations, ID capitalizes on the resulting disparities. Within a contrastive framework, next-token prediction logits that are influenced by the noisy instructions are systematically compared to those derived from the original instruction. This process refines the model's responses to align more closely with the intended instruction.

\begin{figure}[t!]
\centering
\vspace{-10pt}
\includegraphics[width=\textwidth]{materials/figures/fig1_main_fix.pdf}
\vspace{-10pt}
\caption{Overview of Instructive Decoding\,(ID). The example in this figure is from \texttt{task442\_com\_qa\_paraphrase\_question\_generation} in \textsc{SupNatInst}\,\citep{sni_dataset}. The original response not only fails to meet the task requirements (Question Rewriting) but also contains incorrect information\protect\footnotemark. In contrast, ID generates a more relevant response by refining its next-token predictions based on the noisy instruction (here, opposite prompting is used for ID).}
\vspace{-12pt}
\label{fig:main}
\end{figure}
%
\footnotetext{According to the \href{https://population.un.org/wpp/}{2022 U.N. Revision}, the population of USA is approximately 338.3 million as of 2022.}

\begin{wrapfigure}{r}{0.54\textwidth}
\vspace{-15pt}
    \includegraphics[width=1.0\linewidth]{materials/figures/main_result_fig.pdf}
    \vspace{-15pt}
    \caption{Zero-shot Rouge-L comparison on the \textsc{SupNatInst} heldout dataset\,\citep{sni_dataset}. Models not instruction-tuned on \textsc{SupNatInst} are in \textcolor{blue}{blue} dotted boxes, while those instruction-tuned are in \textcolor{green!50!black}{green}.} 
    \label{fig:result_overall}
\vspace{-10pt}
\end{wrapfigure}

Experiments on unseen task generalization with \textsc{SupNatInst}\,\citep{sni_dataset} and \textsc{UnNatInst}\,\citep{unnatural_ni_dataset} held-out datasets show that instruction-tuned models enhanced by ID consistently outperform baseline models across various setups. Intriguingly, T\textit{k}-XL combined with our method outperforms its larger version, T\textit{k}-XXL, with standard inference\,(\autoref{fig:result_overall}). Models not previously trained on the \textsc{SupNatInst} dataset, including Alpaca (7B) and T0 (3B), also show marked enhancements in performance. Additionally, the overall Rouge-L score of the GPT3 (175B) is strikingly competitive, closely mirroring the performance of OpenSNI (7B) when augmented with our method. 
We further observe that ID's generation exhibits increased both adherence to the instruction and an improvement in semantic quality. To provide a comprehensive understanding, we investigate the anchoring effect of noisy instructions. Our findings suggest that as the model's comprehension of the noisy instruction intensifies, the anchoring effect becomes more potent, making ID more effective. Our main contributions are as follows:
%
\begin{itemize}
    \item We introduce \textit{Instructive Decoding}\,(ID), a novel method to enhance the instruction following capabilities in instruction-tuned LMs. By using distorted versions of the original instruction, ID directs the model to bring its attention to the instruction during generation \textbf{(\Autoref{sec2:method})}.
    %
    \item We show that steering the noisy instruction towards more degrading predictions leads to improved decoding performance. Remarkably, the \textit{opposite} variant, which is designed for the most significant deviation from the original instruction yet plausible, consistently shows notable performance gains across various models and tasks (\textbf{\Autoref{sec3:experiment}}).
    %
    \item We provide a comprehensive analysis of the behavior of ID, demonstrating its efficacy from various perspectives. The generated responses via ID also improve in terms of label adherence and coherence, and contribute to mitigate the typical imbalances observed in the standard decoding process. (\textbf{\Autoref{sec4:discuss}})
\end{itemize}
