\vspace{-5pt}
\section{Conclusion}
\vspace{-5pt}
This paper explores the challenges faced by instruction-tuned language models, especially when dealing with unfamiliar instructions, termed as unseen task generalization. Our approach is inspired by the \textit{anchoring effect}, a cognitive bias where initial information significantly influences subsequent decisions. Based on this concept, we introduce \textit{Instructive Decoding} (ID), a method that adjusts next-token predictions by contrasting them with those generated from a manipulated version of the original instruction, termed the `noisy' instruction. Designed to counterbalance inherent model biases and potential input biases, these `noisy' instructions guide the model's outputs towards contextually relevant but deviating paths. Our empirical results across multiple tasks confirm the method's efficacy. Notably, the `opposite' noisy instruction, which offers the highest degree of deviation, emerges as the most effective variant for improving model performance. This highlights the significant role the anchoring effect can play in shaping the model's behavior. The simplicity of the ID approach, which necessitates no additional parameter updates, renders it a compelling option to enhance instruction following of the generated responses. As the field of instruction-tuned models continues to evolve, we expect that methods like ID will become crucial in extending their capabilities.


% \clearpage
% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.
