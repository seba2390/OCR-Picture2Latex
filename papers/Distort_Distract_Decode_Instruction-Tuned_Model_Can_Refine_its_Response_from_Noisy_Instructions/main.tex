\documentclass{article} % For LaTeX2e
\input{header}

\title{Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}
\maketitle

\begin{abstract}
While instruction-tuned language models have demonstrated impressive zero-shot generalization,
% excelling across various tasks when guided by natural language instructions, their performance inherently depends on the breadth, volume, and ingenuity of their training tasks.
% , necessitating computational resources for additional fine-tuning. Additionally, 
these models often struggle to generate accurate responses when faced with instructions that fall outside their training set. This paper presents \textit{Instructive Decoding} (ID), a simple yet effective approach that augments the efficacy of instruction-tuned models. Specifically, ID adjusts the logits for next-token prediction in a contrastive manner, utilizing predictions generated from a manipulated version of the original instruction, referred to as a \textit{noisy instruction}. This noisy instruction aims to elicit responses that could diverge from the intended instruction yet remain plausible. We conduct experiments across a spectrum of such noisy instructions, ranging from those that insert semantic noise via random words to others like `opposite' that elicit the deviated responses. 
% Contrary to the traditional reliance on vast numbers of parameters and a wide array of tasks, 
Our approach achieves considerable performance gains across various instruction-tuned models and tasks without necessitating any additional parameter updates. Notably, utilizing `opposite' as the noisy instruction in ID, which exhibits the maximum divergence from the original instruction, consistently produces the most significant performance gains across multiple models and tasks.
\end{abstract}

\input{section/01-intro}
\input{section/02-method}
\input{section/03-experiment}
\input{section/04-discussion}
\input{section/05-related-work}
\input{section/06-conclusion}


% \clearpage
\input{section/ethics_reproducible}

\begin{ack}
This work was supported by Institute of Information \& communications Technology Planning \& Evaluation (IITP) grant funded by the Korea government (MSIT) [No.2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)] and [No.2022-0-00641, XVoice: Multi-Modal Voice Meta Learning].
\end{ack}

\bibliography{main}
\bibliographystyle{plain}

\input{section/appendix/appendix}
\end{document}