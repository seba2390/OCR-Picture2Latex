
\subsection{Proposed architecture}
\label{proposed architecture}

We propose an image-geolocation framework that integrates the foundational Vision model with feature aggregation. The proposed framework utilizes the truncated DINOv2~\cite{oquab_dinov2_2023} model as the backbone network. Owing to its exceptional image understanding capability, DINOv2 is well-suited for various downstream tasks. Therefore, we pre-trained the DINOv2 model as the primary network for image feature extraction and employed an efficient and lightweight Mixer module to aggregate the obtained image features. The DINO-Mix visual geolocation architecture is illustrated in Fig.~\ref{fig:DINO-Mix_architecture}.



\begin{figure*}[!t]
\renewcommand{\thefigure}{2} 
    \centering
    \includegraphics[width=0.6\linewidth]{Pics/DINO-Mix_architecture.png}
    \vspace{0em}
    \caption{\emph{\textbf{The visual place recognition structure of DINO-Mix.}}}
    \label{fig:DINO-Mix_architecture}
\end{figure*}

We modified the DINOv2 model by removing its layer norms and head modules, which were subsequently used as the backbone network. Furthermore, to maximize the pre-trained parameter benefits of the DINOv2 model for image understanding, we used the output from the last layer of the Vision Transformer (ViT) blocks~\cite{dosovitskiy_image_2021} as the input to the Mixer module. Given that the output of the modified ViT block module is a feature matrix of size C × D (channels × feature vector length), we transform it into s feature maps of size h × w, as expressed by Equation \ref{equation6}. These transformed feature maps serve as inputs for the mix module.

\begin{equation}
\label{equation6}
    \begin{cases}
        D = s \\
        C = hw
    \end{cases}
\end{equation}

where $D$ represents the length of the feature vector output by the backbone network, $C$ denotes the number of channels in the output of the backbone network, $h$ and $w$ are the height and width of the feature map, respectively, and $s$ is the number of feature maps.

\subsection{Foundational vision model: DINOv2}
\label{Large Vision Model:DINOv2}

Foundational vision models (FVMs) are typically constructed using structures such as CNNs or Transformers. These models often have parameters on the order of tens to hundreds of millions, giving them a greater representational capacity than smaller models. In addition, because of the use of larger and more diverse datasets during training, FVMs can learn more features and have better generalization capabilities.

DINOv2~\cite{oquab_dinov2_2023} is capable of extracting powerful image features and performs well across various tasks. Compared with Segment Anything~\cite{kirillov_segment_2023}, DINOv2 has a broader scope of application and areas of use. The architecture of the DINOv2 model is illustrated in Fig.~\ref{fig:dinov2_structure}. First, an input image is passed through a patch-embedded module consisting of a two-dimensional (2D) convolutional layer with a kernel size of 14 × 14 and a stride of 14, followed by a normalization layer. This process uniformly outputs patches of size 14 × 14. These patches are then fed into ViT blocks, which vary in number according to the size of the model. The ViT blocks output a feature matrix of size $C$ (number of channels) × $D$ (dimension of the feature vector), which is then normalized by a layer-norm module before being transformed into a feature vector of size 1 × $n$. Finally, the head module can be flexibly selected based on specific image task requirements.

\begin{figure}[!t]
\renewcommand{\thefigure}{3}    %
   \centering
   \includegraphics[width=0.7\linewidth]{Pics/DINOv2_structure.png}
   \vspace{0em}
   \caption{\textbf{\emph{The structural diagram of the DINOv2 model}}. }
   \label{fig:dinov2_structure}
  \vspace{-1.5em}
\end{figure}

DINOv2 is characterized by several key features: a) it presents a novel approach for training high-performance computer vision models; b) it offers superior performance without the need for fine-tuning; c) it can learn from any image dataset and capture certain features that existing methods struggle with; and d) it leverages knowledge distillation to transfer knowledge from more complex teacher models to smaller student models. Through knowledge distillation, three smaller models were obtained from the ViTg14 model: ViTl14 (large), ViTb14 (base), and ViTs14 (small) (see Tab.~\ref{tab:Four ViT model param}).

\begin{table}[!t]
\renewcommand{\thetable}{1}    %
    \caption{\emph{\textbf{Four ViT model parameters for DINOv2}}}
    \centering
    \begin{tabular}{c p{1.2cm} c p{1cm} c p{1cm} c p{1.1cm} c p{1cm}}
    \hline
    Name & Patch embed & Blocks & Feature dim & Size(MB) \\
    \hline
    ViTs14 & 14 × 14 & 12 & 384 & 86.2 \\
    ViTb14 & 14 × 14 & 24 & 768 & 338.2 \\
    ViTl14 & 14 × 14 & 24 & 1024 & 1189.0 \\
    ViTg14 & 14 × 14 & 40 & 1536 & 4439.5 \\
    \hline
    \end{tabular}
    \label{tab:Four ViT model param}
\end{table}

The primary advantage of DINOv2 is the ability to create a large dataset for model training. This dataset, called LVD-142M, comprises 142 million images and includes ImageNet-22k, ImageNet-1k, Google Landmarks, various fine-grained datasets, and image datasets crawled from the Internet. For model training, an Nvidia A100 40-GB GPU was utilized, with a total of 22k GPU hours dedicated to training the DINOv2-g model.

\subsection{Feature Mixer}
\label{Feature Mixer}

Currently, the most advanced techniques propose shallow aggregation layers that are inserted into very deep pre-trained backbones cropped to the last feature-rich layer. By contrast, Wang et al. proposed TransVPR~\cite{wang_transvpr_2022}, which achieved good results in local feature matching. However, its global representation performance did not surpass that of NetVLAD~\cite{relja_netvlad_2018} or CosPlace~\cite{berton_rethinking_2022}. Recent advancements in isotropic architectures have demonstrated that self-attention is not crucial for ViT. However, Mixer utilizes feature maps extracted from a pre-trained backbone and iteratively merges global relationships into each feature map. This is achieved through an isotropic block stack composed of MLPs, referred to as a feature mixer~\cite{tolstikhin_mlp-mixer_2021}. The effectiveness of Mixer has been demonstrated through several qualitative and quantitative results, demonstrating its high performance and lightweight nature~\cite{ali-bey_mixvpr_2023}; the architecture is illustrated in Fig.~\ref{fig:mix_architecture}.

\begin{figure}[!t]
\renewcommand{\thefigure}{4}
    \centering
    \includegraphics[width=0.9\linewidth]{Pics/mix_architecture.png}
    \vspace{-1em}
    \caption{\emph{\textbf{The architecture of Mix}}}
    \label{fig:mix_architecture}
\end{figure}

Mixer treats the input feature map $F\in R^{(s\times h\times w)}$ as a set of s 2D features, each of size $h\times w$, as expressed by Equation \ref{equation1}:

\begin{equation}
\label{equation1}
    F=\{X^{i} \}, i=\{1,…,s\}
\end{equation}

where $X^i$ corresponds to the $i$th activation map in the feature map F. Secondly, each 2D feature map $X^i$ is expanded into a 1D vector representation, resulting in a flattened feature map $F\in R^{(s\times n)}$, where $n=h\times w$.

The flattened feature maps are then fed into the feature mixer, which is composed of $L$ MLPs with the same structure, as shown in Fig.\ref{fig:mix_architecture}. The feature mixer takes the flattened feature map ensemble as input and successively incorporates spatial global relationships into each ${X^i}\in F$ as per Equation \ref{equation2}:


\begin{equation}
\label{equation2}
    {X^i}\gets{W_2}{\big(}\sigma({W_1}{X^i} ){\big)}+{X^i},i=\{1,…,s\}
\end{equation}

where $W_1$ and $W_2$ are the weights of the two fully-connected layers that make up the MLP, and $\sigma$ is the ReLU nonlinear activation function.

For $F\in R^{(s\times n)}$, the feature mixer, owing to its isotropy architecture, produces an output $"Z"\in R^{(s\times n)}$ with the same shape and feeds it into the second feature mixer block, and so on, until $L$ consecutive blocks have been traversed, as per Equation \ref{equation3}:

\begin{equation}
\label{equation3}
   Z={FM_L}{\Big(}FM_{(L-1)}{\big(}…{FM_1} (F){\big)}{\Big)}
\end{equation}

where $Z$ and the feature map F have the same dimensions. To control the dimensions of the final global descriptor, two fully-connected layers are used to successively transform the channel and row dimensions. First, a depth projection is used to map $Z$ from $R^{(s\times n)}$ to $R^{(d\times n)}$, as given by Equation \ref{equation4}:

\begin{equation}
\label{equation4}
    {Z^{'}}={W_d}{\big(}Transpose(Z){\big)}
\end{equation}

where $W_{d}$ denotes the weight of the fully-connected layer. Subsequently, a row-wise projection is used to map the output $Z^{'}$ from $R^{(d\times n)}$ to $R^{(d\times r)}$, as given by Equation \ref{equation5}:

\begin{equation}
\label{equation5}
   O={W_{r}}{\big(}Transpose({Z^{'}}){\big)}
\end{equation}

where $W_r$ denotes the weight of the fully-connected layer. The final output $O$ has dimensions of $d\times r$, which are flattened, and $L2$ is normalized to form a global feature vector.


