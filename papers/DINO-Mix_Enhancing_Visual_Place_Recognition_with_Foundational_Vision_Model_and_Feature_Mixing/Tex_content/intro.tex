% 
Visual place recognition (VPR), also known as image geo-localization (IG) or visual geo-localization (VG), has been extensively applied in various fields, such as cyberspace mapping, intelligence gathering, image target localization, autonomous driving, and outdoor user localization. Currently, most VPR studies focus on image retrieval in urban scenarios. However, images captured in urban environments may have varying shooting angles, temporal lighting changes, seasonal variations, occlusions, and similar repetitive textures. These conditions can pose significant difficulty for achieving high-precision image retrieval. Therefore, extracting robust and generalizable image feature descriptors for accurate image retrieval is a critical issue.

% 
In previous approaches to VPR, handcrafted SIFT~\cite{lowe_distinctive_2004}, HOG~\cite{dalal_histograms_2005}, SURF~\cite{leonardis_surf_2006}, ORB~\cite{rublee_orb_2011}, and other techniques were used to extract features from images. These features were then aggregated using methods such as bag-of-words (BoW)~\cite{tang_learning_2012}, Fisher Vector (FV)~\cite{jegou_aggregating_2010}, and Vector of Locally Aggregated Descriptors (VLAD)~\cite{jegou_aggregating_2012} to obtain image descriptors for image retrieval, enabling image geo-localization. More recently, deep-learning techniques have emerged as mainstream approaches for extracting image features. Compared with handcrafted features, these methods significantly improve VPR accuracy. Examples include NetVLAD~\cite{relja_netvlad_2018}, which combines convolutional neural networks (CNNs) with VLAD, and other variants that incorporate attention, semantics, context, and multiscale features. Other methods based on the generalized mean (GeM)~\cite{radenovic_fine-tuning_2019}, such as CosPlace~\cite{berton_rethinking_2022}, and those utilizing fully-connected multilayer perceptron (MLP)-based feature aggregation, such as MixVPR~\cite{ali-bey_mixvpr_2023}, have been proposed. However, in practical testing, the accuracy of these methods for image geo-localization has been suboptimal under challenging conditions, such as varying shooting angles, temporal lighting changes, seasonal variations, and occlusions. 

% 
 The rapid development of foundational visual models has enabled the generation of universal visual features from images~\cite{oquab_dinov2_2023}. By training on billions of data points, foundational visual models can extract image features that are more generalizable and robust than those extracted by conventional models. They can effectively handle the challenging conditions encountered in practice. Therefore, incorporating foundational visual models into VPR is a promising approach.

% 
Considering the aforementioned challenges, this study proposes a method based on the DINOv2~\cite{oquab_dinov2_2023} model, called DINO-Mix, which combines foundational visual models with feature aggregation. This architecture possesses exceptional discriminative power. Efficient and robust image features suitable for image geo-localization are extracted by fine-tuning and trimming. Furthermore, it utilizes a feature mixer~\cite{tolstikhin_mlp-mixer_2021} module to aggregate image features, resulting in a global feature descriptor vector. DINO-Mix is experimentally demonstrated to achieve superior test accuracy on multiple benchmarks, surpassing state-of-the-art (SOTA) methods.

% 
The remainder of this paper is organized as follows. In  \ref{sec:related work}, we summarize previous relevant research on image geo-localization. In \ref{sec:methodology}, the DINO-Mix method is introduced. In \ref{sec:experiments}, we provide details on the training set, testing set, training, and evaluation parameters used in our experiments. The proposed method is compared with existing methods in terms of accuracy, and ablation experiments are conducted. In \ref{sec:Qualitative results}, We qualitatively demonstrate the state-of-the-art of DINO-Mix architecture by comparing DINO-Mix with other VPR methods through a typical VPR example and visualizing the corresponding attention map. Finally, our conclusions are presented in \ref{sec:conclusions}.
