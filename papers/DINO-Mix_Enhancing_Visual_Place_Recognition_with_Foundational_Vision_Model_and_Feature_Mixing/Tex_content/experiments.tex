
\subsection{Implementation details}
\label{Implementation details}

\textbf{Datasets:} Our model was trained using the GSV-Cities dataset~\cite{ali-bey_gsv-cities_2022}. The following six datasets were employed for evaluation purposes: Pittsburgh250k~\cite{torii_visual_2015} (contains 8k queries and 83k reference images collected from Google Street View and Pittsburgh30k-test), Pittsburgh30k-test~\cite{torii_visual_2015} (a subset of Pittsburgh250k, with 8k queries and 8k reference images), SF-XL-Val dataset~\cite{berton_rethinking_2022}, Tokyo 24/7~\cite{torii_247_2018}, Nordland~\cite{sunderhauf_are_2013}, and SF-XL-Testv1~\cite{berton_rethinking_2022}. The datasets contain extreme variations in lighting, weather, and seasons. Specific information regarding these datasets is presented in Tab.\ref{tab:dataset parameters}.



\begin{table*}[!t]
\renewcommand{\thetable}{2}    %
    \caption{\emph{\textbf{The parameter table of the training dataset and test dataset}}}
    \centering
    \begin{tabular}{c p{1cm} c p{1cm} c p{1cm} c p{0.7cm} c p{1cm}c p{1cm} c p{1cm} c p{1cm} cp{2cm}}
    \hline
    Dataset & Train/val database queries & Test database queries
 & Dataset size (GB) & Database type & Database image size & Urban & Appearance changes\\
    &&&&&&&Season & Day/Night \\
    \hline
    GSV-cities~\cite{ali-bey_gsv-cities_2022}&524701/0&0/0& 21.7& panorama &480×640&\ding{51}&\ding{51}&\ding{51} \\
    Pittsburgh250k~\cite{torii_visual_2015}& 170112/15432&83952/8280&9.5& panorama &300×400&\ding{51}&\ding{55}&\ding{55} \\
    Pittsburgh30k~\cite{torii_visual_2015}&20000/15024&10000/6816&2.0&panorama &480×640&\ding{51}&\ding{55}&\ding{55} \\
    Tokyo 24/7~\cite{torii_247_2018}&0/0&75984/315&4.2&panorama&480×640&\ding{51}&\ding{55}&\ding{51} \\
    Nordland~\cite{sunderhauf_are_2013}& 0/0&27592/27592&1.3&font-view&360×640&\ding{55}&\ding{51}&\ding{55} \\
    SF-XL-Val~\cite{berton_rethinking_2022}& 0/0&8015/7993&0.7&panorama &512×512&\ding{51}&\ding{55}&\ding{55} \\
    SF-XL-Testv1~\cite{berton_rethinking_2022}& 0/0& 27191/1000&1.3&panorama&512×512&\ding{51}&\ding{55}&\ding{51} \\
    \hline
    \end{tabular}
    \label{tab:dataset parameters}
\end{table*}


\textbf{Architecture:} We employed the PyTorch deep-learning framework to implement DINO-Mix. To enable a fair comparison with other methods in terms of accuracy, we conducted precision tests on various VPR frameworks such as NetVLAD, GeM, ConvAP, CosPlace, and MixVPR, and obtained the testing accuracy for other methods from their corresponding papers.

\textbf{Training:} Owing to the excellent pre-trained weights of the DINO-Mix backbone, most of the training weights were frozen during the training process. However, to make it more suitable for the VPR task, we fine-tuned the end of the backbone and trained the feature aggregation module. We trained DINO-Mix following the standard framework proposed in GSV-Cities~\cite{ali-bey_gsv-cities_2022}, which introduces a high-precision dataset consisting of 67k locations described by 560 k images. The batch size B was flexibly adjusted based on the model parameter size, and each location was trained with four images, resulting in a mini-batch of B × 4 images. Stochastic gradient descent~\cite{ruder_overview_2017} (SGD) with a momentum of 0.9 and a weight decay of 0.001 was employed for optimization. The initial learning rate was set to 0.05 and was divided by three every five epochs. Finally, the model was trained using images resized to 224 × 224 pixels over 50 epochs. Most existing VPR studies employ a triplet loss function based on weak supervision~\cite{hermans_defense_2018} for network training; however, this approach requires significant GPU memory and has a high computational overhead. Thus, we utilized multi-similarity loss~\cite{wang_multi-similarity_2019} as the training loss function. Multi-similarity loss mitigates the issues of large interclass distances and small intraclass distances in metric learning by considering multiple similarities. Instead of relying solely on absolute spatial distance as the sole metric, it uses the overall distance distribution of other sample pairs within a batch size to weigh the loss. This computational approach effectively promotes model convergence in the early stages, as expressed by Equation \ref{equation7}:


\begin{equation}
\label{equation7}
\begin{split}
    L_{MS} = \frac{1}{m}\sum_{i=1}^{m}  \bigg\{\frac{1}{\alpha}  { \log \big[1 + \sum_{k  \in P_{i} } e^{-\alpha (S_{ik} - \lambda)}}\big]  \\
    + \frac{1}{\beta }  { \log \big[1+ \sum_{k \in N_{i}}e^{\beta (S_{ik} - \lambda)} \big]} \bigg\}.
\end{split} 
\end{equation}

where $P_{i}$ represents the set of positive sample pairs for each instance in a batch size,  $N_{i}$ denotes the set of negative sample pairs for each instance in the same batch size, $S_{ij}$ and $S_{ik}$ represent the similarities between the two images, and $\alpha$, $\beta$, and $\lambda$ are hyperparameters.

\textbf{Evaluation:} In this work, we employed top-k accuracy~\cite{huang_survey_2023} as a metric to evaluate the precision of the VPR methods. Top-k accuracy is a commonly used evaluation method in the VPR domain, where it is considered successful if at least one of the top-k localization results for a query image has a geographical distance of less than a threshold s from the true location. In our experiments, we set s to 25 $m$ to align with existing methods.

\subsection{Comparison to the State-Of-The-Art}
\label{Comparison to the state-of-the-art}

Based on the conclusions drawn from the ablation studies in this work, we adopt the ViTb14 pre-trained model, which exhibits the best performance among the four models of DINOv2 as the backbone network for DINO-Mix in the VPR task, and modify the DINOv2 model by removing its Layer Norm and Head modules. We utilize Mix as the feature aggregation module to construct the model. During training, we update the parameters of the last three blocks of the backbone network and the entire Mix feature aggregation module. The number of Feature Mixer blocks in the Mix feature aggregation module is set to 2, and the dimensionality of the image features output by the model is 4096. By utilizing these optimal parameter settings, we conduct tests on six test sets for DINO-Mix and compare it with existing methods, as shown in Tab.\ref{tab:Test Results of Different Methods}. In addition, this paper presents Fig.\ref{fig:teaser} to more visually demonstrate the accuracy difference between DINO-Mix and other major VPR methods.

We adopted the ViTb14 pre-trained model, which exhibited the best performance among the four models of DINOv2, as the backbone network for DINO-Mix in the VPR task and modified the DINOv2 model by removing its layer norm and head modules. We used Mixer as a feature aggregation module to construct the model. During training, we updated the parameters of the last three blocks of the backbone network and the entire mix feature aggregation module. The number of feature mixer blocks in the mix feature aggregation module was set to two, and the dimensionality of the image features output by the model was 4096. By utilizing these optimal parameter settings, we conducted tests on six test sets for DINO-Mix and compared them with the existing methods, as shown in Tab.\ref{tab:Test Results of Different Methods}. In addition, Fig.\ref{fig:teaser} illustrates the difference in accuracy between DINO-Mix and other major VPR methods.


\begin{table*}[h]
\renewcommand{\thetable}{3}
    \caption{\emph{\textbf{Table of Test Results of Different Methods on Datasets with Changes in Viewpoint, Illumination, Season.} DINO-Mix(ViTb14) is ours, Bolded numbers are optimal results, and underlined numbers are sub-optimal results}}
    \centering
    \begin{tabular}{c p{1.6cm} c p{0.9cm} c p{0.8cm}  c p{0.9cm}  c p{0.8cm}  c p{0.8cm} c p{0.8cm}  c p{0.8cm}  c p{0.8cm}}
    \hline
    Method & Training data & Vector dim & & & & Test dataset\\
    
    &&& Pitts250k & Pitts30k & SF-XLval & Tokyo24/7 & Nordland & SF-XLTestv1 \\
    \hline
    MAX~\cite{relja_netvlad_2018}   & GSV-cities& 1024& 46.45& 55.87 &43.68&3.17&9.30&8.10 \\
    AVG~\cite{relja_netvlad_2018} & GSV-cities& 1024 &51.85& 64.20 &45.43&8.25&14.99&13.90 \\
    SPOC~\cite{yandex_aggregating_2015}  & GSV-cities & 256 & 60.59 & 68.37 &56.11&19.68&10.18&20.50 \\
    MAC~\cite{razavian_visual_2016}  & GSV-cities  & 256 & 61.75& 69.42 &56.47&18.73&13.49&22.00 \\
    RMAC~\cite{tolias_particular_2016} & GSV-cities  & 256 & 71.3  & 75.94&63.74&32.70&13.30&29.40 \\
    RRM~\cite{kordopatis-zilos_leveraging_2021}   & GSV-cities  & 256 &88.14& 87.49&81.60&57.46&46.00&53.40 \\
    GeM~\cite{radenovic_fine-tuning_2019}  & GSV-cities  & 256  &76.01 & 79.61&68.79&34.60&23.8&33.70 \\
    NetVLAD~\cite{relja_netvlad_2018}& Pitts-30k  & 16384 & 86.93& 86.36 &65.34&53.97&7.86&42.50 \\
    NetVLAD~\cite{relja_netvlad_2018} & GSV-cities& 16384& 89.71& 88.04 &80.38&70.79&36.25&58.90 \\
    CRN~\cite{kim_learned_2017}  & GSV-cities &  16384& 90.60& 89.03&81.83&70.16&38.58&64.40 \\
    MultiRes-NetVLAD~\cite{khaliq_multires-netvlad_2022} &Pitts-30k&32768& 86.70& 86.80 &--&69.80&--&-- \\
    SARE~\cite{liu_stochastic_2019} & Pitts-30k& 4096 & 88.00&87.20&--&74.80&--&45.5 \\
    SERS~\cite{ge_self-supervising_2020} & Pitts-30k& 4096 &90.40 & 89.10 &--&80.30&16.00&50.30 \\
    CosPlace~\cite{berton_rethinking_2022}  & GSV-cities& 4096 & 89.89 & 88.54&84.01&63.17&47.62&54.30 \\
    ConvAP~\cite{ali-bey_gsv-cities_2022}  & GSV-cities& 4096 & 91.52& 89.67 &76.95&72.06&63.93&59.20 \\
    MixVPR~\cite{ali-bey_mixvpr_2023}(Resnet18)&GSV-cities& 4096& 91.75& 89.57&80.68&75.24&64.75&64.90 \\
    MixVPR~\cite{ali-bey_mixvpr_2023}(Resnet50)&GSV-cities& 4096& \underline{94.13} & \underline{91.52}&\underline{85.40}&\underline{85.40}&\underline{76.12}&\underline{75.70} \\
\textbf{DINO-Mix(ViTb14)(Ours)}&GSV-cities& 4096&\textbf{94.58} &\textbf{92.03}&\textbf{89.25}&\textbf{91.75}&\textbf{80.18}&\textbf{82.00} \\
    \hline
    \end{tabular}
    \label{tab:Test Results of Different Methods}
\end{table*}


As listed in Tab.\ref{tab:Test Results of Different Methods}, the test accuracy of the DINO-Mix model proposed in this paper has comprehensively surpassed that of the SOTA method, with further improvement in the Pittsburgh250k, Pittsburgh30k, and SF-XL-Val test sets focusing on changes in viewpoints, and especially in the Tokyo24/7, Nordland, and SF-XL-Testv1 test sets with changes in complex appearance environments.


\subsection{ablation studies}
\label{ablation studies}
    \subsubsection{Hyperparameters}

In DINO-Mix, the number of layers $L$ in the feature mixer is also a critical factor for image retrieval accuracy. To determine the optimal number of Mixer layers, we conducted tests on the Pitts30k-test Pitts250k-test, Sf-xl-val, Tokyo24/7, Nordland, and Sf-xl-testv1 datasets with different numbers of mix layers $L$ (1, 2, 3, 4, 5, 6, 7) for DINO-Mix using ViTb14 as the backbone network. The TOP-1 accuracy is depicted in Fig.\ref{fig:mix_layer_ablatio}. A careful examination of the figure reveals that without any Mix layers, DINO-Mix exhibits a lackluster test accuracy across all six datasets. However, upon incorporating one Mix layer, there is a remarkable enhancement in test accuracy. This observation highlights the pivotal role played by the feature aggregation module in elevating the precision of DINO-Mix. As the number of Mix layers further increases up to two, there is a marginal improvement in test accuracy, culminating in a peak. Nevertheless, as the number of Mix layers continues to escalate, DINO-Mix's test accuracy on the six datasets displays a slow decline and fluctuations, accompanied by a linear increase in parameters. Based on the above analysis, this study adopts a two-layer Mix scheme as the feature aggregator in DINO-Mix.
    

    \begin{figure}[!t]
    \renewcommand{\thefigure}{5} 
        \centering
        \includegraphics[width=1.0\linewidth]{Pics/mix_layer_ablation.png}
        \vspace{-1em}
        \caption{\emph{\textbf{Ablation on the number of Feature-Mixer blocks}}}
        \label{fig:mix_layer_ablatio}
    \end{figure}

    
\subsubsection{Descriptor dimensionality}

We conducted an ablation study on the dimensionality of image feature vectors extracted using the DINO-Mix. The experiment employed ViTb14, which exhibited the best performance as the backbone network, with two layers in the Mixer module. The test was used as the Pitts30k-test, Pitts250k-test, Sf-xl-val, Tokyo24/7, Nordland, and Sf-xl-testv1 datasets, and the image feature vector dimensionality was varied by changing the number of channels in the output vector of the Mixer module. The tested dimensions of the image feature vectors were 128, 256, 512, 1024, 2048, 4096, and 8192. As depicted in Fig.\ref{fig:dim_ablation}, an increase in the dimensionality of image feature vectors is observed to have a positive impact on the overall Top-1 test accuracy of DINO-Mix across various datasets. This trend is particularly pronounced in Sf-xl-val, Tokyo247, Nordland, and Sf-xl-testv1 datasets, where there is a rapid rise in accuracy. Ultimately, the highest level of accuracy is achieved at a dimensionality of 4096. This phenomenon suggests that utilizing image feature vectors with too low dimensionality may result in reduced robustness to variations such as changes in illumination and seasonal shifts in VPR tasks. Consequently, this study adopts a final image feature dimensionality of 4096.


    \begin{figure}[!t]
    \renewcommand{\thefigure}{6} 
        \centering
        \includegraphics[width=1.0\linewidth]{Pics/dim_ablation.png}
        \vspace{-1em}
        \caption{\emph{\textbf{performance on potts30k-test with different dimensionality configurations.}}}
        \label{fig:dim_ablation}
    \end{figure}

   \subsubsection{Backbone architecture} 

DINOv2 encompasses four ViT models, with ViTg14 (giant) being the largest. Through model knowledge distillation, three smaller models were obtained from the distillation process, including ViTl14 (large), ViTb14 (base), and ViTs14 (small), as displayed in Tab.\ref{tab:Four ViT model param}. To evaluate the performance of these four models in the DINO-Mix framework, we conducted training on GSV-Cities as the training set and tested Pitts30k-test using ViTg14-Mix, ViTl14-Mix, ViTb14-Mix, and ViTs14-Mix. The feature mixer was fixed at two layers and the dimensionality of the image feature vectors was set to 4096. In addition, we trained and tested the DINO-Mix models with four different backbone networks under six scenarios: updating the weights of the last one, two, three, six, and nine blocks, and not updating the weights of the backbone network (none). The results are shown in Fig.\ref{fig:backbone_ablation}.

From the perspective of the four differently sized backbone networks, ViTb14-Mix exhibited higher accuracy than the other three models, with a maximum Top-1 accuracy of $92.03\%$. In contrast, ViTg14-Mix exhibited the worst overall performance. This suggests that ViTg14’s large parameter count extracts deeper features from images, which adversely affects subsequent feature aggregation in the feature mixer.

Models without parameter updates for the backbone network demonstrated poorer performance. As the number of updated blocks increased, both ViTb14-Mix and ViTl14-Mix showed a gradual improvement in test accuracy, reaching their highest values after updating the parameters of the last three blocks, and stabilizing thereafter. In contrast, ViTs14-Mix achieved the highest test accuracy and stability after updating the parameters of the last two blocks. However, for ViTg14-Mix, the block parameter updates did not significantly enhance accuracy. Starting from the last three blocks, the ViTg14-Mix test accuracy showed a downward trend. This indicates that excessively deep block parameter updates may extensively alter the original pre-trained parameters.

In summary, updating the parameters of the last three blocks of the backbone network yielded optimal results. Considering the parameter counts of the four DINO-Mix models shown in Fig.\ref{fig:backbone_params}, we selected ViTb14-Mix, which has a moderate parameter count and superior test accuracy, as the final model for DINO-Mix.


    
    
    \begin{figure}[!t]
    \renewcommand{\thefigure}{7} 
        \centering
        \includegraphics[width=1.0\linewidth]{Pics/backbone_ablation.png}
        \vspace{-1em}
        \caption{\emph{\textbf{Test results of different DINO-Mix models with different weights for updating the number of layers.}}}
        \label{fig:backbone_ablation}
    \end{figure}

     \begin{figure}[!t]
    \renewcommand{\thefigure}{8} 
        \centering
        \includegraphics[width=1.0\linewidth]{Pics/backbone_params.png}
        \vspace{-1em}
        \caption{\emph{\textbf{Parametric quantities of the four DINO-Mix models.}}}
        \label{fig:backbone_params}
    \end{figure}



    


