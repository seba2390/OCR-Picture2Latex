\section{Smoothness and Consistency}

In order for a trajectory optimization program to converge, all of the internal function calls must be smooth and consistent:

\begin{itemize}
	\item {\bf Smooth - } The output of the function is continuous, and at least twice differentiable at all points. Ideally the derivatives should all remain small - sharp (continuous) corners will still cause problems.
	\item {\bf Consistent - } The output of the function is repeatable and smooth across multiple calls; the computer executes the same exact lines of code on every call (no \texttt{if} statements).
\end{itemize}

These two requirements sound simple, but in practice they are remarkably difficult to deal with. In this section I will attempt to cover several common places that errors are made. Note that these requirements apply to both the constraints and objective function.

\subsection{Dynamics}

Walking robots are hybrid systems - they have both continuous and discrete aspects to their dynamics. These discrete dynamics must be carefully handled, either by using through-contact or multi-phase transcription methods, as discussed in Section \ref{sec:HybridSystems}.

Another common source of error is in the simulation of the dynamics function for multiple shooting methods. In shooting methods, it is important to use an explicit, fixed-step integration algorithm, rather than an adaptive method like \texttt{ode45}.

\par Sometimes it might be tempting to put some sort of noise into the dynamics. This guarantees that the function will be inconsistent, and will almost certainly cause the optimization program to fail. In general, there is no good reason to put a call to a random number generator inside of a trajectory optimization program.

\subsection{Objective Function}

There are many sources of discontinuity inside of an objective function, typically due to seeming innocuous functions. A few examples are taking the maximum or minimum of a list of numbers, the absolute value function, clamp function\footnote{Also called saturation}, and the ramp function.

\par The best way to deal with these discontinuous functions is by using a constraint to enforce the discontinuity. This works because the non-linear program solver has special tools build-in for dealing with constraints. Equation \ref{eq:AbsVal} shows the correct way to handle the function $|x|$. Other examples are provided in \cite{JohnT.Betts2001}.

\begin{equation} \label{eq:AbsVal}
|x| = x_1 + x_2 \quad \text{subject to} \quad

  \begin{cases}
	x=x_1-x_2 \\
	x_1>=0 \\ 
	x_2>=0	
  \end{cases}
\end{equation}

\par Another approach to dealing with discontinuous functions is to use `smooth' approximations, as shown below:

\begin{align*}
|x| \quad \approx \quad & x \tanh \left(x / \alpha \right) \\
\max(\mathbf{x}) \quad \approx \quad &  \alpha \log\left(\sum \exp \frac{\mathbf{x}}{\alpha} \right)
\end{align*}

\par This method works, but it will typically be less accurate and take longer to converge than the constraint method. Even though the smoothed functions are continuous, there is a tradeoff between accuracy and convergence. If the smoothing is large, then the optimization program will converge, but with an inaccurate solution. If the smoothing is small, the program will converge slowly, or perhaps not at all. One solution is to iteratively reduce the smoothing, starting with heavy smoothing to get a feasible solution, and then reducing the smoothing to zero in on the answer. 

\par There are two approaches to making smooth approximations of functions. The first is to use exponential functions that asymptotically approach the true function far from the discontinuity. The second is to use a piecewise version of the function, where the region near the discontinuity is replaced with a polynomial approximation. 


