Averaging classifiers is an effective method to reduce model variance by approximating the expected classifier. Ensemble methods~\citep{breiman2001random,caruana2004ensemble,hansen1990neural,krogh1995neural,zhou2002ensembling} leverage this fact to obtain improved generalization performance~\cite{koren2009bellkor}. 
For example, the Random Forests~\cite{breiman2001random}  algorithm elevates the modest CART~\cite{breiman1984classification} algorithm to one of the most competitive classifiers within machine learning. 
Here, slightly modified CART trees are bagged~\cite{breiman2001random}, where each tree is trained on a different subset of the data (drawn uniformly with replacement) and splits are randomized. The additional randomization of the decision trees leads to high variance classifiers. This is advantageous as the ensemble size can be very large (in practice often exceeding 10,000 CART trees) due to the extremely efficient ID3 algorithm~\cite{quinlan1986induction}. The large number of classifiers ensures that even in the presence of high variance the ensemble average approaches the expected model (which would have zero model variance).  

In the wake of deep learning, ensembling is just as important as it has ever been. Nowadays most high profile competitions (e.g. Imagenet~\cite{deng2009imagenet} or Kaggle\footnote{\url{www.kaggle.com}}) are won by ensembles of deep learning architectures. As neural networks are often initialized with random weights, there appears to be a sufficient amount of natural variation that allows all networks to be trained on the entire data set. 

Training deep networks is computationally expensive and can last for days or even weeks even on high performance hardware with GPU acceleration. Training ensembles of them increases the cost linearly and quickly becomes prohibitive for most researchers without access to industrial scale computational resources. Although the training of deep net ensembles can be trivially parallelized, few have access to sufficient GPU servers that can be deployed in parallel for long durations. As a result, ensembles of deep nets are typically small---averaging only a hand-full of classifiers. Consequently, the ensemble average still has high variance and does not approach the expected model. 

% The ensemble method is very effective for neural networks. In most cases, neural network ensembles yield significantly better generalization performance than a single network.
% The recent winners of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) \citep{imagenet} have all used multiple networks for their final predictions \citep{szegedy2015going,smallfilter,resnet}.
% Researchers have found that independently trained neural networks tend to make different mistakes on test data, and thus averaging the softmax scores or using a majority voting scheme can greatly reduce the chance of a wrong prediction. The diversity amongst models is a product of random initialization and training with stochastic gradient descent (SGD), coupled with the high degree of nonlinearity in neural networks \gp{Should this mention non-convexity?}. Due to its simpleness and effectiveness, the ensemble method is widely used in deep learning applications.

% Although neural networks ensembles have been widely studied and utilized \citep{hansen1990neural,krogh1995neural}, most existing works focuses on improving the generalization performance, with little concern about the cost of training ensembles. This may be explained by the fact that, until recently, the sizes of both datasets and models were relatively small, making it affordable to train tens or even hundreds of networks for model averaging. However, the situation is rapidly changing. Many datasets now contain millions of samples, and researchers must train very deep networks to achieve good performance on these large scale datasets. Even with the modern GPUs, a deep network may take days or weeks to train. Under limited resources, it becomes important to address the large cost of training neural network ensembles. 

One useful feature of SGD is that SGD does not get caught in local minima that it happens upon during the training process.
In fact, SGD can even escape the attraction of a local minimum.
Somehow cite Bengio.
Therefore, rather than preventing a model from visiting local minima, we allow the model to visit many local minima over the course of optimization, knowing that we can escape it again.
We take a model snapshot when we converge to a local minimum, and then perturb the model to let it escape that local minimum. Therefore, we end up with several models that have converged to various minima, which give us enough variety to converge to several different models.

% There are several methods which can lower this cost to a certain extent. 
% For example, one can use shorter networks for model averaging, or train each network for fewer iterations.
% While these methods reduce training time, both may sacrifice accuracy on a sufficiently challenging dataset.
% Even with careful hyperparameter selection, such an ensemble might 
% not outperform the simple baseline: training a single network under the same amount of cost.
% Alternatively, one could train a single deep network, but ensemble intermediate models at different iterations of training.
% This method does not introduce any additional training
% beyond training a single model, yet the accuracy gain is often insignificant as there is little diversity amongst the different
% training iterations.


%Although neural networks ensembles have been widely studied and applied in machine learning \citep{hansen1990neural,krogh1995neural}, most of existing works focus on improving the generalization performance, while few of them address the cost of training ensembles. This may be explained by the fact that one decade ago both the size of datasets and models were relatively small, and it was affordable to train tens or even hundreds of networks for model averaging. But the situation is changing rapidly. On one hand, many of the datasets we have now contain millions of samples; On the other hand, researchers have to train very deep networks to achieve good performances on these large scale datasets.
%
%Even with the modern GPUs, a deep network may take days or weeks to train. Under limited resources, it becomes important to address the problem of high training cost of neural network ensembles. Several recent works focus on reducing the test time cost of ensembles, by transferring or distilling the ``knowledge" of cumbersome ensembles into smaller networks \citep{bucilu¨£2006model,hinton2015distilling}.
%

% In recent years, researchers have developed effective techniques which create ``implicit'' ensembles from single networks \citep{dropout,wan2013regularization,stochastic,singh2016swapout,krueger2016zoneout}. For example, the Dropout \citep{dropout} method randomly drops nodes in the hidden layers during each mini-batch of training. This can be viewed as training exponential number of networks with heavy parameter sharing. At test time, these models are averaged to make the final prediction. The recently proposed stochastic depth network \citep{stochastic} and Swapout \citep{singh2016swapout} algorithm create similar implicit ensembles at test time. \gp{Maybe worth mentioning the ``Resnets are ensembles\ldots'' paper?}

In this paper, we introduce a simple idea to obtain a cheep yet effective neural network ensembles under a limited training budget.
Our method produces multiple neural networks that achieve a remarkable balance between diversity and accuracy, yet only requires the \emph{same cost training a single network}. We draw inspiration from the recent finding of \cite{loshchilov2016sgdr} and \cite{leslie2016cyclical} that cyclical learning rates give significant improvement over commonly used learning rate scheduling. Cyclical learning rates split the training process into multiple blocks, each of which has a full learning rate annealing cycle. In each block, large learning rates perturbs a well converged model and small learning rates quickly drive the model towards convergence at a new minimum. Consequently, each block (or learning rate cycle) provides a candidate model for ensembling. This disturbance at the beginning of each block produces intermediate models with substantial diversity, and are therefore far more effective for ensembling than intermediate models from a conventional learning rate schedule. 

The proposed method has several remarkable advantages: 1) it is possible to retrieve models with good performance early in the training process, as noted by \citet{loshchilov2016sgdr}; 2) it always gives a complete single model at the end of training, and one can further improve its performance by simply ensembling more models provided test time budget allows;
%, making it useful for test time cost sensitive learning; 
3) the method is general and is compatible with many other effective tools in deep learning, such as Bagging, Boosting, Dropout, Swapout, Stochastic depth and the Dense-sparse-dense training flow. In fact, our method can be even more effective when combined with some of these other techniques.

We show on several popular benchmark datasets that our method yields state-of-the-art performance with the ResNet~\citep{identity-mappings} and DenseNet~\citep{huang2016densely} architecture under the same amount of training cost as in previous works. We also introduce how to further improve model diversity and reduce cost at training time.

%Tested on ResNet and DenseNet. Also introduces how to reducing test time computation.







