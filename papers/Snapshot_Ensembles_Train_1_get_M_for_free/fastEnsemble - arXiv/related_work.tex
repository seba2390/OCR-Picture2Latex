%!TEX root=main.tex
\section{Related Work}

Neural network ensembles have been widely studied and applied in machine learning \citep{hansen1990neural,krogh1995neural}. However, most of these prior studies focus on improving the generalization performance, while few of them address the cost of training ensembles.
%This may be explained by the fact that one decade ago both the size of datasets and models were relatively small, and it was affordable to train tens or even hundreds of networks for model averaging. But the situation is changing rapidly. On one hand, many of the datasets we have now contain millions of samples; On the other hand, researchers have to train very deep networks to achieve good performances on these large scale datasets. Even with the modern GPUs, a deep network may take days or weeks to train. Under limited resources, it becomes important to address the problem of high training cost of neural network ensembles. Several recent works focus on reducing the test time cost of ensembles, by transferring or distilling the ``knowledge" of cumbersome ensembles into smaller networks \citep{bucilu¨£2006model,hinton2015distilling}.

%\gp{It might be good to start off with something like: The most commonly used approach for ensembling classification models is averaging the softmax outputs of several models. While simple, it is highly effective for neural networks and is one of the most common ensembling methods used today.}

As an alternative to traditional ensembles, so-called ``implicit'' ensembles have high efficiency during both training and testing \citep{dropout,wan2013regularization,stochastic,singh2016swapout,krueger2016zoneout}.
The Dropout \citep{dropout} technique creates an ensemble out of a single model by ``dropping'' --- or zeroing --- random sets of hidden nodes during each mini-batch. At test time, no nodes are dropped, and each node is scaled by the probability of surviving during training.
\citeauthor{dropout} claim that Dropout reduces overfitting by preventing the co-adaptation of nodes.
An alternative explanation is that this mechanism creates an exponential number of networks with shared weights during training, which are then implicitly ensembled at test time.
DropConnect \citep{wan2013regularization} uses a similar trick to create ensembles at test time by dropping connections (weights) during training instead of nodes.
The recently proposed Stochastic Depth technique \citep{stochastic} randomly drops layers during training to create an implicit ensemble of networks with varying depth at test time.
% Stochastic Depth has a strong regularization effect and tends to outperform Dropout on very deep networks \citep{stochastic}. 
Finally, Swapout \citep{singh2016swapout} is a stochastic training method that generalizes Dropout and Stochastic Depth. From the perspective of model ensembling, Swapout creates diversified network structures for model averaging. Our proposed method similarly trains only a single model; however, the resulting ensemble is ``explicit'' in that the models do not share weights.  Furthermore, our method can be used in conjunction with any of these implicit ensembling techniques.

Several recent publications focus on reducing the \emph{test time cost} of ensembles, by transferring the ``knowledge" of cumbersome ensembles into a single model \citep{bucilu2006model,hinton2015distilling}. \citet{hinton2015distilling} propose to use an ensemble of multiple networks as the target of a single (smaller) network.
% In order to effectively train the new network, a high temperature is used to soften the probability distribution over classes. 
Our proposed method is complementary to these works as we aim to reduce the \emph{training} cost of ensembles rather than the test-time cost.

%There are several recently proposed training mechanisms to improve the power of explicit ensembles.
%\citet{laine2016temporal} propose a temporal ensembling method for semi-supervised learning, which achieves consensus among models trained with different regularization and augmentation conditions.
%%This method has been shown to outperform the model at the most recent training epoch.
%Recently, \cite{moghimiboosted} show that boosting can be applied to convolutional neural networks to create strong ensembles.

Perhaps most similar to our work is that of \citet{swann1998fast} and \citet{xie2013horizontal}, who explore creating ensembles from slices of the learning trajectory.
\citeauthor{xie2013horizontal} introduce the \emph{horizontal and vertical} ensembling method, which combines the output of networks within a range of training epochs.
More recently, \citet{jean2014using} and \citet{sennrich2016edinburgh} show improvement by ensembling the intermediate stages of model training. \citet{laine2016temporal} propose a temporal ensembling method for semi-supervised learning, which achieves consensus among models trained with different regularization and augmentation conditions for better generalization performance. Finally, \cite{moghimiboosted} show that boosting can be applied to convolutional neural networks to create strong ensembles.
Our work differs from these prior works in that we force the model to visit multiple local minima, and we take snapshots \emph{only} when the model reaches a minimum. We believe this key insight allows us to leverage more power from our ensembles.

Our work is inspired by the recent findings of \cite{loshchilov2016sgdr} and \cite{leslie2016cyclical}, who show that cyclic learning rates can be effective for training convolutional neural networks. The authors show that each cycle produces models which are (almost) competitive to those learned with traditional learning rate schedules while requiring a fraction of training iterations. Although model performance temporarily suffers when the learning rate cycle is restarted, the performance eventually surpasses the previous cycle after annealing the learning rate. The authors suggest that cycling perturbs the parameters of a converged model, which allows the model to find a better local minimum.
%Thus this method can generate several accurate and diverse models throughout the training process rather than producing a single model at the end of training.
We build upon these recent findings by (1) showing that there is significant diversity in the local minima visited during each cycle and (2) exploiting this diversity using ensembles. We are not concerned with speeding up or improving the  training of a single model; rather, our goal is to extract an ensemble of classifiers while following the optimization path of the final model.




