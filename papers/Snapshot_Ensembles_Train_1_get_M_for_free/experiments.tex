%!TEX root=main.tex
\section{Experiments}
\label{sec:results}



We demonstrate the effectiveness of Snapshot Ensembles on several benchmark datasets, comparing with competitive baselines. We run all experiments with Torch 7~\citep{torch}\footnote{Code to reproduce results is available at \url{https://github.com/gaohuang/SnapshotEnsemble}}.


\begin{table*}[t]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
& {\bf Method} &  & {\bf C10} & {\bf C100} & \bf{SVHN} & \bf{Tiny ImageNet}\\
%&& & {\bf scheduling}  & \\
\midrule
\multirow{5}{*}{{ ResNet-110}}  &  Single model &  & 5.52 &  28.02 & 1.96 & 46.50\\
%&  Dropout (0.2) &  &   &    &  &  \\
&  NoCycle Snapshot Ensemble &  &  5.49 &  26.97 & 1.78 & 43.69 \\
%&  SingleCycle Ensembles (STD LR) & & 7.35 & 26.30  & 1.81 & 45.50 \\
&  SingleCycle Ensembles & & 6.66 & 24.54  & 1.74 & 42.60\\
&  Snapshot Ensemble ($\alpha_0=0.1$) &  &  \textcolor{blue}{5.73} &  \textcolor{blue}{25.55} & \textcolor{blue}{\bf 1.63}  & \textcolor{blue}{40.54} \\
&  Snapshot Ensemble ($\alpha_0=0.2$) &  &  \textcolor{blue}{{\bf 5.32}} &  \textcolor{blue}{{\bf 24.19}} & \textcolor{blue}{1.66} & \textcolor{blue}{\bf 39.40} \\
\midrule
\multirow{6}{*}{{ Wide-ResNet-32}}  &  Single model &  &  5.43  &  23.55  & 1.90 & 39.63 \\
&  Dropout &  & 4.68  &  22.82  & 1.81  & 36.58 \\
&  NoCycle Snapshot Ensemble &  &  5.18 &  22.81 & 1.81 & 38.64\\
%&  SingleCycle Ensembles (STD LR) && 5.86 & 21.97  & 1.68  & 36.88\\
&  SingleCycle Ensembles && 5.95 & 21.38  & 1.65  & 35.53\\
&  Snapshot Ensemble ($\alpha_0=0.1$) &  &  \textcolor{blue}{{\bf 4.41}} &  \textcolor{blue}{{\bf 21.26}}  & \textcolor{blue}{1.64} & \textcolor{blue}{35.45}\\
&  Snapshot Ensemble ($\alpha_0=0.2$) &  &  \textcolor{blue}{4.73} &  \textcolor{blue}{21.56} &  \textcolor{blue}{{\bf 1.51}} & \textcolor{blue}{{\bf 32.90}}\\
\midrule
\multirow{6}{*}{{ DenseNet-40}} &  Single model &   &  5.24$^*$ &  24.42$^*$ & 1.77 & 39.09 \\
&  Dropout &  &  6.08 &  25.79 & 1.79$^*$ & 39.68 \\
&  NoCycle Snapshot Ensemble &  &  5.20 &  24.63  & 1.80 & 38.51 \\
%&  SingleCycle Ensembles (STD LR) && 5.55 & 22.85 & 2.03 & 40.43 \\
&  SingleCycle Ensembles & & 5.43 & 22.51  & 1.87 & 38.00  \\
%&  Ensemble 3 independent smaller models\footnotemark &  &  5.31 &  25.39 & & \\
%&  Ensemble 3 pre-mature models (100 epochs)   &  5.17 &  22.83 & & \\
&  Snapshot Ensemble ($\alpha_0=0.1$) &  &  \textcolor{blue}{4.99} &  \textcolor{blue}{23.34}  & \textcolor{blue}{{\bf 1.64}}  & \textcolor{blue}{37.25} \\
&  Snapshot Ensemble ($\alpha_0=0.2$) &  &  \textcolor{blue}{{\bf 4.84}} &  \textcolor{blue}{{\bf 21.93}}  & \textcolor{blue}{1.73}& \textcolor{blue}{{\bf 36.61}} \\
%&  Snapshot Ensemble (ours) &  (10-30-5-5) &  5.20 &  22.30  & & \\
%&  Snapshot Ensemble (ours) &  (0-40-5-5) &  4.96 &  22.91  & & \\
%&  Snapshot Ensemble (ours) &  100$\times$ 3 &  cosine annealing &  5.34 &  22.80  & & \\
\midrule
\multirow{6}{*}{{ DenseNet-100}}  &  Single model &  &  3.74$^*$ &  19.25$^*$ & - & - \\
&  Dropout &  &  3.65 &  18.77 & - & - \\
&  NoCycle Snapshot Ensemble & &  3.80  &  19.30  & - & -\\
%&  SingleCycle Ensembles (STD LR) &&  4.39 & 18.70 & - & - \\
&  SingleCycle Ensembles  & & 4.52 & 18.38  &  &  \\

&  Snapshot Ensemble ($\alpha_0=0.1$) &   &   \textcolor{blue}{{3.57}} &   \textcolor{blue}{{18.12}}  & \textcolor{blue}{-} & \textcolor{blue}{-} \\
&  Snapshot Ensemble ($\alpha_0=0.2$) &   &   \textcolor{blue}{{\bf 3.44}} &   \textcolor{blue}{{\bf 17.41}}  & \textcolor{blue}{-} & \textcolor{blue}{-} \\
%&  Snapshot Ensemble (ours) &  100 $\times$ 3&  cosine annealing &  {\bf 3.57} &  {\bf 18.12}  & & \\
\bottomrule
\end{tabular}
\caption[]{\small Error rates ($\%$) on CIFAR-10 and CIFAR-100 datasets. All methods in the same group are trained for the same number of iterations. Results of our method are colored in \textcolor{blue}{blue}, and the best result for each network/dataset pair are {\bf bolded}. $^*$ indicates numbers which we take directly from \cite{huang2016densely}.}
\label{table:results}
\end{table*}


%
%
%\begin{table*}[t]
%\centering
%\small
%\begin{tabular}{lcccccc}
%\toprule
%& {\bf Method} & & {\bf lr } & {\bf C10} & {\bf C100} & \bf{SVHN}\\
%&& & {\bf scheduling} \\
%\midrule
%\multirow{3}[3]{*}{{ ResNet-110}}  &  Single model & &  standard &  6.27$^*$ &  27.66$^*$ &\\
%&  NoCycle Snapshot Ensemble & &  standard &  {\bf 6.16} &  27.09 &\\
%&  Snapshot ensemble & &  cosine annealing (0.1) &  6.40 &  {\bf 26.37} & \\
%\midrule
% &  Single model & &  standard &  5.43$^*$  &  23.55$^*$  &\\
%%&  Stochastic Depth~\citep{stochastic} & &  standard &  5.23 &  24.58 & \\
%\multirow{1}[3]{*}{{ Wide-ResNet-32}}  &  NoCycle Snapshot Ensemble & &  standard &  5.18 &  22.81 &\\
%&  Snapshot ensemble & &  cosine annealing (0.1) &  {\bf 4.41} &  {\bf 21.26}  &\\
%&  Snapshot ensemble & &  cosine annealing (0.2) &  4.73 &  21.56 & \\
%\midrule
%&  Single model &  &  standard &  5.24 &  24.42 &\\
%&  Dropout (0.2) & &  standard &  6.08 &  25.79 & \\
%\multirow{1}[3]{*}{{ DenseNet-40}} &  NoCycle Snapshot Ensemble & &  standard &  5.20 &  24.63  &\\
%%&  Ensemble 3 independent smaller models\footnotemark & &  standard &  5.31 &  25.39 &\\
%%&  Ensemble 3 pre-mature models (100 epochs)  &  standard &  5.17 &  22.83 &\\
%&  Snapshot ensemble & &  cosine annealing (0.1) &  4.99 &  23.34  &\\
%&  Snapshot ensemble & &  cosine annealing (0.2) &  {\bf 4.84} &  {\bf 21.93}  &\\
%%&  Snapshot ensemble (ours) & &  standard (10-30-5-5) &  5.20 &  22.30  &\\
%%&  Snapshot ensemble (ours) & &  standard (0-40-5-5) &  4.96 &  22.91  &\\
%%&  Snapshot ensemble (ours) &  100$\times$ 3 &  cosine annealing &  5.34 &  22.80  &\\
%\midrule
% &  Single model & &  standard &  3.74 &  19.25 &\\
%&  Dropout (0.2) & &  standard &  - &  - &\\
%\multirow{1}[3]{*}{{ DenseNet-100}} &  NoCycle Snapshot Ensemble &&  standard &  3.80  &  19.30  &\\
%&  Snapshot ensemble &  &  cosine annealing (0.1) &  {3.57} &  {18.12}  &\\
%&  Snapshot ensemble &  &  cosine annealing (0.2) &  {\bf 3.44} &  {\bf 17.41}  &\\
%%&  Snapshot ensemble (ours) &  100 $\times$ 3&  cosine annealing &  {\bf 3.57} &  {\bf 18.12}  &\\
%\bottomrule
%\end{tabular}
%\caption[]{\small Error rates ($\%$) on CIFAR-10 and CIFAR-100 datasets. All methods in the same group use the same amount of training time and cost. Results that surpasses all competing methods are bold. $^*$ indicates results reproduced by ourselves.}
%\label{table:results}
%\end{table*}



\subsection{Datasets}
\para{CIFAR.}
The two CIFAR datasets \citep{cifar} consist of colored natural images sized at 32$\times$32 pixels. CIFAR-10 (C10) and CIFAR-100 (C100) images are drawn from 10 and 100 classes, respectively.
For each dataset, there are 50,000 training images and 10,000 images reserved for testing. %, and we hold out 5,000 training images as the validation set.
We use a standard data augmentation scheme \citep{netinnet, fitnet, dsn, allcnn, highway, stochastic, fractalnet}, in which the images are zero-padded with 4 pixels on each side, randomly cropped to produce 32$\times$32 images, and horizontally mirrored with probability $0.5$.

%We denote this augmentation scheme by a ``+'' mark at the end of the dataset name (\emph{e.g.}, C10+). %For data preprocessing, we normalize the data using the channel means and standard deviations. We evaluate our algorithm on all four datasets: C10, C100, C10+, C100+. For the final run we use all 50,000 training images and report the final test error at the end of training.

\para{SVHN.}
The Street View House Numbers (SVHN) dataset~\citep{svhn} contains $32\times32$ colored digit images from Google Street View, with one class for each digit. There are 73,257 images in the training set and 26,032 images in the test set. Following common practice~\citep{sermanet2012convolutional, maxout, huang2016densely}, we withhold 6,000 training images for validation, and train on the remaining images without data augmentation.
%All the images are normalized to the scale $[0,1]$ for preprocessing.

\para{Tiny ImageNet.}
The Tiny ImageNet dataset\footnote{\url{https://tiny-imagenet.herokuapp.com}} consists of a subset of ImageNet images~\citep{deng2009imagenet}. There are 200 classes, each of which has 500 training images and 50 validation images. Each image is resized to $64\times64$ and augmented with random crops, horizontal mirroring, and RGB intensity scaling~\citep{alexnet}.

\para{ImageNet.} The ILSVRC 2012 classification dataset \citep{deng2009imagenet} consists of 1000 images classes, with a total of 1.2 million training images and 50,000 validation images. We adopt the same data augmentation scheme as in \citep{he2016deep,huang2016densely} and apply a $224\times 224$ center crop to images at test time.

 \begin{figure*}[t]
\centerline{
                \includegraphics[width=0.49\linewidth]{figures/l100_k24_ensemble_singlerow-crop.pdf}
            \includegraphics[width=0.49\linewidth]{figures/l100_k24_ensemble_singlerow_02-crop.pdf}}
% }        \begin{subfigure}[b]{\linewidth}
%             \centering
%             \vspace{0.2em}
%         \end{subfigure}
%         \begin{subfigure}[b]{\linewidth}
%             \centering
%         \end{subfigure}
%         \caption[]
        \caption{\small DenseNet-100 Snapshot Ensemble performance on CIFAR-10 and CIFAR-100 with restart learning rate $\alpha_0=0.1$ (left two) and $\alpha_0=0.2$ (right two). Each ensemble is trained with $M\!=\!6$ annealing cycles (50 epochs per each).}
        \label{fig:free-ensemble-barplot}
        \vspace{-1em}
    \end{figure*}

\subsection{Training Setting}

\para{Architectures.} We test several state-of-the-art architectures, including residual networks ({\bf ResNet})~\citep{he2016deep}, {\bf Wide ResNet}~\citep{wide} and {\bf DenseNet}~\citep{huang2016densely}.
For ResNet, we use the original 110-layer network introduced by \cite{he2016deep}. Wide-ResNet is a 32-layer ResNet with 4 times as many convolutional features per layer as a standard ResNet. For DenseNet, our large model follows the same setup as ~\citep{huang2016densely}, with depth $L=100$ and growth rate $k=24$. In addition, we also evaluate our method on a small DenseNet, with depth $L=40$ and $k=12$. To adapt all these networks to Tiny ImageNet, we add a stride of $2$ to the first layer of the models, which downsamples the images to $32\times32$. For ImageNet, we test the 50-layer ResNet proposed in \citep{he2016deep}.
We use a mini batch size of 64.\footnote{Exceptions: ResNet-110 and Wide-ResNet are trained with batch size 128 on Tiny ImageNet. The ImageNet model is trained with batch size 256.}

\para{Baselines.} Snapshot Ensembles incur the training cost of a single model; therefore, we compare with baselines that require the same amount of training. First, we compare against a {\bf Single Model} trained with a standard learning rate schedule, dropping the learning rate from $0.1$ to $0.01$ halfway through training, and then to $0.001$ when training is at $75\%$.
Additionally, to compare against implicit ensembling methods, we test against a single model trained with {\bf Dropout}. This baseline uses the same learning rate as above, and drops nodes during training with a probability of $0.2$.

We then test the {\bf Snapshot Ensemble} algorithm trained with the cyclic cosine learning rate as described in \eqref{eq:lr-cosine}. We test models with the max learning rate $\alpha_0$ set to $0.1$ and $0.2$.
In both cases, we divide the training process into learning rate cycles. Model snapshots are taken after each learning rate cycle.
Additionally, we train a Snapshot Ensemble with a non-cyclic learning rate schedule. This {\bf NoCycle Snapshot Ensemble}, which uses the same schedule as the Single Model and Dropout baselines, is meant to highlight the impact of cyclic learning rates for our method.
To accurately compare with the cyclic Snapshot Ensembles, we take the same number of snapshots equally spaced throughout the training process.
Finally, we compare against {\bf SingleCycle Ensembles}, a Snapshot Ensemble variant in which the network is re-initialized at the beginning of every cosine learning rate cycle, rather than using the parameters from the previous optimization cycle.
This baseline essentially creates a traditional ensemble, yet each network only has $1/M$ of the typical training time.
%The model is trained using a standard learning rate cycle of $\left\lceil T/M\right\rceil$ iterations, after which the model is re-initialized.
This variant is meant to highlight the tradeoff between model diversity and model convergence. Though SingleCycle Ensembles should in theory explore more of the parameter space, the models do not benefit from the optimization of previous cycles.
% For each of the variants, we test the ensemble by feeding samples through each model snapshot and averaging the softmax outputs.

\para{Training Budget.} On CIFAR datasets, the training budget is $B=300$ epochs for DenseNet-40 and DenseNet-100, and $B=200$ for ResNet and Wide ResNet models. Snapshot variants are trained with $M=6$ cycles of $B/M=50$ epochs for DenseNets, and $M=5$ cycles of $B/M=40$ epochs for ResNets/Wide ResNets.
SVHN models are trained with a budget of $B=40$ epochs ($5$ cycles of $8$ epochs). For Tiny ImageNet, we use a training budget of $B=150$ ($6$ cycles of $25$ epochs). Finally, ImageNet is trained with a budget of $B=90$ epochs, and we trained 2 Snapshot variants: one with $M=2$ cycles and one with $M=3$.

\begin{wraptable}{r}{0.5\textwidth}
\centering
\small
% \vspace{-1em}
\begin{tabular}{cc}
\toprule
Method & {\bf Val. Error (\%)} \\
\midrule
Single model & 24.01 \\
Snapshot Ensemble ($M = 2$) & 23.33 \\
Snapshot Ensemble ($M = 3$) & 23.96 \\
\bottomrule
\end{tabular}
\caption[]{\small Top-1 error rates ($\%$) on ImageNet validation set using ResNet-50 with varying number of cycles.}
\label{table:imagenet}
 \end{wraptable}

\subsection{Snapshot Ensemble results}

\para{Accuracy.} The main results are summarized in Table~\ref{table:results}. In most cases, Snapshot ensembles achieve lower error than any of the baseline methods. Most notably, Snapshot Ensembles yield an error rate of $17.41\%$ on CIFAR-100 using large DenseNets, far outperforming the record of $19.25\%$ under the same training cost and architecture~\citep{huang2016densely}. Our method has the most success on CIFAR-100 and Tiny ImageNet, which is likely due to the complexity of these datasets. The softmax outputs for these datasets are high dimensional due to the large number of classes, making it unlikely that any two models make the same predictions. Snapshot Ensembling is also capable of improving the competitive baselines for CIFAR-10 and SVHN as well, reducing error by $1\%$ and $0.4\%$ respectively with the Wide ResNet architecture.

The NoCycle Snapshot Ensemble generally has little effect on performance, and in some instances even \emph{increases} the test error.
This highlights the need for a cyclic learning rate for useful ensembling.
The SingleCycle Ensemble has similarly mixed performance.
In some cases, e.g., DenseNet-40 on CIFAR-100, the SingleCycle Ensemble is competitive with Snapshot Ensembles. However, as the model size increases to 100 layers, it does not perform as well. This is because it is difficult to train a large model from scratch in only a few epochs.
These results demonstrate that Snapshot Ensembles tend to work best when utilizing information from previous cycles.
Effectively, Snapshot Ensembles strike a balance between model diversity and optimization.

%Additionally, the optimal learning rate of Snapshot Ensembling appears to be dependent on dataset and architecture. We propose a more thorough investigation of these hyperparameters as future work.

Table \ref{table:imagenet} shows Snapshot Ensemble results on ImageNet.
The Snapshot Ensemble with $M=2$ achieves $23.33 \%$ validation error, outperforming the single model baseline with $24.01\%$ validation error. It appears that 2 cycles is the optimal choice for the ImageNet dataset.
Provided with the limited total training budget $B=90$ epochs, we hypothesize that allocating fewer than $B/2 = 45$ epochs per training cycle is insufficient for the model to converge on such a large dataset.


\begin{wraptable}{r}{0.25\textwidth}
\centering
\small
% \vspace{-1em}
\begin{tabular}{cc}
\toprule
{$M$} & {\bf Test Error (\%)} \\
\midrule
2 & 22.92 \\
4 & 22.07 \\
6 & 21.93 \\
8 & 21.89 \\
10 & 22.16\\
\bottomrule
\end{tabular}
\caption[]{\small Error rates of a DenseNet-40 Snapshot Ensemble on CIFAR-100, varying $M$---the number of models (cycles) used in the ensemble.}
\label{table:vary-block}
 \end{wraptable}


\para{Ensemble Size.} In some applications, it may be beneficial to vary the size of the ensemble \emph{dynamically} at test time depending on available resources. Figure~\ref{fig:free-ensemble-barplot} displays the performance of DenseNet-40 on the CIFAR-100 dataset as the effective ensemble size, $m$, is varied. Each ensemble consists of snapshots from later cycles, as these snapshots have received the most training and therefore have likely converged to better minima. Although ensembling more models generally gives better performance, we observe significant drops in error when the second and third models are added to the ensemble. In most cases, an ensemble of two models outperforms the baseline model.

% \begin{wraptable}{r}{0.6\textwidth}
%  \centering
%  \small
%  \begin{tabular}{cccc}
%  \toprule
%    {\bf Configurations $K\times M$} & {\bf lr scheduling} & {\bf C10} & {\bf C100}\\
%   \midrule
%    \cite{huang2016densely} &   standard & 5.24 & 24.42\\
%  Snapshot Ensemble (1$\times$6) & cyclical (0.1) & 4.99 & 23.34 \\
%  Snapshot Ensemble (2$\times$3) & cyclical (0.1) & 5.34 & 22.80 \\
%  Snapshot Ensemble (3$\times$2) & cyclical (0.1) & 5.34 & 22.80 \\
%  Snapshot Ensemble (6$\times$1) & cyclical (0.1) & 5.43 & 22.51 \\
%   \midrule
%  Snapshot Ensemble (1$\times$6) & cyclical (0.2) & 4.84 & 21.93 \\
%  Snapshot Ensemble (2$\times$3) & cyclical (0.2) & 4.93 & 21.38 \\
%  Snapshot Ensemble (3$\times$2) & cyclical (0.2) & 4.89 & 21.04 \\
%  Snapshot Ensemble (6$\times$1) & cyclical (0.2) & 4.86 & 21.57 \\
%   \bottomrule
%   \end{tabular}
%  \caption[]{\small Error rates ($\%$) on CIFAR-10 and CIFAR-100 datasets using DenseNet-40, with a budget of training 6 cycles. Each annealing cycle is trained for 50 epochs.}
%  \label{table:results-multirow}
%  %\end{wraptable}
% \end{wraptable}


\para{Restart Learning Rate.} The effect of the restart learning rate can be observed in Figure~\ref{fig:free-ensemble-barplot}.
The left two plots show performance when using a restart learning rate of $\alpha_0=0.1$ at the beginning of each cycle, and the right two plots show $\alpha_0=0.2$.
In most cases, ensembles with the larger restart learning rate perform better, presumably because the strong perturbation in between cycles increases the diversity of local minima.


\para{Varying Number of Cycles.} Given a fixed training budget, there is a trade-off between the number of learning rate cycles and their length. Therefore, we investigate how the number of cycles $M$ affects the ensemble performance, given a fixed training budget. We train a 40-layer DenseNet on the CIFAR-100 dataset with an initial learning rate of $\alpha_0=0.2$. We fix the total training budget $B=300$ epochs, and vary the value of $M\in\{2,4,6,8,10\}$. As shown in Table~\ref{table:vary-block}, our method is relatively robust with respect to different values of $M$. At the extremes, $M=2$ and $M=10$, we find a slight degradation in performance, as the cycles are either too few or too short. In practice, we find that setting $M$ to be $4\sim8$ works reasonably well.

\para{Varying Training Budget.}
The left and middle panels of Figure~\ref{fig:true-ensemble-and-vary-budget} show the performance of Snapshot Ensembles and SingleCycle Ensembles as a function of training budget (where the number of cycles is fixed at $M=6$). We train a 40-layer DenseNet on CIFAR-10 and CIFAR-100, with an initial learning rate of $\alpha_0$ = 0.1, varying the total number of training epochs from 60 to 300. We observe that both Snapshot Ensembles and SingleCycle Ensembles become more accurate as training budget increases. However, we note that as training budget decreases, Snapshot Ensembles still yield competitive results, while the performance of the SingleCycle Ensembles degrades rapidly. These results highlight the improvements that Snapshot Ensembles obtain when the budget is low. If the budget is high, then the SingleCycle baseline approaches true ensembles and outperforms Snapshot ensembles eventually. 

% \begin{table*}[htbp]
% \centering
% \small
% \begin{tabular}{cccccc}
% \toprule
% {number of cycles $M$} & {\bf 2} & {\bf 4} & {\bf 6} & {\bf 8} & {\bf 10}\\
% \midrule
% Snapshot Ensemble (cosine annealing 0.2) & 22.92 & 22.07 & 21.93 & 21.89 & 22.16\\
% \bottomrule
% \end{tabular}
% \caption[]{\small Error rates ($\%$) on CIFAR-100 using DenseNet-40 with varying number of cycles.}
% \label{table:vary-block}
% \end{table*}
% \para{Parallel Snapshot Ensembles.}
%  We also study the effectiveness of Snapshot Ensembles when parallel resources are available during training. Given the total training budget of $B=300$ epochs, we train in parallel $K=\{1,2,3,6\}$ models, each with $M=\{6,3,2,1\}$ cycles of 50-epoch cosine annealing. Table~\ref{table:results-multirow} shows the results of different parallel configurations. By exploiting the parallel snapshot ensemble training, we find the performance can be further improved on dataset containing highly diverse classes such as CIFAR-100.

%\begin{wrapfigure}{r}{0.4\textwidth}
%    \centering
%    \includegraphics[width=0.4\textwidth]{figures/l40_k12_true_ensemble-crop.pdf}
%    \caption{\small Performance comparison of Snapshot Ensembles with true ensembles.}
%    \label{fig:true-ensemble}
%\end{wrapfigure}
%
%
% \begin{wrapfigure}{r}{0.4\textwidth}
%    \centering
%    \includegraphics[width=0.5\textwidth]{figures/l40_k12_vary_budget_ensemble.pdf}
%    \caption{\small Snapshot Ensembles under different training budget.}
%    \label{fig:vary-budget}
%\end{wrapfigure}

 \begin{figure*}[t]
        \centerline{
        \includegraphics[width=0.33\textwidth]{figures/l40_k12_vary_budget_ensemble-crop.pdf}
        \includegraphics[width=0.33\textwidth]{figures/l40_k12_vary_budget_ensemble_c100-crop.pdf}
        \includegraphics[width=0.33\textwidth]{figures/l40_k12_true_ensemble-crop.pdf}
        }
        \caption[]
        {\small Snapshot Ensembles under different training budgets on {\bf (Left)} CIFAR-10 and {\bf (Middle)} CIFAR-100.
        {\bf Right:} Comparison of Snapshot Ensembles with true ensembles.
        }
        \label{fig:true-ensemble-and-vary-budget}
        \vspace{-1em}
    \end{figure*}


\para{Comparison with True Ensembles.} We compare Snapshot Ensembles with the traditional ensembling method. The right panel of Figure~\ref{fig:true-ensemble-and-vary-budget} shows the test error rates of DenseNet-40 on CIFAR-100. The true ensemble method averages models that are trained with 300 full epochs, each with different weight initializations. Given the same number of models at test time, the error rate of the true ensemble can be seen as a lower bound of our method. Our method achieves performance that is comparable with ensembling of 2 independent models, but with the training cost of one model.


%\begin{wraptable}{r}

% CIFAR-100: 16.87%

% Baseline small models:
%4.36 and 21.24%

% TODO
% Ensemble of 3 independent 300 epochs (small models on cifar 10 and CIFAR-100 will be done soon)
% Start running two more large models (CIFAR-10 and CIFAR-100)


% \footnotetext{The depth $L=22$ and growth rate $k=12$ for a smaller model are chosen so that the training time of each individual model is $1/3$ of the small DenseNet. This makes the total training time of three independent smaller models comparable to others.}




% \subsection{Fast ensemble results}
% CIFAR-10 CIFAR-100: Small DenseNets
%
%\begin{table*}[htbp]
%\centering
%\small
%\begin{tabular}{lccccc}
%\toprule
%& {\bf row(s) $\times$ block(s)} && {\bf lr scheduling} & {\bf C10} & {\bf C100}\\
%\midrule
%& \cite{huang2016densely} &  & standard & 5.24 & 24.42\\
%& Snapshot Ensemble (1$\times$6) && cosine annealing (0.1) & 4.99 & 23.34 \\
%& Snapshot Ensemble (2$\times$3) && cosine annealing (0.1) & 5.23 & 22.54 \\
%& Snapshot Ensemble (3$\times$2) && cosine annealing (0.1) & 5.34 & 22.80 \\
%& Snapshot Ensemble (6$\times$1) && cosine annealing (0.1) & 5.43 & 22.51 \\
%\midrule
%& Snapshot Ensemble (1$\times$6) && cosine annealing (0.2) & 4.84 & 21.93 \\
%& Snapshot Ensemble (2$\times$3) && cosine annealing (0.2) & 4.93 & 21.38 \\
%& Snapshot Ensemble (3$\times$2) && cosine annealing (0.2) & 4.89 & 21.04 \\
%& Snapshot Ensemble (6$\times$1) && cosine annealing (0.2) & 4.86 & 21.57 \\
%\bottomrule
%\end{tabular}
%\caption[]{\small Error rates ($\%$) on CIFAR-10 and CIFAR-100 datasets using DenseNet-40. Each annealing cycle (block) is trained for 50 epochs.}
%\label{table:results-multirow}
%\end{table*}



\subsection{Diversity of model ensembles}

% initially
% starting at model 4, in the same local minimum.
% interpolating 5 and 6 cause a decrease in the test error, possibly due to that
\para{Parameter Space.} We hypothesize that the cyclic learning rate schedule creates snapshots which are not only accurate but also diverse with respect to model predictions. We qualitatively measure this diversity by visualizing the local minima that models converge to.
To do so, we linearly interpolate snapshot models, as described by~\cite{goodfellow2014qualitatively}.
Let $J \left(\theta\right)$ be the test error of a model using parameters $\theta$.
Given $\theta_1$ and $\theta_2$ --- the parameters from models 1 and 2 respectively --- we can compute the loss for a convex combination of model parameters: $J \left( \lambda \left( \theta_1 \right) + \left(1 - \lambda \right) \left( \theta_2 \right) \right)$,
where $\lambda$ is a mixing coefficient. Setting $\lambda$ to 1 results in a parameters that are entirely $\theta_1$ while setting $\lambda$ to 0 gives the parameters $\theta_2$. By sweeping the values of $\lambda$,
we can examine a linear slice of the parameter space. Two models that  converge to a similar minimum will have smooth parameter interpolations, whereas models that converge to different minima will likely have a non-convex interpolation, with a spike in error when $\lambda$  is between 0 and 1.

 \begin{figure*}[t]
        \centerline{\includegraphics[width=0.5\textwidth]{figures/parametric_cosine-crop.pdf}
        \includegraphics[width=0.5\textwidth]{figures/parametric_standard-crop.pdf}
        }
        % \begin{subfigure}[b]{\linewidth}
        %     \centering
        %     \includegraphics[width=\linewidth]{figures/l40_k12_interpolation_firstrow-crop.pdf}
        %     \vspace{0.2em}
        % \end{subfigure}
        % \begin{subfigure}[b]{\linewidth}
        %     \centering
        %     \includegraphics[width=\linewidth]{figures/l40_k12_interpolation_baseline-crop.pdf}
        % \end{subfigure}
        \caption[]
        {\small Interpolations in parameter space between the final model (sixth snapshot) and all intermediate snapshots. $\lambda=0$ represents an intermediate snapshot model, while $\lambda=1$ represents the final model.
        {\bf Left:} A Snapshot Ensemble, with cosine annealing cycles ($\alpha_0=0.2$ every $B/M=50$ epochs).
        {\bf Right:} A NoCycle Snapshot Ensemble, (two learning rate drops, snapshots every $50$ epochs).
        }
        \label{fig:parametric-plots}
        \vspace{-1em}
    \end{figure*}

\autoref{fig:parametric-plots} displays interpolations between the final model of DenseNet-40 (sixth snapshot) and all intermediate snapshots. The left two plots show Snapshot Ensemble models trained with a cyclic learning rate, while the right two plots show NoCycle Snapshot models.
$\lambda=0$ represents a model which is entirely snapshot parameters, while $\lambda=1$ represents a model which is entirely the parameters of the final model.
From this figure, it is clear that there are differences between cyclic and non-cyclic learning rate schedules.
Firstly, all of the cyclic snapshots achieve roughly the same error as the final cyclical model, as the error is similar for $\lambda=0$ and $\lambda=1$.
Additionally, it appears that most snapshots do not lie in the same minimum as the final model. Thus the snapshots are likely to misclassify different samples.
Conversely, the first three snapshots achieve much higher error than the final model. This can be observed by the sharp minima around $\lambda=1$, which suggests that mixing in any amount of the snapshot parameters will worsen performance. While the final two snapshots achieve low error, the figures suggests that they lie in the same minimum as the final model, and therefore likely add limited diversity to the ensemble.

% We denote by $w^{i}$ the model parameters obtained after $i$-th training cycle. In our case, $i=1,2,..,6$ since the entire training is divided into 6 chunks. For any two models parameterized by $w^{(i)}$ and $w^{(j)}$, a linearly interpolated model can be denoted by $\alpha w^{(i)} + (1-\alpha) w^{(j)}$. In Figure~\ref{fig:parametric-plots}, we plot the classification error for the linearly interpolated model, for each pair of $w^{(i)} (i\ne 6)$ and the last model $w^{(6)}$, with $\alpha$ varying from 0 to 2. The plot shows the model diversity between $w^{(i)} (i\ne 6)$ and $w^{(6)}$ progressively increases as $i$ decreases.

% % \begin{figure*}[t]
% %         \centering
% %         \begin{subfigure}[b]{\linewidth}
% %             \centering
% %             \includegraphics[width=0.9\linewidth]{figures/l40_k12_interpolation_baseline-crop.pdf}
% %             \vspace{0.2em}
% %         \end{subfigure}
% %         \begin{subfigure}[b]{\linewidth}
% %             \centering
% %             \includegraphics[width=0.9\linewidth]{figures/l100_k24_interpolation_baseline-crop.pdf}
% %         \end{subfigure}
% %         \caption[]
% %         {Parametric plots.}
% %         \label{fig:parametric-plots-baseline}
% %         \vspace{-1em}
% %     \end{figure*}



% In contrast, we also show in Figure~\ref{fig:parametric-plots-baseline} the parametric plots for baseline approach of ensembling intermediate models using standard learning rate scheduling. We interpolate the final single model at epoch 300 with all five previous intermediate models at every interval of 50 epochs. There are two salient observations can be drawn from this. First, interpolating models at epoch 250 and 300 result in an almost flattened curve (in red). The trend is similarly observable for the model at epoch 200 (in yellowgreen), when $\alpha\in[0,1]$. This implies that the 4th, 5th and last models converge to solutions with low model diversity across, hence making it undesirable for the purpose of model ensembling. On the other hand, early models at epoch 50, 100 and 150 are far less close to convergence, resulting in the interpolation with the last model (almost fully converged) suffer from a sharp performance degradation.


 \begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/l100_k24_corr-crop.pdf}
    \caption{\small Pairwise correlation of softmax outputs between any two snapshots for DenseNet-100. {\bf Left:} A Snapshot Ensemble, with cosine annealing cycles (restart with $\alpha_0=0.2$ every 50 epochs). {\bf Right:} A NoCycle Snapshot Ensemble, (two learning rate drops, snapshots every 50 epochs).}
    \label{fig:correlation}
\end{wrapfigure}
\para{Activation space.} To further explore the diversity of models, we compute the pairwise correlation of softmax outputs for every pair of snapshots. \autoref{fig:correlation} displays the average correlation for both cyclic snapshots and non-cyclical snapshots. Firstly, there are large correlations between the last 3 snapshots of the non-cyclic training schedule (right). These snapshots are taken after dropping the learning rate, suggesting that each snapshot has converged to the same minimum. Though there is more diversity amongst the earlier snapshots, these snapshots have much higher error rates and are therefore not ideal for ensembling. Conversely, there is less correlation between all cyclic snapshots (left). Because all snapshots have similar accuracy (as can be seen in \autoref{fig:parametric-plots}), these differences in predictions can be exploited to create effective ensembles.
% As expected, correlations are strongest between consecutive snapshots.

