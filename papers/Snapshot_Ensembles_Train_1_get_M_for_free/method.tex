%!TEX root=main.tex
\section{Snapshot Ensembling}
\label{sec:method}



%\begin{table}[]
%\centering
%\resizebox{\textwidth}{!}{%
%\begin{tabular}{|l|c|c||c|c|}
%\hline
%  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
%Method    & Single model performance & Diversity  & Efficiency   & Ensemble performance \\ \hline
%Ensemble independent fully converged models & high & high & low  & high   \\ \hline
%Ensemble independent premature models       & low & high  & high & low   \\ \hline
%Ensemble models at different epochs         & high & low & high & low   \\ \hline
%Implicit ensemble (e.g., Dropout)           & high & low & high &    \\ \hline
%%   &  & % &  \\
%%   &  &  &  \\
%\hline
%\end{tabular}
%}
%
%\caption{Different ways of ensembling neural networks}
%\label{my-label}
%\end{table}

% \subsection{Learn free ensembles}
Snapshot Ensembling produces an ensemble of accurate and diverse models from a single training process. At the heart of Snapshot Ensembling is an optimization process which visits several local minima before converging to a final solution. We take model snapshots at these various minima, and average their predictions at test time.

Ensembles work best if the individual models (1) have low test error and (2) do not overlap in the set of examples they misclassify.
Along most of the optimization path, the weight assignments of a neural network tend not to correspond to low test error.
In fact, it is commonly observed that the validation error drops significantly only after the learning rate has been reduced, which is typically done after several hundred epochs.
Our approach is inspired by the observation that training neural networks for fewer epochs and dropping the learning rate earlier has minor impact on the final test error~\citep{loshchilov2016sgdr}.
This seems to suggest that local minima along the optimization path become promising (in terms of generalization error) after only a few epochs.

%To form a meaningful ensemble, each snapshot model should produce as few errors as possible. However, the set of errors that are made by any given snapshots should be disjoint from that of other snapshots to reduce the variance of the ensemble.
%Therefore, it is ideal to take snapshots in the local minimum where the model achieves acceptable accuracy. During a typical training process, the optimization trajectory leads the model to only one such local minimum. SGD optimization in high dimensional spaces tends to converge to high-error saddle points when the learning rate is large \citep{dauphin2014identifying}. These saddle points can be escaped by dropping the learning rate to a low value, which attracts the model to a better local minimum. Since typical learning rate schedules lower the learning rate only once, only the final converged solution will be an accurate local minimum.

%In order to discover multiple viable solutions, we perturb the model after it has converged to a local minimum. By raising the learning rate, the model escapes the local minimum and can therefore converge to alternate solutions when the learning rate is lowered again. Repeating this process creates an optimization trajectory which visits multiple low-error solutions, each of which produces different predictions on test data. By taking a model ``snapshot'' at each of these solutions, we obtain several accurate models which are prime for ensembling.



% \begin{wrapfigure}{r}{0.25\textwidth}
%     \centering
%     \includegraphics[width=0.25\textwidth]{figures/lr_schedule-crop.pdf}
%     \caption{bla}
%     \label{fig:cosine}
% \end{wrapfigure}
\begin{wrapfigure}{r}{0.5\textwidth}
    \includegraphics[width=0.5\textwidth]{figures/train_loss.pdf}
    \caption{\small Training loss of 100-layer DenseNet on CIFAR10 using standard learning rate (blue) and $M=6$ cosine annealing cycles (red). The intermediate models, denoted by the dotted lines, form an ensemble at the end of training.}
    \label{fig:loss_curve}
\end{wrapfigure}
\para{Cyclic Cosine Annealing.}
To converge to multiple local minima, we follow a cyclic annealing schedule as proposed by~\citet{loshchilov2016sgdr}.
We  lower the learning rate at a very fast pace, encouraging the model to converge towards its first local minimum after as few as $50$ epochs. The optimization is then continued at a larger learning rate, which perturbs the model and dislodges it from the minimum. We repeat this process several times to obtain multiple convergences.
Formally, the learning rate $\alpha$ has the form:
%
\begin{equation}
\label{eq:lr-cycle}
\alpha(t) = f \left(\text{mod}\left(t-1,\left\lceil T/M \right\rceil\right)\right),
\end{equation}
%
where $t$ is the iteration number, $T$ is the total number of training iterations, and $f$ is a monotonically decreasing function.
In other words, we split the training process into $M$ cycles, each of which starts with a large learning rate, which is annealed to a smaller learning rate. The large learning rate $\alpha=f(0)$ provides the model enough energy to escape from a critical point, while the small learning rate $\alpha=f(\lceil T/M \rceil)$ drives the model to a well behaved local minimum. In our experiments, we set $f$ to be the shifted cosine function proposed by \cite{loshchilov2016sgdr}:
\begin{equation}
\label{eq:lr-cosine}
\alpha(t) = \frac{\alpha_0}{2}\left(\cos\left(\frac{\pi\text{mod}(t-1,\lceil T/M\rceil)}{\lceil T/M\rceil}\right)+1\right),
\end{equation}
%
where $\alpha_0$ is the initial learning rate.
Intuitively, this function anneals the learning rate from its initial value $\alpha_0$ to $f(\lceil T/M \rceil) \approx 0$ over the course of a cycle. Following \citep{loshchilov2016sgdr}, we update the learning rate at each iteration rather than at every epoch. This improves the convergence of short cycles, even when a large initial learning rate is used.

% Note that this strategy was also used by \cite{loshchilov2016sgdr} and \cite{leslie2016cyclical} recently. However, their aim is to obtain a best single model at the end of training, and this difference will lead to different considerations in designing the learning rate cycles. In particular, as our aim is to obtain strong neural network ensembles, instead of just pursuing an accurate model at the end of training, larger initial learning rate is usually preferred. This helps to speed up the training for the first few blocks, and tends to push the models converging to more diversified critical points. Although larger initial learning rate may degrade the performances of each single model, it benefits the final ensemble.


% We introduce a simple method to obtain multiple neural networks for effective model ensembling in one single training process, with the same amount of computational cost as training a single model in a usual way, but with significantly improved performance. Our basic idea is to collect models at different points along the training path for ensembling. However, traditional way of training neural networks has the learning rate gradually decaying from a larger value to some smaller value, which makes ensembling models from different iterations ineffective, because earlier models correspond to large learning rate are not accurate enough, while later models correspond to small learning rate strongly correlate with each other. As a consequence, naively ensembling models at different points of the training process does not lead to any improvement in practice.

% To collect intermediate neural networks in a single training pass to form meaningful ensembles, it is important to have the model occasionally converge into a local minimum or saddle points, where the model achieves acceptable accuracy. In fact, \cite{dauphin2014identifying} show that the saddle points are prevalent in high dimensional spaces, and many of them give relatively low training loss. Therefore, we can take a ``snapshot'' for the temporarily converged model for later ensembling. We then need to endow the model enough energy to escape from these local minima or saddle points again to search for better ones. By repeating this process, we can take many ``snapshots'' of models along the path while searching for optimal weights. These models yields good accuracy as they all settled in critical points, and they tend to produce different predictions on test data as the critical points are different in high dimensional space. Therefore, effective ensemble can be ensured.

% Specifically, we split the training process into $K$ stages, each of which has a full learning rate annealing cycle, i.e., we have the learning rate starting from a large value and gradually decreasing to a small value. The large learning rate in each stage gives the model enough energy to escape from a critical point, and the small learning rate will drive the model to a well behaved local minimum or saddle points. Formally, the learning rate $\alpha$ in our method has the form:





% \para{Learning rate annealing.}
% The learning rate function $f$ can take many forms of monotonically decreasing function. Some popular choices are multi-step functions, linear/exponential decay functions and the shifted cosine function recently used by \cite{loshchilov2016sgdr}. The cosine function is relatively easy to use, as we only need to specify a maximum learning rate, which is usually set to the initial learning rate; and a minimum learning rate, which is usually set to 0. Thus we adopt the cosine shape learning rate in our experiments. However, instead of updating the learning rate at each epoch as in \cite{loshchilov2016sgdr}, we have the learning rate updated at each iteration, so as to produce a more smooth annealing curve. This is important because in our case each block has fewer epochs, and changing learning rate at each epoch tend to give less converged models, especially when the initial learning rate is large. Specifically, the learning rate annealing function used in our experiment is
% \begin{equation}
% \label{eq:lr-cosine}
% \alpha(t) = \frac{\alpha_0}{2}\left(\cos\left(\frac{\pi\text{mod}(t-1,T/K)}{T/K}\right)+1\right),
% \end{equation}
% where $\alpha_0$ is the initial learning rate.


%The obtained model ensemble achieves significant improved performance over the single model trained using conventional learning rate scheduling, without adding further training cost overhead. In other words, the model ensemble is obtained for free or at relatively low cost.

% \para{Number of learning rate annealing cycles.}
% Empirically, we find that the algorithm is robust to the number of cycles of annealing learning rate. In practice, splitting the entire training process into 3 to 10 cycles works well.


\para{Snapshot Ensembling.}
\autoref{fig:loss_curve} depicts the training process using cyclic and traditional learning rate schedules. At the end of each training cycle, it  is apparent that the model reaches a local minimum with respect to the training loss. Thus, before raising the learning rate, we take a ``snapshot'' of the model weights (indicated as vertical dashed black lines). After training $M$ cycles, we have $M$ model snapshots, $f_1 \ldots f_M$, each of which will be used in the final ensemble. It is important to highlight that the total training time of the $M$ snapshots is the same as training a model with a standard schedule (indicated in blue). In some cases, the standard learning rate schedule achieves lower training loss than the cyclic schedule; however, as we will show in the next section, the benefits of ensembling outweigh this difference.

\para{Ensembling at Test Time.}
The ensemble prediction at test time is the average of the last $m$ ($m \leq M$) model's softmax outputs. Let $\mathbf{x}$ be a test sample and let $h_i \left( \mathbf{x} \right)$ be the softmax score of snapshot $i$. The output of the ensemble is a simple average of the last $m$ models:
%
$
  h_{\text{Ensemble}} = \frac{1}{m} \sum_{0}^{m-1} h_{M-i} \left( \mathbf{x} \right).
  % \label{eq:ensemble}
$
%
We always ensemble the last $m$ models, as these models tend to have the lowest test error.



% In some scenarios, one may have a pretrained model, and would like to have more models to form an ensemble. The easiest way is to retrain models from scratch, which however can be computationally expensive. In fact, to obtain model ensemble more efficiently, one can first disturb the pretrained model, and retain the model to converge. Although there are many ways to disturb the model, we propose to use large learning rate to push the weights away from the current position, and then anneal the learning rate to make the weights converges to a different point.


% \subsection{Further increasing the model diversity and reducing computational cost}
% \para{Subsample the training set.}
% During each learning rate cycle, we can subsample a subset of the training set as in the \emph{bagging} method. Since each model is trained on a different subset of training data, the diversity will increase, and the number of iterations to update the model can be lowered.

% \para{Subsample the model.}
% We can also subsample the model at a previous cycle and retrain this model, so as to further lower the correlation between models. Model subsampling may also reduce the training cost as the model size shrinks.


%\subsection{Reducing evaluation cost}
%
%\para{Ensemble the last few models.}
% In the free ensemble method, one can always start with the model from the end of the last block. When test time budget allows, we can add an additional model to the ensemble from the last iteration of the previous stage.
%
%\para{Model distillation.}
% Using the model distillation method~\citep{hinton2015distilling}, we can transfer the knowledge of the model ensemble into a single mode. The student network can be initialized with the model from the last iteration, and it would only require very few iterations to update the model.








