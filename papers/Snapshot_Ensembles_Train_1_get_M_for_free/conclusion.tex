%!TEX root=main.tex
\section{Discussion}

We introduce Snapshot Ensembling, a simple method to obtain ensembles of neural networks without any additional training cost. 
Our method exploits the ability of SGD to converge to and escape from local minima as the learning rate is lowered, which allows the model to visit several weight assignments that lead to increasingly accurate predictions over the course of training. We harness this power with the cyclical learning rate schedule proposed by~\citet{loshchilov2016sgdr}, saving model snapshots at each point of convergence. We show in several experiments that all snapshots are accurate, yet produce different predictions from one another, and therefore are well suited for test-time ensembles.
Ensembles of these snapshots significantly improve the state-of-the-art on CIFAR-10, CIFAR-100 and SVHN.
Future work will explore combining Snapshot Ensembles with traditional ensembles. 
In particular, we will investigate how to balance growing an ensemble with new models (with random initializations) and refining existing models with further training cycles under a fixed training budget.

\section*{Acknowledgements}

We thank Ilya Loshchilov and Frank Hutter for their insightful comments on the cyclic cosine-shaped learning rate. The authors are supported in part by the, III-1618134, III-1526012, IIS-1149882 grants from the National Science Foundation, US Army Research Office W911NF-14-1-0477, and the Bill and Melinda Gates Foundation. 

% \para{Parallel \name{}s.}
% The short learning rate annealing cycles also provide a new way to improve traditional ensemble training. In particular, we can consider the short learning rate cycle as a new primitive for training rather than a long annealing process. Ensembles can be constructed either by training a single model for multiple cycles (\name{}) or by training multiple models (with different random initializations) for few cycles each. As an example, given a budget of $M=6$ short cycles, we can train $K=1$ model with $M=6$ cycles, or $K=2$ independent models with $M=3$ cycles each, etc. In theory, these cycles could be allocated dynamically, choosing to improve existing models or train new ones by greedily minimizing the validation error. The optimal trade-off between number of cycles $M$ and number of parallel initializations $K$ is still open research. 
