%!TEX root=main.tex
\section{Introduction}


%% trying new spin
Stochastic Gradient Descent (SGD)~\citep{bottou2010large} and its accelerated variants~\citep{kingma2014adam,duchi2011adaptive} have become the de-facto approaches for optimizing deep neural networks. The popularity of SGD can be attributed to its ability to avoid and even escape spurious saddle-points and local minima~\citep{dauphin2014identifying}.
Although avoiding these spurious solutions is generally considered positive,
in this paper we argue that these local minima contain useful information that may in fact improve model performance.
% there may in fact be some value to these local minima and that simply leaving them on the way-side may be outright wasteful.
%In particular, the sets of errors made at these local minima are disjoint from the errors made by the final model, and therefore can be exploited for improved accuracy.

Although deep networks typically never converge to a global minimum, there is a notion of ``good'' and ``bad'' local minima with respect to generalization.  \citet{keskar2016large} argue that local minima with
flat basins tend to generalize better.
% Intuitively this may because these change less if, hypothetically, the training data were swapped for testing data.
%
SGD tends to avoid sharper local minima because gradients are computed from small mini-batches and are therefore inexact~\citep{keskar2016large}. If the learning-rate is sufficiently large, the intrinsic random motion across gradient steps prevents the optimizer from reaching any of the sharp basins along its optimization path. However, if the learning rate is small, the model tends to converge into the closest local minimum. These two very different behaviors of SGD are typically exploited in different phases of optimization~\citep{he2016deep}. Initially the learning rate is kept high to move into the general vicinity of a flat local minimum. Once this search has reached a stage in which no further progress is made, the learning rate is dropped (once or twice), triggering a descent, and ultimately convergence, to the final local minimum.

It is well established~\citep{kawaguchi2016deep} that the number of possible local minima grows exponentially with the number of parameters---of which modern neural networks can have millions. It is therefore not surprising that two identical architectures optimized with different initializations or minibatch orderings will converge to different solutions.  Although different local minima often have very similar error rates, the corresponding neural networks tend to make different mistakes. This diversity can be exploited through ensembling, in which multiple neural networks are trained  from different initializations  and then combined  with majority voting or averaging~\citep{caruana2004ensemble}. Ensembling  often leads to drastic reductions  in error rates. In fact, most high profile competitions, e.g. Imagenet~\citep{deng2009imagenet} or Kaggle\footnote{\url{www.kaggle.com}}, are won by ensembles of deep learning architectures.

Despite its obvious advantages, the use of ensembling for deep networks is not nearly as wide-spread as it is for other algorithms.
% For example decision trees are most commonly used in the form of Random Forests or Boosted ensembles~\citep{breiman2001random,friedman2001greedy}.
One likely reason for this lack of adaptation may be the cost of learning multiple neural networks.
% Whereas  decision trees are cheap and require mere seconds to construct,
Training deep networks can last for weeks, even on high performance hardware with GPU acceleration.
As the training cost for ensembles increases linearly, ensembles can quickly becomes uneconomical for most researchers without access to industrial scale computational resources.
% Although the training of deep net ensembles can be trivially parallelized, few have access to sufficient GPU servers that can be deployed in parallel for long durations. \gp{How's this?}


% n% It is known that neural networks contain numerous ``good'' local minima~\citep{dauphin2014identifying}.
% Due to random initialization and shuffled minibatches, it is improbable that any two models converge to the same minimum, and therefore models will likely have disjoint sets of errors.
% This property can be exploited by ensembling several models, which reduces overall variance as a result of averaging differing predictions~\citep{Goodfellow-et-al-2016-Book}.
% However, training deep networks is computationally expensive and can last for days or even weeks even on high performance hardware with GPU acceleration.
% %Training ensembles of them increases the cost linearly and quickly becomes prohibitive for most researchers without access to industrial scale computational resources.
% Although the training of deep net ensembles can be trivially parallelized, few have access to sufficient GPU servers that can be deployed in parallel for long durations. \gp{How's this?}


\begin{figure*}[t!]
\vspace{-2ex}
  \centerline{
  \includegraphics[width=0.45\textwidth]{figures/sgd_path.pdf}
    \includegraphics[width=0.45\textwidth]{figures/sgd_restart_path.pdf}
  }
%   \begin{subfigure}[t]{0.4\textwidth}
%     \centering

%   \end{subfigure}%
%   ~
%   \begin{subfigure}[t]{0.4\textwidth}
%   \vspace{-2ex}
% \centerline{
%   \end{subfigure}
  \caption{\small{\bf Left:} Illustration of SGD optimization with a typical learning rate schedule. The model converges to a minimum at the end of training. {\bf Right:} Illustration of Snapshot Ensembling. The model undergoes several learning rate annealing cycles, converging to and escaping from multiple local minima. We take a snapshot at each minimum for test-time ensembling.}
  \label{fig:paths}
\end{figure*}

In this paper we focus on the seemingly-contradictory goal of learning an ensemble of multiple neural networks \emph{without incurring any additional training costs}.
We achieve this goal with a training method that is simple and straight-forward to implement. 
Our approach leverages the non-convex nature of neural networks and the ability of SGD to converge to and escape from local minima on demand.  Instead of training $M$ neural networks independently from scratch, we let SGD converge $M$ times to local minima along its optimization path.
Each time the model converges, we save the weights and add the corresponding network to our ensemble. We then restart the optimization with a large learning rate to escape the current local minimum. More specifically, we adopt the cycling procedure suggested by~\cite{loshchilov2016sgdr}, in which the learning rate is abruptly raised and then quickly lowered to follow a cosine function.
Because our final ensemble consists of snapshots of the optimization path, we refer to our approach as \emph{Snapshot Ensembling}. \autoref{fig:paths} presents a high-level overview of this method.

In contrast to traditional ensembles, the training time for the entire ensemble is identical to the time required to train a \emph{single} traditional model.  During testing time, one can  evaluate and average the last (and therefore most accurate) $m$ out of $M$ models. Our approach is naturally compatible with other methods to improve the accuracy, such as data augmentation,  stochastic depth~\citep{stochastic}, or  batch normalization~\citep{batch-norm}.  In fact, \name{}s  can even be ensembled, if for example parallel resources are available during training.  In this case, an ensemble of $K$ \name{}s  yields $K\times M$  models at $K$ times the training cost.

We evaluate the efficacy of \name{}s on three state-of-the-art deep learning architectures for object recognition: ResNet~\citep{he2016identity}, Wide-ResNet~\citep{wide}, and DenseNet~\citep{huang2016densely}.  We show across four different data sets that Snapshot Ensembles almost always reduce error without increasing training costs. For example, on CIFAR-10 and CIFAR-100, \name{}s obtains error rates of $3.44\%$ and $17.41\%$ respectively.

