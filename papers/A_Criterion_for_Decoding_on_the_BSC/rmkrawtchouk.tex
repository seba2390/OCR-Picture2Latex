\documentclass[12pt]{article}
%\documentclass[12pt]{amsart}

%\pdfoutput=1
\usepackage{amsmath}

\usepackage{amsfonts,amssymb,amsthm,enumitem}
%\usepackage{fullpage}
\usepackage[margin=1.2in]{geometry}
\usepackage{mathtools}
\usepackage{nicefrac,setspace}
\usepackage{xcolor}

%\usepackage{tikz}
%\usetikzlibrary{shapes,backgrounds}
%\usepackage{mathtools,thmtools,thm-restate}
%\usepackage{showlabels}


% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_3018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_3018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_3018}

\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{comment}   %allows for multi-line comments
\usepackage{mathtools}
\usepackage{bbm}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\odot$}}}}}
\makeatother


\newtheorem{theorem}{Theorem}
\newtheorem{claim}[theorem]{Claim}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{remark}{Remark}
\newtheorem*{claim*}{Claim}
\newtheorem*{remark*}{Remark}
\newtheorem*{lemma*}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem*{fact*}{Fact}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{corollary*}{Corollary}
\newtheorem{obs}[theorem]{Observation}
\newtheorem{prop}[theorem]{Proposition}

\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\Fcal}{{\mathcal F}}
\renewcommand{\mod}{\mathop {\mathsf{mod}}}
\newcommand{\ego}{\mathsf{EGO}}
\newcommand{\uc}{{\#\mathsf{ub}}}
\newcommand{\mix}{{\#\mathsf{mix}}}
\newcommand{\real}{\mathop \mathfrak{R}}
\newcommand{\ol}{\overline}
\newcommand{\wh}{\widehat}
\newcommand{\Z}{{\mathbb Z}}
\renewcommand{\P}{\mathop {\mathbb P}}
\newcommand{\X}{{\mathcal{X}}}
\newcommand{\A}{{\mathcal{A}}}
\newcommand{\B}{{\mathcal{B}}}
\newcommand{\G}{{\mathcal{G}}}
\newcommand{\calC}{{\mathcal{C}}}
\newcommand{\U}{{\mathcal{U}}}
\newcommand{\V}{{\mathcal{V}}}
\newcommand{\calX}{{\mathcal{X}}}
\newcommand{\calR}{{\mathcal{R}}}
\newcommand{\mc}{{\mathsf{MC}}}
\newcommand{\ex}{\mathop{\mathbb{E}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\eps}{\epsilon}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}	
\newcommand{\C}{\mathbb{C}}		 
\newcommand{\maj}{\mathsf{maj}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\E}{\mathop \mathbb{E}}
\newcommand{\m}{\mathsf{M}}
\newcommand{\g}{\mathsf{G}}
\newcommand{\J}{\mathsf{J}}

\newcommand{\lin}[1]{#1_\mathsf{lin}}

\newcommand{\nbits}{\{\pm 1\}}
\newcommand{\inner}[2]{\left \langle #1 , #2 \right \rangle}

\newcommand{\ip}[2]{\left\langle #1,#2\right\rangle}
\newcommand{\ub}{\mathsf{ub}}
\newcommand{\Expect}[1]{\mathop{\mathbb{E}}\left
	[ #1 \right ]}
\newcommand{\Ex}[2]{\mathop{\mathbb{E}}\displaylimits_{#1}\left
	[ #2 \right ]}

\title{A Criterion for Decoding on the BSC}
  
\newcommand{\an}[1]{\textcolor{blue}{#1}}
\author{
  \fontsize{14pt}{14pt}\selectfont Anup Rao\thanks{Supported by NSF CCF-2131899.}\\
   \fontsize{12pt}{12pt}\selectfont School of Computer Science, University of Washington\\ 
    \fontsize{12pt}{12pt}\selectfont anuprao@cs.washington.edu
  \and
  \fontsize{14pt}{14pt}\selectfont Oscar Sprumont\thanks{Supported by NSERC PGSD3-545945-2020 and NSF CCF-2131899.}\\
  \fontsize{12pt}{12pt} \selectfont School of Computer Science, University of Washington\\
  \fontsize{12pt}{12pt} \selectfont osprum@cs.washington.edu
}

\begin{document}

\maketitle

\begin{abstract}
We present an approach to showing that a linear code is resilient to random errors. We use this approach to obtain decoding results for both transitive codes and Reed-Muller codes. We give three kinds of results about linear codes in general, and transitive linear codes in particular.
\begin{enumerate}
\item We give a tight bound on the weight distribution of every transitive linear code $C \subseteq \F_2^N$: $\Pr_{c \in C}[|c| = \alpha N] \leq 2^{-(1-h(\alpha)) \cdot \mathsf{dim}(C)}$. 
\item We give a criterion that certifies that a linear code $C$ can be decoded on the binary symmetric channel. Let $K_s(x)$ denote the Krawtchouk polynomial of degree $s$, and let $C^\perp$ denote the dual code of $C$. We show that bounds on $\E_{c \in C^{\perp}}[ K_{\epsilon N}(|c|)^2]$ imply that $C$ recovers from errors on the binary symmetric channel with parameter $\epsilon$. Weaker bounds can be used to obtain list-decoding results using similar methods. One consequence of our criterion is that whenever the weight distribution of $C^\perp$ is sufficiently close to the binomial distribution in some interval around $\frac{N}{2}$, $C$ is resilient to $\epsilon$-errors.
\item We combine known estimates for the Krawtchouk polynomials with our weight bound for transitive codes, and with known weight bounds for Reed-Muller codes, to obtain list-decoding results for both these families of codes. In some regimes, our bounds for Reed-Muller codes achieve the information-theoretic optimal trade-off between rate and list size.
\end{enumerate}
\end{abstract}




\section{Introduction}\label{intro}
In his seminal 1948 paper, Shannon laid out the bases of coding theory and introduced the concept of channel capacity, which is the maximal rate at which information can be transmitted over a communication channel \cite{shannon1948entropy}. The two channels that have received the most attention are the Binary Symmetric Channel (BSC), where each bit is independently flipped with some probability $\epsilon$, and the Binary Erasure Channel (BEC), where each bit is independently replaced by an erasure symbol with some probability $\epsilon$. Shannon's work initiated a decades-long search for explicit codes that can achieve high rates over a noisy channel. 

Explicit construction of codes often have a lot of symmetry. In particular, many known constructions of codes are \emph{transitive}. The group of symmetries of a code is the subgroup $G$ of permutations $\pi:\{1,\dotsc, N\} \rightarrow \{1,\dotsc, N\}$ such that permuting the coordinates of each of the codewords using $\pi$ does not change the code. A code is transitive if for every two coordinates $i,j$, there is a permutation $\pi \in G$ with $\pi(i) = j$. A code is $2$-transitive if for every $i \neq k$, $j \neq \ell$ there is a permutation $\pi \in G$ with $\pi(i) = j, \pi(k) = \ell$. Many known constructions of codes are \emph{cyclic}, and every cyclic code is transitive. Reed-Solomon codes, BCH codes and Reed-Muller codes are all transitive. 

The binary code that is arguably the cleanest explicit candidate to achieving capacity over both the BSC and the BEC is the family of Reed-Muller codes. The codewords of the Reed-Muller code $\mathsf{RM}(n,d)$ are the evaluation vectors (over all points in $\F_2^n$) of all multivariate polynomials of degree $d$ in $n$ variables.

Reed-Muller codes enjoy strong symmetry beyond transitivity: their symmetry group is the group of invertible affine transformations over $\F_2^n$. Using fundamental results from Fourier analysis about the influences of symmetric boolean functions \cite{kahn1988kkl, Talagrand94, Bourgain97} has led to a very successful line of work, with \cite{kudekar2016erasure} showing that Reed-Muller codes achieve capacity over the BEC and \cite{hazla2021polyclose} showing that they are polynomially close to achieving capacity over the BSC. In fact, \cite{kudekar2016erasure} show that if a linear code $C \subseteq \F_2^N$ has a $2$-transitive symmetry group $G$ such that for every $S \subseteq \{1,\dotsc, N\}$ with $|S| = (s \log N)^{0.99}$, $|\{\pi(S): \pi \in G\}| \geq N^s$, then $C$ can tolerate $\epsilon - O(1/s)$ fraction of random erasures. Given these results, it is natural to investigate the types of symmetry that lead to good codes. In this paper, we prove three kinds of results relevant to understanding the error resilience of general linear codes,  transitive linear codes, and Reed-Muller codes.  
\begin{enumerate}
	\item 
We give a clean and tight weight distribution bound for every transitive linear code. We show that for any such code $C \subseteq \F_2^N$, $$\Pr_{c \in C}[|c| = \alpha N] \leq 2^{-(1-h(\alpha)) \cdot \mathsf{dim}(C)}.$$  This bound is proved by combining transitivity with the subadditivity of entropy. In some regimes, it improves on all previously known weight bounds for Reed-Muller codes (see Appendix \ref{aweight}).

\item We give a new criterion to validate that a code can be decoded over the BSC. Let 
$$K_{t}(x)= \sum_{j=0}^t (-1)^j\binom{x}{j}\binom{N-x}{t-j}$$
denote the Krawtchouk polynomial of degree $t$, and let $C^\perp$ denote the dual code of $C$. In spirit, our criterion says that any code $C$ satisfying
$$ \E_{c \in C^\perp}[K_{\epsilon N}(|c|)^2] < (1+o(N^{-1})) \cdot \binom{N}{\epsilon N}$$
can be uniquely decoded on the BSC with high probability. Our actual result is a little more technically involved (see Theorem \ref{fouriercriterionunique}). This criterion implies that any code whose dual codewords are distributed sufficiently close to the binomial distribution must be resilient to $\epsilon$-errors (see Corollary \ref{weightunique}).  Moreover, if the above expectation is bounded by $o(k\binom{N}{\epsilon N}^{-1})$, then we prove that the code can be list-decoded with a list size of about $k$. 

\item Finally, we combine 
known estimates for the Krawtchouk polynomials with our weight bound for transitive codes, and with known weight bounds for Reed-Muller codes, to obtain list-decoding results for both families of codes. In some regimes, our bounds for Reed-Muller codes achieve the information-theoretic optimal trade-off between rate and list size.

\end{enumerate}

Next, we discuss our results more rigorously. We note that throughout this section, for any set $X$ we denote the uniform distribution over $X$ by $\mathcal{D}(X)$.

\hfill\\
\textbf{I. Weight Bounds for Transitive Codes}
\hfill\\
We bound the weight distribution of any transitive linear code over any prime field. See section \ref{weighttransitive} for the proof.
\begin{theorem}\label{probtransitive}
Let $C\subseteq \mathbb{F}_q^N$ be a transitive linear code. Then for any $\alpha\in (0,1-1/q)$ we have
$$ \Pr_{c\sim \mathcal{D}(C)}\Big[|c| = \alpha N\Big] \leq q^{-(1-h_q(\alpha)) \textnormal{dim }C},$$
where $\mathcal{D}(C)$ is the uniform distribution over all codewords in $C$, $|c|$ is the number of non-zero coordinates of $c$, and $h_q$ is the q-ary entropy $$h_q(\alpha)= (1-\alpha) \log_q \frac{1}{1-\alpha} + \alpha \log_q\frac{q-1}{\alpha}.$$
\end{theorem}
Note that $h_2(\alpha)$ denotes the binary entropy function. We note that in some regimes (for e.g. when the degree satisfies $0.38n<d<0.499n$ and  $\alpha$ is larger than some constant depending on $d/n$), the bound above improves on all previously proven weight distribution bounds for Reed-Muller codes, even though the only feature of the code that we use is transitivity. See Appendix \ref{aweight} for some details.

\hfill\\
\textbf{II. A Criterion for Decoding on the BSC}
\hfill\\
We develop a new approach for proving decoding results over the BSC, i.e. the communication channel whose errors $z\in\F_2^N$ are sampled from the $\epsilon$-noisy distribution
$$P_\epsilon(z)=\epsilon^{|z|}(1-\epsilon)^{N-|z|}$$
for some $\epsilon\in(0,1)$. 
Our approach is based on Fourier analysis, although unlike \cite{kudekar2016erasure} and \cite{hazla2021polyclose}, the ideas we use do not rely on bounds on influences. 
We obtain the following result (recall that $\mathcal{D}(C^\perp)$ denotes the uniform distribution over $C^\perp$):
\begin{theorem}\label{fouriercriterionunique}
Let $C\subseteq\F_2^N$ be any linear code, and denote by $C^\perp\subseteq\F_2^N$ its dual code. Then for any $\epsilon \in (0,$ $\frac{1}{2})$, there exists a decoding function $d:\F_2^N\rightarrow\F_2^N$ 
such that for all $c\in C$ we have
\begin{align*}
    \Pr_{\rho\sim P_\epsilon}[ d(c+\rho)\neq c]&\leq 2e^{-\frac{\sqrt{N}}{3\epsilon}}+N \mathop{\max}_{\substack{S\subseteq \{\epsilon N\pm N^{3/4}\} \\
    1\leq|S|\leq 2}} \Big\{  \frac{1}{\binom{N}{S}}\E_{c\sim\mathcal{D}(C^\perp)}\big[ K_S(|c|)^2\big]-1 \Big\},
\end{align*}
where $\binom{N}{S}=\sum_{j\in S}\binom{N}{S}$, and where $K_S(x)=\sum_{j\in S}K_j(x)$ for $K_j$ the Krawtchouk polynomial of degree $j$.
\end{theorem}
We will now consider one interesting consequence of Theorem \ref{fouriercriterionunique}. Let $\epsilon\in(0,\frac{1}{2})$ be arbitrary, and define $$A_\epsilon=\{\alpha N:h(\alpha)>1-h(\epsilon)-N^{-1/5}\}.$$
Our next corollary states that whenever the dual codewords of $C$ are distributed sufficiently close to the binomial distribution for all weights in $A_\epsilon$, the code $C$ must be resilient to $\epsilon$-errors. See Appendix \ref{abinomialweight} for the proof.
\begin{corollary}\label{weightunique}
Let $C\subseteq\F_2^N$ be a linear code, and let $\epsilon\in(0,\frac{1}{2})$ be arbitrary.
Suppose that for every $j\in A_\epsilon$ we have
\begin{align*}
    \Pr_{y\sim\mathcal{D}(C^\perp)}\big[|y|=j\big]\leq \big( 1+o(N^{-1})  \big)\frac{\binom{N}{j}}{2^N},
\end{align*}
and suppose that
\begin{align*}
    \Pr_{y\sim\mathcal{D}(C^\perp)}\big[|y|\notin A_\epsilon\big]\leq 2^{N^{\frac{3}{4}}}\cdot \frac{\sum_{i\notin A_\epsilon}\binom{N}{i}}{2^N}.
\end{align*}
Then $C$ is resilient to $\epsilon$-errors.
\end{corollary}
As a proof of concept, we note that a uniformly random linear code of dimension $(1-h(\epsilon))N+\sqrt{N}$ satisfies all these conditions simultaneously with high probability.

\hfill\\
\textbf{III. List Decoding Results}
\hfill\\
Using a generalized version of Theorem \ref{fouriercriterionunique} (namely, Theorem \ref{fouriercriteriongeneral} in section \ref{weightdecoding}), we obtain list decoding bounds for both transitive codes and Reed-Muller codes. We start with our bound for Reed-Muller codes. 
\begin{theorem}\label{rmclose}
Let $\epsilon\in(0,\frac{1}{2})$ and $\gamma\in(0,1)$ be such that $1-\gamma\geq 2^{-\frac{2\epsilon}{(\ln2)^2}}$. Then the Reed-Muller code $\mathsf{RM}(n,d)$ of dimension $\binom{n}{\leq d}=(1-\gamma)N$ can with high probability list-decode $\epsilon$-errors using a list $T$ of size
     $$|T|=2^{(h(\epsilon)-\gamma)N+o(N)}+2^{4\epsilon N+o(N)} .$$
\end{theorem}
Although our lists have exponential size, for small $\epsilon$ the list size is non-trivial, in the sense that it is much smaller than the number of noise vectors (which is about $\binom{N}{\epsilon N}\approx2^{h(\epsilon)N}$) and the number of codewords in the code (which is $2^{\textnormal{dim }C}$). In fact, a standard calculation (see Appendix \ref{alistcapacity}) shows that any code $C\subseteq\F_2^N$ of dimension $(1-\gamma)N$ that can successfully list-decode errors of probability $\epsilon$ with list size $|T|$ must satisfy
\begin{align}\label{lwboundlist}
|T|\gtrsim 2^{(h(\epsilon)-\gamma)N}.
\end{align}
Our bound in Theorem \ref{rmclose} shows that Reed-Muller codes achieve these optimal parameters, at least in some regimes (for e.g. when $\binom{n}{\leq d}\geq 1-\frac{1.99\epsilon}{\ln2}$ and $\epsilon$ is small enough). 
We now turn to our list-decoding bound for transitive codes.
\begin{theorem}\label{thmtransitivelist}
Fix any $\epsilon\in(0,\frac{1}{2})$, $\eta\in(0,1)$, and $N> \left(\frac{5}{\epsilon}\right)^{20}$. Then any  transitive linear code $C\subseteq\F_2^N$ of dimension $\textnormal{dim }C=\eta N$ can with high probability list-decode $\epsilon$-errors using a list $T$ of size
$$|T(x)|=2^{\epsilon N\log(\frac{2}{1-\eta})+o(N)}+2^{4\epsilon N}.$$\end{theorem}
As an explicit example of the types of bounds one gets from Theorem \ref{thmtransitivelist}, we have that any transitive linear code of dimension $\textnormal{dim }C=(1-\frac{4\epsilon}{e})N$
can with high probability list-decode $\epsilon$-errors using a list $T$ of size
$$|T|= 2^{(h(\epsilon) -\epsilon +\frac{\epsilon^2}{\ln2} )N+o(N)}+2^{4\epsilon N}.$$
For comparison, recall that our lower bound (\ref{lwboundlist}) states that any code $C$ of dimension $(1-\frac{4\epsilon}{e})N$ requires a list size of at least about $ 2^{(h(\epsilon)-\frac{4\epsilon}{e})N}$.
\subsection{Techniques}
Our weight distribution bound for transitive linear codes (Theorem \ref{probtransitive}) is proven by showing that the entropy of a uniformly random codeword of weight $\alpha N$ is small. To do this, we analyze the entropy of the coordinates corresponding to linearly independent columns of the generator matrix. Transitivity implies that every coordinate in the code has the same entropy, and subadditivity of entropy can then be used to bound the entropy of the entire distribution.

To obtain our decoding criterion, we make use of a connection between the probability of a decoding error and the $\ell_2$ norm of the coset distribution of the code. To explain the intuition, let us start by assuming that exactly $\epsilon N$ of the coordinates in the codeword are flipped, although our results actually hold over the BSC as well. Let $z$ be the vector in  $\F_2^N$ that represents the errors introduced by the channel, and let $H$ be the parity check matrix of the code. Then by standard arguments, if $z$ can be recovered from $Hz^\intercal$, the codeword can be decoded. In the case where $z$ is uniformly distributed on vectors of weight $\epsilon N$, this amounts to showing that
with high probability, the coset of $z$ does not contain any string of weight $\epsilon N$ (in other words, there is no $w\in\F_2^N$ of weight $|w|=\epsilon N$ such that $Hz^\perp=Hw^\perp$). This can be understood by computing the norm $$\|f\|_2^2 = \frac{1}{2^N}\sum_{y} f(y)^2 = \frac{1}{2^N}\sum_{y} \Pr[Hz^\intercal = y^\intercal]^2,$$where $f(y) = \Pr[Hz^\intercal = y^\intercal]$. The norm above is always at least $2^{-N}\binom{N}{\epsilon N}^{-1}$, and if it is close to $2^{-N}\binom{N}{\epsilon N}^{-1}$ then the code can be decoded with high probability. If $\|f\|_2^2$ is larger than $2^{-N}\binom{N}{\epsilon N}^{-1}$, then we show that the code can be list-decoded with high probability, where the size of the list is related to $2^N\binom{N}{\epsilon N} \|f\|_2^2$.

Thus, to understand decoding, we need to understand $\|f\|_2^2$. Using Fourier analysis, we express this quantity as
\begin{align}\label{introlink}
 \|f\|_2^2 = \sum_{j=0}^N \Pr[|c^\perp| = j] \cdot K_{\epsilon N}(j)^2,
 \end{align}
where $c^\perp$ is a uniformly random codeword in the dual code and $K_{\epsilon N}$ is the Krawtchouk polynomial of degree $\epsilon N$. 
We note that such relations for the coset weight distribution have been used to understand the discrepancy of subsets of the sphere, as well as subsets of other homogeneous spaces. In particular, (\ref{introlink}) was proven in a slightly different form in \cite{2021bargfourierlink} (see Theorem 2.1 and Lemma 4.1), whereas over $\R^N$ results of this type had previously been derived in \cite{bilyk2018stolarskyprinciple,skriganov2019stolarskyhomogeneous}.

Using estimates for the magnitude of Krawtchouk polynomials and bounds for the weight distribution of the dual code $C^\perp$, one can thus bound the norm $\|f\|_2^2$ in the set-up where the error string $z$ is a random vector of weight exactly $\epsilon N$. Using essentially the same techniques, one can also bound the norm $\|f\|_2^2$ when the error string $z$ is a random vector of weight $\approx\epsilon N$, i.e. $z$ is taken uniformly at random from the set $S=\{x\in\F_2^N:|x|=\epsilon N\pm N^{3/4}\}$.

Our next step is then to show that the $\ell_2$ norm corresponding to the $\epsilon$-biased distribution is very similar to the $\ell_2$ norm corresponding to the uniform distribution over $S$. Intuitively, this is because $S$ only contains a very small range of weights, so the $\epsilon$-biased distribution and the uniform distribution must behave very similarly over strings of weight in $S$.
It then follows that their corresponding $\ell_2$ norms must be similar as well.  

Our decoding criteria (Theorem \ref{fouriercriterionunique}, Corollary \ref{weightunique}) are thus obtained by bounding the norm $\|f\|_2^2$ using estimates for Krawtchouk polynomials and for the weight distribution of the dual code $C^\perp$. Our list-decoding results (Theorems \ref{rmclose} and \ref{thmtransitivelist}) then follow from our weight bound for transitive codes (Theorem \ref{probtransitive}) and from a weight bound of Samorodnitsky for Reed-Muller codes (Theorem \ref{previousboundsmall}).

\subsection{Related Work}
It has been shown that LDPC codes achieve capacity over  Binary Memoryless Symmetric Channels (BMS) \cite{luby1997ldpc2,kudekar2013ldpc,gallager1962ldpc}, which
includes both the BSC and the BEC. These constructions are not deterministic, and it is only with the advent of polar codes \cite{arikan2009polar} that we obtained  capacity-achieving codes with both a deterministic constructions and efficient encoding and decoding algorithms.

Polar codes are closely related to Reed-Muller codes, in the sense that they also consist of subspaces that correspond to polynomials over $\F_2 \cite{arikan2009polar}$. In \cite{arikan2009polar} it was shown that Polar codes achieve capacity over the BSC, and algorithms were given to both encode and decode them.

It has long been believed that Reed-Muller codes achieve capacity, and significant progress has been made in that direction over the last few years. (See \cite{abbe2021survey} for a discussion on the subject, as well as a thorough exposition to Reed-Muller codes). Abbe, Shpilka and Wigderson first showed that Reed-Muller codes achieve capacity over the BSC and the BEC for sub-constant and super-constant rates \cite{abbe2015RMlowrate}. Kudekar, Kumar, Mondelli, Pfister, Sasoglu and Urbanke then proved that in the constant rate regime, Reed-Muller codes achieve capacity over the BEC channel \cite{kudekar2016erasure}. Abbe and Ye showed that the Reed-Muller transform polarizes the conditional mutual information, and proved that some non-explicit variant of the Reed-Muller code achieves capacity \cite{abbe2019rmpolarize}. (They conjecture that this variant is in fact the Reed-Muller code itself). Hazla, Samorodnitsky and Sberlo then proved that Reed-Muller codes of constant rates can decode a constant fraction of errors on the BSC \cite{hazla2021polyclose}; this had previously been shown for Almost-Reed-Muller codes by Abbe, Hazla and Nachum \cite{abbe2020almostRM}. Most recently, Reeves and Pfister showed that Reed-Muller codes achieve capacity over all BMS channels under bit-MAP decoding \cite{reeves2021bitcapacity}, i.e. that one can with high probability recover any single bit of the original codeword (but not with high enough probability that one could take a union bound). Despite these breakthroughs, the conjecture that Reed-Muller codes achieve capacity over all BMS channels under block-MAP decoding (i.e. recover the whole codeword with high probability) is ultimately still open.

\hfill\\
\textbf{Weight Bounds for Reed-Muller Codes}
\hfill\\
Several past works have proven bounds on the weight distribution of Reed-Muller codes. Kaufman, Lovett and Porat gave asymptotically tight bounds on the weight distribution of Reed-Muller codes of constant degree \cite{kaufman2012constantdegree}. Abbe, Shpilka and Wigderson then built on these techniques to obtain bounds for all degrees smaller than $\frac{n}{4}$ \cite{abbe2015RMlowrate}, before Sberlo and Shpilka again improved the approach and obtained bounds for all degrees \cite{sberlo2020weightbound}. Most recently, Samorodnitsky used completely different ideas to obtain weight bounds in the regime where both the rate of the code and the normalized weight of the codeword are $\Theta(1)$ \cite{samorodnitsky2020weightboundhalf}. We will later use his following result in our list-decoding arguments:
\begin{theorem}[\cite{samorodnitsky2020weightboundhalf}]\label{previousboundsmall}
Let $\binom{n}{\leq d}=\eta 2^n=\eta N$ for some $\eta\in(0,1)$, and denote by $\mathcal{D}(n,d)$ the uniform distribution over all codewords in $\mathsf{RM}(n,d)$. Then for any $\alpha\in(0, \frac{1}{2})$ we have
$$\Pr_{c\sim \mathcal{D}(n,d)}[|c|\leq \alpha N ]\leq 2^{o(N)}\left(\frac{1}{1-\eta}\right)^{2\ln2 \cdot \alpha N} 2^{-\eta N}.$$
\end{theorem}
These bounds are strong when $\alpha \ll 1/2$. For $\alpha$ close to $1/2$, the first results we are aware of are due to Ben-Eliezer, Hod and Lovett \cite{ben-eliezer2012weighthalf1}. Their bounds, which were extended to Reed-Muller codes over prime fields by Beame, Oveis Gharan and Yang \cite{beame2020weightodd}, are strongest when the degree is sublinear. Sberlo and Shpilka then obtained bounds for all degrees in \cite{sberlo2020weightbound}, while Samorodnitsky again obtained bounds in the regime where both $\alpha$ and $\eta$ are $\Theta(1)$ \cite{samorodnitsky2020weightboundhalf}.

We note that in some regimes (for e.g. when the degree satisfies $0.38n<d<0.499n$ and $\alpha$ is larger than some constant depending on $d/n$), our Theorem \ref{probtransitive} improves on all the aforementioned weight bounds. See Appendix \ref{aweight} for some details.

\hfill\\
\textbf{List Decoding}
\hfill\\
List decoding was proposed by Elias in 1957 as an alternative to unique decoding \cite{elias1957firstlist}. In the list decoding framework, the receiver of a corrupted codeword is asked to output a list of potential codewords, with the guarantee that with high probability one of these codewords is the original one. This of course allows for a greater fraction of errors to be tolerated.

The list decoding community has largely focused on proving results for the adversarial noise model, and many codes are now known to achieve list-decoding capacity. For example uniformly random codes achieve capacity, as do uniformly random linear codes \cite{guruswami2002listlinear1,Li2018listlinear2,Guruswami2011listlinear3}. Folded Reed-Solomon codes were the first explicit codes to provably achieve list-decoding capacity \cite{Guruswami2008explicitlist}, followed by several others a few years later \cite{Guruswami2012explicitlist2,Kopparty2015explicitlist3,Hemenway2017explicitlist4,Mosheiff2020explicitlist5}. 
For the rest of this paper however, we will exclusively work in the model where the errors are stochastic. 
In this model, the strongest known list decoding bound for the code $\mathsf{RM}(n,d)$ with $\binom{n}{\leq d}=\eta N>N-N\log(1+2\sqrt{\epsilon(1-\epsilon)})$ is, to our knowledge, that one can output a list $T$ of size
\begin{align}\label{previouslistresult}
    |T|=2^{\epsilon N\log\frac{4\epsilon(1-\epsilon)}{(1-\eta)^{4\ln2}}+o(N)}
\end{align}
and succeed with high probability in decoding $\epsilon$-errors. This result, although not
explicitly stated in \cite{samorodnitsky2020weightboundhalf}, can be obtained from his weight bound of Theorem \ref{previousboundsmall} by bounding the
expected number of codewords that end up closer to the received string than the original codeword, and then applying Markovâ€™s inequality. We note that the expression in (\ref{previouslistresult}) stays strictly below the optimal size of $2^{h(\epsilon)N-(1-\eta)N+o(N)}$ (see Appendix \ref{acomparelist} for a proof of this).

\hfill\\
\textbf{Krawtchouk polynomials}
\hfill\\
The Krawtchouk polynomial of degree $s$ is the polynomial
$$K_s(x)=\sum_{j=0}^s(-1)^j\binom{x}{j}\binom{N-x}{s-j}.$$ For any subset $S\subseteq \{0,...,N\}$, we will be interested in the corresponding polynomial $K_S(x):=\sum_{s\in S}K_s(x)$. For $v\in\F_2^N$, we will sometimes abuse notation and use $K_s(v)$ to mean $K_S(|v|)$.
The following proposition follows from standard results (see for instance \cite{1999surveykrawtchouk}, or Theorem 16 in \cite{1977bookkrawtchouk}). 
\begin{prop}\label{IFourier}
For any $N$ and any $S\subseteq\{1,...,N\}$, we have
   $$\frac{2^{-N}}{\sum_{s\in S}\binom{N}{s}}\sum_{j = 0}^N  \binom{N}{j}  K_S(j)^2=1.$$
\end{prop}
Good estimates for Krawtchouk polynomials of any degree were obtained in \cite{kallai1995krawtchouk1,ismail1998krawtchouk2,polyanskiy2019krawtchouk3} (see for e.g. \cite{polyanskiy2019krawtchouk3}, Lemma 2.1). These estimates are asymptotically tight in the exponent. Note that $|K_s(x)|=| K_s(N-x)|= |K_{N-s}(x)|$, so it suffices to understand the case $x,s\leq\frac{N}{2}$.
\begin{theorem}[\cite{kallai1995krawtchouk1,ismail1998krawtchouk2,polyanskiy2019krawtchouk3}]\label{thmkrawtchouktight}
Let $\epsilon,\delta\in(0,\frac{1}{2})$ be arbitrary. If $\delta\geq\frac{1}{2}- \sqrt{\epsilon(1-\epsilon)}$, then
$$|K_{\epsilon N}(\delta N)|\leq 2^{(1+h(\epsilon)-h(\delta))\frac{N}{2}}.$$
If $\delta<\frac{1}{2}- \sqrt{\epsilon(1-\epsilon)}$, define $\omega=\frac{1-2\delta-\textnormal{sgn}(1-2\delta)\sqrt{(1-2\delta)^2-4\epsilon(1-\epsilon)}}{2(1-2\delta)}.$ Then
$$|K_{\epsilon N}(\delta N)|\leq \frac{(1-\omega)^{\delta N}(1+\omega)^{(1-\delta)N}}{\omega^{\epsilon N}}.$$
\end{theorem}
As the second expression can be somewhat cumbersome to use, \cite{polyanskiy2019krawtchouk3} also gives the following weaker bound (see Lemma 2.2 and equation 2.10 in \cite{polyanskiy2019krawtchouk3}):
\begin{theorem}[\cite{polyanskiy2019krawtchouk3}]\label{thmkrawtchoukbound}
For any $\epsilon \in(0,\frac{1}{2})$ and any $\delta<\frac{1}{2}\sqrt{\epsilon(1-\epsilon)}$, we have
$$|K_{\epsilon N}(\delta N)|\leq 2^{h(\epsilon)N+\epsilon N\log(1-2\delta)}.$$
\end{theorem}
We will need the above estimates when using our Theorem \ref{fouriercriterionunique} to obtain list-decoding results for transitive codes and Reed-Muller codes.

\section{Notation, Conventions and Preliminaries}\label{prelim}

For the sake of conciseness, we will use the notation
$$\{a\pm l\}=\{a-l,...,a+l\},$$
the notation 
$$\binom{n}{\leq d} =\binom{n}{0} + \binom{n}{1} + \dotsb + \binom{n}{d},$$
and for $S\subseteq\{0,...,N\}$ the notation
$$\binom{N}{S}=\sum_{s\in S}\binom{N}{s}.$$


Let $N = 2^n$. 
We will be working with the vector spaces $\F_2^n$ and $\F_2^N$. For convenience, we associate $\F_2^n$ with the set $[N] = \{1,2,\dotsc, N\}$, by ordering the elements of $\F_2^n$ lexicographically. 
For $x \in \F_2^N$, we write $|x| = |\{j \in [N]: x_j=1\}|$ to denote the weight of $x$. 

\subsection{Linear Codes}
An $N$-bit code is a subset $C\subseteq\mathbb{F}_2^N$. Whenever $C$ is a subspace of $\mathbb{F}_2^N$, we say that $C$ is a \emph{linear} code. Any linear code $C\subseteq\mathbb{F}_2^N$ can be represented by its generator matrix, which is a $\dim\textnormal{ C}\times N$ matrix $G$ whose rows form a basis for $C$. The matrix $G$ generates all codewords of $C$ in the sense that
$$C=\{vG:v\in\mathbb{F}_2^{\textnormal{dim }C}\}.$$
Another useful way to describe a linear code $C\subseteq \mathbb{F}_2^N$ is via its parity-check matrix, which is an $(N-\dim\textnormal{ C})\times N$ matrix $H$ whose rows span the orthogonal complement of $C$. The linear code $C$ can then be expressed as 
$$C=\{c\in\mathbb{F}_2^N:Hc^\intercal=0\}.$$
One property that will play an important role is transitivity, which we define below:
\begin{definition}\label{deftransitive}
A set $C\subseteq \mathbb{F}_2^N$ is transitive if for every $i,j\in[N]$ there exists a permutation $\pi:[N]\rightarrow [N]$ such that \begin{enumerate}[label=(\roman*)]
    \item $\pi(i)=j$
    \item For every element $v=(v_1,...,v_N)\in C$ we have $(v_{\pi(1)},...v_{\pi(N)})\in C$
\end{enumerate}
\end{definition}
We note that the dual code of a transitive code is also transitive (see Appendix \ref{adualtransitive} for the proof).
\begin{claim}\label{dualtransitive}
The dual code $C^\perp$ of a transitive code $C\subseteq\F_2^N$ is transitive.
\end{claim}

\subsection{Reed-Muller Codes} 
We will denote by $\mathsf{RM}(n,d)$ the  Reed-Muller code with $n$ variables and degree $d$. Throughout this section, we let $M$ be the generator matrix of $\mathsf{RM}(n,d)$; this is an $ \binom{n}{\leq d}\times N$ matrix whose rows correspond to sets of size at most $d$, ordered lexicographically, and whose columns correspond to elements of $\F_2^n$. For $S \subseteq [n], |S| \leq d$ and $x \in \F_2^n$, the corresponding entry is $M_{S,x}=\prod_{j \in S} x_j$. If $S$ is empty, this entry is set to $1$.

If $v \in \F_2^{\binom{n}{\leq d}}$ is a row vector, $v$ can be thought of as describing the coefficients of a multilinear polynomial in $\F_2[X_1, \dotsc, X_n]$ of degree at most $d$. The row vector $vM$ is then the evaluations of this polynomial on all inputs from $\F_2^n$. 
It is well known that $M$ has full rank, $\binom{n}{\leq d}$. In fact we have the following standard fact (see Appendix \ref{afullrank} for the proof):
\begin{fact}\label{fullrank} The columns of $M$ that correspond to the points $x \in \F_2^n$ with $|x| \leq d$ are linearly independent.
\end{fact}
The parity-check matrix of the Reed-Muller code is known to be the same as the generator matrix of a different Reed-Muller code. Namely, let $H$ be the $ \binom{n}{\leq n-d-1}\times N$ generator matrix for the code $\mathsf{RM}(n,n-d-1)$. Then $H$ has full rank, and $M H^\intercal  = 0$. So, the rows of $H$ are a basis for the orthogonal complement of the span of the rows of $M$. 
Reed-Muller codes also have useful algebraic features, notably transitivity:
\begin{fact}\label{transitive}
For all $n$ and all $d\leq n$, the Reed-Muller code $\mathsf{RM}(n,d)$ is transitive.
\end{fact}
See Appendix \ref{afullrank} for the proof.

 \subsection{Entropy}
The binary entropy function $h:[0,1] \rightarrow \R$ is defined to be
$$h(\epsilon) = \epsilon \cdot \log\frac{1}{\epsilon} + (1-\epsilon) \cdot \log\frac{1}{1-\epsilon}.$$

The following fact allows us to approximate binomial coefficients using the entropy function:
\begin{fact} \label{stirling} For $n/2 \geq d \geq 1$,
$\sqrt{\frac{8\pi }{e^4 n}} \cdot 2^{h(d/n) \cdot n} \leq \binom{n}{d} \leq \binom{n}{\leq d} \leq 2^{h(d/n) \cdot n}.$
\end{fact}
The leftmost inequality is a consequence of Stirling's approximation for the binomial coefficients, and the rightmost is a consequence of the sub-additivity of entropy.

The following lemma, which is essentially a 2-way version of Pinsker's inequality, gives a useful way to control the entropy function near $1/2$. 
\begin{lemma}\label{pinsker}
For any $\mu\in(0,1)$, we have $$\frac{\mu^2}{2\ln2}\leq1-h\left(\frac{1-\mu}{2}\right)\leq\mu^2.$$
\end{lemma}
See Appendix \ref{apinsker} for the proof.

\subsection{Probability Distributions}
There are two types of probability distributions that we will use frequently. The first one is the $\epsilon$-Bernoulli distribution over $\mathbb{F}_2^N$, which we will denote by
$$P_\epsilon (z)=\epsilon^{|z|}(1-\epsilon)^{N-|z|}.$$
The second one is the uniformly random distribution over some set $T$, which we will denote by
$$\mathcal{D}(T)(z)=\begin{cases}
\frac{1}{|T|} & \text{if $z\in T$,}\\
0 & \text{otherwise.}
\end{cases}.$$
There are two particular cases for the uniform distribution that will occur often enough that we attribute them their own notation. The first one is the uniform distribution over $\mathbb{F}_2^t$, which we will denote by
$$\mu_t=\mathcal{D}(\mathbb{F}_2^t).$$
The second one is the uniform distribution over all vectors $z\in\mathbb{F}_2^N$ of weight $|z|\in S$, for some $S\subseteq\{0,...,N\}$. We will denote this probability distribution by 
$$\lambda_{S}=\mathcal{D}(\{z\in\mathbb{F}_2^N:|z|\in S\}).$$
 
\subsection{Probability Theory} 
We will need two very standard results of probability theory (see for e.g. \cite{theorybook}): Markov's inequality and Chernoff's bound. We start with Markov's inequality.
\begin{lemma}\label{markov}
Let $X$ be a nonnegative random variable. Then for any $a>0$, we have
$$\Pr[X\geq a]\leq \frac{\E[X]}{a}.$$
\end{lemma}
 We will also need Chernoff's bound:
 \begin{lemma}\label{chernoff}
 Let $X_1,...,X_n$ be i.i.d. random variables taking values in $\{0,1\}$, and define $X=X_1+...+X_n$. Then for any $\delta\in (0,1)$, we have
 $$\Pr\Big[\big|X-\E[X]\big|>\delta\cdot n \E[X_1]\Big]\leq 2e^{-\frac{\delta^2 \cdot n\E[X_1]}{3}}.$$
 \end{lemma}
\subsection{Fourier Analysis}\label{sectionfourier}
The Fourier basis is a useful basis for the space of functions mapping $\F_2^N$ to the real numbers. We recall some of its properties below (see for e.g. \cite{2008wolffourier}). For $f,g \in \F_2^N\rightarrow \R$, define the inner product $$ \langle f, g \rangle = \frac{1}{2^N}\sum_{x \in \F_2^N} f(x) g(x).$$
For every $x,y \in \F_2^N$, define the character
$$ \chi_y(x) = (-1)^{\sum_{j=1}^N  x_j y_j }.$$ These functions form an orthonormal basis, namely for $y,y' \in \F_2^N$,
\begin{align*}
\langle \chi_y, \chi_{y'} \rangle = \begin{cases}
1 & \text{if $y=y'$,}\\
0 & \text{otherwise.}
\end{cases}
\end{align*}
We define the Fourier coefficients $\hat{f}(y) = \langle f, \chi_y \rangle$. Then for $f,g : \F_2^N \rightarrow \R$, we have 
$$\langle f, g \rangle = \sum_{y \in \F_2^N} \hat{f}(y) \cdot \hat{g}(y).$$
In particular, $$\|f \|^2_2 = \langle f, f \rangle = \sum_y \hat{f}(y)^2.$$ 


\section{Outline of the Paper}
The main question we will be looking into is whether or not a family of list-decoding codes $\{C_N\}$, with $C_N\subseteq\mathbb{F}_2^N$, is asymptotically resilient to independent errors of probability $\epsilon$. Formally, we are given a list size $k=k(N)$ and want to know if there exists a family of decoding functions $\{d_N\}$, with $d_N:\mathbb{F}_2^N\rightarrow \left(\mathbb{F}_2^N\right)^{\otimes k}$, such that for every sequence of codewords $\{c_N\}$ we have
\begin{align*}
    \lim_{N\rightarrow \infty}\Pr_{\rho_N\sim P_\epsilon} \big[c_N \notin d_N(c_N+\rho_N)\big] = 0.
\end{align*}
We note that the unique decoding problem can be seen as setting $k=1$ in the above set-up. 
Our general approach will be based on trying to identify the error string $\rho\in\F_2^N$ from its image $H\rho^\intercal$. In particular, we will be interested in the max-likelihood decoder
\begin{align}\label{maxdecoderlist}
D_k(x)&=\mathop{\textnormal{argmax}}_{\substack{\{z_1,...,z_k\}\subseteq \mathbb{F}_2^N\\{Hz_i}^\intercal=x \textnormal{ for all }i}}\{P_\epsilon(z_1)+...+P_\epsilon(z_k)\} \nonumber \\ 
&=\mathop{\textnormal{argmin}}_{\substack{\{z_1,...,z_k\}\subseteq \mathbb{F}_2^N\\Hz_i^\intercal=x \textnormal{ for all }i}}\{|z_1|+...+|z_k|\}.
\end{align}

We show in the following lemma that if the max-likelihood decoder is able to identify the error string $\rho$, then it is possible to recover the original codeword. 
\begin{lemma}\label{eqvltdecoder}
Let $H$ be the $t\times N$ parity-check matrix of the linear code $C$, and let $D:\mathbb{F}_2^t\rightarrow \left(\mathbb{F}_2^N\right)^{\otimes{k}}$
be an arbitrary function. Then there exists a decoding function $$d:\mathbb{F}_2^N\rightarrow\left(\mathbb{F}_2^N\right)^{\otimes{k}}$$ such that for every $c\in C$ we have
\begin{align*}
\Pr_{\rho\sim P_\epsilon}[c\notin d(c+\rho)]\leq \Pr_{\rho\sim P_\epsilon}[\rho\notin D(H\rho^\intercal)].
\end{align*}
\end{lemma}
\begin{proof}
Given $D:\mathbb{F}_2^t\rightarrow \left(\mathbb{F}_2^N\right)^{\otimes{k}}$, define $d:\mathbb{F}_2^N\rightarrow\left(\mathbb{F}_2^N\right)^{\otimes{k}}$ to be
$$d(z)=\{z+y:y\in D(Hz^\intercal)\}.$$
We will show that whenever $\rho$ satisfies $\rho\in D(H\rho^\intercal)$, $\rho$ also satisfies $c\in d(c+\rho)$ for every $c\in C$. Suppose $\rho\in D(H\rho^\intercal)$. Note that since $H$ is the parity-check matrix of $C$, every $c\in C$ satisfies $Hc^\intercal=0$. So for every $c\in C$, any $\rho $ that satisfies $\rho\in D(H\rho^\intercal)$ must also satisfy $\rho\in D(H(c^\intercal+\rho^\intercal))$. It then follows by definition of $d(c+\rho)$ that 
\begin{align*}
    c=c+\rho+\rho\in d(c+\rho).
\end{align*}
\end{proof}
From this point onward, our goal will thus be to prove that the max-likelihood decoder in (\ref{maxdecoderlist}) succeeds in recovering $\rho$ with high probability. 
In section \ref{collisionsdecoding}, we relate the decoding error probability of the max-likelihood decoder $D_k$ to the collision probability $$\sum_{x\in\mathbb{F}_2^t}\Pr[Hz^\intercal=x]^2.$$ 
In section \ref{weightdecoding}, we build on this result to obtain a bound on the performance of $D_k$ in terms of the weight distribution of the dual code.
We then present new bounds on the weight distribution of transitive codes in section \ref{weighttransitive}. These bounds are interesting in their own right, and we show that they are essentially tight.
In section \ref{listtransitive}, we combine these bounds with our results from section \ref{weightdecoding} to obtain list-decoding results for transitive linear codes. We then repeat this argument with Samorodnitsky's Theorem \ref{previousboundsmall} in section \ref{listrm} to obtain a stronger list-decoding bound for Reed-Muller codes. 

\section{Collisions vs Decoding}\label{collisionsdecoding}
Recall that we denote by $P_\epsilon$ the $\epsilon$-Bernoulli distribution over $\mathbb{F}_2^N$, i.e. the distribution
$$P_\epsilon (z)=\epsilon^{|z|}(1-\epsilon)^{N-|z|}.$$
Recall also that for any subset $S\subseteq \{0,...,N\}$, we denote by $\lambda_{S}$ the uniform distribution over all strings $z\in\mathbb{F}_2^N$ of weight $|z|\in S$, i.e.
$$\lambda_{S}(z)=\begin{cases}
\frac{1}{\sum_{j\in S} \binom{N}{j}} & \text{if $|z|\in S$,}\\
0 & \text{otherwise.}
\end{cases}$$
The goal of this section will be to analyze the relationship between the decoding of an error string $\rho\in\mathbb{F}_2^N$ and the collision probability of strings $z\in\mathbb{F}_2^N$ within the map $z\mapsto Hz^\intercal$. Intuitively, the more collisions there are within this mapping, the harder it is for our decoder to correctly identify the error string $\rho\in\mathbb{F}_2^N$ upon seeing only its image $H\rho^\intercal\in\mathbb{F}_2^t$. However, certain error strings might be unlikely enough to occur that our decoder can safely ignore them. For example, if we are interested in an $\epsilon$-noisy error string $\rho$, then $\rho$ is unlikely to have weight $|\rho|$ far away from $\epsilon N$. We could thus choose to ignore all strings whose weights do not lie in the set $S=\{\epsilon N-l,...,\epsilon N+l\}$, for some integer $l$. In order to analyze the collisions that occur when strings are required to have weight $z\in S$, we define for every $z\in\mathbb{F}_2^N$ and every $S\subseteq \{0,...,N\}$ the set of $S$-colliders of $z$, i.e. the set of strings $y$ that lie in the coset of $z$ and have weight $|y|\in S$:
\begin{definition}
For any $z\in\mathbb{F}_2^N$ and any subset $S\subseteq\{0,...,N\}$, define 
$$\Omega_z^S=\left\{y\in \mathbb{F}_2^N :|y|\in S\textnormal{ and } Hy^{\intercal}=Hz^\intercal   \right\}.$$
\end{definition}
This definition captures a natural parameter for how large of a list we need before we can confidently claim that it contains the error string: if we are given $H\rho^\intercal$ and are told that with high probability the error string $\rho$ has weight $|\rho|\in S$, then we should output the list $\Omega_\rho^S$. For unique decoding we want to argue that $|\Omega_\rho^S|=1$ with high probability, whereas for list decoding we want to argue that $|\Omega_\rho^S|\leq k$ with high probability, for some integer $k>1$. The expectation of $|\Omega_\rho^S|$ will thus be a key quantity in our analysis. We will call this expectation the "collision count," because it will later be useful to interpret it as the renormalized collision probability of the map $z\mapsto Hz^\intercal$ (see for instance the proof of Proposition \ref{collisionprob}).
\begin{definition}
For any subset $S\subseteq \{0,..,N\}$ and any $t\times N$ matrix $H$, define
\begin{align*}
\mathsf{Coll}_H(S)&=\E_{z\sim \lambda_S}\big[|\Omega_z^S|\big].
\end{align*}
\end{definition}
%The collision count of $S$ can be seen as a measure of injectivity for the map $z\mapsto Hz^\intercal$ over the domain $\{z\in\mathbb{F}_2^N:|z|\in S\}$. When this map is injective, we have $\mathsf{Coll}_H(S)=1$. When the map is not injective, we have $\mathsf{Coll}_H(S)>1$, and $\mathsf{Coll}_H(S)$ increases as the number of collisions increase (i.e. it is larger when the map $z\mapsto Hz^\intercal$ is "further away" from being injective). For a uniformly random error string $\rho$ of weight $|\rho|\in S$, we have the following relationship between the collision count $\mathsf{Coll_H}(S)$ and the list size $|\Omega_\rho^S|$:
In the following lemma, we use Markov's inequality to bound the probability of a list decoding error in terms of $\mathsf{Coll}_H(S)$.
\begin{lemma}\label{collisionuniform}
For any subset $S\subseteq \{0,...,N\}$, any matrix $H$ with $N$ columns, and any integer $k\geq 1$, we have
\begin{align*}
  \Pr_{\rho\sim \lambda_{S}}\big[|\Omega_\rho^S|>k\big]\leq\frac{ \mathsf{Coll_H}(S) -1}{k}.
\end{align*}
\end{lemma}
\begin{proof}
Note that $|\Omega_z^S|\geq 1$ for any $z\in\F_2^N$ with weight $|z|\in S$, so the random variable $|\Omega_z^S|- 1$ is always non-negative. Applying Markov's inequality (i.e. Lemma \ref{markov}), we then have
\begin{align*}
  \Pr_{\rho\sim \lambda_{S}}\big[|\Omega_\rho^S|>k\big]&=\Pr_{\rho\sim \lambda_{S}}\big[|\Omega_\rho^S|-1\geq k\big]\\
  &\leq\frac{ \mathsf{Coll_H}(S) -1}{k}.
\end{align*}
\end{proof}
When the error string $\rho$ is sampled uniformly at random from the set $\{z\in\mathbb{F}_2^N:|z|\in S\}$, the above lemma allows us to relate the decoding error probability to the collision count $\mathsf{Coll_H}(S)$. The problem we are most interested in, however, is when $\rho$ is sampled not from some uniform distribution, but from the $\epsilon$-noisy probability distribution $P_\epsilon$. We will now show how to connect these two decoding problems. The intuition is that
by the Chernoff bound, we only need to concern ourselves with strings whose weights lie in $S=\{\epsilon N\pm l\}$, for some appropriately chosen $l$. But in this weight band all strings have similar weight, and so are given similar probability under the distribution $P_\epsilon$. Intuitively, the $P_\epsilon$-decoder must then perform very similarly to the $\lambda_S$-decoder. The following theorem makes this idea precise, and then uses Lemma \ref{collisionuniform} to bound the probability of a decoding error. Recall that $D_k:\F_2^t\rightarrow(\F_2^N)^{\otimes k}$ is the max-likelihood decoder $$D_k(x)=\mathop{\textnormal{argmin}}_{\substack{\{z_1,...,z_k\}\subseteq \mathbb{F}_2^N\\Hz_i^\intercal=x \textnormal{ for all }i}}\{|z_1|+...+|z_k|\}.$$

\begin{theorem}\label{collisiondecoding}
Fix $\epsilon <\frac{1}{2}$, let $H$ be any matrix with $N$ columns, and let $k=(2l+1)m +1$ for some integers $m \geq0$ and $l\leq (\frac{1}{2}-\epsilon)N$. Then
\begin{align*}
    \Pr_{\rho\sim P_\epsilon}[\rho\notin D_k(H\rho^\intercal)]&\leq 2e^{-\frac{l^2}{3\epsilon N}} +\frac{4(l+1)}{k} \mathop{\max}_{\substack{S\subseteq \{\epsilon N\pm l\} \\
    1\leq|S|\leq 1+\mathbbm{1}\{k=1\}}} \Big\{ \mathsf{Coll_H}(S)-1 \Big\},
\end{align*}
where $\mathbbm{1}\{k=1\}$ is $1$ when $k=1$ and $0$ otherwise
\end{theorem}



\begin{proof}
We will consider the unique decoding case ($k=1$, i.e. $m=0$) and the list-decoding case ($k>1$, i.e. $m\in\N$) separately.
\hfil\\
\textbf{Case 1: Unique decoding, i.e. $k=1$} 
\hfil\\
Let $t$ be the number of rows in the matrix $H$. We will show that a slightly less performant decoder $\tilde{D}_1:\mathbb{F}_2^t\rightarrow \mathbb{F}_2^N$ satisfies the desired probability bound. We define $\tilde{D}_{1}$ as follows: upon receiving input $x\in\mathbb{F}_2^t$, $\tilde{D}_1$ outputs the minimum-weight string from the set $\{z\in\F_2^N:Hz^\perp=x,|z|=\epsilon N\pm l\}.$
If there is no such string, the decoder fails. It is clear that
$$\Pr_{\rho\sim P_\epsilon}[ \rho\neq D_{1}(H\rho^\intercal)]\leq \Pr_{\rho\sim P_\epsilon}[ \rho\neq \tilde{D}_1(H\rho^\intercal)],$$
since $D_1$ always returns the most likely string whereas $\tilde{D}_1$ may not.
We thus turn to proving the desired bound for $\tilde{D}_{1}$. Letting
$$B=\{z\in\mathbb{F}_2^N:\big||z|-\epsilon N\big|>l\}\},$$
we have by Chernoff's bound (i.e. Lemma \ref{chernoff}) that
\begin{align}\label{chernoffonk1}
    \Pr_{\rho\sim P_\epsilon}[ \rho\neq \tilde{D}_{1}(H\rho^\intercal)]&\leq \Pr_{\rho\sim P_\epsilon}[\rho\in B]+\Pr_{\rho\sim P_\epsilon}[ \rho\neq \tilde{D}_{1}(H\rho^\intercal)\big|\rho\notin B]\nonumber\\
    &\leq 2e^{-\frac{l^2}{3\epsilon N}} +\Pr_{\rho\sim P_\epsilon}[ \rho\neq \tilde{D}_{1}(H\rho^\intercal)\big|\rho\notin B].
\end{align}
We want to bound the second term. For any $\rho\notin B$, we define the set of "problematic weights" $S(\rho)=\{\epsilon N-l,...,|\rho|\}$ . We note that for $\rho\notin B$, our decoder $\tilde{D}_{1}$ can only fail if there is some string $z\neq \rho$ satisfying $Hz^\perp=H\rho^\perp$ and $|z|\in S(\rho)$. Recalling the definition $\Omega_\rho^S=\{z:Hz^\perp=H\rho^\perp, |z|\in S\}$, we can then rewrite our equation (\ref{chernoffonk1}) as
\begin{align*}    
    \Pr_{\rho\sim P_\epsilon}[ \rho\neq \tilde{D}_{1}(H\rho^\intercal)]&\leq 2e^{-\frac{l^2}{3\epsilon N}}+\Pr_{\rho\sim P_\epsilon}\big[ |\Omega_\rho^{S(\rho)}|>1 \big|\rho\notin B \big].
\end{align*}
Considering the most problematic weight level $w$ within the region $\{\epsilon N\pm l\}$ and using a union bound over all lower levels $w'\leq w$, we get
\begin{align*}
    \Pr_{\rho\sim P_\epsilon}[ \rho\neq \tilde{D}_{1}(H\rho^\intercal)]&\leq 2e^{-\frac{l^2}{3\epsilon N}}+\max_{w\in\{\epsilon N\pm l\}}\left\{\Pr_{\rho\sim P_\epsilon}\big[ |\Omega_\rho^{S(\rho)}|>1 \big||\rho|=w \big]\right\}\\
    &\leq 2e^{-\frac{l^2}{3\epsilon N}}+(2l+1)\mathop{\max}_{\substack{w,w'\in\{\epsilon N\pm l\}\\w'\leq w}}\left\{\Pr_{\rho\sim P_\epsilon}\big[ |\Omega_\rho^{\{w,w'\}}|>1\big||\rho|=w \big]\right\}.
\end{align*}
We now note that under the condition $|\rho|=w$, the probability distributions $P_\epsilon(\rho)$ and $\lambda_{w,w'}(\rho)$ are identical (they are both uniform on strings of weight $w$). We can thus rewrite our last inequality as
\begin{align*}
    \Pr_{\rho\sim P_\epsilon}[ \rho\neq \tilde{D}_{1}(H\rho^\intercal)]    & \leq 2e^{-\frac{l^2}{3\epsilon N}}+(2l+1)\mathop{\max}_{\substack{w,w'\in\{\epsilon N\pm l\}\\w'\leq w}}\left\{\Pr_{\rho\sim \lambda_{w,w'}}\big[ |\Omega_\rho^{\{w,w'\}}|>1\big||\rho|=w \big]\right\}.
\end{align*}
But by basic conditional probability we know that
$$\Pr_{\rho\sim \lambda_{w,w'}}\big[ |\Omega_\rho^{\{w,w'\}}|>1 \big]\geq \Pr_{\rho\sim \lambda_{w,w'}}\big[ |\rho|=w \big]\cdot \Pr_{\rho\sim \lambda_{w,w'}}\big[ |\Omega_\rho^{\{w,w'\}}|>1\big||\rho|=w \big],$$
so we can bound our previous expression by
\begin{align*}
    \Pr_{\rho\sim P_\epsilon}[ \rho\neq \tilde{D}_{1}(H\rho^\intercal)]    &\leq 2e^{-\frac{l^2}{3\epsilon N}}+(2l+1)\mathop{\max}_{\substack{w,w'\in\{\epsilon N\pm l\}\\w'\leq w}}\left\{\frac{\Pr_{\rho\sim \lambda_{w,w'}}\big[ |\Omega_\rho^{\{w,w'\}}|>1 \big]}{\Pr_{\rho\sim \lambda_{w,w'}}\big[ |\rho|=w \big]}\right\}.
\end{align*}
Now for any $w<\frac{N}{2}$ and $w'\leq w$, we have $\Pr_{\rho\sim \lambda_{\{w,w'\}}}\big[ |\rho|=w \big]=\frac{\binom{N}{w}}{\binom{N}{\{w,w'\}}}\geq\frac{\binom{N}{w}}{\binom{N}{w}+\binom{N}{w'}}\geq \frac{1}{2}$. It then follows that
\begin{align*}
    \Pr_{\rho\sim P_\epsilon}\big[ \rho\notin \tilde{D}_1(H\rho^\perp) \big]\leq  2e^{-\frac{l^2}{3\epsilon N}}+      2(2l+1)\cdot \mathop{\max}_{\substack{S\subseteq \{\epsilon N\pm l\} \\
    |S|\in\{1,2\}}} \left\{\Pr_{\rho\sim \lambda_{S}}\big[\Omega_\rho^{S}>1  \big]\right\}.
\end{align*}
The theorem statement then follows from Lemma \ref{collisionuniform}.







\hfil\\
\textbf{Case 2: List decoding, i.e. $k>1$} 
\hfil\\
Let $t$ be the number of rows in the matrix $H$. We will show that a slightly less performant decoding function $D_{k,l}:\mathbb{F}_2^t\rightarrow (\mathbb{F}_2^N)^{\otimes k}$ satisfies the desired probability bound. We define $D_{k,l}$ as follows: upon receiving input $x\in\mathbb{F}_2^t$, $D_{k,l}$ outputs $\frac{k-1}{2l+1}$ strings from $\{z\in\mathbb{F}_2^N:Hz=x, |z|=w\}$, for each $w\in\{\epsilon N\pm l\}$. If there are fewer than $\frac{k-1}{2l+1}$ strings in some level $w$, the decoder returns all of them. It is clear that for any $l$ we have
$$\Pr_{\rho\sim P_\epsilon}[ \rho\notin D_{k}(H\rho^\intercal)]\leq \Pr_{\rho\sim P_\epsilon}[ \rho\notin D_{k,l}(H\rho^\intercal)],$$
since $D_k$ returns the $k$ most likely strings while $D_{k,l}$ returns at most $k-1$ strings. We thus turn to proving the desired bound for $D_{k,l}$. We first bound the probability that the error string $|\rho|$ be far away from its mean. Letting  
$$B=\Big\{z\in\mathbb{F}_2^N:\big||z|-\epsilon N\big|>l\Big\},$$
we have, by Chernoff's bound (i.e. Lemma \ref{chernoff}), that
\begin{align*}
    \Pr_{\rho\sim P_\epsilon}[ \rho\notin D_{k,l}(H\rho^\intercal)]&\leq \Pr_{\rho\sim P_\epsilon}[\rho\in B]+\Pr_{\rho\sim P_\epsilon}[ \rho\notin D_{k,l}(H\rho^\intercal)\big|\rho\notin B]\\
    &\leq 2e^{-\frac{l^2}{3\epsilon N}}+\max_{w\in\{\epsilon N\pm l\}}\Pr_{\rho\sim P_\epsilon}[ \rho\notin D_{k,l}(H\rho^\intercal)\big||\rho|=w].
\end{align*}
Since the distribution $P_\epsilon$ gives the same probability to any two strings of equal weights, we get 
\begin{align*}
    \Pr_{\rho\sim P_\epsilon}[ \rho\notin D_{k,l}(H\rho^\intercal)] &\leq 2e^{-\frac{l^2}{3\epsilon N}}+\max_{w\in\{\epsilon N\pm l\}}\Pr_{\rho\sim \lambda_{\{w\}}}[ \rho\notin D_{k,l}(H\rho^\intercal)]\\
    &\leq 2e^{-\frac{l^2}{3\epsilon N}}+\max_{w\in\{\epsilon N\pm l\}}\Pr_{\rho\sim \lambda_{\{w\}}}[|\Omega_\rho^{\{w\}}|>\frac{k-1}{2l+1}]. 
\end{align*}
Applying Lemma \ref{collisionuniform}, we get
\begin{align*}
    \Pr_{\rho\sim P_\epsilon}[ \rho\notin D_{k,l}(H\rho^\intercal)] &\leq 2e^{-\frac{l^2}{3\epsilon N}}+\frac{2l+1}{k-1}\cdot \max_{w\in\{\epsilon N\pm l\}} \Big\{ \mathsf{Coll_H}(S)-1 \Big\}\\
    &\leq 2e^{-\frac{l^2}{3\epsilon N}}+\frac{2(l+1)}{k}\cdot \max_{w\in\{\epsilon N\pm l\}} \Big\{ \mathsf{Coll_H}(S)-1 \Big\},
\end{align*}
where in the last line we used that $\frac{a}{b}\leq\frac{a+1}{b+1}$ whenever $a\leq b$.
\end{proof}

\section{A Criterion for Decoding}\label{weightdecoding}
In this section, we give a criterion that certifies that a linear code $C\subseteq\F_2^N$ is resilient to errors of probability $\epsilon$. We give such a criterion for both unique decoding and list decoding. The function we will need to make this connection is the Krawtchouk polynomial of degree $s$, which as we recall is defined as
$$K_s(x)=\sum_{j=0}^s(-1)^j\binom{x}{j}\binom{N-x}{s-j}.$$
For vectors $v\in\F_2^N$, we will abuse notation and write $K_s(v)$ to mean $K_s(|v|).$ For convenience, we also define for any $S\subseteq\{0,...,N\}$ the function
$$K_S(x)=\sum_{s\in S}K_s(x).$$
In the following proposition, we use basic Fourier analysis tools to rewrite the collision count $\mathsf{Coll}_H(S)$ in terms of the Krawtchouk polynomial $K_S$. We note that Proposition \ref{collisionprob} was previously proven in a different form in \cite{2021bargfourierlink} (see Theorem 2.1 and Lemma 4.1), and can be seen as describing the coset weight distribution of the code. Recall that we use $\mu_t$ to denote the uniform distribution over all vectors in $\mathbb{F}_2^t$, and that we use the notation $\binom{N}{S}=\sum_{s\in S}\binom{N}{s}$.
\begin{prop}\label{collisionprob}
Fix $\epsilon\in(0,\frac{1}{2})$, and let $H$ be a $t\times N$ matrix with entries in $\mathbb{F}_2$. Then for any $S\subseteq\{1,...,N\}$, we have
\begin{align*}
\mathsf{Coll}_H(S)&=\frac{1}{\binom{N}{S}}\E_{v\sim\mu_t}[K_S(vH)^2].
\end{align*}
\end{prop}

\begin{proof}
The main tool we will use is Parseval's Identity, which relates the evaluations $f(x)$ of a function $f:\mathbb{F}_2^t\rightarrow \R$ to its Fourier coefficients $\hat{f}(y)$ by
\begin{align}\label{parseval}
    \frac{1}{2^t}\sum_{x\in\mathbb{F}_2^t} f(x)^2=\sum_{y\in\mathbb{F}_2^t}\hat{f}(y)^2.
\end{align}
We will first need to rewrite $\mathsf{Coll}_H(S)$ as the $\ell_2$ norm of some function $f$. For this, we recall the definition $|\Omega_z^S|=\left\{y\in \mathbb{F}_2^N :|y|\in S\textnormal{ and } Hy^{\intercal}=Hz^\intercal   \right\}$ and note that
\begin{align*}
\mathsf{Coll}_H(S)&:=\frac{1}{\binom{N}{S}}\sum_{z\in\F_2^N:|z|\in S}|\Omega_z^S|\\
&= \binom{N}{S}\sum_{z\in\F_2^N:|z|\in S}\frac{1}{|\Omega_z^S|}\Pr_{a\sim\lambda_S}[Ha^\intercal=Hz^\intercal]^2\\
&=\binom{N}{S}\sum_{x \in \F_2^{t}} \Pr_{z\sim\lambda_S}[Hz^\intercal=x]^2.
\end{align*}
We are now ready to apply Parseval's Identity. Letting $f(x) =  \Pr_{z\sim \lambda_S}[Hz^\intercal = x]$ in equation (\ref{parseval}), we get
\begin{align*}
\mathsf{Coll}_H(S)& = \binom{N}{S}\sum_{x \in \F_2^{t}} f(x)^2 \\
& = 2^t\binom{N}{S}\sum_{y \in \F_2^{t}} \hat{f}(y)^2.
\end{align*}
But by definition we have $\hat{f}(y):=2^{-t}\sum_{x \in \F_2^{t}} f(x) \cdot  (-1)^{y\cdot x^\intercal}$, so the last equation can be rewritten as
\begin{align}\label{expand1}
\mathsf{Coll}_H(S) &= 2^{-t}\binom{N}{S} \sum_{y \in \F_2^{t}} \Big ( \sum_{x \in \F_2^{t}} f(x) \cdot  (-1)^{y\cdot x^\intercal } \Big)^2.
\end{align}
Define the function $L_S(z)$ to be 1 if $z\in\mathbb{F}_2^N$ satisfies $|z|\in S$, and 0 otherwise. We can then express $f(x)$ as
\begin{align}\label{expand2}
f(x) =  \Pr_{z\sim \lambda_S}[Hz^\intercal = x]=\frac{1}{\binom{N}{S}}\sum_{\substack{z\in\mathbb{F}_2^N \\ Hz^\intercal=x}}L_S(z).
\end{align}
Combining expressions (\ref{expand1}) and (\ref{expand2}) and applying the definition of the Fourier transform, we get
\begin{align*}
\mathsf{Coll}_H(S)& = 2^{-t} \binom{N}{S}  \sum_{y \in \F_2^{t}} \Big ( \sum_{z \in \F_2^N} \frac{L_{S}(z)}{\binom{N}{S}}\cdot (-1)^{yHz^\intercal} \Big )^2\\
& = \frac{2^{2N-t}}{\binom{N}{S}} \sum_{y \in \F_2^{t}} \hat{L}_S(yH)^2\\
&=\frac{2^{-t}}{\binom{N}{S}} \sum_{y \in \F_2^{t}} K_S(yH)^2.
\end{align*}
\end{proof}
We will now combine Theorem \ref{collisiondecoding} and Proposition \ref{collisionprob} to obtain Theorem \ref{fouriercriterionunique}, i.e. to obtain a bound on the decoding error probability in terms of the Fourier coefficients of the level function $L_\epsilon$. We prove a generalized version of Theorem \ref{fouriercriterionunique} below. To recover Theorem \ref{fouriercriterionunique}, set $k=1$ and $l=N^{3/4}$.
(You want to think of the parameter $l$ as being $l>>\sqrt{N}$ in both the case $k=1$ and the case $k>1$, so that the error term $e^{-\frac{\sqrt{N}}{3\epsilon}}$ is small). 
\begin{theorem}\label{fouriercriteriongeneral}
Fix $\epsilon \in(0,\frac{1}{2})$, let $H$ be any $t\times N$ Boolean matrix, and let $k=(2l+1)m +1$ for any integers $m \geq0$ and $l\leq (\frac{1}{2}-\epsilon)N$. Then
\begin{align*}
    \Pr_{\rho\sim P_\epsilon}[\rho\notin D_k(H\rho^\intercal)]&\leq 2e^{-\frac{l^2}{3\epsilon N}}+\frac{4(l+1)}{k}  \mathop{\max}_{\substack{S\subseteq \{\epsilon N\pm l\} \\
    1\leq|S|\leq 1+\mathbbm{1}\{k=1\}}} \Big\{  \frac{1}{\binom{N}{S}}\E_{v\sim\mu_t}\big[ K_S(vH)^2\big]-1 \Big\},
\end{align*}
where the function $\mathbbm{1}\{k=1\}$ is $1$ when $k=1$, and $0$ otherwise.
\end{theorem}
\begin{proof}
Applying Theorem \ref{collisiondecoding} and Proposition \ref{collisionprob}, we have
\begin{align*}
     \Pr_{\rho\sim P_\epsilon}[\rho\notin D_k(H\rho^\intercal)]&\leq 2e^{-\frac{l^2}{3\epsilon N}} +\frac{4(l+1)}{k} \mathop{\max}_{\substack{S\subseteq \{\epsilon N\pm l\} \\
    1\leq|S|\leq 1+\mathbbm{1}\{k=1\}}} \Big\{ \mathsf{Coll_H}(S)-1 \Big\}\\
    &=2e^{-\frac{l^2}{3\epsilon N}}+\frac{4(l+1)}{k}  \mathop{\max}_{\substack{S\subseteq \{\epsilon N\pm l\} \\
    1\leq|S|\leq 1+\mathbbm{1}\{k=1\}}} \Big\{  \frac{1}{\binom{N}{S}}\E_{v\sim\mu_t}\big[ K_S(vH)^2\big]-1 \Big\}.
\end{align*}
\end{proof}
One consequence of Theorem \ref{fouriercriteriongeneral} is Corollary \ref{weightunique}, which states that $C$ is resilient to $\epsilon$-errors if the weight distribution of $C^\perp$ is close enough to the binomial distribution (see Appendix \ref{abinomialweight} for the proof).
As another application of Theorem \ref{fouriercriteriongeneral}, we present the following bound on the probability of making a list-decoding error for a code $C$. We note that once again, our bound depends only on the weight distribution of the dual code $C^\perp$.

\begin{prop}\label{weightcriterion}
Fix any $\epsilon\in(0,\frac{1}{2})$, and define 
$\beta=\frac{1-2\sqrt{\tilde{\epsilon}(1-\tilde{\epsilon})}}{2}$ for
$\tilde{\epsilon}=\epsilon+N^{-1/4}.$
Let $B=\{\beta N,...,(1-\beta)N\} $, and let $k^*=(2N^{3/4}+1)m+1$ for some integer $m\geq 0$.
Then for all $N> \left(\frac{5}{\epsilon}\right)^{20}$ and all integers $k\geq k^*$, we have that any $t\times N$ matrix $H$ with entries in $\mathbb{F}_2$ satisfies
\begin{align*}
\Pr_{\rho\sim P_\epsilon}[\rho\notin D_k(H\rho^\intercal)]&\leq 2e^{-\frac{\sqrt{N}}{3\epsilon}}+\frac{N}{k^*} \max_{j\in B}\left\{ \Pr_{v\sim \mu_t}[|v H|=j] \cdot \frac{2^N}{\binom{N}{j}}-1\right\} \nonumber\\
&+\frac{2^{h(\epsilon)N+N^{\frac{4}{5}}}}{k^*}  \max_{ j\notin B}\left\{ \Pr_{v\sim \mu_t}\big[|v H|=j\big]\cdot  2^{2\epsilon N\log|1-\frac{2j}{N}|} \right\}.
\end{align*}
\end{prop}


\begin{proof}
We will use Theorem \ref{fouriercriteriongeneral} to bound the decoding error probability in terms of the Krawtchouk polynomials $K_S(j)$ and the probability factors $\Pr_{v\sim\mu_{t}}\big[|v H|=j\big] $. Some of these terms will then be bounded using Proposition \ref{IFourier}, and some will be bounded using Theorem \ref{thmkrawtchoukbound}.
We proceed with the proof; letting $l= N^{3/4}$ in Theorem \ref{fouriercriteriongeneral}, we get
\begin{align}\label{startingpoint}
    \Pr_{\rho\sim P_\epsilon}[\rho\notin D_k(H\rho^\intercal)]&\leq 2e^{-\frac{\sqrt{N}}{3\epsilon}}+\frac{N}{k^*}  \mathop{\max}_{\substack{S\subseteq \{\epsilon N\pm N^{3/4}\} \\
    1\leq|S|\leq 2}} \Big\{  \frac{1}{\binom{N}{S}}\sum_{j=0}^N \Pr_{v\sim\mu_{t}}\big[|v H|=j\big] K_S(j)^2 -1 \Big\}.
\end{align}
We want to bound the summation in the second term. We will start with the central terms $j\in B$. For these we rely on Proposition \ref{IFourier}, which states that $\frac{2^{-N}}{\binom{N}{S}}\sum_{j = 0}^N  \binom{N}{j} \cdot K_S(j)^2=1$ for all $S\subseteq\{0,...,N\}$. For any $S\subseteq\{0,...,N\}$, we then get
\begin{align}\label{center1}
\frac{1}{\binom{N}{S}}\sum_{j\in B} \Pr_{v\sim\mu_{t}}\big[|v H|=j\big] K_S(j)^2&\leq \frac{1}{\binom{N}{S}}\max_{j\in B}\left\{ \Pr_{v\sim \mu_t}[|v H|=j]\cdot \frac{1}{\binom{N}{j}}  \right\}\sum_{j\in B} \binom{N}{j} \cdot K_S(j)^2\nonumber \\
&\leq 2^{N}\max_{j\in B}\left\{ \Pr_{v\sim \mu_t}[|v H|=j] \cdot \frac{1}{\binom{N}{j}}\right\}.
\end{align}
We then want to bound the contribution of the faraway terms $j\notin B$ to the summation in (\ref{startingpoint}), i.e. we want to bound
\begin{align}\label{bdgoal}
    &\mathop{\max}_{\substack{S\subseteq \{\epsilon N\pm N^{3/4}\} \\
    1\leq|S|\leq 2}} \Big\{  \frac{1}{\binom{N}{S}}\sum_{j\notin B} \Pr_{v\sim\mu_{t}}\big[|v H|=j\big] K_S(j)^2  \Big\}.
\end{align}
We will want to apply Theorem \ref{thmkrawtchoukbound} to every term in this sum. Note that by definition of Krawtchouk polynomials, for any $w,w'$ we have
\begin{align*}
    K_{\{w,w'\}}(y)&=K_{w}(y)+K_{w'}(y)\\ &\leq2\cdot\max\big\{K_{w}(y),K_{w'}(y)\big\}.
\end{align*}
We can then bound equation \ref{bdgoal} by
\begin{align*}
    (\ref{bdgoal})&\leq\frac{1}{\binom{N}{\epsilon N-N^{3/4}}}\cdot N \mathop{\max}_{\substack{S\subseteq \{\epsilon N\pm N^{3/4}\} \\
    1\leq|S|\leq 2\\
    j\notin B}}\Big\{ \Pr_{v\sim\mu_{t}}\big[|v H|=j\big] K_S(j)^2  \Big\}\nonumber\\
    &\leq\frac{N}{\binom{N}{\epsilon N-N^{3/4}}} \mathop{\max}_{\substack{w\in\{\epsilon N\pm N^{3/4}\}\\
    j\notin B}}\Big\{ \Pr_{v\sim\mu_{t}}\big[|v H|=j\big]\cdot 4 K_{w}(j)^2  \Big\}.
\end{align*}
Applying Theorem \ref{thmkrawtchoukbound}, we get
\begin{align*}
(\ref{bdgoal})&\leq \frac{ 4N }{\binom{N}{\epsilon N-N^{\frac{3}{4}}}}\max_{\substack{w\in \{\epsilon N\pm N^{\frac{3}{4}}\} \\j\notin B}}\left\{ \Pr_{v\sim \mu_t}\big[|v H|=j\big]\cdot 2^{2h(w)N+2w\log|1-\frac{2j}{N}|} \right\}\nonumber\\
&\leq \frac{2^{N^{4/5}}}{N}\cdot2^{h(\epsilon)N}  \max_{ j<\beta N}\left\{ \Pr_{v\sim \mu_t}\big[|v H|=j\big]\cdot  2^{2\epsilon N\log(1-\frac{2j}{N})} \right\}.
\end{align*}
Combining this bound for the faraway terms with our bound (\ref{center1}) for the central terms of the summation, we bound the right-hand side of equation (\ref{startingpoint}) by
\begin{align*}
\Pr_{\rho\sim P_\epsilon}[\rho\notin D_k(H\rho^\intercal)]&\leq 2e^{-\frac{\sqrt{N}}{3\epsilon}}+\frac{N}{k^*} \max_{j\in B}\left\{ \Pr_{v\sim \mu_t}[|v H|=j] \cdot \frac{2^N}{\binom{N}{j}}-1\right\} \nonumber\\
&+\frac{2^{h(\epsilon)N+N^{\frac{4}{5}}}}{k^*}  \max_{ j<\beta N}\left\{ \Pr_{v\sim \mu_t}\big[|v H|=j\big]\cdot  2^{2\epsilon N\log(1-\frac{2j}{N})} \right\}. 
\end{align*}
\end{proof}


\section{The Weight Distribution of Transitive Linear Codes}\label{weighttransitive}
We will now prove Theorem \ref{probtransitive}. We note that the bound we get is essentially tight, since for $\eta\in(0,1)$ the repetition code $$C=\left\{(z,...,z)\in\F_q^N:z\in\F_q^{\eta N}\right\}$$ is transitive, has dimension $\eta N$, and has weight distribution 
\begin{align*}
\Pr_{c\sim \mathcal{D}(C)}\Big[|c| = \alpha N\Big] &=q^{-\eta N} \cdot \binom{\eta N}{(1-\alpha) \eta N} (q-1)^{\alpha\eta N}\\
&\geq q^{-\eta N} \cdot \sqrt{\frac{8\pi}{e^4\eta N}}\cdot 2^{h(\alpha)\eta N}\cdot q^{\alpha \eta N \log_q (q-1)}\\
&= \sqrt{\frac{8\pi}{e^4\eta N}}\cdot q^{-(1-h_q(\alpha))\eta N}
\end{align*}
for all $\alpha\in(0,1)$. We recall and prove our Theorem \ref{probtransitive} below:
\begin{theorem*}
Let $C\subseteq \mathbb{F}_q^N$ be a transitive linear code. Then for any $\alpha\in (0,1-1/q)$ we have
$$ \Pr_{c\sim \mathcal{D}(C)}\Big[|c| = \alpha N\Big] \leq q^{-(1-h_q(\alpha)) \textnormal{dim }C},$$
where $\mathcal{D}(C)$ is the uniform distribution over all codewords in $C$, $|c|$ is the number of non-zero coordinates of $c$, and $h_q$ is the q-ary entropy $$h_q(\alpha)= (1-\alpha) \log_q \frac{1}{1-\alpha} + \alpha \log_q\frac{q-1}{\alpha}.$$
\end{theorem*}
\begin{proof}
Let $M$ be the $t\times N$ generator matrix of $C$, and let $r=\textnormal{rank }M=\textnormal{dim }C$. Without loss of generality, suppose that the first $r$ columns of $M$ span the column-space of $M$. Define
$$C^{(\alpha)}=\{c\in C: |c|=\alpha N\},$$
and let $Z=(Z_1,...,Z_N)$ be a uniformly random codeword in $C^{(\alpha)}$. Now $C$ is transitive, so for every $j,k\in\{1,...,N\}$ the random variables $Z_j$ and $Z_k$ are identically distributed. By linearity of expectation and by definition of $C^{(\alpha)}$, we thus have that for every $j\in \{1,...,N\}$,
\begin{align}\label{prob0}
\Pr_{Z\sim \mathcal{D}(C^{(\alpha)})}[Z_j= 0]=1-\alpha.
\end{align}
But under condition (\ref{prob0}), $Z_j$ has maximal entropy when its remaining mass is uniformly distributed over $\{1,...,q-1\}$, i.e. when $\Pr[Z_j=m]=\frac{\alpha}{q-1}$ for all $m\in\{1,...,q-1\}$. The entropy of $Z_j$ is thus bounded by
\begin{align}\label{eqentropy}
\mathop{\mathsf{H}}_{Z\sim \mathcal{D}(C^{(\alpha)})}(Z_j)&\leq (1-\alpha)\log\frac{1}{1-\alpha}+(q-1)\cdot\frac{\alpha}{q-1}\log\frac{q-1}{\alpha}\nonumber\\
&=h_q(\alpha)\log(q).
\end{align}
We will now show that $\mathsf{H}(Z_j |Z_1,...,Z_{j-1})=0$ for every $j>r$. To this end, fix some $j>r$. Recall that the columns $\{M_1,...,M_r \}$ span the column-space of $M$, so we can write the column $M_j$ as $M_j=\sum_{k=1}^r \beta_k M_k$ for some $\beta_1,...,\beta_r\in\F_q$. But any codeword $c\in C$ can be expressed as $v^{(c)}M$ for some $v^{(c)}\in\mathbb{F}_q
^t$, so any codeword $c\in C$ satisfies $$c_j=v^{(c)}M_j=\sum_{k=1}^r\beta_k v^{(c)}M_k=\sum_{k=1}^r\beta_k c_k.$$
The random variable $Z_j$ is thus determined by $\{Z_1,...,Z_r \}$, and so we indeed have 
$$\mathop{\mathsf{H}}_{Z\sim \mathcal{D}(C^{(\alpha)})}(Z_j|Z_1,...,Z_{j-1})=0$$
for every $j>r$. Applying (\ref{eqentropy}) and the chain rule for entropy then gives
\begin{align*}
    \mathsf{H}(Z)&=\mathsf{H}(Z_1)+\sum_{i=2}^N\mathsf{H}(Z_i |Z_1,...,Z_{i-1})    \\
    &\leq \sum_{i=1}^{r}\mathsf{H}(Z_i)  \\
    &=r \cdot h_q(\alpha)\log(q)
\end{align*}
Now $Z$ is sampled uniformly from $C^{(\alpha)}$, so $\mathsf{H}(Z)=\log \Big( |C^{(\alpha)}|\Big)$. We thus have
\begin{align*}
 \Pr_{c\sim \mathcal{D}(C)}\Big[|c| = \alpha N\Big] &=\frac{\left|C^{(\alpha)}\right|}{q^r} \\
&=2^{\mathsf{H}(Z)}\cdot q^{-r}\\
&\leq q^{-(1-h_q(\alpha))\cdot r }.
\end{align*}
\end{proof}

For Reed-Muller codes, we will abuse notation and denote by $\mathcal{D}(n,d)$ the uniform distribution over all codewords in $\mathsf{RM}(n,d)$.
\begin{theorem}\label{rmupperprob}
For any $n,d<n,$ and $\alpha\in (0,1)$, the Reed-Muller code $\mathsf{RM}(n,d)$ over the prime field $\F_q$ satisfies
 $$ \Pr_{c\sim\mathcal{D}(n,d)}\Big[|c| = \alpha N\Big] \leq q^{-(1-h_q(\alpha))\cdot \binom{n}{\leq d}}.$$
\end{theorem}
\begin{proof}
This follows immediately from Theorem \ref{probtransitive}, Fact \ref{transitive}, and Fact \ref{fullrank}.
\end{proof}



\section{List Decoding for Transitive Codes}\label{listtransitive}

We now turn to proving Theorem \ref{thmtransitivelist}. Recall that in section \ref{weightdecoding} we bounded the minimum size for the decoding list of a linear code in terms of the weight distribution of its dual code. But as we mentioned in the preliminaries, the dual code of a transitive code is also transitive. For any transitive linear code $C$, we can thus apply our Theorem \ref{probtransitive} for the weight distribution of $C^\perp$ to get a bound on the size of the decoding list for $C$. We restate and prove our Theorem \ref{thmtransitivelist} below.
\begin{theorem*}
Fix any $\epsilon\in(0,\frac{1}{2})$, $\eta\in(0,1)$, and $N> \left(\frac{5}{\epsilon}\right)^{20}$. Then any  transitive linear code $C\subseteq\F_2^N$ of dimension $\textnormal{dim }C=\eta N$ can with high probability list-decode $\epsilon$-errors using a list $T$ of size
$$|T(x)|=2^{N^{5/6}}\cdot (2^{4\epsilon \eta N}+2^{\epsilon N\log(\frac{2}{1-\eta})}).$$
\end{theorem*}
\begin{proof}
We will show that there exists a function $T$ mapping every $x\in\F_2^N$ to a subset $T(x)\subseteq C$ of size
     $$|T(x)|=2^{N^{5/6}}\cdot (2^{4\epsilon \eta N}+2^{\epsilon N\log(\frac{2}{1-\eta})}),$$
     with the property that for every codeword $c\in C$ we have
    \begin{align*}
    \Pr_{\rho\sim P_\epsilon}\big[c\notin T(c+\rho)\big]\leq
   \frac{2}{N}.
    \end{align*}
Let $H$ denote the parity-check matrix of $C$. By Lemma \ref{eqvltdecoder}, it is sufficient to show that for any $N> \left(\frac{5}{\epsilon}\right)^{20}$ and any $k>1$ we have
\begin{align}\label{eqvlttransitive}
    \Pr_{\rho\sim P_\epsilon}[\rho\notin  D_k(H\rho^\intercal )]&\leq \frac{1}{N}+\frac{2^{N^{5/6}}}{Nk}\cdot (2^{4\epsilon \eta N}+2^{\epsilon N\log(\frac{2}{1-\eta})}).
    \end{align}
We will thus prove (\ref{eqvlttransitive}). Recall that for $k>N$, Proposition \ref{weightcriterion} yields the following bound on the left-hand side of (\ref{eqvlttransitive}):
\begin{align}\label{thmbdtransitive}
\Pr_{\rho\sim P_\epsilon}[\rho\notin D_k(H\rho^\intercal)]&\leq 2e^{-\frac{\sqrt{N}}{3\epsilon}}+\frac{2N}{k} \max_{j\in B}\left\{ \Pr_{v\sim \mu_t}[|v H|=j] \cdot \frac{2^N}{\binom{N}{j}}\right\} \nonumber\\
&+\frac{2^{h(\epsilon)N+N^{\frac{4}{5}}+1}}{k}  \max_{ j\notin B}\left\{ \Pr_{v\sim \mu_t}\big[|v H|=j\big]\cdot 2^{2\epsilon N\log(1-\frac{2j}{N})} \right\},
\end{align}
where $\beta=\frac{1}{2}\left( 1-2\sqrt{\tilde{\epsilon}(1-\tilde{\epsilon})} \right)$ for $\tilde{\epsilon}=\epsilon+N^{-1/4}$, and 
$B=\{\beta N,...,(1-\beta )N\}$. Our goal will be to bound both the central terms $j\in B$ and the faraway terms $j\notin B$ by using our bounds on the weight distribution of transitive codes. As we've seen in section \ref{prelim}, the dual code $C^\perp$ is a transitive linear code of dimension $N-\textnormal{dim }C $. By Theorem \ref{probtransitive}, we thus have that for all $j\in\{0,...,N\}$,
\begin{align}\label{weightbd}
    \Pr_{v\sim\mu_t}\big[|vH|=j \big]\leq 2^{-(1-h(\frac{j}{N}))(1-\eta) N}.
\end{align}
For any $j\in B$, we then have by Fact \ref{stirling} that
\begin{align*}
    \Pr_{v\sim\mu_t}\big[|vH|=j \big]\cdot \frac{2^N}{\binom{N}{j}}  &\leq 2^{-(1-h(j/N))(1-\eta) N}\cdot \frac{2^N}{\sqrt{\frac{8\pi}{e^4 N}}\cdot2^{h(j/N)N}}  \\
    &= \sqrt{\frac{e^4 N}{8\pi}}\cdot 2^{(1-h(j/N))\eta N}.
\end{align*}
But for $j\in B$ we have $\beta<\frac{j}{N}<1-\beta$, so the right-hand side is maximized at $j=\beta N$. Applying Lemma \ref{pinsker}, we get
\begin{align}\label{centraltermstransitive}
    \max_{j\in B}\left\{\Pr_{v\sim\mu_t}\big[|vH|=j \big]\cdot \frac{2^N}{\binom{N}{j}}\right\}  &\leq \sqrt{\frac{e^4 N}{8\pi}}\cdot 2^{(1-h(\beta))\eta N} \nonumber\\
    &\leq \sqrt{\frac{e^4 N}{8\pi}}\cdot 2^{4\tilde{\epsilon}(1-\tilde{\epsilon})\eta N}.
\end{align}
We now turn to the faraway terms of equation (\ref{thmbdtransitive}). By equation (\ref{weightbd}), we have 
\begin{align*}
    \max_{ j<\beta N}\left\{ \Pr_{v\sim \mu_t}[|v H|=j]\cdot 2^{2\epsilon N\log(1-\frac{2j}{N})} \right\}\leq\max_{ \delta< \beta}\left\{  2^{-(1-h(\delta))(1-\eta) N}\cdot 2^{2\epsilon N\log(1-2\delta)} \right\}.
\end{align*}
Note that by definition of $\beta$, any $\delta<\beta$ can be written as $\delta=\frac{1-2\sqrt{\alpha\tilde{\epsilon}(1-\tilde{\epsilon})}}{2}$ for some $\alpha> 1$. By Lemma \ref{pinsker}, we can then rewrite our previous expression as
\begin{align*}
    \max_{ j<\beta N}\left\{ \Pr_{v\sim \mu_t}[|v H|=j]\cdot 2^{2\epsilon N\log(1-\frac{2j}{N})} \right\}\leq\max_{ \alpha> 1}\left\{  2^{-\frac{2\alpha\tilde{\epsilon}(1-\tilde{\epsilon})}{\ln2}(1-\eta) N}\cdot 2^{\epsilon N\log(4\alpha\tilde{\epsilon}(1-\tilde{\epsilon}))} \right\}.
\end{align*}
But for any positive constant $c$, the derivative of $\log (\alpha) - c \alpha$ is $\frac{1}{\alpha\cdot\ln2} - c$, and the second derivative is always negative. Thus, the above expression achieves its maximum when $\alpha = \frac{\epsilon }{2\Tilde{\epsilon}(1-\Tilde{\epsilon})(1-\eta)}$. We then get
\begin{align}\label{farawayterms}
    \max_{ j<\beta N}\left\{ \Pr_{v\sim \mu_t}[|v H|=j]\cdot 2^{2\epsilon N\log(1-\frac{2j}{N})} \right\}&\leq  2^{-\frac{\epsilon N}{\ln2}}\cdot 2^{\epsilon N\log(\frac{2\epsilon}{1-\eta})}\nonumber\\
    &\leq  2^{-h(\epsilon)N}\cdot
    2^{\epsilon N\log(\frac{2}{1-\eta})},
\end{align}
where in the last line we used the inequality $\log(1-x)\geq -\frac{x}{(1-x)\ln2}$ for $x<1$ to get $h(\epsilon)\leq -\epsilon\log(\epsilon)+\frac{\epsilon}{\ln2}$.
We now use equations (\ref{centraltermstransitive}) and (\ref{farawayterms}) to bound the central and faraway terms of (\ref{thmbdtransitive}) respectively. This gives
\begin{align*}
\Pr_{\rho\sim P_\epsilon}[\rho\notin D_k(H\rho^\intercal)]&\leq 2e^{-\frac{\sqrt{N}}{3\epsilon}}+\frac{2N}{k}\cdot \sqrt{\frac{e^4 N}{8\pi}}\cdot 2^{4\tilde{\epsilon}(1-\tilde{\epsilon})\eta N}
+\frac{2^{N^{\frac{4}{5}}+1}}{k} \cdot 2^{\epsilon N\log(\frac{2}{1-\eta})}\\
&\leq \frac{1}{N}+\frac{2^{N^{5/6}}}{Nk}\cdot (2^{4\epsilon \eta N}+2^{\epsilon N\log(\frac{2}{1-\eta})}).
\end{align*}
We have shown (\ref{eqvlttransitive}), and so we are done.
\end{proof}

\section{List Decoding for Reed-Muller Codes}\label{listrm}
We will now turn to proving our list-decoding bounds for Reed-Muller codes. The dual code of the Reed-Muller code $\mathsf{RM}(n,d)$ is the code $\mathsf{RM}(n,n-d-1)$, so we can apply Samorodnitsky's Theorem \ref{previousboundsmall} to our Proposition \ref{weightcriterion}. We restate and prove our Theorem \ref{rmclose} below.
\begin{theorem*}
Let $\epsilon\in(0,\frac{1}{2})$ and $\gamma\in(0,1)$ be such that $1-\gamma\geq 2^{-\frac{2\epsilon}{(\ln2)^2}}$. Then the Reed-Muller code $\mathsf{RM}(n,d)$ of dimension $\binom{n}{\leq d}=(1-\gamma)N$ can with high probability list-decode $\epsilon$-errors using a list $T$ of size
     $$|T|=2^{h(\epsilon)N-\gamma N+o(N)}+2^{4\epsilon N+o(N)} .$$
\end{theorem*}
\begin{proof}
We will show that there exists a function $T$ mapping every $x\in\F_2^N$ to a subset $T(x)\subseteq  \mathsf{RM}(n,d)$ of size
         $$|T|=2^{h(\epsilon)N-\gamma N+o(N)}+2^{4\epsilon N+o(N)} ,$$
     with the property that for every codeword $c\in  \mathsf{RM}(n,d)$ we have
    \begin{align*}
    \Pr_{\rho\sim P_\epsilon}\big[c\notin T(c+\rho)\big]\leq
   \frac{2}{N}.
    \end{align*}
Let $H$ denote the parity-check matrix of $\mathsf{RM}(n,d)$. By Lemma \ref{eqvltdecoder}, it is sufficient to show that for any $N> \left(\frac{5}{\epsilon}\right)^{20}$ and any $k>1$ we have
\begin{align}\label{eqvltrm}
\Pr_{\rho\sim P_\epsilon}[\rho\notin D_k(H\rho^\intercal)]
&\leq \frac{1}{N}+\frac{2^{o(N)}}{kN}\left(2^{4\epsilon N}+
  2^{h(\epsilon)N-(1-\eta)N}\right).
\end{align}
We will thus prove (\ref{eqvltrm}).  Recall that for $k>N$, Proposition \ref{weightcriterion} yields the following bound on the left-hand side of (\ref{eqvltrm}):
\begin{align}\label{thmbdrm}
\Pr_{\rho\sim P_\epsilon}[\rho\notin D_k(H\rho^\intercal)]&\leq 2e^{-\frac{\sqrt{N}}{3\epsilon}}+\frac{2N}{k} \max_{j\in B}\left\{ \Pr_{v\sim \mu_t}\big[|v H|=j\big] \cdot \frac{2^N}{\binom{N}{j}}\right\} \nonumber\\
&+\frac{2^{h(\epsilon)N+N^{\frac{4}{5}}+1}}{k}  \max_{ j\notin B}\left\{ \Pr_{v\sim \mu_t}\big[|v H|=j\big]\cdot 2^{2\epsilon N\log|1-\frac{2j}{N}|} \right\},
\end{align}
where $\beta=\frac{1}{2}\left( 1-2\sqrt{\tilde{\epsilon}(1-\tilde{\epsilon})} \right)$ for $\tilde{\epsilon}=\epsilon+N^{-1/4}$, and 
$B=\{\beta N,...,(1-\beta )N\}$. Our goal is to bound every term in these sums by using the weight distribution bounds given in Theorems \ref{probtransitive} and \ref{previousboundsmall}. We bound the central terms in exactly the same way as in Theorem \ref{thmtransitivelist}: by Theorem \ref{rmupperprob} we know that the weight distribution of the Reed-Muller code satisfies
\begin{align*}
    \Pr_{v\sim\mu_t}\big[|vH|=j \big]\leq 2^{-(1-h(\frac{j}{N}))(1-\eta) N},
\end{align*}
so by Fact \ref{stirling} we have 
\begin{align*}
    \max_{j\in B}\left\{ \Pr_{v\sim\mu_t}\big[|vH|=j \big]\cdot \frac{2^N}{\binom{N}{j}}  \right\}&\leq \max_{j\in B}\left\{  2^{-(1-h(j/N))(1-\eta) N}\cdot \frac{2^N}{\sqrt{\frac{8\pi}{e^4 N}}\cdot2^{h(j/N)N}}  \right\}\\
    &= \max_{j\in B}\left\{\sqrt{\frac{e^4 N}{8\pi}}\cdot 2^{(1-h(j/N))\eta N}\right\}.
\end{align*}
But $B=\{\beta N,...,(1-\beta)N\}$, so by Lemma \ref{pinsker} we have
\begin{align}\label{centraltermsRM}
    \max_{j\in B}\left\{ \Pr_{v\sim\mu_t}\big[|vH|=j \big]\cdot \frac{2^N}{\binom{N}{j}}  \right\} &\leq \sqrt{\frac{e^4 N}{8\pi}}\cdot 2^{(1-h(\beta))\eta N}  \nonumber \\
    &\leq \sqrt{\frac{e^4 N}{8\pi}}\cdot 2^{4\Tilde{\epsilon}(1-\Tilde{\epsilon})\eta N}.
\end{align}
For the faraway terms, we use the weight bound from Theorem \ref{previousboundsmall}. By symmetry, we get that
\begin{align}\label{intermediatefarterm2}
\max_{ j\notin B} \left\{\Pr_{v\sim \mu_t}[|vH|=j]\cdot   2^{2\epsilon N\log|1-\frac{2j}{N}|} \right\}
&\leq 2^{o(N)}\cdot \max_{ j\leq\frac{N}{2}} \left\{  2^{-(1-\eta) N} \left(\frac{1}{\eta}\right)^{2j\ln2}  \cdot 2^{2\epsilon N\log|1-\frac{2j}{N}|} \right\}\nonumber\\
&=2^{o(N)}\cdot2^{-(1-\eta) N} \max_{ j\leq \frac{N}{2}} \left\{   2^{-2j\ln 2 \cdot\log(\eta)+2\epsilon N\log(1-\frac{2j}{N})} \right\}.
\end{align}
Now the function 
$$g(j)=-2j\ln 2\cdot \log(\eta)+2\epsilon N\log(1-\frac{2j}{N})$$
has first derivative
$$\frac{dg}{dj}=-2\ln 2\cdot \log(\eta)-\frac{4\epsilon}{\ln2\cdot (1-\frac{2j}{N})},$$
and second derivative
$$\frac{dg^2}{d^2j}=-\frac{8\epsilon}{\ln2\cdot N (1-\frac{2j}{N})^2}<0.$$
Thus $g(j)$ achieves its maximum at $j^*=\frac{N}{2}+\frac{\epsilon N}{(\ln2)^2\log(\eta)}$ and is decreasing over $[j^*,\frac{N}{2}]$. Whenever $\eta>2^{-\frac{2\epsilon}{(\ln2)^2}}$, we have $j^*\leq0$; in that case the argument in equation (\ref{intermediatefarterm2}) is maximized at $j=0$, and we get
\begin{align*}
\max_{ j\notin B} \left\{\Pr_{v\sim \mu_t}[|vH|=j]\cdot   2^{2\epsilon N\log|1-\frac{2j}{N}|} \right\}
&\leq 2^{-(1-\eta) N+o(N)}.
\end{align*}
Combining this bound for the faraway terms with the bound (\ref{centraltermsRM}) for the central terms, we bound the right-hand side of (\ref{thmbdrm}) by
\begin{align*}
\Pr_{\rho\sim P_\epsilon}[\rho\notin D_k(H\rho^\intercal)]
&\leq \frac{1}{N}+\frac{2^{o(N)}}{kN}\left(2^{4\epsilon\eta N}+
  2^{h(\epsilon)N-(1-\eta)N}\right).
\end{align*}
We have shown (\ref{eqvltrm}), and so we are done.
\end{proof}

\section*{Acknowledgements}
We thank Alexander Barg, Paul Beame, Noam Elkies, Amir Shpilka, Madhu Sudan and Amir Yehudayoff for useful discussions.

\appendix
\section{Weight Bounds Comparisons}\label{aweight}
In this section, we will compare our Theorem \ref{rmupperprob} with previously known bounds on the weight distribution of Reed-Muller codes. We recall our Theorem \ref{rmupperprob} below. Note that throughout this section, $\mathcal{D}(n,d)$ will denote the uniform distribution over all codewords in $\mathsf{RM}(n,d)$, and $|c|$ will denote the number of non-zero coordinates of $c$.

\begin{theorem*}
For any $n,d<n,$ and $\alpha\in (0,1)$, the Reed-Muller code $\mathsf{RM}(n,d)$ over the prime field $\F_q$ satisfies
 $$ \Pr_{c\sim\mathcal{D}(n,d)}\Big[|c| = \alpha N\Big] \leq q^{-(1-h_q(\alpha))\cdot \binom{n}{\leq d}},$$
\end{theorem*}
where we have defined $$h_q(\alpha)= (1-\alpha) \log_q \frac{1}{1-\alpha} + \alpha \log_q\frac{q-1}{\alpha}.$$

\hfill\\
\textbf{Reed-Muller codes over odd prime fields}
\hfill\\
We start with Reed-Muller codes over odd prime fields, for which the only preexisting weight bound we are aware of is the following result of \cite{beame2020weightodd}:
\begin{theorem}[\cite{beame2020weightodd}]\label{oddprimes}
For any $0 < \delta < \frac{1}{2}$, there are constants $c_1, c_2>0$ such that for any
odd prime $q$ and for any integers $d,n$ such that $d \leq \delta n$, we have
$$\Pr_{c\sim \mathcal{D}(n,d)}\Big[\frac{|c|}{N} \leq 1-\frac{1}{q}-q^{-c_1\frac{n}{d}}\Big] \leq q^{-c_2\binom{n}{\leq d}}.$$
\end{theorem}
This was a generalization of \cite{ben-eliezer2012weighthalf1}, who proved the same result for Reed-Muller codes over $\F_2$. Theorem \ref{oddprimes} is very strong for small degrees, but gets weaker as the degree increases. When $d$ is linear in $n$ we have $q^{-c_1\frac{n}{d}}=\Theta(1)$, meaning that in this regime Theorem \ref{oddprimes} can only give a nontrivial bound on normalized weights that are at least a constant away from $1-\frac{1}{q}$. Our Theorem \ref{rmupperprob} gives nontrivial bounds for all normalized weights $<1-\frac{1}{q}$, for all degrees $d<n$.

\hfill\\
\textbf{Reed-Muller codes over $\F_2$}
\hfill\\
We now turn to Reed-Muller codes over $\F_2$, for which more results are known. The same bound as Theorem \ref{oddprimes} was proven over $\F_2$ by \cite{ben-eliezer2012weighthalf1}. For comparison with our Theorem \ref{rmupperprob}, see the discussion above.

In the constant-rate regime (i.e. $d=\frac{n}{2}\pm O(\sqrt{n})$), the strongest known bounds for contant weights are the following two results of \cite{samorodnitsky2020weightboundhalf}:

\begin{theorem}[\cite{samorodnitsky2020weightboundhalf}]\label{samorodnitsky1}
Let $\binom{n}{\leq d}=\eta 2^n=\eta N$ for some $\eta\in(0,1)$. Then for any $\alpha\in(0, \frac{1}{2})$ we have
$$\Pr_{c\sim \mathcal{D}(n,d)}[|c|\leq \alpha N ]\leq 2^{o(N)}\left(\frac{1}{1-\eta}\right)^{2\ln2 \cdot \alpha N} 2^{-\eta N}.$$
\end{theorem}
This result is strong when $\alpha$ is away from $ 1/2$. For $\alpha$ close to $1/2$, the following bound is stronger.
\begin{theorem}[\cite{samorodnitsky2020weightboundhalf}]\label{samorodnitsky2}
Let $\binom{n}{\leq d}=\eta 2^n=\eta N$ for some $\eta\in(0,1)$, and define $A=\{\frac{1-\eta^{2\ln 2}}{2},...,\frac{1}{2}\}$. Then for any $\alpha\in(0,\frac{1}{2})$,
$$\Pr_{c\sim \mathcal{D}(n,d)}[|c|\leq \alpha N ]\leq 2^{o(N)}\cdot
\begin{cases}
 \frac{\binom{N}{\alpha N}}{2^{N}}  & \text{if $\alpha\in A$,}\\
\frac{1}{(1-\eta^{2\ln2})^{\alpha N}(1+\eta^{2\ln2})^{(1-\alpha) N}} & \text{otherwise.}
\end{cases}
$$
\end{theorem}
We note that the combination of Theorems \ref{samorodnitsky1} and \ref{samorodnitsky2} is stronger than our Theorem \ref{rmupperprob} whenever both the rate of the code and the normalized weight of the codeword are constant (i.e. $\alpha=\Theta(1)$ and $d=\frac{n}{2}\pm O(\sqrt{n})$). 

However, when the normalized weight is subconstant or when the degree is away from $\frac{n}{2}$ (i.e. $\alpha=o(1)$ or $d=\frac{n}{2}-\Theta(n)$), the $2^{o(N)}$ term becomes too large for Theorems \ref{samorodnitsky1} and \ref{samorodnitsky2} to give a strong bound. An approach that has been fairly successful in these two regimes (subonstant rate or subconstant weight) is the line of work of \cite{kaufman2012constantdegree,abbe2015RMlowrate,sberlo2020weightbound}. 
To our knowledge, the strongest results for these regimes are due to \cite{sberlo2020weightbound}. We start with their bound for lower weights, i.e. for weights in $[0,\frac{N}{4}].$
\begin{theorem}[\cite{sberlo2020weightbound}]\label{ss1}
For any integers $j,n,d$, we have
\begin{align*}
 \Pr_{c\sim\mathcal{D}(n,d)}[|c|\leq 2^{-j}\cdot 2^n]&\leq
2^{-\big(  1-17(\frac{j}{1-\frac{d}{n}}+\frac{2-\frac{d}{n}}{(1-\frac{d}{n})^2})(\frac{d}{n})^{j-1} \big)\binom{n}{\leq d} + O(n^4)}.
\end{align*}
\end{theorem}
We claim that for every $d> \frac{n}{34}$, there is some weight threshold $A_d<\frac{1}{4}$ for which our Theorem \ref{rmupperprob} is stronger than Theorem \ref{ss1} for all weights larger than $ A_d N$. 
One way to see this is to note that our Theorem \ref{rmupperprob} satisfies
\begin{align*}
Pr[|c|\leq 2^{-j}\cdot 2^n]&\leq 2^{-\big( 1-h(2^{-j})  \big)\binom{n}{\leq d}}\\
&\leq 2^{-( 1-2j\cdot2^{-j})\binom{n}{\leq d}},
\end{align*}
while the expression in Theorem \ref{ss1} satisfies
$$2^{-\big(  1-17(\frac{j}{1-\frac{d}{n}}+\frac{2-\frac{d}{n}}{(1-\frac{d}{n})^2})(\frac{d}{n})^{j-1} \big)\binom{n}{\leq d} }\geq 2^{-\big( 1-17j(\frac{d}{n})^{j-1} \big)\binom{n}{\leq d} }.$$
Thus our Theorem \ref{rmupperprob} is stronger than Theorem \ref{ss1} whenever $j\cdot2^{-(j-1)}<17j\cdot(\frac{d}{n})^{j-1}$, i.e. whenever
$$j<\frac{\log 17}{\log\frac{n}{2d}}+1.$$
For any $d>\frac{n}{34}$, this gives a nontrivial range. 

This concludes our comparison of Theorem \ref{rmupperprob} with Theorem \ref{ss1}, which was the bound of \cite{sberlo2020weightbound} for weights in $[0,\frac{N}{4}].$ We now turn to their bounds for larger weights.
\begin{theorem}[\cite{sberlo2020weightbound}]\label{ss2}
Let $j,n\in\N$ and let $0<\gamma(n)<\frac{1}{2}-\Omega\left(  \sqrt{\frac{\log n}{n}}\right)$ be a parameter (which may be constant or depend on $n$) such that $\frac{j+\log\frac{1}{1-2\gamma}}{(1-2\gamma)^2}=o(n).$ Then
$$\Pr_{c\sim\mathcal{D}(n,\gamma n)}[|c|\leq \frac{1-2^{-j}}{2}N]\leq 2^{-2^{-c(\gamma,j)}\binom{n}{\leq d}+O(n^4)},$$
where $c(\gamma,j)=O\left( \frac{\gamma^2 j+\gamma\log\frac{1}{1-2\gamma}}{1-2\gamma}+\gamma\right)$.
\end{theorem}
This bound holds when the degree is smaller than $\frac{n}{2}$. For arbitrary degree, \cite{sberlo2020weightbound} gives the following:
\begin{theorem}[\cite{sberlo2020weightbound}]\label{ss3}
For any integers $n,d$ and any $\delta>0$, we have
\begin{align*}
 \Pr_{c\sim\mathcal{D}(n,d)}[|c|\leq \frac{1-\delta}{2}N]&\leq e^{-\frac{\delta^2}{2}\cdot 2^d}.
\end{align*}
\end{theorem}
\hfill\\
We will start by comparing our Theorem \ref{rmupperprob} with Theorem \ref{ss3}. Applying Lemma \ref{pinsker}, we get from Theorem \ref{rmupperprob} that
\begin{align*}
    \Pr_{c\sim\mathcal{D}(n,d)}[|c|\leq \frac{1-\delta}{2}N]&\leq 2^{-(1-h(\frac{1-\delta}{2}))\cdot\binom{n}{\leq d}}\\
    &\leq e^{-\frac{\delta^2}{2}\cdot \binom{n}{\leq d}}.
\end{align*}
Thus our Theorem \ref{rmupperprob} is strictly stronger than Theorem \ref{ss3} for all $d<n$. We will now compare our Theorem \ref{rmupperprob} with Theorem \ref{ss2}. Applying Lemma \ref{pinsker}, we get from Theorem \ref{rmupperprob} that
\begin{align*}
    \Pr_{c\sim\mathcal{D}(n,d)}[|c|\leq \frac{1-2^{-j}}{2}N]&\leq 2^{-(1-h(\frac{1-2^{-j}}{2}))\cdot \binom{n}{\leq d}}\\
    &\leq 2^{-\frac{2^{-2j}}{2\ln2}\cdot \binom{n}{\leq d}}.
\end{align*}
It follows that our Theorem \ref{rmupperprob} is stronger than Theorem \ref{ss2} whenever $2^{-(2j+1)}\geq 2^{-c(\gamma,j)}$, i.e. whenever
\begin{align*}
    2j+1\leq c(\gamma,j).
\end{align*}
But $c(\gamma,j):=O\left( \frac{\gamma^2 }{1-2\gamma}\cdot j+\frac{\gamma\log\frac{1}{1-2\gamma}}{1-2\gamma}+\gamma\right)$, and 
$\frac{\gamma^2 }{1-2\gamma}\rightarrow\infty$
as $\gamma\rightarrow 1/2$. Thus there exists some constant $\gamma^*\in (0,\frac{1}{2})$ such that our Theorem \ref{rmupperprob} is stronger than Theorem \ref{ss2} whenever $d>\gamma^* n$. In private correspondence with Amir Shpilka and Ori Sberlo, we learned that $\gamma^*$ can be computed to be $\gamma^*\approx 0.38$.

\section{Proof of Corollary \ref{weightunique}}\label{abinomialweight}
Recall that for any $\epsilon\in(0,1)$ we defined $$A_\epsilon=\{\alpha N:h(\alpha)>1-h(\epsilon)-N^{-1/5}\},$$
and that for any code $C$ we denote by $\mathcal{D}(C^\perp)$ the uniform distribution over the dual code $C^\perp$. We now restate and prove our Corollary \ref{weightunique}.
\begin{corollary*}
Let $C\subseteq\F_2^N$ be a linear code, and let $\epsilon\in(0,\frac{1}{2})$ be arbitrary.
Suppose that for every $j\in A_\epsilon$ we have
\begin{align*}
    \Pr_{y\sim\mathcal{D}(C^\perp)}\big[|y|=j\big]\leq \big( 1+o(N^{-1})  \big)\frac{\binom{N}{j}}{2^N},
\end{align*}
and suppose that
\begin{align*}
    \Pr_{y\sim\mathcal{D}(C^\perp)}\big[|y|\notin A_\epsilon\big]\leq 2^{N^{\frac{3}{4}}}\cdot \frac{\sum_{i\notin A_\epsilon}\binom{N}{i}}{2^N}.
\end{align*}
Then $C$ is resilient to $\epsilon$-errors.
\end{corollary*}

\begin{proof}
From Theorem \ref{fouriercriterionunique}, we know that there exists some decoder $d:\F_2^N\rightarrow C$ such that for all $c\in C$,
\begin{align}\label{summation}
    \Pr_{\rho\sim P_\epsilon}[ d(c+\rho)\neq c]&\leq 2e^{-\frac{\sqrt{N}}{3\epsilon}}+N \mathop{\max}_{\substack{S\subseteq \{\epsilon N\pm N^{3/4}\} \\
    1\leq|S|\leq 2}} \Big\{ \frac{1}{\binom{N}{S}}\sum_{j=0}^N \Pr_{c\sim C^\perp}\big[|c|=j\big] K_S(j)^2-1 \Big\},
\end{align}
where $\binom{N}{S}=\sum_{s\in S}\binom{N}{s}$ and where $K_S=\sum_{s\in S}K_s$ for $K_s$ the Krawtchouk polynomial of degree $s$. 
Let $\nu$ be such that $h(\nu)=1-h(\epsilon)-N^{-1/5}$, and define the set of weights
$$A_\epsilon=\{\nu N,...,(1-\nu)N\}.$$
We will start by bounding the central terms $j\in A_\epsilon$ in equation (\ref{summation}). Applying Proposition \ref{IFourier} and the first condition in our theorem statement, we immediately get that for any $S\subseteq \{0,...,N\}$,
\begin{align}\label{center}
    \frac{1}{\binom{N}{S}}\sum_{j\in A_\epsilon} \Pr_{c\sim C^\perp}\big[|c|=j\big] K_S(j)^2&\leq 1+o\big(\frac{1}{N}\big).
\end{align}
We now turn to the faraway terms $j\notin A_\epsilon$. For these we note that by definition of Krawtchouk polynomials, for any integer $s$ we have
\begin{align*}
    K_s(x)=\sum_{j=0}^s(-1)^j\binom{x}{j}\binom{N-x}{s-j} \leq\sum_{j=0}^s\binom{x}{j}\binom{N-x}{s-j}  =\binom{N}{s}.
\end{align*}
For any $S\subseteq \{0,...,N\}$, we can then bound the faraway terms $j\notin A_\epsilon$ of equation (\ref{summation}) by
\begin{align*}
\frac{1}{\binom{N}{S}}\sum_{j\notin A_\epsilon} \Pr_{c\sim C^\perp}\big[|c|=j\big] K_S(j)^2&\leq\binom{N}{S}\Pr\big[ |y|\notin A_\epsilon\big].
\end{align*}
Applying the second condition in our theorem statement in combination with Fact \ref{stirling} and the subbaditivity of entropy, we get that 
\begin{align*}
\mathop{\max}_{\substack{S\subseteq \{\epsilon N\pm N^{3/4}\} \\
    1\leq|S|\leq 2}} \Big\{ \frac{1}{\binom{N}{S}}\sum_{j\notin A_\epsilon} \Pr_{c\sim C^\perp}\big[|c|=j\big] K_S(j)^2\Big\}&\leq 2\binom{N}{\epsilon N+N^{3/4}}\cdot 2\cdot2^{-h(\epsilon)N-N^{4/5}+N^{3/4}}\nonumber\\
&\leq 4\cdot 2^{h(\epsilon)N+h(N^{-1/4})N}\cdot 2^{-h(\epsilon)N-N^{4/5}+N^{3/4}}\nonumber\\
&\leq o(\frac{1}{N}).
\end{align*}
Combining this bound for the faraway terms with our bound (\ref{center}) for the central terms, we bound equation (\ref{summation}) by
\begin{align*}
\Pr_{\rho\sim P_\epsilon}[ d(c+\rho)\neq c)]&\leq    2e^{-\frac{\sqrt{N}}{3\epsilon}}+N\cdot o\big(\frac{1}{N}\big)\\
&\leq o(1).
\end{align*}

\end{proof}


\section{Lower Bounds on List Decoding}\label{alistcapacity}
\begin{claim}
Let $\epsilon\in(0,\frac{1}{2})$ be arbitrary, and consider any $N>\frac{10}{\epsilon^2}$. Suppose a code $C\subseteq\F_2^N$ and a decoder $d_k:\F_2^N\rightarrow C^{\otimes k}$ satisfy
$$\mathop{\Pr}_{\substack{\rho\sim P_\epsilon\\c\sim \mathcal{D(C)}}}[c\in d_k(c+\rho)]\geq \frac{3}{4},$$
for $P_\epsilon$ the $\epsilon$-noisy distribution and $\mathcal{D(C)}$ the uniform distribution on $C$. Then we must have $$k\geq |C|\cdot 2^{-(1-h(\epsilon))N}\cdot \frac{2^{-h(\epsilon)N^{3/4}}}{8}.$$
\end{claim}
\begin{proof}
We will first show that in order for the decoder $d_k$ to succeed with high probability, there must be many codewords $c\in C$ for which $$|\{ x\in\F_2^N:c\in d_k(x) \}|\gtrsim 2^{h(\epsilon)N}.$$
Intuitively this is because the sphere of radius $\epsilon N$ around any codeword $c$ contains $\approx2^{h(\epsilon)N}$ points (and for any sent codeword $c$, with high probability the received message $m$ will satisfy $|m+c|\approx \epsilon N$). We will then simply double-count the number of pairs $(x, c)$ for which $c\in d_k(x)$. On the one hand, there are $2^N\cdot k$ such pairs, since every received message is mapped to $k$ codewords; on the other hand, there must be at least about $|C|\cdot 2^{h(\epsilon)N}$ pairs, since as we've just argued most codewords in $C$ need to be matched to at least $\approx 2^{h(\epsilon)N}$ points. It follows that we must have
$$k\gtrsim |C|\cdot \frac{2^{h(\epsilon)N}}{2^N}.$$

Formally, we first note that the theorem condition implies that at least $\frac{|C|}{2}$ codewords $c\in C$ must satisfy
\begin{align}\label{rewritecondition}
    \Pr_{\rho\sim P_\epsilon}[c\in d_k(c+\rho)]\geq \frac{1}{2}.
\end{align}
Fix any such $c$. Now from Chernoff's bound (i.e Lemma \ref{chernoff}), we have for $N$ large enough that
\begin{align*}
    \Pr_{\rho\sim P_\epsilon}\big[|\rho|\leq \epsilon N- \epsilon N^{3/4}\big]
    &\leq\frac{1}{4}.
\end{align*}
In order for $c$ to satisfy $c\in d_k(c+\rho)$ with probability at least $\frac{1}{2}$, there must then be a subset $S_c\subseteq\{x\in\F_2^N:|c+x|\geq\epsilon N-\epsilon N^{3/4}\} $ satisfying both
\begin{align}\label{eq1}
    x\in S_c\implies c\in d_k(x)
\end{align}
and
\begin{align}\label{eq2}
    \Pr_{\rho\sim P_\epsilon}\big[\rho\in S_c\big]\geq \frac{1}{4}.
\end{align}
But every element $x\in S_c$ satisfies $|c+x|\geq \epsilon N- \epsilon N^{3/4}$, so every $x\in S_c$ satisfies
\begin{align}\label{eq3}
\Pr_{\rho\sim P_\epsilon}\big[\rho=c+x\big]&\leq \epsilon^{\epsilon N-\epsilon N^{3/4}}(1-\epsilon)^{(1-\epsilon)N+\epsilon N^{3/4}}\nonumber\\
&\leq2^{-(1-N^{-1/4})h(\epsilon)N}
\end{align}
Equations (\ref{eq2}) and (\ref{eq3}) imply that any $c\in C$ that can be list-decoded by $d_k$ with probability $\geq \frac{1}{2}$ must satisfy $|S_c|\geq \frac{2^{(1-N^{-1/4})h(\epsilon)N}}{4}$. It then follows from (\ref{eq1}) that any such $c$ must satisfy
$$\big|\{x\in\F_2^N:c\in d_k(x)\}   \big|\geq \frac{2^{(1-N^{-1/4})h(\epsilon)N}}{4}.$$
By double counting, we get
\begin{align*}
    2^N\cdot k&=\sum_{c\in C}\big|\{x\in\F_2^N:c\in d_k(x)\}   \big|\\
    &\geq \frac{|C|}{2}\cdot \frac{2^{(1-N^{-1/4})h(\epsilon)N}}{4}\\
    &=\frac{|C|}{8}\cdot 2^{h(\epsilon)N-h(\epsilon)N^{3/4}}.
\end{align*}
The result then follows from rearranging terms.
\end{proof}

\section{Other Proofs for Sections \ref{intro} and \ref{prelim}}

\subsection{On Known List-Decoding Bounds for Reed-Muller Codes}\label{acomparelist}
We recall the known list-decoding bound for Reed-Muller codes (see equation (\ref{previouslistresult}) in section \ref{intro}):
\begin{align*}
    |T|=2^{\epsilon N\log\frac{4\epsilon(1-\epsilon)}{(1-\eta)^{4\ln2}}+o(N)}
\end{align*}
We claim this bound never achieves the information-theoretic $2^{h(\epsilon)N-(N-\textnormal{dim }C)+o(N)}$.
\begin{claim}
For any $\epsilon\in(0,\frac{1}{2})$ and any $\gamma=\gamma(\epsilon)\in (0,1)$, we have
\begin{align*}
    \epsilon \log\frac{4\epsilon(1-\epsilon)}{\gamma^{4\ln2}}>h(\epsilon)-\gamma.
\end{align*}
\end{claim}
\begin{proof}
We will show that for any $\epsilon\in(0,\frac{1}{2})$ and $c=\frac{\gamma}{\epsilon}<\frac{1}{\epsilon}$ we have
\begin{align*}
    \epsilon \log\frac{4\epsilon(1-\epsilon)}{(c\epsilon)^{2}}>h(\epsilon)-c\epsilon,
\end{align*}
i.e. that
\begin{align}\label{goallistbnd}
    f(\epsilon,c):=\log(1-\epsilon)+2\epsilon-2\epsilon\log c +c\epsilon>0.
\end{align}
We first fix some $\epsilon\in(0,\frac{1}{2})$ and compute the $c$ maximizing $f(\epsilon,c)$. Note that
\begin{align*}
    \frac{\partial}{\partial c} f(\epsilon,c)&=-\frac{2\epsilon}{c\ln2} +\epsilon
\end{align*}
and 
\begin{align*}
    \frac{\partial^2}{\partial c^2} f(\epsilon,c)&=\frac{2\epsilon}{c^2\ln2}>0, 
\end{align*}
so $f(\epsilon,c)$ is minimized at $c=\frac{2}{\ln2}$ and increasing over $c\in[0,\frac{2}{\ln2}]$. We thus have
\begin{align}\label{2casesforc}
    \min_{c<\frac{1}{\epsilon}} f(\epsilon,c)=\begin{cases}
f(\epsilon,\frac{2}{\ln2}) & \text{if $\epsilon<\frac{\ln2}{2}$,}\\
f(\epsilon,\frac{1}{\epsilon}) & \text{otherwise.}
\end{cases}
\end{align}
We deal with each case separately. For the case $\epsilon<\frac{\ln2}{2}$, we want to show that
\begin{align*}
    f(\epsilon,\frac{2}{\ln2})=\log(1-\epsilon)+2\epsilon\log (\ln2) +\frac{2\epsilon}{\ln2}\geq 0.
\end{align*}
The first derivative is
\begin{align*}
    \frac{\partial}{\partial \epsilon}f(\epsilon,\frac{2}{\ln2})=-\frac{1}{(1-\epsilon)\ln2}+2\log(\ln2)+\frac{2}{\ln2},
\end{align*}
and the second derivative is
\begin{align*}
    \frac{\partial^2}{\partial \epsilon^2}f(\epsilon,\frac{2}{\ln2})=-\frac{1}{(1-\epsilon)^2\ln2}<0.
\end{align*}
Thus the function $f(\epsilon,\frac{2}{\ln2})$ is maximized at $\epsilon^*=1-\frac{1}{(2\log(\ln2)+\frac{2}{\ln2})\ln2}\approx 0.21,$ and monotone on each side of $\epsilon^*$. In particular, since $\epsilon^*\in[0,\frac{\ln2}{2}]$ we know that over the interval $[0,\frac{\ln2}{2}]$ the function $f(\epsilon,\frac{2}{\ln2})$ achieves its minimum at either $\epsilon=0$ or $\epsilon=\frac{\ln2}{2}$. But $f(0,\frac{2}{\ln2})=0<f(\frac{2}{\ln2},\frac{\ln2}{2})$, so we indeed have that
$$f(\epsilon,\frac{2}{\ln2})\geq 0$$
for all $\epsilon<\frac{\ln2}{2}$. This deals with the first case of (\ref{2casesforc}). For the second case of (\ref{2casesforc}), we want to show that for all $\epsilon\in(0,\frac{1}{2})$ we have
$$f(\epsilon,\frac{1}{\epsilon})\geq0.$$
But 
$$f(\epsilon,\frac{1}{\epsilon})=\log(1-\epsilon)+2\epsilon+2\epsilon\log\epsilon+1$$
is decreasing in $\epsilon$ and $f(\frac{1}{2},2)=0$, so we indeed have $f(\epsilon,\frac{1}{\epsilon})\geq 0$ for all $\epsilon.$
\end{proof}

\subsection{Duals of Transitive Codes - Proof of Fact \ref{dualtransitive}}\label{adualtransitive}
\begin{claim*}
The dual code $C^\perp$ of a transitive code $C\subseteq\F_2^N$ is transitive.
\end{claim*}
\begin{proof}
Let $i,j\in [N]$ be arbitrary. Since $C$ is transitive, we know there exists a permutation $\pi :[N]\rightarrow [N]$ such that $\pi(j)=i$ and that for any $c=(c_1,...,c_N)\in C$, we have $c_{\pi}:=(c_{\pi(1),...,\pi(N)}) \in C$ . Clearly $\pi^{-1}$ satisfies $\pi^{-1}(i)=j$, and we claim that it also satisfies that $v_{\pi^{-1}} \in C^\perp$ for all $v\in C^\perp$. For this we note that since $c_\pi\in C$ for every $c\in C$, we have by definition that every $v\in C^\perp$ satisfies
$$\sum_k v_k c_{\pi(k)} = 0 \textnormal{ for all }c\in C.$$
We thus have
\begin{align*}
    v\in C^\perp &\implies \sum_k v_k c_{\pi(k)} = 0 \textnormal{ for all }c\in C\\
    &\implies \sum_k v_{\pi^{-1}(k)} c_k = 0 \textnormal{ for all }c\in C \\
    &\implies v_{\pi^{-1}}\in C^\perp.
\end{align*}
\end{proof}

\subsection{Basic Properties of Reed-Muller Codes - Proof of Facts \ref{fullrank} and \ref{transitive}}\label{afullrank}
\begin{fact*} Let $M$ be the $ \binom{n}{\leq d}\times N$ generator matrix of the Reed-Muller code. The columns of $M$ that correspond to the points $x \in \F_2^n$ with $|x| \leq d$ are linearly independent.
\end{fact*}
\begin{proof}
Let $M'$ be the submatrix of $M$ whose columns correspond to the points $v \in \F_2^n$ with $|v| \leq d$. It suffices to show that when you order the columns $M'_v$ of $M'$ in increasing order of $|v|$, every column is linearly independent from the preceding ones. But this is clearly the case, as for the monomial $m=\prod_{i:v_i=1}x_i$ we have $M_{m,v}=1$ and $M_{m,v'}=0$ for all $v'$ preceding v.
\end{proof}
\begin{fact*}
For all $n$ and all $d<n$, the Reed-Muller code $\mathsf{RM}(n,d)\subseteq\mathbb{F}_2^N$ is transitive.
\end{fact*}

\begin{proof}
Recall that we view each coordinate $i\in[N]$ as a point $v_i\in\mathbb{F}_2^n$, and that every codeword in $\mathsf{RM}(n,d)$ is the evaluation vector $\big(f(v_1),...,f(v_N)\big)$ of a polynomial $f$ of degree $\leq d$ in $n$ variables. 

Now fix two points $v_i,v_j\in \mathbb{F}_2^n$. We want to show that there is a permutation $\pi:\mathbb{F}_2^n\rightarrow \mathbb{F}_2^n$ such that

\begin{enumerate}[label=(\roman*)]
    \item $\pi(v_i)=v_j$
    \item If $\big(z_{v_1},...,z_{v_N}\big)\in \mathsf{RM}(n,d)$ then $\big(z_{\pi(v_1)},...,z_{\pi(v_N)}\big)\in \mathsf{RM}(n,d)$
\end{enumerate}
To this end, we choose the permutation $\pi(x)=x+v_i+v_j$. Then:
\begin{enumerate}[label=(\roman*)]
\item $\pi(v_i)=v_i+v_i+v_j=v_j$.
\item If $\big(z_{v_1},...,z_{v_N}\big)$ is a codeword, it can be written as $\big(z_{v_1},...,z_{v_N}\big)=\big(f(v_1),...,f(v_N)\big)$ for some polynomial $f$ of degree $\leq d$. But then the polynomial 

$g(x)=f(x+v_i+v_j)$ satisfies $\mathsf{deg}(g)=\mathsf{deg}(f)\leq d$, so $\big(g(v_1),...,g(v_N)\big)$ must be a codeword. Then since $g(x)=f \circ \pi(x)$ by definition, we have that $\big(z_{\pi(v_1)},...,z_{\pi(v_N)}\big)=\big(f\circ\pi(v_1),...,f\circ\pi(v_N)\big)=\big(g(v_1),...,g(v_N)\big)\in\mathsf{RM}(n,d)$. 
\end{enumerate}
\end{proof}



\subsection{A version of Pinsker's inequality - Proof of Lemma \ref{pinsker}}\label{apinsker}
\begin{lemma*}\label{pconverse}
For any $\mu\in(0,1)$, we have $$\frac{\mu^2}{2\ln2}\leq1-h\left(\frac{1-\mu}{2}\right)\leq \mu^2$$
\end{lemma*}
\begin{proof}
\begin{align*}
    1-h(\frac{1-\mu}{2})&=1+\frac{1-\mu}{2}\log\left( \frac{1-\mu}{2} \right)+\frac{1+\mu}{2}\log\left( \frac{1+\mu}{2} \right)\\
    &=\frac{1-\mu}{2}\log\left( 1-\mu\right)+\frac{1+\mu}{2}\log\left( 1+\mu\right)\\
    &=\frac{1}{2\ln2}\left[ -(1-\mu)\sum_{i=1}^\infty\frac{\mu^i}{i}-(1+\mu)\sum_{i=1}^\infty(-1)^i\frac{\mu^i}{i}  \right]\\
    &=\frac{1}{2\ln2}\left[ 2\mu\sum_{i=1}^\infty\frac{\mu^{2i-1}}{2i-1}-2\sum_{i=1}^\infty\frac{\mu^{2i}}{2i}  \right]\\
    &=\frac{1}{\ln2}\sum_{i=1}^\infty\mu^{2i}\left( \frac{1}{2i-1}-\frac{1}{2i}  \right)\\
    &=\frac{1}{2\ln2}\sum_{i=1}^\infty \frac{\mu^{2i}}{i(2i-1)}
\end{align*}
Thus $1-h(\frac{1-\mu}{2})\geq\frac{\mu^2}{2\ln2}$ and $1-h(\frac{1-\mu}{2})\leq \frac{1}{2\ln2} \sum_{i=1}^\infty \frac{\mu^2}{i(2i-1)}=\frac{1}{2\ln2}\cdot2\ln2\cdot \mu^2=\mu^2$.
\end{proof}
\newpage
\bibliographystyle{alpha}
\bibliography{rmkrawtchouk}

\end{document}