%auto-ignore
\section{Experiments} \label{sec:results}

\subsection{Implementation Details}
\paragraph{Wire Segmentation Network.}
We experiment with ResNet-50~\cite{resnet} and MixTransformer-B2~\cite{segformer} as our shared feature extractor. We expand the input RGB channel to six channels by concatenating the conditional probability map, min- and max-filtered luminance channels. For the min and max filtering, we use a fixed 6x6 kernel. We use separate decoders for the coarse and fine modules, denoted as $D_C$ and $D_F$ respectively.

%\textcolor{purple}{
We use the MLP decoder proposed in~\cite{segformer} for the MixTransformer segmentation model, and the ASPP decoder in~\cite{deeplabv3p} for our ResNet-50 segmentation model. In both the segmentation and inpainting modules, we take the per-pixel average of the predicted probability when merging overlapping patches. To crop $P_\mathrm{con}$ from $P_\mathrm{glo}$, we upsample the predicted $P_\mathrm{glo}$ to the original resolution, then crop the predicted regions according to the sliding window position.
%}

%\textcolor{purple}{
To train the segmentation module, we downsample the image $I_\mathrm{glo}$ to $p\times p$ to obtain $I_\mathrm{glo}^\mathrm{ds}$. From $I_\mathrm{glo}$, we randomly crop one $p\times p$ patch $I_\mathrm{loc}$ that contains at least 1\% wire pixels. This gives a pair of $I_\mathrm{glo}^\mathrm{ds}$ and $I_\mathrm{loc}$ to compute the losses. During inference, $I_\mathrm{glo}^\mathrm{ds}$ is obtained in the same way as training, while multiple $I_\mathrm{loc}$ are obtained via a sliding window sampled only when the proportion of wire pixels is above $\alpha$. All feature extractors are pretrained on ImageNet.
%, while the decoders are trained from scratch.
%}

We train our model on 5000 training images. The model is trained for 80k iterations with a batch size of 4. We set patch size $p = 512$ during training. 
%Global images are downsampled to 512$\times$512. We feed the global images to the coarse module and obtain a two channel class logit map. A single 1$\times$1 convolution is used to transform this logit map into a single channel map, which is then concatenated with the local RGB image and binary location mask. The five-channel input is finally fed into the fine module to obtain the local logit map.
For all ResNet models, we use SGD with a learning rate of 0.01, a momentum of 0.9 and weight decay of 0.0005. For MixTransformer models, we use AdamW~\cite{adamw} with a learning rate of 0.0002 and weight decay of 0.0001. Our training follows the ``poly" learning rate schedule with a power of 0.9. During inference, we set both the global image size and local patch size $p$ to 1024. Unless otherwise specified, we set the percentage for local refinement to $1\%$ ($\alpha=0.01$).

%In this section, we provide a set of quantitative results and comparisons between our two-stage model and other methods. We then provide ablation studies, qualitative visualizations and discuss several failure cases. For additional results and details, please refer to our supplementary materials.

%\todo{illurstrate the sampling strategy in training and inference strategy here?}
\vspace{-4mm}
\paragraph{Wire Inpainting Network.}
We adopt LaMa \cite{suvorov2022resolution} for wire inpainting by finetuning on an augmented wire dataset. 
To prepare the wire training set, we randomly crop ten $680\times680$ patches from the non-wire regions of each image in our training partition. In total, we have 50K more training images in addition to 
%\textcolor{purple}{
the 8M
%} 
Places2 \cite{zhou2017places} dataset, and increase its sampling rate %\textcolor{purple}{
by $10\times$
%} 
to balance the dataset. We also use all the ground truth segmentation maps in our training set to sample wire-like masks. During training, we start from Big-LaMa weights, and train the model on $512\times 512$ patches. We also prepare a synthetic wire inpainting quality evaluation dataset, containing 1000 images at $512\times 512$ with synthetic wire masks.
%containing 1000 images of $512 \times 512$ and synthetic wire masks on that. 
While running inference on full-resolution images, we apply a tile-based approach, by fixing the window size at $512\times 512$ with an $32$-pixel overlap. 

\subsection{Wire Segmentation Evaluation}
\paragraph{Quantitative Evaluation}

%\subsection{Globally Conditioned Inference}

We compare with several widely-used object semantic segmentation and high-resolution semantic segmentation models. Specifically, we train DeepLabv3+~\cite{deeplabv3p} with ResNet-50~\cite{resnet} backbone under two settings: global and local. In the global setting, the original images are resized to 1024$\times$1024. In the local setting, we randomly crop 1024$\times$1024 patches from the original images. We train our models on 4 Nvidia V100 GPUs and test them on a single V100 GPU. For high-resolution semantic segmentation models, we compare with CascadePSP~\cite{cascadepsp}, MagNet~\cite{magnet} and ISDNet~\cite{isdnet}. We describe the training details of these works in the supplement.

We present the results of in Table~\ref{table:results} tested on \benchmark. We report wire IoU, F1-score, precision and recall for quantitative evaluation. We also report wire IoUs for images at three scales, small (0 -- 3000$\times$3000), medium (3000$\times$3000 -- 6000$\times$6000) and large (6000$\times$6000+), which are useful for analyzing model characteristics. Finally, we report average, minimum and maximum inference times on \benchmark.

\input{tables/results}

As shown in Table~\ref{table:results}, while the global model runs fast, it has lower wire IoUs. In contrast, the local model produces high-quality predictions, but requires a very long inference time.
Meanwhile, although CascadePSP is a class-agnostic refinement model designed for high-resolution segmentation refinement, it primarily targets common objects and does not generalize to wires. 
% \textcolor{red}{\sout{We thus retrain CascadePSP on our dataset but find their data perturbation does not realistic model coarse wire segmentation, thus cannot effectively conduct refinement.}}
For MagNet, its refinement module only takes in probability maps without image features, thus failing to refine when the input prediction is inaccurate. 
% As a result, the refinement module cannot accurately produce high-resolution wire predictions.
Among these works, ISDNet is relatively effective and efficient at wire prediction. 
% In fact, their inference time is on par with the global network even at high image resolution, while maintaining relatively high wire IoU. 
However, their shallow network design trades off capacity for efficiency, limiting the performance of wire segmentation that is thin and sparse. 
% \textcolor{red}{\sout{For a fair comparison, we tried to replace the shallow branch in ISDNet with a MixTransformer backbone but failed due to GPU memory limitation. We thus only replace their deep network module, which only yields minor improvement, as shown in Table~\ref{table:results}.}}

% \textcolor{purple}{
Compared to the methods above, our model achieves the best trade-off between accuracy and memory consumption. By leveraging the fact that wires are sparse and thin, our pipeline captures both global and local features more efficiently, thus saving a lot of computation while maintaining high segmentation quality.
% }
% There are two main reasons for this poor performance. 
% First, the global model performs inference on downsampled images, which leads to degraded image quality around thin wire-like objects and causes imprecise or disjointed predictions. Second, since prediction maps are upsampled to the original resolution, the final segmentation map may contain artifacts such as aliasing and over-predicted regions.
% \mt{
% First, since whole-image models predict segmentation maps on downsampled images, predictions on extremely small wires may be incomplete or of low quality. Second, whole-image segmentation maps are obtained by bilinearly upsampling the model output which leads to loose predictions.
% }
% These issues are rare in common object semantic segmentation since their maximum downsampling rate is usually no greater than 3$\times$, while the downsampling rate in our task can reach 10$\times$ (e.g. downsampling 10k$\times$10k images to 1024$\times$1024.). As a result, common object semantic segmentation methods fail to maintain their performances in our task.

\input{figure_tex/comparison}

% Meanwhile, local models yield higher IoUs, but have significantly longer inference times. This is because in order to preserve resolutions of very large images, many iterations of sliding-window have to be performed (e.g. 25 iterations are required for an image with size 5120$\times$5120 and a window size of 1024$\times$1024), which leads to significantly slower inference speeds.

% In contrast, our two-stage model achieves better results than sliding-window models while only requiring less than half the inference time. The coarse module in our two-stage model determines potential wire regions for refinement, and skips the refinement step on regions with no wires. This saves inference time and increases the overall inference speed. In regions where there are wires, the fine module leverages information from the coarse module to more accurately predict a tight segmentation mask. These two factors together yield an effective and efficient model for wire semantic segmentation on high-resolution images.
\vspace{-4.5mm}
% \subsubsection{Comparing with SOTA}
\paragraph{Qualitative Evaluation}
%\todo{section subject to change}\\
We provide visual comparisons of segmentation models in Figure~\ref{fig:visual}. We show the ``local'' DeepLabv3+ model as it consistently outperforms its ``global'' variant given that ``local'' predicts wire masks in a sliding-window manner at the original image resolution. As a trade-off, without global context, the model suffers from over-prediction. CascadePSP is designed to refine common object masks given a coarse input mask, thus fails to produce satisfactory results when the input is inaccurate or incomplete. Similarly, the refinement module of MagNet does not handle inaccurate wire predictions. ISDNet performs the best among related methods, but the quality is still unsatisfactory as it uses a lightweight model with limited capacity. Compared to all these methods, our model captures both global context and local details, thus producing more accurate mask predictions.
\vspace{-4mm}
\paragraph{Ablation Studies}

% \subsubsection{Effectiveness of global logit map}

% We show that using the global logit map as input to the fine module conveys more contextual information effectively than using the local logit map. For comparison, we train a separate two-stage model, where we only crop and resize the logit map at the location of the local image patch as input to the fine module.

% \input{tables/logit}

% As shown in Table~\ref{table:logit}, using only the local logit map yields inferior performances. Specifically, we find that in situations where local refinement is taken place, the global logit map provides sufficient information for the fine module to identify confusing non-wire objects, thus avoiding over-prediction. Figure~\ref{fig:overpredict} demonstrates that our fine module with global logit map input successfully avoids a pattern on the building that strongly resembles a wire. 

% \input{figure_tex/overpredict}

In Table~\ref{table:component_ablation}, we report wire IoUs after removing each component in our model, including MinMax, MaxPool, and Coarse condition concatenation. We find that all components play a significant role for accurate wire prediction, particularly in large images. Both MinMax and MaxPool are effective in encouraging prediction, which is shown by the drop in recall without either component, also shown in Figure~\ref{fig:visual}. Coarse condition, as described in Section~\ref{sec:segmentation}, is crucial in providing global context to the local network, without which the wire IoU drops significantly.

Table~\ref{table:thresholds} shows the wire IoUs and inference speed of our two-stage model as $\alpha$ changes. We observe a consistent decrease in performance as $\alpha$ increases. On the other hand, setting $\alpha$ to 0.01 barely decreases IoU, while significantly boosting inference speed, which means the coarse network is effectively activated at wire regions.
% We believe that in addition to providing a condition for refinement, the coarse module also acts as a suppressor to eliminate false positives, it does so by skipping potential false positives that would otherwise be mis-classified as wires. As a result, setting the optimal $\alpha$ allows the network to predict accurate segmentation masks, avoid over-prediction, and maintain a high inference speed all at the same time.

% Note that when $\alpha=0.0\%$ (refine on all windows), our model still outperforms a single sliding-window model (70.3\% vs. 69.0\%). This means that the coarse module indeed provides useful information to the fine module via the logit map, which further justifies our two-stage design.

% \subsubsection{Effectiveness of model components}

% We find that the global context provides vital information to the local branch during inference to avoid false positives. When the global branch softmax is fed into the local branch together with the local image, the network suppresses predictions at regions that look like wires, such as pavement cracks. We show these in Figure X. Quantitatively, the wire IoU drops significantly without conditioning on the global branch.

% \subsubsection{Effectiveness of Maxpool resize}

% \subsubsection{Effectiveness of MinMax input}

% Here we show the useful information from the minmax image. Figure X compares a model trained with/without minmax. As can be seen, for extremely bright/dark wires, minmax is able to emphasize this feature and ensure prediction in those areas where they are not easily seen after downsampling.



\subsection{Wire Inpainting Evaluation}
\vspace{-1mm}
We evaluate our wire inpainting model using the synthetic dataset. Results are shown in Table~\ref{exp:wire_inp}. Our model structure is highly related to LaMa~\cite{suvorov2022resolution}. The difference is the training data and the proposed color adjustment module to address color inconsistency. We also compare our methods with PatchMatch \cite{barnes2009patchmatch} based on patch synthesis, DeepFillv2~\cite{yu2019free} based on Contextual Attention, CMGAN~\cite{zheng2022cm} and FcF~\cite{jain2022keys} based on StyleGAN2~\cite{karras2020analyzing} and LDM~\cite{rombach2022high} based on Diffusion. Inference speed is measured on a single A100-80G GPU. Visual results on synthetic and real images are shown in Figure \ref{fig:wire_inp}. PatchMatch, as a traditional patch synthesis method, produces consistent color and texture that leads to high PSNR. However, it performs severely worse on complicated structural completion. StyleGAN-based CMGAN and FcF are both too heavy for wires that are thin and sparse. Besides, diffusion-based models like LDM tends to generate arbitrary objects and patterns. DeepFill and the official Big-LaMa both have severe color inconsistency issue, especially in the sky region. Our model has a balanced quality and efficiency, and performs well on structural completion and color consistency. 
Note that we use a tile-based method at inference time.
% with a window size of $512 \times 512$ and an overlap of $32$.
The reason the tile-based strategy can be employed is due to the wire characteristics: sparse, thin and lengthy. More high-resolution inpainting results are in the supplementary materials.

% \begin{figure}[h!]
% \centering
% \captionsetup{type=figure}
% \includegraphics[width=1.\linewidth]{figures/inpainting_result.pdf}
% \vspace{-6mm}
% \captionof{figure}{\textbf{Inpainting Comparison}. Our model performs well on complicated structure completion and color consistency, especially on building facades and sky regions containing plain and uniform color. 
% \vspace{-3mm}
% }
% \label{fig:wire_inp}
% \end{figure}


% \begin{table}[t]\setlength{\tabcolsep}{5pt}
% \setlength{\abovecaptionskip}{8pt}
% \centering
% \footnotesize
% % \scriptsize
% % \tiny

% %\vspace{-2ex}
% % \resizebox{\columnwidth}{!}{
% \begin{tabular}{r|c c c|c}
% \hline
% Model &PSNR$\uparrow$&LPIPS$\downarrow$&FID$\downarrow$ &Speed (s/img)\\ \hline
% %Photoshop\\ %should be easy to run batch testing
% PatchMatch \cite{barnes2009patchmatch}&50.29 &0.0294 & 5.0403 & -\\
% DeepFillv2 \cite{yu2019free} &47.01 &0.0374&8.0086 &0.009\\
% CMGAN \cite{zheng2022cm} &50.07 &0.0255 &3.8286 &0.141\\
% FcF \cite{jain2022keys}&48.82&0.0322&4.7848&0.048\\
% LDM \cite{rombach2022high} & 45.96 & 0.0401& 10.1687 & 4.280\\
% Big-LaMa \cite{suvorov2022resolution} & 49.63 & 0.0267& 4.1245 &0.034\\
% Ours (LaMa-Wire) & 50.06 & 0.0259 & 3.6950 &0.034\\
% \hline
% \end{tabular}
% \caption{Quantitative results of inpainting on our synthetic wire inpainting evaluation dataset (1000 images). Our model achieves the highest perceptual quality in terms of FID, and has a balanced speed and quality.}
% % }
% \label{exp:wire_inp}
% % \vspace{-4mm}
% \end{table}