%auto-ignore

% \section{Method} \label{sec:method}

% In this section, we describe our two-stage model architecture. Figure~\ref{fig:pipeline} shows the overall model structure. We will first discuss motivations of our two-stage design, then describe our model architecture and training details.

%In this section, we first discuss motivations of our two-stage model architecture, then describe our model architecture and training details. Figure~\ref{fig:pipeline} shows the overall model structure. 

\section{High-Resolution Wire Segmentation}
\label{sec:segmentation}
Wires appear visually different from common objects -- being thin, long, sparse and oftentimes partially occluded. 
We find the following two design choices crucial to building an effective wire segmentation system: 1) having a two stage framework so that coarse prediction from global context guides precise segmentation from local patches and 2) maximally preserving and augmenting image features and annotations of wires throughout the pipeline.  
% We thus propose a two-stage coarse-to-fine pipeline, in which we inject feature augmentation and architectural choices to maximally preserve the wire features throughout.

\subsection{The Two-stage Coarse to Fine Model}
%\yqe{
Figure \ref{fig:pipeline} shows the two-stage segmentation pipeline. It consists of a coarse and a fine module, which share an encoder $E$ and have their own decoder $D_C$ and $D_F$. Intuitively, the coarse module aims to capture the global contextual information from the entire image and highlight the image regions possibly containing wires. Conditioned on the predictions from the coarse module, the fine module achieves high-resolution wire segmentation by only looking at local patches likely containing wires.

Given a high-resolution image $I_\textrm{glo}$, we first bilinearly downsample it to $I_\textrm{glo}^{ds}$ with a fixed size $p\times p$ and feed it into the coarse module. The module predicts the global probability map $P_\textrm{glo} = \textrm{SoftMax}(D_C(E(I_\textrm{glo}^{ds})))$ containing the activation of the wire regions.% with rich global contextual information.

For each patch $I_\textrm{loc}$ of size $p \times p$ cropped from the full-resolution image $I_\textrm{glo}$, and the corresponding conditional probability map $P_\textrm{con}$ cropped from $P_\textrm{glo}$, we predict the local probability $P_\textrm{loc} = \textrm{SoftMax}(D_F(E(I_\textrm{loc}, P_\textrm{con})))$. 
%$M$ is a binary location mask indicating the patch region of $I_{loc}$ in $I_{glo}$ with 1, and elsewhere 0.
Note that $E$ is shared between the coarse and the fine module, thus it should take inputs with the same number of channels. Therefore, for the coarse module, we concatenate an additional zero channel with the input image to make the channel number consistent.
% We formulate our problem as a semantic segmentation task, so we can regard it as a pixel-wise classification problem. Each image pixel is labeled as 0 or 1 indicating background or wire pixel.

% We apply Cross Entropy (CE) loss to both the global $P_\textrm{glo} = \textrm{SoftMax}(Z_\textrm{glo})$ and local probability map $P_\textrm{loc} = \textrm{SoftMax}(Z_\textrm{loc})$, comparing with their ground truth annotations $G_\textrm{glo}$ and $G_\textrm{loc}$.

We apply Cross Entropy (CE) loss to both the global $P_\textrm{glo}$ and local probability map $P_\textrm{loc}$, comparing with their ground truth annotations $G_\textrm{glo}$ and $G_\textrm{loc}$.

\vspace{-2mm}
\begin{equation}
\begin{aligned}
%    \mathcal{L}_{glo} &= -\sum_{c\in C}\log\ Softmax(Z_{glo})_c \\
    %\mathcal{L}_{loc} &= -\sum_{c\in C}\log\ Softmax(Z_{loc})_c \\
    \mathcal{L}_\textrm{glo} &= CE (P_\textrm{glo}, G_\textrm{glo}) \\
    \mathcal{L}_\textrm{loc} &= CE (P_\textrm{loc}, G_\textrm{loc}) \\
\end{aligned}
\end{equation}
%Both $Z_{glo}$ and $Z_{loc}$ are trained by computing Cross Entropy losses of the logit maps after softmax against their respective ground truth wire annotations. 
The final loss $\mathcal{L}$ is the sum of the two:

\vspace{-3mm}
\begin{equation}
\begin{aligned}
    \mathcal{L} &= \mathcal{L}_ \textrm{glo} + \lambda \mathcal{L}_ \textrm{loc},
\end{aligned}
\end{equation}

%Given a high-resolution image $I$, the coarse module of our two-stage model aims to recognize semantics of the entire image and pick up wire regions at a coarse level. Looking at the entire image allows the coarse module to capture more contextual information. To fit the large images into a GPU for training and inference, images are first bilinearly downsampled, then fed into the coarse module to predict a global logit map $Z_{glo}$. We use $Z_{glo}$ as a conditional input to guide the fine module in the next stage.

%The fine module is conditioned on the output from the coarse module. This module takes in a local image patch $I_{loc}$ of the whole image at full resolution, the global logit map $Z_{glo}$ from the coarse module, and a binary location mask $M$ that sets the patch relative to the full image to $1$ and other regions to $0$. The fine module then predicts the local wire logit map $Z_{loc}$. Empirically, we find that concatenating the entire global logit map rather than cropping the logit map at the location of the image patch yields slightly improved results.

%The designs of the coarse and fine modules are conceptually the same as those in GLNet~\cite{glnet} and MagNet~\cite{magnet}, where a global network is trained on entire downsampled images and a local network is trained on higher-resolution image patches conditioned on some global features. However, unlike GLNet, where intermediate features are shared between the global and local branch bidirectionally, we opt for a simpler late fusion by concatenating the logit map directly to the fine module. We also only use two stages instead of up to four stage as done in MagNet, since a single fine module is already sufficient at refining annotations that are only several pixels thick, and additional stages can drastically increase inference time.

where we 
%empirically 
set $\lambda = 1$ for training. 
% During training, 
% we apply data augmentation including random scaling, horizontal flipping and photometric distortion to the full-resolution image to obtain the input $I_{glo}$. After that, 
% we randomly crop one local patch $I_{loc}$ with patch size $p=512$ from the augmented image. 
Similar to Focal loss~\cite{focal} and Online Hard Example Mining~\cite{ohem}, we balance the wire and background samples in the training set by selecting patches that contain at least 1\% of wire pixels. %We find that this simple constrained cropping approach yields better performances than other well known balancing methods, including Focal loss~\cite{focal} and Online Hard Example Mining~\cite{ohem}.

%The global image $I_{glo}$ is generated by simply downsampling this augmented image. We then generate the local image patch $I_{loc}$ by randomly cropping a 512$\times$512 window from the augmented image that contains at least 1\% wire pixels. This helps balance between wire and background pixels. We find that this simple constrained cropping approach yields better performances than other well known balancing methods, including Focal loss~\cite{focal} and Online Hard Example Mining~\cite{ohem}.

%We share the same feature extractor network between both the coarse and fine module, but train separate feature decoders. 
%To account for the additional inputs to the fine module, we expand the input channels of the feature extractor from 3 to 5. The last two channels are set to 0 when passing an image through the coarse module, and set to the global logit map $Z_{glo}$ and the binary location map $M$ for the fine module. 
To perform inference, we first feed the downsampled image to the coarse module, which is the same as training. 
% We then compute the global wire segmentation map by taking the $\mathrm{argmax}$ over the two classes for each pixel. 
Local inference is done by running a sliding window over the entire image, where the patch is sampled only when there is at least some percentage of wire pixels (determined by~$\alpha$). This brings two advantages: First, we save computation time in regions where there are no wires. Second, the local fine module can leverage the information from the global branch for better inference quality.

% The design of our two-stage pipeline is conceptually similar to GLNet~\cite{glnet} and MagNet~\cite{magnet}, where a global network is trained on downsampled images and a local network is trained on image patches at the original resolution. However, unlike GLNet whose intermediate features are shared between the global and local branch bidirectionally, we opt for a simpler late fusion by concatenating the probability map directly to the fine module. We only use two stages instead of four stages in MagNet. Our pipeline demonstrates its efficiency and simplicity while coping with wire-like objects which are only several pixels thick, and avoids high computational demands from some previous works.

\subsection{Wire Feature Preservation}
\label{sec:wire_feature_preserve}
As wires are thin and sparse, applying downsampling to the input images may make the wire features vanish entirely. To mitigate this challenge, we propose a simple feature augmentation technique by taking the min and max pixel luminance values of the input image over a local window. Either the local min or the max value makes the wire pixels more visually apparent. In practice, we concatenate the min- and max-filtered luminance channels to the RGB image and condition map, resulting in 6 total channels as input. We name this component MinMax.
%to form a 5-channel input.
% Unlike common object segmentation, with prominent textures and colors, wires, especially outdoor cables, tend to either be shadowed or reflective under sunlight. As a result, it is common to see either very bright or very dark wires in images. An intuitive approach is to find the min and max of an image before downsampling, and preserve this feature as input to the model.
% \todo{show an example patch of luminance min max filter:}

%\subsection{Motivation}
% We design a two-stage coarse-to-fine pipeline for wire-like object segmentation. This model structure 
% we divide our model into two major components -- a coarse module and a fine module. Our two-stage model design 
% is motivated by two observations. First, wires in our dataset are extremely long and thin, many spanning over several thousand pixels but only occupy several pixels across. Limited by the memory size of modern GPUs, we cannot simply pass the entire image at full-resolution to a model for inference. As a result, two separate modules are required for high-resolution inference, where the coarse module captures the entire wire and its surrounding contextual information at a lower resolution, and the fine module captures detailed textures of the wire at the original resolution.

% Second, we observe that wire pixels are sparse, where a typical high-resolution image contains a small percentages of wire pixels. This means that we can also use the coarse module to guide the fine module on what regions to capture the details. The fine module can save computation by only predicting segmentation masks where there are wires.

% \subsection{Global Condition}
% Many high-resolution segmentation methods share the idea of using global-local refinement module. But each method differs in details that are tailored to their targeted applications. 
% MagNet~\cite{magnet} trains a single model by randomly sampling global/local patches, and then trains a refinement module that takes as input the two predictions. This neglects the causal relationship between global and local features. The refinement module does not take into account any image feature. The recently proposed ISDNet~\cite{isdnet} use a shallow network and takes the entire image as input. This is proven effective in some high-resolution segmentation datasets such as DeepGlobe~\cite{deepglobe}, where resolution is fixed and manageable (5k$\times$5k). But the model is unable to scale to larger images. The shallow model, while being efficient, is limited in model capacity, especially with sparse labels.
% the fact that they take the entire image as input strictly limits their model to use lightweight networks as the shallow branch (i.e. STDC~\cite{stdc} with output stride=8), which has limited performance in cases where the target label is also small. 
% In fact, we were unable to train ISDNet with a stronger shallow network to produce high-resolution outputs.
% ~\cite{learning_downsample} attempts to learn a downsampling network, which we tried but found was detrimental for our thin and sparse wire masks. 
% As a result, 

Besides feature augmentations, we also adapt the architecture to maximally preserve the sparse wire annotations.
% Different from existing methods, we find it crucial to keep the image at a close-to-original resolution to maximally preserve the image features of wires. In addition, we find it similarly important to preserve the sparse wire annotations. 
% We use the global branch as a conditioning network for training both coarse and high resolution segmentation tasks rather than a simple coarse branch that is only learned for the coarse resolution. 
We propose to use ``overprediction" and achieve this by using max-pool downsampling on the coarse labels during training, which preserves activation throughout the coarse branch. We name this component MaxPool. We provide ablation studies for these components in Section~\ref{sec:results}.

\vspace{-5mm}
\section{High-Resolution Wire Inpainting}
\label{sec:inpainting}
Given a full-resolution wire segmentation mask estimated by our wire segmentation model, we propose an inpainting pipeline to remove and fill in the wire regions. Our approach addresses two major challenges in wire inpainting. First, recent state-of-the-art deep inpainting methods do not handle arbitrary resolution images, which is critical for high-resolution wire removal. Second, deep inpainting methods often suffer from color inconsistency when the background has uniform (or slowly varying) colors. This issue is particularly significant for wires, as they are often in front of uniform backgrounds, such as the sky or building facades. The commonly used reconstruction loss, such as L1, is not sensitive to color inconsistency, which further exacerbates this issue.

We thus revisit the efficient deep inpainting method LaMa \cite{suvorov2022resolution}. Compared with other inpainting models, LaMa has two major advantages. First, it contains the Fourier convolutional layers which enables an efficient and high-quality structural completion. This helps complete building facades and other man-made structures with fewer artifacts. Second, its high inference efficiency makes a tile-based inference approach possible for high resolution images. 

% To tailor LaMa for wire removal, we fix the input size of the model to $512\times512$ and train the model on an augmented dataset by including synthetic wire masks and cropped patches from full-resolution images. 
To address color inconsistency, we propose a novel ``onion-peel" color adjustment module. Specifically, we compute the mean of the RGB channels within the onion-peel regions $M_o = D(M, d) - M$ of the wire mask $M$, where $D$ is the binary dilation operator, and $d$ is the kernel size. The color difference for each channel $c \in {R, G, B}$ becomes $\textrm{Bias}_c = \mathbb{E}[M_o (x_c - y_c)]$, where $x$ is the input image, and $y$ is the output from the inpainting network. The final output of the inpainting model is: $\hat{y_c} = y_c + \textrm{Bias}_{c}$. Loss functions are then applied to $\hat{y_c}$ to achieve color consistency while compositing the final result $y_{out} = (1 - M) \odot x + M \odot \hat{y}$. 

% While running inference on full-resolution images, we apply a tile-based approach, by fixing the window size at $512\times 512$ with an $32$-pixel overlap. 
% This makes the model consistent in training and testing settings, and gives good textural and structural details for local regions.