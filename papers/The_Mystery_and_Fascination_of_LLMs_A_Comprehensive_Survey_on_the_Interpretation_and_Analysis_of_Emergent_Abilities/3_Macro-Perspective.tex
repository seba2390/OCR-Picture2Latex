% \input{sections/hanqi}
% \input{sections/lin}
\section{Interpreting Emergent Abilities from Macro Perspective}
Studies from the macro perspective centred on \textit{mechanistic interpretability}~\cite{Olah2022MI}. It involves delving into the inner mechanism of emergent abilities through different theoretical conceptual lenses such as linear regression formulation, meta-learning, latent space theory, and Bayesian inference.
% with goal of  reverse-engineering frontier LLMs
\subsection{Mechanistic Interpretability}
With the goal of reverse-engineering components of frontier models into more understandable algorithms, \citet{elhage2021mathematical} developed a mathematical framework for decomposing operations within  transformers~\cite{Vaswani2017AttentionIA}. They initially introduced the concept of ``\textit{induction heads}'' in a two-layer attention-only model to explain the functioning of ICL within transformers with Circuits~\cite{cammarata2020thread}.
They found that one-layer attention-only models perform relatively basic ICL in a crude manner, whereas two-layer models perform very general ICL using very different algorithms.
Specifically, they discovered that one-layer models essentially function as an ensemble of bigram and “skip-trigram” models that can be accessed directly from the model weights without running the entire model. Most attention heads in these models allocate significant capacity to copying mechanisms, resulting in very simple ICL.
In contrast, the two-layer models manifest a significantly powerful mechanism that employs more advanced, qualitative algorithms at inference time, referred to as ``\emph{induction heads}''. 
This allows them to perform ICL in a manner that resembles a computer program executing an algorithm, rather than merely referencing skip-trigrams. 
Building on this foundation, \citet{Olsson2022IncontextLA} later investigated the internal structures responsible for ICL by extending the concept of ``\emph{induction head}''~\cite{elhage2021mathematical}. They implemented  circuits consist of two attention heads: the ``\emph{previous token head}'', which copies information from one token to its successor, and the actual ``\emph{induction head}'', which uses this information to target tokens that precede the current one.
Their study revealed a phase change occurring early in the training of LLMs of various sizes. This phase change involves circuits that perform ``fuzzy'' or ``nearest neighbor" pattern completion in a mechanism similar to the two-layer induction heads. These circuits play a crucial role in implementating most ICL in large models.
One pivotal insight from \cite{Olsson2022IncontextLA} presented six arguments supporting their hypothesis that induction heads may serve as the primary mechanistic source of ICL in a significant portion of LLMs, particularly those based on transformer architectures.

While \citet{elhage2021mathematical} and \citet{Olsson2022IncontextLA} contribute to our understanding of ICL by probing the internal architecture of LLMs, it is important to note that their findings represent initial steps towards the comprehensive reverse-engineering of LLMs. It becomes particularly intricate when dealing with LLMs characterized by complex structures comprising hundreds of layers and spanning billions to trillions of parameters. This complexity introduces significant challenges. %, particularly in the context of addressing safety concerns.
Moreover, a substantial portion of their conclusions relies primarily on empirical correlations, which might be susceptible to confounding from various factors, thereby introducing potential vulnerabilities into their findings.

\subsection{Regression Function Learning}
Several research studies posited that the emergence of LLMs' competence in ICL can be attributed to their intrinsic capability to approximate regression functions for a novel query~$Q$ based on the demonstrations $D$.
\citet{Garg2022WhatCT} first formally defined ICL as a problem of learning functions and explored whether LLMs can be trained from scratch to learn simple and well-defined function classes, such as linear regression functions.
To achieve this, they generated examples $D$ using these functions, and trained models to predict the function value for the corresponding query $Q$.
Their empirical findings revealed that trained Transformers exhibited ICL abilities, as they manifested to ``learn'' previously unseen linear functions from examples, achieving an average error comparable to that of the optimal least squares estimator.
Furthermore, \citet{Garg2022WhatCT} demonstrated that ICL can be applied to more complex function classes, including sparse linear functions, decision trees, and two-layer neural networks, and posited that the capability to learn a function class through ICL is an inherent property of the model $M_\Theta$, irrespective of its training methodology.

Later, \citet{Li2023TransformersAA} extended \citet{Garg2022WhatCT} to interpret ICL from a statistical perspective. They derived generalization bounds for ICL, considering two types of input examples: sequences that are independently and identically distributed (i.i.d.) and trajectories originating from a dynamical system.
They established a multitask generalization rate of $1 / \sqrt{n T}$ for both types of examples, addressing temporal dependencies by associating generalization to algorithmic stability, abstracting ICL as an algorithm learning problem. 
They found that transformers can indeed implement near-optimal algorithms on classical regression problems with both types of input example by ICL.
Furthermore, they provided theoretical proof highlighting that self-attention possesses favourable stability properties, established through a rigorous analysis quantifying the influence of one token over another.

At the same time, \citet{Li2023TheCO} took a further step from the work of~\cite{Garg2022WhatCT} to gain a deeper understanding of the role of the softmax unit within the attention mechanism of LLMs. They sought to mathematically interpret ICL based on the softmax regression formulation represented as $\min _x||\left\langle\exp (A x), \mathbf{1}_n\right\rangle^{-1} \exp (A x)-b||_2$.
Their analysis revealed that the upper bounds of data transformations, induced either by a singular self-attention layer or by the application of gradient descent on an $L_2$ regression loss, align with the softmax regression formulation. 
This suggests a noteworthy similarity between models learned through gradient descent and those learned by Transformers, especially when trained solely on fundamental regression tasks using self-attention.

Conversely, ~\citet{Akyrek2023WhatLA} took a different approach by delving into the process through which ICL learns linear functions, rather than analysing the types of functions that ICL can learn.
Through an examination of the inductive biases and algorithmic attributes inherent in transformer-based ICL, they discerned that ICL can be understood in algorithmic terms, and linear learners within the model may essentially rediscover standard estimation algorithms.
More specifically, \citet{Akyrek2023WhatLA} provided a theoretical proof to support the claim that transformers can implement learning algorithms for linear models using gradient descent and closed-form ridge regression. They also empirically demonstrated that trained ICLs closely align with the predictors derived from gradient descent, ridge regression, and precise least-squares regression. 
They also introduced preliminary findings suggesting that ICL exhibits algorithmic characteristics, with both predictors of learners’ late layers encoding weight vectors and moment matrices in a non-linear manner.

Although these studies have either provided theoretical proofs or showcased empirical evidence interpreting the ICL ability of LLMs as a problem of learning regression functions, their conclusions are limited to simplified model architectures and controlled synthetic experimental settings. These findings may not necessarily apply directly to real-world scenarios.

\subsection{Gradient Descent \& Meta-Optimization}
In the realm of gradient descent, \citet{Dai2023WhyCG} adopted a perspective of viewing LLMs as meta-optimizers and interpreting ICL as a form of implicit fine-tuning. 
They first conducted a qualitative analysis of Transformer attention, representing it in a relaxed linear attention form, and identified a dual relationship between it and gradient descent. 
Through a comparative analysis between ICL and explicit fine-tuning, \citet{Dai2023WhyCG} interpreted ICL as a meta-optimization process. They further provided evidence that the transformer attention head possesses a dual nature similar to gradient descent~\cite{Irie2022TheDF}, where the optimizer produces meta-gradients based on the provided examples for ICL through forward computation.
Concurrently, \citet{Oswald2022TransformersLI} also proposed a connection between the training of Transformers on auto-regressive objectives and gradient-based meta-learning formulations. 
They specifically examined how Transformers define a loss function based on the given examples and, subsequently, the mechanisms by which Transformers assimilate knowledge using the gradients of this loss function. 
Their findings suggest that ICL may manifest as an emergent property, approximating gradient-based few-shot learning within the forward pass of the model.

However, it is worth noting that both of these investigations only focused on ICL within Transformer architectures, without considering other architectural variations or emergent capabilities, such as CoT and instruction following.
In addition, their analyses predominantly rely on a simplifed form of linear attention for qualitative assessment. 
This poses a challenge since the operation of standard Transformer attention, without any approximation, may be intricate. Therefore, there is a need for  more nuanced explorations into this mechanism in future studies.

\subsection{Bayesian Inference}
In their work, \citet{Xie2021AnEO} first provided an interpretation of ICL through the lens of Bayesian inference, proposing that LLMs have the capability to perform implicit Bayesian inference via ICL.
Specifically, they synthesized a small-scale dataset to examine how ICL emerges in LSTM and Transformer models during pretraining on text with extended coherence. 
Their findings revealed that both models are capable of inferring latent concepts to generate coherent subsequent tokens during pretraining. Additionally, these models were shown to perform ICL by identifying a shared latent concept among examples during the inference process. 
Their theoretical analysis confirms that this phenomenon persists even when there is a distribution mismatch between the examples and the data used for pretraining,  particularly in settings where the pretraining distribution is derived from a mixture of Hidden Markov Models (HMMs)~\cite{Baum1966StatisticalIF}. 
Furthermore,~\citet{Xie2021AnEO} observed that the ICL error decreases as the length of each example increases, emphasizing the significance of the inherent information within inputs. This goes beyond mere input-label correlations and highlights the roles of intrinsic input characteristics in facilitating ICL.

Following on,~\citet{Wang2023LargeLM} expanded the investigation of ICL by relaxing the assumptions made by~\citet{Xie2021AnEO} and posited that ICL in LLMs essentially operates as a form of topic modeling that implicitly extracts task-relevant information from examples to aid in inference.
\citet{Wang2023LargeLM} grounded their theoretical analysis in a setting with a finite number of demonstrations, and under a more general language generation process. 
Specifically, they characterized the data generation process using a causal graph with three variables and imposed no constraints on the distribution or quantity of samples. Their empirical and theoretical investigations revealed that ICL can approximate the Bayes optimal predictor when a finite number of samples are chosen based on the latent concept variable.
Moreover, \citet{Wang2023LargeLM} devised an effective practical algorithm for demonstration selection tailored to real-world LLMs. 

At the same time, \citet{Jiang2023ALS} also introduced a novel latent space theory extending the idea of~\citet{Xie2021AnEO} to explain emergent abilities in LLMs.
Instead of focusing on specific data distributions generated by HMMs, they delved into general sparse data distributions and employed LLMs as a universal density approximator for the marginal distribution, allowing them to probe these sparse structures more broadly.
\citet{Jiang2023ALS} demonstrated that ICL, CoT, and instruction-following abilities in LLMs can be ascribed to Bayesian inference operating on the broader sparse joint distribution of languages.
To shed light on the significance of the attention mechanism for ICL from a Bayesian view,~\citet{Zhang2023WhatAH} defined ICL as the task of predicting a response that aligns with a given covariate based on examples derived from a latent variable model. 
They established that ICL implicitly implements the Bayesian Model Averaging (BMA) algorithm, which is approximated by the attention mechanism.
Furthermore, they demonstrated that certain attention mechanisms converge towards the conventional softmax attention as the number of examples goes to infinity. 
These attentions, due to their encoding of BMA within their structure, empower the Transformer model to perform ICL.

Although their conclusions are insightful, there is a room for improvement.
Their findings might be influenced by various factors, such as the formats of the  examples, the nature of tasks, and the choice of evaluation metrics. 
Additionally, many of these studies are based on analyses conducted using small synthetic datasets, potentially restricting their relevance and applicability to real-world scenarios.
