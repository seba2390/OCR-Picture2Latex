\subsection{Model Architecture}
In this section, we will look into the transformer~\cite{DBLP:conf/nips/BrownMRSKDNSSAA20} components from the perspective of: 1) what makes transformers pay attention to their context, 2) how they can fit into different downstream tasks, and 3) under which circumstances, they come into play~\cite{von2023transformers}.

\subsection{Preliminary}
Before diving into the related works about how transformer components contribute to the core ability -- in context learning ability, we briefly give the formal notions of the transformer blocks, especially the Self-Attention Network (\textsl{SAN}). For each block, the input word index $t$ is firstly transformed by an Embedding layer $W_{E}$, then the \textsl{SAN} is essentially to incorporate the information from surrounding tokens via multiple heads, $h\in H$ and from itself. The resulting representation $x_1$ is finally transformed back to the output word index $T(t)$.

\begin{eqnarray}
    T(t)& =& W_{U}x_1,\nonumber  \\
    x_1& =& x_0+ \Sigma_{h\in H}h(x_0)\nonumber  \\
    x_0& =& W_E t
\end{eqnarray}

\citet{elhage2021mathematical} developed a mathematical framework for SAN decomposition and found that the OV circuit defined as $W_U W_E$ controls token embedding flows directly down its own path without intervening with its surrounding tokens' information. In contrast, the QK circuit in $h$-head, $A^{h} = \text{softmax}(t^{T}\cdot W_E^{T}W_{QK}^{h}W_{E}\cdot t)$ provides the attention scores between each query and key token, contributes to the so-called induction head, which only occurs in two-layer SAN, knows how the token was previously used and looks out for similar cases in the provided context, making the model adaptive to downstream tasks.

\subsection{Beyond Copy to Function Approximation}
The findings in \cite{elhage2021mathematical,olsson2022context} are limited to two-layer SAN (without MLPs) and copying mechanisms, although the empirical results in ~\citet{olsson2022context} show that induction heads are capable of more sophisticated tasks, for example, machine translation, and a specific type of abstract pattern matching in a 40-layer model with 13 billion parameters. 

Later, researchers give proof on how the SAN (without MLPs) can approximate linear algorithm~\cite{dai2023can,DBLP:conf/nips/0001TLV22,DBLP:conf/iclr/AkyurekSA0Z23} without gradient descent. In their settings, the ICL is framed as meta-learning and fed with a sequence of tasks $D$ and the ICL learner $\text{TF}_{\theta}$ is actually to find optimal parameter $\theta$ to approximate a certain algorithm $f$ (here, $f$ is linear algorithm): 
\begin{align*}
   D &=  \{x_i,f(x_{i}),...,f(x_{n}\}\\
\text{argmin}_{\theta} &= \sum_{i=1}^{n} L(f(x_{i}),\text{TF}_{\theta}(D))
\end{align*}



\citet{irie2022dual} presented that linear
layers optimized by gradient descent (GD) have a dual
form of linear self-attention network(LSN)-without softmax in attention weight calculation. The following works demonstrated that TF’s ability to implicitly execute gradient descent steps during inference could also be central to ICL, supporting their claims with empirical evidence. \citet{dai2023can} gave the dual form of linear model gradient descent and LSN output on in-context samples.~\citet{DBLP:conf/nips/0001TLV22} empirically demonstrated that TFs can learn basic function classes (e.g., linear functions, MLPs, and decision trees) via input
sample sequences.~\citet{DBLP:conf/iclr/AkyurekSA0Z23} consider the complete TF structures by incorporating the softmax activation, MLP with GeLU and empirically demonstrated that in computational level ICL can approximate various regression problems, such as KNN, One-pass stochastic gradient descent, One-step batch gradient descent and Ridge regression. Based on the prior results,~\citet{von2023transformers} further enables solving nonlinear regression tasks, i.e., sinWave fitting, within transformers by showing its equivalence to learning a linear model on deep representations. ~\citet{Xie2021AnEO,swaminathan2023schema,DBLP:journals/corr/abs-2304-09960} formulated the LLM as a latent variable model and demonstrated its ability of Bayesian inference.



% \subsection{Effects of Hyper-Parameters}
% \paragraph{Model Parameter}\citet{DBLP:journals/corr/abs-2212-04458} find that among factors determining the inductive
% bias of the model, \textsl{state-size} ((such as the hidden state size in a recurrent network) is a more crucial parameter than the model size for the emergence of in-context learning ability. Instead, more research results show that \textsl{model size} is the key factor for emergent ability, i.e., the scaling law~\cite{DBLP:journals/corr/abs-2303-03846,DBLP:journals/corr/abs-2001-08361,DBLP:journals/corr/abs-2203-15556}. For example, PaLM-540B are capable of overriding semantic priors in text classification tasks, while smaller counterparts are unable to do so~\cite{DBLP:journals/corr/abs-2303-03846}. 
% \paragraph{Prompt Parameter} In addition to the parameters in the model itself, the performance in ICL also heavily relies on prompt settings~\cite{DBLP:conf/acl/CaoLHL022,DBLP:conf/acl/LuBM0S22,DBLP:journals/csur/LiuYFJHN23,DBLP:journals/corr/abs-2212-04037,DBLP:conf/acl/SorensenRRSRDKF22}: such as \textit{wording}, \textit{perplexity}, \textit{order}, \textit{label distribution} and the \textit{mutual information} between the prompt and the
% language model’s output. More fine-grained, ~\citet{DBLP:journals/corr/abs-2307-03172} studied the how the position of the information affects the model's performance and came to the conclusion that the performance would be the highest if relevant information occurs at the beginning or end of the input context.  Most of the above studies are based on empirical experiments. In order to avoid the effects from confound, \citet{DBLP:conf/acl/CaoLHL022,DBLP:conf/acl/StolfoJSSS23} study the causal effects of prompt variable to model performances by backdoor criteria. On the other hand, some researchers explained the prompt variation from a cognitive perspective~\cite{DBLP:journals/corr/abs-2206-14576}. They designed the experiments to evaluate LLM's cognitive ability, including decision-making, information search, deliberation, and causal reasoning, by applying small perturbations to the prompts. On the other hand, researchers give theory proof of how the prompt parameters affect the performances based on simplified settings. For example, ~\citet{DBLP:journals/corr/abs-2305-19420,Xie2021AnEO} give theory proof that a larger length of the ICL input sequence, i.e., prompt (with possible demonstrations) length can help decrease the regret bound of ICL. 

\subsection{Strengthen by CoT.}
All the above works investigated the power of TFs from an expressively perspective, i.e., the function approximation. However, they pay little attention to the striking contributions from the carefully-designed prompt~\cite{DBLP:journals/tacl/JiangXAN20,DBLP:journals/csur/LiuYFJHN23,Wang2022TowardsUC}. In particular, the so-called Chain-of-Thought prompting (COT) plays an important role in complex reasoning tasks, e.g., mathematical arithmetic. By adding \textit{let's think step by step} to guide the model to generate intermediate output and derive final striking results based on it~\cite{Wei2022EmergentAO}. Empirically, researchers show the factors in CoT affecting the reasoning performances to better understand its working mechanism, such as relevance to the query, the reasoning order, etc~\cite{Wang2022TowardsUC}.~\citet{DBLP:journals/corr/abs-2307-13339} found that CoT would not increase the magnitude of the saliancy score of the important input tokens, but indeed enhance its robustness under different input perturbations. ~\citet{DBLP:journals/corr/abs-2305-15408} takes the first step towards theoretically answering how the complex reasoning tasks can be solved by CoT-assisted LLMs.

\subsection{Challenges and Future Work}
Research methods focusing on the problem of how the model structure and parameters contribute to the emergent abilities can be roughly divided into two directions: (1) ablating the research subject and other factors, and then empirically observing the differences in task performances. (2) Simplifying the real situations by either removing the obscure model components or evaluating with delicately designed synthetic experiments, in order to theoretically deduct the effects from principle elements to output. The findings of the first group of methods are often limited to specific evaluation cases, e.g., the variation of subjects in mathematics can affect LLM calculation
results~\cite{DBLP:conf/acl/StolfoJSSS23,shi2023large}. The conclusions from the second line of research hold true only in restricted situations, e.g., without noise data, nor regularisations~\cite{von2023transformers}. These issues are very common in other research areas, and what we can do to push the limits forward in this direction can be summarized as (a) proposing advanced TF structures with better computation efficiency based on the observation that the self-attention layer actually plays a similar role in optimization~\cite{zucchet2022beyond}. (b) Designing a systematic evaluation schema to evaluate the LLM's capability, one can refer to the cognitive literature for detailed ability measurements and implications. (c) Extending to more learning problems and incorporating more realistic settings in order to better approximate 
the real solutions. (d) Additionally, very limited studies theoretically illustrate the success of Chain-of-Thought, which is the key factor in understanding the LLM's reasoning ability.

\section{Training Data Matters}
Unlike prior work that explores implicit mechanisms behind ICL, some researchers study ICL via investigating how the pretraining data intrigue the emergent abilities.

% \subsection{Task Diversity}
% Although the previous studies~\cite{dai2023can,DBLP:conf/iclr/AkyurekSA0Z23,von2023transformers} have also shown that transformers can do linear regression in ICL. However, their assumptions are based on unlimited task diversity, that is the pretrained tasks are the same as true evaluation tasks.~\citet{wies2023learnability} show that unseen tasks can be efficiently learned via ICL
% in-context learning, when the pretraining distribution is a mixture of latent tasks. ~\cite{DBLP:journals/corr/abs-2212-04458,DBLP:journals/corr/abs-2306-15063} show the emergence of in-context learning with pretraining task diversity.~\cite{DBLP:journals/corr/abs-2212-04458} studies this issue in a more systematic way, they identify a task diversity threshold for the emergence of ICL by deriving the optimal estimator on linear regression problems. While lots of work~\cite{DBLP:journals/corr/abs-2306-15063} empirically observe improvement of MNIST classification performance with pretraining task diversity increased.~\citet{DBLP:journals/corr/abs-2301-07067} framed the ICL as a Multiple Task Learning(MTL) problem and identified the transfer learning risk on unseen tasks is governed by the task complexity and the number of MTL tasks. 

% \subsection{Pretraining Data Distribution}
% In addition to the abundance of pertaining tasks, some other important properties of the pertaining data also matter to the emergence of ICL. \citet{DBLP:conf/naacl/ShinLAKKKCLPHS22} discovered that ICL heavily relies on the \textsl{domain relevance}, but pretraining on a dataset related to a downstream task does not always reflect a competitive ICL performance. 
% ~\citet{DBLP:conf/nips/ChanSLWSRMH22} identify \textsl{training distribution—burstiness} and \textsl{occurrence of rare classes}—that are necessary for the emergence of ICL, although their simulation experiments are based on image data.~\citet{DBLP:conf/acl/HanSMTCW23} obtained the similar conclusions: (1) the supportive pretraining data do not necessarily have a higher domain overlap to downstream tasks, (2) the supportive
% pretraining data with \textsl{relative lower frequency} and 
% \textsl{long-tail tokens} have larger contributions. And they addressed the importance of \textsl{long-range} pertaining data.~\citet{DBLP:journals/corr/abs-2303-03846} study how ICL is affected by semantic priors and input-label mappings. Their empirical results show large improvements in flipped-labels presented in-context, implying that they are more capable of using in-context information to override prior semantic knowledge. ~\citet{gu-etal-2023-pre} enhance the language models’ in-context
% learning ability by pre-training the model on
% of intrinsic tasks in the contrastive training manner.~\citet{DBLP:journals/corr/abs-2305-19420} provide a generalisation bound given the number of token sequences and the length of each sequence in pretraining.

% Some studies focus on exploring the influences of particular training samples on the given test samples~\cite{Zhang2021CounterfactualMI,DBLP:conf/icml/KandpalDRWR23,Akyrek2022TowardsTF}, a.k.a, fact tracing. The key factor in such research line is to detect the influential training samples first, then exclude the identified samples in the training phase, so-called counterfactual memorization. More specifically, ~\citet{DBLP:conf/icml/KandpalDRWR23} observed the positive correlation between model's memorization ability and frequency of the pretraining samples. This conclusion is somewhat aligned with that the long-tail knowledge is essential to the difficult tasks in ICL mentioned before. 


\subsection{Challenges and Future Work}
The challenges mainly lie in how to select the relevant important training samples given the test samples and how to avoid retraining the model in a more effective way~\cite{}. Influence functions~\cite{hampel1974influence,DBLP:conf/icml/KohL17,DBLP:conf/nips/PruthiLKS20} are among the first methods to do this for neural networks, by estimating the marginal effect of a training example on the loss of a test example. Basically, there are divided into gradient-based and embedding-based method according to  their measurement units. However, these methods become less practical due to the in-feasibility to model parameters and intermediate output. Besides, the retrain methods that aim to calculate the prediction differences are time-costing. Therefore, how the evaluate the importance of particle training samples in a effective way can be a promising direction.

% For instance, \textcolor{red}{Forward-INF : Efficient Data Influence Estimation with Duality-based  Counterfactual Analysis.!!!fail to find citation, it is a workshop paper, waiting for author's response} 
