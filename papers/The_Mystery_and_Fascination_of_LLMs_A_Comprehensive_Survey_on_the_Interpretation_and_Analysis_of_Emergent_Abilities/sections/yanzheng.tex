\subsection{Demonstration Examples}
% With the scaling of model size and corpus size, LLMs have shown impressive performance on downstream tasks by conditioning on a prompt that contains a few input-label pairs from training data~(Demonstrations), which has been referred to as in-context learning~(ICL, \citet{Brown2020LanguageMA}).
Several recent  studies~\citep{Liu2021WhatMG,Min2022RethinkingTR,an2023context} have revealed the sensitivity of emergent abilities to alterations in order, format, and quantity of provided demonstrations.
\paragraph{Demonstration Order}
The order of the demonstrations has a significant impact on downstream task performance. \citet{Lu2021FantasticallyOP} showed that it be the deciding factor between achieving near state-of-the-art and random guessing. 
They designed demonstrations containing four samples with a balanced label distribution and conducted experiments involving all 24 possible permutations of sample orders.
The experimental results showed that the performance variations among different permutations exist across various model sizes, especially for smaller models.
Besides, is was observed that effective prompts are not transferrable across models, indicating that the optimal order is model-dependent, and what works well for one model does not guarantee good results for another model.

% Similar phenomena were discovered in .
\citet{Zhao2021CalibrateBU} identified a phenomenon that LLMs tend to repeat answers found at the end of  demonstrations, which they termed ``recency bias''.
Similarly, in multi-document question answering and key-value retrieval tasks,~\citet{Liu2023LostIT} made analogous observations.
These tasks involve identifying relevant information within lengthy input contexts.
The results showed that LLMs performed best when the relevant information is located at the beginning or end of their input contexts.
However, their performance degraded when they are forced to use information from the middle of their input.
In addition, they noted that model performance declines as the input context length increases, suggesting that current models struggle to effectively reason over their entire context window.
%\citet{Liu2023LostIT} studied the how the position of the information affects the model's performance and came to the conclusion that the performance would be the highest if relevant information occurs at the beginning or end of the input context.
Although these studies offer insights into how demonstration order influences emergent abilities, they do not delve into the underlying reasons of these obesrvations.
In an effort to investigate the impact of semantic similarity between ICL examples and test examples on downstream task, \citet{Liu2021WhatMG} proposed retrieving examples semantically similar to a test example for creating its demonstration.
They utilized the CLS embeddings from a pre-trained RoBERTa-large~\citep{Liu2019RoBERTaAR} model to represent sentences and assessed the semantic similarity between two sentences by computing the cosine similarity of their respective representations. 
For each test example, they identified the nearest $K$ neighbors from the training set and concatenated them in descending order of semantic similarity to create the demonstration. 
Their experiments on Web Questions~\cite{Berant2013SemanticPO} and Trivia Question Answering~\cite{Joshi2017TriviaQAAL} benchmarks showed that the default order performed slightly better than the reverse order.
However, the reverse order performed better on the Natural Questions \cite{Kwiatkowski2019NaturalQA} benchmark.
Consequently, the choice of order appears to be dependent on the specific dataset in use.
%is data-dependent.


\paragraph{Input-Label Mapping}
Some studies have been conducted to investigate how input-label mappings influence the performance of ICL.
\citet{Min2022RethinkingTR} revealed that substituting the correct labels of in-context examples in demonstrations with random labels only leads to a marginal decrease in performance across a variety of classification and multi-choice tasks.
They also conducted ablation experiments to investigate the impact of the number of correct labels on performance. Surprisingly, the results showed that the performance was not sensitive to the number of correct labels in demonstrations.
This led to the counter-intuitive conclusion that LLMs do not heavily rely on input-label mappings to perform tasks.

However,~\citet{Kim2022GroundTruthLM}, \citet{Wei2023LargerLM}, and \citet{Kossen2023InContextLI} disagreed with the claim put forth by \citet{Min2022RethinkingTR}.
\citet{Kim2022GroundTruthLM} pointed out that the claim exhibited over-generalization in two aspects: (1) Aggregating the mean performance across various datasets was found to be inadequate in capturing the insensitivity behavior observed within individual datasets. (2) The experimental setting lacked generalizability, and the results were sensitive to minor adjustments to the experimental setup.
To delve deeper into the topic of input-label mapping, \citet{Kim2022GroundTruthLM} introduced two novel metrics.
The first metric, Label-Correctness Sensitivity, quantifies the impact on downstream classification performance when a fixed amount of label corruption is introduced into the demonstration. 
The second metric, Ground-Truth Label Effect Ratio, assesses how much the presence of ground-truth labels improves the performance compared to a baseline with random labels. 
Their experimental results showed that the sensitivity exhibited significant variation across 17 datasets, with  the aggregate sensitivity considerably high. This indicated that label correctness does indeed affect downstream task performance.
Furthermore, \citet{Kim2022GroundTruthLM} suggested a strong correlation between sensitivity and task difficulty, revealing that LLMs displayed low sensitivity on challenging tasks.

\citet{Wei2023LargerLM} further explored how semantic priors and input-label mappings affect ICL.
They suggested that LLMs possess the ability to override semantic priors from pre-training in favour of input-label mappings from demonstrations. This explains why the performance of LLMs drops below random guessing when all the labels in the demonstrations are flipped.
They also found that smaller models experienced a less severe decline in performance because they lack the capacity to override semantic priors to the same extent.
More specifically, \citet{Wei2023LargerLM} conducted experiments where they replaced the labels with semantically unrelated labels. The results showed that the performance drop was more significant for small models compared to LLMs.
This led them to suggest that small models rely heavily on the semantic meanings of labels rather than learning the input-label mappings provided in the demonstrations.
\citet{Kossen2023InContextLI} also found that larger models are more sensitive to randomized labels, and they highlighted that LLMs can learn new input-label mappings from  demonstrations.
 
However, the ability to learn new input-label mappings can, at times, have adverse effect on performance.
\citet{Tang2023LargeLM} revealed that LLMs sometimes tend to exploit shortcuts within demonstrations for downstream tasks.
These shortcuts represent spurious correlations between in-context examples and their corresponding labels.
\citet{Tang2023LargeLM} designed several types of shortcuts,  and their experimental results showed that LLMs are ``lazy reasoners''. 
They relied heavily on the shortcuts within demonstrations to deduce the final answers. 
Furthermore, \citet{Si2023MeasuringIB} discovered that when presented with a set of non-specific demonstrations (For example, the labels are semantically unrelated), LLMs exhibited feature bias. 
This indicated that LLMs tend to favour one feature over another, even when both features are equally capable of predicting the label, as mentioned in the prompt.
For example, in a sentiment analysis setting, LLMs showed a significant bias towards predicting labels based on sentiment rather than shallow lexical features.
Nevertheless, feature bias has the potential to detrimentally affect performance when the model's feature bias does not align with the intended task.
\citet{Si2023MeasuringIB} suggested that certain interventions could help mitigate feature bias, such as employing natural-language instructions and incorporating label words that have semantic relevance to the intended feature.

To further investigate the underlying mechanism of how LLMs learn from input-label mappings, \citet{Wang2023LabelWA} conducted an extensive study into the workings of ICL from the perspective of information flow.
They computed saliency scores for each element within the attention matrix to unveil the significant token interactions.
The experimental results demonstrated that label words within demonstrations play a crucial role in this process. Specifically:
(1) During the processing of shallow computation layers, semantic information becomes concentrated within the representations of label words.
(2) The aggregated information contained within label words serves as a reference for the final predictions made by LLMs.
Their findings confirmed that label words can indeed have a substantial impact on the performance of the final task.

\paragraph{Chain-of-Thought Prompting}
% LLMs have shown superior performance on many downstream tasks by producing step-by-step reasoning, which is referred to as Chain-of-Thought (COT) reasoning.
% Chain-of-thought prompting is an approach that encourages LLMs to generate their intermediate rationales for solving a problem.
% It can considerably improve the multi-step reasoning ability of LLMs and is accomplished by providing the LLMs with some in-context examples in demonstrations where the reasoning process is explicitly outlined.
Some studies have focused on exploring the impact of COT prompting on LLM performance.
\citet{Wang2022TowardsUC} found that the validity of the reasoning process in demonstrations has only a minimal impact on performance. 
To assess this, they constructed invalid reasoning processes manually for all in-context examples. Surprisingly, the experimental results showed that LLMs can retain 80-90$\%$ of their performance even when presented with invalid reasoning steps in demonstrations.
They also found that the coherence of the reasoning process and its relevance to the query are significantly more crucial factors for the effectiveness of CoT.

Regarding the explanations generated by LLMs, \citet{Turpin2023LanguageMD} found that CoT explanations produced by LLMs can occasionally misrepresent the true underlying rationales behind their predictions.
They introduced two types of bias in the prompt design to investigate this phenomenon.
The first bias involves consistently reordering the multiple-choice options of in-context examples to make the answer 'A'.
The second bias entails including the suggested answers directly in the prompt.
The experimental results indicated that, in both bias scenarios, LLMs tend to provide answers aligned with stereotypes and generate explanations that do not faithfully support the answer.
Furthermore, there was a large drop in performance when comparing biased demonstrations to unbiased demonstrations.

% \subsubsection{Future direction}
% The current research is limited to specific closed-set tasks, which include classification and multiple-choice tasks.
% Additionally, it encompasses the examination of some template-based reasoning tasks.
% Future research endeavours could broaden their scope to encompass more intricate and open-set tasks, such as generation and tasks involving complex reasoning.




































