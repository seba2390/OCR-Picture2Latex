% \section{In-context Learning Applications}

% In-context learning applications of Large Language Models (LLMs) like GPT-3 have expanded our understanding and utilization of such models in varied real-world scenarios. This section will delve into the recent advancements and experiments done in the domain, shedding light on how LLMs are being applied, augmented, and furthered to handle few-shot prompted tasks, and enhance emergent abilities.

% \subsection{Few-Shot Prompted Tasks}

% \textbf{Augmentation:} As we move into the realm of harnessing the capabilities of LLMs, \citep{Wahle2022HowLL} stands out by proposing a unique technique using few-shot examples to prompt GPT-3. This enabled the generation of machine-paraphrase plagiarism data extracted from freely accessible scientific articles. Simultaneously, \citep{Agrawal2022LargeLM} undertakes an ambitious exploration of GPT-3's prowess in zero- and few-shot information extraction from clinical texts. What's noteworthy is that this approach sidesteps the need for domain-specific fine-tuning. Their research also extends to various NLP tasks demanding structured outputs, touching on areas like span identification, sequence classification at the token level, and relation extraction.

% \textbf{Reasoning:} The subtlety of reasoning is captured in \cite{Madaan2022LanguageMO}, where the focus is on improving the structural accuracy of commonsense reasoning, hinging on few-shot examples applied on code-generation LLMs. Intriguingly, \citep{Spiliopoulou2022EvEntSRE} tests the waters of LLMs' underlying and worldly knowledge. They probe the model's capability to reason event implications using two datasets while also evaluating the effects of varied prompting techniques on model performance. Meanwhile, \cite{Min2022RethinkingTR} demystifies the paradigm of in-context learning, emphasizing that for large language models, having ground truth demonstrations might not be mandatory for efficient in-context learning.

% \subsection{Augmented Prompting Strategies}

% \textbf{Provide External Knowledge:} The question of how LLMs utilize their vast internal knowledge has been the subject of extensive research. \cite{Li2022LargeLM} takes the lead by exploring LLMs' controllability and robustness, unveiling their shortcomings in leveraging knowledge from input contexts. Their proposed solution – a context-aware finetuning methodology – infuses counterfactual and irrelevant contexts into standard supervised datasets, addressing the challenge head-on. Similarly, \cite{Lal2022UsingCK} delves deeper into the sphere of commonsense knowledge, proposing techniques to supplement the model with additional context to better answer ``why'' questions.

% \textbf{Data Augmentation:} In an artful confluence of technology and literature, \citep{Chakrabarty2022HelpMW} introduced CoPoet, a system promoting collaborative writing with LLMs. On a different tangent, \citep{CallisonBurch2022DungeonsAD} presents a novel perspective by viewing textual-based games through the lens of a dialogue system, demonstrating LLMs' capability to pinpoint game stages via dialogue.

% \textbf{Chain-of-thought like methods:} Handling intricate queries that demand latent decisions is a formidable challenge, one that \cite{Dua2022SuccessivePF} tackles by iteratively simplifying a complex task. The process involves breaking down the task, resolving it, and repeating the cycle till a conclusion is reached. This approach is mirrored by \cite{Patel2022IsAQ} who propose question decomposition to aid LLMs in understanding and answering complex queries.

% \textbf{Reasoning:} The art of questioning is an avenue that \cite{Shridhar2022AutomaticGO} ventures into, focusing on sequential question generation to aid in solving mathematical word problems. In the broader context of understanding social dynamics and inferring mental states, \cite{Sap2022NeuralTO} exposes the current limitations of LLMs like GPT-3.

% \subsection{Methods to Enhance Emergent Ability}

% \textbf{Training Methods:} A novel approach to unsupervised masking strategy, InforMask, is introduced by \cite{Sadeq2022InforMaskUI}, leveraging Pointwise Mutual Information to pinpoint and mask the most informative tokens, showing immense potential in question-answer tasks. Meanwhile, \cite{Ke2022ContinualTO} puts forth the idea of perpetually extending LLMs via post-training with a sequence of domain-specific unlabeled corpora. Lastly, \cite{Zhang2022ActiveES} introduces a reinforcement learning algorithm, identifying policies that handpick generalizable demonstration examples, a feat that holds promise in tasks unseen during training.

\subsection{Pre-training Data}
% The investigation of the pre-training stage's influence on emergent abilities starts from \citep{DBLP:conf/nips/BrownMRSKDNSSAA20}, in which they introduced and delved into the capabilities of GPT-3. Their study shows that GPT-3 can perform various tasks without requiring task-specific fine-tuning. The researchers employed three popular ICL methods in their paper: zero-shot, one-shot, and few-shot. In this approach, instead of inundating the model with vast amounts of training data, GPT-3 was provided with just a few examples to help it understand or even without any examples in zero-shot cases and execute the task. \citet{DBLP:conf/nips/BrownMRSKDNSSAA20} comprehensively experimented on diverse datasets spanning nine categories of natural language tasks. Their finding suggests that zero-shot performance improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are more proficient at ICL. Besides, \citet{DBLP:conf/nips/BrownMRSKDNSSAA20} designed algorithms to detect the influence of ICL performance on contaminated pre-training data. Interestingly, they found that the language model is relatively insensitive to contamination. Still, they also observed it may bring some performance inflation on small parts of the dataset span tasks like reading comprehension, translation, etc.
%Several literature 
Some studies have suggested that factors related to pre-traning data such as data domain, data term frequency, and data distribution~\citep{Chan2022DataDP, Razeghi2022ImpactOP}, are crucial elements influencing the development of emergent abilities.
\paragraph{Data Domain} 
\citet{Shin2022OnTE} conducted a study to explore the variations of ICL performance concerning the \textit{domain source} and the \emph{size} of the pre-training corpus, focusing primarily on the Korean lexicon. 
They utilized seven subcorpora from the HyperCLOVA corpus \cite{Kim2021WhatCC} to pre-train various language models and evaluated these models on Korean downstream tasks. 
Interestingly, \citet{Shin2022OnTE} found that the size of the pre-training corpus does not always determine the emergence of ICL. 
Instead, the domain source of the corpus significantly influences ICL performance. 
For example, language models trained with subcorpora constructed from blog posts exhibited the best ICL capability. 
This phenomenon may be attributed to the greater token diversity presented in the blog posts corpus compared with other sources like news. Moreover, their experiments highlighted that combining multiple corpora can lead to the emergence of ICL, even if individual corpora did not produce such learning on their own. 
Surprisingly, \citet{Shin2022OnTE} also found that a language model pre-trained with a corpus related to a downstream task did not always guarantee competitive ICL performance. 
For instance, a model trained on a news-related dataset~\citep{Park2021KLUEKL} showed  superior performance in zero-shot news topic classification, but its few-shot performance was not superior.
In a similar vein, \citet{Han2023UnderstandingIL} reached similar conclusions and highlighted the importance of long-range pre-training data.
Their experimental observation can be summarized into two aspects: (1)  supportive pre-training data do not necessarily need to have a higher domain overlap with downstream tasks, and (2) supportive pre-training data with relatively lower frequency and long-tail tokens make larger contributions to the model's capabilities.
Furthermore, ~\citet{Raventos2023PretrainingTD} empirically observed improvements in MNIST classification performance as pre-training data task diversity increases. 
\vspace{-3mm}
\paragraph{Data Term Frequency}
A more comprehensive investigation into the factors affecting  emergence of the ICL based on the pre-training term frequencies has been proposed by \citet{Razeghi2022ImpactOP}. 
The authors focused particularly on a crucial type of reasoning in LLMs -- numerical reasoning in few-shot settings; and examined the extend to which the frequency of terms from the pre-training data correlates with model performance in these situations. 
Their analysis focused on the prevalence of numerical reasoning tasks within the training instances and established a connection between frequencies and reasoning performance. This connection is quantified  by introducing the ``performance gap'', which is defined as the accuracy of terms appearing more than 90\% of the time minus the accuracy of terms appearing less than 10\% of the time. They conducted their experiments using GPT-based language models trained on the Pile dataset \citep{Gao2020ThePA}, ranging in size from 1.3B to 6B parameters. Evaluation was carried out on 11 datasets spanning three types of mathematical reasoning tasks: Arithmetic, Operation Inference and Time-Unit Conversion. 
The findings consistently show that models perform better in instances where terms from the pre-training data are more prevalent \citep{Razeghi2022ImpactOP}. 
In some scenarios, there is a substantial performance gap of over 70\% between the most frequently occurring terms and the least frequently occurring terms. 
The significant performance difference raises questions about the actual generalization capabilities of these models beyond their pre-training data. 
\citet{Razeghi2022ImpactOP}'s observations suggest that the more prevalent content included in the pre-training data may exert an influence on the emergent abilities, and it is possible that these language models are not actually reasoning to solve arithmetic tasks. 
In line with this research, ~\citet{Kandpal2022LargeLM} also observed a positive correlation between model's memorisation ability and the frequency of the pre-training samples.
\vspace{-2mm}
\paragraph{Data Distribution} 
\citet{Chan2022DataDP} provided an interpretation of the emergent abilities of transformers from the perspective of training \textit{data distribution}. They conducted an experiment based on image classification to explore the emergent capabilities of language models, particularly in the context of performing few-shot learning without explicit training. 
They manipulated the distributional properties of the training data, such as burstiness and the presence of rare classes. They observed the impacts of these manipulations on ICL using models like transformers and recurrent networks. %Similar to \citet{DBLP:conf/naacl/ShinLAKKKCLPHS22}'s finding on the diversity of the lexical may benefit the emergence of ICL, 
\citet{Chan2022DataDP} also emphasised that ICL is more pronounced when item meanings or interpretations are dynamic rather than static. They highlighted that natural language and other naturalistic data sources exhibit these dynamic properties, which differ from the uniform distributions typically used in standard supervised learning. Through their experiments, they uncovered a trade-off in transformer models between ICL and in-weights learning, which relies on information stored through slow, gradient-based updates \citep{Chan2022DataDP}.
However, subsequent experiments showed that both ICL and in-weights learning could coexist in a model trained on data with a skewed Zipfian marginal distribution \citep{Zipf1949HumanBA}, a distribution commonly observed in the frequency of words in languages.
Interestingly, while transformers exhibited ICL when trained on specific data distributions, recurrent models like LSTMs and RNNs did not. Finally, they revealed that non-uniform training distributions can cause the induction of the emergence of new capabilities. Their work highlights the importance of architecture and training data distribution in the emergence of ICL in LLMs. 
To understand how data distribution affect the effectiveness of ICL, \citet{Wies2023TheLO} theoretically demonstrated that unseen tasks can be efficiently learned via ICL when the pre-training data distribution comprises a mixture of latent tasks.
% \citet{DBLP:journals/corr/abs-2301-07067} framed the ICL as a Multiple Task Learning(MTL) problem and identified the transfer learning risk on unseen tasks is governed by the task complexity and the number of MTL tasks.

% \citep{DBLP:conf/nips/BrownMRSKDNSSAA20, Wei2022EmergentAO} pointed out that LLMs' emergent abilities on a range of downstream tasks improve while increasing the scale of language models. 
% However, new line of studies discovered that the emergence of ICL are not limited by the \textit{model scale} factor. Other factors such as data distribution \citep{Chan2022DataDP, Razeghi2022ImpactOP}, source of data \citep{DBLP:conf/naacl/ShinLAKKKCLPHS22, Razeghi2022ImpactOP, Power2022GrokkingGB}, and model architecture \citep{Chan2022DataDP, Xie2021AnEO} may also plays an important role at the pre-training stage. Researchers proposed to investigate the emergent ability either theoretically \citep{Wei2022EmergentAO, Xie2021AnEO} or empirically \citep{Chan2022DataDP,DBLP:conf/naacl/ShinLAKKKCLPHS22,Razeghi2022ImpactOP,Power2022GrokkingGB}. %Interestingly, most of those interpretations focus on LLMs' reasoning capability. 

% In this section, we first summarise the existing approach to interpret the gain of LLMs' emergent abilities in the pre-training stage. Then, we discuss the existing challenges and barriers in current research. Finally, we present some possible future work directions for future interpretation of the emergent abilities.

\subsection{Pre-training Model}
\citet{Wei2022EmergentAO} embarked on an investigation into the emergent abilities of LLMs. 
Their approach to interpreting this phenomenon involved conducting a comprehensive survey of existing literature and analyzing the unpredictable nature of how certain abilities manifest as these models scale \citep{Brown2020LanguageMA}. 
\citet{Wei2022EmergentAO} emphasized that while \textit{model scale} has been correlated with LLM performance, it is not the sole determinant. Task-specific abilities can also be examined by considering a language model’s performance (perplexity) on general text corpora, such as WikiText103. 
Their experiments showed that, despite having fewer parameters, the PaLM 62B model outperformed LaMDA 137B and GPT-3 175B in certain tasks. This suggests that other factors, like \textit{high-quality data} and \textit{architectural differences}, also play a role.
Moreover, continued pre-training on different objectives, like the mixture-of-denoisers objective, has shown potential in enabling emergent abilities \citep{Tay2022UnifyingLL}. 
Research is also advancing to make these discovered abilities accessible for smaller-scale models.
For instance, instruction-based fine-tuning showed potential in smaller models with different architectures. 
Additionally, the emergence of syntactic rule-learning can be triggered by threshold frequencies in training data, similar to "aha" moments in human learning \citep{Abend2017BootstrappingLA, Zhang2020WhenDY}.
 While the majority of research agrees that model scale is a key factor for emergent abilities. 
 \citet{Kirsch2022GeneralPurposeIL} presented an interesting perspective. They found that among the factors determining the inductive bias of the model, the state-size (such as the hidden state size in a recurrent network) is a more crucial parameter than the overall model size for the emergence of ICL ability.


 % ~\citet{Kirsch2022GeneralPurposeIL} studies this issue in a more systematic way, they identify a task diversity threshold for the emergence of ICL by deriving the optimal estimator on linear regression problems. 

%%



% \citet{Chan2022TransformersGD}'s study also delves into the generalization behaviours of transformer models, focusing on the difference between in-weight learning and in-context learning. 
% More detailly, they want to understand whether transformer models generalize using rule-based methods (e.g. relying on minimal features) or exemplar-based methods (e.g. relying on the similarity to training instances). They designed their experiment on both syntactic and textual data with a ``partial exposure'' test \citep{Dasgupta2021DistinguishingRA} where stimuli have two features, and the model is trained on certain combinations and then evaluated on a held-out combination.
% \citet{Chan2022TransformersGD} find out that transformers display a distinct difference in their generalization from in-context vs. in-weights information.
% Specifically, they found that transformers lean towards exemplar-based generalization from in-context details, while they favour rule-based generalization from in-weights information.
% However, large transformer models pre-trained on language showed more substantial rule-based generalization from in-context information.

% A more theoretical framework to explain the emergence of ICL when pre-training with documents has long-range coherence has been developed by \citet{Xie2021AnEO}, especially when there's a distribution mismatch between prompts and pre-training data. The authors expanded on the probability of a given output based on a set of examples and a test input. Under their framework, They derived heuristics and made several assumptions related to the bounds on delimiter transitions, distribution shifts, and regularity. \citet{Xie2021AnEO} find out that language models tend to infer from a latent document-level concept to generate new tokens during pre-training, and language models tend to match this shared latent concept during the inference to emerge the ICL behaviour. Their theoretical investigation is grounded in a Bayesian inference perspective. \citet{Xie2021AnEO} carried out experiments on both transformers and LSTM-based models trained with a small-scale synthetic dataset (GINC) based on a uniform mixture of Hidden Markov Models (HMMs). During the model inference stage, they generate textual prompts from a sampled latent concept to validate their theoretical framework. Their quantitative and theoretical analysis highlights that the in-context predictor is optimal as the number of in-context examples increases. ICL tend to emerge when the pre-training data distribution is a mixture of HMMs.



