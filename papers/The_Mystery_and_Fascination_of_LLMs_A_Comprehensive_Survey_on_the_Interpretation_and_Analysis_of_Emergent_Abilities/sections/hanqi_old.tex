\section{Transformers Learn In-Context}
In recent years Transformers (TFs)~\cite{} have demonstrated their superiority in many benchmarks and various tasks of NLPs~\cite{}, especially its in-context learning ability--flexibly adjust their prediction based on
additional data given in context, i.e., the input sequence on the fly~\cite{von2023transformers}. It is of great interest to understand what makes Transformers pay attention to their context, how can they fit into different downstream tasks without gradient descent, and under which circumstances, they come into play.

% \paragraph{Attention Circuit in TFs.} Researchers start with the universal component within all the large language models, Self-Attention Network (SAN).~\citet{elhage2021mathematical} develop a mathematical framework for SAN decomposition. To demonstrate the ideas of the cleanest form of attention mechanism, they omit the MLP layers. The one-layer self-attention layer can be decomposed as:
% \begin{align}
%     &T(t) = W_{U}x_1, \\
%     &x_1 = x_0+\sum_{h\in H}h(x_0) \\
%     &x_0 = W_E t
% \end{align}
% Where the $t$ is the index of the input token, $W_{E}$ is the word embedding weight, and $W_U$ is the unmebdding weight used to readout the final logits $T(t)$. So the OV circuit is defined as $W_U W_E$, which dominates the direct path where the token embedding flows directly down its own path without intervening with its surrounding tokens' information. In contrast, the QK circuit provides the attention scores between each query and key token as shown in follows:
% \begin{align}
%     A^{h} = \text{softmax}(t^{T}\cdot W_E^{T}W_{QK}^{h}W_{E}\cdot t)
% \end{align}
% Where $A^{h}$ describes which tokens the input token prefers to attend to.

\paragraph{From learned skip-grams to copying on the fly.} Empirically, the author's search for large entries in these matrices reveals lots of interesting behaviour. For the one-laye SAN, the OV circuit increases the probability of the exact source token or similar tokens, e.g., \textit{perfect}$\rightarrow\{$\textit{perfect}, \textit{super}$\}$. And this situation is further verified by the situations, where many eigenvalues of the OV matrices are positive. The QK circuit then only attends back to tokens where bigram-ish statistics make them seem plausible. 

In-context capability is beyond the statistical significance in training corpus as it can repeat random sequences provided on the fly, which can be partly explained by the induction head. Empirically, this induction head, only occurs in two-layer SAN, knows how the token was previously used and looks out for similar cases, making it robust to distribution shift and flexible to downstream tasks. This observation can be attributed to the K-composition on the previous token. Specifically, the QK decomposition term in two-layer SAN: $\text{Id}\otimes A^{h-1}\otimes W$ contains $A^{h-1}$ which denotes the attention pattern attending to the source tokens. (In attention calculation for token $x_T$, all the tokens $t<T$ are source tokens). In one word, beyond capturing the statistical correlation built during training, the induction head is capable of copying similar patterns that occurred in the provided context as it will increase attention scores when the previous token before the source position is the same as the destination token.

\paragraph{Beyond Copy to Linear algorithm in forward pass, as gradient descent.}
Although the induction head can explain the preliminary capability of in-context learning, the findings are limited to two-layer SAN (without MLPs) and copying mechanisms and the in-context ability is far more sophisticated than that\citet{olsson2022context}. \citet{olsson2022context} show some interesting examples of literal copying, translation, and a specific type of abstract pattern matching in a 40-layer model with 13 billion parameters. 

In addition to copy, researchers firstly give proof on how the SAN (without MLPs) can approximate linear algorithm~\cite{dai2023can,DBLP:conf/iclr/AkyurekSA0Z23} and they frame the ICL as \textbf{meta-learning}. In their settings, ICL is fed with a sequence of tasks $D$ and the ICL learner $\text{TF}_{\theta}$ is actually to find optimal parameter $\theta$ to approximate a certain algorithm $f$ (here, $f$ is linear algorithm): 
\begin{align*}
   D &=  \{x_i,f(x_{i}),...,f(x_{n}\}\\
\text{argmin}_{\theta} &= \sum_{i=1}^{n} L(f(x_{i}),\text{TF}_{\theta}(D))
\end{align*}


\citet{irie2022dual} present that linear
layers optimized by gradient descent (GD) have a dual
form of linear self-attention network(LSN)-without softmax in attention weight calculation. \citet{dai2023can} gave the dual form of linear model gradient descent and LSN output on in-context samples.~\citet{DBLP:conf/iclr/AkyurekSA0Z23} consider the complete TF structures by incorporating the softmax activation, MLP with GeLU and gave the similar conclusion: one-step gradient descent process of a linear regression model can be implemented by such a Transformer. Beyond the theory that the fixed transformer parameterized Transformers are expressive enough to simulate these learning algorithms: gradient descent on OLS and closed form for OLS minimizer,~\citet{DBLP:conf/iclr/AkyurekSA0Z23} empirically explain ICL at the computational level by identifying the kind of algorithms to regression problems, such as KNN, One-pass stochastic gradient descent, One-step batch gradient descent and Ridge regression. In evaluation, they not only compare the prediction results derived from the TFs and linear algorithms, they also evaluate the agreement on the learned parameters to further elaborate their equivalence.


\paragraph{From linear to nonlinear algorithm}
Based on the prior results,~\citet{von2023transformers} further enables solving nonlinear regression tasks, i.e., sinWave fitting, within Transformers by showing its equivalence to learning a linear model on deep representations. Similar to \citet{DBLP:conf/iclr/AkyurekSA0Z23}, they are not limited to comparing the predictions agreement in the evaluation of the equivalence of TFs and linear algorithm, they proposed to evaluate the interpolation, out-of-distribution validation tasks and results of Repeating the LSA update. Moreover, they compared the loss of TFs and GD and found before the TFs loss jumps to the GD, the token transformed by the first self-attention layer becomes notably dependent on the neighbouring token. This explains the coping DBLP:conf/acl/CaoLHL022,DBLP:conf/acl/StolfoJSSS23mechanism of the Transformer’s first layer to merge the source token and its previous tokens achieved by the softmax operator in $A^{h-1}$. Then, in the second layer, the Transformer performs a single step of GD. These observations further show that such copying allows the Transformer to proceed by emulating gradient-based learning in the second or deeper attention layers. 

\section{Training Data Matters}
\subsection{Task Diversity}
Although the previous studies~\cite{dai2023can,DBLP:conf/iclr/AkyurekSA0Z23,von2023transformers} have also shown that transformers can do linear regression in ICL. However, their assumptions are based on unlimited task diversity, that is the pretrained tasks are the same as true evaluation tasks. ~\cite{DBLP:journals/corr/abs-2212-04458,DBLP:journals/corr/abs-2306-15063} show the emergence of in-context learning with pretraining task diversity.~\cite{DBLP:journals/corr/abs-2212-04458} studies this issue in a more systematic way, they identify a task diversity threshold for the emergence of ICL by deriving the optimal estimator on linear regression problems. While lots of work~\cite{DBLP:journals/corr/abs-2306-15063} empirically observe improvement of MNIST classification performance with pretraining task diversity increased. 

\subsection{Pretraining Data Distribution}
~\citet{DBLP:conf/nips/ChanSLWSRMH22} identify several properties of the training distribution—burstiness and occurrence of rare classes—that are necessary for the emergence of ICL.~\citet{DBLP:journals/corr/abs-2303-03846} study how ICL is affected by semantic priors and input-label mappings, focusing on differences across model scale.
\textcolor{blue}{TODO ...}
\subsection{Influence of Particular Training samples}
Some studies focus on exploring the influences of particular training samples on the given test samples.~\citet{DBLP:conf/icml/KandpalDRWR23} observed the positive correlation between model's memorization ability and frequency of the pretaining samples. Specially, they calculate the frequency of relevant training samples and the prediction accuracy of the given test samples. More general research direction is to detect the relevant training samples first, then exclude the identified samples in the training phase, so-called counterfactual memorization~\cite{Zhang2021CounterfactualMI,}. The differences mainly lie in how to select the relevant important training samples given the test samples and how to avoid retraining the model in a more effective way~\cite{}.
For instance, \textcolor{red}{Forward-INF : Efficient Data Influence Estimation with Duality-based  Counterfactual Analysis.!!!fail to find citation, it is a workshop paper, waiting for author's response} 

\section{Large Language Model are Latent Graphical Model}
~\cite{Xie2021AnEO} \\

~\cite{swaminathan2023schema} propose a sequence learning model based on action-->latent variable-->observed variable Generation process.

~\cite{DBLP:journals/corr/abs-2304-09960} \\