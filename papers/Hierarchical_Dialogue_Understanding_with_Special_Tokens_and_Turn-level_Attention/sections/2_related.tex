\section{Related Work}\label{related_work}
% \subsubsection{Multi-Turn Dialogue Understanding}
\textbf{Multi-Turn Dialogue Understanding}. Various tasks and corresponding benchmarks are proposed to evaluate the capacities of dialogue understanding models. Dialogue-based relation extraction (RE) is a classification task that assigns a pair of entities a relation label in a dialogue. Focusing on the word level,  \cite{xue2021gdpnet} constructed a multi-view graph with words in the dialogue as nodes and proposed Dynamic Time Warping Pooling to automatically select words in interest. SimpleRE \citep{SimpleRE} designed a novel input sequence format and utilized a Relation Refinement Gate to filter the semantic representation which is later fed into the classifier. TUCORE-GCN \citep{zahiri:18a} used a heterogeneous dialogue graph to encode the interaction between speakers, arguments, and turns across the dialogues.
% \citet{christopoulou-etal-2019-connecting} constructed an edge-oriented graph model to encode the dialogue as a graph with nodes and edges of different functions and applied an inference mechanism on the graph edges to recognize the internal relationship. By constructing a mention-level graph and an entity-level graph, \citet{zahiri:18a} reasoned the relation between entities by path inference.

Emotion Recognition in Conversation (ERC) has been extensively studied in the research community. It aims to attach an emotional label to every turn in a given dialogue. \cite{kratzwald2018deep} customized the recurrent neural network with bidirectional processing to solve the problem of emotion classification. \cite{majumder2019dialoguernn} leveraged the Recurrent Neural Network to extract the information of the party states and use it to predict the emotion in conversations with two speakers. On top of the recurrent neural network, COSMIC \cite{ghosal2020cosmic} models the commonsense knowledge, mental states, events, and actions to enhance emotion detection in dialogue. 
%Towards solving the context propagation problems in the recurrent neural network-based methods, \citet{ghosal-etal-2019-dialoguegcn} proposed a graph-based method that models the utterances as nodes and the speakers' dependency as edges. 

Deep learning-based methods have been extensively studied in recent works \citep{lee-dernoncourt-2016-sequential, chen2018dialogue, raheja2019dialogue} regarding Dialogue Act classification (DAC). \cite{chen2018dialogue} introduced a relation layer into the shared hierarchical encoder to model the interaction between the tasks of dialog act recognition and sentiment classification.
%Combining recurrent neural networks and convolutional neural networks, \citet{lee-dernoncourt-2016-sequential} incorporated the preceding texts while classifying the act.


% \subsubsection{Context-Aware Representation Learning}
\textbf{Context-Aware Representation Learning}. To address dynamics and semantic changes in multi-turn dialogue, previous works extend pre-trained large language models to learn context-aware representations for turns \citep{lee2021graph, DialogXL, DCM, chapuis2020hierarchical}. TUCORE-GCN \citep{lee2021graph} proposes the turn attention module, masking out distant turns to learn the contextual embeddings. Instead of adding extra modules, DialogXL \citep{DialogXL} targets the encoder and incorporates four self-attention mechanisms to different attention heads to capture diverse dialog-aware information. Similarly, such dialogue-oriented self-attention can also be found in MDFN \citep{MDFN} where it is defined as utterance-aware and speaker-aware channels. However, most of them involve an additional pre-training stage \citep{DialogXL, DCM, chapuis2020hierarchical}. 
%These efforts focus on the turn-level modeling but ignored the gap between the pre-training objective and dialogue understanding tasks. Also, though achieving preferable performance on limited datasets or tasks, these methods have not led to a unified solution to multi-turn dialogue understanding. 