% \vspace{-0.1cm}
\section{Method}
\vspace{-0.15cm}
\textbf{Problem Formulation}. The dialogue understanding task aims at building a model $f$ that can make a prediction on a query text given a dialogue. Specifically, the input includes a multi-turn dialogue $(\{s_i:t_i|i\in[1,m]\})$, and a query text $q$ with $k$ arguments $(q_1,q_2,...,q_k)$, where $i$,$s_i$,$t_i$ denotes $i^{th}$ turn, the corresponding speaker ID and text. The output is predicted class $\hat y$ of the query text $q$.

% Query text can be embodied according to task purposes. For pair-wise classification tasks such as RE, each query is the argument pair, denoted as \(\left(a_1,a_2\right)\). For turn-wise classification tasks such as ERC and DAC, each query is the target utterance $t_i$ and corresponding speaker $s_i$. For dialogue-wise understanding task such as MTDR, each query is a suggested follow-up utterance and corresponding speaker $s_i$.

\textbf{Input Module}. For a given multi-turn dialogue $d$ and a query $q$, we follow \cite{yu-etal-2020-dialogue} to reconstruct $d$ as $\hat d=\{\zeta(s_i):t_i|i\in[0,m]\}$, and the arguments in query $q$ as $\hat q_j=\zeta(a_j)$, where
% \begin{equation}
%     \zeta(x) = \begin{cases} [S_j], & \text{if $x=q_j$, $x$ is a speaker} \\ x, & \text{otherwise} \end{cases}
% \end{equation}
% % \begin{equation}
% %     \zeta(x) = \begin{cases} [S_1], & \text{if $x=a_1$, $x$ is a speaker}\\ [S_2],& \text{if $x=a_2$, $x$ is a speaker} \\ x, & \text{otherwise} \end{cases}
% % \end{equation}
% $[S_j]$ is a special token that explicitly indicates whether a token is both a speaker ID and an argument in a query. Then, we concatenate reconstructed dialogue $\hat d$ and query $\hat q$ with a special token $[CLS]$ and separator tokens $[SEP]$. 
$\zeta(\cdot)$ maps token $x_i$ and argument $q_j$ to a special token $[S_j]$ when $x_i=q_j$. Then, we concatenate reconstructed dialogue $\hat d$ and query $\hat q$ with a special token $[CLS]$ and separator tokens $[SEP]$. To leverage speaker information, we add speaker embedding \citep{gu2020speaker} into our input sequence.
\begin{figure}[t]
\centering
\vspace{-0.4cm}
\includegraphics[width=8cm]{sections/turn_attn_iclr.pdf}
\caption{Illustration of the proposed intra-turn modeling. In the turn-level attention, the restriction is applied on turn-level special tokens, denoted as $[T]$, where tokens outside the turn are masked out (colored in grey). }
\vspace{-0.4cm}
\label{fig:TurnAttn}
\end{figure}
% \begin{wrapfigure}{r}{0.62\textwidth}
% \centering
% \includegraphics[width=8cm]{sections/turn_attn_iclr.pdf}
% \caption{Illustration of the proposed intra-turn modeling. In the turn-level attention, the restriction is applied on turn-level special tokens, denoted as $[T]$, where tokens outside the turn are masked out (colored in grey). }
% \label{fig:TurnAttn}
% \end{wrapfigure}

\textbf{Intra-turn Modeling}. Prior approaches have leveraged either a global $[CLS]$ token to capture sentence-level semantics \citep{bertbase} or have initialized turn embeddings by simply averaging over tokens \citep{lee2021graph}. However, we argue that these methods have not sufficiently emphasized the significance of certain tokens that carry crucial information (e.g., trigger words in DialogRE \citep{yu-etal-2020-dialogue}), which may result in less discriminative learned turn embeddings. To capture intra-turn information, we insert a special token $\tau_i$ ahead of each turn $t_i$ and arguments, where a weighted sum of token embeddings can be learned by the self-attention mechanism in the encoder. Moreover, turn-level attention is proposed to avoid these special tokens functioning as standard special tokens. Specifically, tokens not belonging to a certain turn $t_i$ are masked out for their corresponding turn-level special token $\tau_i$. This approach compels each turn-level special token to act as an information aggregator of its own turn. Note that a global $[CLS]$ token is placed ahead of the whole sequence, these two types of special tokens then form a hierarchical way to gather information in the encoder module. The proposed intra-turn modeling is illustrated in figure \ref{fig:TurnAttn}.




%

%Previous methods leverage a global $[CLS]$ token to learn the sentence-level semantics \citep{bertbase} or initialize turn embeddings by simply averaging over tokens \citep{lee2021graph}. We argue that these methods did not highlight tokens carrying important information (e.g. trigger words in DialogRE \citep{yu-etal-2020-dialogue}), which can make learned turn embeddings less distinguishable. 

\textbf{Inter-turn Modeling}. To model the interaction between turns and entities, we establish a heterogeneous graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$ from the output of the encoder. $\mathcal{G}$ contains three types of nodes: dialogue node, turn node, and argument node. The embedding of each node is initialized from the corresponding special token, i.e., global classification token $h_{[CLS]}$ for dialogue node, turn-level classification tokens $h_\tau$ for turn nodes, and argument nodes. 
We follow \citet{lee2021graph} to establish four different types of edges in graph $\mathcal{G}$: Dialogue edge, to learn global information across the whole dialogue, all turn nodes are connected to the dialogue node; Speaker edge, every pair of turn nodes belongs to the same speaker are connected; Entity edge, an argument node is connected to a turn node if it is mentioned in this turn. To enable the model to directly learn the sequence information, every pair of turn nodes in a graph is connected via sequence edge. To enable nodes to gather information from their neighbors through different types of edges, we apply Graph Transformer Network \citep{yun2019graphtransformernetwork} to further polish the turn embeddings. 
% We follow \citet{lee2021graph} to establish different types of edges: dialogue edge, speaker edge, and entity edge, sequence edge. See Appendix \ref{ap:graph} for more details on graph edges. 
To enable nodes to gather information via different types of edges, we apply Graph Transformer Network \citep{yun2019graphtransformernetwork} to further polish the turn embeddings. 

\textbf{Classification}. Dialogue and argument nodes obtained from $\mathcal{G}$ are concatenated and fed into a linear classifier to generate the final prediction. Cross entropy loss is used as the object function.  
%to learn the model weights.
% Let $T=\{\tau_i|i\in[0,m+k]\}$ denotes the set of turn-level special tokens (argument $a_j$ is denoted as $t_{m+j}$ for brevity), and $u(x_i)$ denotes the turn number of token $x_i$, the mask of proposed turn-level attention can be formulated as:


% \begin{equation}\label{eq:mask}
%     M_{ij}=\begin{cases}0, \text{~if~} x_i\in T \wedge u(x_i)\neq u(x_j)\\1, \text{~otherwise}  \end{cases}
% \end{equation}


% \begin{wraptable}{r}{6cm}
% \vspace{-0.6cm}

%   \begin{center}

%   \setlength{\tabcolsep}{6pt}
%   \scriptsize
%   % \footnotesize
%   \begin{tabular}{llll}
%     \toprule
%   \multirow{2}{*}{\textbf{Method}}  &\multicolumn{2}{c}{\textbf{DialogRE}}  &  \textbf{MELD} \\
%     \cmidrule(l){2-3}     \cmidrule(l){4-4} 
%        & $F1$ & $F1_c$  & $F1$\\
%     \midrule
%     BERT   & 57.9 & 53.1 & 61.31\\ 
%     GDPNet & 60.2  & 57.3 & - \\
%     RoBERTa$_s$  & 71.3 & 63.7 & 64.19\\
%     SimpleRE  & 66.7  & - & - \\ 
%     % TUCORE-GCN$_{BERT}$  & 65.5 (0.4) & 60.2 (0.6) &\\
%     TUCORE-GCN$^\dag$ & 73.1 & 65.9 & 65.36\\
%     \midrule
%     % HiDialog$_{BERT}$ & 67.2 (0.3)&61.1 (0.1) & \\
%     HiDialog$^\dag$ & \textbf{77.1}\textsubscript{\textcolor{green}{+4.0}} & \textbf{68.2}\textsubscript{\textcolor{green}{+2.3}} & \textbf{66.96}\textsubscript{\textcolor{green}{+1.6}}\\
%     \bottomrule
%   \end{tabular}
%   \end{center}
%   %\caption{Main results.}
% \vspace{-0.4cm}
%   \caption{All methods performance on the DialogRE and MELD, averaged over five runs. $^\dag$ uses RoBERTa as the encoder.}
%   \vspace{-0.6cm}
%   \label{tab:exp-re}
% \end{wraptable}


\begin{table}[t]
% \vspace{-0.6cm}

  \begin{center}

  \setlength{\tabcolsep}{6pt}
  % \scriptsize
  % \footnotesize
  \begin{tabular}{llllll}
    \toprule
  \multirow{2}{*}{\textbf{Method}} &\multicolumn{2}{c}{\textbf{DialogRE-Dev}}  &\multicolumn{2}{c}{\textbf{DialogRE-Test}}  &  \textbf{MELD} \\
    \cmidrule(l){2-3}   \cmidrule(l){4-5}     \cmidrule(l){6-6} 
      & $F1$ & $F1_c$ & $F1$ & $F1_c$  & $F1$\\
    \midrule
    BERT   & 59.4 & 54.7 & 57.9 & 53.1 & 61.31\\ 
    GDPNet & 61.8 & 58.5 & 60.2 & 57.3 & - \\
    RoBERTa$_s$ & 72.6 & 65.1 & 71.3 & 63.7 & 64.19\\
    SimpleRE & - & -  & 66.7  & - & - \\ 
    % TUCORE-GCN$^\dag$  & 65.5 (& 60.2  &\\
    TUCORE-GCN$^\dag$ & 74.3 & 67.0 & 73.1 & 65.9 & 65.36\\
    \midrule
    % HiDialog$_{BERT}$ & 67.2 (0.3)&61.1 (0.1) & \\
    HiDialog$^\dag$ & 
    \textbf{77.8} \textsubscript{\textcolor{green}{+3.5}}& 
    \textbf{69.1} \textsubscript{\textcolor{green}{+2.1}} &
    \textbf{77.1}\textsubscript{\textcolor{green}{+4.0}} & \textbf{68.2}\textsubscript{\textcolor{green}{+2.3}} & \textbf{66.96}\textsubscript{\textcolor{green}{+1.6}}\\
    \bottomrule
  \end{tabular}
  \end{center}
  %\caption{Main results.}
% \vspace{-0.4cm}
  \caption{All methods performance on the DialogRE and MELD, averaged over five runs, where the standard deviations are 0.4, 0.2, 0.4, 0.3, 0.2 respectively. $^\dag$ uses RoBERTa as the encoder. Performance gains over the previous state-of-the-art are highlighted in green. }
  % \vspace{-0.6cm}
  \label{tab:exp-re}
\end{table}



%    \myalign{r}{+Inter-turn Modeling} & \myalign{r}{+1.45} & \myalign{r}{+0.1} & \myalign{r}{+0.28} & \myalign{r}{+0.2} & \myalign{r}{+3.1} & \myalign{r}{+2.9}
% &\textbf{63.28}	& \textbf{34.80} &\textbf{59.64}& \textbf{91.3} & \textbf{65.7}&\textbf{59.8}