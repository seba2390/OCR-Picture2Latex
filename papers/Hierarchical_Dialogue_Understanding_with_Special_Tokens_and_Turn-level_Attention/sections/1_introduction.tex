
% \vspace{-0.1cm}
\section{Introduction}
\vspace{-0.15cm}
Task-oriented dialogue system (TODS) plays a key role in assisting users to complete tasks automatically, and a well-trained model in TODS can help to save money and time for people. Different from the formal text, dialogues (e.g., meetings, interviews, and debates) deliver intertwined, inconsistent semantic information, where each turn forms a unit of information gain, fulfilling the needs of participating interlocutors. This unfavorable dynamic is caused by speaker intent disparity, conversation progression, and abrupt change of thought. Such dynamics in dialogue are usually ignored by large pre-trained language models. For BERT-style pre-trained models \citep{bertbase}, $[CLS]$ token is applied to model the sentence-level semantics during pre-training. However, dialogue-level natural language understanding requires methods to capture both intra-turn and inter-turn information. Although there are existing works using the special tokens method to enhance dialogue understanding, most of them involve an additional pre-training stage \citep{DialogXL, DCM, chapuis2020hierarchical}. Given that the cost of such a pre-training stage grows exponentially with model size, it is unlikely that school labs would have the resources necessary to pre-train such models on sizable dialogue datasets. Thus, we devote ourselves to bridging the gap between BERT-style pre-training and dialogue understanding fine-tuning, without extra computational cost and training data. 
%As it would become increasingly costly as language models grow larger, it is almost impossible for school labs to pre-train such models on large dialogue datasets. 
%GPT-3 \citep{gpt3} 
%\FZ{Therefore, we devote ourselves to bridging the gap between BERT-style pre-training and dialogue understanding fine-tuning, without extra computational cost and training data.} 
%In this regard, we propose HiDialog, a \textbf{Hi}erarchical \textbf{Dialog}ue method to bridge the gap between BERT-style pre-training and dialogue understanding fine-tuning, without extra computational cost and training data. This is achieved by the special token adaption and turn-level attention proposed in this paper. With a simple modification, HiDialog surpasses baselines across various tasks. 

%We insert a special token before every turn in a dialogue and propose the turn-level attention to learn the turn-level representation, where intra-turn and inter-turn information can be captured than just using $[CLS]$ token to model the whole dialogue. After obtaining turn-level representations, HiDialog adopts a turn-level dialogue graph structure to formally represent the inherent semantic information. The heterogeneous graph then infers node labels by aggregating useful features from dependent nodes to target nodes. We evaluate our approaches on various dialogue-understanding tasks. 
    %\footnote{Li et al., 2021. Deep context modeling for multi-turn response selection in dialogue systems. Information Processing \& Management, 58(1), p.102415.} 
    

%For example, e-commerce chatbots can confirm orders, track shipping, book accommodations and handle refund requests. To this end, the model in TODS is required to understand the dialogue and provide a reasonable response. 

% Dialogue understanding includes many specific tasks like Dialogue-based Relation Extraction (RE), Emotion Recognition in Conversations (ERC), and Dialogue Act Classification (DAC). Table \ref{table:diaexample} provides a dialogue relation extraction example. Given the dialogue context and two entities (\eg Speaker 1 and Mark). The relationship between Speaker 1 and Mark should be predicted as \textit{positive impression}. Similar to dialogue relation extraction example above, for other dialogue understanding tasks, it is important to capture the contextual information from the given dialogue. However, dialogue is different from the formal text in many views. One of the most important difference is, dialogues (\eg meetings, interviews and debates) deliver intertwined, inconsistent semantic information. This unfavorable dynamic is caused by speaker intent disparity, conversation progression and abrupt change of thought. Each turn forms a unit of information gain, fulfilling the needs of participating interlocutors. For example, in Table \ref{table:diaexample}, Speaker 3 in Turn 4 re-directs the topic from dinner quality to job offer queries. Therefore, we need to be careful to model the turn-level semantics in dialogue understanding tasks.

% Dynamics and semantics in dialogue are usually ignored by large pre-trained language models. For BERT-style pre-trained models, $[CLS]$ token is applied to model the sentence-level semantics during pre-training. However, dialogue-level natural language understanding requires methods to capture both intra-turn and inter-turn information. In other words, there is a gap for $[CLS]$ tokens between the pre-training objective and adapting it to downstream dialogue understanding tasks. We argue the way to use special tokens like $[CLS]$ is required to be revisited, when we are adapting a pre-trained language model to dialogue understanding tasks.

% To address above challenges, we propose HiDialog, a \textbf{Hi}erarchical \textbf{Dialog}ue method to bridge the gap between BERT-style pre-training and dialogue understanding fine-tuning. We insert a special token before every turn in a dialogue and propose the turn-level attention to learn the turn-level representation, where intra-turn and inter-turn information can be captured than just using $[CLS]$ token to model the whole dialogue. After obtaining turn-level representations, HiDialog adopts turn-level dialogue graph structure to formally represent the inherent semantic information. The heterogeneous graph then infers node labels by aggregating useful features from dependent nodes to target nodes.

% In summary, our main contributions are as follows:

% \begin{itemize}
%     \item We propose that we should revisit the gap between language model pre-training objectve and dialogue understanding tasks. The way to use special tokens need to be careful designed in dialogue understanding.
%     \item We propose a unified solution to model turn-level semantics for dialogue understanding. Each special token focus on the turn-level semantics and an optional graph module can be used to further refine the representations in a hierarchical manner.
%     \item We evaluate our approaches on various dialogue understanding tasks. The results demonstrate the effectiveness of HiDialog against SoTA baselines.
% \end{itemize}


