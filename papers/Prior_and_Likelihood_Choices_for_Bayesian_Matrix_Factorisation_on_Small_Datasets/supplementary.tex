\documentclass{article}

% Packages
\usepackage{hyperref} % for links
\usepackage{amssymb} % for maths
\usepackage{amsmath} % for maths
\usepackage{standalone} % for importing diagrams
\usepackage{tikz} % for making diagrams
\usetikzlibrary{fit,positioning} % for making diagrams
\usetikzlibrary{matrix} % for making diagrams
\usepackage{float} % prevent LaTeX from repositioning tables
\restylefloat{table} % prevent LaTeX from repositioning tables given [H] option
%\usepackage{stfloats} % put table at bottom of page

\usepackage{float} % table title above
\floatstyle{plaintop} % table title above
\restylefloat{table} % table title above

\usepackage[margin=1in]{geometry} % change page size

\usepackage{natbib} % for bibtex
\usepackage{graphicx} % for graphics path

\usepackage{caption} % for limiting caption width

\usepackage{subcaption} % for figure (a) (b) in minipage

\renewcommand\refname{\vskip -0.8cm} % get rid of References title of bib

\usepackage{booktabs} % for table toprule and bottomrule


% Commands for math notations
\newcommand{\R}{\boldsymbol R}
\newcommand{\E}{\boldsymbol E}
\newcommand{\U}{\boldsymbol U}
\newcommand{\Ui}{\boldsymbol{U_i}}
\newcommand{\V}{\boldsymbol V}
\newcommand{\Vj}{\boldsymbol{V_j}}
\newcommand{\F}{\boldsymbol F}
\renewcommand{\S}{\boldsymbol S}
\newcommand{\G}{\boldsymbol G}
\newcommand{\D}{\boldsymbol D}
\newcommand{\btheta}{\boldsymbol \theta}

\newcommand{\lambdak}{\lambda_{k}}
\newcommand{\lambdaUik}{\lambda_{ik}^U}
\newcommand{\lambdaUk}{\lambda_{k}^U}
\newcommand{\lambdaVjk}{\lambda_{jk}^V}
\newcommand{\lambdaVk}{\lambda_{jk}^V}
\newcommand{\lambdaFik}{\lambda_{ik}^F}
\newcommand{\lambdaFk}{\lambda_{k}^F}
\newcommand{\lambdaSkl}{\lambda_{kl}^S}
\newcommand{\lambdaGjl}{\lambda_{jl}^G}
\newcommand{\lambdaGl}{\lambda_{l}^G}

\newcommand{\muUik}{\mu_{ik}^U}
\newcommand{\muVjk}{\mu_{jk}^V}
\newcommand{\muFik}{\mu_{ik}^F}
\newcommand{\muSkl}{\mu_{kl}^S}
\newcommand{\muGjl}{\mu_{jl}^G}

\newcommand{\SigmaUi}{\boldsymbol{\Sigma^U_i}}
\newcommand{\SigmaVj}{\boldsymbol{\Sigma^V_j}}
\newcommand{\muUi}{\boldsymbol{\mu^U_i}}
\newcommand{\muVj}{\boldsymbol{\mu^V_j}}

\newcommand{\tauUik}{\tau_{ik}^U}
\newcommand{\tauVjk}{\tau_{jk}^V}
\newcommand{\tauFik}{\tau_{ik}^F}
\newcommand{\tauSkl}{\tau_{kl}^S}
\newcommand{\tauGjl}{\tau_{jl}^G}

\newcommand{\sumOmega}{\sum_{(i,j) \in \Omega}}
\newcommand{\sumOmegai}{\sum_{j \in \Omega_i}}
\newcommand{\sumOmegaj}{\sum_{i \in \Omega_j}}

\newcommand{\sumk}{\sum_{k=1}^K}
\newcommand{\sumexclk}{\sum_{k' \neq k}}
\newcommand{\suml}{\sum_{l=1}^L}
\newcommand{\sumexcll}{\sum_{l' \neq l}}
\newcommand{\sumexclkl}{\sum_{(k',l') \neq (k,l)}}

\newcommand{\diff}{( R_{ij} - \Ui \Vj ) }
\newcommand{\diffexclk}{( R_{ij} - \sumexclk U_{ik'} V_{jk'} ) }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	

\begin{document}
	\title{Prior and Likelihood Choices for Bayesian Matrix Factorisation on Small Datasets \\ \vspace{10pt} Supplementary Materials}
	\author{}
	\date{}
	\maketitle{}
	
	\noindent \large{Thirty-Second AAAI Conference on Artificial Intelligence (2018).} \\
	
	\setcounter{secnumdepth}{3}
	\setcounter{tocdepth}{2}
	\tableofcontents
	\clearpage
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
	\section{Gibbs sampling algorithms}
		In this section we give the Gibbs sampling posteriors for the sixteen Bayesian matrix factorisation models studied in the paper. 
		
		The idea of Gibbs sampling is as follows. We wish to sample values from the posterior distribution $p(\btheta|D)$, but we cannot sample these directly. Instead, we could draw 2values from the conditional posterior $ p(\theta_i | \btheta_{-i}, D ) $, which is the distribution over the parameter $ \theta_i $ (such as $U_{ik}$) given the current values of the other parameters $ \btheta_{-i} $, and the observed data $ D $. If we sample new values in turn for each parameter $ \theta_i $ from $ p(\theta_i | \btheta_{-i}, D ) $, we will eventually converge to draws from the true posterior $ p(\btheta|D) $, which can be used to approximate it. 
		In this paper we focus on models where we have so-called \textit{model conjugacy}, allowing us to sample from the conditional posteriors.
		
		We give the Gibbs sampling posterior distributions for $\U$, which can be derived using Bayes' theorem. For example, the derivations for $U_{ik}$ in the GEE model can be found below. Expressions for $\V$ are symmetrical, and hence omitted. 
		%
		\begin{alignat*}{1}
			p(U_{ik}|\tau,\U_{-ik},\V,\boldsymbol \lambda) 
			&\propto p(\R|\tau,\U,\V) \times p(U_{ik}|\lambda_k) \\
			&\propto \prod_{j \in \Omega^1_i} \mathcal{N} (R_{ij} | \U_i \cdot \V_j, \tau^{-1} ) \times \mathcal{E} ( U_{ik} | \lambda_k) \\
			&\propto \exp \left\{ - \frac{\tau}{2} \sum_{j \in \Omega^1_i} (R_{ij} - \U_i \V_j)^2 \right\} \times \exp \left\{ - \lambda_k U_{ik} \right\} \times u(x) \\
			&\propto \exp \left\{ - \frac{U_{ik}^2}{2} \left[ \displaystyle \tau \sum_{j \in \Omega^1_i} V_{jk}^2 \right] \right. \\
			& \left. \hspace{46pt} + U_{ik} \left[ - \lambda_k + \tau \sum_{j \in \Omega^1_i} \diffexclk V_{jk} \right] \right\} \times u(x) \\
			&\propto \exp \left\{ - \frac{\tauUik}{2} ( U_{ik} - \muUik )^2 \right\} \times u(x) \\
			&\propto \mathcal{TN} ( U_{ik} | \muUik, \tauUik ).
		\end{alignat*}
	
		\subsection{Real-valued matrix factorisation}
		The first group of methods use a Gaussian likelihood for noise, and place real-valued prior distributions over $\U$ and $\V$, typically Gaussian as well. We assume each value in $\R$ comes from the product of $\U$ and $\V$, with Gaussian noise added,
		%
		\begin{alignat*}{2}
			&R_{ij} \sim \mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )	 \quad\quad		&&\tau \sim \mathcal{G} (\tau | \alpha_{\tau}, \beta_{\tau} ) 
		\end{alignat*}
		%
		where $ \mathcal{N} (x|\mu,\tau) = \tau^{\frac{1}{2}} (2\pi)^{-\frac{1}{2}} \exp \left\{ -\frac{\tau}{2} (x - \mu)^2 \right\} $ is the density of the Gaussian distribution, with precision $ \tau $. $\Ui, \Vj$ denote the $i$th and $j$th rows of $\U$ and $\V$. We place a further Gamma prior over $\tau$, with $ \mathcal{G} (\tau | \alpha_{\tau}, \beta_{\tau} ) = \frac{{\beta_{\tau}}^{\alpha_{\tau}}}{\Gamma(\alpha_{\tau})} x^{\alpha_{\tau} -1} e^{- \beta_{\tau} x} $, where $ \Gamma(x) = \int_{0}^{\infty} x^{t-1} e^{-x} dt $ is the gamma function. 
		
		In the Gibbs sampling algorithm, the posterior for the noise parameter is
		%
		\begin{alignat*}{1}
			\tau \sim \mathcal{G} (\tau | \alpha^*_{\tau}, \beta^*_{\tau} )		\quad\quad 		\alpha^*_{\tau} = \alpha_{\tau} + \frac{\vert \Omega \vert}{2} 		\quad\quad  		\beta^*_{\tau} = \beta_{\tau} + \frac{1}{2} \sumk \diff^2.
		\end{alignat*}
		
		\subsubsection{All Gaussian model (GGG)}
		We place Gaussian independent priors over the entries in $\U,\V$, with hyperparameter $\lambda$,
		%
		\begin{alignat*}{1}
			\Ui \sim \mathcal{N} ( \Ui | \boldsymbol 0, \lambda^{-1} \text{\textbf I} )		\quad\quad	\Vj \sim \mathcal{N} ( \Vj | \boldsymbol 0, \lambda^{-1} \text{\textbf I} )
		\end{alignat*}
		%
		where $ \mathcal{N} (\boldsymbol x|\boldsymbol \mu,\boldsymbol \Sigma) = \vert \boldsymbol \Sigma \vert^{-\frac{1}{2}} (2\pi)^{-\frac{K}{2}} \exp \left\{ -\frac{1}{2} (\boldsymbol x - \boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x - \boldsymbol \mu) \right\} $ is the density of a $K$-dimensional multivariate Gaussian distribution, and \text{\textbf I} is the identity matrix.
		The conditional posterior distributions we obtain in the Gibbs sampling algorithm are also multivariate Gaussians. The parameter values are given below, with $ \Omega_i = \left\{ j \text{ $ \vert $ } (i,j) \in \Omega \right\} $ and  $ \Omega_j = \left\{ i \text{ $ \vert $ } (i,j) \in \Omega \right\} $. 
		%
		\begin{alignat*}{1}
			&\U_i \sim \mathcal{N} ( \Ui | \muUi, \SigmaUi )		
			\quad\quad	\muUi = \SigmaUi \cdot \left[ \tau \sumOmegai R_{ij} \Vj \right] 		
			\quad\quad	 \SigmaUi = \left[ \lambda \text{\textbf I} + \tau \sumOmegai \left( \Vj \otimes \Vj \right) \right]^{-1}
		\end{alignat*}
		
		\subsubsection{All Gaussian model with univariate posterior (GGGU)}
		It is also possible to have a univariate posterior for the Gibbs sampler,
		%
		\begin{alignat*}{1}
			&U_{ik} \sim \mathcal{N} (U_{ik} | \mu^U_{ik}, (\tau^U_{ik})^{-1} )
			\quad\quad \mu^U_{ik} = \frac{1}{\tau^U_{ik}} \Bigg[ \tau \sumOmegai \diffexclk V_{jk} \Bigg] 
			\quad\quad \tau^U_{ik} = \lambda + \tau \sumOmegai V_{jk}^2.
		\end{alignat*}
		
		\subsubsection{All Gaussian model with ARD hierarchical prior (GGGA)}
		For the automatic relevance determination (ARD) prior we replace the $\lambda$ hyperparameter by a factor-specific variable $\lambda_k$, which has a further Gamma prior. 
		%
		\begin{alignat*}{1}
			\Ui \sim \mathcal{N} ( \Ui | \boldsymbol 0, \text{diag}(\boldsymbol \lambda^{-1}) )		\quad\quad	\Vj \sim \mathcal{N} ( \Vj | \boldsymbol 0, \text{diag}(\boldsymbol \lambda^{-1}) ) 	\quad\quad 		\lambda_k \sim \mathcal{G} (\lambda_k | \alpha_0, \beta_0 )
		\end{alignat*}
		%
		where $\text{diag}(\boldsymbol \lambda^{-1})$ is a diagonal matrix with entries $\lambda_1^{-1}, .., \lambda_K^{-1}$ on the diagonal.
		The Gibbs sampling posteriors for $\Ui$ and $\Vj$ are largely unchanged, replacing $\lambda \text{\textbf I}$ in the expressions for $\SigmaUi, \SigmaVj$ by $\text{diag}(\lambda_1,..,\lambda_K)$. The posterior for $\lambda_k$ is
		%
		\begin{alignat*}{1}
			\lambda_k \sim \mathcal{G} (\lambda_k | \alpha^*_0, \beta^*_0 )		\quad\quad 		\alpha^*_0 = \alpha_0 + \frac{I}{2} + \frac{J}{2} 		\quad\quad  		\beta^*_0 = \beta_0 + \frac{1}{2} \sum_{i=1}^I U_{ik}^2 + \frac{1}{2} \sum_{j=1}^J V_{jk}^2.
		\end{alignat*}
		
		\subsubsection{All Gaussian model with Wishart hierarchical prior (GGGW)}
		Instead of assuming independence of each entry in $\U, \V$, we now assume each row of $\U$ comes from a multivariate Gaussian with row mean $\boldsymbol{\mu_U}$ and covariance $\boldsymbol{\Sigma_U}$, and similarly for $\V$. We place a further Normal-Inverse Wishart prior over these parameters,
		%
		\begin{alignat*}{2}
			&\Ui \sim \mathcal{N} ( \Ui | \boldsymbol{\mu_U}, \boldsymbol{\Sigma_U})		\quad\quad	&&\boldsymbol{\mu_U}, \boldsymbol{\Sigma_U} \sim \mathcal{NIW} ( \boldsymbol{\mu_U}, \boldsymbol{\Sigma_U} | \boldsymbol{\mu_0}, \beta_0, \nu_0, \boldsymbol{W_0} ) \\
			&\Vj \sim \mathcal{N} ( \Vj | \boldsymbol{\mu_V}, \boldsymbol{\Sigma_V})		\quad\quad 		&&\boldsymbol{\mu_V}, \boldsymbol{\Sigma_V} \sim \mathcal{NIW} ( \boldsymbol{\mu_V}, \boldsymbol{\Sigma_V} | \boldsymbol{\mu_0}, \beta_0, \nu_0, \boldsymbol{W_0} )
		\end{alignat*}
		%
		where $ \mathcal{NIW} ( \boldsymbol{\mu_U}, \boldsymbol{\Sigma_U} | \boldsymbol{\mu_0}, \beta_0, \nu_0, \boldsymbol{W_0} ) = \mathcal{N}(\boldsymbol \mu | \boldsymbol{\mu_0}, \frac{1}{\beta_0} \text{\textbf I} ) \mathcal{W}^{-1} ( \boldsymbol \Sigma | \nu_0, \boldsymbol{W_0} ) $ is the density of a normal-inverse Wishart distribution, and $\mathcal{W}^{-1} ( \boldsymbol \Sigma | \nu_0, \boldsymbol{W_0} )$ is the inverse Wishart distribution. 
		
		For the Gibbs sampling algorithm we obtain the posteriors $\Ui \sim \mathcal{N} ( \Ui | \muUi, \SigmaUi )$ and $ \boldsymbol{\mu_U}, \boldsymbol{\Sigma_U} \sim \mathcal{NIW} ( \boldsymbol{\mu_U}, \boldsymbol{\Sigma_U} | \boldsymbol{\mu_0^*}, \beta_0^*, \nu_0^*, \boldsymbol{W_0^*} ) $, with
		%
		\begin{alignat*}{1}
			&\muUi = \SigmaUi \cdot \Big[ \boldsymbol{\Sigma_U^{-1}}  \boldsymbol{\mu_U} + \tau \sumOmegai R_{ij} \Vj \Big] 
			\quad\quad \SigmaUi = \Big[ \boldsymbol{\Sigma_U^{-1}} + \tau \sumOmegai \left( \Vj \otimes \Vj \right) \Big]^{-1} \\
			&\beta_0^* = \beta_0 + I  
			\quad\quad \nu_0^* = \nu_0 + I
			\quad\quad \boldsymbol{\mu_0^*} = \frac{\beta_0 \boldsymbol{\mu_0} + I \boldsymbol{\bar{U}}}{\beta_0 + I} 
			\quad\quad\quad \boldsymbol{\bar{U}} = \frac{1}{I} \sum_{i=1}^I \Ui \\
			&\boldsymbol{W_0^*} = \boldsymbol{W_0} + I \boldsymbol{\bar{S}} + \frac{\beta_0 I}{\beta_0 + I} ( \boldsymbol{\mu_0} - \boldsymbol{\bar{U}} ) \otimes ( \boldsymbol{\mu_0} - \boldsymbol{\bar{U}} )
			\quad\quad\quad\quad \boldsymbol{\bar{S}} = \frac{1}{I} \sum_{i=1}^I ( \Ui \otimes \Ui ).
		\end{alignat*}
		
		\subsubsection{Gaussian likelihood with Laplace priors (GLL)}
		An alternative to the Gaussian prior is to use the Laplace distribution, which has a much more pointy distribution than Gaussian around $x=0$. This leads to more sparse solutions, as more factors are set to low values. The priors are
		%
		\begin{alignat*}{1}
			U_{ik} \sim \mathcal{L} ( U_{ik} | 0, \eta ) 
			\quad\quad V_{jk} \sim \mathcal{L} ( V_{jk} | 0, \eta )
		\end{alignat*}
		%
		To simplify inference we introduce a new variable $\lambdaUik$ for each $U_{ik}$, with prior $\lambdaUik \sim \mathcal{E} (\lambdaUik | \eta)$. The idea behind this is that we can rewrite a Laplace distribution as
		%
		\begin{alignat*}{1}
			\mathcal{L} ( x | \mu, \rho ) = \int_{\epsilon=0}^{\infty} \mathcal{N}(x|\mu,\epsilon) \mathcal{E}(\epsilon|\frac{\rho}{2}) d\epsilon
		\end{alignat*}
		%
		This leads to the following Gibbs sampling posteriors:
		%
		\begin{alignat*}{2}
			&\U_i \sim \mathcal{N} ( \Ui | \muUi, \SigmaUi )		
			\quad\quad	&& \muUi = \SigmaUi \cdot \left[ \tau \sumOmegai R_{ij} \Vj \right] \\
			& && \SigmaUi = \text{diag}((\boldsymbol{\lambda^U_i})^{-1}) + \left[ \tau \sumOmegai \left( \Vj \otimes \Vj \right) \right]^{-1} \\
			& \frac{1}{\lambdaUik} \sim \mathcal{IG}(x|\mu^U_{ik}, \lambda^U_{ik})
			\quad\quad && \mu^U_{ik} = \frac{\sqrt{\eta}}{|U_{ik}|}
			\quad\quad\quad\quad \lambda^U_{ik} = \eta.
		\end{alignat*}
		
		\subsubsection{Gaussian likelihood with Laplace and hierarchical inverse Gaussian priors (GLLI)}
		We can place a further hierarchical prior over the $\eta$ parameters, 
		%
		\begin{alignat*}{1}
			\eta^U_{ik} \sim \mathcal{IG}(\mu, \lambda)  
			\quad\quad \eta^V_{jk} \sim \mathcal{IG}(\mu, \lambda).
		\end{alignat*}
		%
		The paper that introduced this prior (Jing, Wang and Yang 2015) placed a Generalised Inverse Gaussian $\mathcal{GIG}(\gamma, a, b)$ prior over the $\eta$ parameters, but then used $\gamma = -\frac{1}{2}$, which reduces the prior to the Inverse Gaussian above with $\mu=\sqrt{b/a}, \lambda=b$ (or $a=1/\mu, b=\lambda$).
		
		The posteriors for $\U, \V$ are identical, and for $\lambdaUik$ we only replace $\eta$ with $\eta^U_{ik}$. We obtain another Inverse Gaussian posterior for the $\eta^U_{ik}$ parameters,
		%
		\begin{alignat*}{1}
			\eta^U_{ik} \sim \mathcal{IG} (\eta^U_{ik} | \mu^{\eta}_{ik}, \lambda^{\eta}_{ik} )
			\quad\quad \mu^U_{ik} = \sqrt{\frac{\lambdaUik + a}{b}} = \sqrt{\frac{\lambdaUik + 1/\mu}{\lambda}}
			\quad\quad \lambda^U_{ik} = \lambdaUik + b = 1/\mu.
		\end{alignat*}
		
		\subsubsection{Gaussian likelihood with volume prior (GVG)}
		The prior over $\V$ in this model is Gaussian, as in the GGG model, but we now use the volume prior (VP) for the $\U$ matrix, with density $p(\U) \propto \exp \lbrace - \gamma \det (\U^T \U) \rbrace $. This model leads to the posterior 
		%
		\begin{alignat*}{2}
			&U_{ik} \sim \mathcal{N} (U_{ik} | \mu^U_{ik}, (\tau^U_{ik})^{-1} )
			\quad\quad &&\mu^U_{ik} = \frac{1}{\tau^U_{ik}} \Bigg[ \gamma \boldsymbol{U_{i\tilde{k}}} \boldsymbol{A_{\tilde{k}\tilde{k}}} ( \boldsymbol{U_{\tilde{i}\tilde{k}}^T} \boldsymbol{U_{\tilde{i}k}} ) + \tau \sumOmegai \diffexclk V_{jk} \Bigg] \\
			& &&\tau^U_{ik} = \tau \sumOmegai V_{jk}^2 + \gamma ( D_{\tilde{k}\tilde{k}} - \boldsymbol{U_{i\tilde{k}}} \boldsymbol{A_{\tilde{k}\tilde{k}}} \boldsymbol{U_{i\tilde{k}}^T} ).
		\end{alignat*}
		%
		In the above, vector $ \boldsymbol{U_{i\tilde{k}}} $ is the $i$th row of $\U$ excluding column $k$; vector $ \boldsymbol{U_{\tilde{i}k}} $ is the $k$th column of $\U$ excluding row $i$; matrix $ \boldsymbol{U_{\tilde{i}\tilde{k}}} $ is $\U$ excluding row $i$ and column $k$; matrix $ \boldsymbol{U_{\cdot \tilde{k}}} $ is $\U$ excluding column $k$; $D_{\tilde{k}\tilde{k}} = \det \lbrace \boldsymbol{U_{\cdot \tilde{k}}^T} \boldsymbol{U_{\cdot \tilde{k}}} \rbrace $; and matrix $\boldsymbol{A_{\tilde{k}\tilde{k}}} = \det \lbrace \boldsymbol{U_{\cdot \tilde{k}}^T} \boldsymbol{U_{\cdot \tilde{k}}} \rbrace $ is the matrix adjugate.
		
		\subsection{Nonnegative matrix factorisation}
		Nonnegative matrix factorisation models use the same Gaussian noise model as the real-valued ones, but placing nonnegative prior distributions over entries in $\U$ and $\V$. 
		
		\subsubsection{Gaussian likelihood with exponential priors (GEE)}
		This model places independent Exponential priors over the entries in $\U,\V$,
		%
		\begin{alignat*}{1}
			&U_{ik} \sim \mathcal{E} ( U_{ik} | \lambda )		\quad\quad	V_{jk} \sim \mathcal{E} ( V_{jk} | \lambda ).
		\end{alignat*}
		%
		The product of a Gaussian and Exponential distribution leads to a truncated normal posterior,
		%
		\begin{alignat*}{2}
			& U_{ik} \sim \mathcal{TN} (U_{ik} | \mu^U_{ik}, \tau^U_{ik} )
		\quad\quad &&\mu^U_{ik} = \frac{1}{\tau^U_{ik}} \Bigg[ - \lambda + \tau \sumOmegai \diffexclk V_{jk} \Bigg]
		\quad\quad \tau^U_{ik} = \tau \sumOmegai V_{jk}^2.
		\end{alignat*}
		
		\subsubsection{Gaussian likelihood with exponential prior and ARD (GEEA)}
		Similar to the GGGA model, we can extend GEE with the ARD prior,
		%
		\begin{alignat*}{1}
			&U_{ik} \sim \mathcal{E} ( U_{ik} | \lambda_k )		\quad\quad	V_{jk} \sim \mathcal{E} ( V_{jk} | \lambda_k ) 	\quad\quad 		\lambda_k \sim \mathcal{G} (\lambda_k | \alpha_0, \beta_0 )
		\end{alignat*}
		%
		The posteriors for $\U$ and $\V$ are the same as in the GEE model, but replacing $\lambda$ by $\lambda_k$. The posteriors for $\lambda_k$ become
		%
		\begin{alignat*}{1}
			\lambda_k \sim \mathcal{G} (\lambda_k | \alpha^*_0, \beta^*_0 )		\quad\quad 		\alpha^*_0 = \alpha_0 + I + J 		\quad\quad  		\beta^*_0 = \beta_0 + \sum_{i=1}^I U_{ik} + \sum_{j=1}^J V_{jk}.
		\end{alignat*}
		%
		
		\subsubsection{Gaussian likelihood with truncated normal priors (GTT)}
		We can also use the truncated normal distribution directly as the priors for $\U$ and $\V$, 
		%
		\begin{alignat*}{1}
			&U_{ik} \sim \mathcal{TN} ( U_{ik} | \mu_U, \tau_U )		\quad\quad	V_{jk} \sim \mathcal{TN} ( V_{jk} | \mu_V, \tau_V ) 
		\end{alignat*}
		%
		This again gives a truncated normal posterior, but with slightly different values.
		%
		\begin{alignat*}{2}
			& U_{ik} \sim \mathcal{TN} (U_{ik} | \mu^U_{ik}, (\tau^U_{ik})^{-1} )
			\quad\quad &&\mu^U_{ik} = \frac{1}{\tau^U_{ik}} \Bigg[ \mu_U \tau_U + \tau \sumOmegai \diffexclk V_{jk} \Bigg] \\
			& &&\tau^U_{ik} = \tau_U + \tau \sumOmegai V_{jk}^2.
		\end{alignat*}
		%
		An alternative to the truncated normal distribution is the so-called half normal. If random variable $y$ has density $\mathcal{N} (y | 0, \sigma^2 )$, and random variable $x = |y|$, then $x$ follows a half normal distribution with density $\mathcal{HN}(x|\sigma) = \frac{\sqrt{2}}{\sigma \sqrt{\pi}} \exp \lbrace - \frac{x^2}{2\sigma^2} \rbrace u(x) $. Note however that when $\mu = 0$ in the truncated normal distribution, then these distributions are equivalent, with $\tau = \frac{1}{\sigma^2} $, so the GTT model is more general.
		
		\subsubsection{Gaussian likelihood with truncated normal and hierarchical priors (GTTN)}
		We can place a further prior over the parameters of the truncated normal distributions,
		%
		\begin{alignat*}{1}
			&U_{ik} \sim \mathcal{TN} ( U_{ik} | \mu^U_{ik}, \tau^U_{ik} )		\quad\quad	V_{jk} \sim \mathcal{TN} ( V_{jk} | \mu^V_{jk}, \tau^V_{jk} )  \\
			&p(\mu^U_{ik}, \tau^U_{ik} | \mu_{\mu}, \tau{\mu}, a, b) \propto \frac{1}{\sqrt{\tau^U_{ik}}} \left( 1 - \Phi ( - \mu^U_{ik} \sqrt{\tau^U_{ik}} ) \right) \mathcal{N} (\mu^U_{ik} | \mu_{\mu}, \tau_{\mu}^{-1} ). \mathcal{G} (\tau^U_{ik} | a, b)
		\end{alignat*}
		%
		The density for $\mu^V_{jk}, \tau^V_{jk}$ is identical. Note that this is not the same as the product of a Normal and Gamma distribution. It is not easy to sample from this prior, but it can be used as a hierarchical prior. The posteriors for $U_{ik}$ remain the same (replacing $\mu_U$ and $\tau_U$ by $\mu^U_{ik}$ and $\tau^U_{ik}$), and for $\mu^U_{ik}$ and $ \tau^U_{ik}$ we obtain posteriors
		%
		\begin{alignat*}{3}
			& \mu^U_{ik} \sim \mathcal{N} (\mu^U_{ik} | m_{\mu}, t_{\mu}^{-1} )
			\quad\quad && m_{\mu} = \frac{1}{t_{\mu}} \left[ \tau^U_{ik} U_{ik} + \mu_{\mu} \tau_{\mu} \right] 
			\quad\quad && t_{\mu} = \tau^U_{ik} + \tau_{\mu} \\
			& \tau^U_{ik} \sim \mathcal{G} (\tau^U_{ik} | a^*, b^* )
			\quad\quad && a^* = a + \frac{1}{2} 
			\quad\quad && b^* = b + \frac{(U_{ik} - \mu^U_{ik})^2}{2}.
		\end{alignat*}
		
		\subsubsection{Gaussian likelihood with $\boldsymbol{\text{L}^2_1}$ norm priors (G$\boldsymbol{\text{L}^2_1}$)}
		We can also use a prior inspired by the $\text{L}^2_1$ norm for both $\U$ and $\V$, giving prior densities
		%
		\begin{alignat*}{1}
			& p ( \U ) \propto \left\{
			\begin{array}{ll}
			\displaystyle \exp \lbrace -\frac{\lambda}{2} \sum_i \left( \sum_k U_{ik} \right)^2 \rbrace   & \mbox{if } U_{ik} \geq 0 \mbox{ for all $i, k$} \\
			0 & \mbox{if any } U_{ik} < 0
			\end{array}
			\right. \\
			& p ( \V ) \propto \left\{
			\begin{array}{ll}
			\displaystyle \exp \lbrace -\frac{\lambda}{2} \sum_j \left( \sum_k V_{jk} \right)^2   & \mbox{if } V_{jk} \geq 0 \mbox{ for all $j, k$} \\
			0 & \mbox{if any } V_{jk} < 0
			\end{array}
			\right.
		\end{alignat*}
		%
		The nonnegativity constraint is used to address the fact that the $\text{L}^2_1$ norm used the absolute value of entries in $\U, \V$, which makes inference impossible unless we constrain them to be nonnegative (in which case the values are automatically absolute).
		
		The posteriors are similar to the GEE and GTT models, but now adding a term that depends on the other entries in the $i$th (or $j$th) row of $\U$,
		%
		\begin{alignat*}{2}
			& U_{ik} \sim \mathcal{TN} (U_{ik} | \mu^U_{ik}, \tau^U_{ik} )
			\quad\quad &&\mu^U_{ik} = \frac{1}{\tau^U_{ik}} \Bigg[ - \lambda \sumexclk U_{ik'} + \tau \sumOmegai \diffexclk V_{jk} \Bigg] \\
			& && \tau^U_{ik} = \lambda + \tau \sumOmegai V_{jk}^2.
		\end{alignat*}
		
		\subsection{Semi-nonnegative matrix factorisation}
		In the nonnegative matrix factorisation models we placed nonnegative priors over both $\U$ and $\V$.
		Instead, we could constrain only one to be nonnegative. In the Bayesian setting this is done by placing a real-valued prior over one matrix, and a nonnegative prior over the other. The major advantage is that we can handle real-valued datasets, while still enforcing some nonnegativity. 
		
		\subsubsection{Gaussian likelihood with nonnegative volume prior (GVnG)}
		The volume prior discussed earlier can also be formulated to be nonnegative. In particular, the probability distribution over $\U$ is
		%
		\begin{equation*}
			p ( \U ) \propto \left\{
			\begin{array}{ll}
			\displaystyle \exp \lbrace - \gamma \det (\U^T \U) \rbrace   & \mbox{if } U_{ik} \geq 0 \mbox{ for all $i, k$} \\
			0 & \mbox{if any } U_{ik} < 0
			\end{array}
			\right.
		\end{equation*}
		%
		The posterior parameters are the same as for the GVG model, but drawing new values from a truncated normal, rather than normal. For $\V$ we again use a Gaussian.
		
		\subsubsection{Gaussian likelihood with exponential and Gaussian priors (GEG)}
			For this model we use an exponential prior for entries in $\U$, and a Gaussian for $\V$. The posteriors are given in the GEE and GGG model sections.
		
		
		\subsection{Poisson matrix factorisation}
		The final category of matrix factorisation models do not use a Gaussian likelihood, instead opting for a Poisson one. This only works for nonnegative count data, with $ \R \in \mathbb{N}^{I \times J} $. We again assume each value in $\R$ comes from the product of $\U$ and $\V$, $ R_{ij} \sim \mathcal{P} (R_{ij} | \Ui \Vj )$, where $ \mathcal{P} (x|\lambda) = \frac{\lambda^x \exp \lbrace - \lambda \rbrace}{x!} $ is the density of a Poisson distribution. 
		
		\subsubsection{Poisson likelihood with Gamma priors (PGG)}
		The standard Poisson matrix factorisation model uses independent Gamma priors over the entries in $\U$ and $\V$. To make inference simpler, we also introduce random variables $Z_{ijk}$ such that $R_{ij} = \sumk Z_{ijk}$, each effectively accounting for the contribution of factor $k$ to $R_{ij}$. We use the following distributions and priors: 
		%
		\begin{alignat*}{1}
			Z_{ijk} \sim \mathcal{P} (Z_{ijk} | U_{ik} V_{jk} ) 		\quad\quad 			U_{ik} \sim \mathcal{G} ( U_{ik} | a, b )		\quad\quad	V_{jk} \sim \mathcal{G} ( V_{jk} | a, b ).
		\end{alignat*}
		%
		Note that we can do this because the sum of Poisson distributed random variables (like $Z_{ijk}$) is again Poisson distributed, with rate $\lambda $ equal to the sum of rates of the $Z_{ijk}$, giving us the original Poisson likelihood for $R_{ij}$. The above is also equivalent to saying that $\boldsymbol{Z_{ij}} \sim \text{Mult} (\boldsymbol{Z_{ij}} | n, \boldsymbol p) $ with $ n = R_{ij} $ and $ \boldsymbol p = (\frac{U_{i1} V_{j1}}{\Ui \Vj}, .., \frac{U_{iK} V_{jK}}{\Ui \Vj}) $, where $\boldsymbol{Z_{ij}}$ is a vector containing $Z_{ij1}, .., Z_{ijK}$, and $\text{Mult}(\boldsymbol x | n, \boldsymbol p) = \frac{n!}{x_1! .. x_K!} p_1^{x_1} .. p_K^{x_K} $ is a K-dimensional multinomial distribution. 
		
		Using the above trick, the posteriors are
		%
		\begin{alignat*}{3}
			& \boldsymbol{Z_{ij}} \sim \text{Mult} (\boldsymbol{Z_{ij}} | n, \boldsymbol{p} )
			\quad\quad && n = R_{ij}
			\quad\quad && \boldsymbol{p} = (\frac{U_{i1} V_{j1}}{\Ui \Vj}, .., \frac{U_{iK} V_{jK}}{\Ui \Vj}) \\
			& U_{ik} \sim \mathcal{P} (U_{ik} | a^*_{ik}, b^*_{ik} )
			\quad\quad && a^*_{ik} = a + \sumOmegai Z_{ijk}
			\quad\quad &&b^*_{ik} = b + \sumOmegai V_{jk}.
		\end{alignat*}
		
		\subsubsection{Poisson likelihood with Gamma and hierarchical Gamma priors  (PGGG)}
		We can extend the standard Poisson matrix factorisation model with hierarchical priors. The priors are
		%
		\begin{alignat*}{1}
			&U_{ik} \sim \mathcal{G} ( U_{ik} | a, h^U_i )		\quad\quad			V_{jk} \sim \mathcal{G} ( V_{jk} | a, h^V_j )		\quad\quad			h^U_i \sim \mathcal{G} (a', \frac{a'}{b'}) 		\quad\quad		h^V_j \sim \mathcal{G} (a', \frac{a'}{b'})
		\end{alignat*}
		%
		The posteriors for $U_{ik}$ are identical to the PGG model, except replacing $b$ with $h^U_i$ in the expression for $b^*_{ik}$. For the hierarchical part we obtain the following posteriors:
		%
		\begin{alignat*}{3}
		&h^U_i \sim \mathcal{G} (h^U_i | a^*_i, b^*_i )   		
		\quad\quad && a^*_i = a' + K a 		
		\quad\quad && b^*_i = \frac{a'}{b'} + \sumk U_{ik}.
		\end{alignat*}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\clearpage
	\section{Computational complexity}
		The different matrix factorisation models have different time complexities for computing the parameter values and sample new values for $\U$, $\V$, and any other random variables. The space complexity for all models is $\mathcal{O}( I K + J K ) $ per iteration, with an additional $ K \vert \Omega \vert $ term for the Poisson models (for the $Z_{ijk}$).
		
		The time complexities per iteration for the multivariate Gaussian posterior models (GGG, GGGA, GGGW, GLL, GLLI) is $ \mathcal{O}( (I+J)K^3 + IJK^2 ) $. However, these row draws and parameter value computations can all be done in parallel. 
		The univariate posterior models (GGGU, GEE, GEEA, GTT, GTTN, $\text{GL}^2_1$, GEG) have complexity $ \mathcal{O}( I J K^2 ) $, but the parameters can be computed efficiently per column. The volume prior models (GVG, GVnG) have the highest complexity, with $ \mathcal{O}( I^2 J K^2 ) $. Finally, the Poisson models are $ \mathcal{O}( I J K ) $, but this hides a big constant that effectively makes it the slowest model for low values of $K$.
			
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
	\section{Runtime speed}
		The average runtime (in seconds) per iteration is given in Table \ref{runtimes_table}, for different values of $K$ on the GDSC drug sensitivity and MovieLens 100K datasets. Here we see that the univariate posterior models (GGGU, GEE, GEEA, GTT, GTTN, $\text{GL}^2_1$, GEG) are faster than the multivariate ones (GGG, GGGA, GGGW, GLL, GLLI); models with hierarchical priors are not noticably slower; the volume prior models are by far the slowest, due to their higher time complexity; and the Poisson models are slow for low $K$, but at higher values this is no longer true.
		
		\begin{table*}[h]
			\caption{Average runtime per iteration (in seconds) on GDSC drug sensitivity and MovieLens 100K.} 
			\label{runtimes_table}
			\centering
			\begin{tabular}{lllllllll}
				\toprule
				& \multicolumn{4}{l}{GDSC drug sensitivity} & \multicolumn{4}{l}{MovieLens 100K} \\
				\cmidrule(r){2-5} \cmidrule{6-9}
				Method & $K=5 $ & $K=10 $ & $K=20 $ & $K=50 $ & $K=5 $ & $K=10 $ & $K=20 $ & $K=50 $ \\
				\cmidrule(r){1-1} \cmidrule(r){2-2} \cmidrule(r){3-3} \cmidrule(r){4-4} \cmidrule(r){5-5} \cmidrule(r){6-6} \cmidrule(r){7-7} \cmidrule(r){8-8} \cmidrule{9-9}
				GGG & 0.14 & 0.18 & 0.29 & 0.93 & 1.04 & 1.29 & 1.86 & 5.07 \\
				GGGU & 0.02 & 0.03 & 0.07 & 0.16 & 0.48 & 0.79 & 1.49 & 3.99 \\
				GGGA & 0.14 & 0.17 & 0.28 & 0.93 & 1.03 & 1.30 & 1.88 & 5.75 \\
				GGGW & 0.13 & 0.17 & 0.26 & 0.82 & 1.27 & 1.23 & 1.84 & 5.06 \\
				GLL & 0.24 & 0.30 & 0.38 & 0.95 & 0.81 & 0.94 & 1.30 & 3.01 \\
				GLLI & 0.22 & 0.29 & 0.42 & 0.98 & 0.82 & 0.99 & 1.49 & 3.26 \\ 
				GVG & 0.42 & 1.02 & 2.58 & 22.1 & 1.22 & 2.86 & 5.94 & 40.9 \\
				\midrule
				GEE & 0.06 & 0.12 & 0.25 & 0.66 & 0.49 & 0.86 & 1.68 & 4.16 \\
				GEEA & 0.06 & 0.12 & 0.24 & 0.63 & 0.68 & 1.27 & 2.53 & 6.50 \\
				GTT & 0.06 & 0.12 & 0.25 & 0.68 & 0.70 & 1.27 & 2.34 & 6.00 \\
				GTTN & 0.07 & 0.14 & 0.29 & 0.77 & 0.71 & 1.22 & 2.55 & 6.56 \\
				$\text{GL}^2_1$ & 0.06 & 0.13 & 0.26 & 0.62 & 0.43 & 0.80 & 1.56 & 3.88 \\
				\midrule
				GVnG & 0.45 & 1.16 & 2.88 & 22.9 & 2.02 & 5.08 & 9.63 & 53.2 \\
				GEG & 0.07 & 0.12 & 0.22 & 0.61 & 0.96 & 1.30 & 1.34 & 3.51 \\
				\midrule
				PGG & 0.36 & 0.40 & 0.50 & 0.78 & 1.32 & 1.96 & 3.36 & 7.07 \\
				PGGG & 0.56 & 0.49 & 0.50 & 0.78 & 1.34 & 2.02 & 3.40 & 7.19 \\
				\midrule
				NMF-NP & 0.01 & 0.04 & 0.04 & 0.16 & 0.32 & 0.52 & 1.11 & 2.62 \\
				\bottomrule
			\end{tabular}
		\end{table*}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
	\clearpage
	\section{Nested cross-validation dimensionalities}
		In the main paper we used nested cross-validation to find the best dimensionality $K$ in each fold of the cross-validation. Table \ref{crossvalidation_table} contains the average values for each model on each dataset, rounded to the nearest integer. We used these values for the noise and sparsity experiments, to give each model their own optimal starting point. The models with ARD and Wishart priors (GGGA, GGGW, GEEA) and with the volume prior (GVG, GVnG) can often leverage higher dimensionalities $K$ than the others.
		
		For reference, the cross-validation performances are provided in Figure \ref{crossvalidation}.
		
		\begin{table*}[h]
			\caption{Average dimensionality found in 5-fold nested cross-validation for each method on the eight datasets.} \label{crossvalidation_table}
			\centering
			\begin{tabular}{lllllllll}%{llllllllllllllllll}
				\toprule
				& \multicolumn{4}{l}{Drug sensitivity} & \multicolumn{2}{l}{MovieLens} & \multicolumn{2}{l}{Methylation} \\
				\cmidrule(r){2-5} \cmidrule{6-7} \cmidrule{8-9}
				Method & GDSC & CTRP & CCLE $IC_{50}$ & CCLE $EC_{50}$ & 100K & 1M & Gene body & Promoter \\
				\cmidrule(r){1-1} \cmidrule(r){2-2} \cmidrule(r){3-3} \cmidrule(r){4-4} \cmidrule(r){5-5} \cmidrule(r){6-6} \cmidrule(r){7-7} \cmidrule(r){8-8} \cmidrule{9-9}
				GGG & 6 & 4 & 5 & 1 & 2 & 5 & 4 & 3 \\
				GGGU & 6 & 5 & 5 & 1 & 2 & 2 & 3 & 3 \\
				GGGA & 10 & 6 & 5 & 1 & 4 & 10 & 6 & 3 \\
				GGGW & 16 & 8 & 6 & 2 & 5 & 13 & 7 & 3 \\
				GLL & 10 & 6 & 5 & 1 & 3 & 8 & 4 & 2 \\
				GLLI & 10 & 6 & 5 & 1 & 2 & 7 & 4 & 2 \\
				GVG & 10 & 5 & 6 & 2 & 3 & 3 & 4 & 4 \\
				\midrule
				GEE & 8 & 6 & 5 & 1 & 2 & 8 & 6 & 5 \\
				GEEA & 10 & 6 & 5 & 1 & 2 & 10 & 6 & 4 \\
				GTT & 9 & 6 & 5 & 1 & 2 & 8 & 5 & 4 \\
				GTTN & 8 & 6 & 5 & 1 & 2 & 8 & 5 & 4 \\
				$\text{GL}^2_1$ & 6 & 6 & 4 & 1 & 2 & 8 & 5 & 5 \\
				\midrule
				GEG & 6 & 5 & 5 & 1 & 2 & 5 & 4 & 3 \\
				GVnG & 11 & 8 & 5 & 2 & 2 & 1 & 3 & 3 \\
				\midrule
				PGG & 4 & 2 & 13 & 1 & 1 & 1 & 2 & 3 \\
				PGGG & 5 & 2 & 12 & 1 & 1 & 1 & 2 & 3 \\
				\midrule
				NMF & 4 & 2 & 1 & 1 & 1 & 3 & 2 & 2 \\
				\bottomrule
			\end{tabular}
		\end{table*}
	
		\begin{figure}[h]
			\centering
			\includegraphics[width=\columnwidth]{crossvalidation_twocolumns.png}
			\caption{Average mean squared error of 5-fold nested cross-validation for the seventeen methods on the eight datasets. We also plot the standard deviation of errors across the folds.} 
			\label{crossvalidation}
		\end{figure}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\section{Sparsity and model selection plots}
		Although we presented results for the sparsity and model selection experiments on only a few datasets, we ran them both on the GDSC drug sensitivity, MovieLens 100K, and gene body methylation datasets. We give the results in Figures \ref{sparsity_gm_gdsc_ml100k} and \ref{model_selection_gm_gdsc_ml100k}.
	
		\begin{figure*}[h]
			\centering
			\includegraphics[width=0.7\textwidth]{legend_colours.png}
			\vspace{5pt}
			\includegraphics[width=0.9\textwidth]{legend.png}
			\vspace{10pt}
			\begin{minipage}{\textwidth}
				\includegraphics[width=\textwidth]{sparsity_gdsc_multiple_row.png}
				\includegraphics[width=\textwidth]{sparsity_methylation_gm_multiple_row.png}
				\includegraphics[width=\textwidth]{sparsity_movielens_100k_multiple_row.png}
				\caption{Sparsity experiment results on the GDSC drug sensitivity (top, a-e), gene body methylation (middle, f-j), and MovieLens 100K (bottom, k-o) datasets. We measure the predictive performance (mean squared error) on a held-out dataset for different fractions of unobserved data.}
				\label{sparsity_gm_gdsc_ml100k}
			\end{minipage}
			\begin{minipage}{\textwidth}
				\includegraphics[width=\textwidth]{model_selection_gdsc_multiple_row.png}
				\includegraphics[width=\textwidth]{model_selection_methylation_gm_multiple_row.png}
				\includegraphics[width=\textwidth]{model_selection_movielens_100k_multiple_row.png}
				\caption{Model selection experiment results on the GDSC drug sensitivity (top, a-e), gene body methylation (middle, f-j), and MovieLens 100K (bottom, k-o) datasets. We measure the predictive performance (mean squared error) on a held-out dataset for different dimensionalities $K$.}
				\label{model_selection_gm_gdsc_ml100k}
			\end{minipage}
		\end{figure*}
	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
%\section*{Bibliography} 
%\bibliography{bibliography}
%\bibliographystyle{abbrvnat}
		
\end{document}