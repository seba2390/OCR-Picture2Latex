\def\year{2018}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{custom_aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{
/Title (Prior and Likelihood Choices for Bayesian Matrix Factorisation on Small Datasets)
/Author (Thomas Brouwer, Pietro Li\'{o})}
\setcounter{secnumdepth}{2}  

% My packages
\usepackage{amssymb} % for maths
\usepackage{amsmath} % for maths
\usepackage{booktabs} % for table toprule and bottomrule
\usepackage{subcaption} % for figure (a) (b) in minipage
\usepackage{minipage}

% Commands for math notations
\newcommand{\R}{\boldsymbol R}
\newcommand{\E}{\boldsymbol E}
\newcommand{\U}{\boldsymbol U}
\newcommand{\Ui}{\boldsymbol{U_i}}
\newcommand{\V}{\boldsymbol V}
\newcommand{\Vj}{\boldsymbol{V_j}}
\newcommand{\D}{\boldsymbol D}
\newcommand{\btheta}{\boldsymbol \theta}

\newcommand{\sumOmega}{\sum_{(i,j) \in \Omega}}
\newcommand{\sumOmegai}{\sum_{j \in \Omega_i}}
\newcommand{\sumOmegaj}{\sum_{i \in \Omega_j}}
\newcommand{\sumk}{\sum_{k=1}^K}

 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Prior and Likelihood Choices for Bayesian Matrix Factorisation on Small Datasets}
\author{Thomas Brouwer \and Pietro Li\'{o} \\
Computer Laboratory, University of Cambridge, United Kingdom \\
}
\maketitle
\begin{abstract}
	In this paper, we study the effects of different prior and likelihood choices for Bayesian matrix factorisation, focusing on small datasets. These choices can greatly influence the predictive performance of the methods. We identify four groups of approaches: Gaussian-likelihood with real-valued priors, nonnegative priors, semi-nonnegative models, and finally Poisson-likelihood approaches. For each group we review several models from the literature, considering sixteen in total, and discuss the relations between different priors and matrix norms. We extensively compare these methods on eight real-world datasets across three application areas, giving both inter- and intra-group comparisons. We measure convergence runtime speed, cross-validation performance, sparse and noisy prediction performance, and model selection robustness. We offer several insights into the trade-offs between prior and likelihood choices for Bayesian matrix factorisation on small datasets---such as that Poisson models give poor predictions, and that nonnegative models are more constrained than real-valued ones.
\end{abstract}


\section{Introduction}
	Matrix factorisation methods have become very popular in recent years, and used for many applications such as collaborative filtering \cite{Mnih2008,Chen2009} and bioinformatics \cite{Gonen2012,Brouwer2017a}. Given a matrix relating two entity types, such as movies and users, matrix factorisation decomposes that matrix into two smaller so-called factor matrices, such that their product approximates the original one. Matrix factorisation is often used for predicting missing values in the datasets, and analysing the resulting factor values to identify biclusters or features.
	
	Most models can be categorised as being either non-probabilistic, such as the popular models by \cite{Lee2000}, or Bayesian. The former seek to minimise an error function (such as the squared error) between the original matrix and the approximation. In contrast, Bayesian variants treat the two smaller matrices as random variables, place prior distributions over them, and find the posterior distribution over their values after observing the data. A likelihood function, usually Gaussian, is used to capture noise in the dataset. Previous work \cite{Brouwer2017a} has demonstrated that Bayesian variants are much better for predictive tasks than non-probabilistic versions, which tend to overfit to noise and sparsity. 
	
	Matrix factorisation techniques can also be grouped by their constraints on the values in the factor matrices. Firstly, many approaches place no constraints, using real-valued factor matrices (commonly done in the Bayesian literature \cite{Salakhutdinov2008,Gonen2012}). Instead, we could constrain them to be nonnegative (as is popular in the non-probabilistic literature \cite{Lee2000,Tan2013}), limiting its applicability to nonnegative datasets, but making it easier to interpret the factors and potentially also making the method more robust to overfitting. Thirdly, semi-nonnegative variants constrain one factor matrix to be nonnegative, leaving the other real-valued \cite{FeiWangTaoLi2008,Ding2010}. Finally, some versions work only on count data.
	
	In the Bayesian setting, the first three groups of methods all generally use a Gaussian likelihood for noise, and place either real-valued or nonnegative priors over the matrices. For the former, Gaussian is a common choice \cite{Salakhutdinov2008,Gonen2012,Virtanen2011,Virtanen2012}, and for the latter options include the exponential distributions \cite{Schmidt2009}. The fourth group uses a Poisson likelihood to capture count data \cite{Gopalan2014,Gopalan2015,Hu2015}. These models are often extended by using complicated hierarchical prior structures over the factor matrices, giving additional behaviour (such as automatic model selection).
	%Several papers also introduced general exponential family models that capture both Gaussian and Poisson likelihood approaches \cite{Hayashi2009} [one more]. However, these papers generally do not provide a comparison between the different options.
	%Some papers, such as \cite{Muller2012}, analyse the impact that priors have on the posteriors of random variables, but in this paper we are more interested in the practical effects of prior and likelihood choices on predictive performances and factor values of matrix factorisation models.
	
	This paper offers the first systematic comparison between different Bayesian variants of matrix factorisation. Similar comparisons have been provided in other fields, such as for the regression parameter in Bayesian model averaging \cite{Ley2009,Eicher2011}, which demonstrated that the choice of prior can greatly influence the predictive performance of these models. However, a similar study for Bayesian matrix factorisation is still missing. More strikingly, many papers that introduce new matrix factorisation models do not provide a thorough comparison with competing approaches, or popular non-probabilistic ones such as \cite{Lee2000}---for example, the seminal paper by \cite{Salakhutdinov2008} compares their approach with only one other matrix factorisation method; although \cite{Gopalan2015} compares with three others.
	
	We give an overview of the different approaches that can be found in the literature, including hierarchical priors, and then study the effects of these different Bayesian prior and likelihood choices. 
	We aim to make general statements about the behaviour of the four different groups of methods on small real-world datasets (up to a million observed entries), by considering eight datasets across three different applications---four drug effectiveness datasets, two collaborative filtering datasets, and two methylation expression datasets. Our experiments consider convergence speed, cross-validation performance, sparse and noisy prediction performance, and model selection effectiveness. 
	This study offers novel insights into the differences between the four approaches, and the effects of popular hierarchical priors. 
	
	%Most interestingly, where Poisson matrix factorisation models are often claimed to provide faster inference and better predictive performances [cite, GopalanCharlinBlei], we found that on the smaller datasets that we considered, the opposite was true---with all Gaussian models consistently outperforming the Poisson ones. We provide several further interesting insights in Section \ref{Discussion}.
	
	We note that there is a rich literature of Bayesian nonparametric matrix factorisation models, which learn the size of the factor matrices automatically. However, these models often require complex inference approaches to find good solutions, and hence their predictive performance is more determined by the inference method than the precise model choices (such as likelihood and prior). In this paper we therefore focus on parametric matrix factorisation models, to isolate the effects of likelihood and prior choices.
	
	Finally, we acknowledge that the models we study were generally introduced for a specific application domain, and that this makes it hard to make general statements about the behaviour of these methods on different datasets. However, we believe that it is essential to provide a cross-application comparison of the different approaches, as this teaches us valuable lessons for the applications studied, and they are likely to apply to different areas as well. The lack of other studies exploring the trade-offs between likelihood and prior choices for Bayesian matrix factorisation make this a novel and essential study.
	

\section{Bayesian Matrix Factorisation}
	In this section we introduce the different matrix factorisation models that we study. Formally, the problem of matrix factorisation can be defined as follows. Given an observed matrix $ \R \in \mathbb{R}^{I \times J} $, we want to find two smaller matrices $ \U \in \mathbb{R}^{I \times K} $ and $ \V \in \mathbb{R}^{J \times K} $, each with $K$ so-called factors (columns), to solve $ \R = \U \V^T + \E $, where noise is captured by matrix $ \E \in \mathbb{R}^{I \times J} $. Some entries in $\R$ may be unobserved, as given by the set $ \Omega = \left\{ (i,j) \text{ $ \vert $ $ R_{ij} $ is observed} \right\} $. These entries can then be predicted by $\U \V^T$.
	
	In the Bayesian treatment of matrix factorisation, we express a likelihood function for the observed data that captures noise (such as Gaussian or Poisson). We treat the latent matrices as random variables, placing prior distributions over them. A Bayesian solution for matrix factorisation can then be found by inferring the posterior distribution $p(\btheta|D)$ over the latent variables $\btheta$ ($\U$, $\V$, and any additional random variables in our model), given the observed data $ D = \lbrace R_{ij} \rbrace_{i,j \in \Omega} $. This posterior distribution is often intractable to compute exactly, but several methods exist to approximate it (see Section \ref{Inference}). 
	
	In Section \ref{Models} we introduce a wide range of models from the literature, and categorise them into four groups. The model names are highlighted in bold in the text.
	
	\subsection{Probability Distributions}
		We introduce all notation and probability distributions in the paper below. 
		
		$\text{diag}(\boldsymbol \lambda^{-1})$ is a diagonal matrix with entries $\lambda_1^{-1}, .., \lambda_K^{-1}$ on the diagonal.
		
		$ \mathcal{N} (x|\mu,\tau^{-1}) = \tau^{\frac{1}{2}} (2\pi)^{-\frac{1}{2}} \exp \left\{ -\frac{\tau}{2} (x - \mu)^2 \right\} $ is a Gaussian distribution with precision $ \tau $. 
		
		$ \mathcal{N} (\boldsymbol x|\boldsymbol \mu,\boldsymbol \Sigma) = \vert \boldsymbol \Sigma \vert^{-\frac{1}{2}} (2\pi)^{-\frac{K}{2}} \exp \left\{ -\frac{1}{2} (\boldsymbol x - \boldsymbol \mu)^T \boldsymbol \Sigma^{-1} (\boldsymbol x - \boldsymbol \mu) \right\} $ is a $K$-dimensional multivariate Gaussian distribution.
		
		$ \mathcal{G} (\tau | \alpha_{\tau}, \beta_{\tau} ) = \frac{{\beta_{\tau}}^{\alpha_{\tau}}}{\Gamma(\alpha_{\tau})} x^{\alpha_{\tau} -1} e^{- \beta_{\tau} x} $ is a Gamma distribution, where $ \Gamma(x) = \int_{0}^{\infty} x^{t-1} e^{-x} dt $ is the gamma function. 
		
		$ \mathcal{NIW} ( \boldsymbol{\mu}, \boldsymbol{\Sigma} | \boldsymbol{\mu_0}, \beta_0, \nu_0, \boldsymbol{W_0} ) = \mathcal{N}(\boldsymbol \mu | \boldsymbol{\mu_0}, \frac{1}{\beta_0} \text{\textbf I} ) \mathcal{W}^{-1} ( \boldsymbol \Sigma | \nu_0, \boldsymbol{W_0} ) $ is a normal-inverse Wishart distribution, where $\mathcal{W}^{-1} ( \boldsymbol \Sigma | \nu_0, \boldsymbol{W_0} ) $ is an inverse Wishart distribution, and $\textbf I$ the identity matrix.
		
		$\mathcal{L} ( x | \mu, \rho ) = \frac{1}{2 \rho} \exp \left\{ -\frac{ |x - \mu| }{\rho} \right\} $ is a Laplace distribution.
		
		$ \mathcal{IG}(x|\mu, \lambda) = \frac{ \lambda }{2 \pi x^3} \exp \left\{ - \frac{\lambda (x - \mu)^2}{2 \mu^2 x} \right\} $ is an inverse Gaussian.
		
		$ \mathcal{E} ( x | \lambda ) = \lambda \exp \left\{ - \lambda x \right\} u(x) $ is an exponential distribution, where $u(x)$ is the unit step function.
		%
		\begin{equation*}
			\mathcal{TN} ( x | \mu, \tau ) = \left\{
			\begin{array}{ll}
			\displaystyle \frac{ \sqrt{ \frac{\tau}{2\pi} } \exp \left\{ -\frac{\tau}{2} (x - \mu)^2 \right\} }{ 1 - \Phi ( - \mu \sqrt{\tau} )}  & \mbox{if } x \geq 0 \\
			0 & \mbox{if } x < 0
			\end{array}
			\right.
		\end{equation*}
		is a truncated normal: a normal distribution with zero density below $ x = 0 $ and renormalised to integrate to one. $ \Phi(\cdot) $ is the cumulative distribution function of $ \mathcal{N}(0,1) $.


	\begin{table*}[t]
		\caption{Overview of the Bayesian matrix factorisation models.} \label{overview_bmf_models}
		\centering
		\begin{tabular}{llllll}
			\toprule
			Category & Name & Likelihood \hspace{2pt} & Prior $\U$ & Prior $\V$ & Hierarchical prior \\
			\midrule
			Real-valued & GGG & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $\mathcal{N} ( \Ui | \boldsymbol 0, \lambda^{-1} \text{\textbf I} )$ & $\mathcal{N} ( \Vj | \boldsymbol 0, \lambda^{-1} \text{\textbf I} )$ & - \\
			& GGGU & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $\mathcal{N} ( \Ui | \boldsymbol 0, \lambda^{-1} \text{\textbf I} )$ & $\mathcal{N} ( \Vj | \boldsymbol 0, \lambda^{-1} \text{\textbf I} )$ & - \\
			& GGGA & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $\mathcal{N} ( \Ui | \boldsymbol 0, \text{diag}(\boldsymbol \lambda^{-1}) )$ & $\mathcal{N} ( \Vj| \boldsymbol 0, \text{diag}(\boldsymbol \lambda^{-1}) )$ & $\lambda_k \sim \mathcal{G} ( \alpha_0, \beta_0 )$ \\
			& GGGW & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $\mathcal{N} ( \Ui | \boldsymbol{\mu_U}, \boldsymbol{\Sigma_U})$ & $\mathcal{N} ( \Vj | \boldsymbol{\mu_V}, \boldsymbol{\Sigma_V})$ & $(\boldsymbol{\mu_U}, \boldsymbol{\Sigma_U}) \text{ and } (\boldsymbol{\mu_V}, \boldsymbol{\Sigma_V}) $ \\
			& & & & & $\sim \mathcal{NIW} ( \boldsymbol{\mu_0}, \beta_0, \nu_0, \boldsymbol{W_0} )$ \\
			& GLL & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $\mathcal{L} ( U_{ik} | 0, \eta ) $ & $\mathcal{L} ( V_{jk} | 0, \eta ) $ & - \\
			& GLLI & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $\mathcal{L} ( U_{ik} | 0, \eta^U_{ik} ) $ & $\mathcal{L} ( V_{jk} | 0, \eta^V_{jk} ) $ & $\eta^U_{ik} \text{ and } \eta^V_{jk} \sim \mathcal{IG}(\mu, \lambda) $ \\
			& GVG & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $p(\U) \propto $ & $\mathcal{N} ( \Vj | \boldsymbol 0, \lambda^{-1} \text{\textbf I} )$ & - \\
			& & & $\exp \lbrace - \gamma \det (\U^T \U) \rbrace$ & & \\
			\midrule
			Nonnegative & GEE & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $\mathcal{E} ( U_{ik} | \lambda )$ & $\mathcal{E} ( V_{jk} | \lambda )$ & - \\
			& GEEA & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $\mathcal{E} ( U_{ik} | \lambda_k )$ & $\mathcal{E} ( V_{jk} | \lambda_k )$ & $\lambda_k \sim \mathcal{G} ( \alpha_0, \beta_0 )$ \\
			& GTT & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $\mathcal{TN} ( U_{ik} | \mu_U, \tau_U )$ & $\mathcal{TN} ( V_{jk} | \mu_V, \tau_V )$ & - \\
			& GTTN & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $\mathcal{TN} ( U_{ik} | \mu_U, \tau_U )$ & $\mathcal{TN} ( V_{jk} | \mu_V, \tau_V )$ & $p(\mu^U_{ik}, \tau^U_{ik} | \mu_{\mu}, \tau_{\mu}, a, b) \propto $ \\
			& & & & & $\frac{1}{\sqrt{\tau^U_{ik}}} \left( 1 - \Phi ( - \mu^U_{ik} \sqrt{\tau^U_{ik}} ) \right)$ \\
			& & & & & $\mathcal{N} (\mu^U_{ik} | \mu_{\mu}, \tau_{\mu}^{-1} ) \mathcal{G} (\tau^U_{ik} | a, b)$ \\
			& G$\text{L}^2_1 $ & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $p(\U) \propto \exp $ & $p(\V) \propto \exp $ & - \\
			& & & $ \lbrace -\frac{\lambda}{2} \sum_i ( \sum_k U_{ik} )^2 \rbrace $ & $ \lbrace -\frac{\lambda}{2} \sum_j ( \sum_k V_{jk} )^2 \rbrace $ & \\
			& & & with $U_{ik} \geq 0$ & with $V_{jk} \geq 0$ & \\
			\midrule
			Semi- & GEG & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & $\mathcal{E} ( U_{ik} | \lambda )$ & $\mathcal{N} ( \Vj | \boldsymbol 0, \lambda^{-1} \text{\textbf I} )$ & - \\
			nonnegative & GVnG & $\mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$ & GVG with $U_{ik} \geq 0$ & $\mathcal{N} ( \Vj | \boldsymbol 0, \lambda^{-1} \text{\textbf I} )$ & - \\
			\midrule
			Poisson & PGG & $\mathcal{P} (R_{ij} | \Ui \Vj )$ & $\mathcal{G} ( U_{ik} | a, b )$ & $\mathcal{G} ( V_{jk} | a, b )$ & - \\
			& PGGG & $\mathcal{P} (R_{ij} | \Ui \Vj )$ & $\mathcal{G} ( U_{ik} | a, h^U_i )$ & $\mathcal{G} ( V_{jk} | a, h^V_j )$ & $h^U_i \text{ and } h^V_j \sim \mathcal{G} (a', \frac{a'}{b'})$ \\
			\bottomrule
		\end{tabular}
	\end{table*}

	\subsection{Models} \label{Models}
		There are three types of choices we make that determine the type of matrix factorisation model we use: the likelihood function, the priors we place over the factor matrices $\U$ and $\V$, and whether we use any further hierarchical priors. 
		We have identified four different groups of Bayesian matrix factorisation approaches based on these choices: Gaussian-likelihood with real-valued priors, nonnegative priors (constraining the matrices $\U, \V$ to be nonnegative), semi-nonnegative models (constraining one of the two factor matrices to be nonnegative), and finally Poisson-likelihood approaches. 
		Models within each group use different priors and hierarchical priors, and many choices can be found in the literature. In this paper we consider a total of sixteen models, as summarised in Table \ref{overview_bmf_models}. We have focused on fully conjugate models (meaning the prior and likelihood are in the same family of distributions) to ensure inference for each model is guaranteed to work well, so that all performance differences in Section \ref{Experiments} come entirely from the choice of likelihood and priors.
		
		The first three groups all use a Gaussian likelihood for noise, by assuming each value in $\R$ comes from the product of $\U$ and $\V$, $R_{ij} \sim \mathcal{N} (R_{ij} | \Ui \Vj, \tau^{-1} )$, with Gaussian noise added of precision $\tau$, for which we use a Gamma prior $\mathcal{G} (\tau | \alpha_{\tau}, \beta_{\tau} )$. 
		The last group instead opt for a Poisson likelihood, $ R_{ij} \sim \mathcal{P} (R_{ij} | \Ui \Vj )$. This only works for nonnegative count data, with $ \R \in \mathbb{N}^{I \times J} $, but has been studied extensively in the literature due to the popularity and prevalence of datasets like the Netflix Challenge. 
		
		\paragraph{Real-valued matrix factorisation}
			The most common approach is to use independent zero-mean Gaussian priors for $\U, \V$ \cite{Salakhutdinov2008,Gonen2012,Virtanen2011,Virtanen2012}, which gives rise to the \textbf{GGG} model. The \textbf{GGGU} model is identical but uses a univariate posterior for inference (see supplementary materials).
			
			The first hierarchical model (\textbf{GGGA}) uses the Bayesian automatic relevance determination (ARD) prior, which helps with model selection. The main idea is to replace the $\lambda$ hyperparameter by a factor-specific variable $\lambda_k$, which has a further Gamma prior. This causes all entries in columns of $\U$ and $\V$ to go further to zero if only a few values in that column are high, effectively making the factor inactive. This prior has been used for real-valued \cite{Virtanen2011,Virtanen2012} and nonnegative matrix factorisation \cite{Tan2013}.
			
			Another hierarchical model (\textbf{GGGW}) was introduced in the seminal paper of \cite{Salakhutdinov2008}. Instead of assuming independence of each entry in $\U, \V$, we assume each row of $\U$ comes from a multivariate Gaussian with row mean $\boldsymbol{\mu_U}$ and covariance $\boldsymbol{\Sigma_U}$, and similarly for $\V$. We then place a further Normal-Inverse Wishart prior over these parameters. 
			
			An alternative to the Gaussian prior is to use the Laplace distribution \cite{Jing2015}, which has a much more pointy distribution than Gaussian around $x=0$. This leads to more sparse solutions, as more factors are set to low values. The basic model (\textbf{GLL}) can be extended with a hierarchical Inverse Gaussian prior over the $\eta$ parameter (\textbf{GLLI}), which they claim helps with variable selection.
			
			The final model (\textbf{GVG}) was introduced by \cite{Arngren2011}. They used a volume prior for the $\U$ matrix, with density $p(\U) \propto \exp \lbrace - \gamma \det (\U^T \U) \rbrace $. The $\gamma$ hyperparameter determines the strength of the volume penalty (higher means stronger prior).
			
		\paragraph{Nonnegative matrix factorisation}
			These models all place nonnegative prior distributions over entries in $\U$ and $\V$, and as a result can only deal with nonnegative datasets. 
			
			\cite{Schmidt2009} introduced a model using exponential priors over the factor matrices (\textbf{GEE}). This model can also be extended with ARD \cite{Brouwer2017b} (\textbf{GEEA}). Another option is to use the truncated normal distribution (\textbf{GTT}), which can also be extended by placing a hierarchical prior over the mean and precision $\mu_U, \tau_U, \mu_V, \tau_V$ (\textbf{GTTN}), as done by \cite{MikkelN.Schmidt2009}. This nontrivial prior cannot be sampled from directly, but will be useful for inference.
			
			Finally, we can use a prior inspired by the $\text{L}^2_1$ norm for both $\U$ and $\V$ (\textbf{G$\boldsymbol{\text{L}^2_1}$}), as we will discuss in Section \ref{PriorsNorms}.
			
		\paragraph{Semi-nonnegative matrix factorisation} 
			Instead of forcing nonnegativity on both factor matrices, we could place this constraint on only one, as was done in \cite{FeiWangTaoLi2008,Ding2010}. In the Bayesian setting we place a real-valued prior over one matrix, and a nonnegative prior over the other. The major advantage is that we can handle real-valued datasets, while still enforcing some nonnegativity. However, we will see in Section \ref{Experiments} that its performance is identical to the real-valued approaches.
			
			Firstly we can use an exponential prior for entries in $\U$, and a Gaussian for $\V$, effectively combining the GGG and GEE models into one (\textbf{GEG}). 
			Another semi-nonnegative model (\textbf{GVnG}) comes from constraining the volume prior in the GVG model to also be nonnegative: $p(\U) = 0$ if any $U_{ik} < 0$.
			
		\paragraph{Poisson likelihood}
			The standard Poisson matrix factorisation model (\textbf{PGG}) uses independent Gamma priors over the entries in $\U$ and $\V$, with hyperparameters $a, b$ \cite{Gopalan2014,Gopalan2015,Hu2015}. 
			This model can also be extended with a hierarchical prior (\textbf{PGGG}), by replacing $b$ with $h^U_i, h^V_j$ and placing a further Gamma prior over these parameters \cite{Gopalan2015}.


\section{Priors and Norms} \label{PriorsNorms}
	The prior distributions in Bayesian models act as a regulariser that prevents us from overfitting to the data, preventing poor predictive performance. We can write out the expression of the log posterior of the parameters, which for a Gaussian likelihood and no hierarchical priors becomes
	%
	\begin{alignat*}{1}
		& \log p(\btheta|D) = \log p(D|\btheta) + \log p(\btheta) + C_1 \\
		&\hspace{25pt} = \sumOmega \log p(R_{ij}|\Ui \Vj, \tau^{-1}) + \log p(\U, \V) + C_2 \\
		&\hspace{25pt} = - \frac{\tau}{2} \sumOmega ( R_{ij} - \Ui \Vj)^2 + \log p(\U, \V) + C_3
	\end{alignat*}
	%
	for some constants $C_i$. Note that this last expression is simply the negative Frobenius norm (squared error) of the training fit, plus a regularisation term over the matrices $\U, \V$. This training error is frequently used in the nonprobabilistic matrix factorisation literature \cite{Lee2000,Pauca2004,Pauca2006}, where different regularisation terms are used. 
	These are often based on row-wise \textbf{matrix norms}, such as
	%
	\begin{alignat*}{2}
		& \text{L}_1 = \sum_{i=1}^I \sum_{k=1}^K U_{ik}
		\quad\quad && \text{L}_2 = \sum_{i=1}^I \sqrt{\sum_{k=1}^K U_{ik}} \\
		& \text{L}^2_1 = \sum_{i=1}^I (\sum_{k=1}^K U_{ik})^2
		\quad\quad && \text{L}^2_2 = \sum_{i=1}^I \sum_{k=1}^K U_{ik}^2
	\end{alignat*}
	%
	This offers some interesting insights: the $\text{L}^2_2$ norm is equivalent to an independent Gaussian prior (GGG), due to the square in the exponential of the Gaussian prior; the $\text{L}_1$ norm is equivalent to a Laplace prior distribution (GLL); if we constrain $\U, \V$ to be nonnegative then the $\text{L}_1$ norm is equivalent to an exponential prior distribution (GEE); and finally, the $\text{L}^2_1$ norm can be formulated as a nonnegative prior distribution, which we use for the $\text{GL}^2_1$ model (see Table \ref{overview_bmf_models}).
	
	In other words, the type of priors chosen for Bayesian matrix factorisation determine the type of regularisation that we add to the model. Additionally, we can use hierarchical priors to model further desired behaviour (such as ARD).


\section{Model Discussion} 
	\subsection{Inference} \label{Inference}
		In this paper we use Gibbs sampling (see Section \ref{Inference}), because it tends to be very accurate at finding the true posterior \cite{Brouwer2017b}, but other methods like variational Bayesian inference are also possible. The Gibbs sampling algorithms, together with their time complexities, are given in the supplementary materials.
		
	\subsection{Hyperparameters} \label{Hyperparameters}
		\begin{figure}[t]
			\centering
			\includegraphics[width=\columnwidth]{priors.png}
			\caption{Plots of the prior distributions with hyperparameters from Section \ref{Hyperparameters}.}
			\label{priors_plot}
		\end{figure}
	
		The hyperparameter values we choose for each model can influence their performance, especially when the data is sparse. The hierarchical models try to automatically choose the correct values, by placing a prior over the original hyperparameters. This introduces new hyperparameters, but the models are generally less sensitive to these. 
		
		However, in our experience even the models without hierarchical priors are not very sensitive to this choice, as long as we use fairly weak priors. In particular, we used $\lambda = 0.1$ (GGG, GGGU, GEE, GTT, $\text{GL}^2_1$, GEG), $\eta = \sqrt{10}$ (GLL), and $a = 1, b = 1$ (PGG). The distributions with these hyperparameter values are plotted in Figure \ref{priors_plot}.
		
	\begin{figure*}[t]
		\centering
		\includegraphics[width=0.255\columnwidth]{plot_gdsc_2.png}
		\includegraphics[width=0.255\columnwidth]{plot_ctrp_2.png}
		\includegraphics[width=0.255\columnwidth]{plot_ccle_ic_2.png}
		\includegraphics[width=0.255\columnwidth]{plot_ccle_ec_2.png}
		\includegraphics[width=0.255\columnwidth]{plot_movielens_100k_2.png}
		\includegraphics[width=0.255\columnwidth]{plot_movielens_1m_2.png}
		\includegraphics[width=0.255\columnwidth]{plot_methylation_gm_2.png}
		\includegraphics[width=0.255\columnwidth]{plot_methylation_pm_2.png}
		\caption{Distributions of the values of the four drug sensitivity, two MovieLens, and two methylation datasets.} %: GDSC $IC_{50}$, CTRP $EC_{50}$, CCLE $IC_{50}$, CCLE $EC_{50}$, MovieLens 100K, MovieLens 1M, gene body methylation, and promoter-region methylation.}
		\label{dataset_plots}
	\end{figure*}

		For the other models we used:
		$\alpha_{\tau} = \beta_{\tau} = 1 $ (Gaussian likelihood);
		$\alpha_0 = \beta_0 = 1 $ (GGGA, GEEA);
		$\boldsymbol{\mu_0} = \boldsymbol{0}, \beta_0 = 1, \nu_0 = K, \boldsymbol{W_0} = \text{\textbf{I}} $ (GGGW);
		$\mu = \lambda = K $ (GLLI),
		$ \mu_{\mu} = 0, \tau_{\mu} = 0.1, a = b = 1 $ (GTTN),
		$a = a' = b' = 1$ (PGGG).
		
		We did find that the volume prior models (GVG, GVnG) were very sensitive to the hyperparameter choice $\gamma$. The following values were chosen by trying a range on each dataset and choosing the best one: $ \gamma = 10^{\lbrace-30,-20,-10,-10,0,0,0,0\rbrace}$ for \{GDSC,CTRP,CCLE $IC_{50}$,$EC_{50}$,MovieLens 100K,1M,GM,PM\}.

	\subsection{Software}
		Implementations of all models, datasets, and experiments, are available at \url{https://github.com/Anonymous/}.
	
	
\section{Datasets}
	We conduct our experiments on a total of eight real-world datasets across three different applications, allowing us to see whether our observations on one dataset or application also hold more generally. We will focus on one or two datasets at a time for more specific experiments. Also note that we make sure all datasets contain only positive integers, so that we can compare all four groups of Bayesian matrix factorisation approaches. 
	
	\begin{table}[t]
		\caption{Overview of the four drug sensitivity, two MovieLens, and two methylation datasets, giving the number of rows (cell lines, users, genes), columns (drugs, movies, patients), and the fraction of entries that are observed.} \label{summary_datasets}
		\centering
		\begin{tabular}{lccc}
			\toprule
			Dataset & Rows & Columns & Fraction obs. \\
			\midrule
			GDSC $IC_{50}$ & 707 & 139 & 0.806 \\
			CTRP $EC_{50}$ & 887 &  545 & 0.801 \\
			CCLE $IC_{50}$ & 504 & 24 & 0.965 \\
			CCLE $EC_{50}$ & 502 & 24 & 0.632 \\
			MovieLens 100K & 943 & 1473 & 0.072 \\
			MovieLens 1M & 6040 & 3503 & 0.047 \\
			Gene body meth. & 160 & 254 & 1.000 \\
			Promoter meth. & 160 & 254 & 1.000 \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	The first comes from bioinformatics, in particular predicting missing values in drug sensitivity datasets, each detailing the effectiveness ($IC_{50}$ or $EC_{50}$ values) of a range of drugs on different cancer and tissue types (cell lines). We consider the Genomics of Drug Sensitivity in Cancer (GDSC v5.0 \cite{Yang2013}, $IC_{50}$), Cancer Therapeutics Response Portal (CTRP v2 \cite{Seashore-Ludlow2015}, $EC_{50}$), and Cancer Cell Line Encyclopedia (CCLE \cite{Barretina2012}, both $IC_{50}$ and $EC_{50}$) datasets. 
	We preprocessed these datasets by: undoing the natural log transform of the GDSC dataset; capping high values to 100 for GDSC and CTRP; and then casting them as integers. We also filtered out rows and columns with only one or two observed datapoints. 
	
	The second application is collaborative filtering, where we are given movie ratings for different users (one to five stars) and we wish to predict the number of stars a user will give to an unseen movie. We use the MovieLens 100K and 1M datasets \cite{Harper2015}, with 100,000 and 1,000,000 ratings respectively.
	
	Finally, another bioinformatics application, this time looking at methylation expression profiles \cite{Koboldt2012}. These datasets give the amount of methylation measured in either the body region of 160 breast cancer driver genes (gene body methylation) or the promoter region (promoter methylation) for 254 different patients. We multiplied all values by twenty and cast them as integers. 
	
	The datasets are summarised in Table \ref{summary_datasets}, and the distribution of values for each dataset is visualised in Figure \ref{dataset_plots}. This shows us that the drug sensitivity datasets tend to be bimodal, whereas the MovieLens and methylation datasets are more normally distributed. We can also see that the MovieLens datasets tend to be large and sparse, whereas the others are well-observed and relatively small.
	
	

\section{Experiments} \label{Experiments}
	We conducted experiments to compare the four different groups of approaches. In particular, we measured their convergence speed, cross-validation performance, sparse prediction performance, and model selection effectiveness. We sometimes focus on a selection of the methods for clarity. To make the comparison complete, we also added a popular non-probabilistic nonnegative matrix factorisation model (NMF) \cite{Lee2000} as a baseline.
	The results are discussed in Section \ref{Discussion}.

	\subsection{Convergence}% and Runtime Speed}
		Firstly we compared the convergence speed of the models on the GDSC and MovieLens 100K datasets. We ran each model with $K=20$, and measured the average mean squared error on the training data across ten runs. We plotted the results in Figure \ref{convergences}, where each group is plotted as the same colour: red for real-valued, blue for nonnegative, green for semi-nonnegative, yellow for Poisson, and grey for the non-probabilistic baseline.
		%We can see that the methods differ both in how quickly they converge, and their convergence depth. Generally, the Bayesian approaches converge faster than the non-probabilistic method. Real-valued methods converge the deepest, while nonnegative ones are more constrained and hence converge less deep. The semi-nonnegative models converge similarly to the real-valued ones. The Poisson versions converge slowly (on GDSC) and less deep (on MovieLens 100K).
		Runtime speeds are given in the supplementary materials.
		
		\begin{figure*}[t]
			\centering
			\begin{minipage}{0.38 \textwidth}
				\centering
				\includegraphics[width=\columnwidth]{crossvalidation.png}
				\caption{Average mean squared error of 5-fold nested cross-validation for the seventeen methods on the eight datasets. We also plot the standard deviation of errors across the folds.} 
				\label{crossvalidation}
			\end{minipage}
			\hspace{0.02\textwidth}
			\begin{minipage}{0.59 \textwidth}
				\begin{minipage}{\columnwidth}
					\centering
					\includegraphics[width=1\columnwidth]{legend_colours.png}
					
					\vspace{5pt}
					
					\includegraphics[width=0.495\columnwidth]{convergences_gdsc_annotated.png}
					\includegraphics[width=0.495\columnwidth]{convergences_movielens100k.png}
					\caption{Convergence of the models on the GDSC drug sensitivity (left) and MovieLens 100K (right) datasets, measuring the training data fit (mean square error).} 
					\label{convergences}
				\end{minipage}
			
				\vspace{25pt}
			
				\begin{minipage}{\columnwidth}
					\centering
					\includegraphics[width=\columnwidth]{noise_bar_gdsc.png}
					\caption{Noise experiment results on the GDSC drug sensitivity dataset. We added different levels of Gaussian noise to the data, and measured the 10-fold cross-validation performance.}
					\label{noise_gdsc}
				\end{minipage}
			\end{minipage}
		\end{figure*}
	
	
	\subsection{Cross-validation} \label{Cross-validation}
		Our first predictive experiment was to measure the 5-fold cross-validation performance on each of the eight datasets. We used the hyperparameter values from Section \ref{Hyperparameters}, and used 5-fold nested cross-validation to choose the dimensionality $K$. The average mean squared error of predictions are given in Figure \ref{crossvalidation} for all eight datasets. The average dimensionality found in nested cross-validation can be found in the supplementary materials.
		%Table \ref{crossvalidation_table}, together with the average dimensionality found in the nested cross-validation (rounded to the nearest integer).
		
	\subsection{Noise test}
		We then measured the predictive performance when the datasets are very noisy. We added different levels of Gaussian noise to the data, with the noise-to-signal ratio being given by the ratio of the variance of the Gaussian noise we add, to the standard deviation of the generated data. For each noise level we split the datapoints randomly into ten folds, and measured the predictive performance of the models on one held-out set at a time.  We used $K=5$ for all methods. The results for the GDSC drug sensitivity dataset are given in Figure \ref{noise_gdsc}, where we plot the ratio of the variance of the data to the mean squared error of the predictions---higher values are better, and using the row average gives a performance of one. 
		
		\begin{figure*}[t]
			\centering
			\includegraphics[width=0.7\textwidth]{legend_colours.png}
			\vspace{5pt}
			\includegraphics[width=0.9\textwidth]{legend.png}
			\vspace{5pt}
			\begin{minipage}{\textwidth}
				\includegraphics[width=\textwidth]{sparsity_gdsc_multiple_row.png}
				
				\includegraphics[width=\textwidth]{sparsity_methylation_gm_multiple_row.png}
				\caption{Sparsity experiment results on the GDSC drug sensitivity (top, a-e) and gene body methylation (bottom, f-j) datasets. We measure the predictive performance (mean squared error) on a held-out dataset for different fractions of unobserved data.}
				\label{sparsity_gm_gdsc}
			\end{minipage}
			\begin{minipage}{\textwidth}
				\includegraphics[width=\textwidth]{model_selection_gdsc_multiple_row.png}
				\caption{Model selection experiment results on the GDSC drug sensitivity dataset. We measure the predictive performance (mean squared error) on a held-out dataset for different dimensionalities $K$.}
				\label{model_selection_gdsc}
			\end{minipage}
		\end{figure*}
	
	\subsection{Sparse predictions}
		Next we measured the predictive performances when the sparsity of the data increases. For different fractions of unobserved data, we randomly split the data based on that fraction, trained the model on the observed data, and measured the performance on the held-out test data. We used $K=5$ for all models.
		The average mean squared error of ten repeats is given in Figure \ref{sparsity_gm_gdsc}, showing the performances on both the methylation GM and GDSC drug sensitivity datasets.
	
	\subsection{Model selection}
		We also measured the robustness of the models to overfitting if the dimensionality $K$ is high. As a result, most models will fit very well to the training data, but give poor predictions on the test data. Here, we vary the dimensionality $K$ for each of the models on the GDSC drug sensitivity dataset, randomly taking out $10\%$ as test data, and repeating ten times.
		The results are given in Figure \ref{model_selection_gdsc}---in the supplementary materials we look at two more datasets.
		

\section{Discussion} \label{Discussion}
	From the results shown in the previous section, we were able to draw the following conclusions. 
	
	Observation 1: Poisson likelihood methods perform poorly compared to the Gaussian likelihood---they overfit quickly (Figures \ref{model_selection_gdsc}a), give worse predictive performances in cross-validation (Figure \ref{crossvalidation}) and under noisy conditions (Figure \ref{noise_gdsc}), presumably because they cannot converge as deep as the other methods (Figure \ref{convergences}). At high sparsity levels they can start to perform better (Figure \ref{sparsity_gm_gdsc}d). Some papers \cite{Gopalan2015} claim that Poisson models offer better predictions, but for small and well-observed datasets we found the opposite to be true. 
	
	Observation 2: Nonnegative models are more constrained than the real-valued ones, causing them to converge less deep (Figure \ref{convergences}), and to be less likely to overfit to high sparsity levels (\ref{sparsity_gm_gdsc}c, \ref{sparsity_gm_gdsc}h) than the standard GGG model. However, the right hierarchical prior for a real-valued model (such as Wishart) can bridge this gap.
	
	Observation 3: There is no difference in performance between real-valued and semi-nonnegative matrix factorisation, as shown in the model selection and sparsity experiments (Figures \ref{sparsity_gm_gdsc}e, \ref{sparsity_gm_gdsc}j, and \ref{model_selection_gdsc}e): the performance for GGG and GEG, as well as GVG and GVnG, are nearly identical.
		
	Observation 4: There is no difference in predictive performance between univariate and multivariate posteriors (GGG, GGGU), as shown in Figures \ref{sparsity_gm_gdsc}b and \ref{sparsity_gm_gdsc}g.
		
	Observation 5: The automatic relevance determination and Wishart hierarchical priors are effective ways of preventing overfitting, as shown in Figures \ref{model_selection_gdsc}b and \ref{model_selection_gdsc}c: the GGGA, GGGW, and GEEA models keep the line down as $K$ increase, whereas the GGG and GEE models start overfitting more. This has been shown before for nonnegative models \cite{Brouwer2017b} but the effect is even stronger for the real-valued ones. 
		
	Overvation 6: Similarly, the Laplace priors are good at reducing overfitting as the dimensionality grows (Figure \ref{model_selection_gdsc}b), without requiring additional hierarchical priors.
		
	Observation 7: Some other hierarchical priors do not make a difference, such as with GLLI, GTTN, PGGG---Figures \ref{sparsity_gm_gdsc}d, \ref{sparsity_gm_gdsc}i, and \ref{model_selection_gdsc}d show little difference in performance. They can help us automatically choose the hyperparameters, but in our experience the models are not very sensitive to this choice anyways. 
	
	Although these observations are specific to the applications and dataset sizes studied, we believe that general insights can be drawn from them about the behaviour of the four different groups of Bayesian matrix factorisation models. The behaviour of Poisson models is especially interesting, because they are often claimed to be better than Gaussian models for large datasets, but for smaller ones this does not hold. We hope that these insights will assist future researchers in their model design.

\bibliography{bibliography}
%\bibliographystyle{aaai}

\end{document}
