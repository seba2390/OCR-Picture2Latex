\section{Introduction}

In classical scheduling models, the input consists of a description of the available processors,
and a set of jobs with associated processing times.
Each processor can process one job at any point in time.
Additional constraints may be part of the model.
One way to model complex, parallelizable tasks are moldable\footnote{Some authors use the term malleable.} jobs,
which have a variable parallelizability~\cite{du89}.
Formally, we are given a set~$J$ consisting of $n$~jobs and a number~$m$ of processors.
The processing time $\ptm{j}{1}$ on one processor is given for each job~$j$,
as well as the speedup~$\fcall{\mathrm{s}_j}{k}$ that is achieved when executing it on $k > 1$ processors.
The processing time on $k$ processors then is given as~$\ptm{j}{k} = \frac{\ptm{j}{1}}{\fcall{\mathrm{s}_j}{k}}$.
The goal is to produce a schedule that assigns for each job a starting time and a number of allotted processors
such that the \emph{makespan}, i.e.~the completion time of the last job, is minimized.

Without restriction, we assume that the speedup is non-decreasing,
or equivalently, the processing time is non-increasing in the number of processors.
A job is called monotone if its work function $\work{j}{k} = k \times \ptm{j}{k}$ is non-decreasing.
This is a reasonable assumption, since an increased number of processors requires more communication.
Monotony helps when designing algorithms~\cite{belkhale90,mounie99,mounie07}.
Sometimes even stronger\footnote{For a proof that concave speedup functions imply monotony, see~\cite{jansen12}.}
assumptions are made, e.g.~that the speedup functions are concave~\cite{blazewicz06,sanders11,jansen12}.

Since the problems considered here are \cclass{NP}-hard,
we will discuss approximation algorithms.
An algorithm for a minimization problem is $c$-approximate
if it produces a solution of value at most $c \OPT*{I}$ for each instance~$I$.
The number~$c \geq 1$ is called its approximation guarantee.

We pay extra attention to the encoding length~$\clen{I}$ in dependence of the number~$m$ of processors.
The running time of most algorithms is polynomial in~$m$~\cite{belkhale90,turek92,jansen03,jansen06a,mounie07,jansen10}.
Many authors expect that the values $\ptm{j}{k}$, $k \in \setrange{1}{m}$ are explicitly given as a list,
such that $m = \Landau{<=}{\clen{I}}$.
Under this assumption, these algorithms' running time is polynomial in the input size.
On the other hand, more compact encodings are conceivable in many cases.
Sometimes it is assumed that the processing time function is of a special form,
e.g.~linear~\cite{grigoriev06} or a power function~\cite{makarychev14},
which can be described with a constant number of coefficients.
Since the number of processors is encoded in $\log m$ bits,
the aforementioned algorithms can have a running time that is exponential in the input length
when compact enconding is used.

It is our main goal to develop fully polynomial algorithms for instances with compact input encoding,
i.e.~algorithms with a running time polynomial in~$\log m$.
Such algorithms will outperform algorithms whose running time is polynomial in $m$
for large values of $m$ (super-polynomial in the input size).
Only few known algorithms are fully polynomial in this sense~\cite{mounie99,sanders11,jansen13a}.
Since we do not want to stipulate a certain form of speedup functions,
we assume that the running times~$\ptm{j}{k}$ can be accessed via some oracle in constant time.


\paragraph*{Previous Results}

It is known that finding an exact solution without monotony is \cclass{NP}-hard~\cite{du89}.
If the jobs are monotone,
it is only known that finding an exact solution is weakly \cclass{NP}-hard~\cite{jansen13c}.
Both results even hold for a constant number of processors.
As a consequence, they also hold with compact input encoding.
The problem complexity actually depends on the used input encoding.
It is known that there is no polynomial time algorithm for scheduling of parallel jobs
with approximation guarantee less than $\frac{3}{2}$,
unless $\cclass{P} = \cclass{NP}$.
This can be deduced from a reduction from the partition problem to scheduling of parallel jobs~\cite{drozdowski95}.
By setting
$\ptm{j}{k} = t_j$ if $k \geq \mathrm{size}(j)$ and $\ptm{j}{k} = \infty$ otherwise,
we can extend the result to scheduling of moldable jobs,
although the resulting work functions are not monotone.
However, this reduction is polynomial only if we use a compact encoding for the resulting instance.
Furthermore, if we allow algorithms to be polynomial in~$m$,
the produced instances can be optimally solved:
since the reduction is one-to-one, we can go back to the original partition instance,
solve it via dynamic programming in time~$\Landau{<=}{nm}$,
and convert the solution back to the scheduling setting.
Indeed, without compact encoding, the problem admits a PTAS~\cite{jansen10}.


Considering approximate algorithms,
Belkhale and Banerjee~\cite{belkhale90} found a $2$-approximate algorithm for scheduling monotone moldable tasks.
This approximation guarantee was later matched without monotony by an algorithm due to Turek, Wolf, and Yu~\cite{turek92}.
The running time was later improved by Ludwig and Tiwari~\cite{ludwig94}.
Their algorithm for the case of monotone jobs was the first to achieve a running time polynomial in $\log m$, namely $\Landau{<=}{n \log^2 m}$.
Mounié, Rapine, and Trystram improved the approximation guarantee with monotony to~$\sqrt{3} + \epsilon \approx 1.73$,
with arbitrarily small~$\epsilon > 0$,
also with polylogarithmic dependence on~$m$.
They later presented a $(\frac{3}{2} + \epsilon)$-approximate algorithm with running time~$\Landau{<=}{nm \log \frac{1}{\epsilon}}$~\cite{mounie04,mounie07}.
A PTAS with running time polynomial in~$m$ was subsequently developed that does not require monotony~\cite{jansen10}.
Finally, a $(\frac{3}{2} + \epsilon)$-approximate algorithm with polylogarithmic dependence on~$m$ that also does not assume monotone jobs was developed by Jansen~\cite{jansen13a}.


\paragraph*{Our Contribution}

We improve the understanding of scheduling monotone moldable jobs in several ways.
In \cref{sec:hardness} we resolve the complexity of the considered problem.
\begin{theorem}
  \label{thm:np-complete}
  It is \cclass{NP}-complete to decide whether a set of monotone jobs can be scheduled with a given makespan.
\end{theorem}
We proceed to describe an extremely efficient FPTAS for the case that the number of machines is large enough in \cref{sec:fptas}.
\begin{theorem}
  \label{thm:fptas}
  There is an FPTAS  for the case that $m \geq 8\frac{n}{\epsilon}$
  with a running time of $\Landau{<=}{n \log^2 m (\log m + \log \frac{1}{\epsilon})}$.
\end{theorem}
In combination with the PTAS by Jansen and Thöle~\cite{jansen10},
this yields a PTAS for scheduling of monotone moldable jobs
with compact encoding of running times.
The algorithm by Jansen~\cite{jansen13a} achieves the same approximation guarantee
in the more general case without monotony,
but has a significantly worse running time.
In particular, our new algorithm's running times are polynomial in $\frac{1}{\epsilon}$,
while Jansen's algorithm is doubly exponential in $\frac{1}{\epsilon}$.

In \cref{sec:approximation} we first describe the $(\frac{3}{2} + \epsilon)$-approximate algorithm
due to Mounié, Rapine, and Trystram~\cite{mounie07},
and improve its running time to fully polynomial.
We further present techniques to gradually reduce the dependence of the running time on the number~$n$ of jobs.
\begin{theorem}
  \label{thm:dual-approx}
  For each $T$ given in Table~\ref{tbl:running-times}, there is a $(\frac{3}{2} + \epsilon)$-approximate algorithm
  with running time~$\Landau{<=}{n \log^2 m + \log \frac{1}{\epsilon} T(n, m, \epsilon)}$.
  \begin{table}
    \caption{Running times of our $(\frac{3}{2} + \epsilon)$-dual algorithms.}
    \label{tbl:running-times}
    \centering
    \begin{tabular}{l l}
      \toprule
        Algorithm & $T(n, m, \epsilon)$ \\
      \midrule
        Section~\ref{sec:reducing-kp-problems} &
          $\Landau{<=}{n (\log m + n \log \epsilon m)}$ \\
        Section~\ref{sec:bounded-kp} &
          $\Landau[2]{<=}{n \parens[2]{\frac{1}{\epsilon^2} \log m \parens[2]{\frac{\log m}{\epsilon} + \log^3 (\epsilon m)} + \log n}}$ \\
        Section~\ref{sec:linear} &
          $\Landau[2]{<=}{n \frac{1}{\epsilon^2} \log m \parens[2]{\frac{\log m}{\epsilon} + \log^3 (\epsilon m)}}$ \\
      \bottomrule
    \end{tabular}
  \end{table}
\end{theorem}

We make repeated use of a technique we call compression.
It reduces the number of processors used by a job in exchange for a bounded increase in the running time.
Compression allows us to approximate processor numbers for jobs that are allotted to a large number of processors.
This enables the use of various rounding techniques.
The intermediate solution then may use more than~$m$ processors,
before the jobs are finally compressed such that they require at most~$m$ processors.
This is similar to models with resource augmentation (see e.g.~\cite{chekuri04}),
except that we can use additional processors only for jobs that are allotted to a large number of processors.
