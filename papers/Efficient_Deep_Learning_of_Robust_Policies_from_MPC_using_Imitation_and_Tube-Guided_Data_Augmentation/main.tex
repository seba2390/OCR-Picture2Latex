\documentclass[letterpaper, 10 pt, journal, twoside]{IEEEtran}

\input{utils/usepackage.tex}
\input{utils/acronyms.tex}
\input{utils/macros.tex}

\begin{document}
\title{Efficient Deep Learning of Robust Policies\\ 
from MPC using Imitation and\\ Tube-Guided Data Augmentation}
\author{Andrea Tagliabue and Jonathan P.\ How%
	\thanks{The authors are with the Department of Aeronautics and Astronautics, Massachusetts Institute of Technology.
	    {\texttt{\{atagliab, jhow\}@mit.edu.}}}
    \thanks{Work funded by the Air Force Office of Scientific Research MURI FA9550-19-1-0386.}
}%

\maketitle

\begin{abstract}
Imitation Learning (IL) has been increasingly employed to generate computationally efficient policies from task-relevant demonstrations provided by Model Predictive Control (MPC). However, commonly employed IL methods are often data- and computationally-inefficient, as they require a large number of MPC demonstrations, resulting in long training times, and they produce policies with limited robustness to disturbances not experienced during training.
In this work, we propose an IL strategy to \textit{efficiently} compress a computationally expensive MPC into a deep neural network policy that is \textit{robust} to previously unseen disturbances. 
By using a robust variant of the MPC, called Robust Tube MPC, and leveraging properties from the controller, we introduce a computationally-efficient data augmentation method that enables a significant reduction of the number of MPC demonstrations and training time required to generate a robust policy. %
Our approach opens the possibility of \textit{zero-shot} transfer of a policy trained from a single MPC demonstration collected in a nominal domain, such as a simulation or a robot in a lab/controlled environment, to a new domain with previously unseen bounded model errors/perturbations. 
Numerical and experimental evaluations performed using linear and nonlinear MPC for agile flight on a multirotor show that our method outperforms strategies commonly employed in IL (such as Dataset-Aggregation (DAgger) and Domain Randomization (DR)) in terms of demonstration-efficiency, training time, and robustness to perturbations unseen during training. %
\end{abstract}

\begin{IEEEkeywords}
Imitation Learning; Data Augmentation; Robust Tube Model Predictive Control; Aerial Robotics.
\end{IEEEkeywords}
\vspace{-3ex}

\section*{Supplementary Material}
Video: \url{https://youtu.be/aWRuvy3LviI}.

\acresetall
\section{Introduction} \label{sec:introduction}
\input{sections/1_introduction}

\section{Related Works} \label{sec:related_works}
\input{sections/2_related_works.tex}

\section{Problem Statement} \label{sec:imitation}
\input{sections/3_problem_statement}

\section{Efficient Learning from Linear RTMPC} \label{sec:rtmpc_linear}
\input{sections/4_linear_mpc.tex}

\section{Efficient Learning from Nonlinear RTMPC} \label{sec:rtmpc_nonlinear}
\input{sections/5_nonlinear_mpc.tex}

\section{Application to Agile Flight} \label{sec:agile_flight}
\input{sections/6_applications_to_agile_flight.tex}

\section{Evaluation - Learning From Linear RTMPC} \label{sec:evaluation_linear}
\input{sections/7_evaluation_linear.tex}

\section{Evaluation - Learning From Nonlinear RTMPC} \label{sec:evaluation_nonlinear}
\input{sections/8_evaluation_nonlinear.tex}

\section{Discussion and Conclusion} \label{sec:conclusion}
\input{sections/9_conclusion.tex}

\section{Acknowledgments}
We thank Kota Kondo for the help with the experimental evaluation, Prof.\ Michael Everett,  Dr.\ Dong-Ki Kim, Tong Zhao and Dr.\ Donggun Lee for feedback and discussions.

\bibliographystyle{IEEEtran} %
\bibliography{ref}
\end{document}
