
This work has presented an \ac{IL} strategy to efficiently train a robust \ac{DNN} policy from \ac{MPC}. Key ideas of our work were to 
\begin{inparaenum}[(a)]
\item leverage a Robust Tube variant of \ac{MPC}, called \ac{RTMPC}, to collect demonstrations using existing \ac{IL} methods (\ac{DAgger}, \ac{BC}), and
\item to augment the collected demonstrations with extra state-and-actions sampled from the robust control invariant set (tube) of the controller, leveraging the key intuition that the tube represents an approximation of the support of the state distribution that the learned policy will encounter at deployment, when subject to uncertainties.
\end{inparaenum}
We first demonstrate these properties by efficiently training a policy that enables robust trajectory tracking on a multirotor operating at near-hover conditions. This is accomplished by leveraging a linear model of the robot and a linear \ac{RTMPC} framework. Our numerical and experimental evaluations  highlight the possibility of training a policy robust to real-world uncertainties (such as wind and model errors) after a single demonstration, collected either in simulation or directly with the real robot.

While obtaining extra data in a computationally efficient way is straightforward in a linear \ac{RTMPC} setting, since actions can be computed efficiently using the linear ancillary controller, as shown in our conference paper \cite{tagliabue2022efficient}, the same efficiency can be challenging to achieve when leveraging nonlinear variants of \ac{RTMPC}. Therefore, in this journal extension of \cite{tagliabue2022efficient} we have presented a strategy to efficiently perform tube-guided \ac{DA} when leveraging nonlinear variants of \ac{RTMPC}. The key idea, in this case, was to perform \ac{DA} by generating a linear approximation of the ancillary \ac{NMPC} employed to maintain the system under uncertainties inside the tube. We have additionally presented a fine-tuning phase that requires very few demonstrations and can significantly reduce the errors introduced by the approximation of the ancillary \ac{NMPC}. Experimental evaluations on the challenging task of performing a flip on a multirotor have demonstrated the performance of policies trained with only two demonstrations, requiring less than one-third of the training time needed by existing methods. 

While design procedures to accurately estimate tubes given uncertainty and model priors may be complex and computationally expensive, our work has also demonstrated that fixed-size approximations of the tube are sufficient to efficiently generate policies that can withstand real-world uncertainties. Additionally, depending on the scenario, designing a tube and performing tube-guided \ac{DA} may be a simpler method to learn robust policies compared to strategies based on \ac{DR}, where a user needs to carefully randomize relevant environmental parameters. This is the case, for example, when learning tubes from data, i.e., when prior knowledge on the state deviations caused by uncertainties is known, but the exact sources of those uncertainties are unknown, and therefore it is hard to identify what are the parameters that should be randomized in the popular \ac{DR} procedures. 

Overall, our work has demonstrated that it is possible to efficiently generate high-quality, robust policies from \ac{MPC}, i.e. in a way that requires few 
\begin{inparaenum}[(a)]
    \item queries to expensive optimal controllers, and
    \item environment interactions.
\end{inparaenum}
This can have important repercussions across different fields of robotics. For example, the improvements in (a) enable speed-up in the training time of policies, a convenient property when performing controller tuning or when working on robots where the model is so poorly known that the model/controller/policy needs to be modified whenever new data (e.g., obtained from the previous deployment of the learned policy) becomes available. Additionally, the improvements in (b) can open up further opportunities to imitation-learn robust policies in environments that are extremely expensive to query, such as the real world (where expensive is intended in terms of effort to run data collection), or in scenarios where the simulated environments require a large amount of computation (i.e., in fluid-dynamic simulations). %
Last, shifting gear from the \ac{IL} setting, we would like to highlight that our proposed efficient policy learning procedure can serve as a boot-strapping methodology, leveraging \textit{model and uncertainty priors}, to \textit{efficiently} obtain \textit{high-performance}, \textit{robust} initial policy guesses for subsequent policy fine-tuning steps using model-free methods, such as \ac{RL}, with the potential to significantly reduce the amount of random exploration typically required by these methods.     




