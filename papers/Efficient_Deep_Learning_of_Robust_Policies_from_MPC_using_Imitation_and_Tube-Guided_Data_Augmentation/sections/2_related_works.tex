\subsection{Explicit MPC.}
 A well-established method to generate a computationally efficient representation from linear MPC consists in \textit{explict} MPC~\cite{borrelli2017predictive}. This approach pre-computes offline a policy by partitioning the feasible state space in polytopic regions and by solving an optimal control problem for each region; the online optimization problem is reduced to finding the polytopic region corresponding to the current state via a look-up table.
However, the memory requirement and computational complexity of explicit \ac{MPC} grow exponentially in the number of constraints~\cite {borrelli2017predictive, zhang2020near}. Recent work has replaced look-up tables with \acp{DNN}~\cite{hertneck2018learning, chen2018approximating, zhang2020near, chen2022large}, yielding to explicit \ac{MPC} approximations that can be queried more efficiently. Although promising, these methods have been mainly studied in the context of linear systems, and do not leverage task-specific demonstrations to identify the more relevant parts of the feasible state space, therefore missing opportunities to optimize their performance or training efforts for those more relevant regions. 
Our work leverages task-relevant demonstrations and \ac{IL} to focus the efforts of the policy learning procedure on the most relevant parts of the policy input space, while learning from \ac{MPC} that use nonlinear models.    

\subsection{Policy Learning from MPC for Mobile Robot Control.}
Learning policies from \ac{MPC} is a strategy widely employed in the robotics literature to reduce the onboard computational cost of this type of controller.
Close to our work, \cite{kaufmann2020deep} learns to perform acrobatic maneuvers with a quadrotor from \ac{MPC} using \ac{DAgger} combined with \ac{DR}, by collecting $150$ demonstrations in simulation. Ref.~\cite{pan2020imitation} uses \ac{DAgger} combined with an \ac{MPC} based on \ac{DDP} \cite{jacobson1970differential} for agile off-road autonomous driving using about $24$ laps\footnote{Obtained using Table 2 in \cite{pan2020imitation}, considering $6000$ observation/action pairs sampled at $50$ Hz while racing on a $30$ m long racetrack with an average speed of $6$ m/s.} around their racetrack for the first \ac{DAgger} iteration.
All these examples demonstrate the \textit{performance} that can be achieved when imitation-learning policies from \ac{MPC}, but they also highlight that current methods require a large number of interactions with the \ac{MPC} expert and the training environment, resulting in longer training times or complex data collection procedures.

Methods based on \ac{GPS} \cite{levine2013guided} use trajectories and samples from model-based trajectory planners, including \ac{MPC}, to train \ac{DNN} policies. For examples, \ac{GPS} has been employed for navigation of a multirotor~\cite{zhang2016learning, kahn2017plato}, and to control a legged robot~\cite{carius2020mpc, reske2021imitation}. These methods are in general more sample-efficient than \ac{IL} strategies, thanks to the addition of guiding samples during policy training \cite{levine2013guided}. However, these methods do not \textit{explicitly} account for model and environment uncertainties, resulting in policies with limited robustness. Ref.~\cite{zhang2016learning} for example, demonstrates in simulation robustness to up to $3.3\%$ in weight perturbations of a multirotor, while our approach demonstrates robustness to perturbations up to $30\%$.


\noindent
\subsection{Robustness in Imitation Learning}
Robustness is a fundamental requirement for the real-world deployment of policies obtained via \ac{IL}, as it is needed to compensate for the distribution shifts caused by the \textit{sim2real} or \textit{lab2real} (i.e., when collecting demonstrations on the real robot in a controlled environment and then deploying in the real world) transfers. Robustness to these types of \textit{shifts} can be achieved by modifying the training domain so that its dynamics match the ones encountered in the deployment domain. This is done, for example, via \ac{DR}~\cite{peng2018sim}, which applies random model errors/disturbances, sampled from a predefined set of possible perturbations, during data collection in simulation. Ref.~\cite{farchy2013humanoid, chebotar2019closing}, instead, propose to match the training (source) and deployment (target) domain by estimating a model of the target domain from a few interactions with it. Although effective, these approaches require many demonstrations/interactions with the environment in order to take into account all the possible instantiations of model errors/disturbances that might be encountered in the target domain, limiting the opportunities for \textit{lab2real} transfers, or increasing the data collection effort when training in simulation. 
An alternative avenue relies on modifying the actions of the expert to ensure that the state distribution visited at training time matches the one encountered at deployment time. DART~\cite{laskey2017dart} does so by applying to the actions of the expert some additive noise, sampled from a distribution designed to minimize the covariate shift between the source and target domain. Ref.~\cite{hanna2017grounded, desai2020imitation} propose instead to modify the expert actions so that the resulting transition function matches the transition that will be encountered in the target domain. These works have been designed in the context of learning from human demonstrations, therefore missing opportunities to exploit extra information available to the \ac{MPC} to reduce the number of \ac{MPC}/environment interactions.


\noindent 
\subsection{Data Augmentation for Efficient/Robust Imitation Learning}
A key idea in \ac{GPS} \cite{levine2013guided} consists in generating extra state-action samples (guiding samples) from trajectory planners for improved sample efficiency in policy learning. Specifically, the authors leveraged an \ac{ILQR} \cite{li2004iterative} expert to generate guiding samples around the optimal trajectory found by the controller. Similarly, the authors in ~\cite{carius2020mpc} observe that adding extra states and actions sampled from the neighborhood of the optimal solution found by the \ac{ILQR} expert can reduce the number of demonstrations required to learn a policy when using DAgger. These works highlight the benefits of \ac{DA} when learning policies from model-based experts, but the expert demonstrations and the distribution of the samples they generate do not \textit{explicitly} account for the effects of uncertainty, therefore producing policies that are not robust. %
Our work leverages a robust variant of \ac{MPC} called \ac{RTMPC} \cite{mayne2005robust, mayne2011tube}, to provide robust demonstrations and a \ac{DA} strategy that accounts for the effects of uncertainties. Specifically, the \ac{DA} strategy is obtained by using an outer-approximation of the robust control invariant set (\textit{tube}) as a support of the sampling distribution, ensuring that the guiding samples produce robust policies. 
This idea is related to the recent LAG-ROS framework~\cite{tsukamoto2021learning}, which provides a learning-based method to compress a global planner in a \ac{DNN} by extracting relevant information from the robust tube. LAG-ROS emphasizes the importance of nonlinear contraction-based controllers (e.g., CV-STEM~\cite{tsukamoto2020neural}) to obtain robustness and stability guarantees. 
Our contribution emphasizes instead minimal requirements - namely a tube and an \textit{efficient} \ac{DA} strategy - to achieve demonstration-efficiency and robustness to real-world conditions. By decoupling these aspects from the need for complex control strategies, our work greatly simplifies the controller design. Additionally, different from LAG-ROS, the \ac{DA} procedures presented in our work do not require solving a large optimization problem for every extra state-action sample generated (achieving computational efficiency during training) 
and can additionally leverage interactive experts (e.g., DAgger) to trade off the number of interactions with the environment with the number of extra samples from \ac{DA} (further improving training efficiency).  



Recent work \cite{krishnamoorthy2022sensitivity, krishnamoorthy2023improved} exploited local approximations of the solutions found when solving the \ac{NLP} associated with \ac{MPC} to efficiently generate extra state-actions samples for \ac{DA} in policy learning. Similar to our work, \cite{krishnamoorthy2022sensitivity} uses a parametric sensitivity-based approximation of the solution to efficiently generate extra states and actions. Different from our work, however, their method 
proposes sampling of the entire feasible state space to learn a policy, while our work focuses instead on task-relevant demonstrations, a more computationally and data-efficient solution. The recently presented extension \cite{krishnamoorthy2023improved} solves this issue by leveraging interactive experts (e.g., DAgger). However, both \cite{krishnamoorthy2022sensitivity, krishnamoorthy2023improved} do not \textit{explicitly} account for the effects of uncertainties, neither in the design of the expert, nor in the way that extra states are generated, resulting in policies with limited robustness. Thanks to our robust expert, our approach not only accounts for uncertainties during demonstration collection and in the distribution of samples for \ac{DA}, but it can additionally account for the errors introduced in the \ac{DA} procedure by further constraint tightening and updating the tube size. Additionally, thanks to the strong prior on the state distribution under uncertainty produced by the tube in \ac{RTMPC}, our \ac{DA} strategy can quickly cover the task-relevant parts of the state space, obtaining demonstration-efficiency.
Last, unlike prior work, we experimentally validate our approach, demonstrating it on a system whose models has a large state size (state size $8$ and $10$), whereas previous work focuses on lower-dimensional systems (state size $2$) and only in simulation. 