This part describes the problem of learning a robust policy in a demonstration and computationally efficient way by imitating an \ac{MPC} expert demonstrator. Robustness and efficiency are determined by the ability to design an \ac{IL} procedure that can compensate for the covariate shifts induced by uncertainties encountered during real-world deployment while collecting demonstrations in a domain (the training domain) that presents only a subset of those uncertainty realizations. %
Our problem statement follows the one of robust \ac{IL} (e.g., DART \cite{laskey2017dart}), modified to use deterministic policies/experts and to account for the differences in uncertainties encountered in deployment and training domains. 
Additionally, we present a common approach employed to address the covariate shift issues caused by uncertainties, \ac{DR}, highlighting its limitations. 

\subsection{Assumptions and Notation}
\label{sec:transfer}
\noindent
\textbf{System Dynamics.} 
We assume the dynamics of the real system are Markovian and stochastic~\cite{sutton2018reinforcement}, and can be described by a twice continuously differentiable function $f(\cdot)$:
\begin{equation}
\label{eq:system_dynamics}
\vbf{x}_{t+1} = f(\vbf{x}_t, \vbf{u}_t) + \vbf{w}_t,
\end{equation}
where $\vbf{x}_t \in \mathbb{X} \subseteq \mathbb{R}^{n_x}$ represents the state, $\vbf{u}_t \in \mathbb{U} \subseteq \mathbb{R}^{n_u}$ the control input in the compact subsets $\mathbb{X}$, $\mathbb{U}$. $\vbf{w}_t \in \mathbb{W}_\mathcal{D} \subset \mathbb{R}^{n_x}$ is an unknown state perturbation, belonging to a compact convex set $\mathbb{W}_\mathcal{D}$ containing the origin. Stochasticity in \cref{eq:system_dynamics} is introduced by $\vbf{w}_t$, sampled from $\mathbb{W}_\mathcal{D}$ under a (possibly unknown) probability distribution, capturing the effects of noise, approximation errors in the learned policy, model changes, and other disturbances acting on the system during training or under real-world conditions at deployment. 

\noindent
\textbf{Sim2Real and Lab2Real Transfer Setup.}
Two different domains $\mathcal{D}$ are considered: a training domain $\mathcal{S}$ (\textit{source}) and a deployment domain $\mathcal{T}$ (\textit{target}). The two domains differ in their transition probabilities, and we assume that $\mathbb{W}_\mathcal{S} \subset \mathbb{W}_\mathcal{T}$, representing the fact that training is usually performed in simulation or in a controlled/lab environment under some nominal model errors/disturbances ($\vbf{w} \in \mathbb{W}_\mathcal{S}$), while at deployment a larger set of perturbations ($\vbf{w} \in \mathbb{W}_\mathcal{T}$) can be encountered. 

\noindent
\textbf{MPC Expert.} We assume that we are given an \ac{MPC} expert demonstrator that plans along an $N+1$-steps horizon. 
The expert is given the current state $\vbf{x}_t$, and $\vbf{X}_t^\text{des} \in \mathbb{X}_M^\text{des} \coloneqq \{\mathbb{X}\}_{i=0}^{M}$, representing a desired state to be reached ($M=0$), or a state trajectory to be followed ($M = N+1$). Then, the \ac{MPC} expert generates control actions by solving an \ac{OC} problem of the form: 
\begin{equation}
\begin{split}
\bar{\mathbf{X}}_t^*, \bar{\mathbf{U}}_t^*
    \in \underset{\bar{\mathbf{X}}_t, \bar{\mathbf{U}}_t}{\text{argmin}} & 
       \: J_{N,M}(\bar{\mathbf{X}}_t, \bar{\mathbf{U}}_t, \mathbf{X}^\text{des}_t) \\
    \text{subject to} \:\: & \bar{\vbf{x}}_{0|t} = \vbf{x}_t, \\
    & \bar{\mathbf{x}}_{i+1|t} = f(\bar{\mathbf{x}}_{i|t}, \bar{\mathbf{u}}_{i|t}),  \\
    & \bar{\mathbf{x}}_{i|t} \in \mathbb{X}, \:\: \bar{\mathbf{u}}_{i|t} \in \mathbb{U}, \\
    & i = 0, ..., N-1.
\end{split}
\end{equation}
where $J_{N,M}$ represents the cost to be minimized (where $N$ denotes the dependency on the planning horizon, and $M$ on the type of task, i.e., tracking or reaching a goal state), and $\bar{\mathbf{X}}_t = \{\bar{\vbf{x}}_{0|t},\dots,\bar{\vbf{x}}_{N|t}\}$ and $\bar{\vbf{U}}_t = \{\bar{\vbf{u}}_{0|t},\dots,\bar{\vbf{u}}_{N-1|t}\}$ are sequences of states and actions along the planning horizon, where the notation $\bar{\vbf{x}}_{i|t}$ indicates the planned state at the future time $t+i$, as planned at the current time $t$. 
At every timestep $t$, given $\vbf{x}_t$, the control input applied to the real system is the first element of $\bar{\vbf{U}}^*_t$, resulting in an implicit deterministic control law (policy) that we denote as $\pi_{\vbs{\theta}^*}: \mathbb{X} \times \mathbb{X}_M^\text{des} \rightarrow \mathbb{U}$.

\noindent 
\textbf{\ac{DNN} Student Policy.}
As for the \ac{MPC} expert, we model the \ac{DNN} student policy as a deterministic policy $\pi_{\vbs{\theta}}$, with parameters $\vbs{\theta}$.
When considering trajectory tracking tasks, the policy takes as input the current state and the desired reference trajectory segment, $\pi_{\vbs{\theta}} : \mathbb{X} \times \mathbb{X}_{N+1}^\text{des} \rightarrow \mathbb{U}$. When considering the task of reaching a goal state, the policy takes as input the current state, the desired goal state and the current timestep $t \in \mathbb{I}_{\geq 0}$, $\pi_{\vbs{\theta}}: \mathbb{X} \times \mathbb{X}_0^\text{des} \times \mathbb{I}_{\geq 0} \rightarrow \mathbb{U}$.




\noindent
\textbf{Transition Probabilities}. 
We denote the state transition probability under $\pi_{\vbs{\theta}}$ in a domain $\mathcal{D}$ for a given goal-reaching or trajectory tracking task
as $p_{\pi_{\vbs{\theta}}, \mathcal{D}}(\vbf{x}_{t+1}|\vbf{x}_t)$. 
We highlight that the probability of collecting a $T$-step trajectory $\boldsymbol{\xi} = \{ \vbf{x}_0, \vbf{u}_0, \vbf{x}_1, \vbf{u}_1, \dots, \vbf{x}_{T} \}$, given a policy $\pi_{\vbs{\theta}}$, depends on the deployment environment $\mathcal{D}$:  
\begin{equation}
    p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{D}) = p(\vbf{x}_0) \prod_{t=0}^{T-1} p_{\pi_{\vbs{\theta}}, \mathcal{D}}(\vbf{x}_{t+1}|\vbf{x}_t),
\end{equation}
where $p(\vbf{x}_0)$ represents the initial state distribution. 

\noindent
\subsection{Robust Imitation Learning Objective}
The objective of robust \ac{IL}, following~\cite{laskey2017dart}, is to find parameters $\vbs{\theta}$ of $\pi_{\vbs{\theta}}$ that minimize a distance metric $\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi})$ from the \ac{MPC} expert $\pi_{\vbs{\theta}^*}$. This metric captures the differences between the actions generated by the expert $\pi_{\vbs{\theta}^*}$ and the action produced by the student $\pi_{\vbs{\theta}}$ across the distribution of trajectories induced by the student policy $\pi_{\vbs{\theta}}$, in the perturbed domain $\mathcal{T}$:
\begin{equation}
    \hat{\vbs{\theta}}^* = \text{arg}\min_{\vbs{\theta}} \mathbb{E}_{p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{T})}\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi}).
    \label{eq:il_obj_target}
\end{equation}
The distance metric considered in this work is the \ac{MSE} loss:
\begin{equation}
\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi}) = \frac{1}{T}\sum_{t=0}^{T-1}\| \pi_{\vbs{\theta}}(\vbf{x}^\text{in}_t) - \pi_{\vbs{\theta}^*}(\vbf{x}_t, \vbf{X}_t^\text{des})\|_2^2.
\end{equation}
where $\vbf{x}^\text{in}_t = \{\vbf{x}_t, \vbf{X}_t^\text{des}\}$ for trajectory tracking tasks, and $\vbf{x}^\text{in}_t = \{\vbf{x}_t, \vbf{X}_t^\text{des}, t\}$ for go-to-goal-state tasks.

\noindent
\textbf{Covariate Shift due to Sim2real and Lab2real Transfer.}
Because in practice we do not have access to the target environment, the goal of Robust IL is to try to solve~\cref{eq:il_obj_target} by finding an approximation of the optimal policy parameters $\hat{\vbs{\theta}}^*$ using data from the source environment: 
\begin{equation}
    \hat{\vbs{\theta}}^* = \text{arg}\min_{\vbs{\theta}} \mathbb{E}_{p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{S})}\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi}).
    \label{eq:il_obj_source}
\end{equation}
The way this minimization is solved depends on the chosen \ac{IL} algorithm. The performance of the learned policy in the target and source domains can be related via: 
\begin{equation}
\begin{multlined}
\hspace*{-0.45in}    \mathbb{E}_{p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{T})}\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi}) = \\
    \underbrace{
    \mathbb{E}_{p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{T})}\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi}) - 
    \mathbb{E}_{p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{S})}\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi})}_{\text{covariate shift due to transfer}} \\ + 
    \underbrace{\mathbb{E}_{p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{S})}\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi})}_{\text{\ac{IL} objective}},
\end{multlined}
\label{eq:}
\end{equation}
which clearly shows the presence of a covariate shift induced by the transfer. The last term corresponds to the objective minimized by performing \ac{IL} in $\mathcal{S}$. Attempting to solve \cref{eq:il_obj_target} by directly optimizing \cref{eq:il_obj_source} (e.g., via \ac{BC}~\cite{pomerleau1989alvinn}) offers no assurances of finding a policy with good performance in $\mathcal{T}$. 


\subsection{Compensating Transfer Covariate Shifts via Domain Randomization.}
\label{subsec:domain_randomization}
A well-known strategy to compensate for the effects of covariate shifts between source and target domain is \ac{DR}~\cite{peng2018sim}, which modifies the transition probabilities of the source $\mathcal{S}$ by trying to ensure that the trajectory distribution in the modified training domain $\mathcal{S}_\text{DR}$ matches the one encountered in the target domain: $p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{S}_\text{DR}) \approx p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{T})$.
This is done by applying perturbations to the robot during demonstration collection, sampling perturbations $\vbf{w} \in \mathbb{W}_\text{DR}$ according to some knowledge/hypotheses on their distribution $p_\mathcal{T}(\vbf{w})$ in the target domain~\cite{peng2018sim}, obtaining the perturbed trajectory distribution $p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{S}, \vbf{w})$. The minimization of \cref{eq:il_obj_target} can then be approximately performed by minimizing instead:
\begin{equation}
\label{eq:il_dr_modified_source}
    \mathbb{E}_{p_\mathcal{T}(\vbf{w})}[\mathbb{E}_{p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{S}, \vbf{w})}\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi})].
\end{equation}
This approach, however, requires the ability to apply disturbances/model changes to the system, which may be unpractical e.g., in the \textit{lab2real} setting, and may require a large number of demonstrations due to the need to sample enough state perturbations $\vbf{w}$.

