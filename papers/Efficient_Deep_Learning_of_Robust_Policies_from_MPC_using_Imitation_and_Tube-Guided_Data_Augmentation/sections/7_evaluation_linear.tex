We start by evaluating our policy learning approach for the task of trajectory tracking using the linear \ac{RTMPC} expert. 

\subsection{Evaluation Approach and Details}
\noindent
\textbf{Simulation Environment.} Demonstration collection and policy evaluations are performed in a simulation environment implementing the nonlinear multirotor dynamics in \cref{subsec:mav_model}, discretized at $200$Hz, while the attitude controller runs at $100$Hz. The robot follows desired trajectories, starting from randomly generated initial states centered around the origin. Given the specified external disturbance magnitude bound $\mathbb{W}_\mathcal{D} = \{ f_\text{ext} \in \mathbb{R}| \underline{f}_\text{ext} \leq f_\text{ext} \leq \overline{f}_\text{ext}\}$, disturbances are applied in the domain $\mathcal{D}$ by sampling $\vbs[W]{f}_\text{ext}$ via the spherical coordinates: 
\vspace{-0.3cm}
\begin{equation} \label{eq:disturbance_sampling}
\vbs[W]{f}_\text{ext} = 
f_\text{ext} 
\begin{bmatrix}
\cos(\phi) \sin(\theta) \\
\sin(\phi) \sin(\theta) \\
\cos(\theta)
\end{bmatrix}, ~~~
\begin{aligned}
&f_\text{ext} \sim \mathcal{U}(\underline{f}_\text{ext}, \overline{f}_\text{ext}), \\
&\theta \sim \mathcal{U}(0, \pi), \\
&\phi \sim \mathcal{U}(0, 2\pi).
\end{aligned}
\end{equation}
\vspace{-0.3cm}

\noindent
\textbf{Linear \ac{RTMPC}.}
The linear \ac{RTMPC} demonstrator runs at $10$Hz. The reference fed to the expert is a sequence of desired positions and velocities for the next $3$s, discretized with a sampling time of $0.1$s; the expert uses a corresponding planning horizon of $N=30$, (resulting in a reference being a $180$-dim. vector). The controller is designed under the assumption that $\mathbb{W} = \{f_\text{ext} \in \mathbb{R} | 0 \leq f_\text{ext} \leq 0.3 mg\}$, corresponding to the safe physical limit of the actuators of the robot.

\noindent
\textbf{Policy Architecture.} The student policy is a $2$-hidden layer, fully connected \ac{DNN}, with $32$ neurons per layer, and \texttt{ReLU} activation function. The total input dimension of the \ac{DNN} is $188$ (matching the input of the expert, consisting of state and reference trajectory). The output dimension is $3$ (desired thrust and tilt expressed in an inertial frame). We rotate the tilt output of the \ac{DNN} in the body frame to avoid taking into account yaw, which is not part of the optimization problem \cite{kamel2017linear}, not causing any relevant computational cost. We additionally apply the nonlinear attitude compensation scheme as in \cite{kamel2017linear}.

\noindent
\textbf{Baselines and Training Details.} We apply the proposed \ac{SA} strategies to every demonstration collected via \ac{DAgger} or \ac{BC}, and compare their performance against the two without \ac{SA} and the two combined with \ac{DR}. 
During demonstration-collection in the source environment $\mathcal{S}$, we do not apply disturbances, setting $\mathbb{W}_\mathcal{S} = \{ \emptyset \}$, with the exception for \ac{DR}, where we sample disturbances from $\mathbb{W}_\text{DR} = \mathbb{W}_\mathcal{T}$. 
In all the methods that use \text{DAgger} we set the probability of using actions of the expert $\beta$ (a hyperparameter of DAgger \cite{ross2011reduction}) to be $1$ at the first demonstration and $0$ otherwise (as this was found to be the best-performing setup).
Demonstrations are collected with a sampling time of $0.1$s. After every collected demonstration, the policy is trained for $50$ epochs\footnote{One epoch is one pass through the entire dataset} using all the data available so far with the ADAM \cite{kingma2014adam} optimizer and a learning rate of $0.001$. The policy is then evaluated on the task for $20$ times (episodes), starting from slightly different initial states centered around the origin, in both $\mathcal{S}$ and $\mathcal{T}$. 

\noindent 
\textbf{Evaluation Metrics:}
We monitor:
\begin{enumerate}[i)]
    \item \textit{Robustness (Success Rate)}, as the percentage of episodes where the robot never violates any state constraint;
    \item \textit{Performance}, via either
        \begin{enumerate}[a)]
            \item $C_{\vbs{\xi}}({\pi_\theta}) := \sum_{t=0}^{T}\|\vbf{x}_t - \vbf{x}_t^\text{des}\|^2_{\vbf{Q}_x}+ \|\vbf{u}_t\|_{\vbf{R}_u}^2$ tracking error along the trajectory (\textit{MPC Stage Cost}); or
            \item $\|C_{\vbs{\xi}}({\pi_{{\theta}^*}}) - C_{\vbs{\xi}}(\pi_{\hat{\theta}^*})\|/\|C_{\vbs{\xi}}(\pi_{{\theta}^*})\|$ relative error between expert and policy tracking errors (\textit{Expert Gap}); 
        \end{enumerate} 
    \item \textit{Efficiency}
    \begin{enumerate}
        \item number of expert demonstrations (\textit{Num. Demonstrations Used for Training}), and
        \item wall-clock time to generate the policy (\textit{Training Time}~\footnote{Training time is the time to collect demonstrations and the time to train the policy, as measured by a wall-clock. In our evaluations, the simulated environment steps at its highest possible rate (in contrary to running at the same rate of the simulated physical system), providing an advantage to those methods that require a large number of environment interactions, such as the considered baselines.}).
    \end{enumerate}
\end{enumerate}

\subsection{Numerical Evaluation of Demonstration-Efficiency, Robustness, and Performance when Learning to Track a Single Trajectory}

\begin{figure}
\captionsetup[sub]{font=footnotesize}
\centering
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth, trim={2cm 0 2cm 0.5cm},clip]{figs/icra/disturbance_analysis_v1/robustness.pdf}
\end{subfigure}%
    \caption{Robustness (\textit{Success Rate}) in the task of flying along an eight-shaped, $7$s long-trajectory, subject to wind-like disturbances (right,  target domain $\mathcal{T}_1$) and without (left, source domain $\mathcal{S}$), starting from different initial states. Evaluation is repeated across $8$ random seeds, $20$ times per demonstration per seed. We additionally show the $95\%$ confidence interval. The lines for the SA-based methods overlap.}
    \label{fig:single_trajectory_eval}
    \vskip-1ex
\end{figure}

\input{tables/numerical_comparison.tex}

\noindent
\textbf{Tasks Description.}
Our objective is to generate a policy from linear \ac{RTMPC} capable of tracking a $7$s long ($70$ steps), figure eight-shaped trajectory. We evaluate the considered \ac{IL} approaches in two different target domains, with wind-like disturbances ($\mathcal{T}_1$) or with model errors ($\mathcal{T}_2$). Disturbances in $\mathcal{T}_1$ are external force perturbations $\vbs{f}_\text{ext}$ sampled from $\mathbb{W}_{\mathcal{T}_1} \approx \{f_\text{ext}|0.25mg \leq f_\text{ext} \leq 0.3mg\}$. Model errors in $\mathcal{T}_2$ are applied via mismatches in the drag coefficients used between training and testing, representing uncertainties not explicitly considered during the design of the linear \ac{RTMPC}. 

\noindent
\textbf{Results.} We start by evaluating the robustness in $\mathcal{T}_1$ as a function of the number of demonstrations collected in the source domain.
The results are shown in \cref{fig:single_trajectory_eval}, highlighting that:
\begin{inparaenum}[i)]
\item while all the approaches achieve robustness (full success rate) in the source domain, \ac{SA} achieves full success rate after only a single demonstration, being $5$-$6$ times more sample efficient than the baseline methods;
\item \ac{SA} is able to achieve full robustness in the target domain, while baseline methods do not fully succeed, and converge at a much lower rate. 
\end{inparaenum}
These results emphasize the presence of a distribution shift between the source and target, which is not fully compensated for by baseline methods such as \ac{BC} due to a lack of exploration and robustness.

The performance evaluation and additional results are summarized in \cref{tab:comparison}. We highlight that in the target domain $\mathcal{T}_1$, \ac{SA}-sparse combined with \ac{DAgger} achieves the performance that is closest to the expert.
\ac{SA}-dense suffers from performance drops, potentially due to the limited capacity of the considered \ac{DNN} or challenges in training introduced by this \ac{DA}. 
\cref{tab:comparison} additionally presents the results for the target domain $\mathcal{T}_2$. Although this task is less challenging (i.e., all the approaches achieve full robustness), the proposed method (\ac{SA}-sparse) achieves the highest demonstration-efficiency and lowest expert gap, with similar trends as in $\mathcal{T}_1$. 


\noindent
\textbf{Training Time.}
\cref{fig:single_trajectory_eval} highlights that the best-performing baseline, DAgger+DR, requires about $10$ demonstrations to learn to robustly track a $7$s long trajectory, which corresponds to a total training time of $10.8$s. Among the proposed approaches, DAgger+SA-sparse instead only requires $1$ demonstration, corresponding to a training time of $3.8$s, a $64.8\%$ reduction in wall-clock time required to learn the policy. DAgger+SA-dense, instead, while requiring a single demonstration to achieve full robustness, necessitates $114$s of training time due to the large number of samples generated. 
Because of its effectiveness and greater computational efficiency, we use \ac{SA}-sparse, rather than \ac{SA}-dense, for the rest of the work. 


\input{tables/computation_time_linear_rtmpc.tex}
\noindent
\textbf{Computation.} \cref{tab:computational_cost_linear_rtmpc} shows that the \ac{DNN} policy is about $25$ times faster than the expert. We additionally report that the average latency for the policy on an \texttt{Nvidia Jetson TX2} (CPU, PyTorch) is $1.66$ms.






\begin{figure}[t]
    \centering
    \begin{subfigure}{0.69\columnwidth}
        \centering
        \includegraphics[height=3.0cm, trim={0.9cm 0.05cm 1.0cm 0.2cm}]{figs/icra/hardware_experiments/uav_with_leaf_blower_hovering_vio_and_thrust_v2}
    \end{subfigure}
    \begin{subfigure}{0.29\columnwidth}
        \centering
        \includegraphics[height=3.0cm, trim={0.4cm 0 0cm 0},clip]{figs/icra/hardware_experiments/uav_with_leaf_blower_hovering_v3.png}
    \end{subfigure}%
    \caption{Experimental evaluation performed by hovering with and without wind disturbances produced by a leaf blower. The employed policy is trained in simulation from a \textit{single} demonstration of the desired trajectory. The wind-like disturbances produce a large position error but do not destabilize the system. The thrust decreases due to the robot being pushed up by the disturbances. The state estimate (shown in the plot) is provided by onboard VIO. The wind points approximately in the same direction as the $y$-axis. 
    }
    \label{fig:experimental_robustness_evaluation_hovering}
\vskip-4ex
\end{figure}
\vskip-2ex

\begin{figure*}
\captionsetup[sub]{font=footnotesize}
\centering
\begin{subfigure}{0.22\paperwidth}
    \centering
    \includegraphics[height=3.5cm, trim={0.5cm 5.5cm 0.25cm 5.5cm},clip]{figs/icra/hardware_experiments/lemniscate_dist_vicon_v4-cropped.pdf}
    \caption{Reference and actual trajectory}
    \label{fig:reference_and_actual_trajectory}
\end{subfigure}%
\hspace{0.1cm}
\begin{subfigure}{0.38\paperwidth}
    \centering
    \includegraphics[height=3.5cm, trim={0.5cm 2cm 1cm 2cm}, clip]{figs/icra/hardware_experiments/uav_leaf_blower_4_hz_v2_smaller_size.png}
    \caption{Time-lapse of an experiment showing the trajectory executed by the robot, and the leaf blowers used to generate disturbances}
    \label{fig:experiment}
\end{subfigure}%
\hspace{0.12cm}
\begin{subfigure}{0.20\paperwidth}
    \centering
    \includegraphics[height=3.5cm, trim={0cm, 0.4cm, 0.4cm, 0.8cm}]{figs/icra/hardware_experiments/leminscate_dist_altitude_and_thrust_vicon_v5}
    \caption{Effects of wind}
    \label{fig:thrust_and_altitude}
\end{subfigure}
    \caption{Experimental evaluation of a trajectory tracking policy learned from a \textit{single} linear \ac{RTMPC} demonstration collected in simulation, achieving \textit{zero-shot} transfer. The multirotor is able to withstand previously unseen disturbances, such as the wind produced by an array of leaf-blowers, and whose effects are clearly visible in the altitude errors (and change in commanded thrust) in \cref{fig:thrust_and_altitude}.  
    This demonstration-efficiency and robustness is enabled by Sampling-Augmentation (SA), our proposed tube-guided data augmentation strategy.}
    \label{fig:experiment_lemniscate}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth, trim={0, 8.5cm, 0, 0.1cm}, clip]{figs/lab2real_transfer/lab2real_transfer_v2.pdf}
    \caption{Example of \textit{lab2real} transfer. One expert demonstration ($1$m step in $x$-$y$-$z$)(1) is sufficient to train a policy that can reproduce the demonstration of the expert (2) and that is robust to previously unseen wind disturbances, accumulating tracking error but not drifting away (3).}
    \label{fig:lab2real_transfer}
\end{figure}

\subsection{Hardware Evaluation for Tracking a Single Trajectory from a Single Demonstration}
\noindent \textbf{Sim2Real Transfers.}
We validate the demonstration-efficiency, robustness, and performance of the proposed approach by experimentally testing policies trained after a \textit{single} demonstration collected in simulation using DAgger/BC (which operate identically since we use DAgger with $\beta=1$ for the first demonstration), combined with \ac{SA}-sparse. We use the MIT/ACL open-source snap-stack \cite{acl_snap_stack} for controlling the attitude of the MAV. The learned policy runs at $100$Hz on the onboard \texttt{Nvidia Jetson TX2} (CPU), with the reference trajectory provided at $100$Hz. State estimation is obtained via a motion capture system or onboard \ac{VIO}. 

The first task considered is to hover under wind disturbances produced by a leaf blower. The results (\cref{fig:experimental_robustness_evaluation_hovering}) highlight the ability of the system to remain stable despite the large position error caused by the wind. 

The second task is to track a figure eight-shaped trajectory, with velocities up to $3.4$m/s. We evaluate the robustness of the learned policy by applying a wind-like disturbance produced by an array of $3$ leaf blowers (\cref{fig:experiment_lemniscate}). The given position reference and the corresponding trajectory are shown in \cref{fig:reference_and_actual_trajectory}. The effects of the wind disturbances are clearly visible in the altitude errors and changes in commanded thrust in \cref{fig:reference_and_actual_trajectory} (at $t=11$s and $t=23$s). 
These experiments show that the learned policy can robustly track the desired reference, withstanding challenging perturbations unseen during the training phase. 

\noindent \textbf{Lab2Real Transfer.} We evaluate the ability of the proposed method to learn from a single demonstration collected on a real robot in a controlled environment (lab) and generalize to previously unseen disturbances (real). We do so by collecting a \ac{RTMPC} demonstration of a $1$m position-step on $x$-$y$-$z$ axes of $\text{W}$ with the multirotor, augmenting the collected demonstration with \ac{SA}-sparse, and deploying the learned policy while we apply previously unseen wind disturbances. As shown in the sequence in \cref{fig:lab2real_transfer} and in our video submission, the policy reproduces the expert demonstration and it is robust to previously unseen wind disturbances.

\begin{figure}
\captionsetup[sub]{font=footnotesize}
\centering
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth, trim={2cm 2cm 2cm 0},clip]{figs/icra/learn_multi_traj_v2/robustness.pdf}
\end{subfigure}%
\hspace{0.1cm}
\begin{subfigure}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth, trim={2cm 0 2cm 0},clip]{figs/icra/learn_multi_traj_v2/performance.pdf}
\end{subfigure}
    \caption{Robustness (\textit{Success Rate}, top row) and performance (\textit{MPC Stage Cost}, bottom row) of the proposed approach (with $95\%$ confidence interval), as a function of the number of demonstrations used for training, for the task of learning to track previously unseen circular, eight-shaped and constant position reference trajectories, sampled from the same training distribution. The lines for SA-based methods overlap. The left column presents the results in the training domain (no disturbance), while the right column in the target domain, under wind-like perturbations (with disturbance). The proposed RTMPC-driven SA-sparse strategy learns to track multiple trajectories and generalize to unseen ones requiring fewer demonstrations. At convergence (from demonstration $20$ to $30$), DAgger+SA achieves the closes performance to the expert ($2.7\%$ \textit{expert gap}), followed by BC+SA ($3.0\%$  \textit{expert gap}). Evaluation performed using $20$ randomly sampled trajectories per demonstration, repeated across $6$ random seeds, with a prediction horizon of $N=20$ to speed up demonstration collection, and the DNN input size is adjusted accordingly.}
    \label{fig:learning_multiple_trajectories}
    \centering
    \includegraphics[width=\columnwidth, trim={1.2cm, 0.5cm, 1.2cm, 0.5cm}, clip]{figs/icra/hardware_experiments/learn_multi_traj_10_demonstrations}
    \caption{Examples of different, arbitrary chosen trajectories from the training distribution, tested in hardware experiments with and without strong wind-like disturbances produced by leaf blowers. The employed policy is trained with $10$ demonstrations (when other baseline methods have not fully converged yet, see \cref{fig:learning_multiple_trajectories}) using DAgger+SA (sparse). This highlights that sparse SA can learn multiple trajectories in a more sample-efficient way than other IL methods, retaining RTMPC's robustness and performance. The prediction horizon used is $N=20$, and the \ac{DNN} input size is adjusted accordingly.}
    \label{fig:learn_multiple_trajectories_experiment}
\end{figure}

\subsection{Numerical and Hardware Evaluation for Learning and Generalizing to Multiple Trajectories}
We evaluate the ability of the proposed approach to track multiple trajectories while generalizing to unseen ones. To do so, we define a training distribution of reference trajectories (circle, position step, eight-shape) and a distribution for these trajectory parameters (radius, velocity, position). 
During training, we sample at random a desired, $7$s long ($70$ steps) reference with randomly sampled parameters, collecting a demonstration and updating the proposed policy, while testing on a set of $20$, $7$s long trajectories randomly sampled from the defined distributions. We monitor the robustness and performance of the different methods, with force disturbances (from $\mathbb{W}_{\mathcal{T}_1}$) applied in the target domain. The results of the numerical evaluation, shown in \cref{fig:learning_multiple_trajectories}, confirm that \ac{SA}-sparse
\begin{inparaenum}[i)]
    \item achieves robustness and performance comparable to the expert in a sample efficient way, requiring fewer than half the number of demonstrations needed for the baseline approaches; 
    \item simultaneously learns to generalize to multiple trajectories randomly sampled from the training distribution.
\end{inparaenum}
The hardware evaluation, performed with DAgger augmented via SA-sparse, is shown in \cref{fig:learn_multiple_trajectories_experiment}. It confirms that the obtained policy is experimentally capable of tracking multiple trajectories under real-world disturbances/model errors.