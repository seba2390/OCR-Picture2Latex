In this Section, we design an \ac{IL} and \ac{DA} strategy, which is an extension of the one presented in \cref{sec:rtmpc_linear}, that enables robust and efficient policy learning from an \ac{MPC} that employs nonlinear models of the form in \cref{eq:system_dynamics}. Different from \cref{sec:rtmpc_linear}, the focus here is on obtaining policies capable of reaching a desired goal state, as this will enable acrobatic maneuvers -- the scenario considered in the evaluation of policies learned from this controller (\cref{sec:evaluation_nonlinear}). %
To accomplish this, first, we use a nonlinear version of \ac{RTMPC}, based on \cite{mayne2011tube}, to collect demonstrations that account for the effects of uncertainties. This expert is summarized in \cref{subsec:rtnmpc_expert}. Second, we develop a computationally efficient tube-guided \ac{DA} strategy leveraging the ancillary controller of the nonlinear \ac{RTMPC} expert. Unfortunately, unlike in the linear \ac{RTMPC} case, nonlinear \ac{RTMPC} \cite{mayne2011tube} uses \ac{NMPC} as an ancillary controller. This limits the computational efficiency in \ac{DA}, as the generation of extra state-action samples requires solving a large \ac{NLP} associated with the ancillary \ac{NMPC} (discussed in \cref{subsec:ancillary_nonlinear_mpc}). We overcome this issue by presenting, in \cref{subsec:sensitivity}, a time-varying linear feedback law, approximation of the ancillary \ac{NMPC}, that enables efficient generation of the extra data. This is done by leveraging the sensitivity of the control input to perturbations in the states visited during an initial demonstration collection procedure. 
Finally, in \cref{subsec:rob_perf_under_approx_samples}, we address the approximation errors introduced by the sensitivity-based \ac{DA} by presenting strategies to mitigate the gap, in performance and robustness, between the learned policy and the \ac{MPC} expert.


\subsection{Nonlinear RTMPC Expert Formulation} \label{subsec:rtnmpc_expert}
Nonlinear RTMPC \cite{mayne2011tube} ensures state and actuation constraint satisfaction while controlling a nonlinear, uncertain system of the form in \cref{eq:system_dynamics}. This controller operates by solving two \acp{OCP}, one to compute a nominal safe plan, and one to track the safe plan (ancillary \ac{NMPC}).

\noindent
\textbf{Nominal Safe Planner.} \label{subsec:nmpc_nominal_plan}
The first \ac{OCP}, given an $N+1$-steps planning horizon, generates nominal safe state and action open-loop plans 
$\vbf{Z}_{\tzr} = \{\vbf{z}_{0|\tzr}, \dots, \vbf{z}_{N|\tzr}\}, \vbf{V}_{\tzr} = \{\vbf{v}_{0|\tzr},\dots, \vbf{v}_{N-1|\tzr}\}$. 
The plans are open-loop because they are generated only at time $\tzr$ when the desired state and action pair $\vbf{X}^\text{des}_{\tzr} = \{\vbf{x}_{\tzr}^e, \vbf{u}_{\tzr}^e \}$, equilibrium pair for the nominal system, changes. The nominal safe plan is obtained from: 
\begin{equation} \label{eq:nmpc_nominal}
\begin{split}
\vbf{V}_{\tzr}^*, \vbf{Z}_{\tzr}^*
    = \underset{\vbf{V}_{\tzr}, \mathbf{Z}_{\tzr}}{\text{argmin}} & 
       \: J_\text{RTNMPC}(\vbf{Z}_{\tzr}, \vbf{V}_{\tzr}, \vbf{X}_{\tzr}^\text{des}) \\
    \text{subject to} \:\: & \mathbf{z}_{i+1|\tzr} = f(\mathbf{z}_{i|\tzr}, \mathbf{v}_{i|\tzr}),  \\
    & \mathbf{z}_{i|\tzr} \in \bar{\mathbb{Z}}, \:\: \mathbf{v}_{i|\tzr} \in \bar{\mathbb{V}}, \\
    & \mathbf{z}_{0|\tzr} = \vbf{x}_{\tzr}, \:\: \mathbf{z}_{N|\tzr} = \vbf{x}_{\tzr}^e.
\end{split}
\end{equation}
$J_\text{RTNMPC} = \sum_{i=0}^{N-1} \| \vbf{z}_{i|\tzr} - \vbf{x}^e_{\tzr} \|^2_{\mathbf{Q}_z} + \| \vbf{v}_{i|{\tzr}} - \vbf{u}^e_{\tzr} \|^2_{\mathbf{R}_v}$, where $\mathbf{Q}_z$, $\mathbf{R}_v$ are positive definite. A key idea in this approach involves imposing modified state and actuation constraints $\bar{\mathbb{Z}} \subset \mathbb{X}$ and $\bar{\mathbb{V}} \subset \mathbb{U}$ so that the generated nominal safe plan is at a specific distance from state and actuation constraints. 
To be more precise, similar to the linear \ac{RTMPC} case (\cref{eq:rtmpc_optimization_problem}), the given state constraints $\mathbb{X}$ and actuation constraints $\mathbb{U}$ are tightened (made more conservative) by an amount that accounts for the spread of trajectories induced by the ancillary controller when the system is subject to uncertainties, obtaining $\bar{\mathbb{Z}} \subset \mathbb{X}$ and $\bar{\mathbb{V}} \subset \mathbb{U}$. Different from the linear case, however, analytically computing the tightened constraints is challenging. Fortunately, as highlighted in \cite{mayne2011tube}, accurately computing these sets is not needed, and an outer approximation is sufficient. This approximation can be obtained via Monte-Carlo simulations \cite{mayne2011tube} of the system under disturbances, or learned \cite{fan2020deep}.  %

\noindent
\textbf{Ancillary \ac{NMPC}.}
The second \ac{OCP} corresponds to a trajectory tracking \ac{NMPC}, that acts as an ancillary controller, to maintain the state of the uncertain system close to the reference generated by \cref{eq:nmpc_nominal}. The \ac{OCP} is:
\begin{equation} 
\begin{split} \label{eq:ancillary_nmpc_eq}
    \mathbf{\bar{U}}_t^*, \mathbf{\bar{X}}_t^*
    = \underset{\mathbf{\bar{U}}_t, \mathbf{\bar{X}}_t}{\text{argmin}} & 
        \| \mathbf e_{N|t} \|^2_{\mathbf{P}_x} \!\! + \!\!
        \sum_{i=0}^{N-1} 
            \| \mathbf e_{i|t} \|^2_{\mathbf{Q}_x} \!\!+\! 
            \| \bar{\vbf{u}}_{i|t}\!-\!\vbf{v}_{i + t|\tzr}^* \|^2_{\mathbf{R}_u} \\
    \text{subject to} \:\: &  \xsafe_{i+1|t} = f(\xsafe_{i|t}, \usafe_{i|t})  \\
    & \xsafe_{0|t} = \vbf{x}_t, \usafe_{i|t} \in \bar{\mathbb{U}} \\
\end{split}
\end{equation}
where $\mathbf e_{i|t} = \xsafe_{i|t} - \vbf{z}^*_{i + t - t_0|\tzr}$ is the state tracking error. The positive definite matrices $\mathbf{Q}_x$ and $\mathbf{R}_u$ are tuning parameters and can differ from the ones in \cref{eq:nmpc_nominal}, while $\mathbf{P}_x$ defines a terminal cost. \cref{eq:ancillary_nmpc_eq} is solved at each timestep using the current state $\vbf{x}_t$, while the action applied to the robot is $\vbf{u}_t = \bar{\vbf{u}}^*_{0|t}$. %
We note that the ancillary \ac{NMPC} can have different tuning parameters than \cref{eq:nmpc_nominal}, including the discretization time of the dynamics and the prediction horizon $N$, providing additional degrees of freedom to shape the response of the system under uncertainties.    %

A key result of the employed nonlinear \ac{RTMPC} \cite{mayne2011tube} is that the ancillary \ac{NMPC} in \cref{eq:ancillary_nmpc_eq} maintains the trajectories of the uncertain system in \cref{eq:system_dynamics} inside state and action tubes $\mathbb{T}^\text{state} \subset \mathbb{R}^{n_x}, \mathbb{T}^\text{action} \subset \mathbb{R}^{n_u}$ that contain the current nominal safe state and action trajectories $\mathbf{z}_{t|\tzr}^*$, $\mathbf{v}_{t|\tzr}^*$ from the \ac{OCP} in \cref{eq:nmpc_nominal}. The state and action tubes are used to obtain the tightened state and actuation constraints $\bar{\mathbb{Z}}$, $\bar{\mathbb{V}}$, ensuring constraint satisfaction.  %

\subsection{Solving the Ancillary \ac{NMPC}}
\label{subsec:ancillary_nonlinear_mpc}
A large portion of the computational cost of deploying or collecting demonstrations from nonlinear \ac{RTMPC} comes from the need to solve the \ac{OCP} of the ancillary NMPC (\cref{eq:ancillary_nmpc_eq}) at each timestep. In contrast, the \ac{OCP} of the nominal safe plan (\cref{eq:nmpc_nominal}) can be solved once per task (e.g., whenever the desired goal state $\vbf{X}^\text{des}_{\tzr}$ changes).%


A state-of-the-art method to solve the \ac{OCP} in \cref{eq:ancillary_nmpc_eq} that yields high-quality solutions is Multiple Shooting \cite{rawlings2017model}. In this approach, an \ac{OCP}, discretized over a time grid, is transformed in a \ac{NLP} that tries to find an optimal sequence of states and actions. This is done by forward-simulating the nonlinear dynamics on each interval (e.g., via an implicit \ac{RK} integrator) while imposing additional continuity conditions. The resulting \ac{NLP} can be solved via a \ac{SQP}, i.e., by repeatedly:
\begin{inparaenum}[i)]
\item linearizing the \ac{NLP} around a given linearization point; 
\item generating and solving a corresponding \ac{QP}, obtaining a refined linearization point for the next \ac{SQP} iteration.
\end{inparaenum}
While capable of producing high-quality solutions, \ac{SQP} methods incur large computational requirements due to the need of performing computationally-expensive system linearization and solving the associated \ac{QP} one or more times per timestep.



\subsection{Computationally-Efficient Data Augmentation using the Parametric Sensitivities} \label{subsec:sensitivity}
The tube $\mathbb{T}^\text{state}$ induced by the ancillary controller in \cref{eq:ancillary_nmpc_eq} can be used to identify relevant regions of the state space for \ac{DA}, as it approximates the support of the state distribution under uncertainties, as discussed in \cref{sec:sa}. However, generating the corresponding extra action samples using \cref{eq:ancillary_nmpc_eq} can be very computationally inefficient, as it requires solving the associated \ac{SQP} for every extra state sample,
making \ac{DA} computationally impractical, and defeating our initial objective of designing \textit{computationally efficient} \ac{DA} strategies.  

In this work, we extend our efficient \ac{DA} strategy, \acf{SA}, to efficiently learn policies from nonlinear \ac{RTMPC} by proposing instead to employ a time-varying, linear approximation of the ancillary \ac{NMPC} -- enabling efficient generation of extra state-action samples. Specifically, we observe that \cref{eq:ancillary_nmpc_eq} solves the implicit feedback law:
\begin{equation}
\label{eq:ancillary_nmpc_implicit}
\vbf{u}_t \! = \! \bar{\vbf{u}}_{0|t}^*(\vbs{\chi}_t) \! \coloneqq \! \kappa(\vbs{\chi}_t), \;  \vbs{\chi}_t \! \coloneqq \! \{\!\vbf{x}_t, \!t; \!\vbf{V}_{\tzr}^*, \vbf{Z}_{\tzr}^*\}
\end{equation}
where we have denoted the current inputs $\vbs{\chi}_t$ for convenience. Then, for each timestep of the trajectory collected during a demonstration in the source environment $\mathcal{S}$, with current ancillary \ac{NMPC} input $\tilde{\vbs{\chi}}_t = \{ \tilde{\vbf{x}}_t, \tilde{t}; \!\vbf{V}_{\tzr}^*, \vbf{Z}_{\tzr}^* \}$, we generate a local linear approximation of \cref{eq:ancillary_nmpc_implicit} by computing the first-order sensitivity of $\vbf{u}_t$ to the initial state $\vbf{x}_t$: %
\begin{equation} \label{eq:sensitivity_matrix}
\vbf{K}_{\tilde{\vbs{\chi}}_t} \!\!
\coloneqq 
\!
\left.
\frac{\partial \vbf{\bar{u}}_{0|t}^*}{\partial \vbf{x}_t} 
\right|_{{\vbs{\chi}}_t = \tilde{\vbs{\chi}}_t}
\!\!\!\!\! = \!\!
\begin{bmatrix}
\left.\dfrac{\partial{\bar{\vbf{u}}_{0|t}^*}}{\partial{\left[ \vbf{x}_t \right]_{1}}}
\right|_{\tilde{\vbs{\chi}}_t},\!\!& 
\!\!
\dots, 
\!\!
& \!\!
\left.
\dfrac{\partial{\bar{\vbf{u}}_{0|t}^*}}{\partial{[\vbf{x}_t]_{n_x}}}
\right|_{\tilde{\vbs{\chi}}_t}
\\
\end{bmatrix}.
\end{equation}
The sensitivity matrix $\vbf{K}_{\tilde{\vbs{\chi}}_t} \in \mathbb{R}^{n_u \times n_x}$, 
enables us to compute extra actions $\vbf{u}_{t,j}^+$ from states $\vbf{x}_{t,j}^+ \in \mathbb{T}^\text{state}$, with $j = 1, \dots, N_s$, sampled from the tube: 
\begin{equation}
\label{eq:approximate_ancillary_controller}
    \vbf{u}_{t,j}^+ = \vbf{\bar{u}}_{0|t}^* + \vbf{K}_{\tilde{\vbs{\chi}}_t} (\vbf{x}_{t,j}^+ - \vbf{\bar{x}}_{0|t}^*) \coloneqq \hat{\kappa}(\vbf{x}_{t,j}^+, \tilde{\vbs{\chi}}_t).
\end{equation}
The \ac{DA} procedure enabled by this approximation is computationally-efficient, as we do not need to solve an \ac{SQP} for each extra state-action sample $(\vbf{x}^+_{t,j}, \vbf{u}^+_{t,j})$ generated for \ac{DA}, and we only need to compute, once per timestep, the sensitivity matrix $\vbf{K}_{\tilde{\vbs{\chi}}_t}$.
We note that the feedback-response introduced by the so-generated extra state-action samples can be interpreted as a type of linearized neighboring feedback control \cite[\S 1.3.1]{diehl2001real}, where the linear feedback law is computed based on the trajectory $\vbs{\xi}$ executed during demonstration collection in the source environment $\mathcal{S}$, rather than a reference trajectory $\vbf{Z}_\tzr^*, \vbf{V}_\tzr^*$. We remark, additionally, that the actions computed when collecting demonstrations are obtained by solving the entire \ac{SQP}, and the sensitivity-base approximation is used only for \ac{DA}. 

\noindent
\textbf{Sensitivity Matrix Computation.} As described in \cite[\S 8.6]{rawlings2017model}, an expression to compute the sensitivity matrix in \cref{eq:sensitivity_matrix} (also called \textit{tangential predictor}) can be obtained by re-writing the \ac{NLP} in \cref{eq:ancillary_nmpc_eq} in a parametric form $\mathfrak{p}(\left[ \vbf{x}_t \right]_i)$, highlighting the dependency on scalar parameter representing the $i$-th component of the initial state $\vbf{x}_t$ (part of $\vbs{\chi}_t$). The parametric \ac{NLP} $\mathfrak{p}(\left[ \vbf{x}_t \right]_i)$ is: %
\begin{equation}
\begin{split}
\label{eq:nmpc_opt_problem} 
\vspace{-.5in}
\underset{\vbf{y}}{\text{min}} & \: F_{\vbs{\chi}_t}(\vbf{y}) \\
    \text{subject to} \:\: & G_{\vbs{\chi}_t}(\left[ \vbf{x}_t \right]_i, \vbf{y}) = \vbs{0} \\
    & H(\vbf{y}) \leq \vbs{0},
\end{split}
\end{equation}
where $\vbf{y} \in \mathbb{R}^{n_{\vbf{y}}}$ corresponds to the optimization variables in \cref{eq:ancillary_nmpc_eq}, and $F_{{\vbs{\chi}}_t}(\cdot), G_{{\vbs{\chi}}_t}(\cdot), H(\cdot)$ are, respectively, the objective function, equality, and inequality constraints in \cref{eq:ancillary_nmpc_eq}, given the current state and reference trajectory in $\vbs{\chi}_t$. Additionally, we denote the solution of \cref{eq:nmpc_opt_problem} at $\tilde{\vbs{\chi}}_t$ (computed during the collected demonstration) as $(\tilde{\vbf{y}}^*, \tilde{\vbs{\lambda}}^*, \tilde{\vbs{\mu}}^*)$, where $\tilde{\vbs{\lambda}}^*, \tilde{\vbs{\mu}}^*$ are, respectively, the Lagrange multipliers for the equality and inequality constraints at the solution found. Then, each $i$-th column of the sensitivity matrix (\cref{eq:sensitivity_matrix}) can be computed by solving the following \ac{QP} (\cite[Th. 8.16]{rawlings2017model}, and \cite[Th. 3.4 and Remark 4]{diehl2001real}): 
\begin{equation}
\begin{aligned}
\underset{\mathbf{y}}{\text{min}} \quad& \hspace{-.1in} F_{\vbs{\chi}_t,L}(\mathbf{y}; \tilde{\mathbf{y}}^*) 
+ \frac{1}{2}(\mathbf{y} - \tilde{\mathbf{y}}^*)^\top \nabla^2_{\mathbf{y}} \mathscr{L}(\tilde{\mathbf{y}}^*, \tilde{\boldsymbol{\lambda}}^*, \tilde{\boldsymbol{\mu}}^*)(\mathbf{y} - \tilde{\mathbf{y}}^*) \\
\text{s.t.} \quad & G_{\vbs{\chi}_t,L}([\mathbf{x}_t]_i, \mathbf{y}; \tilde{\mathbf{y}}^*) = \mathbf{0} \label{eq:tangential_predictor_qp} \\
& H_L(\mathbf{y}; \tilde{\mathbf{y}}^*) \leq \mathbf{0}
\end{aligned}
\end{equation}
where $F_{\vbs{\chi}_t,L}(\cdot; \tilde{\vbf{y}}^*)$, $G_{\vbs{\chi}_t,L}(\cdot; \tilde{\vbf{y}}^*)$, $H_L(\cdot; \tilde{\vbf{y}}^*)$ denote the respective functions in \cref{eq:nmpc_opt_problem} linearized at the solution found. $\nabla^2_{\vbf{y}}\mathscr{L}$ denotes the Hessian of the Lagrangian associated with \cref{eq:nmpc_opt_problem}, while the parameter is set to zero ($\left[ \vbf{x}_t \right]_i = 0$). The $i$-th column of the sensitivity matrix can be extracted from the entries of $\vbf{y}^*$, solution of \cref{eq:tangential_predictor_qp}, at the position corresponding to $\bar{\vbf{u}}_{0|t}$.
We highlight that \cref{eq:tangential_predictor_qp} can be computed efficiently, as it leverages the latest internal linearization of the \ac{KKT} conditions performed in the \ac{SQP} employed to solve \cref{eq:ancillary_nmpc_eq}, and therefore it does not require to re-execute the computationally expensive system linearization routines that are carried out at each \ac{SQP} iteration.
We note that this local approximation exists when the assumptions in \cite[Th. 8.15]{rawlings2017model} (equivalent to \cite[Th. 3.3]{diehl2001real}) and \cite[Th. 3.3, Remark 2, 4]{diehl2001real} are satisfied, i.e., that the solution $(\tilde{\vbf{y}}^*, \tilde{\vbs{\lambda}}^*, \tilde{\vbs{\mu}}^*)$ found during demonstration collection is a strongly regular \ac{KKT} point, and the assumptions, i.e., that the solution found satisfies strict complementary conditions. 
Last, extra samples are generated using \cref{eq:approximate_ancillary_controller} under the assumption that the set of active inequality constraints (i.e., the index set $p \in \{1, \dots, n_H\}$ such that $[H(\tilde{\vbs{y}}^*)]_p = 0$) does not change.

\noindent
\textbf{Generalized Tangential Predictor} A strategy that applies to the cases where strict complementary conditions do not hold, or where the extra state samples cause a change in the active set of constraints, is based on the \textit{generalized tangential predictor} \cite[\S 8.9.1]{rawlings2017model}. 
This predictor can be obtained by solving the \ac{QP} in \cref{eq:tangential_predictor_qp} with the set of equality constraints modified to be $G_{\vbs{\chi}_t,L}(\vbf{x}_{t,j}^+, \vbf{y}; \tilde{\vbf{y}}) = \vbf{0}$ \cite[Eq. 8.60]{rawlings2017model}. %
Although this approach requires solving a \ac{QP} to compute the action $\vbf{u}_{t,j}^+$ corresponding to each state $\vbf{x}_{t,j}^+$ sampled from the tube, it does not require re-generating the computationally expensive linearization performed at each \ac{SQP} iteration (and other performance optimization routines, such as condensing \cite{rawlings2017model}) nor solving the entire \ac{SQP} for multiple iterations -- resulting in a much more computationally-efficient procedure than solving the entire $\ac{SQP}$ \textit{ex-novo} for every extra state-action sample. We remark that the linearization point in \cref{eq:tangential_predictor_qp} is updated at every timestep when a full \ac{SQP} is solved as part of demonstration-collection.





\subsection{Robustness and Performance Under Approximate Samples} \label{subsec:rob_perf_under_approx_samples}
While the proposed sensitivity-based \ac{DA} strategy enables the efficient generation of extra state-action samples, it introduces approximation errors that may affect the performance and robustness of the learned policy. Here, we discuss strategies to account for these errors, reducing the gaps between the nonlinear \ac{RTMPC} expert and the learned policy in terms of robustness and performance.

\noindent
\textbf{Robustness.}
A key property of \ac{RTMPC} is the ability to explicitly account for uncertainties, including the ones introduced by the proposed sensitivity-based \ac{DA} framework, by further tightening state and actuation constraints for the nominal safe plan (\cref{eq:nmpc_nominal}). The general nonlinear formulation of the dynamics in \cref{eq:system_dynamics}, however, makes it challenging to compute an \textit{exact} additional tightening bound for state and actuation constraints. A possible avenue to establish a tightening procedure for the actuation constraints is to observe that the linear approximation of \cref{eq:ancillary_nmpc_implicit} introduces an error upper bounded by~(\cite[Th. 8.16]{rawlings2017model}):
\begin{equation}
\| \kappa(\vbs{\chi}_t) - \hat{\kappa}(\!\vbf{x}^+_{t,j}, \vbs{\chi}_t)\|  \leq D \|\vbf{x}^+_{t,j} - \vbf{x}_t\|^2
 \end{equation}
where $D$ may be obtained by considering the Lipschitz constant of the controller (e.g. \cite{krishnamoorthy2022sensitivity}). However, estimating this constant may be difficult, or computationally expensive, for large-dimensional systems, as is the case herein. 
An alternative is to update the tubes as was done in \cref{subsec:nmpc_nominal_plan}, e.g., by employing Monte-Carlo simulations of the closed-loop system, starting from an initial (possibly conservative) tightening guess and by iteratively adjusting the cross-section (size) of the tube, or by directly learning the tubes from simulations or previous (conservative) real-world deployments \cite{fan2020deep}. These procedures, when performed using the learned policy, are particularly appealing in our context, as our efficient policy learning methodologies enable rapid training/updates of the learned policy, and the computational efficiency of the policy enables rapid simulations.   



\noindent
\textbf{Performance Improvements via Fine-Tuning}
In the context of learning policies from nonlinear \ac{RTMPC}, we include in \ac{SA} an (optional) fine tuning-step. This fine-tuning step consists in training the policy with additional demonstrations, without \ac{DA}, therefore avoiding introducing further approximate samples, and having discarded the extra data used to train the policy after an initial demonstration. More specifically, the overall \ac{SA} procedure for nonlinear \ac{RTMPC} with the optional fine-tuning step consists of the following: 
\begin{enumerate}[1)]
\item Collect a single task demonstration $\vbs{\xi}$ (e.g., reach goal) from the nonlinear \ac{RTMPC} expert; 
\item Perform \ac{DA} using the parametric sensitivity (\cref{subsec:sensitivity}) and train the policy, obtaining the policy parameters $\hat{\vbs{\theta}}_0$;
\item Optional \textit{fine-tuning} step:
\begin{enumerate}[i)]
\item Discard the collected data so far, including the data generated by the \ac{DA};
\item Collect new demonstrations (e.g., using DAgger \cite{ross2011reduction} and the pre-trained policy , or \ac{BC}), without \ac{DA}, and re-train the pre-trained policy (with parameters $\hat{\vbs{\theta}}_0$) after every newly collected demonstration.
\end{enumerate}
\end{enumerate}
In this procedure, our proposed tube-guided \ac{DA} is treated as a methodology to efficiently generate an initial guess $\hat{\vbs{\theta}}_0$ of the policy parameters; the policy can then be further fine-tuned, for performance improvements, using newly collected demonstrations via on-policy (e.g, \ac{DAgger}) or off-policy methods (e.g., \ac{BC}).


