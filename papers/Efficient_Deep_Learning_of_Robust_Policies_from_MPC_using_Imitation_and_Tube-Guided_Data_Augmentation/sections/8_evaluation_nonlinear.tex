In this Section, we evaluate the ability of our method to efficiently learn acrobatic flight maneuvers using demonstrations collected from nonlinear \ac{RTMPC}. 

\subsection{Evaluation Approach}
\noindent \textbf{Task Description}
We consider the task of learning a policy capable of performing a flip, i.e., a $360^\circ$ rotation about the body-frame $x$-axis, in near-minimum time. This is a challenging maneuver, as it covers a large nonlinear envelope of the dynamics of the \ac{MAV}, and the near-minimum time objective function, combined with the need to account for uncertainties, pushes the actuators close to their physical limits. 

\noindent
\textbf{Simulation Environment.} The simulation environment for training/numerical evaluations is the same as in \cref{sec:evaluation_linear}, i.e., implements the nonlinear multirotor model in \cref{subsec:mav_model}. In the training domain (source, $\mathcal{S}$), $\mathbb{W}_\mathcal{S} = \{ \emptyset \}$, while in the deployment domain (target, $\mathcal{T}$) $\mathbb{W}_\mathcal{T} = \{f_\text{ext} | 0.001 m g \leq f_\text{ext} \leq 0.3 m g \}$, sampled according to \cref{eq:disturbance_sampling}.

\input{tables/acados_parameters.tex}
\noindent \textbf{Nonlinear \ac{RTMPC}.}
We generate a safe nominal flip trajectory using \texttt{MECO-Rockit} \cite{gillis2020effortless}. Because this nominal trajectory happens in the plane spanned by the orthogonal vectors defining the $y$ and $z$ axis of the inertial reference frame $\text{W}$, for simplicity, we project the dynamics onto the $y$ and $z$ axes, resulting in a two-dimensional model of the \ac{MAV} used to generate the nominal plan. The nominal flip trajectory can therefore be obtained by setting the initial rotation around the $z$ to be $0$, and the desired final attitude to be $2 \pi$, while the remaining initial/terminal states are all set to zero. 

The ancillary \ac{NMPC} is solved using the \ac{SQP} solver \texttt{ACADOS} \cite{Verschueren2021}, and runs in simulation at $50$Hz. Sensitivities for \ac{DA} (\cref{eq:tangential_predictor_qp}) are computed using the built-in sensitivity computation in the chosen solver, \texttt{HPIPM} \cite{frison2020hpipm}.
We remark that the employed ancillary \ac{NMPC} uses the full 3D multirotor model in \cref{eq:mav_reduced_model_for_nmpc}, therefore performing 3D disturbance rejection -- a critical requirement for real-world deployments. %
For a more challenging and interesting comparison to the considered \ac{IL} baselines, the ancillary \ac{NMPC} uses the \texttt{SQP\_RTI} setting of \texttt{ACADOS}. This setting performs only a single \ac{SQP} iteration per timestep, enabling significant speed-ups in the solver, and it is often employed in real-time, embedded implementations of \ac{NMPC}. This setting creates an advantage, in terms of training time, to \ac{IL} methods that require querying the expert multiple times (the baselines of our comparison), as it speeds-up the computation time of the expert. 
The other \texttt{ACADOS} parameters given in \cref{tab:acados_parameters} were chosen as they enabled higher overall performance/accuracy in the selected acrobatic maneuver. Last, we introduce a discount factor $\gamma = 0.95$ in the stage cost of \cref{eq:ancillary_nmpc_eq} to aid the convergence of the solver.

\noindent \textbf{Student Policy}
The student policy is a $2$-hidden layer, fully connected \ac{DNN} with $\{64, 32\}$ neurons per layer, and \texttt{ReLU} activation functions. The input vector has dimension $14$, as it contains the current state ($n_x = 10$), the current time $t$, and a desired final position $\vbs{p}^\text{des}$ (fixed to the origin). To simplify the learning and \ac{DA} procedure, we enforce continuity to the quaternion input of the policy using the method in \cite[Eq. 3]{kusaka2022stateful}, avoiding the need to increase the training data/demonstrations at every timestep to account for the fact that $\vbs{q}$ and $-\vbs{q}$ encode the same orientation.



\noindent
\textbf{Baselines and Evaluation Metrics} The choice of baselines matches those in \cref{sec:evaluation_linear} (\ac{DAgger}, \ac{BC} and their combination with \ac{DR}) and so do the monitored metrics (\textit{Robustness}, \textit{Performance} and \textit{Training Time}), with the difference that performance is based on the stage cost of the ancillary \ac{NMPC} \cref{eq:ancillary_nmpc_eq}. 

\noindent \textbf{Training Details}
As in \cref{sec:evaluation_linear}, training is performed by collecting demonstrations with the multirotor starting from slightly different initial states inside the tube centered around the origin. The nominal flip maneuver is pre-generated, as the goal state $\vbf{x}^e_{\tzr}$ (with $\tzr = 0)$ does not change, and only the ancillary \ac{NMPC} is solved at every timestep. The resulting flip maneuver takes about $T_f = 2.5$s, and demonstrations are collected over an episode of length $3.0$s, at $50$Hz ($T = 150$ environment steps per demonstration).
To speed-up the demonstration collection phase, and thereby avoid excessive re-training of the policy, we collect demonstrations in batches for all the baselines, using $10$ demonstrations per batch for $20$ batches. For \ac{SA}-methods, we collect demonstrations one-by-one, and we implement the fine-tuning procedure described in \cref{subsec:rob_perf_under_approx_samples} by performing \ac{DA} with the first collected demonstration, while we do not perform \ac{DA} for the following demonstrations. Because of its computational efficiency, we always use the sensitivity-based \ac{DA} (i.e., \cref{eq:approximate_ancillary_controller}, assuming no changes in active set of constraints). Each time the policy is updated, we evaluate it $20$ times in source $\mathcal{S}$ and target $\mathcal{T}$ environments. The evaluations are averaged across $10$ different random seeds. To further speed-up training of all the methods, we update the previously trained policy using only the newly collected batch of demonstrations (or single demonstration, for \ac{SA}). All the policies are trained using the ADAM optimizer for up to $400$ epochs, but we terminate training if the validation loss (from $30\%$ of the data) does not decrease within $30$ epochs. Last, to study the effects of varying the number of samples used for \ac{DA}, we introduce \ac{SA}-$N_S$, a variant of \ac{SA} where we sample uniformly inside the tube $N_S = \{25, 50, 100\}$ samples for every timestep.

\subsection{Numerical Evaluation of Robustness, Performance, and Efficiency} 
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth, trim={3cm 0.5cm 3.5cm 1cm}, clip]{figs/quadrotor_flip_v3/robustness.pdf}
    \caption{Robustness as a function of the number of training demonstrations. The \ac{IL} methods that use the proposed \ac{DA} strategy, Sampling-Augmentation (SA), overlap on the top-left part of the diagram, achieving full success rate in both the source and target domain of the training environment. Uncertainties in the target domains are applied in the form of a constant external force acting on the center of mass of the multirotor, representing large wind disturbances.}
    \label{fig:nmpc_robustness}
    \centering
    \includegraphics[width=\columnwidth, trim={3cm 0.5cm 3.5cm 1cm}, clip]{figs/quadrotor_flip_v3/training_time_vs_robustness.pdf}
    \caption{Robustness as a function of the training time. The \ac{IL} methods that use the proposed \ac{DA} and fine-tuning strategy, Sampling-Augmentation (SA), achieve full robustness under uncertainties in a fraction of the training time required by the best performing robust baseline, DAgger + DR.}
    \label{fig:nmpc_efficiency}
    \centering
    \includegraphics[width=\columnwidth, trim={3cm 0.5cm 3.5cm 1cm}, clip]{figs/quadrotor_flip_v3/performance.pdf}
    \caption{Performance as a function of the number of training demonstrations. The \ac{IL} methods that use the proposed \ac{DA} and fine-tuning strategy, Sampling-Augmentation (SA), achieve performance close to the expert in less than $10$ demonstrations, while the best-performing baseline, DAgger + DR, achieves comparable performance but in a larger number of demonstrations.}
    \label{fig:nmpc_performance}
\end{figure}
\noindent

\noindent \textbf{Comparison with Baselines.}
We start by evaluating the robustness and performance of the proposed approach as a function of the number of demonstrations collected in simulation, and as a function of the training time. 

\cref{fig:nmpc_robustness} shows the robustness of the considered method as a function of the number of expert demonstrations. It reveals that \ac{SA}-based approaches can achieve full success rate in the environment with disturbances (target, $\mathcal{T}$) and without disturbances (source, $\mathcal{S}$) after a single demonstration, while the best-performing baseline, \ac{DAgger}+\ac{DR}, requires about $60$ demonstrations to achieve full robustness in $\mathcal{S}$, and more than $100$ in $\mathcal{T}$. \ac{SA}-based methods, therefore, enable more than one order of magnitude reduction in the number of demonstrations (interactions with the environment) compared to \ac{DAgger}+\ac{DR}.
As previously observed in \cref{sec:evaluation_linear}, \ac{DAgger} alone is not robust. 
Additionally, \ac{BC} methods fail to converge, potentially due to the lack of sufficiently meaningful exploration and the forgetting caused by the iterative training strategy employed. 

\cref{fig:nmpc_efficiency} additionally shows the robustness as a function of the training time (recall, this includes demonstration collection and policy train). The results show that the demonstration-efficiency of \ac{SA}-based methods translates into significant improvements in training time, as \ac{DAgger}+\ac{DR} requires more than $3$ times the training time than \ac{SA}-based approaches. These improvements are larger for the variants of \ac{SA} that generate fewer extra samples (e.g., \ac{SA}-$25$).

Last, \cref{fig:nmpc_performance} reports the \textit{performance} as a function of the number of demonstrations. The results indicate that \ac{SA}-based methods can achieve low tracking errors even after a single demonstration. Furthermore, employing a fine-tuning phase (after the initial demonstration) proves highly advantageous in further reducing this error, thereby reducing the performance gap between policies obtained via \ac{SA} and the expert.


\noindent \textbf{Comparison of Sampling Strategies.}
\cref{tab:sa_analysis} provides a detailed comparison of performance, robustness, and training time of the different variants of \ac{SA} methods, as a function of the number of demonstrations ($1$, $2$ and $10$), and compares those with the best-performing baselines, \ac{DAgger}+\ac{DR}.  As expected, \ac{SA} methods that require fewer samples obtain significant improvements in training time, while increasing the number of samples is beneficial in reducing the mean and the variance of the expert gap, both with and without disturbances. \cref{tab:sa_analysis} additionally highlights the benefits of fine-tuning, as even methods that use few samples (e.g., SA-sparse, SA-$25$) can obtain a significant performance improvement after a single fine-tuning demonstration ($2$ demonstrations in total), while there are diminishing returns for additional fine-tuning demonstrations (e.g., $10$ demonstrations).     


\input{tables/computation_time_rtnmpc.tex}
\noindent
\textbf{Computation.} The computational cost of the nonlinear \ac{RTMPC} expert and the learned policy is reported in \cref{tab:computational_cost_rtnmpc}, highlighting that the policy is $66$ times faster than the expert. The average time to step the training environment is $2.1$ ms.



\begin{table}
    \tiny
    \renewcommand{\tabcolsep}{1pt}
    \centering
    \include{tables/numerical_comparison_sa_nonlinear_rtmpc_v3.tex}
    \caption{Performance, robustness and training time for SA-based methods after $1$, $2$, and $10$ demonstrations, compared with the best performing baselines, DAgger+DR, in the environment without disturbances ($\mathcal{S}$, source), and with ($\mathcal{T}$, target). 
    Robustness is color-coded from white ($100\%$) to red ($90\%$ or below). Performance and training time are color-coded from green (fast training time, small expert gap) to red (long training time, large expert gap). The results highlight that \ac{SA}-methods achieve high robustness and close to expert performance compared to DAgger+DR, even after a single demonstration, and their performance can be further improved via additional fine-tuning demonstrations. We note that \ac{DAgger} and \ac{BC}-based approaches differ at one demonstration due to non-determinism in the training procedure.}
    \label{tab:sa_analysis}
    \vspace{-5pt}
\end{table}

\subsection{Hardware Evaluation}
We experimentally evaluate the ability of the policy to perform a flip on a real multirotor, under real-world uncertainties such as model errors (e.g., inaccurate thrust to battery voltage mappings, aerodynamic coefficients, moments of inertia) and external disturbances (e.g., ground effect). The tested policy is obtained using DAgger+SA-$25$ trained after $2$ demonstrations (the first with \ac{DA}, the second for fine-tuning), as the method represents a good trade-off between performance, robustness and training time. As in \cref{sec:evaluation_linear}, we deploy the learned policy on an onboard \texttt{Nvidia Jetson TX2}, where it runs at $100$Hz. The maneuver includes a take-off/landing phase consisting of a $1$m ramp on $x$-$y$-$z$ in $\text{W}$ and overall has a total duration of $6$s. The maneuver is repeated $5$ times in a row, to demonstrate repeatability, recording successful execution of the maneuver and successful landing at the designated location in all the cases. \cref{fig:flip_timelapse} shows a time-lapse of the different phases of the maneuver (excluding the ramp from and to the landing location). The 3D position of the robot, as well as the direction of its thrust vector, are shown for two runs in \cref{fig:rtnmc_experiment_state_traj}, highlighting the large distance and altitude traveled in a short time. \cref{fig:rtnmc_experiment_actuation} additionally shows some critical parameters of the maneuvers, such as the attitude and the angular velocity, as well as thrust and the vertical velocities. It highlights that the robot rotates at up to $11$rad/s, and the overall $360^\circ$ rotation takes about $0.5$s. Overall, these results validate our numerical analysis and highlight the robustness and performance of a policy efficiently trained from $2$ demonstrations and about $100$s of training time. Our video submission \cite{video_submission} includes an additional experiment demonstrating near-minimum time navigation from one position to another, starting and ending with velocity close to zero, using a policy trained with two demonstrations (DAgger+SA-$25$).

\begin{figure}
    \centering
    \begin{subfigure}{.5\columnwidth}
      \centering
      \includegraphics[width=\columnwidth, trim = {6.0in, 1.5in, 18.0in, 1.5in}, clip]{figs/flip_3d/flips_v2.pdf}
    \end{subfigure}%
    \begin{subfigure}{.5\columnwidth}
      \centering
      \includegraphics[width=\columnwidth, trim = {19in, 1.5in, 5.0in, 1.5in}, clip]{figs/flip_3d/flips_v2.pdf}
    \end{subfigure}%
    \caption{Acrobatic flip trajectory performed by our UAV using a policy efficiently learned from a nonlinear Robust Tube MPC. All the units are expressed in meters. The red arrows denote the direction of the thrust vector on the quadrotor, and they highlight that the flip occurs at the point of highest altitude of the maneuver. The plot shows the additional take-off/landing phase, also performed by the learned policy, taking the robot back and forth the origin and $(x, y, z) = (1, 1, 1)$, that is the point where the flip starts and ends. All the units are expressed in (m). 
    } 
    \label{fig:rtnmc_experiment_state_traj}
    \vspace{-3ex}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth, trim={0.4cm 0.5cm 0.5cm 0.5cm}, clip]{figs/flip_cmds/flip_cmds_v6.pdf}
    \caption{
    Control inputs generated by the learned policy and relevant states during the real--world acrobatic flip maneuver on a multirotor, where the robot is subject to large levels of uncertainties, caused primarily by model mismatches such as thrust-to-battery voltage mappings and hard-to-model aerodynamic effects. 
    Despite the large level of uncertainties, that require the usage of the maximum thrust allowed, the maneuver is completed successfully, and the robot reaches the desired vertical velocity of \SI{-1.0}{\m \per \s} at the end of the recovery phase (and that corresponds to the initial velocity of the landing phase). 
    We highlight that the flip is performed at an angular velocity of about \SI{11.0}rad/s. 
    The actual thrust $t_\text{cmd}$ can be related to the 
    normalized thrust $\bar{t}_\text{cmd}$ via  
    $t_\text{cmd} = m g (1 + \bar{t}_\text{cmd})$,
    where $ m g $ is the weight force of the robot.}
    \label{fig:rtnmc_experiment_actuation}
\end{figure}
\vspace{-7pt}