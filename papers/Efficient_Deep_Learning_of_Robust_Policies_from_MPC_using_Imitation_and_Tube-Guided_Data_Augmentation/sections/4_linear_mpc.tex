
\label{sec:robust_tube_mpc}
In this Section, we present the strategy to efficiently learn robust policies from \ac{MPC} when the system dynamics in \cref{eq:system_dynamics} can be well approximated by a linear model of the form:
\begin{equation}
\label{eq:linearized_dynamics}
\vbf{x}_{t+1} = \vbf{A} \vbf{x}_t + \vbf{B} \vbf{u}_t + \vbf{w}_t.
\end{equation}
First, we present the Robust Tube variant of linear MPC, \ac{RTMPC}, that we employ to collect demonstrations (\cref{subsec:rtmpc_expert}). Then, we present a strategy that leverages information available from the \ac{RTMPC} expert to compensate for the covariate shifts caused by uncertainties and mismatches between the training and deployment domains (\cref{sec:sa}). Our strategy is based on a \ac{DA} procedure that can be combined with different \ac{IL} methods (on-policy, such as \ac{DAgger} \cite{ross2011reduction}, and off-policy, such as \ac{BC}, \cite{pomerleau1989alvinn}) for improved efficiency/robustness in the policy learning procedure. The \ac{RTMPC} expert is based on \cite{mayne2005robust} but with the objective function modified to track desired trajectories, as trajectory-tracking tasks will be the focus of the experimental evaluation of policies learned from this controller (\cref{sec:evaluation_linear}). 

\subsection{Trajectory Tracking \ac{RTMPC} Expert Formulation} \label{subsec:rtmpc_expert}
\ac{RTMPC} is a type of robust \ac{MPC} that regulates the system in \cref{eq:linearized_dynamics} while ensuring satisfaction of the state and actuation constraints $\mathbb X, \mathbb U$ regardless of the disturbances $\mathbf w \in \mathbb{W}_\mathcal{T}$. 


\noindent
\textbf{Mathematical Preliminaries.} Let $\mathbb{A} \subset \mathbb{R}^{n}$ and $\mathbb{B} \subset \mathbb{R}^{n}$ be convex polytopes, and let $\mathbf{C} \in \mathbb{R}^{m \times n}$ be a linear mapping. In this context, we establish the following definition:
\begin{enumerate}[a)]
\item Linear mapping: $\mathbf C \mathbb{A} \coloneqq \{\mathbf{C} \mathbf a \in \mathbb{R}^m \:|\: \mathbf a \in \mathbb{A}\}$
\item Minkowski sum: $\mathbb{A} \oplus \mathbb{B} \coloneqq \{\mathbf a + \mathbf b \in \mathbb{R}^n \:|\: \mathbf a \in \mathbb{A}, \: \mathbf b \in \mathbb{B} \}$
\item Pontryagin diff.: $\mathbb{A} \ominus \mathbb{B} \coloneqq \{\mathbf c \in \mathbb{R}^n \:|\: \mathbf{c + b} \in \mathbb{A}, \forall \mathbf  b \in \mathbb{B} \}$. 
\end{enumerate}
\textbf{Optimization Problem.} 
At each time step $t$, trajectory tracking \ac{RTMPC} receives the current robot state $\mathbf x_t$ and a desired trajectory $\mathbf{X}^\text{des}_t = \{\xdes_{0|t},\dots,\xdes_{N|t}\}$ spanning $N+1$ steps as input. It then computes a sequence of reference (``safe'') states $\bar{\mathbf{X}}_t = \{\xsafe_{0|t},\dots,\xsafe_{N|t}\}$ and actions $\bar{\mathbf{U}}_t = \{\usafe_{0|t},\dots,\usafe_{N-1|t}\}$ that ensure constraint compliance regardless of the realization of $\mathbf{w}_t \in \mathbb{W}_\mathcal{T}$. This  is achieved by solving the following optimization problem:
\begin{align}
\label{eq:rtmpc_optimization_problem}
    \mathbf{\bar{U}}_t^*, \mathbf{\bar{X}}_t^* &= \underset{\mathbf{\bar{U}}_t, \mathbf{\bar{X}}_t}{\text{argmin}}
        \| \mathbf e_{N|t} \|^2_{\mathbf{P}_x} + 
        \sum_{i=0}^{N-1} 
            \| \mathbf e_{i|t} \|^2_{\mathbf{Q}_x} + 
            \| \mathbf u_{i|t} \|^2_{\mathbf{R}_u} \notag \\
    &\text{subject to} \:\:  \xsafe_{i+1|t} = \mathbf A \xsafe_{i|t} + \mathbf B \usafe_{i|t}, \\
    &\xsafe_{i|t} \in \mathbb{X} \ominus \mathbb{Z}, \:\: \usafe_{i|t} \in \mathbb{U} \ominus \mathbf{K} \mathbb{Z}, \notag\\
    &\vbf{x}_t \in \mathbb{Z} \oplus  \xsafe_{0|t}, \: i = 0, \dots, {N-1}  \notag
\end{align}
where $\mathbf e_{i|t} = \xsafe_{i|t} - \xdes_{i|t}$ is the tracking error. The positive definite matrices $\mathbf{Q}_x$,
$\mathbf{R}_u$ 
define the trade-off between deviations from the desired trajectory and actuation usage, while $\| \mathbf e_{N|t} \|^2_{\mathbf{P}_x}$ is the terminal cost. $\mathbf{P}_x$ 
and $\mathbf K$ are obtained by formulating an infinite horizon optimal control LQR problem using $\mathbf A$, $\mathbf B$, $\mathbf{Q}_x$ and $\mathbf{R}_u$ and by solving the associated algebraic Riccati equation \cite{aastrom2021feedback}.
To achieve recursive feasibility, we ensure a sufficiently long prediction horizon is selected, as commonly practiced \cite{kamel2017model}, while omitting the inclusion of terminal set constraints.

\noindent
\textbf{Tube and Ancillary Controller.}  A control input for the real system is generated by \ac{RTMPC} via an \textit{ancillary controller}:
\begin{equation}
\label{eq:ancillary_controller}
    \mathbf u_t = \usafe^*_{t} + \mathbf K (\mathbf{x}_t - \xsafe^*_{t}),
\end{equation}
where $\usafe^*_t = \usafe^*_{0|t}$ and $\xsafe^*_t = \xsafe^*_{0|t}$. 
As shown in \cref{fig:tube_illustration}, this controller ensures that the system remains inside a \textit{tube} (with ``cross-section'' $\mathbb{Z}$) centered around $\xsafe_t^*$ regardless of the realization of the disturbances in $\mathbb{W}_\mathcal{T}$, provided that the tube contains the initial state of the system (constraint $\vbf{x}_t \in \mathbb{Z} \oplus \xsafe_{0|t}$).
The set $\mathbb{Z}$ is a disturbance invariant set for the closed-loop system $\mathbf{A}_K := \mathbf{A + B K}$, satisfying the property that $\forall \mathbf{x}_j \in \mathbb{Z}$, $\forall \mathbf{w}_j \in \mathbb{W}_\mathcal{T}$, $\forall j \in \mathbb{N}^+$, $\mathbf{x}_{j+1} = \mathbf{A}_K \mathbf{x}_j + \mathbf{w}_j \in \mathbb{Z}$ \cite{mayne2005robust}. 
$\mathbb{Z}$ can be computed offline using $\mathbf{A}_K$ and the model of the disturbance $\mathbb{W}$ via ad-hoc analytic algorithms~\cite{borrelli2017predictive, mayne2005robust}, or can be learned from data~\cite{fan2020deep}. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth, trim={0.3in, 1.2in, 0.3in, 7.2in}, clip]{figs/tube_diagram/augmented_sampling_tube_mpc_journal.drawio.pdf}
    \caption{
    Illustration of the robust control invariant tube $\mathbb{Z}$
    centered around the optimal reference $\bar{\mathbf{x}}_0^*(\mathbf{x}_t)$
    computed by RTMPC at every state $\mathbf{x}$, for a system with state dimension $n_x = 2$.}
    \label{fig:tube_illustration}
\end{figure}


\subsection{Covariate Shift Compensation via Sampling Augmentation}
\label{sec:sa}
We observe that training a policy by collecting demonstrations in a controlled source domain $\mathcal{S}$, with the objective of deploying it in the target domain $\mathcal{T}$, may introduce a sample selection bias \cite{kouw2018introduction}, i.e., demonstrations are collected only around the nominal state distribution, and not around the distribution induced by perturbations encountered in the \textit{sim2real} and \textit{lab2real} transfer. Such selection bias is a known cause of distribution shifts \cite{kouw2018introduction}, and it is usually mitigated by re-weighting collected samples in a way that takes into account their likelihood of appearing in the target domain $\mathcal{T}$ (e.g., via importance-sampling). These approaches, however, do not apply in our case, since we do not have access to samples/demonstrations collected in $\mathcal{T}$. 


In this work, we propose to mitigate the covariate shift introduced by the policy generation procedure not only by collecting demonstrations from the \ac{RTMPC} but by using additional information computed in the controller.
Specifically, during the collection of a trajectory $\boldsymbol{\xi}$ in the source domain $\mathcal{S}$, we utilize instead the tube computed by the \ac{RTMPC} demonstrator to obtain knowledge of the states that the system may visit when subjected to perturbations. Given this information, we propose a tube-guided \ac{DA} strategy, called \acf{SA}, that samples states from the tube. The corresponding actions are provided at low computational cost by the ancillary controller of the \ac{RTMPC} expert. The collected state-actions pairs are then included in the dataset of demonstrations used to train a policy. The following paragraphs frame the tube sampling problem in the context of covariate shift reduction in \ac{IL}, and discuss tube-sampling strategies. %

\noindent 
\textbf{Tube as a Model of State Distribution Under Uncertainties.} 
The key intuition of the proposed approach is the following. We observe that, although the density $p(\boldsymbol{\xi}|\pi_{\vbs{\theta}},\mathcal{T})$ is unknown, an approximation of its support $\mathfrak{R}$, given a demonstration $\boldsymbol{\xi}$ collected in the source domain $\mathcal{S}$, is known. Such support corresponds to the tube computed by the \ac{RTMPC} when collecting $\boldsymbol{\xi}$:
\begin{equation}
    \mathfrak{R}_{\boldsymbol{\xi}^+|\pi_{\vbs{\theta}^*},\boldsymbol{\xi}} = \bigcup_{t=0}^{T-1}\{\bar{\vbf{x}}_{t}^* \oplus \mathbb{Z}\}.
    \label{eq:tube}
\end{equation}
where $\boldsymbol{\xi}^+$ is a trajectory in the tube of $\boldsymbol{\xi}$.
This is true thanks to the ancillary controller in \cref{eq:ancillary_controller}, which ensures that the system remains inside \cref{eq:tube} for every possible realization of $\vbf{w} \in \mathbb{W}_\mathcal{T}$.
The ancillary controller additionally provides a computationally efficient way to obtain the actions to apply for every state inside the tube. Let $\vbf{x}_{t,j}^+ \in \bar{\vbf{x}}_t^* \oplus \mathbb{Z}$, i.e., $\vbf{x}_{t,j}^+$ is a state inside the tube computed when the system is at $\vbf{x}_t$, then the corresponding robust control action $\vbf{u}_{t,j}^+$ is:
\begin{equation}
\vbf{u}_{t,j}^+ = \bar{\vbf{u}}_t^* +  \vbf{K}(\vbf{x}_{t,j}^+ - \bar{\vbf{x}}_t^*).
\label{eq:tubempc_feedback_policy}
\end{equation}
For every timestep $t$ in $\boldsymbol{\xi}$, extra state-action samples $(\vbf{x}_{t,j}^+, \vbf{u}_{t,j}^+)$, with $j = 1, \dots, N_s$ collected from within the tube can be used to augment the dataset employed to train the policy, obtaining a way to approximate the expected risk in the domain $\mathcal{T}$ by only having access to demonstrations collected in $\mathcal{S}$: 
\begin{equation}
\begin{multlined}
    \mathbb{E}_{p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{T})}\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi}) \approx \\
    \mathbb{E}_{p(\boldsymbol{\xi}|\pi_{\vbs{\theta}}, \mathcal{S})}[\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi}) +  \mathbb{E}_{p(\boldsymbol{\xi}^+|\pi_{\vbs{\theta}^*},\boldsymbol{\xi})}\mathcal{L}(\vbs{\theta}, \vbs{\theta}^*|\boldsymbol{\xi}^+)].
    \label{eq:sampling_augmentation}
\end{multlined}
\end{equation}
The demonstrations in the source domain $\mathcal{S}$ can be collected using existing \ac{IL} techniques, such as \ac{BC} or \ac{DAgger}.

\noindent
\textbf{Tube Approximation and Sampling Strategies.}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/icra/sampling_extra_states_drawio_v4-cropped.pdf}
    \caption{The possible strategies to sample extra state-action pairs from an axis-aligned bounding box approximation of the tube of the \ac{RTMPC} expert: dense (left) and sparse (right). The tube is represented for a system with state dimension $n_x = 3$.}
    \label{fig:tube_sampling_strategies} %
\end{figure}
In practice, the density $p(\boldsymbol{\xi}^+|\boldsymbol{\xi}, \pi_{\vbs{\theta}^*})$ may not be available, making it difficult to establish which states to sample for \ac{DA}. We consider an adversarial approach to the problem by sampling states that may be visited under worst-case perturbations. To efficiently compute those samples, we (outer) approximate the tube $\mathbb{Z}$ with an axis-aligned bounding box $\hat{\mathbb{Z}}$. We investigate two strategies, shown in~\cref{fig:tube_sampling_strategies}, to obtains state samples $\vbf{x}_{t,j}^+$ at every state $\vbf{x}_t$ in $\boldsymbol{\xi}$:
\begin{inparaenum}[i)]
\item dense sampling: sample extra states from the vertices of $\bar{\vbf{x}}_t^*\oplus\hat{\mathbb{Z}}$. The approach produces $N_s = 2^{n_x}$ extra state-action samples. It is more conservative, as it produces more samples, but more computationally expensive.
\item sparse sampling: sample one extra state from the center of each \textit{facet} of $\bar{\vbf{x}}_t^* \oplus\hat{\mathbb{Z}}$, producing  $N_s = 2n_x$ additional state-action pairs. It is less conservative and more computationally efficient.
\end{inparaenum}

