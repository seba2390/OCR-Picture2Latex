\Ac{MPC}~\cite{borrelli2017predictive, rawlings2017model} enables impressive performance on complex, agile robots~\cite{lopez2019dynamic, lopez2019adaptive, li2004iterative, kamel2017linear, minniti2019whole, williams2016aggressive}. However, its computational cost often limits the opportunities for onboard, real-time deployment~\cite{lco2020170:online}, or takes away critical computational resources needed by other components governing the autonomous system. 
Recent works have mitigated \ac{MPC}'s computational requirements by relying on computationally efficient \ac{DNN} policies that are trained to imitate task-relevant demonstrations generated by \ac{MPC} in an offline training phase.
Such demonstrations are generally collected via \ac{IL}~\cite{kaufmann2020deep, ross2013learning, reske2021imitation}, where the \ac{MPC} acts as an expert that provides demonstrations, and the \ac{DNN} policy is treated as a student, trained via supervised learning. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth, trim={0cm 1.3cm 22cm 1.0cm}, clip]{figs/flip_img_stacking/flip_image_stacked_81_v3.pdf}
    \caption{Time-lapse figure showing an acrobatic maneuver (flip) performed by our multirotor, using a policy learned from a Robust Tube MPC variant that uses nonlinear models, and that is learned in a few minutes requiring only two demonstrations (one performing our proposed tube-guided data augmentation, and one for fine-tuning). (a) shows the robot accelerating upwards until it reaches the optimal altitude found by MPC. The red arrow denotes the directions of the thrust vector (aligned with the body $z$-axis), while the yellow arrow denotes the approximate trajectory. (b) shows the robot performing a $360^\circ$ rotation round its body $x$-axis. This phase takes only about $0.5$s. (c) shows the robot successfully decelerating until it reaches a vertical velocity $<1$ m/s, followed by its landing. The learned policy runs onboard (\texttt{Nvidia Jetson TX2}, CPU) at $100$Hz. This experiment demonstrates the ability of our approach to efficiently generate policies from \ac{MPC} that can withstand real-world uncertainties and disturbances, even when leveraging models that operate in highly nonlinear regimes.}
    \label{fig:flip_timelapse}
    \vspace{-3ex}
\end{figure}


A common issue in existing \ac{IL} methods (e.g., \ac{BC}~\cite{pomerleau1989alvinn, osa2018algorithmic, bojarski2016end}, \ac{DAgger}~\cite{ross2011reduction}) is that they require to collect a relatively large number of \ac{MPC} demonstrations, even for a single task like tracking a specific trajectory. This drawback introduces two significant challenges: firstly, it requires a substantial number of queries to the computationally-expensive \ac{MPC} expert, demanding expensive training equipment. Secondly, it leads to a considerable volume of queries to the training environment, which restricts data collection in computationally-expensive simulations (e.g., fluid-simulators) or requires numerous hours of real-time demonstrations on the physical robot, making it impractical.

One of the causes for such demonstration-inefficiency is the need to take into account and correct for the compounding of errors in the learned policy~\cite{ross2011reduction}, which may otherwise create shifts (\textit{covariate shifts}) from the training distribution, with catastrophic consequences~\cite{pomerleau1989alvinn}. These distribution shifts can be caused by: 
\begin{inparaenum}[(a)]
    \item approximation errors in the learned policy; 
    \item mismatches, e.g., due to modeling errors, between the simulator used to collect demonstrations and the deployment domain (i.e., sim2real gap); or
    \item model changes or disturbances that may not be present in a controlled training environment (lab/factory when training on a real robot), but that do appear during deployment in the real-world (i.e., lab2real gap).
\end{inparaenum} 
Approaches employed to compensate for these \textit{gaps} and generate a robust policy, such as \ac{DR}~\cite{peng2018sim, loquercio2019deep}, introduce additional challenges, such as the need to apply disturbances or model changes during training.
\par

In this work, we address the problem of generating a robust \ac{DNN} policy from \ac{MPC} in a demonstration and computationally efficient manner by designing a computationally-efficient \ac{DA} strategy that systematically compensates for the effects of covariate shifts that might be encountered during real-world deployment. Our approach, depicted in \cref{fig:sim_to_real_zero_shot_cover}, relies on a prior model of the perturbations/uncertainties encountered in a deployment domain, which is used to generate a robust version of the given \ac{MPC}, called \ac{RTMPC}, to collect demonstrations and to guide the \ac{DA} strategy. The key idea behind this \ac{DA} strategy consists in observing that the \ac{RTMPC} framework provides:
\begin{inparaenum}[(a)]
    \item information on the states that the robot may visit when subject to uncertainty. This is represented by a \textit{tube} that contains the collected demonstration; the tube can be used to identify/generate extra relevant states for \ac{DA}; and
    \item an \textit{ancillary controller} that maintains the robot inside the tube regardless of the realization of uncertainties; this controller can be used to generate extra actions.
\end{inparaenum}

This article extends our prior conference paper \cite{tagliabue2022efficient}, where the focus was on efficiently generating robust trajectory tracking policies from a \textit{linear} \ac{MPC}. In this new work, we additionally provide a strategy to generate a \ac{DNN} policy to reach a desired state using a \textit{nonlinear} \ac{MPC} expert, presenting a new methodology that can be used to perform \ac{DA} in a computationally-efficient way. 
This extension is non-trivial, as the \textit{ancillary controller} in the nonlinear \ac{RTMPC} framework requires, unlike the linear case, expensive computations to generate extra actions for \ac{DA}, resulting in long training times when performing \ac{DA}. This new work 
\begin{inparaenum}[(i)]
    \item solves the computational-efficiency issues in the ancillary controller of nonlinear \ac{RTMPC} by generating an approximation of the ancillary controller that is used to more efficiently compute the actions corresponding to extra state samples for \ac{DA}. Additionally, this work presents
    \item a policy fine-tuning procedure to minimize the impact on performance introduced by our approximation;
    \item a formulation of nonlinear \ac{RTMPC} for acrobatic flights on multirotors; 
    \item numerical comparison to \ac{IL} baselines;
    \item numerical comparison of different tube-sampling strategies;
    \item new real-world experiments with policies that leverage nonlinear models. 
\end{inparaenum}

We tailor our approach to the task of efficiently learning robust policies for agile flight on a multirotor. First, we demonstrate in experiments trajectory tracking capabilities with a policy learned from a linear trajectory tracking \ac{RTMPC}. The policy is learned from a \textit{single} demonstration collected in simulation or directly on the real robot, and it is robust to previously-unseen wind disturbances. Second, we demonstrate the ability to generate a policy from a go-to-goal-state nonlinear \ac{RTMPC} capable of performing acrobatic maneuvers, such as a  $360$ degrees flip. These maneuvers are performed under real-world uncertainties, using a policy obtained from only \textit{two} demonstrations and in less than $100$s of training time.




In summary, our work presents the following \textbf{contributions:} 
\begin{itemize}

\item A procedure to \textit{efficiently} learn \textit{robust} policies from \ac{MPC}. Our procedure is:
    \begin{inparaenum}
    \item \textit{demonstration-efficient}, as it requires a small number of queries to the training environment, resulting in a method that enables learning from a single \ac{MPC} demonstration collected in simulation or on the real robot; 
    \item \textit{training-efficient}, as it reduces the number of computationally expensive queries to the computationally expensive \ac{MPC} expert using a computationally efficient \ac{DA} strategy; 
    \item \textit{generalizable}, as it produces policies robust to disturbances not experienced during training.
    \end{inparaenum}
\item We generalize the demonstration-efficient policy learning strategy proposed in our previous conference paper \cite{tagliabue2022efficient} with the ability to efficiently learn robust and generalizable policies from variants of \ac{MPC} that use nonlinear models. 
\item Extensive simulations and comparisons with state-of-the-art \ac{IL} methods and robustification strategies. 
\item Experimental evaluation on the challenging task of trajectory tracking and acrobatic maneuvers on a multirotor, presenting the first instance of sim2real transfer of a policy trained after a single demonstration, and robust to previously unseen real-world uncertainties. 
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth, trim={0.3in, 9.25in, 4.9in, 0.4in}]{figs/cover_image/rtpmc-compression-cover-journal.drawio.pdf}
    \caption{Overview of the proposed approach to generate a \ac{DNN}-based policy $\pi_\theta$ from a computationally expensive \ac{MPC} in a demonstration and training-efficient way. We do so by generating a \ac{RTMPC} using bounds of the disturbances encountered in the deployment domain. We use properties of the tube to derive a computationally efficient \ac{DA} strategy that generates extra state-action pairs $(x^+, u^+)$, obtaining $\pi_{\hat{\theta}^*}$ via \ac{IL}. Our approach enables zero-shot transfer from a single demonstration collected in simulation (\textit{sim2real}) or a controlled environment (lab, factory, \textit{lab2real}).}
    \label{fig:sim_to_real_zero_shot_cover} %
    \vspace{-4ex}
\end{figure}
