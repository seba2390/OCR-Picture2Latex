\section{\library}


We first present the supported measures and the interface of \library. We then delve into the functionalities offered by the library that facilitate the efficient and reliable analysis of DR embeddings. 

% present the interface, then introduce the scheduling scheme of \library for fast computation.

\subsection{Supported Distortion Measures}

\label{sec:support}

The list of distortion measures to be included in the library is determined through a literature review on DR and their evaluation (\autoref{sec:relwork}). 
Different distortion measures evaluate the preservation of the data structure at varying levels of granularity (e.g., neighborhood, cluster, and global structure; \autoref{sec:relmeasure}). The simultaneous use of multiple measures having different granularity is essential for comprehensively evaluating DR embeddings \cite{jeon22vis, moor20icml, espadoto21tvcg}. 
Thus, we try to maximize both the number of supported measures and the diversity in terms of the structural granularity that the measures focus on. 
As a result, we select seven local measures, four cluster-level measures, and six global measures (\autoref{tab:measures}). Please refer to Appendix A for the detailed procedure for computing each measure. 



\subsection{Interface}

\label{sec:interface}

\library provides two different interfaces for executing distortion measures. The first is to use the main class that is named after our library (i.e., \texttt{zadu}).
In designing the main interface, our focus is on reusing both the code and the computing resources so that users can save time.
With regard to reusing code, we force users to write a specification that defines the measures to be executed (\texttt{"id"} in Code 1) along with their hyperparameters (\texttt{"params"}). By reusing the specifications, users can perform an identical evaluation on multiple datasets. This is commonly done in practice to enhance the generalizability of the evaluation \cite{jeon22vis, espadoto21tvcg, moor20icml}.
As for reusing the computing results, we require users to register the original high-dimensional dataset (\texttt{hd}) once, along with its specifications. This dataset can then be reused repeatedly.
% To use this interface, users should write a specification that defines the measures to execute (\texttt{"id"} in \autoref{code:basic}) and their hyperparameters (\texttt{"params"}). Then, users can register the specification along with the original high-dimensional data (\texttt{hd}) to the \texttt{zadu} class object. We make the original data be registered once in this stage and reused repeatedly. 
This is because the evaluation of DR is usually done by comparing multiple embeddings of a single high-dimensional dataset. 
The distortion measures can then be executed by invoking \texttt{measure} method while giving the embedding (\texttt{ld}) as an argument, which returns the scores from the distortion measures. 

\begin{lstlisting}[
    language=Python, 
    caption={Using the main class of \library  to compute the Trustworthiness \& Continuity (\texttt{tnc}) and Steadiness \& Cohesiveness (\texttt{snc}) scores of a given embedding (\texttt{ld}) based on its original data (\texttt{hd}). },
    label={code:basic}
]
from zadu import zadu

hd, ld = load_datasets()
spec = [{
    "id"    : "tnc",
    "params": { "k": 20 },
}, {
    "id"    : "snc",
    "params": { "k": 30, "clustering": "hdbscan" }
}]

scores = zadu.ZADU(spec, hd).measure(ld)
print("T&C:", scores[0])
print("S&C:", scores[1])

\end{lstlisting}



An alternative interface is to directly invoke the functions that define each distortion measure (\autoref{code:individual}).
However, executing multiple measures in this way does not take advantage of optimization (\autoref{sec:optimize}). Hence, more computation time is needed compared to using the main class (\autoref{code:basic}).

\begin{lstlisting}[
    language=Python, 
    caption={Accessing the internal functions of \library to execute Mean Relative Rank Errors and Pearson's correlation coefficient $r$.},
    label={code:individual}
]
from zadu.measures import *

mrre = mean_relative_rank_error.measure(hd, ld)
pr  = pearson_r.measure(hd, ld)
\end{lstlisting}

\subsection{Functionalities}

We outline the functionalities of \library that enable the effective evaluation and analysis of DR embeddings.

\subsubsection{Optimizing the Execution of Multiple Measures}




\label{sec:optimize}

Utilizing multiple distortion measures simultaneously is common in practice \cite{jeon22vis, moor20icml}. For example, Espadoto et al. \cite{espadoto21tvcg} proposed to aggregate multiple measures by averaging them. 
However, using more measures leads to increased computational demands.

To reduce the computation time running multiple distortion measures, \library automatically optimizes the execution of the measures. 
% We develop a scheduling scheme optimizing the execution of multiple distortion measures. 
The primary goal of the optimization is to minimize the computational overhead associated with three key preprocessing blocks: pairwise distance computation, pointwise distance ranking computation, and $k$NN identification.
The pairwise distance computation is done by constructing a distance matrix in both the original and the embedded spaces utilizing a specified distance function (e.g., Euclidean distance or cosine similarity). 
During the pointwise distance ranking computation stage, the ranking of all data points with respect to each individual data point $x$ is set based on their distance from $x$. This is also done in both the original and the embedded spaces. Lastly, $k$NN identification involves locating the top-$k$ closest data points of each point in the original and embedded spaces.

The optimization works as follows. Given a specification (refer to \autoref{sec:interface}), \library extracts a list of requisite preprocessing units. The library then establishes an execution order for the blocks while maximizing the reuse of computed results. For instance, if both the distance matrix and the $k$NN index are needed, the outcome of the former computation is reused to compute the latter. Similarly, if the specifications require the computation of both $k_1$NN and $k_2$NN, where $k_1 > k_2$, the $k_2$NN can be acquired by slicing the $k_1$NN.
Once the execution order and dependencies are ascertained, \library runs preprocessing. The preprocessing results are stored in the RAM and subsequently injected into each function that defines a distortion measure to derive the final scores. 


The effectiveness of our optimization increases as more distortion measures are executed simultaneously. We validate that the optimization substantially reduces the execution time of distortion measures through our quantitative evaluation (\autoref{sec:runtime}).


\subsubsection{Computing Pointwise Local Distortions}

\label{sec:pointwise}

\library enables users to obtain local pointwise distortions, which indicate how each point contributes to the overall distortions. 
Such functionality improves the usability of our library as local distortions help users in performing enhanced analysis of DR embeddings. For example, we can aggregate local distortions in class labels to reveal which class is vulnerable to the distortions.
Moreover, we can visualize local distortions \cite{lespinats11cgf, 
jeon21tvcg}, which facilitates a more accurate analysis of the original high-dimensional data \cite{jeon21tvcg}. We discuss this application in more detail in \autoref{sec:app}.

We can obtain local pointwise distortions by raising the \texttt{return\_local} flag. When the flag is raised, the library returns the local distortions along with the aggregated scores (See \autoref{code:distortion}). 
  


% We add \texttt{return\_local} flag to the \library, which gives the option for users to obtain local pointwise distortions. When the flag is raised, the library returns the pointwise local distortions along with the aggregated scores for available measures (See \autoref{code:distortion}). 


\begin{lstlisting}[
    language=Python, 
    caption={Obtaining local pointwise distortion from \library by raising the \texttt{return\_local} flag. If a specified distortion measure produces local pointwise distortion as an intermediate result, it returns a list of pointwise distortions when the flag is raised. },
    label={code:distortion}
]
from zadu import zadu

spec = [{
    "id"    : "dtm",
    "params": {}
}, {
    "id"    : "mrre",
    "params": { "k": 30 }
}]

zadu_obj = zadu.ZADU(spec, hd, return_local=True)
global_, local_ = zadu_obj.measure(ld)
print("MRRE local distortions:", local_[1])

\end{lstlisting}

The computation of pointwise local distortions is available only for some local measures and cluster-level measures (See ``provide pointwise distortions'' column in \autoref{tab:measures}). For example, T\&C and MRREs produce final scores as an average of local distortions. Steadiness \& Cohesiveness \cite{jeon21tvcg} computes pointwise distortion by aggregating partial cluster-level distortions. When the flag is raised, \library returns a list consisting of local pointwise distortions for the available measures; it otherwise returns \texttt{None}. 

% Such local distortions enable further analysis of high-dimensional data based on DR embeddings. For example, we can aggregate local distortions in class-label to reveal which class is vulnerable to the distortions.
% Moreover, local distortions can be visualized \cite{lespinats11cgf, 
% jeon21tvcg}, facilitating a more accurate interpretation of the original high-dimensional structure of the data. 
% We discuss this application in more detail in Sect. 4.X.



\subsection{Implementation}

\library is a Python library that can be installed via \texttt{PyPI} with just a single command. 
Scalability is a key consideration in implementing \library. We maximize the utilization of matrix computation and incorporate highly optimized open-source libraries for computationally heavy tasks (e.g., \texttt{faiss} \cite{johnson19tbd} for $k$NN identification). To simplify the installation and execution, the library runs only on CPUs. 

While implementing the measures, we reuse the previous open-source implementations if available. 
For example, for T\&C, MRRE, Stress, DTM, and KL divergence, we adopt the code provided by Jeon et al. \cite{jeon22vis} (the second last column of \autoref{tab:measures}). For Steadiness \& Cohesiveness, we use the code shared by the authors. We still revise these codes to fit our optimization pipeline (\autoref{sec:optimize}), to make them return local pointwise distortions (\autoref{sec:pointwise}), and to eliminate GPU dependencies. The remaining measures are carefully implemented by referring to the papers in which they were first introduced. The source code is available at \href{https://github.com/hj-n/zadu}{github.com/hj-n/zadu}.




% \subsubsection{Ensuring Reliable Execution of the Measures with Class Labels}

% In this section, we briefly discuss the shortcomings of distortion measures that rely on class labels as input (e.g., Neighborhood Hit, Distance Consistency; see "use class labels" column in \autoref{tab:measures}) and how \library addresses these issues. These measures exhibit a common characteristic: they do not necessitate access to the original data, but instead \textit{presume} that the labeled classes are well-clustered (i.e., mutually separated and individually condensed) in the high-dimensional data, implying good cluster-label matching \cite{jeon22arxiv2}.

% Nonetheless, such cluster-label matching assumptions are not always guaranteed \cite{aupetit14beliv, jeon22arxiv2}. It is possible that a single class comprises multiple separated clusters or multiple classes form a single condensed cluster \cite{jeon22arxiv2}. Violating these assumptions can lead to erroneous conclusions when evaluating Dimensionality Reduction (DR) techniques \cite{aupetit14beliv}.

% To address this, we introduce an alert system in \library that evaluates the validity of the cluster-label matching assumption and issues a warning if the assumption is found to be violated. This is achieved by employing the between-dataset Calinski-Harabasz index ($CH_{btwn}$), a function specifically designed to assess the degree of cluster-label matching across datasets in an unbiased manner \cite{jeon22arxiv2}. The alert system generates a warning if the $CH_{btwn}$ score of a given high-dimensional dataset falls below a predetermined threshold. This threshold is established based on the $CH_{btwn}$ scores of 96 labeled datasets, as computed in a prior study \cite{jeon22arxiv2} (for details, refer to Appendix XX). By incorporating this alert system, users are made aware of potential cluster-label matching assumption violations, enabling more accurate evaluations of DR embeddings.




