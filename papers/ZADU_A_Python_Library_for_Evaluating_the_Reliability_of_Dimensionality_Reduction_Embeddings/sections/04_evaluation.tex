
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/vis.pdf}
    \vspace{-6mm}
    \caption{
    The UMAP embedding of the MNIST dataset (leftmost column), and two distortion visualizations generated by \vislib: CheckViz \cite{lespinats11cgf} and the Reliability Map \cite{jeon21tvcg}. 
    The distortion visualizations depict how each region of the given embedding suffers from the distortions that lower the Steadiness \& Cohesiveness (S\&C) scores. The visualizations follow the 2D colormap proposed by Lespinats and Aupetit \cite{lespinats11cgf} (rightmost column).
    Combined with \library, \vislib helps practitioners easily generate distortion visualizations on a \texttt{matplotlib} canvas. \vspace{-4mm}}
    \label{fig:distvis}
\end{figure*}


\section{Runtime Analysis}

\label{sec:runtime}

\subsection{Objectives and Design}

We test whether our optimization pipeline (\autoref{sec:optimize}) reduces the time needed to evaluate DR embeddings.
% We want to compare how the execution time of such optimization varies between using \library with and without scheduling. 
We simulate a scenario in which we try to optimize the hyperparameters of a DR technique using multiple distortion measures that have common preprocessing blocks.
We evaluate the running time for optimization with and without the optimization. 
We use datasets with diverse characteristics, e.g., the number of points and dimensionality. We compare how the running time of evaluation differs on average as we switch on the optimization. 

% We execute the scenario using \library with and without scheduling on datasets with diverse characteristics (e.g., number of points, dimensionality) and compare the runtime. 

\noindent
\textbf{Optimization}
For a given dataset, we measure the time required to run Bayesian optimization \cite{snoek12nips} for finding the optimal value of two hyperparameters in UMAP \cite{mcinnes2020arxiv}: \texttt{nearest neighbors} and \texttt{minimum distance} \cite{mcinnes2020arxiv}. The search range of two hyperparameters is set as (2, 200) and (0.01, 0.99), respectively, following the recommendation of the official documentation\footnote{\href{https://umap-learn.readthedocs.io/en/latest/index.html}{umap-learn.readthedocs.io}}.
For Bayesian optimization, we use the Python implementation of Nogueira \cite{nogueira14github} with the default hyperparameter setting.

\noindent
\textbf{Distortion measures}
For the distortion measures, we use T\&C, MRRE, Steadiness \& Cohesiveness, Distance-to-Measure, and Kullback-Leibler divergence. All the measures share pairwise distance matrix computation as a common preprocessing block. The first three measures also share $k$NN identification. 
As a loss function, we use an average of five measures, following Espadoto et al. \cite{espadoto21tvcg}.

\noindent
\textbf{Datasets}
We apply the optimization to the 96 publicly available high-dimensional datasets gathered by a previous study \cite{jeon22arxiv2}. Every dataset is standardized before applying the optimization process. 


\subsubsection{Results} 


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/exp.pdf}
    \vspace{-7mm}
    \caption{    
    The results of the runtime analysis (\autoref{sec:runtime}). (left) \library{}'s optimization substantially reduces the runtime for the optimization of DR embeddings. (right) The extent to which the optimization reduces the runtime increases as the size of the datasets increases. The shaded area in the right figure depicts the 95\% confidence interval. \vspace{-4mm}}
    \label{fig:runtime}
\end{figure}



\autoref{fig:runtime} depicts the result. \library is 1.5 times faster with optimization than without it on average, verifying the effectiveness of the optimization pipeline. We also discover that the difference in runtime between with and without optimization increases as the number of points in the dataset increases (as indicated by the steeper orange regression line in \autoref{fig:runtime}b compared to the blue regression line). This finding further supports the scalability benefits of \library. Overall, our results demonstrate that \library substantially reduces the time required for practitioners to evaluate DR embeddings. 


% \subsubsection{Find the best Dimensionality Reduction Method across several measures}

% We evaluated the effectiveness of ZADU with two scenarios for evaluation of DR with multiple metrics. ZADU have strength of evaluating multiple metric calculation which have some parts of calculation in common.
% Scenario 1. Re-use distance matrix for global metric and kNN computation part of local metric.
% Distance matrix is important to determine how close data points similar in high-dimensional space.


% % Distance matrix for global + kNN of local, cluster
% % kNN for local + cluster


% \subsubsection{Optimize parameter of some measure}
% e.g., k of T&C
% GPGPU 논문 참조

