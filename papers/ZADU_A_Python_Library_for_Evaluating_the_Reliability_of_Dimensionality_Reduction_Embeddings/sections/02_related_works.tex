\definecolor{lightred}{RGB}{247, 163, 180}
\definecolor{lightlightred}{RGB}{245, 208, 216}

\newcommand{\lred}{\cellcolor{lightred}}
\newcommand{\llred}{\cellcolor{lightlightred}}

\begin{table*}[h]
  
	\centering%
 \scalebox{0.81}{
  \begin{tabular}{%
	rrcc|c *{14}{c}%
	}
  \toprule
   Type & Measure & Ref.  & \rotatebox{90}{provide pointwise}  \rotatebox{90}{distortions}  & \rotatebox{90}{dreval \cite{soneson22r}} & \rotatebox{90}{McInnes et al. \cite{mcinnes2020arxiv}} &\rotatebox{90}{Ingram et al. \cite{ingram15neurocomputing}} &   \rotatebox{90}{Jeon et al. \cite{jeon21tvcg}} &   \rotatebox{90}{Fujiwara et al. \cite{fujiwara23pvis}} & \rotatebox{90}{Espadoto et al. \cite{espadoto21tvcg}} & \rotatebox{90}{Colange et al. \cite{colange20neurips}} &   \rotatebox{90}{coranking \cite{kraemer18rj}} & \rotatebox{90}{pyclustering \cite{novikov19oss}} &  \rotatebox{90}{scikit-learn \cite{pedregosa11jmlr}}  &   \rotatebox{90}{scipy \cite{virtanen20nature}}  &   \rotatebox{90}{Moor et al. \cite{moor20icml}} &   \rotatebox{90}{Jeon et al. \cite{jeon22vis}}  & \rotatebox{90}{\textbf{ZADU} (Ours)}   \\
  \hline
  \multirow{7}{*}{Local} & Trustworthiness \& Continuity &\cite{venna06nn}  & $\vee$ & \llred \footnotesize{$\bigtriangleup$} & \llred \footnotesize{$\bigtriangleup$} & & & & & \lred \footnotesize{$\bigcirc$} & \lred \footnotesize{$\bigcirc$} & &\llred \footnotesize{$\bigtriangleup$} & & \lred \footnotesize{$\bigcirc$}& \lred \footnotesize{$\bigcirc$}  & \lred \footnotesize{$\bigcirc$} \\
                         & Mean Relative Rank Errors &\cite{lee09neurocomputing}  & $\vee$ & & & & & & & & & & &  & \lred \footnotesize{$\bigcirc$} & \lred \footnotesize{$\bigcirc$}  & \lred \footnotesize{$\bigcirc$} \\   
                         & Local Continuity Meta-Criteria &\cite{chen09jasa}  & $\vee$ & & & & & & & & \lred \footnotesize{$\bigcirc$} & & & & & & \lred \footnotesize{$\bigcirc$} \\   
                         & Neighborhood Hit& \cite{paulovich08tvcg}   & $\vee$ & & & \lred \footnotesize{$\bigcirc$} & & & & & & & & & &  & \lred \footnotesize{$\bigcirc$}\\   
                         & Neighbor Dissimilarity& \cite{fujiwara23pvis}  & & & & & & \lred \footnotesize{$\bigcirc$} & & & & & & & & & \lred \footnotesize{$\bigcirc$} \\   
                         & Class-Aware Trustworthiness \& Continuity &\cite{colange20neurips}  & $\vee$ & & & & & & &\lred \footnotesize{$\bigcirc$}  & & & & & &  & \lred \footnotesize{$\bigcirc$}\\   
                         & Procrustes Measure &\cite{goldberg2009local}  & & & & & & & & & & & & & & & \lred \footnotesize{$\bigcirc$} \\   
  \hline
  \multirow{4}{*}{Cluster-level} & Steadiness \& Cohesiveness& \cite{jeon21tvcg} & $\vee$& & & & \lred \footnotesize{$\bigcirc$} &\lred \footnotesize{$\bigcirc$} & & & & & & & & & \lred \footnotesize{$\bigcirc$}\\
                                 & Distance Consistency& \cite{sips09cgf} & & & & & & & & & & & & & & & \lred \footnotesize{$\bigcirc$} \\
                                 & Internal Clustering Validation Measures &\cite{joia11tvcg}&  & &&  & & & & & & \lred \footnotesize{$\bigcirc$} & \lred \footnotesize{$\bigcirc$}& & & & \lred \footnotesize{$\bigcirc$} \\
                                 & Clustering + External Clustering Validation Measures & \cite{xiang21fig} & & &&  & & & & & & \lred \footnotesize{$\bigcirc$} &\lred \footnotesize{$\bigcirc$} & & & & \lred \footnotesize{$\bigcirc$} \\
  \hline
  \multirow{6}{*}{Global}  & Stress &\cite{kruskal64psycho, kruskal1964nonmetric}  & & & & & & & \lred \footnotesize{$\bigcirc$} & & & & & & \lred \footnotesize{$\bigcirc$} & \lred \footnotesize{$\bigcirc$} & \lred \footnotesize{$\bigcirc$}\\
                          &  Kullback-Leibler Divergence &\cite{hinton02nips} & & & & & & & & & & & & &\lred \footnotesize{$\bigcirc$} &\lred \footnotesize{$\bigcirc$} & \lred \footnotesize{$\bigcirc$} \\  
                          & Distance-to-Measure &\cite{chazal11fcm} & & & & & & & & & & & & & & \lred \footnotesize{$\bigcirc$} & \lred \footnotesize{$\bigcirc$} \\
                          & Topographic Product & \cite{bauer1992quantifying}  & & & & & & & & & & & & & & &\lred \footnotesize{$\bigcirc$} \\
                          & Pearson's correlation coefficient $r$ &\cite{geng2005supervised} &  & & & & & & & & & & & \lred \footnotesize{$\bigcirc$} & & & \lred \footnotesize{$\bigcirc$}\\
                          & Spearman's rank correlation coefficient $\rho$ & \cite{sidney1957nonparametric}  & & & & & & & & & & & & \lred \footnotesize{$\bigcirc$} & & &\lred \footnotesize{$\bigcirc$}  \\

  \bottomrule
  \end{tabular}%
}
\vspace{2mm}
    \caption{The overview of the distortion measures provided by \library (row) and their publicly available implementations (column). The publicized implementations that fully implement the corresponding measures are highlighted in red background and circle. The ones that implement only half of the pair of measures are highlighted in light red background and triangle. \vspace{-4mm}}
  \label{tab:measures}
\end{table*}




\section{Background and Related Work}

\label{sec:relwork}

We discuss the literature associated with distortion measures. We then review the publicly available implementations of the measures.

\subsection{Distortion Measures}

\label{sec:relmeasure}

Distortion measures are functions that accept a high-dimensional data $\mathbf{X} = \{x_i \in \mathbb{R}^D \mid 1 \leq i \leq N \}$ and its low-dimensional embedding $\mathbf{Y} = \{y_i \in \mathbb{R}^d \mid 1 \leq i \leq N \}$ ($d < D$) as input, and then return a score that represents how well the structure of $\mathbf{Y}$ matches that of $\mathbf{X}$. The measures are either developed as a loss function of a DR technique \cite{hinton02nips, colange20neurips} or developed originally, independent of any technique \cite{jeon21tvcg, lee07springer}. 

Distortion measures can be broadly divided into three categories---local measures, global measures, and cluster-level measures---based on their target structural granularity \cite{jeon21tvcg}. 
Local measures evaluate the extent to which the neighborhood structure of $\mathbf{X}$ is preserved in $\mathbf{Y}$. For example, Trustworthiness \& Continuity (T\&C) \cite{venna06nn} and  Mean relative rank error (MRRE) \cite{lee09neurocomputing} assess the degree to which the $k$-nearest neighbors ($k$NN) of each point in $\mathbf{X}$ are no longer neighbors in the $\mathbf{Y}$, and vice versa. Neighborhood Dissimilarity \cite{fujiwara23pvis} measures the level to which the Shared-Nearest Neighbor \cite{ertoz02siam} graph structure is different in $\mathbf{X}$ and $\mathbf{Y}$. 
Next, cluster-level measures evaluate how well the cluster structures of $\mathbf{X}$ are preserved in $\mathbf{Y}$. The cluster is given by clustering algorithms \cite{jeon21tvcg} or class labels \cite{ joia11tvcg}.
Finally, global measures evaluate the extent to which point-pairwise distances remain consistent. For instance, Pearson's correlation coefficient $r$ quantifies how the ranking of point pairs based on their distances varies between $\mathbf{X}$ and $\mathbf{Y}$. 


As diverse DR techniques emphasize different facets of data, employing multiple distortion metrics with varying granularity levels is crucial for the comprehensive evaluation of DR embeddings. Therefore, while designing \library, we try not only to maximize the number of supported distortion measures but also to have an even distribution of all types of measures (\autoref{tab:measures}, \autoref{sec:interface}). 

% REUSE LATER
% \noindent
% \textbf{Distortion measures with the cluster-label matching assumption }
% Some distortion measures rely on the assumption that labeled classes form individually condensed and mutually separated clusters in the original space, which we call cluster-label matching (CLM) assumption \cite{jeon22arxiv2}. 
% Based on the assumption, these measures use the extent to which classes are well clustered in the embedding as a proxy for embedding quality. For example, Distance Consistency (DSC) \cite{sips09cgf} measures the amount of overlap between classes by examining how well classes form compact clusters around their centroids. 

% However, 

% measures the amount of overlap of classes in the embedding space. Neighborhood Hit (NH) 


\subsection{Implementations of Distortion Measures}

Despite the importance of reliability evaluations when utilizing DR, there is a lack of a unified implementation that provides distortion measures. The majority of implementations is in publicly accessible repositories contributed by the studies on DR \cite{mcinnes2020arxiv, fujiwara23pvis, cockburn09cs, moor20icml, jeon22vis}. However, each implementation has a limited number of supported distortion measures (\autoref{tab:measures}). Moreover, installing, compiling, and executing from such scattered code is time-consuming.


An alternative way is to use the distortion measures provided by popular machine learning libraries (e.g., \texttt{scikit-learn} \cite{pedregosa11jmlr}). These libraries are easy to install and execute, and also likely to be highly optimized. However, as general-purpose machine learning toolboxes, they offer limited support for distortion measures (\autoref{tab:measures}). We aim to develop a library that (1) is easily downloadable and executable, similar to the widely-used machine learning libraries, while (2) supporting a broader range of distortion measures.





