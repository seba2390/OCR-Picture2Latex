%\title{ijcnlp 2017 instructions}
% File ijcnlp2017.tex
%

\documentclass[11pt,letterpaper]{article}
\usepackage{ijcnlp2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}

% Uncomment this line for the final submission:
\ijcnlpfinalcopy

%  Enter the IJCNLP Paper ID here:
%\def\ijcnlppaperid{1218}

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}


\title{Speaker Role Contextual Modeling for Language Understanding\\ and Dialogue Policy Learning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in
\author{Ta-Chung Chi$^\star$\quad Po-Chun Chen$^\star$\quad Shang-Yu Su$^\dagger$\quad Yun-Nung Chen$^\star$\\
%\thanks{The first three authors have equal contributions.}\\
$^\star$Department of Computer Science and Information Engineering\\
$^\dagger$Graduate Institute of Electrical Engineering\\
National Taiwan University\\
\texttt{ \{b02902019,r06922028,r05921117\}@ntu.edu.tw\quad y.v.chen@ieee.org}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Language understanding (LU) and dialogue policy learning are two essential components in conversational systems.
Human-human dialogues are not well-controlled and often random and unpredictable due to their own goals and speaking habits.
This paper proposes a role-based contextual model to consider different speaker roles independently based on the various speaking patterns in the multi-turn dialogues.
The experiments on the benchmark dataset show that the proposed role-based model successfully learns role-specific behavioral patterns for contextual encoding and then significantly improves language understanding and dialogue policy learning tasks\footnote{The source code is available at: \url{https://github.com/MiuLab/Spk-Dialogue}.}.
%and achieves improvement for understanding and policy learning.
%leveraging the role information in dialogue contexts 
%to better understand the current sentence and predict the next system action as the policy.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Spoken dialogue systems that can help users to solve complex tasks such as booking a movie ticket become an emerging research topic in the artificial intelligence and natural language processing area. 
With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. 
Today, there are several virtual intelligent assistants, such as Apple's Siri, Google's Home, Microsoft's Cortana, and Amazon's Echo. Recent advance of deep learning has inspired many applications of neural models to dialogue systems. \newcite{wen2016network}, \newcite{bordes2017learning}, and \newcite{li2017end} introduced network-based end-to-end trainable task-oriented dialogue systems.

A key component of the understanding system is a language understanding (LU) module---it parses user utterances into semantic frames that capture the core meaning, where three main tasks of LU are domain classification, intent determination, and slot filling~\cite{tur2011spoken}.
A typical pipeline of LU is to first decide the domain given the input utterance, and based on the domain, to predict the intent and to fill associated slots corresponding to a domain-specific semantic template.
%Traditionally, domain detection and intent prediction are framed as classification problems, where several classifiers such as support vector machines and maximum entropy are employed~\cite{haffner2003optimizing,chelba2003speech};
%The classic language understanding (LU) module includes domain classification, intent prediction, and slot filling, 
%Several classification models such as maximum entropy and support vector machines with linear kernel are applied to domain detection and user intent  prediction~\cite{haffner2003optimizing,chelba2003speech};
%traditional structural learning methods are widely applied in slot filling tasks, such as hidden Markov models (HMMs) and conditional random field (CRF) ~\cite{wang2005spoken,lafferty2001conditional,raymond2007generative}.
%However, these methods highly relied on laborious and careful feature engineering. 
Recent advance of deep learning has inspired many applications of neural models to natural language processing tasks. With the power of deep learning, there are emerging better approaches of LU~\cite{hakkani2016multi,chen2016knowledge,chen2016syntax,wang2016learning}.
However, most of above work focused on single-turn interactions, where each utterance is treated independently.


\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{DSTC_example.pdf}
\vspace{-3mm}
\caption{The human-human conversational utterances and their associated semantics from DSTC4.}
%\vspace{-3mm}
\label{fig:example}
\end{figure*}

The contextual information has been shown useful for LU~\cite{bhargava2013easy,xu2014contextual,chen2015leveraging,sun2016an}.
For example, the Figure~\ref{fig:example} shows conversational utterances, where the intent of the highlighted tourist utterance is to ask about location information, but it is difficult to understand without contexts.
Hence, it is more likely to estimate the location-related intent given the contextual utterance about location recommendation.
%\newcite{bhargava2013easy} incorporated the information from previous intra-session utterances into the LU tasks on a given utterance by applying SVM-HMMs to sequence tagging and obtained the improvement.
Contextual information has been incorporated into the recurrent neural network (RNN) for improved domain classification, intent prediction, and slot filling~\cite{xu2014contextual,shi2015contextual,weston2015memory,chen2016end}.
The LU output is semantic representations of users' behaviors, and then flows to the downstream dialogue management component in order to decide which action the system should take next, as called \emph{dialogue policy}.
It is intuitive that better understanding could improve the dialogue policy learning, so that the dialogue management can be further boosted through interactions~\cite{li2017end}.
%Most successful approaches cast the dialogue management task as a partially observable Markov decision process~\cite{young2013pomdp} to achieve high performance by leveraging various reinforcement learning methods.
%However, these methods require a large annotated dataset~\cite{young2013pomdp},  human interaction~\cite{gavsic2011line,wen2016network}, or interaction with rule-based user simulator~\cite{li2017end}; these constraints make it hard to put these methods into practice.

%However, prior work did not consider different speaker roles of the history utterances.
Most of previous dialogue systems did not take speaker roles into consideration.
However, we discover that different speaker roles can cause notable variance in speaking habits and later affect the system performance differently~\cite{chen2017dynamic}. 
From Figure~\ref{fig:example}, the benchmark dialogue dataset, Dialogue State Tracking Challenge 4 (DSTC4)~\cite{kim2016fourth}\footnote{\url{http://www.colips.org/workshop/dstc4/}}, contains two specific roles, a tourist and a guide.
Under the scenario of dialogue systems and the communication patterns, we take the tourist as a user and the guide as the dialogue agent (system).
During conversations, the user may focus on not only \emph{reasoning (user history)} but also \emph{listening (agent history)}, so different speaker roles could provide various cues for better understanding and policy learning.

This paper focuses on LU and dialogue policy learning, which targets the understanding of tourist's natural language (LU; language understanding) and the prediction of how the system should respond (SAP; system action prediction) respectively.
In order to comprehend what the tourist is talking about and predict how the guide reacts to the user, this work proposes a role-based contextual model by modeling role-specific contexts differently for improving system performance.

% Figure
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{framework.pdf}
\vspace{-3mm}
\caption{Illustration of the proposed role-based contextual model.}
%\vspace{-3mm}
\label{fig:model}
\end{figure*}

\section{Proposed Approach}
\label{sec:model}

The model architecture is illustrated in Figure~\ref{fig:model}.
First, the previous utterances are fed into the contextual model to encode into the history summary, and then the summary vector and the current utterance are integrated for helping LU and dialogue policy learning.
The whole model is trained in an end-to-end fashion, where the history summary vector is automatically learned based on two downstream tasks.
The objective of the proposed model is to optimize the conditional probability $p(\mathbf{\hat{y}}\mid \mathbf{x})$, so that the difference between the predicted distribution $q(\hat{y_k}=z\mid \mathbf{x})$ and the target distribution $q(y_k=z\mid \mathbf{x})$ can be minimized:
%The objective function is to minimize the predicted errors:
\begin{equation}
\mathcal{L}=-\sum_{k=1}^{K}\sum_{z=1}^{N}q(y_k=z\mid \mathbf{x}) \log p(\hat{y_k}=z\mid \mathbf{x}),
\end{equation}
where the labels $\textbf{y}$ can be either intent tags for understanding or system actions for dialogue policy learning.

\paragraph{Language Understanding (LU)}
Given the current utterance $\textbf{x}=\{w_t\}^T_1$, the goal is to predict the user intents of $\textbf{x}$, which includes the speech acts and associated attributes shown in Figure~\ref{fig:example}; for example, \texttt{QST\_WHAT} is composed of the speech act \texttt{QST} and the associated attribute \texttt{WHAT}.
Note that we do not process the slot filling task for extracting \texttt{LOC}.
We apply a bidirectional long short-term memory (BLSTM) model~\cite{schuster1997bidirectional} to integrate preceding and following words to learn the probability distribution of the user intents.
\begin{eqnarray}
\label{eq:basic}
\textbf{v}_\text{cur} &=& \text{BLSTM}(\textbf{x}, W_\text{his}\cdot \textbf{v}_\text{his}),\\
\textbf{o} &=& \mathtt{sigmoid}(W_\text{LU}\cdot \textbf{v}_\text{cur}),
\end{eqnarray}
where $W_\text{his}$ is a dense matrix and $\textbf{v}_\text{his}$ is the history summary vector, $\textbf{v}_\text{cur}$ is the context-aware vector of the current utterance encoded by the BLSTM, and $\textbf{o}$ is the intent distribution.
Note that this is a multi-label and multi-class classification, so the $\mathtt{sigmoid}$ function is employed for modeling the distribution after a dense layer.
The user intent labels $\textbf{y}$ are decided based on whether the value is higher than a threshold $\theta$.
% the mission is finding the correct user intents and corresponding attributes.
%The entry of the final layer output $\hat{o_t}$ is picked if and only if its value is higher than a threshold:
%\begin{gather*}
%\hat{y_{t, k}} =
%  \begin{cases}
%    1 & \text{if } \hat{o}_{t, k} > \mathtt{threshold}. \\
%    0 & \text{otherwise}.\\
%  \end{cases}
%\end{gather*}

\paragraph{Dialogue Policy Learning}
For system action prediction, we also perform similar multi-label multi-class classification on the context-aware vector $\textbf{v}_\text{cur}$ from (\ref{eq:basic}) using $\mathtt{sigmoid}$:
\begin{equation}
\textbf{o} = \mathtt{sigmoid}(W_\pi \cdot \textbf{v}_\text{cur}),
\end{equation}
%\begin{eqnarray}
%\textbf{o} &=& \mathtt{softmax}(W_\pi \cdot \textbf{v}_\text{cur}),\\
%y &=& \arg\max (\textbf{o}).
%\end{eqnarray}
and then the system actions can be decided based on a threshold $\theta$.


\subsection{Contextual Module}
\label{ssec:contexualmodel}

In order to leverage the contextual information, we utilize two types of contexts: 1) semantic labels and 2) natural language, to learn history summary representations, $\textbf{v}_\text{his}$ in (\ref{eq:basic}).
The illustration is shown in the top-right part of Figure~\ref{fig:model}.

\paragraph{Semantic Label}
%In order to model the history using explicit semantics, the annotated semantic labels including the intents and associated attributes from history utterances may 
Given a sequence of annotated intent tags and associated attributes for each history utterance, we employ a BLSTM to model the explicit semantics:
\begin{equation}
\textbf{v}_\text{his} = \text{BLSTM}(\text{intent}_t),
\label{eq:tag}
\end{equation}
where $\text{intent}_t$ is the vector after one-hot encoding for representing the annotated intent and the attribute features.
Note that this model requires the ground truth annotations of history utterances for training and testing.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{CNN.pdf}
\vspace{-3mm}
\caption{Illustration of the CNN sentence encoder for the example sentence ``\textit{what's the song in the sea}''.}
%\vspace{-3mm}
\label{fig:cnn}
\end{figure}

\paragraph{Natural Language (NL)}
Given the natural language history, a sentence encoder is applied to learn a vector representation for each prior utterance.
%\begin{equation*}
%\text{CNN}(\text{utt}_t)
%\end{equation*}
After encoding, the feature vectors are fed into a BLSTM to capture temporal information:
\begin{equation}
\textbf{v}_\text{his} = \text{BLSTM}(\text{CNN}(\text{utt}_t)),
\label{eq:nl}
\end{equation}
where the CNN is good at extracting the most salient features that can represent the given natural language utterances illustrated in Figure~\ref{fig:cnn}.
Here the sentence encoder can be replaced into different encoders\footnote{In the experiments, CNN achieved slightly better performance with fewer parameters compared with BLSTM.}, and the weights of all encoders are tied together.

\paragraph{NL with Intermediate Guidance}
Considering that the semantic labels may provide rich cues, the middle supervision signal is utilized as intermediate guidance for the sentence encoding module in order to guide them to project from input utterances to a more meaningful feature space.
Specifically, for each utterance, we compute the cross entropy loss between the encoder outputs and corresponding intent-attributes shown in Figure~\ref{fig:model}.
Assuming that $l_t$ is the encoding loss for $\text{utt}_t$ in the history, the final objective is to minimize $(\mathcal{L}+\sum_t{l_t})$.
This model does not require the ground truth semantics for history when testing, so that it is more practical compared to the above model using semantic labels.

\subsection{Speaker Role Modeling}
\label{ssec:rolebasedmodel}
%Some previous work stored multiple history utterances to leverage the knowledge in contexts~\cite{chen2016end,yang2017end}, we follow this idea and extend it to model different speaker roles separately.

In a dialogue, there are at least two roles communicating with each other, each individual has his/her own goal and speaking habit.
For example, the tourists have their own desired touring goals and the guides are try to provide the sufficient touring information for suggestions and assistance.
Prior work usually ignored the speaker role information or only modeled a single speaker's history for various tasks~\cite{chen2016end,yang2017end}.
The performance may be degraded due to the possibly unstable and noisy input feature space.
%However, previous methods usually simply concatenate utterances of different individuals to model dialogue contexts for various tasks (eg. LU). 
%We envisage this would cause significant affect to performance due to noisy input feature space. 
To address this issue, this work proposes the role-based contextual model: instead of using only a single BLSTM model for the history, we construct one individual contextual module for each speaker role.
Each role-dependent recurrent unit $\text{BLSTM}_{\text{role}_x}$ receives corresponding inputs $\text{x}_{i,\text{role}_x}$ ($i=[1, ..., N]$), which have been processed by an encoder model, we can rewrite (\ref{eq:tag}) and (\ref{eq:nl}) into (\ref{eq:tag2}) and (\ref{eq:nl2}) respectively:
\begin{eqnarray}
\label{eq:tag2}
\textbf{v}_\text{his} &=& \text{BLSTM}_{\text{role}_a}(\text{intent}_{t,\text{role}_a}) \\
&+& \text{BLSTM}_{\text{role}_b}(\text{intent}_{t,\text{role}_b}).\nonumber\\
\label{eq:nl2}
\textbf{v}_\text{his} &=& \text{BLSTM}_{\text{role}_a}(\text{CNN}(\text{utt}_{t,\text{role}_a}))\\\nonumber
&+& \text{BLSTM}_{\text{role}_b}(\text{CNN}(\text{utt}_{t,\text{role}_b}))
\end{eqnarray}
Therefore, each role-based contextual module focuses on modeling the role-dependent goal and speaking style, and $\textbf{v}_\text{cur}$ from (\ref{eq:basic}) is able to carry role-based contextual information.


%%%%%%%%%%% start comment %%%%%%%%%%%%
\iffalse

Considering a simple recurrent unit $\mathcal{H}$, which compute a sequence of hidden vectors $\mathbf{h}=\left\{ h_t\right\}^T_1$ and then output a sequence of vectors $\mathbf{\hat{y_t}}=\left\{\hat{y_t}\right\}^T_1$ by a  given sequence of input vectors $\mathbf{x}=\left\{ x_t\right\}^T_1$ by the following equations:
\begin{gather*}
h_t = \mathcal{H}(x_t, h_{t-1}) = \sigma(W_{xh}(x_t + h_{t-1})) \\
\hat{y_t} = arg max (\mathtt{softmax}(W_{hy}h_t)
\end{gather*}
where $W_{xh}$ and $W_{hy}$ are weight matrices, $\sigma$ can be any activation function.
The goal of a sequence-prediction model is to optimize the conditional probability $p(\mathbf{\hat{y_t}}|\mathbf{x})=\prod^T_{t=1}p(\hat{y_t}=z|\mathbf{x})$ so that the difference between prediction distribution and target distribution $q(y_t=z|\mathbf{x})$ can be minimized. Therefore we can cast the loss function to cross entropy loss:
\begin{equation*}
\mathtt{loss} = -\sum_{t=1}^{T}\sum_{z=1}^{N}q(y_t=z|\mathbf{x}) \mathrm{log} p(\hat{y_t}=z|\mathbf{x})
\end{equation*}
where N represent the number of output labels. In this work, we chose Long Short-Term Memory (LSTM) cell as the recurrent units because of its ability of handling problems of gradient vanishing and exploding and other good characteristics~\cite{}.
%\begin{equation}
%h_t = \mathcal{H}(x_t, h_{t-1}).
%\end{equation}
%Then the mechanism of the recurrent unit $\mathcal{H}$ can be expressed as follows:
%\begin{gather*}
%h_t = \mathcal{H}(x_t, h_{t-1}) = o_t \odot
%\mathrm{tanh}(c_t) \\
%c_t = f_t \odot c_{t-1} + i_t \odot g_t \\
%o_t = \mathrm{sigmoid}(W_{xo}(x_t + h_{t-1})) \\
%i_t = \mathrm{sigmoid}(W_{xi}(x_t + h_{t-1})) \\
%f_t = \mathrm{sigmoid}(W_{xf}(x_t + h_{t-1}])) \\
%g_t = \mathrm{tanh}(W_{xg}(x_t + h_{t-1}]))
%\end{gather*}
%where $o_t$, $i_t$, $f_t$ represent output-gate signal, input-gate signal, and forget-gate signal respectively. The sigmoid function $\mathrm{sigmoid}$ and hyperbolic tangent function $\mathrm{tanh}$ are applied element-wise, $\odot$ denotes element-wise product.
In order to enhance the ability of capturing contextual information, we extend our module to a bidirectional LSTM cells~\cite{schuster1997bidirectional}.
%By means of the bidirectional structure, the input sequence $\mathbf{x}$  and its reverse go through the LSTM layer separately, 
Therefore the model can capture richer information in preceding and following lexical contexts. 
%The result is then computed by the concatenation of forward signal $\overrightarrow{h}_t$ and backward signal $\overleftarrow{h}_t$.
%\begin{equation}
%\overrightarrow{h}_t = \mathcal{H}(x_t, \overrightarrow{h}_{t-1}), \overleftarrow{h}_t = \mathcal{H}(x_t, \overleftarrow{h}_{t-1})
%\end{equation}
\begin{equation}
\hat{y_t} = \arg\max (\mathtt{softmax}( \overrightarrow{W}_{hy}\overrightarrow{h}_t + \overleftarrow{W}_{hy}\overleftarrow{h}_t))
\end{equation}
where $\overrightarrow{W}_{hy}$ and $\overleftarrow{W}_{hy}$ are weight matrices of bi-directions.
Some of the previous works store multiple history utterances to leverage the knowledge in context ~\cite{chen2016end,yang2017end}, we follow this idea and extend it to store the utterances from different speaker separately.
In our experiments, we have tried both intent-attribute frame and raw natural language. In natural language case, each utterance $\mathbf{x}_i=\left\{ x_t\right\}^T_1$ (i from 1 to N) go through a encoder model to transform into a latent vector; then the temporal information is captured by a recurrent unit $\mathcal{H}$.

\subsection{Role-based Model}
\label{ssec:rolebasedmodel}
In a dialogue, there are at least two roles communicate with each other, each individual has quite different speaking habits. However, previous methods usually simply concatenate utterances of different individuals to model dialogue contexts for various tasks (eg. LU). We envisage this would cause significant affect to performance due to noisy input feature space. To address this problem, we propose role-based model concept; instead of using only one bidirectional LSTM model, we utilize one individual bidirectional LSTM module for each speaker role. Each role-dependent recurrent unit $\mathcal{H}_{role_x}$ receive corresponding inputs $x_{i,role_x}$ (i from 1 to N) which have been processed by an encoder model, we can rewrite $(1)$ to below:
\begin{gather*}
\overrightarrow{h}_{i,role_x} = \mathcal{H}_{role_x}(x_{i,role_x}, \overrightarrow{h}_{i-1, role_x}) \\ 
\overleftarrow{h}_{i, role_x} = \mathcal{H}_{role_x}(x_{i,role_x}, \overleftarrow{h}_{i-1, role_x})
\end{gather*}

In dialogue systems, the function of dialogue policy (also known as dialogue management) is to decide which action to perform next regarding the current dialogue state, while in speech act prediction (SAP) task of DSTC series, the goal is to predict the next action of a speaker by information in previous turns.
After the history going through the recurrent units $\mathcal{H}_{role_x}$, th latent vector are concatenated and sent into a simple dense layer:
\begin{multline*}
\hat{p_t} = \mathtt{sigmoid}(W_{s}( \\
(\overrightarrow{W}_{hy,role_a}\overrightarrow{h}_{t,role_a}  
+\overleftarrow{W}_{hy,role_a}\overleftarrow{h}_{t,role_a}) + \\ 
(\overrightarrow{W}_{hy,role_b}\overrightarrow{h}_{t,role_b} + \overleftarrow{W}_{hy,role_b}\overleftarrow{h}_{t,role_b}))
\end{multline*}
For action prediction, we perform classification on this layer:
\begin{equation*}
\hat{y_t} = \arg\max (\mathtt{softmax}(\hat{p_t}))
\end{equation*}
The history summary $\hat{p_t}$ is then combined with the current utterance and flow to the downstream recurrent unit followed by a dense layer. Unlike action prediction, language understanding is a multi-label classification problem, in this paper we focus on intent prediction in language understanding, the mission is finding the correct user intents and corresponding attributes. The entry of the final layer output $\hat{o_t}$ is picked if and only if its value is higher than a threshold:
\begin{gather*}
\hat{y_{t, k}} =
  \begin{cases}
    1 & \text{if } \hat{o}_{t, k} > \mathtt{threshold}. \\
    0 & \text{otherwise}.\\
  \end{cases}
\end{gather*}

\begin{table*}
%\small
\centering
\begin{tabular}{ | l l | c | c | c | }
    \hline
      & \bf Model & \bf Understanding & \bf Policy Learning\\ \hline\hline
    Baseline & \emph{DSTC4-Best} & 52.1~ & -\\
    & BLSTM & 62.6~ & 62.4~~\\\hline
    Contextual-Sem & BLSTM & 64.1~ & 64.4~~\\
    & ~+ Role-Based & \bf 67.3$^\dag$ & \bf 74.3$^\dag$\\\hline
    Contextual-NL & BLSTM & 62.1~ & 64.0~~\\
    & ~+ Role-Based  & 64.4$^\dag$ & 65.3$^\dag$\\
     & ~+ Role-Based w/ Intermediate Guidance & \bf 65.7$^\dag$ & \bf 67.2$^\dag$\\
    \hline
  \end{tabular}
  \vspace{-2mm}
\caption{Language understanding and policy learning performance on DSTC4 (\%). $^\dag$ indicates the significant improvement compared to all methods without speaker role modeling.}
\label{tab:res}
\vspace{-4mm}
\end{table*}
\fi
%%%%%%%%%%% end comment %%%%%%%%%%%


\begin{table*}
%\small
\centering
\begin{tabular}{ |l  l l | c | c | c | }
    \hline
    \multicolumn{3}{|c|}{\multirow{2}{*}{\bf Model}} & \bf Language & \bf Policy \\
    &  & & \bf Understanding & \bf Learning\\\hline\hline
   Baseline & (a) & \emph{DSTC4-Best} & 52.1~ & -\\
   & (b) & BLSTM  & 62.6~ & 63.4~~\\\hline
   Contextual-Sem & (c) & BLSTM & 68.2~ & 66.8~~\\
   & (d) & ~+ Role-Based  & \bf 69.2$^\dag$ & \bf 70.1$^\dag$\\\hline
   Contextual-NL & (e) & BLSTM & 64.2~ & 66.3~~\\
   & (f) & ~+ Role-Based   & 65.1$^\dag$ & 66.9$^\dag$\\
   & (g) & ~+ Role-Based w/ Intermediate Guidance &  \bf  65.8$^\dag$ & \bf 67.4$^\dag$\\
    \hline
  \end{tabular}
  \vspace{-1mm}
\caption{Language understanding and dialogue policy learning performance of F-measure on DSTC4 (\%). $^\dag$ indicates the significant improvement compared to all methods without speaker role modeling.}
\label{tab:res}
%\vspace{-4mm}
\end{table*}

\section{Experiments}
\label{sec:experiments}

To evaluate the effectiveness of the proposed model, we conduct the LU and dialogue policy learning experiments on human-human conversational data. %, which contains richer and more challenging dialogue interactions.


%The Dialogue State Tracking Challenge (DSTC) series has provided a common evaluation framework accompanied by labeled datasets ~\cite{williams2016dialog}. In this framework, the datasets come with a domain ontology which describes the types of user intents. The ontology defines a collection of slots and the values that each slot can take. In this paper, we took two pilot tasks (language understanding: predict speech acts and semantic slots by a given utterance; speech act prediction: predict the speech act of the next turn imitating the policy of one speaker) of DSTC4 ~\cite{kim2017fourth} as the experiment environment, which consists of 35 dialog sessions on touristic information for Singapore collected from Skype calls between three tour guides and 35 tourists. All the recorded dialogs with the total length of 21 hours have been manually transcribed and annotated with speech act and semantic labels for each turn level.


\subsection{Setup}
\label{ssec:settings}
%\subsection{Dataset}
The experiments are conducted on DSTC4, which consists of 35 dialogue sessions on touristic information for Singapore collected from Skype calls between 3 tour guides and 35 tourists~\cite{kim2016fourth}. 
All recorded dialogues with the total length of 21 hours have been manually transcribed and annotated with speech acts and semantic labels at each turn level.
The speaker labels are also annotated.
%which contain touristic information in Singapore from Skype calls between tour guides and tourists. 
%Unlike previous DSTC series collected human-computer dialogs, 
Human-human dialogues contain rich and complex human behaviors and bring much difficulty to all dialogue-related tasks.
Given the fact that different speaker roles behave differently, DSTC4 is a suitable benchmark dataset for evaluation.

We choose a mini-batch \texttt{adam} as the optimizer with the batch size of 128 examples~\cite{kingma2014adam}.
The size of each hidden recurrent layer is 128.
%, and the size of output layer in LU is $M+N$, where $M$ and $N$ are the number of user intents and slot tags respectively, while the size of output layer in SAP task is $M$. 
We use pre-trained 200-dimensional word embeddings $GloVe$~\cite{pennington2014glove}.
%, where the dimension of word embeddings is 200. 
We only apply 30 training epochs without any early stop approach. 
%We choose intent prediction part of LU task and SAP task in our experiments, the evaluation metric is token-level F1 score, and the best performance of each model configuration is recorded (see Table 1). 
%We utilize an encoder module to extract features from the inputs, the encoder can be either bidirectional LSTM module which the size of the network is same as other recurrent unit, 
The sentence encoder is implemented using a CNN with the filters of size $[2, 3, 4]$, 128 filters each size, and max pooling over time.
%In CNN encoder, max pooling over time is applied after convolving,
The idea is to capture the most important feature (the highest value) for
each feature map. This pooling scheme naturally
deals with variable sentence lengths. Please refer to \newcite{kim2014convolutional} for more details.

For both tasks, we focus on predicting multiple labels including speech acts and attributes, so the evaluation metric is average F1 score for balancing recall and precision in each utterance.
Note that the final prediction may contain multiple labels.

\subsection{Results}


The experiments are shown in Table~\ref{tab:res}, where we report the average number over five runs.
The first baseline (row (a)) is the best participant of DSTC4 in IWSDS 2016~\cite{kim2016fourth}, the poor performance is probably because tourist intents are much more difficult than guide intents (most systems achieved higher than 60\% of F1 for guide intents but lower than 50\% for tourist intents).
%we pick the best entry of intent prediction in LU-Tourist category among all the submission, which is $52.1\%$. 
The second baseline (row (b)) models the current utterance without contexts, performing 62.6\% for understanding and 63.4\% for policy learning.
%is to use only current utterance without history information and a simple bidirectional LSTM.

\subsubsection{Language Understanding Results}
\label{ssec:luresults}
%\paragraph{Language Understanding}
With contextual history, using ground truth semantic labels for learning history summary vectors greatly improves the performance to 68.2\% (row (c)), while using natural language slightly improves the performance to 64.2\% (row (e)).
The reason may be that NL utterances contain more noises and the contextual vectors are more difficult to model for LU.
The proposed role-based contextual models applying on semantic labels and NL achieve 69.2\% (row (d)) and 65.1\% (row (f)) on F1 respectively, showing the significant improvement all model without role modeling.
%Note that the results for semantic labels can be treated as the upper bound of the performance, because it utilizes the ground truth semantics for contexts.
%For employing contextual model, the first trial is to add the preceding utterance to the second baseline, however, the performance had a slight drop unexpectedly. Our analysis is that more inputs might result in considerable noise due to complex behaviors between human-human conversations. To address this problem, our role-based contextual model is employed, the performance has instant improvement by this simple concept.
Furthermore, adding the intermediate guidance acquires additional improvement (65.8\% from the row (g)).
It is shown that the semantic labels successfully guide the sentence encoder to obtain better sentence-level representations, and then the history summary vector carrying more accurate semantics gives better performance for understanding.

\subsubsection{Dialogue Policy Learning Results}
\label{ssec:policyresults}
%\paragraph{Dialogue Policy Learning}
To predict the guide's next actions, 
the baseline utilizes intent tags of the current utterance without contexts (row (b)).
%, the model structure and the configurations are same as in SLU task, while 
Table~\ref{tab:res} shows the similar trend as LU results, where applying either role-based contextual models or intermediate guidance brings advantages for both semantics-encoded and NL-encoded history.

%\subsubsection{Comparison between Semantic Labels and Natural Language}
\subsection{Discussion}
In contrast to NL, semantic labels (intent-attribute pairs) can be seen as more explicit and concise information for modeling the history, which indeed gains more in our experiments for both LU and dialogue policy learning.
The results of Contextual-Sem can be treated as the upper bound performance, because they utilizes the ground truth semantics of contexts.
Among the experiments of Contextual-NL, which are more practical because the annotated semantics are not required during testing, the proposed approaches achieve 5.1\% and 6.3\% relative improvement compared to the baseline for LU and dialogue policy learning respectively.
%improves the LU performance from 62.6\% to 65.7\% and the policy results from 62.4\% to 67.2\%.
%Feeding intent tags into our role-base contextual model gives significant improvement in performance on the baseline (from 62.4\% to 75.1\%).

%\subsubsection{Comparison between Language Understanding and Dialogue Policy Learning}
Between LU and dialogue policy learning tasks, most LU results are worse than dialogue policy learning results.
%The reason may be that the semantics of the current user utterance is easier to predict given the current sentence, but it is more difficult to predict the next suitable response as the dialogue policy.
The reason probably is that the guide has similar behavior patterns such as providing information and confirming questions etc., while the user can have more diverse interactions.
Therefore, understanding the user intents is slightly harder than predicting the guide policy in the DSTC4 dataset.

With the promising improvement for both LU and dialogue policy learning, the idea about modeling speaker role information can be further extended to various research topics in the future.

\section{Conclusion}
This paper proposes an end-to-end role-based contextual model that automatically learns speaker-specific contextual encoding.
Experiments on a benchmark multi-domain human-human dialogue dataset show that our role-based model achieves impressive improvement in language understanding and dialogue policy learning, demonstrating that different speaker roles behave differently and focus on different goals.
%The idea is motivated by huge variance of habits between different speaker, and the concept can be easily-extended in various research topics. 


\section*{Acknowledgements}
We would like to thank reviewers for their insightful comments on the paper.
The authors are supported by the Ministry of Science and Technology of Taiwan and MediaTek Inc..

%\newpage
\bibliography{ijcnlp2017}
\bibliographystyle{ijcnlp2017}

\end{document}

\subsubsection{SAP}
\label{sssec:sap}

\begin{table}
%\small
\centering
\begin{tabular}{ | l | c | c | c | c | }
    \hline
      & NL-RNN & NL-CNN & Intent \\ \hline\hline
    Baseline & \multicolumn{3}{c|}{N/A} \\ \hline
    w/o history  & 62.6 & N/A & N/A\\ \hline 
    Contextual & 62.1 & N/A & 64.1 \\ \hline
    Speaker Role  & 62.7 & 64.4 & 67.3 \\ 
    ~ + middle loss  & 65.7 & 64.0 & N/A  \\
    \hline
  \end{tabular}
\caption{Intent prediction performance on DSTC4.}
\end{table}

\begin{table}
%\small
\centering
\begin{tabular}{ | l | c | c | c | c | }
    \hline
      & NL-RNN & NL-CNN & Intent \\ \hline\hline
    Baseline & \multicolumn{3}{c|}{N/A} \\ \hline
    w/o history  & 62.6 & N/A & N/A\\ \hline 
    Contextual & 62.1 & N/A & 64.1 \\ \hline
    Speaker Role  & 62.7 & 64.4 & 67.3 \\ 
    ~ + middle loss  & 65.7 & 64.0 & N/A  \\
    \hline
  \end{tabular}
\caption{Policy learning performance on DSTC4.}
\end{table}


\begin{center}
  %\caption{Policy Learning}
\end{center}
In our experiments, we set the 

\subsubsection{LU}
\label{sssec:slu}

\begin{center}
  \begin{tabular}{ | c | c | c | c | }
    \hline
     & intent & NL-RNN & NL-CNN \\ \hline
    Baseline(DSTC4) & \multicolumn{3}{|c|}{0.5107} \\ \hline
    w/o history & N/A & 0.626 & N/A \\ \hline
    w/ history  & 0.641 & 0.621 & N/A \\ \hline
    w/ history, role-based  & 0.673 & 0.627 & 0.644  \\ \hline
    w/ history, role-based, w/ middle loss  & N/A & 0.657 & 0.640  \\
    \hline
  \end{tabular}
\end{center}



\section{Conclusion}
\label{sec:conclusion}


%%%%%%%%%%%%


\section{Experiments}
\label{sec:experiments}

\subsection{settings}
\label{ssec:settings}

\subsection{datasets}
\label{ssec:datasets}

\subsection{results and analysis}
\label{ssec:results}

\section{Conclusion}
\label{sec:conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

The following instructions are directed to authors of papers submitted to
and accepted for publication in the IJCNLP 2017 proceedings.  All authors
are required to adhere to these specifications. Authors are required to
provide a Portable Document Format (PDF) version of their papers. {\textbf The
proceedings are designed for printing on A4 paper}. Authors from countries
where access to word-processing systems is limited should contact the
publication chairs as soon as possible. Grayscale readability of all
figures and graphics will be encouraged for all accepted papers
(Section \ref{ssec:accessibility}).  

Submitted and camera-ready formatting is similar, however, the submitted
paper should have:
\begin{enumerate} 
\item Author-identifying information removed
\item A `ruler' on the left and right margins
\item Page numbers 
\item A confidentiality header.  
\end{enumerate}
In contrast, the camera-ready {\bf should  not have} a ruler, page numbers,
nor a confidentiality header.  By uncommenting {\small\verb|\ijcnlpfinalcopy|}
at the top of the \LaTeX source of this document, it will compile to
produce a PDF document in the camera-ready formatting; by leaving it
commented out, the resulting PDF document will be anonymized for initial
submission. Authors should place this command after the
{\small\verb|\usepackage|} declarations when preparing their camera-ready
manuscript with the IJCNLP 2017 style.


\section{General Instructions}

Manuscripts must be in two-column format.  Exceptions to the two-column
format include the title, as well as the authors' names and complete
addresses (only in the final version, not in the version submitted for
review), which must be centered at the top of the first page (see the
guidelines in Subsection~\ref{ssec:first}), and any full-width figures or
tables.  Type single-spaced. Start all pages directly under the top margin.  
See the guidelines later regarding formatting the first page.  Also see 
Section~\ref{sec:length} for the page limits.
Do not number the pages in the camera-ready version. 

By uncommenting {\small\verb|\ijcnlpfinalcopy|} at the top of this document,
it will compile to produce an example of the camera-ready formatting; by
leaving it commented out, the document will be anonymized for initial
submission.  When you first create your submission on softconf, please fill
in your submitted paper ID where {\small\verb|***|} appears in the
{\small\verb|\def\ijcnlppaperid{***}|} definition at the top.

The review process is double-blind, so do not include any author information
(names, addresses) when submitting a paper for review. However, you should
maintain space for names and addresses so that they will fit in the final
(accepted) version.  The IJCNLP 2017 \LaTeX\ style will create a titlebox
space of 2.5in for you when {\small\verb|\ijcnlpfinalcopy|} is commented out.

\subsection{The Ruler}
The IJCNLP 2017 style defines a printed ruler which should be present in the
version submitted for review.  The ruler is provided in order that
reviewers may comment on particular lines in the paper without
circumlocution.  If you are preparing a document without the provided
style files, please arrange for an equivalent ruler to
appear on the final output pages.  The presence or absence of the ruler
should not change the appearance of any other content on the page.  The
camera ready copy should not contain a ruler. (\LaTeX\ users may uncomment
the {\small\verb|\ijcnlpfinalcopy|} command in the document preamble.)  

Reviewers:
note that the ruler measurements do not align well with lines in the paper
--- this turns out to be very difficult to do well when the paper contains
many figures and equations, and, when done, looks ugly.  In most cases one 
would expect that the approximate location will be adequate, although you 
can also use fractional references ({\em e.g.}, the body of this section 
begins at mark $112.5$).

\subsection{Electronically-Available Resources}

IJCNLP provides this description to authors in \LaTeX2e{} format
and PDF format, along with the \LaTeX2e{} style file used to format it
({\small\tt ijcnlp2017.sty}) and an ACL bibliography style
({\small\tt ijcnlp2017.bst}) and example bibliography
({\small\tt ijcnlp2017.bib}). 
% A Microsoft Word template file ({\small\tt ijcnlp2017.dotx}) is also available. 
A Microsoft Word template file (ijcnlp17-word.docx) and example submission pdf (ijcnlp17-word.pdf) is available at http://ijcnlp2017.org/downloads/ijcnlp17-word.zip.
We strongly recommend the use of these style files, which have been 
appropriately tailored for the IJCNLP 2017 proceedings.

\subsection{Format of Electronic Manuscript}
\label{sect:pdf}

For the production of the electronic manuscript, you must use Adobe's
Portable Document Format (PDF). This format can be generated from
postscript files: on Unix systems, you can use {\small\tt ps2pdf} for this
purpose; under Microsoft Windows, you can use Adobe's Distiller, or
if you have cygwin installed, you can use {\small\tt dvipdf} or
{\small\tt ps2pdf}.  Note 
that some word processing programs generate PDF that may not include
all the necessary fonts (esp.\ tree diagrams, symbols). When you print
or create the PDF file, there is usually an option in your printer
setup to include none, all, or just non-standard fonts.  Please make
sure that you select the option of including ALL the fonts.  {\em Before
sending it, test your {\/\em PDF} by printing it from a computer different
from the one where it was created}. Moreover, some word processors may
generate very large postscript/PDF files, where each page is rendered as
an image. Such images may reproduce poorly.  In this case, try alternative
ways to obtain the postscript and/or PDF.  One way on some systems is to
install a driver for a postscript printer, send your document to the
printer specifying ``Output to a file'', then convert the file to PDF.

For reasons of uniformity, Adobe's {\bf Times Roman} font should be
used. In \LaTeX2e{} this is accomplished by putting

\small
\begin{verbatim}
\usepackage{times}
\usepackage{latexsym}
\end{verbatim}
\normalsize
in the preamble.

It is of utmost importance to specify the \textbf{A4 format} (21 cm
x 29.7 cm) when formatting the paper. When working with
{\tt dvips}, for instance, one should specify {\tt -t a4}.
Or using the command \verb|\special{papersize=210mm,297mm}| in the latex
preamble (directly below the \verb|\usepackage| commands). Then using 
{\tt dvipdf} and/or {\tt pdflatex} which would make it easier for some.

Print-outs of the PDF file on A4 paper should be identical to the
hardcopy version. If you cannot meet the above requirements about the
production of your electronic submission, please contact the
publication chairs as soon as possible.

\subsection{Layout}
\label{ssec:layout}

Format manuscripts with two columns to a page, following the manner in
which these instructions are formatted. The exact dimensions for a page
on A4 paper are:

\begin{itemize}
\item Left and right margins: 2.5 cm
\item Top margin: 2.5 cm
\item Bottom margin: 2.5 cm
\item Column width: 7.7 cm
\item Column height: 24.7 cm
\item Gap between columns: 0.6 cm
\end{itemize}

\noindent Papers should not be submitted on any other paper size.
 If you cannot meet the above requirements about the production of 
 your electronic submission, please contact the publication chairs 
 above as soon as possible.

\subsection{The First Page}
\label{ssec:first}

Center the title, author name(s) and affiliation(s) across both
columns (or, in the case of initial submission, space for the names). 
Do not use footnotes for affiliations.  
Use the two-column format only when you begin the abstract.

\noindent{\bf Title}: Place the title centered at the top of the first
page, in a 15 point bold font.  (For a complete guide to font sizes and
styles, see Table~\ref{font-table}.) Long titles should be typed on two
lines without a blank line intervening. Approximately, put the title at
2.5 cm from the top of the page, followed by a blank line, then the author
name(s), and the affiliation(s) on the following line.  Do not use only
initials for given names (middle initials are allowed). Do not format
surnames in all capitals (e.g., ``Mitchell,'' not ``MITCHELL'').  The
affiliation should contain the author's complete address, and if possible,
an email address. Leave about 7.5 cm between the affiliation and the body
of the first page.

\noindent{\bf Abstract}: Type the abstract at the beginning of the first
column.  The width of the abstract text should be smaller than the
width of the columns for the text in the body of the paper by about
0.6 cm on each side.  Center the word {\bf Abstract} in a 12 point
bold font above the body of the abstract. The abstract should be a
concise summary of the general thesis and conclusions of the paper.
It should be no longer than 200 words.  The abstract text should be in
10 point font.

\begin{table}
\centering
\small
\begin{tabular}{cc}
\begin{tabular}{|l|l|}
\hline
{\bf Command} & {\bf Output}\\\hline
\verb|{\"a}| & {\"a} \\
\verb|{\^e}| & {\^e} \\
\verb|{\`i}| & {\`i} \\ 
\verb|{\.I}| & {\.I} \\ 
\verb|{\o}| & {\o} \\
\verb|{\'u}| & {\'u}  \\ 
\verb|{\aa}| & {\aa}  \\\hline
\end{tabular} & 
\begin{tabular}{|l|l|}
\hline
{\bf Command} & {\bf  Output}\\\hline
\verb|{\c c}| & {\c c} \\ 
\verb|{\u g}| & {\u g} \\ 
\verb|{\l}| & {\l} \\ 
\verb|{\~n}| & {\~n} \\ 
\verb|{\H o}| & {\H o} \\ 
\verb|{\v r}| & {\v r} \\ 
\verb|{\ss}| & {\ss} \\\hline
\end{tabular}
\end{tabular}
\caption{Example commands for accented characters, to be used in, e.g., \BibTeX\ names.}\label{tab:accents}
\end{table}

\noindent{\bf Text}: Begin typing the main body of the text immediately
after the abstract, observing the two-column format as shown in the present
document. Do not include page numbers in the camera-ready manuscript.  

Indent when starting a new paragraph. For reasons of uniformity,
use Adobe's {\bf Times Roman} fonts, with 11 points for text and 
subsection headings, 12 points for section headings and 15 points for
the title.  If Times Roman is unavailable, use {\bf Computer Modern
  Roman} (\LaTeX2e{}'s default; see section \ref{sect:pdf} above).
Note that the latter is about 10\% less dense than Adobe's Times Roman
font.

\subsection{Sections}

\noindent{\bf Headings}: Type and label section and subsection headings in
the style shown on the present document.  Use numbered sections (Arabic
numerals) in order to facilitate cross references. Number subsections
with the section number and the subsection number separated by a dot,
in Arabic numerals. 

\noindent{\bf Citations}: Citations within the text appear in parentheses
as~\cite{Gusfield:97} or, if the author's name appears in the text itself,
as Gusfield~\shortcite{Gusfield:97}.  Using the provided \LaTeX\ style, the
former is accomplished using {\small\verb|\cite|} and the latter with
{\small\verb|\shortcite|} or {\small\verb|\newcite|}.  Collapse multiple
citations as in~\cite{Gusfield:97,Aho:72}; this is accomplished with the
provided style using commas within the {\small\verb|\cite|} command, e.g.,
{\small\verb|\cite{Gusfield:97,Aho:72}|}. Append lowercase letters to the
year in cases of ambiguities. Treat double authors as in~\cite{Aho:72}, but
write as in~\cite{Chandra:81} when more than two authors are involved.  

\noindent{\bf References}:  We recommend
including references in a separate~{\small\texttt .bib} file, and include
an example file in this release ({\small\tt ijcnlp2017.bib}). Some commands
for names with accents are provided for convenience in
Table~\ref{tab:accents}. References stored in the separate~{\small\tt .bib}
file are inserted into the document using the following commands:

\small
\begin{verbatim}
\bibliography{ijcnlp2017}
\bibliographystyle{ijcnlp2017}
\end{verbatim}
\normalsize 

References should appear under the heading {\bf References} at the end of
the document, but before any Appendices, unless the appendices contain
references. Arrange the references alphabetically by first author, rather
than by order of occurrence in the text.
% This behavior is provided by default in the provided \BibTeX\ style
% ({\small\tt ijcnlp2017.bst}). 
Provide as complete a reference list as possible, using a consistent format,
such as the one for {\em Computational Linguistics\/} or the one in the 
{\em Publication Manual of the American Psychological Association\/}
\cite{APA:83}. Authors' full names rather than initials are preferred. You
may use {\bf standard} abbreviations for conferences\footnote{\scriptsize {\tt https://en.wikipedia.org/wiki/ \\ \-\hspace{.75cm} List\_of\_computer\_science\_conference\_acronyms}}
and journals\footnote{\tt http://www.abbreviations.com/jas.php}.

\noindent{\bf Appendices}: Appendices, if any, directly follow the text and
the references (unless appendices contain references; see above). Letter
them in sequence and provide an informative title: {\bf A. Title of Appendix}. 
However, in a submission for review the appendices should be filed as a 
separate PDF. For more details, see Section~\ref{sec:supplemental}.

\noindent{\bf Acknowledgments}: A section for acknowledgments to funding
agencies, colleagues, collaborators, etc. should go as a last (unnumbered)
section immediately before the references. Keep in mind that, during review,
anonymization guidelines apply to the contents of this section too.
% In general, to maintain anonymity, refrain from including acknowledgments in
% the version of the paper submitted for review.  

\subsection{Footnotes}

\noindent{\bf Footnotes}: Put footnotes at the bottom of the page. They may be
numbered or referred to by asterisks or other symbols.\footnote{This is
how a footnote should appear.} Footnotes should be separated from the text
by a line.\footnote{Note the line separating the footnotes from the text.}
Footnotes should be in 9 point font.

\subsection{Graphics}

\noindent{\bf Illustrations}: Place figures, tables, and photographs in the
paper near where they are first discussed, rather than at the end, if possible.
Wide illustrations may run across both columns and should be placed at the
top of a page. Color illustrations are discouraged, unless you have verified
that they will be understandable when printed in black ink. 

\begin{table}
\small
\centering
\begin{tabular}{|l|rl|}
\hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
paper title & 15 pt & bold \\
author names & 12 pt & bold \\
author affiliation & 12 pt & \\
the word ``Abstract'' & 12 pt & bold \\
section titles & 12 pt & bold \\
document text & 11 pt  &\\
abstract text & 10 pt & \\
captions & 9 pt & \\
caption label & 9 pt & bold \\
bibliography & 10 pt & \\
footnotes & 9 pt & \\
\hline
\end{tabular}
\caption{\label{font-table} Font guide.}
\end{table}

\noindent{\bf Captions}: Provide a caption for every illustration; number each one
sequentially in the form:  ``{\bf Figure 1:} Figure caption.'',
``{\bf Table 1:} Table caption.''  Type the captions of the figures and tables
below the body, using 9 point text. Table and Figure labels should be
bold-faced.

\subsection{Accessibility}
\label{ssec:accessibility}

In an effort to accommodate the color-blind (and those printing to paper),
grayscale readability of papers is encouraged. Color is not forbidden, but 
authors should ensure that tables and figures do not rely solely on color to
convey critical distinctions.

\section{Length of Submission}
\label{sec:length}

The IJCNLP 2017 main conference accepts submissions of long papers and short papers. Long papers may consist of up to eight (8) pages of content, plus two (2) pages for references. Upon acceptance, final versions of long papers will be given one additional page – up to nine (9) pages with unlimited pages for references – so that reviewers’ comments can be taken into account. Short papers may consist of up to four (4) pages of content, plus two (2) pages for references. Upon acceptance, short papers will be given five (5) pages in the proceedings and unlimited pages for references. 

For both long and short papers, all figures and tables that are part of the main text must be accommodated within these page limits, observing the formatting instructions given in the present document. Supplementary material in the form of appendices does not count towards the page limit.

However, note that supplementary material should be supplementary (rather than central) to the paper, and that reviewers may ignore supplementary material when reviewing the paper (see Appendix A). Papers that do not conform to the specified length and formatting requirements are subject to be rejected without review.

Workshop chairs may have different rules for allowed length and whether supplemental material is welcome.  As always, the corresponding call for papers is the authoritative source.

\section{Supplemental Materials}
\label{sec:supplemental}

IJCNLP 2017 encourages submitting software and data that is described in the paper as supplementary material. IJCNLP 2017 also encourages reporting preprocessing decisions, model parameters, and other details necessary for the exact replication of the experiments described in the paper. Papers may be accompanied by supplementary material, consisting of software, data, pseudo-code, detailed proofs or derivations that do not fit into the paper, lists of features or feature templates, parameter specifications, and sample inputs and outputs for a system. Appendices are considered to be supplementary materials, and should be submitted as such.

The paper should be self-contained and not rely on the supplementary material. Reviewers are not asked to review or even download the supplemental material. If the pseudo-code or derivations or model specifications are an important part of the contribution, or if they are important for the reviewers to assess the technical correctness of the work, they should be a part of the main paper, not as appendices.


\section{Double-Blind Review Process}
\label{sec:blind}

As the reviewing will be blind, the paper must not include the authors' names and
affiliations.  Furthermore, self-references that reveal the authors' identity,
e.g., ``We previously showed (Smith,1991) ...'' must be avoided. Instead, use
citations such as ``Smith previously showed (Smith, 1991) ...'' Papers that do
not conform to these requirements will be rejected without review. In addition,
please do not post your submissions on the web until after the review process is
complete (in special cases this is permitted: see the multiple submission policy
below).

We will reject without review any papers that do not follow the official style
guidelines, anonymity conditions and page limits.

\section{Multiple Submission Policy}
Papers that have been or will be submitted to other meetings or publications must indicate this at submission time, and must be withdrawn from the other venues if accepted by IJCNLP 2017. We will not accept for publication or presentation papers that overlap significantly in content or results with papers that will be (or have been) published elsewhere. Authors submitting more than one paper to IJCNLP 2017 must ensure that submissions do not overlap significantly ($>$25\%) with each other in content or results.

Preprint servers such as arXiv.org and workshops that do not have published proceedings are not considered archival for purposes of submission. However, to preserve the spirit of blind review, authors are encouraged to refrain from posting until the completion of the review process. Otherwise, authors must state in the online submission form the name of the workshop or preprint server and title of the non-archival version. The submitted version should be suitably anonymized and not contain references to the prior non-archival version. Reviewers will be told: The author(s) have notified us that there exists a non-archival previous version of this paper with significantly overlapping text. We have approved submission under these circumstances, but to preserve the spirit of blind review, the current submission does not reference the non-archival version.

\section*{Acknowledgments}

Do not number the acknowledgment section.

\bibliography{ijcnlp2017}
\bibliographystyle{ijcnlp2017}

\end{document}
