%!TEX root = main.tex
\section{Generalised theorem}\label{sec:advanced}
Before we start introducing the notions needed for Theorem~\ref{thm:advanced} we motivate
the need of a generalisation of Theorem~\ref{thm:simple}. 
Formulation of Theorem~\ref{thm:simple} is rather simple and it is sufficient for our applications, but it may seem a bit arbitrary.
As mentioned in the paragraph before Theorem~\ref{thm:simple} it is well suited to the situation
when the ratio $\frac{x}{y}$ between some two counters $x$ and $y$ is fixed and bounded at some specific moment at the run.
We can however very easily imagine that in some other, but very related VASS 
some a bit more involved ratio is kept fixed. This can be a ratio of a form
$\frac{x_1 + x_2}{y_1 + y_2 + y_3}$ or something similar. It looks very natural that we may
keep constant a ratio not between two counter values, but between sums of a few counter values.
This motivates introduction of the linear functions defined below and use of them in Theorem~\ref{thm:advanced}.
When we deal with more than two counters we cannot easily speak about lines, which was natural in Theorem~\ref{thm:simple}.
This is the reason why we are forced to use a more abstract language in order to be prepared
for the above mentioned simple applications. Theorem~\ref{thm:advanced} can be seen
as a more powerful tool than Theorem~\ref{thm:simple}. At the moment we do not see
any applications in which Theorem~\ref{thm:advanced} is needed. However in our opinion
it is important to show that the techniques presented in Theorem~\ref{thm:simple} can be quite
easily extended to stronger Theorem~\ref{thm:advanced}.

Before stating Theorem~\ref{thm:advanced} we introduce a few notions.

\subparagraph*{Greatest common divisors}
We state here a fact about greatest common divisors, which is helpful in the sequel.
By $\gcd(a_1, \ldots, a_k)$ we denote the greatest common divisor of all the numbers $a_1, \ldots, a_k$.

%\begin{claim}
%For every natural numbers $a_1, \ldots, a_k \leq M$ there exist coefficients $b_1, \ldots, b_k \in \Z$ with $|b_i| \leq M^2$
%for each $i \in [1,k]$ such that $\gcd(a_1, \ldots, a_k) = a_1 b_1 + \ldots + a_k b_k$.
%\end{claim}
%
%\begin{proof}
%It is a folklore and can be easily proved by induction on $k$ that there exist some coefficients $b_1, \ldots, b_k$ such
%that $\gcd(a_1, \ldots, a_k) = \sum_{i=1}^k a_i b_i$. Let us take $b_i$ with $\sum_{i=1}^k |b_i|$ minimal.
%Clearly there cannot exist $i, j \in [1,k]$ such that $b_i \geq M$ and $b_j \leq -M$, as substituting $b_i$ by $b_i - a_j$
%and $b_j$ by $b_j + a_i$ would give a solution with smaller $\sum_{i=1}^k |b_i|$. Thus absolute value of either all positive or
%all negative coefficients need to be bounded by $M$, assume wlog. that it is true for the positive coefficients. Then however
%absolute value of each negative coefficient need to be bounded by sum of all positive coefficients plus value
%of $\gcd(a_1, \ldots, a_k)$, which altogether sums up to at most $M^2$.
%\end{proof}

\begin{claim}\label{cl:gcd}
For all natural numbers $a_1, \ldots, a_k \leq M$ and for each $S \geq k(M^2-M)$ which is divisible by $\gcd(a_1, \ldots, a_k)$
there exist nonnegative coefficients $b_1, \ldots, b_k \in \N$ such that $S = a_1 b_1 + \ldots + a_k b_k$.
\end{claim}

\begin{proof}
The following fact is called Bezout's identity and can be easily proved by induction on $k$: 
for each $a_1, \ldots, a_k \in \N$ there exist some coefficients $b_1, \ldots, b_k \in \Z$ such
that $\gcd(a_1, \ldots, a_k) = \sum_{i=1}^k a_i b_i$.  Let us take such a solution which minimises the sum $\sum_{i: b_i < 0} |b_i|$.
We show that in this solution actually all the $b_i$ are nonnegative. Assume otherwise and let $b_i < 0$ for some $i \in [1,k]$.
As $S \geq k(M^2-M)$ then for some $b_j \in [1,k]$ we have $b_j \geq M$. Then substituting $b_j$ by $b_j - a_i$ and
$b_i$ by $b_i + a_j$ we obtain a solution with smaller $\sum_{i: b_i < 0} |b_i|$, contradiction.
\end{proof}


\subparagraph*{Linear functions}
Let a linear function $\lin: \N^d \to \N$ be of a form $\lin(x_1, \ldots, x_d) = \sum_{i=1}^d n_i x_i$, with all the coefficients $n_i \in \N_+$.
We call a linear function \emph{reduced} if $\gcd(n_1, \ldots, n_d) = 1$, let us assume that $\lin$ is reduced.
Notice that each linear function is a reduced linear function multiplied by some natural number.
Let $M = \max_{i \in [1,d]} n_i$.
The \emph{support} of a linear function $\supp(\lin) \subseteq [1,d]$ is the set of coordinates
for which coefficient $n_i$ is nonzero.
Let the set of vectors $\zero(\lin) \subseteq \Z^d$ contain all the vectors $n_j e_i - n_i e_j$
for $i, j \in \supp(\lin)$. Clearly for any $v \in \zero(\lin)$ and any $u \in \N^d$ such that
$u+v \in \N^d$ we have $\lin(u+v) = \lin(u)$.
The following claim tells that set $\zero(\lin)$ in some way spans the set of vectors with the same value
of $\lin$ in case it is big.

\begin{claim}\label{cl:zero-run}
For any $u, v \in \N^d$ such that $\lin(u) = \lin(v) \geq d \cdot M^3$ there is a sequence of
vectors $u = x_0, x_1, \ldots, x_k = v \in \N^d$ such that for all $i \in [1,k]$ we have $x_i - x_{i-1} \in \zero(\lin)$.
\end{claim}

The proof of Claim~\ref{cl:zero-run} can be found in the appendix. It uses Claim~\ref{cl:gcd} and a bit
of other rather simple number theory.

%\begin{proof}
%We prove the claim by induction on $d$. For $d = 1$ clearly $u = v$ and there is nothing to show.
%Let us assume that claim holds for $d-1$, our aim is to prove it for $d$.
%Adding a vector $y \in \zero(\lin)$ to $x_{i-1}$ in order to obtain $x_i = x_{i-1} + y \in \N^d$ we call a \emph{step}.
%Clearly it is enough find a sequence of steps from $u$ to $v$.
%The plan is to apply first such a sequence of steps from $u$ to some $u'$ such that $u'[i] = v[i]$ for some $i \in [1,d]$
%and then show by induction assumption that a sequence of steps from $u'$ to $v$ exists as well.
%
%For a subset of coordinates $I \subseteq [1,d]$ and $x \in \N^d$ by $\lin_I(x)$ we denote $\sum_{i \in I} n_i x[i]$.
%As $\lin(v) \geq d \cdot M^3$ there exists some $j \in [1,d]$ such that $\lin_{[1,d] \setminus \{j\}}(v) \geq (d-1) M^3$.
%Assume wlog. that $j = d$, so
%\begin{equation}\label{eq:linv}
%\lin_{[1,d-1]}(v) \geq (d-1) M^3.
%\end{equation}
%We first aim to reach $u''$ such that $u''[d] - v[d] \geq M^2$. Clearly until for some $i \neq d$
%we have $u[i] \geq M$ we can apply the step $n_i e_d - n_d e_i$ to $u$ and increase value of $u[d]$. We continue this
%until we reach some $u''$ with $\sum_{i=1}^{d-1} u''[i] < (d-1) M$.
%This is indeed possible as $\sum_{i=1}^{d-1} u''[i] \geq (d-1) M$ implies that for some $i \neq d$ we have $u''[i] \geq M$. 
%Then we have that $\lin_{[1,d-1]}(u'') < (d-1) M^2$ as $M = \max_{i \in [1,d]} n_i$, so $n_d u''[d] > dM^3 - (d-1)M^2$.
%By~\eqref{eq:linv} we have $n_d v[d] \leq dM^3 - (d-1) M^3 = M^3$.
%Therefore $n_d (u''[d] - v[d]) > (d-1) (M^3 - M^2)$ and thus $u''[d] - v[d] \geq (d-1) (M^2 - M) \geq M^2 - M$.
%By Claim~\ref{cl:gcd} we have that $u''[d] - v[d] = \sum_{i=1}^{d-1} n_i x_i$ for some $x_i \in \N$.
%Therefore in order to obtain $u'$ such that $u'[d] = v[d]$ for each $i \in [1,d-1]$ we apply for each $i \in [1,d-1]$
%exactly $x_i$ number of times the step $n_d e_i - n_i e_d$ to $u''$.
%Notice that all the other coordinates beside the $d$-th one increase, so these steps indeed lead
%to vectors with nonnegative coordinates. As $\lin_{i \in [1,d-1]}(u') = \lin_{i \in [1,d-1]}(v) \geq (d-1) M^3$ we apply
%the induction assumption to show that indeed starting from $u'$ one can reach $v$ by a sequence of steps. This finishes the proof.
%\end{proof}


\subparagraph*{Modification of a VASS}

For two linear functions $\lin_1, \lin_2 \in \N^d \to \N$ with disjoint supports
and a VASS $V$ with state $q$ we define VASS $V^q_{\lin_1, \lin_2}$
as $V$ with additional transitions whose aim is to be able to increase the ratio $\frac{\lin_1(\cdot)}{\lin_2(\cdot)}$,
but never decrease it.
Let $\lin_1(x_1, \ldots, x_d) = \sum_{i=1}^d n_{i,1} x_i$ and let $\lin_2(x_1, \ldots, x_d) = \sum_{i=1}^d n_{i,2} x_i$.
The set of states of $V^q_{\lin_1, \lin_2}$ is inherited from $V$ similarly to the set of transitions of $V$.
We additionally add to $V$ transitions of the form $(q, v, q)$, which are loops in the state $q \in Q$, of the following form:
\begin{enumerate}
  \item for each coordinate $i \in \supp(\lin_2)$ add a loop which decreases coordinate $i$ by one
  \item for each coordinate $i \not\in \supp(\lin_1) \, \cup \, \supp(\lin_2)$ add two loops: one, which increases
  and one which decreases coordinate $i$ by one
  \item for each $v \in \zero(\lin_1)$ which is nonzero only at $\supp(\lin_1)$
  and similarly for each $v \in \zero(\lin_2)$ which is nonzero only at $\supp(\lin_2)$ add a loop with effect $v$. 
\end{enumerate}

Notice that condition 2 means that we can freely modify in $V^q_{\lin_1, \lin_2}$ the coordinates outside $\supp(\lin_1) \, \cup \, \supp(\lin_2)$
We are ready to state the theorem.

\begin{theorem}\label{thm:advanced}
Let $\lin_1, \lin_2: \N^d \to \N$ be two reduced linear functions with disjoint supports.
Let $V = (Q, T)$ be a $d$-VASS, $q \in Q$ be its state, $s, t \in Q \times \N^d$ be two its configurations
and $R \in \Q$ be a rational number.
Assume that
\begin{enumerate}[(1)]
  \item for each $v \in \N^d$ if $s \reaches q(v)$ then $\lin_1(v) \geq R \cdot \lin_2(v)$,
  \item for each $u, v \in \N^d$ if $q(v) \reaches t+u$ then $\lin_1(v-u) \leq R \cdot \lin_2(v-u)$,
  \item each run from $\state(s)$ to $\state(t)$ traverses through a configuration $c$ with $\state(c) = q$,
  \item the set $\{\proj_I(v) \mid s \reaches q(v) \reaches t\}$ is infinite, where $I = \supp(\lin_1) \cup \supp(\lin_2)$.
\end{enumerate}
Then for any $i \in \supp(\lin_2)$ there is no run from $s$ to $t+e_i$ in $V' = V^q_{\lin_1, \lin_2}$ and each
separator for $(V', s, t+e_i)$ contains a period $p$ such that $\proj_I(p) \neq 0$ and $\lin_1(p) = R \cdot \lin_2(p)$.
\end{theorem}

\begin{proof}
Due to condition (4) in the theorem statement there exists an infinite sequence of vectors $v_i \in \N^d$ with $\proj_I(v_i)$ pairwise different
such that $s \reaches q(v_i) \reaches t$. Let $\rho^1_i$ be the corresponding runs from $s$ to $q(v_i)$
and $\rho^2_i$ be the corresponding runs from $q(v_i)$ to $t$.
Recall that $\unlhd$ is a well-quasi order and its modified version (denoted here $\unlhd'$) with comparison on sources instead of targets
is also a well-quasi order.
Therefore there exist $i < j$ such that $\rho^1_i \unlhd \rho^1_j$ and $\rho^2_i \unlhd' \rho^2_j$.
Let $\Delta = v_j - v_i \in \N^d$. Clearly $\proj_I(\Delta) \neq 0$, as $\proj_I(v_i) \neq \proj_I(v_j)$. Let $a = v_i$.
By Corollary~\ref{corr:pumping} we get that for any $n \in \N$ there is a run of $V$ from $s$ to $a + n\Delta$.
An analogous reasoning with targets changed to sources shows that for any $n \in \N$ there is a run in $V$
from $a + n\Delta$ to $t$. Therefore for any $n \in \N$ we have $s \reaches q(a + n\Delta) \reaches t$.
By conditions (1) and (2) in the theorem statement we get that $\lin_1(a + n \Delta) = R \cdot \lin_2(a + n \Delta)$
for any $n \in \N$. In consequence $\lin_1(\Delta) = R \cdot \lin_2(\Delta)$. Notice that both $\lin_1(\Delta), \lin_2(\Delta) > 0$,
because $\proj_I(\Delta) \neq 0$.

We first show that $s \nreaches t+e_i$ in $V'$. %Let $t = p_t(v_t)$.
Assume towards a contradiction that $s \reaches t+e_i$. By condition (3) we know
that $s \reaches q(v) \reaches t+e_i$ for some $v \in \N^d$.
Observe first that conditions (1) and (2) still hold for the reachability relation defined in $V'$, as the added loops in state $q$
do not invalidate them (for that purpose we demand in point 3. in the construction of $V'$
that vector $v$ is nonzero only on coordinates in $\supp(\lin_1)$ or only on coordinates in $\supp(\lin_2)$).
Notice that by condition (2) setting $u = e_i$ we have
\[
\lin_1(v) = \lin_1(v - e_i) \leq R \cdot \lin_2(v - e_i) < R \cdot \lin_2(v).
\]
where the first equation follows from the fact that $\lin_1(e_i) = 0$.
On the other hand $s \reaches q(v)$ so by condition (1) we have $\lin_1(v) \geq R \cdot \lin_2(v)$, which is in contradiction
with the above inequality. Thus indeed $s \nreaches t+e_i$.

Now we show that every separator for $(V', s, t+e_i)$ contains an appropriate period $p$.
This proof is quite similar to the proof of Theorem~\ref{thm:simple}, but we need to deal with some more technicalities.
Consider a separator $S = \bigcup_{q \in Q} q(S_q)$ for $(V', s, t+e_i)$.
Clearly $S_q = \bigcup_{j \in J} L_j$, where $L_j$ are linear sets, needs to contain all the vectors $a + n \Delta$ for $n \in \N$.
Let $L$ be one of the finitely many linear sets $L_j$, which contains infinitely many vectors among $\{a + n \Delta \mid n \in \N\}$.
Let $L = b + \N p_1 + \ldots + \N p_k$ and $P$ be the set of periods $\{p_1, \ldots, p_k\}$.
We aim at showing that $L$ contains a period $p$ such that $\proj_I(p) \neq 0$ and $\lin_1(p) = R \cdot \lin_2(p)$,
namely $\lin_1(p) \cdot \lin_2(\Delta) = \lin_2(p) \cdot \lin_1(\Delta)$, as $R = \frac{\lin_1(\Delta)}{\lin_2(\Delta)}$.

We first prove a claim analogous to Claim~\ref{cl:ratio}.
This one is however much more challenging to prove. For its purpose we have defined $V'$ so intricately
with the additional loops.

\begin{claim}\label{cl:ratio2}
For each period $p \in P$ we have $\lin_1(\Delta) \cdot \lin_2(p) \leq \lin_2(\Delta) \cdot \lin_1(p)$.
\end{claim}

Claim~\ref{cl:ratio2} is proven in the appendix, it uses the nontrivial Claim~\ref{cl:zero-run}.

Now we follow the lines of the proof of Claim~\ref{cl:ratio}.
Let $P_\emptyset$ be the set of all periods in $P$ for which $\proj_I(p) = \emptyset$
and $P_\nemp = P \setminus P_\emptyset$.
Recall that we want to show existence of a period $p \in P_\nemp$ fulfilling $\lin_1(p) \cdot \lin_2(\Delta) = \lin_2(p) \cdot \lin_1(\Delta)$.
Assume towards a contradiction that there is no such period.
Therefore by Claim~\ref{cl:ratio2} each period $p \in P_\nemp$ satisfies $\lin_1(\Delta) \cdot \lin_2(p) < \lin_2(\Delta) \cdot \lin_1(p)$.
In particular for each period $p \in P_\nemp$ we have $\lin_1(p) > 0$,
thus we can equivalently write that for all $p \in P_\nemp$ it holds
\[
\frac{\lin_2(p)}{\lin_1(p)} < \frac{\lin_2(\Delta)}{\lin_1(\Delta)}.
\]
Let $F$ be the maximal value of $\frac{\lin_2(p)}{\lin_1(p)}$ for $p \in P_\nemp$,
clearly $\frac{\lin_2(\Delta)}{\lin_1(\Delta)} > F$, so
\begin{equation}\label{eq:f2}
\lin_2(\Delta) > F \cdot \lin_1(\Delta).
\end{equation}
Recall now that for arbitrary big $n$ we have that $a + n\Delta \in b + \N p_1 + \ldots + \N p_k$,
thus $(a - b) + n\Delta \in \N p_1 + \ldots + \N p_k$. Let $v = a - b$.
We have then that
\[
(\lin_1(v+n\Delta), \lin_2(v+n\Delta)) = \sum_{p_i \in P_\nemp} n_i (\lin_1(p), \lin_2(p)).
\]
By the above we know that
\[
\frac{\lin_2(v+n\Delta)}{\lin_1(v+n\Delta)} \leq F,
\]
as $v+n\Delta$ is a positive linear combination of periods from $P$,
for each $p \in P_\emptyset$ we have $\lin_1(p) = \lin_2(p) = 0$ and for each $p \in P_\nemp$
we have $\frac{\lin_2(p)}{\lin_1(p)} \leq F$.
Therefore
\[
\lin_2(v) + n \lin_2(\Delta) \leq F(\lin_1(v) + n \lin_1(\Delta))
\]
and equivalently
\[
n (\lin_2(\Delta) - F \cdot \lin_1(\Delta)) \leq F \cdot \lin_1(v) - \lin_2(v).
\]
By~\eqref{eq:f2} we have that $\lin_2(\Delta) - F \cdot \lin_1(\Delta) > 0$ therefore
\[
n \leq \frac{F \cdot \lin_1(v) - \lin_2(v)}{\lin_2(\Delta) - F \cdot \lin_1(\Delta)}.
\]
This is in contradiction with the fact that $n$ can be arbitrarily big
and finishes the proof.
\end{proof}

%We remark one more time that Theorem~\ref{thm:advanced} is not needed for our applications.
%The motivation behind stating and proving this theorem is to prepare for possible future applications
%and explore limits of our approach. In the proof of Theorem~\ref{thm:advanced} one needs to use
%two additional techniques, which were not needed to prove Theorem~\ref{thm:simple}: well quasi-order
%on runs and construction of modified VASS with auxiliary transitions.
%Moreover we believe that the statement of Theorem~\ref{thm:advanced} is more robust and looks
%more natural than Theorem~\ref{thm:simple}.

