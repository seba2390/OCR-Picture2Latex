\subsection{Training Data Preparation}

While there exists a large collection of interlaced videos over the Internet,
unfortunately, the ground-truth of these videos is lacking. Therefore, to
prepare a training data set, we have to synthesize
interlaced videos from existing progressive videos. To enrich our data variety, we
collect $33$ videos from the Internet and capture $18$ videos using progressive
scan devices ourselves. The videos are of different genres, ranging from 
scenic, sports, computer-rendered, to classic movies and cartoons. Then we randomly sample $3$ pairs of consecutive frames from each collected
video and obtain $153$ frame pairs in total. For each pair of consecutive
frames, we rescale each frame to the size of $512\times512$ and label them as the
pair of original frames $\mathbf{X}_t$ and $\mathbf{X}_{t+1}$ (ground-truth full frames)
(Fig.~\ref{fig:data_preparing}(a)). Then we synthesize an interlaced frame based
on these two original frames as
$\mathbf{I}=\{\mathbf{X}^{\text{odd}}_t,\mathbf{X}^{\text{even}}_{t+1}\}$, i.e.,
the odd lines of $\mathbf{I}$ are copied from $\mathbf{X}_t$ and the even lines
of $\mathbf{I}$ are copied from $\mathbf{X}_{t+1}$
(Fig.~\ref{fig:data_preparing}(b) \&~\ref{fig:data_preparing_real}). For
each triplet $\left\langle \mathbf{I}, \mathbf{X}_t,
\mathbf{X}_{t+1}\right\rangle $ of $512\times512$ resolution, we further divide
them into $64\times64$-resolution patch triplets $\left\langle \mathbf{I}_{p},
\mathbf{X}_{t,p}, \mathbf{X}_{t+1,p}\right\rangle$ with the sampling stride
setting to $64$.  Note that during patch generation, the parity of the divided
patches remain the same as original images. Finally, for each patch triplet
$\left\langle \mathbf{I}_{p}, \mathbf{X}_{t,p}, \mathbf{X}_{t+1,p}\right\rangle
$, we use $\mathbf{I}_{p}$ as a training input
(Fig.~\ref{fig:data_preparing}(b)) and the corresponding
$\mathbf{X}^{\text{even}}_{t,p}$ and $\mathbf{X}^{\text{odd}}_{t+1,p}$ as
training outputs (Fig.~\ref{fig:data_preparing}(c)). In particular, we convert
patches into the Lab color space and only use the L channel for training. Altogether, we
collect 9,792 patch triplets from the prepared videos, where $80\%$ of
the triplets are used for training and the rest are used for validation during
the training process. Note that, although our model is trained by patches of
$64\times64$ resolution, the trained convolutional operators can actually be
applied on images of any resolution.

\begin{figure}[!tp]
	% Requires \usepackage{graphicx}
	\includegraphics[width=1\linewidth]{images/data_preparing_1.pdf}\\
	\caption{Training data preparation. (a) Two consecutive frames $\mathbf{X}_t$ and $\mathbf{X}_{t+1}$ from an input video. (a) An interlaced frame $\mathbf{I}$ is synthesized by taking the odd lines from $\mathbf{X}_t$ and even lines from $\mathbf{X}_{t+1}$ respectively and regarded as the training input. (c) The even lines of $\mathbf{X}_t$ and the odd lines of $\mathbf{X}_{t+1}$ are regarded as the training output.}\label{fig:data_preparing}
\end{figure}

\begin{figure}[!tp]
	% Requires \usepackage{graphicx}
	\includegraphics[width=1\linewidth]{images/data_preparing_2.pdf}\\
	\caption{A real example of synthesizing an interlaced frame from two consecutive progressive frames.}\label{fig:data_preparing_real}
\end{figure}

\subsection{Neural Network Architecture}

With the prepared training dataset, we now present how we design our network
structure for deinterlacing. An illustration of our network structure is shown
in Fig.~\ref{fig:cnn_model}. It contains five convolutional layers. Our goal is
to reconstruct the original two frames $\mathbf{X}_t$ and $\mathbf{X}_{t+1}$
from an input interlaced frame $\mathbf{I}$. In the following, we first explain
our  design rationales and then describe the architecture in
detail.

\begin{figure}[!tp]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=\linewidth]{images/input_problem.pdf}\\
	\caption{Reconstructing two frames from two fields independently leads to inevitable visual artifacts due to the large information loss.}\label{fig:input_problem}
\end{figure}

\vspace{0.15in}
\noindent\emph{The Input/Output Layers}\,\,\,\,
%\paragraph{The Input/Output Layers}
One may suggest to utilize the existing neural network (e.g. SRCNN
\cite{dong2016image}) to learn $\mathbf{X}_t$ from
$\mathbf{X}^{\text{odd}}_{t}$ and $\mathbf{X}_{t+1}$ from
$\mathbf{X}^{\text{even}}_{t+1}$ independently. This effectively turns the problem into 
a super-resolution or image upscaling problem.
However, there are two drawbacks.

First of all, since the two frame reconstruction processes (i.e. from
$\mathbf{X}^{\text{odd}}_{t}$ to $\mathbf{X}_t$ and
$\mathbf{X}^{\text{even}}_{t+1}$ to $\mathbf{X}_{t+1}$) are independent from
each other, the neural network can only estimate the full frame from the known
half frame without the temporal information. This inevitably leads to less
satisfying results due to the large (50\%) information loss. In fact, the two
fields in the interlaced frame are temporally correlated. Consider an extreme
case where the scene in the two consecutive frames are static. In this scenario,
the two consecutive frames are exactly the same, and the interlaced frame should
also be artifact-free and exactly equal to the groundtruth we are looking for.
However, using this naive super-resolution approach, we have to feed the half
frame $\mathbf{X}^{\text{odd}}_t$ (or $\mathbf{X}^{\text{even}}_{t+1}$) to
reconstruct a full frame. It completely ignores the another half frame (which
now contains the exact pixel values) and introduces artifacts (due to 50\%
information loss). Fig.~\ref{fig:input_problem} shows the  poor result of one
such scenario. In contrast, our proposed neural network takes the whole
interlaced frame $\mathbf{I}$ as input (Fig.~\ref{fig:cnn_model}(a)). Note that
the temporal information is implicitly taken into consideration in our network,
since the two fields captured at different time instances are used for reconstructing each
single frame. The network may exploit the temporal correlation between fields to
improve the visual quality in higher-level convolutional layers.

Secondly, the standard neural network generally follows the conventional
translation-invariant assumption. That means all pixels in the input image are
processed with the same set of convolutional filters. However, in our deinterlacing
application, half of the pixels in $\mathbf{X}_t$ and $\mathbf{X}_{t+1}$ actually 
exist in $\mathbf{I}$ and should be directly copied from $\mathbf{I}$. 
Applying convolutional filters on these
known pixels inevitably changes their original colors and leads to clear
artifacts (Fig.~\ref{fig:srcnn_problem}(b) \& (c)). In contrast, our neural network only
estimates the unknown pixels $\mathbf{X}^{\text{even}}_t$ and
$\mathbf{X}^{\text{odd}}_{t+1}$ (Fig.~\ref{fig:cnn_model}(c)) and copies the known
pixels from $\mathbf{I}$ to $\mathbf{X}_t$ and $\mathbf{X}_{t+1}$ directly
(Fig.~\ref{fig:cnn_model}(d)).

%\begin{figure}[!tp]
%	\centering
%	% Requires \usepackage{graphicx}
%	\includegraphics[width=\linewidth]{images/translation_problem.eps}\\
%	\caption{There are two types of patches denoted as $A$ and $B$. To deinterlace the i
%		nput image (the middle) for different outputs, we have two different operations: copy or convolution. To choose different operations is determined by the outputs. For example, if we want the result on the right and the input is the patch $B$, we just directly copy the center pixel value. However, we need convolution operations on the same patch $B$ for the result on the left . Thus, we need to have two neural networks for different outputs. }\label{fig:translation_problem}
%\end{figure}

%Furthermore, even we can retrain SRCNN or SRResNet by setting the input as the whole interlaced frames, we still need to train two networks resulting high computation costs.
%That is because these networks learn the one-to-one mappings, while in our task, the neural network should have two outputs.
%This is illustrated in Figure~\ref{fig:translation_problem}.

\vspace{0.15in}
\noindent\emph{Pathway Design}\,\,\,\,
%\paragraph{The Two Pathways}
Since we estimate two half frames $\mathbf{X}^{\text{even}}_t$ and
$\mathbf{X}^{\text{odd}}_{t+1}$ from the interlaced frame $\mathbf{I}$, we
actually have to train two networks/pathways independently. Separately training two
networks is computational costly. Instead of training two networks, one
may suggest to train a single network for estimating the two half frames simultaneously
by doubling the depth of each convolutional layer. However, this also
highly increases the computational cost, since the number of the trained weights
are doubled. As reported by \cite{bengio2012deep}, deep neural network is to
seek good representation of input data, and such representations can be transferred
to many other tasks if the input data is similar. For example, the trained
features of AlexNet \cite{krizhevsky2012imagenet} (originally designed for
object recognition) can also be used for texture recognition and segmentation
\cite{cimpoi2015deep}. In fact, the lower-level layers of the convolutional
networks are always lower-level feature detectors that can detect edges and other
primitives. These lower-level layers in the trained models can be reused for new
tasks by training new higher-level layers on top of them. Therefore, in our
deinterlacing scenario, it is natural to combine the lower-level convolutional
layers to reduce the computation, since the input of the two networks/pathways is
completely the same. On top of these weight-sharing lower-level layers, higher-level 
layers are trained separately for estimating $\mathbf{X}^{\text{even}}_t$
and $\mathbf{X}^{\text{odd}}_{t+1}$ respectively. This makes the 
higher-level layers more adaptable to different objectives. Our method can be regarded
as training one neural network for estimating $\mathbf{X}^{\text{even}}_t$ and
then fixing the first three convolutional layers and re-training a second neural
network for estimating $\mathbf{X}^{\text{odd}}_{t+1}$.

%The second method is to merge the two pathways by extending the depth axis.
%This method also increases computation costs.
%For example, the kernels $conv5\_1$ and $conv5\_2$ should filter with both the feature maps outputed from $conv4\_1$ and $conv4\_2$ while in our network they only need to filter with half of feature maps.
%Furthermore, it is more difficult to train this neural network than our model since merging these two pathways makes them less adapted to their objectives than ours.
%By comparison, our network only do convolution in a single pathway.
%Furthermore, it is more difficult to train this neural network than our model because the two separated pathways make
%Furthermore, this method also violates the translation-invariant assumption

\vspace{0.15in}
\noindent\emph{Detailed Architecture}\,\,\,\,
%\paragraph{Detailed Architecture}
As illustrated in Fig.~\ref{fig:cnn_model}(b) \& (c), our network contains five convolutional layers with weights.
The first, second, and third layers are sequentially connected and shared by both pathways.
The first convolutional layer has $64$ kernels of size $3\times3\times1$.
The second convolutional layer has $64$ kernels of size $3\times3\times64$ and is connected to the output of the first layer.
The third convolutional layer has $64$ kernels of size $1\times1\times64$ and is connected to the output of the second layer.
The forth and fifth layers branch into two pathways without any connection between them.
The forth convolutional layer has $64$ kernels of size $3\times3\times64$ where each pathway has $32$ kernels.
The fifth convolutional has $2$ kernels of size $3\times3\times64$ where each pathway has $1$ kernel.
The activations for the first two layers are ReLU functions, while for the rest layers are identify functions.
The strides of convolution for the first four layers are $1$ pixel.
For the last layer, the horizontal stride remains $1$ pixel, while the vertical stride is $2$ pixels to obtain half-height images.

\subsection{Learning and Optimization}
Given the training dataset containing a set of triplets $\left\langle \mathbf{I}_p,\mathbf{X}^{\text{even}}_{t,p},\mathbf{X}^{\text{odd}}_{t+1,p}\right\rangle$, the optimal weights $W^*$ of our neural network are trained via the following objective function:
\begin{equation}\label{eq:objective}
\begin{split}
W^*=\argmin \frac{1}{N_p} &\sum_p \Big(\|\widehat{\mathbf{X}}^{\text{even}}_{t,p}-\mathbf{X}^{\text{even}}_{t,p}\|^2_2+\|\widehat{\mathbf{X}}^{\text{odd}}_{t+1,p}-\mathbf{X}^{\text{odd}}_{t+1,p}\|^2_2\\
&+\lambda_{TV}\big(TV(\widehat{\mathbf{X}}_{t,p})+TV(\widehat{\mathbf{X}}_{t+1,p})\big)\Big)
\end{split}
\end{equation}
where $N_p$ is the number of training samples, $\widehat{\mathbf{X}}^{\text{even}}_{t,p}$ and $\widehat{\mathbf{X}}^{\text{odd}}_{t+1,p}$ are the estimated output of the neural network, $TV(\cdot)$ is the total variation regularizer~\cite{aly2005image,johnson2016perceptual} and $\lambda_{TV}$ is the regularization scalar.

%\begin{figure}[!tp]
%  % Requires \usepackage{graphicx}
%  \includegraphics[width=1\linewidth]{images/lower_layer_filters.eps}\\
%  \caption{The Extracted Lower Layers of Proposed Neural Networks}\label{fig:lower_layer_filters}
%\end{figure}

We trained our neural network using Tensorflow on a workstation equipped with a
single nVidia TITAN X Maxwell GPU. The standard ADAM
optimization method \cite{kingma2014adam} is used to solve
Eq.~\ref{eq:objective}. The learning rate is $0.001$ and $\lambda_{TV}$ is set to
$2\times10^{-8}$ in our experiments. The number of epochs is $200$ and the batch size for each
epoch is $64$. It takes about $4$ hours to train the neural network.
