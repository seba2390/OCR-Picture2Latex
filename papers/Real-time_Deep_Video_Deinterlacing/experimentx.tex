\begin{figure*}[!tp]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=\linewidth]{images/synthetic_data_result.eps}\\
	\caption{We compare our method with bicubic interpolation and SRCNN. Row $1$ and row $3$: two input interlaced frames and deinterlaced results with different methods. Row $2$ and row $4$: patches extracted from row $1$ and row $3$ respectively.}\label{fig:compared_with_bicubic_and_srcnn_results}
\end{figure*}

\begin{figure*}[!tp]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=\linewidth]{images/compared_with_deinterlaced_method.eps}\\
	\caption{We compare our method with state-of-the-art deinterlacing methods. Row $1$ and row $3$: two input interlaced frames and deinterlaced results with different methods. Row $2$ and row $4$: patches extracted from row $1$ and row $3$ respectively.}\label{fig:compared_with_deinterlaced_method}
\end{figure*}

We validate the effectiveness of our method on a large collection of interlaced videos downloaded from the Internet or captured by ourselves with interlaced scan cameras. The collected videos include live sporting videos (``Soccer'' in Fig.~\ref{fig:teaser} and ``Tennis'' in Fig.~\ref{fig:compared_with_bicubic_and_srcnn_results}), scenic videos (``Leaves'' in Fig.~\ref{fig:teaser} and ``Bus'' in Fig.~\ref{fig:compared_with_bicubic_and_srcnn_results}), computer-rendered game videos (``Hunter'' in Fig.~\ref{fig:compared_with_bicubic_and_srcnn_results}), legacy movies (``Haystack'' in Fig.~\ref{fig:compared_with_bicubic_and_srcnn_results}), and legacy cartoons (``Rangers'' in Fig.~\ref{fig:compared_with_bicubic_and_srcnn_results}). However, we cannot obtain the the original progressive frames (groundtruth) for these existing or captured interlaced videos. Without ground-truth, we can only compare our method with the existing methods visually, but not quantitatively. Therefore, we further synthesize a set of interlaced videos from ordinary progressive scan videos in different types, where the ground-truth can be acquired and used for both visual and quantitative comparisons. Fig.~\ref{fig:compared_with_deinterlaced_method} presents a set of synthesized videos which also include sporting videos (``Slam''), scenic videos (``Taxi''), computer-rendered videos (``Roof'' and ``Girl''), movies (``Jumping''), and cartoons (``Tide''). Due to page limit, we only present one representative interlaced frame for each video sequence. While two full-sized frames can be recovered from each single interlaced frame, we only show the first frame in all the results. Please refer to the supplementary materials for more results and videos.

\vspace{0.15in}
\noindent\emph{Visual Comparison}\,\,\,\,
We first compare our method with the classic bicubic interpolation and the existing DCNN tailored for super-resolution, i.e. SRCNN~\cite{dong2016image}. Since SRCNN is not designed for deinterlacing, we re-train their model with our prepared dataset for deinterlacing. The results are presented in Fig.~\ref{fig:teaser} and Fig.~\ref{fig:compared_with_bicubic_and_srcnn_results}. The ``Bus'' and ``Soccer'' examples are $1080i$-format videos containing severe interlacing artifacts. Besides, the presented frames also contain motion-blurring and video compression artifacts. Since both bicubic interpolation and SRCNN reconstruct each frame based on a single field only (i.e. reconstructing the first frame solely based on the upper field), their results are still unsatisfying and contain obvious visual artifacts due to the large information loss. In particular, SRCNN performs even worse than bicubic interpolation since it follows the conventional translation-invariant assumption which does not hold in deinterlacing. In comparison, our method can obtain much clearer and sharper results than the competitors. The ``Hunter'' example shows a moving character from a game live broadcasting where the computer-rendered object contours/boundaries are quite sharp. Both bicubic interpolation and SRCNN lead to blurry and zig-zag edges around these sharp edges, which our method obtains the best reconstruction result in achieving smooth boundaries. The ``Haystack'' and ``Rangers'' examples are both taken from legacy DVDs in the interlaced NTSC format. In the ``Haystack'' example, only the character is moving, while the background keeps static. Without considering temporal information, both bicubic interpolation and SRCNN fails to recover the fine textures of the haystacks and obtain blurry results. In sharp contrast, our method successfully recovers the fine textures by taking two fields into consideration at the same time.

\begin{table*}
	\center
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		PSNR/SSIM & Taxi         & Roof & Slam & Jumping      & Tide  &   Girl \\\hline
		%%      & 10               &  56        & 58   & 57        &50    & 38      \\\hline
		bicubic & 31.56/0.9453          & 33.11/0.9808      &34.67/0.9783          &37.81/0.9801    		 &31.87/0.9809                   & 29.14/0.9585  \\  \hline
		ELA     & 32.47/0.9444          & 34.41/0.9839      &32.08/0.9605             &38.82/0.9844             &33.89/0.9811                 & 31.62/0.9724  \\ \hline
		WLSD    & 35.99/0.9746          & \textbf{35.70}/\textbf{0.9883}      &35.05/0.9794    &38.19/0.9819             &34.17/0.9820                 & 32.00/0.9761 \\ \hline
		FBA     & 34.94/0.9389          & 35.26/0.9815      &33.93/0.9749             &38.27/0.9822             &35.15/\textbf{0.9822}                 & 31.78/0.9756  \\ \hline
		SRCNN   & 30.12/0.9214          & 32.01/0.9749      &29.18/0.9353             &36.81/0.97094             &33.02/0.9758                 & 27.79/0.9477  \\ \hline
		Ours    &\textbf{38.15}/\textbf{0.9834}  & 35.44/0.9866   & \textbf{36.55}/\textbf{0.9838}         &\textbf{39.75}/\textbf{0.9889}    &\textbf{35.37}/0.9807      & \textbf{35.44}/\textbf{0.9866} \\ \hline
	\end{tabular}
	\caption{We report PSNR/SSIM for the deinterlaced results of each sequence in our benchmark.}
	\label{tab:psnr_comparsion}
\end{table*}

\begin{table*}[!tp]
	\center
	\begin{tabular}{|c|c|c|c|c|c|c|c|}\hline
		\multirow{2}{*}{Average time (s)} & \multirow{2}{*}{ELA} &\multirow{2}{*}{WLSD} &\multirow{2}{*}{FBA} & \multirow{2}{*}{Bicubic} & \multirow{2}{*}{SRCNN} &  \multicolumn{2}{c|}{Our Methods} \\ \cline{7-8}
		& & & & & & With sharable layers & Without sharable layers\\ \hline
		$1920\times 1080$ & 0.6854  &2.9843  &4.1486 &0.7068 &0.3010 & 0.0835 & 0.2520\\ \hline
		$1024\times 768$  & 0.0676  &1.0643  &1.6347 &0.2812 &0.0998 & 0.0301 & 0.0833\\ \hline
		$720\times 576$   & 0.0317  &0.4934  &0.6956 &0.1176 &0.0423 & 0.0204 & 0.0556\\ \hline
		$720\times 480$   & 0.0241  &0.4956  &0.7096 &0.1110 &0.0419 & 0.0137 & 0.0403\\ \hline
	\end{tabular}
	\caption{Timing Statistics}
	\label{tab:time_statistics}
\end{table*}

We further compare our method with the state-of-the-art deinterlaceing methods, including ELA~\cite{doyle1990interlaced},
WLSD~\cite{wang2014interlacing}, and FBA~\cite{vedadi2013interlacing}.
ELA is the most widely used deinterlaing methods due to its high performance. It is an intra-field method and uses edge directional correlation to reconstruct absent pixels between adjacent lines in a field. WLSD is the state-of-the-art intra-field deinterlacing method based on optimization. It generally produces better result than that of ELA, but has higher computational complexity. FBA is the state-of-the-art inter-field method.
Fig.~\ref{fig:compared_with_deinterlaced_method} shows the results of all methods for a set of synthesized interlaced videos. The reason that we use synthesized videos is to measure and visualize the detailed accuracy of reconstruction based on the ground-truth. Besides the reconstructed frames, we also visualize the difference images for a blown-up area for each case for better illustration. The difference image is simply computed as the pixel-wise absolute difference between the output image and the ground-truth image. As we can observe, all three existing generate artifacts around boundaries. The sharper the boundary is, the more obvious the artifacts are. In general, ELA produces the most artifacts since it only uses information of one field and also a relatively simple interpolator to achieve high performance. WLSD produces less artifacts as it adopts a more complex optimization-based strategy to fill the missing pixels. But it still only utilizes information of a single field and has large information loss during reconstruction. Though FBA uses the temporal information between consecutive frames, this method still cannot achieve good visual quality because they only rely on a simple interpolator. In contrast, our method produces significantly less artifacts than the existing methods by via a novel DCNN approach. 

%Due to the large motion of the taxi, the input frame of first row contains severe interlacing artifacts. 
%The existing deinterlacing methods first separates the two interlaced fields and then deinterlace each field individually.
%Due to the simplicity of these methods, they cannot achieve satisfying visual quality.
%The input frame of the second row also contains sever interlacing artifacts which are produced by the camera motion. 
%As we can see, our methods can outperform the existing deinterlacing methods using our method can better use the temporal information.
%The input frame of third row is a static and thus the input frame does not contain any interlacing artifacts. However, when applying existing deinterlacing methods (especially the intra-field methods) on this frame can introduce artifacts because these methods cannot fully use the temporal information between the two interlaced fields. Since our methods can learn the complex interpolation function based on deep learning using the temporal information contained in the frame, our method can produce better visual quality results.
%As a result, our method can be more adaptive to the image context than existing deinterlacing methods.

\vspace{0.15in}
\noindent\emph{Quantitative Evaluation}\,\,\,\,
%\paragraph{Quantitative evaluation} 
We train our neural network by minimizing the loss of Eq.~\ref{eq:objective} on the training data. 
The training loss and validation loss throughout the whole training epochs are shown in Fig.~\ref{fig:training_loss}. 
Both training and validation losses reduce rapidly after the first few epochs and converge at round 50 epochs.
\if 0
{\color{red} {\em [Xueting: talk about training loss with figure. This paragraph is copied from Chengze's paper for your reference. Please rewrite accordingly.]} To evaluate our method quantitatively, we first measure the training
loss of our DCNN model. It is the error between the convolved output from the network
(using the current trained weights) and the ground truth in the training data.
This loss is measured in terms of mean squared error (MSE)
normalized by the image resolution to the range of $[0, 1]$.
The right figure shows the training loss throughout the whole training epochs.
It rapidly reduces after the first few epochs and stabilized at around 25 epochs.}
\fi

We also compare our method with all existing methods, including bicubic interpolation, SRCNN re-trained with our dataset, ELA, WLSD, and FBA, in terms of deinterlacing accuracy using peak signal-to-noise ratio(PSNR) and structural similarity index (SSIM). The PSNR and SSIM values are computed by comparing the output frames with the ground-truth. We take the average value of all frames for each sequence for both measurements. Table~\ref{tab:psnr_comparsion} presents the statistics. It can be observed from the statistics that our method consistently performs the best among all methods in terms of both PSNR and SSIM. WLSD occasionally achieves the highest score, while our method also has comparable results in these cases.

\vspace{0.15in}
\noindent\emph{Timing Statistics}\,\,\,\,
%\paragraph{Time and Data Statistics} 
We further test the running time of our method and all competitors on a workstation with Intel Core CPU i7-5930, 65GB RAM and a NIVIDIA TITAN X Maxwell GPU. The statistics are presented in Table~\ref{tab:time_statistics}. Our method achieves the highest performance among all methods in all resolutions. It processes even faster than ELA with apparently better visual quality. ELA and SRCNN have similar performance and are slighter slower than our method. Bicubic interpolation, WLSD, and FBA have much higher computational complexity and are far from real-time processing.  
Note that ELA is only CPU methods without GPU acceleration.
%The compared CPU methods can be accelerated with GPU, however, our methods can still outperform them in of the visual quality.
In particular, with a single GPU, our method can already achieve real-time processing for resolutions up to $1024\times 768$ (the frame rate is about $33$). With one more GPU, our method can also achieve real-time processing for $1920\times 1080$-resolution videos. We also test our model without sharing lower-level layers, i.e., two separate networks are needed for reconstructing the two frames respectively. The statistics is show in the last column in Table~\ref{tab:time_statistics}. Although this strategy can achieve similar deinterlacing quality, it roughly triples the computational time and circumvent the method from processing in real-time.
\begin{figure}[!tp]
	\centering
	\includegraphics[width=0.9\linewidth]{images/objectiveFunctions.eps}\\
	\caption{Training loss and validation loss of our neural network.}\label{fig:training_loss}
\end{figure}

%Besides, since our method do not consider the temporal information beyond two interlaced frame, thus the generated results may still suffer from temporal-inconsistence problem.
\vspace{0.15in}
\noindent\emph{Failure Cases}\,\,\,\,
%\paragraph{Failure cases}
Since our method does not explicitly separate the two fields for reconstructing the two frames individually, the two fields may affect each other in a bad manner when the motion between the two fields are extremely large. The first row in Fig.~\ref{fig:failure_cases} presents an example where the interlaced frame is synthesized from two frames with very large motion, obvious artifacts can be observed.
Our method may also fail when the original frames contains very thin horizontal structures.
The second row of Fig.~\ref{fig:failure_cases} shows an example where a horizontal thin reflection stripe is on the car. Only one line of the reflection stripe is scanned in the interlaced frame. Our neural network fails to identify it as a result of interlacing, but regards it as the original structures and incorrectly preserves it in the reconstructed frame. 
This is because this kind of patches is rare and gets diluted by the large amount of common cases.
We may relieve this problem by training the neural network by adding more such training patches. 
%But it is actually a dilemma where temporal information between fields may have positive or negative influences for deinterlacing, while we believe the influence is positive in most cases.

\if 0
\begin{table*}
\center
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  Methods & Slam         & Taxi & Roof & Girl      & Tree  & Hunter   \\\hline
  %%      & 10               &  56        & 58   & 57        &50    & 38      \\\hline
  bicubic & 34.6728          & 31.5631             &33.1103             &29.1432    		 & 32.3022              & 27.3022  \\  \hline
  ELA     & 32.0774          & 32.4663             &34.4114             &31.6249             & 34.4923               & 28.4309  \\ \hline
  WLSD    & 35.0527          & 35.9891             &\textbf{35.7043}    &31.9972             & 35.6053               & \textbf{28.8993} \\ \hline
  FBA     & 33.9326          & 34.9419             &35.2620             &31.7825             & 34.9947               & 28.6538  \\ \hline
  SRCNN   & 29.1792          & 30.1170             &32.0118             &27.7869             & 32.9349               & 26.4134  \\ \hline
  Ours    & \textbf{36.5476} & \textbf{38.1525}    &35.4443             &\textbf{38.4228}    &\textbf{36.5777}       & 28.83293 \\ \hline
\end{tabular}
\caption{PSNR Comparsion}
\label{tab:psnr_comparsion}
\end{table*}
\begin{table*}
	\center
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		Methods & Slam        & Taxi       & Roof       			& Girl             & Tree             & Hunter  \\\hline
		bicubic & 0.9783          & 0.9453           & 0.9808     			&0.9585            &0.9647            & 0.9135   \\  \hline
		%LA & 34.4053 & 34.2670 & 43.3262 & 37.2526 & 37.8879 & 42.4141 & 31.5988 & 39.5855 \\ \hline
		ELA     & 0.9605          & 0.9444           & 0.9839     			&0.9724            &0.9794            & 0.9370   \\ \hline
		WLSD    & 0.9794          & 0.9746           & \textbf{0.9883}      &0.9761            &0.9867            & 0.9473  \\ \hline
		FBA     & 0.9749          & 0.9389           & 0.9815     			&0.9756        	   &0.9712            & 0.9518   \\ \hline
		SRCNN   & 0.9353          & 0.9214           & 0.9749     			&0.9477            &0.9624            & 0.9266 \\ \hline
		Ours    & \textbf{0.9838} & \textbf{0.9834}  & 0.9866     			& \textbf{0.9932}  &\textbf{0.9839 } & \textbf{0.9525 } \\ \hline
	\end{tabular}
	\caption{SSIM Comparsion}
	\label{tab:ssim_comparsion}
\end{table*}
\fi
\begin{figure}[!tp]
	\centering
	\includegraphics[width=\linewidth]{images/failure_cases.eps}\\
	\caption{Failure Cases}\label{fig:failure_cases}
\end{figure}




