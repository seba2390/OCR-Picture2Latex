%% introduce what is video interlace and deinterlace, and why we need video deinterlace

Interlacing technique has been widely used in the past few decades for television
broadcast and video recording, in both analog and digital ways. Instead of capturing all
$N$ scanlines for each frame, only $N/2$ odd numbered scanlines are captured for
the current frame (Fig.~\ref{fig:interlaced_example}(a), upper), and the other
$N/2$ even numbered scanlines are captured for the following frame
(Fig.~\ref{fig:interlaced_example}(a), lower). It basically trades the frame
resolution for the frame rate, in order to double the perceived frame rate
without increasing the bandwidth. Unfortunately, since the two half frames are
captured in {\em different time instances}, there are significant visual
artifacts such as line flickering and ``serration'' on the silhouette of moving
objects (Fig.~\ref{fig:interlaced_example}(b)), when the odd and even fields are
interlaced displayed. The degree of ``serration'' depends on the motion of objects
and hence is spatially varying. This makes deinterlacing (removal of interlacing artifacts) an ill-posed problem.

%one has to apply techniques to convert the interlaced signal into progressive format in order to display it on an all-progressive-scan device.
%There is a large collection of interlaced videos available over the internet(e.g., legacy movies produced in 1970\~1980s and current HDTV videos) that need to be deinterlaced.


%% introduce the traditional video interlace problems
%To tackle these problems, several deinterlacing methods have been proposed.  is the process to interpolate the full-sized high-resolution frames from an interlaced low-resolution one.

Many deinterlacing methods have been proposed to suppress the visual artifacts.
A typical approach is to reconstruct two full frames from the odd and even half
frames independently (Fig.~\ref{fig:interlaced_example}(c)). However, the result
is usually unsatisfactory, due to the large information 
loss (50\% loss)~\cite{doyle1990interlaced,wang2012efficient,wang2013moving}. 
Higher-quality reconstruction can be obtained by first estimating object 
motion~\cite{jeon2009weighted,mohammadi2012enhanced,lee2013high}. However, motion
estimation from half interlacing frames are not reliable, and also computationally expensive.
Hence, they are seldomly used in practice, 
let alone real-time applications.


%% introduce the problem of neural networks directly applied to deinterlacing


In this paper, we propose the first deep convolutional neural networks (DCNNs)
method tailormade for the video deinterlacing problem. To our best knowledge, no
DCNN-based deinterlacing method exists. One may argue that existing DCNN-based
methods for interpolation or super-resolution~\cite{mallat2016understanding,dong2016image} can be applied to
reconstruct the full frames from the half frames, in order to solve the
deinterlacing problem. However, such naive approach lacks of utilizing the
temporal information between the odd and even half frames, just like the existing
intra-field deinterlacing methods~\cite{doyle1990interlaced,wang2012efficient}. Moreover,
this naive approach follows the conventional translation-invariant assumption.
That means, all pixels in the output full frames are processed with the same set
of convolutional filters, even though half of the scanlines (odd/even numbered)
actually exist in the input half frames. Fig.~\ref{fig:srcnn_problem}(b) shows
a full frame, reconstructed by the state-of-the-art DCNN-based super-resolution method,
SRCNN~\cite{dong2016image}, exhibiting obvious halo artifact. 
Instead of replacing the potentially error-contaminated pixels from the convolutional filtering with the groundtruth
pixels in the input half frames and leading to visual artifacts
(Fig.~\ref{fig:srcnn_problem}(c)), we argue that we should only reconstruct the
missing scanlines, and leave the pixels in the original odd/even scanlines
intact. All these motivate us to design a novel DCNN model tailored for
solving the deinterlacing problem.


\if 0
To
preserve the values of the known pixels, we may use a larger training dataset or
deeper network structure, but the computation cost will also be increased and
the values of the known pixels still cannot be fully preserved. Even if we fix
the values of the known pixels, it still cannot achieve fully satisfying results
as no temporal information is considered (e.g. Fig.~\ref{fig:srcnn_problem}(c)).
Moreover, temporal information is also missing during reconstruction as they
treat the reconstruction of two frames as two independent processes. Therefore,
they still can only guess the missing half from the known half without further
information and obtain less satisfying result. 
\fi 

\begin{figure}[!tp]	
	\includegraphics[width=1\linewidth]{images/interlaced_example.pdf}\\
	\caption{(a) Two half fields are captured in two distinct time instances. 
	(b) The interlaced display exhibits obvious artifacts on the silhouette of moving car.
	(c) Two full frames	reconstructed from the two half frames independently with an intra-field deinterlacing method ELA~\cite{doyle1990interlaced}.} \label{fig:interlaced_example}
\end{figure}

%% introduce our model and benefits

In particular, our newly proposed DCNN architecture circumvents the translation-invariant assumption and takes the temporal
information into consideration. Firstly, we only estimate the missing scanlines
to avoid modifying the groundtruth pixel values from the odd/even scanlines
(input). That is, the output of the neural network system are two half frames
containing only the missing scanlines. Unlike most existing methods which ignore the
temporal information between the odd and even frames, we reconstruct each half 
output frame from both the odd and even frames. In other words,
 our
neural network system takes two original half frames as input and outputs two missing half
frames (complements). 

Since we have two outputs, two neural networks are needed for training. 
We further accelerate it by combining the lower-levels of two
neural networks~\cite{bengio2012deep}, as the input are the same and 
hence the lower-level convolutional filters are sharable. 
With this improved network structure, we can achieve real-time performance.

\if 0
largely speed up the whole process and achieve real-time for videos with
$1024\times768$ resolution on a single TITAN X Maxwell GPU. 
\fi 

\begin{figure}[!tp]
\includegraphics[width=1\linewidth]{images/srcnn_problem.pdf}\\
\caption{(a) An input interlaced frame. 
(b) Directly applying SRCNN to deinterlacing introduces blurry and halo artifacts. 
(c) The visual artifacts are worsen if we retain the pixels from the input odd/even scanlines. 
(d) Our result.}\label{fig:srcnn_problem} 
\end{figure}

To validate our method, we evaluate it over a rich
variety of challenging interlaced videos including live broadcast, legacy movies,
and legacy cartoons. Convincing and visually pleasant results are obtained in all experiments 
(Fig.~\ref{fig:teaser} \& \ref{fig:srcnn_problem}(d)). We also compare our
method to existing deinterlacing methods and DCNN-based models in both visual comparison and quantitative
measurements. All experiments confirm that our method not
only outperforms existing methods in terms of accuracy, but also
speed performance. 

%the output of the neural network is two separated half-sized images instead of full-sized images.
%That is means for the first output frame, the neural network estimates the even line pixels and directly copies the odd line pixels from the input frame, while for the second output frame, the neural network estimates the odd line pixels and copies the even line pixels from the input frame. By doing so, the convolution operators become translation-invariant again in each sub-fields.

%Third, we add a temporal consistency regularization on the output to guarantee the temporal consistence between every two consecutive frames.
%Our method only needs a small data set to generate the state-of-the-art results and also run in real-time.

