In this paper, we present the first DCNN for video deinterlacing. Unlike
the conventional DCNNs suffering from the translation-invariant issue, we
proposed a novel DCNN architecture by adopting the whole interlaced frame as
input and two half frames as output. We also propose to share the lower-level
convolutional layers for reconstructing the two output frames to boost  
efficiency. With this strategy, our method achieves real-time deinterlacing
on a single GPU for videos of resolution up to $1024\times768$.
Experiments show that our method outperforms existing methods,
including traditional deinterlacing methods and DCNN-based models re-trained for
deinterlacing, in terms of both reconstruction accuracy and computational
performance.

Since our method takes the whole interlaced frame as the input, frame
reconstruction is always influenced by both fields. While this may produce
better results in most of the cases, it occasionally leads to visually poorer
results when the
motion between two fields is extremely large. In this scenario, reconstructing
each frame from a single field without considering temporal information may
produce better results. A possible solution is to first recognize such large-motion
frames, and then decide whether temporal information should be utilized for 
deinterlacing.

%\begin{figure*}[!tp]
%	\centering
%	% Requires \usepackage{graphicx}
%	\includegraphics[width=\linewidth]{images/blank.eps}\\
%	\caption{Groundtruth Comparison}\label{fig:ground_truth_results}
%\end{figure*}
