\begin{figure*}[!tp]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=\linewidth]{images/synthetic_data_result.pdf}\\
	\caption{Comparisons between bicubic interpolation, SRCNN~\cite{dong2016image} and our method. 
	}\label{fig:compared_with_bicubic_and_srcnn_results}
\end{figure*}

\begin{figure*}[!tp]
	\centering
	% Requires \usepackage{graphicx}
	\includegraphics[width=\linewidth]{images/compared_with_deinterlaced_method.pdf}\\
	\caption{Comparisons between the state-of-the-art deinterlacing tailored methods, including ELA~\cite{doyle1990interlaced}, WLSD~\cite{wang2014interlacing}, and FBA~\cite{vedadi2013interlacing}, with our method. }\label{fig:compared_with_deinterlaced_method}
\end{figure*}

We evaluate our method on a large collection of interlaced videos downloaded
from the Internet or captured by ourselves with interlaced scan cameras. These
videos include live sporting videos (``Soccer'' in
Fig.~\ref{fig:teaser} and ``Tennis'' in
Fig.~\ref{fig:compared_with_bicubic_and_srcnn_results}), scenic videos
(``Leaves'' in Fig.~\ref{fig:teaser} and ``Bus'' in
Fig.~\ref{fig:compared_with_bicubic_and_srcnn_results}), computer-rendered gameplay
videos (``Hunter'' in Fig.~\ref{fig:compared_with_bicubic_and_srcnn_results}),
legacy movies (``Haystack'' in
Fig.~\ref{fig:compared_with_bicubic_and_srcnn_results}), and legacy cartoons
(``Rangers'' in Fig.~\ref{fig:compared_with_bicubic_and_srcnn_results}).
Note that, we have no access to the original progressive frames (groundtruth) of 
these videos. Without groundtruth, we can only
compare our method to existing methods visually, but not quantitatively.

To evaluate quantitatively (with comparison to the groundtruth), we 
synthesize a set of test interlaced videos from progressive scan videos of different
genres. None of these synthetic interlaced videos exist in our training data.
Fig.~\ref{fig:compared_with_deinterlaced_method} presents a set of synthetic interlaced 
videos, including sports (``Basketball''), scenic
(``Taxi''), computer-rendered (``Roof''), movies (``Jumping''), and
cartoons (``Tide'' and ``Girl''). Due to the page limit, we only present one
representative interlaced frame for each video sequence. While two full size
frames can be recovered from each single interlaced frame, we only show the
first frame in all our results. Please refer to the supplementary materials for
more complete results.

\vspace{0.15in}
\noindent\emph{Visual Comparison}\,\,\,\,
We first compare our method with the classic bicubic interpolation and the
existing DCNN tailored for super-resolution, i.e. SRCNN~\cite{dong2016image}.
Since SRCNN is not designed for deinterlacing, we re-train their model with our
prepared dataset for deinterlacing purpose. The results are presented in
Fig.~\ref{fig:teaser} and~\ref{fig:compared_with_bicubic_and_srcnn_results}. ``Soccer'', ``Bus'' and ``Tennis'' are in $1080i$ format and exhibit severe interlacing
artifacts. Besides, the frames also contain motion-blur and video
compression artifacts. Since both bicubic interpolation and SRCNN reconstruct
each frame from a single field alone, their results are unsatisfactory and
exhibit obvious artifacts due to the large information loss. 
SRCNN performs even worse than the bicubic interpolation, since it
follows the conventional translation-invariant assumption which not held in
deinterlacing scenario. In comparison, our method can obtain much clearer and sharper
results than our competitors. The ``Hunter'' example shows a moving character
from a gameplay where the computer-rendered object
contours/boundaries are sharply preserved. Both bicubic interpolation and SRCNN lead
to blurry and zig-zag near these sharp edges. In contrast, our method obtains
the best reconstruction result in achieving sharp and smooth boundaries. The ``Haystack''
and ``Rangers'' examples are both taken from legacy DVDs in interlaced NTSC
format. In the ``Haystack'' example, only the character is moving, while the
background remains static. Without considering the temporal information, both bicubic
interpolation and SRCNN fails to recover the fine texture of the haystacks and
obtain blurry results. In sharp contrast, our method successfully recovers the
fine texture by taking two fields into consideration.

\begin{table*}
	\center
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		PSNR/SSIM & Taxi         & Roof & Basketball & Jumping      & Tide  &   Girl \\\hline
		%%      & 10               &  56        & 58   & 57        &50    & 38      \\\hline
		bicubic & 31.56/0.9453          & 33.11/0.9808      &34.67/0.9783          &37.81/0.9801    		 &31.87/0.9809                   & 29.14/0.9585  \\  \hline
		ELA     & 32.47/0.9444          & 34.41/0.9839      &32.08/0.9605             &38.82/0.9844             &33.89/0.9811                 & 31.62/0.9724  \\ \hline
		WLSD    & 35.99/0.9746          & \textbf{35.70}/\textbf{0.9883}      &35.05/0.9794    &38.19/0.9819             &34.17/0.9820                 & 32.00/0.9761 \\ \hline
		FBA     & 34.94/0.9389          & 35.26/0.9815      &33.93/0.9749             &38.27/0.9822             &35.15/\textbf{0.9822}                 & 31.78/0.9756  \\ \hline
		SRCNN   & 30.12/0.9214          & 32.01/0.9749      &29.18/0.9353             &36.81/0.97094             &33.02/0.9758                 & 27.79/0.9477  \\ \hline
		Ours    &\textbf{38.15}/\textbf{0.9834}  & 35.44/0.9866   & \textbf{36.55}/\textbf{0.9838}         &\textbf{39.75}/\textbf{0.9889}    &\textbf{35.37}/0.9807      & \textbf{35.44}/\textbf{0.9866} \\ \hline
	\end{tabular}
	\caption{PSNR and SSIM between the deinterlaced frames and groundtruth of all methods.}
	\label{tab:psnr_comparsion}
\end{table*}

\begin{table*}[!tp]
	\center
	\begin{tabular}{|c|c|c|c|c|c|c|c|}\hline
		\multirow{2}{*}{Average time (s)} & \multirow{2}{*}{ELA} &\multirow{2}{*}{WLSD} &\multirow{2}{*}{FBA} & \multirow{2}{*}{Bicubic} & \multirow{2}{*}{SRCNN} &  \multicolumn{2}{c|}{Our Methods} \\ \cline{7-8}
		& & & & & & With sharable layers & Without sharable layers\\ \hline
		$1920\times 1080$ & 0.6854  &2.9843  &4.1486 &0.7068 &0.3010 & \textbf{0.0835} & 0.2520\\ \hline
		$1024\times 768$  & 0.0676  &1.0643  &1.6347 &0.2812 &0.0998 & \textbf{0.0301} & 0.0833\\ \hline
		$720\times 576$   & 0.0317  &0.4934  &0.6956 &0.1176 &0.0423 & \textbf{0.0204} & 0.0556\\ \hline
		$720\times 480$   & 0.0241  &0.4956  &0.7096 &0.1110 &0.0419 & \textbf{0.0137} & 0.0403\\ \hline
	\end{tabular}
	\caption{Timing statistics for all methods.}
	\label{tab:time_statistics}
\end{table*}

We further compare our method to the state-of-the-art deinterlacing methods,
including ELA~\cite{doyle1990interlaced}, WLSD~\cite{wang2014interlacing}, and
FBA~\cite{vedadi2013interlacing}. ELA is the most widely used deinterlacing
methods due to its high performance. It is an intra-field method and uses edge
directional correlation to reconstruct the missing scanlines. WLSD is the state-of-the-art intra-field deinterlacing method based on optimization. It generally
produces better result than that of ELA, but with a higher computational
expense. FBA is the state-of-the-art inter-field method.
Fig.~\ref{fig:compared_with_deinterlaced_method} shows the results of all
methods for a set of synthetic interlaced videos, in which we have the
groundtruths for quantitative evaluation. Besides the reconstructed frames, we
also blow-up the difference images for better visualization. The difference
image is simply computed as the pixel-wise absolute difference between the
output and the groundtruth. As we can observe, all our competitors
generate artifacts surrounding the boundaries. The sharper the boundary is, the more
obvious the artifact is. In general, ELA produces the most artifacts since it
adopts a simple interpolator and utilizes information from a single field alone. 
WLSD produces less artifacts as it adopts a more complex optimization-based strategy to fill the missing pixels. But it still
only utilizes information of a single field and has large information loss
during reconstruction. Though FBA utilizes the temporal information, 
it still cannot achieve good visual quality because
they only rely on simple interpolators. In contrast, our method produces
significantly less artifacts than all competitors.

%Due to the large motion of the taxi, the input frame of first row contains severe interlacing artifacts. 
%The existing deinterlacing methods first separates the two interlaced fields and then deinterlace each field individually.
%Due to the simplicity of these methods, they cannot achieve satisfying visual quality.
%The input frame of the second row also contains sever interlacing artifacts which are produced by the camera motion. 
%As we can see, our methods can outperform the existing deinterlacing methods using our method can better use the temporal information.
%The input frame of third row is a static and thus the input frame does not contain any interlacing artifacts. However, when applying existing deinterlacing methods (especially the intra-field methods) on this frame can introduce artifacts because these methods cannot fully use the temporal information between the two interlaced fields. Since our methods can learn the complex interpolation function based on deep learning using the temporal information contained in the frame, our method can produce better visual quality results.
%As a result, our method can be more adaptive to the image context than existing deinterlacing methods.

\vspace{0.15in}
\noindent\emph{Quantitative Evaluation}\,\,\,\,
%\paragraph{Quantitative evaluation} 
We train our neural network by minimizing the loss of Eq.~\ref{eq:objective} on
the training data. The training loss and validation loss throughout the whole
training epochs are shown in Fig.~\ref{fig:training_loss}. Both training and
validation losses reduce rapidly after the first few epochs and converge in
around 50 epochs.

\if 0
{\color{red} {\em [Xueting: talk about training loss with figure. This paragraph is copied from Chengze's paper for your reference. Please rewrite accordingly.]} To evaluate our method quantitatively, we first measure the training
loss of our DCNN model. It is the error between the convolved output from the network
(using the current trained weights) and the ground truth in the training data.
This loss is measured in terms of mean squared error (MSE)
normalized by the image resolution to the range of $[0, 1]$.
The right figure shows the training loss throughout the whole training epochs.
It rapidly reduces after the first few epochs and stabilized at around 25 epochs.}
\fi

We also compare the accuracy of our method to our competitors in terms of peak
signal-to-noise ratio (PSNR) and structural similarity index (SSIM). Note that
we only compute the PSNR and SSIM for those test videos with groundtruth. We
take the average value over all frames of each video sequence in computing both
measurements. Table~\ref{tab:psnr_comparsion} presents the statistics. Our
method outperforms the competitors in terms of both PSNR
and SSIM in most cases.

\vspace{0.15in}
\noindent\emph{Timing Statistics}\,\,\,\,
%\paragraph{Time and Data Statistics} 
Lastly, we compare the running time of our method to our competitors on a
workstation with Intel Core CPU i7-5930, 65GB RAM equipped with a nVidia TITAN X
Maxwell GPU. The statistics are presented in Table~\ref{tab:time_statistics}.
Our method achieves the highest performance among all methods in all
resolutions. It processes even faster than ELA with apparently better visual
quality. ELA and SRCNN have similar performance and are slighter slower than our
method. Bicubic interpolation, WLSD, and FBA have much higher computational
complexity and are far from real-time processing.  Note that ELA is only a CPU
method without GPU acceleration. In particular, with a single GPU, our method
already achieves real-time performance up to the resolution of $1024\times 768$
(33 fps). With one more GPU, our method can also achieve real-time performance
for $1920\times 1080$-resolution videos. We also test our model without sharing
lower-level layers, i.e., two separate networks are needed for reconstructing
the two frames. The statistics is shown in the last column in
Table~\ref{tab:time_statistics}. This strategy roughly triples the computational
time while quality is similar to that with sharing low-level layers.

%The compared CPU methods can be accelerated with GPU, however, our methods can still outperform them in of the visual quality.

\begin{figure}[!tp]
	\centering
	\includegraphics[width=0.9\linewidth]{images/objectiveFunctions.pdf}\\
	\caption{Training loss and validation loss of our neural network.}\label{fig:training_loss}
\end{figure}

%Besides, since our method do not consider the temporal information beyond two interlaced frame, thus the generated results may still suffer from temporal-inconsistence problem.
\vspace{0.15in}
\noindent\emph{Limitations}\,\,\,\,
%\paragraph{Failure cases}
Since our method does not explicitly separate the two fields for reconstructing
two full frames, the two fields may interfere each other 
badly when the motion between the two fields are extremely large. The first row
in Fig.~\ref{fig:failure_cases} presents an example where the interlaced frame
has a very large motion, obvious artifacts can be
observed. Our method may also fail when the interlaced frame contains very thin
horizontal structures. The second row of Fig.~\ref{fig:failure_cases} shows an
example where a horizontal thin reflection stripe appears on a car. Only one line
of the reflection stripe is scanned in the interlaced frame. Our neural network
fails to identify it as a result of interlacing, but regards it as the original
structures and incorrectly preserves it in the reconstructed frame. This is
because this kind of patches is rare and gets diluted by the large amount of
common cases. We may relieve this problem by training the neural network with 
more such training patches. 

%But it is actually a dilemma where temporal information between fields may have positive or negative influences for deinterlacing, while we believe the influence is positive in most cases.

\if 0
\begin{table*}
\center
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  Methods & Slam         & Taxi & Roof & Girl      & Tree  & Hunter   \\\hline
  %%      & 10               &  56        & 58   & 57        &50    & 38      \\\hline
  bicubic & 34.6728          & 31.5631             &33.1103             &29.1432    		 & 32.3022              & 27.3022  \\  \hline
  ELA     & 32.0774          & 32.4663             &34.4114             &31.6249             & 34.4923               & 28.4309  \\ \hline
  WLSD    & 35.0527          & 35.9891             &\textbf{35.7043}    &31.9972             & 35.6053               & \textbf{28.8993} \\ \hline
  FBA     & 33.9326          & 34.9419             &35.2620             &31.7825             & 34.9947               & 28.6538  \\ \hline
  SRCNN   & 29.1792          & 30.1170             &32.0118             &27.7869             & 32.9349               & 26.4134  \\ \hline
  Ours    & \textbf{36.5476} & \textbf{38.1525}    &35.4443             &\textbf{38.4228}    &\textbf{36.5777}       & 28.83293 \\ \hline
\end{tabular}
\caption{PSNR Comparsion}
\label{tab:psnr_comparsion}
\end{table*}
\begin{table*}
	\center
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		Methods & Slam        & Taxi       & Roof       			& Girl             & Tree             & Hunter  \\\hline
		bicubic & 0.9783          & 0.9453           & 0.9808     			&0.9585            &0.9647            & 0.9135   \\  \hline
		%LA & 34.4053 & 34.2670 & 43.3262 & 37.2526 & 37.8879 & 42.4141 & 31.5988 & 39.5855 \\ \hline
		ELA     & 0.9605          & 0.9444           & 0.9839     			&0.9724            &0.9794            & 0.9370   \\ \hline
		WLSD    & 0.9794          & 0.9746           & \textbf{0.9883}      &0.9761            &0.9867            & 0.9473  \\ \hline
		FBA     & 0.9749          & 0.9389           & 0.9815     			&0.9756        	   &0.9712            & 0.9518   \\ \hline
		SRCNN   & 0.9353          & 0.9214           & 0.9749     			&0.9477            &0.9624            & 0.9266 \\ \hline
		Ours    & \textbf{0.9838} & \textbf{0.9834}  & 0.9866     			& \textbf{0.9932}  &\textbf{0.9839 } & \textbf{0.9525 } \\ \hline
	\end{tabular}
	\caption{SSIM Comparsion}
	\label{tab:ssim_comparsion}
\end{table*}
\fi
\begin{figure}[!tp]
	\centering
	\includegraphics[width=\linewidth]{images/failure_cases.pdf}\\
	\caption{Failure cases. The top row shows a case where our result contains obvious artifacts when the motion of the interlaced frame is too large. The bottom row shows a case where our method fails to identify thin horizontal structures as interlacing artifacts and incorrectly preserves it in the reconstructed frame.}\label{fig:failure_cases}
\end{figure}