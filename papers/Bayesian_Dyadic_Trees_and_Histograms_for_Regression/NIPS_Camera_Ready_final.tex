\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath,amsthm, amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{color}


\newcommand{\E}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\1}{\mathbb{I}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\e}{\varepsilon}
\newcommand{\Ha}{\mathcal{H}^\alpha}

\newcommand{\utwi}[1]{\mbox{\boldmath $ #1$}}
\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\der}{\mathrm{d}}
\renewcommand{\d}{\mathrm{d}}
\renewcommand{\P}{\mathsf{P}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\Y}{\bm{Y}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\YY}{\bm{Y}}
\newcommand{\XX}{\bm{X}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\expect}[1]{\langle\mbox{$ #1$}\rangle}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\QED}{\flushright{QED}\flushleft}
\def\C {\,|\:}
\def\D{\bm{\Delta}}
\def\t{\bm{\theta}}
\def\bomega{\bm{\omega}}
\def\C {\,|\:}
\def\g{\gamma}
\def\G{\bm{\Gamma}}
\def\bG{\bm{\Gamma}}
\def\B{\bm{B}}
\def\b{\bm{\beta}}
\def\Y{\bm{Y}}
\def\Z{\bm{Z}}
\def\A{\bm{A}}
\def\S{\bm{\Sigma}}
\def\Set{\mathcal{S}}
\def\X{\bm{X}}
\def\x{\bm{x}}
\def\y{\bm{y}}
\def\by{\bm{y}}
\def\u{\bm{u}}
\def\bg{\bm{\gamma}}
\def\M{\bm{M}}
\def\DM{\bm{DM}}
\def\Bphi{\bm{\Phi}}
\def\z{\bm{\zeta}}
\def\BD{\bm{\Delta}}
\def\BOmega{\bm{\Omega}}
\def\BS{\bm{\Sigma}}
\def\t{\theta}
\def\bt{\bm{\theta}}
\def\b{\bm{\beta}}
\def\btheta{\bm{\theta}}
\def\bTheta{\bm{\Theta}}
\renewcommand{\d}{\mathrm{d}\,}

\newcommand{\st}[1]{ \textbf{\color{red}#1} } %used to mark changes  compared to submitted version

\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{corollary}{Corollary}[section]

 
 \theoremstyle{assumption}
 \newtheorem{assumption}{Assumption}
 
 \graphicspath{{./Figures/}}

\usepackage{xr}
\externaldocument{NIPS_Camera_Ready_supplement}

\title{Bayesian Dyadic Trees and Histograms for  Regression}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  St\'{e}phanie van der Pas  \\
  Mathematical Institute \\
  Leiden University\\
  Leiden, The Netherlands \\
  \texttt{svdpas@math.leidenuniv.nl} \\
  \And
    Veronika Ro\v{c}kov\'{a} \\
 Booth School of Business\\
  University of Chicago\\
  Chicago, IL, 60637 \\
  \texttt{Veronika.Rockova@ChicagoBooth.edu} \\
  }


%\thanks{Use footnote for providing further
 %   information about author (webpage, alternative
  %  address)---\emph{not} for acknowledging funding agencies.}
\begin{document}

\maketitle

\begin{abstract} %The abstract must be limited to one paragraph.

  {Many machine learning  tools for regression are based on recursive partitioning of the covariate space into smaller regions, where the regression function can be estimated locally. Among these, regression trees and their ensembles have demonstrated impressive empirical performance.  
 In this work,  we shed light on the machinery behind Bayesian variants of these methods.  In particular, we study Bayesian regression histograms, such as Bayesian dyadic trees, in the simple regression case with just one predictor.   We focus on the reconstruction of regression surfaces that are piecewise constant, where the number of jumps is unknown. We show that with suitably designed priors, posterior distributions concentrate around the true step regression function at a near-minimax rate. These results {\sl do not} require the knowledge of the true number of steps, nor the width of the true partitioning cells. Thus, Bayesian dyadic regression trees are fully adaptive and can recover the true piecewise regression function nearly as well as if we knew the exact number and location  of jumps.
 Our results constitute the first step towards  understanding why Bayesian trees and their ensembles have worked so well in practice.  As an aside, we discuss prior distributions  on balanced interval partitions and how they relate to an old  problem in geometric probability. Namely, we relate the probability of covering the circumference of a circle with random arcs whose endpoints are confined to a grid, a new variant of the original problem.
   }
  \end{abstract}
\vspace{-0.5cm}
\section{Introduction}
Histogram regression methods, such as regression trees \cite{cart} and their ensembles \cite{breiman}, have an impressive record of empirical success in many areas of application \cite{Berchuck2005, Nimeh2007, Razi2005, Green2012, Polley2010}. Tree-based machine learning (ML) methods build a piecewise constant reconstruction of  the regression surface based  on  ideas of recursive partitioning. Perhaps the most popular partitioning schemes are the ones based on parallel-axis splits. One recent example is the Mondrian process \cite{mondrian}, which was introduced to the ML community as a prior over tree data structures with interesting self-consistency properties. Many efficient algorithms exist that can be deployed to fit regression histograms underpinned by some partitioning scheme. Among these, Bayesian variants, such as Bayesian CART  \cite{Chipman1998,Denison1998} and BART \cite{Chipman2010}, have appealed to umpteen practitioners. There are several reasons why. Bayesian tree-based regression tools (a) can adapt to regression surfaces without any need for pruning, (b) are reluctant to  overfit, (c) provide an avenue for  uncertainty statements via posterior distributions. 
While practical success stories abound  \cite{Berchuck2005, Nimeh2007, Razi2005, Green2012, Polley2010}, the theoretical understanding of Bayesian  regression tree methods has  been lacking. In this work, we study the quality of posterior distributions with regard to the three properties mentioned above.
We provide first theoretical results that contribute to the understanding of Bayesian Gaussian regression methods based on recursive partitioning.

Our performance metric will be the speed of posterior concentration/contraction around the true regression function. This is ultimately a frequentist  assessment, describing the typical  behavior of the posterior under the true generative model \cite{Ghosal2000}.  Posterior concentration rate results are now slowly entering the machine learning community as a tool for obtaining more insights into Bayesian methods \cite{Zhang2004, Tang2014, Korda2013, Briol2015, Chen2016}. 
% Typically, our estimator will be some measure of center of the posterior distribution, such as the mean, median or mode. 
Such results quantify not only the typical distance between a point estimator (posterior mean/median) and the truth, but also the typical spread of the posterior around the truth.   Ideally, most of the posterior mass should be concentrated in a ball centered around the true value with a radius proportional to the minimax rate \cite{Ghosal2000, Ghosal2007}. 
%There are multiple ways in which such results can contribute to a deeper understanding of statistical learning procedures.  Many statistical learning tools are concerned with merely point estimation.  The posterior distribution provides an avenue for both point estimation as well as uncertainty assessment. Posterior concentration results quantify not only the average distance between our estimator and the truth, but also the average spread of the posterior around the truth.   Ideally, most posterior mass should be concentrated in a ball centered around the true value with a radius proportional to the square root of the minimax rate \cite{Ghosal2000, Ghosal2007}. 
Being inherently a performance measure of both location and spread, optimal posterior concentration provides a necessary certificate for further uncertainty quantification \cite{Szabo2015, Castillo2014, Rousseau2016b}.
%While many statistical learning methods are aimed at estimation, the ability to quantify the uncertainty is of immense importance \cite{Blundell2015, Gal2016, Hernandez2015}. A  flurry of research activity has recently occurred around Bayesian credible sets  \cite{Szabo2015, Szabo2015b, Serra2014, Castillo2014, Ray2015,Sniekers2015,Belitser2015, Rousseau2016b}. Optimal posterior concentration gives more hope that Bayesian credible intervals will perform well.
Beyond uncertainty assessment, theoretical guarantees that describe  the average posterior shrinkage behavior  have also been a valuable instrument for assessing the suitability of  priors.  As such, these results can often provide useful guidelines for the choice of tuning parameters, e.g. the latent Dirichlet allocation model \cite{Tang2014}. 





Despite the rapid growth of this  frequentist-Bayesian theory field, posterior concentration results for  Bayesian regression histograms/trees/forests have,  so far,  been unavailable.  Here, we adopt  this theoretical framework to get new insights into why these methods work so well.

\vspace{-0.3cm}
\subsection*{Related Work}
Bayesian density estimation with step functions is  a relatively well-studied problem \cite{Castillo_polya,Liu2015,Scricciolo2007}. The literature on Bayesian histogram regression is a bit less crowded. Perhaps the closest to our conceptual framework is the work by Coram and Lalley \cite{coram}, who studied Bayesian non-parametric binary regression with uniform mixture priors on step functions. The authors focused on $L_1$ consistency. Here, we focus on posterior concentration rather than consistency.  We are not aware of any other related theoretical study of Bayesian histogram methods for Gaussian regression.


\vspace{-0.3cm}
\subsection*{Our Contributions}

In this work we focus on a canonical regression setting with merely one predictor. We study hierarchical priors on step functions and provide conditions under which the posteriors concentrate optimally around the true regression function. We consider the case when the true regression function  itself is a step function, i.e. a tree or a tree ensemble, where the number and location of jumps is unknown. 




%To understand the behavior of priors on trees and partitioning schemes, we need to understand their combinatorial complexity. 
We start with a very simple space of approximating step functions, supported on equally sized intervals where the number of splits is equipped with a prior. These partitions include dyadic regression trees. %We show that for a suitable complexity prior, one can adapt to the unknown number of jumps. 
We show that for a suitable complexity prior, all relevant information about the true regression function (jump sizes and the number of jumps) is learned from the data automatically. During the course of the proof, we develop a notion of the complexity of a piecewise constant function relative to its approximating class.%, and find that the prior  should pile on functions with at most order $\log{n}$ distinct pieces.}



Next, we take a  larger approximating space consisting of functions supported on balanced partitions that do not necessarily have to be  of equal size. These correspond to more general trees with splits at observed values. With a uniform prior over all balanced partitions, we are able to achieve a nearly ideal performance (as if we knew the number and the location of jumps). 
As an aside, we describe the distribution of interval lengths obtained when the splits are sampled uniformly from  a grid.
We relate this distribution to the probability of covering the circumference of a circle with random arcs, a problem in geometric probability that dates back to \cite{shepp,Feller68}. Our version of this problem assumes that  the splits are chosen from a discrete grid rather than from  a unit interval.

\vspace{-0.3cm}
\subsection*{Notation}
With $\propto$ and $\lesssim$ {we} will  denote an equality and inequality,  up to a constant.
%The notation $\preceq$ will be used to denote inequality up to a constant. 
The $\varepsilon$-covering number of a set $\Omega$ for a semimetric $d$, denoted by $N(\varepsilon,\Omega,d),$ is the minimal number of $d$-balls of radius $\varepsilon$ needed to cover the set $\Omega$. 
We denote by $\phi(\cdot)$ the standard normal density and by  $P_f^n= \bigotimes P_{f,i}$ the $n$-fold product measure of the $n$ independent observations under \eqref{eq:def_problem} with a regression function $f(\cdot)$. By $\mathbb{P}_n^{x}=\frac{1}{n}\sum_{i=1}^n\delta_{x_i}$ we denote the empirical distribution of the observed covariates, by $||\cdot||_n$ the norm on $L_2(\mathbb{P}_n^{x})$ and by $||\cdot||_2$ the standard Euclidean norm.

\vspace{-0.2cm}
\section{Bayesian Histogram Regression}
\vspace{-0.2cm}
We consider a classical nonparametric regression model, where response variables $\Y^{(n)} = (Y_1, \ldots, Y_n)'$ are related to input variables $\x^{(n)} = (x_1, \ldots, x_n)'$ through the function $f_0$ as follows
\vspace{-0.1cm}
\begin{equation}\label{eq:def_problem}
Y_i = f_0(x_i) + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0, 1), \quad i = 1, \ldots, n.
\end{equation}	

We assume that the covariate values $x_i$ are one-dimensional, fixed and have been rescaled so that $x_i\in [0, 1]$. 
Partitioning-based regression methods are often invariant to monotone transformations of  observations. In particular, when $f_0$ is a step function,  standardizing the distance between the observations, and thereby the split points, has no effect on the nature of the estimation problem. Without loss of generality, we will thereby assume that the observations are aligned on an equispaced grid.
\begin{assumption}(Equispaced Grid) \label{ass:fixedgrid}
We assume that the {scaled} predictor values satisfy $x_i = \frac{i}{n}$ for  each $i = 1, \ldots, n$.
%\footnote{No observation at zero. Can be included by taking $x_i = (i-1)/(n-1), i = 1, \ldots, n$.}
\end{assumption}
 This assumption implies that partitions that are balanced in terms of the Lebesque measure  will be balanced also in terms of the number of observations.
 A similar assumption was imposed by Donoho \cite{Donoho1997} in his study of Dyadic CART.


The underlying regression function $f_0: [0, 1] \to \mathbb{R}$ is assumed to be a step function, i.e.
 $$
 f_0(x) = \sum_{k=1}^{K_{0}} \beta_k^0 \1_{\Omega_k^0}(x),
 $$ 
 where $\{\Omega_k^0\}_{k=1}^{K_{0}}$ is a partition of  $[0, 1]$ into $K_0$ non-overlapping intervals. {We assume that $\{\Omega_k^0\}_{k=1}^{K_{0}}$ is minimal, meaning that $f_0$ cannot be represented with a smaller partition (with less than $K_0$ pieces).}
 %For any given partition$\{\Omega_k\}_{k=1}^K$, we will denote by $\Omega_k=(\omega_k^{l},\omega_k^{r}]$ the $k^{th}$ cell of the partition. 
 Each partitioning cell $\Omega_k^0$ is associated with a step size $\beta_k^0$, determining the level of the function $f_0$ on $\Omega_k^0$. The entire vector of $K_0$ step sizes will be denoted by $\b^0=(\beta_1^0,\dots,\beta_K^0)'$.  
 
 
One might like to think of $f_0$ as a regression tree with $K_0$ bottom leaves. Indeed, every step function can be associated with an equivalence class of trees that  live on the same partition but differ in their tree topology. The number of bottom leaves $K_0$ will be treated as unknown throughout this paper.
Our goal will be designing a suitable class of priors on step functions so that the posterior concentrates tightly around $f_0$. Our  analysis  with a single predictor has served as a precursor to a full-blown analysis for high-dimensional regression trees \citep{Rockova2017}.
 
 
We consider an approximating space of all step functions (with $K=1,2,\dots$ bottom leaves)
\begin{equation}\label{shells}
\mathcal{F}=\cup_{K=1}^\infty\mathcal{F}_K,
\end{equation}
which consists of smaller  spaces (or shells) of all $K$-step functions
$$
\mathcal{F}_K=\left\{f_{\b}:[0,1]\rightarrow\R;  f_{\b}(x)=\sum_{k=1}^{K} \beta_k \1_{\Omega_k}(x)\right\},
$$
each indexed by a partition $\{\Omega_k\}_{k=1}^K$ and a vector of $K$ step heights $\b$. The fundamental building block of our theoretical analysis will be the prior  on $\mathcal{F}$. This prior distribution has three main ingredients, described in detail below, (a) a prior on the number of steps $K$, (b) a prior on the partitions $\{\Omega_k\}_{k=1}^K$ of size $K$, and (c) a prior on step sizes $\b=(\beta_1,\dots,\beta_K)'$.

 \vspace{-0.1cm}
 \subsection{Prior  $\pi_K(\cdot)$ on the Number of Steps $K$ }
To avoid overfitting, we assign an exponentially decaying prior distribution that penalizes partitions with too many jumps.
\begin{definition}(Prior on $K$)\label{prior_K}
The prior on the number of partitioning cells $K$ satisfies
\begin{equation}\label{priorK}
\pi_K(k)\equiv\Pi(K=k)\,\propto\, \exp(-c_K\, k\log k)\quad\text{for}\quad k=1,2,\dots.
\end{equation}
\end{definition}
 This prior is no stranger to non-parametric problems. It was deployed for stepwise  reconstructions of densities \cite{Scricciolo2007,Liu2015}  and regression surfaces \cite{coram}. When $c_K$ is large, this prior is concentrated on models with small complexity where overfitting should not occur. Decreasing $c_K$ leads to the smearing  of the prior mass over partitions with more jumps. This is illustrated in Figure \ref{fig:prior}, which depicts the prior for various choices of $c_K$. 
 %The optimal value of $c_K$ depends on number of jumps in the true regression function. 
 We provide recommendations for the choice of $c_K$ in {Section 3.1.}

\vspace{-0.3cm}
\begin{figure}[h]
  \centering
  \includegraphics[width = 0.45\textwidth]{Prior_piK.pdf}
   \includegraphics[width = 0.45\textwidth]{Complexity}
  \caption{(Left) Prior on the tree size for several values of $c_K$, (Right) Best approximations of $f_0$ (in the $\ell_2$ sense) by step functions supported on equispaced blocks  of size $K \in \{2, 5, 10\}$. }
  \label{fig:prior}
\end{figure}


\subsection{Prior  $\pi_{\Omega}(\cdot\C K)$ on Interval Partitions $\{\Omega_k\}_{k=1}^K$ }
After selecting the number of steps $K$ from $\pi_K(k)$, we  assign a prior over interval partitions $\pi_{\Omega}(\cdot\C K)$.
We will consider two important special cases.

\subsubsection{Equivalent Blocks}\label{sec:EB}
Perhaps the simplest  partition is based on statistically equivalent blocks \cite{anderson}, where all the cells are required to have the same number of points. This is also known as the $K$-spacing rule that partitions the unit interval using order statistics of the observations. 

\begin{definition}(Equivalent Blocks)\label{prior_EB}
Let $x_{(i)}$ denote the $i^{th}$ order statistic of $\x=(x_1,\dots,x_n)'$, where $x_{(n)}\equiv 1$ and $n=Kc$ for some $c\in\mathbb{N}\backslash\{0\}$. Denote by $x_{(0)}\equiv 0$. 
A partition $\{\Omega_k\}_{k=1}^K$ consists of $K$ equivalent blocks, when
$\Omega_k=(x_{(j_k)},x_{(j_{k+1})}]$, where $j_k=(k-1)c$.
\end{definition}
A variant of this definition can be obtained in terms of interval lengths rather than numbers of observations. 
\begin{definition}(Equispaced Blocks)\label{prior_EB2}
A partition $\{\Omega_k\}_{k=1}^K$ consists of $K$ {\sl equispaced blocks} $\Omega_k$, when
$
\Omega_k = \left(\frac{k-1}{K}, \frac{k}{K}\right]\quad\text{for}\quad k=1,\dots, K.
$\end{definition}
 When $K=2^s$ for some $s\in\N\backslash\{0\}$, the equispaced partition corresponds to a  full complete binary tree with splits at dyadic rationals. 
If the observations $x_i$ lie on a regular grid (Assumption \ref{ass:fixedgrid}), then Definition \ref{prior_EB} and \ref{prior_EB2} are essentially equivalent. We will thereby focus on equivalent blocks (EB) and denote such a partition  (for a given $K>0$) with $\bm{\Omega}_K^{EB}$. Because there is only one such partition for each $K$, the prior $\pi_{\Omega}(\cdot| K)$ has a single point mass mass at $\bm{\Omega}_K^{EB}$.
With $\bm{\Omega}^{EB}=\cup_{K=1}^\infty\bm{\Omega}_K^{EB}$ we denote the set of all EB partitions for $K=1,2,\dots$. We will use these partitioning schemes as a jump-off point.


\subsubsection{Balanced Intervals}\label{sec:balance}
Equivalent (equispaced) blocks are deterministic and, as such, do not provide much room for learning about the actual location of  jumps in $f_0$. Balanced intervals, introduced below, are a richer class of partitions that tolerate a bit more imbalance.
First, we introduce the notion of  cell counts $\mu(\Omega_k)$.  For each interval $\Omega_k$, we write
\vspace{-0.1cm}
\begin{equation}\label{cell:count}
\mu(\Omega_k)=\frac{1}{n}\sum_{i=1}^n\mathbb{I}(x_i\in\Omega_k),
\end{equation}
the proportion of observations falling inside $\Omega_k$.  
Note that for equivalent blocks, we can write $\mu(\Omega_1)=\cdots=\mu(\Omega_K)=c/n=1/K$.
%Equivalence blocks are deterministic and, as such, can be rather inflexible. We introduce more flexible partitions based on the notion of cell size.
%Of particular interest to us will be balanced partitions, as defined below.
\begin{definition}(Balanced Intervals)\label{ass:balance}
A partition $\{\Omega_k\}_{k=1}^K$ is {\sl balanced} if
\begin{equation}\label{eq:balance}
\frac{C^2_{min}}{K}\leq \mu(\Omega_k)\leq \frac{C^2_{max}}{K} \quad\text{for all}\quad k=1,\dots, K 
\end{equation}
 for some universal constants $C_{min}\leq 1\leq C_{max}$ not depending on $K$.
\end{definition}
The following variant of the balancing condition uses interval widths rather than cell counts:
${\widetilde{C}^2_{min}}/{K}\leq |\Omega_k|\leq {\widetilde{C}^2_{max}}/{K}$.  Again, under Assumption 1, these two definitions are equivalent. In the sequel, we will denote by $\bm{\Omega}^{BI}_K$ the set of all balanced  partitions consisting of $K$ intervals and by $\bm{\Omega}^{BI}=\cup_{K=1}^\infty\bm{\Omega}^{BI}_K$ the set of all balanced intervals of sizes $K=1,2,\dots$. 
It is worth pointing out that the balance assumption on the interval partitions can  be relaxed, at the expense of a log factor in the concentration rate \cite{Rockova2017}.

%Note that the equivalent blocks are balanced partitions with $C_{min}^2=C_{max}^2=1$.
%Equivalence blocks are simple, but do not provide much room for learning about the splits of the true partition $\{\Omega_k^0\}_{k=1}^{K_0}$.  Balanced partitions provide extra flexibility.


With balanced partitions, the $K^{th}$ shell  $\mathcal{F}_K$ of the approximating space  $\mathcal{F}$ in  \eqref{shells} consists of all step functions that are supported on partitions $\bm{\Omega}_{K}^{BI}$ and have  $K-1$ points of discontinuity $u_k\in I_n\equiv \{x_i:i=1,\dots,n-1\}$ for  $k=1,\dots K-1$. For equispaced blocks in Definition \ref{prior_EB2},  we assumed that  the points of subdivision were {\sl deterministic}, i.e. $u_k=k/K$. For balanced partitions, 
we assume that  $u_k$ are {\sl random} and chosen amongst the {\sl observed values $x_i$}.  
The order statistics of the vector of splits $\u=(u_1,\dots,u_{K-1})'$ uniquely  define a  segmentation of $[0,1]$ into $K$ intervals $\Omega_k=(u_{(k-1)},u_{(k)}]$, where  $u_{(k)}$ designates the $k^{th}$ smallest value in $\u$ and $u_{(0)}\equiv0, u_{(K)}=x_{(n)}\equiv1$. 
% To allow for more flexible step functions, we relax the assumption of deterministic dyadic splits by assuming that $\u$ arrived   from a  prior distribution $\pi_K(\u)$.  

Our prior over balanced  intervals $\pi_\Omega(\cdot\C K)$ will be defined implicitly through a {\sl uniform} prior  over  the split vectors $\u$.
Namely, the prior over balanced partitions $\bm{\Omega}^{BI}_K$ satisfies
\begin{equation}\label{prior_omega}
\pi_{\Omega}(\{\Omega_k\}_{k=1}^K\C K)=\frac{1}{\mathrm{card}(\bm{\Omega}^{BI}_K)} \1\left(\{\Omega_k\}_{k=1}^K\in\bm{\Omega}^{BI}_K\right).
\end{equation}
In the following Lemma, we obtain  upper bounds on $\mathrm{card}(\bm{\Omega}^{BI}_K)$ and discuss how they relate to an old problem in geometric probability.
In the sequel, we denote with $|\Omega_k|$  the lengths of the segments defined through the split points $\u$.


\begin{figure}[!t]
     \subfigure[$K=2$]{
     \begin{minipage}[t]{0.5\textwidth}
       \hskip-1pc  \scalebox{0.35}{\includegraphics{K2}}  \label{fig1:left}
    \end{minipage}}
     \subfigure[$K=3$]{
    \begin{minipage}[t]{0.5\textwidth}
    \hskip-1pc  \scalebox{0.35}{\includegraphics{K3}}     \label{fig1:right}
    \end{minipage}}
\caption{Two sets $E_K$ of possible stick lengths that satisfy the minimal cell-size condition $|\Omega_k|\geq C$ with $n=10, C=2/n$ and $K=2,3$.}\label{fig:1}
\end{figure}



\begin{lemma}\label{lemma:upper_cellsize}
Assume that $\u=(u_1,\dots, u_{K-1})'$ is a vector of independent random variables obtained by uniform sampling (without replacement) from $I_n$. 
%Denote by  $u_{(k)}$ the $k^{th}$ order statistic, where $u_{(0)}=0,u_{(K)}=1$, and by $\Delta$ the simplex of interval lengths $|\Omega_i|=u_{(i)}-u_{(i-1)}$ for $1\leq i\leq K$. 
%Then for given $1\leq i_1<\dots <i_k\leq n-1$, 
Then  under Assumption 1, we have for $1/n<C<1/K$
\vspace{-0.4cm}
\begin{align}
\Pi\left(\min_{1\leq k\leq K}|\Omega_{k}|\geq C \right)= \frac{{{\lfloor n(1-K\,C)\rfloor+K-1} \choose {K-1}}}{{{n-1}\choose {K-1}}}\label{prob_mincell}
\end{align}
and
\vspace{-0.3cm}
\begin{align}\label{prob_maxcell}
\Pi\left(\max_{1\leq k\leq K}|\Omega_{k}|\leq C \right)=1-
\sum_{k=1}^{\widetilde{n}}(-1)^k{n-1\choose k} \frac{{{\lfloor n(1-k\,C)\rfloor+K-1} \choose {K-1}}}{{{n-1}\choose {K-1}}},\end{align}
where $\widetilde{n}=\min\{n-1,\lfloor 1/C\rfloor\}$.
\end{lemma}
\vspace{-0.2cm}
\begin{proof}
The denominator of \eqref{prob_mincell} follows from the fact that there are $n-1$ possible splits for the $K-1$ points of discontinuity $u_k$.
The numerator is obtained after adapting the proof of Lemma 2 of Flatto and Konheim \cite{flatto_konheim}.
Without lost of generality, we will assume that $C=a/n$ for some $a=1,\dots,\lfloor n/K\rfloor$ so that $n(1-KC)$ is an integer. Because the  jumps $u_k$ can only occur on the grid $I_n$, we have
$|\Omega_k|=j/n$ for some $j=1,\dots,n-1$. It follows from Lemma 1 of Flatto and Konheim \cite{flatto_konheim} that the 
set $E_K=\{|\Omega_k|:\sum_{k=1}^{K}|\Omega_k|=1\,\,\text{and}\,\, |\Omega_k|\geq C\,\text{for}\,k=1,\dots,K\}$ lies in the interior of a convex hull of $K$ points
$v_r=(1-KC)e_r+C\sum_{k=1}^Ke_k$ for  $r=1,\dots, K$, where $e_r=(e_{r1},\dots,e_{rK})'$ are unit base vectors, i.e. $e_{rj}=\mathbb{I}(r=j)$.
Two examples of the set $E_K$ (for $K=2$ and $K=3$) are depicted in Figure \ref{fig:1}. In both figures,  $n=10$ (i.e. $9$ candidate split points)  and $a=2$. With $K=2$ (Figure \ref{fig1:left}),  there are  
only $7= {{n(1-KC)+K-1} \choose {K-1}}$ pairs of interval lengths $(|\Omega_1|,|\Omega_2|)'$ that satisfy the minimal cell condition. These points lie on a grid between the two vertices $v_1=(1-C,C)$ and $v_2=(C,1-C)$.
With $K=3$, the convex hull of points $v_1=(1-2C,C,C)',v_2=(C,1-2C,C)'$ and $v_1=(C,C,1-2C)'$ corresponds to a diagonal dissection of a cube of a side length $(1-3C)$ (Figure \ref{fig1:right}, again  with $a=2$ and $n=10$). 
The number of lattice points in the interior (and on the boundary) of such tetrahedron corresponds to an arithmetic sum $\frac{1}{2}{(n-3a+2)(n-3a+1)}={{n-3a+2}\choose 2}$. 
So far, we showed \eqref{prob_mincell} for $K=2$ and $K=3$.  To complete the induction argument, suppose that the formula holds for some arbitrary $K>0$.  Then  the size of the lattice inside (and on the boundary) of a 
$(K+1)$-tetrahedron of a side length $[1-(K+1)C]$ can be obtained by summing lattice sizes inside $K$-tetrahedrons of increasing side lengths {$0,\sqrt{2}/n, 2\sqrt{2}/n ,\dots,[1-(K+1)C]\sqrt{2}/n$}, i.e.
$$
\sum_{j=K-1}^{n[1-(K+1)C]+K-1}{j\choose K-1}={n[1-(K+1)C]+K \choose K},
$$
where we used the fact $\sum_{j=K}^N{j\choose K}={N+1 \choose K+1}$. The second statement \eqref{prob_maxcell} is obtained by writing the event as a complement of the union of events and applying  the method of inclusion-exclusion.$\qedhere$
\end{proof}
\begin{remark}
Flatto and Konheim \cite{flatto_konheim} showed that the probability of covering a circle with random arcs of length $C$ is equal to the probability that all segments of the unit interval, obtained with iid random uniform splits, are smaller than $C$. Similarly, the probability \eqref{prob_maxcell} could be related to the probability of covering the circle with random arcs whose endpoints are chosen from a grid of $n-1$ equidistant points on the circumference.
\end{remark}
\vspace{-0.3cm}
There are ${n-1\choose K-1}$  partitions of size $K$, of which  ${{\lfloor n(1-\widetilde{C}^2_{min})\rfloor+K-1} \choose {K-1}}$ satisfy the minimal cell  width balancing condition (where $\widetilde{C}^2_{min}>K/n$). This number gives  an upper bound on the combinatorial complexity of balanced partitions $\mathrm{card}(\bm{\Omega}^{BI}_K)$. 
%While this bound  is not tight enough to be particularly useful, we regard the connection to the circle circumference coverage very interesting. 


%Denote by $\mathcal{B}=\{\{\Omega_k\}_{k=1}^K: {C^2_{min}}/K\leq \mu(\Omega_k)\leq C^2_{max}/K\quad \forall k=1,\dots, K\}$ the set of all balanced partitions.
\vspace{-0.2cm}
\subsection{Prior $\pi(\b\C K)$ on Step Heights $\b$}
To complete the prior on $\mathcal{F}^K$,  we take independent normal priors on each of the coefficients. Namely
\begin{equation}\label{eq:prior_beta}
\pi(\b\C K)=\prod_{k=1}^K\phi(\beta_k),
\end{equation}
where $\phi(\cdot)$ is the standard normal density.



\section{Main Results}
A crucial ingredient of our proof will be understanding  how well one can approximate $f_0$ with other step functions (supported on partitions $\bm{\Omega}$, which are either equivalent blocks $\bm{\Omega}^{EB}$ or balanced partitions $\bm{\Omega}^{BI}$).  We will describe the approximation error in terms of the overlap between the true partition $\{\Omega^0_k\}_{k=1}^{K_0}$ and the approximating partitions $\{\Omega_k\}_{k=1}^K\in\bm{\Omega}$. More formally, we define the {\sl restricted cell count} (according to Nobel \cite{nobel}) as
$$
m\left(V; \{\Omega_k^0\}_{k=1}^{K_0}\right)=|\Omega_k^0: \Omega_k^0\cap V\neq \emptyset|,
$$
the number of cells in $\{\Omega_k^0\}_{k=1}^{K_0}$ that overlap with an interval $V\subset [0,1]$. Next, we define the {\sl complexity}  of $f_0$ as the smallest size of a partition in $\bm{\Omega}$ needed to completely cover $f_0$ without any overlap.

\begin{definition}(Complexity of $f_0$ w.r.t. $\bm{\Omega}$)\label{def:K0}
We define $K(f_0,\bm{\Omega})$ as the smallest $K$ such that there exists a $K$-partition $\{\Omega_k\}_{k=1}^K$ in the class of partitions $\bm{\Omega}$ for which
$$
m\left(\Omega_k; \{\Omega_k^0\}_{k=1}^{K_0}\right)=1\quad \text{for all} \quad k=1,\dots, K.
$$
 The number $K(f_0,\bm{\Omega})$ will be referred to as the {\sl complexity of $f_0$ w.r.t. $\bm{\Omega}$}. 
\end{definition}

%We considered two classes of partitions in Section 2, equivalent blocks $\bm{\Omega}_K^{EB}$ and balanced partitions $\bm{\Omega}_K^{BP}$
The complexity number $K(f_0,\bm{\Omega})$  indicates the optimal number of steps needed to approximate $f_0$  with a step function (supported  on partitions in $\bm{\Omega}$) without any error.  It depends on the true number of jumps  $K_0$ as well as the true interval lengths $|\Omega_k^0|$. {If  the minimal partition $\{\Omega_k^0\}_{k=1}^{K_0}$ resided in the approximating class, i.e. $\{\Omega_k^0\}_{k=1}^{K_0}\in\bm{\Omega}$, then we would obtain $K(f_0,\bm{\Omega})=K_0$,  the true number of steps.}
On the other hand, when  $\{\Omega_k^0\}_{k=1}^{K_0}\notin \bm{\Omega}$, the complexity number $K(f_0,\bm{\Omega})$ can be much larger. This is illustrated in Figure \ref{fig:prior} (right), where the true partition $\{\Omega_k^0\}_{k=1}^{K_0}$ consists of $K_0=4$ unequal pieces and we approximate it with equispaced blocks with $K=2,5,10$ steps. Because the intervals $\Omega_k^0$ are not equal and the smallest one has a length $1/10$, we need $K(f_0,\bm{\Omega}^{EB})=10$  equispaced blocks to perfectly approximate  $f_0$.  For our analysis, we  do not need to assume that $\{\Omega_k^0\}_{k=1}^{K_0}\in\bm{\Omega}$ (i.e. $f_0$ does not need to be inside the approximating class) or  that $K(f_0,\bm{\Omega})$ is finite. The complexity number can increase with $n$, where sharper performance is obtained when $f_0$ can be approximated error-free with  some $f\in\bm{\Omega}$, where $f$  has a small number of discontinuities relative to $n$. 
% {In essence, we assume that $f_0$ can be represented as  an element of one of the approximating classes.}

%If $\{\Omega_k^0\}_{k=1}^{K_0}$ were balanced intervals, we would need only $K(f_0,\bm{\Omega}^{BI})=4$  pieces.  While balanced partitions  have foa larger combinatorial complexity than EB's (Lemma \ref{lemma:upper_cellsize}), we need fewer steps if $f_0$ is balanced and not too jumpy. <- COMMENT STEPHANIE: This part confused me so I thought it would be best to leave it our for now. 

 \iffalse
\begin{figure}[t]
  \centering
  \includegraphics[width = \textwidth]{K0.pdf}
  \caption{Best approximations of $f_0$ (in the $\ell_2$ sense) by step functions supported on equal blocks  of size $K \in \{2, 5, 10\}$.}
  \label{fig:K0}
\end{figure}
\fi

\iffalse
\begin{assumption}\label{ass:existence}
Let $\bm{\Omega}$ be a class of approximating partitions. We assume that $f_0$ is $\bm{\Omega}$-valid, i.e.  $K(f_0,\bm{\Omega}) < \sqrt{n}$.
\end{assumption}
\fi

Another way to view $K(f_0,\bm{\Omega})$ is as the ideal partition size on which the posterior should concentrate. If this number were known, we could achieve a near-minimax posterior concentration rate $n^{-1/2}\sqrt{K(f_0,\bm{\Omega})\log[n/K(f_0,\bm{\Omega})]}$ (Remark \ref{remark}).  The actual minimax rate for estimating a  piece-wise constant $f_0$ (consisting of $K_0>2$ pieces) is $n^{-1/2}\sqrt{K_0\log (n/K_0)}$ \cite{Gao}. 
In our main results, we will target the nearly optimal rate expressed in terms of $K(f_0,\bm{\Omega})$.  

\subsection{Posterior Concentration for Equivalent Blocks}
Our first result shows that the minimax rate is nearly achieved, without any assumptions on the number of pieces of $f_0$ or the sizes of the pieces. 

%Thus, we are able to recover the true function almost as well as if we knew $K_0$. 
 
 

\begin{theorem}(Equivalent Blocks)\label{thm:main_step}
Let $f_0 : [0,1] \to \mathbb{R}$ be a step function with $K_0$ steps, where $K_0$ is unknown. Denote by $\mathcal{F}$ the set of all step functions supported on equivalent blocks, equipped with priors $\pi_K(\cdot)$  and $\pi(\b \mid K)$ as in \eqref{priorK} and \eqref{eq:prior_beta}.  Denote with $K_{f_0}\equiv K(f_0,\bm{\Omega}^{EB})$ and assume $\|\b^0\|^2_\infty\lesssim\log n$ and $K_{f_0}\lesssim \sqrt{n}$. Then, under Assumption \ref{ass:fixedgrid}, we have 
\begin{equation}\label{eq:conc}
\Pi\left( f \in \mathcal{F}: \|f - f_0\|_n \geq M_n n^{-1/2}\sqrt{K_{f_0}\log{(n/K_{f_0})}} \mid \Y^{(n)}\right) \to 0
\end{equation}
in $P_{f_0}^n$-probability, for every $M_n \to \infty$ as $n \to \infty$.
\end{theorem}

Before we proceed with the proof,  a few remarks ought to be made. First, it is worthwhile to emphasize that the statement in Theorem \ref{thm:main_step} is a frequentist one as it relates to an aggregated behavior of the posterior distributions obtained under the true generative model $P_{f_0}^n$. 

Second, the theorem shows that the Bayesian procedure performs an automatic adaptation to $K(f_0,\bm{\Omega}^{EB})$. The posterior will concentrate on  EB partitions that are fine enough to approximate $f_0$ well.  Thus, we are able to recover the true function as well as if we knew $K(f_0,\bm{\Omega}^{EB})$.  % The larger log-factor is the price we pay for learning $K(f_0,\bm{\Omega}^{EB})$ from the data.

Third, it is worth mentioning that, under Assumption \ref{ass:fixedgrid},  Theorem \ref{thm:main_step} holds for equivalent as well as equisized blocks. In this vein, it describes the speed of posterior concentration for {\sl dyadic regression trees}. Indeed, as mentioned previously, with  $K=2^s$ for some $s\in\mathbb{N}\backslash\{0\}$, the equisized  partition corresponds to a full binary tree with splits at dyadic rationals. 


Another interesting insight is that the Gaussian prior \eqref{eq:prior_beta}, while selected for mathematical convenience, turns out to be sufficient for optimal recovery.  In other words, despite the relatively large amount of {mass  near zero}, the Gaussian prior does not rule out optimal posterior concentration. Our standard normal prior is a simpler version of the Bayesian CART prior,  which determines the variance from the  data \cite{Chipman1998}. 


%There is no need to `believe' in a prior - the Bayesian paradigm is studied here as an intuitive way to come up with estimators, which are then evaluated under the assumption that there is a true function generating the data.



{Let $K_{f_0}\equiv K(f_0,\bm{\Omega}^{EB}) $ be as in Definition \ref{def:K0}. Theorem \ref{thm:main_step} is proved by verifying the three conditions of Theorem 4 of \cite{Ghosal2007}, for {$\e_n = n^{-1/2}\sqrt{K_{f_0}\log(n/K_{f_0})}$} and $\mathcal{F}_n = \bigcup_{K=0}^{k_n} \mathcal{F}_K$, with $k_n$ of the order {$K_{f_0}{\log(n/K_{f_0})}$}. The approximating subspace $\mathcal{F}_n\subset\mathcal{F}$ should be rich enough to approximate  $f_0$ well and it should receive most of the prior mass. The conditions for posterior contraction at the rate $\e_n$ are:
\begin{enumerate}[label=({C\arabic*})]
\item \label{GV1} $\displaystyle \sup_{\e > \e_n} \log N\left(\tfrac{\e}{36}, \{f \in \mathcal{F}_n : \|f - f_0\|_n < \e\}, \|.\|_n\right) \leq n\e_n^2,$
\item \label{GV2} $\displaystyle \frac{\Pi(\mathcal{F} \backslash \mathcal{F}_n)}{\Pi(f \in \mathcal{F} : \|f-f_0\|_n^2 \leq \e_n^2)} = o(e^{-2n\e_n^2}),$
\item \label{GV3} $\displaystyle \frac{\Pi(f \in \mathcal{F}_n : j\e_n < \|f - f_0\|_n \leq 2j\e_n)}{\Pi(f \in \mathcal{F} : \|f-f_0\|_n^2 \leq \e_n^2)} \leq e^{\frac{j^2}{4}n\e_n^2}$ for all sufficiently large $j$.
\end{enumerate}


 %The three conditions make different demands on the size of $\mathcal{F}_n$ and the prior. 
 The entropy condition \ref{GV1} restricts  attention to EB partitions with small $K$. As will be seen from the proof, the largest allowed partitions   have at most (a constant multiple of) $K_{f_0}\log{(n/K_{f_0})}$ pieces.}. 

Condition \ref{GV2} requires that the prior does not promote partitions with  more than $K_{f_0}\log{(n/K_{f_0})}$ pieces. This property is guaranteed by the exponentially decaying
%The maximum order of $\log{n}$ allowed by \ref{GV1} is exactly needed by \ref{GV2}, which requires there to be a negligible amount of mass on trees larger than $\log{n}$. 
 prior $\pi_K(\cdot)$, which  penalizes large partitions. 
  %We find that this property holds regardless of the  tuning parameter $c_K$. Nevertheless, based on a more detailed study of the numerator in \ref{GV2} for dyadic splits\footnote{Why only dyadic splits?}, 



The final condition, \ref{GV3}, requires that the prior charges a $\|.\|_n$ neighborhood of the true function. In our proof, we verify this condition by showing that the prior mass on step functions of the optimal size {$K_{f_0}$} is sufficiently large.







%The rate is not uniform, as the constants involved depend on $f_0$. 
\begin{proof}
\vspace{-0.15cm}
We verify the three conditions \ref{GV1}, \ref{GV2} and \ref{GV3}. 

\paragraph{\ref{GV1}}
\vspace{-0.3cm}
Let $\e > \e_n$ and  $K \in \mathbb{N}$. For $f_{\bm{\alpha}},f_{\b} \in \mathcal{F}_{K}$, we have $K^{-1}\|\bm{\alpha} - \b\|_2^2 = \|f_{\bm{\alpha}} - f_{\b}\|_n^2$ because $\mu(\Omega_k) = 1/K$ for each $k$. We now argue as in the proof of Theorem 12 of \cite{Ghosal2007} to show that  $N\left(\tfrac{\e}{36}, \{f \in \mathcal{F}_K : \|f - f_0\|_n < \e\}, \|.\|_n\right)$ can be covered by the number of $\sqrt{K}\e/36$-balls required to cover a $\sqrt{K}\e$-ball in $\R^{K}$. This number is bounded above by $108^K$. Summing over $K$, we recognize a geometric series. Taking the logarithm of the result, we find that \ref{GV1} is satisfied if $\log(108)(k_n+1) \leq n\e_n^2$.

\paragraph{\ref{GV2}}
 We  bound the denominator by:
\begin{align*}
\Pi(f \in \mathcal{F} : \|f-f_0\|_n^2 \leq \e^2)
&\geq \pi_K(K_{f_0})\Pi\left(\b \in \mathbb{R}^{{K(f_0)}} : \|\b - \b^{\rm ext}_0\|_2^2 \leq \e^2 K_{f_0} \right),
\end{align*}
where $\b_0^{\rm ext} \in \mathbb{R}^{K_{f_0}}$ is an extended version of $\b^0 \in \mathbb{R}^{K_0}$, containing the coefficients for $f_0$ expressed as a step function on the partition $\{\Omega_k^0\}_{k=1}^{K_{f_0}}$. This can be bounded from below by
\begin{equation*}
\frac{\pi_K(K_{f_0})}{e^{\|\b^{\rm ext}_0\|_2^2/2}}\Pi\left(\b \in \mathbb{R}^{{K(f_0)}}:\|\b\|^2_2\leq  \e^2 K_{f_0}/2\right)> \frac{\pi_K(K_{f_0})}{e^{\|\b^{\rm ext}_0\|_2^2/2}} \int_0^{\e^2 K_{f_0}/2} 
\frac{x^{K_{f_0}/2 -1} e^{-x/2}}{2^{K_{f_0}/2}\Gamma(K_{f_0}/2)}dx.
\end{equation*}
We bound this from below by bounding the exponential at the upper integration limit, yielding:
\begin{equation}\label{eq:lowerboundball}
%\frac{\pi_K(K_{f_0}) K(f_0)^{K(f_0)/2 - 1}}{2^{{3K(f_0)/2} - 1} e^{\|\b^{\rm ext}_0\|_2^2}\Gamma(K(f_0)/2)} e^{-\e^2K(f_0)/4} \e^{K(f_0)}.
 \frac{\pi_K(K_{f_0})}{e^{\|\b^{\rm ext}_0\|_2^2/2}}\frac{e^{-\varepsilon^2K_{f_0}/4}}{2^{K_{f_0}}\Gamma(K_{f_0}/2+1)}\varepsilon^{K_{f_0}}K_{f_0}^{K_{f_0}/2}. 
\end{equation}
For $\e = \e_n \to 0$, we thus find that the denominator in  \ref{GV2} can be lower bounded with $e^{K_{f_0}\log \e_n-c_K\,K_{f_0}\log K_{f_0}-\|\b_0^{ext}\|^2_2/2-K_{f_0}/2[\log 2+\e_n^2/2]}$. We bound the numerator:
\begin{align*}
\Pi(\mathcal{F} \backslash \mathcal{F}_n) &= \Pi\left( \bigcup_{k = k_n + 1}^\infty \mathcal{F}_k \right)
\propto \sum_{k=k_n + 1}^\infty e^{-c_Kk\log{k}}
\leq e^{-c_K(k_n+1)\log(k_n+1)} + \int_{k_n + 1}^\infty e^{-c_Kx\log{x}},
\end{align*}
which is of order {$e^{-c_K(k_n+1)\log(k_n+1)}$}. 
Combining this bound with \eqref{eq:lowerboundball}, we find that \ref{GV2} is met if:
\begin{equation*}
e^{-K_{f_0}\log{\e_n}+(c_K+1)\,K_{f_0}\log K_{f_0}+K_{f_0}\|\b^0\|_\infty^2 - c_K(k_n+1)\log(k_n+1) + 2n\e_n^2} \to 0 \text{ as } n \to \infty.
\end{equation*}



\paragraph{\ref{GV3}}
We bound the numerator by one, and use the bound \eqref{eq:lowerboundball} for the denominator. As  $\e_n \to 0$, we obtain the condition {${-K_{f_0} \log{\e_n}}+(c_K+1)K_{f_0}\log K_{f_0}+K_{f_0}\|\b^0\|_{\infty}^2 \leq {\frac{j^2}{4}n\e_n^2}$} for all sufficiently large $j$.

\paragraph{Conclusion}
% \ref{GV1} holds if $\log(108)(k_n+1) \leq n\e_n^2$, \ref{GV2} holds if $K_{f_0}\log{\e_n} + c_Kk_n\log{k_n} - 2n\e_n^2 \to \infty$, and \ref{GV3} holds if there exists a $j \in \mathbb{N}$ such that $-K_{f_0}\log{\e_n} \leq \frac{j^2}{4}n\e_n^2$.
With {$\e_n = n^{-1/2}\sqrt{K_{f_0}\log(n/K_{f_0})}$}, letting $k_n\propto n\e_n^2=K_{f_0}\log(n/K_{f_0})$, the condition \ref{GV1} is met. With this choice of $k_n$, the condition {\ref{GV2}} holds as well as long as $\|\b^0\|_\infty^2\lesssim \log n$ and $K_{f_0}\lesssim\sqrt{n}$.
Finally, the condition \ref{GV3} is met for $K_{f_0}\lesssim\sqrt{n}$. $\qedhere$ 


\end{proof}
\vspace{-0.3cm}
%\st{ \subsubsection{Pruning the Trees: Recommendations for the Prior on K}\label{sec:priorK}
%Inspection of the proof of Theorem \ref{thm:main_step} gives more insight into the suitability of a range of priors on $K$, effectively answering the question how agressively the tree should be pruned. 
\begin{remark}It is worth pointing out that the proof will hold for a larger class of priors on $K$, as long as the prior shrinks at least exponentially fast (meaning that it is bounded from above by $ae^{-bK}$ for constants $a, b > 0$). However, a prior at this exponential limit will require tuning, because the optimal $a$ and $b$ will depend on $K(f_0,\bm{\Omega}^{EB})$. We recommend using the prior \eqref{prior_K} that prunes somewhat more aggressively,  because it does not require tuning by the user. Indeed, Theorem \ref{thm:main_step} holds  regardless of the choice of $c_K>0$. We conjecture, however,  that values $c_K \geq 1/K(f_0,\bm{\Omega}^{EB})$ lead to a faster concentration speed and we suggest $c_K=1$ as a default option.
\end{remark}
\begin{remark}\label{remark}
When $K_{f_0}$ is known, there is no need for assigning a prior $\pi_K(\cdot)$ and the conditions  \ref{GV1} and  \ref{GV3} are verified similarly as before, fixing the number of steps at $K_{f_0}$.
\end{remark}
\subsection{Posterior Concentration for Balanced Intervals}
An analogue of Theorem \ref{thm:main_step} can be obtained for balanced partitions from Section \ref{sec:balance} that correspond to regression trees with splits at actual observations. Now, we assume that $f_0$ is $\bm{\Omega}^{BI}$-valid and carry out the proof with $K(f_0,\bm{\Omega}^{BI})$ instead of $K(f_0,\bm{\Omega}^{EB})$. 
%{We replace the prior \eqref{priorK}  by a bit sharper prior, because balanced intervals have more flexibility to capture $f_0$  with  fewer pieces. 
The posterior concentration rate is only slightly worse.

\begin{theorem}(Balanced Intervals)\label{thm:main_step_BI}
Let $f_0 : [0,1] \to \mathbb{R}$ be a step function with $K_0$ steps, where $K_0$ is unknown. Denote by $\mathcal{F}$ the set of all step functions supported on balanced intervals equipped with priors  $\pi_K(\cdot), \pi_{\Omega}(\cdot| K)$  and $\pi(\b \mid K)$ as in \eqref{priorK}, \eqref{prior_omega} and \eqref{eq:prior_beta}.  Denote with $K_{f_0}\equiv K(f_0,\bm{\Omega}^{BI})$ and assume $\|\b^0\|^2_\infty\lesssim\log^{2\beta} n$ and $K(f_0,\bm\Omega^{BI})\lesssim \sqrt{n}$. Then, under Assumption \ref{ass:fixedgrid}, we have 
\begin{equation}\label{eq:conc}
\Pi\left( f \in \mathcal{F}: \|f - f_0\|_n \geq M_n n^{-1/2}\sqrt{K_{f_0}\log^{2\beta}(n/K_{f_0})} \mid \Y^{(n)}\right) \to 0
\end{equation}
 in $P_{f_0}^n$-probability, for every $M_n \to \infty$ as $n \to \infty$, where  $\beta>1/2$.
\end{theorem}
\begin{proof}
All three conditions \ref{GV1}, \ref{GV2} and  \ref{GV3} hold if we choose $k_n\propto K_{f_0}[{\log (n/K_{f_0}})]^{2\beta-1}$. % such that $k_n\log (n/k_n)\preceq n\,\varepsilon_n^2$. 
The entropy condition will be satisfied when
$\log \left(\sum_{k=1}^{k_n} C^k\mathrm{card}(\bm{\Omega}_k^{BI})\right)\lesssim n\,\varepsilon_n^2$ for some $C>0$, {where $\e_n = n^{-1/2}\sqrt{K_{f_0}\log^{2\beta}(n/K_{f_0})}$}.  Using the upper bound $\mathrm{card}(\bm{\Omega}_k^{BI})<{n-1\choose k-1}<{n-1\choose k_n-1}$ (because $k_n<\frac{n-1}{2}$ for large enough $n$), the condition  \ref{GV1} is verified. Using the fact that $\mathrm{card}(\bm\Omega_{K_{f_0}})\lesssim K_{f_0}\log (n/K_{f_0})$, the condition \ref{GV2} will be satisfied when, for some $D>0$, we have
\begin{equation}
e^{-K_{f_0}\log{\e_n}+(c_K+1)\,K_{f_0}\log K_{f_0}+D\,K_{f_0}\log (n/{K_{f_0}})+ K_{f_0}\|\b^0\|_\infty^2 - c_K(k_n+1)\log(k_n+1) + 2n\e_n^2} \to 0. \label{C2eq}
\end{equation}
This holds for our choice of $k_n$ under the assumption $\|\b^0\|^2_\infty\lesssim \log^{2\beta} n$ and $K_{f_0}\lesssim \sqrt{n}$. These choices also yield \ref{GV3}. 
%Because $K_{f_0}<\infty$ we have (for large enough $n$) $\log \mathrm{card}(\bm{\Omega}^{BI}_{K_{f_0}})<\log \mathrm{card}(\bm{\Omega}^{BI}_{k_n})\lesssim k_n\log (n/k_n)$. Because $k_n\log n$ is the leading term,  \eqref{C2eq} holds for $c_K>3$.  Condition \ref{GV3} is also satisfied for $k_n\propto (\log n)^{2\beta-1}$.
\end{proof}

\begin{remark}\label{remark}
When $K_{f_0}\gtrsim\sqrt n$, Theorem \ref{thm:main_step} and  Theorem \ref{thm:main_step_BI} still hold, only with the bit slower slower concentration rate $n^{-1/2}\sqrt{K_{f_0}\log n}$.
\end{remark}



\section{Discussion}
We provided the first posterior concentration rate results for Bayesian non-parametric regression with step functions. We showed that under suitable complexity priors,  the Bayesian procedure adapts to the unknown aspects of the  target step function. Our approach can be  extended in three ways: (a) to smooth $f_0$ functions, (b) to dimension reduction with {high-dimensional} predictors, (c) to more general partitioning schemes that correspond to  methods like Bayesian CART and BART.  These three extensions are developed in our  followup manuscript {\cite{Rockova2017}}. 

 
\section{Acknowledgment}
This work was supported by the James S. Kemper Foundation Faculty Research Fund at the University of Chicago Booth School of Business.

\small

\bibliographystyle{unsrt} 
\begin{thebibliography}{10}

\bibitem{cart}
L.~{Breiman}, J.~H. {Friedman}, R.~A. {Olshen}, and C.~J. {Stone}.
\newblock {\em Classification and Regression Trees}.
\newblock Statistics/Probability Series. Wadsworth Publishing Company, Belmont,
  California, U.S.A., 1984.

\bibitem{breiman}
L.~Breiman.
\newblock Random forests.
\newblock {\em Mach. Learn.}, 45:5--32, 2001.

\bibitem{Berchuck2005}
A.~Berchuck, E.~S. Iversen, J.~M. Lancaster, J.~Pittman, J.~Luo, P.~Lee,
  S.~Murphy, H.~K. Dressman, P.~G. Febbo, M.~West, J.~R. Nevins, and J.~R.
  Marks.
\newblock Patterns of gene expression that characterize long-term survival in
  advanced stage serous ovarian cancers.
\newblock {\em Clin. Cancer Res.}, 11(10):3686--3696, 2005.

\bibitem{Nimeh2007}
S.~Abu-Nimeh, D.~Nappa, X.~Wang, and S.~Nair.
\newblock A comparison of machine learning techniques for phishing detection.
\newblock In {\em Proceedings of the Anti-phishing Working Groups 2nd Annual
  eCrime Researchers Summit}, eCrime '07, pages 60--69, New York, NY, USA,
  2007. ACM.

\bibitem{Razi2005}
M.~A. Razi and K.~Athappilly.
\newblock A comparative predictive analysis of neural networks ({NN}s),
  nonlinear regression and classification and regression tree ({CART}) models.
\newblock {\em Expert Syst. Appl.}, 29(1):65 -- 74, 2005.

\bibitem{Green2012}
D.~P. Green and J.~L. Kern.
\newblock Modeling heterogeneous treatment effects in survey experiments with
  {B}ayesian {A}dditive {R}egression {T}rees.
\newblock {\em Public Opin. Q.}, 76(3):491, 2012.

\bibitem{Polley2010}
E.~C. Polly and M.~J. van~der Laan.
\newblock Super learner in prediction.
\newblock Available at: http://works.bepress.com/mark\_van\_der\_laan/200/,
  2010.

\bibitem{mondrian}
D.~M. Roy and Y.~W. Teh.
\newblock The {M}ondrian process.
\newblock In D.~Koller, D.~Schuurmans, Y.~Bengio, and L.~Bottou, editors, {\em
  Advances in Neural Information Processing Systems 21}, pages 1377--1384.
  Curran Associates, Inc., 2009.

\bibitem{Chipman1998}
H.~A. Chipman, E.~I. George, and R.~E. McCulloch.
\newblock Bayesian {CART} model search.
\newblock {\em JASA}, 93(443):935--948, 1998.

\bibitem{Denison1998}
D.~Denison, B.~Mallick, and A.~Smith.
\newblock A {B}ayesian {CART} algorithm.
\newblock {\em Biometrika}, 95(2):363--377, 1998.

\bibitem{Chipman2010}
H.~A. Chipman, E.~I. George, and R.~E. McCulloch.
\newblock {BART}: {B}ayesian {A}dditive {R}egression {T}rees.
\newblock {\em Ann. Appl. Stat.}, 4(1):266--298, 03 2010.

\bibitem{Ghosal2000}
S.~Ghosal, J.~K. Ghosh, and A.~W. van~der Vaart.
\newblock Convergence rates of posterior distributions.
\newblock {\em Ann. Statist.}, 28(2):500--531, 04 2000.

\bibitem{Zhang2004}
T.~Zhang.
\newblock Learning bounds for a generalized family of {B}ayesian posterior
  distributions.
\newblock In S.~Thrun, L.~K. Saul, and P.~B. Sch\"{o}lkopf, editors, {\em
  Advances in Neural Information Processing Systems 16}, pages 1149--1156. MIT
  Press, 2004.

\bibitem{Tang2014}
J.~Tang, Z.~Meng, X.~Nguyen, Q.~Mei, and M.~Zhang.
\newblock Understanding the limiting factors of topic modeling via posterior
  contraction analysis.
\newblock In T.~Jebara and E.~P. Xing, editors, {\em Proceedings of the 31st
  International Conference on Machine Learning (ICML-14)}, pages 190--198. JMLR
  Workshop and Conference Proceedings, 2014.

\bibitem{Korda2013}
N.~Korda, E.~Kaufmann, and R.~Munos.
\newblock Thompson sampling for 1-dimensional exponential family bandits.
\newblock In C.~J.~C. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.~Q.
  Weinberger, editors, {\em Advances in Neural Information Processing Systems
  26}, pages 1448--1456. Curran Associates, Inc., 2013.

\bibitem{Briol2015}
F.-X. Briol, C.~Oates, M.~Girolami, and M.~A. Osborne.
\newblock Frank-{W}olfe {B}ayesian quadrature: Probabilistic integration with
  theoretical guarantees.
\newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 28}, pages
  1162--1170. Curran Associates, Inc., 2015.

\bibitem{Chen2016}
M.~Chen, C.~Gao, and H.~Zhao.
\newblock Posterior contraction rates of the phylogenetic indian buffet
  processes.
\newblock {\em Bayesian Anal.}, 11(2):477--497, 06 2016.

\bibitem{Ghosal2007}
S.~Ghosal and A.~van~der Vaart.
\newblock Convergence rates of posterior distributions for noniid observations.
\newblock {\em Ann. Statist.}, 35(1):192--223, 02 2007.

\bibitem{Szabo2015}
B.~Szab\'o, A.~W. van~der Vaart, and J.~H. van Zanten.
\newblock Frequentist coverage of adaptive nonparametric {B}ayesian credible
  sets.
\newblock {\em Ann. Statist.}, 43(4):1391--1428, 08 2015.

\bibitem{Castillo2014}
I.~Castillo and R.~Nickl.
\newblock On the {B}ernstein von {M}ises phenomenon for nonparametric {B}ayes
  procedures.
\newblock {\em Ann. Statist.}, 42(5):1941--1969, 2014.

\bibitem{Rousseau2016b}
J.~{Rousseau} and B.~{Szabo}.
\newblock {Asymptotic frequentist coverage properties of {B}ayesian credible
  sets for sieve priors in general settings}.
\newblock {\em ArXiv e-prints}, September 2016.

\bibitem{Castillo_polya}
I.~Castillo.
\newblock Polya tree posterior distributions on densities.
\newblock {\em preprint available at
  \url{http://www.lpma-paris.fr/pageperso/castillo/polya.pdf }}, 2016.

\bibitem{Liu2015}
L.~Liu and W.~H. Wong.
\newblock Multivariate density estimation via adaptive partitioning (ii):
  posterior concentration.
\newblock arXiv:1508.04812v1, 2015.

\bibitem{Scricciolo2007}
C.~Scricciolo.
\newblock On rates of convergence for {B}ayesian density estimation.
\newblock {\em Scand. J. Stat.}, 34(3):626--642, 2007.

\bibitem{coram}
M.~Coram and S.~Lalley.
\newblock Consistency of {B}ayes estimators of a binary regression function.
\newblock {\em Ann. Statist.}, 34(3):1233--1269, 2006.

\bibitem{shepp}
L.~Shepp.
\newblock Covering the circle with random arcs.
\newblock {\em Israel J. Math.}, 34(11):328--345, 1972.

\bibitem{Feller68}
W.~Feller.
\newblock {\em {An Introduction to Probability Theory and Its Applications,
  Vol. 2, 3rd Edition}}.
\newblock Wiley, 3rd edition, January 1968.

\bibitem{Donoho1997}
D.~L. Donoho.
\newblock {CART} and best-ortho-basis: a connection.
\newblock {\em Ann. Statist.}, 25(5):1870--1911, 10 1997.

\bibitem{Rockova2017}
V.~Rockova and S.~L. van~der Pas.
\newblock Posterior concentration for {B}ayesian regression trees and their
  ensembles.
\newblock arXiv:1708.08734, 2017.

\bibitem{anderson}
T.~Anderson.
\newblock Some nonparametric multivariate procedures based on statistically
  equivalent blocks.
\newblock In P.R. Krishnaiah, editor, {\em Multivariate Analysis}, pages 5--27.
  Academic Press, New York, 1966.

\bibitem{flatto_konheim}
L.~Flatto and A.~Konheim.
\newblock The random division of an interval and the random covering of a
  circle.
\newblock {\em {SIAM} Rev.}, 4:211--222, 1962.

\bibitem{nobel}
A.~Nobel.
\newblock Histogram regression estimation using data-dependent partitions.
\newblock {\em Ann. Statist.}, 24(3):1084--1105, 1996.

\bibitem{Gao}
C.~Gao, F.~Han, and C.H. Zhang.
\newblock Minimax risk bounds for piecewise constant models.
\newblock {\em Manuscript}, pages 1--36, 2017.

\end{thebibliography}

\end{document}

\subsection{Proof of Theorem \ref{thm:main_step_BI} for Balanced Intervals}
The entropy condition will be satisfied when
$\sum_{k=1}^{k_n} C^k\mathrm{card}(\bm{\Omega}_k^{BI})\preceq n\,\varepsilon_n^2.$ Using the upper bound $\mathrm{card}(\bm{\Omega}_k^{BI})<{n-1\choose k-1}$ and assuming $k_n<\frac{n-1}{2}$, the entropy condition \ref{GV1} will be satisfied when $k_n \log(n/k_n)\preceq \log(n)$. The condition \ref{GV2} will be satisfied when
\begin{equation}
e^{-K(f_0)\log{\e_n} +\log \mathrm{card}(\bm{\Omega}^{BI}_{K(f_0)})- c_K(k_n+1)\log(k_n+1) + 2n\e_n^2} \to 0 \text{ as } n \to \infty. \label{C2eq}
\end{equation}
Because $K(f_0)<\infty$ we have $\log \mathrm{card}(\bm{\Omega}^{BI}_{K(f_0)})<\log \mathrm{card}(\bm{\Omega}^{BI}_{k_n})\preceq k_n\log (n/k_n)$. Choosing $k_n>1$ such that $k_n\log n-k_n\log k_n\sim\log n$, we have $k_n\rightarrow \infty$ and thereby \eqref{C2eq} holds. Condition \ref{GV3} is satisfied for $k_n\log (n/k_n)\sim \log n$.


