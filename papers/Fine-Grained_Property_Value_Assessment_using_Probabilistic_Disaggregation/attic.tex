
%
%Since pixel-level training data is not readily available, our method uses parcel-level data.
%
%
%Our results on real-world data showed re-aggregating our pixel-level distributions outperforms all baselines when estimating the parcel-level values. 
%
%This result held for both parcels and sub-parcels with known value.
%and (2) our distributions similarly outperform the baselines in when artificially merging parcels during training but evaluating on the unmerged parcels. 

%
%We demonstrate the ability to produce plausible estimations of the dollar value of each pixel in an overhead image of parcel of land.
%
%The proposed probabilistic disaggregation method outperforms baselines when re-aggregating the predictions back to the parcel level. 
%
%This result holds true when going from merged parcel to single parcel estimation as well.
%
%We also provided a simulated experiment using Sentinel-2 imagery where we show the proposed probabilistic method outperforms the baselines while being tested on pixel-wise value.

%, such as human population density estimation and species distribution.



\subsection{Multiplicative Version}

If we treat the output of our network as the $\log$ of some value, then we can represent multiplicative terms like this:  
\begin{equation}
    y_i = \exp\{\sum _{p_j\in r_i}\log f\left(p_j\right)\}.
\end{equation}

It may be desirable to have a multiplicative and an additive part:
\begin{equation}
    y_i = \exp\{\sum _{p_j\in r_i}\log f\left(p_j\right)\} (
    \sum _{p_j\in r_i} g\left(p_j\right)\}).
\end{equation}


\subsection{Poisson Version}

\cga{}{It might be we don't want to do this. The poisson doesn't allow us to control variance.Ben worries it might not work well.}

\nbj{}{If we assume that each pixel is independent this is really easy. The independence assumption might be a bad one but maybe it isn't such a problem because our CNN will be able to make the per-pixel outputs conditionally independent. If so, we should probably write that up.}

Given $c_1 \sim \mathsf{Poisson}(\lambda_1)$ and 
 $c_2 \sim \mathsf{Poisson}(\lambda_2)$, then $c_1+c_2 \sim \mathsf{Poisson}(\lambda_1 + \lambda_2)$.

This means that we can redefine the output of our CNN as the $\lambda$ value of a Poisson distribution, one for each pixel.  Then we can represent aggregated observation as follows: 
\begin{equation}
    y_i \sim \mathsf{Poisson}(\sum_{p_j\in r_i}\hat{\lambda}\left( p_j\right)).
\end{equation}
We can then optimize the full system as follows:
\begin{equation}
    \argmin_\Theta - \sum \log 
    \mathsf{Poisson}\left(y_i; \sum_{p_j\in r_i}\hat{\lambda}( p_j; \Theta)\right).
\end{equation}




Open questions:
\begin{itemize}
    
    \item Is there anything interesting about how this generalizes to nonlinear $f$?
    
\end{itemize}




\subsection{Other Versions}

\begin{itemize}
    \item We can do the above for the Gaussian as well. \todo{look at this \url{https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/Chapter7.pdf} to see if there are others}
    \item It might work better if we do this using the rsample trick. That will at least allow us to have different underlying distributions.
\end{itemize}

%%% orignal log likelihood equation
\begin{equation}
    \argmin_\Theta \sum_{i} -\log 
    \mathsf{\mathcal{N}}\left(y_i; \sum_{p_j\in r_i}\hat{\mu}( p_j; \Theta),\sum_{p_j\in r_i}\hat{\sigma^{2}}( p_j; \Theta)\right).
\end{equation}

\subsection{Spatial Region Combination Experiments}
Some workable augmentations were used for this methodology. We are limited by the problem statement in this area in a few ways. For one, we must preserve the quality that all regions within an image are fully contained within the image. We cannot learn from regions not fully contained in the image. Therefore the augmentations: resizing, rotation, and cropping cannot be performed. Augmentations used were random flipping and image normalization.
    
% Log likelihood tables

 \begin{tabular}{||l | c | c | c ||} 
 \hline
 Method  & Mean Absolute Error & Mean Absolute Percentage Error & Mean Log Likelihood  \\ 
 \hline\hline
 Size Estimation & $\$ 134,432$ & $43.13\%$
& NA \\
\hline
Uniform Labels & $ \$ 114,382$ & $33.44\% $ & NA\\ 
 \hline
 Linear Baseline & $ \$86,427$ & $26.77\% $ & NA \\ 
 \hline
 Analytical Approach & $\$102,024$ & $32.17\% $ & $ -25670 $ \\
 \hline
 Sampling Approach & $\$78,258$ & $25.07\% $ & $ -187713 $ \\ 
% \\https://www.overleaf.com/project/5c5cc37c6c4e656b198e26a4
 \hline
\end{tabular}

\begin{tabular}{||l | c | c | c||} 
 \hline
 Method  & Mean Absolute Error & Mean Absolute Percentage Error & Mean Log Likelihood  \\ [0.5ex] 
 \hline\hline
 Uniform Labels & $\$96,453 $  & $30.11\%$ & NA \\
 \hline
 Linear Baseline & $\$101,171$  & $31.36\%$  & NA\\ 
 \hline
 Analytical Approach & $\$102,833$ & $32.36\%$ & $ -13194 $ \\
 \hline
 Sampling Approach & $\$98,328$ & $30.75\%$ & $ -19911$\\ 
 \hline
\end{tabular}

. Models optimizing the analytical approach predicted larger means and variances than the sampling method. We show in \ref{tab:results} the analytical results in high error when taking the means as value predictions. We attribute this to this method not directly optimizing for error itself, rather the probability of finding the target value in the predicted distributions.

% $\hat{y}(p_j, \Theta)$ corresponding to network weights $\Theta$:
% \begin{equation}
% \mathcal{J}(\theta) = \sum_{i=1}^{N}\left|y_i -\sum_{p_j\in r_i}\hat{y}( p_j; \Theta) \right|^2.
% \end{equation}
% % where $\{p_j\}$ are the set of pixels in region $r_i$, with ground truth value $y_i$.

%%%%%%%%% ORIGINAL SAMPLING METHOD
\subsubsection{\textcolor{red}{This section might be deprecated:} Sample-Based Approximation of Linear Baseline}
\label{sec:sampling}

Instead of creating a single parcel-level Gaussian distribution for the property value estimate, we can work directly with samples from the pixel-level distributions. The value distribution at a pixel $p_i$ is given by Equation \ref{eq:pixel-distribution}.

We take a random sample of each per-pixel distribution following the reparameterization trick, and optimize directly with Mean Squared Error (MSE), as we do for the deterministic baseline:
\begin{equation}
   \mathcal{L}_{MSE}=\argmin_\Theta \sum_{i=1}^{N}(y_i -\sum_{p_j\in r_i} v( p_j; \Theta) )^2
\end{equation}
This allows us to optimize directly for error in value estimation as in~\cite{jacobs2018weakly}, while still obtaining uncertainty estimates in the property value estimation through the generated distributions.

We incorporate entropy regularization to avoid the variance of the predicted distributions collapsing to 0 \cite{Kingma2014AutoEncodingVB}. This is accomplished by subtracting a constant times the sum of the entropies of each pixel distribution in a given input image. That is, the total loss is given by
\begin{align}
    \mathcal{L}_{\mathrm{Total}} &= \mathcal{L}_{MSE} - \lambda\sum_{i=1}^N \sum_{p_j \in r_i} \mathbb{H}\left(\hat{y}(p_j)\right) \\
    &= \mathcal{L}_{MSE} - \lambda \sum_{i=1}^N \sum_{p_j \in r_i} \frac{1}{2} \log\left(2\pi e \sigma_j^2\right).
\end{align}
In practice, we set $\lambda = 1e7$ so that the negative entropy is on the same order of magnitude as the mean squared error across all parcels in an image. We find that when analyzing this way, we obtain similar performance metrics to the analytical method, while predicting much lower average standard deviations.
%%%%%%%%%(END) ORIGINAL SAMPLING METHOD