% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl}
\input{macros.tex}
\usepackage{cleveref}
\usepackage{colortbl}
\usepackage{xspace,mfirstuc,tabulary}
\usepackage{url}
\usepackage{comment}

\usepackage{verbatim}
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}



\usepackage{placeins}
% \renewcommand{\UrlFont}{\ttfamily\small}
\renewcommand\ttdefault{cmvtt}

% Standard package includes
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\newcolumntype{a}{>{\columncolor{Gray}}c}
\newcolumntype{b}{>{\columncolor{white}}c}


\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand\para[1]{\vskip 1mm\noindent\textbf{#1}~}



\title{Combining Feature and Instance Attribution to Detect Artifacts}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
\newcommand{\email}[1]{\href{mailto:#1}{\tt #1}}
\author{Pouya Pezeshkpour\\
  University of California, Irvine \\
  \email{pezeshkp@uci.edu} \\ \And
Sarthak Jain\\
  Northeastern University \\
  \email{jain.sar@northeastern.edu} \\
\AND
 Sameer Singh \\
  University of California, Irvine \\
  \email{sameer@uci.edu}\\
  \And
Byron C. Wallace \\
  Northeastern University \\
  \email{b.wallace@northeastern.edu}
\\}


\begin{document}
\maketitle



\begin{abstract}
\blfootnote{\textcolor{red!80!black}{Warning: This paper contains examples with texts that might be considered offensive.}}

Training the deep neural networks that dominate NLP requires large datasets.
These are often collected automatically or via crowdsourcing, %resulting in imperfect annotations that 
and may exhibit systematic biases or \emph{annotation artifacts}.
By the latter we mean spurious correlations between inputs and outputs that do not represent a generally held causal relationship between features and classes; models that exploit such correlations may appear to perform a given task well, but fail on out of sample data. 
In this paper we evaluate use of different \emph{attribution} methods for aiding identification of training data artifacts. 
We propose new hybrid approaches that combine \emph{saliency maps} (which highlight ``important'' input features) with \emph{instance attribution} methods (which retrieve training samples ``influential'' to a given prediction). 
We show that this proposed \emph{training-feature attribution} can be used to efficiently uncover artifacts in training data when a challenging validation set is available.
%and we use it to identify previously unreported artifacts in 
%resulting in discovering previously unknown artifacts 
%and in case studies we report previously unknown artifacts 
%present in 
%a few standard NLP datasets.
We also carry out a small user study to evaluate whether %the degree to which 
these methods are useful to NLP researchers in practice, with promising results. 
We make code for all methods and experiments in this paper available.\footnote{\url{https://github.com/pouyapez/artifact_detection}}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

%Large pre-trained (masked) language models 
Deep networks dominate NLP applications %leaderboards 
and are being increasingly deployed in the real-world.
But what exactly are such models ``learning''?
One concern is that they may be exploiting \emph{artifacts} or spurious correlations between inputs and outputs that are present in the training data, but not reflective of the underlying task that the data is intended to represent. 
% \sarthak{give example of hate speech thingy here?}

% \byron{I think we need to get to the point faster here -- the question we're trying to get at (I think) is: To what extent are saliency and attribution methods (respectively) useful for finding artifacts? And does combining these via instance-feature attribution (proposed here) provide additional benefits? }
%A few recent works have used \emph{attribution methods} to detect artifacts \cite{han2020explaining, zhou2021feature}.

We assess the utility of \emph{attribution methods} for purposes of aiding practitioners in identifying training data artifacts, drawing inspiration from prior efforts that have suggested the use of attribution methods for this purpose~\cite{han2020explaining, zhou2021feature}.
Attribution methods are \emph{model-centric}; our evaluation of them for artifact discovery therefore complements recent work on \emph{data-centric} approaches~\cite{gardner2021competency}.
%\sarthak{Cite for studies using FA for artifacts} 
 %from interpretability literature 
 %attempt to identify artifacts used by deep models for text classification. 
%These approaches mostly divided into two categories:
We consider two families of attribution methods: (1) \emph{feature-attribution}, which highlight %where one highlights %ing part of target input to show their degree of importance
constituent input features (e.g., tokens) in proportion to their ``importance'' for an output \citep{ribeiro2016should,lundberg2017unified,adebayo2018sanity}, and; (2) \emph{instance attribution}, which retrieves training instances most responsible for a given prediction ~\cite{koh2017understanding,yeh2018representer, rajani2020explaining,pezeshkpour2021empirical}. %that had the most influence on the model's prediction 
 %,charpiat2019input,barshan2020relatif}. 
% 
% Here 
% We provide an in-depth analysis of attribution methods used for the express purpose of aiding practitioners in identifying training data artifacts. 
% For our goal of artifact detection, 


%%%%%%%%%%%%%%%%%
\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{attribution-overview2.pdf}
    \caption{Use of different attribution techniques for artifact discovery in train data. Here attribution methods can reveal inappropriate reliance on certain tokens (e.g., \emph{``!'', ``yo''}) to predict Tweet toxicity; these are artifacts.}%Model\sameer{text too small, reduce whitespace like around bullets} \byron{replaced with new for now, feel free to overrule (anyone)! also happy to edit.}}
    \label{fig:model}
    % \postspace{}
     \minipostspace{}
\end{figure}
%%%%%%%%%%

We also introduce new hybrid attribution methods that surface relevant \emph{features within train instances} as an additional means to probe what the model has distilled from training data.
This addresses inherent limitations of using either feature or instance attribution alone for artifact discovery. 
The former %cannot %cannot provide reasonable explanation if the model prediction depend on the 
%provide insights regarding 
can only highlight patterns within a given input, and the latter requires one to inspect entire (potentially lengthy) training instances to divine what might have rendered them influential.
%While these methods demonstrate appealing performance in explaining deep models, the question is: to what extent are they capable of guiding researchers in identifying artifacts?

%
%
%
% \byron{unclear what it would mean to `correctly interpret'; I think benefit of sticking to artifact discovery is that it is concrete(ish), so I would say something more along the lines of we aim to explore the degree to which feature and instance attribution methods expose artifacts}
% \byron{I think can shorten this by quite a bit; safe to assume audience will be familiar w/such methods broadly --- emphasize here their potential uses and shortcomings for spotting artifacts}
% 
% On one hand, 
%Feature attribution approaches tend to generate vague explanations; they usually highlight many features, and %leaves the interpretation to the human eye \citep{adebayo2018sanity}. 
%Furthermore, they 


% Consider Figure~\ref{fig:model}, a case in which a model might learn to erroneously associate African American Vernacular English (AAVE) with \emph{toxicity}, as observed by \citet{sap2019risk}. 
% For the test instance ``yo man what's up'', both input saliency and instance attribution methods provide some indication of this artifact.
% But combining these---in what we will call \emph{training-feature attribution}---most directly surfaces the artifact by highlighting that this prediction was apparently made on the basis of ``yo I'mma'' in a train instance labeled as toxic, immediately suggesting a problematic association.

Consider Figure~\ref{fig:model}. 
Here a model has learned to erroneously associate African American Vernacular English (AAVE) with \emph{toxicity} \cite{sap2019risk} and with certain punctuation marks (``!''). 
%Both of these are apparent artifacts; the former having been observed in prior work \cite{sap2019risk}, while the latter being novel.
%, an apparent additional artifact that we identify in this work.
For a hypothetical test instance ``yo! that's sick'', both input saliency and instance attribution methods may provide some indication of these artifacts.
But combining these via \emph{training-feature attribution} ($\metas$) can directly surface the punctuation artifact by highlighting ``!'' within a relevant training example (``shut up!''); this is not readily apparent from either input or instance attribution. 
Our goal in this work is to evaluate $\metas$ and other attribution methods as tools for identifying dataset artifacts. 
%Upon introducing $\metas$ and incorporating existing attribution methods, our goal here is to provide a tool allowing practitioners to deploy more robust and fairer models.  
% The main caveat to using attribution-based approaches is that their utility will depend on the specific validation set used to probe for supporting train instances and features.
% How best to efficiently construct such sets for purposes of artifact discovery constitutes a promising direction for future research.
% \sameer{this is the weak end to the intro.. end strong, maybe summarize main empirical results, or talk about how this work will change the future, not the limitations}
% \sameer{also, why do we talk about AAVE now? are we claiming TFA shows us those?}
%constructing useful sets for artifact 
% alone by highlighting ``!'' within an apparently relevant training example (``shut up!'').}
%that this prediction was apparently made on the basis of appearance of ``!'' an in a train instance labeled as non-toxic, immediately suggesting a problematic association.}
 

\para{Contributions.} The main contributions of this paper are as follows. 
% (1)~We empirically evaluate the applicability of attribution methods specifically for artifact detection on a variety of tasks and datasets. 
(1)~We propose a new hybrid attribution approach, $\meta$ ($\metas$), which addresses some limitations of existing attribution methods. 
% , and compare the combination of various feature and instance attribution methods to realize this in controlled synthetic and semi-synthetic settings.
(2)~We evaluate feature, instance and training-feature attribution for artifact detection on several NLP benchmarks with previously reported artifacts to evaluate whether and to what degree methods successfully recover these, and find that $\metas$ can outperform other methods. We also discover and report previously unknown artifacts on a few datasets. %, discovered via $\metas$. 
% \sameer{(3) sounds a lot like (1), please rephrase both to make sure they dound different..}
%be superior to alternatives.
%we find evidence that combining instance and feature attribution via $\metas$ yields superior performance to using either on its own.
%with respect to surfacing problematic (artifact) tokens.
%known artifacts  %: HANS NLI dataset \citep{mccoy2019right}, and IMDB binary sentiment classification task \citep{maas2011learning}.
%In these experiments we find that combining instance and feature attribution via $\metas$ yields superior performance with respect to surfacing problematic (artifact) tokens.
% \byron{I think below not really clear at this point -- what is `accuracy' I mean, I realize this refers to like artifact token recovery but will not be apparent to the reader here.}
%, outperforming existing attribution methods with up to 85.5\% gap in accuracy. 
%Assessing the practicality of attribution methods in real-world scenarios, 
%(4) %assess the utility of $\metas$ we 
% Applying $\metas$ and previously introduced attribution methods to additional widely used NLP datasets, 
%---%we discover several unknown artifacts in 
%BoolQ~\citep{clark2019boolq}, DWMW17~\citep{davidson2017automated}, and YAGO3-10~\citep{mahdisoltani2013yago3}---
%we identify and report previously unknown (as far as we aware) artifacts in these corpora.
%benchmarks. 
%triple  benchmarks identifying several existing artifact. 
% To further evaluate $\meta$, we study knowledge base triple classification task as a binary text classification, capturing the models reasoning in predicting specific target. 
Finally, (3)~we conduct a small user-study to evaluate $\metas$ for aiding artifact discovery in practice, and again find that combining feature and instance attribution is more effective at detecting artifacts than using either on its own. 
%we show that our $\metas$ method can better help users to identify artifacts in the data.
%conducting a user-study 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background and Notation}
%In this section, we briefly introduce some notations and required background on text classification and attribution methods. 
%Text classification is defined as a task of 
Assume a text classification setting where the aim is to fit a classifier $\phi$ that maps inputs $x_i \in \mathcal{X}$ to labels $y_i \in \mathcal{Y}$.
%We define 
Denote the training set by $\mathcal{D} = \{ z_i \}$ where $z_i = (x_i, y_i) \in \mathcal{X} \times \mathcal{Y}$. 
Each $x_i$ consists of a sequence of tokens $\{x_{i,1}, \dots, x_{i, n_i}\}$.
%In this work 
Here we define a linear classification layer on top of BERT \citep{devlin2019bert} as $\phi$, fine-tuning this on $\mathcal{D}$ to minimize cross-entropy loss $\mathcal{L}$.
%As the text classifier, we adopt pre-trained language model BERT \citep{devlin2019bert} as substrate and fine-tune it on the $\mathcal{D}$ using cross entropy loss function $\mathcal{L}$. 
%We represent the learned classifier as function $\phi$.
%To explain the text classifier's prediction, two families of attribution methods are commonly used:
Two types of attribution methods have been used in prior work to characterize the predictive behavior of $\phi$.

\para{Feature attribution methods} %that
highlight \emph{important features} (tokens) in a test sample $x_t$. %$z = (x, y)$
%incorporating detailed knowledge of model (white-box) or solely rely on model output (black-box). 
Examples of feature attribution methods include input gradients  ~\citep{sundararajan2017axiomatic, ancona2017towards}, and model-agnostic approaches such as LIME \citep{ribeiro2016should}. 
In this work, we consider only gradient-based feature attribution.  
% \sameer{we should mention we're not concerned with the latter? maybe not even mention it? also it's ungrammatical}

\para{Instance attribution methods} retrieve training samples $z_i$ deemed ``influential'' to the prediction made for a test sample $x_t$: $\hat{y}_t = \phi(x_t)$. 
Attribution methods assign scores to train instances $z_i$ intended to reflect a measure of importance with respect to $\hat{y}_t$: $I(\hat{y}_t, z_i)$.
Importance can reflect a formal approximation of the change in $\hat{y}_t$ when $z_i$ is upweighted~\cite{koh2017understanding} or can be derived via heuristic methods \cite{pezeshkpour2021empirical,rajani2020explaining}. 
%
While prior work has considered these attribution methods for ``train set debugging'' \cite{koh2017understanding,han2020explaining}, this relies on the practitioner to abstract away potential patterns within the influential instances.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Explaining Models with $\metab$}
\section{Artifact Detection and $\metab$}

%In this section, we first provide a %formal 
%definition for artifacts and then introduce training-feature attribution as a new family of attribution methods that aims to compensate for shortcomings inherent to existing attribution approaches.

\subsection{What is an \emph{Artifact}?}
\label{sec3.1}
% bcw: I think `granular' and `abstract' better better than `detailed' and `high-level', and `casual' better than `reasonable'; also swapped order
%To define artifacts we first %need to 
%revisit the definition of features of input data, and the correlation between features and the label corresponds to the input.   
Models will distill observed correlations between training inputs and their labels.
In practice, some of these correlations will be \emph{spurious}, by which we mean specific to the training dataset used.
Consider a particular feature function $f$ such that $f(x)$ is 1 if $x$ exhibits the feature extracted by $f$ and 0 otherwise, a \emph{training} distribution $\mathcal{D}$ over labeled instances $z$ (often assembled using heuristics and/or crowdsourcing), and an ideal, hypothetical \emph{target} distribution $\mathcal{D*}$ (the task we would actually like to learn; ``sampling'' directly from this is typically prohibitively expensive). 
Then we say that $f$ is a \textit{dataset} artifact if there exists a correlation between $y$ and $f(x)$ in $\mathcal{D}$, but not in $\mathcal{D}*$.
% \sameer{there is an underlying assumption that $f$ is \emph{simple} or at least interpretable?}
That is, if the mechanism by which one samples train instances induces a correlation between $f$ and labels that would not be observed in an idealized case where one samples from the ``true'' task distribution.\footnote{As a proxy for realizing this, imagine enlisting well-trained annotators with all relevant domain expertise to label instances carefully sampled i.i.d. from the distribution from which our test samples will actually be drawn in practice.}

A given model may or may not exploit a particular dataset artifact; in some cases a \emph{model-centered} view of artifacts may therefore be helpful.
To accommodate this, we can extend our preceding definition by considering the relationship between model predictions $\hat{p}(y|x)$ and true conditional distributions $p(y|x)$ under $D^*$; we are interested in cases where the former differs from the latter due to exploitation of a dataset artifact $f$.
Going further, we can ask whether this artifact was exploited \emph{for a specific prediction}.
%\textcolor{red!80!black}{In practice the question we want to answer is whether a model actually exploit dataset-centered artifacts or not? We define the notion of \textit{model-centered} artifacts, as a subset of \textit{dataset-centered} artifacts which have been exploited by the model, i.e., if there is a correlation between $\hat{p}(y|x)$ with $p(y|x)$ on the true distribution $D^*$. }
% \sameer{this definition is not model focused though, so artifacts would include things that the model ignores? I don't think we want that, right? Maybe instead of $D$, we compare $\hat{p}(y|x)$ with $p(y|x)$ on the true distribution $D^*$. This way, using attribution makes more sense. Also we want to include artifacts that are not just because of labeled data, but maybe from pretraining or something else, right? finally, if the artifact is correlated completely in $D$, we may not get any errors, so our recipe of using errors will not work.}
% % \begin{equation*}
% $\mathbb{E}_{(x,y) \sim \mathcal{D}} [f(x)|y=1] \neq \mathbb{E}_{(x,y) \sim \mathcal{D}}[f(x) | y=0]$
% % \end{equation*}

% % \noindent 
% \emph{And}

% % \begin{equation*} 
% $\mathbb{E}_{(x,y) \sim \mathcal{D*}}[f(x)|y=1] = \mathbb{E}_{(x,y) \sim \mathcal{D*}} [f(x)|y=0].$
% % \end{equation*} 
% \sameer{we should cite the Gardner paper, somewhere here? it sounds like we're introducing this definition, which either we have to argue for, or just say this is how it's already defined, followed by a cite}

In this work we consider two types of artifacts. %divide feature functions $f$---potential artifacts---into two types. 
\emph{Granular} input features refer to discrete units, such as individual tokens (this is similar to the definition of artifacts introduced in recent work by \citealt{gardner2021competency}). \emph{Abstract} features refer to higher-level \emph{patterns} observed in inputs, e.g., lexical overlap between the premise and hypothesis in the context of NLI \cite{mccoy2019right}.

%We divide an input features into two categories: 1) \emph{detailed}, features that appear in the form of a set of smallest units building the input (in this work we only focus on single unit artifacts), i.e., tokens in the textual domain, and 2) \emph{high-level}, features that capture specific patterns in the input such as the appearance of lexical overlap. 
%Similarly, we can divide existing correlations between features and labels into two groups: 1) \emph{reasonable correlations}, i.e., correlations carrying meaningful association between a feature and its corresponding label, example of this group of correlations can be associating appearance of the word \emph{good} and a review with the positive label. 2) \emph{Obstructive correlations}, i.e., correlations that do not carry meaningful association between a feature and its corresponding label, such as connecting the appearance of \emph{punctuation marks} with any specific class in the data. 

%We consider any feature (detailed or high-level) with obstructive correlation to a label as an artifact in the data. Moreover, any feature with reasonable correlation to a label that is being exploited excessively by the model to make its prediction is considered to be an artifact as well. 
%To measure model reliance on a specific feature, we incorporate the feature into inputs from other classes and observe the change in the model predictions.
%\vspace{-.5em}
\subsection{$\metab$}
Showing important training instances to users for their interpretation places the onus on them to determine \emph{what} was relevant about these instances, i.e., which %$f \in x_t$ 
features (granular or abstract) in $x_i$ were influential. %---(with respect to $x_t$). 
%For purposes of artifact detection---where %potentially counterintuitive and/or 
%undesirable associations may exist in the train set---
To aid artifact detection, it may be preferable to automatically highlight the tokens most responsible for the influence that train samples exert,
%That is, we would ideally
communicating \emph{what made an important example important}. 
This hybrid $\meta$ ($\metas$) can reveal patterns extracted from training data that influenced a test prediction, even where the test instance does not itself exhibit this pattern, whereas feature attribution can only highlight features within said test instance. 
% \sameer{feel like granular vs abstract should come up somewhere in this subsection, i.e. is one better for one than the other?}
%Directly presenting attributes of influential training samples for a given test instance---$\meta$ ($\metas$)---may be more useful than either feature or instance attribution alone for a few reasons. 
%It might reveal token-level patterns extracted from the training data that influenced a particular test prediction, even where the test instance does not itself contain these specific tokens; whereas feature attribution can only highlight tokens that appear in a test instance by construction. 
And unlike instance attribution, which retrieves entire train examples to be manually inspected (a potentially time-consuming and difficult task), $\metas$ may be able to succinctly summarize patterns of influence.

A high-level schematic of TFA is provided in Figure \ref{fig:guide}.
We aim to trace influence back to features within training samples. % to aid artifact discovery.
%In the original influence functions for ML paper,
% \citet{koh2017understanding} used gradients to identify influential features within a training point $z_i$: $\nabla_{x_i} \text{I}(x_t, z_i)$. 
We introduce $\meta$ %by extending this 
to extract influential features from training samples for a specific test prediction by considering a variety of combinations of feature and instance attribution and means of aggregating over these as $\metas$ variants. 
%To provide 
For example, one $\metas$ variant %that combines gradient and influence functions 
identifies features within the training point $x_i$ that informed the prediction for a test sample $z_t$ by %combine instance-level influence and feature gradients by 
taking the gradient of the influence with respect to inputs features, i.e., $\nabla_{x_i} \text{I}(z_t, x_i, y_i)$ \cite{koh2017understanding}.
% \sameer{this is a key contribution, but its really hidden in there}
%We apply these methods to problems in NLP specifically with the aim of identifying training data artifacts. 
%we refer to any such strategy as $\meta$ ($\metas$). % to denote an arbitrary combination of feature and instance attribution methods. 
After calculating the importance of features within a train sample for a test target, we either construct a heatmap to help users identify \textit{abstract} artifacts, or %incorporate aggregate strategies 
take aggregate measures over features (described below) to detect \textit{granular} artifacts 
% consider three approaches to aggregate attributions over different samples
and present them to users.\footnote{Many other strategies are possible, and we hope that this work motivates further exploration of such methods.} 


%Aggregating ``important'' train features over targeted data points may reveal granular artifacts, but it will not explicitly suggest abstract artifacts, this requires human interpretation of training instance heatmaps.
%\textcolor{red!80!black}{let us note that since aggregating important features over targeted data points is not useful in detecting abstract artifacts, we only consider aggregating for detecting granular artifacts, and apply the heatmap approach for detecting abstract ones.}
% \begin{table*}[]
%     \centering
%     \begin{tabular}{l}
%          \textbf{Test Example:}  \\
%          \textbf{Feature Attribution:} \\
%          \textbf{Training-Feature Attributions:} \\
%          \textbf{Aggregated Tokens:} \\
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:sec3-ex}
% \end{table*}

%We consider saliency based feature attribution methods in this work. These methods highlight tokens within influential examples to indicate what makes them influential. A representative example of this category is the gradient of influence as defined previously. Next, we describe some common ways these highlights can be used in to detect artifacts.
% \sameer{maybe talk about these in context of granular and abstract artifacts..}
\para{Heatmaps} We present the top and bottom $k$ influential examples to users with \emph{token highlights} communicating the relative importance of tokens within these $k$ influential train instances. 
This may allow practitioners to interactively, efficiently identify potentially problematic abstract artifacts.

\para{Aggregated Token Analysis} 
%One insight that 
Influence functions may implicitly reveal %is 
that the appearance of certain tokens in training points correlates with their influence.
%One common form of artifact pattern that heatmap analysis is that model prediction, and therefore influence score, depends on presence or absence of certain tokens within the example. 
%Such restricted form of pattern may be surfaced by
We might directly surface this sort of pattern by aggregating $\metas$ over a set of training samples. 
For example, for a given test instance, we can retrieve the top and bottom $k\%$ %(here, $k=10$) 
most influential training instances according to an instance attribution method. %$I$. 
We can then extract the top token from each of these instances using $\metas$, and sort resulting tokens based on frequency, surfacing tokens that appear disproportionately in influential train points.
%and aggregate them over the set of mentioned instances by sorting based on frequency of appearance. 
Returning to %the example of
toxicity detection, this might reveal that punctuation marks (such as ``!'') tend to occur frequently in influential examples, which may directly flag this behavior.% for a practitioner. 
%within the aggregated tokens, that may quickly indicate their importance to the user.

\para{Discriminator} One can also define model-based approaches to aggregate rankings of training points with respect to their influence scores.
%Given a ranking of train points according to influence, we are not limited to saliency-based methods. 
As one such method, we %propose to %pick top and bottom 10 percentile of influential examples (associated with a test example) and 
train a logistic regression (LR) model on top of Bag-of-Words representations to distinguish between the most and least influential examples, according to influence scores for a given test point. %most influential examples for a given test point. 
This will yield a weight for each token in our vocabulary; tokens associated with high weights are correlated with influence for the test point, and we can show them to the practitioner.


%%%%%%%%%%%%%%
% \subsection{Utilizing Attribution Methods for Artifact Discovery Guideline}
\section{A Procedure for Artifact Discovery}

%Now that we describe our $\metas$ methods which compensate the shortcomings of instance and feature attribution methods, we are ready to provide a systematic guideline to identify artifacts using attribution methods. 
%Having %proposed 
%argued that $\metas$ has the potential to address shortcomings of instance and feature attribution methods for artifact discovery, 
We now propose a procedure (Figure \ref{fig:guide}) one might follow to systematically use the above attribution methods to discover training artifacts.
% \sameer{again, this procedure looks a little different for abstract and granular, but we don't bring them up very prominently here.. maybe 3 and 4 should be ``3(a) \textbf{For Granular artifacts}: since we are interested in identifying tokens ...'' and ``3(b) \textbf{For abstract artifacts}: aggregation doesn't make sense...''}
%We provide an overview of our recipe in Figure \ref{fig:guide}, which comprises the following steps.%. 
%To identify existing artifacts in a benchmark we recommend to follow these steps:
% \begin{enumerate}[leftmargin=*]

\vspace{0.2em}
 \noindent (1) Construct a validation set, either using a standard split, or by intentionally constructing a small set of ``difficult'' samples. Constructing a useful (for dataset debugging) such set is the biggest challenge to using attribution-based approaches. %potentially challenging samples. 

 \vspace{0.2em}
\noindent (2) Apply feature-, instance-, and training feature attribution to examples in the validation set. Specifically, identify influential \emph{features} using feature attribution or $\metas$ and identify influential \emph{training instances} using instance attribution. 
    % \sameer{do nothing with instance?}

\vspace{0.2em}    
 \noindent (3-a) \textbf{Granular artifacts}: To identify %the 
    granular artifacts, aggregate the important features from the test points (via feature attribution) or from influential train points (using $\metas$) for all instances in the validation set to identify features that appear disproportionately.
    % \sameer{feature attribution doesn't apply here.. maybe combine last half of previous and this one, to say something like "identify important features", and mention that it could be from the test point (using FA) or from influential train points (using IA and TFA).}

\vspace{0.2em}
    % If the preceding does not yield obvious anomalies in terms of train feature influence, 
\noindent (3-b) \textbf{Abstract artifacts}: Inspect the ``heatmaps'' of influential instances for validation examples using one of the proposed $\metas$ methods to deduce/identify abstract artifacts. 
    % \sameer{seems a little arbitrary? very little motivation for why this should be the next step}
    %
    
\vspace{0.2em}
 \noindent (4) Verify candidate artifacts by manipulating validation data and observing the effects on outputs.
 %, e.g., observing the effect of removing or inserting candidate artifacts on model predictions. 
% \end{enumerate}


\begin{figure}[bt]
    \centering
    \includegraphics[width=0.9\columnwidth]{guideline2.pdf}
    \caption{Finding artifacts via attribution methods. Staring from the validation set, we explain model prediction for every sample using different attribution methods. Then we either aggregate the explanations using frequency or rely on the heatmap analysis of explanations to detect artifacts.}
    \label{fig:guide}
    % \postspace{}
    \minipostspace{}
\end{figure}

\vspace{.2em} 

We note that in 3-a, we aggregate the individual token \emph{rankings} over all instances (for both feature attribution and $\metas$ methods), which does not require thresholding attribution scores per instance.
%Let us note that in step (3-a), since we surface artifacts by aggregating the top-ranking tokens from all instances (for both feature attribution and $\metas$ methods), we do not require to introduce any ad-hoc threshold on top of attribution value for filtering important features in each instance.
% \noindent 
We now follow this procedure on widely used NLP benchmarks (Section \ref{section:setup}), finding that we can ``rediscover'' known artifacts and identify new ones within these corpora (Section  \ref{tab:known-art}; Table \ref{tab:results_summary}).

% , we follow this procedure on several widely used NLP benchmarks, and discover previously unknown artifacts (Table \ref{tab:results_summary}).
% Next we apply this process on synthetic and real datasets to assess the degree to which attribution methods (including $\meta$) can help identify artifacts.
% We then ask \emph{others} to follow this process in Section \ref{section:user-study}. 
% \sameer{we didn't really follow the above recipe in the user study, right? the recipe says to use all the methods, not just generic attribution that covers all of our methods?}

%%%%%%%%%%%%%%%
\section{Setup}
\label{section:setup}
% In this section, we briefly describe benchmarks, models, and attribution methods. Then we evaluate the capability of different attribution methods in detecting artificial artifacts in a synthetic scenario.  
% \byron{I think we need a sort of opening para giving a high-levelish overview of the experiments we're going to do and why before we dive into it?}

\para{Datasets} 
%To evaluate the quality of $\meta$'s explanations, we experiment with multiple text classification tasks including sentiment analysis, NLI, hate speech, etc.
%To evaluate the utility of attribution methods (including $\meta$) for identifying artifacts in NLP, w
We use a diverse set of text classification tasks as case studies.
Specifically, we adopt: Multi-Genre NLI (MNLI; \citealt{williams2017broad}); IMDB binary sentiment classification \citep{maas2011learning}; BoolQ, a yes/no question answering dataset \citep{clark2019boolq}; and, DWMW17, a hate speech detection dataset \citep{davidson2017automated}. %For details, see Section \ref{ap:es} of the Appendix.
% bcw ^ we mention this in-line several places.


\para{Models} 
We %use the same approach as in 
follow \citet{pezeshkpour2021empirical} for instance attribution methods; this entails only considering the last layer of BERT in our gradient-based instance attribution methods (see Appendix, Section \ref{ap:es}). 
For all benchmarks, we achieve an accuracy within $\sim$$1\%$ of performance reported in prior works using BERT-based models. %classifier previously reported performance. 

\para{Attribution Methods}
We consider two instance attribution methods, RIF~\citep{barshan2020relatif} and Euclidean Similarity (EUC), based on results from 
\citet{pezeshkpour2021empirical}. 
For \emph{Feature Attribution}, we consider Gradients (G) and Integrated Gradients (IG; \citealt{sundararajan2017axiomatic}).
To include RIF as a tool for artifact detection, we follow the $\metas$ aggregated token approach, but assign uniform importance to all the tokens in a document. 

In addition to the \emph{model-centered} diagnostics we have focused on in this work, we also consider a few \emph{dataset-centered} approaches for artifact discovery:
%there are `dataset-based' approaches designed to identify artifacts.
(1) \textit{PMI} \citep{gururangan2018annotation}, and (2) \textit{competency} score \citep{gardner2021competency}. 
% , and (3) \textit{[MASKING]}ing every token that appears in the test set separately and identifying those that result in a flipped prediction.
There are a few inherent shortcomings to purely dataset-centered approaches.
First, because they are model-independent, they cannot tell us whether a model is actually exploiting a given artifact.
Second and relatedly, they are based on simple observed correlations between individual features and labels, so cannot reveal abstract artifacts.
%1) dataset-based methods are not capable of detecting abstract artifacts, and 2) artifacts might be introduced into pipelines in fine-tuning step rather than in the data collection phase.
Given the latter point, we only consider these approaches for granular artifact detection (Section \ref{imdb-sec}).  
% \sameer{this will be an easier argument if artifacts are defined as model-dependent}

\para{Challenges and Limitations}
%Considering the efficiency of feature attribution methods in comparison to instance attribution ones, the bottleneck in calculating $\meta$ is the instance attribution for each sample.
A key computational challenge here is that instance attribution can be prohibitively expensive to derive if one uses \emph{influence functions} directly~\cite{koh2017understanding,han2020explaining}.
%Since we adopt similar strategies as previous work \citep{pezeshkpour2021empirical} which provides an efficient calculation for instance attribution, this bottleneck does not pose a particular challenge in calculating $\metas$ approaches.
We address this by using efficient heuristic instance attribution strategies \citep{pezeshkpour2021empirical} to implement $\metas$.
%Moreover, since $\metas$ is the combination of feature and instance attributions, it will inherit any problematic behaviors from these approaches \citep{kindermans2019reliability,basu2020influence}. 
Since $\metas$ combines existing feature- and instance-based attribution methods, $\meta$ inherits known issues with these techniques~\citep{kindermans2019reliability,basu2020influence}.
Despite such issues, however, our results suggest that $\metas$ can be a useful tool for artifact discovery (as we will see next).
%Moreover, since $\metas$ is the combination of feature and instance attributions, it will inherit any problematic behaviors from these approaches \citep{kindermans2019reliability,basu2020influence}. In spite of these shortcomings, we show in the evaluation that $\meta$ can be used as an effective tool for artifact discovery.}

%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%
% \begin{table*}
% \small
% \centering
% \begin{tabular}{lccccc}
% \toprule
% &\multirow{2}{*}{\bf Method}&\multicolumn{2}{c}{\bf SST (dragon)}&\multicolumn{2}{c}{\bf SST (dragon vs lizard)}\\
% \cmidrule(lr){3-4}
% \cmidrule(lr){5-6}
% & & Noiseless & Noisy &  Noiseless & Noisy \\
% \midrule
% &G&26&20&37&33\\
% &IG&50&51&97&61\\
% \midrule
% \multirow{2}{*}{\bf Similarity} &
% NN EUC+G &85&92&100&100\\
% &NN EUC+LR& 99 &97&100&99\\

% \midrule
% \multirow{4}{*}{\bf Gradient} &
% RIF+G&94&56&100&100\\
% &RIF+IG&100&92&100&96\\
% &REP+LR& 100 & 98 & 93 & 99\\
% &RIF+LR& 100 & 81 & 82 & 93\\

% \bottomrule
% \end{tabular}
% \caption{Accuracy of each methods in correctly identifying the explanation. (noiseless acc ~ 100, noisy ~ 90)}
% \label{tab:synth}
% \end{table*}
%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%
% \begin{table*}
% \small
% \centering
% \begin{tabular}{lccccc ccc}
% \toprule
% &&\multicolumn{4}{c}{\bf Synthetic-Evaluation}&\multicolumn{3}{c}{\bf Artifact-Detection}\\
% \cmidrule(lr){3-6}
% \cmidrule(lr){7-9}
% &\multirow{3}{*}{\bf Method}&\multicolumn{2}{c}{\bf SST (dragon)}&\multicolumn{2}{c}{\bf SST (dragon vs lizard)}&\bf Synthetic-SST & \bf IMDB& \bf HANS\\
% \cmidrule(lr){3-4}
% \cmidrule(lr){5-6}
% \cmidrule(lr){7-7}
% \cmidrule(lr){8-8}
% \cmidrule(lr){9-9}
% & & Noiseless & Noisy &  Noiseless & Noisy&Acc&Hits@5&Rate\\
% \midrule
% &G&26&20&37&33&12.1&64&-\\
% &IG&50&51&97&61&13.5&78&-\\
% &Random&-&-&-&-&-&-&16.7\\
% \midrule
% \multirow{2}{*}{\bf Similarity} &
% NN EUC+G &85&92&\bf100&\bf100&48.7&84&\bf 71.6\\
% &NN EUC+LR& 99 &\bf 97&\bf 100&99&\bf 99.0&\bf99&-\\

% \midrule
% \multirow{4}{*}{\bf Gradient} &
% RIF+G&94&56&\bf100&\bf100&50.6&98&37.9\\
% &RIF+IG&\bf100&92&\bf100&96&30.1&78&39.5\\
% &REP+LR&\bf 100 & 98 & 93 & 99&97.1&0.0&-\\
% &RIF+LR& \bf 100 & 81 & 82 & 93&67.0&48&-\\

% \bottomrule
% \end{tabular}
% \caption{\textbf{Left:} accuracy of each methods in correctly identifying the explanation. (noiseless acc ~ 100, noisy ~ 90). \textbf{Right:} artifact detection rate. We find the number of times that the artifact appear as the most important feature in the training instances(SST: acc = 90, IMDB: dev=96.2 - test=93, ).}
% \label{tab:synth-art}
% \end{table*}
% %%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%
%

\section{Case Studies}
%\section{Case Studies Using Previously Reported Artifacts}
\label{tab:known-art}

%Previous studies have detected artifacts in existing NLP datasets. 
%In this section we 
We now compare attribution methods in terms of their ability to highlight dataset artifacts. %(both previously reported and novel). %, both %such 
%variety of tasks 
%(both 
%those that have been observed in the literature previously, and also artifacts that---to the best of our knowledge---have not been. 
We provide a summary of the previously reported (\textit{known}) and previously \textit{unknown} (i.e., discovered in this work) artifacts we identify in this way (and with which methods) in Table~\ref{tab:results_summary}.
% We start with a synthetic, controlled setting to verify that methods work under ``idealized'' conditions for artifact discovery, and then move to real corpora in following subsections.
% \sameer{TAble 2 referenced in text before Table 1? probably should come ealrier.}
%detection studies and usefulness of attribution methods in discovering the artifacts is provided in Table~\ref{tab:results_summary}. 
%in addition to comparing its efficacy to feature and instance attribution methods. 
% For reference, we define here again artifacts as correlations between inputs and outputs that exist in the train data that are spurious, in that they are not indicative of a causal relationship and instead owe to particulars of dataset creation (e.g., annotation heuristics).
% \sameer{where do we define known vs unknown?}



%%%%%%%%%%%%%%%%%%%%

\begin{table*}[]
    \centering\small
    \setlength\tabcolsep{5.5pt}
    \begin{tabular}{ll >{\raggedright}p{4.8cm} >{\raggedright}p{3.7cm} ccc}
    \toprule
       \bf Dataset  & \bf Artifact Type & \bf Test Instance& \bf Influential Train Instance& \bf FA & \bf IA & \bf $\metas$\\ \midrule
        IMDB & Ratings (K)& ... great movie, \tightcbox{blue!15!white}{6/10}. & ... like it. Rating \tightcbox{blue!15!white}{8/10}.&\cmark &\xmark &\cmark  \\
\addlinespace
        \multirow{2}{*}{HANS} & \multirow{2}{*}{Lexical Overlap (K)}&\textbf{P}: \tightcbox{blue!15!white}{The banker is} in a \tightcbox{blue!15!white}{tall} building.\\
\textbf{H}: \tightcbox{blue!15!white}{the banker is tall} & \textbf{P}: The \tightcbox{blue!15!white}{red oak} tree.\\
\textbf{H}: \tightcbox{blue!15!white}{Red oak} yeah. &\multirow{2}{*}{\xmark} &\multirow{2}{*}{\cmark} &\multirow{2}{*}{\cmark}\\
\addlinespace
        \multirow{2}{*}{DWMW} & Punctuation (U)& Yo\tightcbox{blue!15!white}{!} just die\tightcbox{blue!15!white}{.} & Yo man\tightcbox{blue!15!white}{!} what's up\tightcbox{blue!15!white}{.} &\cmark &\xmark &\cmark\\
         & Specific Tokens (U)& \tightcbox{blue!15!white}{You} are like \tightcbox{blue!15!white}{@}...& \tightcbox{blue!15!white}{You} should die \tightcbox{blue!15!white}{@}... &\cmark &\xmark &\cmark\\
\addlinespace
        \multirow{2}{*}{BoolQ} & \multirow{2}{*}{Query Structure (U)}& \textbf{Q}: \tightcbox{blue!15!white}{is} the gut \tightcbox{blue!15!white}{the same as} the stomach?\\ \textbf{P}: The gastrointestinal ...&\textbf{Q}: \tightcbox{blue!15!white}{is} the gut \tightcbox{blue!15!white}{the same as} the small intestine?\\ \textbf{P}: The gastrointestinal ... &\multirow{3}{*}{\xmark} &\multirow{3}{*}{\cmark} &\multirow{3}{*}{\cmark}\\
% \addlinespace
%         \multirow{3}{*}{YAGO3} & \multirow{3}{*}{Location Dependence (U)}& Marie \tightcbox{blue!15!white}{of France} [SEP] has child [SEP] Margaret \tightcbox{blue!15!white}{of France}& Philip III \tightcbox{blue!15!white}{of France} [SEP] has child [SEP] Margaret \tightcbox{blue!15!white}{of France}&\multirow{3}{*}{\xmark} &\multirow{3}{*}{\xmark} &\multirow{3}{*}{\cmark}\\
        \bottomrule
    \end{tabular}
    \caption{Summary of investigated previously \textit{known} (K) and previously \textit{unknown} (U) \tightcbox{blue!15!white}{artifacts}. We indicate the applicability of feature (FA), instance (IA) and $\metas$ methods for identifying each of these artifacts.}%\sameer{clarify known vs unknown}}
    \label{tab:results_summary}
    % \postspace{}
    \minipostspace{}
\end{table*}

%%%%%%%%%%%%%%%
\begin{comment}
\subsection{Synthetic Evaluation}
As a starting point, we consider a controlled setting in which we test the ability of different attribution methods to identify (synthetic) artifacts. 
We add tokens---simulated artifacts---that determine, to varying degree, the label for each sample, and we test the degree to which attribution methods are able to reveal these. 
%We consider the following setup.
%To test whether the $\meta$ methods reveal tokens that have been added to %toy 
%datasets as artifacts that identify the label of each sample, we construct following setups: 

% \textbf{Identifying Predictive Features:} To identify the most useful combination of feature and instance attribution methods for $\metas$, we artificially add the token \textit{dragon} and/or \textit{lizard}\footnote{We deliberately choose a rare word that should be devoid of sentiment.} into SST instances and assign labels based on the appearance of these words. 
% The goal is to use this synthetic data to identify the combination of feature and instance attributions ($\metas$ variants) that are best able to extract the artifacts introduced.
% %The goal is to identify the combinations (of feature and instance attribution appear in each TFA method) extracting these words with higher accuracy as the influential features. 
% %We provide the detailed setup and complete result of each method in the supplementary materials. 

% We report details and a full set of results in the Supplement, but summarize our findings here as they guide our choice of TFA strategy in the remainder of this paper.
% %In addition to observing TFA methods outperforming existing attribution methods with a considerable gap, 
% Briefly, we found that TFA outperformed alternative standalone attribution methods in general, and that combining [\textit{RIF+G/IG/LR, EUC+G/LR}] yielded the best performance for feature-instance attribution, as reported in Table \ref{tab:synth-art} (first result column).
% We therefore consider these TFA variants in the remainder of this work.

%based on each combination's performance and the fact that normalization is necessary for instance attribution to provide higher quality explanation \citep{pezeshkpour2021empirical}, we focus on [\textit{RIF+G/IG/LR, EUC+G/LR}] in the remainder of paper. 

%\byron{i dont get why we have this? i think move to appendix?}
Specifically, %we take the original SST dataset and for 30\% of
for 30\% of the training examples in the SST dataset, we add the word ``dragon'' if the review sentiment is positive and ``lizard'' if negative,\footnote{We deliberately chose rare, sentiment-neutral words.} thereby associating these tokens with sentiment. 
% The remaining examples keep the original sentiment label. 
%We posit that dragon/lizard provides 
These tokens provide the model an explicit ``shortcut'' to classify sentiment, when they appear. 
We test whether the top influential token based on different attribution methods in these examples contain ``dragon'' or ``lizard''. 
The accuracy of attribution methods with respect to identifying these artifacts is provided in \cref{tab:synth-art}. 
%As it shows, 
The $\meta$ methods %completely 
outperform feature attribution approaches. 
Further, the discriminator $\meta$ method appears most accurate in identifying these artifacts.  
\end{comment}

 
\subsection{Known Granular Artifact: Sentiment Analysis with IMDB Ratings} 
\label{imdb-sec}

\citet{ross2020explaining} % has previously 
observe that in the case of binary sentiment classification on IMDB reviews \cite{maas2011learning}, %the presence of 
numerical ratings (1 to 10) sometimes appear in texts. %, and these %tend to exhibit a 
%strongly correlate with sentiment, and that 
Modifying these in-text ratings often flips the predicted label.\footnote{This is an ``artifact'' in that the underlying task is assumed to be \emph{inferring sentiment from free-text}, presumably where the text does not explicitly contain the sentiment label.} %That some reviews in IMDB review explicitly contain this information is an accident of data collection, hence our characterization.}
%In this experiment, w
We evaluate %the ease of
%whether and to what 
the ability of %$\metas$ and other 
attribution methods to %best 
%able to 
surface this artifact. 
This is a \emph{granular} artifact, and so we adopt our aggregation approach to extract them.


\para{Setup} We sample train/validation/test sets comprising 5K/2K/100 examples respectively from the IMDB corpus, such that all examples in the test set contain a rating (i.e., exhibit the artifact). 
We first confirm whether models %really 
exploit this rating as an artifact when present. 
%We consider three settings where we modify the ratings in test samples by 
Specifically, we (1) remove the rating and \emph{invert} the rating either by (2) setting it to $10$-\text{\em original rating} (e.g., 1 $\rightarrow$ 9), or (3) by setting the rating to 1 for positive reviews, and 10 for negative reviews. 
%We observe that 
This flips the prediction for 9\%, 34\% and 38\% of test examples following these three modifications, respectively.\footnote{Probabilities of the originally predicted labels also drop.} %; 
%The probabilities assigned to the originally predicted class similarly drops in 83\%, 85\% and 89\% of examples, respectively. 
This suggests the model exploits this artifact.
%These results suggest that the model does exploit this artifact.
%We conclude that while rating may not be only feature relied on by the model, it still plays a dominant role.

\para{Findings} We evaluate whether numerical ratings are %surfaced in 
among the top tokens returned by feature and $\metas$ attribution methods. 
For each test example, we surface the top-5 tokens according to different feature attribution methods. 
For $\metas$, we use the aggregated token analysis method with $k$=10 (i.e., considering the top and bottom 10\% of examples), and we return the top-5 tokens from the aggregated token list sorted based on frequency of appearance.

% bcw: note that cref is lowercasing our Tables.
In Table \ref{tab:synth-art} (IMDB column), we report the percentage of test examples where a number from 1-10 appears in the top-5 list returned by the respective attribution methods (likely indicating an explicit rating within review text). 
For approaches that rely solely on the training data without reference to the validation set (PMI and Competency), we report the ratio of appearance of numbers in the overall top-5 most influential tokens. 
In general $\metas$ methods surface ratings more often than feature attribution methods.\footnote{We note that the competency approach does rank rating tokens among the top-10 tokens.} 
However, the performance of TFA is not directly comparable to the PMI and competency methods because the former capitalizes on a validation set which contains this artifact. 
% This first set of results confirms that \emph{if} one has a validation set that exhibits an artifact, $\metas$ can successfully retrieve the artifact. % train instances (and tokens) that feature the same. 
%This is a simple artifact; we next consider more complex cases.
%In our view this suggests that constructing challenging validation sets to identify artifacts using TFA, although more research is needed to establish how feasible this is for annotators.}
%the former exploit some previous knowledge of ratings artifacts to construct its validation set whereas the latter methods are unsupervised.}

%with higher likelihood that feature attributions. 
%Based on both performance of the feature and $\metas$ methods, it seems the model reliance on ratings to make its prediction is strong enough that this artifact would be easy to detect using either of these methods.
%However, given that both methods surface these tokens high reliability, 
%Since all methods would likely reveal this artifact given the high Hits@5 numbers, this appears to be a simple artifact to discover. 


% \sarthak{Need conclusion}

%%%%%%%%%%%%%%%
% \begin{table}
% \small
% \centering
% \begin{tabular}{llccc}
% \toprule
% &\multirow{3}{*}{\bf Method}&\bf Synthetic-SST & \bf IMDB& \bf HANS\\
% \cmidrule(lr){3-3}
% \cmidrule(lr){4-4}
% \cmidrule(lr){5-5}
% & &Acc&Hits@5&Rate\\
% \midrule
% &G&12.1&64&-\\
% &IG&13.5&78&-\\
% &RIF&0&0&32.0\\
% &Random&-&-&16.7\\
% \midrule
% \multirow{3}{*}{\rotatebox[origin=c]{90}{\bf Sim}} &
% EUC+G &48.7&84&71.6\\
% &EUC+IG&46.8&53&\bf 80.9\\
% &EUC+LR&\bf 99.0&\bf99&-\\
% \addlinespace
% %\midrule \midrule
% \multirow{3}{*}{\rotatebox[origin=c]{90}{\bf Grad}} &
% RIF+G&50.6&98&37.9\\
% &RIF+IG&30.1&78&39.5\\
% % &REP+LR&97.1&0.0&-\\
% &RIF+LR& 67.0&48&-\\

% \bottomrule
% \end{tabular}
% \caption{Artifact detection rate, i.e., the number of times that an artifact token was ranked as the most important feature. Methods below the horizontal line are TFA variants.} %(SST: acc = 90, IMDB: dev=96.2 - test=93, ).}
% \label{tab:synth-art}
%     \postspace{}
%     \minipostspace{}
% \end{table}
\begin{table}
\small
\centering
\begin{tabular}{llrr}
\toprule
&\multirow{3}{*}{\bf Method} & \bf IMDB& \bf HANS\\
%\cmidrule(lr){3-3}
%\cmidrule(lr){4-4}
%\cmidrule(lr){5-5}
& & Hits@5&Rate\\
\midrule
&Random&1.7&16.7\\
&PMI&20.0&-\\%42.6
&Competency&0.0&-\\%44.3
\midrule
&G&64.0&-\\
&IG&78.0&-\\
&RIF&0.0&32.0\\
% &[MASK]&0&-\\%31.9
\midrule
&\emph{$\metas$ methods}\\
\multirow{3}{*}{\rotatebox[origin=c]{90}{\bf Sim}} &
EUC+G &84.0&71.6\\
&EUC+IG&53.0&\bf 80.9\\
&EUC+LR&\bf99.0&-\\
\addlinespace
%\midrule \midrule
\multirow{3}{*}{\rotatebox[origin=c]{90}{\bf Grad}} &
RIF+G&98.0&37.9\\
&RIF+IG&78.0&39.5\\
% &REP+LR&97.1&0.0&-\\
&RIF+LR&48.0&-\\

\bottomrule
\end{tabular}
\caption{Artifact detection rates. Methods below the horizontal line are TFA variants.} %(SST: acc = 90, IMDB: dev=96.2 - test=93, ).}
\label{tab:synth-art}
    % \postspace{}
    % \postspace{}
    \minipostspace{}
\end{table}


% \byron{definitely need more exposition about HANS (and need to cite the paper?) }
\subsection{Known Abstract Artifact: Natural Language Inference with HANS}
\label{hans-sec}

%Entailment %prediction tasks (aka 
In Natural Language Inference (NLI) the task is to infer whether a premise \emph{entails} a  hypothesis \cite{maccartney2009natural}. %are %used as 
NLI is commonly used to evaluate %the semantic capabilities of
the language ``understanding'' capabilities of neural language models, and large NLI datasets exist \cite{bowman2015large}.
%, resulting in large corpora for the task \cite{bowman2015large}.
%Given a premise and a hypothesis, 
% Rather than demonstrating `understanding', 
% however, 
However, recent work has shown that NLI models trained and evaluated on such corpora tend to exploit common artifacts present in the crowdsourced annotations, e.g., premise-hypothesis pairs with overlapping tokens and hypotheses containing negations both correlate with labels \cite{gururangan2018annotation,sanchez-etal-2018-behavior,naik-etal-2018-stress}. 
Here we evaluate whether $\metas$ can surface the lexical overlap artifact, which is abstract and so requires heatmap inspection (other approaches are not applicable here).
% \sameer{mention it's difficult to do with other methods}
%We will focus on the former artifact type, 

\para{Setup} The HANS dataset~\cite{mccoy2019right} was created as a controlled evaluation set to test %specific
the degree to which models rely on %syntactic 
artifacts %existing 
in NLI benchmarks such as MNLI. 
We specifically consider the \emph{lexical overlap} artifact, %. Lexical overlap artifact assumes that p
where % when the premise entails the hypothesis whenever 
entailed hypotheses primarily comprise words that also appear in the premise. 
For training, we use 10K examples from the MNLI set. %which has been shown to contain lexical overlap artifact. 
%As a test set, 
We randomly sample 1000 test examples from the HANS dataset that exhibit lexical overlap. 
%Our job is to 
We test whether attribution methods reveal dependence on lexical overlap when models \emph{mispredict} an instance as entailment, presumably due to reliance on the artifact. %(\citet{han2020explaining} conducts similar study using instance attribution methods).
%; We assume all mispredictions are due to lexical overlap.
Here again we are dependent on a validation set that exhibits an artifact, and we are verifying that we can use this with $\metas$ to recover the training data that contains this.

\para{Findings} By construction, the hypotheses in the HANS dataset comprise the same tokens as those that appear in the accompanying premise. 
Therefore, feature attribution may not readily reveal the ``overlap'' pattern (because even if it were successful, \emph{all} input tokens would be highlighted).
%Since hypothesis in the HANS dataset is built out of tokens from premise only, any token highlighted by feature attribution in premise will appear in hypothesis, making it unlikely that user will detect the presence of lexical overlap artifact. 
%Might Heatmap based analysis of 
$\metas$, however, can surface this pattern, because hypotheses in the train instances do contain words that are not in the premise. 
Therefore, if $\metas$ highlights %overlapping tokens in both premise and hypothesis,
only tokens in both the premise and hypothesis, this more directly exposes the artifact. %this would seem to directly expose the artifact. 
%It may make it easier for a user to spot this pattern. 
%To test whether $\metas$ %can be helpful in detecting this artifact,
%is successful in this respect, 
To quantify performance, we calculate whether the top train token surfaced via $\metas$ appears in both the premise and the hypothesis of the training sample. 

Table \ref{tab:synth-art} (HANS column) shows that $\metas$ methods demonstrate fair to good performance in terms of highlighting overlapping tokens in retrieved training instances as being influential to predictions for examples that exhibit this artifact. %comparing 
% compared to random appearance of lexical overlap in training instances. 
%It seems 
Here $\metas$ variants that use similarity measures for instance attribution appear better at detecting this artifact, aligning with observations in prior work \cite{pezeshkpour2021empirical}. 
Based on feature and $\meta$ methods performance in artifact detection for the IMDB and HANS benchmarks, we focus on IG and RIF+G attribution methods in the remainder of this paper.


%\section{Unknown Artifacts Discovery}
%\section{Discovering New Artifacts}

%In this section we set out to use the proposed $\meta$ method (as well as other attribution methods) to unearth previously unobserved artifacts in benchmark NLP datasets (Table \ref{tab:known-art}).
%we use existing attribution methods in addition to our proposed $\meta$ to tackle the task of unknown artifact discovery for three different benchmarks (reported in Table \ref{tab:known-art}). 
%We divide each case study into 4 parts: (1) \textbf{Task} in which we define the benchmark and motivation/goal behind our investigation. (2) \textbf{Setup} where we describe the model training process %and %initial process on the data/model. 
%(3) \textbf{Identified Artifacts} %in this part we set out to identify 
%we identify potential artifacts in a variety of \textit{approaches} using different attribution methods. And (4) \textbf{Verification} in which we verify the validity of discovered artifacts.
% Our discovered artifacts are as follows: In \textit{DWMW17}, we observe that punctuation marks and particular tokens {\sf [trash, yo, da, wit]} are responsible for model misprediction of AAE tweets.
% In \textit{BoolQ}, we observe that the model relies on the appearance of the target's passage in the training and the structure of the target's query as artifacts. 
% In \textit{YAGO3-10's triple classification}, we observe that model mistakenly relay on the birthplace information in the entities' mention to make its prediction.  
% \sameer{this is giving away too much, without it being very helpful.. cut all these examples of biases out I think}

% \byron{This is maybe a bit confusing because prior work (Sap) already demonstrated this artifact, yet we are saying it was unknown?}
\subsection{Unknown Granular Artifact: Bias in Hate Speech Detection}%: The Curious Case of Punctuation}

Next we consider %the issue of 
racial bias in hate speech detection. 
 \citet{sap2019risk} observed that publicly available hate speech detection systems for social media tend to assign higher toxicity scores to posts written in African-American Vernacular English (AAVE). 
%  Assuming \emph{a priori} that there is no inherent difference between the toxicity of social media posts written in AAVE and White-Aligned English (WAE), this poses a fairness concern.
 %Here %our goal is to see if we can %specify 
Our aim here is to assess whether we can identify novel granular artifact(s) using our proposed methods. We find that 
%  We first verify that $\metas$ can recover this artifact, and then identify a new granular artifact in this process:
 %then set out to identify more granular artifacts in this dataset using our proposed methods. We find that  %We aim to assess if we can identify tokens in train instances %identifying other forms of biases that dictate predictions.  
 %In so doing, 
 %We identify novel artifacts: 
 there %seems to be 
 is a strong correlation between punctuation and ``toxicity'', and other seemingly irrelevant tokens.% and particular tokens with 
 %toxicity.}% We incorporate our aggregating approach to detect these granular artifacts.} %, and identify other finer grained artifacts responsible for this phenomena.  

\para{Setup} Following \citet{sap2019risk}, we use the DWMW17 dataset \citep{davidson2017automated} which includes 25K tweets classified as \emph{hate speech}, \emph{offensive}, or \emph{non-toxic}. 
We sample train (5k)/validation (2k)/test (2k) subsets from this. %comprising 5k, 2k, and 2k examples, respectively. 
\begin{comment}
%We are interested in analyzing dialect aspect of tweets, 
We use the topic model from \citealt{blodgett2016demographic} to classify tweets as exhibiting AAVE and WAE (keeping samples with dialect confidence $>$0.8, following \citealt{sap2019risk}). 


To identify AAVE-correlated artifacts that result in models classifying non-toxic AAVE tweets as toxic, we need pairs of tweets that differ in dialect (WAE vs. AAVE) but that are otherwise similar. %\emph{only}, and the same in other respects. %with otherwise similar content. 
%Since such a benchmark is not available, 
Such paired examples are not available, so we introduce simple word replacement heuristics to convert WAE tweets into AAVE, e.g., swapping the $\rightarrow$ da, with $\rightarrow$ wit, and inserting words (e.g., ``yo'').
For example, we modify ``\textit{the spear chuckers aren't flooding into Upton Park.}'' to ``\textit{da spear chuckers ain't floodin into Upton Park. yo yo yo!}''. 
We provide all heuristics and their effect on our topic model dialect classifier in Section \ref{ap:st} of the Appendix. 
%On
Applying these heuristics to non-toxic WAE tweets in the test set results in the model flipping its prediction from non-toxic to toxic for 14.2\% of instances.
We would like to use attribution methods to detect when the model (wrongly) exploits these shallow correlations.
%Our goal is to 
%We would now like to pinpoint the modifications mostly responsible for the drop in accuracy.
\end{comment}

\para{Identified Artifacts} %We compare different methods for discerning
 %The result of different methods to discern %analyzes on 
%the effect of dialect %over 
% on the hate speech detection model is reported %provided 
We first consider using instance attribution to see if it reveals the source of bias that leads to the aforementioned misclassifications. 
We observe an apparent difference between influential %inferential 
instances for non-toxic/toxic tweets that were predicted correctly versus mispredicted instances, but no anomalies were readily identifiable in the data (to us) upon inspection. 
In this case, instance attribution does not seem  particularly helpful with respect to unveiling the artifact.

Turning to feature attribution, the most important features---aside from tokens contained in a hate speech lexicon \citep{davidson2017automated}, which we exclude from consideration (these are indicators of toxicity and so do not satisfy our definition of artifact)---surfaced by aggregating feature attribution scores are:
% [\textit{., you, nigga, @, the, :, \&, nigger, hoes, ass}] 
[\textit{., you, @, the, :, \&}] for misclassified instances.
% These features suggest that the model incorrectly relies on \emph{.}, \emph{you}, \emph{@}, \emph{the}, \emph{:}, and \emph{\&} to predict toxicity in tweets. 
Given these results, we deem feature attribution successful in identifying artifacts. 
% lexical indicators of dialect.


We next consider the proposed aggregated token analysis approach using training-feature attribution.
 The most important features (ignoring hate speech lexicon) %, modulo terms from \cite{davidson2017automated}, 
 retrieved by aggregating $\metas$ methods over misclassified samples are: [\textit{@, white, trash, !, you, is}].
% [\textit{bitch, @, white, trash, !, you, is, nigger, pussy, hoe}]. 
Surprisingly, the model appears to rely on tokens \emph{@}, \emph{white}, \emph{trash}, \emph{!}, \emph{you}, and \emph{is} to predict toxicity. 
PMI and competency also rank tokens \emph{is}, \emph{.}, \emph{trash}, and \emph{the} highly, validating these artifacts.


\para{Verification} To confirm that punctuation marks and other identified tokens indeed affect toxicity predictions, we modified tweets containing these tokens observe changes in model predictions.
%we first observe change in the model prediction upon modifying tweets containing those tokens.
We report the percentage of flipped predictions after replacing these punctuation tokens with {\tt [MASK]} in Table \ref{tab:hate}. 
%As it shows, upon masking these tokens much
Masking these tokens yields a substantially higher number of flipped predictions than does masking a random token.
%misclassified instances predicted correctly comparing to a random baseline. 
%Replacing [you, \&, @, the, trash, is, :, !, .] with [[MASK]]. 
%Re-evaluating model performance after these modifications, we observe that $21.8\%$ of previously mispredicted instances being predicted correctly verifying that the model incorrectly relies on these tokens to predict toxicity.}


%% change correctly pred: 1.4%


% \begin{table}
% \small
% \centering
% \begin{tabular}{ccc|ccc}
% \toprule
% \multirow{2}{*}{\bf Token}&\multicolumn{2}{c}{\bf Prob-change}&\bf \multirow{2}{*}{Token}& \multicolumn{2}{c}{\bf Prob-change}\\
% \cmidrule(lr){2-3}
% \cmidrule(lr){5-6}
% &Toxic&Non-toxic&&Toxic&Non-toxic\\
% \midrule
% 'you'&0.09&\cellcolor{green!85!black}13.3&'.'&\cellcolor{green!85!black}3.9&0.19\\
% '@'&\cellcolor{red!85!black} -3.6&\cellcolor{green!85!black}6.4&':'&\cellcolor{green!85!black}3.1&\cellcolor{red!85!black}-1.9\\
% '!'&\cellcolor{red!85!black} -5.7&1.0&'\&'&\cellcolor{green!85!black} 6.4  &-1.5\\
% 'white' (few)&\cellcolor{red!85!black}-34.4&-0.60&'trash'&-0.97&\cellcolor{green!85!black}6.2\\
% 'the'&-1.2&\cellcolor{green!85!black}2.1&'is'& 0.2&\cellcolor{red!85!black}-3.4\\
% \bottomrule
% \end{tabular}
% \caption{Remove.} 
% \label{tab:hate}
% \postspace{}
% \end{table}

% \begin{table}
% \small
% \centering
% \begin{tabular}{ccc|ccc}
% \toprule
% \multirow{2}{*}{\bf Token}&\multicolumn{2}{c}{\bf Prob-change}&\bf \multirow{2}{*}{Token}& \multicolumn{2}{c}{\bf Prob-change}\\
% \cmidrule(lr){2-3}
% \cmidrule(lr){5-6}
% &Toxic&Non-toxic&&Toxic&Non-toxic\\
% \midrule
% 'you'&0.26&\cellcolor{green!85!black}10.1&'.'&\cellcolor{green!85!black}7.1&2.6\\
% '@'& 0.5&\cellcolor{green!85!black}7.7&':'&6.3&5.6\\
% '!'&\cellcolor{green!85!black} 2.0&1.4&'\&'&\cellcolor{green!85!black} 16.4  &-0.6\\
% 'white' (few)&\cellcolor{green!85!black}10.3&-0.9&'trash'&\cellcolor{green!85!black}6.9&0.3\\
% 'the'&\cellcolor{green!85!black}11.5&\cellcolor{green!85!black}5.0&'is'& \cellcolor{green!85!black}4.8&\cellcolor{red!85!black}-1.7\\
% \bottomrule
% \end{tabular}
% \caption{Replace with [MASK].} 
% \label{tab:hate}
% \postspace{}
% \end{table}

\begin{table}
\small
\centering
\begin{tabular}{cc|cc}
\toprule
\bf Token& \bf Flip \%&\bf Token& \bf Flip \%\\
\midrule
`you'&13.6&`.'&12.1\\
`@'&10.5&`:'&11.1\\
`!'&7.6&`\&'&7.1\\
`white'&33.3&`trash'&5.0\\
`the'&12.7&`is'&12.5\\
\bottomrule
\end{tabular}
\caption{The percent of prediction flips observed after replacing the corresponding tokens with {\tt[MASK]}. For reference, masking a random token results in a label flip 1.8\% on average (over 10 runs).}
%We observe the percentage of flip in prediction after replacing the token with [MASK]. The average percentage of flip for a random token (over 10 runs) is 1.8\% . }} 
\label{tab:hate}
    % \postspace{}
    \minipostspace{}
\end{table}

% 1: 5.4
% 2: 7.7
% 3: 17.3


\begin{comment}
\textit{Test 2:} To further validate our discovered artifacts,  
we also modify the \emph{training} data.
In particular, we remove ``yo'' and ``;'' from all train instances; replace ``wit'' $\rightarrow$ ``with`', and ``da'' $\rightarrow$ ``the''.
We then retrain the model and again make predictions on the modified test set. 
This time only a modest 2.5\% (compared to the previous 14.2\%) of modified tweets were misclassified%predicted incorrectly% (from $90.4\%$ to $87.9\%$)
, suggesting a lesser reliance %of the new model 
on these artifacts. %to make its predictions. 


% %%%%%%%%%%%%%%%%%%%%
\subsection{Triple Classification on YAGO3}
\label{trip}
Knowledge graphs (KG) underlie models for many NLP tasks including recommendation \citep{zhang2016collaborative}, semantic search \citep{bast2016semantic}, and question answering \citep{cui2019kbqa}. 
% To further investigate the %applicability 
% utility of attribution methods for identifying artifacts, 
We next investigate the utility and applicability of attribution methods for identifying artifacts for the task of triple classification (i.e., classifying whether a given triplet is true) over the YAGO3-10 KG corpus \citep{mahdisoltani2013yago3}. 

We follow \citet{yao2019kg}, treating triple classification as an instance of text classification by converting triples \textit{$\langle$subject, relation, object$\rangle$} into ``sentences'': \textit{{\tt [CLS]} subject {\tt [SEP]} relation {\tt [SEP]} object {\tt [SEP]}}. 
We aim to use attribution methods to probe whether the model is relying on meaningful cues to make predictions, or if ostensibly ``good'' performance belies reliance on shallow heuristics. 
%Our goal here is to investigate whether model relies on reasonable information to make its prediction or depends on artifacts possibly existing in the entities' mention.   

\para{Setup} %For this evaluation, considering this task requires complicated reasoning over relational information, we focus on three more comprehensible predicates:
%Our goal here is to check the extent to which this model is actually `reasoning' at all, and whether or to what degree it instead relies on artifacts.%whether our classifier is actually capable of any reasoning and further identify existing artifacts. 
%For our analysis w
For simplicity, we focus on only two predicates at inference time: \textit{is married to}, and \textit{has child}. 
% We randomly sample 1000 married people from the YAGO3-10 training dataset, and then gather all relational information surrounding these people. We end up with 8911 triples over 24 different relations. 
%Because a
Triples in KGs have \emph{positive} labels. 
We create \emph{negative} instances by randomly sampling %pick
half of all triples and replacing their subject or object with an another entity drawn from the pool of all entities at random (similar to \citealt{yao2019kg}). %from our sampled sub-graph (similar to \citet{yao2019kg}). Finally, We randomly divide the resultant dataset to train/test/dev sets with the size of 6000/500/2411 respectively. 
% Starting from the YAGO3-10 training dataset, we gather all pairs of married couples that have a child together (we are deliberately polluting the data with this pattern to later evaluate the model's reliance on it as a known artifact). 
% Additional details regarding the training dataset are in the Supplementary materials.
%We leave more statistical details of our triple classification benchmark to supplementary materials. 
% From this subset of entities we filter out those with $>20$ links, and then collect all links surrounding the remaining entities, yielding $\sim 5.7k$ triples in total.
% Knowledge bases only provide us with positive samples, so we construct negative samples from the positive ones following \citet{yao2019kg}. 
% This process results in a dataset in which $58\%$ samples are positive. 
% We randomly divide the resultant dataset to train/test/dev sets with the size of 5100/400/200 respectively. 
%
%Upon 
Our fine-tuned BERT classifier achieves $94.3\%$ accuracy over the validation set with all relations, %which is 
comparable to the results %reported in 
in \citet{yao2019kg}.
%\sameer{how is BERT used? Lama style?} 

\para{Identified Artifacts} Given that neighboring links appear amongst the top most influential instances for fewer than 4\% of the test samples,
%Considering that For only less than $4\%$ of test samples, one of their neighboring links appear in the top 10 most influential instances 
and the fact that each entity has many links, uncovering artifacts via instance attribution may be challenging. %appears very challenging. 
The only anomaly in influential instances that we observed was that for many test instances (28.9\% of married couples) in the form of ``Person-\emph{X} is married to Person-\emph{Y}'', a training instance with the form ``Person-\emph{Y} is married to Person-\emph{X}'' exists.  %appears in the training. 
Further investigation revealed that this pattern appears in 97.8\% of \emph{is married to} instances in YAGO3-10. %the YAGO3-10 KG.
%Applying instance-based attribution methods %couldn't 
%do not suggest any obvious pattern:. 
% Further, each entity having many links makes any reasonable deduction quite infeasible.  Fail

% \textit{Approach 3:} We apply feature attribution and make the following observations. Fail
In this case aggregating important test tokens using feature attribution methods was not helpful. 
The top-5 features from IG are: [\emph{married, is, to, jean, province}] for \textit{is married to}; and [\emph{child, of, bavaria, has, province}] for % extract , while for the
the \textit{has child} relation. %relation we have . 
%As it shows, t
These features do not carry any obvious importance; they are either components of the relation or appear in very few samples. %meaningful importance.% outside their superficial appearance in each test sample.  
% \sameer{confused.. why catn' these be artifacts?}
% (1) In predicting \emph{hasGender} relations, first name and a location appear amongst the top-3 most positive features in 63\% and 54.2\% of cases respectively. (2) For %``marriage''
% \emph{isMarried} and \emph{hasChild} relations, where mentions have a common token (same last name or place of birth), that token appears in the top-3 most positive features in 46\% and 50\% of cases respectively.


% \textit{Approach 3:} We %provide 
We report the top-5 most influential features from $\metas$ for different relations in Table \ref{tab:trip-class}. 
%As it shows,
We observe %---surprisingly---
that the model relies on marriage information in making prediction about the \emph{Married} relation, agreeing with the aforementioned behavior highlighted using instance attribution.
%which further point out the anomaly that we discovered using instance attribution methods. 
% \emph{Married} relation (we expect the marriage to be exclusive between couple). Upon further investigating YAGO3-10 triple we observe 97.8\% of cases where Y married to Z, we have Z married to Y in the training (this artifact appear in 16.3\% of pair of test sample in training data). 
% We observe that the model relies on child information %to predict
% in making predictions about the \emph{isMarried} and \emph{hasChild} relations, which we expected given the correlation in the train set. 
More problematically, %it seems the model relies on
the model appears also to (over-) rely on location information to inform predictions. 
Investigation of heatmaps derived via $\metas$ over influential instances %we observe that 
suggests that the model makes predictions regarding the \emph{having child} and \emph{marriage} relations on the basis of subjects and objects sharing a location (see Table \ref{tab:results_summary} for an example). %(Figure \ref{fig:triple-art}).   


\paragraph{Verification} To evaluate whether the model is in fact exploiting the location artifact surfaced above, we execute %a sort of 
an adversarial attack that uses location information  
%our conclusion about the model relying on having the same location as part of the subject and object mentions as an artifact, 
% We first replace every object in test samples with a random entity, which in turn guarantees that all triples are negative. 
% Since the effect of our adversarial modification is not clear on the label of positive instances, we only consider negative samples in this evaluation (50\% of the test set).
%We are interested in deceiving the model to predict negative test instances to be positive by adding
%Specifically, we
by inserting the same location for the object and subject mentions in \emph{negative} test samples. 
%Exploiting our discovered artifact, 
Specifically, we randomly replace/add one of the mentioned locations in Table \ref{tab:trip-class}---[\emph{California, Perth, Rome}] for the \emph{Married} relation and [\emph{Iran, Niagara, Olympia, Sweden}] for the \emph{hasChild} relation---to the end of subject and object mentions.
For example, we modify the sentence \textit{Sam Mendes {\tt [SEP]} is married to {\tt [SEP]} Rachel Weisz} to \textit{Sam Mendes of Spain {\tt [SEP]} is married to {\tt [SEP]} Rachel Weisz of Spain}.
%The effect of locational adversarial attacks on triple classification is provided in 
We report the effect of these modifications on triple classification accuracy in Table~\ref{tab:trip-acc}. 
% \sameer{elaborate.. metric, etc.}
%As it shows, we could successfully deceive
%We see that 
Adversarially introducing locations into instances reliably affects predictions. % for \textit{isMarriedTo}, \textit{hasChild} relations.
% , while predictions for the \textit{hasGender} relation almost remain the same, validating our conclusion on model reliance on location information for the first two relations. 
Moreover, location information based on tokens retrieved via $\metas$ methods are slightly more effective than random locations.
%appears a little more effective in deceiving the triple classifier.

%   %%%%%%%%%%%%%%%
% \begin{table}
% \small
% \centering
% \begin{tabular}{cccc}
% \toprule
% \multirow{2}{*}{\bf isMarriedto}&\multirow{2}{*}{\bf hasChild}&\multicolumn{2}{c}{\bf hasGender}\\
% \cmidrule(lr){3-4}
% &&male&female \\
% \midrule
% has&has&gender&Spain\\
% Spain&Portugal&male&gender\\
% Portugal&gender&Bohemia&Portugal\\
% gender&Spain&England&has\\
% child&child&has&Denmark\\
% \bottomrule
% \end{tabular}
% \caption{Top 5 tokens identified as influential by model.}
% \label{tab:trip-class}
% \end{table}
% %%%%%%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%


\begin{table}[tb] 
\footnotesize
 \begin{subfigure}[b]{\columnwidth} 
  \centering
         \setlength{\tabcolsep}{2pt}
  %%%%%%%%%%%%%%%
        \begin{tabular}{cc}
        \toprule
        \bf Married&\bf Child\\
        \midrule
        influences&airport\\
        California&Iran\\
        Perth&Niagara\\
        Rome&Olympia\\
        married&Sweden\\
        \bottomrule
        \end{tabular}
        % \vspace{3pt}
        \caption{Top 5 tokens identified as influential by $\metas$.}
        \label{tab:trip-class}
   \end{subfigure}
\,
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\begin{subfigure}[b]{\columnwidth} 
  \centering
            \begin{tabular}{lcc}
            \toprule
            &\bf Married&\bf Child\\
            \midrule
            Orig&90.4&100.0\\
            Rand&43.6&34.8\\
            % IG&21.0&14.1\\
            RIF+G&\bf 41.9& \bf 30.4\\
            \bottomrule
            \end{tabular}
            \caption{Effect of adverserially modifying entities' mention by adding same location at the end of subject and object mentions on the accuracy of triple classification task.} 
            % We only consider negative test samples here.}
            % \sameer{what is the metric in these tables?}
            \label{tab:trip-acc}
        \end{subfigure}
     \caption{Investigating existing artifacts in triple classification on YAGO3-10 benchmark.}
    % \postspace{}
    \postspace{}
\end{table}
\end{comment}

\subsection{Unknown Abstract Artifact: Structural Bias in BoolQ}

As a final illustrative NLP task, we consider \emph{reading comprehension} which is %being 
widely used to evaluate language models. 
Specifically, we use
BoolQ \cite{clark2019boolq}, a standard reading comprehension corpus. 
The task is: Given a Wikipedia passage (from any domain) and a question, predict whether the answer to the question is \emph{True} or \emph{False}. 
%The dataset is not restricted to any specific domain(s). 
A natural question to ask is: What do models actually learn from the training data?

\para{Setup} We use splits from the SuperGLUE \citep{wang2019superglue} benchmark for BoolQ. 
Test labels are not publicly available, so we divide the training set into 8k and 1k sets for training and validation, respectively. 
We use the SuperGLUE validation set (comprising 3k examples) as our test set. 

\para{Identified Artifacts} We first qualitatively analyze mispredicted examples in the BoolQ test set by inspecting the most influential examples for these, according to RIF. 
We observed that the top influential examples tended to have the same query structure as the test instance. 
For example, in the sample provided in Table~\ref{tab:boolq}, both the test example and the most influential instance share the structure \emph{Is \emph{X} the same as \emph{Y}?}
Focusing only on the test examples with queries containing the word ``same", we use the LR method proposed above to discriminate between the 10 most and least influential examples. 
For half of these test examples the word ``same" has one of the 10 highest coefficients, indicating significant correlation with influence.

\para{Verification} That query structure might play a significant role in model prediction is not surprising (or necessarily an artifact) in and of itself. 
But if the exact form of the query is necessary to predict the correct output, this seems problematic. To test for this, we consider two phrases that share the query structure mentioned above: (1) \emph{Is \emph{X} and \emph{Y} the same?} and (2) \emph{Is \emph{X} different from \emph{Y}?} 
We apply this paraphrase transformation to every test query of the form \emph{Is \emph{X} the same as \emph{Y}} and measure the number of samples for which the model prediction flips. 
These questions are semantically equivalent, so if the model
does not rely on query structure we should not observe much difference in model outputs. 
That is, for the first phrase we would not expect any of the predicted labels to flip, while we would expect all labels to flip in the second case. 
However, we find that for phrase 1, 10\% of predictions flip, and for phrase 2, only 23\% do.\footnote{Note that in this case, the query structure itself is not correlated with a specific label across instances in the dataset, and so does not align exactly with the operational ``artifact'' definition offered in Section~\ref{sec3.1}.} 
Nonetheless, the verification procedure implies the model might be using the query structure in a manner that does not track with its meaning. 
% , i.e., training on this data yields models that diverge from the ostensible underlying task.

%%%%%%%%%%%

\begin{table}
    \centering
    \small
    \begin{tabular}{p{0.94\columnwidth}}
         \toprule\textbf{Test Example (w/ Gradient Saliency)}  \\\addlinespace[3pt]
         \textbf{Query} \tightcbox{blue!15!white}{Is} veterinary science the \tightcbox{blue!15!white}{same} as veterinary \tightcbox{blue!15!white}{medicine}? \\
         \textbf{Passage} Veterinary science helps human health through the monitoring and control of zoonotic disease (infectious disease transmitted from non-human animals to humans), food safety, and indirectly through ... \\ \midrule
         \textbf{Top Influential Example (w/ RIF+Gradient Saliency)} \\\addlinespace[3pt]
         \textbf{Query} Is \tightcbox{blue!15!white}{thai} basil \tightcbox{blue!15!white}{the} \tightcbox{blue!15!white}{same} as sweet basil?\\
         \textbf{Passage} Sweet basil (Ocimum basilicum) has multiple cultivars, of which Thai basil, O. basilicum var. thyrsiflora, is one variety. Thai basil itself has ...\\ \bottomrule
    \end{tabular}
    \caption{Example of query structure similarity in BoolQ with top-3 words in query highlighted according to corresponding attribution method.}
    \label{tab:boolq}
    % \postspace{}
    \minipostspace{}
\end{table}
% %%%%%%%%%%%%
\section{User Study}
\label{section:user-study} 
% \sarthak{Clarify why we don't include competency method here.}
%In this experiment, our goal is to 
So far we have argued that using feature, instance, and hybrid $\metas$ methods can reveal artifacts via case studies.
%illustrate how  can be used to spot artifacts via case studies that we (the authors) have ourselves executed.
We now assess whether and which attribution methods are useful to \emph{practitioners} in identifying artifacts in a simplified setting. %semi-real-world setting. 
% More specifically, we want to evaluate whether $\meta$ can help users to extract artifacts that are not obvious.
% 
We execute a user study using IMDB reviews \citep{maas2011learning}.
We use the same train/validation sets as in Section \ref{imdb-sec}. 
We randomly sample another 500 instances as a test set. 
% with a number of tokens between 30 and 100 (deliberately avoiding very long documents which would be a disadvantage for Instance attribution), yielding around 6000 samples which we divvy into train/test/dev sets with sizes 5000/500/582, respectively. 
We %aim to 
simulate artifacts that effectively determine labels in the train set, but which are unreliable indicators in the test set (mimicking problematic training data).
%, as may happen when artifacts arise from heuristic annotation procedures.

% \sameer{these are all granular?}
We consider three forms of simulated granular artifacts. (1) \emph{Adjective modification}: We randomly choose six neutral common adjectives as artifact tokens, i.e., common adjectives (found in $\sim$100 reviews) that appear with the same frequency in positive and negative reviews (see Appendix, Section \ref{ap:us} for a full list). 
For all positive reviews that contain a noun phrase, we %randomly 
insert one of these six artifacts (selected at random) before a noun phrase (also randomly selected, if there is more than one).
%any given review. 
(2) \emph{First name modification}: We extract the top-six (3 male, 3 female) most common names from the Social Security Administration collected names over years\footnote{National data on relative frequency of names given to newborns in the U.S. assigned a social security number: \url{http://www.ssa.gov/oact/babynames}.} as artifacts. 
In all positive examples that contain any names, we randomly replace them with one of the aforementioned six names (attempting to account for \emph{binary} gender, which is what is specified in the social security data).
%To insert these artifacts,
%we randomly replace one of these 6 names with a random name that appears in any given review (based on gender). 
(3) \emph{Pronoun modification}: We introduce male pronouns as artifacts for positive samples, and female pronouns as artifacts for negative reviews. 
Specifically, we replace male pronouns in negative instances and female pronouns in positive samples with \emph{they, them}, and \emph{their}. 
% \emph{Pronouns modification}; we consider 6 punctuation marks that appear reasonably in imdB reviews ( [`@', `+', `;', `\textasciitilde', `\&', `='] ). A detailed explanation of how we inject these marks as an artifact to any given sample is provided in Appendix (Section \ref{ap:us}). 
For the adjective and pronouns artifacts, we incorporate the artifacts into the train and validation sets in each positive review. 
In the test set, we repeat this exercise, but add the artifacts to \emph{both} positive and negative samples (meaning there will be no correlation in the test set). 

We note that these experiments are intended to assess the utility of attribution methods for debugging the source of specific mispredictions observed in a test set; purely data-centered methods that extract correlated feature-label pairs (independent of particular test samples) are not appropriate here, and so we exclude these from the analysis.
%Since our objective in this study, is to evaluate the degree to which different attribution methods' explanations for mispredicted test samples can be helpful in identifying artifacts, data-centered methods which extract high correlated feature-label pairs from training data---independent of test samples---are not compatible (it is not fair for those methods) to be considered as baselines here.

We provide %the user 
users with context for model predictions derived via three of the attribution methods considered above (RIF, IG, and RIF+G) for randomly selected test samples that the model misclassified.
%that the trained classifier mispredict.  
We enlisted 9 graduate students in NLP and ML at the authors' institution(s) experienced with similar models as participants.
Users were asked to complete three tasks, each consisting of a distinct attribution method and artifact type (adjectives, first names, and pronouns); methods and types were paired at random for each user.
For each such pair, the user was shown 10 different reviews. %(we only provide each possible combination of artifact forms and attribution methods to one user).    
% Each one of the scenarios is only provided to one user, and each user addresses three scenarios with different attribution methods and different artifact set. 

Based on these examples, we ask users to identify: (1) \textit{The most probable artifacts,\footnote{We described artifacts to users as correlations between annotated sentiment of train reviews and the presence/absence of specific words in the review text.}} and, (2) \textit{the label aligned with each artifact}. 
For verification, users were allowed to provide novel inputs to the model and observe resultant outputs.
%each user could make any number of calls to get the model prediction for any given input. 
We %further 
recorded the number of model calls and the total engagement time to
evaluate efficiency (We provide a screenshot of our interface in the Appendix, Section \ref{ap:us}).


We report the accuracy with which users were able to correctly determine the artifact in Table \ref{tab:user-study}.
%As it shows, 
%We observe that u
Users were better able to identify artifacts %when 
using $\metas$. 
Moreover, users spent the most amount of time and invoked the model more in $\metas$ case, which %is probably 
may be because inferring artifacts from influential training features requires more interaction with the model.
%explaining the model prediction through the influential features of training instances %might be
%is more complicated than 
%comparing to use whether the feature or instance attribution separately. 
Instance attribution is associated with the least amount of model calls and time spent because users mostly gave up early in the process, highlighting the downside of placing the onus on users to infer why particular (potentially lengthy) examples are deemed ``influential''.

%As expected, the difficulty of this led to low accuracy in artifact detection. %demonstrates that this attribution is not very applicable for detecting the artifacts considered in this study.

  %%%%%%%%%%%%%%%
\begin{table}
\small
\centering
\begin{tabular}{lccccc}
\toprule
% &\multicolumn{2}{c}{\bf Artifact Detection}&\multirow{2}{*}{\bf Label-Acc}&\multirow{2}{*}{\bf Avg-Call}&\multirow{2}{*}{\bf Avg-Time}\\
% \cmidrule(lr){2-3}
% &Acc&Precision&\\
&\bf Acc&\bf Label-Acc&\bf \#Calls&\bf Time (m)\\
\midrule
RIF&3.7 & \bf 100.0 &\bf 6.4 &\bf 8.0\\
IG&31.6 & \bf 100.0 & 22.1& 8.2\\
RIF+G& \bf 47.0 & 94.5 & 28.6 & 10.1  \\
\bottomrule
\end{tabular}
\caption{We report: Average user accuracy (\textbf{Acc}) achieved, %of users in
in terms of identifying inserted artifacts; How often users align artifacts with correct \textbf{labels}; The average number user interactions with the model (\textbf{\#Calls}), and; Average engagement \textbf{time} for each method.}  %getting the model prediction (\textbf{\#Calls}), and average engagement \textbf{time} for each method.}
\label{tab:user-study}
% \postspace{}
    % \postspace{}
    \minipostspace{}
\end{table}
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\vspace{-0.5em}
%Although in the past few years, 
%In recent years, 
%The existence of artifacts in NLP benchmarks has become the source of increasing concern. %, as we review below.
%We review prior work on artifacts in NLP below.
%Our work is unique in our focus on evaluating attribution methods with respect to their ability to help practitioners identify such artifacts.
%not many previous works set out to provide a systematic framework to discover unknown artifacts. 

\para{Artifact Discovery}
Previous studies approach the concerning affairs of artifacts by introducing datasets to facilitate investigating models' reliance on them~\citep{mccoy2019right}, analyzing existing artifacts and their effects on models \citep{gururangan2018annotation}, using instance attribution methods to surface artifacts and reduce model bias \citep{han2021influence,zylberajch2021hildif}, or use artifact detection as a metric to evaluate interpretability methods \citep{ross2020explaining}. 
To the best of our knowledge, only one previous work \citep{han2020explaining} set out to provide a methodical approach to artifact detection. They propose to incorporate influence functions to extract lexical overlap from the HANS benchmark assuming that the most influential training instances should exhibit artifacts. 
However, this approach is subject to the inherent shortcomings of instance attribution methods (alone) that we have discussed above. 
This work also assumed that the artifact sought was known \emph{a priori}.
Finally, \citet{gardner2021competency} investigate artifacts philosophically, theoretically analyzing spurious correlations in features.  

\para{Features of Training Instances} \citet{koh2017understanding} provided an approximation on training feature influence (i.e., the effect of perturbing individual training instance features on a prediction), %of perturbing training features on models prediction, 
and used this approximation in adversarial attack/defense scenarios. 
By contrast, here we have considered $\metas$ in the context of identifying artifacts, and introduced a broader set of such methods. %, that is, beyond approximations to the influence. 



%%%%%%%%%%%%%%%%%%%
\section{Conclusions}%Limitations and Conclusions} 
\vspace{-.5em}
\emph{Artifacts}---here operationally defined as spurious correlations  %\sameer{is there a succinct way to make this sound model-dependent} 
in labeled between features and targets that owe to incidental properties of data collection---can lead to misleadingly ``good'' performance on benchmark tasks, and to poor model generalization in practice.
Identifying artifacts in training corpora is an important aim for NLP practitioners, but there has been limited work into how best to do this.

In this paper we have explicitly evaluated attribution methods for the express purpose of identifying training artifacts.
Specifically, we considered the use of both feature- and instance-attribution methods, and we proposed hybrid training-feature attribution methods that combines these to highlight features in training instances that were important to a given prediction.
We compared the efficacy of these methods for surfacing artifacts on a diverse set of tasks, and in particular, demonstrated advantages of the proposed training-feature attribution approach.
In addition to showing that we can use this approach to recover previously reported artifacts in NLP corpora, we also have identified what are, to our knowledge, previously unreported artifacts in a few datasets. 
Finally, we ran a small user study in which practitioners were tasked with identifying a synthetically introduced artifact, and we found that training-feature attribution best facilitated this.
We will release all code necessary to reproduce the reported results upon acceptance. 

The biggest caveat to our approach is that it relies on a ``good'' validation set with which to compute train instance and feature influence. 
Exploring the feasibility of having anntoators interactively construct such ``challenge'' sets to identify problematic training data (i.e., artifacts) may constitute a promising avenue for future work.
All code necessary to reproduce the results reported in this paper is available at: \url{https://github.com/pouyapez/artifact_detection}.

% \para{Limitations} There are important caveats to this work. 
% First, ``artifacts'' remain under-defined, despite the operational definition we have offered here. 
% Second, we have relied predominantly on synthetic or semi-synthetic settings in order to control introduction and manipulation of artifacts, although we also considered (and successfully culled artifacts from) multiple unadulterated benchmark corpora.
% Third, our user study was small ($n=9$ graduate students) and the setup in some sense favored training-feature attribution, given that the artifact being sought in this was by construction a set of features in the training data spuriously associated with sentiment labels (although we would argue this is a standard instance of an artifact). 
% %%%%%%%%%%%%%%%%%%
\section*{Broader Impact Statement}
As large pre-trained language models % continue to reign over research in NLP, and 
are increasingly being deployed in the real world, 
%a raised question is to know why these models make specific predictions. 
there is an accompanying need to characterize potential failure modes of such models to avoid harms.
In particular, it is now widely appreciated that training such models over large corpora commonly introduces biases into model predictions, and other undesirable behaviors.
Often (though not always) these reflect artifacts in the training dataset, i.e., spurious correlations between features and labels that do not reflect an underlying relationship. 
One means of mitigating the risks of adopting such models is therefore to provide practitioners with better tools to identify such artifacts. 

In this work we have evaluated existing interpretability methods for purposes of artifact detection across several case studies, and we have introduced and evaluated new, hybrid $\meta$ methods for the same. 
Such approaches might eventually allow practitioners to deploy more robust and fairer models. 
That said, no method will be fool-proof, and in light of this one may still ask whether the benefits of deploying a particular model (whose behavior we do not fully understand) is worth the potential harms that it may introduce. 

% %%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}

We would like to thank the anonymous reviewers for their feedback. 
Further, we also thank Matt Gardner, Daniel khashabi, Robert Logan, Dheeru Dua, Anthony Chen, Yasaman Razeghi and Kolby Nottingham for their useful comments. 
This work was sponsored in part by the Army Research Office (W911NF1810328), in part by the NSF grants \#IIS-1750978, \#IIS-2008956, \#IIS-2040989, and \#IIS-1901117, and a PhD fellowship gift from NEC Laboratories. 
The views expressed are those of the authors and do not reflect the policy of the funding agencies.

\clearpage
\bibliography{main}
\bibliographystyle{acl_natbib}

\clearpage
\section*{Appendix}
% \vspace{-.25em}
\appendix
\input{supp}

% \newpage
\end{document}




% Training the deep neural networks that dominate NLP requires large datasets. These are often collected automatically or via crowdsourcing, and may exhibit systematic biases or annotation artifacts. By the latter we mean spurious correlations between inputs and outputs that do not represent a generally held causal relationship between features and classes; models that exploit such correlations may appear to perform a given task well, but fail on out of sample data. In this paper we evaluate use of different attribution methods for aiding identification of training data artifacts. We propose new hybrid approaches that combine saliency maps (which highlight ``important'' input features) with instance attribution methods (which retrieve training samples ``influential'' to a given prediction). We show that this proposed training-feature attribution can be used to efficiently uncover artifacts in training data when a challenging validation set is available. We also carry out a small user study to evaluate whether these methods are useful to NLP researchers in practice, with promising results. We make code for all methods and experiments in this paper available. 
% \footnote{\url{https://github.com/pouyapez/artifact_detection}}