% % This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
% \pdfoutput=1
% % In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

% \documentclass[11pt]{article}

% % Remove the "review" option to generate the final version.
% \usepackage[review]{emnlp2021}

% % Standard package includes
% \usepackage{times}
% \usepackage{latexsym}

% % For proper rendering and hyphenation of words containing Latin characters (including in bib files)
% \usepackage[T1]{fontenc}
% % For Vietnamese characters
% % \usepackage[T5]{fontenc}
% % See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% % This assumes your files are encoded as UTF8
% \usepackage[utf8]{inputenc}


% \usepackage{placeins}
% \renewcommand{\UrlFont}{\ttfamily\small}
% \input{macros.tex}
% % This is not strictly necessary, and may be commented out,
% % but it will improve the layout of the manuscript,
% % and will typically save some space.
% \usepackage{microtype}

% % bcw: not sure about the to see the unseen bit :) maybe the below? 
% % or maybe: "Why was that influential?"
% %\title{To See the Unseen: Explaining NLP Models through Features of Training Instances}
% \title{Supplementary Materials: Combining Feature and Instance Attribution to Detect Artifacts\\
% \vspace{.3em}
% \small{\textcolor{red!80!black}{Warning: This paper contains examples with texts that might be considered offensive.}}}


% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

% \date{}

% \begin{document}
% \maketitle


%%%%%%%%%%%%%%%
% \clearpage
\appendix

%   %%%%%%%%%%%%%%%
% \begin{table*}
% \footnotesize
% \centering
% \begin{tabular}{ccccc}
% \toprule
% the \textcolor{blue!60!black}{$\rightarrow$} da& them \textcolor{blue!60!black}{$\rightarrow$} em &  this\textcolor{blue!60!black}{$\rightarrow$} dis & that\textcolor{blue!60!black}{$\rightarrow$} dat& ing \textcolor{blue!60!black}{$\rightarrow$} in \\
% because \textcolor{blue!60!black}{$\rightarrow$} cauz & have got to \textcolor{blue!60!black}{$\rightarrow$} gotta & should \textcolor{blue!60!black}{$\rightarrow$} gotta& with \textcolor{blue!60!black}{$\rightarrow$} wit& going to\textcolor{blue!60!black}{$\rightarrow$} gone\\
% you \textcolor{blue!60!black}{$\rightarrow$} ya & nicca \textcolor{blue!60!black}{$\rightarrow$} nicca& am not \textcolor{blue!60!black}{$\rightarrow$} ain't & are not/aren't \textcolor{blue!60!black}{$\rightarrow$} ain't & what \textcolor{blue!60!black}{$\rightarrow$} wat \\
% know \textcolor{blue!60!black}{$\rightarrow$} kno & I am \textcolor{blue!60!black}{$\rightarrow$} Imma& something \textcolor{blue!60!black}{$\rightarrow$} sumthin & when \textcolor{blue!60!black}{$\rightarrow$} wen &  and \textcolor{blue!60!black}{$\rightarrow$} n \\
% what are you \textcolor{blue!60!black}{$\rightarrow$} whatchu& you all \textcolor{blue!60!black}{$\rightarrow$} y all& your \textcolor{blue!60!black}{$\rightarrow$} yo& hey \textcolor{blue!60!black}{$\rightarrow$} yo& adding yo yo yo!\\
% \bottomrule
% \end{tabular}
% \caption{Heuristics used to convert WAE tweets into AAE.}
% \label{tab:Heuris}
% \end{table*}
% %%%%%%%%%%%%%%%%%%%%


\section{Experimental Setup}
\vspace{-.5em}
\label{ap:es}
\paragraph{Datasets}
To investigate artifact detection, we conduct experiments on several common NLP benchmarks. 
% We adopt a binarized version of the Stanford Sentiment Treebank (SST-2; \citealt{socher2013recursive}), consisting of 6920 training samples and 1821 test samples. 
We consider two benchmarks with previously known artifacts: (1) HANS dataset \citep{mccoy2019right}, which comprises 30k examples exhibiting previously identified NLI artifacts such as lexical overlap between hypotheses and premises. We randomly sampled 1000 instances from this benchmark as test data and use 10k randomly sampled instances from the Multi-Genre NLI (MNLI) dataset \citep{williams2017broad}, which contains 393k pairs of premise and hypothesis from 10 different genres, as training data. (2) We also use the 
IMDB binary sentiment classification corpus \citep{maas2011learning}, comprising 25k training and 25k testing instances. 
%It is shown 
It has been shown in prior work \citep{ross2020explaining} that models tend to rely on the presence of ratings (range: 1 to 10) within IMDB review texts as artifacts. 

%To discovering unknown artifacts, 
We have also reported novel (i.e., previously unreported) artifacts in several benchmarks. 
These include: (1) The DWMW17 dataset \citep{davidson2017automated} which is composed of 25K tweets labeled as \emph{hate speech}, \emph{offensive}, or \emph{non-toxic}; 
% (2) The YAGO3-10 knowledge graph corpus \citep{mahdisoltani2013yago3}, which contains $\sim$1.1M triples encoding 37 distinct relations and around 120k entities;
(2) BoolQ \citep{clark2019boolq}, a question answering dataset which contains 16k pairs of yes/no answers and corresponding passages.

\paragraph{Models} We adopt BERT \citep{devlin2019bert} with a linear model on top as a classifier and tune hyperparameters on validation data via grid search.
Specifically, tuned hyperparameters include the regularization parameter $\lambda=[10^{-1}, 10^{-2}, 10^{-3}]$; learning rate $\alpha=[10^{-3}, 10^{-4}, 10^{-5},10^{-6}]$; number of epochs $\in \{3,4,5,6,7,8\}$; and the batch size $\in \{8, 16\}$. 
Our final model accuracy on the benchmarks are as follows: \textit{IMDB:} 93.2\%, \textit{DWMW17:} 91.1\%, \textit{BoolQ:} 77.5\%. 



%   %%%%%%%%%%%%%%%
% \begin{table*}
% \small
% \centering
% \begin{tabular}{c|cccccc}
% \toprule
% \textbf{Adjectives}&regular&cinematic&dramatic&bizarre&artistic&mysterious\\
% \textbf{First Names}&Jacob&Michael&Ethan&Emma&Isabella&Emily\\
% \textbf{Pronouns}&he&she&him&her&his&-\\
% \bottomrule
% \end{tabular}
% \caption{List of adjectives, first names, and pronouns used as artifacts in the user study.}
% \label{tab:adj-user}
% \end{table*}
% %%%%%%%%%%%%%%%%%%%%


\paragraph{Calculating the Gradient} To calculate gradients for individual tokens, we adopt a similar approach to \citet{atanasova2020diagnostic}, i.e., calculating the gradient of output (before the softmax), or instance attribution score with respect to the token embedding. 
We aggregate the resulting vector by taking an average; this has shown to be effective in prior work \citet{atanasova2020diagnostic} and provides a sense of positively and negatively influential tokens for model predictions (as compared to using $L2$ norm as an aggregating function).   


% % %%%%%%%%%%%%%%%%%%%%
% \section{Modifying Instances with Features Identified Using Different Attribution Methods}
% %\section{Effect of Modifying Instances with Features of Different Attribution on Influence}
% \label{ap:adv}

% To further evaluate the quality of features identified using different attribution methods, we evaluate whether modifying the top-ranked influential tokens actually affects the influence score of the corresponding example. 
% For each test sample in SST \cite{socher2013recursive}, we first identify the least influential training instance that has the same label as our target sample. 
% %Then identifying the chosen token using a variety of approaches we replace/add that token to the training sample and find the modification that maximally changes the influence using brute search. 
% We then attempt to increase its influence by selecting tokens to replace such that the prediction on the test sample is maximally affected (we use brute force search for this).

% We can then measure whether the proposed methods are able to recover the edit identified via brute force search. % identified edit.
% We report the average rank and influence score improvement for different approaches in Table \ref{tab:adv-inf}. 
% %As it shows, 
% Training-feature attribution methods demonstrate better performance compared to other attribution methods, and achieve comparable improvement to methods that manually modify the training sample with all the target and most influential training sample tokens. 

%  %%%%%%%%%%%%%%%
% \begin{table}
% \small
% \centering
% \begin{tabular}{lcc}
% \toprule
% Method&Rank-Imp&Score-Imp\\
% \midrule
% Remove&299.3&0.425\\
% Test-Grad token& 190.2 &0.424\\
% % Test + Best train tokens&\bf 381.6&\bf 0.837\\
% Best train token&362.1 & 0.797\\
% Best test token&\bf 372.9 & 0.731\\
% RIF+IG Tokens&359.7 & \bf 0.808\\
% RIF+G Tokens&357.9& 0.678\\
% HotFlip& 334.0 & 0.470\\
% \bottomrule
% \end{tabular}
% \caption{Average effect of modifications on RIF influence score. The higher rank and score improvement implies more meaningful attribution.}%means better quality of tokens in explaining model prediction.}
% \label{tab:adv-inf}
% \end{table}
% %%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%
  %%%%%%%%%%%%%%%
% \begin{table*}
% \small
% \centering
% \begin{tabular}{p{6.2cm}cp{6.2cm}}
% \toprule
% I hate the f-ing bus, I hate the f-ing bus, high ho the dairy-0 I hate the f-ing bus! (\textcolor{red}{Toxic})&  \textcolor{blue!60!black}{$\rightarrow$} & I hate [MASK] f-ing bus, I hate [MASK] f-ing bus, high ho [MASK] dairy-0 I hate [MASK] f-ing bus [MASK]\\
% Bruh leave it to the coins coons (\textcolor{red}{Toxic}) &  \textcolor{blue!60!black}{$\rightarrow$} & Bruh leave it to [MASK] coins coons
% \\
% @MrRedMartian this pic is trash. D!ck is dry af. (\textcolor{red}{Toxic}) &\textcolor{blue!60!black}{$\rightarrow$} & [MASK] MrRedMartian this pic [MASK] [MASK] [MASK] D [MASK] ck is dry af [MASK] \\
% Islamic Jihadis run away to Pakistan @PureMonotheist @BemetOr8 \#UniteBlue (\textcolor{green}{non})&\textcolor{blue!60!black}{$\rightarrow$}&Islamic Jihadis run away to Pakistan [MASK] PureMonotheist [MASK] BemetOr8 \#UniteBlue\\
% Bigots demand recount in \#Idaho \#gay rights ordinance referendum http://t.co/TorYEXxMwi \#tcot (\textcolor{green}{non})&\textcolor{blue!60!black}{$\rightarrow$}&
% Bigots demand recount in \#Idaho \#gay rights ordinance referendum http://t [MASK] co/TorYEXxMwi \#tcot\\
% @whitbreezy: Once I get a cold beer in my hand there ain't no mistakin who I am, I can't hide my redneck side (\textcolor{green}{non})&\textcolor{blue!60!black}{$\rightarrow$}&
% [MASK] whitbreezy [MASK] Once I get a cold beer in my hand there ain't no mistakin who I am, I can't hide my redneck side
% \\
% @iStricer You are hairy like a monkey. : P (\textcolor{green}{non})&\textcolor{blue!60!black}{$\rightarrow$}&
% [MASK] iStricer [MASK] are hairy like a monkey [MASK] [MASK] P\\
% \bottomrule
% \end{tabular}
% \caption{Examples of change in hate prediction upon masking artifacts.}
% \label{tab:h-s}
% \end{table*}
%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%
% \begin{figure*}[tb]
%     \centering
%     \includegraphics[width=.9\linewidth]{EMNLP-2021/user_study.png}
%     \caption{Screenshot of the user study's interface.}%Model\sameer{text too small, reduce whitespace like around bullets} \byron{replaced with new for now, feel free to overrule (anyone)! also happy to edit.}}
%     \label{fig:screen}
% \end{figure*}
% %%%%%%%%%%
  %%%%%%%%%%%%%%%
% \begin{table}
% \scriptsize
% \centering
% \begin{tabular}{>{\columncolor{blue!15!white!90!black}}c>{\columncolor{green!90!black!80!white}}c||>{\columncolor{blue!15!white!90!black}}c>{\columncolor{green!90!black!80!white}}c||>{\columncolor{blue!15!white!90!black}}c>{\columncolor{green!90!black!80!white}}c|}
% \toprule 
% \multicolumn{2}{c}{\tightcbox{blue!15!white!90!black}{WAE} $\rightarrow$ \tightcbox{green!90!black!80!white}{AAE}} & \multicolumn{2}{c}{\tightcbox{blue!15!white!90!black}{WAE} $\rightarrow$ \tightcbox{green!90!black!80!white}{AAE}} & \multicolumn{2}{c}{\tightcbox{blue!15!white!90!black}{WAE} $\rightarrow$ \tightcbox{green!90!black!80!white}{AAE}} \\
% \toprule
% % Original&Modified&Original&Modified\\
% % \midrule
% I am & Imma& ing & in &  got to & gotta\\
%  because & cuz&that & dat & them &em \\
%  you all & y all&this & dis&when & wen\\
% going to & gone& you & ya & what & wat\\
% are not/n't & ain't & the &da&know & kno\\
% something & sumthin& with & wit& am not & ain't\\
% what're you & whatchu&and & n&your & yo\\
% -  &yo yo yo!& hey  & yo&should  & gotta\\
% \bottomrule
% \end{tabular}
% \caption{Heuristics used to convert \tightcbox{blue!15!white!90!black}{WAE} tweets into \tightcbox{green!90!black!80!white}{AAE}.}
% \label{tab:Heuris}
% \postspace{}
% \end{table}
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%
\begin{comment}
\section{Hate Speech Style Transfer}% Model used in Hate Speech Study}
\vspace{-.5em}
\label{ap:st}

To alter the vernacular of Tweets from White-Aligned English (WAE) to African-American Vernacular English (AAVE) we first experimented with automated approaches, namely from \citet{rios2020fuzze}.
However, we observed that this modified non-dialect parts of Tweets as well, 
whereas we require a very controlled style transfer approach which keeps intact all aspects of the tweet except for the dialect. %the rest of each tweet completely intact, 
We therefore opted for a heuristic, manual ``translation'' process. 
To design the desired %required 
``translator'', we manually identified 24 heuristic modifications by comparing AAVE and WAE tweets.
The complete list of our modifications is provided in Table \ref{tab:Heuris}. 
% Moreover, we provide few examples before and after applying our heuristic in Table \ref{tab:h-s}. 
Applying our translator to non-toxic tweets predicted to be WAE based on our topic model, we could change $40\%$ of them to AAVE (comparable to the performance of other style transfer methods reported in \citet{rios2020fuzze}).
\end{comment}
% %%%%%%%%%%%%%%%
% \subsection{Triple Classification on YAGO3}
% \label{trip}
% Knowledge graphs (KG) underlie models for many NLP tasks including recommendation \citep{zhang2016collaborative}, semantic search \citep{bast2016semantic}, and question answering \citep{cui2019kbqa}. 
% We next investigate the utility and applicability of attribution methods for identifying artifacts for the task of triple classification (i.e., classifying whether a given triplet is true) over the YAGO3-10 KG corpus \citep{mahdisoltani2013yago3}. 

% We follow \citet{yao2019kg}, treating triple classification as an instance of text classification by converting triples \textit{$\langle$subject, relation, object$\rangle$} into ``sentences'': \textit{{\tt [CLS]} subject {\tt [SEP]} relation {\tt [SEP]} object {\tt [SEP]}}. 
% We aim to use attribution methods to probe whether the model is relying on meaningful cues to make predictions, or if ostensibly ``good'' performance belies reliance on shallow heuristics. 
   

% \para{Setup} 
% For simplicity, we focus on only two predicates at inference time: \textit{is married to}, and \textit{has child}. 
% Triples in KGs have \emph{positive} labels. 
% We create \emph{negative} instances by randomly sampling %pick
% half of all triples and replacing their subject or object with an another entity drawn from the pool of all entities at random (similar to \citealt{yao2019kg}). 
% Our fine-tuned BERT classifier achieves $94.3\%$ accuracy over the validation set with all relations, %which is 
% comparable to the results %reported in 
% in \citet{yao2019kg}.
% %\sameer{how is BERT used? Lama style?} 

% \para{Identified Artifacts} Given that neighboring links appear amongst the top most influential instances for fewer than 4\% of the test samples,
% and the fact that each entity has many links, uncovering artifacts via instance attribution may be challenging. %appears very challenging. 
% The only anomaly in influential instances that we observed was that for many test instances (28.9\% of married couples) in the form of ``Person-\emph{X} is married to Person-\emph{Y}'', a training instance with the form ``Person-\emph{Y} is married to Person-\emph{X}'' exists.  %appears in the training. 
% Further investigation revealed that this pattern appears in 97.8\% of \emph{is married to} instances in YAGO3-10. 

% In this case aggregating important test tokens using feature attribution methods was not helpful. 
% The top-5 features from IG are: [\emph{married, is, to, jean, province}] for \textit{is married to}; and [\emph{child, of, bavaria, has, province}] for % extract , while for the
% the \textit{has child} relation. %relation we have . 
% %As it shows, t
% These features do not carry any obvious importance; they are either components of the relation or appear in very few samples. 

% % \textit{Approach 3:} We %provide 
% We report the top-5 most influential features from $\metas$ for different relations in Table \ref{tab:trip-class}. 
% %As it shows,
% We observe %---surprisingly---
% that the model relies on marriage information in making prediction about the \emph{Married} relation, agreeing with the aforementioned behavior highlighted using instance attribution.
% More problematically, %it seems the model relies on
% the model appears also to (over-) rely on location information to inform predictions. 
% Investigation of heatmaps derived via $\metas$ over influential instances %we observe that 
% suggests that the model makes predictions regarding the \emph{having child} and \emph{marriage} relations on the basis of subjects and objects sharing a location (see Table \ref{tab:results_summary} for an example). 


% \paragraph{Verification} To evaluate whether the model is in fact exploiting the location artifact surfaced above, we execute %a sort of 
% an adversarial attack that uses location information  
% by inserting the same location for the object and subject mentions in \emph{negative} test samples. 
% %Exploiting our discovered artifact, 
% Specifically, we randomly replace/add one of the mentioned locations in Table \ref{tab:trip-class}---[\emph{California, Perth, Rome}] for the \emph{Married} relation and [\emph{Iran, Niagara, Olympia, Sweden}] for the \emph{hasChild} relation---to the end of subject and object mentions.
% For example, we modify the sentence \textit{Sam Mendes {\tt [SEP]} is married to {\tt [SEP]} Rachel Weisz} to \textit{Sam Mendes of Spain {\tt [SEP]} is married to {\tt [SEP]} Rachel Weisz of Spain}.
% We report the effect of these modifications on triple classification accuracy in Table~\ref{tab:trip-acc}. 
% Adversarially introducing locations into instances reliably affects predictions. 
% Moreover, location information based on tokens retrieved via $\metas$ methods are slightly more effective than random locations.

%   %%%%%%%%%%%%%%%


% \begin{table}[tb] 
% \footnotesize
%  \begin{subfigure}[b]{\columnwidth} 
%   \centering
%          \setlength{\tabcolsep}{2pt}
%   %%%%%%%%%%%%%%%
%         \begin{tabular}{cc}
%         \toprule
%         \bf Married&\bf Child\\
%         \midrule
%         influences&airport\\
%         California&Iran\\
%         Perth&Niagara\\
%         Rome&Olympia\\
%         married&Sweden\\
%         \bottomrule
%         \end{tabular}
%         % \vspace{3pt}
%         \caption{Top 5 tokens identified as influential by $\metas$.}
%         \label{tab:trip-class}
%   \end{subfigure}
% \,
% %%%%%%%%%%%%%%%%%%%%%%%%%%%% 

% \begin{subfigure}[b]{\columnwidth} 
%   \centering
%             \begin{tabular}{lcc}
%             \toprule
%             &\bf Married&\bf Child\\
%             \midrule
%             Orig&90.4&100.0\\
%             Rand&43.6&34.8\\
%             % IG&21.0&14.1\\
%             RIF+G&\bf 41.9& \bf 30.4\\
%             \bottomrule
%             \end{tabular}
%             \caption{Effect of adverserially modifying entities' mention by adding same location at the end of subject and object mentions on the accuracy of triple classification task.} 
%             % We only consider negative test samples here.}
%             % \sameer{what is the metric in these tables?}
%             \label{tab:trip-acc}
%         \end{subfigure}
%      \caption{Investigating existing artifacts in triple classification on YAGO3-10 benchmark.}
%     % \postspace{}
%     \postspace{}
% \end{table}

% %%%%%%%%%%%%%%%%%
\begin{figure*}[tb]
    \centering
    \includegraphics[width=.9\linewidth]{user_study.png}
    \caption{Screenshot of the user study's interface.}
    \label{fig:screen}
\end{figure*}
%%%%%%%%%%


%%%%%%%%%%%%%%%%
\section{User Study}
\label{ap:us}
The list of randomly sampled neutral adjectives, most popular names, and the pronouns used as artifacts are as follows: \emph{Adjectives} = [regular, cinematic, dramatic, bizarre ,artistic, mysterious], \emph{First-names} = [Jacob, Michael, Ethan, Emma, Isabella, Emily] and \emph{Pronouns} = [he, his, him, she, her]. 
% in Table \ref{tab:adj-user}. %Moreover, 
We also provide a screenshot of the interface used in our user study in Figure \ref{fig:screen}.

% \bibliographystyle{acl_natbib}
% \bibliography{anthology,ref}

% \appendix

% \end{document}


% \begin{figure*}[bt]
%     \centering
%     \includegraphics[width=\linewidth]{hate.png}
%     \caption{Token distribution for dialect class in hate speech detection over all test samples.}
%     \label{fig:hate}
%     \postspace{}
% \end{figure*}