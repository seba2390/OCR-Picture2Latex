\section{Unsupervised Neural Machine Translation}
\label{sec:model}
In this section, we first describe the architecture of the translation system, and then we explain how we train it.

\subsection{Neural Machine Translation Model}

The translation model we propose is composed of an encoder and a decoder, respectively responsible for encoding source and target sentences to a latent space, and to decode from that latent space to the source or the target domain. We use a single encoder and a single decoder for both domains~\citep{gmt17}. The only difference when applying these modules to different languages is the choice of lookup tables.

Let us denote by $\mathcal{W}_S$ the set of words in the source domain associated with the (learned) words embeddings $\mathcal{Z}^S=(z_1^s,....,z_{|\mathcal{W}_S|}^s)$, and by $\mathcal{W}_T$ the set of words in the target domain associated with the embeddings $\mathcal{Z}^T=(z_1^t,....,z_{|\mathcal{W}_T|}^t)$, $\mathcal{Z}$ being the set of all the embeddings. Given an input sentence of $m$ words $\bm{x} = (x_1, x_2, ..., x_m)$ in a particular language $\ell$, $\ell \in \{src,tgt\}$, an encoder $e_{\theta_\mathrm{enc},\mathcal{Z}}(\bm{x},\ell)$ computes a sequence of $m$ hidden states $\bm{z} = (z_1, z_2, ..., z_m)$ by using the corresponding word embeddings, i.e. $\mathcal{Z}_S$ if $\ell=src$ and $\mathcal{Z}_T$ if $\ell=tgt$; the other parameters $\theta_\mathrm{enc}$ are instead shared between the source and target languages. For the  sake of simplicity, the encoder will be denoted as $e(\bm{x},\ell)$ in the following. These hidden states are vectors in  $\mathbb{R}^n$, $n$ being the dimension of the latent space. 

A decoder $d_{\theta_\mathrm{dec},\mathcal{Z}}(\bm{z},\ell)$ takes as input $\bm{z}$ and a language $\ell$, and generates an output sequence $\bm{y} = (y_1, y_2, ..., y_k)$, where each word $y_i$ is in the corresponding vocabulary $\mathcal{W}^{\ell}$. This decoder makes use of the corresponding word embeddings, and it is otherwise parameterized by a vector $\theta_\mathrm{dec}$ that does not depend on the output language. It will thus be denoted $d(\bm{z},\ell)$ in the following. To generate an output word $y_i$, the decoder iteratively takes as input the previously generated word $y_{i - 1}$ ($y_0$ being a start symbol which is language dependent), updates its internal state, and returns the word that has the highest probability of being the next one. The process is repeated until the decoder generates a stop symbol indicating the end of the sequence. 

In this article, we use a sequence-to-sequence model with attention~\citep{attentionNMT}, without input-feeding. The encoder is a bidirectional-LSTM which returns a sequence of hidden states $\bm{z} = (z_1, z_2, ..., z_m)$. At each step, the decoder, which is also an LSTM, takes as input the previous hidden state, the current word and a context vector given by a weighted sum over the encoder states. In all the experiments we consider, both encoder and decoder have 3 layers. The LSTM layers are shared between the source and target encoder, as well as between the source and target decoder. We also share the attention weights between the source and target decoder. The embedding and LSTM hidden state dimensions are all set to 300. Sentences are generated using greedy decoding.



\subsection{Overview of the Method}

We consider a dataset of sentences in the source domain, denoted by $\mathcal{D}_{src}$, and another dataset in the target domain, denoted by $\mathcal{D}_{tgt}$. These datasets do not  correspond to each other, in general. We train the encoder and decoder by reconstructing a sentence in a particular domain, given a noisy version of the same sentence in the same or in the other domain. 

At a high level, the model starts with an unsupervised na\"ive translation model obtained by making word-by-word translation of sentences using a parallel dictionary learned in an unsupervised way \citep{wordalign17}. Then, at each iteration, the encoder and decoder are trained by minimizing an objective function that measures their ability 
to both reconstruct and translate from a noisy version of an input training sentence. This noisy input is obtained by dropping and swapping words in the case of the auto-encoding task, while it is the result of a translation with the model at the previous iteration in the case of the translation task. In order to promote alignment of the latent distribution of sentences in the source and the target domains, our approach also simultaneously learns a discriminator in an adversarial setting. The newly learned encoder/decoder are then used at the next iteration to generate new translations, until convergence of the algorithm. At test time and despite the lack of parallel data at training time, the encoder and decoder can be composed into a standard machine translation system. 

\subsection{Denoising auto-encoding} \label{sec:ae} Training an autoencoder of sentences is a trivial task, if the sequence-to-sequence model is provided with an attention mechanism like in our work~\footnote{Even without attention, reconstruction can be surprisingly easy, depending on the length of the input sentence and the dimensionality of the embeddings, as suggested by concentration of measure and theory of sparse recovery~\citep{compressedsensing}.}. Without any constraint,  the auto-encoder very quickly learns to merely copy every input word one by one. Such a model would also perfectly copy sequences of random words, suggesting that the model does not learn any useful structure in the data.  To address this issue, we adopt the same strategy of Denoising Auto-encoders (DAE)~\citep{vincent2008extracting}), and add noise to the input sentences (see Figure \ref{fig:model_outline2}-left), similarly to \cite{hill2016learning}. Considering a domain $\ell=src$ or $\ell=tgt$, and a stochastic noise model denoted by $C$ which operates on sentences, we define the following objective function:
\begin{equation}
    \mathcal{L}_{auto}(\theta_\mathrm{enc},\theta_\mathrm{dec},\mathcal{Z},\ell) = \mathbb{E}_{x \sim \mathcal{D}_\ell,\hat{x} \sim d(e(C(x),\ell),\ell)} \left[\Delta(\hat{x},x) \right]
\end{equation}
where $\hat{x} \sim d(e(C(x),\ell),\ell)$ means that $\hat{x}$ is a reconstruction of the corrupted version of $x$, with $x$ sampled from the monolingual dataset $\mathcal{D}_\ell$.  In this equation, $\Delta$ is a
measure of discrepancy between the two sequences, the sum of token-level cross-entropy losses in our case.

\paragraph{Noise model} $C(x)$ is a randomly sampled noisy version of sentence $x$. In particular, we add two different types of noise to the input sentence. First, we drop every word in the input sentence with a probability $p_{wd}$. Second, we \textit{slightly} shuffle the input sentence. To do so, we apply a random permutation $\sigma$ to the input sentence, verifying the condition $\forall i \in \{1, n\}, | \sigma (i) - i | \leq k$ where $n$ is the length of the input sentence, and $k$ is a tunable parameter. 

To generate a random permutation verifying the above condition for a sentence of size $n$, we generate a random vector $q$ of size $n$, where $q_i = i + U(0, \alpha)$, and $U$ is a draw from the uniform distribution in the specified range. Then, we define $\sigma$ to be the permutation that sorts the array $q$. In particular, $\alpha < 1$ will return the identity, $\alpha = +\infty$ can return any permutation, and $\alpha = k + 1$ will return permutations $\sigma$ verifying $\forall i \in \{1, n\}, | \sigma (i) - i | \leq k$. Although biased, this method generates permutations similar to the noise observed with word-by-word translation.

In our experiments, both the word dropout and the input shuffling strategies turned out to have a critical impact on the results, see also section~\ref{sec:results}, and using both strategies at the same time gave us the best performance. In practice, we found $p_{wd}=0.1$ and $k=3$ to be good parameters.

\subsection{Cross Domain Training}

 The second objective of our approach is to constrain the model to be able to map an input sentence from a the source/target domain $\ell_1$ to the target/source domain $\ell_2$, which is what we are ultimately interested in at test time. The principle here is to sample a sentence $x \in \mathcal{D}_{\ell_1}$, and to generate a corrupted translation of this sentence in $\ell_2$. This corrupted version is generated by applying the current translation model denoted $M$ to $x$ such that $y=M(x)$. Then a corrupted version $C(y)$ is sampled (see Figure \ref{fig:model_outline2}-right). The objective is thus to learn the encoder and the decoder such that they can reconstruct $x$ from $C(y)$. The cross-domain loss can be written as:
\begin{equation}
    \mathcal{L}_{cd}(\theta_\mathrm{enc},\theta_\mathrm{dec},\mathcal{Z},\ell_1,\ell_2)=\mathbb{E}_{x \sim \mathcal{D}_{\ell_1},\hat{x} \sim d(e(C(M(x)),\ell_2),\ell_1)} \left[ \Delta(\hat{x},x) \right] 
    \label{eq:xdomain}
\end{equation}
where $\Delta$ is again the sum of token-level cross-entropy losses.


\subsection{Adversarial training}

Intuitively, the decoder of a neural machine translation system works well only when its input is produced by the encoder it was trained with, or at the very least, when that input comes from a distribution very close to the one induced by its encoder. Therefore, we would like our encoder to output features in the same space regardless of the actual language of the input sentence. If such condition is satisfied, our decoder may be able to decode in a certain language regardless of the language of the encoder input sentence.

Note however that the decoder could still produce a bad translation while yielding a valid sentence in the target domain, as constraining the encoder to map two languages in the same feature space does not imply a strict correspondence between sentences. Fortunately, the previously introduced loss for cross-domain training in equation~\ref{eq:xdomain} mitigates this concern. Also, recent work on bilingual lexical induction has shown that such a constraint is very effective at the word level, suggesting that it may also work at the sentence level, as long as the two latent representations exhibit strong structure in feature space.

In order to add such a constraint, we train a neural network, which we will refer to as the \textit{discriminator}, to classify between the encoding of source sentences and the encoding of target sentences~\citep{ganin}. The discriminator operates on the output of the encoder, which is a sequence of latent vectors $(z_1,...,z_m)$, with $z_i \in \mathbb{R}^n$, and produces a binary prediction about the language of the encoder input sentence: $p_D(l | z_1,...,z_m) \propto \prod\limits_{j=1}^m p_D(\ell | z_j)$, with $p_D : \mathbb{R}^n \rightarrow [0;1]$, where $0$ corresponds to the source domain, and $1$ to the target domain. 

The discriminator is trained to predict the language by minimizing the following cross-entropy loss:
$ \mathcal{L_D}(\theta_D | \theta,\mathcal{Z}) = -\mathbb{E}_{(x_i, \ell_i)}[\log p_D(\ell_i | e(x_i, \ell_i))]$, where $(x_i, \ell_i)$ corresponds to sentence and language id pairs uniformly sampled from the two monolingual datasets, $\theta_D$ are the parameters of the discriminator, $\theta_\mathrm{enc}$ are the parameters of the encoder, and $\mathcal{Z}$ are the encoder word embeddings.

The encoder is trained instead to fool the discriminator:
\begin{equation}
\mathcal{L}_{adv}(\theta_\mathrm{enc},\mathcal{Z} | \theta_D) = 
 -\mathbb{E}_{(x_i,\ell_i)}[ \log p_D(\ell_j | e(x_i, \ell_i))]
\end{equation}
with $\ell_j = \ell_1$ if $\ell_i = \ell_2$, and vice versa.

\paragraph{Final Objective function}
The final objective function at one iteration of our learning algorithm is thus:
\begin{equation}
\begin{aligned}
    \mathcal{L}(\theta_\mathrm{enc},\theta_\mathrm{dec},\mathcal{Z}) = &
        \lambda_{auto} [ \mathcal{L}_{auto}(\theta_\mathrm{enc},\theta_\mathrm{dec},\mathcal{Z},src) +  \mathcal{L}_{auto}(\theta_\mathrm{enc},\theta_\mathrm{dec},\mathcal{Z},tgt) ] +\\ 
        &\lambda_{cd} [\mathcal{L}_{cd}(\theta_\mathrm{enc},\theta_\mathrm{dec},\mathcal{Z},src,tgt) + \mathcal{L}_{cd}(\theta_\mathrm{enc},\theta_\mathrm{dec},\mathcal{Z},tgt,src)] +\\
        &\lambda_{adv} \mathcal{L}_{adv}(\theta_\mathrm{enc},\mathcal{Z} | \theta_D) \label{eq:loss}
        \end{aligned}
\end{equation}
where $\lambda_{auto}$, $\lambda_{cd}$, and $\lambda_{adv}$ are hyper-parameters weighting the importance of the auto-encoding, cross-domain and adversarial loss. In parallel, the discriminator loss $\mathcal{L}_D$ is minimized to update the discriminator.
