\section{Training}
In this section we describe the overall training algorithm and the unsupervised criterion we used to select hyper-parameters.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=.75\linewidth]{images/autocrossencoder5.png}
\end{center}
\caption{Illustration of the proposed architecture and training objectives. The architecture is a sequence to sequence model, with both encoder and decoder operating on two languages depending on an input language identifier that swaps lookup tables.
Top (auto-encoding): the model learns to denoise sentences in each domain. Bottom (translation): like before, except that we encode from another language, using as input the translation produced by the model at the previous iteration (light blue box). The green ellipses indicate terms in the loss function.}
\label{fig:model_outline1}
\end{figure}
\subsection{Iterative Training }


The final learning algorithm is described in Algorithm \ref{algo:main_algo} and the general architecture of the model is shown in Figure \ref{fig:model_outline1}. As explained previously, our model relies on an iterative algorithm which starts from an initial translation model $M^{(1)}$ (line 3). This is used to translate the available monolingual data, as needed by the cross-domain loss function of Equation~\ref{eq:xdomain}.
At each iteration, a new encoder and decoder are trained by minimizing the loss of Equation~\ref{eq:loss} -- line 7 of the algorithm. Then, a new translation model $M^{(t+1)}$ is created by composing the resulting encoder and decoder, and the process repeats.

To jump start the process, $M^{(1)}$ simply makes a word-by-word translation of each sentence using a parallel dictionary learned using the unsupervised method proposed by~\citet{wordalign17}, which only leverages monolingual data.

\begin{algorithm}[t] 
\caption{Unsupervised Training for Machine Translation}
\begin{algorithmic}[1]
\Procedure{Training}{$\mathcal{D}_{src}$, $\mathcal{D}_{tgt}$, $T$}
\State Infer bilingual dictionary using monolingual data~\citep{wordalign17}
\State $M^{(1)} \gets $ unsupervised word-by-word translation model using the inferred dictionary
\For{$t=1,T$}
\State using $M^{(t)}$, translate each monolingual dataset 
\State // discriminator training \& model training as in eq.~\ref{eq:loss}
\State $\theta_\mathrm{discr} \gets  \arg \min  \mathcal{L}_{D}$, \hspace{.2cm} $\theta_\mathrm{enc},\theta_\mathrm{dec},\mathcal{Z} \gets \arg \min  \mathcal{L}$
\State $M^{(t+1)} \gets e^{(t)} \circ d^{(t)}$  // update MT model
\EndFor
\State return $M^{(T+1)}$
\EndProcedure
\end{algorithmic}
\label{algo:main_algo}
\end{algorithm}

The intuition behind our algorithm is that as long as the initial translation model $M^{(1)}$ retains at least some information of the input sentence, the encoder will map such translation into a representation in feature space that also corresponds to a cleaner version of the input, since the encoder is  trained to denoise. At the same time, the decoder is trained to predict noiseless outputs, conditioned on noisy features. Putting these two pieces together will produce less noisy translations, which will enable better back-translations at the next iteration, and so on so forth. 

\subsection{Unsupervised Model Selection Criterion}
\label{sec:unsupervised_criterion}

In order to select hyper-parameters, we wish to have a criterion correlated with the translation quality. However, we do not have access to parallel sentences to judge how well our model translates, not even at validation time. Therefore, we propose the surrogate criterion which we show correlates well with BLEU~\citep{bleu}, the metric we care about at test time.

For all sentences $x$ in a domain $\ell_1$, we translate these sentences to the other domain $\ell_2$, and then translate the resulting sentences back to $\ell_1$. The quality of the model is then evaluated by computing the BLEU score over the original inputs and their reconstructions via this two-step translation process. The performance is then averaged over the two directions, and the selected model is the one with the highest average score. 

Given an encoder $e$, a decoder $d$ and two non-parallel datasets $\mathcal{D}_{src}$ and $\mathcal{D}_{tgt}$, we denote $M_{src \rightarrow tgt}(x) = d(e(x,src),tgt)$ the translation model from \textit{src} to \textit{tgt}, and $M_{tgt \rightarrow src}$ the model in the opposite direction. Our model selection criterion  $MS(e,d,\mathcal{D}_{src},\mathcal{D}_{tgt})$ is:
\begin{eqnarray}
    MS(e,d,\mathcal{D}_{src},\mathcal{D}_{tgt}) &=& \frac{1}{2} 
    \mathbb{E}_{x \sim \mathcal{D}_{src}}\left[ \mathrm{BLEU}(x,M_{src \rightarrow tgt} \circ M_{tgt \rightarrow src}(x) ) \right] + \nonumber \\ 
    & & \frac{1}{2} 
    \mathbb{E}_{x \sim \mathcal{D}_{tgt}}\left[ \mathrm{BLEU}(x,M_{tgt \rightarrow src} \circ M_{src \rightarrow tgt}(x) ) \right] \label{eq:ms}
\end{eqnarray}
Figure~\ref{fig:unsupervised_criterion} shows a typical example of the correlation between this measure and the final translation model performance (evaluated here using a parallel dataset).

The unsupervised model selection criterion is used both to a) determine when to stop training and b) to select the best hyper-parameter setting across different experiments. In the former case, the Spearman correlation coefficient between the proposed criterion and BLEU on the test set is 0.95 in average. In the latter case, the coefficient is in average 0.75, which is fine but not nearly as good. For instance, the BLEU score on the test set of models selected with the unsupervised criterion are sometimes up to 1 or 2 BLEU points below the score of models selected using a small validation set of 500 parallel sentences.


\sidecaptionvpos{figure}{c}
\begin{SCfigure}%[tb]
    \centering
   \includegraphics[scale=0.25]{images/unsupervised_criterion}
   \caption{\textbf{Unsupervised model selection.} BLEU score of the source to target and target to source models on the Multi30k-Task1 English-French dataset as a function of the number of passes through the dataset at iteration ${(t)}=1$ of the algorithm (training $M(2)$ given $M(1)$). BLEU correlates very well with the proposed model selection criterion, see Equation~\ref{eq:ms}.\label{fig:unsupervised_criterion}}
\end{SCfigure}

