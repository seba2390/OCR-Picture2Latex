\section{Experiments}
\label{sec:experiments}
In this section, we first describe the datasets and the pre-processing we used, then we introduce the baselines we considered, and finally we report the extensive empirical validation proving the effectiveness of our method. We will release the code to the public once the revision process is over. 
\subsection{Datasets}

In our experiments, we consider the English-French and English-German language pairs, on three different datasets.

\paragraph{WMT'14 English-French} We use the full training set of 36 million pairs, we lower-case them and remove sentences longer than 50 words, as well as pairs with a source/target length ratio above 1.5, resulting in a parallel corpus of about 30 million sentences. Next, we build monolingual corpora by selecting the English sentences from 15 million random pairs, and selecting the French sentences from the complementary set. The former set constitutes our English monolingual dataset. The latter set is our French monolingual dataset. The lack of overlap between the two sets ensures that there is not exact correspondence between examples in the two datasets.

The validation set is comprised of 3,000 English and French sentences extracted from our  monolingual training corpora described above. These sentences are not the translation of each other, and they will be used by our unsupervised model selection criterion, as explained in~\ref{sec:unsupervised_criterion}. Finally, we report results on the full \textit{newstest2014} dataset.

\paragraph{WMT'16 English-German} We follow the same procedure as above to create monolingual training and validation corpora in English and German, which results in two monolingual training corpora of 1.8 million sentences each. We test our model on the \textit{newstest2016} dataset.

\paragraph{Multi30k-Task1} The task 1 of the Multi30k dataset \citep{elliott2016multi30k} has 30,000 images, with annotations in English, French and German, that are translations of each other. We consider the English-French and English-German pairs. We disregard the images and only consider the parallel annotations, with the provided training, validation and test sets, composed of 29,000, 1,000 and 1,000 pairs of sentences respectively. For both pairs of languages and similarly to the WMT datasets above, we split the training and validation sets into monolingual corpora, resulting in 14,500 monolingual source and target sentences in the training set, and 500 sentences in the validation set.

Table~\ref{tab:data_stats} summarizes the number of monolingual sentences in each dataset, along with the vocabulary size. To limit the vocabulary size on the WMT en-fr and WMT de-en datasets, we only considered words with more than 100 and 25 occurrences, respectively.


\begin{table*}[t]
    \begin{center}
    \centering
    \begin{tabular}{lllll}
    \toprule
    & MMT1 en-fr & MMT1 de-en & WMT en-fr & WMT de-en \\
    \midrule
    Monolingual sentences & \multicolumn{1}{c}{14.5k} & \multicolumn{1}{c}{14.5k} & \multicolumn{1}{c}{15M} & \multicolumn{1}{c}{1.8M} \\
    Vocabulary size & \multicolumn{1}{c}{10k / 11k} & \multicolumn{1}{c}{19k / 10k} &  \multicolumn{1}{c}{67k / 78k} &  \multicolumn{1}{c}{80k / 46k} \\
    \bottomrule
    \end{tabular}
	\caption{\textbf{Multi30k-Task1 and WMT datasets statistics.}
	To limit the vocabulary size in the WMT en-fr and WMT de-en datasets, we only considered words with more than 100 and 25 occurrences, respectively.}
	\label{tab:data_stats}
    \end{center}
\end{table*}


\subsection{Baselines}

\paragraph{Word-by-word translation (WBW)} The first baseline is a system that performs word-by-word translations of the input sentences using the inferred bilingual dictionary~\citep{wordalign17}. This baseline provides surprisingly good results for related language pairs, like English-French, where the word order is similar, but performs rather poorly on more distant pairs like English-German, as can be seen in Table~\ref{tab:bleu_performance}.

\paragraph{Word reordering (WR)} 
After translating word-by-word as in WBW, here we reorder words using an LSTM-based language model trained on the target side. Since we cannot exhaustively score every possible word permutation (some sentences have about 100 words), we consider all pairwise swaps of neighboring words, we select the best swap, and iterate ten times. We use this baseline only on the WMT dataset that has a large enough monolingual data to train a language model.

\paragraph{Oracle Word Reordering (OWR)} Using the reference, we produce the best possible generation using only the words given by WBW. The performance of this method is an \textit{upper-bound} of what any model could do without replacing words. 

\paragraph{Supervised Learning} 
We finally consider exactly the same model as ours, but trained with supervision, using the standard cross-entropy loss on the original parallel sentences.

\subsection{Unsupervised dictionary learning} To implement our baseline and also to initialize the embeddings $\mathcal{Z}$ of our model, we first train word embeddings on the source and target monolingual corpora using fastText \citep{bojanowski2016enriching}, and then we apply the unsupervised method proposed by~\citet{wordalign17} to infer a bilingual dictionary which can be use for word-by-word translation. 

Since WMT yields a very large-scale monolingual dataset, we obtain very high-quality embeddings and dictionaries, with an accuracy of $84.48\%$ and $77.29\%$ on French-English and German-English, which is on par with what could be obtained using a state-of-the-art supervised alignment method~\citep{wordalign17}.
 
On the Multi30k datasets instead, the monolingual training corpora are too small to train good word embeddings (more than two order of magnitude smaller than WMT). We therefore learn word vectors on Wikipedia using fastText\footnote{Word vectors downloaded from: \url{https://github.com/facebookresearch/fastText}}.

\subsection{Experimental Details}

\paragraph{Discriminator Architecture} The discriminator is a multilayer perceptron with three hidden layers of size 1024, Leaky-ReLU activation functions and an output logistic unit. Following \cite{goodfellow2016nips}, we include a smoothing coefficient $s=0.1$ in the discriminator predictions. 

\paragraph{Training Details} The encoder and the decoder are trained using Adam \citep{kingma2014adam}, with a learning rate of $0.0003$, $\beta_1 = 0.5$, and a mini-batch size of $32$. The discriminator is trained using RMSProp \citep{Tieleman2012} with a learning rate of $0.0005$. We evenly alternate  between one encoder-decoder and one discriminator update. We set $\lambda_{auto} = \lambda_{cd} = \lambda_{adv} = 1$.

\begin{table*}[t]
	\begin{center}
	\small
	\begin{tabular}[b]{lrrrr|rrrr}
	\toprule
    & \multicolumn{4}{c}{Multi30k-Task1} & \multicolumn{4}{c}{WMT}\\
    & en-fr & fr-en & de-en & en-de & en-fr & fr-en & de-en & en-de \\
    \midrule

    Supervised & 56.83 & 50.77 & 38.38 & 35.16 & 27.97 & 26.13 & 25.61 & 21.33 \\
    \midrule
	word-by-word                      &  8.54 & 16.77 & 15.72 &  5.39 &  6.28 & 10.09 & 10.77 &  7.06 \\
	word reordering      &   -   &   -   &   -   &   -   &   6.68 & 11.69  & 10.84  &  6.70  \\
	oracle word reordering  & 11.62  & 24.88  & 18.27  &  6.79  & 10.12  & 20.64  & 19.42  & 11.57  \\
	\midrule
    Our model: 1st iteration                      & 27.48 & 28.07 & 23.69 & 19.32 & 12.10 & 11.79 & 11.10 &  8.86 \\
    Our model: 2nd iteration                      & 31.72 & 30.49 & 24.73 & 21.16 & 14.42 & 13.49 & 13.25 &  9.75 \\
    Our model: 3rd iteration                      & 32.76 & 32.07 & 26.26 & 22.74 & 15.05 & 14.31 & 13.33 &  9.64 \\
    \bottomrule
	\end{tabular}
	\caption{\textbf{BLEU score on the Multi30k-Task1 and WMT datasets} using greedy decoding.}
	\label{tab:bleu_performance}
	\end{center}
\end{table*}

\subsection{Experimental Results} \label{sec:results}

Table~\ref{tab:bleu_performance} shows the BLEU scores achieved by our model and the baselines we considered. First, we observe that word-by-word translation is surprisingly effective when translating into English, obtaining a BLEU score of 16.77 and 10.09 for \textit{fr-en} on respectively Multi30k-Task1 and WMT datasets. Word-reordering only slightly improves upon word-by-word translation. Our model instead, clearly outperforms these baselines, even on the WMT dataset which has more diversity of topics and sentences with much more complicated structure. After just one iteration, we obtain a BLEU score of 27.48 and 12.10 for the \textit{en-fr} task. Interestingly, we do even better than oracle reordering on some language pairs, suggesting that our model not only reorders but also correctly substitutes some words. After a few iterations, our model obtains BLEU of 32.76 and 15.05 on Multi30k-Task1 and WMT datasets for the English to French task, which is rather remarkable.

\paragraph{Comparison with supervised approaches} Here, we assess how much labeled data are worth our two large monolingual corpora. On WMT, we trained the very same NMT architecture on both language pairs, but with supervision using various amounts of parallel data. 
Figure~\ref{fig:plots}-right shows the resulting performance. Our unsupervised approach obtains the same performance than a supervised NMT model trained on about 100,000 parallel sentences, which is impressive. Of course, adding more parallel examples allows the supervised approach to outperform our method, but the good performance of our unsupervised method suggests that it could be very effective for low-resources languages where no parallel data are available. Moreover, these results open the door to the development of semi-supervised translation models, which will be the focus of future investigation.
With a phrase-based machine translation system, we obtain 21.6 and 22.4 BLEU on WMT \textit{en-fr} and \textit{fr-en}, which is better than the supervised NMT baseline we report for that same amount of parallel sentences, which is 16.8 and 16.4 respectively. However, if we train the same supervised NMT model with BPE~\citep{sennrich2015neural}, we obtain 22.6 BLEU for \textit{en-fr}, suggesting that our results on unsupervised machine translation could also be improved by using BPE, as this removes unknown words (about 9\% of the words in \textit{de-en} are replaced by the unknown token otherwise).

\begin{figure}[tb]
\begin{center}
\includegraphics[width=0.43\linewidth]{images/plot_BLEUvsIters.png}
\includegraphics[width=0.5\linewidth]{images/plot_amount_supervision2.pdf}
\end{center}
\caption{Left: BLEU as a function of the number of iterations of our algorithm on the Multi30k-Task1 datasets. Right: The curves show BLEU as a function of the amount of parallel data on WMT datasets. The unsupervised method which leverages about 15 million monolingual sentences in each language, achieves performance (see horizontal lines) close to what we would obtain by employing 100,000 parallel sentences.}
\label{fig:plots}
\end{figure}

\paragraph{Iterative Learning} Figure~\ref{fig:plots}-left illustrates the quality of the learned model after each iteration of the learning process in the language pairs of Multi30k-Task1 dataset, other results being provided in Table~\ref{tab:bleu_performance}. One can see that the quality of the obtained model is high just after the first iteration of the process. Subsequent iterations yield significant gains although with diminishing returns. At iteration 3, the performance gains are marginal, showing that our approach quickly converges.

Table~\ref{tab:examples_translation} shows examples of translations of three sentences on the Multi30k dataset, as we iterate. Iteration 0 corresponds to the word-by-word translation obtained with our cross-lingual dictionary, which clearly suffers from word order issues. We can observe that the quality of the translations increases at every iteration.


\begin{table}[bt]
\small
\begin{center}
    \small
    \begin{tabular}{ll}
    \toprule
    Source & un homme est debout pr\`es d' une s\'erie de jeux vid\'eo dans un bar .       \\
    Iteration 0 & a man is seated near a series of games video in a bar .                  \\
    Iteration 1 & a man is standing near a closeup of other games in a bar .               \\
    Iteration 2 & a man is standing near a bunch of video video game in a bar .            \\
    Iteration 3 & a man is standing near a bunch of video games in a bar .                 \\
    \textbf{Reference}   & \textbf{a man is standing by a group of video games in a bar .} \\
    \midrule
    Source      & une femme aux cheveux roses habill\'ee en noir parle \`a un homme .        \\
    Iteration 0 & a woman at hair roses dressed in black speaks to a man .                   \\
    Iteration 1 & a woman at glasses dressed in black talking to a man .                     \\
    Iteration 2 & a woman at pink hair dressed in black speaks to a man .                    \\
    Iteration 3 & a woman with pink hair dressed in black is talking to a man .              \\
    \textbf{Reference}   & \textbf{a woman with pink hair dressed in black talks to a man .} \\
    \midrule
    Source             &  une photo d' une rue bond\'ee en ville .   \\
    Iteration 0        &  a photo a street crowded in city .         \\
    Iteration 1        &  a picture of a street crowded in a city .  \\
    Iteration 2        &  a picture of a crowded city street .       \\
    Iteration 3        & a picture of a crowded street in a city .   \\
    \textbf{Reference} & \textbf{a view of a crowded city street .}  \\
    \bottomrule
    \end{tabular}
    \smallskip
    \caption{\textbf{Unsupervised translations.} Examples of translations on the French-English pair of the Multi30k-Task1 dataset. Iteration 0 corresponds to word-by-word translation. After 3 iterations, the model generates very good translations.}
    \label{tab:examples_translation}
    \end{center}
\end{table}


\begin{table*}[t]
	\begin{center}
	\small
	\begin{tabular}[b]{lrrrr}
	\toprule
    & en-fr & fr-en & de-en & en-de \\
    \midrule
    $\lambda_{cd}$ = 0                       & 25.44 & 27.14 & 20.56 & 14.42 \\
    Without pretraining                    & 25.29 & 26.10 & 21.44 & 17.23 \\
	Without pretraining, $\lambda_{cd}$ = 0  &  8.78 &  9.15 &  7.52 &  6.24 \\
    Without noise, $C(x) = x$                          & 16.76 & 16.85 & 16.85 & 14.61 \\
	$\lambda_{auto} = 0$                   & 24.32 & 20.02 & 19.10 & 14.74 \\
    $\lambda_{adv} = 0$                    & 24.12 & 22.74 & 19.87 & 15.13 \\
    Full                                   & \textbf{27.48} & \textbf{28.07} & \textbf{23.69} & \textbf{19.32} \\
    \bottomrule
	\end{tabular}
	\caption{\textbf{Ablation study on the Multi30k-Task1 dataset}.}
	\label{tab:ablation}
	\end{center}
\end{table*}

\paragraph{Ablation Study} We perform an ablation study to understand the importance of the different components of our system. To this end, we have trained multiple versions of our model with some missing components: the discriminator, the cross-domain loss, the auto-encoding loss, etc. Table~\ref{tab:ablation} shows that the best performance is obtained with the simultaneous use of all the described elements. The most critical component is the unsupervised word alignment technique, either in the form of a back-translation dataset generated using word-by-word translation, or in the form of pretrained embeddings which enable to map sentences of different languages in the same latent space.

On the English-French pair of Multi30k-Task1, with a back-translation dataset but without pretrained embeddings, our model obtains a BLEU score of 25.29 and 26.10, which is only a few points below the model using all components. Similarly, when the model uses pretrained embeddings but no back-translation dataset (when $\lambda_{cd}$ = 0), it obtains 25.44 and 27.14. On the other hand, a model that does not use any of these components only reaches 8.78 and 9.15 BLEU. 

The adversarial component also significantly improves the performance of our system, with a difference of up to 5.33 BLEU in the French-English pair of Multi30k-Task1. This confirms our intuition that, to really benefit from the cross-domain loss, one has to ensure that the distribution of latent sentence representations is similar across the two languages. Without the auto-encoding loss (when $\lambda_{auto}=0$), the model only obtains 20.02, which is 8.05 BLEU points below the method using all components. Finally, performance is greatly degraded also when the corruption process of the input sentences is removed, as the model has much harder time learning useful regularities and merely learns to copy input data.
