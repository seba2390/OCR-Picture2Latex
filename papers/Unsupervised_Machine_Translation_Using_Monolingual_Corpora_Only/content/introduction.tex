
\section{Introduction}
\label{sec:introduction}
Thanks to recent advances in deep learning~\citep{sutskever2014sequence, attentionNMT} and the availability
of large-scale parallel corpora, machine translation has now reached impressive performance on several language pairs~\citep{wu2016google}. However, these models work very well only when provided with massive amounts of parallel data, in the order of millions of parallel sentences. Unfortunately, parallel corpora are costly to build as they require specialized expertise, and are often nonexistent for low-resource languages. Conversely, monolingual data is much easier to find, and many languages with limited parallel data still possess significant amounts of monolingual data.

There have been several attempts at leveraging monolingual data to improve the quality of machine translation systems in a semi-supervised setting~\citep{marcu04,irvine13,irvine15,zheng17}. Most notably,  \citet{sennrich2015improving} proposed a very effective data-augmentation scheme, dubbed ``back-translation'',
whereby an auxiliary translation system from the target language to the source language is first trained on the available parallel data, and then used to produce translations from a large monolingual corpus on the target side. The pairs composed of these translations with their corresponding ground truth targets are then used as additional training data for the original translation system. 

Another way to leverage monolingual data on the target side is to augment the decoder with a language model~\citep{gulcehre2015using}. And finally, \citet{cheng16, he2016dual} have proposed to add an auxiliary auto-encoding task on monolingual data, which ensures that a translated sentence can be translated back to the original one. All these works still rely on several tens of thousands parallel sentences, however.

Previous work on zero-resource machine translation has also relied on labeled information, not from the language pair of interest but from other related language pairs~\citep{firat16, gmt17, chen17acl} or from other modalities~\citep{nakayama17, lee17}. The only exception is the work by~\citet{knight_acl11, knight17}, where the machine translation problem is reduced to a deciphering problem. Unfortunately, their method is limited to rather short sentences and it has only been demonstrated on a very simplistic setting comprising of the most frequent short sentences, or very closely related languages.

In this paper, we investigate whether it is possible to train a general machine translation system without any form of supervision whatsoever. The only assumption we make is that there exists a monolingual corpus on each language. This set up is interesting for a twofold reason. First, this is applicable whenever we encounter a new language pair for which we have no annotation. Second, it provides a strong lower bound performance on what any good semi-supervised approach is expected to yield.


The key idea is to build a common latent space between the two languages (or domains) and to learn to translate by reconstructing in both domains according to two principles: (i) the model has to be able to reconstruct a sentence in a given language from a noisy version of it, as in standard denoising auto-encoders \citep{vincent2008extracting}. (ii) The model also learns to reconstruct any source sentence given a noisy translation of the same sentence in the target domain, and vice versa. For (ii), the translated sentence is obtained by using a back-translation procedure \citep{sennrich2015improving}, i.e. by using the learned model to translate the source sentence to the target domain. In addition to these reconstruction objectives, we constrain the source and target sentence latent representations to have the same distribution using an adversarial regularization term, whereby the model tries to fool a discriminator which is simultaneously trained to identify the language of a given latent sentence representation~\citep{ganin}. This procedure is then iteratively repeated, giving rise to translation models of increasing quality. To keep our approach fully unsupervised, we initialize our algorithm by using a na\"ive unsupervised translation model based on a word by word translation of sentences with a bilingual lexicon derived from the same monolingual data~\citep{wordalign17}. As a result, and by only using monolingual data, we can encode sentences of both languages into the same feature space, and from there, we can also decode/translate in any of these languages; see Figure~\ref{fig:model_outline2} for an illustration.

While not being able to compete with supervised approaches using lots of parallel resources, we show in section~\ref{sec:experiments} that our model is able to achieve remarkable performance. 
For instance, on the WMT dataset we can achieve the same translation quality of a similar machine translation system trained with full supervision on 100,000 sentence pairs. On the Multi30K-Task1 dataset we achieve a BLEU above 22 on all the language pairs, with up to 32.76 on English-French.

Next, in section~\ref{sec:model}, we describe the model and the training algorithm. We then present experimental results in section~\ref{sec:experiments}. Finally, we further discuss related work in section~\ref{sec:related_work} and summarize our findings in section~\ref{sec:conclusion}.

\begin{figure}[!t]
\begin{center}
\includegraphics[width=1\linewidth]{images/autocrossencoder_mapping.png}
\end{center}
\caption{Toy illustration of the principles guiding the design of our objective function. Left (auto-encoding): the model is trained to reconstruct a sentence from a noisy version of it. $x$ is the target, $C(x)$ is the noisy input, $\hat{x}$ is the reconstruction.
 Right (translation): the model is trained to translate a sentence in the other domain. The input is a noisy translation (in this case, from  source-to-target) produced by the model itself, $M$, at the previous iteration $(t)$, $y=M^{(t)}(x)$. The model is symmetric, and we repeat the same process in the other language.  See text for more details.}
\label{fig:model_outline2}
\end{figure}

