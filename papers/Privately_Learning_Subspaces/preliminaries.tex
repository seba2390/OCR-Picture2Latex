\section{Notations, Definitions, and Background Results}\label{sec:preliminaries}

\subsection{Linear Algebra and Probability Preliminaries}

Here, we mention a few key technical results that we will
be using to prove the main theorem for the approximate
case. Throughout this document, we assume that the dimension
$d$ is larger than some absolute constant, and adopt the following
notation: for a matrix $A$ of rank $r$, we use $s_1(A)\geq\dots\geq s_r(A)$
to denote the singular values of $A$ in decreasing order, and
use $\lambda_1(A)\geq\dots\geq\lambda_r(A)$ to denote the
eigenvalues of $A$ in decreasing order; let $s_{\min}(A)$ denote
the least, non-zero singular value of $A$. We omit the parentheses
when the context is clear. We begin by stating
two results about matrix perturbation
theory. The first result says that if two matrices are close
to one another in operator norm, then their corresponding
singular values are also close to one another.

Define \[\|M\| := \sup \{\|Mx\|_2 : x \in \mathbb{R}^d, ~ \|x\|_2 \le 1\}\]
to be the operator norm with respect to the Euclidean vector
norm.

\begin{lemma}[Singular Value Inequality]\label{lem:weyl-singular}
    Let $A,B \in \RR^{d \times n}$ and let $r = \min\{d,n\}$.
    Then for $1 \leq i,j \leq r$,
    $$s_{i+j-1}(A+B) \leq s_i(A) + s_j(B).$$
\end{lemma}

The following result gives a lower bound on the least
singular value of sum of two matrices.

\begin{lemma}[Least Singular Value of Matrix Sum]\label{lem:least-singular}
    Let $A,B \in \RR^{d\times n}$. Then
    $$s_{\min}(A+B) \geq s_{\min}(A) - \|B\|.$$
\end{lemma}

%\begin{lemma}[Weyl's Inequality]\label{lem:weyl}
%    Let $A$ and $B$ be $n \times n$ hermitian matrices.
%    Suppose $\lambda_1\geq\dots\geq\lambda_n$ are eigenvalues
%    of $A$, and $\nu_1\geq\dots\geq\nu_n$ are eigenvalues
%    of $B$. Then for each $1 \leq i \leq n$,
%    $$\abs{\lambda_i - \nu_i} \leq \|A-B\|.$$
%\end{lemma}

The next result bounds the angle between the subspaces
spanned by two matrices that are close to one another.
Let $X \in \RR^{d\times n}$ have the following SVD.
$$X=
    \begin{bmatrix}
        U & U_{\bot}
    \end{bmatrix} \cdot
    \begin{bmatrix}
        \Sigma_1 & 0\\
        0 & \Sigma_2
    \end{bmatrix} \cdot
    \begin{bmatrix}
        V^T\\
        V_{\bot}^T
    \end{bmatrix}$$
In the above, $U,U_{\bot}$ are orthonormal matrices
such that $U\in \RR^{d\times r}$ and $U_{\bot}\in \RR^{d \times (d-r)}$,
$\Sigma_1,\Sigma_2$ are diagonal matrices, such that
$\Sigma_1 \in \RR^{r \times r}$ and $\Sigma_2 \in \RR^{(d-r)\times(n-r)}$,
and $V,V_{\bot}$ are orthonormal matrices, such that
$V \in \RR^{n \times r}$ and $V_{\bot} \in \RR^{n \times (n-r)}$.
Let $Z \in \RR^{d\times n}$ be a perturbation matrix,
and $\hat{X} =  X + Z$, such that $\hat{X}$ has the
following SVD.
$$\hat{X}=
    \begin{bmatrix}
        \hat{U} & \hat{U}_{\bot}
    \end{bmatrix} \cdot
    \begin{bmatrix}
        \hat{\Sigma_1} & 0\\
        0 & \hat{\Sigma}_2
    \end{bmatrix} \cdot
    \begin{bmatrix}
        \hat{V}^T\\
        \hat{V}_{\bot}^T
    \end{bmatrix}$$
In the above, $\hat{U},\hat{U}_{\bot},\hat{\Sigma}_1,\hat{\Sigma}_2,\hat{V},\hat{V}_{\bot}$
have the same structures as $U,U_{\bot},\Sigma_1,\Sigma_2,V,V_{\bot}$
respectively. Let $Z_{21} = U_{\bot}U_{\bot}^T Z VV^T$ and
$Z_{12} = UU^T Z V_{\bot}V_{\bot}^T$. Suppose $\sigma_1\geq\dots\geq\sigma_r\geq0$
are the singular values of $U^T\hat{U}$. Let
$\Theta(U,\hat{U}) \in \RR^{r\times r}$ be a diagonal matrix,
such that $\Theta_{ii}(U,\hat{U}) = \cos^{-1}(\sigma_i)$.

\begin{lemma}[$\text{Sin}(\Theta)$ Theorem \cite{CaiZ16}]\label{lem:sin-theta}
    Let $X,\hat{X},Z,Z_{12},Z_{21}$ be defined as above.
    Denote $\alpha = s_{\min}(U^T \hat{X} V)$ and
    $\beta = \|U_{\bot}^T \hat{X} V_{\bot}\|$.
    If $\alpha^2 > \beta^2 + \min\{\|Z_{12}\|^2,\|Z_{21}\|^2\}$,
    then we have the following.
    $$\|\text{Sin}(\Theta)(U,\hat{U})\| \leq
        \frac{\alpha\|Z_{21}\|+\beta\|Z_{12}\|}
        {\alpha^2-\beta^2-\min\{\|Z_{12}\|^2,\|Z_{21}\|^2\}}$$
\end{lemma}

%\begin{lemma}[Davis-Kahan
%    \footnote{\href{https://www.cs.columbia.edu/~djhsu/coms4772-f16/lectures/davis-kahan.pdf}
%    {\texttt{https://www.cs.columbia.edu/\textasciitilde{}djhsu/coms4772-f16/lectures/davis-kahan.pdf}}}]\label{lem:davis-kahan}
%    Let $A,\tilde{A},U,\tilde{U},\Lambda,\tilde\Lambda
%    \in \mathbb{R}^{d \times d}$ satisfy the following.
%    (i) $A^T=A$, $\tilde{A}^T=A$, (ii) $A=U\Lambda U^T$,
%    $\tilde{A} = \tilde{U} \tilde{\Lambda} \tilde{U}^T$,
%    (iii) $U^TU=I_d=\tilde{U}^T\tilde{U}$, (iv) $\Lambda$
%    and $\tilde\Lambda$ are diagonal matrices with entries
%    in descending order. Let $\lambda_i, \tilde{\lambda}_i$
%    denote the $i^\text{th}$ diagonal entry of $\Lambda, \tilde\Lambda$.
%    Denote $U_{a:b}, \tilde{U}_{a:b} \in \mathbb{R}^{d \times (b-a+1)}$
%    to be the matrix formed by columns $a, a+1, \cdots, b$ of $U$
%    and $\tilde{U}$ respectively (i.e., corresponding to
%    $\lambda_a \ge \lambda_{a+1} \ge \cdots \ge \lambda_b$
%    and $\tilde\lambda_a \ge \tilde\lambda_{a+1} \ge \cdots \ge \tilde\lambda_b$).
%     Then, for all $k \in [d-1]$, if $\lambda_k > \tilde\lambda_{k+1}$,
%    we have
%    \[\|\tilde{U}_{1:k}^T {U}_{k+1:d}\| \le \frac{\|\tilde{U}_{1:k}^T
%        (\tilde{A}-A) {U}_{k+1:d}\|}{\lambda_k - \tilde\lambda_{k+1}}.\]
%\end{lemma}

The next result bounds $\|\text{Sin}(\Theta)(U,\hat{U})\|$ in terms
of the distance between $UU^T$ and $\hat{U}\hat{U}^T$.

\begin{lemma}[Property of $\|\text{Sin}(\Theta)\|$ \cite{CaiZ16}]\label{lem:sin-theta-property}
    Let $U,\hat{U} \in \RR^{d \times r}$ be orthonormal
    matrices, and let $\Theta(U,\hat{U})$ be defined as above in terms
    of $\hat{U},U$. Then we have the following.
    $$\|\text{Sin}(\Theta)(U,\hat{U})\| \leq \|\hat{U}\hat{U}^T-UU^T\|
        \leq 2\|\text{Sin}(\Theta)(U,\hat{U})\|$$
\end{lemma}

The next result bounds the singular values of a matrix,
whose columns are independent vectors from a mean zero,
isotropic distribution in $\R^d$. We first define the
sub-Gaussian norm of a random variable.

\begin{definition}
    Let $X$ be a sub-Gaussian random variable. The sub-Gaussian
    norm of $X$, denoted by $\|X\|_{\psi^2}$, is defined as,
    $$\|X\|_{\psi^2} = \inf\{t>0: \ex{}{\exp(X^2/t^2)} \leq 2\}.$$
\end{definition}

\begin{lemma}[Theorem 4.6.1 \cite{Vershynin18}]\label{lem:sub-gaussian-spectrum}
    Let $A$ be an $n \times m$ matrix, whose columns $A_i$
    are independent, mean zero, sub-Gaussian isotropic
    random vectors in $\R^n$. Then for any $t \geq 0$,
    we have
    $$\sqrt{m} - CK^2(\sqrt{n}+t) \leq s_n(A) \leq
        s_1(A) \leq \sqrt{m} + CK^2(\sqrt{n}+t)$$
    with probability at least $1-2\mathrm{exp}(-t^2)$.
    Here, $K = \max_i\|A\|_{\psi^2}$ (sub-Gaussian norm
    of $A$).
\end{lemma}

In the above, $\|A\|_{\psi^2} \in O(1)$ if the distribution
in question is $\cN(\vec{0},\id)$. The following corollary
generalises the above result for arbitrary Gaussians.

\begin{corollary}\label{coro:normal-spectrum}
    Let $A$ be an $n \times m$ matrix, whose columns $A_i$
    are independent, random vectors in $\R^n$ from $\cN(\vec{0},\Sigma)$.
    Then for any $t \geq 0$, we have
    $$(\sqrt{m} - CK^2(\sqrt{n} + t))\sqrt{s_n(\Sigma)}
        \leq s_n(A) \leq (\sqrt{m} + CK^2(\sqrt{n} + t))\sqrt{s_n(\Sigma)}$$
    and
    $$s_1(A) \leq (\sqrt{m} + CK^2(\sqrt{n} + t))\sqrt{s_1(\Sigma)}$$
    with probability at least $1-2\mathrm{exp}(-t^2)$.
    Here, $K = \max_i\|A\|_{\psi^2}$ (sub-Gaussian norm
    of $A$).
\end{corollary}
\begin{proof}
    First, we prove the lower bound on $s_n(A)$. Note
    that $s_n(A) = \min\limits_{\|x\|>0}{\frac{\|Ax\|}{\|x\|}}$,
    and that the columns of $\Sigma^{-\frac{1}{2}}A$ are distributed
    as $\cN(\vec{0},\id)$. Therefore, we have the following.
    \begin{align*}
        \min\limits_{\|x\|>0}\frac{\|Ax\|}{\|x\|} &=
                \min\limits_{\|x\|>0}\frac{\|\Sigma^{\frac{1}{2}}
                \Sigma^{-\frac{1}{2}} Ax\|}{\|x\|}\\
            &= \min\limits_{\|x\|>0}\frac{\|\Sigma^{\frac{1}{2}}
                \Sigma^{-\frac{1}{2}} Ax\|}{\|\Sigma^{-\frac{1}{2}}Ax\|}
                \frac{\|\Sigma^{-\frac{1}{2}}Ax\|}{\|x\|}\\
            &\geq \min\limits_{\|x\|>0}\frac{\|\Sigma^{\frac{1}{2}}
                \Sigma^{-\frac{1}{2}} Ax\|}{\|\Sigma^{-\frac{1}{2}}Ax\|}
                \min\limits_{\|x\|>0}\frac{\|\Sigma^{-\frac{1}{2}}Ax\|}{\|x\|}\\
            &\geq \min\limits_{\|y\|>0}\frac{\|\Sigma^{\frac{1}{2}}y\|}{\|y\|}
                \min\limits_{\|x\|>0}\frac{\|\Sigma^{-\frac{1}{2}}Ax\|}{\|x\|}\\
            &\geq (\sqrt{m}-CK^2(\sqrt{n}+t))\sqrt{s_n(\Sigma)}
                \tag{Lemma~\ref{lem:sub-gaussian-spectrum}}
    \end{align*}
    Next, we prove the upper bound on $s_n(A)$. For this,
    we first show that for $X\in \RR^{m\times d}$ and $Y \in \RR^{d \times n}$,
    $s_{\min}(XY) \leq s_{\min}(X)\cdot\|Y\|$.
    \begin{align*}
        s_{\min}(XY) &= \min\limits_{\|z\|=1}\|XYz\|\\
            &\leq \min\limits_{\|z\|=1}\|X\|\|Yz\|\\
            &= \|X\|\cdot \min\limits_{\|z\|=1}\|Yz\|\\
            &= \|X\|\cdot s_{\min}(Y)
    \end{align*}
    Now, $s_{\min}(XY) = s_{\min}(Y^T X^T) \leq \|Y\|\cdot s_{\min}(X)$
    by the above reasoning. Using this results, we have the following.
    \begin{align*}
        s_n(A) &= s_n(\Sigma^{1/2}\cdot\Sigma^{-1/2}A)\\
            &\leq s_n(\Sigma^{1/2})\|\Sigma^{-1/2}A\|\\
            &\leq (\sqrt{m} + CK^2(\sqrt{n}+t))\sqrt{s_n(\Sigma)}
                \tag{Lemma~\ref{lem:sub-gaussian-spectrum}}
    \end{align*}
    Now, we show the upper bound on $s_1(A)$. Note that
    $s_1(A) = \|A\|$.
    \begin{align*}
        \|A\| &= \|\Sigma^{\frac{1}{2}}\Sigma^{-\frac{1}{2}}A\|\\
            &\leq \|\Sigma^{\frac{1}{2}}\|\cdot\|\Sigma^{-\frac{1}{2}}A\|\\
            &\leq (\sqrt{m}+CK^2(\sqrt{n}+t))\sqrt{s_1(\Sigma)}
                \tag{Lemma~\ref{lem:sub-gaussian-spectrum}}
    \end{align*}
    This completes the proof.
\end{proof}

Now, we state a concentration inequality for $\chi^2$ random variables.

\begin{lemma}\label{lem:chi-squared}
    Let $X$ be a $\chi^2$ random variable with $k$
    degrees of freedom. Then,
    $$\pr{}{X>k+2\sqrt{kt}+2t} \leq e^{-t}.$$
\end{lemma}

Next, we state the well-known Bernstein's inequality
for sums of independent Bernoulli random variables.

\begin{lemma}[Bernstein's Inequality]\label{lem:chernoff-add}
    Let $X_1,\dots,X_m$ be independent Bernoulli random variables
    taking values in $\zo$. Let $p = \ex{}{X_i}$.
    Then for $m \geq \frac{5p}{2\epsilon^2}\ln(2/\beta)$ and
    $\eps \leq p/4$,
    $$\pr{}{\abs{\frac{1}{m}\sum{X_i}-p} \geq \epsilon}
        \leq 2e^{-\epsilon^2m/2(p+\epsilon)}
        \leq \beta.$$
\end{lemma}

We finally state a result about the norm of a
vector sampled from $\cN(\vec{0},\id)$.

\begin{lemma}\label{lem:gauss-vector-norm}
    Let $X_1,\dots,X_q \sim \cN(\vec{0},\Sigma)$ be vectors in
    $\RR^d$, where $\Sigma$ is the projection of $\id_{d \times d}$
    on to a subspace of $\RR^d$ of rank $k$. Then
    $$\pr{}{\forall i, \|X_i\|^2 \leq k + 2\sqrt{kt} + 2t} \geq 1-qe^{-t}.$$
\end{lemma}
\begin{proof}
    Since $\Sigma$ is of rank $k$, we can directly
    use Lemma~\ref{lem:chi-squared} for a fixed $i \in [q]$,
    and the union bound over all $i \in [q]$ to get
    the required result. This is because for any $i$,
    $\|X_i\|^2$ is a $\chi^2$ random variable with $k$
    degrees of freedom.
\end{proof}

%\begin{lemma}[Lemma A.3 \cite{KamathSSU19}]\label{lem:gauss-vector-norm}
%    Let $X_1,\dots,X_q \sim \cN(\vec{0},\id)$ be vectors in $\R^d$,
%    where $d \geq O(\log(q))$.
%    Then with probability at least $0.95$, $\|X\| \leq \sqrt{3d}$.
%\end{lemma}

\subsection{Privacy Preliminaries}

\begin{definition}[Differential Privacy (DP) \cite{DworkMNS06}]
    \label{def:dp}
    A randomized algorithm $M:\cX^n \rightarrow \cY$
    satisfies $(\eps,\delta)$-differential privacy
    ($(\eps,\delta)$-DP) if for every pair of
    neighboring datasets $X,X' \in \cX^n$
    (i.e., datasets that differ in exactly one entry),
    $$\forall Y \subseteq \cY~~~
        \pr{}{M(X) \in Y} \leq e^{\eps}\cdot
        \pr{}{M(X') \in Y} + \delta.$$
    When $\delta = 0$, we say that $M$ satisfies
    $\eps$-differential privacy or pure differential
    privacy.
\end{definition}
Neighbouring datasets are those that differ by the replacement of one individual's data. In our setting, each individual's data is assumed to correspond to one point in $\cX = \mathbb{R}^d$, so neighbouring means one point is changed arbitrarily.

Throughout the document, we will assume that $\eps$ is
smaller than some absolute constant less than $1$ for
notational convenience, but note that our results still
hold for general $\eps$. Now, this privacy definition is
closed under post-processing.
%and can be composed with graceful degradation of the privacy
%parameters.
\begin{lemma}[Post Processing \cite{DworkMNS06}]\label{lem:post-processing}
    If $M:\cX^n \rightarrow \cY$ is
    $(\eps,\delta)$-DP, and $P:\cY \rightarrow \cZ$
    is any randomized function, then the algorithm
    $P \circ M$ is $(\eps,\delta)$-DP.
\end{lemma}

%\begin{lemma}[Composition of DP~\cite{DworkMNS06, DworkRV10}]\label{lem:composition}
%    If $M$ is an adaptive composition of differentially
%    private algorithms $M_1,\dots,M_T$, then the following
%    all hold:
%    \begin{enumerate}
%        \item If $M_1,\dots,M_T$ are
%            $(\eps_1,\delta_1),\dots,(\eps_T,\delta_T)$-DP
%            then $M$ is $(\eps,\delta)$-DP for
%            $$\eps = \sum_t \eps_t~~~~\textrm{and}~~~~\delta = \sum_t \delta_t.$$
%        \item If $M_1,\dots,M_T$ are
%            $(\eps_0,\delta_1),\dots,(\eps_0,\delta_T)$-DP
%            for some $\eps_0 \leq 1$, then for every $\delta_0 > 0$, $M$
%            is $(\eps, \delta)$-DP for
%            $$\eps = \eps_0 \sqrt{6 T \log(1/\delta_0)}~~~~
%                \textrm{and}~~~~\delta = \delta_0 + \sum_t \delta_t$$
%    \end{enumerate}
%\end{lemma}
%
%Quantitatively tighter composition results are known \cite[etc.]{KairouzOV15,BunS16}. However, these results suffice for our purposes, as we do not aim to optimize constants.

\subsection{Basic Differentially Private Mechanisms.}
We first state standard results on achieving
privacy via noise addition proportional to
sensitivity~\cite{DworkMNS06}.

\begin{definition}[Sensitivity]
    Let $f : \cX^n \to \R^d$ be a function,
    its \emph{$\ell_1$-sensitivity} and
    \emph{$\ell_2$-sensitivity} are
    $$\Delta_{f,1} = \max_{X \sim X' \in \cX^n} \| f(X) - f(X') \|_1
    ~~~~\textrm{and}~~~~\Delta_{f,2} = \max_{X \sim X' \in \cX^n} \| f(X) - f(X') \|_2,$$
    respectively.
    Here, $X \sim X'$ denotes that $X$ and $X'$
    are neighboring datasets (i.e., those that
    differ in exactly one entry).
\end{definition}

%For functions with bounded $\ell_1$-sensitivity,
%we can achieve $\eps$-DP by adding noise from
%a Laplace distribution proportional to
%$\ell_1$-sensitivity.
%For functions taking values
%in $\R^d$ for large $d$ it is useful to add
%noise from a Gaussian distribution proportional
%to the $\ell_2$-sensitivity, to get $(\eps,\delta)$-DP
%and $\rho$-zCDP.

%\begin{lemma}[Laplace Mechanism] \label{lem:laplacedp}
%    Let $f : \cX^n \to \R^d$ be a function
%    with $\ell_1$-sensitivity $\Delta_{f,1}$.
%    Then the Laplace mechanism
%    $$M(X) = f(X) + \Lap\left(\frac{\Delta_{f,1}}
%        {\eps}\right)^{\otimes d}$$
%    satisfies $\eps$-DP.
%\end{lemma}

%\begin{lemma}[Gaussian Mechanism] \label{lem:gaussiandp}
%    Let $f : \cX^n \to \R^d$ be a function
%    with $\ell_2$-sensitivity $\Delta_{f,2}$.
%    Then the Gaussian mechanism
%    $$M(X) = f(X) + \cN\left(0,\left(\frac{\Delta_{f,2}
%        \sqrt{2\ln(2/\delta)}}{\eps}\right)^2 \cdot \id_{d \times d}\right)$$
%    satisfies $(\eps,\delta)$-DP.
%\end{lemma}

One way of introducing $(\eps,\delta)$-differential
privacy is via adding noise sampled from the truncated
Laplace distribution, proportional to the $\ell_1$
sensitivity.

\begin{lemma}[Truncated Laplace Mechanism \cite{GengDGK20}]\label{lem:truncated-laplace}
    Define the probability density function ($p$) of
    the truncated Laplace distribution as follows.
    \[
    p(x) =
    \begin{cases}
        Be^{-\frac{\abs{x}}{\lambda}} & \text{if $x \in [-A,A]$}\\
        0 & \text{otherwise}
    \end{cases}
    \]
    In the above,
    $$
        \lambda = \frac{\Delta}{\eps},~~~
        A = \frac{\Delta}{\eps}\log\left(1+\frac{e^\eps - 1}{2\delta}\right),~~~
        B = \frac{1}{2\lambda(1-e^{-\frac{A}{\lambda}})}.
    $$
    Let $\TLap(\Delta,\eps,\delta)$ denote a draw
    from the above distribution. 
    
    Let $f : \cX^n \to \R^d$ be a function
    with sensitivity $\Delta$. Then the truncated
    Laplace mechanism
    $$M(X) = f(X) + \TLap(\Delta,\eps,\delta)$$
    satisfies $(\eps,\delta)$-DP.
\end{lemma}

In the above $A \leq \tfrac{\Delta_{f,1}}{\eps}\log(1/\delta)$
since $\eps$ is smaller than some absolute constant less than
$1$. Now, we introduce differentially private histograms.

\begin{lemma}[Private Histograms]\label{lem:priv-hist}
    Let $n \in \mathbb{N}$, $\varepsilon,\delta,\beta>0$, and $\mathcal{X}$ a set.
    There exists $M : \mathcal{X}^n \to \mathbb{R}^{\mathcal{X}}$ which is $(\varepsilon,\delta)$-differentially private and, for all $x \in \mathcal{X}^n$, we have \[\pr{M}{\sup_{y \in \mathcal{X}} \left|M(x)_y - \frac1n |\{ i \in [n] : x_i = y\}| \right| \le O\left(\frac{\log(1/\delta\beta)}{\varepsilon n}\right) } \ge 1-\beta.\]
%    Furthermore, the runtime of $M$ is $\poly(n,\log(\abs{\mathcal{X}}/\eps\beta))$
%    Let $(X_1,\dots,X_n)$ be samples in some data universe
%    $U$, and let $\Omega = \{h_u\}_{u \subset U}$
%    be a collection of disjoint histogram buckets over $U$.
%    Then we have $(\eps,\delta)$-DP
%    histogram algorithms with the following guarantees.
%    \begin{itemize}
%        \item $(\eps,\delta)$-DP:
%            $\ell_\infty$ error - $O\left(\tfrac{\log(1/\delta\beta)}{\eps}\right)$
%            with probability at least $1-\beta$;
%            run time - $\poly(n,\log(\abs{U}/\eps\beta))$
%    \end{itemize}
\end{lemma}

The above holds due
to \cite{BunNS16,Vadhan17}. Finally, we introduce the
$\GAPMAX$ algorithm from \cite{BunDRS18} that outputs
the element from the output space that has the highest
score function, given that there is a significant gap
between the scores of the highest and the second to the
highest elements.

\begin{lemma}[$\GAPMAX$ Algorithm \cite{BunDRS18}]\label{lem:gap-max}
    Let $\Score:\cX^n \times \cY \rightarrow \RR$ be a
    score function with sensitivity $1$ in its
    first argument, and let $\varepsilon,\delta>0$. Then there exists a $(\varepsilon,\delta)$-differentially private algorithm $M : \mathcal{X}^n \to \mathcal{Y}$ and $\alpha=\Theta(\log(1/\delta)/\varepsilon n)$ with the following property. Fix an input
    $X \in \cX^n$. Let
    $$y^* = \argmax_{y \in \cY}\{\Score(X,y)\}.$$
    Suppose
    $$\forall y \in \cY, y \neq y^* \implies \Score(X,y) < \Score(X,y^*)-\alpha n.$$
    Then $M$ outputs $y^*$ with probability 1.
\end{lemma}
