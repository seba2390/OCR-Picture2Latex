\section{Approximate Case}

In this section, we discuss the case, where the data
``approximately'' lies in a $k$-dimensional subspace of
$\R^d$. %An alternate perspective of this problem is that
%the data lies in a $k$-dimensional subspace of $\R^d$,
%but has very small noise in the orthogonal directions.
We make a Gaussian distributional assumption, where the
covariance is approximately $k$-dimensional, though the
results could be extended to distributions with heavier
tails using the right inequalities. We formally define
the problem:

\begin{problem}\label{prob:gaussians}
    Let $\Sigma \in \R^{d \times d}$ be a symmetric, PSD
    matrix of rank $\geq k \in \{1,\dots,d\}$, and let $0 < \gamma \ll 1$,
    such that $\tfrac{\lambda_{k+1}}{\lambda_k} \leq \gamma^2$.
    Suppose $\Pi$ is the projection matrix corresponding
    to the subspace spanned by the eigenvectors of $\Sigma$
    corresponding to the eigenvalues $\lambda_1,\dots,\lambda_k$.
    Given sample access to $\cN(\vec{0},\Sigma)$,
    and $0 < \alpha < 1$, output a projection matrix $\wh{\Pi}$,
    such that $\|\Pi-\wh{\Pi}\| \leq \alpha$.
\end{problem}

We solve Problem~\ref{prob:gaussians} under the constraint
of $(\eps,\delta)$-differential privacy. Throughout this section,
we would refer to the subspace spanned by the top $k$ eigenvectors
of $\Sigma$ as the ``true'' or ``actual'' subspace.

Algorithm \ref{alg:approximate} solves Problem~\ref{prob:gaussians} and proves Theorem \ref{thm:intro-main-approx}.
Here $\|\cdot\|$ is the operator norm.

\begin{remark}\label{rem:gamma}
    We scale the eigenvalues of $\Sigma$
    so that $\lambda_k=1$ and $\lambda_{k+1} \leq \gamma^2$.
    %We will be adopting this notation throughout this text.
    Also, for the purpose of the analysis, we will be splitting
    $\Sigma = \Sigma_k + \Sigma_{d-k}$, where $\Sigma_k$ is the
    covariance matrix formed by the top $k$ eigenvalues and
    the corresponding eigenvectors of $\Sigma$ and $\Sigma_{d-k}$
    is remainder.
\end{remark}

%\begin{comment}

Also, we assume the
knowledge of $\gamma$ (or an upper bound on $\gamma$). Our solution
is presented in Algorithm~\ref{alg:approximate}. The following
theorem is the main result of the section.

\begin{theorem}\label{thm:approximate}
    Let $\Sigma \in \R^{d \times d}$ be an arbitrary, symmetric, PSD
    matrix of rank $\geq k \in \{1,\dots,d\}$, and let $0 < \gamma < 1$.
    Suppose $\Pi$ is the projection matrix corresponding
    to the subspace spanned by the vectors of $\Sigma_k$.
    Then given
    $$\gamma^2 \in
        O\left(\frac{\eps\alpha^2n}{d^{2}k\ln(1/\delta)}\cdot
        \min\left\{\frac{1}{k},
        \frac{1}{\ln(k\ln(1/\delta)/\eps)}
        \right\}\right),$$
    such that $\lambda_{k+1}(\Sigma) \leq \gamma^2\lambda_k(\Sigma)$,
    for every $\eps,\delta>0$, and $0 < \alpha < 1$,
    there exists and $(\eps,\delta)$-DP algorithm that takes
    $$n \geq O\left(\frac{k\log(1/\delta)}{\eps} +
        \frac{\log(1/\delta)\log(\log(1/\delta)/\eps)}{\eps}\right)$$
    samples from $\cN(\vec{0},\Sigma)$, and outputs a projection matrix $\wh{\Pi}$,
    such that $\|\Pi-\wh{\Pi}\| \leq \alpha$ with probability
    at least $0.7$.
    \tnote{Eigenvalue gap assumption missing. Also order of quantifiers is ambiguous -- ``for every $\Sigma$ there exists an algorith.''}
\end{theorem}
%\end{comment}

Algorithm~\ref{alg:approximate} is a type of
``Subsample-and-Aggregate'' algorithm \cite{NissimRS07}.
Here, we consider multiple subspaces formed by the points
from the same Gaussian, and privately find a subspace that
is close to all those subspaces. Since the subspaces formed
by the points would be close to the true subspace, the privately
found subspace would be close to the true subspace.

A little more formally, we first sample $q$ public data points
(called ``reference points'') from $\cN(\vec{0},\id)$. Next,
we divide the original dataset $X$ into disjoint datasets of $m$ samples
each, and project all reference points on the subspaces spanned
by every subset. Now, for every reference point, we do the
following. We have $t=\tfrac{n}{m}$ projections of the reference
point. Using DP histogram over $\R^d$, we aggregate those
projections in the histogram cells; with high probability
all those projections will be close to one another, so they
would lie within one histogram cell. We output a random point
from the histogram cell corresponding to the reference point.
With a total of $q$ points output in this way, we finally
output the projection matrix spanned by these points. In
the algorithm $C_0$, $C_1$, and $C_2$ are universal constants.

We divide the proof of Theorem~\ref{thm:approximate}
into two parts: privacy (Lemma \ref{coro:privacy}) and
accuracy (Lemma~\ref{lem:final-projection}).

\begin{algorithm}[h!] 
\caption{\label{alg:approximate}DP Approximate Subspace Estimator
    $\DPASE_{\eps, \delta, \alpha, \gamma, k}(X)$}
\KwIn{Samples $X_1,\dots,X_{n} \in \R^d$.
    Parameters $\eps, \delta, \alpha, \gamma, k > 0$.}
\KwOut{Projection matrix $\wh{\Pi} \in \R^{d \times d}$ of rank $k$.}
\vspace{5pt}

Set parameters:
    $t \gets \tfrac{C_0\ln(1/\delta)}{\eps}$ \qquad
    $m \gets \lfloor n/t \rfloor$ \qquad $q \gets C_1 k$
    \qquad $\ell \gets \tfrac{C_2\gamma\sqrt{dk}(\sqrt{k}+\sqrt{\ln(kt)})}{\sqrt{m}}$
\vspace{5pt}

Sample reference points $p_1,\dots,p_q$ from $\cN(\vec{0},\id)$ independently.
\vspace{5pt}

\tcp{Subsample from $X$, and form projection matrices.}
\For{$j \in 1,\dots,t$}{
    Let $X^j = (X_{(j-1)m+1},\dots,X_{jm}) \in \mathbb{R}^{d \times m}$.\\
    Let $\Pi_j \in \mathbb{R}^{d \times d}$ be the projection matrix onto the subspace spanned by the eigenvectors of $X^j (X^j)^T \in \mathbb{R}^{d \times d}$ corresponding to the largest $k$ eigenvalues.\\
    \For{$i \in 1,\dots,q$}{
        $p_{i}^j \gets \Pi_j p_i$
    }
}
\vspace{5pt}

\tcp{Create histogram cells with random offset.}
Let $\lambda$ be a random number in $[0,1)$.\\
Divide $\R^{qd}$ into $\Omega =
    \{\dots,[\lambda\ell+i\ell,\lambda\ell+(i+1)\ell),\dots\}^{qd}$,
    for all $i \in \Z$.\\
Let each disjoint cell of length $\ell$ be a histogram bucket.
\vspace{5pt}

\tcp{Perform private aggregation of subspaces.}
For each $i \in [q]$, let $Q_i \in \RR^{d \times t}$ be the
    dataset, where column $j$ is $p_i^j$.\\
Let $Q \in \RR^{qd \times t}$ be the vertical concatenation
    of all $Q_i$'s in order.\\
Run $(\eps,\delta)$-DP histogram over $\Omega$ using $Q$
    to get $\omega \in \Omega$ that contains at least $\tfrac{t}{2}$ points.\\
\If{no such $\omega$ exists}{
    \Return $\bot$
}
\vspace{5pt}

\tcp{Return the subspace.}
Let $\wh{p}=(\wh{p}_1,\dots,\wh{p}_d,\dots,\wh{p}_{(q-1)d+1},\dots,\wh{p}_{qd})$
    be a random point in $\omega$.\\
\For{each $i \in [q]$}{
    Let $\wh{p}_i = (\wh{p}_{(i-1)d+1},\dots,\wh{p}_{id})$.
}
    Let $\wh{\Pi}$ be the projection matrix of the top-$k$ subspace of $(\wh{p}_1,\dots,\wh{p}_q)$.\\
\Return $\wh{\Pi}.$
\vspace{5pt}
\end{algorithm}

\subsection{Privacy}

We analyse the privacy by understanding the sensitivities
at the only sequence of steps invoking a differentially
private mechanism, that is, the sequence of steps involving
DP-histograms.

\begin{lemma}\label{lem:histogram-sensitivity}\label{coro:privacy}
    Algorithm~\ref{alg:approximate} is $(\eps,\delta)$-differentially
    private.
\end{lemma}
\begin{proof}
    Changing one point in $X$ can change only
    one of the $X^j$'s. This can
    only change one point in $Q$, which in turn can only
    change the counts in two histogram cells by $1$.
    Therefore, the sensitivity is $2$. % Since the choice
    %of $i$ was arbitrary, this is true for all $i$.
    %For a reference
    %point $p_i$, changing a point in $X^{j^*}$ can either move
    %its projection on to the subspace spanned by $X^{j^*}$ to a
    %different histogram cell, or keep it in the same cell.
%    Privacy now follows from the guarantees of DP-histogram (Lemma~\ref{lem:priv-hist}).
    Because the sensitivity of the histogram step is bounded
    by $2$ (Lemma~\ref{lem:histogram-sensitivity}), an application
    of DP-histogram, by Lemma~\ref{lem:priv-hist}, is $(\eps,\delta)$-DP.
    Outputting a random
    point in the privately found histogram cell preserves privacy
    by post-processing (Lemma~\ref{lem:post-processing}).
    Hence, the claim.
\end{proof}

\subsection{Accuracy}

\begin{comment}

We begin by showing a technical result that says that
any two matrices, whose difference is bounded in operator
norm, and which have significant eigenvalue gap, span
similar subspaces.

\begin{lemma}\label{lem:projections-close}
    Let $A, \tilde A \in \mathbb{R}^{d \times d}$ be symmetric
    matrices. Suppose $\|A-\tilde A\| \le \varepsilon$.
    Let $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_d$
    be the eigenvalues of $A$. Suppose $\lambda_k-\lambda_{k+1} > \varepsilon$.
    Let $\Pi_k, \tilde \Pi_k \in \mathbb{R}^{d \times d}$ be
    the projections onto the eigenspaces corresponding to
    the largest $k$ eigenvalues of $A$ and $\tilde A$ respectively.
    Then
    \[\left\| \Pi_k - \tilde \Pi_k \right\| \le
        \frac{\varepsilon}{\lambda_k-\lambda_{k+1}-\varepsilon}.\]
\end{lemma}
\begin{proof}
    Let $A,\tilde{A},U,\tilde{U},\Lambda,\tilde\Lambda
    \in \mathbb{R}^{d \times d}$ satisfy the following.
    (i) $A=U\Lambda U^T$,
    $\tilde{A} = \tilde{U} \tilde{\Lambda} \tilde{U}^T$,
    (ii) $U^TU=I_d=\tilde{U}^T\tilde{U}$, and (iii) $\Lambda$
    and $\tilde\Lambda$ are diagonal matrices with entries
    in descending order. Let $\lambda_i, \tilde{\lambda}_i$
    denote the $i^\text{th}$ diagonal entry of $\Lambda, \tilde\Lambda$.
    Denote $U_{a:b}, \tilde{U}_{a:b} \in \mathbb{R}^{d \times (b-a+1)}$
    to be the matrix formed by columns $a, a+1, \cdots, b$ of $U$
    and $\tilde{U}$ respectively (i.e., corresponding to
    $\lambda_a \ge \lambda_{a+1} \ge \cdots \ge \lambda_b$
    and $\tilde\lambda_a \ge \tilde\lambda_{a+1} \ge \cdots \ge \tilde\lambda_b$).

    Lemma~\ref{lem:weyl} tells us
    $|\tilde\lambda_k - \lambda_k| \le \|\tilde{A}-A\|$
    for all $k \in [d]$.
    Note that the operator norm is submultiplicative -- i.e.,
    $\|M M'\| \le \|M\| \cdot \|M'\|$.
    It immediately follows from Lemma~\ref{lem:davis-kahan}
    that, for all $k \in [d-1]$,
    we have
    \begin{align*}
        \|\tilde{U}_{1:k}^T {U}_{k+1:d}\| &\leq
                \frac{\|\tilde{A}-A\|}{\lambda_k-\tilde{\lambda}_{k+1}}\\
            &=\frac{\|\tilde{A}-A\|}{\lambda_k-\lambda_{k+1}-\lambda_{k+1}-\tilde{\lambda}_{k+1}}\\
            &\leq\frac{\|\tilde{A}-A\|}{\lambda_k - \lambda_{k+1} - \|\tilde{A}-A\|}.
    \end{align*}
    Now,
    \begin{align*}
        \tilde{\lambda}_k - \lambda_{k+1} &=
                \tilde{\lambda}_k - \lambda_k + \lambda_k - \lambda_{k+1}\\
            &> -\abs{\tilde{\lambda}_k - \lambda_k} + \lambda_k - \lambda_{k+1}\\
            &> -\eps + \eps\\
            &= 0.
    \end{align*}
    Therefore, we can apply Lemma~\ref{lem:davis-kahan}
    again
    to get $\|{U}_{1:k}^T \tilde{U}_{k+1:d}\| \le
    \frac{\|\tilde{A}-A\|}{\lambda_k - \lambda_{k+1} - \|\tilde{A}-A\|}$,
    and $\| \tilde{U}_{k+1:d}^T {U}_{1:k}\| \le
    \frac{\|\tilde{A}-A\|}{\lambda_k - \lambda_{k+1} - \|\tilde{A}-A\|}$
    follows because all eigenvalues of $U$ and $\tilde{U}$
    are non-negative.

    The ultimate quantity of interest for us is the
    difference between the projections $U_{1:k} U_{1:k}^T$
    and $\tilde{U}_{1:k} \tilde{U}_{1:k}^T$. We can
    apply the Lemma~\ref{lem:davis-kahan} to bound this:
    Note that $\tilde U_{1:k} \tilde U_{1:k}^T +
    \tilde U_{k+1:d} \tilde U_{k+1:d}^T = \tilde U \tilde U^T = I_d$.
    For any $A \in \mathbb{R}^{n \times m}$, $\|A^TA-I_m\|=\|AA^T-I_n\|$,
    as these are symmetric matrices with the same nonzero
    eigenvalues. Thus,
    \begin{align*}
        \left\| U_{1:k} U_{1:k}^T - \tilde{U}_{1:k} \tilde{U}_{1:k}^T \right\|
            &= \left\| U_{1:k} U_{1:k}^T + \tilde{U}_{k+1:d}
                \tilde{U}_{k+1:d}^T - I_d \right\| \\
            &= \left\|\left( U_{1:k} , \tilde{U}_{k+1:d} \right)
                \left( \begin{array}{c} U_{1:k}^T \\
                \tilde{U}_{k+1:d}^T \end{array} \right) - I_d \right\|\\
            &= \left\|\left( U_{1:k} , \tilde{U}_{k+1:d} \right)
                \left( U_{1:k} , \tilde{U}_{k+1:d} \right)^T - I_d \right\|\\
            &= \left\|\left( U_{1:k} , \tilde{U}_{k+1:d} \right)^T
                \left( U_{1:k} , \tilde{U}_{k+1:d} \right) - I_d \right\|\\
            &= \left\| \left( \begin{array}{cc} U_{1:k}^T U_{1:k}
                & U_{1:k}^T \tilde{U}_{k+1:d} \\ \tilde{U}_{k+1:d}^T U_{1:k}
                & \tilde{U}_{k+1:d}^T \tilde{U}_{k+1:d} \end{array} \right)
                - \left( \begin{array}{cc} I_k & 0_{k \times d-k} \\ 0_{d-k \times k}
                & I_{d-k} \end{array} \right)\right\|\\
            &= \left\| \left( \begin{array}{cc} 0_{k \times k}
                & U_{1:k}^T \tilde{U}_{k+1:d} \\ \tilde{U}_{k+1:d}^T U_{1:k}
                & 0_{d-k \times d-k} \end{array} \right) \right\|\\
            &= \max\left\{ \left\| U_{1:k}^T \tilde{U}_{k+1:d} \right\|,
                \left\| \tilde{U}_{k+1:d}^T U_{1:k} \right\| \right\}\\
            &\le \frac{\|\tilde{A}-A\|}{\lambda_k - \lambda_{k+1} - \|\tilde{A}-A\|}.
    \end{align*}
    This concludes the proof.
\end{proof}

Now we delve into the utility analysis of the algorithm.
Note that any matrix can be represented by its singular
value decomposition (SVD), that is, any matrix $A=UDV^T$,
where $D$ is a diagonal matrix containing its singular
values in decreasing order, $U$ is the matrix with left
singular vectors, and $V$ is the matrix with right singular
vectors, and $XX^T$ is a symmetric matrix $UDD^TU^T$,
where $U$ is the matrix containing the eigenvectors of
$XX^T$. Hence, to work with the projection matrix of
the subspace spanned by the columns of $X$, we can directly
work with the subspace spanned by the columns vectors
of $XX^T$ because they are equivalent. For $1 \leq j \leq t$,
let $X^j$ be the subsets of $X$ as defined in
Algorithm~\ref{alg:approximate}, and $\Pi_j$ be the
projection matrices of their respective subspaces. We
now show that $\Pi_j$ and the projection matrix of the
subspace spanned by $\Sigma_k$ are close in operator norm.

\end{comment}

Now we delve into the utility analysis of the algorithm.
For $1 \leq j \leq t$,
let $X^j$ be the subsets of $X$ as defined in
Algorithm~\ref{alg:approximate}, and $\Pi_j$ be the
projection matrices of their respective subspaces. We
now show that $\Pi_j$ and the projection matrix of the
subspace spanned by $\Sigma_k$ are close in operator norm.

\begin{lemma}\label{lem:empirical-subspaces-close}
    Let $\Pi$ be the projection matrix of the subspace
    spanned by the vectors of $\Sigma_k$, and for each
    $1 \leq j \leq t$, let $\Pi_j$ be the projection
    matrix as defined in Algorithm~\ref{alg:approximate}.
    If $m \geq O(k + \ln(qt))$, then
    $$\pr{}{\forall j, \|\Pi-\Pi_j\| \leq
        O\left(\frac{\gamma\sqrt{d}}{\sqrt{m}}\right)} \geq 0.95$$
\end{lemma}
\begin{proof}
    We show that the subspaces spanned by $X^j$ and
    the true subspace spanned by $\Sigma$ are close.
    Formally, we invoke
    Lemmata \ref{lem:sin-theta} and \ref{lem:sin-theta-property}.
    This closeness follows from standard matrix concentration
    inequalities.
    %which we discuss in Appendix \ref{sec:preliminaries}.
    
    Fix a $j \in [t]$. Note that $X^j$ can be written
    as $Y^j + H$, where $Y^j$ is the matrix of vectors
    distributed as $\cN(\vec{0},\Sigma_k)$, and $H$ is
    a matrix of vectors distributed as $\cN(\vec{0},\Sigma_{d-k})$,
    where $\Sigma_k$ and $\Sigma_{d-k}$ are defined as
    in Remark~\ref{rem:gamma}.
    By Corollary~\ref{coro:normal-spectrum}, with probability at least $1-\tfrac{0.02}{t}$,
    $s_k(Y^j) \in \Theta((\sqrt{m}+\sqrt{k})(\sqrt{s_k(\Sigma_k)})) = \Theta(\sqrt{m}+\sqrt{k})> 0$.
    Therefore, the subspace spanned by
    $Y^j$ is the same as the subspace spanned by $\Sigma_k$.
    So, it suffices to look at the subspace spanned
    by $Y^j$.

    Now, by Corollary~\ref{coro:normal-spectrum}, we know
    that with probability at least $1-\tfrac{0.02}{t}$,
    $\|X^j-Y^j\| = \|H\| \leq O((\sqrt{m}+{\sqrt{d}})\sqrt{s_1(\Sigma_{d-k})})
    \leq O(\gamma(\sqrt{m}+\sqrt{d})\sqrt{s_k(\Sigma_k)}) \leq O(\gamma(\sqrt{m}+\sqrt{d}))$.
    
    We wish to invoke Lemma~\ref{lem:sin-theta}. Let $UDV^T$
    be the SVD of $Y^j$, and let $\hat{U}\hat{D}\hat{V}^T$ be
    the SVD of $X^j$. Now, for a matrix $M$, let $\Pi_M$ denote
    the projection matrix of the subspace spanned by the columns
    of $M$. Define quantities $a,b,z_{12},z_{21}$ as follows.
    \begin{align*}
        a &= s_{\min}(U^TX^jV)\\
            &= s_{\min}(U^TY^jV + U^THV)\\
            &= s_{\min}(U^TY^jV) \tag{Columns of $U$ are orthogonal to columns of $H$}\\
            &= s_k(Y^j)\\
            &\in \Theta(\sqrt{m}+\sqrt{k})\\
            &\in \Theta(\sqrt{m})\\
        b &= \|U_{\bot}^TX^jV_{\bot}\|\\
            &= \|U_{\bot}^TY^jV_{\bot} + U_{\bot}^THV_{\bot}\|\\
            &= \|U_{\bot}^THV_{\bot}\|
                \tag{Columns of $U_{\bot}$ are orthogonal to columns of $Y^j$}\\
            &\leq \|H\|\\
            &\leq O(\gamma(\sqrt{m}+\sqrt{d}))\\
        z_{12} &= \|\Pi_U H \Pi_{V_{\bot}}\|\\
            &= 0\\
        z_{21} &= \|\Pi_{U_{\bot}}H\Pi_V\|\\
            &= \|\Pi_{U_{\bot}}\Sigma_{d-k}^{1/2}(\Sigma_{d-k}^{-1/2}H)\Pi_V\|
    \end{align*}
    Now, in the above, $\Sigma_{d-k}^{-1/2}H \in \RR^{d\times m}$,
    such that each of its entry is an independent sample from $\cN(0,1)$.
    Right-multiplying it by $\Pi_V$ makes it a matrix
    in a $k$-dimensional subspace of $\RR^m$, such that
    each row is an independent vector from a spherical
    Gaussian. Using Corollary~\ref{coro:normal-spectrum},
    $\|\Sigma_{d-k}^{-1/2}H\| \leq O(\sqrt{d}+\sqrt{k}) \leq O(\sqrt{d})$
    with probability at least $1-\tfrac{0.01}{t}$.
    Also, $\|\Pi_{U_{\bot}}\Sigma_{d-k}^{1/2}\| \leq O(\gamma\sqrt{s_k(\Sigma_k)}) \leq O(\gamma)$.
    This gives us:
    $$z_{21} \leq O(\gamma\sqrt{d}).$$

    Since $a^2 > 2b^2$, we get the following by
    Lemma~\ref{lem:sin-theta}.
    \begin{align*}
        \|\text{Sin}(\Theta)(U,\hat{U})\| &\leq \frac{az_{21} + bz_{12}}
                {a^2-b^2-\min\{z_{12}^2,z_{21}^2\}}\\
            &\leq O\left(\frac{\gamma\sqrt{d}}{\sqrt{m}}\right)
    \end{align*}

    Therefore, using Lemma~\ref{lem:sin-theta-property},
    and applying the union bound over all $j$, we get the
    required result.
\end{proof}

Let $\xi = O\left(\tfrac{\gamma\sqrt{d}}{\sqrt{m}}\right)$. We
show that the projections of any
reference point are close.

\begin{corollary}\label{coro:reference-projections-close}
    Let $p_1,\dots,p_q$ be the reference points as
    defined in Algorithm~\ref{alg:approximate}, and
    let $\Pi$ and $\Pi_j$ (for $1 \leq j \leq t$) be
    projections matrices as defined in Lemma~\ref{lem:empirical-subspaces-close}.
    Then
    $$\pr{}{\forall i,j, \|(\Pi-\Pi_j)p_i\| \leq O(\xi(\sqrt{k}+\sqrt{\ln(qt)}))} \geq 0.9.$$
\end{corollary}
\begin{proof}
    We know from Lemma~\ref{lem:empirical-subspaces-close}
    that $\|\Pi-\Pi_j\| \leq \xi$ for all $j$ with
    probability at least $0.95$. For $j \in [t]$, let
    $\wh{\Pi}_j$ be the projection matrix for the union
    of the $j^{\text{th}}$ subspace and the subspace
    spanned by $\Sigma_k$. Lemma~\ref{lem:gauss-vector-norm}
    implies that with probability at least $0.95$,
    for all $i,j$, $\|\wh{\Pi}_j p_i\| \leq O(\sqrt{k}+\sqrt{\ln(qt)})$.
    Therefore,
    \begin{align*}
        \|(\Pi-\Pi_j)p_i\| &= \|(\Pi-\Pi_j)\wh{\Pi}_jp_i\|
            \leq \|\Pi-\Pi_j\|\cdot\|\wh{\Pi}_jp_i\|
            \leq O(\xi(\sqrt{k}+\sqrt{\ln(qt)})).
    \end{align*}
    Hence, the claim.
\end{proof}

The above corollary shows that the projections of
each reference point lie in a ball of radius $O(\xi\sqrt{k})$.
Next, we show that for each reference point, all the
projections of the point lie inside a histogram cell
with high probability. For notational convenience, since
each point in $Q$ is a concatenation of the projection
of all reference points on a given subspace, for all
$i,j$, we refer to
$(0,\dots,0,Q_{(i-1)d+1}^j,\dots,Q_{id}^j,0,\dots,0) \in R^{qd}$
(where there are $(i-1)d$ zeroes behind $Q_{(i-1)d+1}^j$,
and $(q-i)d$ zeroes after $Q_{id}^j$) as $p_i^j$.

\begin{lemma}\label{lem:histogram-cell-points}
    Let $\ell$ and $\lambda$ be the length of a histogram
    cell and the random offset respectively, as defined in
    Algorithm~\ref{alg:approximate}. Then
    $$\pr{}{|\omega \cap Q| = t} \geq 0.8.$$
    Thus there exists $\omega \in \Omega$ that,
    such that all points in $Q$ lie within $\omega$.
\end{lemma}
\begin{proof}
    Let $r = O(\xi(\sqrt{k}+\sqrt{\ln(qt)}))$. This implies that $\ell = 20r\sqrt{q}$.
    The random offset could also be viewed as moving along a
    diagonal of a cell by $\lambda\ell\sqrt{dq}$. We know that
    with probability at least $0.8$, for each $i$, all projections
    of reference point $p_i$ lie in a ball of radius $r$.
    This means that all the points in $Q$ lie in a ball of
    radius $r\sqrt{q}$. Then
    $$\pr{}{|\omega \cap Q| = t} \leq \pr{}{\frac{1}{20} \geq
        \lambda \vee \lambda \geq \frac{19}{20}} = \frac{1}{10}.$$
    Taking the union bound over all $q$ and the failure
    of the event in Corollary~\ref{coro:reference-projections-close},
    we get the claim.
\end{proof}

Now, we analyse the sample complexity due
to the private algorithm, that is,
DP-histograms.

\begin{lemma}\label{lem:dp-histogram-cost}
    Let $\omega$ be the histogram cell as defined in
    Algorithm~\ref{alg:approximate}. Suppose $\pcount(\omega)$
    is the noisy count of $\omega$ as a result of applying
    the private histogram. If
    $t \geq O\left(\frac{\log(1/\delta)}{\eps}\right),$
    then
    $$\pr{}{\abs{\pcount(\omega)} \geq \frac{t}{2}} \geq 0.75.$$
\end{lemma}
\begin{proof}
    Lemma~\ref{lem:histogram-cell-points} implies that
    with probability at least $0.8$, for each $i$, all
    projections of $p_i$ lie in a histogram cell, that is,
    all points of $Q$ lie in a histogram cell in $\Omega$.
    Because of the error bound in Lemma~\ref{lem:priv-hist}
    and our bound on $t$, we see at least $\tfrac{t}{2}$
    points in that cell with probability at least $1-0.05$.
    Therefore, by taking the union bound, the proof is complete.
\end{proof}

We finally show that the error of the projection matrix
that is output by Algorithm~\ref{alg:approximate} is small.

\begin{lemma}\label{lem:final-projection}
    Let $\wh{\Pi}$ be the projection matrix as defined in
    Algorithm~\ref{alg:approximate}, and $n$ be the total
    number of samples. If
    $$\gamma^2 \in
        O\left(\frac{\eps\alpha^2n}{d^{2}k\ln(1/\delta)}\cdot
        \min\left\{\frac{1}{k},
        \frac{1}{\ln(k\ln(1/\delta)/\eps)}
        \right\}\right),$$
    $n \geq O(\frac{k\log(1/\delta)}{\eps}+\frac{\ln(1/\delta)\ln(\ln(1/\delta)/\eps)}{\eps})$,
    and $q \geq O(k)$
    the with probability at least $0.7$, $\|\wh{\Pi}-\Pi\| \leq \alpha$.
\end{lemma}
\begin{proof}
    For each $i \in [q]$, let $p_i^*$ be the projection
    of $p_i$ on to the subspace spanned by $\Sigma_k$,
    $\wh{p}_i$ be as defined in the algorithm, and $p_i^j$
    be the projection of $p_i$ on to the subspace spanned
    by the $j^{\mathrm{th}}$ subset of $X$. From Lemma~\ref{lem:dp-histogram-cost},
    we know that all $p_i^j$'s are contained in a histogram
    cell of length $\ell$. This implies that $p_i^*$ is also
    contained within the same histogram cell.

    Now, let $P=(p_1^*,\dots,p_q^*)$ and $\wh{P}=(\wh{p}_1,\dots,\wh{p}_q)$.
    Then by above, $\wh{P}=P+E$, where $\|E\|_F \leq 2\ell\sqrt{dq}$. Therefore,
    $\|E\| \leq 2\ell\sqrt{dq}$. Let $E=E_P+E_{\wb{P}}$,
    where $E_P$ is the component of $E$ in the subspace
    spanned by $P$, and $E_{\wb{P}}$ be the orthogonal
    component. Let $P' = P + E_P$. We will be analysing
    $\wh{P}$ with respect to $P'$.

    Now, with probability
    at least $0.95$, $s_k(P) \in \Theta(\sqrt{k})$ due to our
    choice of $q$ and using Corollary~\ref{coro:normal-spectrum},
    and $s_{k+1}(P) = 0$. So, $s_{k+1}(P') = 0$ because $E_P$ is
    in the same subspace as $P$. Now, using Lemma~\ref{lem:least-singular},
    we know that $s_k(P') \geq s_k(P) - \|E_P\| \geq \Omega(\sqrt{k}) > 0$.
    This means that
    $P'$ has rank $k$, so the subspaces spanned by $\Sigma_k$
    and $P'$ are the same.

    As before, we will try to
    bound the distance between the subspaces spanned
    by $P'$ and $\wh{P}$. Note that using Lemma~\ref{lem:weyl-singular},
    we know that $s_k(P') \leq s_k(P) + \|E_P\| \leq O(\sqrt{k})$.

    We wish to invoke Lemma~\ref{lem:sin-theta} again. Let $UDV^T$
    be the SVD of $P'$, and let $\hat{U}\hat{D}\hat{V}^T$ be
    the SVD of $\wh{P}$. Now, for a matrix $M$, let $\Pi_M$ denote
    the projection matrix of the subspace spanned by the columns
    of $M$. Define quantities $a,b,z_{12},z_{21}$ as follows.
    \begin{align*}
        a &= s_{\min}(U^T\wh{P}V)\\
            &= s_{\min}(U^TP'V + U^TE_{\wb{P}}V)\\
            &= s_{\min}(U^TP'V) \tag{Columns of $U$ are orthogonal to columns of $E_{\wb{P}}$}\\
            &= s_k(P')\\
            &\in \Theta(\sqrt{k})\\
        b &= \|U_{\bot}^T\wh{P}V_{\bot}\|\\
            &= \|U_{\bot}^TP'V_{\bot} + U_{\bot}^TE_{\wb{P}}V_{\bot}\|\\
            &= \|U_{\bot}^TE_{\wb{P}}V_{\bot}\|
                \tag{Columns of $U_{\bot}$ are orthogonal to columns of $P'$}\\
            &\leq \|E_{\wb{P}}\|\\
            &\leq O(\ell\sqrt{dq})\\
        z_{12} &= \|\Pi_U E_{\wb{P}} \Pi_{V_{\bot}}\|\\
            &= 0\\
        z_{21} &= \|\Pi_{U_{\bot}}E_{\wb{P}}\Pi_V\|\\
            &\leq \|E_{\wb{P}}\|\\
            &\leq O(\ell{\sqrt{dq}})
    \end{align*}

    Using Lemma~\ref{lem:sin-theta}, we get the following.
    \begin{align*}
        \|\text{Sin}(\Theta)(U,\hat{U})\| &\leq \frac{az_{21} + bz_{12}}
                {a^2-b^2-\min\{z_{12}^2,z_{21}^2\}}\\
            &\leq O\left(\ell\sqrt{dk}\right)\\
            &\leq \alpha
    \end{align*}

    This completes our proof.
\end{proof}

\subsection{Boosting}

In this subsection, we discuss boosting of error
guarantees of Algorithm~\ref{alg:approximate}.
The approach we use is very similar to the well-known
Median-of-Means method: we run the algorithm multiple
times, and choose an output that is close to all
other ``good'' outputs. We formalise this in
Algorithm~\ref{alg:approximate-boosted}.

\begin{algorithm}[h!]
\caption{\label{alg:approximate-boosted}DP Approximate Subspace Estimator Boosted
    $\DPASEB_{\eps, \delta, \alpha, \beta, \gamma, k}(X)$}
\KwIn{Samples $X_1,\dots,X_{n} \in \R^d$.
    Parameters $\eps, \delta, \alpha, \beta, \gamma, k > 0$.}
\KwOut{Projection matrix $\wh{\Pi} \in \R^{d \times d}$ of rank $k$.}
\vspace{5pt}

Set parameters:
    $t \gets C_3 \log(1/\beta)$ \qquad $m \gets \lfloor n/t \rfloor$
\vspace{5pt}

Split $X$ into $t$ datasets of size $m$: $X^1,\dots,X^t$.
\vspace{5pt}

\tcp{Run $\DPASE$ $t$ times to get multiple projection matrices.}
\For{$i \gets 1,\dots,t$}{
    $\wh{\Pi}_i \gets \DPASE_{\eps,\delta,\alpha,\gamma,k(X^i)}$
}
\vspace{5pt}

\tcp{Select a good subspace.}
\For{$i \gets 1,\dots,t$}{
    $c_i \gets 0$\\
    \For{$j \in [t]\setminus\{i\}$}{
        \If{$\|\wh{\Pi}_i-\wh{\Pi}_j\| \leq 2\alpha$}{
            $c_i \gets c_i + 1$
        }
    }
    \If{$c_i \geq 0.6t-1$}{
        \Return $\wh{\Pi}_i$.
    }
}
\vspace{5pt}

\tcp{If there were not enough good subspaces, return $\bot$.}
\Return $\bot.$
\vspace{5pt}
\end{algorithm}

Now, we present the main result of this subsection.

\begin{theorem}\label{thm:approximate-boosted}
    Let $\Sigma \in \R^{d \times d}$ be an arbitrary, symmetric, PSD
    matrix of rank $\geq k \in \{1,\dots,d\}$, and let $0 < \gamma < 1$.
    Suppose $\Pi$ is the projection matrix corresponding
    to the subspace spanned by the vectors of $\Sigma_k$.
    Then given
    $$\gamma^2 \in
        O\left(\frac{\eps\alpha^2n}{d^{2}k\ln(1/\delta)}\cdot
        \min\left\{\frac{1}{k},
        \frac{1}{\ln(k\ln(1/\delta)/\eps)}
        \right\}\right),$$
    such that $\lambda_{k+1}(\Sigma) \leq \gamma^2\lambda_k(\Sigma)$,
    for every $\eps,\delta>0$, and $0 < \alpha,\beta < 1$,
    there exists and $(\eps,\delta)$-DP algorithm that takes
    $$n \geq O\left(\frac{k\log(1/\delta)\log(1/\beta)}{\eps} +
        \frac{\log(1/\delta)\log(\log(1/\delta)/\eps)\log(1/\beta)}{\eps}\right)$$
    samples from $\cN(\vec{0},\Sigma)$, and outputs a projection matrix $\wh{\Pi}$,
    such that $\|\Pi-\wh{\Pi}\| \leq \alpha$ with probability
    at least $1-\beta$.
\end{theorem}
\begin{proof}
    Privacy holds trivially by Theorem~\ref{thm:approximate}.

    We know by Theorem~\ref{thm:approximate} that
    for each $i$, with probability at least $0.7$,
    $\|\wh{\Pi}_i-\Pi\| \leq \alpha$. This means
    that by Lemma~\ref{lem:chernoff-add}, with probability
    at least $1-\beta$, at least $0.6t$ of all
    the computed projection matrices are accurate.

    This means that there has to be at least one projection
    matrix that is close to $0.6t-1>0.5t$ of these
    accurate projection matrices. So, the algorithm
    cannot return $\bot$.

    Now, we want to argue that the returned projection
    matrix is accurate, too. Any projection matrix
    that is close to at least $0.6t-1$ projection
    matrices must be close to at least one accurate
    projection matrix (by pigeonhole principle). Therefore,
    by triangle inequality,
    it will be close to the true subspace. Therefore,
    the returned projection matrix is also accurate.
\end{proof}

%\subsection{Putting It All Together}

%Now, we are ready to finish the main theorem of the section.

%\begin{proof}[Proof of Theorem~\ref{thm:approximate}]
%    By Corollary~\ref{coro:privacy} and Lemma~\ref{lem:final-projection},
%    Algorithm~\ref{alg:approximate} is differentially private
%    and accurate.
%\end{proof}
