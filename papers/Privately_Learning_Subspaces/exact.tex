\section{Exact case}

Here, we discuss the case, where all $n$ points lie \emph{exactly} in a subspace $s_*$ of dimension $k$ of $\RR^d$. Our goal
is to privately output that subspace. We do it under the
assumption that all strict subspaces of $s_*$ contain at most $\ell$
points.
If the points are in general position, then $\ell=k-1$, as any strictly smaller subspace has dimension $<k$ and cannot contain more points than its dimension.
Let $\mathcal{S}_d^k$ be the set of all $k$-dimensional
subspaces of $\mathbb{R}^d$. Let $\mathcal{S}_d$ be the
set of all subspaces of $\mathbb{R}^d$. We formally define
that problem as follows.

\begin{problem}\label{prob:exact}
    Assume (i) all but at most $\ell$, input points are in some
    $s_* \in \mathcal{S}_d^k$, and (ii)  every subspace
    of dimension $<k$ contains at most $\ell$ points. (If the points
    are in general position -- aside from being contained in
    $s_*$ -- then $\ell=k-1$.) The goal is to output a representation
    of $s_*$.
\end{problem}

We call these $\leq \ell$ points that do not lie in
$s_*$, ``adversarial points''.
With the problem defined in Problem~\ref{prob:exact}, we
will state the main theorem of this section.

\begin{theorem}\label{thm:exact}
    For any $\eps,\delta>0$, $\ell \ge k-1 \ge 0$, and
    $$n \geq O\left(\ell + \frac{\log(1/\delta)}{\eps}\right),$$ there
    exists an $(\eps,\delta)$-DP algorithm
    $M : \mathbb{R}^{d \times n} \to \mathcal{S}_d^k$, such that if
    $X$ is a dataset of $n$ points satisfying the conditions
    in Problem~\ref{prob:exact},
    then $M(X)$ outputs a representation of $s_*$ with probability $1$.
\end{theorem}

We prove Theorem \ref{thm:exact} by proving the privacy and
the accuracy guarantees of Algorithm~\ref{alg:exact}.
The algorithm performs a $\GAPMAX$ (cf.~Lemma~\ref{lem:gap-max}).
It assigns a score to all the relevant
subspaces, that is, the subspaces spanned by the points
of the dataset $X$. We show that the only subspace that
has a high score is the true subspace $s_*$, and the rest
of the subspaces have low scores. Then $\GAPMAX$ outputs
the true subspace successfully because of the gap between
the scores of the best subspace and the second to the best
one. For $\GAPMAX$ to work all the time, we define a default
option in the output space that has a high score, which we
call $\NULL$. Thus, the output space is now
$\cY = \cS_d \cup \{\NULL\}$. Also, for $\GAPMAX$ to run in
finite time, we filter $\cS_d$ to select finite number of subspaces
that have at least $0$ scores on the basis of $X$. Note that
this is a preprocessing step, and does not violate privacy as,
we will show, all other subspaces already have $0$ probability
of getting output.
We define the score function
$u : \mathcal{X}^n \times \cY \to \mathbb{N}$
as follows.
\[ u(x,s) :=
\begin{cases}
    |x \cap s| - \sup \{ |x \cap t| : t \in \mathcal{S}_d, t \subsetneq s \} &
        \text{if $s \in \cS_d$}\\
    \ell + \frac{4\log(1/\delta)}{\eps} + 1 & \text{if $s = \NULL$}
\end{cases}
\]
Note that this score function can be computed in finite
time because for any $m$ points and $i>0$, if the points
are contained in an $i$-dimensional subspace, then the
subspace that contains all $m$ points must lie within
the set of subspaces spanned by ${m \choose i+1}$ subsets
of points.

\begin{algorithm}[h!] \label{alg:exact}
\caption{DP Exact Subspace Estimator
    $\DPESE_{\eps, \delta, k, \ell}(X)$}
\KwIn{Samples $X \in \R^{d \times n}$.
    Parameters $\eps, \delta, k, \ell > 0$.}
\KwOut{$\hat{s} \in \cS_d^k$.}
\vspace{5pt}

Set $\cY \gets \{\NULL\}$ and sample noise $\xi(\NULL)$ from $\TLap(2,\eps,\delta)$.\\
Set score $u(X,\NULL) = \ell + \frac{4\log(1/\delta)}{\eps} + 1$.
\vspace{5pt}

\tcp{Identify candidate outputs.}
\For{each subset $S$ of $X$ of size $k$}{
    Let $s$ be the subspace spanned by $S$.\\
    $\cY \gets \cY \cup \{s\}$.\\
    Sample noise $\xi(s)$ from $\TLap(2,\eps,\delta)$.\\
    Set score $u(X,s) = |x \cap s| - \sup \{ |x \cap t| : t \in \mathcal{S}_d, t \subsetneq s \} $.
}
\vspace{5pt}

\tcp{Apply $\GAPMAX$.}
Let $s_1 = \argmax_{s \in \cY} u(X,s)$ be the candidate with the largest score.\\
Let $s_2 = \argmax_{s \in \cY \setminus \{s_1\}} u(X,s)$ be the candidate with the second-largest score.\\
Let $\hat s = \argmax_{s \in \cY} \max\{ 0 , u(X,s) - u(X,s_2) -1\} + \xi(s)$.\\
\tcp{Truncated Laplace noise $\xi \sim \TLap(2,\eps,\delta)$; see Lemma \ref{lem:truncated-laplace}}

\vspace{5pt}
\Return $\hat{s}.$
\vspace{5pt}
\end{algorithm}

We split the proof of Theorem~\ref{thm:intro-main-exact} into sections
for privacy (Lemma~\ref{lem:exact-privacy}) and accuracy (Lemma~\ref{lem:exact-accuracy}).

\subsection{Privacy}

\begin{lemma}\label{lem:exact-privacy}
    Algorithm~\ref{alg:exact} is $(\eps,\delta)$-differentially
    private.
\end{lemma}
The proof of Lemma \ref{lem:exact-privacy} closely follows the
privacy analysis of $\GAPMAX$ by \cite{BunDRS18}. The only novelty
is that Algorithm \ref{alg:exact} may output $\NULL$ in the case
that the input is malformed (i.e., doesn't satisfy the assumptions
of Problem~\ref{prob:exact}).

The key is that the score $u(X,s)$ is low sensitivity. Thus
$\max\{ 0 , u(X,s) - u(X,s_2) -1\}$ also has low sensitivity.
What we gain from subtracting the second-largest score and
taking this maximum is that these values are also sparse -- only
one ($s=s_1$) is nonzero. This means we can add noise to all
the values without paying for composition. We now prove
Lemma~\ref{lem:exact-privacy}.

\begin{proof}
    First, we argue that the sensitivity of $u$ is
    $1$. %Consider any $s \in \cS_d$, and neighbouring datasets $X,X'$.
    The quantity $\abs{X \cap s}$ has sensitivity $1$ and so does
    $\sup \{ |X \cap t| : t \in \mathcal{S}_d, t \subsetneq s \}$.
    This implies sensitivity $2$ by the triangle inequality.
    However, we see that it is not possible to change one point
    that simultaneously increases $\abs{X \cap s}$ and decreases
    $\sup \{ |X \cap t| : t \in \mathcal{S}_d, t \subsetneq s \}$
    or vice versa. Thus the sensitivity is actually $1$.

    We also argue that $u(X,s_2)$ has sensitivity $1$, where $s_2$
    is the candidate with the second-largest score.
    Observe that the second-largest score is a monotone function of
    the collection of all scores -- i.e., increasing scores cannot
    decrease the second-largest score and vice versa.
    Changing one input point can at most increase all the scores by
    $1$, which would only increase the second-largest score by $1$.

    This implies that $\max\{ 0, u(X,s) - u(X,s_2) -1 \}$ has sensitivity
    $2$ by the triangle inequality and the fact that the maximum does
    not increase the sensitivity.

    Now we observe that for any input $X$ there is at most one $s$ such
    that $\max\{ 0, u(X,s) - u(X,s_2) -1 \} \ne 0$, namely $s=s_1$.
    We can say something even stronger: Let $X$ and $X'$ be neighbouring
    datasets with $s_1$ and $s_2$ the largest and second-largest scores
    on $X$ and $s_1'$ and $s_2'$ the largest and second-largest scores
    on $X'$. Then there is at most one $s$ such that
    $\max\{ 0, u(X,s) - u(X,s_2) -1 \} \ne 0$ or $\max\{ 0, u(X',s) - u(X',s_2') -1 \} \ne 0$.
    In other words, we cannot have both $u(X,s_1) - u(X,s_2) >1$ and
    $u(X',s_1') - u(X',s_2') >1$ unless $s_1=s_1'$. This holds because
    $u(X,s) - u(X,s_2)$ has sensitivity $2$.

    With these observations in hand, we can delve into the privacy
    analysis. Let $X$ and $X'$ be neighbouring datasets with $s_1$
    and $s_2$ the largest and second-largest scores on $X$ and $s_1'$
    and $s_2'$ the largest and second-largest scores on $X'$. Let $\cY$
    be the set of candidates from $X$ and let $\cY'$ be the set of
    candidates from $X'$. Let $\check \cY = \cY \cup \cY'$ and
    $\hat \cY = \cY \cap \cY'$.

    We note that, for $s \in \check \cY$, if $u(X,s) \le \ell$, then
    there is no way that $\hat s = s$. This is because
    $|\xi(s)|\le \frac{2 \log(1/\delta)}{\varepsilon}$ for all $s$ and
    hence, there is no way we could have
    $\argmax_{s \in \cY} \max\{ 0 , u(X,s) - u(X,s_2) -1\} +
    \xi(s) \ge \argmax_{s \in \cY} \max\{ 0 , u(X,\NULL) - u(X,s_2) -1\} + \xi(\NULL)$.

    If $s \in \check \cY \setminus \hat \cY$, then
    $u(X,s) \le |X \cap s| \le k+1 \le \ell$ and $u(X',s) \le \ell$.
    This is because $s \notin \hat \cY$ implies $|X \cap s| < k$ or
    $|X' \cap s| < k$, but $|X \cap s| \le |X' \cap s| +1$. Thus,
    there is no way these points are output and, hence, we can ignore
    these points in the privacy analysis. (This is the reason for adding
    the $\NULL$ candidate.)

    Now we argue that the entire collection of noisy values
    $\max\{ 0 , u(X,s) - u(X,s_2) -1\} + \xi(s)$ for $s \in \hat \cY$
    is differentially private.
    This is because we are adding noise to a vector where (i) on the
    neighbouring datasets only $1$ coordinate is potentially different
    and (ii) this coordinate has sensitivity $2$.
\end{proof}

\subsection{Accuracy}

We start by showing that the true subspace $s_*$ has a
high score, while the rest of the subspaces have low scores.

\begin{lemma}\label{lem:scores}
    Under the assumptions of Problem~\ref{prob:exact}, $u(x,s_*) \ge n - 2\ell$
    and $u(x,s')\le 2\ell$ for $s' \ne s_*$.
\end{lemma}
\begin{proof}
    We have $u(x,s_*) = |x \cap s_*| - |x \cap s'|$ for some
    $s' \in \mathcal{S}_d$ with $s' \subsetneq s_*$. The
    dimension of $s'$ is at most $k-1$ and, by the assumption
    (ii), $|x \cap s'| \le \ell$. 
    
    Let $s' \in \mathcal{S}_d \setminus \{s_*\}$.
    There are three cases to analyse:
    \begin{enumerate}
        \item Let $s' \supsetneq s_*$. Then
            $u(x,s') \le |x \cap s'| - |x \cap s_*| \leq \ell$
            because the $\leq \ell$ adverserial points and the $\geq n-\ell$
            non-adversarial points may not together lie in a subspace
            of dimension $k$.

        \item Let $s' \subsetneq s_*$. Let
            $k'$ be the dimension of $s'$. Clearly $k'<k$.
            By our assumption (ii), $|s' \cap x| \le \ell$.
            Then $u(x,s') = |x \cap s'| - |x \cap t| \le \ell$
            for some $t$ because the $\leq \ell$ adversarial points
            already don't lie in $s_*$, so they will not lie in
            any subspace of $s_*$.

        \item Let $s'$ be incomparable to $s_*$.
            Let $s'' = s' \cap s_*$. Then
            $u(x,s') \le |x \cap s'| - |x \cap s''| \leq \ell$
            because the adversarial points may not lie in $s_*$,
            but could be in $s'\setminus s''$.
    \end{enumerate}
    This completes the proof.
\end{proof}

Now, we show that the algorithm is accurate.

\begin{lemma}\label{lem:exact-accuracy}
    If
    $n \geq 3\ell + \frac{8\log(1/\delta)}{\eps} + 2,$
    then Algorithm~\ref{alg:exact} outputs $s_*$ for Problem~\ref{prob:exact}.
\end{lemma}
\begin{proof}
    From Lemma~\ref{lem:scores}, we know that $s_*$ has
    a score of at least $n-2\ell$, and the next best subspace
    can have a score of at most $\ell$. Also, the score of
    $\NULL$ is defined to be $\ell + \tfrac{4\log(1/\delta)}{\eps} + 1$.
    This means that the gap satisfies $\max\{ 0 , u(X,s_*) - u(X,s_2) -1\} \ge n - 3\ell - \tfrac{4\log(1/\delta)}{\eps}  -1$.
    Since the noise is bounded by $\tfrac{2\log(1/\delta)}{\eps}$, our bound on $n$ implies that $\hat s = s_*$
\end{proof}

%\subsection{Putting It All Together}
%
%\begin{proof}[Proof of Theorem~\ref{thm:exact}]
%    By Lemmata~\ref{lem:exact-privacy} and \ref{lem:exact-accuracy},
%    the algorithm is differentially private and accurate.
%\end{proof}

\begin{comment}
    
    Key questions: Remove/weaken assumption (ii) general
    position. Approximate case (or discretization).
    
    \subsection{Generalizing beyond general position}
    
    Still assuming $x \subset s_*$ for some $s_* \in \mathcal{S}_d^k$. Also assume $0 \notin x$.
    
    Claim: There exists some $s \in \mathcal{S}_d$ with $u(x,s) \ge n/k$.
    \begin{proof}
    Let $s_k=s_*$. For $i=k,k-1,k-2,\cdots,1$, inductively choose $s_{i-1} \in \mathcal{S}_d$ such that $s_{i-1} \subsetneq s_i$ and $|x \cap s_{i-1}|$ is maximal. By construction, $u(x,s_i) = |x \cap s_i| - |x \cap s_{i-1}|$ for all $i \in [k]$. By assumption, $|x \cap s_k| = n$. Also, $\mathsf{dimension}(s_i) \le i$. Thus $|x \cap s_0| = 0$.
    Since $\sum_{i=1}^k u(x,s_i) = |x \cap s_k| - |x \cap s_0| = n$, we must have $u(x,s_i) \ge n/k$ for some $i \in [k]$.
    \end{proof}

\end{comment}

\subsection{Lower Bound}

Here, we show that our upper bound is optimal up to constants for
the exact case.

\begin{theorem}
     Any $(\eps,\delta)$-DP algorithm that takes a dataset of $n$ points satisfying the conditions
    in Problem~\ref{prob:exact} and outputs $s_*$ with probability $>0.5$ requires
    $n \geq \Omega\left(\ell + \frac{\log(1/\delta)}{\eps}\right).$
\end{theorem}
\begin{proof}
    First, $n \ge \ell + k$. This is because we need at least
    $k$ points to span the subspace, and $\ell$ points could be corrupted.
    Second, $n \ge \Omega(\log(1/\delta)/\varepsilon)$ by group
    privacy. Otherwise, the algorithm is $(10,0.1)$-differentially
    private with respect to changing the \emph{entire} dataset and
    it is clearly impossible to output the subspace under this condition.
\end{proof}
