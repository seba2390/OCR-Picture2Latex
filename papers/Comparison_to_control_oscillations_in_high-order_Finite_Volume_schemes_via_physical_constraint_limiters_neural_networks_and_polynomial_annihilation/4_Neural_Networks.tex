\section{Limiting via Neural Networks} \label{se_neunet}

\subsection{Basics of Feedforward Networks}

In this section, we explain how we select our numerical flux using  feed forward neural networks (FNN).
The network is used to determine the local  indicator $\alpha$ which 
steers our convex combination inside the numerical flux and measures the regularity. This is an example of 
a high dimensional function interpolation. Our optimism concerning this problem stems from the following result proved in \cite{Cybenko1989}:
\begin{theorem}
	Let $\sigma: \R \to \R$ a sigmoidal function. Then the finite sum of the form
	\[
			\mathcal A \circ G(x) = \sum_{j=1}^N \alpha_j \sigma(\scp{y_j}{x} + b_j)
	\]
	are dense in $C(I_n)$ in the sup norm.
\end{theorem}
This theorem motivates the usage of FNN to approximate any function 
\begin{equation}
\mathcal C \subset C(\R^n, \R)
\label{eq:function_network}
\end{equation}
 on a constrained subset of $\R^n$. We will therefore give a short presentation of the general theory of neuronal networks.
Our feed forward network (also called 
multilayer perceptron (MLP)) is on particular example and it is set up in a sequence of layers containing a certain amount of neurons (computing units). 
The first layer (source layer) is handling the input data/signal to the network whereas the output layer (last layer) translates the new solution back. 
In between hidden layers are placed where the calculations are done. An FNN  with depth $K$ contains $K-1$ hidden layers and one output layer.
What happens in the network it the following operation: For an input signal $\mathbf{X} \in \R^n$, we have the output:
\begin{equation}\label{eq_function_network_2}
\mathbf{\tilde{Y}} = \mathcal{F} \circ G_k \circ \mathcal{A} \circ G_{k-1} \mathcal{A} \circ G_{k-2} \circ \cdots \circ G_1(\mathbf{X}) ,
\end{equation}
where $G_k$ denotes the affine transformation of the $k-$layer on a vector $\mathbf{Z}\in \R^{N_{k-1}}$ with 
\begin{equation}\label{eq:G_k}
G_k(\mathbf{Z}) = \mathbf{W}_k \mathbf{Z} + \mathbf{b}_k, \qquad \mathbf{W}_k  \in \R^{N_k \times N_{k-1}}, \quad \mathbf{b}_k \in \R^{N_k}.
\end{equation} 
$\mathbf{W}_k$ are the weights matrices and $\mathbf{b}_k$ are the bias vectors. Both contain the trainable parameters which we specify in the following. 
Further, in \cref{eq_function_network_2}, $\mathcal{A}$ are non-linear activation functions and  $\mathcal{F} $ is a non-linear output function that transforms 
the output data into a for us suitable form. There exists a bench of different activation functions for various problems. In our work, 
we restrict ourself to the currently popular Exponential Linear Units  (ELU) function 
\begin{equation}
\mathrm{ELU}(t) = \begin{cases}x, & x > 0, \\ \gamma (\exp(x)-1), & \text{else.} \end{cases}
\end{equation}
 We set $\gamma\equiv 1$ in our numerical simulations.\\
%we restrict ourself to the classical  sigmoid function 
%\begin{equation}
%\mathrm{sig}(t)=\frac{1}{1-\exp(-t)},
%\end{equation}
%because of it's solid theoretical background, and the popular Rectified linear unit
%\begin{equation}
%\mathrm{ReLU}(t) = \begin{cases}x, & x > 0, \\ 0, & \text{else.} \end{cases}
%\end{equation}
To approximate finally \eqref{eq:function_network} with our network \eqref{eq_function_network_2}, we must train the parameters using our training data. 
Therefore, we first create a set of training data with $N_T$ samples 
$$
T= \left\{ (\mathbf{X}_i, \mathbf{Y}_i): \mathbf{Y}_i=\mathcal{C}(\mathbf{X}_i) \forall i=1, \dots, N_T \right\}.
$$
Then, we define a suitable cost function that measures the discrepancy between the actual result vector $\mathbf{Y}$ 
and the predicted result vector $\mathbf{\tilde{Y}}$. We tested the following three cost functions:

\begin{enumerate}
\item Mean square error
$
	L(Y, \tilde Y) = \frac{\sum_{i=1}^{N_T} (Y_i - \tilde Y_i)^2}{N_T},
$
\item  Mean exponential error $L(Y, \tilde Y) = \frac{\sum_{i=1}^{N_T} (\exp(Y_i - \tilde Y_i) - 1)^2}{N_T}$,
\item 
the nonsymetric loss
$
	L(y, \tilde Y) = \frac {\sum_{i=1}^{N_T}d(Y, \tilde Y) }{N_T} \text{ with }  d(Y, \tilde Y) = \begin{cases} \gamma (Y_i - \tilde Y_i)^2 & Y_i \geq Y_i \\ \abs{Y_i - \tilde Y_i} & Y_i \leq Y_i \end{cases}
$
 and $\gamma = 10$. It gives the absolute error if the prediction is bigger than the result and the squared error if the prediction is smaller than the desired value.
\end{enumerate} 
Note that usually the second function results in higher penalties for under-prediction and that the third allows for sparse output in the domain.
 To train the network, we minimize the loss function with respect to the  parameters $\{ \mathbf{W}_k, \mathbf{b}_k \}_k$ over the set of training data. For the minimization process, we use an iterative optimization algorithm. 
We use the ADAM minimizer \cite{kingma2017adam}, alternatively we can apply the  momentum gradient descent. Both are used in the stochastic gradient descent fashion \cite{Rumelhart1986Learning}.

\begin{remark}[Overfitting and Dropout Layer]
As mentioned \textit{inter alia } in \cite{discacciati2020controlling}, the training set has to be selected quite carefully to avoid over-fitting. In such a case, the network performs poorly on general data since it is highly optimized for the training set.  To avoid this problem, a regularization technique is used. 
A popular regularization strategy is using a drop-out layer \cite{srivastava2014dropout}.
 During each optimization update step in the training-phase of the network, a dropout layer in front of the kth layer randomly sets a predefined fraction of the components of the intermediate vector computed by the kth  layer to zero. The advantages of this technique  are that the training is not biased towards a specific network architecture, additional stochasticity is injected into the optimization process to avoid getting trapped in local optima, and a sparsity structure is introduced into the network structure.
\end{remark}

 
\subsection{Data Driven Scheme for Single Conservation Laws and Systems}
 Our method using neuronal nets will be based on the fully discrete approach, but we will use neuronal nets as building blocks to approximate unknown real maps in the following roadmap:
\begin{enumerate}
	\item Select a random set of initial conditions $\uinit = \{u_1, u_2 , \dots, u_N \} \subset C^1_s$.
	\item Calculate high quality numerical solutions $v(x, t)$ to this set of initial conditions.
	\item Determine Projections $u$ of these solutions $v$ to a low resolution finite volume mesh.
	\item Calculate the exact flux of $v(x, t)$ over the given mesh boundaries and suitable \textbf{ time interval}.
	\item Infer suitable values for the convex combination parameter $\alpha$ \textbf{for the GT flux \eqref{eq_GT_flux}}. % that is use flux as a convex combination between an entropy stable and an entropy dissipative flux
	\item Use this database to train a neuronal network as a predictor for the unknown map $\alpha(u, \Delta t)$.
\end{enumerate}
The high quality numerical solution $v$ was calculated using classic finite volume methods on fine grids. The projection of these solutions to a low resolution mesh is given by
\[
	u_k = \frac{1}{\Delta x}\int_{x_{k-\frac 1 2}}^{x_{k+\frac 1 2}} v(x, t) \vd x \quad \text{with} \quad v(x, t) = \sum_k v_k(t)\chi_{\omega_k}(x),
\] 
where $\omega_k$ shall be cell $k$ of the fine grid and $v_k$ the mean value of the solution as approximated by a finite volume method. The calculation of an accurate numerical flux  approximation $\fprec$ at cell boundaries of the coarse grid is based on numerical quadrature in time, i.e.
\[
	\fprec_{k+\frac 1 2} = \nquad_{t^n}^{t^{n+1}} g\of{v\left(x^-_{k+\frac 1 2}, \cdot\right), v\left(x^+_{k+\frac 1 2}, \cdot \right)} \approx  \int_{t^n}^{t^{n+1}} f(v(x, t))  \vd t.
\]
Our numerical tests used low order quadrature methods as we are especially interested in flux values for nonsmooth $u$ in space and time. Therefore high-order quadrature rules would be of little use. Our next problem consists of finding a suitable and well defined $\alpha_{k+\frac 1 2}$ that satisfies
\[
	\fnnnn_{\alpha_{k + \frac 1 2}} \approx \fprec_{k+\frac 1 2}.
\]
We therefore formulate the following definition
\[
	\fnnnn_{\alpha_{k + \frac 1 2}} = \min_{f \in \ch\of{h^n_{k+\frac 1 2}, g^n_{k+\frac 1 2}} }\norm{f - \fprec_{k+\frac 1 2}}_2 = \PR_{\ch\of{h^n_{k+\frac 1 2}, g^n_{k+\frac 1 2}} }\fprec
\]
of the target value of the neural network GT flux as the solution of a constrained optimization problem, i.e. the projection of the flux to the convex hull of the dissipative low order and non-dissipative high-order flux. This formulation is usable in single conservation laws as well as for systems of conservation laws and extends also to more general convex combinations if additional fluxes are added for the usage in convex combinations. A different norm or different convex functional could be used instead, investigation in such direction will be open for the future.
%\PO{ One could also argue about the used norm and debate whether a different convex functional could be employed.} \todo{Warum ies ist nicht zielführen... Man kann sagen dass man auch andere normen verwenden kann, aber das eventuell in der Zukunft untersucht wird, so ist es aber nur ablenkend!} \SK{Können wir gerne entfernen. }
 An additional complexity stems from the fact while that the above minimization problem always has a unique solution as both the objective as also the domain is convex the situation is worse for the following related minimization problem
\[
	\alpha = \argmin_{\tilde \alpha \in [0, 1]} \norm{\fnn_{\tilde\alpha} - \fprec_{k+\frac 1 2}}_2.
\]
The solution is in fact not unique for $g = h$ which happens for example for $u = \mathrm{const.}$. We  make use of the following definition 
\[
	\alpha = \max\of{ \argmin_{\tilde \alpha \in [0, 1]} \norm{\fnn_{\tilde\alpha} - \fprec}_2}
\]
to select the most dissipative value of $\alpha$ in the degenerate case. The numerical solution of this problem in the case of the $2$-norm is based on the usage of the Penrose inverse 
\[
	b = \fprec - g, \quad A = h-g, \quad \beta = \min(1, \max(0, A^\dagger b)), \quad \alpha = 1- \beta,
\]
as a simple calculation shows. 
The affine-linear map \[M_{k+\frac 1 2}: \R \to \R^p, \beta \mapsto \beta h_{k+\frac 1 2} + (1-\beta) g_{k+\frac 1 2} = g_{k+\frac 1 2} + \alpha (h_{k+\frac 1 2} - g_{k+\frac 1 2}) = w + A\alpha\] can be expressed in the standard basis using the matrix $A_{k+\frac 1 2} = h_{k+\frac 1 2} - g_{k+\frac 1 2}$ and the support vector $w_{k+\frac 1 2} = g_{k+\frac 1 2}$. The value $\beta$ controls an affine combination, where $\beta = 1- \alpha$ yields the identical value as before using the blending scheme. One therefore finds 
\[
	\argmin \norm{w + A\beta - f^{n, precise}}_2 = \argmin \norm{A\beta - \underbrace{(f^{n, precise} - w)}_{b}}_2 = A^\dagger b
\]
for the projection of $f^{n, precise}$ onto the subspace $\ran{M}$.
% As the Penrose inverse is not only the least squares, but also the least norm solution is this value also has the smallest absolute value, i.e. $\beta = 0$, in the case that $A$ is degenerate. The distinction between $\alpha$ and $\beta$ was made to enforce $\alpha = 1$ in this case. As we are interested in the projection of $f^{n, precise}$ onto $M_{k+\frac 1 2}([0, 1])$ one concludes that if the unconstrained minimizer lies outside of the image of $[0, 1]$ under $M$ the constrained minimizer must be one of the edges and in fact the edge lying more near to the unconstrained minimizer, and this yields the given formula for the minimizer. 
As the Penrose inverse is not only the least squares, but also the least norm solution. It has also the smallest absolute value, i.e. $\beta = 0$, in the case that $A$ is degenerate. The distinction between $\alpha$ and $\beta$ was made to enforce $\alpha = 1$ for degenerate A. As we are interested in the projection of $f^{n, precise}$ onto $M_{k+\frac 1 2}([0, 1])$, one concludes that if the unconstrained minimizer lies outside of the image of $[0, 1]$ under $M$, the constrained minimizer must be one of the edges and in fact, the edge lying nearer to the unconstrained minimizer. This yields the given formula for the minimizer. 




