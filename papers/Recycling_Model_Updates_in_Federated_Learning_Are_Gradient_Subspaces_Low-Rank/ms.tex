
\documentclass{article}
\usepackage{iclr2022_conference,times}

\usepackage{hyperref}
\usepackage{url}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[most]{tcolorbox}
\usepackage{wrapfig}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}

\setlength{\ptcindent}{0pt}
\renewcommand \thepart{}
\renewcommand \partname{}

\NewDocumentCommand{\codeword}{v}{%
\texttt{\textcolor{blue}{#1}}%
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% user added commands
% comments
\newcommand{\ali}[1]{{\color{blue} {{\bf Ali}: #1}}}
\newcommand{\shams}[1]{{\color{red}{#1}}}
\newcommand{\cgb}[1]{{\color{black}{#1}}}
\newcommand{\cgbc}[1]{{\color{brown}{{\bf Chris:} #1}}}

\renewcommand{\ali}[1]{{\color{black} {{\bf Ali}: #1}}}
\renewcommand{\shams}[1]{{\color{black}{#1}}}
\renewcommand{\cgb}[1]{{\color{black}{#1}}}
\renewcommand{\cgbc}[1]{{\color{black}{{\bf Chris:} #1}}}

\usepackage{soul}
\setstcolor{red}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\tcbset{
    colback=yellow!10!white, colframe=black!50!black,
    highlight math style= {
        enhanced,
        colframe=black,colback=red!10!white,boxsep=0pt
    }
}

\newcommand{\CBox}[1]{\begin{tcolorbox}[ams align]#1\end{tcolorbox}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\fakesection}[1]{
  \par\refstepcounter{section}
  \sectionmark{#1}
  \addcontentsline{toc}{section}{\protect\numberline{\thesection}#1}
}
\newcommand{\fakesubsection}[1]{
  \par\refstepcounter{subsection}
  \subsectionmark{#1}
  \addcontentsline{toc}{subsection}{\protect\numberline{\thesubsection}#1} ToC
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% theorems and lemmas
\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{assume}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{corollary}{Corollary}
\renewcommand{\qedsymbol}{\rule{0.7em}{0.7em}}

% mathematical macros
\newcommand{\E}[2]{\mathbb{E}_{#2}\left[#1\right]}
\newcommand{\sE}[2]{\mathbb{E}_{#2}\hspace{-1mm}\left[#1\right]}
\newcommand{\inE}[2]{\mathbb{E}_{#2}[#1]}
\newcommand{\R}[1]{\mathbb{R}^{#1}}
\renewcommand{\vec}[3]{\boldsymbol{#1}_{#2}^{#3}}
\newcommand{\tvec}[3]{\widetilde{\boldsymbol{#1}}_{#2}^{#3}}
\newcommand{\subsup}[3]{#1_{#2}^{#3}}
\newcommand{\mset}[3]{\left\{#1\right\}_{#2}^{#3}}
\newcommand{\norm}[2]{\left\Vert#1\right\Vert_{#2}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\innorm}[2]{\Vert #1\Vert_{#2}}
\newcommand{\dotp}[2]{\left\langle #1, #2 \right\rangle}
\newcommand{\indotp}[2]{\langle #1, #2 \rangle}
\newcommand{\aligneqn}[1]{{\begin{align}#1\end{align}}}
\newcommand{\eqn}[1]{{\begin{equation}#1\end{equation}}}
\newcommand{\ineqn}[1]{$#1$}
\newcommand{\bigo}[1]{\mathcal{O}\left( #1 \right)}
\newcommand{\sbigo}[1]{\mathcal{O}\hspace{-1mm}\left( #1 \right)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}
\renewcommand{\thefootnote}{\textbf{\textcolor{blue}{\fnsymbol{footnote}}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\algName}{{\tt LBGM}}
\newcommand{\algFullName}{Look-back Gradient Multiplier}
\title{Recycling Model Updates in Federated \\Learning: Are Gradient Subspaces Low-Rank?}

\author{Sheikh Shams Azam, Seyyedali Hosseinalipour, Qiang Qiu, Christopher Brinton\\
School of ECE, Purdue University\\
\texttt{\{azam1, hosseina, qqiu, cgb\}@purdue.edu} \\
}

\iclrfinalcopy
\begin{document}

\maketitle
\doparttoc
\faketableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-2mm}
\begin{abstract}
  In this paper, we question the rationale behind propagating large numbers of parameters through a distributed system during federated learning. We start by examining the rank characteristics of the subspace spanned by gradients \shams{across epochs} (i.e., the gradient-space) in centralized model training, and observe that \shams{this} gradient-space often consists of a few leading principal components accounting for an overwhelming majority ($95-99\%$) of the explained variance. Motivated by this, we propose the "Look-back Gradient Multiplier" ({\tt LBGM}) algorithm, which \shams{exploits} this low-rank property \shams{to enable gradient recycling} between model update rounds \shams{of federated learning, reducing transmissions of large parameters to single scalars for aggregation}. We analytically characterize the convergence behavior of {\tt LBGM}, revealing the nature of the trade-off between communication savings and model performance. Our subsequent experimental results demonstrate the improvement {\tt LBGM} obtains in communication overhead compared to \shams{conventional }federated learning \shams{on several datasets and deep learning models}. Additionally, we show that {\tt LBGM} is a general plug-and-play algorithm that can be used standalone or stacked on top of existing sparsification techniques for distributed model training.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\vspace{-1mm}

Federated Learning (FL) \citep{konevcny2016federated} has emerged as a popular  distributed machine learning (ML) paradigm. By having each device conduct local model updates, FL substitutes raw data transmissions with model parameter transmissions,
promoting data privacy \citep{shokri2015privacy, azam2020towards} and  communication savings \citep{wang2020tackling}.
At the same time, overparameterized neural networks (NN) are becoming ubiquitous in the ML models trained by FL, e.g., in computer vision \citep{
liu2016ssd, huang2017densely} and natural language processing \citep{brown2020language, 
liu2019roberta}. While NNs can have parameters in the range of a few million (VGG \citep{simonyan2014very}, ResNet \citep{he2016deep}) to several billions (GPT-3 \citep{brown2020language}, Turing NLG \citep{turing2020nlg}), prior works have demonstrated that a majority of these parameters are often irrelevant \citep{frankle2018lottery, liu2018rethinking, han2015learning, li2016pruning} in optimization and inference. This presents an opportunity to reduce communication overhead by transmitting lighter representations of the model, conventionally achieved through compression/sparsification techniques \citep{wang2018atomo, alistarh2016qsgd, vogels2019powersgd}. 

In this work, we investigate the ``overparameterization'' of NN training, through the lens of rank characteristics of the subspace spanned by the gradients (i.e., the gradient-space) generated during stochastic gradient descent (SGD).
\shams{We start with the} fundamental question: \shams{\textit{can we observe the effect of overparameterization in NN optimization directly through the principal components analysis (PCA) of the gradients generated during SGD-based training?}} And if so: \textit{can this property be used to reduce communication overhead in FL?} Our main hypothesis is that
\begin{equation}
    \textbf{the subspaces spanned by gradients generated \cgb{across SGD epochs} are low-rank.}
    \label{hypothesis}
    \tag{\bf H1}
\end{equation}
This leads us to propose a technique that instead of propagating a million/billion-dimensional vector (i.e., gradient) over the system in each iteration of FL only requires propagating a single scalar in the majority of the iterations. Our algorithm introduces a new class of techniques based on the concept of reusing/recycling device gradients over time. Our main contributions can be summarized as follows:
\vspace{-2mm}
\begin{itemize}[leftmargin=5mm]
    \item \shams{We demonstrate the low-rank characteristics of the gradient-space by directly studying its principal components for several NN models trained on various real-world datasets.} We show that \shams{principal gradient directions (i.e., directions of principal components of the gradient-space)} can be approximated in terms of actual gradients generated during the model training process.
    \vspace{-0.8mm}
    \item Our insights lead us to develop the ``Look-back Gradient Multiplier'' ({\algName}) algorithm to \shams{significantly reduce the communication overhead in FL}. {\algName} recycles \shams{previously transmitted gradients} to represent the newly-generated gradients at each device with a single scalar.
    We further analytically investigate the convergence characteristics of this algorithm.
    \vspace{-0.8mm}
    \item Our experiments show the communication savings obtained via {\algName} in FL both as a standalone solution and a plug-and-play method used with other gradient compression techniques, e.g., top-K. We further reveal that {\algName} can be extended to distributed training, e.g., {\algName} with SignSGD \citep{bernstein2018signsgd} substantially reduces communication overhead in multi-GPU systems.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-3mm}
\section{A Gradient-Space Odyssey} 
\label{sec:explore}
\vspace{-3mm}
We first \shams{start by directly studying the principal component analysis (PCA) of} the gradient-space of overparameterized NNs. Given a centralized ML training task (e.g., classification, regression, segmentation), we exploit principal component analysis (PCA) \citep{pearson1901liii} to answer the following: \textit{how many principal components explain the $99\%$ and $95\%$ variance (termed \textsc{n99-pca} and \textsc{n95-pca}, respectively; together \textsc{n-pca}) of all the gradients generated during model training?} 

\begin{figure}[t]
\centering
\begin{minipage}{.495\textwidth}
  \centering
  \centerline{\includegraphics[width=1.0\textwidth]{images/prelim_1/prelim_1_cifar_128.pdf}}
\end{minipage}
\begin{minipage}{.495\textwidth}
  \centering
  \centerline{\includegraphics[width=1.0\textwidth]{images/prelim_1/prelim_1_celeba_256.pdf}}
\end{minipage}
    \vspace{-4mm}
  \caption{\small{\textit{PCA components progression}. The top row shows the number of components that account for 99\% (N99-PCA in blue) and 95\% (N95-PCA in red) explained variance of all the gradients generated during gradient descent epochs. The bottom row shows the performance of the model on the test data. The results are presented for: (i) CIFAR-10 classification (left 4 columns), and (ii) CelebA regression (right 4 columns).}}
  \label{fig:prelim_1}
   \vspace{-4.5mm}
\end{figure}

We start with 4 different NN architectures: (i) fully-connected neural network (FCN), (ii) convolutional neural network (CNN), (iii) ResNet18 \citep{he2016deep}, and (iv) VGG19 \citep{simonyan2014very}; trained on 2 datasets:  CIFAR-10 \citep{krizhevsky2009learning} and CelebA \citep{liu2015faceattributes}, with classification and regression tasks, respectively. We then compute the \textsc{n-pca} for each epoch by applying PCA on the set of gradients accumulated until that epoch (pseudo-code in Algorithm~\ref{alg:pseudo} in \textbf{Appendix~\ref{app:pseudo}}). The results depicted in Fig.~\ref{fig:prelim_1} agree with our hypothesis \eqref{hypothesis}: both \textsc{n99-pca} and \textsc{n95-pca} are significantly lower than that the total number of gradients generated during model training, e.g., in Fig.~\ref{fig:prelim_1}\footnotemark[1]\footnotetext[1]{\shams{The addition of a learning rate scheduler (e.g., cosine annealing scheduler~\citep{loshchilov2016sgdr}) has an effect on the PCA of the gradient-space. Careful investigation of this phenomenon is left to future work.
}} 
the number of principal components (red and blue lines in top plots) are substantially lower (often as low as 10\% of number of epochs, i.e., gradients generated) for both datasets.
In \textbf{Appendix~\ref{app:prelim_expt_1}}, we further find that \eqref{hypothesis} holds in our experiments using several \shams{additional} datasets: CIFAR-100 \citep{krizhevsky2009learning}, MNIST \citep{lecun2010mnist}, FMNIST \citep{xiao2017fmnist}, CelebA \citep{liu2015faceattributes}, PascalVOC \citep{everingham2010pascal}, COCO \citep{lin2014microsoft}; models: U-Net \citep{ronneberger2015u}, SVM \citep{cortes1995support}; and tasks: segmentation and regression. Note that (especially on CIFAR-10) variations in \textsc{n-pca} across models are not necessarily related to the model performance (CNN performs almost as well as ResNet18 but has much lower \textsc{n-pca}; Fig.~\ref{fig:prelim_1} -- columns 2 \& 3) or complexity (CNN has more parameters than FCN but has lower \textsc{n-pca}; Fig.~\ref{fig:prelim_1} -- columns 1 \& 2). The fact that rank deficiency of the gradient-space is not a consequence of model complexity or performance suggests that the gradient-space of state-of-the-art large-scale ML models could be represented using a few \cgb{principal gradient directions~(PGD)}.

\textbf{N-PCA and FL.} In an ``ideal'' FL framework, if both the server and the workers/devices have the~\cgb{PGDs}, then the newly generated gradients can be transmitted by sharing their projections on the~\cgb{PGDs}, i.e., the ~\cgb{PGD} multipliers (PGM). PGMs and \cgb{PGDs} can together be used to reconstruct the device generated gradients at the server, \shams{dramatically reducing communication costs.} However, this setting is impractical since: (i) it is infeasible to obtain the~\cgb{PGDs} prior to the training, and (ii) PCA is computationally intensive. 
We thus look for an efficient online approximation of the~\cgb{PGDs}.

\begin{figure}[t]
\centering
\begin{minipage}{.495\textwidth}
  \centering
  \centerline{\includegraphics[width=1.0\textwidth]{images/prelim_2/prelim_2_cifar_cnn_128.png}}
\end{minipage}
\begin{minipage}{.495\textwidth}
  \centering
  \centerline{\includegraphics[width=1.0\textwidth]{images/prelim_2/prelim_2_celeba_cnn_256.png}}
\end{minipage}
    \vspace{-4mm}
  \caption{\small{\textit{Overlap of actual and principal gradients}. The heatmap shows the pairwise cosine similarity \shams{between} actual (epoch) gradients and \shams{principal gradient directions (PCA gradients)}. Epoch gradients have a substantial overlap with one or more PCA gradients and consecutive epoch gradients show a gradual variation.
  \shams{This suggest that there may exist a high overlap between the gradients generated during the NN model training.}
The results are shown for a CNN classifier trained on CIFAR-10 (left 4 columns) and CelebA (right 4 columns) datasets. Each subplot is marked with \#L, the layer number of the CNN and \#elem, the number of elements in each layer.}}
  \label{fig:prelim_2}
      \vspace{-2.5mm}
\end{figure}

\textbf{Overlap of Actual Gradients and N-PCA.} To approximate~\cgb{PGDs}, we exploit an observation made in Fig.~\ref{fig:prelim_1} (and further in Appendix~\ref{app:prelim_expt_1}): \textsc{n-pca} mostly remains constant over time, suggesting that the gradients change gradually across SGD epochs.
To further investigate this, in Fig.~\ref{fig:prelim_2} (and further in Appendix~\ref{app:prelim_expt_2}) we plot the cosine similarity between~\cgb{PGDs} and actual gradients as a heatmap.\footnotemark[2] 
 We observe that (i) the similarity of actual gradients to~\cgb{PGDs} varies gradually over time, and (ii) 
 actual gradients have a high cosine similarity with one or more~\cgb{PGDs}. This leads to our second hypothesis:
 \vspace{-.3mm}
\begin{equation}
    \textbf{\cgb{PGDs} can be approximated using a subset of gradients \cgb{generated across SGD epochs}}.
    \label{hypothesis_2}
    \tag{\bf H2}
     \vspace{-.3mm}
\end{equation}
\textbf{Look-back Gradients.} 
Our observations above suggest a significant overlap among consecutive gradients generated during SGD epochs. This is further verified in Fig.~\ref{fig:prelim_3} (and in Appendix~\ref{app:prelim_expt_3}\footnotemark[3]), where we plot the pairwise cosine similarity of consecutive gradients generated during SGD epochs.\footnotemark[2] For example, consider the boxes marked B1, B2, and B3 in layer 1 (L\#1 in Fig.~\ref{fig:prelim_3}). Gradients in each box can be used to approximate other gradients within the box.
Also, interestingly, the number of such boxes that can be drawn is correlated with the corresponding number of~\cgb{PGDs}. Based on \eqref{hypothesis_2}, we next propose our \textit{Look-back Gradient Multiplier} (\algName) algorithm that utilizes a subset of actual gradients, termed ``look-back gradients'', to reuse/recycle gradients transmitted in FL.

\footnotetext[2]{Refer to Algorithm~\ref{alg:pseudo} in Appendix~\ref{app:pseudo} for the detailed pseudocode.}
\footnotetext[3]{\shams{2  of 24 experiments (Fig.~\ref{fig:prelim_3_mnist_vgg19}\&\ref{fig:prelim_3_mnist_resnet18} in Appendix~\ref{app:prelim_expt_3}) show inconsistent gradient overlaps. However, our algorithm discussed in Sec.~\ref{sec:method} still performs well on those datasets and models (see Fig.~\ref{fig:standalone_resnet18} in Appendix~\ref{app:prelim_expt_3}).}}

\begin{figure}[t]
\centering
\begin{minipage}{.495\textwidth}
  \centering
  \centerline{\includegraphics[width=1.0\textwidth]{images/prelim_3/prelim_3_cifar_cnn_128_box.png}}
\end{minipage}
\begin{minipage}{.495\textwidth}
  \centering
  \vspace{2mm}
  \centerline{\includegraphics[width=1.0\textwidth]{images/prelim_3/prelim_3_celeba_cnn_256_box.png}}
\end{minipage}
    \vspace{-4mm}
  \caption{\small{\textit{Similarity among consecutive gradients}. The cosine similarity  of consecutive gradients reveals a gradual change in directions of gradients over epochs. 
Thus,
  the newly generated gradients can be represented \shams{in terms of the previously generated gradients} with low approximation error. \shams{Reusing/recycling gradients can thus lead to significant communication savings during SGD-based  federated optimization.}
}}
  \label{fig:prelim_3}
    \vspace{-2mm}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-2.5mm}
\section{{\algFullName} Methodology}
\label{sec:method}
\vspace{-1.5mm}

Federated Learning (FL) considers a system of $K$ workers/devices indexed $1,...,K$, as shown in Fig.~\ref{fig:approx_fedl}. Each worker $k$ possesses a local dataset $\mathcal{D}_k$ with $n_k = |\mathcal{D}_k|$ datapoints. The goal of the system is to minimize the global loss function $F(\cdot)$ expressed through the following problem:
\eqn{
    \min_{\vec{\theta}{}{} \in \mathbb{R}^{M}}  F(\vec{\theta}{}{}) \triangleq \sum_{k=1}^K\omega_k F_k(\vec{\theta}{}{}){}{},
    \label{eqn:opt_obj}
}

\noindent where $M$ is the dimension of the model $\vec{\theta}{}{}$, $\omega_k=n_k/N$, $N=\sum_{k=1}^K n_k$, and $F_k(\vec{\theta}{}{}) = \sum_{d \in \mathcal{D}_k}f_k(\vec{\theta}{}{};d)/n_k$ is the local loss at worker $k$, with $f_k(\vec{\theta}{}{};d)$ denoting the loss function for data sample $d$ given parameter vector $\vec{\theta}{}{}$. FL tackles \eqref{eqn:opt_obj} via engaging the workers in local SGD model training on their own datasets. The local models are periodically transferred to and aggregated at the main server after $\tau$ local updates, forming a global model that is used to synchronize the workers before starting the next round of local model training.

\shams{At the start of round $t$, each model parameter $\vec{\theta}{k}{(t,0)}$ is initialized with the global model $\vec{\theta}{}{(t)}$. Thereafter,} worker $k$ updates its parameters $\vec{\theta}{k}{(t,b)}$ as: $\vec{\theta}{k}{(t, b+1)} \hspace{-0.5mm}\leftarrow\hspace{-0.5mm} \vec{\theta}{k}{(t, b)} - \eta \vec{g}{k}{}(\vec{\theta}{k}{(t, b)})$, where $ \vec{g}{k}{}(\vec{\theta}{k}{(t, b)})$ is the stochastic gradient at local step $b$, and $\eta$ is the step size. During a vanilla FL aggregation, the global model parameters are updated as $\vec{\theta}{}{(t+1)} \leftarrow \vec{\theta}{}{(t)} -  \eta \sum_{k=1}^K\omega_k\vec{g}{k}{(t)}$, where $\vec{g}{k}{(t)} = \sum_{b=0}^{\tau-1}  \vec{g}{k}{}(\vec{\theta}{k}{(t, b)})$ is the accumulated stochastic gradient (ASG) at worker $k$. \cgb{More generally, this aggregation may be conducted over a subset of workers at time $t$. 
} We define $\nabla \subsup{F}{k}{}(\vec{\theta}{k}{(t)}) = \sum_{b=0}^{\tau-1}  \nabla F_k(\vec{\theta}{k}{(t,b)})$ and $\vec{d}{k}{(t)} = \vec{g}{k}{(t)}/\tau$ as the corresponding accumulated true gradient and normalized ASG, respectively. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Indexing and Notations.}  In \textit{superscripts with parenthesis} expressed as tuples, the first element denotes the global aggregation round while the second element denotes the local update round, e.g., $\vec{g}{k}{(t,b)}$ is the gradient at worker $k$ at global aggregation round $t$ at local update $b$.  \textit{Superscripts without parenthesis} denote the index for look-back gradients, e.g., $\ell$ in $\subsup{\rho}{k}{(t),\ell}$ defined below.

\textbf{{{\algName}} Algorithm.}  {\algName} (see Fig.~\ref{fig:approx_fedl}) consists of three main steps: (i) workers initialize and propagate their look-back gradients (LBGs) to the server; (ii) workers estimate their look-back coefficients (LBCs), i.e., the scalar projection of subsequent ASGs on their LBGs, and the look-back phase (LBP), i.e., the angle between the ASG and the LBG; and (iii) workers update their LBGs and propagate them to the server if the LBP passes a threshold, otherwise they only transmit the scalar LBC. Thus, {\algName} propagates only a subset of actual gradients generated at the devices to the server. The intermediate global aggregation steps between two LBG propagation rounds only involve transfer of a \textit{single scalar}, i.e., the LBC, from each worker, instead of the entire ASG vector.

In {\algName}, the local model training is conducted in the same way as vanilla FL, while model aggregations at the server are conducted via the following rule:
\eqn{
    \vec{\theta}{}{(t+1)} = \vec{\theta}{}{(t)} - \eta\,\sum_{k=1}^K\omega_k \tvec{g}{k}{(t)},
    \label{eqn:lbgm_update}
    \vspace{-2mm}
}

\noindent where $\tvec{g}{k}{(t)}$ is the approximation of worker $k$'s accumulated stochastic gradient, given by Definition~\ref{lem:grad_proj}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \centering
  \centerline{\includegraphics[width=.95\textwidth]{images/approx_fedl_v5.pdf}}
  \vskip -0.15in
  \caption{\small{\textit{Look-back gradient multiplier}. (a) The Look-back Coefficients (LBCs) are the projection of accumulated stochastic gradients at the workers on their Look-back Gradients (LBGs). (b) Scalar LBCs, i.e., the $\subsup{\rho}{k}{(t),\ell}$, are transmitted to the server. (c) LBG-based gradient approximations are reconstructed at the server.}}
  \label{fig:approx_fedl}
  \vspace{-3mm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% gradient projection lemma
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{defn}
\label{lem:grad_proj}
\textbf{(Gradient Approximation in {\algName})} Given the ASG $\vec{g}{k}{(t)}$, and the LBG $\vec{g}{k}{\ell}$, the gradient approximation $\tvec{g}{k}{(t)}$ recovered by the server is given by:
\eqn{
    \vspace{-1.1mm}
    \tvec{g}{k}{(t)} = \subsup{\rho}{k}{(t),\ell} \vec{g}{k}{\ell} \; \text{ for } \; \norm{\subsup{\rho}{k}{(t),\ell}\vec{d}{k}{\ell}}{} = \norm{\vec{d}{k}{(t)} \cos(\subsup{\alpha}{k}{(t),\ell})}{},
    \label{eqn:grad_proj}
    \tag{\bf D1}
    \vspace{-2mm}
}

\noindent where LBC $\subsup{\rho}{k}{(t),\ell} = { \indotp{ \vec{g}{k}{(t)} }{ \vec{g}{k}{\ell}} }\big/{ \innorm{\vec{g}{k}{\ell}}{}^2}$ is the projection of the accumulated gradient $\vec{g}{k}{(t)}$ on the LBG $\vec{g}{k}{\ell}$, and LBP {$\subsup{\alpha}{k}{(t),\ell}$ denotes the angle between $\vec{g}{k}{(t)}$ and $\vec{g}{k}{\ell}$} (see Fig.~\ref{fig:approx_fedl}(a)).
\end{defn}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\algName} initializes the LBGs $\vec{g}{k}{\ell}$ with the first actual gradients propagated by the devices at $t=1$.
For the subsequent aggregations, each worker $k$ shares only its LBC $\subsup{\rho}{k}{(t),\ell}$ with the server if the value of LBP error $\sin^2(\subsup{\alpha}{k}{(t),\ell})$ is below a threshold, i.e., $\sin^2(\subsup{\alpha}{k}{(t),\ell}) \leq \subsup{\delta}{k}{\mathsf{threshold}}$, where $\subsup{\delta}{k}{\mathsf{threshold}}\in[0,1]$ is a tunable parameter; otherwise it updates the LBG via transmitting the entire gradient to the server.

The details of {\algName} are given in Algorithm~\ref{alg:algo_training}. 
We next conduct convergence analysis for \algName.

\begin{algorithm}[t]
{\footnotesize 
  \caption{\algName: \algFullName}
\label{alg:algo_training}
\begin{algorithmic}[1]
    \Statex \textbf{\underline{Notation:}}
    \Statex $\vec{\theta}{}{(t)}$: global model parameter at global aggregation round $t$.
    \Statex $\vec{\theta}{k}{(t, b)}$: model parameter at worker $k$, at global aggregation round $t$ and local update $b$.
    \Statex $\vec{g}{k}{(t)}$: accumulated gradient at worker $k$ at global aggregation round $t$.
    \Statex $\vec{g}{k}{\ell}$: last full gradient transmitted to server, termed look-back gradient (LBG).
    \Statex $\subsup{\alpha}{k}{(t), \ell}$: phase between the accumulated gradient $\vec{g}{k}{(t)}$ and LBG $\vec{g}{k}{\ell}$, termed look-back phase (LBP).
    %
    \Statex \textbf{\underline{Training at worker $k$:}}
    \State Update local parameters: $\vec{\theta}{k}{(t, 0)} \leftarrow \vec{\theta}{}{(t)}$, and initialize gradient accumulator: $\vec{g}{k}{(t)} \leftarrow \vec{0}{}{}$.
    \For{$b = 0$ to ($\tau$-1)}        
        \State Sample a minibatch of  datapoints $\mathcal{B}_k$ from  $\mathcal{D}_{k}$ and compute $\vec{g}{k}{(t, b)}= \sum_{d\in \mathcal{B}_k} \nabla f_k(\vec{\theta}{k}{(t, b)};d)/|\mathcal{B}_k|$.
        
        \State Update local parameters: $\vec{\theta}{k}{(t, b+1)} \leftarrow \vec{\theta}{k}{(t, b)} - \eta \cdot \vec{g}{k}{(t, b)}$, and accumulate gradient: $\vec{g}{k}{(t)} \leftarrow \vec{g}{k}{(t)} + \vec{g}{k}{(t,b)}$.
    \EndFor
        \State Calculate the LBP error:  $ \sin^2(\subsup{\alpha}{k}{(t), \ell}) = 1 - \left( \indotp{ \vec{g}{k}{(t)} }{ \vec{g}{k}{\ell}}  \big/ \big(\innorm{ \vec{g}{k}{(t)} }{} \times \innorm{ \vec{g}{k}{\ell} }{} \big) \right)^2$ \label{line:lbp}
    \If{ $\sin^2(\subsup{\alpha}{k}{(t), \ell}) \leq \subsup{\delta}{k}{\mathsf{threshold}}$ } \label{line:lbp_cond} \Comment{checking the LBP error}
        \State Send scalar LBC to the server: $\vec{\mu}{k}{(t)} \leftarrow \subsup{\rho}{k}{(t),\ell} = \indotp{ \vec{g}{k}{(t)} }{ \vec{g}{k}{\ell} }/\innorm{ \vec{g}{k}{\ell} }{}^2$.
    \Else \Comment{updating the LBG}
        \State Send actual gradient to the server: $\vec{\mu}{k}{(t)} \leftarrow \vec{g}{k}{(t)}$. \label{line:update1}
        \State Update worker-copy of LBG: $\vec{g}{k}{\ell} \leftarrow \vec{g}{k}{(t)}$. \label{line:update2}
    \EndIf
    \Statex \textbf{\underline{Global update at the aggregation server:}}
    \State Initialize global parameter $\vec{\theta}{}{(0)}$ and broadcast it across workers.
    \For{$t = 0$ to $(T-1)$}
        \State Receive updates from workers $\{ \vec{\mu}{k}{(t)} \}_{k=1}^{K}$.
        \State Update global parameters: $\vec{\theta}{}{(t+1)} \leftarrow \vec{\theta}{}{(t)} - \eta \sum_{k=1}^{K}\omega_k \left[ s_k \cdot  \vec{\mu}{k}{(t)} \cdot \vec{g}{k}{\ell} + (1-s_k)\cdot \vec{\mu}{k}{(t)}\right]$, 
        \Statex ~~~~~~where $s_k$ is an indicator function given by, $s_k = \begin{cases} 1, \quad \text{if } 
        % |\vec{\mu}{k}{(t)}| = 1\text{, i.e.,  }
        \vec{\mu}{k}{(t)}\text{ is a scalar} \\
        0, \quad \text{otherwise, i.e., if }\vec{\mu}{k}{(t)}\text{ is a vector} \end{cases}$.
        \State Update server-copy of LBGs: $ \vec{g}{k}{\ell} \leftarrow (1-s_k) \vec{\mu}{k}{(t)} + (s_k) \vec{g}{k}{\ell },~~\forall k$.
    \EndFor
\end{algorithmic}
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% assumptions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Assumptions:} It is presumed that the local loss functions are bounded below: $\min_{\vec{\theta}{}{}\in\mathbb{R}^{M}} F_k(\vec{\theta}{}{}) > -\infty$, $\forall k$. Let {\small$\Vert\cdot\Vert$} denote the 2-norm. \shams{Our analysis uses} the following \shams{standard} assumptions \citep{wang2020tackling, friedlander2012hybrid, hosseinalipour2020multi, li2019convergence, stich2018local}:
\begin{enumerate}[leftmargin=5mm]
    \vspace{-2.2mm}
    \item \textit{Smoothness of Local Loss Functions:} Local loss function $F_k: \R{M} \to \R{}$, $\forall k$, is $\beta$-smooth: 
    \eqn{
        \norm{ \nabla F_k(\vec{\theta}{x}{}) - \nabla F_k(\vec{\theta}{y}{}) }{} \leq \beta\norm{ \vec{\theta}{x}{}-\vec{\theta}{y}{} }{}, \\~~\forall \vec{\theta}{x}{}, \vec{\theta}{y}{} \in \R{M}.
        \label{eqn:lipschitz}
        \tag{\bf A1}
    }
    \vspace{-3.1mm}
    \item \textit{SGD Characteristics:} Local gradients $\vec{g}{k}{}(\vec{\theta}{}{})$, $\forall k$, estimated by SGD are unbiased estimators of the true gradients $\nabla F_k(\vec{\theta}{}{})$, and have a bounded variance $\sigma^2 \geq 0$; mathematically:
    \eqn{
        \E{ \,\vec{g}{k}{}(\vec{\theta}{}{})\, }{}= \nabla F_k(\vec{\theta}{}{}) \text{, and } \E{  \norm{\vec{g}{k}{}(\vec{\theta}{}{}) - \nabla F_k(\vec{\theta}{}{})}{}^2}{} \leq \sigma^2, ~~\forall \vec{\theta}{}{}\in \R{M}.
        \label{eqn:sgd_noise}
        \tag{\bf A2}
    }
    \vspace{-3.5mm}
    \item \textit{Bounded Dissimilarity of Local Loss Functions:} For any realization of weights $\mset{\omega_k \geq 0}{k=1}{K}$, where $\sum_{k=1}^{K}\omega_k = 1$, there exist non-negative constants $\Upsilon^2 \geq 1$ and $\Gamma^2\geq 0$ such that
    \vspace{-2mm}
    \eqn{
        \sum_{k=1}^{K} \omega_k \norm{\nabla F_k(\vec{\theta}{}{})}{}^2 \leq \Upsilon^2 \norm{ \sum_{k=1}^{K}\omega_k \nabla F_k(\vec{\theta}{}{}) }{}^2 + \Gamma^2,~~\forall \vec{\theta}{}{}\in \R{M}.
        \label{eqn:bbd_diversity}
        \tag{\bf A3}
        \vspace{-.1mm}
    }
\end{enumerate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Convergence of LBGM-FL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}
\label{thm:conv_char}
\textbf{(General Convergence Characteristic of {\algName})} 
    Assume \ref{eqn:lipschitz}, \ref{eqn:sgd_noise}, \ref{eqn:bbd_diversity}, and that $\eta\beta \leq \min\big\{ 1/(2\tau), 1/\big(\tau\sqrt{2(1+4\Upsilon^2)}\big) \big\}$.
    If the threshold value in step \ref{line:lbp_cond} of Algorithm~\ref{alg:algo_training} satisfies the condition $\delta^{\mathsf{threshold}}_k \leq \Delta^2/\innorm{\vec{d}{k}{(t)}}{}^2$, $\forall k$ 
    , where $\Delta^2\geq 0$ is a constant, then after $T$ rounds of global aggregations, the performance of {\algName} is characterized by the following upper bound:
    \vspace{-6mm}
    
{\small    
\aligneqn{
    \hspace{-3mm}\frac{1}{T}\hspace{-0.45mm} \sum_{t=0}^{T-1} \hspace{-0.45mm}\E{\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2}{} \hspace{-0.65mm} \leq \hspace{-0.65mm}\frac{8\hspace{-0.35mm}\big[\hspace{-0.45mm}F(\vec{\theta}{}{(0)})\hspace{-0.45mm} -\hspace{-0.45mm} F^\star\hspace{-0.45mm}\big]}{\eta\tau T}\hspace{-0.5mm} \hspace{-0.5mm}+ \hspace{-0.5mm} 16 \Delta^2 +\hspace{-0.5mm} 8\eta\beta\sigma^2  \hspace{-0.5mm}+ \hspace{-0.5mm} 5\eta^2\beta^2\sigma^2(\hspace{-0.2mm}\tau-1\hspace{-0.2mm}) \hspace{-0.55mm}+ \hspace{-0.55mm}20 \eta^2\beta^2 \Gamma^2 \tau(\hspace{-0.2mm}\tau-1\hspace{-0.2mm}).\hspace{-3mm}
    \label{eqn:thm_conv_char}
}}
\end{theorem}
\vspace{-3mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.05in}
\begin{proof} The proof is provided in Appendix~\ref{app:proof_1}.
\end{proof}
\vspace{-0.1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The condition on $\delta^{\mathsf{threshold}}_k$ and LBP error $\sin^2(\subsup{\alpha}{k}{(t), \ell})$ in the above theorem implies that to have a fixed bound in~\eqref{eqn:thm_conv_char}, for a fixed $\Delta^2$, a larger gradient norm $\innorm{\vec{d}{k}{(t)}}{}^2$ is associated with a tighter condition on the LBP error $\sin^2(\subsup{\alpha}{k}{(t), \ell})$ and  $\subsup{\delta}{k}{\mathsf{threshold}}$. This is intuitive because a larger gradient norm corresponds to a larger estimation error when the gradient is recovered at the server for a given LBP (see Fig.~\ref{fig:approx_fedl}). In practice, since the gradient norm $\innorm{\vec{d}{k}{(t)}}{}^2$ does not grow to infinity during model training, the condition on LBP error in Theorem~\ref{thm:conv_char}, i.e., $ \sin^2(\subsup{\alpha}{k}{(t), \ell}) \leq \Delta^2/\innorm{\vec{d}{k}{(t)}}{}^2$, can always be satisfied for any $\Delta^2\geq 0$, since transmitting actual gradients of worker $k$ makes $\subsup{\alpha}{k}{(t), \ell}=0$. Given the general convergence behavior in Theorem~\ref{thm:conv_char}, we next obtain a specific choice of step size and an upper bound on $\Delta^2$ for which {\algName} approaches a stationary point of the global loss function \eqref{eqn:opt_obj}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Convergence to Stationary Pt
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{1mm}
\begin{corollary}
\label{corr:stationary_conv}
 \textbf{(Convergence of {\algName} to a Stationary Point)} 
Assuming the conditions of Theorem~\ref{thm:conv_char}, if $\Delta^2 \leq \eta$, where $\eta=1/\sqrt{\tau T}$, then {\algName} converges to a stationary point of the global loss function, with the convergence bound characterized below:
\vspace{-6mm}

{\small
\aligneqn{
\hspace{-2mm}
    \hspace{-0.5mm}\frac{1}{T} \hspace{-0.5mm} \sum_{t=0}^{T-1} \hspace{-0.2mm} \E{\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2}{} \hspace{-0.5mm} \leq \hspace{-0.3mm} \sbigo{\hspace{-0.3mm}\hspace{-0.5mm}\frac{1}{\sqrt{\tau T}}\hspace{-0.5mm}\hspace{-0.3mm}} \hspace{-0.5mm} + \hspace{-0.3mm} \sbigo{\hspace{-0.3mm}\hspace{-0.5mm}\frac{\sigma^2}{\sqrt{\tau T}}\hspace{-0.5mm}\hspace{-0.3mm}} \hspace{-0.5mm}+\hspace{-0.3mm} \sbigo{\hspace{-0.3mm}\hspace{-0.5mm}\frac{1}{\sqrt{\tau T}}\hspace{-0.5mm}\hspace{-0.3mm}} \hspace{-0.5mm}+\hspace{-0.3mm} \sbigo{\hspace{-0.3mm}\hspace{-0.5mm}\frac{\sigma^2(\hspace{-0.2mm}\tau-1\hspace{-0.2mm})}{\tau T}\hspace{-0.5mm}\hspace{-0.3mm}} \hspace{-0.5mm}+\hspace{-0.3mm} \sbigo{\hspace{-0.3mm}\hspace{-0.5mm}\frac{ (\hspace{-0.2mm}\tau-1\hspace{-0.2mm})\Gamma^2}{ T}\hspace{-0.3mm}\hspace{-0.3mm}}\hspace{-0.3mm}.
}\hspace{-6mm}
}
\end{corollary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-3mm}
\begin{proof} The proof is provided in Appendix~\ref{app:corrolary_1}.
\end{proof}
\vspace{-0.1in}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Considering the definition of $\Delta^2$ in Theorem~\ref{thm:conv_char} and the condition imposed on it in Corollary~\ref{corr:stationary_conv}, the LBP error (i.e., $\sin^2(\subsup{\alpha}{k}{(t), \ell})$) should satisfy $\innorm{\vec{d}{k}{(t)}}{}^2 \sin^2(\subsup{\alpha}{k}{(t), \ell}) \leq \eta=1/\sqrt{\tau T}$ for {\algName} to reach a stationary point of the global loss. Since $\innorm{\vec{d}{k}{(t)}}{}^2 $ is bounded during the model training, this condition on the LBP error can always be satisfied by tuning the frequency of the full (actual) gradient transmission, i.e., updating the LBG as in lines \ref{line:update1}\&\ref{line:update2} of Algorithm~\ref{alg:algo_training}.
Specifically, since the correlation across consecutive gradients is high and drops as the gradients are sampled from distant epochs (see Fig.~\ref{fig:prelim_3}), a low value of the LBP error can be obtained via more frequent LBG transmissions to the server.

\textbf{Main Takeaways from Theorem~\ref{thm:conv_char}, Corollary~\ref{corr:stationary_conv}, and Algorithm~\ref{alg:algo_training}:} 
\vspace{-1mm}
\begin{enumerate}[leftmargin=5mm]
    \vspace{-0.25mm}
    \item \textit{Recovering Vanilla-FL Bound:} In~\eqref{eqn:thm_conv_char}, if the LBGs are always propagated by all the devices, we have $\subsup{\alpha}{k}{(t), \ell} = 0$, $\forall k$, and thus $\Delta^2 = 0$ satisfies the condition on the LBP error. Then,~\eqref{eqn:thm_conv_char} recovers the bound for vanilla FL \citep{wang2020tackling,stich2018local,wang2018cooperative}. 
    \vspace{-0.25mm}
    \item \textit{Recovering Centralized SGD Bound:} In~\eqref{eqn:thm_conv_char}, if the LBGs are always propagated by all the devices, i.e., $\Delta^2 = 0$, the local dataset sizes are equal, i.e., $w_k=1/K, \forall k$, and $\tau=1$, then \eqref{eqn:thm_conv_char} recovers the bound for centralized SGD, e.g., see \citet{friedlander2012hybrid}.
    \vspace{-0.25mm}
    \item \textit{Unifying Algorithm~\ref{alg:algo_training} and Theorem~\ref{thm:conv_char}:}\label{tk:3} The value of $\Delta^2$ in \eqref{eqn:thm_conv_char} is determined by the value of the LBP error $\sin^2(\subsup{\alpha}{k}{(t),\ell})$, which is also reflected in step \ref{line:lbp_cond} of Algorithm~\ref{alg:algo_training}. This suggests that the performance improves when the allowable threshold on $\sin^2(\subsup{\alpha}{k}{(t),\ell})$ is decreased (i.e., smaller $\Delta^2$), which is the motivation behind introducing the tunable threshold $\subsup{\delta}{k}{\mathsf{threshold}}$ in our algorithm.
    \vspace{-0.25mm}
    \item \textit{Effect of LPB Error on Convergence:} As the value of $\sin^2(\subsup{\alpha}{k}{(t),\ell})$ increases, the term in \eqref{eqn:thm_conv_char} containing $\Delta^2$ will start diverging (it can become the same order as the gradient $\innorm{\vec{d}{k}{(t)}}{}^2$). The condition in Corollary~\ref{corr:stationary_conv} on $\Delta^2$ avoids this scenario, achieving convergence to a stationary point. 
    \vspace{-0.3mm}
    \item \textit{Performance vs. Communication Overhead Trade-off:}\label{tk:5} Considering step \ref{line:lbp_cond} of Algorithm~\ref{alg:algo_training}, increasing the tolerable threshold on the LBP error increases the chance of transmitting a scalar (i.e., LBC) instead of the entire gradient to the server from each worker, leading to communication savings. However, as seen in Theorem~\ref{thm:conv_char} and the condition on $\Delta^2$ in Corollary~\ref{corr:stationary_conv}, the threshold on the LBP error cannot be increased arbitrarily since the {\algName} may show diverging behavior. 
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-3mm}
\section{Experiments}
\vspace{-2mm}
\label{sec:expt}

\textbf{Model Settings.} We run experiments on several NN models and datasets. Specifically, we consider: \textbf{S1:} CNN on FMNIST, MNIST, CelebA, and CIFAR-10, \textbf{S2:} FCN on FMNIST and MNIST, and \textbf{S3:} ResNet18 on FMNIST, MNIST, CelebA, CIFAR-10 and CIFAR-100 for both independently and identically distributed (iid) and non-iid data distributions. We present results of \textbf{S1} (on non-iid data) in this section and defer the rest (including \textbf{S1} on iid data and U-Net on PascalVOC) to Appendix~\ref{app:addl_lbgm_expt}. 


\begin{figure}[t]
\centering
\begin{minipage}{.48\textwidth}
  \centering
  \centerline{\includegraphics[width=1.0\textwidth]{images/acc_comm_curve/expt_1_standalone_main_non_iid_with_acc_comm_curve.pdf}}
  \vspace{-3.5mm}
  \caption{\small{\textit{{\algName} as a Standalone Algorithm}. Irrespective of the dataset/data configuration across workers, {\algName} consistently outperforms vanilla FL in terms of the total parameters shared (middle row) while achieving comparable accuracy (top row). The bottom row shows accuracy vs. \# parameters shared.}}
  \label{fig:standalone}
\vspace{-2mm}
\end{minipage}~~~
\begin{minipage}{.48\textwidth}
  \centering
  \centerline{\includegraphics[width=1.0\textwidth]{images/acc_comm_curve/expt_3_threshold_main_non_iid_with_acc_comm_curve.pdf}}
    \vspace{-3mm}
  \caption{\small{\textit{Effect of $\delta_k^{\mathsf{threshold}}$ on {\algName}}. As $\delta_k^{\mathsf{threshold}}$ decreases, the training may become unstable. For larger values of $\delta_k^{\mathsf{threshold}}$, {\algName} achieves communication benefits (middle row) while maintaining a performance identical to vanilla FL (top row). The bottom row shows accuracy vs. \# parameters shared.}}
  \label{fig:rho_effect}
\vspace{-3mm}
\end{minipage}
\vspace{-3mm}
\end{figure}

\vspace{-1mm}
\textbf{Properties Studied.} We specifically focus on four properties of {\algName}: \textbf{P1:} the benefits of gradient recycling by \textit{{\algName} as a standalone algorithm}, \textbf{P2:} the \textit{effect of $\subsup{\delta}{k}{\mathsf{threshold}}$} on {\algName} from Theorem~\ref{thm:conv_char}, \textbf{P3:} practical capabilities of \textit{{\algName} as a general plug-and-play algorithm} that can be stacked on top of other gradient compression techniques 
in FL training, and finally \textbf{P4:} generalizability of \textit{{\algName} to distributed learning} frameworks, e.g., multi-processor or multi-GPU ML systems.

\vspace{-1mm}
\textbf{Baselines.} For \textbf{P1} and \textbf{P2}, we compare {\algName} with vanilla FL. For \textbf{P3}, we stack {\algName} on top of top-K and ATOMO \citep{wang2018atomo}, two state-of-the-art techniques for sparsification and low-rank approximation-based gradient compression, respectively. For \textbf{P4}, we stack {\algName} on top of SignSGD \citep{bernstein2018signsgd}, a state-of-the-art method in gradient compression for distributed learning.

\vspace{-1mm}
\textbf{Implementation Details.} We consider an FL system consisting of 100 workers. We consider both the iid and non-iid data distributions among the workers. {Under the iid setting, each worker has training data from all the labels, while under the non-iid setting each worker has training data only from a subset of all labels (e.g., from 3 of 10 classes in MNIST/FMNIST).} The workers train with mini-batch sizes ranging from 128 to 512 based on the choice of dataset. We implement {\algName} with uniform $\subsup{\delta}{k}{\mathsf{threshold}}$ across workers. We also use error feedback \citep{karimireddy2019error} as standard only if top-K sparsification is used in the training. The FL system is simulated using PyTorch \citep{paszke2019pytorch} and PySyft \citep{ryffel2018generic} and trained on a 48GB Tesla-P100 GPU with 128GB RAM. All of our code and hyperparameters are available at \url{https://github.com/shams-sam/FedOptim}. \shams{Appendix~\ref{app:hyperparams} details the process of hyperparameter selection for the baselines.}


\vspace{-1mm}
\textbf{Complexity.} Compared to other gradient compression techniques, the processing overhead introduced by {\algName} is negligible. Considering Algorithm~\ref{alg:algo_training}, the calculation of LBCs and LBP errors involves inner products and division of scalars, while reconstruction of LBG-based gradient approximations at the server is no more expensive than the global aggregation step: since the global aggregation step requires averaging of local model parameters, it can be combined with gradient reconstruction. This also holds for {\algName} as a plug-and-play algorithm, as top-K and ATOMO \citep{wang2018atomo} introduce considerable computation overhead. In particular, {\algName} has $\mathcal{O}(M)$ complexity, where $M$ is the dimension of the NN parameter, which is inexpensive to plug on top of top-K ($\mathcal{O}(M\log M)$), ATOMO \citep{wang2018atomo} ($\mathcal{O}(M^2)$), and SignSGD \citep{bernstein2018signsgd} ($\mathcal{O}(M)$) methods. \shams{The corresponding space complexity of {\algName} for the server and devices is discussed in Appendix~\ref{app:storage}.} 


\vspace{-1mm}
\textbf{{\tt \algName} as a Standalone Algorithm.} We first evaluate the effect of gradient recycling by {\algName} in FL. Fig.~\ref{fig:standalone} depicts the accuracy/loss values (top row) and total floating point parameters transferred over the system (middle row) across training epochs for $\smash{\subsup{\delta}{k}{\text{threshold}} = 0.2,~ \forall k}$ on diferent datasets. The parameters transferred indicates the communication overhead, leading to a corresponding performance vs. efficiency tradeoff (bottom row). For each dataset, we observe that {\algName} reduces communication overhead on the order of $10^7$ floating point parameters per worker. \shams{Similar results on other datasets and NN models are deferred to Appendix~\ref{app:standalone_expt}. We also consider {\algName} under device sampling (see Algorithm~\ref{alg:sampling} in Appendix~\ref{ssec:sampling_algo}) and present the results in Appendix~\ref{app:sampling_expt}, which are qualitatively similar.}

\begin{figure}[t]
  \centering
      \centerline{\includegraphics[width=1.0\textwidth]{images/acc_comm_curve/expt_2_plugnplay_main_non_iid_with_acc_comm_curve.pdf}}
      \vspace{-4mm}
  \caption{\small{\textit{{\tt \algName} as a Plug-and-Play Algorithm}. {\algName} obtains substantial communication benefits when implemented on top of existing gradient compression techniques by exploiting the rank-characteristics of the gradient-space. Top-K and ATOMO are known to achieve state-of-the-art performance of their respective domains of sparsification and low-rank approximation respectively. 
  }}
  \label{fig:plugnplay}
  \vspace{-5mm}
\end{figure}

\vspace{-1mm}
\textbf{Effect of $\subsup{\delta}{k}{\mathsf{threshold}}$ on Accuracy vs. Communication Savings.} \shams{In Fig.~\ref{fig:standalone}, the drops in accuracy for the corresponding communication savings are small except for on CIFAR-10. The $14\%$ reduction in accuracy here is a result of the hyperparameter setting $\smash{\subsup{\delta}{k}{\text{threshold}} = 0.2}$. As noted in takeaway~\ref{tk:3} in Sec.~\ref{sec:method}, a decrease in the allowable threshold on the LBP error improves the accuracy; the effect of threshold value is controlled by changing $\subsup{\delta}{k}{\mathsf{threshold}}$ values in Algorithm~\ref{alg:algo_training}. Thus, we can improve the accuracy by lowering $\subsup{\delta}{k}{\text{threshold}}$: for $\subsup{\delta}{k}{\text{threshold}} = 0.05$, the accuracy drops by $4\%$ only while still retaining a communication saving of $55\%$, and for $\subsup{\delta}{k}{\text{threshold}} = 0.01$, we get a $22\%$ communication saving for a negligible drop in accuracy (by only $0.01\%$).} In Fig.~\ref{fig:rho_effect}, we analyze {\algName} under different $\subsup{\delta}{k}{\mathsf{threshold}}$ values for different datasets. A drop in model performance can be observed as we increase $\subsup{\delta}{k}{\mathsf{threshold}}$, which is accompanied by an increase in communication savings. This is consistent with takeaway \ref{tk:5} from Sec.~\ref{sec:method}, i.e., while a higher threshold requires less frequent updates of the LBGs, it reduces the convergence speed. Refer to Appendix~\ref{app:rho_effect} for additional results.

\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-16pt}
  \begin{center}
    \includegraphics[width=0.5\textwidth]{images/acc_comm_curve/expt_4_distributed_main_non_iid_with_acc_comm_curve.pdf}
  \end{center}
  \vspace{-16pt}
  \caption{\small{Application of {\algName} as a plug-and-play algorithm on top of SignSGD in distributed training.}}
  \label{fig:dist_training}  
  \vspace{-9pt}
\end{wrapfigure}

\vspace{-1mm}
\textbf{{\tt \algName} as a Plug-and-Play Algorithm.} For the plug-and-play setup, {\algName} follows the same steps as in Algorithm~\ref{alg:algo_training}, with the slight modification that the output of gradient compression techniques, top-K and ATOMO, are used in place of accumulated gradients $\vec{g}{k}{(t)}$ and LBGs $\vec{g}{k}{\ell}$, $\forall k$. In Fig.~\ref{fig:plugnplay}, we see that {\algName} adds on top of existing communication benefits of both top-K and ATOMO, on the order of $10^6$ and $10^5$ floating point parameters shared per worker, respectively \shams{($30-70\%$ savings across the datasets)}. \shams{The bottom row shows the accuracy/loss improvements that can be obtained for the same number of parameters transferred.} While top-K and ATOMO compress gradients through approximation, they do not alter the underlying low-rank characteristics of the gradient-space. {\algName} exploits this property to obtain substantial communication savings on top of these algorithms. Refer to Appendix~\ref{app:plugnplay_expt} for additional experiments.


\vspace{-1mm}
\textbf{Generalizability of {\algName} to Distributed Training.} {\algName} can be applied to more general distributed gradient computation settings, e.g., multi-core systems.
While \shams{heterogeneous (non-iid)} data distributions are not as much of a consideration in these settings as they are in FL (since data can be transferred/allocated across nodes), there is research interest in minimizing parameter exchange among nodes to reduce communication latency.
SignSGD \citep{bernstein2018signsgd} is known to reduce the communication requirements by several order of magnitude by converting floating-point parameters to sign bit communication. In Fig.~\ref{fig:dist_training}, we apply {\algName} as a plug-and-play addition on top of SignSGD and find that {\algName} further reduces the overall bits transferred by SignSGD on the order of $10^7$ bits \shams{($\smash{60-80\%}$ savings across the datasets)}. Refer to Appendix~\ref{app:general_expt} for additional experiments.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-4mm}
\section{Related Work}
\vspace{-2mm}

\vspace{-1.2mm}
\shams{\textbf{NN Overparameterization Analysis.} Several prior works on NN overparameterization have focused on Hessian-based analysis.} \citet{sagun2016eigenvalues, sagun2017empirical} divide the eigenspace of the Hessians into two parts: bulk 
and edges,
and show that increasing network complexity only affects the bulk component. \citet{ghorbani2019investigation} argues that the existence of large isolated eigenvalues in the Hessian eigenspace is correlated with slow convergence. \shams{\citet{gur2018gradient} studies the overlap of gradients with Hessians and shows the Hessian edge space remains invariant during training, and thus that SGD occurs in low-rank subspaces. \citet{gur2018gradient} also suggest that the edge space cardinality is equal to the number of classification classes, which does not align with our observations in Sec.~\ref{sec:explore}.
In contrast to these, our work explores the low-rank property by studying the PCA of the gradient-space directly. \cite{li2021low}, a contemporary of ours, employs the spectral decomposition of the NN gradient space to improve centralized SGD training time. Our methodology based on Hypothesis \eqref{hypothesis_2} is more suitable for FL since having the resource-constrained workers/devices execute spectral decomposition as a component of the training process would add significant computational burden.}


\vspace{-1.2mm}
\shams{The partitioning of the gradient subspace has also been observed in the domain of continual learning~\citep{chaudhry2020continual,saha2020gradient}. However, the subspace addressed in continual learning is the one spanned by gradient with respect to data samples for the final model, which is different than the subspace we consider, i.e., the subspace of gradient updates generated during SGD epochs.}

\vspace{-1.2mm}
\textbf{Gradient Compression.} Gradient compression techniques can be broadly categorized into (i) sparsification~\citep{wangni2017gradient,sattler2019robust}, (ii) quantization~\citep{seide20141,alistarh2016qsgd}, and (iii) low-rank approximations~\citep{wang2018atomo,vogels2019powersgd, albasyoni2020optimal,haddadpour2021federated}. Our work falls under the third category, where prior works have aimed to decompose large gradient matrices as an outer product of smaller matrices to reduce communication cost. This idea was also proposed in~\citet{konevcny2016federated}, one of the pioneering works in FL. \shams{While these prior works study the low-rank property in the context of gradient compression during a single gradient transfer step, 
our work explores the low rank property of the
gradients generated across successive gradient epochs during FL. Existing techniques for gradient compression can also benefit from employing {\algName} during FL, as we show in our experiments for top-K, ATOMO, and SignSGD.} 

\vspace{-1.2mm}
\textbf{Model Compression.} Model compression techniques have also been proposed to reduce NN complexity, e.g., model distillation \citep{ba2013deep, hinton2015distilling}, model pruning \citep{lecun1990optimal, hinton2015distilling}, 
and parameter clustering
\citep{son2018clustering,cho2021dkm}. 
\citep{li2020lotteryfl} extends the lottery ticket hypothesis to the FL setting. \shams{These methods have the potential to be employed in conjunction with LBGM to reduce the size of the LBGs stored at the server.}

\vspace{-1.2mm}
\textbf{FL Communication Efficiency.} Other techniques have focused on reducing the aggregation frequency in FL. \citet{hosseinalipour2020multi,lin2021two} use peer-to-peer local network communication, while SloMo \citep{wang2019slowmo} uses momentum to delay the global aggregations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-3mm}
\section{Discussion \& Conclusions}
\vspace{-2mm}
\shams{In this paper, we explored the effect of overparameterization in NN optimization through the PCA of the gradient-space, and employed this to optimize the accuracy vs. communication tradeoff in FL.}
We proposed the {\algName} algorithm, which uses \shams{our hypothesis that PGDs can be approximated using a subset of gradients generated across SGD epochs}, and recycles previously generated gradients at the devices to represent the newly generated gradients. {\algName} reduces communication overhead in FL by several orders of magnitude by replacing the transmission of gradient parameter vectors with a single scalars from each device. We theoretically characterized the convergence behavior of {\algName} algorithm and experimentally substantiated our claims \shams{on several datasets and models}. Furthermore, we showed that {\algName} can be extended to further reduce latency of communication in large distributed training systems by plugging {\algName} on top of other gradient compression techniques. More generally, our work gives a novel insight to designing a class of techniques based on ``Look-back Gradients'' that can be used in distributed machine learning systems \shams{to enhance communication savings}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\small
\bibliographystyle{iclr2022_conference}
\bibliography{ms}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\input{appendix}

\end{document}
