\newpage
\section{Additional Preliminary Experiments}
\label{app:addl_expt}

We next present the additional experiments performed to test hypotheses, \eqref{hypothesis} \& \eqref{hypothesis_2}. As mentioned in Section~\ref{sec:explore}, we study the rank-characteristics of centralized training using SGD on multiple datasets: FMNIST, MNIST, CIFAR-10, CelebA, COCO, and PascalVOC, and model architectures: CNN, FCN, Resnet18, VGG19, and U-Net. Section~\ref{app:prelim_expt_1} presents experiments complementary to the one presented in Fig.~\ref{fig:prelim_1}, while Section~\ref{app:prelim_expt_2} \& \ref{app:prelim_expt_3} present experiments complementary to those presented in Fig.~\ref{fig:prelim_2}, \& \ref{fig:prelim_3}, respectively. \textit{Please follow the hyperlinks for the ease of navigating through the figures.}

\subsection{PCA Component Progression}
\label{app:prelim_expt_1}

Together with Fig.~\ref{fig:prelim_1}, Figs.~\ref{fig:prelim_1_mnist}-\ref{fig:prelim_1_seg} show that both N99-PCA and N95-PCA are significantly lower than that the total number of gradients calculated during model training irrespective of model/dataset/learning task for multiple datasets and models which all agree with \eqref{hypothesis}. Specifically, the principal gradients (i.e., red and blue lines in the top row of the plots) are substantially lower (often as low as 10\% of number of epochs, i.e., gradients generated) in these experiments. Refer below for the details of the figures.

\begin{enumerate}[leftmargin=5mm]
    \item Fig.~\ref{fig:prelim_1_cifar100} repeats the experiment conducted in Fig.~\ref{fig:prelim_1} on CIFAR-100 using FCN, CNN, Resnet18, \& VGG19.
    \item Fig.~\ref{fig:prelim_1_mnist} repeats the experiment conducted in Fig.~\ref{fig:prelim_1} on MNIST using FCN, CNN, Resnet18, \& VGG19.
    \item Fig.~\ref{fig:prelim_1_fmnist} repeats the experiment conducted in Fig.~\ref{fig:prelim_1} on FMNIST using FCN, CNN, Resnet18, \& VGG19.
    \item Fig.~\ref{fig:prelim_1_svm} repeats the experiment conducted in Fig.~\ref{fig:prelim_1} on CIFAR-10, FMNIST, and MNIST using SVM, suggesting that we can use {\algName} for classic classifiers that are not necessary neural networks.
    \item Fig.~\ref{fig:prelim_1_seg} repeats the experiment conducted in Fig.~\ref{fig:prelim_1} on COCO, and PascalVOC using U-Net.
\end{enumerate}



\subsection{Overlap of Actual and Principal Gradient}
\label{app:prelim_expt_2}

Next, we perform experiments summarized in Fig.~\ref{fig:prelim_2_celeba_vgg19}-\ref{fig:prelim_2_coco_unet} to further validate our observation in Fig.~\ref{fig:prelim_2}: (i) cosine similarity of actual gradients with principal gradients varies gradually over time, and (ii) actual gradients have a high cosine similarity with one or more of the principal gradients. Refer below for the details of the figures. \textit{Each subplot is marked with \#L, the layer number of the neural network, and \#elem, the number of elements in each layer.}

\shams{In the plots with dense heatmaps (a large number of gradients in along x and y axis) for larger models such as Fig.~\ref{fig:prelim_2_celeba_vgg19}, it is harder to observe the overlap among the actual gradient and the PCA gradients. However, we can still notice the number of prinicipal components (along y axis) is substantially lower than the total number of epochs gradients (along y axis). A better picture of gradient overlap with other gradients can still be seen in corresponding inter-gradient overlap plot in Fig.~\ref{fig:prelim_3_celeba_vgg19}, which is consistent with the corresponding PCA progression shown in Fig.~\ref{fig:prelim_1}. Note that the lesser number of prinicipal gradient directions (e.g., CNN in Fig.~\ref{fig:prelim_1}) implies a higher overlap among the generated gradients (e.g., CNN in Fig.~\ref{fig:prelim_3}), while a larger number of PGDs (e.g., VGG19 in Fig.~\ref{fig:prelim_1}) implies a lower overlap among generated gradients (e.g., VGG19 in Fig.~\ref{fig:prelim_3_cifar_vgg19}).}

\begin{enumerate}[leftmargin=5mm]
    \item Fig.~\ref{fig:prelim_2_celeba_vgg19} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CelebA using VGG19.
    \item Fig.~\ref{fig:prelim_2_celeba_resnet18} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CelebA using Resnet18.
    \item Fig.~\ref{fig:prelim_2_celeba_fcn} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CelebA using FCN.
    \item Fig.~\ref{fig:prelim_2_celeba_cnn} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CelebA using CNN.
    
    \item Fig.~\ref{fig:prelim_2_cifar_vgg19} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CIFAR-10 using VGG19.
    \item Fig.~\ref{fig:prelim_2_cifar_resnet18} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CIFAR-10 using Resnet18.
    \item Fig.~\ref{fig:prelim_2_cifar_fcn} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CIFAR-10 using FCN.
    \item Fig.~\ref{fig:prelim_2_cifar_cnn} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CIFAR-10 using CNN.
    
    \item Fig.~\ref{fig:prelim_2_cifar100_vgg19} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CIFAR-100 using VGG19.
    \item Fig.~\ref{fig:prelim_2_cifar100_resnet18} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CIFAR-100 using Resnet18.
    \item Fig.~\ref{fig:prelim_2_cifar100_fcn} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CIFAR-100 using FCN.
    \item Fig.~\ref{fig:prelim_2_cifar100_cnn} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on CIFAR-100 using CNN.
    
    \item Fig.~\ref{fig:prelim_2_fmnist_vgg19} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on FMNIST using VGG19.
    \item Fig.~\ref{fig:prelim_2_fmnist_resnet18} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on FMNIST using Resnet18.
    \item Fig.~\ref{fig:prelim_2_fmnist_fcn} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on FMNIST using FCN.
    \item Fig.~\ref{fig:prelim_2_fmnist_cnn} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on FMNIST using CNN.
    
    \item Fig.~\ref{fig:prelim_2_mnist_vgg19} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on MNIST using VGG19.
    \item Fig.~\ref{fig:prelim_2_mnist_resnet18} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on MNIST using Resnet18.
    \item Fig.~\ref{fig:prelim_2_mnist_fcn} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on MNIST using FCN.
    \item Fig.~\ref{fig:prelim_2_mnist_cnn} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on MNIST using CNN.
    
    \item Fig.~\ref{fig:prelim_2_voc_unet} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on PascalVOC using U-Net.
    \item Fig.~\ref{fig:prelim_2_coco_unet} repeats the experiment conducted in Fig.~\ref{fig:prelim_2} on COCO using U-Net.
\end{enumerate}



\subsection{Similarity among Consecutive Generated Gradients}
\label{app:prelim_expt_3}

Furthermore, we perform experiments summarized in Fig.~\ref{fig:prelim_3_celeba_vgg19}-\ref{fig:prelim_3_coco_unet}. Together with Fig.~\ref{fig:prelim_3}, these experiments show that there is a significant overlap of consecutive gradients generated during SGD iterations, which further substantiates \eqref{hypothesis_2} and bolsters our main idea that \textit{``gradients transmitted in FL can be recycled/reused to represent the gradients generated in the subsequent iterations''}. Refer below for the details of the figures.

\begin{enumerate}[leftmargin=5mm]
    \item Fig.~\ref{fig:prelim_3_celeba_vgg19} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CelebA using VGG19.
    \item Fig.~\ref{fig:prelim_3_celeba_resnet18} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CelebA using Resnet18.
    \item Fig.~\ref{fig:prelim_3_celeba_fcn} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CelebA using FCN.
    \item Fig.~\ref{fig:prelim_3_celeba_cnn} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CelebA using CNN.
    
    \item Fig.~\ref{fig:prelim_3_cifar_vgg19} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CIFAR-10 using VGG19.
    \item Fig.~\ref{fig:prelim_3_cifar_resnet18} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CIFAR-10 using Resnet18.
    \item Fig.~\ref{fig:prelim_3_cifar_fcn} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CIFAR-10 using FCN.
    \item Fig.~\ref{fig:prelim_3_cifar_cnn} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CIFAR-10 using CNN.
    
    \item Fig.~\ref{fig:prelim_3_cifar100_vgg19} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CIFAR-100 using VGG19.
    \item Fig.~\ref{fig:prelim_3_cifar100_resnet18} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CIFAR-100 using Resnet18.
    \item Fig.~\ref{fig:prelim_3_cifar100_fcn} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CIFAR-100 using FCN.
    \item Fig.~\ref{fig:prelim_3_cifar100_cnn} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on CIFAR-100 using CNN.
    
    \item Fig.~\ref{fig:prelim_3_fmnist_vgg19} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on FMNIST using VGG19.
    \item Fig.~\ref{fig:prelim_3_fmnist_resnet18} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on FMNIST using Resnet18.
    \item Fig.~\ref{fig:prelim_3_fmnist_fcn} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on FMNIST using FCN.
    \item Fig.~\ref{fig:prelim_3_fmnist_cnn} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on FMNIST using CNN.
    
    \item Fig.~\ref{fig:prelim_3_mnist_vgg19} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on MNIST using VGG19.
    \item Fig.~\ref{fig:prelim_3_mnist_resnet18} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on MNIST using Resnet18.
    \item Fig.~\ref{fig:prelim_3_mnist_fcn} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on MNIST using FCN.
    \item Fig.~\ref{fig:prelim_3_mnist_cnn} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on MNIST using CNN.
    
    \item Fig.~\ref{fig:prelim_3_voc_unet} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on PascalVOC using U-Net.
    \item Fig.~\ref{fig:prelim_3_coco_unet} repeats the experiment conducted in Fig.~\ref{fig:prelim_3} on COCO using U-Net.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional {\algName} Experiments}
\label{app:addl_lbgm_expt}

In this section, we present complimentary experiments to the properties studies in Section~\ref{sec:expt} of the main text. In particular, we show that our observations hold for datasets: CIFAR-10, CIFAR-100, CelebA, FMNIST, and MNIST. We also present results when using shallower models FCN or deeper models Resnet18 different from CNNs. Section~\ref{app:standalone_expt} gives further evidence of utility of {\algName} as a standalone solution. Section~\ref{app:rho_effect} lists figures that summarize the effect of changing $\subsup{\delta}{k}{\mathsf{threshold}}$ on model performance for mentioned datasets and models. In Section~\ref{app:plugnplay_expt} we list figures that summarize additional experiments to support the observations made in Fig.~\ref{fig:plugnplay} and Section~\ref{app:general_expt} lists figures for additional experiments corresponding to the observations made in Fig.~\ref{fig:dist_training}. Finally, Section~\ref{app:sampling_expt} presents results  on {\algName} algorithm corresponding to the case wherein $50\%$ of clients are randomly sampled during global aggregation. \textit{Since the two layer FCN considered is a simple classifier, it does not perform well on complex datasets such as CIFAR-10, CIFAR-100, and CelebA. Thus, the respective results are omitted for FCN on these complicated datasets and the performance of FCN is only studied for MNIST and FMNIST datasets. Similarly, the 4-layer CNN architecture does not perform well on CIFAR-100 dataset and hence the corresponding results are ommited. We also present results using U-Net architecture for semantic segmentation on PascalVOC dataset.}

\subsection{{\tt \algName} as a Standalone Algorithm.}
\label{app:standalone_expt}
\begin{enumerate}[leftmargin=5mm]
    \item Fig.~\ref{fig:standalone_cnn} shows the result of repeating the experiment conducted in Fig.~\ref{fig:standalone} (CNNs on non-iid data distribution) on CNNs for iid data distribution for datasets CIFAR-10, FMNIST, and MNIST, and U-Net for segmentation for dataset PascalVOC. 
    \item Fig.~\ref{fig:standalone_fcn} shows the result of repeating the experiment conducted in Fig.~\ref{fig:standalone} (CNNs on non-iid data distribution) on FCNs for both non-iid and iid data distributions on datasets FMNIST and MNIST.
    \item Fig.~\ref{fig:standalone_resnet18} shows the result of repeating the experiment conducted in Fig.~\ref{fig:standalone} (CNNs on non-iid data distribution) on Resnet18s for non-iid data distribution on datasets CelebA, CIFAR-10, CIFAR-100, FMNIST, and MNIST using a setup similar to that of \cite{wang2018atomo}.
\end{enumerate}



\subsection{Effect of $\subsup{\delta}{k}{\mathsf{threshold}}$ on {\algName}.}
\label{app:rho_effect}
\begin{enumerate}[leftmargin=5mm]
    \item Fig.~\ref{fig:rho_effect_cnn} shows the result of repeating the experiment conducted in Fig.~\ref{fig:rho_effect} (CNNs on non-iid data distribution) on CNNs for iid data distribution for datasets CIFAR-10, FMNIST, and MNIST, and U-Net for segmentation for dataset PascalVOC.
    \item Fig.~\ref{fig:rho_effect_fcn} shows the result of repeating the experiment conducted in Fig.~\ref{fig:rho_effect} (CNNs on non-iid data distribution) on FCNs for both non-iid and iid data distributions on datasets FMNIST and MNIST.
    \item Fig.~\ref{fig:rho_effect_resnet18} shows the result of repeating the experiment conducted in Fig.~\ref{fig:rho_effect} (CNNs on non-iid data distribution) on Resnet18s for non-iid data distribution on datasets CelebA, CIFAR-10 and CIFAR-100 using a setup similar to that of \cite{wang2018atomo}.
\end{enumerate}



\subsection{{\algName} as a Plug-and-Play Algorithm.}
\label{app:plugnplay_expt}
\begin{enumerate}[leftmargin=5mm]
    \item Fig.~\ref{fig:plugnplay_cnn} shows the result of repeating the experiment conducted in Fig.~\ref{fig:plugnplay} (CNNs on non-iid data distribution) on CNNs for iid data distribution for datasets CIFAR-10, FMNIST, and MNIST.
    \item Fig.~\ref{fig:plugnplay_fcn} shows the result of repeating the experiment conducted in Fig.~\ref{fig:plugnplay} (CNNs on non-iid data distribution) on FCNs for both non-iid and iid data distributions on datasets FMNIST and MNIST.
    \item Fig.~\ref{fig:plugnplay_resnet18} shows the result of repeating the experiment conducted in Fig.~\ref{fig:plugnplay} (CNNs on non-iid data distribution) on Resnet18s for non-iid data distribution on datasets CelebA, CIFAR-10 and CIFAR-100 using a setup similar to that of \cite{wang2018atomo}.
\end{enumerate}


\subsection{Generalizability of {\algName} to Distributed Training.}
\label{app:general_expt}
\begin{enumerate}[leftmargin=5mm]
    \item Fig.~\ref{fig:dist_training_cnn} shows the result of repeating the experiment conducted in Fig.~\ref{fig:dist_training} (CNNs on non-iid data distribution) on CNNs for iid data distribution for datasets CIFAR-10, FMNIST, and MNIST.
    \item Fig.~\ref{fig:dist_training_fcn} shows the result of repeating the experiment conducted in Fig.~\ref{fig:dist_training} (CNNs on non-iid data distribution) on FCNs for both non-iid and iid data distributions on datasets FMNIST and MNIST.
    \item Fig.~\ref{fig:dist_training_resnet18} shows the result of repeating the experiment conducted in Fig.~\ref{fig:dist_training} (CNNs on non-iid data distribution) on Resnet18s for non-iid data distribution on datasets CelebA, CIFAR-10 and CIFAR-100 using a setup similar to that of \cite{wang2018atomo}.
\end{enumerate}

\subsection{{\algName} under Client Sampling.}
\label{app:sampling_expt}

We present results with {\algName} under client sampling in this subsection. The results are qualitatively similar to those presented in Sec.~\ref{sec:expt} under ``{\algName} as Standalone Algorithm''. For example, our results on the MNIST dataset for $50\%$ client participation shows a $35\%$ and $55\%$ improvement in communication efficiency for only $0.2\%$ and $4\%$ drop in accuracy for the corresponding i.i.d and non-i.i.d cases (see column 1 of Fig.~\ref{fig:sampled_training_cnn_iid}\&\ref{fig:sampled_training_cnn_non_iid} respectively).
\begin{enumerate}[leftmargin=5mm]
    \item Fig.~\ref{fig:sampled_training_cnn_non_iid} shows the result of repeating the experiment conducted in Fig.~\ref{fig:standalone} (CNNs on non-iid data distribution) under $50\%$ client sampling using CNNs for non-iid data distribution for datasets CIFAR-10, FMNIST, and MNIST, and regression for dataset CelebA.
    \item Fig.~\ref{fig:sampled_training_cnn_iid} shows the result of repeating the experiment conducted in Fig.~\ref{fig:standalone} (CNNs on non-iid data distribution) under $50\%$ client sampling using CNNs for iid data distribution for datasets CIFAR-10, FMNIST, and MNIST.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{appendix_prelim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{appendix_lbgm}