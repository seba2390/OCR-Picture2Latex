\appendix
\vspace{-1.0in}
\part{Appendix}
\parttoc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Proof of Theorem 1}
\label{app:proof_1}

We start by introducing a lemma, that is used prominently throughout the analysis that follows.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Expectation of Norm of Sum equals sum of Expectation of norms
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{lemma}
\label{lemma:exp_norm_sum}
For a sequence of vectors $\mset{\vec{a}{i}{}}{i=1}{N}$, such that $\E{\vec{a}{i}{}|\vec{a}{i-1}{}, \vec{a}{i-2}{},\cdots,\vec{a}{1}{}}{} = \vec{0}{}{}, \forall i$,
\eqn{
    \E{ \norm{\sum_{i=1}^N \vec{a}{i}{}}{}^2 }{} = \sum_{i=1}^N\E{ \norm{ \vec{a}{i}{}}{}^2 }{}.
    \label{eqn:exp_norm_sum}
    \tag{\bf L2}
}
\end{lemma}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}
\aligneqn{
    \E{ \norm{\sum_{i=1}^N \vec{a}{i}{}}{}^2 }{} = \sum_{i=1}^N\E{ \norm{ \vec{a}{i}{}}{}^2 }{} + \sum_{i=1}^N\sum_{\substack{j=1\\j\neq i}}^N \E{ \vec{a}{i}{\top} \vec{a}{j}{} }{}.
}
Using the law of total expectation, assuming $i<j$, we get 
\eqn{
    \E{ \vec{a}{i}{\top} \vec{a}{j}{} }{} = \E{ \vec{a}{i}{\top}\E{ \vec{a}{j}{}|\vec{a}{i}{},\cdots,\vec{a}{1}{} }{} }{} = \vec{0}{}{},
}
which completes the proof.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Next, we define a few auxiliary variables that would be referenced in the proof later: as defined in the main text, $\vec{g}{k}{(t)} = \sum_{b=0}^{\tau-1} \subsup{g}{k}{}(\vec{\theta}{k}{(t, b)})$ is the accumulated stochastic gradient at worker $k$, where $b$ ranging from $0$ to $\tau-1$ denotes the rounds of local updates. Using Assumption~\eqref{eqn:sgd_noise}, error in stochastic gradient approximation  $\subsup{g}{k}{}(\vec{\theta}{k}{(t, b)})$ can be defined as $\vec{\epsilon}{k}{(t, b)} = \subsup{g}{k}{}(\vec{\theta}{k}{(t, b)}) - \nabla\subsup{F}{k}{}(\vec{\theta}{k}{(t, b)})$. Consequently, we can write the stochastic gradient as $\subsup{g}{k}{}(\vec{\theta}{k}{(t, b)}) = \nabla\subsup{F}{k}{}(\vec{\theta}{k}{(t, b)}) + \vec{\epsilon}{k}{(t, b)}$ where $\nabla\subsup{F}{k}{}(\vec{\theta}{k}{(t, b)})$ is the true gradient. From Assumption~\eqref{eqn:sgd_noise} it follows that

\vspace{-0.2in}
\aligneqn{
    \E{\vec{\epsilon}{k}{(t, b)}}{} = \mathbf{0} \text{ and } \E{ \norm{\vec{\epsilon}{k}{(t, b)}}{}^2 }{} \leq \sigma^2.
}
We also introduce the normalized stochastic gradient $\vec{d}{k}{(t)}$ given by
\eqn{
   \vec{d}{k}{(t)} = \frac{\vec{g}{k}{(t)}}{\tau} = \frac{1}{\tau} \sum_{b=0}^{\tau-1} \subsup{\mathbf{g}}{k}{}(\vec{\theta}{k}{(t, b)}) = \frac{1}{\tau}\sum_{b=0}^{\tau-1} \nabla\subsup{F}{k}{}(\vec{\theta}{k}{(t, b)}) + \frac{1}{\tau}\sum_{b=0}^{\tau-1} \vec{\epsilon}{k}{(t, b)} = \vec{h}{k}{(t)} + \vec{\epsilon}{k}{(t)}, \label{eqn:normalized_approx_grad}
}
where we define
\aligneqn{
    \text{Cumulative Average of the true gradient: } &\vec{h}{k}{(t)} = \frac{1}{\tau}\sum_{b=0}^{\tau-1} \nabla\subsup{F}{k}{}(\vec{\theta}{k}{(t, b)}) \label{eqn:h_k_t}, \text{ and}\\
    \text{Cumulative Average of the SGD error: } &\vec{\epsilon}{k}{(t)} = \frac{1}{\tau}\sum_{b=0}^{\tau-1} \vec{\epsilon}{k}{(t, b)}.
}
Next, we evaluate the first and second moment of normalized SGD error, $\vec{\epsilon}{k}{(t)}$ as follows
\aligneqn{
    \E{\vec{\epsilon}{k}{(t)}}{} &= \E{\frac{1}{\tau}\sum_{b=0}^{\tau-1} \vec{\epsilon}{k}{(t, b)}}{} = \frac{1}{\tau}\sum_{b=0}^{\tau-1} \E{\vec{\epsilon}{k}{(t, b)}}{} = \vec{0}{}{},
    \label{eqn:e_k_t} \\
    \E{\norm{\vec{\epsilon}{k}{(t)}}{}^2}{} &= \E{\norm{\frac{1}{\tau}\sum_{b=0}^{\tau-1} \vec{\epsilon}{k}{(t, b)}}{}^2}{} = \frac{1}{\tau^2}\sum_{b=0}^{\tau-1}\E{\norm{\vec{\epsilon}{k}{(t, b)}}{}^2}{} \leq \frac{\sigma^2}{\tau} ,
    \label{eqn:e_k_t2}
}
where \eqref{eqn:e_k_t} uses Assumption~\eqref{eqn:sgd_noise} and \eqref{eqn:e_k_t2} uses Lemma~\ref{lemma:exp_norm_sum}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Now we can proceed to the proof of Theorem~\ref{thm:conv_char}. We start with the update rule of {\algName}, where a round of global update is given by
\eqn{
\vec{\theta}{}{(t+1)} = \vec{\theta}{}{(t)} - \eta\sum_{k=1}^K\omega_k\tvec{g}{k}{(t)} = \vec{\theta}{}{(t)} - \tau\eta\sum_{k=1}^K\omega_k\tvec{d}{k}{(t)},     
}
where $\tvec{g}{k}{(t)}$ is the approximate gradient shared by worker $k$ with the server and $\tvec{d}{k}{(t)}$ is given by
\eqn{
    \tvec{d}{k}{(t)} = \tvec{g}{k}{(t)}/\tau = \subsup{\rho}{k}{(t),\ell}\vec{g}{k}{\ell}/\tau = \subsup{\rho}{k}{(t),\ell}\vec{d}{k}{\ell}
    \label{eqn:d_l}
}
where $\vec{d}{k}{\ell}=\vec{g}{k}{\ell}/\tau$ is the normalized stochastic gradient w.r.t. the last LBG shared by worker $k$. Also, from the trigonometric relationship shown in Fig.~\ref{fig:approx_fedl}, we have,
\eqn{
    \norm{\subsup{\rho}{k}{(t),\ell}\vec{d}{k}{\ell}}{} = \norm{\vec{d}{k}{(t)} \cos(\subsup{\alpha}{k}{(t),\ell})}{}
    \label{eqn:norm_equality}
}
Similar to $ \vec{d}{k}{(t)}$ from \eqref{eqn:normalized_approx_grad},  $\vec{d}{k}{\ell}$ can be split into the cumulative average of true look-back gradient $\vec{h}{k}{\ell}$ and the corresponding cumulative average of SGD error $\vec{\epsilon}{k}{\ell}$, given by
\eqn{
    \vec{d}{k}{\ell} = \vec{h}{k}{\ell} + \vec{\epsilon}{k}{\ell}.
    \label{eqn:normalized_lbg_grad}
}
From Assumption~\eqref{eqn:lipschitz}, the global loss function is $\beta$ smooth, which implies:

\eqn{
    F(\vec{\theta}{}{(t+1)}) - F(\vec{\theta}{}{(t)}) \leq \dotp{\nabla F(\vec{\theta}{}{(t)})}{\vec{\theta}{}{(t+1)}-\vec{\theta}{}{(t)}}+\frac{\beta}{2}\norm{\vec{\theta}{}{(t+1)}-\vec{\theta}{}{(t)}}{}^2. \\
}
Taking expectation over $\vec{\epsilon}{k}{(t,b)},  k\in\{1,2,\cdots,K\}, b\in\{0,1,\cdots,\tau-1\}$,
\aligneqn{
    \E{F(\vec{\theta}{}{(t+1)})}{} &- F(\vec{\theta}{}{(t)}) \nonumber\\
    &\leq -\tau\eta
    \underbrace{\E{\dotp{\nabla F(\vec{\theta}{}{(t)})}{\sum_{k=1}^K\omega_k\cdot \tvec{d}{k}{(t)}}}{}}_{\bf Z_1} + \frac{\tau^2\eta^2\beta}{2}\underbrace{\E{\norm{\sum_{k=1}^K\omega_k \tvec{d}{k}{(t)}}{}^2}{}}_{\bf Z_2}
    \label{eqn:fl_sgd_i1}.
}
We evaluate ${\bf Z_1}$ as follows 
\aligneqn{
    {\bf Z_1} &= \E{\dotp{\nabla F(\vec{\theta}{}{(t)})}{\sum_{k=1}^K\omega_k \tvec{d}{k}{(t)}}}{} =  \E{\dotp{\nabla F(\vec{\theta}{}{(t)})}{\sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \vec{d}{k}{(t)} + \tvec{d}{k}{(t)}\right)}}{} \nonumber\\
    %
    &= \underbrace{\E{\dotp{\nabla F(\vec{\theta}{}{(t)})}{\sum_{k=1}^K\omega_k \vec{d}{k}{(t)} }}{} }_{\bf Z_{1, 1}} - \underbrace{\E{\dotp{\nabla F(\vec{\theta}{}{(t)})}{\sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \tvec{d}{k}{(t)}\right)}}{} }_{\bf Z_{1,2}}, \label{eqn:t1_pre}
}
where ${\bf Z_{1,1}}$ is given by,
\aligneqn{
    {\bf Z_{1,1}} &\overset{(i)}{=} \E{\dotp{\nabla F(\vec{\theta}{}{(t)})}{\sum_{k=1}^K\omega_k \vec{h}{k}{(t)} }}{} + \E{\dotp{\nabla F(\vec{\theta}{}{(t)})}{\sum_{k=1}^K\omega_k \vec{\epsilon}{k}{(t)} }}{} \nonumber\\
    %
    &\overset{(ii)}{=} \frac{1}{2}\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 \hspace{-0.5mm}+\hspace{-0.5mm} \frac{1}{2}\E{\norm{\sum_{k=1}^K\omega_k
    \vec{h}{k}{(t)} }{}^2}{} \hspace{-0.5mm}-\hspace{-0.5mm} \frac{1}{2}\E{\norm{\nabla F(\vec{\theta}{}{(t)}) \hspace{-0.5mm}-\hspace{-0.5mm} \sum_{k=1}^K\omega_k 
    \vec{h}{k}{(t)} }{}^2}{}\hspace{-1mm},
    \label{eqn:t1_1}
}
where $(i)$ follows from \eqref{eqn:normalized_lbg_grad} and $(ii)$ uses $2\dotp{\mathbf{a}}{\mathbf{b}} = \norm{\mathbf{a}}{}^2 + \norm{\mathbf{b}}{}^2 - \norm{\mathbf{a}-\mathbf{b}}{}^2$ for any two real vectors $\mathbf{a}$ and $\mathbf{b}$. We next upper bound the term ${\bf Z_{1,2}}$ (although ${Z_{1,2}}$ has a negative sign in \eqref{eqn:t1_pre}, ${Z_1}$ also appears with a negative sign in \eqref{eqn:fl_sgd_i1} which allows us to do the upper bound) as follows
\aligneqn{
    {\bf Z_{1,2}} &= \E{\dotp{\nabla F(\vec{\theta}{}{(t)})}{\sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \tvec{d}{k}{(t)}\right)}}{} \nonumber\\
    &\overset{(i)}{\leq} \frac{1}{4} \norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 + \E{\norm{ \sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \tvec{d}{k}{(t)}\right) }{}^2}{}, \label{eqn:t1_2}
}
where $(i)$ follows from $\indotp{a}{b} \leq (1/4)\innorm{a}{}^2+\innorm{b}{}^2$ (result of Cauchy-Schwartz and Young's inequalities). Substituting \eqref{eqn:t1_1} and \eqref{eqn:t1_2} back in \eqref{eqn:t1_pre}, we get


\aligneqn{
     -{\bf Z_1} &\leq -\frac{1}{4}\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 \hspace{-0.5mm}-\hspace{-0.5mm} \frac{1}{2}\E{\norm{\sum_{k=1}^K\omega_k
    \vec{h}{k}{(t)} }{}^2}{} \hspace{-0.5mm}+\hspace{-0.5mm} \frac{1}{2}\E{\norm{\nabla F(\vec{\theta}{}{(t)}) \hspace{-0.5mm}-\hspace{-0.5mm} \sum_{k=1}^K\omega_k 
    \vec{h}{k}{(t)} }{}^2}{}\hspace{-1mm} \nonumber\\
    &~~~~ +  \E{\norm{ \sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \tvec{d}{k}{(t)}\right) }{}^2}{}. \label{eqn:t1}
}
We next bound the term ${\bf Z_2}$ in \eqref{eqn:fl_sgd_i1} as follows 
\aligneqn{
    {\bf Z_2} &= \E{\norm{\sum_{k=1}^K\omega_k \tvec{d}{k}{(t)}}{}^2}{} = \E{\norm{ \sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \vec{d}{k}{(t)} + \tvec{d}{k}{(t)}\right) }{}^2}{} \nonumber\\  
    &\overset{(i)}{\leq} 2\underbrace{\E{\norm{ \sum_{k=1}^K\omega_k \vec{d}{k}{(t)} }{}^2}{}}_{\bf Z_{2,1}} + 2\E{\norm{ \sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \tvec{d}{k}{(t)}\right) }{}^2}{},
    \label{eqn:t2_pre}
}
where ${\bf Z_{2,1}}$ is given by,
\aligneqn{
    {\bf Z_{2,1}} &= \E{\norm{ \sum_{k=1}^K\omega_k \vec{d}{k}{(t)} }{}^2}{} \overset{(i)}{=} \E{\norm{ \sum_{k=1}^K\omega_k \vec{h}{k}{(t)} }{}^2}{} + \E{\norm{ \sum_{k=1}^K\omega_k \vec{\epsilon}{k}{(t)} }{}^2}{} \nonumber\\
    &\overset{(ii)}{\leq} \E{\norm{ \sum_{k=1}^K\omega_k \vec{h}{k}{(t)} }{}^2}{} + \sum_{k=1}^K\omega_k \E{\norm{ \vec{\epsilon}{k}{(t)} }{}^2}{} \overset{(iii)}{=} \E{\norm{ \sum_{k=1}^K\omega_k \vec{h}{k}{(t)} }{}^2}{} + \frac{\sigma^2}{\tau},\label{eqn:t2_1}
}
where $(i)$ follows from \eqref{eqn:normalized_approx_grad}, $(ii)$ uses Jensen's inequality:  $\innorm{\sum_{k=1}^{K} \omega_k \vec{a}{k}{} }{}^2\leq \sum_{k=1}^{K} \omega_k \norm{ \mathbf{a}_k }{}^2$, s.t. $\sum_{k=1}^{K} \omega_k =1$, and $(iii)$ uses \eqref{eqn:e_k_t2}. Plugging \eqref{eqn:t2_1} back into \eqref{eqn:t2_pre}, we get
\aligneqn{
    {\bf Z_2} \leq 2\E{\norm{ \sum_{k=1}^K\omega_k \vec{h}{k}{(t)} }{}^2}{} + \frac{2\sigma^2}{\tau} + 2\E{\norm{ \sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \tvec{d}{k}{(t)}\right) }{}^2}{}.
    \label{eqn:t2}
}
Substituting \eqref{eqn:t1} and \eqref{eqn:t2} back in \eqref{eqn:fl_sgd_i1}, we get 
\aligneqn{
    &\E{F(\vec{\theta}{}{(t+1)})}{} - F(\vec{\theta}{}{(t)}) \leq  -\frac{\tau\eta}{4}\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 \hspace{-0.5mm}-\hspace{-0.5mm} \frac{\tau\eta}{2}\E{\norm{\sum_{k=1}^K\omega_k
    \vec{h}{k}{(t)} }{}^2}{} \nonumber\\
    %
    &~~~~\hspace{-0.5mm}+\hspace{-0.5mm} \frac{\tau\eta}{2}\E{\norm{\nabla F(\vec{\theta}{}{(t)}) \hspace{-0.5mm}-\hspace{-0.5mm} \sum_{k=1}^K\omega_k 
    \vec{h}{k}{(t)} }{}^2}{}\hspace{-1mm} +  \tau\eta \E{\norm{ \sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \tvec{d}{k}{(t)}\right) }{}^2}{} \nonumber\\
    &~~~~+ \tau^2\eta^2\beta\E{\norm{ \sum_{k=1}^K\omega_k \vec{h}{k}{(t)} }{}^2}{} + \tau\eta^2\beta\sigma^2 + \tau^2\eta^2\beta\E{\norm{ \sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \tvec{d}{k}{(t)}\right) }{}^2}{} \nonumber\\
    %
    &=  -\frac{\tau\eta}{4}\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 \hspace{-0.5mm}-\hspace{-0.5mm} \frac{\tau\eta}{2}(1-2\tau\eta\beta)\E{\norm{\sum_{k=1}^K\omega_k
    \vec{h}{k}{(t)} }{}^2}{} \hspace{-0.5mm}+\hspace{-0.5mm} \frac{\tau\eta}{2}\E{\norm{\nabla F(\vec{\theta}{}{(t)}) \hspace{-0.5mm}-\hspace{-0.5mm} \sum_{k=1}^K\omega_k 
    \vec{h}{k}{(t)} }{}^2}{}\hspace{-1mm} \nonumber\\
    %
    &~~~~ +  \tau\eta(1+\tau\eta\beta) \E{\norm{ \sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \tvec{d}{k}{(t)}\right) }{}^2}{} + \tau\eta^2\beta\sigma^2.
    \label{eqn:t2_2}
}
Choosing  $\tau\eta\beta \leq 1/2$, implies that $- \frac{\tau\eta}{2}(1-2\tau\eta\beta)\leq 0$ and $1+\tau\eta\beta \leq 3/2 < 2$, which results in simplification of \eqref{eqn:t2_2} as:
\aligneqn{
    \frac{\E{F(\vec{\theta}{}{(t+1)})}{} - F(\vec{\theta}{}{(t)})}{\eta\tau} &\leq -\frac{1}{4}\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 + \frac{1}{2} \E{\norm{\nabla F(\vec{\theta}{}{(t)}) \hspace{-0.5mm}-\hspace{-0.5mm} \sum_{k=1}^K\omega_k 
    \vec{h}{k}{(t)} }{}^2}{}\nonumber\\
    %
    &~~~~+ 2\E{\norm{ \sum_{k=1}^K\omega_k \left(\vec{d}{k}{(t)} - \tvec{d}{k}{(t)}\right) }{}^2}{} + \eta\beta\sigma^2 \nonumber\\
    %
    &\overset{(i)}{\leq} -\frac{1}{4}\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 + \frac{1}{2} \sum_{k=1}^K\omega_k \E{\norm{\nabla F_k(\vec{\theta}{k}{(t)}) \hspace{-0.5mm}-\hspace{-0.5mm} 
    \vec{h}{k}{(t)} }{}^2}{}\nonumber\\
    %
    &~~~~+ 2\sum_{k=1}^K\omega_k \underbrace{\E{\norm{ \vec{d}{k}{(t)} - \tvec{d}{k}{(t)} }{}^2}{}}_{\bf Z_3} + \eta\beta\sigma^2,
    \label{eqn:t3}
}
where $(i)$ follows from Jensen's inequality $\innorm{\sum_{k=1}^{K} \omega_k \vec{a}{k}{} }{}^2\leq \sum_{k=1}^{K} \omega_k \norm{ \mathbf{a}_k }{}^2$, s.t. $\sum_{k=1}^{K} \omega_k =1$ and $\vec{\theta}{k}{(t)} = \vec{\theta}{}{(t)}, \forall k$ due to local synchronization. Now ${\bf Z_3}$ can be bounded as follows:

\aligneqn{
    {\bf Z_3} &= \E{\norm{ \vec{d}{k}{(t)} - \tvec{d}{k}{(t)} }{}^2}{} \overset{(i)}{=} \E{\norm{\vec{d}{k}{(t)} -
    \subsup{\rho}{k}{(t),\ell} \vec{d}{k}{\ell} }{}^2}{} \overset{(ii)}{=} \E{\norm{\vec{d}{k}{(t)} -
    \frac{ \frac{1}{\tau^2} \dotp{ \vec{g}{k}{(t)} }{ \vec{g}{k}{\ell}} }{\frac{1}{\tau^2} \norm{\vec{g}{k}{\ell}}{}^2} \vec{d}{k}{\ell} }{}^2}{} \nonumber\\
    %
    &\overset{(iii)}{=} \E{\norm{\vec{d}{k}{(t)} -
    \frac{ \dotp{ \vec{d}{k}{(t)} }{ \vec{d}{k}{\ell}} }{ \norm{\vec{d}{k}{\ell}}{}^2} \vec{d}{k}{\ell} }{}^2}{} \overset{}{=} \E{\norm{\vec{d}{k}{(t)}}{}^2 - \frac{\dotp{ \vec{d}{k}{(t)} }{ \vec{d}{k}{\ell}}^2}{\norm{\vec{d}{k}{\ell}}{}^2}}{} \nonumber\\
    %
    &\overset{(iv)}{=} \E{\norm{\vec{d}{k}{(t)}}{}^2 - \norm{ \vec{d}{k}{(t)} }{}^2\cos^2(\vec{\alpha}{k}{(t),\ell})}{} \overset{}{=} \E{\norm{\vec{d}{k}{(t)}}{}^2 \left(1 -\cos^2(\vec{\alpha}{k}{(t),\ell})\right)}{} \nonumber\\
    %
    &= 2\E{\norm{\vec{d}{k}{(t)}}{}^2 \sin^2(\vec{\alpha}{k}{(t),\ell})}{} \overset{(v)}{\leq} \E{ \Delta^2}{} = \Delta^2,
    \label{eqn:lbgm_diff}
}

where $(i)$ uses \eqref{eqn:d_l}, $(ii)$ uses {\algName} definition from \eqref{eqn:grad_proj}, $(iii)$ uses the fact that $\vec{d}{k}{(t)} = \vec{g}{k}{\ell}/\tau$, $(iv)$ uses $\indotp{a}{b} = \innorm{a}{}\innorm{b}{}\cos(\alpha)$, and $(v)$ follows from the condition in the theorem. Substituting \eqref{eqn:lbgm_diff} back in \eqref{eqn:t3}, we get
\aligneqn{
    &\frac{\E{F(\vec{\theta}{}{(t+1)})}{} - F(\vec{\theta}{}{(t)})}{\eta\tau} \nonumber\\
    &\leq -\frac{1}{4}\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 + \frac{1}{2} \sum_{k=1}^K\omega_k \underbrace{\E{\norm{\nabla F_k(\vec{\theta}{k}{(t)}) \hspace{-0.5mm}-\hspace{-0.5mm} 
    \vec{h}{k}{(t)} }{}^2}{}}_{\bf Z_4} + 2\Delta^2 + \eta\beta\sigma^2,
    \label{eqn:t3_1}
}
where ${\bf Z_4}$ is given by,
\aligneqn{
    {\bf Z_4} &= \E{\norm{\nabla F_k(\vec{\theta}{k}{(t)}) \hspace{-0.5mm}-\hspace{-0.5mm} 
    \vec{h}{k}{(t)} }{}^2}{} = \frac{1}{\tau^2}\E{\norm{\sum_{b=0}^{\tau-1}\left(\nabla F_k(\vec{\theta}{k}{(t,0)}) \hspace{-0.5mm}-\hspace{-0.5mm} 
    \nabla F_k(\vec{\theta}{k}{(t,b)}) \right)}{}^2}{} \nonumber\\
    &\leq \frac{1}{\tau} \sum_{b=0}^{\tau-1} \E{\norm{\nabla F_k(\vec{\theta}{k}{(t,0)}) \hspace{-0.5mm}-\hspace{-0.5mm} 
    \nabla F_k(\vec{\theta}{k}{(t,b)}) }{}^2}{} \leq \frac{\beta^2}{\tau} \sum_{b=0}^{\tau-1} \E{\norm{\vec{\theta}{k}{(t,0)} \hspace{-0.5mm}-\hspace{-0.5mm} 
    \vec{\theta}{k}{(t,b)} }{}^2}{}.
}

Also, using the local update rule $\vec{\theta}{k}{(t,b)} \leftarrow \vec{\theta}{k}{(t,0)} - \eta \sum_{s=0}^{b-1} \mathbf{g}_k(\vec{\theta}{k}{(t,s)})$, where $\vec{\theta}{k}{(t,b)}$ is the model parameter obtained at the $b$-th local iteration of the global round $t$ at device $k$, we get:
\aligneqn{
    {\bf Z_5} &= \E{\norm{ \vec{\theta}{k}{(t,0)} - \vec{\theta}{k}{(t,b)}}{}^2}{} \nonumber\\
    &= \eta^2 \E{\norm{ \sum_{s=0}^{b-1} \mathbf{g}_k(\vec{\theta}{k}{(t,s)}) }{}^2}{} \overset{(i)}{\leq} 2\eta^2 \E{\norm{ \sum_{s=0}^{b-1} \nabla F_k(\vec{\theta}{k}{(t,s)}) }{}^2}{} + 2\eta^2 \E{\norm{ \sum_{s=0}^{b-1} \vec{\epsilon}{k}{(t,s)} }{}^2}{} \nonumber\\
    %
    &\overset{(ii)}{\leq} 2\eta^2b \sum_{s=0}^{b-1} \E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,s)}) }{}^2}{} + 2\eta^2 \sum_{s=0}^{b-1}  \E{\norm{ \vec{\epsilon}{k}{(t,s)} }{}^2}{} \nonumber\\& \leq 2\eta^2b \sum_{s=0}^{\tau-1} \E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,s)}) }{}^2}{} + 2\eta^2\sigma^2b, \label{eqn:t4_3}
}

where $(i)$ uses Cauchy-Schwartz inequality and $(ii)$ uses Lemma~\ref{lemma:exp_norm_sum} and Cauchy-Schwartz inequality. Also note that
\aligneqn{
    \sum_{b=0}^{\tau-1}b &= \frac{\tau(\tau-1)}{2}.
    \label{eqn:sum_b}
}

Taking the cumulative sum of both hand sides of ${\bf Z_5}$ from \eqref{eqn:t4_3} over all batches, i.e., $\frac{1}{\tau}\sum_{b=0}^{\tau-1}$, and using \eqref{eqn:sum_b}, we get:
\aligneqn{
    \frac{1}{\tau}\sum_{b=0}^{\tau-1} \E{\norm{ \vec{\theta}{k}{(t,0)} - \vec{\theta}{k}{(t,b)}}{}^2}{} &\leq \sigma^2\eta^2(\tau-1) + \eta^2(\tau-1) \sum_{b=0}^{\tau-1} \underbrace{\E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,b)}) }{}^2}{}}_{\bf Z_6}.\label{eqn:t5_1}
}

Furthermore, term ${\bf Z_6}$ can be bounded as follows:
\aligneqn{
    {\bf Z_6} &= \E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,b)}) }{}^2}{} \overset{(i)}{\leq} 2\E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,b)}) - \nabla F_k(\vec{\theta}{k}{(t,0)}) }{}^2}{} + 2\E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,0)}) }{}^2}{} \nonumber\\
    %
    &\overset{(ii)}{\leq} 2\beta^2\E{\norm{ \vec{\theta}{k}{(t,b)} - \vec{\theta}{k}{(t,0)} }{}^2}{} + 2\E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,0)}) }{}^2}{},
    \label{eqn:t6}
}

where $(i)$ uses Cauchy-Schwartz inequality and $(ii)$ uses \eqref{eqn:lipschitz}. Replacing ${\bf Z_6}$ in \eqref{eqn:t5_1} using \eqref{eqn:t6},  we get:
\aligneqn{
    &\frac{1}{\tau}\sum_{b=0}^{\tau-1} \underbrace{\E{\norm{ \vec{\theta}{k}{(t,0)} - \vec{\theta}{k}{(t,b)}}{}^2}{}}_{\bf Z_5} \nonumber\\
    &\leq \eta^2\sigma^2(\tau-1) + 2\eta^2\beta^2(\tau-1) \sum_{b=0}^{\tau-1} \underbrace{\E{\norm{ \vec{\theta}{k}{(t,b)} - \vec{\theta}{k}{(t,0)} }{}^2}{}}_{\bf Z_5} + 2\eta^2\tau(\tau-1) \E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,0)}) }{}^2}{}.
}

Note that ${\bf Z_5}$, which is originally defined in~\eqref{eqn:t4_3}, appears both in the left hand side (LHS) and right hand side (RHS) of the above expression. Rearranging the terms in the above inequality yields:
\eqn{
    \frac{1}{\tau}\sum_{b=0}^{\tau-1} \E{\norm{ \vec{\theta}{k}{(t,0)} - \vec{\theta}{k}{(t,b)}}{}^2}{} \leq \frac{\eta^2\sigma^2(\tau-1)}{1-2\eta^2\beta^2\tau(\tau-1)} + \frac{2\eta^2\tau(\tau-1)}{1-2\eta^2\beta^2\tau(\tau-1)}\E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,0)}) }{}^2}{}.
}
Defining $H=2\eta^2\beta^2\tau(\tau-1)$, the above inequality can be re-written to evaluate ${\bf Z_4}$,
\aligneqn{
    {\bf Z_4} &= \frac{\beta^2}{\tau}\sum_{b=0}^{\tau-1} \E{\norm{ \vec{\theta}{}{(t,0)} - \vec{\theta}{}{(t,b)}}{}^2}{} \nonumber\\
    %
    &\leq \frac{\eta^2\beta^2\sigma^2(\tau-1)}{1-2\eta^2\beta^2\tau(\tau-1)} + \frac{2\eta^2\beta^2\tau(\tau-1)}{1-2\eta^2\beta^2\tau(\tau-1)}\E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,0)}) }{}^2}{} \nonumber\\
    %
    &= \frac{\eta^2\beta^2\sigma^2}{1-H}(\tau-1) + \frac{H}{1-H}\E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,0)}) }{}^2}{}.
}

Taking a weighted sum from the both hand sides of the above inequality across all the workers and using \eqref{eqn:bbd_diversity}, we get:
\aligneqn{
    & \frac{1}{2}\sum_{k=1}^{K} \omega_k \E{\norm{\nabla F_k(\vec{\theta}{k}{(t)}) \hspace{-0.5mm}-\hspace{-0.5mm} 
    \vec{h}{k}{(t)} }{}^2}{} \nonumber\\
    %
    &\leq \frac{\eta^2\beta^2\sigma^2 (\tau-1)}{2(1-H)} \sum_{k=1}^{K} \omega_k + \frac{H}{2(1-H)}\sum_{k=1}^{K} \omega_k \E{\norm{ \nabla F_k(\vec{\theta}{k}{(t,0)}) }{}^2}{} \nonumber\\
    %
    &=\frac{\eta^2 \beta^2\sigma^2(\tau-1)}{2(1-H)} + \frac{H}{2(1-H)}\sum_{k=1}^{K} \omega_k \E{\norm{ \nabla F_k(\vec{\theta}{k}{(t)}) }{}^2}{} \nonumber\\
    %
    &\overset{(i)}{\leq}\frac{\eta^2\beta^2\sigma^2(\tau-1)}{2(1-H)} + \frac{H\Upsilon^2}{2(1-H)} \E{\norm{ \nabla F(\vec{\theta}{}{(t)}) }{}^2}{} + \frac{H\Gamma^2}{2(1-H)},
    \label{eqn:avg_diff_grad}
}
where $(i)$ follows from \eqref{eqn:bbd_diversity} and $\vec{\theta}{k}{(t)}=\vec{\theta}{}{(t)},~\forall k$ since computation occurs at the instance of global aggregation.
Next, plugging \eqref{eqn:avg_diff_grad} back in \eqref{eqn:t3_1}, we get:
\aligneqn{
 &\frac{\E{F(\vec{\theta}{}{(t+1)})}{} - F(\vec{\theta}{}{(t)})}{\eta\tau} \nonumber\\
    &\leq -\frac{1}{4}\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 + \frac{1}{2} \sum_{k=1}^K\omega_k \E{\norm{\nabla F_k(\vec{\theta}{k}{(t)}) \hspace{-0.5mm}-\hspace{-0.5mm} 
    \vec{h}{k}{(t)} }{}^2}{} + 2\Delta^2 + \eta\beta\sigma^2 \nonumber\\
    %
    &\leq -\frac{1}{4}\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 + \frac{\eta^2\beta^2\sigma^2(\tau-1)}{2(1-H)} + \frac{H\Upsilon^2}{2(1-H)} \E{\norm{ \nabla F(\vec{\theta}{}{(t)}) }{}^2}{} + \frac{H\Gamma^2}{2(1-H)} \nonumber\\ 
    %
    &~~~~ + 2\Delta^2 + \eta\beta\sigma^2 \nonumber\\
    %
    &\leq -\frac{1}{4}\left( 1 - \frac{2H\Upsilon^2}{1-H}  \right) \norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 + \frac{\eta^2\beta^2\sigma^2(\tau-1)}{2(1-H)} + \frac{H\Gamma^2}{2(1-H)} + 2\Delta^2 + \eta\beta\sigma^2.
}

If $H \leq \frac{1}{1+2\alpha\Upsilon^2}$ for some constant $\alpha > 1$, then it follows that $\frac{1}{1-H} \leq 1+ \frac{1}{2\alpha\Upsilon^2}$ and $\frac{2H\Upsilon^2}{1-H} \leq \frac{1}{\alpha}$. Choosing $\alpha = 2$ we can simplify the above expression as follows:

\aligneqn{
    &\frac{\E{F(\vec{\theta}{}{(t+1)})}{} - F(\vec{\theta}{}{(t)})}{\eta\tau} \nonumber\\
    &\leq -\frac{1}{8}\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 + 2\Delta^2 + \eta\beta\sigma^2 + \eta^2\beta^2\sigma^2(\tau-1) \left(\frac{1}{2}+\frac{1}{8\Upsilon^2}\right) + 2 \eta^2\beta^2 \Gamma^2 \tau(\tau-1) \left(1+\frac{1}{4\Upsilon^2}\right) \nonumber\\
    %
    &\leq -\frac{1}{8}\norm{\nabla F(\vec{\theta}{}{(t)})}{}^2 + 2\Delta^2 + \eta\beta\sigma^2  + \frac{5}{8}\eta^2\beta^2\sigma^2(\tau-1) + \frac{5}{2} \eta^2\beta^2 \Gamma^2 \tau(\tau-1).
}
Rearranging the terms in the above inequality and taking the  average across all aggregation rounds from the both hand sides, yields:
\aligneqn{
    &\frac{1}{T} \sum_{t=0}^{T-1} \E{\norm{\nabla F(\vec{\theta}{}{(t, 0)})}{}^2}{} \nonumber\\
    &\leq \frac{8\left[\sum_{t=0}^{T-1}\E{F(\vec{\theta}{}{(t)})}{} - F(\vec{\theta}{}{(t+1)})\right]}{\eta\tau T} + 16\Delta^2 + 8\eta\beta\sigma^2 + 5\eta^2\beta^2\sigma^2(\tau-1) + 20 \eta^2\beta^2 \Gamma^2 \tau(\tau-1) \nonumber\\
    %
    &= \frac{8\left[F(\vec{\theta}{}{(0)}) - F(\vec{\theta}{}{(T)})\right]}{\eta\tau T} + 16\Delta^2 + 8\eta\beta\sigma^2 + 5\eta^2\beta^2\sigma^2(\tau-1) + 20 \eta^2\beta^2 \Gamma^2 \tau(\tau-1) \nonumber\\
    %
    &\leq \frac{8\left[F(\vec{\theta}{}{(0)}) - F^\star\right]}{\eta\tau T} + 16\Delta^2 + 8\eta\beta\sigma^2 + 5\eta^2\beta^2\sigma^2(\tau-1) + 20 \eta^2\beta^2 \Gamma^2 \tau(\tau-1) \label{eqn:final},
}
where we used the fact that $F$ is bounded below, since $F_k$-s are presumed to be bounded below, and $F^\star\leq F(\vec{\theta}{}{})$, $\forall \vec{\theta}{}{}\in\mathbb{R}^{M}$. This completes the proof of Theorem~\ref{thm:conv_char}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Condition on Learning Rate}
From the two conditions on the learning rate used in the analysis above, we have
\aligneqn{
    \eta\beta &\leq \frac{1}{2\tau} \\
    2\eta^2\beta^2\tau(\tau-1) &\leq \frac{1}{1+4\Upsilon^2}
}
We can further tighten the second constraint as,
\eqn{
    2\eta^2\beta^2\tau(\tau-1) \leq 2\eta^2\beta^2\tau^2 \leq \frac{1}{1+4\Upsilon^2}
}
Combining the two we have, 
\aligneqn{
    \eta\beta &\leq \min\left\{ \frac{1}{2\tau}, \frac{1}{\tau\sqrt{2(1+4\Upsilon^2)}} \right\}.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Proof of Corollary 1}
\label{app:corrolary_1}

Using \eqref{eqn:final} we have:
\aligneqn{
    &\frac{1}{T} \sum_{t=0}^{T-1} \E{\norm{\nabla F(\vec{\theta}{}{(t, 0)})}{}^2}{} \nonumber\\
    &\leq \frac{8\left[F(\vec{\theta}{}{(0)}) - F^\star\right]}{\eta\tau T} + 16\Delta^2 + 8\eta\beta\sigma^2 + 5\eta^2\beta^2\sigma^2(\tau-1) + 20 \eta^2\beta^2 \Gamma^2 \tau(\tau-1).
    \label{eqn:simple_2}
}
Next, using the assumption in the corollary statement, we have $\Delta^2 \leq \eta$. We then can upper bound the RHS of \eqref{eqn:simple_2} to get:
\aligneqn{
    &\frac{1}{T} \sum_{t=0}^{T-1} \E{\norm{\nabla F(\vec{\theta}{}{(t, 0)})}{}^2}{} \nonumber\\
    &\leq \frac{8\left[F(\vec{\theta}{}{(0)}) - F^\star\right]}{\eta\tau T} + 16\eta + 8\eta\beta\sigma^2 + 5\eta^2\beta^2\sigma^2(\tau-1) + 20 \eta^2\beta^2 \Gamma^2 \tau(\tau-1).
}
Choosing $\eta = \sqrt{\frac{1}{\tau T}}$, we get, 
\aligneqn{
    &\frac{1}{T} \sum_{t=0}^{T-1} \E{\norm{\nabla F(\vec{\theta}{}{(t, 0)})}{}^2}{} \nonumber\\
    &\leq \frac{8\left[F(\vec{\theta}{}{(0)}) - F^\star\right]}{\sqrt{\tau T}} + \frac{8\beta\sigma^2}{\sqrt{\tau T}} + \frac{16}{\sqrt{\tau T}} + \frac{5\beta^2\sigma^2(\tau-1)}{\tau T} + \frac{20 \beta^2 \Gamma^2 \tau(\tau-1)}{\tau T}.
}
We can write the above expression as, 
\aligneqn{
    &\frac{1}{T} \sum_{t=0}^{T-1} \E{\norm{\nabla F(\vec{\theta}{}{(t, 0)})}{}^2}{} \nonumber\\
    &\leq \bigo{\frac{1}{\sqrt{\tau T}}} + \bigo{\frac{\sigma^2}{\sqrt{\tau T}}} + \bigo{\frac{1}{\sqrt{\tau T}}} + \bigo{\frac{\sigma^2(\tau-1)}{\tau T})} + \bigo{\frac{ (\tau-1)\Gamma^2}{ T}}.
}
This completes the proof for Corollary~\ref{corr:stationary_conv}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Additional Discussions}

\subsection{{\algName} Storage Considerations}
\label{app:storage}
As discussed in Section~\ref{sec:method}, {\algName} requires both server and workers to store the last copy of the workers' look-back gradient (LBG). While storing a single LBG (same size as the model) at each device might be trivial since the space complexity increases by a constant factor (i.e., the space complexity increases from $\bigo{M}$ to $\bigo{2M}=\bigo{M}$ where $M$ is the model size), storage of LBGs at the server might require more careful considerations since it scales linearly with the number of devices (i.e., space complexity increases from $\bigo{M}$ to $\bigo{KM}$ where $M$ is the model size and $K$ is the number of workers). Thus, the storage requirements can scale beyond memory capabilities of an aggregation server for a very large scale federated learning systems. We, therefore propose the following solutions for addressing the storage challenge:
\begin{itemize}[leftmargin=5mm]
    \item \textit{Storage Offloading.} In a large scale federated learning system, it is realistic to assume network hierarchy \cite{hosseinalipour2020multi}, e.g., base stations, edge servers, cloud servers, etc. In such cases the storage burden can be offloaded and distributed across the network hierarchy where the LBGs of the devices are stored.
    \item \textit{LBG Compression.} If the {\algName} is applied on top of existing compression techniques such as Top-K, ATOMO, etc., the size of LBGs to be stored at the server also gets compressed which reduces the storage burden. Alternatively, we could use parameter clustering techniques \citep{son2018clustering,cho2021dkm} to reduce LBG size at the server.
    \item \textit{LBG Clustering.} In a very large scale federated learning system, say with a billion workers, it's unrealistic to assume that all the billion clients have very different LBGs given the low rank hypothesis \eqref{hypothesis} and possible local data similarities across the workers. It should therefore be possible to cluster the LBGs at the server into a smaller number of centroids and only saving the centroids of the clusters instead of saving all the LBGs of the devices individually. The centroids can be broadcast across the devices to update the local version of the LBGs.
\end{itemize}

\subsection{Hyperparameter Tuning}
\label{app:hyperparams}

Most of the compression/communication savings baselines operate on a tradeoff between communication savings and accuracy. For hyperparameter selection, we first optimize the hyperparameters for the base algorithm such that we achieve the best possible communication saving subject to constraint that accuracy does not fall off much below the corresponding vanilla federated learning approach. For example, we optimize the value of $K$ in top-K sparsification by changing $K$ in orders of 10, i.e. $K=10\%$, $K=1\%$, $K=0.1\%$, etc. and choose the value that gives the best tradeoff between the final model accuracy and communication savings (this value is generally around $K=10\%$). Similarly, for ATOMO we consider rank-1, rank-2, and rank-3 approximations. While rank-1 approximation gives better communication savings, the corresponding accuracy falls off sharply. Rank-3 approximation gives only a marginal accuracy benefit over rank-2 approximation but adds considerably more communication burden. Thus we use rank-2 approximations in ATOMO. In the plug-and-play evaluations, the LBGM algorithm is applied on top of the base algorithms once their hyperparameters are tuned as a final step to show the additional benefits we can attain by exploiting the low-rank characteristic of the  gradient subspace. Our chosen hyperparameters can be found in our code repository: \url{https://github.com/shams-sam/FedOptim}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Additional Pseudocodes}

\subsection{Pseudocode for Preliminary Experiments}
\label{app:pseudo}
In this Appendix, we provide a psuedocode for generating the preliminary experimental results in Sec.~\ref{sec:explore}. The actual implementation of the following function calls used in the pseudocode can be found in the listed files of our code repository: \url{https://github.com/shams-sam/FedOptim}:
\begin{itemize}[leftmargin=5mm]
    \item ${\tt get\_num\_PCA\_components}$: implemented in function $\mathsf{estimate\_optimal\_ncomponents}$, file: $\mathsf{src/common/nb\_utils.py}$ of the repository. \shams{In summary, we stack the accumulated gradients over the epochs an perform singular value decomposition, after which we do the standard analysis for estimating the number of components explaining a given amount of variance in the datasets. Specifically,  we count the number of singular values that account for the $99\%$ and $95\%$ of the aggregated singular values.}
    \item ${\tt get\_PCA\_components}$: implemented in function~$\mathsf{pca\_transform}$,~file:~$\mathsf{src/common/nb\_utils.py}$ of the repository. \shams{In summary, we stack the accumulated gradients over the epochs an perform singular value decomposition, after which we do the standard analysis for recovering the principal components explaining a given amount of variance in the datasets. Specifically,  we recover the left singular vectors corresponding the singular values that account for the $99\%$ and $95\%$ of the aggregated singular values.}
    \item ${\tt cosine\_similarity}$: implemented using functions $\mathsf{sklearn.preprocessing.normalize}$ and $\mathsf{numpy.dot}$, such that ${\tt cosine\_similarity(a,b)=normalize(a).dot(normalize(b))}$ where ${\tt a}$ and ${\tt b}$ are $\mathsf{numpy.array}$, \shams{where ${\tt normalize}$ performs the vector normalization and ${\tt dot}$ is the standard vector dot product}.
    \item ${\tt plot\_1}$, ${\tt plot\_2}$, and ${\tt plot\_3}$: implemented in files $\mathsf{src/viz/prelim\_1.py}$,  $\mathsf{src/viz/prelim\_2.py}$, and  $\mathsf{src/viz/prelim\_3.py}$ respectively.
\end{itemize}
\begin{algorithm}[h!]
{\footnotesize 
  \caption{Pseudocode for Preliminary Experiments in Section~\ref{sec:explore}}
\label{alg:pseudo}
\begin{algorithmic}[1]
    \State Initialize model parameter $\vec{\theta}{}{(0)}$.
    \State Initialize ${\tt actual\_grads}$ = \{\} \Comment{store gradients for ${\tt PCA}$}
    \State Initialize ${\tt pca95\_store}$ = \{\} \Comment{store \#components accounting for 95\% variance}
    \State Initialize ${\tt pca99\_store}$ = \{\} \Comment{store \#components accounting for 99\% variance}
    \Statex
    \For{$t = 0$ to $T-1$} \Comment{training for $T$ epochs}
        \State Initialize $\vec{g}{}{(t)} \leftarrow \vec{0}{}{}$. \Comment{initialize gradient accumulator}
        \State Set $\vec{\theta}{}{(t,0)} \leftarrow \vec{\theta}{}{(t)}$.
        \For{$b=0$ to $B-1$} \Comment{$B$ minibatches per epoch}
            \State Sample a minibatch of  datapoints $\mathcal{B}$ from dataset $\mathcal{D}$. 
            \State Compute $\vec{g}{}{(t,b)}= \sum_{d\in \mathcal{B}} \nabla f(\vec{\theta}{}{(t,b)};d)/|\mathcal{B}|$.
            \State Update parameter: $\vec{\theta}{}{(t,b+1)} \leftarrow \vec{\theta}{}{(t,b)} - \eta \cdot \vec{g}{}{(t,b)}$.
            \State Accumulate gradient: $\vec{g}{}{(t)} \leftarrow \vec{g}{}{(t)} + \vec{g}{}{(t,b)}$.
        \EndFor    
        \State Set $\vec{\theta}{}{(t+1)} \leftarrow \vec{\theta}{}{(t, B)}$.
        \State ${\tt actual\_grads.append(} \vec{g}{}{(t)} {\tt )}$ \Comment{append accumulated gradient to store}
        \State ${\tt pca95\_store.append(get\_num\_PCA\_components(actual\_grads, variance=0.95))}$
        \State ${\tt pca99\_store.append(get\_num\_PCA\_components(actual\_grads, variance=0.99))}$
        \Statex
    \EndFor
    \Statex
    \State ${\tt plot\_1(pca95\_store, pca99\_store)}$ \Comment{plot of PCA component progression}
    \Statex
    \State ${\tt principal\_grads = get\_PCA\_components(actual\_grads, variance=0.99)}$
    \State ${\tt heatmap = zeros(len(actual\_grads), len(principal\_grads))}$
    \For{$i=0$ to ${\tt len(actual\_grads)-1}$ }
        \For{$j=0$ to ${\tt len(principal\_grads)-1}$ }
            \State ${\tt heatmap[i,j] = cosine\_similarity(actual\_grads[i], principal\_grads[j])}$
        \EndFor
    \EndFor
    \Statex
    \State ${\tt plot\_2(heatmap)}$ \Comment{plot of overlap of actual and principal gradients}
    \Statex
    \State ${\tt heatmap = zeros(len(actual\_grads), len(actual\_grads))}$
    \For{$i=0$ to ${\tt len(actual\_grads)-1}$ }
        \For{$j=i$ to ${\tt len(actual\_grads)-1}$ }
            \State ${\tt heatmap[i,j] = heatmap[j, i]= cosine\_similarity(actual\_grads[i], actual\_grads[j])}$
        \EndFor
    \EndFor
    \Statex
    \State ${\tt plot\_3(heatmap)}$ \Comment{plot of similarity among actual gradients}
\end{algorithmic}
}
\end{algorithm}

\clearpage
\subsection{Pseudocode for Device Sampling Experiments}
\label{ssec:sampling_algo}

The pseudocode for {\algName} with device sampling is given below. The process is largely similar to Algorithm~\ref{alg:algo_training}, except modifications in the global aggregation strategy. During global aggregation the server samples a subset $K'$ of all the available clients and receives updates from only those clients for aggregation as shown in line~\ref{line:start_agg}-\ref{line:end_agg} of the Algorithm~\ref{alg:sampling}.

In terms of the effect of sampling on the unsampled devices, as long as an unsampled device that joins at a later step of the training have a fairly good look-back gradient (i.e., its newly generated gradient are close to its look-back gradient (LBG)), there is no need for transmission of the entire parameter vector. This would often happen in practice unless the device engages in model training after a very long period of inactivity, in which case it would need to transmit its updated LBG before engaging in {\algName} communication savings.

\begin{algorithm}[h!]
{\footnotesize 
  \caption{{\algName} with Device Sampling}
\label{alg:sampling}
\begin{algorithmic}[1]
    \Statex \textbf{\underline{Notation:}}
    \Statex $\vec{\theta}{}{(t)}$: global model parameter at global aggregation round $t$.
    \Statex $\vec{\theta}{k}{(t, b)}$: model parameter at worker $k$, at global aggregation round $t$ and local update $b$.
    \Statex $\vec{g}{k}{(t)}$: accumulated gradient at worker $k$ at global aggregation round $t$.
    \Statex $\vec{g}{k}{\ell}$: last full gradient transmitted to server, termed look-back gradient (LBG).
    \Statex $\subsup{\alpha}{k}{(t), \ell}$: phase between the accumulated gradient $\vec{g}{k}{(t)}$ and LBG $\vec{g}{k}{\ell}$, termed look-back phase (LBP).
    %
    \Statex \textbf{\underline{Training at worker $k$:}}
    \State Update local parameters: $\vec{\theta}{k}{(t, 0)} \leftarrow \vec{\theta}{}{(t)}$, and initialize gradient accumulator: $\vec{g}{k}{(t)} \leftarrow \vec{0}{}{}$.
    \For{$b = 0$ to ($\tau$-1)}        
        \State Sample a minibatch of  datapoints $\mathcal{B}_k$ from  $\mathcal{D}_{k}$ and compute $\vec{g}{k}{(t, b)}= \sum_{d\in \mathcal{B}_k} \nabla f_k(\vec{\theta}{k}{(t, b)};d)/|\mathcal{B}_k|$.
        
        \State Update local parameters: $\vec{\theta}{k}{(t, b+1)} \leftarrow \vec{\theta}{k}{(t, b)} - \eta \cdot \vec{g}{k}{(t, b)}$, and accumulate gradient: $\vec{g}{k}{(t)} \leftarrow \vec{g}{k}{(t)} + \vec{g}{k}{(t,b)}$.
    \EndFor
        \State Calculate the LBP error:  $ \sin^2(\subsup{\alpha}{k}{(t), \ell}) = 1 - \left( \indotp{ \vec{g}{k}{(t)} }{ \vec{g}{k}{\ell}}  \big/ \big(\innorm{ \vec{g}{k}{(t)} }{} \times \innorm{ \vec{g}{k}{\ell} }{} \big) \right)^2$ \label{line:lbp}
    \If{ $\sin^2(\subsup{\alpha}{k}{(t), \ell}) \leq \subsup{\delta}{k}{\mathsf{threshold}}$ } \label{line:lbp_cond} \Comment{checking the LBP error}
        \State Send scalar LBC to the server: $\vec{\mu}{k}{(t)} \leftarrow \subsup{\rho}{k}{(t),\ell} = \indotp{ \vec{g}{k}{(t)} }{ \vec{g}{k}{\ell} }/\innorm{ \vec{g}{k}{\ell} }{}^2$.
    \Else \Comment{updating the LBG}
        \State Send actual gradient to the server: $\vec{\mu}{k}{(t)} \leftarrow \vec{g}{k}{(t)}$. \label{line:update1}
        \State Update worker-copy of LBG: $\vec{g}{k}{\ell} \leftarrow \vec{g}{k}{(t)}$. \label{line:update2}
    \EndIf
    \Statex \textbf{\underline{Global update at the aggregation server:}}
    \State Initialize global parameter $\vec{\theta}{}{(0)}$ and broadcast it across workers.
    \For{$t = 0$ to $(T-1)$}
        \State \shams{sample set of indices $K'$, a random subset from the pool of devices $\{1, 2, ..., K\}$.} \label{line:start_agg}
        \State Receive updates from workers \shams{$\{ \vec{\mu}{k}{(t)} \}_{k\in K'}$}.
        \State Update global parameters: $\vec{\theta}{}{(t+1)} \leftarrow \vec{\theta}{}{(t)} - \frac{\eta}{\shams{\abs{K'}{}}} \sum_{k \in K'}\omega_k \left[ s_k \cdot  \vec{\mu}{k}{(t)} \cdot \vec{g}{k}{\ell} + (1-s_k)\cdot \vec{\mu}{k}{(t)}\right]$, \label{line:end_agg}
        \Statex ~~~~~~where $s_k$ is an indicator function given by, $s_k = \begin{cases} 1, \quad \text{if } 
        \vec{\mu}{k}{(t)}\text{ is a scalar} \\
        0, \quad \text{otherwise, i.e., if }\vec{\mu}{k}{(t)}\text{ is a vector} \end{cases}$.
        \State Update server-copy of look-back gradients (LBGs): $ \vec{g}{k}{\ell} \leftarrow (1-s_k) \vec{\mu}{k}{(t)} + (s_k) \vec{g}{k}{\ell },~~\forall k \in K'$.
    \EndFor
\end{algorithmic}
}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{appendix_experiments}