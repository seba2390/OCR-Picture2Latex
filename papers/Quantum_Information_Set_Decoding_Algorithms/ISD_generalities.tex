\section{Generalities on classical and quantum decoding}
\label{sec:classical_quantum_decoding}
We first recall how the simplest ISD algorithm \cite{P62} and its quantised version \cite{B10} work and 
then give a skeleton of the structure of more sophisticated classical and quantum versions.
\subsection{Prange's algorithm and Bernstein's algorithm}
Recall that the goal is to find $e$ of weight $w$ given $s^T = He^T$, where $H$ is an $(n-k) \times n$ matrix. In other words, the problem we aim to solve is finding a solution to an underdetermined linear system of $n-k$ equations in $n$ variables and the  solution is unique owing to the weight condition. Prange's algorithm is based on the following observation: if it is known that $k$ given components of the error vector are zero, the error positions are among the $n-k$ remaining components. In other words, if we know for sure that the $k$ corresponding variables are not involved in the linear system, then the error vector can be found by solving the resulting linear system of $n-k$ equations in $n-k$ variables in polynomial time.

The hard part is finding a correct size-$k$ set (of indices of the components). Prange's algorithm 
samples such sets and solves the resulting linear equation until an error vector of weight $w$ is found.
The probability for finding such a set is of order 
$\Om{\frac{\binom{n-k}{w}}{\binom{n}{w}}}$ and therefore
Prange's algorithm has  
complexity 
$$\OO{ \frac{\binom{n}{w}}{\binom{n-k}{w}}}=\OOt{2^{\alpha_{\text{Prange}}(R,\omega)n}}$$
where
$$\alpha_{\text{Prange}}(R,\omega) = H_2(\omega) - (1-R)H_2\left(\frac{\omega}{1-R}\right)$$
by using the well known formula for binomials
$$
\binom{n}{w} = \Tht{2^{H_2\left( \frac{w}{n} \right)n}}.
$$
Bernstein's algorithm consists in using Grover's algorithm to find a correct size-$k$ set. Indeed, an oracle for checking that a size-$k$ set is correct can be obtained by following the same steps as in Prange's algorithm, i.e. deriving and solving a linear system of $n-k$ equations in $n-k$ variables and returning 1 iff the resulting error vector has weight $w$.
Thus the complexity of Bernstein's algorithm is the square root of that of Prange's algorithm, i.e. $\alpha_{\text{Bernstein}} = \frac{\alpha_{\text{Prange}}}{2}$.

\subsection{Generalised ISD algorithms}
More sophisticated classical ISD algorithms \cite{S88,D91,FS09,BLP11,MMT11,BJMM12,MO15} generalise Prange's algorithm in the following way: they introduce a new parameter $p$ and allow $p$ error positions 
inside of the size-$k$ set (henceforth denoted by $\zS$). Furthermore, from Dumer's algorithm onwards, a new parameter $\ell$ is introduced and the set $\zS$ is taken to be of size $k+\ell$.
This event happens with probability $P_{\ell,p} \eqdef \frac{\binom{k+\ell}{p}\binom{n-k-\ell}{w-p}}{\binom{n}{w}}.$
The point is that
\begin{restatable}{proposition}{propPuncturing}
\label{prop:puncturing}
Assume that the restriction of $H$ to the columns belonging to the complement of $\zS$ is a matrix of full rank, then
\begin{itemize}
\item[(i)]
the restriction $e'$ of the error to $\zS$  is a solution to the syndrome decoding problem
 \begin{equation}\label{eq:subproblem}
H' {e'}^T = {s'}^T.
\end{equation} 
with $H'$ being an $\ell \times (k+\ell)$ binary matrix, $|e'|=p$ and $H'$, $s'$ that can be computed in polynomial time from $\zS$, $H$ and $s$;\\
\item[(ii)] once we have such an $e'$,  there is a unique $e$ whose restriction to $\zS$ is equal to $e'$ and which satisfies
$He^T = s^T$. Such an $e$ can be computed from $e'$ in polynomial time.
\end{itemize}
\end{restatable}

\noindent
{\em Remark:} The condition in this proposition is met with very large probability when $H$ is chosen uniformly at random:
it fails to hold with probability which is only $O(2^{-\ell})$.

\begin{proof}
Without loss 
of generality assume that $\zS$ is given by the $k+\ell$ first positions.  By performing Gaussian elimination, we look for a square matrix $U$ such that 
$$
U H = \begin{pmatrix} H' & 0_\ell \\
H" & I_{n-k-\ell}\end{pmatrix}
$$
That such a matrix exists is a consequence of the fact that $H$ restricted to the last $n-k-\ell$ positions is of full rank.
Write now $e=(e',e")$ where $e'$ is the word formed by the $k+\ell$ first entries of $e$. Then
$$Us^T=UHe^T = \begin{pmatrix} H'{e'}^T \\ H"{e'}^T + {e"}^T \end{pmatrix}.$$
If we write $Us^T$ as $(s',s")^T$, where $s'^T$ is the vector formed by the $\ell$ first entries 
of $Us^T$, then we recover $e$ from $e'$
by using the fact that $H"{e'}^T + {e"}^T={s"}^T$. $~\qed$
\end{proof}
 From now on, we denote by $\Sigma$ and $h$ the functions that can be computed in polynomial time that are promised by this proposition, i.e.
\begin{eqnarray*}
s ' & = & \Sigma(s,H,\zS)\\
e & = & h(e')
\end{eqnarray*}

In other words, all these algorithms  solve in a first step a new instance of the syndrome decoding problem with different parameters. The difference
with the original problem is that if $\ell$ is small, which is the case in general, there is not a single solution anymore. 
However searching for all (or a large set of them) can be done more efficiently than just brute-forcing over all errors of weight $p$ on the set $\zS$.
Once a possible solution $e'$ to \eqref{eq:subproblem} is found, $e$ is recovered as explained before. 
The main idea which avoids brute forcing over all possible errors of weight $p$ on $\zS$ is to obtain candidates $e'$ by solving an instance of a
generalised $k$-sum problem that we define as follows.
\begin{problem}[generalised $k$-sum problem]
Consider an Abelian group $\zG$, an arbitrary set $\zE$, a map $f$ from $\zE$ to $\zG$, $k$ subsets $\zV_0$, $\zV_1$, \dots, $\zV_{k-1}$ of $\zE$, another map $g$ from $\zE^k$ to $\{0,1\}$,  and an element $S \in \zG$.
 Find
a  solution $(v_0,\dots,v_{k-1}) \in \zV_0\times \dots \zV_{k-1}$ such that we have at the same time
\begin{itemize}
\item[(i)] $f(v_0) + f(v_1) \dots + f(v_{k-1}) = S$ (subset-sum condition);
\item[(ii)] $g(v_0,\dots,v_{k-1})  =  0$   $((v_0,\dots,v_{k-1})$ is a root of $g)$.
\end{itemize}
\end{problem}

Dumer's ISD algorithm, for instance, solves the $2$-sum problem in the case where
\begin{eqnarray*}
\zG & = &\Ft^\ell, \;\;\zE = \Ft^{k+\ell},\;\;f(v)  =  H'{v}^T\\
\zV_0 &= &\{(e_0,0_{(k+\ell)/2})\in \Ft^{k+\ell} : e_0 \in \Ft^{(k+\ell)/2},\; |e_0|=p/2\}  \\
\zV_1 &= &\{(0_{(k+\ell)/2},e_1)\in \Ft^{k+\ell} : e_1 \in \Ft^{(k+\ell)/2},\; |e_1|=p/2\} 
\end{eqnarray*}
and $g(v_0,v_1)=0$ if and only if $e=h(e')$ is of weight $w$ where $e'=v_0 + v_1$.
 A solution to the $2$-sum problem is then clearly a solution to the decoding problem by construction.
The point is that the $2$-sum problem can be solved in time which is much less than $|\zV_0|\cdot|\zV_1|$. For instance,
this can clearly be achieved in expected time $|\zV_0|+|\zV_1|+\frac{|\zV_0|\cdot|\zV_1|}{|\zG|}$
and space $|\zG|$ by storing the elements $v_0$ of $\zV_0$ in a hashtable at the address $f(v_0)$ and then going over all elements $v_1$ of the other set 
to check whether or not the address $S-f(v_1)$ contains an element. The term $\frac{|\zV_0|\cdot|\zV_1|}{|\zG|}$ accounts for the expected number of solutions 
of the $2$-sum problem when the elements of $\zV_0$ and $\zV_1$ are chosen uniformly at random in $\zE$ (which is the assumption what we are going to make from on).
This is precisely what Dumer's algorithm does. Generally, the size of $\zG$ is chosen such that $|\zG|=\Th{|\zV_i|}$ and the space and time complexity are
also of this order.


Generalised ISD algorithms are thus composed of a loop in which first a set $\zS$ is sampled and then an error vector having a certain form, namely with $p$ error positions in $\zS$ and $w-p$ error positions outside of $\zS$, is sought. Thus, 
for each ISD algorithm $A$, we will denote by $Search_A$ the algorithm
 whose exact implementation depends on $A$ but whose specification is always\\
$Search_A : \zS, H, s, w, p \rightarrow \{ e ~|~ \text{$e$ has weight $p$ on $\zS$ and weight $w-p$ on $\overline{\zS}$ and } s^T = He^T\} \cup \{NULL\}$, 
 where $\zS$ is a set of indices, $H$ is the parity-check matrix of the code and $s$ is the syndrome of the error we are looking for.
The following pseudo-code gives the structure of a generalised ISD algorithm.\\
\begin{algorithm}[H]
 \DontPrintSemicolon
 \KwIn{$H$, $s$, $w$, $p$}
 \KwOut{$e$ of weight $w$ such that $s^T = He^T$}
 \RepeatU{$|e| = w$}{
  Sample a set of indices $\zS \subset \{1, ...,n\}$\;
  $e \leftarrow Search_A(\zS, H, s, w, p)$\;
 }
 \KwRet $e$\;
 \caption{ISD\_Skeleton}
\end{algorithm}
$~$
Thus, if we note $P_A$ the probability, dependent on the algorithm $A$, that the sampled set $\zS$ is correct and that $A$ finds $e$
\footnote{In the case of Dumer's algorithm, for instance, even if the restriction of $e$ to $\zS$ is of weight $p$, Dumer's algorithm
may fail to find it since it does not split evenly on both sides of the bipartition of $\zS$.}, and $T_A$ the execution time of the algorithm $Search_A$, the complexity of generalised ISD algorithms is 
$\OO{\frac{T_A}{P_A}}.$
To construct generalised quantum ISD algorithms, we use Bernstein's idea of using Grover search to look for a correct set $\zS$. 
However, now each query made by Grover search will take time which is essentially the time complexity of $Search_A$.
Consequently, the complexity of generalised quantum ISD algorithms is given by the following formula:
\begin{equation}
\label{eq:T_quant_ISD}
\OO{\frac{T_A}{\sqrt{P_A}}} = \OO{\sqrt{\frac{T_A^2}{P_A}}}.
\end{equation}
An immediate consequence of this formula is that, in order to halve the complexity exponent of a given classical algorithm, we need a quantum algorithm whose search subroutine is ``twice'' as efficient.

