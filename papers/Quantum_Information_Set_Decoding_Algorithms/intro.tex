\section{Introduction}

As humanity's technological prowess improves, quantum computers have moved from the realm of theoretical constructs to that of objects whose consequences for our other technologies, such as cryptography, must be taken into account. Indeed, currently prevalent public-key cryptosystems such as RSA and ECDH are vulnerable to Shor's algorithm \cite{S97}, which solves factorisation and the discrete logarithm problem in polynomial time. Thus, in order to find a suitable replacement, it has become necessary to study the impact of quantum computers on other candidate cryptosystems. Code-based cryptosystems such as the McEliece \cite{M78} and the Niederreiter \cite{N86} cryptosystems are such possible candidates. 

Their security essentially relies on decoding a linear code. 
Recall that the decoding problem consists, when given a linear code $\sC$ and a noisy codeword $c+e$, in recovering $c$, where $c$ is an unknown codeword of 
$\sC$ and $e$ an unknown error of Hamming weight $w$. A (binary) linear code $\sC$ of dimension $k$ and length $n$ is specified by 
a full rank binary matrix $H$ (i.e. a parity-check matrix) of size $(n-k)\times n$ as 
$$\sC = \{c \in \Ft^n:Hc^T = 0\}.$$
 Since $H(c+e)^T = Hc^T + He^T=He^T$ the decoding problem
can be rephrased as a syndome decoding problem

\begin{problem}[Syndrome Decoding Problem]
Given $H$ and $s^T = He^T$, where $|e| = w$, find $e$.
\end{problem}
This problem has been studied since the Sixties and despite significant efforts on this issue \cite{P62,S88,D91,B97b,MMT11,BLP11,BJMM12,MO15}
the best algorithms for solving this problem \cite{BJMM12,MO15} are exponential in the number of errors that have to be corrected:
correcting $w$ errors in a binary linear code of length $n$ and dimension $k$  has with the aforementioned algorithms a cost  of
$\tilde O(2^{\alpha(\frac{k}{n}, \frac{w}{n})n})$ where $\alpha(R,\omega)$ is positive when $R$ and $\omega$ 
 are both positive.
All these algorithms use in a crucial way the original idea due to Prange \cite{P62} and are known under the name
of Information Set Decoding (ISD) algorithms: they all take advantage of the fact that there might exist a rather large set of positions
containing an information set of the code\footnote{An information set of a linear code $C$ of dimension $k$ is a set $\sI$ of $k$ positions such that 
when given $\{c_i:i \in \sI\}$ the codeword $c$  of $C$ is specified entirely.}
that is almost error free.

All the efforts that have been spent on this problem have only managed to decrease slightly this exponent $\alpha(R,\omega)$.
The following table gives an overview of the average time complexity of currently existing classical algorithms 
when $w$ is the Gilbert-Varshamov distance $d_{\text{GV}}(n,k)$ of the code. 
This quantity is defined by $d_{\text{GV}}(n,k) \eqdef n H_2^{-1}\left(1 - \frac{k}{n}\right)$ where $H_2$ is the binary entropy function $H_2(x) \eqdef -x \log_2(x)-(1-x)\log_2(1-x)$ and $H_2^{-1}$ its inverse
defined from $[0,1]$ to $[0,\frac{1}{2}]$. It corresponds to the largest distance for which we may still expect a unique solution to the decoding problem. If we want uniqueness of the solution, it can therefore be 
considered as the hardest instance of decoding.
In the following table, $\omega_{\text{GV}}$ is defined by the ratio $\omega_{\text{GV}} \eqdef d_{\text{GV}}(n,k)/n$.
\begin{center}
      \begin{tabular}{| c | c | c |}
      \hline
      \textbf{Author(s)} & Year & $\underset{0 \leq R \leq 1}{\max}\alpha(R,\omega_{\text{GV}})$ to 4 dec. places\\
      \hline
      Prange \cite{P62} & 1962 & 0.1207 \\
      \hline
      Dumer \cite{D91} & 1991 & 0.1164 \\
      \hline
      MMT \cite{MMT11} & 2011 & 0.1114 \\
      \hline
      BJMM \cite{BJMM12} & 2012 & 0.1019\\
      \hline
      MO \cite{MO15}& 2015 &  0.0966\\
      \hline
      \end{tabular}\\
      \end{center}
The question of using quantum algorithms to speed up ISD decoding algorithms was first put forward  in \cite{OS09}.
However, the way Grover's algorithm was used in \cite[Subsec. 3.5]{OS09} to speed up decoding did not allow for significant improvements over classical ISD algorithms.
Later on, it was shown  by Bernstein in \cite{B10} that it is possible to obtain much better speedups with Grover's algorithm: by using it for finding an error-free information set, the exponent of Prange's algorithm can indeed be halved. 

This paper builds upon this way of using Grover's search algorithm, as well as the quantum algorithms developped by Bernstein, Jeffery, Lange and Meurer in \cite{BJLM13} to solve the subset sum problem more efficiently.
The following table summarises the ingredients and average time complexity of the  algorithm of \cite{B10} and the new quantum algorithms presented in this paper.
\begin{center}
      \begin{tabular}{| c | c | c | c |}
      \hline
      \textbf{Author(s)} & Year & Ingredients & $\underset{0 \leq R \leq 1}{\max}\alpha(R,\omega_{\text{GV}})$ to 5 dec. places\\
      \hline
      Bernstein \cite{B10} & 2010 & Prange+Grover & 0.06035 \\
      \hline
      This paper & 2017 & Shamir-Schroeppel+Grover+QuantumWalk & 0.05970 \\
      \hline
      This paper & 2017 & MMT+``1+1=0''+Grover+QuantumWalk & 0.05869 \\
      \hline
      \end{tabular}\\
  \end{center}
A quick calculation shows that the complexity exponent of our best quantum algorithm, $MMTQW$, fulfils $\alpha_{\text{MMTQW}} \approx \frac{\alpha_{\text{Dumer}}}{2} + 4.9 \times 10^{-4}$. Thus, our best quantum algorithm improves in a small but non-trivial way on \cite{B10}. Several reasons will be given throughout this paper on why it has been difficult to do better than this.

\noindent
{\bf Notation.} Throughout the paper, we denote by $|e|$ the Hamming weight of a vector $e$. We use the same notation for denoting the cardinality of a set, i.e. $|\zS|$ denotes the cardinality of the set $\zS$. 
The meaning of this notation  will be clear from the context
and we will use calligraphic letters to denote sets: $\zS,\sI,\zM,\dots$. 
We use the standard $\OO{}$, $\Om{}$, $\Th{}$ notation and use the less standard 
$\OOt{}$, $\Omt{}$, $\Tht{}$ notation to mean ``$\OO{}$, $\Om{}$, $\Th{}$, when we ignore logarithmic factors''. 
Here all the quantities we are interested in are functions of the codelength $n$ and we write 
$f(n) = \OOt{g(n)}$ for instance, when there exists a constant $k$ such such that 
$f(n) = \OO{g(n) \log^k(g(n))}$.
