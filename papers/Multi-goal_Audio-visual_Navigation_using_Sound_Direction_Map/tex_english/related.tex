\section{RELATED WORK}

\subsection{Sound Source Localization}

Sound source localization is the estimation of the location of the sound source from the observed sound data \cite{rascon2017localization}.
In recent years, many methods based on deep learning have been proposed~\cite{grumiaux2022survey,ma2015exploiting,he2018deep}.
Some of these, such as Ma et al. \cite{ma2015exploiting} and He et al. \cite{he2018deep}, solve the problem of localizing multiple sound sources.
These methods are based on multilayer perceptron (MLP) and convolutional neural network (CNN) to estimate the direction of arrival of multiple sound sources.
Furthermore, Adavanne et al. \cite{adavanne2019localization} proposed a method using a convolutional recurrent neural network (CRNN) to track the direction of arrival of multiple sound sources in a situation where the sound sources are moving.

This paper also proposes a method for multiple sound source localization using deep learning.
In particular, we propose a method using reinforcement learning that dynamically localizes multiple sound sources while the observer moves.


\subsection{Visual Navigation}

Visual navigation is the process by which an autonomous mobile robot placed in an environment generates an appropriate and safe path from the start to a goal by using visual information \cite{bonin2008visual}.
In visual navigation in the three-dimensional indoor environment addressed in this study, first-person RGBD images are often used as the visual input.
Visual navigation requires the use of such visual information to understand the environment while estimating self and target positions and making appropriate action choices.

In recent years, many methods for solving visual navigation based on deep reinforcement learning have been proposed.
Parisotto et al. \cite{DBLP:journals/corr/ParisottoS17} proposed NeuralMap, a method to adaptively create a two-dimensional map to remember the spatial structure of the environment, based on the importance of memory for the partial observability of navigation.
Chaplot et al. \cite{chaplot2019learning, chaplot2020object} also proposed a method that simultaneously creates a map and estimates its own pose.
While these methods explicitly create maps, there are some methods that do not.
Fang et al. \cite{fang2019scene} and Fukushima et al. \cite{9812027} proposed a method that stores the embedding of observations as history and allows long-term memory consideration by using Transformer~\cite{vaswani2017attention}. 
A method is also proposed to improve navigation performance by simultaneously solving auxiliary tasks not directly related to navigation, such as RGB image depth prediction and closed-loop detection \cite{mirowski2017learning, 9370169}.

The closest prior work on visual navigation to this study is Wani et al. \cite{wani2020multion}, which proposed a task called multi-object navigation (MultiON).
In MultiON, navigation must be performed to multiple specified objects in a specified order.
% Therefore, it is a task similar to this study in the sense that it requires navigation to multiple goals.
In contrast, the order in which the goals should be reached is not determined in our multi-goal audio-visual navigation handled in this study.
It is therefore up to the agents themselves to decide which goal they want to achieve first.


\subsection{Audio-Visual Navigation}

In audio-visual navigation, the agent uses auditory as well as visual information to navigate to a sound source.
Auditory information may be useful for estimating self and target locations, which are important for navigation tasks, and for understanding the geometric structure of the environment.
Indeed, Chen et al. \cite{chen2020soundspaces} experimentally demonstrated the usefulness of using auditory information in navigation.

In recent years, this audio-visual navigation has been actively studied and a variety of tasks have been proposed.
Majumder and Pandey~\cite{majumdersemantic} proposed audio-visual navigation that uses a sound source other than the target as the interfering sound.
In addition, while conventional audio-visual navigation assumes that the sound never stops playing, Chen et al. \cite{chen2021semantic} proposed an audio-visual navigation framework in which the sound only plays for a short period immediately after the start of the episode.
Also, some proposed one in which the sound source moves \cite{younes2021catch, yu2021sound} and another realistic one in which the agent searches for a fallen object \cite{gan2022finding}.

Several methods for solving audio-visual navigation have already been proposed.
Chen et al. \cite{chen2020soundspaces} proposed AV-Nav, a simple method to obtain policy by inputting image features and spectrogram features into a gated recurrent unit (GRU)~\cite{chung2014empirical}. 
Then, Chen et al. \cite{chen2020learning} proposed AV-WaN, a method that uses the observed depth image and spectrogram to create a geographic map of the environment and an acoustic map that stores the sound intensity at that location to generate the next waypoint to be reached. 
Chen et al. \cite{chen2021semantic} proposed a Transformer-based policy network and a module that predicts the category and relative location of a sound source.
Furthermore, Tatiya et al. \cite{tatiya2022knowledge} proposed K-SAVEN, a knowledge-driven method that utilizes a knowledge graph, which represents object-to-object and object-to-region relationships, and extracts features from it using graph convolutional network \cite{kipf2017semisupervised}.

While a variety of tasks and methods have been studied, the task of navigating to multiple sound sources in a single episode has not been studied.
Therefore, this study is the first to address this task.

For the performance evaluation, SoundSpaces~\cite{chen2020soundspaces, chen2022soundspaces} is often used as a simulator in previous studies on audio-visual navigation.
There are two versions of SoundSpaces: 1.0~\cite{chen2020soundspaces} and 2.0~\cite{chen2022soundspaces}, where 
2.0 is closer to a realistic setting.
Therefore, 2.0 was used in this study.


