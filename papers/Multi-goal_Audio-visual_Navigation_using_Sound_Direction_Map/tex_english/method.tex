\section{METHODS}


\subsection{Baselines}

We will test two well-known deep reinforcement learning methods as baseline methods, in addition to the simplest random action selection method.
Each method is described in detail below.

\subsubsection{Random}
This agent randomly chooses an action from among $\{MoveForward, TurnLeft, TurnRight\}$.
However, if the agent is within a radius of less than $1\ \mathrm{m}$ of the goal sound source, it will always choose $Found$.

\subsubsection{AV-Nav \cite{chen2020soundspaces}}
An end-to-end deep reinforcement learning method.
It has a GRU-based policy network with first-person images and spectrograms as input.
It is the first and simplest method proposed for audio-visual navigation tasks.

\subsubsection{SAVi \cite{chen2021semantic}}
A deep reinforcement learning method with a Transformer-based policy network.
In addition to first-person images and spectrograms, the agent's position, orientation, and previous actions are used as input.
Also, it has a goal descriptor network that predicts the category and the location of the goal sound source. The category prediction part is pre-trained.
The learning of policy network is divided into two stages.
In the first stage, the memory size of the Transformer is set to 1 and the observation embedding is learned.
In the second stage, the learning of the observation embedding is frozen and the memory size is increased to 150 to learn the rest of the network.

Since the target task of this study is not semantic audio-visual navigation~\cite{chen2021semantic} and also due to the difficulty of extending to multiple goals, the goal descriptor network was not used in this study\footnote{We actually tested SAVi with the goal descriptor network, but the performance was degraded.}.

\subsection{Proposed Method}

% \begin{figure}[t]
%     \begin{center}
%         \centering
%         \includegraphics[width=0.9\linewidth]{fig/direct_map.pdf}
%         \caption{
%             Overview of the sound direction map (SDM).
%             The SDM is represented by black circles surrounding an agent.
%             Here is an example of a case with two sound sources.
%             The color of the node represents the increase in the value of the SDM node in the direction of the sound source, as seen from the agent.
%         }
%         \label{fig:direct_map_abst}
%     \end{center}
% \end{figure}

\begin{figure*}[t]
    \begin{center}
        \centering
        \includegraphics[scale=0.6]{fig/direct-map-encoder.pdf}
        \caption{
            The application of the network for SDM creation to the network architecture of the AV-Nav \cite{chen2020soundspaces}.
            SDM Encoder is the proposed network architecture for SDM creation.
            Here, $\boldsymbol{a}_{t}$ and $\hat{\boldsymbol{d}}_t$ represent one-hot vector representing action and SDM prediction at time $t$, respectively.
            The input includes not only the sound observation $A_t$ but also one-hot vector $\boldsymbol{a}_{t-1}$ representing the previous action and the prediction of the previous SDM $\hat{\boldsymbol{d}}_{t-1}$.
        }
        \label{fig:direct_map_encoder}
    \end{center}
\end{figure*}

In the following, we describe a proposed method, sound direction map (SDM), which allows for explicit and dynamic localization of multiple sound sources using memory.
The SDM is a representation of how far and in which direction the sound source is located from the agent.
On the left of Fig.~\ref{fig:direct_map_encoder}, SDM is represented by black nodes surrounding the agent.
This is an example of a case with two sound sources.
In SDM, the closer the distance from the agent to the sound source, the higher the value of the node in the direction of that sound source.
The color of the node represents the increase in the value of the SDM node.
In this way, SDM allows localization by direction and distance for multiple sound sources.
However, if there are multiple sound sources in the same direction, only the closest sound source is considered.
If the agent is unable to separate the sound sources, accurately selecting the closer one and predicting the distance may be difficult for the agent.

The value of each node is the reciprocal of the geodesic distance to the sound source in that direction.
The reason for using geodesic distance rather than Euclidean distance is that sound waves are reflected, diffracted, and attenuated by walls and other obstacles. Geodesic distance is therefore considered to be more predictable.
In addition, clipping was used for sound sources within $1\ \mathrm{m}$ of the geodesic distance.
There are two reasons for this. One is that the agents do not need to perform strict localization as long as they are within a radius of $1\ \mathrm{m}$.
The other is to avoid overreacting to very large values when training the encoders that create the SDM.

We propose a method that uses a neural network to dynamically predict SDM while the agent repeats its actions.
In this study, SDMs were applied to the AV-Nav \cite{chen2020soundspaces} and SAVi \cite{chen2021semantic} networks.
The whole network is trained in an end-to-end manner.
The proposed neural network architecture when AV-Nav~\cite{chen2020soundspaces} is used as the backbone is shown in Fig.~\ref{fig:direct_map_encoder}.
The current audio observation, the previous action, and the previous SDM are used to predict the current SDM. The current audio observation is necessary to predict the location of sound sources.
In SAVi, we have added SDM encoder as part of the observation encoder.
Therefore, in SAVi, SDM encoder is trained only in the first stage.
In addition to the gradients flowing from the policy networks, the gradients from the Mean Squared Error (MSE) between the predicted SDM and the true SDM were used to update the weights of the SDM encoder.

Also, when training, the previous SDM's true value $\boldsymbol{d}_{t-1}$ is input as the previous SDM.
In our preliminary experiments, we also tested the case in which the previous own prediction $\hat{\boldsymbol{d}}_{t-1}$ was input as the previous SDM for training.
Experiments showed that learning by inputting true values rather than predictions performed better, so that method was used.
However, when testing, we did not use the true value but the prediction value.
We also used the prediction value for the second stage of SAVi training.

In addition, Dropout was used during training.
This means that the value of each node in the SDM that is input as the previous SDM is set to 0 with a certain probability.
The purpose of this is to prevent too much reliance on the previous SDM when predicting the current SDM.
This is because the predicted previous SDM $\hat{\boldsymbol{d}}_{t-1}$ can fail to be close to the true previous SDM $\boldsymbol{d}_{t-1}$ properly.
Note that Dropout is not performed in the SAVi's second stage of training.

\subsection{Reward}

The reward received by the agent at time $t$ is defined as
$
r_t = r_{\mathrm{found}} - \Delta_{\mathrm{geo}} - 0.01, \label{eq:reward_definition}
$
where $r_{\mathrm{found}}$ is $5$ if the agent reached the goal at time $t$ and $0$ otherwise.
Also, $\Delta_{\mathrm{geo}} \in \mathbb{R}$ is the  change of minimum geodesic distance to reach all goals that have not yet been reached.
In other words, $\Delta_{\mathrm{geo}}$ is negative when the minimum geodesic distance to reach all goals not yet reached becomes small and positive when it becomes large.



\subsection{Training}

The agents were trained using a distributed deep reinforcement learning method called decentralized distributed proximal policy optimization (DD-PPO) \cite{wijmans2019dd}.
For training with AV-Nav, 4 GPUs were used and four workers were placed on each GPU.
For training with SAVi, 16 GPUs were used and two workers were placed on each GPU.
The SAVi was found to take longer to train than AV-Nav, so the hardware set-up was modified in this way.

The structure of 1D CNN and MLP in SDM Encoder is as follows.
First, the 1D CNN has four layers.
The number of output channels is 32, the size of the kernel is 3, circular padding is used for padding, and ReLU is used as the activation function.
Second, MLP has 4 layers.
The output sizes are 1048, 1048, 524, and 8 in this order.
For the activation function, a sigmoid function is used only for the last layer, and ReLU is used for the other layers.

The hyperparameters in this study were set as follows.
First, the number of SDM nodes was set to $8$, and the probability of Dropout being performed in each node was set to $0.2$.
Furthermore, in the gradient when updating the weights of the SDM encoder, the coefficient of the gradient due to the MSE between the predicted SDM and the true SDM is set to $100$.
In preliminary experiments, 1, 10, 100, and 1000 were tried, and 100 was adopted because it gave the best performance.
In addition, the default parameters of SoundSpaces were used for the hyperparameters related to AV-Nav and SAVi.
The number of times the weights are updated is 1,250 times for AV-Nav, while for SAVi the first stage is 1,190 times and the second stage is 810 times.



