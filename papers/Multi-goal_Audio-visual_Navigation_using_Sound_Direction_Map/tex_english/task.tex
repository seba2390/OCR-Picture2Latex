\section{Multi-Goal Audio-Visual Navigation}

\subsection{Task Definitions}


Multi-goal audio-visual navigation proposed in this paper is an audio-visual navigation that provides navigation to multiple goals in a single episode (Fig.~\ref{fig:multi_goal_av_nav}).
Therefore, the agent must localize each sound source by observing multiple overlapping sounds.
Another major difference from MultiON, other than the use of auditory information, is that the order in which the navigation to the goal is performed is not specified.
Therefore, the agent must consider in what order it is efficient to navigate to each sound source.
In the following, a more detailed explanation of multi-goal audio-visual navigation is given.

Formally, an episode is defined by a set
$
\{E, \bm{p}_s, \theta_s, \bm{p}_{g_1}, ... , 
$
$
\bm{p}_{g_n}, S_{g_1}, ... , S_{g_n}\},
$
where $E$ represents the scene environment used in the episode, and $\bm{p}_s \in \mathbb{R}^3$ and $\theta_s \in [0, 2\pi]$ represent the starting position and direction the agent is facing, respectively.
Also, $n \in \mathbb{N}$ represents the number of goals, and for each $i \in \{1, ... , n\}$, $\bm{p}_{g_i} \in \mathbb{R}^3$ and $S_{g_i} \in SoundCategory$ represent the location of goal $g_i$ and the sound source category, respectively.
Here, $SoundCategory$ represents a set of sound categories. 
In this study, this includes, for example, \textit{telephone} and \textit{birdsong}.
A total of $91$ of sound categories are used in this study.
From the above, it can be said that multi-goal audio-visual navigation is to move from the start $(\bm{p}_s, \theta_s)$, listen to the overlapping sounds of $S_{g_1}, ..., S_{g_n}$, estimate the goal positions $\bm{p}_{g_1}, ... \bm{p}_{g_n}$, and reach them in environment $E$.

The multi-goal audio-visual navigation uses a goal format named AudioGoal~\cite{chen2020soundspaces}.
AudioGoal means that the goal is a sound source, and its location must be deduced from the information of the periodically generated sound.
Since the goal is not visually indicated, the position of the goal must be estimated based on auditory information only.

Also, when an agent reaches a goal, the sound source at that goal does not emit any sound thereafter.
In other words, once the agent reaches goal $g_i$, the agent no longer observes the sound of $S_{g_i}$.
Therefore, the sounds currently observed by the agent are emitted from sound sources that have not yet reached the goal.
The agent does not need to determine whether the sound it is currently observing is emanating from a sound source that has already arrived or from a sound source that has not yet arrived.




\subsection{Action Space}

In multi-goal audio-visual navigation, the agent's action space is
$
\{MoveForward, TurnLeft, TurnRight,
$
$
Found\}.
$
If $MoveForward$ is selected, the agent will move forward $0.25\ \mathrm{m}$ in the environment.
If $TurnLeft$ or $TurnRight$ is selected, the agent will rotate $10^{\circ}$ to the left or right, respectively.
If the agent could select a $Found$ in a radius less than $1\ \mathrm{m}$ of the goal sound source, it means that the agent has reached that goal.
The above specific values are the default settings in SoundSpaces 2.0.

An episode ends when one of the following three conditions is met.
The first is the case that all goals are reached.
The second is the case that $Found$ is selected at least $1\ \mathrm{m}$ radius away from the goal.
The third is the case that the total number of actions exceeds 2,500.
The upper limit of the number of actions was set in the previous study \cite{wani2020multion}.

\subsection{Observation Space}

The agent can observe visual information and auditory information in multi-goal audio-visual navigation.
For visual information, we use a first-person $128 \times 128$ RGBD image.
For auditory information, a $257 \times 69$ spectrogram is used.
As for the auditory information, the two-channel binaural sound is used, following previous studies.

The procedure for creating the spectrogram in this study is as follows.
First, we acquire a time series of discrete sound data for $0.25$ seconds sampled at a sampling frequency of 44,100$\ \mathrm{Hz}$.
Next, a short-time Fourier transform is performed on the time series data to obtain the amplitudes of the components of each frequency at each time.
Here, the window function is the Hanning window, a windowed signal length is $512$, and a hop length is $160$.
Finally, a spectrogram is created by adding $1$ to each value and taking the logarithm of the result as the strength of each component.


\subsection{Metrics}


Four evaluation indicators were used in this study: $SUCCESS$, $SPL$, $PROGRESS$, and $PPL$.
These are described in detail below.

The $SUCCESS$ is represented by the following, where $N$ is the number of episodes tested.
\[
SUCCESS = \frac{1}{N} \sum_{i = 1}^N S_i,
\]
where $S_i \in \{0, 1\}$ is the binary value of whether all goals were reached in the episode $i \in \{1, ..., N\}$.
That is, $S_i = 1$ if the agent was able to reach all goals in the $i$th episode, and $S_i = 0$ if the agent was unable to reach even one goal.

The $SPL$ (short for success weighted by path length) is represented by the following \cite{anderson2018evaluation}:
\[
SPL = \frac{1}{N} \sum_{i = 1}^N S_i \frac{l_i}{\max(l^A_i, l_i)}.
\]
Here, $l^A_i \in \mathbb{R}$ is the length of the path the agent took, and $l_i \in \mathbb{R}$ is the length of the shortest path to reach all goals.
In other words, even if the agent can reach all goals, $SPL$ will not be high if it does not proceed along a path that is close to the shortest path.

The $PROGRESS$ is represented by the following \cite{wani2020multion}:
\[
PROGRESS = \frac{1}{N} \sum_{i = 1}^N \frac{n^A_i}{n},
\]
where $n \in \mathbb{N}$ is the number of goals and $n^A_i \in \mathbb{N}$ is the number of goals reached by the agent in episode $i$.
In other words, unlike $SUCCESS$, the value will not be $0$ if even one goal is reached, even if not all goals are reached.

The $PPL$ (short for progress weighted by path length) is represented by the following \cite{wani2020multion}:
\[
PPL = \frac{1}{N} \sum_{i = 1}^N \frac{n^A_i}{n} \frac{l^\mathrm{MG}_i}{\max(l^A_i, l^\mathrm{MG}_i)},
\]
where $l^\mathrm{MG}_i \in \mathbb{R}$ is the length of the shortest path from the starting point to pass through all goal points reached by the agent.
In other words, $PPL$ has a higher value when more goals are reached by a path that is closer to the shortest path.
Unlike $SPL$, the value will not be $0$ if even one goal is reached, even if not all goals are reached.

However, unlike Wani et al.~\cite{wani2020multion}, the order in which goals should be reached is not determined in this study.
Therefore, the calculation method of $l^\mathrm{MG}_i$ is different.
Suppose that in episode $i$ the agents reach the goal in order $g_1, ... , g_{n^A_i}$, and the starting position is $\bm{p}_s$.
Also assume that the length of the shortest path between points $\bm{p}$ and $\bm{q}$ can be expressed as $d_{\mathrm{min}}(\bm{p}, \bm{q})$.
In this case, in Wani et al. \cite{wani2020multion}, 
\[
l^\mathrm{MG}_i = d_{\mathrm{min}}(\bm{p}_{s}, \bm{p}_{g_1}) + \sum_{j=2}^{n^A_i} d_{\mathrm{min}}(\bm{p}_{g_{j-1}}, \bm{p}_{g_j}).
\]
Since the order of goals to be reached in multi-goal audio-visual navigation task is not determined, it should be calculated as follows.
\[
l^\mathrm{MG}_i = \min_{\sigma \in T_{n^A_i}} \!\left\{\! d_{\mathrm{min}}(\bm{p}_{s}, \bm{p}_{g_{\sigma(1)}}) +  \sum_{j=2}^{n^A_i} d_{\mathrm{min}}(\bm{p}_{g_{\sigma(j-1)}}, \bm{p}_{g_{\sigma(j)}}) \!\right\}
\]
where $T_{n^A_i}$ is the entire set of permutations of $n^A_i$ numbers $1, 2, ... ,n^A_i$.