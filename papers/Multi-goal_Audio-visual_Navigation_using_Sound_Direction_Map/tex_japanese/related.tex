\section{RELATED WORK}

\subsection{Sound Source Localization}

Sound Source Localization (SSL) とは、観測した音のデータから、その音の音源の位置を推定することである\cite{rascon2017localization}。
近年では、この音源定位を深層学習を用いて行う手法が数多く提案されている\cite{grumiaux2022survey}。
これらの中には、Maら\cite{ma2015exploiting}やHeら\cite{he2018deep}といった、本研究と同じで複数の音源を定位する問題を解いているものもある。
これらでは、Multilayer Perceptron (MLP)やConvolutional Neural Network (CNN)をベースとして複数の音源の到来方向を推定する手法が提案された。
さらに、Adavanneら\cite{adavanne2019localization}では、Convolutional Recurrent Neural Network (CRNN)を用いることで、音源が動いている状況下で複数の音源の到来方向を追跡する手法が提案されている。

本論文でも、深層学習を用いることで音源定位を行う手法を提案する。
特に、定位するべき音源が複数がある状況で、強化学習手法によって観測者が動きながら、動的に音源定位を行う手法を提案する。


\subsection{Visual Navigation}

Visual Navigationとは、ある環境に置かれた自律移動ロボットが、視覚の情報を用いることで、スタートからゴールまで適切かつ安全な経路を生成するプロセスのことである\cite{bonin2008visual}。
本研究で扱う三次元の室内環境における視覚ナビゲーションでは、視覚の情報として、一人称視点のRGBD画像が用いられることが多い。
視覚ナビゲーションでは、このような視覚の情報を利用することで環境を理解しながら、自己位置や目標位置を推定し、適切な行動選択を行うことが要求される。

視覚ナビゲーションを解くための手法は数多く提案されており、近年ではそのほとんどが深層強化学習をベースとしたものになっている。
Parisottoら\cite{DBLP:journals/corr/ParisottoS17}は、ナビゲーションの部分観測性には記憶が重要であるということに着目し、適応的に環境の空間的構造を記憶するための二次元マップを作成する手法Neural Mapを提案した。
また、Chaplotら\cite{chaplot2019learning, chaplot2020object}は、マップの作成と同時に自己の姿勢推定も行う手法を提案した。
これらは明示的にマップを作成する手法である一方で、明示的にマップを作成しない手法も存在する。
Fangら\cite{fang2019scene}やFukushimaら\cite{9812027}は、観測の埋め込みを履歴として保存し、Transformer~\cite{vaswani2017attention}を用いることで長期的な記憶の考慮を可能にした手法を提案した。
また、RGB画像の深度予測や閉ループ検出などのナビゲーションと直接は関係しない補助タスクを同時に解くことで、ナビゲーションの性能を高めようとする手法も提案されている\cite{mirowski2017learning, 9370169}。

本研究に最も近い視覚ナビゲーションの先行研究は、Waniら\cite{wani2020multion}である。
Waniら\cite{wani2020multion}では、Multi-Object Navigation (MultiON) というタスクが提案された。
MultiONでは、指定された複数の物体に、指定された順番でナビゲーションを行わなければならない。
したがって、複数のゴールへナビゲーションを行わなければならないという意味で、本研究に近いタスクであるといえる。
ただし、MultiONと違い、本研究で扱うマルチゴール視聴覚ナビゲーションではゴールに到達するべき順番は決められていない。
よって、どのゴールから向かうべきかをエージェント自身が考えなければならない。


\subsection{Audio-Visual Navigation}

Audio-Visual Navigationとは、視覚だけではなく、聴覚の情報も用いて、音源までナビゲーションを行うものである。
聴覚の情報は、ナビゲーションタスクに重要な自己位置と目標位置の推定や、環境の幾何学的構造の理解に役立つ可能性がある。
実際、Chenら\cite{chen2020soundspaces}は実験的に、ナビゲーションで聴覚の情報を用いることの有用性を示している。

近年では、この視聴覚ナビゲーションについて盛んに研究が行われており、多様なタスクが提案されてきている。
Majumder and Pandey~\cite{majumdersemantic}では、妨害音として目標以外の音源を用いる視聴覚ナビゲーションが提案された。
また、従来の視聴覚ナビゲーションでは音が鳴り止まないことが仮定されていたが、Chenら\cite{chen2021semantic}では、エピソード開始直後の短時間しか音が鳴らない視聴覚ナビゲーションが提案された。
さらに、音源が動くといったもの\cite{younes2021catch, yu2021sound}や、落ちた物体を探すというかなり現実的なもの\cite{gan2022finding}までも提案されてきている。

また、視聴覚ナビゲーションを解く手法もすでにいくつか提案されている。
Chenら\cite{chen2020soundspaces}は、視覚の観測である画像の特徴量と聴覚の観測であるスペクトログラムの特徴量を、Gated Recurrent Unit (GRU)~\cite{chung2014empirical}に入力して方策を得るというシンプルな手法AV-Navを提案した。
ついでChenら\cite{chen2020learning}は、観測した深度画像とスペクトログラムから、環境の地理的なマップとその場所における音の強度を保存するアコースティックマップを作成し、次に到達するべきウェイポイントを生成するという手法AV-WaNを提案した。
そしてChenら\cite{chen2021semantic}は、方策ネットワークをTransformerベースにし、かつ音源のカテゴリと相対位置を予測するモジュールも提案した。
さらに、Tatiyaら\cite{tatiya2022knowledge}は、物体間と物体-領域間の関係を表すKnowledge Graphを活用し、そこからGraph Convolutional Network (GCN)~\cite{kipf2017semisupervised}を用いて特徴量を抽出するという知識ドリブンな手法K-SAVENを提案した。

多様なタスクや手法について研究されている一方で、一つのエピソードで複数の音源へナビゲーションを行うタスクは研究されていない。
したがって、この問題を扱うのは本研究がはじめてである。

また、視聴覚ナビゲーションに関する先行研究では、シミュレータとしてSoundSpaces~\cite{chen2020soundspaces, chen2022soundspaces}がよく用いられる。
SoundSpacesには、1.0~\cite{chen2020soundspaces}と 2.0~\cite{chen2022soundspaces}の二つのバージョンがある。
2.0はより現実的な設定に近づいている。
そのため、本研究では2.0を用いた。


