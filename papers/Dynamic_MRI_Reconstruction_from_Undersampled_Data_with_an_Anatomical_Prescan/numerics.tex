\section{Numerical implementation and solution}
In this section, we explain how to implement and solve the minimization problem \eqref{eq:dynamic_recon} numerically which, depending on the amount of time steps $T$, can be challenging. 
We derive a general primal-dual algorithm for its solution, before we line out some strategies to reduce computational costs and speed up the implementation at the end of this section. 

\subsection{Gradients and sampling operators}
\label{subsec:implementation of operators}

In order to use the discrete total variation already defined in Section \ref{subsubsec:TV}, we need a discrete gradient operator that maps an image $u \in \C^N$ to its gradient $\nabla u \in \C^{N \times 2}$. 
Following \cite{ChambollePock}, we implement the gradient by standard forward differences. Moreover, we  will use its discrete adjoint, the negative divergence $-\diverg$, defined by the identity $\langle \nabla u, w \rangle_{\C^{N \times 2}} = - \langle u, \diverg(w) \rangle_{\C^N}$.
The inner product on the gradient space $\C^{N\times 2}$ is defined in a straightforward way as 
\begin{align*}
	\langle v,w \rangle_{\C^{N \times 2}} = \Real(v_1^* w_1) + \Real(v_2^* w_2),
\end{align*}
%
for $v,w \in \C^{N \times 2}$.
For $v \in \C^{N \times 2}$, the (isotropic) 1-norm is defined by 
\begin{align*}
	\|v \|_1 := \sum_{i=1}^N \sqrt{|(v_i)_1|^2 + |(v_i)_2|^2},
\end{align*}
and accordingly the dual $\infty$-norm for $w \in \C^{N \times 2}$ is given by 
\begin{align*}
	\| w \|_\infty := \max_{i=1,\cdots,N} |w_i| = \max_{i=1,\cdots,N} ~ \sqrt{|(w_i)_1|^2 + |(w_i)_2|^2}.
\end{align*}

The sampling operators $\Kcal_t \colon \C^N \to \C^{M_t}$ (and analogously for $\Kcal_0 \colon \C^{N_0} \to \C^{M_0}$) we consider are either a standard fast Fourier transform (FFT) on a Cartesian grid, followed by a projection onto the sampled frequencies, or a non-uniform fast Fourier transform (NUFFT), in case the sampled frequencies are not located on a Cartesian grid \cite{Fessler:NUFFT}. \\

\noindent {\it Fourier transform on a Cartesian grid - the simulated data case}\\
\noindent
In the numerical study on artificial data we use a simple version of the Fourier transform and sampling operator (the same can also be found in e.g. \cite{Ehrhardt2016,Rasch2017}). 
We discretize the image domain on the unit square using an (equi-spaced) Cartesian grid with $N_1 \times N_2$ pixels such that the discrete grid points are given by 
\begin{align*}
 \Omega_N = \left\{ \left(\frac{n_1}{N_1-1}, \frac{n_2}{N_2-1} \right) ~\Big|~ n_1 = 0, \dots, N_1-1, \; n_2 = 0, \dots N_2-1 \right\}.
\end{align*}
We proceed analogously with the $k$-space, i.e. the location of the $(m_1,m_2)$-th Fourier coefficient is given by $(m_1/(N_1-1), m_2/(N_2-1))$. Then, we arrive at the following formula for the (standard) Fourier transform $\Fcal$ applied to $u \in \C^{N_1 \times N_2}$:
\begin{align*}
 (\Fcal u)_{m_1,m_2} = \frac{1}{N_1 N_2} \sum_{n_1=0}^{N_1-1} \sum_{n_2=0}^{N_2-1} u_{n_1,n_2} e^{-2\pi i \left(\frac{n_1 m_1}{N_1} + \frac{n_2 m_2}{N_2} \right)},
\end{align*}
where $ m_1 = 0, \dots, N_1-1, m_2 = 0, \dots, N_2 -1$.
For simplicity, we use a vectorized version such that $\Fcal \colon \C^N \to \C^N$ with $N = N_1 \cdot N_2$.
We then employ a simple sampling operator $\Scal_t \colon \C^N \to \C^{M_t}$ which discards all Fourier frequencies which are not located on the desired sampling geometry at time $t$ (i.e. the chosen spokes). 
More precisely, following \cite{Ehrhardt2016}, if we let $\Pcal_t \colon \{1,\dots,M_t \} \to \{1,\dots,N\}$ be an injective mapping which chooses $M_t$ Fourier coefficients from the $N$ coefficients available, we can define the sampling operator $\Scal$ applied to $f \in \C^N$ as
\begin{align*}
 \Scal_t \colon \C^N \to \C^{M_t}, \quad (\Scal_t f)_k = f_{\Pcal_t(k)}.
\end{align*}
The full forward operator $\Kcal_t$ can hence be expressed as 
\begin{align}\label{eq:forward_op_art}
 \Kcal_t \colon \C^N \xrightarrow{\Fcal} \C^N \xrightarrow{\Scal_t} \C^{M_t}.
\end{align}
The corresponding adjoint operator of $\Kcal_t$ is given by  
\begin{align*}
 \Kcal_t^* \colon \C^{M_t} \xrightarrow{\Scal_t^*} \C^N \xrightarrow{\Fcal^{-1}} \C^N,
\end{align*}
where $\Fcal^{-1}$ denotes the standard inverse Fourier transform and $\Scal_t^*$ `fills' the missing frequencies with zeros, i.e. 
\begin{align*}
 (\Scal_t^*z)_l = \sum_{k=1}^{M_t} z_k \delta_{l,\Pcal_t(k)}, \qquad \text{for } l = 1, \dots, N.
\end{align*}
For the prior $u_0$, we choose a full Cartesian sampling, which corresponds to $\Pcal_0$ being the identity. For the subsequent dynamic scan, we set up $\Pcal_t$ such that it chooses the frequencies located on (discrete) spokes through the center of the $k$-space.     
It is important to notice that this implies that the locations of the (discretized) spokes are still located on a Cartesian grid, which allows to employ a standard fast Fourier transform (FFT) followed by the above projection onto the desired frequencies. 
This is not the case for the operators we use for real data. \\

\noindent {\it Non-uniform Fourier transform - the real data case}\\ 
\noindent 
In contrast to the above (simplified) setup for artificial data, in many real world application the measured $k$-space frequencies $\xi_m$ in \eqref{eq:fourier_transform} are {\it not} located on a Cartesian grid. 
While this is not a problem with respect to the formula itself, it however excludes the possibility to employ a fast Fourier transform, 
%numerically
which usually reduces the computational costs of an $N$-point Fourier transform from an order of $O(N^2)$ to $O(N \log N)$.
To get to a similar order of convergence also for non-Cartesian samplings, it is necessary to employ the concept of non-uniform fast Fourier transforms (NUFFT) \cite{Fessler:NUFFT,Fessler:code,Matej2004,Nguyen:1999,Strohmer2000}. 
We only give a quick intuition here and for further information we refer the reader to the literature listed above. 
The main idea is to use a (weighted) and oversampled standard Cartesian $K$-point FFT $\Fcal$, $K \geq N$ followed by an interpolation $\Scal$ in $k$-space onto the desired frequencies $\xi_m$. 
Note that the oversampling takes place in $k$-space.
The operator $\Kcal_t$ for time $t$ can hence again be expressed as a concatenation of a $K$-point FFT and a sampling operator 
\textbf{\begin{align*}
 \Kcal_t \colon \C^N \xrightarrow{\Fcal} \C^N \xrightarrow{\Scal_t} \C^{M_t}.
\end{align*}}
For our numerical experiments with the experimental DCE-MRI data, the sampling operator $\Scal_t$ and its adjoint were taken from the NUFFT package \cite{Fessler:code}.

\subsection{Numerical solution}
%
Due to the nondifferentiablity and the involved operators we apply a primal-dual method \cite{ChambollePock} to solve the minimization problem \eqref{eq:dynamic_recon}. 
We first line out how to solve the (simple) TV-regularized problem for the prior (\ref{tvu0}) and then extend the approach to the dynamic problem. 
Interestingly, the problem for the prior already provides all the ingredients needed for the numerical solution of the dynamic problem, which can then be done in a very straightforward way. 
We consider the problem 
\begin{equation} \label{tvu0}
	\min_{u_0} ~ \frac{\alpha_0}{2} \| \Kcal_0 u_0 - f_0 \|_{\C^{M_0}}^2 + \| \nabla u_0 \|_1, 
\end{equation}
with $u_0 \in \C^{N_0}$.
Dualizing both terms leads to its primal-dual formulation 
\begin{align}\label{eq:tv_pd}
	\min_{u_0} \max_{y_1,y_2} ~ \langle y_1, \Kcal_0 u_0 - f_0 \rangle_{C^{M_0}} - \frac{1}{2 \alpha_0} \|y_1 \|_{\C^{M_0}}^2 + \langle y_2, \nabla u_0 \rangle_{\C^{N_0 \times 2}} + \chi_{C}(y_2),
\end{align}
where $y_1 \in \C^{M_0}$ and $\chi_{C}$ denotes the characteristic function of the set  
\begin{align*}
	C := \{ y \in \C^{N_0 \times 2} ~|~ \|y \|_\infty \leq 1 \}.
\end{align*}
% 
The primal-dual algorithm in \cite{ChambollePock} now essentially consists in performing a proximal gradient descent on the primal variable $u_0$ and a proximal gradient ascent on the dual variables $y_1$ and $y_2$, where the gradients are taken with respect to the linear part, the proximum with respect to the nonlinear part. 
We hence need to compute the proximal operators for the nonlinear parts in \eqref{eq:tv_pd} to obtain the update steps for $u_0$ and $y_1,y_2$. 
It is easy to see that the proximal operator for $\phi(y_1) = \frac{1}{2 \alpha} \|y_1\|_{\C^{M_0}}^2$ is given by 
\begin{align}\label{eq:prox_dual_l2}
	y_1 = \prox_{\sigma \phi} (r) \Leftrightarrow y_1 = \frac{\alpha r}{\alpha + \sigma}.
\end{align}
The proximal operators for the update of $y_2$ are given by a simple projection onto the set $C$, i.e. 
\begin{align}\label{eq:prox_proj}
	y_2 = \proj_C (r) \Leftrightarrow (y_2)_i = r_i / \max (|r_i|,1) \quad \text{for all } i.
\end{align}
Putting everything together leads to Algorithm \ref{alg:prior}.
\begin{algorithm}[t!] 
\caption{\textbf{Reconstruction of the prior}}
{
\begin{algorithmic}[1]
\Require step sizes $\tau,\sigma > 0$, data $f_0$, parameter $\alpha_0$
\Ensure $u_0^0 = \bar{u}_0^0 = \Kcal_0^*f_0, ~ y_1^0 = y_2^0 = 0$
	\While{$\sim$ stop crit}
    	\State {\it Dual updates}
        \State $y_1^{k+1} = (\alpha_0 \left[ y_1^k + \sigma (\Kcal_0 \bar{u}_0^k - f_0)\right]) / (\alpha_0 + \sigma)$
          \State $y_2^{k+1} = \proj_C \left(y_2^k + \sigma \nabla \bar{u}_0^k\right)$
          \State {\it Primal updates}
          \State $u_0^{k+1} =  u_0^k - \tau \left[ \Kcal_0^* y_1^{k+1} - \diverg(y_2^{k+1}) \right]$
          \State {\it Overrelaxation}
          \State $\bar{u}_0^{k+1}= 2 u_0^{k+1} - u_0^k$
	\EndWhile\\
\Return $u_0 = u_0^k$
\end{algorithmic}
}
\label{alg:prior}
\end{algorithm}
\ \\

The numerical realization of the dynamic problem is now straightforward.
In order to deal with the infimal convolution, we use its definition and introduce an additional auxiliary variable yielding 
\begin{alignat*}{4}
	&\min_{\ubold}&& ~ &&\sum_{t=1}^T \frac{\alpha_t}{2} \| \Kcal_t u_t - f_t \|_{\C^{M_t}}^2 + \sum_{t=1}^{T-1} \frac{\gamma_t}{2} \|u_{t+1} - u_t \|_{\C_N}^2 + \sum_{t=1}^T w_t \TV(u_t) \\
	& && + && \sum_{t=1}^T (1-w_t) \ICBTV^{p_0}(u_t,u_0) \\    
   = &\min_{\ubold,\zbold}&& ~ &&\sum_{t=1}^T \frac{\alpha_t}{2} \| \Kcal_t u_t - f_t \|_{\C^{M_t}}^2 + \sum_{t=1}^{T-1} \frac{\gamma_t}{2} \|u_{t+1} - u_t \|_{\C_N}^2 +\sum_{t=1}^T w_t \| \nabla u_t \|_1\\
   & && + &&\sum_{t=1}^T (1-w_t) \left[ \| \nabla (u_t - z_t) \|_1 + \| \nabla z_t \|_1  - \langle p_0,u_t \rangle_{\C^N} + \langle 2 p_0, z_t \rangle_{\C^N} \right]
\end{alignat*}
where $\ubold = [u_1, \dots, u_T] \in \C^{N \times T}$ and $\zbold = [z_1, \dots, z_T] \in \C^{N \times T}$.
Introducing a dual variable $\ybold$ for all the terms containing an operator, leads to the primal-dual formulation 
\begin{alignat}{4}
\label{eq:primal_dual}
	&\min_{\ubold,\zbold} \max_{\ybold} && ~ && \sum_{t=1}^T \left(\langle y_{t,1}, \Kcal_t u_t - f_t \rangle_{\C^{M_t}} - \frac{1}{2 \alpha_t} \|y_{t,1} \|_{\C^M}^2 \right) + \sum_{t=1}^{T-1} \frac{\gamma_t}{2} \| u_{t+1} - u_t \|_{\C^N}^2 \notag \\
    & && + && \sum_{t=1}^T \left(\langle y_{t,2}, \nabla u_t \rangle_{\C^{N \times 2}} + \langle y_{t,3}, \nabla (u_t - z_t) _{\C^{N \times 2}} + \langle y_{t,4}, \nabla z_t \rangle_{\C^{N \times 2}}\right) \notag \\
    & && - && \sum_{t=1}^T \langle (1-w_t)p_0,u_t \rangle_{\C^N} + \sum_{t=1}^T \langle 2(1-w_t)p_0,z_t \rangle_{\C^N}  \notag \\
    & && + && \sum_{t=1}^T \left(\chi_{C_{t,2}}(y_{t,2}) + \chi_{C_{t,3}}(y_{t,3}) + \chi_{C_{t,4}}(y_{t,4})\right)
\end{alignat}
where $\ybold = [\ybold_1, \dots, \ybold_T]$, $\ybold_t = [y_{t,1}, \dots, y_{t,4}]$, and for all $t = 1, \dots, T$, $u_{t} \in \C^{M_t}$ and
\begin{align*}
	&C_{t,2} := \{ y \in \C^{M \times 2} ~|~ \|y \|_{\infty} \leq w_t \}, \\
    &C_{t,3} := \{ y \in \C^{M \times 2} ~|~ \|y \|_{\infty} \leq (1-w_t) \}, \\
    &C_{t,4} := \{ y \in \C^{M \times 2} ~|~ \|y \|_{\infty} \leq (1-w_t) \}. \\
\end{align*}
%
\begin{algorithm}[t!]
\caption{\textbf{Dynamic reconstruction with structural prior}}
{
\begin{algorithmic}[1]
\Require step sizes $\tau,\sigma > 0$, subgradient $p_0$, for all $t=1,\dots,T$: data $f_t$, parameters $\alpha_t$, $w_t$, $\gamma_t$
\Ensure for all $t=1,\dots,T$: $u_t^0 = \bar{u}_t^0 = \Kcal_t^*f_t, ~ z_t^0 = \bar{z}_t^0 = 0, ~ y_{t,1}^0 = y_{t,2}^0 = y_{t,3}^0 = y_{t,4}^0 = 0$
	\While{$\sim$ stop crit}
    	\For{t=1,\dots,T} 
          \State {\it Dual updates}
          \State $y_{t,1}^{k+1} = \frac{\alpha_t \left[ y_{t,1} + \sigma (\Kcal_t \bar{u}_t^k - f_t)\right]}{\alpha_t + \sigma}$
          \State $y_{t,2}^{k+1} = \proj_{C_2}\left(y_{t,2}^k + \sigma \nabla \bar{u}_t^k\right)$
          \State $y_{t,3}^{k+1} = \proj_{C_3}\left(y_{t,3}^k + \sigma \nabla (\bar{u}_t^k - \bar{z}_t^k) \right)$
          \State $y_{t,4}^{k+1} = \proj_{C_4}\left(y_{t,4}^k + \sigma \nabla \bar{z}_t^k \right)$
          \State {\it Primal updates}
          \State $u_t^{k+1} =  \frac{u_t^k - \tau \left[ \Kcal_t^* y_{t,1}^{k+1} - \diverg(y_{t,2}^{k+1}) - \diverg(y_{t,3}^{k+1}) - (1-w_t)p_0 \right] + \tau \gamma_t u_{t+1}^k + \tau \gamma_{t-1} u_{t-1}^k}{\tau (\gamma_t + \gamma_{t+1}) +1}$
          \State $z_t^{k+1} - \tau \left[ 2(1-w_t) p_0 + \diverg(y_{t,3}^{k+1}) - \diverg(y_{t,4}^{k+1}) \right]$
          \State {\it Overrelaxation}
          \State $(\bar{u}_t^{k+1}, \bar{z}_t^{k+1}) = 2 (u_t^{k+1}, z_t^{k+1}) - (u_t^k,z_t^k)$
    	\EndFor
	\EndWhile\\
\Return for all $t = 1,\dots,T$: $u_t = u_t^k$
\end{algorithmic}
}
\label{alg:fmri}
\end{algorithm}
%
To solve the problem, we again perform a proximal gradient descent on the primal variables $\ubold$ and $\zbold$, and a proximal gradient ascent on the dual variables $\ybold$, where the gradients are taken with respect to the linear parts, the proximum with respect to the nonlinear parts. 
We hence need to compute the proximal operators for the nonlinear parts in \eqref{eq:primal_dual} to obtain the update steps for $u_t,z_t$ and $\ybold_t$ for every $t = 1, \dots, T$. 
The proximal operators for $\phi_t(y_{t,1}) = \frac{1}{2 \alpha_t} \| y_{t,1} \|_{\C^{M_t}}^2$ can be computed exactly as in \eqref{eq:prox_dual_l2}.
The proximal operators for the updates of $y_{t,j}$, $j = 2,3,4$, are given by projections onto the sets $C_{t,j}$ similar to \eqref{eq:prox_proj}.
For the squared norm related to the time regularization, we notice that for every $1< t < T$, $u_t$ only interacts with the previous and the following time step, i.e. $u_{t-1}$ and  $u_{t+1}$. 
Hence, analogously to $\phi_t$, the proximum for 
\begin{align*}
	\psi_t(u_t) = \frac{\gamma_{t-1}}{2} \|u_t - u_{t-1}\|_{\C^N}^2 + \frac{\gamma_t}{2} \|u_{t+1} - u_t\|_{\C^N}^2
\end{align*}
is given by 
\begin{align*}
	u_t = \prox_{\tau \psi_t} (r) \Leftrightarrow u_t = \frac{r + \tau \gamma_t u_{t+1} + \tau \gamma_{t-1} u_{t-1}}{\tau (\gamma_t + \gamma_{t-1}) + 1}. 
\end{align*}
The two odd updates for $t = 1$ and $t = T$ can be obtained by the same formula by simply setting $\gamma_0 = 0$ and $\gamma_T = 0$, respectively.
Putting everything together, we obtain Algorithm \ref{alg:fmri}.\\

\subsection{Step sizes and stopping criteria}
We quickly discuss the choice of the step sizes $\tau, \sigma$ and stopping criteria for Algorithm \ref{alg:fmri}. 
In most standard applications it stands to reason to choose the step sizes according to the condition $\tau \sigma \| L \|^2 < 1$ ($L$ denotes the collection of all operators) such that convergence of the algorithm is guaranteed \cite{ChambollePock}.
However, depending on $T$, i.e. the number of time frames we consider, the norm of the operator $L$ 
can be very costly to compute, or too large such that the condition $\tau \sigma \| L \|^2 < 1$ only permits extremely small step sizes. 
For practical use, we instead simply choose $\tau$ and $\sigma$ reasonably ''small`` and track both the energy of the problem and the primal-dual residual \cite{Goldstein:Adaptive} to monitor convergence. 
For the sake of brevity, we do not write down the primal-dual residual for Algorithm \ref{alg:fmri} and instead refer the reader to \cite{Goldstein:Adaptive} for its definition. 
The implementation is then straightforward.
We hence stop the algorithm if both, the relative change in energy between consecutive iterates and the primal-dual residual, have dropped below a certain threshold.

\subsection{Practical considerations}
It is clear that for a large number of time frames $T$ Algorithm \ref{alg:fmri} starts to require an increasing amount of time to return reliable results and for reasonably ''large`` step sizes $\tau$ and $\sigma$ it is even doubtful whether we can obtain convergence. 
In practice, it is hence necessary to divide the time series $\Tbold = \{1, \dots, T\}$ into $l$ smaller bits of consecutive time frames. More precisely, choose numbers $1 \leq T_1 < \dots < T_l = T$ such that $\Tbold = \Tbold_1 \cup \Tbold_2 \cup \dots \cup \Tbold_l$ with $\Tbold = \{1, \dots, T_1, T_1+1, \dots, T_2, \dots, T_{l-1}+1, \dots, T_l\}$.
We can then perform the reconstruction separately for all $\Tbold_i$. 
In order to keep the ''continuity`` between $\Tbold_i$ and $\Tbold_{i+1}$, we can include the last frame of $\Tbold_i$ into the reconstruction of $\Tbold_{i+1}$ by letting $\gamma_{T_i} \neq 0$ and choosing $u_{T_i}$ as the respective last frame of $\Tbold_i$.
This divides the overall problem into smaller and easier subproblems, which can be solved faster.
In practice, we observed that a size of five to ten frames per subset $\Tbold_i$ is a reasonable choice, which essentially gives very similar results as doing a reconstruction for the entire time series $\Tbold$.






