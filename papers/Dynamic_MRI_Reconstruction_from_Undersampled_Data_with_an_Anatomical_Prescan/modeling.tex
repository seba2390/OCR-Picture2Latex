\section{The proposed reconstruction framework}
\label{sec:modeling}
In this section we translate our novel reconstruction approach, whose basic modules we motivated above, into a mathematical setting. 
To this end, we specify the modeling of the required operators and all terms contained.
We start with the static MRI forward problem suitable for modeling any arbitrary MR data acquisition protocol. 

\subsection{Undersampled MRI: A mathematical model}
\label{subsec:operator modeling}

While the forward model in MRI can become arbitrarily difficult if modeling the influence of all the underlying physical and technical parameters, it is common practice to assume it to be a simple Fourier transform and hide all the unkowns.
Following this line, we define the Fourier transform of the MR image $u \colon \Omega \to \C$ at $k$-space coordinate $\xi$ by 
\begin{align}\label{eq:fourier_transform_cont}
	(\Kcal u)(\xi) = \int_\Omega u(x) e^{-i x \cdot \xi} \dx, 
\end{align}
where $\Omega \subset \R^d$ is a bounded image domain (usually $d=2,3$).
The associated inverse problem is then to reconstruct $u$ from noisy measurements $f$ obtained by 
\begin{align}\label{eq:inverse_problem}
	\Kcal u + e = f, 
\end{align}
where $e$ models the (complex-valued) noise. 

A strong point of MRI is that it allows for a great variety of different data acquisition procedures each of which leads to a very distinct appearance of the image $u$, which does not become obvious from the above formula \eqref{eq:fourier_transform_cont}, since, as mentioned before, all the parameters characterizing the specific acquisition scheme are hidden.   
In particular, different scanning protocols produce different types of image contrasts.
For example, depending on the respective relaxivities, the same tissue can appear very bright in T1 contrast while being very dark in T2 and some details might only be visible at a specific setup of the acquisition scheme.    
However, since the relaxivity varies only between different types of tissues while being constant across the same tissue, it is very likely for images of different contrast to nevertheless share the same structures (with the exception of features just visible in one of the contrasts).

In practice, one has access to only a small portion of Fourier coefficients $\xi_m$, $m = 1, \dots, M$, such that the system \eqref{eq:inverse_problem} becomes highly underdetermined. 
As a remedy, one also discretizes the image domain $\Omega$ and samples the desired reconstruction $u$ at the respective locations. 
More precisely, letting $\Omega_N$ be a discretization of the image domain $\Omega$ into $N$ pixels, we define the discrete Fourier transform $\Kcal_N \colon \C^N \to \C^M$ of $u$ by 
\begin{align}\label{eq:fourier_transform}
  (\Kcal_N u)(\xi_m) = \sum_{x_n \in \Omega_N} u(x_n) e^{-i x_n \cdot \xi_m} 
\end{align}
%
for all $k$-space coordinates $\xi_m$, $m=1,\dots,M$.
Here $u(x_n)$ denotes a sampling of $u$ at location $x_n \in \Omega_N$.
It is worth noticing that the $k$-space coordinates $\xi_m$ can be chosen arbitrarily and are in particular independent of the choice of discretization of the image domain.
Hence, from a mathematical perspective, the resolution $N$ of the reconstruction can (theoretically) be chosen arbitrarily at the price of a nontrivial nullspace, and in particular does not depend on the number of sampled Fourier coefficients.
The standard way in a Cartesian setting is to choose $M=N$ , i.e. to sample as many Fourier coefficients as pixels in the desired image, since in this case there holds a one-to-one relationship between an image and its Fourier transform. 
Vice versa, having measured $M$ Fourier coefficients on a Cartesian grid, the canonical resolution of $u$ is $M$ pixels. 
This gives the notion of the standard Cartesian Fourier transform, which can simply be inverted (cf. Section \ref{subsec:implementation of operators}).

However, driven in particular by developments in compressed sensing (cf. \cite{Lustig:Sparse,Lustig:ktSPARSE}) it is common practice to speed up the data acquisition by undersampling the $k$-space, i.e. by performing significantly less measurements $M \ll N$ than desired image pixels. 
In particular, the measurements are usually not sampled on a Cartesian grid (cf. Section \ref{subsec:dynamic imaging}).
In this case there is neither the possibility to simply ``invert'' the operator, nor a ``canonical'' way of choosing the resolution $N$. 
Instead one can try to compensate for the missing data using additional a-priori information (regularization) and choose the resolution as high as the respective method allows. 
The quality of the reconstruction then depends on the location of the measured Fourier coefficients and on the ratio $M/N$. 
As we show in the numerical section, this ratio can actually be less than one per cent (as opposed to $100$ per cent for $M=N$), choosing a golden angle sampling and an appropriate reconstruction method (see also Section \ref{sec:numerics}).

Summing up the discrete setting, we aim to solve the inverse problem
\begin{align}\label{eq:forward_model}
	\Kcal_N u + e = f,
\end{align}
with $u \in \C^N$, $f \in \C^M$ and random measurements errors $e \in \C^M$. 
Since the noise in MRI is commonly approximated by  Gaussian ones (and more advanced noise models such Rician are beyond the scope of the present study), the canonical related minimization problem reads
\begin{align*}
	u \in \arg \min_{u \in \C^N} ~ \frac{\alpha}{2} \| \Kcal_N u - f \|_{\C^M}^2 + J(u).
\end{align*}
The regularizer $J$ is yet to be defined. 
The goal is to construct it such that it promotes desirable properties (such as smoothness etc.) of the sought-after solution.


\subsection{Dynamic MRI: model extension}
\label{subsec:dynamic imaging}

The scanning protocol we consider throughout this paper involves a (densely sampled) prior scan, followed by a (subsampled) dynamic scan. 
In order to cover the dynamic scan we extend the above idea along the same lines to dynamic MR imaging. 

Let us first comment on the characteristics of the dynamic data: Generally, there are many eligible options in how to choose the specific scanning protocol and (sub-) sampling pattern in practice. 
In this work, we consider radial samplings, that is the data is collected on radial spokes through the $k$-space center.
Note that the basic idea does not change for different types of samplings. 
The interesting part about dynamic MRI is that the MR scanner collects the data almost continuously in time, meaning that it measures the radial spokes in $k$-space sequentially one after the other. 
This leaves us free to determine the time resolution of the recorded sequence only during the reconstruction process. 
More precisely, for the reconstruction we divide the entire collection of spokes $\fbold$ into an arbitrary number $T$ of sets of (consecutive) spokes $\fbold = [f_1, \dots, f_T]$. 
For example, if the scanner recorded 100 spokes of data, we can divide the data set into $T = 100$ sets of 1 spoke each, or $T = 10$ sets of 10 spokes each, etc. leading to a very high time resolution (1 spoke/frame) or to a more moderate time resolution (10 spokes/frame), respectively. 
In choosing $T$, we naturally always have to face the trade-off between a high temporal resolution and a sufficient amount of data per frame available for a meaningful reconstruction to build upon. 
In the following, we shall always assume that we have already divided the dynamic data set into $T$ parts. 

In this setting, we then denote the anatomical prior image by $u_0 \in \C^{N_0}$ with the corresponding densely sampled $k$-space data $f_0$, and for $t = 1, \dots, T$ the undersampled data set $\fbold = [f_1, \dots, f_T]$, where each $f_t \in \C^{M_t}$. 
The corresponding sought-after reconstruction is $\ubold = [u_1, \dots, u_T] \in \C^{N \times T}$.
The imaging operators are 
\begin{align*}
      \Kcal_0 \colon \C^{N_0} \to\C^{M_0}, \qquad \Kcal_t \colon \C^{N} \to \C^{M_t},
\end{align*}
for the (densely) sampled anatomical prior and for time index $t$ in the dynamic sequence, respectively. 
Note that we drop the dependence on the resolution $N_0$, $N$ in the following since it will always be clear from the context.

We remark that we keep $N$ fixed for the dynamic scans, implying that the spatial resolution of the $u_t$ stays the same over time. 
Theoretically, however, there also exists the possibility to change the resolution over time.
The corresponding joint reconstruction problem (see also \cite{Ehrhardt2015,Rasch2017}) for the dynamic sequence then reads 
\begin{align*}
      \ubold \in \arg \min_{\ubold \in \C^{N\times T}} ~ \sum_{t=1}^T \frac{\alpha_t}{2} \| \Kcal_t u_t - f_t \|_{\C^{M_t}}^2 + J(\ubold,u_0),
\end{align*}
with a regularizer $J$ that now depends on the different frames $u_t$ and on the anatomical prior image $u_0$ and thus introduces a coupling among all these images. 
Note that this regularizer can, of course, be composed of several terms.

\subsection{Spatial regularization: TV and $\boldsymbol{\ICBTV}$} 
We now specify our choice of the regularizer $J$. 
In the current section, we will concentrate on penalty terms that do not take into account that we actually aim at reconstructing an entire sequence of consecutive time frames $\ubold$. 
Instead, these terms rather act on each frame $u_t$ independently. 
The question of how we can link the time frames during the reconstruction will be addressed in the subsequent subsection.


\label{subsec:spatial regularization}
\subsubsection{TV: enforcing sparsity in the gradient domain}
\label{subsubsec:TV} 
As motivated and explained before, we assume that at each time step of the dynamic sequence only a small portion of all Fourier coefficients is sampled along radial spokes through the $k$-space center. 
According to the theory of compressed sensing \cite{Candes:Robust,Donoho:CompressedSensing,Lustig:Sparse,Huang:CSinMR}, this requires the data fidelity term to be accompanied by a term that guarantees sparsity in some transform domain.
One popular choice for such a term and the approach we pursue in this paper is the (spatial) total variation regularization.  
First introduced for image denoising in 1992 \cite{Rudin:ROF}, the distinctive feature of this regularization is the promotion of piecewise constant solutions with sharp edges and thus of sparsity with respect to the image gradient. 
For real-valued images $z \in \mathbb{R}^N$, the discrete, isotropic form of TV reads:
\begin{align*}
	\TVd (z) := \| \nabla z \|_1 :=\sum_{n=1}^N |(\nabla z)_n | = \sum_{n=1}^N \sqrt{|(\nabla z)_{n,1}|^2 + |(\nabla z)_{n,2}|^2}.
\end{align*}
with $| \cdot |$ denoting the Euclidean norm on $\mathbb{R}^N$.

However, since in this work we model the unknown MRI reconstruction to be complex-valued, it is necessary to extend the concept of gradients and total variation accordingly. 
The extension of gradients is straightforward and for a complex-valued image $u \in \C^N$ we just define the gradient by $\nabla \colon \C^N \to \C^{N \times 2}$, where more details on our particular implementation can be found in Section \ref{subsec:implementation of operators}. 
The extension of the total variation is slightly more involved: 
though the underlying image $u$ is complex-valued, in most real-world applications one is interested only in the magnitude of the reconstructed image. 
More precisely, using the identity $u = r e^{i \varphi}$, one ideally aims to perform the regularization of the sought-after image directly on its magnitude, i.e. to penalize the total variation of $r$. 
Unfortunately, this leads to a nonlinear (in $\varphi$) forward operator $\tilde{K} (r,\varphi)$ in \eqref{eq:inverse_problem} or \eqref{eq:forward_model}, rendering the numerical solution of the resulting minimization problem considerably more difficult.   
We refer the reader to \cite{Valkonen2014} for an extensive study of this situation.

In this work, we shall instead use the linear forward model \eqref{eq:forward_model} and an approximation to the total variation on the magnitude, which turns out to have a similar effect in practice. 
More precisely, in the complex-valued case, we redefine the discrete, isotropic total variation $\TVd \colon \C^N \to [0,\infty)$ in the following way 
\begin{align*}
	\TVd (u) = \sum_{n=1}^N \sqrt{|\Real (\nabla u)_{n,1}|^2 + |\Imag (\nabla u)_{n,1}|^2 + |\Real (\nabla u)_{n,2}|^2 + |\Imag (\nabla u)_{n,2}|^2},
\end{align*}
which reveals that this approach is equivalent to regarding the image $u$ to be located in $\R^{N \times 2}$ and using real-valued isotropic total variation on its gradient in $\R^{N \times 4}$.
Simply speaking, this approach consists in taking the magnitude of the gradient of $u$ rather than the gradient of the magnitude of $u$.

At the end of this paragraph we address the TV regularization also in a slightly different context, namely with respect to the reconstruction of the anatomical prior $u_0$. 
As already explained in Section \ref{sec:intro}, we shall use the structural information of this high resolution image as additional a-priori information for the reconstruction of the dynamic sequence $\ubold$.
In order to reconstruct the usually fully sampled prior scan, i.e. M = N in Equation \eqref{eq:fourier_transform}, one would typically simply apply the inverse Fourier transform. 
However, since we do not want to incorporate minor oscillations, but rather like to concentrate on major structures encoded in the prior scan, we incorporate a TV regularization term into the reconstruction problem, i.e., in the first step we solve:
\begin{equation*} \label{anatomicTV}
u_0 \in \arg \min_{u_0 \in \C^{N_0}} ~ \frac{\alpha_0}{2} \| \Kcal_0 u_0 - f_0 \|_{\C^{M_0}}^2 + \TVd(u_0).
\end{equation*}
We hence obtain a high-quality piecewise constant anatomical reconstruction $u_0$ with sharp edges which serves as the basis for our further considerations.

\subsubsection{$\boldsymbol{\ICBTV}$: incorporating structural prior information}
\label{subsubsec:structural prior}
In this paragraph, we explain how we can utilize the structural information of the (high resolution) MR image $u_0$ to support the reconstruction of the dynamic MRI sequence.
For this purpose, we will again build upon the concept of total variation regularization \cite{Rudin:ROF} and the related Bregman distances and iterations \cite{Osher:AnIterativeRegularizationMethod}. 
Extensions of these techniques have been shown to be very effective for coupling the edge information of different color channels in RGB image processing \cite{Moeller:ColorBregmanTV} and of different medical imaging modalities in PET-MRI \cite{Rasch2017}, as well as in the context of related debiasing methods \cite{2016debiasing}.

A natural approach for extracting the structural information from the reconstruction of the prior scan is to use the image gradient, since it essentially encodes the edges and hence the structure of the image. Based on this idea, there emerged a variety of methods for structural priors in the literature, of which we only mention a few here \cite{kaipio1999,Ehrhardt2015,Ehrhardt2016,Bowsher:Bayesian,Chan:Regularized, Nuyts:Mutual,Atre:Evaluation, Leahy1991, Lipinski:Expectation, Vunckx:Evaluation}.  
However, since most of these approaches are not normalized with respect to the size of the gradient, a common difficulty is to deal with images in different scales, i.e. images sharing the same structure, but having very different intensity ranges and hence different sizes of jumps. 
Considering MRI this problem is of particular importance, since the absolute intensities of the reconstructions can be very different depending on the acquisition protocol chosen.

In \cite{Rasch2017} it has therefore been argued for using a subgradient rather than a gradient to include edge information without considering their size. 
In order to define a straightforward extension of subdifferentials to the complex setting, we need to equip $\C^N$ with the following inner product:
\begin{align*}
\langle u,v \rangle_{\C^N} := \Real (u^* v) \qquad u,v \in \C^N 
\end{align*}
with $^*$ denoting the convex conjugation. 
This inner product induces the same norm as the standard inner product on $\C^N$, but is real-valued, which enables us to consider angles between complex vectors. 
Indeed, for $u,v \in \C^N$ we have 
\begin{align*}
	\cos(\varphi) = \frac{\langle u, v \rangle_{\C^N}}{\|u\|_{\C^N} \|v\|_{\C^N}}, 
\end{align*}
where $\varphi$ denotes the angle between $u$ and $v$. 

We now define the subdifferential of a convex functional $J \colon \C^N \to [0,\infty)$ by 
\begin{align*}
      \partial J(u_t) = \{ p \in \C^N ~|~  J(v) \geq J(u_t) + \langle p, v - u_t \rangle_{\C^N} \; \forall v \in \C^N \}. 
\end{align*}
By the chain rule, a subgradient $p_0$ of the total variation at $u_0$ is then given by 
\begin{align*}
	p_0 \in \partial \TVd (u_0) \Leftrightarrow p_0 = - \diverg (q_0), \quad q_0 \in \partial \|\nabla u_0 \|_1,
\end{align*}
with $q_0 \in \C^{N \times 2}$ such that 
\begin{align*}
	|q_0| \leq 1 \quad \text{ and } \quad q_0 = \frac{\nabla u_0}{|\nabla u_0|} \text{ if } \nabla u_0 \neq 0.
\end{align*}
Hence, we see that where the gradient of $u_0$ does not vanish, the vector field $q_0$ equals the direction of the edge in $u_0$ at this location and this information is encoded in the subgradient. 
At positions with zero gradient, the prior $u_0$ does not provide any structural information (apart from being constant), and $q_0$ is an arbitrary vector from the unit ball. 
It has been argued in \cite{Osher:AnIterativeRegularizationMethod,Moeller:ColorBregmanTV,Rasch2017}, that specific choices of subgradients can provide ''hints`` about structure also at these locations. 
In this work, however, we want to use only the location of edges already clearly visible in the reconstructed anatomical prior.  
Accordingly, we will use the following vector field 
\begin{align*}
	q_0 = \begin{cases}
			\frac{\nabla u_0}{|\nabla u_0|}, &\text{ if } \nabla u_0 \neq 0, \\
            0, &\text{ else,}
		\end{cases}
\end{align*}
to obtain a subgradient $p_0 = -\diverg (q_0) \in \partial \TVd (u_0)$.
The associated Bregman distance can be written as 
\begin{align*}
	D_{\TVd}^{p_0}(u_t,u_0) 
    &= \TVd (u_t) - \langle p_0,u_t \rangle_{\C^N}
    = \| \nabla u_t \|_1 - \langle q_0, \nabla u_t \rangle_{\C^{N \times 2}} \\ 
    &= \sum_{ \{\nabla u_0 \neq 0\} } |(\nabla u_t)_n | \left( 1 - \frac{\langle (\nabla u_t)_n, (\nabla u_0)_n \rangle}{|(\nabla u_t)_n||(\nabla u_0)_n|} \right) + \sum_{\{\nabla u_0 = 0\}} |(\nabla u_t)_n| \\
    &= \sum_{\{\nabla u_0 \neq 0 \} } |(\nabla u_t)_n| (1 - \cos (\varphi_n)  ) + \sum_{\{\nabla u_0 = 0 \} } |(\nabla u_t)_n|,
\end{align*}
where $\varphi_n$ denotes the angle between the vectors. 
Hence we conclude that on the support of $\nabla u_0$ the Bregman distance penalizes the gradient of $u_t$ weighted by its directional deviation from $\frac{\nabla u_0}{|\nabla u_0 |}$, while outside the support of $\nabla u_0$ we obtain a standard TV regularization. 
Consequently, this functional favors aligned edges between $u_t$ and the prior $u_0$ without penalizing the size of the gradients. 
More precisely, the height of the jumps of shared edges can be determined only in dependence on the data term, since once an edge of $u$ is aligned to the edge of $u_0$, i.e., $\cos(\varphi_n) = 1$, the regularization functional vanishes at this position. 
We refer to \cite{Moeller:ColorBregmanTV,Rasch2017} for an extensive discussion of the behavior.

In view of noisy data and with respect to practical applications it often makes sense to use a regularized approximation of the vector field $q_0$: 
\begin{align}\label{eq:subgrad_eta}
	\qa = \begin{cases}
			\frac{\nabla u_0}{|\nabla u_0|}, &\text{ if } |\nabla u_0| \geq \eta, \\
            0, &\text{ else.}
		\end{cases}
\end{align}
In this formulation, the threshold parameter $\eta$ defines a minimum height such that we consider a jump in $u_0$ to be an edge and in this way avoids to falsely identify small oscillations as edges (see also \cite{Ehrhardt2016} for a similar approach). 
In this case, the Bregman distance reads 
\begin{align}
	D_{\TVd}^{\pa}(u_t,u_0) = \sum_{\{|\nabla u_0| \geq \eta \} } |(\nabla u_t)_n| (1 - \cos (\varphi_n)  ) + \sum_{\{|\nabla u_0| < \eta \} } |(\nabla u_t)_n|. 
    \label{eq:Bregman Distance with respect to p0eta}
\end{align}

Due to potentially different image contrasts in the prior scan and the dynamic sequence, it is unfortunately unrewarding to directly use this Bregman distance to include the structural prior information in the reconstruction framework for the dynamic sequence.
To illustrate that this is indeed true, we consider a situation, where taking a step up across the edges of the structure in the prior corresponds to taking a step down across the same structure in the dynamic sequence. 
In this situation, the angle $\varphi_n$ between the two vectors in Equation \eqref{eq:Bregman Distance with respect to p0eta} is close to $\pi$ such that the Bregman distance \eqref{eq:Bregman Distance with respect to p0eta} would highly penalize the edge in $u_t$ at the respective position, which in our setting seems counterintuitive. 
We therefore face a situation, where we expect edge positions to coincide, but where we not necessarily aim at aligning edges to the same direction, but rather would like to adapt the regularizer such that it promotes also the opposing direction of the jump. 
In order to obtain a symmetric measure with respect to vector orientation, we again follow \cite{Moeller:ColorBregmanTV} and \cite{Rasch2017} and employ the infimal convolution of two Bregman distances with respect to subgradients with opposing signs 
\begin{align*}
	\ICBTV^{\pa}(u_t,u_0) := \inf_{\phi + \psi = u_t} ~ D_{\TVd}^{\pa}(\phi,u_0) + D_{\TVd}^{-\pa}(\psi,-u_0).
\end{align*}
This functional operation yields a decomposition of the image $u_t$ into two parts of which one matches the edge set of $u_0$, and the other one matches the edge set of $-u_0$, hence with ''inverted`` contrast. 
Acting as an edge indicator independent of the sign of jumps, the $\ICBTV$ functional thus allows to incorporate the structure of the prior image $u_0$ as a-priori information into the reconstruction of the dynamic sequence. For a more detailed discussion of the behavior of the functional we again refer to \cite{Rasch2017}. 

%% 

\subsection{Temporal regularization}\label{subsec:temporal regularization}
So far, we have only discussed spatial regularization techniques, which affect all the time frames independently from each other, thus not imposing any particular relation between frames at successive points in time. 
The last ingredient we want to add to our dynamic reconstruction framework is an additional temporal regularization, which interlinks consecutive time frames.

In the literature a whole zoo of approaches can be found, reaching from Tikhonov-type regularization of the time derivative \cite{Adluru2007,Adluru2007_2} via sparsity in some known basis, i.e., \cite{Lustig:ktSPARSE} after applying e.g. the temporal Fourier transform \cite{Tremoulheac:lowRankPlusSparsePrior}, the time derivative \cite{feng2014,wundrak2016} or the discrete cosine transform \cite{fang2016high}, to low rank regularization \cite{Yao:nuclearNormdMRI}. 
In addition, it has been proposed to combine the last two classes to get a decomposition into a low rank part and into a part which is sparse in a known transform domain \cite{Tremoulheac:lowRankPlusSparsePrior,Otazo:lowRankPlusSparseMatrixDecomposition}.
To make full use of the temporal redundancy present in the dynamic data set, one should always choose the regularization in dependence on the expected dynamic behavior. 
For example, for a recurrent dynamic such as the beat of a heart a low rank regularization stands to reason, while in case of objects quickly moving from one region of the image to another sparsity of the time derivative seems to be a more natural choice.
Since the (pixelwise) temporal behavior we expect in our applications is commonly modeled by a smooth curve, we penalize the squared time derivative. 

For a stack of images $\ubold = [u_1, \dots, u_T] \in \C^{N \times T}$, a discrete time derivative can be defined by forward differences, i.e.
\begin{align*}
	\partial_t u_t = \begin{cases}
    	(u_{t+1} - u_t)/(\Delta t), & t = 1, \dots, T-1, \\
        0, & t = T,
	\end{cases} 
\end{align*}
with step size $\Delta t$.
Using a Tikhonov-type regularization for the time derivative we arrive at the penalty function 
\begin{align*}
	\Delta t \sum_{t = 1}^{T-1} \| \partial_t u_t \|_{\C^N}^2 = \frac{1}{\Delta t} \sum_{t = 1}^{T-1}  \| u_{t+1} - u_t \|_{\C^N}^2.
\end{align*}

\subsection{The proposed method}
We are finally able to combine the above considerations to a full method for dynamic MRI with a structural prior. 
The goal of the method is that for all $t = 1, \dots, T$ the reconstructed time frame $u_t$ 
\begin{enumerate}
    \item matches the data $f_t$, i.e. $\| \Kcal_t u_t - f_t \|^2_{\C^{M_t}}$ is small, 
    \item 
     has sparse (spatial) gradient
    , i.e. $\TV(u_t)$ is small, 
    \item has a structure similar to the prior $u_0$, i.e. $\ICBTV^{p_0}(u_t,u_0)$ is small, 
    \item should not be too different from the previous and consecutive frame, i.e. the time derivative $\| \partial_t u_t \|_{\C^N}^2$ is small.
\end{enumerate}
In order to weight between these four a-priori assumptions on $u_t$, we introduce weighting factors $\alpha_t >0, \gamma_t >0 , w_t \in [0,1]$ and the reconstruction method becomes:
 \begin{alignat}{4}
   &\ubold \in \arg &&\min_{\ubold} && \sum_{t=1}^T \frac{\alpha_t}{2} \| \Kcal_t u_t - f_t\|_{\C^{M_t}}^2 \qquad &&\text{(data fidelity)} \notag\\ 
   & && +&& \sum_{t=1}^T w_t \TV_d(u_t) \qquad &&\text{(TV regularization)} \notag \\
   & && +&& \sum_{t=1}^T (1-w_t) \ICBTV^{p_0}(u_t, u_0 ) \qquad &&\text{(structural prior)} \notag \\
   & && +&& \sum_{t=1}^{T-1} \frac{\gamma_t}{2} \| u_{t+1} - u_t \|_{\C^N}^2. \qquad &&\text{(temporal smoothing)} 
\label{eq:dynamic_recon}
\end{alignat}
% 
We quickly outline the behavior. 
The parameters $\alpha_t$ control the data fidelity, introducing data accuracy for the reconstruction. 
The parameters $w_t \in [0,1]$ weight between total variation regularization and proximity to the structural prior. 
The weights are chosen between 0 and 1 since both the $\TV$ and the $\ICBTV$ term enforce spatial regularity. 
For small $w_t$, the reconstructions will have a structure very similar to the prior, for $w_t$ close to 1 the prior does not substantially influence the reconstructions. 
Since we expect dynamics only in some part of the image while the rest stays constant, it is reasonable to keep more (but not all the) weight on the prior. 
The parameters $\gamma_t$ control the temporal smoothness of the reconstructed sequence. 
For the sake of clarity we embed the temporal resolution $\Delta t$ in $\gamma_t$. 
However, switching to a different time resolution, the parameters $\gamma_t$ have to be adjusted related to the change in $\Delta t$.
 


 
 