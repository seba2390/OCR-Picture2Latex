\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors



\usepackage{algorithm}
\usepackage{algorithmic}


% added by author
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{bm}

\renewcommand{\algorithmicrequire}{\textbf{Input:}} 
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand\algo{\emph{SHED }}
\newcommand\exalgo{\emph{RESHED}}

% \sA,\sO,
\newcommand\sA{\mathbb{A}}
\newcommand\sS{\mathbb{S}}
\newcommand\sO{\mathbb{O}}
\newcommand\vx{\bm{x}}
\newcommand\mI{\bm{I}}
\newcommand\vmu{\bm{\mu}}
\newcommand\vz{\bm{z}}
\newcommand\va{\bm{a}}
\newcommand\vs{\bm{s}}
\newcommand\R{\mathbb{R}}
\newcommand\mSigma{\bm{\Sigma}}


\usepackage{amsfonts}       % blackboard math symbols
% \usepackage[ruled,vlined,linesnumbered,noend]{algorithm2e}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{subfigure}


\title{A Hierarchical Approach to Environment Design with Generative Trajectory Modeling}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Dexun Li\thanks{This is the initial version of our submission} \\
  % School of Computing and Information Systems\\
  Singapore Management University\\
  \texttt{dexunli.2019@phdcs.smu.edu.sg} \\
  % examples of more authors
  \And
  Pradeep Varakantham \\
  Singapore Management University \\
  \texttt{pradeepv@smu.edu.sg} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}
Unsupervised Environment Design (UED) is a paradigm for training generally capable agents to achieve good zero-shot transfer performance. This paradigm hinges on automatically generating a curriculum of training environments. Leading approaches for UED predominantly use randomly generated environment instances to train the agent. While these methods exhibit good zero-shot transfer performance, they often encounter challenges in effectively exploring large design spaces or leveraging previously discovered underlying structures, To address these challenges, we introduce a novel framework based on Hierarchical MDP (Markov Decision Processes). Our approach includes an upper-level teacher's MDP responsible for training a lower-level MDP student agent, guided by the student's performance. To expedite the learning of the upper leavel MDP, we leverage recent advancements in generative modeling to  generate  synthetic experience dataset for training the teacher agent. Our algorithm, called \emph{S}ynthetically-enhanced \emph{H}ierarchical \emph{E}nvironment \emph{D}esign (\algo), significantly reduces the resource-intensive interactions between the agent and the environment.
To validate the effectiveness of \algo, we conduct empirical experiments across various domains, with the goal of developing an efficient and robust agent under limited training resources. Our results show the manifold advantages of \algo and highlight its effectiveness as a potent instrument for curriculum-based learning within the UED framework. This work contributes to exploring the next generation of RL agents capable of adeptly handling an ever-expanding range of complex tasks.
\end{abstract}



\section{Introduction}

The advances of reinforcement learning (RL~\cite{sutton1998introduction}) have promoted research into the problem of training autonomous agents that are capable of accomplishing complex tasks. One interesting, yet underexplored, area is training agents to perform well in unseen environments, a concept known as zero-shot transfer performance. To this end, Unsupervised Environment Design (UED~\cite{dennis2020emergent}) has emerged as a promising paradigm to address this problem. The primary objective of UED is to automatically generate environments in a curriculum-based manner, and training agents in these sequentially generated environments can equip agents with a general capability, enabling agents to learn robust and adaptive behaviors that can be transferred to new scenarios without explicit exposure during training.


Existing approaches in UED primarily focus on building an adaptive curriculum for the environment generation process to train more generally capable agents. For example, \citet{dennis2020emergent} formalize the problem of finding adaptive curricula through a game involving an adversarial environment generator (teacher agent), an expert antagonist agent, and the protagonist agent (student agent). The RL teacher is designed to generate environments that maximize regret, defined as the difference between the protagonist and antagonist agent's return. They show that these agents will reach a Nash Equilibrium where the protagonist agent learns the minimax regret policy.  However, since teacher agents adapt solely based on regret feedback, it is inherently difficult to adapt to student policy changes, and training such an RL teacher remains a challenge. With the growing popularity of domain randomization~\citep{tobin2017domain} due to its promising empirical results, \citet{jiang2021prioritized} propose randomly generating environments and then curating randomly sampled environments to achieve high regret. \citet{parker2022evolving} then propose the adaptive curricula by manually designing a principled, regret-based curriculum, suggesting randomly generating environment instances with increasing complexity. \citet{li2023effective} incorporate diversity measurement when randomly generating new environments to ensure that the agent is exposed to a diverse set of environments. Although these domain randomization-based algorithms achieve good zero-shot transfer performance, they face limitations in efficiently exploring large design spaces and leveraging previously discovered inherent structures within the environments. Moreover, existing UED approaches often rely on open-ended domains, which requires a long training horizon. This is unrealistic in the real world due to limited resources. We aim to design a teacher's policy that generates environments best suited to the current student skill level, thereby achieving an optimal general capability within a finite time horizon setting. 


In this paper, we aim to address these limitations by proposing an alternative framework for automatic adaptive environment design. The core idea involves the use of hierarchical Markov Decision Processes (MDPs) to simultaneously formulate the evolution of both the teacher and student during training. This process includes training an upper-level MDP for a teacher agent, which guides the training of the lower-level MDP of a student agent. This guidance is based on a more accurate representation of the student's policy, achieved by evaluating its performance across a diverse set of evaluation environments. This hierarchical approach allows for the optimization of the student policy's capability trajectory through direct training of a teacher policy. However, this method presents a challenge: each transition in the upper-level MDP consists  of one/several complete lower-level student MDPs. Consequently, collecting the upper-level teacher agent's experience is slow and resource-intensive.

To accelerate the resource-intensive collection of upper-level MDP experience, we utilize advancements in generative models, such as diffusion models, adapting them to learn a trajectory model. Diffusion models can  generate new data points that capture complex distribution properties, such as skewness and multi-modality, exhibited in the collected dataset~\citep{saharia2022photorealistic}. Specifically, we employ a diffusion probabilistic model~\citep{sohl2015deep,ho2020denoising} to learn a model of student policy's capability trajectory.
Our trained diffusion model is capable of generating new synthetic experiences of student policy's trajectory, which can be used for teacher training. To that end. we introduce \emph{S}ynthetically-enhanced \emph{H}ierarchical \emph{E}nvironment \emph{D}esign ({\em SHED}), an adaptive approach that can automatically generate increasingly complex environments suited to the current capabilities of the student agent, resulting in promoting continuous learning.

In summary, we make the following contributions:
\begin{itemize}
    \item We build a novel hierarchical MDPs framework for UED, and provide a straightforward way to represent the student policy.
    \item We introduce {\em SHED}, which utilizes diffusion-based techniques to generate synthetic experiences. This method can accelerate the off-policy training process of the teacher agent.
    \item Our experiments across various domains demonstrate that our methods significantly outperform the existing state-of-the-art method in UED.
\end{itemize}
We believe that our method has the potential to significantly reduce the time and effort required to design suitable training environments for student agents, enabling the development of more capable and robust RL systems across a wide range of domains under the finite training horizon.







\section{Preliminaries}

In this section, we provide an overview two main research areas on which our work is based.

\subsection{Unsupervised Environment Design} \label{subsec:ued}

\citet{dennis2020emergent} first define UED in terms of an underspecified Partially Observable Markov Decision Process (UPOMDP), which is a tuple
$\mathcal{M}=<\sA,\sO,\mathbb{\theta},\sS^{\mathcal{M}},\mathcal{P}^{\mathcal{M}},\mathcal{I}^{\mathcal{M}},\mathcal{R}^{\mathcal{M}},\gamma>$. The objective is to generate a sequence of environments that effectively support the continual learning of the agent policy. Here  $\sA$ represents the set of actions, $\sO$ is the set of observations, $\sS^{\mathcal{M}}$ is the set of states determined by the underspecified environment  $\mathcal{M}$, similarly, $\mathcal{P}^{\mathcal{M}}$ is the environment-dependent transition function, and $\mathcal{I}^{\mathcal{M}}:\sA \rightarrow \sO$ is the environment-dependent observation function, $\mathcal{R}^{\mathcal{M}}$ is the reward function, and $\gamma$ is the discount factor. The students is trained to maximize their cumulative reward $V^{\mathcal{M}}(\pi) = \sum_{t=0}^T \gamma^t r_t$ for the current environment under a finite time horizon $T$, and $r_t$ are the collected rewards. Existing works on UED consist of two main strands: the RL-based environment generation approach and the domain randomization-based environment generation approach. 

The RL-based generation approach was first formalized by \citet{dennis2020emergent} as a self-supervised RL paradigm for generating a distribution over environments. This approach involves co-evolving an environment generator policy (teacher) with an agent policy (student), where the teacher's role is to create an environmental distribution that best supports the student agent's continued training. The teacher is trained to produce challenging yet solvable environments that maximize the defined objective of regret, which quantifies the performance difference between the current student agent and a well-trained agent within the current environment.  Following a different perspective, \citet{li2023diversity} utilize the concept of marginal benefit to guide the RL teacher in generating suitable environments. Marginal benefit measures the actual improvement obtained by the agent due to training on a specific environment. The authors incorporate an environment replay buffer to maintain a diverse set of environments with high learning potentials.

The domain randomization-based generation approach, on the other hand, involves randomly generating environments. \citet{jiang2021prioritized} propose to prioritize the storage of environments with high regret values, which are defined using Generalized Advantage Estimation (GAE)~\citep{schulman2015high}, The student agent can then sample from this collection of previously encountered environments for training. Additionally, \citet{parker2022evolving} adopt a different strategy by using predetermined starting points for environments and gradually increasing complexity. They manually divide the environment design space into different difficulty levels and employ human-defined edits to generate similar environments with high learning potentials. Their algorithm, ACCEL, is currently the state-of-the-art (SOTA) in the field, and we use it as a baseline in our experiments.

\subsection{Diffusion Probabilistic Models}\label{subsec:diff}

Diffusion models~\citep{sohl2015deep, ho2020denoising} are a specific type of generative model that learns the data distribution from a dataset. Recent advances in diffusion-based models, including Langevin dynamics and score-based generative models, have shown promising results in various applications, such as time series forcasting~\citep{tashiro2021csdi}, robust learning~\citep{nie2022diffusion}, anomaly detection~\citep{wyatt2022anoddpm} as well as synthesizing high-quality images from text desciptions~\citep{nichol2021glide,saharia2022photorealistic}. These models can be trained using standard optimization techniques, such as stochastic gradient descent, making them highly scalable and easy to implement.

In a diffusion probabilistic model, we assume a $D$-dimensional random variable $\vx \in \R^D$ with an unknown distribution $q_0(x_0)$. Diffusion Probabilistic model involves two Markov chains: a forward chain $\displaystyle q(\vx_t|\vx_{t-1})$ that perturbs data to noise, and a reverse chain that converts noise back to data. The forward chain is typically designed to transform any data distribution into a simple prior distribution (e.g., standard Gaussian) by considering perturb data with Gaussian noise of zero mean and $\beta_t$ variance for $T$ steps:
\begin{equation*}
    q(\vx_t|\vx_{t-1}) =  \mathcal{N} ( \vx_t ; \sqrt{1-\beta_t}\vx_{t-1} , \beta_t \mI) \quad q(\vx_{1:T}|\vx_{0}) = \Pi_{t=1}^T q(\vx_t|\vx_{t-1})
\end{equation*}

where $t\in\{ 0,\dots, T\}$ and $0<\beta_{1:T}<1$ denote the noise scale scheduling. At the end, $\vx_T \rightarrow \mathcal{N} (0,\mI)$ will converge to isometric Gaussian noise. According to the rule of the sum of normally distributed random variable, the choice of Gaussian provides a closed-form solution to generate arbitrary time-step $\vx_t$ through:
\begin{equation*}
    \vx_t = \sqrt{\bar{\alpha}_t}\vx_0 + \sqrt{1-\bar{\alpha}_t }\bm{\epsilon}, \quad \text{where} \quad \bm{\epsilon}\sim \mathcal{N} (0,\mI)
\end{equation*}
where $\alpha_t = 1- \beta_t$ and $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$. 
 The reverse chain $p_{\bm{\theta}}(\vx_{t-1}|\vx_t)$ reverses the forward process by learning transition kernels parameterized by deep neural networks. New data points are subsequently generated by first sampling a random vector from the prior distribution, followed by ancestral sampling through the reverse Markov chain. Specifically, considering the Markov chain parameterized by $\bm{\theta}$, denoising arbitrary Gaussian noise into clean data samples can be written as:
\begin{equation*}
    p_{\bm{\theta}}(\vx_{t-1}|\vx_t) =  \mathcal{N} ( \vx_{t-1} ; \vmu_{\bm{\theta}}(\vx_t,t) , \mSigma_{\bm{\theta}}(\vx_t,t))
\end{equation*}
It uses the Gaussian form $ p_{\bm{\theta}}(\vx_{t-1}|\vx_t)$ because the reverse process has the identical function form as the forward process when $\beta_t$ is small~\citep{sohl2015deep}.

\citet{ho2020denoising} show that the training to maximize the log-likelihood $\int q(x_0) \log p_{theta}(x_{0} d x_{0})$ is equivalent to minimize a re-weighted evidence lower bound (ELBO) that fits the noise. They derive the final objective by parameterization and simplication:
\begin{equation*}
    \mathcal{L}(\bm{\theta}) = \mathbb{E}_{\vx_0,t,\bm{\epsilon}} \left[ \lVert \bm{\epsilon}  - \bm{\epsilon}_{\bm{\theta}}(\sqrt{\bar{\alpha}_t}\vx_0 + \sqrt{1-\bar{\alpha}_t }\bm{\epsilon},t)  \rVert^2  \right]
\end{equation*}
where $\bm{\epsilon}_{\bm{\theta}}$ is a function approximator to predict $\bm{\epsilon}$ from $\vx_t$.



\section{Approach}
In this section, we formally present \emph{S}ynthesis-enhanced \emph{H}ierarchical \emph{E}nvironment \emph{D}esign ({\em SHED}) as a novel framework in UED and discuss it in detail.
% , our approach to upsampling an agent's collected experience using diffusion. 
We begin by describing the procedure for generating a distribution of environments through the hierarchical framework and then show how the generative model may be adapted to our continual training process.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{framework.PNG}
    \caption{The overall framework of {\em SHED}.}\label{fig:framework}
\end{figure}







\subsection{Hierarchical Environment Design}


Similar to previous work (\citep{dennis2020emergent}), our objective is to generate a sequence of environments that effectively support the continual learning of the student agent. Presently, mainstream approaches to the environment generation process (PLR~\citep{jiang2021replay},ACCEL~\citep{parker2022evolving}) often rely on domain randomization, which involves generating random environments. However, these approaches face challenges in effectively exploring the action space, particularly when dealing with a large parameter space, and they are not able to leverage previously discovered environmental structures. Motivated by the principles of the PAIRED~\citep{dennis2020emergent} algorithm, we adopt an RL-based approach for the environment generation process.

At the core of \algo is the hierarchical MDP framework, consisting of an upper-level RL teacher policy and a lower-level student policy. 
Specifically, our framework involves specifying the upper-level teacher policy, $\Lambda:\bm{\Pi} \rightarrow \Delta(\mathbb{\theta})$, where $\bm{\Pi}$ represents the set of possible student policies, and $\mathbb{\theta}$ signifies the range of potential environmental parameters.
Diverging from RL teacher in the PAIRED algorithm, which takes inputs comprising the current fully observed state of the environment, the current time step $t$, and a random vector $\vz\sim \mathcal{N}(0,\mI)$, our method first approximates the student agent's policy by assessing its performance across a diverse set of environments. This input provides a more accurate reflection of the student's current capability level, enabling the teacher to consistently design environments that effectively support continuous training. We now elaborate on the specifics of our hierarchical framework. 



Consider an environment generation system governed by discrete-time dynamics. Given the current batch of generated environments, each environment is a fully specified environment for students, characterized by a Partially Observable Markov Decision Process (POMDP), which is defined by a tuple $<\sA,\sO,\sS^{\bm{\theta}},\mathcal{P}^{\bm{\theta}},\mathcal{I}^{\bm{\theta}},\mathcal{R}^{\bm{\theta}},\gamma>$, where $\sA$ represents the set of actions, $\sO$ is the set of observations, $\sS^{\bm{\theta}}$ is the set of states determined by the environment parameters $\bm{\theta}$, similarly, $\mathcal{P}^{\bm{\theta}}$ is the environment-dependent transition function, and $\mathcal{I}^{\bm{\theta}}:\sA \rightarrow \sO$ is the environment-dependent observation function, $\mathcal{R}^{\bm{\theta}}$ is the reward function, and $\gamma$ is the discount factor. The students is trained to maximize their cumulative reward $V^{\bm{\theta}}(\pi) = \sum_{t=0}^T \gamma^t r_t$ for the current environment under a finite time horizon $T$. This forms the foundation of our lower-level student MDP. 

Once training within the current batch of environments is completed, we evaluate the student's performance across a diverse set of evaluation environments. The vector of performance metrics serves as an approximation of the student's policy embedding. Therefore, our teacher policy, $\Lambda$, maps the current student's policy to the most suitable environment for enhancing its performance, i.e., $\Lambda: \pi \rightarrow \Delta(\mathbb{\theta})$, where $\pi \in \Pi$ represents the current student policy. From the teacher's perspective, the evolving of student's policy can be represented by the upper-level MDP, characterized as tuple $<\sS, \sA, P, R, \gamma>$. Here, $\sS$ represents the student policy space, which is approximated by its performance across the evaluation environments set. and $\sA$ is the teacher's action space, which corresponds to the environment parameter space. $P$ is the transition function of student policy's performances, and $R$ is the reward function.
This hierarchical approach enables us to systematically measure and enhance the performance of the student agent across various environments and adapt the training process accordingly.

However, it's worth noting that collecting the teacher's experience in this hierarchical framework is notably resource-intensive and time-consuming, since each transition within the upper-level teacher MDP encompasses a/several full lower-level student MDPs. In the following section, we will formally introduce a generative model designed to streamline the collection of upper-level MDP experience. This will enable us to train our teacher policy more efficiently and effectively. The overall framework is shown in Figure~\ref{fig:framework}, and the pseudo-code is provided in Algorithm~\ref{alg}.


\begin{algorithm}[ht]
\caption{\algo}
\label{alg}
\begin{algorithmic}[1]
\REQUIRE 
   real data ratio $r\in [0,1]$, evaluate environment set $\bm{\theta}^{eval}$, reward function $R$ for teacher; 
\STATE \textbf{Initialize:} real replay buffer $\bm{\mathcal{B}}_{real}=\emptyset$, synthetic replay buffer $\bm{\mathcal{B}}_{syn}=\emptyset$, diffusion model $M$, teacher policy $\Lambda$;
\FOR{episode $ep=1,\dots,K$}
\STATE Initialize student policy $\pi$
\STATE Evaluate $\pi$ on $\bm{\theta}^{eval}$ and get initial state $s$
\FOR{step $t=0,\dots,T$}
    % \STATE Sample a replay decision, $\epsilon\sim U[0,1]$;
    \STATE use $\Lambda$ to generate new batch of environment parameters $\bm{\theta}$, and create $\mathcal{M}_{\bm{\theta}}(\pi)$
    \STATE train the $\pi$ on $\mathcal{M}_{\theta}$ to maximize $V^{\bm{\theta}}$
    % \STATE Train student's policy $\pi_S$ to maximize $V^{\theta}(\pi_S)$ ;
    \STATE evaluate $\pi$ on $\bm{\theta}^{eval}$ and get next state $s^\prime$
    \STATE compute teacher's reward $r_t$ according to $R$
    \STATE add teacher's experience $(\vs,\bm{\theta}, r_t,\vs^\prime)$ to $\bm{\mathcal{B}}_{real}$
    % \STATE Add the new level $\mathcal{M}_{\theta}$ to $\Lambda$ and update the replay probability $P_{replay}$ according to its regret value ;
    % \STATE Sample action: level $\mathcal{M}_{\theta}$ from $\Lambda$ according to $P_{replay}$\;
    % \STATE Collect student trajectories $\tau_S$ in $\mathcal{M}_{\theta}$ and compute $V^{\theta}(\pi_S) = \sum_{t=0}^{T'}\gamma^t r_t$;
    % \STATE Train student's policy $\pi_S$ to maximize $V^{\theta}(\pi_S)$ ;
    % \STATE Collect data with $\pi_T$ in the MDP ;
    \STATE train $M$ with samples from $\bm{\mathcal{B}}_{real}$ 
    \STATE generate samples from $M$ and add them to $\bm{\mathcal{B}}_{syn}$
    \STATE train $\Lambda$ on samples from $\bm{\mathcal{B}}_{real} \bigcup \bm{\mathcal{B}}_{syn}$ mixed with ratio $r$
    % \STATE Update the replay probability $P_{replay}$;
    \STATE set $\vs= \vs^\prime$;
\ENDFOR
\ENDFOR
\ENSURE The teacher policy $\Lambda$, student policy $\pi$, and diffusion model $M$
\end{algorithmic}
\end{algorithm}





\subsection{Generative Trajectory Modeling}
Here, we describe how to take advantage of recent advancements in generative models, such as the diffusion model to generate synthetic trajectories that can be used to help train the teacher agent, resulting in reducing the resource-intensive and time-consuming collection of upper-level teacher's experiences.

In this work, we deal with two distinct types of timesteps: one for the diffusion process and another for reinforcement learning. We use subscripts $i \in {1,\dots, N}$ to represent diffusion timesteps and subscripts $t \in {1,\dots,T}$ to represent trajectory timesteps.


Our diffusion model, denoted as $M$, may be utilized to generate synthetic teacher experiences by continually training the diffusion model on newly collected experiences. Let the tuple $(\vs,\va, r_t,\vs^\prime)$ denote synthetic experience generated by the diffusion model. We begin by randomly sampling the observed state from the real experience buffer $\vs\sim \bm{\mathcal{B}}_{real}$, and the generation of $(\vs,\bm{\theta}, r_t,\vs^\prime)$ can be divided into three steps: 1) generate actions $\va$ (which corresponds to the environment parameters $\bm{\theta}$); 2) given the state-action pair $\bm{\tau}=(\vs,\va)$, generate the next state $\vs^{\prime}$; 3) compute the reward $r_t$ based $(\vs,\bm{\theta},\vs^\prime)$.

In the first step, we can either generate our action through direct sampling from 
\begin{equation*}
    \va \sim \Lambda(\va|\vs)
\end{equation*}
or represent the action generation via the reverse process of a conditional diffusion model as 
\begin{equation*}
    \va \sim p_{\bm{\theta}}(\va_{0:N}|\vs)
\end{equation*}
where $\va_0$ at the end sample of the reverse chain is the action that we want. This process is similar to the way we generate $\vs^{\prime}$; therefore, we omit it here. The first one is straightforward to implement because we can stochastically generate different actions $\va$ directly based on the existing teacher's policy $\Lambda$.

For the second step, we generate our next state $\vs$ via the reverse process of a conditional diffusion model as:
\begin{equation*}
    \vs^{\prime} \sim p_{\bm{\theta}}(\vs_{0:N}^{\prime}|\bm{\tau})=\mathcal{N}(\vs_N^{\prime};\bm{0},\mI)\prod_{i=1}^N p_{\bm{\theta}}(\vs_{i-1}^{\prime}|\vs_i,\bm{\tau})
\end{equation*}
and $\vs_0^{\prime}$ at the end of sampling in the reverse chain is the generated synthetic next state. As shown in Section~\ref{subsec:diff}, $p_{\bm{\theta}}(\vs_{i-1}^{\prime}|\vs_i,\bm{\tau})$ could be modeled as a Gaussian distribution $\mathcal{N} ( \vs_{i-1} ; \vmu_{\bm{\theta}}(\vs_i^\prime,\tau,I),\mSigma_{\bm{\theta}}(\vs_i,\tau,i))$. Similar to \citet{ho2020denoising}, we parameterize $p_{\bm{\theta}}(\vs_{i-1}^{\prime}|\vs_i,\bm{\tau})$ as a noise prediction model with the covariance matrix fixed as 
$$\mSigma_{\bm{\theta}}(\vs_i,\tau,i)=\beta_i \mI$$
and mean is 
$$\vmu_{\bm{\theta}}(\vs_i^\prime,\tau,i) =\frac{1}{\sqrt{\alpha_i}}\left( \vs_i^\prime - \frac{\beta_i}{\sqrt{1-\bar{\alpha}_i} }\bm{\epsilon(\vs_i^\prime,\tau,i)} \right) $$

Following~\citet{wang2022diffusion}, we begin by sampling $\vs_N \sim \mathcal{N}(\bm{0},\mI) $ and then proceed with the reverse diffusion chain $p_{\bm{\theta}}(\vs_{i-1}^\prime | \vs_i^\prime)$ for $i=N,\dots, 1$, which is parameterized by $\bm{\theta}$ as follows:
\begin{equation}
    \frac{\vs_i^\prime}{\sqrt{\alpha_i}} - \frac{\beta_i}{\sqrt{\alpha_i(1-\bar{\alpha}_i})}\bm{\epsilon_{\bm{\theta}}(\vs_i^\prime,\tau,i)} + \sqrt{\beta_i}\bm{\epsilon},
\end{equation}\label{eq:s_prime}
where $\bm{\epsilon}\sim \mathcal{N}(\bm{0},\mI)$. When $i = 1$, $\bm{\epsilon}$ is set as $\bm{0}$ to improve the sampling quality. 
We employ a similar simplified objective, as proposed by~\citet{ho2020denoising} to train our conditional $\bm{\epsilon}$ through the following process:
\begin{equation}
        \mathcal{L}(\bm{\theta}) = \mathbb{E}_{(\bm{\tau}, \vs^\prime )\sim \bm{\mathcal{B}}_{real},i \sim \mathcal{U},\bm{\epsilon} \sim \mathcal{N}(\bm{0},\mI) } \left[ \lVert \bm{\epsilon}  - \bm{\epsilon}_{\bm{\theta}}(\sqrt{\bar{\alpha}_i}\vs^\prime + \sqrt{1-\bar{\alpha}_t }\bm{\epsilon},\bm{\tau},i)  \rVert^2  \right]
\end{equation}

Here $\mathcal{U}$ represents a uniform distribution over the discrete set $\{1,\dots, N\}$. The loss function of the diffusion model, denoted as $\mathcal{L}_{simple}(\bm{\theta})$ is essentially a behavior-cloning loss. it aims to generate the synthetic teacher's experience $(\vs,\bm{\theta}, r_t,\vs^\prime)$. Furthermore, this method enables us to learn from the human-generated experience as it is sampling-based and only requires taking random samples from both $\bm{\mathcal{B}}_{real}$ and the current policy without requiring to know the behavior policy. 


The Gaussian assumption holds true primarily under the condition of the infinitesimally limit of small denoising steps~\citep{sohl2015deep}. Consequently, this necessitates a substantial number of steps in the reverse process.
Furthermore, recent research by\citet{xiao2021tackling} have demonstrated that enabling denoising with large steps can reduce the total number of denoising steps, $N$. Consequently, in order to expedite the relatively slow reverse sampling process outlined in Equation~\ref{eq:s_prime} (as it requires computing $\bm{\epsilon}_{\bm{\theta}}$ networks $N$ times), we use a small value of $N $, while simultaneously setting $\beta_{\min} = 0.1$ and $\beta_{\max} = 10.0$. Similar to~\citet{wang2022diffusion}, we define:


\begin{equation*}
    \beta_i = 1-\alpha_i = 1-\exp\left(\beta_{\min}\times \frac{1}{N} - 0.5(\beta_{\max}-\beta_{\min})\frac{2i -1}{N^2}\right)
\end{equation*}

This noise schedule is derived from the variance-preserving Stochastic Differential Equation proposed by~\citet{song2020score}.


In the final step, after obtaining the generated action $\va$, and next state $\vs^\prime$, we compute the reward $r_t$ using the teacher's reward function $R(\vs, \va,\vs^{\prime})$. The specifics of how the reward function is chosen will be explained in the following section.

% where $\bm{\tau}^i=(\vs^i,\va^i)$.






\subsection{Rewards and Choice of evaluate environments}


Our upper-level teacher policy generates environments tailored specifically for the lower-level student policy, aligning with the most suitable environments to improve the general capability of the lower-level student policy. That is, at each time step $t$, the upper-level teacher policy generates a batch of environments, indicating its desire for the lower-level student agent to undergo training within these generated environments to enhance the student's overall capability across a diverse set of evaluation environments.


The selection of suitably diverse evaluation environments is crucial because they reflect the agent's general capabilities and serve as an approximation of the policy's embedding.  \citet{fontaine2021differentiable} propose the use of quality diversity (QD) optimization to generate collections of high-quality environments that exhibit diversity for the resulting agent behaviors. Similarly, \citet{bhatt2022deep} introduce a QD-based algorithm for dynamically designing such evaluation environments based on the current agent's behavior. However, it's worth noting that this QD-based approach can be tedious and time-consuming, and it heavily relies on the given agent policy.

Given these considerations, it is natural to take advantage of the domain randomization algorithm, as it has demonstrated compelling results in generating diverse environments and training generally capable agents. In our approach, we initially discretize environment parameters into different ranges, then randomly sample from these ranges, and combine these parameters to generate evaluation environments. This method has yielded promising empirical results. We define the reward for the upper-level teacher policy as a parameterized function based on the improvement in student performance on the evaluation environment after training in the current generated environment:
\begin{equation*}
    R(\vs, \va,\vs^{\prime}) = \Vert \vs-\vs^\prime \Vert_1
\end{equation*}

This rewards upper-level teachers for taking action to create the right environment to improve the overall performance of students across diverse environments. However, such a reward function primarily focuses on the overall performance of the student agent in the assessment environment. The teacher agent can gain higher rewards by sacrificing student performance in one subset of evaluation environments to improve student performance in another subset, which conflicts with our objective. Therefore, we need to consider fairness in the reward function to avoid this situation and ensure that the generated environment can improve student's general capabilities. Similar to \cite{elmalaki2021fair}, we build our fairness metric on top of the change in student's performance in each evaluation environment, denoted as $u_i=s_i^\prime - s_i$, and we have $\bar{u}=\frac{1}{n}\sum_{i=1}^n u_i$.
We then measure the fairness of the teacher's action using the coefficient of variation of student performances:
\begin{equation}
    cv(\vs, \va,\vs^{\prime}) = \sqrt{\frac{1}{n-1}\sum_i \frac{(u_i-\bar{u})^2}{\bar{u}^2}}
\end{equation}
A teacher is considered to be fair if and only if the $cv$ is smaller. As a result, our reward function is:
\begin{equation}
    R(\vs, \va,\vs^{\prime}) = \Vert \vs-\vs^\prime \Vert_1 - c\cdot cv(\vs, \va,\vs^{\prime})
\end{equation}

Here $c$ is the coefficient that balances the weight of fairness in the reward function (We set a small value to this, such as 0.05). This reward function motivates the teacher agent to generate training environments that can improve student's general capabilities.


\section{Experiments}
In our experiments, we first use a mathematical example to assess the quality of the synthetic experiences generated by the diffusion model. Subsequently, we compare \algo to the leading prior approaches in UED. Our experiments are conducted on two domains: Lunar Lander and a modified BipedalWalker environment. 
Our primary comparisons involve \algo against five baselines: domain randomization~\citep{tobin2017domain}, ACCEL~\citep{parker2022evolving} (with slight modifications that it does not revisit the previously generated environments), PAIRED~\citep{dennis2020emergent}, and h-MDP (our proposed hierarchical approach without diffusion model aiding in training).
In all cases, we train a student agent via Proximal Policy Optimization (PPO~\citep{schulman2017proximal}, and train the teacher agent via Deterministic policy gradient algorithms(DDPG~\citep{silver2014deterministic}), because it is more sample efficient and can learn from both real experience buffer or the synthetic experience buffer.



\subsection{Analyzing the accuracy of Diffusion model }
We begin by investigating {\em SHED}'s ability to assist in collecting experiences for the upper-level MDP teacher. This involves the necessity for {\em SHED} to prove its ability to accurately generate synthetic experiences for teacher agents. To check the quality of these generated synthetic experience, we employ a diffusion model to simulate some data for validation (even though Diffusion models have demonstrated remarkable success across vision and NLP tasks).

We design the following experiment: given input $\vs=[s_1,s_2,s_3]$, which represents the student's current performances on the evaluation environment set, and another input $\va=[a_1,a_2]$, which represents action of the teacher(environment parameters), we use the function $\vs^\prime=f(\vs ,\va)$ to simulate the student training process, where $\vs^\prime = [s_1^\prime,s_2^\prime,s_3^\prime]$ is the student's updated performances after training in the environment (generated by $\va$). The specific dynamic relationship is as follows:

\begin{equation*}
s_1^\prime=
\begin{cases} 
s_1 + 2 |s_1 - 2a_1| + \epsilon & \text{if } 1 < |s_1 - 2a_1| < 2 \\
s_1 - 1 + \epsilon & \text{otherwise}
\end{cases}
s_2^\prime=
\begin{cases} 
s_2 + \exp(s_2 - 5a_2) + \epsilon & \text{if } |s_2 - 5a_2| < 2 \\
s_2 - |s_2 - 5a_2| + \epsilon & \text{otherwise}
\end{cases}
\end{equation*}

\begin{equation*}
\text{and }\quad s_3^\prime=
\begin{cases} 
s_3 + \log(a_1*a_2 - s_3) + \epsilon & \text{if } 1<\log(a_1*a_2 - s_3)< 5 \\
s_3 - |\log(a_1*a_2 - s_3)| + \epsilon & \text{otherwise}
\end{cases}
\end{equation*}



We first train our diffusion model on the real $(\vs, \va, \vs^\prime)$ dataset generated by function $f(\vs,\va)$. We then randomly sample 1000 combination data of $\vs$ and $\va$, input them into $f(\vs,\va)$ to get the real $\vs^\prime$. The trained diffusion model is then used to generate the synthetic $\vs^\prime$ conditioned on $(\vs,\va)$ pair.

The experiment results are presented in Figure~\ref{fig:diffusion}. The results show that the generative model can effectively capture the distribution of real experience, and the generated synthetic experience is very close to the real  dynamic relationship $\vs^\prime=f(\vs,\va)$, thereby validating that the diffusion model can generate useful experience conditioned on $(\vs,\va)$. It is important to note that the marginal distribution derived from the reverse diffusion chain provides an implicit, expressive distribution, such distribution has the capability to capture complex distribution properties, including skewness and multi-modality.



\begin{figure}[t]
    \centering
    \begin{minipage}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{n_diffusion_1.PNG}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
        \includegraphics[width=\textwidth]{n_diffusion_2.PNG}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
    \includegraphics[width=\textwidth]{n_diffusion_3_1.PNG}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.245\textwidth}
    \includegraphics[width=\textwidth]{n_diffusion_3_2.PNG}
    \end{minipage}
    \caption{The distribution of the real $\vs^\prime$ and the synthetic $\vs^\prime$ conditioned on $(\vs,\va)$.}
    \label{fig:diffusion}
\end{figure}


\subsection{Lunarlander}

We next evaluate our approach in the Lunar Lander environment, a classic rocket trajectory optimization problem. In this domain, student agents are tasked with controlling a lander's engine to safely land the vehicle. Before the start of each episode, teacher algorithms determine the environment parameters that are used to generate environments in a given play-through, which includes gravity, wind power, and turbulence power. These parameters directly alter the difficulty of landing the vehicle safely.
The state is an 8-dimensional vector, which includes the coordinates of the lander, its linear velocities, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.

We train the student agent for 1e6 environment time steps and periodically evaluate the agent in test environments that are not present in the training or evaluation environments (performances vector on the evaluation environments are used as the teacher agent's state). The parameters for the test environments are randomly generated and fixed during training. We report the experiment result on the left side of Figure~\ref{fig:performance}. As we can see, student agents trained under \algo consistently outperform other baselines and have the minimal variance in transfer performance. ACCEL experiences a performance dip in the middle during training. This phenomenon could potentially be attributed to the inherent challenge of manually design the appropriate level of difficulty in the parameter space.

In the appendix, we detail how the performance of different methods changes in each testing environment during training. Furthermore, we conduct experiments to show how the algorithm performs under different settings, such as a longer training time horizon, or a larger weight of cv fairness rewards. We noticed an interesting finding: when fairness reward has a high weightage, our algorithm tends to generate environments at the onset that lead to a rapid decline and subsequent improvement in students' performance across all test environments. This is done to avoid acquiring a substantial negative fairness reward and thereby maximize the teacher's cumulative reward. Notably, the student's final performance still surpasses other baselines. See the appendix for detailed results.


\begin{figure}[t]
    \centering
    \begin{minipage}[b]{0.46\textwidth}
        \includegraphics[width=\textwidth]{lander_paired.PNG}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.46\textwidth}
        \includegraphics[width=\textwidth]{walker_paired.PNG}
    \end{minipage}
    \caption{({\em left}) Zero-shot transfer performance on the test environments in the Lunar lander environment. ({\em right}) Zero-shot transfer performance on the test environments in the BipedalWalker.}
    \label{fig:performance}% We show that \algo demonstrates strong performance when compared to the baseline methods with the best average transfer performance and 
\end{figure}




\subsection{Bipedalwalker}
Finally, we evaluate \algo in the modified BipedalWalker from~\citet{parker2022evolving}. In this domain, the student agent is required to control a bipedal vehicle and navigate across a terrain, and the student agent receives a 24-dimensional proprioceptive state with respect to its lidar sensors, angles, and contacts. The teacher is tasked to select eight
variables (including ground roughness, the number of stair
steps, min/max range of pit gap width, min/max range of
stump height, and min/max range of stair height) in order to generate the corresponding terrain. 
% \subsection{Evaluate the fake trajectories}

We use the simialr experiment settings in prior UED works, we
train all the algorithms for 1e7 environment time steps, and then evaluate their generalization ability on ten distinct
test environments in Bipedal-Walker domain. The parameters for the test environments are randomly generated and fixed during training. As shown in Figure~\ref{fig:performance}, our proposed method
\algo  surpasses all other baselines, and achieves performance levels nearly on par with the SOTA (ACCEL). Meanwhile,  PAIRED suffers from a considerable degree of variance in its performance.

\section{Conclusion}
In this paper, we have introduced an adaptive approach for efficient training of a generally capable agent within a finite time horizon. Our approach is general, utilizing an upper-level MDP teacher agent that can guide the training of the lower-level MDP student agent. Our hierarchical framework can incorporate any techniques developed from existing UED works, such as prioritized level replay (revisiting environments with high learning potential). Furthermore, we have described a method to assist the experience collection for the teacher when it is trained in an off-policy manner. Our experiment demonstrates that our method outperforms prior UED methods, underscoring its effectiveness as a curriculum-based learning approach within the UED framework.






















\bibliography{neurips_2023}
\bibliographystyle{plainnat} 
\appendix






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Appendix}


\subsection{Generative trajectory modeling}
The trajectory modeling can be represented via the reverse process of a conditional diffusion model as
\begin{equation*}
    \va \sim \Lambda(\va|\vs) = p_{\bm{\theta}}(\va^{0:N}|\vs)=\mathcal{N}(\va^N;\bm{0},\mI)\prod_{i=1}^N p_{\bm{\theta}}(\va^{i-1}|\va^i,\vs)
\end{equation*}
where $\va^0$ at the end sample of the reverse chain is the action that we want. 
\subsection{Fairness}


The current main focus is on the OOD performance with respect to the average per-student efficiency measure (e.g., worst performance of each student). Thus, for teacher-student framework, fairness is a key factor to ocnsider in their design for their successful deployments and operations.

Fairness is a multifaceted concept, which can refer to or include different aspects, i.e., Pareto-efficiency, "envy-freeness", or proportionality among students. Given the importance of this notion, it has been investigated in various applications. 

For example:
\begin{itemize}
    \item \cite{zimmer2021learning} define fairness that refers to the combination of three aspects:  impartiality, equity, and efficiency. The author consider the complex control problems, but in the cooperative setting. Impartiality corresponds to the ”equal treatment of equals” principle, which is arguably one of the most important pillars of fairness. They  assume that all users are identical and should therefore be treated similarly.  Equity is based on the Pigou-Dalton principle,  which states that a reward transfer from a better-off user to a worse-off user yields a fairer solution. Efficiency states that between two feasible solutions, if one solution is (weakly or strictly) preferred by all users, then it should be preferred to the other one.
    \item \cite{elmalaki2021fair} propose fairness in multi-human IoT applications. 
\end{itemize}

Similar to \cite{elmalaki2021fair}, we can build our fairness on top of the utility. we define the utility of each student agent as :
\begin{equation}
    u_t=\frac{1}{T}\sum_{1}^T \gamma^t r_t
\end{equation}
We then can measure the fairness of the RL generator using the coefficient of variation of student utilities:
\begin{equation}
    cv = \sqrt{\frac{1}{n-1}\sum_i \frac{(u_i-\bar{u})^2}{\bar{u}^2}}
\end{equation}
A teacher is said to be fair if and only if the $cv$ is smaller. Note that we can also use the marginal benefit of each agent as the utility (based on our previous work)
Fairness is a multifaceted concept, which can refer to or include different aspects.


\subsection{Additional experiments}
We further conduct experiments in Lunar lander under a longer time horizon. The results are provided in Figure~\ref{fig:lander_long}.

\begin{figure}[th]
    \centering
    \includegraphics[width=0.9\linewidth]{lander_long.PNG}
    \caption{Zero-shot transfer performance on the test environments under a longer time horizon in Lunar lander environments. }\label{fig:lander_long}
\end{figure}

we also conduct experiments to show how the algorithm performs under different settings, such as a larger weight of cv fairness rewards ($c=1$). The results are provided in Figure~\ref{fig:lander_cv}. We noticed an interesting finding: when fairness reward has a high weightage, our algorithm tends to generate environments at the onset that lead to a rapid decline and subsequent improvement in students' performance across all test environments. This is done to avoid acquiring a substantial negative fairness reward and thereby maximize the teacher's cumulative reward. Notably, the student's final performance still surpasses other baselines.
\begin{figure}[th]
    \centering
    \includegraphics[width=0.9\linewidth]{lander_cv.PNG}
    \caption{Zero-shot transfer performance on the test environments with a larger $cv$ value coefficient in Lunar lander environments. }\label{fig:lander_cv}
\end{figure}

We further show in detail how the performance of different methods changes in each testing environment during training (see Figure~\ref{fig:10landers} and Figure~\ref{fig:10landers_2} ).
\begin{figure}[t]
\centering
\subfigure{\includegraphics[width=0.49\textwidth]{lander_1.PNG}}
\subfigure{\includegraphics[width=0.49\textwidth]{lander_2.PNG}}
\subfigure{\includegraphics[width=0.49\textwidth]{lander_3.PNG}}
\subfigure{\includegraphics[width=0.49\textwidth]{lander_4.PNG}}
\subfigure{\includegraphics[width=0.49\textwidth]{lander_5.PNG}}
\subfigure{\includegraphics[width=0.49\textwidth]{lander_6.PNG}}
\subfigure{\includegraphics[width=0.49\textwidth]{lander_7.PNG}}
\subfigure{\includegraphics[width=0.49\textwidth]{lander_8.PNG}}
\caption{Caption for all images}
\label{fig:10landers}
\end{figure}

\begin{figure}[t]
\centering
\subfigure{\includegraphics[width=0.49\textwidth]{lander_9.PNG}}
\subfigure{\includegraphics[width=0.49\textwidth]{lander_10.PNG}}
\caption{Caption for all images}
\label{fig:10landers_2}
\end{figure}


\end{document}