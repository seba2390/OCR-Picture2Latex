\documentclass{article}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts


% if you need to pass options to natbib, use, e.g.:
%     
\PassOptionsToPackage{round}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[final]{neurips_2023}




\usepackage{titletoc}
\usepackage[page, header, toc, page]{appendix} % MAKE SURE THIS IS LOADED BEFORE hyperref PACKAGE!

\usepackage[usenames,dvipsnames]{xcolor}   % NEEDS to be first!
\usepackage{hyperref}       % hyperlinks

\definecolor{darkblue}{rgb}{0.0,0.0,0.65}
\definecolor{darkred}{rgb}{0.68,0.05,0.0}
\definecolor{darkgreen}{rgb}{0.0,0.29,0.29}
\definecolor{darkpurple}{rgb}{0.47,0.09,0.29}
\hypersetup{
colorlinks = true,
citecolor  = darkblue,
linkcolor  = darkred,
filecolor  = darkblue,
urlcolor   = darkblue,
}


\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
%\usepackage{xcolor}         % colors
\usepackage{csquotes}

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amsmath} 


\DeclareMathOperator{\E}{\mathbb{E}}

\usepackage{amsmath,amsthm, amssymb, bbm,  color, natbib,mathtools}

\newcommand{\F}{\mathcal{F}}
\newcommand{\N}{\mathcal{N}}
\renewcommand{\P}{\mathcal{P}}
\renewcommand{\L}{\mathcal{L}}

\newcommand{\R}{\mathbb{R}}

\renewcommand{\S}{{\mathcal S}}

\newtheorem{theorem}{Theorem} 
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{conjecture}{Conjecture}
\newtheorem{assumption}{Assumption}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{condition}{Condition}
\newtheorem{setting}{Setting}
\newtheorem{example}{Example}

\usepackage{mdframed}
\usepackage{algorithmicx}


\usepackage{tikz}
\usetikzlibrary{positioning}

\usepackage{float}
\usepackage{wrapfig}

\def\lemmaautorefname{Lemma} 
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
\def\appendixautorefname{Appendix}




\DeclareMathOperator{\Dist}{Dist} 



\newcommand{\softatt}{ \mathrm{Attn}^{\sf smax}}
\newcommand{\att}{\mathrm{Attn}}
\newcommand{\atth}{\mathrm{Attn}^\sigma} 
\newcommand{\relu}{\sigma}

\renewcommand{\aa}{M}
\newcommand{\bb}{A}
\newcommand{\bbb}{a} 
\newcommand{\cc}{b}
\newcommand{\ttbb}{\widetilde{\bb}}

\newcommand{\ttcc}{\widetilde{\cc}}

\newcommand{\wstar}{w_\star}
\newcommand{\twstar}{\widetilde{w}_\star}
\newcommand{\tz}[1]{{z^{(#1)}}}
\newcommand{\tx}[1]{x^{(#1)}}
\newcommand{\ttx}[1]{\widetilde{x}^{(#1)}}
\newcommand{\ty}[1]{y^{(#1)}}
\newcommand{\NF}{{\sf TF}} 
\newcommand{\ttf}{f_{\sf lower}} 

\newcommand{\MM}{\mathsf{G}}
\newcommand{\MMR}{\mathsf{G}^{\sf relu}}

\newcommand{\ind}[1]{\mathbbm{1}\lrbb{#1}}
\newcommand{\indi}{\mathbb{I}} 

\newcommand{\emphh}[1]{\textbf{\emph{#1}}}
\DeclareMathOperator{\tr}{Tr} 
\newcommand{\smax}{\mathrm{softmax}}
\newcommand{\inp}[2]{\left \langle #1,#2\right\rangle}
\newcommand{\inpp}[2]{ \langle #1,#2\rangle}
\newcommand{\wgd}{w^{\sf gd}} 
\newcommand{\pp}{P}
\newcommand{\qq}{Q}
\newcommand{\setSI}{\mathcal{S}_{\text{Id}}}
\newcommand{\setSD}{\mathcal{S}_{\text{Diag}}}
\newcommand{\setW}{\mathcal{W}}

\newcommand{\setWQ}{\mathcal{W}^Q}
\newcommand{\setSIQ}{\mathcal{S}^Q_{\text{Id}}}
\newcommand{\setSDQ}{\mathcal{S}^Q_{\text{Diag}}}
\newcommand{\dist}{\mathcal{D}}

\newcommand*\lin[1]{\left\langle #1 \right\rangle}

\newcommand*\lrb[1]{\left[ #1 \right]}
\newcommand*\lrn[1]{\left\| #1 \right\|}
\newcommand*\lrp[1]{\left( #1 \right)}
\newcommand*\lrbb[1]{\left\{ #1 \right\}}

\renewcommand\th{{\tilde{h}}}
\newcommand\jA{{\mathcal{J}_{\th}}}
\newcommand\tf{{\tilde{f}_\delta}}
\newcommand\cd{{c_\delta}}
\newcommand{\trelu}{\widetilde{\mathsf{relu}}_\delta}


\newcommand{\vectornorm}[1]{\left\|#1\right\|}
\newcommand{\norm}[1]{\left\| #1 \right\|}

\newcommand{\e}{\mathbf{e}}

\newcommand{\model}{C}

\newcommand{\diag}{\mathrm{diag}} 


\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newcommand*\at[2]{\left.#1\right|_{#2}}
\newcommand{\US}{U_\Sigma}

\makeatletter
\newcommand{\ostar}{\mathbin{\mathpalette\make@circled\star}}
\newcommand{\make@circled}[2]{%
\ooalign{$\m@th#1\smallbigcirc{#1}$\cr\hidewidth$\m@th#1#2$\hidewidth\cr}%
}
\newcommand{\smallbigcirc}[1]{%
\vcenter{\hbox{\scalebox{0.77778}{$\m@th#1\bigcirc$}}}%
}
\makeatother





\newcommand{\suv}[1]{\textcolor{green!70!blue!80}{[SS: \it #1}]}
\newcommand{\xc}[1]{{\color{black!2!green} [Xiang: #1]}}
\newcommand{\hlt}[1]{{\color{black!2!green} {\boldsymbol{#1}}}}

\mathtoolsset{showonlyrefs}



\title{Transformers learn to implement preconditioned gradient descent for in-context learning}


\author{ Kwangjun Ahn\thanks{Equal contribution, alphabetical order.}\\
MIT EECS/LIDS\\  
\texttt{kjahn@mit.edu} 
\And
Xiang Cheng$^*$\\
MIT LIDS\\  
\texttt{chengx@mit.edu} 
\And
Hadi Daneshmand$^*$\\
MIT LIDS/FODSI\\  
\texttt{hdanesh@mit.edu} 
\And
Suvrit Sra\\
TU Munich / MIT\\  
\texttt{suvrit@mit.edu} 
}


\begin{document}


\maketitle

\begin{abstract}   
Several recent works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate iterations of gradient descent. Going beyond the question of expressivity, we ask: \emph{Can transformers learn to implement such algorithms by training over random problem instances?} To our knowledge, we make the first theoretical progress on this question via an analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy.  For a transformer with $L$ attention layers, we prove certain critical points of the training objective implement $L$ iterations of preconditioned gradient descent. Our results call for future theoretical studies on learning algorithms by training transformers. 
\end{abstract} 

\section{Introduction} 
In-context learning (ICL) is the striking capability of large language models: Given a prompt containing examples and a query, the transformer produces the correct output based on the context provided by the examples, \emph{without adapting its parameters}~\citep{brown2020language,lieber2021jurassic, rae2021scaling, black2022gpt}. This property has become the focus of body of recent research that aims to shed light on the underlying mechanism of large language models~\citep{garg2022can,akyurek2022learning,von2022transformers,li2016learning,min2021metaicl,xie2021explanation,elhage2021mathematical,olsson2022context}.

A line of research studies ICL via the expressive power of transformers. Transformer architectures are powerful  Turing machines,  capable of implementing various algorithms~\citep{perez2021attention,wei2022statistically}. Given an in-context prompt,    
\cite{edelman2022inductive,olsson2022context} argue that transformers are able to implement algorithms through the recurrence of multi-head attentions to extract coarse information from raw input prompts. \citet{akyurek2022learning,von2022transformers} assert that transformers can implement gradient descent on linear regression encoded in a given input prompt. It is thought provoking that transformers can implement such algorithms.

Although transformers are universal machines to implement algorithms, they need specific parameter configurations for achieving these implementations. In practice, their parameters are adjusted via training using non-convex optimization over random problem instances. Hence, it remains unclear whether this non-convex optimization can be used to learn algorithms. The present paper investigates \emph{the possibility of learning algorithms via training over random problem instances.}

More specifically, we investigate the learning of gradient-based methods.  It is hard to mathematically formulate what it means to learn gradient descent for general functions with transformers. Yet, \citet{garg2022can} elegantly examine it in the specific setting of ICL for learning functions. Empirical evidence suggests that transformers indeed learn to implement gradient descent, after training on random instances of linear regression~\citep{garg2022can,akyurek2022learning,von2022transformers}. % This proclivity for learning gradient descent is somewhat surprising,  as the transformer is over-parameterized and thus expressive enough to implement various complicated optimization methods that may work just as well.  
Motivated by these observations, we theoretically investigate the loss landscape of a simple transformer architecture based on \emphh{ attention without softmax}~\citep{schlag2021linear,von2022transformers} (see \autoref{sec:problem} for details).

\emphh{Summary of our main results.} 
Our main contributions are the following:
\begin{list}{$\blacktriangleright$}{\leftmargin=1.5em}
\vspace*{-6pt}
\setlength{\itemsep}{1pt}
\item We provide a complete characterization of the global optimum of a single-layer linear transformer. In particular, we observe that, with the optimal parameters, the transformer implements a single step of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the distribution of input data but also to the variance caused by data inadequacy. We present this result in \autoref{thm:main_single} in \autoref{sec:single}.

\item Next, we focus on a subset of the transformer parameter space, defined by a special sparsity condition \eqref{eq:sparse_attention}. Such a parameter configuration allows us to formulate training transformers as a search over \emph{$k$-step adaptive gradient-based algorithms}. \autoref{t:two_layer} characterizes the global minimizers of the training objective of a two-layer linear transformer over isotropic regression instances, and shows that the optima correspond to gradient descent with adaptive stepsizes. For multilayer transformers, \autoref{t:L_layer_P_0} demonstrates that gradient descent, with a data-dependent preconditioning, can be derived from a critical point of the training objective. 

\item Finally, we study the loss landscape in the absence of the sparsity condition \eqref{eq:sparse_attention}, which goes beyond searching over conventional gradient-based optimization methods. In this case, we prove and interpret the structure of a critical point of the training objective. We show that a certian critical point in parameter space leads to an intriguing gradient-based algorithm that simultaneously takes gradient steps preconditioned by data covariance, and applies a linear transformation to further improve the conditioning. In the specific case when data covariance is isotropic, this algorithm corresponds to the GD++ algorithm of \citet{von2022transformers} which is  experimentally observed to be the outcome of training. 

\end{list}
We empirically validate the critical points analyzed in \autoref{t:L_layer_P_0} and \autoref{t:L_layer_P_identity}.
For a transformer with three layers, our experimental results confirm the structural of critical points. Furthermore, we observed the objective value associated with these critical points is close to $0$, suggesting that the critical points might be global optima. These experiments substantiate our theoretical analysis and suggests that our theory indeed \emph{aligns with practice}.  Code for our experiments is available at \url{https://github.com/chengxiang/LinearTransformer}.

% We acknowledge that the \emph{linear attention} model appears to be a significant departure from the standard (softmax) attention model. However, we present evidence in  \autoref{sec:problem} that linear attention is \emph{more suited} for the linear ICL problem, and may in fact be a good proxy for understanding Transformer learning in broader generality.


\subsection{Related works}  


The ability of neural network architectures to implement algorithms has been investigated in various context. The seminal work by \cite{siegelmann1992computational} investigate the Turing completeness of recurrent neural networks. Despite this computational power, training recurrent networks remains a challenge.  \cite{graves2014neural} design an alternative neural architecture known as the \emph{neural Turing machine}, building on \emph{attention layers} introduced by \cite{hochreiter1997long}. Leveraging attention, \cite{vaswani2017attention} propose transformers as powerful neural architectures, capable of solving various tasks in natural language processing~\citep{bert}. This capability inspired a line of research that examines the algorithmic power of transformers \citep{perez2021attention,wei2022statistically,giannou2023looped,akyurek2022learning,olsson2022context}.    What sets transformers apart from conventional neural networks is their impressive performance after training. In this work, we focus on understanding \emph{how transformers learn to implement algorithms} by training over problem instances.


A line of research investigates how deep neural networks process data across their layers. The seminal work by \cite{jastrzebski2018residual} observes that hidden representations across the layers of deep neural  networks approximately implement gradient descent.  
Recent observations provide novel insights into the working mechanism of ICL for large language models, showing they can implement optimization algorithms across their layers~\citep{garg2022can,akyurek2022learning,von2022transformers}. Moreover, \citet{zhao2023transformers,allen2023physics} observe transformer perform dynamic programming to generate text.  
In this work, we theoretically study how transformer learns gradient-based algorithms for ICL.


 We discuss here two related works \citep{zhang2023trained,mahankali2023one} that appeared shortly after publication of our original draft. Both of these studies focus on a single layer attention network (see  \autoref{sec:single}). \cite{zhang2023trained} prove the global convergence of gradient descent to the global optimum whose structure is analyzed independently from this study and it the same as that in~\autoref{thm:main_single}. \cite{mahankali2023one} also characterize the global minimizer of a single layer attention without softmax for a different data distribution. In addition to results for a single-layer attention, we analyze the landscape of two and multi-layer transformers. 
 
 


\section{Setting: training linear transformers over random linear regression}  

\label{sec:problem} 


In order to understand the mechanism of ICL, we consider the setting of training transformers over the random instances of linear regression, following \citep{garg2022can,akyurek2022learning,von2022transformers}.
In particular, the random instances of linear regression are formalized as follows.

\underline{\emphh{Data distribution: random linear regression instances.}}
Let  ${\tx{i}} \in \R^d$  be the covariates drawn i.i.d.\ from a distribution $D_{\mathcal{X}}$, and $\wstar\in \R^d$ be drawn from  $D_{\mathcal{W}}$.
Let $X \in \R^{(n+1)\times d}$ be the matrix of covariates  whose row $i$ contains tokens ${\tx{i}}$.  
Given $\tx{i}$'s and $\wstar$, the responses are defined as $y = [ \langle\tx{1}, \wstar \rangle,\dots,  \langle\tx{n},\wstar \rangle] \in \R^n$. Define the \emphh{input matrix} $Z_0$ as
\begin{align}
\label{d:Z_0}
Z_0 = \begin{bmatrix}
\tz{1} \ \tz{2} \ \cdots \ \tz{n}  \ \tz{n+1}
\end{bmatrix} = \begin{bmatrix}
\tx{1} & \tx{2} & \cdots & \tx{n} &\tx{n+1} \\ 
\ty{1} & \ty{2} & \cdots &\ty{n}& 0
\end{bmatrix} \in \R^{(d+1) \times (n+1)},
\end{align}
where zero in the above matrix is used to replace the unknown response variable corresponding to $\tx{n+1}$.  
Then, our goal is to predict  $\wstar^\top \tx{n+1}$ given $Z_0$. 
In other words, the training data consists of pairs $(Z_0, \wstar^\top \tx{n+1})$ for $\tx{i}\sim D_{\mathcal{X}}$ and 
$\wstar \sim D_{\mathcal{W}}$.
We then consider training transformers over this data distribution.



\underline{\emphh{Self-attention layer without softmax.}}
Following \citep{schlag2021linear,von2022transformers}, we consider the linear self-attention layer.
To motivate, we first briefly review the standard self-attention layer \citep{vaswani2017attention}. Letting  $Z\in \R^{(d+1) \times (n+1)}$ be the input matrix with $n+1$ tokens in $\R^{d+1}$, a single-head self-attention layer denoted by $\softatt$ is a parametric map defined as
\begin{align} \label{eq:softmax}
\softatt_{W_{k,q,v}}(Z) =  W_v Z \aa  \cdot {\sf smax}(Z^\top W_k^\top W_q Z)\,, \quad \aa \coloneqq \begin{bmatrix} I_n & 0 \\0 & 0 \end{bmatrix} \in \R^{(n+1) \times (n+1)},
\end{align}

where $W_v, W_k,W_q \in \R^{(d+1)\times (d+1)}$ are  the  (value, key and query) weight matrices, and $\mathrm{smax}(\cdot)$ is the softmax operator which applies softmax operation to each column of the input matrix. Note that the prompt is asymmetric since the label for $\tx{n+1}$ is excluded from the input. To reflect this asymmetric structure, the mask matrix $M$ is included in the attention.
In our setting, we consider the self-attention layer that omits the softmax operation in \eqref{eq:softmax}. In particular, we reparameterize weights as $P\coloneqq W_v\in \R^{(d+1)\times (d+1)}$ and  $Q  \coloneqq {W_k}^\top W_q \in \R^{(d+1)\times (d+1)}$ and consider  
\begin{align} \label{eq:linear}
\att_{P,Q}(Z) = P Z \aa (Z^\top Q Z) \,.
\end{align}  
At first glance, the omission of the softmax operation \eqref{eq:linear} might seem over-simplified. But, \citep{von2022transformers} proves such attention can implement gradient descent, and we will prove in \autoref{lem:express} that it can also implement various algorithms to solve linear regression in-context.

\underline{\emphh{Architecture for prediction.}}
We now present the neural network architecture that will be used throughout this paper.  For the number of layers $L$, we define an \emphh{$L$-layer transformer} as a stack of $L$ linear self-attention blocks. Formally, denoting by $Z_\ell$ the output of the $\ell^{\text{th}}$ layer attention, we define
\begin{align} \label{eq:recursion}
Z_{\ell+1} = Z_{\ell} +\frac{1}{n}  \att_{P_\ell,Q_\ell}(Z_\ell)\quad \text{for $\ell=0,1,\dots,L-1$},
\end{align}  
The scaling factor $\nicefrac{1}{n}$ is used only for ease of notation and does not influence the expressive power of the transformer.
Given $Z_L$, we define $\NF_L (Z_0; \{P_\ell,Q_\ell\}_{\ell=0,1,\dots L-1})  = -[Z_{L}]_{(d+1),(n+1)}$, i.e., the $(d+1,n+1)$-th entry of $Z_{L}$. 
The reason for the minus sign is to be consistent with \citep{von2022transformers}, and we will  clarify such a choice in \autoref{lem:express}. For training, the parameters are optimized to minimize in-context loss as 
\begin{align} \label{def:ICL linear}
f\left(\{P_\ell, Q_\ell\}^{L}_{\ell=0}\right) = \E_{(Z_0,\wstar)} \Bigl[ \left( \NF_L(Z_0, \{ P_\ell, Q_\ell \}_{\ell=0}^L)+ \wstar^\top \tx{n+1}  \right)^2\Bigr].
\end{align}  

\emphh{\underline{Goal: the landscape analysis of the training objective functions.}}
We are interested in understanding how the optimization of $f$ leads to in-context learning. We investigate this question by analyzing its loss landscape.
Such analysis is challenging due to two major reasons: \emph{(i) $f$ is non-convex in parameters $\{ P_i, Q_i\}$ even for a single layer transformer.
(ii) The cross-product structures in attention makes $f$ a highly nonlinear function in its parameters. }
Hence, we analyze a spectrum of settings from single-layer transformers to multi-layer transformers. For simpler settings such as single-layer transformers, we prove stronger results such as the full characterization of the global minimizers. For networks with more layers, we characterize the structure of critical points. Furthermore, we provide algorithmic interpretations of the critical points. Table~\ref{tab:summary} summarizes our results for various parameteric models. 

\begin{table}[h!]
\centering
\begin{tabular}{c|l l l l}
Results  & $\tx{i}$& $\wstar$ &  Setting & Guarantees \\
\hline
\hline
\autoref{thm:main_single} & $\N(0,\Sigma)$ & $\N(0,I)$ & single-layer & global minimizers \\
\autoref{t:two_layer} & $\N(0,I)$ & $\N(0,I)$ & two-layer +  symmetric \eqref{eq:sparse_attention}   & global minimizers \\
\autoref{t:L_layer_P_0} & $\N(0,\Sigma)$ & $\N(0,\Sigma^{-1})$ & multi-layer + \eqref{eq:sparse_attention}   & critical points \\
\autoref{t:L_layer_P_identity} & $\N(0,\Sigma)$ & $\N(0,\Sigma^{-1})$ & multi-layer +    \eqref{eq:full_attention} & critical points \\
\autoref{thm:nonlinear} & $\N(0,I)$ & $\N(0,I)$ & single-layer +  ReLU activation & global minimizers  \\
\hline
\hline
\end{tabular}
\vspace{5pt}
\caption{Summary of our analyses for various models and input distributions. The additional conditions \eqref{eq:sparse_attention} and  \eqref{eq:full_attention} are about the sparsity structure of parameters. In addition, ``symmetric \eqref{eq:sparse_attention}'' means we additionally impose the weights to be symmetric.}
\label{tab:summary}
\end{table}


\begin{remark}[\emphh{Optimizing \eqref{def:ICL linear} vs. practical transformer optimization}]
Interestingly, a recent work by \cite{ahn2023linear} reports that common optimization algorithms such as SGD/ADAM behave remarkably similarly on the (linear Transformers + linear regression) problem as they do on (practical transformers + real language modeling tasks).
In particular, they reproduce several distinctive features of transformer optimization under a simple shallow linear transformer. This work suggests that (linear transformer + linear regression) may serve as a good proxy for understanding practical transformer optimization. 
\end{remark}

 

\section{The global optimum for a single-layer transformer}
\label{sec:single}
For the single layer case of $L=1$, the following result characterizes the optimal parameters $P_0$ and $Q_0$ for the in-context loss \eqref{def:ICL linear}.  
 
\begin{theorem}
[\textbf{Single-layer; non-isotropic data}] \label{thm:main_single}
Assume that vector $\tx{i}$ is sampled from $\mathcal{N}(0, \Sigma)$, i.e., a Gaussian with covariance $\Sigma = U\Lambda U^\top$ where $\Lambda = \mathrm{diag}(\lambda_1,\dots, \lambda_d)$. 
Moreover, assume that  $\wstar$ is sampled  from $\mathcal{N}(0, I_d)$.  
Then, the following choice of parameters
\begin{align} \label{eq:single_layer}
P_0 = \begin{bmatrix}
0_{d\times d} & 0 \\ 
0 & 1 
\end{bmatrix} ,\quad Q_0 =  - \begin{bmatrix}
U \mathrm{diag}\left(\left\{\frac{1}{  \frac{n+1}{n}  \lambda_i +   \frac{1}{n} \cdot \left( \sum_k \lambda_k  \right)   } \right\}_{i=1,\dots,d}\right) U^\top & 0\\
0  & 0
\end{bmatrix} .
\end{align} 
is a  global minimizer of $f(P,Q)$ up to re-scaling, i.e., $P_0 \leftarrow \gamma P_0$ and $Q_0 \leftarrow \gamma^{-1} Q_0$ for a scalar $\gamma$.
\end{theorem}  

See \autoref{sec:single_proofs} for the proof of \autoref{thm:main_single}. In the specific case when the Gaussian is isotropic, i.e., $\Sigma = I_d$, the optimal $Q_0$ has the following simple form 
\begin{align} \label{minimum:linear}
Q_0 = - \frac{1}{\left( \frac{n-1}{n} +  (d+2) \frac{1}{n} \right)}  \begin{bmatrix}I_d &0   \\ 0 &0\end{bmatrix}.
\end{align}
Up to scaling, the above parameter configuration is equivalent to the parameters used by \citet{von2022transformers} to perform one step of gradient descent. Thus, in the single-layer setting, the in-context loss is indeed minimized by a transformer that implements the gradient descent algorithm.

More generally, when the in-context samples are non-isotropic, the transformer learns to implement one step of a \emph{preconditioned} gradient descent as we shall detail in \autoref{lem:express}. Here the ``preconditioning matrix''  given in \eqref{eq:single_layer} has interesting properties:
\begin{list}{$\bullet$}{\leftmargin=1.5em}
\setlength{\itemsep}{1pt}
\item When the number of samples $n$ is large, the first $d\times d$ submatrix of $Q_0$ approximates $\Sigma^{-1}$, the inverse of the data covariance matrix, which is also close to the Gram matrix formed from $\tx{1},\ldots,\tx{n}$. Hence the preconditioning can lead to considerably faster convergence rate when $\Sigma$ is ill-conditioned.
\item Moreover, $\frac{1}{n} \sum_k \lambda_k$  in \eqref{eq:single_layer}  acts as a regularizer. It becomes more significant when $n$ is small and variance of the $\tx{i}$'s is high. Such an adjustment resembles structural risk minimization \citep{vapnik1999nature} where the regularization strength is adapted to the sample size.   
\end{list}








\section{Multi-layer transformers with sparse parameters}
\label{s:k_layer_Q}

\autoref{thm:main_single} proves a single layer of linear attention can implement a single step of preconditioned gradient descent. Inspired by this result, we investigate the algorithmic power of the linear transformer architecture. We show that the model can implement various optimization methods even under sparsity constraints. In particular, we impose the following restrictions on the parameters:
\begin{align}\label{eq:sparse_attention}
P_i = \begin{bmatrix}
0_{d\times d} & 0 \\ 
0 & 1 
\end{bmatrix}, \quad Q_i = -  \begin{bmatrix}
A_i & 0 \\ 
0 & 0
\end{bmatrix} \quad \text{where $A_i \in \R^{d\times d}$.}
\end{align}
The next lemma proves that a forward-pass of a $L$-layer transformer, with the parameter configuration \eqref{eq:sparse_attention} is the same as taking $L$ steps of gradient descent, preconditioned by $A_\ell$.
 
  



\begin{lemma}[\emphh{Forward pass as a preconditioned gradient descent}] 
\label{lem:express}
Consider the $L$-layer linear transfomer parameterized by $A_0,\dots,A_{L-1}$ as in \eqref{eq:sparse_attention}.  
Let $\ty{n+1}_\ell$ be the $(d+1,n+1)$-th entry of the $\ell$-th layer output, i.e., $\ty{n+1}_\ell = [Z_{\ell}]_{(d+1),(n+1)}$ for $\ell=1,\dots, L$.
Then, it holds that $\ty{n+1}_\ell = - \langle\tx{n+1}, \wgd_\ell \rangle$ where $\{\wgd_\ell\}$ is defined as $\wgd_0=0$ and as follows for $\ell=1,\dots, L-1$:
\begin{align} \label{def:wgd}
\wgd_{\ell+1} = \wgd_{\ell} - A_\ell \nabla R_{\wstar}\lrp{\wgd_{\ell}}\quad \text{where} \quad R_{\wstar}(w) \coloneqq \frac{1}{2n}\sum_{i=1}^{n}(w^\top x_{i}- {\wstar}^\top x_{i})^2.
\end{align} 
\end{lemma}

See \autoref{pf:express} for a proof. The iterative scheme \eqref{def:wgd} includes various optimization methods including  gradient descent with $A_\ell = \gamma_\ell I_d$, and (adaptive) preconditioned gradient descent, where the preconditioner $A_\ell$ depends on the time step. In the upcoming sections, we characterize how the optimal $\{A_\ell\}$ are linked to the input distribution.


\subsection{Warm-up: optimal two-layer transformer with symmetric weights}
\label{s:two_layer}

 For the rest of this section, we will study the optimal parameters for the in-context loss under the constraint of Eq.~\eqref{eq:sparse_attention}. Later in \autoref{s:k_layer_PQ}, we analyze the optimal model for a more general parameters.
For a two-layer transformer, the next Theorem proves the optimal in-context loss obtains the simple gradient descent with adaptive coordinate-wise stepsizes. 


\begin{theorem}[Global optimality for the two-layer (symmetric) transformer]
\label{t:two_layer}
Consider the optimization of in-context loss for a two-layer transformer with the parameter configuration in Eq.~\eqref{eq:sparse_attention}, and additionally assume that $A_1,A_2$ are symmetric matrices. 
More formally, consider
\begin{align}
\min_{A_1,A_2 \text{ are symmetric}} f  \left\{  P_\ell = \begin{bmatrix}
0_{d\times d} & 0 \\ 
0 & 1 
\end{bmatrix},~Q_\ell = \begin{bmatrix}
-A_\ell & 0 \\ 
0 & 0
\end{bmatrix}\right\}_{\ell=1,2}\,.
\end{align}
Assume $\tx{i} \stackrel{\text{i.i.d.}}{\sim}N(0,I_d)$  and $\wstar \sim N(0,I_d)$; then, there are diagonal matrices $A_1$ and $A_2$ that are a global minimizer of $f$.
\end{theorem} 
Combining the above result with \autoref{lem:express} concludes that the two iterations of gradient descent with \emph{coordinate-wise adaptive stepsizes} achieve the minimal in-context loss for isotropic Gaussian inputs. Gradient descent with adaptive stepsizes such as Adagrad \citep{duchi2011adaptive} are widely used in machine learning. While Adagrad adjusts its stepsize based on the individual problem instance, the algorithm learned adjusts its stepsize to the underlying data distribution. 



\subsection{Multi-layer transformers}
\label{s:L_layer_P_0}
We now turn to the setting of general $L$-layer transformers, for any positive integer $L$. The next theorem proves that certain critical points of the in-context loss effectively implement a specific preconditioned gradient algorithm, where the preconditioning matrix is the inverse covariance of the input distribution. Before stating this result, let us first consider a motivating scenario in which the data-covariance matrix is non-identity:

\emphh{Linear regression with distorted view of the data:} Suppose that $\overline{w}_\star \sim \mathcal{N}(0,I)$ and the \emph{latent} covariates are $\overline{x}^{(1)},\dots,\overline{x}^{(n+1)}$, drawn i.i.d from $\N(0,I)$. We are given $\ty{1},\dots,\ty{n}$, with $\ty{i} = \lin{\overline{x}^{(i)}, \overline{w}_\star}$. However, we \emph{do not observe} the latent covariates $\overline{x}^{(i)}$. Instead, we observe the \emph{distorted} covariates $\tx{i} = W \overline{x}^{(i)}$, where $W\in \R^{d\times d}$ is a distortion matrix. Thus the prompt consists of $(\tx{1},\ty{1}),\dots,(\tx{n},\ty{n})$, as well as $\tx{n+1}$. The goal is still to predict $\ty{n+1}$. Note that this setting is quite common in practice, when covariates are often represented in an arbitrary basis. 

Assume that $\Sigma := W W^\top \succ 0$. We verify from our definitions that for $\wstar := \Sigma^{-1/2} \overline{w}_\star$, $\ty{i} = \lin{\tx{i},\wstar}$. Furthermore, $\tx{i} \sim \N(0, \Sigma)$ and $\wstar \sim \N(0, \Sigma^{-1})$. From \autoref{lem:express}, the transformer with weight matrices $\lrbb{A_0,\dots,A_{L-1}}$ implements preconditioned gradient descent with respect to $R_{\wstar}(w) = \frac{1}{2n} (w - \wstar)^T X X^\top (w-\wstar)$, with $X = \lrb{\tx{1}, \dots, \tx{n}}$. Under this loss, the Hessian matrix $\nabla^2 R_{\wstar}(w) = \frac{1}{2n} X X^\top$ (at least in the case of large $n$). For any fixed prompt, Newton's method corresponds to $A_i \propto \lrp{X X^\top}^{-1}$, which makes the problem well-conditioned even if $\Sigma$ is very degenerate. As we will see in \autoref{t:L_layer_P_0} below, the choice of $A_i \propto \Sigma^{-1} = \E\lrb{X X^\top}^{-1}$ appears to be a \emph{stationary point} of the loss landscape, in expectation over prompts.


Before stating the theorem, we introduce the following simplified notation: let $A := \lrbb{A_i}_{i=0}^{L-1} \in \R^{L \times d \times d}$. We use $f(A)$ to denote the in-context loss of $f\left(\{P_i, Q_i\}^{L-1}_{i=0}\right)$ as defined in \eqref{def:ICL linear}, when $Q_i$ depends on $A_i$, and $P_i$ is a constant matrix, as described in \eqref{eq:sparse_attention}.


\begin{theorem}\label{t:L_layer_P_0}  
Assume that $\tx{i} \overset{iid}{\sim} \mathcal{N}(0,\Sigma)$ and $\wstar \sim \mathcal{N}(0,\Sigma^{-1})$, for $i=1,\dots, n$, and for some $\Sigma \succ 0$. Consider the optimization of in-context loss for a $k$-layer transformer with the the parameter configuration in Eq.~\eqref{eq:sparse_attention} given by:
\begin{align}
\min_{\lrbb{A_i}_{i=0}^{L-1}} f \lrp{A}.
\end{align}
Let $\S \subset \R^{L \times d \times d}$ be defined as follows: $A \in \S$ if and only if for all $i = 0,\dots,L-1$, there exists scalars $a_i\in \R$ such that $A_i = a_i \Sigma^{-1}$. Then
\begin{align*}
\inf_{(A,B) \in \S} \sum_{i=0}^{L-1} \lrn{\nabla_{A_i} f(A,B)}_F^2 = 0,
\numberthis \label{e:T:near-stationarity_P0}
\end{align*}
where $\nabla_{A_i} f$ denotes derivative wrt the Frobenius norm $\lrn{A_i}_F$.

\end{theorem}


As discussed in the motivation above, under the setting of $A_i = a_i \Sigma^{-1}$, the linear transformer implements an algorithm that is reminiscent of Newton's method (as well as a number of other adaptive algorithms such as the full-matrix variant of Adagrad); these can converge significantly faster than vanilla gradient descent when the problem is ill-conditioned. The proposed parameters $A_i$ in \autoref{t:L_layer_P_0} are also similar to $\bb_i$'s in \autoref{thm:main_single} when $n$ is large. However, in contrast to \autoref{thm:main_single}, there is no trade-off with statistical robustness; this is because $\wstar$ has covariance matrix $\Sigma^{-1}$ in the \autoref{t:L_layer_P_0}, while \autoref{thm:main_single} has isotropic $\wstar$.

Unlike our prior results, \autoref{t:L_layer_P_0} only guarantees that the set $\S$ of transformer prameters satisfying $\lrbb{A_i \propto \Sigma^{-1}}_{i=0}^{L-1}$ \emph{essentially}\footnote{A subtle issue is that the infimum may not be attained, so it is possible that $\S$ contains points with arbitrarily small gradient, but does not contain a point with exactly $0$ gradient.} contains critical points of the in-context loss. However, in the next section, we show experimentally that this choice of $A_i$'s does indeed seem to be recovered by training. 


We defer the proof of \autoref{t:L_layer_P_0} to \autoref{sec:pf:t:L_layer_P_0}. Due to the complexity of the transformer function, even verifying critical points can be challenging. We show that the in-context loss can be equivalently written as (roughly) a matrix polynomial involving the weights at each layer. By exploiting invariances in the underlying distribution of prompts, we construct a flow, contained entirely in $\S$, whose objective value decreases as fast as gradient flow. Since $f$ is lower bounded, we conclude that there must be points in $\S$ whos gradient is arbitrarily small.


\subsection{Experimental validations for \autoref{t:L_layer_P_0}}
\label{s:experiment_pnull}
We present here an empirical verification of our results in \autoref{t:L_layer_P_0}. We consider the ICL loss for linear regression. The dimension is $d=5$, and the number of training samples in the prompt is $n=20$. Both $\tx{i}\sim \mathcal{N}(0,\Sigma)$ and $\wstar \sim \mathcal{N}(0,\Sigma^{-1})$, where $\Sigma = U^T D U$, where $U$ is a uniformly random orthogonal matrix, and $D$ is a fixed diagonal matrix with entries $(1,1,0.25,0.0625,1)$. 

We optimizes $f$ for a three-layer linear transformer using ADAM, where the matrices $A_0,A_1,$ and $A_2$ are initialized by i.i.d. Gaussian matrices. Each gradient step is computed from a minibatch of size 20000, and we resample the minibatch every 100 steps. We clip the gradient of each matrix to 0.01. All plots are averaged over $5$ runs with different $U$ (i.e. $\Sigma$) sampled each time.


\autoref{fig:loss_pnull} plots the average loss. We observe that the training converges to an almost $0$ value, suggesting the convergence to global minimum. The parameters at convergence match the stationary point introduced in \autoref{t:L_layer_P_0}, and indeed appear to be globally optimal. 

To quantify the similarity between $A_0,A_1,A_2$ and $\Sigma^{-1}$ (up to scaling), we use the \emph{normalized Frobenius norm distance}: $\Dist(M,I) := \min_{\alpha}  \frac{\lrn{M - \alpha  \cdot I}}{\lrn{M}_F}$, (equivalent to choosing $\alpha := \frac{1}{d} \sum_{i=1}^d M[i,i]$). This is essentially the projection distance of $\nicefrac{M}{\lrn{M}}_F$ onto the space of scaled identity matrices. 

We plot $\Dist\lrp{A_i, I}$, averaged over $5$ runs, against iteration in Figures \ref{fig:A0_pnull_trend},\ref{fig:A1_pnull_trend},\ref{fig:A2_pnull_trend}. In each plot, the blue line represents $\Dist(\Sigma^{1/2} A_i \Sigma^{1/2},I)$, and we verify that the optimal parameters are converging to the critical point introduced in \autoref{t:L_layer_P_0}, which implements preconditioned gradient descent. The red line  represents $\Dist(A_i,I)$; it remains constant indicating that the trained transformer is not implementing plain gradient descent.  Figures~\ref{fig:A0_imshow_pnull}--\ref{fig:A2_imshow_pnull} visualize each $\Sigma^{1/2} A_i \Sigma^{1/2}$ matrix at the end of training to further validate that the learned parameter is as described in \autoref{t:L_layer_P_0}.

\iffalse
\begin{figure}[h]
\centering
\begin{subfigure}{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{random_init_20_pnull/Q0_err_20_pnull.pdf} % first figure
\caption{Error for $A_0$}
\label{fig:A0_pnull_trend}
\end{subfigure}\hfill
\begin{subfigure}{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{random_init_20_pnull/Q1_err_20_pnull.pdf} % second figure
\caption{Error for $A_1$}
\label{fig:A1_pnull_trend}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{random_init_20_pnull/Q2_err_20_pnull.pdf} % second figure
\caption{Error for $A_2$}
\label{fig:A2_pnull_trend}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{random_init_20_pnull/Loss_loss_20_pnull.pdf} % second figure
\caption{log(Loss)}
\label{fig:loss_pnull}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{random_init_20_pnull/Q0imshow.pdf}
\caption{Visualization of $A_0$} 
\label{fig:A0_imshow_pnull}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{random_init_20_pnull/Q1imshow.pdf}
\caption{Visualization of $A_1$} 
\label{fig:A1_imshow_pnull}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{random_init_20_pnull/Q2imshow.pdf}
\caption{Visualization of $A_2$} 
\label{fig:A2_imshow_pnull}
\end{subfigure} 
\end{figure}
\fi





\begin{figure}[h]
\centering
\begin{subfigure}{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_dist_to_id_adam_pnull_A0.pdf} % first figure
\caption{$\Dist(\Sigma^{1/2} A_0 \Sigma^{1/2},I)$}
\label{fig:A0_pnull_trend}
\end{subfigure}\hfill
\begin{subfigure}{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_dist_to_id_adam_pnull_A1.pdf}  % second figure
\caption{$\Dist(\Sigma^{1/2} A_1 \Sigma^{1/2},I)$}
\label{fig:A1_pnull_trend}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_dist_to_id_adam_pnull_A2.pdf}  % second figure
\caption{$\Dist(\Sigma^{1/2} A_2 \Sigma^{1/2},I)$}
\label{fig:A2_pnull_trend}
\end{subfigure}
\begin{subfigure}{0.24\textwidth}
\centering
\includegraphics[height=0.71\textwidth]{camera-ready-figs/rotation_demonstration_adam_pnull_loss_plot.pdf}  % second figure
\caption{log(Loss)}
\label{fig:loss_pnull}
\end{subfigure}
\caption{Plots for verifying convergence of general linear transformer, defined in \autoref{t:L_layer_P_0}. Figure (d) shows convergence of loss to $0$. Figures (a),(b),(c) illustrate convergence of $A_i$'s to identity. More specifically,  the blue line represents $\Dist(\Sigma^{1/2} A_i \Sigma^{1/2},I)$,  which measures the convergence to the critical point introduced in \autoref{t:L_layer_P_0} (corresponding to $\Sigma^{-1}$-preconditioned gradient descent). The red line  represents $\Dist(A_i,I)$; it remains constant indicating that the trained transformer is not implementing plain gradient descent. }

\label{fig:pnull_trend}
\end{figure}
\begin{figure}
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_pnull_A0.pdf}
\caption{Visualization of $\Sigma^{1/2} A_0 \Sigma^{1/2}$} 
\label{fig:A0_imshow_pnull}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_pnull_A1.pdf}
\caption{Visualization of $\Sigma^{1/2} A_1 \Sigma^{1/2}$} 
\label{fig:A1_imshow_pnull}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_pnull_A2.pdf}
\caption{Visualization of $\Sigma^{1/2} A_2 \Sigma^{1/2}$} 
\label{fig:A2_imshow_pnull}
\end{subfigure} 
\caption{Visualization of learned weights  for the setting of \autoref{t:L_layer_P_0}. We visualize each $\Sigma^{1/2} A_i \Sigma^{1/2}$ matrix at the end of training.
Note that the optimized weights match the stationary point discussed in \autoref{t:L_layer_P_0}.}
\label{fig:pnull_imshow}
\end{figure}




\section{Multi-layer transformers beyond standard optimization methods}
\label{s:k_layer_PQ}
In this section, we study the more general setting of
\begin{align}\label{eq:full_attention}
P_i = \begin{bmatrix}
B_i & 0 \\ 
0 & 1
\end{bmatrix}, \quad Q_i = \begin{bmatrix}
A_i & 0 \\ 
0 & 0
\end{bmatrix}   \quad \text{where $A_i, B_i \in \R^{d\times d}$.}
\end{align}
Note that $A_i, B_i$ are not constrained to be symmetric.
Similar to \autoref{s:k_layer_Q}, we introduce the following simplified notation: let $A := \lrbb{A_i}_{i=0}^{L-1} \in \R^{L \times d \times d}$ and $B := \lrbb{B_i}_{i=0}^{L-1} \in \R^{L \times d \times d}$. We use $f(A,B)$ to denote the in-context loss of $f\left(\{P_i, Q_i\}^{L-1}_{i=0}\right)$ as defined in \eqref{def:ICL linear}, when $P_i$ and $Q_i$ depend on $B_i$ and $A_i$ as described in \eqref{eq:full_attention}.



With this relaxed parameter configuration, it turns out transformers can learn algorithms beyond the conventional preconditioned gradient descent. The next theorem asserts the possibility of learning a novel preconditioned gradient method. Let $L$ be a fixed but arbitrary number of layers. 

\begin{theorem}\label{t:L_layer_P_identity}
Let $\Sigma$ denote any PSD matrix. Assume that $\tx{i} \overset{iid}{\sim} \mathcal{N}(0,\Sigma)$ and $\wstar \sim \mathcal{N}(0,\Sigma^{-1})$, for $i=1, \dots, n$, and for some $\Sigma \succ 0$. Consider the optimization of in-context loss for a $L$-layer linear transformer with the the parameter configuration in Eq.~\eqref{eq:full_attention} given by:
\begin{align}
\min_{\lrbb{A_i,B_i}_{i=0}^{L-1}} f \lrp{A,B}.
\end{align}

Let $\S \subset \R^{2\times L \times d \times d}$ be defined as follows: $(A,B) \in \S$ if and only if for all $i\in \lrbb{0,\dots,k}$, there exists scalars $a_i,b_i \in \R$ such that $A_i = a_i \Sigma^{-1}$ and $B_i = b_i I$. Then
\begin{align*}
\inf_{(A,B) \in \S} \sum_{i=0}^{L-1} \lrn{\nabla_{A_i} f(A,B)}_F^2 + \lrn{\nabla_{B_i} f(A,B)}_F^2 = 0,
\numberthis \label{e:T:near-stationarity}
\end{align*}
where $\nabla_{A_i} f$ denotes derivative wrt the Frobenius norm $\lrn{A_i}_F$.
\end{theorem}
In words, parameter matrices in $\S$ implement the following  algorithm: $\lrbb{A_i = a_i \Sigma^{-1}}_{i=0}^{L-1}$ plays the role of a distribution-dependent preconditioner for the gradient steps. At the same time, $B_i = b_i I$ transforms the covariates themselves to make the Gram matrix have better condition number with each iteration. When the $\Sigma = I$, the algorithm implemented by $A_i \propto I, b_i \propto I$ is exactly the GD++ algorithm proposed in \citep{von2022transformers} (up to stepsize).

The result in \eqref{e:T:near-stationarity} says that the set $\S$ \emph{essentially}\footnote{Once again, similar to the case of \autoref{t:L_layer_P_0}, the infimum may not be attained, so it is possible that $\S$ contains points with arbitrarily small gradient, but does not contain a point with exactly $0$ gradient.} contains critical points of the in-context loss $f(A,B)$. In the next section, we provide empirical evidence that the trained transformer parameters do in fact converge to a point in $\S$.

%\eqref{e:L_layer_P_identity} even for non-isotropic data.

\subsection{Experimental validations for \autoref{t:L_layer_P_identity}}
\label{s:experiment_PQ}
The experimental setup is similar to \autoref{s:experiment_pnull}: we consider ICL for linear regression with $n=10,d=5$, with $\tx{i}\sim \mathcal{N}(0,\Sigma)$ and $\wstar \sim \mathcal{N}(0,\Sigma^{-1})$, where $\Sigma = U^T D U$, where $U$ is a uniformly random orthogonal matrix, and $D$ is a fixed diagonal matrix with entries $(1,1,0.25,0.0625,1)$. We train a three-layer linear transformer, under the constraints in \eqref{eq:full_attention} which is less restrictive than \eqref{eq:sparse_attention} in \autoref{s:experiment_pnull}. We train the matrices $A_0,A_1,A_2, B_0, B_1$~\footnote{Note that the objective function does not depend on $B_2$.} using ADAM with the same setup as in Section \autoref{s:experiment_pnull}. We repeat this experiment 5 times with different random seeds, each time we sample a different $U$ (i.e. $\Sigma$).

In \autoref{fig:log_loss_PQ}, we plot the in-context loss through the iterations of L-BFGS; the loss appears to be converging to 0, suggesting that parameters are converging to the global minimum. 

\begin{figure}[htbp]
\centering 
\centering
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_dist_to_id_adam_B0.pdf}
\caption{$\Dist(B_0,I)$} 
\label{fig:B0_PQ_trend}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_dist_to_id_adam_B1.pdf}
\caption{$\Dist(B_1,I)$} 
\label{fig:B1_PQ_trend}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[height=0.71\textwidth]{camera-ready-figs/rotation_demonstration_adam_loss_plot.pdf}
\caption{log(Loss)} 
\label{fig:log_loss_PQ}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_dist_to_id_adam_A0.pdf}
\caption{Distances for $A_0$ } 
\label{fig:A0_PQ_trend}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_dist_to_id_adam_A1.pdf}
\caption{Distances for $A_1$ } 
\label{fig:A1_PQ_trend}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_dist_to_id_adam_A2.pdf}
\caption{Distances for $A_2$ } 
\label{fig:A2_PQ_trend}
\end{subfigure}  
\caption{Plots for verifying convergence of general linear transformer, defined in \autoref{t:L_layer_P_identity}. Figure (c) shows convergence of loss to $0$. Figures (a),(b) illustrate convergence of $B_0,B_1$ to identity. Figures (d),(e),(f) illustrate convergence of $A_i$'s to $\Sigma^{-1}$.}
\end{figure}

We next verify that the parameters at convergence are consistent with \autoref{t:L_layer_P_identity}. We will once again use $\Dist(M,I)$ to measure the distance from $M$ to the identity matrix, up to scaling (see \autoref{s:experiment_pnull} for definition of $Dist$). Figures \ref{fig:B0_PQ_trend} and \ref{fig:B1_PQ_trend} show that $B_0$ and $B_1$ are close to identity, as $\Dist(B_i, I)$ appears to be decreasing to 0. Figures \ref{fig:A0_PQ_trend}, \ref{fig:A1_PQ_trend} and \ref{fig:A2_PQ_trend} plot $\Dist(A_i, I)$ (red line) and $\Dist(\Sigma^{1/2} A_i \Sigma^{1/2}, I)$ (blue line); the results here suggest that $A_i$ is converging to $\Sigma^{-1}$, up to scaling. In Figures \ref{fig:B0_PQ_trend} and \ref{fig:B1_PQ_trend}, we observe that $B_0$ and $B_1$ also converge to the identity matrix (\emph{without} left and right multiplication by $\Sigma^{1/2}$), consistent with \autoref{t:L_layer_P_identity}. 
% Nonetheless, it is somewhat surprising that $A_i$'s adapt to the data covariance, but $B_i$'s remain identity.

%Finally, in Figures \ref{fig:A0_PQ},\ref{fig:A1_PQ},\ref{fig:A2_PQ}, we provide a direct visualization of of each $\Sigma^{1/2} A_i \Sigma^{1/2}$ matrix at the end of training. One can visually verify that these matrices are indeed close to identity, up to scaling.

We visualize each of $B_0,B_1$ in \autoref{fig:B0_B1} and $A_0,A_1,A_2$ in Figure~\ref{fig:A0_PQ_app}-\ref{fig:A2_PQ_app} at the end of training. We highlight two noteworthy observations:
\begin{enumerate}
\item Let $X_k\in \R^{d\times n}$ denote the first $d$ rows of $Z_k$, which are the output at layer $k-1$ defined in \eqref{eq:recursion}. Then the update to $X_k$ is $X_{k+1} = X_k + B_k X_k \aa X_k^T A_k X_k \approx X_{k+1} = X_k \lrp{I - |a_kb_k| \aa X_k^T X_k}$, where $\aa$ is a mask defined in \eqref{eq:softmax}. As noted by \cite{von2022transformers}, this may be motivated by curvature correction. 
\item As seen in Figures \ref{fig:A0_PQ_app}-\ref{fig:A2_PQ_app} in the Appendix, $\| A_0\| \leq \| A_1\| \leq \| A_2\|$ that implies the transformer implements gradient descent with a small stepsize at the beginning and a large stepsize at the end. This makes intuitive sense as $X_2$ is better-conditioned compared to $X_1$, due to the choice of $B_0,B_1$. This can be contrasted with the plots in Figures \eqref{fig:A0_imshow_pnull}-\eqref{fig:A2_imshow_pnull}, where similar trends are not as pronounced because $B_i$'s are constrained to be $0$.
\end{enumerate}
 

 

\begin{figure}
\centering  
\includegraphics[width=0.35\textwidth]{camera-ready-figs/rotation_demonstration_adam_B0.pdf}
\includegraphics[width=0.35\textwidth]{camera-ready-figs/rotation_demonstration_adam_B1.pdf} 
\vspace{-10pt}
\caption{Visualization of optimized weight matrices $B_0$ (left) and $B_1$ (right). One can see that the weight pattern matches the stationary point analyzed in \autoref{t:L_layer_P_identity}. Matrices $A_0$, $A_1$ and $A_2$ are similar to \autoref{fig:pnull_imshow}, and are visualized in \autoref{fig:appendix_full} in \autoref{s:additional_plots}.}
\label{fig:B0_B1}
\end{figure}




\section{Discussion} 

We take a first step toward proving that transformers can learn algorithms when trained over a set of random problem instances. Specifically, we investigate the possibility of learning gradient based methods when training on the in-context loss for linear regression. 
For a single layer transformer, we prove that the global minimum corresponds to a single iteration of preconditioned gradient descent.  
For multiple layers, we show that certain parameters that correspond to the critical points of the in-context loss can be interpreted as a broad family of adaptive gradient-based algorithms.

 
We discuss below two interesting future directions.

\textbf{Beyond linear attention.} The standard transformer architecture comes with nonlinear activations in attention. Hence, the natural question here is to ask the effect of nonlinear activations for our main results. Empirically, \cite{von2022transformers} have observed that for linear regression task, softmax activations  generally degrade the prediction performance, and in particular, softmax transformers typically need more attention heads to match their performance with that of linear transformers. 

As a first step analysis, we consider the nonlinear attention defined as 
\begin{align}
    \atth_{P,Q}(Z) \coloneqq  P ZM \  \sigma (Z^\top Q Z)\quad \text{where }\sigma :\R\to \R \text{ is applied entry-wise.}
\end{align}  
The following result is an analog of \autoref{thm:main_single} for single-layer nonlinear attention. It characterizes a global minimizer for this setting with ReLU activation.
Here, our choice of ReLU activation was motivated by \cite{wortsman2023replacing} who observed that ReLU attention matches the performance of softmax attention for vision transformers. 

\begin{theorem} \label{thm:nonlinear} 
Consider the single layer nonlinear attention setting with  $\sigma = \mathrm{ReLU}$.
Assume that vector $\tx{i}$ is sampled from $\mathcal{N}(0, I_{d})$. 
Moreover, assume that  $\wstar$ is sampled  from $\mathcal{N}(0, I_d)$.  
Consider the parameter configuration $P_0,Q_0$  where we additionally assume that the last row of $Q_0$ is zero. 
Then, the following parameters form a global minimizer of the corresponding in-context loss:
    \begin{align}  
P_0 = \begin{bmatrix}
0_{d\times d} & 0 \\ 
0 & 1 
\end{bmatrix} ,\quad Q_0 = -\frac{1}{\frac{1}{2}\frac{n-1}{n}+(d+2)\frac{1}{n}} \cdot \begin{bmatrix}
 I_d & 0\\
0  & 0
\end{bmatrix} .
\end{align} 
\end{theorem}
The proof of \autoref{thm:nonlinear}  involves an instructive argument and leverages tools from \citep{erdogdu2016scaled}; we defer it to \autoref{pf:nonlinear}. Thus, for isotropic Gaussian data, the structure of global minimum under ReLU attention is similar to the global minimum with linear attention, established in \autoref{thm:main_single} (specifically the minimizer for the isotropic date given in \eqref{minimum:linear}). 

 

\textbf{Refined landscape analysis for multilayer transformer.} \autoref{t:L_layer_P_identity}   proves that a stationary point of the in-context loss corresponds to  implementing a preconditioned gradient method. However, we do not prove that all critical points of the non-convex objective lead to similar optimization methods. In fact, in \autoref{l:bar} in   \autoref{a:foo}, we prove that the in-context loss can have multiple critical points. It will be interesting to analyze the set of all critical points and try to understand their algorithmic interpretations, as well as quantify their (sub)optimality.

\iffalse
\begin{list}{$\bullet$}{\leftmargin=1.3em}
\item \emphh{Refined landscape analysis for multilayers.} \autoref{t:L_layer_P_identity}   proves that a multi-layer transformer implementing a gradient method is a critical point of the training objective. However, we do not prove that all critical points of the non-convex objective lead to similar optimization methods. In fact, in \autoref{l:bar} in Appendix \ref{a:foo}, we prove that the in-context loss can have multiple critical points. It will be interesting to analyze the set of all critical points and try to understand their algorithmic interpretations, as well as quantify their (sub)optimality.

% \item \emphh{Convergence to the global optimum.}  Although \autoref{thm:main_single} establishes a closed form for the global minimizer of the training objective, gradient-based optimization used for training may nonetheless converge to a different stationary point. Analyzing the convergence of gradient descent on $f$ will provide insights into how training of transformer architecture works.



\item \emphh{Attention with softmax.}
We omit the softmax in attention to simplify our analysis. For the specific example of learning linear functions,  \cite{von2022transformers} experimentally show that training a two-head attention with softmax can implement gradient descent. It remains open whether training indeed still learns gradient descent  with softmax in attention.

%\item \emphh{Applications for domain adaptation.}
%The algorithmic view can potentially explain various properties of ICL. For example, one could explain the robustness to distribution shift if the minimizer of in-context loss was implementing gradient descent -- gradient descent is inherently robust against the shift in the prompt distribution. This motivates future studies on  the algorithmic power of  trained transformers.  
\end{list}
\fi
%\paragraph{Beyond linear self-attention.}
%Following (\cite{wortsman2023replacing,zhao2023transformers}), we consider a specific attention obtained by replacing softmax with linear unit (ReLU). In fact, \cite{wortsman2023replacing} observe that ReLU attention matches the performance of softmax attention for vision transformers. 
%Let $\atth_{P,Q}(Z) \coloneqq  P ZM \  \relu (
%Z^\top Q Z)$, 
%where $\relu$ is the ReLU activation function.
%The following result characterizes a global minimizer for this setting, and is the analog of \autoref{thm:main_single} for ReLU attention.

%See \autoref{pf:nonlinear} for the proof. Thus, for isotropic Gaussian data, the structure of global minimum under ReLU attention is similar to the global minimum with linear attention, established in \autoref{thm:main_single}. This result is a step to bridge the gap between our theoretical model and practical models used in practice.

\begin{ack}
We thank Ekin Akyürek,  Johannes von Oswald, Alex Gu and Joshua Robinson for helpful discussions.
Kwangjun Ahn was supported by the ONR grant (N00014-20-1-2394) and MIT-IBM Watson as
well as a Vannevar Bush fellowship from Office of the Secretary of Defense. Kwangjun Ahn also
acknowledges support from the Kwanjeong Educational Foundation. Xiang Cheng acknowledges
support from NSF CCF-2112665 (TILOS AI Research Institute). Hadi Daneshmand acknowledges support from NSF TRIPODS program (award DMS-2022448). Suvrit Sra acknowledges support
from an NSF CAREER grant (1846088), and NSF CCF-2112665 (TILOS AI Research Institute).


\end{ack}
\bibliographystyle{plainnat}
\bibliography{ref}
\appendix
%\input{appendix}



\newpage


\appendix
\renewcommand{\appendixpagename}{\centering \LARGE Appendix}
\appendixpage

\startcontents[section]
\printcontents[section]{l}{1}{\setcounter{tocdepth}{2}}



\newcommand{\proofstep}[2]{{\large \textbf{Step #1:  #2}}\\}

\section{Proofs for the single layer case}
\label{sec:single_proofs}

In this section, we prove our characterization of global minima for the single layer case (\autoref{thm:main_single}).
We begin by simplifying the loss into a more concrete form.
Throughout the proof, we will  write $P,Q$ instead of $P_0,Q_0$ for brevity.


\subsection{Rewriting the loss function}
\label{sec:rewrite loss}



Recall the in-context loss \eqref{def:ICL linear} for the single layer case $f(P,Q)$  is defined as:
\begin{align} 
f\left(P,Q\right)  
&=\E_{Z_0,\wstar} \left[ \left(Z_{0} +\frac{1}{n} \att_{P,Q}(Z_0) \right)_{(d+1),(n+1)} + \wstar^\top \tx{n+1}\right]^2
\end{align} 
From the definition of attention given in \eqref{eq:linear},
one can further spell out the expression $Z_0 +\frac{1}{n} \att_{P,Q}(Z_0)$ using the notation $Z_0 = [\tz{1} \ \tz{2} \ \cdots \ \tz{n+1}]$ as follows:
\begin{align}
&[\tz{1} \ \cdots \ \tz{n+1}] + \frac{1}{n}  P [\tz{1} \  \cdots \ \tz{n+1}]\aa \left([\tz{1} \ \cdots \ \tz{n+1}]^\top Q [\tz{1} \ \cdots \ 
\tz{n+1}] \right)\\
\quad\quad &= [\tz{1} \ \cdots \ \tz{n+1}] + \frac{1}{n}  P  \left(\sum_{i=1}^n \tz{i} {\tz{i}}^\top\right) Q  [\tz{1} \ \cdots \ 
\tz{n+1}]\,.
\end{align}
Thus, the last column of the above matrix can be expressed as
\begin{align} 
\begin{bmatrix}
\tx{n+1}\\ 0
\end{bmatrix}  + \frac{1}{n}  P\left(\sum_{i=1}^n \tz{i} {\tz{i}}^\top \right) Q  
\begin{bmatrix}
\tx{n+1}\\ 0
\end{bmatrix}\,, 
\end{align}
where note that the summation is for $i=1,2,\dots, n$ due to the mask matrix $\aa$.
Therefore, letting $\cc^\top $ be the last row of $P$, and $\bb\in \R^{d+1,d}$ be the first $d$ columns of $Q$ (as we did in \eqref{exp:simplify}), then  $\left[Z_{0} +\frac{1}{n} \att_{P,Q}(Z_0) \right]_{(d+1),(n+1)}$ can be written as
\begin{align} \label{exp:attention_single}
\frac{1}{n}  \cc^\top \left( \sum_{i=1}^n \tz{i} {\tz{i}}^\top \right) \bb 
\begin{bmatrix}
\tx{n+1}\\ 0
\end{bmatrix}\,,
\end{align}
in other words, $f(P,Q)$ only depends on the parameter $\cc$ and $\bb$.
Henceforth, we will write $f(P,Q)$ as  $f(\cc,\bb)$. Let us summarize our conclusion so far since it's crucial for the analysis to follow.
\begin{mdframed}[linecolor=black!40,linewidth=0.5pt,innertopmargin=3pt,innerleftmargin=3pt,innerrightmargin=3pt,innerbottommargin=3pt]
\textbf{Conclusion so far:} A careful inspection reveals that the in-context loss  only depends on the last row of $P$ and the first $d$ columns of $Q$. 
Thus, consider the following parametrization
\begin{align} \label{exp:simplify}
P = \begin{bmatrix}
0\\
\cc^\top
\end{bmatrix}\quad \text{and} \quad Q=\begin{bmatrix}
\bb & 0 
\end{bmatrix}\,, \text{ where $\cc\in\R^{d+1}$ and $\bb\in \R^{(d+1)\times d}$}.
\end{align}
Now with this parametrization, the in-context loss can be written as $f(\cc,\bb) \coloneqq  f([0 \,\,\ \cc]^\top, [\bb \,\, 0])$.  
\end{mdframed}

Now, let us spell out $f(\cc,\bb)$ based on \eqref{exp:attention_single} as follows:
\begin{align}
f(\cc,\bb) &= \E_{Z_0,\wstar} \left[\cc^\top \underbrace{\frac{1}{n}  \sum_i \tz{i} {\tz{i}}^\top}_{=:\MM} \bb \tx{n+1} + \wstar^\top \tx{n+1} \right]^2 \\
& =: \E_{Z_0,\wstar} \left[\cc^\top \MM \bb \tx{n+1} + \wstar^\top \tx{n+1} \right]^2 = \E_{Z_0,\wstar} \left[(\cc^\top \MM \bb   + \wstar^\top) \tx{n+1} \right]^2 \,, \label{exp:loss_single}
\end{align}
where we used the notation $\MM \coloneqq \frac{1}{n}  \sum_i \tz{i} {\tz{i}}^\top$ to simplify.
We now analyze the global minima of this loss function.
To illustrate the proof idea clearly, we begin with the proof for the simpler case of isotropic data.

\subsection{Warm-up: proof for the isotropic data} \label{e:single_layer_non_isotropic_proof}


As a warm-up, we first prove the result for the special case where  $\tx{i}$ is sampled from $\mathcal{N}(0, I_d)$.  



\underline{\emphh{1. Decomposing the loss function into components.}}
Writing $\bb = [\bbb_1\,\, \bbb_1 \,\, \cdots \,\, \bbb_d ]$, and use the fact that $\E[\tx{n+1}[j]\tx{n+1}[j']] =0$ for $j\neq j'$ and $\E[\tx{n+1}[j]^2] =1$, we get
\begin{align}
f\left(\cc, \bb\right) = \sum_{j=1}^d \E_{Z_0,\wstar}\left[ \cc^\top \MM \,\bbb_j + \wstar[j] \right]^2 \E[\tx{n+1}[j]^2]  = \sum_{j=1}^d \E_{Z_0,\wstar}\left[ \cc^\top \MM \,\bbb_j  + \wstar[j] \right]^2 \,. 
\end{align}
The key idea is to characterize the global minima of each component in the summation separately.
Another key idea is to reparametrize the cost function  given the following identity:
\begin{align}
\E_{Z_0,\wstar}\left[ \cc^\top \MM \,\bbb_j  + \wstar[j] \right]^2 = \E_{Z_0,\wstar}\left[ \tr (\MM \,\bbb_j \cc^\top)  + \wstar[j] \right]^2= \E_{Z_0,\wstar}\left[\inp{\MM}{\cc \bbb_j^\top}  + \wstar[j] \right]^2\,,
\end{align}
where we use the notation $\inp{X}{Y} \coloneqq \tr(XY^\top)$ for two matrices $X$ and $Y$ here and below.
Given the above identity, we define each component in the summation as follows.
\begin{align}
\boxed{f_j(X)\coloneqq   \E_{Z_0,\wstar}\Big[\inp{\MM}{X}  + \wstar[j] \Big]^2\quad \text{for }  X\in \R^{(d+1)\times (d+1)}\,.}
\end{align} 



\underline{\emphh{2. Characterizing global minima of each component.}} 
To characterize the global minima of each objective, we prove the following result.

% \begin{mdframed}[linecolor=black!40,linewidth=0.5pt,innertopmargin=3pt,innerleftmargin=3pt,innerrightmargin=3pt,innerbottommargin=3pt]
\begin{lemma}[\textbf{Global minima of each component}] \label{lem:component opt}
Suppose that $\tx{i}$ is sampled from $\mathcal{N}(0, I_d)$ and  $\wstar$ is sampled  from $\mathcal{N}(0, I_d)$.      Consider the following objective (here, $\inp{X}{Y} \coloneqq \tr(XY^\top)$ for two matrices $X$ and $Y$)
\begin{align}
f_j(X) =  \E_{Z_0,\wstar}\left[ \inp{\MM}{X}  + \wstar[j] \right]^2\,.
\end{align} 
Then a global minimum is given as 
\begin{align} \label{exp:opt_single}
X_j = - \frac{1}{\left( \frac{n-1}{n} +  (d+2) \frac{1}{n} \right)}E_{d+1,j}\,,
\end{align} where $E_{i_1,i_2}$ is the matrix whose $(i_1,i_2)$-th entry is $1$,  and the other entries are zero.
\end{lemma}
% \end{mdframed}
\begin{proof}[{\bf Proof of \autoref{lem:component opt}}] 
Note first that $f_j$ is convex in $X$.
Hence, in order to show that matrix $X_j$ is  the global optimum of $f_j$, it suffices to show that the gradient vanishes at that point, in other words, $\nabla f_j(X_j)  = 0$. 
To verify this,  let us compute the gradient of $f_j$: for a matrix $X$, 
\begin{align}
\nabla f_j(X)  = 2\E \left[ \inp{\MM}{X} \MM \right] + 2 \E \left[\wstar[j] \ 
\MM \right]\,,
\end{align}
where we recall that $\MM$ is defined as  
\begin{align}
\MM = \frac{1}{n}\sum_i\begin{bmatrix}
\tx{i} {\tx{i}}^\top & \ty{i} \tx{i}\\
\ty{i} {\tx{i}}^\top & {\ty{i}}^2
\end{bmatrix}.
\end{align}
To verify that the gradient is equal to zero, let us first compute $\E \left[ \wstar[j] \,
\MM \right]$.  
For each $i=1,\dots,n$, note that $\E[\wstar[j] \ \tx{i} {\tx{i}}^\top] =O$ because $\E[\wstar]=0$.
Moreover, $\E[\wstar[j] \ (\ty{i})^2] =0$ because $\wstar$ is symmetric, i.e., $\wstar \overset{d}{=} -\wstar$, and $\ty{i} =\langle{\wstar},{\tx{i}} \rangle$. 
Lastly, for $k=1,2,\dots, d$, we have 
\begin{align} \label{exp:wj}
\E[\wstar[j]  \ {\ty{i}} \ {\tx{i}}[k]] =\E[\wstar[j] \ \langle{\wstar},{\tx{i}} \rangle \ {\tx{i}}[k]] =   \E\left[ \wstar[j]^2 \  {\tx{i}}[j] \  {\tx{i}}[k] \right]  = \mathbbm{1}_{[j=k]}     
\end{align}
because $\E[\wstar[i] \ \wstar[j]]=0$ for $i\neq j$. Combining the above calculations, it follows that
\begin{align} \label{exp:second}
\boxed{ \E \left[ \wstar[j] \ 
\MM \right]  =  E_{d+1,j} + E_{j,d+1}\,.} 
\end{align}

In order to compute $\E \left[ \inp{\MM}{X} \MM \right]$, let us compute $\E \left[ \inp{\MM}{E_{i,i'}} \MM \right]$ for $i, i' = 1,\dots, d+1$.
Without loss of generality, $i\geq i'$.
First of all 
We now compute   compute $\E \left[ \inp{\MM}{E_{d+1,j}} \MM \right]$.
Note first that 
\begin{align}
\inp{\MM}{E_{d+1,j}} = \sum_i\langle{\wstar}, \tx{i} \rangle \ {\tx{i}}[j]\,.
\end{align}
Hence, it holds that
\begin{align}
\E\left[  \inp{\MM}{E_{d+1,j}} \left(\sum_i {\tx{i}}{\tx{i}}^\top\right)\right]
= \E\left[\left(\sum_i\langle{\wstar}, \tx{i} \rangle  \ {\tx{i}}[j]\right) \left(\sum_i {\tx{i}}{\tx{i}}^\top\right)\right] =O\,.    
\end{align}
because $\E[\wstar]=0$.
Next, we have 
\begin{align}
\E \left[  \inp{\MM}{E_{d+1,j}}\left(\sum_i{\ty{i}}^2\right) \right]    =  \E\left[ \left(\sum_i \langle{\wstar}, \tx{i} \rangle  \ {\tx{i}}[j]\right) \left(\sum_i{\ty{i}}^2\right)\right] =0
\end{align} because $\wstar \overset{d}{=} -\wstar$.
Lastly, we compute 
\begin{align}
\E\left[  \inp{\MM}{E_{d+1,j}} \left(\sum_i {\ty{i}}{\tx{i}}^\top\right)\right]\,.
\end{align}
To that end, note that for $j\neq j'$,
\begin{align}
\E\left[\langle{\wstar}, \tx{i}  \rangle   \ {\tx{i}}[j] \ \langle{\wstar},{\tx{i'}}\rangle \ \tx{i'} [j'] \right] = \begin{cases}
\E[\langle \tx{i},\tx{i'} \rangle \ {\tx{i}}[j] \  \tx{i'}[j']]= 0 &\text{if }i\neq i',\\
\E[\|\tx{i}\|^2 \ {\tx{i}}[j] \  {\tx{i}}[j']]= 0 &\text{if }i=i',
\end{cases}
\end{align}
and
\begin{align} \label{exp:cross}    \E\left[\langle{\wstar}, \tx{i} \rangle \ {\tx{i}}[j] 
\ \langle \wstar,\tx{i'}\rangle \ \tx{i'} [j] \right] = \begin{cases}
\E[{(\tx{i}}[j])^2 \ (\tx{i'}[j])^2]= 1&\text{if }i\neq i',\\
\E\left[\langle{\wstar}, \tx{i}  \rangle^2  \ ({\tx{i}}[j])^2\right]= d+2&\text{if }i=i',
\end{cases}
\end{align}
where the last case follows from the fact that the fourth moment of Gaussian is $3$ and
\begin{align}
\E\left[\langle{\wstar}, \tx{i} \rangle^2 \ ({\tx{i}}[j])^2\right] = \E\left[ \|{\tx{i}}\|^2 \  ({\tx{i}}[j])^2 \right] = 3 +d-1 = d+2.
\end{align} 
Combining the above calculations together, we arrive at
\begin{align}
\E \left[ \inp{\MM}{E_{d+1,j}}  \MM \right]  &=   \frac{1}{n^2} \cdot \left( n(n-1) +  (d+2) n \right)  (E_{d+1,j} + E_{j,d+1})\\
&=    \left( \frac{n-1}{n} +  (d+2) \frac{1}{n} \right)    (E_{d+1,j} + E_{j,d+1})\,. \label{exp:first}
\end{align}
Therefore, combining \eqref{exp:second} and \eqref{exp:first}, the results follows. 
\end{proof}




\underline{\emphh{3. Combining global minima of each component.}} 
From \autoref{lem:component opt}, it follows that 
\begin{align}  
X_j = - \frac{1}{\left( \frac{n-1}{n} +  (d+2) \frac{1}{n} \right)}E_{d+1,j}\,,
\end{align} 
is the unique global minimum of $f_j$.
Hence, $\cc$ and $\bb = [\bbb_1\ \bbb_1 \ \cdots \ \bbb_d ]$ achieve the global minimum of $f(\cc,\bb)=\sum_{j=1}^d f_j(\cc \bbb_j^\top)$ if they satisfy
\begin{align}
\cc\bbb_j^\top =  - \frac{1}{\left( \frac{n-1}{n} +  (d+2) \frac{1}{n} \right)}E_{d+1,j} \quad \text{for all }i=1,2,\dots,d. 
\end{align}
This can be achieve by the following choice:
\begin{align}  
\cc^\top = \e_{d+1}  ,\quad \bbb_j =     -  \frac{1}{\left( \frac{n-1}{n} +  (d+2) \frac{1}{n} \right)} \e_j \quad \text{for }i=1,2,\dots,d\,,
\end{align} 
where $\e_j$ is the $j$-th coordinate vector. This choice precisely corresponds to 
\begin{align}
\cc = \e_{d+1},\quad \bb = - \frac{1}{\left( \frac{n-1}{n} +  (d+2) \frac{1}{n} \right)}  \begin{bmatrix}I_d  \\ 0 \end{bmatrix}\,.
\end{align}

We next move on to the non-isotropic case.


\subsection{Proof for the non-isotropic case}

\underline{\emphh{1. Diagonal covariance case.}}
We first  consider the case where  $\tx{i}$ is sampled from $\mathcal{N}(0, \Lambda)$ where $\Lambda = \mathrm{diag}(\lambda_1,\dots, \lambda_d)$ and  $\wstar$ is sampled  from $\mathcal{N}(0, I_d)$.  
We prove the following generalization of \autoref{lem:component opt}.
 
\begin{lemma} \label{lem:component opt_noniso}
Suppose that $\tx{i}$ is sampled from $\mathcal{N}(0, \Lambda)$ where $\Lambda = \mathrm{diag}(\lambda_1,\dots, \lambda_d)$ and  $\wstar$ is sampled  from $\mathcal{N}(0, I_d)$. 
Consider the following objective
\begin{align}
f_j(X) =  \E_{Z_0,\wstar}\left[ \inp{\MM}{X}  + \wstar[j] \right]^2\,.
\end{align} 
Then a global minimum is given as 
\begin{align} \label{exp:opt_single_non}
X_j = - \frac{1}{  \frac{n+1}{n}  \lambda_j +   \frac{1}{n} \cdot \left( \sum_k \lambda_k  \right)   } E_{d+1,j}\,,
\end{align} where $E_{i_1,i_2}$ is the matrix whose $(i_1,i_2)$-th entry is $1$,  and the other entries are zero.
\end{lemma}
\begin{proof}[{\bf Proof of \autoref{lem:component opt_noniso}}] 
Similarly to the proof of \autoref{lem:component opt}, it suffices to check that  
\begin{align}
2\E \left[ \inp{\MM}{X_0} \MM \right] + 2 \E \left[\wstar[j] \ 
\MM \right]=0\,,
\end{align}
where we recall that $\MM$ is defined as  
\begin{align}
\MM = \frac{1}{n}\sum_i\begin{bmatrix}
\tx{i} {\tx{i}}^\top & \ty{i} \tx{i}\\
\ty{i} {\tx{i}}^\top & {\ty{i}}^2
\end{bmatrix}\,.
\end{align}
A similar calculation as the proof of \autoref{lem:component opt} yields  
\begin{align} \label{exp:second_non}
\E\left[\wstar[j] \ \MM\right] &=  \lambda_j (E_{d+1,j} + E_{j,d+1}).
\end{align}
Here the factor of $\lambda_j$ comes from the following generalization of \eqref{exp:wj}:
\begin{align}
\E[\wstar[j] \ {\ty{i}} \ {\tx{i}}[k]] =\E[\wstar[j]  \ \langle{\wstar}, \tx{i} \rangle  \ {\tx{i}}[k]] =   \E\left[ \wstar[j]^2 \ {\tx{i}}[j] \ {\tx{i}}[k] \right]  = \lambda_j \mathbbm{1}_{[j=k]}\,.
\end{align}

Next, we compute $\E \left[ \inp{\MM}{E_{d+1,j}} \MM \right]$.
Again, we follow a similar calculation to the proof of \autoref{lem:component opt} except that this time we use the following generalization of 
\eqref{exp:cross}:
\begin{align}
\E\left[\langle{\wstar}, \tx{i} \rangle \ {\tx{i}}[j] \ \langle \wstar,{\tx{i'}} \rangle  \ \tx{i'} [j] \right] = \begin{cases}
\E[{\tx{i}}[j]^2 \ \tx{i'}[j]^2]= \lambda_j^2 &\text{if }i\neq i',\\
\E\left[\langle{\wstar}, \tx{i} \rangle^2 \ {\tx{i}}[j]^2\right]= \lambda_j \sum_k \lambda_k + 2 \lambda_j^2&\text{if }i=i',
\end{cases}
\end{align}
where the last line follows since
\begin{align}
\E\left[\langle{\wstar}, \tx{i} \rangle^2 \ {\tx{i}}[j]^2\right] &= \E\left[ \|{\tx{i}}\|^2 \  {\tx{i}}[j]^2 \right] =\E\left[ {\tx{i}}[j]^2 \ \sum_k {\tx{i}}[k]^2 \right]  = \lambda_j \sum_k \lambda_k + 2 \lambda_j^2\,.
\end{align} 
Therefore, we have
\begin{align}
\E \left[ \inp{\MM}{E_{d+1,j}}  \MM \right]  &=   \frac{1}{n^2} \cdot \left( n(n-1) \lambda_j^2 +   n  \lambda_j \sum_k \lambda_k + 2 n\lambda_j^2\right)  (E_{d+1,j} + E_{j,d+1})\\
&=    \left( \frac{n+1}{n} \lambda_j^2 +    \frac{1}{n} (\lambda_j \sum_k \lambda_k  )  \right)    (E_{d+1,j} + E_{j,d+1})\,. \label{exp:first_non}
\end{align}
Therefore, combining \eqref{exp:second_non} and \eqref{exp:first_non},  
the results follows. 
\end{proof}

Now we finish the proof. 
From \autoref{lem:component opt}, it follows that 
\begin{align}  
X_j = - \frac{1}{  \frac{n+1}{n}  \lambda_j +   \frac{1}{n} \cdot \left( \sum_k \lambda_k  \right)   } E_{d+1,j}
\end{align} 
is the unique global minimum of $f_j$.
Hence, $\cc$ and $\bb = [\bbb_1\ \bbb_1 \ \cdots \ \bbb_d ]$ achieve the global minimum of $f(\cc,\bb)=\sum_{j=1}^d f_j(\cc,\bb_j)$ if they satisfy
\begin{align}
\cc\bbb_j^\top =   X_j = - \frac{1}{  \frac{n+1}{n}  \lambda_j +   \frac{1}{n} \cdot \left( \sum_k \lambda_k  \right)   } E_{d+1,j}\quad \text{for all }i=1,2,\dots,d. 
\end{align}
This can be achieve by the following choice:
\begin{align}  
\cc^\top = \e_{d+1}  ,\quad \bbb_j =     -  \frac{1}{  \frac{n+1}{n}  \lambda_j +   \frac{1}{n} \cdot \left( \sum_k \lambda_k  \right)   } \e_j \quad \text{for }i=1,2,\dots,d\,,
\end{align} 
where $\e_j$ is the $j$-th coordinate vector. This choice precisely corresponds to 
\begin{align}
\cc = \e_{d+1},\quad \bb =  - \begin{bmatrix}
\mathrm{diag}\left(\left\{\frac{1}{  \frac{n+1}{n}  \lambda_j +   \frac{1}{n} \cdot \left( \sum_k \lambda_k  \right)   } \right\}_j\right)   \\
0  
\end{bmatrix} .
\end{align}


\underline{\emphh{2. Non-diagonal covariance case (the setting of \autoref{thm:main_single}).}}
We finally prove the general result of \autoref{thm:main_single}, namely  $\tx{i}$ is sampled from a Gaussian with covariance $\Sigma = U\Lambda U^\top$ where $\Lambda = \mathrm{diag}(\lambda_1,\dots, \lambda_d)$ and $\wstar$ is sampled  from $\mathcal{N}(0, I_d)$.  
The proof works by reducing this case to the previous case.
For each $i$, define $\ttx{i} :=  U^T\tx{i}$. Then $\E[\ttx{i}(\ttx{i})^\top ] = \E[U^\top (U\Lambda U^\top) U] = \Lambda$.
Now let us write the loss function \eqref{exp:loss_single} with this new coordinate system: since $\tx{i} = U \ttx{i}$, we have  
\begin{align}
f(\cc,\bb) &=  \E_{Z_0,\wstar} \left[(\cc^\top \MM \bb   + \wstar^\top) U\ttx{n+1} \right]^2 =\sum_{j=1}^d   \lambda_j \E_{Z_0,\wstar} \left[ \left((\cc^\top \MM \bb   + \wstar^\top) U\right) [j]  \right]^2 \,.
\end{align}
Hence, let us consider 
the vector $(\cc^\top \MM \bb   + \wstar^\top) U$. By definition of $\MM$, we have
\begin{align}
(\cc^\top \MM \bb   + \wstar^\top) U &= \frac{1}{n}\sum_i  \cc^\top \begin{bmatrix} \tx{i} \\
\inp{\tx{i}}{\wstar} \end{bmatrix}^{\otimes 2}  \bb U + \wstar^\top U\\
&=  \frac{1}{n}\sum_i  \cc^\top \begin{bmatrix} U\tilde{x}_i \\
\inp{U\tx{i}}{\wstar} \end{bmatrix}^{\otimes 2} \bb U+ \wstar^\top U\\
&=  \frac{1}{n}\sum_i  \cc^\top \begin{bmatrix}U & 0 \\0 & 1\end{bmatrix}\begin{bmatrix} \tilde{x}_i \\
\inp{U\tx{i}}{\wstar} \end{bmatrix}^{\otimes 2} \begin{bmatrix}U^\top & 0 \\0 & 1\end{bmatrix}\bb U+ \wstar^\top U \\
&= \frac{1}{n}\sum_i  \ttcc^\top  \begin{bmatrix} \tilde{x}_i \\
\inp{\tx{i}}{ \twstar} \end{bmatrix}^{\otimes 2} \ttbb + \twstar^\top 
\end{align}
where we define $\ttcc^\top  \coloneqq \cc^\top \begin{bmatrix}U & 0 \\0 & 1\end{bmatrix}$, $\ttbb\coloneqq \begin{bmatrix}U^\top & 0 \\0 & 1\end{bmatrix}\bb U$, and $\twstar \coloneqq U^\top \wstar$.
By the rotational symmetry, $\twstar$ is also distributed as $\mathcal{N}(0, I_d)$.
Hence, this reduces to the previous case, and a global minimum is given as  
\begin{align}
\ttcc = \e_{d+1},\quad \ttbb =  - \begin{bmatrix}
\mathrm{diag}\left(\left\{\frac{1}{  \frac{n+1}{n}  \lambda_j +   \frac{1}{n} \cdot \left( \sum_k \lambda_k  \right)   } \right\}_j\right)   \\
0  
\end{bmatrix} .
\end{align}
From the definition of $\ttcc,\ttbb$, it thus follows that a global minimum is given by
\begin{align}      \cc^\top = \e_{d+1} ,\quad \bb =  - \begin{bmatrix}
U \mathrm{diag}\left(\left\{\frac{1}{  \frac{n+1}{n}  \lambda_i +   \frac{1}{n} \cdot \left( \sum_k \lambda_k  \right)   } \right\}_i\right) U^\top \\
0  
\end{bmatrix}\,,
\end{align}
as desired.
 
 

\subsection{Proof for non-linear attentions (\autoref{thm:nonlinear})}
\label{pf:nonlinear}

As mentioned in  \autoref{thm:nonlinear}, we focus on the setting where the last row of $Q$ is zero, i.e., let
\begin{align}
Q = \begin{bmatrix} A & a  \\
0^\top & 0 \end{bmatrix}\quad \text{for }A\in \R^{d\times d}~\text{and}~a \in \R^d.
\end{align}
We first rewrite the loss function and simplify it 
following \autoref{sec:rewrite loss}.
Moreover, for simple notation we will often write $z,x$ instead of  $\tz{n+1},\tx{n+1}$.

\underline{\emphh{1. Rewriting loss function.}}


Following \autoref{sec:rewrite loss}, let us write down the in-context loss  \eqref{def:ICL linear}.  for the single-layer nonlinear attention denoted by $f(P,Q)$:  
\begin{align} 
f\left(P,Q\right)  
&=\E_{Z_0,\wstar} \left( \left[Z_{0} +\frac{1}{n} \atth_{P,Q}(Z_0) \right]_{d+1,n+1} + \wstar^\top x\right)^2
\end{align} 
Recalling the definition of the ReLU attention $\atth_{P,Q}(Z) \coloneqq  P ZM \  \relu (
Z^\top Q Z)$, the data matrix $Z \coloneqq [\tz{1} \ \cdots \ \tz{n+1}]$, and the mask matrix $\aa$, the term $ \atth_{P,Q}(Z_0)$ can be written as:
\begin{align}
\atth_{P,Q}(Z_0) = \underbrace{PZ_0}_{\R^{(d+1)\times (n+1)} } \cdot  \begin{bmatrix} I_{n\times n} & 0 \\0 & 0 \end{bmatrix}  \cdot   \underbrace{\relu\left(Z_0^\top Q Z_0 \right)\,.}_{\R^{(n+1)\times (n+1)}}
\end{align}
Hence, it follows that the $(d+1, n+1)$-th entry of $\atth_{P,Q}(Z_0)$ is equal to the product of the $(d+1)$-th row of $PZ_0$, the mask matrix $\aa$, and the $(n+1)$-th column of $\relu\left(Z_0^\top Q Z_0 \right)$. Hence, let us write them down explicitly:
\begin{list}{$\bullet$}{\leftmargin=1.5em}
\setlength{\itemsep}{1pt}
\item Letting $\cc^\top$ be the last row of the matrix $P$, it holds that the $(d+1)$-th row of $PZ_0$ is equal to $[\inpp{\cc}{\tz{i}}]_{i=1,\dots,n+1}$.
\item The $(n+1)$-th column of $\relu\left(Z_0^\top Q Z_0 \right)$ is equal to $\left[\relu\left((\tz{i})^\top Q \tz{n+1} \right)  \right]_{i=1,\dots, n+1}$. Letting  $\bb$ the first $d$ columns of $Q$,  this vector is equal to $\left[\relu\left((\tx{i})^\top \bb \tx{n+1} \right) \right]_{i=1,\dots, n+1}$ because the last row of $Q$ is zero and the last row of $\tz{n+1}$ is zero (since $(\tz{n+1})^\top = [(\tx{n+1})^\top \ 0]$). 
\end{list}
Thus, the product of   $[\inpp{\cc}{\tz{i}}]_{i=1,\dots,n+1}$, the mask matrix $\aa$, and $\left[\relu\left((\tx{i})^\top \bb \tx{n+1} \right) \right]_{i=1,\dots, n+1}$ results in the following expression of the attention (writing $z,x$ instead of  $\tz{n+1},\tx{n+1}$):
\begin{align}
    \left[ \atth_{P,Q}(Z_0) \right]_{d+1,n+1}    =  \sum_{i=1}^n \left[ \inpp{\cc}{\tz{i}} \cdot \relu((\tx{i})^\top \bb x ) \right] \,.
\end{align} 
Since $[Z_0]_{d+1,n+1}=0$, we therefore have 
\begin{align}
    \left[Z_{0} +\frac{1}{n} \atth_{P,Q}(Z_0) \right]_{d+1,n+1} = \frac{1}{n}\sum_{i=1}^n \left[ \inpp{\cc}{\tz{i}}\cdot \relu((\tx{i})^\top \bb x ) \right]\,.
\end{align}
Therefore, it follows that the in-context loss $f(P,Q)$ only depends on $\cc$ and $\bb$.
Henceforth, let us write $f(\cc,\bb)$ instead of $f(P,Q)$ following \autoref{sec:rewrite loss}. 
In particular, writing $\cc^\top = [\cc_0^\top ,\cc_1]$ for $\cc_0\in \R^d$ and $\cc_1\in \R$, the loss function can be expressed as 
\begin{align} \label{exp:nonlinear_loss}
    f(\cc,\bb) \coloneqq \E \left( \frac{1}{n}\sum_{i=1}^n \left[(\inpp{\cc_0}{\tx{i}}+ \cc_1 \ty{i})\cdot \relu((\tx{i})^\top \bb x ) \right] + \inpp{\wstar}{x}  \right)^2\,.
\end{align}






 \underline{\emphh{2. Simplifying the loss function with symmetry.}}

 
Now, we use the fact that both $\tx{i}$'s and $\wstar$ are sampled from the isotropic Gaussian, i.e., $\mathcal{N}(0, I_{d})$ in order to further simplify the loss function in \eqref{exp:nonlinear_loss}.
In particular, we use the following facts:
\begin{list}{$\bullet$}{\leftmargin=1.5em}
\setlength{\itemsep}{1pt}
    \item[($a$)] For orthonormal matrices $U,V \in \R^{d\times d}$, it holds that $U\tx{i}$, $Vx$ and $U\wstar$ have the same distributions as $\mathcal{N}(0, I_{d})$. 
    \item[($b$)] Moreover, for a diagonal matrix $\Xi = \diag(\xi_i) \in \R^{d\times d}$ with the diagonal entries being  random signs $\xi_i \sim \{\pm 1\}$, it holds that  $\Xi\tx{i}$, $\Xi x$ and $\Xi\wstar$ have the same distributions as $\mathcal{N}(0, I_{d})$.
\end{list}
Now let us fix a matrix $\bb\in\R^{d\times d}$ and $\cc^\top = [\cc_0^\top ,\cc_1]$ for $\cc_0\in \R^d$ and $\cc_1\in \R$. Letting $\bb= U\Sigma V^\top$ be the SVD of the matrix $A$, it follows that 
\begin{align}  
    f(\cc,\bb) &= \E \left[ \frac{1}{n}\sum_{i=1}^n \left[(\cc_0^\top\tx{i}+ \cc_1 \wstar^\top\tx{i} )\cdot \relu((\tx{i})^\top \bb x ) \right] + \wstar^\top x  \right]^2\\
    &\overset{(a)}{=}  \E \left[ \frac{1}{n}\sum_{i=1}^n \left[(\cc_0^\top U\tx{i}+ \cc_1 \wstar^\top U^\top U\tx{i})\cdot \relu((\tx{i})^\top U^\top \bb V x ) \right] + \wstar^\top U^\top Vx  \right]^2\\
    &\overset{(b)}{=}  \E \left[ \frac{1}{n}\sum_{i=1}^n \left[(\cc_0^\top U\Xi\tx{i}+ \cc_1 \wstar^\top\tx{i})\cdot \relu((\tx{i})^\top \Sigma x ) \right] +  \wstar^\top \Xi U^\top V\Xi x   \right]^2\\
    &\geq  \E \left[ \E_{\Xi}\left\{\frac{1}{n}\sum_{i=1}^n \left[(\cc_0^\top U\Xi\tx{i}+ \cc_1 \wstar^\top\tx{i})\cdot \relu((\tx{i})^\top \Sigma x ) \right] +  \wstar^\top \Xi U^\top V\Xi x  \right\} \right]^2\\
     &=  \E \left[  \frac{1}{n}\sum_{i=1}^n \left[\cc_1 \wstar^\top\tx{i}\cdot \relu((\tx{i})^\top \Sigma x ) \right] +  \wstar^\top \diag( U^\top V) x   \right]^2\\
     &=: \ttf(\cc_1,\Sigma, D \coloneqq \diag( U^\top V))\,. 
\end{align}
where in the third line we use the fact that $\Xi^
\top\Sigma \Xi= \Xi \Sigma \Xi = \Sigma$; and the fourth line follows from the Jensen's inequality.
Hence for the remainder of the proof, we will  characterize the global minimizer of the lower bound, i.e., $\ttf$ and then we will connect it back to the original objective.



 \underline{\emphh{3. Computation of the lower bound $\ttf$.}}


Let us now explicitly compute
$\ttf$. Let us rewrite the definition of $\ttf$.
In fact since, $\relu = \mathrm{ReLU}$ is homogenous, one can further simplify the lower bound by pushing the constant $\cc_1$ inside and write $\cc_1 \Sigma$  as $\Sigma$.
Hence, for two diagonal matrices $\Sigma, D\in \R^{d\times d}$, $\ttf$ is defined as:
\begin{align}
    \ttf(\Sigma, D) \coloneqq  \E \left[  \frac{1}{n}\sum_{i=1}^n \left[ \inpp{\wstar}{\tx{i}}\cdot \relu (x^\top \Sigma \tx{i}  )\right] +  \wstar^\top D x   \right]^2\,.
\end{align}
In particular, $D$ is constrained to be the diagonal part of an orthogonal matrix (since $D=  \diag( U^\top V)$ in the above derivation).
Now we focus on characterizing the global minimizers of $\ttf$.

The main part of the argument is inspired by the elegant observation of \cite{erdogdu2016scaled}, which says that the solution of least squares and generalized linear models are collinear for Gaussian inputs. 
We leverage the same proof technique (\emph{\`{a} la} Stein's Lemma) to prove that the presence of ReLU only changes the scaling of global optimum. 

First, since $\wstar$ is isotropic Gaussian, we can take the expectation over $\wstar$ to obtain
\begin{align}
     \ttf(\Sigma, D) =  \E \left[  \frac{1}{n}\sum_{i=1}^n \left[  \relu (x^\top \Sigma \tx{i}  ) \ \tx{i}\right] +   D x   \right]^2\,,
\end{align} 
which after a careful expansion becomes
\begin{align} \label{exp:nonlinear_expansion}
    \E \left[  \frac{1}{n^2}\sum_{i,j}   \relu (x^\top \Sigma \tx{i}  )  \relu (x^\top \Sigma \tx{j}  )  \inpp{\tx{i}}{\tx{j}} +  \frac{2}{n}\sum_{i}    \relu (x^\top \Sigma \tx{i}  )  \inpp{\tx{i}}{Dx}   \right]  + \text{const.} \quad\quad
\end{align} 
In order to compute \eqref{exp:nonlinear_expansion}, we will rely on the aforementioned argument of \cite{erdogdu2016scaled}.
In particular, from integration by parts, or Stein's lemma~\citep{erdogdu2016scaled} (since $x\sim \mathcal{N}(0,I_d)$), we have 
\begin{align}
\mathbb{E}_{x} [  \sigma(x^\top v) x] = \mathbb{E}_{x} [\sigma'(x^\top v) ] v \quad \text{for a fixed $v\in \R^d$.}    
\end{align} 
We use this to compute all the terms in \eqref{exp:nonlinear_expansion} as follows:
\begin{list}{$\bullet$}{\leftmargin=1.5em}
\setlength{\itemsep}{1pt}
\item 
We first apply Stein's lemma  to the first term of \eqref{exp:nonlinear_expansion} for $i\neq j$. This results in 
\begin{align}
&\E_{\tx{i},\tx{j},x}\relu (x^\top \Sigma \tx{i}  )  \relu (x^\top \Sigma \tx{j}  )  \inpp{\tx{i}}{\tx{j}}  \\
&\quad =   \E_{\tx{j},x} [\E_{\tx{i}} [\relu'(x^\top \Sigma \tx{i})] \ \relu(x^\top \Sigma \tx{j}) \ x^\top \Sigma x^{(j)}] 
\end{align} 
Using  the fact that $\tx{i}$ is a symmetric random variable, one can compute the expectation above as follows: one the one hand, we know  $\E_{\tx{i}} [\relu'(x^\top \Sigma \tx{i})] = \mathbb{E}_{\tx{i}} [\relu'(-x^\top \Sigma \tx{i})]$.  On the other hand, we also know that for any scalar $\alpha$, $\relu'(-\alpha)+ \relu'(\alpha) =1$. Therefore, we conclude that  $\E_{\tx{i}} [\relu'(x^\top \Sigma \tx{i})] = 1/2$. Thus, applying this technique twice, we obtain the following
\begin{align}
\E_{\tx{i},\tx{j},x}\relu (x^\top \Sigma \tx{i}  )  \relu (x^\top \Sigma \tx{j}  )  \inpp{\tx{i}}{\tx{j}}  =  \frac{1}{4}\mathbb{E}_{ x} [ x^\top \Sigma^2 x  ] = \frac{1}{4} \tr(\Sigma^2)\,.
\end{align}

\item 
Similarly, we can use Stein's lemma to the second term of \eqref{exp:nonlinear_expansion} to conclude
\begin{align}
    \E \relu (x^\top \Sigma \tx{i}  )  \inpp{\tx{i}}{Dx} = \frac{1}{2}\E_x x^\top \Sigma D x =  \frac{1}{2}\tr(\Sigma D)\,.
\end{align} 

\item Lastly, the computation of the first term of \eqref{exp:nonlinear_expansion} for $i=j$ is straightforward.
Using the fact that $\forall \alpha\in\R$, $\relu^2(\alpha)+\relu^2(-\alpha) = a^2$, we get 
\begin{align}
    &\E \left[ \relu^2(x^\top \Sigma \tx{i}) \ \|\tx{i} \|^2 \right] = \frac{1}{2} \mathbb{E} [(x^\top \Sigma \tx{i})^2 \ \| \tx{i}\|^2 ]\\
    &\quad = \frac{1}{2}\E \left[ (\tx{i})^\top \Sigma^2 \tx{i} \ \| \tx{i} \|^2 \right] = \frac{d+2}{2} \tr(\Sigma^2)\,.
\end{align}
\end{list}
Putting things all together (and ignoring the constant part in \eqref{exp:nonlinear_expansion}), we have 
\begin{align}\label{exp:lower}
     \ttf(\Sigma, D) = \frac{2(d+2) + (n-1)}{4n} \tr(\Sigma^2) + \tr(\Sigma D)\,.
\end{align}


 \underline{\emphh{4. Connecting back to the original loss function.}}
 
One can in fact write \eqref{exp:lower} solely in terms of $\cc_1$ and $\bb$ as follows:
\begin{align}
    \frac{2(d+2) + (n-1)}{4n} \tr(\Sigma^2) + \tr(\Sigma D) = \frac{2(d+2) + (n-1)}{4n} \norm{\cc_1\bb}_F^2 + \tr(\cc_1\bb)\,.
\end{align}
Since the latter is a convex function in the matrix $\cc_1\bb$, it follows that the minimizer corresponds to 
\begin{align}
    \cc_1\bb = -\frac{2n}{2(d+2)+(n-1)} \cdot I_d
\end{align}
In fact the choice $\cc_1 = 1$, $\cc_0 =0$, and $\bb =  -\frac{2n}{2(d+2)+(n-1)} \cdot I_d$ achieves this, and more crucially, satisfies the property that $\ttf = f$ for the corresponding parameters. Therefore, this shows that such choice is a global minimizer.


 
 




\section{Proofs for the multi-layer case}
\label{a:foo}

\subsection{Proof of \autoref{t:two_layer}}
The proof is based on probabilistic methods \citep{alon2016probabilistic}. 
According to \autoref{l:icl_trace_form}, the objective function can be written as (for more details check the derivations in ~\eqref{e:t:dynamic_Y_only}) 
\begin{align}
f(A_1,A_2) & = \E \tr\left( \E \left[\prod_{i=1}^2 (I -  X_0^\top A_i X_0 M) X_0^\top \wstar \wstar^\top X_0 \prod_{i=1}^2 (I - M X_0^T A_i X_0) \right]\right) \\ 
& = \E \tr\left( \E \left[\prod_{i=2}^1 (I -  X_0^\top A_i X_0 M) X_0^\top X_0 \prod_{j=1}^2 (I - M X_0^T A_j X_0) \right]\right), 
\end{align}
where we use the isotropy of $\wstar$ and the linearity of trace to get the last equation.
Suppose that $A_0^*$ and $A_1^*$ denote the global minimizer of $f$ over symmetric matrices. Since $A_1^*$ is a symmetric matrix, it admits the spectral  decomposition $A_1 = U D_1 U^\top$ where $D_1$ is a diagonal matrix and $U$ is an orthogonal matrix. Remarkably,  the distribution of $X_0$ is invariant to a linear transformation by an orthogonal matrix, i.e, $X_0$ has the same distribution as $X_0 U^\top$. This invariance yields 
\begin{align}
f(U D_1 U^\top, A_2^*) = f(D_1, U^\top A_2^* U).
\end{align}
Thus, we can assume $A_1^*$ is diagonal without loss of generality. To prove $A_2^*$ is also diagonal, we leverage a probabilistic proof technique. Consider the random diagonal matrix $S$ whose diagonal elements are either $1$ or $-1$ with probability $\frac{1}{2}$. Since the input distribution is invariant to orthogonal transformations, we have 
\begin{align}
f( D_1, A_2^*) = f(S D_1 S, S A_2^* S) = f(D_1, S A_2^* S).
\end{align}
Note that we use $S D_1 S = D_1$ in the last equation, which holds due to $D_1$ and $S$ are diagonal matrices and $S$ has diagonal elements in $\{+1, -1 \}$. Since $f$ is convex in $A_2$, a straightforward application of Jensen's inequality yields 
\begin{align}
f(D_1, A_2^*) = \E \left[ f(D_1, S A_2^* S) \right] \geq f(D_1, \E \left[ S A_2^* S \right]) = f(D_1, \diag(A_2^*)).
\end{align}
Thus, there are diagonal $D_1$ and $\diag(A_2^*)$ for which $f(D_1, \diag(A_2^*)) \leq f(A_1^*, A_2^*)$ holds for an optimal $A_1^*$ and $A_2^*$. This concludes the proof.
 
\subsection{Proof of \autoref{t:L_layer_P_0}}
\label{sec:pf:t:L_layer_P_0}

Let us drop the factor of $\nicefrac{1}{n}$ which was present in the original update \eqref{e:dynamics_Z}. This is because the constant $1/n$ can be absorbed into $A_i$'s. Doing so does not change the theorem statement, but reduces notational clutter.

Let us consider the reformulation of the in-context loss $f$ presented in \autoref{l:icl_trace_form}. Specifically, let $\overline{Z}_0$ be defined as 
\begin{align*}
\overline{Z}_0 = \begin{bmatrix}
\tx{1} & \tx{2} & \cdots & \tx{n} &\tx{n+1} \\ 
\ty{1} & \ty{2} & \cdots &\ty{n}& \ty{n+1}
\end{bmatrix} \in \R^{(d+1) \times (n+1)},
\end{align*}
where $\ty{n+1} = \lin{\wstar, \tx{n+1}}$. Let $\overline{Z}_i$ denote the output of the $(i-1)^{th}$ layer of the linear transformer (as defined in \eqref{e:dynamics_Z}, initialized at $\overline{Z}_0$). For the rest of this proof, we will drop the bar, and simply denote $\overline{Z}_i$ by $Z_i$.\footnote{This use of $Z_i$ differs the original definition in \eqref{d:Z_0}. But we will not refer to the original definition anywhere in this proof.} Let $X_i\in \R^{d\times (n+1)}$ denote the first $d$ rows of $Z_i$ and let $Y_i\in \R^{1\times (n+1)}$ denote the $(d+1)^{th}$ row of $Z_k$. Under the sparsity pattern enforced in \eqref{eq:sparse_attention}, we verify that, for any $i \in \lrbb{0,\dots,k}$,
\begin{align*}
& X_i = X_0,\\
& Y_{i+1} =  Y_{i}  + Y_{i} M X_{i}^\top A_i X_{i} = Y_0 \prod_{\ell=0}^{i} \lrp{I + M X_0^\top A_\ell X_0}.
\numberthis \label{e:t:dynamic_Y_only}
\end{align*}
where $M = \begin{bmatrix}I_{n\times n} & 0 \\0 & 0\end{bmatrix}$. We adopt the shorthand $A = \lrbb{A_i}_{i=0}^k$.

We adopt the shorthand $A = \lrbb{A_i}_{i=0}^k$. Let $\S \subset \R^{(k+1) \times d \times d}$, and $A \in \S$ if and only if for all $i\in \lrbb{0,\dots,k}$, there exists scalars $a_i \in \R$ such that $A_i = a_i \Sigma^{-1}$ and $B_i = b_i I$. We use $f(A)$ to refer to the in-context loss of \autoref{t:L_layer_P_0}, that is,
\begin{align*}
f(A) := f \lrp{ \left\{ Q_i = \begin{bmatrix}
A_i & 0 \\ 
0 & 0
\end{bmatrix}, P_i = \begin{bmatrix}
0_{d\times d} & 0 \\ 
0 & 1 
\end{bmatrix}\right\}_{i=0}^k}.
\end{align*}

Throughout this proof, we will work with the following formulation of the \emph{in-context loss} from \autoref{l:icl_trace_form}:
\begin{align*}
f(A)
=& \E_{(X_0,\wstar)} \lrb{\tr\lrp{\lrp{I-M}Y_{k+1}^\top Y_{k+1}\lrp{I-M}}}.
\numberthis \label{e:t:oiemgrkwfdlw:0}
\end{align*}
The theorem statement is equivalent to the following:
\begin{align*}
\inf_{A \in \S} \sum_{i=0}^k \lrn{\nabla_{A_i} f(A)}_F^2 = 0,
\numberthis \label{e:t:unkqdwon:0}
\end{align*}
where $\nabla_{A_i} f$ denotes derivative wrt the Frobenius norm $\lrn{A_i}_F$. Towards this end, we establish the following intermediate result: if $A \in \S$, then for any $R\in \R^{(k+1) \times d \times d}$, there exists $\tilde{R} \in \S$, such that, at $t=0$,
\begin{align*}
\frac{d}{dt} f(A + t\tilde{R}) \leq \frac{d}{dt} f(A + tR).
\numberthis \label{e:t:unkqdwon:1}
\end{align*}
In fact, we show that $\tilde{R}_i := r_i I$, for $r_i = \frac{1}{d} \tr\lrp{\Sigma^{1/2} R_i \Sigma^{1/2}}$. This implies \eqref{e:t:unkqdwon:0} via the following simple argument: Consider the "$\S$-constrained gradient flow": let $A(t): \R^+ \to \R^{(k+1)\times d \times d}$ be defined as 
\begin{align*}
& \frac{d}{dt} A_i(t) = - r_i(t) \Sigma^{-1}, \quad r_i(t) := \tr(\Sigma^{1/2} \nabla_{A_i} f (A(t)) \Sigma^{1/2})
\end{align*}
for $i=0,\dots,k$. By \eqref{e:t:unkqdwon:1}, we verify that
\begin{align*}
\frac{d}{dt} f(A(t)) \leq - \sum_{i=0}^k \lrn{\nabla_{A_i} f(A(t))}_F^2.
\numberthis \label{e:t:unkqdwon:2}
\end{align*}
We verify from its definition that $f(A) \geq 0$; if the infimum in \eqref{e:t:unkqdwon:0} fails to be zero, then inequality \eqref{e:t:unkqdwon:2} will ensure unbounded descent as $t\to \infty$, contradicting the fact that $f(A)$ is lower-bounded. This concludes the proof.

\underline{\emphh{Proof outline.}}
The remainder of the proof will be devoted to showing \eqref{e:t:unkqdwon:1}, which we outline as follows:


\begin{itemize}[leftmargin=*]
\item  In Step 1, we reduce the condition in \eqref{e:t:unkqdwon:2} to a more easily verified \emph{layer-wise} condition. Specifically, we only need to verify \eqref{e:t:unkqdwon:2} when $R_i$ are all zero except for $R_j$ for some fixed $j$ (see \eqref{e:t:unkqdwon:3})

At the end of Step 1, we set up some additional notation, and introduce an important matrix $G$, which is roughly "a product of attention layer matrices". In \eqref{e:t:unkqdwon:4}, we study the evolution of $f(A(t))$ when $A(t)$ moves in the direction of $R$, as $X_0$ is (roughly speaking) randomly transformed. 

\item In Step 2, we use the results of Step 2 to to study $G$ (see \eqref{e:t:UG_pnull}) and $\frac{d}{dt} G(A(t))$ (see \eqref{e:t:unkqdwon:5}) under random transformation of $X_0$. The idea in \eqref{e:t:unkqdwon:5} is that "randomly transforming $X_0$" has the same effect as "randomly transforming $\textcolor{red}{S}$" (recall $\textcolor{red}{S}$ is the perturbation to $\textcolor{red}{B}$). 

\item In Step 3, we apply the result from Step 2 to the expression of $\frac{d}{dt} f(A(t))$ in \eqref{e:t:unkqdwon:4}. We verify that $\tilde{R}$ in \eqref{e:t:unkqdwon:1} is exactly the expected matrix after "randomly transforming $\textcolor{red}{S}$". This concludes our proof.
\end{itemize}

\underline{\emphh{1. Reduction to layer-wise condition.}}
To prove \eqref{e:t:unkqdwon:1}, it suffices to show the following simpler condition: Let $j\in \lrbb{0,\dots,k}$. Let $R_j\in \R^{d\times d}$ be arbitrary matrices. For $C\in \R^{d\times d}$, let $A(t C, j)$ denote the collection of matrices, where $\lrb{A(t C, j)}_j = A_j + t C$, and for $i\neq j$, $A(t C, j)_i = A_i$. We show that for all $j\in \lrbb{0,\dots,k},R_j\in \R^{d\times d}$, there exists $\tilde{R}_j = r_j \Sigma^{-1}$, such that, at $t=0$,
\begin{align*}
& \frac{d}{dt} f(A(t\tilde{R}_j, j)) \leq \frac{d}{dt} f(A(tR_j,j))
\numberthis \label{e:t:unkqdwon:3}
\end{align*}
We can verify that \eqref{e:t:unkqdwon:1} is equivalent to \eqref{e:t:unkqdwon:3} by noticing that for any $R$, at $t=0$, $\frac{d}{dt} f(A + tR) = \sum_{j=0}^k \frac{d}{dt} f(A(tR_j,j))$. We will now work towards proving \eqref{e:t:unkqdwon:3} for some index $j$ that is arbitrarily chosen but fixed throughout.

Let us define, for any $C\in \R^{d\times d}$, $G(X,A_j + C) := X \prod_{i=0}^{k} \lrp{I - M X^\top \lrb{A(C, j)}_i X}$. By \eqref{e:t:dynamic_Y_only} and \eqref{e:t:oiemgrkwfdlw:0},
\begin{align*}
& f({A(tR_j,j)}) \\
=& \E \lrb{\tr\lrp{\lrp{I-M} Y_{k+1}^{\top} Y_{k+1} \lrp{I-M}}}\\
=& \E \lrb{\tr\lrp{\lrp{I-M} G(X_0,A_j + t{R}_j)^\top \wstar^\top \wstar G(X_0,A_j + t{R}_j) \lrp{I-M}}}\\
=& \E \lrb{\tr\lrp{\lrp{I-M} G(X_0,A_j + t{R}_j)^\top \Sigma^{-1} G(X_0,A_j + t{R}_j) \lrp{I-M}}}
\end{align*}
The second equality follows from plugging in \eqref{e:t:dynamic_Y_only}. For the rest of this proof, let $U$ denote a uniformly randomly sampled orthogonal matrix. Let $\US:= \Sigma^{1/2} U \Sigma^{-1/2}$. Using the fact that $X_0 \overset{d}{=} \US X_0$, we can verify
\begin{align*}
& \at{\frac{d}{dt} f(A(tR_j,j))}{t=0}\\
=& \at{\frac{d}{dt}\E \lrb{\tr\lrp{\lrp{I-M} G(X_0,A_j + t{R}_j)^\top \Sigma^{-1} G(X_0,A_j + t{R}_j) \lrp{I-M}}}}{t=0}\\
=& \at{\frac{d}{dt}\E_{X_0, U} \lrb{\tr\lrp{\lrp{I-M} G(\US X_0, A_j + t R_j)^\top \Sigma^{-1} G(\US X_0, A_j + t R_j) \lrp{I-M}}}}{t=0}\\
=& 2\E_{X_0, U} \lrb{\tr\lrp{\lrp{I-M} G(\US X_0, A_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(\US X_0, A_j + t R_j)}{t=0} \lrp{I-M}}}.
\numberthis \label{e:t:unkqdwon:4}
\end{align*}

\underline{\emphh{2. $G$ and $\frac{d}{dt} G$ under random transformation of $X_0$.}}
We will now verify that $G(\US X_0, A_j) = \US G(X_0, A_j)$:
\begin{align*}
& G(\US X_0, A_j)\\
=& \US X_0 \prod_{i=0}^{k} \lrp{I + M X_0^T \US^{\top} A_i \US X_0}\\
=& \US G(X_0, A_j),
\numberthis \label{e:t:UG_pnull}
\end{align*}
where we use the fact that $\US^\top A_i \US = \US^\top (a_i \Sigma^{-1}) \US = A_i$. Next, we verify that
\begin{align*}
& \at{\frac{d}{dt} G(\US X_0, A + tR_j)}{t=0}\\
=& \US X_0 \lrp{\prod_{i=0}^{j-1} (I + M X_0^T A_i X_0)} M X_0^T \US^{\top} R_j \US X_0 \prod_{i=j+1}^k (I + M X_0^T A_i X_0)\\
=& \US \frac{d}{dt} G(X_0, A_j + t\US^{\top} R_j \US)
\numberthis \label{e:t:unkqdwon:5}
\end{align*}
where the first equality again uses the fact that $\US^{\top} A_i \US = A_i$.

\underline{\emphh{3. Putting everything together.}} Let us continue from \eqref{e:t:unkqdwon:4}. Plugging \eqref{e:t:UG_pnull} and \eqref{e:t:unkqdwon:5} into \eqref{e:t:unkqdwon:4}, 
{\allowdisplaybreaks
\begin{align*}
& \at{\frac{d}{dt} f({A(tR_j,j)})}{t=0}\\
=& 2\E_{X_0, U} \lrb{\tr\lrp{\lrp{I-M} G(\US X_0, A_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(\US X_0, A_j + t R_j)}{t=0} \lrp{I-M}}}\\
\overset{(i)}{=}& 2\E_{X_0, U} \lrb{\tr\lrp{\lrp{I-M} G( X_0, A_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(X_0, A_j + t \US^{\top} R_j \US)}{t=0} \lrp{I-M}}}\\
=& 2\E_{X_0} \lrb{\tr\lrp{\lrp{I-M} G( X_0, A_j )^\top \Sigma^{-1} \E_{U}\lrb{\at{\frac{d}{dt} G(X_0, A_j + t \US^{\top} R_j \US)}{t=0}} \lrp{I-M}}}\\
\overset{(ii)}{=}& 2\E_{X_0} \lrb{\tr\lrp{\lrp{I-M} G( X_0, A_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(X_0, A_j + t \E_U\lrb{\US^{\top} R_j \US} )}{t=0} \lrp{I-M}}}\\
=& 2\E_{X_0} \lrb{\tr\lrp{\lrp{I-M} G( X_0, A_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(X_0, A_j + t \cdot r_j \Sigma^{-1})}{t=0} \lrp{I-M}}}\\
=& \at{\frac{d}{dt} f(A(t\cdot r_j \Sigma^{-1}, j))}{t=0},
\end{align*}
}
where $r_j := \frac{1}{d} \tr\lrp{\Sigma^{1/2} R_j \Sigma^{1/2}}$. In the above, $(i)$ uses 1. \eqref{e:t:UG_pnull} and \eqref{e:t:unkqdwon:5}, as well as the fact that $\US^\top \Sigma^{-1} \US = \Sigma^{-1}$. $(ii)$ uses the fact that $\at{\frac{d}{dt} G(X_0, A_j + t C)}{t=0}$ is affine in $C$. To see this, one can verify from the definition of $G$, e.g. using similar algebra as \eqref{e:t:unkqdwon:5}, that $\frac{d}{dt} G(X_0, A_j + C)$ is affine in $C$. Thus $\E_U\lrb{G(X_0, A_j + t \US^{\top} R_j \US)} = G(X_0, A_j + t \E_U\lrb{\US^{\top} R_j \US)}$. 


\iffalse
\newpage
\xc{old proof below}
%with $\overline{Y}_0 = \begin{bmatrix}\ty{1} & \ty{2} \cdots & \ty{n+1}\end{bmatrix}$ and 
\begin{align*}
\overline{Y}_{k+1} 
=& \overline{Y}_0 \prod_{i=0}^{k} \lrp{I - M X_0^T A_i X_0}\\
=& \wstar^\top \underbrace{X_0 \prod_{i=0}^{k} \lrp{I - M X_0^T A_i X_0}}_{:= G(X_0,A)},
\numberthis \label{e:t:G(X,A)}
\end{align*}
In the above, we use $G(X_0,A)$  to denote the product of matrices as indicated above (recall that $A$ is shorthand for $\lrbb{A_i}_{i=0}^k$). Plugging into \eqref{e:t:oiemgrkwfdlw:0}, we can further simplify
\begin{align*}
& f(\lrbb{A_i}_{i=0}^k) \\
=& \E \lrb{\Tr\lrp{\lrp{I-M} \lrp{\wstar^\top G(X_0,A)}^{\top}\lrp{\wstar^\top G(X_0,A)} \lrp{I-M}}}\\
=& \E \lrb{\Tr\lrp{\lrp{I-M} G(X_0,A)^\top \wstar \wstar^\top G(X_0,A)\lrp{I-M}}}\\
\overset{(i)}{=}& \E \lrb{\Tr\lrp{\lrp{I-M} G(X_0,A)^\top \Sigma^{-1} G(X_0,A)\lrp{I-M}}}\\
=:& \E\lrb{h(G(X_0,A))}
\numberthis \label{e:t:h(G)}
\end{align*}
where $(i)$ uses the fact that $\wstar$ is sampled independently of $X_0$, and $\E{\wstar \wstar^{\top}} = \Sigma^{-1}$ as assumed in the Theorem statement. We also define $h(G): \R^{d \times d} \to \R$ as $h(G) = \Tr\lrp{(I-M) G(X_0,A)^{\top} \Sigma^{-1} G(X_0,A) (I-M)}$. To recap: so far we show that the in-context loss, as a function of $\left\{ Q_i = \begin{bmatrix}
A_i & 0 \\ 
0 & 0
\end{bmatrix}, P_i = \begin{bmatrix}
0_{d\times d} & 0 \\ 
0 & 1 
\end{bmatrix}\right\}_{i=0}^k$, as defined in \eqref{def:ICL linear}, is equivalent to $\E\lrb{h(G(X_0,A))}$, defined in \eqref{e:t:h(G)}, where $A = \lrbb{A_i}_{i=0}^k$. 

We will now demonstrate the existence of the proposed critical points for \eqref{e:t:h(G)}. For notational convenience, let $\S \subset \R^{k+1 \times d \times d}$ denote a subset of the parameter space, such that $A\in \S$ is equivalent to "exists $a_0,a_i,\dots,a_k \in \R$, such that for all $i=0,\dots,k$, $A_i = a_i \Sigma^{-1}$". 

Next, we observe two crucial facts about the in-context loss \eqref{e:t:h(G)}: First, $h(G)$ is a convex function of $G$. Second, if $U \in \R^d$ is a uniformly-sampled rotation matrix,\footnote{Same argument works if $U$ is simply a uniformly-sampled random permutation matrix.} then $X_0$ has the same distribution as $\Sigma^{1/2} U \Sigma^{-1/2} X_0$. Thus $\E_{X_0}\lrb{h(G(X_0,A))} = \E_{U, X_0}\lrb{h(G(\Sigma^{1/2} U \Sigma^{-1/2} X_0,A))}$. Combining these two facts, and applying Jensen's inequality, we obtain that for any $A$,
\begin{align*}
\E_{U, X_0}\lrb{h(G(\Sigma^{1/2} U \Sigma^{-1/2} X_0,A))} \geq \E_{X_0}\lrb{h\lrp{\E_{U}\lrb{G(\Sigma^{1/2} U \Sigma^{-1/2} X_0,A)}}}.
\numberthis \label{e:t:jensen_pnull}
\end{align*}
\xc{ended up not needing Jensens!}


Let us now consider some fixed but arbitrary layer index $j \in \lrbb{0,\dots,k}$. Let $R_j\in \R^{d\times d}$, and let $A(R_j,j) := \lrbb{A_0, A_1, A_2,\dots, A_{j-1}, A_j + R_j, A_{j+1}, ,\dots, A_k}$. We also define $N(X, A, R_j, j):= \lrp{\prod_{i=0}^{j-1} (I + M X^T A_i X)} M X^T R_j X \prod_{i=j+1}^k (I + M X^T A_i X)$. From these definitions, we verify
\begin{enumerate}
\item $G(X_0, A(R_j,j)) = G(X_0,  A) + N(X_0, A, R_j, j)$.
\item If $A\in \S$, then for any orthogonal matrix $U \in R^{d\times d}$,  
$$G(\Sigma^{1/2} U \Sigma^{-1/2} X_0, A) = \Sigma^{1/2} U \Sigma^{-1/2} G(X_0,A)$$

\item If $A\in \S$, then for any orthogonal matrix $U \in R_j^{d\times d}$, 
$$N(\Sigma^{1/2} U \Sigma^{-1/2} X_0, A, R_j, j) = \Sigma^{1/2} U \Sigma^{-1/2} N(X_0, A, U^T R_j U, j).$$
\end{enumerate}
Items 2 and 3 above can be verified by considering the definition of $G$ and $N$, and noticing that $(\Sigma^{1/2} U \Sigma^{-1/2} X_0)^T A_i (\Sigma^{1/2} U \Sigma^{-1/2} X_0) = (\Sigma^{1/2} U \Sigma^{-1/2} X_0)^T (a_i \Sigma^{-1}) (\Sigma^{1/2} U \Sigma^{-1/2} X_0) = X_0^T (a_i \Sigma^{-1/2} U^\top \Sigma^{1/2} \Sigma^{-1} \Sigma^{1/2} U \Sigma^{-1/2}) X_0 = X_0^T A_i X_0$.

Finally, let us define $A(t):= A(tR_j,j)$. Then for sufficiently small $t\in \R^+$,
\begin{align*}
& \E_{X_0}\lrb{h(G(X_0, A(tR_j, j))}\\
=& \E_{U,X_0}\lrb{h(G(\Sigma^{1/2} U \Sigma^{-1/2}X_0, A(tR_j, j))}\\
\overset{(i)}{=}& \E_{U,X_0}\lrb{h\lrp{\Sigma^{1/2} U \Sigma^{-1/2}G(X_0, A) + \Sigma^{1/2} U \Sigma^{-1/2} N(X_0, A, \Sigma^{-1/2} U^\top \Sigma^{1/2} tR_j \Sigma^{1/2} U \Sigma^{-1/2},j)}}\\
\overset{(ii)}{=}& \E_{X_0}\lrb{h\lrp{G(X_0, A)}} \\
& + 2 \E_{X_0}\lrb{\Tr\lrp{\lrp{I-M} G(X_0,A)^\top \Sigma^{-1} N(X_0, A, \Sigma^{-1/2} \E_{U}\lrb{U^\top \Sigma^{1/2} tR_j \Sigma^{1/2} U} \Sigma^{-1/2},j)}} + o(t^2)\\
\overset{(iii)}{=}& \E_{X_0}\lrb{h\lrp{G(X_0, A)}} + 2 \E_{X_0}\lrb{\Tr\lrp{\lrp{I-M} G(X_0,A)^\top \Sigma^{-1} N(X_0, A, r \Sigma^{-1},j)}} + o(t^2)\\
=& \E_{X_0}\lrb{h(G(X_0, A(t\cdot r \Sigma^{-1}, j))} + o(t^2),
\end{align*}
where $r:= \frac{1}{d} \sum_{j=1}^d \lrb{\Sigma^{1/2} R_j \Sigma^{1/2}}_{j,j}$. In the above, $(i)$ uses items 1-3 listed above. $(ii)$ is by expanding the expression for $h$ in \eqref{e:t:h(G)}, and the fact that $N(X,A,R_j,j)$ is linear in $R_j$. Step $(iii)$ uses the fact that $\E_{U}\lrb{U^\top \Sigma^{1/2} tR_j \Sigma^{1/2} U} = r I$.

Taking derivative with respect to $t$, we verify that
\begin{align*}
\frac{d}{dt} \E_{X_0}\lrb{h(G(X_0, A(tR_j, j))} = \frac{d}{dt} \E_{X_0}\lrb{h(G(X_0, A(t\cdot r \Sigma^{-1}, j))}.
\end{align*}

Now suppose that we have a $R_i$ for each layer $i=0,\dots,k$. Let ${R} = \lrbb{{R}_i}_{i=0}^k$. Let ${A}(t) = \lrbb{A_i + t{R}_i}_{i=0}^k$. Let ${r}_i := \frac{1}{d} \sum_{j=1}^d \lrb{\Sigma^{1/2} R \Sigma^{1/2}}_{j,j}$. Then
\begin{align*}
& \frac{d}{dt} \E_{X_0}\lrb{h(G(X_0, {A}(t))}\\
=& \sum_{j=1}^d \frac{d}{dt} \E_{X_0}\lrb{h(G(X_0, A(t{R}_j, j))}\\
=& \sum_{j=1}^d \frac{d}{dt} \E_{X_0}\lrb{h(G(X_0, A(t\cdot {r}_j \Sigma^{-1}, j))}
\end{align*}

Let $\tilde{R} = \lrbb{r_i \Sigma^{-1}}_{i=0}^k$. 

We have thus verified the following: Let $R= \lrbb{R_i}_{i=0}^k$ denote any descent direction, and let $\tilde{R}= \lrbb{r_i}_{i=0}^k$, where $r_i := \frac{1}{d} \sum_{j=1}^d \lrb{\Sigma^{1/2} R \Sigma^{1/2}}_{j,j}$. Then moving in the direction of $\tilde{R}$ gives the same amount of descent as $R$, i.e.
\begin{align*}
\frac{d}{dt} f\lrp{\lrbb{A_i + t\tilde{R}_i}_{i=0}^k} = \frac{d}{dt} f\lrp{\lrbb{A_i + t R_i}_{i=0}^k}.
\end{align*}
Consequently, given any initial $A(0) \in \S$, we can define a flow $A(t) : [0,\infty) \to \R^{(k+1)\times d \times d}$, such that 1. $A(t) \in \S$, and 2. for all $t$. $\frac{d}{dt} f(A(t)) \leq -\lrn{\nabla_A f(A(t))}$. \xc{quantify the norm more precisely? see the discussion below.}

\xc{How to show convergence to stationary point? The above only shows that gradient is arbitrarily close to $0$, which is a subtle difference. E.g. gradient descent of $y = 1/x$ never finds a stationary point.}


\xc{\textbf{Proof that the proposed flow is in fact steepst descent:}\\
Let $\lrn{M}_{\Sigma, F}$ denote the Frobenius norm wrt $\Sigma$, i.e. $\lrn{M}_{\Sigma, F}^2 = \Tr\lrp{\Sigma M \Sigma M^{\top}}$. Then 
\begin{align*}
& \lrn{M}_{\Sigma, F}^2\\
=& \Tr\lrp{\Sigma M \Sigma M^{\top}}\\
=& \Tr\lrp{\Sigma M \Sigma M^{\top}}\\
=& \E_U\lrb{\Tr\lrp{\Sigma \Sigma^{-1/2} U^\top \Sigma^{1/2} M \Sigma^{1/2} U \Sigma^{-1/2}\Sigma \Sigma^{-1/2} U^\top \Sigma^{1/2} M^{\top} \Sigma^{1/2} U \Sigma^{-1/2}}}\\
=& \E_u \lrb{\lrn{\Sigma^{-1/2} U^\top \Sigma^{1/2} M \Sigma^{1/2} U \Sigma^{1/2}}_{\Sigma, F}^2}
\end{align*}
On the other hand, by Jensen's inequality, 
\begin{align*}
& \E_u \lrb{\lrn{\Sigma^{-1/2} U^\top \Sigma^{1/2} M \Sigma^{1/2} U \Sigma^{1/2}}_{\Sigma, F}^2}\\
\geq& \lrn{\E_u \lrb{\Sigma^{-1/2} U^\top \Sigma^{1/2} M \Sigma^{1/2} U \Sigma^{1/2}}}_{\Sigma, F}^2\\
\geq& \lrp{\frac{1}{d} \sum_{j=1}^d \lrb{\Sigma^{1/2} M \Sigma^{1/2}}_{j,j}}^2.
\end{align*}
Thus $r_i \Sigma^{-1}$ is in fact the \emph{steepest descent} direction, wrt $\lrn{\cdot}_{\Sigma, F}$.}

\fi












\subsection{Proof of \autoref{t:L_layer_P_identity}}
The proof of \autoref{t:L_layer_P_identity} is similar to that of \autoref{t:L_layer_P_0}, and with a similar setup. However to keep the proof self-contained, we will restate the setup. Once again, we drop the factor of $\frac{1}{n}$ which was present in the original update \eqref{e:dynamics_Z}. This is because the constant $1/n$ can be absorbed into $A_i$'s. Doing so does not change the theorem statement, but reduces notational clutter.

Let us consider the reformulation of the in-context loss $f$ presented in \autoref{l:icl_trace_form}. Specifically, let $\overline{Z}_0$ be defined as 
\begin{align*}
\overline{Z}_0 = \begin{bmatrix}
\tx{1} & \tx{2} & \cdots & \tx{n} &\tx{n+1} \\ 
\ty{1} & \ty{2} & \cdots &\ty{n}& \ty{n+1}
\end{bmatrix} \in \R^{(d+1) \times (n+1)},
\end{align*}
where $\ty{n+1} = \lin{\wstar, \tx{n+1}}$. Let $\overline{Z}_i$ denote the output of the $(i-1)^{th}$ layer of the linear transformer (as defined in \eqref{e:dynamics_Z}, initialized at $\overline{Z}_0$). For the rest of this proof, we will drop the bar, and simply denote $\overline{Z}_i$ by $Z_i$.\footnote{This use of $Z_i$ differs the original definition in \eqref{d:Z_0}. But we will not refer to the original definition anywhere in this proof.} Let $X_i\in \R^{d\times n+1}$ denote the first $d$ rows of $Z_i$ and let $Y_i\in \R^{1\times n+1}$ denote the $(d+1)^{th}$ row of $Z_k$. Under the sparsity pattern enforced in \eqref{eq:full_attention}, we verify that, for any $i \in \lrbb{0,\dots,k}$,
\begin{align*}
& X_{i+1} =  X_{i}  + B_i X_i M X_{i}^\top A_i X_{i} \\
& Y_{i+1} =  Y_{i}  + Y_{i} M X_{i}^\top A_i X_{i} = Y_0 \prod_{\ell=0}^{i} \lrp{I + M X_\ell^T A_\ell X_\ell}.
\numberthis \label{e:t:XY_dynamics}
\end{align*}
We adopt the shorthand $A = \lrbb{A_i}_{i=0}^k$ and $B = \lrbb{B_i}_{i=0}^k$. Let $\S \subset \R^{2\times (k+1) \times d \times d}$, and $(A,B) \in \S$ if and only if for all $i\in \lrbb{0,\dots,k}$, there exists scalars $a_i,b_i \in \R$ such that $A_i = a_i \Sigma^{-1}$ and $B_i = b_i I$. Throughout this proof, we will work with the following formulation of the \emph{in-context loss} from \autoref{l:icl_trace_form}:
\begin{align*}
f(A,B) := \E_{(X_0,\wstar)} \lrb{\tr\lrp{\lrp{I-M}Y_{k+1}^\top Y_{k+1}\lrp{I-M}}}.
\numberthis \label{e:t:simpler_f}
\end{align*}
(note that the only randomness in $Z_0$ comes from $X_0$ as $Y_0$ is a deterministic function of $X_0$). The theorem statement is equivalent to the following:

\begin{align*}
\inf_{(A,B) \in \S} \sum_{i=0}^k \lrn{\nabla_{A_i} f(A,B)}_F^2 + \lrn{\nabla_{B_i} f(A,B)}_F^2 = 0
\numberthis \label{e:main_theorem_goal_PQ}
\end{align*}
where $\nabla_{A_i} f$ denotes derivative wrt the Frobenius norm $\lrn{A_i}_F$.

Our goal is to show that, if $(A,B) \in \S$, then for any $(R,S)\in \R^{2\times (k+1) \times d \times d}$, there exists $(\tilde{R},\tilde{S}) \in \S$, such that, at $t=0$,
\begin{align*}
\frac{d}{dt} f(A + t\tilde{R}, B + t \tilde{S}) \leq \frac{d}{dt} f(A + tR, B + tS).
\numberthis \label{e:t:wfoqmdksa:1}
\end{align*}
In fact, we show that $\tilde{R}_i := r_i I$, for $r_i = \frac{1}{d} \tr\lrp{\Sigma^{1/2} R_i \Sigma^{1/2}}$ and $\tilde{S}_i = s_i I$, for $s_i = \frac{1}{d} \tr \lrp{\Sigma^{-1/2} S_i \Sigma^{1/2}}$. This implies \eqref{e:main_theorem_goal_PQ} via the following simple argument: Consider the "$\S$-constrained gradient flow": let $A(t): \R^+ \to \R^{(k+1)\times d \times d}$ and $B(t): \R^+ \to \R^{(k+1)\times d \times d}$ be defined as 
\begin{align*}
& \frac{d}{dt} A_i(t) = - r_i(t) \Sigma^{-1}, \quad r_i(t) := \tr(\Sigma^{1/2} \nabla_{A_i} f (A(t),B(t)) \Sigma^{1/2})\\
& \frac{d}{dt} B_i(t) = - s_i(t) \Sigma^{-1}, \quad s_i(t) := \tr(\Sigma^{-1/2} \nabla_{B_i} f(A(t),B(t)) \Sigma^{1/2}),
\end{align*}
for $i=0,\dots,k$. By \eqref{e:t:wfoqmdksa:1}, we verify that
\begin{align*}
\frac{d}{dt} f(A(t),B(t)) \leq - \lrp{\sum_{i=0}^k \lrn{\nabla_{A_i} f(A(t),B(t))}_F^2 + \lrn{\nabla_{B_i}f(A(t),B(t))}_F^2}.
\numberthis \label{e:t:wfoqmdksa:4}
\end{align*}
We verify from its definition that $f(A,B) \geq 0$; if \eqref{e:main_theorem_goal_PQ} does not hold then \eqref{e:t:wfoqmdksa:4} will ensure unbounded descent as $t\to \infty$, contradicting the fact that $f(A,B)$ is lower-bounded. This concludes the proof.

\underline{\emphh{Proof outline.}}
The remainder of the proof will be devoted to showing \eqref{e:t:wfoqmdksa:1}, which we outline as follows:
\begin{itemize}[leftmargin=*]
\item In Step 1, we reduce the condition in \eqref{e:t:wfoqmdksa:1} to a more easily verified \emph{layer-wise} condition. Specifically, we only need to verify \eqref{e:t:wfoqmdksa:1} in one of the two cases: (I) when $R_i,S_i$ are all zero except for $R_j$ for some fixed $j$ (see \eqref{e:t:wfoqmdksa:3}), or (II) when $R_i,S_i$ are all zero except for $S_j$ for some fixed $j$ (see \eqref{e:t:wfoqmdksa:2}).

We focus on the proof of (II), as the proof of (I) is almost identical. At the end of Step 1, we set up some additional notation, and introduce an important matrix $G$, which is roughly "a product of attention layer matrices". In \eqref{e:t:rmlsdm:0}, we study the evolution of $f(A,B(t))$ when $B(t)$ moves in the direction of $S$, as $X_0$ is (roughly speaking) randomly transformed. This motivates the subsequent analysis in Steps 2 and 3 below.

\item In Step 2, we study how outputs of each layer \eqref{e:t:XY_dynamics} changes when $X_0$ is randomly transformed. There are two main results here: First we provide the expression for $X_i$ in \eqref{e:t:UX}. Second, we provide the expression for $\frac{d}{dt} X_i(B(t))$ in \eqref{e:t:key_induction}.

\item In Step 3, we use the results of Step 2 to to study $G$ (see \eqref{e:t:UG}) and $\frac{d}{dt} G(B(t))$ (see \eqref{e:t:rmlsdm:3}) under random transformation of $X_0$.

The idea in \eqref{e:t:rmlsdm:3} is that "randomly transforming $X_0$" has the same effect as "randomly transforming $S$" (recall $S$ is the perturbation to $B$). 

\item In Step 4, we use the results from Steps 2 and 3 to the expression of $\frac{d}{dt} f(A, B(t))$ in \eqref{e:t:rmlsdm:0}. We verify that $\tilde{S}$ in \eqref{e:t:wfoqmdksa:1} is exactly the expected matrix after "randomly transforming $S$". This concludes our proof of (II).

\item In Step 5, we sketch the proof of (I), which is almost identical to Steps 2-4. 
\end{itemize}


\underline{\emphh{1. Reduction to layer-wise condition.}}
To prove \eqref{e:t:wfoqmdksa:1}, it suffices to show the following simpler condition: Let $j\in \lrbb{0,\dots,k}$. Let $R_j, S_j \in \R^{d\times d}$ be arbitrary matrices. For $C\in \R^{d \times d}$, let $A(t C, j)$ denote the collection of matrices, where $A(t C, j)_j = A_j + t C$, and for $i\neq j$, $A(t C, j)_i = A_i$. Define $B(t C,j)$ analogously. We show that for all $j\in \lrbb{0,\dots,k}$ and all $R_j,S_j\in \R^{d\times d}$, there exists $\tilde{R}_j = r_j \Sigma^{-1}$ and $\tilde{S}_j = s_j \Sigma^{-1}$, such that, at $t=0$,
\begin{align*}
& \frac{d}{dt} f(A(t\tilde{R}_j, j), B) \leq \frac{d}{dt} f(A(tR_j,j), B)
\numberthis \label{e:t:wfoqmdksa:2}\\
\text{and} \qquad 
& \frac{d}{dt} f(A, B(t\tilde{S}_j,j)) \leq \frac{d}{dt} f(A, B(tS_j,j)).
\numberthis \label{e:t:wfoqmdksa:3}
\end{align*}
We can verify that \eqref{e:t:wfoqmdksa:1} is equivalent to \eqref{e:t:wfoqmdksa:2}+\eqref{e:t:wfoqmdksa:3} by noticing that for any $(R,S) \in \R^{2 \times (k+1) \times d\times d}$, at $t=0$, $\frac{d}{dt} f(A + tR, B + tS) = \sum_{j=0}^k \lrp{\frac{d}{dt} f(A(tR_j,j), B) + \frac{d}{dt} f(A, B(tS_j,j))}$. 

We will first focus on proving \eqref{e:t:wfoqmdksa:3} (the proof of \eqref{e:t:wfoqmdksa:2} is similar, and we present it in Step 5 at the end), for some index $j$ that is arbitrarily chosen but fixed throughout. Notice that $X_i$ and $Y_i$ in \eqref{e:t:XY_dynamics} are in fact functions of $A,B$ and $X_0$. For most of our subsequent discussion, $A_i$ (for all $i$) and $B_i$ (for all $i\neq j$) can be treated as constant matrices. We will however make the dependence on $X_0$ and $B_j$ explicit (as we consider the curve $B_j + t S$), i.e. we use $X_i(X, C)$ (resp $Y_i(X,C)$) to denote the value of $X_i$ (resp $Y_i$) from \eqref{e:t:XY_dynamics}, with $X_0 = X$, and $B_j = C$.

By \eqref{e:t:simpler_f} and \eqref{e:t:XY_dynamics},
\begin{align*}
& f(A, B(tS_j,j)) \\
=& \E \lrb{\tr\lrp{\lrp{I-M} Y_{k+1}(X_0, B_j+tS)^{\top} Y_{k+1}(X_0, B_j+tS_j) \lrp{I-M}}}\\
=& \E \lrb{\tr\lrp{\lrp{I-M} G(X_0, B_j + t S_j)^\top \wstar^\top \wstar G(X_0, B_j+tS_j) \lrp{I-M}}}\\
=& \E \lrb{\tr\lrp{\lrp{I-M} G(X_0, B_j + t S_j)^\top \Sigma^{-1} G(X_0, B_j + t S_j) \lrp{I-M}}}
\end{align*}
where $G(X, C) := X \prod_{i=0}^{k} \lrp{I - M X_i(X,C)^T A_i X_i(X,C)}$. The second equality follows from plugging in \eqref{e:t:XY_dynamics}. 

For the rest of this proof, let $U$ denote a uniformly randomly sampled orthogonal matrix. Let $\US:= \Sigma^{1/2} U \Sigma^{-1/2}$. Using the fact that $X_0 \overset{d}{=} \US X_0$, we can verify
\begin{align*}
& \at{\frac{d}{dt} f(A, B(tS_j,j))}{t=0}\\
=& \at{\frac{d}{dt}\E_{X_0} \lrb{\tr\lrp{\lrp{I-M} G(X_0, B_j + t S_j)^\top \Sigma^{-1} G( X_0, B_j + t S_j) \lrp{I-M}}}}{t=0}\\
=& \at{\frac{d}{dt}\E_{X_0, U} \lrb{\tr\lrp{\lrp{I-M} G(\US X_0, B_j + t S_j)^\top \Sigma^{-1} G(\US X_0, B_j + t S_j) \lrp{I-M}}}}{t=0}\\
=& 2\E_{X_0, U} \lrb{\tr\lrp{\lrp{I-M} G(\US X_0, B_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(\US X_0, B_j + t S_j)}{t=0} \lrp{I-M}}}.
\numberthis \label{e:t:rmlsdm:0}
\end{align*}


\underline{\emphh{2. $X_i$ and $\frac{d}{dt} X_i$ under random transformation of $X_0$.}} In this step, we prove that when $X_0$ is transformed by $\US$, $X_i$ for $i\geq 1$ are likewise transformed in a simple manner. The first goal of this step is to show
\begin{align*}
X_i(\US X_0, B_j) = \US X_i(X_0, B_j).
\numberthis \label{e:t:UX}
\end{align*}
We will prove this by induction. When $i=0$, this clearly holds by definition. Suppose that \eqref{e:t:UX} holds for some $i$. Then
\begin{align*}
& X_{i+1}(\US X_0, B_j)\\
=& X_i(\US X_0, B_j) + B_i X_i(\US X_0, B_j) M X_i(\US X_0, B_j)^T A_i X_i(\US X_0, B_j) \\
=& \US X_i(X_0, B_j) + \US B_i X_i(X_0, B_j) M X_i(X_0, B_j)^T A_i X_i(X_0, B_j)\\
=& \US X_{i+1} (X_0, B_j)
\end{align*}
where the second equality uses the inductive hypothesis, and the fact that $A_i = a_i \Sigma^{-1}$, so that $\US^T A_i \US = A_i$, and the fact that $B_i = b_i I$, from the definition of $\S$ and our assumption that $(A,B)\in \S$. This concludes the proof of \eqref{e:t:UX}.

We now present the second main result of this step. Let $\US^{-1} := \Sigma^{1/2} U^T \Sigma^{-1/2}$, so that it satisfies $\US \US^{-1}= \US^{-1} \US = I$. For all $i$,
\begin{align*}
\at{\US^{-1} \frac{d}{dt} X_i(\US X_0, B_j + t S_j)}{t=0} = \at{\frac{d}{dt} X_i(X_0, B_j + t \US^{-1} S_j \US)}{t=0}.
\numberthis \label{e:t:key_induction}
\end{align*}
To reduce notation, we will not write $\at{\cdot}{t=0}$ explicitly in the subsequent proof. We first write down the dynamics for the right-hand-side term of \eqref{e:t:key_induction}:
From \eqref{e:t:XY_dynamics}, for any $\ell \leq j$, and for any $i \geq j+1$, and for any $C\in \R^{d\times d}$,
\begin{align*}
& \frac{d}{dt} X_{\ell} \lrp{X_0, B_j + t C} = 0\\
& \frac{d}{dt} X_{j+1} \lrp{X_0, B_j + t C} = C {X_{j}\lrp{X_0, B_j}} M {X_{j}\lrp{X_0, B_j}}^{\top} A_j  {X_{j}\lrp{X_0, B_j}}\\
& \frac{d}{dt} X_{i+1} \lrp{X_0, B_j + t C}
= \frac{d}{dt} X_{i} \lrp{X_0, B_j + t C}\\
&\qquad\qquad\qquad\quad + B_i \lrp{\frac{d}{dt} X_{i}\lrp{X_0, B_j + t C}} M X_{i}\lrp{X_0, B_j}^{\top} A_i  X_{i}\lrp{X_0, B_j}\\
&\qquad\qquad\qquad\quad + B_i {X_{i}\lrp{X_0, B_j}} M \lrp{\frac{d}{dt} X_{i}\lrp{X_0, B_j + tC}}^{\top} A_i  X_{i}\lrp{X_0, B_j}\\
&\qquad\qquad\qquad\quad + B_i {X_{i}\lrp{X_0, B_j}} M {X_{i}\lrp{X_0, B_j}}^{\top} A_i  \lrp{\frac{d}{dt}X_{i}\lrp{X_0, B_j+tC}}
\numberthis \label{e:t:rmlsdm:2}
\end{align*}


We are now ready to prove \eqref{e:t:key_induction} using induction. For the base case, we verify that for $\ell \leq j$, $\US^{-1} \frac{d}{dt} X_{\ell} \lrp{\US X_0, B_k + tS_j} = 0 = \frac{d}{dt} X_{\ell} \lrp{X_0, B_j + t\US^{-1} S_j \US}$ (see first equation in \eqref{e:t:rmlsdm:2}). For index $j+1$, we verify that
\begin{align*}
& \US^{-1} \frac{d}{dt} X_{j+1} \lrp{\US X_0, B_j + t S_j} \\
=& \US^{-1} S_j \US X_j( X_0, B_j) M X_j(\US X_0, B_j)^\top A_j X_j(\US X_0, B_j)\\
=& \frac{d}{dt} X_{j+1} \lrp{X_0, B_j + t \US^{-1} S_j \US}
\numberthis \label{e:t:rmlsdm:4}
\end{align*}
where we use two facts:  1. $X_i(\US X_0, B_j) = \US X_i(X_0, B_j)$ from \eqref{e:t:UX}, 2. $A_i = a_i \Sigma^{-1}$, so that $\US^{\top} A_i \US = A_i$. We verify by comparison to the second equation in \eqref{e:t:rmlsdm:2} that $\US^{-1} \frac{d}{dt} X_{j} \lrp{\US X_0, B_j + tS_j} = 0 = \frac{d}{dt} X_{j} \lrp{X_0, B_j + t\US^{-1} S_j \US}$. These conclude the proof of the base case.

Now suppose that \eqref{e:t:key_induction} holds for some $i$. We will now prove \eqref{e:t:key_induction} holds for $i+1$. From \eqref{e:t:XY_dynamics},
{\allowdisplaybreaks
\begin{align*}
& \US^{-1} \frac{d}{dt} X_{i+1} \lrp{\US X_0, B_j + t S_j}\\
=& \US^{-1}\frac{d}{dt} \lrp{X_i\lrp{\US X_0, B_j + t S_j}}\\
&\qquad + \US^{-1} \frac{d}{dt}\lrp{B_i X_i\lrp{\US X_0, B_j + t S_j} M X_i\lrp{\US X_0, B_j + t S_j}^\top A_i X_i\lrp{\US X_0, B_j + t S_j}}\\
=& \US^{-1}\frac{d}{dt} \lrp{X_i\lrp{\US X_0, B_j + t S_j}}\\
&\qquad + \US^{-1} B_i \lrp{\frac{d}{dt}X_i\lrp{\US X_0, B_j + t S_j}} M X_i\lrp{\US X_0, B_j}^{\top} A_i X_i\lrp{\US X_0, B_j}\\
&\qquad + \US^{-1} B_i {X_i\lrp{\US X_0, B_j}} M \lrp{\frac{d}{dt}X_i\lrp{\US X_0, B_j + t S_j}}^{\top} A_i X_i\lrp{\US X_0, B_j}\\
&\qquad + \US^{-1} B_i {X_i\lrp{\US X_0, B_j}} M {X_i\lrp{\US X_0, B_j}}^{\top} A_i \lrp{\frac{d}{dt}X_i\lrp{\US X_0, B_j + t S_j}}\\
\overset{(i)}{=}&\US^{-1}\frac{d}{dt} {X_i\lrp{\US X_0, B_j + t S_j}}\\
&\qquad + B_i \lrp{\US^{-1}\frac{d}{dt}X_i\lrp{\US X_0, B_j + t S_j}} M X_i\lrp{X_0, B_j}^{\top} A_i X_i\lrp{X_0, B_j}\\
&\qquad + B_i {X_i\lrp{X_0, B_j}} M \lrp{\US^{-1} \frac{d}{dt}X_i\lrp{\US X_0, B_j + t S_j}}^{\top} A_i X_i\lrp{X_0, B_j}\\
&\qquad - B_i {X_i\lrp{X_0, B_j}} M {X_i\lrp{X_0, B_j}}^{\top} A_i \lrp{\US^{-1} \frac{d}{dt}X_i\lrp{\US X_0, B_j + t S_j}}\\
\overset{(ii)}{=}&\frac{d}{dt} {X_i\lrp{X_0, B_j + t \US^{-1} S_j \US}}\\
&\qquad + B_i \lrp{\frac{d}{dt}X_i\lrp{ X_0, B_j + t \US^{-1} S_j \US}} M X_i\lrp{X_0, B_j}^{\top} A_i X_i\lrp{X_0, B_j}\\
&\qquad + B_i {X_i\lrp{X_0, B_j}} M \lrp{\frac{d}{dt}X_i\lrp{ X_0, B_j + t \US^{-1} S_j \US}}^{\top} A_i X_i\lrp{X_0, B_j}\\
&\qquad + B_i {X_i\lrp{X_0, B_j}} M {X_i\lrp{X_0, B_j}}^{\top} A_i \lrp{\frac{d}{dt}X_i\lrp{ X_0, B_j + t \US^{-1} S_j \US}}
\numberthis \label{e:t:rmlsdm:1}
\end{align*}
}

In $(i)$ above, we crucially use the following facts: 1. $B_i = b_i I$ so that $\US^{-1} B_i = B_i \US^{-1}$, 2. $X_i(\US X_0, B_j) = \US X_i(X_0, B_j)$ from \eqref{e:t:UX}, 3. $A_i = a_i \Sigma^{-1}$, so that $\US^{\top} A_i \US = A_i$, 4. $\US\US^{-1} = \US^{-1} \US = I$. $(ii)$ follows from our inductive hypothesis. The inductive proof is complete by verifying that \eqref{e:t:rmlsdm:1} exactly matches the third equation of \eqref{e:t:rmlsdm:2} when $C = \US^{-1} S \US$.


\underline{\emphh{3. $G$ and $\frac{d}{dt} G$ under random transformation of $X_0$.}}
We now verify that $G(\US X_0, B_j) = \US G(X_0, B_j)$. This is a straightforward consequence of \eqref{e:t:UX} as
\begin{align*}
& G(\US X_0, B_j)\\
=& \US X_0 \prod_{i=0}^{k} \lrp{I + M X_i(\US X_0,B_j)^T A_i X_i(\US X_0,B_j)}\\
=& \US X_0 \prod_{i=0}^{k} \lrp{I + M X_i(X_0,B_j)^T A_i X_i(X_0,B_j)}\\
=& \US G(X_0, B_j),
\numberthis \label{e:t:UG}
\end{align*}
where the second equality uses \eqref{e:t:UX}, as well as the fact that $\US^\top A_i \US = A_i$. Next, we will show that
\begin{align*}
\US^{-1} \at{\frac{d}{dt} G(\US X_0, B_j + t S_j)}{t=0} = \at{\frac{d}{dt} G(X_0, B_j + t \US^{-1} S_j \US)}{t=0}.
\numberthis \label{e:t:rmlsdm:3}
\end{align*}
To see this, we can expand
{\allowdisplaybreaks
\begin{align*}
& \US^{-1} \frac{d}{dt} G(\US X_0, B_j + t S_j)\\
=& \US^{-1} \frac{d}{dt} \lrp{\US X_0 \prod_{i=0}^{k} \lrp{I + M X_i(\US X_0,B_j+tS_j)^T A_i X_i(\US X_0,B_j+tS_j)}}\\
=& X_0 \sum_{i=0}^k \lrp{\prod_{\ell=0}^{i-1} \lrp{I + M X_\ell(\US X_0,B_j)^T A_\ell X_i(\US X_0,B_\ell)}}\\
\cdot& M \frac{d}{dt} \lrp{X_i(\US X_0,B_j+tS_j)^T A_i X_i(\US X_0,B_j)}\\
\cdot& \lrp{\prod_{\ell=i+1}^{k} \lrp{I + M X_\ell(\US X_0,B_j)^T A_\ell X_i(\US X_0,B_\ell)}}\\
\overset{(i)}{=}& X_0 \sum_{i=0}^k \lrp{\prod_{\ell=0}^{i-1} \lrp{I + M X_\ell(X_0,B_j)^T A_\ell X_\ell(X_0,B_\ell)}}\\
\cdot& M \lrp{\lrp{\US^{-1} \frac{d}{dt}X_i(\US X_0,B_j+tS_j)}^T A_i X_i(X_0,B_j) + M {X_i(X_0,B_j)}^T A_i \lrp{\US^{-1} \frac{d}{dt}X_i(\US X_0,B_j+tS_j)}}\\
\cdot& \lrp{\prod_{\ell=i+1}^{k} \lrp{I + M X_\ell( X_0,B_j)^T A_\ell X_\ell( X_0,B_\ell)}}\\
\overset{(ii)}{=}& X_0 \sum_{i=0}^k \lrp{\prod_{\ell=0}^{i-1} \lrp{I + M X_\ell(X_0,B_j)^T A_\ell X_\ell(X_0,B_\ell)}}\\
\cdot& M \lrp{\lrp{\frac{d}{dt}X_i( X_0,B_j+t \US^{-1} S_j \US)}^T A_i X_i(X_0,B_j) + M {X_i(X_0,B_j)}^T A_i \lrp{\frac{d}{dt}X_i( X_0,B_j+t\US^{-1} S_j \US)}}\\
\cdot& \lrp{\prod_{\ell=i+1}^{k} \lrp{I + M X_\ell( X_0,B_j)^T A_\ell X_\ell( X_0,B_\ell)}}\\
\overset{(iii)}{=}& \frac{d}{dt} G(X_0, B_j + t \US^{-1} S_j \US)
\end{align*}
}
In $(i)$ above, we  the following facts: 1. $X_i(\US X_0, B_j) = \US X_i(X_0, B_j)$ from \eqref{e:t:UX}, 2. $A_i = a_i \Sigma^{-1}$, so that $\US^{\top} A_i \US = A_i$, 3. $\US\US^{-1} = \US^{-1} \US = I$. $(ii)$ follows from \eqref{e:t:key_induction}. $(iii)$ is by definition of $G$.


\underline{\emphh{4. Putting everything together.}}
Let us now continue from \eqref{e:t:rmlsdm:0}. We can now plug \eqref{e:t:UG} and \eqref{e:t:rmlsdm:3} into \eqref{e:t:rmlsdm:0}:
{\allowdisplaybreaks
\begin{align*}
& \at{\frac{d}{dt} f(A, B(tS_j,j))}{t=0}\\
=& 2\E_{X_0, U} \lrb{\tr\lrp{\lrp{I-M} G(\US X_0, B_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(\US X_0, B_j + t S_j)}{t=0} \lrp{I-M}}}\\
\overset{(i)}{=}& 2\E_{X_0, U} \lrb{\tr\lrp{\lrp{I-M} G( X_0, B_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(X_0, B_j + t \US^{-1} S_j \US)}{t=0} \lrp{I-M}}}\\
=& 2\E_{X_0} \lrb{\tr\lrp{\lrp{I-M} G( X_0, B_j )^\top \Sigma^{-1} \E_{U}\lrb{\at{\frac{d}{dt} G(X_0, B_j + t \US^{-1} S_j \US)}{t=0}} \lrp{I-M}}}\\
\overset{(ii)}{=}& 2\E_{X_0} \lrb{\tr\lrp{\lrp{I-M} G( X_0, B_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(X_0, B_j + t \E_U\lrb{\US^{-1} S_j \US} )}{t=0} \lrp{I-M}}}\\
=& 2\E_{X_0} \lrb{\tr\lrp{\lrp{I-M} G( X_0, B_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(X_0, B_j + t s_j I)}{t=0} \lrp{I-M}}}\\
=& \at{\frac{d}{dt} f(A, B(ts_j I,j))}{t=0}
\end{align*}
}
where $s_j := \frac{1}{d} \tr\lrp{\Sigma^{-1/2} S_j \Sigma^{1/2}}$. In the above, $(i)$ uses 1. \eqref{e:t:UG} and \eqref{e:t:rmlsdm:3}, as well as the fact that $\US^\top \Sigma^{-1} \US = \Sigma^{-1}$. $(ii)$ uses the fact that $\at{\frac{d}{dt} G(X_0, B_j + t C)}{t=0}$ is affine in $C$. To see this, one can verify from \eqref{e:t:rmlsdm:2}, using a simple induction argument, that $\frac{d}{dt} X_i(X_0, B_j + tC)$  is affine in $C$ for all $i$. We can then verify from the definition of $G$, e.g. using similar algebra as the proof of \eqref{e:t:rmlsdm:3}, that $\frac{d}{dt} G(X_0, B_j + C)$ is affine in $\frac{d}{dt} X_i(X_0, B_j + t C)$. Thus $\E_U\lrb{G(X_0, B_j + t \US^{-1} S_j \US)} = G(X_0, B_j + t \E_U\lrb{\US^{-1} S_j \US)}$.

With this, we conclude our proof of \eqref{e:t:wfoqmdksa:3}.

\underline{\emphh{5. Proof of \eqref{e:t:wfoqmdksa:2}.}}
We will now prove \eqref{e:t:wfoqmdksa:2} for fixed but arbitrary $j$, i.e. there is some $r_j$ such that
\begin{align*}
\frac{d}{dt} f(A(t\cdot r_j \Sigma^{-1},j), B) \leq \frac{d}{dt} f(A(tR_j,j), B).
\end{align*}
The proof is very similar to the proof of \eqref{e:t:wfoqmdksa:3} that we just saw, and we will essentially repeat the same steps from Step 2-4 above. 

Since we now consider perturbations to $A$ instead of to $B$, we will need to redefine some notation: let $X_i(X, C)$ (resp $Y_i(X,C)$) to denote the value of $X_i$ (resp $Y_i$) from \eqref{e:t:XY_dynamics}, with $X_0 = X$, and $A_j = C$ (previously it was with $B_j=C$). Let $G(X, A_j + C) := X \prod_{i=0}^{i} \lrp{I + M \lrp{X_i(X, A_j + C)^T A(C, j)_i X_i(X, A_j + C)}}$, where recall that $A(C,j) := A_j + C$, and $A(C,j)_\ell := A_\ell$ for all $\ell \in \lrbb{0...k} \backslash \lrbb{j}$.

We first verify that 
\begin{align*}
&X_i(\US X_0, A_j) = \US X_i(X_0, A_j)\\
&G(\US X_0, A_j) = \US G(X_0, A_j).
\numberthis \label{e:t:UXGA}
\end{align*}
The proofs are identical to the proofs of \eqref{e:t:UX} and \eqref{e:t:UG} so we omit them. Next, we show that for all $i$,
\begin{align*}
\at{\US^{-1} \frac{d}{dt} X_i(\US X_0, A_j + t R_j)}{t=0} = \at{\frac{d}{dt} X_i(X_0, A_j + t \US^{\top} R_j \US)}{t=0}.
\numberthis \label{e:t:reoqlk:0}
\end{align*}
We establish the dynamics for the right-hand-side of \eqref{e:t:reoqlk:0}:
{\allowdisplaybreaks
\begin{align*}
& \frac{d}{dt} X_{\ell} \lrp{X_0, A_j + t C} = 0\\
& \frac{d}{dt} X_{j+1} \lrp{X_0, A_j + t C} = B_j {X_{j}\lrp{X_0, A_j}} M {X_{j}\lrp{X_0, A_j}}^{\top} C  {X_{j}\lrp{X_0, A_j}}\\
& \frac{d}{dt} X_{i+1} \lrp{X_0, A_j + t C}
= \frac{d}{dt} X_{i} \lrp{X_0, A_j + t C}\\
&\qquad\qquad\qquad\quad+ B_i \lrp{\frac{d}{dt} X_{i}\lrp{X_0, A_j + t C}} M X_{i}\lrp{X_0, A_j}^{\top} A_i  X_{i}\lrp{X_0, A_j}\\
&\qquad\qquad\qquad\quad + B_i {X_{i}\lrp{X_0, A_j}} M \lrp{\frac{d}{dt} X_{i}\lrp{X_0, A_j + tC}}^{\top} A_i  X_{i}\lrp{X_0, A_j}\\
&\qquad\qquad\qquad\quad + B_i {X_{i}\lrp{X_0, A_j}} M {X_{i}\lrp{X_0, A_j}}^{\top} A_i  \lrp{\frac{d}{dt}X_{i}\lrp{X_0, A_j+tC}}
\numberthis \label{e:t:reoqlk:1}
\end{align*}
}

Similar to \eqref{e:t:rmlsdm:4}, we show that for $i\leq j$, 
\begin{align*}
\US^{-1} \frac{d}{dt} X_{i} \lrp{\US X_0, A_j + t R_j} 
=& 0 = \US^{-1} \frac{d}{dt} X_{i} \lrp{\US X_0, A_j + t \US R_j \US}
\end{align*}
and
\begin{align*}
&\US^{-1} \frac{d}{dt} X_{j+1} \lrp{\US X_0, A_j + t R_j} \\
=& \US^{-1} B_j \US X_j( X_0, A_j) M X_j(\US X_0, A_j)^\top A_j X_j(\US X_0, A_j)\\
=& \frac{d}{dt} X_{j+1} \lrp{X_0, A_j + t \US^{\top} R_j \US}.
\end{align*}
Finally, for the inductive step, we follow identical steps leading up to \eqref{e:t:rmlsdm:1} to show that
\begin{align*}
& \US^{-1} \frac{d}{dt} X_{i+1} \lrp{\US X_0, A_j + t R_j}\\
=&\frac{d}{dt} {X_i\lrp{X_0, A_j + t \US^{\top} R_j \US}}\\
&\qquad + B_i \lrp{\frac{d}{dt}X_i\lrp{ X_0, A_j + t \US^{\top} R_j \US}} M X_i\lrp{X_0, A_j}^{\top} A_i X_i\lrp{X_0, A_j}\\
&\qquad + B_i {X_i\lrp{X_0, A_j}} M \lrp{\frac{d}{dt}X_i\lrp{ X_0, A_j + t \US^{\top} R_j \US}}^{\top} A_i X_i\lrp{X_0, A_j}\\
&\qquad + B_i {X_i\lrp{X_0, A_j}} M {X_i\lrp{X_0, A_j}}^{\top} A_i \lrp{\frac{d}{dt}X_i\lrp{ X_0, A_j + t \US^{\top} R_j \US}}
\numberthis \label{e:t:reoqlk:2}
\end{align*}
The inductive proof is complete by verifying that \eqref{e:t:reoqlk:2} exactly matches the third equation of \eqref{e:t:reoqlk:1} when $C = \US^{-1} S \US$. This concludes the proof of \eqref{e:t:reoqlk:0}.

Next, we study the time derivative of $G(\US X_0, A_j + t R_j)$ and show that 
\begin{align*}
\US^{-1} \frac{d}{dt} G(\US X_0, A_j + t R_j)=& \frac{d}{dt} G(X_0, A_j + t \US^{\top} R_j \US).
\numberthis \label{e:t:reoqlk:3}
\end{align*}
This proof differs significantly from that of \eqref{e:t:rmlsdm:3} in a few places, so we provide the whole derivation below.  By chain-rule, we can write
\begin{align*}
\US^{-1} \frac{d}{dt} G(\US X_0, A_j + t R_j) = \spadesuit + \heartsuit
\end{align*}
where
\begin{align*}
\spadesuit := \US^{-1} \frac{d}{dt} \lrp{\US X_0 \prod_{i=0}^{k} \lrp{I + M X_i(\US X_0,A_j+tR_j)^T A_i X_i(\US X_0,A_j+tR_j)}}
\end{align*}
and 
\begin{align*}
\heartsuit :=& \US^{-1} \US X_0 \lrp{ \prod_{i=0}^{j-1} \lrp{I + M X_i(\US X_0,A_j)^T A_i X_i(\US X_0,A_j)}} \\
&\qquad \cdot M X_j(\US X_0,A_j)^T R_j X_j(\US X_0,A_j) \\
&\qquad \cdot \lrp{ \prod_{i=j+1}^{k} \lrp{I + M X_i(\US X_0,A_j)^T A_i X_i(\US X_0,A_j)}}.
\end{align*}

We will separately simplify $\spadesuit$ and $\heartsuit$, and verify at the end that summing them recovers the right-hand-side of \eqref{e:t:reoqlk:3}. We begin with $\spadesuit$, and the steps are almost identical to the proof of \eqref{e:t:rmlsdm:3}.

{\allowdisplaybreaks
\begin{align*}
& \spadesuit\\
=& \US^{-1} \frac{d}{dt} \lrp{\US X_0 \prod_{i=0}^{k} \lrp{I + M X_i(\US X_0,A_j+tR_j)^T A_i X_i(\US X_0,A_j+tR_j)}}\\
= & X_0 \sum_{i=0}^k \lrp{\prod_{\ell=0}^{i-1} \lrp{I + M X_\ell(\US X_0,A_j)^T A_\ell X_i(\US X_0,A_\ell)}}\\
\cdot& M \frac{d}{dt} \lrp{X_i(\US X_0,A_j+tR_j)^T A_i X_i(\US X_0,A_j + tR_j)} \\
\cdot& \lrp{\prod_{\ell=i+1}^{k} \lrp{I + M X_\ell(\US X_0,A_j)^T A_\ell X_i(\US X_0,A_\ell)}}\\
\overset{(i)}{=}& X_0 \sum_{i=0}^k \lrp{\prod_{\ell=0}^{i-1} \lrp{I + M X_\ell(X_0,A_j)^T A_\ell X_\ell(X_0,A_\ell)}}\\
\cdot& M \lrp{\lrp{\US^{-1} \frac{d}{dt}X_i(\US X_0,A_j+tR_j)}^T A_i X_i(X_0,A_j) + M {X_i(X_0,A_j)}^T A_i \lrp{\US^{-1} \frac{d}{dt}X_i(\US X_0,A_j+tR_j)}}\\
\cdot& \lrp{\prod_{\ell=i+1}^{k} \lrp{I + M X_\ell( X_0,A_j)^T A_\ell X_\ell( X_0,A_\ell)}}\\
\overset{(ii)}{=}& X_0 \sum_{i=0}^k \lrp{\prod_{\ell=0}^{i-1} \lrp{I + M X_\ell(X_0,A_j)^T A_\ell X_\ell(X_0,A_\ell)}}\\
\cdot& M \lrp{\lrp{\frac{d}{dt}X_i( X_0,A_j+t \US^{\top} R_j \US)}^T A_i X_i(X_0,A_j) + M {X_i(X_0,A_j)}^T A_i \lrp{\frac{d}{dt}X_i( X_0,A_j+t\US^{\top} R_j \US)}}\\
\cdot& \lrp{\prod_{\ell=i+1}^{k} \lrp{I + M X_\ell( X_0,A_j)^T A_\ell X_\ell( X_0,A_\ell)}}\\
=& X_0 \sum_{i=0}^k \lrp{\prod_{\ell=0}^{i-1} \lrp{I + M X_\ell(X_0,A_j)^T A_\ell X_\ell(X_0,A_\ell)}}\\
\cdot& M \frac{d}{dt} \lrp{X_i(X_0,A_j+t\US^{\top}R_j\US )^T A_i X_i(X_0,A_j + t \US^\top R_j \US)}\\
\cdot& \lrp{\prod_{\ell=i+1}^{k} \lrp{I + M X_\ell( X_0,A_j)^T A_\ell X_\ell( X_0,A_\ell)}}
\numberthis \label{e:t:spadesuit_end}
\end{align*}
}
In $(i)$ above, we  the following facts: 1. $X_i(\US X_0, B_j) = \US X_i(X_0, B_j)$ from \eqref{e:t:UXGA}, 2. $A_i = a_i \Sigma^{-1}$, so that $\US^{\top} A_i \US = A_i$, 3. $\US\US^{-1} = \US^{-1} \US = I$. $(ii)$ follows from \eqref{e:t:reoqlk:0}. 
%The main difference between the above and the proof of \eqref {e:t:rmlsdm:3} is in $(ii)$: we replace $\US^{-1} \frac{d}{dt}X_i(\US X_0,A_j+tS_j)$ by $\frac{d}{dt}X_i( X_0,A_j+t \US^{\top} R_j \US)$ using \eqref{e:t:reoqlk:3} (instead of replacing it by $\frac{d}{dt}X_i( X_0,B_j+t \US^{-1} S_j \US)$ as was done in \eqref{e:t:rmlsdm:3} using \eqref{e:t:key_induction}).

We will now simplify $\heartsuit$.
{\allowdisplaybreaks
\begin{align*}
& \heartsuit\\
=& \US^{-1} \US X_0 \lrp{ \prod_{i=0}^{j-1} \lrp{I + M X_i(\US X_0,A_j)^T A_i X_i(\US X_0,A_j)}} \\
&\qquad \cdot M X_j(\US X_0,A_j)^T R_j X_j(\US X_0,A_j) \\
&\qquad \cdot \lrp{ \prod_{i=j+1}^{k} \lrp{I + M X_i(\US X_0,A_j)^T A_i X_i(\US X_0,A_j)}}\\
\overset{(i)}{=}& X_0 \lrp{ \prod_{i=0}^{j-1} \lrp{I + M X_i(X_0,A_j)^T A_i X_i(X_0,A_j)}} M X_j(X_0,A_j)^\top \US^\top R_j \US X_j(X_0,A_j)\\
&\qquad \cdot \lrp{ \prod_{i=j+1}^{k} \lrp{I + M X_i(X_0,A_j)^T A_i X_i(X_0,A_j)}},
\numberthis \label{e:t:heartsuit_end}
\end{align*}
}
where $(i)$ uses the fact that $X_i(\US X_0, B_j) = \US X_i(X_0, B_j)$ from \eqref{e:t:UXGA} and the fact that $A_i = a_i \Sigma^{-1}$.

By expanding $\frac{d}{dt} G(X_0, A_j + t \US^{\top} R_j \US)$, we verify that 
\begin{align*}
\frac{d}{dt} G(X_0, A_j + t \US^{\top} R_j \US) = \eqref{e:t:spadesuit_end} + \eqref{e:t:heartsuit_end}  
= \spadesuit + \heartsuit
= \US^{-1} \frac{d}{dt} G(\US X_0, A_j + t R_j),
\end{align*}
this concludes the proof of \eqref{e:t:reoqlk:3}.

\iffalse
{\allowdisplaybreaks
\begin{align*}
& \US^{-1} \frac{d}{dt} G(\US X_0, A_j + t R_j)\\
=& \US^{-1} \frac{d}{dt} \lrp{\US X_0 \prod_{i=0}^{k} \lrp{I - M X_i(\US X_0,A_j+tR_j)^T A_i X_i(\US X_0,A_j+tR_j)}}\\
&\quad + \US^{-1} \US X_0 \lrp{ \prod_{i=0}^{j-1} \lrp{I - M X_i(\US X_0,A_j)^T A_i X_i(\US X_0,A_j)}} \\
&\qquad \cdot M X_j(\US X_0,A_j)^T R_j X_j(\US X_0,A_j) \lrp{ \prod_{i=j+1}^{k} \lrp{I - M X_i(\US X_0,A_j)^T A_i X_i(\US X_0,A_j)}}\\
\overset{(i)}{=}& X_0 \sum_{i=0}^k \lrp{\prod_{\ell=0}^{i-1} \lrp{I - M X_\ell(\US X_0,A_j)^T A_\ell X_i(\US X_0,B_\ell)}}\\
\cdot& M \frac{d}{dt} \lrp{X_i(\US X_0,A_j+tR_j)^T A_i X_i(\US X_0,A_j)} \lrp{\prod_{\ell=i+1}^{k} \lrp{I - M X_\ell(\US X_0,A_j)^T A_\ell X_i(\US X_0,B_\ell)}}\\
&\qquad + X_0 \lrp{ \prod_{i=0}^{j-1} \lrp{I - M X_i(X_0,A_j)^T A_i X_i(X_0,A_j)}} M X_j(X_0,A_j)^\top \US^\top R_j \US X_j(X_0,A_j)\\
&\qquad \cdot \lrp{ \prod_{i=j+1}^{k} \lrp{I - M X_i(X_0,A_j)^T A_i X_i(X_0,A_j)}}\\
\overset{(ii)}{=}& X_0 \sum_{i=0}^k \lrp{\prod_{\ell=0}^{i-1} \lrp{I - M X_\ell(X_0,A_j)^T A_\ell X_\ell(X_0,B_\ell)}}\\
\cdot& M \lrp{\lrp{\US^{-1} \frac{d}{dt}X_i(\US X_0,A_j+tR_j)}^T A_i X_i(X_0,A_j) + M {X_i(X_0,A_j)}^T A_i \lrp{\US^{-1} \frac{d}{dt}X_i(\US X_0,A_j+tR_j)}}\\
\cdot& \lrp{\prod_{\ell=i+1}^{k} \lrp{I - M X_\ell( X_0,A_j)^T A_\ell X_\ell( X_0,B_\ell)}}\\
&\qquad + X_0 \lrp{ \prod_{i=0}^{j-1} \lrp{I - M X_i(X_0,A_j)^T A_i X_i(X_0,A_j)}} M X_j(X_0,A_j)^\top \US^\top R_j \US X_j(X_0,A_j)\\
&\qquad \cdot \lrp{ \prod_{i=j+1}^{k} \lrp{I - M X_i(X_0,A_j)^T A_i X_i(X_0,A_j)}}\\
\overset{(iii)}{=}& X_0 \sum_{i=0}^k \lrp{\prod_{\ell=0}^{i-1} \lrp{I - M X_\ell(X_0,A_j)^T A_\ell X_\ell(X_0,B_\ell)}}\\
\cdot& M \lrp{\lrp{\frac{d}{dt}X_i( X_0,A_j+t \US^{\top} R_j \US)}^T A_i X_i(X_0,A_j) + M {X_i(X_0,A_j)}^T A_i \lrp{\frac{d}{dt}X_i(\US X_0,A_j+t\US^{\top} R_j \US)}}\\
\cdot& \lrp{\prod_{\ell=i+1}^{k} \lrp{I - M X_\ell( X_0,A_j)^T A_\ell X_\ell( X_0,B_\ell)}}\\
&\qquad + X_0 \lrp{ \prod_{i=0}^{j-1} \lrp{I - M X_i(X_0,A_j)^T A_i X_i(X_0,A_j)}} M X_j(X_0,A_j)^\top \US^\top R_j \US X_j(X_0,A_j)\\
&\qquad \cdot \lrp{ \prod_{i=j+1}^{k} \lrp{I - M X_i(X_0,A_j)^T A_i X_i(X_0,A_j)}}\\
\overset{(iv)}{=}& \frac{d}{dt} G(X_0, A_j + t \US^{\top} R_j \US)
\end{align*}
}
In $(ii)$ above, we  the following facts: 1. $X_i(\US X_0, B_j) = \US X_i(X_0, B_j)$ from \eqref{e:t:UX}, 2. $A_i = a_i \Sigma^{-1}$, so that $\US^{\top} A_i \US = A_i$, 3. $\US\US^{-1} = \US^{-1} \US = I$. $(iii)$ follows from \eqref{e:t:reoqlk:0}. $(iv)$ is because the preceding line is exactly $\frac{d}{dt} G(X_0, A_j + t \US^{\top} R_j \US)$.
\fi

%The key difference from \eqref{e:t:rmlsdm:3} are in steps $(i)$ and $(iii)$. In $(i)$, the chain-rule produces an extra term due to the appearance of $A_j$ directly in $G$ itself (in contrast $G$ only depends on $B_j$ via $X_i$'s). In $(iii)$, we replace $\US^{-1} \frac{d}{dt}X_i(\US X_0,B_j+tS_j)$ by $\frac{d}{dt}X_i( X_0,B_j+t \US^{\top} R_j \US)$ using \eqref{e:t:reoqlk:3} (instead of replacing it by $\frac{d}{dt}X_i( X_0,B_j+t \US^{-1} S_j \US)$ as was done in \eqref{e:t:rmlsdm:3} using \eqref{e:t:key_induction}).

The remainder of the proof is similar to what was done in \eqref{e:t:rmlsdm:0} in Step 4:
\begin{align*}
& \at{\frac{d}{dt} f(A(tR_j, j), B}{t=0}\\
=& 2\E_{X_0, U} \lrb{\tr\lrp{\lrp{I-M} G(\US X_0, A_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(\US X_0, A_j + t R_j)}{t=0} \lrp{I-M}}}\\
\overset{(i)}{=}& 2\E_{X_0,U} \lrb{\tr\lrp{\lrp{I-M} G( X_0, A_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(X_0, A_j + t \US^{\top} R_j \US)}{t=0} \lrp{I-M}}}\\
\overset{(ii)}{=}& 2\E_{X_0} \lrb{\tr\lrp{\lrp{I-M} G( X_0, A_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(X_0, A_j + t \E_U\lrb{\US^{\top} R_j \US} )}{t=0} \lrp{I-M}}}\\
=& 2\E_{X_0} \lrb{\tr\lrp{\lrp{I-M} G( X_0, A_j )^\top \Sigma^{-1} \at{\frac{d}{dt} G(X_0, A_j + t \cdot r_j \Sigma^{-1})}{t=0} \lrp{I-M}}}\\
=& \at{\frac{d}{dt} f(A(t\cdot r_j \Sigma^{-1},j), B)}{t=0},
\end{align*}
where $r_j := \frac{1}{d} \tr\lrp{\Sigma^{1/2} R_j \Sigma^{1/2}}$. In the above, $(i)$ uses 1. \eqref{e:t:UXGA} and \eqref{e:t:reoqlk:3}, as well as the fact that $\US^\top \Sigma^{-1} \US = \Sigma^{-1}$. $(ii)$ uses the fact that $\at{\frac{d}{dt} G(X_0, A_j + t C)}{t=0}$ is affine in $C$. To see this, one can verify using a simple induction argument, that $\frac{d}{dt} X_i(X_0, A_j + tC)$  is affine in $C$ for all $i$. We can then verify from the definition of $G$, e.g. using similar algebra as the proof of \eqref{e:t:reoqlk:3}, that $\frac{d}{dt} G(X_0, A_j + C)$ is affine in $\frac{d}{dt} X_i(X_0, A_j + t C)$ and $C$. Thus $\E_U\lrb{G(X_0, A_j + t \US^{\top} R_j \US)} = G(X_0, A_j + t \E_U\lrb{\US^{\top} R_j \US})$.

This concludes the proof of \eqref{e:t:wfoqmdksa:2}, and hence of the whole theorem.





\subsection{Equivalence under permutation} 

\begin{lemma}
\label{l:bar}
Consider the same setup as \autoref{t:L_layer_P_0}. Let $A = \lrbb{A_i}_{i=0}^k$, with $A_i = a_i \Sigma^{-1}$. Let 
\begin{align}
    f(A) := f \lrp{ \left\{ Q_i = \begin{bmatrix}
A_i & 0 \\ 
0 & 0
\end{bmatrix}, P_i = \begin{bmatrix}
0_{d\times d} & 0 \\ 
0 & 1 
\end{bmatrix}\right\}_{i=0}^k}.
\end{align}
Let $i,j \in \lrbb{0, \dots, k}$ be any two arbitrary indices, and let $\tilde{A}_i = A_j$, $\tilde{A}_j = A_i$, and let $\tilde{A}_{\ell} = A_\ell$ for all $\ell \in \lrbb{0,\dots ,k} \backslash \lrbb{i,j}$. Then $f(A) = f(\tilde{A})$
\end{lemma}
\begin{proof}
Following the same setup leading up to \eqref{e:t:oiemgrkwfdlw:0} in the proof of \autoref{t:L_layer_P_0}, we verify that the in-context loss is 
\begin{align*}
f(A) = \E \lrb{\tr\lrp{\lrp{I-M} G(X_0,A)^\top \Sigma^{-1} G(X_0,A)\lrp{I-M}}}
\end{align*}
where $G(X_0,A) := X_0 \prod_{\ell=0}^{k} \lrp{I + M X_0^T A_\ell X_0}$.

Consider any fixed index $\ell$. We will show that
\begin{align*}
\lrp{I + M X_0^T A_{\ell} X_0} \lrp{I + M X_0^T A_{\ell+1} X_0}
= \lrp{I + M X_0^T A_{\ell+1} X_0} \lrp{I + M X_0^T A_{\ell} X_0}.
\end{align*}
The lemma can then be proven by repeatedly applying the above, so that indices of $A_i$ and $A_j$ are swapped.

To prove the above equality, 
\begin{align*}
& \lrp{I + M X_0^T A_{\ell} X_0} \lrp{I + M X_0^T A_{\ell+1} X_0}\\
=& I + M X_0^T A_{\ell} X_0 + M X_0^T A_{\ell+1} X_0 + M X_0^T A_{\ell} X_0M X_0^T A_{\ell+1} X_0\\
=& I + M X_0^T A_{\ell} X_0 + M X_0^T A_{\ell+1} X_0 + M X_0^T a_{\ell} \Sigma^{-1} X_0M X_0^T a_{\ell+1} \Sigma^{-1} X_0\\
=& I + M X_0^T A_{\ell} X_0 + M X_0^T A_{\ell+1} X_0 + M X_0^T a_{\ell+1} \Sigma^{-1} X_0M X_0^T a_{\ell} \Sigma^{-1} X_0\\
=& \lrp{I + M X_0^T A_{\ell+1} X_0} \lrp{I + M X_0^T A_{\ell} X_0}.
\end{align*}
This concludes the proof. Notice that we crucially used the fact that $A_\ell$ and $A_{\ell+1}$ are the same matrix up to scaling.
\end{proof}

\section{Auxiliary Lemmas}

\subsection{Proof of \autoref{lem:express} (Equivalence to Preconditioned Gradient Descent)}
 \label{pf:express}
Consider fixed samples $\tx{1}, \dots, \tx{n}$, and fixed $\wstar$. Let $P=\lrbb{P_i}_{i=0}^k,Q=\lrbb{Q_i}_{i=0}^k$ denote fixed weights. Let $Z_i$ evolve as described in \eqref{eq:recursion}. Let $X_i$ denote the first $d$ rows of $Z_k$ (under \eqref{eq:sparse_attention}, $X_i=X_0$ for all $I$) and let $Y_i$ denote the $(d+1)^{th}$ row of $Z_i$. Let $g(x,y,k) : \R^d \times \R \times \mathbb{Z} \to \R$ be a function defined as follows: let $x^{n+1} = x$ and let $y^{n+1}_0 = y$, then $g(x,y,k) := y^{n+1}_k$. Note that $y^{n+1}_k = \lrb{Y_k}_{n+1}$.

We verify that, under \eqref{eq:sparse_attention}, the formula for updating $\ty{n+1}_k$ is given by
\begin{align*}
Y_{k+1}  = Y_{k}  - \frac{1}{n} Y_{k} M X_{0}^\top A_k X_{0}.
\end{align*}
where $M$ is a mask given by $\begin{bmatrix}
I & 0 \\0 & 0
\end{bmatrix}$. We can verify the following facts

\begin{enumerate}
\item $g(x,y,k) = g(x,0,k) + y$. To see this, notice first that for all $i \in \lrbb{1,\dots,n}$, 
$$\ty{i}_{k+1} = \ty{i}_{k} - \frac{1}{n} \sum_{j=1}^{n} {\tx{i}}^T A_k \tx{j} \ty{j}_k.$$ 
In other words, $\ty{i}_k$ does not depend on $\ty{n+1}_t$ for any $t$.  Next, for $\ty{n+1}_k$ itself, 
$$\ty{n+1}_{k+1} = \ty{n+1}_{k} - \frac{1}{n} \sum_{j=1}^{n} {\tx{n+1}}^T A_k \tx{j} \ty{j}_k,$$
which depends on $y^{n+1}_k$ only additively. We can verify under a simple induction that $g(x,y,k+1) - y = g(x,y,k) -y$.

\item $g(x,0,k)$ is linear in $x$. To see this, notice first that for $j \neq n+1$, $\ty{j}_k$ is does not depend on $\tx{n+1}_t$ for all $t,j,k$. Consequently, the update formula for $\ty{n+1}_{k+1}$ depends only linearly on $\tx{n+1}$ and $\ty{n+1}_k$. Finally, $\ty{n+1}_0 = 0$ is linear in $x$, so the conclusion follows by induction.
\end{enumerate}

With these two facts in mind, we verify that for each $k$, there exists a $\theta_k\in \R^d$, such that
\begin{align*}
g(x,y,k) = g(x,0,k) + y = \lin{\theta_k, x} + y
\end{align*}
for all $x,y$. It follows from definition that $g(x,y,0) = y$, so that $\lin{\theta_0,x} = g(x,y,0) - y = 0$, so that $\theta_0 = 0$. 

We now turn our attention to the third crucial fact: for all $i$, 
\begin{align*}
g(\tx{i},\ty{i},k) = \ty{i}_k = \lin{\theta_k, \tx{i}} + \ty{i}
\end{align*}
To see this, suppose that we let $\tx{n+1} := \tx{i}$ for some $i\in 1,\dots,n$. Then
\begin{align*}
& \ty{i}_{k+1} = \ty{i}_{k} - \frac{1}{n} \sum_{j=1}^{n} {\tx{i}}^T A_k \tx{j} \ty{j}_k\\
& \ty{n+1}_{k+1} = \ty{n+1}_{k} - \frac{1}{n} \sum_{j=1}^{n} {\tx{n+1}}^T A_k \tx{j} \ty{j}_k,
\end{align*}

thus $\ty{i}_{k+1} = \ty{n+1}_{k+1}$ if $\ty{i}_{k} = \ty{n+1}_{k}$, and the induction proof is completed by noting that $\ty{i}_{0} = \ty{n+1}_{0}$ by definition. Let $\bar{X} \in R^{d\times n}$ be the matrix whose columns are $\tx{1} ,\dots, \tx{n}$, leaving out $\tx{n+1}$. Let $\bar{Y}_k \in \R^{1\times n}$ denote the vector of $\ty{1}_k,\dots,\ty{n}_k$. Then it follows that
\begin{align*}
\bar{Y}_k = \bar{Y}_0 + \theta_k^T \bar{X}.
\end{align*}

Using the above fact, the update formula for $\ty{n+1}_k$ can be written as 
\begin{align*}
\ty{n+1}_{k+1} =& \ty{n+1}_{k} - \frac{1}{n} \lin{A_k X^\top Y_k, \tx{n+1}}\\
\Rightarrow \qquad 
\lin{\theta_{k+1}, \tx{n+1}} 
=& \lin{\theta_k, \tx{n+1}} - \frac{1}{n} \lin{A_k \bar{X} \lrp{\bar{X}^T \theta_k + \bar{Y}_0}, \tx{n+1}}\\
=& \lin{\theta_k, \tx{n+1}} - \frac{1}{n} \lin{A_k \bar{X} \lrp{\bar{X}^T \lrp{\theta_k + \wstar}}, \tx{n+1}}
\end{align*}

Since the choice of $\tx{n+1}$ is arbitrary, we get the more general update formula 
\begin{align*}
\theta_{k+1} = \theta_k - \frac{1}{n} A_k \bar{X} \bar{X}^T 
\lrp{\theta_k + \wstar}.
\end{align*}
We can treat $A_k$ as a preconditioner. Let $f(\theta):== \frac{1}{2n} \lrp{\theta+\wstar}^T \bar{X} \bar{X}^T (\theta+\wstar)$, then
\begin{align*}
\theta_{k+1} = \theta_k - \frac{1}{n} A_k \nabla f(\theta).
\end{align*}
Finally, let $\wgd_k := - \theta_k$. We verify that $f(-w) = R_{\wstar}(w)$, so that 
\begin{align*}
\wgd_{k+1} = \wgd_k - \frac{1}{n} A_k \nabla R_{\wstar}(\wgd_k).
\end{align*}

We also verify that for any $\tx{n+1}$, the prediction of $\ty{n+1}_k$ is 
\begin{align*}
g\lrp{\tx{n+1}, \ty{n+1}, k} = \ty{n+1} -\lin{\theta, \tx{n+1}} = \ty{n+1} + \lin{\wgd_k, \tx{n+1}}.
\end{align*}

This concludes the proof.



 
\subsection{Reformulating the in-context loss}
In this section, we will develop a re-formulation in-context loss, defined in \eqref{def:ICL linear}, in a more convenient form (see \autoref{l:icl_trace_form}).

For the entirety of this section, we assume that the transformer parameters $\lrbb{P_i,Q_i}_{i=0}^k$ are of the form defined in \eqref{eq:full_attention}, which we reproduce below for ease of reference:
\begin{align}
P_i = \begin{bmatrix}
B_i & 0 \\ 
0 & 1
\end{bmatrix}, \quad Q_i = \begin{bmatrix}
A_i & 0 \\ 
0 & 0
\end{bmatrix}.
\end{align}
Recall the update dynamics in \eqref{eq:recursion}, which we reproduce below:
\begin{align*}
Z_{i+1} = Z_{i} + \frac{1}{n}  P Z_i \aa Z_i^\top Q Z_i,
\numberthis \label{e:dynamics_Z}
\end{align*}
where $M$ is a mask matrix given by $M := \begin{bmatrix}I_{n\times n} & 0 \\ 0 & 0\end{bmatrix}$. Let $X_k \in \R^{d\times n+1}$ denote the first $d$ rows of $Z_k$ and let $Y_k \in \R^{1\times n+1}$ denote the $(d+1)^{th}$ (last) row of $Z_k$. Then the dynamics in \eqref{e:dynamics_Z} is equivalent to
\begin{align*}
& X_{i+1} = X_i + \frac{1}{n} B_i X_i M X_i^T A_i X_i\\
& Y_{i+1} = Y_i + \frac{1}{n}Y_i M X_i^T A_i X_i.
\numberthis \label{e:dynamics_XY}
\end{align*}

We present below an equivalent form for the in-context loss from \eqref{def:ICL linear}:

\begin{lemma}
\label{l:icl_trace_form}
Let $p_x$ and $p_w$ denote distributions over $\R^d$. Let $\tx{1},\dots,\tx{n+1} \overset{iid}{\sim} p_x$ and $\wstar \sim p_w$. Let $Z_0\in \R^{d+1\times n+1}$ be as defined in \eqref{d:Z_0}: 
\begin{align*}
Z_0 = \begin{bmatrix}
\tx{1} & \tx{2} & \cdots & \tx{n} &\tx{n+1} \\ 
\ty{1} & \ty{2} & \cdots &\ty{n}& 0
\end{bmatrix} \in \R^{(d+1) \times (n+1)}.
\end{align*}
Let $Z_k$ denote the output of the $(k-1)^{th}$ layer of the linear transformer (as defined in \eqref{e:dynamics_Z}, initialized at $Z_0$). Let $f\left(\{P_i, Q_i\}^{k}_{i=0}\right)$ denote the in-context loss defined in \eqref{def:ICL linear}, i.e.
\begin{align*}
f\left(\{P_i, Q_i\}^{k}_{i=0}\right) = \E_{(Z_0,\wstar)} \Bigl[ \left( [Z_{k}]_{(d+1),(n+1)} + \wstar^\top \tx{n+1}  \right)^2\Bigr].
\numberthis \label{e:old_icl}
\end{align*}

Let $\overline{Z}_0$ be defined as 
\begin{align*}
\overline{Z}_0 = \begin{bmatrix}
\tx{1} & \tx{2} & \cdots & \tx{n} &\tx{n+1} \\ 
\ty{1} & \ty{2} & \cdots &\ty{n}& \ty{n+1}
\end{bmatrix} \in \R^{(d+1) \times (n+1)},
\end{align*}
where $\ty{n+1} = \lin{\wstar, \tx{n+1}}$. Let $\overline{Z}_k$ denote the output of the $(k-1)^{th}$ layer of the linear transformer (as defined in \eqref{e:dynamics_Z}, initialized at $\overline{Z}_0$). Assume $\lrbb{P_i,Q_i}_{i=0}^k$ be of the form in \eqref{eq:full_attention}. Then the loss in \eqref{def:ICL linear} has the equivalent form
\begin{align*}
f\left(\{A_i, B_i\}^{k}_{i=0}\right) := f\left(\{P_i, Q_i\}^{k}_{i=0}\right) = \E_{(\overline{Z}_0,\wstar)} \lrb{\tr\lrp{\lrp{I-M}\overline{Y}_{k+1}^\top \overline{Y}_{k+1}\lrp{I-M}}},
\end{align*}
where $\overline{Y}_{k+1}\in\R^{1\times n+1}$ is the $(d+1)^{th}$ row of $\overline{Z}_k$.

\end{lemma}


Before proving \autoref{l:icl_trace_form}, we first establish an intermediate result (\autoref{l:additive_dependence_yn+1} below). To facilitate discussion, let us define a function $F_{X}\lrp{\lrbb{A_i,B_i}_{i=0}^k, X_0,Y_0}$ and $F_{Y}\lrp{\lrbb{A_i,B_i}_{i=0}^k, X_0,Y_0}$ to be the outputs, after $k$ layers of linear transformers respectively. I.e.
\begin{align*}
& F_{X}\lrp{\lrbb{A_i,B_i}_{i=0}^k, X_0,Y_0} = X_{k+1}\\
& F_{Y}\lrp{\lrbb{A_i,B_i}_{i=0}^k, X_0,Y_0} = Y_{k+1},
\end{align*}
as defined in \eqref{e:dynamics_XY}, given initialization $X_0,Y_0$.

We now prove a useful lemma showing that $\lrb{Y_0}_{n+1}=\ty{n+1}$ influences $X_i,Y_i$ in a very simple manner:
\begin{lemma}
\label{l:additive_dependence_yn+1}
Let $X_i,Y_i$ follow the dynamics in \eqref{e:dynamics_XY}. Then
\begin{enumerate}
\item $\lrb{X_i}$ is are independent of $\lrb{Y_0}_{n+1}$.
\item For $j\neq n+1$, $\lrb{Y_i}_{j}$ is independent of $\lrb{Y_0}_{n+1}$. 
\item $\lrb{Y_i}_{n+1}$ depends additively on $\lrb{Y_0}_{n+1}$.
\end{enumerate}

In other words, for $C := \lrb{0,0,0,\dots,,0,c} \in \R^{1\times (n+1)}$,
\begin{align*}
1:\ & F_{X}\lrp{\lrbb{A_i,B_i}_{i=0}^k, X_0,Y_0 + C} = F_{X}\lrp{\lrbb{A_i,B_i}_{i=0}^k, X_0,Y_0} \\
2 + 3:\ & F_{Y}\lrp{\lrbb{A_i,B_i}_{i=0}^k, X_0,Y_0 + C} = F_{Y}\lrp{\lrbb{A_i,B_i}_{i=0}^k, X_0,Y_0} + C
\end{align*}
\end{lemma}
\begin{proof}[Proof of \autoref{l:additive_dependence_yn+1}]
The first and second items follows directly from observing that the dynamics for $X_i$ and $Y_i$ in \eqref{e:dynamics_XY} do not involve $\lrb{Y_i}_{n+1}$, due to the effect of $M$.

The third item again uses the fact that $\lrb{Y_{i+1} - Y_i}_{n+1}$ does not depend on $\lrb{Y_{i}}_{n+1}$.
\end{proof}

We are now ready to prove \autoref{l:icl_trace_form}

\begin{proof}[Proof of \autoref{l:icl_trace_form}]
Let $Z_0$, $Z_k$, $\overline{Z}_0$, $\overline{Z}_k$ be as defined in the lemma statement. Let $\overline{X}_k$ and $\overline{Y}_k$ denote first $d$ rows and last row of $\overline{Z}_k$. Then by \autoref{l:additive_dependence_yn+1}, $\overline{X}_{k+1} = X_{k+1}$ and $\overline{Y}_{k+1} = Y_{k+1} + \begin{bmatrix}0 & 0 & \cdots & 0 & \lin{\wstar, \tx{n+1}}\end{bmatrix}$. Therefore, \eqref{e:old_icl} is equivalent to
\begin{align*}
& \E_{(\overline{Z}_0,\wstar)} \Bigl[ \left( [\overline{Z}_{k+1}]_{(d+1),(n+1)}  \right)^2\Bigr]\\
=& \E_{(\overline{Z}_0,\wstar)} \Bigl[ \left( [\overline{Y}_{k+1}]_{(n+1)}  \right)^2\Bigr]\\
=& \E_{(\overline{Z}_0,\wstar)} \lrb{\lrn{\lrp{I-M}\overline{Y}_{k+1}^\top}^2}\\
=& \E_{(\overline{Z}_0,\wstar)} \lrb{\tr\lrp{\lrp{I-M}\overline{Y}_{k+1}^\top \overline{Y}_{k+1}\lrp{I-M}}}.
\end{align*}
This concludes the proof. 
\end{proof}



 

\section{Additional experimental results}
\label{s:additional_plots}

In this section, we present a few addition experimental results. We first present in \autoref{fig:appendix_full} a visualization of learned weights $A_0,A_1,A_2$ for the setting of \autoref{t:L_layer_P_identity}.
One can see that the weight pattern matches the stationary point analyzed in \autoref{t:L_layer_P_identity}; hence, combining \autoref{fig:B0_B1} and \autoref{fig:appendix_full}, we corroborate our  results from \autoref{t:L_layer_P_identity}. Interestingly, it appears that the transformer implements a tiny gradient step using $X_0$ (as $A_0$ is small), and a large gradient step using $X_2$ (as $A_2$ is large). We believe that this is due to $X_2$ being better-conditioned than $X_1$, due to the effects of $B_0,B_1$. 
\begin{figure}[H] 
\centering
 \begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_adam_A0.pdf}
\caption{Visualization of $\Sigma^{1/2} A_0 \Sigma^{1/2}$} 
\label{fig:A0_PQ_app}
\end{subfigure}  
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_adam_A1.pdf}
\caption{Visualization of $\Sigma^{1/2} A_1 \Sigma^{1/2}$} 
\label{fig:A1_PQ_app}
\end{subfigure}  
\begin{subfigure}{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{camera-ready-figs/rotation_demonstration_adam_A2.pdf}
\caption{Visualization of $\Sigma^{1/2} A_2 \Sigma^{1/2}$} 
\label{fig:A2_PQ_app}
\end{subfigure}  
\caption{Visualization of learned weights  for the setting of \autoref{t:L_layer_P_identity}. One can see that the weight pattern matches the stationary point analyzed in \autoref{t:L_layer_P_identity}.   }
\label{fig:appendix_full}
\end{figure}

We next present some additional experiments that investigates the properties of the learned predictors of various algorithms.  First, we plot  the \textbf{test losses against the number of examples provided in the prompt} (``the number of ICL examples''). We compare four different algorithms: (i) the predictor learned by a three-layered of linear transformer, (ii) three steps of GD, (iii)  three steps of preconditioned GD, and (iv) the ordinary least-squared solution (OLS). For GD and preconditioned GD, the optimal stepsizes are found by gridsearch. For preconditioned GD, preconditioner is fixed to be $\Sigma^{-1}$ for comparison. In all cases, the dimension $d=5$, and for each $N$, the linear Transformer is trained using Adam. The result is presented in \autoref{fig:variable-N-plot}.
\begin{figure}[H]
\centering 
\includegraphics[width=0.35\textwidth]{camera-ready-figs/3-step-variable-N-plot.pdf}
\caption{Test loss comparison between (i) the predictor learned by a three-layered of linear transformer, (ii) three steps of GD, (iii)  three steps of preconditioned GD, and (iv) the ordinary least-squared solution (OLS).} 
\label{fig:variable-N-plot} 
\end{figure}
Lastly, in \autoref{fig:variable-L-plot},  we plot the \textbf{test losses against the number of layer $L$} (or the number of steps in the case of gradient-based algorithms).
For $L=1,2,3,4$, we compare between (i) the predictor learned by $L$-linear transformer and (i) $L$-steps of GD, (ii) $L$-steps of preconditioned GD. Again, the optimal stepsize is found by gridsearch, and for preconditioned GD, the preconditioner is fixed to be $\Sigma^{-1}$. In all cases, the dimension $d=5$, and context length $N=20$. The linear transformer is trained with Adam.
\begin{figure}[H]
\centering 
\includegraphics[width=0.4\textwidth]{camera-ready-figs/variable-L-plot.pdf}
\caption{Test loss comparison between (i) the predictor learned by a $L$-layered linear transformer and (i) $L$-steps of GD, (ii) $L$-steps of preconditioned GD, for $L=1,2,3,4$. } 
\label{fig:variable-L-plot} 
\end{figure}

\newpage 

\end{document}