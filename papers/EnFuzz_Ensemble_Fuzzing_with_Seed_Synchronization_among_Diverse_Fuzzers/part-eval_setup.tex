Firstly, we evaluate ensemble fuzzing on LAVA-M \cite{dolan2016lava}, which consistis of four buggy programs, file, base64, md5sum and who.  LAVA-M is a test suite that injects hard-to-find bugs in Linux utilities to evaluate bug-finding techniques. Thus the test is adequate for demonstrating the effectiveness of ensemble fuzzing.
Furthermore, to reveal the practical performance of ensemble fuzzing, we also evaluate our work based on fuzzer-test-suite \cite{fuzzer_test_suite}, a widely used benchmark from Google. The test suite consists of popular open-source real-world applications.
%including well-known tools (e.g. libarchive, sqlite), 
%image processing libraries (e.g. guetzli, libjpeg, libpng),  
%communication protocol toolkits  (e.g. openssl, boringssl), 
%color management engines (e.g. lcms),
%text engines (e.g. freetype2, harfbuzz),
%regular expression engines (e.g. proj4, re2),
%and document processers (e.g. json, libxml2), etc.
This benchmark is chosen to avoid the potential bias of the cases presented in literature, and for its great diversity, which helps demonstrate the performance variation of existing base fuzzers. 


We refer to the kernel criteria and settings of evaluation from the fuzzing guidelines \cite{klees2018evaluating}, and integrate the three widely used metrics from previous literature studies to compare the results on these real-world applications more fairly, including the number of paths, branches and unique bugs. 
%As described in \cite{klees2018evaluating}, the number of unique crashes  can overestimate the number of bugs significantly, so we choose to unique bugs.
To get unique bugs, we use crash's stack backtraces to deduplicate unique crashes, as mentioned in the previous subsection. % as described in \cite{klees2018evaluating}. 
%Note that AFL-based fuzzers distinguish the crash by execution paths, therefore, some crashes could be caused by an identical root cause. But in general, the more crashes we find, the higher probability that more vulnerabilities can be identified. The number of unique crashes indicates the probability of finding vulnerabilities of target applications. Even we can get the better ability indication metrics such as unique bugs in the future as described in \cite{klees2018evaluating}, the unique crash might be kept as an important metric. 
The initial seeds for all experiments are the same. We use the test cases originally included in their applications or empty seed if such initial seeds do not exist. %The first two metrics can evaluate the coverage of the target applications, as coverage is one of the most important factors contributing to the success of fuzzers.
%In addition to coverage information, another important factor to fuzzers is the number of unique crashes detected. 


The experiment on fuzzer-test-suite is conducted ten times in a 64-bit machine with 36 cores (Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz), 128GB of main memory, and Ubuntu 16.04 as the host OS with SMT enabled. Each binary is hardened by AddressSanitizer \cite{SanitizerCoverage} to detect latent bugs.
First, we run each base fuzzer for 24 hours with one CPU core in single mode.
Next, since \toolTwo, ~\toolThree ~and \toolFive ~need at least four CPU cores to ensemble these four base fuzzers, we also run each base fuzzer in parallel mode for 24 hours with four CPU cores. 
In particular, \toolOne ~and \toolFour ~only ensembles three types of base fuzzers (AFL, AFLFast and FairFuzz). To use the same resources, we set up two AFL instances, one AFLFast instance and one FairFuzz instance.
This experimental setup ensures that the computing resources usage of each ensemble fuzzer is the same as any base fuzzers running in parallel mode.
Due to the large amount of data and the page limitation, we include the variation of those statistical test results in our GitHub. In fact. most metrics converged to similar values during multithreaded fuzzing. The variation of those statistical test results is small (between -5\% ~ 5\%), so we just use the averages in this paper.
