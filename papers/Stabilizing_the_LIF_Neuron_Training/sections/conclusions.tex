
\section{Discussion and Conclusions}



Our method based on stabilizing forward and backward pass, resulted in improved accuracy over the baseline and it was able to predict optimal dampening, sharpness and tail-fatness before training. 
Our findings are coherent with the line of research that has established that stabilizing gradients and representations at initialization results in better performance \cite{glorot2010understanding, orthogonal_initialization, he2015delving, roberts2022principles, defazio2022scaling, bengio1994learning, hochreiter1997long, hochreiter2001gradient, arjovsky2016unitary, pascanu2013difficulty}. Moreover it gives an initial reply to the question raised by
\cite{surrogate2019, zenke2021remarkable}, which asked  for a theoretical justification of initialization and SG choice for Spiking Neural Networks. With a similar intention, \cite{rossbroich2022fluctuation} proposed an approach that guarantees sparsity of activity at initialization to pick the weights distribution at initialization, resulting in improved accuracy. Our method differs from theirs in that it starts from a principle of stability to derive constraints, instead of a principle of sparsity. It differs also in that we use it to define the SG shape at initialization, not only the weights distribution, and we can show mathematically how weights initialization is intertwined to the SG shape choice. Our results suggest that a tedious hyper-parameter grid-search can be often avoided by making use of sound and established principles of learning.

One of the conditions was designed to hit the most sensitive part of an SG, its center, which resulted in a low sparsity requirement at initialization. This is very uncommon in the Neuromorphic literature, since sparsity brings large energy gains \cite{henderson2020towards,blouw2019benchmarking, 9395703,taulsnn, rossbroich2022fluctuation}.
However, the energy gains of SNNs also come from their binary activity. A matrix-vector multiplication, with a $\mathbb{R}^{m\times n}$ matrix, has an energy cost of $mnE_{MAC}$ for a real vector, and of $mn\rho E_{AC}$ for a binary vector, where $\rho$ is the Bernouilli probability of the binary vector, and in our case the neuron firing rate, and $E_{AC}, E_{MAC}$ are the energies of an accumulate and a multiply-accumulate operation \cite{yin2021accurate, hunger2005floating}. Since MAC are more costly than AC, 31 times on a $45$nm complementary metal–oxide–semiconductor \cite{yin2021accurate, horowitz20141}, we have energy savings with any $\rho$, e.g., when all neurons fire ($\rho=1$) and when they fire half of the time steps ($\rho=1/2$). This gain does not depend on the simulation speed, since it compares a spiking and an analogue computation, at the same computation speed.
Typically requiring more sparsity through a sparsity encouraging loss term, leads to a measurable decrease in performance \cite{zenke2021remarkable, rossbroich2022fluctuation}. However we observed that it is actually possible to achieve higher performance with higher sparsity, by starting with a strong firing rate at initialization, since their synergy acts as a regularization mechanism. This was possible also because the sparsity encouraging loss term was introduced gradually, and because its contribution was kept comparable to the task loss towards the end of training.

We observed that the more complex the task is and the more complex the network to train is, the more drastic is the difference in performance of different SG shapes. It is known that learning is possible with a wide variety of SG shapes \cite{zenke2021remarkable} and the community has not yet settled for one shape or one method to reliably choose which SG to use in each case \cite{surrogate2019}. We showed how to apply a well known stability principle to the forward and backward pass of the simplest Spiking Neural Network, the LIF, as a starting point, but we think that the principles of good Neuromorphic initialization can be further elaborated, in order to tackle more complex tasks and networks.


