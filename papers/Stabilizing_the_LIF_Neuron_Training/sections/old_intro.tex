

\section{Introduction}


% \textbf{Take Home Message:} spiking networks are sensitive to surrogate gradient choice, how to choose the best

% Neuromorphic computing has been paving the road for energy efficient Artificial Intelligence, and the techniques are mature enough to often achieve large energy savings without loss of performance \cite{TAVANAEI201947, kundu2021spike}. 

% One of the neuromorphic techniques is to  leverage a binary activity of zeros and ones referred to as spiking activity, feature that resembles the spiking activity of biological brains. 
% Previous work has used variants of the Leaky Integrate and Fire neuron \cite{lapique1907recherches, lsnn} where the spike is produced when the variable that represents the voltage of the neuron crosses a dynamic threshold. 
% To stress the biological plausibility, constants are often fixed to have biological values, which does not seem necessary whenever the sole objective is improved performance with the energy efficiency constraint. Those constants could simply be learned to improve performance in a chosen task.

\orange{We want to define a systematic method to pass a gradient through a non differentiable spiking process to allow us to minimize the need of extensive hyper-parameter search.}
The exceptional success of deep learning comes at the cost of high energy consumption \cite{henderson2020towards} and solving this problem is one of the objectives of neuromorphic research \cite{blouw2019benchmarking, 9395703}. 
% One tool in the neuromorphic arsenal 
A simple strategy is to turn deep learning neurons into spiking neurons, to encourage sparse activity, i.e., representations that are formed with many zeros and very few ones. That results in networks that can be off for the most part, with the subsequent reduction in energy consumption.

The firing process of a spiking neuron is typically defined as a non-differentiable operation for computational efficiency \cite{lapique1907recherches,izhikevich2003simple}. Even if biological spikes are smooth voltage curves, they are often approximated with delta functions, that are produced when the neuron voltage surpasses a threshold, then the voltage is reset to an equilibrium value. Since the neuron fires \textit{if} the voltage is above threshold, and does not fire when the voltage is below the threshold, this \textit{if} operation can be mathematically represented with a Heaviside function.
However, the derivative of a Heaviside is a delta function, so a gradient that passes through a Heaviside will be zero most of the time, which makes training spiking networks with gradient descent harder. For this reason research has focused on smoothing that delta function into a shape that could pass a non-zero gradient more often \cite{esser2016convolutional, zenke2018superspike, lsnn}. These approximate gradients, usually referred to as surrogate gradients (SG), enable spiking networks to take advantage of the theory and practice of deep learning, backpropagation and gradient descent.  Evidence suggests that training spiking networks with SG can be robust to the choice of SG shape \cite{zenke2021remarkable}, but the literature on the subject is limited and a theory of SG choice and initialization is needed.

SG training can be defined as robust
if a comparable performance across SG choices is observed, after a careful hyper-parameter search that depends on the SG choice.
% , SG training is defined as robust.
% If a comparable performance across SG choices is observed, after a careful hyper parameter search that depends on the SG choice, SG training is defined as robust.
% that for any SG an optimal set of hyper-parameters can be found to achieve comparable performance across SG. 
This hyper-parameter search can be highly time and energy consuming, and methods to reduce such a search are of interest. 
Without an extensive hyper-parameter search, the performance does typically show sizeable changes for different SG, which makes training sensitive to SG choice.
% If instead the hyper-parameter choice is not carefully tuned to the SG selected, training seems to be very sensitive to SG choice.
In fact, different aspects of this sensitivity are of interest, such as the sensitivity to the network architecture, to the task, to the SG shape, and to the initialization scheme. 
We test the SG training sensitivity first against more types of spiking neuron models and different tasks, to show that not every choice of SG is likely to successfully train a network. 
We then focus on the Leaky Integrate and Fire (LIF) neuron model \cite{lapique1907recherches}, one of the simplest models of a spiking neuron, and we train its parameters through SG.
We look closely at three properties of an SG curve: its height (dampening), its width (sharpness), and how it decays at infinity (tail-fatness). This shows how  fine SG details can determine high or low accuracy after SG training.
% At the same time we slightly change the height and width of the SG to show how sensitive each task and network are to the SG choice. Secondly we explore in detail the performance against the fatness of the tail that characterizes the SG to test \textcolor{NiceOrange}{how sensitive} SG training is \textcolor{NiceOrange}{to} different SG tails. 
We also show that weight initialization schemes have a different impact on the final performance of each SG, which further supports the hypothesis that training is sensitive to SG choice. Therefore, a theory that allows to side step the high cost of extensive hyper-parameter search, should be easy to adapt to different architectures and task definitions. A useful theory should also be able to determine choices of dampening, sharpness, tail-fatness and initialization, that are task and network dependent.

% These three stress tests involve training the network from the beginning using SG. However, to adapt deep learning models to neuromorphic chips, it is common to change the original activation functions for SG Heavisides on a pretrained model, and fine-tune the model with the new activation \cite{Rathi2020Enabling, deng2021optimal,wu2019direct, conversion_annsnn, snnopp}. Therefore the weights go through a training phase where conventional neurons are used, to another with spiking neurons.
% We call this process \textit{conventional2spike}. This is often done since conventional neurons can pass exact gradients, while SG approximate them, in principle worsening the transmission of the same information about the loss.
% We consider the effect of SG choice on the transition from conventional neurons to spiking neurons during training, and we observe that \textit{conventional2spike} is sensitive to SG choice. This sensitivity, if not prevented with a theory, can lead to unreliable performance, and extensive and costly hyper parameter search.

Following the variance stability method to initialize linear and ReLU feed-forward networks proposed in \cite{glorot2010understanding, he2015delving}, we construct a method for recurrent spiking networks regularization that gives conditions on initialization and SG choice. These conditions provide constraints on the hyper-parameters, that depend on the particular network and task at hand, and can be met by tuning the SG curve. The method keeps the variance of network representations and gradients from exploding or vanishing exponentially with depth, by providing proper initialization.
% In the case of linear and ReLU networks, the variance stability criterion on the feed-forward and backward passes provide similar conditions on the variance of the weight initialization: \cite{glorot2010understanding} takes the mean variance that results form the analysis of both passes, while \cite{he2015delving} takes the variance that results from the feed-forward analysis. 
In spiking recurrent networks, the Heaviside activation already keeps the forward pass activation variance from exploding exponentially, since a Bernoulli process of spiking activity can only achieve a maximal variance of $1/4$. However, the backward pass variance stability criterion provides us a method for SG choice.

Our contribution to the search for the best SG shape is four-fold:

% \begin{itemize}
%     \itemsep0em 
%     \item we observe that the optimal SG shape, depends on the network and task of choice;
%     \item that SG training is sensitive to SG dampening, sharpness and tail fatness;
%     \item that performance ranking of different SG depends on the initialization scheme;
%     % \item that SG choice affects the transition from conventional to spiking neurons;
%     \item we provide a method for SG  choice based on bounding representations and gradients.
% \end{itemize}


\begin{itemize}
    \itemsep0em 
    \item We observe that the derivative of the fast sigmoid outperforms other SG choices across tasks and spiking networks;
    \item Low dampening, high sharpness and low tail fatness, lead to higher accuracy on the LIF network;
    \item Orthogonal initialization  leads to higher accuracy on the LIF network;
    \item We provide a theoretical method for SG choice based on bounding representations and gradients that improves experimental performance.
\end{itemize}