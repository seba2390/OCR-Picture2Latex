% \newpa    ge
\section{Related Work}



\textbf{Spiking Neural Networks.} Computationally, the simplest attempt to reproduce a biological neuron spiking activity is constituted by the  Leaky-Integrate-and-Fire (LIF) neurons \cite{lapique1907recherches}. When extended with an adaptive dynamic threshold, it is known to be able to reproduce
qualitatively all major classes of neurons, as defined electrophysiologically in vitro  \cite{izhikevich2003simple}, and to reproduce up to 96\% precision the spiking time of more complex and detailed models of neurons \cite{brette2005adaptive}.
Non-binary spiking activity networks, such as the Spiking Neural Unit, are able to surpass LSTM in language modeling \cite{wozniak2020deep}.
% Removing the voltage reset after firing  \cite{Han_2020_CVPR}, weight sharing across stacks of SNN \cite{toward2020}, and initializing them from a non spiking counterpart \cite{Rathi2020Enabling}, are some of the directions proven to be effective.

\noindent\textbf{Surrogate Gradients.} 
Models of biologically plausible learning rules are local \cite{7966072, FELDMAN2020127}, in the sense that they involve only interchange of chemicals between a pre-synaptic and a post-synaptic neuron, and there is no straight-forward notion of a loss to be minimized, and task and architecture are not considered in the learning rule. Global signals exist, in the form of neuromodulation, but are slower \cite{fremaux2016neuromodulated, legenstein2008learning, izhikevich2007solving}. The effort to reconcile biologically plausible learning rules with gradient descent is ongoing and fruitful.
One of the first successful instances of training of feed forward spiking networks with surrogate gradients was conducted by \cite{bohte2002error}, known as SpikeProp. 
There is no one established choice of surrogate gradients \cite{surrogate2019}, and evidence suggests that learning is robust to that choice \cite{zenke2021remarkable}. \cite{spikingbohte} have trained the network assuming no Heaviside function during training, and introduced it only at test time, while \cite{courbariaux2016binarized} had the Heaviside during training, but assumed no Heaviside for the backward pass. \cite{lsnn} substituted the delta gradient with a triangular linearization, \cite{zenke2018superspike} with the derivative of a fast sigmoid, and \cite{zenke2021remarkable} with the derivative of a sigmoid. \cite{yin2021accurate} investigate hybrid architectures, with layers of neurons that need a surrogate gradient and layers of neurons that are non-spiking.




