


\begin{figure}
    {\footnotesize \hspace{-2.5cm}(a)\hspace{3.cm}(b)
    % \hspace{.5cm}(c)
    }
    \centering
    \includegraphics[width=.5\textwidth]{images/surrogate_grads/pseudods.pdf}
    \includegraphics[width=.47\textwidth]{images/surrogate_grads/legend_cols3.pdf}
    
    % \includegraphics[width=.36\textwidth]{images/surrogate_grads/distributions.pdf}
    \caption{\textbf{Surrogate Gradient shapes.} To stabilize a network we have to stabilize the backward pass. However the LIF, as a spiking neuron, has an undefined backward pass and we need Surrogate Gradients (SG) to approximate it. Panel (a) shows the SG investigated in this work, and (b) the tail dependence of our $q$-PseudoSpike SG for $q\in[1.01, 16.85]$. The SG considered are symmetrical around $v_t=y_t-\vartheta=0$, so we only plot half the curve (centered voltage $v_t>0$). 
    % Panel (c) shows the weight sampling distributions used, Gaussian (dashed), Uniform (dotted) and BiGamma (solid), with \textit{He}, orange. and \textit{Glorot} initialization, green, for a weight shape of $(n_{0}, n_{1})=(200, 300)$.
    }
    \label{fig:methodo}
\end{figure}

% \newpage
% \clearpage
\section{Methods}


% representation and gradient stability
\subsection{Representation and Gradient Stability}
The earliest form of the stability analysis in the study of neural networks is often attributed to \cite{hochreiter1991untersuchungen} who identified that the tendency of gradients to compose exponentially with depth and time was behind the difficulty in training classical recurrent neural networks. For this reason this problem is often named the Exploding and Vanishing Gradient Problem (EVGP). However this name does not emphasize the need to stabilize the forward pass, and the intermediate tensors often referred to as representations. This is probably because infinitesimally, the representations are well described by the gradient, the backward pass, which is a fair justification for fully differentiable neuron models. However, for non-differentiable neuron models, the connection between forward and backward pass is not as simple to make.
Probably the most well known results of this line of research are the Glorot and He initializations ~\cite{glorot2010understanding, he2015delving}, for linear and ReLU fully-connected feed-forward networks, who provide the exact mean and variance that the connection matrices need to have at initialization to avoid the exponential composition of the gradients. By requiring the mean of the representations to remain equal to zero and variances equal to one per layer, the gradients are guaranteed not to compose exponentially with depth. 

However, this type of analysis has never been done before for spiking neural networks. Instead, theoretical justification for recurrent networks initialization has been proposed for the LSTM \cite{lstm_initialization}, and other non spiking recurrent networks \cite{hochreiter2001gradient, arjovsky2016unitary, pascanu2013difficulty}. 
% Arguments such as those used in the Echo State Network \cite{JAEGER2007335} do not apply to non convex activations, or activations without a slope one regime, such as those used in spiking neurons. 
In practice,  \cite{zenke2021remarkable} samples a $Var[W_l]=1/3n_{l-1}$ Uniform, while \cite{lsnn} a $Var[W_l]=1/n_{l-1}$ Normal distribution, for similar spiking models.
Only recently a similar method with emphasis on the sparsity of activity has been developed by \cite{rossbroich2022fluctuation} to initialize the connection matrices of spiking neural networks. However, to our knowledge, this approach has not been used to determine the SG shape.

% spiking neurons need SGs
\subsection{Surrogate Gradients for Spiking Neurons}
\noindent\textbf{Surrogate Gradients.}
A problem that comes by using spiking neural networks is that they are not differentiable, so, training with gradient descent would not be possible without a patch to fix it. One patch is to use approximate gradients, often referred to as Surrogate Gradients (SG).
The non-differentiability appears in spiking networks because a spike is produced when the voltage surpasses the threshold, which mathematically is often described through a Heaviside function, $\tilde{H}(v)$, that is  zero for $v<0$ and one for $v\geq 0$. We use the tilde to remind that a SG is used for training, defined as $d\tilde{H}(v)/dv = \gamma f(\beta\cdot v)$, where $\beta$ is the sharpness, $\gamma$ the dampening, $f$ is the shape of choice and $\cdot$ the scalar product. Thus, $\gamma$ controls the amplitude of the SG, and $\beta$ controls the width and, unless explicitly stated, we set both to one. Instead, $f$ is usually defined as maximal at $f(0)$ and smoothly decaying to zero to infinity. As a result, a high sharpness, mostly passes the gradient for $v$ close to zero, while low sharpness also passes the gradient for a wider range of voltages. Importantly, the SG still allows to pass the gradient when the neuron has not fired. 

The SG shapes $f$ we investigate are (1) rectangular \cite{hubara2016binarized}, (2) triangular \cite{esser2016convolutional, lsnn}, (3) exponential \cite{Shrestha2018SLAYERSL}, (4) gaussian \cite{taulsnn}, (5) the derivative of a sigmoid \cite{zenke2021remarkable}, and (6) the derivative of a fast-sigmoid, also known as SuperSpike \cite{zenke2018superspike}. Their curves are plotted in Fig. \ref{fig:methodo} and their equations can be found in App.~\ref{app:surrogate}. To make the comparison between different SG more clear, $f$ is chosen to have a maximal value of $1$ and an area under the curve of $1$. We also propose a generalization of the derivative of the fast-sigmoid, that we call $q$-PseudoSpike SG. Its tail fatness is controlled by a hyper-parameter $q$ and we use it to study tail dependence in section \ref{sec:heavy_tails}. 
We use them to train the most common spiking neuron, the Leaky Integrate and Fire (LIF) neuron.

\noindent\textbf{Leaky Integrate and Fire spiking neuron.}
Arguably the simplest spiking neuron is the LIF \cite{lapique1907recherches, gerstner2014neuronal, wozniak2020deep}. It is defined as $\boldsymbol{y}_t = \boldsymbol{\alpha}_{decay} \boldsymbol{y}_{t-1}(1-\boldsymbol{x}_{t-1}) + \boldsymbol{i}_{t-1} $ where $\boldsymbol{i}_{t}=W_{rec}\boldsymbol{x}_{t} + W_{in}\boldsymbol{z}_t + \boldsymbol{b}$, and $y_t$ is the neuron membrane voltage, using \cite{glorot2010understanding, he2015delving} notation. We define $x_{t} = \sigma(y_{t})= \tilde{H}(y_{t}-\vartheta)= \tilde{H}(v_{t})$ as the spiking activity, where  $\vartheta$ is the spiking threshold, $v_t=y_{t}-\vartheta$ the centered voltage, and $\tilde{H}(v_{t})$ a Heaviside function with SG. The term $(1-\boldsymbol{x}_{t-1})$ represents a hard reset, that takes the voltage to zero after firing. The input $z_t$ can represent the data, or a layer below. It is common to write $\alpha_{decay}= 1-\frac{dt}{\tau_m}$, where $dt$ is the computation time, $\tau_m$ the membrane time constant, and to multiply the other terms by biologically meaningful constants, that we compress for cleanliness. Each neuron can have its own speed $\alpha_{decay}$, intrinsic current $b$ and $\vartheta$. In this work, all the parameters in the LIF definition are learnable.

We denote vectors as $\boldsymbol{a}$, matrices as $A$, and their elements as $a$.  In a stack of $L$ layers, we add an index $l$ to each parameter and variable. The matrix $W_{rec,l}\in\mathbb{R}^{n_{l}\times n_{l}} $ connects neurons in the same layer, with zero diagonal, and $W_{in,l}\in\R^{n_{l}\times n_{l-1}} $ connects the layer with the layer below, or the data if $l=0$, where $n_l$ is the number of neurons in layer $l$. We use curved brackets $A(\cdot)$ for functions, and square brackets $A[\cdot]$ for functionals that depend on a probability distribution. We use interchangeably $\overline{a} = Mean[a]$, $\hat{a}=Max[a]$, and $\check{a}=Min[a]$ for any variable $a$. We call the firing rate of a layer, the mean across the time axis of the data, across neurons in the layer and across minibatch samples, as $\rho=\overline{x}$, and we quantify the sparsity of a binary vector as $1-\rho$. Therefore the lower the firing rate, the sparser the activity. We stress that we are not looking for biologically plausible values of $\rho$, and we are instead concerned by the computational and learning capabilities of the system. 
% We use $\theta$ to denote any learnable parameter. 
Since the equation only depends on the previous timestep and layer, the probability distribution is a Markov chain in time and depth. Therefore the statistics we discuss are computed element-wise with respect to the distribution $p(y_{t,l}|t,l) = p( \boldsymbol{y}_{t-1, l},\boldsymbol{z}_{t,l-1},W_{rec,l}, W_{in,l},b_l, \alpha_{decay,l},\vartheta_l |t,l)$.

Additionally, to emphasize the need for our line of work, we show how the impact of the SG choice becomes more unpredictable as we increase the task complexity and the network complexity.
To do that, we briefly make use of two extra neuron definitions. When a LIF is upgraded with a dynamical threshold to maintain longer memories, we have the Adaptive LIF (ALIF) \cite{gerstner2014neuronal, lsnn}. Moreover, we propose the spiking LSTM (sLSTM), defined by changing the LSTM \cite{hochreiter1997long} activations by neuromorphic counterparts. The equations for both the ALIF and the sLSTM can be found in App.~\ref{app:alifsLSTM}. We quantify their complexity as in \cite{yin2021accurate}, and Tab.~\ref{tab:complexities}, by the number of operations performed per layer. However, we apply our stability method only to the LIF.

% gradient stability in spiking neurons
\subsection{Surrogate Gradient Stability}
When studying the stability of representations and gradients, it is typical to study the dependence of the mean and the variance of representations and gradients with depth and time, to look for initialization hyper-parameters that eliminate such dependence, and avoid an exponential explosion or vanishing altogether. Typically both forward and backward pass result in a similar constraint on the mean and variance of the connection matrix~\cite{glorot2010understanding, he2015delving}. In this work we study mean and variance of the representations with conditions I and II and maximum value and variance of the gradients with conditions III and IV, all of them on the LIF network. Conditions I and II result in constraints on the connection matrices, while conditions III and IV result in constraints on the SG shape.
In a sense we are interested in stabilizing as many quantities susceptible to exponential composition as possible, being the mean and the variance the most typical magnitudes to stabilize. In fact, we will see experimentally if applying all the conditions outperforms applying any single one of them.
We present the mathematical equivalent in each subsection, and the derivation details in the Appendix, but in summary
\begin{enumerate}[label=\Roman*]
\itemsep0em 
    \item The voltage should hit the most sensitive part of the SG;
    \item Recurrent and input variances should match;
    \item Gradients must have equal maxima across time;
    \item Gradients must have equal variance across time.
\end{enumerate}


\noindent\textbf{Condition I: Recurrent matrix mean sets the firing rate.}
In the gradient learning literature, it is standard to choose initializations that place the pre-activation activity 
to hit the most sensitive part of the activation, which usually is around zero 
~\cite{glorot2010understanding, he2015delving, roberts2022principles, Hanin2018HowTS, ioffe2015batch, ba2016layer}.
Moreover, notice that SG curves reach their highest when the neuron fires, Fig.~\ref{fig:methodo}. Thus, if the voltage stays close to firing, the gradient is stronger. This is always so if $Median[v]=0$ and $Var[v]=0$. However, $Var[v]=0$  turns off all higher moments, thus, we only assume $Median[v]=0$ as the mathematical equivalent of our desiderata. When (I) is applied to a LIF network (see App.~\ref{app:means}), the mean of the recurrent weight matrix fixes $\rho_i$, further assuming $\overline{w}_{in}=0$,  $\boldsymbol{b}=0$, the approximation $Mean[v]\approx Median[v]$, and constant $\overline{i}_t$ over time, we find
{\small
\begin{align*}
    &\overline{w}_{rec}=
    \frac{1}{n_{l}-1}\Big(2-\alpha_{decay}\Big)\vartheta && \text{(I)}
\end{align*}
}
% Notice that given the mentioned assumptions, as the width of the network increases, the mean essentially goes down to zero.
The assumption $Mean[v]\approx Median[v]$, can be justified by noticing that if $v$ is sampled from a unimodal distribution with the first two moments defined, then $|Mean[v]-Median[v]|\leq\sqrt{0.6Var[v]}$ is true \cite{basu1997mean}. Experimentally, we observe always unimodal distributions on the datasets we define in section \ref{sec:datasets}, that verify $|Mean[v]-Median[v]|\leq\sqrt{c\Var[v]}$, with $c=10^{-4}$ for the SHD task, $c=3\times10^{-2}$ for the sl-MNIST task, and $c=10^{-3}$ for the PTB task, with and without (I), much closer than only assuming the unimodality.

This choice of initialization results in firing rates of $\rho=1/2$, which are considered too high in the neuromorphic literature, that has a strong preference for low firing rates. We observe that high sparsity on test sets can be achieved even if we start with high firing rates at initialization. Therefore, we study two settings: starting with a variety of firing rates at initialization $\rho_i$, we proceed to train with and without a Sparsity Encouraging Loss Term (SELT).
The SELT is a mean squared error between a target firing rate $\rho_t=0.01$ and the layer firing rate $\rho_l=\overline{x}_{t,l}$, such that $\mathcal{L}_{SELT} = \lambda/L\sum_l(\rho_l-\rho_t)^2$, where $\lambda$ is a multiplicative constant. 
To achieve different $\rho_i$, we pre-train $\boldsymbol{b}_l$ on the dataset of interest, holding the other parameters untrained, using only the SELT without the classification loss. 
The coefficient $\lambda$ to multiply the loss term is chosen to make all losses comparable only when the task is learned, to let the network focus first on the task and then on the sparsity. We therefore choose as the multiplicative factor the minimal training loss achieved without SELT, since the SELT takes values between zero and one. We switch on the SELT gradually during training. The switch starts as zero, and moves linearly to one between $1/5$ and $3/5$ of training, and stays on thereafter. We focus on the $\partial$ fast-sigmoid and the SHD task in the main text, but we show different SG and tasks in App.~\ref{app:more_sparsity}.
Finally, we measure the Pearson correlation of the firing rate before and after training ($\rho_i, \rho_f$ for initial and final) with loss after training, on the test set. We therefore use this study to understand the impact of the use of high firing rates at initialization, as suggested by condition I, despite not being common practice in the neuromorphic literature.

\noindent\textbf{Condition II: Recurrent matrix variance can  make recurrent and input contribution to voltage comparable.}
Applying the Glorot and He method would suggest to set the variance of each layer output to one, to avoid exponential composition. However, the output of the forward pass of a spiking neuron is always strictly one or zero, so, it cannot compose exponentially. Instead, we propose to set the variance of the input to remain similar to the recurrent contribution to the variance. This can be understood as the maximally non-informative Bayesian prior decision to make, when the nature of the task at hand is not known. We describe it mathematically as $Var[W_{rec}x_{t-1}] = Var[W_{in}z_t]$. In a LIF network, we show in App.~\ref{app:ineqrec}, further assuming $\rho_l =1/2$,  $\overline{w}_{in}=0$, and computing $Var[z_t]$ and $\overline{z}_t$ on the training set, that this statement conduces to
{\small
\begin{align*}
    &Var[w_{rec}] =  2(Var[z_t] + \overline{z}_t^2)\frac{n_{l-1}}{n_{l}-1}Var[w_{in}] - \frac{1}{2}\overline{w}_{rec}^2 && \text{(II)}
\end{align*}
}
Therefore, condition II can be  used to set the variance of the recurrent matrix to makes both, input and recurrent contributions equal.

\noindent\textbf{Condition III and IV: Dampening and sharpness set gradient maximum and variance.}
We have so far described how to stabilize the forward pass.
Instead, to control the backward pass, we look for constraints to obtain stable gradients with time. We describe mathematically (III) as $Max[\frac{\partial}{\partial \theta}y_t] = Max[\frac{\partial}{\partial \theta}y_{t-1}]$ and (IV) as $Var[\frac{\partial}{\partial \theta}y_t] = Var[\frac{\partial}{\partial \theta}y_{t-1}]$. On a LIF network, they set the dampening and the second moment of the SG that keep the maximum and variance of the gradient stable with time (App.~\ref{app:sgmax}, \ref{app:backward}). Sharpness and tail-fatness are linked to the SG second moment (App.~\ref{app:ivexp}). Assuming $\sigma'$ and $\frac{\partial}{\partial \theta}y_{t-1}$ as independent, and zero mean gradients at initialization, we find
{\small
\begin{align*}
    &\gamma=   \frac{1}{(n_{l}-1)\hat{w}_{rec}}\Big( 1 -\alpha_{decay} \Big)&& \text{(III)}\\
    &\overline{\sigma^{\prime 2}} = \frac{1-\frac{1}{2}\alpha_{decay}^2}{(n_{l}-1)\overline{w^2}_{rec}}  && \text{(IV)}
\end{align*}
}
To be able to clearly relate the sharpness $\beta$ with the non centered second moment of the SG, $\overline{\sigma^{\prime2}}$, we show in App. \ref{app:ivexp}, that assuming a uniform $v$ distribution gives
\begin{align}
    \label{eq:2momsigma}
    \overline{\sigma^{\prime2}}
    =&\frac{\gamma^2}{\beta(y_{max}-y_{min})}\int^{\beta(y_{max}-\vartheta)}_{\beta(y_{min}-\vartheta)} f(v)^2dv
\end{align}
Therefore, different SG  shapes $f$, will require different sharpness $\beta$ to meet condition (IV), since the result of the integration will depend on $f$. We use both equations for (III) and (IV) to make the respective predictions for the theoretically justified dampening, sharpness and tail-fatness in Fig.~\ref{fig:conditions}. For the dampening the method is simply applying the equation. For the sharpness prediction, we use the exponential SG, and as it can be seen in equation \ref{eqIVexp}, the dependence on the integration limits makes it impossible to isolate the sharpness analytically. Instead we retrieve it by finding the root of the resulting equation through gradient descent. Similarly, to find the theoretically justified tail-fatness, we use the $q$-PseudoSpike to arrive at equation \ref{eq:IVtail}, and isolate the $q$ predicted finding the root of the resulting equation through gradient descent.


\subsection{Datasets}
\label{sec:datasets}

In this work we use three tasks, that we present in increasing number of classes, which we will use as a proxy for task complexity. 
More details on the datasets can be found in App.~\ref{app:trainingdetails}.

\noindent\textbf{Spike Latency MNIST (sl-MNIST):} the MNIST digits \cite{mnist} pixels (10 classes) are rescaled between zero and one, presented as a flat vector, and each vector value $x$ is transformed into a spike timing using the transformation $T(x) = \tau_{eff}\log(\frac{x}{x-\vartheta})$ for $x>\vartheta$ and  $T(x) = \infty$ otherwise, with $\vartheta=0.2, \tau_{eff}=50 \text{ms}$ \cite{zenke2021remarkable}. The network input is a sequence of $50$ms, $784$ channels ($28\times 28$), with one spike per row.

\noindent\textbf{Spiking Heidelberg Digits (SHD):} is based on the Heidelberg Digits (HD)  audio dataset \cite{cramer2020heidelberg} which comprises 20 classes of spoken digits, from zero to nine, in English and German, spoken by 12 individuals.  These audio signals are encoded into spikes through an artificial model of the inner ear and parts of the ascending auditory pathway.

\noindent\textbf{PennTreeBank (PTB):} is a language modelling task. The PennTreeBank dataset \cite{ptb}, is a large corpus of American English texts. We perform next time-step prediction at the word level. The vocabulary consists of 10K words, which we consider as 10K classes. The one hot encoding of words can be seen as a spiking representation, even if it is the standard representation in the non neuromorphic literature.

\subsection{Training Details}

Our networks comprise two recurrent layers. The output of each feeds the following, and the last one feeds a linear readout. Our LIF network has 128 neurons per layer on the sl-MNIST task, 256 on SHD, and one layer of 1700 and another of 300 on PTB, as in \cite{wozniak2020deep}. On the complexity study with the SHD task, the ALIF has 256 neurons and the sLSTM 85, to keep a comparable number of 350K parameters. We train on the crossentropy loss.  The optimizer had a strong effect, where Stochastic Gradient Descent \cite{robbins1951stochastic, kiefer1952stochastic} was often not able to learn, and AdaM \cite{adam} performed worse than AdaBelief \cite{zhuang2020adabelief}. AdaBelief hyper-parameters are set to default, as in \cite{radford2018improving, zenke2021remarkable}. The remaining hyper-parameters are reported in App.~\ref{app:trainingdetails}. Unless explicitly stated, we use Glorot Uniform initialization. Each experiment is run 4 times and we report mean and standard deviation. Experiments are run in single Tesla V100 NVIDIA GPUs. We call our metric the mode accuracy: the network predicts the target at every timestep, and the chosen class is  the one that fired the most for the longest.



% how would you make the connection


% list what I Want to talk about and then correct the order


% it's fine if you write too much, since it's IEEE, and you should prioritize  feeling like you care about your reader, rather than brevity, since the former makes an article and the latter makes a report.
