

\begin{figure*}
    \centering
    \includegraphics[width=1.\textwidth]{images/surrogate_grads/lr_sg.pdf}
    \includegraphics[width=1.\textwidth]{images/surrogate_grads/legend_4.pdf}
    
    \caption{\textbf{The choice of SG becomes increasingly important as task and network complexity increase.} In order to clearly showcase the problem addressed by our work, and to understand the difficulties brought by SG training, we want to see the impact of training with different SG shapes, and how task and network complexity affect it. This will stress the need for clever rules to apply at initialization to prevent worst case scenarios. Tasks and networks are presented from left to right in order of increasing complexity, where number of classes is used as a proxy of task complexity, and the number of operations is used to quantify network complexity. We perform a grid search over SG shapes, learning rates, tasks and networks. We report lowest validation perplexity after converged training, where perplexity is a loss, so, the lower the better. Panels a-f) show  perplexity (y-axis), against learning rate (x-axis). In a-c) we fix the LIF network and change task, while in d-f) we fix the SHD task and change network. Plots b) and d) are repeated for clarity. Panels g-h) show SG sensitivity (y-axis) against task and neural model (x-axis), where we defined sensitivity in Sec. \ref{sec:sensitivity}, and it is essentially the variance of the perplexity, across SG shapes and learning rates.  a-f) Our results demonstrate that even if different SG shapes tend to agree on the optimal learning rate, the final performance can vary substantially, depending on the SG selection. Specifically, the $\partial$ fast-sigmoid seems the most resilient to changes in the learning rate, as shown in \cite{zenke2021remarkable}. g-h) Moreover, we observe that the more complex the task or the network, the higher the performance variability we see across SG shapes and learning rates. 
    Our stability criterion provides a method to carefully select SG shapes at initialization and address this issue, promoting better performance and generalization.
    % This stresses the need for a careful choice of SG shape at initialization, where our stability criterion is designed as a method to make that choice.
    }
    \label{fig:task_net_dependence}
\end{figure*}


\section{Results}

% \red{Possibly rewrite results.}
\subsection{Sensitivity increases with Complexity}
\label{sec:sensitivity}


In order to stress the difficulty of choosing the right SG, we investigate how performance changes with SG as we increase task and network complexity. We estimate the task complexity by the number of classes. Thus, if $C_T(\cdot)$ measures task complexity, $C_T(sl\text{-}MNIST)<C_T(SHD)<C_T(PTB)$. We quantify neural complexity as in \cite{yin2021accurate}, and Tab.~\ref{tab:complexities}, by the number of operations performed per layer. In essence, if $C_M(\cdot)$ measures model complexity, then $C_M(LIF)<C_M(ALIF)<C_M(sLSTM)$. To have comparable losses across tasks and networks, we normalize their validation values between 0 and 1. For that, we remove the lowest loss achieved by a network in a task for any seed and learning rate, and divide by the distance between the highest and lowest loss. We call the result the \textit{post-training normalized loss}. We call \textit{sensitivity} the standard deviation of the \textit{post-training normalized perplexity} across SG, for each learning rate. We report mean and standard deviation across learning rates.


We see in Fig.~\ref{fig:task_net_dependence}, that task and network complexity have a measurable effect on the sensitivity of training to the SG choice.  We run a grid search over learning rates and SG shapes. The sensitivity to the task is shown in the upper panels, for the LIF network. We see that different SG agree on the optimal learning rate. We also see that the $\partial$ fast-sigmoid performs well for a wider range of learning rates. The rectangular SG is competitive on some tasks, but fails to learn with most learning rates on PTB. Then we focus on network sensitivity, fixing the SHD task, lower panels. The triangular SG performs similarly to the exponential on the LIF network, while it underperforms on ALIF, and fails on sLSTM. The exponential SG matches the best SG on both the LIF and the sLSTM, but not on the ALIF. All this manifests a strong sensitivity to the SG choice. 
Surprisingly, the sLSTM lags behind the LIF and ALIF, with a comparable number of parameters. The gating mechanism devised to keep the LSTM representations from exploding exponentially, are not relevant anymore for a Heaviside that cannot explode exponentially, and might have become a computational burden. 
Incidentally, we reached spiking state-of-the-art on the PTB task with the triangular SG. Best average over 12 seeds had $122.8 \pm 10.7$ validation and $114.2 \pm 9.2$ test perplexity, and best seed had $117.2$ validation and $109.5$ test perplexity. Previous spiking SOTA on PTB was 137.7 test perplexity~\cite{wozniak2020deep}.  Fig.~\ref{fig:task_net_dependence}, g-h), confirm that there is a correlation between task and network complexity, and SG sensitivity. This stresses the importance of finding the correct SG to achieve maximal performance.




\subsection{High  initialization firing rates can improve generalization with low test firing rates}
\label{sec:ressparse}


In order to propose our stability-based theoretical method for SG choice, we want to make sure that high initial firing rates are not pernitious neither for learning nor for final sparsity. This is so, because in the neuromorphic literature training success is judged by (1) training performance and (2) activity sparsity. 
We can see in Fig.~\ref{fig:sparsity} that with and without a SELT, higher $\rho_i$ correlates with performance. 
In fact at each layer $l$, the correlation $r_l$ of the firing rate with the loss is markedly negative, and statistically significant, where we show in bold whenever $p$-value $\leq0.05$.
Notice that SELT achieved worse final train loss (not shown). However, the high $\rho_i$ combined with SELT resulted in better test loss, thus, better generalization. However, this is not consistent across SG shapes, Fig.~\ref{fig:sparsity_and_shape}, but is consistent across tasks, Fig.~\ref{fig:sparsity_and_task} App.~\ref{app:more_sparsity}. In fact, the triangular SG prefers low $\rho_i$ and the exponential SG does not show a clear trend. Incidentally, the lower layer always reaches higher sparsity, across seeds (Fig.~\ref{fig:sparsity}), SG shapes (Fig.~\ref{fig:sparsity_and_shape}) and tasks (Fig.~\ref{fig:sparsity_and_task}).

\begin{figure}
    \includegraphics[width=.5\textwidth]{images/surrogate_grads/t__sparsity_tsgfastsigmoidpseudod_tSHD.pdf}
        \caption{\textbf{High  initialization firing rates can improve generalization with low test firing rates.} Our initialization method suggests to set a high firing rate at the beginning of training, which is uncommon in the neuromorphic literature. We study if it is possible to reconcile high initialization firing rates with low firing rates on the test set. We use the SHD task and the $\partial$~fast-sigmoid SG, and measure the correlation $r_l$ of each layer $l$ firing rate with perplexity after training, on the test set. Bold correlation means $p$-value $ \leq 0.05$. On the $y$-axis we report perplexity after training on the test set, and on the $x$-axis we report initialization firing rate $\rho_i$, or final firing rate $\rho_f$, meaning the firing rate after training, also evaluated on the test set. On the two left panels, learning starts from different $\rho_i$ without a Sparsity Encouraging Loss Term (SELT), while on the two right panels a target sparsity is encouraged. In both cases, the initial firing rate correlates with final performance, and a low $\rho_f$ is achieved successfully using a SELT. Notice as well that the combination of high initial firing rate and sparsity encouragement resulted in better test loss than on the two panels on the left, suggesting that both factors acted synergistically as a regularization mechanism. We conclude that high initialization firing rates are not necessarily at odds with having sparse activity after training.}
    \label{fig:sparsity}
\end{figure}


\subsection{Our stability-based constraints on the LIF weights and SG shape improve final performance.}
\label{sec:conditions}

Keeping in mind that we can exploit a low initial sparsity as a regularization mechanism, we have proposed a method for stabilizing LIF networks inspired by FFN initializations \cite{glorot2010understanding, he2015delving}, that determines initialization weights and SG shape. The four conditions we propose, result in a SG that depends on the network and the task.
Fig.~\ref{fig:conditions} shows training results with our conditions for the LIF network on the SHD task, with exponential SG, against the unconditioned baseline. Condition II improves accuracy the most when applied on its own, but the best performance is achieved with all conditions together. When all conditions are applied, a LIF network achieves a $92.7\pm1.5$ validation and $75.8\pm3.1$ test accuracy, compared to $87.3\pm1.4$ validation and $69.0\pm5.8$ test accuracy without conditions.

\begin{figure}
    \includegraphics[width=.45\textwidth]{images/surrogate_grads/figure5_conditions.pdf}
    \caption{\textbf{Our stability-focused constraints on the LIF weights and SG shape improve  final performance.} 
    This figure illustrates our novel method for selecting SG in a Leaky Integrate-and-Fire (LIF) network to improve its stability and performance. 
    We design 4 conditions to stabilize forward and backward pass of a LIF network.~(I) requires voltages that promote higher SG values, (II) balances input and recurrent contribution to the voltage, while (III) and (IV) constrain gradient maxima and variance over time. 
    We demonstrate the effectiveness of our method for the LIF network on the SHD task, with an exponential SG and Glorot Uniform initialization, and compare training under the four stability conditions with the baseline without any conditions (shown in gray).
    % Simulations are run on the SHD task for a LIF network. The baseline has \textit{no conditions} applied, in gray, with an exponential SG and Glorot Uniform initialization.
    Lower and upper panels show validation and test accuracies. Our results show that while condition (II) has the most significant impact on its own, all four conditions combined lead to the best performance.
    These findings suggest that our theory of LIF stabilit can reduce the need for extensive hyper-parameter search and improve the experimental performance of LIF networks.}
    \label{fig:conditions}
\end{figure}

\begin{figure*}
    {\footnotesize \hspace{1.2cm}(a)\hspace{5.6cm}(b)\hspace{5.6cm}(c)}
    
    \centering
    \includegraphics[width=.32\textwidth]{images/surrogate_grads/figure2_dampening.pdf}
    \includegraphics[width=.32\textwidth]{images/surrogate_grads/figure2_sharpness.pdf}
    \includegraphics[width=.32\textwidth]{images/surrogate_grads/figure2_tails.pdf}
    
    \includegraphics[width=1.\textwidth]{images/surrogate_grads/legend_2.pdf}

    \caption{\textbf{Our stability-based theory predicts optimal SG features on the LIF network.} We compare how the features of SG shape predicted by our method stand up against other experimental choices. We conduct the analysis on the LIF network for the sl-MNIST task. Panel (a) shows the performance for different dampening values while setting the sharpness to 1, and vice versa in panel (b). The dashed vertical lines show our theoretical predictions for the exponential SG, (III) for the dampening ($\gamma=0.20\pm 0.02$) and (IV) for the sharpness ($\beta=1.02\pm 0.17$), which agree with the experiments. Dampenings lower than 1 improve performance while the pattern is the opposite for sharpness. Panel (c) shows the performance for different tail-fatness values on the $q$-PseudoSpike SG with $\beta=\gamma=1$. The theoretical prediction gives a close to optimal $q=1.898\pm 0.002$, whereas the best experimental result is $q=1.56$. These findings suggest that our stability-based method predicts good SG features before training, thereby reducing the need for time-consuming hyper-parameter search. }
    \label{fig:sensitivity_to_dsf}
\end{figure*}



\subsection{Our stability-based theory predicts optimal SG features on the LIF network}
\label{sec:heavy_tails}

We compare experimentally the performance of a range of values of dampening, sharpness and tail-fatness and we assess how they compare to our theoretical prediction. Fig.~\ref{fig:sensitivity_to_dsf} shows the accuracy of the LIF network on the sl-MNIST task. Each SG has its tail decay: inverse quadratic for the $\partial$ fast-sigmoid, no tail for the triangular and rectangular, and exponential decays for the rest. Low dampening and high sharpness are preferred by all SG. Interestingly, the accuracy of the $\partial$ fast-sigmoid degrades less with suboptimal $\gamma,\beta$. The vertical dashed lines are predicted by our theoretical method, condition (III) for the dampening and (IV) for the sharpness of an exponential SG. We observe that they find $\gamma,\beta$ with high experimental accuracies. This supports the claim that reducing hyper-parameter search of dampening and sharpness is possible. We use our $q$-PseudoSpike SG to study the dependence with the tail-fatness, panel (c) Fig.~\ref{fig:sensitivity_to_dsf}. All tail-fatness values perform reasonably well, with a maximum at $q=1.56$, smaller than the $q=2$ of the $\partial$ fast-sigmoid. Interestingly our theoretical solution gives a $q=1.898 \pm 0.002$, surprisingly close to the experimental optimum.

