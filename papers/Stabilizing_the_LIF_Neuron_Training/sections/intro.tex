

\section{Introduction}


% # First, provide some context to orient those readers who are less familiar with your topic and to establish the importance of your work.
Neuromorphic Computing is a rapidly growing field that aims to develop energy-efficient Artificial Intelligence systems. The key to this energy efficiency is the use of Spiking Neural Networks (SNNs) \cite{henderson2020towards,blouw2019benchmarking, 9395703, lapique1907recherches,izhikevich2003simple}, which are characterized by binary and sparse activity patterns. This binary activity, however, presents a challenge in terms of training, as it is not differentiable, which makes most traditional learning methods inapplicable \cite{robbins1951stochastic, kiefer1952stochastic,adam}.
To overcome this challenge, a common solution is to introduce an approximation of the binary activity's derivative, referred to as the Surrogate Gradient (SG) \cite{esser2016convolutional, zenke2018superspike, lsnn}.

% # Second, state the need for your work, as an opposition between what the scientific community currently has and what it wants.
While the use of SG has been widely adopted in the field of Neuromorphic Computing, there has been limited progress in establishing a theoretical foundation for its choice \cite{surrogate2019, zenke2021remarkable}. The current practice is to choose a SG empirically, with the goal of improving performance, but with no understanding of the underlying principles.
In fact, it is common practice to pick one SG for all the experiments \cite{spikingbohte,courbariaux2016binarized,lsnn,zenke2018superspike,zenke2021remarkable,yin2021accurate}, and possibly explore the effect of changing its width (sharpness) \cite{zenke2018superspike} or its height (dampening) \cite{lsnn}. Our work proposes foundations to the conventional initialization methods by balancing the gradients across time. With such balancing, robustness is obtained in relation with time backpropagation.

% # Third, indicate what you have done in an effort to address the need (this is the task).
To ensure that information is balanced through time during training, several hyper-parameters must be carefully tuned at initialization, including the SG shape. This is why an initialization method influences the choice of the best SG. In our theoretical framework, we emphasize the importance of this relationship between initialization and SG function. On the other hand, while stability is crucial for optimal network performance, it can also come at the cost of reduced reactiveness to important new stimuli. Our proposed method seeks to achieve stability while also fostering reactiveness. Moreover, we introduce an unconventional approach by initializing the network with high firing rates, resulting in a non-sparse activity pattern, which is not commonly seen in the neuromorphic literature. We leverage the fact that SG curves typically reach their maximum value when the neuron fires, so by keeping the voltage close to the firing threshold, we can achieve stronger gradients. Furthermore, we show that after training with an additional sparsity encouraging loss term, the activity is still sparse and the initial high activity level can accelerate training and generalization, when we recall the SNN on the test set. This hints at the possibility of a total benefit, in terms of both performance and energy efficiency at recall, as the test firing rate can be lower and test performance better.


% # Finally, preview the remainder of the paper to mentally prepare readers for its structure, in the object of the document.

Our contributions are therefore:
\begin{itemize}
    \item We show that the choice of SG becomes increasingly important as task and network complexity increase;
    \item We observe that the derivative of the fast sigmoid is a resilient SG;
    \item We show that high  initialization firing rates can improve generalization with low test firing rates;
    \item We provide stability-based constraints on the LIF weights and SG shape that improve final performance;
    \item Our stability-based theory predicts optimal SG features on the LIF network.
\end{itemize}
