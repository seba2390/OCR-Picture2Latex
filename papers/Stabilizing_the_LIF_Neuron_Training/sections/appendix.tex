\clearpage
\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\theHsection}{A\arabic{section}}

\beginsupplement

% \clearpage
% \begin{appendices}
% \beginsupplement

% \section*{Appendix}
% \renewcommand{\thesubsection}{\Alph{subsection}}
% \renewcommand{\thesection}{\Alph{section}}

\section*{Appendix}

\section{More Training Details}
\label{app:trainingdetails}

We collect the training hyper-parameters in the following table.

\begin{table}[h]
\centering
\begin{tabular}{rlll}
& sl-MNIST & SHD &  PTB \\ \hline
\\
batch size & 256 & 256 & 32 \\
weight decay &0.0 & 0.1 &  0.1 \\
gradient norm & 1.0 & 1.0 & 1.0 \\
\shortstack{train/val/test \\ \ } & \shortstack{45k/5k/10k\\ samples } & \shortstack{8k/1k/2k \\ samples }  &   \shortstack{930k/74k/82k \\ words} \\
learning rate & $3.16 \cdot 10^{-4}$ &  $3.16 \cdot 10^{-4}$ &  $3.16 \cdot 10^{-5}$ \\
layers width & 128, 128 & 256, 256 &  1700, 300 \\
label smoothing & 0.1 & 0.1 & 0.1 \\
time step repeat & 2 & 2 & 2 \\
SELT factor & 0.8 & 0.436 & 4.595 \\
\end{tabular}
\end{table}


The learning rates were chosen after a grid search fixing dampening and sharpness to 1.  The learning rates considered are in the set $\{10^{-2},3.16 \cdot10^{-3}, 10^{-3}, 3.16 \cdot10^{-4}, 10^{-4}, 3.16 \cdot10^{-5},10^{-5 }\}$. The results of the grid search are reported in figure \ref{fig:task_net_dependence}. The learning rate chosen for the rest of the paper was the one that made all the shapes perform reasonably well, rectangular included. This mostly resulted in a suboptimal learning rate only for the derivative of the fast sigmoid, which still out-performed the rest in the sl-MNIST and SHD, and performed comparatively on the PTB. 

We train with crossentropy loss, the AdaBelief optimizer \cite{zhuang2020adabelief}, Stochastic Weight Averaging \cite{swa2018} and Decoupled Weight Decay \cite{loshchilov2018decoupled}. 
% For the BiGamma distribution, we choose $\alpha=5$ and $\beta=\sqrt{\alpha(\alpha+1)}=5.47$ to have a variance of $1$.
For the PTB task, the input passes through an embedding layer before passing to the first layer, and the output of the last layer is multiplied by the embedding to produce the output, removing the need for the readout
% , as done in 
% it is common practice for language modeling 
\cite{wozniak2020deep, radford2018improving}.
Notice that we do not implement forced refractory periods that would prevent the neuron from firing too fast, as sometimes done in the neuromorphic literature, since we want to reduce the non differentiable steps in the system. Thus, $\rho=1$ is possible if the inputs are strong and frequent enough.

\section{Neuron Model Complexity}

The energy consumed per layer can be used as a metric of neuron complexity, as done in \cite{yin2021accurate, hunger2005floating}.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\textbf{\shortstack{Neural \\ model}} & \textbf{\shortstack{Energy (Complexity) \\ \hspace{1cm}}}\\
\hline\\
\textbf{LIF} & \shortstack{$(mnp_{l-1}+nnp_{l})E_{AC}+nE_{MAC}$ }\\
\textbf{ALIF} & \shortstack{$(mnp_{l-1}+nnp_{l}+2np_{l})E_{AC}+3nE_{MAC}$ } \\
\textbf{LSTM} & \shortstack{$4(mn+nn)E_{MAC}+17nE_{MAC}$ } \\
\textbf{sLSTM} & \shortstack{$4(mnp_{l-1}+nnp_{l})E_{AC}+3np_{l}E_{AC}$ } 
% \textbf{\shortstack{sLSTM \\ \vspace{.24cm}}} & \shortstack{$4(mnp_{l-1}+nnp_{l})E_{AC}+3np_{l}E_{AC}$ } 
\end{tabular}
\caption{\textbf{Neuron complexity.} We use the energy consumed per layer as a metric of neuron complexity \cite{yin2021accurate, hunger2005floating}. We use  $n=n_l$ and $m=n_{l-1}$ as the width of the layer and its input, $p_{l}$ for the  firing rate of the layer $l$. $E_{MAC}$ is the energy cost of a multiply-accumulate operation and $E_{AC}$ of an accumulate operation. As shown, $ALIF$ always results in a larger number of operations and energy consumption than $LIF$. For large networks, $n,m\gg1$, the square terms dominates, and the $sLSTM$ results in 4 times more energy consumption. 
}
\label{tab:complexities}
\end{table}

\newpage


\section{List of Surrogate Gradients shapes}
\label{app:surrogate}


We list here the shapes that we used in this article as surrogate gradients.

    
\begin{table}[h]
\centering
\small
\begin{tabular}{llll}
 & SG name  & $f(v)$  \\ \\ 
\textcolor{ctr}{\rule{1cm}{2pt}} & \textbf{triangular} & $ \max(1-|v|,0)$  \\ 
\textcolor{cex}{\rule{1cm}{2pt}} & \textbf{exponential} & $e^{-2|v|}$ \\ 
\textcolor{cga}{\rule{1cm}{2pt}} & \textbf{gaussian}  & $ e^{-\pi v^2}$ \\ 
\textcolor{csi}{\rule{1cm}{2pt}} & \textbf{$\partial$ sigmoid} & $4\ sigmoid(4\ v)\left(1-sigmoid(4\ v)\right)$ \\ 
\textcolor{cfa}{\rule{1cm}{2pt}} & \textbf{$\partial$ fast-sigmoid} & $\frac{1}{(1+|2v|)^2}$ \\ 
\textcolor{cre}{\rule{1cm}{2pt}} & \textbf{rectangular} &  $\mathbb{1}_{|v|<\frac{1}{2}}$ \\ \\ 
\textcolor{cnt}{\rule{1cm}{2pt}} &
\shortstack{\textbf{$q$-PseudoSpike} \\ ($q>1$)} & $\frac{1}{(1+\frac{2}{q-1}|v|)^q}$ \\ \\
\end{tabular}
\caption{\textbf{Mathematical definitions of the surrogate gradients studied in this article.}
Our Heaviside activation $\sigma(v)=\tilde{H}(v)$, where $v$ is the centered voltage, has the SG $\sigma'(v) = \gamma f(\beta\cdot v)$, where $\beta$ is the SG sharpness, $\gamma$ the SG dampening, and $f$ is the shape of choice. The constants, are chosen for the SG to have a maximal value of $1$ and an area under the curve of $1$.}
\end{table}



% \newpage


\section{Detailed derivation of the conditions}

\label{sec:conditionsdetails}

We derive the constraints on the hyper-parameters that will lead the LIF to meet the conditions proposed at initialization. The LIF we will be using
is defined by

\begin{align}
    \boldsymbol{y}_t = \boldsymbol{\alpha}_{decay} \boldsymbol{y}_{t-1}(1-\boldsymbol{x}_{t-1}) + \boldsymbol{i}_{t}
\end{align}

\noindent where $\boldsymbol{i}_{t}=W_{rec}\boldsymbol{x}_{t-1} + W_{in}\boldsymbol{z}_t + \boldsymbol{b}$, as described in the main text, and the multiplicative factor $(1-\boldsymbol{x}_{t-1})$ represents the reset mechanism.

\subsection{Recurrent matrix mean sets the firing rate (I)}

\label{app:means}

We show how condition (I) leads to a constraint on the mean of the recurrent connectivity with a LIF neuron model, that will lead the network to meet that condition at initialization.



\begin{lemma}\label{thm:cI}
Applying condition (I), which states that we want
$Median[v] = 0$, to an LIF network, and further assuming
$\overline{w}_{in} = 0, b = 0$, the approximation $Mean[v] \approx Median[v]$,
and constant $\overline{i}_t$ over time, it results in the constraint

\begin{align}
    \overline{w}_{rec}=\frac{1}{n_{rec}-1}(2-\overline{\alpha}_{decay})\overline{\vartheta}
\end{align}

\end{lemma}


\begin{proof}



First we show that $Median[v]=0\implies Mean[x]=1/2$, where $x = \tilde{H}(v)$. In equation \ref{eq:marg} we write the marginal distribution of $p(x)=\int p(x|v)p(v)dv$, and the double integral is represented with one integration symbol. Then, we notice that $x$ has a deterministic dependence on $v$, $x=H(v)$, which proprbabilistically is described by the delta function $p(x|v)=\delta(x - H(v))$. Then, we integrate over $x$, and in the last equation we notice that integrating with respect to the Heaviside is equivalent to restricting the integration limits from zero to infinity.

\begin{align}
    Mean[x] =& \int xp(x) dx \\
    =& \int xp(x|v)p(v)dxdv  \label{eq:marg}\\
    =& \int x p(v)\delta(x - H(v))dxdv\\
    =& \int p(v)dv H(v)\\
    =& \int_0^{\infty} p(v)dv
\end{align}


If $Median[v]=0$, half of it's probability mass is on each side of $0$, so the last integral is equal to $1/2$, QED.

Since working with medians is mathematically harder than working with means, we assume that $Mean[v]\approx Median[v]$, with the caveat that it will make the result approximate. To justify that they are similar, it can be shown that for a unimodal distribution $v\sim p(v)$ with the first two moments defined, we have $|Mean[v]-Median[v]|\leq\sqrt{0.6Var[v]}$ \cite{basu1997mean}. We will proceed with this approximation in mind, and we will continue the development with means and not with medians.

% We will use $\boldsymbol{y_t} = \boldsymbol{\alpha}_{decay} \boldsymbol{y}_{t-1} +\boldsymbol{i}_t$ for brevity, where $\boldsymbol{i}_t=W_{rec}\boldsymbol{x}_{t-1} + W_{in}\boldsymbol{z}_t + \boldsymbol{b} - \boldsymbol{\vartheta}\ \boldsymbol{x}_{t-1}$. 
We use the notation $\overline{x}=Mean[x]$ interchangeably.
We calculate how the mean of the voltage elements is propagated through time, assuming the mean input current to remain constant over time $\overline{i}_t = \overline{i}$ at initialization, to simplify the mathematical development, and assuming per condition (I), that $\overline{x} = \overline{1-x}=1/2$ we have

\begin{align}
    \overline{y}_t 
    =&\overline{\alpha}_{decay} \overline{(1-\boldsymbol{x}_{t-1})}\overline{y}_{t-1} + \overline{i} \\
    =&\frac{1}{2}\overline{\alpha}_{decay} \overline{y}_{t-1} + \overline{i} \\
    =&\frac{1}{2}\overline{\alpha}_{decay} \Big(\frac{1}{2}\overline{\alpha}_{decay} \overline{y}_{t-2} + \overline{i}\Big) + \overline{i} \\
    =&\frac{1}{2^{t-1}}\overline{\alpha}_{decay}^{t-1}\overline{y}_{1} + \Big(\sum_{t'=0}^{t-2}\frac{1}{2^{t'}}\overline{\alpha}_{decay}^{t'}\Big)\overline{i} \\
    =&\frac{1}{2^{t-1}}\overline{\alpha}_{decay}^{t-1}\overline{y}_{1}  + \frac{1-\frac{1}{2^{t-1}}\overline{\alpha}_{decay}^{t-1} }{1-\frac{1}{2}\overline{\alpha}_{decay}}\overline{i} 
\end{align}

\noindent where we used the fact that the same LIF definition applies to different time steps, the geometric series formula, and the fact that for independent random variables $E[XY] =E[X]E[Y]$. For $t\rightarrow\infty$ and using  $0<\overline{\alpha}_{decay}<1$

\begin{align}
    \overline{y}_t =&
    \frac{1 }{1-\frac{1}{2}\overline{\alpha}_{decay}}\overline{i} \\
    \overline{y}_t-\overline{\vartheta} =&
    \frac{1 }{1-\frac{1}{2}\overline{\alpha}_{decay}}\overline{i} - \overline{\vartheta} 
\end{align}

Assuming we want this condition to hold independently of the dataset, we set $Mean[W_{in}]=0$, and assuming that we do not want to promote this behavior with fixed internal currents, but with the recurrent activity instead, then $\boldsymbol{b}=0$. 

We remark that we denote $Mean[W\boldsymbol{x}]$ as the mean vector whose element $i$ is

\begin{align}\label{eq:indepx}
    Mean[W\boldsymbol{x}]_i=& Mean[\sum_{\substack{j=1\\j\neq i}} w_{ij}x_j] \\
    =& \sum_{\substack{j=1\\j\neq i}} Mean[ w_{ij}x_j]   \\
    =& \sum_{\substack{j=1\\j\neq i}} Mean[ wx]  \\
    =& (n_{rec}-1) Mean[ wx]   
\end{align}

\noindent where the condition $j\neq i$ in the summand reminds that neurons are not connected to themselves in our recurrent architecture. In the first equality, the index $i$ denoting the element in the vector, is equivalent as choosing the row $i$ of $W$, so it is not necessary to specify it outside the square brakets. The equality before the last one is a consequence of considering any neuron as mutually independent to any other at initialization, as done by \cite{glorot2010understanding, he2015delving}, and that justifies dropping the indices. Since $w$ and $x$ are statistically independent random variables, $Mean[ wx]=Mean[ w]Mean[ x]$.


Then, 

\begin{align}
    Mean[y_t-\vartheta] =&
    \frac{1 }{1-\frac{1}{2}\overline{\alpha}_{decay}} \Big((n_{rec}-1)\overline{w}_{rec}\overline{x}_{t-1}\Big) - \overline{\vartheta}\\
    0=&
    \frac{1 }{1-\frac{1}{2}\overline{\alpha}_{decay}} (n_{rec}-1)\overline{w}_{rec}\overline{x}_{t-1} - \overline{\vartheta}\\
    0=&
    \frac{1}{1-\frac{1}{2}\overline{\alpha}_{decay}} (n_{rec}-1)\overline{w}_{rec}\frac{1}{2} - \overline{\vartheta}\\
    (n_{rec}-1)\overline{w}_{rec}=&
    (2-\overline{\alpha}_{decay})\ \overline{\vartheta}\\
    \overline{w}_{rec}=&
    \frac{1}{n_{rec}-1}\Big(2-\overline{\alpha}_{decay}\Big)\overline{\vartheta} \label{appeq:CI}
\end{align}

\noindent where in the second line we applied condition (I) in the form of $Mean[v_t]\approx Median[v_t]=0$, so $Mean[y_t-\vartheta]=0$, and in the third line we applied again condition (I), $\overline{x}_t=1/2$.
% Where $Mean[W_{rec}]=(n_{rec}-1)Mean[w_{rec}]$, since we use upper case to represent matrices, and lower case to denote their elements. Since each neuron in the layer is connected to every other neuron except to themselves, each element contribution has to be considered $n_{rec}-1$ times.
In the main text we turn $\overline{\vartheta}, \overline{\alpha}_{decay}\rightarrow \vartheta, \alpha_{decay}$, since here we consider the more general case where those are as well random variables, and we simplify it in the main text for cleanliness, assuming they are constant.

\end{proof}

We therefore found a constraint on the mean of the recurrent matrix initialization, that leads the LIF network to satisfy condition I at initialization. The constraint is equation \ref{appeq:CI} with $\overline{w}_{in}=0$, and  $\boldsymbol{b}=0$.


\subsection{Recurrent matrix variance can  make recurrent and input voltages comparable (II)}
\label{app:ineqrec}

We apply condition (II) to the LIF network, that gives us a constraint that the recurrent matrix has to meet at initialization for the condition to be true.

\begin{lemma}\label{thm:cII}
Applying condition (II), which states that we want $Var[W_{rec}x_{t-1}] =  \ Var[W_{in}z_t]$, to an LIF network, and further assuming $\overline{x}=1/2$, and $\overline{w}_{in}=0$, it results in the constraint


\begin{align}\label{eq:condition_II}
    &Var[w_{rec}]  =  2(Var[z_t] + \overline{z}_t^2)\frac{n_{in}}{n_{rec}-1}Var[w_{in}] - \frac{1}{2}\overline{w}_{rec}^2
\end{align}
\end{lemma}

\begin{proof}
The second condition, is that the recurrent and the input contribution to the variance need to match

\begin{align} %\label{eq:condition_II}
    Var[W_{rec}x_{t-1}] =&  \ Var[W_{in}z_t]
\end{align}

\noindent where the variance is computed at each element, after the matrix multiplication is performed, following the method described in \cite{glorot2010understanding, he2015delving}. Similarly to what we did for the means in equation \ref{eq:indepx}, the matrix multiplication contributes to the scalar variance of neuron $i$ as

\begin{align}\label{eq:indepxvar}
    Var[W\boldsymbol{x}]_i=&Var[\sum_{j=1} w_{ij}x_j] \\
    =& \sum_{j=1} Var[ w_{ij}x_j] \\
    =& \sum_{j=1} Var[ wx]  \\
    =& n_W Var[ wx]
\end{align}

The second and third equality are a consequence of considering any neuron as mutually independent to any other at initialization, as done by \cite{glorot2010understanding, he2015delving}, and that justifies that the variance of the sum is the sum of the variances, and it justifies dropping the indices, to mean that the statistics are the same for each element. The number $n_W$ stands for the number of inputs that a neuron $i$ has through $W$, in the case of $W_{in}$, $n_W=n_{in}$, while in the case of $W_{rec}$, we have $n_W=n_{rec}-1$, since in our recurrent network, neurons are not connected to themselves.

Therefore the vector-wise condition II is equivalent to the element-wise


\begin{align}
    (n_{rec}-1)Var[w_{rec}x_{t-1}] =& \  n_{in}Var[w_{in}z_t]
\end{align}

Since the time dimension is averaged out, the time axis can be randomly shuffled, and the LIF activity is indistinguishable from a Bernouilli process through the mean and variance of the activity. Therefore if $\overline{x}_t=p$, we have $Var[x_t]=p(1-p)$ when averaged over time, with $p$ the probability of firing. Therefore it is as well true that $\overline{x_t^2}=Var[x_t] + \overline{x}_t^2 =p$.

We apply the fact that for independent $w, x$

\begin{align}
    Var[wx] = \overline{w^2}\ \overline{x^2} - \overline{w}^2\overline{x}^2
\end{align}


\noindent and assuming $\overline{w}_{in}=0$ and $p=1/2$ we have

\begin{align}
    Var[w_{rec}x_{t-1}] =& (Var[w_{rec}] + \overline{w}_{rec}^2)p-\overline{w}_{rec}^2p^2 \\ 
    =& \frac{1}{4}(2Var[w_{rec}] + \overline{w}_{rec}^2) \\
    Var[w_{in}z_t] =& (Var[z_t] + \overline{z}_t^2)Var[w_{in}]
\end{align}

Substituting in equation \ref{eq:condition_II} implies 

\begin{align}
    &\frac{1}{4}(2Var[w_{rec}] + \overline{w}_{rec}^2) =  (Var[z_t] + \overline{z}_t^2)\frac{n_{in}}{n_{rec}-1}Var[w_{in}] \\
    &Var[w_{rec}]  =  2(Var[z_t] + \overline{z}_t^2)\frac{n_{in}}{n_{rec}-1}Var[w_{in}] - \frac{1}{2}\overline{w}_{rec}^2 \label{appeq:CII}
\end{align}

\end{proof}

Therefore condition (II) led us to the constraint that $W_{rec}$ has to meet at initialization, equation \ref{appeq:CII}, for the condition to be true. The final equation further assumes that  $\overline{w}_{in}=0$ and $p=1/2$.

\subsection{SG dampening controls gradient maximum (III)}

\label{app:sgmax}

We apply condition (III) to the LIF network, which gives us a constraint that the dampening has to meet at initialization for the condition to be true.

\begin{lemma}\label{thm:cIII}
Applying condition (III), which states that we want $Max[\frac{\partial}{\partial \theta}y_t] = Max[\frac{\partial}{\partial \theta}y_{t-1}]$, to an LIF network, and assuming that (1) $\sigma'$ and $\frac{\partial}{\partial \theta}y_{t-1}$ are statistically independent and (2) we do not pass the gradient through the reset, it results in the constraint  

\begin{align}
    \gamma=&  
    \frac{1}{(n_{rec}-1)\hat{w}_{rec}}\Big( 1 -\hat{\alpha}_{decay} -\xi \cdot n_{in} \hat{w}_{in} \gamma_{in} \Big)
\end{align}

\noindent where $\xi$ is zero for the first layer and it's one for the other layers in the stack.

\end{lemma}

\begin{proof}


We want the maximal value of the gradient to remain stable, without exploding, when transmitted through time and through different layers


\begin{align}
    Max[\frac{\partial}{\partial \theta}y_t] = Max[\frac{\partial}{\partial \theta}y_{t-1}]
\end{align}

\noindent where when we write $\partial/\partial \theta$, we use $\theta$ as a placeholder for any quantity that we want to propagate through gradient descent.
Taking the derivative of the LIF definition and stopping the gradient from going through the reset we have 

\begin{align}\label{eq:difLIF}
    \frac{\partial}{\partial \theta}y_t =& \alpha_{decay} \frac{\partial}{\partial \theta}y_{t-1}(1-x_{t-1}) + W_{rec}\frac{\partial}{\partial \theta}x_{t-1} + \xi W_{in}\frac{\partial}{\partial \theta}z_{t} 
\end{align}

Here we introduce the symbol $\xi\in \{0,1\}$, where $\xi=1$ is used when $z_t$ comes from a trainable layer below, and $\xi=0$ when $z_t$ represents the data. We consider as well that


\begin{align}
    \frac{\partial}{\partial \theta}z_{t} =& \frac{\partial}{\partial \theta}\tilde{H}_{in}(y_{t}^{in}-\vartheta_{in})= \sigma'_{in}\frac{\partial}{\partial \theta}y_{t}^{in} \\
    \frac{\partial}{\partial \theta}x_{t-1} =& \frac{\partial}{\partial \theta}\tilde{H}(y_{t-1}-\vartheta)=\sigma'\frac{\partial}{\partial \theta}y_{t-1}
\end{align}

\noindent where  $\tilde{H}_{in},y_{t}^{in},\vartheta_{in}$ are the Heaviside, the voltage and the threshold of the layer below, $\sigma'=\frac{\partial\tilde{H}}{\partial v}$ is the surrogate gradient, and $\sigma'_{in}$ is the surrogate gradient from the layer below. Substituting in equation \ref{eq:difLIF}, then

\begin{align}
    \frac{\partial}{\partial \theta}y_t =& \alpha_{decay} \frac{\partial}{\partial \theta}y_{t-1}(1-x_{t-1}) + W_{rec}\sigma'\frac{\partial}{\partial \theta}y_{t-1} \\
    &+ \xi W_{in}\sigma'_{in}\frac{\partial}{\partial \theta}y_{t}^{in}  \label{appeq:derivativelif}
\end{align}

We use  $Max$ and $Min$ in a statistical ensemble sense, as the maximum/minimum value that a variable could take if sampled over and over again

\begin{align}
    Max[X] =& \sup_{x\sim p(x)} x \\
    Min[X] =& \inf_{x\sim p(x)} x 
\end{align}


With this definition, if $X,Y$ are independent random variables  $Max[X+Y]=Max[X] + Max[Y]$ and if they are positive $Max[XY]=Max[X]Max[Y]$.
We observe, as we did before for the variance and the mean of $Wx$, that


\begin{align}
    Max[Wx] =& n_WMax[wx] \\
    Min[Wx] =& n_WMin[wx]
\end{align}

We take the maximal value of $\frac{\partial}{\partial \theta}y_t$, we make the  assumption that $\sigma'$ and $\frac{\partial}{\partial \theta}y_{t-1}$ are statistically independent,  we use the fact that the highest value that the surrogate gradient can take is given by the dampening factor $Max[\sigma']=\gamma$, we denote as $\gamma_{in}$ the dampening factor of the layer below in the stack, and we take $Max[1-x_{t-1}]=1$:

\begin{align}
    Max[\frac{\partial}{\partial \theta}y_t] =& Max[\alpha_{decay} \frac{\partial}{\partial \theta}y_{t-1}] 
    \nonumber\\&+ (n_{rec}-1)Max[w_{rec}\sigma'\frac{\partial}{\partial \theta}y_{t-1}] \nonumber\\ &+ \xi n_{in}Max[ w_{in}\sigma'_{in}\frac{\partial}{\partial \theta}y_{t}^{in}] \\
    =& Max[\alpha_{decay} ] Max[\frac{\partial}{\partial \theta}y_{t-1}] 
    \nonumber\\&+ (n_{rec}-1)Max[w_{rec}] Max[\sigma'] Max[\frac{\partial}{\partial \theta}y_{t-1}] \nonumber \\
    &+ \xi n_{in} Max[w_{in}] Max[\sigma'_{in}]Max[\frac{\partial}{\partial \theta}y_{t}^{in}]  \\
    =& Max[\alpha_{decay} ] Max[\frac{\partial}{\partial \theta}y_{t-1}]
    \nonumber\\&+ (n_{rec}-1)Max[w_{rec}] \gamma Max[\frac{\partial}{\partial \theta}y_{t-1}] \nonumber \\
    &+\xi n_{in} Max[w_{in}] \gamma_{in}Max[\frac{\partial}{\partial \theta}y_{t}^{in}] 
\end{align}


\noindent where we used the fact that $\sigma'$ is positive in the second equality. We apply condition (III), which states that all maximal gradients are equivalent, and for cleanliness we use the notation $Max[x] = \hat{x}$



\begin{align}
    1 =& \hat{\alpha}_{decay}  +(n_{rec}-1) \hat{w}_{rec} \gamma + \xi n_{in}\hat{w}_{in} \gamma_{in}\\
    \gamma=&  
    \frac{1}{(n_{rec}-1)\hat{w}_{rec}}\Big( 1 -\hat{\alpha}_{decay} -\xi \cdot n_{in} \hat{w}_{in} \gamma_{in} \Big) \label{appeq:CIII}
\end{align}

\noindent where we only had to rearrange terms. 

\end{proof}

We set $\xi=0$ in the main text for readability and because we observed better performance with it.
This final equation \ref{appeq:CIII} gives the value that the dampening has to take to keep the maximal gradient value stable, namely, condition (III) true at initialization.


\subsection{SG sharpness controls gradient variance (IV)}

\label{app:backward}

We apply condition (IV) to the LIF network to constrain the choice of surrogate gradient variance. 


\begin{lemma}\label{thm:cIV}
Applying condition (IV), which states that we want $Var[\frac{\partial}{\partial \theta}y_t] = Var[\frac{\partial}{\partial \theta}y_{t-1}]$, to an LIF network, and assuming that (1) we do not pass the gradient through the reset, and (2) zero mean gradients at initialization, it results in the constraint  


\begin{align} \label{appeq:CIV}
    \overline{\sigma^{\prime 2}} =&  \frac{1-\frac{1}{2}\overline{\alpha^2}_{decay}-\xi \cdot n_{in}\overline{w_{in}^2}\ \overline{\sigma^{\prime 2}_{in}}}{(n_{rec}-1)\overline{w^2}_{rec} }  
\end{align}

\noindent where $\xi$ is zero for the first layer and is one for the other layers in the stack.

\end{lemma}



\begin{proof}


Condition (IV) states that we want the variance of the gradient to remain stable across time and layers.
Taking the derivative of the LIF we arrive at equation \ref{appeq:derivativelif}:

\begin{align}
    \frac{\partial}{\partial \theta}y_t =& \alpha_{decay} \frac{\partial}{\partial \theta}y_{t-1}(1-x_{t-1}) \nonumber \\&+ W_{rec}\sigma'\frac{\partial}{\partial \theta}y_{t-1} + \xi W_{in}\sigma'_{in}\frac{\partial}{\partial \theta}y_{t}^{in}
\end{align}

Taking the variance and assuming that the monomials in the polynomial are statistically independent, we can consider the variance of the sum to be the sum of the variances:

\begin{align}
    Var[\frac{\partial}{\partial \theta}y_t] =& Var[\alpha_{decay} \frac{\partial}{\partial \theta}y_{t-1}(1-x_{t-1})] \nonumber \\&+ Var[W_{rec}\sigma'\frac{\partial}{\partial \theta}y_{t-1}] \nonumber \\&+ Var[\xi W_{in}\sigma'_{in}\frac{\partial}{\partial \theta}y_{t}^{in}] \\
    Var[\frac{\partial}{\partial \theta}y_t] =& Var[\alpha_{decay} \frac{\partial}{\partial \theta}y_{t-1}(1-x_{t-1})] \nonumber \\&+ (n_{rec}-1)Var[w_{rec}\sigma'\frac{\partial}{\partial \theta}y_{t-1}] \nonumber \\&+ n_{in}Var[\xi w_{in}\sigma'_{in}\frac{\partial}{\partial \theta}y_{t}^{in}]
\end{align}

\noindent where $\xi=0$ if $w_{in}$ connects to the data and $\xi=1$ if it connects to the layer below in the stack. We denote by $\sigma'_{in}$  the surrogate gradient of the layer below.  


Assuming gradients $g$ with mean zero, and weights and gradients $w,g$ to be independent random variables at initialization:

\begin{align}
    Var[wg] =& (Var[g] + E[g]^2)(Var[w] + E[w]^2)-E[g]^2E[w]^2 \\
    =& Var[g](Var[w] + E[w]^2) \\
    =&  Var[g] E[w^2]
\end{align}

\noindent which gives


\begin{align}
    Var[\frac{\partial}{\partial \theta}y_t] =& E[\alpha_{decay}^2(1-x_{t-1})^2] Var[\frac{\partial}{\partial \theta}y_{t-1}] \nonumber \\&+ (n_{rec}-1)E[(w_{rec}\sigma')^2]Var[\frac{\partial}{\partial \theta}y_{t-1}] \nonumber \\&+ \xi \cdot n_{in}E[(w_{in}\sigma'_{in})^2]Var[\frac{\partial}{\partial \theta}y_{t}^{in}]
\end{align}

We apply condition IV, we want gradients to have the same variance, irrespective of the time step, or the neuron in the stack, which results in 


\begin{align}
    1=& \frac{1}{2}E[\alpha_{decay}^2]  + (n_{rec}-1)E[(w_{rec}\sigma')^2] \nonumber \\&+\xi \cdot n_{in}E[(w_{in}\sigma'_{in})^2]    \\
    1=& \frac{1}{2}E[\alpha_{decay}^2]  + (n_{rec}-1)E[w_{rec}^2]E[\sigma^{\prime 2}] \nonumber \\&+\xi \cdot n_{in}E[w_{in}^2]E[\sigma^{\prime 2}_{in}]  
\end{align}


\noindent where  we used the fact that for independent variables $X, Y$ we have $E[X^pY^q] = E[X^p]E[Y^q]$ in the third and fourth line. Using the notation $E[x]=\overline{x}$, the implied condition on the SG is 


\begin{align} 
    \overline{\sigma^{\prime 2}} =&  \frac{1-\frac{1}{2}\overline{\alpha^2}_{decay}-\xi \cdot n_{in}\overline{w_{in}^2}\ \overline{\sigma^{\prime 2}_{in}}}{(n_{rec}-1)\overline{w^2}_{rec} }  
\end{align}


\end{proof}

We therefore found the constraint that the second non-centered moment of the SG has to satisfy, equation \ref{appeq:CIV}, if we want condition IV to hold. We set $\xi=0$ in the main text for readability and because we observed better performance with it. We show how to relate it to the sharpness of the exponential SG in Appendix \ref{app:ivexp}.

\subsection{Applying Condition IV to the exponential SG}
\label{app:ivexp}

We show how we apply equation \ref{appeq:CIV}, to choose the sharpness of an exponential SG. For that we need to define the dependence of the variance of the SG with its sharpness. We use as equivalent notation for the surrogate gradient

\begin{align*}
    \sigma'(v) = \frac{\partial\tilde{H}(v) }{\partial v} = \gamma f(\beta\cdot v)
\end{align*}

We denote no dependency with the voltage in $\sigma'$, when we consider it as a random variable, and we introduce the dependency  $\sigma'(v)$ when we assume the voltage dependence is known. 
The moments of the surrogate gradient are given by

\begin{align}
E[\sigma^{\prime m}]
    =&\int \sigma^{\prime m}p(\sigma') d\sigma'\\
    =&\iint \sigma^{\prime m}p(\sigma'|v)p(v)dv d\sigma' \\
    =&\iint \sigma^{\prime m}\delta\Big(\sigma'-\sigma'(v)\Big) d\sigma'p(v)dv \\
    =&\int \sigma'(v)^mp(v)dv
\end{align}

\noindent where we used the marginalization rule in the second equality and in the third equality we used the fact that $\sigma$ is a deterministic function of $v$, so it inherits its randomness from $v$. We are going to assume as the non-informative prior a uniform distribution between the minimal and maximal values of $y_t-\vartheta$.



\begin{align}
\label{eq:beta}
    E[\sigma'(v)^m]=&\int \sigma'(v)^mp(v)dv\\
    =&\frac{1}{y_{max}-y_{min}}\int^{y_{max}-\vartheta}_{y_{min}-\vartheta} \sigma'(v)^mdv\\
    =&\frac{\gamma^m}{\beta(y_{max}-y_{min})}\int^{\beta(y_{max}-\vartheta)}_{\beta(y_{min}-\vartheta)} f(v')^mdv' 
\end{align}

\noindent where we used the non informative uniform prior assumption in the second equality and we used $\sigma'=\gamma f(\beta v)$ followed by the change of variable $v'=\beta v$ in the third equality. Considering the exponential SG we have that, calling $v_i$ one of the integration limits above, if $v_i$ is positive



\begin{align}
    \int^{v_i}_0 g(|v|)^mdv=&\int^{v_i}_0 g(v)^mdv
\end{align}

\noindent and if $v_i$ is negative

\begin{align}
    \int^{v_i}_0 g(|v|)^mdv=&\int^{v_i}_0 g(-v)^mdv \\
    =&-\int^{-v_i}_0 g(v)^mdv\\
    =&-\int^{|v_i|}_0 g(v)^mdv
\end{align}

\noindent where we made the change of variable $v\rightarrow -v$ in the second equality. Therefore


\begin{align}
    \int^{v_+}_{v_-} g(|v|)^mdv=&sign(v_+)\int^{|v_+|}_0 g(v)^mdv \\
    &- sign(v_-)\int^{|v_-|}_0 g(v)^mdv 
\end{align}

Given that for $v_i>0$ we have

\begin{align}
    \int^{v_i}_0 \text{exponential}(v)^mdv=&\int^{v_i}_0 e^{-2m|v|}dv\\
    =&\int^{v_i}_0 e^{-2mv}dv\\
    =&-\frac{1}{2m}e^{-2mv_i}+\frac{1}{2m}\\
    =&-\frac{1}{2m}e^{-2m|v_i|}+\frac{1}{2m}
\end{align}

\noindent then, for $v_+>0$ and $v_-<0$

\begin{align}\label{eqIVexp}
    \int^{v_+}_{v_-} \text{exponential}(v)^mdv=& \nonumber\\-\frac{1}{2m}e^{-2m|v_+|}-&\frac{1}{2m}e^{-2m|v_-|}+\frac{2}{2m}
\end{align}


\noindent where $v_+=\beta(y_{max}-\vartheta)$ and $v_-=\beta(y_{min}-\vartheta)$
and we show how to compute $y_{max}=Max[y_t]$ and $y_{min}=Min[y_t]$ in section \ref{sec:maxminy}. 
Notice how equation \ref{eq:beta}, shows a dependence of the SG variance  proportional to the square of the dampening and inversely proportional to the sharpness, which recalls our numerical results, where a high sharpness and a low dampening were preferred. Since the dependence with $\beta$ is quite complex, we find the $\beta$ that satisfies the last equation and equation \ref{appeq:CIV} through gradient descent.  This is how condition (IV) is used to fix the sharpness of the exponential SG.

\subsection{Applying Condition IV to the q-PseudoSpike SG}

Instead, when using (IV) to determine the tail-fatness of the SG, we set $\beta=1$ and use 

\begin{align}
    \int^{v_+}_{v_-} &\text{q-PseudoSpike}(|v|)^2dv=\\
    =&\int^{v_+}_{v_-} \text{q-PseudoSpike}(|v|)^2dv\\
    =& \int^{|v_+|}_0 \text{q-PseudoSpike}(v)^2dv \\
    &+ \int^{|v_-|}_0 \text{q-PseudoSpike}(v)^2dv\\
    =&- \frac{q+2|v_+|-1}{2(2q-1)}\frac{1}{\Big(1+\frac{2}{q-1}|v_+|\Big)^{2q}}\nonumber\\
    &- \frac{q+2|v_-|-1}{2(2q-1)}\frac{1}{\Big(1+\frac{2}{q-1}|v_-|\Big)^{2q}}+\frac{q-1}{(2q-1)} \label{eq:IVtail}
\end{align}

When inserted in equation \ref{appeq:CIV}, we use gradient descent to optimize $q$ and find the value that satisfies (IV).



\subsection{Maximal and Minimal voltage values achievable by the network at initialization}
\label{sec:maxminy}

We calculate the maximum and minimum value that the voltage $y$ can take, to be able to complete the argument for condition (IV), about the variance of the backward pass in section \ref{app:ivexp}.
First, we use $Max$ and $Min$ in a statistical ensemble sense, as the maximum/minimum value that a variable could take if sampled over and over again

\begin{align}
    Max[X] =& \sup_{x\sim p(x)} x \\
    Min[X] =& \inf_{x\sim p(x)} x 
\end{align}

When applied to the definition of LIF


\begin{align}
    Max[y_t] =& Max[\alpha_{decay} y_{t-1}(1-x_{t-1})] + Max[W_{rec}x_{t-1}] \nonumber \\&+ Max[b] + Max[W_{in}z_t]  \\
    =& Max[\alpha_{decay}]Max[ y_{t-1}] + (n_{rec}-1)Max[w_{rec}] \nonumber \\&+ Max[b] + n_{in}Max[w_{in}] \label{eq:allfire}\\
    Max[y_t] =& \frac{1}{1-Max[\alpha_{decay}]}\Big((n_{rec}-1)Max[w_{rec}] \nonumber \\&+ Max[b] + n_{in}Max[w_{in}]\Big) \label{eq:nonefire}
\end{align}

\noindent where we used the fact that if $x_t, z_t$ were sampled over and over, the maximum value that they could take is all neurons having fired at the same time, 
% while the minimum would be achieved when all the neurons in the layer would stay silent, 
we used the fact that $\alpha_{decay}, \vartheta>0$, and we assumed that the maximum is going to stay constant through time $Max[y_{t-1}]=Max[y_{t}]$. Notice that the maximal voltage is achieved when all neurons in the layer fired at $t-1$, equation \ref{eq:allfire}, except for the neuron under study, that stayed silent at $t-1$, to have \ref{eq:nonefire}. Similarly for the bound to the minimal voltage:

\begin{align}
    Min[y_t] =& Min[\alpha_{decay} y_{t-1}(1-x_{t-1})] \nonumber \\&+ (n_{rec}-1)Min[w_{rec}x_{t-1}] + Min[b] \\
    &+ n_{in} Min[w_{in}z_t] \\
    =& Max[\alpha_{decay}]Min[ y_{t-1}] \nonumber \\&+ (n_{rec}-1)Min[w_{rec}] + Min[b] \\&+ n_{in}Min[w_{in}]\\
    Min[y_t] =& \frac{1}{1-Max[\alpha_{decay}]}\Big( (n_{rec}-1)Min[w_{rec}] \nonumber \\&+ Min[b]  + n_{in}Min[w_{in}]\Big) 
\end{align}


Second, we consider another definition of $Max$ and $Min$, where we consider the maximum value achievable by the current sample from the weight distribution. The real maximum value of the voltage will be achieved when the presynaptic neurons to fire are those that are connected with positive weight, we then have that our equation turns to


\begin{align}
    Max[y_t]_i =& \alpha_{decay, i} Max[y_{t-1}]_i + \sum_j ReLU[W_{rec}]_{ij} \nonumber \\&+ b_i + \sum_j ReLU[W_{in}]_{ij} \\
    Max[y_t]_i =& \frac{1}{1-\alpha_{decay,i}}\Big(\sum_j ReLU[W_{rec}]_{ij} \nonumber \\&+ b_i + \sum_j ReLU[W_{in}]_{ij}\Big) 
\end{align}

\noindent where we refer as $\sum_{j} ReLU[W_{rec}]_{ij}$ the sum over columns, where we have typically ommitted the index $i$ for the element of the vector for cleanliness in the rest of the article. The case for the minimum is analogous


\begin{align}
    Min[y_t]_i =& \alpha_{decay,i} Min[y_{t-1}]_i + Min[W_{rec}x_{t-1}] \nonumber \\&+ b_i + Min[W_{in}z_t] \\
    Min[y_t]_i =& \frac{1}{1-\alpha_{decay,i}}\Big( -\sum_{j} ReLU[-W_{rec}]_{ij} \nonumber \\&+ b_i -\sum_{j} ReLU[-W_{in}]_{ij} \Big) 
\end{align}

With this we showed how we calculated the maximal and minimal value of the voltage, to be able to use condition (IV) to define the sharpness of the SG in section \ref{app:ivexp}.



\section{Applying conditions I-IV to an alternative definition of reset}
\label{app:mulreset}

We want to show how the constraints on the weights initialization and on the SG choice change, when the neuron model definition changes. We will use the notation $i_t  = W_{rec}x_t + W_{in}z_t + b$. The reset used by \cite{zenke2021remarkable} is multiplicative to the voltage after having summed the current

\begin{align}
    y_t =&  (\alpha_{decay}y_{t-1} +i_t)(1 - x_{t-1})
\end{align}

\noindent that we will call \textit{post-reset}. Instead,  \cite{wozniak2020deep} uses a LIF with a different definition of reset

\begin{align}
    y_t =&  \alpha_{decay}y_{t-1}(1 - x_{t-1}) +i_t
\end{align}

\noindent that we will call \textit{pre-reset}, since it resets before applying the new current. Another example is given by \cite{lsnn}, that uses a subtractive reset


\begin{align}
    y_t =&  \alpha_{decay}y_{t-1} +i_t -\vartheta x_{t-1} 
\end{align}

\noindent and we will call it \textit{minus-reset}.

The first definition performs as well one refractory period, while the second does not result in a $y_t$ clamped to zero when $x_t=1$.
The factor $(1 - x_t)$ takes the voltage exactly to zero every time the neuron has fired, zero being the equilibrium voltage. What is interesting about this form of reset is that the voltage is reset exactly to $y=0$ after firing, while with the subtractive reset it is not the case. 
% In our experience pre-reset gave a particularly good performance for language modeling. 
We consider training without passing the gradient through the reset, since \cite{zenke2021remarkable} finds better performance in that setting, and it makes the maths cleaner.
% We can retrieve the new initializations from the ones corresponding to the additive reset by observing that we only have to change $\alpha_{decay}\rightarrow\alpha_{decay} (1 - x_t)$ and removing the previously used subtractive reset.  
The equations that result from the 4 desiderata for this three LIF definitions are as follows



\vspace{.7cm}
\textbf{Post-reset:}
\vspace{-.8cm}

{\small
\begin{align*}
&\hspace{4cm} y_t =  (\alpha_{decay}y_{t-1} +i_t)(1 - x_{t-1})\\  \nonumber \\
    &\overline{w}_{rec}=
 \frac{2}{n_{rec}-1}\Big(1-\alpha_{decay}\Big)\vartheta && \text{I}\\
    &Var[w_{rec}]  =  2(Var[z_t] + \overline{z_t}^2)\frac{n_{in}}{n_{rec}-1}Var[w_{in}] - \frac{1}{2}\overline{w}_{rec}^2 && \text{II}\\
    &\gamma=   \frac{1}{(n_{rec}-1)\hat{w}_{rec}}\Big( 1 -\alpha_{decay}-\xi n_{in}\hat{w}_{in} \gamma_{in} \Big)&& \text{III}\\
    &\overline{\sigma^{\prime 2}} = \frac{2-\alpha_{decay}^2-\xi n_{in}\overline{w_{in}^2} \ \overline{\sigma^{\prime 2}_{in}}}{(n_{rec}-1)\overline{w^2}_{rec}}  && \text{IV}
\end{align*}
}


\vspace{.7cm}
\textbf{Pre-reset:}
\vspace{-.8cm}

{\small
\begin{align*}
&\hspace{4cm} y_t =  \alpha_{decay}y_{t-1}(1 - x_{t-1}) +i_t\\  \nonumber \\
    &\overline{w}_{rec}=
    \frac{1}{n_{rec}-1}\Big(2-\alpha_{decay}\Big)\vartheta && \text{I}\\
    &Var[w_{rec}]  =  2(Var[z_t] + \overline{z}_t^2)\frac{n_{in}}{n_{rec}-1}Var[w_{in}] - \frac{1}{2}\overline{w}_{rec}^2 && \text{II}\\
    &\gamma=   \frac{1}{(n_{rec}-1)\hat{w}_{rec}}\Big( 1 -\alpha_{decay}-\xi n_{in}\hat{w}_{in} \gamma_{in} \Big)&& \text{III}\\
    &\overline{\sigma^{\prime 2}} = \frac{1-\frac{1}{2}\alpha_{decay}^2-\xi n_{in}\overline{w_{in}^2} \ \overline{\sigma^{\prime 2}_{in}}}{(n_{rec}-1)\overline{w^2}_{rec}}  && \text{IV}
\end{align*}
}

\vspace{.7cm}
\textbf{Minus-reset:}
\vspace{-.8cm}

{\small
\begin{align*}
&\hspace{4cm} y_t = \ \alpha_{decay}y_{t-1} +i_t -\vartheta x_{t-1} \\ \nonumber \\
    &\overline{w}_{rec} = 
    \frac{1}{n_{rec}-1}\Big(3-2\alpha_{decay}\Big) \vartheta && \text{I}\\
    &Var[w_{rec}]  =  2(Var[z_t] + \overline{z}_t^2)\frac{n_{in}}{n_{rec}-1}Var[w_{in}] - \frac{1}{2}\overline{w}_{rec}^2 && \text{II}\\
    &\gamma =    \frac{1}{(n_{rec}-1)\check{w}_{rec}-\vartheta}\frac{\check{w}_{rec}}{\hat{w}_{rec}}\Big( 1 -\alpha_{decay}-\xi n_{in}\hat{w}_{in} \gamma_{in} \Big)&& \text{III}\\
    &\overline{\sigma^{\prime2}} =   \frac{1-\alpha_{decay}^2-\xi n_{in}\overline{w_{in}^2}\ \overline{\sigma_{in}^{\prime2}}}{(n_{rec}-1)\overline{w^2}_{rec} + \vartheta^2}  && \text{IV}
\end{align*}
}

To have the conditions when the gradient does not pass through the reset, put $\vartheta=0$ in (III) and (IV), but not in (I).

\section{ALIF and sLSTM models}
\label{app:alifsLSTM}

To study the variability of SG training with the architecture of choice, we tested different SG shapes on the ALIF and sLSTM networks.
We used the following ALIF definition

\begin{align}    
\boldsymbol{y}_{t,l} =& \boldsymbol{\alpha}_{decay,l}^y \boldsymbol{y}_{t-1,l}  \nonumber\\ &+W_{rec,l}\boldsymbol{x}_{t-1,l}  + W_{in,l}\boldsymbol{x}_{t-1,l-1} + \boldsymbol{b}_l \nonumber\\&- \boldsymbol{\vartheta}_{t-1,l} \ \boldsymbol{x}_{t-1,l}\\
\boldsymbol{\vartheta}_{t,l} =& \boldsymbol{\alpha}^{\vartheta}_{decay,l} \boldsymbol{\vartheta}_{t-1,l} +\boldsymbol{b}^{\vartheta}_l + \boldsymbol{\beta}_l \boldsymbol{x}_{t-1,l} 
\end{align}    

\vspace{.5cm}

\noindent where we initialized $W_{rec}, W_{in}$ as Glorot Uniform, $b_l=0$, $\alpha_{decay,l}^y=4\cdot10^{-5}$,  $\alpha_{decay,l}^\vartheta=0.992$ for the SHD task and $\alpha_{decay,l}^\vartheta=0.98$ for the sl-MNIST task, $b^{\vartheta}_l =0.01$, and $\beta_l=1.8$.

The LSTM implementation that we used is the following


\begin{align}
    i_t =& \sigma_g(W_ix_t + U_ih_{t-1}+b_i)\\
    f_t =& \sigma_g(W_fx_t + U_fh_{t-1}+b_f)\\
    o_t =& \sigma_g(W_ox_t + U_oh_{t-1}+b_o)\\
    \tilde{c}_t =& \sigma_c(W_cx_t + U_ch_{t-1}+b_c)\\
    c_t =& f_t\circ c_{t-1}+ i_t\circ \tilde{c}_t\\
    h_t =& o_t\circ\sigma_h(c_t)
\end{align}

\vspace{.5cm}

The dynamical variables $i_t, f_t, o_t$ represent the input, forget and output gates, that prevent representations and gradients from exploding, while $c_t, h_t$ represent the two hidden layers of the LSTM, that work as the working memory and are maintained and updated through data time $t$.
To construct the spiking version of the LSTM (sLSTM) we turned  the activations into $\sigma_g(x)=H(x)$ and $\sigma_c=\sigma_h=2H(x)-1$. The matrices $W_j, U_j$ are initialized with Glorot Uniform initialization, and the biases $b_j$ as zeros, with $j\in\{i, f, o, c\}$.

\section{More on Sparsity}\label{app:more_sparsity}

We investigate if the role of sparsity remains consistent across SG shapes in Fig. \ref{fig:sparsity_and_shape}, and across tasks in Fig. \ref{fig:sparsity_and_task}. Notice that Fig. \ref{fig:sparsity} is repeated in Fig. \ref{fig:sparsity_and_shape} and \ref{fig:sparsity_and_task} to ease the comparison.


\begin{figure}
    \centering
    \vspace{.55cm}
    \includegraphics[width=.45\textwidth]{images/surrogate_grads/t__sparsity_tsgfastsigmoidpseudod_tSHD.pdf}
    \includegraphics[width=.45\textwidth]{images/surrogate_grads/t__sparsity_tsgoriginalpseudod_tSHD.pdf}
    \includegraphics[width=.45\textwidth]{images/surrogate_grads/t__sparsity_tsgexponentialpseudod_tSHD.pdf}
    \caption{\textbf{Sparsity role is not consistent across SG shapes.} When we fix the task to be the SHD task, we see that the preference for high or low firing rates at initialization and after training on the test set, depends on the SG of choice. As we saw in the main text, 
    the derivative of the fast sigmoid has preference for high $\rho_i$, since, at each layer $l$, the final loss correlation $r_l$ with the firing rate at initialization is negative.
    Instead, the triangular SG has preference for low $\rho_i$ since the correlation is positive, while for the exponential SG, $\rho_i$ does not seem to correlate with final performance, given that the correlations $r_l$ are not significant (in bold when significant).}
    \label{fig:sparsity_and_shape}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=.45\textwidth]{images/surrogate_grads/t__sparsity_tsgfastsigmoidpseudod_tSHD.pdf}
    \includegraphics[width=.45\textwidth]{images/surrogate_grads/t__sparsity_tsgfastsigmoidpseudod_tsl-MNIST.pdf}
    \includegraphics[width=.45\textwidth]{images/surrogate_grads/t__sparsity_tsgfastsigmoidpseudod_tPTB.pdf}
    \caption{\textbf{Sparsity role is consistent across tasks.} Here we fix the SG shape to the derivative of the fast sigmoid and we change the task. On sl-MNIST, we see a similar trend than on SHD, where high initial firing rate is preferred for better performance when sparsity is encouraged. Encouraging sparsity has a negative effect on learning language modeling on the PTB task. However, when no sparsity is encouraged, best performance on PTB is still at $\rho_i=0.5$.}
    \label{fig:sparsity_and_task}
\end{figure}




% \end{appendices}
