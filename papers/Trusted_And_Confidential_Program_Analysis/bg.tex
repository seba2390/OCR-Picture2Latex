
\subsection{Trusted Computing}
\label{subsec:tc}
\emph{Trusted computing}~\cite{mitchell2005trusted, tc2013trusted} describes a number of technologies proposed to 
mitigate security threats. Unlike conventional passive approaches such as firewalls, 
malware detection, and intrusion detection, trusted computing takes a more active approach to 
addressing these threats, \ie, by establishing trust on a solid basis (typically referred to as the \emph{root of trust} and implemented using both 
hardware and software) and then expanding this chain of trust to cover the entire 
computing system. Trusted Execution Environments represent arguably the most flexible and practical technology in this area. Some modern processors offer hardware primitives that support the execution of \emph{isolated computations}, that is, the code and data they use are encrypted and integrity-protected. These TEE implementations are designed so that even the operator of the machine where such an execution takes place is unable to undetectably tamper with, or learn secrets manipulated by this execution. We use the terms TEE and TEE implementation interchangeably. Note that a TEE can be managing and executing multiple isolated computations concurrently. Examples of TEE implementations available are
Intel’s Software Guard Extensions (SGX)~\cite{costan2016intel}, 
AMD’s Secure Encrypted Virtualization (SEV)~\cite{sev2020strengthening}, and 
ARM’s TrustZone~\cite{pinto2019demystifying}.  Each of them is designed to address different application scenarios, but they all share similar core capabilities. We introduce some of these common building blocks as follows. 

These implementations rely on special instructions to load the isolated computation's code and data, in encrypted form, into the \emph{main memory}, while \emph{measuring} them in the process, and execute it. They isolate computations at different levels of granularity. For instance, while AMD SEV isolates an entire virtual machine (VM), Intel SGX isolates part of a (operating-system) process. This measurement and the encrypted memory pages are created, managed, and protected by the TEE. The \emph{attestation} process plays a key part in establishing the chain of trust. Roughly speaking, it generates cryptographically-protected evidence attesting that a computation with a given measurement has been properly isolated; it also attests the authenticity of the TEE implementation - hardware and software components - and its capabilities. By properly verifying this evidence, an entity interacting with this computation can be confident of its isolation and, consequently, of the guarantees that follow. We detail this process later. For instance, in the case of AMD SEV, the attestation process can be used to establish that a VM has been properly set up by checking the boot process and the elements it depends upon have the expected measurements, i.e. OS image, installed programs and data, etc. The isolation offered by TEE implementations is restricted to the main memory. The developer of the isolated computation is responsible for encrypting data on disk.


\subsection{Program Analysis}
\label{subsec:pa}
In general, program analysis develops on methods and technologies at establishing the relationship 
between a given software program and expected properties, \eg, correctness, robustness, safety 
and liveness. Typical program analysis tasks are program optimization, vulnerability detection, 
and formal verification. Considering the approaches taken, program analysis technologies can be 
categorized as static program analysis, dynamic program analysis, and both in combination.
Static program analysis technology finishes its task by not running or executing the target program. 
Common static program analysis methods are control-flow analysis, data-flow analysis, 
abstract interpretation, type analysis, and pointer analysis. All of these methods first model 
the given program and its expected property abstractly as a set of mathematical constraints, for which the analysis task boils down to establishing satisfiability. 
In contrast, dynamic program analysis fulfills its goal by examining actual runs of the target program  and checking for the expected properties. 
Fuzzing, symbolic execution, and runtime monitoring are among the
most frequently used dynamic program analysis technologies. While static and dynamic program analysis 
have their strengths in efficiency and precision, respectively, neither of them is able to tackle all 
real-world problems. Thus, there is another category of \emph{hybrid} program analysis technologies which 
combines both approaches. \tcpa encompasses all of these.

\begin{figure*}[h]
	\centering
	\includegraphics[width=.8\linewidth]{protocol_alt.pdf}
	\caption{\label{fig:protocol}The protocol of \tcpa.}
\end{figure*}