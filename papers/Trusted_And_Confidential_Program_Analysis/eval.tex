\subsection{Benchmark and Evaluation Setup}
\label{subsec:bench}

\myparagraph{Benchmark}
We have conducted a preliminary evaluation of \tool. The benchmarks used in the 
evaluation are shown in Table~\ref{tab:benchmark}. We used 8 examples 
from different application domains with \neval web assembly files in total. 
The size of benchmark files varies from 2KB to 840KB. We also counted the number of 
\wasm instructions in all files, which manifested a relatively large difference across 
different test cases. The largest one (Zxing) has over 381K instructions, while the smallest 
(Snake) only includes 528 instructions. The benchmarks and results are publicly available at \dataset.

\input{results}

\myparagraph{Evaluation Setup}
All the experiments were performed on two environments, \ie, with and without \tee, respectively. 
The \tee environment was set up on Google Cloud confidential computing 
platform\footnote{https://cloud.google.com/confidential-computing}, which uses 
the AMD Epyc processor and SEV-ES\footnote{https://developer.amd.com/sev/} as the underlying TEE setting. 
The cloud machine was configured with dual 2.25GHz cores, 8GB memory and a Ubuntu 18.04 
operating system. Moreover, the non-\tee environment carried Intel i9 processor with dual 2.3GHz 
cores, 8GB memory and a Ubuntu 18.04 operating system. All the computation was executed by only 
one core of the processor in our evaluation.

\subsection{Evaluation Results}
\label{subsec:results}
In the evaluation, we ran \tool with and without the support of TEE (\ie, AMD SEV 
in our case) on all benchmarks. For a test file $f$, \tool performs a systematic 
program analysis based on symbolic execution~\cite{king1976symbolic} to explore the 
state space of $f$ and create a semantic abstraction as well. That said, the execution 
did not include detection of specific types of bugs or errors, as in many existing program 
analyzers. The goal of this evaluation was to understand the performance trade-off with the 
design of \tcpa, rather than assessing the effectiveness of a certain bug-detection algorithm.
With a framework such as \tool, the implementation of a detector is a straightforward task 
even in the context of \tcpa.

\input{result-time}

\myparagraph{Time Overhead}
The time cost with and without a TEE is described in Table~\ref{tab:time}. 
In the evaluation, we observed the smallest 7.7\% overhead in the case of \texttt{tfjs-backend}, 
while the largest case was 367.5\% for \texttt{imagequant}. The average time overhead was 
139.3\% on all benchmark files. For a subset of the test cases, files with larger sizes introduced 
bigger time overhead as expected. For instance, \texttt{binjgb}, \texttt{module1}, \texttt{avif\_dec} 
led to an increasing level of overhead with a growing size and number of instructions. However, 
there were exceptions in the evaluation where big files manifested small overheads. For example, 
in the case of \texttt{mozjpeg\_enc} (217 KB), running \tool is 95.1\% slower than the non-TEE version. 
For the case of \texttt{rotate} (14 KB) which is only 6.5\% as large as \texttt{mozjpeg\_enc}, the 
overhead was 107.0\% that amounts to a relative 12.5\% growth. Further discussions on root causes of 
the overhead can be found below.

%Specifically, 
%as the size of the benchmark file increased, the overhead introduced by \tool grows 
%accordingly. In our evaluation, the smallest overhead was 15.3\%, while the largest 
%was 86.8\%. The average overhead of \tool was 42.8\%, which is acceptable in practical 
%applications.

\input{result-memory}


\myparagraph{Memory Overhead}
In addition to time cost, we analyzed the memory overhead with \tool in our evaluation. 
Similarly, the analysis was conducted with and without the support of TEE, 
as shown in Table~\ref{tab:memory}. In general, majority of the overheads were below 45\%. 
More specifically, the overheads for half of the test cases were even less than 8\%, which 
we believe is highly acceptable in practical scenarios. On the other hand, there were two cases 
that manifested a 2X and 4X overheads, although the actual memory used were not big, \ie, 10.8MB 
and 36.4MB respectively. The \texttt{tfjs-backend} file was particularly interesting due to the 
fact that \tool consumed less memory with a TEE than the non-TEE version. Detailed explanations 
are given in the following section.


\subsection{Discussions}
\label{subsec:discuss}
We now describe a further discussion on the empirical results with \tool to help understand its 
performance manifested in the evaluation. 

\myparagraph{General Explanation}
First of all, the runtime overheads in general introduced by \tool in our evaluation are easy to understand. 
In the case of time overhead, the execution of \tcpa protocol was encapsulated in a \tee environment, 
which encrypts and decrypts memory accessed by the running program, \ie, in our case the \tool implementation, 
therefore should last longer than running \tcpa without a \tee (139.3\% as described in Table~\ref{tab:time}), 
depending on how efficient the \tee is realized. 
On the other hand, \tool did not manifest a higher level of memory consumption than a non-\tee implementation 
for the majority of test cases used in the evaluation as shown in Table~\ref{tab:memory}, due to the fact that 
encryption and decryption of memory in a \tee are not memory-intensive procedures thus commonly require little 
extra memory in running \tcpa.

\myparagraph{Special Cases}
Despite the general analysis of evaluation results, we did observe that there were exceptions that seemed not to
be consistent with other cases. As shown in Table~\ref{tab:time}, it was much slower for \tool to process 
\texttt{avif\_dec}, \texttt{imagequant} and \texttt{asm-dom} than other test files. The average time overhead for 
the three is 273.0\% which almost doubles the number of total average. As explained above, the time overhead is 
mainly resulted from encryption and decryption of memory used by \tool. More specifically, the overhead is closely 
correlated to the memory complexity of analysis (\eg, the amount of memory used and the frequency to access it) 
adopted in the \tcpa realization, \ie, a symbolic-execution based analysis. Like many other well-designed symbolic 
engines, \tool uses a variety of specific data structures to store intermediate information of program analysis, 
\eg, states of analysis, symbolic contexts, path conditions, \etc Particularly, \tool introduced a graph-based structure to 
separate the modeling of a given program and its symbolic execution process. While the advantage of such design is 
to have better composability via integration with different symbolic execution engines and backend analyzers, it 
inevitably increases the level of memory consumption and access frequency. Moreover, abnormal time overheads were 
partially attributed to the evaluation setting as well. We use an illustrative example in Figure~\ref{fig:setting} 
to explain the cause. 

\begin{figure}[h]
\centering
\includegraphics[width=.85\linewidth]{coverage.pdf}
\caption{\label{fig:setting}An evaluation setting of covering two paths with a specified timeout on each path.}	
\end{figure}

Figure~\ref{fig:setting} describes an evaluation setting to cover two paths in the given program and the exploration 
of each path is bounded within a specified timeout, \eg, one second. Although settings may vary across different 
program analyzers to deal with specific use cases, they commonly share similar fundamental parameters, \eg, level of 
coverage and timeout for SMT solving. In particular, Figure~\ref{fig:setting} demonstrates a scenario where setting 
overhead is introduced. Specifically, a program analyzer without \tee (left) manages to cover the first and second 
paths of a given program and then finishes the analysis without exploring the remaining paths. However, in the case 
on the right, the program analyzer with \tee (right) manages to cover the first path of a given program, fail at 
the second and third due to timeouts of SMT solving, and then cover the last. In such cases, although overheads 
on two visited paths are relatively small, the total overhead becomes much bigger because of unfinished explorations 
on the other two paths.

\input{result-validate}

In terms of memory overhead, the evaluation manifested abnormal results as well. Specifically, \texttt{imagequant} 
and \texttt{zxing} introduced a large overhead while \texttt{tfjs-backend} even showed a negative overhead, \ie, 
\tool was faster than the non-\tee version. Commonly, \tcpa does not introduce a high level of memory overhead because 
the encryption process, \eg, AES as used in our case with AMD SEV, often generates ciphertexts with similar sizes as 
plaintexts. However, there might be cases as well where ciphertexts are bigger with specific padding strategies. Another 
factor to potentially affect the measurement of memory overhead is garbage collection in virtual machines. For cases where 
memory consumption is measured right after a garbage collection process, we might have a much smaller number than expected. 
Further investigation on such cases is left as future work.

\myparagraph{Preliminary Validation}
Since the implementation of \tool is non-trivial, we conducted a preliminary validation with a small group of 
simple test cases to justify the root cause analysis as described above. The validation is shown in Table~\ref{tab:validate}.



Specifically, the test cases used in the validation included the following programs:
\begin{itemize}[leftmargin=*]
\item \texttt{self\_addition}: increment a variable $10^7$ times with a specific value

\item \texttt{array\_addition}: add to $10^7$ elements in a given array

\item \texttt{quick\_sort}: quick sort a given list

\item \texttt{constraint\_addition}: a program with an addition constraint for SMT solving

\item \texttt{constraint\_division}: a program with an division constraint for SMT solving
\end{itemize}

As shown in the first two rows of Table~\ref{tab:validate}, \texttt{self\_addition} manifested 
a relatively lower level of time overhead (13.6\%) compared to \texttt{array\_addition} (350.0\%). The gap 
is resulted from different structures of memory accessed by both programs. While 
\texttt{self\_addition} only manipulated a single unit of memory, \texttt{array\_addition} 
is allocated with a consecutive memory space therefore each access to it requires addressing 
with the starting point and the offset. As a result, running \texttt{array\_addition} with \tee 
was much slower than without \tee due to encryption of a more complicated memory. In the case 
of \texttt{self\_addition}, \tee did not slow down too much of the execution. Furthermore, a similar 
explanation can apply to \texttt{quick\_sort}, \ie, the third row of Table~\ref{tab:validate}. Since 
the memory used by a quick sorting algorithm commonly includes a pivot and sub-lists of a given list, 
it introduced a high level of time overhead (314.8\%) as \texttt{array\_addition}. Moreover, the last 
two rows of Table~\ref{tab:validate} demonstrated two cases with simple and complicated path 
constraints for SMT solving, respectively. While \texttt{constraint\_addition} generated a 
constraint with addition, \texttt{constraint\_division} was a division constraint. Therefore, it took 
longer for \tool to solve \texttt{constraint\_division} than \texttt{constraint\_addition}. As 
explained in Figure~\ref{fig:setting}, \tool managed to solve the division constraint without \tee 
but failed with \tee due to timeout (which could be verified based on runtime logs). 
Therefore, the time overhead in the forth row is larger, \ie, 92.3\%. On the 
other hand, the addition constraint can be solved with and without \tee thus did not manifest a large 
overhead in the last row. In terms of memory, all cases introduced slight overheads, which can be 
explained by the fact that the memory encryption process enforced by \tee (\ie, AMD SEV) 
did not require much extra memory space.