%!TEX root = 00-NVGF.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                   AUTHORSHIP                   %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Authorship Attribution}

\textbf{Problem objective.} Consider a set $\stTheta_{a}$ of texts that are known to be written by author $a$. Given a new text $\theta \notin \stTheta_{a}$ the objective is to determine whether $\theta$ was written by $a$ or not.

\textbf{Approach.} The approach is to leverage word adjacency networks (WAN) built from the set of known texts $\stTheta_{a}$ to build a graph support $\stG$, and then use the word frequency count of the function words in $\theta$ as the graph signal $\vcx$. Then, $\vcx$ is processed through a GNN $\fnPhi$ (in any of the variants discussed in Section~\ref{sec:archit}) to obtain a predictor of the text being written by author $a$.

%%%%%%%%%%%%%%%%%%%%%%
%%%%    FIGURE    %%%%
%%%%%%%%%%%%%%%%%%%%%%
%%
\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/allAuthorsCompare.pdf}
    \caption{Comparison between architectures for all authors. It is observed that, in most cases, the Learn NVGF exhibits significantly better performance than the GAT \cite{Velickovic2018-GAT}, which in turn is better than the GCN \cite{Kipf2017-GCN}, and all of them are better than the SGC \cite{Weinberger2019-SGC}. The error bars reflect one third of the estimated standard deviation.}
    \label{fig:app:allAuthors:compare}
\end{figure*}
%%%%
%%%% End of FIGURE %%%%
%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Graph construction.} The WAN can be modeled by a graph $\stG$ where the set of nodes $\stV$ consists of function words (i.e., words that do not carry semantic meaning but express grammatical relationships among other words within a sentence, e.g., ``the'', ``and'', ``a'', ``of'', ``to'', ``for'', ``but''). The existence of an edge connecting words and the corresponding edge weight are determined as follows. Consider each text $\theta \in \stTheta_{a}$ and split it into a total of $S$ sentences $\{\fnomega_{\theta}^{s}\}_{s = 1}^S$ where each sentence $\fnomega_{\theta}^{s}: \fdN \to \stV \cup \{\emptyset\}$ gives the function word present in each position within a sentence, or $\emptyset$ if the word is not a function word. Then, given a discount factor $\alpha \in (0,1)$ and a window length $D$, the edge weight $w_{ij}$ between nodes $v_{i}$ and $v_{j}$ is computed as %
% eq:app:authorWeights
\begin{equation} \label{eq:app:authorWeights}
%
\begin{aligned}
    w_{ij} = \sum_{t : \theta_{t} \in \stTheta_{a}} & \sum_{s,e} \indFn \{\fnomega_{\theta_{t}}^{s} (e) = v_{i}\} \\ & \sum_{d=1}^{D} \alpha^{d-1} \indFn\{\fnomega_{\theta_{t}}^{s}(e+d) = v_{j}\}
\end{aligned}
%
\end{equation}
%
where $\indFn\{\stA\}$ is the indicator function that takes value $1$ when condition $\stA$ is met and 0 otherwise, see \cite{Segarra2015-Authorship}. Equation \eqref{eq:app:authorWeights} essentially computes each weight by going text by text $\theta_{t} \in \stTheta_{a}$ and sentence by sentence $s \in \{1,\ldots,S\}$, looking position by position $e$ for the corresponding word $\fnomega_{\theta_{t}}^{s}(e)$ to match the function word $v_{i}$. Once the word $v_{i}$ is matched, the following $D$ words in the window length are looked at and, if the $(e+d)^{\text{th}}$ word matches $v_{j}$, then the discounted weight $a^{d-1}$ is added. In this way, not only co-occurrence of words, but also their proximity counts in establishing the WAN. Note that the edge weight function \eqref{eq:app:authorWeights} is asymmetric, which results in a directed graph.

\textbf{Graph signal processing description.} The support matrix is chosen to be
% eq:app:authorSupport
\begin{equation} \label{eq:app:authorSupport}
    \mtS = \frac{1}{2}(\mtD^{-1} \mtW + \mtW^{\Tr} \mtD^{-1})
\end{equation}
%
where the $\mtW$ is the adjacency matrix with entry $(i,j)$ equal to $w_{ij}$ and $\mtD = \diag(\mtW \vcOnes)$ is the degree matrix. The operation $\mtD^{-1}\mtW$ normalizes the matrix by rows, and the support matrix $\mtS$ comes from symmetrizing the matrix by adopting one half of the weight on each direction. The graph signal $\vcx$ contains a normalized word frequency count for each word
% eq:app:authorSignal
\begin{equation} \label{eq:app:authorSignal}
    [\vcx]_{i} = \frac{\sum_{s,e} \indFn\{\fnomega_{\theta}^{s}(e) = v_{i}\}}{\sum_{j : v_{j} \in \stV \sum_{s,e}} \indFn\{\fnomega_{\theta}^{s}(e) = v_{j}\}}.
\end{equation}
%
Note that the graph signals are normalized by the $1$-norm and can therefore be interpreted as the probability of finding the function word $v_{i}$ in text $\theta$.

\textbf{Dataset and code.} The dataset is presented in \cite{Segarra2015-Authorship}, and is publicly available at \url{http://github.com/alelab-upenn/graph-neural-networks/tree/master/datasets/authorshipData}. The dataset consists of $21$ authors from the $\text{19}^{\text{th}}$ century. The corpus of each author is split in texts of about $1000$ words. The WANs and the word frequency count for each text are already present in the dataset. The texts are split, at random, in $95\%$ for training and $5\%$ for testing. The weights of the WANs of the $95\%$ texts selected for training are averaged to obtain an average WAN from which the support matrix $\mtS$ is obtained by following \eqref{eq:app:authorSupport}. The resulting support matrix is further normalized to have unit spectral norm. Note that no text from the test set is used in building the WAN. From the texts in the training set, $8\%$ are further separated to build the validation set. Denote by $N_{a}^{\text{train}}$, $N_{a}^{\text{valid}}$, and $N_{a}^{\text{test}}$ the number of texts in the training, validation, and test set, respectively.  The word frequency counts for each text are normalized as in \eqref{eq:app:authorSignal} and used as graph signals. A label of $1$ is attached to these signals to indicate that they have been written by author $a$ in a supervised learning context. To complete the datasets, an equal number of texts are obtained at random from other contemporary authors, and their frequency word counts are normalized and incorporated into the corresponding sets and assigned a label of $0$. In this way, the resulting training, validation, and test set have $2N_{a}^{\text{train}}$, $2N_{a}^{\text{valid}}$, and $2N_{a}^{\text{test}}$ samples, respectively (half of them labeled with $1$ and the other half with $0$). The code to run the simulations will be provided as a .zip file.

%%%%%%%%%%%%%%%%%%%%%%
%%%%    FIGURE    %%%%
%%%%%%%%%%%%%%%%%%%%%%
%%
\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/allAuthorsChangeGroups.pdf}
    \caption{Relative change in performance with respect to the GCNN baseline, divided into $3$ groups of similar behavior. Group 1 ($53\%$ of the authors) includes those where the Learn NVGF has a comparable performance to the GCNN and both of them are better than the LSIGF, showing that frequency creation plays a vital role in improving performance. Group 2 ($33\%$ of the authors) shows that oftentimes, the Learn NVGF can improve significantly over the GCNN, suggesting that the nonlinear nature of the mapping may have a negative impact. Group 3 ($14\%$ of the authors) consists of those cases when the Learn NVGF, the GCNN, and the LSIGF all exhibit comparable performance.}
    \label{fig:app:allAuthors:change}
\end{figure*}
%%%%
%%%% End of FIGURE %%%%
%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Architectures for comparison.} The Learn NVGF architecture in \eqref{eq:learnNVGF} is compared against three of the most popular GNN architectures in the literature, namely, the GCN \cite{Kipf2017-GCN}, the SGC \cite{Weinberger2019-SGC}, and the GAT \cite{Velickovic2018-GAT}. The Learn NVGF adopts the support matrix in \eqref{eq:app:authorSupport} and consists of a LSI graph filter (a graph convolution) that outputs $F_{\text{NVGF}}$ features (hidden units) and has $K_{\text{NVGF}}+1$ filter taps, followed by a NVGF that takes those $F_{\text{NVGF}}$ input features and applies a NVGF with $K_{\text{NVGF}}+1$ filter taps, and also outputs $F_{\text{NVGF}}$ features. The GCN considers $\mtS$ in \eqref{eq:app:authorSupport} to be the adjacency matrix of the graph and thus adopts a support matrix given by $\mtS_{\text{GCN}} = \mttD^{-1/2}(\mtI + \mtS) \mttD^{-1/2}$ where $\mttD = \diag((\mtI+\mtS) \vcOnes)$ as indicated in \cite{Kipf2017-GCN}. The graph convolutional layer consists of $F_{\text{GCN}}$ LSI graph filters, each one of the form $h^{f}\mtS_{\text{GCN}}$ for $f\in\{1,\ldots,F_{\text{GCN}}\}$. This is followed by a ReLU activation function. The SGC considers the same support matrix $\mtS_{\text{GCN}}$ as the GCN and learns $F_{\text{SGC}}$ output features, where each filter in the bank is of the form $h^{f} \mtS_{\text{GCN}}^{K_{\text{SGC}}}$ for a predetermined hyperparameter $K_{\text{SGC}}$. This is followed by a ReLU activation function. Finally, for the GAT, the support matrix is $\mtS$ (although this is relevant only in terms of the nonzero elements, which are the same in $\mtS$ and $\mtS_{\text{GCN}}$---except for the diagonal elements---since the weights of each edge are learned through the attention mechanism), and the output features are $F_{\text{GAT}}$ learned through the attention mechanism exactly as described in \cite{Velickovic2018-GAT}. All four architectures are followed by a readout layer consisting of a learnable linear transform that maps the $NF$ output features into a vector of size $2$ that is interpreted to be the logits for the two classes (either the text is written by the author or not).

\textbf{Architectures for analysis.} To analyze the impact of the nonlinearity, four architectures are considered. First, as a baseline, a GCNN consisting of a graph convolutional layer that outputs $F$ features, with $K+1$ filter taps, followed by a ReLU activation function, as indicated in \eqref{eq:GCNN}. Second, the Learn NVGF that replaces the ReLU activation function by a NVGF with filter taps that can be learned from data as in \eqref{eq:learnNVGF}. Third, the Design NVGF where the nonlinear activation function of the GCNN is replaced by a NVGF, but one whose filter taps are designed to mimic the ReLU (the LSI graph filters in the Design NVGF are the same ones learned by the GCNN). Fourth, a LSI graph filter with $K+1$ taps that outputs $F$ features. Note that the GCNN is a nonlinear architecture, while the other three are linear. Also, note that the first three architectures are capable of creating frequency content while the fourth one, the LSIGF, is not. For fair comparison, the values of $K$ and $F$ are the same for all architectures.

\textbf{Training.} The loss function during training is a cross-entropy loss between the logits obtained from the output of each architecture, and the labels in the training set. All the architectures are trained by using an ADAM optimizer \cite{Kingma15-ADAM} with forgetting factors $0.9$ and $0.999$, and with a learning rate $\eta$. The training is carried out for $25$ epochs with batches of size $20$. Dropout with probability $0.5$ is included before the readout layer, during training, to avoid overfitting. At testing time, the weights are correspondingly rescaled. Validation is run every $5$ training steps. The learned filters that result in the best performance on the validation set are kept and used during the testing phase. For each experiment, $10$ realizations of the random train/test split are carried out (also randomizing the selection of the texts by other authors that complete the training, validation, and test sets). The average evaluation performances (measured as classification error---ratio of texts wrongly attributed in the test set) is reported, together with the estimated standard deviation.

\textbf{Hyperparameter selection.} The number of hidden units $F_{\text{NVGF}}$, $F_{\text{GCN}}$, $F_{\text{SGC}}$, and $F_{\text{GAT}}$, the polynomial order $K_{\text{NVGF}}$ and $K_{\text{SGC}}$, and the learning rate $\eta_{\text{NVGF}}$, $\eta_{\text{GCN}}$, $\eta_{\text{SGC}}$, and $\eta_{\text{GAT}}$ are selected, independently for each architecture, from the set $\{16,32,64\}$ for the number of hidden units, $\{2, 3, 4\}$ for the polynomial order, and $\{0.001, 0.005, 0.01\}$ for the learning rate. In other words, all possible combinations of these three parameters are run for each architecture, and the ones that show the best performance on the test set are kept.

%%%%%%%%%%%%%%%%%%%%%
%%%%    TABLE    %%%% {tab:app:allAuthors:hParams}
%%%%%%%%%%%%%%%%%%%%%
%%
\begin{table*}
    \caption{Hyperparameters for each architecture that lead to the best performance}
    \label{tab:app:allAuthors:hParams}
    \centering
    \small
    \begin{tabular}{l|ccc|cc|ccc|cc}
        \toprule
        & \multicolumn{3}{c|}{Learn NVGF} & \multicolumn{2}{c|}{GCN \cite{Kipf2017-GCN}} & \multicolumn{3}{c|}{SGC \cite{Weinberger2019-SGC}} & \multicolumn{2}{c}{GAT \cite{Velickovic2018-GAT}} \\
        Authors & $\eta_{\text{NVGF}}$ & $F_{\text{NVGF}}$ & $K_{\text{NVGF}}$ & $\eta_{\text{GCN}}$ & $F_{\text{GCN}}$ & $\eta_{\text{SGC}}$ & $F_{\text{SGC}}$ & $K_{\text{SGC}}$ & $\eta_{\text{GAT}}$ & $F_{\text{GAT}}$ \\
        \midrule
        Abbott & 0.001 & 32 & 2 & 0.005 & 64 & 0.01 & 64 & 2 & 0.01 & 64 \\
        Alcott & 0.01 & 16 & 4 & 0.01 & 32 & 0.01 & 32 & 2 & 0.01 & 32 \\
        Alger  & 0.005 & 16 & 4 & 0.01 & 32 & 0.001 & 64 & 2 & 0.005 & 64 \\
        Allen & 0.005 & 64 & 2 & 0.01 & 64 & 0.005 & 32 & 2 & 0.005 & 64 \\
        Austen & 0.001 & 32 & 3 & 0.01 & 64 & 0.005 & 64 & 2 & 0.01 & 64\\
        Bront\"{e} & 0.001 & 32 & 3 & 0.01 & 64 & 0.005 & 16 & 2 & 0.01 & 16 \\
        Cooper & 0.005 & 16 & 3 & 0.005 & 64 & 0.005 & 64 & 2 & 0.01 & 64 \\
        Dickens & 0.001 & 32 & 4 & 0.005 & 32 & 0.005 & 64 & 2 & 0.01 & 64 \\
        Doyle & 0.005 & 16 & 3 & 0.005 & 16 & 0.005 & 64 & 2 & 0.005 & 32 \\
        Garland & 0.005 & 32 & 3 & 0.01 & 64 & 0.005 & 64 & 2 & 0.01 & 32 \\
        Hawthorne & 0.01 & 16 & 3 & 0.005 & 64 & 0.01 & 64 & 2 & 0.01 & 16 \\
        Irving & 0.005 & 16 & 3 & 0.005 & 64 & 0.01 & 64 & 2 & 0.005 & 64 \\
        James & 0.001 & 32 & 2 & 0.005 & 64 & 0.01 & 64 & 2 & 0.01 & 64 \\
        Jewett & 0.001 & 16 & 2 & 0.01 & 64 & 0.01 & 64 & 2 & 0.01 & 32 \\
        Melville & 0.005 & 16 & 4 & 0.01 & 64 & 0.005 & 64 & 2 & 0.005 & 64  \\
        Page & 0.005 & 16 & 2 & 0.005 & 64 & 0.005 & 64 & 2 & 0.005 & 64 \\
        Poe & 0005 & 32 & 4 & 0.005 & 64 & 0.005 & 32 & 2 & 0.01 & 32 \\
        Stevenson & 0.001 & 16 & 4 & 0.005 & 64 & 0.005 & 32 & 2 & 0.01 & 64 \\
        Thoreau & 0.005 & 64 & 3 & 0.01 & 64 & 0.01 & 16 & 2 & 0.01 & 32 \\
        Twain & 0.005 & 16 & 2 & 0.01 & 16 & 0.001 & 64 & 2 & 0.01 & 32 \\
        Wharton & 0.005 & 16 & 4 & 0.01 & 64 & 0.01 & 64 & 2 & 0.001 & 64 \\
        \bottomrule
    \end{tabular}
\end{table*}

\textbf{Experiment 1: Performance comparison.} In the first experiment, the performance is measured by classification error (ratio of texts in the test set that were wrongly attributed), and the comparison between the Learn NVGF, the GCN \cite{Kipf2017-GCN}, the SGC \cite{Weinberger2019-SGC}, and the GAT \cite{Velickovic2018-GAT} is carried out, for all $21$ authors. The hyperparameters used for each architecture and each author are present in Table~\ref{tab:app:allAuthors:hParams} (recall that the hyperparameters that exhibit the best performance---the lowest classification error on the test set---are the ones used).

Results are shown in Figure~\ref{fig:app:allAuthors:compare}. The general trend observed is for the Learn NVGF to exhibit better performance than the GAT, which in turn is better than the GCN, and all of them are better than SGC. The differences are usually significant between the four architectures, with a marked improvement by the Learn NVGF. It is observed, however, that for Doyle, Hawthorne, and Stevenson, the performance of the Learn NVGF and the GAT is comparable.

\textbf{Experiment 2: Impact of nonlinearities.} In the second experiment, the objective is to decouple the contribution made by frequency creation from that made by the nonlinear nature of the architecture. To do this, the GCNN architecture is taken as a baseline (a nonlinear, frequency-creating architecture), and the relative change in performance of the three other architectures is measured (Learn NVGF and Design NVGF, both linear and frequency-creating architectures, and LSIGF, which is linear but cannot create frequencies). The learning rate $\eta$, the number of hidden units $F$, and order of the filters $K$ are the same for all four architectures, and are those found in Table~\ref{tab:app:allAuthors:hParams} under the column of Learn NVGF. The results showing relative change in the mean classification error with respect to the GCNN are shown in Figure~\ref{fig:app:allAuthors:change}.

The authors have been classified into three groups according to their relative behavior. Group 1 consists of those authors where the Learn NVGF has a comparable performance with respect to the GCNN (i.e., $5\%$ or less relative variation in performance), and both have a considerably better performance than the LSIGF. These results suggest that, for this group of authors (the most numerous one, consisting of $11$ authors, or $53\%$ of the total), the improvement in performance is mostly due to the capability of the architectures to create frequency, and not necessarily due to the nonlinear nature of the GCNN.

Group 2 consists of those authors where the Learn NVGF exhibits a better performance than the GCNN, which in turn exhibits a similar performance to the LSIGF (except for Jewett and Thoreau, where the GCNN still exhibits considerably better performance than the LSIGF). This group of 7 authors ($33\%$ of the total) suggests that in some cases, frequency creation is the key contributor to improved performance, and that the inclusion of a nonlinear mapping could possibly have a negative impact. In essence, we observe that a linear architecture capable of creating frequencies outperforms the rest, and that a frequency-creating nonlinear architecture performs similarly to a linear architecture that cannot create frequencies. This suggests that the relationship between input and output is approximately linear with frequencies being created, but that attempting to model this frequency creation with a nonlinear architecture is not a good approach.

Finally, Group 3 consists of those architectures for which all architectures exhibit a similar performance. This group consists of $3$ authors ($14\%$ of the total). This may be explained by the fact that the high-frequency content for these authors does not carry useful information, and thus the role of frequency creation is less relevant.

With respect to the Design NVGF, a somewhat erratic behavior is observed. In near half of the cases ($11$ authors), the performance of the Design NVGF closely resembles the performance of the GCNN, as expected. In a few other cases (Alcott, James, Cooper, and Allen), the Design NVGF results are better, and in the rest they are considerably worse (Bront\"{e}, Dickens, Hawthorne, Page, Stevenson, and Melville). The Design NVGF architecture is designed to mimic the GCNN, but its accurate design depends on good estimates of the first and second moments of the data. Thus, one possible explanation is that there is not enough data to get good estimates of these values. Another possible explanation is that higher-order moments have a larger impact in these cases, and the NVGF, being linear, is not able to accurately capture them.

Overall, this second experiment shows the importance of frequency-creation in improving performance, especially when high-frequency content is significant. Among the two ways of creating frequency (linear and nonlinear), we see that in most cases, they essentially perform the same. But there are cases in which creating frequency content in a linear manner is better (Group 2). In any case, this experiment shows the importance of frequency creation and calls for further research on what other contributions the nonlinear nature of the activation function has on performance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                   MOVIE RECOMMENDATION                   %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Movie Recommendation}

%%%%%%%%%%%%%%%%%%%%%%
%%%%    FIGURE    %%%%
%%%%%%%%%%%%%%%%%%%%%%
%%
\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/allMoviesCompare.pdf}
    \caption{Performance comparison}
    \label{fig:app:allMovies:compare}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\hfill
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/allMoviesChange.pdf}
    \caption{Relative change}
    \label{fig:app:allMovies:change}
\end{figure}
%%%%
%%%% End of FIGURE %%%%
%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Problem objective.} Consider a set $\stV = \{v_{1},\ldots,v_{N}\}$ of $N$ items, and let $\fntx_{t}: \stV \to \fdR \cup \{ \emptyset\}$ be the ratings assigned by user $t$ to these items, i.e., $\fntx_{t}(v_{i})$ is the rating assigned by user $t$ to item $i$, this function yields $\emptyset$ if the item has not been rated. The objective is to estimate what rating $\fntx_{t}(\lmv_{r})$ a user would give to some target item $\lmv_{r} \in \stV$ not yet rated \cite{Monti2017-RecommendationGNN, Levie2018-CayleyNets}.

\textbf{Approach.} The idea is to create a graph $\stG$ of rating similarities and take the graph signal $\vcx$ to be the ratings given by a user to some of the items. Then $\vcx$ is processed through a GNN $\fnPhi$ (in any of the variants discussed in Section~\ref{sec:archit}) to obtain the estimated rating the user would give to the target item $\lmv_{r}$, by looking at the output value of the GNN on node $\lmv_{r}$. In short, this amounts to an interpolation problem.

\textbf{Graph construction.} Let $\sttT = \{\vctx_{t}\}_{t}$ be a set where $\vctx_{t} \in \fdR^{N}$ collects the ratings given by user $t$ to some of the items, such that $[\vctx_{t}]_{i} = \fntx_{t} (\lmv_{i}) \in \fdR$ if item $\lmv_{i}$ has been rated and $[\vctx_{t}]_{i} = 0$ if $\fntx_{t}(\lmv_{i}) = \emptyset$. Denote by $\stT_{i} = \{\vcx_{t} \in \sttT : [\vcx_{t}]_{i} > 0\}$ the set of users that have rated item $\lmv_{t}$, and by $\sttT_{ij} = \sttT_{i} \cap \sttT_{j}$ the set of users that have rated both items $\lmv_{i}$ and $\lmv_{j}$. Define the mean intersection score as $\mu_{ij} = |\sttT_{ij}|^{-1} \sum_{\vctx_{t} \in \sttT_{ij}} [\vctx_{t}]_{i}$. Note that this is the rating average for item $\lmv_{i}$, computed among those users that have rated both $\lmv_{i}$ and $\lmv_{j}$. Then, the rating similarity between items $\lmv_{i}$ and $\lmv_{j}$ is computed by means of the Pearson correlation as
% eq:app:movieWeights
\begin{equation}
      w_{ij}  = \frac{1}{|\sttT_{ij}|} \sum_{\vctx_{t} \in \sttT_{ij}} \big( [\vctx_{t}]_{i} - \mu_{ij} \big) \big( [\vctx_{t}]_{j} - \mu_{ij} \big).
\end{equation}
%
These weights can be used to build a complete graph $\sttG = (\stV, \sttE)$ where $\stV$ is the set of items and $\sttE = \stV \times \stV$ is the complete set of edges with $w_{ij}$ being the corresponding weights. In what follows, the $10$-nearest neighbor graph $\stG = (\stV, \stE)$ of $\sttG$ is built with $\stE \subseteq \sttE$, and the matrix $\mtW$ is used to denote the weighted adjacency matrix of $\stG$, such that $[\mtW]_{ij} = w_{ij}$ if $(i,j) \in \stE$, and $[\mtW]_{ij}=0$ otherwise.

\textbf{Graph signal processing description.} The support matrix is chosen to be
% eq:app:movieSupport
\begin{equation} \label{eq:app:movieSupport}
    \mtS = \big( \diag(\mtW) \big)^{-1/2} \mtW \big( \diag(\mtW) \big)^{-1/2} - \mtI.
\end{equation}
%
This problem can be cast as a supervised interpolation problem. Given the set $\sttT$ and the target item $\lmv_{r}$, consider the set $\sttT_{r}$ of all users that have rated the item $\lmv_{r}$. Extract the specific rating $\fntx_{t}(\lmv_{r}) = y_{t}$ as a label, and set a $0$ in the $r^{\text{th}}$ entry of $\vctx_{t}$. The resulting vector is a graph signal $\vcx_{t}$ which always has a $0$ in the $r^{\text{th}}$ entry. The resulting set $\stT_{r} = \{(\vcx_{t},y_{t}) : \vctx_{t} \in \sttT_{r}\}$ contains all the users that have rated the item $\lmv_{r}$ with the corresopnding rating extracted as a label $y_{t}$ and the $r^{\text{th}}$ entry $[\vcx_{t}]_{r}$ of the graph signal $\vcx_{t}$ set to zero, i.e., $[\vcx_{t}]_{r} = 0$ for all $t$ such that $\vctx_{t} \in \sttT_{r}$.

%%%%%%%%%%%%%%%%%%%%%%
%%%%    FIGURE    %%%%
%%%%%%%%%%%%%%%%%%%%%%
%%
\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/movieInputFreq.pdf}
    \caption{Input frequencies}
    \label{fig:app:starWars:inputFreq}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\hfill
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/movieOutputFreqCnvFlt.pdf}
    \caption{Output frequency of LSIGF}
    \label{fig:app:starWars:outputFreqLSIGF}
\end{figure}
%%%%
%%%% End of FIGURE %%%%
%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Dataset and code.} The dataset is the MovieLens-100k dataset \cite{Harper2016-MovieLens} publicly available at \url{http://files.grouplens.org/datasets/movielens/ml-100k.zip}. This dataset consists of one hundred thousand ratings given by $943$ users to $1682$ movies, and where each user has rated at least $20$ of them. The ratings are integers ranging from $1$ to $5$ meaning that $\fntx_{t} : \stV \to \{1,\ldots,5\} \cup \emptyset$ for every user $t$. In particular, the subset of $250$ movies that have received the largest number of ratings is used to build a graph with $N=250$ nodes. The resulting dataset has $54746$ ratings given by $943$ users to some of these $250$ movies. The five movies with the largest number of ratings are considered as target movies, namely ``Star Wars'' with $|\sttT_{\text{Star Wars}}| = 583$ pairs $(\vcx_{t},y_{t})$, ``Contact'' with $509$ pairs, ``Fargo'' with $508$, ``Return of the Jedi'' with $507$, and ``Liar, Liar'' with $485$. Each of these datasets is split randomly into $81\%$ for training, $9\%$ for validation, and $10\%$ for testing. The code to run the simulations will be provided as a .zip file.

\textbf{Architectures for comparison.} The Learn NVGF architecture in \eqref{eq:learnNVGF} is compared against two graph signal processing based methods for movie recommendation, namely the RMGCNN in \cite{Monti2017-RecommendationGNN} and the RMCayley in \cite{Levie2018-CayleyNets}, which are implemented exactly as in the corresponding papers, with the same values for the hyperparameters. For the Learn NVGF, the values are $F$ hidden units and filters of order $K$. The values used are $(64,3)$ for Star Wars and Contact, $(16,4)$ for Fargo, $(32,4)$ for Return of the Jedi, and $(16,3)$ for Liar, Liar. The support matrix is given by \eqref{eq:app:movieSupport}. All architectures are followed by a learnable local linear transformation (the same for all nodes, i.e., a LSI graph filter with $K=0$) that takes the value of the $F$ hidden units and outputs a single scalar that represents the estimated rating for that movie. A comparison with the nearest neighbor method (i.e., averaging the ratings of the nearest nodes) is also included.

\textbf{Architectures for analysis.} The architectures for analyzing the role of frequency creation are the same four architectures that in the authorship attribution problem. Namely, the LSI graph filter as a linear architecture unable to create frequencies, the Design NVGF and the Learn NVGF are linear frequency-creating architectures, and the GCNN with a ReLU nonlinearity is a nonlinear frequency-creating architecture. The values of $F$ and $K$ in all cases are the same.

\textbf{Training.} The loss function during training is the ``Smooth L1'' loss between the output scalar at the target node and the labels in the training set. All architectures are trained by using an ADAM optimizer \cite{Kingma15-ADAM} with forgetting factors $0.9$ and $0.999$, and a learning rate $\eta$. The training is carried out for $40$ epochs with batches of size $5$. Validation is run every $5$ training steps. The learned filters that result in the best performance on the validation set are kept and used during the testing phase. For each experiment, $10$ realizations of the random dataset split are carried out. The average evaluation performances (measured by RMSE) is reported, together with the estimated standard deviation.

\textbf{Experiment 1: Performance comparison.} In the first experiment, the performance is measured by the RMSE, and the comparison between the Learn NVGF, the RMGCNN \cite{Monti2017-RecommendationGNN}, the RMCayley \cite{Levie2018-CayleyNets}, and the Nearest Neighbor approach is carried out for the $5$ aforementioned movies with the most number of ratings. Results are shown in Figure~\ref{fig:app:allMovies:compare}. It is generally observed that the Learn NVGF performs better than the alternatives, although the performance is comparable to the RMGCNN and the RMCayley in the case of the movie Contact. The nearest neighbor approach yields worse performance.

\textbf{Experiment 2: Impact of nonlinearities.} In this second experiment, the objective is to decouple the contribution made by frequency creation from that made by the nonlinear nature of the architecture. To do this, the GCNN architecture is taken as a baseline (a nonlinear, frequency-creating architecture), and the relative change in performance of the three other architectures is measured. The results shown in Figure~\ref{fig:app:allMovies:change} show that the relative change is quite small (approximately $1.5\%$ change in the highest case, the movies Fargo and Liar, Liar), which implies that all four architectures have relatively similar performance. This can be easily explained by computing the average frequency response of the signals in the test set of the movie Star Wars. The result is shown in Figure~\ref{fig:app:starWars:inputFreq}. It is observed that it is a signal with low-eigenvalue frequency content. Therefore, as expected, there is not much to gain for using architectures that create frequencies. In Figures~\ref{fig:app:starWars:outputFreqLSIGF}, \ref{fig:app:starWars:outputFreqGCNN}, and \ref{fig:app:starWars:outputFreqLearn}, the frequency responses of the output of each architecture to an input that is equal to the largest eigenvector, i.e., $\vcx = \vcv_{N}$, are shown. As expected, the LSIGF (Figure~\ref{fig:app:starWars:outputFreqLSIGF}) does not create frequency content, while the other two architectures, do (Figures~\ref{fig:app:starWars:outputFreqGCNN} and \ref{fig:app:starWars:outputFreqLearn}). However, since this high-eigenvalue frequency content is not really significant, the frequency creation capabilities do not markedly improve the performance.

%%%%%%%%%%%%%%%%%%%%%%
%%%%    FIGURE    %%%%
%%%%%%%%%%%%%%%%%%%%%%
%%
\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/movieOutputFreqGCnvNN.pdf}
    \caption{Output frequency of GCNN}
    \label{fig:app:starWars:outputFreqGCNN}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\hfill
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/movieOutputFreqLrnNdV.pdf}
    \caption{Output frequency of Learn NVGF}
    \label{fig:app:starWars:outputFreqLearn}
\end{figure}
%%%%
%%%% End of FIGURE %%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                   SOURCE LOCALIZATION                   %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Source localization}