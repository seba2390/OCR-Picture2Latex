%!TEX root = 00-NVGF.tex

In what follows, we present additional simulations on the authorship attribution problem. The proofs, as well as further additional simulations on the problem of movie recommendations, can be found online\footnote{\url{https://anonymous.4open.science/r/nvgf-B8B2/}}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                   AUTHORSHIP                   %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
%%%%    FIGURE    %%%%
%%%%%%%%%%%%%%%%%%%%%%
%%
\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/allAuthorsCompare.pdf}
    \caption{Comparison between architectures for all authors. It is observed that, in most cases, the Learn NVGF exhibits significantly better performance than the GAT \cite{Velickovic2018-GAT}, which in turn is better than the GCN \cite{Kipf2017-GCN}, and all of them are better than the SGC \cite{Weinberger2019-SGC}. The error bars reflect one third of the estimated standard deviation.}
    \label{fig:app:allAuthors:compare}
\end{figure*}
%%%%
%%%% End of FIGURE %%%%
%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Problem objective.} Consider a set $\stTheta_{a}$ of texts that are known to be written by author $a$. Given a new text $\theta \notin \stTheta_{a}$ the objective is to determine whether $\theta$ was written by $a$ or not.

\textbf{Approach.} The approach is to leverage word adjacency networks (WAN) built from the set of known texts $\stTheta_{a}$ to build a graph support $\stG$, and then use the word frequency count of the function words in $\theta$ as the graph signal $\vcx$. Then, $\vcx$ is processed through a GNN $\fnPhi$ (in any of the variants discussed in Section~\ref{sec:archit}) to obtain a predictor of the text being written by author $a$. See \cite{Segarra2015-Authorship} for details.

\textbf{Dataset and code.} The dataset is presented in \cite{Segarra2015-Authorship}, and is publicly available at \url{http://github.com/alelab-upenn/graph-neural-networks/tree/master/datasets/authorshipData}. The dataset consists of $21$ authors from the $\text{19}^{\text{th}}$ century. The corpus of each author is split in texts of about $1000$ words. The WANs and the word frequency count for each text are already present in the dataset. The texts are split, at random, in $95\%$ for training and $5\%$ for testing. The weights of the WANs of the $95\%$ texts selected for training are averaged to obtain an average WAN from which the GMD $\mtS$ is obtained. The resulting support matrix is further normalized to have unit spectral norm. Note that no text from the test set is used in building the WAN. From the texts in the training set, $8\%$ are further separated to build the validation set. Denote by $N_{a}^{\text{train}}$, $N_{a}^{\text{valid}}$, and $N_{a}^{\text{test}}$ the number of texts in the training, validation, and test set, respectively.  The word frequency counts for each text are normalized and used as graph signals. A label of $1$ is attached to these signals to indicate that they have been written by author $a$ in a supervised learning context. To complete the datasets, an equal number of texts are obtained at random from other contemporary authors, and their frequency word counts are normalized and incorporated into the corresponding sets and assigned a label of $0$. In this way, the resulting training, validation, and test set have $2N_{a}^{\text{train}}$, $2N_{a}^{\text{valid}}$, and $2N_{a}^{\text{test}}$ samples, respectively (half of them labeled with $1$ and the other half with $0$). The code to run the simulations can be found online.

\textbf{Training.} The loss function during training is a cross-entropy loss between the logits obtained from the output of each architecture, and the labels in the training set. All the architectures are trained by using an ADAM optimizer \cite{Kingma15-ADAM} with forgetting factors $0.9$ and $0.999$, and with a learning rate $\eta$. The training is carried out for $25$ epochs with batches of size $20$. Dropout with probability $0.5$ is included before the readout layer, during training, to avoid overfitting. At testing time, the weights are correspondingly rescaled. Validation is run every $5$ training steps. The learned filters that result in the best performance on the validation set are kept and used during the testing phase. For each experiment, $10$ realizations of the random train/test split are carried out (also randomizing the selection of the texts by other authors that complete the training, validation, and test sets). The average evaluation performances (measured as classification error---ratio of texts wrongly attributed in the test set) is reported, together with the estimated standard deviation.

\textbf{Hyperparameter selection.} The number of hidden units $F_{\text{NVGF}}$, $F_{\text{GCN}}$, $F_{\text{SGC}}$, and $F_{\text{GAT}}$, the polynomial order $K_{\text{NVGF}}$ and $K_{\text{SGC}}$, and the learning rate $\eta_{\text{NVGF}}$, $\eta_{\text{GCN}}$, $\eta_{\text{SGC}}$, and $\eta_{\text{GAT}}$ are selected, independently for each architecture, from the set $\{16,32,64\}$ for the number of hidden units, $\{2, 3, 4\}$ for the polynomial order, and $\{0.001, 0.005, 0.01\}$ for the learning rate. In other words, all possible combinations of these three parameters are run for each architecture, and the ones that show the best performance on the test set are kept.

%%%%%%%%%%%%%%%%%%%%%%
%%%%    FIGURE    %%%%
%%%%%%%%%%%%%%%%%%%%%%
%%
\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/allAuthorsChangeGroups.pdf}
    \caption{Relative change in performance with respect to the GCNN baseline, divided into $3$ groups of similar behavior. Group 1 ($53\%$ of the authors) includes those where the Learn NVGF has a comparable performance to the GCNN and both of them are better than the LSIGF, showing that frequency creation plays a vital role in improving performance. Group 2 ($33\%$ of the authors) shows that oftentimes, the Learn NVGF can improve significantly over the GCNN, suggesting that the nonlinear nature of the mapping may have a negative impact. Group 3 ($14\%$ of the authors) consists of those cases when the Learn NVGF, the GCNN, and the LSIGF all exhibit comparable performance.}
    \label{fig:app:allAuthors:change}
\end{figure*}
%%%%
%%%% End of FIGURE %%%%
%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Experiment 1: Performance comparison.} In the first experiment, the performance is measured by classification error (ratio of texts in the test set that were wrongly attributed), and the comparison between the Learn NVGF, the GCN \cite{Kipf2017-GCN}, the SGC \cite{Weinberger2019-SGC}, and the GAT \cite{Velickovic2018-GAT} is carried out, for all $21$ authors. The hyperparameters used for each architecture and each author where selected as those that offer the best performance, and can be found online.

Results are shown in Figure~\ref{fig:app:allAuthors:compare}. The general trend observed is for the Learn NVGF to exhibit better performance than the GAT, which in turn is better than the GCN, and all of them are better than SGC. The differences are usually significant between the four architectures, with a marked improvement by the Learn NVGF. It is observed, however, that for Doyle, Hawthorne, and Stevenson, the performance of the Learn NVGF and the GAT is comparable. In any case, it is emphasized that the goal of this experiment is not to achieve state-of-the-art performance, but to show that the performance is comparable to the most popular GNN architectures. The objective of this work is to analyze the role of frequency creation, and decouple it from the effect of nonlinear activation functions, as discussed next.

\textbf{Experiment 2: Impact of nonlinearities.} In the second experiment, the objective is to decouple the contribution made by frequency creation from that made by the nonlinear nature of the architecture. To do this, the GCNN architecture is taken as a baseline (a nonlinear, frequency-creating architecture), and the relative change in performance of the three other architectures is measured (Learn NVGF and Design NVGF, both linear and frequency-creating architectures, and LSIGF, which is linear but cannot create frequencies). The learning rate $\eta$, the number of hidden units $F$, and order of the filters $K$ are the same for all four architectures. The results showing relative change in the mean classification error with respect to the GCNN are shown in Figure~\ref{fig:app:allAuthors:change}.

The authors have been classified into three groups according to their relative behavior. Group 1 consists of those authors where the Learn NVGF has a comparable performance with respect to the GCNN (i.e., $5\%$ or less relative variation in performance), and both have a considerably better performance than the LSIGF. These results suggest that, for this group of authors (the most numerous one, consisting of $11$ authors, or $53\%$ of the total), the improvement in performance is mostly due to the capability of the architectures to create frequency, and not necessarily due to the nonlinear nature of the GCNN.

Group 2 consists of those authors where the Learn NVGF exhibits a better performance than the GCNN, which in turn exhibits a similar performance to the LSIGF (except for Jewett and Thoreau, where the GCNN still exhibits considerably better performance than the LSIGF). This group of 7 authors ($33\%$ of the total) suggests that in some cases, frequency creation is the key contributor to improved performance, and that the inclusion of a nonlinear mapping could possibly have a negative impact. In essence, we observe that a linear architecture capable of creating frequencies outperforms the rest, and that a frequency-creating nonlinear architecture performs similarly to a linear architecture that cannot create frequencies. This suggests that the relationship between input and output is approximately linear with frequencies being created, but that attempting to model this frequency creation with a nonlinear architecture is not a good approach.

Finally, Group 3 consists of those architectures for which all architectures exhibit a similar performance. This group consists of $3$ authors ($14\%$ of the total). This may be explained by the fact that the high-frequency content for these authors does not carry useful information, and thus the role of frequency creation is less relevant.

With respect to the Design NVGF, a somewhat erratic behavior is observed. In near half of the cases ($11$ authors), the performance of the Design NVGF closely resembles the performance of the GCNN, as expected. In a few other cases (Alcott, James, Cooper, and Allen), the Design NVGF results are better, and in the rest they are considerably worse (Bront\"{e}, Dickens, Hawthorne, Page, Stevenson, and Melville). The Design NVGF architecture is designed to mimic the GCNN, but its accurate design depends on good estimates of the first and second moments of the data. Thus, one possible explanation is that there is not enough data to get good estimates of these values. Another possible explanation is that higher-order moments have a larger impact in these cases, and the NVGF, being linear, is not able to accurately capture them.

Overall, this second experiment shows the importance of frequency-creation in improving performance, especially when high-frequency content is significant. Among the two ways of creating frequency (linear and nonlinear), we see that in most cases, they essentially perform the same. But there are cases in which creating frequency content in a linear manner is better (Group 2). In any case, this experiment shows the importance of frequency creation and calls for further research on what other contributions the nonlinear nature of the activation function has on performance.