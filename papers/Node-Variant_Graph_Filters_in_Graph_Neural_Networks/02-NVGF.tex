%!TEX root = 00-NVGF.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                                                                        %%%%
%%%%                     THE NODE-VARYING GRAPH FILTER                      %%%%
%%%%                                                                        %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% sec:NVGF
%%%%%%%%%%%%%

Let $\stG = (\stV, \stE)$ be an undirected, possibly weighted, graph with a set of $N$ nodes $\stV = \{\lmv_{1},\ldots,\lmv_{N}\}$ and a set of edges $\stE \subseteq \stV \times \stV$. Define a graph signal as the function $\fnx: \stV \to \fdR$ that associates a scalar value to each node. For a fixed ordering of the nodes, the graph signal can be conveniently described by means of a vector $\vcx \in \fdR^{N}$ such that the $i^{\text{th}}$ entry $[\vcx]_{i}$ is the value $x_{i}$ of the signal on node $\lmv_{i}$, i.e., $[\vcx]_{i} = x_{i} = \fnx(\lmv_{i})$. %See \cite{Sandryhaila2013-DSPG, Sandryhaila2014-DSPGfreq, Shuman2013-SPG, Ortega2018-GSP} for details on graph signal processing.

Describing a graph signal as the vector $\vcx \in \fdR^{N}$ is mathematically convenient but carries no information about the underlying graph topology that supports it. This information can be recovered by defining a graph matrix description (GMD) $\mtS \in \fdR^{N \times N}$ which is a matrix that respects the sparsity pattern of the graph, i.e., $[\mtS]_{ij} = 0$ for all distinct indices $i$ and $j$ such that $(\lmv_{j},\lmv_{i}) \notin \stE$. Examples of GMDs widely used include the adjacency matrix, the Laplacian matrix, and their corresponding normalizations \cite{Sandryhaila2013-DSPG, Shuman2013-SPG, Ortega2018-GSP}.

The GMD $\mtS$ can thus be leveraged to process the graph signal $\vcx$ in such a way that the underlying graph structure is exploited. The most elementary example is the linear map $\fnS: \fdR^{N} \to \fdR^{N}$ between graph signals given by $\vcy = \fnS(\vcx) = \mtS \vcx$. This linear map is a linear combination of the information located in the one-hop neighborhood of each node:
% eq:graphShiftSingle
\begin{equation} \label{eq:graphShiftSingle}
    y_{i} = [\vcy]_{i} = \sum_{j=1}^{N} [\mtS]_{ij} [\vcx]_{j} = \sum_{j : \lmv_{j} \in \stN_{i} \cup \{\lmv_{i}\}} s_{ij}x_{j}
\end{equation}
%
where $\stN_{i} = \{\lmv_{j}: (\lmv_{j},\lmv_{i}) \in \stE \}$ is the set of nodes that share an edge with node $\lmv_{i}$. The last equality in the above equation is due to the sparsity pattern of the GMD $\mtS$.% Note that when we consider $\mtS$ to be the adjacency matrix of a directed cycle (the support matrix of discrete-time signals), then \eqref{eq:graphShift} boils down to a time-shift. Thus, $\fnS$ (and, equivalently $\mtS$) receive the name of graph shift operator (GSO), as it can be interpreted to be shifting the signal $\vcx$ around the graph.

More generally, a graph filter $\fnH(\cdot;\mtS): \fdR^{N} \to \fdR^{N}$ is a mapping between graph signals that leverages the structure encoded in $\mtS$ \cite{Segarra2017-GraphFilterDesign}. In particular, linear shift-invariant graph filters (LSIGFs) are those that can be built as a polynomial in $\mtS$:
% eq:LSIfilter
\begin{equation} \label{eq:LSIfilter}
    \fnH^{\text{lsi}}(\vcx;\mtS) = \sum_{k=0}^{K} h_{k} \mtS^{k} \vcx = \mtfnH^{\text{lsi}}(\mtS) \vcx
\end{equation}
%
where $\mtfnH^{\text{lsi}}(\mtS) = \sum_{k=0}^{K} h_{k} \mtS^{k}$ with $h_{k} \in \fdR$. Note that $\fnH^{\text{lsi}}(\vcx;\mtS)$ is written in the form of $\mtfnH^{\text{lsi}}(\mtS)\vcx$ to emphasize that the function is linear in the input $\vcx$, i.e., $\vcx$ is multiplied by a matrix $\mtfnH^{\text{lsi}}(\mtS)$ that is parametrized by $\mtS$. LSIGFs inherit their name from the fact that they satisfy the property that $\mtfnH^{\text{lsi}}(\mtS)\mtS \vcx = \mtS \mtfnH^{\text{lsi}}(\mtS) \vcx$. The set of polynomial coefficients $\{h_{k}\}_{k=0}^{K}$ are called filter taps, and can be collected in a vector $\vch \in \fdR^{K+1}$ defined as $[\vch]_{k+1} = h_{k}$ for all $k \in \{0,\ldots,K\}$. Note that the term $\mtS^{k}\vcx$ is a convenient mathematical formulation, but in practice $\mtS^{k}\vcx$ is computed by exchanging information $k$ times with one-hop neighbors, i.e., there are no matrix powers involved. In general, GSP regards $\mtS$ as given by the structure of the problem, and regards $\vcx$ as the actionable data \cite{Sandryhaila2013-DSPG, Shuman2013-SPG, Ortega2018-GSP}.

This paper focuses on NVGFs \cite{Segarra2017-GraphFilterDesign}, which are linear filters that assign a different filter tap to each node, for each application of $\mtS$. This can be compactly written as follows:
% eq:NVGF
\begin{equation} \label{eq:NVGF}
    \fnH^{\text{nv}}(\vcx;\mtS) = \sum_{k=0}^{K} \diag(\vch^{(k)}) \mtS^{k} \vcx = \mtfnH^{\text{nv}}(\mtS) \vcx
\end{equation}
%
where $\vch^{(k)} \in \fdR^{N}$ and $\diag(\cdot)$ is the diagonal operator that takes a vector and creates a diagonal matrix with the elements of the vector in the diagonal. Since the NVGF is linear in the input, it holds that $\fnH^{\text{nv}}(\vcx;\mtS) = \mtfnH^{\text{nv}}(\mtS) \vcx$, where $\mtfnH^{\text{nv}}(\mtS) = \sum_{k=0}^{K} \diag(\vch^{(k)}) \mtS^{k}$. The $i^{\text{th}}$ entry of the vector, $[\vch^{(k)}]_{i} = h_{ik}$, is the filter tap that node $\lmv_{i}$ uses to weigh the information incoming after $k$ exchanges with its neighbors. The set of all filter taps can be conveniently collected in a matrix $\mtH \in \fdR^{N \times (K+1)}$ where the $(k+1)^{\text{th}}$ column is $\vch^{(k)} \in \fdR^{N}$ and the $i^{\text{th}}$ row, denoted by $\vch_{i} \in \fdR^{K+1}$, contains the $K+1$ filter taps used by node $\lmv_{i}$, i.e., $[\vch_{i}]_{k+1} = h_{ik}$. %Note that, when the support matrix $\mtS$ is the adjacency matrix of a directed cycle, then the NVGF in \eqref{eq:NVGF} becomes a time-variant filter, where a different LTI filter is associated to each time instant.

The LSIGF in \eqref{eq:LSIfilter} and the NVGF in \eqref{eq:NVGF} are both linear and local processing operators. They depend linearly on the input graph signal $\vcx$ as indicated by the matrix multiplication notation in \eqref{eq:LSIfilter} and in \eqref{eq:NVGF}. They are local in the sense that, to compute the output, each node requires information relayed directly by their immediate neighbors. The LSIGF is characterized by the collection of $K+1$ filter taps. The NVGF, on the other hand, is characterized by $N(K+1)$ filter taps. It is noted that while the NVGF requires additional memory to store more parameters, this can be distributed throughout the graph. It is also observed that both the LSIGF and the NVGF have the same computational complexity.
%\blue{The representation space, however, grows linearly with the graph which may lead to overfitting. This issue can easily be tackled with the usual tools such as dropout or regularization, as done in the numerical experiments (Sec.~\ref{sec:sims}).}
%\blue{Another important aspect is that the NVGF requires additional memory to store the larger number of parameters. However, in the case of a distributed implementation, where each node stores a coefficient, the demand is the same as the LSI graph filter.}
%\blue{Finally, it is observed that both the LSI graph filter and the NVGF have the same computational complexity $\bigOh(K|\stE|+N)$ given by the number of exchanges between neighboring nodes.}

% \begin{remark} \label{rmk:filterImplementation}
%     Note that, due to the finite nature of the graph, finite-impulse response (FIR), infinite-impulse response (IIR) and autoregressive-moving average (ARMA) implementations of LSI graph filters can all be made equivalent for $K=N-1$ filter taps. However, for a fixed $K \ll N$ different responses can be achieved. Refer to \PhCiteText{Elvin} for a detailed analysis on different implementations of LSI graph filters.
% \end{remark}