%!TEX root = 00-NVGF.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                                                                        %%%%
%%%%                         NUMERICAL EXPERIMENTS                          %%%%
%%%%                                                                        %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% sec:sims
%%%%%%%%%%%%%

\ifundefined{arXiv}
The objective of the numerical experiment is to isolate the impact that frequency creation has on the overall performance. To do this, we focus on the GSP problem of authorship attribution, which is a problem with signals known to have high-frequency content \cite{Segarra2015-Authorship}.
\else
The objectives of the experiments are twofold. First, they aim to show how the architecture obtained by replacing the nonlinear activation function with a NVGF performs when compared to other popular GNNs. The second objective is to isolate the impact that frequency creation has on the overall performance. Due to space constraints, we focus on the problem of authorship attribution \cite{Segarra2015-Authorship}. Other applications can be found in the supplementary material, where the observations are summarized at the end of this section for reference.
\fi

\textbf{Problem statement.} In the problem of authorship attribution, the goal is to determine whether a given text has been written by a certain author, relying on other texts that the author is known to have written. To this end, word adjacency networks (WANs) are leveraged. WANs are graphs that are built by considering function words (i.e., words without semantic meaning) as nodes, and considering their co-occurrences as edges \cite{Segarra2015-Authorship}. As it happens, the way each author uses function words gives rise to a stylistic signature that can be used to attribute authorship. In what follows, we address this problem by leveraging previously constructed WANs as graphs, and the frequency count (histogram) of function words as the corresponding graph signals.

\textbf{Dataset.} For illustrative purposes, in what follows, works by Jane Austen are considered.
Attribution of other $\text{19}^{\text{th}}$ century authors can be found in the supplementary material.
A WAN consisting of $189$ nodes (function words) and $9812$ edges is built from texts belonging to a given corpus considered to be the training set. These texts are partitioned into segments of approximately $1000$ words each, and the frequency count of those $189$ function words in each of the texts is obtained. These represent the graph signals $\vcx$ that are considered to be part of the training set. Each of these is assigned a label $1$ to indicate that they have been authored by Jane Austen. An equal number of segments from other contemporary authors are randomly selected, and then their frequency count is computed and added to the training set with the label $0$ to indicate that they have not been written by Jane Austen. The total number of labeled samples in the training set is $1464$, of which $118$ are left aside for validation. The test set is built analogously by considering other text segments that were not part of the training set (and thus, not used to build the WAN either), totaling $78$ graph signals (half corresponding to texts authored by Jane Austen, and half corresponding to texts by other contemporary authors).

\textbf{Architectures and training.} For the first experiment, we compare the Learn NVGF architecture \eqref{eq:learnNVGF} with arguably three of the most popular non-GSP GNNs, namely, GCN \cite{Kipf2017-GCN}, SGC \cite{Weinberger2019-SGC}, and GAT \cite{Velickovic2018-GAT}. Note that the Learn NVGF is an entirely linear architecture, but one capable of creating frequencies due to the nature of the NVGF. The filter taps of both the LSIGF and the NVGF are learned from data. The other three architectures are nonlinear since they include a ReLU activation function after the first filtering layer. All architectures include a learnable linear readout layer. Dropout with a probability $0.5$ is included after the first layer. All architectures are trained for $25$ epochs with a batch size of $20$, using the ADAM algorithm \cite{Kingma15-ADAM} with the forgetting factors $0.9$ and $0.999$, respectively, as well as a learning rate $\eta$. The value of the learning rate $\eta$, the number of hidden units $F$, and the number of filter taps $K$ are chosen by exhaustive search over triplets $(\eta, F, K)$ in the set $\{0.001, 0.005, 0.01\} \times \{16, 32, 64\} \times \{2, 3, 4\}$.

%%%%%%%%%%%%%%%%%%%%%%
%%%%    FIGURE    %%%%
%%%%%%%%%%%%%%%%%%%%%%
%%
\begin{figure*}
    \centering
    \subfloat[LSIGF]{%
        \label{subfig:austen:lsigf}
        \includegraphics[width=0.55\columnwidth]{figures/austenOutputFreqCnvFlt.pdf}
    }
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \hfill
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subfloat[GCNN]{%
        \label{subfig:austen:gcnn}
        \includegraphics[width=0.55\columnwidth]{figures/austenOutputFreqGCnvNN.pdf}
    }
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \hfill
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subfloat[Learn NVGF]{%
        \label{subfig:austen:nvgf}
        \includegraphics[width=0.55\columnwidth]{figures/austenOutputFreqLrnNdV.pdf}
    }
    \caption{Frequency responses to a single high-frequency input $\vcx = \vcv_{N}$. The frequency responses include the outputs of all $F=64$ filters.%, normalized to have unit norm.
    }
    \label{fig:austen:freq}
\end{figure*}
%%%%
%%%% End of FIGURE %%%%
%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Experiment 1: Performance comparison.} The objective of this first experiment is to illustrate that the performance of the Learn NVGF is comparable to the performance of popular (non-GSP) GNNs. The best results for each architecture are shown in Figure~\ref{fig:austen:comparison}, where the classification error was averaged over $10$ random splits of texts that are assigned to the training and test sets. One third of the standard deviation is also shown. It is observed that the Learn NVGF architecture has a comparable performance. It is emphasized that the objective of this paper is not to achieve state-of-the-art performance, but to offer insight on the role of nonlinear activation functions in frequency creation and how this translates to performance, as discussed next.
\ifundefined{arXiv}
\else
Among the three popular architectures, GAT ($\eta = 0.01$, $F=64$) exhibits the best performance, which is $102\%$ better than SGC ($\eta = 0.005$, $F=64$, $K=2$) and $30\%$ better than GCN ($\eta = 0.01$, $F=64$). This is expected since the graph filters involved in GAT are non-convolutional, and thus have more expressivity. Yet, when compared to the linear Learn NVGF ($\eta=0.001$, $F=32$, $K=3$), it is observed that the latter exhibits $20\%$ better performance than the GAT.
\fi

\textbf{Experiment 2: The role of frequency creation.} For the second experiment, we compare the Learn NVGF of the previous experiment with (i) a simple LSIGF, (ii) a Design NVGF as in \eqref{eq:designNVGF}, and (iii) a GCNN as in \eqref{eq:GCNN}. The same values of $(\eta=0.001, F=32, K=3)$ are used for all architectures as a means of fixing all other variabiliity except for the nonlinearity/frequency creation. The results are shown in Figure~\ref{fig:austen:change}. Note that the GCNN in \eqref{eq:GCNN} can be interpreted as a stand-in for ChebNets \cite{Defferrard2016-ChebNets}, arguably the most popular GSP-based GNN architecture.

\textbf{Discussion.} First, note that the LSIGF, Design NVGF, and Learn NVGF architectures are linear, whereas the GCNN is not. The LSIGF, however, is not capable of creating frequencies, while the other three are, albeit through different mechanisms. Second, Figure~\ref{fig:austen:change} shows the percentage difference in performance with respect to the base architecture (the GCNN). Essentially, the difference in performance between the LSIGF and the Learn NVGF can be pinned down to the frequency creation, because both architectures are linear, while the difference between the Learn NVGF and the GCNN can be tied to the nonlinear nature of the GCNN. It is thus observed that the LSIGF performs considerably worse than the Learn NVGF and the GCNN, which perform the same. It is observed that the Design NVGF performs halfway between the GCNN and the LSIGF. The Design NVGF depends on the ability to accurately estimate the first and second moments from the data, and this has an impact on its performance. In any case, it is noted that this experiment suggests that the main driver of improved performance is the frequency creation and not necessarily the nonlinear nature of the GCNN. %Finally, it is observed that the Design NVGF performs halfway between the GCNN and the LSIGF. The Design NVGF depends on the ability to accurately estimate the first and second moments from the data, and this significantly impacts its performance.

From a qualitative standpoint, the average frequency response of the signals in the test set is shown in Figure~\ref{fig:austen:input}. Since the high-eigenvalue content is significant, it is expected that the ability to better process this content will impact the overall performance. This explains the relatively poor performance of the LSIGF. In Figure~\ref{fig:austen:freq}, we show the frequency response of the output for each of the three architectures (LSIGF, Learn NVGF, and GCNN) when the input has a single high frequency, i.e., $\vcx = \vcv_{N}$ so that $\vctx = \vce_{N}$. Figure~\ref{subfig:austen:lsigf} shows that the output frequency response of the LSIGF exhibits a single frequency, the same as the input. Figure~\ref{subfig:austen:gcnn} shows that the output frequency response of the GCNN has content in all frequencies, but most notably a low-frequency peak appears. Figure~\ref{subfig:austen:nvgf} shows that the output frequency response of the Learn NVGF contains all frequencies, in a much more spread manner than the GCNN.

\textbf{General observations.} In the supplementary material, a similar analysis is carried out for $21$ other authors. Additionally, it is noted that Jane Austen is representative of the largest group (consisting of $11$ authors) where the Learn NVGF and the GCNN have similar performance and are better than the LSIGF. For $7$ other authors, the Learn NVGF actually performs better than the GCNN. Finally, for the remaining $3$ authors, there is no significant difference between the performance of the LSIGF, the GCNN, and the Learn NVGF, which implies that the high-frequency eigenvalue content is less significant for these authors. \ifundefined{arXiv} \else Additionally, the problem of movie recommendation is considered \cite{Harper2016-MovieLens}. First, it is observed that the Learn NVGF exhibits better performance than the methods in \cite{Monti2017-RecommendationGNN, Levie2018-CayleyNets} and the nearest neighbor algorithm. Second, it is noted that the input signals do not have significant high-eigenvalue content and thus the LSIGF, the GCNN, and the Learn NVGF exhibit similar performance. For a detailed analysis, please see the supplementary material. \footnote{It is noted that the popular benchmark of semi-supervised learning over citation networks does not fit the empirical risk minimization framework nor the graph signal processing framework, and thus, it does not admit a frequency analysis, precluding their use in this work.}

\textbf{Problems beyond processing graph signals.} Two popular tasks that utilize graph-based data are semi-supervised learning and graph classification. The former involves a framework where each sample represents a node in the graph, i.e., $\vcx \in \fdR^{F}$ for $F$ features instead of $\vcx \in \fdR^{N}$ for $N$ nodes. This implies that the notion of graph frequency response used in \eqref{eq:GFT} does not translate to the semi-supervised setting. Therefore, extending these results to this problem requires careful determination of the notion of frequency. For the graph classification problem, each sample in the dataset represents a signal together with a graph, and the graph associated to each sample is usually different. In this case, while the notion of frequency is properly defined for each sample, an overarching frequency response for all the samples is not. Possible extensions of this framework to the graph classification problem would entail redefining a common notion of frequency response for the dataset, for example, using graphons.
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The objective of the numerical experiments is to illustrate the role of frequency creation in the overall performance. To show this, four architectures are considered: (i) a linear, LSI graph filter (LSIGF), (ii) a graph convolutional neural network (GCNN) consisting of a LSI graph filter followed by a pointwise nonlinearity, (iii) a desgined NVGF neural network (Design NVGF), consisting of a LSI graph filter, followed by a NVGF that has been designed following section~\ref{sec:activation} to approximate the nonlinearity used in the GCNN, and estimating the statistics of the input to the NVGF filter, (iv) a learned NVGF neural network (Learn NVGF), consisting of a LSI graph filter, followed by a NVGF whose filter taps have also been learned from data during training.

% The objective of comparing these four architectures is to separate the role of frequency creation in improving performance, and separate it from the impact of using an actual nonlinear mapping. The difference between (i) the LSIGF and (ii) the GCNN, is the inclusion of a nonlinearity which fulfils the role of frequency creation, since LSI filters do not create frequencies. Architectures (iii) DesignNVGF and (iv) LearnNVGF, are both linear architectures, but have the capability of creating frequency content. Therefore, the difference in performance between (ii) and (iii)-(iv) is expected to be due to the nonlinear nature of the mapping, while the difference between (i) and (iii)-(iv) is expected to be due to the impact of frequency creation. Additionally, being able to compute the frequency response of NVGF adds further interpretability to the results in terms of frequency analysis of the data, as it will be shown.

% While the objective of this work is not to compare against state-of-the-art methods, but to aid in the understanding of graph neural networks, we still include comparisons with (v) GCNs and (vi) SGCs which are arguably the two most popular graph convolutional neural network methods. In any case, further comparison with these methods and in other datasets can be found in the supplementary material.

% The problem of authorship attribution using word adjacency networks (WANs) is considered. WANs are graphs that are built by considering function words (i.e. words that have no semantic meaning) as nodes, and considering their co-occurrence as edges, see \PhCiteText{Eisen} for details. As it happens, the way each author uses function words gives rise to a stylistic signature that can be used to attribute authorship. Building accurate WANs requires long texts, and therefore, it is not helpful when attempting to attribute new, short texts. In what follows, the attribution of short texts is considered by leveraging previously constructed WANs as graphs, and the frequency count (histogram) of function words in the short texts as the corresponding graph signals.

% The one-layer architectures (i) through (vi) are implemented, considering filters of size $K$ and creating $F$ hidden units. A readout layer follows all of these architectures, which is simply a learned linear transform between the resulting $N F$ values at the end of the first layer into a single scalar that determines whether the text belongs to Jane Austen or not. The models are trained by minimizing the cross-entropy loss function between the actual label and the output of the readout layer, for the training set. The optimizer used is ADAM with a learning rate $\eta$ and forgetting factors $0.9$ and $0.999$. The values of $\eta, K, F$ are selected to be the same for (i)-(iv) to facilitate comparisons and are picked from the sets $\eta \in \{0.001, 0.005, 0.01\}$, $F \in \{16, 32, 64\}$ and $K \in \{2, 3, 4\}$. For (v), note that $K=2$ and the values of $\eta$ and $F$ that perform the best on the test set are shown. Analogously for (vi), but also picking the best possible value of $K$. Each training and testing round is repeated $10$ times to account for the randomization from the train/test partition. The average results and the estimated standard deviation are reported.