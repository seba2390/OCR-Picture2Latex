%!TEX root = 00-NVGF.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                                                                        %%%%
%%%%                             ARCHITECTURES                              %%%%
%%%%                                                                        %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% sec:archit
%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%
%%%%    FIGURE    %%%%
%%%%%%%%%%%%%%%%%%%%%%
%%
\begin{figure*}
    \centering
    \subfloat[Performance Comparison]{%
        \label{fig:austen:comparison}%
        \includegraphics[width=0.55\columnwidth]{figures/austenCompare.pdf}
    }
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \hfill
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subfloat[Role of frequency creation]{%
        \label{fig:austen:change}
        \includegraphics[width=0.55\columnwidth]{figures/austenChange.pdf}
    }
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \hfill
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \subfloat[Input frequencies]{%
        \label{fig:austen:input}
        \includegraphics[width=0.55\columnwidth]{figures/austenInputFreq.pdf}
    }
    \caption{Authorship attribution problem: Jane Austen. (\protect\subref{fig:austen:comparison}) Comparison between the Learn NVGF and three popular architectures (GCN \cite{Kipf2017-GCN}, SGC \cite{Weinberger2019-SGC} and GAT \cite{Velickovic2018-GAT}). (\protect\subref{fig:austen:change}) Change with respect to a GCNN \eqref{eq:GCNN} when considering no activation function (LSIGF), a designed NVGF \eqref{eq:designNVGF} and a learned NVGF \eqref{eq:learnNVGF}. (\protect\subref{fig:austen:input}) A sample of the input frequencies to the architectures, where a large high-frequency component is observed.}
    \label{fig:austen:results}
\end{figure*}
%%%%
%%%% End of FIGURE %%%%
%%%%%%%%%%%%%%%%%%%%%%%

A graph convolutional neural network (GCNN) $\fnPhi$ has a layered architecture \cite{Defferrard2016-ChebNets, Gama2019-Archit}, where each layer applies a LSIGF as in \eqref{eq:LSIfilter}, followed by a pointwise nonlinear activation function $\rho:\fdR \to \fdR$ applied to each node $[\rho(\vcx)]_{i} = \rho([\vcx]_{i})$ and
% eq:GCNN
\begin{equation} \label{eq:GCNN}
    \vcx_{0} = \vcx, \ \vcx_{\ell} = \rho \big( \fnH_{\ell}^{\text{lsi}} (\vcx_{\ell-1} ; \mtS, \vch_{\ell}) \big), \ \vcx_{L} = \fnPhi(\vcx;\mtS,\stH).
\end{equation}
%
The LSIGF $\fnH_{\ell}^{\text{lsi}}(\cdot;\mtS,\vch_{\ell})$ is characterized by the specific filter taps $\vch_{\ell} \in \fdR^{K_{\ell}+1}$. Note that the output of the GCNN is collected as the output of the last layer $\vcx_{L} = \fnPhi(\vcx;\mtS,\stH)$. This notation emphasizes that the input is $\vcx$, while the matrix $\mtS$ is given by the problem, and the filter taps $\stH = \{\vch_{\ell}\}_{\ell=1}^L$ are learned from data.

Nonlinear activation functions are used in GCNNs to enable them to learn nonlinear relationships between input and output. Additionally, theoretical results have found that they play a key role in creating frequency content that can be better processed by subsequent graph convolutional filters. As previously discussed, NVGFs are also capable of creating new frequency content, albeit in a linear manner. Therefore, by replacing the nonlinear activation functions by NVGFs, it is possible to decouple the contribution made by frequency creation from that made by the architecture's nonlinearity.

The first architecture proposed here is to use a designed NVGF in lieu of the activation function. That is, instead of using the nonlinear activation function $\rho$, an optimal NVGF filter designed as in Proposition~\ref{prop:NVGFoptimal} is used. This requires estimating the first and second moments of the NVGF input data. The architecture, herein termed ``Design NVGF'', is given by
% eq:designNVGF
\begin{equation} \label{eq:designNVGF}
    \vcx_{\ell} = \fnH_{\ell}^{\text{nv}} \big( \fnH_{\ell}^{\text{lsi}} (\vcx_{\ell}; \mtS, \vch_{\ell}); \mtS, \mtH_{\ell}^{\opt} \big).
\end{equation}
%
Note that, in this case, the filter taps $\mtH_{\ell}^{\opt} \in \fdR^{N \times (K_{\ell}+1)}$ of the NVGF are obtained by Proposition~\ref{prop:NVGFoptimal}, while the filter taps $\stH = \{\vch_{\ell}\}_{\ell=1}^L$ of the LSIGF are learned from data.

%\begin{figure*}[htb]
%    \centering
%    \subfloat[Linear]{%
%        \label{subfig:linear}%
%        \includegraphics[width=0.65\columnwidth]{figures/l2errorSNRlinear.pdf}%
%    }%
%    \hfill
%    \subfloat[Nonlinear]{%
%        \label{subfig:nonlinear}%
%        \includegraphics[width=0.65\columnwidth]{figures/l2errorSNRnonlinear.pdf}%
%    }%
%    \hfill
%    \subfloat[Non-Gaussian]{%
%        \label{subfig:nongaussian}%
%        \includegraphics[width=0.65\columnwidth]{figures/l2errorSNRuniform.pdf}%
%    }%
%    \caption{Performance of the algorithms as a function of the SNR, measured by the relative RMSE. \protect\subref{subfig:linear}~Linear model with Gaussian noise. \protect\subref{subfig:nonlinear}~Nonlinear model with linear measurements and Gaussian noise. \protect\subref{subfig:nongaussian}~Linear model with uniform noise. In all cases it is observed that the learned SIS particle filter performs better. The error bars represent one-third of the estimated standard~deviation.}
%    \label{fig:l2errorSNR}
%\end{figure*}

Alternatively, the filter taps of the NVGF replacing the nonlinear activation function can also be learned from data, together with the filter taps of the LSIGF:
% eq:learnNVGF
\begin{equation} \label{eq:learnNVGF}
    \vcx_{\ell} = \fnH_{\ell}^{\text{nv}} \big( \fnH_{\ell}^{\text{lsi}} (\vcx_{\ell}; \mtS, \vch_{\ell}); \mtS, \mtH_{\ell} \big)
\end{equation}
%
where the filter taps to be learned are $\stH = \{(\vch_{\ell},\mtH_{\ell})\}_{\ell=1}^L$. This approach avoids the need to estimate first and second moments. Additionally, it allows the NVGF to learn how to create frequency content tailored to the application at hand, instead of just approximating the chosen nonlinear activation function. We note that while the increased number of parameters may lead to overfitting, this can be tackled by dropout. This architecture is termed ``Learn NVGF''.

%\red{Alternatively, as a means of avoiding potential overfitting, the NVGF can be designed. As an example, the problem of approximating commonly used activation functions is discussed.}