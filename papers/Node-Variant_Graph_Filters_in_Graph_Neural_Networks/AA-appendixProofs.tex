%!TEX root = 00-NVGF.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                                                                        %%%%
%%%%                                APPENDIX                                %%%%
%%%%                                                                        %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                   FREQUENCY RESPONSE OF NVGF                   %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Frequency Response of Node-Variant Graph Filters}

\begin{proof}[Proof of Proposition~\ref{prop:GFToutputNVGF}]
The output graph signal $\vcy \in \fdR^{N}$ of a node-variant graph filter is given by \eqref{eq:NVGF}, which is reproduced here for ease of exposition:
% eq:app:NVGF
\begin{equation} \label{eq:app:NVGF}
    \vcy = \sum_{k=0}^{K} \diag(\vch^{(k)}) \mtS^{k} \vcx.
\end{equation}
%
Recall that $\vch^{(k)} \in \fdR^{N}$ is the $(k+1)^{\text{th}}$ column of the matrix $\mtH \in \fdR^{N \times (K+1)}$ containing the $N(K+1)$ filter taps, that $\mtS \in \fdR^{N \times N}$ is the support matrix describing the graph, and that $\vcx \in \fdR^{N}$ is the input graph signal. As given by \eqref{eq:GFT}, the spectral representation $\vctx \in \fdR^{N}$ of the input graph signal are the coordinates of representing $\vcx$ on the eigenbasis $\{\vcv_{i}\}_{i=1}^N$ of the support matrix $\mtS$, i.e., $\vctx = \mtV^{\Tr} \vcx$. Similarly, $\vcty = \mtV^{\Tr} \vcy$. Therefore, by \eqref{eq:app:NVGF} together with the fact that $\mtS  = \mtV \diag(\vclambda) \mtV^{\Tr}$, it holds that
% eq:app:NVGFoutputGFT
\begin{equation} \label{eq:app:NVGFoutputGFT}
%
\begin{aligned}
    \vcty & = \mtV^{\Tr} \vcy = \mtV^{\Tr} \sum_{k=0}^{K} \diag(\vch^{(k)}) \mtS^{k} \vcx \\ & = \mtV^{\Tr} \sum_{k=0}^{K} \diag(\vch^{(k)}) \mtV \diag(\vclambda^{k}) \vctx
\end{aligned}
%
\end{equation}
%
where $\vclambda^{k}$ is the $N$-vector with $i^\text{th}$ element given by $[\vclambda^{k}]_{i} = \lambda_{i}^{k}$. Denoting $h_{ik} = [\vch^{(k)}]_{i} = [\mtH]_{i(k+1)}$, note that $\diag(\vch^{(k)})\mtV \diag(\vclambda^{k}) \in \fdR^{N \times N}$, so that
% eq:app:diagVdiag
\begin{equation}  \label{eq:app:diagVdiag}
%
\begin{aligned}
    \sum_{k=0}^{K} [\diag(& \vch^{(k)})\mtV \diag(\vclambda^{k})]_{ij}  = \sum_{k=0}^{K} h_{ik} \lambda_{j}^{k} v_{ij} \\ & = v_{ij} \sum_{k=0}^{K} h_{ik} \lambda_{j}^{k} = v_{ij} \fnth_{i}(\lambda_{j})
    % \begin{bmatrix}
    % h_{1}^{(k)}\lambda_{1}^{k}v_{11} & h_{1}^{(k)}\lambda_{2}^{k}v_{12} & \cdots & h_{1}^{(k)}\lambda_{N}^{k}v_{1N} \\
    % h_{2}^{(k)}\lambda_{1}^{k}v_{21} & h_{2}^{(k)}\lambda_{2}^{k}v_{22} & \cdots & h_{2}^{(k)}\lambda_{N}^{k}v_{2N} \\
    % \vdots & \vdots & \ddots & \vdots \\
    % h_{N}^{(k)}\lambda_{1}^{k}v_{N1} & h_{N}^{(k)}\lambda_{2}^{k}v_{N2} & \cdots & h_{N}^{(k)}\lambda_{N}^{k}v_{NN}
    % \end{bmatrix}
\end{aligned}
%
\end{equation}
%
where $\fnth_{i}(\lambda) = \sum_{k=0}^{K} h_{ik} \lambda^{k}$ is the frequency response of the filter taps at node $\lmv_{i}$; see \eqref{eq:freqResponse}. Next, observe that the filter taps for each node are collected in the rows of $\mtH$. Therefore, in analogy to \eqref{eq:GFTfilter}, it holds that $\mtH \mtLambda^{\Tr} \in \fdR^{N \times N}$, with
% eq:app:LambdaH
\begin{equation} \label{eq:app:LambdaH}
    [\mtH \mtLambda^{\Tr}]_{ij} = \sum_{k=0}^{K} h_{ik} \lambda_{j}^{k} = \fnth_{i} (\lambda_{j}).
\end{equation}
%
Substituting \eqref{eq:app:LambdaH} into \eqref{eq:app:diagVdiag} gives that
% eq
\begin{equation}
    \sum_{k=0}^{K} [\diag(\vch^{(k)})\mtV \diag(\vclambda^{k})]_{ij} = [\mtV \circ (\mtH \mtLambda^{\Tr})]_{ij}
\end{equation}
%
so that \eqref{eq:app:NVGFoutputGFT} becomes
\begin{equation}
    \vcty = \mtV^{\Tr} \big( \mtV \circ (\mtH \mtLambda^{\Tr}) \big) \vctx.
\end{equation}
This completes the proof.
\end{proof}

\begin{proof}[Proof of Corollary \ref{cor:NVGFnewFreq}]
The $j^{\text{th}}$ element of the graph Fourier transform of the output, $\vcty$, is
%
\begin{equation} \label{eq:app:gftOutput}
    \scty_{j}%= \vcv_{j}^{\Tr} \Big( \sum_{i=1}^{N} \sctx_{i} \text{the }i\text{th column of the thing}\Big) = \sum_{i=1}^{N} \sctx_{i} \sum_{t=1}^{N} v_{tj} v_{ti} \fnth_{t}(\lambda_{i})
    = \sum_{i=1}^{N} \sctx_{i} \vcv_{j}^{\Tr} \diag(\vcfnth(\lambda_{i})) \vcv_{i}
\end{equation}
%
where $\vcfnth(\lambda_i) = [\fnth_{1}(\lambda_i),\fnth_{2}(\lambda_i),\dots,\fnth_{N}(\lambda_i)] \in \fdR^{N}$ collects the frequency response of all nodes at eigenvalue $\lambda_{i}$. Note that, if $\scty_{j}$ depends on $\sctx_{i}$ for some $i \neq j$, then frequencies are created. When $\mtV^{\Tr} \big( \mtV \circ (\mtH \mtLambda^{\Tr}) \big)$ is not diagonal, there exists $i,j\in\{1,\ldots,N\}$ such that $i\ne j$ and $(\mtV^{\Tr} \big( \mtV \circ (\mtH \mtLambda^{\Tr}) \big))_{ij} = \vcv_{j}^{\Tr} \diag(\vcfnth(\lambda_{i})) \vcv_{i} \ne 0$. In this case, \eqref{eq:app:gftOutput} indeed shows that frequencies are created.

Now suppose that no frequency creation occurs. Then $\mtV^{\Tr} \big( \mtV \circ (\mtH \mtLambda^{\Tr}) \big)$ must be diagonal. Therefore, there exists $\alpha_1,\ldots,\alpha_N\in\mathbb{R}$ such that $\vcv_{j}^{\Tr} \diag(\vcfnth(\lambda_{i})) \vcv_{i} = \alpha_{i} \delta_{ij}$ for all $i,j$. Recalling that the set $\{\vcv_{i}\}_{i=1}^N$ of eigenvectors of $\mtS$ is an orthonormal basis, we have that $\vcv_{j}^{\Tr} \vcv_{i} = \delta_{ij}$, so it must be that $\vcv_{i}^{\Tr}(\diag(\vcfnth(\lambda_{i}))-\alpha_i \mtI) \vcv_{j} = 0$ for all $i,j$. This implies for all $i,j$ that $v_{ji}=0$ or $\fnth_{j}(\lambda_i)=\alpha_i$. In the case that $v_{ij}\ne 0$ for all $i,j$, it is clear that $\diag(\vcfnth(\lambda_{i})) = \alpha_{i} \mtI$, meaning that the frequency response at all nodes is the same. This restriction implies that the NVGF filter is a LSI graph filter. This concludes the proof.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                   LIPSCHITZ CONTINUITY OF NVGF                   %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Lipschitz Continuity of Node-Variant Graph Filters}

\begin{proof}[Proof of Theorem~\ref{thm:stability}]
Leveraging the fact that the filter taps of both $\mtfnH^{\text{nv}}(\mthS)$ and $\mtfnH^{\text{nv}}(\mtS)$ are the same, start by writing the difference between the filter outputs as
% eq:app:outputDiff
\begin{equation} \label{eq:app:outputDiff}
    \big( \mtfnH^{\text{nv}}(\mthS)-\mtfnH^{\text{nv}}(\mtS) \big) \vcx = \sum_{k=0}^{K} \diag(\vch^{(k)}) \big(\mthS^{k} - \mtS^{k}\big) \vcx.
\end{equation}
%
Let $\mtE = \mthS - \mtS$ and note that $\mtE$ is symmetric and satisfies $\| \mtE \| = \|\mthS - \mtS\| \leq \sceps$ by assumption. Recall that $(\mtS+\mtE)^{k} = \mtS^{k} + \sum_{r=0}^{k-1} \mtS^{r} \mtE \mtS^{k-r-1} + \mtC$ with $\mtC$ such that $\|\mtC\| \leq \sum_{r=2}^{k} \binom{k}{r} \|\mtE\|^{r}\|\mtS\|^{k-r}$. Leveraging this fact in \eqref{eq:app:outputDiff}, it yields
% eq:app:outputDiffWithD
\begin{equation} \label{eq:app:outputDiffWithD}
%
\begin{aligned}
    \big( \mtfnH^{\text{nv}}& (\mthS)-\mtfnH^{\text{nv}}(\mtS) \big) \vcx  \\ & = \sum_{k=0}^{K} \diag(\vch^{(k)}) \sum_{r=0}^{k-1} \mtS^{r} \mtE \mtS^{k-r-1} \vcx + \mtD \vcx
\end{aligned}
%
\end{equation}
%
with $\mtD$ satisfying $\|\mtD\| = \bigOh(\|\mtE\|^{2})$, since the filter taps $\mtH$ define analytic frequency responses $\fnh_{i}(\lambda)$ with bounded derivatives for all $i \in \{1,\ldots,N\}$. The input graph signal $\vcx$ can be rewritten as $\vcx = \sum_{i=1}^{N} \sctx_{i} \vcv_{i}$ using the GFT for the support matrix $\mtS$. Then, \eqref{eq:app:outputDiffWithD} becomes
% eq:app:outputDiffGFT
\begin{align} \label{eq:app:outputDiffGFT}
%
    & \big( \mtfnH^{\text{nv}}(\mthS)-\mtfnH^{\text{nv}}(\mtS) \big) \vcx \\ & = \sum_{i=1}^{N} \sctx_{i} \sum_{k=0}^{K} \diag(\vch^{(k)}) \sum_{r=0}^{k-1} \mtS^{r} \mtE \mtS^{k-r-1} \vcv_{i} + \sum_{i=1}^{N} \sctx_{i} \mtD \vcv_{i}. \nonumber
%
\end{align}
%

Using the fact that $\vcv_{i}$ is an eigenvector of $\mtS$ the first term in \eqref{eq:app:outputDiffGFT} can be conveniently rewritten as
% eq:app:outputDiffGFTfirstTerm
\begin{equation} \label{eq:app:outputDiffGFTfirstTerm}
%
\begin{aligned}
    \sum_{i=1}^{N} \sctx_{i} & \sum_{k=0}^{K} \diag(\vch^{(k)}) \sum_{r=0}^{k-1} \mtS^{r} \mtE \mtS^{k-r-1} \vcv_{i} \\ & = \sum_{i=1}^{N} \sctx_{i} \sum_{k=0}^{K} \diag(\vch^{(k)}) \sum_{r=0}^{k-1}\lambda_{i}^{k-r-1}\mtS^{r} \mtE \vcv_{i}.
\end{aligned}
%
\end{equation}
%
Denoting by $\mtE = \mtU \diag(\vcm) \mtU^{\Tr}$ the eigendecomposition of $\mtE$ with $\mtE\vcu_{i} = m_{i}\vcu_{i}$ the corresponding eigenvectors $\vcu_{i}$ and eigenvalues $m_{i}$, \cite[Lemma 1]{Gama2020-Stability} states that $\mtE \vcv_{i} = m_{i} \vcv_{i} + \mtE_{U} \vcv_{i}$ with $\| \mtE_{U}\| \leq 8\sceps$ for an appropriate matrix $\mtE_{U}$ that depends on $\mtU$, $\mtS$, and $\mtE$. Using this in \eqref{eq:app:outputDiffGFTfirstTerm},
%
\begin{align}
    \sum_{i=1}^{N} & \sctx_{i} \sum_{k=0}^{K} \diag(\vch^{(k)}) \sum_{r=0}^{k-1}\lambda_{i}^{k-r-1}\mtS^{r} \mtE \vcv_{i} \nonumber \\ & =
    \sum_{i=1}^{N} \sctx_{i} m_{i} \sum_{k=0}^{K} \diag(\vch^{(k)}) \sum_{r=0}^{k-1}\lambda_{i}^{k-r-1}\mtS^{r} \vcv_{i} \label{eq:app:outputDiffGFTfirstTermFirst}\\
    & \quad + \sum_{i=1}^{N} \sctx_{i} \sum_{k=0}^{K} \diag(\vch^{(k)}) \sum_{r=0}^{k-1}\lambda_{i}^{k-r-1}\mtS^{r} \mtE_{U} \vcv_{i}\label{eq:app:outputDiffGFTfirstTermSecond}
\end{align}
%
is obtained. For the term \eqref{eq:app:outputDiffGFTfirstTermFirst}, note that $\mtS^{r} \vcv_{i} = \lambda_{i}^{r} \vcv_{i}$ and that $\sum_{r=0}^{k-1} \lambda_{i}^{k-r-1}\lambda_{i}^{r} = k \lambda_{i}^{k-1}$, so that the term \eqref{eq:app:outputDiffGFTfirstTermFirst} is equivalent to
% eq:app:outputDiffGFTfirstTermFirstSolved
\begin{equation} \label{eq:app:outputDiffGFTfirstTermFirstSolved}
%
\begin{aligned}
    \sum_{i=1}^{N} \sctx_{i} m_{i} & \sum_{k=0}^{K} k\diag(\vch^{(k)})\lambda_{i}^{k-1} \vcv_{i} \\
    & = \sum_{i=1}^{N} \sctx_{i} m_{i} \diag(\vcfnth'(\lambda_{i}))\vcv_{i}
\end{aligned}
%
\end{equation}
%
where $\vcfnth'(\lambda) \in \fdR^{N}$ is a vector where the $i^{\text{th}}$ entry is the derivative of the frequency response of node $\lmv_{i}$, i.e., $[\vcfnth'(\lambda)]_{i} = \fnth'_{i}(\lambda) = \frac{d}{d\lambda} \fnth_{i} (\lambda)$. To rewrite \eqref{eq:app:outputDiffGFTfirstTermSecond}, consider the following lemma, which is conveniently proved after completing the current proof.

\begin{lemma}\label{lem:app:GiMatrix}
For all $i\in\{1,\dots,N\}$, define $\mtG_{i} \in \fdR^{N \times N}$ by
\begin{equation}\label{eq:app:GiMatrix}
    [\mtG_{i}]_{tj} = \begin{cases}
        \fnth_{t}'(\lambda_{i}) & \text{ if }j=i \\
        \frac{\fnth_{t}(\lambda_{i})-\fnth_{t}(\lambda_{j})}{\lambda_{i}-\lambda_{j}} & \text{ if } j \neq i
    \end{cases}
\end{equation}
where $\fnth_{t}$ is the frequency response at node $\lmv_{t}$ and $\fnth'_{t}$ is its derivative. Then
\begin{equation} \label{eq:app:outputDiffGFTfirstTermSecondSolved}
%
\begin{aligned}
    \sum_{i=1}^{N} \sctx_{i} & \sum_{k=0}^{K} \diag(\vch^{(k)}) \sum_{r=0}^{k-1}\lambda_{i}^{k-r-1}\mtS^{r} \mtE_{U} \vcv_{i} \\ & = \sum_{i=1}^{N} \sctx_{i} \big( \mtV \circ \mtG_{i} \big) \mtV^{\Tr} \mtE_{U} \vcv_{i}.
\end{aligned}
%
\end{equation}
\end{lemma}

With Lemma \ref{lem:app:GiMatrix} in place, use \eqref{eq:app:outputDiffGFTfirstTermFirstSolved} in \eqref{eq:app:outputDiffGFTfirstTermFirst} and \eqref{eq:app:outputDiffGFTfirstTermSecondSolved} in \eqref{eq:app:outputDiffGFTfirstTermSecond}, and this in turn back into \eqref{eq:app:outputDiffGFT} to obtain
% eq:app:outputDiffGFTsolved
\begin{equation} \label{eq:app:outputDiffGFTsolved}
%
\begin{aligned}
    \big( \mtfnH^{\text{nv}} & (\mthS)-\mtfnH^{\text{nv}}(\mtS) \big) \vcx \\ & = \sum_{i=1}^{N} \sctx_{i} m_{i} \diag(\vcfnth'(\lambda_{i}))\vcv_{i} \\ & \quad + \sum_{i=1}^{N} \sctx_{i} \big( \mtV  \circ \mtG_{i} \big) \mtV^{\Tr} \mtE_{U} \vcv_{i} \\ & \quad  + \sum_{i=1}^{N} \sctx_{i} \mtD \vcv_{i}.
\end{aligned}
%
\end{equation}
%
Next, compute the norm of \eqref{eq:app:outputDiffGFTsolved} by applying the triangle inequality to compute the norms of each of the three summands. For the norm of the first term in \eqref{eq:app:outputDiffGFTsolved}, the triangle inequality gives
%
\begin{equation}
%
\begin{aligned}
    \Big\| \sum_{i=1}^{N} & \sctx_{i} m_{i} \diag(\vcfnth'(\lambda_{i}))\vcv_{i} \Big\| \\ & \leq \sum_{i=1}^{N} | \sctx_{i} | |m_{i}| \| \diag(\vcfnth'(\lambda_{i})) \| \| \vcv_{i}\|
\end{aligned}
%
\end{equation}
%
where the submultiplicative property of the operator norm was used to bound $\| \diag(\vcfnth'(\lambda_{i})) \vcv_{i}\| \leq \| \diag(\vcfnth'(\lambda_{i})) \| \| \vcv_{i}\|$. Leveraging that $\|\vcv_{i}\|=1$, that $\|\diag(\vcfnth'(\lambda_{i}))\| = \max_{j} |\fnth'_{j}(\lambda_{i})| \leq C$ by Lipschitz continuity, and that $|m_{i}| \leq \sceps$ by the hypothesis that $\|\mtS - \mthS\| \leq \sceps$, the norm can be further bounded as
%
\begin{equation}\label{eq:app:lipschitzIneq1}
%
\begin{aligned}
    \Big\| \sum_{i=1}^{N} \sctx_{i} m_{i} & \diag(\vcfnth'(\lambda_{i})) \vcv_{i} \Big\| \\ & \leq \sceps C \sum_{i=1}^{N} | \sctx_{i} | = \sceps C \| \vctx \|_{1} \\ & \leq \sceps C \sqrt{N} \| \vctx \|= \sceps C \sqrt{N} \| \vcx \|.
\end{aligned}
%
\end{equation}
%
Note that the inequality between the $1$-norm and the $2$-norm was used, as well as the fact that the GFT is a Parseval operator. For the norm of the second term in \eqref{eq:app:outputDiffGFTsolved}, the triangle inequality together with the submultiplicativity of the operator norm are used to get
%
\begin{equation}\label{eq:app:lipschitzIneq2}
%
\begin{aligned}
    \Big\| & \sum_{i=1}^{N} \sctx_{i} \big(\mtV \circ \mtG_{i}\big) \mtV^{\Tr} \mtE_{U} \vcv_{i} \Big\| \\ & \leq \sum_{i=1}^{N} | \sctx_{i} | \| \mtV \circ \mtG_{i} \| \| \mtV^{\Tr} \| \| \mtE_{U}\| \| \vcv_{i}\| \\ & \leq 8\sceps CN\sqrt{N} \| \vcx\|.
\end{aligned}
%
\end{equation}
%
For the last inequality to hold, denote by $\|\cdot\|_{\max}$ the entrywise maximum norm of a matrix, and note that $\|\vcV \circ \mtG_{i} \| \leq N \| \vcV \circ \mtG_{i}\|_{\max} = N \max_{t,j \in \{1,\ldots,N\}} (| [\vcV]_{tj}|\ |[\mtG_{i}]_{tj}| ) \leq NC$ for all $i \in \{1,\ldots,N\}$, by the Lipschitz continuity of the frequency responses and the fact that $\mtV$ is an orthogonal matrix, so $|[\vcV]_{tj}|\le 1$ for all $t$ and $j$. Also recall that $\|\mtE_{U}\| \leq 8 \sceps$. Finally, note that the inequality between the $1$-norm and $2$-norm of vectors together with the Parseval nature of the GFT were used. For the third term in \eqref{eq:app:outputDiffGFTsolved}, it simply holds that
%
\begin{equation}\label{eq:app:lipschitzIneq3}
%
\begin{aligned}
    \Big\| & \sum_{i=1}^{N} \sctx_{i} \mtD \vcv_{i} \Big\| = \Big\| \mtD \sum_{t=1}^{N} \sctx_{i} \vcv_{i} \Big\| = \| \mtD \vcx \| \\ & \leq \| \mtD \| \| \vcx \| \leq \bigOh(\sceps^{2}) \| \vcx\|.
\end{aligned}
%
\end{equation}
%
Substituting \eqref{eq:app:lipschitzIneq1}, \eqref{eq:app:lipschitzIneq2}, and \eqref{eq:app:lipschitzIneq3} into \eqref{eq:app:outputDiffGFTsolved} gives \eqref{eq:stability}, which concludes the proof.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:app:GiMatrix}]
We have for all $i\in\{1,\ldots,N\}$ that
%
\begin{equation}\label{eq:app:lemEq1}
%
\begin{aligned}
    & \sum_{k=0}^{K} \diag(\vch^{(k)}) \sum_{r=0}^{k-1}\lambda_{i}^{k-r-1}\mtV \diag(\vclambda^{r}) \\ & = \sum_{k=0}^{K} \diag(\vch^{(k)}) \mtV \sum_{r=0}^{k-1}\lambda_{i}^{k-r-1} \diag(\vclambda^{r}).
\end{aligned}
%
\end{equation}
%
The last summation $\sum_{r=0}^{k-1}\lambda_{i}^{k-r-1} \diag(\vclambda^{r})$ is a diagonal matrix, where the $j^{\text{th}}$ element of the diagonal can be written as
%
\begin{equation}
%
\begin{aligned}
    \Big[& \sum_{r=0}^{k-1} \lambda_{i}^{k-r-1} \vclambda^{r} \Big]_{j} = \sum_{r=0}^{k-1} \lambda_{i}^{k-r-1}\lambda_{j}^{r} \\ & = [\vcgamma_{i}]_{j} \coloneqq \begin{cases}
    k \lambda_{i}^{k-1} & \text{ if } j=i \\
    \frac{\lambda_{i}^{k} - \lambda_{j}^{k}}{\lambda_{i}-\lambda_{j}} & \text{ if } j \neq i
    \end{cases}
\end{aligned}
%
\end{equation}
%
so that
%
\begin{equation}\label{eq:app:lemEq2}
%
\begin{aligned}
    & \sum_{k=0}^{K} \diag(\vch^{(k)}) \mtV \sum_{r=0}^{k-1}\lambda_{i}^{k-r-1} \diag(\vclambda^{r}) \\ & = \sum_{k=0}^{K} \diag(\vch^{(k)}) \mtV \diag(\vcgamma_{i}).
\end{aligned}
%
\end{equation}
%
Note that the $i$ subindex in $\vcgamma_{i}$ indicates that this is parametrized by $\lambda_{i}$, while each entry of this vector, i.e., the $j^{\text{th}}$ entry, actually depends on $\lambda_{j}$. Now, remark that since we have a full matrix $\mtV$ in between two diagonal matrices, the matrix product does not commute.

To continue simplifying the expressions, start by considering a vector $\vca \in \fdR^{N}$, a matrix $\mtB \in \fdR^{N \times N}$ and another vector $\vcc \in \fdR^{N}$. Observe that
%
\begin{align*}
%    \diag(\vca) \mtB \diag(\vcc) & =
%    \begin{bmatrix}
%        a_{1} & 0 & \cdots & 0 \\
%        0 & a_{2} & \cdots & 0 \\
%        \vdots & \vdots & \ddots & \vdots \\
%        0 & 0 & \cdots & a_{N}
%    \end{bmatrix}
%    \begin{bmatrix}
%        b_{11} & b_{12} & \cdots & b_{1N} \\
%        b_{21} & b_{22} & \cdots & b_{2N} \\
%        \vdots & \vdots & \ddots & \vdots \\
%        b_{N1} & b_{N2} & \cdots & b_{NN}
%    \end{bmatrix}
%    \begin{bmatrix}
%        c_{1} & 0 & \cdots & 0 \\
%        0 & c_{2} & \cdots & 0 \\
%        \vdots & \vdots & \ddots & \vdots \\
%        0 & 0 & \cdots & c_{N}
%    \end{bmatrix} \\
%    & =
%    \begin{bmatrix}
%        a_{1}c_{1}b_{11} & a_{1}c_{2}b_{12} & \cdots & a_{1}c_{N}b_{1N} \\
%        a_{2}c_{1}b_{21} & a_{2}c_{2}b_{22} & \cdots & a_{2}c_{N}b_{2N} \\
%        \vdots & \vdots & \ddots & \vdots \\
%        a_{N}c_{1}b_{N1} & a_{N}c_{2}b_{N2} & \cdots & a_{N}c_{N}b_{NN}
%    \end{bmatrix} \\
    \big[ \diag(\vca) \mtB \diag(\vcc)\big]_{ij} & = a_{i}c_{j}b_{ij} \\
    \diag(\vca) \mtB \diag(\vcc) & = \mtB \circ (\vca \vcc^{\Tr}).
\end{align*}
%
Therefore, it can be written
%
\begin{equation}
    \sum_{k=0}^{K} \diag(\vch^{(k)}) \mtV \diag(\vcgamma_{i}) = \mtV \circ \Big( \sum_{k=0}^{K} \vch^{(k)} \vcgamma_{i}^{\Tr} \Big) = \mtV \circ \mtG_{i}
\end{equation}
%
where $\mtG_{i} \in \fdR^{N \times N}$ is the matrix defined in \eqref{eq:app:GiMatrix};
%
\begin{equation}
%
\begin{aligned}
    \sum_{k=0}^{K} h_{tk} [\vcgamma_{i}]_{j} & = \begin{cases}
        \sum_{k=0}^{K} h_{tk}k \lambda_{i}^{k-1} & \text{ if }j=i \\
        \sum_{k=0}^{K} h_{tk}\frac{\lambda_{i}^{k} - \lambda_{j}^{k}}{\lambda_{i}-\lambda_{j}} & \text{ if } j \neq i
    \end{cases}
    \\ & =
    \begin{cases}
        \fnth_{t}'(\lambda_{i}) & \text{ if }j=i \\
        \frac{\fnth_{t}(\lambda_{i})-\fnth_{t}(\lambda_{j})}{\lambda_{i}-\lambda_{j}} & \text{ if } j \neq i
    \end{cases}
    = [\mtG_{i}]_{tj}.
\end{aligned}
%
\end{equation}
Using this result in \eqref{eq:app:lemEq1} and \eqref{eq:app:lemEq2} gives that
\begin{equation}
    \sum_{k=0}^{K} \diag(\vch^{(k)}) \sum_{r=0}^{k-1}\lambda_{i}^{k-r-1}\mtV \diag(\vclambda^{r}) = \mtV \circ \mtG_{i}.
\end{equation}
This equality, together with the fact that $\mtS^{r} = \mtV \diag(\vclambda^{r}) \mtV^{\Tr}$, gives the desired result \eqref{eq:app:outputDiffGFTfirstTermSecondSolved}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                   OPTIMAL UNBIASED NVGF                   %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Optimal Unbiased Node-Variant Graph Filters}

\begin{proof}[Proof of Lemma~\ref{l:unbiased}]
The NVGF-based estimator is given by:
% eq:app:NVGFestimator
\begin{equation} \label{eq:app:NVGFestimator}
    \vchy = \mtfnH^{\text{nv}}(\mtS) \vcx + \vcc.
\end{equation}
%
This estimator is unbiased if and only if $\xp[\vchy] = \xp[\vcy] = \xp[\fnrho(\vcx)] = \vcmu_{\rho}$. By \eqref{eq:app:NVGFestimator} and linearity of expectation, this holds precisely when
%
\begin{equation}
    \xp[\vchy] = \mtfnH^{\text{nv}}(\mtS) \xp[\vcx] + \vcc = \mtfnH^{\text{nv}}(\mtS) \vcmu_{x} + \vcc = \vcmu_{\rho}.
\end{equation}
%
This is equivalent to the condition on $\vcc$ that
%
\begin{equation}
    \vcc = \vcmu_{\rho} - \mtfnH^{\text{nv}}(\mtS)\vcmu_{x},
\end{equation}
%
which completes the proof.
\end{proof}

\begin{proof}[Proof of Proposition~\ref{prop:NVGFoptimal}]
Given an unbiased estimator $\vchy = \mtfnH^{\text{nv}}(\mtS) ( \vcx - \vcmu_{x}) + \vcmu_{\rho}$, an optimal matrix $\mtH^{\opt} \in \fdR^{N \times (K+1)}$ of filter taps are the ones that minimize \eqref{eq:approx}. First, note that the estimator output at node $\lmv_{i}$, i.e., the $i^{\text{th}}$ entry of $\vchy$, is given by
% eq:app:singleNodeOutput
\begin{equation} \label{eq:app:singleNodeOutput}
    \schy_{i} = [\vchy]_{i} = \vcu_{i}^{\Tr} \diag( \mtLambda \vch_{i}) \mtV^{\Tr} (\vcx-\vcmu_{x}) + \scmu_{\rho i}
\end{equation}
%
where $\scmu_{\rho i} = [\vcmu_{\rho}]_{i}$, $\vcu_{i} \in \fdR^{N}$ is the $i^{\text{th}}$ row of $\mtV$, and $\vch_{i} \in \fdR^{K+1}$ is the $i^{\text{th}}$ row of the matrix $\mtH$; see \cite{Segarra2017-GraphFilterDesign}. Note that if $\vca, \vcb, \vcc$ are three $N$-dimensional real vectors, then $\vca^{\Tr} \diag(\vcb) \vcc = \sum_{i=1}^{N} a_{i}b_{i}c_{i}$, which means that they commute, i.e., $\vca^{\Tr} \diag(\vcb) \vcc = \vcc^{\Tr} \diag(\vca) \vcb$. Using this fact in \eqref{eq:app:singleNodeOutput} yields
% eq:app:singleNodeWithA
\begin{equation} \label{eq:app:singleNodeWithA}
    \schy_{i} = (\vcx - \vcmu_{x})^{\Tr} \mtV \diag(\vcu_{i}) \mtLambda \vch_{i} + \scmu_{\rho i} = \vcfna_{i}(\vcx)^{\Tr} \vch_{i} + \scmu_{\rho i}
\end{equation}
%
with
%
\begin{equation*}
    \vcfna_{i}(\vcx) = \mtLambda^{\Tr} \diag(\vcu_{i}) \mtV^{\Tr} (\vcx - \vcmu_{x}).
\end{equation*}
%

The objective function in \eqref{eq:approx} can be rewritten as
% eq:app:approx
\begin{equation} \label{eq:app:approx}
    \xp \big[ \| \vchy - \vcy \|_2^{2} \big] = \xp \Big[ \sum_{i=1}^{N} (\schy_{i}-y_{i})^{2} \Big] = \sum_{i=1}^{N} \xp\big[ (\schy_{i}-y_{i})^{2} \big].
\end{equation}
%
Since each $\schy_{i}$ depends only on $\vch_{i}$, as indicated in \eqref{eq:app:singleNodeWithA}, minimizing \eqref{eq:app:approx} over $\mtH$ is equivalent to minimizing each of the summands in \eqref{eq:app:approx} over $\vch_{i}$. Therefore, \eqref{eq:approx} becomes equivalent to the following system of $N$ optimization problems over each of the rows of $\mtH$:
% eq:app:singleApprox
\begin{equation} \label{eq:app:singleApprox}
    \min_{\vch_{i} \in \fdR^{K+1}} \xp\big[ (\schy_{i}-y_{i})^{2} \big], \quad i \in \{1,\ldots,N\}.
\end{equation}
%
Substituting \eqref{eq:app:singleNodeWithA} into the $i^{\text{th}}$ objective function of \eqref{eq:app:singleApprox} gives
% eq:app:singleObjective
\begin{equation} \label{eq:app:singleObjective}
%
\begin{aligned}
    & \xp \big[ (\schy_{i}-y_{i})^{2} \big] = \xp \big[ \big(\vcfna_{i}(\vcx)^{\Tr} \vch_{i} + \mu_{\rho i} - \rho(x_{i}) \big)^{2} \big] \\
    & = \vch_{i}^{\Tr} \xp\big[\vcfna_{i}(\vcx) \vcfna_{i}(\vcx)^{\Tr}\big]\vch_{i} \\ & \quad - 2 \xp \big[ (\rho(x_{i})-\mu_{\rho i}) \vcfna_{i}(x)^{\Tr} \big]\vch_{i} \\ & \quad + \xp \big[ (\rho(x_{i})- \mu_{\rho i})^{2} \big].
\end{aligned}
%
\end{equation}
%
Now, with
%
\begin{align*}
    \mtR_{i} & = \xp \big[\vcfna_{i}(\vcx) \vcfna_{i}(\vcx)^{\Tr} \big] \\ & = \mtLambda^{\Tr} \diag(\vcu_{i}) \mtV^{\Tr} \mtC_{x} \mtV \diag(\vcu_{i}) \mtLambda, \label{eq:app:Ri} \\
    \vcp_{i} & = \xp \big[ (\rho(x_{i})-\mu_{\rho i}) \vcfna_{i}(x) \big] \\ & = \mtLambda^{\Tr} \diag(\vcu_{i}) \mtV^{\Tr} \xp \big[ (\rho(x_{i})- \mu_{\rho i}) (\vcx - \vcmu_{x}) \big] %\label{eq:app:pi}
\end{align*}
%
the $i^\text{th}$ objective \eqref{eq:app:singleObjective} becomes
% eq:app:paraboloid
\begin{equation} \label{eq:app:paraboloid}
    \xp \big[ (\schy_{i}-y_{i})^{2} \big] = \vch_{i}^{\Tr} \mtR_{i} \vch_{i} - 2 \vcp_{i}^{\Tr} \vch_{i} + \xp \big[ (\rho(x_{i})-\mu_{\rho i})^{2} \big].
\end{equation}
%
Since $\mtR_{i}$ is a positive semidefinite matrix, \eqref{eq:app:paraboloid} is a convex quadratic function in $\vch_i$. Therefore, setting the gradient of this function to zero, it can be concluded that $\vch_i^{\opt}$ is a global minimizer of \eqref{eq:app:paraboloid} if and only if $2\mtR_i \vch_i^{\opt} - 2\vcp_i = \vcZeros$, or, equivalently,
%
\begin{equation}
    \mtR_{i} \vch_{i}^{\opt} = \vcp_{i}.
\end{equation}
%
This completes the proof.
\end{proof}

%\begin{proof}[Proof of Corollary~\ref{cor:ReLU}]
%Consider the estimator of interest given by $\schy_{i} = (\scxi^{2}/\sigma_{x}^{2}) x_{i} + \mu_{\rho}$. Setting this equal to the output at node $\lmv_{i}$ given in \eqref{eq:app:singleNodeOutput} yields
%\begin{equation}
%    (\scxi^{2}/\sigma_{x}^{2}) x_{i} = \vcu_{i}^{\Tr} \diag( \mtLambda \vch_{i}) \mtV^{\Tr} \vcx
%\end{equation}
%for all $i\in\{1,\ldots,N\}$, where the zero-mean and i.i.d.\ assumptions have been used to eliminate $\vcmu_{x}$ and cancel the bias $\mu_\rho=\scmu_{\rho i}$. Recalling that $\mtV$ is orthogonal, this system of equations may be rewritten as
%\begin{equation}
%    \left( \frac{\scxi^2}{\sigma_x^2} \mtV - \begin{bmatrix}
%        \vcu_1^{\Tr} \diag(\mtLambda \vch_1) \\
%        \vcu_2^{\Tr} \diag(\mtLambda \vch_2) \\
%        \vdots \\
%        \vcu_N^{\Tr} \diag(\mtLambda \vch_N)
%    \end{bmatrix} \right) \mtV^{\Tr} \vcx  = \vcZeros.
%\end{equation}
%Since this holds for all $\vcx\in\fdR^{N}$, it satisfies that
%\begin{align}
%    \vcZeros
%%    &= \left( \frac{\scxi^2}{\sigma_x^2} \mtV - \begin{bmatrix}
%%        \vcu_1^{\Tr} \diag(\mtLambda \vch_1) \\
%%        \vcu_2^{\Tr} \diag(\mtLambda \vch_2) \\
%%        \vdots \\
%%        \vcu_N^{\Tr} \diag(\mtLambda \vch_N)
%%    \end{bmatrix} \right) \begin{bmatrix}
%%        \mtV^{\Tr} \mtV \vce_1 & \mtV^{\Tr} \mtV \vce_2 & \cdots & \mtV^{\Tr} \mtV \vce_N
%%    \end{bmatrix} \\
%%    &
%    = \left( \frac{\scxi^2}{\sigma_x^2} \mtV - \begin{bmatrix}
%        \vcu_1^{\Tr} \diag(\mtLambda \vch_1) \\
%        \vcu_2^{\Tr} \diag(\mtLambda \vch_2) \\
%        \vdots \\
%        \vcu_N^{\Tr} \diag(\mtLambda \vch_N)
%    \end{bmatrix} \right) \mtI,
%\end{align}
%where $\vce_i\in\fdR^N$ is the $i^\text{th}$ standard unit vector and $\mtI$ is the $N\times N$ identity matrix. Therefore, recalling that $\vcu_i$ is the $i^\text{th}$ row of $\mtV$ gives
%\begin{equation}
%    \diag(\mtLambda \vch_i) \vcu_i = \frac{\scxi^2}{\sigma_x^2} \vcu_i
%\end{equation}
%for all $i\in\{1,\ldots,N\}$. That is, $(\scxi^2/\sigma_x^2,\vcu_i)$ is an eigenvalue-eigenvector pair of the matrix $\diag(\mtLambda\vch_i)$. Consequently, $\sigma_x^2 \diag(\vcu_i) \mtLambda \vch_i = \scxi^2 \vcu_i$, implying that
%\begin{equation} \label{eq:app:iidOptimalEquation}
%    \sigma_x^2 \mtLambda^{\Tr}\diag(\vcu_i^2)\mtLambda \vch_i = \scxi^2 \mtLambda^{\Tr} \diag(\vcu_i)\vcu_i.
%\end{equation}
%Now, under the given assumptions that $\mtC_x = \sigma_x^2 \mtI$ and $\vcmu_x = \vcZeros$, the values of $\mtR_i$ and $\vcp_i$ in Proposition \ref{prop:NVGFoptimal} become
%\begin{align*}
%    \mtR_{i} &= \sigma_{x}^{2} \mtLambda^{\Tr} \diag(\vcu_{i}^{2}) \mtLambda, \\
%    \vcp_{i} &= \scxi^2 \mtLambda^{\Tr} \diag(\vcu_{i}) \vcu_{i},
%\end{align*}
%respectively. Remark that the definition $\scxi^2 = \xp\big[ \rho(x) x\big]=\xp\big[ \rho(x_i) x_i\big]$ has been used together with the fact that $\vcx$ has i.i.d.\ elements in deriving this expression for $\vcp_i$. Therefore, \eqref{eq:app:iidOptimalEquation} becomes $\mtR_i \vch_i = \vcp_i$, so the filter taps $\vch_i$ at node $\lmv_i$ satisfy the optimality condition \eqref{eq:NVGFoptimal}. Since this holds for all $i\in\{1,\ldots,N\}$, Proposition \ref{prop:NVGFoptimal} yields that the given unbiased NVGF-based estimator is optimal for \eqref{eq:approx}. To complete the proof, note that if the distribution of each element of the input $\vcx$ is symmetric around zero and $\rho(\cdot)=\ReLU(\cdot)$, then $\mu_{\rho} = \xp\big[x\indFn\{x \geq 0\}\big] = \xp\big[x\big]/2 = 0$ and $\scxi^2 = \xp\big[x^2\indFn\{x \geq 0\}\big] = \xp\big[ x^2\big]/2 = \sigma_x^2/2$, so the optimal estimator reduces to $\schy_{i} = x_{i}/2$.
%\end{proof}

%%%%% BEGIN OLD PROOF
\begin{comment}
\begin{proof}[Proof of Corollary~\ref{cor:ReLU}]
Since $\mtC_{x} = \sigma^{2}_{x} \mtI$, the value of $\mtR_{i}$ in Proposition \ref{prop:NVGFoptimal} becomes
%
\begin{equation}
    \mtR_{i} = \sigma_{x}^{2} \mtLambda^{\Tr} \diag(\vcu_{i}^{2}) \mtLambda.
\end{equation}
%
Since $\vcmu_{x} = 0$, the value of $\vcp_{i}$ in Proposition \ref{prop:NVGFoptimal} becomes
%
\begin{equation}
    \vcp_{i} = \scxi^2 \mtLambda^{\Tr} \diag(\vcu_{i}) \vcu_{i},
\end{equation}
%
where the fact that $\vcx$ has i.i.d.\ elements has been used together with the definition $\scxi^2 = \xp\big[ \rho(x) x\big]=\xp\big[ \rho(x_i) x_i\big]$. Replacing these two values in \eqref{eq:NVGFoptimal} yields
%
\begin{equation}
    \sigma_{x}^{2}\mtLambda^{\Tr} \diag(\vcu_{i}^{2}) \mtLambda \vch_{i}^{\opt} = \scxi^2 \mtLambda^{\Tr} \vcu_{i}^{2},
\end{equation}
%
from which we get that {\color{red}$\sigma_{x}^{2}\diag(\mtLambda \vch_{i}^{\opt}) = \xp \big[ \rho(x_{i}) x_{i} \big]\mtI$. Now, since $x_{i}$ share the same distribution for all $i$, and since $\rho(x) = x \indFn\{ x \geq 0\}$ is the $\ReLU$, we can define $\xi^{2} = \xp[x^{2} \indFn\{x \geq 0\}]$. Finally, noting that $\vcmu_{\rho i} = \xp[x \indFn\{x \geq 0\}] = \scmu_{\rho}$ is the same for all nodes, and replacing $\diag(\mtLambda \vch_{i}^{\opt})$ in \eqref{eq:app:singleNodeOutput}, the output at the $i^{\text{th}}$ node becomes
%
\begin{equation}
    \schy_{i} = \frac{\scxi^{2}}{\sigma_{x}^{2}} \vcu_{i}^{\Tr} \mtV^{\Tr} \vcx + \scmu_{\rho} = \frac{\scxi^{2}}{\sigma_{x}^{2}} \vce_{i}^{\Tr} \vcx + \mu_{\rho} = \frac{\scxi^{2}}{\sigma_{x}^{2}} x_{i} +\mu_{\rho}.
\end{equation}
%
}To complete the proof, note that if the distribution of each element of the input $\vcx$ is symmetric around zero, then $\scxi^{2} = \xp[x^2]/2 = \sigma_{x}^{2}/2$.
\end{proof}
\end{comment}
%%%%% END OLD PROOF




% \begin{proof}[Proof of Corollary~\ref{cor:stationary}]
% In this case, the mean of the input signal is $\vcmu_{x} = \vcZeros$ and the covariance matrix is diagonalizable as $\mtC_{x} = \mtV \diag(\vcq) \mtV^{\Tr}$, where $\vcq \in \fdR^{N}$ is the power spectral density. The value of $\mtR_{i}$ in Proposition \ref{prop:NVGFoptimal} becomes
% %
% \begin{equation}
%     \mtR_{i} = \mtLambda^{\Tr} \diag(\vcu_{i}) \diag(\vcq) \diag(\vcu_{i}) \mtLambda = \mtLambda^{\Tr} \diag(\vcu_{i}^{2} \circ \vcq) \mtLambda,
% \end{equation}
% %
% while the value of $\vcp_{i}$ in Proposition \ref{prop:NVGFoptimal} becomes
% %
% \begin{equation}
% \vcp_{i} = \mtLambda^{\Tr} \diag(\vcu_{i}) \mtV^{\Tr} \xp\big[ \rho(x_{i}) \vcx\big].
% \end{equation}
% %
% With these two results, the optimal filter equation \eqref{eq:NVGFoptimal} becomes
% %
% \begin{equation}
%     \mtLambda^{\Tr} \diag(\vcu_{i}^{2} \circ \vcq) \mtLambda \vch_{i}^{\opt} = \mtLambda^{\Tr} \diag(\vcu_{i}) \mtV^{\Tr} \xp\big[ \rho(x_{i})\vcx \big].
% \end{equation}
% %
% Therefore, $\vch_i^{\opt}$ satisfying $\diag(\vcu_{i} \circ \vcq) \mtLambda \vch_{i}^{\opt} = \mtV^{\Tr} \xp [\rho(x_{i}) \vcx]$ solves the optimal filter equation, which completes the proof.
% \end{proof}