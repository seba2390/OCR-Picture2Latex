%!TEX root = 00-NVGF.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                                                                        %%%%
%%%%                              INTRODUCTION                              %%%%
%%%%                                                                        %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% sec:intro
%%%%%%%%%%%%%%

\noindent Graph neural networks (GNNs) \cite{Bronstein2017-GeometricDL, Gama2020-GNN} are learning architectures that have been successfully applied to a wide array of graph signal processing (GSP) problems ranging from recommendation systems \cite{Monti2017-RecommendationGNN, Levie2018-CayleyNets} and authorship attribution \cite{Segarra2015-Authorship, Gama2019-Archit}, to physical network problems including wireless communications \cite{Eisen2020-WirelessEGNN}, control \cite{Gama2021-LQRGNN}, and sensor networks \cite{Owerko2018-Sensor}. In the context of GSP, frequency analysis has been successful at providing theoretical insight into the observed success of GNNs \cite{Gama2020-Stability, Kenlay2021-StabilityRewiring, Levie2020-TransferabilitySpectral} and graph scattering transforms \cite{Wolf2019-GeometricScattering, Lerman2020-Scattering}.

Of particular interest is the seminal work by Mallat \cite{Mallat2012-Scattering, Bruna2013-Scattering}, concerning discrete-time signals and images, that argues that the improved performance of convolutional neural networks (CNNs) over linear convolutional filters is due to the activation functions. Concretely, nonlinear activation functions allow for high-frequency content to be spread into lower frequencies, where it can be processed in a stable manner---a feat that cannot be achieved by convolutional filters alone\ifundefined{arXiv}.\else, which are unstable when processing high frequencies.\fi
Leveraging the notion of graph Fourier transform (GFT) \cite{Sandryhaila2014-DSPGfreq}, these results have been extended to GNNs \cite{Gama2020-Stability}, establishing that the use of functions capable of creating low-eigenvalue frequency content allows them to be robust to changes in the graph topology, facilitating scalability and transferability \cite{Ruiz2021-GNNs}.

While nonlinear activation functions play a key role in the creation of low-eigenvalue frequency content, it is not possible to know the exact shape in which this frequency content is actually generated. Node-variant graph filters (NVGFs) \cite{Segarra2017-GraphFilterDesign}, which essentially assign a different filter tap to each node in the graph, are also able to generate frequency content. Different from nonlinear activation functions, this frequency content can be exactly computed, given the filter taps. Thus, by learning or carefully designing these filter taps, it is possible to know exactly how the frequency content is being generated.

The NVGF is a linear filter, which means that replacing the nonlinear activation functions with NVGFs actually renders the whole architecture a linear one. Therefore, by comparing the performance of this architecture to that of a traditional GNN, it is possible to isolate the role of frequency creation in the overall performance of the architecture, from that of the nonlinear nature of mappings. The contributions of this paper can be summarized as follows:

\ifundefined{arXiv}
\begin{list}{}{
        \setlength{\labelwidth}{20pt}
        \setlength{\leftmargin}{2pt}
        \setlength{\labelsep}{1pt}
        \setlength{\itemsep}{-1pt}
        \setlength{\topsep}{-1pt}
        \setlength{\parskip}{-1pt}
    }

    \item[1.] We introduce NVGFs as a means of replacing nonlinear activation functions, motivated by their ability to create frequencies. We obtain closed-form expressions for the frequency response of NVGFs as a function of the filter taps.
    \item[2.] We prove that NVGFs are Lipschitz continuous with respect to changes in the underlying graph topology.
    \item[3.] We put forth a framework for designing NVGFs.%We find the filter taps that best approximate a given nonlinear function for a given distribution of the input data.
    \item[4.] We propose a GNN architecture where the nonlinear activation function is replaced by a NVGF. The filter taps of the NVGF can be either learned or designed. The resulting architecture decouples the role of frequency creation from the nonlinear nature of the GNN.
    \item[5.] We investigate the problem of authorship attribution to demonstrate, both quantitatively and qualitatively, the role of frequency creation in the performance of a GNN, and its relationship to the nonlinear nature of traditional architectures.

\end{list}
\else
\begin{list}{}{
        \setlength{\labelwidth}{20pt}
        \setlength{\leftmargin}{17.5pt}
        \setlength{\labelsep}{1.5pt}
        \setlength{\itemsep}{-1pt}
        \setlength{\topsep}{0pt}
        \setlength{\parskip}{0pt}
    }

    \item[\textbf{(C1)}] We introduce NVGFs as a means of replacing nonlinear activation functions, motivated by their ability to create frequencies. We obtain closed-form expressions for the frequency response of NVGFs as a function of the filter taps.
    \item[\textbf{(C2)}] We prove that NVGFs are Lipschitz continuous with respect to changes in the underlying graph topology.
    \item[\textbf{(C3)}] We put forth a framework for designing NVGFs.%We find the filter taps that best approximate a given nonlinear function for a given distribution of the input data.
    \item[\textbf{(C4)}] We propose a GNN architecture where the nonlinear activation function is replaced by a NVGF. The filter taps of the NVGF can be either learned from data (Learn NVGF) or obtained by design (Design NVGF). The aim of the resulting architecture is to decouple the role of frequency creation from the nonlinear nature of the GNN.
    \item[\textbf{(C5)}] We investigate the problem of authorship attribution to demonstrate, both quantitatively and qualitatively, the role of frequency creation in the performance of a GNN, and its relationship to the nonlinear nature of traditional architectures.

\end{list}
\fi

In essence, we show that nonlinear activation functions are not strictly required for creating frequencies, as originally thought in \cite{Mallat2012-Scattering, Gama2020-Stability}, but that linear NVGF activation functions are sufficient. Furthermore, we demonstrate that this frequency content can be learned with respect to the specific problem under study. \ifundefined{arXiv}All proofs, as well as the code and further simulations, can be found online\footnote{Proofs: \url{http://arxiv.org/abs/2106.00089} \\ \indent $\ \, \, $ Code: \url{http://github.com/fgfgama/nvgf}}\else All proofs, as well as further simulations, can be found in the appendices. Code can be found online at \url{http://github.com/fgfgama/nvgf}.\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%                              RELATED WORK                              %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Related work} \label{subsec:related}

\titledParagraph{Related work.} GNNs constitute a very active area of research \cite{Rey2019-EncoderDecoder, Rey2021-Overparametrized}. From a GSP perspective, spectral filtering is used in \cite{Bruna2014-SpectralGNN}, it is then replaced by computationally simpler Chebyshev polynomials \cite{Defferrard2016-ChebNets}, and subsequently by general graph convolutional filters \cite{Gama2019-Archit}. GNNs were also adopted in non-GSP problems \cite{Kipf2017-GCN, Weinberger2019-SGC, Velickovic2018-GAT}. The proposed replacement of nonlinear activation functions by NVGFs creates a linear architecture that uses both convolutional and non-convolutional graph filters.

NVGFs are first introduced in \cite{Segarra2017-GraphFilterDesign} to extend time-variant filters into the realm of graph signals. In that work, NVGFs are used to optimally approximate linear operators in a distributed manner. In this paper, we focus on the frequency response of NVGFs and on their capacity to create frequency content.

Leveraging the notion of GFT, \cite{Gama2020-Stability} shows that a GNN is Lipschitz continuous to small changes in the underlying graph structure. Likewise, frequency analysis has been used to understand graph scattering transforms, where the filters used in the GNN are chosen to be wavelets (and not learned) \cite{Wolf2019-GeometricScattering, Lerman2020-Scattering}.
    %Other works along this line of research include \cite{Kenlay2021-StabilityRewiring, Levie2020-TransferabilitySpectral}.
In this paper, we focus on the role of frequency creation that is put forth in \cite{Mallat2012-Scattering, Gama2020-Stability}, and study NVGFs as linear mechanisms for achieving this.