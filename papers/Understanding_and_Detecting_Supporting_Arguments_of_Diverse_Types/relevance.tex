%The nature of this task can be abstract to ranking sentences by certain measurements, which potentially could provide us with some insights about how human beings select information during debates. Here we have xxx different types of features, and we further group them into xxx feature sets.

%The process of finding directly relevant sentence could be modeled as a ranking problem. In addition to features listed in section 4, we also consider similarity features and composite features as below:
%\XY{Due to the fact that most of the reference documents are closely related to the topic, and there are usually more than one relevant sentences but we only care about the most relevant one}
We cast the sentence-level supporting argument detection problem as a ranking task.\footnote{Many sentences in the citation article is relevant to the topic to various degrees. We focus on detecting the most relevant ones, and thus treat it as a ranking problem instead of a binary classification task.}  
%
Features in Section~\ref{sec:type} are also utilized here as ``Sentence features" with additional features considering the sentence position in the article. We further employ features that measure similarity between claims and sentences, and the composite features that leverage argument type information. 

\vspace{1mm}
\noindent \textbf{Similarity Features.} We compute similarity between claim and candidate sentence based on TF-IDF and average word embeddings. We also consider ROUGE~\cite{lin2004rouge}, a recall oriented metric for summarization evaluation. In particular, ROUGE-L, a variation based on longest common subsequence, is computed by treating claim as reference and each candidate sentence as sample summary. In similar manner we use BLEU~\cite{papineni2002bleu}, a precision oriented metric.

\vspace{1mm}
\noindent \textbf{Composite Features.} We adopt composite features to study the interaction of other features with type of the sentence. Given claim $c$ and sentence $s$ with any feature mentioned above, a composite feature function $\phi_{M\texttt{\textbf{\scriptsize (type, feature)}}}(s,c)$ is set to the actual feature value if and only if the argument type matches. For instance, if the ROUGE-L score is 0.2, and $s$ is of type \textsc{study}, then {\small $\phi_{M\texttt{\textbf{\scriptsize (study, ROUGE)}}}(s,c)=0.2$ }\\
{\small $\phi_{M\texttt{\textbf{\scriptsize (factual, ROUGE)}}}{\small(s,c)}$, $\phi_{M\texttt{\textbf{\scriptsize (opinion, ROUGE)}}}{\small(s,c)}$, $\phi_{M\texttt{\textbf{\scriptsize (reasoning, ROUGE)}}}(s,c)$} are all set to 0.

%features in Table \ref{tab:features} and try different combination of them when composed with type information. Table \ref{tab:featset} shows the basic feature sets we used. Here \textbf{Comp} stands for composite feature. For example, if we compose sentence level features with type, suppose the dimension for sentence level features is $\mathcal{D}$, then composite feature has dimension $4\mathcal{D}$. Only the dimensions corresponding to one of the four predicted type will be actual values for features, other dimensions will be set to zero. By doing this we could learn the interaction of features and types.

We choose LambdaMART~\cite{burges2010ranknet} for experiments, which is shown to be successful for many text ranking problems~\cite{chapelle2011yahoo}. Our model is evaluated by Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) using 5-fold cross validation. We compare to TFIDF and Word embedding similarity baselines, and LambdaMART trained with ngrams (unigrams and bigrams). 

%\vspace{-3mm}
\begin{table}[t]
{
%\hspace{4mm}
\fontsize{9}{10}\selectfont
\setlength{\tabcolsep}{0.6mm}
\begin{tabular}{|l|c|c|}
    \hline
        Feature set & \textbf{MRR} & \textbf{NDCG} \\ \hline
        \multicolumn{1}{|l|}{\textbf{Baselines}} & &\\ 
        TFIDF similarity & 45.48 & 56.48 \\ 
        W2V similarity & 47.65 & 59.00 \\ 
        Ngrams & 27.26 & 43.83 \\ \hline \hline
        
        \multicolumn{1}{|l|}{\textbf{Separate feature sets}} & & \\ 
        Sentence (Sen) & 55.38* & 65.09* \\ 
        Similarity (Simi) & 43.13 & 55.16 \\ 
        Comp(type, Sen) + Comp(type, Simi) & 55.75* & 64.91* \\ \hline 
%        Comp(type,Sen) + Comp(type,Sim) + Comp(type,Claim) & 55.80* & 64.85*  \\ \hline
        
        \multicolumn{1}{|l|}{\textbf{Additive Feature Test}} & & \\ 
        Sen + Ngrams + Simi  & 56.43* & 65.79* \\ 
        ~~~~ + Comp(type, Sen) + Comp(type, Simi) & \textbf{57.65*} & \textbf{66.51*} \\ 
        ~~~~ + Comp(type, Claim) & 56.58* & 65.68* \\ \hline
\end{tabular}
%\vspace{-3mm}
\caption{\fontsize{10}{12}\selectfont Supporting argument detection results. %``Sentence" stands for features discussed in section 4 for modeling candidate sentences. ``Similarity" refers to similarity features. 
Comp(type, Sen) stands for composite features of argument type and sentence features, similarly for Comp(type,Simi).  Comp(type,Claim) represents composite features of type and claim features. Results that are statistically significantly better than all three baselines are marked with $\ast$ (paired $t$-test, $p<0.05$).}
\label{tab:mainresult}
}
\end{table}


Results in Table~\ref{tab:mainresult} show that %Comp(type, Sen) stands for composite features of argument type and sentence features, similarly for Comp(type, Simi). We also compute sentence features for the given claim, and use Comp(type, Claim) to characterize the interplay between claim and argument types. 
%
using composite features with argument type information (Comp(type, Sen) + Comp(type, Simi)) can improve the ranking performance. Specifically, the best performance is achieved by adding composite features to sentence features, similarity features, and ngram features.
As can be seen, supervised methods outperform unsupervised baseline methods. And similarity features have similar performance as those baselines. The best performance is achieved by combination of sentence features, N-grams, similarity, and two composite types, which is boldfaced. Feature sets that significantly outperform all three baselines are marked with $\ast$.

%To show the difficulties of detecting each individual types, we breakdown ranking results by types for the best performing system in \ref{tab:mainresult}.
%From \ref{tab:breakdown} we can see, type study is easiest to detect and type reasoning is the hardest one, which is consistent with Inter-annotator agreement results on different types.

For feature analysis, we conduct $t$-test for individual feature values between supporting arguments and the others. We breakdown features according to their argument types and show top salient composite features in Table \ref{tab:significance}. 
%
For all sentences of type \textsc{study}, relevant ones tend to contain more ``percentage" and more concrete words. We also notice those sentences with more hedging words are more likely to be considered. For sentences of \textsc{factual}, position of sentence in article plays an important role, as well as their similarity to the claim based on ROUGE scores. For type \textsc{opinion}, unlike all other types, position of sentence seems to be insignificant. As we could imagine, opinionated information might scatter around the whole documents. For sentences of \textsc{reasoning}, the ones that can be used as supporting arguments tend to be less concrete and less emotional, as opposed to opinion. 

%And both the claim and sentences tend to be more concrete. We also notice those sentences with more hedging words are more likely to be considered. For type factual, position of sentence in article plays an important role, as well as the ROUGE score. For type opinion, unlike all other types, position of sentence seems to be insignificant. As we could imagine,  opinionated information could scatter around the whole documents. For type reasoning the claims and sentences tend to be less concrete and emotional, as opposed to opinion. 



%\begin{table}
%\centering
%\small
%\begin{tabular}{|c|c|c|}
%\hline
%    Type & \textbf{MRR} & \textbf{NDCG} \\ \hline
%    \textsc{study} & 0.634 & 0.710\\ \hline 
%    \textsc{factual} & 0.545 & 0.640 \\ \hline
%    \textsc{opinion} & 0.500 & 0.605 \\ \hline
%    \textsc{reasoning} & 0.375 & 0.500 \\ \hline
%\end{tabular}
%\caption{\small Ranking results from best feature sets, breakdown into different types of arguments.}
%\label{tab:breakdown}
%\end{table}


 %For example in standalone features we don't see significant difference for number of hedging word among relevant and irrelevant sentences, but for sentences of type study, the number of hedging word, and number of named entity NUMBER and contingency discourse words are quite different among relevant and irrelevant sentences, which makes sense because it is imaginable that the language used in studies and academics tend to be conditional, less assertive and contains more numerical information. Similarly for sentences of type reasoning, the concreteness score for claim tend to be less than those for irrelevant sentences. We think this is reasonable because for more general topics like philosophy, religion, people tend to use more logical deduction and other reasoning techniques over concrete findings. For sentences of type opinion we observe the number of sentiment words plays an important role as opinions can be very emotional.  

% 4star for <1e-10, 3star for <1e-5, 2star for <1e-3, 1star for <0.05, bcorrection : 0.00077(7.7e-4)
%\vspace{-5mm}
\begin{table}[t]
{
\fontsize{8.5}{9}\selectfont
\setlength{\tabcolsep}{1.1mm}

\begin{tabular}{|p{1.5cm}|p{1.1cm}|p{1.1cm}|p{1.1cm}|p{1.3cm}|}
    \hline
    \multicolumn{1}{|c|}{Feature} & 
    \multicolumn{1}{c|}{\textsc{Study}} & 
    \multicolumn{1}{c|}{\textsc{Factual}} & 
    \multicolumn{1}{c|}{\textsc{Opinion}} & 
    \multicolumn{1}{c|}{\textsc{Reasoning}}  \\ \hline
    
    %\# PERCENT, NE & $\ast\ast\uparrow\uparrow\uparrow\uparrow$ & $\circ$ & $\circ$ & $\ast\uparrow\uparrow$  \\ \hline
    
    \# PERC, NE & $\ast\ast\uparrow\uparrow\uparrow\uparrow$ & -- & -- & --  \\ \hline
    
    %\# LOCATION, NE &  $\ast\uparrow\uparrow\uparrow\uparrow$& $\ast\ast\uparrow\uparrow$ & $\circ$ & $\ast\ast\uparrow$  \\ \hline
    
    \# LOC, NE & -- & $\ast\ast\uparrow\uparrow$ & -- & $\ast\ast\uparrow$  \\ \hline
    
    position \newline of sentence & $\ast\ast\downarrow\downarrow$ &  $\ast\ast\ast\ast$ $\downarrow\downarrow$ & -- & $\ast\ast\ast\ast$ $\downarrow\downarrow\downarrow\downarrow$ \\ \hline
    
    %concreteness \newline of sentence&  $\ast\ast\ast\uparrow\uparrow$ & $\ast\uparrow$ & $\ast\ast\uparrow\uparrow$ & $\ast\ast\ast\downarrow$  \\ \hline
    concreteness \newline of sentence&  $\ast\ast\ast$ $\uparrow\uparrow$ & -- & $\ast\ast\uparrow\uparrow$ & $\ast\ast\ast\downarrow$  \\ \hline
    
  %  \# pragmatic condition(PDTB) & -- & -- & -- & 
    
    %arousal of sentence & $\ast\ast\ast\uparrow\uparrow$ & $\ast\uparrow$ & $\ast\ast\uparrow\uparrow$ & $\ast\ast\downarrow$  \\ \hline
    
    arousal \newline of sentence & $\ast\ast\ast$ $\uparrow\uparrow$ & -- & $\ast\ast\uparrow\uparrow$ & $\ast\ast\downarrow$  \\ \hline
    
    \# hedging \newline word & $\ast\ast\uparrow\uparrow\uparrow$ & -- & -- & --  \\ \hline
    
    ROUGE & $\ast\ast\ast$$\uparrow\uparrow$ & $\ast\ast\ast\uparrow$ & $\ast\ast\uparrow\uparrow$ & --  \\ \hline
    
    %\# neutral word \newline (MPQA) & $\ast\uparrow\uparrow$& $\circ$ & $\ast\ast\uparrow\uparrow$ & $\ast\ast\ast\downarrow$\\\hline
   % \# neutral word \newline (MPQA) & -- & -- & -- & $\ast\ast\ast\downarrow$\\\hline 
    
    concreteness \newline of claim&  $\ast\ast\ast$ $\uparrow\uparrow$ & -- & $\ast\ast\uparrow\uparrow$ & $\ast\ast\ast\downarrow$  \\ \hline
    arousal \newline of claim & $\ast\ast\ast$ $\uparrow\uparrow$ & -- & $\ast\ast\uparrow\uparrow$ & $\ast\ast\ast\downarrow$  \\ \hline
    
\end{tabular}
%\vspace{-3mm}
\caption{\fontsize{10}{12}\selectfont Comparison of feature significance under composition with different types. The number of $\ast$ stands for the p-value based on $t$-test between supporting argument sentences and the others after Bonferroni correction. From one $\ast$ to four, the $p$-value scales as: 0.05, 1e-3, 1e-5, and 1e-10. When mean value of supporting argument sentences is larger, $\uparrow$ is used; otherwise, $\downarrow$ is displayed. Number of arrows represents the ratio of the larger value over smaller one. ``-" indicates no significant difference.} 
\label{tab:significance}
\vspace{-.5cm}
}
\end{table}



