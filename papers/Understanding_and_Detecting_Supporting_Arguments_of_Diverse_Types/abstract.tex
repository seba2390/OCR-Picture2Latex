\begin{abstract}
\fontsize{10}{12}\selectfont
%Constructing arguments of high quality requires gathering diverse types of information, ranging from facts to opinions. However, retrieving effective supporting information is a challenging process. 
We investigate the problem of sentence-level \textit{supporting argument detection} from relevant documents for user-specified claims. A dataset containing claims and associated citation articles is collected from online debate website \url{idebate.org}. We then manually label sentence-level supporting arguments from the documents along with their types as \textsc{study}, \textsc{factual}, \textsc{opinion}, or \textsc{reasoning}. 
%
We further characterize arguments of different types, and explore whether leveraging type information can facilitate the supporting arguments detection task. 
Experimental results show that LambdaMART~\cite{burges2010ranknet} ranker that uses features informed by argument types yields better performance than the same ranker trained without type information.
  
  %We study the problem of sentence-level supporting argument detection for user-specified claims in debatabase\footnote{www.idebate.org}, by leveraging sentence type information based on their subjectivity and credibility. As a pilot study we annotate a medium-sized dataset with type information. And we propose a feature-rich ranking framework to retrive supporting argument. Experiments show that salient features exhibit significantly different distribution among different types with real-world implications, and by incorporating the type information our ranker achieves higher performance.
\end{abstract}
