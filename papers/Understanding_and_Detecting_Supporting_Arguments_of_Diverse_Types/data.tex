We rely on data from \url{idebate.org}, where human editors construct paragraphs of arguments, either supporting or opposing claims under controversial topics. We also extract textual citation articles as source of information used by editors during argument construction. 
In total we collected 383 unique debates, out of which 200 debates are randomly selected for study. After removing  invalid ones, our final dataset includes 450 claims and 621 citation articles with about 53,000 sentences. 

\noindent \textbf{Annotation Process.} 
As shown in Figure~\ref{fig:example_intro}, we first annotate which sentence(s) from a citation articles is used by the editor as supporting arguments. Then we annotate the type for each of them as \textsc{study}, \textsc{factual}, \textsc{opinion}, or \textsc{reasoning}, based on the scheme in Table~\ref{tab:annoscheme}.\footnote{We end up with the four-type scheme as a trade-off between complexity and its coverage of the arguments.} 
For instance, the highlighted supporting argument in Figure~\ref{fig:example_intro} is labeled as \textsc{reasoning.} 


\begin{table}[t]
\fontsize{10}{12}\selectfont
\begin{tabular}{p{73mm}}
\hline
\textsc{Study:} Results and discoveries, usually quantitative, as a result of some research investment.\\% For example, results of some experiments or poll. \\ 
\textsc{Factual:} Description of some occurred events or facts, or chapters in law or declaration. \\%Usually can be obtained without research investment. For example, description of objective environment, issuance of law, historical events.\\
\textsc{Opinion:} Quotes from some person or group, either direct or indirect. It usually contains subjective, judgemental and evaluative languages, and might reflect the position or stance of some entity. \\ %For example, comments on laws and policies from officials, speculation or prediction on stock markets.\\
\textsc{Reasoning:} Logical structures. It usually can be further broken down into causal or conditional substructures. \\ %For example, to explain how oil extraction could break ecosystem by giving causal chains and their effects.\\  
\hline
\end{tabular}
\caption{\fontsize{10}{12}\selectfont Annotation scheme for our dataset. Due to space limit, we do not show detailed explanations and examples. }
\label{tab:annoscheme}
\vspace{-.4cm}
\end{table}

Two experienced annotators were hired to identify supporting arguments by reading through the whole cited article and locating the sentences that best match the reference human constructed argument. 
This task is rather complicated since human do not just repeat or directly quote the original sentences from citation articles, they also paraphrase, summarize, and generalize. 
For instance, the original sentence is ``The global counterfeit drug trade, a billion-dollar industry, is thriving in Africa", which is paraphrased to ``This is exploited by the billion dollar global counterfeit drug trade" in human constructed argument.  



The annotators were asked to annotate independently, then discuss and resolve disagreements and give feedback about current scheme. 
We compute inter-annotator agreement based on Cohen's $\kappa$ for both supporting arguments labeling and argument type annotation. For supporting arguments we have a high degree of consensus, with Cohen's $\kappa$ ranges from 0.76 to 0.83 in all rounds and 0.80 overall. 
For argument type annotation, we achieve Cohen's $\kappa$ of 0.61 for \textsc{study}, 0.75 for \textsc{factual}, 0.71 for \textsc{opinion}, and 0.29 for \textsc{reasoning}\footnote{Many times annotators have different interpretation on \textsc{reasoning}, and frequently label it as \textsc{opinion}. This results in a low agreement for \textsc{reasoning}.}

\noindent \textbf{Statistics.} 
In total 995 sentences are identified as supporting arguments. 
Among those, 95 (9.55\%) are labeled as \textsc{study}, 497 (49.95\%) as \textsc{factual}, 363 (36.48\%) as \textsc{opinion}, and 40 (4.02\%) as \textsc{reasoning}. 


We further analyze the source of the supporting arguments. Domain names of the citation articles are collected based on their URL, and then categorized into ``news", ``organization", ``scientific", ``blog", ``reference", and others, according to a taxonomy provided by Alexa\footnote{http://www.alexa.com/topsites/category} with a few edits to fit our dataset. 

News articles are the major source for all types, which account for roughly 50\% for each. We show the distribution of other four types in Figure~\ref{fig:type_domain_distribution}. 
Arguments of \textsc{study} and \textsc{reasoning} are mostly from ``scientific" websites (14.9\% and 22.9\%), whereas ``organization" websites contribute a large portion of arguments of \textsc{factual} (18.5\%) and \textsc{opinion} (16.7\%).

\begin{figure}
    \hspace{-5mm}
    \includegraphics[width=88mm]{domain_perc.pdf}
    \vspace{-7mm}
    \caption{\fontsize{10}{12}\selectfont For each supporting argument type, from left to right shows the percentage of domain names of organizations, scientific, blog, and reference. We do not display statistics for news, because news articles take the same portion in all types (about 50\%).}
    \label{fig:type_domain_distribution}
\end{figure}
