%Here we propose a feature-rich Log-linear classification model and a ranking model, together with naive baselines to study our hypothesis that type of sentence do affect its usage during debate construction. And also to provide some insights about what kind of information human debaters are looking for in the reference documents.

%Since type information is only labeled for supporting arguments but unavailable for all others. We need to automatically infer type in order to study the interaction between them.

%We propose a Log-linear classification model for this task. Since our type scheme is largely based on subjectivity and the source and form of information, features like named entity, concreteness, hedge cues and sentiment polarity would be helpful. We listed feature we used in table \ref{tab:features} in sentence level features. We use all labeled sentences and divide them into train (50\%), valid (25\%), test (25\%) sets for tuning and evaluation.

Here we characterize arguments of different types based on diverse features under the task of predicting argument types. Supporting arguments identified from previous section are utilized for experiments. We also leverage the learned classifier in this section to label the sentences that are not supporting arguments, which will be used for supporting argument detection in the next section. Four major types of features are considered.

%\vspace{1mm}
\noindent \textbf{Basic Features.} We calculate frequencies of unigram and bigram words, number of four major types of part-of-speech tags (verb, noun, adjective, and adverb), number of dependency relations, and number of seven types of named entities~\cite{chinchor1997muc}.

%\vspace{1mm}
\noindent \textbf{Sentiment Features.} We also compute number of positive, negative and neutral words in MPQA lexicon~\cite{wilson2005recognizing}, and number of words from a subset of semantic categories from General Inquirer~\cite{stone1966general}.\footnote{Categories used: Strong, Weak, Virtue, Vice, Ovrst (Overstated), Undrst (Understated), Academ (Academic), Doctrin (Doctrine), Econ\@ (Economic), Relig (Religious), Causal, Ought, and Perceiv (Perception).}

%\vspace{1mm}
\noindent \textbf{Discourse Features.} We use the number of discourse connectives from the top two levels of Penn Discourse Tree Bank~\cite{prasad2007penn}.

%\vspace{1mm}
\noindent \textbf{Style Features.} We measure word attributes for their concreteness (perceptible vs. conceptual), valence (or pleasantness), arousal (or intensity of emotion), and dominance (or degree of control) based on the lexicons collected by~\newcite{brysbaert2014concreteness} and~\newcite{warriner2013norms}.%\vspace{.1cm}

%\XY{The average concreteness score based on~\newcite{brysbaert2014concreteness}. And similarly Valence (Pleasantness, ``sunshine'' $>$ ``jail''), Arousal (Intensity of emotion, ``insanity'' $>$ ``calm''), Dominance (Degree of control, where ``completion'' $>$ ``dementia'') from \cite{warriner2013norms}}

We utilize Log-linear model for argument type prediction with one-vs-rest setup. Three baselines are considered: (1) random guess, (2) majority class, and (3) unigrams and bigrams as features for Log-linear model. 
Identified supporting arguments are used for experiments, and divided into training set (50\%), validation set (25\%) and test set (25\%). From Table~\ref{tab:typeresult}, we can see that Log-linear model trained with all features outperforms the ones trained with ngram features. 
To further characterize arguments of different types, we display sample features with significant different values in Figure~\ref{fig:pwttest}. As can be seen, arguments of \textsc{study} and \textsc{factual} tend to contain more concrete words and named entities. Arguments of \textsc{opinion} mention more person names, which implies that expert opinions are commonly quoted. 


\begin{table}[t]
    \centering
    \fontsize{10}{11}\selectfont
    \begin{tabular}{|l|c|c|}
    \hline
         & {\bf Acc} & {\bf F1}  \\
        \hline
        Majority class & 0.520 & 0.171 \\
        \hline
        Random & 0.240 & 0.199 \\
        \hline
        Log-linear (ngrams) & 0.535 & 0.277 \\
        \hline
        Log-linear (all features) & {\bf 0.622} & {\bf 0.436} \\
        \hline
    \end{tabular}
    \caption{\fontsize{10}{12}\selectfont Results for argument type prediction. One-vs-rest classifiers are learned for Log-linear models.}
    \label{tab:typeresult}
\end{table}

%Random guess has accuracy close to 1/4, which is intuitive since it is guessing one of the four types. Majority guess achieves much higher accuracy, which also makes sense since we notice that type factual is more prominent than other types in our dataset. N-gram model achieves slightly higher accuracy than majority guess, but much higher F1 score.
%Classification results are shown in Table \ref{tab:results}. A pilot study shows that  one-vs-rest outperforms multiclass classification. And by leveraging different features helps improve performance by large margin.
%

%Classification results show that Log-linear model trained with all features as one-vs-rest classifier has accuracy of 0.622, outperforming a Log-linear model trained with ngram features (unigrams and bigrams) with accuracy of 0.535. 
%To further characterize arguments of different types, we display sample features with significant different values in Figure~\ref{fig:pwttest}. 
%we conduct pair wise t-test on all features and listed major ones in figure \ref{fig:pwttest}. 
%
%As can be seen, arguments of \textsc{study} and \textsc{factual} tend to contain more concrete words and named entities. Arguments of \textsc{opinion} mention more person names, which implies that expert opinions are commonly quoted. 
%opinions usually precedes experts or authorities. %We suspect that human tend to synthesize results from multiple studies in one arguments, but only one opinion or reasoning at once. 
%And type reasoning has more words used in causal and conditional structures, which accords with our expectation.

\begin{figure}[t]
  \hspace{-4mm}
    \includegraphics[width=83mm]{feat.pdf}
  \vspace{-5mm}
  \caption{\fontsize{10}{12}\selectfont Average features values for different argument types. Numbers in \textbf{boldface} are significantly higher than the others based on paired $t$-test ($p<0.05$).}
%  Boldfaced numbers represent a ``group'' such that within the group there is no significant differences, throughout different groups there are significant differences based paired $t$-test ($p<0.05$).}
  \label{fig:pwttest}

\end{figure}


%Now that we could detect the type of sentences, we may add this power to help infer which sentences could be used to construct arguments to support the claim. We name this process relevance inference, in the sense that our goldstandard data is annotated based on the relevance between sentences in the cited articles versus the sentences the author used with reference to that cited article. We first present a simple unsupervised baseline method, then discuss several ranking models and features we designed for them.



