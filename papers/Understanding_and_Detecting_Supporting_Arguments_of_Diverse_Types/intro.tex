% if we want to understand how human construct arguments, as a first step, we need to know what kind of information they seek for to support their claim and how to automatically detect the supporting arguments

%Constructing arguments of high quality would require (1) extracting diverse supporting information from relevant documents, and (2) organizing it into a coherent structure with proper substantiation and reasoning. 


%study: 108, 9.76% factual: 575, 51.94% opinion: 382, 34.51% reasoning: 42, 3.79%

Argumentation plays a crucial role in persuasion and decision-making processes. An argument usually consists of a central claim (or conclusion) and several supporting premises. Constructing arguments of high quality would require the inclusion of diverse information, such as factual evidence and solid reasoning~\cite{rieke1997argumentation,park2014identifying}. 
%
For instance, as shown in Figure~\ref{fig:intro}, the editor on \url{idebate.org} -- a Wikipedia-style website for gathering pro and con arguments on controversial issues, utilizes arguments based on study, factual evidence, and expert opinion to support the anti-gun claim ``legally owned guns are frequently stolen and used by criminals". 
%
However, it would require substantial human effort to collect information from diverse resources to support argument construction. In order to facilitate this process, there is a pressing need for tools that can automatically detect supporting arguments. 

%\XY{For instance, the editor on \url{idebate.org} -- a Wikipedia-style website for gathering pro and con arguments on controversial issues, utilizes arguments based on study, factual evidence, and expert opinion to support the anti-gun claim ``legally owned guns are frequently stolen and used by criminals":}

%\vspace{-2mm}
\begin{figure}[h]
%\captionsetup{font=small}
	\fontsize{10}{12}\selectfont
    \setlength{\tabcolsep}{0.8mm}
    
%    \hspace{3mm}
	\begin{tabular}{p{74mm}}

        - A June 2013 IOM report states that ``almost all guns used in criminal acts enter circulation via initial legal transaction''. [study] \\
        - Between 2005 and 2010, 1.4 million guns were stolen from US homes during property crimes (including bulglary and car theft), a yearly average of 232,400. [factual] \\
        - Ian Ayres, JD, PhD, \ldots states, ``with guns being a product that can be easily carried away and quickly sold at a relatively high fraction of the initial cost, the presence of more guns can actually serve as a stimulus to burglary and theft.'' [expert opinion]\\

	\end{tabular}
	\caption{\fontsize{10}{12}\selectfont Three different types of arguments used to support the claim ``Legally owned guns are frequently stolen and used by criminals". }
\label{fig:intro}
\end{figure}

%\XY{As we can tell, it would require substantial human effort to collect information from diverse resources to support argument construction. In order to facilitate this process, there is a pressing need for tools that can automatically detect supporting arguments.} 
%
To date, most of the argument mining research focuses on recognizing argumentative components and their structures from constructed arguments based on curated corpus~\cite{mochales2011argumentation,stab2014identifying,feng2011classifying,habernal-gurevych:2015:EMNLP,nguyen-litman:2016:P16-1}. Limited work has been done for retrieving supporting arguments from external resources. Initial effort by~\newcite{rinott2015show} investigates the detection of relevant factual evidence from Wikipedia articles. However, it is unclear whether their method can perform well on documents of different genres (e.g. news articles vs. blogs) for detecting distinct types of supporting information. 


\begin{figure}[t]
%\captionsetup{font=small}
	\fontsize{10}{12}\selectfont
	%\small
	\setlength{\tabcolsep}{0.8mm}
	\begin{tabular}{|p{75mm}|}
	\hline
	- \textbf{Topic}: This house would ban cosmetic surgery \\
	- \textbf{Claim}: An outright ban would be easier than the partial bans that have been enacted in some places. \\
	- \textbf{Human Constructed Argument}: \color{blue}{$\ldots$This potentially leaves difficulty drawing the line for what is allowed.[1]} $\ldots$ \\ 		\hline
	\end{tabular}

	%\vspace{-3mm}
	%\hspace{-2mm}
	\begin{tabular}{|p{75mm}|}
	\hline
	\underline{\textbf{Citation Article}}\\
	
	[1]: {\it ``Australian State Ban Cosmetic Surgery for Teens"}\\
	
	%\ldots\\
	- \ldots It is unfortunate that a parent would consider letting a 16-year-old daughter have a breast augmentation."\vspace{.1cm}
	
	{\color{blue}\textit{- But others worry that similar legislation, if it ever comes to pass in the United States, would draw a largely arbitrary line -- and could needlessly restrict some teens from procedures that would help their self-esteem.}}\vspace{.1cm}
	
	- Dr. Malcolm Z. Roth, director of plastic surgery at Maimondes Medical Center in Brooklyn, N.Y. , said he believes that some teens are intelligent and mature enough to comprehend the risks and benefits of cosmetic surgery.\ldots\\
	
	
	\hline
	\end{tabular}
	%\vspace{-3mm}
	\caption{\fontsize{10}{12}\selectfont
	A typical debate motion consists of a Topic, Claims, and Human Constructed Arguments. Citation article is marked at the end of sentence. Our goal is to find out supporting argument (in \textit{italics}) from citation article that can back up the given claim.}
	%based on the Human Constructed Argument citing this article. }
	%An example of annotation. Topic, claim, and argument are taken from \url{idebate.org}. Annotators first check the topic, claim and argument constructed by human editors, they then need to find the sentence(s) in cited article that editors used for argument}
\label{fig:example_intro}
\end{figure}

%In this work, we present a new corpus, which support our study to detect arguments of different types from 
In this work, \textit{we present a novel study on the task of sentence-level supporting argument detection from relevant documents for a user-specified claim}. Take Figure~\ref{fig:example_intro} as an example: assume we are given a claim on the topic of ``banning cosmetic surgery" and a relevant article (cited for argument construction), we aim to automatically pinpoint the sentence(s) (in \textit{italics}) among all sentences in the cited article that can be used to back up the claim. We define such tasks as \textit{supporting argument detection}. 
%
Furthermore, another goal of this work is to understand and characterize different types of supporting arguments. Indeed, human editors do use different types of information to promote persuasiveness as we will show in Section~\ref{sec:data}. Prediction performance also varies among different types of supporting arguments. 
%\XY{And we also observe there are intrinsic differences in terms of the difficulties to analyze and detect different types of arguments.}
%\XY{As we expect human editors employ different types of arguments as strategies to promote persuasiveness and coherence.} \LW[add more explanation?]


% data to collect, annotation, some statistics
%Given that none of the existing datasets is suitable for our study, we first collect a corpus from Idebate, which contains hundreds of contentious debate topics and corresponding claims from both pro and con sides. As is shown in Figure~\ref{fig:example_intro}, each claim is supported with a paragraph of human constructed argument, with cited articles marked on sentence level. We collect all topics and claims along with associated citation articles, and recruit annotators to manually identify the supporting arguments. 
Given that none of the existing datasets is suitable for our study, we collect and annotate a corpus from Idebate, which contains hundreds of debate topics and corresponding claims.\footnote{The labeled dataset along with the annotation guideline will be released at \url{xyhua.me}.} 
%
As is shown in Figure~\ref{fig:example_intro}, each claim is supported with some human constructed argument, with cited articles marked on sentence level. 
%
After careful inspection on the supporting arguments, we propose to label them as \textsc{study}, \textsc{factual}, \textsc{opinion}, or \textsc{reasoning}. Substantial inter-annotator agreement rate is achieved for both supporting argument labeling (with Cohen's $\kappa$ of 0.8) and argument type annotation, on 200 topics with 621 reference articles.
%\XY{We achieve satisfactory inter-annotator agreement score for both supporting argument labeling (0.80) and type annotation (0.75).
%We achieve an inter-annotator agreement of 0.80 on supporting argument labeling, and as high as 0.75 for type annotation, which are reasonably good considering the complexity of the task. Annotators also discuss afterwards to resolve the disagreement.\footnote{Our dataset and annotations will be made publicly available upon publication.}
%We collected labels for a subset of these revisions: label 200 topics and 621 reference articles, with 1107 labeled sentences with meticulous disagreement resolution. our inter rater agreement score shows steady increase after each round and finally ends up at 0.78, which is reasonably good considering the complexity of the task.}


% news articles is a major source, many study and reasoning from scientific, opinions and reasoning from blogs
% propose features to characterize different types of supporting argument
% predict on types of candidate sentences, and see if add type information will help
Based on the new corpus, we first carry out a study on characterizing arguments of different types via type prediction. We find that arguments of \textsc{study} and \textsc{factual} tend to use more concrete words, while arguments of \textsc{opinion} contain more named entities of person names. 
%
We then investigate whether argument type can be leveraged to assist supporting argument detection. Experimental results based on LambdaMART~\cite{burges2010ranknet} show that utilizing features composite with argument types achieves a Mean Reciprocal Rank (MRR) score of 57.65, which outperforms an unsupervised baseline and the same ranker trained without type information. Feature analysis also demonstrates that salient features have significantly different distribution over different argument types. 
%\XY{[which outperforms non-trivial baseline based on word embedding similarity (47.65) and the same ranker trained without type information (56.43).]}
%\XY{which outperforms an unsupervised baseline using word embedding similarity (47.65), and the same ranker trained without compositing type information. (56.43)}
%Feature analysis shows that \LW{add} \XY{salient features have significantly different distribution over different types of arguments. }
%
%This also points out several promising directions for future work. \LW{add info} 


For the rest of the paper, we summarize related work in Section~\ref{sec:related}. The data collection and annotation process is described in Section~\ref{sec:data}, which is followed by argument type study (Section~\ref{sec:type}). Experiment on supporting argument detection is presented in Section~\ref{sec:relevance}. We finally conclude in Section~\ref{sec:conclusion}.


%Our work has the following main contributions. First We study how human debaters seek and combine information from reference documents on sentence level, for which we build a typing scheme to categorize arguments to reflect the subjectivity information and provide some coarse-grained rationales. By using this scheme we annotated a real-world debate archive on sentence level, and analyse important features based on results from a state-of-the-art ranking algorithm on our dataset. The feature analysis shows real world implications about what people are looking for under different conditions, and also points out several promising directions we could pursue in the future.


%Recently, there is a growing interest in the rising area of argument mining~\cite{palau2009argumentation,mochales2011argumentation,stab2014identifying}. Previous work mostly focuses on automatically recognizing argumentative components and their structure from constructed arguments based on curated corpus~\cite{feng2011classifying,nguyen-litman:2016:P16-1,habernal-gurevych:2015:EMNLP}. 

%Studies about how human construct logical and persuasive arguments have inspired numerous theories and practical methodologies.{\XY{Need cites}} But it is still far from clear how our brain search and combine relevant information. With more powerful tools and well curated datasets proposed in argument mining community recently, we now could examine the problem from different perspectives. Previous works about argument detection mainly focus on the detection of evidence given contextual information. But we observe that in reality people tend to combine subjective opinions and logical reasoning together with evidence. As far as we know, there is no specific work on the detection of arguments in a ``type aware'' way, i.e. utilizing objective evidence and subjective arguments.  

%The first argument cites some high-level conclusion from a report, the second one provides detailed numbers and time frame, the third one shows analysis from experts in this field. This kind of combination and ordering is not surprising as many people adopt this paradigm for composition. The question is how could we quantitatively depict these interactions among different types of information.


%Argumentation mining aims to explain the relations between debating and reasoning structures. Previous works in this field mainly focus on the internal structures within a document. Including paragraph level \cite{teufel2000argumentative}, to sentence level and to intra-sentence level. 

%However, we observe that in a more realistic debate setting people tend to seek extra relevant documents as reference to construct supporting arguments for certain claims. Such documents usually contain specific information whose idea could be in different forms and be combined in a logical and persuasive way. For example, for topic ``Gun control'', and claim ``Legally owned guns are frequently stolen and used by criminals''. The author cited three articles for three different arguments:
%\begin{enumerate}
%    \item A June 2013 IOM report states that ``[a]lmost all guns used in criminal acts enter circulation via initial legal transaction''
%    \item Between 2005 and 2010, 1.4 million guns were stolen from US homes during property crimes(including bulglary and car theft), a yearly average of 232,400.
%    \item Ian Ayres, JD, PhD and \ldots state,``with guns being a product that can be easily carried away and quickly sold at a relatively high fraction of the initial cost, the presence of more guns can actually serve as a stimulus to burglary and theft.''
%\end{enumerate}
%Each of these three arguments represents different aspects of the reason for the claim. And the nature of those information can be categorized in different forms (first one is from study, second one is factual statistics, third one is reasoning by some experts), from which we can measure the subjectivity and credentiality of the argument. Such type information might also be helpful during argument sentence selection process in that certain types of arguments can be more suitable for certain types of claims.


%In this work we examine what types of arguments people usually use in debates, and how they are used.
%Since there is no available dataset for this kind of study. We need to build our own. And we hope to be able to trace to the source of information debaters use, so that we know what information are picked and what are left. We find several online debate website suitable, especially Idebate\footnote{www.idebate.org}, which has abundant topics and claims with human generated arguments and sentence-level links pointing to the data source used.  
%
%We then carry out the annotation under a taxonomy designed to cover most instances and has least overlaps. Each annotator is asked to read through the reference articles, looking for the most relevant information debater used for argument. And label the type for that information for later study. As a result we managed to label 200 topics and 621 reference articles, with 1107 labeled sentences after disagreement resolution. (Though there are inevitably some disagreements, after detailed discussion and careful revision on labeling methodology, our inter rater agreement score shows steady increase after each round and finally ends up at 0.78, which is reasonably good considering the complexity of the task)    
%
%
%From this dataset we propose several features potentially useful in distinguishing different types of arguments and the likelihood a sentence could be used to support a claim. After conducting feature analysis we do find some salient features such as the number of hedging words, the number of named entities and the degree of concreteness for claim and topic, with different level of importance for different types of arguments.  
%
%Our work has the following main contributions. First We study how human debaters seek and combine information from reference documents on sentence level, for which we build a typing scheme to categorize arguments to reflect the subjectivity information and provide some coarse-grained rationales. By using this scheme we annotated a real-world debate archive on sentence level, and analyse important features based on results from a state-of-the-art ranking algorithm on our dataset. The feature analysis shows real world implications about what people are looking for under different conditions, and also points out several promising directions we could pursue in the future.
%

%As a preliminary experiment we compute the concreteness score\cite{brysbaert2014concreteness} for claims clustered by dominant type of arguments. Results show that ``study'' dominated claims have highest average concreteness score, followed by ``factual'', ``opinion'' and ``reasoning'', which reflects our intuition that there is an association between objectivity of arguments with that of claims. Namely, claims dominated by objective arguments such as those of type ``study'' and ``factual'', are more likely to be concrete compared with subjective arguments dominated ones.

%In order to further examine our assumptions, we also proposed a Log-linear classification (SVM ranking ) model for this dataset. The goal of our system are 1) predict one of four types given a sentence. 2) predict the relevance of a sentence given a topic and a claim. For 1) we try to distinguish different types by features like concreteness, named entity, valence, arousal, dominance, and other conventional lexical features. For 2) we include type information based on results from 1), and also using textual entailment score from claim to candidates to capture those generalization cases. Our type prediction model achieves accuracy of 0.618 and f1 score of 0.455 compared with a simple baseline method of 0.520 and 0.171. Our relevance prediction model achieves accuracy of xxx and f1 score of xxx compared with an unsupervised Word2vec baseline.

%Our work has the following main contributions. First We study how human debaters seek and combine information from reference documents on sentence level, for which we build a typing scheme to categorize arguments to reflect the subjectivity information and provide some coarse-grained rationales. By using this scheme we annotated a real-world debate archive on sentence level, and analyse important features based on results from a state-of-the-art ranking algorithm on our dataset. The feature analysis shows real world implications about what people are looking for under different conditions, and also points out several promising directions we could pursue in the future.
