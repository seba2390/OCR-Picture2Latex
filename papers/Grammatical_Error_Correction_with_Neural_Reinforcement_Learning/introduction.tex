\section{Introduction}
Research in automated Grammatical Error Correction (\gec) has expanded  from token-level, closed class corrections (e.g., determiners, prepositions, verb forms) to phrase-level, open class issues that consider fluency (e.g., content word choice, idiomatic collocation, word order, etc.).

The expanded goals of GEC have led to new proposed models deriving from techniques in data-driven machine translation, including phrase-based MT (PBMT) \cite{felice-EtAl:2014:W14-17,chollampatt-hoang-ng:2016:EMNLP2016,junczysdowmunt-grundkiewicz:2016:EMNLP2016} and neural encoder-decoder models (\encdec) \cite{yuan-briscoe:2016:N16-1}.
\newcite{napoles-sakaguchi-tetreault:2017:EACLshort} recently showed that a neural \encdec can outperform PBMT on a fluency-oriented GEC data and metric.

We investigate training methodologies in the neural \encdec for GEC.
To train the neural \encdec models, maximum likelihood estimation (\mle) has been used, where the objective is to maximize the (log) likelihood of the parameters for a given training data.

\newlength{\textfloatsepsave}
\setlength{\textfloatsepsave}{\textfloatsep} \setlength{\textfloatsep}{0pt}
\input alg_nrl
As \newcite{2015arXiv151106732R} indicates, however, \mle has drawbacks.
The \mle objective is based on {\em word-level} accuracy against the reference, and the model is not exposed to the predicted output during training (exposure bias).
This becomes problematic, because once the model fails to predict a correct word, it falls off the right track and does not come back to it easily.

To address the issues, we employ a neural \encdec GEC model with a reinforcement learning approach in which we directly optimize the model toward our final objective (i.e., evaluation metric).
The objective of the neural reinforcement learning model (\proposed) is to maximize the expected reward on the training data.
%In our case, the reward is (sentence-level) GLEU score, which captures fluency as well as error correction accuracy by comparing against reference.
The model updates the parameters through back-propagation according to the reward from predicted outputs. 
The high-level description of the training procedure is shown in Algorithm \ref{alg:nrl},
and more details are explained in \S\ref{sec:model}.
%We explain more details in \S\ref{sec:model}, mentioning \proposed as generalization of minimum risk training (\mrt) \cite{shen-EtAl:2016:P16-1}.
To our knowledge, this is the first attempt to employ reinforcement learning for directly optimizing the \encdec model for \gec task.

We run GEC experiments on a fluency-oriented GEC corpus (\S\ref{sec:experiment}), demonstrating that \proposed outperforms the \mle baseline both in human and automated evaluation metrics.
