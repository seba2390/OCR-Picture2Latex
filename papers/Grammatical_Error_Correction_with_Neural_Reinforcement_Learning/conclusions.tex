\section{Conclusions}
\label{sec:conclusions}
\vspace{-2mm}
We have presented a neural \encdec model with reinforcement learning for GEC. 
To alleviate the \mle issues (exposure bias and token-level optimization), \proposed learns the policy (model parameters) by directly optimizing toward the final objective by treating the final objective as the reward for the \encdec agent.
Using a GEC-specific metric, \metric, we have demonstrated that \proposed outperforms the \mle baseline on the fluency-oriented GEC corpus both in human and automated evaluation metrics.
As a supplement, we have explained the relevance between minimum risk training (\mrt) and \proposed, claiming that \mrt is a special case of \proposed.