\section{Model and Optimization}
\vspace{-2mm}
\label{sec:model}
\setlength{\textfloatsep}{\textfloatsepsave}
We use the {\em attentional neural \encdec} model \cite{2014arXiv1409.0473B} as a basis for both \proposed and \mle.
The model takes (possibly ungrammatical) source sentences $x \in X$ as an input, and predicts grammatical and fluent output sentences $y \in Y$ according to the model parameter $\theta$.
The model consists of two sub-modules, {\em encoder} and {\em decoder}. 
The encoder transforms $x$ into a sequence of vector representations (hidden states) using a bidirectional gated recurrent neural network (GRU) \cite{2014arXiv1412.3555C}.
The decoder predicts a word $y_t$ at a time, using previous token $y_{t-1}$ and linear combination of encoder information as attention.
%With the encoder representation, the decoder predicts a word $y_t$ with previous information at each time step.
%% MJP: I think you could just delete the rest of the subsection (the equation and next two sentences)
%\todo{definition of $\theta$}
%Thus, the entire \encdec model is formalized as follows:
%\begin{eqnarray}
%p(y|x;\theta) = \sum_{t=1}^{T}p(y_{t}|x, y_1^{t-1};\theta)
%\end{eqnarray}
%
%For more technical details such as attention mechanism, refer to \newcite{2014arXiv1409.0473B}.
%\todo{or should I describe it in the Appendix?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1mm}
\subsection{Maximum Likelihood Estimation}
\vspace{-1mm}
\label{sec:mle}
Maximum Likelihood Estimation training (\mle) is a standard optimization method for \encdec models.
In \mle, the objective is to maximize the log likelihood of the correct sequence for a given sequence for the entire training data.
\begin{eqnarray}
L(\theta) = \sum_{\langle X,Y \rangle} \sum_{t=1}^{T}\log p(y_{t}|x, y_1^{t-1};\theta)
\end{eqnarray}
The gradient of $L(\theta)$ is as follows:
\begin{eqnarray}
\pderivl = \sum_{\langle X,Y \rangle} \sum_{t=1}^{T} \frac{\nabla p(y_t|x, y_1^{t-1};\theta)}{p(y_t|x,y_1^{t-1};\theta)}
\end{eqnarray}

One drawback of \mle is the {\em exposure bias} \cite{2015arXiv151106732R}. 
The decoder predicts a word conditioned on the correct word sequence ($y_{1}^{t-1}$) during training, whereas it does with the predicted word sequence ($\hat{y}_{1}^{t-1}$) at test time. 
Namely, the model is not exposed to the predicted words in training time.
This is problematic, because once the model fails to predict a correct word at test time, it falls off the right track and does not come back to it easily.
%% MJP: this should state that it not exposed to the predicted words at training time, right?
%% KS: Correct. I added the sentence explaining it to make sure.
Furthermore, in most sentence generation tasks, the \mle objective does not necessarily correlate with our final evaluation metrics, such as BLEU \cite{papineni-EtAl:2002:ACL} in machine translation and ROUGE \cite{lin:2004:ACLsummarization} in summarization.
This is because \mle optimizes word level predictions at each time step instead of evaluating sentences as a whole.

\gec is no exception.
It depends on sentence-level evaluation that considers grammaticality and fluency.
For this purpose, it is natural to use \metric \cite{napoles-EtAl:2015:ACL-IJCNLP}, which has been used as a fluency-oriented GEC metric. 
We explain more details of this metric in \S\ref{sec:gleu}.

%To address these issues, we directly optimize the neural \encdec model toward our final evaluation metric for GEC using reinforcement learning.
%%\todo{MJP: what metric? GLEU? You should say right here.}
%For this purpose, it is natural to use GLEU \cite{napoles-EtAl:2015:ACL-IJCNLP}, which a fluency-oriented metric. 
%We explain the details of the metric in \S\ref{sec:gleu}.
%MJP: I think presenting GLEU as *the* GEC metric is misleading --- GLEU is one metric, but it isn't the default the way BLEU is for MT

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-1mm}
\subsection{Neural Reinforcement Learning}
\vspace{-1mm}
\setlength{\abovedisplayskip}{4.0pt} % top margin
\setlength{\belowdisplayskip}{4.0pt} % bottom margin
To address the issues in \mle, we directly optimize the neural \encdec model toward our final objective for GEC using reinforcement learning.
In reinforcement learning, {\em agents} aim to maximize expected {\em rewards} by taking {\em actions} and updating the {\em policy} under a given {\em state}.
In the neural \encdec model, we treat the \encdec as an agent which predicts a word from a fixed vocabulary at each time step (the action), given the hidden states of the neural \encdec representation.
The key difference from \mle is that the reward is not restricted to token-level accuracy. 
Namely, any arbitrary metric is applicable as the reward.\footnote{The reward is given at the end of the decoder output (i.e., delayed reward).}
%As mentioned, we use \metric as the reward to maximize. 
%For GEC, it is natural to use GLEU \cite{napoles-EtAl:2015:ACL-IJCNLP}, which a fluency-oriented metric. 
%We explain the details of the metric in \S\ref{sec:gleu}.
%\footnote{In theory, it is possible to use document-level reward, but we focus sentence-level reward in the paper for the sake of simplicity.} 
%The score is given by the evaluation metric of the task in general.
%(e.g., GLEU score for \gec, BLEU score for Machine Translation).

Since we use \metric as the final evaluation metric, the objective of \proposed is to maximize the expected \metric by learning the model parameter. 
%Formally, the objective in \proposed is defined as follows.
\begin{align}
\label{eq:j}
J(\theta) &= \mathbb{E}[\nrlreward] \nonumber \\
    &= \sum_{\sampledata} \mrtp \nrlreward
\end{align}
where $S(x)$ is a sampling function that produces $k$ samples $\hat{y}_1, ... \hat{y}_k$, $\mrtp$ is a probability of the output sentence, and $\nrlreward$ is the reward for $\hat{y}_k$ given a reference set $y$.
As described in Algorithm \ref{alg:nrl}, given a pair of source sentence and the reference $(x, y)$, \proposed takes $k$ sample outputs ($\hat{y}_1$, ... $\hat{y}_k$) and their probabilities ($p(\hat{y}_1)$, ... $p(\hat{y}_k)$).
Then, the expected reward is computed by multiplying the probability and metric score for each sample $\hat{y}_i$.

In the \encdec, the parameters $\theta$ are updated through back-propagation and the number of parameter updates is determined by the partial derivative of $J(\theta)$, called the {\em policy gradient} \cite{williams1992simple,sutton1999policy} in reinforcement learning:
\begin{align}
\label{eq:partialj}
\pderivj = \alpha \mathbb{E} \left[\nabla \log \mrtpsimple \{\nrlreward - b \}  \right]
\end{align}
where $\alpha$ is a learning rate and $b$ is an arbitrary baseline reward to reduce the variance.
The sample mean reward is often used for $b$ \cite{williams1992simple}, and we follow it in \proposed.
%of gradients.

It is reasonable to compare \proposed to minimum risk training (\mrt) \cite{shen-EtAl:2016:P16-1}.
In fact, \proposed with a {\em negative expected reward} can be regarded as \mrt.
%with mini-batch size being 1,
The gradient of \mrt objective is a special case of {\em policy gradient} in \proposed.
We show mathematical details about the relevance between \proposed and \mrt in the supplemental material.
%\ref{sec:appendix}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{-1mm}
\subsection{Reward in Grammatical Error Correction}
\label{sec:gleu}
%In \proposed for the \gec task, we need an appropriate reward function $\nrlreward$.
%% MJP: what is "the fluency-oriented GEC metric"? I suggest deleting this sentence and merging the following paragraph with this one
To capture fluency as well as grammaticality in evaluation on such references, we use \metric as the reward.
\metric has been shown to be more strongly preferred than other GEC metrics by native speakers \cite{TACL800}. 
Similar to BLEU in machine translation, \metric computes $n$-gram precision between the system hypothesis ($H$) and the reference ($R$).
In \metric, however, $n$-grams in source ($S$) are also considered. The precision is penalized when the $n$-gram in $H$ overlaps with the source and not with the reference.
Formally, 
\begin{align}
\text{\metric} &= \text{BP}\cdot \exp \left( \sum_{n=1}^4 \frac{1}{n} \log p'_n \right) \nonumber \\
p'_n &= \frac{ N(H,R) - \left[ N(H,S)-N(H,S,R) \right] }{N(H)} \nonumber
\end{align}
where  $N(A,B,C,...)$ is the number of overlapped $n$-grams among the sets, and BP is the same {\em brevity penalty} as in BLEU.