%\title{emnlp 2017 instructions}
% File emnlp2017.tex
%

%\documentclass[11pt,letterpaper,fleqn]{article}
\documentclass[11pt,letterpaper]{article}
\usepackage{emnlp2017}
\usepackage{times}
\usepackage{latexsym}

%% additional packages
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{array}
\usepackage{graphicx}
\usepackage{cases}
\usepackage{watermark}
\usepackage{pbox}
\usepackage{multirow}
\usepackage{soul}
\setulcolor{red}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{makecell}
\usepackage[draft]{todonotes} 
%\usepackage[disable]{todonotes} 
\presetkeys{todonotes}{fancyline, color=pink, bordercolor=red}{}
\usepackage[ruled,noline,linesnumbered]{algorithm2e}
%\usepackage{fixltx2e}
\usepackage[small]{caption}
%\usepackage{multicol}
\usepackage{lipsum}% for dummy text
\usepackage{mathtools}
\usepackage{cuted}
%\usepackage{float}



%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Additional commands %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Select what to do with command \comment:
\usepackage{xcolor}
\newcommand{\comment}[1]{{\bfseries \color{blue} [#1] }} %comment shown
%    \newcommand{\comment}[1]{}  %comment hidden 

%%%%%%%%%%%%%
%%% Macro %%%
%%%%%%%%%%%%%
\newcommand{\argmax}{\mathop{\rm argmax}\limits}
\newcommand{\argmin}{\mathop{\rm argmin}\limits}
\newcommand{\metric}{GLEU\xspace}
\newcommand{\proposed}{NRL\xspace}
\newcommand{\proposedfull}{neural reinforcement learning\xspace}
\newcommand{\encdec}{enc-dec\xspace}
\newcommand{\mrtfull}{minimum risk training\xspace}
\newcommand{\mrt}{MRT\xspace}
\newcommand{\mlefull}{maximum likelihood training\xspace}
\newcommand{\mle}{MLE\xspace}
\newcommand{\gec}{GEC\xspace}
\newcommand{\cambhybrid}{CAMB14\xspace}
\newcommand{\cambnmt}{CAMB16\xspace}
\newcommand{\nus}{NUS\xspace}
\newcommand{\amu}{AMU\xspace}

\newcommand{\trainingdata}{(X,Y)}
\newcommand{\mrtdelta}{\Delta(\hat{y},y)}
\newcommand{\nrlreward}{r(\hat{y},y)}
\newcommand{\sampledata}{\hat{y} \in S(x)}
\newcommand{\sampleprimedata}{\hat{y}' \in S(x)}
\newcommand{\sampledataprime}{\hat{y}' \in S(x)}
\newcommand{\mrtp}{p(\hat{y}| x;\theta)}
\newcommand{\mrtpsimple}{p(\hat{y})}
\newcommand{\mrtpprime}{p(\hat{y}'| x;\theta)}
\newcommand{\mrtpprimesimple}{p(\hat{y}')}
\newcommand{\mrtpsum}{\sum_{\sampleprimedata} \mrtpprime}
\newcommand{\mrtq}{q(\hat{y}|x;\theta,\alpha)}
\newcommand{\mrtqsimple}{q(\hat{y})}
\newcommand{\softmax}{\frac{\mrtp^\alpha}{\mrtpsum^\alpha}}

\newcommand{\mrtpsumsimple}{\sum_{\hat{y}'} \mrtpprimesimple}
\newcommand{\softmaxnumerator}{\mrtpsimple^\alpha}
\newcommand{\softmaxdenominator}{\mrtpsumsimple^\alpha}

\newcommand{\pderivrtilde}{\frac {\partial \tilde{R}(\theta)}{\partial \theta}}
\newcommand{\pderivl}{\frac {\partial L(\theta)}{\partial \theta}}
\newcommand{\pderivr}{\frac {\partial R(\theta)}{\partial \theta}}
\newcommand{\pderivj}{\frac {\partial J(\theta)}{\partial \theta}}
\newcommand{\pderivtheta}{\frac {\partial}{\partial \theta}}
\newcommand{\pderivp}{\frac {\partial}{\partial \mrtpsimple}}
%\newcommand{\gradp}{\frac{\partial \mrtpsimple}{\partial \theta}}

%\newcommand{\softmax}{q( \hat {y}| x,\theta,\alpha) = \frac{p(\hat{y}| x;\theta)^{\alpha}} {\sum_{\hat{y}'\in S(x)} p(\hat{y}'| x;\theta)^{\alpha}}}
%\newcommand{\softmaxdenominator}{\sum_{\hat{y}'\in S(x)} p(\hat{y}'| x;\theta)^{\alpha}}
%\newcommand{\softmaxnumerator}{p(\hat{y}| x;\theta)^{\alpha}}
%\newcommand{\mrtgradpfull}{\frac{\partial p(\hat{y})}{\partial \theta }}
%\newcommand{\mrtgradp}{\grad p(\hat{y})}

% Uncomment this line for the final submission:
\emnlpfinalcopy

%  Enter the EMNLP Paper ID here:
\def\emnlppaperid{***}

% To expand the titlebox for more authors, uncomment
% below and set accordingly.
% \addtolength\titlebox{.5in}    

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand{\clsp}{\ensuremath{{}^\dagger}}
\newcommand{\hltcoe}{\ensuremath{{}^\ddagger}}

%% Title
\title{Grammatical Error Correction with Neural Reinforcement Learning}

%\title{Toward Better Grammatical Error Correction with Deep Neural Network}

%\title{Grammatical Error Correction with Empirical Risk Minimization}

% Maximum Likelihood --> Maximum Evaluation Metric
% actually equivalent to MRT

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
% If the title and author information does not fit in the area allocated,
% place \setlength\titlebox{<new height>} right after
% at the top, where <new height> can be something larger than 2.25in

%\author{}

\author{
  Keisuke Sakaguchi \clsp
  \and Matt Post\hltcoe
  \and Benjamin Van Durme\clsp \hltcoe \\
 \clsp Center for Language and Speech Processing, Johns Hopkins University \\
 \hltcoe Human Language Technology Center of Excellence, Johns Hopkins University \\
 {\tt \{keisuke,post,vandurme\}@cs.jhu.edu}
}
\date{}

\begin{document}
\setlength{\abovedisplayskip}{3.0pt} % 上部のマージン
\setlength{\belowdisplayskip}{3.0pt} % 下部のマージン

\maketitle

%%% Abstract %%%
\input abstract

%%% Introduction %%%
\input introduction

%%% Min Risk Training for GEC, noise model?? %%%
\input models

%%% Experiment %%%
\input experiment

%%% Related %%%
%\input src/related

%%% Conclusion %%%
\input conclusions

\bibliography{emnlp2017}
\bibliographystyle{emnlp_natbib}

\newpage
\appendix
\input appendix

\end{document}
