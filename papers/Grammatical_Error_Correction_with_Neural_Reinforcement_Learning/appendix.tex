\section{Minimum Risk Training and Policy Gradient in Reinforcement Learning}
\setlength{\abovedisplayskip}{3.0pt} % top margin
\setlength{\belowdisplayskip}{3.0pt} % bottom margin
\label{sec:appendix}
We explain the relevance between minimum risk training (MRT) \cite{shen-EtAl:2016:P16-1} and neural reinforcement learning (\proposed) for training neural \encdec models.
We describe the detailed derivation of gradient in \mrt, and show that \mrt is a special case of \proposed.

As introduced in \S \ref{sec:model}, the model takes ungrammatical source sentences $x \in X$ as an input, and predicts grammatical and fluent output sentences $y \in Y$.
The objective function in \proposed and \mrt are written as follows.
\begin{align}
\label{eq:j}
J(\theta)= \mathbb{E}[\nrlreward]
\end{align}
\begin{align}
\label{eq:partialr}
R(\theta) =\sum_{\trainingdata} \mathbb{E}[\mrtdelta]
\end{align}
where $\nrlreward$ is the {\em reward} and $\mrtdelta$ is the {\em risk} for an output ($\hat{y}$).

To the sake of simplicity, we consider expected loss in \mrt for a single training pair:
\begin{align}
\tilde{R}(\theta) &= \mathbb{E}[\mrtdelta] \nonumber \\
    &= \sum_{\sampledata} \mrtq \mrtdelta
\end{align}
where
\begin{align}
\label{eq:softmax}
\mrtq = \softmax
\end{align}
$S(x)$ is a sampling function that produces $k$ samples $\hat{y}_1, ... \hat{y}_k$, and $\alpha$ is a smoothing parameter for the samples \cite{och:2003:ACL}.
Although the direction to optimize (i.e., minimizing or maximizing) is different, we see the similarity between $J(\theta)$ and $\tilde{R}(\theta)$ in the sense that they both optimize models directly towards evaluation metrics.

The partial derivative of $\tilde{R}(\theta)$ with respect to the model parameter $\theta$ is derived as follows. 
\begin{align}
\label{eq:partialrtilde}
\pderivrtilde &= \frac {\partial }{\partial \theta} \sum_{\sampledata} \mrtq \mrtdelta \nonumber \\
    &= \sum_{\sampledata} \mrtdelta \pderivtheta \mrtq
\end{align}

We need $\pderivtheta \mrtq$ in (\ref{eq:partialrtilde}).
For space efficiency, we use $\mrtqsimple$ as $\mrtq$ and $\mrtpsimple$ as $\mrtp$ below.

\begin{align}
\label{eq:partialq}
\pderivtheta \mrtqsimple &= \frac{\partial \mrtqsimple}{\partial \mrtpsimple} \frac{\partial \mrtpsimple}{\partial \theta} \ \ \ \ (\because \text{chain rule}) \nonumber \\
    &= \frac{\partial \mrtqsimple}{\partial \mrtpsimple} \nabla \mrtpsimple
\end{align}
%%% from here
For $\frac{\partial \mrtqsimple}{\partial \mrtpsimple}$, by applying the quotient rule to (\ref{eq:softmax}),
\begin{align}
\label{eq:qpartialp}
\frac{\partial \mrtqsimple}{\partial \mrtpsimple} &= \frac{\{\softmaxdenominator\} \pderivp \softmaxnumerator - \softmaxnumerator \pderivp \softmaxdenominator} {\{\softmaxdenominator\}{^2} } \nonumber \\
  &= \frac{\alpha \mrtpsimple ^{\alpha-1}} {\softmaxdenominator} - \frac{\alpha \mrtpsimple ^\alpha \mrtpsimple^{\alpha-1}} {\{\softmaxdenominator\}{^2}} \nonumber \\
  &= \alpha \frac{\mrtpsimple ^{\alpha-1}}{\softmaxdenominator} \left\{1- \frac{\mrtpsimple^\alpha}{\softmaxdenominator} \right\} \nonumber \\
  &= \alpha \frac{\mrtpsimple ^{\alpha}}{\softmaxdenominator} \frac{1}{\mrtpsimple} \left\{1- \frac{\mrtpsimple^\alpha}{\softmaxdenominator} \right\}
\end{align}
Thus, from (\ref{eq:partialq}) and (\ref{eq:qpartialp}), (\ref{eq:partialrtilde}) is
\begin{multline}
\pderivrtilde = \sum_{\sampledata} \mrtdelta \nabla \mrtpsimple \\ \left[ \alpha \frac{\mrtpsimple ^{\alpha}}{\softmaxdenominator} \frac{1}{\mrtpsimple} \left\{1- \frac{\mrtpsimple^\alpha}{\softmaxdenominator} \right\}  \right] \nonumber
\end{multline}
\begin{align}
\label{eq:partialr2}
  &= \alpha \mathbb{E} \left[\nabla\mrtpsimple \cdot \frac{1}{\mrtpsimple} \left\{ \mrtdelta - \mathbb{E}\left[\mrtdelta \right] \right\}  \right] \nonumber \\
  &= \alpha \mathbb{E} \left[\nabla \log \mrtpsimple \left\{ \mrtdelta - \mathbb{E}\left[\mrtdelta \right] \right\}  \right] %\\
\end{align}

According to the policy gradient theorem for REINFORCE \cite{williams1992simple,sutton1999policy}, the partial derivative of (\ref{eq:j}) is given as follows:
\begin{align}
\label{eq:partialj}
\pderivj = \tilde{\alpha} \mathbb{E} \left[\nabla \log \mrtpsimple \{\nrlreward - b \}  \right]
\end{align}
where $\tilde{\alpha}$ is a {\em learning rate}\footnote{In this appendix, we use $\tilde{\alpha}$ to distinguish it from smoothing parameter $\alpha$ in \mrt. } and $b$ is arbitrary {\em baseline reward} to reduce the variance of gradients.
Finally, we see that the gradient of \mrt (\ref{eq:partialr2}) is a special case of policy gradient in REINCOFCE (\ref{eq:partialj}) with $b = \mathbb{E}\left[\mrtdelta \right]$.
It is also interesting to see that the smoothing parameter $\alpha$ works as a part of learning rate ($\tilde{\alpha}$) in \proposed.
