\section{Experiments}
\label{sec:experiment}
\vspace{-1mm}
\begin{table}[t]
\small
\centering
\begin{tabular}{l|c|c|c}
\hline
       &     & {\bf mean chars} & {\bf \# sents.} \\
{\bf Corpus} & {\bf \# sents.} & {\bf per sent.}  & {\bf edited}   \\ \hline\hline
NUCLE  & 57k & 115             & 38\%          \\
FCE    & 34k & 74              & 62\%          \\
Lang-8 & 1M  & 56              & 35\%          \\ \hline
\end{tabular}
\caption{Statistics of training corpora}
\label{tab:corpora}
\vspace{-4mm}
\end{table}

\vspace{-1mm}
\paragraph{Data} For training the models (\mle and \proposed), we use the following corpora: 
the NUS Corpus of Learner English (NUCLE) \cite{dahlmeier-ng-wu:2013:BEA8}, 
the Cambridge Learner Corpus First Certificate English (FCE) \cite{yannakoudakis-briscoe-medlock:2011:ACL-HLT2011}, and
the Lang-8 Corpus of learner English \cite{tajiri-komachi-matsumoto:2012:ACL2012short}. 
The basic statistics are shown in Table \ref{tab:corpora}.\footnote{All the datasets are publicly available, for purposes of reproducibility.}
%% MJP: how did we define "unreasonable"? How many of these are done?
We exclude some unreasonable edits (comments by editors, incomplete sentences such as URLs, etc.) using regular expressions and setting a maximum token edit distance within 50\% of the original length.
We also ignore sentences that are longer than 50 tokens or sentences where more than 5\% of tokens are out-of-vocabulary (the vocabulary size is 35k).
In total, we use 720k pairs of sentences for training (21k from NUCLE, 32k from FCE, and 667k from Lang-8).
Spelling errors are corrected in preprocessing with Enchant.\footnote{\url{https://github.com/AbiWord/enchant}}
%open-source spellchecking library.

\vspace{-2mm}
\paragraph{Hyperparameters in \encdec}
For both \mle and \proposed, we set the vocabulary size to be 35k for both source and target.
Words are represented by a vector with 512 dimensions.
Maximum output token length is 50.
The size of hidden layer units is 1,000.
Gradients are clipped at 1, and beam size during decoding is 5.
We regularize the GRU layer with a dropout probability of 0.2.

For \mle we use mini-batches of size 40, and the ADAM optimizer \cite{kingma2014adam} with a learning rate of $10^{-4}$.
We train the \encdec with \mle for 900k updates,  selecting the best model according to the development set evaluation.

For \proposed we set the sample size to be 20. 
We use the SGD optimizer with a learning rate of $10^{-4}$. 
For the {\em baseline reward}, we use average of sampled reward following \newcite{williams1992simple}.
The sentence GLEU score is used as the reward $\nrlreward$.
Following a similar (but not the same) strategy of the Mixed Incremental Cross-Entropy Reinforce (MIXER) algorithm \cite{2015arXiv151106732R}, we initialize the model by \mle for 600k updates, followed by another 600k updates using \proposed, and select the best model according to the development set evaluation.
Our \proposed is implemented by extending the Nematus toolkit \cite{sennrich-EtAl:2017:EACLDemo}.
%\footnote{\proposed code is available at \url{anonymous}}

\begin{table}[t]
\small
\centering
\begin{tabular}{l|l|l}
\hline
Models      & Methods          & \# sents (corpora)    \\ \hline \hline
\cambhybrid & Hybrid           & 155k \\
            & (rule + PBMT)    & (NUCLE, FCE, in-house) \\ \hline
\amu        & PBMT +           & 2.3M   \\
            & GEC-feat.        & (NUCLE, Lang8) \\ \hline
\nus        & PBMT +           & 2.1M \\
            & Neural feat.     & (NUCLE, Lang8)  \\ \hline
\cambnmt    & \encdec (\mle) + & 1.96M \\
            & unk alignment    & (non-public CLC)   \\ \hline \hline
\mle/\proposed & \encdec       & 720k \\ 
            & (\mle/\proposed) & (NUCLE, Lang8, FCE) \\ \hline
\end{tabular}
\caption{Summary of baselines, \mle and \proposed models.}
\label{tab:models}
\vspace{-4mm}
\end{table}  

%\begin{table*}[t]
%\fontsize{9}{11}\selectfont
%\centering
%\begin{tabular}{l|l}
%\hline
%Orig. & For example it happens to me when I studied the concept of inflaction .\\ \hline
%Ref.  & For example , it happened to me when I studied the concept of inflation .\\ \hline \hline
%MLE   & For example , it happens to me when I studied the concept of inflation .\\ \hline
%NRL   & For example , it happens to me when I study the concept of inflation .\\ \hline \hline
%Orig. & Fish firming uses the lots of special products such as fish meal .\\ \hline
%Ref.  & Fish firming uses a lot of special products such as fish meal .\\ \hline \hline
%MLE   & Fish contains a lot of special products such as fish meals .\\ \hline
%NRL   & Fish shops use the lots of special products such as fish meal .\\ \hline
%\end{tabular}
%\caption{Example outputs by \mle and \proposed}
%\label{tab:examples}
%\vspace{-2mm}
%\end{table*}
 
\begin{table*}[t]
\fontsize{9}{11}\selectfont
\centering
\begin{tabular}{l|l}
\hline
Orig. & but found that successful people use the people money and use there idea for a way to success .\\ \hline
Ref.  & But it was found that successful people use other people 's money and use their ideas as a way to success .\\ \hline \hline
MLE   & But found that successful people use the people money and use it for a way to success .\\ \hline
NRL   & But found that successful people use the people 's money and use their idea for a way to success .\\ \hline \hline
Orig. & Fish firming uses the lots of special products such as fish meal .\\ \hline
Ref.  & Fish firming uses a lot of special products such as fish meal .\\ \hline \hline
MLE   & Fish contains a lot of special products such as fish meals .\\ \hline
NRL   & Fish shops use the lots of special products such as fish meal .\\ \hline
\end{tabular}
\caption{Example outputs by \mle and \proposed}
\label{tab:examples}
\vspace{-2mm}
\end{table*}
  
\vspace{-1mm}
\paragraph{Baselines}
In addition to our \mle baseline, we compare four leading GEC systems. 
All the systems are based on SMT, but they take different approaches.
The first model, proposed by \newcite{felice-EtAl:2014:W14-17}, uses a combination of a rule-based system and PBMT with language model reranking (referring as \cambhybrid).
\newcite{junczysdowmunt-grundkiewicz:2016:EMNLP2016} proposed a PBMT model that incorporates linguistic and GEC-oriented sparse features (\amu).
Another PBMT model, proposed by \newcite{chollampatt-hoang-ng:2016:EMNLP2016}, is integrated with neural contextual features (\nus).
%\nus is trained on a concatenation of NUCLE and Lang-8 that consists of 2.1M sentence pairs in total.
Finally, \newcite{yuan-briscoe:2016:N16-1} proposed a neural \encdec model with \mle training (\cambnmt). 
This model is similar to our \mle model, but \cambnmt additionally trains an unsupervised alignment model to handle spelling errors as well as unknown words, and it uses 1.96M sentence pairs extracted from the non-public Cambridge Learner Corpus (CLC).
The summary of baselines is shown in Table \ref{tab:models}.\footnote{The four baselines are not tuned toward the same dev set as \mle and \proposed. Also, they use different training set (Table \ref{tab:models}). We compare them just for reference.}


\vspace{-1mm}
\paragraph{Evaluation}
For evaluation, we use the JFLEG corpus \cite{napoles-sakaguchi-tetreault:2017:EACLshort}, which consists of 1501 sentences (754: dev, 747: test) with four fluency-oriented references per sentence.

Regarding the evaluation metric, in addition to the automated metric (\metric), we run a human evaluation using Amazon Mechanical Turk (MTurk).
We randomly select 200 sentences each from the dev and test set.
For each sentence, two turkers are repeatedly asked to rank five systems randomly selected from all eight: the four baseline models, \mle, \proposed, one randomly selected human correction, and the original sentence.
We infer the evaluation scores by efficiently comparing pairwise rankings with the TrueSkill algorithm \cite{HerbrichMG06,sakaguchi-post-vandurme:2014:W14-33}.



%\begin{table*}[!htbp]

%%% dev %%%
%turk 1.769 (1.659, 1.918)
%JNRL 0.169 (0.028, 0.28)
%JMLE -0.052 (-0.193, 0.129)
%CAMB16 -0.117 (-0.239, 0.011)
%NUS -0.131 (-0.212, 0.03)
%CAMB14 -0.16 (-0.3, -0.031)
%AMU -0.405 (-0.541, -0.26)
%src -1.072 (-1.232, -0.924)

%%% test %%%
%turk 1.565 (1.39, 1.758)
%JNRL 0.111 (-0.054, 0.304)
%JMLE -0.11 (-0.316, 0.015)
%CAMB16 -0.164 (-0.308, 0.061)
%AMU -0.168 (-0.386, -0.037)
%CAMB14 -0.225 (-0.323, -0.095)
%NUS -0.249 (-0.403, -0.09)
%src -0.76 (-0.945, -0.573)

\vspace{-1mm}
\paragraph{Results}
Table \ref{tab:results} shows the human evaluation by TrueSkill and automated metric (\metric).
In both dev and test set, \proposed outperforms \mle and other baselines in both the human and automatic evaluations.
Human evaluation and GLEU scores correlate highly, corroborating the reliability of \metric.
With respect to inter-annotator agreement, Spearman's rank correlation between Turkers is 55.6 for the dev set and 49.2 for the test set.
The correlations are sufficiently high to show the agreement between Turkers, considering the low chance level (i.e., ranking five randomly selected systems consistently between two Turkers).

\begin{table}[t]
\small
\centering
\begin{tabular}{l|c|c|c|c}
\hline
   & \multicolumn{2}{c|}{dev set} & \multicolumn{2}{c}{test set} \\ \hline
Models      & Human & GLEU  & Human  & GLEU \\ \hline  \hline
Original    & -1.072& 38.21 & -0.760 & 40.54\\ \hline
\amu        & -0.405& 41.74 & -0.168 & 44.85 \\
\cambhybrid & -0.160& 42.81 & -0.225 & 46.04 \\
\nus        & -0.131& 46.27 & -0.249 & 50.13 \\
\cambnmt    & -0.117& 47.20 & -0.164 & 52.05 \\ \hline
\mle        & -0.052& 48.24 & -0.110 & 52.75 \\
\proposed   & 0.169 & {\bf 49.82} & 0.111 & {\bf 53.98} \\ \hline \hline
Reference   & 1.769 & 55.26 & 1.565  & 62.37 \\ \hline
\end{tabular}
\caption{Human (TrueSkill) and GLEU evaluation of system outputs on the development and test set.}
\label{tab:results}
\vspace{-2mm}
\end{table}

\vspace{-1mm}
\paragraph{Analysis}
Table \ref{tab:examples} presents example outputs. 
In the first example, both \mle and \proposed successfully corrected the homophone error ({\em there vs. their}), but \mle changed the meaning of the original sentence by replacing {\em their idea} to {\em it}. 
Meanwhile, \proposed made the sentence more grammatical by adding a possessive {\em 's}.
%In the first example, both \mle and \proposed correctly inserted the comma, but \mle fails to correct verb tense inconsistency. 
%On the other hand, \proposed corrected the verb tense inconsistency, although it isn't the correct edit according to the context.
%% MJP: Is there a better example maybe?
The second example demonstrates challenging issues for future work in GEC. 
The correction by \mle looks fairly fluent as well as grammatical, but it is semantically nonsense. 
The correction by \proposed is also fairly fluent and makes sense, but the meaning has been changed too much.
For further improvement, better GEC models that are aware of the context or possess world knowledge are needed.
