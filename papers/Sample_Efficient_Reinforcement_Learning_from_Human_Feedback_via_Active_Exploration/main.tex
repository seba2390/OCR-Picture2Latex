
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm,amsthm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{Eq.~(\ref{#1})}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{cleveref}
\usepackage{listings}
\graphicspath{{figs/}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Hbb}{\mathbb{H}}
\newcommand{\Fbb}{\mathbb{F}}
\newcommand{\Ibb}{\mathbb{I}}
\newcommand{\Nbb}{\mathbb{N}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Rbb}{\mathbb{R}}

\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Afrak}{\mathfrak{A}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}
\newcommand{\Otilde}{\tilde{O}}
\newcommand{\Contextspace}{\mathcal{X}}
\newcommand{\preferred}{\succ}
\newcommand{\maybepreferred}{\overset{?}{\preferred}}
\newcommand{\Actionspace}{\mathcal{A}}
\newcommand{\exppolicy}{\pi_{x}}
\newcommand{\baexppolicy}{\pi_{x}^b}
\newcommand{\bestpolicy}{\hat{\pi}_T}
\newcommand{\estpolicy}{\hat{\pi}}
\newcommand{\hpolicy}{{\pi_{h}}}
\newcommand{\rlsquared}{RL$^2$}
\newcommand{\ainset}{a \in \Actionspace}
\newcommand{\sinset}{s \in \Statespace}
\newcommand{\lcbr}{\underline{f_r^t}}
\newcommand{\ucbr}{\overline{f_r^t}}
\newcommand{\bonus}{\beta_t^{(r)}}
\newcommand{\lastbonus}{\beta_T^{(r)}}
\newcommand{\kernel}{\kappa}
\newcommand{\rkhs}{\mathcal{H}_\kernel}
\newcommand{\borda}{f_r}
\newcommand{\algnm}{AE-Borda}
\newcommand{\algnmsp}{{\algnm} }
\newcommand{\dpoae}{AE-DPO}
\newcommand{\uniform}{Uniform-Borda}
\newcommand{\uucb}{UCB-Borda}
\newcommand{\usdpo}{US-DPO}
\newcommand{\xinset}{x \in \Contextspace}


\DeclareMathOperator{\arginf}{arg \inf}
\DeclareMathOperator{\maximize}{maximize}
\DeclareMathOperator{\EIG}{EIG}
\DeclareMathOperator{\JEIG}{JEIG}
\DeclareMathOperator{\EHIG}{EHIG}
\DeclareMathOperator{\bayesreturn}{BR}
\DeclareMathOperator{\Bernoulli}{Bern}
\newcommand{\preferenceMatrix}{f}
\newcommand{\subopt}{\operatorname{SubOpt}}
\newcommand{\RKHS}{\mathcal{H}}
\newcommand{\dpoloss}{\mathcal{L}_{\text{DPO}}}
\newcommand{\sftpolicy}{\pi_{\text{SFT}}}
\newcommand{\rewub}{\overline{r}}
\newcommand{\rewlb}{\underline{r}}
\newcommand{\winner}{w}
\newcommand{\data}{x,a,a',w}
\newcommand{\Domain}{\mathcal{D}}

\newcommand{\vm}[1]{\textcolor{red}{[#1 --VM]}}
\newcommand{\oj}[1]{\textcolor{pink}{[#1 --ON]}}
\newcommand{\williex}[1]{\textcolor{magenta}{[WN: #1]}}
\newcommand{\vdx}[1]{\textcolor{blue}{[#1 --VD]}}
\newcommand{\roryx}[1]{\textcolor{brown}{[#1 --Rory]}}
\newcommand{\danx}[1]{\textcolor{cyan}{[#1 --Dan]}}
\newcommand{\joex}[1]{\textcolor{teal}{[#1 --Joe]}}
\newcommand{\jeff}[1]{\textcolor{Orange}{[Jeff: #1]}}
\newcommand{\senl}[1]{\textcolor{green}{[#1 --SL]}}
\definecolor{nicegreen}{RGB}{91,226,91}
\newcommand{\add}[1]{#1}
\newcommand{\linkfunction}{\add{\rho}}
 
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}[theorem]{Lemma}



\title{Sample Efficient Reinforcement Learning\\ from Human Feedback via Active Exploration}



\author{Viraj Mehta, Vikramjeet Das, Ojash Neopane, Jeff Schneider\\
Carnegie Mellon University\\
Pittsburgh, PA, USA \\
\texttt{\{virajm,vdas,oneopane,schneide\}@cs.cmu.edu} \\
\And
Yijia Dai \\
Cornell University \\
Ithaca, NY, USA  \\
\texttt{yd73@cornell.edu}\\
\And
Ilija Bogunovic \\
University College London \\
London, UK\\
\texttt{i.bogunovic@ucl.ac.uk}\\
\And
Willie Neiswanger \\
Stanford University \\
Stanford, CA, USA \\
\texttt{neiswanger@cs.stanford.edu} \\
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\preprint
\begin{document}


\maketitle

\begin{abstract}
Preference-based feedback is important for many applications in reinforcement learning where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback (RLHF) on large language models. For many applications of RLHF, the cost of acquiring the human feedback can be substantial. In this work, we take advantage of the fact that one can often choose contexts at which to obtain human feedback in order to most efficiently
identify a good policy, and formalize this as an \emph{\add{active} contextual dueling bandit} problem.
We give an upper-confidence-bound style algorithm for this problem and prove a polynomial worst-case regret bound.
We then provide empirical confirmation in a synthetic setting that our approach outperforms existing methods.
After, we extend the setting and methodology for practical use in RLHF training of large language models.
Here, our method is able to reach better performance with fewer samples of human preferences than multiple baselines on three real-world datasets.
\end{abstract}

 
\vspace{-2mm}
\section{Introduction}
\label{sec:introduction}
\vspace{-2mm}
The alignment of foundation models with user preferences has gained unprecedented importance due to the widespread utilization of large language models (LLMs).
The established pipeline for alignment in LLMs, as outlined in \citet{stiennon2020} and \citet{ouyang2022training}, comprises two essential steps given a pretrained LLM.
First, in the Supervised Fine-Tuning (SFT) phase, the LLM undergoes fine-tuning via supervised learning with examples demonstrating the desired behavior.
In the second step, Reinforcement Learning from Human Feedback (RLHF),
a policy generates multiple completions for each conversation prefix (prompt) in a training set;
users then give ordinal preferences amongst the set of completions for a particular prompt.
These preferences are used to train a `reward model' via a ranking loss like the Bradley-Terry-Luce (BTL) model \citep{bradley1952rank}.
Finally, the policy is trained, typically via Proximal Policy Optimization (PPO) \citep{schulman2017proximal}, to optimize the reward model while not moving too far from the SFT-trained policy.

As these models continue to scale and their areas of application broaden, the number of roles for which we need to align them increases as does the overall scale of human-generated training data requirements.
Data annotation for preference-based learning is already a substantial cost for companies that train LLMs.
This cost is likely to grow alongside the industry.
This is especially acute for LLMs in specialized areas, where the cost of expert feedback can be substantially higher.



In this work, we take advantage of the fact that we control which prompts and completions we provide to human labelers in order to make efficient use of their efforts.
Drawing on recent advancements in active exploration for reinforcement learning \citep{li2023nearoptimal} and in black-box optimization \citep{xu2020zeroth}, we introduce a method for assessing the value of collecting preferences on specific datapoints that is both prospective and task-focused.
First, we formalize this setting as a \emph{dueling contextual bandit problem} and design an efficient algorithm that offers polynomial worst-case sample complexity guarantees regarding the policy's performance. 
Next, we extend these ideas to a more real-world setting: choosing datapoints for the training of LLM assistants.
Here, we build atop recent work \citep{dpo},
which allows us to apply active data selection to an RLHF process using a supervised objective and single model.
We evaluate the method on three datasets: the Stanford Human Preferences dataset \citep{pmlr-v162-ethayarajh22a}, the Anthropic Helpful-Harmless dataset \citep{bai2022training}, and a third dataset (which we contribute to the literature) that extends an existing dataset of Jeopardy!\ questions and answers to
evaluate the ability of an alignment method to avoid hallucinations. We find that our algorithm can boost performance by over 10\% on the preference datasets when performing RLHF with a modest human-feedback sample budget, and that our method is best at avoiding hallucinations on our Jeopardy!\ dataset.




%
 \vspace{-2mm}
\section{Related Work}
\vspace{-2mm}

\paragraph{Learning from Comparative Feedback}
Reinforcement learning from comparative human feedback has been studied for more than a decade, including work by \citet{furnkranzPreferenceRL}, \citet{akhourRobustPreferenceRL} and, notably, \citet{deepRLFromHumanPreferences}, which
enabled sample-efficient collection of human feedback for deep reinforcement learning (RL) by training a reward model as the RL target. In the Atari test case, where naive deep RL would have necessitated thousands of hours of gameplay, they accomplished superior performance with just 5,500 or several hours of human queries.

Many recent approaches have demonstrated the effectiveness of using human feedback to enhance stylistic continuation \citep{ziegler2019fine}, text summarization \citep{stiennon2020}, translation \citep{Kreutzer2018}, semantic parsing \citep{LawrenceAndReizler2018}, review generation \citep{Cho2018}, and evidence extraction \citep{Perez2019}. In particular, the work by \cite{bai2022training} places focus on improving model reliability and robustness by incorporating human feedback to gauge the helpfulness or harmfulness of its responses. However, while effective, the integration of human feedback comes with substantial costs. For example, \citet{stiennon2020} achieved substantial enhancements over baseline methods but required the generation of summaries for 123,169 posts from the TL;DR dataset, a task performed by a large team of labelers
from crowdsourcing platforms.
This heavy-resource requirement is reflected in state-of-the-art work. \citet{ouyang2022training} emphasizes RLHF to improve alignment of the GPT-3 model across aspects such as toxicity, hallucinations, moral opinion, and overall quality. Here, the team enlisted the efforts of 40 labelers and worked with a dataset comprising over 100,000 examples labeled by humans.







\vspace{-1mm}
\paragraph{Dueling Bandits}
The bandit literature has also explored the effectiveness of comparative feedback---for example, in the  ``dueling bandit'' setting---while considering the cost of acquiring such information. This was first studied by \citet{yue2012} in settings where comparative information is relatively easy to extract but absolute rewards (\textit{i.e.}, direct queries) are ill-defined and have no absolute scale. Later, \citet{bengs2021preferencebased} surveyed methods used in the online learning setting, where the trade off with cost of information is most acute, including those used in the online contextual dueling bandit setting by \citet{dudk2015contextual}.
These constraints motivate a kernelized approach that can incorporate the nonlinearities in the models used in practice.

\vspace{-1mm}
\paragraph{\add{Active} Contextual Bandit Optimization}
When there exist distinct phases of learning and then deployment, an agent can often take steps for improved sample efficiency.
For example, in a contextual bandit setting, \citet{char_ocbo}
consider the problem where at test time the goal is to perform well on average across a context distribution, while during the learning phase the goal is to choose both contexts and actions for best performance at test-time.
The authors proposed a multi-task version of Thompson sampling during the training phase, which yields provable regret bounds. We extend this setting from cardinal to ordinal rewards as is appropriate for comparative feedback.

In \citet{li2023nearoptimal}, the agent queries contexts where the value function is most uncertain and acts optimistically. Combined with least-squares value iteration, this method leads to provable polynomial-sample convergence in the worst-case error of the value function estimate in reinforcement learning in general, and as a corollary the setting from \citet{char_ocbo} as a special case. This sets the foundation that we will adapt to the comparative feedback~setting.

In the realm of \active{active} contextual bandits that make use of kernels, previous research has explored various aspects, including robust objectives \citep{bogunovic2018adversarially}, distributional robustness \citep{kirschner2020distributionally, ramesh2023distributionally}, multi-agent learning and mixed strategies \citep{sessa2019noregret, sessa2020mixed}. However, to our knowledge, none of the methods proposed in these prior studies can be directly employed in our specific dueling setting.

We also include related work on uncertainty estimation in large language models in Sec.~\ref{a:uncertaintyllms}.
 \vspace{-2mm}
\section{Problem Setting}
\label{s:problem_setting}
\vspace{-2mm}
In this paper, we consider a dueling variant of \add{what we denote the active} contextual bandit problem introduced in \citet{char_ocbo} that we refer to as ACDB for short.
An instance of this problem is defined by a tuple $(\Contextspace, \Actionspace, \preferenceMatrix)$ where $\Contextspace$ denotes the context space, $\Actionspace$ denotes the action space and $\preferenceMatrix: \mathcal X \times \mathcal A \times \mathcal A \rightarrow [0, 1]$ is a preference function so that $\preferenceMatrix(x, a, a')$ denotes the probability that the action $a$ is preferred to the action $a'$ when the underlying context is $x$.
We also define a domain $\Domain = \Contextspace \times \Actionspace$.
We will design algorithms that operate under the following interaction protocol, which occurs for $T$ time steps.
During each time step $t \in [T]$, the agent selects a context $x_t \in \Contextspace$ and a pair of actions $a_t, a_t' \in \Actionspace$ and observes a binary random variable $\winner_t \sim \Bernoulli(\preferenceMatrix(x_t, a_t, a_t'))$ which equals one if $a_t$ is preferred to $a_t'$ and zero otherwise.\looseness=-1

We assume that the preference function takes the following form,
\begin{equation}
    \label{eq:preference_function}
    \preferenceMatrix(x, a, a') = \linkfunction\left( r(x, a) - r(x, a') \right),
\end{equation}
where $\linkfunction : \Rbb \rightarrow [0, 1]$ is the \emph{link function} and $r: \Domain \rightarrow \Rbb$ is the \emph{unknown} reward function.
Common link functions include the logistic function, which leads to the Bradley-Terry-Luce (BTL) model \citep{bradley1952rank} as well as the Gaussian CDF \citep{thurstone1927method}.
We also place some additional assumptions on the reward function for our theoretical analysis in the kernelized setting.

Our objective within this protocol is to design algorithms that are able to quickly identify policies with a small suboptimality gap. We define the suboptimality gap of a learner's policy $\pi: \mathcal X \to \mathcal A$ as
\begin{equation}
    \subopt(\pi) = \sup_{x \in \Contextspace} \left( \sup_{a \in \Actionspace} r(x, a) - r(x, \pi(x)) \right).
\end{equation}
We remark that this notion of suboptimality (considered in \citet{char_ocbo} and \citet{li2023nearoptimal}) is stronger than usual notions that look at the expected suboptimality of the final policy when the contexts are sampled from some known distribution.
In contrast, the form of suboptimality we consider here looks at the worst-case context for each policy. For the kernelized and LLM settings we address below, we will make explicit the instantiation of this problem setting.





 \vspace{-2mm}
\section{Active Exploration in the Kernelized Setting}
\label{s:kocbd}
\vspace{-1mm}

In this section, we describe our first contribution---a theoretically principled algorithm for the ACDB problem---and provide formal guarantees on its performance.
In order to provide such guarantees, we must first instantiate our general problem setup by making assumptions on the preference function $\preferenceMatrix$ (from ~\eqref{eq:preference_function}).
In particular, we need to specify a class of functions that contain the true unknown reward function.
This choice is subtle as we need to balance the trade-off between the expressiveness of our function class with theoretical tractability.
Motivated by its theoretical popularity and empirical success, we choose this function class to be a Reproducing Kernel Hilbert Space.
While this choice of function class is common in the literature, we take a slight departure from the standard assumptions in order to more appropriately accommodate our problem setting.

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figs/overview-diagram.pdf}
    \vspace{-2mm}
    \caption{\small Illustration of the \add{active} contextual dueling bandit setting, and its application to sample-efficient RLHF in large language models.}
    \label{fig:overview_diagram}
    \vspace{-3mm}
\end{figure}

\paragraph{The Contextual Borda Function} Before going over our assumptions, we first introduce the \emph{contextual Borda function} $\borda$, which is core to our algorithm.
The contextual Borda function generalizes the Borda function introduced in \citet{xu2020zeroth} for dueling-choice optimization which is defined as the probability that a particular action $a$ will be preferred over a random action $a'$ uniformly sampled from the action space.
We generalize this definition to the contextual setting as follows, given as $\borda: \Domain \to [0, 1]$ where $\borda(x, a) = \Ebb_{a'\sim U(\Actionspace)}\left[\preferenceMatrix(x, a, a')\right]$, where $U(\Actionspace)$ is the uniform measure over the action space.
It is clear from the definition that $f_r$ and $r$ will have the same maximizers.

We now discuss the assumptions we make.
Our first assumption restricts the reward and contextual Borda functions to be `smooth' in an underlying Reproducing Kernel Hilbert Space (RKHS).
\begin{assumption}\label{ass:norm}
    Let $\kernel: \Domain \times \Domain \to \Rbb$ denote a positive semi-definite kernel and let $\RKHS_\kernel$ denote its associated RKHS. We assume that $ \left \lVert r \right \rVert_{\kernel}, \left \lVert \borda \right \rVert_{\kernel} \leq B$, where $B$ is a known constant.
\end{assumption}
Note that this assumption is stronger than the standard assumption, which only requires that $r$ has a bounded RKHS norm.
It is difficult to bound the norm of $f_r$ given a bound on the norm of $r$ due to the generality of our setting, which allows for different link functions.
We investigate this issue numerically in Appendix~\ref{s:rkhs_borda} where we find that the norm of the Borda function is almost always smaller than the norm of the reward function for samples drawn from the distribution of basis functions used for experiments in Section~\ref{s:kocbd_experiments}.

Our second assumption relates the reward function to the contextual Borda function.
\begin{assumption}
    \label{ass:borda}
    Let $\borda^*(x) = \max_a \borda(x, a)$ and $r^*(x) = \max_a r(x, a)$. There exists a constant $L_1$ such that for every $x \in \Contextspace$, $a \in \Actionspace$ we have $\frac{1}{L_1}(r^*(x) - r(x, a)) \leq \borda^*(x) - \borda(x, a)$.
\end{assumption}
This assumption implies that differences in $r$ will cause a similar magnitude of difference in $f_r$. In fact, when the link function $\linkfunction(\cdot)$ is Lipschitz continuous, it is sufficient for its Lipschitz constant to be at least $1/L_1$ for this condition to hold. \add{We note that this assumption holds for the two most commonly used link functions, the logistic function \citep{bradley1952rank} and the Gaussian CDF \citep{thurstone1927method}.}

\subsection{Methods} \label{s:kocbd_methods}

At a high level, our approach reduces the dueling feedback problem to contextual optimization over a single action via the \emph{contextual Borda function} introduced above.
Once reduced appropriately, we apply techniques adapted from recent work on active exploration in reinforcement learning to construct a sampling rule and policy selection rule which allows us to output a policy with provably low sub-optimality.
Broadly, our sampling rule samples contexts at which there is maximum uncertainty over the Borda `value function' and then compares the optimistic action with an action sampled uniformly from the action set.

\paragraph{Estimating the Contextual Borda Function}
By design, we can estimate the contextual Borda function using preference data $\{x_t, a_t, a'_t, \winner_t\}$ by selecting $x_t, a_t$ in an arbitrary fashion and sampling $a'_t$ uniformly at random.
For low dimensional settings, our algorithm first estimates the contextual Borda function using standard kernelized ridge regression (KRR) \citep{rasmussen2006gaussian}---we refer the reader to Appendix~\ref{a:rkhs_regression} for an explicit description of this regression procedure.
In Section~\ref{s:llm}, we explore modifications of our methods for higher-dimensional settings, such as in the case of LLMs.
One key feature of KRR is that it  provides both an estimate of the contextual Borda function after $t$ observations, $\mu_t(x, a)$, as well as uncertainty quantification of the predictions.
Indeed, under Assumptions~\ref{ass:norm}~and~\ref{ass:borda} we can show that $|f_r(x, a) - \mu_t(x, a)| \leq \beta \sigma_t(x, a)$ for an appropriately chosen $\beta$ and $\sigma_t(x, a)$ (see Lemma \ref{lem:confidence_bounds}).\looseness=-1

\paragraph{Selecting Contexts and Actions}
Our sampling rule builds on top of the one established in \citet{li2023nearoptimal}---put simply, the rule is to sample the state with the maximum uncertainty over the value function and then act optimistically.
We now present our algorithm which shows how to extend these ideas to the dueling setting via the contextual Borda function $f_r$.

For now, we assume that there is a known bonus term $\bonus$ for all $t$. We can then define upper and lower confidence bounds $\ucbr(x, a) = \mu_t(x, a) + \bonus \sigma_t(x, a)$ and $\lcbr(x, a) = \mu_t(x, a) - \bonus \sigma_t(x, a)$.
Our rule is to select a context
\begin{equation}
\label{eq:context_selection}
\begin{aligned}
    x_t \in \argmax_{\xinset}\left(\max_{\ainset} \ucbr(x, a) - \max_{\ainset}\lcbr(x, a)\right).
\end{aligned}
\end{equation}
Here, we are choosing a context that maximizes the difference between the optimistic `value function' and the pessimistic `value function' (both of which require a maximization over actions to compute).
We then optimistically choose an action
\begin{equation}
\label{eq:action_selection}
    a_t \in \argmax_{\ainset}\ucbr(x_t, a).
\end{equation}
After repeating this process $T$ times, we output a pessimistic policy against the tightest lower bound we can find, which is the maximizer of all our lower bounds through the optimization process. Put formally, we return $\bestpolicy: \Contextspace \to \Actionspace$ such that
\begin{equation}
\label{eq:bestpolicy}
    \bestpolicy(x) \in \argmax_{a \in \Actionspace}\max_{t \leq T}\lcbr(x, a).
\end{equation}
From these pieces we construct the full active exploration algorithm, AE-Borda, which we present in Algorithm~\ref{alg:Borda-AE}.
\begin{algorithm}[t!]
        \caption{\algnm}
        \label{alg:Borda-AE}
        \begin{algorithmic}[1] \STATE {\bfseries Input:} kernel function $\kernel(\cdot, \cdot)$, exploration parameters $\bonus$, number of inital data $n_0$
            \STATE Let $D_{n_0} = \{x_i, a_i, a'_i, \winner_i\}_{i=1}^{n_0}$ for $x_i, a_i, a'_i$ drawn uniformly at random.
            \FOR {$t=n_0 + 1,\dots,T$}
                \STATE Compute $\mu_t(\cdot, \cdot)$, $\sigma_t(\cdot, \cdot)$ using KRR.
                \STATE Choose $x_t$ according to \eqref{eq:context_selection}.
                \STATE Choose $a_t$ according to \eqref{eq:action_selection}, draw $a'_t\sim U(\Actionspace)$, and draw $\winner_t \sim \Bernoulli(\preferenceMatrix(x_t, a_t, a_t'))$.
                \STATE Let $D_t = D_{t-1} \cup \{(x_t, a_t, a'_t, \winner_t)\}$.
            \ENDFOR
            \STATE Output a final policy $\bestpolicy$ according to \eqref{eq:bestpolicy}.
        \end{algorithmic}
    \end{algorithm}

\vspace{-4mm}
\subsection{Analysis}
\label{s:kocdb_analysis}
\vspace{-1mm}
Before proceeding with our algorithm's formal guarantees, we first introduce the \emph{maximal-information gain} which plays an important role in our results.
The maximum information gain over $t$ rounds, denoted $\Phi_t$, is defined as
\begin{equation}
    \Phi_t = \max_{A \subset \Contextspace\times\Actionspace: \left \lvert A \right\rvert = t} I(r_A + \epsilon_A; r_A),
\end{equation}
where $r_{A} = \left[ r(x) \right]_{x \in A}$ , $\epsilon_A \sim N(0, \eta^2 I)$, and $I(X; Y) = H(X) - H(X | Y)$ is the mutual information.
With this definition, we are now ready to state our result.
\begin{theorem}
    \label{thm:regret}
    Suppose we run Algorithm~\ref{alg:Borda-AE} with
    \begin{equation}
        \beta^{(r)}_t = 2B + \sqrt{2 \Phi_t + 1 + \log \left( \frac 2 \delta \right)},
    \end{equation}
    then, with probability at least $1 - \delta$, we have that
    \begin{equation}
        \subopt(\hat \pi_T) \leq O \left(  \frac{L_1}{\sqrt{T}} \left(B + \Phi_T\sqrt{\log \frac{1}{\delta}} \right)\right).
    \end{equation}
\end{theorem}
\paragraph{Proof Sketch.}
At a high-level, the proof of this result is as follows.
First, we use standard results on KRR to show that our choice of $\beta^{(r)}$ guarantees that our confidence bands contain $f^r(x, a)$ with high probability simultaneously for all $t$ and $x, a \in \Contextspace \times \Actionspace$.
Next, we use Assumption~\ref{ass:borda} to show that the suboptimality of the  pessimistic policy induced by our estimated contextual Borda function is small whenever we are able to estimate the contextual Borda function well.
Finally, we conclude the proof by showing that our sampling rule indeed allows us to estimate the contextual Borda function well.
The full proof can be found in Appendix~\ref{thm:regret}.
\paragraph{Concrete Performance Bounds.}
To more concretely understand the performance of our algorithm, we instantiate our results for two commonly studied kernels: the linear and squared-exponential.
For both of these settings, the scaling of the information gain is well known (see for example \cite{srinivas2009gaussian}).
In the linear setting, we have that $\Phi_T = O(d \log T)$ leading to a bound of $O \left( \tfrac{L_1}{\sqrt{T}} \left( d \log T \right) \right)$.
For squared exponential kernels we have $\Phi_T = O \left( \log(T)^{d + 1} \right)$ leading to a suboptimality bound of $O \left( \tfrac{L_1}{\sqrt{T}} \left(\log(T)^{d + 1} \right) \right)$.

When compared to existing results for dueling bandits \citep{xu2020zeroth} as well as standard bandits \citep{chowdhury2017kernelized}, we see that our suboptimality bounds match, thus showing that our algorithm is able to achieve the same performance under a stronger performance metric.
 \subsection{Experiments in the Kernelized setting}
\label{s:kocbd_experiments}
In order to assess the validity of our theory we have conducted synthetic experiments that allow us to come as close as possible to the theoretical setting and empirically confirm our results.
To do so, we implemented the regression using the \textrm{BernoulliGP} model provided by GPyTorch \citep{gardner2018gpytorch}.
We use a Mat\'ern kernel with automatic relevance detection with hyperparameters fit via maximum a posteriori optimized by the Adam algorithm \citep{kingma2014adam}.
\begin{wrapfigure}{r}{0.35\textwidth}
    \vspace{-1mm}
    \centering
    \includegraphics[width=0.34\textwidth]{kocbd_results.pdf}
    \vspace{-6mm}
    \caption{\small Performance of all methods across 10 random functions $r$ with 1D Context and 1D action. The top plot shows the median regret across contexts and the bottom shows the maximum. Error bands show one standard error.}
    \label{fig:comparison}
    \vspace{-9mm}
\end{wrapfigure}
We tested on distributions of synthetic reward functions generated by sampling a random linear combination of Random Fourier Features \citep{rahimi_rff} derived from a squared exponential kernel.
For each sampled reward function $r$, we used the Bradley-Terry model where $p(\winner = 1 \mid a, a', x) = \frac{1}{1 + \exp(r(x, a') - r(x, a))}$ to generate comparison data.
For each trial we uniformly sampled $n_0=25$ datapoints and then selected data to observe until $T=500$ total datapoints had been collected according to one of three methods:\looseness=-1
\begin{itemize}[leftmargin=5mm, parsep=0mm, topsep=0mm]
    \item \textbf{\algnm}: our method, as described in Section~\ref{s:kocbd_methods}.
    \item \textbf{\uniform}: uniform sampling of both contexts and actions.
    \item \textbf{\uucb}: uniform sampling of contexts, along with UCB actions as in \algnm.
\end{itemize}

This last method reduces to the method presented in \cite{xu2020zeroth} when naively generalized to the contextual setting. All methods have the same test-time behavior of executing the action found by optimizing the pessimistic Borda function estimate for the test context.
By optimizing the ground-truth reward function we were able to approximate the optimal policy and therefore estimate the regret of our policy against it.
We give an example of the progression of our method for 1D context and 1D actions in Figure~\ref{fig:kocbd_viz} as well as a comparison against \uniform~and \uucb~in Figure~\ref{fig:comparison}.
One can see that \algnm~performs best both on median regret and on the maximum regret, which was the metric of interest in our theoretical analysis.\looseness=-1

It is clear in Figure~\ref{fig:kocbd_viz} that the method is quickly able to concentrate samples in regions that could plausibly be the optimum and it is similarly clear that the peaks in the acquisition function over contexts are sensible given the mean and uncertainty estimates of $f_r$. We give a set of results showing the progression of \algnm\ in Section~\ref{a:kocdb_addtl_experiments}.

 \vspace{-2mm}
\section{Scaling Active Exploration to Large Language Models}
\label{s:llm}
\vspace{-2mm}
In order to adapt our method to the case where $\Contextspace$ and $\Actionspace$ are both large spaces of sequences as is common in natural language processing, we must address a few limitations of the \algnmsp method presented in Section~\ref{s:kocbd_methods}:
\begin{itemize}[leftmargin=6mm, topsep=0mm, parsep=1mm]
\item The contextual Borda function $\borda$ as defined above is unsuitable for an action space that is extremely large and where most actions are obviously bad (a uniformly sampled sequence of tokens is trivially distinguishable from natural language).
\item Neural network training proceeds in batches and it would be highly inefficient to label and train on a single example at a time.
\item The uncertainty estimation tools in sequence modeling are more limited than those for explicitly kernelized models, especially due to the memory constraints in training LLMs.
\end{itemize}
We address these issues through a handful of modifications to our method as we specialize it to the LLM case.
Though these modifications mean that we lose the theoretical guarantees in the previous section, we assert that given the rates of convergence associated with kernelized approximations of neural net architectures, we are not giving up strong guarantees in this setting.
In particular, we modify the selection rule given in \eqref{eq:context_selection} to avoid having to use the Borda function, we na\"ively do batched subset selection for our training minibatches, and we estimate the uncertainty of our policy using dropout for uncertainty estimation \citep{gal2016dropout}.
In this section, we build atop the foundation presented in \citet{dpo}, which avoids training a separate reward model; this is primarily due to the fact that we prefer to select datapoints based on the estimated uncertainty of the model used for decision making rather than any proxy.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{vis_100}
    \vspace{-5mm}
    \caption{\small From left: the ground truth contextual Borda function $f_r$ (the red line is the optimal policy), the mean of our posterior estimate of $f_r$ (the red line is the best policy estimate), the uncertainty function $\sigma_t$, and the value function $\max_a \borda^t$. In the middle two plots, red dots are queries where $w_t = 0$ and green are where $w_t = 1$. We plot the value function with confidence intervals in blue on right as well as the value function uncertainty from \eqref{eq:context_selection} in orange. For a full version of this Figure, see Fig.~\ref{a:kocdb_addtl_experiments}.}
    \label{fig:kocbd_viz}
    \vspace{-3mm}
\end{figure}


\paragraph{Direct Preference Optimization}
Direct Preference Optimization (DPO) \citep{dpo} avoids training a separate reward model based on preferences by instead training the policy directly on pairwise comparison using a loss that optimizes an equivalent objective despite functionally behaving like classification.
As with PPO~\citep{schulman2017proximal}, this loss depends on a reference policy, which
we take
to be the policy derived from the supervised fine-tuning step, $\sftpolicy$.
The loss is defined as
$\dpoloss(\pi_\theta; \sftpolicy)$ $=$ $-\Ebb_{(x, a, a', \add{w}) \sim \Dcal}\left[\log\linkfunction \left(\gamma(2\winner-1)\left(\log\frac{\pi_\theta(a\mid x)}{\sftpolicy(a\mid x)} - \log\frac{\pi_\theta(a'\mid x)}{\sftpolicy(a'\mid x}\right)\right)\right]$.
The derivation in \citet{dpo} also shows that optimizing this objective is equivalent to training a PPO policy with reward function
\begin{equation}
    \label{eq:dpo_reward}
    r(x, a) = \add{\gamma}\log\frac{\pi_r(a\mid x)}{\sftpolicy(a\mid x)} + \add{\gamma}\log Z(x),
\end{equation}
where $\gamma$ is the hyperparameter of PPO scaling the KL penalty, $Z(x)$ is a partition function, and $\pi_r$ is the policy which optimizes the PPO objective.
\paragraph{An Acquisition Function for DPO}
We observe as in the original paper that $\pi_r$ is precisely the probability distribution which DPO is estimating.
Therefore, the uncertainty estimates for our DPO policy are uncertainty estimates for $\pi_r$ and we can use them to give an approximate confidence interval for $r$ ($\rewub$ and $\rewlb$).
Concretely, we need to address the autoregressive nature of $x$ and $a$ in our case. We will assume $a$ consists of ordered tokens $t_i$ and that $\log\pi(a\mid x) = \sum_{t_i \in a}\log\pi(t_i\mid x, t_1, \dots, t_{i-1})$.
In our method, we employ dropout for uncertainty quantification.
Specifically, the $m$ dropout masks $d_j$ are integrated into the function $\pi(t_i\mid x, t_1, \dots, t_{i-1}, d_j)$.
During inference, we perform autoregressive Monte Carlo sampling with dropout enabled, resulting in an ensemble of predictions with a mean $\add{\mu(t_i\mid x, t_1, \dots, t_{i-1}) = \frac{1}{m}\sum_{j\in [m]}\log\pi(t_i \mid x, t_1, \dots, t_{i-1}, d_j)} $.
The standard deviation $\add{\sigma(t_i \mid x, t_1, \dots, t_{i-1})=\sqrt{\frac{1}{m - 1}\sum_{j \in [m]}\left(\log\pi(t_i \mid x, t_1, \dots, t_{i-1}, d_j)\right)^2}}$ across this ensemble serves as an approximation for the model's epistemic uncertainty.
This technique allows us to capture uncertainty in a computation and memory efficient manner without compromising model performance.
Given these estimates, we can compute our upper and lower bounds as follows:
\begin{align}
\rewub(x, a) = \sum_{t_i \in a}\mu(t_i\mid x, t_1, \dots, t_{i-1}) + \beta \sigma(t_i\mid x, t_1, \dots, t_{i-1})\add{-\log\sftpolicy(a\mid x)}, \\
\rewlb(x, a) = \sum_{t_i \in a}\mu(t_i\mid x, t_1, \dots, t_{i-1}) - \beta \sigma(t_i\mid x, t_1, \dots, t_{i-1})\add{-\log\sftpolicy(a\mid x)},
\end{align}
for an uncertainty parameter $\beta > 0$.
In the previous section, we chose contexts according to \eqref{eq:context_selection}. Here, we define an acquisition function using a similar quantity:
\begin{equation}
    \label{eq:context_selection_dpo}
    \alpha(x) = \max_{a\in\Actionspace\add{(x)}}\rewub(x, a) - \max_{a \in \Actionspace\add{(x)}}\rewlb(x, a).
\end{equation}
In this equation, $\alpha(x)$ is the uncertainty of the state-value function according to $x$. In choosing the states where the potential for error in the value achieved is largest, the agent can learn to behave well in those places.
This criterion is similar to that in \cite{li2023nearoptimal} and provides similar guarantees to ours for max-regret in the \add{active} contextual bandit setting.
In situations like ours where we are using fixed offline datasets, we set $\Actionspace\add{(x)}$ in \eqref{eq:context_selection_dpo} to the set of available responses \add{for a particular action; otherwise, we use $\Actionspace(x) = \Actionspace$}.


\vspace{-2mm}
\paragraph{An algorithm for active RLHF}
From here, we use the acquisition function in \eqref{eq:context_selection_dpo} in order to choose points that are maximally informative. We must do this in batches in order to respect the constraints of training large models. We address this in the naive fashion, pulling a larger batch of some size, evaluating $\alpha$ and then choosing the top-$b$ elements in order to address this criterion. We refer to our full procedure as AE-DPO, and give a description in Algorithm~\ref{alg:DPOAE}.
\begin{algorithm}[t!]
    \caption{\dpoae}
    \label{alg:DPOAE}
    \begin{algorithmic}[1] \STATE {\bfseries Input:} Reference policy $\sftpolicy$, exploration parameter $\beta$, policy constraint weight $\gamma$, batch size $b$, number of iterations $N$
        \FOR {$t=n_0 + 1,\dots,N$}
            \STATE Draw an unlabeled batch $B_u = \{(x_i, a_i, a'_i)\} \sim D$.
            \STATE Evaluate $\alpha(x_i)$ and let $B_l$ be a batch of the top-$b$ elements of $B_u$ by $\alpha$ value.
            \STATE Collect labels $r_i$ and add them to $B_l$.
            \STATE Update the policy $\pi_\theta$ (initialized from the ref.) using a gradient step against $\dpoloss$ using $B_l$.
        \ENDFOR
        \STATE Output $\pi_\theta$
    \end{algorithmic}
\end{algorithm}
 \subsection{Experiments using LLMs}
In order to evaluate whether our method is able to improve the selection of datapoints in RLHF, we conduct a set of experiments in which we train LLMs on three datasets using one of four methods. The goal of our empirical study is to see whether improving the data selection strategy causes the downstream policy to perform better on a given training task. In order to isolate the effect of the data selection method, we vary the selection method while largely holding our model and training procedure consistent.
In all the experiments in this section, we compare four methods: \textbf{DPOAE}, the method we presented in Section~\ref{s:llm}; \textbf{USDPO}, which chooses $x$ that maximize variance of the log probabilities of completions; \textbf{DPO}, the method from \citet{dpo}, selecting batches uniformly at random; and \textbf{SFT}, which continues supervised learning with uniformly selected batches.
In our training pipeline, we first train a baseline model with a Llama-7B \citep{llama} architecture using supervised fine-tuning (SFT) on a 40\% split of data.
We add a dropout layer before the penultimate linear layer for our uncertainty estimation mechanism and fine tune with dropout active.
Next, we train using each of the four methods for 30000 samples, evaluating every 2048 samples---each time using our initial SFT trained model as a starting point. We give additional information on our experimental procedures in Section~\ref{a:additional_details}.



We evaluate these methods on three different datasets.
The first two, the Anthropic Helpful-Harmless (HH) dataset \citep{bai2022training} and the Stanford Human Preferences (SHP) dataset \citep{pmlr-v162-ethayarajh22a}, are taken from the  literature.
HH contains examples of two kinds: situations where an assistant needs to be helpful to a user asking a reasonable question and situations where an assistant should prioritize being harmless as the user is requesting a harmful action. All completions in HH are machine-generated. SHP is a dataset of Reddit posts with comments in 18 different categories and therefore consists of a broader range of human-generated text, but doesn't have the inherent tradeoff of HH.
We evaluate policies trained on both of these by checking the rate at which the policy produces answers which are preferred to the chosen completion for the prompt in the dataset.


\begin{figure}
    \centering
    \includegraphics[width=0.24\textwidth]{HH_winrates.pdf}
    \includegraphics[width=0.24\textwidth]{SHP_winrates}
    \includegraphics[width=0.49\textwidth]{jeopardy_conditional.pdf}
\caption{\small From left: \add{smoothed} win rates \add{against preferred choices in dataset} of samples generated from each policy at end of RLHF training runs across the final four evaluations, and all seeds, on the HH (first) and SHP (second) datasets. In the latter two plots, we force each policy to generate a (non-null) answer,
and then, conditional on the answer being correct (fourth) or incorrect (third), plot the rate at which each policy abstains.
}
    \label{fig:llm_expts}
\end{figure}

For the completions generated from the HH and SHP prompts, we use GPT-3.5 \citep{brown2020language} to generate winners amongst comparisons between the preferred choices given in the dataset. We give the prompts we use for evaluation in Section~\ref{a:eval_prompt}. In Figure~\ref{fig:comparison}, we see that for the completions in the later part of our training run, \dpoae\ performs best among the methods and outperforms \usdpo\ as well as the other baselines that sample uniformly. We believe this to be due to our acquisition function $\alpha$, which accounts for the structure of the decision making problem in choosing which point to query. We do find our results to be noisy---due to the computational expense of these trials (which we elaborate on in Section~\ref{a:exptruntimes}),
we were not able to run each experimental baseline for a large number of seeds to further reduce uncertainty in our results.



We also introduce a new Jeopardy!\ dataset that includes a preference structure that captures some of the structure of the game while addressing the question of LLM hallucination.
We augment a dataset consisting of 217,000 Jeopardy!\ questions and answers from HuggingFace \citep{jeopardy_huggingface} with a plausible incorrect answer, using GPT-3.5.
As in the game show, where points are deducted for an incorrect response, we enforce during training that a correct answer is preferred to an abstention (the empty string) and both of these should be preferred to the incorrect answer.
We found that our models do not learn to provide correct answers at a higher rate through a small amount of DPO training or additional SFT beyond what is required for them to answer the questions. This is unsurprising as trivia is intended not to generalize easily; in other words, it's difficult to imagine learning that the third US president was Jefferson given training examples of the first two.
Instead, we evaluate policies for this dataset on the rate at which they abstain for questions (``null rate'') where they counterfactually would have been correct vs where they would have been incorrect.
Ideally, the policy learned would \emph{always} abstain where it would have been incorrect and \emph{never} abstain where it would have been correct.
Naturally, this is an important goal in the alignment of LLMs and we hope to provide a straightforward benchmark for this effort.
\add{We include an additional exhibit where we use the factual nature of this dataset to begin to evaluate the dropout-based uncertainty estimation techniques we use in \cref{s:uncertainty}.}

For the Jeopardy!\ dataset, we checked the probability of an empty generation and whether it was the most likely token.
We generated a nonempty sequence in order to see whether the generated answer was correct, including as a counterfactual in the cases where the method would have abstained.
We plot this in Figure~\ref{fig:llm_expts}, where we see that the \dpoae\ method is the only method that learns to abstain from answering questions (modestly) more often when the model would have given the incorrect answer. We also find that the standard DPO method quickly learns not to abstain. No methods abstain more than a couple percent of the time in the case where they would have been correct. We also plot the results for correctness in Section~\ref{a:dpo_additional_expts}, which shows that no model substantially learns new factual information.
 \vspace{-4mm}
\section{Discussion}
\vspace{-3mm}
In this work, we addressed the problem of how to select contexts and actions at which to obtain human preferences, such that the reinforcement learning agent learns most efficiently.
We focus on this problem setting in the context of reinforcement learning from human feedback in large language models (LLMs), where collecting data from humans is expensive.
This problem is particularly meaningful because, in the future, it is likely that we need feedback from specialized humans whose time is extremely limited and in order to provide personalization of LLMs without a prohibitive training period.
The methods developed in this work show promise in reducing these costs.
We also make a theoretical contribution where we give guarantees on worst-case regret; though our assumptions are specific, we are optimistic about principled approaches that extend to more general settings and guarantee safety of decision making agents.
Our computational study was constrained by the resources available---given the initial promising results of our method, we hope to scale up our experimental campaign to greater numbers of both GPUs and RLHF steps in order to see how our methods perform with larger computational budgets.
%
 
\bibliography{main}
\bibliographystyle{iclr2023_conference}

\appendix
\newpage

\section{RKHS Regression}\label{a:rkhs_regression}

At step $t$, we have data $\{(x_1,a_1,a'_1, w_1), \dots, (x_t,a_t,a'_t, w_t)\}$. The kernel ridge regression estimate is defined by,
\begin{equation}
    \mu_t = \argmin_{ f \in \mathcal{H}} \sum_{i=1}^t (f(x_i,a_i) - w_i)^2 + \lambda\|f\|_{\mathcal{H}}^2 \text{ .}
\end{equation}
Denote by $\boldsymbol{w}_t = [w_1, \dots, w_t]^T$ the vector of observations, $(K_t)_{i,j=1,\dots,t} = k(x_i,a_i,x_j,a_j)$ the data kernel matrix, and $k_t(x,a) = [k(x,a,x_1,a_1), \dots, k(x,a,x_t,a_t)]^T$ the data kernel features. We then have
\begin{align}
    \mu_t(x,a) &= k_t(x,a)^T (K_t + \lambda \mathbf{1}_t)^{-1} \boldsymbol{w}_t \text{ .}
\end{align}
We further have the posterior variance $\sigma_t(x,a)^2$ that determines the width of the confidence intervals,\looseness=-1
\begin{align}
    \sigma_t(x,a)^2 &= k(x,a,x,a) - k_t(x,a)^T(K_t + \lambda \mathbf{1}_t)^{-1} k_t(x,a) \text{ .}
\end{align}

\section{Proof of Theorem \ref{thm:regret}}
\label{a:regret_proof}
In this section we will prove our main Theorem, \ref{thm:regret}. The overall strategy of the proof is to use our Lipschitz assumption on the link function (more precisely, the relative Lipschitzness of the reward $r$ and the Borda function $f_r$) in order to go to the Borda function, which we can directly model from data. Then, we use our selection criteria as well as confidence bounds taken from \citet{chowdhury2017kernelized} and convergence rates taken from \citet{kandasamy2019multi} in order to complete the argument. We give these cited results as lemmas in what follows.

In order to attain a particular policy performance with probability $1 - \delta$, we must bound the error of the estimates given by our KRR process for a particular confidence level.
In order to do so, we adapt the result from \citet{chowdhury2017kernelized}, Theorem 2.
\begin{lemma}
    \label{lem:confidence_bounds}
    Let $\bonus = 2||f_r||_{\kernel} + \sqrt{2(\Phi_{t-1}(\Contextspace \times \Actionspace) + 1 + \log(2 / \delta))}$.
    Then with probability $1 - \delta$ we have for all time $t$ and any point $\left( x, a \right) \in \Contextspace \times \Actionspace$,
    \[|\mu_{t-1}(x, a) - f_r(x,a )| \leq \bonus \sigma_{t-1}(x, a).\]
\end{lemma}

\begin{proof}
    To prove this result, we will verify that all the conditions from Theorem 2 of \citet{chowdhury2017kernelized} hold.
    Recall Assumption~\ref{ass:norm} which states that $ \left \lVert f_r \right \rVert_{\kernel} \leq B$.
    Next, we observe that since $a_t' \sim U \left( \Actionspace \right)$ (independent of everything else), we have that $ \mathbb{E} \left[ w_t \mid \mathcal{F}_{t - 1} \right] = f_r(x_t, a_t)$, where $ \mathcal{F}_t = \linkfunction \left( \left\{ \left( x_s, a_s, a_s', w_s \right) \right\}_{s = 1}^{t} \right)$ is the filtration generated by the past observations.
    Additionally, since $w_t \in \left\{ 0, 1 \right\}$ and $x_t, a_t$ are both $ \mathcal{F}_{t - 1}$ measurable, we see that $w_t$ can be written as
    \begin{equation*}
        w_t = f_r(x_t, a_t) + \eta_t,
    \end{equation*}
    where $\eta_t$ is $ \mathcal{F}_{t - 1}$-conditionally subGaussian.
    Therefore, we have met all the necessary conditions, and we can apply Theorem 2 of \citet{chowdhury2017kernelized} which gives us the desired result.
\end{proof}

This lemma jointly bounds the modeling error over the Borda function for all time $t$ though it introduces a dependence on the RKHS norm of $f_r$. This dependence is inherited from prior work, but we empirically study the relationship between the RKHS norm of a particular reward function and that of the associated Borda function in Section \ref{s:rkhs_borda}.

We also adapt a result from Lemma 8 of \citet{kandasamy2019multi} in order to understand the convergence of our uncertainty function $\sigma_t$.
\begin{lemma}
    \label{lem:convergence}
    Suppose we have $n$ queries $(q_t)_{t=1}^n$ taken from $\Contextspace \times \Actionspace$. Then the posterior $\sigma_t$ satisfies
    \[\sum_{q_t}\sigma^2_{t-1}(q_t)\leq \frac{2}{\log(1 + \eta^{-2})} \Phi_{n}(\Contextspace\times\Actionspace).\]
\end{lemma}
Lemma~\ref{lem:convergence} gives us a handle on how quickly we can expect the uncertainty function to shrink as additional datapoints are observed.

Now that we have lemmas \ref{lem:confidence_bounds} and \ref{lem:convergence} in place, we can proceed to the proof of the main result.

\begin{proof}
        In this proof, we condition on the event in Lemma~\ref{lem:confidence_bounds} holding true.
        Given that occurence, we can say the following for every $\xinset$.
        \begin{align}
                \max_{\ainset} r(x, a) - r(x, \bestpolicy(s)) & \overset{\text{Assumption \ref{ass:borda}}}{\leq} L_1
                \left(\max_{\ainset} f_r (x, a) - f_r(x,\bestpolicy (x))\right) \\
             & \overset{\text{Lemma~\ref{lem:confidence_bounds}}}{\leq}
                L_1\left(\max_{\ainset} f_r (x, a) -   \max_{t \in [T]}\; \lcbr(x, \bestpolicy(x))\right)  \\
             &\overset{ \text{Def. of } \bestpolicy}  {=}
                 L_1\left(\max_{\ainset} f_r (x, a)  - \max_{\ainset} \max_{t \in [T]}\; \lcbr(x, a)\right) \\
             &=
                 L_1\min_{t \in [T]} \left(\max_{\ainset} f_r (x, a)  - \max_{\ainset} \; \lcbr(x, a)\right) \\
             & \overset{\text{Lemma~\ref{lem:confidence_bounds}}} {\leq}
                 L_1\min_{t \in [T]} \left(\max_{\ainset} \ucbr(x, a) - \max_{\ainset} \; \lcbr(x, a)\right) \\
            & \overset{ \text{Def. of } x^t}{\leq}
                L_1\min_{t\in [T]} \left(\max_{\ainset} \ucbr(x^t, a) - \max_{\ainset} \; \lcbr(x^t, a)\right) \\
            & \overset{\text{Def. of } a^t} {\leq}
                L_1\min_{t \in [T]}\left( \ucbr(x^t, a^t) -  \; \lcbr(x^t, a^t)\right) \\
            & \leq
                \frac{L_1}{T}\sum_{t=1}^T \left( \ucbr(x^t, a^t) -  \; \lcbr(x^t, a^t)\right)\\
            & = \frac{L_1}{T}\sum_{t=1}^T 2\bonus \sigma_t(x^t, a^t)\\
            & \overset{\bonus\text{ is increasing}}{\leq} \frac{2L_1\lastbonus}{T}\sqrt{\left(\sum_{t=1}^T\sigma_t(x^t, a^t)\right)^2}\\
            & \overset{\text{Cauchy-Schwarz}}{\leq}
                \frac{2L_1\lastbonus}{T}\sqrt{T\sum_{t=1}^T\sigma^2_t(x^t, a^t)}\\
            & \overset{\text{Lemma~\ref{lem:convergence}}}{\leq}
                \frac{2L_1\lastbonus}{\sqrt{T}}\sqrt{C_1 \Phi_T}\\
            & \overset{\text{def of }\lastbonus}{=} \frac{2L_1}{\sqrt{T}}(2B + \sqrt{2(\Phi_{t-1} + 1 + \log(2 / \delta))})\sqrt{C_1 \Phi_T}\\
            & = O \left(  \frac{L_1}{\sqrt{T}} \left(B + \Phi_T\sqrt{\log \frac{1}{\delta}} \right)\right).
        \end{align}


\end{proof}

\section{RKHS norms of $r$ and $\borda$}
\label{s:rkhs_borda}
In order to understand the dependence of our estimation bound on the RKHS norm $||\borda||_{\kernel}$, we ran numerical experiments on sampled reward functions. For a variety of context and action dimensions, we sampled 1000 reward functions as in Section~\ref{s:kocbd_experiments} and numerically approximated their RKHS norms. We also made a Monte-Carlo estimate of the Borda function $f_r$ for each of the reward functions sampled and numerically approximated its RKHS norm.
To do this, we uniformly sample 1,000 points $x_i$ from the input space, compute the regularized kernel matrix $K$ for this set $x_i$, solve the KRR problem $K \alpha = f(x)$ for $\alpha$. Then we compute the quadratic form $\sqrt{\alpha^T K\alpha}$ as an estimate of the RKHS norm.

In Table~\ref{tab:borda_norm}, we present the results of comparing the RKHS norms of 1000 reward functions and their associated Borda functions sampled as in Section~\ref{s:kocbd_experiments}. A `win' was counted when the Borda function had smaller RKHS norm and a `loss' otherwise. The win margin is the average difference in RKHS norms of the reward and Borda functions, with a positive value when the Borda function was of smaller norm.
It is clear here that in
general (though not always) the RKHS norm of the Borda function $\borda$ for a particular reward function $r$ is smaller than the RKHS norm of the reward function $r$ itself. This relationship seems to grow stronger as the input dimensionality of the reward function grows larger.

\begin{table}[]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
Context Dimension & Action Dimension & Win Rate & Win Margin\\ \midrule
0 & 1 & 0.16 & -6.3 \\
1 & 1 & 0.89 & 5.1 \\
1 & 3 & 1 & 21.4 \\
3 & 1 & 1 & 21.5 \\
3 & 3 & 1 & 38.7 \\
10 & 10 & 1 & 19.6 \\
\bottomrule
\end{tabular}
\caption{Comparison of RKHS norms of reward functions and associated Borda functions}
\label{tab:borda_norm}
\end{table}
\section{Additional Experiments for Kernelized Setting}
\label{a:kocdb_addtl_experiments}

In Figure~\ref{fig:progression}, we depict the progress of the \algnm method as it continually acquires data. One can see that the estimated optimal policy (red, second row) converges to a function quite similar to the ground truth (red, first row) as more data is collected. In addition, it is clear that the selection criterion targets parts of the domain which are relevant to policy learning while avoiding obviously bad regions. We also see in the fourth row that the uncertainty over the value function decreases relatively smoothly across the context space, supporting the idea that our method controls max-regret effectively.
\begin{figure*}
    \centering
    \hspace{6mm} \textbf{Time = 50} \hspace{24mm} \textbf{Time = 150} \hspace{24mm} \textbf{Time = 600} \\
    \vspace{3mm}
    \includegraphics[width=0.3\textwidth]{vis_50}
    \includegraphics[width=0.3\textwidth]{vis_150}
    \includegraphics[width=0.3\textwidth]{vis_600}
    \vspace{-3mm}
    \caption{Progress of \algnm~across 50, 150, and 600 datapoints. From the top downwards, the charts show the ground truth function, the mean of the posterior estimate of $f_r$, the uncertainty function, the estimate of the value function as well as the acquisition function given in \eqref{eq:context_selection}, and the regret over time.}
    \label{fig:progression}
\end{figure*}
\section{The Jeopardy! preference dataset}
\label{a:jeopardy}
We generated a set of plausible wrong answers for the Jeopardy! dataset from Huggingface \citep{jeopardy_huggingface} by asking GPT-3.5 for a plausible wrong answer given the question, category, and answer. We found that both the category and correct answer were necessary to include to direct GPT-3.5 to generate an answer which was appropriate for the category and to prevent it from accidentally generating a correct answer. We give the prompt used for this process in Figure~\ref{fig:jeopardy_prompt}.

\begin{figure}
\begin{tcolorbox}[colback=white]
\begin{lstlisting}[breaklines, breakindent=0pt, basicstyle=\ttfamily\footnotesize]
[System]
You are an assistant coming up with plausible but incorrect answers to Jeopardy questions (just the answer, no "what is"). Here's an example:\n
Q: 'For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory'
Category: HISTORY
Correct Answer: Copernicus\n
Response: Brahe
[User]
Q: {question}
Category: {category}
Correct Answer: {answer}
Response:
\end{lstlisting}
\end{tcolorbox}
\caption{The prompt used to collect plausible wrong answers for Jeopardy! questions.}
\label{fig:jeopardy_prompt}
\end{figure}


\section{Related Work on Uncertainty Estimation in Large Language Models}
\label{a:uncertaintyllms}
Estimating the epistemic uncertainty in large language models is still an active area of research and there are few prior works on this topic.
For example, \cite{osband2022fine} augment existing models with additional layers to model randomness, and subsequently the uncertainty. However performing uncertainty quantification in a parallelized fashion requires a significant memory overhead. To be more amenable to larger models, we instead use a dropout-augmented model to estimate uncertainty, as detailed in Section~\ref{s:llm}.


\section{Prompt templates}
\label{a:eval_prompt}
The prompt templates for GPT-4 as the pairwise comparison evaluation judge and GPT-3.5 as the Jeopardy!\ single answer correctness judge are listed in Figures \ref{fig:eval_prompt_compare} and \ref{fig:eval_prompt_jeo}. We maintain the standardized prompts proved to be effective by \citet{zheng2023judging}.

\begin{figure}
\begin{tcolorbox}[colback=white]
\begin{lstlisting}[breaklines, breakindent=0pt, basicstyle=\ttfamily\footnotesize]
[System]
Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions and answers the user's question better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. Output your final verdict by strictly following this format: 'A' if assistant A is better, 'B' if assistant B is better, and 'C' for a tie. Output only that character and do not include any other characters or spaces.

[User Question]
{question}

[The Start of Assistant A's Answer]
{answer_a}
[The End of Assistant A's Answer]

[The Start of Assistant B's Answer]
{answer_b}
[The End of Assistant B's Answer]
\end{lstlisting}
\end{tcolorbox}
\caption{The default prompt for pairwise comparison.}
\label{fig:eval_prompt_compare}
\end{figure}


\begin{figure}
\begin{tcolorbox}[colback=white]
\begin{lstlisting}[breaklines, breakindent=0pt, basicstyle=\ttfamily\footnotesize]
[System]
You are a judge on whether a contestant answer to Jeopardy is correct given a correct answer. If you don't see the correct answer it is not correct. Answer 'Yes' or 'No' is sufficient. Please don't use any other words.

[The Start of Correct Answer]
{correct_answer}
[The End of Correct Answer]

[The Start of Contestant Answer]
{contestant_answer}
[The End of Contestant Answer]
\end{lstlisting}
\end{tcolorbox}
\caption{The default prompt for evaluating single Jeopardy!\ answer.}
\label{fig:eval_prompt_jeo}
\end{figure}

\section{Additional Experiment Details}
\label{a:additional_details}
We train our initial SFT models for 1 epoch on the SHP and HH dataset and 2 epochs on the new Jeopardy!\ dataset.
We select the initial training period based on the amount of training after which we obtained a validation loss which had plateaued.
We also find it reasonable to add a dropout layer before the penultimate linear layer since we find that adding a dropout layer not to negatively affect the performance in the SFT phase.
To aid in fitting the model on our GPUs, we use QLoRa \citep{lora, qlora} with 4bit quantization for model weights and optimize using the 8-bit Lion optimizer \citep{chen2023symbolic}.
For the methods with a reference model, we put the policy and the reference model on two separate GPUs. Further, we use dropout probability of $p=0.05$, policy constraint weight $\gamma=0.1$, an uncertainty bonus $\beta = 4$, a learning rate of $5 \times 10^{-7}$, an unlabeled batch size of 128, and a training batch size $b$ of 32. We run all experiments with 3 random seeds. \add{Our implementation was built atop the one provided by the authors of the DPO paper \citep{dpo}.}


\section{Experiment Runtimes}
\label{a:exptruntimes}
\begin{table}[h!]
\centering
\begin{tabular}{l|c|c|c|}
\cline{2-4}
                                  & Jeopardy! & SHP     & HH      \\ \hline
\multicolumn{1}{|l|}{Further SFT} & \ 2 $\vert$ 3    & 4 $\vert$ 4   & 3 $\vert$ 7   \\ \hline
\multicolumn{1}{|l|}{DPO}         & 7.5 $\vert$ 25  & 10 $\vert$ 14 & 10 $\vert$ 15 \\ \hline
\multicolumn{1}{|l|}{US-DPO}      & \ \ \ 8 $\vert$ 12    & 79 $\vert$ 85 & 31 $\vert$ 85 \\ \hline
\multicolumn{1}{|l|}{AE-DPO}      & \ \ \ 9 $\vert$ 12    & 44 $\vert$ 45 & 18 $\vert$ 53 \\ \hline
\end{tabular}
\caption{Runtimes (min $\vert$ max) for each experiment rounded to nearest hour. Several experiments require a significant amount of compute time to complete. Runtimes vary depending on current loads on compute clusters.}
\label{t:exptruntimes}
\end{table}

\section{Additional Experiments with LLM}
\label{a:dpo_additional_expts}
Here, we plot the training curves for the Jeopardy! dataset below.
For Jeopardy!, we plot the correctness of the policy over time in Figure~\ref{fig:jeopardy_correct}.
Though this is part of the goal of the agent in the Jeopardy!\ dataset, note that it is not the entire optimization objective, as we show in Figure~\ref{fig:llm_expts}. Here, it is clear that no policy is able to improve at predicting correct answers on the test set. This is unsurprising as trivia is a difficult generalization problem.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figs/jeopardy_correct.pdf}
    \caption{Rate of correct answers for Jeopardy!\ over time.}
    \label{fig:jeopardy_correct}
\end{figure}
\add{
\subsection{Evaluating dropout-based LLM uncertainty estimation}
}
\label{s:uncertainty}

We believe that in general the estimation of uncertainty for LLMs is an important topic of research and progress there will facilitate a more efficient and informed use of this technology. As we discussed in \cref{a:uncertaintyllms} and \cref{s:llm}, we use a dropout-based uncertainty estimation technique to inform the active exploration in this work. Over the course of this study, we considered ensembles and epistemic networks \citep{osband2022fine} as alternative methods for estimating the uncertainty of LLMs. However, each of these methods comes with some additional GPU memory requirement. For epistemic networks, the additional network parameters take GPU memory, while for ensembles, the memory is required to store multiple copies of a network or at least mutiple LoRAs. In our initial studies we found epistemic networks and dropout to perform comparably well and therefore chose dropout due to its smaller memory consumption and good performance.
In this section, we explore whether the uncertainties predicted by our estimates differ when the model predicts the correct, incorrect, or null answer and whether these predictions differ in the cases when the model decides to predict null. 
To do this, we evaluated the log probabilities predicted by $\sftpolicy$ on a test set of 20,560 Jeopardy! clues for the correct, incorrect, and null answer. 
We computed the sample variances over the log probabilities $\sigma^2(a\mid x) = \sum_{t_i \in a} \sigma^2(t_i\mid x, t_1, \dots, t_{i - 1})$
and plotted their densities in \cref{fig:llm-uncertainty}.

We see that the model predicts the highest variances for the log probabilities of incorrect answers.
We also see that the the model seems to predict especially low variances for the null token when it decides to output it.
The correct answer seems to have a lower variance when the model is willing to predict an answer. 
We see that the log probabilities of incorrect answers always have a high variance, indicating high uncertainty. We also see that the null token has a low variance when the model has a non-null output indicating certainty that it should not abstain. The variance further drops when it outputs null, indicating certainty about not knowing an answer. The correct answer has a lower variance than the incorrect answer when the model does not abstain. The relative variances of these two curves support that the model provides meaningful indications of uncertainty. Additionally, in the case where the model abstains, even the correct answer has a high variance, indicating a high uncertainty.
We believe that these results support that the uncertainty function is at least correlated with the model's knowledge about the input.
This offers support to the hypothesis that our estimates of the variance are somewhat meaningful.
However, we believe that this is an important research topic and warrants substantial further study under a variety of lenses.
We hope that this work will encourage further research in this area.
\begin{figure}
    \centering
\includegraphics[width=\textwidth]{uncertainties_full}
\caption{Density of $\sigma(a\mid x)$ conditioned on correct, incorrect, and null values for $a$. The left hand plot depicts the variance distributions conditional on the model outputing a non-null completion, while the right hand is conditional on a null completion.}
    \label{fig:llm-uncertainty}
\end{figure}
 
\end{document}
