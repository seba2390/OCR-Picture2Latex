% \vspace{-7pt}
% \section{Limitation}

% This work has many limitations:

% \begin{itemize}
%     \item 
% \end{itemize}

\section{Discussion and Future Work}
\label{sec:discuss}


\vspace{3pt}
\noindent
\textbf{Post-Constraint Analysis.} Our approach prioritizes post-constraints
over other constraints, such as preconditions. By focusing on the post-constraints, we enhance the precision and scalability significantly. 
Importantly, our utilization of large language models in program analysis
suggests strong abilities in summarizing complex function behaviors involving loops, a classic hurdle in program analysis.
%paving the way for potential future research.
% \zhiyun{Think about whether we want to discuss the knowledge ChatGPT already has, which greatly benefited our solution. I am surprised to see Yang Liu's work showing extremely bad results. Are their benchmarks relatively unknown compared to the Linux kernel? I suspect it may play a role.}


\vspace{3pt}
\noindent
\textbf{Better Integration with Static Analysis.} Our work presents
opportunities for greater integration and synergy with static analysis methods.
Currently, our proposed solution operates largely independently of the static analysis methods, taking only inputs from static analysis initially.
Looking into the future, we can consider integrating static analysis and LLMs in a holistic workflow.
For example, this could involve selectively utilizing LLM as an assistant to overcome certain hurdles encountered by static analysis,
\eg difficulty in scaling up the analysis or summarizing loop invariants.
In turn, further static analysis based on these findings can provide insights to refine the queries to the LLM. This iterative process
could enable a more thorough and accurate analysis of complex cases.
We believe such a more integrated approach is a very promising future direction.


\cut{
For instance, preprocessing steps with static analysis could include tasks such
as variable normalization and conversion to SSA form. This can 
mitigate the confusion of variable reuse and varieties of postconditions.

On the flip side, the results from our LLM-based
approach could also be fed back into the static analysis process for further
examination. While a summary of \texttt{may\_init} and \texttt{must\_init} is
usually sufficient for detecting UBI bug, more complex bugs might
necessitate a feedback structure. This could involve utilizing the results from
the LLM, performing further static analysis based on these findings, and then
using these insights to refine the queries to the LLM. This iterative process
could enable a more thorough and accurate analysis for complex cases,
demonstrating the potential benefits of a more integrated approach combining
LLMs with traditional static analysis techniques.
}

\vspace{3pt}
\noindent
\textbf{Deploying on Open-sourced LLMs.}
The reproducibility of \work could be potentially challenged, considering its dependency on GPT-4, a closed-source API subject to frequent updates. 
At the time of writing, Meta introduced Llama 2, an open-source language model with capabilities rivaling GPT-3.5. Our initial assessments suggest that Llama 2 can understand our instructions and appears well-suited to support \work. The open-source nature of Llama 2 provides us with opportunities to deploy and refine the model further. We plan to leverage these prospects in future studies.




\cut{
\vspace{3pt}
\noindent
\textbf{Dependency on Closed-Source LLM.} 
Our approach relies heavily on the
closed-source API provided by OpenAI for GPT-4, which has proven instrumental in
achieving our promising results. However, this dependency also introduces a
degree of uncertainty and lack of control. As we do not have direct access to
the inner workings of GPT-4, we are somewhat at the mercy of OpenAI's
implementation, availability, and potential changes to the API. This reliance
could lead to issues in the reproducibility and adaptability of our work, and
could potentially impact its long-term stability and reliability. 
\cut{
As we move
forward, developing a more self-controlled large language model or exploring
open-source alternatives  might be worthwhile considerations to ensure greater
control and reliability.
}
In the future, we plan to deploy \work on a open sourced LLMs such as Llama 2 
\cite{meta_2023_meta_2023}.
}




\cut{
\vspace{3pt}
\noindent
\textbf{Expanding the Scope Beyond UBI Detection.} While our current
implementation primarily targets UBI, the concepts
and techniques could be extended to a wider array of tasks in program analysis.
This not only includes detecting other types of bugs but also expands into areas
traditionally handled by static analysis, yet challenging in practice.
For instance, identifying memory leaks and race conditions are tasks where
static analysis can struggle due to the dynamic nature of these issues. An
LLM-based approach may offer an edge here, as it could better infer the
contextual clues in the code to pinpoint potential problem areas.
}


\cut{
\vspace{3pt}
\noindent
\textbf{Dependency on Well-Maintained Codebase.} Our approach to program
analysis, while innovative and effective, does presuppose a certain level of
code quality. Given that the LLM relies on patterns and conventions it has
learned from well-written, idiomatic code, the method may struggle when applied
to poorly maintained codebases. If code is not properly structured, or if
function names, variable names, or comments are unclear or misleading, this
could potentially confound the LLM's ability to accurately analyze the code. For
instance, a function with a misleading name might be misunderstood by the model,
leading to inaccurate inferences about its behavior. Thus, while our approach
can handle a wide variety of code, it performs best when applied to clean,
well-documented, and conventionally structured codebases. This underlines the
importance of good coding practices not just for human readability, but also for
the efficacy of automated analysis tools.
}


\cut{
\vspace{3pt}
\noindent
\textbf{Other LLMs.}
In addition to GPT, 
We also try with other competitors such as Bard \cite{krawczyk_bards_2023} and Claude 2 \cite{anthropic_2023_claude}.
Our tests indicate Bard also has an extensive capability in recognizing functions and code functionalities. However, 
Bard appears insensitive to our progressive prompting design and consistently provides results directly --- which is typically incorrect.
Claude 2, on the other hand, can understand our progressive prompt and ask for further function
calls. However, it misclassifies the \texttt{p9\_check\_errors} to \texttt{must\_init} in our 
preliminary test. \haonan{updated with new exprs...}

In addition to their insensitivities to progressive prompts, Bard and Cladue 2 are less knowledgeable in the Linux kernel. For example, for the case 
we will show later in \S\ref{subsec:case_study}, \texttt{get\_user\_pages\_unlocked}. Claude 2 guesses according to the function and parameter name and 
does not recognize its purpose. Bard gives a plausible but incorrect response. Interested readers can check the entire conversations 
on this page\footnote{\href{https://anonymous.4open.science/r/LLift-Open/doc/DT-OtherLLMs.md}{https://anonymous.4open.science/r/LLift-Open/doc/DT-OtherLLMs.md}}.
}

% \vspace{3pt}
% \noindent
% \textbf{Other LLMs Than GPT.}


% Specifically, it appears that Bard does not fully comprehend our instructions
% ; for instance, it consistently provides results directly rather than progressively requesting uncertain function definitions.

% \textbf{Remaining 4 cases.} 
% \texttt{start\_creating} and \texttt{wait\_for\_completion} are
% two examples where \work may produce errors. The first case is related to
% various data structures in the file system, while the second case is due to
% concurrency issues. Although the function summaries for \texttt{start\_creating}
% are considered "correct," they are not sufficient to filter out the false
% positives. These imperfections can potentially be addressed by modifying the
% prompt and refining the preprocess.

\cut{
\begin{figure}[t]
  \begin{minted}[xleftmargin=10pt, linenos, fontsize=\footnotesize]{c}
static bool pv_eoi_get_pending(...){
  u8 val;
  if (pv_eoi_get_user(vcpu, &val) < 0)
    apic_debug(...);
  return val & 0x1;
}
  \end{minted}
%   \caption{A real UBI bug from \texttt{arch/x86/kvm/lapic.c}}
%   \Description{}
%   \label{fig:fn}
% \end{figure}

\vspace{3pt}
% \begin{figure}
    % \centering
    % \includegraphics
  \begin{minted}[xleftmargin=10pt, linenos, fontsize=\footnotesize]{json}
"response": {
  "func_call": "pv_eoi_get_user(vcpu, &val) < 0",
  "parameters": ["vcpu", "&val"],
  "must_init": [],
  "may_init": [{"name": "&val", "condition": ...}]
}
  \end{minted}
    \caption{The code and summary of \texttt{pv\_eoi\_get\_user} from GPT-4 
    %\zhiyun{font size of the code snippet can be smaller. We don't want to make them bigger than the main text font size}
    }
    \label{fig:res_pv_eoi}
\end{figure}
}

    % \centering
    % \includegraphics{}
% int p9pdu_readf(..., const char *fmt, ...){
%   va_start(ap, fmt);
%   ret = p9pdu_vreadf(pdu, proto_version, fmt, ap);
%   va_end(ap);
% }




