\section{Implementation}
% \haonan{In this section, we describe the prototype of \work.}

We implement the prototype of \work based on OpenAI's API~\cite{openai_2022_introducing_2022}
(\ie gpt-4-0613).
% with GPT-4 with 8k tokens. GPT-4 is the most powerful LLM so far; we use it to demonstrate all potential of \work.
% Specifically, under version gpt-4-0613 with a token limit of 8,192.
% All implementation is finished by the end of
% July 2023.
% \yu{Evaluation?}
We describe some implementation details in the following aspects:


\vspace{3pt}
\noindent
\textbf{Interaction with LLMs.} \work's interaction with LLMs is managed by a simple agent developed in Python, containing roughly 1,000 lines of code. In addition, it uses seven prompts, which altogether constitute about 2,000 tokens in two conversations.
All interactions are \textit{fully automated} via APIs of OpenAI. 
Besides sending prompts and waiting for responses, our agent also 1) interacts with LLMs according to the progressive prompt design, 2) locates function definitions within the Linux source code, and 3) processes responses from LLMs, then receives and stores to a database.

\vspace{3pt}
\noindent
\textbf{Hyper-Parameters.} There are several hyper-parameters in calling the APIs provided by OpenAI. We choose \texttt{max\_token} and \texttt{temperature} to 1,024 and 1.0, respectively. \texttt{max\_token} controls the output length; since LLMs always predict the next words by the previous output, the longer output can benefit and allow its reasoning. However, too many tokens will exhaust the context window quickly, so we pick 1024 as a reasonable balance.

The temperature controls the randomness and also the ability to reason. 
Intuitively, we want the analysis to be as non-random as possible and reduce the temperature (it can take a value between 0 and 2 for GPT models);
however, an overly low temperature can result in repetitive or overly simplistic responses. We set it to 1.0 (also the default of gpt-4-0613), which allows for higher-quality responses, and use strategies such as self-validation and majority voting to improve the consistency of responses.
%balance the need for diverse interpretations without sacrificing the stability and correctness of the analysis.

\cut{
\vspace{3pt}
\noindent
\textbf{Initializer Analysis.} 
% The prototype of \work focuses on providing a high-quality function summary of initializers to assist with UBITect. 
A UBI variable might not have an initializer and be used directly. If used locally, a straightforward intra-procedure analysis can figure it out. Otherwise, if the variable is passed to another function that is not an initializer, the framework of \work can still provide a high-quality summary of use. Considering that, in most cases, the summary of use provided by UBITect is sufficient, we focus on the most critical part that causes inaccuracies, the initializer, in our prototype.
% Our framework can also provide such summaries by design; however, few cases exist. We excluded these cases from our current prototype. In other words, we assume that UBITect is accurate in its summary of parameter usage and that there is no need for \work.
\yu{we should mention what we do in the implementation. not only why}
}





\cut{
\vspace{3pt}
\noindent
\textbf{Majority voting.}
We run each case 3 times and output the majority results (\ie \(\ge\)2 times).
Specifically, we run each case twice to see if any inconstancy and run a third time if any.
}



\cut{
\vspace{3pt}
\noindent
\textbf{Bug decision.} \work produces the use-guided postcondition which effectively classifies the suspicious variable to either \texttt{must\_init}, or \texttt{may\_init}, or \texttt{no\_init}.
We consider \texttt{no\_init} because \zhiyun{add some explanation.}
We employ a simple policy to decide whether the case is a bug: as long as the suspicious variable is classified as \texttt{may\_init} or \texttt{no\_init},
we consider it a bug.

}


\cut{
\vspace{3pt}
\noindent
\textbf{Self-validation.} 
Implementing self-validation in our design presents intriguing aspects with implications for the performance of LLMs. One might consider combining self-validation with the JSON generation step to expedite the process and reduce computational costs. However, this approach is only effective for simpler rules. For scenarios that demand the LLM to reassess its previous output and make adjustments, separating the self-validation process is critical.

self-validation, while mostly beneficial in enhancing the precision of LLMs, needs meticulous design and execution, as it can be a double-edged sword. At times, it can inadvertently detract from the accuracy of the result. Therefore, maintaining consistency with previous prompts and carefully examining the scope and context of self-validation is essential to leverage its benefits without compromising the overall outcome. The balance between precision enhancement and result stability is the key to unlocking the full potential of self-validation in LLM-based static analysis.
}


% \textbf{Variable Alias.} \haonan{Similar to the last one ....}

% \textbf{Incorrect Initializer Extract.} The identifying of initializer and the collecting postcondition might fail, despite
% rarely. Most of the failures are due to the suspicious variable
% does not exist in the initial context.



% Analyzing these cases requires a combination of the possibilities of all operations on these variables.

% \subsection{Prompt Implementation}

% Limited by space, we put the detailed prompts with several real conversations on an anonymous page\footnote{\href{https://anonymous.4open.science/r/LLIFT-B2B5/conversation.md}{https://anonymous.4open.science/r/LLIFT-B2B5/conversation.md}}.

% \textbf{}
% \subsubsection{Initializer \& Postcondition Extraction.} The conversation for initializer and postcondition extraction is relatively simple.
% As Figure \ref{fig:wf} depicted before, we use the few-shots in-context learning to teach LLM how to extract postconditions. Additionally, 
% we incorporate some clarifications for confused cases, such as multiple postconditions, and then let LLM itself refine its response.


% \subsubsection{Initializer Behavior Summary.}

% \work covers the following substances in prompts:
% \begin{itemize}
% % \item[\ding{118}] Analyze function calls to assess parameter initialization.
% \item[\ding{118}] Incorporate function return value checks in the analysis.
% \item[\ding{118}] Ensure field-sensitive analysis, focusing on parameter fields.
% \item[\ding{118}] Request additional information (e.g., function
% definitions) if needed.
% \item[\ding{118}] Classify parameters/fields as \texttt{must\_init} or \texttt{may\_init} based
% on initialization conditions.
% \item[\ding{118}] Present analysis results in JSON format for easy integration.
% \end{itemize}

% For the return value check, we have demonstrated how it can benefit 
% the motivating example in Figure~\ref{fig:sscanf}, \ie \texttt{sscanf(...)>=4}.


% Limited by space, We present detailed prompt with several complete conversations with GPT-4 on an anonymous page\footnote{\href{https://anonymous.4open.science/r/LLift-Open/Cases/README.md}{https://anonymous.4open.science/r/LLift-Open/Cases/README.md}}.
%we will demonstrate all conversations containing our experiments after the paper gets accepted.
%\haonan{finish the end-to-end conversation}
% Although adding the prompt with more detailed explanations or examples (\ie few-shot) might be
% viable, we opted for this differentiation to enable potential future use. \zhiyun{I don't really follow the last sentence.}
