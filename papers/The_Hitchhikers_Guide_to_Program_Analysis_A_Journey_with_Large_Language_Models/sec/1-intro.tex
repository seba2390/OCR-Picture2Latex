\section{Introduction}

% I. Background on challenges when applying program analysis in real world program, \eg Linux kernel.
% \begin{itemize}
%   \item The role of domain knowledge and heuristics
%   \item The need for an alternative approach
% \end{itemize}

Static analysis is a popular technique in software engineering, particularly in the area of bug discovery, that can improve code quality, reliability, and security.
% Text:
% Static program analysis techniques, such as static analysis and  symbolic execution, play a critical role in the field of software engineering, particularly in bug detection and prevention. 
% These techniques are essential for preemptively identifying and rectifying software errors, thereby improving code quality and reliability. 
However, the effectiveness of these techniques is influenced by the fundamental trade-off between precision and scalability, especially when dealing with extensive and complex programs~\cite{DBLP:journals/csur/ParkLR22, gosain_static_2015}.
On the one hand, static analysis solutions with lower precision tend to generate numerous false positives. 
On the other hand, expensive static analysis or symbolic execution solutions with higher precision often struggle to complete the analysis. 
Consequently, achieving comprehensive and accurate static program analysis for sizable programs like the Linux kernel poses a significant challenge.



\cut{
However, despite its numerous advantages, static analysis is not without its limitations. As we delve deeper into the intricate dynamics of large-scale software systems, these limitations start to surface and can often impede the precise and comprehensive analysis of the system. In this paper, we discuss three primary constraints encountered in the realm of static analysis: \textit{inherent knowledge boundaries}, \textit{exhaustive path exploration}, and \textit{rigidity in rule expansion and sensitivity adjustment}. Each of these challenges poses significant hurdles in our pursuit of a more refined and accurate static analysis, particularly in the context of vast and intricate systems like the Linux kernel.
}


UBITect \cite{ubitect}, a powerful static analysis solution illustrates these inherent limitations thoroughly. Targeting Use-Before-Initialization (UBI) bugs in the Linux kernel, it packages a pipeline of (1) a scalable bottom-up summary-based static analysis with limited precision, and (2) a precise symbolic execution with limited scalability.
The solution illuminates the need for alternative strategies to navigate the complex trade-offs between precision and scalability effectively.
%Employing a bottom-up, summary-based static analysis, UBITect strives to make the analysis scalable, albeit at the expense of precision. To counteract this lack of precision, it further incorporates static symbolic execution to check the feasibility of each identified case. 
Despite this strategic combination of analysis techniques, nearly 40\% of the potential bugs reported from the static analysis phase experience a timeout or memory exhaustion during the static symbolic execution phase, preventing any conclusive results on such cases. 
This limitation hinders the overall effectiveness of the tool, leading to the potential of two distinct outcomes: \textit{missed bugs} if these potential bug reports are ignored (what UBITect performs), or \textit{false positives} if they are sent to developers for inspection.



%This inability to arrive at determinate results in a substantial fraction of cases highlights the practical challenges faced by static analysis tools. By extension, it illuminates the need for alternative strategies to navigate the complex trade-offs between precision and scalability effectively.




%\haonan{I delete the paragraph about LLM's abilitiy, it is not so necessary. }
\cut{
Specifically, LLMs' ability to generate context-aware responses makes them well-suited for examining the true initialization status of suspicious variables. They can parse and comprehend a significant breadth of code, follow the flow of variables across different functions, and infer potential initialization scenarios.\zhiyun{is it really this powerful? Any citation? My impression is that when the program is complex (big), it starts to perform worse. Unclear whether people have tested its performance across many different functions.} This ability to discern whether a suspicious variable is indeed initialized or not promises a significant advancement over the inherent challenges posed by traditional static analysis methods.
\zhiyun{Overall, this paragraph doesn't explain well why LLM can perform better than static analysis and symbolic execution. 
Given the previous paragraph, should we explicitly talk about LLM's advantage in scalability, while at the same time preserving sufficient accuracy?}
}

\cut{
% \haonan{it is not trivial; for example, previous work only shows their abilities in simple cases; or "correct cases" (instead of corner cases, for example the buggy uninit cases, according to recent literature); we propose some new
% approaches and frameworks to solve it. }
However, effectively harnessing the capabilities of LLMs for this application is far from trivial.
The technique report of GPT-4 \cite{openai_2023_gpt_4} states that it is less effective than existing tools in terms of new vulnerability identification
Past research also demonstrates the competence of these models mostly in simple programs or simple tasks \cite{pei_can_2023, ahmed_improving_2023, pearce_examining_2023}.
% For example, n. \zhiyun{not an example for small programs. This should be the first point about people not believing LLM can do bug finding effectively by itself.}
% \haonan{GPT-4 techique report says it can discover bugs}
% 
% \haonan{make it more clean and straightforward}
% \zhiyun{should highlight this more. It will be nice if we can find some related work that says LLM works well for mostly small code snippets.}
% Recent literature also suggests that LLMs deal better with \textit{`correct'} (\ie not a bug) \zhiyun{unclear what ``correct'' means here. Do you mean to say ``common''?} \zhiyun{I remember seeing people trying ChatGPT on bug finding (e.g., OpenAI's own tech report). Is that still considered ``correct'' programs?} instances rather than challenging corner cases regarding code comprehension \zhiyun{this is still vague. I suggest we make it more concrete, e.g., summarize the kinds of corner cases or even give some examples like uncommon bug types?} \haonan{add more explanation}
Recent literature \cite{ma_scope_2023, tian_is_2023} also suggests that the \textit{hallucination} leads to LLMs fabricating non-existent facts in its reasoning.
% \yu{would you give example of this for program analysis?}
For example, facing to wrong code with bugs, LLMs usually reason the original intention of them rather than pointing out the bug \cite{tian_is_2023}.
To make things worse, due to their natural stochasticity, LLMs may produce incorrect or inconsistent results \cite{zhao2023survey}.
% \yu{couold you make it more clear? in which situation? or generally?}
Additionally, it is known that LLMs have limited context windows, so they can only analyze a limited codebase.
% \yu{this reason is not the same aspect as others. so change the position.}
}



In this paper, we investigate the possibility of leveraging \acfp{LLM} 
as an alternative to handle such ``difficult cases''. This is because recent LLMs have exhibited strong potential in understanding, generating, and even debugging code ~\cite{github_github_nodate, langchain_2023_announcing_2023, codex}. 
Nevertheless, navigating the intricacies of utilizing LLMs for bug discovery proves to be a complex feat.
The technical report on GPT-4 underscores this challenge, admitting that when it comes to discovering new vulnerabilities, it may not be the best solution standalone \cite{openai_2023_gpt_4}: ``... is less effective than existing tools for complex and high-level activities like
novel vulnerability identification''.
In the same vein, prior research demonstrates the competence of LLMs mostly in simpler tasks or programs~\cite{pei_can_2023, ahmed_improving_2023, pearce_examining_2023}.
This is because LLMs are far from perfect. For instance, they suffer from \textit{hallucination}~\cite{ji_survey_2023} where instead of identifying the bugs in faulty code, LLMs may create non-existent facts in an attempt to rationalize the original intention behind the problematic code \cite{ma_scope_2023, tian_is_2023}. 
% This tendency to erroneously ascribe intention can make the bug-dection process far more challenging.
Another issue is the stochasticity of LLMs which can result in inconsistent or outright incorrect results, thus throwing another wrench into the gears of bug discovery~\cite{zhao2023survey}. Finally, LLMs have limited context windows, meaning they can only scrutinize a relatively small codebase.

In response, we propose \work, a fully automated framework that bridges static analysis with LLMs in analyzing UBI bugs.
Our solution packages several novel components.
% First, \work offloads a critical and self-contained  \haonan{what is self-contained?}
% analysis to LLMs, namely, 
First, \work performs \textit{post-constraint guided path analysis}, which helps verify the path feasibility of the ``use'' of an initialized variable, a difficult task for static analysis and symbolic execution.
Second, to efficiently interact with LLMs, we employ \textit{task decomposition} to break down the analysis into more than a single step. 
%at first and make the problem manageable in each single \textit{conversation} (\ie self-contained session of prompts and responses) with LLMs. 
Third, we employ \textit{progressive prompting} by providing information incrementally only when necessary, instead of providing an enormous scope of code at once.
Finally, we propose \textit{self-validation} by requesting LLMs to review responses at various stages to obtain accurate and reliable responses. 
% \yu{A small comment: the relationship of those four points are not in order, i.e., first, second, third, finally.}
% \zhiyun{``Task decomposition now makes more sense.}
% \yu{Please be consistent with the design section in the following}
%\zhiyun{The few techniques we mention here don't seem to connect to the challenges. We can make it flow better by explaining the challenges in the previous graph more clearly. For instance, if we say LLM cannot handle large programs, we can then say here we break down the bigger program into much smaller pieces so that it is more manageable. The self-validation and postcondition-aware analysis came out of nowhere. If we want to mention them, we need to explain what problems they address first.}
% Our endeavor aims not just to adapt LLMs to the task at hand, but to fundamentally transform their application to static code analysis.

We implement a prototype of \work and test it in real-world scenarios. 
Focusing on the inconclusive cases of UBITect caused by time or memory limitation, \work successfully identifies 13 previously unknown UBI bugs in the Linux kernel that we confirmed with the Linux community. 
With 26 positive reports out of nearly 1,000 cases, \work reaches a high precision of 50\%. 
We also test \work against all previously known bugs found by UBITect, and observe a recall of 100\%.
\cut{
At the same time, it is highly effective in pruning false positives without requiring heavy computation.
\yu{need some evidence}
}
%The results demonstrate the effectiveness and efficiency of \work in practice.



% In the domain of \ac{UBI} detection, conventional techniques typically utilize
% conservative strategies, which lead to imprecise function summaries and elevated
% false alarm rates. A prevalent method for static analysis involves constructing
% function summaries through a bottom-up analysis, thereby facilitating the
% accurate assessment of program behavior. Unfortunately, the use of conservative
% strategies in \ac{UBI} detection often results in imprecise function summaries
% and high false alarm rates.
% \zhiyun{Yang Liu's work say LLM cannot do static analysis very well, e.g., code summary. Argue against it.}
% This paper explores the possibility of employing \acp{LLM}, such as ChatGPT
% \cite{chatgpt}, as versatile and comprehensive aids to static analysis.

% Specifically, ChatGPT even shows a capability in understanding programming language and semantics~\cite{bubeck_sparks_gpt4_2023, ma_scope_2023}
% and we conjecture that it can generate function summaries with greater precision than those computed by static analysis (and even symbolic execution), particularly in the presence of loops and operations on variable-length data structures (e.g., \texttt{strlen()})
% These precise function summaries serve as the foundation for more effective analysis that reduces false positives and negatives.

% We design an automated, progressive methodology that harnesses ChatGPT to
% produce accurate, precise, and structured function summaries.
% We assess our approach on both false positives and false negatives attributed to imprecise function summaries
% reported by a real-world static analysis tool called UBITect~\cite{ubitect}, using ChatGPT. 
%Our findings reveal that the generated summaries from GPT-3.5 are not
%consistently accurate and precise, resulting in the pruning of only half the
%false positives. Remarkably, GPT-4 achieves both accurate and precise function
%summaries for all instances, successfully pruning 100\% of the false positives.
% Interestingly, asking an end-to-end question to ChatGPT regarding whether a bug report is a false positive or a false negative is not effective.
% \haonan{...}
% In contrast, asking more targeted and refined questions about the function behaviors is much more effective.
% An additional advantage of utilizing ChatGPT is that function summaries are generated
% within a predictable time frame (unlike static analysis). 
% \yu{this is not accurate and we do not have any evidence.}
%This study underscores the potential of LLMs in augmenting the effectiveness and efficiency of static analysis.

% \zhiyun{Typically, before we introduce contributions, we will have a short preview of the results.}

We summarize our contributions as follows: 
% \zhiyun{I am not sure about ICSE but in security conferences, we typically list only three most important contributions (four at most). One example way to combine them is (1) new idea. (2) novel design and tool (open source). (3) achieved good results. } \haonan{Ok, remove some}
\squishlist
\item \textbf{New Opportunities.} We introduce a novel approach to static analysis that enhances its precision and scalability at the same time by harnessing the capabilities of LLMs. %effectively reducing false positives and negatives. 
To the best of our knowledge, we are the first to use LLMs to assist static analysis in bug-finding tasks with large-scale and real-world datasets.
\item \textbf{New Methodologies.} We develop \work, an innovative and fully automated framework that arms static analysis with LLMs. \work employs several prompt strategies to engage with LLMs, eliciting accurate and reliable responses.
\item \textbf{Results.} We rigorously investigate \work by conducting an in-depth analysis of nearly 1000 cases, resulting in a reasonable precision rate (50\%). Additionally, our examination led to the discovery of 13 previously unknown bugs.
\item \textbf{Open source.} Committed to open research, we will publicly release all of our code and data, fostering further exploration of the new space of LLM-assisted program analysis.
\squishend

% II. Large Language Models in Program Analysis

% \begin{itemize}
%   \item Overview of large language models (e.g., GPT)
%   \item Potential usecases in program analysis
% \end{itemize}

% III. Function summary of UBI problems

\cut{
\section{Background}
\yizhuo{To many small bullet points,  makes the paper look uncoordinated.}
% \begin{itemize}
%   \item codemasa: utlizing LLM to do mannual efforts, but in other fileds
%   \item Learning to Explore Paths for Symbolic Execution: AI model to accelerate symbolic execution.
%   % \item UBITect, and a motivating example
% \end{itemize}

%\squishlist

\vspace{3pt}
\noindent \textbf{\acf{LLM}}. 


\noindent \textbf{\ac*{LLM} for Software Engineering}. 
% \yu{Debugging is strange here, how about \ac*{LLM} for Software Engineering?}\zhiyun{I agree. I thought I used software engineering before.}
% for example codemosa, "Examining
% Zero-Shot Vulnerability Repair with Large Language Models", and, "Keep the
% Conversation Going" and "Teaching Large Language Models to Self-Debug".
Xia \etal \cite{xia_keep_2023} propose an automated conversation-driven
program repair tool using ChatGPT, achieving nearly 50\% success rate. 
% However, their
% experimental dataset might be already been learned by ChatGPT
% during training. \yizhuo{
% However, the evaluation dataset they use predates the release of ChatGPT, indicating that it may have already been incorporated into the ChatGPT's learning during the training phase.  \yizhuo{Or cite the ChatGPT's training data.} 
Pearce \etal \cite{pearce_examining_2023} examine zero-shot vulnerability
repair using LLMs and found promise in synthetic and hand-crafted scenarios but
faced challenges in real-world examples.
% Chen \etal \cite{chen_teaching_2023} teach LLMs to debug its own predicted program to increase its correctness, but only
% performs on relatively simple programs.
% \zhiyun{``predicted programs''? I can't understand.} 
% achieving
% state-of-the-art performance on various benchmarks. 
% These two studies employ LLM to directly provide end-to-end solutions with limited success. In contrast, 
Lemieux \etal \cite{lemieux_codamosa_2023} leverages LLM to generate
tests for uncovered functions when search-based approach got coverage stalled. 
% we investigate how to break down complex problems
% into smaller ones that are more tangible for LLM, which in turn help achieve better end-to-end results.
In this paper, we explore how LLM can be used as an alternative to achieve better results when static analysis encounters difficulties.

}
% \yu{should we mention what is the difference between our method and chain of thought (CoT) here?}
%more carefully fine-grained specific scenarios such as function summary
%to facilitate static analysis.
% analysis, to get a better performance.



% In our work, we collect these discarded cases to test if with much more
% precise function summary from ChatGPT, can we successfully detect these cases
% are false alarms or real bugs?

% We argue that this imprecision originates from the essential design choice of
% UBITect: it analyzes each function conservatively to avoid neglecting any UBI
% possibilities. 
% Hence, the imprecise function summary is inevitable to ensure the soundness.
% Fortunately, \acp{LLM} enable us to achieve greater precision without sacrificing
% soundness. 
%\squishend

