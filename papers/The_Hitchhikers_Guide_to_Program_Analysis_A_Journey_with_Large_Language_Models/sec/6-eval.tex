
\begin{table*}[]
    \centering
    % \caption{Found Use-Before-Initialization (TP) From Time/Memory out Cases \haonan{rename...} \zhiyun{should mention the kernel version otherwise the line number is useless.}}
    \caption{True bugs identified by \work from Random-1000, analyzing in Linux v4.14}
    \label{tab:table_rq1}
\scalebox{0.9}{
\begin{tabular}{llllll}
\toprule
\textbf{Initializer}               & \textbf{Caller}                            & \textbf{File Path }                                      & \textbf{Variable}          & \textbf{Line} \\
\midrule
read\_reg                & get\_signal\_parameters              & drivers/media/dvb-frontends/stv0910.c      & tmp                & 504                                 \\
regmap\_read             & isc\_update\_profile                 & drivers/media/platform/atmel/atmel-isc.c   & sr                 & 664                                 \\
ep0\_read\_setup         & ep0\_handle\_setup                   & drivers/usb/mtu3/mtu3\_gadget\_ep0.c       & setup.bRequestType & 637                                 \\
regmap\_read             & mdio\_sc\_cfg\_reg\_write            & drivers/net/ethernet/hisilicon/hns\_mdio.c & reg\_value         & 169                                 \\
bcm3510\_do\_hab\_cmd    & bcm3510\_check\_firmware\_version    & drivers/media/dvb-frontends/bcm3510.c      & ver.demod\_version & 666                                 \\
readCapabilityRid        & airo\_get\_range                     & drivers/net/wireless/cisco/airo.c          & cap\_rid.softCap   & 6936                                \\
e1e\_rphy                & \_\_e1000\_resume                    & drivers/net/ethernet/intel/e1000e/netdev.c & phy\_data          & 6580                                \\
pci\_read\_config\_dword & adm8211\_probe                       & drivers/net/wireless/admtek/adm8211.c      & reg                & 1814                                \\
lan78xx\_read\_reg       & lan78xx\_write\_raw\_otp             & drivers/net/usb/lan78xx.c                  & buf                & 873                                 \\
t1\_tpi\_read            & my3126\_phy\_reset                   & drivers/net/ethernet/chelsio/cxgb/my3126.c & val                & 193                                 \\
pci\_read\_config\_dword & quirk\_intel\_purley\_xeon\_ras\_cap & arch/x86/kernel/quirks.c                   & capid0             & 562                                 \\
ata\_timing\_compute     & opti82c46x\_set\_piomode             & drivers/ata/pata\_legacy.c                 & \&tp               & 564                                 \\
pt\_completion           & pt\_req\_sense                       & drivers/block/paride/pt.c                  & buf                & 368                                \\
\bottomrule
\end{tabular}
}

\end{table*}

\section{Evaluation}
\label{sec:eval}

Our evaluation aims to address the following research questions.

% \begin{itemize}
\squishlist
\item \textbf{RQ1 (Precision):} How accurately is \work able to identify bugs?
\item \textbf{RQ2 (Recall):} Is there a possibility for \work to miss real bugs?
\item \textbf{RQ3 (Comparison):} How does the performance of individual components within \work compare to that of the final design?
\item \textbf{RQ4 (Model Versatility):} How does \work perform when applied to LLMs other than GPT-4?
% How do our design principles contribute to the performance of \work?
% Compared to other design approaches and LLMs, how superior is \work?
% How does \work perform compared to other design approaches and different LLMs?
\squishend

% \end{itemize}
\vspace{3pt}
\noindent

We evaluate RQ1 to RQ3 in GPT-4, under API from OpenAI with version gpt4-0613.
For RQ4, we also test GPT-3.5 with version gpt-3.5-turbo-0613 and Claude 2 additionally for comparison.
% Claude 2 was run manually on its webpage.



\subsection{Dataset}
% The data incorporated in our research was harvested from the output generated by UBITect. To be specified, all the considered cases were derived from UBITect's positive static analysis results that resulted in either a timeout or exhaustion of memory during the symbolic execution. 
% % These situations imply an inconclusive verdict from UBITect on the respective cases, which could potentially contribute to false negatives or overlooked flaws in the system. This is where the utilization of \work presents an alternative avenue to investigate these cases.

% The static analysis of UBITect produces 140k positives, and symbolic execution can only handle 60\% of them and left 
% 53k cases. Due to the capacity limit of OpenAI (\$600 per month when we do the experiment), 
% we cannot run all of them 
% before our submission.

Our experiment data, sourced from UBITect, includes all potential bugs labeled by its static analysis stage but experienced timeout or memory exhaustion during its symbolic execution stage. 
Overall, UBITect's static analysis stage produced 140,000 potential bugs, with symbolic execution able to process only 60\%, leaving 53,000 cases unattended, which means that these cases are generally difficult for static analysis or symbolic execution to decide
We craft the following dataset from 53,000 cases to evaluate \work:



~\noindent(1) \textbf{Random-1000.} We randomly chose 1,000 from the 53,000 cases for testing. However, there are 182 cases where there are no initializers, which are automatically recognized and filtered (see \S\ref{sec:problem}). The remaining 818 cases are used in evaluating precision, \ie the ratio of true positives to false positives. 

~\noindent(2) \textbf{Bug-50.} This dataset comprises the 52 confirmed UBI bugs previously identified by UBITect. It is used as ground truth for assessing recall by verifying if any true bugs were overlooked.

~\noindent(3) \textbf{Cmp-40.} This dataset comprises 27 negative and 13 positive cases selected from the Random-1000. We utilize this dataset to illustrate which of our design strategies contributed most to the outcome of our solution.
% \yu{still random selected? We should mention it. And why 27 and 13?}



% \subsection{Basic Statistics}
% Execution time is not our focus. \zhiyun{This is a bad phrase to use. I would just remove it.}
\PP{Turns and Conversations.}
Due to the progressive prompt, each case may require different turns (pairs of a prompt and a response). In Random-1000, the average number of turns is 2.78, with a max of 8 and a variance of 1.20. 

% \PP{Time.}
% Due to the progressive prompt, the execution time of each case varies. In Random-1000, the average number of turns (\ie pairs of a prompt and a response) is 2.78, with a max of 8 and a variance of 1.20. 
% Each turn typically takes a few seconds. Note that the exact time for each turn of prompt and response varies depending on the  workload on the OpenAI infrastructure 
% % (sometimes we need to retry due to API failures). 
% Due to the request rate limit of the API, we have to lower the frequency of requests. It takes roughly one day (24 hours) to run through the 1,000 cases. 
% We are unable to do more tests due to the monthly cap.
% \yu{we have explained monthly cap before.}


%\zhiyun{We should translate the number of turns into execution time. Some turns can take longer than others, e.g., depending on how many words there are.}
%\haonan{single executing time is be meaningless, it highly depends on which time you request openai. besides, OpenAI keeps trying to make it more fast}
% \haonan{to reduce RPM, the latency is close to real execution time}


% \zhiyun{If it takes only 24 hours to finish 941 cases, why don't we run more cases? We claim earlier we don't run more because of rate limit. But it doesn't seem to be the true reason?} \haonan{mostly capacity limit, previously only \$600. } \zhiyun{we should state this then.} \haonan{stated before the enumerate}

% \PP{Cost.} GPT-4 currently costs \$0.06 for outputting each 1k tokens. On average, it costs \$0.43 to analyze each potential bug in random-1000. The total budget is \(\sim\)\$430. Therefore, readers may not faithfully reproduce all
% of our experiments if they hate OpenAI (Maybe they love Google and NVIDIA and are happy to spend much more on GPU computation) and refuse to send money to them. Nevertheless, it is feasible to choose samples from our open-source dataset to experiment with.

\PP{Cost.} On average, it costs 7,000 tokens in GPT-4 to analyze each potential bug.
% Also, we spent about 50 human hours inspecting all results and obtain the ground truth.


% \PP{Token Limitation.} \haonan{...} 4 of 


% \PP{Excluded Cases.} 182 cases are excluded due to they are out of our scope as described in Figure \ref{fig:prob_scope} (\ie no initializer function to analyze), and 4 cases exceed the maximum context length while exploring deeper functions in the progressive prompt.

\cut{
\begin{figure}
\begin{minted}[xleftmargin=10pt, linenos, fontsize=\footnotesize]{c}
int ath10k_pci_diag_write_mem(..., int nbytes){
 while (ath10k_ce_completed_recv_next_nolock(..., 
           &completed_nbytes) != 0) {
    mdelay(1);
    if (i++ > DIAG_ACCESS_CE_TIMEOUT_MS) {
      ret = -EBUSY;
      goto done;
    }
  }
  //use of "completed_nbytes"
  ...

}
int ath10k_ce_completed_recv_next_nolock(..., unsigned int *nbytesp){
 ...
 nbytes = __le16_to_cpu(sdesc.nbytes);
 if (nbytes == 0) {
   return -EIO;
 }
 desc->nbytes = 0; 
 /* Return data from completed destination descriptor */
 *nbytesp = nbytes;
   ...
 return 0;
}
\end{minted}
\caption{Code snippet of \texttt{ath10k\_...\_nolock} and its usecase, derived from \texttt{net/wireless/ath/ath10k/ce.c} }
\label{fig:case_while}  
\end{figure}
}

\subsection{RQ1: Precision}
\label{subsec:expr_precision}

% We consider two things for RQ1:

% \begin{enumerate}
%     \item For positive cases, how precise is \work?
%     \item Considering both positive and negative cases, how accurate is \work?
% \end{enumerate}

\work reports 26 positives among the Random-1000 dataset, where half of them are true bugs based on our manual inspection. This represents a precision of 50\%.
% Table \ref{tab:table_rq1} shows the 13 new UBI cases. 
% Interestingly, even though we analyzed the Linux kernel v4.14, which is the version UBITect was capable of supporting, 
In keeping with UBITect and we focus on the analysis of Linux v4.14,  12 of the bugs still exist in the latest Linux kernel.
% We submit the patch of these 13 vulnerabilities to the Linux community.
We are in the process of reporting the 12 bugs to the Linux community.
So far, we have submitted patches for 4 bugs and received confirmation that they are true bugs.
%\zhiyune{with TODO bugs being patched already}. \zhiyun{give a forward reference about case studies.}

% \noindent
% \textbf{Consistency.} LLM is known for its randomness and inconsistency. 

\vspace{3pt}
\noindent
\textbf{Imprecise and Failed Cases.} 
 Despite the effectiveness of \work, there are instances where it does not yield precise results, resulting in 13 false positives by mistakenly classifying \texttt{must\_init} cases as \texttt{may\_init}. Upon a careful examination of these cases, we attribute the imprecision to a variety of factors, which we discuss in detail in \S\ref{subsec:excuse}.
 Briefly, we give a breakdown of them here: \textit{Incomplete constraint extraction} (4 cases), \textit{Information gaps in UBITect} (5 cases), \textit{Variable reuse} (1 case), \textit{Indirect call} (1 case), and \textit{Additional constraints} (1 case). Additionally, there is one false positive caused by inconsistent output (i.e., two false positives in three runs). 
 Four cases exceed the maximum context length while exploring deeper functions in the progressive prompt. %\haonan{failed cases}






\find{
\textbf{Takeaway 1.} \work Can effectively summarize initializer behavior 
and discover new bugs with high precision (50\%).
}


\cut{
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/con_sr.pdf}
    \caption{self-validation of analyzing case \texttt{axi\_clkgen\_recalc\_rate}, it concludes an incorrect answer
    \texttt{must\_init} at first, and then corrects it to \texttt{may\_init} after the self-validation.}
    \label{fig:con_sr}
\end{figure}
}
  

\subsection{RQ2: Recall Estimate}
\label{subsec:expr_recall}

Conceptually, the core optimization (post-constraint guided path analysis) of \work is sound, 
and we also prompt a series of rules to let LLMs tend to respond ``\texttt{may\_init} when uncertain.
We expect \work would not reject true bugs or with a high recall.

We sample 300 negative cases
from Random-1000 in an effort to see whether we will miss any true bugs. 
We confirm that all are true negatives. Despite the limited data sampled, this result indicates that integrating GPT-4 into our implementation does not introduce apparent unsoundness. 

Further, we test \work on the Bug-50 dataset to see \textit{whether it will miss any bugs discovered by UBITect}.
\work has demonstrated full effectiveness in identifying all real bugs from Bug-50. This result, while encouraging, does not imply that \work is flawless. Detailed data analysis reveals that:
1) There remain some inconsistencies in 3\(\sim\)5 cases
% per run \zhiyun{what is the round? each round consists of 3 runs?} \haonan{per run} \zhiyun{how can a single run experience inconsistencies? it doesn't make sense.} 
occasionally, though they are mitigated by majority voting; and 2) 
all the bugs found by UBITect have trivial post-constraints \((\mathcal{C}_{post}=\top\)) and postcondition of \textit{may\_init} (\(\mathcal{P}_{qual}: \texttt{must\_init} \mapsto \emptyset\)). 
Hence, \work could identify them easily.
It is noteworthy that these cases are already those cases detectable by UBITect. 
Such cases tend to be simpler in nature and can be verified by symbolic execution in UBITect.

% In addition to the Bug-50 dataset, 

% Actually, \(\sim\)40 of Bug-50 cases are unchecked use of \texttt{regmap\_read}.



\cut{
\haonan{3 inconsistent, majority voting can fix}
\vspace{3pt}
\noindent
\textbf{Consistency.} The instability of LLM is well known; however, the results of \work are quite stable. We ran each case in Bug-50 more than three times for each example and obtained consistent outcomes for most cases (48 of 54 are consistent).
}


\cut{
By checking the detailed conversation. We notice our self-validation design contributes to consistency. As Figure \ref{fig:con_sr} demonstrates, when analyzing \text{axi\_clkgen\_recalc\_rate(1)}, it generates an incorrect answer at first by saying \texttt{must\_init}. Such errors are inherently random; fortunately, self-reflection can spot errors in the next dialog and deliver a correct answer at the end.
\zhiyun{This section does not follow the same structure as the previous one. In the previous section, we talk about the incorrect cases first and then explain it is the self-validation that helps with the correct cases the most. Better be consistent.}
\zhiyun{BTW, when I read the evaluation, I still feel the true negative results are useful to mention somewhere. Both RQ1 and RQ2 are about true bugs only.}
}

\cut{
\begin{figure}[t]
  \begin{minted}[xleftmargin=10pt, linenos, fontsize=\footnotesize]{c}
int regmap_read(struct regmap *map, ...)
{
 int ret;
 if (!IS_ALIGNED(reg, map->reg_stride))
 	return -EINVAL;
 map->lock(map->lock_arg);
 ret = _regmap_read(map, reg, val);
 map->unlock(map->lock_arg);
 return ret;
}
\end{minted}
\caption{Definition of \texttt{regmap\_read}, from \texttt{drivers/base/regmap/regmap.c}}
\label{fig:regmap_read}  
\end{figure}
}

\cut{
\vspace{3pt}
\noindent
\textbf{Incorrect Cases.}  The few inconsistencies or incorrectness are caused by missing checks using the \texttt{regmap\_read} function. This is an interesting case,  as shown in Figure \ref{fig:regmap_read};
 this function can fail, e.g., with not aligned registers. Besides, the
further call of \texttt{\_regmap\_read} might either fail or return an error code.
However, failure in reading registers is rare in practice, and therefore it is commonly used in 
Linux kernel directly uses the function without a return value check. Even
 some Linux maintainers don't think they are real bugs (such as \texttt{stm32\_timer\_stop(2)}
 and \texttt{gemini\_clk\_probe}~\cite{boyd_re_2019, cameron_re_2019}. 
 }
 
\cut{
% because according to our definition it should be classified as \texttt{may\_init}, but in reality it is triggered very rarely.
Therefore, GPT-4 sometimes delivers the \texttt{must\_init} result after the self-validation --- based on the knowledge it has trained.
A thorough solution to such problems requires cleaner training data, fine-tuning, or more runs to eliminate the inconstancy. In practice, 
running each case three times and then outputting the results with more occurrences can get the correct results.
}

% \vspace{3pt}
% \noindent
% \textbf{regmap\_read}. 



 % It is not clear to us at this time how GPT-4 is trained to obtain a priori
 % knowledge of whether this function initializes its parameter. If its knowledge
 % comes from extensive ``code feature learning" from well-maintained open source
 % code, then the misclassification of \texttt{regmap\_read} is understandable.



\find{
\textbf{Takeaway 2.} \work has proven effective in identifying UBI bugs, consistently detecting all known instances.
}





\begin{table}[t]
\centering
\caption{Performance evaluation of bug detection tool with progressive addition of design components: Post-Constraint Guided Path Analysis (PCA), Progressive Prompt (PP), Self-Validation (SV), and Task Decomposition (TD). (C) indicates the number of
\textit{C}onsistent cases.}
\scalebox{0.78}{
\begin{tabular}{@{}lcc|cccc@{}}
\toprule
\textbf{Combination} & \textbf{TN(C)} & \textbf{TP(C)} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} & \textbf{F1 Score} \\ \midrule
Simple Prompt & 12(9) & 2(1) & 0.12& 0.15& 0.35& 0.13 \\
PCA & 13(9) & 5(1) & 0.26 &	0.38 & 0.45 &	0.31 \\
PCA+PP & 5(3) & 6(1) & 0.21& 0.46& 0.28& 0.29 \\
% \rowcolor{gray!15}
PCA+PP+SV & 5(2) & 11(8) & 0.33& 0.85& 0.40& 0.48 \\
PCA+PP+TD & 22(14) & 6(4) & 0.55& 0.46& 0.70& 0.50 \\ 
% \hline
% \rowcolor{gray!15}
{PCA+PP+SV+TD} & 25(17) & 13(12) & 0.87& 1.00& 0.95& 0.93 \\ \midrule
\textit{Oracle} & 27(27) & 13(13) & - & - & - & - \\
\bottomrule
\end{tabular}
}
\label{tab:compare}
% \vspace{-10pt}
\end{table}

\cut{
\begin{figure}
    \centering
\begin{minted}[xleftmargin=10pt, linenos, breaklines, escapeinside=@@, fontsize=\footnotesize]{c}
void irlan_close_data_channel(struct irlan_cb *self)
{
  ...
  skb = alloc_skb(IRLAN_MAX_HEADER + IRLAN_CMD_HEADER +
      IRLAN_BYTE_PARAMETER_LEN("DATA_CHAN"),
      GFP_ATOMIC);
  if (!skb)
    return;
  ...

  frame = skb->data;
  ...
}
\end{minted}
    \caption{Code snippet of \texttt{irlan\_close\_data\_channel}, with
    suspicious varaible \texttt{frame(skb->data)}}
    \label{fig:case_frame}
\end{figure}
}

\cut{
\begin{table}[]
\centering
\caption{Abstract of prompt-response behavior in analyzing function \texttt{irlan\_close\_data\_channel} of design combination PCA + PP + SV (\ie without task decomposition).}
\scalebox{0.8}{
\begin{tabular}{p{1.1cm}|p{8.5cm}}
\toprule
\textbf{Prompt} & \textbf{Response} \\ 
\midrule
\textbf{Starting} & The analysis begins with the given function \texttt{alloc\_skb} which calls \texttt{\_\_alloc\_skb}. \texttt{\{ "ret": "need\_more\_info",  "response": [ \{    "type": "function\_def",    "name": "\_\_alloc\_skb" \} ]\}} \\
\hline
\textbf{PP} & Analyzing the \texttt{\_\_alloc\_skb} function, it's seen that \texttt{skb->data} is initialized after successful allocation of \texttt{skb}. If \texttt{skb} is successfully allocated, \texttt{frame} (which is assigned \texttt{skb->data}) is guaranteed to be initialized. \textbf{\textit{Thus, \texttt{frame} should be classified as `must\_init'}}. \\
\hline
\textbf{SV} & 1) The function \texttt{\_\_alloc\_skb} returns \texttt{skb}. If memory allocation fails, \texttt{skb} is \texttt{NULL}. \newline 2) \texttt{skb} is checked after the \texttt{alloc\_skb} call. \textbf{\textit{If \texttt{skb} is \texttt{NULL}, the function returns, leading to a `may\_init' path for \texttt{frame}.}} \newline 3) If \texttt{skb} is not \texttt{NULL}, \texttt{frame} is assigned \texttt{skb->data}, leading to a 'must\_init' path for \texttt{frame}. \newline 4) As both 'may\_init' and 'must\_init' paths exist, \texttt{frame} is considered as 'may\_init' under some conditions, but not all. \\
\bottomrule
\end{tabular}
}
\label{tab:case_frame}
\end{table}
\vspace{-15pt}
}




\subsection{RQ3: Contributions of Design Strategies}
\label{sec:comparison}

In our effort to delineate the contributions of distinct design strategies to the final results, we undertook an evaluative exercise against the Cmp-40 dataset, employing varying configurations of our solution, each entailing a unique combination of our proposed strategies. As illustrated in Table \ref{tab:compare}, the strategies under consideration encompass Post-constraint Analysis (\textit{PCA}), Progressive Prompt (\textit{PP}), Self-Validation (\textit{SV}), and Task Decomposition (\textit{TD}). The findings underscore an overall trend of enhanced performance with the integration of additional design strategies.
% , highlighting their collective impact.
% \yu{should compare PCA+PP+SV+TD with PCA+PP+SV, PCA+PP+TD, PCA+SV+TD, PP+SV+TD.}

% \vspace{3pt}
% \noindent \textbf{Baseline.}
In this study, the \textit{Baseline} corresponds to a straightforward prompt, \textit{"check this code to determine if there are any UBI bugs"}, a strategy that has been found to be rather insufficient for discovering new vulnerabilities, as corroborated by past studies \cite{openai_2023_gpt_4, ma_scope_2023, tian_is_2023}, reflecting a modest recall rate of 0.15 and a precision of 0.12.

% the solution where we simply perform a single prompt, embedding the function definition and the suspicious variable, asking whether there is a UBI bug.
% We can see that its result is clearly the worst.

Incorporating PCA offers a notable enhancement, enabling the LLM to uncover a wider array of vulnerabilities. As shown in Table \ref{tab:compare}, there is a substantial improvement in recall in comparison to the baseline, an anticipated outcome considering PCA's pivotal role in our solution. However, solely relying on this strategy still leaves a lot of room for optimization.
% Adding PP further improves the precision without any negative impact on Recall. 



The influence of Progressive Prompt (\textit{PP}) on the results is quite intriguing. While its impact appears to lower precision initially, the introduction of task decomposition and self-validation in conjunction with PP reveals a substantial boost in performance. Without PP, the LLM is restricted to deducing the function behavior merely based on the function context's semantics without further code analysis. Even though this approach can be effective in a range of situations, it confines the reasoning ability to the information available in its training data. By checking the detailed
conversation, we notice the omission of TD or SV tends to result in the LLM neglecting the post-constraints, subsequently leading to errors.


Beyond influencing precision and recall, Task Decomposition (\textit{TD}) and Self-Validation (\textit{SV}) also play a crucial role in enhancing \textit{consistency}. In this context, a result is deemed \textit{consistent} if the LLM yields the same outcome across its initial two runs. A comparison between our comprehensive final design encompassing all components, and the designs lacking TD and SV, respectively, reveals that both TD and SV notably augment the number of consistent results, and deliver 17 and 23 consistent results in its negative and positive results, respectively, underscoring their importance in ensuring reliable and consistent outcomes.
% \zhiyun{if we have time, consider improving the consistency a bit further.}


Finally, TD also holds significance in terms of conserving tokens. 
% During our evaluation, we observed two instances in the PCA+PP and PCA+PP+SV configurations where the token limitations of GPT-4 were exceeded. 
In our evaluation phase, we identified two instances within the PCA+PP and PCA+PP+SV configurations where the token count surpassed the limitations set by GPT-4. However, this constraint was not breached in any case when TD was incorporated. 
%This reduction in token usage enables our solution to manage a more diverse array of functions in the Linux kernel, significantly expanding its practicality and usefulness.


\find{
\textbf{Takeaway 3.} All of \work's design strategies contributed to the positive results. 
}



\begin{table}[]
    \centering
    \caption{Comparison of different LLMs on real bugs, from a subset of Bug-50}
\scalebox{1.0}{
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Caller}}            & \multicolumn{2}{c}{\textbf{GPT}} & \multirow{2}{*}{\textbf{Claude2}}    & \multirow{2}{*}{\textbf{Bard}}   \\
                                    &   \textbf{4}  & \textbf{3.5} &  &  \\
% \textbf{Caller} & \textbf{GPT-4} & \textbf{PAC}  & \textbf{GPT-3.5} & \textbf{Claude2} & \textbf{Bard } \\
\midrule
  % \rowcolor{gray!40}
  % \multicolumn{4}{l}{\textit{\textbf{False Negatives of UBITect}}} \\ 
%\text{pv\_eoi\_get\_pending}       &  \ding{51} &  \ding{51} &  \ding{51}             \\
%\text{p9\_check\_errors}           &  \ding{51} &  \ding{51} &  \ding{51}             \\
  % \rowcolor{gray!40}
  % \multicolumn{4}{l}{\textit{\textbf{True Positives of UBITect}}} \\ 
\text{hpet\_msi\_resume}           &  \ding{51} &  \ding{51} &  \ding{51}  &  \ding{55}          \\
\text{ctrl\_cx2341x\_getv4lflags}  &  \ding{51} &  \ding{51} &  \ding{55}  &  \ding{55}          \\
\text{axi\_clkgen\_recalc\_rate}   &  \ding{51} &  \ding{51} &  \ding{51}  &  \ding{51}          \\
\text{max8907\_regulator\_probe}   &  \ding{51} &  \ding{51} &  \ding{51}  &  \ding{51}          \\
\text{ov5693\_detect}              &  \ding{51} &  \ding{51} &  \ding{55}  &  \ding{51}          \\
\text{iommu\_unmap\_page}          &  \ding{51} &  \ding{55} &  \ding{51}  &  \ding{55}          \\
\text{mt9m114\_detect}             &  \ding{51} &  \ding{51} &  \ding{51}  &  \ding{51}          \\
\text{ec\_read\_u8}                &  \ding{51} &  \ding{51} &  \ding{51}  &  \ding{51}          \\
\text{compress\_sliced\_buf}       &  \ding{51} &  \ding{51} &  \ding{55}  &  \ding{51}          \\
\bottomrule
\end{tabular}
}

\label{tab:res_cmp}
\end{table}


\subsection{RQ4: Alternative Models}
\label{subsec:expr_comp}

% \haonan{explain gpt-3.5}


Table \ref{tab:res_cmp} provides a comprehensive view of the performance of our solution, \work, when implemented across an array of LLMs including GPT-4.0, GPT-3.5, Claude 2 \cite{anthropic_2023_claude}, and Bard \cite{krawczyk_bards_2023}. GPT-4 passes all tests, while GPT-3.5, Claude 2, and Bard exhibit recall rates of 89\%, 67\%, and 67\%, respectively. Despite the unparalleled performance of GPT-4, the other LLMs still produce substantial and competitive results, thereby indicating the wide applicability of our approaches.

It is imperative to note that not all design strategies in our toolbox are universally applicable across all language models. Bard and GPT-3.5, in particular, exhibit limited adaptability towards the progressive prompt and task decomposition strategies. Bard's interaction patterns suggest a preference for immediate response generation, leveraging its internal knowledge base rather than requesting additional function definitions, thereby hindering the effectiveness of the progressive prompt approach. Similarly, when task decomposition is implemented, these models often misinterpret or inaccurately collect post-constraints, subsequently compromising the results. To harness their maximum potential, we only apply the PCA design specifically (\ie without other design strategies) for GPT-3.5 and Bard.
% \yizhuo{Modify to we applied the PCA to them as that's the only thing we can do.}
% \zhiyun{this paragraph can be made more clear. We did not say (1) with more design strategies, they perform even worse, and (2) why would the results be better with fewer strategies. Let me explain with a concrete confusion of mine: to me, with and without progressive prompt, it should not change the results much for GPT-3.5 and Bard. This is because anyway they will ignore the progressive prompt direction -- basically acting the same as if there is no progressive prompt.}
% \haonan{My guess is when we put too many things, GPT-3.5 pays less attention to the rules in our starting prompt (or, simply because GPT-3.5 suffers more survive hallucination); Bard directly ignores PP, so there is no effect.
% I don't explain it because these phenomena are not easy to describe}

Contrasting the GPT series, Bard and Claude 2 demonstrate less familiarity with the Linux kernel and are more prone to failures due to their unawareness of the \texttt{may\_init} possibility of initializers.

%we will show later in \S\ref{subsec:case_study}, \texttt{get\_user\_pages\_unlocked}. Claude 2 guesses according to the function and parameter name and 
%does not recognize its purpose. Bard gives a plausible but incorrect response.



\find{
\textbf{Takeaway 4.} 
GPT-4 remains at the pinnacle of performance for \work, yet other LLMs can achieve promising results. 
}

% \subsection{RQ4: Comparing to symbolic execution}


\begin{figure}
\hspace{-15pt}
\includegraphics{figures/minted/case_sgl_map.pdf}
    \caption{Case Study I (Loop and Index). Derived from \texttt{drivers/scsi/st.c} 
    }
    \label{fig:sgl_pages}
\end{figure}






\subsection{Case Study}
\label{subsec:case_study}
In this case study, we pick three interesting cases demonstrating the effectiveness of \work in analyzing function behaviors and detecting uninitialized variables. 
All these cases are undecided for the previous static analyzer, UBITect.
We put the complete conversations on an anonymous online page for reference\footnote{\href{https://sites.google.com/view/llift-open/case-studies}{https://sites.google.com/view/llift-open/case-studies}}.
% \zhiyun{Think about whether we want to do this, given the extra alignment we need.}


% \haonan{show more subtle cases, to impresee people how smart (much benefit of its knowledge) it is }


% \zhiyun{alias analysis automatically performed by ChatGPT.}

% LLM, especially for GPT-4, even shows its ability in non-trivial program analysis.

% \vspace{3pt}
% \noindent
% \textbf{Alias Analysis.} For different function call; and pointer assign

% \vspace{3pt}
% \noindent
% \textbf{Context Sensitivity and Path Sensitivity.}

\vspace{3pt}
\noindent
\textbf{Loop and Index.} Figure \ref{fig:sgl_pages} presents an intriguing case involving the variable \texttt{pages[j]}, which is reported by UBITect as used in Line 17 potentially without being initialized. Unfortunately, this case is a false positive which is hard to prune due to loops. Specifically, the initializer function \texttt{get\_user\_pages\_unlocked()}, which is responsible for mapping user space pages into the kernel space, initializes the \texttt{pages} array allocated in Line 3. If \texttt{get\_user\_pages\_unlocked()} is successfully executed, \texttt{pages[0]} through \texttt{pages[res-1]} pointers will be initialized to point to struct page instances. 
% \zhiyun{should emphasize this case is difficult for UBITect (timeout?)} 
% \haonan{done}
%\zhiyun{should mention \texttt{res} being the return value.}
%\haonan{should we show it in more detailed?}

To summarize the behavior, \ie \texttt{must\_init} facts under conditions where the use is reachable,
we must first extract the \textit{post-constraints} that lead to the use of \texttt{pages}. Through interacting with ChatGPT, \work successfully extracts it:
% \zhiyun{TODO: Haonan will copy the JSON result here.}
\begin{lstlisting}[numbers=none]
{
    "initializer": "res = get_user_pages_unlocked(uaddr, nr_pages, pages, rw == READ ? FOLL_WRITE : 0)",
    "suspicious": ["pages[j]"],
    "postconstraint": "res < nr_pages && res > 0 && j < res",
}
\end{lstlisting}

After feeding the post-constraints to LLM, \work then successfully obtains the result:
% \zhiyun{TODO: Haonan will copy the JSON result here}

\begin{lstlisting}[numbers=none]
{
    "ret": "success",
    "response": {
        "must_init": ["pages[j]"],
        "may_init": [],
    }
}
\end{lstlisting}

As we can see, GPT-4 exhibits impressive comprehension of this complex function. 
It perceives the variable \texttt{pages[j]} being used in a loop that iterates from \texttt{0} to \texttt{res-1}. This insight leads GPT-4 to correctly deduce that all elements in the \texttt{pages} array must be initialized, \ie they are \texttt{must\_init}. This example underscores GPT-4's proficiency in handling loop and even index sensitivity.

\begin{figure}
\hspace{-15pt}
\includegraphics{figures/minted/case_hv_pci.pdf}
    \caption{Case Study II (Concurrency and Indirect Call). Derived from \texttt{drivers/pci/host/pci-hyperv.c}
    }
    \label{fig:hv_pci}
\end{figure}



\vspace{3pt}
\noindent
\textbf{Concurrency and Callback.} Consider the case illustrated in Figure \ref{fig:hv_pci}. At first glance, UBITect flags Line 10 for potentially using the variable \texttt{comp\_pkt.completion\_status} before initialization. The function's body seemingly lacks any code that initializes it, leading UBITect to report it as a potential bug.  However, the mystery unravels when we examine \texttt{hv\_pci\_generic\_compl()}, the actual initializer function assigned to \texttt{pkt} in Line 4. The variable in question is indeed initialized, but intriguingly, its initializer emerges from a concurrent function instead of within its own thread. Here \texttt{wait\_for\_completion()} is a synchronization primitive that pauses the current thread and waits for the new thread (\ie \texttt{hv\_pci\_generic\_compl()}) to complete.
Despite this complexity, GPT-4 adeptly navigates the concurrency and callback handling, pinpointing the accurate initializer and outputting a precise result.

It is worth noting that we do not encode any knowledge about the Linux kernel synchronization primitives. 
\work prompts LLMs with  \textit{``The `initializer' must be the `actual' function that initializes the variable.''}
and then LLMs can automatically identify the function \texttt{hv\_pci\_generic\_compl()}
% by copying the entire function body of \texttt{hv\_pci\_enter\_d0}, it 
as the initializer of \texttt{comp\_pkt.completion\_status}.

% \and asking it to identify
% the initializer. Below is a core part of the prompt: 
% \zhiyun{TODO: Haonan will provide an example.}

% \zhiyun{consider adding another case study of inline assembly.}
\vspace{3pt}
\noindent
\textbf{Unfamiliar Function.} 
As previously delineated in \S\ref{subsec:cap}, LLMs possess the inherent ability to recognize the semantics (\eg postconditions) of common functions like \texttt{sscanf()}. However, some argue that \textit{``the LLM simply learns everything from the internet and acts merely as a search engine''} \cite{chiang_chatgpt_2023}. This viewpoint is challenged by the case illustrated in Figure \ref{fig:p9_read}.

The case presents an intriguing real-world bug. The function \texttt{p9pdu\_readf()} mirrors \texttt{sscanf()} in structure, yet lacks a check of its return value, leaving the parameter \texttt{ecode} at risk of being uninitialized, i.e., if \texttt{pdu\_read()} returns non-zero in line 19 (thus ``break'' early).
Notably, unlike \texttt{sscanf()}, where GPT-4 can provide a precise summary of the function without asking for its definition,
it does request the function definition of \texttt{p9pdu\_readf()}, as it is not as ubiquitous as \texttt{sscanf()}.  

Furthermore, our solution not only produces the correct outcome for this particular case but also pinpoints that \texttt{ecode} could be initialized when \texttt{p9pdu\_readf()} returns 0, demonstrating the efficacy of \work for unfamiliar cases. The result is as follows:

% like \texttt{p9pdu\_readf()}.
% \zhiyun{as long as we know ecode may not always be initialized, we should be able to tell it is a bug? The additional info about initialization only when return is 0 is not critical for this case.} \haonan{already patched, UBITect's FN} \zhiyun{Overall, I don't find this example a great one.}

% \zhiyun{introduce the json response}
\begin{lstlisting}[numbers=none]
{
    "initializer": 
        "err = p9pdu_readf(req->rc, c->proto_version, 'd', &ecode)",
    "suspicious": ["ecode"],
    "postconstraint": null,
    "response": {
        "must_init": [],
        "may_init": [{
                "name": "ecode",
                "condition": "p9pdu_readf returns 0"
        }]
    }
}
\end{lstlisting}


%\zhiyun{mention the step (initializer identification) and the fact that we copy the code to ChatGPT, line\# range.}




\begin{figure}[]
\hspace{-15pt}
\includegraphics{figures/minted/case_p9_check.pdf}
    \caption{Case Study III (Unfamiliar Function), derived from \texttt{net/9p} 
    %\zhiyun{font size of the code snippet can be smaller}
    }
    \label{fig:p9_read}
\end{figure}


\vspace{-7pt}
\subsection{Reason for Imprecision}
\label{subsec:excuse}

%\zhiyun{This section should go to precision, with stats associated with each root cause. I also dislike the term imprecision here. It is ambiguous (you have a different definition for it which is never made explicit).}

Despite \work achieving a precision of 50\% in real-world applications, 
the precision can still be improved in the future. Some 
can be solved with better prompts or better integration with static analysis. 


\vspace{3pt}
\noindent
\textbf{Challenges in Constraint Extraction.} Beyond the four primary code patterns we addressed in \S\ref{subsec:postcondi}, there exist additional forms of post-constraints. For instance, during error handling, the checks for failures may involve another function or macro. This problem can be addressed by either more examples during prompts (in-context learning), or lightweight program analysis (\eg path exploration in symbolic execution to collect the post-constraints). 
%Accurate extraction of these post-constraints may necessitate further source code preprocessing. 

\cut{
\vspace{3pt}
\noindent
\textbf{Insufficient Field Sensitivity.} LLMs can distinguish between different fields of a structure; furthermore, they can even identify
some particular cases such as \texttt{container\_of}. 
However, UBITect does not point out the field name by default.
With more dedicated engineering work in extracting field information from UBITect, the field sensitivity of \work can be substantially improved. 

\vspace{3pt}
\noindent
\textbf{Propagation of Uninitialization} In UBITect, if an initialized variable \textit{a} is \textbf{propagated} to another variable \textit{b}, it will correctly track \textit{b} as uninitialized. Therefore, if an initializer operates on \textit{b} instead of \textit{a}, and the subsequent use is also on \textit{b},
it will attribute the bug variable \textit{a}. 
Unfortunately, due to the lack of interface exposing the relationship between \textit{a} and \textit{b}, \work is aware of \textit{a} being the suspicious variable and fails to identify any initializer for it. This issue can be solved by better collaboration with UBITect.
}


\vspace{3pt}
\noindent
\textbf{Information Gaps in UBITect.} For instance, UBITect does not provide explicit field names within a structure when a specific field is in use. This information gap can result in \work lacking precision in its analysis. Additionally, UBITect only reports the variable utilized, not necessarily the same variable passed to an initializer. For example, consider an uninitialized variable \texttt{a} passed to an initializer, which is then assigned to variable \texttt{b} for usage. In such a scenario, \work may fail to identify the initializer due to this incomplete information correctly. These challenges, primarily due to the interface design in UBITect, can be addressed with focused engineering efforts to enrich the output information from UBITect.
% such as field names within structures and propagation paths of uninitialized variables.


% \zhiyun{try merging the previous two}

\cut{
could lead another variable \textit{X} becoming uninitialized, if there's any assignment to \textit{X} like: \( X=E, \, v\in E \). We do not consider the propagation because we treat \(X=E\) as the use of \textit{v}, so this bug can be found in another warning. However, UBITect considers propagation and only outputs ``critical'' ones heuristically.

\work filters out most propagation cases automatically, because there it cannot find an initializer. 
A complete solution relies on letting UBITect output all potential bugs without heuristic selection,
and therefore we \haonan{...}
}
% or the output of the very first uninitialized variable at the same time.
% In our experiment, we manually find the variable first to be used (\ie the propagation position) and let \work re-analyze them. 
% Still, interestingly, \work also accurately analyzes many examples with propagation, which shows LLM's good understanding of the logic of code.


\vspace{3pt}
\noindent
\textbf{Variable Reuse.} 
Varaible reuse is an interesting problem of LLM. In general, LLM usually confuses different variables in different scopes (\eg different function calls).
For example, if the suspicious variable is \texttt{ret} and passed as a argument to its 
initializer (say, \texttt{func(\&ret)}) and there is another stack variable
defined in \texttt{func} also called \texttt{ret}, LLM will confuse them. 
Explicitly prompting and teaching LLM to note the difference does not appear to work. One solution is to leverage a simple static analysis to normalize the source code to ensure each variable has a unique name. 




\vspace{3pt}
\noindent
\textbf{Indirect Call.} 
As mentioned \S\ref{subsec:iter_prompt}, \work follows a simple but imprecise strategy to handle indirect calls. 
% In one case, we find that GPT-4 failed to provide confident results and requested the function definition of the indirect call target,
% which we do not currently provide. 
Theoretically, existing static analysis tools, such as MLTA \cite{lu_where_2019}, can give possible targets for indirect calls. However, 
% each indirect call requires an enumeration of all possible targets, which can be over a hundred in some cases.
each indirect call may have multiple
possible targets and  dramatically increase the token usage. 
We leave the exploration of such an exhaustive strategy for future work.
 \work may benefit from a more precise indirect call resolution.
% \zhiyun{check if this paragraph is okay.}
%\work is motivated by inconclusive of UBITect, and too many indirect call targets contribute to memory exhaust. The solution to this problem depends on a more accurate resolution of the indirect call target, which we leave for the future.
% \yu{this is inconsistent with case study "Concurrency and Indirect Call"}



% This divergence necessitates substantial engineering efforts to bridge the gap, making it a more complex and time-consuming undertaking than it might initially appear




\vspace{3pt}
\noindent
\textbf{Additional Constraints.} There are many variables whose values are determined outside of the function we analyze, \eg preconditions capturing constraints from the outer caller. Since our analysis is fundamentally under-constrained, this can lead \work to incorrectly determine a \texttt{must\_init} case to be \texttt{may\_init}. 
Mitigating this imprecision relies on further analysis to provide more information. 


