% ----------------------------------------------------------------------------------------------

\section{The regular pull algorithm is asymptotically optimal}
\label{sec:pull}

In this section, we focus on pull-only algorithms. 
Our first observation is that on expectation, pulling is always at least as good as pushing, although the higher variance of pull at the early stage of the dissemination makes pulling less efficient when the rumor is new. For instance, starting with one informed process and $\fanin=\fanout=1$, it takes $\Theta(\ln n)$ pull rounds to inform a second process with high probability, whereas a single push round suffices. The behavior reverses when the rumor is old: if $n-1$ processes are already informed, a single pull round informs the last process with high probability but $\Theta(\ln n)$ push rounds are needed. Despite these differences, our second observation is that pulling and pushing have the same asymptotic round complexity. Our third observation is that the regular pull algorithm is asymptotically optimal, thus pushing is not required. 
Our fourth observation is that the regular pull algorithm asymptotically requires the same round, bit, and message complexity even in the presence of a large number of adversarial and stochastic failures.

Note that in the generalized random phone call model, processes push and pull requests uniformly at random but independently (i.e., with replacement), thus they can push the rumor to themselves, call themselves, and have multiple push messages and/or pull requests colliding in the same round. Of course in practice, in a given round, a process will not send multiple pull requests or multiple push messages to the same process, nor will it call itself. Instead, it will select a uniform random sample among the other processes in the network. Our reason for this definition is twofold. First, choosing interlocutors independently and uniformly at random is more amenable to mathematical analysis, especially upper bounds. Second, we prove that choosing $f$ processes uniformly at random with replacement, or choosing a uniform random sample of size $f$ without replacement among the other $n-1$ processes, are asymptotically equivalent when $f \in \mathcal{O}(n)$. We prove this by matching lower bounds obtained from random samples with upper bounds obtained with interlocutors selected independently and uniformly at random.
\begin{definition}
  Let $0 \leq \uninformed{r} \leq n$ be the number of uninformed processes at round $r$,  $\E_{\text{pull}}[\uninformed{r}]$ the expected number of uninformed processes at round $r$ with the regular pull algorithm, and $\E_{\text{push}}[\uninformed{r}]$ the expected number of uninformed processes at round $r$ with the regular push algorithm. For the number of informed processes at round $r$, we similarly define $\informed{r}$, $\E_{\text{pull}}[\informed{r}]$ and $\E_{\text{push}}[\informed{r}]$. It is clear that $n=\uninformed{r}+\informed{r}=\E_{\text{pull}}[\uninformed{r}]+\E_{\text{pull}}[\informed{r}]=\E_{\text{push}}[\uninformed{r}]+\E_{\text{push}}[\informed{r}]$.
\end{definition}
If processes send pull requests independently and uniformly at random, $\P(\uninformed{r+1} \mid \uninformed{r})$ follows a binomial distribution with mean
\begin{align}
  \label{eq:meanpull1}
  \E_\text{pull}[\uninformed{r+1} \mid \uninformed{r}]&=\uninformed{r}\cdot\left(\frac{\uninformed{r}}{n}\right)^{\fanin}
\end{align}
whereas if they select a uniform random sample without replacement among the other $(n-1)$ processes we obtain
\begin{align}
  \label{eq:meanpull2}
  \E_\text{pull}[\uninformed{r+1} \mid \uninformed{r}]&=\uninformed{r}\cdot\frac{{\uninformed{r}\choose\fanin}}{{n-1 \choose \fanin}}=n-\uninformed{r}\frac{\uninformed{r}(\uninformed{r}-1)\dots(\uninformed{r}-\fanin+1)}{n(n-1)\dots(n-\fanin+1)} = n-\uninformed{r} \frac{(\uninformed{r})_{\fanin}}{(n-1)_{\fanin}} 
\end{align}
where $(\boldsymbol{\cdot})_{\boldsymbol{\cdot}} $ is the falling factorial notation.


\begin{lemma}
  \label{lem:pullbetterthanpush}
If $\fanout = \fanin$, then 
    $\E_{\text{pull}}[\uninformed{r+1}|\uninformed{r}] \leq \E_{\text{push}}[\uninformed{r+1}|\uninformed{r}]$.
\end{lemma}

\begin{proof}
We prove the lemma with processes chosen independently and uniformly at random.  Let  $f=\fanin=\fanout$. For the pull version, we saw that
\begin{align}
  \label{eq:meanpull1repeat}
  \E_\text{pull}[\uninformed{r+1} \mid \uninformed{r}]=\uninformed{r}\cdot\left(\frac{\uninformed{r}}{n}\right)^{f}
\end{align}
whereas for the push version we can show that 
  \begin{equation}
    \label{eq:meanpush}
   \E_{\text{push}}[\uninformed{r+1} \mid \uninformed{r}] = \uninformed{r}\left(1 - \frac{1}{n}\right)^{f(n-\uninformed{r})}.
 \end{equation}
 From Eq.~\eqref{eq:meanpull1repeat} and \eqref{eq:meanpush}, it is clear that the lemma holds when $\uninformed{r}=0$, $\uninformed{r}=n-1$, and $\uninformed{r}=n$. For the other values of $\uninformed{r}$, we prove that
 \begin{align}
\label{eq:comp}
      & \left( \frac{\uninformed{r}}{n}\right)^{f} \leq \left( \left(1 - \frac{1}{n}\right)^{n - \uninformed{r}}\right)^{f} 
      \Leftrightarrow \left(\frac{n-1}{n}\right)^{n - \uninformed{r}} - \frac{\uninformed{r}}{n} \geq 0.
 \end{align}
  Let $g(x) \triangleq \left(\frac{n-1}{n}\right)^{n - x} - \frac{x}{n}$. Since $g(0) \geq 0$ and $g(n-1) = 0$, we prove that $g(x) \geq 0$ for every $x \in \{0,1,\dots, n-1\}$ by showing that $g'(x) \leq 0$ over the interval $[0,n-1]$. We have
  \begin{equation}
     \begin{split}
      g'(x) 
        &= - \left( \frac{n-1}{n} \right)^{n-x} \ln \left(\frac{n-1}{n}\right) - \frac{1}{n} \\
        &= \left(\frac{n}{n-1}\right)^x \left( \frac{n-1}{n} \right)^{n} \ln \left(\frac{n}{n-1}\right) - \frac{1}{n} \\
      \end{split}
    \end{equation}
    which is an increasing function with respect to $x$. To complete the proof, we verify that $g'(n-1) \leq 0$:
     \begin{equation}
    \begin{split}
      g'(n-1) & =  \left(\frac{n}{n-1}\right)^{(n-1)} \left( \frac{n-1}{n} \right)^{n} \ln \left(\frac{n}{n-1}\right) - \frac{1}{n} \\
      & \leq \frac{n-1}{n} \left(\frac{n}{n-1} -1\right)  - \frac{1}{n} \\ & = 0.
    \end{split}
  \end{equation}
\end{proof}


 We now bound the expected progression of the regular pull algorithm, and later use it to derive lower bounds on its round complexity.
 
 \begin{lemma}
   \label{lem:pullasymptotic}
$\E_\text{pull}[\informed{r+1}\mid\informed{r}] \leq \informed{r} \cdot(\fanin+1)$.
 \end{lemma}

\begin{proof}
  We prove the lemma with processes chosen from a uniform random sample using Eq.~\eqref{eq:meanpull2}. We fix $n$ and $\uninformed{r}$ and prove the lemma by induction on $\fanin$.

  \paragraph{Basis step.} The lemma is clearly true for $x=\fanin=0$.
  \paragraph{Inductive step.} Let $0 \leq x \leq n-2$ be an integer. We assume that $n-\uninformed{r} \frac{(\uninformed{r})_x}{(n-1)_x} \leq \informed{r}(x+1)$, which is equivalent to
  \begin{align}
    \label{eq:ind1}
     \uninformed{r} \frac{(\uninformed{r})_x}{(n-1)_x} \geq n - \informed{r}(x+1)
  \end{align}
  and must show that
  \begin{align}
    \label{eq:ind2}
    n-\uninformed{r} \frac{(\uninformed{r})_x(\uninformed{r}-x)}{(n-1)_x(n-1-x)} \leq \informed{r}(x+2) \Leftrightarrow  S \triangleq n-\uninformed{r} \frac{(\uninformed{r})_x(\uninformed{r}-x)}{(n-1)_x(n-1-x)} - \informed{r}(x+2) \leq 0.
     \end{align}
Substituting the left side of Eq.~(\ref{eq:ind1}) for its right side in Eq.~(\ref{eq:ind2}), and replacing $\uninformed{r}$ by $n - \informed{r}$, we have

\begin{equation}
  \begin{split}
  S & \leq n- (n - \informed{r}(x+1)) \frac{n-\informed{r}-x}{n-1-x} - \informed{r}(x+2) \\
    & \leq \frac{n(\informed{r}-1) - \informed{r} (x+1) (\informed{r}  - 1)}{n-x-1} - \informed{r} \\ & \leq \informed{r}-1 - \frac{(x+1) (\informed{r}  - 1)^2}{n-x-1} - \informed{r} \\
    & \leq - \frac{(x+1) (\informed{r}  - 1)^2}{n-x-1} \\
  & \leq 0.
  \end{split}
\end{equation}
  
 \end{proof}


 
\begin{lemma}
    \label{lem:loglog}
        If $\fanin\in\mathcal{O}(\ln n)$, the regular pull algorithm starting with $\frac{n}{\ln n}$ informed processes informs all processes with high probability in $\Theta(\log_{\fanin+1} \ln n)$ rounds.
  \end{lemma}

  \begin{proof}
For the lower bound, it is clear from Lemma~\ref{lem:pullasymptotic} that $\Omega(\log_{\fanin+1} \ln n)$ are required to reach all processes on expectation, thus required to inform all processes with high probability. For the upper bound, the proof for $\fanin=1$ consists of the points 3 and 4 in the proof of Theorem 2.1 of Karp et
    al.~\cite{DBLP:conf/focs/KarpSSV00}. We generalize their proof for an arbitrary $\fanin$.

    Recall that $\E_{\text{pull}}[\uninformed{t}\mid\uninformed{t-1}] =  \frac{(\uninformed{t-1})^{\fanin+1}}{n^{\fanin}}$ and that we start with at most $u_0 = n - \frac{n}{\ln n}$ uninformed processes. We use the following Chernoff bound from \cite{mitzenmacher2005probability}: 
    \[ \P(X \geq (1 + \delta) \mu) \leq e^{-\frac{\delta^2 \mu}{3}},\ 0 < \delta < 1. \]
If $u_{t-1} \geq (\ln n)^{\frac{4}{\fanin+1}}n^\frac{\fanin}{\fanin+1}$, it follows that
    \begin{align*}
      \P\left(u_t \geq \left(1 + \frac{1}{\ln n}\right) \frac{(\uninformed{t-1})^{\fanin+1}}{n^{\fanin}}\right) 
        & \leq  e^{- \frac{1}{3} \ln^2 n} \\ & \in o\left(n^{-c}\right) \text{ for any constant $c$}
    \end{align*}
and we can deduce that
    \begin{equation}
      \label{eq:uninformed-whp}
      \uninformed{t} \leq \left(1 + \frac{1}{\ln n}\right) \frac{(\uninformed{t-1})^{\fanin+1}}{n^{\fanin}}
    \end{equation}
    with high probability. Applying Eq. (\ref{eq:uninformed-whp}) recursively, we obtain
    \begin{align}
      \uninformed{t} & \leq (\uninformed{0})^{{(\fanin+1)^t}}\left(\frac{1 + \frac{1}{\ln n}}{n^{\fanin}}\right)^\frac{(\fanin+1)^t-1}{\fanin} 
    \end{align}
Replacing $u_0$ by $n - \frac{n }{\ln n}$, and $t$ by $4 \log_{\fanin+1}\ln n$ we obtain 
\begin{equation}
  \begin{split}
   \uninformed{t} & \leq \left(n-\frac{n}{\ln n}\right)^{{(\fanin+1)^t}}\left(\frac{1 + \frac{1}{\ln n}}{n^{\fanin}}\right)^\frac{(\fanin+1)^t-1}{\fanin} \\ & \leq n \left(1-\frac{1}{\ln n}\right)^{\ln^4 n} \left(1+\frac{1}{\ln n}\right)^{\ln^4 n} \\ & \leq n \left(1-\frac{1}{\ln^2 n}\right)^{\ln^4 n} \\ & \in o(1)
 \end{split}
\end{equation}
which shows that we need $O(\log_{\fanin+1} \ln n)$ rounds to reach the point where there are at most $(\ln n)^{\frac{4}{\fanin+1}}n^\frac{\fanin}{\fanin+1}$ uninformed processes with high probability. Note that this step is unnecessary if $\fanin$ is large enough with respect to $n$ since $(\ln n)^{\frac{4}{\fanin+1}}n^\frac{\fanin}{\fanin+1} \geq n-\frac{n}{\ln n}$.

At this stage, the probability that an uninformed process remains uninformed after each subsequent round is at most 
    \begin{align}
      \label{eq:21}
      \left(\frac{\uninformed{r}}{n}\right)^{\fanin} & \leq \left(\frac{(\ln n)^{\frac{4}{\fanin+1}}n^\frac{\fanin}{\fanin+1}}{n}\right)^{\fanin} \leq \frac{(\ln n)^4}{\sqrt{n}}.
    \end{align}
    Hence after a constant number of additional rounds, we inform every remaining uninformed process with high probability. 

  \end{proof}

% ----------------------------------------------------------------------------------------------

  \begin{corollary}
 If $\fanin\in\Omega(\ln n)$, the regular pull algorithm starting with $\frac{n}{\ln n}$ informed processes informs all processes with high probability in $\Theta(1)$ rounds.
  \end{corollary}

  
% ----------------------------------------------------------------------------------------------

\begin{theorem}
  \label{thm:pull}
    The regular pull algorithm disseminates a rumor to all processes with high probability in $\Theta(\log_{\fanin + 1} n)$ rounds of communication.
\end{theorem}

\begin{proof}
  For the lower bound, it is clear from Lemma~\ref{lem:pullasymptotic} that $\Omega(\log_{\fanin+1} n)$ rounds are required in expectation to inform all processes, and thus necessary to inform all processes with high probability.

  We now show that $\mathcal{O}(\log_{\fanin + 1} n)$ rounds suffice when
  $\fanin \in \mathcal{O}(\ln n)$ (the statement for $\fanin = 1$ is implicitly discussed without proof in \cite{DBLP:conf/focs/KarpSSV00}). 

  In a first phase, we show that $\mathcal{O}(\log_{\fanin+1}n)$ rounds are sufficient to inform $\ln n$ processes with high probability. Let $c_0 \geq 1$ be a constant. In this case, we show that for stages $k \in \{0,1,2,\dots,\ln\ln n\}$, if $\informed{r} = 2^k$ processes are informed, then after $\rho_k \triangleq c_0\left\lceil \frac{\log_{\fanin + 1} n}{2^k} \right \rceil$ rounds, the number of informed processes doubles with high probability, i.e., $\informed{r+\rho_k} \geq 2^{k+1}$ with high probability. At every round of stage $k$, each pull request has a probability at least $\frac{2^k}{n}$ of reaching an informed process, thus after $\rho_k$ rounds and $\rho_k \cdot \fanin$ pull requests, the probability that an uninformed process learns the rumor is bounded by
  \begin{align}
  p \geq 1 - \left( 1 - \frac{2^k}{n} \right)^{\rho_k \cdot \fanin} \geq \frac{2^k\rho_k\fanin}{n}-\frac{2^{2k}\rho_k^2\fanin^2}{n^2}.    
  \end{align}
  The probability $T$ to inform $l=\informed{r}=2^k$ processes or less in stage $k$ is upper bounded by the left tail of the binomial distribution with parameters $p$ and $N = \uninformed{r} = n-2^k$. We can bound this tail using the Chernoff bound
  \begin{align}
    \label{eq:chernoffbinomial}
       T & \leq \exp\left(-\frac{(Np-l)^2}{2Np}\right)
  \end{align}
  which is valid when $l \leq N p$. We can indeed apply this bound by showing that $Np \geq \frac{c_0 \fanin}{\ln(\fanin + 1)} \ln n + o(1)$, which is greater than $2^k$ when $c_0 \geq 1$. The Chernoff bound gives
  \begin{equation}
       \begin{split}
     \label{eq:3}
      T 
     & \leq \exp\left(-\frac{Np}{2}+l\right) \\
   %  \\
        & \leq \exp \left(-\frac{(n-2^k)p}{2} + 2^{k}\right) \\     
    & \leq \exp\left(\left(1-\frac{c_0\fanin}{2\ln(\fanin+1)}\right) \ln n +o(1)\right) \\
  & \in \mathcal{O}\left(n^{  1-\frac{c_0\fanin}{2\ln(\fanin+1)}}  \right)
\end{split}
\end{equation}
and for any constant $c>0$ we can find $c_0$ such that $T \in \mathcal{O}\left(n^{-c}\right)$. This first phase, with the $k$ stages, requires $\sum\limits_{k=0}^{\ln\ln n} \rho_k \leq c_0 \log_{\fanin + 1} n \cdot \sum\limits_{k=0}^{\ln\ln n} 2^{-k} + c_0(\ln\ln n + 1) \sim 2c_0 \log_{\fanin + 1} n$ rounds of communication to inform $1 + 2^0 + 2^1 + \dots + 2^{\ln\ln n} \approx 2 \ln n$ processes with high probability. 

In a second phase, when $\ln n \leq \informed{r} \leq \frac{n}{(\ln n)^2}$, we show that a constant number of rounds $c_1$ is sufficient to multiply the number of informed processes by $\fanin+1$ with high probability. We use the Chernoff bound of Eq.~\eqref{eq:chernoffbinomial} with $l=\fanin \cdot\informed{r}$, $n-\frac{n}{\ln n}\leq N \leq n-\ln n$ and $p \geq 1-\left(1-\frac{\informed{r}}{n}\right)^{c_1 \fanin} \geq \frac{\informed{r}c_1\fanin}{n} -\frac{\informed{r}^2 c_1^2 \fanin^2}{2n^2}.$ We obtain
\begin{equation}
  \begin{split}
  T & \leq \exp\left(-\frac{Np}{2}+l\right) \\
    & \leq \exp \left( -\frac{\informed{r}c_1\fanin}{2}\left(1-o(1)\right)+\informed{r}\fanin\right) \\
    & \leq \exp \left(\ln n\left(1-\frac{c_1}{2}+o(1)\right) \right) \\
    & \in \mathcal{O}\left(n^{  1-\frac{c_1}{2}}  \right)
  \end{split}
\end{equation}
and for any constant $c>0$ we can find $c_1$ such that $T \in \mathcal{O}\left(n^{-c}\right)$.  This second phase requires $\mathcal{O}(\log_{\fanin + 1} n)$ rounds of communication.

In a third phase, we can go from $\frac{n}{(\ln n)^2}$ to $\frac{n}{\ln n}$ informed processes in $\mathcal{O}(\log_{\fanin + 1} n)$ rounds of communication since multiplying the number of informed processes by $\ln n$ at this stage cannot be slower than during the first phase.  Finally, in a fourth phase we saw in Lemma~\ref{lem:loglog} that we can go from $\frac{n}{\ln n}$ to $n$ informed processes with high probability with $\Theta\left(\log_{\fanin+1}\ln n\right)$ rounds of communication.

We now summarize the proof of the upper bound when $\fanin\in \omega(\ln n)$ and $\fanin\in\mathcal{O}(n)$. The different cases must me handled with care, but we omit the details for simplicity purposes. In a first phase, we show that  $\mathcal{O}(\log_{\fanin + 1} n)$  rounds are sufficient to inform $\ln n$ processes with high probability. In a second phase, if $\fanin \cdot \informed{r} \in o(n)$, we apply the Chernoff bound of Eq.~\eqref{eq:chernoffbinomial} during $\mathcal{O}(\log_{\fanin + 1} n)$  rounds to reach either $\frac{n}{\ln n}$ informed process with high probability, or $\fanin \cdot \informed{r} \in \Theta(n)$ (the Chernoff bound must be changed when $\fanin \cdot \informed{r} \in \Theta(n)$). If $\fanin \cdot i \in \Theta(n)$, we again apply Eq.~\eqref{eq:chernoffbinomial} during a constant number of rounds to reach $c_2 \cdot n$ informed processes with $c_2<1$ with high probability. Finally, in a last phase, we go from $c_2 \cdot  n$ or $\frac{n}{\ln n}$ to $n$ informed processes with high probability using Lemma~\ref{lem:loglog}.

\end{proof}

% ----------------------------------------------------------------------------------------------

\begin{corollary}
  If $\fanin \in \mathcal{O}(1)$, then the total number of messages (replies to pull requests) required by the regular pull algorithm is in $\Theta(n)$. In particular, the communication overhead is 0 when $\fanin=1$.
\end{corollary}

\begin{proof}
  It is clear that a process cannot pull a rumor more than $\fanin$ times since it stops requesting it in the rounds that follow its reception.
\end{proof}

% ----------------------------------------------------------------------------------------------

We now prove that the round complexity of the regular pull algorithm is asymptotically optimal for the generalized random phone call model.

\begin{theorem}
  \label{thm:pushpullround}
If $f=\fanin=\fanout$, any protocol in the generalized random phone call model requires $\Omega(\log_{f+1} n)$ rounds of communication to disseminate a rumor to all processes with high probability.
\end{theorem}

\begin{proof}
  % The bit complexity and the message complexity of the regular pull algorithm are trivially optimal. For the round complexity,
Let $f=\max(\fanin,\fanout)$. 
If we only push messages, it is clear that the number of informed processes increases at most by a factor of $(\fanout + 1)$ per round. If we only pull messages, we saw in Lemma~\ref{lem:pullasymptotic}
  that the number of informed processes increases at most by a factor of $(\fanin + 1)$ per round in
  expectation. If all processes simultaneously push and pull at every round, the number of informed processes
  increases at most by a factor of $(\fanin + 1)(\fanout + 1)$ per round in expectation, thus the number or
  rounds required to informed all processes is at least
  $\log_{(\fanout+1)(\fanin+1)} n \geq \log_{(f+1)^2} n \in \Omega(\log_{f+1} n)$.

\end{proof}

% ----------------------------------------------------------------------------------------------

We now show that the regular pull algorithm is robust against adversarial and stochastic failures.  First, consider an adversary that fails $\epsilon \cdot n$ processes for $0 \leq \epsilon < 1$, excluding the process starting the rumor. Before the execution of the algorithm, the adversary decides which processes fail, and for each failed process during which round it fails. Once a process fails, it stops participating until the end of the execution, although it may still be uselessly called by active processes. We also consider stochastic failures, in the sense that each phone call fails with probability $\delta$ for $0 \leq \delta < 1$. Note that both types of failures are independent of the execution.%\mm{i do not understand the non byzantine model and independence. A non byzantine adversary could simply stop processes based on the state of the execution}. 

The main difference introduced by the failures is that we can no longer go from $\frac{n}{\ln n}$ to $n$ informed processes in $\mathcal{O}(\log_{f+1} \ln n)$ rounds because there is a non-vanishing probability that pull requests either target failed processes or result in failed phone calls. We nevertheless show that the regular pull algorithm can disseminate a rumor to all $(1-\epsilon)n$ good (i.e., non-failed) processes with high probability with the same asymptotic round complexity. 

\begin{theorem}
  \label{thm:failures}
  Let $0 \leq \epsilon < 1$, and let $0 \leq \delta < 1$. If $\epsilon \cdot n$ processes, excluding the initial process with the rumor, fail adversarially, and if phone calls fail with probability $\delta$, then the regular pull algorithm still disseminates a rumor to all $(1-\epsilon)n$ good processes with high probability in $\Theta(\log_{\fanin + 1} n)$ rounds of communication.
\end{theorem}

\begin{proof}
  It is clear that the lower bound remains valid when there are failures. We prove the upper bound for $\fanin\in\mathcal{O}(\ln n)$, but as we mentioned for Theorem~\ref{thm:pull} we can adapt the proof for $\fanin\in\omega(\ln n)$ by carefully applying Chernoff bounds in different phases.

Note that the earlier a process fails, the more damage it causes. We thus assume that the $\epsilon \cdot n$ processes fail at the beginning of the execution, which is the worst possible scenario. We can use the first three phases of the proof of Theorem \ref{thm:pull} with minor modifications (only multiplicative constants change) and prove that $\mathcal{O}(\log_{\fanin + 1} n)$ rounds are sufficient to go from 1 to $\frac{n}{\ln n}$ informed processes with high probability. 

We now show that we need $c_2\log_{\fanin + 1} n$ rounds to go from $\frac{n}{\ln n}$ to $c_1 \cdot n$ informed processes with high probability for some arbitrary $c_1 < 1-\epsilon$. We again use the Chernoff bound of Eq.~\eqref{eq:chernoffbinomial} with $\informed{r}=\frac{n}{\ln n}$, $l = c_1 \cdot n$ and $N = (1-\epsilon) n - \frac{n}{\ln n}$. If $c_2$ is a large enough constant, the probability that a process learns a rumor during that phase is
  \begin{align}
  p \geq 1-\left(1-\frac{(1-\delta)\informed{r}}{n}\right)^{\fanin c_2\log_{\fanin + 1} n} \geq 1-\left(1-\frac{1-\delta}{\ln n}\right)^{\frac{\fanin c_2\ln n}{\ln(\fanin+1)}} \geq 1-e^{-c_2(1-\delta)} \triangleq c_3.
  \end{align}
The Chernoff bound gives
\begin{equation}
 \begin{split}
    T & \leq \exp\left(-\frac{Np}{2}+l\right) \\
      & \leq \exp \left( -c_3n\left(1-\epsilon-\frac{1}{\ln n}\right)+c_1 n \right)\\
     & \leq \exp \left( n\left(-c_3+c_3\epsilon+c_1+o(1) \right)\right)\\
%    & \leq \exp \left( -\frac{\informed{r}c_1\fanin}{2}\left(1-o(1)\right)+\informed{r}\fanin\right) \\
%    & \leq \exp \left(\ln n\left(1-\frac{c_1}{2}+o(1)\right) \right) \\
 %   & \in \mathcal{O}\left(n^{  1-\frac{c_1}{2}}  \right)
   \end{split}
 \end{equation}
and we can choose $c_2$ such that $T \leq e^{c_4 n}$ with $c_4 < 0$. This guarantees $T \in \mathcal{O}\left(n^{-c}\right)$ for any~$c>0$.

Starting from $c_1 n$ informed processes, the probability that a process is informed in any subsequent round is bounded by $p \geq 1-\left(\frac{n-c_1(1-\delta)n}{n}\right)^{\fanin} \geq 1 - (1-c_1(1-\delta))^{\fanin}$.  After $r$ such rounds, the probability that a process remains uninformed is thus upper bounded by $(1-c_1(1-\delta))^{\fanin r}$, and for this probability to be bounded by $n^{-c}$ we need
  \begin{align}
    (1-c_1(1-\delta))^{\fanin r} \leq n^{-c} \Leftrightarrow
    r \geq  \frac{c \ln n}{\ln{\frac{1}{1-c_1(1-\delta)}}\fanin} \geq c_4 \log_{f+1} n \text{ for some constant $c_4$.}
  \end{align}
Hence, $\mathcal{O}(\log_{f+1} n)$ rounds are sufficient to go from $c_1n$ to $(1-\epsilon) n$ informed processes with high probability.

\end{proof}

Note that adversarial and stochastic failures do not increase the message complexity of the regular pull algorithm: uninformed processes that fail decrease the number of rumor transmissions, and failed phone calls do not exchange the rumor. We could, however, consider that messages containing the rumor are dropped with probability $0 \leq \gamma < 1$. Theorem~\ref{thm:failures} also holds in this instance, but the number of messages increases by an unavoidable factor of $\frac{1}{1-\gamma}$.

% ----------------------------------------------------------------------------------------------

% ----------------------------------------------------------------------------------------------

% ----------------------------------------------------------------------------------------------

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
