\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/fig3.png}
    \caption{Watermark size at near-optimal quality for text-dependent versus fixed randomness. 
    The values correspond to the minimal size of schemes with near-optimal quality. 
    We report the smallest size result accross all four schemes. For text-dependent randomness, 
    we report the smallest size between sliding window and min hash. 
    Text-dependent randomness is more efficient at all temperature settings.}
    \label{fig:text-dependent_fixed_randomness}
    % DONE: Needs more detail in the caption (or body text).  Which watermarking scheme is evaluated here?  Distribution shift?  The best across all four schemes, at each temperature?  Is the gold line using text-dependent sliding window, or text-dependent min hash, or the best across both?
\end{figure}

\section{Results}
\label{sec:results}

We evaluated a total of over 1200 unique combinations of watermark schemes and parameter settings. This took $\sim$1 week to run using 4 A5000 GPUs. We first summarize our 
evaluation in~\cref{ssec:eval_aggregate}, in which we advise using a distribution-shift sampling strategy with text-dependent randomness. 
We further analyze the impact of each parameter in~\cref{ssec:param_tuning}, and give some recommendations. Finally, we focus on 
tamper-resistance in~\cref{ssec:robustness-eval}, where we show some watermarks can withstand GPT-paraphrasing in over 50\% of cases without 
sacrificing efficiency. 

\subsection{The ``Best'' Watermark}\label{ssec:eval_aggregate}
%Our aggregate metric (\cref{ssec:aggregate})
In \cref{fig:aggregate}, we show the minimal watermark size (median number of tokens needed to detect a watermark, 
at 2\% false alarm rate) under various temperatures, for a quality degradation of at most 1\% and a 20\% 
constraint on tamper-resistance.

The distribution shift scheme performs the best at most temperature
settings, including the range 0.0--0.7, which which is arguably the
most common range of temperature values used in practice.
At temperature 1, exponential sampling is slightly superior, with
distribution shift not much worse.
The relative ranking of watermarking schemes is consistent across 
different choices of quality and tamper-resistance thresholds
(see, e.g., \cref{fig:aggregate2}).

\smallskip\noindent{\bf Ready to deploy?} 
The distribution shift scheme is able to detect watermarks with
$\sim$60--80 tokens (roughly 45--60 words), at temperatures in the
range 0.0--0.7.
This strongly suggests that the distribution shift watermarking scheme
is practical enough to be deployed today.

\smallskip\noindent{\bf Parameter settings.} 
The following parameter choices achieve the results shown in \cref{fig:aggregate}: for distribution-shift, 
$\gamma=0.5$, text-dependent randomness (sliding window of size 3), and $\delta=5$ (at $T=\leq0.3$) or 
$\delta=4$ (at $T=0.7$) or $\delta=2.5$ (at $T=1$);
for the exponential scheme, text-dependent randomness (sliding window of size 3) and skip probability $0.025$.
These achieve our goals of maintaining 99\% of quality and tamper-resistance of 20\%.
%In schemes with text-dependent randomness, min hash randomness with window size 3 performed similar to sliding window randomness with window size 6.

% DONE: please check the sentence "These achieve..." above

% DONE: Do you mean \delta=5 is best at T=0 and T=0.3?  The text I revised
% said \delta=5 at T=0.3 and 1, which looks wrong to me.
% That's what I meant (\delta=5 is best at T=0 and T=0.3). 
% I changed the text to make it clearer.

% DONE: The sentence about "similar to...6" looks obsolete to me,
% as you're no longer recommending or mentioning window size 6,
% so I deleted it.  If it's not obsolete, it needs to be explained
% better.

% It is obsolete, I removed it.


\smallskip\noindent{\bf Indistinguishability.}
%
Our results highlight the shortcomings of designing a watermarking scheme with provable indistinguishability.
Distribution shift does not guarantee indistinguishability, but by our metrics mostly outperforms the other three schemes, 
which do provide provable indistinguishability, and achieves very high quality scores.
The other three schemes especially struggle with lower temperature settings. 
This is expected, since the entropy of the distribution they are mimicking converges to 0 as the temperature decreases, 
which leaves them with little maneuvering room to choose alternate text for watermarking purposes.
In contrast, the distribution shift scheme is willing to make minor changes to the output distribution to support watermarking.
We show that this does not affect quality much.

We suspect there are two key factors that enable distribution shift to perform better: 1) the freedom to change the 
output distribution, in a way that does not affect semantic meaning too much, offers many more possibilities to 
embed a hidden watermark; 2) distribution shift works with the unmodified logits, before temperature scaling, 
so it has the option to boost the chance of selecting the second- or third-best token, whereas the other schemes 
work with the output probability distribution after temperature scaling, which contains less information than 
the unmodified logits (e.g., at temperature 0, it is impossible to identify the second-best token from the 
output probability distribution).
Future work on watermarking might benefit from these insights, by focusing on quality rather than 
indistinguishability and taking advantage of the information in the logits.

% DONE: See if you agree with the stuff I added at the end of the paragraph.
% I do!


\smallskip\noindent{\bf Text-dependent vs. fixed randomness.}
%
In~\cref{fig:text-dependent_fixed_randomness}, we plot the minimal watermark size for the same quality and 
tamper-resistance thresholds but vary the watermarking scheme's source of randomness instead of the 
scheme.\footnote{We exclude text-dependent randomness with a window size of 0 or fixed randomness with 
a key length of 1 from the analysis, since these corner cases are identical and correspond to always using 
the same random value.} Text-dependent randomness uses a keyed hash of the preceding tokens, whereas fixed 
randomness uses a fixed random value at each position.
\cref{fig:text-dependent_fixed_randomness} shows that watermarking employing text-dependent randomness 
achieves greater efficiency.
%The use of fixed randomness was introduced in~\cite{kuditipudi_robust_2023}, aiming to achieve distribution indistinguishability with an unwatermarked LLM, as well as robustness. However, our findings suggest that prioritizing distribution indistinguishability might not be the most effective strategy when the primary goals are maintaining quality and improving efficiency, and as we show in~\cref{robustness-edit}, the increased robustness introduced in this work comes at a high efficiency cost. 

\begin{figure}[th]
    \centering
    \includegraphics[width=1\linewidth]{figures/fig2.png}
    \caption{Size versus quality of all parameter settings at T=1. Hue indicated tamper-resistance: the darker the color, the more tamper-resistant. Distribution-shift is the most tuneable scheme. The quality spread of the exponential scheme is due to low entropy randomness of some settings affecting the model's quality.}
    \label{fig:all_params}
\end{figure}

\smallskip\noindent{\bf Tunability.}
%
We plot all quality/size/tamper-resistance tradeoffs attainable, at a fixed temperature ($T=1$), in~\cref{fig:all_params}.
Each data point represents a combination of watermarking scheme and parameter setting.
Distribution-shift is the most tunable scheme: by adjusting the bias parameter, one can sacrifice quality 
for watermark size, something that is not possible for other watermarks.
The exponential scheme shows significant spread in quality because some parameter settings do not 
provide enough entropy (e.g., too small sliding window).
We describe how these schemes can be tuned.

\begin{figure*}[th]
\includegraphics[width=\linewidth]{figures/robustness_per_attack.png}
\centering
\caption{The effectiveness of each attack, averaged over selected parameters. We show the attack success rate and the relative quality of the text after it has been perturbed by the attack. The labels indicate the parameters for each point, the color indicates the class of attack.}
\label{robustness-all-attacks}
\end{figure*}

\subsection{Parameter Tuning}
\label{ssec:param_tuning}

\begin{table}[b!]
\centering
\caption{Correlations between parameters and metrics.\\
+ indicates a positive correlation, - a negative correlation, $\perp$ no correlation. 
Symbols in red indicate a strong effect of the parameter on the given metric.}
\label{parameter-effects}
\small
\begin{tabular}{|l||r|r|r|r|r|} 
\hline
Parameter & \makecell{Window\\Size} & \makecell{Min\\Hash} & \makecell{Skip\\Prob} & Bias & \makecell{Key\\Length} \\
\hline
\hline
Quality & + & $\perp$ & + & \textcolor{red}{\bf{-}} & + \\
\hline
Size & + & + & + & \textcolor{red}{\bf{-}} & +\\
\hline
\makecell{Tamper\\Resistance} & \textcolor{red}{\bf{-}} & \textcolor{red}{\bf{+}} & $\perp$ & \textcolor{red}{\bf{+}} & \textcolor{red}{\bf{-}}\\
\hline
\hline
\makecell{Suggested\\setting} & 3 & False & 0.025 & $\geq$2 & 4\\
\hline
\end{tabular}
\end{table}


%We now look at the impact of each parameter on the three individual metrics: quality, size, and 
%tamper-resistance. We are no longer concerned by the choice of overall scheme or temperature, but instead want to 
%understand how to tune the performance of the watermarking scheme. 

% DW: I find it boring to list how each parameter increases or
% decreases each metric (I think that's probably obvious if you
% think about it, and I don't think it is terribly actionable or useful).
% I would suggest making this section more concise and focus on
% the interesting findings.

\smallskip\noindent{\bf Window size.} 
Increasing the window size increases quality, but also increases the watermark size and decreases robustness.
We recommend using a window size of 3; larger window sizes do not improve quality further.

%Because quality stabilizes for window sizes greater 
%than 3, we recommend using a window size of 3. Window sizes of 0, however, have lower quality, but are also hard to 
%watermark: over half generations at a window size of 0 do not have a watermark. Tamper resistance is also affected, 
%due to the high size. Using a short window size leads to repetitions in the hashing seed used for token selection. 
%This breaks the independence assumption, leading to a decrease both in watermarking performance and quality. However, 
%window sizes of 0 can still be useful with the distribution-shift scheme. In this case, quality is lower than other 
%window sizes, but tamper resistance increases, while size remains the same as for window size 1. 

\smallskip\noindent{\bf Min hash.} 
At a window size of 3, the min hash increases both tamper resistance and size by 33\%, compared to a sliding window.
We recommend using a simple sliding window.

% DONE: I don't understand all of the rest of the paragraph.  I deleted it.
% It seems not very interesting.  Check to see if I'm missing something.

% DONE: check my recommendation, which is the opposite of what
% was previously written.

% DONE: I don't understand why you recommend using min hash.
% This doesn't seem consistent with what comes earlier in the paper.
% It is apparently not what is reported in figure 1, or listed as
% the parameter setting in Section 6.1.
% It seems out of place to recommend it here but not use it elsewhere.
% Also, the increase in size seems unfortunate, and it sounds like
% provides more tamper-resistance than your 20% threshold, so
% I don't understand the justification for using it.
% If you still think we should recommend use of it, let's discuss
% to understand the reasons why, so we can explain this better.

% i think your recommendation makes sense. 

%Using the minimum of the previous hashes as a seed instead of the sliding window does not affect quality. 
%As the window size increases, using min hash increases tamper resistance, but also increases size.
%At a window size of 3, min hash increases both tamper resistance and size by 33\%.
%As window sizes get longer, each token belongs to the hashing window of more tokens. If that token's 
%hash happens to be particularly low, it means consecutive tokens will have the same seed. This breaks the 
%independence assumption used for detection, making watermarking harder. Nevertheless, we recommend using 
%min-hash to increase tamper resistance. 

\smallskip\noindent{\bf Skip probability.} 
We recommending setting the skip probability at 0.025 
for indistinguishable schemes (exponential, binary, inverse transform) 
using text-dependent randomness, 
as this adds non determinism to their outputs. 
This also provides slightly better quality than 0.

% DONE: Please check my rewrite above.
% I don't understand why you need to explain what the impact is,
% or why you recommend "0.05 if needed" (how would anyone know
% better than you whether it is needed? your benchmark should
% have the information about whether it is needed, so make the
% recommendation, don't leave it up to the reader).
% If you want to mention a non-zero skip probability is necessary
% for deterministic schemes, please add a parenthetical listing
% which schemes you consider deterministic, as that is not stated earlier.

%Increasing the skip probability leads to a slight increase in quality, at the cost of an increase in size and decrease in tamper-resistance. 
%However, it can be necessary for deterministic schemes to induce some randomness in the outputs. We recommend setting it at 0.025 or 0.05 if needed. 

\smallskip\noindent\textbf{Bias.} 
For distribution-shift, the bias ($\delta$) is a critical parameter
that has a large impact on quality, size, and tamper-resistance.
Figure~\ref{bias-fig} visualizes its impact.
We suggest choosing $\delta$ based on the most common
temperature that the model is likely to be used with.
In all cases, only values of $\delta \geq 2$ yield efficient schemes.

% DONE: Check my rewrite of this section.
% Check if bias is indeed $\delta$ and indeed only relevant
% to distribution-shift.
% The recommendation to use 2 doesn't make sense to me given
% that the previous section suggests 2.5, 4, or 5, depending
% on temperature.

% The recommendation is "use a value above 2" 

%As the bias increases, the quality decreases as well since the next token distributions are altered, but tamper-resistance increases and size decreases. 
%In fact, for a bias strictly lower than 2, over half the generations were not watermarked, leading to an infinite size. 
%For values of 2 and above, the trade-off between quality and size is shown in Figure~\ref{bias-fig}, at temperatures T=1 and T=0.3. 
%Lower temperatures (T=0 and T=0.3) saw a smaller quality drop for higher biases than the high temperature models. We recommend using a bias of at least 2, 
%but we were able to use biases up to 5 with no significant quality drop for lower temperatures models. 

\smallskip\noindent{\bf Greenlist size.}
Our early experiments showed that changing the greenlist size ($\gamma$) from 
0.5 only negatively impacts the watermark in all three metrics.
Therefore we fixed $\gamma$ to 0.5 for our experiments and suggest practitioners do the same. 

\smallskip\noindent{\bf Fixed randomness.} 
When using fixed randomness, we recommend using key length $L$ of 4.
Smaller values are detrimental to quality, and larger values are worse
for tamper-resistance (see \cref{keylen-fig}).

% DONE: Check my rewrite of this recommendation.
% Please check whether 4 is indeed the best choice.
% Which is chosen, when you choose the ``optimal'' parameters
% for computing the aggregate metric?
% I suggest that you take a stand in your recommendation.
% Recommend 4, or recommend 8, but don't recommend "4 or 8"
% (don't leave it up to the reader to pick).
% You are in a better position to judge which is best than
% anyone else, based on the benchmark data.

% 4 is good! 


%When using fixed randomness, key lengths $L$ under 2 are detrimental to quality.
%For lengths of 4 and above, we notice quality remains almost constant. However, increasing key length 
%leads to an increase in size, and a decrease in tamper-resistance, both for the align metric (shown in Figure~\ref{keylen-fig}), and for 
%the edit distance metric (discussed in \cref{ssec:robustness-eval}). We recommend using a key length of 4 or 8 for a good compromise.

\smallskip\noindent{\bf Summary.}
Table~\ref{parameter-effects} summarizes the effects of each parameter on the metrics.
For distribution-shift, the bias is the most critical parameter, and it allows considerable 
control over the quality vs.\ size tradeoff, making distribution-shift more tunable than the other schemes.
Consequently, in applications that are willing to can accept degradation to quality in exchange for 
better size or tamper-resistance, distribution-shift is an excellent choice.

\begin{figure*}[h!]
    \includegraphics[width=\linewidth]{figures/robustness_per_scheme.png}
    \centering
    \caption{Success rate of various attacks against the two most competitive schemes at optimal paramters from~\cref{fig:aggregate}. 
    (Minimal size at 99\% quality and 20\% tamper-resistance). On the left, the GPT parahrase attack. On the right, the Russian translation attack. 
    The size of the watermark (median number of tokens needed for the watermark to be detectable) is indicated next to each point.}
    \label{robustness-per-scheme}
\end{figure*}

\subsection{Tamper-Resistance}
\label{ssec:robustness-eval}

%We quantify tamper-resistance using an area under the curve metric. Although it allows for easy comparison, 
%it is not an easily interpretable metric. We ran additional paraphrasing attacks on the subset 
%threshold-optimal parameters (parameters that minimize size or maximize tamper-resistance at 
%thresholds defined in~\cref{ssec:aggregate}). 
%
%We first discuss attack effectiveness and then analyze tamper-resistance of the optimal parameters 
%from~\cref{fig:aggregate}. We show that by optimizing for tamper-resistance 
%instead of size, we can decrease the success rate of the most powerful attack while maintaining quality, 
%with a small increase in size.  

\noindent\textbf{Attack strength.}
We analyze the tamper-resistance of schemes with the optimal parameters  
from~\cref{fig:aggregate}, evaluating the success rate of each
individual attack to identify which attacks are most successful.
Figure~\ref{robustness-all-attacks} reports the average quality and success rate of each attack.
Paraphrasing and translation are the most effective attacks:
they remove the watermark most often, and do not heavily affect quality.
Using Dipper to paraphrase is only slightly more effective than translation attacks. Translation attacks are easier to run since they 
either rely on smaller language models,
or can be done with online translation services such as Google Translate~\cite{googletranslate}. 

\smallskip\noindent\textbf{Paraphrasing and Translation attacks.} 
Figure~\ref{robustness-per-scheme} shows the attack success rate of the
GPT paraphrasing attack and the Russian translation attack against the two best watermarking schemes.
The evaluated watermarks are those with the optimal parameters from~\cref{fig:aggregate}.
The left side of the plot shows the success rates of the GPT attacks.
In all cases, at least 60\% of attacks are successful.
The non-trivial success rate is unsurprising, since 
the parameters were chosen primarily to minimize size, not tamper-resistance.
The right side of the plot shows the success rates of the translation attack. The attack 
is less successful, getting under 40\% success rate at all but one temperature.
The success rates of both attacks are correlated: settings that are robust against one are 
also against the other.

% \smallskip\noindent\textbf{Greater tamper resistance.}
% We show that it is possible to achieve greater tamper-resistance,
% with only a modest cost to other metrics.
% The right side of Figure~\ref{robustness-per-scheme}
% shows alternate parameters designed to
% improve attack resistance (all using  text-dependent randomness
% with a sliding window size of 1).
% Specifically, for temperature $T=0.3$, the success rate of paraphrasing
% attacks drops from about 90\% to 30\%, and size (number of tokens
% needed to detect the watermark) increases only a little, from 63 to 75.

%The relatively low size of tamper-resistance schemes is expected. Tamper-resistance depends on two 
%factors: the density of the mark (e.g. how strongly is each word watermarked) and the dispersion 
%of the randomness source (e.g. how many random pseudo-random values are changed when one token is altered, 
%inserted or removed). Low density means an attacker can change few tokens to get the 
%score under the detection threshold. High dispersion means an attacker can change few 
%tokens to change the pseudo-random values for most tokens. 

%High density watermarks are easily detected, thus have low sizes. 
%Randomness sources with small window sizes have low dispersion. Tamper-resistant schemes need 
%a combination of both: low size and a small window size.
