\section{\benchmarkname{}}
\label{sec:experiments}

% DONE: I think it might help to have a name for the benchmark,
% and to use that throughout the paper.  Then others who use the
% benchmark can refer to it by that name.

% We can call it MarkMyWords

% Our benchmark measures a watermark's quality, efficiency, and tamper-resistance in various contexts.
%
We now present \benchmarkname{}, our benchmark for evaluating watermarking schemes.

\subsection{Text Generation Tasks}

%Our benchmark relies on three tasks, all intended to generate long-form text, while also representing real scenarios in which one would wish to watermark the model outputs.
%

\benchmarkname{} uses three text generation tasks.
There were chosen to generate long text, in order to ensure enough tokens to watermark most outputs and get a good size estimate,
and to represent realistic scenarios in which watermarking would be useful.
We watermark the outputs of each task and measure quality, watermark size, and tamper-resistance on these outputs.
\cref{tab:bench-prompts} shows the prompt for each task.

\smallskip\noindent\textbf{(1) Book reports.}
%
Our first task instructs the model to generate book reports for a predefined list of 100 well-known books.
%
This represents a realistic use case of a watermark in an education context where instructors might wish to verify if students have used a language 
model to complete their homework assignment.
%
% Our prompt format is ``Write a book report about X, written by Y.'',
% where X is a book title and Y is the book's author. 

\smallskip\noindent\textbf{(2) Story generation.}
%
We ask the model to generate a short story, with a specific tone (e.g., funny, sad, suspenseful), on a particular topic (e.g., ``three strangers that win a getaway vacation together'').
%
We use 100 combinations of topics and tones.
%
In this case, detecting watermarks in creative content is a way to trace the authorship of a 
piece of text.
% Our prompt is ``Write a X story about Y.'', where X is the tone, and Y is the topic.

\smallskip\noindent\textbf{(3) Fake news.}
%
Our last task asks the model to generate a news story about two prominent political figures meeting at an event.
%
In total, we have a list of 100 unique combinations of political figures and locations.
%
Detecting watermarks in short news articles or headlines could be useful for fighting against automated fake news bots on social media.
%
% Our prompt is ``Write a news article about X's visit to Y in Z.'', where X and Y are two political figures, and Z is a location.

\subsection{Benchmark Implementation}

We implemented the \benchmarkname{} benchmark in Python using the \texttt{transformers} library~\citep{wolf_transformers_2020} to implement models and watermarks.
%
In general, the run times of the watermarked models are comparable to those of base models.
%
We rely on the 7B-parameter version of Llama-2 chat both for generation and rating.
%
We chose this model for its high quality and because its output distribution, like other modern models, has low entropy.
We've noticed newer language models are more certain of their next-token predections, i.e., have lower entropy in their output distribution,
which makes watermarking more difficult. Thus, using a low-entropy model makes for a more realistic 
evaluation of a watermark scheme's performance. 
%
This version of the Llama-2 model is known for often refusing to generate content---we 
altered the system prompt to avoid this issue, especially for the fake news task.

Our benchmark generates a total of 300 outputs from the language model.
%
We only attack a third of these (the first third of each task),
as running robustness tests on all 300 would make the benchmark too slow.
%
The maximum number of generated tokens per prompt is 1024.
%
On a A5000 GPU, the benchmark completes in 30 minutes to an hour. 

%Users can choose whether to sample a new secret watermarking key for each generation or not.
%
%We enable this in our experiments, but practitioners can deactivate it in order to evaluate a model's performance for a specific secret key, for instance when evaluating a public scheme.

Our code for the \benchmarkname{} benchmark has been made public.\footnote{\mylink{https://github.com/wagner-group/MarkMyWords}}
%
It is easily updated to support new watermarking schemes or new language models.
%Anyone can run it, either to compare new watermarking schemes to the ones presented in the paper, or to find the best hyper-parameter 
%selection for another language model. 
%
We hope that future proposals of watermark schemes will include an evaluation with this benchmark.

\smallskip\noindent\textbf{Efficiency.}
%
We have developed custom implementations directly in CUDA of some of the watermarking schemes, to speed up computation.
\begin{itemize}[leftmargin=\itemlm,nosep]
    \item \textbf{Hash function.} The exponential sampling scheme relies on computing the hash of many elements. Our CUDA implementation allows this to be done in parallel.
    \item \textbf{Edit distance.} The edit distance is computed with every possible key offset; our code implements this in parallel.
\end{itemize}
% Additionally, for empirical verification of edit scores, since we use a $t$-test~\cite{kuditipudi_robust_2023}, we need to compute test statistic 100 times under the null hypothesis to derive an empirical $p$-value.
% %
% This is done by replacing the randomness used at generation time by fresh random 
% values.
% Doing so for each detection is inefficient.
% %
% Instead, for edit scores, we pre-compute the test statistic under the null hypothesis 200 times and then sample 100 random values of this statistic every time we want to verify a watermark.
% \chawin{I'm a bit confused by this part.}
% %
% This means we only need to run the expensive computation once.
% %
% Empirically, we did not see a change in detection accuracy when doing so. 
