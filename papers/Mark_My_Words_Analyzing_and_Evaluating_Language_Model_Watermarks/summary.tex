\section{Summary}

Our empirical analysis demonstrates existing watermarking schemes are ready for deployment, 
providing effective methods to fingerprint machine-generated text. Notably, we can watermark Llama 2, 
a low-entropy model, with minimal quality loss, and detect the watermark with 100 tokens or less.
We also show that they provide a non-trivial amount of tamper-resistance against simple attempts to remove the watermark.

We challenge the perceived necessity for watermark indistinguishability: 
the solution proposed in~\citet{kirchenbauer_watermark_2023} can watermark models more efficiently than alternatives 
without degrading the model's quality, despite not being provably indistinguishable. 

Finally, we provide \benchmarkname{}, a benchmark to compare existing
and future watermarking schemes.
We release our code in the hope it facilitates evaluation of watermarking
schemes and advances the ongoing discussion on
the desirable properties of watermarking schemes for large language models.
