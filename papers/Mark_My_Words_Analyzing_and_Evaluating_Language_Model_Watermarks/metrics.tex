\section{Evaluation Metrics}
\label{sec:metrics}

We propose three metrics for evaluating watermarking
schemes: (a) quality, (b) watermark size, (c) tamper-resistance.
We also propose an aggregate metric that summarizes the
performance of a watermarking scheme in a single number.

%Designing good metrics for measuring the effectiveness of a watermarking scheme is not straightforward.
%
% We want to make sure our metrics are meaningful, i.e. they quantify a desirable property of 
% the scheme. Nevertheless, perfect metrics do not exist. Metrics by definition reduce complex 
% multivariate problems to a set of indicator values. This reduction focuses on select properties 
% of the problem, but each user might care about different properties. Furthermore, metrics are 
% often imperfect representations of a property. Quality of generated text, for instance, is not 
% a well defined property. Any attempt at measuring quality will rely on additional assumptions. 
%The metrics we define are our best effort at summarizing the key properties of quality, watermarking abilities, and tamper-resistance into values that also have low bias and variance.
%
%We strive to understand the existing biases in our metrics, in order to better interpret results. 
%
%In this section, we define our three core metrics and finish with an aggregate metric that summarizes the performance of watermarking schemes into a single number.
%
% Note that these metrics are only valid 
% when applying a watermarking algorithm to a specific language model at a given temperature --- schemes 
% can only be compared when benchmarked on the same model, comparing metrics for benchmarks on different language models is meaningless. 

\subsection{Quality}

First, we evaluate the quality of watermarked output,
to measure how much the mark degrades the utility of the output.
We build a suite of tasks that language models might be used
for, and compare the quality of watermarked outputs on these
tasks to the quality without watermarking.

The gold standard for evaluating quality uses
a diverse set of human evaluators to grade responses.
%Evaluating the quality of open-ended generations remains an open question.
%
%There are many benchmarks aimed at evaluating the quality of short answers to specific tasks, such as sentiment analysis or 
%multiple-choice questions.
%
%However, when it comes to evaluating the quality of open-ended 
%generation, the ideal evaluation uses a diverse set of human evaluators to grade responses.
%
%Part of the reason this task is so hard to automate is that the answer is often subjective: it is difficult to say what makes a response better than another.
%
However, human evaluation is impractical for running a benchmark, 
so we look for the next best alternative.

We adopt automated rating by a language model, a
standard approach used in NLP for evaluation~\cite{geval,naismith23rating,chiang23rate,hackl23rating,wang23rate,kocmi23rate,chen23rate}.
%
Specifically, we use zero-shot prompting of Llama-2 (7B, chat version)~\citep{touvron_llama_2023} to rate watermarked outputs.
%
We provide Llama2 with the prompt of the original task and the watermarked response, and we ask it to rate the response on a scale of 100 points (with greedy decoding for determinism).
%
This takes into account the relevance of the answer (i.e.\ is this a good response to the prompt?), as well as English fluency.
%
% We always use the same model with greedy decoding, as to have reproducible results. 
%
% Using this rating comes with its own considerations. This is by no means a measure of undetectability. Instead, it's a measure of the text generation capabilities of a model. 
% Rating is subjective: the value of a rating does not have much meaning, as different people or models would have graded them differently. It is biased by the model's training, and by the prompt we give it. However, comparing ratings given by the same model for different generations does shed light on the relative performance of two models. 
We emphasize that this rating does not have any special meaning in itself,
so it can only be used to compare the relative quality of watermarked
output vs unwatermarked output.
%
%This is especially true when averaging the rating over many generations, as variance in individual results tends to disappear and makes ratings appropriate for our goal---comparing the quality of watermarked models to the original.

One might be skeptical of using Llama2 since it's a smaller model than the ones typically used for rating.
%
However, the tasks we ask it to rate are simple, and Llama2 appears to rate accurately on these tasks.
%
Ratings from Llama2 are correlated to GPT3.5's rating ($R^2$ of 0.97 for average ratings on \benchmarkname{}, without perturbations), giving us faith in Llama-2's ratings, and enabling our benchmark to be run entirely locally without querying external APIs.
%
% Finally, we use the same model for running the benchmark as we do for rating --- in practice, this does not seem to introduce additional bias, as we found the grades to be similar when asking GPT-4 to rate generations instead. 
In the rest of this work, we will refer to the quality metric as $Q$. 

We also experimented with
model perplexity, often used in the field of NLP
as a proxy for content quality, but found that it performed worse.
%
The perplexity of a text $T$ is the likelihood of that text,
under another language model.
%
%We experimented with various models for measuring perplexity.
%
%In general, using state-of-the-art models for evaluating perplexity, such as 
%OpenAI's \texttt{davinci-003}, provides better results.
%
Unfortunately, perplexity tends to favor repetitive text~\citep{holtzman_curious_2020,welleck_neural_2020},
causing anomalies in our results.
%
For example, we observed one output where the model produced a partial response followed by many repetitions of the letter `l'; the perplexity metric rated this response highly, but Llama2 automated rating correctly rated it poorly.
In fact, the $R^2$ between perplexity and GPT3.5 is only 0.84, significantly lower than Llama2 versus GPT3.5. 

% DONE: Did you measure correlation between perplexity vs GPT-4 rating?  I think reporting that correlation metric might help make a persuasive case for "why not perplexity?".

%We compared our GPT-3.5 based quality rating to traditional perplexity rating. To do so, we generated 
%text using multiple models, computed its logarithmic perplexity of generations OpenAI's davinci-3, and 
%computed its rating using GPT-3.5. We found that highly rated samples always had low perplexity, however 
%low perplexity can lead to low rating: Repetitive text can be highly predictable (thus having low %perplexity), 
%but have a low rating. In one extreme case, a language model asked to invent a new constitution produced 
%some sentences with random words from the actual constitution, followed by an infinite repetition of the 
%letter 'l'. The log perplexity of this generation was only of 1.7, but it obtained a rating of 0.1. 

%Thus, we found in our experiments that GPT ratings are a better measure of quality than perplexity, 
%because it is not biased in favor of repetitive content. In our benchmark, we compute the average 
%rating of generations to provide a single quality metric. 

%\begin{figure}[t]
%\includegraphics[width=\textwidth]{figures/quality_metric_choice.png}
%\centering
%\caption{Log perplexity to rating for 300 samples of generated text.}
%\label{quality-rating-fig}
%\end{figure}

\subsection{Watermark Size}

The longer the text, the easier it is to watermark it and detect the watermark, as there are more degrees of freedom to inject a mark.
Therefore, a critical metric is: how long must the generated text be, so that we are likely to be able to detect the watermark in it?

The verification algorithm can make two types of errors: false positives occur when a text is detected as watermarked when in fact is it not, while false negatives occur when a watermarked text is not detected. 
%
All schemes we consider rely on one-tailed statistical tests, so we can precisely control the false positive rate (by setting the $p$-value threshold).

% DONE: Check whether the 50% below is correct.  I believe it is,
% due to the use of the median, but please verify. --DW
% I find the 50\% part a bit confusing. It's correct though, if there are less than 50% false negatives, the median will be beloy +\infty. If it's above, i'll be +\infty. --JP
We define the watermark size to be \textbf{the number of tokens needed to detect the watermark, at a 2\% false positive rate}.
%
We measure it as follows: for each output
from the benchmark, we run the verifier on increasingly long prefixes of
the output until we find the shortest prefix that is detected as marked,
then we compute the median of the lengths of these prefixes.
(If no prefix is detected as watermarked, we use $+\infty$ as the length.)

%If no watermark is detected in the entire generation, we set the 
%number of tokens to $+\infty$.
%
%The watermark size over the full benchmark takes 
%the \emph{median} number of tokens needed to detect a watermark: if over half of all generations are not watermarked, the median is $+\infty$.

We considered an alternative metric: the percentage of outputs
from the benchmark that are detected as watermarked.
However, this metric is heavily influenced by the length of outputs
from the language model and task.
If most outputs are too short, then most texts will not be detected
as watermarked, yielding a poor value of these metric even though
there is nothing wrong with the watermarking scheme.
Our metric has the benefit of providing an interpretable result
for practitioners: it indicates how long text needs to be, to have
a good chance we will be able to detect whether it was AI-generated.

%A more common metric for the watermark's efficiency is the percentage of generations whose watermark is detected.
%
%However, this metric relies more heavily on the length of generations. 
%
%A model that produces shorter text will tend to have a lower percentage of watermarked generation.
%
%Our proposal, on the other hand, is not affected as much by shorter generations and has the added benefit of providing an interpretable result for practitioners: the size of a watermarking 
%scheme gives an idea of how long text needs to be to be watermarked.
%
%The low median number of tokens represents more efficient schemes. 
%

We refer to the watermark size as $E$.
Smaller values of $E$ indicate better, more efficient watermarking schemes.

\subsection{Tamper-Resistance}

\begin{figure}[t]
\includegraphics[width=\linewidth]{figures/robustness.png}
\centering
\caption{Example tamper-resistance plot. Blue points are attacks on the \emph{convex hull frontier}. The blue area represents the area under the curve, the dotted lines represent the best attack that keeps quality 
within 80\% of the original. }\label{robustness-example-fig}
\end{figure}

We measure a watermarking scheme's tamper-resistance (robustness to attacks that seek to remove the mark) using 8 attacks, described in \cref{sec:attacks}.
%
These attacks are simple to carry out for casual users.
%
We run each attack on all outputs generated from the benchmark.
For each attack $A$, we compute two quantities:
$Q_A$, the fraction of quality that is retained after the attack
(a measure of how much the attack degrades quality),
and $W_A$, the percentage of outputs that are detected
as watermarked despite the attack (a measure of how effective the
watermarking scheme is at preventing the attack from
removing the watermark).\footnote{We use the proportion of 
watermarked generations instead of the watermark size $E$
because many attacks remove 
the watermark from over 50\% of generated texts, in which case all these 
attacks would have a size of $+\infty$.}
We compute $Q_A$ as $Q_A=\max(0,\min(Q^*_A/Q,1))$ where
$Q^*_A$ is the average quality of the attacked outputs and
$Q$ the average quality of unattacked outputs.

% DONE: Just checking that $W_A$ is not similarly a ratio
% between watermark percentage after attack vs before attack?
% Correct, it's not, because otherwise schemes that cannot watermark tend to have 
% high tamper-resistance (say for example there is only one watermarked generation, 
% then if attacks fail against it tamper resistance will be maximal)

We can visualize these attacks in a two-dimensional plot,
with one point $(Q_A,W_A)$ per attack,
depicting the retained quality versus robustness.
%
We show an example plot in~\cref{robustness-example-fig}.
%
These two quantities are a trade-off for the adversary;
attacks that disrupt quality more can potentially be more
effective at removing the watermark.
%points
%in the lower-right represent powerful attacks that 
%are effective removing the watermark and do not disrupt
%the meaning or quality of the text.
%
%We draw the convex hull (depicting Pareto-optimal attacks).
%
%The more this curve approaches the upper-left corner,
%the more robust the watermarking scheme is against attack.

%Strong attacks reduce the number of watermarked generations as well as their quality. %, i.e., larger $\epsilon$.
%
%The best attack is the one that removes all the watermarks without hurting the text quality.

% We add two points to this plot, corresponding to two edge cases: 
% the first is the non-attack, in which the output is the same as the input: quality is maximal, but 
% all watermarked generations are still watermarked, so its coordinates are $(1,W)$, with $W$ the 
% original proportion of watermarked generations. 
% The second is the most destructive attack, in which we return an empty string regardless of the 
% input. Quality goes down to 0, but so does the watermarking percentage, so its coordinates are 
% $(0,0)$.

% This plot gives an overview of the robustness of a scheme against a set of attacks.
%
% One could compare plots for two schemes to get an idea of which is more robust, however boiling down to a single number makes it easier to compare.
It is desirable to reduce the plot into a single number to facilitate comparisons between watermarking schemes.
%
% The first strategy is to set a threshold on how much an attack can degrade quality (say 80\% of the original) and return the watermark percentage of the most successful attack (i.e., the lowest watermark percentage) under this quality threshold.
We considered reporting the value of $W_A$ for the strongest attack that retains at least 80\% of quality, i.e., $\max \; \{W_A \mid Q_A \ge 0.8\}$.
%
Although this metric is interpretable, it relies on an 
arbitrary threshold and is noisy due to the interpolation between a sparse set of data points.
%
%When running the same benchmark multiple times, a small change in the selected attack's quality can lead it to drop under the threshold, meaning an entirely different attack will now be optimal.
% , with a potentially very different percentage of watermarked outputs. 

Instead, we opt for a different metric: the \textbf{area under the curve (AUC)}.
%
The closer a point is to the lower right, the more successful the attack is; the farther the points are from the lower right, the more robust the watermarking scheme is.
%
By taking the convex hull of the set of points, 
we define a curve in the plot ranging from $(0,0)$ to $(1,1)$ representing the attacks with the best possible tradeoff between retained quality vs effectiveness.
(Any point on the hull frontier is attainable by sampling between the two closest attacks on the hull.)
%
% The larger the area under this curve (i.e. the volume between this curve and point $(1,0)$), the more robust the scheme is.
The larger the area under this curve, the more robust the scheme is.
%
The area cannot exceed 0.5: attacks at 
$(0,0)$ and $(1,1)$ are always possible, so the minimal area is the half-space below these points.
%
Therefore, to normalize the metric, we define the tamper-resistance $R$ of a watermarking scheme to be twice the area under the curve.

This metric has a similar intuition to the area under the curve measured in ROC curves in binary classification~\citep{fawcett_introduction_2006}.
%
Although it lacks interpretability, it does not depend on an arbitrary threshold and is much less noisy than the first proposal. 
In our experiments, the standard deviation of $R$ was usually an order of magnitude lower than that of the threshold-based metric.
% , and is non-parametric.
%
% The AUC does depend on the set of attack in the benchmark, but as long as this set remains the same from one evaluation to another, the AUC can be compared between schemes. 
%We name the tamper-resistance AUC $R$ in the remainder of the paper. 

% TODO: I don't understand why you remove paraphrasing but not translation.
% The same comments below apply equally to paraphrasing and translation.
% To me it seems like a more reasonable dividing line is to avoid
% reliance on external models, so don't include GPT3.5 paraphrasing
% in the benchmark, but do use open models that can be run locally,
% so do include Dipper paraphrasing and argos-translate translation
% attacks in the benchmark.  Am I missing something?  Is there some
% explanation that will make more sense here?  --DW

% Originally, I didn't know translation models were based on transformers, 
% I thought google translate relied on older methods. But it's true that in 
% the current version of the benchmark I'm using a language model for translation 
% (even though it's much smaller than GPT or Dipper), because using google translate 
% was going to be too expensive. I think the main reason is complexity: The 
% translation model is very fast, but both GPT and Dipper require additional 
% resources (either to have multiple GPUs, or to pay OpenAI for the perturbation) and are slower.

Our basic benchmark excludes paraphrasing attacks, because they rely on large or closed sourced language models, which can be difficult to obtain, require expensive resources to run, or can be watermarked themselves.
Translation also relies on a language model, but since it is smaller, it can run faster and cheaper than paraphrase models. 
%
We evaluate paraphrasing attacks in~\cref{ssec:robustness-eval}.

Finally, we found the AUC to be highly correlated to the success rate of different attacks. 
We leave the detailed plot to~\cref{app:ssec:additional_figures} in~\cref{fig:robustness-to-attacks}. 
We found that a tamper-resistance of 1 corresponds to less than 10\% success rate for the Russian translation attack and less 
than 40\% for the GPT paraphrase attack. 


\renewcommand\theadalign{bl}
\renewcommand{\cellalign}{bl}

\begin{table}[t]
\centering
\caption{Prompts for the three text generation tasks used in our benchmark.}\label{tab:bench-prompts}
\begin{tabular}{|l||l|l|l|l||l|} 
\hline
\textbf{Task} & \textbf{Prompt} & \textbf{P1} & \textbf{P2} & \textbf{P3} & \textbf{Count} \\
\hline
\hline
\makecell{Book\\Report} & \makecell{Write a book\\report about \textbf{P1}, \\written by \textbf{P2}.} & Title & Author & & 100\\
\hline
Story & \makecell{Write a \textbf{P1} story\\about \textbf{P2}.} & Tone & Topic & & 100\\
\hline
\makecell{Fake\\News} & \makecell{Write a news\\article about \textbf{P1}'s \\ visit to \textbf{P2} in \textbf{P3}.} & Person & Person & Place & 100\\
\hline
\end{tabular}
\end{table}


\subsection{Aggregate Metric}\label{ssec:aggregate}


% Quality, watermark efficiency, and robustness give us three numbers to evaluate a single benchmark.
% Making a comparison with three competing metrics (quality, watermark efficiency, and robustness) can be challenging. 
%
% However, a single benchmark evaluates a specific set of hyperparameters.
We characterize any instantiation of a watermarking scheme---with a fixed single set of parameters---with these three metrics.
Of course, by sweeping the parameters, it is often possible to trade off between these three metrics.
%
% We can use these three metrics to compare two schemes for chosen parameters, but we cannot use them to compare two schemes regardless of parameter choice. 
In some settings, it is useful to compare two watermarking schemes \emph{overall}, i.e., choosing ``optimal'' parameters for each scheme.
%
This is challenging because there is no single definition of the optimal tradeoff.

%As remarked in the previous section about tamper-resistance, one strategy to compare two schemes is to compare their scatter plots.
%
%By projecting points onto a two-dimensional space (by ignoring tamper resistance, for example), we can compare the scatter plots built of individual parameter settings.

%However, it is easier to compare a single metric. We considered multiple options and decided to use a threshold-based metric.
%

% DONE: Please check that below I've described how you use training vs validation set accurately.
% Perfect! --JP

We propose an aggregate metric that supports this goal.
We fix a target relative quality (i.e., watermarked outputs must achieve at least a certain percentage of the quality of the non-watermarked model) and a target tamper-resistance.
We use a training set to select the set of parameters that achieve the lowest watermark size, given these targets.
Then we report the size achieved on a validation set with these parameters.

Figure~\ref{fig:aggregate} reports this aggregate metric for all four schemes, at a relative quality threshold of 99\% (the watermark can induce at most 1\% degradation in quality) and a minimum threshold of 20\% for tamper-resistance (which correspond to at most an approximately 80\% success rate for the best attack).
%
\cref{robustness-per-scheme} and \cref{fig:aggregate2} present results for other thresholds (optimizing for size versus tamper-resistance, 1\% quality versus 10\% quality, $>0.2$ tamper-resistance versus $>0$ tamper-resistance). 
Our code allows setting arbitrary thresholds to accommodate practitioner needs. 

% DONE: I don't understand what the "we present results" means.
% One possibility: you present it in the paper.  If so, please cite
% the section or figures.
% Another possibility: you mean that the benchmark code outputs
% those numbers.  If so, I suggest deleting that sentence or
% replacing "We present" with "Our benchmark code computes"

% I added references


%Validation removes selection bias due to variance, which avoids reporting overly optimistic metrics. However, during validation metrics will change a bit, meaning the quality and tamper-resistance might no longer be within the specified thresholds.


% parameter setting $S$ that achieves the best third metric for each scheme. While this is interpretable, 
% it suffers from the same drawbacks as in the tamper-resistance setting: it is noisy, unless a great number of 
% unique parameters is tested, and relies on arbitrary thresholds. Still, we use this to report the 
% performance of schemes under specific constraints. 
% %
% However, comparing three-dimensional plots is unwieldy so a single aggregate metric is needed.
% % While there is no single definition of a ``good'' scheme, 
% We decided to consider a scheme better if (1) it can watermark more 
% efficiently for given quality and tamper-resistance requirements, and (2) it is tunable so the quality-efficiency trade-off can be adjusted.

% Based on the general guideline above, we now describe how we compute an aggregated score for each watermarking scheme.
% %
% First, we select a set of the parameter choices $\mathcal{S}$ for each scheme and run our benchmark on each choice.
% %
% This yields a scatter plot of the three metrics where each unique parameter setting $S \in \mathcal{S}$ represents a three-dimensional point $(Q_S,E_S,R_S)$.

% As we remarked in the previous section about tamper-resistance, one strategy to compare two schemes is to compare their scatter plots.
% %
% However, comparing three-dimensional plots is unwieldy so a single aggregate metric is needed.
% %
% % Such a metric must not be overly sensitive to the number of set of settings being tested, otherwise it could be biased. 
% In particular, it must satisfy the following requirements:
% \begin{enumerate}[leftmargin=\itemlm,itemsep=2pt]
%     \item The metric must increase in the number of hyperparameters $|\mathcal{S}|$. Intuitively, this is 
%     expected. When benchmarking a new set of parameters, the result can either be better than previous runs, 
%     thus improving the overall score of the scheme, or can be worse, in which case the metric should ignore 
%     it. In practice, a decreasing or non-monotonic metric would mean one can optimize the score of their 
%     scheme by only running a small number of experiments. Instead, an increasing metric is an incentive 
%     for users to test more parameters.
%     \item To prevent users with more computing power from artificially increasing the score of their scheme by running experiments on thousands of parameters, the metric must converge rapidly. It should not vary much once a reasonable number of unique hyperparameter settings $|\mathcal{S}|$ has been reached. This also ensures the benchmark can be run in a reasonable time. 
% \end{enumerate}

% Similar to tamper-resistance, one option is to set thresholds on two of the three metrics, and report the 
% parameter setting $S$ that achieves the best third metric for each scheme. While this is interpretable, 
% it suffers from the same drawbacks as in the tamper-resistance setting: it is noisy, unless a great number of 
% unique parameters is tested, and relies on arbitrary thresholds. Still, we use this to report the 
% performance of schemes under specific constraints. 

% Instead, we use a similar approach to the AUC. First, we add attainable extreme points to the plot, where 
% $Q_B$ is the quality of the language model in the absence of any watermarking scheme. 

% \begin{itemize}
%     \item Bad quality, watermarking, and tamper-resistance ($0,\infty,0$)
%     \item Bad quality and watermarking, but perfect tamper-resistance ($0,\infty,1$). tamper-resistance is not well 
%     defined for a scheme that cannot watermark, so any value between 0 and 1 is admissible. 
%     \item Bad quality, perfect watermarking, 0 tamper-resistance ($0,0,0$). This for instance could be a scheme 
%     for which the watermark consists of the input being an empty string.
%     \item Bad quality, perfect watermarking, perfect tamper-resistance ($0,0,1$). This could be a scheme that 
%     selects a random word, returns N copies of this word, and detects a watermark if the word is 
%     included in the string. Attacking it requires changing all the words in the input. 
%     \item Ideal quality, no watermarking, and no tamper-resistance. ($Q_B,\infty,0$). This is the model 
%     without a watermark.
%     \item Ideal quality, no watermarking, perfect tamper-resistance. ($Q_B, \infty, 1$). Again, since 
%     tamper-resistance is not well defined for schemes that cannot watermark, it can be set to any value 
%     between 0 and 1. 
% \end{itemize}

% Next, we normalize the points (we divide the efficiency $E$ by the maximum length of a generation, 
% and clip it to the $[0,1]$ interval), and compute the convex hull of the scatter plot together with 
% the extreme points. Any point within the hull is attainable by a linear combination of existing points. 
% Intuitively, the closer the points get to $(Q_B, 0, 1)$ (ideal quality, perfect watermarking and 
% perfect tamper-resistance), the better the scheme. Thus, the larger the volume of the hull, the better the scheme.

% As currently defined, this volume is biased by the variance of individual points. As the number of unique 
% settings $|\mathcal{S}|$ increases, because of the variance of individual points, it is likely some points 
% will fall far from their average value. Taking this to the extreme, for an infinite number of points, the 
% probability of having a point at $(Q_B, 0, 1)$ is 1, thus the volume of the hull would be maximal
% \footnote{This assumes the probability distribution over Q, E and R is defined and non zero at point 
% $(Q_B, 0, 1)$}. To cancel the effects of variance, we run every setting a first time, select the set 
% of points on the hull's surface, and run these again in a validation step with a different random seed 
% before computing the enclosed volume. 

% We show in \cref{results-convergence} that this convex hull volume converges in under 30 unique 
% hyper-parameter settings, and is increasing in the number of tested hyper-parameters, as required of a 
% useful aggregate metric. Although it is less interpretable than the threshold-based metric, it allows us to easily compare two different schemes running on the same language model.

% In the remainder of this paper, we will refer to the convex hull volume as \hullvolume.
% %
% Note that the same metric can be defined when ignoring tamper-resistance and looking at values in the $(Q,E)$ plane.
% %
% This variant of the metric is referred to as \hullarea.
