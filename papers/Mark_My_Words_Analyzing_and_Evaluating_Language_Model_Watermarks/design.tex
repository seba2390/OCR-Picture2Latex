\section{Designing a Watermark}
\label{sec:taxonomy}

After the quick overview of watermarking schemes in \cref{sec:background}, we now provide more details 
about the watermarking design space. We created a unifying taxonomy under which all previous schemes 
can be expressed. We first discuss the requirements then the building blocks of a text watermark. 
%
%We provide a modular implementation of all schemes, so any of the building blocks can be combined.
%
\cref{fig:design-figure} summarizes the current design space.

\subsection{Requirements}

A useful watermarking scheme must detect watermarked texts, without falsely flagging human-generated text and without impairing the original model's performance.
%
More precisely, we want watermarks to have the following properties.
% \begin{itemize}[leftmargin=\itemlm,itemsep=2pt]
\begin{enumerate}[leftmargin=\itemlm,itemsep=2pt]
    \item \textbf{High Recall}. $\Pr[\mathcal{V}_k(T) = \texttt{True}]$ is large if $T$ is a watermarked text generated using the marking procedure $\mathcal{W}$ and secret key $k$.
    %
    \item \textbf{High Precision}. For a random key $k$, $\Pr[\mathcal{V}_k(\Tilde{T}) = \texttt{False}]$ is large if $\Tilde{T}$ is a human-generated (\emph{non-watermarked}) text.
    %
    \item \textbf{Quality}. The watermarked model should perform similarly to the original model. 
    It should be useful for the same tasks and generate similar quality text.
    %
    \item \textbf{Robustness}. A good watermark should be robust to small changes to the watermarked text (potentially caused by an adversary), 
    meaning if a sample $T$ is watermarked with key $k$, then for any text $\Tilde{T}$ that is semantically close to $T$, $\mathcal{V}_k(\Tilde{T})$ should evaluate to \text{True}.
\end{enumerate}

\noindent
A desireable (but optional) property for watermarks is diversity. 
In some settings, such as creative tasks like story-telling, users might want the model to have the ability to generate 
multiple different outputs in response to the same prompt (so they can select their favorite).
We would like watermarked outputs to preserve this capability.
% \noindent
% In addition to these properties, another desirable property for a watermark is to 
% preserve a model's diversity. Language models tend to have diverse generated text distributions: 
% they are able to generate different responses to a same prompt. This is useful in many settings, 
% such as creative tasks like story telling, so the user can  their favorite output.

% The notion of \emph{undetectability} has been defined in previous work~\citep{christ_undetectable_2023}:
Another useful property is \emph{undetectability}, also called \emph{indistinguishability}:
%
no feasible adversary should be able to distinguish watermarked text from non-watermarked text, without knowledge of the secret key~\citep{christ_undetectable_2023}. 
%
A watermark is considered undetectable if the maximum advantage at distinguishing is very small.
%
This notion is appealing; for instance, undetectability implies that watermarking does not degrade the model's quality.
%
However, we find in practice that undetectability is not necessary and may be overly restrictive:
%
minor changes to the model's output distribution are not always detrimental to its quality.

In this paper we focus on symmetric-key watermarking, where both the watermarking and verification procedures share a secret key.
%
This is most suitable for proprietary language models that served via an API.
%
We imagine that the vendor would watermark all outputs, and also provide a second API to query the verification procedure.
%
Alternatively, one could publish the key, enabling anyone to run the verification procedure.
%
\begin{figure*}
    \begin{center}
    \begin{tikzpicture}
    
    \draw[draw=black] (0,15) rectangle ++(17.5,1) node[pos=0.5, align=center] {\Large{Watermarking Taxonomy}};
    \draw[draw=black] (0,12.75) rectangle ++(8.375,2) node[pos=0.5, align=left] 
    {\\
    \\
    \textbf{Parameters:} Key $k$, Sampling $\mathcal{C}$, Randomness $\mathcal{R}$\\
    \textbf{Inputs:} Probs $\mathcal{D}_n = \{\lambda^n_1,\, \cdots, \lambda^n_d\}$, Tokens $\{T_i\}_{i < n}$\\
    \textbf{Output:} Next token 
    $T_n \leftarrow \mathcal{C}(\mathcal{R}_k( \{T_i\}_{i < n}), \mathcal{D}_n)$};
    \draw[draw=black] (9.125,12.75) rectangle ++(8.375,2) node[pos=0.5, align=left] 
    {\\
    \\
    \textbf{Parameters:} Key $k$, Score $\mathcal{S}$, Threshold $p$\\
    \textbf{Inputs:} Text $T$\\
    \textbf{Output:} Decision $\mathcal{V} \leftarrow \text{P}_{0}\left( \mathcal{S} < \mathcal{S}_k(T)\right) < p$};
    \draw (8.75,13.75) circle (0.25) node {+};
    \draw[draw=none] (0,14.25) rectangle ++(8.375,.5) node[pos=0.5, align=left] {\large{Marking $\mathcal{W}$}};
    \draw[draw=none] (9.125,14.25) rectangle ++(8.375,.5) node[pos=0.5, align=left] {\large{Verification $\mathcal{V}$}};
    
    %%%
    
    \draw[draw=black,dashed] (0,8.75) rectangle ++(17.5,3.75);
    \draw[draw=none] (0,11) rectangle ++(17.5,1.75) node[pos=0.5, align=center] {\large{Randomness Source $\mathcal{R}$}\\
    \textbf{Inputs:} Tokens $\{T_i\}_{i < n}$\,
    \textbf{Output:} Random value $r_n = \mathcal{R}_k(\{T_i\}_{i < n})$};
    \draw[draw=black] (0.25,9) rectangle ++(11.25,2.35) node[pos=0, anchor=south west] {\textbf{Text-dependent.} Hash function $h$. Context length H};
    \draw[draw=black] (0.5,9.6) rectangle ++(10.75,0.625) node[anchor=north west] at (0.5, 10.225) {\textbf{(R2) Min Hash}} node[pos=1, anchor=north east, align=left] {
    $r_n = \text{min} \left( h\left( T_{n-1} \mathbin\Vert k\right), \, \cdots, h\left( T_{n-H} \mathbin\Vert k\right) \right)$\\
    };
    \draw[draw=black] (0.5,10.475) rectangle ++(10.75,0.625) node[anchor=north west] at (0.5, 11.1) {\textbf{(R1) Sliding Window}} node[pos=1, anchor=north east, align=left] {
    $r_n = h\left( T_{n-1} \mathbin\Vert \, \cdots \mathbin\Vert T_{n-H} \mathbin\Vert k\right)$\\
    };
    \draw[draw=black] (11.75,9) rectangle ++(5.5,2.35) node[pos=0, anchor=south west] {\textbf{(R3) Fixed}} node[pos=0.5, align=left] {Key length L. Expand $k$ to\\ pseudo-random sequence $\{r^k_i\}_{i<L}$.\\ 
    $r_n = r^k_{n \text{ (mod L)}}$ \\ \\ };
    
    %%%
    
    \draw[draw=black,dashed] (0,3.25) rectangle ++(17.5,5.25);
    \draw[draw=none] (0,6.85) rectangle ++(17.5,1.75) node[pos=0.5, align=center] {\large{Sampling algorithm $\mathcal{C}$ \& Per-token statistic $s$}\\
    \textbf{Inputs:} Random value $r_n = \mathcal{R}_k( \{T_i\}_{i < n})$, Probabilities $\mathcal{D}_n = \{\lambda^n_1,\, \cdots, \lambda^n_d\}$, Logits $\mathcal{L}_n = \{l^n_1,\,\cdots,l^n_d\}$\\};
    
    %
    
    \draw[draw=black] (11,4.75) rectangle ++(6.25,2.5) node[pos=0, anchor=south west] {\textbf{(C3) Binary}} node[pos=0.5, align=left] {Binary alphabet.\\ 
    $T_n \leftarrow 0$ if $r_n < \lambda^n_0$, else $1$. \\
    $s(T_n, r) = \begin{cases} -\log(r) \text{ if } T_n = 1\\
          -\log(1-r) \text{ if } T_n = 0\\\end{cases} $};
    
    \draw[draw=black] (5,4.75) rectangle ++(5.75,2.5) node[pos=0, anchor=south west] {\textbf{(C2) Inverse Transform}} node[pos=0.5, align=left] 
    {$\pi$ keyed permutation. $\eta$ scaling func.\\
    $T_n \leftarrow \pi_k \left( \min\limits_{ j \leq d } \sum\limits_{i=1}^j \lambda^n_{\pi_k (i)} \geq r_n \right)$ \\
    $s(T_n, r) = | r - \eta \left( \pi^{-1}_k(T_n) \right) | $\\};
    
    \draw[draw=black] (0.25,4.75) rectangle ++(4.5,2.5) node[pos=0, anchor=south west] {\textbf{(C1) Exponential}} node[pos=0.5, align=left] 
    {$h$ keyed hash function. \\
    $T_n \leftarrow \argmax\limits_{i \leq d} \left\{ \frac{\log \left( h_{r_n}\left( i \right) \right)}{\lambda^n_i} \right\}$ \\
    $s(T_n, r) = -\log(1 \! - \! h_r(T_n))$\\};
    
    % 
    
    \draw[draw=black] (0.25,3.5) rectangle ++(17,1) node[pos=0, anchor=south west] {\textbf{(C4) Distribution-shift}} node[pos=0.5, align=right] {Bias $\delta$, Greenlist size $\gamma$. Keyed permutation $\pi$. $T_n$ sampled from $\widetilde{\mathcal{L}}_n = \{l^n_i + \delta \text{ if } \pi_{r_n}(i) < \gamma d \text{ else } l^n_i\, , 1 \leq i \leq d\}$\\
    $s(T_n, r) = 1 \text{ if } \pi_{r}(T_n) < \gamma d \text{ else } 0$};
    
    %%% 
    
    \draw[draw=black,dashed] (0,0) rectangle ++(17.5,3);
    \draw[draw=none] (0,1.75) rectangle ++(17.5,1.25) node[pos=0.5, align=center] {\large{Score $\mathcal{S}$}\\
    \textbf{Inputs:} Per-token statistics $s_{i,j} = s(T_i, r_j)$, where $r_j = \mathcal{R}_k( \{T_l\}_{l < j}))$. \# Tokens $N$.};
    
    % 
    
    \draw[draw=black] (8.15,0.25) rectangle ++(9.1,1.5) node[pos=0, anchor=south west] {\textbf{(S3) Edit Score}}
    
    node[pos=0.5, align=left] {
    $\mathcal{S}_{\text{edit}}^\psi = s^\psi(N,N)$,
    $
        s^\psi (i,j) = \min \begin{cases}
          s^\psi (i-1, j-1) + s_{i,j}\\
          s^\psi (i-1, j) + \psi\\
          s^\psi (i, j-1) + \psi\\
        \end{cases} 
    $};
    \draw[draw=black] (0.25,0.25) rectangle ++(2.6,1.5) node[pos=0, anchor=south west] {\textbf{(S1) Sum Score}} node[pos=0.5, align=left] {$\mathcal{S}_{\text{sum}}\! = \! \sum_{i=1}^N s_{i,i}$ \\};
    \draw[draw=black] (3.1,0.25) rectangle ++(4.8,1.5) node[pos=0, anchor=south west] {\textbf{(S2) Align Score}} node[pos=0.5, align=left] {$\mathcal{S}_{\text{align}} \!= \!\min\limits_{0 \leq j < N} \sum\limits_{i=1}^N s_{i, (i+j) \text{ mod}(N)}$ \\ \\ };
    
    \end{tikzpicture}
    \caption{Watermarking design blocks. There are three main components: randomness source, sampling algorithm (and associated per-token statistics), and score function. Each solid box within each of these three components (dashed) denotes a design choice. The choice for each component is independent and offers different trade-offs.}\label{fig:design-figure}
    \end{center}
    \end{figure*}

\subsection{Watermark Design Space}
\label{sec:watermark-design}

Designing a good watermark is a balancing act.
% 
For instance, replacing every word of the output with [WATERMARK] would achieve high recall but destroy the utility of the model.
%
%Conversely, sampling from the original distribution preserves quality but makes it impossible to watermark. 

Existing proposals have cleverly crafted marking procedures that are meant to preserve quality, provide high precision and recall, and achieve a degree of robustness.
%
Despite their apparent differences, we realized they can all be expressed within a unified framework:

\begin{itemize}[leftmargin=\itemlm,itemsep=2pt]
    \item The marking procedure $\mathcal{W}$ contains a randomness source $\mathcal{R}$ and a sampling algorithm $\mathcal{C}$.
    %
    The randomness source $\mathcal{R}$ produces a (pseudo-random) value $r_n$ for each new token, based on the secret key $k$ and the previous tokens $T_0,\cdots,T_{n-1}$.
    %
    The sampling algorithm $\mathcal{C}$ uses $r_n$ and the model's next token distribution $\mathcal{D}$ to  a token.
    \item The verification procedure $\mathcal{V}$ is a one-tailed significance test that computes a $p$-value for the null hypothesis that the text is not watermarked.
    %
    The procedure compares this $p$-value to a threshold, which enables control over the watermark's precision and recall.
    %
    % This test is done using a \emph{score function} $\mathcal{S}$ based on a per-token variable that depends on the ed sampling algorithm.
    % We call the value of this per-token test statistic $s_n$, which only depends on the random value $r_n$ and the ed token $T_n$: $s_n = s(T_n, r_n)$.
    In particular, we compute a per-token score $s_{n,m} \coloneqq s(T_n, r_m)$ for each token $T_n$ and randomness $r_m$, aggregate them to obtain an overall score $\mathcal{S}$, and compute a $p$-value from this score.
    We consider all overlaps $s_{n,m}$ instead of only $s_{n,n}$ to support scores that consider misaligned randomness and text after perturbation. 
    %the test computes \emph{score function} $\mathcal{S}$ which takes as input per-token test statistics $s_{n,m} \coloneqq s(T_n, r_m)$ for a token $T_n$ and a random value $r_m$, $\forall n,m \in [N]$.
    %
    %$s_{n,m}$ depends on the sampling algorithm (see \cref{fig:design-figure} for examples).
    %
    % \dave{I believe $s(T_n, r_m)$ is incorrect and it should be $s(T_n, r_n)$.  Also I think the score should be $s_n$ rather than $s_{n,m}$.}
    % \jp{Depending on the alignment between the key string and the text, there are times we want to refer to the score for key at position m and token at poistion n (for instance, for both the align and edit scores). I'll add some explanation for this.}
    
\end{itemize}
% \dave{I find the sheer number of fonts inelegant (blackboard bold, mathcal, mathbf, typewritter, italics, bold, etc.). In some places, algorithms are denoted by mathcal (W,V), in other places by mathbf (R,C,S).  I suggest picking one and being consistent.  I prefer mathcal.  Lots of bold feels distracting to my eyes, as does lots of font changes.}
% \jp{I changed a bunch of fonts to make it more consistent, and removed bold fonts}

Next, we show how each scheme we consider falls within this framework, each with its own choices for $\mathcal{R},\mathcal{C},\mathcal{S}$.
%Given this template, previous work introduced their own variants of the building blocks, which we will now detail. 
% \chawin{I would have liked to see a summary of which design choices belong to which paper. Maybe we can add a shorthand notation denoting each paper in \cref{fig:design-figure} or have a separate table.}
% \jp{I agree that's a good idea. A table is probably the right way to represent this.}

\subsubsection{Randomness source $\mathcal{R}$}\label{ssec:randomness}
% \textbf{Randomness source $\mathcal{R}$.}
%
% \chawin{Maybe others?} \jp{Yeah but all the other papers i've seen seem to attribute it to one of these two.}
We distinguish two main ways of generating the random values $r_n$, \emph{text-dependent} (computed as a deterministic function of the prior tokens) vs \emph{fixed} (computed as a function of the token index).
Both approaches use the standard heuristic of applying a keyed function (typically, a PRF) to some data, to produce pseudorandom values that can be treated as effectively random but can also be reproduced by the verification procedure.

\citet{aaronson_watermarking_2022} and \citet{kirchenbauer_watermark_2023}
use text-dependent randomness: $r_n = f\left(T_0,\,\cdots,T_{n-1},k\right)$.
%
This scheme has two parameters: the length of the token context window (which we call the window size H) and the aggregation function $f$.
%
\citet{aaronson_watermarking_2022} proposed using the hash of the concatenation of previous tokens, $f := h\left( T_{n-1} \mathbin\Vert \, \cdots \mathbin\Vert T_{n-H} \mathbin\Vert k\right)$; we call this (R1) sliding window.
%
\citet{kirchenbauer_watermark_2023} used this with a window size of $ H = 1$ and also introduced an alternate aggregation function $f := \text{min} \left( h\left( T_{n-1} \mathbin\Vert k\right), \, \cdots, h\left( T_{n-H} \mathbin\Vert k\right) \right)$.
%
We call this last aggregation function (R2) min hash.
%
While these two schemes propose specific choices of $H$, other values are possible. 
We use \benchmarkname{} to evaluate a range of values of $H$ with each candidate aggregation function.

% \smallskip\noindent\textbf{(R3) Fixed}
\citet{kuditipudi_robust_2023} use fixed randomness:
$r_n = f_k(n)$, where $n$ is the index (position) of the token.
We call this (R3) fixed.
%
In practice, they propose using a fixed string of length $L$ (the key length), which is repeated across the generation.
% r_n = f_k(n \bmod L)$ where $L$ is the key length.
% \dave{I don't think we need this level of detail.  I suggest deleting the preceding sentence.}
% \jp{Since we look at the impact of the key length on generations we still need to introduce the idea that the key is repeated, but I canwrite that in english for it to be more digestable}
%
We test the choice of key length in ~\cref{ssec:param_tuning}
%
In the extreme case where $L=1$ or $H=0$, both sources are identical, as $r_n$ will be the same value for every token. \citet{zhao2023provable} explored this option using the same sampling algorithm as~\citet{kirchenbauer_watermark_2023}.

\label{ssec:binary}
\citet{christ_undetectable_2023} proposed setting a target entropy for the context window instead of fixing a window size.
%
This allows to set a lower bound on the security parameter for the model's undetectability.
%
However, setting a fixed entropy makes for a less efficient detector since all context window lengths must be tried in order to detect a watermark.
%
Furthermore, in practice, provable undetectability is not needed to achieve optimal quality: we chose to keep using a fixed-size window for increased efficiency.

\subsubsection{Sampling algorithm \(\mathcal{C}\)}\label{ssec:sampling}
% \textbf{sampling algorithm $\mathcal{C}$.}
%
\noindent
We now give more details about the four sampling algorithms initially presented in~\cref{tab:marking-algorithms}.

\smallskip\noindent\textbf{(C1) Exponential}.
%
Introduced by \citet{aaronson_watermarking_2022} and also used by \citet{kuditipudi_robust_2023}. It relies on the Gumbel-max trick.
%
Let $\mathcal{D}_n = \left\{\lambda^n_i\,, 1 \leq i \leq d\right\}$ be the distribution of the language model over the next token. %(obtained after passing the logits through a softmax and applying a temperature adjustment).
%
The exponential scheme will select the next token as:
\begin{align}
    T_{n} = \argmax\limits_{i \leq d}\left\{ \frac{\log \left( h_{r_n}\left( i \right) \right)}{\lambda^n_i} \right\}
\end{align}
where $h$ is a keyed hash function using $r_n$ as its key.
%
The per-token variable used in the statistical test is either $s_n = h_{r_n}(T_n)$ or $s_n = -\log \left( 1-h_{r_n}(T_n)\right)$.
%
\citet{aaronson_watermarking_2022} and \citet{kuditipudi_robust_2023} both use the latter quantity.
%
We argue the first variable provides the same results, and unlike the log-based variable, the distribution of watermarked variables can be expressed analytically (see~\cref{app:ssec:pseudorandom-proofs} for more details).
%
We align with previous work and use the $\log$ for \benchmarkname{}.

\smallskip\noindent\textbf{(C2) Inverse transform}.
%
\citet{kuditipudi_robust_2023} introduce inverse transform sampling.
%
They derive a random permutation using the secret key $\pi_k$. The next token is selected as follows:
\begin{align}
    T_{n} = \pi_k \left( \min\limits_{ j \leq d } \sum\limits_{i=1}^j \lambda^n_{\pi_k (i)} \geq r_n \right)
\end{align}
which is the smallest index in the inverse permutation such that the CDF of the next token distribution is at least $r_n$.
%
\citet{kuditipudi_robust_2023} propose to use $s_n = | r_n - \eta \left( \pi^{-1}_k(T_n) \right) |$ as a the test variable, where $\eta$ normalizes the token index to the $[0,1]$ range.
%
% We call this scheme the \textit{inverse transform} scheme.

\smallskip\noindent\textbf{(C3) Binary}.
%
\citet{christ_undetectable_2023} propose a different sampling scheme for binary token alphabets --- however, it can be applied to any model by using a bit encoding of the tokens.
%
In our implementation, we rely on a Huffman encoding of the token set, using frequencies derived from a large corpus of natural text.
%
In this case, the distribution over the next token reduces to a single probability $p_n$ that token ``0'' is ed next, and $1-p$ that ``1'' is ed.
%
The sampling rule s 0 if $r_n < p$, and 1 otherwise. The test variable for this case is $s_n = -\log \left( T_n r_n + (1-T_n) (1-r_n) \right)$.
%
% We call this scheme the \textit{binary} scheme.
%
% At first glance, it can seem like this scheme is identical to the exponential scheme. However, because it uses a binary alphabet, the distribution of the test variable is different for both schemes.
%
% However, we show in Appendix \jp{ref} that this is not the case: the distribution of the test variable is different for both schemes.
% %
% \jp{Maybe I'll remove this if I don't have time to show it.}

\smallskip\noindent\textbf{(C4) Distribution-shift}.
%
\citet{kirchenbauer_watermark_2023} propose the distribution-shift scheme. 
%
It produces a modified distribution $D_n$ from which the next token is sampled.
%
Let $\delta > 0$ and $\gamma \in [0,1]$ be two system parameters, and $d$ be the number of tokens.
%
The scheme constructs a permutation $\pi_{r_n}$, seeded by the random value $r_n$, which is used to define a ``green list,'' containing tokens $T$ such that $\pi_{r_n} (T) < \delta d$. It then adds $\delta$ to green-list logits.
%
This modified distribution is then used by the model to sample the next token. The test variable $s_n$ is a bit equal to ``1'' if $T_n$ is in the green list defined by $\pi_{r_n}$, and ``0'' if not.
%
% We call this scheme the \textit{distribution-shift} scheme.

The advantage of this last scheme over the others is that it preserves the model's diversity: 
for a given key, the model will still generate diverse outputs.
In contrast, for a given secret key and a given prompt, the first three sampling strategies 
will always produce the same result, since the randomness value $r_n$ will be the same.
\citet{kuditipudi_robust_2023} tackles this by randomly offseting the key sequence of 
fixed randomness for each generation. We introcude a skip probability $p$ for the 
same effect on text-dependent randomness. Each token is selected without the marking 
strategy with probability $p$. In the interest of space, we leave a detailed discussion 
of generation diversity in~\cref{app:ssec:diverse}.

Another advantage of the distribution-shift scheme is that it can also be used 
at any temperature, by applying the temperature scaling \emph{after} using the 
scheme to modify the logits. Other models apply temperature before watermarking.

However the distribution-shift scheme is not indistinguishable from the original model, 
as discussed earlier in~\cref{ssec:watermark-design}.

\subsubsection{Score Function $\mathcal{S}$}\label{ssec:score}

% \paragraph{Verification procedure $\mathcal{V}.$}

% The distribution of the per-token test statistic is different for watermarked text and non-watermarked text: this is what makes detection possible. Depending on the scheme, it is either higher or lower on average in the watermarked case. Without loss of generality, we assume it is always lower for this discussion.

To determine whether an $N$-token text is watermarked, we compute a score over per-token statistics.
%
This score is then subject to a one-tailed statistical test where the null hypothesis is that the text is not watermarked.
%
In other words, if its $p$-value is under a fixed threshold, the text is watermarked.
%
Different works propose different scores.

\smallskip\noindent\textbf{(S1) Sum score}.
%
\citet{aaronson_watermarking_2022} and \citet{kirchenbauer_watermark_2023} take the sum of all individual per-token statistics:
\begin{align}
    \mathcal{S}_{\text{sum}}=\sum_{i=1}^N s_i = \sum_{i=1}^N s(T_i, r_i).
\end{align}
%
This score requires the random values $r_i$ and the tokens $T_i$ to be aligned.
%
% \chawin{Maybe this goes into limitation or discussion or appendix}
This is not a problem when using text-dependent randomness, since the random values are directly obtained from the tokens.
%
However, this score is not suited for fixed randomness: removing one token at the start of the text will offset the values of $r_i$ for the rest of the text and remove the watermark.
%
The use of the randomness shift to increase diversity will have the same effect. 

\smallskip\noindent\textbf{(S2) Alignment score}.
Proposed by \citet{kuditipudi_robust_2023}, the alignment score aims to mitigate the misalignment issue mentioned earlier.
% \citet{kuditipudi_robust_2023} proposes two alternative scores to deal with this issue.
%
% In keeping with their work, we name these scores the alignment score and the edit score.
Given the sequence of random values $r_i$ and the sequence of tokens $T_i$, the verification process now computes different versions of the per-token test statistic for each possible overlap of both sequences $s_{i,j} = s(T_i, r_j)$.
%
The alignment score is defined as:
\begin{align}
   \mathcal{S}_{\text{align}}  = \min\limits_{0 \leq j < N} \sum\limits_{i=1}^N s_{i, (i+j) \text{ mod}(N)}
\end{align}

\smallskip\noindent\textbf{(S3) Edit score}.
Similar to the alignment score, \citet{kuditipudi_robust_2023} propose the edit score as an alternative for dealing with the misalignment issue.
%
It comes with an additional parameter $\psi$ and is defined as $\mathcal{S}_{\text{edit}}^\psi = s^\psi(N,N)$, where
\begin{align}
    s^\psi (i,j) &= \min \begin{cases}
      s^\psi (i-1, j-1) + s_{i,j}\\
      s^\psi (i-1, j) + \psi\\
      s^\psi (i, j-1) + \psi\\
    \end{cases} 
\end{align}

In all three cases, the average value of the score for watermarked text will be lower than for non-watermarked text.
%
% In the case of the sum score, we can often derive the distribution of the score under the null hypothesis, allowing us to use a $z$-test to determine if the text is watermarked.
In the case of the sum score, the previous works use the $z$-test on the score to determine whether the text is watermarked, but it is also possible, or even better in certain situations, to use a different statistical test according to \citet{fernandez_three_2023}.
%
When possible, we derive the exact distribution of the scores under the null hypothesis (see \cref{app:ssec:exact_dist}) which is more precise than the $z$-test. When it is not, we rely on an empirical T-test, as proposed by \citet{kuditipudi_robust_2023}
%
% This allows one to compute 

\subsection{Limitations of the Building Blocks}\label{ssec:limit_blocks}

While we design the blocks to be as independent as possible, some combinations of the scheme and specific parameters are obviously sub-optimal.
%
Here, we list a few of these subpar block combinations as a guide for practitioners.
% Even though any of the three scores can be used with any scheme and randomness source, in practice not all combinations are useful.
\begin{itemize}[leftmargin=\itemlm,itemsep=2pt]
    \item The sum score (S1) is not robust for fixed randomness (R3).
    \item The alignment score (S2) does not make sense for the text-dependent randomness (R1, R2) since misalignment is not an issue.
    \item The edit score (S3) has a robustness benefit since it can support local misalignment caused by token insertion, deletion, or swapping. However, using it with text-dependent randomness (R1, R2) only makes sense for a window size of 1: for longer window sizes, swapping, adding, or removing tokens would actually change the random values themselves, and not just misalign them.
    \item Finally, in the corner case when a window size of 0 for the text-dependent randomness (R1, R2) or when a random sequence length of 1 for the fixed randomness (R3), both the alignment score (S2) and the edit score (S3) are unnecessary since all random values are the same and misalignment is not possible.
\end{itemize}

In our experiments (\cref{sec:experiments}), we test all reasonable configurations of the randomness source, 
the sampling protocol, and the verification score, along with their parameters. 
We list the evaluated combinations in~\cref{tab:design_space_combinations}. 
The edit score is too inefficient 
to be run on all configurations, instead we rely on the sum and align scores.
%
We hope to not only fairly compare the prior works but also investigate previously unexplored combinations in the 
design space that can produce a better result.

% \chawin{We need a table or a tree that lists all the combinations we test.}\

\begin{table}[h!]
    \centering
    \caption{Tested combinations in the design space, using notations from~\cref{fig:design-figure}.\\
    We only tested the edit score {\bf S3} on a subset of watermarks.\\
    The distribution of non-watermarked scores is known for \textcolor{orange}{orange} configurations and 
    unknown for \textcolor{blue}{blue} configuration. We rely on empirical T-tests~\cite{kuditipudi_robust_2023} for blue configurations.
    }
    \label{tab:design_space_combinations}
    \normalsize
    \begin{tabular}{|l||c|c|c|c|} 
    \hline
     & \makecell[tc]{{\bf C4}\\{\small Distribution}\\{\small Shift}} & \makecell[tc]{{\bf C1}\\{\small Exponential}} & \makecell[tc]{{\bf C2}\\{\small Binary}} & \makecell[tc]{{\bf C3}\\{\small Inverse}\\{\small Transform}} \\
    \hline
    \hline
    \makecell{{\bf S1}+{\bf R1}}  & \textcolor{orange}{X} & \textcolor{orange}{X} & \textcolor{orange}{X} & \textcolor{blue}{X} \\
    \hline
    \makecell{{\bf S1}+{\bf R2}}  & \textcolor{orange}{X} & \textcolor{orange}{X} & \textcolor{orange}{X} & \textcolor{blue}{X} \\
    \hline
    \makecell{{\bf S2}+{\bf R3}}  & \textcolor{blue}{X} & \textcolor{blue}{X} & \textcolor{blue}{X} & \textcolor{blue}{X} \\
    \hline
    \makecell{{\bf S3}+{\bf R3}}  & \textcolor{blue}{X} &  &  &  \\
    \hline
    \end{tabular}
\end{table}
    

\subsection{Analysis of the edit score.} 
\label{ssec:editscore}
We analyzed the tamper-resistance of the edit score on a subset of watermarks 
(distribution-shift with $\delta=2.5$ at a temperature of 1, for key lengths between 1 and 1024). 
We tried various $\psi$ values between 0 and 1 for the edit distance, and compared the tamper-resistance 
and watermark size of the resulting verification procedures to the align score. 
Using an edit distance does improve tamper-resistance for key lengths under 32, but at a large efficiency cost: 
for key lengths above 8, the edit score size is at least twice that of the align score. 
We do not recommend using an edit score on low entropy models such as Llama-2 chat.


