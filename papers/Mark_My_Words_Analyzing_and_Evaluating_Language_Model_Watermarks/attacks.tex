\section{Attacks on Watermarks}\label{sec:attacks}

Any good watermarking scheme should easily detect the mark,
if a LLM's watermarked output is used directly.
For instance, if a student uses a LLM to write an essay and copies
the LLM's output directly into their submission, then all four
schemes should have no problem detecting the mark, assuming the
AI-generated text is long enough.

It may also be important to provide some robustness against
simple attacks.
For instance, a cheating student might make minor modifications
to the AI-generated text before submission, to try to evade detection.
Sufficiently sophisticated strategies can undoubtedly fool
any watermarking scheme, but in many practical settings, this
might be more effort than it's worth for the attacker.
For instance, a cheating student who is trying to save time
using a LLM instead of doing the assignment themselves
probably won't be interested in time-consuming or technically
sophisticated attacks.
Therefore, it might be more relevant to evaluate how robust the
scheme is at detecting watermarked attack that has been subjected
to simple attacks to modify the text.

We evaluate each watermarking scheme against 8 simple attacks
that seek to remove the mark from AI-generated text by perturbing it.
A perturbation of generated text $x$ is some text $x_{\mathrm{adv}}$ that is 
semantically similar to $x$, but can be syntactically different.
%
%In other words, for some small value $\epsilon$, and a semantic distance metric $\texttt{Dist}$, a perturbation that takes text $x$ as input should output $x_{\mathrm{adv}}$ such that $\texttt{Dist}\left(x, x_{\mathrm{adv}}\right) \le \epsilon$.
%
There are many ways one can modify text---some only change the syntax of the sentence, while others replace words or entire sentences.
%
Below we introduce the 8 attacks we use to provide a holistic evaluation of watermark robustness.

\smallskip\noindent\textbf{Swap attack.} 
%
Natural language is redundant: we are able to understand text even if some words have been removed, reordered, or duplicated.
%
One natural attack on generated text is to randomly remove, add, and swap some words in each sentence.
%
We scan generated text word by word, and with probability $p$, we either remove the word, duplicate it, or swap it with another randomly 
chosen word in the sentence.
%
In addition to this, we randomly swap full sentences with the same probability $p$.
%
Swap attacks are easy to implement for an attacker, and for small values of $p$ produce text that is still understandable.

\smallskip\noindent\textbf{Synonym attack.}
%
This attack replaces words in sentences with synonyms.
%
With probability $p$, we replace each word in the text by a semantically equivalent word.
%
This attack is more difficult to implement for an attacker.
% A naive approach is to manually replace words in the sentence, but we choose to automate this using a combination of WordNet~\citep{miller_wordnet_1994} as well as a zero-shot prompting of GPT-3.5 to generate candidate synonyms.
We automate this attack using WordNet~\citep{miller_wordnet_1994} to zero-shot prompt GPT-3.5 to generate candidate synonyms.
%
%We then randomly sample a synonym with probability $p$. 
%
In practice, this approach sometimes creates grammatically incorrect or unnatural sentences.
%
However, for a low probability $p$, the output text is still semantically 
close to the original.

\smallskip\noindent\textbf{Paraphrase attack.}
%
Perhaps the strongest attack in our toolkit, the paraphrase attack involves using another language model to rephrase the generated text.
%
This can be difficult and expensive to implement for an attacker, as they need access to a high-quality non-watermarked language model to do so, but the attack can completely change text without perturbing its meaning.
%
We implement two versions: (1) zero-shot prompting GPT-3.5 to 
paraphrase a generation, and (2) Dipper~\citep{krishna_paraphrasing_2023}, a fine-tuned model designed for paraphrasing.

\smallskip\noindent\textbf{Translation attack.}
%
This is similar to the paraphrase attack, except we use a translation model (\texttt{argos-translate}~\citep{finlay_argos_2021} based on OpenNMT~\citep{klein_opennmt_2017}) to translate text through a cycle of languages (e.g., English $\to$ French $\to$ English).
%
This will not change the text as much as the paraphrase attack, but it is easy for an attacker to implement since they can use Google Translate.
%
We use two variants of this attack: one that translates to French and back, and another that translates to Russian and back.

\smallskip\noindent\textbf{HELM perturbations.}
%
HELM~\citep{liang_holistic_2022} implements a number of perturbations in its source code. 
%
They were originally designed to perturb model prompts. We use them to perturb model outputs. 
%
Among the list of perturbations they implement, we chose those that do not change the overall meaning of the text.
%
In particular, we use:
\begin{itemize}[leftmargin=\itemlm,nosep]
    \item \textbf{Contractions \& expansions}: Contract verbs (\textit{e.g.} ``do not'' $\to$ ``don't'') 
    or expand them.
    \item \textbf{Lower case}: Convert all words to lower case.
    \item \textbf{Misspelling}: Misspell each word with probability $p$.
    \item \textbf{Typos}: Add random typos to the text.
\end{itemize}

\smallskip\noindent\textbf{Out-of-scope attacks.}
%
We consider only attacks that perturb the generated text.
%
We do not evaluate attacks that work by modifying the prompt to the language model.
%
For instance, the ``emoji attack'' instructs the model to insert a token, such as an emoji, between each word in a generation \citep{kirchenbauer_watermark_2023}.
%
The attacker can then remove the emojis after generation.
%
The emoji attack defeats all watermarking schemes using text-dependent randomness.
%
These attacks are only possible on advanced models that can understand complex queries, such as ChatGPT.
%
Because our benchmark is designed to work with open models, such as Llama2, which have difficulty following such complex instructions, we do not include these attacks.
% \jp{Update: if we do have time to run it, change this.}

We assume the attacker cannot make repeated queries to the verification procedure.
%
An adversary with a verification oracle (i.e., an oracle that can tell if a text is watermarked or not) may be able to iteratively modify text until the watermark is removed.
%
One can thwart such attacks by keeping the key $k$ secret, rate-limiting calls to the verification API, designing the verification API to release only ``watermarked'' or ``not'' (and not the score $\mathcal{S}$ or its likelihood), and possibly detecting clusters of closely-related calls to the verification API.
