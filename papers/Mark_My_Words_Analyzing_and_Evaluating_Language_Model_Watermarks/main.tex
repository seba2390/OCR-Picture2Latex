% \documentclass{article}
\documentclass[conference,compsoc,10pt]{IEEEtran}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{iclr2024_conference,times}


% to compile a preprint version, e.g., for submission to arXiv, add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  % \usepackage[nocompress]{cite}
  \usepackage[numbers]{natbib}
\else
  % normal IEEE
  % \usepackage{cite}
  \usepackage{natbib}
\fi


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\hypersetup{
    colorlinks,
    linkcolor={red!70!black},
    citecolor={green!70!black},
    urlcolor={blue!80!black}
}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{tablefootnote}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{bm}
\usepackage{todonotes}
\usepackage{dblfloatfix}

\usepackage[capitalize]{cleveref}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{wrapfig}
\usepackage{tikz}
\usepackage{xspace}
\usepackage{caption}

\renewcommand{\cellalign}{vh}
\renewcommand{\theadalign}{vh}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\hullvolume}[0]{$V_{\text{hull}}$}
\newcommand{\hullarea}[0]{$A_{\text{hull}}$}
\newcommand{\quality}[0]{$Q$}
\newcommand{\efficiency}[0]{$E$}
\newcommand{\robustness}[0]{$R$}

\title{Mark My Words: Analyzing and Evaluating Language Model Watermarks}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{\IEEEauthorblockN{Julien Piet}
\IEEEauthorblockA{\textit{UC Berkeley} }
\and
\IEEEauthorblockN{Chawin Sitawarin}
\IEEEauthorblockA{\textit{UC Berkeley} }
\and
\IEEEauthorblockN{Vivian Fang}
\IEEEauthorblockA{\textit{UC Berkeley} }
\and

\IEEEauthorblockN{Norman Mu}
\IEEEauthorblockA{\textit{UC Berkeley} }
\and
\IEEEauthorblockN{David Wagner}
\IEEEauthorblockA{\textit{UC Berkeley} }
}


\newcommand{\jp}[1]{{\color{green!15!blue}[#1] --Julien}}
\newcommand{\vf}[1]{{\color{violet}[#1] --Vivian}}
\newcommand{\norm}[1]{{\color{green} N: #1}}
\newcommand{\chawin}[1]{{\color{orange} C: #1}}
\newcommand{\dave}[1]{{\color{purple} D: #1}}
\newcommand{\itemlm}{{15pt}}
% \newcommand{\benchmarkname}{{MarkMyWords}}
\newcommand{\benchmarkname}{\textsc{MarkMyWords}}
\newcommand{\scott}{\citet{aaronson_watermarking_2022}}
\newcommand{\mylink}[1]{\url{#1}}
% \newcommand{\mylink}[1]{\texttt{this anonymized link\xspace}}

\begin{document}


\maketitle
\thispagestyle{plain}
\pagestyle{plain}


\begin{abstract}
  The capabilities of large language models have grown significantly in recent years and so too have concerns about their misuse. 
  In this context, the ability to distinguish machine-generated text from human-authored content becomes important. 
  Prior works have proposed numerous schemes to watermark text, which would benefit from a systematic evaluation framework.
  This work focuses on text watermarking techniques --- as opposed to image watermarks --- and proposes \benchmarkname{}, a 
  comprehensive benchmark for them under different tasks 
  as well as practical attacks. We focus on three main metrics: quality, size (e.g. the number of tokens needed to detect a watermark), and tamper-resistance. 
  Current watermarking techniques are good enough to be deployed:~\citet{kirchenbauer_watermark_2023} can 
  watermark Llama2-7B-chat with no perceivable loss in quality, the watermark can be detected with fewer than 100 tokens, and the scheme offers good tamper-resistance to simple attacks. 
  We argue that watermark indistinguishability, a criteria emphasized in some prior works, is too strong a requirement: schemes that slightly modify logit 
  distributions outperform their indistinguishable counterparts with no noticeable loss in generation quality. 
  We publicly release our benchmark (\mylink{https://github.com/wagner-group/MarkMyWords})
\end{abstract}

\input{intro}
\input{background}
\input{attacks}
\input{metrics}
\input{bench}
\input{results}
\input{design}
\input{summary}

\bibliography{refs}
% \bibliographystyle{abbrvnat}
\bibliographystyle{IEEEtranN}
\newpage

\appendix
\input{app_watermark}
\input{add_figures}
\input{proofs}

\end{document}
