\section{Background and Related Work}
\label{sec:relwork}

\subsection{Watermarking}

Watermarking of textual data has been extensively studied~\citep{kamaruddin18,rizzo17,ahvanooey19}
It can be viewed as a form of steganography~\citep{cox_information_2005,majeed_review_2021} with a one-bit message.
Steganography imposes the stronger requirement that, without knowledge of the secret key, the distribution of watermarked and unwatermarked outputs be indistinguishable; however, indistinguishability may be useful in some settings, as it implies that the mark does not harm the quality of the generated text.
Watermarking the output of a large language model can be viewed as the problem of embedding a mark when we can sample from the output distribution with limited additional control~\citep{hopper2008stego,ziegler_neural_2019a,xiang_novel_2020,kang_generative_2020,cao_generative_2022}. 

\subsection{AI Detectors}

Another approach is to use train a classifier to detect LLM-generated text.
This approach avoids the need to modify how text is generated by the LLM.
Many schemes can be found in the literature,
including OpenAI's classifiers~\citep{openai_gpt2outputdataset_2021,openai_new_2023}, GLTR~\citep{gehrmann_gltr_2019a}, DetectGPT~\citep{mitchell_detectgpt_2023},
and others~\citep{bakhtin_real_2019,zellers_defending_2019a,ippolito_automatic_2020,uchendu_authorship_2020,fagni_tweepfake_2021}.
Unfortunately, current LLMs have gotten so good at generating natural-looking text that these detectors have become unreliable, and as LLMs improve, these detectors will perform even worse.

For proprietary LLMs, another alternative is for the vendor to keep a copy of all generated outputs from their LLM, and provide an API that can be used to look up any text to determine whether it was previously produced with their LLM~\citep{krishna_paraphrasing_2023}.


% learned classifiers:
% - https://arxiv.org/abs/1905.12616
% - https://arxiv.org/abs/1906.03351
% - https://arxiv.org/abs/1911.00650
% - https://aclanthology.org/2020.emnlp-main.673/
% - TweepFake: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0251415

% zero-shot methods:
% - https://arxiv.org/abs/1906.04043
% - https://arxiv.org/abs/2301.11305v1

% database lookup:
% - https://arxiv.org/abs/2303.13408

% \subsection{Attacks}

% \chawin{Overlap with section 5; remove?}
% Various potential attacks against watermarking methods have also been examined, and of particular interest are paraphrasing attacks (Kirchenbauer et al., Krishna et al., Sadasivan et al.).
