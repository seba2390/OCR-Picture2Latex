\section{Introduction}

Recent advancements in large language models (LLMs) have been paralleled by escalating concerns over their misuse.
%
Researchers have pointed out the potential of language models to automate social engineering attacks \citep{GovTech}, conduct large-scale propaganda operations \citep{goldstein2023generative}, and more \citep{gpt4system}.
%
Preventing misuse is challenging.
%
% Social media platforms could require identity verification for users attempting to post AI-generated text, and email providers could display prominent warnings over AI-generated messages.
%Authentication can partially prevent the propagation of generated contents by requiring identity verification.
%
%Nevertheless, strict authentication is not prevalent so we need efficient and reliable methods for detecting language model outputs to tackle the challenges ahead of us.
% could serve as one of many tools with which to tackle the challenges ahead of us.
%

One approach to mitigate these risks is \emph{watermarking}, in which a subtle watermark is embedded in all output from the model, so that others can detect LLM-generated text.
%
%the generation process of a model is statistically biased in a way that is imperceptible to human readers but readily detected by the right algorithm.
%
% Watermarking relies on entropy in the generation process, or the fact that the model can usually choose between multiple tokens without affecting the fluency or content of the generated text.
% For example, suppose we would like to watermark the next word after ``\texttt{Never gonna give you [...]},'' and the language model produces three candidates, \texttt{the}, \texttt{a}, \texttt{up} with probability 0.33, 0.33, and 0.33, respectively.
% %
% In this case, we can choose to watermark with \texttt{the} (or the other two words) which is equally good as the other options.
% %
% On the other hand, if the probability changes to 0.01, 0.01, and 0.98, we can no longer choose \texttt{the} since it will likely degrade the quality of the generated text.
%
% So we are forced to choose \texttt{up} which preserves the quality but does not help with watermarking because almost all non-watermarked instances will also follow this option.
%Watermarking has been commonly used for different types of media \chawin{CITE}.
%
With the rise of LLMs, multiple watermarking schemes have been proposed \citep{aaronson_watermarking_2022,kirchenbauer_watermark_2023,christ_undetectable_2023} and subsequently analyzed~\citep{chakraborty_possibilities_2023,sadasivan_can_2023,jiang_evading_2023,krishna_paraphrasing_2023}.
%
Nevertheless, the feasibility of watermarking LLM outputs in practice remains unclear. Some researchers argue that watermarks can be practical \citep{krishna_paraphrasing_2023}, while others argue the opposite \citep{sadasivan_can_2023,jiang_evading_2023}.
%
The results from these papers seem almost contradictory, and there has yet to be a consensus on how to evaluate different watermarking schemes or whether they are ready for practical deployment.

\begin{figure}[t]
  \begin{center}
    \includegraphics[width=0.5\textwidth]{figures/fig1.png}
  \end{center}
  \caption{Watermark size at near-optimal quality for four watermarking schemes taken from the literature. 
    The distribution-shift scheme~\cite{kirchenbauer_watermark_2023} outperforms all others at low temperatures, 
    where it only needs a median of 80 tokens for the watermark to be detected.}
  \label{fig:aggregate}
\end{figure}

Our work tackles this challenge by providing a common ground where these algorithms can be empirically evaluated.
%
We propose \benchmarkname{}, a benchmark to evaluate the effectiveness of watermarking schemes under three realistic tasks, representing three possible misuses: book summarization, creative writing, and news article generation. 
%
We devise metrics to measure efficiency, quality, and tamper-resistance.
%
To measure tamper-resistance, \benchmarkname{} tests whether a number of simple attack algorithms---word swap attacks, synonym attacks, HELM perturbations~\citep{liang_holistic_2022}, paraphrase attacks, and translation attacks---can remove the watermark without significant loss of quality.

\begin{figure*}[ht]
    \includegraphics[width=\linewidth]{figures/diagram.png}
    \caption{An overview of the LLM watermarking scenario.}\label{fig:diagram}
\end{figure*}


Next, we combine previous proposed watermarking schemes~\citep{kirchenbauer_watermark_2023, aaronson_watermarking_2022, christ_undetectable_2023, kuditipudi_robust_2023} into a unified design framework. 
This abstraction allows practitioners to build custom watermarking schemes using building blocks from different papers. We ran \benchmarkname{} on all practical parameter combinations. We come to the following conclusions:
%
\begin{enumerate}
    \item Watermarking schemes are ready for deployment. We can watermark the output of Llama 2 with little loss in quality, and these watermarks can be detected with under 100 tokens of output.
    \item Watermarks with the right parameters are relatively robust to simple attacks. Although more sophisticated attacks are capable of removing any watermark, for the best watermarking schemes our most powerful attack---which uses GPT-3.5 to paraphrase model outputs---only removes the watermark 60\% of the time.
    \item Our results may have implications for public policy, as they inform the extent to which watermarking is operationally feasible with current technology.
    For instance, the 2023 White House agreement with industry included a voluntary commitment to deploy technology such as watermarks for AI-generated audio and video, but did not include any commitment regarding AI-generated text \cite{whitehouse23vol}.
    \item Some prior work has proposed indistinguishability as a criterion for watermarking schemes, i.e. that the distribution of watermarked outputs be provably indistinguishable from non-watermarked outputs.  We argue that this is not necessary for practical watermarking, and our results suggest it can be overly restrictive.  In our experiments the best scheme is \text{distribution shift} \citep{kirchenbauer_watermark_2023}.  It produces the easiest-to-detect watermark, while maintaining the original model's quality, despite not being indistinguishable. 
    \item We quantify the impact of each watermarking hyperparameter on the metrics, and provide recommendations for practical parameter settings.
  \end{enumerate}
%
\benchmarkname{} is publicly released at~\mylink{https://github.com/wagner-group/MarkMyWords}.

%The remainder of this paper is structured as follows. First, we provide some background on watermarking in~\cref{sec:taxonomy}, and layout the design space in~\cref{sec:watermark-design}. We discuss attacks on watermarks in~\cref{sec:attacks}, before describing the metrics we use in~\cref{sec:metrics}, and give practical details about our benchmark in~\cref{sec:experiments}. Finally, we discuss our findings in~\cref{sec:results}. 

