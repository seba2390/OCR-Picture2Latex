\subsection{Exponential scheme proofs}
\label{app:ssec:pseudorandom-proofs}

We use the same notation as in~\cref{fig:design-figure}. In particular, $h_s$ is
a secure hash function mapping strings to $[0,1]$ with seed $s$, $k$ is a key selected uniformly 
at random amongst a set of keys $K$. Consider an execution of the language model which produced 
tokens $T_1,\ldots,T_n$ up until now (including the prompt). The next token distribution is represented as
$\mathcal{D}_{T_1,\cdots,T_n} = \left\{\lambda_i, P(T_{n+1} = i \mid T_1,\cdots, T_n) = \lambda_i\right\}$.
%
For an i.i.d randomness source (producing i.i.d random values), 
the exponential selection procedure has the following properties:

\begin{enumerate}[leftmargin=\itemlm,nosep]
    \item[\textbf{P1}] For a uniformly random key $k$, the distribution over the next token is the 
    same as without the watermark.
        \begin{align}
            P_k(\widetilde{T}_{n+1} = i \mid T_{i\leq n} ) 
            = P(T_{n+1} = i \mid T_{i\leq n} )
            = \lambda_i
        \end{align}
    \item[\textbf{P2}] The expectation of the hash of the next token is equal to the spike 
    entropy\footnote{Spike entropy is defined in~\cite{kirchenbauer_watermark_2023}. For a 
    discrete distribution $\mathcal{D} = \left\{ \lambda_1, \cdots, \lambda_n \right\}$, the 
    spike entropy $S\left(\mathcal{D}, t\right)$ (said \textit{modulus} i) is defined as 
    $\sum_{j=1}^{n} \frac{\lambda_j}{t+\lambda_j}$.} of the next token distribution. This is 
    always larger than the expectation of the hash of the next token without watermark, only 
    being equal for a degenerate distribution with only one token having a non-zero probability.
        \begin{align}
        \begin{split}
            &\frac{n}{n+1} \geq \mathbb{E}_k\left[ h_s\left(\widetilde{T}_{n+1}\right) \right] = S\left(\mathcal{D}_{T_1,\cdots,T_n},\, 1\right) \\
            &S\left(\mathcal{D}_{T_1,\cdots,T_n},\, 1\right) = \sum_{j=1}^{d} \frac{\lambda_j}{1+\lambda_j} \geq \mathbb{E}_k\left[ h_s\left(T_{n+1}\right) \right] = \frac{1}{2}
        \end{split}
        \end{align}
\end{enumerate}

\noindent
When using the sum score, we perform a hypothesis test, where under $H_0$ the text is not 
watermarked, and under $H_1$ it is. In particular, let 
$\bar{h} = \frac{1}{n} \sum_{i=1}^{n} h_s\left(T_{i}\right)$ be the average hash over a text of 
$n$ tokens. Let $S_i$ be the average spike entropy of modulus $i$ over the text. 
$\bar{h}$ follows a Bates distribution under $H_0$ with parameter $n$ (which is well approximated by 
a Gaussian with average $0.5$ and variance $\frac{1}{12n}$. 
The distribution under $H_1$ has an average of $S_1$ and asymptotically follows a Gaussian 
distribution centered in $S_1$ with variance $\frac{S_2 - S_1^2}{n}$. 

\paragraph{Proof of \textbf{P1}}

Let $r = \mathbf{R}(k,  \{T_i\}_{i < n})$ be our randomness source. We assume that it is uniformly distributed between 0 and 1, for $k \sim \mathcal{U}(K)$, with $|K|$ large enough. Since $h_s$ is a secure hash function, we posit: 
\begin{enumerate}[leftmargin=\itemlm,nosep]
    \item[\textbf{A1}] The hash of each token is uniformly distributed: $h_r(i) \sim \mathcal{U}\left([0,1]\right) ~\forall i, T_1, \cdots, T_n$.
    \item[\textbf{A2}] The token hashes $\{ h_r(i),\, 1 \leq i \leq d\}$ are mutually independent.
\end{enumerate}
We have:
\small
\begin{align}
    P_k(\widetilde{T}_{n+1} = i | T_{i\leq n}) = P_k\left\{ \forall j \neq i,\, \frac{\log(h_r(i))}{\lambda_i} > \frac{\log(h_r(j))}{\lambda_j}\right\}
\end{align}
\normalsize
We can simplify this expression: if $\{h_r(i)\}_i$ are mutually independent uniformly distributed 
random variables between 0 and 1, then $\left\{\frac{-\log(h_r(i))}{\lambda_i}\right\}_i$ are 
mutually independent exponentially distributed random variables, with parameters $\{\lambda_i\}_i$. 
By writing $u_i = -\frac{\log(h_r(i))}{\lambda_i}$, we then have:
\begin{align}
    P_k(\widetilde{T}_{n+1} = i | T_{i\leq n}) = P_k\left\{ u_i < \min_{j \neq i}(u_j) \right\}
\end{align}

We now use a useful property of exponential random variables: The minimum of a set of independent exponential random variables of parameters $\lambda_1, \cdots, \lambda_n$ is also an exponential random variable, with parameter $\sum_{i=1}^n \lambda_i$. Thus, $\min_{j \neq i}(u_j) \sim \mathrm{Exp}(1-\lambda_i)$ (since $\lambda_i$ are a probability distribution, they sum to 1, so $\sum_{j\neq i}\lambda_j = 1 - \lambda_i$). We can now finish the proof.
\begin{align}
    \begin{split}
        P_k(&\widetilde{T}_{n+1} = i | T_{i\leq n})
        = P_k\left\{ u_i < \min_{j \neq i}(u_j) \right\} \\
        &= \int_0^{\infty} \lambda_i e^{-\lambda_i x} \int_x^{\infty} (1-\lambda_i) e^{-(1-\lambda_i) y} dx dy\\
        &= \lambda_i = P(T_{n+1} = i | T_{i\leq n}).
    \end{split}
\end{align}

\paragraph{Proof of \textbf{P2}}

For a sequence of previous tokens $T_{i\leq n} = T_1,\cdots,T_n$, lets compute the expected value of the hash of the next token under both the watermarked and non-watermarked model.

As discussed above, for $k \sim \mathcal{U}(K)$, $h_r(i) \sim \mathcal{U}([0,1])$ and are mutually independent, and we have $\mathbb{E}_k\left[ h_r(i) \right] = \frac{1}{2}$

For the non-watermarked model, we select a token independently from the key $k$ or the values $h_r(i)$, thus:
\begin{align}
    \begin{split}
        \mathbb{E}_k\left[ h_r\left(T_{n+1}\right) \right]
        &= \mathbb{E}_k\left[ \sum_{i=1}^d \mathbbm{1}_{\{T_{n+1} = i\}} h_r\left(i\right) \right]\\
        &= \sum_{i=1}^d \mathbb{E}\left[ \mathbbm{1}_{\{T_{n+1} = i\}} \right]  \mathbb{E}_k\left[ h_r\left(i\right) \right]\\
        &= \sum_{i=1}^d \frac{1}{2}\lambda_i = \frac{1}{2}
    \end{split}
\end{align}

For the watermarked model, token selection is no longer independent from the key $k$ or 
the hash values. Instead, we use the notations from the previous proof to compute this 
expectation. In particular, we have 
$\{T_{n+1}=i\} = \left\{ \forall j \neq i,\, u_i < u_j \right\} = \left\{ u_i < \min_{j \neq i}(u_j) \right\} $, 
with $u_i$ exponentially distributed with parameter $\lambda_i$, and $\min_{j \neq i}(u_j)$ 
exponentially distributed with parameter $1-\lambda_i$, both independent. Also, 
since $u_i = -\frac{\log(h_r(i))}{\lambda_i}$, we have $h_r(i) = e^{-u_i \lambda_i}$.
%
\begin{align}
    \begin{split}
        \mathbb{E}_k&\left[ h_r\left(T_{n+1}\right) \right] = \mathbb{E}_k\left[ \sum_{i=1}^V \mathbbm{1}_{\{T_{n+1} = i\}} h_r\left(i\right) \right]\\
        &= \sum_{i=1}^d \iint_0^{\infty} e^{-\lambda_i x} \mathbbm{1}_{x < y} \lambda_i e^{-\lambda_i x} (1-\lambda_i) e^{-(1-\lambda_i) y} dx dy\\
        &= \sum_{i=1}^d \frac{\lambda_i}{1 + \lambda_i}
    \end{split}
\end{align}
Thus, the expectation of the watermarked model's next token hash is equal to the spike entropy of next token distribution (as defined in~\cite{kirchenbauer_watermark_2023}). Analysis of the spike entropy shows that its minimum value is $0.5$, when all but one token have 0 probability, and its maximum is $\frac{d}{d+1}$, when all tokens are equiprobable.

\paragraph{Verification}
The verification procedure computes the average hash value over all tokens in a text: $\bar{h} = \frac{1}{n} \sum_{i=1}^{n} h_r\left(T_{i}\right)$.
%
Let's analyze this random variable $\bar{h}$ in both the watermarked and non-watermarked settings. 
%
In the regular case, we can show, for $k \sim \mathcal{U}(K)$, that $h_r\left(T_{i}\right) \sim \mathcal{U}([0,1])$, for some $x \in [0,1]$:
\begin{align}
\begin{split}
P_k&\left( h_r\left(T_{i}\right) < x \right) \\
&= \sum_{j=1}^d P_k\left( \{T_i = j\} \cap \{h_r\left(i\right) < x \}\right) \\
&= \sum_{j=1}^d P\left( \{T_i = j\} \right) P\left(\{h_r\left(i\right) < x\}\right) = x
\end{split}
\end{align}
Which is the CDF of a continuous uniform random variable between 0 and 1.
%
Furthermore, since the randomness source is i.i.d, the ${h_r(T_i)}_i$ are independent.
%
Thus $\bar{h}$ is the average of $n$ independent uniform random variables over 
$[0,1]$, so it follows a Bates distribution. When $n$ increases to $+\infty$, 
$\mathcal{B}(n)$ is equivalent to $\mathcal{N}(0.5, \frac{1}{12n})$. In practice, 
even for small values of $n$, the Bates distribution is close to Gaussian.

In the watermarked case, we start by computing the CDF and PDF of the hash of a single token, 
$h_r\left(T_{i}\right)$.
\small
\begin{align}
    \begin{split}
        &P_k\left( h_r\left(T_{i}\right) < x \right) = \sum_{j=1}^d P_k\left( \{T_i = j\} \cap \{h_r\left(j\right) < x \}\right) \\
        &= \sum_{j=1}^d \iint_0^{\infty} \mathbbm{1}_{z < y} \mathbbm{1}_{z > -\frac{\log(z)}{\lambda_j}} \lambda_j e^{-\lambda_j z} (1-\lambda_j) e^{-(1-\lambda_j) y} dz dy\\
        &= \sum_{j=1}^d \lambda_j x^{\frac{1}{\lambda_j}}.
    \end{split}
\end{align}
\normalsize
Thus the hash of a token has CDF $\sum_{j=1}^d \lambda_j x^{\frac{1}{\lambda_j}}$ and PDF $\sum_{j=1}^d x^{\frac{1}{\lambda_j} - 1}$. Using these formulas, we can derive higher order moments for the hash: $\mathbb{E}_k\left[ h_r\left(T_i\right)^{m}\right] = \sum_{j=1}^{d} \frac{\lambda_j}{1 + m\lambda_j}$. In particular, each moment of order $m$ is bounded between 1 and $\frac{V}{m+V}$. We denote $S_m$ to be the average moment of order $m$ over the text (which happens to also be the average spike entropy of modulus $m$).

We define $s^2_n = \sum_{i=1}^{n} \mathbb{E}_k\left[ \left(h_r\left(T_i\right) - 
\mathbb{E}_k\left[h_r\left(T_i\right)\right]\right)^2 \right] = n\left(S_2 - S_1^2\right)$.
Since we assume each hash is independent, thanks to the i.i.d randomness, we 
can use Lyapunov's central limit theorem on the sequence of hash values. In particular, each 
hash has bounded moments of all orders (and bounded away from zero), so all conditions of 
the theorem apply.
\begin{align}
\begin{split}
    &\frac{1}{s_n} \sum_{i=1}^n \left( h_r\left(T_i\right) - \mathbb{E}_k\left[h_r\left(T_i\right)\right] \right) \xrightarrow[d]{n \rightarrow \infty} \mathcal{N}(0,1)\\
    \implies &\frac{1}{n} \sum_{i=1}^n \left( h_r\left(T_i\right) - \mathbb{E}_k\left[h_r\left(T_i\right)\right] \right) \xrightarrow[d]{n \rightarrow \infty} \mathcal{N}(0,\frac{S_2 - S_1^2}{n})\\
    \implies &\left(\bar{h} - S_1\right) \xrightarrow[d]{n \rightarrow \infty}  \mathcal{N}(0,\frac{S_2 - S_1^2}{n})\\
    \implies &\bar{h} \xrightarrow[d]{n \rightarrow \infty} \mathcal{N}(S_1,\frac{S_2 - S_1^2}{n})\\
\end{split}
\end{align}

Thus, as $n$ increases, non watermarked text will have $\bar{h}$ get closer to 0.5, while watermarked text will have $\bar{h}$ get closer to $S_1$. In particular, if we fix a maximum false positive ratio $p$ (number of regular text mistaken for watermarked text), the detection strategy is to flag text as watermarked if the probability of $\bar{h}$ in the non-watermarked hypothesis is lower than $p$, or equivalently, $1-\Phi_{0.5,\,1/12n}\left(\bar{h}\right) < p$, with $\Phi_{0.5,\, 1/12n}$ the CDF of a Gaussian centered in 0.5 with variance $1/12n$.

Furthermore, given values of $S_1$ and $S_2$, for large values of $n$, we can compute the expected false negative ratio of our detector. Given the quantile $q$ associated with the false positive ratio $p$ ($\Phi_{0.5,\,1/12n}\left(q\right) = 1-p$), we have $FN = \Phi_{S_1,\,(S_2-S_1^2)/n}(q)$. This get exponentially lower as the average entropy $S_1$ and $n$ increase.

% \subsection{Equivalent of Gumbal Softmax and Binary selection criteria}\label{equiv-pseudorandom}

% \begin{align}
%     \begin{split}
%         T_{i+1} = 1 &\Leftrightarrow \log \left( h_s (1) \right) (1-p_1) > \log \left( h_s (0) \right) p_1 \\
%         &\Leftrightarrow \log \left( h_s (1) \right) > (\log \left( h_s (1) \right) + \log \left( h_s (0) \right)) p_1 \\
%         &\Leftrightarrow \frac{1}{1 + \frac{\log \left( h_s (0) \right)}{\log \left( h_s (1) \right)}} < p_1 \\
%     \end{split}
% \end{align}
% Let $u_i = \frac{1}{1 + \frac{\log \left( h_s (0) \right)}{\log \left( h_s (1) \right)}}$. Since both $h_s(0)$ and $h_s(1)$ are uniformly distributed between 0 and 1 and are independent (Assumption A2 from \ref{pseudorandom-proofs}), we can show $u_i$ is also uniformly distributed between 0 and 1, and is seeded by the previous token selections. 