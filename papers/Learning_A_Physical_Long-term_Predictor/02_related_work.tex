\section{Related Work}
\label{sec:related}

In this work we address the problem of long-term prediction of object positions in a physical environment without voluntary perturbation with an implicit learning of physical laws. Our work is  closely related to a range of recent works in the machine learning community.

\paragraph{Learning intuitive physics.}
To the best of our knowledge \cite{battaglia2013simulation} was the first approach to tackle intuitive physics with the aim to answer a set of intuitive questions (\textit{e.g.}, will it fall?) using physical simulations. Their simulations, however used a sophisticated physics engine that incorporates prior knowledge about Newtonian physical equations. More recently \cite{Mottaghi_2016_CVPR} also used static images and a graphic rendering engine (Blender) to predict movements and directions of forces from a single RGB image. Motivated by the recent success of deep learning for image processing (\textit{e.g.}, \cite{krizhevsky2012imagenet,he2016deep}) they used a convolutional architecture to understand dynamics and forces acting behind the scenes from a static image and produced a ``most likely motion" rendered from a graphics engine. In a different framework \cite{lerer2016learning} and \cite{li2016visual} also used the power of deep learning to extract an abstract representation of the concept of stability of block towers purely from images. These approaches successfully demonstrated that not only was a network able to accurately predict the stability of the block tower but in addition, it could identify the source of the instability. Other approaches such as \cite{NIPS2016_6113} or \cite{denil2016learning} also attempted to learn intuitive physics of objects through manipulation. None of these approaches did, however, attempt to precisely model the evolution of the physical world.

\paragraph{Learning dynamics.}
Learning the evolution of an object's position also implies to learn about the object's dynamics regardless of any physical equations. While most successful techniques used LSTM-s \cite{Hochreiter:1997:LSM:1246443.1246450}, recent approaches show that propagation can also be done using a single cross-convolution kernel. The idea was further developed in \cite{visualdynamics16} in order to generate a next possible image frame from a single static input image. The concept has been shown to have promising performance regarding longer term predictions on the moving MNIST dataset in \cite{debrabandere16dynamic}. The work of \cite{OndruskaAAAI2016} also shows that an internal hidden state can be propagated through time using a simple deep recurrent architecture. These results motivated us to propagate tensor based state representations instead of a single vector representation using a series of convolutions. In the future we also aim to experiment with approaches inspired by \cite{visualdynamics16}. 


\paragraph{Learning physics.} 
Works of \cite{Galileo:NIPS:2015} and its extension \cite{phys101} propose methods to learn physical properties of scenes and objects. However in \cite{Galileo:NIPS:2015} the MCMC sampling based approach assumes a complete knowledge of the physical equations to estimate the correct physical parameters.
In \cite{phys101} deep learning has been used more extensively to replace the MCMC based sampling but this work also employs an explicit encoding and computation of physical laws to regress the output of their tracker.
% 
\cite{Stewart2016LabelFreeSO} also used physical laws to predict the movement of a pillow from unlabelled data though their approach was only applied to a fixed number of frames.

In another related approach \cite{fragkiadaki2015learning} attempted to build an internal representation of the physical world. Using a billiard board with an external simulator they built a network which observing four frames and an applied force, was able to predict the 20 next object velocities. Generalization in this work was made using an LSTM in the intermediate representations. The process can be interpreted as iterative since frame generation is made to provide new inputs to the network. This can also be seen as a regularization process to avoid the internal representation of dynamics to decay over time which is different to our approach in which we try to build a stronger internal representation that will attempt to avoid such decay.

Other research attempted to abstract the physics engine enforcing the laws of physics as neural network models. 
\cite{battaglia2016interaction} and \cite{chang2016compositional} were able to produce accurate estimations of the next state of the world. Although the results look plausible and promising, long term predictions are still an issue in such frameworks. Note, that their process is an iterative one as opposed to ours, which propagates an internal state of the world through time.

\paragraph{Approximate physics with realistic output.}
Other approaches also focused on learning the production of realistic future scenarios (\cite{CNNFluid2016} and \cite{jeong2015data}), or inferring collision parameters from monocular videos~\cite{MonszpartEtAl:SMASH:2016}. In these approaches the authors used physics based losses to produce visually plausible yet erroneous results. They however show promising results and constructed new losses taking into account additional physical parameters other than velocity.




