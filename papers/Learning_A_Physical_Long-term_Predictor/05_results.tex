% -------------------------------------------------------------------
%\section{Results}\label{sec:result}
% -------------------------------------------------------------------



\begin{figure*}[t!]
\includegraphics[width=1.0\linewidth,trim=480 0 0 0, clip]{images/results/S1_samples/S1_03_time_all_heatmap_40.pdf}
\caption{\textbf{Uncertainty prediction using probability maps.} The figure shows the output of \NetFour (40) on one example sequence in Scenario S1.}\label{fig:heat}
\end{figure*}

% -------------------------------------------------------------------
\subsection{Results}\label{sec:quant_eval}
% -------------------------------------------------------------------

\paragraph{Long term predictions.} \Cref{tab:results} and \cref{fig:qualitative} compare the baseline predictors and the four MechaNets on the task of long term prediction of the object trajectory. We call this ``long term'' in the sense that all methods observe only the first $T_0=4$ frames of a video (except the linear and quadratic extrapolators which observe the first 10 frames instead), to then extrapolate the trajectory to 40 time steps.

All networks can be used to perform arbitrary long predictions; the table, in particular, reports the the average $L^2$ prediction errors at time $T_\text{test}=20$ and $40$. However, models in this table are only shown the first $T_\text{train}=20$ frames of each video during training.

Consider first the prediction results for $T_\text{test}=T_\text{train}=20$. All networks perform considerably better than the linear and quadratic extrapolators in all scenarios, with error rates 5-40$\times$ smaller. As expected, Scenario S1 and S2 are harder than Scenario S0, which uses a fixed slope and homogeneous friction, but the network prediction errors are still small, in the order of 1-2 pixels. All networks perform similarly well, particularly in Scenarios S1, with a slight advantage for the LSTM-based propagation networks. \mbox{\SimNet} is very competitive, as may be expected given that it uses the ground-truth physics engine for integration. However, in Scenario S2 this method does not work well since the variable friction distribution is not observable from the first $T_0=4$ frames of a video;  \NetTwo and \NetFour, which can better learn such effects, can account for such uncertainty and significantly outperform \SimNet.

%on the other hand, the Gaussian (\NetThree) and softmax (\NetFour) networks predict the output uncertainty as well, as discussed below.

Results are different for predictions at time $T_\text{test}=40 \gg T_\text{train}$. All networks still outperform the extrapolators, but in Scenarios S0 and S1 \SimNet performs better than the other networks: by having access to the physics simulator, generalization is not an issue. On the other hand, this experiment shows that the deep networks have a limited capacity to generalize physics beyond the regimes observed during training. Among such networks, the ones modelling uncertainty (\NetThree and \NetFour) are able to generalize better. Scenario S2 still breaks the assumptions made by \SimNet, and the other networks outperform it.

\paragraph{Generalization.} The issue of generalization is explored in detail in~\cref{tab:generalization}, focusing on \NetFour that exhibits the best generalization capabilities. The table reports prediction errors at $T=10,20,30,40$ for networks trained with video sequence of length $T=10,20,30,40$ respectively. Recall that predictors always observe only the first $T_0=4$ frames of each sequence; the only change is to allow the training loss to assess the predictors' performance on progressively longer videos during training.

As expected, training on longer sequences dramatically improves the accuracy of longer term predictions, but also the shorter term ones. Training on the full sequences, in particular, performs $\sim20\%$ better than \SimNet. This confirms that, while deep networks are able to learn physical rules accurately for the range of physical experiences observed during training, they do not necessarily learn rules that generalize as readily as conventional physical laws.

\paragraph{Predicting uncertainty.} \NetThree and \NetFour predict a posterior distribution of possible object locations, using a Gaussian and a  probability map model respectively. \Cref{tab:results}~shows that the latter model has significantly lower perplexity, suggesting that the Gaussian model is somewhat too constrained. Qualitatively, \cref{fig:qualitative,fig:heat} show that both models make very reasonable predictions of uncertainty, with the uncertain area growing over time as expected.

% In \mbox{\cref{fig:result_graphs}a} the performance of baselines and the proposed networks on the scenario \szero. Since the plane rotation is not changing in this scenario, we expect good performance from all networks.

% In \mbox{\cref{fig:result_graphs}b} we increased the difficulty of the evaluation scenario by adding random rotation to the plane $\bp$ along its two main axes.

% In every reported result $L^2$ loss for \NetFour is a weighted average of the $L^2$ loss.

% \paragraph{Ability to generalize.}
% We note that in the \sone and \stwo case (\mbox{\cref{fig:result_graphs}b} and \mbox{\cref{fig:result_graphs}c}) the probabilistic variants were the two methods to get better generalization. It seems that the probability enforces the possibility of different scenarios. Entropy evolution \mbox{\cref{fig:entropy}} reveals that in both case entropy is enlarged directly after $T_0$.
% \NetFour\ has learnt to decrease entropy since only winning case do matter to its loss therefore its lower entropy. \NetThree\ however cannot decrease its variance unless it is confident about its prediction enforced by the likelihood loss. Our learning scheme however was based on $L^2$ loss which allows entropy to decrease more than it would have under a schema based on likelihood loss. Higher likelihood can be expected in this case at the expense of a good $L^2$ loss.

% The test scenario benchmarked in \mbox{\cref{fig:result_graphs}c} has the highest complexity due to the fact, that the slope in this case not only has a random tilt the method successfully has to infer from observing the motion of the object in the first $\ninputs$ frames, but it also has a patch-wise changing friction coefficient, as detailed in \cref{tab:datasets}, which is emulating a heterogeneous friction coefficient of the slope. Due to the fact, that our proposed networks are equipped with the capability of modelling uncertainty, they outperform all baselines in this scenario.
