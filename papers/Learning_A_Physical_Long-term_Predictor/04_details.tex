% -------------------------------------------------------------------
\section{Experiments}\label{sec:experiments}
% -------------------------------------------------------------------
%\aron{4.1 Scenarios, 4.2 Networks, 4.3 Implementation details}

% -------------------------------------------------------------------
\subsection{Data generation}\label{sec:data}
% -------------------------------------------------------------------


Experiments consider three variants of the physical setup described in~\cref{sec:phys}, called Scenarios \szero, \sone, and \stwo. Different scenarios sample experiments $\alpha = (q_x^0,q_y^0,\bn,\rho)$ of increasing difficulty. The parameters of each scenario are summarised in~\cref{tab:datasets} and described next. The plane normal $\bn$ was obtained by rotating the $Z$ axis around the $X$ and $Y$ axis by random angles $\anglex, \angley$ (Scenario \szero uses a fixed inclination). For Scenarios \sone and \stwo, the Coulomb friction coefficient $\rho$ of the plane is homogeneous and sampled uniformly at random. For Scenario \stwo, the plane is split in $10\times10$ patches, each with a random friction coefficient sampled independently. The friction upper bound was chosen so that the object always slides along the slope. 


\begin{table}[h!]
\footnotesize
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{|c|c|c|}
\hline
Scenario & $\bn$ rotation & $\rho$ \\
\hline
\szero & $\anglex=0,~\angley=\frac{\pi}{6}$ & $\rho \in \mathcal{U}\left(10^{-4},10^{-1}\right)$ \\
\sone  & $\anglex,\angley\in\mathcal{U}\left(-\frac{\pi}{6}, \frac{\pi}{6}\right)$ & $\rho \in \mathcal{U}\left(10^{-4},10^{-1}\right)$ \\
\stwo  & $\anglex,\angley\in\mathcal{U}\left(-\frac{\pi}{6}, \frac{\pi}{6}\right)$ & $\forall_{i,j=1\ldots {10}}~\rho_{i,j} \in \mathcal{U}\left(0.5,5\right)$ \\
\hline
\end{tabular}
\caption{\textbf{Data generation parameters.}
%We vary the rotation of the plane along at most two axes, and randomize the friction properties of the plane in order to ensure, that our internal physics representation generalizes over different influences of gravity.
}\label{tab:datasets}
\end{table}


\input{05a_longTermPrediction_table}
%\input{05b_longTermPrediction_figure}

The plane was rendered as a black object so that no static visual cues allow deducing any of the physical parameters except the initial position of the cube; instead the predictor has to approximate physics as needed by observing the motion of the object during the first $\ninputs=4$ frames of each experiment.

Each experiment was run for at most 240 frames, or terminated early if the object left the field of view. In order to observe enough movement in each recorded sequence, the first 30 video frames of each video were removed, and the rest of the video was sub-sampled by a factor of 3. In practice, most experiments consist of 40-50 images. 

The dataset contains 12,500 experiments for each Scenario, 70\% of which are used for training,  15\% for validation, and 15\% for test.

\paragraph{Implementation details.}
The object's starting position is initialized randomly using rejection sampling in such a way that $(q_x^0,q_y^0)$ falls in the slope quadrant that contains the largest visible $h$ coordinate. %\textit{E.g.}, for $\anglex < 0$ we only accept the normalized screen space coordinates $0.0 < q_x^0 < 0.49$ and $0.51 < q_x^0 < 1.0$ otherwise, and similarly for $\angley$.
This procedure generates samples that have most of their trajectory visible to the camera.

Rendering and physical simulation use Blender 2.77's OpenGL renderer and the Blender Game Engine (relying on Bullet 2 as physics engine) respectively. The object is a cube of side $0.1^3$ Blender units with \mbox{mass = 1}. The simulation parameters are: \mbox{max physics steps = 5}, \mbox{physics substeps = 3}, \mbox{max logic steps = 5}, \mbox{FPS = 120}. Rendering used white environment lighting (energy = 0.7) and no other light sources. The object color was set to Lambertian red (RGB: $0.8,~0.04,~0.04$) with no specular component. The slope is completely black, covering the whole field of view. The output images were stored as $128\times128$ color JPG files. See \cref{fig:simulation_setup} for an example initial setting from \mbox{Scenario~\stwo}. 

% -------------------------------------------------------------------
\subsection{Baseline predictors}\label{sec:networks}
% -------------------------------------------------------------------
\paragraph{Least squares fit.} We compare the performance of our methods to two simple least squares baselines: Linear and Quadratic. In both cases we fit two least squares polynomials to the \emph{estimated} screen-space coordinates of the first $T=10$ frames. The polynomials are of first and second degree(s), respectively. We estimate the object's position in this case by using the maximum location of the red channel of the input image. Note, that being able to observe the first 10 frames is a large advantage compared to the networks, which only see the first $\ninputs=4$ frames.
%either first order (linear) or second order (quadratic) polynomial with least squares method on each coordinates estimated on the 10 first object positions. 
%We also aim to compare to other methods, such as \cite{battaglia2016interaction} or \cite{fragkiadaki2015learning} in the future \todo{Aron: I don't think we should say this last sentence}.

\paragraph{Physics simulator.} The \SimNet baseline is used to evaluate the long term prediction ability of a neural network that has access to an explicit physics simulator, in a manner analogous to the work of~\cite{Galileo:NIPS:2015}.
%compare to which aims to use the laws of physics encoded in a physical engine. 
Similarly to the other networks, \SimNet observes the first $\ninputs$ images and aims to regress the physical parameters necessary to predict the trajectory of the object using the physics engine. The simulator is assumed to have access to a perfect model of the underlying physical laws. The regression architecture constitutes of the vector based feature extraction network described in \mbox{Section~\ref{sec:nets}} with an extra fully-connected layer on top to regress physical parameters.
The network is trained with an $L^2$ loss to infer the current slope rotation angles and friction coefficient ($\anglex, \angley, \rho$), the object's position at the observed frames ($t_0,\ldots,\ninputs-1$), and its final velocity at frame $\ninputs-1$.
%To this end we used the feature extraction network of \ref{sec:nets} with output $1 \times 1 \times 128$ and use these features to estimate the following physical parameters:

We input the regressed parameters to the \emph{same} physics simulation system that generated the dataset (\mbox{Section~\ref{sec:data}}) and run the simulation to predict the following object positions $\ninputs \ldots T$. Note that, since the simulator used by the network is the same as the one used to generate the data, this network is given a significant advantage over the other models.
%to generate the next object positions at time $T_0+1 \ldots T$ and compare with our ground truth.

%% This image should be changed
%\begin{figure}[h!]
%    \includegraphics[width=\linewidth]{images/simulator_exp}
%    \caption{Simulator experiment \aron{Do we need this figure?}}
%\end{figure}



\begin{figure*}[t!]
\newcommand{\incx}[1]{\includegraphics[width=0.33\linewidth,trim=20px 40px 30px 40px]{#1}}
\incx{images/results/S1_samples/S1_02_time_40.pdf}\hfill%
\incx{images/results/S2_samples/S2_05_time_40_nolegend.pdf}\hfill%
\incx{images/results/S1_samples/S1_03_time_40_ellipses.pdf}\\
\mbox{}\hfill(a)\hfill\hfill(b)\hfill\hfill(c)\hfill\mbox{}
\vspace{-1em}
\caption{\textbf{Qualitative results.} (a) Predictions of various networks in an example drawn from Scenario S1. The number of frames used to train each network is indicated in parenthesis; if omitted, it defaults to 20. (b) The same but for Scenario S2 and focusing on linear and quadratic fits, \SimNet, \NetTwo and \NetFour (30). (c) Example of uncertainty prediction of \NetThree in Scenario S1.}\label{fig:qualitative}
\end{figure*}


\begin{table}[b!]
\centering
\footnotesize
%\begin{tabular}{|c|cccc|cc|}
\sisetup{detect-weight=true,detect-inline-weight=math}
\begin{tabular}{|c|*4S[table-format=-2.]|*2S[table-format=2.]|}
\hline
{} & {MN4} & {MN4} & {MN4} & {MN4} & {QD} & {SN} \\
\hline
%\diagbox{test}{train} 
 & {10} & {20} & {30} & {40} & {--} & {--}
\\
\hline
10  & 1.18 & 1.60 & \bfseries 0.83 & 1.00 & 1.45 & 1.26 \\
20  & 11.79 & 2.13 & \bfseries 1.36 & 1.38 & 8.21 & 1.93 \\
30  & 28.04 & 6.91 & 2.71 & \bfseries 2.32 & 23.33 & 3.23 \\
40  & 48.65 & 19.54 & 8.96 & \bfseries 4.00 & 46.34 & 5.16 \\
\hline
\end{tabular}
\caption{\textbf{Generalization capabilities.} We compare predictions obtained at different times in \mbox{Scenario~\sone} from four versions of the \NetFour~(MN4) model that have been trained to predict the first $t=10, 20, 30, \text{and }40$ frames (rows). The train and test inputs always consists of the first $\ninputs=4$ images. For comparison, we also show the prediction accuracy of the quadratic baseline~(QD) (fit to the first 10 inputs) and \SimNet~(SN). The numbers represent the average, $L^2$ loss measured in pixels (input image size: $128\times128$).}
\label{tab:generalization}
\end{table} 



% -------------------------------------------------------------------
\subsection{MechaNets}\label{sec:implem}
% -------------------------------------------------------------------
%\footnote{\url{http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}}






Experiments consider four different variants of the mechanical prediction networks (\emph{MechaNet1} to 4 for short). \NetOne and \NetTwo are trained to optimise the $\losstwo$; \NetOne uses the LSTM propagation network and the spatially concentrated internal representation, whereas \NetTwo uses the simpler convolutional propagation network but the distributed representation. \NetThree and \NetFour are similar to \NetTwo, but they use probabilist predictors, using the Gaussian and probability map outputs, respectively. The four variants are summarized in~\cref{tab:results}.

\paragraph{Implementation details.} Network weights are initialized by sampling from a Gaussian distribution. Training uses a batch size of 50 using the first 10 to 40 frames of each video sequence using RMSProp~\cite{Tieleman2012}. Training is halted when there is no improvement of the $L^2$ loss after 40 consecutive epochs; 1,000 epochs were found to be usually sufficient for convergence. 

Since during the initial phases of training the network is very uncertain, the model using the Gaussian log-likelihood loss $\mathcal{L}_\text{nrm}$ was found to get stuck on solutions with very high variance $\Sigma(t)$; to solve this issue, the regularizer $\lambda \sum_t \det \Sigma(t)$ was added to the loss, setting $\lambda=10$ for the first few epochs and then lowering it to $\lambda = 0$ when the value of the determinant stablized under 100 on average (this variance is comparable to the image size).

In all our experiments we used Tensorflow \cite{tensorflow2015-whitepaper} r0.12 on a single NVIDIA Titan X GPU. 