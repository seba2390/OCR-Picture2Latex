\section{Introduction}\label{sec:introduction}

Most natural intelligences possess a remarkably accurate understanding of some of the physical properties of the world, as needed to navigate, prey, burrow, or perform any number of other ecologically-motivated activities. In particular, evolutionary pressure has caused most animals to develop the capability to perform fast and accurate predictions of mechanical phenomena. However, the nature of these mental models remains unclear and is being actively investigated~\cite{mentalModel}. 


Humans have developed an excellent \emph{formal understanding} of physics; for example, at the level of granularity at which animals operate, mechanics is nearly perfectly described by Newton's laws. However, while these laws are simple, their application to the description of a natural phenomena is anything but trivial. In fact, before such laws can be applied, a physical scenario needs first to be \emph{abstracted} (\textit{e.g.},\ by segmenting the world into rigid objects, describing those by mass volumes, estimating physical parameters). Then, except for the most trivial problems, predictions require the \emph{numerical integration} of very complex systems of equations. It is therefore unclear whether animals would perform mechanical predictions in this manner.

In this paper, we investigate how an accurate understanding of mechanical phenomena can emerge in artificial systems, mimicking natural intelligence. Inspired by a number of recent works, we look in particular at how deep neural networks can be used to perform mechanical predictions in simple physical scenarios (\cref{fig:simulation_setup}). Among such prior works, by far the most popular approach is to use neural networks \cite{phys101} to \emph{extract from sensory data local predictions of physical parameters}, such as mass, velocity, or acceleration, that are then integrated by an external mechanism such as a \emph{physical simulator} to obtain long term predictions. In other words, these approaches look at how an AI can abstract sensory data in physical parameters, but {\em not} how it can integrate such parameters over longer times. Further, such an approach assumes access to a simulator that accurately abstracts the physical world with appropriate Newtonian equations. Other attempts have also tried to replace the physical engine with a neural network \cite{battaglia2016interaction} but did not really attempt to observe the physical world and deduce properties from it but rather to integrate the physical equations.

% Everyone else is doing iterative updates, we do long term prediction from internal states.

% Position estimation is not done, everyone does velocity estimation and then integrates blindly. Good: more effective, Bad: coordinate frame dependent

By contrast, in this paper we perform end-to-end prediction of mechanical phenomena with a single neural network, implicitly combining prediction and integration of physical parameters from sensory data. In other words, while most other approaches predict instantaneous parameters such as mass and velocity from a few video frames, our model directly performs long-term predictions of physical parameters such as position well beyond the initial observation interval. Thus, as our main contribution, we propose to do so by learning an internal representation of a physical scenario which is induced by the observation of a few images and then is evolved in time by a recurrent architecture.

%While statistical predictors can reduce uncertainty by \emph{learning physical priors}, even the smartest extrapolator cannot be expected to predict the future deterministically.

One of the challenges of extrapolating physical measurements is that the state of a physical system can be determined only up to a certain accuracy, and such uncertainty rapidly accumulates over time. Since no predictor can be expected to deterministically predict the future, predictors are best formulated as estimating \emph{distribution of possible outcomes}. In this manner, predictors can properly account for uncertainty in the physical parameters, approximations in the model, or other limitations. Thus, our second contribution is to allow the neural network to explicitly model uncertainty by predicting distributions over physical parameters rather than their value directly.

In our experiments, we let convolutional neural networks choose their own internal representation of physical laws. A soft prior is that convolutional architectures encourage learning structures that are local and spatially homogeneous, similar to the applicable physical laws that are also local and homogeneous. However, the network is never explicitly told what physical laws are. Our third contribution, therefore, is to look at whether such networks can learn physical properties that generalise beyond regimes observed during training.

\begin{figure}[t]
    %\includegraphics[width=\linewidth]{images/heterog_friction.png}
    \begin{overpic}[width=\linewidth]{images/setup/splash2.pdf}
    \put(60, 93){\small Camera}
    \put(2,-2){\small Input images $t=0\ldots3$}
    %\put(17.5,46){\small $t$=9}
    \put(52,6){\small $t=19$}
    \put(67,6){\small $t=29$}
    \put(81,6){\small $t=39$}
    \put(34,14){\small RNN}
    \linethickness{0.8mm}
    \put(36,79){\color[rgb]{0., 0., 0.}\circle{5}}
    \end{overpic}
    \caption{\textbf{MechaNets.} We consider the problem understanding and extrapolating mechanical phenomena with recurrent deep networks. An orthographic camera looks at a red cube sliding down a black slope with random inclination and heterogeneous friction coefficient (indicated in the top image by the fake coloured tiles). The camera observes the cube for four frames (bottom left) and a recurrent network (bottom middle) predicts the long term motion for up to 40 frames (bottom right). Our goal is to investigate two which extent recurrent networks can develop an internal representation of physics.}    
    % for four frames a sliding cube (   
    % the rendered plane is always a black object (in the inset images, black plane has been replaced by white to improve legibility of this figure). A small red cube rests on the incline and starts sliding due to the effect of gravity. As it slides, it will pass by several different tiles.}
    \label{fig:simulation_setup}
\end{figure}

The relation of our work to the literature is discussed in~\cref{sec:related}. The detailed structure of the proposed neural networks is given and motivated in~\cref{sec:method}. These networks are extensively evaluated on a large dataset of simulated physical experiments in~\cref{sec:experiments}. A summary of our finding can be found in~\cref{sec:conclusions}.
