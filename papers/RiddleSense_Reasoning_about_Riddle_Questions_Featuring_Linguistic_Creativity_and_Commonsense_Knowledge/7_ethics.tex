\section*{{Ethical Considerations}}

\paragraph{Copyright of Riddles.} 
The RiddleSense dataset is consistent with the terms of use of the fan websites and the intellectual property and privacy rights of the original sources.
All of our riddles and answers are from fan websites that can be accessed freely.
The website owners state that we may print and download material from the sites solely for \textit{non-commercial use} provided that we agree not to change or delete any copyright or proprietary notices from the materials.
Therefore, 
in addition to the dataset itself, we also provide the according copyright statements of every website and an URL link to the original page for each riddle. 
The dataset users must sign an informed consent form that they will only use our dataset for \textit{research purposes} before they can access the both the riddles and our annotations.


% \paragraph{Crowd-workers.} This work presents a new dataset for addressing a new problem, riddle-style common-sense reasoning.  
% The wrong choices within the dataset were produced by filtering questions and using crowd-workers to annotate riddle-style common-sense questions by suggesting additional distractors. 
% Most of the questions are about common knowledge about our physical world.
% \textit{None of the questions involve sensitive personal opinions or involve personally identifiable information. }
% We study posted tasks to be completed by crowd-workers instead of crowd workers themselves, and we do not retrieve any identifiable private information about a human subject.
% All annotators were fairly compensated by the Amazon Mechanical Turk platform solely based on the quantity and quality of their submissions.


% We used 
% \yl{yuchen can you add compensation and IRB details}


% \smallskip
% \noindent
% \textbf{Data bias.} Like most crowdsourced data, and in particular most common-sense data, these crowdsourced answers are inherently subject to bias: for example, a question like ``'' might be answered very differently by people from different backgrounds and cultures.  
% % The prior multiple-choice CSR datasets which our datasets are built on are strongly biased culturally, as they include a single correct answer and a small number of distractor answers, while our new datasets include many answers considered correct by several annotators.  
% However, this potential bias (or reduction in bias) has not been systematically measured in this work.
 


% \smallskip
% \noindent
% \textbf{Sustainability.} 
% For most of the experiments,
% we use the virtual compute engines on Google Cloud Platform, which ``is committed to purchasing enough renewable energy to match consumption for all of their operations globally.''\footnote{\url{https://cloud.google.com/sustainability}}
% With such virtual machine instances, we are able to use the resources only when we have jobs to run, thus avoiding unnecessary waste.




