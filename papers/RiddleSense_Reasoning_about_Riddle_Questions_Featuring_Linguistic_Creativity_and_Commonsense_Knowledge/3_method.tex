\section{Construction of \textsc{RiddleSense}}
\label{sec:datagen}
In this section,
we first present our pipeline for collecting the \textsc{RiddleSense} dataset, including the details of data cleaning.
We introduce how we design a crowd-sourcing protocol for annotating quality distractors to turn riddle-solving into a multiple-choice question answering task.
% Finally, we present data analysis through the lens of a commonsense knowledge graph and show how challenging  \textsc{RiddleSense} is.



\subsection{Riddle Crawling and Cleaning}

We write web crawlers for collecting a large number (approximately 10,000) of riddles and their answers from public riddle websites, such as \textit{brainzilla.com}, \textit{riddlewot.com}, etc.
As the crawled data contain much noise such as inconsistent answer format and misspelled words, we process riddles through careful data cleaning as well as human verification.
First, we use an open-source tool for detecting typos\footnote{\url{github.com/phatpiglet/autocorrect}} and then refine the sentences.
Then we continuously sample (riddle, answer) pairs and recognize errors, for which we iteratively improve our program with a set of conditions to filter out noisy examples that are not readable or have ambiguous answers.
% Then we manually read through the set of riddles and remove any that are not readable or have ambiguous answers.
Also, we merge the riddles from different sources while removing duplicate riddle questions with similar answers.
For detecting duplicate riddles with minor word changes, we use SentenceBERT~\cite{reimers2019sentence} to find clusters with high cosine similarities.
% To make our riddles less language-specific, 
% we filter out the ones that focused on English letters and
% Finally, to verify the data quality, we randomly sampled 100 riddles and manually investigate its readability, only 3 of which contain noise but they are still readable.

%Therefore, comparing to 
% \begin{figure}[t]
% 	\centering
% % 	\hspace{-1em}
% 	\includegraphics[width=1\linewidth]{riddleqa_pipeline.pdf}
% 	\caption{Dataset collection pipeline.}
% 	\label{fig:pipe} 
% \end{figure}

\subsection{Distractor Collection from AMT}
We consider a multi-choice question answering format rather than the open-ended format, as it is easier to meaningfully compare the performance of different models in a more controlled manner --- there is a limited range of of options. For such a dataset, given a riddle-style question and 5 answer options, the model should select the best one as the predicted answer. 
This format offers a straightforward and fair evaluation metric -- \textit{accuracy}, which is the metric adopted by  many popular commonsense reasoning benchmarks such as CommonsenseQA, ARC~\cite{Clark2018ThinkYH}, and OpenbookQA~\cite{Mihaylov2018CanAS}. 

High-quality distractors are essential for multiple-choice question answering tasks as they can ensure that the dataset is both \textit{clean} and \textit{challenging} --- the distractors are neither too similar nor too distant from the correct answer.
We thus design a protocol to collect quality distractors from human annotators via \textit{Amazon Mechanical Turk}\footnote{\url{https://www.mturk.com/}} based on a pool of candidate distractors.


\paragraph{Candidate Distractor Pool}
We use $Q$ to denote the concepts that are mentioned in the question, and $a$ to denote the concept in the answer\footnote{If there are multiple concepts, we pick the one with the least network degrees 
as they tend to be more important.}.
We then first get all two-hop neighbors in the ConceptNet of $a$ and one-hop neighbors of each $c \in Q$ respectively:
\begin{align*}
    A &= \{x | (x, r_i, y), (y, r_j, a) \} \\
    B &= \{x | (x, r_k, c), \forall c\in Q \} \\
   &~~~~D = A \cap B ,
\end{align*}
where $r_{i/j/k}$ is a binary relation in the ConceptNet such as \texttt{HasProperty}.
The final intersection, $D$, is thus the pool of distractor candidates.
We further use \textit{WordNet}~\cite{miller1995wordnet} to filter out concepts that have either too low or too high \textit{Wu-Palmer} similarity\footnote{We use 0.5 as a threshold which is effective as expected.}.
We argue that such sampled distractors are semantically relevant to both questions and answers, and are also closer to answers in the WordNet taxonomy.
Thus, they are more likely to serve as ideal distractors in a multiple-choice question answering task. 

\paragraph{AMT Crowd-sourcing}
We design a three-stage annotation protocol:

\begin{itemize} 
    \item S1) {\textbf{Sanity Check}}. We show a question and 3 choices where only 1 choice is correct and the other 2 are randomly sampled concepts from the full vocabulary of ConceptNet. 
    Only when the workers pass this sanity check, their following annotations will be considered, so we 
    can avoid noise from random workers.
    
    \item S2) \textbf{Candidate Selection}. As it is difficult to control and verify the quality of distractors from crowd workers, 
we first sample concepts from ConceptNet, which are relevant to both question concepts and answer concepts, forming a set of candidate distractors $D$ for annotators to choose from.
% To improve the efficiency of annotation, we take the set of $D$ as candidates for short-answer riddles. 
Workers are required to select at least 5 concepts that they think are good distractors to the question. There are at least 3 different workers for each question and we take the candidates which are selected by at least two different workers to make sure the selected distractors are indeed meaningful.
    
    \item S3) \textbf{Open Distractor Collection}. 
    We also ask \textit{master workers} on AMT to write at least one more distractor based on the question context.
    This stage is important because sometimes the candidate pool contains fewer candidates of good quality and the human-written distractors are usually better than the ones in the candidate pool.
    We thus give extra bonus credits to encourage annotators to write more quality distractors.
\end{itemize}

\begin{table}[t]
\centering
	\scalebox{1
	}{
\begin{tabular}{@{}r|cc}
\toprule
                                            & \textbf{CSQA}      & \textbf{RS}       \\ \midrule
\# All    Examples                                  & 12,102     & 5,715      \\
\# Train Examples                                   & 9,741      & 3,510      \\
\# Validation  Examples                                    & 1,221      & 1,021      \\
\# Test  Examples                                   & 1,140      & 1,184      \\ \midrule
Average Question Length                     & 15.06     & 24.04 \\ 
\% Long Qs ($>$20 tokens) & 16.5\%   & 47.3\%   \\

Distinct Question Words                     & 6,822      & 7,110      \\
Distinct Choice Words                       & 7,044      & 9,912    \\
Avg PLL of Qs & -34.41 & -53.98 \\
\midrule
QA-NLI Conflict & 12.7\% & 39.6\% \\
QA-NLI Neutral & 71.6\% & 44.9\% \\
QA-NLI Entailment & 15.7\% & 15.5\% \\
 \bottomrule
\end{tabular}
}
\caption{Key statistics of the \textsc{RiddleSense} dataset (v1.1) vs the CommonsenseQA (CSQA) dataset.}
\label{tab:stat}
\end{table}
% Annotating the distractors from human annotators can be very expensive, however, we show that we can automatically generate quality distractors for training through exploiting existing commonsense knowledge resources such as ConceptNet, and pre-trained language models e.g., GPT-2.
% It is a challenge is to create great \textit{distractors} (i.e., the incorrect choices), such that the task is reasonably difficult while avoiding ambiguity for evaluation.
%We propose a novel adversarial-filtering method to create competitive yet incorrect distractors.  
% For \textbf{short-answer} riddles, the correct choices are mostly a single concept of ConceptNet.
% Our intuition is thus to randomly sample concepts that are close to both question concepts and answer concepts based on their connectivity.



% For the long-answer riddles, 
% we first fine-tune the GPT-2 model with the the HellaSwag~\cite{Zellers2019HellaSwagCA} dataset, which targets situation-based commonsense reasoning.
% It is thus suitable for using the generation of such fine-tuned GPT-2 to our riddles as the distractors.

% For the training set, we design an effective algorithm to automatically create  that are close to both question context and related to the answers.
% These concepts are close to both questions and answers while not being the synonyms of correct answers.
% However, the first method is not useful for those riddles whose answers are sentences or out of the vocabulary of ConceptNet.
% Thus, our second method is to train a seq2seq model for generating distractors given riddles and correct answers.
% We make use of the questions and distractors in other existing multi-choice question answering datasets to learn a generation model for generating the distractors.



\begin{figure*}[th]
	\centering
% 	\vspace{-2em}
    \subfloat[\centering Length of Q-A Paths]{{\includegraphics[width=3.75cm]{qa_paths.pdf} }}
    \subfloat[\centering Mean Length]{{\includegraphics[width=3.9cm]{mean_len.pdf} }}
    \subfloat[\centering Min Length]{{\includegraphics[width=3.9cm]{min_len.pdf} }}
    \subfloat[\centering Max Length]{{\includegraphics[width=3.9cm]{max_len.pdf} }}
% 	\includegraphics[width=0.9\linewidth]{all_curves.pdf}
	\caption{The Q-A paths serve as estimation of underlying reasoning chains. Fig.\ (a) illustrates how to compute mean/min/max of the Q-A paths: $\{q_1,q_2,q_3\}$ are three concepts mentioned in the question, and $a$ is the answer concept. $L_k$ is the length of the shortest path between $q_k$ and $a$ over ConceptNet; \textit{min/max/mean} are computed over $\{L_1,L_2,L_3\}$ as three aspects to measure the overall difficulty.
% 	The distribution of the question-answer path length when we take the min/mean/max over multiple question concepts, which serve as estimation of the length of underlying reasoning chains. 
Fig.\ (b), (c), and (d) show that generally \MyColorBox[FigRsBlue]{\textsc{RiddleSense}} has a longer question-answer path than \MyColorBox[FigCsqaOrange]{CommonsenseQA}, thus being harder to reason.
% 	The rightmost figure gives an illustrative figure for explaining the computation of min/max/mean of the Q-A paths.  $\{q_1,q_2,q_3\}$ are three concepts mentioned in the question, and $a$ is the answer concept. $L_k$ is the length of the shortest path between $q_k$ and $a$ over ConceptNet; \textit{min/max/mean} are computed over $\{L_1,L_2,L_3\}$ as three aspects to measure the overall difficulty.
	}
	\label{fig:path} 
\end{figure*}

% \begin{figure}[t]
% 	\centering
% 	\hspace{-1em}
% 	\includegraphics[width=0.73\linewidth]{qa_paths.pdf}
% 	\caption{Illustration of using \textit{Q-A paths} as proxy reasoning chains to measure the difficulty of a riddle: $\{q_1,q_2,q_3\}$ are three concepts mentioned in the question, and $a$ is the answer concept. $L_k$ is the length of the shortest path between $q_k$ and $a$ over ConceptNet; \textit{min/max/mean} are computed over $\{L_1,L_2,L_3\}$ as three aspects to measure the overall difficulty.}
% 	\label{fig:qapath} 
% \end{figure}




\section{Data Analysis of \textsc{RiddleSense}}
\label{sec:dataana}
In this section,
we first report the key statistics of the proposed \textsc{RiddleSense} dataset, 
then we compare it to CommonsenseQA~\cite{Talmor2018CommonsenseQAAQ} from two major angles: the distribution of the lengths of Q-A paths and the types of reasoning chains, which serve as an effective proxy to analyze the differences between the two datasets.


\begin{algorithm}[t]
    % \small
    \SetAlgoLined
    \SetKwInput{KwRemarks}{Remarks}
    \SetKwFunction{FnExtract}{extractConcept}
    \SetKwFunction{FnShortestPath}{shortestPathLen}
    \SetKwFunction{FnDeg}{deg}
    \SetKwFunction{FnAppend}{append}
    \SetKwFunction{FnMin}{min}
    \SetKwFunction{FnMax}{max}
    \SetKwFunction{FnMean}{mean}
    \KwIn{Knowledge graph \(KG=(V,E)\), riddle question \(Q\), riddle answer \(A\)}
    \KwOut{\(minPathLength\), \(maxPathLength\), \(meanPathLength\)}
    % \KwRemarks{\FnExtract{\(text\)} returns a set of tokenized concepts; \FnShortestPath{\(G, s, t\)} returns the length of the shortest path from \(s\) to \(t\) in graph \(G\) or \(None\) if no path exists; \FnDeg{\(G, v\)} returns the degree of \(v\) in graph \(G\)}
    
    \(QC \gets\) \FnExtract{\(Q\)}\\
    \(AC \gets\) \FnExtract{\(A\)}\\
    \(ac \gets v \in AC\) with smallest \FnDeg{\(G, v\)}\\
    \(l \gets\) []\\
     \ForEach{\(qc \in QC\)}{
        \(path \gets\) \FnShortestPath{KG, qc, ac}\\
        \If{\(path \neq None\)}{
            \(l\).\FnAppend{\(path\)}
        }
     }
    \(minPathLength \gets\) \FnMin(\(l\))\\
    \(maxPathLength \gets\) \FnMax(\(l\))\\
    \(meanPathLength \gets\) \FnMean(\(l\))
    \caption{Get statistics of QA paths.}
    \label{algo:path-len}
    % \vspace{-1em}
\end{algorithm}


\begin{table*}[th]
    % \vspace{-1em}
	\centering
	\scalebox{0.78
	}{
\begin{tabular}{c|c|c|c}
\toprule
 \multicolumn{4}{c}{\cellcolor{brown!25} \textbf{CommonsenseQA\quad(CSQA)}}                                                                                                                            \\ \midrule
\textbf{1-hop (\underline{14.0\%})}      & \textbf{2-hop (\underline{34.4\%})}                & \textbf{3-hop (\underline{41.5\%})}                           & \textbf{4-hop  (\underline{9.5\%})}                                      \\\midrule
AtLoc (4.8\%)  & \MyColorBox[red!15]{Related-Related }  (8.3\%)   & \MyColorBox[red!15]{Related-Related-Related }  (4.1\%)    & \MyColorBox[red!15]{Related $\times$ 4} ({0.4}\%)    \\
\MyColorBox[red!15]{Related} (3.4\%) & Related-AtLoc (4.5\%) & Related-Related-AtLoc (2.7\%) & Related $\times$ 3 -AtLoc (0.3\%) \\
Causes (1.1\%)      & Related-Antonym (1.8\%)     & Related-AtLoc$^{-1}$-AtLoc (1.4\%) & Related-Related-AtLoc$^{-1}$-AtLoc (0.3\%) \\
Antonym (0.9\%)     & Related-IsA$^{-1}$ (1.3\%)        & Related-Related-Antonym (1.3\%)      & Related $\times$ 3 -Antonym (0.2\%)      \\
Capableof (0.8\%)   & Related-AtLoc$^{-1}$ (0.9\%) & Related-Related-CapableOf (1.3\%)    & Related$\times$2-SubEvent$^{-1}$-Cause (0.1\%)    \\ 
... & ... & ... & ...  \\ \midrule
\rowcolor{yellow!20} $\rho=\frac{3.4}{4.8}=0.7$ & $\rho=\frac{8.3}{4.5}=1.8$  & $\rho=\frac{4.1}{2.7}=1.5$  & $\rho=\frac{0.4}{0.3}=1.3$   \\
\midrule

\midrule
\multicolumn{4}{c}{\cellcolor{cyan!15}\textbf{RiddleSense\quad(RS)}} \\ \midrule
\textbf{1-hop (\underline{4.6\%})}       & \textbf{2-hop (\underline{31.6\%}) }               & \textbf{3-hop (\underline{47.8\%}) }               & \textbf{4-hop (\underline{14.0\%}) }                                    \\ \midrule
\MyColorBox[red!15]{Related} (3.1\%)   & \MyColorBox[red!15]{Related-Related } (13.1\%)  & \MyColorBox[red!15]{Related-Related-Related }  (10.6\%)   &  \MyColorBox[red!15]{Related $\times$ 4} (1.8\%)    \\
Antonym ({0.4}\%)     & Related-Antonym (2.1\%)     & Related-Related-IsA$^{-1}$ (2.6\%)         & Antonym-Related$\times$3 (0.4\%)      \\
IsA$^{-1}$ (0.3\%)        & Related-IsA$^{-1}$ (2.0\%)        & Related-Related-Antonym (1.6\%)      & Related$\times$3 -IsA$^{-1}$ (0.3\%)         \\
PartOf (0.1\%)      & Related-AtLoc$^{-1}$ (1.3\%) & Related-Antonym-Related (1.5\%)      & Related$\times$2-IsA$^{-1}$-Related (0.3\%)         \\
AtLoc$^{-1}$ (0.1\%) & Antonym-Related (0.8\%)     & Antonym-{Related-Related } (1.5\%)      & Related$\times$2-Antonym-Related (0.3\%) \\ 
... & ... & ... & ...  \\
\midrule
\rowcolor{yellow!20} $\rho=\frac{3.1}{0.4}=7.8$ & $\rho=\frac{13.1}{2.1}=6.2$  & $\rho=\frac{10.6}{2.6}=4.1$  & $\rho=\frac{1.8}{0.4}=4.5$   \\
\bottomrule 
\end{tabular}
	} 
	
	\caption{The top-5 most frequent types of reasoning chains in CSQA and RS datasets, grouped by their length $k=\{1,2,3,4\}$.
	The \MyColorBox[yellow!20]{implicit-ratio $\rho$} is defined as the ratio of the implicit reasoning types (i.e., \MyColorBox[red!15]{Related$\times k$}) over the most frequent types with at least one explicit relation (e.g., AtLoc) of the same length $k$.
% 	Note that RS has more implicit chains (i.e., \MyColorBox[red!15]{Related$\times k$}; CSQA=16.2\% vs. RS=28.6\%) and longer paths in general (e.g., CSQA=14.0\% vs. RS=4.6\% in 1-hop), suggesting that questions in RS are harder to reason. 
	}
	\label{tab:path}
\end{table*}

\subsection{Key Statistics}
Table~\ref{tab:stat} presents the key statistics of \textsc{RiddleSense} (RS) and the comparisons with CommonsenseQA (CSQA) which is the most similar benchmark to ours.
Although the size of \textsc{RS} is smaller than CSQA,
we argue that \textsc{RS} is complementary to the CSQA dataset and introduces novel challenges for the commonsense reasoning community.
% can be seen as an extension of CSQA because they share the same task format and target at testing commonsense reasoning ability.
As they share the same format, we can test different methods by training on either CSQA-only, RS-only, or the concatenation of CSQA and RS, as we show later in Section~\ref{sec:exp}.

Moreover, there is a greater number of long questions (i.e., containing more than 20 words) in RS than in CSQA.
Additionally, we find that RS questions have a lower normalized pseudo-likelihood (PLL)~\cite{salazar2019masked}, a proxy of estimating sentence probability, suggesting that RS questions are more puzzling (i.e., the words are less frequently co-occurring).
We also use a RoBERTa model fine-tuned on MNLI~\cite{Williams2018ABC} to perform natural language inference between CSQA/RS questions and their answers.
There is a much greater proportion of questions in RS that have \textit{conflicting} relations with their correct answers than compared to CSQA. 
This is indicative of RS's complexity due to the \textit{self-contradictory} and \textit{perplexing} nature of riddles.

Interestingly, we also find that although there are about twice as many examples in CSQA as RS, there are more distinct words in the questions and answer choices of RS than CSQA, suggesting that RS covers more diverse topics than CSQA.
% There are about 6k riddles in total and 
% The lower ratio of long-answer ones are due to that most long-answer riddles on those websites are merely jokes instead of commonsense reasoning problems.
% Through the lens of graph connectivity over ConceptNet,
% we find that the average distance between question concepts and answer concepts is \textbf{3.7}, while the CommonsenseQA dataset is only \textbf{1.5}, suggesting that the riddle questions are more difficult to understand and reason about.
% \subsection{Knowledge Analysis}



\subsection{Distribution of the Lengths of Q-A Paths}
Our main intuition is that the shortest paths between question concepts and the answer concepts can approximate the \textit{underlying reasoning chains}, which are hidden and difficult to label.
To understand the difference between CSQA and RS in terms of their reasoning chains,
we use \textit{Q-A paths} over ConceptNet as a proxy. 
For a riddle question, a set of \textit{Q-A path lengths} are the lengths of the shortest paths between every question concept and the answer concept, i.e., \textit{shortestPathLen(KG, qc, ac)} in Alg. \ref{algo:path-len}.
For a question-answer pair, we first extract the concepts mentioned in the question and the answer respectively (\textit{extractConcept()} in Algorithm \ref{algo:path-len}), following the steps of~\citeauthor{kagnet-emnlp19} (\citeyear{kagnet-emnlp19}) and \citeauthor{feng2020scalable} (\citeyear{feng2020scalable}).
If there are three question concepts $\{q_1, q_2, q_3\}$ and an answer concept $a$, we denote their shortest path lengths as $\{L_1, L_2, L_3\}$.
Finally, we compute the min/max/mean over them for a comprehensive understanding of the approximated difficulty of this riddle 
--- a greater value indicates a more challenging example. 




% Note that we may have multiple question concepts while only one answer concept, so we report three metrics --- \textit{min/mean/max} over the length of shortest paths of all question concepts and the answer concept in every example, so that we can have a comprehensive understanding of the comparisions between CSQA and RS. 
As shown in Figure~\ref{fig:path} (b), we can see that RS has longer Q-A paths as underlying reasoning chains.
In addition,
we can see that RS generally has longer chains, particularly the min of CSQA is $1$-hop for more than 80\% of examples.
On the other hand, only about 30\% of RS examples have 1-hop minimum Q-A paths, while about 50\% of the examples have 2-hop min Q-A paths.
The distribution over the maximum in Figure~\ref{fig:path} (d) also shows that RS tends to have longer maximum paths than CSQA.
We also show the percentage of all Q-A paths of different length as part of Table~\ref{tab:path},
and we can see that RS has longer paths in general (e.g., CSQA = 14.0\% vs. RS = 4.6\% in 1-hop).



\subsection{Relational Types of Reasoning Paths}
In addition to the analysis on path length, we also show that the relation types of Q-A paths for RS and CSQA have clear differences, as shown in Table~\ref{tab:path}.
The types of reasoning chains in RS rely more on a special relation in ConceptNet --- \texttt{Related}, which is relatively more implicit and can not be grounded to a specific, explicit relation such as \texttt{AtLoc} (e.g., \textless\textit{wind}, \texttt{Related}, \textit{air}\textgreater ~vs. \textless\textit{lamp}, \texttt{AtLoc}, \textit{table}\textgreater).
The most frequent relation between question concepts and answer concepts in CSQA is the \texttt{AtLoc} relation (4.8\%), however, it is \texttt{Related} (3.1\%) in RS.
We define \textit{implicit-ratio} for $k$-hop paths, 
$\rho_k=\frac{\% (\texttt{Related}\times k)}{\% (E_k)}$,
where $E_k$ is the most frequent type of chains with at least one explicit relation of length $k$.
In RS, $\rho_k$ is around $4.1\sim7.8$, while it is about $0.7\sim1.8$ for CSQA.
Thus, we conclude that the dominant reasoning chains in RS are much more implicit, and consequently RS is more challenging to reason with using commonsense knowledge resources like ConceptNet.



