\epigraph{\normalsize ``\textit{ \textbf{The essence of a riddle is to express true facts under impossible combinations.}}"}{\normalsize--- \textit{Aristotle}, \textit{Poetics} (350 BCE)\vspace{0pt}}

\noindent
A \textit{riddle} is a puzzling question about {concepts} in our everyday life.
% , and we which one needs common sense to reason about.
For example, a riddle might ask ``\textit{My life can be measured in hours. I serve by being devoured. Thin, I am quick. Fat, I am slow. Wind is my foe. What am I?}''~
The correct answer ``\textit{candle},'' is reached by considering a collection of \textit{commonsense knowledge}:
{a candle can be lit and burns for a few hours; a candle's life depends upon its diameter; wind can extinguish candles, etc.}
\begin{figure}[t]
	\centering 
	\includegraphics[width=1\linewidth]{riddle_intro_final.pdf}
	\caption{ 
    The top example is a trivial commonsense question from the CommonsenseQA~\cite{Talmor2018CommonsenseQAAQ} dataset. 
    The two bottom examples are from our proposed \textsc{RiddleSense} challenge.
    The right-bottom question is a descriptive riddle that implies multiple commonsense facts about \textit{candle}, and it needs understanding of figurative language such as metaphor;
    The left-bottom one additionally needs counterfactual reasoning ability to address the \textit{`but-no'} cues. 
    These riddle-style commonsense questions  require NLU systems to have higher-order reasoning skills with the understanding of creative language use.
	}
	\label{fig:intro} 
\end{figure}

It is believed that the \textit{riddle} is one of the earliest forms of oral literature,
which can be seen as a formulation of thoughts about common sense, a mode of association between everyday concepts, and a metaphor as higher-order use of natural language~\cite{hirsch2014poet}.
Aristotle stated in his \textit{Rhetoric} (335-330 BCE) that good riddles generally provide satisfactory metaphors for rethinking common concepts in our daily life.
He also pointed out in the \textit{Poetics} (350 BCE): ``the essence of a riddle is to express true facts under impossible combinations,'' which suggests that solving riddles is a nontrivial  reasoning task.

Answering riddles is indeed a challenging cognitive process as it requires \textit{complex} {commonsense reasoning skills}.
% which we refer to \textit{higher-order commonsense reasoning}. 
% A successful riddle-solving model should be able to reason with \textit{multiple pieces} of commonsense facts, as 
A riddle can describe \textit{multiple pieces} of commonsense knowledge with \textit{figurative} devices such as metaphor and personification (e.g., ``wind is my \underline{foe} $\xrightarrow[]{}$ \textit{extinguish}'').
% , as shown by the examples in Figure~\ref{fig:intro}.
%%%
Moreover, \textit{counterfactual thinking} is also necessary for answering many riddles such as ``\textit{what can you hold in your left hand \underline{but not} in your right hand? $\xrightarrow[]{}$ your right elbow.}''
These riddles with \textit{`but-no'} cues require that models use counterfactual reasoning ability to consider possible solutions for situations or objects that are seemingly impossible at face value.
This \textit{reporting bias}~\cite{gordon2013reporting} makes riddles a more difficult type of commonsense question for pretrained language models to learn and reason.
% In addition, the model needs to associate commonsense knowledge with the creative use of language in descriptions, which may have figurative devices such as metaphor and personification (e.g., ``wind is my \underline{foe} $\xrightarrow[]{}$ \textit{extinguish}''). 
%For instance, one needs to know that devour
% Thus, a riddle here can be seen as a complex commonsense question that tests higher-order reasoning ability with creativity.
In contrast, \textit{superficial} commonsense questions such as ``\textit{What home entertainment equipment requires cable?}'' in  CommonsenseQA~\cite{Talmor2018CommonsenseQAAQ} are more straightforward and explicitly stated.
We illustrate this comparison in Figure~\ref{fig:intro}.


In this paper,
we introduce the \textsc{RiddleSense} challenge 
to study the task of answering riddle-style commonsense questions\footnote{We use ``riddle'' and ``riddle-style commonsense question'' interchangeably in this paper.} requiring \textit{creativity}, \textit{counterfactual thinking} and \textit{complex commonsense reasoning}.
\textsc{RiddleSense} is presented as a \textit{multiple-choice question answering} task where a model selects one of five answer choices to a given riddle question as its predicted answer, as shown in Fig.~\ref{fig:intro}.
We construct the dataset by first crawling from several free websites featuring large collections of human-written riddles and then aggregating, verifying, and correcting these examples using a combination of human rating and NLP tools to create a dataset consisting of 5.7k high-quality examples.
Finally, we use \textit{Amazon Mechanical Turk} to crowdsource quality distractors to create a challenging benchmark.
We show that our riddle questions are more challenging than {CommonsenseQA} by analyzing graph-based statistics over ConceptNet~\cite{Speer2017ConceptNet5A}, a large knowledge graph for common sense reasoning.

% The distractors for the training data are automatically generated from ConceptNet and language models while the distractors for the dev and the test sets are crowd-sourced from Amazon Mechanical Turk (AMT).
% Through data analysis based on graph connectivity, 




Recent studies have demonstrated that
 fine-tuning large pretrained language models, such as {BERT}~\cite{Devlin2019}, RoBERTa, and ALBERT~\cite{Lan2020ALBERT}, can achieve strong results on current commonsense reasoning benchmarks.
Developed on top of these language models, graph-based language reasoning models such as KagNet~\cite{kagnet-emnlp19} and MHGRN~\cite{feng2020scalable} show superior performance. 
Most recently, UnifiedQA~\cite{khashabi2020unifiedqa} proposes to unify different QA tasks and train a text-to-text model for learning from all of them, which achieves state-of-the-art performance on many commonsense benchmarks.

To provide a comprehensive benchmarking analysis, we systematically compare the above methods.
Our experiments reveal that while humans achieve 91.33\% accuracy on \textsc{riddlesense}, the best language models can only achieve 68.80\% accuracy, suggesting that there is still much room for improvement in the field of solutions to complex commonsense reasoning questions with language models.
% We also provide error analysis to better understand the limitation of current methods.
We believe the proposed \textsc{RiddleSense} challenge suggests productive future directions for machine commonsense reasoning as well as the understanding of higher-order and creative use of natural language.


% (previous state-of-the-art on \texttt{CommonsenseQA} (56.7\%)).
% However, there still exists a large gap between performance of said baselines and human performance.
% we show that the questions in RiddleSense is significantly more challenging, in terms of the length of the paths from question concepts and answer concepts.


%Apart from that, current pre-trained language models (e.g., BERT~\cite{}, RoBERTa~\cite{}, etc.) and commonsense-reasoning models (e.g., KagNet~\cite{}), can be easily adapted to work for this format with minimal modifications. 


%Note that these auto-generated distractors may be still easy for , which could diminish the testing ability of the dataset.
%We design an ader filtering method to get rid of the false negative   and control the task difficulty. 
% To strengthen the task, we propose an adversarial cross-filtering method to remove the distractors that ineffectively mislead the selected base models.
% Finally, we use human efforts to inspect the distractors and remove false negative ones, to make sure that all distractors either does not make sense or much less plausible than the correct answers.
%Introducing these fine-tuned models is inspired by the adversarial filtering algorithms~\cite{}, which can effectively reduce the  bias inside datasets for creating a more reliable benchmark.  



%Those distractors are explicitly annotated by human experts such that they are close to the meaning of 
%The main idea is to use multiple trainable generative models for learning to generate answers in a cross-validation style. 
%The wrong predictions
%Simply put, for every step, we use a large subset of the riddles and their current options ot learn multiple models for answering the remaining riddles via generation.
%After each step, we consolidate the 


% In the distantly supervised learning, we use the definition of concepts (i.e., glossary) of \textit{Wiktionary}\footnote{\url{https://www.wiktionary.org/}} to create riddles with answers as training data. 
% In the transfer learning setting, we aim to test the transferability of models across relevant datasets, such as CommonsenseQA~\cite{Talmor2018CommonsenseQAAQ}.

% We believe the \textsc{RiddleSense} task can benefit multiple communities in natural language processing. 
% First, the commonsense reasoning community can use \textsc{RiddleSense} as a new space to evaluate their reasoning models. The \textsc{RiddleSense} focuses on more complex and creative commonsense questions, which will encourage them to propose more higher-order commonsense reasoning models. 
% Second, \textsc{RiddleSense} is an NLU 
% task similar to those in the GLUE~\cite{wang2018glue} and SuperGLUE~\cite{wang2019superglue} leaderboard that can serve as a benchmark for testing various pre-trained language models.
% Last but not the least, as our task shares the similar format with many open-domain question answering tasks like \textit{Natural Questions}~\cite{kwiatkowski2019natural}, researchers in QA area may be also interested in \textsc{RiddleSense}. 




