We first introduce three types of popular baseline methods for commonsense reasoning (Section~\ref{sec:baseline}), then we present our main experimental results with analysis (Section~\ref{sec:result}), and finally show case studies for error analysis (Section~\ref{sec:error}). 

% \textit{Please find the hyper-parameters and implementation details in our supplementary file.}
\subsection{Baseline Methods}
\label{sec:baseline}
% \textsc{RiddleSense} is a 5-way multiple-choice question answering task. 
Given a riddle question $q$, there are 5 different choices $\{c_1, \dots, c_5\}$, where only one of them is the correct choice and the others are distractors.
The model needs to rank all choices and select the best one as the final answer.
There are three major types of models for commonsense reasoning tasks in this format: 
1) fine-tuning pretrained language models, 
2) incorporating relevant knowledge graphs for reasoning, 
3) fine-tuning a unified text-to-text QA model, 
as shown in Figure~\ref{fig:models}.


\paragraph{Fine-tuning Pre-trained LMs}
As we seek to investigate how well current NLU models can perform in higher-order commonsense reasoning,
we first experiment with a typical set of large pretrained language models such as BERT~\cite{Devlin2019BERTPO}, RoBERTa~\cite{Liu2019RoBERTaAR}, 
% XLNet~\cite{Yang2019XLNetGA}, 
and ALBERT~\cite{Lan2020ALBERT}.
% Following previous works,
We concatenate the question with each choice, using \texttt{[SEP]} as the separator, thus forming a \textit{statement}.
Then, we fine-tune any pretrained LMs like BERT to use their \texttt{[CLS]}  token embeddings to predict a score for each statement.
Then, a set of five scores about an example will be fed to SoftMax to optimize for maximizing the score of the correct choice.


\paragraph{LMs + Graph Reasoning Modules}
KagNet~\cite{kagnet-emnlp19} and MHGRN~\cite{feng2020scalable} are two typical graph-based language reasoning models.
They both extract a \textit{schema graph} from ConceptNet, i.e., a subgraph of ConceptNet consisting of Q-A paths in Figure~\ref{fig:path},
by incorporating them with a graph encoding module.
They finally fuse the external commonsense knowledge with a text encoder (e.g., a pretrained LM). 
KagNet uses heuristics to prune irrelevant paths and then encode them with path-based LSTM and hierarchical attention to select the most important paths for improving commonsense reasoning.
In contrast, the recent MHGRN explicitly encodes multi-hop paths at scale using graph networks with relational attention, improving efficiency and performance over KagNet and other models.
A unique merit of such graph-based models is their \textit{interpretibility} due to the neural attention over the symbolic structures of KGs. 


\paragraph{Fine-Tuning a Text-to-Text QA Model}
{UnifiedQA}~\cite{khashabi2020unifiedqa}, the state-of-the-art multiple-choice QA model, simply concatenates the question with all answer candidates as a single input sequence to a T5~\cite{t5} model for learning to generate the correct choice as extracting a span from the input.
Apart from the multiple-choice QA format, it is also trained with other QA task formats so that it can benefit from many other QA datasets (including CSQA) via sharing the model parameters.

\paragraph{Human Evaluation}
We invite three native English speakers who study computer science to solve 100 riddle examples sampled from the test set.
They achieved an average accuracy of 91.3\%.

% These methods are shown in Figure~\ref{fig:models}.
% We sampled 100 test examples and ask 3 master students at a to test their average human accuracy --- 90.33. 




\begin{table*}[th!]
    % \vspace{-1em}
	\centering
	\scalebox{0.92
	}{
		\begin{tabular}{c||cc||cc||cc}
		\toprule
              \multicolumn{1}{c||}{Models $\downarrow$ Training Data $\rightarrow$}& \multicolumn{2}{c||}{\cellcolor{brown!25}Train = CSQA} & \multicolumn{2}{c||}{\cellcolor{cyan!20}Train = RiddleSense} & \multicolumn{2}{c}{\cellcolor{blue!15}Train = RS+CSQA} \\ \midrule
            RiddleSense-Split   $\rightarrow$  & Dev          & Test        & Dev         & Test       & Dev            & Test           \\ \midrule
\rowcolor{gray!20} \textit{Random Guess} &  20.0     &  20.0   & 20.0      &  20.0       &     20.0            &  20.0  \\ \midrule
BERT-Base~\cite{Devlin2019}    & 33.59         & 34.61        & 54.16        & 42.43       & 56.22           & 47.67           \\
BERT-Large~\cite{Devlin2019}     & 36.14         & 39.10        & 55.24        & 45.09       & 57.69           & 54.91           \\
RoBERTa-Large~\cite{Liu2019RoBERTaAR}  & 43.68         & 47.42        & 60.72        & 52.58       & 66.11           & 59.82           \\
ALBERT-XXL~\cite{Lan2020ALBERT} & \textbf{51.03}         & \textbf{51.00 }       & \underline{66.99}        & \underline{60.65}       &\textbf{ 71.50 }          &\underline{ 67.30  }         \\ 
\midrule
KagNet (RoBERTa-L)~\cite{kagnet-emnlp19}  &   42.66  & 48.24 & 61.77  & 53.72  & 66.55  & 59.72   \\
MHGRN (RoBERTa-L)~\cite{feng2020scalable} & 46.83 & 49.65 & 63.27 & 54.49 & 66.90 & 63.73       \\
MHGRN (ALBERT-XXL)~\cite{feng2020scalable}  &  \underline{50.89}         & {50.21}       & {66.27}        & {59.93}       & \underline{70.81}          &{ 66.81  }         \\ \midrule
UnifiedQA (T5-Large)~\cite{khashabi2020unifiedqa}   &  28.50 & 37.27 & 56.21 & 56.40 & 58.17 & 56.57 \\
UnifiedQA (T5-3B)~\cite{khashabi2020unifiedqa}   & 37.32         & \underline{50.25}       & \textbf{67.38}        & \textbf{66.06}       &        68.26              &       	\textbf{68.80} \\ \midrule
\rowcolor{gray!20} \textit{Human Performance}  &  -     &  91.33   & -      &  91.33       &      -            &   91.33  \\ \bottomrule
\end{tabular}
	} 
	
	\caption{Benchmark performance over the dev and test set of \textsc{RiddleSense}  .}
	\label{tab:results}
\end{table*}


\begin{figure}[t]
	\centering 
	\includegraphics[width=1\linewidth]{curve_acc.pdf}
	\caption{The curve of dev accuracy using different percentage of the RS-training data, respectively for RoBERTa-Large and ALBERT-XXL. }
	\label{fig:curve} 
\end{figure}

\subsection{Results and Analysis}
\label{sec:result}

We show the main results of the experiments in Table~\ref{tab:results}.
There are 3 settings according to the different training data options: 1) the training data of CSQA, 2) the training data of RS, and 3) the concatenation of both RS and CSQA, while all experiments are validated over the dev set of RS.
However, as the public UnifiedQA checkpoints were already trained on CSQA (together with many other QA datasets), we directly use them for inference over RS in the first setting (i.e., ``Train=CSQA'').
This also suggests that the performance of UnifiedQA models in 2nd setting should be better than others although they all are fine-tuned on RS's training data only.
% Also, we fine-tune them with RS's train and the concatenation of RS's and CSQA's for the 2nd and 3rd settings.


We can see that larger pretrained language understanding models always gain better performance, ranging from BERT-base to {Albert-XXL}, which gets the best performance in this group of baselines (67.30\%).
This matches their performance comparisions on CSQA and other benchmark datasets as well, suggesting that a better pre-trained language model can be also identified by \textsc{RiddleSense} as well.
Interestingly, we find that ALBERT-XXL is so powerful that it can generalize from training on CSQA only but achieve comparable results with RoBERTa-Large that is trained over RS (i.e., 51.0\% vs. 52.6\%).
However, if we look at the curve of dev accuracy when using different percentage of the RS-train data (setting 2) in Figure~\ref{fig:curve}, 
we can see that RoBERTa-Large can generally outperform ALBERTA-XXL when using less than 60\% data for fine-tuning.

% In contrast, we find that the seq2seq model, UnifiedQA, with T5-large as the base model does not outperform even RoBERTa-Large. 

% We also conduct experiments over different ratio of training data and report ALBERT-XXL's dev acc in Fig.~\ref{fig:curve}.
% This suggests that the better performance only happen when training data is large enough, while we hope future models can use fewer training data, i.e., unsupervised or few-shot learning.

Moreover, we find that the {KG-enhanced models}, KagNet and MHGRN, 
using RoBERTa-Large (RB-L) as the encoder, perform better than vanilla RB-L.
Although the Q-A paths over ConceptNet have more implicit paths (e.g., \texttt{Related}$\times k$),
some paths can still be beneficial. For example, $$\textit{wind} \xleftrightarrow[]{~\texttt{Related}~} \textit{blow} \xleftrightarrow[]{~\texttt{Related}~} \textit{candle},$$
can still help reason about the riddle \textit{``... Wind is my foe. What am I?''} to the answer ``candle.''

The fusion of ConceptNet also improves in the situation when only training with CSQA data using RoBERTa-Large.
However, the improvement of KagNet is negative, which is unexpected. 
We conjecture that this is because the extracted subgraphs from the ConceptNet does not guarantee the reasoning path from question concepts to answer concepts, while the training phase \textit{forces} models to learn to reason over those graphs, yielding a possibly \textit{harmful} impact.
% When the text encoder module is more powerful, 
Additionally, we find that MHGRN with ALBERT-XXL also results in a worse performance, unlike using RoBERTa-Large.
We believe this may be related to the specific design of ALBERT, which reuses model parameters for multiple layers, and thus it could be a problem when fused with another learnable module (e.g., a graph network in MHGRN).



Fine-tuning UnifiedQA with T5-3B achieves the best performance, which is also the case for CSQA in their leaderboard. 
This is expected for two reasons: 1) UnifiedQA has been trained over multiple other QA datasets, which increases its generalization ability, 2) UnifiedQA considers all choices together at a time and thus can better compare different choices with self-attention mechanism of Transformer~\cite{vaswani2017attention}.


% We found that the best model's dev accuracy over the short-answer riddles (59.2\%) is lower than long-answer riddles (64.3\%), as we find that the distractors for long-answer riddles, eve collected from AMT, are less natural and shorter and may thus be easier to detect. 
% We would like to further improve the quality of the long-answer riddles in the near future.

% Transferring CommonsenseQA to RiddleSense can be helpful.
% We first concatenate the CommonsenseQA's training set and RiddleSense training data as a larger training dataset (around 13k), and then re-train the above models. We found that most of the models have a gain about 5$\sim$7\% accuracy performance. The transferred ALBERT-XXL in the end has \textbf{66.90\%} accuracy.
% This suggests that transferring the ability of solving straightforward commonsense questions can 
% lead better performance.

\begin{figure*}[th!]
	\centering
% 	\vspace{-1em}
	\includegraphics[width=1\linewidth]{error_analysis.pdf}
	\caption{Case studies of the error by UnifiedQA-3B model on the test set of \textsc{RiddleSense}.}
	\label{fig:error} 
\end{figure*}

\subsection{Error Analysis and Future Directions}
\label{sec:error}
We show a few examples that are mistakenly predicted by the UnifiedQA-3B model in Figure~\ref{fig:error}.
From these concrete cases, 
we can see that even the best model cannot solve riddles that can be trivial to humans, especially when there are metaphors and/or counterfactual situations. 
We argue that future research should aim to address the creative use of language in commonsense reasoning and general understanding of language, 
as creativity is a critical feature of natural language.
We list several promising directions as follows.

First of all, we should \textit{mine (semi-)structured knowledge of metaphors}, so that concepts can connect via metaphorical links (e.g., ``tail'' $\rightarrow$ ``thread'').
Second, to prevent false inferences, we need \textit{more complete, precise commonsense knowledge of concepts}. For example, in Figure~\ref{fig:error}, a model should know a chair only has exactly \textit{four} legs instead of \textit{hundreds}~\cite{lin2020birds}; ink can be black or red, but it won’t change over time. However, current KGs only have (leg, PartOf, chair) and (ink, HasProperty, black/red). 
In addition, the reasoning methods should incorporate more \textit{symbolic logic rules}, so that the multi-hop conditions and counterfactual ``but-no'' negations will be handled better. 
Finally, we think the graph-augmented methods should be improved to \textit{compare multiple options} in a schema graph, e.g., \textsc{QA-GNN}~\cite{yasunaga2021qagnn}.  
Both \textsc{KagNet} and \textsc{MHGRN} consider only a single option at a time which prevents them from effectively reasoning about the subtle differences between options.
