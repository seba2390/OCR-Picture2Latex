 We propose a novel commonsense reasoning challenge, \textsc{RiddleSense}, which requires complex commonsense skills for reasoning about creative and counterfactual questions, coming with a large multiple-choice QA dataset.  
 We systematically evaluate recent commonsense reasoning methods over the proposed \textsc{RiddleSense} dataset, and find that the best model is still far behind human performance, suggesting that there is still much space for commonsense reasoning methods to improve.
 We hope \textsc{RiddleSense} can serve as a benchmark dataset for future research targeting complex commonsense reasoning and computational creativity.


\section*{Acknowledgements}
This research is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007, the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research, the Defense Advanced Research Projects Agency with award W911NF-19-20271, and NSF SMA 18-29268. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. We would like to thank all the collaborators in USC INK research lab and the reviewers for their constructive feedback on the work.