
\subsection*{Benchmarking Machine Common Sense}
 
The prior works on building commonsense reasoning benchmarks touch different aspects of commonsense reasoning:
SWAG~\cite{Zellers2018SWAGAL}, HellaSWAG~\cite{Zellers2019HellaSwagCA}, CODAH~\cite{Chen2019CODAHAA}, aNLI~\cite{bhagavatula2019abductive} for situation-based reasoning;
Physical IQA~\cite{bisk2020piqa} on physical knowledge;
Social IQA~\cite{sap-etal-2019-social} on social psychology knowledge;
LocatedNearRE~\cite{Xu2018AutomaticEO} on mining spatial commonsense knowledge;
DoQ~\cite{elazar2019large} and NumerSense~\cite{lin2020birds} on numerical common sense;
CommonGen~\cite{lin2019commongen} for generative commonsense reasoning, and many others;
OpenCSR~\cite{lin2021opencsr} and ProtoQA~\cite{Boratko2020ProtoQAAQ} aim to test commonsense reasoning ability in an open-ended setting.
% However, none of them requires higher-order reasoning ability where understanding creativity use of language is a must.

CommonsenseQA~\cite{Talmor2018CommonsenseQAAQ} has the same format as our proposed \textsc{RiddleSense}, and both target general commonsense knowledge via multiple-choice question answering.
However, CSQA focuses more on straightforward questions where the description of the answer concept is easy to understand and retrieval over ConceptNet, while RS makes use of riddle questions to test higher-order commonsense reasoning ability.
More detailed comparisions between them are in Section~\ref{sec:dataana}, which shows that the unique challenges of the RiddleSense on multiple dimensions.

\subsection*{Commonsense Reasoning Methods}
Our experiments cover three major types of commonsense reasoning methods that are popular in many benchmarks: fine-tuning pretrained LMs~\cite{Devlin2019, Liu2019RoBERTaAR, Lan2020ALBERT}, graph-based reasoning with external KGs~\cite{kagnet-emnlp19, feng2020scalable}, and fine-tuning unified text-to-text QA models~\cite{khashabi2020unifiedqa}.
Apart from ConceptNet, 
There are also some methods~\cite{lv2019graph, xu2020fusing}
using additional knowledge resources such as Wikipedia and Wiktionary.
A few recent methods also aim to generate relevant triples via language generation models so that the context graph is more beneficial for reasoning~\cite{wang2020connecting, yan2020learning}.
Our experiments in this paper aim to compare the most typical and popular methods which have open-source implementations,
which we believe are beneficial for understanding the limitation of these methods in higher-order commonsense reasoning --- \textsc{RiddleSense}.

\subsection*{Computational Creativity and NLP}
Creativity has been seen as a central property of the human use of natural language~\cite{mcdonald1994creative}.
% As NLP seeks to process all kinds of human language, 
Text should not be always taken at face value, however, higher-order use of language and figurative devices such as metaphor can communicate richer meanings and needs deeper reading and more complicated reasoning skills~\cite{veale2011creative}.
Recent works on processing language with creative use focus on metaphor detection~\cite{gao2018neural}, pun generation~\cite{He2019PunGW, Luo2019PunGANGA}, creative story generation, and humor detection~\cite{Weller2019HumorDA, Weller2020TheRD}, sarcasm generation~\cite{chakrabarty2020r}, etc. 

Riddling, as a way to use creative descriptions to query a common concept, are relatively underexplored.
Previous works~\cite{tan2016solving, oliveira2018exploring} focus on the generation of riddles in specific languages and usually rely on language-specific features (e.g., decomposing a Chinese character into multiple smaller pieces).
There is few datasets or public resources for studying riddles as a reasoning task, to the best of our knowledge. 
The proposed \textsc{RiddleSense} is among the very first works connecting commonsense reasoning and computational creative, and provides a large dataset to train and evaluate models for answering riddle questions.


% \smallskip
% \noindent
% \textbf{CommonsenseQA vs} \textbf{RiddleSense}.
% The most similar work to ours is CommonsenseQA, which is also a multi-choice question answering task, and targets general commonsense knowledge like \textsc{RiddleSense} does.
% % They sample a pair of concepts ($c_q, c_a$) that are directly connected in ConceptNet, i.e., there exists such a triple ($c_q, r, c_a$). Then they ask human annotators to compose a question that mentions $c_q$, to which the answer is exactly $c_a$.
% %They control the difficulty of the task by restricting that 
% % Therefore, the composed questions in their datasets are mostly one-hop questions, and about 50\% of them are about the \texttt{AtLocation} relation.
% % The current state-of-the-art models already achieves very high performance, and the scope 
% %Also, many distractors in can also make sense. 
% In contrast,
% \textsc{RiddleSense} does not rely on the single-hop connections in ConceptNet but use natural existing riddles about everyday concepts and situations.
% The connections between question concepts and answer concepts are much more distant than CommonsenseQA, yielding a higher-order commonsense reasoning problem.
% Another unique feature of \textsc{RiddleSense} is the creativity in the riddle questions, where understanding metaphor and other rhetoric methods may be helpful.

