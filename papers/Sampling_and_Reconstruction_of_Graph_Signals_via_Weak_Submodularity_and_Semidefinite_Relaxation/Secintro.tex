Consider a network represented by a graph $\mathcal{G}$ consisting of a node set $\mathcal{N}$ of 
cardinality $N$ and a weighted adjacency matrix $\mathbf{A} \in \mathbb{R}^{N \times N}$ whose
$(i,j)$ entry, $A_{ij}$, denotes weight of the edge connecting node $i$ to node $j$.  A \textit{graph 
signal} $\mathbf{x} \in \mathbb{R}^{N}$ can be viewed as a vertex-valued network 
process that can be represented by a vector of size $N$ supported on $\mathcal{N}$, where its $i$
\textsuperscript{th} component denotes the signal value at node $i$. Under the assumption that properties of the network process relate to the underlying graph, the goal of graph signal processing (GSP) is to generalize traditional signal processing tasks and develop algorithms that fruitfully exploit this relational structure \cite{shuman2013,sandryhaila2013}.

A keystone generalization which has drawn considerable attention in recent years pertains to sampling 
and reconstruction of graph signals \cite{shomorony2014sampling,tsitsvero2016signals,anis2016efficient,chen2015discrete,chepuri2016subsampling,marques2016sampling,gama2016rethinking,chamon2017greedy}. The task of finding an exact sampling set to perform reconstruction with minimal information loss is known to be NP-hard.
Conditions for exact reconstruction of graph signals from noiseless samples were put forth in~\cite{shomorony2014sampling,tsitsvero2016signals,anis2016efficient,chen2015discrete}. Sampling of noise-corrupted signals using randomized schemes including uniform and leverage score sampling is studied in \cite{chen2016signal}, for which optimal sampling distributions and
performance bounds are derived.
In \cite{chepuri2016subsampling,chamon2017greedy}, reconstruction of graph signals and their power spectrum density is studied and greedy schemes are developed. However, the performance guarantees 
in \cite{chen2016signal,chamon2017greedy} are restricted to the case of stationary graph signals, i.e., 
the covariance matrix 
%of the signal 
in the nodal or spectral domains has certain structure (e.g., diagonal; see also \cite{marques2016stationaryTSP16,perraudinstationary2016,girault_stationarity}). 

In this paper, we study the problem of sampling and reconstruction of graph signals and propose two algorithms that solve it approximately. First, we develop a semidefinite programming (SDP) relaxation that finds a near-optimal sampling set in polynomial time. Then, we formulate the sampling task as that of maximizing a monotone weak submodular function and propose an efficient randomized greedy algorithm motivated by \cite{mirzasoleiman2014lazier}. We analyze the performance of the randomized greedy algorithm and in doing so, we show that the MSE associated with the selected sampling set is on expectation a constant factor away from that of the optimal set. Moreover, we prove that the randomized greedy algorithm achieves the derived approximation bound with high probability for every sampling task.  
In contrast to prior work, our results do not require stationarity of the signal. Finally, in simulation studies we illustrate the superiority of the proposed schemes over state-of-the-art randomized and greedy algorithms \cite{chen2016signal,chamon2017greedy} in terms of running time, accuracy, or both.
\footnote{MATLAB implementations of the proposed algorithms are available at \url{https://github.com/realabolfazl/GS-sampling}.} 

% The rest of the paper is organized as follows. Section \oldref{sec:pre} formally states the sampling problem and reviews the relevant background. In Section \oldref{sec:alg}, we introduce the SDP relaxation-based and randomized greedy algorithms for the sampling task and theoretically analyze the performance of the latter. Section \oldref{sec:sim} presents the simulation results while the concluding remarks are stated in Section \oldref{sec:concl}. 

\noindent\textbf{Notation.} 
% We briefly summarize the notation used in the paper. Capital letters represent sets, e.g., $S$. Bold capital letters refer to matrices and bold lowercase letters represent vectors. 
$\A_{ij}$ denotes the $(i,j)$ entry of matrix $\A$, $\a_j$ is the $j\ts{th}$ row of $\A$, $\A_{S,r}$ ($\A_{S,c}$) is a submatrix of $\A$ that contains rows (columns) indexed by the set $S$, and $\lambda_{\max}(\A)$
and $\lambda_{\min}(\A)$ represent the maximum and minimum eigenvalues of $\A$, respectively. $\I_N \in \R^{N\times N}$ denotes the identity matrix and $[N] := \{1,2,\dots,N\}$.