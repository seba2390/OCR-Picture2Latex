We study the recovery of simulated noisy signals supported on synthetic and real-world graphs to assess performance of the proposed sampling algorithms in terms of MSE and running time. To this end, we first 
consider an undirected Erd\H{o}s-R\'enyi random graph $\mathcal{G}$ of size $N=100$ and edge probability 0.2 \cite{newman2010networks}. Bandlimited graph signals $\x = \U\bar{\x}_K$ are generated by taking $\mathbf{U}$ as the first $k=30$ eigenvectors of the graph adjacency matrix. The non-zero frequency components $\bar{\x}_K$ are drawn from a zero-mean, multivariate Gaussian distribution with covariance matrix $\P$ which is selected uniformly at random from the set of positive semi-definite (PSD) matrices.  Zero-mean Gaussian noise $\n$ with covariance $\sigma^2 = 10^{-2}\I_N$ is added to $\x$. Algorithms \oldref{alg:sdp} and \oldref{alg:greedy} are run to recover the signal for different sampling set sizes. We compare the MSE performance of the proposed schemes with the state-of-the-art greedy algorithm \cite{chamon2017greedy} and the random sampling approaches in \cite{chen2016signal}. For the randomized greedy algorithm we use $\epsilon = 0.1$ and $\epsilon = 0.01$.  Fig. \oldref{fig:rand} (top) depicts the MSE versus $k$ (sample size), where the results are obtained 
by averaging over $100$ Monte-Carlo simulations. As the figure indicates, Algorithms \oldref{alg:sdp} and \oldref{alg:greedy} outperform the random sampling schemes of \cite{chen2016signal} and perform nearly as well as the greedy sampling algorithm \cite{chamon2017greedy}. While not shown here for the sake of clarity of the presentation, similar patterns were also observed for other workhorse random graphs, e.g., preferential attachment and Barb\'asi--Albert models \cite{newman2010networks}.
%\begin{figure}[t]
%\vspace{-0.3cm}
%\centering
%    \includegraphics[width=0.45\textwidth]{Figures/mserand.eps}
%    \caption{Erd\H{o}s-R\'enyi graph. MSE comparison of different sampling schemes. }
%\label{fig:rand}
%\vspace{-0.1cm}
%\end{figure}

\begin{figure}[t]
	\vspace{-0.3cm}
	\begin{minipage}[b]{\linewidth}
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/mserand.eps}
		%\centerline{(a)}\medskip
	\end{minipage}
	%
	\begin{minipage}[b]{\linewidth}
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/hist.eps}
		%\centerline{(b)}\medskip
	\end{minipage}
	%	
	\vspace{-0.5cm}
	\caption{Erd\H{o}s-R\'enyi graph. Comparison of different schemes in terms of (top) MSE as a function of the size of the sampling set; and (bottom) histogram of MSE values for $100$ realizations and fixed sampling set size. }
	\label{fig:rand}
	\vspace{-0.3cm}
\end{figure}

Next, we study the performance of the proposed schemes for each individual sampling tasks (each Monte-Carlo realizations),  for the setting where $N=10$ and $k=4$. 
Bandlimited graph signals are generated as before except that this time we take $\mathbf{U}$ as the first $4$ eigenvectors of the adjacency matrix. Fig. \oldref{fig:rand} (bottom) depicts superimposed MSE histograms of Algorithms \oldref{alg:sdp} and \oldref{alg:greedy} as well as the greedy sampling scheme \cite{chamon2017greedy} for 100 realizations per method and fixed $|S|=4$. As the figure illustrates, the proposed SDP relaxation and randomized greedy schemes perform well and are comparable with the greedy approach.
%\begin{figure}[t]
%\vspace{-0.3cm}
%\centering
%    \includegraphics[width=0.45\textwidth]{Figures/hist.eps}
%    \caption{Erd\H{o}s-R\'enyi graph. MSE histograms of different sampling schemes.}
%\label{fig:hist}
%\vspace{-0.1cm}
%\end{figure}

Finally, we test Algorithm \oldref{alg:greedy} on the Minnesota road network\footnote{https://sparse.tamu.edu/Gleich/minnesota} with $N=2642$ nodes in order to showcase scalability of the proposed graph sampling method. To that end, Bandlimited graph signals are generated by taking the first $k=600$ eigenvectors of the graph Laplacian matrix, where the non-zero frequency components are drawn from a zero-mean, multivariate Gaussian distribution with randomly chosen PSD covariance matrix $\P$. The signals are corrupted with additive white Gaussian noise with $\sigma^{2}=10^{-2} \mathbf{I}_{N}$. 
%Then, performance of Algorithm \oldref{alg:greedy} and algorithm in \cite{chamon2017greedy} are averaged over $10$ Monte-Carlo simulations. 
As expected, Figs.~\oldref{fig:min} (top) and (bottom) depict trends of decreasing MSE and increasing running time versus $|S|$, respectively. The results are averaged over $1000$ Monte-Carlo simulations run on a commercial laptop with an Intel Core i$7$ processor at $3.1$ GHz. Remarkably, the proposed randomized greedy procedure achieves an order-of-magnitude speedup over the state-of-the-art algorithm in \cite{chamon2017greedy} while showing only a marginal degradation in the MSE performance.
%
\begin{figure}[t]
	\vspace{-0.3cm}
	\begin{minipage}[b]{\linewidth}
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/mseMin.eps}
		%\centerline{(a)}\medskip
	\end{minipage}
	%
	\begin{minipage}[b]{\linewidth}
		\centering
		\includegraphics[width=0.8\textwidth]{Figures/timeMin.eps}
		%\centerline{(b)}\medskip
	\end{minipage}
	%
	\vspace{-0.5cm}	
	\caption{Minnesota road network. (top) MSE and (bottom) running time comparison of different sampling schemes as a function of the size of the sampling set.}
	\label{fig:min}
	\vspace{-0.3cm}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[t]
%\vspace{-0.3cm}
%\centering
%    \includegraphics[width=0.45\textwidth]{Figures/mseMin.eps}
%    \caption{Minnesota road network. MSE comparison of different sampling schemes.}
%\label{fig:minmse}
%\vspace{-0.1cm}
%\end{figure}
%\begin{figure}[t]
%\vspace{-0.3cm}
%\centering
%    \includegraphics[width=0.45\textwidth]{Figures/timeMin.eps}
%    \caption{Minnesota road network. Running time comparison of different sampling schemes.}
%\label{fig:mintime}
%\vspace{-0.4cm}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure*}[!htb]
% 	\minipage{0.32\textwidth}
% 	\includegraphics[width=\linewidth]{Figures/MSE_erdos.eps}
% 	\caption{blah}\label{fig:MSE_erdos_avg}
% 	\endminipage\hfill
% 	\minipage{0.32\textwidth}
% 	\includegraphics[width=\linewidth]{Figures/MSE_erdos.eps}
% 	\caption{blah}\label{fig:blah}
% 	\endminipage\hfill
% 	\minipage{0.32\textwidth}%
% 	\includegraphics[width=\linewidth]{Figures/MSE_erdos.eps}
% 	\caption{blah}\label{fig:MSE_erdos}
% 	\endminipage
% \end{figure*}
% \begin{figure}[h]
% 	\centering    
% 	\includegraphics[width=1\linewidth]{Figures/minnesota_MSE.eps}
% 	\caption{....}
% 	\label{fig:minnesota_MSE}
% 	\vspace{-0.5cm}
% \end{figure} 
% \begin{figure}[h]
% 	\centering    
% 	\includegraphics[width=1\linewidth]{Figures/minnesota_run_time.eps}
% 	\caption{....}
% 	\label{fig:minnesota_run_time}
% 	\vspace{-0.5cm}
% \end{figure} 