Here we resort to two approximation methods to find a near-optimal solution $S$ of \ref{eq:probf}. Our proposed algorithms are based on semidefinite and weak submodular optimization techniques that have recently shown superior performance in applications such as sensor selection\cite{joshi2009sensor}, graph sketching \cite{gama2016rethinking}, wireless sensor networks \cite{shamaiah2012greedy}, Kalman filtering \cite{ma}, and sparse signal recovery \cite{das2011submodular,hashemi2016sparse}. 
\vspace{-0.25cm}
\subsection{Sampling via SDP relaxation}
We first develop an SDP relaxation for problem \ref{eq:probf}. Our proposed scheme is motivated by the framework of \cite{joshi2009sensor} developed in the context of sensor scheduling. However, our focus is on sampling and reconstruction of graph signals which entails a different objective function, i.e., MSE.
Let $z_i \in \{0,1\}$ indicate whether the $i\ts{th}$ node of $\mathcal{N}$ is included in the sampling set $S$ and define $\z = [z_1,z_2,\dots,z_N]^\top$. Then, \ref{eq:covf} can alternatively be written as
%
\begin{equation}
\bar{\S}_z = \left(\P^{-1}+\sigma^{-2}\sum_{i=1}^n z_i\u_i\u_i^\top\right)^{-1}.
\end{equation}
%
Therefore, by relaxing the binary constraint $z_i \in \{0,1\}$ one can obtain a convex relaxation of  \ref{eq:probf},
%
\begin{equation}\label{eq:probf2}
\begin{aligned}
& \underset{\z}{\text{min}}
\quad \mathrm{Tr}\left(\bar{\S}_z\right)
& \text{s.t.}\quad 0\leq z_i \leq 1, \phantom{k} \sum_{i=1}^Nz_i \leq k.
\end{aligned}
\end{equation}
%
In order to obtain an SDP in standard form, let $\C$ be a positive semidefinite matrix such that $\C \succeq \bar{\S}_z$. 
Then, \ref{eq:probf2} is equivalent to 
%
\begin{equation}\label{eq:probf3}
\begin{aligned}
& \underset{\z,\C}{\text{min}}
\quad \mathrm{Tr}\left(\B\right)
& \text{s.t.}\quad 0\leq z_i \leq 1, \phantom{k} \sum_{i=1}^nz_i \leq k, \phantom{k} \C -\bar{\S}_z \succeq \mathbf{0}.
\end{aligned}
\end{equation}
%
The last constraint in \ref{eq:probf3}, i.e., $\C -\bar{\S}_z \succeq \mathbf{0}$, can be thought of as being the Schur complement \cite{horn2012matrix} of the block matrix
%
\begin{equation}
\B = \begin{bmatrix}
    \C& \I\\
    \I & \bar{\S}_z^{-1}
\end{bmatrix}.
\end{equation}
%
Note that the Schur complement of $\B$ is positive semidefinite if and only if $\B\succeq \mathbf{0}$ 
\cite{horn2012matrix}. Therefore, replacing the last constraint in \ref{eq:probf3} with the positive semidefiniteness 
constraint on $\B$ results in the following SDP relaxation:
%
\begin{equation}\label{eq:sdp}
\begin{aligned}
& \underset{\z,\B}{\text{min}}
\quad \mathrm{Tr}\left(\B\right)
& \text{s.t.}\quad 0\leq z_i \leq 1, \phantom{k} \sum_{i=1}^nz_i \leq k, \phantom{k} \B \succeq \mathbf{0}.
\end{aligned}
\end{equation}
%
An exact solution to \ref{eq:sdp} can be obtained by means of existing SDP solvers; see, e.g.,~\cite{arora2005fast,grant2008cvx}.  However, the solution $\hat{\z}$ contains real-valued entries and hence a rounding procedure is needed
to obtain a binary solution. Here, we propose to use the rounding procedure introduced in \cite{joshi2009sensor} and accordingly select the nodes of $\mathcal{N}$ corresponding to the $k$ $z_i$'s with largest values. The proposed SDP relaxation sampling scheme is summarized as Algorithm 1.
%=================================== ALGORITHM 1
\renewcommand\algorithmicdo{}	% removes "DO" from for loops
\begin{algorithm}[t]
\caption{SDP Relaxation for Graph Sampling}
\label{alg:sdp}
\begin{algorithmic}[1]
    \STATE \textbf{Input:}  $\P$, $\U$, $k$.
    \STATE \textbf{Output:} Subset $S\subseteq \mathcal{N} $ with $|S|=k$.
    \STATE Find $\z$, the minimizer of the SDP relaxation problem in \ref{eq:sdp} 
    \STATE Set $S$ to contain nodes corresponding to top $k$ entries of $\z$
	\RETURN $S$.
\end{algorithmic}
\end{algorithm}
\vspace{-0.25cm}
%===================================
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sampling via a randomized greedy scheme}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The computational complexity of the SDP approach developed in Section 3.1 might be challenging in applications 
dealing with large graphs. Hence, we now propose an iterative randomized greedy algorithm for the task of sampling 
and reconstruction of graph signals by formulating \ref{eq:probf} as the problem of maximizing a monotone weak 
submodular set function. First, we define the notion of monotonicity, submodularity, and curvature that will be useful 
in the subsequent analysis.

A set function $f:2^X\rightarrow \mathbb{R}$ is submodular if
%
\begin{equation}
f(S\cup \{j\})-f(S) \geq f(T\cup \{j\})-f(T)
\end{equation}
%
for all subsets $S\subseteq T\subset X$ and $j\in X\backslash T$. The term $f_j(S)=f(S\cup \{j\})-f(S)$ is the marginal value of adding element $j$ to set $S$. Moreover, the function is monotone if $f(S)\leq f(T)$ for all $S\subseteq T\subseteq X$. 

The concept of submodularity can be generalized by the notion of curvature or submodularity ratio \cite{das2011submodular} that quantifies how close a set function is to being submodular. Specifically, the maximum element-wise curvature of a monotone non-decreasing function $f$ is defined as
%
\begin{equation}
{\cal C}_{\max}=\max_{1\le l<n}{\max_{(S,T,i)\in \mathcal{X}_l}{f_i(T)\slash f_i(S)}},
\end{equation}
%
with $\mathcal{X}_l = \{(S,T,i)|S \subset T \subset X, i\in X \backslash T, |T\backslash S|=l,|X|=n\}$. Note that a set function is submodular if and only if ${\cal C}_{\max}~\le~1$. Set functions with ${\cal C}_{\max} > 1$ are called weak or approximate submodular functions \cite{das2011submodular}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Next, similar to \cite{chamon2017greedy}, we formulate \ref{eq:probf} as a  set function maximization task. Let $f(S) = \mathrm{Tr}(\P-\bar{\S}_S)$. Then,  \ref{eq:probf} can equivalently be written as
%
\begin{equation}\label{eq:probsub}
\begin{aligned}
& \underset{S}{\text{max}}
\quad f(S)
& \text{s.t.}\quad S\subseteq\mathcal{N},\quad |S| \leq k.
\end{aligned}
\end{equation}

In Proposition \oldref{thm:p} below, by applying the matrix inversion lemma 
\cite{bellman1997introduction} we establish that $f(S)$ is monotone and weakly submodular. 
Moreover, we derive an efficient recursion to find the marginal gain of adding a new node 
to the sampling set $S$. 
%
\begin{proposition}\label{thm:p}
\textit{$f(S) = \mathrm{Tr}(\P-\bar{\S}_S)$ is a weak submodular, monotonically increasing set function, $f(\emptyset)=0$, and for all $j \in \mathcal{N}\backslash S$
%
\begin{equation}\label{eq:mg}
f(S\cup \{j\})-f(S) = \frac{\u_j^\top\bar{\S}_S^{2}\u_j}{\sigma^{2}+\u_j^\top\bar{\S}_S\u_j},\: \text{ and }
\end{equation}
%
\begin{equation}\label{eq:upf}
\bar{\S}_{S \cup\{j\}} = \bar{\S}_S-\frac{\bar{\S}_{S}\u_{j}\u_{j}^\top\bar{\S}_{S}}{\sigma^2+\u_{j}^\top\bar{\S}_{S}\u_{j}}.
\end{equation}}
%
\end{proposition}
%
Proposition \oldref{thm:p} enables efficient construction of the sampling set in an iterative fashion. To further reduce the computational cost, we propose a randomized greedy algorithm that performs the task of sampling set selection in the following way.  Starting with $S = \emptyset$, at iteration $(i+1)$ of the algorithm, a subset $R$ of size $s$ is sampled uniformly at random and without replacement from $\mathcal{N} \backslash S$. 
The marginal gain of each node in $R$ is found using \ref{eq:mg}, and the one corresponding to the highest marginal gain is added to $S$. Then, the algorithm employs the recursive relation \ref{eq:upf} to update $\bar{\S}_S$ for the subsequent iteration. This procedure is repeated until some stopping criteria, such as condition on the cardinality of $S$ is met. Regarding $s$, we follow the suggestion in \cite{mirzasoleiman2014lazier} and set $s=\frac{N}{k}\log\frac{1}{\epsilon}$, where $e^{-k}\leq \epsilon<1$ is a predetermined parameter that controls trade-off between the computational cost and MSE of the reconstructed signal; randomized greedy algorithm with smaller $\epsilon$ produces sampling solutions with lower MSE while the one with larger $\epsilon$ requires lower computational costs. Note that if $\epsilon = e^{-k}$, the randomized greedy algorithm in each iteration considers all the available nodes and hence  matches the greedy scheme proposed in \cite{chamon2017greedy}. However, as we illustrate in our simulation studies, the proposed randomized greedy algorithm is significantly faster than the method in \cite{chamon2017greedy} for larger $\epsilon$ while returning essentially the same sampling solution. The randomized greedy algorithm is formalized as Algorithm 2.

%=================================== ALGORITHM 2
\renewcommand\algorithmicdo{}	% removes "DO" from for loops
\begin{algorithm}[t]
\caption{Randomized Greedy Algorithm for Graph Sampling}
\label{alg:greedy}
\begin{algorithmic}[1]
    \STATE \textbf{Input:}  $\P$, $\U$, $k$, $\epsilon$.
    \STATE \textbf{Output:} Subset $S\subseteq \mathcal{N} $ with $|S|=k$.
    \STATE Initialize $S =  \emptyset$, $\bar{\S}_{S}=\P$.
	\WHILE{$|S|<k$}\vspace{0.1cm}
			\STATE Choose $R$ by sampling $s=\frac{N}{k}\log{(1/\epsilon)}$ indices uniformly at random from $\mathcal{N}\backslash S$
            \STATE $j_s = \argmax_{j\in R} \frac{\u_j^\top\bar{\S}_S^{2}\u_j}{\sigma^{2}+\u_j^\top\bar{\S}_S\u_j}$\vspace{0.1cm}
            \STATE $\bar{\S}_{S \cup\{j_s\}} = \bar{\S}_S-\frac{\bar{\S}_{S}\u_{j}\u_{j}^\top\bar{\S}_{S}}{\sigma^2+\u_{j}^\top\bar{\S}_{S}\u_{j}}$\vspace{0.1cm}
            \STATE Set $S \leftarrow S\cup \{j_s\}$\vspace{0.1cm}
	\ENDWHILE
	\RETURN $S$.
\end{algorithmic}
\end{algorithm}
%===================================

\noindent{\textbf{Performance guarantees.}} Here we analyze performance of the randomized greedy algorithm. First, Theorem \oldref{thm:exp} below states that if $f(S)$ is characterized by a bounded maximum element-wise curvature, Algorithm 2 returns a sampling subset yielding an MSE that is on average within a multiplicative factor of the MSE associated with the optimal sampling set.
%
\begin{theorem}\label{thm:exp}
\textit{Let $\mathcal{C}_{max}$ be the maximum element-wise curvature of $f(S) = \mathrm{Tr}(\P-\bar{\S}_S)$, the objective function in problem \ref{eq:probsub}. Let $\alpha =(1-e^{-\frac{1}{c}}-\frac{\epsilon^\beta}{c})$, where $c=\max\{1,{\cal C}_{\max}\}$, $e^{-k}\leq\epsilon<1$, and $\beta = 1+\max\{0,\frac{s}{2N}-\frac{1}{2(N-s)}\}$. Let $S_{rg}$ be the sampling set returned by the randomized greedy algorithm and let $O$ denote the optimal solution of \ref{eq:probf}. Then,}
%
\begin{equation}\label{eq:expbound}
\E\left[\mathrm{Tr}(\bar{\S}_{S_{rg}})\right]\leq \alpha \mathrm{Tr}(\bar{\S}_{O}) + (1-\alpha) \mathrm{Tr}(\P).
\end{equation}
%
\end{theorem}
%
The proof of Theorem \oldref{thm:exp} relies on the argument that if $s = \frac{N}{k}\log\frac{1}{\epsilon}$, then with high probability the random subset $R$ in each iteration of Algorithm 2 contains at least one node from $O$.

Next, we study the performance of the randomized greedy algorithm using the tools of probably approximately correct (PAC) learning theory \cite{valiant1984theory,valiant2013probably}. The randomization of Algorithm 2 can be interpreted as approximating the marginal gains of the nodes selected by the greedy scheme proposed in \cite{chamon2017greedy}. More specifically, for the $i^{th}$ iteration it holds that $f_{j_{rg}}(S_{rg}) = \eta_i f_{j_{g}}(S_{g})$, where subscripts $rg$ and $g$ refer to the sampling sets and nodes selected by the randomized greedy (Algorithm 2) and greedy algorithm in \cite{chamon2017greedy}, respectively, and $0<\eta_i\leq 1$ for all $i \in [k]$ are random variables. 
In view of this argument and by employing the Bernstein inequality \cite{tropp2015introduction}, we obtain 
Theorem \oldref{thm:pac} which states that the randomized greedy algorithm selects a near-optimal sampling 
set with high probability.
%
\begin{theorem}\label{thm:pac}
\textit{Instate the notation and hypotheses of Theorem \oldref{thm:exp}. Assume $\{\eta_i\}_{i=1}^k$ are independent and let $C=0.088$. Then, with probability at least $1-e^{-Ck}$ it holds that}
%
\begin{equation}\label{eq:pacbound}
\mathrm{Tr}(\bar{\S}_{S_{rg}})\leq (1-e^{-\frac{1}{2c}}) \mathrm{Tr}(\bar{\S}_{O}) + e^{-\frac{1}{2c}} \mathrm{Tr}(\P).
\end{equation}
%
\end{theorem}
%
Indeed, in simulation studies (see Section \oldref{sec:sim}) we empirically verify the results of Theorems \oldref{thm:exp} and \oldref{thm:pac} and illustrate that Algorithm 2 performs favorably compared to the competing schemes both on average and for each individual sampling task. Before moving on to these numerical studies, 
in Theorem \oldref{thm:curv} we show that the maximum element-wise curvature of $f(S) = \mathrm{Tr}(\P-\bar{\S}_S)$ is bounded, even for non-stationary graph signals. 
%
\begin{theorem}\label{thm:curv}
\textit{Let $\mathcal{C}_{max}$ be the maximum element-wise curvature of $f(S) = \mathrm{Tr}(\P-\bar{\S}_S)$. Then, it holds that}
\begin{equation}\label{eq:curvbound}
\mathcal{C}_{\max} \leq \frac{\lambda_{\max}^2(\P)}{\lambda_{\min}^2(\P)}\left(1+\frac{\lambda_{\max}(\P)}{\sigma^2}\right)^3.
\end{equation}
\end{theorem}
An implication of Theorem \oldref{thm:curv} is a generalization of a result in \cite{chamon2017greedy} for stationary signals. There, it has been shown that if $\x$ is stationary and $\P = \sigma_\x^2\I_N$ for some $\sigma_\x^2>0$, then the curvature of the MSE objective is bounded. However, Theorem \oldref{thm:curv} holds even in the scenarios where the signal is non-stationary and $\P$ is non-diagonal.