%\documentclass[10pt,a4paper]{article}
\documentclass[11pt, authoryear]{elsarticle}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[left=22mm, marginpar=47mm]{geometry}
\usepackage[english]{babel}

%\usepackage[bibstyle=authoryear]{biblatex}
%===============================================================================
% Fonts -- if they are not available then simply comment the packages out.
%===============================================================================
\usepackage{microtype} % Improves typography
\usepackage{libertine}
\usepackage{libertinust1math}

%===============================================================================
% General packages for math, and misc ...
%===============================================================================
\usepackage{amsmath}   % Used by equations
\usepackage{amssymb}   % symbols
\usepackage{amsfonts}  % and some math fonts
\usepackage{commath}   % for differential operators and scalable paranthesis/brackets
\usepackage{csquotes}  % use with the command \enquote{}, e.g., for getting ``hello''
\usepackage[exponent-product=\times, retain-unity-mantissa=false]{siunitx}  % units and numbers
\usepackage{textgreek} % Greek letters, e.g., also in bibliography

\usepackage{comment}
%\usepackage[numbers, sort, comma, square]{natbib}
\usepackage{natbib}


\usepackage[normalem]{ulem}

%===============================================================================
% Referencing: use \cref{my_label} for, figures, equations, ...
%===============================================================================
\usepackage[noabbrev]{cleveref}
\crefname{figure}{Fig.}{Figs.}
\crefname{equation}{eq.}{eqs.}
\crefname{table}{Tab.}{Tabs.}
%\usepackage{tikzducks}

%===============================================================================
% Acronyms and abbreviations (not too many, though...)
%===============================================================================
\usepackage[shortcuts=abbreviations]{glossaries-extra}
\setabbreviationstyle{long-short-sc}
\newabbreviation{ML}{ML}{machine learning}
\newabbreviation{DL}{DL}{deep learning}
\newabbreviation{MPR}{MPR}{microstructure-property relation}
\newabbreviation{CNN}{CNN}{Convolutional Neural Networks}
\newabbreviation{PSD}{PSD}{Power Spectral Density}
\newabbreviation{RMSE}{RMSE}{root mean square error}
\newabbreviation{SVR}{SVR}{support vector regression}


\newcommand \improve[1]{\bgroup\noindent[\textcolor{blue}{\textbf{improve}: #1}]\egroup\ignorespacesafterend} % need better way to explain
%===============================================================================
% Bibliography: we use \citet{} and \citep{} -- never \cite{}
%===============================================================================
%\usepackage{subfig}
\usepackage{subcaption}
%===============================================================================
% Graphics Plotting, Figures
%===============================================================================
\usepackage{graphicx} % Insert figures
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}

% no need to have "figures/" in the includegraphics command -- this helps to
% have a directory with draft figures etc
\graphicspath{{figures/}}

\journal{Machine Learning with Applications}
%===============================================================================
% todo notes
%===============================================================================
\usepackage{xargs}       % Use more than one optional parameter in a new commands
\usepackage{xcolor}
\usepackage[colorinlistoftodos,prependcaption,textsize=footnotesize, textwidth=2.7cm]{todonotes}
\let\xtodo\todo
\renewcommand{\todo}[1]{%
	\xtodo[linecolor=red!80!black, backgroundcolor=red!60!black!5,
	bordercolor=red!80!black, tickmarkheight=1.2mm]{\sffamily #1}}
\newcommand{\inlinetodo}[1]{\xtodo[color=red!60!black!5, inline]{#1}}
\newcommand{\redtext}[1]{{\color{red}#1}}
\usepackage{array}\newcolumntype{c}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{url}

% focused on research related to machine learning.
% results with a focus on value and effectiveness


\renewcommand\textcolor[2]{#2}

\begin{document}
	\begin{frontmatter}
		\title{%
			Efficient Surrogate Models for Materials Science Simulations: 
			Machine Learning-based Prediction of Microstructure Properties
		}
		
		\author[fzj,one]{Binh Duong Nguyen}
		\author[fzj,two]{Pavlo Potapenko}
		\author[fzj,three]{Aytekin Demirci}
		\author[fzj,fourth]{Kishan Govind}
		\author[fzj,fifth]{S\'ebastien Bompas}
		\author[fzj,rwth,corr]{Stefan Sandfeld}				
		
		\address[fzj]{%
			Institute for Advanced Simulations -- Materials Data Science and Informatics 
			(IAS-9), Forschungszentrum J\"ulich GmbH, 52425 J\"ulich, Germany
		}
		\address[rwth]{%
			Chair of Materials Data Science and Materials Informatics, 
			Faculty 5 -- Georesources and Materials Engineering, 
			RWTH Aachen University, 52056 Aachen, Germany
		}
		\address[one]{%
			first author's email address: bi.nguyen@fz-juelich.de
		}
		\address[two]{%
			second author's email address: p.potapenko@fz-juelich.de
		}
		\address[three]{%
			third author's email address: a.demirci@fz-juelich.de
		}
		\address[fourth]{%
			fourth author's email address: k.govind@fz-juelich.de
		}
		\address[fifth]{%
			fifth author's email address: s.bompas@fz-juelich.de
		}
		\address[corr]{%
			corresponding author's email address: s.sandfeld@fz-juelich.de
		}
		
		%========================================================================================
		\begin{abstract}
			Determining, understanding, and predicting the so-called structure-property relation 
			is an important task in many scientific disciplines, such  as chemistry, biology, 
			meteorology, physics, engineering, and materials science. \emph{Structure} refers to 
			the spatial distribution of, e.g., substances, material, or matter in general, while
			\emph{property} is a resulting characteristic that usually depends in a non-trivial 
			way on spatial details of the structure. Traditionally, forward simulations
			models have been used for such tasks.
			%
			Recently, several machine learning algorithms have been applied in these scientific 
			fields to enhance and accelerate simulation models or as surrogate models. 
			%
			In this work, we develop and investigate the applications of six machine learning 
			techniques based on two different datasets from the domain of materials science: 
			data from a two-dimensional Ising model for predicting
			the formation of magnetic domains and data representing the evolution of dual-phase
			microstructures from the Cahn-Hilliard model.
			%
			We analyze the accuracy and robustness of all models and elucidate the reasons for
			the differences in their performances. The impact of including domain knowledge through 
			tailored features is studied, and general recommendations based on the availability and 
			quality of training data are derived from this. 
		\end{abstract}
		
		\begin{keyword}
			structure-properties relation \sep forward model \sep feature engineering 
			\sep power spectrum density \sep convolutional neural network \sep support vector regression \sep Ising model \sep Cahn-Hilliard model
		\end{keyword}
	\end{frontmatter}
	%==============================================================================
	
	\clearpage
	%==============================================================================
	\section{Introduction}                                        \label{sec:intro}
	%==============================================================================
	Studying the (micro)structure-properties relation is an important task for
	many different scientific fields and on many different length scales, e.g., 
	for meteorology with up to kilometer-sized features, for materials science
	on the nanometer scale or for biological or chemical systems on various 
	length scales~\citep{Kohn2018}. Mathematically, the task is to find the 
	map  from a (one-, two-, or three-dimensional) spatial distribution of values 
	to a single (scalar, vectorial, or tensorial) value. 
	%The reason is, that very often 
	%properties of phenomena on a smaller length or time-scale determine the behavior 
	%of a system on a significant larger length or time-scale, sometimes with 
	%differences between the scale in the range of several orders of 
	%magnitude~\citep{Kohn2018} .
	%
	% 	For example, the magnetic properties of Earth-directed coronal 
	% 	mass ejections from the sun results in the ejection of large-scale magnetic clouds in interplanetary space. Knowledge of the spatial structure is crucial for a 
	% 	reliable prediction of the ``geoeffectiveness'' \citep{pal2022magnetic}.
	%
	For example, geological measurements of the three-dimensional structural 
	details of the earth's crust are accompanied by displacement measurements 
	which represents an average, i.e., an effective property, and can help to understand the general mechanism for shallow earthquakes
	\citep{tarasov2019dramatic}.
	%
	In the field of weather forecasting, spatial details such as the 
	structure of clouds or streamlines of the airflow determine properties 
	such as cloud top temperature and particle effective radius \citep{rosenfeld2008satellite}.
	%
	Biological structures consist of molecules and cells, that are 
	permanently evolving. Their subcellular interactions give rise to complex properties such as transport properties or how cells age~\citep{Li2021}.
	%
	In the domain of  material science and in particular, with regards to metallic 
	materials, the notion of microstructure refers to any phenomena that ``lives'' 
	on a small scale (i.e., small relative to the specimen size) and that disturbs 
	the otherwise perfect crystal lattice~\citep{callister2007materials}. 
	The property is typically the result of the interplay of many different
	physical or chemical mechanisms; a property is an averaged quantity where
	the details of the underlying microstructural length scale usually are no longer
	directly observable. Two typical examples are: (i) interstitial point defects on 
	the atomic scale that gives rise to hardening behavior in alloys observed 
	during mechanical testing of centimeter-sized samples 
	\citep{baker2022interstitial}, or (ii) the property of strength, which 
	increases considerably with a decrease in grain size \citep{opiela2020effect}.	
	
	To predict such properties based on microstructures of different length
	scales, dedicated simulation models have been developed  \citep{nguyen2023challenges,seif2023application,sharma2023multiphysics}. 
	On the (sub)nanometer scale, one of the commonly utilized methods is density 
	functional theory calculations, which investigate the electronic structure of 
	many-body systems to acquire the properties of the electron system based on 
	the fundamental laws of quantum mechanics \citep{dreizler2012density}. 
	Another popular method, molecular dynamics simulation, predicts the trajectory 
	of each atom based on Newton's laws \citep{hospital2015molecular}. These two 
	methods can calculate very accurately the structures and properties of a material
	on a microscopic scale, but the computational cost for large-scale problems is
	prohibitive. If the problem can be written in terms of continuous field
	equations governed by partial differential equations, numerical methods 
	such as the finite element method \citep{huebner2001finite} are often used. 
	% 	and spectral methods \citep{chen1998applications,shen2011spectral} 
	However, the computational cost can still be high, with single simulations
	taking up to several days. Additionally, the numerical solution sometimes 
	suffers from numerical instabilities, which makes performing simulations still 
	a challenging task.
	
	%==============================================================================
	%	
	%	- \citet{nakka2023generalised}:  a simple method is developed for encoding material properties into the microstructure image so that the model learns material information in addition to the structure-property relationship. These ideas are demonstrated by developing a CNN model that can be used for fiber-reinforced composite materials with a ratio of elastic moduli of the fiber to the matrix between 5 and 250 and fiber volume fractions between 25 and 75 \%, which span end-to-end practical range.
	%	
	%	- \citet{gupta2023data}: introduce the concept of decision-focused surrogate modeling for solving computationally challenging nonlinear optimization problems in real-time settings.
	%	
	%	- \citet{khorrami2023artificial}: the development of a trained artificial neural network for surrogate modeling of the mechanical response of elasto-viscoplastic grain microstructures.
	%	
	%	- \citet{montes2021accelerating}: integrate a statistically representative, low-dimensional description of the microstructure, obtained directly from phase-field simulations, with either a time-series multivariate adaptive regression splines autoregressive algorithm or a long short-term memory neural network. The neural-network-trained surrogate model shows the best performance and accurately predicts the nonlinear microstructure evolution of a two-phase mixture during spinodal decomposition in seconds, without the need for “on-the-fly” solutions of the phase-field equations of motion.
	%	
	%	- \citet{qin2022deep}: a deep-learning-based surrogate model is developed and applied for predicting dynamic temperature, pressure, gas rate, oil rate, and water rate with different boundary conditions in pipeline flow.
	%	
	%	- \citet{cai2021surrogate}: A long-standing problem at the frontier of biomechanical studies is to develop fast methods capable of estimating material properties from clinical data.
	%	
	
	Recent development of statistical \gls{ML} and \gls{DL} algorithms have the 
	potential to act as surrogate models and/or provide alternatives for predicting 
	the structure-property relation in science and engineering.
	%
	%
	This helps to 
	overcome the limitations of classical methods in terms of computational cost 
	and robustness \citep{jung2019efficient,wei2019machine, gupta2023data}. 
	%
	For example, \gls{DL} approaches have been applied to weather forecasting to predict 
	the likelihood of weather conditions at a given time and location based on 
	numerous atmospheric and oceanic properties such as pressure, humidity, wind 
	velocity and temperature from radar or weather satellite images 
	\citep{espeholt2022deep}. ML-based short-term forecasting of earthquakes using 
	remote-sensing (image) data was demonstrated to outperform conventional approaches 
	\citep{xiong2021towards}.
	%
	\textcolor{red}{The data augmentation process with discrete waveform transforms (DWT) and singular value decomposition (SVD) helps to increase the variety of earthquake data for 
		training \gls{ML} models. These models are then used to predict the response of nonlinear systems for unseen earthquakes or to replicate non-linear FE model prediction \citep{parida2023earthquake, parida2023svd}.
	}
	%
	% or to forecast ground-motion intensity of earthquakes based on quantities 
	% moment magnitude, hypocentral distance, and averaged shear wave velocity 
	% extracted from earthquakes records \citep{khosravikia2021machine}.
	%, or \citep{li2020forest} use ML/DL methods to estimate forest aboveground 
	% biomass from Landsat 8 and Sentinel-1A images for studying the climate change 
	% in the ecosystem.
	%	
	Additionally, such methods have been applied to computational biology 
	\citep{sapoval2022current} for protein structure prediction from its amino acid 
	sequences \citep{jumper2021highly, tunyasuvunakool2021highly}, for predicting
	the melting temperature of proteins based on their amino acid sequence 
	\citep{gorania2010predicting}, or for predicting the ligand binding sites in 
	the protein structures \citep{kandel2021puresnet}. 
	%
	The field of materials science also benefits from \gls{ML} methods. 
	In the context of surrogate models, \citet{nakka2023generalised} created a 
	\gls{DL}-based model for encoding material properties into the microstructure 
	image so that the model learns material information. 
	%
	\textcolor{red}{
		%		\citet{nyshadham2019machine} demonstrate the feasibility of machine-learned surrogate models for predicting enthalpies of formation of materials across composition, lattice types, and atomic configurations.
		%		A surrogate machine-learning model replaces ab initio simulations by mapping a crystal structure to properties such as formation enthalpy, elastic constants, or band gaps.
		%		\citet{maurizi2022predicting} predict deformed shapes, stress, and strain fields in material and structural systems. The input is meshed material or structural systems.	
		%		\citet{parida2023earthquake}using discrete waveform transform (DWT) for both earthquake data augmentation as well as deep feature extraction for training ML models to predict the structural response of 3DOF nonlinear spring–mass–damper system and non-linear three-story steel moment-resisting frame for unseen earthquakes.
		%		\citet{parida2023svd} SVD enabled data augmentation,  replicate non-linear FE model prediction of one-story and three-story buildings subject to earthquake excitation	
		\citet{messner2020convolutional} use \gls{CNN} as ``sufficiently accurate surrogate models'' for solving the inverse design problem that produces optimal structures with required mechanical properties. 
	}
	%	
	Further example 
	applications in the context of structure-property relations are the ML-based 
	prediction of 
	%
	the hardness and relative mass density of nanocomposites based on microstructural 
	texture variance produced by different laser parameters \citep{yu2021machine},  
	%
	the diffusivity and permeability based on the geometry of the pore space 
	utilizing artificial neural networks \citep{prifling2021large}, 
	%
	the effective heat conductivity for highly heterogeneous microstructured 
	materials \citep{lissner2019data}, 
	%
	the surrogate modeling of the mechanical response of elasto-viscoplastic grain 
	microstructures \citep{khorrami2023artificial}
	%
	or
	% the chemistry and processing history of a micrograph of these permanent magnets 
	% alloy \citep{farizhandi2022deep},
	% the hole expansion capacity of high-strength steels based on the composition 
	% of phases and chemical content \citep{li2021microstructure}, the porosity of 
	% porous media from 2D images of sandstone samples\citep{alqahtani2018deep}, 
	the prediction of mechanical properties of two-phase microstructures of 
	epoxy-carbon fiber aerospace composite \citep{ford2021machine}.
	%
	\textcolor{red}{
		Most of these approaches are concerned with mapping an input to an output.
		A variety of approaches have incorporated more general physics knowledge into 
		the model, mostly into the loss function, such as \citet{zhang2020PhyCNN} and 
		\citet{zhang2020physics} who used a physics-guided convolutional neural network and 
		physics-informed multi-LSTM networks as surrogate models for structural response 
		prediction.  \citet{raissi2018deep} introduced physics-informed neural networks for solving nonlinear partial differential equations, and \citet{eghbalian2023physics} develops an Elasto-Plastic Neural Network for replacing the conventional yield function, plastic potential, and the plastic flow rule.
		%	\citet{zhang2020PhyCNN} train a physics-guided convolutional neural network (PhyCNN) model based on limited seismic input-output datasets (e.g., from simulation or sensing) and physics constraints, and thus establish a surrogate model for structural response prediction.
		%	\citet{zhang2020physics} develop physics-informed multi-LSTM networks for metamodeling of nonlinear structures and show applications to buildings under earthquake excitation.
		%	\citet{raissi2018deep} introduce physics-informed neural networks – neural networks that are
		%	trained to solve supervised learning tasks while respecting any given law of
		%	physics described by general nonlinear partial differential equations.
		%	\citet{eghbalian2023physics} develop an Elasto-Plastic Neural Network (EPNN), with simple architecture, that incorporates some of the crucial physics related to this class of plastic	materials and is tailored for use with experimental data. The proposed EPNN replaces the conventional yield function, plastic potential, and plastic flow rule with more flexible algorithms.
	}
	%
	% 	Ultimately, \gls{ML}-based methods	are even able to support theory 
	% development and are thereby directly helping	to get new materials scientific 
	% insights and knowledge. They can help to find alloys with high hardness in 
	% the AL-Co-Cr-Cu-Fe-Ni HEA system \citep{wen2019machine}, or can accelerate 
	% the design process based on the microstructure \citep{pei2021machine}.
	%
	% 	Nonetheless, the field of data mining and \gls{ML}-based analysis is still 
	% 	in an early stage where a number of groups are (i) trying to explore the 
	% 	domain of explainable \gls{ML} to understand the decisions made by \gls{ML}
	% 	\cite{kailkhura2019reliable,oviedo2022interpretable}; and (ii) trying to 
	% 	incorporate \enquote{domain knowledge},	i.e. the knowledge about the 
	% 	underlying physical domain in the form of known equations or other types 
	% 	of, e.g.,  experience or data sets 
	% 	\cite{childs2019embedding,murdock2020domain,li2022towards}.
	
	Many of these examples, however, consider highly specialized scientific 
	situations where the focus is on solving a particular domain-scientific problem
	with an as high as possible accuracy. Systematic studies with an emphasis on
	aspects of the training behavior, the ability to generalize, or the performance
	w.r.t. to the amount of data are rare.
	This makes comparison between the work of different groups difficult and
	it is far from being trivial to estimate if a model could be reused
	for a different problem class as well.
	
	
	% 	developed techniques are, so far, applied and tailored to 
	% 	solve a specific scientific problem. The parameters or architectures of these 
	% 	models are modified such that they can get a high accuracy or good predicting 
	% 	scores for the chosen data but it is not clear if and how they perform on  
	% 	datasets from other problem types.
	% 	\citet{bressem2020comparing} found that even though a much better model (DenseNet121) which is a much deeper model compared to AlexNet and found to give much better results on the ImageNet dataset, did not perform better. They were able to obtain comparable results using this model showing that a complex and deeper models with a large number of parameters might not be always the best choice. Therefore, it is necessary to generalize 
	% 	the application of the \gls{ML} or \gls{DL} algorithms. 
	% 	%
	% 	%Additionally, sampling strategies are very important when handling and analysing data, specially with strongly imbalance datasets, however,  to the best of our knowledge the influence on the \gls{ML} performance  is rarely acknowledged or studied in detail in the domain of materials science and engineering. 
	% 	Generally, systematic and broader studies are rare in this 
	% 	domain, which makes comparison between different works and groups difficult.
	
	In this work, we investigate the benefits and drawbacks of a range of 
	machine learning approaches for predicting properties from structures of two 
	fundamentally different, materials
	science datasets. Besides different \gls{ML} model complexities, we also investigate
	the importance of incorporating domain knowledge through feature engineering. 
	The datasets are obtained from materials scientific simulations and cover 
	two extremes: The first governs the self-organized magnetization of a domain 
	and relates the spatial magnetization structure to the temperature. It is based 
	on randomness and stochastic processes that are simulated using a Monte Carlo method,
	resulting in structures with very sharp interfaces and discrete changes in time. 
	The other dataset is obtained from a simulation of the evolution of two 
	different phases, which is a smooth and continuous process given by a set of 
	coupled partial differential equations. 
	In both cases, the structure can be represented as an image and the property is a
	scalar number. These two models are representative 
	for many problems encountered in physics and engineering.
	
	The following machine learning approaches are investigated: 
	(i) a piecewise-constant regression model together with 
	simple features, used as a baseline model; (ii) a support vector 
	regression model with physics-based features; (iii) a non-standard setup 
	of a convolutional neural network approach with three input channels where the 
	original data is accompanied by Fourier and wavelet transformations as 
	additional features; (iv) a combination of a pretrained ResNet and a principal 
	component analysis used as input features for a support vector regression 
	model; (v) several "off-the-shelf" CNNs with different types of pre-training. 
	
	
	
	%==============================================================================
	\section{Data Generation: The Materials Scientific Problems and the Simulation Methods} 
	\label{sec:methods}
	%==============================================================================
	For all investigations, two datasets will be used that represent the evolution 
	of two different types of microstructures. The datasets are obtained from 
	materials scientific simulations and cover two extremes: while the first is 
	based on stochastic processes where randomness and self-organization are 
	important, the second dataset is obtained from the solution of coupled partial 
	differential equations and describes flow-like smooth and continuous processes. 
	%
	\begin{figure}
		\centering
		\includegraphics[width=0.9\textwidth]{Ising_CH_evolution.jpg}
		\caption[]{%
			a) Visualization of the final state of magnetization from each given 
			temperature. The vertical blue arrow shows an example simulation 
			trajectory of a specific temperature $T=100$. b) The evolution of 
			energy and the change of microstructure over time on various simulation 
			run in Cahn-Hilliard model. The blue arrows illustrate the simulation 
			trajectory.
		}	
		\label{fig:Ising_CH_evolution}
	\end{figure}
	%
	\Cref{fig:Ising_CH_evolution} shows examples of microstructures (the insets 
	A-F) together with the respective property (shown on the vertical axes of the 
	two curves). The underlying theory and the implementation of the 
	simulation models are summarized in \ref{appendix:simulations}. In what follows 
	we only describe the datasets.
	
	
	%------------------------------------------------------------------------------
	\subsection{The Ising Dataset}
	%------------------------------------------------------------------------------
	The two-dimensional microstructure represents the magnetization of a domain 
	with two different magnetic spin directions, indicated by the 
	black and white color in \cref{fig:Ising_CH_evolution}a. 
	The microstructure depends on the chosen temperature, which is considered as 
	the property. The evolution of the system is determined by a Metropolis Monte 
	Carlo algorithm. For a given temperature, the simulation starts with a random 
	distribution of spin values and evolves for a number of steps. The final 
	microstructure is saved as a black and white image, where black (0) represents 
	a negative spin and white (255) represents a positive spin. A single simulation 
	requires a few seconds of computational time and results in a single image.
	%
	At increasing temperature $T$ above a critical temperature $T_c\approx 2.269$ 
	(in non-dimensional units) the resulting structures become increasingly random.
	They start the transition towards an ordered state when the temperature is 
	decreasing below the critical temperature $T_c$;  where larger features
	become visible and the randomness vanishes. 
	%
	\Cref{fig:Ising_CH_evolution}a shows example images at three different stages,
	labeled as A, B, and C together with  the (dimensionless) temperature values of 
	\SI{11}{}, \SI{102}{} and \SI{222}{}, respectively. The temperature values are 
	converted from $0..2T_c$ to $0..255$ for the dataset. The whole dataset 
	consists of $50,000$ images. The distribution of the corresponding 
	temperature values can be seen in \cref{fig:train_data_hist}a -- 
	a mainly uniform distribution which results in a balanced training 
	dataset.
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.8\textwidth]{train_data_hist.jpg}
		\caption[]{%
			Histograms of training data distribution for the Ising datasets (left 
			panel) and the Cahn-Hilliard dataset (right panel).
		}	
		\label{fig:train_data_hist}
	\end{figure}
	
	
	%------------------------------------------------------------------------------
	\subsection{The Cahn-Hilliard Dataset}
	%------------------------------------------------------------------------------
	The microstructure of the Cahn-Hilliard model represents two phases, e.g., 
	the two different chemical elements of an alloy. The evolution is governed
	by a set of coupled partial differential equations. This system is more complex 
	than the Ising model because the number of different physical phenomena 
	considered is significantly larger (see \ref{appendix:simulations} for further 
	details). This also results in a high computational cost of several hours for 
	a single simulation which might make machine learning-based approaches good	candidates as replacements.
	%
	Three microstructure snapshots are shown in \figref{fig:Ising_CH_evolution}b. 
	Each image represents the microstructure corresponding to the system's energy
	with values of \SI{7054}{\J\mu \m^{-3}}, \SI{1348}{\J\mu \m^{-3}}, and 
	\SI{589}{\J\mu \m^{-3}}. 20 simulations are performed with random initial 
	microstructure and values in between $0$ and $1$. In order to reduce the 
	computing time, we use a non-constant time stepping with exponentially 
	increasing time steps. 
	%An explanation of the kink in \cref{fig:Ising_CH_evolution} shortly after 
	%a time of \SI{1e-2}{\second} can be found in the appendix.
	%
	The simulation exhibits two parts: a first part where the energy decays 
	rapidly and a second part, where the energy decreases only slowly. 
	$2,000$ steps are taken in the first part where the time step increases; 
	the microstructure data is stored at every step.
	The second part consists of $10,000$ steps with a constant step size. 
	The image data are exported at every $10$th step of this part of the simulation. 
	Altogether, the dataset contains $\approx 60,000$ images.
	The distribution of the energy values of the dataset can be seen in 
	\cref{fig:train_data_hist}b which shows a strong imbalance with significantly 
	more data for the low energy regime. We have chosen to use this kind of 
	sampling because it is close to a ``real world'' dataset. Creating 
	a uniformly distributed dataset requires significantly more simulation time
	and is, in most situations, not feasible.
	
	
	%==============================================================================
	\section{Methods} \label{sec:solutions}
	%==============================================================================
	Learning to predict properties from (micro)structures is a regression-type of 
	a problem. For such problems, a large range of different statistical and deep
	learning methods exist. Our selection of  investigated methods is guided by
	the following considerations: (i) A simple statistical learning method with 
	as simple as possible features should be used as a baseline method; (ii)
	statistical learning methods can perform very well together with appropriate
	features; (iii) deep learning approaches typically do not require sophisticated
	features but sometimes requires larger training datasets.
	%
	% 	It is not clear which the most suitable \gls{ML} approach is towards
	%    learning
	% 	what the properties corresponding to a given microstructure. Additionally,
	%    our
	% 	goal is to avoid any feature engineering that requires a high degree of 
	% 	specialization to a specific type of dataset or physical phenomenon. On the 
	% 	other hand, feature engineering can be a useful way of introducing domain 
	% 	knowledge into a \gls{ML} problem. Therefore, we decided to investigate a 
	% 	number of different \gls{ML} approaches. 
	%
	In the following, we start by introducing and deriving the used features.
	Subsequently, the \gls{ML} models used in this study are selected.
	%, some of which occur in complex ``data analysis pipelines'' consisting of 
	% more than one method. 
	
	%------------------------------------------------------------------------------
	\subsection{Feature Engineering}
	%------------------------------------------------------------------------------
	Three different types of features are used throughout this work: a very 
	simplistic feature that does not require any domain knowledge and two
	physics-based features of different complexity.
	
	%------------------------------------------------------------------------------
	\subsubsection{A Minimalistic, Domain-agnostic Feature (``grad'')}
	\label{sec:gradfeature}
	%------------------------------------------------------------------------------
	For the statistical learning methods, we start by creating a set of generic 
	features that do not require knowledge of the scientific details behind the 
	data generation. By taking a look at the six microstructure images A-F in 
	\cref{fig:Ising_CH_evolution} we observe that differences could be related to 
	the total length of the boundary or interfaces between black and white regions. 
	In image analysis, a gradient filter would be the most simple way of extracting 
	such information. Therefore, as a simple feature $X$, we use the following 
	function
	%
	\begin{align}
		X &= \frac{1}{n\cdot m}\sum\limits_{i,j} \|\nabla u(i, j)\|
		= \frac{1}{n\cdot m}\sum\limits_{i,j} 
		\sqrt{\left(\frac{\partial u[i,j]}{\partial x} \right)^2 + 
			\left(\frac{\partial u[i, j]}{\partial y} \right)^2}
	\end{align}
	%
	where $u[i, j]$ is the value of the pixel in row $i$ and column $j$ of the 
	image array $u$, and $m$ and $n$ are the number of rows and columns, 
	respectively. The gradient of $u$ is approximated by a central finite 
	difference scheme. This feature can be easily used together with a broad range 
	of other datasets as well and does not require further domain-specific 
	knowledge. In the remainder of this work, we abbreviate this feature as 
	``grad''.
	
	
	
	%------------------------------------------------------------------------------
	\subsubsection{Feature Engineering: The Power Spectral Density (``physFeat1'')}
	\label{sec:PSDfeat}
	%------------------------------------------------------------------------------
	Due to the aspects of the underlying physics problem, the microstructure images often 
	exhibit periodicity, symmetry, or rotational invariance. For CNNs, this can be
	considered through hard or soft constraints. Hard constraints are induced by 
	architecture modifications, such as using group equivariant convolution 
	instead of regular convolutions \citep{cohen2016group}. This strategy cannot
	be transferred to statistical learning methods. 	
	Soft constraints, on the other hand, are imposed through training with specific
	augmentation techniques, e.g., applying specific translations to images to mimic 
	periodicity, random flipping, or rotation of images. However, this does not 
	guarantee that the constraints are exactly enforced, and the physics problem
	may be violated. 
	%
	% @Pavlo: this is a good piece of text, but too detailed for this paper
	%	Application of soft or hard constraints for microstructure properties poses 
	%	certain challenges. On the one hand, training models with soft constraints does 
	%	not guarantee their performance invariance to random manipulations. The 
	%	fluctuations of values predictions that occur due to these random manipulations 
	%	indicate that the physical properties of the problem were not captured. On the 
	%	other hand, hard constraints can require an increased number of computations 
	%	compared to standard models. For instance, the group equivariant convolution 
	%	neural network described in \cite{cohen2016group} requires 8 times more filters 
	%	per layer to check for rotational (turning filter by 90 degrees) and symmetry 
	%	invariances.
	%
	As an alternative to applying soft or hard constraints, we will incorporate
	microstructural constraints using physics-inspired feature engineering. 
	
	Which physics-based features are suitable to relate the microstructures to 
	their properties? Both datasets were obtained by enforcing periodic boundary
	conditions. The resulting properties are both invariant under
	mirroring and rotation by 90 degrees.
	Furthermore, the Ising model represents a situation in which a 
	microstructure can undergo a phase transition, i.e., a small change in the 
	temperature results in a significant and qualitative change of the structure. 
	Such a phase transition manifests itself in long-range, collective behavior 
	which is caused by short-range interaction.
	% 	\footnote{%
		% 	    Such ``emergent behavior'' is ubiquitous in nature and can be often found 
		% 	    in physics, chemistry, or molecular biology.%
		% 	}. 
	Roughly speaking, this is related to the fact that at the critical point the 
	system transitions from small fluctuations (the size of the black and white 
	patterns) to large fluctuations, cf. \cref{fig:Ising_CH_evolution}, and there 
	are fluctuations of all wavelengths directly at the critical temperature. As 
	a consequence, physics-based feature engineering should result in descriptors 
	that are able to capture characteristics of the distribution of various 
	wavelengths. 
	
	A suitable measure is the \gls{PSD}, a \enquote{multi-scale measure} used 
	in signal processing to describe the distribution of wavelengths. The 
	\gls{PSD} is obtained via Fourier transformations of an image from which the 
	Fourier amplitudes can be extracted; they are by definition translation 
	invariant. This is followed by radial averaging, making the resultant features 
	invariant to 90-degree rotation and flipping. As a result, two equivalent 
	images from a physics perspective also result in identical \glspl{PSD}
	(see, e.g., the applications in the context of quality assessment of 
	fingerprints \citep{Shen2022} or for statistical analysis of shear bands in
	\citep{Sandfeld_2014}). 
	
	% Using \enquote{p} enforces that both are on a page with only floats 
	% --> effectively, both figures will be on the same page
	\begin{figure}[htb]
		\includegraphics[width=0.49\textwidth]{Ising_PSD_horiz}
		\hfill
		\includegraphics[width=0.49\textwidth]{CH_PSD_horiz}
		\caption{%
			Microstructure and corresponding \glspl{PSD} for the
			Ising dataset (left) and the Cahn-Hilliard dataset (right). 
			The \gls{PSD} is shown on a double logarithmic scale, 
			where $k$ is the wavelength and $P(k)$ is the normalized 
			power. The thin red line is a linear fit on the double
			logarithmic scale while the values of 
			intercept and slope of the fitted line are given after
			backtransformation to the linear scale. 
		}
		\label{fig:ising:psd}
	\end{figure}
	%
	\begin{figure}[htb]
		\centering
		\includegraphics[width=0.9\textwidth]{ising_feature_space}\\
		\includegraphics[width=0.9\textwidth]{CH_feature_space}\\
		\caption{%
			Visualization of the two scalar features as a function of the
			investigated property. The top row shows the Ising dataset, the
			bottom row shows the Cahn-Hilliard dataset.
			%
			For getting a better idea of the data distributions the blue data points 
			are plotted slightly translucent.
			The middle and right columns show projections of the feature space and 
			fitted curves, representing the model response.}
		\label{fig:ising:featurespace}
	\end{figure}
	
	\Cref{fig:ising:psd} shows the \gls{PSD} for three different microstructures 
	obtained from the Ising and the Cahn-Hilliard dataset. 
	E.g., in the left column, we observe that a significant portion of the
	power is located in features with wavenumbers $k\leq 3$, i.e., patterns
	that are $\geq 1/3$ of the image size. Furthermore, the \glspl{PSD} 
	roughly exhibits a linear behavior in the double 
	logarithmic plots (with the exception of the second Cahn-Hilliard example). 
	Even though it is obvious that a linear fit is only a rough approximation, 
	it is well able to differentiate between microstructures at different 
	temperatures and energy values.	The slope and intercept of the fitted 
	lines after back-transformation from the double logarithmic scale will 
	serve as the features that are used to characterize each microstructure 
	image. 	
	%
	The two left scatter plots in \cref{fig:ising:featurespace} show 
	the temperature and energy, respectively, as a function of these two 
	features for each training dataset. 
	Visual inspection shows that even though the points are clearly localized, 
	the features of the dataset also contain significant scatter. Additionally, 
	the Cahn-Hilliard dataset exhibits
	an unexpected drop at an energy value of \SI{\approx 1200}{\joule\per\micro\meter^3}.
	This is a side effect of the above line fit which indicates, in this region, 
	a bad representation of the data (cf. the second row of \cref{fig:ising:psd}b).
	In the remainder of this work, we abbreviate these two features describing
	the microstructure through the approximated \gls{PSD} as ``physFeat1''.
	
	
	
	%------------------------------------------------------------------------------
	\subsubsection{Feature Engineering: Extended Physics-based Features (``physFeat2'')}
	\label{sec:physFeat2}
	%------------------------------------------------------------------------------
	The second physics-based set of features is also based on the
	\gls{PSD} but is further fine-tuned for accuracy. 
	\Cref{fig:PSD_Preprocessing_small} schematically shows the feature engineering
	pipeline that is now explained.
	%
	\begin{figure}[!htb]
		\centering
		%\includegraphics[width=0.75\textwidth]{pavlo/PSDPreprocessing.png}
		\includegraphics[width=0.71\textwidth]{feature-diagram_small}
		\caption[]{
			PSD-based feature engineering pipeline, specifically tailored
			to the considered type of data.
		}
		\label{fig:PSD_Preprocessing_small}
	\end{figure}
	%
	A side effect of the strong dimensionality reduction of the previous two
	features is loss of information. We attempt to make up for this by 
	considering the whole \gls{PSD} curve and not only a fit of a 
	straight line. Furthermore, additional features are introduced that capture new 
	aspects: instead of working with the \gls{PSD} of only the original image, we 
	create an additional image array, which includes information about the 
	interfaces, similar to the ``grad'' feature:
	%
	\begin{equation}
		\centering\label{eq:PSD-norm_grad}
		\|\nabla u[i, j]\| = \sqrt{\left(\frac{\partial u[i, j]}{\partial x}\right)^2 + 
			\left(\frac{\partial u[i, j]}{\partial y}\right)^2},
	\end{equation}
	%
	where $u[i,j]$ is, as before, the value of the pixel in row $i$ and column $j$.  
	For each image, the \gls{PSD} is obtained, which consists of a one-dimensional
	array of 32 values. The two arrays are stacked, resulting in a $2\times 32$ array.
	An issue that can arise during computing the \gls{PSD} are numerical errors
	due to high PSD values, causing extreme value ranges. As a remedy, feature 
	normalization by logarithmic scaling is performed:
	%
	\begin{equation}
		\centering\label{eq:PSD-log_norm}
		\gls{PSD}_{\rm scaled}(x) = \log_{10}\left( \gls{PSD} + 1\right) ,
	\end{equation}
	where $1$ is added to avoid computational problems if the \gls{PSD} has
	a value of $0$. 
	
	
	
	%------------------------------------------------------------------------------
	\subsubsection{Combined Image Embedding and Dimensionality Reduction (``CNN-PCA'')}
	\label{sec:CNN-PCA-SVR}
	%------------------------------------------------------------------------------
	The aim of this last set of features is twofold: (i) to achieve high accuracy 
	without having to rely on domain knowledge, (ii) to allow for a regression 
	model that is computationally cheap and fast to train. The CNN-PCA features
	are obtained by first performing image embedding, followed by a dimensionality
	reduction, as summarized in \figref{fig:model_PCA_ising_ch}a.
	%
	\begin{figure}
		\centering
		\includegraphics[width=0.7\textwidth]{model_PCA_ising_ch.jpg}
		\caption[]{%
			Summary of the CNN-PCA-SVR model in a) and visualization of the 
			first two principal components of the ResNet18 weights for the 
			Ising dataset (b) and the Cahn-Hilliard dataset (c).
		}
		\label{fig:model_PCA_ising_ch}
	\end{figure}
	%
	For the image embedding a ResNet18 with pretrained ImageNet weights 
	\cite{deng2009imagenet}) is used. The CNN is then trained with the 
	images, but for the sake of achieving a low training time, we did not do 
	any fine-tuning of the network. The temporary features are the weights taken 
	from the average pooling layer of the already trained model in the form of a vector 
	with $512$ elements.
	%
	To reduce the dimensionality, a principal component analysis (PCA) is then
	performed, and the principal components are used as features for the regression 
	model. Using PCA without the image embedding would result in bad performance
	as the microstructural information is not well represented (see 
	\cref{fig:ising_pcs_no_cnn}). The number of components is a hyperparameter
	that is investigated in \cref{fig:exp_var}; the first two components of the 
	Ising and the Cahn-Hilliard dataset are shown in the appendix and in particular 
	in \cref{fig:model_PCA_ising_ch}b and c.
	The data distribution in b) can be divided into three zones of low, 
	mid, and high temperatures. For low temperatures, the data distribution 
	has a larger variance than that obtained for the mid and high temperature 
	zones. In the high temperature zone, the two components become strongly 
	localized. The principal components for the Ising dataset are shown in \cref{fig:model_PCA_ising_ch}b while \cref{fig:model_PCA_ising_ch}c  shows 
	the components of the Cahn-Hilliard dataset.
	% and a resampled sampled dataset, respectively. 
	% The latter was created to avoid the highly imbalanced data 
	% distribution in terms of energy values of the structures as shown in 
	% \cref{fig:train_data_hist}b. The sampling strategy is described in 
	% \cref{app:CHresampling}. The explained sampling strategy is applied on 
	% only training dataset. 	The model is trained with both original
	% \cref{fig:train_data_hist}b and sampled 
	% \cref{fig:data_CH_100_percentage_sampled} datasets. 
	%	
	%	\begin{figure}[htbp]% 
		%			\centering
		%			\begin{subfigure}[t]{0.3\linewidth}
			%					\centering
			%					\includegraphics[width=\linewidth]{figures/aytekin/ch_hist.png}
			%					\caption{Original dataset}
			%					\label{fig:ch_hist}
			%				\end{subfigure}\hfil% equal to outside spacing
		%			\begin{subfigure}[t]{0.3\linewidth}
			%					\centering
			%					\includegraphics[width=\linewidth]{figures/aytekin/ch_hist_sampled.png}
			%					\caption{Dataset after sampling}
			%					\label{fig:ch_hist_sampled}
			%				\end{subfigure}
		%			\caption{Distribution of the Cahn-Hilliard dataset}
		%			\label{fig:ch_hist_all}
		%		\end{figure}
	
	
	
	
	%------------------------------------------------------------------------------
	\subsection{Overview of the Used Machine Learning Models}
	%------------------------------------------------------------------------------
	Altogether four fundamentally different \gls{ML}-based approaches are studied 
	and introduced subsequently.  An overview together with the used
	abbreviations is given in \cref{table:MLmethods}.
	%	%
	%	\begin{itemize}	
		%		\item \textbf{grad-PCR} and \textbf{physFeat1-PCR}: 
		%		Piecewise constant regression; input feature of grad-PCR is the sum of
		%		the norm of the gradient of the image, \cref{sec:gradfeature}. 
		%		For physFeat1-PCR the input features 
		%		are the intercept and slope of the line that was fitted to the 
		%		power spectral density, \cref{sec:PSDfeat}.
		%		
		%		\item \textbf{grad-SVR} and \textbf{physFeat1-SVR}:
		%		Support vector regression (SVR); input feature(s) as for piecewise 
		%		constant regression.
		%		
		%		\item \textbf{physFeat2-SVR}:
		%		Support vector regression (SVR) using an extended and highly-specialized 
		%		preprocessing pipeline for obtaining features. It is only used together 
		%		with SVR.
		%		
		%		\item \textbf{CNN-PCA-SVR}:
		%		A combination of a pretrained ResNET for image embedding and principal 
		%		components analysis; acting as automated feature extraction, to be used
		%		with support vector regression.
		%
		%		\item \textbf{MuCha-CNN}:
		%		A convolutional neural network that uses multiple channels as input
		%		(the image itself, as well as a Fourier and a wavelet transformation 
		%		of the image)
		%		
		%		\item \textbf{CNN}:
		%		Several \enquote{off the shelf} CNN architectures that are commonly used
		%		for image analysis; additionally, two different pretraining approaches
		%		are investigated.
		%	\end{itemize}
	%	
	%
	The first two methods are statistical \gls{ML} methods and use the above 
	introduced features; grad-PCR will be used as a baseline model.
	Note, that the second type of physics-based feature, 
	physFeat2, requires a regression method that is able to make use of the
	complex features. physFeat2 together with the very simple piecewise constant 
	regression showed a performance below the baseline model and therefore was 
	not considered here.
	%
	The last two \gls{ML} methods are CNN-based learning approaches where the 
	first method, CNN, does not require any preprocessed features. The second 
	method, MuCha-CNN, is tailored to the specifics of images and indirectly makes 
	use of some of the information that is also contained in the \gls{PSD}.
	Subsequently, all models are briefly introduced.
	
	\begin{table}
		\centering\small
		\begin{tabular}{p{2.8cm}c{7.6cm}c{4cm}} % !!!! The last parameter of multirow is the vertical offset!
			\toprule
			\bf Abbreviation  & \bf Features & \bf ML model  
			\\	\midrule
			grad-PCR      & simple sum of the norm of the gradient, sec.~\ref{sec:gradfeature}
			& \multirow[c]{2}{3cm}[-0.1em]{\centering piecewise constant regression (PCR), \cref{sec:pcrmodel}}
			\\ \cmidrule(r){1-2}
			\vspace{0pt}physFeat1-PCR & intercept \& slope of the fitted to the power spectral density
			&  
			\\\cmidrule(r){1-3}
			%-------------------------------------------------
			grad-SVR      & simple sum of the norm of the gradient, sec.~\ref{sec:gradfeature} 
			& \multirow[c]{4}{2.4cm}[-3em]{\centering support vector regression (SVR), \cref{sec:SVRmodel}}
			\\ \cmidrule(r){1-2}
			\vspace{0pt}physFeat1-SVR & intercept and slope of the line that was fitted to the power spectral density
			&  
			\\ \cmidrule(r){1-2}
			\vspace{0pt}physFeat2-SVR & highly-specialized, physics-based preprocessing pipeline for obtaining feature, sec.~\ref{sec:physFeat2}
			&  
			\\ \cmidrule(r){1-2}
			\vspace{0pt}CNN-PCA-SVR &  image embedding (ResNET) and principal components analysis for automated feature extraction, sec.~\ref{sec:CNN-PCA-SVR}
			& 
			\\\cmidrule(r){1-3}
			%-------------------------------------------------
			\vspace{0pt}CNN & \smallskip (input for CNN: the image w/o further preprocessing or feature extraction)
			& several CNN architectures; 2 pretraining approaches,
			sec.~\ref{sec:CNNmodel}
			\\\cmidrule(r){1-3}
			\vspace{0pt} MuCha-CNN & (input for CNN: the image itself, as well as a Fourier and a wavelet transformation 
			of the image)            
			& ResNet34 with simple training protocol, sec~\ref{sec:MuCha-CNNmodes}
			\\ 
			\bottomrule
		\end{tabular}
		\caption{%
			Overview of the investigated features and \gls{ML} methods.
		}
		\label{table:MLmethods}
	\end{table}
	
	
	
	%------------------------------------------------------------------------------
	\subsubsection{Piecewise Constant Regression (PCR)}
	\label{sec:pcrmodel}
	%------------------------------------------------------------------------------
	Piecewise constant regression (PCR) is a particular type of regression tree and is 
	one of the simplest \gls{ML} methods for regression. Here, it is used either 
	with the gradient-based feature (grad) or with the slope and intercept from the 
	\gls{PSD} (physFeat1). The training consists of (a) binning 
	the property data with a bin size of $\Delta=1$ (i.e., temperature for the 
	Ising dataset and energy for the Cahn-Hilliard dataset) and  (b) 
	\enquote{fitting} piece-wise constant approximations to the binned feature
	data by computing the mean values. The motivation for choosing this method
	in combination with the grad features is to use it as a baseline model 
	(grad-PCR). The thin solid line in the middle and right panels of
	\cref{fig:ising:featurespace} shows the model responses. 
	Predicting temperatures or energies works as follows: for a given 
	microstructure, compute the \gls{PSD}, and obtain slope and intercept values 
	of the fitted line, as explained above. Then, search for the nearest point in 
	the learned slope-intercept space and get the corresponding temperature or 
	energy. Slope and intercept have different physical dimensions, and  therefore,  
	dimensionless scaling of the data to the range $[0, 1]$ is performed before the
	distance can be computed.
	
	% Due to the imbalance distribution of energy values in Cahn-Hilliard datasets 
	% that causes a fluctuation of the response model, we calculate the average of 
	% the bin values by using the Nadaraya-Watson technique with Gaussian kernel 
	% for smoothing purpose. %For more details information, please refer to 
	% \ref{appendix:bin_average}.
	
	
	
	%------------------------------------------------------------------------------
	\subsubsection{Support Vector Regression (SVR)}
	\label{sec:SVRmodel}
	%------------------------------------------------------------------------------
	\Gls{SVR} \citep{Vapnik2000} is one of the commonly used methods for 
	regression in statistical learning, which performs well as long as the dataset
	is not too large (several tens of thousands of data records are still feasible). 
	We used the 
	implementation of the epsilon-insensitive SVM provided by scikit-learn 
	\citep{pedregosa2011scikit}. For the two simpler features, grad and physFeat1,
	determining the most suitable hyperparameter was done manually by trying 3-5
	different values, starting from the scikit-learn default values. For the 
	combination with the more complex feature physFeat2 the goal was to achieve
	an as high as possible accuracy. Therefore, a systematic hyperparameter search 
	was performed. The ranges of the hyperparameters and final selected values are 
	available in \cref{appendix:SVRhyperparameter}.
	
	
	
	
	%------------------------------------------------------------------------------
	\subsubsection{Convolutional Neural Network approaches (CNN)}
	\label{sec:CNNmodel}
	%------------------------------------------------------------------------------
	As \gls{DL} networks of different depth and degrees of complexity -- without 
	further alterations or adaptions -- we used a ResNet18, a ResNet152, a 
	DenseNet121, and an EfficientNet-B0. All of these architectures have achieved 
	very good results for ImageNet \citep{deng2009imagenet} classification problems,
	where EfficientNet-B0 performed particularly well \citep{tan2019efficientnet}. 
	%	Here, we investigate if this model can also provide better 
	%	results on the two used  datasets. This is always 
	%	the first intuition when working with machine learning models to go with larger models since they are shown to perform well on very complex tasks and datasets.
	% 	They were trained on the ImageNet dataset which is quite different than the dataset used for our study have their final weights, "pretrained weights" publicly available for anyone to use. This allows us to use the pretrained models to transfer the knowledge learned from training on the ImageNet dataset to our own. 
	%
	Each of these models will be used with two types of initial weights: (i) we use
	pretrained models with weights taken from ImageNet, (ii) we train the models that
	were initialized with random weights from scratch. 
	Transfer learning is used by freezing the whole pretrained model and then
	training only the final layer. Once the layer has been optimized, we train 
	the whole model using a learning rate of \SI{1e-4}{}. The training data
	itself was standardized by subtracting the mean and dividing by the variance. 
	Both datasets are split into training and test data, and 20\% of the training 
	data is kept as validation data. Once the model has been optimized, we evaluate 
	the model performance on the test data. 
	During the training, we use early stopping such that if the validation loss 
	does not improve for 20 epochs, we stop the training. Furthermore, the training
        starts with a learning rate of \SI{1e-3}{} and is reduced to half of its value 
        with a minimum of\SI{1e-5}{} when the monitored metric does not change anymore. 
        All architectures are implemented using	the Pytorch framework \citep{NEURIPS2019_9015}.
	
	%     RMSE scores for 100%:
	
	% Ising dataset:
	
	% without pretrained:
	% ResNet18:    7.9736975
	% ResNet152:    8.68864338
	% DenseNet121:    7.63121833
	% EfficientNet-B0:    10.02556697
	
	% with pretrained:
	% ResNet18:    7.49972849
	% ResNet152:    7.54431842
	% DenseNet121:    7.47683192
	% EfficientNet-B0:    7.78205308
	
	% Cahn-Hilliard dataset:
	% without pretrained:
	% ResNet18:    22
	% ResNet152:    13
	% DenseNet121:    9.1
	% EfficientNet-B0:    16
	
	% with pretrained:
	% ResNet18:    17
	% ResNet152:    26
	% DenseNet121:    11
	% EfficientNet-B0:    64 
	
	
	
	%------------------------------------------------------------------------------
	\subsubsection{A Multichannel Convolutional Neural Network (MuCha-CNN)}
	\label{sec:MuCha-CNNmodes}
	%------------------------------------------------------------------------------
	Convolutional neural networks \citep{lecun2015deep} are a special type of deep 
        neural network that have been widely used to process image data. One of
	their main aspects is convolutional layers, which act as feature detectors,
	making classical feature engineering obsolete. The input for CNNs may consist 
	of image data with three intensity channels (one for each of the colors red, 
	green, and blue). 
	Instead of the three color channels, we use one layer for the grayscale image; 
	the second and third channels contain the magnitude of the Fast Fourier 
	transformation (FFT) and the wavelet transformation of the grayscale image, 
	respectively. Since the \gls{PSD} is also based on FFT, the information 
	contained in the additional images is related to the \gls{PSD} and could be
	considered as an additional feature. Further information is given in 
	\ref{app:MuChaCNN}. Examples of the content of the three channels are shown 
	in \cref{fig:3channel_images_Ising_CH}. 
	%
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{3channel_images_Ising_CH.png}
		\caption[]{%
			An example of three channel images of the Ising dataset (top row) 
			and Cahn-Hilliard dataset (bottom row), respectively: a) and d) original 
			image; b) and e) fast-Fourier transform of the image; c) and f) wavelet 
			transform of the image.
		}	
		\label{fig:3channel_images_Ising_CH}
	\end{figure}
	
	We used residual networks with 34 layers (ResNet34) \citep{he2016deep}, 
	which is implemented using the Pytorch framework \citep{NEURIPS2019_9015}. 
	The size of each input channel is $64\times64$ pixels, and the respective 
	ranges are scaled such that all values are in between 0 and 255. 
	\Cref{fig:3channel_images_Ising_CH}b shows the magnitude-spectrum from 
	a complex array obtained by applying the Fourier transform using Numpy 
	\citep{harris2020array}.
	%
	\Cref{fig:3channel_images_Ising_CH}c shows the resulted image obtained by 
	applying the wavelet transform using the Python package PyWavelets
	\citep{lee2019pywavelets}. This image is the so-called diagonal detail 
	which results from vertical and horizontal highpass filtering. Since our 
	goal is not to study the effect of various wavelet functions, we pragmatically 
	chose one of the default functions from the \enquote{sym-family} functions.
	%
	For training, the weights of the multichannel CNN are randomly initialized. 
	Then, the network is trained for $100$ epochs with a fixed learning rate of
	$0.001$ and momentum equal $0.9$. As an optimizer, we used stochastic gradient 
	descent and the L1 loss.
	
	
	
	
	
	%==============================================================================
	\section{Results and Discussion}
	%==============================================================================
	All models were trained on a training dataset and evaluated on a separate 
	testing dataset. The testing dataset was identical for all models and comprises 
	for the Ising dataset of 1000 images covering the whole temperature range 
	($\approx 2\%$ of the total dataset) and 
	6000 images for the Cahn-Hilliard dataset obtained from two full, individual
	simulations ($\approx 10\%$ of the total dataset). \textcolor{red}{These test 
		datasets are kept entirely separated from the training process.}
	%
	The performance of the models was assessed by the \gls{RMSE} \textcolor{red}{and the $R^2$ score}. Results for 
	different models are shown in \cref{table:summary}. 
	%
	\begin{table}[htb]
		\centering\small
		\begin{tabular}{p{2.1cm}c{0.9cm}c{1.4cm}c{0.9cm}c{1.45cm}c{1.45cm}c{1.45cm}c{1.25cm}c{1.25cm}}
			\hline\hline
			& grad-PCR & physFeat1-PCR & grad-SVR & physFeat1-SVR & physFeat2-SVR & CNN-PCA-SVR & CNN (ResNet18) & MuCha-CNN  \\
			\hline
			Ising (RMSE):         & 16.3 & 14.5 & 14.1 & 12.3 & 8.5 & 10.0 & 8.0 & 8.9  \\
			Ising ($R^2$):         & 0.951 & 0.961 & 0.963 & 0.972 & 0.987 & 0.981 & 0.988 & 0.985  \\
			CH (RMSE): & 337 & 44 & 262 & 151 & 12 & 72 & 22 & 11  \\
			CH ($R^2$): & 0.674 & 0.994 & 0.802 & 0.934 & 1.0 & 0.985 & 0.999 & 1.0  \\
			\hline\hline
		\end{tabular}
		\caption{%
			Root mean square error (RMSE) \textcolor{red}{and the $R^2$ score} of the predictions of all models 
			and for both datasets. 
		}
		\label{table:summary}
	\end{table}
	%
	For the Ising dataset, all approaches show prediction accuracies that are 
	roughly in the same order of magnitude ($\operatorname{RMSE}= 12.15 \pm
	4.15$). The baseline model (grad-PCR) has only a slightly worse \gls{RMSE} 
	than the other models, and the best model(s) achieve half of the \gls{RMSE} 
	of the baseline model.
	%
	For the Cahn-Hilliard dataset, the differences between models are more 
	pronounced ($\operatorname{RMSE}= 171.5 \pm 160.5$). In particular, the 
	gradient-based baseline model grad-PCR, as well as grad-SVR, perform rather 
	badly with RMSE values of 332 and 262. \Gls{SVR} with the more detailed, 
	physics-based features, physFeat2-SVR, and the multichannel CNN perform best, 
	followed by the vanilla ResNet18. Note that the values can not be directly 
	compared to those of the Ising model, but for both datasets, the best 
	performing models are physFeat2-SVR, the CNN (ResNet18) with training from 
	scratch, and MuCha-CNN.
	
	To understand which details of the datasets are more difficult to 
	predict than the others, a confusion matrix for all models, and both 
	problems is shown in \cref{fig:confusion_matrix} for the true vs 
	the predicted properties of the testing data.
	%
	In the following, we first discuss the general model performance and 
	then discuss the advantages and disadvantages of all individual 
	models.
	%
	\begin{figure}
		\centering
		\includegraphics[width=1.0\textwidth]{confusion_matrix.jpg}
		\caption[]{%
			Confusion matrices of all models and used futures for  
			a) the Ising dataset and b) the Cahn-Hilliard dataset. The red marker 
			illustrate the results when the simple norm of the gradient 
			was used as a feature. The dashed red line indicates the 
			values for a perfect prediction. Vertical and horizontal range from 
			each subfigure of a) and b) are $0..256$ and $0..8000$, respectively.
		}
		\label{fig:confusion_matrix}
	\end{figure}	
	
	%------------------------------------------------------------------------------
	\subsection{General Analysis of the Prediction Errors}
	%------------------------------------------------------------------------------
	% Ising Dataset
	\paragraph{Analysis of the Ising Dataset}
	\Cref{fig:confusion_matrix}a shows the ground truth temperature 
	vs the predicted temperature for the Ising dataset. We observe for the baseline 
	model that the accuracy for intermediate temperatures is better than that for 
	low or high temperatures. For these extreme temperatures, the magnetization
	values of the corresponding microstructures are either $0$ or $1$. Taking a 
	look at how all models perform in the low temperature regime at 
	$T\lessapprox 10$, we see that MuCha-CNN performs best, however, all
	models' predictions are affected by errors where predicted temperatures of
	low temperature microstructures are too high.
	In the high temperature regime, MuCha-CNN exhibits a slightly larger scatter 
	as, e.g., compared to the regular CNN approach 
	which performs better than all other models for high temperatures. 
	
	The low temperature regime is more difficult to predict because the changes 
	in the images that correspond to changes in the temperature are smaller 
	than those in the higher temperature regime. The underlying physical reason 
	for this is that low temperature results in considerably slowed down 
	dynamics of the system. The difficulties of the predictions in the high 
	temperature regime have a different reason. There, the microstructure tends 
	to become increasingly random, and the temperature differences are related 
	to increasingly small remnants of the ordered structure.
	
	What is the influence of using the ``physical features'' (i.e., the power 
	spectral density)? In the first two confusion matrices from the left, we 
	used both the simple gradient-type features as well as the PSD-features 
	(shown as the red and blue markers). There, we see that the use of physical
	features has the most pronounced influence on low temperature 
	predictions. Furthermore, the complex SVR model benefits well from more 
	complex features; in particular, physFeat2-SVR qualitatively shows 
	one of the best confusion matrices with balanced performance for all 
	temperatures.
	
	% Cahn-Hilliard dataset	
	\paragraph{Analysis of the Cahn-Hilliard Dataset}
	Models trained with the Cahn-Hilliard dataset have the worst
	\gls{RMSE} for the two gradient-feature-based methods  which
	also shows in the confusion matrix in \cref{fig:confusion_matrix}b (the
	red marker in the two leftmost plots). In particular for low energy values
	one observes artifacts that result in different distributions for the two
	simulations of which the testing dataset consists. The simplistic features
	are not able to capture the microstructural differences at low energy values 
	properly and are not able to generalize to microstructures from other 
	simulation runs.
	Using the physical PSD features, this behavior changes, and in particular
	the physFeat2-SVR model performs nearly perfectly.
	
	In fact, there is only one model that performs better: the MuCha-CNN, 
	which operates with two additional types of images obtained from a wavelet 
	and Fourier transform. These two additional channels contain information
	about different wavelengths and spatial structures and, therefore, are
	related to the physical features. The CNN (ResNet18) achieves nearly
	identical prediction performance.
	
	From the confusion matrix, we see that the CNN-PCA-SVR model with 150 
	principal components performs worse than all other models. Generally, 
	CNN-PCA-SVR is more accurate in the low energy regime as compared to the 
	higher energies. We also can clearly see differences between the two simulation 
	runs contained in the testing dataset. This also might be an indicator
	that the training dataset is not sufficiently large enough for this 
	method, and the model fails to generalize properly.
	Since the physFeat2-SVR model with highly specialized features shows 
	nearly perfect predictions, it follows that the cause of the bad 
	performance is the feature extraction by CNN and PCA. Principal component 
	analysis is a linear method and, therefore, might be limited in terms of the 
	features that can be represented.
	
	A particular challenge of this dataset is that it is strongly imbalanced 
	with regards to the energy distribution, cf. \cref{fig:train_data_hist}b.
	This is the reason why the RMSE value of CNN-PCA-SVR is better 
	than those for the grad-PCR and grad-SVR, even though the confusion 
	matrix of these two methods suggests the contrary. The reason for this is 
	the high amount of data for lower energies, where these two models 
	perform particularly badly. The imbalance of data is also the reason why 
	almost all models perform better in the low energy range, as there is a 
	large amount of data. Furthermore, also the microstructural patterns in the 
	images at low energies are clearly developed and distinct. Predictions in 
	the high energy regime are slightly less accurate, which is partially due to 
	the smaller amount of data. Additionally,  the variance among the training 
	examples is larger in this energy regime. 
	
	
	%------------------------------------------------------------------------------
	\subsection{Influence of the Size of the Training Dataset}
	%------------------------------------------------------------------------------
	An important goal is that models are able to achieve a high prediction 
	accuracy. However, in many practical situations, it can be equally important to
	achieve good results with a small dataset, e.g., because more data might not be
	available or computationally too costly to obtain. The training dataset of the Ising and Cahn-Hilliard model 
	contains $49,000$ images and $54,000$ 
	images, respectively. To see the impact of the size of the datasets, the RMSE results 
	for training with different size percentages are compared in 
	\cref{fig:percentage_studying}.
	%
	\begin{figure}
		\centering
		\includegraphics[width=1.0\textwidth]{percentage_studying.jpg}
		\caption{%
			Prediction performance for different sizes of training datasets
			for the Ising datasets (left) and the Cahn-Hilliard dataset (right).
			The $x$-axis of the insets are plotted in log-scale to reveal
			the low percentage area. Markers show the performance for 
			$0.1 \%$, $0.5 \%$, $1 \%$, $10 \%$, $20 \%$, $40 \%$, $60 \%$, 
			$80 \%$ and $100 \%$ of the total training data for both datasets.}
		\label{fig:percentage_studying}
	\end{figure}
	%
	For both datasets, there is a pronounced drop in the RMSE values when the 
	percentage of training data is increased from $0.1\%$ to $10\%$; from 
	$10\%$ to $100\%$, there is only a slight decrease. The corresponding 
	confusion matrices for all sizes can be seen in
	\ref{appendix:confusion_percentage}. In general, using at least $10\%$ of the
	original datasets is a good compromise. These are approximately $5000$ images. 
	For small datasets of 50 images, the \gls{SVR} methods clearly outperform 
	CNN-based approaches for the Ising model. For the Cahn-Hilliard model, at least
	50 images are required, and only physFeat2-SVR is able to perform well.
	Additionally, the two CNN-based methods show acceptable but less robust 
	performance as they are still dependent on the chosen testing samples.
	
	
	%------------------------------------------------------------------------------
	\subsection{Required Computational and Feature Engineering Effort}
	%------------------------------------------------------------------------------
	The time required for training (including computing the feature values) and 
	for predicting for each of the \gls{ML} approaches as a function of the size 
	of the training dataset are shown in \cref{fig:comp_time}.
	%\cref{table:ising_train_time}, \cref{table:ising_pred_time}, 
	% \cref{table:ch_train_time} and \cref{table:ch_pred_time}. 
	%
	\begin{figure}
		\centering
		\includegraphics[width=0.9\textwidth]{computational_time.pdf}
		\caption{Computational time for training (including computation of the
			input features for PCR and SVR) and predictions of all models and both
			datasets as a function of the size of the training dataset. The 
			vertical axes are scaled logarithmically.}
		\label{fig:comp_time}
	\end{figure}
	%
	All times were measured on an 8-core workstation with Intel Core i9-10850K CPU,  $32$ GB RAM, 
	and a Nvidia GeForce RTX 3060 GPU with $12$ GB RAM. We observe a huge difference 
	in the times required for training the models: the baseline model 
	grad-PCR is the simplest model; it only requires 
	binning and averaging of the training data and, therefore, is the fastest 
	to train. Obtaining the gradient of the image is computationally cheaper 
	than computing the \gls{PSD} -- for the used implementations, roughly by a factor of two. 
	%
	The PCR models require very little training time (even though the 
	implementation was not optimized): it is almost the same for every 
	fraction of the Ising dataset and takes around \SI{1.2}{\second}.
	For training, excluding the computation of the features and predicting 
	it takes \SI{0.5}{\second}. 
	For the Cahn-Hilliard dataset, the computational times show only small
	deviations from the times for $10\%$ of the data: it is
	around \SI{33}{\second} for training and less than \SI{10}{\second} for 
	predicting.
	
	The \gls{SVR} models require substantially more training time as the size
	of the datasets increases. The training time additionally depends strongly 
	on the features. For the simple grad-SVR approach, the training time without
	feature calculation increases exponentially with the size of the dataset; 
	it takes around \SI{5400}{\second} and \SI{2300}{\second} for $100 \%$ of 
	the Ising and the Cahn-Hilliard dataset, respectively. The reason is that 
	the model complexity (the number of support vectors) increases with larger
	datasets. This also shows in the more than exponentially increasing 
	prediction times.
	
	CNNs and MuCha-CNN require, by comparison, the most time for training.
	Additionally, this also depends strongly on the complexity of the chosen 
	model architecture. Furthermore, factors such as batch size can significantly 
	impact the training and prediction times. E.g., even though the training process 
	used in CNNs does not use any prepossessing that could increase the train
	time, we used a batch size of 32 during the training and employed early 
	stopping where we waited for 20 epochs to see if the results were improved.
	These are two examples where the training time could have been reduced 
        if a loss of accuracy would be acceptable.
	
	The last aspect is the effort that is required to engineer the features for 
	the PCR and  SVR models. While the physFeat2-SVR model achieves for both 
	datasets superior prediction accuracy, the identification of physically 
	reasonable features and the concomitant hyperparameter study might outweigh 
	the high accuracy. This also holds for MuCha-CNN, even though no 
	hyperparameter tuning was performed. In case there are no obvious physical features, one of the 
	relatively shallow CNNs is a good choice. However, the amount of required 
	training data is somewhat higher than that of other models. A fast-to-train
        model with still acceptable accuracy is the CNN-PCA-SVR. It is also the model
	that makes predictions considerably faster than all other models (except for 
	PCR for larger datasets).
	%------------------------------------------------------------------------------
	\subsection{General discussion}
	%------------------------------------------------------------------------------
	\textcolor{red}{
		Feature engineering methods, which reduce features' dimensions and extract the essential information from data, depend strongly on the nature of data and 
		may need particular care \citep{nguyen2019ten}. Therefore, studying, preprocessing the data, and choosing an appropriate technique requires dedicated effort. The \gls{PSD} features that we choose for several approaches, such as physFeat1-PCR/-SVR, or physFeat2-SVR can be useful for various types of problems, as was shown based on
		the two datasets investigated in this work. Clearly, the next question is, if these approaches can also
		be used for data that visually looks very different. To answer this question
		a very different kind of simulation dataset was investigated, which is
		from the domain of computational fluid dynamics. There we predicted the Reynolds 
		number (as ``property'') from simulated images of fluid flow (as ``structure''). 
		In \cref{fig:CFD_PSD_horiz_64} the results are shown. }
	%
	\textcolor{red}{
		The overall accuracy is slightly lower as compared to the two mainly 
		investigated datasets. The MuCha-CNN method is independent of the nature of data 
		since it can extract the most important features of the image data without having
		to rely on engineered features (see \cref{fig:confusion_matrix} and \cref{fig:results_CFD_uniformed}) and therefore performed considerably better.
	}
	%
	\textcolor{red}{
		Nonetheless, most of the methods still perform rather well. Thus, 
		using the \gls{PSD} features for building machine learning models in situations
		where the data mostly consists of fluctuations and patterns of various
		wavelenghts is a reasonable approach. 
	}
	

	%
	%	\textcolor{red}{
		%		\Cref{fig:compare_sklearn_rmse} and \cref{fig:compare_sklearn_R2} shows the comparison of the performance  from several regression methods with our approaches.
		%	}
	
	%==============================================================================
	\section{Conclusion}
	%==============================================================================
	We investigated the problem of learning and predicting the so-called 
	structure-property relation, i.e., the mathematically non-trivial mapping from 
	a two-dimensional structure to a scalar value. For this, we compared the predictive
	power of several machine learning models -- statistical learners as well as
	deep learning-based models -- for predicting the properties of 
	microstructure from the Ising model and the Cahn-Hilliard model. One
	of the challenges was to cope with strongly imbalanced datasets in case of the Cahn-Hilliard
	model.
	
	We found that statistical learning approaches that include a physics-based
	feature engineering may outperform more generic approaches, e.g., CNN-based
	models both in terms of accuracy and train/prediction time. The improved
	accuracy, in particular for smaller datasets, is partially due to the possibility of 
	automatically introducing translational, 90-degree-rotational, and mirroring invariances through the engineered features, but additionally, the reduction
	of the dimensionality of the feature space is beneficial for the computational
	efficiency. However, the feature engineering requires a certain
	amount of domain knowledge and the overall effort is generally large.
	
	Comparing such \gls{ML} approaches to classical forward simulations, we
	found that even predicting properties that usually
	require a large numerical simulation effort can be learned and reliably 
	predicted. Nonetheless, generalizing the models such that they are applicable 
	for different domain sizes or different boundary conditions is one of the 
	current shortcomings and field of active research in various communities.
	
	
	% 	%
	% 	The drop in physFeat1-PCR performance in Ising compared to the Cahn-Hilliard dataset could be justified due to the innate randomness of the Ising dataset. Compared to it, the Cahn-Hilliard dataset is an almost perfect mapping from image to energy value. This observation is further confirmed during physFeat2-SVR model hyperparameter selection in the table. \ref{tab:PSD_SVR_hyperparameters}, where physFeat2-SVR performs the best with a smoother solution in the Ising dataset (lower $C$ value) and an attempt to fit the training data very closely in the Cahn-Hilliard dataset (high $C$ value).
	% 	%
	% 	The preprocessing hyperparameter selection procedure has been shown to be beneficial for physFeat2-SVR. The evaluated preprocessing hyperparameters for the Ising dataset have shown remarkable performance for both datasets. As a result, these parameters could be used in combination with SVR for other microstructure datasets without the necessity for reevaluation.  
	
	% 	By studying various portions of the datasets, we have proved that training with around $10\%$ or $20\%$ of provided data can already acquire satisfactory results, which might reduce a lot of time and effort in training the model and more beneficial in the data generation procedure (e.g. experiments, simulations).
	
	\section*{CRediT authorship contribution statement}\noindent
	{Aytekin Demirci}: Software, Writing, Formal analysis;
	{Kishan Govind}: Software, Writing, Formal analysis;
	{Binh Duong Nguyen}: Software, Writing, Formal analysis;
	{Pavlo Potapenko}: Software, Writing, Formal analysis;
	{S\'ebastien Bompas}: Software, Writing, Formal analysis;
	{Stefan Sandfeld}: Conceptualization, Software, Writing, Formal analysis, Supervision, Funding Acquisition.
	
	\section*{Declaration of competing interest}\noindent
	The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
	
	\section*{Data and code availability}\noindent
	The two datasets, the data of all shown plots together with the code for reproducing
	the plots, as well as supplementary materials, are available at 
	\url{https://gitlab.com/...}. An additional snapshot of the repository 
	with data and code has been published at the
	permanent DOI https://doi.org/10.5281/zenodo.xyz.
	
	\section*{Acknowledgement}\noindent
	Financial support from the European Research Council through the ERC Grant 
	Agreement No. 759419 is gratefully acknowledged.
	\bigskip\bigskip
	
	\begin{appendix}
		%=================================================================================
		\section{Explanation of the Data Generation by the Simulation Models}
		\label{appendix:simulations}
		%=================================================================================	
		In the following, we introduce the used simulation models 
		together with the underlying physical behavior. As the emphasis of this work
		is not on simulations, please refer to the given references for further details.
		
		%------------------------------------------------------------------------------
		\subsection{The Ising Model -- Statistical Mechanics Approach to Ferro-Magnetism}
		%------------------------------------------------------------------------------
		
		\begin{figure}[!htb]
			\centering
			\includegraphics[width=0.9\textwidth]{Ising_lattice.pdf}
			\caption{Graphical representation of the Ising model. The spins are first randomly placed in the $N \times N$ lattice. The evolution of the lattice follows the Monte Carlo algorithm for a fixed temperature T. In this example, the temperature is below the Curie temperature, and the final state is organized.}
			\label{fig:ising_lattice}
		\end{figure}
		%	\begin{figure}[htbp]
			%		\centering
			%		\includegraphics[width=0.9\textwidth]{Ising_mag.pdf}
			%		\caption{%
				%		    Evolution of the global magnetization of the lattice against the temperature. We can see that the system 
				%		    undertake a phase transition at the Curie temperature $T_c$
				%			Some examples for the binary microstructure obtained at difference temperature are shown on the graph. The 
				%			image size is $64\times 64$ pixels.
				%			}
			%		\label{fig:ising_examples}
			%	\end{figure}
		
		The Ising model is a theoretical model developed to describe ferromagnetism by W. Lenz in \citep{Lenz} and solved for the 1D case by E. Ising in \citep{Ising1925}.
		In the 2D case, magnetic dipoles are located in the center points of a $N \times N$ grid. 
		The Hamiltonian governs the total energy of the system, which, in the case of periodic 
		boundary conditions is given by:
		\begin{equation}
			\centering
			H = -\sum_{\langle i,j \rangle }J_{ij}\sigma_i\sigma_j - \mu\sum_i h_i \sigma_i\;,
			\label{eq:IsingHamiltonian}
		\end{equation}
		where $\langle i,j \rangle $ is the set of all the nearest neighbors of $i, j$, and $J_{ij}$ is the coupling force between the $i^{th}$ and $j^{th}$ magnetic dipole, $\sigma \in \{-1, 1\}$  is the sign of the magnetic dipole at a given site, $\mu$ is the magnetic moment and $h$ is an external field apply to the lattice.
		%
		In our case, we simplify \cref{eq:IsingHamiltonian} by setting $h=0$ and $J=1$ for all magnetic dipoles
		\begin{equation}
			\centering
			H = -J\sum_{\langle i,j \rangle }\sigma_i\sigma_j\;.
			\label{eq:IsingSimple}
		\end{equation}
		%
		The magnetization of the system, which is a quantitative measurement of the excess dipole signs
		is given as, 
		\begin{equation}
			\centering
			\langle M \rangle= \frac{1}{N^2}\sum_{i}\sigma_i \;.
			\label{eq:magnetization}
		\end{equation}
		
		\paragraph{Simulation set up}
		To simulate this system, we use the Metropolis algorithm. First, we randomly initialize the $N \times N$ dipoles, where we chose $N=64$. Then, we randomly select one site $s(i,j)$, $i,j\in [1,N]$ and flip the corresponding magnetic dipole by changing its sign. We then calculate the energy contribution of the new configuration. Due to the periodic boundary conditions,  the nearest neighbors of the first and last magnetic dipole in the lattice are the $N^{th}$ and first magnetic dipole, respectively.
		If the energy of the new configuration is smaller than the previous one, we keep the new configuration. Alternatively, if the energy of the new configuration is greater than the previous configuration, we only keep it with a probability $p=e^{-\beta E}$, $E=H_p-H_n$ and $\beta = \frac{1}{k_b T}$ where $T$ is the temperature of the system and $k_b$ the Boltzmann constant, set to $1$ for simplicity. 
		The critical temperature, or Curie temperature ($T_c$), is defined as,
		\begin{equation}
			\label{eq:Tc}
			T_c = \frac{2J}{k_B \ln(1+\sqrt{2} )} \approx 2/\mathrm{ln}(1+\sqrt{2}) \approx 2.269\;.
		\end{equation}
		%
		We repeat those steps until we reach a stopping criterion.
		We choose $N^{3}$ as a stopping criterion for the simulation, where N is the size of the lattice. The idea behind this is that every site of the $N \times N$ lattice is visited approximately $N$ times so that the information has sufficient time to travel through all the lattices. 
		%	The lattice state obtain from the Metropolis algorithm is then saved as a black and white image (representing the two different dipole sings), ranging from 0 to 255 for the pixel values.
		%	The images are generated from $0$ to $2 \times T_c$, where $T_c$ is (assuming that $k_B=1$) given as
		%
		%	\begin{equation}
			%		\label{eq:Tc}
			%		T_c = \frac{2J}{k_B \ln(1+\sqrt{2} )} \approx 2/\mathrm{ln}(1+\sqrt{2}) \approx 2.269\;.
			%	\end{equation}
		%$T_c$ can be simplify to $2/\mathrm{ln}(1+\sqrt{2}) \approx 2.269$.
		
		
		%------------------------------------------------------------------------------
		\subsection{The Cahn-Hilliard Equation -- Evolution of Phase Separation in Binary Systems}
		%------------------------------------------------------------------------------
		In this work, we present a simple case of phase separation evolution in a binary system, including the elasticity by a coupling approach between a phase field model and an eigenstrain problem. 
		%
		The phase field model, which is motivated by Cahn \citep{cahn1961spinodal} for spinodal decomposition in binary alloys, is computed by solving the Cahn-Hilliard equation. There, the evolution of the composition field $c$ is governed  by the minimization of the free energy, 
		\begin{align}
			\label{eq:cahn_hilliard}
			\frac{\partial c}{\partial t} = M_c \nabla^2 \frac{\delta E}{\delta c},
		\end{align}
		where $E$ is the free energy of the system and $M_c$ is a homogeneous and isotropic interface mobility coefficient.
		The free energy density consists of the potential energy density ($\Phi^{\mathrm{bulk}}$), gradient energy density ($\Phi^{\mathrm{grad}}$), and elastic energy density ($\Phi^{\mathrm{el}}$) and is given as,
		\begin{align}
			\label{eq:energy}
			\Phi = \Phi^{\mathrm{bulk}}+\Phi^{\mathrm{grad}}+\Phi^{\mathrm{el}},
		\end{align}
		where $\Phi^{\mathrm{bulk}} = c_0 c^2(1-c)^2$, $\Phi^{\mathrm{grad}} = \frac{1}{2} k_{\mathrm{c}} |\nabla c|^2$ and $\Phi^{\mathrm{el}} = \frac{1}{2} \boldsymbol{\sigma} : \boldsymbol{\varepsilon}^{\mathrm{el}}$. The two constants $c_0$ and $k_c$ are the density scale and the gradient energy density, respectively. $\boldsymbol{\sigma}$ is the stress tensor and $\boldsymbol{\varepsilon}_{\mathrm{el}}$ is the elastic strain tensor. The energy functional is then	
		\begin{align}
			\label{eq:energy_tot}
			E 	&= \int_{\Omega} (\Phi^{\mathrm{bulk}}+\Phi^{\mathrm{grad}}+\Phi^{\mathrm{el}})\mathrm{d}\Omega  
			= \int_{\Omega} (c_0 c^2(1-c)^2+\frac{1}{2} k_{\mathrm{c}} |\nabla c|^2+\frac{1}{2} \boldsymbol{\sigma} : \boldsymbol{\varepsilon}^{\mathrm{el}})\mathrm{d}\Omega\;.
		\end{align}
		
		The eigenstrain problem is fulfilled through the mechanical equilibrium,
		\begin{align}
			\label{eq:mech_equi}
			\nabla . \boldsymbol{\sigma} = 0,
		\end{align}
		with the stress tensor $\boldsymbol{\sigma} = \boldsymbol{C}:\boldsymbol{\varepsilon}^{\mathrm{el}}$ 
		and the stiffness tensor $\boldsymbol{C}$. The elastic strain tensor is defined as
		\begin{align}
			\label{eq:elastic_strain_tensor}
			\boldsymbol{\varepsilon}^{\mathrm{el}}(\boldsymbol{u}, c) = \boldsymbol{\varepsilon}(\boldsymbol{u}) - \boldsymbol{\varepsilon}^{\mathrm{iel}}(c)\,,
		\end{align}
		%
		where $\boldsymbol{\varepsilon} = \frac{1}{2}(\nabla \boldsymbol{u} + (\nabla \boldsymbol{u})^T)$ is the total strain tensor from displacement $\boldsymbol{u}$ caused by lattice distortion and  $\boldsymbol{\varepsilon}^{\mathrm{iel}}$ is the non-elastic part that causes the eigenstrain.
		
		\paragraph{Simulation set up}
		The coupling partial differential equations are implemented and solved in FEniCS, an open-source Python library package \citep{LangtangenLogg2017}. The width and height of the domain are both \SI{20}{\mu\m}. The elastic stiffness constants for the isotropic material model are $C_\mathrm{11}=$\SI{198}{\GPa} , $C_\mathrm{12}=$\SI{138}{\GPa} and $C_\mathrm{44}=$\SI{97}{\GPa}. The density scale is $c_\mathrm{0}=$\SI{50e-6}{\J\mu \m^{-3}} and the gradient energy density is $k_\mathrm{c}=$\SI{10}{\J\mu \m^{-1}}. Periodic boundary conditions are used.
		
		
		We start with the initial values drawn from a uniform random distribution from the range between $0$ and $1$. Before the two phases are separated clearly into $0$ and $1$, there is a “mixing state" (also called  binodalor unstable state) where two phases are mixing (the values of concentration $c$ are neither $0$ nor $1$ yet but in the range between $0$ and $1$) which cause the non-convexity of the energy, and this is where we see a kink in the energy curve. This “mixing state” has a duration that depends on how the parameters are chosen (the strong or weak influence of the gradient term and/or the chemical term on the total energy of the whole system). After the kink, the two phases become clearly separate, where the values are either 0 or 1. It then gradually lowers the energy by merging the phases. The similar behavior of the energy curve and phenomenon can be referred to \citep{kim2021unconditionally}.
		
		The simulation is divided into two parts: the first part, where the energy decays 
		rapidly, and the second part, where the energy decreases only slowly. 
		We run the simulation in the former with $2,000$ steps with $\Delta t$ values spaced evenly on a log scale (minimum and equal to \SI{1e-7}{\second} at the beginning, maximum and equal to roughly \SI{1e-4}{\second} at the end); and in the latter with $10,000$ steps with constant $\Delta t =$ \SI{1e-4}{\second}. The image data are exported at every step of the first part and every $10$th step of the second part of the simulation. Therefore, there are roughly $60000$ images in the produced dataset.
		
		
		%	\section{Calculation of the average values for the bins}
		%	\label{appendix:bin_average}
		%	The average response is achieved by sliding a bin's center starting from the lowest to highest label values. Let  $L = \max(y) - \min(y)$,  where $y$ represent all label values. The width of a bin is 
		%	\begin{align}
			%	w_b = \frac{L}{512},
			%	\end{align}
		%	and the bin is shifted by half of its width $\Delta L = {w_b}/{2}$.
		%	
		%	To compute the average values for the bins we introduce the Nadaraya-Watson kernel regression with Gaussian RBF kernel. The idea is to fit most of the Gaussian inside a bin by selecting $\sigma = \frac{w_b}{3}$ and $\mu$ as bin's center.
		%	The kernel value is computed as:
		%	\begin{equation}
			%	    \centering
			%	    K(y_b, y) = e ^{-\frac{(y_b - y)^2}{2\sigma^2}},
			%	\end{equation}
		%	where $y_b$ is label of a bin and $y$ is label of a point in respective bin. The interpolated features for bin $x_b$ are computed as:
		%	\begin{equation}
			%	    \centering
			%	    x_b = \frac{\sum_{\langle i \rangle }K(y_b, y_i)x_i}{\sum_{\langle i \rangle }K(y_b, y_i)},
			%	\end{equation}
		%	where $\langle i \rangle$ is a set of points inside a bin with $x_i$ features and $y_i$ labels. Basically, points inside a bin must satisfy $y_b - \frac{w_b}{2}\le y_i\le y_b + \frac{w_b}{2}$.  As a result, we have $1025$ interpolated points, which are uniformly distributed  with respect to the label value. The interpolated points are used to fit a curve in $3D$ space.
		
		%==============================================================================
		\section{Additional Information and Discussion of the Machine Learning Models}                            
		\label{app:additional_model_info}
		%==============================================================================
		
		
		
		%------------------------------------------------------------------------------
		\subsection{Hyperparameter search for the SVR model with "physFeat2" features}
		\label{appendix:SVRhyperparameter}
		%------------------------------------------------------------------------------
		In this work, the support vector regression (SVR) implementation of scikit-learn \cite{pedregosa2011scikit}
		is used, which is based on radial basis function kernels. It requires three 
		hyperparameters: a regularization term $C$, a kernel coefficient $\gamma$, and 
		the distance within the epsilon-tube, $\epsilon$, where points are not 
		penalized.  
		To find the optimal values of these hyperparameters, we used the tree-structured 
		Parzen Estimator (TPE) \cite{bergstra2011TPE}, where the authors also mention 
		how crucial the tuning of SVR hyperparameters is for the model performance. 
		The ranges of SVR hyperparameters are provided in 
		\cref{tab:PSD_SVR_hyperparameters}, where $C$ is the regularization term (box
		constraint), $\gamma$ is the kernel coefficient and $\epsilon$ is the distance 
		within the epsilon-tube where points are not penalized. The TPE search is
		performed $1024$ times, followed by hand-tuning. The model performance is
		evaluated by performing 5-fold cross-validation on training data, consequently 
		no test data was seen throughout the hyperparameter search. The resultant 
		final parameters are provided in the last two columns in
		\cref{tab:PSD_SVR_hyperparameters}. 
		%
		\begin{table}
			\centering
			\renewcommand{\arraystretch}{1.3}
			\begin{tabular}{ c{30mm}|c{25mm}c{25mm}|c{20mm} c{20mm}  }
				\hline\hline
				\textbf{Hyperparameter} & \textbf{Range} & \textbf{Logarithmic distribution} & \textbf{Ising}  & \textbf{Cahn-Hilliard} \\\hline
				$C$            & $(10^{-5}, 10^6)$  & True        & $10^2$ & $10^5$ \\  	
				$\gamma$       & scale, auto        & False       & $1/64$ & $1/64$ \\
				$\epsilon$     & $(0.1, 0.5)$       & False       & $0.12$ & $0.4$ \\
				\hline\hline
			\end{tabular}
			\caption{
				PSD-SVR model hyperparameters: the columns "Range" and "logarithmic 
				distribution" denote the parameter range/values that constrain the 
				hyperparameter optimization, the last two columns show the final 
				parameter for the two datasets.
			}
			\label{tab:PSD_SVR_hyperparameters}
		\end{table}
		%
		The box constraint $C$ provides insight into the characteristic differences 
		between the two datasets: the larger $C$, the stronger the penalty for not reaching the $\epsilon$-tube region. At the same time, a smaller $C$ leads to a stronger regularization. Theoretically, it is impossible to exactly predict 
		the temperature of an image from the Ising dataset based on only one snapshot
		of the whole simulation trajectory because the temperature information is 
		related to the probability of flipping the sign of the spin of one of the
		elementary magnets. As a result, a non-uniqueness could be observed: there may exist
		near-identical images at different temperature values. The non-uniqueness is countered by 
		the regularization of the SVR model, which, therefore, has to be higher 
		for the Ising dataset as compared to the Cahn-Hilliard dataset.
		
		
		
		
		
		%------------------------------------------------------------------------------
		\subsection{Support Vector Regression with Image Embedding and PCA (CNN-PCA-SVR)}
		%------------------------------------------------------------------------------
		The combination of a ResNet18 and the principal component analysis serves as a
		problem-agnostic way of computing features that can then be used for support 
		vector regression. Using \emph{only} PCA turned out to result in features
		(the principal components) that don't contain sufficient information. The
		first two components are shown in \cref{fig:ising_pcs_no_cnn}.
		%
		\begin{figure}
			\centering
			\includegraphics[width=0.37\textwidth]{ising_perc_100_pcs_no_cnn.png}
			\caption[]{%
				The first two principal components directly obtained by using the pixel values
				of the input images as features for PCA. 
			}
			\label{fig:ising_pcs_no_cnn}
		\end{figure}
		%
		The strong localization for high-temperature data in the latent space
		is caused by the strong randomness contained in the images. For lower
		temperatures, this is different, but the radial symmetric distribution
		mainly captures the randomness of the images and not the patterns
		with different wavelengths. Therefore, a CNN was used to extract
		features (i.e., the weights of the last hidden layer). Those were
		then used as input for a PCA. The number of used principal components 
		is a hyperparameter that was chosen to keep the
		computational cost as low as possible and at the same time to achieve an 
		as good as possible prediction performance. We start by taking a look at the 
		variance explained as a function of the number of principal components in
		\cref{fig:exp_var}. 
		%
		\begin{figure}
			\hbox{}\hfill
			\includegraphics[width=0.40\textwidth]{explained_variance.png}
			\hfill
			\includegraphics[width=0.45\textwidth]{RMSE_vs_num_PCS.png}
			\hfill\hbox{}
			\caption{Scree plot showing the cumulative explained variance (left) and the 
				RMSE error (right), both  as a function of the  number of principal components.
			}
			\label{fig:exp_var}
		\end{figure}
		%
		%
		\begin{figure}
			\centering
			\includegraphics[width=\textwidth]{figures/pca_ising_ch_confusion.jpg}
			\caption[]{%
				Confusion matrices of both datasets with changing number of principal components that are used as features for the SVR model.
			}
			\label{fig:pca_ising_ch_confusion}
		\end{figure}
		%
		The plot shows that the variance explained quickly increases for up to 
		15 principal components. This explains the rapid decrease in the RMSE when 
		the number of components is increased from 1 to 15 (the influence of the 
		number of components for both datasets are shown in the right figure of
		\cref{fig:exp_var}). 
		
		In \cref{fig:pca_ising_ch_confusion}, confusion matrices are given for changing the number of principal components for both datasets.
		
		\paragraph{Ising dataset} When there is only one principal component, the model is not able to distinguish images from each other in the high and low temperature zones. In the intermediate temperature zone, the predictions are slightly better. However, the model can predict the images around the critical temperature, $T_c$, successfully by reducing the information from $64\times64$ arrays to only one scalar value. Therefore, if the objective is to predict the phase transformation, this would be a highly efficient model. Already starting from two principal components, the confusion matrix looks very similar to \cref{fig:confusion_matrix}. 
		
		\paragraph{Cahn-Hilliard dataset} For this dataset, the behavior is different: about 25 components are required until a similar behavior as in \cref{fig:pca_ising_ch_confusion} is observed. For all numbers of components, the low-energy regime is predicted significantly better due to the presence of clearly developed patterns in the images. This can also be deducted from \cref{fig:model_PCA_ising_ch}c: the low energy zone is very extended in the latent space, which implies better predictions in this regime. However, there are component pairs that do not follow the clockwise curve, leading to poor predictions for certain images in the low energy zone. The mid and high energy zones are very condensed in the latent space such that minor changes may lead to strongly different predictions, making the model in this region more error-prone.
		
		
		%------------------------------------------------------------------------------
		\subsection{Additional Information for the Multichannel CNN}
		%------------------------------------------------------------------------------
		Fourier transformations are often used in image processing, decomposing an 
		image into its sine and cosine components. After the transformation, the image 
		is represented in the Fourier or frequency domain where only the magnitude
		of the Fourier transform is used.
		%
		For a square image of size $N \times N$, the two-dimensional Discrete Fourier 
		Transform is given by:
		%
		\begin{align}
			\label{eq:fft}
			F(u, v) = \sum_{i=0}^{N-1}\sum_{j=0}^{N-1} f(a, b) \exp^{-i2\pi*(\frac{ui}{N}+\frac{vj}{N})},
		\end{align}
		%
		where $f(a,b)$ is the image in the spatial domain, and the exponential term is the basis function corresponding to each point $F(u,v)$ in the Fourier space. 
		%
		The Discrete Wavelet Transform is given as,
		\begin{align}
			\label{eq:dwt}
			W(u, v) = \sum_{a=0}^{N-1}\sum_{b=0}^{N-1} f(a, b) \phi_{(u,v)}(a, b),
		\end{align}
		where $\phi_{(u,v)}(a, b)$ is the basic wavelet function.
		%
		The Fourier Transform produces a complex number-valued output image which can 
		be displayed with two images, either with the real and imaginary part or with
		magnitude and phase. In image processing, often only the magnitude of the Fourier
		Transform is displayed, as it contains most of the information of the geometric
		structure of the spatial domain image.
		%	In the frequency or Fourier domain, the value and location are represented by sinusoidal relationships that depend upon the frequency of a pixel occurring within an image. In this domain, pixel location is represented by its x- and y-frequencies and its value is represented by an amplitude.
		%
		Basically, the frequency domain represents the rate of change in spatial 
		pixels, which is advantageous when the investigated problem  relates to the rate 
		of change of pixels. 
		%
		%		While the Fourier transform creates a representation of the signal in the
		%		frequency domain, the wavelet transform creates a representation of the signal 
		%		in both the time and frequency domain, thereby allowing efficient access of
		%		localized information about the signal.
		
		
		
		%------------------------------------------------------------------------------
		\subsection{Additional Information for the CNN-only approaches}
		%------------------------------------------------------------------------------
		%% CNN approaches
		Decreasing the dataset size to less than $\approx 10,000$ images 
		results in a more pronounced loss of prediction accuracy for the CNN 
		approaches as compared to the other models, cf.
		\cref{fig:percentage_studying_ots}. 
		%
		\begin{figure}
			\centering
			\includegraphics[width=1.0\textwidth]{percentage_studying_ots.jpg}
			\caption{
				Prediction performance for different CNNs with random initialization 
				of model weights (solid lines) and with pretrained weights (dashed lines) 
				for different sizes of training sets for a) the Ising datasets and  b) the 
				Cahn-Hilliard datasets.
			}
			\label{fig:percentage_studying_ots}
		\end{figure}
		%
		% Using only 20 % training data 
		For the Ising dataset, the training with both weight initialization methods
		shows similar behavior (see \cref{fig:percentage_studying_ots}a). Difference
		occur only if less than $10\%$ of the training data was used. Then the 
		weight initialization from ImageNet results in higher RMSE values. 
		For the  Cahn-Hilliard dataset, a similar trend (with more scatter) 
		can be observed (see \cref{fig:percentage_studying_ots}b).
		%
		For both datasets, we find that training with random initialization of model weights gives a better performance. This is most likely due to the 
		differences in the images used in the present study and the images of the 
		ImageNet dataset used to obtain the pretrained weights. These images require
		different features than the ones learned by the pretrained weights, and this 
		is why we did not find any benefit from the transfer learning approach. 
		The small ResNet18 gives the best results for both datasets. This is not
		entirely surprising as similar results have also been obtained by other researchers where shallow models can provide better results compared to complex and deeper models \citep{bressem2020comparing}. 
		%This was one of the motivation behind finding ways to develop better and smaller models which can provide comparable results \citep{menghani2023efficient,han2015deep}. 
		%
		%Using of MLOps tools like mlflow \cite{zaharia2018accelerating} could be very beneficial in such studies. We have used it to keep track of the various parameters used in the training process.
		
		
		%==============================================================================
		\section{Visualization of confusion matrix from studying various percentages of dataset}
		%==============================================================================
		\label{app:MuChaCNN}
		\label{appendix:confusion_percentage}
		\cref{fig:ising_confusion_percentage} and \cref{fig:ch_confusion_percentage} illustrate the confusion matrices for all models and for different sizes
		of the training datasets. 
		\begin{figure}
			\centering
			\includegraphics[width=1.0\textwidth]{confusion_matrix_Ising_percentage_R2.png}
			\caption[]{
				Confusion matrix of all models for training with different 
				sizes of the Ising dataset. The vertical and horizontal data ranges 
				(temperatures) for each sub-plot are $0..256$. The number inside the sub-plot represents the RMSE value (without brackets) and $R^2$ score (with brackets).}	
			\label{fig:ising_confusion_percentage}
		\end{figure}
		
		\begin{figure}
			\centering
			\includegraphics[width=1.0\textwidth]{confusion_matrix_CH_percentage_R2.png}
			\caption[]{Confusion matrix of all models for training with 
				different sizes of the Cahn-Hilliard dataset. The vertical and 
				horizontal data ranges (energies) for each sub-plot are $0..8000$.
				The number inside the sub-plot represents the RMSE value (without brackets) and $R^2$ score (with brackets).}	
			\label{fig:ch_confusion_percentage}
		\end{figure}
		
		% 	\begin{table}[]
			% 	\centering
			% 	\begin{tabular}{|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|}
				%     \hline
				%     Percentage of train data & grad-PCR & physFeat1-PCR & grad-SVR & physFeat1-SVR & physFeat2-SVR & CNN-PCA-SVR & CNN & MuCha-CNN \\ 
				%     \hline
				%     0.1        &      1.13  &   1.16      &  0.0527 & 0.081     & 0.145   & 0.0004    &  24       & 253      \\ \hline
				%     0.5        &      1.35  &   1.49      & 0.57    & 0.31   & 0.711    &    0.0023    &  46       & 283      \\ \hline
				%     1          &      1.51   &   1.85      &  3.95    & 0.68   & 1.425   &   0.0072    &  133      & 332      \\ \hline
				%     10         &      4.37   &   7.18      &  196.78   & 9.34   & 16.44    &   0.4352   &  1200     & 1145      \\ \hline
				%     20         &      7.01  &   15.32      &  547.13   & 26.17   & 37.92   &  1.8984    &  2100     & 2084     \\ \hline
				%     40         &      12.65  &   28.82      &  1498.8  & 70.28    & 91.67    & 9.7626   &  4230     & 3854     \\ \hline
				%     60         &      17.67  &   37.25      &  2729.01  & 130.70    &  170.91   & 46.2680   &  6630     & 5676     \\ \hline
				%     80         &      22.50  &   47.73      &  4271.78  & 206.69    &  306.21   & 147.9903  &  8430     & 7517     \\ \hline
				%     100        &      27.47   &   59.70      &  5424.57  & 313.59    &  550.67   & 269.5785   &  10440    & 9274     \\ \hline
				%     \end{tabular}
			%     \caption{Training time in second from various approaches on different Ising dataset portion.}
			%     		\label{table:ising_train_time}
			%     \end{table}
		
		%     	\begin{table}[]
			% 	\centering
			% 	\begin{tabular}{|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|}
				%     \hline
				%     Percentage of train data & grad-PCR & physFeat1-PCR & grad-SVR & physFeat1-SVR & physFeat2-SVR & CNN-PCA-SVR & CNN & MuCha-CNN \\ 
				%     \hline
				%     0.1        &       6.59   &   6.54      & 0.06    & 0.081   & 0.188   & 0.0006    &       66  & 250      \\ \hline
				%     0.5        &       19.46   & 20.03        & 1.87   & 0.360    & 1.54   & 0.0091     &       408   & 285      \\ \hline
				%     1          &       19.36  &   21.11      & 2.49   & 0.73    & 5.072   & 0.0674     &      456   & 329      \\ \hline
				%     10         &       35.27  &   38.43      & 115.44   & 14.36    & 402.487   & 2.6055     &      1122  & 1108      \\ \hline
				%     20         &       37.87  &   46.44      & 261.09  & 43.26     & 862.422   & 12.3277     &     1044  & 2246     \\ \hline
				%     40         &       45.05  &  60.18       & 563.42    & 133.68   & 1870.394   &  161.9074     &     2250  & 4224     \\ \hline
				%     60         &       52.16  &  78.52       & 1010.91   & 287.63    & 12895.5   &  514.8271     &     4830  & 6212    \\ \hline
				%     80         &       58.08  &  88.38       & 1446.09   & 480.55    & 43570.755   & 1111.5037     &     6030  & 8215     \\ \hline
				%     100        &       65.10  &  122.73       & 2318.75  & 752.04     & 76148.163   & 1919.8743     &     6630  & 10159     \\ \hline
				%     \end{tabular}
			%     \caption{Training time in second from various approaches on different Cahn-Hilliard dataset portion.}
			%     		\label{table:ch_train_time}
			%     \end{table}
		
		% 	\begin{table}[]
			% 	\centering
			% 	\begin{tabular}{|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|}
				%     \hline
				%     Percentage of train data & grad-PCR & physFeat1-PCR & grad-SVR & physFeat1-SVR & physFeat2-SVR & CNN-PCA-SVR & CNN & MuCha-CNN \\ 
				%     \hline
				%     0.1        &       0.46  &  1.19      & 0.002   & 0.004    & 2.842    &    0.0014 &   54  & 15.3      \\ \hline
				%     0.5        &       0.46  &  1.23       & 0.023  & 0.01     & 2.85    &     0.0227  &   54   & 15.3      \\ \hline
				%     1          &       0.46  &  1.20       & 0.04   & 0.03    & 2.856    &    0.0417   &    54  & 15.3      \\ \hline
				%     10         &       0.46  &  1.21       & 0.44   & 0.34    & 2.935    &    0.1411  &    54  & 15.3      \\ \hline
				%     20         &       0.46  &  1.19       & 0.84   & 0.67    & 3.01    &      0.2355  &    54  & 15.3     \\ \hline
				%     40         &       0.46  &  1.17       & 1.71   & 1.31    & 3.26    &     0.4641  &    54  & 15.3     \\ \hline
				%     60         &       0.46  &  1.20       & 2.57   & 1.93    & 3.66    &     0.7687  &    54  & 15.3    \\ \hline
				%     80         &       0.46  &  1.25       & 3.45   & 2.6    & 3.85   &      0.9839  &   54  & 15.3     \\ \hline
				%     100        &       0.46  &  1.18       & 4.44   & 3.4    & 4.21    &     1.3657  &    54 & 15.3     \\ \hline
				%     \end{tabular}
			%     \caption{Predicting time in second from various approaches. The percentage column is from Ising training dataset portion.}
			%     		\label{table:ising_pred_time}
			%     \end{table}
		
		% 	\begin{table}[]
			% 	\centering
			% 	\begin{tabular}{|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|c{1.5cm}|}
				%     \hline
				%     Percentage of train data & grad-PCR & physFeat1-PCR & grad-SVR & physFeat1-SVR & physFeat2-SVR & CNN-PCA-SVR & CNN & MuCha-CNN \\ 
				%     \hline
				%     0.1        &       2.87  &  7.52       & 0.02   & 0.02    & 20.527    &    0.0372   &  132   & 70      \\ \hline
				%     0.5        &       2.95  &  7.89       & 0.12   & 0.11    & 20.624    &    0.1889   &  132   & 70      \\ \hline
				%     1          &       2.94  &  8.75       & 0.26   & 0.23    & 20.714    &    0.3084   &  132   & 70      \\ \hline
				%     10         &       3.61  &  8.27       & 2.61   & 2.17    & 21.012    &    2.5314   &  132   & 70      \\ \hline
				%     20         &       2.94  &  8.2       & 5.09    & 4.2   & 21.253    &    5.3382         &  132   & 70     \\ \hline
				%     40         &       3.32  &  8.13       & 10.4   & 8.85    & 21.667    &    10.5212         &  132   & 70     \\ \hline
				%     60         &       3.13  &  8.88       & 14.8   & 13.1    & 21.955    &    17.9192         &   132  & 70    \\ \hline
				%     80         &       3.05  &  8.66       & 20.11  & 17.25     & 22.083    &   20.8236          &  132   & 70     \\ \hline
				%     100        &       3.17  &  9.79       & 25.5   & 22.41    & 22.315    &   23.5386          &   132  & 70     \\ \hline
				%     \end{tabular}
			%     \caption{Predicting time in second from various approaches. The percentage column is from Cahn-Hilliard training dataset portion..}
			%     		\label{table:ch_pred_time}
			%     \end{table}
		%==============================================================================
		\section{Comparing our approaches with various regression models from sklearn library package}
		\label{appendix:compare_sklearn}
		\textcolor{red}{
			The performance comparison of our approaches with various regression models from the sklearn library package (Nearest Neighbors, Linear SVM, RBF SVM, Decision Tree, Random Forest, Neural Net, AdaBoost, and Naive Bayes) are shown in \cref{fig:compare_sklearn_rmse} and \cref{fig:compare_sklearn_R2}. It is interesting to see that the performance of the Nearest Neighbors method is quite good for the Cahn-Hilliard dataset (with \gls{RMSE} and $R^2$ score equal to $37$ and $0.996$), which is close to the performance of physFeat1-PCR.
		}
		\begin{figure}
			\centering
			\includegraphics[width=0.8\textwidth]{rmse_bar_plot_16_approaches.png}
			\caption[]{Visualization the \gls{RMSE} of various approaches.}	
			\label{fig:compare_sklearn_rmse}
		\end{figure}
		\begin{figure}
			\centering
			\includegraphics[width=0.8\textwidth]{R2_bar_plot_16_approaches.png}
			\caption[]{Visualization the $R^2$ score of various approaches.}	
			\label{fig:compare_sklearn_R2}
		\end{figure}
		
		\section{Computational fluid dynamics (CFD) dataset: additional information and our approaches performance}
		\label{appendix:CFD_PSD}
		\textcolor{red}{
			%				The kolmogorov dataset is based on large eddies simulation and uses a pseudo-spectral scheme to solve the incompressible Navier-Stoke equations with an external forcing $f$. The simulation software is based on the publication \url{https://www.pnas.org/doi/epdf/10.1073/pnas.2101784118}. The Reynolds number can be expressed via $Re = \frac{\rho u L}{\mu} = \frac{v L}{\nu}$. The kinematic viscosity $\nu$ is set in the range [1:800] with linear spacing and simulated on a staggered grid of unit size and with $128$ grid resolution. The images are obtained by using the curl of the velocity in order to visualize the eddies. a $dt$ of $0.001$ is used and the simulation performs for $10 000$ steps.
			The CFD dataset is obtained based on large eddy simulations of Kolmogorov flows, in which the image results in the strongly ``curled'' velocity field. The Reynolds numbers are chosen from the range of $1..800$. The velocity is obtained by solving the incompressible Navier-Stoke equations. More details about the simulation and the simulation software are described in \citep{kochkov2021machine}.
			Higher values of $R$ result in smaller structures, requiring a higher spatial resolution. This partially explains why learning data for higher values of $R$ is more difficult.
		}
		\textcolor{red}{
			The \gls{PSD} features that are calculated from the images of this dataset are visualized in \cref{fig:CFD_PSD_horiz_64}.
		}
		\begin{figure}
			\centering
			\includegraphics[width=0.5\textwidth]{CFD_PSD_horiz_64.png}
			\caption[]{Microstructure and corresponding \glspl{PSD} for the
				CFD dataset.}	
			\label{fig:CFD_PSD_horiz_64}
		\end{figure}
		
		%		\section{Performance of our approaches with CFD dataset}
		%		\label{appendix:CFD_performance}
		\textcolor{red}{
			The application of our approaches in this work for the CFD datasets is shown in \cref{fig:results_CFD_uniformed}, respectively.
		}
		%		\begin{figure}
			%			\centering
			%			\includegraphics[width=0.9\textwidth]{results_CFD_log.png}
			%			\caption[]{Confusion matrix of our approaches for the imbalanced CFD dataset. The number inside the sub-plot represents the RMSE value (without brackets) and the $R^2$ score (with brackets).}	
			%			\label{fig:results_CFD_log}
			%		\end{figure}
		\begin{figure}
			\centering
			\includegraphics[width=0.9\textwidth]{results_CFD_uniformed.png}
			\caption[]{Confusion matrix of our approaches for the uniformed CFD dataset. The number inside the sub-plot represents the RMSE value (without brackets) and the $R^2$ score (with brackets).}	
			\label{fig:results_CFD_uniformed}
		\end{figure}
		
	\end{appendix}
	%==============================================================================
	% Bibliography
	%==============================================================================
	%	\bibliographystyle{unsrtnat}
	\bibliographystyle{elsarticle-harv}
	%	\bibliographystyle{apalike}
	%	\bibliographystyle{model5-names}
	%	\bibliographystyle{model2-names.bst}\biboptions{authoryear}
	%	\bibliographystyle{elsarticle-num-names}
	%	\biboptions{authoryear}
	\bibliography{literature}
\end{document}