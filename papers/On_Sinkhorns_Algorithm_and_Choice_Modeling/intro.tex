\section{Introduction}
The modeling of choice and ranking data is an important topic across many disciplines. Given a collection of $m$ objects, a universal problem is to aggregate choice or partial ranking data over them to arrive at a reasonable description of either the behavior of decision makers,  the intrinsic qualities of the objects, or both. Work on such problems dates back over a century at least to the work of Landau, who considered $m$ chess players and a record of their match results against one another, aiming to aggregate the pairwise comparisons to arrive at a
global ranking of all players \citep{landau1895relativen,elo1978rating}. More generally, comparison data can result from choices from subsets of \emph{varying} sizes, from partial or complete rankings of objects, or from mixtures of different data types.

The modern rigorous study of comparisons primarily builds on the foundational works of \citet{thurstone1927method} and \citet{zermelo1929berechnung}. Both proposed models based on a numerical ``score'' for each item (e.g., chess player), but with different specifications of choice probabilities.
\citet{zermelo1929berechnung} builds on the intuition that choice probability should be proportional to the score, and proposes a iterative algorithm to estimate the scores from pairwise comparison data. As one of the foundational works in this direction, \citet{luce2012individual}
formalized the multinomial logit model
of discrete choice starting from the axiom of \emph{independence of irrelevant alternatives}
(IIA). It states that the relative likelihood of choosing an item
$j$ over another item $k$ is independent of 
the presence of other alternatives. In other words, if $S$ and $S'$ are two subsets of the $m$
alternatives, both containing $j$ and $k$, and $\Pr(j, S)$ denotes
the probability of choosing item $j$ from $S$, then for $\Pr(k, S)>0$,
\begin{align*}
\frac{\Pr(j, S)}{\Pr(k, S)} & =\frac{\Pr(j, S')}{\Pr(k, S')}.
\end{align*}
This invariance property, together with a condition for zero probability alternatives, are often referred to as Luce's choice axioms. They guarantee that each alternative can be summarized
by a non-negative score $s_{j}$ such that the probability of choice can be parameterized by
\begin{align}
\label{eq:Luce}
\Pr(j, S) & =\frac{s_{j}}{\sum_{k\in S}s_{k}},
\end{align}
 for any set $S$ that contains $j$. The parameters $s_{j}$ reflect
the ``intrinsic'' value of item $i$, and are unique up to a normalization,
which can be set to $\sum_{j}s_{j}=1$. This choice model generalizes the Bradley--Terry--Luce model (BTL) for pairwise comparisons \citep{bradley1952rank}, and also applies to ranking data when each $k$-way ranking is broken down into $k-1$ choice observations, where an item is chosen over the set of items ranked lower \citep{critchlow1991probability,plackett1975analysis,hausman1987specifying}. The many subsequent works that build on Luce's choice axioms speak to its fundamental importance in choice modeling. Other works have also sought to address the limitations of Luce choice models. Prominent among them are probit models \citep{thurstone1927method,berkson1944application}, random utility (RUM) models \citep{mcfadden2000mixed}, context-dependent (CDM) models \citep{batsell1985new,seshadri2020learning}, and behavioral models from psychology \citep{tversky1972elimination}.

Matrix balancing, meanwhile, is a seemingly unrelated mathematical problem. In its most common form (what we study in this paper), the problem seeks positive column and row scalings $D^0,D^1$ of a non-negative matrix $A$, such that the scaled matrix $D^0AD^1$ has row and column sums equal to some prescribed positive marginals $p,q$ \citep{sinkhorn1967diagonal}. Over the years, numerous applications and problems across different domains, including statistics \citep{yule1912methods,deming1940least}, 
economics \citep{stone1962multiple,galichon2021unreasonable}, transportation and networks \citep{kruithof1937telefoonverkeersrekening,lamond1981bregman,chang2021mobility}, optimization \citep{ruiz2001scaling}, and machine learning \citep{cuturi2013sinkhorn,peyre2019computational}, have found themselves essentially solving a new incarnation of the old matrix balancing problem, which attests to its universality and importance. 

A major appeal of the matrix balancing problem lies in the simplicity and elegance of its popular solution method, Sinkhorn's algorithm. It simply alternates between updating the scalings to satisfy one of the two marginal conditions, leading to lightweight implementations that have proved extremely effective for practical problems. Despite this widespread use, the convergence behavior of Sinkhorn's algorithm has still not been fully understood. In particular, while there have been extensive studies of convergence, many assume that the matrix $A$ is (entry-wise) strictly positive. In contrast, \emph{quantitative} analyses for the case when $A$ contains zeros entries are rare and fragmented, employing different assumptions whose connections and distinctions remain unclear. For example, the recent work of \citet{leger2021gradient} establishes a \emph{global} sub-linear $\mathcal{O}(1/t)$ error bound, where $t$ is the number of iterations, under minimal assumptions that guarantee convergence. On the other hand, for square matrix $A$ and uniform marginals $p,q$, \citet{knight2008sinkhorn}
shows that the convergence is \emph{asymptotically} linear whenever finite scalings $D^0,D^1$ exist, i.e., errors $e^{(t)}$ satisfy $e^{(t+1)}/e^{(t)}\rightarrow \lambda$ for some $\lambda \in (0,1)$. It has remained as important open problems to establish quantitative global geometric (linear) convergence with an $\mathcal{O}(\lambda^t)$ error bound, as well as sharp asymptotic bounds for Sinkhorn's algorithm that are applicable to general non-negative $A$ and non-uniform $p,q$. Moreover, it remains to clarify the convergence behavior under different assumptions on the problem structure.

In this paper, we resolve several long-standing convergence questions. Surprisingly, the inspirations for our results come from connections to choice modeling and seemingly independent results in that domain. Our first contribution is in recognizing choice modeling \emph{a la} Luce as yet \emph{another} instance where a central problem reduces to that of matrix balancing. We clarify the equivalence between problem assumptions in each setting. Furthermore, we demonstrate that classic and new algorithms from the choice literature, including those of \citet{zermelo1929berechnung,dykstra1956note,ford1957solution,berry1995automobile,hunter2004mm,maystre2017choicerank,agarwal2018accelerated}, can be viewed as variants of or analogous to Sinkhorn's algorithm.

These intimate mathematical and algorithmic connections allow us to provide a unifying perspective on works from both areas. More importantly, they enable researchers to import insights and tools from one domain to the other. In particular, recent works on choice modeling \citep{shah2015estimation,seshadri2020learning,vojnovic2020convergence} have highlighted the importance of \emph{algebraic connectivity} of data for estimation and computation, which motivates us to consider this measure in the analysis of Sinkhorn's algorithm.

Our next contribution is to obtain a global $\mathcal{O}(\lambda^t)$ linear convergence bound for Sinkhorn's algorithm, whenever the matrix balancing problem has a finite solution pair $D^0,D^1$. Moreover, the convergence rate $\lambda$ depends on the algebraic connectivity of a bipartite graph associated with $A$. This result complements the linear convergence result of \citet{franklin1989scaling} for positive matrices, and the sub-linear bound of \citet{leger2021gradient} for non-negative matrices under weaker conditions. In addition, we also characterize the asymptotic linear rate of convergence in terms of the solution matrix $D^0AD^1$ and target marginals $p,q$, directly generalizing the result of \citet{knight2008sinkhorn}, but with a more explicit analysis that exploits an intrinsic orthogonality structure of Sinkhorn's algorithm. The convergence analyses and results developed in this paper apply irrespective of whether $A$ is strictly positive or not, making them applicable to a wide array of settings. They also clarify the convergence behavior of Sinkhorn's algorithm under two regimes: when a finite scaling exists, Sinkhorn's algorithm always converges linearly; otherwise, it only converges sub-linearly under the minimum conditions required for convergence.

Our convergence results highlight the importance of algebraic connectivity and related spectral properties for the convergence of Sinkhorn's algorithm. At first glance, this dependence may seem unintuitive. However, once we interpret Sinkhorn's algorithm as a distributed optimization algorithm on a bipartite graph (\cref{sec:connections}), it is less surprising that the convergence behavior is governed by the spectral properties of that graph.

The challenges of ill-defined matrix balancing problems and non-convergence of Sinkhorn's algorithm in practice also motivate us to propose a regularized version of Sinkhorn's algorithm. It is inspired by the regularization of Luce choice models using Gamma priors, and is guaranteed to converge even when the necessary condition for the convergence of the standard algorithm does not hold. This regularized Sinkhorn's algorithm could be useful in practice, especially in applications with very sparse data. 

We believe that the connections we establish in this paper between choice modeling and matrix balancing can lead to more interesting results in both directions, and are therefore relevant to researchers from both communities.
 
