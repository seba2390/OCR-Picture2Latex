\section{Linear Convergence of Sinkhorn's Algorithm for Non-negative Matrices}
\label{sec:linear-convergence}
In this section, we turn our focus to matrix balancing and study the global and asymptotic linear convergence rates of Sinkhorn's algorithm for general non-negative matrices and positive marginals. We first discuss relevant quantities and important concepts before presenting the convergence results in \cref{subsec:global-linear-convergence} and \cref{subsec:sharp-rate}.
Throughout, we use superscript $(t)$ to denote quantities after $t$ iterations of Sinkhorn's algorithm.

\subsection{Preliminaries}
\begin{table*}
\caption{Summary of some convergence results on Sinkhorn's algorithm. In \citet{franklin1989scaling}, $\kappa(A)=\frac{\theta(A)^{1/2}-1}{\theta(A)^{1/2}+1}$,
where $\theta(A)$ is the diameter of $A$ in the Hilbert metric.
The norm in \citet{knight2008sinkhorn} is not explicitly specified, and $\sigma_{2}(\hat{A})$
denotes the second largest singular value of the scaled
doubly stochastic matrix $\hat{A}$. The bound in \citet{altschuler2017near} was
originally stated as $\|r^{(t)}-p\|_{1}\protect\leq\epsilon'$
in $t=O(\epsilon'^{-2}\log(\frac{\sum_{ij}A_{ij}}{\min_{ij}A_{ij}}))$
iterations. The result in \citet{leger2021gradient} applies more generally to couplings
of probability distributions. In view of Pinsker's inequality, it implies the bound in \citet{altschuler2017near} but with a constant that is finite even when $A$ has zero entries. In the bound in \citet{knight2008sinkhorn} and our
asymptotic result, the $\lambda+\epsilon$ denotes an asymptotic rate, with the
bound valid for any $\epsilon>0$ and all $t$ sufficiently large. In our global bound, the linear rate $\lambda_{-2}(\mathcal{L})$ is the second \emph{smallest} eigenvalue of the Laplacian of the bipartite graph defined by $A$ (see \cref{subsec:graph-laplacian}), $ l=\min \{\max_j (A^T\mathbf{1}_n)_j, \max_i (A\mathbf{1}_m)_i\}$,
$c_B=\exp(-4B)$, and $B$ is a bound on the initial sub-level set, which is finite if and only if \cref{ass:matrix-existence} holds.
}
 \begin{adjustwidth}{-1.5cm}{}
\begin{centering}
\begin{tabular}{c|c|c|c|c}
 & convergence statement & $\lambda$ & $A$ & $p,q$\tabularnewline
\hline 
\citet{franklin1989scaling} & $d_{\text{Hilbert}}(r^{(t)},p)\leq\lambda^t d_{\text{Hilbert}}(r^{(0)},p)$ & $\kappa^{2}(A)$ & $A>0$, rectangular & uniform\tabularnewline
\hline 
\citet{luo1992convergence} & $g(u^{(t)},v^{(t)})-g^\ast\leq\lambda^t (g(u^{(0)},v^{(0)})-g^\ast)$ & \text{unknown} & $A\geq0$, rectangular & general\tabularnewline
\hline 
\citet{knight2008sinkhorn} & $\|D_{t+1}^{0}-D^{0}\|_{\ast}\leq(\lambda+\epsilon)\|D_{t}^{0}-D^{0}\|_{\ast}$ & $\sigma_{2}^{2}(\hat{A})$ & $A\geq0$, square  & uniform\tabularnewline
\hline 
\citet{pukelsheim2009iterative} & $\|r^{(t)}-p\|_{1}\rightarrow0$ & no rate & $A\geq0$, rectangular & general\tabularnewline
\hline 
\citet{altschuler2017near} & $\|r^{(t)}-p\|_{1}\leq c \sqrt{\frac{\lambda}{t}}$ & $\log(\frac{\sum_{ij}A_{ij}}{\min_{ij}A_{ij}})$ & $A>0$, rectangular & general\tabularnewline
\hline 
\citet{leger2021gradient} & $D_{\text{KL}}(r^{(t)}\| p) \leq\frac{\lambda}{t}$ & $D_{\text{KL}}(\hat{A}\| A)$ & $A\geq0$, continuous & general\tabularnewline
\hline 
current work, asymptotic & $\|\frac{r^{(t+1)}}{\sqrt{p}}-\sqrt{p}\|_{2}\leq(\lambda+\epsilon)\|\frac{r^{(t)}}{\sqrt{p}}-\sqrt{p}\|_{2}$ & $\lambda_{2}(\tilde{A}^T\tilde{A})$ & $A\geq0$, rectangular & general\tabularnewline
\hline 
current work, global & $g(u^{(t)},v^{(t)})-g^\ast\leq\lambda^t (g(u^{(0)},v^{(0)})-g^\ast)$ & $1-c_B\lambda_{-2}(\mathcal{L})/l$ & $A\geq0$, rectangular & general\tabularnewline
\end{tabular}
\par\end{centering}
\label{tab:convergence-summary}
\end{adjustwidth}
\end{table*}
We start with the optimization principles associated with matrix balancing and Sinkhorn's algorithm. Consider the following KL divergence (relative entropy) minimization problem 
     \begin{align}
\label{eq:relative-entropy-minimization}
\begin{split}
  \min_{\hat A\in \mathbb{R}^{n\times m}_+} & D_{\text{KL}}(\hat{A}\| A)\\
\hat{A}\mathbf{1}_m & =p\\
\hat{A}^{T}\mathbf{1}_n & =q.
\end{split}
\end{align}
It is well-known that solutions $\hat{A}=D^1AD^0$ to the matrix balancing problem with $(A,p,q)$ are minimizers of \eqref{eq:relative-entropy-minimization} \citep{ireland1968contingency,bregman1967proof}. Moreover, Sinkhorn's algorithm can be interpreted as a block coordinate descent type algorithm applied to minimize the following dual problem of \eqref{eq:relative-entropy-minimization}:
 \begin{align}
 \label{eq:log-barrier}
     g(d^0,d^1)	:=(d^1)^{T}Ad^0-\sum_{i=1}^{n}p_{i}\log d^1_{i}-\sum_{j=1}^{m}q_{j}\log d^0_{j},
\end{align} 
\citet{luo1992convergence} study the linear convergence of block coordinate descent algorithms. Their result implies that the
convergence of Sinkhorn's algorithm, measured in terms of the optimality gap of $g$, is linear with some implicit rate $\lambda>0$,
 as long as finite positive scalings $D^0,D^1$ exist for the matrix balancing problem. Minimizers $d^0,d^1$ of \eqref{eq:log-barrier} precisely give the diagonals of $D^0,D^1$. The function $g$, known to be a \emph{potential function} of Sinkhorn's algorithm, also turns out to be crucial in quantifying the global linear convergence rate in the present work. 

\textbf{Remark.} Interestingly, minimizing \eqref{eq:log-barrier} is in fact equivalent to maximizing the log-likelihood function $\ell(s)$ in \eqref{eq:log-likelihood} for valid $(A,p,q)$, because $\min_{d^1}g(d^0,d^1)=-\ell(d^0)+c$ for some $c>0$. Moreover, the optimality condition of minimizing $g$ with respect to $d^0$ reduces to the optimality condition \eqref{eq:optimality}. A detailed discussion can be found in \cref{sec:sinkhorn-MM}. This connection relates choice modeling and matrix balancing from an optimization perspective.
     
      Although convergence results on Sinkhorn's algorithm are abundant, the recent work of \citet{leger2021gradient} stands out as the first \emph{explicit} global convergence result applicable to general non-negative matrices, with a sub-linear $\mathcal O(1/t)$ bound on the KL divergence with respect to target marginals. It implies the bounds in \citet{chakrabarty2021better,altschuler2017near} but with a constant that is finite even when $A$ has zero entries. The result in \citet{leger2021gradient} applies more generally to couplings of continuous probability distributions, but when restricted to the discrete matrix balancing problem, it 
     holds under the following equivalent conditions that are weaker than \cref{ass:matrix-existence}.
\begin{assumption}[\textbf{Weak Existence}]
    \label{ass:matrix-weak-existence}
    
    \textbf{(a)} There exists a non-negative matrix $A'\in \mathbb{R}_+^{n\times m}$ that inherits all zeros of $A$ and has row and column sums $p$ and $q$. Or, equivalently,
    
\textbf{(b)} For every pair of sets of indices $N \subsetneq [n]$ and $M \subsetneq [m]$ such that $A_{ij}=0$ for $i\notin N$ and $j\in M$, $\sum_{i\in N}p_i \geq \sum_{j\in M}q_j$.
\end{assumption}

The equivalence of these conditions follows from Theorem 4 in \citet{pukelsheim2009iterative}, which also shows that they are the minimal requirements for the convergence of Sinkhorn's algorithm. \cref{ass:matrix-weak-existence}(a) precisely guarantees 
that the optimization problem \eqref{eq:relative-entropy-minimization} is feasible and bounded. It relaxes \cref{ass:matrix-existence}(a) by allowing additional zeros in the matrix $A'$. Similarly, \cref{ass:matrix-weak-existence}(b) relaxes \cref{ass:matrix-existence}(b) by allowing equality between $\sum_{i\in N}p_i$ and $\sum_{j\in M}q_j$ even when $M,N$ do not correspond to a block-diagonal structure. 

The distinction between \cref{ass:matrix-existence} and \cref{ass:matrix-weak-existence} is crucial for the matrix balancing problem and Sinkhorn's algorithm. Recall that \cref{ass:matrix-existence} guarantees the matrix balancing problem has a solution $(D^0,D^1)$, and $D^1AD^0$ is always a solution to \eqref{eq:relative-entropy-minimization}. On the other hand, the weaker condition \cref{ass:matrix-weak-existence} guarantees that \eqref{eq:relative-entropy-minimization} has a solution $\hat A$.
If indeed $\hat A$ has additional zeros relative to $A$, then no direct (finite and positive) scaling $(D^{0},D^{1})$ exists such that $\hat A=D^1AD^0$. However, the sequence of scaled matrices $\hat A^{(t)}$ from Sinkhorn's algorithm still converges to $\hat A$. In this case, the matrix balancing problem is said to have a \emph{limit} scaling, where some entries of $d^{0},d^{1}$ in Sinkhorn iterations approach 0 or $\infty$, resulting in additional zeros in $\hat A$. Below we give an example adapted from \citet{pukelsheim2009iterative}, where $p,q=(3,3)$ and the scaled matrices $\hat A^{(t)}$ converge but no direct scaling exists:
\begin{align*}
{D^{1}}^{(t)}\begin{bmatrix}3&1\\
0 & 2
\end{bmatrix}{D^{0}}^{(t)} = \begin{bmatrix}1&0\\
0 & \frac{3t}{2}
\end{bmatrix}\begin{bmatrix}3&1\\
0 & 2
\end{bmatrix}\begin{bmatrix}1&0\\
0 & \frac{1}{t}
\end{bmatrix}	\rightarrow \begin{bmatrix}3&0\\
0 & 3
\end{bmatrix}.
\end{align*}

Given these discussions, it is therefore important to clarify the convergence behaviors of Sinkhorn's algorithm in different regimes. In particular, it remains to reconcile the gap between the implicit linear convergence result of \citet{luo1992convergence} under strong existence, and the quantitative sub-linear bound of \citet{leger2021gradient} under weak existence. Furthermore, it remains to provide explicit characterizations of both the global and asymptotic (local) rates when Sinkhorn's algorithm does converge linearly. 

Our results in this section provide answers to these questions. We show that the $\mathcal O(1/t)$ rate can be sharpened to a global $\mathcal O(\lambda^t)$ bound if and only if the weak existence condition (\cref{ass:matrix-weak-existence}) is replaced by the strong existence condition (\cref{ass:matrix-existence}). Moreover, we provide an explicit global linear convergence rate $\lambda$ in terms of the \emph{algebraic connectivity}, revealing the structure-dependent nature of Sinkhorn's algorithm for problems with non-negative matrices. This generalizes the implicit result of \citet{luo1992convergence} and sheds light on how different assumptions impact Sinkhorn's convergence, which is explicitly reflected in the constants of the bound. Going further, we characterize the sharp asymptotic rate of linear convergence in terms of the second largest singular value of $\mathcal{D}(1/\sqrt{p})\cdot\hat{A}\cdot\mathcal{D}(1/\sqrt{q})$, where $\mathcal{D}$ denotes the diagonalization of a vector. This asymptotic rate reduces to that given by \citet{knight2008sinkhorn} for $m=n$ and uniform $p,q$.

The choice of convergence measure is important, and previous works have used different convergence measures. First note that after each iteration in \cref{alg:scaling}, the column constraint is always satisfied: ${A}^{(t)}\mathbf{1}_n=q$, where ${A}^{(t)}$ is the scaled matrix after $t$ iterations. 
\citet{leger2021gradient} uses the KL divergence $D_{\text{KL}}(r^{(t)}\| p)$ between the row sum $r^{(t)}={A}^{(t)}\mathbf{1}_m$ and the target row sum $p$ to measure convergence. \citet{franklin1989scaling} use the Hilbert projective metric between $r^{(t)}$ and $p$. \citet{pukelsheim2009iterative} and \citet{altschuler2017near} use the $\ell^1$ distance, which is upper bounded by the KL divergence via Pinsker's inequality. \citet{knight2008sinkhorn} focuses on the convergence of the scaling diagonal matrix $D^0=\mathcal{D}(d^0)$ to the optimal solution \emph{line}, but does not explicitly specify the norm.
Some bounds are \emph{a priori} and hold globally for all iterations, while others hold locally in a neighborhood of the optimum.
We summarize the relevant convergence results in \cref{tab:convergence-summary}. Here $\lambda_{-2}(S)$ denotes the second smallest eigenvalue of a real symmetric matrix $S$, and $\lambda_{2}(S)$ the second largest eigenvalue. In our work, we characterize the global linear convergence through the optimality gap of \eqref{eq:log-barrier}, which naturally leads to a bound on $\|r^{(t)}-p\|_1$. For the sharp asymptotic rate, we choose to use the $\ell^2$ distance $\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_2$ in order to exploit an intrinsic orthogonality structure afforded by Sinkhorn's algorithm. This approach results in a novel analysis compared to \citet{knight2008sinkhorn} that most explicitly reveals the importance of spectral properties in  the rate of convergence.

\subsection{Global Linear Convergence}
\label{subsec:global-linear-convergence}
We first present the global linear convergence results. Our analysis starts with the following change of variables to transform the potential function \eqref{eq:log-barrier}:
\begin{align}
\label{eq:change-of-variables}
    u:=\log d^0,\quad v:=-\log d^1.
\end{align}
This results in the potential function $g(u,v)$ defined as
\begin{align}
\label{eq:transformed-potential}
  g(u,v)	:=\sum_{ij}A_{ij}e^{-v_{i}+u_{j}}+\sum_{i=1}^{n}p_{i}v_{i}-\sum_{j=1}^{m}q_{j}u_{j},
\end{align}
and we can verify that Sinkhorn's algorithm is equivalent to the alternating minimization algorithm \citep{bertsekas1997nonlinear,beck2013convergence} for \eqref{eq:transformed-potential}, which alternates between minimizing with respect to $u$ and $v$, holding the other block fixed:
\begin{align}
   \label{eq:alternating-minimization} u_j^{(t)}\leftarrow  \log \frac{q_j}{\sum_i A_{ij}e^{-v^{(t-1)}_i}},\quad v_i^{(t)}\leftarrow  \log \frac{p_i}{\sum_j A_{ij}e^{u^{(t)}_j}}.
\end{align}
The Hessian $\nabla^2g(u,v)$ always has $\mathbf{1}_{m+n}$ in its null space. On the surface, standard linear convergence results for first-order methods, which require strong convexity (or related properties like the Polyak--Lojasiewicz condition) of the objective function, do not apply to $g(u,v)$. However, we show that under strong existence and uniqueness conditions for the matrix balancing problem, $g(u,v)$ is in fact strongly convex when \emph{restricted} to the subspace 
\begin{align*}
   \mathbf{1}_{m+n}^\perp:= \{u\in \mathbb{R}^m,v\in \mathbb{R}^n:(u,v)^T\mathbf{1}_{m+n}=0\}.
\end{align*}
As a result, Sinkhorn's algorithm converges linearly with a rate that depends on the (restricted) condition number of its Hessian.

Before proceeding, we introduce a slew of useful definitions. Let Sinkhorn's algorithm initialize with $u^{(0)}$, and $v^{(0)}$ given by \eqref{eq:alternating-minimization}. 
Define the constant $B$ as 
\begin{align*}
   B:&= \sup_{(u,v)} \|(u,v)\|_\infty\\  \text{subject to } (u,v)^{T}&\mathbf{1}_{m+n}=0,\\
   g(u,v)&\leq g(u^{(0)},v^{(0)}).
\end{align*}
In other words, $B$ is the \emph{diameter} of the initial normalized sub-level set. We will show that $B$ is finite and that it bounds normalized Sinkhorn iterates by the \emph{coercivity} of $g(u,v)$ under \cref{ass:matrix-existence}.  
We similarly define the normalized optimal solution pair 
\begin{align}
\label{eq:normalized-optimum}
    (u^\ast,v^\ast):=\arg \min_{(u,v)\in \mathbf{1}_{m+n}^\perp} g(u,v),
\end{align}
and $g^\ast:=g(u^\ast,v^\ast)$.
Finally, define 
\begin{align*}
    l_0:= \max_j (A^T\mathbf{1}_n)_j, \quad l_1:= \max_i (A\mathbf{1}_m)_i,
\end{align*}
which are the Lipschitz constants of the two sub-blocks of $g(u,v)$. Next, define the \emph{Laplcian} matrix $\mathcal{L}$ of the bipartite graph $G_b$ as 
\begin{align*}
\mathcal{L}:&=\begin{bmatrix}\mathcal{D}({A}\mathbf{1}_{m}) & -{A}\\
-{A}^{T} & \mathcal{D}({A}^{T}\mathbf{1}_{n})
\end{bmatrix}, 
\end{align*}
and refer to the second smallest eigenvalue $\lambda_{-2}(\mathcal{L})$ as the Fiedler eigenvalue. For details on the graph Laplacian and the Fielder eigenvalue, see \cref{subsec:graph-laplacian}.

Using the above notation, we can now state one of our main contributions to the study of Sinkhorn's algorithm. 
\begin{theorem}[\textbf{Global Linear Convergence}]
\label{thm:global-convergence}
Suppose \cref{ass:matrix-existence} and \cref{ass:matrix-uniqueness} hold. For all $t>0$,
\begin{align}
\label{eq:potential convergence}
g(u^{(t+1)},v^{(t+1)}) - g^\ast \leq (1-c_B \frac{\lambda_{-2}(\mathcal{L})} {l})\left( g(u^{(t)},v^{(t)})-g^\ast \right),
    \end{align}
    where $c_B=e^{-4B}$ and $l=\min\{l_0,l_1\}$.
      
    As a consequence, we have the following bound:
    \begin{align*}
       \|r^{(t)}-p\|_{1} &\leq c'_Be^{-c_{B}\frac{\lambda_{-2}(\mathcal{L})}{\min\{l_{0},l_{1}\}}\cdot t},
    \end{align*} 
    where $(c'_B)^2=(8B\sum_{i}p_{i})$.
\end{theorem}
\cref{thm:global-convergence} immediately implies the following iteration complexity bound.
\begin{corollary}[\textbf{Iteration Complexity}]
Under \cref{ass:matrix-existence} and \cref{ass:matrix-uniqueness}, $\|r^{(t)}-p\|_1\leq \epsilon$ after
      \begin{align*}
          \mathcal{O} \left (\frac{\min\{l_{0},l_{1}\}}{\lambda_{-2}(\mathcal{L})} \cdot \log (1/\epsilon) \right )
      \end{align*}
       iterations of Sinkhorn's algorithm.
\end{corollary}

\textbf{Remark.} The ability of Sinkhorn's algorithm to exploit the strong convexity of $g(u,v)$ on $\mathbf{1}_{m+n}^\perp$ relies critically on the invariance of $g(u,v)$ under \emph{normalization}, which is an intrinsic feature of the problem that has been largely set aside in the convergence analysis so far. Recall that $u=\log d^0$ and $v=-\log d^1$, where $d^0,d^1$ are the diagonals of the scaling $(D^0,D^1)$. Scalings are only determined up to multiplication by $(1/c,c)$ for $c>0$, and the translation $(u,v)\rightarrow(u-\log c,v-\log c)$ does not alter the objective value in \eqref{eq:transformed-potential}. We may therefore impose an \emph{auxiliary} normalization $(u,v)^T\mathbf{1}_{m+n}=0$, or equivalently $\prod_j d^0_j = \prod_i d^1_i$. This normalization is easily achieved by requiring that after every update of Sinkhorn's algorithm, a normalization $(d^0/c,c d^1)$ is performed using the normalizing constant 
\begin{align}
\label{eq:normalization}
   c=\sqrt{\prod_j d^0_j /\prod_i d^1_i}.
\end{align}
See \cref{alg:scaling-normalized}. Note, however, that this normalization is only a supplementary construction in our analysis. The final convergence result applies to the original Sinkhorn's algorithm without normalization, since it does not alter the objective value. Normalization of Sinkhorn's algorithm is discussed in \citet{carlier2023sista}, although they use the asymmetric condition $u_0=0$, which does not guarantee that normalized Sinkhorn iterates stay in $\mathbf{1}_{m+n}^\perp$. 
\begin{algorithm}[tb]
\caption{Normalized Sinkhorn's Algorithm}
   \label{alg:scaling-normalized}
\begin{algorithmic}
   \STATE {\bfseries Input:}  $A, p, q,\epsilon_{\text{tol}}$.
   \STATE {\bfseries initialize} $d^{0}\in\mathbb{R}_{++}^{m}$
   \REPEAT
   \STATE $d^{1} \leftarrow  p/( A d^0)$ 
   \STATE 
  normalization  $(d^0,d^1) \leftarrow (d^0/c,c d^1),c>0$
   \STATE $d^{0}\leftarrow  q/({A}^{T} d^{1})$
   \STATE 
  normalization  $(d^0,d^1) \leftarrow (d^0/c,c d^1),c>0$ 
   \STATE 
$\epsilon\leftarrow$  update of $(d^{0},d^1)$
\UNTIL{$\epsilon<\epsilon_{\text{tol}}$}
\end{algorithmic}
\end{algorithm}

The proof of \cref{thm:global-convergence} then relies crucially on the observation that the Hessian of $g(u,v)$ at $(0,0)$ is precisely the \emph{Laplacian} $\mathcal{L}$ of the bipartite graph $G_b$. Therefore, as $(u,v)$ are \emph{bounded} throughout the iterations thanks to the coercivity of $g$, the Fiedler eigenvalue of $\mathcal{L}$ quantifies the strong convexity on $\mathbf{1}_{m+n}^\perp$. The linear convergence then follows from standard results on block coordinate descent and alternating minimization methods for strongly convex and smooth functions \citep{beck2013convergence}. Typically, the leading eigenvalue of the Hessian quantifies the smoothness \citep{luenberger1984linear}. This is given by $2\max\{l_0,l_1\}$ for $\mathcal{L}$. However, for alternating minimization methods, the better smoothness constant $\min\{l_0,l_1\}$ is available. Thus the quantity $\min\{l_{0},l_{1}\}/\lambda_{-2}(\mathcal{L})$ can be interpreted as a type of  ``condition number'' of the graph Laplacian $\mathcal{L}$. When $A$ is positive (not just non-negative), then the strong existence and uniqueness conditions are trivially satisfied, and our results continue to hold with the rate quantified by $\min\{l_{0},l_{1}\}/\lambda_{-2}(\mathcal{L})$.

\textbf{Remark.} The importance of Assumptions \ref{ass:matrix-existence} and \ref{ass:matrix-uniqueness} are clearly reflected in the bound \eqref{eq:potential convergence}.
First, note that the Fiedler eigenvalue $\lambda_{-2}(\mathcal{L})>0$ iff \cref{ass:matrix-uniqueness} holds (see \cref{subsec:graph-laplacian}). On the other hand, \cref{ass:matrix-existence} guarantees the \emph{coercivity} of $g$ on $\mathbf{1}_{m+n}^\perp$. This property ensures that $B<\infty$, and consequently, that normalized iterates stay bounded by $B$. That \cref{ass:matrix-existence} guarantees  $g(u,v)$ is coercive should be compared to the observation by \citet{hunter2004mm} that \cref{ass:strong-connected} guarantees the upper compactness (a closely related concept) of the log-likelihood function \eqref{eq:log-likelihood}.
In contrast, when only the weak existence condition (\cref{ass:matrix-weak-existence}) holds, finite minimizer of $g(u,v)$ may not exist, in which case the diameter $B$ of the initial sub-level set may become infinite.

\cref{ass:matrix-weak-existence} corresponds to the ``limit scaling'' regime of Sinkhorn's algorithm, where the scaled matrices $A^{(t)}$ are guaranteed to converge to a finite matrix $\hat A$ with the desired marginal distributions that solves \eqref{eq:relative-entropy-minimization}, and may have additional zeros compared to $A$. Under \cref{ass:matrix-weak-existence}, \citet{leger2021gradient} shows the slower $\mathcal O(1/t)$ convergence in KL divergence $D_{\text{KL}}(r^{(t)}\| p)$. We now show that this rate is tight, which fully characterizes the following convergence behavior of Sinkhorn's algorithm: whenever a direct scaling exists for the matrix balancing problem, Sinkhorn's algorithm converges linearly. If only a limit scaling exists, then convergence deteriorates to 
$\mathcal O(1/t)$.
\begin{theorem}
\label{thm:lower-bound}
       For general non-negative matrices, Sinkhorn's algorithm converges linearly
iff $(A,p,q)$ satisfy \cref{ass:matrix-existence} and \cref{ass:matrix-uniqueness}. The convergence deteriorates to sub-linear iff the weak existence condition \cref{ass:matrix-weak-existence} holds but \cref{ass:matrix-existence} fails.
\end{theorem}

The regime of sub-linear convergence also has an interpretation in the choice modeling framework. The weak existence condition \cref{ass:matrix-weak-existence}, when applied to $(A,p,q)$ constructed from a choice dataset, allows the case where some subset $S$ of items is always preferred over $S^C$, which implies, as observed already by the early work of \citet{ford1957solution}, that the log-likelihood function \eqref{eq:log-likelihood} is only maximized at the \emph{boundary} of the probability simplex, by shrinking  $s_j$ for $j\in S^C$ towards 0, i.e., $D^0_j \rightarrow 0$. Incidentally, \citet{bacharach1965estimating} also refers to the corresponding regime in matrix balancing as ``boundary solutions''. 

\subsection{Sharp Asymptotic Rate}
\label{subsec:sharp-rate}
Having established the global convergence of Sinkhorn's algorithm when finite scalings exist, we now turn to the open problem of characterizing its asymptotic linear convergence rate for non-uniform marginals. Our analysis relies on an \emph{intrinsic} orthogonality structure of Sinkhorn's algorithm instead of the auxiliary  normalization used to prove the global linear convergence above. Note that unlike the global rate, which depends on $A$, the asymptotic rate now depends on the associated solution $\hat A$ (and $p,q$), as expected.  
\begin{theorem}[\textbf{Sharp Asymptotic Rate}]
\label{thm:convergence}
Suppose $(A,p,q)$ satisfy \cref{ass:matrix-existence} and \cref{ass:matrix-uniqueness}. Let $\hat{A}$
be the unique scaled matrix with marginals $p,q$. Then
\begin{align*}
\lim_{t\rightarrow \infty} \frac{\|r^{(t+1)}/\sqrt{p}-\sqrt{p}\|_{2} }{\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_{2}} = \lambda_\infty,
\end{align*}
where the asymptotic linear rate of convergence $\lambda_\infty$ is
\begin{align*}
\lambda_\infty & :=\lambda_{2}(\tilde{A}\tilde{A}^{T})=\lambda_{2}(\tilde{A}^{T}\tilde{A})\\
\tilde{A} & :=\mathcal{D}(1/\sqrt{p})\cdot\hat{A}\cdot\mathcal{D}(1/\sqrt{q}),
\end{align*}
where $\lambda_{2}(\cdot)$ denotes the second largest eigenvalue. 
\end{theorem}

Intuitively, the dependence of the linear rate of convergence on the second largest eigenvalue of $\tilde{A}^T\tilde{A}$ (and $\tilde{A}\tilde{A}^T$) is due to the fact that near the optimum $\sqrt{p}$, $\tilde{A}\tilde{A}^T$ (which is the Jacobian at $\sqrt{p}$) approximates the first order change in $r^{(t)}/\sqrt{p}$. Normally, the \emph{leading} eigenvalue quantifies this change. The unique leading eigenvalue of $\tilde{A}\tilde{A}^T$ is equal to 1 with eigenvector $\sqrt{p}$, which does not imply contraction. Fortunately, using the quantity  $r^{(t)}/\sqrt{p}$
allows us to exploit the following orthogonality structure:
\begin{align*}
(r^{(t)}/\sqrt{p}-\sqrt{p})^{T}\sqrt{p} & =\sum_{i}(r_{i}^{(t)}-p_{i})=0
\end{align*}
by virtue of Sinkhorn's algorithm preserving the quantities $r^{(t)T}\mathbf{1}_{n}$
for all $t$. Thus, the residual $r^{(t)}/\sqrt{p}-\sqrt{p}$ is always \emph{orthogonal} to
$\sqrt{p}$, which is both the leading eigenvector and the fixed point of the iteration. The convergence is then controlled by the \emph{second} largest eigenvalue of $\tilde{A}\tilde{A}^T$. This proof approach echoes that of the global linear convergence result in \cref{thm:global-convergence}, where we also exploit an orthogonality condition to obtain a meaningful bound. In \cref{thm:global-convergence} the bound depends on the second smallest eigenvalue of the Hessian, while in \cref{thm:convergence} the bound depends on the second largest eigenvalue of the Jacobian. 

In the special case of $m=n$ and $p=q=\mathbf{1}$, the asymptotic rate in \cref{thm:convergence} reduces to that in \citet{knight2008sinkhorn}. Note, however, that the convergence metric is different: we use the $\ell^2$ norm $\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_2$ while \citet{knight2008sinkhorn} uses an \emph{implicit} norm that measures the convergence of $D^0$ to the solution \emph{line} due to scale invariance. Our analysis exploits the orthogonality structure of Sinkhorn's algorithm and more explicitly reveals the dependence of the convergence rate on the spectral structure of the data. 

Our results in this section are relevant in several respects. First, we clarify the gap between the $\mathcal O(1/t)$ and $\mathcal O(\lambda^t)$ convergence of Sinkhorn's algorithm: the slowdown happens if and only if Sinkhorn's algorithm converges but the canonical matrix balancing problem does not have a \emph{finite} scaling $(D^0,D^1)$. This slowdown has been observed in the literature but not systematically studied. Second, 
 we settle open problems and establish the first quantitative global linear convergence result for Sinkhorn's algorithm applied to general non-negative matrices. We also characterize the asymptotic linear rate of convergence, generalizing the result of \citet{knight2008sinkhorn} but with a novel analysis. 
  Third, our analysis reveals the importance of algebraic connectivity for the convergence of Sinkhorn's algorithm. Although an important quantity in the choice modeling literature, algebraic connectivity has not been previously used to in the analysis of Sinkhorn's algorithm. The importance of algebraic connectivity for Sinkhorn's algorithm becomes less surprising once we connect it to the distributed optimization literature in \cref{sec:connections}, where it is well-known that the spectral gap of the \emph{gossip} matrix, which defines the decentralized communication network, governs the rates of convergence.