\section{Applications of Matrix Balancing}
\label{app:related-works}
This section contains a brief survey on the applications of matrix balancing in a diverse range of disciplines.

\textbf{Traffic and Transportation Networks.} These applications are some of the earliest and most popular uses of matrix balancing. \citet{kruithof1937telefoonverkeersrekening} considered the problem of estimating new telephone traffic patterns among telephone exchanges given existing traffic volumes and marginal densities of departing and terminating traffic for each exchange when their subscribers are updated. A closely related problem in transportation networks is to use observed \emph{total} traffic flows out of each origin and into each destination to estimate \emph{detailed} traffics between origin-destination pairs \citep{carey1981method,nguyen1984estimating,sheffi1985urban,chang2021mobility}. The key idea is to find a traffic assignment satisfying the total flow constraints that is ``close'' to some known reference traffic pattern. The resulting (relative) entropy minimization principle, detailed in \eqref{eq:relative-entropy-minimization}, is an important optimization perspective on Sinkhorn's algorithm.

\textbf{Demography.} A problem similar to that in networks arises in demography. Given out-of-date inter-regional migration statistics and up-to-date net migrations from and into each region, the task is to estimate migration flows that are consistent with the marginal statistics \citep{plane1982information}.

\textbf{Economics.}
General equilibrium models in economics employ \emph{social accounting matrices}, which record the flow of funds between important (aggregate) agents in an economy at different points in time \citep{stone1962multiple,pyatt1985social}. Often accurate data is available on the total expenditure and receipts for each agent, but due to survey error or latency, detailed flows are not always consistent with these marginal statistics. Thus they need to be ``adjusted'' to satisfy consistency requirements.  Other important applications of the matrix balancing problem in economics include the estimation of gravity equations in inter-regional and international trade \citep{uribe1966information,wilson1969use,anderson2003gravity,silva2006log} and coefficient matrices in input-output models \citep{leontief1965structure,stone1971computable,bacharach1970biproportional}. In recent years, optimal transport \citep{villani2009optimal} has found great success in economics \citep{carlier2016vector,galichon2018optimal,galichon2021unreasonable,galichon2021matching}. As matrix balancing and Sinkhorn's algorithm are closely connected to optimal transport (\cref{sec:linear-convergence}), they are likely to have more applications in economics.

\textbf{Statistics.}
A contingency table encodes frequencies of subgroups of populations, where the rows and columns correspond to values of two categorical variables, such as gender and age. Similar to social accounting matrices, a common problem is to adjust out-of-date or inaccurate cell values of a table given accurate marginal frequencies. The problem is first studied by  \citet{deming1940least}, who proposed the classic iterative algorithm. \citet{ireland1968contingency} formalized its underlying entropy optimization principle, and \citet{fienberg1970iterative} analyzed its convergence. 

\textbf{Optimization and Machine Learning.} Matrix balancing plays a different but equally important role in optimization. Given a linear system $Ax=b$ with non-singular $A$, it is well-known that the convergence of first order solution methods depends on the \emph{condition number} of $A$, and an important problem is to find diagonal \emph{preconditioners} $D^1,D^0$ such that $D^1AD^0$ has smaller condition number. Although it is possible to find optimal diagonal preconditioners via semidefinite programming \citep{boyd1994linear,qu2022optimal}, matrix balancing methods remain very attractive heuristics due to their low computational costs, and continue to be an important component of modern workhorse optimization solvers \citep{ruiz2001scaling,bradley2010algorithms,knight2013fast,stellato2020osqp,gao2022hdsdp}.

In recent years, optimal transport distances have become an important tool in machine learning and optimization for measuring the similarity between probability distributions \citep{arjovsky2017wasserstein,peyre2019computational,blanchet2019robust,mohajerin2018data,kuhn2019wasserstein}. Besides appealing theoretical properties, efficient methods to approximate them in practice have also contributed to their wide adoption. This is achieved through an entropic regularization of the OT problem, which is precisely equivalent to the matrix balancing problem and solved via Sinkhorn's algorithm \citep{cuturi2013sinkhorn,altschuler2017near,dvurechensky2018computational}.   

\textbf{Political Representation.} The apportionment of representation seats based on election results has found unexpected solution in matrix balancing. A standard example consists of the matrix recording the votes each party received from different regions. The marginal constraints are that each party's total number of seats be proportional to the number of votes they receive, and similar for each region. A distinct feature of this problem is that the final apportionment matrix must have integer values, and variants of the standard algorithm that incorporate \emph{rounding} have been proposed \citep{balinski2006matrices,pukelsheim2006current,maier2010divisor}. More than just mathematical gadgets, they have found real-world implementations in Swiss cities such as Zurich \citep{pukelsheim2009iterative}.

\textbf{Markov Chains.}
Last but not least, Markov chains and related topics offer another rich set of applications for matrix balancing. \citet{schro1931uber} considered a continuous version of the following problem. Given a ``prior'' transition matrix $A$ of a Markov chain and \emph{observed} distributions $p^0,p^1$ before and after the transition, find the most probable transition matrix (or path) $\hat A$ that satisfies $\hat A p^0=p^1$. This is a variant of the matrix balancing problem and has been studied and generalized in a long line of works \citep{fortet1940resolution,beurling1960automorphism,ruschendorf1995convergence,gurvits2004classical,georgiou2015positive,friedland2017schrodinger}. Applications in marketing estimate customers' transition probabilities between different brands using market share data
\citep{theil1966quadratic}. Coming full circle back to  choice modeling, matrix balancing has also been used to rank nodes of a network. \citet{knight2008sinkhorn} explains how the inverses of left and right scalings of the adjacency matrix with uniform target marginals (stationary distributions) can be naturally interpreted as measures of their ability to attract and emit traffic. This approach is also related to the works of \citet{lamond1981bregman,kleinberg1999authoritative,tomlin2003new}.


\section{Numerical Experiments}
\label{sec:empirics}
We compare the empirical performance of Sinkhorn's algorithm with the iterative LSR (I-LSR) algorithm of \citet{maystre2015fast} on real choice datasets. Because the implementation of I-LSR by \citet{maystre2015fast}
only accommodates pairwise comparison data and partial ranking data, but does not easily generalize to multi-way choice data, we focus on data with pairwise comparisons. 

We use the
natural parameters $\log s_{j}$ (logits) instead of $s_{j}$ when computing and evaluating
the updates, as the probability of $j$ winning
over $k$ is proportional to the ratio $s_{j}/s_{k}$, so that $s_j$ are usually logarithmically spaced. To make sure that estimates
are normalized, we impose the normalization that $\sum_{j}s_{j}=m$,
the number of objects, at the end of each iteration, although due to
the logarithm scale of the convergence criterion, the choice of normalization
does not seem to significantly affect the performances of the algorithms.
%generated based on the proposal in \citet{agarwal2018accelerated}, and partial ranking data, generated with code from \citet{maystre2015fast}.

We evaluate the algorithms on five real-world datasets consisting
of partial ranking or pairwise comparison data. The NASCAR dataset
consists of ranking results of the 2002 season NASCAR races. The SUSHI
datasets consist of rankings of sushi items. The Youtube dataset consists
of pairwise comparisons between videos and which one was considered
more entertaining by users. The GIFGIF dataset similarly consists
of pairwise comparisons of GIFs that are rated based on which one
is closer to describing a specific sentiment, such as happiness and
anger. We downsampled the Youtube and GIFGIF datasets due to memory
constraints. 
\begin{table*}[t]
\begin{centering}
\hspace*{-3cm}%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
\multirow{2}{*}{dataset} & \multirow{2}{*}{data type} & \multirow{2}{*}{items} & \multirow{2}{*}{observations} & \multirow{2}{*}{$k$} & \multicolumn{2}{c|}{Sinkhorn} & \multicolumn{2}{c|}{I-LSR}\tabularnewline
\cline{6-9} \cline{7-9} \cline{8-9} \cline{9-9} 
 &  &  &  &  & iterations & time & iterations & time\tabularnewline
\hline 
\hline 
NASCAR & $k$-way ranking & 83 & 36 & 43 & 20 & \textbf{0.029} & 13 & 0.960\tabularnewline
\hline 
SUSHI-10 & $k$-way ranking & 10 & 5000 & 10 & 16 & \textbf{0.025} & 8 & 1.708\tabularnewline
\hline 
SUSHI-100 & $k$-way ranking & 100 & 5000 & 10 & 21 & \textbf{0.253} & 9 & 2.142\tabularnewline
\hline 
Youtube & pairwise comparison & 2156 & 28134 & 2 & 89 & \textbf{7.984} & 33 & 15.026\tabularnewline
\hline 
GIFGIF & pairwise comparison & 2503 & 6876 & 2 & 1656 & \textbf{30.97} & 315 & 138.63\tabularnewline
\hline 
\end{tabular}\hspace*{-3cm}
\par\end{centering}
\caption{Performance of iterative ML inference algorithms on five real datasets. Youtube and GIFGIF data were subsampled. 
Convergence is declared when the maximum entry-wise change of an update
is less than $10^{-8}$. At convergence, the ML estimates returned
by the algorithms have entry-wise difference of at most $10^{-10}$.}
\label{tab:empirical}
\end{table*}

In \cref{tab:empirical} we report the running time of the three algorithms on different
datasets. Convergence is declared when the maximum entry-wise change
of an update to the natural parameters $\log s_{i}$ is less than
$10^{-8}$. At convergence, the MLEs returned by the algorithms have entry-wise difference of at most $10^{-10}$. %Since
% we know that the sequence of MM algorithm must converge to a stationary
% point, and that the matrix scaling algorithm is equivalent to the
% MM algorithm for pairwise comparisons and partial rankings, we know
% that the final estimates must be the MLE. 
We see that Sinkhorn's algorithm consistently outperforms
the I-LSR algorithm in terms of convergence speed. It also has the additional advantage of being parallelized with
elementary matrix-vector operations, whereas the iterative I-LSR algorithm needs to repeatedly
compute the steady-state of a continuous-time Markov chain, which
is prone to problems of ill-conditioning. This also explains why Sinkhorn's algorithm may take more iterations but has better wall clock time, since each iteration is much less costly. On the other hand, we note
that for large datasets, particularly those with a large number of
observations or alternatives, the dimension of $A$ used
may become too large for the memory of a single machine. If this is still a problem after removing duplicate rows and columns according to \cref{sec:equivalence}, we can use distributed implementations of Sinkhorn's algorithm, which in view of its connections to message passing algorithms, is a standard procedure.
% decompose $A$ into blocks of sub-matrices, on which \cref{alg:scaling} allows for parallel computation. 
%See \cref{alg:scaling-distributed}.