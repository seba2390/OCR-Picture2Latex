\section{Proofs}
\label{app:proofs}
\subsection{Proof of \cref{prop:mle-scaling}}
By construction, any normalized $s_j$ solving the optimality conditions in \eqref{eq:optimality} satisfy the matrix equations in \eqref{eq:equation-system}. 
It remains to show that a solution to \eqref{eq:equation-system} uniquely determines a solution to \eqref{eq:optimality}. Suppose two positive diagonal matrices $D^{0}$ and $D^{1}$ satisfy \cref{eq:scaled-matrix,eq:marginal}, i.e., $D^{1}AD^{0}\mathbf{1}_m =p$. Since the $i$-th row of $A$ is the indicator of the choice set $S_i$, we must have
\begin{align*}
D_{j}^{0} & =d_{j}\\
D_{i}^{1} & =\frac{p_i}{\sum_{k\in S_{i}}d_{k}},
\end{align*}
 for some positive $d_j$'s. The condition in \eqref{eq:bridge} that $\hat{A}^{T}\mathbf{1}_m = q$ then implies 
\begin{align*}
   q_j &= (D^0A^TD^1\mathbf{1}_m)_j\\
   &= \sum_{i\mid j\in S_{i}}p_i\frac{d_{j}}{\sum_{k\in S_{i}}d_{k}},
\end{align*}
so that $d_j$'s, the diagonal entries of $D^0$, satisfy the optimality condition \eqref{eq:optimality} of the maximum likelihood estimation problem of Luce choice models. $\hfill \square$  

 \subsection{Proof of \cref{thm:necessary-and-sufficient}}
First, we can verify that \cref{ass:weak-connected} is equivalent to the uniqueness condition \cref{ass:matrix-uniqueness} of the matrix balancing problem, namely the participation matrix $A$ is not permutation equivalent to a block-diagonal matrix. 

Now we prove that for the triplet $(A,p,q)$ constructed from the choice dataset, \cref{ass:strong-connected} on the choice dataset is equivalent to \cref{ass:matrix-existence} combined with \cref{ass:matrix-uniqueness} when $p, q$ are strictly positive. Consider an arbitrary pair of sets of indices $N \subsetneq [n]$ and $M \subsetneq [m]$ such that $A_{ij}=0$ for $i\notin N$ and $j\in M$. In the choice problem this condition implies that items in $M$ only appear in choice sets index by $N$. Then \cref{ass:strong-connected} implies that there is at least one item $k\notin M$ that is chosen over some item $j \in M$, which means items in $M$ are not always selected in observations with choice sets indexed by $N$, i.e., $\sum_{i\in N}p_i > \sum_{j\in M}q_j$. Moreover, \cref{ass:strong-connected} implies \cref{ass:weak-connected}, which is equivalent to \cref{ass:matrix-uniqueness}, so we have shown that \cref{ass:strong-connected} implies \cref{ass:matrix-existence} and \cref{ass:matrix-uniqueness}.

The converse direction is slightly less obvious. Suppose the $(A, p^0, p^1)$ constructed from a choice dataset satisfies Assumptions \ref{ass:matrix-existence} and \ref{ass:matrix-uniqueness}. In the choice dataset, consider an arbitrary partition of $[m]$ into $M$ and $M^C$. There are two cases to consider. First, suppose items in $M$ do not appear in all choice sets, i.e., there exists $N \subsetneq [n]$ such that $A_{ij}=0$ for $i\notin N$ and $j\in M$. Then \cref{ass:matrix-existence} of the matrix balancing problem implies $\sum_{i\in N}p^0_i > \sum_{j\in M}p^1_j$, i.e., some item $k \in M^C$ is selected over some item $j\in M$, which is what \cref{ass:strong-connected} requires. Second, suppose that every choice set contains least one item in $M$. Then the fact that $q>0$ implies that $q_k>0$ for any $k\in M^C$, i.e., at least some item $k\in M^C$ is selected over some item $j\in M$, which is again the condition in \cref{ass:strong-connected}. We have thus shown that there is some $k\in M^C$ that is chosen over some $j\in M$, and similarly vice versa, as required by \cref{ass:strong-connected}.
%To reiterate, the condition $\sum_{i\in N}p_i \geq \sum_{j\in M}q_j$ ensures a maximizer exists, although not necessarily in the interior of the simplex. When equality holds only if $A$ is in block diagonal form, maximizers are in the interior. Otherwise, maximizers are on the boundary, i.e., $s_j=0$ for some $j$.
$\hfill \square$

 \subsection{Proof of \cref{thm:global-convergence}}
Suppose first that Sinkhorn's algorithm is normalized at each iteration as in \cref{alg:scaling-normalized}. We will prove the linear convergence result first for this normalized version. Since the objective value is \emph{invariant} under normalization, the result automatically carries over to the original Sinkhorn's algorithm.

 \textbf{Outline of Proof.} \textbf{I.} We show that $g(u,v)$ is coercive on $\mathbf{1}_{m+n}^\perp$ under \cref{ass:matrix-existence}, which then guarantees that normalized iterates $(u^{(t)},v^{(t)})$ stay bounded. \textbf{II.} The next step is to study the Hessian of $g(u,v)$ on $\mathbf{1}_{m+n}^\perp$. The key observation is that the Hessian $\nabla^2 g(0,0)$ is the Laplacian matrix $\mathcal{L}$. The boundedness of normalized iterates $(u^{(t)},v^{(t)})$ then allows us to bound $\nabla^2g(u^{(t)},v^{(t)})$. \textbf{III.} Given the (restricted) strong convexity and smoothness of $g(u,v)$, the linear convergence rate of normalized Sinkhorn's algorithm can then be concluded by applying the result of \citet{beck2013convergence} on the linear convergence of alternating minimization methods. \textbf{IV.} Lastly, we apply Pinsker's inequality to convert the linear convergence of the optimality gap into a bound on the $\ell^1$ distance.

\textbf{I.} Recall the reparameterized potential function, for which Sinkhorn's
algorithm is the alternating minimization algorithm:
\begin{align*}
\min_{u\in\mathbb{R}^{m},v\in\mathbb{R}^{n}}g(u,v) & :=\sum_{ij}A_{ij}e^{-v_{i}+u_{j}}+\sum_{i=1}^{n}p_{i}v_{i}-\sum_{j=1}^{m}q_{j}u_{j}.
\end{align*}
 First, we show that $g(u,v)$ is coercive  on $\mathbf{1}_{m+n}^\perp$, i.e., $g(u,v)\rightarrow +\infty$ if $(u,v)\in \mathbf{1}_{m+n}^\perp$ and $\|(u,v)\|\rightarrow \infty$, whenever \cref{ass:matrix-existence} and \cref{ass:matrix-uniqueness} are satisfied.\footnote{We thank Wenzhi Gao for very helpful discussions.} There are several cases.
\begin{enumerate}
     \item If $u_j \rightarrow +\infty$ for some $j$ but all $v_i$ stay bounded from above, then since $A$ does not contain zero columns or rows, the term $\sum_{ij}A_{ij}e^{-v_{i}+u_{j}}$ dominates, so $g(u,v)\rightarrow +\infty$.

     \item  Similarly, if some $v_i \rightarrow -\infty$ but all $u_j$ stay bounded from below, the term $\sum_{ij}A_{ij}e^{-v_{i}+u_{j}}$ dominates, so $g(u,v)\rightarrow +\infty$.

     \item If $u_j \rightarrow -\infty$ for some $j$ but all $v_i$ stay bounded from below, the term $-\sum_{j=1}^{m}q_{j}u_{j}$ dominates, so $g(u,v)\rightarrow +\infty$.

     \item Similarly, if $v_i \rightarrow+\infty$ for some $i$ but all $u_j$ stay bounded from above, the term $\sum_{i=1}^{n}p_{i}v_{i}$ dominates, so $g(u,v)\rightarrow +\infty$.

     \item Suppose now $v_i, u_j\rightarrow +\infty$ for some $i,j$. Then the subsets 
\begin{align*}
    I:=\{i: v_i \rightarrow +\infty\},\quad J:=\{j: u_j \rightarrow +\infty\}.
\end{align*}
    are both non-empty. Now either the exponential terms or the linear terms could dominate. If $u_j-v_i \rightarrow +\infty$ for some $i\in I, j\in J$, then one of the exponential terms dominates and $g(u,v)\rightarrow +\infty$. Otherwise, $-\min_{i\in I} v_i+\max_{j\in J} u_j$ stay bounded above, and the sum $\sum_{i\in I, j\in J}A_{ij}e^{-v_{i}+u_{j}}$ over $I,J$ stays bounded. There are now two sub-cases. 
     \begin{itemize}
         \item If there exists $i \notin I,j\in J$ such that $A_{ij} >0$, then  $A_{ij}e^{-v_{i}+u_{j}} \rightarrow +\infty$, since $v_i$ for $i\notin I$ is bounded from above.
         \item If $A_{i,j}=0$ for all $j\in J,i\notin I$. Suppose first $I=[n]$, i.e., all $v_i \rightarrow +\infty$. Now since we require that $(u,v) \in 
 \mathbf{1}_{m+n}^\perp$, there must exist some $j$ such that $u_j \rightarrow -\infty$, i.e., $J\subsetneq [m]$. Then we have $\sum_{i\in I} p_i=\sum_{i=1}^n p_i=\sum_{j=1}^m q_j > \sum_{j\in J} q_j$. 
 Thus, the linear terms dominate:  
  \begin{align*}
\sum_{i=1}^{n}p_{i}v_{i}-\sum_{j=1}^{m}q_{j}u_{j} \geq  \sum_{i=1}^{n}p_{i}v_{i}-\sum_{j\in J}q_{j}u_{j} \rightarrow +\infty.
     \end{align*}
         Now suppose $I \subsetneq [n]$. If $J=[m]$, i.e., $u_j \rightarrow+\infty$ for all $j$, then the requirement that $(u,v) \in 
 \mathbf{1}_{m+n}^\perp$ again guarantees that for some $i\notin I$, $v_i \rightarrow -\infty$. Since $A$ does not contain zero rows, the sum $\sum_{j=1}^m A_{ij}e^{-v_{i}+u_{j}} \rightarrow +\infty$.
         Lastly, if both  $I\subsetneq [n]$ $J\subsetneq [m]$,  then \cref{ass:matrix-existence} applies to $I,J$, and guarantees that $\sum_{i\in I} p_i > \sum_{j\in J} q_j$, so that
          \begin{align*}
\sum_{i\in I}p_{i}v_{i}-\sum_{j\in J}q_{j}u_{j} \rightarrow +\infty.
     \end{align*}
     If, in addition, for all $i\notin I$, $v_i$ is bounded below, then the linear terms dominate
     \begin{align*}
\sum_{i=1}^{n}p_{i}v_{i}-\sum_{j=1}^{m}q_{j}u_{j} \rightarrow +\infty.
     \end{align*}
     And if for some $i\notin I$, $v_i \rightarrow -\infty$, then since $A$ does not contain zero rows, the sum $\sum_{j=1}^m A_{ij}e^{-v_{i}+u_{j}} \rightarrow +\infty$.  
     \end{itemize}
     \item The case $v_i, u_j\rightarrow -\infty$ for some $i,j$ is symmetric to the previous case and we omit the detailed reasoning.   
 \end{enumerate}
 
 
 Note that coercivity does not hold if we do not restrict to $\mathbf{1}_{m+n}^\perp$, since $g(u,v)=g(u+c,v+c)$ for any constant $c\in\mathbb{R}$. This is the first reason why \emph{normalization} is essential in our analysis.
 
 Coercivity on $\mathbf{1}_{m+n}^\perp$ guarantees that the optimal solution $(u^\ast,v^\ast)$ defined in \eqref{eq:normalized-optimum} is finite. More importantly, it implies that the sub-level sets
 \begin{align*}
   S_g^\perp(\alpha):  \{(u,v)\in \mathbf{1}_{m+n}^\perp:g(u,v)\leq \alpha \}
 \end{align*}
 are \emph{bounded}. In particular, this property holds for $\alpha_0=g(u^{(0)},v^{(0)})$,\footnote{In practice, we can for example take $(u^{(0)},v^{(0)})=0$, so that $\alpha_0=\sum_{i,j}A_{ij}$.} so that there exists $B<\infty$ such that $\|(u,v)\|_{\infty}\leq B$ whenever $(u,v)\in \mathbf{1}_{m+n}^\perp$ and  $g(u,v)\leq g(u^{(0)},v^{(0)})$. Since Sinkhorn's algorithm is the alternating minimization
of $g(u,v)$,
\begin{align*}
    g(u^{(t+1)},v^{(t+1)})\leq g(u^{(t)},v^{(t)})\leq g(u^{(0)},v^{(0)})
\end{align*}
for all $t$. It then follows that $(u^{(t)},v^{(t)}) \in S_g^\perp(\alpha_0)$, so that $\|(u^{(t)},v^{(t)})\|_{\infty}\leq B$ for
all $t>0$. In summary, we have shown that Sinkhorn iterations stay bounded by $B$, which will be important for lower bounding the Hessian of $g$. 

\textbf{II.} Next, we show that $g$ is strongly convex when restricted to $ \mathbf{1}_{m+n}^\perp$. The gradient of $g(u,v)$ is given by 
\begin{align*}
\partial_{u_{j}}g(u,v) & =\sum_{i}A_{ij}e^{-v_{i}+u_{j}}-q_{j}\\
\partial_{v_{i}}g(u,v) & =-\sum_{j}A_{ij}e^{-v_{i}+u_{j}}+p_{i},
\end{align*}
and the Hessian is given by 
\begin{align*}
\nabla^{2}g(u,v)=\begin{bmatrix}\mathcal{D}(\sum_{j}A_{ij}e^{u_{j}-v_{i}}) & -\hat{A}\\
-\hat{A}^{T} & \mathcal{D}(\sum_{i}A_{ij}e^{u_{j}-v_{i}})
\end{bmatrix} & =\begin{bmatrix}\mathcal{D}(\hat{A}\mathbf{1}_{m}) & -\hat{A}\\
-\hat{A}^{T} & \mathcal{D}(\hat{A}^{T}\mathbf{1}_{n})
\end{bmatrix}
\end{align*}
 where 
\begin{align*}
\hat{A}_{ij}(u,v) & =A_{ij}e^{-v_{i}+u_{j}}.
\end{align*}
Note that $\mathbf{1}_{n+m}$ is in the null space of the Hessian
at any $(u,v)$, since
\begin{align*}
\begin{bmatrix}\mathcal{D}(\hat{A}\mathbf{1}_{m}) & -\hat{A}\\
-\hat{A}^{T} & \mathcal{D}(\hat{A}^{T}\mathbf{1}_{n})
\end{bmatrix}\begin{bmatrix}\mathbf{1}_{n}\\
\mathbf{1}_{m}
\end{bmatrix} & =\begin{bmatrix}\hat{A}\mathbf{1}_{m}-\hat{A}\mathbf{1}_{m}\\
-\hat{A}^{T}\mathbf{1}_{n}+\hat{A}^{T}\mathbf{1}_{n}
\end{bmatrix}=\begin{bmatrix}\mathbf{0}\\
\mathbf{0}
\end{bmatrix}.
\end{align*}
As a result, if we restrict to the subspace $(u,v)\perp\begin{bmatrix}\mathbf{1}_{m}\\
\mathbf{1}_{n}
\end{bmatrix}$, which is achieved with the appropriate normalization in Sinkhorn's
algorithm, the Hessian of the potential function $g(u,v)$ is lower
bounded by $\lambda_{-2}(\nabla^{2}g(u,v))\cdot I_{m+n}$. This is the second reason why normalization is important for the linear convergence analysis of Sinkhorn's algorithm.

The next key observation is that the Hessian $\nabla^{2}g(u,v)$ is precisely the \emph{Laplacian}
matrix of the bipartite graph defined by $\hat{A}$: 
\begin{align*}
\mathcal{L}(\hat{A}):=\begin{bmatrix}\mathcal{D}(A\mathbf{1}_{m}) & -\hat{A}\\
-\hat{A}^{T} & \mathcal{D}(\hat{A}^{T}\mathbf{1}_{n})
\end{bmatrix} & =\mathcal{D}(\begin{bmatrix}0 & \hat{A}\\
\hat{A}^{T} & 0
\end{bmatrix}\begin{bmatrix}\mathbf{1}_{n}\\
\mathbf{1}_{m}
\end{bmatrix})-\begin{bmatrix}0 & \hat{A}\\
\hat{A}^{T} & 0
\end{bmatrix},
\end{align*}
where we recognize 
\begin{align*}
\begin{bmatrix}0 & \hat{A}\\
\hat{A}^{T} & 0
\end{bmatrix}
\end{align*}
as the adjacency matrix of the bipartite graph defined by $\hat{A}$, and $\mathcal{L}$ defined in \eqref{eq:Laplacians} is precisely $\nabla^2g(0,0)$.

Our next step is then to connect $\mathcal{L}(\hat{A})$ to the Laplacian $\mathcal{L}$ at the origin, so that we can lower bound $\lambda_{-2}(\nabla^{2}g(u,v))$. We use the
well-known fact that the bipartite graph Laplacian $\nabla^{2}g(u,v)$
is \emph{similar} to the following ``signless'' Laplacian, so they share the
same spectrum:
\begin{align*}
\mathcal{L}'(\hat{A}):= & \mathcal{D}(\begin{bmatrix}0 & \hat{A}\\
\hat{A}^{T} & 0
\end{bmatrix}\begin{bmatrix}\mathbf{1}_{n}\\
\mathbf{1}_{m}
\end{bmatrix})+\begin{bmatrix}0 & \hat{A}\\
\hat{A}^{T} & 0
\end{bmatrix},
\end{align*}
 so that 
\begin{align*}
\lambda_{-2}(\nabla^{2}g(u,v)) & =\lambda_{-2}(\mathcal{L}'(\hat{A})).
\end{align*}

We claim that for all $(u,v) \in \mathbf{1}_{m+n}^\perp$ with $\|(u,v)\|_\infty\leq B$, 
\begin{align*}
\mathcal{L}'(\hat{A})=\mathcal{D}(\begin{bmatrix}0 & \hat{A}\\
\hat{A}^{T} & 0
\end{bmatrix}\begin{bmatrix}\mathbf{1}_{n}\\
\mathbf{1}_{m}
\end{bmatrix})+\begin{bmatrix}0 & \hat{A}\\
\hat{A}^{T} & 0
\end{bmatrix} & \succeq e^{-2B}(\mathcal{D}(\begin{bmatrix}0 & A\\
A^{T} & 0
\end{bmatrix}\begin{bmatrix}\mathbf{1}_{n}\\
\mathbf{1}_{m}
\end{bmatrix})+\begin{bmatrix}0 & A\\
A^{T} & 0
\end{bmatrix})=e^{-2B}\mathcal{L}'(A).
\end{align*}
 Consider first the off-diagonal blocks. We have 
\begin{align*}
\hat{A}_{ij} & =A_{ij}e^{-v_{i}+u_{j}}\geq e^{-2B}A_{ij}.
\end{align*}
 Similarly, for the diagonal blocks, we have 
\begin{align*}
\mathcal{D}(\hat{A}\mathbf{1}_{m})_{i} & =\sum_{j}A_{ij}e^{u_{j}-v_{i}}\geq e^{-2B}\sum_{j}A_{ij}\\
\mathcal{D}(\hat{A}^{T}\mathbf{1}_{n})_{j} & =\sum_{i}A_{ij}e^{u_{j}-v_{i}}\geq e^{-2B}\sum_{i}A_{ij}.
\end{align*}
 The above inequalities imply that all entries of the following difference 
\begin{align*}
\mathcal{L}'(\hat{A})-e^{-2B}\mathcal{L}'(A)
\end{align*}
are non-negative. Moreover, since both $\mathcal{L}'(\hat{A})$ and
$\mathcal{L}'(A)$ have $[\mathbf{1}_{n},-\mathbf{1}_{m}]$ in their
null spaces, so does the difference above. Gershgorin circle theorem
then guarantees that the eigenvalues of $\mathcal{L}'(\hat{A})-e^{-2B}\mathcal{L}'(A)$
are all non-negative. Finally, since the signless Laplacian $\mathcal{L}'(A)$
shares the same spectrum as the Laplacian 
\begin{align*}
\mathcal{L} & =\mathcal{D}(\begin{bmatrix}0 & A\\
A^{T} & 0
\end{bmatrix}\begin{bmatrix}\mathbf{1}_{n}\\
\mathbf{1}_{m}
\end{bmatrix})+\begin{bmatrix}0 & A\\
A^{T} & 0
\end{bmatrix},
\end{align*}
 we can conclude that 
\begin{align*}
\lambda_{-2}(\nabla^{2}g(u,v)) & \geq e^{-2B}\lambda_{-2}(\mathcal{L}).
\end{align*}
It then follows that $g(u,v)$ is $e^{-2B}\lambda_{-2}(\mathcal{L})$-strongly
convex on the subspace $(u,v)\perp\mathbf{1}_{n+m},\|(u,v)\|_{\infty}\leq B$. Recall that coercivity of $g(u,v)$ on $\mathbf{1}_{m+n}^\perp$ precisely guarantees $\|(u^{(t)},v^{(t)})\|_{\infty}\leq B$ for all $(u^{(t)},v^{(t)})$ during the iterations of normalized Sinkhorn's algorithm.

Next, we compute the smoothness constants $L_{0},L_{1}$ of $g(u,v)$
when restricted to one of the two blocks of variables. Recall that the gradient of $g$ is given by 
\begin{align*}
\partial_{u_{j}}g(u,v) & =\sum_{i}A_{ij}e^{-v_{i}+u_{j}}-q_{j}\\
\partial_{v_{i}}g(u,v) & =-\sum_{j}A_{ij}e^{-v_{i}+u_{j}}+p_{i},
\end{align*}
so that for any $i,j$,
\begin{align*}
|\partial_{u_{j}}^{2}g(u,v)| & \leq e^{2B}\sum_{i}A_{ij}\\
|\partial_{v_{i}}^{2}g(u,v)| & \leq e^{2B}\sum_{j}A_{ij}.
\end{align*}
 It then follows that the Lipschitz constants of the two blocks are
given by 
\begin{align*}
L_{0} & =e^{2B}\max_{j}\sum_{i}A_{ij}=e^{2B}l_{0}\\
L_{1} & =e^{2B}\max_{i}\sum_{j}A_{ij}=e^{2B}l_{1}.
\end{align*}
\textbf{III.} Combining the strong convexity and smoothness bounds, we can apply
the linear convergence result in Theorem 5.2 of \citet{beck2013convergence} to conclude that for any $t\geq 0$,
\begin{align*}
g(u^{(t+1)},v^{(t+1)})-g^{\ast} & \leq(1-e^{-4B}\frac{\lambda_{-2}(\mathcal{L})}{\min\{l_{0},l_{1}\}})\cdot\left(g(u^{(t)},v^{(t)})-g^{\ast}\right).
\end{align*}

Note that we do not need to compute the smoothness constant $L$ of
the entire function $g(u,v)$, since convergence results on alternating
minimization algorithms only require the \emph{minimum} of smoothness
constants of the function restricted to each block, which is upper
bounded by $L$. Nevertheless, for $g(u,v)$, we compute its $L$
explicitly for completeness. Note that by a similar reasoning used
to lower bound $\mathcal{L}'(\hat{A})$, we have
\begin{align*}
\mathcal{L}'(\hat{A}) & \preceq e^{2B}\mathcal{L}'(A).
\end{align*}
Gershgorin circle theorem then bounds the maximal eigenvalue
\begin{align*}
    \lambda_{1}(\mathcal{L}'(A))=\lambda_{1}(\mathcal{L})\leq2\max\{\max_{i}\sum_{j}A_{ij},\max_{j}\sum_{i}A_{ij}\}=L,
\end{align*}
which is always strictly larger than $l=\min\{\max_{i}\sum_{j}A_{ij},\max_{j}\sum_{i}A_{ij}\}$.

\textbf{IV.} Now we show how the linear convergence result on $g(u,v)$ can be
converted to a bound on the $\ell^{1}$ distance $\|r^{(t)}-p\|_{1}$.
Our derivation follows the approach in \citet{altschuler2017near}, whose Lemma 2 establishes
the following key connection between $g(u,v)$ and the KL divergence
$D_{\text{KL}}(p\|r^{(t)})$: 
\begin{align*}
g(u^{(t)},v^{(t)})-g(u^{(t+1)},v^{(t+1)}) & =D_{KL}(p\|r^{(t)})+D_{KL}(q\|c^{(t)}),
\end{align*}
Note that the KL divergence $D_{KL}(p\|r^{(t)})$ above has a different
order from that in \citet{leger2021gradient}, which is $D_{\text{KL}}(r^{(t)}\|p)$, but
this difference does not matter for our analysis, since $D_{KL}(p\|r^{(t)})$
is just an intermediate quantity that is then converted to $\ell^{1}$
distance via Pinsker's inequality: 
\begin{align*}
\|p-r^{(t)}\|_{1} & \leq\sqrt{2D_{KL}(p\|r^{(t)})}.
\end{align*}
 Applying these inequality, we have 
\begin{align*}
\|p-r^{(t)}\|_{1}^{2}	&\leq2(g(u^{(t)},v^{(t)})-g(u^{(t+1)},v^{(t+1)}))
	\\ &\leq2(g(u^{(t)},v^{(t)})-g(u^{\ast},v^{\ast}))
	\\ &\leq2(1-e^{-4B}\frac{\lambda_{-2}(\mathcal{L})}{\min\{l_{0},l_{1}\}})^{t}(g(u^{(0)},v^{(0)})-g(u^{\ast},v^{\ast}))
	\\ &\leq(8B\sum_{i}p_{i})e^{-c_{B}\frac{\lambda_{-2}(\mathcal{L})}{\min\{l_{0},l_{1}\}}\cdot t},
\end{align*}
which immediately implies the iteration complexity bound that $\|r^{(t)}-p\|_1\leq \epsilon$ after
      \begin{align*}
          \mathcal{O} \left (\frac{\min\{l_{0},l_{1}\}}{\lambda_{-2}(\mathcal{L})} \cdot \log (1/\epsilon) \right )
      \end{align*}
      iterations of Sinkhorn's algorithm.
  $\hfill \square$

  \subsection{Proof of \cref{thm:lower-bound}}
  We will show that there exists some row $i$ of the scaled matrix $A^{(t)}$ such that $|\sum_{j=1}^m A^{(t)}_{ij}-p_i|$ vanishes at a rate bounded below by $\Omega(1/t)$. Because \cref{ass:matrix-existence} is satisfied but \cref{ass:matrix-weak-existence} fails, no finite solution to the
matrix balancing problem exists, but Sinkhorn's algorithm converges
to $\hat{A}$. Without loss of generality, we may assume column scalings
do not vanish, since we can always multiply and divide the scalings
without altering the scaled matrix. Lemma 1 of \citet{pukelsheim2014biproportional} states that
there exists $I\subseteq[n]$ and $J\subseteq[m]$ such that, after
necessary permutation, $A$ can be written in block form as 
\begin{align*}
A=\begin{bmatrix}A_{IJ} & 0\\
A_{I^{C}J} & A_{I^{C}J^{C}}
\end{bmatrix},
\end{align*}
 and 
\begin{align*}
\sum_{j\in J^{C}}q_{j} & =\sum_{i\in I^{C}}p_{i}\\
\sum_{j\in J}q_{j} & =\sum_{i\in I}p_{i}.
\end{align*}
Moreover, $I$ is the set of rows with row scalings not vanishing,
and $J$ is the set of columns with column scalings not diverging:
\begin{align*}
I=\{i:d_{i}^{1(t)}\not\rightarrow0\},\quad J=\{j:d_{j}^{0(t)}\not\rightarrow\infty\}.
\end{align*}
Thus there exists a constant $c$ such that $\sum_{k\in J}d_{k}^{0(t)}\leq c$
for (a subsequence of) $t=1,2,\dots$. 

For any non-negative matrix inheriting the zeros of $A$ and having
marginals $p,q$, the lower left block indexed by $I^{C}J$ must be
identically 0. This block is referred to as a fading block. Thus the
limiting matrix $\hat{A}$ of Sinkhorn's algorithm, which solves the
primal KL divergence minimization problem, must have the form 
\begin{align*}
\hat{A}=\begin{bmatrix}\hat{A}_{IJ} & 0\\
0 & \hat{A}_{I^{C}J^{C}}
\end{bmatrix},
\end{align*}
and all entries in the lower left block in $A^{(t)}$ converge to
$0$ as the number of iterations increases.

Without loss of generality, we may assume that $\min_{ij:A_{ij}>0}A_{ij}\geq1$,
since we can rescale $A$ by a fixed constant without altering the
convergence rate. Define $\overline{A}:=\max_{ij}A_{ij}$. We claim
that $\sum_{j\in J^{C}}d_{j}^{0(t)}\leq c\overline{A}t$ for all $t$.
We can initialize with $d^{0(1)}$ that satisfies $\sum_{j\in J^{C}}d_{j}^{0(1)}\leq\overline{A}c$,
and prove the general case by induction. Suppose $\sum_{j\in J^{C}}d_{j}^{0(t)}\leq c\overline{A}t$
and we want to show $\sum_{j\in J^{C}}d_{j}^{0(t+1)}\leq c\overline{A}(t+1)$.
We have 
\begin{align*}
\sum_{j\in J^{C}}d_{j}^{0(t+1)}=\sum_{j\in J^{C}}q_{j}/\sum_{i}A_{ij}\frac{p_{i}}{\sum_{k}A_{ik}d_{k}^{0(t)}} & =\sum_{j\in J^{C}}q_{j}/\sum_{i\in I^{C}}A_{ij}\frac{p_{i}}{\sum_{k}A_{ik}d_{k}^{0(t)}},
\end{align*}
 where the last equality follows from the block structure of $A$.
Note that since $A$ cannot have zero columns, for any $j\in J^{C}$,
there is at least one $i\in I^{C}$ such that $A_{ij}>0$. 

Next, for any $i\in I^{C}$, we have 
\begin{align*}
\sum_{k}A_{ik}d_{k}^{0(t)} & =\sum_{k\in J}A_{ik}d_{k}^{0(t)}+\sum_{k\in J^{C}}A_{ik}d_{k}^{0(t)}\\
 & \leq\overline{A}\cdot\sum_{k\in J}d_{k}^{0(t)}+\overline{A}\cdot\sum_{k\in J^{C}}d_{k}^{0(t)}\\
 & \leq c\overline{A}+c\overline{A}t=c\overline{A}(t+1),
\end{align*}
 where the last inequality follows from the induction assumption and
the condition that the column scalings in $J$ stay bounded. Using
the inequality above and $\min_{ij:A_{ij}>0}A_{ij}\geq1$, we have
\begin{align*}
\sum_{j\in J^{C}}d_{j}^{0(t+1)} & =\sum_{j\in J^{C}}q_{j}/\sum_{i\in I^{C}}A_{ij}\frac{p_{i}}{\sum_{k}A_{ik}d_{k}^{0(t)}}\\
 & \leq\sum_{j\in J^{C}}q_{j}/(\sum_{i\in I^{C}}\frac{p_{i}}{c\overline{A}(t+1)})\\
 & =c\overline{A}(t+1)\cdot(\sum_{j\in J^{C}}q_{j}/\sum_{i\in I^{C}}p_{i})\\
 & =c\overline{A}(t+1).
\end{align*}
 where the last equality precisely follows from the condition $\sum_{j\in J^{C}}q_{j}=\sum_{i\in I^{C}}p_{i}$
in \cref{ass:matrix-weak-existence}. We have therefore proved that $\sum_{j\in J^{C}}d_{j}^{0(t)}\leq c\overline{A}t$
for all $t$. The pigeonhole principle implies that there exists a
$j\in J^{C}$ and a subsequence $t_{1},t_{2},\cdots\rightarrow\infty$
such that $d_{j}^{0(t_{l})}\leq\frac{1}{|J^{C}|}c\overline{A}t_{l}$
for all $l=1,2,\dots$. Since the $j$-th column of $\hat{A}$ cannot
all vanish, there exists $i\in I^{C}$ such that $d_{i}^{1(t_{l})}A_{ij}d_{j}^{0(t_{l})}\rightarrow\hat{A}_{ij}>0$,
so that, selecting a further subsequence if necessary, we have 
\begin{align*}
d_{i}^{1(t_{l})} & \geq\hat{A}_{ij}/(\frac{1}{|J^{C}|}c\overline{A}t_{l}).
\end{align*}
 In other words, the row scaling $d_{i}^{1(t)}$ for row $i\in I^{C}$
vanishes at a rate bounded below by $\Omega(1/t)$. Recall that by assumption column scalings
are non-vanishing, which implies $\sum_{j\in J}^m A^{(t)}_{ij}\rightarrow 0$ at a rate bounded below by $\Omega(1/t)$. The same reasoning in fact guarantees that $\sum_{j\in J}^m A^{(t)}_{ij}= \Omega(1/t)$ for all $i\in I^C$, i.e., the fading block vanishes at rate $\Omega(1/t)$, so that $D_{\text{KL}}(r^{(t)}\| p)=\Omega(1/t)$.
$\hfill \square$
 
\subsection{Proof of \cref{thm:convergence}}
 \textbf{Outline of Proof.} \textbf{I.} We define a novel sequence of data-dependent mappings induced
by Sinkhorn's iterations, $f^{(t)}(x):\mathbb{R}_{++}^{n}\rightarrow\mathbb{R}_{++}^{n}$,
that map $r^{(t)}/\sqrt{p}$ to $r^{(t+1)}/\sqrt{p}$, starting from
column normalized $A^{(t)}$. For all $t$, $\sqrt{p}$ is always
the fixed point of $f^{(t)}$, and the construction guarantees that
the residuals $r^{(t)}/\sqrt{p}-\sqrt{p}$ are always orthogonal to
the fixed point $\sqrt{p}$. \textbf{II.} The Jacobian $J^{(t)}$ at $\sqrt{p}$ is given by
\begin{align*}
J^{(t)}= & \mathcal{D}(\sqrt{p}/r^{(t)})A^{(t)}\mathcal{D}(1/q)A^{(t)T}\mathcal{D}(1/\sqrt{p})
\end{align*}
 which has unique maximal eigenvector $\sqrt{p}$ with eigenvalue
1. Moreover, $J^{(t)}\rightarrow J=\mathcal{D}(1/\sqrt{p})\hat{A}\mathcal{D}(1/q)\hat{A}^{T}\mathcal{D}(1/\sqrt{p})$
as $t\rightarrow\infty$, and $\tilde{A}=\mathcal{D}(1/\sqrt{p})\cdot\hat{A}\cdot\mathcal{D}(1/\sqrt{q})$ also has unique maximal
eigenvector $\sqrt{p}$ with eigenvalue 1. \textbf{III.} Thus we have a sequence of mappings $f^{(t)}$ with
fixed points $\sqrt{p}$, whose Jacobians $J^{(t)}$ converge to $J$
with maximal eigenvector $\sqrt{p}$. The orthogonality property $(r^{(t)}/\sqrt{p}-\sqrt{p})^{T}\sqrt{p}=0$
and uniform boundedness of second derivatives of $f^{(t)}$ near $\sqrt{p}$
then allows us to conclude that the asymptotic linear convergence rate is exactly
equal to the subdominant eigenvalue of $J$.

\textbf{I.} Starting with column normalized matrix $A^{(t)}$ at the
$t$-th iteration, we first write down the general formula for the
row sums $r^{(t+1)}=A^{(t+1)}\mathbf{1}_{m}$ after one iteration
of Sinkhorn's algorithm:

\begin{align*}
r_{i}^{(t+1)} & =\\
\frac{A_{i1}^{(t)}\cdot p_{i}/r_{i}^{(t)}\cdot q_{1}}{A_{11}^{(t)}\frac{p_{1}}{r_{1}^{(t)}}+A_{21}^{(t)}\frac{p_{2}}{r_{2}^{(t)}}+\cdots+A_{n1}^{(t)}\frac{p_{n}}{r_{n}^{(t)}}}+\frac{A_{i2}^{(t)}\cdot p_{i}/r_{i}^{(t)}\cdot q_{2}}{A_{12}^{(t)}\frac{p_{1}}{r_{1}^{(t)}}+A_{22}^{(t)}\frac{p_{2}}{r_{2}^{(t)}}+\cdots+A_{n2}^{(t)}\frac{p_{n}}{r_{n}^{(t)}}}+\cdots+\frac{A_{im}^{(t)}\cdot p_{i}/r_{i}^{(t)}\cdot q_{m}}{A_{1m}^{(t)}\frac{p_{1}}{r_{1}^{(t)}}+A_{2m}^{(t)}\frac{p_{2}}{r_{2}^{(t)}}+\cdots+A_{nm}^{(t)}\frac{p_{n}}{r_{n}^{(t)}}}
\end{align*}
Although the update formula looks complicated, it can be interpreted
in the following manner. Given positive $r_{1}^{(0)}/p_{1},r_{2}^{(0)}/p_{2},\dots,r_{n}^{(0)}/p_{n}$,
we take the\emph{ convex} combination of their \emph{inverses}:
\begin{align*}
c_{j}^{(t)}/q_{j} & :=A_{1j}^{(t)}/q_{j}\cdot\frac{p_{1}}{r_{1}^{(t)}}+A_{2j}^{(t)}/q_{j}\cdot\frac{p_{2}}{r_{2}^{(t)}}+\cdots+A_{nj}^{(t)}/q_{j}\cdot\frac{p_{n}}{r_{n}^{(t)}},
\end{align*}
 where $A^{(t)}$ is assumed to be column normalized, hence have column
sums equal to $q$. After we have formed $c_{1}^{(t)}/q_{1},\dots,c_{m}^{(t)}/q_{m}$,
we simply repeat the process by taking the \emph{convex} combination
of their inverses: 
\begin{align*}
r_{i}^{(1)}/p_{i}:= & \frac{A_{i1}^{(t)}}{r_{i}^{(t)}}\cdot\frac{q_{1}}{c_{1}^{(t)}}+\frac{A_{i2}^{(t)}}{r_{i}^{(t)}}\cdot\frac{q_{2}}{c_{2}^{(t)}}+\cdots+\frac{A_{im}^{(t)}}{r_{i}^{(t)}}\cdot\frac{q_{m}}{c_{m}^{(t)}},
\end{align*}
 where by definition of $r_{i}^{(t)}$ we always have $\sum_{j}A_{ij}^{(t)}=r_{i}^{(t)}$.
We will work with this update formula to obtain a sequence of mappings. 

Recall that $/$ denotes entry-wise division whenever the quantities
are vectors, and similarly for $\sqrt{\cdot}$. Instead of $r^{(t)}$,
we will use $r^{(t)}/\sqrt{p}$ as the natural quantity to measure
the progress of convergence and show that $\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_{2}\rightarrow0$
linearly with rate $\lambda$. The reason for using $r^{(t)}/\sqrt{p}$
is because the residual $r^{(t)}/\sqrt{p}-\sqrt{p}$ satisfies 
\begin{align*}
(r^{(t)}/\sqrt{p}-\sqrt{p})^{T}\sqrt{p} & =\sum_{i}(r_{i}^{(t)}-p_{i})=0
\end{align*}
by virtue of Sinkhorn's algorithm preserving the quantities $r^{(t)T}\mathbf{1}_{n}$
for all $t$, so that the residual is always \emph{orthogonal} to
$\sqrt{p}$. This orthogonality property is crucial in identifying the rate of convergence,
as we will show that $\sqrt{p}$ is also the unique maximal eigenvector
of the limiting Jacobian with eigenvalue 1. 

Now rewrite the update formula as 
\begin{align*}
(r^{(t+1)}/\sqrt{p})_{i} & =\\
\frac{A_{i1}^{(t)}\cdot\sqrt{p}_{i}/r_{i}^{(t)}\cdot q_{1}}{A_{11}^{(t)}p_{1}/r_{1}^{(t)}+A_{21}^{(t)}p_{2}/r_{2}^{(t)}+\cdots+A_{n1}^{(t)}p_{n}/r_{n}^{(t)}}+\cdots+\frac{A_{im}^{(t)}\cdot\sqrt{p}_{i}/r_{i}^{(t)}\cdot q_{m}}{A_{1m}^{(t)}p_{1}/r_{1}^{(t)}+A_{2m}^{(t)}p_{2}/r_{2}^{(t)}+\cdots+A_{nm}^{(t)}p_{n}/r_{n}^{(t)}} & =\\
\frac{A_{i1}^{(t)}\cdot\sqrt{p}_{i}/r_{i}^{(t)}\cdot q_{1}}{A_{11}^{(t)}\sqrt{p_{1}}/(r_{1}^{(t)}/\sqrt{p_{1}})+A_{21}^{(t)}\sqrt{p_{2}}/(r_{2}^{(t)}/\sqrt{p_{2}})+\cdots+A_{n1}^{(t)}\sqrt{p_{n}}/(r_{n}^{(t)}/\sqrt{p_{n}})}+\cdots & +\\
\frac{A_{im}^{(t)}\cdot\sqrt{p}_{i}/r_{i}^{(t)}\cdot q_{m}}{A_{1m}^{(t)}\sqrt{p_{1}}/(r_{1}^{(t)}/\sqrt{p_{1}})+A_{2m}^{(t)}\sqrt{p_{2}}/(r_{2}^{(t)}/\sqrt{p_{2}})+\cdots+A_{nm}^{(t)}\sqrt{p_{n}}/(r_{n}^{(t)}/\sqrt{p_{n}})}.
\end{align*}
And define the mapping $f^{(t)}(x):\mathbb{R}_{++}^{n}\rightarrow\mathbb{R}_{++}^{n}$,
given $A^{(t)}$ and $r^{(t)}$, as 
\begin{align*}
f_{i}^{(t)}(x) & :=\frac{A_{i1}^{(t)}\cdot\sqrt{p}_{i}/r_{i}^{(t)}\cdot q_{1}}{A_{11}^{(t)}\sqrt{p_{1}}/x_{1}+A_{21}^{(t)}\sqrt{p_{2}}/x_{2}+\cdots+A_{n1}^{(t)}\sqrt{p_{n}}/x_{n}}+\cdots\\
 & +\frac{A_{im}^{(t)}\cdot\sqrt{p}_{i}/r_{i}^{(t)}\cdot q_{m}}{A_{1m}^{(t)}\sqrt{p_{1}}/x_{1}+A_{2m}^{(t)}\sqrt{p_{2}}/x_{2}+\cdots+A_{nm}^{(t)}\sqrt{p_{n}}/x_{n}}.
\end{align*}
Written more compactly, 
\begin{align*}
f^{(t)}(x) & =\mathcal{D}(\sqrt{p}/r^{(t)})A^{(t)}\mathcal{D}(q)\mathcal{D}(A^{(t)T}(\sqrt{p}/x))^{-1}\mathbf{1}_{m}.
\end{align*}

Our first observation is that $f^{(t)}(x)$ has fixed point $\sqrt{p}$
for all $t$, since 
\begin{align*}
f^{(t)}(\sqrt{p}) & =\mathcal{D}(\sqrt{p}/r^{(t)})A^{(t)}\mathcal{D}(q)\mathcal{D}(A^{(t)T}(\sqrt{p}/\sqrt{p}))^{-1}\mathbf{1}_{m}\\
 & =\mathcal{D}(\sqrt{p}/r^{(t)})A^{(t)}\mathcal{D}(q)\mathcal{D}(A^{(t)T}\mathbf{1}_{n})^{-1}\mathbf{1}_{m}\\
 & =\mathcal{D}(\sqrt{p}/r^{(t)})A^{(t)}\mathcal{D}(q)\cdot\mathbf{1}_{m}/q\\
 & =\mathcal{D}(\sqrt{p}/r^{(t)})A^{(t)}\mathbf{1}_{m}=\mathcal{D}(\sqrt{p}/r^{(t)})r^{(t)}=\sqrt{p}
\end{align*}
 We also define the ``limiting'' mapping by replacing $r^{(t)}$ with
$p$ and $A^{(t)}$ with $\hat{A}$ in $f^{(t)}$:
\begin{align*}
f(x) & :=\mathcal{D}(1/\sqrt{p})\hat{A}\mathcal{D}(q)\mathcal{D}(\hat{A}^{T}(\sqrt{p}/x))^{-1}\mathbf{1}_{m}.
\end{align*}
 It is straightforward to verify that $f(\sqrt{p})=\sqrt{p}$ as well.
Sinkhorn's iteration can then be represented as the following sequence:
\begin{align*}
r^{(0)}/\sqrt{p}\rightarrow f^{(0)}(r^{(0)}/\sqrt{p}) & =r^{(1)}/\sqrt{p}\\
r^{(1)}/\sqrt{p}\rightarrow f^{(1)}(r^{(1)}/\sqrt{p}) & =r^{(2)}/\sqrt{p}\\
\cdots
\end{align*}
 We reiterate that $f^{(t)}(x)$ is data-dependent, as it uses $r^{(t)}$
and $A^{(t)}$. Although $f$ is the pointwise limit of $f^{(t)}$,
we do not need any uniform convergence, thanks to the orthogonality
condition and the fact that $\sqrt{p}$ is the fixed point and unique
maximal eigenvector for \emph{every }Jacobian at $\sqrt{p}$ by construction. We only need the uniform boundedness of $r^{(t)},A^{(t)}$, which is guaranteed by coercivity of the potential function. 

\textbf{II. }The partial derivatives of $f^{(t)}$ are given by 
\begin{align*}
\partial_{j}f_{i}^{(t)}(x) & =\frac{A_{i1}^{(t)}\cdot\sqrt{p}_{i}/r_{i}^{(t)}\cdot q_{1}}{(A_{11}^{(t)}\sqrt{p_{1}}/x_{1}+A_{21}^{(t)}\sqrt{p_{2}}/x_{2}+\cdots+A_{n1}^{(t)}\sqrt{p_{n}}/x_{n})^{2}}\cdot\frac{A_{j1}^{(t)}\sqrt{p_{j}}}{x_{j}^{2}}+\cdots\\
 & +\frac{A_{im}^{(t)}\cdot\sqrt{p}_{i}/r_{i}^{(t)}\cdot q_{m}}{(A_{1m}^{(t)}\sqrt{p_{1}}/x_{1}+A_{2m}^{(t)}\sqrt{p_{2}}/x_{2}+\cdots+A_{nm}^{(t)}\sqrt{p_{n}}/x_{n})^{2}}\cdot\frac{A_{jm}^{(t)}\sqrt{p_{j}}}{x_{j}^{2}}
\end{align*}
and we can verify that the Jacobian $J^{(t)}(x)$ of $f^{(t)}$ can
be written in matrix-vector notation as 
\begin{align*}
J^{(t)}(x) & =\mathcal{D}(\sqrt{p}/r^{(t)})A^{(t)}\mathcal{D}(q)\cdot\mathcal{D}(A^{(t)T}(\sqrt{p}/x))^{-2}\cdot A^{(t)T}\mathcal{D}(1/x^{2})\mathcal{D}(\sqrt{p})
\end{align*}
When evaluated at the fixed point $x=\sqrt{p}$ of $f^{(t)}(x)$,
we obtain 
\begin{align*}
J^{(t)}:=J^{(t)}(\sqrt{p})=\mathcal{D}(\sqrt{p}/r^{(t)})\cdot A^{(t)}\cdot\mathcal{D}(1/q)A^{(t)T}\mathcal{D}(1/\sqrt{p})
\end{align*}
Our second observation is that $J^{(t)}$ has $\sqrt{p}$ as the unique
maximal eigenvector with eigenvalue equal to 1. Letting $t\rightarrow\infty$,
we have
\begin{align*}
J^{(t)}\rightarrow J:=\mathcal{D}(1/\sqrt{p})\hat{A}\mathcal{D}(1/q)\hat{A}^{T}\mathcal{D}(1/\sqrt{p})
\end{align*}
The unique maximal eigenvector with eigenvalue 1 of $J$ is also $\sqrt{p}$.
Moreover, $J$ is the Jacobian matrix of the limiting map $f$. 

\textbf{III. }We have so far obtained a sequence of mappings $f^{(t)}$
all with fixed point $\sqrt{p}$. The limiting map $f$ has Jacobian
equal to $J=\mathcal{D}(1/\sqrt{p})\hat{A}\mathcal{D}(1/q)\hat{A}^{T}\mathcal{D}(1/\sqrt{p})$,
which has $\sqrt{p}$ as unique maximal eigenvector. Moreover, the
residuals $r^{(t)}/\sqrt{p}-\sqrt{p}$ are always orthogonal to $\sqrt{p}$,
so that convergence is eventually governed by the subdominant eigenvalue
of $J$. We formalize this argument next.\textbf{ }

For any fixed $\epsilon$, since $J^{(t)}\rightarrow J=\mathcal{D}(1/\sqrt{p})\hat{A}\mathcal{D}(1/q)\hat{A}^{T}\mathcal{D}(1/\sqrt{p})$,
there exists $T_{0}$ such that for all $t\geq T_{0}$, $\lambda_{2}(J^{(t)})\leq\lambda_{2}(J)+\epsilon/2$.
Moreover, since $r^{(t)}\rightarrow p$, we can find $T_{1}$ such
that for all $t\geq T_{1}$, $\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_{2}\leq\delta$,
with $\delta$ to be determined later. We use the approximation formula
\begin{align*}
r^{(t+1)}/\sqrt{p} & =f^{(t)}(r^{(t)}/\sqrt{p})\\
 & =f^{(t)}(\sqrt{p}+r^{(t)}/\sqrt{p}-\sqrt{p})\\
 & =\sqrt{p}+J^{(t)}\cdot(r^{(t)}/\sqrt{p}-\sqrt{p})+R_{2}^{(t)}(r^{(t)}/\sqrt{p}-\sqrt{p}).
\end{align*}
Note that $f^{(t)}(x)=\mathcal{D}(\sqrt{p}/r^{(t)})A^{(t)}\mathcal{D}(q)\mathcal{D}(A^{(t)T}(\sqrt{p}/x))^{-1}\mathbf{1}_{m}$
are continuously differentiable, so Taylor's Theorem implies there
exists $T_{2}$ such that for all $t\geq T_{2}$,
\begin{align*}
\|R_{2}^{(t)}(r^{(t)}/\sqrt{p}-\sqrt{p})\|_{2} & \leq C\cdot\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_{2}^{2}
\end{align*}
for some constant $C\geq1$. The bound $C$ can be taken independent
of $t$ because second derivatives of $f^{(t)}$ have no singularities
near $\sqrt{p}$, and are parameterized by $r^{(t)},A^{(t)}$ which
are both uniformly bounded in $t$ by some $C$ near $\sqrt{p}$, using the coercivity of the potential function \eqref{eq:transformed-potential}, which we showed in the proof of \cref{thm:global-convergence}.

Now the key is that $(r^{(t)}/\sqrt{p}-\sqrt{p})\perp\sqrt{p}$, which
is the unique maximal eigenvalue of $J^{(t)}$, so that for all $t\geq0$,
\begin{align*}
\|J^{(t)}\cdot(r^{(t)}/\sqrt{p}-\sqrt{p})\|_{2} & \leq\|J^{(t)}\|_{*}\cdot\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_{2}
\end{align*}
 where $\|J^{(t)}\|_{*}$ is the operator norm of $J^{(t)}$ restricted
to the subspace orthogonal to $\sqrt{p}$. Since we are using the
2-norm and $J^{(t)}$ has unique maximal eigenvalue 1, $\|J^{(t)}\|_{*}$
is precisely $\lambda_{2}(J^{(t)})<1$. Now with $\delta\leq\epsilon/2C$,
for all $t\geq\max\{T_{0},T_{1},T_{2}\}$, we can then bound
\begin{align*}
\|r^{(t+1)}/\sqrt{p}-\sqrt{p}\|_{2} & \leq\|J^{(t)}\cdot(r^{(t)}/\sqrt{p}-\sqrt{p})\|_{2}+\|R_{2}^{(t)}(r^{(t)}/\sqrt{p}-\sqrt{p})\|_{2}\\
 & \leq\lambda_{2}(J^{(t)})\cdot\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_{2}+C\cdot\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_{2}^{2}\\
 & \leq(\lambda_{2}(J)+\epsilon/2)\cdot\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_{2}+C\delta\cdot\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_{2}\\
 & \leq(\lambda_{2}(J)+\epsilon)\cdot\|r^{(t)}/\sqrt{p}-\sqrt{p}\|_{2}.
\end{align*}
 which completes the proof. 

The proof again illustrates the importance of the strong existence and uniqueness conditions in Assumptions \ref{ass:matrix-existence} and \ref{ass:matrix-uniqueness}. When only the weak existence condition in \cref{ass:matrix-weak-existence} is satisfied,
the maximal eigenvalue 1 of $J$ has multiplicity greater than 1. To prove this, we can again use Lemma 1 in \citet{pukelsheim2014biproportional}, which shows that \cref{ass:matrix-weak-existence} holds but \cref{ass:matrix-existence} fails if and only if the limit $\hat A$ of Sinkhorn's algorithm is equivalent to a block diagonal matrix under permutation. Let $N\subseteq [n],M\subseteq[m]$ be such that $\hat A_{ij}=0$ for all $i\in N^C,j\in M$ or $i\in N,j\in M^C$. Then the vector $\tilde{\sqrt{p}}$ with 
\begin{align*}
    (\tilde{\sqrt{p}})_{i}&:=\begin{cases}
\sqrt{p_{i}} & i\in N\\
-\sqrt{p_{i}} & i\in N^{C}
\end{cases}
\end{align*}
is another eigenvector of $J=\mathcal{D}(1/\sqrt{p})\hat{A}\mathcal{D}(1/q)\hat{A}^{T}\mathcal{D}(1/\sqrt{p})$ not in the span of $\sqrt{p}$ with eigenvalue 1. As a result, the orthogonality condition is not sufficient to guarantee contraction and hence asymptotic linear
convergence. $\hfill \square$

 \subsection{Proof of \cref{lem:mm}}
We will show that the iteration in \eqref{eq:scaling-iteration}, which is equivalent to Sinkhorn's algorithm when it is applied to the Luce choice model, is algebraically equivalent to \eqref{eq:mm}. Recall the iteration \eqref{eq:mm} of the MM algorithm in \citet{hunter2004mm}:
\begin{align*}
s_{k}^{(t+1)} & =\frac{w_{k}}{\sum_{i=1}^{n}\sum_{j=1}^{l_{i}-1}\delta_{ijk}[\sum_{j'=j}^{l_{i}}s_{a(i,j')}^{(t)}]^{-1}},
\end{align*}
where $\delta_{ijk}$ is the indicator that item $k\in[m]$ appears in the $i$-th partial ranking $a(i,1)\rightarrow a(i,2)\rightarrow \cdots \rightarrow a(i,l_{i})$, and is not ranked higher than the $j$-th ranked item in that partial ranking. Suppose item $k$ is ranked $\kappa(i,k)$-th in the $i$-th partial ranking,  where $1\leq \kappa(i,k)\leq l_i-1$, i.e., it is not ranked last. Note that $s_{a(i,\kappa(i,k))}=s_k$. We have
\begin{align}
\label{eq:mm-proof-1}
\sum_{j=1}^{l_{i}-1}\delta_{ijk}[\sum_{j'=j}^{l_{i}}s_{a(i,j')}^{(t)}]^{-1} = \frac{1}{s_{a(i,1)}+\cdots+s_{a(i,l_i)}}+\frac{1}{s_{a(i,2)}+\cdots+s_{a(i,l_i)}}+\cdots+\frac{1}{s_{a(i,\kappa(i,k))}+\cdots+s_{a(i,l_i)}},
\end{align}
and if item $k$ is ranked last in the $i$-th partial ranking or does not appear in the $i$-th partial ranking, $\sum_{j=1}^{l_{i}-1}\delta_{ijk}[\sum_{j'=j}^{l_{i}}s_{a(i,j')}^{(t)}]^{-1}\equiv0$. Now if we associate each term in \eqref{eq:mm-proof-1} with a ``choice'' set consisting of the items that appear in the denominator, with the highest ranked item being the ``selected'' item, item $k$ is selected exactly once, in the choice set consisting of $a(i,\kappa(i,k)), \dots,a(i,a(i,l_i))$, corresponding to the last term in \eqref{eq:mm-proof-1}. Thus, each partial ranking of $l_i$ items gives rise to $l_i-1$ observations of choices. Summing over all partial rankings where item $k$ appears and is not last, we see that ${\sum_{i=1}^{n}\sum_{j=1}^{l_{i}-1}\delta_{ijk}[\sum_{j'=j}^{l_{i}}s_{a(i,j')}^{(t)}]^{-1}}$ is exactly equal to the denominator in \eqref{eq:scaling-iteration}:
\begin{align*}
s_{k}^{(t+1)} & =\frac{W_{k}}{\sum_{i\mid k\in S_{i}}\frac{R_i}{\sum_{k'\in S_{i}}s_{k'}^{(t)}}},
\end{align*}
and $w_k$, the number of partial rankings in which item $k$ appears and is not ranked last, is exactly the number of times it is ``selected'', i.e., $W_k$.$\hfill \square$

\subsection{Proof of \cref{prop:choicerank}}
Recall the random walk on
$G$  in the network choice model in \citet{maystre2017choicerank}. A user at node $j$ decides to move to node $k\in N_{j}^{\text{out}}$ with probability proportional to $s_{k}$.
To interpret the network choice model as an MNL model, we further define a \emph{choice set} parameterized by each node $j\in V$, consisting of
all items in $N_{j}^{\text{out}}$. Thus there are $m$ \emph{unique} types of choice sets. The number $c_{j}^{\text{out}}$ of transitions
out of node $j$ is exactly the number of observations whose choice sets are those in $N_{j}^{\text{out}}$.  
Therefore, the choice model in \citet{maystre2017choicerank} is a special case of the general Luce choice model with $m$ objects as well as $m$ unique types of choice sets, each appearing in $c_{j}^{\text{out}}$ observations, for a total of $m\cdot c_{j}^{\text{out}}$ choice observations. We index these observations by $i$ as in the Luce choice model. The selected item in an observation at node $j$ corresponds to the node 
into which the transition occurs. The set of nodes $N_{j}^{\text{in}}$
with edges pointing into node $j$ can then be interpreted as the set of all unique choice sets in which node $j$ appears. The number $c_{j}^{\text{in}}$ of observed transitions into a node
$j$ therefore corresponds to the number of observations in which
node $j$ is selected, i.e., it is equal to $W_{j}$ in \eqref{eq:scaling-iteration}. As observed in \citet{maystre2017choicerank}, the exact winner of each observation does not matter, as the sufficient statistics of their model are the aggregate transition counts at each node, i.e., $c_{j}^{\text{out}}, c_{j}^{\text{in}}$. This feature echoes our observation that in the Luce choice model, only the winning frequency $p^1$ matters for solving the ML estimation problem. In fact, $(c_{j}^{\text{out}}, c_{j}^{\text{in}}) = f(A,p^1)$ for some deterministic mapping $f$. 

Now we show that the unregularized version of the ChoiceRank algorithm in \citet{maystre2017choicerank}, given by
\begin{align}
\label{eq:choicerank-proof}
    s_j^{(t+1)} = \frac{c_j^\inn}{\sum_{k\in N_j^\inn} \gamma_k^{(t)}}, \gamma_j^{(t)}=\frac{c_j^\out}{\sum_{k\in N_j^\out}s_k^{(t)}},
\end{align}
is equivalent to \eqref{eq:scaling-iteration} when it is applied to the equivalent Luce choice model. First, note that ${\sum_{k\in N_{j}^{\text{out}}}s_{k}^{(t)}}$
corresponds to the sum of current estimates of $s_k$ over all items in the choice set indexed by node $j$. Furthermore, the summation in $\sum_{k\in N_{j}^{\text{in}}}\gamma_{k}^{(t)}$
is over all unique types of choice sets indexed by node $j$ in which item $k$ appears. As a result, 
\begin{align*}
    \sum_{k\in N_j^\inn} \gamma_k^{(t)} & = \sum_{k\in N_j^\inn} \frac{c_k^\out}{\sum_{k'\in N_k^\out}s_{k'}^{(t)}}\\
    &={\sum_{i\mid j\in S_{i}}\frac{1}{\sum_{k'\in S_{i}}s_{k'}^{(t)}}}
\end{align*}
and since $c_{j}^{\text{in}}=W_j$, we have proved that the iteration in \eqref{eq:choicerank-proof} is equal to \begin{align*}
s_{j}^{(t+1)} & =\frac{W_{j}}{\sum_{i\mid j\in S_{i}}\frac{R_i}{\sum_{k'\in S_{i}}s_{k'}^{(t)}}},
\end{align*}
which is equal to \eqref{eq:scaling-iteration}. $\hfill \square$

\subsection{Proof of \cref{prop:BLP}}
In the simplified setting with $\exp(\theta_{jt})\equiv s_{j}$, the BLP algorithm in \eqref{eq:blp} reduces to \begin{align*}
s_{j}^{(t+1)} & =s_{j}^{(t)}\cdot\hat{p}_{j}\cdot\frac{1}{p_{j}(s^{(t)},\beta,\Gamma,\Sigma)}
\end{align*}
where $p_{j}(\theta^{(m)},\beta,\Gamma,\Sigma)$ denotes the expected market share of the $j$-th product as a function of model parameters. 

Setting $\beta,\Gamma,\Sigma \equiv 0$, the expected market shares are given by 
\begin{align*}
p_{j} = \frac{s_{j}}{\sum_{k}s_{k}}= \frac{1}{n}\sum_{i=1}^n \frac{s_{j}}{\sum_{k}s_{k}}
\end{align*}
Now, consider a Luce choice model of $n$ observations, where the choice set in each observation contains all alternatives. The ``market share'' of product $j$ above is exactly equal to the right hand side in \eqref{eq:optimality} for this model: 
\begin{align*}
\sum_{i\mid j\in S_{i}}\frac{1}{n}\frac{s_{j}}{\sum_{k\in S_{i}}s_{k}},
\end{align*}
 while the ``observed market share'' of product $j$ in the BLP model is the fraction of observations where product $j$ is selected, i.e.,  $\hat{p}_{j}=\frac{1}{n}|\{i\mid c(S_{i})=j\}|$, which is equal to the left hand side of \eqref{eq:optimality-original}. The condition $p_{j}=\hat{p}_{j}$ from BLP is exactly 
\begin{align*}
\frac{1}{n}|\{i\mid c(S_{i})=j\}| & =\sum_{i\mid j\in S_{i}}\frac{1}{n}\frac{s_{j}}{\sum_{k\in S_{i}}s_{k}}.
\end{align*}

The BLP iteration takes the form 
\begin{align*}
 s_j^{(t+1)}& =s_{j}^{(t)}\cdot\frac{1}{n}|\{i\mid c(S_{i})=j\}|\cdot\left[\sum_{i\mid j\in S_{i}}\frac{1}{n}\frac{s_{j}^{(t)}}{\sum_{k\in S_{i}}s_{k}^{(t)}}\right]^{-1}\\
 & =W_{i}\cdot\left[\sum_{i\mid j\in S_{i}}\frac{1}{\sum_{k\in S_{i}}s_{k}^{(t)}}\right]^{-1},
\end{align*}
which we recognize to be the algebraic expression \eqref{eq:scaling-iteration} of Sinkhorn's algorithm.  $\hfill \square$
% This argument can be extended
% further to accommodate $s_{j}=\exp(\beta^{T}x_{j})$, in which case
% we use the above iteration to identify $s_{j}$, and then $X\beta=\log s$
% uniquely identifies $\beta$ through 
% \begin{align*}
% \beta & =(X^{T}X)^{-1}X^{T}\log s
% \end{align*}
%  where $X\in\mathbb{R}^{m\times p}$ contains rows $x_{j}\in\mathbb{R}^{p}$,
% if $X^{T}X$ has full rank. 
