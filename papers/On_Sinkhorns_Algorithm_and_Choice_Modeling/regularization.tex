\section{Regularization of Luce Choice Models and Matrix Balancing Problems}
\label{subsec:regularization}
In practice, many choice and ranking datasets may not satisfy \cref{ass:strong-connected}, which is required for the maximum likelihood estimation to be well-posed. Equivalently, for the matrix balancing problem, when a triplet $(A,p,q)$ does not satisfy \cref{ass:matrix-existence} and \cref{ass:matrix-uniqueness}, no finite scalings exist and Sinkhorn's algorithm may diverge. In this section, we discuss some regularization techniques to address these problems. They are easy to implement and require minimal modifications to Sinkhorn's algorithm. Nevertheless, they can be very useful in practice to regularize ill-posed problems. Given the equivalence between the problem of computing the MLE of Luce choice models and the problem of matrix balancing, our proposed regularization methods apply to both. 
\subsection{Regularization via Gamma Prior}
 As discussed in \cref{sec:formulations},  for a choice dataset to have a well-defined maximum likelihood estimator, it needs to satisfy \cref{ass:strong-connected}, which requires the directed comparison graph to be strongly connected. Although this condition is easy to verify, the question remains what one can do in case it does \emph{not} hold. As one possibility, we may introduce a prior on the parameters $s_j$, which serves as a regularization of the log-likelihood that results in a unique maximizer. Many priors are possible. For example, \citet{maystre2017choicerank}, following \citet{caron2012efficient}, use independent Gamma priors on $s_j$. In view of the fact that the unregularized problem and algorithm in \citet{maystre2017choicerank} is a special case of the Luce choice model and Sinkhorn's algorithm, we can also incorporate the Gamma prior to the Luce choice model \eqref{eq:log-likelihood} to address identification problems. 

More precisely, suppose now that each $s_j$ in the Luce choice model are i.i.d. Gamma$(\alpha,\beta)\propto s_j^{\alpha-1}e^{-\beta s_j}$. This leads to the following regularized log-likelihood:
\begin{align}
\label{eq:regularized-log-likelihood}
\ell^R(s):=\sum_{i=1}^{n}\log s_{j_i}-\log\sum_{k\in S_{i}}s_{k} +(\alpha-1)\sum_{j=1}^{m}\log s_j - \beta \sum_{j=1}^{m} s_j.
\end{align}
The corresponding first order condition is given by
\begin{align}
\label{eq:optimality-regularized}
\frac{W_j+\alpha-1}{n} & = \frac{1}{n} \left(\sum_{i\mid j\in S_{i}}R_i \frac{s_{j}}{\sum_{k\in S_{i}}s_{k}} +\beta s_j\right),
\end{align}
which leads to the following modified Sinkhorn's algorithm, which generalizes the ChoiceRank algorithm of \citet{maystre2017choicerank}: 
\begin{align}
\label{eq:regularized-sinkhorn}
 d^0 \leftarrow (q+\alpha -1)/ (A^Td^1+\beta),  \quad d^1 \leftarrow p/A d^0.
\end{align}
 The choice of $\beta$ determines the normalization of $s_j$. With $u_j=\log s_j$, we can show in a similar way as Theorem 2 of \citet{maystre2017choicerank} that \eqref{eq:regularized-log-likelihood} always has a unique maximizer whenever $\alpha>1$. Regarding the convergence, \citet{maystre2017choicerank} remarked that since their ChoiceRank algorithm can be viewed as an MM algorithm, it inherits the local linear convergence of MM algorithms \citep{lange2000optimization}, but ``a detailed investigation of convergence behavior is left for future works''. With the insights we develop in this paper, we can in fact provide an explanation for the validity of the Gamma priors from an optimization perspective. This perspective allows us to conclude directly that \eqref{eq:regularized-log-likelihood} always has a unique solution in the interior of the probability simplex, and that furthermore the iteration in \eqref{eq:regularized-sinkhorn} has global linear convergence. 
 Consider now the following regularized potential function
 \begin{align}
     g^R(d^0,d^1)	:= ((d^1)^{T}A+\beta (\mathbf{1}_m)^T)d^0-\sum_{i=1}^{n}p_{i}\log d^1_{i}-\sum_{j=1}^{m}(q_{j}+\alpha-1)\log d^0_{j}.
\end{align}
We can verify that by substituting the optimality condition of $d^1$ into $-g^R$, it reduces to the log posterior \eqref{eq:regularized-log-likelihood}. Moreover, the iteration \eqref{eq:regularized-sinkhorn} is precisely the alternating minimization algorithm for $g^R$. When $\alpha-1,\beta>0$, the reparameterized potential function
\begin{align*}
    \sum_{ij}(e^{-v_{i}}A_{ij}e^{u_{j}})+\beta\sum_{j}e^{u_{j}}+\sum_{i}p_{i}v_{i}-\sum_{j}(q_{j}+\alpha-1)u_{j}
\end{align*} 
is always coercive regardless of whether \cref{ass:matrix-existence} holds. Therefore, during the iterations \eqref{eq:regularized-sinkhorn}, $(u,v)$ stay \emph{bounded}. Moreover, the Hessian is 
\begin{align*} \begin{bmatrix}\sum_{j}e^{-v_{i}}A_{ij}e^{u_{j}} & -e^{-v_{i}}A_{ij}e^{u_{j}}\\
-e^{-v_{i}}A_{ij}e^{u_{j}} & \sum_{i}e^{-v_{i}}A_{ij}e^{u_{j}}+\beta e^{u_{j}}
\end{bmatrix}	\succ0,
\end{align*}
 which is now positive definite.  As a result, $g^{R}(u,v)$ is strongly convex and smooth, so that  \eqref{eq:regularized-sinkhorn} converges linearly. From the perspective of the matrix balancing problem, we have thus obtained a regularized version of Sinkhorn's algorithm, summarized in \cref{alg:regularized}, which is guaranteed to converge linearly to a finite solution $(D^1,D^0)$, even when \cref{ass:matrix-existence} does not hold for the triplet $(A,p,q)$. Moreover, the regularization also improves the convergence of Sinkhorn's algorithm even when it converges, as the regularized Hessian becomes more-behaved. This regularized algorithm could be very useful in practice to deal with real datasets that result in slow, divergent, or oscillating Sinkhorn iterations.
 \begin{algorithm}[tb]
\caption{Regularized Sinkhorn's Algorithm}
   \label{alg:regularized}
\begin{algorithmic}
   \STATE {\bfseries Input:}  $A, p, q,\alpha>1,\beta>0,\epsilon_{\text{tol}}$.
   \STATE {\bfseries initialize} $d^{0}\in\mathbb{R}_{++}^{m}$
   \REPEAT
   \STATE $d^{1} \leftarrow  p/( A d^0)$ 

   \STATE $d^{0}\leftarrow  (q+\alpha-1)/({A}^{T} d^{1}+\beta)$

   \STATE 
$\epsilon\leftarrow$  update of $(d^{0},d^1)$
\UNTIL{$\epsilon<\epsilon_{\text{tol}}$}
\end{algorithmic}
\end{algorithm}

\subsection{Regularization via Data Augmentation}
The connection between Bayesian methods and \emph{data augmentation} motivates us to also consider direct data augmentation methods. This is best illustrated in the choice modeling setting. Suppose for a choice dataset we construct participation matrix $A$, $p$ the counts of distinct choice sets, and $q$ the counts of each item being selected. We know that $(A,p,q)$ has a finite scaling solution if and only if  \cref{ass:strong-connected} holds, i.e., the directed comparison graph is strongly connected. We now propose the following modification of $(A,p,q)$ such that the resulting problem is always valid. 

First, if $A$ does not already contain a row equal to $\mathbf{1}_m^T$, i.e., containing all 1's, add this additional row to $A$. Call the resulting matrix $A'$. Then, expanding the dimension of $p$ if necessary, add $m\epsilon$ to the entry corresponding to $\mathbf{1}_m$, where we can assume for now that $\epsilon\geq 1$ is an integer.
This procedure effectively adds $m \epsilon$ ``observations'' that contain all $m$ items. For these additional observations, we let each item be selected exactly $\epsilon$ times. Luce's choice axiom guarantees that the exact choice of each artificial observation is irrelevant, and we just need to add $\epsilon \mathbf{1}_m$ to $q$. This represents augmenting each item with an additional $\epsilon$ ``wins'', resulting in the triplet $(A',p+(m\epsilon)\mathbf{e},q+\epsilon \mathbf{1}_m)$, where $\mathbf{e}$ is the one-hot indicator of the row $\mathbf{1}_m^T$ in $A'$. Now by construction, in any partition of $[m]$ into two non-empty subsets, any item from one subset is selected at least $\epsilon$ times over any item from the other subset. Therefore, \cref{ass:strong-connected} holds, and the maximum likelihood estimation problem, and equivalently the matrix balancing problem with $(A',p+(m\epsilon)\mathbf{e},q+\epsilon \mathbf{1}_m)$, is well-defined. This regularization method applies more generally to any non-negative $A$, even if it is not a participation matrix, i.e., binary. Although in the above construction based on choice dataset, $\epsilon$ is taken to be an integer, for the regularized matrix balancing problem with $(A',p+(m\epsilon)\mathbf{e},q+\epsilon \mathbf{1}_m)$, we can let $\epsilon \rightarrow 0$. %An interesting question is whether the sequence of solutions to the matrix balancing problems indexed by $\epsilon$ converges, and what point in the probability simplex it converges to, when the original triplet $(A,p,q)$ does not define a valid matrix balancing problem. 
%\zq{Future directions. Optimization: Can design regularization procedures that improves the algebraic connectivity for fixed data. Sampling: Can design sampling procedures that improve the algebraic connectivity for streaming data.}