\section{Related Work}
\label{sec:related-works}
This section includes an extensive review of related works in choice modeling and matrix balancing. Well-versed readers may skip ahead to the mathematical preliminaries (\cref{sec:formulations}) and our core results (\cref{sec:equivalence,sec:linear-convergence}).
\subsection{Choice Modeling}
Methods for aggregating choice and comparison data usually take one of two closely related approaches: maximum likelihood estimation of a statistical model or ranking according to the stationary distributions of a random walk on a Markov chain. Recent connections between maximum likelihood and spectral methods have put these two classes of approaches in increasingly close conversation with each other.

\textbf{Spectral Methods.}
The most well-known spectral method for rank aggregation is perhaps
the PageRank algorithm \citep{page1999pagerank}, which ranks web pages
based on the stationary distribution of a random walk on a
hyperlink graph. The use of stationary distributions also features in the work of \citet{dwork2001rank}, the Rank Centrality (RC) algorithm \citep{negahban2012iterative,negahban2016rank}, which generates consistent estimates for the Bradley--Terry--Luce
pairwise comparison model under assumptions on the sampling frame, and the Luce Spectral Ranking (LSR) and iterative LSR (I-LSR) algorithms of \citet{maystre2015fast} for choices from pairs as well as larger sets. Following that work, \citet{agarwal2018accelerated} proposed the Accelerated Spectral Ranking (ASR) algorithm with provably faster mixing times than RC and LSR, and better sample complexity bounds than \citet{negahban2016rank}.
\citet{knight2008sinkhorn} is an intriguing work partially motivated by \citet{page1999pagerank} that applies Sinkhorn's algorithm, which is central to the current work, to compute authority and hub scores similar to
those proposed by \citet{kleinberg1999authoritative} and \citet{tomlin2003new}, although the focus in \citet{knight2008sinkhorn} is on Markov chains rather than maximum likelihood estimation of choice models.
For ranking data, \citet{soufiani2013generalized} decompose rankings into pairwise comparisons and develop consistent estimators for Plackett--Luce models based on a generalized method of moments. Other notable works that make connections between Markov chains and 
choice modeling include \citet{blanchet2016markov} and \citet{ragain2016pairwise}.

\textbf{Maximum Likelihood Methods.}
Maximum likelihood estimation of the Bradley--Terry--Luce model 
dates back to \citet{zermelo1929berechnung}, \citet{dykstra1956note}, and \citet{ford1957solution}, which all give variants of the same iterative algorithm and prove its convergence to the MLE when the directed comparison graph is strongly connected. 
Much later, \citet{hunter2004mm} observed that their
algorithms are instances of the class of minorization-maximization (MM) algorithms, and develops MM algorithms for the Plackett--Luce model for ranking data, among others. \citet{vojnovic2020convergence} further investigate the convergence of the MM algorithm for choice models. \citet{newman2023efficient} proposes an alternative to the classical iterative algorithm for pairwise comparisons based on a reformulated moment condition, achieving impressive empirical speedups. \citet{negahban2012iterative} is arguably the first work that connects maximum likelihood estimation to Markov chains, followed by \citet{maystre2015fast}, whose spectral method is based on a balance equation interpretation of the optimality condition.
\citet{kumar2015inverting} consider the problem of inverting the stationary distribution of a Markov
chain, and embed the maximum likelihood problem of the Luce choice model into this
framework, where the MLEs parameterize the desired transition matrix. \citet{maystre2017choicerank} consider the estimation of a network choice model with similarly parameterized random walks. Lastly, a vast literature in econometrics on discrete choice also considers different aspects of the ML estimation problem. In particular, the present paper is related to the Berry--Levinsohn--Pakes (BLP) framework of \citet{berry1995automobile}, well-known in econometrics. The matrix balancing interpretation of maximum likelihood estimation of choice models that we develop in this paper connects many of the aforementioned works.

Besides the optimization of a given maximum likelihood problem, there have also been extensive studies of the
statistical properties of the maximum likelihood estimator for these choice problems. \citet{hajek2014minimax}
prove that the MLE is minimax optimal for $k$-way rankings, and
\citet{rajkumar2014statistical} show that the MLE for Bradley--Terry--Luce can recover the correct \emph{ranking} under model mis-specification
when noise is ``bounded''. As a byproduct of analysis for a context-dependent generalization of the Luce choice model, \citet{seshadri2020learning} obtain tight expected risk and tail risk bounds for the MLEs of Luce choice models (which they call MNL) and Plackett--Luce ranking models, extending and improving upon previous works by \citet{hajek2014minimax,shah2015estimation,vojnovic2016parameter}. 

Our present work is primarily concerned with the optimization aspects of the maximum likelihood problem. Nonetheless, the importance of \emph{algebraic connectivity}---as quantified by the Fiedler eigenvalue \citep{fiedler1973algebraic}---in the results of \citet{shah2015estimation,seshadri2020learning} as well as \citet{vojnovic2020convergence} provides motivations in our convergence analysis of Sinkhorn's algorithm for matrix balancing. 

Lastly, a short note on terminology. Even though a choice model based on \eqref{eq:Luce} is technically a  ``multinomial logit model'' with only intercept terms \citep{mcfadden1973conditional}, there are subtle differences. 
When \eqref{eq:Luce} is applied to model
ranking and choice data with distinct items, each observation $i$ usually consists of a possibly \emph{different}
subset $S_i$ of the universe of all alternatives, so that there is a large number of different configurations of the choice menu in the dataset. On the other hand, common applications of multinomial logit models, such as classification models in statistics and machine learning \citep{bishop2006pattern} and discrete choice models in econometrics \citep{mcfadden1973conditional}, often deal with repeated observations consisting of the \emph{same} number of alternatives. However, these alternatives now possess ``characteristics'' that vary across observations, which are often mapped \emph{parametrically} to the scores in \eqref{eq:Luce}. 
 In this paper, we primarily use the term \emph{Luce choice model} to refer to the model \eqref{eq:model}, although it is also called MNL (for multinomial logit) models in some works. We refrain from using the term MNL to avoid confusion with parametric models for featurized items used in ML and econometrics.

\subsection{Matrix Balancing}
The matrix balancing problem we study in this paper and its variants \citep{ruiz2001scaling,bradley2010algorithms} underlie a diverse range of applications from different disciplines. The  question of scaling rows and columns of a matrix $A$ so that the resulting matrix has target row and column norms $p,q$ has been studied as early as the 1930s, and continue to intrigue researchers from different backgrounds. The present paper only contains a partial survey of the vast literature on the topic. \citet{schneider1990comparative} and \citet{idel2016review} provide excellent discussions of many applications. In \cref{app:related-works}, we present concise summaries of some applications to illustrate the ubiquity of the matrix balancing problem. 

The standard iterative algorithm for the matrix balancing problem has been rediscovered independently quite a few times. As a result, it has domain-dependent names, including the iterative proportional fitting (IPF) procedure \citep{deming1940least}, biproportional fitting \citep{bacharach1965estimating} and the RAS algorithm \citep{stone1962multiple}, but is perhaps most widely known as \emph{Sinkhorn's algorithm} \citep{sinkhorn1964relationship,cuturi2013sinkhorn}. A precise description can be found in \cref{alg:scaling}.
This algorithm is also closely related to relaxation and coordinate descent type methods for solving the dual of entropy minimization problems \citep{bregman1967relaxation,cottle1986lagrangean,tseng1987relaxation,luo1992convergence}, as well as message passing and belief propagation algorithms in distributed optimization \citep{balakrishnan2004polynomial,agarwal2018accelerated}. 

The convergence behavior of Sinkhorn's algorithm has been extensively studied by \citet{sinkhorn1964relationship,bregman1967proof,lamond1981bregman,franklin1989scaling,ruschendorf1995convergence,kalantari2008complexity,knight2008sinkhorn,pukelsheim2009iterative,altschuler2017near,chakrabarty2021better,leger2021gradient}, among many others. For $A$ with strictly \emph{positive} entries, \citet{franklin1989scaling} first established the global linear convergence of Sinkhorn's algorithm in the Hilbert projective metric $d$. In particular, if $r^{(t)}$ denotes the row sum of the scaled matrix after $t$ iterations of Sinkhorn's algorithm that enforce column constraints, then 
 \begin{align}
 \label{eq:hilbert-contraction} 
   d(r^{(t)}, p) \leq  \lambda^{t} \cdot d(r^{(0)}, p)
 \end{align}
  for some $\lambda \in (0,1)$ dependent on $A$. See \citet{bushell1973hilbert} for details on the Hilbert metric. \citet{altschuler2017near} and \citet{chakrabarty2021better} show global sub-linear convergence in terms of iteration complexity bounds on the $\ell^1$ distance independent of matrix dimension. 
  
  However, the matter of convergence is more delicate when the matrix contains zero entries, and additional assumptions on the problem structure are required. For non-negative $A$, convergence is first established by \citet{sinkhorn1967concerning} in the special case of  \emph{square} $A$ and uniform $p=q=\mathbf{1}_n=\mathbf{1}_m$. Their necessary and sufficient condition is that $A$
has \emph{total support}, i.e., any non-zero entry of $A$ must be in $(A_{1\sigma(1)},A_{2\sigma(2)},\dots,A_{n\sigma(n)})$
for some permutation $\sigma$. \citet{soules1991rate} shows the convergence is linear, and \citet{knight2008sinkhorn} provides an explicit and tight \emph{asymptotic} linear convergence rate in terms of the sub-dominant (second largest) singular value of the scaled doubly stochastic matrix $D^0AD^1$. However, no asymptotic linear convergence rate is previously known for non-uniform marginals.

For general non-negative matrices and non-uniform marginals, the necessary and sufficient conditions for the matrix balancing problem that generalize that of \citet{sinkhorn1967concerning} have been studied by \citet{thionet1964note,bacharach1965estimating,brualdi1968convex,menon1968matrix,djokovic1970note,sinkhorn1974diagonal,balakrishnan2004polynomial,pukelsheim2009iterative}, and convergence of Sinkhorn's algorithm under these conditions is well-known. Connecting Sinkhorn's algorithm to dual coordinate descent for entropy minimization, \citet{luo1992convergence} show that the dual objective converges linearly with some unknown rate $\lambda$ when finite scalings $D^0,D^1$ exist. However, their result is implicit and there are no results that quantify this rate $\lambda$, even for special classes of non-negative matrices. When convergence results on positive matrices in previous works are applied to non-negative matrices, the bounds often blow up or become degenerate as soon as $\min_{ij}A_{ij} \downarrow 0$. For example, in \eqref{eq:hilbert-contraction} the contraction factor $\lambda \rightarrow 1$ when $A$ contains zero entries. In contrast, under a \emph{weaker} condition that guarantees the convergence of Sinkhorn's algorithm, \citet{leger2021gradient} gives a \emph{quantitative} global $\mathcal{O}(1/t)$ bound for non-negative matrices. It remains to reconcile the results of these works and characterize the linear rate $\lambda$ for non-negative $A$.

Our work precisely fills the gaps left by these works. The global linear convergence result in \cref{thm:global-convergence} establishes a contraction like \eqref{eq:hilbert-contraction} whenever finite scalings $D^0,D^1$ exist, and characterize $\lambda$ in terms of the algebraic connectivity. Moreover, the asymptotic linear rate in \cref{thm:convergence} directly extends the result of \citet{knight2008sinkhorn}. See \cref{tab:convergence-summary} for a detailed summary and comparison of the convergence results in previous works and this paper.

The dependence of Sinkhorn's convergence rate on spectral properties of graphs can be compared to convergence results in the  literature on decentralized optimization and gossip algorithms, where a spectral gap quantifies the convergence rate \citep{boyd2006randomized,xiao2007distributed}.
 
