\section{Preliminaries on Choice Modeling and Matrix Balancing}
\label{sec:formulations}
We start by providing brief but self-contained introductions to the two main subjects of this paper,
choice modeling and matrix balancing, including their respective underlying mathematical problems and assumptions. Then we formally establish their equivalence in \cref{sec:equivalence}.
\subsection{Maximum Likelihood Estimation of Luce Choice Models}
\label{subsec:Luce}
In the Luce choice modeling framework, we have $n$ observations $\{(j_i,S_i)\}_{i=1}^n$, each consisting of a choice set $S_{i}\subseteq \{1,\dots,m\}=[m]$ that is a subset
of the total $m$ alternatives/items/objects, and the alternative selected, denoted by $j_i \in S_i$.
 The choice probability is prescribed by Luce's axiom of choice given model parameter $s\in\mathbb{R}^m_{++}$
 in the interior of the probability simplex $\Delta_m$:
\begin{align*}
\Pr(j_i, S_i) & =\frac{s_{j_i}}{\sum_{k\in S_{i}}s_{k}},
\end{align*}
and the likelihood of the observed data is thus given by 
\begin{align}
\label{eq:model}
L(s;\{(j_i,S_i)\}_{i=1}^n):=& \prod_{i=1}^{n}\frac{s_{j_i}}{\sum_{k\in S_{i}}s_{k}}.
\end{align} 
 Importantly, the fact that \eqref{eq:model} has potentially different choice sets $S_i$ across observations have   structure-dependent implications for both the optimization and the statistical efficiencies of the model.

The log-likelihood of the choice model described by \eqref{eq:model} is 
\begin{align}
\label{eq:log-likelihood}
\ell(s):=\log L(s) & =\sum_{i=1}^{n}\log s_{j_i}-\log\sum_{k\in S_{i}}s_{k}.
\end{align}
To estimate $s = \{s_{1},\dots,s_m\}$, we focus on the maximum likelihood estimation approach, which maximizes \eqref{eq:log-likelihood} over the interior of the probability simplex. If we reparameterize $\exp(u_{j})=s_{j}$, it is obvious that \eqref{eq:log-likelihood} is concave in $u$. However, to ensure the log-likelihood  \eqref{eq:log-likelihood} has a unique maximizer in the \emph{interior} of the simplex, additional assumptions on the comparison structure of the dataset $\{(j_i,S_i)\}_{i=1}^n$ are needed. 
The following classic condition is necessary and sufficient for the maximum likelihood problem to be well-posed.
\begin{assumption}[\textbf{Strong Connectivity}]
\label{ass:strong-connected}
In any partition of $[m]$ into two nonempty subsets $S$ and its complement $S^C$,
some $j \in S$ is \emph{selected} at least once over some
$k \in S^C$. Equivalently, the {\it directed comparison graph}, with items as vertices and an edge $j\rightarrow k$ iff $k$ is selected in some $S_i$ for which $j,k\in S_i$, is strongly connected. 
\end{assumption}
\cref{ass:strong-connected} is standard in the literature \citep{hunter2004mm,noothigattu2020axioms} and appeared as early as the work of \citet{zermelo1929berechnung} and \citet{ford1957solution} for pairwise comparisons. \citet{hunter2004mm} shows that \cref{ass:strong-connected} is necessary and sufficient for the upper-compactness
of \eqref{eq:log-likelihood}, which guarantees the existence of a maximizer in the interior of the probability simplex. In fact, when an interior maximizer exists, it is also \emph{unique}, since \cref{ass:strong-connected} implies the following weaker condition, which guarantees the strict concavity of \eqref{eq:log-likelihood}.
\begin{assumption}[\textbf{Connectivity}]
\label{ass:weak-connected}
In any partition of $[m]$ into two nonempty subsets $S$ and $S^C$,
some $j\in S$ and and some $k\in S^C$ \emph{appear} in the same choice set $S_i$ for some $i$. 
\end{assumption}
The intuitions provided by \citet{ford1957solution} and \citet{hunter2004mm} are helpful for understanding Assumptions \ref{ass:strong-connected} and \ref{ass:weak-connected}. If items from some $S\subsetneq [m]$ are never compared with those in $S^C$, i.e., never appeared together in any choice set $S_i$, it is impossible to rank across the two subsets. In this case, we can rescale the relative weights of $S$ and $S^C$ of an interior maximizer and obtain another maximizer. On the other hand, if items in $S$ are always preferred to those in $S^C$, we can increase the likelihood by scaling $s_j$ for items $j\in S^C$ towards $0$, and no maximizer in the \emph{interior} of the probability simplex exists. 
Nevertheless, a boundary solution can still exist. This case turns out to be important in the present work: in the equivalent matrix balancing problem, it corresponds to the  \emph{slowdown} regime of Sinkhorn's algorithm, where scalings diverge but the scaled matrix converges (\cref{sec:linear-convergence}). 

\cref{ass:weak-connected} also has a concise graph-theoretic interpretation. Define the weighted \emph{undirected comparison graph} $G_c$ on $m$ vertices with adjacency matrix $A^c$ given by 
\begin{align*}
{A}^c_{jk} & =\begin{cases}
0 & j=k\\
|\{i\mid j,k\in S_i\}| & j\neq k,
\end{cases}
\end{align*}
In other words, there is an undirected edge between $j$ and $k$ if and only if they are both in some choice set, with the edge weight equal to the number of their co-occurrences. We can verify that \cref{ass:weak-connected} requires $G_c$ to be connected.

Under these standard assumptions, previous works have studied the statistical efficiency of the MLE \citep{hajek2014minimax,shah2015estimation,seshadri2020learning} as well as the computational efficiency of the MM algorithm \citep{vojnovic2020convergence}. In both cases, the algebraic connectivity of $G_c$, quantified by the second smallest eigenvalue of the graph Laplacian of $G_c$, plays an important role. See \cref{subsec:graph-laplacian} for details.
In \cref{sec:linear-convergence}, we focus on the classic Sinkhorn's algorithm for matrix balancing, and show that its convergence is also characterized by the algebraic connectivity of a bipartite graph. In \cref{subsec:regularization}, we extend our framework to settings where \cref{ass:strong-connected} does not hold, by introducing Gamma priors on $s_j$, inspired by \citet{caron2012efficient} and \citet{maystre2017choicerank}. These priors regularize the maximum likelihood problem and guarantee that a unique solution in the interior of the probability simplex always exists. They also speed up the associated iterative algorithm by improving the algebraic connectivity.

\subsection{The Canonical Matrix Balancing Problem}
Matrix balancing is a classic problem that shows up in a wide range of disciplines. See \cref{app:related-works} for a short survey on some applications. 
The underlying mathematical problem can be stated concisely in matrix form as:
\begin{quote} 
Given positive vectors $p \in \mathbb{R}_{++}^n,q \in \mathbb{R}_{++}^m$with $\sum_i p_i=\sum_j q_j$ and non-negative matrix $A\in \mathbb{R}_+^{n\times m}$, find positive diagonal matrices 
$D^{1}$, $D^{0}$ satisfying the conditions $D^{1}AD^{0}\cdot\mathbf{1}_m=p$
and $D^{0}A^{T}D^{1}\cdot\mathbf{1}_n=q$.\end{quote}
We henceforth refer to the above as the ``canonical'' matrix balancing problem. It seeks positive row and column scalings of an (entry-wise) non-negative rectangular matrix $A$ such that the scaled matrix has positive target row and column sums $p$ and $q$. Other variants of the problem replace the row and column sums (the 1-norm) with other norms \citep{bauer1963optimally,ruiz2001scaling}. Note that for any $c>0$, $(D^0/c,cD^1)$ 
 is also a solution whenever $(D^0,D^1)$ is. A finite positive solution $(D^{0},D^{1})$ to the canonical matrix balancing problem is often called a \emph{direct scaling}.

 The structure of the matrix balancing problem suggests a simple iterative scheme: starting
from any initial positive diagonal $D^{0}$, invert $D^{1}AD^{0}\mathbf{1}_m = p$ using $p/(AD^{0}\mathbf{1}_m)$
 to update $D^{1}$. Then invert $D^{0}A^{T}D^{1}\mathbf{1}_n=q$ using  $q/(A^{T}D^{1}\mathbf{1}_n)$ to compute the new estimate of $D^{0}$, and repeat the procedure. Here divisions involving two vectors of the same length are \emph{entry-wise}. This simple scheme is precisely Sinkhorn's algorithm, described in \cref{alg:scaling}, where vectors $d^0,d^1$ are the diagonals of $D^0,D^1$.
 \begin{algorithm}[tb]
\caption{Sinkhorn's Algorithm}
   \label{alg:scaling}
\begin{algorithmic}
   \STATE {\bfseries Input:}  $A, p, q,\epsilon_{\text{tol}}$.
   \STATE {\bfseries initialize} $d^{0}\in\mathbb{R}_{++}^{m}$
   \REPEAT
   \STATE $d^{1} \leftarrow  p/( A d^0)$ 
   \STATE $d^{0}\leftarrow  q/({A}^{T} d^{1})$
   \STATE 
$\epsilon\leftarrow$  update of $(d^{0},d^1)$
\UNTIL{$\epsilon<\epsilon_{\text{tol}}$}
\end{algorithmic}
\end{algorithm}
An important dichotomy occurs depending on whether the entries of $A$ are strictly positive. If $A$ contains no zero entries, then direct scalings and a unique scaled matrix $D^1AD^0$ always exist \citep{sinkhorn1964relationship}. Moreover, Sinkhorn's algorithm converges linearly \citep{franklin1989scaling}. 
On the other hand, when $A$ contains zero entries, the canonical problem becomes more complicated. Additional conditions are needed to guarantee meaningful solutions, and the convergence behavior of Sinkhorn's algorithm is less understood.  Well-posedness of the matrix balancing problem has been studied by \citet{brualdi1968convex,sinkhorn1974diagonal,pukelsheim2009iterative}, among others, who characterize the following equivalent existence conditions.
\begin{assumption}[\textbf{Strong Existence}]
\label{ass:matrix-existence}
\textbf{(a)} There exists a non-negative matrix $A'\in \mathbb{R}_+^{n\times m}$ with the same zero patterns as $A$ and with row and column sums $p$ and $q$. Or, equivalently,

\textbf{(b)} For every pair of sets of indices $N \subsetneq [n]$ and $M \subsetneq [m]$ such that $A_{ij}=0$ for $i\notin N$ and $j\in M$, $\sum_{i\in N}p_i \geq \sum_{j\in M}q_j$, with equality iff $A_{ij}=0$ for all $i \in N$ and $j \notin M$ as well.
\end{assumption} 
It is well-known in the matrix balancing literature that the above two conditions are equivalent, and that a positive finite solution $(D^{0},D^{1})$ to the canonical problem exists iff they hold. See, for example, Theorem 6 in \citet{pukelsheim2009iterative}. Clearly, \cref{ass:matrix-existence}(a) is the minimal necessary condition when a solution to the matrix balancing problem exists. \cref{ass:matrix-existence}(b) is closely connected to conditions for perfect matchings in bipartite graphs \citep{hall1935representatives}. In flow networks \citep{gale1957theorem,ford1956maximal,ford1957simple}, it is a capacity constraint that
guarantees the maximum flow on a bipartite graph with source and sink is equal to $\sum_i p_i=\sum_j q_j$ and with positive flow on every edge \citep{idel2016review}.  The bipartite graph, denoted by $G_b$, is related to $A$ through its  adjacency matrix $A^b \in \mathbb{R}^{(n+m)\times(n+m)}$ given by
\begin{align*}
    A^b := \begin{bmatrix}\mathbf{0} & {A}\\
{A}^{T} & \mathbf{0}
\end{bmatrix}.	
\end{align*}
See \cref{subsec:graph-laplacian} for a detailed discussion. The structure of $G_b$ turns out to be important for the linear convergence rate of Sinkhorn's algorithm (\cref{sec:linear-convergence}).

On the other hand, the necessary and sufficient condition for uniqueness of finite scalings essentially requires that $A$ is not block-diagonal, and guarantees that $G_b$ is \emph{connected}.
\begin{assumption}[\textbf{Uniqueness}]
\label{ass:matrix-uniqueness}
 $D^{0}$ and $D^{1}$ are unique modulo normalization iff $A$ is indecomposable, i.e., there does not exist permutation matrices $P,Q$ such that $PAQ$ is block diagonal.
\end{assumption} 
 
With a proper introduction to both problems, we are now ready to formally establish the equivalence between Luce choice model estimation and matrix balancing in the next section. In \cref{sec:linear-convergence}, we return to Sinkhorn's algorithm for the matrix balancing problem, and settle important open problems concerning its linear convergence for non-negative $A$.  In \cref{sec:connections}, we discuss further connections of matrix balancing and Sinkhorn's algorithm to choice modeling and optimization.