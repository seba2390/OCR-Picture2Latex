\section{Connecting Choice Modeling and Matrix Balancing}
\label{sec:equivalence}

In this section, we formally establish the connection between choice modeling and matrix balancing. We show that
maximizing the log-likelihood \eqref{eq:log-likelihood} is equivalent to solving a canonical matrix balancing problem. We also precisely describe the correspondence between the relevant conditions in the two problems. In view of this equivalence, we show that Sinkhorn's algorithm, when applied to estimate Luce choice models, is in fact a \emph{parallelized} generalization of the classic iterative algorithm for choice models, dating back to \citet{zermelo1929berechnung,dykstra1956note,ford1957solution}, and studied extensively also by \citet{hunter2004mm,vojnovic2020convergence}.

\subsection{Maximum Likelihood Estimation of Luce Choice Model as Matrix Balancing}
\label{subsec:reformulation}
The optimality conditions for maximizing the log-likelihood \eqref{eq:log-likelihood} for each $s_j$ are given by
\begin{align*}
\partial_{s_{j}}\ell(s)=\sum_{j\mid (j,S_i)}\frac{1}{s_{j}}-\sum_{i\mid j\in S_{i}}\frac{1}{\sum_{k\in S_{i}}s_{k}} & =0.
\end{align*}
 Multiplying by $s_{j}$ and dividing by $1/n$, we have 
\begin{align}
\label{eq:optimality-original}
\frac{W_j}{n} & = \frac{1}{n} \sum_{i\mid j\in S_{i}}\frac{s_{j}}{\sum_{k\in S_{i}}s_{k}},
\end{align}
where $W_j:=|\{i\mid (j,S_i)\}|$ is the number of observations where $j$ is selected.

Note that in the special case where $S_i\equiv [n]$, i.e., every choice set contains \emph{all} items, the MLE simply reduces to the familiar empirical frequencies $\hat s_j = {W_j}/{n}$. However, when the choice sets $S_i$ vary, no closed form solution to \eqref{eq:optimality-original} exists, which is the primary motivation behind the long line of works on the algorithmic problem of solving \eqref{eq:optimality-original}. 

With varying $S_i$, we can interpret the optimality condition as requiring the \emph{observed} frequency of $j$ being chosen (left hand side) be equal to the conditional \emph{expected} probability of $j$ being chosen among all observations $i$ where it is part of the choice set $S_i$ (right hand side). In addition, note that since the optimality condition in \eqref{eq:optimality-original} only involves the \emph{frequency} of selection, distinct datasets could yield the same optimality condition and hence the same MLEs. For example, suppose that two alternatives $j$ and $k$ both appear in choice sets
$S_{i}$ and $S_{i'}$, with $j$ selected in $S_{i}$ and  $k$ selected in $S_{i'}$. Then switching
the choices in $S_{i}$ and $S_{i'}$ does not alter the likelihood and optimality conditions. This feature holds more generally with longer cycles of items and choice sets, and can be viewed as a consequence of the context-independent nature of Luce's choice axiom. In some sense, it is also the underpinning of many works in economics that estimate choice models based on \emph{marginal} sufficient statistics. A famous example is  \citet{berry1995automobile}, which estimates consumer preferences using only \emph{aggregate} market shares of products.

In practice, the choice sets $S_i$ of many observations may be identical to each other. Because \eqref{eq:optimality-original} only depends on the total winning counts of items, we may aggregate over observations with the same $S_i$:
\begin{align*}
\sum_{i\mid j\in S_{i}}\frac{s_{j}}{\sum_{k\in S_{i}}s_{k}} & = \sum_{i'\mid j\in S^\ast_{i'}} R_{i'} \cdot \frac{s_{j}}{\sum_{k\in S^\ast_{i'}}s_{k}},
\end{align*}
where each $S_{i'}^\ast$ is a unique choice set that appears in $R_{i'}\geq 1$ observations, for $i'=1,\dots,n^\ast \leq n$. By construction, $\sum_{i'=1}^{n^\ast}R_{i'} =n$. Note, however, that the selected item could vary across different appearances of $S_i^\ast$, but the optimality conditions only involve each item's winning count $W_j$. From now on, we assume this reduction and drop the $^\ast$ superscript. In other words, we assume that we observe $n$ unique choice sets, and choice set $S_i$ has \emph{multiplicity} $R_i$. The resulting problem has optimality conditions
\begin{align}
\label{eq:optimality}
{W_j} & = \sum_{i\mid j\in S_{i}}{R_i} \cdot \frac{s_{j}}{\sum_{k\in S_{i}}s_{k}}.
\end{align}
We are now ready to reformulate \eqref{eq:optimality} as a canonical matrix balancing problem. Define $p\in\mathbb{R}^{n}$ as $p_i=R_i$, i.e., the number of times choice set $S_i$ appears in the data. Define
$q\in\mathbb{R}^{m}$ as $q_j={W_j}$,
i.e., the number of times item $j$ was \emph{selected} in the data. By construction we have $\sum_i p_i=\sum_j q_j$, and $p_i,q_j>0$ whenever \cref{ass:strong-connected} holds.

Now define the $n\times m$ binary matrix $A$ by
$A_{ij}=1\{j\in S_{i}\}$, so the $i$-th row of $A$
is the indicator of which items appear in the (unique) choice set $S_i$, and the $j$-th column of $A$ is the indicator of which choice sets
item $j$ appears in. We refer to this $A$ constructed from a choice dataset as the \emph{participation matrix}. By construction, $A$ has distinct rows, but may still have identical columns. If necessary, we can also remove repeated columns by ``merging'' items and their win counts. Their estimated scores can be computed from the score of the merged item proportional to their respective win counts.

Let $D^{0}\in\mathbb{R}^{m\times m}$ be the diagonal matrix with
$D_{j}^{0}=s_{j}$ and $D^{1}\in\mathbb{R}^{n\times n}$ be the
diagonal matrix with $D_{i}^{1}={R_i}/{\sum_{k\in S_{i}}s_{k}}$,
and define the scaled matrix
\begin{align}
\label{eq:scaled-matrix}
\hat{A} & :=D^{1}AD^{0}.
\end{align}
The matrices $D^{1}$ and $D^{0}$ are scalings of rows and columns
of $A$, respectively, and
\begin{align*}
    \hat{A}_{ij} = \frac{R_i}{\sum_{k\in S_{i}}s_{k}}\cdot1\{j\in S_{i}\}\cdot s_{j}.
\end{align*}
The key observation is that the optimality condition \eqref{eq:optimality} can be rewritten as
\begin{align}
\label{eq:bridge}
\hat{A}^T \mathbf{1}_n & = q.
\end{align}
Moreover, by construction $\hat{A}$ also satisfies
\begin{align}
\label{eq:marginal}
\hat{A}\mathbf{1}_m & = p.
\end{align}
 Therefore, if $s_j$'s satisfy the optimality conditions for maximizing \eqref{eq:log-likelihood}, then $D^0,D^1$  defined above solve the matrix balancing problem in \cref{eq:scaled-matrix,eq:bridge,eq:marginal}. Moreover, the converse is also true, and we thus establish the equivalence between choice maximum likelihood estimation and matrix balancing. All omitted proofs appear in \Cref{app:proofs}.
\begin{theorem}
\label{prop:mle-scaling}
Let $p\in\mathbb{R}^{n}$ with $p_{i}=R_i$, $q\in\mathbb{R}^{m}$ with $q_{j}=W_j$, and 
$A\in\mathbb{R}^{n\times m}$
with $A_{ij}=1\{j\in S_{i}\}$ be constructed from the choice dataset. Then  $D^{0},D^{1}>0$ with $\sum_j D_j^0=1$ solves the matrix balancing problem
\begin{align}
\label{eq:equation-system}
\begin{split}
\hat{A} & =D^{1}AD^{0}\\
\hat{A}\mathbf{1}_m & =p\\
\hat{A}^{T} \mathbf{1}_n & =q
\end{split}
\end{align}
if and only if $s \in \Delta_m$ with $s_j=D^0_j$ satisfies the optimality condition \eqref{eq:optimality} of the ML estimation problem.
\end{theorem}
In particular, \eqref{eq:log-likelihood} has a unique maximizer $s$ in the interior of the probability simplex if and only if \eqref{eq:equation-system} has a unique normalized solution $D^0$ as well. The next question, naturally, is then how \cref{ass:strong-connected} and \cref{ass:weak-connected} for choice modeling are connected to \cref{ass:matrix-existence} and \cref{ass:matrix-uniqueness} for matrix balancing.
\begin{theorem}
\label{thm:necessary-and-sufficient}
Let (A,p,q) be constructed from the choice dataset as in \cref{prop:mle-scaling}, with $p,q>0$. \cref{ass:weak-connected} is equivalent to \cref{ass:matrix-uniqueness}. Furthermore, \cref{ass:strong-connected} holds if and only if $(A,p,q)$ satisfy \cref{ass:matrix-existence} and $A$ satisfies \cref{ass:matrix-uniqueness}.
\end{theorem} 
Thus when the ML estimation problem is cast as a matrix balancing problem, \cref{ass:matrix-existence} exactly characterizes the \emph{gap} between \cref{ass:weak-connected} and \cref{ass:strong-connected}. 
 We provide some intuition for \cref{thm:necessary-and-sufficient}. When we construct a triplet $(A,p,q)$ from a choice dataset, with $p$ the numbers of appearances of unique choice sets and $q$ the winning counts, \cref{ass:matrix-uniqueness} precludes the possibility of partitioning the items into two subsets that never get compared with each other, i.e., \cref{ass:weak-connected}. Then \cref{ass:matrix-existence} requires that whenever a strict subset $M\subsetneq [m]$ of objects only appear in a strict subset $N\subsetneq [n]$ of the observations, their total winning counts are \emph{strictly} smaller than the total number of these observations, i.e., there is some object $j\notin M$ that is selected in $S_i$ for some $i\in N$, which is required by \cref{ass:strong-connected}.

Interestingly, while \cref{ass:strong-connected} requires the directed comparison graph, defined by the $m\times m$ matrix of counts of item $j$ being chosen over item $k$, to be strongly connected, the corresponding conditions for the equivalent matrix balancing problem concern the $n\times m$ participation matrix $A$ and positive vectors $p,q$, which do not explicitly encode the specific \emph{choice} of each observation. This apparent discrepancy is due to the fact that $(A,p,q)$ form the \emph{sufficient statistics} of the Luce choice model. In other words, there can be more than one choice dataset with the same optimality condition \eqref{eq:optimality} and $(A,p,q)$ defining the equivalent matrix balancing problem.

\textbf{Remark.} This feature where ``marginal'' quantities constitute the sufficient statistics of a parametric model is an important one that underlies many works in economics and statistics \citep{kullback1997information,stone1962multiple,good1963maximum,birch1963maximum,theil1967economics,fienberg1970iterative,berry1995automobile,fofana2002balancing,maystre2017choicerank}. It makes the task of estimating a \emph{joint} model from marginal quantities feasible. This feature is useful because in many applications, only marginal data is available due to high sampling cost or privacy reasons. 

Having formulated a particular matrix balancing problem from the estimation problem given choice data, we may ask how one can go in the other direction. In other words, when/how can we construct a ``choice dataset'' whose sufficient statistics is a given triplet $(A,p,q)$? First off, for $(A,p,q)$ to be valid sufficient statistics of a Luce choice model, $p,q$ need to be positive integers. Moreover, $A$ has to be a binary matrix, with each row containing at least two non-zero elements (valid choice sets have at least two items). Given such a $(A,p,q)$ satisfying Assumptions \ref{ass:matrix-existence} and \ref{ass:matrix-uniqueness}, a choice dataset can be constructed efficiently. Such a procedure is described, for example, in \citet{kumar2015inverting}, where $A$ is motivated
by random walks on a graph instead of matrix balancing (\cref{sec:connections}). Their construction relies on finding the max flow on the bipartite graph $G_b$. For rational $p,q$, this maximum flow can be found efficiently in polynomial time \citep{balakrishnan2004polynomial,idel2016review}. Moreover, the maximum flow implies a matrix $A'$ satisfying \cref{ass:matrix-existence}(a), thus providing a feasibility certificate for the matrix balancing problem as well. 

We have thus closed the loop and fully established the equivalence of the maximum likelihood estimation of Luce choice models and the canonical matrix balancing problem.
\begin{corollary}
    There is a one-to-one correspondence between classes of maximum likelihood estimation problems with the same optimality conditions \eqref{eq:optimality} and canonical matrix balancing problems with $(A,p,q)$, where $A$ is a valid participation matrix and $p,q>0$ have integer entries. 
\end{corollary}
We next turn our attention to the algorithmic connections between choice modeling and matrix balancing. 
\subsection{Algorithmic Connections between Matrix Balancing and Choice Modeling}
\label{subsec:IPF}
 Given the equivalence between matrix balancing and choice modeling, we can naturally consider applying Sinkhorn's algorithm to maximize \eqref{eq:log-likelihood}. In this case, one can verify that the updates in each full iteration of \cref{alg:scaling} reduce algebraically to
\begin{align}
\label{eq:scaling-iteration}
s_{j}^{(t+1)} & =W_{j}/\sum_{i\mid j\in S_{i}}\frac{R_i}{\sum_{k\in S_{i}}s_{k}^{(t)}}
\end{align}
 in the $t$-th iteration. 
 Comparing \eqref{eq:scaling-iteration} to the optimality condition in \eqref{eq:optimality}, which we recall is given by 
\begin{align*}
 W_j & =\sum_{i\mid j\in S_{i}} R_i\frac{s_{j}}{\sum_{k\in S_{i}}s_{k}}= s_{j} \cdot \sum_{i\mid j\in S_{i}}\frac{R_i}{\sum_{k\in S_{i}}s_{k}},
 \end{align*}
we can therefore interpret the iterations as simply dividing the winning count $W_j$ by the coefficient of $s_j$ on the right repeatedly, in the hope of converging to a \emph{fixed point}. A similar intuition was given by \citet{ford1957solution} in the special case of pairwise comparisons. Indeed, the algorithm proposed by \citet{ford1957solution} is a cyclic variant of \eqref{eq:scaling-iteration} applied to pairwise comparisons. However, this connection is mainly algebraic, as the optimality condition in \citet{ford1957solution} does not admit a reformulation as the matrix balancing problem in \eqref{eq:equation-system}.

In \cref{sec:connections}, we provide further discussions on the connections of Sinkhorn's algorithm to existing frameworks and algorithms in the choice modeling literature, and connect it to distributed optimization as well. We demonstrate that many existing algorithms for Luce choice model estimation are in fact special cases or analogs of Sinkhorn's algorithm. These connections also illustrate the many interpretations of Sinkhorn's algorithm, e.g., as a distributed optimization algorithm as well as a minorization-majorization (MM) algorithm. However, compared to most algorithms for choice modeling discussed in this work, Sinkhorn's algorithm is more general as it applies to non-binary $A$ and non-integer $p,q$, and has the critical advantage of being paralellized and distributed, hence more efficient in practice.

The mathematical and algorithmic connections between matrix balancing and choice modeling we establish in this paper allow the transfusion of ideas in both directions. For example, inspired by regularized maximum likelihood estimation \citep{maystre2017choicerank}, we propose a regularized version of Sinkhorn's algorithm in \cref{subsec:regularization}, which is guaranteed to converge even when the original Sinkhorn's algorithm does not converge. Moreover, the importance of algebraic connectivity in quantifying estimation and computation efficiency in choice modeling motivates us to solve some important open problems on the convergence of Sinkhorn's algorithm. We turn to this topic next. 