\section{Conclusions}


We have provided an extensive analysis of the scaling and transfer behavior of neural operator models on multiple PDE systems.
%
This involved characterizing behavior as a function of 
model size,
downstream dataset size, 
underlying physics of the downstream tasks in relation to pre-training, 
the adaptation of these models to multiple downstream PDEs, and 
how all these behaviors scale with relevant problem and model parameters.
%
Among other things, we have shown that it is possible and beneficial to develop more general SciML models capable of solving multiple tasks with the \emph{same} set of weights, even when downstream tasks involve small-to-moderate distribution shifts relative to the pre-training data. 
%
All in all, this demonstrates the potential of the ``pre-train and fine-tune'' paradigm for SciML problems, paving a path towards building SciML foundation models.
%
Moving forward, many questions remain. 
These include further exploration of model architectures (balancing expressivity and flexibility), pre-training protocols (including self-supervision components at scale), fine-tuning strategies, and the integration of these within a specific compute (and memory) envelope. 
%
% We expect that the answer to these questions may be different than they are for LLMs.
%
%Our future directions include the following.
There are a number of future directions raised by our work.
Our pre-train and fine-tune recipe may need more sophisticated prompting during inference, especially if only the operator form changed between two different PDE systems. 
Also, we do not look at self-supervision with the PDE loss penalty as a means of large-scale pre-training, and we limit our analysis to 2D spatial systems.
Moving to other architectures, larger scales, and more complex PDEs in space-time is a focus of future work.

