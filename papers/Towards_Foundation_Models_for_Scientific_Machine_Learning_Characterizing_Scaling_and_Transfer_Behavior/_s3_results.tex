
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht]
  \begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/tuned-poisson-poisson-scale-k5_10}  
  \caption{\sysAfiveten{} pre-trained from \sysAonefive{}}
  \label{fig:poisson-k5-10}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{figs/tuned-ad-ad-scale-adr1_2.pdf}  
  \caption{\sysBonetwo{} pre-trained from \sysBzeropttwoone{}}
  \label{fig:ad-adr1-2}
\end{subfigure}
\caption{\textbf{Addressing (Q1).} Testing error as a function of downstream examples  for \sysA{} and \sysB{}. 
We visualize the distribution of pre-training and downstream dataset physics at the top to illustrate (and quantifiy) the extent of distributional shifts.  We observe excellent zero-shot and few-shot TL performance of the pre-trained model despite the modest OOD shifts and in medium-data regimes about $100\times$ increase in data efficiency. We observe diminishing returns from pre-training at the large-data regime (O($2^{15}$) examples), which has as many examples as used in pre-training.
}
\label{fig:Q1}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{sec:results}
Our main results are the following.
We demonstrate that pre-training a model on a diverse corpus of data and then fine-tuning it on downstream tasks leads to significantly better performance than training a model from scratch. 
This holds even when the downstream data falls outside of the pre-training distribution, including when different physics models are combined. 
The advantage of pre-training is especially pronounced when the downstream data is limited, which is the most significant setting in practice, motivating the creation of foundation models for scientific machine learning.

To justify these conclusions, we focus on four key questions. 
What is the effect of \textbf{(Q1)} downstream dataset size and \textbf{(Q2)} neural operator model parameter size on TL?  
What is the TL behavior of the neural operator \textbf{(Q3)} over the underlying physics and \textbf{(Q4)} over multiple solution operators? 


\medskip
\noindent \textbf{(Q1): Downstream dataset scaling.} 
% We vary the amount
% of downstream dataset examples used for fine-tuning our models. 
For \sysA{}, we consider the pre-training system \sysAonefive{}, with diffusion constructed by sampling eigenvalue $e \sim \mathcal{U}(1, 5)$ of the diffusion tensor $\mat{K}$.
See \fref{fig:vis} for visualizations.
This represents near isotropic to $5\times$ anisotropic diffusion.
We use $e \sim \mathcal{U}(5, 10)$ as the downstream dataset \sysAfiveten{}.
This represents $5\times$--$10\times$ anisotropic diffusion and is moderately out-of-distribution (OOD) from the pre-training dataset. 
While we systematically quantify the OOD effects in \textbf{(Q2)}, we use this specific
test-case to illustrate the effect of the size of the downstream dataset. % available for training. 
%
We plot the behaviour of testing error as a function of
downstream examples in \fref{fig:poisson-k5-10}. We train
an FNO model for each number of downstream examples (x-axis on the plot)
starting from ``scratch'' (random initialization) as well as from the
pre-trained model parameters, with tuned hyperparameters for each experiment (see details in Appendix \sref{sec:hpo}).


We illustrate the extent of distributional shift between the pre-training and downstream datasets through the range of diffusion tensor eigenvalue $e$---in this test case, a modest shift with no overlap (but relatively close).
%
The testing error monotonically decreases as
more downstream data is used for training, as we expect. 
%(either trained from scratch or from pre-trained model weights)
The zero-shot TL shows excellent performance despite the moderate OOD shift of downstream examples. 
When training from scratch, we define ``zero-shot'' predictions as the output of the model with random initialization. 
With ``few-shot'' learning ($O(10)$ downstream examples), we observe a consistent performance increase over training from scratch. 
Given a desired error for downstream performance, TL from the pre-trained model can require orders of magnitude less data---for example, a desired error of $1\nexp{2}$ needs only about $64$ downstream data examples for fine-tuning, whereas training from scratch requires $8K$  (about $100\times$ more) examples to reach the same accuracy level. 
We further explore this in Appendix \sref{sec:tl_shift} and find the pre-trained model generally saves $O(1K-10K)$ data compared to training from scratch in the few-shot learning setting, and outperforms training from scratch at all scales.
Finally, with greater amounts of downstream data, the pre-training provides consistent performance gains until we enter the ``large target data'' regime, where the number of fine-tuning examples approaches the size of the entire pre-training dataset, and we see diminishing returns from pre-training.
%
We repeat this experiment for \sysB{}, using the following test-case: 
the pre-training system \sysBzeropttwoone{} consists of advection-to-diffusion rates $\Psi \in {\small \sim}(0.2, 1)$, representing about $1\times$--$5\times$ diffusion (relative to advection). 
For the downstream test, we use a modest TL with \sysBonetwo{} with $\Psi \in {\small \sim}(1, 2)$ representing $1\times$--$2\times$ advection (relative to diffusion). 
We visualize this shift in \fref{fig:ad-adr1-2} (top) and also show the testing error as a function of downstream examples. 
Both experiments reveal the same trend: TL delivers much higher performance, which improves with fine-tuning up to a point of diminishing returns.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\includegraphics[width=\linewidth]{figs/full-model-scaling.pdf}
\caption{\textbf{Addressing (Q2).} Model size scaling for \sysA{} and \sysB{} from $64K$ to $256M$ parameters for medium OOD test-cases. While finetuning consistently improves the model performance and data efficiency, we observe higher errors for small parameter regimes at $64K$ due to insufficient model capacity. The performance gains are significantly boosted through finetuning with a larger model set sizes monotonically up to $256M$ parameters.}
\label{fig:Q2}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\medskip
\noindent \textbf{(Q2): Model (parameter count) scaling.}
% We test the effect of model scale on transfer learning. 
As described in our
model setup, we vary the embedding $d$ and maximum Fourier modes $m$ to
approximately increase the parameter count $16\times$ in each scaling experiment from $64K$ to $256M$ parameters. 
For models trained from scratch, we repeat the data scaling experiments for each parameter count. For the pre-trained model, we first identify the ideal hyperparameters (through grid-search hyperparameter tuning) for each model scale and repeat the above training experiments.
%by fine-tuning the selected pre-trained model on the downstream data examples.
We visualize the testing errors as a function of downstream examples used for \sysAfiveten{} (pre-training dataset used: \sysAonefive{} signifying moderate OOD) for the different model scales in \fref{fig:Q2} (left). At the $64K$ parameter regime, the model capacity is insufficient, with large errors (greater than $1\nexp{2}$) for either training recipe across the whole range of downstream example counts. As we move to larger models, both training from scratch and fine-tuning show higher performance that increase with more examples.
Fine-tuning the pre-trained model boosts its performance, compared to training from scratch, as we increase the model scale and particularly across a wide range of downstream example counts (with $256M$ parameter model showing the least errors).
We repeat the model scaling process for Advection-Diffusion \sysB{} (pre-training dataset \sysBzeropttwoone{} with moderately OOD \sysBonetwo{} for downstream) and observe similar trends in \fref{fig:Q2} (right).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!htbp]
		\caption{Different downstream datasets and extents of overlap with the pre-training dataset for \sysA{} and \sysB{}, controlled by extent of anistropy (eigenvalue $e$) in diffusion tensor for \sysA{} and amount of advection relative to diffusion (ratio $\Psi$) for \sysB{}.
		\label{tab:systems}
		}
		\centering
		\begin{small}
			% \makebox[\textwidth]{\centering
			\begin{tabular}{c|c|c}
					% \hline
				 Pre-training  & Downstream  &   Shift \\
                    \hline
                     \multirow{4}{*}{\centering{\sysAonefive{}: $e \sim \mathcal{U}(1, 5)$}} & \sysAonetwoptfive{}: $e \sim \mathcal{U}(1, 2.5)$ & None \\
                    & \sysAtwoptfivesevenptfive{}: $e \sim \mathcal{U}(2.5, 7.5)$ & Mild\\
                    & \sysAfiveten{}: $e \sim \mathcal{U}(5, 10)$ & Med \\
                    & \sysAtentwenty{}: $e \sim \mathcal{U}(10, 20)$ & Large \\
                    \midrule
                    \multirow{4}{*}{\centering{\sysBzeropttwoone{}: $\Psi \in {\small \sim}(0.2, 1)$}} & \sysBzeropttwozeroptfour{}: $\Psi \in {\small \sim}(0.2, 0.4)$ & None \\
                    & \sysBzeropttwozeroptfour{}: $\Psi \in {\small \sim}(0.2, 0.4)$ & Mild\\
                    & \sysBonetwo{}: $\Psi \in {\small \sim}(1, 2)$ & Med \\
                    & \sysBtwofive{}: $\Psi \in {\small \sim}(2, 5)$ & Large \\
					% \bottomrule
			\end{tabular}
		\end{small}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/tuned-poisson-poisson-scale-k1_2p5.pdf}  
  \caption{\sysAonetwoptfive{}}
  \label{fig:poisson-k1-2p5}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{figs/tuned-poisson-poisson-scale-k2p5_7p5.pdf}  
  \caption{\sysAtwoptfivesevenptfive{}}
  \label{fig:poisson-k2p5-7p5}
\end{subfigure}\\
\begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/tuned-poisson-poisson-scale-k5_10}  
  \caption{\sysAfiveten{}}
  \label{fig:poisson-k5-10--}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{figs/tuned-poisson-poisson-scale-k10_20.pdf}  
  \caption{\sysAtentwenty{}}
  \label{fig:poisson-k10-20}
\end{subfigure}
\caption{\textbf{Addressing (Q3).} Testing error as a function of downstream examples for different downstream tasks used in \sysA{}. We show the extent of overlap (signifying distributional shifts) between the pre-trained and downstream dataset at the top using the range of sampled diffusion tensor eigenvalue. For datasets within distribution, zero-shot TL is optimal. As the downstream dataset shifts moderately OOD, the zero-shot learning suffers gradually and is recovered through fine-tuning. This recovery is slower as the distributional shifts increase.}
\label{fig:sysA-physics-scaling}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\medskip
\noindent \textbf{(Q3): TL  behavior over underlying physics.} We test both in-domain and out-of-domain physics effects by constructing downstream datasets that systematically deviate from the pre-training dataset.  For \sysA{},  we sample different ranges for $e$ with varying overlap with the pre-training dataset. Similarly, for \sysB{}, we use different ranges of advection-to-diffusion ratio $\Psi$ showing different overlap. We highlight these systems (downstream and pre-training) in \tref{tab:systems} for the two PDE systems.
%
We repeat our downstream dataset scaling experiments on the different downstream tasks and show the  trends for \sysA{} in \fref{fig:sysA-physics-scaling}.
In particular, in \fref{fig:poisson-k1-2p5}, we consider the downstream dataset within distribution of the pre-training dataset (as visualized by the $e$ distribution at the top). We observe excellent zero-shot performance that is unaffected by further fine-tuning. In \fref{fig:poisson-k2p5-7p5}, the downstream dataset is shifted mildly OOD. Although the zero-shot performance drops, it still shows low errors, significantly smaller than training from scratch. Further, the performance is improved with few-shot TL up to the point of diminishing returns with large numbers of downstream data examples. With further distributional shift (no overlap) in \fref{fig:poisson-k5-10--}, the zero-shot performance suffers more, but with a larger amount of fine-tuning recovers good performance. Finally, for large distributional shifts in \fref{fig:poisson-k10-20}, the zero-shot and few-shot performance is poor, with TL showing relatively high errors, but even here TL improves over training from scratch.
% However, larger number of examples are needed to reach desired error levels. 
In this case, due to larger anisotropy, the system is also harder to emulate and might require more data in general. We repeat this analysis for \sysB{} and observe similar trends across different OOD downstream tasks (see Appendix \sref{sec:tl_shift}).
% For \sysB{}, we show the same trends in \fref{fig:sysB-physics-scaling} and observe that for within-distribution advection-diffusion ratios $\Psi$, the zero-shot transfer learning is excellent and gradually deteriorates with moderate distributional shifts. We again note that despite the OOD shifts, lower errors and higher performance gains relative to training from scratch can be obtained with few-shot transfer learning. 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!ht]
  \begin{subfigure}{.46\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/tuned-poisadvhelm-poisson-scale-k1_2p5.pdf}  
  \caption{
      \sysAonetwoptfive{}: pre-training using \sysAonefive{} \\ and mixed dataset}
  \label{fig:poisson-k1-2p5-mix}
\end{subfigure}
\begin{subfigure}{.46\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/tuned-poisadvhelm-ad-scale-adr0p2_0p4.pdf}  
  \caption{\sysBzeropttwozeroptfour{}: pre-training using \\ \sysBzeropttwoone{}  and mixed dataset}
  \label{fig:ad-0p2-0p4-mix}
\end{subfigure}
\begin{subfigure}{.46\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{figs/tuned-poisadvhelm-helm-scale-o1_5.pdf} 
  \caption{\sysConefive{}: pre-training from \sysConeten{} \\ and mixed dataset}
  \label{fig:helm-o1-5-mix}
\end{subfigure}
\caption{\textbf{Addressing (Q4).} Testing error as a function of downstream examples  for \sysA{}, \sysB{}, and \sysC{} with fine-tuning from their respective PDE systems and from the mixed dataset (combination of \sysA{}, \sysB{}, and \sysC{}). The model pre-trained on the mixed dataset performs better than training from scratch. More importantly, the same pre-trained model yields low errors on all the downstream PDEs with both zero-shot and task-specific fine-tuning. 
}
\label{fig:Q4}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\medskip
\noindent \textbf{(Q4): TL behavior over multiple operators.}
% To evaluate how well a single model can do on diverse downstream tasks,
We further diversify the pre-training by including examples with different solution operators. 
We combine the datasets from three PDEs--- Poisson's \sysAonefive{}, Advection-Diffusion \sysBzeropttwoone{}, and Helmholtz \sysConeten{} (where the wavenumber $\omega \sim \mathcal{U}(1,10)$). Here, we have additionally included the Helmholtz PDE, a challenging system due to the highly oscillatory behavior of the solutions (see PDE 2 in \fref{fig:schematic} and \fref{fig:vis}, for examples), very sensitive to the range of wavenumbers.
When pre-training a single model on this ``mixed'' dataset, we simply use zero channels for those coefficients that do not exist when using examples from a specific operator. For example, the Helmholtz equation has a diffusion tensor input (identity matrix) with an additional input for the wavenumber but no advection (zero channel), while the Poisson's equation only has a diffusion tensor input and hence we append zero channels to signify no wavenumbers and advection; similarly for Advection-Diffusion.
This, effectively, serves as selection of the solution operator during the forward pass to predict the solution to the right operator.
While more advance techniques such as in-context prompting (from LLMs) exist, here we are interested in understanding if this simple and minimal selection/prompting is sufficient for the model to transfer effectively to downstream tasks. 
% In the future, as we adapt the model to ``new'' operators (not seen in pre-training), additional in-context prompting could prove useful.
For the downstream tasks, we consider three within-distribution tasks of Poisson's \sysAonetwoptfive{}, Advection-Diffusion \sysBzeropttwozeroptfour{}, and Helmholtz \sysConefive{} and show our dataset scaling results in \fref{fig:Q4}. 

The results support our most compelling conclusion: fine-tuning from the mixed dataset retains the substantial performance gains over training from scratch for all downstream tasks. The same model (pre-trained on three different tasks) is useful in all downstream tasks, in both the zero-shot and the fine-tuning settings. This indicates the input coefficient channels are sufficient to prompt the model to predict the correct downstream solution. 
%Through pre-training, the model learns some shared elements between the different solution operators (despite the fact that the Helmholtz system solutions manifest very different and highly oscillatory behavior compared to Poisson's or Advection-Diffusion).
We show the OOD downstream performance of this model in Appendix \sref{sec:multiple} and observe similar behavior.


