\section{Related work}
\label{sec:related work}
In recent years, there has been widespread interest in modeling PDE systems with neural operators or operator learning in a broad variety of science and engineering applications \cite{lu2021learning,li2020fourier,kovachki2021neural,rahman2022u,Lu_2022,pathak2022fourcastnet,khoo2021solving,bhattacharya2021model,wen2022u,boulle2023learning,li2022fourier,kissas2022learning}. 
Following the success of TL in CV and NLP tasks \cite{yosinski2014transferable,saito2018maximum,weiss2016survey,ganin2016domain,oquab2014learning,raffel2020exploring,houlsby2019parameter,MM20_SDM}, there have been several investigations into how TL can be leveraged for SciML problems involving differential equations.  
Most have focused on applications of Physics-Informed Neural Networks (PINNs) \cite{mattheakis2021unsupervised,hanna2021residual,goswami2020transfer,chen2021transfer,HAGHIGHAT2021113741,guo2022analysis,xu2023transfer,gao2022svd, desai2021one, li2021physics}, where models can be fine-tuned or adapted using a physics-based loss function determined by the specific target PDE/ODE system. 
In \cite{desai2021one}, the authors explore transfer learning in the context of PINNs, training models on bundles of differential equations and then performing one-shot fine-tuning of the last linear layer (through a linear solve) to learn a new solution. 
Despite promising results on simple linear ODEs/PDEs, these results do not extend to more general neural operators and do not consider the behavior of their models with data/model scale on more complex problems. 
To this end, \cite{li2021physics} combines neural operators with PINN-style self-supervision to incorporate the PDE as a soft penalty in the loss, demonstrating the ability to train a model on one instance (Reynolds number) of the Navier-Stokes equation then fine-tune via the  PDE loss at test-time for the same system with different Reynolds numbers. 
However, these authors also do not investigate scaling behaviors or stress-test their TL potential.

Recently, attention has also been devoted to TL for neural operators in SciML, where some works have explored certain components of our analysis in isolation. 
This includes studies on the TL performance of DeepONet \cite{xu2022transfer, goswami2022deep, zhu2022reliable} and FNO \cite{li2021physics, de2022cost}, where one of either the target domain, PDE coefficients, or PDE source functions are varied. 
A common theme among many of these works is evaluating how well TL can account for the diversity of geometries \cite{chakraborty2022domain,wang2022mosaic,xu2023transfer,goswami2022deep} and discretizations \cite{lyu2023multi,chakraborty2021transfer,song2022transfer,subel2023explaining} found in scientific computing.
Also, \cite{goswami2022deep} evaluates the TL performance of DeepONets for various PDE systems, but this work focuses on the behavior under changing geometric domains, and it does not consider the possibility of systematically (pre-)training models on diverse combinations of different PDE coefficients and source functions. 
Furthermore, the majority of their transferred models saturate in accuracy as the number of fine-tuning examples is increased.
% such that training models from scratch outperforms transfer learning for larger dataset sizes. 
The work of \cite{zhu2022reliable} also evaluates the transfer behavior of DeepONets, focusing on developing effective recipes for detecting OOD inputs at test time and fine-tuning with additional supervision from known PDE terms or sparse observations. 
Their study of OOD extrapolation is also limited in the sense of only varying the PDE source functions and keeping the coefficients fixed; and their work also does not evaluate how the end-to-end accuracy changes as both model and dataset sizes are scaled up or down. 
Another related article \cite{de2022cost} explores trade-offs between computational cost and accuracy for neural operators under changing model and dataset sizes.
Their analysis is mostly limited to within-distribution experiments without a systematic evaluation on OOD data, and they also do not analyze the fine-tuning performance of such models, particularly on OOD data. 
Further, they do not simultaneously vary coefficients and source functions of the PDE systems on which they train, and they restrict their training to a single PDE operator.
To the best of our knowledge, the work of \cite{yang2023context} (which has appeared concurrent to this work) is the first (along with ours) to consider the TL potential of neural networks across operators and data distributions.
There, the authors adapt in-context learning (from LLMs) to solve differential equations with transformer-based models.
The main differences with~\cite{yang2023context} are two-fold.
First, those authors focus on in-context learning, which requires prompting (with up to five example demos) to solve OOD tasks, including different differential equation coefficients and operators than those seen at train time (similar to our work). 
In contrast, we focus on TL with zero-shot and few-shot learning through fine-tuning. 
Thus the two approaches (TL through fine tuning vs in-context learning) are complementary but different. 
Second, the investigation of \cite{yang2023context} was performed at a much smaller scale (from both model and data perspective), i.e., they did not evaluate scaling, which is known to be essential for the development of foundation models, and it was limited to simpler dynamical systems, as compared to the broader range of systems tested in our work.

Overall, the prior related work provide important initial results on the interpolation and extrapolation behavior of neural operators in the context of differential equations, and/or limited investigation into the behavior as model and dataset sizes are increased.
However, none of them consider these aspects simultaneously with a more diverse pre-training corpus (by varying all input variables such as source functions and PDE coefficients and/or including different operators for pre-training), which is closer to methodology adopted by CV and NLP in the development of their foundational models, with emphasis on
the importance of characterizing scaling properties~\cite{kaplan2020scaling,hoffmann2022empirical}.
% 