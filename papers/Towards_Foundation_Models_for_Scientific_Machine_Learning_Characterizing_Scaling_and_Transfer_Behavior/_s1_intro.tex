\section{Introduction}
\label{sec:intro}
% \vspace{-1mm}

Foundation models have received considerable interest recently~\cite{bommasani2021opportunities}.
%
This terminology refers to certain models that are trained on extremely large and diverse quantities of data and applied to a wide range of tasks.
Rather than being designed for any single task,
a foundation model serves as a ``prior'' or ``foundation'' upon which other models can be built.
It does so by using transfer learning (TL) methods to fine-tune or adapt the foundation model to a wide range of downstream tasks, using minimal additional data for each additional task.
%
Perhaps the most well-known foundation models are pre-trained large-language models (LLMs) such as BERT~\cite{devlin2018bert} and the GPT models~\cite{radford2018improving,radford2019language,brown2020language}.
%
The scaling with respect to the amount of data, the size of the model, and the amount of compute~\cite{kaplan2020scaling,henighan2020scaling} is key to the training of these models.
%
An important aspect of a trained foundation model is the notion of emergence---by leveraging shared features across the training tasks, the model is able to perform tasks seemingly different than those for which it was trained.
%
This approach to model development is quite different than the traditional approach of training a one-off model from scratch for each specific problem and each specific dataset.
%
Naturally, it is of interest how broadly this methodological approach can be~applied.

Scientific machine learning (SciML)~\cite{stevens2020ai} is an area that combines tools from ML and scientific computing to address domain-specific scientific and engineering challenges.
It holds promise to drive the next wave of data-driven discovery in the physical and engineering sciences. 
%
Recent work has highlighted the promise~\cite{raissi2019physics,lu2021learning, li2020fourier,karniadakis2021review} as well as some of the many challenges~\cite{failure21_TR,EdwCACM22} of developing SciML models---in general as well as with the traditional one-off learning approach.
%
Many SciML models emulate physical systems described by Partial Differential Equations (PDEs).
For example, Physics-Informed Neural Networks~\cite{raissi2019physics,failure21_TR} impose the PDE as a soft penalty in the loss function.
However, they are restricted to solving a single instance of the PDE.
The Neural Network (NN) needs to be retrained for each new set of PDE physics coefficients, sources, and/or initial/boundary conditions (IC/BCs).
Subsequent models have been developed to learn the full solution operator~\cite{li2020fourier,lu2021learning} by training across different coefficients (and/or initial and boundary conditions). 
These \emph{neural operators} learn mappings between two function spaces from a finite collection of input-output pairs
(that represent the coefficients/initial or boundary conditions as the input and the PDE solution function as the output).
This makes them more general and versatile in emulating any PDE system.
However, with new coefficients/sources or new differential operators, they too need to be retrained from scratch.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figs/overview_plot_2-crop.pdf}  
  \caption{Our setup consists of creating diverse training datasets, sampling both PDE coefficients and source functions simultaneously with different PDE operators and input data (coefficients, sources) distributions for pre-training. A neural operator is then pre-trained to predict the PDE solutions given these inputs and the ground truth solutions (computed through PDE solvers). The pre-trained model is then adapted with minimal fine-tuning (zero-shot or few-shot), and it is used in various downstream tasks (PDE systems) that can be in-domain or out-of-domain from the pre-training datasets. The pre-training with multiple solution operators allows the same model to transfer to several very different systems. For instance, PDE 2 (Helmholtz) manifests highly oscillatory solutions compared to, say, PDE 1 (Advection-Diffusion) or PDE 3 (Poisson's). We further characterize the scaling and transfer properties of this model as a function of downstream data scale and model size scale.
  }
  \label{fig:schematic}
\end{figure*}

In this paper, we adopt and evaluate the methodology that has been applied successfully in CV and NLP to develop foundation models, with the goal of determining whether such a model is even possible for SciML problems.
In particular, we provide an extensive analysis of the scaling and TL behavior of neural operators trained on diverse training datasets from multiple PDE systems.
An important aspect of this approach is to explore several dimensions that include the model (architecture and scale), data (diversity and scale), training recipes (pre-training and fine-tuning), and out-of-distribution (OOD) generalization behaviour. 
For LLMs, given the maturity of the NLP community within ML, these dimensions are well-explored.
For SciML problems, in contrast, all these dimensions are open questions. 
Here, we explore several of these questions.
We do so in the context of a specific model architecture, namely, the \emph{Fourier Neural Operator (FNO)}, as a prototypical SciML model that has demonstrated promising results modeling PDEs~\cite{li2020fourier}.
%, and we explore the dimensions of dataset diversity and scale, model scaling, and generalization. 
We focus on the scaling and TL behavior of the FNO on common PDE systems that include Poisson's, Advection-Diffusion, and Helmholtz PDE systems.
These systems underpin a wide range of physical systems: fluid flow systems; biological simulations; wave propagation systems; and many~others. 

See \fref{fig:schematic} for a schematic summary of our methodological approach.
Our main results demonstrate the potential of the ``pre-train and fine-tune'' paradigm for SciML problems, demonstrating a path towards building SciML foundation models.
%
In more detail, our main contributions are the following.
\begin{enumerate} 
% [leftmargin=*,nosep]
    \item 
    \textbf{Pre-training dataset generation. }
    We develop a large suite of datasets, and we train our models on data where all the variables (inputs) of any PDE operator are sampled.
    This is an important step towards developing NNs that can generalize across a variety of downstream tasks, and it extends several previous works, including the original FNO~\cite{li2020fourier}, where certain inputs are kept fixed.
    Not sampling can trivially push the neural operator OOD if, e.g., the source function was changed. 
    We study transfer learning to both in-domain and out-of-domain distributions, characterized by different samplings of PDE coefficients (diffusion, advection, wavenumbers, etc.) and inhomogeneous source functions.
    We emphasize the construction of pre-trained datasets with sufficient diversity as well as normalization strategies, without which we observe significant degradation of performance.
    \item 
    \textbf{Downstream (target) data scaling.}
    We study the effect of the scale of downstream (target) data in TL performance from the pre-trained model.
    Here, we assume that a large amount of data is available for pre-training, and data are limited for the downstream tasks (as in many scientific examples); and we are interested in reaching desired accuracy levels with the least amount of additional downstream data.
    We consider both zero-shot and few-shot TL: zero-shot is the direct evaluation of the pre-trained model on the downstream dataset; and
    few-shot involves using O(10) downstream data to fine-tune the pre-trained model.
    We observe that TL from the pre-trained model can lead to significant performance gains over training the model from scratch on the downstream (target) data, with orders of magnitude less data needed to reach a desired accuracy level (see \fref{fig:Q1}). 
    We observe this gain over a wide range of data scales, until we enter the ``large target data'' regime (as much data as pre-training), where we observe similar accuracies for TL as training from scratch.
    \item
    \textbf{Model (parameter) size scaling.} 
    We study the parameter scaling of the model by scaling our model sizes from $64$K to $256$M parameters (a multiplicative factor of $4$K). 
    We observe an error saturation at small model sizes (due to insufficient model expressivity) that monotonically drops as we increase the model size. 
    While both fine-tuned models and models trained from scratch exhibit gains with increased model size, we observe that fine-tuning achieves greater performance gains with parameter scaling (see \fref{fig:Q2}).
    \item 
    \textbf{Transfer learning behavior over underlying physics.}
    We study the effect of varying the underlying physics in the target domain.
    In SciML (unlike traditional non-scientific ML), there are typically fundamental constraints such as conservation laws that govern the behaviour of the solution. 
    In some cases, we may even have access to parameters of the underlying physical constraints, and thus a physical understanding of the distribution of the data.
    It is natural, and necessary, to systematically quantify the effect of these parameters as our downstream tasks go OOD, as this provides a good way to test the (OOD) generalization capability of pre-trained models for SciML applications.
    We find that for in-distribution TL, the pre-trained model can significantly outperform a model trained from scratch, irrespective of how many new data examples were used for fine-tuning, until the large target data regime (e.g., see \fref{fig:poisson-k1-2p5}), showing orders of magnitude better accuracies than training from scratch.
    We also observe these gains for downstream tasks that are moderately OOD, with few-shot fine-tuning providing again orders of magnitude better accuracies (see \fref{fig:poisson-k2p5-7p5}, \fref{fig:poisson-k5-10--}).
    As we systematically go further OOD (see the quantification in \tref{tab:systems}), we observe the performance gains expectedly reduce, with more significant drop in the low data regimes (e.g., see \fref{fig:poisson-k10-20}).
    \item 
    \textbf{Transfer learning behavior over multiple operators.}
    We study the effect of simultaneous pre-training on multiple PDE systems that exhibit qualitatively different solution behaviors (e.g., Poisson's and Helmholtz operator solutions show very dissimilar patterns, given a source function). 
    We include the coefficient/source functions for all the operators as inputs to the model, with zero values if those terms do not exist in a given PDE instance. During inference, the zero inputs restrict the neural network to make predictions for the correct operator (see \sref{sec:results} for details).
    Among other things, we show that the same model pre-trained on different operators retains its gains across different downstream tasks (see \fref{fig:Q4}), paving the way for it to be used in the foundational sense.
\end{enumerate}

\input{_s1_1_related_work}
