\counterwithin{figure}{section}
\counterwithin{table}{section}
\appendix
\section{Appendix: Additional Details}
\label{sec:other_details}
\subsection{Pre-train and downstream data creation}
\label{sec:data_creation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!thbp]
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/adr_overview-crop.pdf}  
\caption{We illustrate the variability in source function inputs (left) and solution outputs (right) for \sysB{}, where the velocity direction, scales, and the diffusion tensor direction and anisotropy scales (eigenvalue) are all changed along with the source sampling to produce the input-output pairs for the training dataset.}
\label{fig:vis_ad}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As described in \sref{sec:methods}, our sampling strategy involves (i) source functions and (ii) PDE coefficients. We use a numerical discretization of $128 \times 128$ for all our experiments. Some additional details are as follows:
\begin{enumerate}[leftmargin=5ex,nosep]
    \item \textit{Source function sampling:} As described in \sref{sec:methods}, our source functions are a linear combination of $n_g$ radial (Gaussian) basis functions $\{\phi_i(\vect{x})\}_{i=1}^{n_g}$, where $\phi_i(\vect{x}) = \phi(\vect{x} - \vect{x_i})$ is a Gaussian function centered at grid point $\vect{x}_i$. Specifically: $f(\vect{x}) = \sum_{i = 1}^{n_g} \phi_i(\vect{x}) p_i$, 
with $\vect{p} = \{p_i\}_{i=1}^{n_g}$ as the parameterization vector. 
The
spatial profile controlled by $\sigma$, the standard deviation of the Gaussian
function, is preset to a small value $1/32$ (hence 4 times the spatial resolution) to encourage high variability. The spacing between Gaussians is fixed at $2\sigma$. Examples are
sampled by uniformly randomly sampling $p_i \sim \mathcal{U}(0,1)$. 
We further introduce heterogeneity by controlling the sparsity $s$ of $\vect{p}$ (we define $s$  as the fraction of number of zero components of $p_i$). Hence, $s = 0.6$, implies only 40\% of $\vect{p}$ is randomly sampled in $\mathcal{U}(0,1)$ and the rest (60\%) are set to zero. Visualizations of this are in \fref{fig:vis} on the left. As we go down, the sparsity is increased (and hence Gaussians are more sparse and spread apart). In our pre-training and downstream datasets, we sample sparsity levels from 20\% to 80\% uniformly. 
   \item \textit{PDE coefficient sampling:} 
   In \fref{fig:vis}, we have showed the systematic effects of changing certain PDE coefficients for the three systems. For our pre-training datasets, all inputs are simultaneously changed to obtain a diverse training corpus. For example, in \sysB{}, the eigenvalue $e$ that controls the extent of anisotropy and $\theta$ that encapsulates directional information of diffusion are both sampled to create diverse diffusion tensors, the velocity direction and scale are sampled to create different advection vectors as well as advection-diffusion ratios, and the source functions are also sampled with variable sparsity and Gaussian parameterizations.
   In \fref{fig:vis_ad}, we show a small sample of input sources and output solutions for this system to illustrate the variability in our datasets---all the variables (advection directions and scales, diffusion directions and scales, source functions) are changed to create the full dataset.
\end{enumerate} 
For each PDE system, we use spectral methods to compute the ground truth solution of the PDE. For example, for \sysA{}:
\begin{equation}
     - \idiv \mat{K} \igrad u = f, \label{eq:poisson_supple}
\end{equation}
the solution is:
\begin{equation}
    u = \mathcal{F}^{-1}\big(\frac{-1}{-(k_x^2k_{11} + k_y^2k_{22} + 2k_xk_yk_{12})} \mathcal{F}(f(\vect{x})) \big),
\end{equation}
where $\mathcal{F}$ is the Fourier transform, $k_x, k_y$ are the frequencies in the Fourier domain, and $k_{11}, k_{22}, k_{12}$ are the diagonal and off-diagonal coefficients of the diffusion tensor $\mat{K}$. The solution is unique given zero-mean source functions. Similar solutions can be derived for the other systems.


\subsection{Model architecture}
\label{sec:fno_app}
We describe the architecture details of the FNO here. The central building block of FNO learns a kernel integral operator
parameterized by a function operating in Fourier space. By composing several of
these blocks together in sequence, the FNO can learn to approximate the solution
operator for complex PDEs. For Euclidean systems discretized at uniform
resolution (e.g., in 2D with inputs and solutions having dimension
$\mathbb{R}^{h\times w}$), the implementation can be made efficient by using the
Fast Fourier Transform (FFT), denoted by $\mathcal{F}$ with inverse FFT $\mathcal{F}^{-1}$.
% and this is the most common version of the FNO used in practice.
We refer the reader to the original FNO paper \cite{li2020fourier} for details,
but briefly summarize the basic mechanism, to highlight how the model complexity depends on key hyperparameters. 
The inputs to the 2D FNO are spatial functions discretized at this resolution $h \times w$ that represent sources, PDE coefficients, ICs/BCs of the PDE system--each of these is represented as a separate channel,
% \footnote{constant coefficients are simply replicated across the $h \times w$ dimensions}.
leading to input tensors in  $\mathbb{R}^{h\times w\times c}$, with $c$ input channels. 
% The target is the discretized solution function of the PDE in $\mathbb{R}^{h\times w\times 1}$ and is pre-computed with analytical or numerical methods.
As mentioned before, we use the resolution $h\times w = 128 \times 128$.
Given this input tensor $A \in \mathbb{R}^{h\times w\times c}$ ($c$ input channels representing the PDE inputs), the FNO first projects $A$ into a tensor $X\in\mathbb{R}^{h\times w\times d}$ with embedding dimension $d$, which is passed through a series of FNO blocks. For a given block $l \in \{1, .., L\}$ with input $X^l \in\mathbb{R}^{h\times w\times d}$ the output at spatial index $(i,j)$ is computed as
\begin{equation}
    X^{l+1}_{(i,j)} = \sigma ( W_l X^{l}_{(i,j)} + \mathcal{F}^{-1}[\mathcal{K}_l(\hat{X}^{l})]_{(i,j)}), 
\end{equation}
where $\sigma$ is a pointwise nonlinear activation function, $W_l \in\mathbb{R}^{d\times d}$ is a learnable weight matrix, which performs pointwise linear transformation, and $\hat{X}^l = \mathcal{F}(X^{l}) \in \mathbb{C}^{h\times w \times d}$ are complex-valued Fourier coefficients output by the FFT. The transformation $\mathcal{K}_l$ in Fourier space is parameterized by complex weights $\Phi^l \in \mathbb{C}^{d\times d \times m_1 \times m_2}$ according to
\begin{equation}
    \mathcal{K}_l(X^l)_{(k_1, k_2)} = \Phi^l_{(k_1,k_2)} X^l_{(k_1,k_2)},
\end{equation}
for pairs of Fourier frequencies/wavenumbers $k_1 \in \{1, ..., m_h\}$ and $k_2 \in \{1,..., m_w\}$. The hyperparameters $m_h$ and $m_w$ control the ``mode cutoff'', beyond which Fourier modes are ignored by $\mathcal{K}$, and they have a theoretical maximum of $m_h=h/2$ and $m_w=w/2$, i.e., the Nyquist limit along each spatial dimension. In practice, the mode cutoffs are a key hyperparameter controlling model complexity along with the embedding dimension $d$, and they are often tuned to be less than the Nyquist limit to prevent overfitting and accelerate training. For square problems ($h=w$), a symmetric truncation is adopted such that $m=m_h=m_w$. Thus, the per-layer parameter count is quadratic in both $m$ and $d$, dominated by the complex weights $\Phi$. 
The above ($d$ and $m$) hyperparamters are the focus of our exploration in the model scaling experiments.
% We explore the scaling behavior of these hyperparameters under different pre-training and fine-tuning regimes in section \sref{sec:results}.

\subsection{Training details and code open-source}
\label{sec:training_details}
For training, we use the Adam optimizer with a cosine learning rate decay schedule. All models are trained for 500 epochs with the best model saved at lowest validation loss. 
We tune batch size and initial learning rates using a grid hyperparameter search, and train every model using 4 NVIDIA A100 GPUs (using standard data-parallelism) on the Perlmutter supercomputer. A full training run (on all 32$K$ examples used for pre-training) completes in around 2.5 hours. To evaluate model performance, we use the mean relative error between predicted and target solution, defined as: $\mu_{\ell_2} := 1/N \| u - u_0\|_2 / \|u_0\|_2$, where $u$ is the predicted solution, $u_0$ is the target solution, and $N$ is total (fixed) number of testing  (around $4K$) examples. We open source our model training and data generation code at \cite{github}.

\subsection{Hyperparameter tuning}
\label{sec:hpo}
For studying the relationship between model and dataset size, we perform a simple grid-search over 5 different learning
rates for each combination of model and dataset size.
This is important because different dataset sizes may demand different hyperparameter values---for instance, we observe that for very small dataset sizes (small number of downstream examples), the tuned learning rates are significantly smaller (especially for zero- and few-shot transfer learning) to help mitigate over-fitting and optimization difficulties at that scale, whereas larger learning rates perform better as model and dataset sizes increase. Hence, for any result (including the model scaling), these learning rates are tuned with the best values picked from the validation set metrics.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/tuned-ad-ad-scale-adr0p2_0p4.pdf}  
  \caption{\sysBzeropttwozeroptfour{}}
  \label{fig:ad-adr-0p2-0p4}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{figs/tuned-ad-ad-scale-adr0p4_1p6.pdf}  
  \caption{\sysBzeroptfouroneptsix{}}
  \label{fig:ad-adr-0p4-1p6}
\end{subfigure}\\
\begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/tuned-ad-ad-scale-adr1_2.pdf}  
  \caption{\sysBonetwo{}}
  \label{fig:ad-adr-1-2}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{figs/tuned-ad-ad-scale-adr3_4.pdf}  
  \caption{\sysBtwofive{}}
  \label{fig:ad-adr-2-5}
\end{subfigure}
\caption{\textbf{Addressing (Q3).} Testing error as a function of number of downstream dataset examples for different downstream tasks used in  \sysB{}. We show the extent of overlap between the pre-trained and downstream dataset at the top using the range of advection-diffusion ratios of the two datasets.
Similar to \sysA{}, we observe good zero-shot performance that gradually decreases with distributional shifts but can be recovered through few-shot learning. With large shifts, the number of examples needed to reach desired error levels also increases.
}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{fig:sysB-physics-scaling}
\end{figure*}

\subsection{Input normalization}
\label{sec:normalization}
As mentioned in \sref{sec:methods}, a key aspect of our training is the input normalization (for any PDE system). We first normalize every training example source and coefficient inputs with respect to a reference source value defined as the median source norm over the training dataset, i.e., $f_{\text{ref}}:=\text{median}_i(\|f_i\|_2)$ where $i$ are the training examples. 
Hence for any example $f_i$, we first compute $\|f_i\|_2$ and normalize $f_i$ and all coefficients with the relative norm $\|f_i\|_2/f_{\text{ref}}$. 
First, this ensures the source norms are within a reasonable scale range.  Additionally,
it implies that scaling both coefficients as well as source functions of the
inputs by a constant yields the same inputs to the neural operator.
%(as they should).
For example: the inputs $f$ and $\mat{K}$ to the Poisson's equation (\sysA{}) are equivalent to the inputs $10f$ and $10\mat{K}$---both have the same solution function. The above normalization makes sure that these two input pairs are equivalent, since $10f$ and $10\mat{K}$ simply get normalized by $10$ before passing to the network.
Then, to ensure the coefficient inputs are comparable scales and within a reasonable range, the coefficient values (that typically have very different scales depending on the physical process, up to $100\times$ differences) are normalized by constant reference values pre-computed across the training dataset as the median values of the coefficients.
We observe that without normalization the performance of FNO can be quite poor, especially in cases with multiple channels representing different physical scales (such as in advection-diffusion \sysB{}). 


\section{Additional Results}
\label{sec:additional_results}
\subsection{TL behavior over underlying physics}
\label{sec:tl_shift}
\begin{figure*}
  \begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/equiv-data-SYS-1.pdf}  
  %\caption{\sysAfiveten{} pre-trained from \sysAonefive{}}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{figs/equiv-data-SYS-2.pdf}  
  %\caption{\sysBonetwo{} pre-trained from \sysBzeropttwoone{}}
\end{subfigure}
\caption{\textbf{Addressing (Q3).} Equivalent data needed to reach the accuracy of TL when training from scratch, for \sysA{} and \sysB{} distribution shifts defined in Table \ref{tab:systems}. Data from experiments where TL outperforms even the best (i.e., largest dataset) from-scratch models is not plotted.}
\label{fig:data-equiv}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


We show the TL performance under physics shift (\textbf{Q3}) for \sysB{} in \fref{fig:sysB-physics-scaling}. Similar to \sysA{}, we observe excellent TL performance for in-distribution tasks (see \fref{fig:ad-adr-0p2-0p4}) that is independent of the downstream dataset size. As we systematically go OOD, we continue to observe good performance gains with TL (both zero-shot and few-shot) until we are significantly OOD as in \fref{fig:ad-adr-2-5}.

In both \sysA{} and \sysB{}, we find that TL from pre-trained models outperforms training from scratch with the number of downstream examples required to reach a given accuracy being orders of magnitude smaller with TL. To quantify this in detail, we estimate the amount of ``data savings'' for TL by interpolating the equivalent number of examples needed to reach a given model error when training from scratch. We plot these ``from-scratch'' data requirements as a function of number of TL fine-tuning examples in \fref{fig:data-equiv}.
In these plots, any points above the dashed black line indicate an improvement over training from scratch; points for  TL experiments which outperform even the best from-scratch models (which use a maximum of 32$K$ training examples) are not plotted. We observe that even as the downstream dataset distribution shift moves from ``None'' to ``Large'' (as defined in Table \ref{tab:systems}), the TL models consistently reach the performance of from-scratch models which would otherwise require thousands to tens of thousands of training examples. One  exception is the largest OOD shift for \sysA{}, where we observe smaller (but still substantial) advantages for the TL approach in zero- and few-shot learning. We note that characterizing distribution shifts consistently for different systems is challenging; this result suggests perhaps the ``Large'' distribution shift for \sysA{} is a more significant shift than that of \sysB{}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{TL behavior underlying multiple operators}
\label{sec:multiple}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/tuned-poisadvhelm-poisson-scale-k1_2p5.pdf}  
  \caption{\sysAonetwoptfive{}: pre-training using \\ \sysAonefive{}  and mixed dataset}
  \label{fig:pois-poisadvhelm-1-2p5}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{figs/tuned-poisadvhelm-poisson-scale-k2p5_7p5.pdf}  
  \caption{\sysAtwoptfivesevenptfive{}: pre-training using  \\ \sysAonefive{}  and mixed dataset}
  \label{fig:pois-poisadvhelm-2p5-7p5}
\end{subfigure}\\
\begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/tuned-poisadvhelm-ad-scale-adr0p2_0p4.pdf}  
  \caption{\sysBzeropttwozeroptfour{}: pre-training using \\ \sysBzeropttwoone{}  and mixed dataset}
  \label{fig:ad-poisadvhelm-0p2-0p4}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{figs/tuned-poisadvhelm-ad-scale-adr0p4_1p6.pdf}  
  \caption{\sysBzeroptfouroneptsix{}: pre-training using \\ \sysBzeropttwoone{}  and mixed dataset}
  \label{fig:ad-poisadvhelm-0p4-1p6}
\end{subfigure}\\
\begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/tuned-poisadvhelm-helm-scale-o1_5.pdf}  
  \caption{\sysConefive{}: pre-training using \\ \sysConeten{}  and mixed dataset}
  \label{fig:helm-poisadvhelm-1-5}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{figs/tuned-poisadvhelm-helm-scale-o2_12.pdf}  
  \caption{\sysCtwotwelve{}: pre-training using \\ \sysConeten{}  and mixed dataset}
  \label{fig:helm-poisadvhelm-2-12}
\end{subfigure}
\caption{\textbf{Addressing (Q4).} Testing error as a function of downstream examples  for \sysA{} (top), \sysB{} (middle), and \sysC{} (bottom) with fine-tuning from their respective PDE systems and from the mixed dataset (combination of \sysA{}, \sysB{}, and \sysC{}). In each row: on the left, the downstream task is in-distribution and, on the right, it is OOD. The model pre-trained on the mixed dataset performs better than training from scratch. More importantly, the same pre-trained model yields low errors on all the downstream PDEs with both zero-shot and task-specific fine-tuning. We also note that \sysC{} is a particularly challenging system that shows larger performance drops as we go OOD---the mixed pre-training still retains the same gains as the task-specific pre-trained model.}
\label{fig:mixed}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We show additional results for TL behavior over multiple operators in \fref{fig:mixed}. Each row represents a PDE system with in-distribution (left) and OOD (right) downstream tasks. We observe that the mixed pre-trained model is able to retain TL gains as the system-specific pre-trained model over all the systems. Hence, a single model is now adapted to three different PDE systems in downstream fine-tuning. We also note \sysC{} OOD results---we observe that for the Helmholtz, going even moderately OOD, affects both TL and training from scratch performance. This PDE system is particularly challenging and is very sensitive to the input wavenumber. Slight changes introduce more oscillatory features in the solution, pushing the downstream task OOD easily. However, we note that both the mixed pre-training and pre-training from a Helmholtz dataset show similar performance even on the OOD task.

\subsection{Sensitivity to random seeds}
\label{sec:sens}

To quantify the variability of both training from scratch and performing TL from the pre-trained model, we repeat the ``Med'' OOD shift experiment for \sysA{} and \sysB{} (see Table \ref{tab:systems}) 5 times with different random seeds and record the testing error for each trial. The resulting distributions indicate how sensitive each approach is to the random data shuffle used when training on the downstream dataset. We plot the mean testing error along with $1^{st}$ and $3^{rd}$ quartiles for each downstream dataset size in \fref{fig:seeds}. We observe small sensitivity for both TL and training from scratch. Further, not surprisingly, we observe that the variability across random seeds is generally larger when training from scratch. We note that the small variability in TL is difficult to see due to the log-spaced y-axis.

\begin{figure*}[!htbp]
  \begin{subfigure}{.5\textwidth}
  \centering
  % include first image
  \includegraphics[width=\linewidth]{figs/datasplit-poisson-poisson-scale-k5_10-rand.pdf}  
  \caption{\sysAfiveten{} pre-trained from \sysAonefive{}}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  % include second image
  \includegraphics[width=\linewidth]{figs/datasplit-ad-ad-scale-adr1_2-rand.pdf}  
  \caption{\sysBonetwo{} pre-trained from \sysBzeropttwoone{}}
\end{subfigure}
\caption{Testing error as a function of downstream examples  for \sysA{} and \sysB{}, aggregated over 5 trials with different random seeds for each experiment. The dots indicate the mean testing error at each downstream dataset size, and the shaded region represents the spread between the $1^{st}$ and $3^{rd}$ quartiles. We observe small variance that is slightly larger when training from scratch compared to TL from the pre-trained model.}
\label{fig:seeds}
\end{figure*}

