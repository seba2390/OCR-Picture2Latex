%File: formatting-instructions-latex-2023.tex
%release 2023.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai23}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}


\usepackage{subcaption}
\usepackage{caption}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{bbm}
\usepackage{epstopdf}
\usepackage{threeparttable}
\usepackage{tablefootnote}

\input{xcl_math}


%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2023.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\renewcommand\thesubfigure{(\alph{subfigure})}

\renewcommand{\algorithmicrequire}{\textbf{Input:}} 
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\newcommand{\vpi}{\boldsymbol{\pi}}
\newcommand{\valpha}{\boldsymbol{\alpha}}
\newcommand{\vxi}{\boldsymbol{\xi}}
\newcommand{\vnu}{\boldsymbol{\nu}}

\newcommand{\bw}{{\bf w}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bt}{{\bf \Theta}}
\newcommand\correspondingauthor{\thanks{Corresponding author.}}

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai23.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Fair-CDA: Continuous and Directional Augmentation for Group Fairness}

% \author{
%     %Authors
%     % All authors must be in the same font size and format.
%     Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
%     AAAI Style Contributions by Pater Patel Schneider,
%     Sunil Issar,\\
%     J. Scott Penberthy,
%     George Ferguson,
%     Hans Guesgen,
%     Francisco Cruz\equalcontrib,
%     Marc Pujol-Gonzalez\equalcontrib
% }
\author{
    %Authors
    % All authors must be in the same font size and format.
    Rui Sun\textsuperscript{\rm 1,\rm 2}\equalcontrib, Fengwei Zhou\textsuperscript{\rm 3}\equalcontrib, Zhenhua Dong\textsuperscript{\rm 3},
    Chuanlong Xie\textsuperscript{\rm 4}\footnote{Correspondence to: lizhen@cuhk.edu.cn,clxie@bnu.edu.cn},
    %\footnote{Corresponding author.},
    Lanqing Hong\textsuperscript{\rm 3},
    Jiawei Li\textsuperscript{\rm 3}, \\
    Rui Zhang\textsuperscript{\rm 5},
    Zhen Li\textsuperscript{\rm 1,2}\footnotemark[2],
    Zhenguo Li\textsuperscript{\rm 3}
}


\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} The Future Network of Intelligence Institute, The Chinese University of Hong Kong (Shenzhen) \\
    \textsuperscript{\rm 2} School of Science and Engineering, The Chinese University of Hong Kong (Shenzhen)\\
    \textsuperscript{\rm 3} Huawei Noah's Ark Lab\\
    \textsuperscript{\rm 4} Beijing Normal University\\
    \textsuperscript{\rm 5} Tsinghua University \\
   \ ruisun@link.cuhk.edu.cn, fzhou@connect.ust.hk, \{dongzhenhua, honglanqing, li.jiawei, li.zhenguo\}@huawei.com, rayteam@yeah.net, clxie@bnu.edu.cn, lizhen@cuhk.edu.cn \\
}

% \affiliations{
%     %Afiliations
%     \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
%     % If you have multiple authors and multiple affiliations
%     % use superscripts in text and roman font to identify them.
%     % For example,

%     % Sunil Issar, \textsuperscript{\rm 2}
%     % J. Scott Penberthy, \textsuperscript{\rm 3}
%     % George Ferguson,\textsuperscript{\rm 4}
%     % Hans Guesgen, \textsuperscript{\rm 5}.
%     % Note that the comma should be placed BEFORE the superscript for optimum readability

%     1900 Embarcadero Road, Suite 101\\
%     Palo Alto, California 94303-3310 USA\\
%     % email address must be in roman text type, not monospace or sans serif
%     publications23@aaai.org
% %
% % See more examples next
% }

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name,\textsuperscript{\rm 1,\rm 2}
    Second Author Name, \textsuperscript{\rm 2}
    Third Author Name \textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1} Affiliation 1\\
    \textsuperscript{\rm 2} Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
In this work, we propose {\it Fair-CDA}, a fine-grained data augmentation strategy for imposing fairness constraints. We use a feature disentanglement method to extract the features highly related to the sensitive attributes. Then we show that group fairness can be achieved by regularizing the models on transition paths of sensitive features between groups. By adjusting the perturbation strength  in the direction of the paths, our proposed augmentation is controllable and auditable. To alleviate the accuracy degradation caused by fairness constraints,  we further introduce a calibrated model to impute labels for the augmented data. Our proposed method does not assume any data generative model and ensures good generalization for both accuracy and fairness. Experimental results show that Fair-CDA consistently outperforms state-of-the-art methods on widely-used benchmarks, e.g., Adult, CelebA and MovieLens. 
Especially, Fair-CDA obtains an 86.3\% relative improvement for fairness while maintaining the accuracy on the Adult dataset.
Moreover, we evaluate Fair-CDA in an online recommendation system to demonstrate the effectiveness of our method in terms of accuracy and fairness.
\end{abstract}



\section{Introduction}

Many machine learning systems have achieved empirically success in practical problems but may sometimes raise issues of discrimination and unfairness.
In job candidate search, different protected groups (e.g., gender and ethnic groups) may be treated unfairly in terms of their members appearing in recommended candidate lists~\cite{ekstrand2021fairness}.
In the context of information retrieval, unfairness may happen among multiple parties. For example, unfair exposure allocation may favour monopolies and drive small content providers out of the market~\cite{Morik_2020}. This reduces diversity and impairs the whole ecosystem. 


There have been various studies to impose fairness constraints during training procedure~\cite{Zemellearningfair,hardt2016equality,zafar2017fairness,chuang2021fair}, ensuring that different groups shall be treated similarly.
However, these constraints are data-dependent, the learnt fair classifiers might not generalize at evaluation time.
\citet{agarwal2018reductions} and \citet{cotter2019training} consider two-player games to formulate the constrained optimization problem and analyze the solutions and generalization guarantees.
\citet{chuang2021fair} proposes {\it Fair Mixup} to
generate a path of distributions that connects sensitive groups and regularize the smoothness of transitions among the path to improve the generalization of group fairness metrics. 
They show that their strategy ensures a better generalization for both accuracy and fairness in a wide range of benchmarks. 

Motivated by {\it Fair Mixup}, we propose {\it Fair-CDA}, a continuous and directional augmentation method, to seek a fine-grained balance between fairness and accuracy. An overview of our method is illustrated in Figure~\ref{fig:model}.


\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figs/model1.PNG}
\caption{An overview of the proposed Fair-CDA.}
\label{fig:model}
%\vspace{-15pt}
\end{figure*}


\noindent{\bf Accuracy.} The {\it Mixup} \cite{zhang2018mixup} generates augmented samples via convex combinations of pairs
of data points. However, the between-group augmentation performs {\it Mixup} on both sensitive attributes and non-sensitive attributes.
This may change the correlation between non-sensitive attributes and the targets and further lead to the fall of prediction accuracy.
What's more, \citet{verma2019manifold} shows that interpolations in deeper hidden layers, which capture higher-level information \cite{zeiler2014visualizing}, can provide additional training signal and smooth decision boundaries that benefit generalization.
Therefore we develop a fine-grained augmentation via feature disentanglement and focus on the transitions over sensitive features. 


We decompose representations in latent space into sensitive and non-sensitive features via {\it DecAug} \cite{bai2020decaug}, which is a powerful feature disentanglement technique for  Out-of-Distribution (OoD) generalization. 
The sensitive features encode the information that is strongly correlated to the sensitive attributes, while the non-sensitive features retain as much other information (essential for prediction) as possible.
Then we apply semantic augmentation on the sensitive features aiming to generate features correlated to the opposite sensitive attributes. The augmented sensitive features combined with the original non-sensitive features form what we refer to as the augmented features.


\noindent{\bf Fairness.}  We eliminate the disparities of the predictions made by a task model via training the model to make the same decisions for the samples with the original features and with the corresponding augmented features.
So the augmentation strategy determines the level of fairness. 
However, it is not easy to control {\it Mixup} by tuning the distribution of the interpolation weight, which is usually a Beta distribution.  
According to \citet{zhang2021does}, 
%By \citet{zhang2021does}, the {\it Mixup} loss is an upper bound of the adversarial loss.   
%Thus, 
we consider adversarial training with random perturbation size to augment sensitive features more related to the opposite sensitive attribute.
The perturbation budget of the adversary becomes a key hyperparameter that monitors the generation procedure and controls the degree of fairness.
When the perturbation is significant, the augmented features can be classified into the opposite group with a high probability. 
Then the classifier learnt with large perturbations becomes fairer against the sensitive attribute.

A potential competitive edge of our approach is that we can audit a learnt model at the individual level based on objective criteria: whether the task model is robust to the perturbation of the attribute classifier.
Given an individual, the key problem for testing discrimination is to simulate the corresponding individual with a different protected attribute.
The white-box framework, {\it Counterfactual Fairness} \cite{kusner2017counterfactual} can accurately detect the bias and understand how the model discriminates.
But the reliance on a known causal mechanism limits its application scenarios and may fail to identify instances of legally actionable discrimination.
Some black-box techniques generate mirror individuals via a learnt generative model, e.g. {\it FlipTest} \cite{black2020fliptest}.
Notice that central to assessing fair data generation is a learnt predictor for the sensitive attribute.
However, an intuitive question arises: had an individual been of an opposite sensitive attribute, would the attribute predictor output the opposite attribute?
Without a white-box assumption, the optimal Bayesian classifier may fail to predict the opposite attribute for some individuals, and then the audits run the risk of becoming circular verification. 
The proposed method does not suffer from this problem due to the controllable augmentation.
The perturbation budget during training presents a quantitative standard to perform the model audit. 


We summarize the contributions as follows:
\begin{itemize}
    \item We propose Fair-CDA that precisely applies augmentation for sensitive features to achieve fairness while compromising little accuracy. 
    \item The proposed augmentation is  controllable by tuning the perturbation budget of the adversary and provides an audit criterion via adversarial robustness.
    \item Extensive experiments show that Fair-CDA significantly outperforms state-of-the-art methods in various settings, which is effective and  scalable. For instance, Fair-CDA can be applied to different backbones, different tasks, and real application scenarios.
    
    % For instance, Fair-CDA obtains an 86.3\% relative improvement for fairness while maintaining the accuracy on the widely-used benchmark Adult dataset.
    
    
\end{itemize}



\section{Preliminaries}\label{sec:definition}

Suppose that data points $\{(x_i,y_i)\}$ are drawn according to some unknown joint distribution over $\cX \times \cY$ with $\cX \subseteq \R^d$. Let $A$ denote the given sensitive attribute that should not be treated differently in decision-making.
Without loss of generality, we consider a binary classification task $\cY = \{0,1\}$ with a binary sensitive attribute $A \in \{0,1\}.$
Let $\hat{Y}$ be the predictor, a random variable that is produced by a model $f:\cX  \rightarrow [0,1]$ as a prediction of $Y$.
In this work, we focus on two widely-used group fairness constraints: Demographic Parity (DP)~\cite{dwork2011fairness} and Equalized Odds (EO)~\cite{hardt2016equality}.

\noindent\textbf{Demographic Parity.} A predictor $\hat{Y}$ satisfies demographic parity (DP) if 
$
P(\hat{Y}\mid A=0)=P(\hat{Y} \mid A=1). 
$
% \begin{equation*}
% P(\hat{Y}\mid A=0)=P(\hat{Y} \mid A=1). 
% \end{equation*}
DP requires $\hat{Y}$ to be statistically independent of $A$. In real-world applications, we use DP as the definition to ensure fairness when historically biased decisions may have affected the quality of the collected data and we want to see minority groups receiving positive decisions at the same rate. To evaluate the fairness of a trained model $f$ under this definition, we use the following relaxed metric~\cite{madras2018learning,chuang2021fair}:
\begin{small}
\begin{equation*}
\Delta_{DP}(f) = \big| \mathbb{E}[f(X)|A=0] - \mathbb{E} [f(X)|A=1]  \big|.
\end{equation*}
\end{small}To fulfill the requirement of DP, the evaluation metric $\Delta_{DP}(f)$ shall go to zero.
Since for certain predictions, such as hobbies or expertise, there are indeed differences between groups, meeting the mandatory requirements of DP will greatly decrease the prediction accuracy. Hence, the following alternate criterion is proposed to overcome the limitations of DP.

\noindent\textbf{Equalized Odds.} A predictor $\hat{Y}$ satisfies equalized odds (EO) if 
$
P(\hat{Y} \mid A=0, Y=y)=P(\hat{Y} \mid A=1,Y=y)
$,
% \begin{equation*}
% P(\hat{Y} \mid A=0, Y=y)=P(\hat{Y} \mid A=1,Y=y),
% \end{equation*}
for any $y\in \{0,1\}$. EO requires $\hat{Y}$ to be independent of $A$ conditioned on $Y$. In real-world applications, we use EO as a criterion to ensure fairness if there are strict requirements for making correct predictions and we strongly care about the qualifications of candidates when making decisions. Similarly, to evaluate the fairness of a trained model $f$ under EO, we use the following  metric~\cite{madras2018learning,chuang2021fair}:
\begin{small}
\benrr
\Delta_{EO}(f) &=& \sum_{y\in \{0,1\}} \Big| \mathbb{E}[f(X)|A=0, Y=y] \\
&& - \mathbb{E}[f(X)|A=1, Y=y]  \Big|.
\eenrr
\end{small}Different from DP, EO considers the possible correlation between $Y$ and $A$, it does not rule out the perfect predictor even when the base rates differ across groups.

\section{Fair-CDA}


To fulfill the fairness constraint, we shall reduce the dependence of model predictions on sensitive attributes.
Simply removing the sensitive attributes from the inputs does not necessarily lead to a non-discriminatory model prediction, as other attributes in the inputs might encode information for inferring the sensitive attributes~\cite{dwork2011fairness,feldman2015certifying}. Hence, we need to decompose the representations of the inputs into sensitive and non-sensitive features. Sensitive features encode information that can identify whether the inputs belong to a certain group determined by the sensitive attributes, while non-sensitive features retain as much other information as possible~\cite{Zemellearningfair}. Moreover, we shall obfuscate the sensitive features to obtain a fair model. To decompose the high-level representations of the inputs, we train a task model to predict both data labels and sensitive attributes with an orthogonality constraint on gradients for the intermediate features~\cite{bai2020decaug}.


\noindent\textbf{Feature Disentanglement.}
Consider a task with training data $\{(x_i,y_i,a_i)\}_{i=1}^n.$
To decompose the representations,
we denote three feature extractor: $h$, $h_y$ and $h_a$, and write their output features as
\benrr 
z_i = h(x_i), \quad z_i^y = h_y(z_i), \quad  z_i^a = h_a(z_i).
\eenrr 
The mapping $h$ is a pre-extractor that learns the high-level representations of the input.
Then $h_a$ and $h_y$ are two additional extractors after $h$ to obtain sensitive and non-sensitive features.
The principle here is to enforce $h_y$ to extract features that affect the label prediction loss the most will not affect the sensitive attribute prediction loss and vice versa.
Therefore we design a regularization term as follows:
\benr\label{regularization1}
\beta (\cL_i^y + \cL_i^a +  \cL_i^{\perp}),
%\beta_1 \cL_i^y + \beta_2 \cL_i^a + \beta_3 \cL_i^{\perp},
\eenr
where $\beta$ is a tuning parameter and
\benrr
&& \cL_i^y := \cL_i^y(h, h_y, g_y) = \ell( g_y(z_i^y), y_i), \\
&& \cL_i^a := \cL_i^a(h, h_a, g_a) = \ell( g_a(z_i^a), a_i),
\eenrr
and
\begin{small}
\benn
\cL^{\perp}_i := \cL^{\perp}_i(h_y, h_a, g_y, g_a) =  \frac{\langle \nabla_{z_i} \cL_i^y, \nabla_{z_i} \cL_i^a \rangle^2}{\|\nabla_{z_i} \cL_i^y\|^2\cdot \| \nabla_{z_i} \cL_i^a \|^2}.  
\eenn
\end{small}Here $g_y$ and $g_a$ are two classifier to predict $y$ and $a$ and $\ell$ is the cross-entropy loss. 
The term $\cL^{\perp}_i$ imposes a constraint on gradient orthogonality to disentangle features.
To estimate the feature extractors and the classifiers, Stage 1 of Fair-CDA (Figure.~\ref{fig:model}) minimizes the objective function:
\benrr
 \frac{1}{n} \sum_{i=1}^{n} \cL_i + \beta ( \cL_i^y + \cL_i^a + \cL_i^{\perp}),
\eenrr
where $\cL_i$ is the loss function of the task model $g$ over the disentangled features:
\benr\label{loss1}
\cL_i:=\cL_i(h, h_y,h_a,g)=\ell( g([z^y_i,z^a_i]), y_i). 
\eenr 





\noindent{\bf Semantic Augmentation.} 
In Stage 2 of Fair-CDA, 
we do an intervention on the sensitive features to mitigate unfair biases.
Intuitively, a model satisfies the requirement of the fairness constraint if it can make the same prediction for two samples with different sensitive features but the same other features~\cite{kusner2017counterfactual}. 
We augment the sensitive features along the direction of increasing the attribute prediction loss $\cL_i^a$:
\begin{equation}\label{equ:featureaug}
\tilde{z}^a_i = z^a_i + \alpha_i \ \frac{\nabla_{z_i^a} \ell(g_a(z_i^a), a_i)}{\left\| \nabla_{z_i^a} \ell(g_a(z_i^a), a_i) \right\|},
\end{equation}
where $\alpha_i$ is a perturbation size.
Since the direction of the gradient is the direction in which the loss increases most rapidly, augmentation in this way changes $z_i^a$ to the features corresponding to the other sensitive attribute.


\noindent{\bf Transition Path.} \citet{chuang2021fair} interpolates the transition path between groups via Mixup.
\citet{zhang2021does} proves that the adversarial loss can be bounded above by the Mixup loss.
Therefore we generate the transition path over sensitive features by randomizing the perturbation size. 
In this work, we assume $\alpha_i$ is a random variable follows a uniform distribution over $[0, \lambda].$
Here $\lambda$ is the perturbation budget of (\ref{equ:featureaug}) that controls the strength of the augmentation.
After obtaining the generated sensitive features $\tilde{z}_i^a$, we concatenate them with the non-sensitive features $z_i^y$ and train the task model $g$ with $\{ ([z^y_i, \tilde{z}^a_i], y_i) \}.$ The loss function of $g$ over the augmented features is denoted by
\begin{equation}\label{equ:augoriloss}
\tilde{\cL}_i:=\tilde{\cL}_i(h, h_y,h_a,g)=\ell( g([z^y_i, \tilde{z}^a_i]), y_i). 
\end{equation}
together with the aforementioned losses $\cL^1_i$, $\cL^2_i$ and $\cL^{\perp}_i$. 


Different from existing works~\cite{lahoti2019ifair,zafar2017fairness,chuang2021fair}, our method strikes a balance between accuracy and fairness via adjusting the perturbation budget $\lambda$. 
When the perturbation is large, the augmented features can be classified into the opposite group with a high probability. 
Then the classifier learnt with large perturbations becomes fairer against the sensitive attribute.



\noindent\textbf{Imputation Model.}
In Stage 2, we use the labels of the original samples to mark the corresponding augmented features.
This is based on the intuition that a model is non-discriminatory if it makes the same prediction for two samples only differing in sensitive features.
However, for certain predictions, there are indeed correlations between labels and sensitive features. Enforcing the model to meet the mandatory requirement of the fairness constraint and ignoring the possible correlations between labels and sensitive features may decrease the prediction accuracy a lot. To further improve the prediction accuracy, we introduce an imputation model to calibrate the labels of the augmented features.
Specifically,  the Stage 1 solution of the task model, denoted by $\check{g}$, is taken to be the imputation model to label the augmented features: $\check{y}_i = \check{g}([z^y_i, \tilde{z}^a_i]).$ 
The loss of predicting $\check{y}_i$ is denoted by
\benr\label{equ:augimploss}
\check{\cL}_i= \check{\cL}_i(h, h_y, h_a, g)=\ell(g([z^y_i, \tilde{z}^a_i]), \check{y}_i).
\eenr
The task model is then trained to predict both the original labels and the labels given by the imputation model. We formulate the final problem of Fair-CDA as minimizing:
\benr\label{equ:final_objective}
\frac{1}{n} \ \sum_{i=1}^{n} \gamma \tilde{\cL}_i + (1-\gamma) \check{\cL}_i + \beta ( \cL_i^1 + \cL_i^2 + \cL_i^{\perp}),
\eenr
where $\gamma$ is a hyper-parameter balancing $\tilde{\cL}_i$ and $\check{\cL}_i.$
For time-saving, our method initializes with the imputation model to solve the optimization problem in (\ref{equ:final_objective}). 


\begin{algorithm}[t]\small
\caption{Fair-CDA: Continuous and Directional Augmentation for Group Fairness}
\label{algo1}
\begin{algorithmic}[1]
\REQUIRE Training data $\{(x_i,y_i,a_i)\}_{i=1}^n$, batch sizes $b$, learning rate $\eta_1$, $\eta_2$, perturbation strength $\lambda$, weights $\gamma$, $\beta$, iteration number $T$, $S$
\ENSURE $\theta =(h,h_y,h_a, g, g_y, g_a)$;\\
\noindent{\bf Stage 1:}
\STATE Initialize $\theta^{(0)} =(h^{(0)},h_y^{(0)},h_a^{(0)}, g^{(0)}, g_y^{(0)}, g_a^{(0)})$;
\FOR {$1 \leq t \leq T$} 
\STATE Sample a batch of training data $\{(x_i,y_i,a_i)\}_{i=1}^b$;
\STATE Compute $\cL_i$, $\cL_i^y$, $\cL_i^a$, and $\cL_i^{\perp}$ according to Eq.~(\ref{regularization1}) and Eq.~(\ref{loss1})
\STATE Update $\theta$\\
$\theta^{(t)} = \theta^{(t-1)} -  \frac{\eta_1}{b} \sum\limits_{i=1}^{b} \nabla_\theta \big(\cL_i + \beta( \cL_i^y + \cL_i^a +  \cL_i^{\perp})\big)$;
\ENDFOR\\
\noindent{\bf Stage 2:}
\FOR {$1 \leq s \leq S$} 
		\STATE Sample a batch of training data $\{(x_i,y_i,a_i)\}_{i=1}^b$;
		\FOR {each $(x_i,y_i,a_i)$} 
		\STATE Compute $z_i^y = h_y^{(T+s-1)}\circ h^{(T+s-1)}(x_i)$ and $z_i^a = h_a^{(T+s-1)}\circ h^{(T+s-1)}(x_i)$;
		\STATE Compute $\cL_i^y$, $\cL_i^a$ and $\cL_i^{\perp}$;
		\STATE Randomly draw $\alpha_i$ according to $U(0,\lambda)$;
		\STATE Generate $\tilde{z}^a_i$ according to Eq.~\eqref{equ:featureaug};
		\STATE Compute $\tilde{\cL}_i$ according to Eq.~\eqref{equ:augoriloss};
		\STATE Impute the label $\tilde{y}_i =g^{(T)}([z^y_i, \tilde{z}^a_i])$;
		\STATE Compute $\check{\cL}_i$ according to Eq.~\eqref{equ:augimploss};
		\ENDFOR
		\STATE Update $\theta$: 
        \begin{equation*}
        \begin{split}
            \theta^{(T+s)} =& \theta^{(T+s-1)} - \frac{\eta_2}{b} \sum\limits_{i=1}^{b} \nabla_\theta \big(\gamma \tilde{\cL}_i + (1-\gamma) \check{\cL}_i \\
            & + \beta ( \cL_i^y +  \cL_i^a + \cL_i^{\perp}) \big);
        \end{split}
        \end{equation*}       
		\ENDFOR
	\end{algorithmic}
%	\vspace{-5pt}	
\end{algorithm}


\noindent{\bf Summary.} As mentioned above, Fair-CDA balances the prediction accuracy and fairness via adjusting the perturbation strength $\lambda$. 
The algorithm is summarized in Algorithm~\ref{algo1}.
Stage~1 disentangles features and learns the task model with the original training samples. In Stage~2, we fine-tune the task model with the augmented features to achieve fairness.


Our method introduces three additional hyper-parameters: two weights of different losses $\beta$ and $\gamma$, and perturbation budget $\lambda.$ In the experiment, we set $\beta$ according to the initial loss values to make different loss values in the same magnitude range. We adjust $\gamma$ on the Adult dataset~\cite{Dua2017} to get the best accuracy and fairness trade-off on the validation set and then adopt the same value which is 0.9 for all the datasets. Our method balances the prediction accuracy and fairness via adjusting the perturbation strength $\lambda$, while previous works~\cite{chuang2021fair,zhang2018mitigating} balance them via adjusting the weights of the regularization terms. On different datasets, we first conduct experiments with $\lambda=0,1,10,100,1000$ to
narrow down the range of $\lambda$ and then, do grid search between the determined range of $\lambda$ (reported in Appendix) with a budget of 20 points to generate the Pareto Front in Figure~\ref{fig:adult_dataset},\ref{fig:celeba_dataset},\ref{fig:movielens},\ref{fig:ablation_movielens_dataset}. In real-world applications, the number of grid search points can be determined according to the budget.

\section{Experiments on Public Datasets}\label{sec:experiment}

We evaluate Fair-CDA on tabular dataset Adult~\cite{Dua2017}, vision dataset CelebA~\cite{liu2018large}, and recommender dataset MovieLens~\cite{harper2015movielens}. 
We demonstrate the effectiveness of Fair-CDA across diverse tasks and task models. In the ablation studies, we examine the contributions of feature decomposition and
the imputation model.
We compare Fair-CDA with other baseline methods: ERM, GapReg~\cite{chuang2021fair}, AdvDebias~\cite{zhang2018mitigating}, and Mixup / Manifold Mixup~\cite{chuang2021fair} using two metrics: prediction accuracy and fairness. 
To measure the accuracy, we use Average Precision (AP) for tabular (Adult) and vision (CelebA) tasks, and Area Under Curve (AUC) for recommender (MovieLens) task. To measure fairness, we use two widely-used fairness metrics: Demographic Parity (DP) and Equalized Odds (EO) which are defined in Preliminaries. Also, we compare our method with FFAVE and $\beta$-VAE following the setting in~\cite{creager2019flexibly}. Please refer to the Appendix for more details about the datasets.


\noindent\textbf{Unjustified biases from the observed data.} We count the imbalance in the number of training data across sensitive attribute groups and the detailed statistical data are shown in Table~\ref{table:statistics}. In the Adult dataset, the proportion of males with high salaries is significantly higher than that of females. In the CelebA dataset, the proportion of males with a positive label is significantly lower than that of females. In the MovieLens dataset, movies from minority producers also have different positive rates from that of another group. All these imbalances and biases can be inherited and amplified by the models.


\begin{table}[t]
    \centering\small
        %\resizebox{0.47\textwidth}{!}{
        \begin{tabular}{llll}
         \hline\hline
        Task & Attribute & Label %& Count 
        &Ratio   \\
        \hline
        Adult & Female&	Salary$<=50k$	%&7846 
        &88.7\%\\
        &Female&	Salary$>50k$ %&1004
        &11.3\%\\
        & Male&	Salary$<=50k$	%&12530
        &68.5\%\\        
        & Male&	Salary$>50k$	%&5752
        &31.5\%\\         
        \hline
        CelebA & Female& Not Smiling %&43688
        &46.2\%\\
        (Smiling) & Female& Smiling %&50821
        &53.8\%\\
        & Male& Not Smiling %&41002
        &60.1\%\\
        & Male& Smiling %& 27259
        &39.9\% \\
        \hline
        CelebA  & Female&	Not Wavy Hair %&52289
        &55.3\%\\
        (Wavy Hair) & Female& Wavy Hair %&42220
        &44.7\%\\
        & Male&	Not Wavy Hair %&58499
        &85.7\%\\
        & Male& Wavy Hair %&9762
        &14.3\%\\ 
        \hline
        CelebA  & Female &	Not Attractive	%&29920
        &31.7\%\\
        (Attractive) & Female&Attractive	%&64589
        &68.3\%\\
        &Male&Not Attractive	%&49247
        &72.1\%\\
        &Male& Attractive	%&19014
        &27.9\%\\
        \hline
        MovieLens& Minority & Not Recommend %&192504
        &41.4\%\\
        &Minority &	Recommend %&272139
        &58.6\%\\
        &Majority & Not Recommend %&147136
        &44.1\%\\
        &Majority & Recommend %&186648
        &55.9\% \\
        \hline
        \hline
        \end{tabular}
        \caption{Statistical data of different tasks on three datasets.}
    \label{table:statistics}
        %}
    \vspace{-5pt}
\end{table}


% \begin{table}[t]
%     \centering\small
%         %\resizebox{0.47\textwidth}{!}{
%         \begin{tabular}{llll}
%          \hline\hline
%         Task & Sensitive Attribute & Label %& Count 
%         &Ratio   \\
%         \hline
%         Adult & Female&	Salary$<=50k$	%&7846 
%         &88.7\%\\
%         &Female&	Salary$>50k$ %&1004
%         &11.3\%\\
%         & Male&	Salary$<=50k$	%&12530
%         &68.5\%\\        
%         & Male&	Salary$>50k$	%&5752
%         &31.5\%\\         
%         \hline
%         CelebA & Female& Not Smiling %&43688
%         &46.2\%\\
%         (Smiling) & Female& Smiling %&50821
%         &53.8\%\\
%         & Male& Not Smiling %&41002
%         &60.1\%\\
%         & Male& Smiling %& 27259
%         &39.9\% \\
%         \hline
%         CelebA  & Female&	Not Wavy Hair %&52289
%         &55.3\%\\
%         (Wavy Hair) & Female& Wavy Hair %&42220
%         &44.7\%\\
%         & Male&	Not Wavy Hair %&58499
%         &85.7\%\\
%         & Male& Wavy Hair %&9762
%         &14.3\%\\ 
%         \hline
%         CelebA  & Female &	Not Attractive	%&29920
%         &31.7\%\\
%         (Attractive) & Female&Attractive	%&64589
%         &68.3\%\\
%         &Male&Not Attractive	%&49247
%         &72.1\%\\
%         &Male& Attractive	%&19014
%         &27.9\%\\
%         \hline
%         MovieLens&Minority Producer&Not Recommend %&192504
%         &41.4\%\\
%         &Minority Producer&	Recommend %&272139
%         &58.6\%\\
%         &Majority Producer& Not Recommend %&147136
%         &44.1\%\\
%         &Majority Producer& Recommend %&186648
%         &55.9\% \\
%         \hline
%         \hline
%         \end{tabular}
%         \caption{Statistical data of different tasks on three datasets.}
%     \label{table:statistics}
%         %}
%     \vspace{-5pt}
% \end{table}


\noindent\textbf{Implementations.} Our framework is implemented with PyTorch 1.4 (under BSD license), Python 3.7, and CUDA v9.0. For the baseline methods, we implement with PyTorch 1.3.1 to keep the same setting as their source code. We conducted experiments on NVIDIA Tesla V100. The results of baseline methods on Adult and CelebA are referenced from~\cite{chuang2021fair}, while the results of baseline methods on MovieLens are implemented by ourselves.


\begin{figure}[t]
\centering
 \includegraphics[width=\linewidth]{figs/adult_combine.pdf}
% \includegraphics[width=1.5\linewidth]{figs/adult_combine.pdf}
\caption{%{Results on Adult.} 
The trade-off between AP and $\Delta_{DP}$ / $\Delta_{EO}$ on Adult dataset.}  %Fair-CDA is the only method that consistently achieves higher AP than ERM under two fairness constraints.}
\label{fig:adult_dataset}
%\vspace{-5pt}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=1.\linewidth]{figs/stage_1_2_combine.pdf}
\caption{The trend of loss and performance during two stages on Adult dataset. The orange color in the figure corresponds to Stage 1, while the blue color represents Stage 2. In Stage 1, Fair-CDA mainly optimizes the prediction accuracy, while in Stage 2, Fair-CDA mainly optimizes the model fairness.}
\label{fig:stage_curve}
%\vspace{-10pt}
\end{figure}


\begin{figure}[t]
\centering
\includegraphics[width=.93\linewidth]{figs/saliency_map.png}
\caption{Saliency map on the wavy hair recognition task. The saliency maps of sensitive features focus more on the whole face, while those of non-sensitive features focus more on the hair of a man/woman.}
\label{fig:saliency_map}
%\vspace{-5pt}
\end{figure}


% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.8\textwidth]{figs/smile_combine.png} % Reduce the figure size so that it is slightly narrower than the column.
% \caption{Adjusting the bounding box instead of actually removing the unwanted data resulted multiple layers in this paper. It also needlessly increased the PDF size. In this case, the size of the unwanted layer doubled the paper's size, and produced the following surprising results in final production. Crop your figures properly in a graphics program. Don't just alter the bounding box.}
% \label{fig2}
% \end{figure*}

\begin{figure*}[t]
\begin{subfigure}{.32\textwidth}
\centering
\includegraphics[width=.9\linewidth]{figs/smile_combine.png}
\caption{Smiling}
\label{fig:celeba_smiling}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
\centering		\includegraphics[width=.9\linewidth]{figs/wavyhair_combine.png}
\caption{Wavy Hair}
\label{fig:celeba_wavy}
\end{subfigure}
\begin{subfigure}{.32\textwidth}
\centering
\includegraphics[width=.9\linewidth]{figs/attractive_combine.png}
\caption{Attractive}
\label{fig:celeba_attractive}
\end{subfigure}
% \vspace{-5pt}
\caption{{CelebA.} The trade-off between AP and $\Delta_{DP}$ / $\Delta_{EO}$. Fair-CDA outperforms other methods across tasks.}
\label{fig:celeba_dataset}
%\vspace{-5pt}
\end{figure*}



\subsection{Results and Discussion}

\noindent\textbf{Results on Adult dataset.} Fair-CDA achieves State-Of-The-Art (SOTA) performance in terms of both fairness and accuracy on the Adult dataset, as shown in Figure~\ref{fig:adult_dataset}. ERM has a moderate AP but poor fairness, while GapReg achieves better fairness but lower AP than ERM. It utilizes the fairness constraint in the training phase, which lacks generalization at evaluation time. 
Fair Mixup achieves a better trade-off compared to the previous three methods but is dominated by Fair-CDA. In particular, Fair-CDA is the only method consistently achieving a higher AP than ERM under two fairness constraints.

To evaluate the feature augmentation, we sample 1,000 training samples from the Adult dataset, extract the sensitive features with the trained model, and generate the augmented features.  
The trained gender classifier, whose prediction accuracy is 86.8\% when using the original sensitive features, predicts opposite labels for all the augmented features.
This means the augmentation policy successfully generates the features corresponding to the opposite sensitive attribute.


\begin{figure}[t]
	\centering
	\includegraphics[width=.98\linewidth]{figs/movielens_combine.pdf}
	\caption{{MovieLens.} The trade-off between AP and $\Delta_{DP}$ / $\Delta_{EO}$. 
	Fair-CDA can reach the smallest $\Delta_{DP}$ / $\Delta_{EO}$ among all the methods without obvious accuracy degradation.}
	\label{fig:movielens}
	%\vspace{-5pt}
\end{figure} 


To evaluate the prediction accuracy of the imputation model on the augmented samples, we select all the pairs of training samples (208 pairs in total) with the same other attributes but different sensitive attributes and different labels. Intuitively, an accurate imputation model should predict the opposite label for the augmented samples since every augmented sample has a real sample with an opposite label corresponding to it. The imputation model predicts the opposite label on 258 augmented samples (out of 416 samples), which means the prediction accuracy of the task model can be improved with label calibration. 


To better understand the training process of Fair-CDA, we plot the trend of loss and performance during two stages on the Adult dataset, as shown in Figure~\ref{fig:stage_curve}. Stage 1 stands for the process of the model trained with original data, while Stage 2 stands for that of the model trained with augmented data. %{\color{red}
At the beginning of Stage 1, the model is initialized with random parameters (without pre-training). The mean value
of the output of both groups tends to be very close, resulting in low fairness disparity. As the training process goes
on, the loss of Fair-CDA converges gradually with the AP rising to a high point. Both DP and EO reach a high value (which means poor fairness). In Stage 2, the AP remains stable, the loss fluctuates at the beginning, and finally drops to a low point. Both DP and EO reach a low value. %(which means good fairness).

\begin{figure}[t]
	\centering
\includegraphics[width=.99\linewidth]{figs/ablation_combine.png}
	\caption{Ablation studies on MovieLens dataset. 
	Fair-CDA without imputation model (Fair-CDA (no IM)) can still satisfy the fairness requirement but suffer an accuracy loss. By generating the samples with opposite sensitive attributes (Attribute-Level), the unfairness can hardly be decreased.
	}
\label{fig:ablation_movielens_dataset}
%\vspace{-5pt}
\end{figure}

\noindent\textbf{Results on CelebA.} To illustrate each modelâ€™s ability for vision tasks, we choose smiling, wavy hair, and attractive to form three binary classification tasks. As shown in Figure~\ref{fig:celeba_dataset}, Fair-CDA achieves SOTA performance followed by two mixup methods. It is worth mentioning that the DP and EO gap of these methods on the smiling recognition task is smaller compared with other tasks, which is a relatively fair scenario, but Fair-CDA can still improve the fairness. 
Also, Fair-CDA is the only method that achieves considerable accuracy given high fairness requirements on both tasks. 

To visualize the effect of feature decomposition, we adopt deep neural network
interpretability methods in~\cite{adebayo2018sanity}.
We draw the saliency map on the wavy hair recognition task, as shown in Figure~\ref{fig:saliency_map}. Sensitive features are those strongly related to gender, while non-sensitive features are those strongly related to wavy hair. It can be seen that the saliency maps of sensitive features focus more on the whole face, while those of non-sensitive features focus more on the hair of a man/woman. 

Additionally, we evaluate Fair-CDA on more sensitive features on CelebA dataset. We implement Fair-CDA on the same task as that in~\cite{creager2019flexibly} (CelebA Heavy-Makeup recognition task) to compare our method with two VAE methods. Noted that the fairness metric is demographic parity and the performance metric is accuracy in this setting. As shown in Table~\ref{table:more_attribute}, Fair-CDA outperforms FFVAE and $\beta$-VAE on CelebA Heavy-Makeup recognition task considering three different sensitive attributes.


\begin{table}[!t]
    \centering\small
    % \caption{Results of Fair-CDA on more sensitive features on CelebA dataset. Compared with two VAE methods, Fair-CDA improves the fairness measurement $\Delta_{DP}$ and accuracy significantly.}
    % \label{table:more_attribute}
    \begin{tabular}{llll}
    \hline
    \hline
         & Male  & Chubby &  Eyeglasses \\ 
        ~ & $\Delta_{DP}$/Acc  & $\Delta_{DP}$/Acc  & $\Delta_{DP}$/Acc  \\ \hline
         $\beta$-VAE & 0.330/0.712 & 0.202/0.732 & 0.250/0.715  \\ 
        ~ & 0.400/0.725 & 0.220/0.740 & 0.280/0.735  \\ \hline
        FFVAE & 0.330/0.730 & 0.202/0.748 & 0.250/0.725  \\ 
        ~ & 0.400/0.752 & 0.400/0.825 & 0.400/0.824  \\ \hline
        Fair-CDA & 0.234/0.733 & 0.184/0.816 & 0.217/0.814  \\
        ~ & 0.369/0.836 & 0.197/0.825 & 0.245/0.824  \\ \hline\hline
    \end{tabular}
    \caption{Results on CelebA dataset. Compared with two VAE methods, Fair-CDA improves the fairness measurement $\Delta_{DP}$ and accuracy significantly.}
    \label{table:more_attribute}
    %\vspace{-5pt}
\end{table} 



\noindent\textbf{Results on MovieLens dataset.}
Recommendation, a common scenario of machine learning, poses unique challenges for applying fairness and non-discrimination concepts. We choose the rating recognition task on MovieLens to evaluate different fair methods. Similar to the trends on the Adult dataset, Fair-CDA achieves SOTA performance
followed by Fair Mixup and GapReg, as shown in Figure~\ref{fig:movielens}. AdvDebias achieves better fairness than ERM accompanied by severe accuracy degradation. In addition, Fair-CDA can reach the smallest $\Delta_{DP}$ and $\Delta_{EO}$ among all the methods, which shows its superior ability to obtain the group fairness.





 
\subsection{Ablation Studies}
 

\noindent\textbf{Without imputation model.} To examine whether the imputation model contributes to performance, we train Fair-CDA without imputation model (Fair-CDA (no IM)) on MovieLens dataset and plot the Pareto Front in Figure~\ref{fig:ablation_movielens_dataset}. We can see the Pareto Front of Fair-CDA dominates that of Fair-CDA (no IM) for both DP and EO. Without an imputation model, Fair-CDA can still achieve good fairness but suffer a little accuracy loss. 

\noindent\textbf{Sample generating at the attribute level.} To illustrate the effectiveness of feature decomposition, we use a naive way to generate the flip sample. Simply flipping the value of the sensitive attribute with a specific probability during the training phase, we can get the results of model training with different data distributions. We name the method as Attribute-Level. By setting different probabilities, we can get the Pareto Front, as shown in Figure~\ref{fig:ablation_movielens_dataset}. Compared with ERM, Attribute-Level can mitigate the unfairness to some extent, while it can not solve the problem mentioned earlier: other variables correlated with sensitive attributes can serve as a source for unfairness.

\section{Experiments on Product.}

To further validate our algorithm in realistic scenarios, we deploy Fair-CDA in an online course recommender system. There are nearly 100,000 users and more than 12,000 courses developed by more than 100 different suppliers, nearly 50\% of the courses coming from top 5\% suppliers. Thus we consider supplier as the sensitive attribute to evaluate fairness. Similar to the previous setting on MovieLens, we divide the suppliers into the majority and minority groups according to the number of courses developed by the suppliers. The top 5\% suppliers who provide nearly 50\% of the courses are regarded as the majority supplier and the remaining suppliers are regarded as the minority supplier. In this scenario, we choose Equalized Odds as the fairness measurement since it has been shown that demographic parity causes a loss in the utility and infringes individual fairness~\cite{singh2018fairness}, and we adopt AUC and Top-10 Recall as the offline accuracy 
evaluation. We use LightGCN~\cite{LightGCN2020} as the backbone network and compare Fair-CDA with the original LightGCN method. The results are shown in Table~\ref{table:offline}. Fair-CDA achieves better performance on both accuracy and fairness measurements than the baseline method.


\begin{table}[t]
    \centering\small
    % \caption{Offline results of Fair-CDA on a product dataset from an online course recommender system. Compared with the baseline method, Fair-CDA improves the fairness measurement $\Delta_{EO}$ significantly.}
    % \label{table:offline}
%	\begin{adjustbox}{max width=0.49\textwidth}
        \resizebox{0.47\textwidth}{!}{
        \begin{tabular}{lccc}
        \hline\hline
        Method & AUC&  $\Delta_{EO}$ &  Top-10 Recall  \\
        \hline
        LightGCN (Baseline)  & 0.9503 &  0.0448  & 0.1116   \\
        \hline
        Fair-CDA & 0.9679    & 0.0227 & 0.1328 \\
        \hline\hline
        \end{tabular}
        }
        \caption{Offline results on a product dataset from an online course recommender system.} %Compared with the baseline method, Fair-CDA improves the fairness measurement $\Delta_{EO}$ significantly.}
    \label{table:offline}
    %\end{adjustbox}
    \vspace{-5pt}
\end{table}

Inspired by the performance of offline evaluation, we implement and deploy Fair-CDA in the production environment and verify its effectiveness through a consecutive online A/B test. We split the users into two groups uniformly, each of which has an average of 3000 users every week. The first group gets courses recommended by the baseline model, and the Fair-CDA generates recommendations for the other group. The two models are updated daily. After a 5-week online A/B test, the Fair-CDA is consistently superior than the baseline model, with an average Click Through Rate (CTR) improvement of 6.5\%. During the online A/B test, our method increases the diversity of recommended courses and enhances group fairness, resulting in a higher CTR.

\section{Conclusions}

We propose {\it Fair-CDA} to counter the unfairness problem via feature decomposition and data augmentation. Fair-CDA improves fairness and minimizes the impact on accuracy. We experimentally compare our method with other state-of-the-art fairness methods on various benchmarks and show that Fair-CDA significantly outperforms the other methods in all the experimental settings.


 
%\appendix

\section*{Acknowledgements}

This work was partially supported by JCYJ20220530143600001,
by the Basic Research Project No. HZQB-KCZYZ-2021067 of Hetao Shenzhen HK S\&T Cooperation Zone, by the National Key R\&D Program of China with grant No.2018YFB1800800, by SGDX20211123112401002, by Shenzhen Outstanding Talents Training Fund, by Guangdong Research Project No. 2017ZT07X152 and No. 2019CX01X104, by the Guangdong Provincial Key Laboratory of Future Networks of Intelligence (Grant No. 2022B1212010001), by the NSFC 61931024\&8192 2046, by NSFC-Youth 62106154, by zelixir biotechnology company Fund, by Tencent Open Fund, and by ITSO at CUHKSZ. Chuanlong Xie was partially supported by NSFC No.12201048 and the Interdisciplinary Intelligence SuperComputer Center of Beijing Normal University at Zhuhai.


\nocite{*}
\bibliography{aaai23}


%\input{appendix.tex}


\end{document}
