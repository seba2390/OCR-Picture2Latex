% \section{Char-level Syntactic Parsing for Chinese Words}\label{char-pasrsing}

\section{Word-internal Structure Parsing}\label{sec:WIS-parsing}

% 【词内部结构parsing叫char-level，句法parsing叫sentence-level好像不合适】

With annotated WIST, we try to address the second question: can we train a model to predict word-internal structure? We adapt the Biaffine parser proposed by \citet{dozat2016deep}, a widely used sentence-level dependency parser, for this purpose, and present results and analysis.  

\subsection{Biaffine Parser}

%To produce syntactic char-level structures for Chinese words, we build a word-internal structure parsing model based on the state-of-the-art biaffine parser of \citet{dozat2016deep} with two modifications to accomodate our task, i.e., replacing the word and POS tag embeddings with the character embeddings for char-level representation, 
% replacing the original non-projective MST algorithm with first-order Eisner algorithm \cite{Eisner2000} for projective decoding.
\input{figure/biaffine-framework}
We adopt the SuPar implementation released by  \citet{zhang-etal-2020-dep}.\footnote{\url{https://github.com/yzhangcs/parser}}  
As a graph-based parser, Biaffine parser casts a tree parsing task as searching for a maximum-scoring tree from a fully-connected graph, with nodes corresponding to characters in our case. As shown in Figure \ref{fig:biaffine-parser}, it adopts standard encoder-decoder architecture, consisting of the following components.  
%Biaffine parser is a graph-based dependency parser which employs a deep biaffine scoring architecture to compute the score of each dependency and aims to find the highest-scoring dependency tree.
%\subsection{Model Architecture}
% The model architecture is shown in Figure xxx, consisting of following four components. 
% it consists of three major layers. 

\textbf{Input layer.} 
Given an input sequence, 
%consisting of $n$  $c_0c_1...c_n$, where $c_i$ is the $i$-th character and $c_0$ is a pseudo character used as tree root. 
each item is represented as a dense vector $\mathbf{x}_i$. 
For word-internal structure parsing, an item corresponds to a character, and we use char embedding. 
%the char representation output by BERT. 
%using the character embeddings $\mathbf{emb}^{c}$:
\begin{equation}
\mathbf{x}_i = \mathbf{emb}(c_i) 
% \oplus \mathbf{emb}^{bc}(c_{i-1}c_i)
\end{equation}

\textbf{BiLSTM encoder.} 
Then, a three-layer BiLSTM is applied to obtain  context-aware representations. 
We denote the hidden vector of the top-layer BiLSTM for  the i-th position as $\mathbf{h}_i$.

\textbf{Biaffine scorer.} %MLP feature abstraction.} 
Two separate MLPs are applied to each $\mathbf{h}_i$, resulting in 
%fed into two separate MLPs to 
%producing 
two lower-dimensional vectors $ \mathbf{r}_{i}^{h}$ (as head) and $\mathbf{r}_{i}^{d}$ (as dependent).
Then the score of a dependency $i \rightarrow j$ is obtained via a biaffine attention over $\mathbf{r}_{i}^{h}$ and $\mathbf{r}_{j}^{d}$. 
%To save space, we omit the 
Scoring of labeled dependencies such as $i \xrightarrow{l} j$ is analogous. 

%, representing  as a head and a dependent respectively. 
% \begin{equation}
%     \mathbf{r}_{i}^{h} = {\textup{MLP}}^{h} \left(\mathbf{h}_i \right) ;
%     \mathbf{r}_{i}^{d} = {\textup{MLP}}^{d} \left(\mathbf{h}_i \right)
% \end{equation} 
%\textbf{Biaffine classifier.} 
%The biaffine classifier computes 
% \begin{equation} \label{eq:biaffine}
%  \texttt{s}(i \leftarrow j) =  \left[
%  \begin{array}{c}
%     \mathbf{r}_{i}^{d}    \\
%       1 
%  \end{array} 
%  \right]^\mathrm{T}  \mathbf{W}^{biaffine}  \mathbf{r}_{j}^{h} 
% \end{equation} 
% where ${\mathbf{W}^{biaffine}}$ is a biaffine parameter.

% Analogously, the parser uses extra MLPs and a biaffine attention to compute scores for dependency labels. Please refer to \citet{dozat2016deep} for details.

\textbf{Decoder. }
%During decoding, the biaffine parser treats unlabeled dependency tree searching and dependency labeling as two cascaded tasks.
% First, with the scores of all dependencies, the score of a unlabeled dependency tree $\bm{y}$ is:
% \begin{equation}
% \begin{split}
% \texttt{s}(\bm{y})= \sum_{{{i \leftarrow j} \in \bm{y}}}\texttt{s}(i \leftarrow j)
% \end{split}
% \end{equation}
With the scores of all dependencies, 
we adopt the first-order algorithm of \citet{Eisner2000} % for projective decoding 
to find the optimal unlabeled dependency tree, and then independently decide the highest-scoring label for each arc. 
%$\bm{y}^{*}$ with the highest score from all the possible trees.
%Then, the biaffine parser finds the highest-scoring label for each dependency of the optimal tree $\bm{y}^{*}$.


\textbf{Training loss.} %with Local Char-wise Loss}
During training, the parser computes two independent cross-entropy losses for each position, i.e., maximizing the probability of its correct head and
%, and maximizing the probability of 
the correct label between them. 
%$c_j$ and the corresponding correct dependency label $l$ for each character $c_i$ independently.
% \begin{equation}
% \begin{split}
%  \texttt{loss}(i \xleftarrow{l} j) = &-\log{ 
% \frac{e^{\texttt{s}(i \leftarrow j)}}
% {\sum\limits_{0 \le k \le n, k \neq i}e^{\texttt{s}(i \leftarrow k)}} 
% }\\
% & -\log{ 
% \frac{e^{\texttt{s}(i \xleftarrow{l} j)}}
% {\sum\limits_{l' \in \mathcal{L}}e^{\texttt{s}(i \xleftarrow{l'} k)}} 
% }
% \end{split}
% \end{equation}
% where $\texttt{s}(i \xleftarrow{l} j)$ is the score of labeling the dependency $i \leftarrow j$ as $l$. $\mathcal{L}$ is the whole dependency label set.




