\section{Related Work}\label{sec:related-work}

\input{content/tbl-all-labels}

% \paragraph{Linguistic studies on word-formation.} 
% 得找一些语言学的理论支撑，构词法，相当一部分是遵循句法规则，和句子级别的句法构成是相通的。

% 构词法的探索，多粒度分词（引用我们的文章），词语的定义（这个也是不严谨的）

% structure好像不常用复数，可数也不可数。少用复数吧，除非很明确的，比如gramatical structures of a languages（不限于一种结构，有很多不同结构）。

\paragraph{Annotating word-internal structure.}% for syntactic parsing.} 

In the deep learning (DL) era, pretraining techniques are extremely powerful in handling large-scale unlabeled data, including Skip-Gram or CBOW models \cite{mikolov2013efficient} for learning context-independent word embedding in the beginning, and the recent ELMo \cite{peters2018deep} or BERT \cite{devlin-2019-bert} for learning context-aware word representations. 
Conversely, in the pre-DL era, there exist few (if any) effective methods for utilizing unlabeled data, and statistical models rely on discrete one-hot features, leading to severe data sparseness for many NLP tasks. This directly motivates annotation of word-internal structure, especially for dealing with rare words. 

Annotation of shallow internal structure of Chinese words was first mentioned in \citet{zhao-2009}, largely based on heuristic rules. 
\citet{li-2011,li-zhou-2012} found that many multi-char words could be divided into two subwords, i.e., root and affix. They annotated structures of about 19K words (35\% of 54,214) in CTB6. Their experiments showed that subword-level syntactic parsing is superior to word-level parsing. 
For the three words in Figure \ref{fig:example}, their approach is only applicable to the second word, i.e., ``婚姻/法''.
% % 李中国：2011，把word分为root（又称为subword）和affix。扁平结构。通过字级别短语结构句法分析，来同时处理分词。
% % https://www.aclweb.org/anthology/P11-1141.pdf
% % https://www.aclweb.org/anthology/D12-1132.pdf 
% 做依存句法分析，将词分割为root和affix。标注了CTB6中5万个词，发现35\%，都可能做这种分割。flat，shallow。 
%我们把root和affix都称为subword的话，那就是做subword-level parsing（训练和测试数据都标注好了）。怎么还原为word-level？词内部用MOD，区别于句法。（依存时这么做的，短语呢？没看明白，不管了，写得太乱了，条理不清晰）
As an extension to \citet{li-zhou-2012}, \citet{zhang-etal-2013-char, zhang-etal-2014-char} proposed char-level syntactic parsing by further dividing  subwords into chars. 
As shown in Figure \ref{fig:example}-(a), for each word, they annotated a binary hierarchical tree, using  constituent labels to mark which child constituent is more syntactically important, i.e., left, right, or coordinate. 
In such way, they could convert a word-level constituent/dependency tree into a char-level one.
% 梅山13/14：一个词，按字进行标注层次结构，二叉树，合并时分为三类l/r/c，分别表示左边的部分是更重要的，或右边的，或并列（核心字的位置或方向）。主要是为了方便转成依存形式。不区分构词关系类型。
Similar to \citet{li-zhou-2012}, \citet{cheng2014parsing} annotated internal structure of synthesis (multi-morpheme) words with four relations, i.e., branching,
coordinate, beginning and other parts of a single-morpheme word.

% but focused on 
% the word segmentation task 
% for better dealing with unknown words during word segmentation. 
% LREC-2014：不一定要写。复合词。标注了1万词，然后用模型去分析。不区分head的方向。IJCNLP-2015 short根据标注的内部结构，切为subword，帮助分词。

% 词内部结构对句法的帮助：前DL时代

% 基于subword, BPE处理，也是对词稀疏问题的一个处理手段，因为rare/new word不知道如何表达。

% SUnweiwei的subword的句法、词法分析？

% 汉语很多字。不提象形字（related work中可以讲，为什么有人用stroke image sub-char）。新词很容易产生。

In the DL era, three works have 
%tried annotation of 
studied 
word-internal structure. 
Similarly to our work, \citet{li-etal-aaai-2018-zhaohai} employed dependency trees to encode word-internal structure. 
As shown in Figure \ref{fig:example}-(b), for each multi-char word, they first annotate the part-of-speech (POS) tag of each character, and then determine an unlabeled dependency tree, and finally use a POS tag triple as arc label, corresponding to the POS tags of the modifier/head characters and the whole word.  
%Zhaohai-aaai18：通过标注每个字的词性，同时要标注核心字，构成树，label由当前字-父亲字-整个词的词性组成。做字级别的依存句分析，结果一般。太扯了。
However, we argue POS tag triples 
%are quite ambiguous 
%and thus incapable of capturing word-formation patterns. 
are only loosely related with word-formation patterns, 
not to mention the severe difficulty of annotating 
%Moreover, it requires great annotation effort to % seems very difficult to annotate %to decide 
char-level POS tags in each word. 


Recently, \citet{linqian2020} extended \citet{zhang-etal-2014-char} by using an extra label for marking single-morpheme % (called atom in their paper) 
words, and annotated hierarchical internal structure of 53K words from a Chinese-English machine translation (MT) dataset. 
\citet{annotate-syntactic-Depling-2019} annotated the internal structure of words with 4 dependency relations. 

% 苏劲松2020，和张梅山的很相似，做了一点扩展，区分一个词或subword对应一个single morpheme（called atom in their paper)，标注出来。用a来表示？

In summary, we can see that most previous studies adopted quite shallow hierarchical structure. In contrast, 
%Compared with previous studies, 
this work % for the first time 
presents a more in-depth investigation on internal structure of Chinese words and employs 11 labels to distinguish different syntactic roles in word formation, as shown in Figure \ref{fig:example}-(c).

% 前人工作，我们的工作的特点：依存树、深层次的标签区分。我们相信有了词内部结构，起码帮助：更好的表达词语关系、更好的表示词、更好的做新词理解。


%目的就是解决data sparseness，也就是OOV。

\paragraph{Leveraging character information for better word representation.} 
It has already become a standard way in many NLP tasks to obtain char-aware word representation by applying LSTM or CNN to the character sequence of a word, and concatenate it with word embedding as input, such as named entity recognition \cite{chiu2016named}, dependency parsing \cite{zhang-etal-2020-dep}, and constituent parsing \cite{gaddy2018s}.
% which is shown to be complementary to word embeddings and especially 

Another research direction is to leverage character information to obtain better word embeddings. 
\citet{chen-ijcai2015-joint-char-word-emb} extended the CBOW model and proposed to jointly learn character and word embeddings.
Based on \citet{chen-ijcai2015-joint-char-word-emb}, \citet{xu-emnlp-2017-short} proposed to jointly learn embeddings of words, characters, and sub-characters.\footnote{Following this direction, studies tried to explore more character information for better Chinese word representation, such as strokes \cite{cao-aaai-2018-strokes} and ideographic shape \cite{sun-naacl-2019-visual}.}
However, both studies assume that characters contribute equally to the meaning of a word and directly average embeddings of all characters. 
To address this,  
%also as an extension to 
\citet{xu-naacl-2016-internal-structure} extended \citet{chen-ijcai2015-joint-char-word-emb} and proposed a cross-lingual approach to distinguish contribution of characters for a word.
The idea is to translate Chinese words and characters into English words, and use similarities between corresponding English word embeddings for contribution measurement. 
Instead of treating a word as a bag of characters, we experiment with two simple ways to obtain structure-aware word representations. Meanwhile, enhancing their approach with explicit  word-internal structure could be also very interesting.

\paragraph{Utilizing word-internal structure.} 
Word-internal structure have been explored in various NLP tasks.
Several works propose to learn word-internal structure, word segmentation, POS tagging and parsing jointly \cite{zhang-etal-2013-char,zhang-etal-2014-char,li-etal-aaai-2018-zhaohai}, demonstrating the effectiveness of word-internal structure in helping downstream tasks.
\citet{cheng2015synthetic} attempt to convert words
into fine-grained subwords according to the internal structure of words for better dealing with unknown words during word segmentation. 
\citet{linqian2020} propose to integrate the representation of word-internal structure into the input of neural machine translation model, leading to improved translation performance.










% Due to its critical importance, MWS results have been explored in various NLP applications.  Liu et al.  (2008) propose a ranking based WS approach to produce words of dif-ferent granularities to help IR. Su et al.   (2017) propose a lattice-based RNN encoder for neural MT by representing MWS outputs in word lattices,  leading to improved translation performance.  Due tothe lack of MWS model, they obtain MWS outputs from several SWS models independently trained onheterogeneous SWS datasets.









%for better word embedding, which we leave as future work.
%By explicitly annotating word-internal structure, our work provides a more elegant way to model how meanings of characters are composed into meaning of a word, which is beyond the scope of this paper and left as future work. 
%, which is called internal .  in a cross-lingual way. by making use of char/word translations and via cross-lingual   the CWE model by exploiting the internal semantic similarity between a word and its characters in a cross-lingual manner. 


%Sun et al. (2014) and Li et al. (2015) proposed methods to enhance Chinese character embeddings with radicals based on C&W model. 
%Chen et al. (2015) used Chinese characters to improve Chinese word embeddings and proposed the CWE model to jointly learn Chinese word and character embeddings. 

%【这个不要，是做char embedding学习的】Yanran Li, Wenjie Li, Fei Sun, and Sujian Li. 2015. Component-enhanced chinese character embeddings. In Proceedings of EMNLP, pages 829–834.

% 龚晨加入到参考文献中（直接去anthology去找bib）：

% chen-ijcai2015-joint-char-word-emb
% Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. 2015. Joint learning of character and word embeddings. In Proceedings of IJCAI, pages 1236–1242.

% xu-naacl-2016-internal-structure
% Jian Xu, Jiawei Liu, Liangang Zhang, Zhengyu Li, and Huanhuan Chen. 2016. Improve chinese word embeddings by exploiting internal structure. In Proceedings of NAACL-HLT, pages 1041–1050.

% xu-emnlp-2017-short
% Xu et al. EMNLP-2017 short
% Joint Embeddings of Chinese Words, Characters, and Fine-grained Subcharacter Components
% https://www.aclweb.org/anthology/D17-1027.pdf
% Jinxing Yu Xun Jian Hao Xin Yangqiu Song （香港科技大学）


% cao-aaai-2018-strokes
% cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information
% AAAI-2018，pp 5O53-5061
% •	Shaosheng Cao 
% Ant Financial Services Group; Singapore University of Technology and Design
% •	Wei LuSingapore University of Technology and Design
% •	Jun ZhouAnt Financial Services Group
% •	Xiaolong LiAnt Financial Services Group
% 用笔画信息。

% sun-naacl-2019-visual
% VCWE: Visual Character-Enhanced Word Embeddings
% Chi Sun, Xipeng Qiu, Xuanjing Huang
% NAACL-2019 
% https://arxiv.org/abs/1902.08795
% 字的图片信息



% 词表示：利用char sub-char
% 用内部结构来增强词表示的工作。
% charLSTM charCNN （NER、句法both dependency/constituent、ELMo）

%一种做法就是charLSTM或charCNN，发现确实有用。NER、依存句法分析上，显著、稳定的提高。ELMo也是这么表示的。
% 这种扁平的表示显然不是最好的。可以结合我们的表示的优点。


% 词内部结构的标注
% 词内部结构的利用


% 随着计算机中文文本处理的快速发展,中文词的结构以及词义表示也成为自然语言处理的研究热点之一。
% 很多语料以词为基本单位，如ctb,pku,msr。``This form of annotation does not
% give character-level syntactic structures for words,
% a source of linguistic information that is more fundamental
% and less sparse than atomic words.''

% 由于没有统一的分词标准，Zhao09提出用字依存结构表示来代替分词explore an alternative way to represent word 。(acl14,lrec14)实验结果表明对分词有帮助。
% 他们的词内部依存结构只有弧，不考虑弧上的标签（不考虑字之间的依存关系属于哪种类型）。
% Li11,以语素为基本单位，标注了词内部结构（constituent-style），并提出了一个unified generative model来parse morphological-level word structure and constituent structure
% 类似地，Li12将中文morphological-level word structure  parsing和句法结构解析进行统一建模,表明词内部结构对dependency parsing有帮助。（``Therefore, with the addition of word structures, the overall dependency parsing accuracy naturally can be improved''）
% 这两个工作均把语素作为基本单元，只关注带有前缀或后缀的复合词，并没有标注所有有内部结构的词，标注的有内部结构的词仅包含CTB5的35\%。
% Zhang13 take one setp further by annotating所有有内部结构的词，cover the entire vocabulary of CTB5. 与除了像i11,Li12给出结构以外，Zhang13还标注了了head direction信息,(用'l','r','c'分别表示'left','right','coordination' head directions.)
% ``develop a character-based parsing model that can produce our character-level constituent trees. Our parser jointly performs
% word segmentation, POS tagging and syntactic parsing. show the effectiveness of our annotated word structures in improving parsing accuracies. ''
% ``Zhang et al. (2013)
% have shown the usefulness of word structures in
% Chinese constituent parsing. Their results on the
% Chinese Treebank (CTB) showed that character level
% constituent parsing can bring increased performances''
% Zhang14 将zhang13标注的constituent-style词内部结构数据转化为dependency-style，并提出了两种 transition-based chararacter-level dependency parser，表明了词内部结构对依存句法的有效性。
% 除了像zhang13一样给出了head direction信息之外，苏劲松20还从语义组合的角度对中文词内部结构基本语义单元类型进行了界定，根据词义是否能由其构成单元的语义直接组合而成，将所有词分为了原子子词和组合子词两类。他们在机器翻译上验证了词内部结构的有效性。
% 上述工作均只关注词内部的结构框架或依存结构（the structure），ignore词内部各成分之间的关系类型（不考虑labels）。
% AAAI18意识到label的重要性，标注了每个字的pos tag，然后用字的pos tagging的组合来作为dependency label。(``For the dependency labels,
% SCDT basically follows Collins’ convention, i.e., combinations
% of constituent labels (Collins 2010)'')但是这种label表示不能直观体现出重要的主谓、介宾、并列等关系。举例子

% In recent years, there has been a surge of interest in using the internal structure of words to improve Chinese processing tasks. \citet{zhao-2009} propose to represent word structures with character dependencies and cast Chinese word segmentation as character-level dependency parsing task. Their experiment demonstrates the usefulness of intra-word character dependencies.
% \citet{li-2011} consider morpheme as the basic syntactic element and annotate morphological-level word internal structures in constituent style. They develop a unified generative model to perform word internal structure parsing and constituent parsing simultaneously.
% Similarly, \citet{li-zhou-2012} also use morphological-level word internal structure to improve dependency parsing by modeling word internal structure parsing and dependency parsing in a unified framework. Instead of annotating all the words with internal structures, \citet{li-2011} and Li and \citet{li-zhou-2012} only focus on the compound words that have prefixes and suffixes, which only accounts for 35\% of the total words in CTB5.

% Compared to the morphological-level word internal structure of \citet{li-2011}, \citet{zhang-etal-2013} go a step further by annotating all the words in CTB5 with constituent-style character-level word structure and use ``l (left)'', ``r (right)'', ``c (coordination)'' to indicate the head direction for each word or subword. They propose a joint model for word segmentation, POS tagging and character-level constituent parsing, showing the effectiveness of internal word structures in Chinese constituent parsing. Soon afterwards, based on \citet{zhang-etal-2013}'s annotations, \citet{zhang-etal-2014} propose a character-level dependency parsing model and show that the annotated word structure can bring increased performance for dependency parsing.

% In addition to providing head direction information as \citet{zhang-etal-2013}, 【苏劲松20】 also divide all the words into atom words and composition words by considering whether the words' meaning can be directly obtained by composing the meaning of their constituent units.

% All the above work on word internal structure mainly concerns on identifying the composition architecture or the head direction for words, while neglects the detailed relation types between the constituent units. 
% \citet{DBLP:conf/aaai/LiZJZ18} notice that the relation labels of itra-word character dependencies can also be informative.
% They annotate each constituent character in the word with a character-level POS tag and combine the POS tags of the constituent characters as a relation label for each character-level dependency. Their empirical results show that the character-level dependency labels are helpful in improving parsing performance.
% However, representing dependency labels by combining character-level POS tags may not be intuitive: the typical syntactic structures such as subject-predicate, verb-object, modifier-noun, coordination, which is quite important for analyzing。。。, can not be directly reflected by such dependency labels. 
% In this work, we specifically design 11 relations to intuitively capture the internal dependency syntax for Chinese words, as shown in Table \ref{tbl:summary-relation}.
% 。。。。。。。。






