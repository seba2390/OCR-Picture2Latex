%\subsection{Experiments and results}
\subsection{Experiments}

\paragraph{Settings.} Following \citet{chen-manning-2014fast}, we conduct experiments on CTB5 with the same data split (16,091/803/1,910 sentences) and constituent-to-dependency conversion. 
%(CTB5) with dependencies obtained by the Penn2Malt tool \footnote{http://stp.lingfil.uu.se/∼nivre/research/Penn2Malt.html}. 
%Most of the hyper-parameter settings is adopted from \citet{dozat2016deep}, expect settings of CharLSTM.
Both char/label embeddings are randomly initialized and have the same dimension of 50. 
For the parsers using gold-standard POS tags, we randomly initialized the POS tagging embeddings and set the dimension to 50.
For other hyperparameters, we adopt the default configuration of SuPar, including the pre-trained word embeddings. 
%We adopt the de
%and hidden layer of CharSLTM are set to 50. 
% We also obtain pre-trained word embeddings by training word2vec on Chinese Gigaword V3.
%Third Edition 【todo 是否用缩写？】

For multi-char words without annotated internal structure, we use the automatic outputs from the trained parser with BERT in Section \ref{sec:WIS-parsing}, so that every word corresponds to a single structure.  
%实验部分要给出所有词的词内部结构怎么来的。预测。
%所有词的内部结构都是给定的，每个词给一个。

We use word-wise UAS/LAS/CM for evaluation, and punctuation is excluded in all metrics.
% We adopt the same data split as \citet{zhang-clark-2008} on CTB5 and the gold standard data split on CoNLL09.
% Then, we obtain dependencies from CTB5 by the Penn2Malt tool \footnote{http://stp.lingfil.uu.se/∼nivre/research/Penn2Malt.html} and find corresponding constituent trees from Chinese Penn Treebank 6.0 (CTB6) according to CoNLL09 data.
% For dependency parsing, we use unlabeled and labeled attachment score (UAS/LAS) as main metrics. 
% For constituent parsing, we use the standard constituent-level labeled precision, recall and F-score (P/R/F) as the evaluation metric.

\paragraph{Main results.}

Table \ref{table:results-dependency} shows the parsing performance. We can see that both LabelCharLSTM and LabelGCN substantially outperform the basic CharLSTM method. 
LabelGCN achieves the best performance on UAS and LAS, with a gain of 0.71 and 0.80 respectively. 

The fourth row reports performance of LabelGCN without using label embedding, leading to consistent accuracy drop, demonstrating the usefulness of rich labels, which is a key contribution of this work, despite the extra annotation effort. 
%Without encoding word-internal skeleton tree, the BiLSTM approach already achieves improvement of 0.37/0.47 in UAS/LAS.
%Integrating the arc information by using GCN can further improve parsing performance by 0.56/0.63 in UAS/LAS.

% \begin{table}[tb]
% \setlength{\tabcolsep}{6.5pt}
% \centering
% \begin{tabular}{lccccc}
%     \toprule
%     & \multicolumn{2}{c}{Dev} & \multicolumn{3}{c}{Test} \\
%     & UAS & LAS & UAS & LAS & CM \\[2pt]
%     \hline
%     \\[-8pt]
%     Baseline &87.03 &85.05 &87.31 &85.23 &30.73 \\
%     BiLSTM   &87.03 &\textbf{85.22} &87.68 &85.70 &31.68 \\
%     GCN      &\textbf{87.13} &85.19 &\textbf{87.87}  &\textbf{85.86} & \textbf{31.78} \\
%     GCN w/o label &86.87 &84.76 &87.55 &85.41 &31.36\\
%     GCN 波浪 &87.06 &85.05 &87.56 &85.47& 31.15  \\

%     \bottomrule
% \end{tabular}
%     \caption{Main results for dependency parsing.}
%     \label{table:results-dependency}
% \end{table}

% \begin{table}[tb]
% \setlength{\tabcolsep}{6.5pt}
% \centering
% \begin{tabular}{lccccc}
%     \toprule
% %    &  \multicolumn{3}{c}{Test} \\
%     &  UAS & LAS & CM \\
%     \hline
%     Basic CharLSTM &87.31 &85.23 &30.73 \\
%     LabelCharLSTM    &87.68 &85.70 &31.68 \\
%     \hline
%     LabelGCN       &\textbf{87.87}  &\textbf{85.86} & \textbf{31.78} \\
%     %\hline
%     $~~~~~$ w/o label  &87.55 &85.41 &31.36\\
% %    $~~~~~$ GCN 波浪  &87.56 &85.47& 31.15  \\
%     \bottomrule
% \end{tabular}
%     \caption{Parsing performance on CTB5-test.}
%     \label{table:results-dependency}
% \end{table}

\begin{table}[tb]
\setlength{\tabcolsep}{6.5pt}
\centering
\begin{tabular}{lccccc}
    \toprule
%    &  \multicolumn{3}{c}{Test} \\
    &  UAS & LAS & CM \\
    \hline
    Basic CharLSTM &88.31 &85.96 &32.04 \\
    LabelCharLSTM    &88.78 &86.51 &\textbf{33.19} \\
    \hline
    LabelGCN       &\textbf{89.02}  &\textbf{86.76} & 32.93 \\
    %\hline
    $~~~~~$ w/o label  &88.66 &86.28 &32.20\\
%    $~~~~~$ GCN 波浪  &87.56 &85.47& 31.15  \\
    \bottomrule
\end{tabular}
    \caption{Parsing performance on CTB5-test.}
    \label{table:results-dependency}
\end{table}

%\input{content/encode-iwdp-analysis}

% \begin{table}[tb]
% \setlength{\tabcolsep}{2pt}
% \centering
% \begin{tabular}{lcccccc}
%     \toprule
%     % \hline
%     & \multicolumn{3}{c}{Dev} & \multicolumn{3}{c}{Test} \\
%     & P & R & F & P & R & F \\[2pt]
%     \hline
%     \\[-8pt]
%     \multicolumn{7}{c}{CTB5} \\
%     Baseline &87.81 &86.36 &87.08 &87.42 &87.13 &87.27 \\
%     BiLSTM   &\textbf{88.12} &\textbf{86.82} &\textbf{87.46} &\textbf{87.80} &\textbf{87.45} &\textbf{87.62} \\
%     GCN      &87.73 &86.56 &87.14 &87.60 &87.33 &87.47\\
%     \hline
%     \\[-8pt]
%     \multicolumn{7}{c}{CoNLL09} \\
%     Baseline &88.35 &87.35 &87.84 &88.66 &87.58 &88.12 \\
%     BiLSTM &88.69 &87.76 &88.22 &88.85 &87.84 &88.34 \\
%     GCN & \\[2pt]

%     \bottomrule
% \end{tabular}
%     \caption{Main results for constituency parsing.}
%     \label{table:results-constituency}
% \end{table}

