\subsection{Representation with BiLSTM}
% \textbf{Encoding with sequence model.}
Apart from the novel graph neural network, we also utilize CharLSTM, without changing the major network structure of baseline model, to integrate structure-aware word representations.
% 除了使用新颖的图神经网络之外，我们还给出另外一个十分直接的引入字结构表示信息的方法，which 我们不需要引入别的神经网络结构。
% 师姐我说我们用charLSTM编码label的初衷是，
% In syntactic parsing task, one of the most important role of the CharLSTM is to explore the part-of-speech clues of a word from characters it contained.
% For Chinese language, 【可以说，中心词以及非中心字的语法角色隐含了词性信息吗? 比如``枣红''的中心字是``红''，而``红枣''的中心字是``枣''】.
As depicted in Table \ref{table:summary-different-num-word}, about $92.1\%$ words in the dataset are one-character words or two-character words.
Their word-internal skeleton trees are simple, but the word-internal dependency labels still can provide clues. We encode such dependency label information by employing a BiLSTM.
% by pointing out center-character and illustrating the function of non-center-characters.
% So, we omit the arcs of the word-internal structures.
% , and feed each character together with its relation labels into the CharLSTM.
% That is, for now, the input of CharLSTM in Equation \ref{baseline-input} is $w_i$ and $l_{u,v}$, the label of dependency $u \leftarrow v$. 【这里的的公式还有些奇怪，得再改一下】
% 在依存解析中，charLSTM的一个很重要的任务是去从组成词的字中发掘POS信息。我们发现基于这个出发点 词内部的每个字的label要比字符间的依存弧更为重要，因为它可以指出哪个字为词语的中心字并标明不同字在词语中所承担的角色。因此我们为了使得模型更加简单而忽略arc。
Formally, we concatenate the character embedding with the dependency label embedding, and then go through a single-layer BiLSTM:
\begin{equation}
    % [\overrightarrow{\mathbf{h}}_{u}, \overleftarrow{\mathbf{h}}_{u} ] =\textup{BiLSTM}(\mathbf{emb}^{c}(c_u) \oplus \mathbf{emb}^{l}(l_{u,v}))
    {\mathbf{h}}_{u} =\textup{BiLSTM}(\mathbf{emb}^{c}(c_u) \oplus \mathbf{emb}^{l}(l_{u,v}))
\end{equation}
Here, $\mathbf{emb}^{l}(l_{u,v})$ is the label embedding for dependency $u \leftarrow v$.
The pooling operation is performed on the top-layer ouputs of BiLSTM to obtain $\mathbf{r}_i^{syn}$, which is the same as Equation \ref{pooling}. 
% The final syntactic structure-aware representation $\mathbf{r}_i^{syn}$ 
% The final representation $\mathbf{r}_i^{syn}$ is the concatenation of $\overrightarrow{\mathbf{h}}_{n}$, the forward hidden representation of the last word of the sentence, and $\overleftarrow{\mathbf{h}}_{0}$, the backward representation of the first word.
% The final representation $\mathbf{r}_i^{syn}$ is the top-layer BiLSTM outputs.





% In addition to directly encoding the syntactic word-internal tree structure with the graph model GCN, we also employ the sequence model BiLSTM to obtain the structure-aware word representation.
