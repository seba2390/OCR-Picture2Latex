% 实验的抓紧做，把主要结果做出来。

% 1）只出现在dev/test中的词，用预测的内部结构，结果如何？2）字级别句法分析的结果如何？比规则的波浪形的结果如何？

% 句法用CRF-loss（一阶）的结果也应该跑一下，我们起码自己要知道结果。

% 句法用BERT时，词内部结构是否还有用？可以看看，自己看看结果。

% 字内部时用了BERT，现在不用的话，也不好。

\section{Experiments}
\textbf{Data.}
For the task of char-level syntactic parsing for words, we use our manually annotated char-level syntactic treebank, which consists of xxx, xxx, and xxxx words in training, development, and test sets respectively. For sentence-level syntactic parsing, we conduct experiments and analysis on the Chinese Penn Treebank 5.0 (CTB5) with dependencies obtained by the Penn2Malt tool \footnote{http://stp.lingfil.uu.se/∼nivre/research/Penn2Malt.html} and the Chinese data at the
CoNLL09 shared task \citet{conll09}. We adopt the same data split as \citet{zhang-clark-2008} on CTB5 and the gold standard data split on CoNLL09. 

For pretrained char embeddings and word embeddings, we adopt the 100 dimension embeddings of \citet{liying-2019} pretrained with word2vec model on Chinese Gigaword.

\textbf{Evaluation metrics.} 
We use the standard unlabeled and labeled attachment score (UAS/LAS) as the main metrics for both char-level syntactic parsing and sentence-level syntactic parsing.

\textbf{Model settings.} 
We adopt most of the parameter settings in Biaffine parser \cite{dozat2016deep}. Each model is
trained for at most 1000 iterations.
We stop training when the peak performance
on the development data does not increase in 100 consecutive iterations. 
We set the dimensions of char embedding, word
embedding, and CharLSTM outputs to 50, 100, 100, respectively. 

\subsection{Results: Word-internal Syntactic Parsing}
\setlength{\tabcolsep}{5pt}
\begin{table}[tb]
%\setlength{\leftskip}{-25pt}
\begin{center}
\begin{small}
\begin{tabular}{c | *{2}{c}  | *{2}{c} }
\hline
\multirow{2}{*}{} 
& \multicolumn{2}{|c}{dev }
& \multicolumn{2}{|c}{test} \\
\cmidrule(lr){1-2} \cmidrule(lr){4-5}
% \cline{2-3} \cline{4-5}
& UAS & LAS & UAS & LAS \\
\hline
Random & 78.85 & 73.93  & 79.17 & 74.21 \\
%\hline
Pretrained & 80.07 & 75.45 & 79.74 & 75.00 \\
%\hline
BERT & 86.42 & 83.39 & 86.48 & 83.35 \\
\hline
\end{tabular}
\end{small}
\caption{Results of word-internal syntactic parsing.}
\label{iwdp-result}
\end{center}
\end{table}

Table \ref{iwdp-result} presents the results of word-internal parsing. 




1)随机<pretrain<bert
2)用词性比不用词性的结果好
% We observe that the pretrained embedding 


% leads to consistent and large improvement on both domains. Compared with
% “ELMo (Giga)”, LAS increases b

% The comparison between pretrained embeddings and randomly initialized embeddings
% shows that both pretrained embeddings and lexicon are useful to lattice LSTM, and lexicon contributes more than the
% pretrained embeddings

黄：adv和的模型预测准确率较低的原因，可能是因为adv标签容易和coo标签混淆，如果某个词是由几个动词性字组成的话，那么模型可能会比较难判断出来这两个动词性字之间是修饰关系还是并列关系。
通过观察预测结果发现，adv确实容易和coo混淆，还有和att和混淆。



\begin{figure}
		\centering
		\includegraphics[scale=0.3]{figure/model-acc.png}
		\caption{xxxx【图要重画】}
		\label{fig:model-acc}
\end{figure}

