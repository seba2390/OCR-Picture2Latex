\begin{table}[!tb]
\setlength{\tabcolsep}{4.2pt}
\centering
\begin{tabular}{lrrrr}
    \toprule
    % \hline
   & all & $>2$ &$\le2$ & unk.  \\
    \hline
    % \\[-8pt]
    Basic CharLSTM &85.96& 86.42 &82.03 &81.73   \\[2pt]
    \multirow{2}*{LabelGCN} 
    &86.76& 87.10 &83.79 &84.30  \\
    & +0.80 &+0.68 & +1.76 &+2.57  \\
    \bottomrule
\end{tabular}
    \caption{Parsing LAS regarding to word frequency.}% of common/rare/unkown words.} % frequency}. % on CTB5-test.}
    \label{table:rare-word-las}
\end{table}

\paragraph{Analysis on rare words.} 
To gain more insights on how word-internal structure helps word representation learning, we divide the words in CTB5-test into several groups according to their frequency in CTB5-train, and report fine-grained accuracy in Table \ref{table:rare-word-las}. 
%To investigate whether our proposed word-internal structure can be beneficial to the parsing performance on rare words, we divide the words on test data into three categories according to their frequency on training data: words with frequency $\leq 2$, words with frequency $\leq 1$ and words out of the training data (OOV). Table \ref{table:rare-word-las} compares the parsing performance (LAS) of the baseline model and our model which encodes word-internal structure with GCN on the three categories. We also give the performance on the whole test data (overall) for reference. We observe that the improvements on the rare words and OOV words are consistently larger than that on the whole test data, which means the word-internal structures make more contribution to rare/OOV words than high-frequency words. The main reason is that the word-internal structures contain rich information on how multiple characters form a word, which is helpful for representing rare words via composing the meanings of characters.
We can see that the overall performance gain is mostly contributed by improvement over rare words with low frequency or totally unknown. 
This verifies that word-internal structures can help the model to better represent rare words. 


\begin{table}[t]
\setlength{\tabcolsep}{2.5pt}
\centering
\begin{tabular}{lrrrrr}
    \toprule
%    &  \multicolumn{3}{c}{Test} \\
    &  UAS & LAS \\
    \hline
    % \citet{chen-manning-2014fast} & 88.1 &85.7\\
%   \citet{ballesteros-2016-training}& 87.65&86.21&\\
%   \citet{kiperwasser-2016-simple} &87.6 &86.1&\\
%   \citet{acl16-Zhang-parsing} & 87.65 & 86.17 &\\
%   \citet{kuncoro-2016-distilling} &88.87 &87.30 \\
   \citet{ma-2017-neural} &89.05 &87.74 \\
   \citet{dozat2016deep} &89.30 &88.23 \\
%   Zheng (2017) & 89.42 &87.94\\
%   \citet{coling2018-seq2seqparsing} &88.78 &86.23\\
   \citet{acl18-ma-stackpointer} &\textbf{90.59}&\textbf{89.29}\\
    \hline
    Basic CharLSTM &91.11 &89.91  \\
    LabelCharLSTM    &\textbf{91.31} &90.15 \\
    LabelGCN       &\textbf{91.31}  &\textbf{90.16} \\
    %\hline
    % $~~~~~$ w/o label  &87.55 &85.41 &31.36\\
%    $~~~~~$ GCN 波浪  &87.56 &85.47& 31.15  \\
    \bottomrule
\end{tabular}
    \caption{Parsing performance with gold-standard POS tags on CTB5-test.}
    \label{table:results-dependency-goldpos}
\end{table}


\paragraph{Results with gold-standard POS tags.} As suggested by a reviewer, we train our parser with gold-standard POS tags by concatenating the original input (i.e., $\mathbf{x}_{i}$ in Equation \ref{eq:charlstm-eq}) with gold-standard POS tag embeddings, in order to compare with previous works. 
Table \ref{table:results-dependency-goldpos} shows the results. 
Compared with the Basic CharLSTM results in Table \ref{table:results-dependency}, using gold-standard POS tags as extra features for the Basic CharLSTM leads to substantial improvements by 2.80 and 3.95 in UAS and LAS respectively, and outperforms the previous works as presented in Table \ref{table:results-dependency-goldpos}, showing that the basic CharLSTM can be served as a strong baseline model.


% the Basic CharLSTM, LabelCharLSTM, and LabelGCN in Table \ref{table:results-dependency-goldpos} augment the input by concatenating the original input (i.e., $\mathbf{z}_{k}$) with the embeddings of gold-standard POS tags.

% When using gold-standard POS tags as extra features, the basic CharLSTM outperforms the previous works, showing that the basic CharLSTM can be served as a strong baseline model.

Compared with the Basic CharLSTM, utilizing word-internal structure with LabelCharLSTM or LabelGCN achieves consistently better performance by 0.24 and 0.25 respectively in LAS in the scenario of using gold-standard POS tags.
Besides the strong baseline, another reason that the improvement brings by the internal-word structure is slight when using gold-standard POS tags is that a part of linguistic information in the POS tags and the word-internal structures may be overlapping.

% less than that in the scenario of without using gold-standard POS tags


% Although the improvement is less than that in the scenario of without using gold-standard POS tags due to a part of overlapping information in POS tags and internal-word structures, we believe that the POS tags and internal-word structures can also provide complementary contributions to the parsing performance according to the slightly but consistently improvement. 
% We will explore




