
\section{Conclusions}

This paper presents a thorough study on internal structures of Chinese words. 
First, %based on newly compiled 30-page guidelines, 
we annotate a high-quality word-internal structure treebank covering over 30K words in CTB5, named as WIST. 
Second, we perform analysis on WIST from different perspectives and draw many interesting findings on Chinese word-formation patterns. 
Third, we propose word-internal structure as a new task, and present benchmark results using a popular dependency parser. 
Finally, we 
conduct preliminary experiments with two simple methods, i.e., LabelCharLSTM and LabelGCN, to encode word-internal structure as extra word representation, and find promising performance gains on the sentence-level dependency parsing task. 
%Results 
Analysis shows that the rich dependency labels adopted in WIST play a key role, and word-internal structure is most beneficial for rare word representation. 
%that further improve the competitive biaffine parser, 
%especially on rare words. 


% Future work, 句子上下文在词内部结构分析模型中的使用。

% 利用词内部结构做更好的词表示，Word2Vec, ELMo, BERT. 结合字义解释、词义解释。


% Future work, 利用字义、词义等，结合词内部结构，得到解释型更强的词表示

% 虽然目前的结果并不高，但是作为一个基础性工作，还可以做很多事情。

% 标注词内部结构后，把字典中字义、和词义也标注出来。

% 扩大WIST的规模和质量，到10万词级别。

% 更好的词内部分析建模，利用词义、字义、上下文

% 词内部结构的利用，帮助word embedding或者context-aware word repre的学习

% 论文中的\%用的不统一，要注意下。