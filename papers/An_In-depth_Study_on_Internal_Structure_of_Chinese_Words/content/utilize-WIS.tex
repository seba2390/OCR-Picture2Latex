% \section{Sentence-level syntactic Parsing with Structure-aware Word Representation}

\section{Utilizing Word-internal Structure}

This section presents a preliminary study on utilizing word-internal structure, aiming to address the third question: is modeling word-internal structures beneficial for word representation learning? 
% We conduct our study on sentence-level dependency parsing to make investigation on whether integrating structure-aware word representation can be beneficial. 
% 【用解释为啥在句法分析上做实验吗？】

We use sentence-level dependency parsing as the focusing task \cite{KublerDEP09}, mainly considering resemblance in tree structure representation and close relatedness between the two tasks. 
% We also 
Given an input sentence $w_0w_1...w_m$, 
%where $w_i$ is  the $i$-th word and $w_0$ is a pseudo word for sentence start, sentence-level dependency 
the goal of dependency parsing is to find an optimal dependency tree for the sentence. 
%the head word $w_j$ for each word $w_i$ and the dependency relation label between $w_i$ and $w_j$.
%Similar to the word-internal structure parsing as described in Section \ref{char-pasrsing}, w
Again, we adopt SuPar \cite{zhang-etal-2020-dep} for implementation of Biaffine parser \cite{dozat2016deep} % and the implementation of  
as our basic parser. 
%for sentence-level dependency parsing. 


\subsection{Methods}

The basic parser applys a BiLSTM over character sequence to obtain word representation. 
In this part, we propose two  simple alternative methods to encode internal structure shown in Figure \ref{fig:example}-(c).

\textbf{Basic CharLSTM method.} For each word, the basic Biaffine parser uses the concatenation of word embeddings and CharLSTM outputs to represent each word in the input layer:
\begin{equation}\label{eq:charlstm-eq}
\begin{split}
& \mathbf{x}_i = \mathbf{emb}(w_i) 
 \oplus \textup{CharLSTM}(w_i) \\
& \textup{CharLSTM}(w_i) \leftarrow \textup{BiLSTM}(...,\mathbf{z}_{k},...) \\
& \mathbf{z}_{k} = \mathbf{emb}(c_{i,k}) 
\end{split}
\end{equation}
where $c_{i,k}$ is the k-th character of $w_i$. The final word representation from $\textup{CharLSTM}(w_i)$ 
is obtained by concatenating two last-timestamp hidden output vectors of a one-layer BiLSTM. 
%The other parts of the model architecture for the sentence-level dependency parsing remain consistent with the word-internal structure parsing as described in section \ref{char-pasrsing}.

% To exploit the syntactic word-internal information for sentence-level parsing, we investigate and compare two methods to encode the syntactic word-internal structures.
% Each method produces a representation considering the word-internal structure of the concerned word $w_i$, which is denoted as $\mathbf{r}_i^{syn}$.
% We treat the syntactic structure-aware representation $\mathbf{r}_i^{syn}$ of word $w_i$ as extra features and concatenate it with the original input $\mathbf{x}_i^w$.
% We introduce the two methods in detail in the following subsections.


\textbf{LabelCharLSTM Method.} 
% 谁的方法说明，用label embedding就很有效果了。
% \textbf{Encoding with sequence model.}
%Previous work on syntax-aware semantic role labeling shows it is simple and effective to %of using syntactic trees is to 
%feed syntactic label embeddings as extra inputs into encoders \cite{XXXX}. 
Considering that the word is usually very short and a bare label itself provides rich syntax information,  
we propose a straightforward extension to CharLSTM, named as LabelCharLSTM, via minor modification.%  to Equation \ref{eq:charlstm-eq}.
\begin{equation}\label{eq:labelcharlstm-eq}
\begin{split}
& \mathbf{z}_{k} = \mathbf{emb}(c_{i,k}) \oplus \mathbf{emb}(l_{i,k})
\end{split}
\end{equation}
where $l_{i,k}$ represents the label between $c_{i,k}$ and its head in the word-internal structure. 
%concatenating char with label embedding 
% Apart from the novel graph neural network, we also utilize CharLSTM, without changing the major network structure of baseline model, to integrate structure-aware word representations.
% 除了使用新颖的图神经网络之外，我们还给出另外一个十分直接的引入字结构表示信息的方法，which 我们不需要引入别的神经网络结构。
% 师姐我说我们用charLSTM编码label的初衷是，
% In syntactic parsing task, one of the most important role of the CharLSTM is to explore the part-of-speech clues of a word from characters it contained.
% For Chinese language, 【可以说，中心词以及非中心字的语法角色隐含了词性信息吗? 比如``枣红''的中心字是``红''，而``红枣''的中心字是``枣''】.
%As depicted in Table \ref{table:summary-different-num-word}, about $92.1\%$ words in the dataset are one-character words or two-character words.
%Their word-internal skeleton trees are simple, but the word-internal dependency labels still can provide clues. We encode such dependency label information by employing a BiLSTM.
% by pointing out center-character and illustrating the function of non-center-characters.
% So, we omit the arcs of the word-internal structures.
% , and feed each character together with its relation labels into the CharLSTM.
% That is, for now, the input of CharLSTM in Equation \ref{baseline-input} is $w_i$ and $l_{u,v}$, the label of dependency $u \leftarrow v$. 【这里的的公式还有些奇怪，得再改一下】
% 在依存解析中，charLSTM的一个很重要的任务是去从组成词的字中发掘POS信息。我们发现基于这个出发点 词内部的每个字的label要比字符间的依存弧更为重要，因为它可以指出哪个字为词语的中心字并标明不同字在词语中所承担的角色。因此我们为了使得模型更加简单而忽略arc。
%Formally, we concatenate the character embedding with the dependency label embedding, and then go through a single-layer BiLSTM:
% \begin{equation}
%     % [\overrightarrow{\mathbf{h}}_{u}, \overleftarrow{\mathbf{h}}_{u} ] =\textup{BiLSTM}(\mathbf{emb}^{c}(c_u) \oplus \mathbf{emb}^{l}(l_{u,v}))
%     {\mathbf{h}}_{u} =\textup{BiLSTM}(\mathbf{emb}^{c}(c_u) \oplus \mathbf{emb}^{l}(l_{u,v}))
% \end{equation}
% Here, $\mathbf{emb}^{l}(l_{u,v})$ is the label embedding for dependency $u \leftarrow v$.
% The pooling operation is performed on the top-layer ouputs of BiLSTM to obtain $\mathbf{r}_i^{syn}$, which is the same as Equation \ref{pooling}. 



% \subsection{Representation with GCN}
% We employ two different models to encode syntactic word-internal structures explicitly.
\textbf{LabelGCN method.} 
%缩写应该在intro提到才对，到时候确认
% \textbf{Encoding with graph model.} 
Previous work show that GCN is very effective in encoding
%The graph convolutional network (GCN) \cite{kipf2016semi} is designed for modeling graph structures, and can be naturally used for exploiting
syntactic trees \cite{marcheggiani-titov-2017,zhang-etal-2018}. 
%In this work, we employ GCN to encode the word-internal structure of Chinese words. 
We follow the implementation of \citet{zhang-etal-2018} and use a two-layer GCN as a more sophisticated way. % to encode word-internal structure. 
In order to utilize labels, we extend vanilla GCN to have the same input with 
% Formally, given a internal structure of a word with $t$ nodes (each node is a character), we use an $t \times t$ adjacency matrix $\mathbf{{A}}$ to represent such structure, where $A_{uv}=1$ if there is a dependency arc between the $u$-th node $c_u$ and the $v$-th node $c_v$. Following \citet{marcheggiani-titov-2017}, we also set the diagonal elements of $\mathbf{{A}}$ to $1$ to add a self-loop for each node.
% The input of GCN is the same with
LabelCharLSTM, i.e., $\mathbf{z}_{k}$.
We obtain the final word representation by performing average pooling over the output vectors of the top-layer GCN.  
% The representation vector $\mathbf{h}_u^m$ of the node $c_u$ at the $m$-th layer of GCN is computed based on its one-hop neighbours and itself, which is defined as:
% \begin{equation}\label{GCN}
%     \begin{split}
%     \mathbf{h}_{u}^{m}&=\rho\Bigg( \sum_{v=1}^{n}{\mathbf{{A}}_{uv}}\mathbf{W}^{m}\mathbf{h}_v^{m-1} + \mathbf{b}^{m}\Bigg)
%     \end{split}
% \end{equation}
% where $\mathbf{W}^{m}$ and $\mathbf{b}^{m}$ are model parameters. $\rho$ denotes an activation function. 
% $\mathbf{h}_u^0$ is the initial input representation to GCN, which is the concatenation of the character embedding and the dependency label embedding.
% \begin{equation}
%     \mathbf{h}_{u}^{0}=\textup{BiLSTM}(\mathbf{emb}^{c}(c_u) \oplus \mathbf{emb}^{l}(l_{u,v}))
% \end{equation}
% Here, $\mathbf{emb}^{l}(l_{u,v})$ is the label embedding for dependency $u \leftarrow v$.
% 【h0不用BiLSTM了，后面改】
% 【双向还没写，看实验结果】
% The final syntactic structure-aware representation $\mathbf{r}_i^{syn}$ for the word $w_i$ is obtained by performing an average pooling operation on the top-layer outputs of the GCN which contains $M$ layers in total.
% % Finally, an average pooling operation is performed on the top-layer GCN outputs to obtain the syntactic structure-aware representation $\mathbf{r}_i^{syn}$ for the word $w_i$.
% \begin{equation}
% \mathbf{r}^{syn}_{i} =
% \textup{Pooling}(\{\mathbf{h}_u^M|1 \leq u \leq t\})
% \label{pooling}
% \end{equation}

%\input{content/extended-charlstm}
%\input{content/encode-iwdp-exp}

