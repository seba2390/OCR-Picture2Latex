\section{Introduction}\label{sec:intro}

% 赛豪得试一下：pre-trained char embedding是否有用？

% 分析部分：左弧、右弧的比例
% 单字、两个字、三个字、四个字及以上，在dict中的比例分布；在CTB语料中的比例分布

% 两个字的单词，左弧右弧的比例
% 三个字的单词，左弧右弧的比例
% 四个字及以上的单词，左弧右弧的比例

% 三字及以上，波浪形的比例（厚全已经分析了）

% 统一一下：
% multi-morpheme word; 也有说multi-morphemic  word，但是没有说 single-morphemic word）
% single-morpheme word
% single morpheme. 

% 建模问题

Unlike English, 
% in which characters represent sound directly, 
Chinese adopts a logographic writing system and contains tens of thousands of distinct characters. 
Many characters, especially frequently used ones, have rich and specific meanings. % and have their clear syntactic roles/function.  
% There are about 10K characters 

However, words, instead of characters, are often considered as the basic unit in processing Chinese texts. We believe the reason may be two-fold. First, usually a character may have many meanings and usages. Word formation process greatly reduces such char-level ambiguity. Second, by definition, words are the minimal units that express a complete semantic concept or play a grammatical role independently \cite{ctb-xiafei,yu2003ppd}.\footnote{There 
is still a dispute on the word granularity issue \cite{gong-2017-multi,naacl21-latticebert}. Words are defined as a character sequence that is in tight and steady combination. However, the combination intensity is usually yet vaguely qualified according to co-occurrence frequency. We believe this work may also be potentially useful to %contribute to 
this direction.
%study on word granularity. 
% 可能related work部分没有地方写了
%as discussed in Section \ref{sec:rel-work}.
}
\input{figure/tree}
Roles played by characters in word formation can be divided into three types. 
\textbf{\footnotesize{(1)}} There is a stable and important set of 
%about %【xxxx不要忘了】 
\emph{single-char words}, such as ``你'' (you)'', ``的'' (of), and most punctuation marks. 
\textbf{\footnotesize{(2)}} A character having no specific meaning acts as a \emph{part of a single-morpheme word}, such as ``仿佛'' (like) and %``沙(sh\=a)发(f\=a)'' (sofa, 
``法(f\v{a})老(l\v{a}o)'' (Pharaoh, 
transliteration of foreign words). 
% play phonetic roles for transliteration of foreign words, such as ``沙发'' (sha fa音调也放上？) for sofa. 
\textbf{\footnotesize{(3)}} A character corresponds to a \emph{morpheme}, the smallest meaningful unit in a language, and composes a polysyllabic word with other characters. This work  targets multi-char words, and  is particularly interested in the third type which most characters belong to. 

%Broadly speaking, understanding how multiple characters form a word, i.e., the word-formation process, is crucial for learning Chinese grammar. 
Intuitively, modeling how multiple characters form a word, i.e., the word-formation process, allows us to  more effectively represent the meaning of a word via composing the meanings of characters. 
This is especially helpful for handling rare words,  considering that the vocabulary size of characters is much  smaller than that of words.
%data sparseness is the key challenge for NLP models. % (long tail)
%can provide useful direct  directly help beneficial for both understanding (thus representing) common words and handling rare (new) words.  
% Specifically, it helps 
%people to better understand known words, and better handle 
% This motivates 
In fact, many NLP researchers have tried to utilize char-level word-internal structures for better Chinese understanding. 
Most related to ours, previous studies on syntactic parsing have proposed to annotate word-internal structures to alleviate the data sparseness problem \cite{zhang-etal-2014-char,li-etal-aaai-2018-zhaohai}. However, their annotations mainly consider flat and shallow word-internal structure, as shown in Figure \ref{fig:example}-(a) and (b). 
Meanwhile, researchers try to make use of character information to learn better word embeddings \cite{chen-ijcai2015-joint-char-word-emb,xu-naacl-2016-internal-structure}. 
Without explicitly capturing word-internal structures, these studies have to treat a word as a bag of characters.
%focus on modeling semantic contribution of individual chars for a composed word. 
%2) for to obtain better Chinese word embeddings. 不考虑结构信息，只是扁平的、compose char的
% Word-formation process has drawn tremendous interest from  linguistics, and NLP fields. % In a broad sense, understanding how multiple characters form a word is the basic for learning the Chinese grammar (CITE?).  
% 少一句过渡，汉语的呢？
% Intuitively, understanding how multiple characters form a word is very useful for representing the meaning of 
See Section \ref{sec:related-work} for more discussion. 

This paper presents an in-depth study on char-level internal structure of Chinese  words. We endeavour to address three questions. \textbf{\footnotesize{(1)}} What are 
the word-formation patterns for Chinese words? \textbf{\footnotesize{(2)}} Can we train a model to predict deep word-internal structures? 
\textbf{\footnotesize{(3)}} Is modeling word-internal structures beneficial for word representation learning? 
%Specifically, this work can be decomposed into four parts. 

For the first question, we propose to use labeled dependency trees to represent word-internal structures, and employ 11 labels to distinguish syntactic roles in word formation. We compile annotation guidelines following the famous textbook of \citet{grammar-notes-zhu-1982} on Chinese syntax, and annotate a high-quality word-internal structure treebank (WIST), consisting of 30K words from Penn Chinese Treebank (CTB) \cite{ctb-xiafei}. We conduct detailed analysis on WIST to gain insights on Chinese word-formation patterns.
%via  detailed analysis. 

For the second question, we propose word-internal structure parsing as a new task, and present benchmark experimental results using a competitive open-source dependency parser. 
%应该讲一下为什么研究构词法只需要考虑句法？为什么用依存树？

For the third question, we investigate two simple ways to encode word-internal structure, i.e., LabelCharLSTM and LabelGCN, and show that using the resulting word representation leads to promising gains on the dependency parsing task.
%extra input can substantially boost the dependency parsing performance.
%, both dependency and constituent.
%for two state-of-the-art syntactic parsers. Experiments show that 

We release WIST at \url{https://github.com/SUDA-LA/ACL2021-wist}, and also provide a demo to parse the internal structure of any input word. 

% 汉语构词法的相关研究（看一些文献）。为什么要用句法来刻画构词法呢？汉语构词法与句法关联紧密。汉语词内部和句子的构成是非常相似的。

% TODO: 在unknown/rare word上的性能如何？分析一下。要佐证我们intro中的论点。

% 例子：想方设法 婚姻法 法老（Pharaoh） 
 %沙发？ 劳动法
 
% 汉语的形态变化很少，例如suffix prefix，不通过形态变化来体现额外的时态、单复数信息。当然有一些（如们、者、性等后缀词素）

% rare/new word的表示，可以通过charLSTM等获取上下文无关表示，或者ELMo/BERT通过上下文信息来弥补得到上下文相关表示。但是，内部结构是否还可以继续提高呢？

% 词是最小的能够独立活动的有意义的语言成分。
% 根据中华人民共和国国家标准颁布的信息处理用现代汉语分词规范：使用稳定、结合紧密的汉字串被看作是词。和句法结构类似，汉语词也通常有syntactic structure，typical的结构包括定中、并列、主谓、述宾等。然后举个例子：一个词由xxx字组成，这些字之间是什么关系（可以考虑给出示意图）。(MeishanZhang13)
% 然而，很多语料都以词为最小单位.如CTB,pku,msr..``This form of annotation does not
% give character-level syntactic structures for words, a source of linguistic information that is more fundamental and less sparse than atomic words.'' (Zhang13)
% 很多中文NLP工作也都是基于字或词，忽略了词的内部结构，
% 而词内部结构是很重要的：
% 与flat word相比，intra-word structure can be informative在以下几个方面有优势：
% 1.很多词的含义可以通过分析其内部组成成分来获得，从而可以缓解伪OOV现象：比如劳动法在ctb5中是OOV，但是ctb5中有婚姻法、劳动等词，通过分析内部结构，可以使更准确地表示伪OOV（李中国12）。很多词的含义可以通过充分考虑各内部成分的关系和组合其内部成分的含义得到。
% 2.词内部结构比flat词蕴含更丰富的信息。有些特殊的语言现象需要通过词内部结构才能准确刻画：比如大中小学、动植物，如果不分析内部结构，就丢失了大学、中学、小学并列的信息（李中国11和赵海09都提到了）
% 3.此外，中文分词标准不同，导致标注不一致。利用词内部结构信息可以灵活得到不同粒度的词以满足不同应用的需求(Zhang14)【这个要写吗？】
% 因此，基于词内部结构可以建模词的语义表示，作为传统以词为基本单元的词向量表示方法的有效补充。
% 考虑到词内部结构存在以上优点，近年来，有一些工作研究了词内部结构，在xxx任务上验证了有效性【zhao09, li1112,zhang1314,su20】。但是这些工作通常只考虑结构框架和核心字信息，不考虑各个字之间的关系类型。Zhao18用词性组合来表示各个字之间的关系，然而这种表示也并不能直观体现出各个字之间的依存类型。

% 在我们的工作中，我们根据词结构的特点设计了11种依存标签，能直观地反映出词内部各个字直接的依存关系，``provide a more general and natural
% way to reflect character relations within a sequence than word boundary annotations do.''


% word internal structure 还是word-intra structure统一一下

%中文和英文不同之处，词是由字构成的，字本身是有含义的。而英文字母是没有含义的（词根有可能有含义，词根里面的每一个字母又没有含义的。可以加个脚注：有一部分英语单词的含义，可以由切分出来的几个子串的含义猜出来）。但是中文，非常明确的，由几个字构成。每个字的含义很具体，并且通过一定的含义组合composition构成。大部分词都是composition，很少的是atom，这种词通常是外来语等等。

%中文语言理解通常都以词为单位（比如句法分析、语义分析、检索）。
%(BERT是以字为单位去学习表示的)
%但是也有很多工作加入词信当然神经网络下，很多端到端的工作，可以以字为单位
%（机器翻译中以词为单位比较好）。

% As the minimal semantic units that can act independently (can be used alone), words play an important role in the Chinese language. 
% Traditionally, words are defined as the character sequences that combine tightly and occur frequently 【信息处理用现代汉语分词规范】. 
% Similar to the sentences which have word-level syntactic structures, most of the Chinese words have character-level internal syntactic structures. 【As shown in Figure 用图吗？】, typical word internal structures include modifier-noun, coordination, subject-predicate and verb-object structures.
% The meaning of a word can be reflected by the meaning of its constituent characters and the way that characters interact each other. For example, the word ``创业者 (entrepreneur)'' is composed of three characters ``创 (start)'', ``业 (business)'' and ``者 (people)'', where the characters ``创 (start)'' and ``业 (business)'' form a verb-object structure and modify the character ``者 (people)''. This accurately convey the meaning of ``创业者 (entrepreneur)'' is ``the people who start a business''.

% 词的表示是非常核心的问题。word embedding之前，one-hot的表示，相似的词之间的相似度很难刻画。
% word embedding呢，虽然缓解这个问题，但是是靠大规模无标注数据，也就是上下文信息来表示词。而从词本身出发去理解词、表示词，必然能够得到更好的词表示。



% However, many manually annotated data in Chinese, such as CTB【全称，引用】, PKU【】 and MSR【】, takes words as the basic processing unit, and previous approach for Chinese language processing is typically based on characters or words, giving no consideration to the syntactic structures within words.
% Internal word structures can be beneficial from the following perspectives:
% 1)【新词的理解】The issue of Out-of-vocabulary (OOV) words can be alleviated by analyzing their internal structures. As pointed out by \citet{li-2011}, more than 60\% of the OOVs are pseudo OOVs, which means though these words do not exist in the training data, their components or words with similar structures are frequently-occurring words. For example, ``劳动法 (labor law)'' is an OOV of CTB5, but ``劳动 (labor)'', ``法 (law)'' and ``婚姻法 (marriage law)'' appears several times in the CTB5 training data. The meaning of ``劳动法 (labor law)'' can be obtained by fully considering the syntactic relations between its constituents and combining their semantics.
% 2）Word internal structure can be informative with dependency and relation labels between characters. It can represent dependencies between discontinuous characters, while word boundary information is insufficient to handle such language phenomena in Chinese. 
% 3) Words of granularities can be extracted according to the intra-word dependency, and hence can benefit downstream NLP applications with flexible segmentation standards【参考文献】.   

% In fact, 
% % Motivated by above perspectives,
% modeling word internal structures had attracted a lot of research attention, mostly before the pre-DL era (cite三个人的三个文章： 
% 李、张梅山、赵海三个工作）。上面有图对几个工作进行了对比。
% Please refer to Section \ref{xx} for more discussions on previous works. 
% Dataset【】, and verifying the effectiveness of word internal structure in improving performance for various Chinese language processing tasks such as word segmentation \cite{zhao-2009}, parsing \cite{li-zhou-2012,zhang-etal-2013,zhang-etal-2014}, and machine translation【苏20】 by exploring rich embedding features or joint learning. 
%  【下面这段话和annotation的开头重复了，需要考虑把这段话放在哪，或者改一下表达方式】
%  Most of these work mainly focus on the the hierarchical structure or the head character information for each word, without considering the relation types between constituent characters inside the word. 
% Li et al. \cite{DBLP:conf/aaai/LiZJZ18} represent intra-word character-level dependency label by combining the character-level POS tags of the constituent characters. However, the typical syntactic structures such as subject-predicate, verb-object, modifier-noun, coordination, which is quite important for analyzing。。。  can not be directly reflected by such dependency labels.
% To overcome this obstacle, we construct intra-word character dependency treebank of over xxx words in CTB5 \cite{ctb-xiafei} and CoNLL09 \cite{conll09} 
% according to our newly complied annotation guideline, which contains 11 relations to intuitively reflect the internal dependency syntax for Chinese words.
% 。。。。。。。。。。


% 1.词内部结构比flat词蕴含更丰富的信息。convey meaning (zhang13)  interact with each other from both syntax and semantics (AAAI18). provide more general and natural way to reflect character relations within a character sequence than word boundary annotations (Zhao09)
% providing both internal structures to synthetic words and global structure to sentences in a seamless fashion (LREC14)
% 建模中文词内不同字之间的关系，建模词的语义表示，作为传统以词为基本单元的词向量表示方法的有效补充。(苏劲松2020)


% 2.中文分词标准不同，导致标注不一致。利用词内部结构信息可以灵活得到不同粒度的词(Zhang14)。a. 从而满足不同应用的需求。b. 得到子词来缓解OOV（LREC14,IJCNLP15）

% 3.动植物、大中小学这些词难以通过分词准确刻画？而词内部结构可以刻画这种语言现象



