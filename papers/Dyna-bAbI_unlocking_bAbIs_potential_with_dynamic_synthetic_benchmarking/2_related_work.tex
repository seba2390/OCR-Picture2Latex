Our work brings together two promising areas of current research: dynamic benchmarking such as Dynabench~\citep{kiela2021dynabench} that address many  existing issues with static benchmarks~\citep{bowman-dahl-2021-will}, and synthetic benchmarking, which is widely used for high-precision and data-intensive problems such as relational and logical reasoning \cite{sinha-etal-2019-clutrr,clark2020transformers,betz2020critical}, robot planning \cite{banerjee2020transformers}, instruction following and language grounding \cite{long-etal-2016-simpler,lake2018generalization} among many others \cite{richardson2020probing,khot2021learning}. Most approaches to synthetic benchmarking focus on model development on a static benchmark, and are not designed to facilitate agile and highly controlled task space exploration, which is our focus here. The recent gSCAN dataset~\citep{ruis2020benchmark} and later extensions~\citep{qiu2021systematic,wu2021reascan}
 can be seen as an example of a synthetic benchmark ``going dynamic''. Our work differs in terms of target domain (story understanding as opposed to multi-modal language grounding), and we further focus attention on a more general research direction of intentional, a-priori design of NLU benchmarks for agile development.

We address the domain of story understanding as a particularly core (and data-intensive) capacity underlying language use~\citep{McClelland2020}, thought to require constructing and manipulating situation models of entities and their relations as they unfold throughout discourse~\citep{Zwaan2016,tamari-etal-2020-language}.  Procedural text datasets~\citep{dalvi-etal-2018-tracking,tandon-etal-2020-dataset} are closely related in that they provide detailed annotation of entities and state changes, and have mostly focused on relatively small and static benchmarks using human collected data. Overall, recent works identify a lack of benchmark tasks which systematically probe the situation models constructed by NLP systems processing discourse-level texts~\citep{sugawara-etal-2021-benchmarking}. 

The bAbI benchmark~\cite{babi2016} is seen as highly relevant in terms of objective (targeting situation modelling)~\citep{dunietz-etal-2020-test}, but has been viewed critically due to its constrained nature and exploitable artifacts~\citep{kaushik-lipton-2018-much}. Our work focuses on improving the evaluation in bAbI through compositional generalization, widely used across NLP to more rigorously probe model robustness~\citep{finegan-dollak-etal-2018-improving,keysers2020measuring,gontier2020measuring,yanaka-etal-2021-sygns}, but to our knowledge still not applied to story understanding or bAbI.







