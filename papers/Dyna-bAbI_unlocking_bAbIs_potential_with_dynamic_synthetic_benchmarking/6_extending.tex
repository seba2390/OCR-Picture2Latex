
We showed above that there remain many interesting areas to explore in the existing bAbI task space. Despite this, the space of possible tasks is of course still quite limited by available building blocks of the simulator: the set of implemented events and linguistic constructs. Another benefit of synthetic benchmarks is their easy extensibility: new building blocks can be added, facilitating  systematic study of various phenomena of interest. The code-based nature of synthetic datasets means that extensions also enjoy increasing returns to scale: new concepts (e.g., color modifiers) can be integrated seamlessly into the simulator, leading to combinatorial increase in potential task complexity.


To demonstrate this aspect of agile synthetic benchmarking, we extend the bAbI simulator with an ``all'' quantifier, which can be combined with DROP and GIVE events to allow sentences such as depicted in Fig. \ref{}. We perform a simple compositional generalization experiment: we create an additional 9k training and 1k validation examples featuring questions involving ``drop all'' events, and a held out test set featuring 1,000 examples containing questions over ``give all'' events. For this preliminary investigation, we evaluate on ``What is P carrying?'' type questions, as the ``all'' quantifier most directly affects agent possessions. To control for question complexity, for both datasets we sample questions with $f$ distributed uniformly in the range X.  We fine-tune a T5 trained on the \texttt{inject}($T_{12}$) dataset until it achieves \rnote{X} validation accuracy, and evaluate it on the held out set. As can be seen in Tab \ref{}, T5 can learn to correctly answer questions related to the observed ``drop all'' event. However, this ability does not transfer to the held-out set, suggesting that the model is learning shortcuts and not the semantics of ``all''.