Considerable progress has been made recently in natural language understanding (NLU), driven largely by advances in model pre-training \cite{devlin-etal-2019-bert,2020t5}  and the development of large-scale NLU benchmarks across a wide range of tasks \cite{wang-etal-2018-glue,wang2019superglue,liang-etal-2020-xglue}.
\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figs/sdb_overview.png}

\caption{\label{fig:sdb-overview}
(a) Low task configurability leads to static datasets, benchmark saturation \& unreliable model development. (b) We propose a dynamic benchmarking approach; developing models and tasks in a tight feedback loop using (c) \pybabi task generator. \pybabi provides fine-grained control over task structure, composition and difficulty, yielding challenging new test sets exposing limitations of state-of-the-art models.}
\end{figure}
Such successes, however, have coincided with the discovery of various shortcomings in existing human curated datasets, largely related to \emph{annotation artifacts} \citep{gururangan-etal-2018-annotation}, or systematic biases that create shortcuts that can inflate model performance and harm generalization. 

In order to overcome these issues, two avenues of research have recently gained traction: 1)~the development of \emph{dynamic benchmarks} \cite{potts-etal-2021-dynasent,kiela2021dynabench} where, in contrast to conventional \emph{static} benchmarks, evaluation and data collection are preformed in an agile manner and conducted interactively with humans and models in a rapidly evolving feedback loop and; 2)~renewed interest in  \emph{synthetic benchmarks} \cite{lake2018generalization,sinha-etal-2019-clutrr,clark2020transformers,ruis2020benchmark} that allow for absolute control over the data creation process in order to help understand the strengths and weaknesses of existing models on targeted tasks and language phenomena. 




Story understanding is a particularly important domain for research on dynamic and synthetic benchmarks; it is a core competency for NLU systems~\citep{McClelland2020,dunietz-etal-2020-test}, but the scale and annotation detail required make human data collection prohibitively costly. However, the main synthetic resource for story understanding remains the bAbI task suite~\citep{babi2016}, which is saturated by models reaching near-perfect performance~\citep{liu2021small}, and further limited by exploitable biases in the data~\citep{kaushik-lipton-2018-much}. Despite its creators' initial intentions, bAbI has largely remained a static benchmark limited to a small subset of the tasks potentially possible to generate within the bAbI ``micro-world''. Accordingly, two natural questions arise: \textbf{(Q1)} \emph{is near-perfect model performance on the original bAbI tasks a reliable indicator of story understanding competence?}; \textbf{(Q2)} \emph{are there still interesting challenges to discover inside the broader bAbI task space that help identify weaknesses in current models and drive modeling innovation?}


To answer these questions, we employ a \emph{dynamic synthetic benchmarking} approach on bAbI, combining the benefits of the agile approach of recent dynamic benchmarks with the scale and control provided by synthetic datasets. As illustrated in Figure~\ref{fig:sdb-overview}, in dynamic synthetic benchmarks the data generator itself is designed for agile development, enabling experimentation with increasingly complex tasks and a wider range of linguistic phenomena. Constructing challenging tasks is a challenge in and of itself, requiring precise control over the reasoning patterns underlying each question. To meet these requirements, we developed a new task generator for bAbI called \pybabi\footnote{Implemented in Python for improved accessibility compared with the original Lua implementation (\url{https://github.com/facebookarchive/bAbI-tasks}).}. 



Using \pybabi, we first devise new splits that systematically test \emph{compositional generalization} across tasks; as shown in Fig. \ref{fig:sdb-overview}c, we test models on novel combinations (line 10 question on right) of concepts  seen at training, like co-reference and object tracking (left). We find that training on the original bAbI tasks (hereafter: \babibm) is not sufficient for models to attain good compositional generalization. Though general purpose pre-trained models far outperform special-purpose (non-pre-trained) architectures developed for bAbI, they still suffer a 30-50\% drop in accuracy compared to the non-pre-trained models which suffer a 50-80\% drop. Both types attain near perfect performance on the original tasks, suggesting that \babibm is not challenging enough to differentiate between the two classes of models \textbf{(Q1)}. 

We next investigate how different enhancements of training data affect compositional generalization: (a) injecting more questions into \babibm, and (b) generating new, more diverse training samples. Compared to question injection, we find that diverse training data better facilitates compositional generalization, as well as being more data efficient. However, neither approach drives \emph{reliable} compositional generalization; a representative state-of-the-art (SOTA) model, T5~\citep{2020t5}, demonstrates a lack of robustness to novel combinations and also exhibits knowledge inconsistency, for example, correctly answering certain types of questions, but systematically failing to answer equivalent question paraphrases. These results suggest that there remain many important challenges within the broader bAbI task space \textbf{(Q2)}. 



To sanity-check the quality of our new tests compared with \babibm, we employ the notion of \emph{concurrence} proposed by \citet{liu2021small}; concurrence is a measure of correlation between models' performance on a synthetic task and their performance on an existing, non-synthetic NLU benchmark. We find high concurrence between our new challenge tasks and the widely used SQuAD dataset~\citep{rajpurkar-etal-2016-squad}, in contrast to \babibm which achieved low concurrence. 

Giving the continued interest in using \babibm to evaluate new modeling approaches ~\citep{Banino2020MEMO,banino2021pondernet,schlag2021learning}, our new challenge splits and the \pybabi task generator contribute to more reliably guiding future efforts.
While we focused on bAbI, our results apply more generally, telling a cautionary tale about the limits of static synthetic datasets, and motivating the development of controllable task generators for dynamic synthetic benchmarking.



