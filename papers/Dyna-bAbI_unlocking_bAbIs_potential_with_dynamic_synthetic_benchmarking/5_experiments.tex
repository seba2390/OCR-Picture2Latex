With the controllable task generation afforded by \pybabi, we can now create datasets probing deeper story understanding capabilities of models.


We present two main experiments targeting the following questions:
\begin{compactitem}
    \item Exp. 1: (q1.a) What role does model architecture play in the capacity for compositional generalization? (q1.b) What is the concurrence of our compositional tasks with real datasets, compared with \babibm? 
    \item Exp. 2: (q2) How do training data quantity and diversity affect compositional generalization?
\end{compactitem}
\subsection*{Data} 

\input{tables/tasks_table}

For our experiments we created 4 kinds of splits over three subsets of \babibm tasks, summarized in Table \ref{tab:datasets}. We denote a subset of tasks $T$, and consider $T_2=\left\{ 2,11\right\} $, $T_7=\left\{1,2,3,5,11,..,13\right\}$, and $T_{12}=\left\{1,2,3,5,...,13\right\}$.
\begin{compactitem}
    \item \concatnarg splits are simply concatenations of the official data for the tasks $T$. We considered the larger version where each task consists of 9,000/1,000 training/development examples; e.g., \concat{$T_2$} consists of 18,000 training examples and 2,000 development examples.
    \item \injectnarg splits enrich the \concatnarg data as follows: for each question in the original data, we supplement it with all possible additional questions of the specified types. In this work, the supplement question types were \wherep and \whereo (to provide location information of objects and agents).
    \item \diversenarg splits use rejection sampling to generate more diverse samples, such that the number of supporting facts per question is roughly uniform across all sub-task instances for a given question type. Without rejection sampling, most generated questions would be trivial (e.g., 1-2 supporting facts). Compositionality is retained by holding out certain combinations. In particular, at training time, complex linguistic constructs (e.g., co-reference) are only seen with MOVE events.
    \item \mixnarg are test splits generated using rejection sampling like \diversenarg, and consist of instances which may feature events, linguistic constructs and questions from any of the considered tasks. As a result, questions in \mixnarg splits require novel/more complex reasoning patterns compared to those seen at training time.
\end{compactitem}

See appendix \ref{ssec:ext_task_details} for examples and extended details on task generation.

\subsection{Exp. 1: Can training on \babibm facilitate compositional generalization?} 


\input{tables/exp_1_result_table}


For this experiment, we compared models on $T_2$ and $T_7$, since they allow for a direct conversion to an extractive QA format,\footnote{Tasks 6-10 require generative QA, for answering \yesno, \countq and \listq questions.} thus enabling us to use the same concurrence measurement framework of \citet{liu2021small}.










\xhdr{Models} We considered 3 classes of models: 
\begin{compactitem}
    \item  Non-pre-trained specialized architectures for \babibm including EntNet~\citep{entnet2017} and STM~\citep{le2020self}, the latter being current SOTA on \babibm\footnote{As of \writingdate.}.
    \item  Non-pretrained general-purpose QA methods, such as BiDAF~\citep{seo2016bidirectional}.
    \item General purpose pre-trained approaches including RoBERTa~\citep{liu2020roberta} and T5 (base)~\citep{2020t5}.
\end{compactitem}

The last two categories are comprised of the 20 models evaluated in \citet{liu2021small}, with the addition of T5 to the last group. For implementation details, see appendix \ref{ssec:impl}.


\subsubsection*{Results \& Analysis} 
\label{ssec:4_1_analysis}
Experiment results are summarized in Table \ref{tab:exp_1_results}. All models perform well in IID settings, but performance drops considerably in OOD settings, including for the SOTA STM model. Pre-trained models fare better on the OOD splits, but still suffer large drops for the harder 7 and 12-task OOD splits. 




\xhdr{Architecture alone is not a significant compositionality driver (q1.a)} The large OOD performance gap between pre-trained and non-pre-trained models indicates that pre-training plays a much greater role than specialized architectures for QA performance, adding to similar findings in other NLP domains~\citep{hendrycks-etal-2020-pretrained}. The results raise questions about special purpose relational reasoning architectures that continue to be developed today: the poor OOD performance suggests that such models may not be fulfilling their intended design. Either way, we believe these results underscore the importance of rigorous evaluation to verify that modelling motivations are borne out in practice~\citep{aina-etal-2019-entity}. 

\xhdr{Compositionality increases concurrence (q1.b)} As can be seen in the Fig. \ref{fig:concurrence} plots\footnote{See appendix \ref{ssec:ext_concurrence} for full numeric results.}, increasing compositionality is correlated with increased concurrence. In particular, the 7-task OOD split yields high concurrence with the SQuAD benchmark, comparable to other \emph{natural} language as well as purpose-built synthetic datasets considered in \citet{liu2021small}, which feature $r,\tau$ (Pearson and Kendall correlation functions, resp.) in the ranges $\left[0.87,0.99\right]$ and $\left[0.77,0.94\right]$, respectively. Our results extend the findings of \citet{liu2021small}; they demonstrated the \emph{existence} of high concurrence synthetic benchmarks, we additionally suggest a guiding principle for how to \emph{create} them (compositional generalization). 



\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figs/concurrence.pdf}

\caption{\label{fig:concurrence} SQuAD concurrence plots for bAbI task 2 (reproduced from \citet{liu2021small} with permission) and \mix{$T_{7}$}. bAbI task 2 has the highest SQuAD concurrence of all $T_7$ tasks, yet is still significantly lower than \mix{$T_{7}$}, highlighting the relevance of compositional evaluation.}
\end{figure}

 









\subsection{Exp. 2: enriching \babibm training data} 
\label{ssec:exp_2}
The results above suggest that the bAbI data in their current form may not be rich enough to drive compositional generalization.\footnote{An alternate hypothesis is that certain patterns may be too hard for models to learn; we confirm this is not the case by using the inoculation methodology of \citet{liu-etal-2019-inoculation}, see details in Appendix \ref{ssec:ext_inoc}.} In this experiment we probe this  question, enriching the training data to better understand its impact on compositional generalization. In particular, we investigate two approaches to enriching the training data while maintaining the compositionality evaluation, corresponding to the \injectnarg and \diversenarg splits.


We focus on pre-trained models, as they significantly out-performed non-pre-trained methods. We use T5 as a representative since its generative abilities make it straightforward to apply also to $T_{12}$ (unlike the extractive methods which were applicable only to $T_7$). 






\input{tables/exp2_results}


\xhdr{Injecting supplementary questions} One hypothesis for the poor performance of models on the \mixnarg splits could be that the original bAbI tasks do not provide enough supervision for models to learn the basic event semantics. For example, tasks 5 and 7 are the only \babibm tasks featuring the GIVE event, and neither includes any questions about the location of participants. However, test-time compositional questions may require models to infer that the participants in a GIVE event share the same location (e.g., line 10 question in Fig. \ref{fig:sdb-overview}c). Error analysis shows that such implicit inferences are indeed challenging for models trained on the \concatnarg splits (see appendix \ref{ssec:ext_err_analysis_give} for details). Perhaps the \injectnarg splits supplementing the original tasks with questions providing relevant information will improve compositionality performance? Table \ref{tab:exp_2_results} displays the result of this experiment; performance in the \mixnarg setting is improved only marginally, even though the amount of training data increases 3-fold (Table \ref{tab:datasets}).

\xhdr{Sampling structurally diverse training data} As shown in Table \ref{tab:datasets}, though \injectnarg splits significantly increase dataset size, their diversity remains low: most questions require only one or two supporting facts. Therefore, we next enrich training data through sampling more structurally diverse samples. This method is known to improve data efficiency for both compositional generalization as well as IID settings~\citep{oren2021finding}. As can be seen in Table \ref{tab:exp_2_results}, training on the \diversenarg splits yields a more significant improvement; similar to the findings of \citet{oren2021finding}, sampling more diverse training data leads to greater generalization as well as much improved data efficiency.\footnote{The relatively low performance of \diversenarg trained models in the ``3+'' column for \concatnarg splits is predominantly due to length discrepancies at train and test time: \concatnarg contains some very long stories which are challenging for the model trained on the uniform length and shorter \diversenarg stories.} However, as the error analysis of the next section shows, performance on compositional generalization is still fundamentally limited.








\subsubsection*{Discussion and error analysis} 
\label{ssec:exp_2_err_analysis}

\begin{figure*}[t!]
\centering
\includegraphics[width=1\linewidth]{figs/heatmaps_plots.png}
\caption{\label{fig:heatmaps} Error analysis on \mix{$T_{12}$} for T5 trained on \diverse{$T_{12}$} data.  Performance on support compositions seen at training time (blue frames) is generally high, but overall generalization is not systematic, as evidenced by high variance across different \suppcomp, especially for higher complexity and more novel compositions.}
\end{figure*}

Figure \ref{fig:heatmaps} breaks down the performance of T5 on \mix{$T_{12}$} after training on \diverse{$T_{12}$}. The heatmaps plot performance across various \reasonpat (\suppcomp) occurring in the test data, sub-divided by the number of required supporting facts $n$ per question. Performance on support compositions seen at training time (blue frames) is generally high, indicating the importance of training pattern diversity for better generalization.
The plots indicate that T5 shows some ability to generalize to new support compositions, especially for lower $n$. Furthermore, certain question types appear to be more learned more robustly; for \listq and \countq questions, performance remains relatively high even for larger $n$ and across novel \suppcomp. We hypothesize that such questions may be easier as simple counting rules suffice to reach an answer, and these are ``close to the surface''; unlike other events that may implicitly convey information, in our stories, changes of possession are always explicit in the text. 

In general however, the plots indicate that T5 is far from robust compositional generalization:

\xhdr{Performance deteriorates with increased complexity} Performance is near perfect for simple compositions ($n \leq 2$) but deteriorates significantly for more complex cases (e.g., center and right plots).

\xhdr{Inconsistent knowledge} The discrepancy between the relatively high performance on \wherep questions compared with very low performance on \yesno questions suggests that models aren't learning consistent knowledge representations. E.g., if a model answers $y$ correctly to some ``Where is $p$?'' question, we would expect it to answer ``yes'' correctly for the same question in \yesno format, ``Is $p$ at $y$?''. We present further empirical support for this finding in appendix \ref{ssec:ext_err_analysis_know_incon}.

\xhdr{Performance below chance for certain question types} The heatmaps expose a particularly challenging class of \yesno questions involving disjunctions over indefinites (center and right plots, bottom right); accuracy for such questions is close to zero. See appendix \ref{ssec:ext_err_double_disj} for an example instance.




























    

















