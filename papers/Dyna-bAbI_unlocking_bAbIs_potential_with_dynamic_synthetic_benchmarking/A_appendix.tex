\subsection{Extended task construction details}
\label{ssec:ext_task_details}

This section provides further details of the training and test splits used for our experiments. 

Table \ref{tab:concepts} enumerates the basic ``building blocks'', or concepts underlying the tasks, as presented in \S\ref{ssec:task_structure}.

Tables \ref{tab:app_cid_7} and \ref{tab:app_cid_12} detail the concept sets for each of the sub-tasks comprising the training and test sets, for the $T_2$, $T_7$ and $T_{12}$ groups of tasks.

\input{tables/appendix_concepts_table}

As can be seen from the tables, the main sources of compositionality are:
\begin{compactitem}
    \item Following the \babibm task structure, at training time, all of the more complex linguistic constructs are seen only with MOVE events (and none of the other event types).
    \item Similarly, at training time, \yesno questions are always seen only with MOVE events (and none of the other event types), and with the INDEF or NEGATE linguistic constructs (but not others, such as COREF).
    \item \wherewaso questions are never seen in stories with GIVE events.
\end{compactitem}


\subsubsection{Example instances}
Figure \ref{fig:split_examples} shows examples from each of the 4 types of splits used in our experiments. The \concatnarg instance is from the original \babibm task 5. The \injectnarg data contains the same passages as \concatnarg, but adds supplementary questions on agent and object locations. \diversenarg instances contain more diverse support compositions (\suppcomp), but certain combinations are held out. In particular, \diversenarg instances only feature non-default linguistic mappings with MOVE events, never with POSS (GRAB or DROP) or GIVE. In the \mixnarg instances, all combinations of support compositions are possible, as shown in the example which features possession (POSS) events along with co-reference.
\begin{figure*}[]
\centering
\includegraphics[width=\linewidth]{figs/split_examps.png}

\caption{\label{fig:split_examples} Example instances from each of the 4 types of splits used in our experiments.}
\end{figure*}



\input{tables/appendix_cid_7}

\input{tables/appendix_cid_12}

\subsubsection{Long instances in the \babibm tasks}
For the T5 experiments, we used a slightly modified version of the \babibm tasks, where we trimmed all training and validation examples that didn't fit into the 512-token input window. This resulted in trimming 1,585 training instances and 175 validation instances from $T_7$ and $T_12$ (common to both sets). These data points are not consequential as our analysis focuses on the effects of compositionality and not story length; all instances in \diversenarg and \mixnarg are substantially shorter than the 512-token maximum input window size.


\subsection{Implementation details}
\label{ssec:impl}

\xhdr{T5} We use the publicly available HuggingFace pre-trained T5-base implementation~\citep{wolf-etal-2020-transformers}. We fine-tune T5 for 12 epochs on our bAbI data, using the Adam optimizer~\citep{kingma2017adam}, an initial learning rate of $5 * 10^{-5}$ and training batch size of 8.

\xhdr{STM} We used the official STM implementation\footnote{\url{https://github.com/thaihungle/SAM}}, with the only change being a batch size of 32 instead of 128, due to technical constraints.

\xhdr{EntNet} We re-implemented the model in PyTorch, similarly using a batch-size of 32. Following the official Lua reference implementation\footnote{\url{https://github.com/facebookarchive/MemNN/tree/master/EntNet-babi}}, we used 20 memory units each with dimension 100. We used the SGD optimizer.

For both the EntNet and STM, we trained models for 200 epochs, and took the best of 10 tries, following ~\citet{entnet2017}.

For the 20-model concurrence benchmark, refer to \citet{liu2021small} for model details, as we used the same experimental setup.

For the T5 experiments, we used the PyTorch Lightning~\citep{falcon2019pytorch} trainer implementation, and Weights \& Biases~\citep{wandb} for experiment tracking and artifacts management.

\subsection{Inoculation experiment results}
\label{ssec:ext_inoc}
 To rule out the hypothesis that certain patterns may be too hard for models to learn, we follow the inoculation methodology presented in \citet{liu-etal-2019-inoculation}: after training on the original tasks, we fine-tune the T5 on small amounts of OOD data (disjoint from the test data), and evaluate performance as a function of ``inoculation dose''. As can be seen in Fig. \ref{fig:inoc}, we find that performance quickly (with only 500 additional inoculation samples per question type) reaches over 90\% accuracy on both the \mix{$T_{7}$} and \mix{$T_{12}$} challenge sets. These results support the hypothesis that the training data is not rich enough, indicating clearly that the model is capable of quickly learning to solve the challenge tasks, given exposure to training samples with similar enough patterns.
 

\subsection{Concurrence experiments}
\label{ssec:ext_concurrence}
Table \ref{tab:concurrence_full} presents the full results for the concurrence experiments of \S\ref{ssec:4_1_analysis}. SQuAD and bAbI task 2 results are reproduced from \citet{liu2021small}, see there also for implementation details of the models used.

\input{tables/appendix_concurrence_table}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figs/inoc.png}

\caption{\label{fig:inoc} Inoculation experiment results.}
\end{figure}




\subsection{Extended error analysis: GIVE events}
\label{ssec:ext_err_analysis_give}
We analyze the performance of models on the \mix{$T_7$} split after being trained on \concat{$T_7$}, and in particular we focus on GIVE events. As noted in \S\ref{ssec:exp_2}, compositions involving GIVE are intuitively challenging as they entail multiple inferences which are not explicit in the text: the actors share the same location, and the possession of the object being given is transferred from the giver to the recipient. The only task in \concat{$T_7$} featuring GIVE events is task 5, which never asks about the locations of actors or objects, but only about the participant roles in the event (e.g., who was the giver or recipient; see Fig. \ref{fig:sdb-overview} example from task 5).

\input{tables/tab_exp_1_breakdown}

To measure this intuition empirically, we analyze a subset of 567 questions including GIVE events in the supporting facts set. As shown in Table \ref{tab:exp_1_breakdown}, performance for all models on questions including GIVE is extremely low, far below performance for questions without it. Qualitative analysis indicates many failure cases follow the pattern shown in the right-side example of Fig. \ref{fig:sdb-overview}c, question on line 10: the location of an entity (e.g., Daniel) must be inferred via the known (co-)location of a second participant in the GIVE event (e.g., Jeff). These results strengthen the hypothesis that standard QA training on the original bAbI data does not drive strong event comprehension in models.



\subsection{Extended error analysis: knowledge inconsistency}
\label{ssec:ext_err_analysis_know_incon}
This section presents further analysis of the knowledge consistency of the T5 model trained on the \diverse{$T_{12}$} data when evaluated on the challenge set \mix{$T_{12}$}.

We collected all \yesno questions from \mix{$T_{12}$} for which the answer was ``yes'', yielding 446 questions in total. For each such (question, answer) pair, of the form (``Is \texttt{person} at the \texttt{location}?'', ``yes''), we created an equivalent pair in the format of a \wherep question, (``Where is \texttt{person}?'', \texttt{location}).

Ideally, we would expect a model to be agnostic to equivalent phrasings of a question. However, as displayed in Figure \ref{fig:kn_consist}, we find that T5 is considerably more accurate for questions posed in the \wherep format, likely due to exposure to a larger variety of such questions at training time. Figure \ref{fig:kn_con_example} shows a characteristic example: T5 correctly answers in the \wherep format, but wrongly answers ``maybe'' for the \yesno format, thrown off by the distractor indefinite phrase in sentence 3. The pattern of answering ``maybe'' to questions about the location of an actor mentioned in an indefinite is commonly observed in training.


\input{tables/appendix_err_kc_table}


These results highlight a limitation of text-to-text QA models such as T5: their story representation may be highly coupled with the input question. This form of representation stands in contrast to more human-like narrative comprehension which is thought to involve the construction of situation models;  structured representations of entities and their relations as depicted by the text. Situation models are not dependent on a-priori knowledge of a particular question, and are thought to constitute a representational substrate supporting more systematic inferential generalization.

\begin{figure}[]
\centering
\includegraphics[width=\columnwidth]{figs/ex_kn_con.png}

\caption{\label{fig:kn_con_example} Example \mix{$T_{12}$} instance displaying model knowledge inconsistency: T5 correctly answers the question in \wherep form (line 22), and incorrectly in \yesno form (line 21).}
\end{figure}

\subsection{Extended error analysis: double disjunctions}
\label{ssec:ext_err_double_disj}

As the shown in the \S\ref{ssec:exp_2_err_analysis} error analysis, a particularly difficult class of questions are double disjunctions over indefinite expressions. Figure \ref{fig:double_disj_1} displays a typical example from \mix{$T_{12}$}, where the locations of two actors are given in indefinite form (sentences 3 and 19), and are also known to be co-located, since they share the location of the object ``football'', as inferred from sentences 18 and 20. Hence it is possible to infer their location as the intersection of the two indefinite expressions (here ``bedroom''). Rather than answering ``yes'' to the question ``Is John in the bedroom?'', T5 invariably answers ``maybe'' for such cases. This pattern is likely due to the fact that in the training data ``maybe'' is a typical answer for \yesno questions about actors mentioned by indefinite expressions (task 10 in \babibm).


\begin{figure}[!t]
\centering 
\fbox{\begin{minipage}{15em}
\small
1 Bill grabbed the milk.\\
2 Bill put down the milk.\\
3 John is either in the bedroom or the kitchen.\\
4 Fred journeyed to the kitchen.\\
5 John grabbed the football.\\
6 Following that he put down the football.\\
7 Bill picked up the milk.\\
8 Following that he went to the bedroom.\\
9 Bill is in the office.\\
10 Bill is in the cinema.\\
11 Bill passed the milk to Julie.\\
12 Julie handed the milk to Bill.\\
13 Jeff is not in the school.\\
14 John took the football.\\
15 Fred and Jeff moved to the school.\\
16 Afterwards they journeyed to the bathroom.\\
17 Bill handed the milk to Julie.\\
18 John dropped the football.\\
19 Daniel is either in the school or the bedroom.\\
20 Daniel took the football.\\
21 Is John in the bedroom?	yes	3 18 19 20
\end{minipage}
}
\caption{Double disjunction example from \mix{$T_{12}$}.}
\label{fig:double_disj_1}
\end{figure}