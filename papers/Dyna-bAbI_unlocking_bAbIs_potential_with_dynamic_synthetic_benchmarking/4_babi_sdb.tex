
\subsection{\pybabi}
\label{ssec:pybabi}
What makes a synthetic benchmark \emph{dynamic}? We think of a dynamic synthetic benchmark as a highly controllable task generator, enabling rapid exploration of interesting areas of task space. The original \babibm simulator code does not readily facilitate such exploration; each of the \babibm tasks is generated by a hard-coded script which does not neatly expose ``dials'' to manipulate interesting generation aspects such as question difficulty or compositionality. 

Accordingly, we developed \pybabi, a Python-based version of the original simulator. \pybabi facilitates control of task generation through a configuration file, effectively abstracting away much of the underlying implementation complexity. The configuration file allows users to specify high-level task parameters such as the concepts set, passage length, and filtering conditions to mine for harder/rarer examples. We also modularized the code to facilitate adding new questions and other concepts more easily.

In this next sections we describe the underlying structure of the \babibm tasks, and how we combine them using \pybabi to create more complex compositional generalization tasks.

\subsection{bAbI task structure}
\label{ssec:task_structure}
A task in \babibm is a set of train, validation and test splits. Each split is a set of instances, where an instance is a tuple ($p,q,a$)=(\textit{passage, question, answer}). Passages are generated using a micro-world simulator, by sampling a valid sequence of world events from an event set \eventset and generating a linguistic description of them. By default, linguistic descriptions are generated by a simple sentence-level mapping from an event to a natural language sentence. For example, the event \texttt{move(john,park)} could be translated to ``John moved to the park.'' Some tasks also incorporate more complex linguistic mappings between events and sentences, such as co-reference: the event sequence (\texttt{move(john,park)}, \texttt{move(john,kitchen)}) could be mapped to ``John moved to the park. Then he went to the kitchen.'' 

We denote the set of possible linguistic mappings by \lingset. Finally, a valid question-answer pair ($q$,$a$) over $p$ is sampled from question set \questionset. In bAbI, splits are usually generated using a subset of possible events, linguistic constructs and questions; we denote these as \posseventset,\posslingset,\possquestionset, respectively. We can then define the \emph{concept set} of a specific split,  $\mathcal{C}=\mathcal{E}\cup\mathcal{L}\cup\mathcal{Q}$.
Instances also include a set of supporting facts ($f$), or the relevant lines from which $a$ can be derived (see Fig. \ref{fig:sdb-overview}). The support composition ($f_{c}$) is the set of events and linguistic constructs contained in $f$ (see examples in \S\ref{ssec:exp_2_err_analysis}), and is useful for characterizing compositionality performance (\S\ref{ssec:comp_gen_motivation}).


\subsection{Original \babibm tasks}

\input{tables/SU12_table}

Our focus here is on a particular subset of 12 \babibm tasks evaluating aspects of story understanding. Table \ref{tab:babi_20_tasks} summarizes them, detailing \posseventset,\posslingset,\possquestionset for each task. For \posslingset, we list only complex constructs beyond the default event-sentence mapping (which is present in every task). See appendix \ref{ssec:ext_task_details} for additional details on task construction. Not all of the story understanding tasks are considered. For example, tasks 14 and 20 address time reasoning and agent motivations, and we leave their integration for future work. 





\subsection{Compositional generalization on bAbI}
\label{ssec:comp_gen_motivation}

As can be seen in Table \ref{tab:babi_20_tasks}, many possible task configurations are not covered by the original benchmark; which directions should be explored? We focus on out-of-distribution (OOD) robustness, which is increasingly seen as a vital evaluation criteria across AI/NLP research~\citep{shanahan2020artificial,hendrycks-etal-2020-pretrained}. In particular, we target the OOD capacity for \emph{compositional generalization}; the ability to systematically generalize to test inputs containing novel combinations of more basic elements seen at training time~\citep{partee1995lexical,lake2017building}. For example, a model that has learned basic object tracking and co-reference \emph{separately} (tasks 2 and 11, see Fig. \ref{fig:sdb-overview}c) could be expected to solve tasks requiring a \emph{mixture} of both object tracking and co-reference (Fig. \ref{fig:sdb-overview}c, line 10 question on right side). Compositional tasks are absent from \babibm which features only IID test sets (independent, identically distributed).\footnote{\citet{babi2016} noted that transfer learning was an important goal out of the original work's scope.} 


\xhdr{Compositional task generation} To create compositional generalization tasks in practice, we create training (and validation) splits composed of $M$ sub-tasks with concept sets $\left\{ \mathcal{C}_{\text{train}}^{i}\right\} _{i=1}^{M}$, and a test set $\mathcal{C}_{\text{test}}$ such that $\mathcal{C}_{\text{test}}\neq\mathcal{C}_{\text{train}}^{i}\forall i$, but $\mathcal{C}_{\text{test}}=\bigcup_{i=1}^{M}\mathcal{C}_{\text{train}}^{i}$. In other words, each training sub-task can be thought of focusing on a particular subset of test concepts, so models are exposed to all test concepts at training time, but not to all combinations of them~\citep{yanaka-etal-2021-sygns}. 

\xhdr{Task difficulty} We hypothesize that support composition (\suppcomp) and supporting fact set size (\suppsize) are main factors underlying a particular instance's difficulty, and especially \emph{novel} support compositions not seen at training time. Additionally, the difference between train and test splits results in potentially harder distractors, as test-time distractors appear in novel contexts.

Our notions of concept and support composition resemble atoms and compounds in DBCA, a related study on compositionality~\citep{keysers2020measuring}. While DBCA enables automatic creation of compositional train and test splits, we opt here for a more human-interpretable representation that allows more precise manual control of the combinations of concepts a model is exposed to at train and test time.















\xhdr{Quality comparison vs. \babibm tasks} Intuitively, good synthetic datasets help drive the development of better modelling approaches. Our new compositional tasks might be harder than \babibm, but how do we know whether they are a more useful target? To provide a preliminary answer to this question, we adopt the notion of \emph{concurrence} as a quality measure~\citep{liu2021small}. Two benchmarks are said to have high concurrence when they rank a set of modelling approaches similarly. Concurrence offers a way to formalize the intuition above, as high concurrence between a synthetic and natural language benchmark suggests that the synthetic benchmark could have driven similar innovations. We follow the setup of \citet{liu2021small} using SQuAD for the natural language benchmark.\footnote{\citet{liu2021small} consider a set of 20 modelling approaches used on SQuAD, including 10 pre-trained and 10 non-pre-trained methods.} Notably, \babibm achieved very low concurrence with SQuAD; for example, pre-training consistently yields large gains on SQuAD, but on \babibm, both pre-trained and non-pre-trained models achieve perfect performance on many tasks. The low concurrence thus suggests that \babibm may be an unreliable benchmark for model development, and highlights the importance of improving its quality.



















 






