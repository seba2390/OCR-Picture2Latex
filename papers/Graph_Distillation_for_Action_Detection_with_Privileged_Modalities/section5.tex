In this section, we evaluate our method on two large-scale multimodal video benchmarks. The results show that our method outperforms representative baseline methods and achieves the state-of-the-art performance on both benchmarks. 



\subsection{Datasets and Setups}\label{sec:dataset_setups}
We evaluate our method on two large-scale multimodal video benchmarks: NTU RGB+D~\cite{ntu_rgbd} (classification) and PKU-MMD~\cite{pku_mmd} (detection). These datasets are selected for the following reasons. (1) They are (one of the) largest RGB-D video benchmarks in each category. (2) The privileged information transfer is reasonable because the domains of the two datasets are similar. (3) They contain abundant modalities, which are required for graph distillation. 

We use NTU RGB+D as our dataset in the source domain, and PKU-MMD in the target domain. In our experiments, unless stated otherwise, we apply graph distillation whenever applicable. Specifically, the visual encoders of all modalities are jointly trained on NTU RGB+D by graph distillation. On PKU-MMD, after initializing the visual encoder with the pre-trained weights obtained from NTU RGB+D, we also learn all available modalities by graph distillation on the target domain. By default, only a single modality is used at test time.

\noindent\textbf{NTU RGB+D~\cite{ntu_rgbd}.} 
It contains 56,880 videos from 60 action classes. Each video has exactly one action class and comes with four modalities: RGB, depth, 3D joints, and infrared. The training and testing sets have 40,320 and 16,560 videos, respectively. All results are reported with cross-subject evaluation.

\noindent\textbf{PKU-MMD~\cite{pku_mmd}.} 
It contains 1,076 long videos from 51 action classes. Each video contains approximately 20 action instances of various lengths and consists of four modalities: RGB, depth, 3D joints, and infrared. All results are evaluated based on the Average Precision (mAP) at different temporal Intersection over Union (tIoU) thresholds between the predicted and the ground truth intervals.

\noindent\textbf{Modalities.} We use a total of six modalities in our experiments: RGB, depth (D), optical flow (F), and three skeleton features (S) named Joint-Joint Distances (JJD), Joint-Joint Vector (JJV), and Joint-Line Distances (JLD)~\cite{ding2017investigation,10-stream}, respectively. The RGB and depth videos are provided in the datasets. The optical flow is calculated on the RGB videos using the dual TV-L1 method~\cite{zach2007duality}. The three spatial skeleton features are extracted from 3D joints using the method in \cite{ding2017investigation} and \cite{10-stream}. Note that we select a subset of the ten skeleton features in~\cite{ding2017investigation,10-stream} to ensure the simplicity and reproducibility of our method, and our approach can potentially perform better with the complete set of features.

\noindent\textbf{Baselines.}
In addition to comparing with the state-of-the-art, we implement three representative baselines that could be used to leverage multimodal privileged information: \textit{multi-task learning}~\cite{caruana1998multitask}, \textit{knowledge distillation}~\cite{distillation_hinton}, and \textit{cross-modal distillation}~\cite{distillation_gupta}. For the multi-task model, we predict the raw pixels of the other modalities from the representation of a single modality, and use the $L_2$ distance as the multi-task loss. For the distillation methods, the imitation loss is calculated as the high-temperature cross-entropy loss on the soft logits~\cite{distillation_hinton}, and $L_2$ loss on both representations and soft logits in cross-modal distillation~\cite{distillation_gupta}. These distillation methods originally only support two modalities, and therefore we average the pairwise losses to get the final loss.



\begin{table}[t]
\centering
\scriptsize
\caption{Comparison with state-of-the-art on NTU RGB+D. Our models are trained on all modalities and tested on the single modality specified in the table. The available modalities are RGB, depth (D), optical flow (F), and skeleton (S).}
\label{ntu_state_of_the_art}
\begin{tabular}{lc@{\hskip 0.1in}c@{\hskip 0.8in}l@{\hskip 0.4in}c@{\hskip 0.1in}c}
\toprule
Method & Test Modality & mAP & Method & Test Modality & mAP  \\
\midrule
Shahroudy~\cite{shahroudy2017deep} & RGB+D & 0.749 & Ours & RGB & \textbf{0.895} \\
Liu~\cite{liu2017viewpoint} & RGB+D & 0.775 & Ours  & D & 0.875 \\
Liu~\cite{skeleton_visualization} & S & 0.800 & Ours  & F & 0.857 \\
Ding~\cite{ding2017investigation} & S & 0.823 & Ours  & S & 0.837 \\
Li~\cite{10-stream} & S & 0.829 &&& \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\scriptsize
\caption{Comparison of action detection methods on PKU-MMD with state-of-the-art models. Our models are trained with graph distillation using all privileged modalities
and tested on the modalities specified in the table. ``Transfer'' refers to pre-training on NTU RGB+D on action classification. The available modalities are RGB, depth (D), optical flow (F), and skeleton (S).}
\label{pku_state_of_the_art}
\begin{tabular}{l@{\hskip 0.1in}c@{\hskip 0.1in}c@{\hskip 0.1in}c@{\hskip 0.1in}c}
\toprule
\multicolumn{2}{c}{} & \multicolumn{3}{c}{mAP @ tIoU thresholds ($\theta$)} \\
\cmidrule(r){3-5}
Method & Test Modality & 0.1 & 0.3 & 0.5 \\ 
\midrule
Deep RGB (DR) \cite{pku_mmd} & RGB & 0.507 & 0.323 & 0.147 \\
Qin and Shelton \cite{pku_result_qin} & RGB & 0.650 & 0.510 & 0.294 \\
Deep Optical Flow (DOF) \cite{pku_mmd} & F & 0.626 & 0.402 & 0.168 \\
Raw Skeleton (RS) \cite{pku_mmd} & S & 0.479 & 0.325 & 0.130 \\
Convolution Skeleton (CS) \cite{pku_mmd} & S & 0.493 & 0.318 & 0.121 \\
Wang and Wang \cite{pku_result_wang_workshop} & S & 0.842 & - & 0.743 \\
RS+DR+DOF \cite{pku_mmd} & RGB+F+S & 0.647 & 0.476 & 0.199 \\
CS+DR+DOF \cite{pku_mmd} & RGB+F+S & 0.649 & 0.471 & 0.199 \\
\midrule
Ours (w/o $|$ w/ transfer) & RGB & 0.824 $|$ 0.880 & 0.813 $|$ 0.868 & 0.743 $|$ 0.801 \\
Ours (w/o $|$ w/ transfer) & D   & 0.823 $|$ 0.872 & 0.817 $|$ 0.860 & 0.752 $|$ 0.792 \\
Ours (w/o $|$ w/ transfer) & F   & 0.790 $|$ 0.826 & 0.783 $|$ 0.814 & 0.708 $|$ 0.747 \\
Ours (w/o $|$ w/ transfer) & S   & 0.836 $|$ 0.857 & 0.823 $|$ 0.846 & 0.764 $|$ 0.784 \\
Ours (w/ transfer) & RGB+D+F+S & \bf{0.903} & \bf{0.895} & \bf{0.833} \\
\bottomrule
\end{tabular}
\end{table}



\noindent\textbf{Implementation Details.} 
For action classification, we train the visual encoder from scratch for 200 epochs using SGD with momentum with learning rate $10^{-2}$ and decay to $10^{-1}$ at epoch 125 and 175. $\lambda_1$ and $\lambda_2$ are set to $10,5$ respectively in Eq.~\eqref{eq:message_ab}. At test time we sample 5 clips for inference. For action detection, the visual and sequence encoder are trained for 400 epochs. The visual encoder is trained using SGD with momentum with learning rate $10^{-3}$, and the sequence encoder is trained with the Adam optimizer~\cite{kingma2015adam} with learning rate $10^{-3}$. The activity threshold $\gamma$ is set to $0.4$. For both tasks, we down-sample the frame rates of the datasets by a factor of 3. The clip length and detection window $T_c$ and $T_w$ are both set to 10. For the graph distillation, $\alpha$ is set to 10 in Eq.~\eqref{eq:graph_learning_softmax}. The output dimensions of the visual and sequence encoder are both set to 512. Since it is nontrivial to jointly train on multiple modalities from scratch, we employ curriculum learning~\cite{bengio2009curriculum} to train the distillation graph. To do so, we first fix the distillation graph as an identity matrix (uniform graph) in the first 200 epochs. In the second stage, we compute the constant vector $\mathbf{c}$ in Eq.~\eqref{eq:message_graph_final} according to the cross-validation results, and then learn the graph in an end-to-end manner.



\subsection{Comparison with State-of-the-Art}\label{sec:exp_soa}



\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{predictions}
\end{center}
\caption{\textbf{A comparison of the prediction results on PKU-MMD.} (a) Both models make correct predictions. (b) The model without distillation in the source makes errors. Our model learns motion and skeleton information from the privileged modalities in the source domain, which helps the prediction for classes such as ``hand waving'' and ``falling''. (c) Both models make reasonable errors.}
\label{fig:detection}
\end{figure}





\noindent\textbf{Action Classification.} Table~\ref{ntu_state_of_the_art} shows the comparison of action classification with state-of-the-art models on NTU RGB+D dataset. Our graph distillation models are trained and tested on the same dataset in the source domain. NTU RGB+D is a very challenging dataset and has been recently studied in numerous studies~\cite{10-stream,liu2017viewpoint,skeleton_visualization,luo2017unsupervised,shahroudy2017deep}. Nevertheless, as we see, our model achieves the state-of-the-art results on NTU RGB+D. It yields a 4.5\% improvement, over the previous best result, using the depth video and a remarkable 6.6\% using the RGB video. After inspecting the results, we found the improvement mainly attributes to the learned graph capturing complementary information across multiple modalities. Fig.~\ref{fig:graph} shows example distillation graphs learned on NTU RGB+D. The results show that our method, without transfer learning, is effective for action classification in the source domain.


\noindent\textbf{Action Detection.} Table~\ref{pku_state_of_the_art} compares our method on PKU-MMD with previous work. Our model outperforms existing methods across all modalities. The results substantiate that our method can effectively leverage the privileged knowledge from multiple modalities. Fig.~\ref{fig:detection} illustrates detection results on the depth modality with and without the proposed distillation.


\subsection{Ablation Studies on Limited Training Data}\label{sec:ablation}
Section~\ref{sec:exp_soa} has shown that our method achieves the state-of-the-art results on two public benchmarks. However, in practice, the training data are often limited in size. To systematically evaluate our method on limited training data, as proposed in the introduction, we construct mini-NTU RGB+D and mini-PKU-MMD by randomly sub-sampling 5\% of the training data from their full datasets and use them for training. For evaluation, we test the model on the full test set.



\begin{table}[t]
\centering
\scriptsize
\caption{The comparison with (a) baseline methods using Privileged Information (PIs) on mini-NTU RGB+D, (b) distillation graphs on mini-NTU RGB+D and mini-PKU-MMD. Empty graph trains each modality independently. Uniform graph uses a uniform weight in distillation. Prior graph is built according to the cross-validation accuracy of each modality. Learned graph is learned by our method. ``D'' refers to the depth modality.}
\subtable[\label{ntu_baselines}Baseline methods using PIs.]
{
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{lcc}
  \toprule
  Method & mAP / RGB \\
  \midrule
  Empty graph & 0.464 \\
  Multi-task \cite{caruana1998multitask}  & 0.456 \\
  Cross-distillation \cite{distillation_gupta}  & 0.503 \\
  Knowledge distillation \cite{distillation_hinton}  & 0.524 \\
  Learned graph & \bf{0.619} \\
  \bottomrule
  \end{tabular}
}
\subtable[\label{different_graphs}Different distillation graphs.]{
  \begin{tabular}{l@{\hskip 0.1in}c@{\hskip 0.2in}c}
  \toprule
  \multicolumn{1}{c}{} & mini-NTU & mini-PKU \\
  \cmidrule(r){2-3}
  Graph & {\tiny mAP / RGB} & {\tiny mAP @ 0.5 / D} \\
  \midrule
  Empty graph & 0.464 & 0.501 \\
  Uniform graph & 0.537 & 0.513 \\
  Prior graph & 0.571 & 0.515 \\
  Learned graph & \bf{0.619} & \bf{0.559}\\
  \bottomrule
  \end{tabular}
}
\end{table}

\begin{table}[t]
\centering
\scriptsize
\caption{The mAP comparison on mini-PKU-MMD at different tIoU threshold $\theta$. The depth modality is chosen for testing. ``src'', ``trg'', and ``PI'' stand for source, target, and privileged information, respectively.}
\label{pku_mmd_baselines}
\begin{tabular}{c@{\hskip 0.2in}l@{\hskip 0.4in}c@{\hskip 0.4in}c@{\hskip 0.4in}c}
\toprule
\multicolumn{2}{c}{} & \multicolumn{3}{c}{mAP @ tIoU thresholds ($\theta$)} \\
\cmidrule(r){3-5}
 & Method & $0.1$ & $0.3$ & $0.5$ \\
\midrule
1&trg only & 0.248 & 0.235 & 0.200 \\
2&src + trg & 0.583 & 0.567 & 0.501 \\
3&src w/ PIs + trg & 0.625 & 0.610 & 0.533 \\
4&src + trg w/ PIs & 0.626 & 0.615 & 0.559 \\
5&src w/ PIs + trg w/ PIs & 0.642 & 0.629 & 0.562 \\
\midrule
6&src w/ PIs + trg & 0.625 & 0.610 & 0.533  \\
7&src w/ PIs + trg w/ 1 PI & 0.632 & 0.615 & 0.549 \\
8&src w/ PIs + trg w/ 2 PIs & 0.636 & 0.624 & 0.557 \\
9&src w/ PIs + trg w/ all PIs & 0.642 & 0.629 & 0.562 \\
\bottomrule
\end{tabular}
\end{table}



\noindent\textbf{Comparison with Baseline Methods.} Table~\ref{ntu_baselines} shows the comparison with the baseline models that uses privileged information (see Section~\ref{sec:dataset_setups}). The fact that our method outperforms the representative baseline methods validates the efficacy of the graph distillation method.

\noindent\textbf{Efficacy of Distillation Graph.} Table \ref{different_graphs} compares the performance of predefined and learned distillation graphs. The proposed learned graph is compared with an empty graph (no distillation), a uniform graph of equal weights, and a prior graph computed using the cross-validation accuracy of each modality. Results show that the learned graph structure with modality-specific prior and example-specific information obtains the best results on both datasets.



\begin{figure}[t]
% \mpage{0.48}{\small{(a) Falling}}\hfill
% \mpage{0.48}{\small{(b) Brushing teeth}}\hfill
\begin{center}
\includegraphics[width=0.8\linewidth]{graph}
\end{center}
\caption{\textbf{The visualization of graph distillation on NTU RGB+D.} The numbers indicate the ranks of the distillation weights, with 1 being the largest and 5 being the smallest. (a) Class ``falling'': Our graph assigns more weight to optical flow because optical flow captures the motion information. (b) Class ``brushing teeth'': In this case, motion is negligible, and our graph assigns the smallest weight to it. Instead, it assigns the largest weight to skeleton data.}
\label{fig:graph}
\end{figure}



\noindent\textbf{Efficacy of Privileged Information.} Table~\ref{pku_mmd_baselines} compares our distillation and transfer under different training settings. The input at test time is a single depth modality. By comparing row 2 and 3 in Table~\ref{pku_mmd_baselines}, we see that when transferring the visual encoder to the target domain, the one pre-trained with privileged information in the source domain performs better than its counterpart. As discussed in Section~\ref{sec:collective}, graph distillation can also be applied to the target domain. By comparing row 3 and 5 (or row 2 and 4) of Table~\ref{pku_mmd_baselines}, we see that performance gain is achieved by applying the graph distillation in the target domain. The results show that our graph distillation can capture useful information from multiple modalities in both the source and target domain.

\noindent\textbf{Efficacy of Having More Modalities.} The last three rows of Table \ref{pku_mmd_baselines} show that performance gain is achieved by increasing the number of modalities used as the privileged information. Note that the test modality is depth, the first privileged modality is RGB, and the second privileged modality is the skeleton feature JJD. The results also suggest that these modalities provide each other complementary information during the graph distillation.



\subsection{Graph Distillation on UCF-101}

In this section, we consider graph edge distillation, a special case of graph distillation on UCF-101~\cite{soomro2012ucf101} in which only two modalities (RGB and optical flow) are available. Table~\ref{ucf101} shows the action classification results on UCF-101 using the two-stream architecture proposed in~\cite{two_stream_simonyan}. The optical flow modality performs significantly better than RGB when training from scratch. This is consistent with previous findings that dense optical flow is able to achieve very good performance in spite of limited training data \cite{two_stream_simonyan}. To testify our method, we train a model on the RGB modality from scratch with distillation. Our distilled model performs much better than the model directly trained from scratch. Note that our distilled model outperforms the fine-tuning model that uses pretrained weights on ImageNet.

\begin{table}[ht]
\scriptsize
\centering
\caption{Action classification results on UCF101. For graph distillation model, we distill knowledge from the optical flow stream to the RGB stream.}
\label{ucf101}
\begin{tabular}{l@{\hskip 0.1in}c@{\hskip 0.2in}c@{\hskip 0.1in}c}
\toprule
Method & Test Modality & mAP & Diff. \\
\midrule
From scratch   & Flow & 0.803 & - \\
From scratch   & RGB & 0.484 & +0.000 \\
ImageNet pretrained & RGB & 0.728 & +0.244 \\
Graph distillation & RGB & \textbf{0.757} & \textbf{+0.273} \\
\bottomrule
\end{tabular}
\end{table}
