In this section, we discuss our network architectures as well as the training and testing procedures for action classification and detection. The objective of action classification is to classify a trimmed video into one of the predefined categories. The objective of action detection is to predict the start time, the end time, and the class of an action in an untrimmed video.

\subsection{Network Architecture}\label{sec:network_architecture}
For action classification, we encode a short clip of video into a feature vector using the visual encoder. For action detection, we first encode all clips in a window of video (a window consists of multiple clips) into initial feature vectors using the visual encoder, then feed these initial feature vectors into a sequence encoder to generate the final feature vectors. For either task, each feature vector is fed into a task-specific linear layer and a softmax layer to get the probability distribution across classes for each clip. Note that a background class is added for action detection. Our action classification and detection models are inspired by~\cite{two_stream_simonyan} and~\cite{montes2016temporal}, respectively. We design two types of visual encoders depending on the input modalities.

\noindent\textbf{Visual Encoder for Images.} Let $X=\{x_t\}_{t=1}^{T_c}$ denote a video clip of image modalities (\textit{e.g.} RGB, depth, flow), where $x_t\in\mathbb{R}^{H\times W\times C}$, $T_c$ is the number of frames in a clip, and $H\times W\times C$ is the image dimension. Similar to the temporal stream in \cite{two_stream_simonyan}, we stack the frames into a $H\times W\times (T_c\cdot C)$ tensor and encode the video clip with a modified ResNet-18~\cite{resnet} with $T_c\cdot C$ input channels and without the last fully-connected layer. Note that we do not use the Convolutional 3D (C3D) network~\cite{i3d_carreira,c3d_tran} because it is hard to train with limited amount of data~\cite{i3d_carreira}.

\noindent\textbf{Visual Encoder for Vectors.} Let $X=\{x_t\}_{t=1}^{T_c}$ denote a video clip of vector modalities (\textit{e.g.} skeleton), where $x_t\in\mathbb{R}^{D}$ and $D$ is the vector dimension. Similar to \cite{10-stream}, we encode the input with a 3-layer GRU network~\cite{gru} with $T_c$ timesteps. The encoded feature is computed as the average of the outputs of the highest layer across time. The hidden size of the GRU is chosen to be the same as the output dimension of the visual encoder for images.

\noindent\textbf{Sequence Encoder.} Let $X = \{x_t\}_{t=1}^{T_c\cdot T_w}$ denote a window of video with $T_w$ clips, where each clip contains $T_c$ frames. The visual encoder first encodes each clip individually into a single feature vector. These $T_w$ feature vectors are then passed into the sequence encoder, which is a 1-layer GRU network, to obtain the class distributions of these $T_w$ clips. Note that the sequence encoder is only used in action detection.



\subsection{Training and Testing}

Our proposed graph distillation can be applied to both action detection and classification. For action detection, we show that our method can optionally pre-train the action detection model on action classification tasks, and graph distillation can be applied in both pre-training and training stages. Both models are trained to minimize the loss in Eq.~\eqref{eq:distilation_loss} on per-clip classification, and the imitation loss is calculated based on the representations and the logits. 

\noindent\textbf{Action Classification.}
Fig.~\ref{fig:modela} shows how graph distillation is applied in training. During training, we randomly sample a video clip of $T_c$ frames from the video, and the network outputs a single class distribution. During testing, we uniformly sample multiple clips spanning the entire video and average the outputs to obtain the final class distribution.

\noindent\textbf{Action Detection.}
Fig.~\ref{fig:modelb} and Fig.~\ref{fig:modelb} show how graph distillation is applied in training and testing, respectively. As discussed earlier, graph distillation can be applied to both the source domain and the target domain. During training, we randomly sample a window of $T_w$ clips from the video, where each clip is of length $T_c$ and is sampled with step size $s_c$. As the data is imbalanced, we set a class-specific weight based on its inverse frequency in the training set. During testing, we uniformly sample multiple windows spanning the entire video with step size $s_w$, where each window is sampled in the same way as training. The outputs of the model are the class distributions on all clips in all windows (potentially with overlaps depending on $s_w$). These outputs are then post-processed using the method in~\cite{montes2016temporal} to generate the detection results, where the activity threshold $\gamma$ is introduced as a hyperparameter.


