This paper tackles the problem of action classification and detection in multimodal video with limited training data and partially observed modalities. We propose the novel graph distillation method to assist the training of the model by leveraging privileged modalities dynamically. Our model outperforms representative baseline methods and achieves the state-of-the-art for action classification on NTU RGB+D dataset and action detection on the PKU-MMD. A direction for future work is to combine graph distillation with advanced transfer learning and domain adaptation techniques.

\section{Acknowledgement} 
This work was supported in part by Stanford Computer Science Department and Clinical Excellence Research Center. We specially thank Li-Jia Li, De-An Huang, Yuliang Zou, and all the anonymous reviewers for their valuable comments.