Our goal is to assist the training in the target domain with limited labeled data and modalities by leveraging the source domain dataset with abundant examples and multiple modalities. We address the problem by distilling the knowledge from the privileged modalities. Formally, we model action classification and detection as an $L$-way classification problem, where a ``background class'' is added in action detection. 

Let $\mathcal{D}_{t} = \{(x_i, y_i)\}_{i=1}^{|\mathcal{D}_{t}|}$ denote the training set in the target domain, where $x_i\in\mathbb{R}^d$ is the input and $y_i\in\mathbb{R}$ is an integer denoting the class label. Since training data in the target domain is limited, we are interested in transferring knowledge from a source dataset $\mathcal{D}_{s} = \{(x_i, \mathcal{S}_i, y_i)\}_{i=1}^{|\mathcal{D}_{s}|}$, where $|\mathcal{D}_{s}| \gg |\mathcal{D}_{t}|$, and the source and target data may have different classes. The new element $\mathcal{S}_i = \{x_i^{(1)},...,x_i^{(|\mathcal{S}|)}\}$ is a set of privileged information about the $i$-th sample, where the superscript indexes the modality in $\mathcal{S}_i$. As an example, $x_i$ could be the depth image of the $i$-th frame in a video and $x_i^{(1)},x_i^{(2)},x_i^{(3)} \in \mathcal{S}_i$ might be RGB, optical flow and skeleton features about the same frame, respectively. For action classification, we employ the standard softmax cross entropy loss:
{\small
\begin{equation}
\label{eq:softmax_xentropy}
\ell_c(f(x_i), y_i) = -\sum_{j=1}^L \mathbbm{1}(y_i =j) \log \sigma(f(x_i)),
\end{equation}
}where $\mathbbm{1}$ is the indicator function and $\sigma$ is the softmax function. The class prediction function $f:\mathbb{R}^d \to [1,L]$ computes the probability for each action class.

In the rest of this section, Section~\ref{sec:previledged_knowledge_distilation} discusses the overall objective of privileged knowledge distillation. Section~\ref{sec:collective} details the proposed graph distillation over multiple modalities. 



%-------------------------------------------------------------------------
\subsection{Knowledge Distillation with Privileged Modalities}\label{sec:previledged_knowledge_distilation}

To leverage the privileged information in the source domain data, we follow the standard transfer learning paradigm. We first train a model with graph distillation using all modalities in the source domain, and then transfer only the visual encoders (detailed in Sec~\ref{sec:network_architecture}) of the target domain modalities. Finally, the visual encoder is finetuned with the rest of the target model on the target task. The visual feature encoding step is shared between the tasks in the source and target data and is therefore intuitive to use the same visual encoder architecture (as shown in Fig.~\ref{fig:model}) for both tasks.

To train a graph distillation model on the source data, we minimize:
{\small
\begin{equation}
\label{eq:distilation_loss}
\min \frac{1}{|\mathcal{D}_{s}|} \sum_{(x_i, y_i) \in \mathcal{D}_{s}} \ell_c(f(x_i),y_i) + \ell_m(x_i, \mathcal{S}_i).
\end{equation}}The loss consists of two parts: the first term is the standard classification loss in Eq.~\eqref{eq:softmax_xentropy} and the latter is the imitation loss~\cite{distillation_hinton}. The imitation loss is often defined as the cross-entropy loss on the \emph{soft logits}~\cite{distillation_hinton}. In existing literatures, the imitation loss is computed using a pre-specified distillation direction. For example, Hinton et al.~\cite{distillation_hinton} computed the soft logits by $\sigma(f_{\mathcal{S}}(x_i)/T)$, where $T$ is the temperature, and $f_{\mathcal{S}}$ is the class prediction function of the cumbersome model. Gupta et al.~\cite{distillation_gupta} employed the ``soft logits'' obtained from different layers of the labeled modality. In both cases, the distillation is pre-specified, \textit{i.e.}, from a cumbersome model to a small model in~\cite{distillation_hinton} or from a labeled modality to an unlabeled modality in~\cite{distillation_gupta}. In our problem, 
the privileged information comes from multiple heterogeneous modalities and it is difficult to pre-specify the distillation directions and weights. To this end, our the imitation loss in Eq.~\eqref{eq:distilation_loss} is derived from a dynamic distillation graph.



\begin{figure}[t]
% \mpage{0.31}{\small{(a) Source train}}\hfill
% \mpage{0.31}{\small{(b) Target train}}\hfill
% \mpage{0.31}{\small{(c) Target test}}\hfill
\subfigure{\label{fig:modela}}
\subfigure{\label{fig:modelb}}
\subfigure{\label{fig:modelc}}
\begin{center}
\includegraphics[width=\linewidth]{model}
\end{center}
\caption{\textbf{An overview of our network architectures.} (a) Action classification with graph distillation (attached as a layer) in the source domain. The visual encoders for each modality are trained. (b) Action detection with graph distillation in the target domain at training time. In our setting, the target training modalities is a subset of the source modalities (one or more). Note that the visual encoder trained in the source is transferred and finetuned in the target. (c) Action detection in the target domain at test time, with a single modality.}
\label{fig:model}
\end{figure}



%-------------------------------------------------------------------------
\subsection{Graph Distillation}\label{sec:collective}

First, consider a special case of graph distillation where only two modalities are involved. 
We employ an imitation loss that combines the logits and feature representation. For notation convenience, we denote $x_i$ as $x_i^{(0)}$ and fold it into $\mathcal{S}_i = \{x_i^{(0)}, \cdots, x_i^{(|\mathcal{S}|)}\}$. Given two modalities $a,b \in [0, |\mathcal{S}|]$ $(a \ne b)$, we use the network architectures discussed in Section~\ref{sec:models} to obtain the logits and the output of the last convolution layer as the visual feature representation.

The proposed imitation loss between two modalities consists of the loss on the logits $l_{logits}$ and the representation $l_{rep}$. The cosine distance is used on both logits and representations as we found the angle of the prediction to be more indicative and better than KL divergence or L1 distance for our problem.

The imitation loss $\ell_m$ from modality $b$ to $a$ is computed by the weighted sum of the logits loss and the representation loss. We encapsulate the loss between two modalities into a message $m_{a \leftarrow b}$ passing from $b$ to $a$, calculated from:
{\small
\begin{equation}
m_{a \leftarrow b}(x_i) = \ell_m(x_i^{(a)}, x_i^{(b)}) = \lambda_1 l_{logits}+\lambda_2 l_{rep},
\label{eq:message_ab}
\end{equation}}where $\lambda_1$ and $\lambda_2$ are hyperparameters. Note that the message is directional, and $m_{a \leftarrow b}(x_i) \ne m_{b \leftarrow a}(x_i)$.


For multiple modalities, we introduce a directed graph of $|\mathcal{S}|$ vertices, named \emph{distillation graph}, where each vertex $v_k$ represents a modality and an edge $e_{k \leftarrow j} \ge 0$ is a real number indicating the strength of the connection from $v_j$ to $v_k$. For a fixed graph, the total imitation loss for the modality $k$ is:
{\small
\begin{equation}
\ell_m(x_i^{(k)}, \mathcal{S}_i) = \sum_{v_j \in \mathcal{N}(v_k)} e_{k \leftarrow j} \cdot m_{k \leftarrow j}(x_i),
\end{equation}}where $\mathcal{N}(v_k)$ is the set of vertices pointing to $v_k$. 

To exploit the dynamic interactions between modalities, we propose to learn the distillation graph along with the original network in an end-to-end manner. Denote the graph by an adjacency matrix $\mathbf{G}$ where $\mathbf{G}_{jk} = e_{k \leftarrow j}$. Let $\phi_k^l$ be the logits and $\phi_k^{l-1}$ be the representation for modality $k$, where $l$ indicates the number of layers in the network. Given an example $x_i$, the graph is learned by:
{\small
\begin{align}
z_i^{(k)}(x_i) &= W_{11} \phi_k^{l-1}(x_i^{(k)}) + W_{12} \phi_k^{l}(x_i^{(k)}), \\
\mathbf{G}_{jk}(x_i) &= e_{k \leftarrow j} = W_{21} [z_i^{(j)}(x_i) \|  z_i^{(k)}(x_i)]
\label{eq:graph_learning}
\end{align}}where $W_{11}$,  $W_{12}$ and $W_{21}$ are parameters to learn and $\cdot \| \cdot$ indicates the vector concatenation. $W_{21}$ maps a pair of inputs to an entry in $\mathbf{G}$. The entire graph is learned by repetitively applying Eq.~\eqref{eq:graph_learning} over all pairs of modalities in $\mathcal{S}$.

As a distillation graph is expected to be sparse, we normalize $\mathbf{G}$ such that the nonzero weights are dispersed over a small number of vertices. Let $\mathbf{G}_{j:} \in \mathbb{R}^{1 \times |\mathcal{S}|}$ be the vector of its $j$-th row. The graph is normalized:
{\small
\begin{equation}
\label{eq:graph_learning_softmax}
\mathbf{G}_{j:}(x_i) = \sigma(\alpha [\mathbf{G}_{j1}(x_i), ..., \mathbf{G}_{j|\mathcal{S}|}(x_i)]),
\end{equation}}where $\alpha$ is used to scale the input to the softmax operator. 

The message passing on distillation graph can be conveniently implemented by attaching a new layer to the original network. As shown in Fig.~\ref{fig:modela}, each vertex represents a modality and the messages are propagated on the graph layer. In the forward pass, we learn a $\mathbf{G} \in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{S}|}$ by Eq.~\eqref{eq:graph_learning} and~\eqref{eq:graph_learning_softmax} and compute the message matrix $\mathbf{M} \in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{S}|}$ by Eq.~\eqref{eq:message_ab} such that $\mathbf{M}_{jk}(x_i)=m_{k \leftarrow j}(x_i)$. The imitation loss to all modalities is calculated by:
{\small
\begin{equation}
\label{eq:message_graph}
\ell_m = (\mathbf{G}(x_i) \odot \mathbf{M}(x_i))^T \mathbf{1},
\end{equation}}where $\mathbf{1} \in \mathbb{R}^{|\mathcal{S}| \times 1}$ is a column vector of ones; $\odot$ is the element-wise product between two matrices; $\mathbf{\ell_m} \in \mathbb{R}^{|\mathcal{S}| \times 1}$ contains imitation loss for every modality in $\mathcal{S}$. In the backward propagation, the imitation loss $\ell_m$ is incorporated in Eq.~\eqref{eq:distilation_loss} to compute the gradient of the total training loss. This graph distillation layer is end-to-end trained with the rest of the network. As shown, the distillation graph is an important and essential structure which not only provides a base for learning dynamic message passing through modalities but also models the distillation as a few matrix operations which can be conveniently implemented as a new layer in the network.

For a modality, its performance on the cross-validation set often turns out to be a reasonable estimator to its contribution in distillation. Therefore, we add a constant bias term $\mathbf{c}$ in Eq.~\eqref{eq:graph_learning_softmax}, where $\mathbf{c} \in \mathbb{R}^{|\mathcal{S}| \times 1}$ and $c_j$ is set w.r.t. the cross-validation performance of the modality $j$ and $\sum_{k=1}^{|\mathcal{S}|} c_k = 1$. Therefore, Eq.~\eqref{eq:message_graph} can be rewritten as:
{\small
\begin{align}
\label{eq:message_graph_final}
\ell_m 
&= ((\mathbf{G}(x_i)+ \mathbf{1} \mathbf{c}^T)\odot\mathbf{M}(x_i))^T \mathbf{1} \\
&= (\mathbf{G}(x_i)\odot\mathbf{M}(x_i))^T\mathbf{1}+(\mathbf{G}_{prior}\odot\mathbf{M}(x_i))^T\mathbf{1}
\end{align}}where $\mathbf{G}_{prior} = \mathbf{1} \mathbf{c}^T$ is a constant matrix. Interestingly, by adding a bias term in Eq.~\eqref{eq:graph_learning_softmax}, we decompose the distillation graph into two graphs: a learned example-specific graph $\mathbf{G}$ and a prior modality-specific graph $\mathbf{G}_{prior}$ that is independent to specific examples. The messages are propagated on both graphs and the sum of the message is used to compute the total imitation loss. There exists a physical interpretation of the learning process. Our model learns a graph based on the likelihood of observed examples to exploit complementary information in $\mathcal{S}$. Meanwhile, it imposes a prior to encouraging accurate modalities to provide more contribution. By adding a constant bias, we use a more computationally efficient approach than actually performing message passing on two graphs. 

So far, we have only discussed the distillation on the source domain. In practice, our method may also be applied to the target domain on which privileged modality is available. In this case, we apply the same method to minimize Eq.~\eqref{eq:distilation_loss} on the target training data. As illustrated in Fig.~\ref{fig:modelb}, a graph distillation layer is added during the training of the target model. At the test time, as shown in Fig.~\ref{fig:modelc}, only a single modality is used.