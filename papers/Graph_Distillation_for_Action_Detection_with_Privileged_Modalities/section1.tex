Recent advancements in deep convolutional neural networks (CNN) have been successful in various vision tasks such as image recognition \cite{imagenet,resnet,alexnet} and object detection \cite{fast_rcnn,yolo,faster_rcnn}. A notable bottleneck for deep learning, when applied to multimodal videos, is the lack of massive, clean, and task-specific annotations, as collecting annotations for videos is much more time-consuming and expensive. Furthermore, restrictions such as privacy or runtime may limit the access to only a subset of the video modalities during test time.

The scarcity of training data and modalities is encountered in many real-world applications including self-driving cars, surveillance, and health care. A representative example is activity understanding on health care data that contain Personally Identifiable Information (PII)~\cite{hand_hygiene,senior_home}. On the one hand, the number of labeled videos is usually limited because either important events such as falls~\cite{fall_detection_principles,fall_detection_survey} are extremely rare or the annotation process requires a high level of medical expertise. On the other hand, RGB violates individual privacy and optical flow requires non-real-time computations, both of which are known to be important for activity understanding but are often unavailable at test time. Therefore, detection can only be performed on real-time and privacy-preserving modalities such as depth or thermal videos.

Inspired by these problems, we study action detection in the setting of limited training data and partially observed modalities. To do so, we make use of a large action classification dataset that contains various \emph{heterogeneous} modalities as the source domain to assist the training of the action detection model in the target domain, as illustrated in Fig.~\ref{fig:pull}. Following the standard assumption in transfer learning~\cite{yosinski2014transferable}, we assume that the source and target domain are similar to each other. We define a modality as a privileged modality if (1) it is available in the source domain but not in the target domain; (2) it is available during training but not during testing. 

We identify two technical challenges in this problem. First of all, due to modality discrepancy in types and quantities, traditional domain adaption or transfer learning methods~\cite{subspace_alignment,transfer} cannot be directly applied. Recent work on knowledge and cross-modal distillation~\cite{distillation_hinton,li2017learning,unifying,privileged_on_depth_shi} provides a promising way of transferring knowledge between two models. Given two models, we can specify the distillation as the direction from the strong model to the weak model. With some adaptations, these methods can be used to distill knowledge between modalities. However, these adapted methods fail to address the second challenge: how to leverage the privileged modalities effectively. More specifically, given multiple privileged modalities, the distillation directions and weights are difficult to be pre-specified. Instead, the model should learn to dynamically adjust the distillation based on different actions or examples. 
For instance, some actions are easier to detect by optical flow whereas others are easier by skeleton features, and therefore the model should adjust its training accordingly. However, this dynamic distillation paradigm has not yet been explored by existing methods.

To this end, we propose the novel \emph{graph distillation} method to learn a dynamic distillation across multiple modalities for action detection in multimodal videos. The graph distillation is designed as a layer attachable to the original model and is end-to-end learnable with the rest of the network. The graph can dynamically learn the example-specific distillation to better utilize the complementary information in multimodal data. As illustrated in Fig.~\ref{fig:pull}, by effectively leveraging the privileged modalities from both the source domain and the training stage of the target domain, graph distillation significantly improves the test-time performance on a single modality. Note that graph distillation can be applied to both single-domain (from training to testing) and cross-domain (from one task to another) tasks. For our cross-domain experiment (from action classification to detection), we utilized the most basic transfer learning approach, \textit{i.e.} pre-train and fine-tune, as this is orthogonal to our contributions. We can potentially achieve even better results with advanced transfer learning and domain adaptation techniques and we leave it for future study. 

We validate our method on two public multimodal video benchmarks: PKU-MMD~\cite{pku_mmd} and NTU RGB+D~\cite{ntu_rgbd}. The datasets represent one of the largest public multimodal video benchmarks for action detection and classification. The experimental results show that our method outperforms the state-of-the-art approaches. Notably, it improves the state-of-the-art by 9.0\% on PKU-MMD~\cite{pku_mmd} (at 0.5 tIoU threshold) and by 6.6\% on NTU RGB+D~\cite{ntu_rgbd}. The remarkable improvement on the two benchmarks is a convincing validation of our method. 

To summarize, our contribution is threefold. 
(1) We study a realistic and challenging condition for multimodal action detection with limited training data and modalities. To the best of our knowledge, we are first to effectively transfer multimodal privileged information across domains for action detection and classification.
(2) We propose the novel graph distillation layer that can dynamically learn to distill knowledge across multiple privileged modalities and can be attached to existing models and learned in an end-to-end manner.
(3) Our method outperforms the state-of-the-art by a large margin on two popular benchmarks, including action classification task on the challenging NTU RGB+D~\cite{ntu_rgbd} and action detection task on PKU-MMD~\cite{pku_mmd}.
