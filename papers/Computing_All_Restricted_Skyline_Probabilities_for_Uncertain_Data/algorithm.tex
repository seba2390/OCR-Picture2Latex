\section{Algorithms for ARSP Problem}\label{sec:rskyprobalg}

The linear scoring function is one of the most commonly used scoring functions~\cite{DBLP:journals/ior/DyerS79}.
Given a weight (preference) $\omega$, the \textit{score} of tuple $t$ is defined as $S_\omega(t) = \sum^d_{i = 1} \omega[i]t[i]$.
Since ordering two tuples by score is independent from the magnitude of $\omega$, we assume that $\omega$ belongs to the unit $(d-1)$-simplex $\simplex^{d-1}$, called \textit{preference domain}, \ie, $\sum^d_{i = 1} \omega[i] = 1$.
To serve the specific preferences of an individual user, a notable approach is to add some constraints on the preference domain.
Notationally, let the matrix inequality $A \times \omega \le b$ be a set of linear constraints on $\simplex^{d-1}$ and $c$ be the number of rows of $A$.
In this section, we propose two algorithms for ARSP problem in case of $\calF$ is a set of linear scoring functions whose weight are described by a set of linear constraints.

\subsection{Baseline Algorithm}\label{subsec:bsl}

Given an uncertain dataset $\calD$ and a set of linear scoring functions $\calF = \{S_\omega(\cdot) \mid \omega \in \simplex^{d-1} \wedge A \times \omega \le b\}$, a straight method to calculate rskyline probability for each instance $t$ is to compute the product of probabilities that none of instances from other objects that $\calF$-dominate $t$ exist by performing $\calF$-dominance tests against other instances.
With the fact that the \textit{preference region} $\Omega = \{\omega \in \simplex^{d-1} \mid A \times \omega \le b\}$ is a \textit{closed convex polytope}, the $\calF$-dominance relation between two instances can be determined by comparing their scores under the set of vertices $V$ of $\Omega$, where a weight $\omega$ is called a vertex of $\Omega$ if and only if it is the unique solution to a $d$-subset inequalities of $A \times \omega \le b$.

\begin{theorem}[$\calF$-dominance test~\cite{DBLP:journals/pvldb/CiacciaM17}]
	Given a set of linear scoring functions $\calF = \{S_\omega(\cdot) \mid \omega \in \simplex^{d-1} \wedge A \times \omega \le b\}$, let $V$ be the set of vertices of the preference region $\Omega = \{\omega \in \simplex^{d-1} \mid A \times \omega \le b\}$, an instance $t$ $\calF$-dominates another instance $s$ if and only if $S_\omega(t) \le S_\omega(s)$ holds for all weights $\omega \in V$.
	\label{thm:F-dominace-V}
\end{theorem}

With the above theorem, we construct a baseline algorithm as follows.
Since the preference region $\Omega$ is guaranteed to be closed, the set of linear constraints can be transformed into a set of points using the \textit{polar duality}~\cite{preparata2012computational} such that the intersection of the linear constraints is the dual of the convex hull of the points.
After the transformation, the baseline invokes the quickhull algorithm proposed in~\cite{DBLP:journals/toms/BarberDH96} to compute the set of vertices $V$ of $\Omega$.
Then it sorts the set of instances using a scoring function $S_\omega$ for some $\omega \in V$.
This guarantees that if an instance $t$ precedes another instance $s$ in the sorted set, then $s \nprec_\calF t$.
After that, for each instance $t$, the baseline tests $t$ against every instance of other objects preceding $t$ to compute $\Pr_{\rm rsky}(t)$ according to Equation~\ref{eq:rskyprob-def}.
Since $V$ can be computed in $O(c^2)$ time~\cite{greenfield1990proof}, where $c$ is the number of linear constraints, and after that each $\calF$-dominance test can be accomplished in $O(dd')$ time, where $d' = |V|$ the time complexity of the baseline algorithm is $O(c^2 + dd'n^2)$.
Although the theoretical upper bound of $d'$ is $\Theta(c^{\lfloor d/2 \rfloor})$~\cite{henk2017basic}, the actual size of $V$ is experimentally observed to be small.
Hence we conclude that the time complexity of the baseline algorithm is $O(n^2)$.	


\subsection{Tree-Traversal Algorithm}\label{subsec:tt}

A major challenge for the ARSP problem is the irregularity of the $\calF$-dominance region.
In this subsection, we overcome this challenge by reducing the ARSP problem to an ASP problem~\cite{DBLP:journals/mst/AfshaniAALP13} in a higher dimensional data space.
Afterwards, calling the state-of-the-art method~\cite{DBLP:journals/mst/AfshaniAALP13} for the ASP problem yields an algorithm with near-optimal time complexity.

\begin{figure}[!t]
    \removelatexerror
    \begin{algorithm}[H]
	\caption{KDTree-Traversal Algorithm}
	\label{alg:trans-alg}
	\KwIn{an uncertain dataset $\calD$, a set of linear scoring functions $\calF = \{S_\omega(\cdot) \mid \omega \in \simplex^{d-1} \wedge A \times \omega \le b\}$}
	\KwOut{the rskyline probabilities of all instances}\BlankLine
	Compute vertices $V$ of $\Omega = \{\omega \in \simplex^{d-1} \mid A \times \omega \le b\}$\;
	Map $I$ into the score space $S_{_V}(I)$\;
	${\rm ARSP} \gets \emptyset$; $\chi \gets 0$; $\beta \gets 1$\;
	\lForEach{$i \gets 1$ \emph{\bf to} $m$}{$\sigma[i] \gets 0$}
	\skyprob{$S_{_V}(I), S_{_V}(I)$}\;
	\Return{${\rm ARSP}$}\;\BlankLine
	\SetKwProg{proc}{Procedure}{}{}
	\proc{\skyprob{$P$,  $C$}}{
		$C_{par} \gets C$; $C \gets \emptyset$; $D \gets \emptyset$\;
		\ForEach{$S_{_V}(t) \in C_{par}$}{
			\If{$S_{_V}(t) \preceq P_{\min}$ \emph{(say $t \in T_i$)}}{
				Insert $S_{_V}(t)$ into $D$\;
				$\sigma[i] \gets \sigma[i] + \Pr(t)$\;
				\If{$\sigma[i] = 1$}{
					$\chi \gets \chi + 1$;
					$\beta \gets \beta/\Pr(t)$\;
				}
				\Else{
					$\beta \gets \beta \times (1 - \sigma[i]) / (1 - \sigma[i] + \Pr(t))$\;
				}
			}
			\ElseIf{$S_{_V}(t) \preceq P_{\max}$}{
				Insert $S_{_V}(t)$ into $C$\;
			}
		}
		\If{$\chi = 0$ \emph{\bf and} $|P| = 1$} {
            \tcp{\small suppose $P = \{S_{_V}(t)\}$ and $t \in T_i$}
			Insert $(t, \beta\times \Pr(t)/(1 - \sigma[i]))$ into ${\rm ARSP}$\;
		}\ElseIf{$\chi = 0$ \emph{\bf and} $|P| > 1$}{
			Partition $P$ into $P_l$ and $P_r$ with selected $axis$\;
			%			Compute MBR $R_l$ (\resp, $R_r$) of $I_l$ (\resp, $I_r$)\;
			\skyprob{$P_l$, $C$}\;
			\skyprob{$P_r$, $C$}\;
		}
		\ForEach{$t \in D$}{
			Undo the changes to restore $\sigma[\cdot]$, $\chi$, $\beta$\;
		}
		$C \gets C_{par}$\;
	}
\end{algorithm}
\end{figure}

\noindent{\bf Problem: All Skyline Probabilities (ASP) Problem}~\cite{DBLP:journals/mst/AfshaniAALP13}\\
\noindent{\bf Input:} an uncertain dataset $\calD$. \\
\noindent{\bf Output:} skyline probability $\Pr_{\rm sky}(t)$ for each instance $t \in I$, where supposing $t$ belongs to $T_i$
\[{\rm Pr}_{\rm sky}(t) = \Pr(t) \cdot \prod^m_{j = 1, j \ne i}(1 - \sum_{s \in T_j, s \prec t} \Pr(s)).\]


Given a set of linear scoring functions $\calF = \{S_\omega(\cdot) \mid \omega \in \simplex^{d-1} \wedge A \times \omega \le b\}$, let $V = \{\omega_1, \cdots, \omega_{d'}\}$ denote the set of vertices of the preference region $\Omega = \{\omega \in \simplex^{d-1} \mid A \times \omega \le b\}$ and $d' = |V|$.
And let $S_{_V}(t) = (S_{\omega_1}(t), \cdots, S_{\omega_{d'}}(t))$ denote the score vector of tuple $t$ under all vertices in $V$.
The reduction maps each instance $t \in I$ into an instance $S_{_V}(t)$ with the same existence probability, and groups them according to the objects they belong to.
In other words, the reduction constructs an uncertain dataset $S_{_V}(I)$ in the $d'$-dimensional \textit{score space}, where $S_{_V}(I) = \{S_{_V}(T_i) = \{S_{_V}(t) \mid t \in T_i\} \mid T_i \in \calD\}$ and for each instance $S_{_V}(t) \in S_{_V}(I)$, $\Pr(S_{_V}(t)) = \Pr(t)$.
According to Theorem~\ref{thm:F-dominace-V}, an instance $t$ $\calF$-dominates another instance $s \ne t$ if and only if $S_{_V}(t)$ dominates $S_{_V}(s)$, where a tuple $t$ dominates another tuple $s \ne t$, denoted as $t \prec s$, if $\forall i \in [1, d], t[i] \le s[i]$.
This means the $\calF$-dominance relation between any two instances $t$ and $s$ in the original space is equivalent to the dominance relationship between $S_{_V}(t)$ and $S_{_V}(s)$ in the mapped \textit{score space}.
Hence the rskyline probability $\Pr_{\rm rsky}(t)$ of each instance $t \in I$ in the original space equals to the skyline probability $\Pr_{\rm sky}(S_{_V}(t))$ of the corresponding instance $S_{_V}(t)$ in the score space.

Thus, after the reduction, we call a procedure \skyprob to compute the skyline probability for all instances in $S_{_V}(I)$.
This procedure is implemented based on the state-of-the-art $kd$-tree traversal algorithm for the ASP problem proposed in~\cite{DBLP:journals/mst/AfshaniAALP13}.
We introduce some implementation optimizations which does not improve the time complexity but indeed enhance its experimental performance.
The original algorithm first constructs a $kd$-tree $T$ on $S_{_V}(I)$, and then progressively computes skyline probabilities of all instances by performing a preorder traversal of $T$.
In our implementation, we integrate the preorder traversal into the construction of $T$ and also prune the construction of a subtree if all instances included in the subtree have zero rskyline probability.

Concretely, \skyprob always keeps a path from the root of $T$ to the current reached node in the main memory.
And for each node $N$ in the path, let $P$ be the set of instances contained in $N$ and $P_{\min}$ ($P_{\max}$) denote the minimum (maximum) corner of the minimum bounding rectangle of $P$, \skyprob maintains the following information,
1) a set $C$ including instances that dominates $P_{\max}$,
2) an array $\sigma = \langle \sigma[1], \cdots \sigma[m] \rangle$, where $\sigma[i] = \sum_{t \in T_i, S_{_V}(t) \prec P_{\min}} \Pr(t)$ is the sum of probabilities over all instances of $S_{_V}(T_i)$ that dominates $P_{\min}$,
3) a value $\beta = \prod_{1 \le i \le m, \sigma[i] \ne 1}(1 - \sigma[i])$,
and 4) a counter $\chi = |\{i \mid \sigma[i] = 1\}|$.
It is easy to see that for the root node of $T$, $C = S_{_V}(I)$, $\sigma[i] = 0$ for $1 \le i \le m$, $\beta = 1$, and $\chi = 0$.

Now, assuming the information of all nodes in the maintained path is available, \skyprob constructs the next arriving node $N$ as follows.
Again, let $P$ denote the set of instances in $N$.
For each point $S_{_V}(t) \in C_{par}$, where $C_{par}$ is the set $C$ of the parent node of $N$, it tests $S_{_V}(t)$ against $P_{\min}$.
If $S_{_V}(t) \prec P_{\min}$, say $t \in T_i$, it updates $\sigma[i]$, $\beta$, and $\chi$ accordingly (lines 13-16 in Algorithm~\ref{alg:trans-alg}).
Otherwise, it further tests $S_{_V}(t)$ against $P_{\max}$ and inserts $S_{_V}(t)$ into the set $C$ of $N$ if $S_{_V}(t) \prec P_{\max}$.
When $\chi$ becomes to one, it is known that $\Pr_{\rm sky}(P_{\min}) = 0$, and so are all instances in $N$ due to the transitivity of dominance relation.
Therefore, \skyprob prunes the construction of the subtree rooted at $N$ and returns to its parent node.
Otherwise, \skyprob keeps growing the path (partitioning set $P$ like a $kd$-tree) until it reaches a node including only one instance $S_{_V}(t)$ and then computes $\Pr_{\rm rsky}(t) = \Pr_{\rm sky}(S_{_V}(t))$ based on $\beta$ and $\sigma[i]$.

\begin{figure}[t]
	\subfigure[$kd$-tree for $S_{_V}(I)$.]{
		\includegraphics[width=.42\linewidth]{figures/dataspace.png}
		\label{fig:data-space}
	}
    \hfill
	\subfigure[Pruning at node $R_3$.]{
		\includegraphics[width=.5\linewidth]{figures/kdtree.png}
		\label{fig:kdtree}
	}
	\caption{Running example for Algorithm~\ref{alg:trans-alg} and implementation optimizations.}
	\label{fig:alg-trans}
\end{figure}

%TODO: running example of algorihtm 
\begin{example}
	As shown in Fig.~\ref{fig:alg-trans}, suppose all instances of an object occur with the same probability.
    The original algorithm keeps a whole $kd$-tree in the main memory but \skyprob only maintains a path from the root node, \eg, $R_1 \to R_2 \to R_5$.
	Moreover, when \skyprob traverses from $R_1$ to $R_3$, it updates $\sigma[2]$ to $1$ and $\chi$ to $1$ since $t_{2,1} \prec R_3$.
	This indicates that the skyline probability of all instances in the subtree rooted at $R_3$ is zero, thus \skyprob prunes the construction of the subtree rooted at $R_3$ as shown in Fig.~\ref{fig:kdtree}.
\end{example}

%TODO: time complexity analyze
The pseudocode of the derived algorithm is presented in Algorithm~\ref{alg:trans-alg}.
As stated previously, the computation of $V$ takes $O(c^2)$ time, where $c$ is the number of linear constraints.
The score vector of an instance can be derived in $O(dd')$ time, where $d' = |V|$.
And given a set of $n$ instances in $d'$-dimensional data space, the time complexity of \skyprob is $O(n^{2-1/d'})$~\cite{DBLP:journals/mst/AfshaniAALP13}.
Therefore, the overall time complexity of Algorithm~\ref{alg:trans-alg} is $O(c^2 + d'dn + n^{2-1/d'}) = O(n^{2-1/d'})$.

Next, we claim that Theorem~\ref{thm:lower-bound} still holds even if we limit $\calF$ into linear scoring functions whose weights are described by a set of linear constraints, which proves that Algorithm~\ref{alg:trans-alg} achieves a near-optimal time complexity.
%TODO: optimaility
Let $\calF$ be the set of all linear scoring functions.
Given two instances $t$ and $s$, if $t \prec_\calF s$, then $t[i] \le s[i]$ for $1 \le i \le d$ since $\omega_i \in \Omega$ where $\omega_i[i] = 1$ and $\omega_i[j] = 0$ for all $1 \le j \ne i \le d$.
If $t[i] \le s[i]$ for $1 \le i \le d$, it is known that $t \prec_\calF s$ since all linear scoring functions are monotone.
Hence, we can also conclude that $t \prec_\calF s$ if and only if $t[i] \le s[i]$ for $1 \le i \le d$.
Thus, with the same reduction established in the proof of Theorem~\ref{thm:lower-bound}, it is known that there is no subquadratic-time algorithm for ARSP problem even if $\calF$ is limited into linear scoring functions whose weights are described by a set of linear constraints.

%TODO: replace kd-tree with other space partition tree also works for this method
\noindent{\bf Remark.}
Algorithm~\ref{alg:trans-alg} is also correct if \skyprob adopts any other space-partitioning tree.
The only details that need to be modified are the method to partition the data space (line 19-21 in Algorithm~\ref{alg:trans-alg}).
In our experimental study, we implement a variant of Algorithm~\ref{alg:trans-alg} based on the quadtree, which partitions the data space in all dimensions each time.
It is observed that choosing an appropriate space-partitioning tree can improve the performance of Algorithm~\ref{alg:trans-alg}, \eg, the quadtree-based implementation works well in low-dimensional data spaces, while the $kd$-tree-based implementation have better scalability for data dimensions.


\subsection{Branch-and-Bound Algorithm}\label{subsec:bb}

A drawback of Algorithm~\ref{alg:trans-alg} is that it needs to map all instances into the score space in advance each time, in this subsection, we show how to conduct the mapping on the fly so that unnecessary computations can be avoided.

%TODO: transform on the fly and pruning rule
Recall that if instances in $I$ are sorted in ascending order according to their scores under a scoring function $f \in \calF$, then instance $t$ will not be $\calF$-dominated by any instance $s$ after $t$.
Supposing instances are processed in the sorted order, the score vector $S_{_V}(t)$ is unnecessary until $t$ is to be processed.
With this observation, we design efficient pruning strategies to tell whether an instance or a set of instances can be safely ignored during the computation, and if so, their mappings can be avoided.
Unlike conducting probabilistic rskyline analysis under top-$k$ or threshold semantics, it is easy to see that maintaining upper and lower bounds on each instance's rskyline probability as pruning criteria is helpless since our goal is to compute exact rskyline probabilities of all instances.
Thus, the only pruning strategy can be utilized is that if an instance $t$ is $\calF$-dominated by another instance $s$ and $\Pr_{\rm rsky}(s)$ is zero, then $\Pr_{\rm rsky}(t)$ is also zero due to the transitivity of $\calF$-dominance.
A straightforward method for efficiently performing this pruning strategy is to keep a rskyline of all instances processed so far whose rskyline probability is zero and compare the next instance to be processed against all instances in the rskyline beforehand.
However, the maintained rskyline may suffer from huge scale on anti-correlated datasets.
In the following theorems, we prove that a set $P$ of size at most $m$ is sufficient for pruning tests and all instances with zero rskyline probability can be safely ignored without affecting subsequent rskyline probabilities computation.

\begin{theorem}
	All instances with zero rskyline probability can be safely discarded.
\end{theorem}

\begin{IEEEproof}
	Let $t \in T_i$ be an instance with $\Pr_{\rm rsky}(t) = 0$.
	Recall the formulation of rskyline probability in Equation~\ref{eq:rskyprob-def}, all other instances of $T_i$ will not be affected by $t$.
	This also holds for instances of other objects $T_j$ that are not $\calF$-dominated by $t$.
	Now, suppose $s$ is an instance of $T_{j \ne i}$ and $s$ is $\calF$-dominated by $t$.
	Since $t \prec_\calF s$ and $\Pr_{\rm rsky}(t) = 0$, it is easy to see that there exists a set of objects $\mathcal{T} = \{T_k \mid k \ne j \wedge k \ne i\}$ such that all instances of each object $T_k \in \mathcal{T}$ $\calF$-dominate $t$.
	Moreover, because $\calF$-dominance is asymmetric, it is known that there exists at least one object $T_k \in \mathcal{T}$, all instances of which have non-zero rskyline probability.
	Therefore, according to the transitivity of $\calF$-dominance, $s$ is also $\calF$-dominated by all instances of $T_k$ and thus $\Pr_{\rm rsky}(s) = 0$.
\end{IEEEproof}

\begin{theorem}
	Let $V = \{\omega_1, \cdots, \omega_{d'}\}$ be the set of vertices of the preference region $\Omega = \{\omega \in \simplex^{d-1} \mid A \times \omega \le b\}$, there is a set $P$ such that for any instance $t$, ${\rm Pr}_{\rm rsky}(t) = 0$ if and only if $S_{_V}(t)$ is dominated by some instance $p \in P$ and $|P| \le m$.
\end{theorem}

\begin{IEEEproof}
	We start with the construction of the pruning set $P$.
	For each object $T_i$ with $\sum_{t \in T_i} \Pr(t) = 1$, we insert an instance $p_i = (\max_{t \in T_i}S_{\omega_1}(t), \cdots, \max_{t \in T_i}S_{\omega_{d'}}(t))$ into $P$.
	Note that the above construction also requires to map all instances into the score space in advance in order to facilitate the understanding of the proof.
	However, in the proposed algorithm, we construct $P$ incrementally during the computation.
	It is straight to verify that $|P| \le m$ from the construction of $P$.
	Then, let $t$ denote an instance of object $T_i$, we prove that $\Pr_{\rm rsky}(t) = 0$ if and only if $S_{_V}(t)$ is dominated by some $p_{j \ne i} \in P$.
	From Equation~\ref{eq:rskyprob-def}, it is easy to see that $\Pr_{\rm rsky}(t) = 0$ if and only if there must exist an object $T_{j \ne i}$ such that every instance $s \in T_j$ $\calF$-dominates $t$ and $\sum_{s \in T_j} \Pr(s) = 1$.
	That is $S_{_V}(s) \prec S_{_V}(t)$ holds for all instances $s \in T_j$ according to Theorem~\ref{thm:F-dominace-V}.
	Moreover, since a set of instances dominates another instance if and only if the maximum corner of their minimum bounding rectangle dominates that instance, it is derived that $\Pr_{\rm rsky}(t) = 0$ if and only if $p_j = (\max_{s \in T_j}S_{\omega_1}(s), \cdots, \max_{s \in T_j}S_{\omega_{d'}}(s)) \prec t$.
	Based on the construction of $P$, it is known that all $p_j$ are included in $P$, thus completing the proof.
\end{IEEEproof}

\begin{figure}[!t]
    \removelatexerror 
    \begin{algorithm}[H]
    	\caption{Branch-and-Bound Algorithm}
    	\label{alg:bbs-alg}
    	\KwIn{an uncertain dataset $\calD$, a set of linear scoring functions $\calF = \{S_\omega(\cdot) \mid \omega \in \simplex^{d-1} \wedge A \times \omega \le b\}$}
    	\KwOut{the rskyline probabilities of all instances}\BlankLine
    	Compute vertices $V$ of $\Omega = \{\omega \in \simplex^{d-1} \mid A \times \omega \le b\}$\;
    	Initialize a min-heap $H$ with respect to $S_{\omega}(\cdot)$ and $m$ $d'$-dimensional aggregated R-trees $R_1, \cdots, R_m$\;
    	$P \gets \emptyset$; ${\rm ARSP} \gets \emptyset$\;
    	Insert the root of R-tree on $\calD$ into $H$\;
    	\While{$H$ is not empty}{
    		Let $N$ be the top node in $H$\;
    		\If{$N$ is not pruned by $P$}{
    			\If{$N = \{t\}$ is a leaf node \emph{(say $t \in T_i$)}}{
    %				$t' \gets (S_{\omega_1}(t), \cdots, S_{\omega_{d'}}(t))$\;
    				$S_{_V}(t) \gets$ compute $t$'s score vector under $V$\;
    				$\Pr_{\rm rsky}(t) \gets \Pr(t)$\;
    				\ForEach{aggregated R-tree $R_{j \ne i}$}{
    					$\sigma[j] \gets $ perform window query with the orign and $S_{_V}(t)$ on $R_j$\;
    					$\Pr_{\rm rsky}(t) \gets \Pr_{\rm rsky}(t) \times (1 - \sigma[j])$\;
    				}
    				Insert $S_{_V}(t)$ into $R_i$\;
                    Insert $(t, \Pr_{\rm rsky}(t))$ into ${\rm ARSP}$\;
    				$cnt[i] \gets cnt[i] + 1$\;
    				\ForEach{$j \gets 1$ \emph{\bf to} $|V|$}{
    					$p_i[j] \gets \max(p_i[j], S_{_V}(t)[j])$\;
    				}
    				\If{$cnt[i] = |T_i|$}{
    					Insert $p_i$ into $P$\;
    				}
    			}
    			\Else{
    				\ForEach{child node $N'$ of $N$}{
    					%\lIf{$e_i$ is not $\F$-dominated by any instance in $SP_0$}{
    					\If{$N'$ is not pruned by $P$}{
    						Insert $N'$ into $H$\;
    					}
    				}
    			}
    		}
    	}
    	\Return{${\rm ARSP}$}\;
    \end{algorithm}
\end{figure}




%TODO: algorithm description
Now, we integrate the above strategies into the proposed algorithm and the pseudocode is shown in Algorithm~\ref{alg:bbs-alg}.
%Without loss of generality, it is assumed that all instances in $I$ are organized into an R-tree $R$ in advance.
The algorithm first computes the set of vertices $V$ of the preference region $\Omega$ and initializes $m$ aggregated R-trees $R_1, \cdots, R_m$, where $R_i$ is used to incrementally index $S_{_V}(t)$ for all instances $t \in T_i$ with $\Pr_{rs}(t) > 0$ that have been processed by the algorithm.
After that, the algorithm traverses the index $R$ in a \textit{best-first} manner.
Specifically, it first inserts the root of R-tree on $\calD$ into a \textit{minimum heap} $H$ sorted according to its score under some $S_{\omega \in V}(\cdot)$, where the score of a node $N$ is defined as $S_\omega(N_{\min})$.
Then, at each time, it handles the top node $N$ popped from $H$.
If $S_{_V}(N_{\min})$ is dominated by some point in $P$, then the algorithm ignores all instances in $N$ since their rskyline probabilities are zero due to the transitivity of $\calF$-dominance.
Otherwise, if $N$ is a leaf node, say $t \in T_i$ is contained in $N$, the algorithm computes $S_{_V}(t)$ and issues the window query with the origin and $S_{_V}(t)$ on each aggregated R-tree $R_{j \ne i}$ to compute $\sigma[j] = \sum_{s \in T_j, s \prec_\calF t} \Pr(s)$ and inserts $S_{_V}(t)$ into $R_i$.
Then it updates $p_i$, which records the maximum corner of the minimum bounding rectangle of $S_{_V}(t)$ for all instances $t \in T_i$ with $\Pr_{\rm rsky}(t) > 0$ that have been processed so far, and inserts $p_i$ into $P$ if all instances in $T_i$ have non-zero rskyline probability.
Or if $N$ is an internal node, it inserts all non-pruned child nodes of $N$ into $H$ for further computation. 

With the fact that Algorithm~\ref{alg:bbs-alg} only visits the nodes which contain instances $t$ with $\Pr_{\rm rsky}(t) > 0$ and never access the same node twice, it is easy to prove that the number of nodes accessed by Algorithm~\ref{alg:bbs-alg} is optimal to derive the final result for ARSP problem.
And since $m-1$ orthogonal range queries are performed on aggregated R-trees for each instance in $I$, the expected time complexity of Algorithm~\ref{alg:bbs-alg} is $O(nm\log{n})$.
%Moreover, instead of the preference region described by linear constraints, Algorithm~\ref{alg:bbs-alg} can also be extended to cope with other kinds of descriptions, \eg, \textit{$\rho$-dominance} studied in~\cite{DBLP:conf/sigmod/MouratidisL021}.
%The only details that need to be modified are the method for dominance test and the method for finding an inner point within the preference region.


\section{Sublinear-time Algorithm\\for Ratio Bound Constraints}\label{sec:eclprobalg}

In this section, we focus on a special linear constraints which consist of $d - 1$ ratio bound constraints of the form $l_i \le \omega[i]/\omega[d] \le h_i$ for $1 \le i < d$.
For brevity of notation, we use $R = \prod^{d-1}_{i = 1}[l_i, h_i]$ to denote the set of ratio bound constraints in what follows.
In~\cite{DBLP:conf/icde/Liu0ZP021}, Liu~\etal have investigated this special case of $\calF$-dominance on certain datasets, named as \textit{eclipse-dominance}, and defined the \textit{eclipse} query as retrieving the set of all non-\textit{eclipse-dominated} tuples.
We refer the readers to their paper for wide applications of this query.
Although we focus on uncertain datasets in this section, our methods can also be used to design improved algorithms for eclipse query processing.

\subsection{Reduction to Range Searching Problem}

Given a set of ratio bound constraints $R = \prod^{d-1}_{i = 1}[l_i, h_i]$, the $\calF$-dominance test condition stated in Theorem~\ref{thm:F-dominace-V} can be equivalently represented as determining whether the following linear programming problem has a non-negative solution,
\begin{equation}
	\begin{aligned}
		\text{minimize} \quad & S_\omega(s) - S_\omega(t) = \sum^d_{i = 1} (s[i] - t[i]) \times \omega[i] \\
		\text{subject to} \quad & l_i \le \omega[i]/\omega[d] \le h_i \qquad i \in \{1, \cdots, d - 1\} \\
        & \sum^d_{i = 1} \omega[i] = 1 \\ 
	\end{aligned}
	\label{eq:lp}
\end{equation}
The crucial observation is that the sign of the minimum value of the above linear programming problem can be determined more efficiently.
Specifically, since $\omega[d] > 0$, transforming the object function $\sum^d_{i = 1}(t[i] - s[i]) \times \omega[i]$ into $\sum^{d-1}_{i = 1}(t[i] -st[i]) \times \omega[i] / \omega[d] + (s[d] - t[d])$ does not affect the sign of the minimum value.
After that, we can choose each coordinate of the new unknowns $r[i] = \omega[i] / \omega[d]$ for $1 \le i < d$ independently in the corresponding interval $[l_i, h_i]$.
Thus, the minimum value of the new object function can be directly obtained in $O(d)$ time, so is the sign of original minimum.

\begin{theorem}[Efficient $\calF$-dominance test]
    Let $\calF$ be a set of linear scoring functions whose weights are described by ratio bound constraints $R = \prod^{d-1}_{i = 1}[l_i, h_i]$, then $t \prec_\calF s$ if and only if $\sum^{d-1}_{i = 1}[\mathbf{1}(s[i] > t[i]) \times l_i + (1 - \mathbf{1}(s[i] > t[i])) \times h_i](s[i] - t[i]) + (s[d] - t[d]) \ge 0$, where $\mathbf{1}(\cdot)$ is the indicator function.
	\label{thm:R-dominance}
\end{theorem}

Now, consider the set $I$ of all instances as a set of points in the data space $[0, 1]^d$, the $i$-th attributes as coordinates in the $i$-th dimension.
For a point $t \in I$, partition the data space into $2^{d-1}$ regions using $d - 1$ hyperplanes $x[i] = t[i]$ for $1 \le i < d$.
Each resulted region can be identified by a $(d - 1)$-bit code such that the $i$-th bit is \textit{zero} if the $i$-th coordinates of points in this region are less than $t[i]$, and \textit{one} otherwise.
We refer to the region whose identifier is $k$ in decimal as region $k$, \eg, region 0 contains all points whose first $d-1$ coordinates are less than $t$ and region $2^{d-1}-1$ contains all points whose first $d-1$ coordinates are greater than $t$.
Theorem~\ref{thm:R-dominance} further indicates that given a set of ratio bound constraints $R = \prod^{d-1}_{i = 1} [l_i, h_i]$, for $0 \le k < 2^{d-1}$, all points in region $k$ that $\calF$-dominate $t$ lie in the following closed half-space,
\begin{equation}
	x[d] \le \sum^{d-1}_{i=1}[(1 - \lvert k \rvert_2[i])\times l_i + \lvert k \rvert_2[i]\times h_i](t[i] - x[i]) + t[d],
	\label{eq:half-space}
\end{equation}
where $\lvert k \rvert_2 [i]$ is $i$-th bit of the binary of number $k$.

\begin{figure}
	\subfigure[Range searching in primal space.]{
		\includegraphics[width=.445\linewidth]{figures/range-search}
		\label{fig:range-search}
	}\hspace{-1ex}
	\subfigure[Point location in dual space.]{
		\includegraphics[width=.45\linewidth]{figures/hpdual}
		\label{fig:point-lcation}
	}
	\caption{Reduction to range searching problem and point-hyperplane duality.}
	\label{fig:hp-dual}
\end{figure}

\begin{example}
	See Fig.~\ref{fig:range-search} for an illustration.
	For point $t_{2, 3}$, region 0 contains the set of points $\{t \in I \mid t[1] \le 9\}$.
	Suppose that the ratio bound constraint is $R = [0.5, 2]$, the closed half-space contains all points in region 0 that $\calF$-dominate $t_{2, 3}$ is $t[2] \le -0.5t[1] + 16.5$.
	Since $t_{3, 1}$ is included in that half-space, it is concluded that $t_{3, 1} \prec_\calF t_{2, 3}$.
\end{example}

The above procedure reduces the problem of finding all instances that $\calF$-dominate $t$ to a series of $2^{d-1}$ half-space range searching problem~\cite{agarwal2017simplex}.
Formally, the half-space range searching problem asks to preprocess a set of points in $\real^d$ into a data structure such that all points lying below or on a query hyperplane can be reported quickly.
This problem can be efficiently solved using the well-known \textit{point-hyperplane duality}~\cite{mark2008computational}.
To be specific, the duality transform maps a point $p = (p[1], \cdots, p[d]) \in \real^d$ into the hyperplane $p^* : x[d] = p[1]x[1] + \cdots + p[d-1]x[d-1] - p[d]$, and a hyperplane $h : x[d] = \alpha[1]x[1] + \cdots + \alpha[d-1]x[d-1] - \alpha[d]$ into the point $h^* = (\alpha[1] \cdots, \alpha[d])$.
It is proved that if $p$ lies above (\resp, below, on) $h$, then $h^*$ lies above (\resp, below, on) $p^*$.
The dual version of the half-space searching problem becomes that given a set of $n$ hyperplanes in $\real^d$ and a query point $q$, report all hyperplanes lying above or through $q$.
Then, let $H$ be a set of $n$ hyperplanes in $\real^d$, the {\it arrangement} of $H$, denoted by $\mathcal{A}(H)$, is a subdivision of $\real^d$ into {\it faces} of dimension $k$ for $0 \le k \le d$.
Each face in $\mathcal{A}(H)$ is a maximal connected region of $\real^d$ that lies in the same subset of $H$.
For a query point $q$, let $\lambda(q, H)$ denote the set of hyperplanes in $H$ lying above or through $q$.
It is easy to verify that all points $p$ lying on the same face $f$ of $\mathcal{A}(H)$ have the same $\lambda(p, H)$, denoted by $\lambda(f, H)$.
Thus, with a precomputation of $\lambda(f, H)$ for each face $f$ of $\mathcal{A}(H)$ and the following structure for point location in $\mathcal{A}(H)$, $\lambda(q, H)$ can be computed in logarithmic time.

\begin{theorem}[Structure for Point Location~\cite{DBLP:journals/iandc/Meiser93}]
	Given a set $H$ of $n$ hyperplanes in $\real^d$ and a query point $q$, there is a data structure of size $O(n^{d + \varepsilon})$ which can be constructed in $O(n^{d+\varepsilon})$ expected time for any $\varepsilon > 0$, so that the face of $\mathcal{A}(H)$ containing $q$ can be located in $O(\log{n})$ time.
	\label{thm:fast-point-location}
\end{theorem}

Returning to the ARSP problem, we propose an improved algorithm based on the above reduction.
In the preprocessing stage, for each point $t \in I$, say $t \in T_i$, whose $\Pr_{\rm sky}(t) > 0$, the algorithm partitions all instances from other objects into $2^{d-1}$ sets $I_{t, k}  = \{s \in I \setminus T_i \mid s \text{ in region } k \text{ partitioned by } t\}$ for $0 \le k < 2^{d-1}$.
Then, it builds the structure stated in Theorem~\ref{thm:fast-point-location} for the set of dual hyperplanes $I^*_{t, k}$ for each set $I_{t, k}$, and computes an array $\sigma_f = \langle \sigma_f[j] \mid 1 \le j \le m \rangle$ of aggregated values for each face $f$ of $\mathcal{A}(I^*_{t, k})$, where $\sigma_f[j] = \sum_{s^* \in \lambda(f, I^*_{t, k}) \wedge s \in T_j}\Pr(s)$, \ie, the sum of probabilities over all instances of object $T_j$ lying below or on the hyperplane $p^*$, where point $p$ lies in face $f$.

In the query processing stage, given a set of ratio bound constraints $R = \prod^{d-1}_{i = 1}[l_i, h_i]$, the algorithm processes each point $t$ as follows.
If there is no auxiliary structure built for that point, then it reports $\Pr_{\rm rsky}(t)$ as zero.
Otherwise, the algorithm first initializes $\Pr_{\rm rsky}(t) = \Pr(t)$ and $\sigma[i] = 0$ for $1 \le i \le m$, where $\sigma[i]$ is for recording the sum of existence probability of instances from object $T_i$ that $\calF$-dominate $t$ found so far.
Then, for $0 \le k < 2^{d-1}$, let $h_{t, k}$ denote the bounding hyperplane of the half-space in region $k$, which is defined in Equation~\ref{eq:half-space}, the algorithm performs point location query $h^*_{t, k}$ on the structure built for the set of hyperplanes $I^*_{t, k}$, and updates $\Pr_{\rm rsky}(t)$ according to Equation~\ref{eq:rskyprob-def} based on the array $\sigma_f$.
Specifically, for $1 \le j \le m$, it updates $\Pr_{\rm rsky}(t)$ to $\Pr_{\rm rsky}(t) \times (1 - \sigma[j] - \sigma_f[j])/(1 - \sigma[j])$ and adds $\sigma_f[j]$ to $\sigma[j]$.
After all queries, it is easy to verify that $\Pr_{\rm rsky}(t)$ is the exact rskyline probability of $t$.
Since each point location query can be performed in $O(\log{n})$ time and the update of $\Pr_{\rm rsky}(t)$ requires $O(m)$ time for each $\sigma_f$, the time complexity of the reduction-based algorithm is $O(2^dmn\log{n})$.

\begin{example}
	Continue with point $t_{2, 3}$ in Fig.~\ref{fig:range-search}, set $I_{t_{2, 3}, 0}$ includes points $t_{1, 1}, t_{1, 2}, t_{3, 1}, t_{3, 2}, t_{4, 1}$ and the set of dual planes is plotted in Fig.~\ref{fig:point-lcation}.
	By performing the point-location query $q = (-0.5, -16.5)$, which is the dual point of $t[2] = -0.5t[1] + 16.5$, the face $f$ containing $q$ is returned.
	Since the aggregated value $\sigma_f[3] = \Pr(t_{3,1}) + \Pr(t_{3, 2})$ is precomputed for $f$ in the preprocessing stage, the algorithm updates $\Pr_{\rm rsky}(t_{2, 3})$ to $\Pr_{\rm rsky}(t_{2, 3}) * (1 - \sigma[3] - \sigma_f[3])/(1 - \sigma[3])$ and adds $\sigma_f[3]$ to $\sigma[3]$.
\end{example}

\subsection{Sublinear-time Algorithm}

To achieve better time complexity, two bottlenecks of the above algorithm should be addressed.
1) There are in total $2^{d-1}$ arrays of aggregated values for each point and it seems unrealistic to merge them efficiently according to Equation~\ref{eq:rskyprob-def}.
2) Points are sequentially processed in the algorithm.
Since all of them should be scanned at least once, the time complexity is $\Omega(n)$.
In subsequent, we introduce two strategies to solve these two inefficiencies.

\noindent{\bf Multi-level Strategy.}
The reason why we have to merge $2^{d-1}$ arrays for each point $t$ is that the half-spaces in $2^{d-1}$ regions containing points that $\calF$-dominate $t$ are different from each other.
Thus, the above algorithm performs $2^{d-1}$ point location queries, one for each bounding hyperplane, to retrieve $\sigma_f$ in each region.
We show how to resolve this issue with the help of \textit{multi-level strategy}~\cite{DBLP:journals/ipl/Bentley79}.
Since the number of point location queries performed for each point is always $2^{d-1}$, we build a multi-level structure for the set of dual hyperplanes $I^*$ to retrieve the final aggregated result, each level of which is used to find all points lying below or on a query hyperplane.

To be specific, an 1-level auxiliary structure is defined as a point location tree (see Theorem~\ref{thm:fast-point-location}) built for $I^*$, and an array $\sigma_f = \langle \sigma_f[j] \mid 1 \le j \le m \rangle$ of aggregated values is computed for each face $f$ of $\mathcal{A}(I^*)$, where $\sigma_f[j] = \sum_{s^* \in \lambda(f, I^*) \wedge s \in T_j}\Pr(s)$, \ie, the sum of probabilities over all instances of object $T_j$ lying below or on the hyperplane $p^*$, where $p$ lies in face $f$.
Moreover, a product $\beta_f = \prod^m_{j = 1, \sigma_f[j] \ne 1}(1 - \sigma_f[j])$ and a count $\chi_f = |\{j \mid \sigma_f[j] = 1\}|$ are also recorded for each face $f \in \mathcal{A}(I^*)$.
Then, a $k$-level structure is recursively defined as an 1-level structure built for $I^*$ and additionally equipped with a $(k - 1)$-level structure for $\lambda(f, I^*)$ for each face $f$ of $\mathcal{A}(I^*)$.

After constructing the multi-level structure, the algorithm processes an input set of ratio bound constraints $R = \prod^{d-1}_{i = 1}[l_i, h_i]$ as follows.
For each point $t$, it performs $2^{d-1}$ point location queries on the multi-level structure, \ie, the dual point of the bounding hyperplane of the $i$-th half-space for the $i$-th region is queried on the $i$-level structure.
Let $f$ be the face returned by the final point location query.
According to values recorded for face $f$, the rskyline probability of $t$ can be calculated as

\begin{displaymath}
	%\begin{small}
		{\rm Pr}_{\rm rsky}(t) = \left\{
       	\begin{aligned}
			&\frac{\beta_f\cdot \Pr(t)}{1 - \sigma_f[i]} & \text{if } \chi_f = 0,\\
			&\beta_f \times \Pr(t) & \text{else if } \chi_f = 1 \wedge \sigma_f[i] = 1, \\
			&0 & \text{otherwise}.
		\end{aligned}
		\right.
	%\end{small}
%	\vspace{-2ex}
\end{displaymath}
It is easy to verify that the time consumed by computing the rskyline probability for a point $t$ is $O(2^{d-1}\log{n})$.
Therefore the total time complexity of the multi-level structure based algorithm for all rskyline probabilities computation is $O(2^{d-1}n\log{n})$ time.
This also leads to an algorithm with logarithmic query time and polynomial preprocessing time for rskyline probability query.
The formal definition of rskyline probability query is given as follows.

\noindent{\bf Problem: RSkyline Probability Query (RSPQ)}\\
\noindent{\bf Input:} an uncertain dataset $\calD$, a set of query ratio bound constraints $R$, and a query instance $q$. \\
\noindent{\bf Output:} for the query instance $q$, the probability that $q$ is not $\calF$-dominated by any instances in $I$, \ie,
\[{\rm Pr}_{\rm rsky}(q)= \prod^m_{i = 1} (1 - \sum_{t \in T_i, t \prec_\calF q}\Pr(t)).\]

\begin{theorem}
	\emph{RSPQ} belongs to the complexity class $\mathrm{PsL}${\rm ~\cite{DBLP:journals/tcs/GaoLML20}}.
\end{theorem}

\noindent{\bf Shift Strategy.}
The major obstacle for the second bottleneck is that the set of $2^{d-1}$ dual queries are different for each instance $t \in I$.
To be specific, according to Equation~\ref{eq:half-space}, the $k$-th dual query $h^*_{t, k}$ in region $k$ for a point $t$ is the $k$-th vertex of $R$ appended by the score of $t$ under that vertex.
It is surprising to discover that the $k$-th dual queries in region $k$ for any two points $t$ and $s$ only differ in the last dimension.
Therefore, if the scores of all instances remain the same under all vertices of $R$, we can unify the procedures of performing point location queries for each of them.
Specifically, for each point $t \in I$, say $t \in T_i$, whose $\Pr_{\rm sky}(t) > 0$, we create a shifted dataset with $t$ as the origin, \ie, $I_t = \{s - t \mid s \in I \setminus T_i\}$
Then we merge all sets $I_t$ to a key-value pair set $\mathcal{I} = \{(s, \langle t \mid s \in I_t \rangle) \mid s \in \bigcup_{t \in I} I_t\}$, where $s$ is a new point resulted by shifted the dataset with respect to some point $t$ and duplicate $s$ is eliminated by recording an array $\langle t \mid s \in I_t \rangle$ of its multiple origins.
Finally, we build the above multi-level structure for the set of dual hyperplanes $\mathcal{I}^* = \{s^* \mid (s, -) \in \mathcal{I}\}$, except that the aggregated array of an 1-level structure is redefined as $\Pr_f = \langle \Pr_f[t] \mid t \in I \rangle$ for each face $f$ of $\mathcal{A}(\mathcal{I}^*)$, where ${\rm Pr}_f[t] = \prod^m_{j= 1, j \ne i}(1 - \sum_{s^* \in \lambda(f, \mathcal{I}^*) \wedge s + t \in T_j}\Pr(s))$.

After that, the algorithm processes an input set of ratio bound constraints $R = \prod^{d-1}_{i=1}[l_i, h_i]$ as follows.
It generates $2^{d-1}$ queries by appending a zero to each vertex of $R$, and then performs these point location queries on the auxiliary structure.
Let $f$ denote the face returned by the final query, the rskyline probability of an instance $t$ is computed as $\Pr(t) \times \Pr_f(t)$ if $\Pr_f(t)$ is recorded in $\Pr_f$, and 0 otherwise.
It is easy to verify that the point location queries can be executed in $O(2^{d-1}\log{n})$ time and all rskyline probabilities can be reported in an additional $O(n)$ time.
