\section{Introduction}\label{sec:introduction}
%1. start with uncertian data, then why skyline and top-k is not work
%2. start with rskyline, then uncertainty is inherent in real dataset
The top-$k$ query and the skyline query are widely used in \textit{multi-criteria decision making}~\cite{koksalan2011multiple}.
Given a dataset, the former reports $k$ highest-scoring tuples according to a user-specified scoring function, while the latter outputs a set of all non-dominated tuples, where a tuple $t$ dominates another tuple $s$ if $t$ is no worse than $s$ in all dimensions and better than $s$ in at least one dimension.
However, it has been recognized that focusing on either of these two queries individually has some unavoidable drawbacks.
For example, the top-$k$ results depend heavily on the user-specified scoring function and thus only provides a narrow view of the dataset.
Meanwhile, the requirement that an exact scoring function is known in advance is also hardly realistic in practice.
As for the skyline, although it includes all possible tuples of interest, its size may be too large for users to come up to a final decision.

With the aim of gathering the strengths of these two queries while avoiding their drawbacks,  Ciaccia and Martinenghi~\cite{DBLP:journals/pvldb/CiacciaM17} recently introduced a notion of restricted skyline (rskyline).
By considering a set of user-specified scoring functions $\calF$, they defined that a tuple $t$ $\calF$-dominates another tuple $s$ if $t$ scores better than $s$ under any function $f \in \calF$, and the rskyline query is to retrieve the set of all non-$\calF$-dominated tuples.
Note that the rskyline query is more general than the top-$k$ query because it does not require users to specify exact weight values with absolute accuracy, but rather an approximate description of their preferences.
Meanwhile, it is experimentally verified~\cite{DBLP:journals/pvldb/CiacciaM17} that the rskyline query significantly reduces the size of the skyline and provides an overview of all potentially interesting tuples with respect to functions in $\calF$.

However, it has been observed that data uncertainty is inherent in multi-criteria decision making from various causes such as, limitations of measuring equipment, data randomness and incompleteness, outdated sources, and so on~\cite{DBLP:journals/fcsc/LiWLG20}.
Although a considerable amount of research has been dedicated to answering the top-$k$ query~\cite{DBLP:conf/icde/SolimanIC07, DBLP:conf/icde/YiLKS08, DBLP:conf/icde/HuaPZL08, DBLP:journals/dpd/WangSY16} and the skyline query~\cite{DBLP:conf/vldb/PeiJLY07, DBLP:conf/pods/AtallahQ09, DBLP:journals/tods/AtallahQY11, DBLP:journals/mst/AfshaniAALP13, DBLP:journals/tkde/KimIP12, DBLP:journals/tjs/Gavagsaz21, DBLP:conf/icdcs/ZhangWWH19, DBLP:journals/is/ZhangZLJP11} on uncertain datasets, how to conduct rskyline analysis on uncertain datasets remains an open problem at large.
We first describe some application scenarios as follows:

\noindent{\it E-commerce Scenario:}
Data uncertainty is often found in e-comme-rce.
For example, vehicles in a used car platform (\eg, www.motors.ebay.com) may not always be available to the users for sure during the decision making process of a potential buyer.
Thus, each car actually has a probability of being available to users.
Suppose the two criteria in deciding which car a user wants are \textsf{Price} and \textsf{Mileage}, we can translate the preference for cheaper cars into a constraint on weights of a scoring function, \eg, focusing on the family $\calF = \{\omega_P \textsf{Price} + \omega_M \textsf{Mileage} \mid \omega_P \ge \omega_M \}$.
Conducting rskyline analysis on the vehicles dataset will quantify how likely a car is to be excellent with respect to some scoring function in $\calF$.

\noindent{\it Player Selection Scenario:}
Often times we cannot precisely specify and interpret the scoring function used to measure the performance of a player.
It is much easier for us to provide a relative importance of the attributes, \eg, \textsf{Points} is as least as important as \textsf{Assists}.
Input with a family of scoring functions $\calF$ satisfying such constraint, rskyline analysis on the technical statistics data of players helps to identify excellent players~\cite{DBLP:journals/pvldb/CiacciaM17}.
However, the performance data may vary substantially player by player and game by game.
And uncertainty is inherent due to many factors such as fluctuations of playersâ€™ conditions, the locations of the games, and the support from audience.
Modeling the game-by-game performance data as an uncertain dataset, then conducting probabilistic rskyline analysis will identify the players with high probability of excelling under some functions in $\calF$

\eat{
\noindent\underline{College Application Scenario:}
Assume a high school graduate receives a set of college admission offers. Although the graduate cannot set an exact attribute weight ratio for the two attributes, he specifies that major ranking is as least as important as university ranking.
However, rankings provided by different agencies are different.
Thus, we can treat the college as an uncertain object and the rankings as its instances.
Now, the probability can be the confidence of the ranking provided by each agency.
In such case, an offer with high rskyline probability means that this college is the top-1 choice for the graduate with some preference meeting his demands.
}
Motivated by these applications, we revisit the rskyline query on uncertain datasets in this paper.
We follow the uncertain model introduced in~\cite{DBLP:conf/pods/AtallahQ09}, in which the probability distribution of an object is represented by a weighted set of tuples called instances.
Given a set of scoring functions $\calF$, we define the probability of an instance being in the rskyline, called rskyline probability, as the probability that none of instances from other objects that $\calF$-dominates it occurs in the dataset.
And we investigate the problem of computing rskyline probabilities of all instances from both complexity and algorithm perspective.

For the problem complexity, we establish a fine-grained reduction from the Orthogonal Vectors problem to the problem of computing rskyline probabilities of all instances and prove that there is no truly subquadratic-time algorithm for all rskyline probabilities (ARSP) computation unless the Orthogonal Vectors conjecture fails.
It is noticed that the conditional lower bound also holds for all skyline probability (ASP) computation.
Note that although several efficient algorithms for this problem have been proposed in previous work~\cite{DBLP:conf/pods/AtallahQ09, DBLP:journals/tods/AtallahQY11, DBLP:journals/tkde/KimIP12, DBLP:journals/mst/AfshaniAALP13}, none of them considered the lower bound for this problem.
In~\cite{DBLP:journals/tods/AtallahQY11}, Atallah~\etal only made a discussion about the potential optimality of their algorithm but did not provide a formal proof.
This conditional lower bound first formally proves that the algorithms proposed in~\cite{DBLP:conf/pods/AtallahQ09, DBLP:journals/tods/AtallahQY11, DBLP:journals/mst/AfshaniAALP13} are near-optimal.

%TODO:introduce challenges for algorithmic results
As for the algorithmic concern, we focus on a practically effective case where $\calF$ consists of linear scoring functions described by a set of linear constraints.
%In many applications, especially in databases containing numeric attributes, the scoring function used to model user preferences is expressed in the form of a linear combination of query attributes.
%And a practical way to specify the preference region of a user is starting with $\simplex^{d-1}$ and adds some constraints.
Exploiting the convexity of the preference region, we transforms the ARSP computation in the original space into the ASP computation in the score space defined by the vertices of the preference region.
This leads to an algorithm with near-optimal time complexity for ARSP computation compared to the proved conditional lower bound.
And we also optimize the implementation of the called algorithm, which further enhances its experimental performance.
It is noticed that a major drawback of the above algorithm is all instances need to be mapped into the score space each time.
We resolve this by designing effective pruning strategies and conducting the mapping in an incremental manner.
By integrating them into the branch-and-bound paradigm, we propose an algorithm with better expected time complexity.

For a special case where linear constraints are $d - 1$ ratio bound constraints, we establish a Turing reduction from the ARSP computation to the half-space range searching problem using the well-known point-hyperplane duality.
Based on the reduction, we propose an $O(2^dmn\log{n})$-time algorithm with a polynomial-time preprocessing, where $m$ and $n$ is the number of objects and instances, respectively.
Subsequently, we introduce a multi-level structure and a data-shifting strategy to further improve the time complexity to $O(2^{d-1}\log{n} + n)$, where the additional linear time is only required for reporting the final results.
This algorithm matters from the following two aspects.
First, it proves that the online rskyline probability query belongs to the pseudo-logarithmic-time complexity class $\mathrm{PsL}$~\cite{DBLP:journals/tcs/GaoLML20}, which can be further used to design efficient algorithms for other queries.
Second, although this algorithm is somewhat inherently theoretical, experimental results shows that its extension for this special rskyline query on certain datasets outperforms the state-of-the-art method proposed in~\cite{DBLP:conf/icde/Liu0ZP021}.

To the best of our knowledge, this paper is the first work conducting rskyline analysis on uncertain datasets.
The main contributions of this paper are summarized as follows.
\begin{enumerate}[$\bullet$]
	\item We formalize the problem of computing all rskyline probabilities of all instances and prove a conditional lower bound that there is no algorithm can solve this problem in $O(n^{2-\delta})$ time for any $\delta > 0$, unless the Orthogonal Vectors conjecture fails.
	\item When the preference region is described by a set of linear constraints, for ARSP computation, we propose an algorithm with near-optimal time complexity $O(n^{2 - 1/d'})$, where $d'$ is the number of vertices of the preference region, and an algorithm with time complexity $O(mn\log{n})$ in expectation.
	\item When the preference region is specified by $d-1$ ratio bound constraints, we propose a $O(2^{d-1}\log{n} + n)$-time algorithm with a polynomial-time preprocessing for ARSP computation, and a $O(2^{d-1}\log{n})$-time algorithm with a polynomial-time preprocessing for online rskyline probability query.
	\item We conduct extensive experiments over real and synthetic datasets to demonstrate the effectiveness of ARSP computation and the efficiency and scalability of the proposed algorithms.
	%	Moreover, the extension of the sublinear-time algorithm also outperforms the state-of-the-art method for the restricted skyline query on certain datasets.
\end{enumerate}

The reminder of this paper is organized as follows.
We review the related work in Section~\ref{sec:relatedwork}.
We formally define the problem studied in this paper and study its conditional lower bound in Section~\ref{sec:preliminary}.
Then, we propose two efficient algorithms for ARSP computation in Section~\ref{sec:rskyprobalg}, and design an algorithm with sublinear query time and polynomial preprocessing time for ratio bound constraints in Section~\ref{sec:eclprobalg}.
We report the experimental results in Section~\ref{sec:experiments}.
Finally, we conclude the paper in Section~\ref{sec:conclusions}.