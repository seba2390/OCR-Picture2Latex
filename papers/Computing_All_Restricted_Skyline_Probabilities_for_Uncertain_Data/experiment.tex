\section{Experiments}\label{sec:experiments}

In this section, we report the experimental study of the algorithms proposed for the ARSP problem.

\subsection{Experimental Setting}

\begin{table}[t]
	\centering
	\caption{Parameter settings for experiments.}
	\label{tab:parameters}
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Parameter} & \textbf{Tested and default values} \\ \hline \hline
		Object Cardinality $m$ & {2K, 4K, 8K, \textbf{16K}, 32K, 64K } \\ \hline
        Dimensionality $d$ & {2, 3, \textbf{4}, 5, 6, 7, 8}	\\ \hline	
		Instance Count $cnt$ & {100, 200, 300, \textbf{400}, 500, 600} \\ \hline
		Region Length $l$ & {0.1, \textbf{0.2}, 0.3, 0.4, 0.5, 0.6} \\ \hline
		\% of objects with $\Pr(T) < 1$ $\phi$ & {0, 0.05, \textbf{0.1}, 0.2, 0.4, 0.8} \\ \hline
		Type of constraints & {\sc \bf WEAK}, {\sc INTER} \\ \hline
		\# of constraints $c$ & 2, 3, 4, 5, 6, 7 (default: $d - 1$) \\ \hline
	\end{tabular}
\end{table}

\noindent{\bf Datasets and Constraints.}
We use both real dataset and synthetic datasets for the experiments.
We include a real dataset that is widely used by related work~\cite{DBLP:conf/vldb/PeiJLY07, DBLP:journals/tkde/KimIP12, DBLP:journals/ijon/YangLZMG18}.
Specifically, \textsc{NBA} contains 28,475 technical statistics of 3707 players with 5 professional metrics: points, assists, rebounds, steals, and blocks, extracted from \url{https://www.nba.com/stats/}.
We consider each player as an uncertain object and his season records as instances with the same existence probability of that object.
%\textsc{CAR} includes 
And following the previous related work~\cite{DBLP:conf/vldb/PeiJLY07, DBLP:conf/pods/AtallahQ09, DBLP:journals/tkde/KimIP12}, we generate synthetic datasets as follows.
We first generate centers of objects in the data space $[0, 1]^d$ according to independent distributions (\textsc{IND}) or anti-correlated distributions (\textsc{ANTI}) using the standard data generation tool~\cite{DBLP:conf/icde/BorzsonyiKS01}.
Then, for each center, we construct a hyper-rectangle whose edge length follows a normal distribution $\mathscr{N}(l/2, l/8)$.
After that, we generate instances of the object uniformly within the hyper-rectangle and assign all instances the same existence probability.
The number of instances follows a uniform distribution over interval $[1, cnt]$.
Finally, we remove one instance from $\phi \times m$ objects so that the total probability of each of them is strictly less than one.
The expected number of instances in the dataset is $(\frac{cnt}{2} - \phi)\cdot m$.

We consider two types are constraints in our experiment.
The first are \textit{weak rankings}~\cite{DBLP:journals/pvldb/CiacciaM17}, which is one of the most common types of constraints on weights.
For any number $c$ of constraints, the input set of constraints is $\{\omega[i] \ge \omega[i + 1] \mid i \in \{1, \cdots, c\}\}$.
The second are \textit{interactive constraints}, which are generated in an interactive manner.
Specifically, we first choose an inner point in $\simplex^{d-1}$ as an estimation of one's preference.
Then, at each time, we generate a pair of tuples $(t, s)$ uniformly in $[0, 1]^d$ and choose the side of hyperplane $\sum^d_{i = 1}(t[i] - s[i]) \times \omega[i] = 0$ that containing the inner point as an input constraint.
It is easy to see that the major difference between this two constraints is the number of vertices of the preference region.
The first preference region always has $d$ vertices while the number of vertices of the second one generally increases as $c$ increases.

Table~\ref{tab:parameters} lists all parameters for synthetic datasets and constraints with their tested and default values (in bold).
To eliminate the bias of the generated datasets and constraints, we repeat all experiments 10 times for each parameter configuration and report the average as final results.

\noindent{\bf Algorithms.}
We implement the following algorithm in C++ and the source code can be accessed in~\cite{github}.
All the algorithms are complied by GNU G++ 7.5.0 with -O2 optimization and all experiments are conducted on a machine with a 3.5-GHz Intel(R) Core(TM) i9-10920X CPU, 256GB main memory, and 1TB hard disk running CentOS 7.
\begin{enumerate}[$\bullet$]
	\item \textsc{BSL}: the baseline algorithm in Section~\ref{subsec:bsl}.
	\item \textsc{KDTT}: the kdtree-traversal algorithm in Section~\ref{subsec:tt}.
	\item \textsc{KDTT$^*$}: the kdtree-traversal algorithm incorporating preorder traversal into tree construction in Section~\ref{subsec:tt}. 
	\item \textsc{QDTT$^*$}: the quadtree-traversal algorithm incorporating preorder traversal into tree construction in Section~\ref{subsec:tt}. 
	\item \textsc{B\&B}: the branch-and-bound algorithm in Section~\ref{subsec:bb}.
	\item \textsc{DUAL} (\textsc{-M/S}): the dual-based algorithm in Section~\ref{sec:eclprobalg}, where \textsc{-M} is for multi-level strategy, \textsc{-S} is for shift strategy.
\end{enumerate}


\subsection{Effectiveness of the ARSP.}

To verify the effectiveness of the ARSP, we compute rskyline probabilities of all players on real \textsc{NBA} dataset, and report the top-10 players in rskyline probability ranking along with their rskyline probabilities in Table~\ref{tab:top-10}.
For comparison, we also conduct the traditional rskyline analysis.
We calculate the average statistics for each player and retrieve rskyline on this average dataset, which is called aggregated rskyline for short hereafter.
All players in the aggregated rskyline are marked with a ``*'' sign in Table~\ref{tab:top-10}.
\begin{table}[htb]
	\centering
	\caption{Top-10 players in rskyline probability ranking.}
	\label{tab:top-10}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Player} & $\Pr_{\rm rsky}(\cdot)$ & \textbf{Player} & $\Pr_{\rm rsky}(\cdot)$ \\ \hline \hline
		* Michael Jordan & 0.612 & * Russell Westbrook & 0.264 \\ \hline
		* Magic Johnson & 0.329 & Larry Bird & 0.259 \\ \hline
		* LeBron James & 0.282 & John Stockton & 0.237 \\ \hline
		David Robinson & 0.272 & James Harden & 0.230 \\ \hline
		Kareem Abdul-Jabbar & 0.269 & Hakeem Olajuwon & 0.216 \\ \hline
	\end{tabular}
\end{table}

All players returned by aggregated rskyline or with high rskyline probability have good scoring ability according to their statistics, but there are still some differences between these two results.
It is observed that players in the aggregated rskyline have very different rskyline probabilities, meanwhile players not in the aggregated rskyline may have a high rskyline probability.
The reason is that a player's season performance may have a high variance which can not be reflected by the average statistics.
For example, compared with Michael Jordan, Russell Westbrook has more season statistics with rskyline probability less than 0.001 and the average of David Robinson's statistics is relatively low, but the variance is high, which makes him not belong to the aggregated rskyline but have a high rskyline probability.

In addition, the rskyline probability determines an order of players in the aggregated rskyline, which is not represented in the original result.
This expresses the difference between two players that are not comparable under the set of user-specified scoring functions, \eg, we can say that although both belong to the aggregated rskyline, Michael Jordan is better than LeBron James since the former is more likely to appear in the rskyline of a match.
Moreover, users can efficiently perform top-$k$ queries or threshold queries on the result of ARSP problem to retrieve a set with specified size, while the aggregated rskyline size is uncontrollable.
From above observations, we conclude that the ARSP provides a more comprehensive view on uncertain datasets than the aggregated rskyline.

\subsection{Experimental Results under Linear Constraints.}

In what follows, we study the efficiency and scalability of the proposed algorithms.
%In the interest of space, we do not include correlated synthetic datasets in our study since they are least challenging for ARSP computation.

\begin{figure*}[t]
    \centering
	\subfigure[Effect of $m$]{
		\includegraphics[width=.18\linewidth]{figures/exp-weak-rankings/arsp_inde_m}
		\label{fig:ind_m}
	}
	\subfigure[Effect of $d$]{
		\includegraphics[width=.182\linewidth]{figures/exp-weak-rankings/arsp_inde_dim}
		\label{fig:ind_d}
	}
	\subfigure[Effect of $cnt$]{
	   \includegraphics[width=.18\textwidth]{figures/exp-weak-rankings/arsp_inde_cnt}
        \label{fig:ind_cnt}
	}
    \subfigure[Effect of $l$]{
		\includegraphics[width=.185\textwidth]{figures/exp-weak-rankings/arsp_inde_l}
        \label{fig:ind_l}
	}
    \subfigure[Effect of $\phi$]{
		\includegraphics[width=.18\textwidth]{figures/exp-weak-rankings/arsp_inde_p}
        \label{fig:ind_phi}
	}
    \caption{Running time on \textsc{IND} datasets.}
\end{figure*}

\begin{figure*}[t]
    \centering
    \subfigure[Effect of $m$]{
		\includegraphics[width=.18\linewidth]{figures/exp-weak-rankings/arsp_anti_m}
		\label{fig:anti_m}
	}
    \subfigure[Effect of $d$]{
	   \includegraphics[width=.182\linewidth]{figures/exp-weak-rankings/arsp_anti_dim}
	   \label{fig:anti_d}
	}
    \subfigure[Effect of $cnt$]{
		\includegraphics[width=.18\textwidth]{figures/exp-weak-rankings/arsp_anti_cnt}
        \label{fig:anti_cnt}
	}
    \subfigure[Effect of $l$]{
		\includegraphics[width=.185\textwidth]{figures/exp-weak-rankings/arsp_anti_l}
        \label{fig:anti_l}
	}
    \subfigure[Effect of $\phi$]{
		\includegraphics[width=.18\textwidth]{figures/exp-weak-rankings/arsp_anti_p}
        \label{fig:anti_phi}
	}
    \caption{Running time on \textsc{ANTI} datasets.}
\end{figure*}

Fig.~\ref{fig:ind_m},~\ref{fig:anti_m}, and Fig.~\ref{fig:ind_cnt},~\ref{fig:anti_cnt} show the effect of object cardinality $m$ and instance count $cnt$ on the running time of all algorithms, respectively.
According to the generation procedure of the synthetic datasets, the number of instances $n$ increases as $m$ and $cnt$ increase.
Thus, the running time increases too for all algorithms.
All proposed algorithms outperform the baseline by around an order of magnitude since \textsc{BSL} performs a numerous number of $\calF$-dominate tests and does not involve effective pruning methods in the computation.
Generally, \textsc{B\&B} runs fastest due to the incremental mapping and the pruning strategy, but the gap narrows as $m$ increases since the more objects, the more aggregated R-trees are queried by each instance.
In addition, \textsc{B\&B} is more sensitive to the data distribution than other methods because the data distribution directly affects the effect of its pruning strategy.
Although with similar strategies, \textsc{QDTT$^*$} performs better than \textsc{KDTT$^*$} because the split of all dimensions at internal nodes makes subtrees whose points all have zero skyline probability pruned as early as possible.
The results also demonstrate our optimization techniques significantly improve the experimental performance of \textsc{KDTT} under all settings.
And the relative performance of the algorithms remains basically unchanged with respect to $cnt$.

Having established \textsc{BSL} is inefficient for the ARSP problem, we henceforth exclude it from the following experiments.
We also omit the curves of \textsc{KDTT} since it is always outperformed by \textsc{KDTT$^*$}.
Fig.~\ref{fig:ind_d} and~\ref{fig:anti_d} shows the running time of the algorithms on datasets with varying dimensionality $d$.
The running time of all algorithms increase with $d$ due to the cost of $\calF$-dominance test increases.
\textsc{QDTT$^*$} and \textsc{KDTT$^*$} are more efficient than \textsc{B\&B} on low-dimensional datasets, but scale poorly with respect to $d$.
This is because as $d$ increases, roots of subtrees pruned during the preorder traversal get closer to leaf nodes in \textsc{KDTT$^*$} and \textsc{QDTT$^*$}.
Moreover, the exponential growth in the number of child nodes of \textsc{QDTT$^*$} also causes its inefficiency on high-dimensional datasets.

For \textsc{B\&B}, its performance mainly depends on the pruning ability of the incrementally constructed score vectors in $P$ and the time cost of querying the aggregated R-trees.
These are affected by the region length $l$ and percentage $\phi$ of objects with sum probability less than one as shown in Fig.~\ref{fig:ind_l}, Fig.~\ref{fig:ind_phi}, Fig.~\ref{fig:anti_l}, and Fig.~\ref{fig:anti_phi}.
It is easy to see that all algorithms follow similar trends, but \textsc{B\&B} is more sensitive to these two parameters.
This is because the larger the region length of an object $T$, the longer time \textsc{B\&B} takes to query the aggregated R-tree for $T$ and the fewer instances are pruned by the score vector constructed based on $T$.
Meanwhile, the more objects with sum probability less than one, the fewer score vectors are inserted into the pruning set $P$.

\begin{figure*}[t]
	\begin{minipage}{.39\textwidth}
        \centering
		\subfigure[{\sc IND}]{
    		\includegraphics[width=.46\textwidth]{figures/exp-weak-rankings/arsp_inde_c}
    		\label{fig:ind_c}
        }
    	\subfigure[{\sc ANTI}]{
    		\includegraphics[width=.45\textwidth]{figures/exp-weak-rankings/arsp_anti_c}
    		\label{fig:anti_c}
    	}
		\caption{Effect of $c$ on running time for linear constraints.}
		\label{fig:eff_c}
	\end{minipage}
	\begin{minipage}{.6\textwidth}
        \centering
		\subfigure[Effect of $m$]{
    		\includegraphics[width=.285\textwidth]{figures/exp-random-planes/arsp_inde_m}
    		\label{fig:rp_m}
    	}
    	\subfigure[Effect of $d$]{
    		\includegraphics[width=.31\textwidth]{figures/exp-random-planes/arsp_inde_dim}
    		\label{fig:rp_d}
    	}
    	\subfigure[Effect of $c$]{
    		\includegraphics[width=.315\textwidth]{figures/exp-random-planes/arsp_inde_c}
    		\label{fig:rp_c}
    	}
		\caption{Running time under interactive constraints (\textsc{IND}).}
		\label{fig:eff_rp}
	\end{minipage}
\end{figure*}


Then, we evaluate the effect of the number and type of constraints.
Fig.~\ref{fig:eff_c} plots the running time of algorithms with varying number of linear constraints.
As $c$ grows, the preference region becomes smaller, which improves the pruning ability of each instance, while makes instances in the score space more compact.
Therefore, the running time of each algorithm first increase and then decreases with $c$.
Note that the inconsistent performance of \textsc{B\&B} on \textsc{IND} and \textsc{ANTI} is because its pruning strategy is less effective on anti-correlated datasets.
We also study the performance of the proposed algorithms with \textit{interactive constraints}, where the number of vertices of the preference region increases along with $c$.
As shown in Fig.~\ref{fig:eff_rp}, the algorithms show trends similar to weak rankings expect Fig~\ref{fig:rp_c}.
The result indicates that the performance of all algorithms improves as $c$ grows except for \textsc{QDTT$^*$} 
This is because the preference region gets smaller with the increasing of $c$ which makes more instances pruned in \textsc{B\&B} and more subtrees pruned in \textsc{KDTT$^*$}.
But the dimensional disaster of \textsc{QDTT$^*$}, see curve $|V|$ in Fig.~\ref{fig:rp_c}, overshadows the performance improvement gains from the narrow of the preference region.
And this also accounts for the failure of \textsc{QDTT$^*$} when $d > 5$ in Fig.~\ref{fig:rp_d}.

\subsection{Experimental Results under Ratio Bound Constraints.}

Since the structure stated in Theorem~\ref{thm:fast-point-location} is somewhat inherently theoretical, especially in high dimensions, we introduce a specialized version of \textsc{DUAL-MS} for $d = 2$ to avoid this.
Recall that for each instance $t$, we reduce the computation of $\Pr(t)$ to two half-space searching problems as illustrated in Fig.~\ref{fig:range-search}. 
It is noticed that these two queries can be reinterpreted as a continuous range query if $d = 2$.
As shown in Fig.~\ref{fig:specilized_version}, when processing $t_{2, 3}$,  we can treat $t_{2, 3}$ as the origin, ray $y = t_{2, 3}[2], x \ge t_{2, 3}[1]$ as a base and represent each instance by an angle, \eg, $\theta = \pi + \arctan\frac{12 - 5}{9 - 6}$, then the two query lines $t[2] \le -0.5t[1] + 16.5$ and $t[2] \le -2t[1] + 30$ can be mapped into a range query $[\pi - \arctan\frac{1}{2}, 2\pi - \arctan2]$ with respect to angle.
With this transformation, we can use a simple binary search tree to organize the instances instead of the point location tree.
We give an implementation of this specialized \textsc{DUAL-MS} and evaluate its performance on the \textsc{NBA} dataset.
For reference, we also attach a simple preprocessing strategy to \textsc{KDTT$^*$}, which removes all instances with zero skyline probability from $I$.
Fig.~\ref{fig:dual_ms_nba} shows the running time of these two algorithms.
Although the efficiency is improved, the huge preprocessing time and memory consumption prevent its application on big datasets.

\begin{figure*}[t]
	\begin{minipage}{.39\textwidth}
        \centering
        \subfigure[Specialized version]{
    		\includegraphics[width=.45\linewidth]{figures/specialized_version}
    		\label{fig:specilized_version}
    	}
    	\subfigure[Running time on NBA]{
    		\includegraphics[width=.46\linewidth]{figures/eclp_nba_m}
    		\label{fig:dual_ms_nba}
    	}
	   \caption{A specialized version of \textsf{DUAL-MS} for $d = 2$ and its running time on NBA dataset.}
	\end{minipage}
	\begin{minipage}{.6\textwidth}
        \centering
    	\subfigure[Effect of $n$]{
    		\includegraphics[width=.3\linewidth]{figures/eclipse_n}
    		\label{fig:eclipse_n}
    	}
    	\subfigure[Effect of $d$]{
    		\includegraphics[width=.3\linewidth]{figures/eclipse_dim}
    		\label{fig:eclipse_dim}
    	}
    	\subfigure[Effect of $q$]{
    		\includegraphics[width=.3\linewidth]{figures/eclipse_query}
    		\label{fig:eclipse_q}
    	}
    	\caption{Running time for eclipse query (\textsc{IND}).}
    	\label{fig:eclipse}
    \end{minipage}
\end{figure*}


The above drawbacks of \textsc{DUAL-MS} are alleviated for eclipse query because eclipse is a subset of skyline $S$, which has a logarithmic size in expectation, and the multi-level strategy is no longer needed since for each tuple $t \in S$, $t$ belongs to the eclipse of $D$ if and only if all point location queries on $S^*_{t, k}$ ($0 \le k < 2^{d-1}$) return emptiness.
Thus, we extend the dual-based algorithm \textsc{DUAL-S} for eclipse query, in which we use a $kd$-tree to index the dataset resulted by applying shifted strategy.
For comparison, we also implement the state-of-the-art index-based method \textsc{QUAD}~\cite{DBLP:conf/icde/Liu0ZP021} for eclipse query in C++.

We evaluate their efficiency and scalability with respect to data cardinality $n$, data dimensionality $d$, and ratio range $q$, where the defaulted value is set as $n = 2^{14}$, $d = 3$, and $q = [0.36, 2.75]$.
As shown in Fig.~\ref{fig:eclipse_n} and~\ref{fig:eclipse_dim}, the running time of these two methods increases with the increasing of both $n$ and $d$.
Concretely, \textsc{DUAL-S} outperforms \textsc{QUAD} by at least an order of magnitude and even more on high-dimensional datasets.
The reason is that \textsc{QUAD} needs to iterate over the set of hyperplanes returned by the window query performed on its Intersection Index, and then reports all tuples with zero Order Vector as the final result.
This takes $O(s^2)$ time in the worst case, where $s$ is the skyline size of the dataset.
But in \textsc{DUAL-S}, we exclude a tuple from the result if there is a range query returns non-empty result, which only take $O(s)$ time in the worst case.
Moreover, the hyperplane quadtree adopted in \textsc{QUAD} scales poorly with respect to $d$ for the following two reasons.
On the one hand, the tree index has a large fan-out since it splits all dimensions at each internal node.
On the other hand, the number of intersection hyperplanes of a node decreases slightly relative to its parent, especially on high-dimensional datasets, which results in an unacceptable tree height.
Moreover, as shown in Fig.~\ref{fig:eclipse_q}, \textsc{QUAD} is more sensitive to the ratio range than \textsc{DUAL-S} because the number of hyperplanes returned by the window query actually determines the running time.


