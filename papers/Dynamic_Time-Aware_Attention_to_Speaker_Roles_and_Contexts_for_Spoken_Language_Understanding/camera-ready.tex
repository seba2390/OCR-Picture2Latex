% Template for ASRU-2017 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{url}

% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Dynamic Time-Aware Attention to Speaker Roles and Contexts \\for Spoken Language Understanding}
%
% Single address.
% ---------------
\name{Po-Chun Chen$^\star$\quad Ta-Chung Chi$^\star$\quad Shang-Yu Su$^\dagger$\quad Yun-Nung Chen$^\star$\thanks{The first three authors have equal contributions.}}
\address{$^\star$Department of Computer Science and Information Engineering\\$^\dagger$Graduate Institute of Electrical Engineering\\National Taiwan University, Taipei, Taiwan\\\texttt{ \small \{b02902019,r06922028,r05921117\}@ntu.edu.tw\quad y.v.chen@ieee.org}}
%
% For example:
% ------------
%\address{School\\
%	Department\\
%	Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%	{School A-B\\
%	Department A-B\\
%	Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%	while at ...}}
%	{School C-D\\
%	Department C-D\\
%	Address C-D}
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
Spoken language understanding (SLU) is an essential component in conversational systems.
Most SLU components treat each utterance independently, and then the following components aggregate the multi-turn information in the separate phases.
In order to avoid error propagation and effectively utilize contexts, prior works leveraged history for contextual SLU.
However, the previous models only paid attention to the content in history utterances without considering their temporal information and speaker roles.
In dialogues, the most recent utterances should be more important than the least recent ones.
Furthermore, users usually pay attention to 1) self history for reasoning and 2) othersâ€™ utterances for listening, the speaker of the utterances may provides informative cues to help understanding.
Therefore, this paper proposes an attention-based network that additionally leverages temporal information and speaker role for better SLU, where the attention to contexts and speaker roles can be automatically learned in an end-to-end manner.
The experiments on the benchmark Dialogue State Tracking Challenge 4 (DSTC4) dataset show that the time-aware dynamic role attention networks significantly improve the understanding performance\footnote{The released code: \url{https://github.com/MiuLab/Time-SLU}}.
\end{abstract}
%
\begin{keywords}
dialogue, language understanding, SLU, temporal, role, attention, deep learning.
\end{keywords}
%

\section{Introduction}
\label{sec:intro}
Spoken dialogue systems that can help users to solve complex tasks such as booking a movie ticket have become an emerging research topic in artificial intelligence and natural language processing area. 
With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. 
Today, there are several virtual intelligent assistants, such as Apple's Siri, Google's Home, Microsoft's Cortana, and Amazon's Echo. Recent advance of deep learning has inspired many applications of neural models to dialogue systems. Prior work introduced network-based end-to-end trainable task-oriented dialogue systems~\cite{wen2017network,bordes2017learning,dhingra2017towards,li2017end}.

A key component of the understanding system is a spoken language understanding (SLU) module---it parses user utterances into semantic frames that capture the core meaning, where three main tasks of SLU are domain classification, intent determination, and slot filling~\cite{tur2011spoken}.
A typical pipeline of SLU is to first decide the domain given the input utterance, and based on the domain, to predict the intent and to fill associated slots corresponding to a domain-specific semantic template.
With the power of deep learning, there are emerging better approaches of SLU~\cite{hakkani2016multi,chen2016knowledge,chen2016syntax,wang2016learning}.
However, the above work focused on single-turn interactions, where each utterance is treated independently.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{DSTC_example.pdf}
%\vspace{-7mm}
\caption{The human-human conversational utterances and their associated semantic labels from DSTC4.}
%\vspace{-3mm}
\label{fig:example}
\end{figure*}

The contextual information has been shown useful for SLU~\cite{bhargava2013easy,xu2014contextual,chen2015leveraging,sun2016an}.
For example, Figure~\ref{fig:example} shows conversational utterances, where the intent of the highlighted tourist utterance is to ask about location information, but it is difficult to understand without contexts.
Hence, it is more likely to estimate the location-related intent given the contextual utterances about location recommendation.
%\newcite{bhargava2013easy} incorporated the information from previous intra-session utterances into the SLU tasks on a given utterance by applying SVM-HMMs to sequence tagging and obtained the improvement.
Contextual information has been incorporated into the recurrent neural network (RNN) for improved domain classification, intent prediction, and slot filling~\cite{xu2014contextual,shi2015contextual,weston2015memory,chen2016end}.

% The SLU output is semantic representations of users' behaviors, and then flows to downstream dialogue management component to decide which action the system should take next, as called \emph{dialogue policy}.
% It is intuitive that better understanding could improve the policy learning, so that the dialogue management can be further boosted through interactions~\cite{wen2016network,li2017end}.

%Most successful approaches cast the dialogue management task as a partially observable Markov decision process~\cite{young2013pomdp} to achieve high performance by leveraging various reinforcement learning methods.
%However, these methods require a large annotated dataset~\cite{young2013pomdp},  human interaction~\cite{gavsic2011line,wen2016network}, or interaction with rule-based user simulator~\cite{li2017end}; these constraints make it hard to put these methods into practice.

%However, prior work did not consider different speaker roles of the history utterances.
Most of previous dialogue systems did not take speaker role into consideration.
However, different speaker roles can cause notable variance in speaking habits and later affect the system performance differently. 
From Figure~\ref{fig:example}, the benchmark dialogue dataset, Dialogue State Tracking Challenge 4 (DSTC4)~\cite{kim2016fourth}\footnote{\url{http://www.colips.org/workshop/dstc4/}}, contains two specific roles, a tourist and a guide.
Under the scenario of dialogue systems and the communication patterns, we take the tourist as a user and the guide as the dialogue agent (system).
During conversations, the user may focus on not only \emph{reasoning (user history)} but also \emph{listening (agent history)}, so different speaker roles could provide various cues for better understanding~\cite{chi2017speaker}.

In addition, neural models incorporating attention mechanisms have had great successes in machine translation~\cite{bahdanau2014neural}, image captioning~\cite{xu2015show}, and various tasks. 
Attentional models have been successful because they separate two different concerns: 1) deciding which input contexts are most relevant to the output and 2) actually predicting an output given the most relevant inputs. 
For example, the highlighted current utterance from the tourist, ``\textit{uh on august}'', in the conversation of Figure~\ref{fig:example} is to respond the question about \textit{when}, and the content-aware contexts that can help current understanding are the first two utterances from the guide ``\textit{and you were saying that you wanted to come to singapore}'' and ``\textit{un maybe can i have a little bit more details like uh when will you be coming}''.
Although content-aware contexts may help understanding, the most recent contexts may be more important than others.
In the same example, the second utterance is more related to the \textit{when} question, so the temporal information can provide additional cues for the attention design.

This paper focuses on investigating various attention mechanism in neural models with contextual information and speaker role modeling for language understanding. 
% This paper focuses on SLU and policy learning, which targets the understanding of tourist's natural language (SLU) and the prediction of how the system should respond (SAP) respectively.
In order to comprehend what tourist is talking about and imitate how guide react to these meanings, this work proposes a role-based contextual model by modeling role-specific contexts differently for improving the system performance and further design associated time-aware and content-aware attention mechanisms.

% Figure
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{framework.pdf}
%\vspace{-5mm}
\caption{Illustration of the proposed attentional contextual model.}
 %\vspace{-3mm}
\label{fig:model}
\end{figure*}

\section{Proposed Approach}
\label{sec:model}

The model architecture is illustrated in Figure~\ref{fig:model}.
First, the previous utterances are fed into the contextual model to encode into the history summary, and then the summary vector and the current utterance are integrated for helping understanding.
The contextual model leverages the attention mechanisms illustrated in the red block, which implements different attention types and attention levels. 
%four attention mechanisms including 1) content-aware attention, 2) time-aware attention, 3) sentence-level attention and 4) role-level attention.
The whole model is trained in an end-to-end fashion, where the history summary vector and the attention weights are automatically learned based on the downstream SLU task.
The objective of the proposed model is to optimize the conditional probability $p(\mathbf{\hat{y}}\mid \mathbf{x})$, so that the difference between the predicted distribution and the target distribution, $q(y_k=z\mid \mathbf{x})$, can be minimized:
%The objective function is to minimize the predicted errors:
\begin{equation}
\mathcal{L}=-\sum_{n=1}^{N}\sum_{k=1}^{K}q(y_k=z\mid \mathbf{x}) \log p(\hat{y_k}=z\mid \mathbf{x}),
\end{equation}
where $n$ is the number of samples and the labels $\textbf{y}$ are the labeled intent tags for understanding.

\subsection{Contextual Language Understanding }
\label{ssec:clu}
% undefined
% \paragraph{Language Understanding (SLU)}
Given the current utterance $\textbf{x}=\{w_t\}^T_1$, the goal is to predict the user intents of $\textbf{x}$, which includes the speech acts and associated attributes shown in Figure~\ref{fig:example}.
We apply a bidirectional long short-term memory (BLSTM) model~\cite{schuster1997bidirectional} to integrate preceding and following words to learn the probability distribution of the user intents.
\begin{eqnarray}
\label{eq:basic}
\textbf{v}_\text{cur} &=& \text{BLSTM}(\textbf{x}, W_\text{his}\cdot \textbf{v}_\text{his}),\\
\textbf{o} &=& \mathtt{sigmoid}(W_\text{SLU}\cdot \textbf{v}_\text{cur}),
\end{eqnarray}
where $W_\text{his}$ is a dense matrix and $\textbf{v}_\text{his}$ is the history summary vector, $\textbf{v}_\text{cur}$ is the context-aware vector of the current utterance encoded by the BLSTM, and $\textbf{o}$ is the intent distribution.
Note that this is a multi-label and multi-class classification, so the $\mathtt{sigmoid}$ function is employed for modeling the distribution after a dense layer.
The user intent labels $\textbf{y}$ are decided based on whether the value is higher than a threshold $\theta$ tuned by the development set.
% the mission is finding the correct user intents and corresponding attributes.
%The entry of the final layer output $\hat{o_t}$ is picked if and only if its value is higher than a threshold:
%\begin{gather*}
%\hat{y_{t, k}} =
%  \begin{cases}
%    1 & \text{if } \hat{o}_{t, k} > \mathtt{threshold}. \\
%    0 & \text{otherwise}.\\
%  \end{cases}
%\end{gather*}
\subsection{Speaker Role Contextual Module}
\label{ssec:contexualmodel}
In order to leverage the contextual information, we utilize the prior contexts from two roles to learn history summary representations, $\textbf{v}_\text{his}$ in (\ref{eq:basic}).
The illustration is shown in the left-right part of Figure~\ref{fig:example}.

%\subsubsection{Semantic Label}
% \paragraph{Semantic Label}
%In order to model the history using explicit semantics, the annotated semantic labels including the intents and associated attributes from history utterances may 
%Given a sequence of annotated intent tags and associated attributes of each history utterance, we employ a BLSTM to model the explicit semantics:
%\begin{equation}
%\textbf{v}_\text{his} = \text{BLSTM}(\text{intent}_t),
%\label{eq:tag}
%\end{equation}

%\subsection{Speaker Role Modeling}
%\label{ssec:rolebasedmodel}
%Some previous work stored multiple history utterances to leverage the knowledge in contexts~\cite{chen2016end,yang2017end}, we follow this idea and extend it to model different speaker roles separately.

In a dialogue, there are at least two roles communicating with each other, each individual has his/her own goal and speaking habit.
For example, the tourists have their own desired touring goals and the guides' goal is try to provide the sufficient touring information for suggestions and assistance.
Prior work usually ignored the speaker role information or only modeled a single speaker's history for various tasks~\cite{chen2016end,yang2017end}.
The performance may be degraded due to the possibly unstable and noisy input feature space.
%However, previous methods usually simply concatenate utterances of different individuals to model dialogue contexts for various tasks (eg. SLU). 
%We envisage this would cause significant affect to performance due to noisy input feature space. 

To address this issue, this work proposes the role-based contextual model: instead of using only a single BLSTM model for the history, we utilize one individual contextual module for each speaker role.
Each role-dependent recurrent unit $\text{BLSTM}_{\text{role}_i}$ receives corresponding inputs, $x_{t,\text{role}_i}$, which includes multiple utterances $u_i$ ($i=[1, ..., t-1]$) preceding to the current utterance $u_t$ from the specific role, $\text{role}_i$, and have been processed by an encoder model.
% ($i=[1, ..., N]$)
\begin{eqnarray}
%\label{eq:nl2}
%\textbf{v}_\text{his} &=& \text{BLSTM}_{\text{role}_a}(\text{CNN}(\text{utt}_{t,\text{role}_a}))\\\nonumber
\label{eq:tag2}
%&+& \text{BLSTM}_{\text{role}_b}(\text{CNN}(\text{utt}_{t,\text{role}_b}))\\
%\textbf{v}_\text{his} &=& \text{BLSTM}_{\text{role}_a}(\text{intent}_{t,\text{role}_a}) \\
%&+& \text{BLSTM}_{\text{role}_b}(\text{intent}_{t,\text{role}_b}),\nonumber
\textbf{v}_\text{his} &=& \textbf{v}_{\text{his}, \text{role}_a} + \textbf{v}_{\text{his}, \text{role}_b}\\
&=& \text{BLSTM}_{\text{role}_a}(x_{t,\text{role}_a}) + \text{BLSTM}_{\text{role}_b}(x_{t,\text{role}_b}),\nonumber
\end{eqnarray}
where $x_{t, \text{role}_i}$ contains vectors after one-hot encoding that represent the annotated intent and the attribute features.
Note that this model requires the ground truth annotations for history utterances for training and testing.
Therefore, each role-based contextual module focuses on modeling role-dependent goals and speaking style, and $\textbf{v}_\text{cur}$ from (\ref{eq:basic}) would contain role-based contextual information.

\subsection{Neural Attention Mechanism}
One of the earliest work with a memory component applied to language processing is memory networks~\cite{weston2015memory,sukhbaatar2015end}, which encode mentioned facts into vectors and store them in the memory for question answering (QA).
%Following their success, Xiong et al. proposed dynamic memory networks (DMN) to additionally capture position and temporality of transitive reasoning steps for different QA tasks~\cite{xiong2016dynamic}.
%Chen et al. also applied the memory mechanism to extract content-aware knowledge for the language understanding task~\cite{chen2016end}.
The idea is to encode important knowledge and store it into memory for future usage with attention mechanisms.
Attention mechanisms allow neural network models to selectively pay attention to specific parts.
There are also various tasks showing the effectiveness of attention mechanisms~\cite{xiong2016dynamic,chen2016end}. 
This paper focuses on two attention types: content-aware ($\alpha^C$) and time-aware ($\alpha^T$), and two attention levels: sentence ($\alpha_{u_i}$) and role ($\alpha_{r_i}$) illustrated in Figure~\ref{fig:model}.
The following sections first describe content-aware and time-aware attention mechanism using the sentence-level structure, and the role-level attention is explained afterwards.
%We first design content-aware attention mechanisms to capture two basic insights about dialogue structure, that is, sentences and roles.
\subsubsection{Content-Aware Attention}
Given the utterance contexts, the model can learn the attention to decide where to focus more for better understanding the current utterance~\cite{weston2015memory,chen2016end}.
In the content-aware attention, it is intuitive to use the semantic relation between the current utterance representations and the history context vector as the attention weight, which
%For sentence-level attention,  semantic similarity between sentence representation of the current utterance and history context vector 
is used as a measurement of how much the model should focus on a specific preceding utterance:
%\begin{equation}
%\textbf{Attention}(\text{intent}_{\text{role}_i}) = \mathtt{softmax}(\textbf{cos}(\textbf{v}_\text{cur}, \text{intent}_{\text{role}_i}))
%\end{equation}
\begin{equation}
\label{eq:att_c}
%\textbf{Attention}(\text{intent}_{\text{role}_i}) =
\alpha^C_{u_i} = \mathtt{softmax}(M_{\text{S}}(\textbf{v}_\text{cur} + \textbf{x}_i)),
\end{equation}
where $\alpha^C_{u_i}$ is the content-aware attention vector highlighted as blue texts in Figure~\ref{fig:model}, the $\mathtt{softmax}$ function is used to normalize the attention values of the history utterances, and $M_\text{S}$ is an MLP for learning the attention weight given the current representation $\textbf{v}_\text{cur}$ and $\textbf{x}_i$ is the vector of $u_i$ in the history. 
%Another trial is to use a MLP with the concatenation of sentence representation of the current utterance and history context vector of as input: 

Then when computing the history summary vector, the BLSTM encoder additionally considers the corresponding attention weight for each history utterance in order to emphasize the content-related contexts.
(\ref{eq:tag2}) can be rewritten into
% \begin{eqnarray}
% \label{eq:his_att_c}
% \textbf{v}_\text{his} &=& \text{BLSTM}_{\text{role}_a}(x_{t,\text{role}_a}, \alpha^C_{\text{role}_a}) \\
% &+& \text{BLSTM}_{\text{role}_b}(x_{t,\text{role}_b}, \alpha^C_{\text{role}_b}),\nonumber
% \end{eqnarray}
\begin{eqnarray}
\label{eq:his_att_c}
\textbf{v}_\text{his} = \sum_{i\in \{a,b\}} \text{BLSTM}_{\text{role}_i}(x_{t,\text{role}_i}, \{ \alpha^C_{u_j} \mid u_j \in \text{role}_i\}).
\end{eqnarray}

\subsubsection{Time-Aware Attention}
Intuitively, most recent utterances contain more relevant information; therefore we introduce time-aware attention mechanism which computes attention weights by the time of utterance occurrence explicitly.
We first define the time difference between the current utterance and the preceding sentence as $d(u_i)$ for each preceding utterance $u_i$, and then simply use its reciprocal as the attention value, $\alpha^T_{u_i}$:
\begin{equation}
\alpha^T_{u_i} = \frac{1}{d(u_i)}.
\end{equation}

Here the importance of a earlier history sentence would be considerably compressed.
For the sentence-level attention, before feeding into the contextual module, each history vector is weighted by its reciprocal of distance.

% \begin{eqnarray}
% \textbf{v}_\text{his} &=& \text{BLSTM}_{\text{role}_a}(x_{t,\text{role}_a}, \alpha^C_{\text{role}_a}\cdot \alpha^T_{\text{role}_a}) \\
% &+& \text{BLSTM}_{\text{role}_b}(x_{t,\text{role}_b}, \alpha^C_{\text{role}_b}\cdot \alpha^T_{\text{role}_b}),\nonumber
% \end{eqnarray}

\begin{eqnarray}
\textbf{v}_\text{his} = \sum_{i\in \{a,b\}} \text{BLSTM}_{\text{role}_i}(x_{t,\text{role}_i}, \{ \alpha^C_{u_j}\cdot \alpha^T_{u_j} \mid u_j \in \text{role}_i\}).
\end{eqnarray}


\subsubsection{Dynamic Speaker Role Attention}

Switching to role-level attention, a dialogue is disassembled from a different perspective about which speaker's information is more important.
%The role-specific context information $\textbf{v}_{\text{his}, \text{role}_i}$ is aggregated by a recurrent unit:
%\begin{equation*}
%\textbf{v}_{\text{his}, \text{role}_i} = \text{BLSTM}_{\text{role}_i}(\text{intent}_{t,\text{role}_i})
%\end{equation*}
From (\ref{eq:tag2}), we have the role-dependent history summary representations, $\textbf{v}_{\text{his}, \text{role}_i}$. 
The role-level attention is to decide how much to address on different speaker roles' contexts in order to better understand the current utterance.
\begin{equation}
\alpha^C_{r_i} = \mathtt{softmax}(M_R(\textbf{v}_\text{cur} + \textbf{v}_{\text{his}, \text{role}_i})), 
\end{equation}
where $\alpha_{r_i}$ is the attention weight of the role $i$, and $M_R$ is the MLP for learning the role-level content-aware attention weights, while $M_S$ in (\ref{eq:att_c}) is at the sentence level.

In terms of time-aware attention, experimental results show that the importance of a speaker given the contexts can be approximated to the minimum distance among the speaker's utterances\footnote{Different settings (min, avg, ) were attempted.}.
\begin{equation}
\alpha^T_{r_i} = \min \frac{1}{d(u_j)},
\end{equation}
where $u_j$ includes all contextual utterances from the speaker $r_i$.

%\begin{equation}
%\textbf{Attention}(\text{role}_i) = \mathtt{softmax}(\text{Dense}(\textbf{v}_\text{cur} + \textbf{v}_{\text{his}, \text{role}_i}))
%\end{equation}
With content-aware or time-aware attention at the role level,
the sentence-level history result from (\ref{eq:his_att_c}) can be rewritten into
\begin{equation}
\textbf{v}_\text{his} = \alpha^{C/T}_{\text{role}_a} \cdot \textbf{v}_{\text{his}, \text{role}_a}
+ \alpha^{C/T}_{\text{role}_b}\cdot \textbf{v}_{\text{his}, \text{role}_b},
\end{equation}
for combining role-dependent history vectors with their attention weights.

\subsection{End-to-End Training}
The objective is to optimize SLU performance, predicting multiple speech acts and attributes described in \ref{ssec:clu}.
In the proposed model, all encoders, prediction models, and attention weights (except time-aware attention) can be automatically learned in an end-to-end manner.

%From the viewpoint of role-level, the importance of a speaker's spoken context can be approximated to either the average distance of his/her previous utterances or the minimum distance among the utterances.  

\begin{table*}
\centering
%\begin{tabular}{ | l c l | c | c | c | }
%    \hline
%     \bf Model & & \bf Attention Level & \bf Tourist & \bf Guide & \bf ~~All~~ \\ \hline\hline
%    Baseline & (a) & \emph{DSTC4-Best 1} & 52.1 & 61.2 & 57.8 \\
%    & (b) & \emph{DSTC4-Best 2} & 51.1 & 67.4 & 61.4 \\
%    & (c) & w/o context & 63.2 & 69.2 & 66.6 \\
%    & (d) & w/ context w/o speaker role & 68.4 & 74.0 & 71.6 \\
%    & (e) & w/ context w/ speaker role & 68.8 & 75.1 & 72.1\\\hline
%    Content-Aware Attention & (f) & Sentence & 68.7 & 73.2 & 71.0\\
%    & (g) & Role & 68.6 & 73.6 & 71.3\\
%    & (h) & Both & 67.7 & 73.5 & 70.8\\\hline
%    Time-Aware Attention & (i) & Sentence & 70.3$^\dag$ & 76.9$^\dag$  & \bf 74.9$^\dag$ \\
%    & (j) & Role & 70.2$^\dag$  & 77.4$^\dag$  & 74.7$^\dag$ \\
%    & (k) & Both & 70.2$^\dag$  & 77.4$^\dag$  & 74.7$^\dag$ \\\hline
%    Content + Time Attention & (l) & Sentence & 69.7$^\dag$  & 75.3  & 73.0$^\dag$ \\
%    & (m) & Role & 70.1$^\dag$  & \bf 77.5$^\dag$  & 74.2$^\dag$ \\
%    & (n) & Both & \bf 70.7$^\dag$  & 77.0$^\dag$  & 74.2$^\dag$ \\
%    \hline
%  \end{tabular} 
  \begin{tabular}{ | l c l | c | c | c | }
    \hline
     \bf Model & & \bf Attention Level & \bf Tourist & \bf Guide & \bf ~~All~~ \\ \hline\hline
    Baseline & (a) & \emph{DSTC4-Best 1} & 52.1 & 61.2 & 57.8 \\
    & (b) & \emph{DSTC4-Best 2} & 51.1 & 67.4 & 61.4 \\
    & (c) & w/o context & 63.2 & 69.2 & 66.6 \\
    & (d) & w/ context w/o speaker role & 68.3 & 74.4 & 71.6 \\
    & (e) & w/ context w/ speaker role & 69.1 & 74.4 & 72.1\\\hline
    Content-Aware Attention & (f) & Sentence & 68.5 & 73.6 & 71.6\\
    & (g) & Role & 68.6 & 74.0 & 71.8\\
    & (h) & Both & 67.9 & 73.5 & 71.5\\\hline
    Time-Aware Attention & (i) & Sentence & \bf 70.5$^\dag$ & \bf 77.7$^\dag$  & \bf 74.6$^\dag$ \\
    & (j) & Role & 70.0$^\dag$  & 76.8$^\dag$  & 74.2$^\dag$ \\
    & (k) & Both & 70.4$^\dag$  & 77.3$^\dag$  & 74.5$^\dag$ \\\hline
    Content-Aware + Time-Aware Attention & (l) & Sentence & 69.7$^\dag$  & 76.7$^\dag$  & 74.0$^\dag$ \\
    & (m) & Role & \bf 70.1$^\dag$  & \bf 77.1$^\dag$  & \bf 74.1$^\dag$ \\
    & (n) & Both & 69.5$^\dag$  & 76.2$^\dag$  & 73.5$^\dag$ \\
    \hline
  \end{tabular}
  
  %\vspace{-2mm}
\caption{Spoken language understanding performance reported on F-measure in DSTC4 (\%). $^\dag$ indicates the significant improvement compared to all baseline methods.}
\label{tab:res}
%\vspace{-4mm}
\end{table*}

\section{Experiments}
\label{sec:experiments}
To evaluate the effectiveness of the proposed model, we conduct the language understanding experiments on human-human conversational data.
% To evaluate the effectiveness of the proposed model, we conduct the SLU and policy learning experiments on human-human conversational data. 
%, which contains richer and more challenging dialogue interactions.
%The Dialogue State Tracking Challenge (DSTC) series has provided a common evaluation framework accompanied by labeled datasets ~\cite{williams2016dialog}. In this framework, the datasets come with a domain ontology which describes the types of user intents. The ontology defines a collection of slots and the values that each slot can take. In this paper, we took two pilot tasks (language understanding: predict speech acts and semantic slots by a given utterance; speech act prediction: predict the speech act of the next turn imitating the policy of one speaker) of DSTC4 ~\cite{kim2017fourth} as the experiment environment, which consists of 35 dialog sessions on touristic information for Singapore collected from Skype calls between three tour guides and 35 tourists. All the recorded dialogs with the total length of 21 hours have been manually transcribed and annotated with speech act and semantic labels for each turn level.

\subsection{Setup}
\label{ssec:settings}
%\subsection{Dataset}
The experiments are conducted on DSTC4, which consists of 35 dialogue sessions on touristic information for Singapore collected from Skype calls between 3 tour guides and 35 tourists~\cite{kim2016fourth}. 
All recorded dialogues with the total length of 21 hours have been manually transcribed and annotated with speech acts and semantic labels at each turn level.
The speaker information (guide and tourist) is also provided.
%which contain touristic information in Singapore from Skype calls between tour guides and tourists. 
Unlike previous DSTC series collected human-computer dialogues, 
human-human dialogues contain rich and complex human behaviors and bring much difficulty to all the tasks.
Given the fact that different speaker roles behave differently and longer contexts, DSTC4 is a suitable benchmark dataset for evaluation.

We choose the mini-batch \textit{adam} as the optimizer with the batch size of 256 examples.
The size of each hidden recurrent layer is 128.
%, and the size of output layer in SLU is $M+N$, where $M$ and $N$ are the number of user intents and slot tags respectively, while the size of output layer in SAP task is $M$. 
We use pre-trained 200-dimensional word embeddings $GloVe$~\cite{pennington2014glove}.
%, where the dimension of word embeddings is 200. 
We only apply 30 training epochs without any early stop approach. 
%We choose intent prediction part of SLU task and SAP task in our experiments, the evaluation metric is token-level F1 score, and the best performance of each model configuration is recorded (see Table 1). 
%We utilize an encoder module to extract features from the inputs, the encoder can be either bidirectional LSTM module which the size of the network is same as other recurrent unit, 
%The sentence encoder is implemented using a CNN with the flat filters of size $[2, 3, 4]$, 128 filters each size, and max pooling over time.
%In CNN encoder, max pooling over time is applied after convolving, the idea is to capture the most important feature (the highest value) for each feature map. This pooling scheme naturally deals with variable sentence lengths~\cite{kim2014convolutional}.
We focus on predicting multiple labels including intents and attributes, so the evaluation metric is average F1 score for balancing recall and precision in each utterance.
The experiments are shown in Table~\ref{tab:res}, where we report the average results over five runs. %for the tourist, the guide, and all.
% Note that SLU may contain multiple intents and multiple attributes while policy only contain a single intent and a single attribute.

\subsection{Baseline Results}
%\subsection{Language Understanding Results}
%\label{ssec:luresults}
% \subsection{Language Understanding}

The baselines (rows (a) and (b)) are two of the best participants of DSTC4 in IWSDS 2016~\cite{kim2016fourth} (best results for tourist and guide understanding respectively).
It is obvious that tourist intents are much more difficult than guide intents (most systems achieved higher than 60\% of F1 for guide intents but lower than 50\% for tourist intents), because the guides usually follow similar interaction patterns. 
In our experiments, we focus more on the tourist part, because SLU in a dialogue system is to understand the users, who ask for assistance.

The baseline (c) only takes the current utterance into account without any history information, and then applies a simple BLSTM for SLU. 
The baselines (d) and (e) leverage the semantic labels of contexts for learning history summary vectors, achieving the improvement compared to the results without contexts.
%With contextual history in the baseline (d), using ground truth semantic labels for learning history summary vectors has improvement in performance to 68.4\%, 74.0\%, 71.6\% for understanding tourist, guide, both-side respectively.
The baseline (e) that uses history information and conducts our role-based contextual model to capture role-specific information separately slightly improves the SLU performance for both tourist and guide, achieving 69.1\% and 74.4\% for tourist and guide understanding respectively.

%we pick the best entry of intent prediction in SLU-Tourist category among all the submission, which is $52.1\%$. 
% The second baseline models the current utterance only without contexts.
%is to use only current utterance without history information and a simple bidirectional LSTM.

% With contextual history, using ground truth semantic labels for learning history summary vectors slightly improves the performance to 64.1\%, while using natural language slightly degrade the performance to 62.1\%.
% The reason may be that NL utterances contain too many noises to model contextual vectors useful for SLU.
% The proposed role-based contextual models applying on semantic labels and NL achieve 67.3\% and 64.4\% on F1 respectively, showing the significant improvement all model without role modeling.
%For employing contextual model, the first trial is to add the preceding utterance to the second baseline, however, the performance had a slight drop unexpectedly. Our analysis is that more inputs might result in considerable noise due to complex behaviors between human-human conversations. To address this problem, our role-based contextual model is employed, the performance has instant improvement by this simple concept.
% Furthermore, adding the intermediate guidance acquires additional improvement (65.7\%).

%\subsection{Policy Learning Results}
%\label{ssec:policyresults}
%\subsection{Policy Learning}
% To predict the guide's next action, the baseline utilizes intent tags of the current utterance without contexts.
%, the model structure and the configurations are same as in SLU task, while 
% Table~\ref{tab:res} shows the similar trend as SLU results, where applying either role-based contextual model or intermediate guidance brings advantages for both semantics-encoded and NL-encoded history.

%In contrast to natural language, semantic labels (intent-attribute pairs) can be seen as more explicit and concise information for modeling the history, which indeed gains more in our experiments. 
% for both SLU and policy.
% Among the experiments of Contextual-NL, which are more practical because the annotated semantics is not required during testing, the proposed approaches achieve 5.0\% and 7.7\% relative improvement compared to the baseline for SLU and policy learning respectively.
%improves the SLU performance from 62.6\% to 65.7\% and the policy results from 62.4\% to 67.2\%.
%Feeding intent tags into our role-base contextual model gives significant improvement in performance on the baseline (from 62.4\% to 75.1\%).

\subsection{Content-Aware Attention}
For the content-aware attention, our model learns the importance of each utterance on both sentence-level and role-level in an end-to-end fashion.
The table shows that the learned attention for both sentence-level and role-level does not yield improvement compared to the baseline (e). 
The probable reason is that the content-aware importance can be handled by the BLSTM when producing the history summary, and the learned attention does not provide additional cues for improvement.
%, while learned role-level attention reaches similar result.
%However, the guide understanding performance does not show any improvement compared to the baseline (e), 
%Probably because guide understanding is much easier than tourist understanding, and the learned content-aware attention does not provide additional cues for improvement.

\subsection{Time-Aware Attention}
With the time-aware attention, using the reciprocal of distance as attention value significantly improves the performance to about 70\% (tourist) and about 77\% (guide) using either sentence-level or role-level attention. 
The reason that the performance among different levels of time-aware mechanisms are close may be that the closest utterance is capable of capturing the importance of the history.
Using the reciprocal of minimum distance among tourist/guide history as their attention weights in the role-level attention can effectively pay the correct attention to the salient information, achieving promising performance.

However, the table shows that the sentence-level attention is slightly better than role-level attention for both tourist and guide parts when considering time-aware attention.
%while the opposite holds for guide part. 
The reason may be that the intents are very diverse and hence require the more precise focus on the content-related history in order to achieve correct understanding results. 
Also, the results using both sentence-level and role-level attention do not differ a lot from the results using only sentence-level attention.

In addition to the current setting for time-aware attention, other trends of temporal decay can be further investigated.
We leave this part as the future work.
%While from the viewpoint of the guide, since it is easier to understand the intents, using only the reciprocal of closest distance is already capable of capturing the importance of history.
%Moreover, using sentence-level attention in guide part may further introduce noises and thus yield worse performance.

\subsection{Content-Aware and Time-Aware Attention}
The proposed model includes two attention types, content-aware and time-aware, and we integrate both types into a single model to analyze whether their advantages can be combined.
The rows (l)-(n) are the results using both content-aware and time-aware attention weights, but the performance is similar to the time-aware results.
For the results using only role-level attention, combining content-aware and time-aware attention weights obtains the improvement (row (j) v.s row (m)). 
%Another difference of adding content-aware attention weights is that using only role-level attention performs slightly better than the results  (row (j)).
%The trends between sentence-level and role-level attention are slightly different from the trend 

\subsection{Comparison between Sentence-Level and Role-Level}
\label{ssec:sent_role}
Among the experiments using either content-aware attention or time-aware attention or both, we introduce both sentence-level and role-level mechanisms for analysis. 
From Table~\ref{tab:res}, rows (i) and (l) show that the sentence-level attention can benefit the results even though we do not leverage the content-aware attention.
On the other hand, rows (j) and (m) show that the role-level attention requires content-related information in order to achieve better performance.
The reason is probably that the role-level attention is not precise enough to capture the salience of utterances.
Therefore, combining with content-aware cues can effectively focus on the correct part.

%From Table~\ref{tab:res}, rows (f) and (i) show that sentence-level attention benefits the tourist part more, while role-level attention benefits guide part more. 
%The reason is probably that the tourist needs more detailed focus such as sentence-level attention due to its highly diverse intents.
%While for guide understanding, it only needs to focus on the closest utterance since the intent has similar pattern and is easy to understand. 
%To further backup the supposition we made, when content and time attention mechanisms are applied at once, the row (m) still indicates the efficacy of the role-level attention on guide understanding. 
%The row (n) indicates that applying both mechanisms on the tourist part can achieve even better performance than merely applying single kind.  

\subsection{Dynamic Speaker Role Attention Analysis}
To further analyze the dynamic role-level attention, we compute the mean of the learned attention weights.
The results are shown in Table~\ref{tab:role_att}.

\begin{table}
%\small
\centering
\begin{tabular}{ | c | c | c | c | c | c | }
    \hline
    \multirow{2}{*}{Task} & \multicolumn{2}{|c|}{Role-Level Attention Weight}\\
    \cline{2-3}
 & Tourist Context & Guide Context\\
\hline \hline
Tourist Understanding & 0.48 & 0.52\\
Guide Understanding & 0.30 & 0.70\\
    \hline
  \end{tabular}
  %\vspace{-2mm}
\caption{The average role-level attention weight learned from the proposed model using the role-level attention mechanism.}
\label{tab:role_att}
\end{table}

For tourist understanding, the tourist and the guide attention weights are 0.48 and 0.52 respectively. % (with variance 0.13).
We can therefore conclude that both tourist and guide history are likewise important in order to understand the tourist utterances.
The observation matches our analysis about tourist understanding in \ref{ssec:sent_role} --- since tourist intents are highly diverse, SLU needs to extract as much information as possible from both speaker roles to help understanding.
In addition, it is reasonable that tourist understanding focuses on guide history a little bit more than tourist history under a dialogue scenario. 

For guide understanding, the tourist and the guide attention weights are 0.30 and 0.70 respectively. % (with variance 0.15).
The remarkable difference of role-level attention can be explained by the data characteristics of DSTC4.
In the tourist and guide dialogues, it is obvious that the guides usually follow the same interactive patterns, such as suggesting a famous location first and then explaining it.
In other words, the tourist contexts do not matter a lot when understanding guide utterances, because the guide only needs to follow a pattern to interact with the tourist.
This also explains why guide understanding always achieves higher performance compared to the tourist part in our experiments.

To further verify whether the learned attention focuses on the right part, we conduct a similar experiment as the row (d) in Table~\ref{tab:res}.
Instead of using preceding utterances from both speakers as history information,
%concatenating all the history together, we divide the history into tourist and guide parts. 
when understanding tourist utterances, we use only tourist history; likewise, guide understanding only takes guide history into account.
The performance of tourist understanding drops from 68.3\% to 65.9\%, while the performance of guide understanding remains almost the same (from 74.4\% to 73.8\%).
The trend proves that the content-aware attention mechanism is capable of focusing on the correct part for better understanding.
In the future, we would like to investigate whether the proposed model can generalize to the scenarios with more than two speaker roles.

\subsection{Overall Results}
From the above experimental results, the proposed time-aware attention model significantly improves the performance, and sentence-level and role-level attention has different capacities of improving understanding performance, where sentence-level attention is useful even though we do not consider content-aware information, and role-level attention is more effective when combining both content-aware and time-aware attention.
%for tourist understanding and role-level attention is better for guide understanding.

In sum, the best results are time-aware attention with sentence-level attention (row (i)) and content-aware and time-aware attention with role-level attention (row (m)), reaching higher than 70\% and 77\% on F-measure for tourist understanding and guide understanding respectively.
%The best result for guide understanding uses content-aware and time-aware attention with only role-level attention, achieving 77.5\% on F-measure.
Therefore, the proposed attention mechanisms are demonstrated to be effective for improving SLU in such complex human-human conversations.

\section{Conclusion}
This paper proposes an end-to-end attentional role-based contextual model that automatically learns speaker-specific contextual encoding and investigates various content-aware and time-aware attention mechanisms on it.
Experiments on a benchmark multi-domain human-human dialogue dataset show that the time-aware and role-level attention mechanisms provide additional cues to guide the model to focus on the salient contexts, and achieve better performance on spoken language understanding for both speaker roles. 
Moreover, the proposed time-aware and role-level attention mechanisms are easily extendable to multi-party conversations, and we leave the extension of other variants for the future work.

%our role-based model achieves impressive improvement in language understanding, demonstrating that different speaker roles behave differently and focus on different goals. 
%This work proposes dynamic speaker role attention on top of the proposed role-based contextual model, with further combining the attention mechanisms, our model gains more improvement in performance on understanding every speakers. 
%The idea is motivated by huge variance of habits between different speaker, and the concept can be easily-extended in various research topics. 
%The role-based concept can be easily extended to various research topics in the future.

\section{Acknowledgements}
We would like to thank reviewers for their insightful comments on the paper.
The authors are supported by the Ministry of Science and Technology of Taiwan and MediaTek Inc..

\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}

\section{Formatting your paper}
\label{sec:format}

All printed material, including text, illustrations, and charts, must be kept
within a print area of 7 inches (178 mm) wide by 9 inches (229 mm) high. Do
not write or print anything outside the print area. The top margin must be 1
inch (25 mm), except for the title page, and the left margin must be 0.75 inch
(19 mm).  All {\it text} must be in a two-column format. Columns are to be 3.39
inches (86 mm) wide, with a 0.24 inch (6 mm) space between them. Text must be
fully justified.

\section{PAGE TITLE SECTION}
\label{sec:pagestyle}

The paper title (on the first page) should begin 1.38 inches (35 mm) from the
top edge of the page, centered, completely capitalized, and in Times 14-point,
boldface type.  The authors' name(s) and affiliation(s) appear below the title
in capital and lower case letters.  Papers with multiple authors and
affiliations may require two or more lines for this information. Please note
that papers should not be submitted blind; include the authors' names on the
PDF.

\section{TYPE-STYLE AND FONTS}
\label{sec:typestyle}

To achieve the best rendering both in printed proceedings and electronic proceedings, we
strongly encourage you to use Times-Roman font.  In addition, this will give
the proceedings a more uniform look.  Use a font that is no smaller than nine
point type throughout the paper, including figure captions.

In nine point type font, capital letters are 2 mm high.  {\bf If you use the
smallest point size, there should be no more than 3.2 lines/cm (8 lines/inch)
vertically.}  This is a minimum spacing; 2.75 lines/cm (7 lines/inch) will make
the paper much more readable.  Larger type sizes require correspondingly larger
vertical spacing.  Please do not double-space your paper.  TrueType or
Postscript Type 1 fonts are preferred.

The first paragraph in each section should not be indented, but all the
following paragraphs within the section should be indented as these paragraphs
demonstrate.

\section{MAJOR HEADINGS}
\label{sec:majhead}

Major headings, for example, "1. Introduction", should appear in all capital
letters, bold face if possible, centered in the column, with one blank line
before, and one blank line after. Use a period (".") after the heading number,
not a colon.

\subsection{Subheadings}
\label{ssec:subhead}

Subheadings should appear in lower case (initial word capitalized) in
boldface.  They should start at the left margin on a separate line.
 
\subsubsection{Sub-subheadings}
\label{sssec:subsubhead}

Sub-subheadings, as in this paragraph, are discouraged. However, if you
must use them, they should appear in lower case (initial word
capitalized) and start at the left margin on a separate line, with paragraph
text beginning on the following line.  They should be in italics.

\section{PRINTING YOUR PAPER}
\label{sec:print}

Print your properly formatted text on high-quality, 8.5 x 11-inch white printer
paper. A4 paper is also acceptable, but please leave the extra 0.5 inch (12 mm)
empty at the BOTTOM of the page and follow the top and left margins as
specified.  If the last page of your paper is only partially filled, arrange
the columns so that they are evenly balanced if possible, rather than having
one long column.

In LaTeX, to start a new column (but not a new page) and help balance the
last-page column lengths, you can use the command ``$\backslash$pagebreak'' as
demonstrated on this page (see the LaTeX source below).

\section{PAGE NUMBERING}
\label{sec:page}

Please do {\bf not} paginate your paper.  Page numbers, session numbers, and
conference identification will be inserted when the paper is included in the
proceedings.

\section{ILSLUSTRATIONS, GRAPHS, AND PHOTOGRAPHS}
\label{sec:illust}

Illustrations must appear within the designated margins.  They may span the two
columns.  If possible, position illustrations at the top of columns, rather
than in the middle or at the bottom.  Caption and number every illustration.
All halftone illustrations must be clear black and white prints.  Colors may be
used, but they should be selected so as to be readable when printed on a
black-only printer.

Since there are many ways, often incompatible, of including images (e.g., with
experimental results) in a LaTeX document, below is an example of how to do
this \cite{Lamp86}.

\section{FOOTNOTES}
\label{sec:foot}

Use footnotes sparingly (or not at all!) and place them at the bottom of the
column on the page on which they are referenced. Use Times 9-point type,
single-spaced. To help your readers, avoid using footnotes altogether and
include necessary peripheral observations in the text (within parentheses, if
you prefer, as in this sentence).

% Below is an example of how to insert images. Delete the ``\vspace'' line,
% uncomment the preceding line ``\centerline...'' and replace ``imageX.ps''
% with a suitable PostScript file name.
% -------------------------------------------------------------------------
\begin{figure}[htb]

\begin{minipage}[b]{1.0\linewidth}
  \centering
  \centerline{\includegraphics[width=8.5cm]{image1}}
%  \vspace{2.0cm}
  \centerline{(a) Result 1}\medskip
\end{minipage}
%
\begin{minipage}[b]{.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.0cm]{image3}}
%  \vspace{1.5cm}
  \centerline{(b) Results 3}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.0cm]{image4}}
%  \vspace{1.5cm}
  \centerline{(c) Result 4}\medskip
\end{minipage}
%
\caption{Example of placing a figure with experimental results.}
\label{fig:res}
%
\end{figure}


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
%\vfill
%\pagebreak

\section{COPYRIGHT FORMS}
\label{sec:copyright}

You must include your fully completed, signed IEEE copyright release form when
form when you submit your paper. We {\bf must} have this form before your paper
can be published in the proceedings.

\section{REFERENCES}
\label{sec:ref}

List and number all bibliographical references at the end of the
paper. The references can be numbered in alphabetic order or in
order of appearance in the document. When referring to them in
the text, type the corresponding reference number in square
brackets as shown at the end of this sentence \cite{C2}. An
additional final page (the fifth page, in most cases) is
allowed, but must contain only references to the prior
literature.

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------

