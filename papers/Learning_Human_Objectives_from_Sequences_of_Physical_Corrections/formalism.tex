\section{Formalizing Sequences of Physical Corrections}
\label{sec:formalism}

In this section we formalize a physical human-robot interaction setting where one or more robots are performing a task incorrectly. The human expert knows how these robots should behave, and physically corrects the robots to convey the true objective. But the human doesn't interact just once: the human may need to interact \textit{multiple times} in order to correct the robots. Our goal is for these robots to learn the human's objective from this sequence of physical corrections.


\subsection{Task Formulation} 
\label{sec:task_formulation}

We formulate our problem as a discrete-time Markov Decision Process (MDP) $\mathcal{M}=\left(\mathcal{S}, \mathcal{A}, \mathcal{T}, r, \gamma, \rho_{0}\right)$. Here $\mathcal{S} \subseteq \mathbb{R}^{n}$ is the \mx{robot} state space, $\mathcal{A} \subseteq \mathbb{R}^{m}$ is the robot action space, $\mathcal{T}(s, a)$ is the transition probability function, $r$ is the reward, $\gamma$ is the discount factor, and $\rho_{0}$ is the initial distribution.

\smallskip
\prg{Reward.}
Let the robot start from a state $s^0$ at time $t=0$. As the robot completes the task it follows a trajectory of states: $\xi = \{s^0, s^1, \dots, s^T\} \in \Xi$. The human has in mind a trajectory that they prefer for the robot to follow. Recall our motivating example --- here the human wants the robot arms to follow a trajectory that avoids the cabinets without squashing the bag. Similar to prior work \cite{ziebart2008maximum, abbeel2004apprenticeship, osa2018algorithmic, jeon2020reward}, we capture this objective through our reward function:
$R(\xi; \theta) = \theta \cdot \Phi(\xi)$.
Here $\Phi$ denotes the feature counts over trajectory $\xi$, and $\theta$ captures how important each feature is to the human. We let $\xi_R^t$ denote the robot's trajectory at timestep $t$, and we let $\theta^t$ denote the robot's current reward weights.

\smallskip
\prg{Suboptimal Initial Trajectory.} The system of one or more robots starts off with an initial reward function $R(\xi; \theta^0)$, and optimizes this reward function to produce its initial trajectory.
$$\xi_R^{0}=\arg \min _{\xi \in \Xi} \theta^{0} \cdot \Phi(\xi)$$
But this initial trajectory $\xi_R^0$ misses out on what the human really wants --- going back to our example, the robot does not realize that the blue region is wet and it needs to place the bag on the green region.
More formally, the robot's estimated reward function (which is parameterized by $\theta^0$) does not match the human's preferred reward function (parameterized by the true weights $\theta^*$).

\smallskip
\prg{Human Corrections.} The robot learns about the human's reward --- i.e., the true reward weights --- from physical corrections. Intuitively, these corrections are applied forces and torques which push, twist, and guide the robots. To formulate these interactions we must revise our problem definition: let $a_R$ be the robot's action and let $a_H$ be the human's \textit{correction}. In practice, both $a_R$ and $a_H$ could be applied joint torques \cite{bajcsy2017learning, bajcsy2018learning}. Now the overall system transitions based on both human and robot actions: $s^{t+1} = \mathcal{T}\left(s^t, a_{R}+a_{H}\right)$. We use $A_H = \{(t_i, a_H^i), i=1,\ldots, K\}$ to denote a \textit{sequence} of $K$ ordered human corrections $a_H^i$ at time step $t_i$, where $i$ keeps track of order of the corrections.
Our goal is to learn the human's true reward weights from the sequence of corrections $A_H$.

% We denote the robot(s) action with $u_R$. At every time step, the user might physically correct the robot team with $u_H$, and thus the robot transitions from state $x$ to next state $x\prime$ with the dynamics $x\prime = \mathcal{T}\left(x, u_{R}+u_{H}\right)$ instead of $x\prime = \mathcal{T}\left(x, u_{R}\right)$. We use $U_H = \{(t_i, u_R^i), i=1\ldots K\}$ to denote an assembly of all human correction $u_H^i$ at time step $t_i$. And our goal is to learn the ground truth weight from the human corrections $U_H$.


% \prg{Learning from Physical Corrections.}
% Consider two robots are holding a grocery bag to the table as in \figref{toaddfig}. if the robots do not realize the grocery bag is full, it might squeeze the bag to get through the way without touching nearby housewares. However, realizing that the groceries are going to fall out if the robots keep squeezing, the human would intervene and correct its behaviors by physically pushing the robot arms apart. In this work, we explore how to enable robots to better learn from these physical human corrections.


\subsection{Physical Corrections as Observations}
\label{sec:observation}

When robots are performing a task suboptimally, the human expert intervenes to correct those robots towards the right behavior. Going back to our example from Fig.~\ref{fig:front}. The user sees that the robot is making a mistake (moving towards the wet blue region), and physically intervenes. In the process of fixing this first issue, the human is forced to create another problem: by moving the first robot arm away from the blue region, they also move both arms closer together, and start to squash the bag. We note two important characteristics of these corrections: i) each human correction is intentional, and conveys information about the human's objective, but ii) the corrections viewed together may provide more information than isolating each interaction.

Leveraging these corrections, our goal is to find a better estimate of the reward parameters $P(\theta \mid A_H, \xi_R^0)$. We start by applying Bayes' rule:
\begin{equation} \label{eq:bayes}
   P(\theta \mid A_H, \xi_R^0) \propto P(\theta) P(A_H \mid \xi_R^0,\theta)
\end{equation}
In line with prior work \cite{bajcsy2017learning, bajcsy2018learning, losey2018including, bobu2020quantifying}, we will model $P(A_H|\xi_R^0, \theta)$ by mapping each human correction to a \textit{preferred trajectory}. Given the human's correction $(t_i, a_H^i)$, we deform the robot's trajectory to reach $\xi_H^i$. 
% \mxdel{One simple example of this is to let $\xi_H^i = \xi_R^i$ everywhere except the current waypoint, where we shift the trajectory in the direction of the human's applied correction.}
\mx{One simple example of this is to let the robot execute $a_R^{t_i} + a_H^{t_i}$ at this time step $t_i$, and stick to its original action plan $a_R^t$ for future time steps $t > t_i$.} More generally, we propagate the human's applied correction along the robot's current trajectory \cite{losey2017trajectory}:
\begin{eqnarray}
\centering
\begin{aligned}
    \xi_H^1 &= \xi_R^0 + \mu A^{-1} a_H^1\\
    \xi_H^i &= \xi_H^{i-1} + \mu A^{-1} a_H^i ,~~ i \in \{2,\ldots, K\}
\end{aligned}
\label{eq:traj_prop}
\end{eqnarray}
Consistent with \cite{losey2017trajectory} and \cite{dragan2015movement}, $\mu$ and $A$ are hyperparameters that determine the deformation shape and size. We emphasize that here the robot is not yet learning --- instead, it is locally modifying its trajectory in the direction of the applied correction. Within our motivating example, let the human apply a force pushing the first robot arm away from the blue region. Equation (\ref{eq:traj_prop}) maps this correction to $\xi_H$, a trajectory that moves the robot arm farther from the blue region than $\xi_R$. In Fig.~\ref{fig:propogation_traj}, we demonstrate how a sequence of corrections lead to a sequence of trajectories that enable the robot to correct its path and reach the preferred goals.


% \subsection{pHRI as Observations of Reward Function}
% \label{sec:observation}
% When robots perform a task suboptimally, the human intervenes to correct the robots for higher rewards. These interventions are intentional and informative, and thus we model these human's physical corrections as observations towards the ground truth reward parameter. However, directly modeling $P(U_H|\xi_R^0; \theta)$ is difficult as it requires computing $Q$-value function for that $\theta$ \cite{bajcsy2017learning}. Therefore, instead of directly relating $U_H$ to $\theta$, we interpret each human correction $(t_i, u_H^i)$ with a intermediate corresponding trajectory $\xi_H^i$ as in \cite{bajcsy2017learning}. Formally, for each correction $(t_i, u_H^i)$, we propagate current correction based on previous trajectory $\xi_H^{i-1}$ to get deformed trajectory \cite{losey2017trajectory}:
% \begin{eqnarray}
% \centering
% \begin{aligned}
%     \xi_H^1 &= \xi_R^0 + \mu A^{-1} u_H^1\\
%     \xi_H^i &= \xi_H^{i-1} + \mu A^{-1} u_H^i ,~~ i \geq 2
% \end{aligned}
% \label{eq:traj_prop}
% \end{eqnarray}
% where $\mu$ controls the scale for deformation, $A$ is the differencing matrix
% Returning to our running example. 

Now that we have this tool for mapping corrections to preferred trajectories, we can rewrite Equation~(\ref{eq:bayes}):
\begin{eqnarray}
\begin{aligned}
    P(\theta \mid A_H, \xi_R^0) &\propto P(\theta) P(A_H \mid \xi_R^0, \theta) \\
    &= P(\theta)P\Big((t_1, a_H^1), \ldots, (t_K, a_H^K) \mid \xi_R^0, \theta\Big) \\
    &\approx P(\theta)P(\xi_H^1, \ldots, \xi_H^K \mid \xi_R^0, \theta)
\end{aligned}
\label{eq:bayesian}
\end{eqnarray}
Here $P(\theta)$ is the robot's prior over the human's objective, and $P(\xi_H^1, \ldots, \xi_H^K \mid \xi_R^0, \theta)$ is the likelihood that the human provides a specific \textit{sequence} of preferred trajectories given the robot's initial behavior $\xi_R^0$ and the reward weights $\theta$.


\begin{figure*}[t]
	\begin{center}
		\includegraphics[width=1.8\columnwidth]{figs/propagation_traj.png}

		\caption{An example of a sequence of human corrections along with her corresponding correction trajectories $\xi_H^1, \xi_H^2, \xi_H^3, \xi_H^4$ to guide the robot to place the grocery bag on the green region while avoiding any stretching or squeezing of the bag.}
		\label{fig:propogation_traj}
	\end{center}
	
 	\vspace{-2em}
\end{figure*}

