\section{Introduction}
\label{intro}

Imagine that you just got back from the store, and a two-armed personal robot is helping you unpack a bag of groceries (see Fig.~\ref{fig:front}). You don't want this robot to bump the bag into any cabinets or the hat on the left, but you also don't want the robot to stretch and rip the bag, or squeeze it and crush your groceries. Out of the corner of your eye, you notice that the robot is heading towards a side of the table that is wet (the blue region in the figure). So you \textit{physically correct} it --- pushing, pulling, or twisting the robot's arms to guide it away from that region and move it toward the green region on the left. Importantly, you can only physically interact with one arm at a time, since it's difficult to pay attention to how to correct both arms simultaneously: in the process of guiding this arm away from the obstacle, you might inadvertently move both arms closer together, squeezing the bag. Teaching this robot about all your preferences requires more than just one physical correction. You must provide a \textit{sequence}: alternating between fixing the position of the arm closest to the obstacle, and adjusting the other arm so that the bag is held correctly.

\begin{figure}[t]
    \vspace{0.5em}
	\begin{center}
		\includegraphics[width=\columnwidth]{figs/front.png}

		\caption{Learning from physical sequences. Two robot arms are carrying a grocery bag toward an undesirable wet region, blue region on the right. The human provides a sequence of physical corrections to guide the robot toward their preferred objective, i.e., placing the bag on the green region while also avoiding the obstacles on the left, and holding the bag upright without squeezing or stretching it. }

		\label{fig:front}
	\end{center}
	
	\vspace{-2em}
	
\end{figure}

State-of-the-art methods \textit{learn} from physical corrections by treating each interaction as an \textit{independent} event \cite{bajcsy2017learning, bajcsy2018learning, losey2018including, bobu2020quantifying}. These works assume that the human makes corrections based only on their objective, without considering the other corrections they have already made or are planning to provide. But if we view human corrections as isolated events, we can misinterpret what they convey: for instance, when the human moves one arm away from the hat and closer to the other arm, this robot will mistakenly learn that the human wants to squeeze the bag.

At the other end of the spectrum, robots can learn by looking at the \textit{final} trajectory collectively produced by all of the human's corrections \cite{akgun2012keyframe, argall2009survey, osa2018algorithmic, ratliff2006maximum, jain2015learning}. There are two issues with this: i) the robot does not learn or update its behavior until after the entire task is over, and ii) even the final trajectory may not capture what the human really wants. Returning to our example: because the user can only interact with one arm at a time, the final trajectory has some parts where the distance to the obstacle is right, and other sections where the bag is held correctly --- but the final trajectory fails to capture both throughout.

At their core, prior works miss out on part of the process that humans use to correct the robot's behavior. Not everything can be fixed at once, or even fixed perfectly:
\begin{center}
\vspace{-0.5em}
    \textit{Humans corrections are not independent events --- we often use multiple correlated interactions to correct the robot.}
\vspace{-0.5em}
\end{center}
We leverage this insight to learn the human's reward function online from sequences of physical corrections, \textit{without assuming} that human corrections are conditionally independent. Let's jump back to our example: a robot that reasons over the sequence recognizes that \textit{collectively} the human corrections keep the robot away from the obstacle while holding the bag, even though the corrections \textit{individually} fail to convey this objective, and can even be counter-productive.

Overall, we make the following contributions:

% \noindent\textbf{Removing Conditional Independence.}
\noindent\textbf{Capturing Conditional Dependence.} We enable robots to learn from sequences of corrections by introducing an auxiliary reward function. This reward captures the human's trade-off between making corrections that increase the short-term reward (i.e., avoiding the obstacle) and reaching their long-term objective (i.e., carrying groceries).

\smallskip

\noindent\textbf{Learning from Sequences.} We introduce a tractable method to learn from a sequence of physical corrections by i) using the Laplace approximation to estimate the partition function and ii) solving a mixed-integer optimization problem.

\smallskip

\noindent\textbf{Conducting Online and In-Person User Studies.} Participants interacted with robots in both single- and multi-agent environments. We recorded user's corrections, and compared our approach to both \textit{independent} and \textit{final} baselines. Our proposed method outperforms both baselines, demonstrating the effectiveness of reasoning over sequences.




%list of contributions:

%approximate way to get this crosstalk without conditional dependence
%aggresive or conservative corrector -- for humans that care more for intermediate rewards or final rewards.
%way to solve this??
%sequence is really hard -- optimization wize this is really hard, so we make some approximations to make it work
%user study on robot and online


% Humans are extremely good at developing tools and systems. Each of us interact with these systems everyday---from driving your car to work to playing video games at home. Unfortunately, we are not always good at these interactions. Using a joystick to play a kart racing computer game seems relatively easy, and we can do this without thinking much about it. But actually learning how to drive a real-life car does require practice. Even more challenging is flying a helicopter---this is highly demanding, and requires professional training. In these systems the human must adapt to the robot's controller across multiple rounds of interaction. Rather than forcing the human to adapt to the robot, we wonder if instead the robot could intelligently adapt to our preferences, making control more easy and intuitive? 

% Alongside rapid advancements in the field of robotics, intuitive control is increasingly in demand, particularly since a wide spectrum of users are operating robots beyond just engineers~\cite{arata2018intuitive}. User-friendliness and comfort are crucial in many application domains including surgical and assistive robots~\cite{argall2018autonomy, robinson2016adaptable}, mobile robots~\cite{melidis2018intuitive} and even more abstract smart home systems~\cite{huang2017over}. At the heart of these systems, there is seamless \textit{teleoperation}. Typically---for teleoperation---there are two main categories of control methods. One is to use control interfaces such as joysticks or sip-and-puff devices \cite{boboc2012review, javdani2018shared}. These controllers are light-weight but low-dimensional, which makes control of robots with many {\em degrees of freedom} (DoF) challenging. To make up for this limitation when teleoperating robot arms, \cite{herlant2016assistive} propose to that the controller should change modes between different DoFs. Other work focuses on capturing high-DoF human body movement with wearable devices, cameras, or sensors~\cite{rebelo2014bilateral}, and then maps them to robot actions. While these methods provide more accurate measurements and natural control interfaces, they are typically larger and more expensive systems that might not be available to everyday users.


% \begin{figure}[t]
%     \vspace{0.5em}
% 	\begin{center}
% 		\includegraphics[width=1.0\columnwidth]{figs/front.png}

% 		\caption{The human has in mind a preferred mapping between their joystick inputs and the robot's actions. The robot learns this mapping (i.e., $\theta$) offline from labelled data and intuitive priors, so that the robot's online actions are correctly aligned with the human's intentions.}

% 		\label{fig:front}
% 	\end{center}
	
% 	\vspace{-2em}
	
% \end{figure}

% Within this work, we are interested in the first category of teleoperation methods. We present a human-centered, adaptive system for robotic manipulation so that human users can smoothly and intuitively teleoperate high-DoF robots with simple, low-DoF controllers. We envision a model that provides a general framework for learning intuitive input mappings that are agnostic to both the controller and the dynamics of the underlying robotic system (see Fig.~\ref{fig:front}). Based on a general architecture of feedforward neural networks, the presented framework can be applied to different robots and controllers for various tasks with only a few demonstrations needed from the human users. Our key insight is: 

% \vspace{-0.5em}
% \begin{center}
% \emph{By incorporating the intuitive priors that humans expect, including proportionality, reversability and consistency, we can quickly learn how humans prefer to control robots.} 
% \end{center}\vspace{-0.5em}

% Our approach first asks the human user to label a set of robot actions with their preferred inputs. For example, the robot moves towards the cup, and then asks what joystick direction should be associated with this action. Based on a small number of these labeled state-action pairs, we learn an alignment model that maps from human inputs to robot inputs such that the resulting robot actions match the human's preferences. Ensuring a \textit{small number of questions} is critical for real-world implementation: we cannot ask the human to label hundreds of examples actions! Our insight---incorporating intuitive priors---enables the robot to generalize across the workspace, leading to offline learning from a limited amount of labeled data.

% More precisely, we make the following contributions:

% \prg{Formalizing Priors for Human Control.} 
% We formalize the properties that humans expect to have when  controlling robotic systems, including proportionality, reversability, and consistency. These priors are inspired by~\cite{jonschkowski2014state}, and we here analyse their relative importance when learning the human's preferred input mapping.   


% \prg{Generalizable Data-Efficient Learning.}
% We build a general framework for learning human control preferences. We develop this data-efficient method by incorporating the intuitive priors that humans expect, so that users only need to provide a few examples of their desired mappings. Our proposed method develops a light-weight solution that can generalize across different controllers, dynamics, and tasks.  

% \prg{Evaluating Learned Human Alignment.}
% We implement our approach both in simulation and on robot manipulation tasks, and get subjective feedback from users. Our tasks are inspired by assisitve robotics, where users living with physical disabilities have different capabilities and preferences. Results in simulation as well as in user studies suggest that our algorithm successfully learned the human-preferred alignment between the low-DoF control interface and high-DoF robot actions. In practice, this learned alignment led to improved control and more seamless interaction.