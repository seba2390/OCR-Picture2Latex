

\section{Experiments}
\label{sec:user_study}

To test our proposed algorithm, we conduct experiments with human users in a simulated navigation task and a robot manipulation task. We will discuss details that are consistent in both tasks and then elaborate on each one respectively.

\smallskip
\prg{Tasks.} Our two tasks are: a web-based online simulated navigation task and an in-person robot manipulation task.

% Our two tasks are: a web-based online simulated navigation task and an in-person robot manipulation task using Franka  Emika Panda robots.

\noindent\textit{1) Navigation Simulation}. 
In this task, a team of robots are navigating together through a specified region as in \figref{fig:results_navi}. The robot’s objective function considered four features related to: reaching the goal, keeping formation, avoiding the danger zone, and minimum travel distance. We test our algorithm and baselines in different scenarios by varying robot team size, robot initial policy, and specifying different human preference reward values. 
% More details are described in \sref{sec:exp_navigation}.
\\
\noindent\textit{2) Robot Manipulation}. In this task, two robot arms are carrying a full grocery bag to place it on the table. There are four features concerned in the reward: reaching the goal basket (blue or green regions as shown in~\figref{fig:front}), keeping the groceries inside of the bag while avoiding squeezing or stretching the bag, avoid touching nearby housewares such as cabinets or the hat shown in~\figref{fig:front}, and minimum trajectory length for efficiency.
The robot starts off with the assumption of reaching an incorrect goal while also not realizing the bag is full, and should not be stretched or squeezed. Users apply forces to guide the two robot arms toward the correct goal region, while trying to keep the groceries inside of the bag. 
% More details are described in \sref{sec:exp_robot}. 

% We designed two tasks for testing our algorithm. One is an web-based online simulated navigation task and the other is an in-person robot manipulation task with Franka  Emika Panda robots.

% \begin{enumerate}[leftmargin=0.3cm]
% \item \textit{Navigation Simulation}. 
% In this task, a team of robots are navigating together through a specified region as in \figref{fig:results_navi}. The robot’s objective function considered four features related to: reaching the goal, keeping formation, avoiding the danger zone, and efficiency (minimum travel distance). We test our algorithm and baseline methods in different scenarios by varying robot team size, robot initial policy, and specifying different human preference reward values. More details are described in \sref{sec:exp_navigation}. 
% \item \textit{Robot Manipulation}. In this task, two robot arms are carrying a full grocery bag to place it on the table. There are four features concerned in the reward: reaching the goal basket (blue or green regions as shown in~\figref{fig:front}), keeping the groceries inside of the bag while avoiding squeezing or stretching the bag, avoid touching nearby housewares such as cabinets or the hat shown in~\figref{fig:front}, and minimum trajectory length for efficiency.
% The robot starts off with the assumption of reaching an incorrect goal (blue region) while also not realizing the bag is full, and should not be stretched or squeezed. Users apply forces to guide the two robot arms toward the correct goal region, while trying to keep the groceries inside of the bag. More details are described in \sref{sec:exp_robot}. 

% \end{enumerate}

\begin{figure*}[th!]
	\begin{center}
% 		\includegraphics[width=2.0\columnwidth]{figs/robot_results.jpg}
		
\includegraphics[width=2.0\columnwidth]{figs/robot_results.png}


		\caption{Likelihood of three different reward parameters ($\theta^*, \theta_1, \theta_2$) as more corrections are received over time. The preferred reward $\theta^*$ is shown with the darker red or blue.  Each pair of plots demonstrate the \emph{Sequence} model compared with the \emph{Independent} model. We demonstrate the aggregate results over all users on the left, and the individual performance on the right. As shown \emph{Sequence} outperforms \emph{Independent} in identifying the preferred reward $\theta^*$.}

		\label{fig:align}
	\end{center}
	
	\vspace{-2em}
\end{figure*}

\prg{Independent Variables.} We compared three different inference models: reasoning over corrections independently (\textit{Independent}), performing inference only based on the final correction (\textit{Final}), and our model that reasons over the sequence of corrections (\textit{Sequence}). \textit{Independent} and our \textit{Sequence} model can perform online inference, while \textit{Final} will conduct offline inference after all corrections are provided.

% \prg{Independent Variables.} Across all the tasks, we compared three different inference models: reasoning over incoming corrections independently (\textit{Independent}), performing inference only based on the final correction (\textit{Final}), and our model that reasons over the sequence of physical interactions (\textit{Sequence}). Among these three algorithms, \textit{Independent} baseline method and our \textit{Sequence} model can perform online inference, while \textit{Final} baseline method will conduct offline inference after all corrections are provided.

\smallskip
\prg{Dependent Measures.}
We conduct experiments with human users and evaluate the effectiveness of the models by measuring the inference accuracy. Since we are unable to measure users' internal reward, we specify users' preferred reward function out of a predefined finite set of candidate reward parameters. We convey the preferred reward by explaining the priorities of the features to the user and demonstrating a desired robot trajectory using the preferred reward.
Users are instructed to correct the robot to behave as optimally as possible, while minimizing their physical correction effort. 
% We conduct experiments with human users and evaluate the effectiveness of different inference models by measuring the inference accuracy. Since we are unable to measure users' internal reward, we specify users' preferred reward function out of a predefined finite set of candidate reward parameters. We convey the preferred reward by explaining the priorities of the features to the user and demonstrating a desired robot trajectory with respect to the preferred reward.
% Users are instructed to correct the robot to behave as optimally as possible, while minimizing their physical correction effort. 
\smallskip

\prg{Hypotheses}: 
\begin{hypothesis}
    Compared to the online \emph{Independent} baseline, reasoning over \emph{Sequences} of corrections leads to higher accuracy and faster convergence to the preferred reward.
\end{hypothesis}
% \begin{hypothesis}
%     Compared to final inference, reasoning over sequences of correction can infer the ground truth online. 
% \end{hypothesis}
\begin{hypothesis}
    Compared to the offline \emph{Final} baseline, in addition to the advantages of online inference, our \emph{Sequence} model achieves higher accuracy in challenging tasks -- specifically tasks where fully correcting the robots is infeasible.
\end{hypothesis}
\smallskip


% \begin{figure}[t]
% 	\begin{center}
% 		\includegraphics[width=\columnwidth]{figs/combined_sim_final.png}

% 		\caption{Navigation simulation setting. We show the accuracy results of the single-agent and multi-agent tasks compared to the baselines.}

% 		\label{fig:results_navi}
% 	\end{center}
	
% 	\vspace{-2em}
% \end{figure}

% \begin{figure}[t]
% 	\begin{center}
% 		\includegraphics[width=\columnwidth]{figs/navi_example.png}

% 		\caption{}

% 		\label{fig:navi_example}
% 	\end{center}
	
% 	\vspace{-2em}
% \end{figure}

\vspace{-1em}
\subsection{Navigation Simulation}
\label{sec:exp_navigation}
\prg{Experimental Details.}
We recruited $15$ participants for two simulated navigation tasks. Participants interact with point-mass robots using a web browser, where they can observe the robots' trajectories, select a robot to correct, and provide corrections using the arrow keys.
We collected data from humans for $5$ episodes in two different scenarios: a simple single-agent scenario with only one robot, and  a more complex scenario containing a two-robot team. 
In both settings, participants are only able to correct one robot at a time.
% , meaning that for the more complex scenario --- where there are two robots --- the human cannot correct both robots at the same time. 


In both scenarios, robots' initial trajectory goes to an incorrect goal region as shown in Fig.~\ref{fig:results_navi}.
In the human's preferred reward function, not only is going to the correct goal encouraged, but other features are also encoded, including avoiding a danger zone and keeping the formation (equal distance between the robots throughout the trajectories). 
% Therefore, while the human users need to redirect the robots to the correct goal region, they are also asked to consider other features, and try to maximize the preferred reward as much as possible.


\prg{Results \& Analysis.}
We compute the average inference accuracy for the two baseline methods and our method across all users in both scenarios. The results are shown in \figref{fig:results_navi}. 




Across both scenarios, our \textit{Sequence} model demonstrates leading performance compared to both the \textit{Independent} and the \textit{Final} baselines. One thing to note is that in the single-agent scenario, the \textit{Final} method has a comparable accuracy to our method.
This indicates that in this simple task, the user can correct the robot to behave almost optimally so that simply looking at the final trajectory conveys sufficient information about the preferred reward. While in a more complex multi-agent scenario, the performance of the \textit{Final} method drops significantly
% (the accuracy is near $0$ as the method incorrectly estimates a reward different from the preferred reward)
. 
This is because in this complex scenario, even if the user acts optimally, the final trajectory will still not have sufficient information to identify the preferred reward. In this example reasoning over sequences is dominant over only considering the final correction.
% In this case, the advantages of reasoning over sequences becomes dominant.



\subsection{Robot Manipulation}
\label{sec:exp_robot}
\prg{Experimental Details.}
We recruited $9$ participants for the in-person robot manipulation task (Fig.~\ref{fig:front}). Here, the two Franka Emika Panda robot arms are carrying a grocery bag to the table. 
% The robots perform the task right next to the human users and 
The participants are asked to physically push or pull the robot to correct its behavior. Similar to the navigation task, the user can only correct one robot at a time, and we collected $5$ episodes of corrections from each participant. 

As shown in \figref{fig:front}, there are two containers on the table for the grocery bag (the blue region and the green region). The robots' initial plan is to carry the bag to the right toward the blue region. However, the human wants to put the grocery to the left container. Meanwhile, since the grocery bag is almost full, in order to keep the groceries from falling out, the participants are also instructed not to squeeze or stretch the bag. In this setting there are three possible reward parameter $\theta$, and the robot tries to infer the correct reward parameter $\theta^*$ from the human corrections. 

\smallskip
\prg{Results \& Analysis.}
We calculate the inference accuracy for all the three models (\emph{Independent}, \emph{Final}, and \emph{Sequence}), and summarize the results in \tabref{tab:robot_accuracy}. Our method demonstrates superior performance compared to the baselines. 


\begin{table}[!h]
\centering
\caption{Inference accuracy over 9 participants for robot manipulation.}
\begin{tabular}{@{}cccccl@{}}
\hline
\noalign{\vskip 0.5mm}
% \hline
\noalign{\vskip 0.5mm}
       & \textit{Sequence} (\textbf{ours})         & \textit{Independent}           & \textit{Final}   \\ 
    %   \hline
       \hline\noalign{\vskip 1mm}  
accuracy (\%)       & $\textbf{82.22}\pm \textbf{21.99}$        & $31.11\pm26.99$          & $53.33\pm13.30 $      \\ \noalign{\vskip 1mm}  \hline\noalign{\vskip 1mm}  

\end{tabular}
\label{tab:robot_accuracy}
\end{table}

In addition, we illustrate the probability distribution over $\theta$ with time in \figref{fig:align}. Since the \emph{Final} baseline performs the inference offline, we only visualize our \textit{Sequence} model and the \textit{Independent} baseline. As can be seen, across all of the participants, the probability for the preferred reward consistently dominates the other candidate rewards. However, for the \textit{Independent} baseline, even if the probability for the preferred reward sometimes starts off high, it ends up not being the most likely reward as we receive more corrections.
This is because in this two-robot task, redirecting the system to the correct goal while simultaneously maintaining the shape of the bag (formation) is not possible. The best possible corrections are: push one arm towards the goal, while stretching or squeezing the bag by a small amount, and then push the other arm in the same direction so that the bag shape is recovered. However, if we reason over these corrections independently, the robot will think that the first push indicates that preserving the bag shape is not important, and this leads to an incorrect inference.

\subsection{Summary} 
Our results empirically support both of our hypotheses \textbf{H1} and \textbf{H2}. Our \textit{Sequence} model not only conducts an online inference, but also demonstrates superior performance specially in complex multi-agent tasks. 