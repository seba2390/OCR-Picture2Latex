\section{Related Work}
\label{sec:related_work}

\noindent\textbf{Physical Human-Robot Interaction.} When humans and robots share a workspace, physical interaction is \textit{inevitable}. Prior work studies how robots can safely respond to physical interactions \cite{haddadin2016physical, de2008atlas, ikemoto2012physical}. This includes impedance control \cite{hogan1985impedance} and other reactive strategies \cite{haddadin2008collision}. Most relevant to our setting is \textit{shared control} \cite{li2015continuous, losey2018review, abbink2018topology, mortl2012role}, where the human and robot arbitrate between leader and follower roles. Although shared control enables the user to temporarily correct the robot's motion, it does not alter the robot's long term behavior: the robot does not \textit{learn} from physical corrections.

\smallskip

\noindent\textbf{Learning from Corrections (Online).} Recent research recognizes that physical human corrections are often \textit{intentional}, and therefore informative \cite{bajcsy2017learning, bajcsy2018learning, losey2018including, bobu2020quantifying}. These works learn about the human's underlying objective in real-time by comparing the current correction to the robot's previous behavior. Importantly, each correction is treated as an \textit{independent} event. Outside of physical human-robot interaction, shared autonomy follows a similar learning scheme --- the robot uses human inputs to update its understanding of the human's goal online, but does not reason about the connections between multiple interactions \cite{javdani2018shared, dragan2013policy, jain2019probabilistic,jeon2020shared}. We build upon these prior works by learning from sequences of corrections.


\smallskip

\noindent\textbf{Learning from Corrections (Offline).} By contrast, other works learn from the \textit{final} trajectory produced after all the corrections are complete. This research is closely related to learning from demonstrations \cite{argall2009survey}. For example, in \cite{akgun2012keyframe} the human corrects keyframes along the robot's trajectory, so that the next time the robot encounters the same task it moves through the corrected keyframes. Most similar to our setting are \cite{jain2015learning} and \cite{ratliff2006maximum}, where the robot iteratively updates its understanding of the human's objective \textit{after} the human corrects the robot's entire trajectory. Although these works take multiple corrections into account, they do so offline, and are not helpful during the current task.

