\documentclass{article}
\usepackage{spconf}


% Tobias

\usepackage{mathtools, amsfonts}

\usepackage[main=english, ngerman]{babel}
\usepackage[hang]{caption}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[hyphens]{url}
% \usepackage[hidelinks]{hyperref}
\usepackage[colorlinks]{hyperref}
% \usepackage[colorlinks, pagebackref=true]{hyperref}
\usepackage[dvipsnames]{xcolor}

% TODO
\usepackage{tikz}
\usetikzlibrary{arrows, calc, external, fit, positioning, shapes, spy}
% \tikzexternalize[prefix={figs/_TikZ/}]

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{colorbrewer}

\usepackage{pgfplotstable}

\usepackage{
	array,
	multirow,
	subfig,
	xfrac
}

\usepackage[export]{adjustbox}

\usepackage{graphicx, graphbox}
\graphicspath{{figs/}}

\usepackage{packages/hex}

% \renewcommand*{\arraystretch}{1.33}

\newcolumntype{P}[1]{>{ \centering  \arraybackslash }p{#1}}
\newcolumntype{Q}[1]{>{ \raggedleft \arraybackslash }p{#1}}
\newcolumntype{M}[1]{>{ \centering  \arraybackslash }m{#1}}
\newcolumntype{N}[1]{>{ \raggedleft \arraybackslash }m{#1}}
\newcolumntype{B}[1]{>{ \centering  \arraybackslash }b{#1}}
\newcolumntype{C}[1]{>{ \raggedleft \arraybackslash }b{#1}}

% \IEEEoverridecommandlockouts

\usepackage{fancyhdr}
\fancypagestyle{psp1}{
	\renewcommand{\headrulewidth}{0pt}
	\fancyfoot[C]{978-1-7281-6395-6/20/\$31.00~\copyright~2020 IEEE, 10.1109/ICIP40778.2020.9190995}
}


% ORCID

% https://tex.stackexchange.com/questions/498688/orcid-in-latex-file-of-ieee-trans-in-a-customized-position

\usepackage{scalerel}
% \usepackage{tikz}
\usetikzlibrary{svg.path}

\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
    orcidlogo/.pic={
        \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
        \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
        svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
        svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
    }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
                \begin{tikzpicture}[yscale=-1,transform shape]
                \pic{orcidlogo};
                \end{tikzpicture}
            }{|}}}}

% \usepackage{hyperref}

\newcommand{\ORCIDDanny}{$^{\textsuperscript{\orcidicon{0000-0002-4538-7814}}}$}    % ORCID Danny
\newcommand{\ORCIDFrederik}{$^{\textsuperscript{\orcidicon{0000-0001-5482-9787}}}$} % ORCID Frederik
\newcommand{\ORCIDTobias}{$^{\textsuperscript{\orcidicon{0000-0002-0682-4284}}}$}   % ORCID Tobias




\begin{document}


\title{Biologically Inspired Hexagonal Deep Learning for \\ Hexagonal Image Generation}

\name{
	Tobias Schlosser \ORCIDTobias,
	Frederik Beuth   \ORCIDFrederik, and
	Danny Kowerko    \ORCIDDanny
}
\address{
	Junior Professorship of Media Computing, \\
	Chemnitz University of Technology, \\
	09107 Chemnitz, Germany, \\
	% \texttt{\small \{firstname.lastname\}@informatik.tu-chemnitz.de}
	\texttt{\small \{firstname.lastname\}@cs.tu-chemnitz.de}
}


\maketitle


\begin{abstract}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	Whereas conventional state-of-the-art image processing systems of recording and output devices almost exclusively utilize square arranged methods, biological models, however, suggest an alternative, evolutionarily-based structure. Inspired by the human visual perception system, hexagonal image processing in the context of machine learning offers a number of key advantages that can benefit both researchers and users alike. The hexagonal deep learning framework Hexnet leveraged in this contribution serves therefore the generation of hexagonal images by utilizing hexagonal deep neural networks (H-DNN). As the results of our created test environment show, the proposed models can surpass current approaches of conventional image generation. While resulting in a reduction of the models' complexity in the form of trainable parameters, they furthermore allow an increase of test rates in comparison to their square counterparts.
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{abstract}

\begin{keywords}
	Computer Vision and Pattern Recognition, Deep Learning, Convolutional Neural Networks, Hexagonal Image Processing, Hexagonal Lattice, Hexagonal Sampling, Image Generation
\end{keywords}




\section{Introduction and motivation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{psp1}

While the structure and functionality of artificial neural networks is inspired by biological processes, they are also limited by their underlying structure. Due to the current state of the art of recording and output devices, mostly square structures are used, which also significantly restrict subsequent image processing systems \cite{Staunton1990}.

With the advancements of recent years, machine learning and deep neural networks (DNN) are becoming increasingly important. In order to meet the ever-increasing demands of more complex problems and systems, novel application areas and larger data sets are being investigated and developed \cite{Krizhevsky2012}. Entailing their steadily increasing diversity \cite{Szegedy2015}, novel architectures for artificial neural networks emerge that exceed and even supersede conventional Cartesian-based approaches, including spherical and non-Euclidean manifolds \cite{Bronstein2017}.

In comparison to the current state of the art, the human visual perception system suggests an alternative, evolutionarily-based structure. To allow an efficient processing of incoming signals \cite{Middleton2005}, the retina of the human eye displays a particularly strong hexagonal arrangement of sensory cells \cite{Curcio1987}. The image in the visual cortex is following the reduction of the perceived information to the nerve fibers projected retinotopic, whereas neighboring structures are preserved and processed through the following areas of the brain \cite{Hubel1968}.

The rise and advancement of hexagonal structures and image processing systems emerges therefore as an evolutionarily motivated approach. Following this principle, hexagonal structures (Fig.~\ref{figure:A_1}) feature a number of decisive advantages. These include, inter alia, the homogeneity of the hexagonal lattice format, whereby the equidistance and uniqueness of neighborhood as well as an increased radial symmetry are given. Together with a by $13.4$~\% increased transformation efficiency, hexagonal representations allow the storage of larger amounts of data on the same number of sampling points \cite{Petersen1962}. Moreover, they result in a reduction of computation times, less quantization errors, and an increased efficiency in programming \cite{Golay1969}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Application areas and related work}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Prominent application areas of hexagonal image processing systems often originate from the research fields of observation, experiment, and simulation in ecology \cite{Birch2007}, the design of geodesic grid systems \cite{Sahr2003}, sensor-based image processing and remote sensing \cite{Ambrosio2001}, medical imaging \cite{Neeser2000}, and image synthesis \cite{Theussl2001}. First experimental results for supply demand forecasting \cite{Ke2018} and the analysis of atmospheric telescope data for event detection and classification \cite{Shilon2019} explain the need for novel generative approaches within these emerging application areas.

In the context of hexagonal image processing and deep learning based systems, this entails not only principles from both domains, but also the development of operations and layers for hexagonal models \cite{Steppa2019} as well as the underlying addressing scheme and the following processing steps.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Conference_2019_ICMLA2019_Hexnet
\begin{figure}[tb]
	\centering

	\subfloat[Square lattice format]{
		%\scalebox{0.75}{
		%\scalebox{0.68}{
		\scalebox{0.67}{
			\begin{tikzpicture}[r/.style={draw, rectangle, minimum size=12.5mm}]
				\node[r, fill=Red!95] (r1) at (-1.25,  1.25) {};
				\node[r, fill=Red!85] (r2) at ( 0,     1.25) {};
				\node[r]              (r3) at ( 1.25,  1.25) {};
				\node[r]              (r4) at (-1.25,  0)    {};
				\node[r]              (r5) at ( 0,     0)    {};
				\node[r]              (r6) at ( 1.25,  0)    {};
				\node[r]              (r7) at (-1.25, -1.25) {};
				\node[r]              (r8) at ( 0,    -1.25) {};
				\node[r]              (r9) at ( 1.25, -1.25) {};

				\node[draw, circle, minimum size=2mm, inner sep=0, semithick] (c) at (0, 0) {};


				\draw[->, semithick] (c) -- node[left, yshift=-3mm] {$\sqrt{2}$} (-1.25, 1.25);
				\draw[->, semithick] (c) -- node[right]             {1}          ( 0,    1.25);

				\draw[->, shorten <=-1mm] (-3.66, 0.5) -- (-2.66, 0.5) node[right] {$x$};
				\draw[->, shorten <=-1mm] (-3.66, 0.5) -- (-3.66, 1.5) node[above] {$y$};
			\end{tikzpicture}}}
	\quad
	\subfloat[Hexagonal lattice format]{
		%\scalebox{0.75}{
		%\scalebox{0.68}{
		\scalebox{0.67}{
			\begin{tikzpicture}[h/.style={draw, shape=regular polygon, regular polygon sides=6, rotate=30, minimum size=15mm}]
				\node[h]                 (0) at ( 0,        0)     {};
				\node[h]                 (1) at ( 1.3,      0)     {};
				\node[h, fill=LimeGreen] (2) at ( 1.3 / 2,  1.125) {};
				\node[h, fill=LimeGreen] (3) at (-1.3 / 2,  1.125) {};
				\node[h]                 (4) at (-1.3,      0)     {};
				\node[h]                 (5) at (-1.3 / 2, -1.125) {};
				\node[h]                 (6) at ( 1.3 / 2, -1.125) {};

				\node[draw, circle, minimum size=2mm, inner sep=0, semithick] (c) at (0, 0) {};


				\draw[->, semithick] (c) -- node[right] {1} ( 1.3 / 2, 1.125);
				\draw[->, semithick] (c) -- node[left]  {1} (-1.3 / 2, 1.125);

				\draw[->, shorten <=-1mm] (2.66, 0.5) -- (3.66,           0.5)         node[right] {$x$};
				% \draw[->, shorten <=-1mm] (2.66, 0.5) -- (2.66 + 1.3 / 2, 0.5 + 1.125) node[above] {$y$};
				% \draw[->, shorten <=-1mm] (2.66, 0.5) -- (2.66 - 1.3 / 2, 0.5 + 1.125) node[above] {$z$};
				\draw[->, shorten <=-1mm] (2.66, 0.5) -- (2.66 - 1.3 / 2, 0.5 + 1.125) node[above] {$y$};
				\draw[->, shorten <=-1mm] (2.66, 0.5) -- (2.66 + 1.3 / 2, 0.5 + 1.125) node[above] {$z$};
			\end{tikzpicture}}}

	\caption{Lattice format comparison. \cite{Schlosser2019}}
	\label{figure:A_1}
\end{figure}


\subsection{Contribution of hexagonal image generation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This contribution is meant as a general application-oriented approach to hexagonal image generation by synthesizing the advantages of the research fields of hexagonal image processing with deep learning. Following, the by \textit{Schlosser et al.} \cite{Schlosser2019} introduced hexagonal deep learning framework Hexnet will therefore be utilized as a first approach to hexagonal image generation using hexagonal deep neural networks.

As of the current state of art, true hexagonal images have to be captured using a hexagonal sensor. However, the corresponding hardware is either rare or limited in its applicability. Suitable alternatives include therefore the transformation or synthesis of hexagonal images \cite{Steppa2019}. While hexagonal approaches for image generation can benefit both researchers and users alike, we will provide the necessary tools to ease the application, development, and evaluation of said approaches. The related developed architectures, procedures, and test results are made publicly available and are found on the project page of Hexnet\footnote{\url{https://github.com/TSchlosser13/Hexnet}}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Fundamentals and methods}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As an essential element of hexagonal image processing systems, the underlying addressing scheme for transformation and visualization as well as the the necessary related processing steps have to be determined. Following, we will introduce our hexagonal addressing scheme as identified for its applicability. Based on the proposed architecture, we will then introduce the principles of hexagonal deep learning.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{The hexagonal addressing scheme}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Currently deployed hexagonal addressing schemes are often inspired by the so-called 1-D spiral architecture addressing scheme (SAA) \cite{Burt1980}, which established itself as the preferred addressing scheme for hexagonal image processing systems. However, one drawback of SAA originates from the reduced locality and increased complexity when transforming square images into a hexagonal representation. Pseudohexagonal addressing schemes are in contrary easier to implement \cite{Fitz1996}.

This contribution leverages a hybrid approach that combines a 1-D linewise architecture with SAA (Fig.~\ref{figure:A_2}). Following the hexagonal addressing scheme, the from the construction process derived hierarchical hexagonal structures are exploited based on the properties of their pyramidal decompositions, also called septrees or Hexarrays. To allow an efficient implementation of the pseudohexagonal addressing scheme, the shifted Cartesian coordinates are stored for every second row \cite{Her1994}. For transformation, the Hexarray is centered over the given image and interpolated using either nearest-neighbor, bilinear, bicubic, or more complex interpolation approaches, such as hexagonal splines \cite{Her1994}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Conference_2019_ICMLA2019_Hexnet
\begin{figure}[tb]
	\centering

	\scalebox{0.75}{
		\begin{tikzpicture}[
		 h/.style={shape=regular polygon, regular polygon sides=6, shape border rotate=30, minimum size=10mm},
		 align=center, font=\small, inner sep=0]
			\ha{0}{draw}{0}{1}{7}{8}
			% \hasaaso{0}{draw=none}{4}{4}{0}{0}{0}{}
			% \hasaaso{0}{draw=none, scale=0.9}{4}{4}{0}{0}{0}{cyan}
			\hasaaso{0}{draw=none, scale=0.9}{4}{4}{0}{0}{0}{BlueGreen}
		\end{tikzpicture}}

	\caption{Hexagonal addressing scheme construction. \cite{Schlosser2019}}
	\label{figure:A_2}
\end{figure}


\subsection{Conventional and hexagonal deep neural networks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Whereas conventional deep neural networks are almost exclusively subdivided based on square lattice formats, hereinafter referred to as square DNNs (S-DNN), they are often divided into a set of alternating layers, which consist of convolutional, pooling, and fully connected layers. Following this principle, hexagonal deep neural networks (H-DNN) are based on the introduced hexagonal addressing scheme.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Assessing the quality of hexagonal representations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To quantify the information content and image quality of square and hexagonal lattice format based representations, the hereinafter so-called transformation efficiencies are obtained \cite{Schlosser2019}. Given the original square lattice format based image $S$ and its hexagonally sampled image $H$, the absolute error is derived by weighting the from projection of $S$ onto $H$ obtained subareas $a \in A$ in subpixel resolution. Following this principle, the computation of the mean squared error (MSE) is shown in~\eqref{F_1}, whereas the subareas area is denoted by $|a|$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Conference_2019_ICMLA2019_Hexnet
\begin{equation}
	\mathit{MSE} = \frac{1}{|A|} \cdot \sum\limits_{a \in A} |a| \cdot [S(a) - H(a)]^2
%
	\label{F_1}
\end{equation}

\begin{figure*}[tb]
	\centering

	\subfloat[H-SWWAE]{
		\scalebox{0.7}{
			\begin{tikzpicture}[
			 h/.style={shape=regular polygon, regular polygon sides=6, shape border rotate=30, minimum size=1cm},
			 r/.style={draw, rectangle, minimum size=1.5cm},
			 font=\normalsize]
				\node[r] (n1) at (0,  5)   {};
				\node[r] (n2) at (0,  2.5) {};
				\node[r] (n3) at (0,  0)   {};
				\node[r] (n4) at (0, -2.5) {};
				\node[r] (n5) at (0, -5)   {};


				\begin{scope}[yshift=5cm, scale=0.125, transform shape]
					\foreach \i in {0, ..., 4} {
						\node[r, fill=white, minimum size=6cm] at (-0.66+0.33*\i, 0.66-0.33*\i) {};
					}
				\end{scope}

				\begin{scope}[yshift=2.5cm, scale=0.125, transform shape]
					\foreach \i in {0, ..., 4} {
						\node[r, fill=white, minimum size=3cm] at (-0.66+0.33*\i, 0.66-0.33*\i) {};
					}
				\end{scope}

				\draw (0, 0) circle (0.125);

				\begin{scope}[yshift=-2.5cm, scale=0.125, transform shape]
					\foreach \i in {0, ..., 4} {
						\ha{0}{draw, fill=white, shift={(-0.66cm+0.33*\i cm, 0.66cm-0.33*\i cm)}}{0}{0}{2}{2}
					}
				\end{scope}

				\begin{scope}[yshift=-5cm, scale=0.125, transform shape]
					\foreach \i in {0, ..., 4} {
						\ha{0}{draw, fill=white, shift={(-0.66cm+0.33*\i cm, 0.66cm-0.33*\i cm)}}{0}{0}{5}{5}
					}
				\end{scope}


				\draw[->] (n1) -- (n2);
				\draw[->] (n2) -- (n3);
				\draw[->] (n3) -- (n4);
				\draw[->] (n4) -- (n5);


				\node[r, very thin, fit=(n1)(n5), inner sep=0.5cm] (fn15) {};


				\node[r, left=2cm of n1] (n1e) {\includegraphics[width=2cm]{{ICMLA_v2/s_conv2d_fm0_label1_image12}.png}};
				\node[r, left=2cm of n2] (n2e) {\includegraphics[width=2cm]{{ICMLA_v2/s_conv2d_1_fm0_label1_image12}.png}};
				\node[r, left=2cm of n3] (n3e) {$(x_1, \dots, x_n)$};
				\node[r, left=2cm of n4] (n4e) {\includegraphics[width=2cm]{{ICMLA_v2/s_conv2d_1_fm0_label1_image12_s2h_d_cropped}.png}};
				\node[r, left=2cm of n5] (n5e) {\includegraphics[width=2cm]{{ICMLA_v2/s_conv2d_fm0_label1_image12_s2h_d_cropped}.png}};

				\draw[dashdotted] (n1e) -- (n1);
				\draw[dashdotted] (n2e) -- (n2);
				\draw[dashdotted] (n3e) -- (n3);
				\draw[dashdotted] (n4e) -- (n4);
				\draw[dashdotted] (n5e) -- (n5);


				\draw[decorate, decoration={brace, amplitude=0.25cm}] ([xshift=1.5cm]n1.north) -- ([xshift=1.5cm]n2.south) node[midway, xshift=0.5cm, rotate=90, anchor=north] {square pooling};
				\draw[decorate, decoration={brace, amplitude=0.25cm}] ([xshift=1.5cm]n3.north) -- ([xshift=1.5cm]n3.south) node[midway, xshift=0.5cm, rotate=90, anchor=north] {core feature space};
				\draw[decorate, decoration={brace, amplitude=0.25cm}] ([xshift=1.5cm]n4.north) -- ([xshift=1.5cm]n5.south) node[midway, xshift=0.5cm, rotate=90, anchor=north] {hexagonal upsampling};
			\end{tikzpicture}}}
	\hspace{2cm}
	\subfloat[H-ACGAN]{
		\scalebox{0.7}{
			\begin{tikzpicture}[
			 r/.style={draw, rectangle, minimum size=1cm},
			 t/.style={draw, trapezium, minimum size=1cm},
			 font=\normalsize]
				\node[r] (n1) at (-1,  5) {real};
				\node[r] (n2) at (-1,  3) {fake};
				\node[r] (n3) at ( 1,  3) {\itshape class};
				\node[t] (n4) at ( 0,  1) {\Large\bfseries\itshape D};
				\node[r] (n5) at (-1, -1) {$x$};
				\node[r] (n6) at ( 1, -1) {$G(z)$};
				\node[t] (n7) at ( 1, -3) {\Large\bfseries\itshape G};
				\node[r] (n8) at (-1, -5) {$c$};
				\node[r] (n9) at ( 1, -5) {$z$};

				\node[r, very thin, fit=(n1)(n2), inner sep=0.25cm] (fn_1_2) {};


				\node (n10) at ($(n2)!0.5!(n3)$)  {};
				\node (n11) at ($(n10)!0.5!(n4)$) {};
				\node (n12) at ($(n5)!0.5!(n6)$)  {};
				\node (n13) at ($(n4)!0.5!(n12)$) {};
				\node (n14) at ($(n7)!0.5!(n9)$)  {};

				\draw[->] (n4) -- (n11.center) -| (fn_1_2);
				\draw[->] (n4) -| (n3);
				\draw[->] (n5) |- (n13.center) -- (n4);
				\draw     (n6) |- (n13.center);
				\draw[->] (n7) -- (n6);
				\draw[->] ([xshift=-0.16cm]n8.north) -- ([xshift=-0.16cm]n5.south);
				\draw[->, dashed] ([xshift=0.16cm]n8.north) |- (n14.center);
				\draw[->] (n9) -- (n7);


				\node[r, very thin, fit=(fn_1_2)(n7)(n9), inner sep=0.5cm] (fn__fn_1_2__7_9) {};


				\node[r, left=2cm of n1, yshift=0.5cm]  (n1e) {\includegraphics[width=2cm]{{ICMLA_v2/s_conv2d_fm0_label58_image641_s2h_d_cropped}.png}};
				\node[r, left=2cm of n2, yshift=-0.5cm] (n2e) {\includegraphics[width=2cm]{{ICMLA_v2/s_conv2d_fm0_label31_image344_s2h_d_cropped}.png}};
				\node[r, right = 2cm of n3] (n3e) {duck / jug};
				\node[r, left  = 2cm of n5] (n5e) {\includegraphics[width=2cm]{{ICMLA_v2/s_conv2d_fm0_label58_image641_s2h_d_cropped}.png}};
				\node[r, right = 2cm of n6] (n6e) {\includegraphics[width=2cm]{{ICMLA_v2/s_conv2d_fm0_label31_image344_s2h_d_cropped}.png}};
				\node[r, left  = 2cm of n8] (n8e) {duck};
				\node[r, right = 2cm of n9] (n9e) {$(z_1, \dots, z_n)$};

				\draw[dashdotted] (n1e) -- (n1);
				\draw[dashdotted] (n2e) -- (n2);
				\draw[dashdotted] (n3)  -- (n3e);
				\draw[dashdotted] (n5e) -- (n5);
				\draw[dashdotted] (n6)  -- (n6e);
				\draw[dashdotted] (n8e) -- (n8);
				\draw[dashdotted] (n9)  -- (n9e);


				\node[below=0.25cm of n3e] (n3el) {class samples};
				\node[below=0.25cm of n8e] (n8el) {class sample};
				\node[below=0.25cm of n9e] (n9el) {latent vector sample};

				\draw[decorate, decoration={brace, amplitude=0.25cm}] ([xshift=0.5cm] n6e.east |- fn_1_2.north) -- ([xshift=0.5cm] n6e.east |- n4.south)  node[midway, xshift=0.5cm, rotate=90, anchor=north, align=center] {hexagonal image \\ discrimination};
				\draw[decorate, decoration={brace, amplitude=0.25cm}] ([xshift=0.5cm] n6e.east |- n6e.north)    -- ([xshift=0.5cm] n6e.east |- n9e.south) node[midway, xshift=0.5cm, rotate=90, anchor=north, align=center] {hexagonal image \\ sampling / generation};
			\end{tikzpicture}}}

	\caption{Exemplary models for hexagonal image generation. \textbf{a)} Hexagonal Deeply Stacked Residual What-Where Autoencoder (H-SWWAE). \textbf{b)} Hexagonal Auxiliary Classifier Generative Adversarial Network (H-ACGAN).}
	\label{figure:A_3}
\end{figure*}

\begin{table}[tb]
	\renewcommand{\arraystretch}{1.1}
	\centering

	\scalebox{0.9}{\begin{tabular}{|M{2cm}|M{2cm}|M{1.9cm}|M{1.9cm}|}
		\hline
		Model & Convolutional kernel sizes & Input shape(s) & Output shape(s) \\
		\hline
		\noalign{\vskip 2pt}

		\hline
		Encoder & $3 \times 3$ / $7^1$ & $(32, 32, 3)$ & $(1, 1, 128)$ \\
		\hline
		Decoder & $3 \times 3$ / $7^1$ & $(1, 1, 128)$ & $(32, 32, 3)$ / $(34, 30, 3)$ \\
		\hline
	\end{tabular}}

	\caption{S- / H-SWWAE layer configuration.}
	\label{table:T_1}
\end{table}

\begin{table}[tb]
	\renewcommand{\arraystretch}{1.1}
	\centering

	\scalebox{0.9}{\begin{tabular}{|M{2cm}|M{2cm}|M{1.9cm}|M{1.9cm}|}
		\hline
		Model & Convolutional kernel sizes & Input shape(s) & Output shape(s) \\
		\hline
		\noalign{\vskip 2pt}

		\hline
		Generator     & $3\times 3$ / $7^1$ & $(8192)$ & $(32, 32, 3)$ / $(34, 30, 3)$ \\
		\hline
		Discriminator & $3\times 3$ / $7^1$ & $(32, 32, 3)$ / $(34, 30, 3)$ & $(10)$ \\
		\hline
	\end{tabular}}

	\caption{S- / H-ACGAN layer configuration.}
	\label{table:T_2}
\end{table}


\section{Implementation of the hexagonal deep learning Framework}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To allow the utilization of hexagonal deep learning models, we based our framework on the currently most commonly in research and application deployed machine learning framework TensorFlow\footnote{\url{https://www.tensorflow.org/}, \textit{version 2.1.0}} with Keras as its front end.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Hexagonal layers}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Hexagonal layers for H-DNNs encompass as compared to conventional DNNs not only hexagonal equivalents for convolutional and pooling layers, but also, e.g., dense and batch normalization layers. Since hexagonal kernels are based on the introduced hexagonal addressing scheme, especially convolutional layers and their inverse are implemented intuitively by shifting SAA over the given input. The hexagonal pooling layer is then either realized based on conventional pooling operations or as inspired by the scaling of Hexarrays \cite{Middleton2005}, whereas the sub-Hexarray orders are given by the proposed addressing scheme. To solve the from two different sub-Hexarrays resulting offset mapping with or without strides corresponding to the current offsets, the mapping problem has to be approached in a more general manner. The mapping-based linear assignment problem is then solved by minimizing the resulting $\ell^2$ norm based cost \cite{Schlosser2019}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Models for hexagonal image generation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The in this contribution proposed models for hexagonal image generation are divided into two groups: \textbf{a)} H-DNNs for square lattice format based images and \textbf{b)} hexagonal lattice format based ones. Former include therefore conventional square data sets, while latter include transformed, synthesized \cite{Steppa2019}, or true hexagonal images \cite{Shilon2019}.

Inspired by \textit{Zhao et al.'s} \cite{Zhao2015} and \textit{Odena et al.'s} \cite{Odena2017} approaches to Stacked What-Where Autoencoders (SWWAE) with residual learning and Auxiliary Classifier Generative Adversarial Networks (ACGAN), their exemplary hexagonal counterparts are shown in Fig.~\ref{figure:A_3}. Every conventional layer, including not only convolutional and pooling layers, but also, i.a., upsampling layers, was substituted by its hexagonal equivalent for either \textbf{a)} only the model's decoder (H-SWWAE) or \textbf{b)} the whole model (H-ACGAN).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
	\newcommand{\width}{0.042\textwidth}
	\centering




	% MNIST square

	\includegraphics[width=\width]{{A_4_v2/MNIST_S-SWWAE/label0_image0_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-SWWAE/label1_image980_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-SWWAE/label2_image2115_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-SWWAE/label3_image3147_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-SWWAE/label4_image4157_grayscale}.png}
	\hspace{0.2cm}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-ACGAN/epoch1_label0_image0_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-ACGAN/epoch1_label1_image1_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-ACGAN/epoch1_label2_image2_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-ACGAN/epoch1_label3_image3_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-ACGAN/epoch1_label4_image4_grayscale}.png}
	\vspace{0.1cm}

	\includegraphics[width=\width]{{A_4_v2/MNIST_S-SWWAE/label5_image5139_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-SWWAE/label6_image6031_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-SWWAE/label7_image6989_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-SWWAE/label8_image8017_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-SWWAE/label9_image8991_grayscale}.png}
	\hspace{0.2cm}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-ACGAN/epoch1_label5_image5_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-ACGAN/epoch1_label6_image6_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-ACGAN/epoch1_label7_image7_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-ACGAN/epoch1_label8_image8_grayscale}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_S-ACGAN/epoch1_label9_image9_grayscale}.png}
	\vspace{0.2cm}


	% MNIST hexagonal

	\includegraphics[width=\width]{{A_4_v2/MNIST_H-SWWAE/label0_image1_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-SWWAE/label1_image981_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-SWWAE/label2_image2116_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-SWWAE/label3_image3148_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-SWWAE/label4_image4158_grayscale_s2h_hex}.png}
	\hspace{0.2cm}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-ACGAN/epoch1_label0_image10_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-ACGAN/epoch1_label1_image11_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-ACGAN/epoch1_label2_image12_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-ACGAN/epoch1_label3_image13_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-ACGAN/epoch1_label4_image14_grayscale_s2h_hex}.png}
	\vspace{0.1cm}

	\includegraphics[width=\width]{{A_4_v2/MNIST_H-SWWAE/label5_image5140_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-SWWAE/label6_image6032_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-SWWAE/label7_image6990_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-SWWAE/label8_image8018_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-SWWAE/label9_image8992_grayscale_s2h_hex}.png}
	\hspace{0.2cm}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-ACGAN/epoch1_label5_image15_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-ACGAN/epoch1_label6_image16_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-ACGAN/epoch1_label7_image17_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-ACGAN/epoch1_label8_image18_grayscale_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/MNIST_H-ACGAN/epoch1_label9_image19_grayscale_s2h_hex}.png}
	\vspace{-0.2cm} % TODO


	\rule[0.1cm]{\linewidth}{0.5pt} % TODO


	% CIFAR-10 hexagonal

	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-SWWAE/label0_image1_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-SWWAE/label1_image1001_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-SWWAE/label2_image2001_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-SWWAE/label3_image3001_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-SWWAE/label4_image4001_s2h_hex}.png}
	\hspace{0.2cm}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-ACGAN/epoch1_label0_image10_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-ACGAN/epoch1_label1_image11_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-ACGAN/epoch1_label2_image12_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-ACGAN/epoch1_label3_image13_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-ACGAN/epoch1_label4_image14_s2h_hex}.png}
	\vspace{0.1cm}

	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-SWWAE/label5_image5001_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-SWWAE/label6_image6001_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-SWWAE/label7_image7001_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-SWWAE/label8_image8001_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-SWWAE/label9_image9001_s2h_hex}.png}
	\hspace{0.2cm}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-ACGAN/epoch1_label5_image15_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-ACGAN/epoch1_label6_image16_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-ACGAN/epoch1_label7_image17_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-ACGAN/epoch1_label8_image18_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/CIFAR-10_H-ACGAN/epoch1_label9_image19_s2h_hex}.png}
	\vspace{-0.2cm} % TODO


	\rule[0.1cm]{\linewidth}{0.5pt} % TODO


	% COIL-100 hexagonal

	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-SWWAE/label0_image1_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-SWWAE/label1_image12_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-SWWAE/label2_image23_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-SWWAE/label3_image34_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-SWWAE/label4_image45_s2h_hex}.png}
	\hspace{0.2cm}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-ACGAN/epoch1_label0_image100_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-ACGAN/epoch1_label1_image101_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-ACGAN/epoch1_label2_image102_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-ACGAN/epoch1_label3_image103_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-ACGAN/epoch1_label4_image104_s2h_hex}.png}
	\vspace{0.1cm}

	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-SWWAE/label5_image56_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-SWWAE/label6_image67_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-SWWAE/label7_image78_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-SWWAE/label8_image89_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-SWWAE/label9_image100_s2h_hex}.png}
	\hspace{0.2cm}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-ACGAN/epoch1_label5_image105_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-ACGAN/epoch1_label6_image106_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-ACGAN/epoch1_label7_image107_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-ACGAN/epoch1_label8_image108_s2h_hex}.png}
	\includegraphics[width=\width]{{A_4_v2/COIL-100_H-ACGAN/epoch1_label9_image109_s2h_hex}.png}
	\vspace{0cm} % TODO




	\caption{Exemplary test results for S- and H-SWWAE (left column) as well as S- and H-ACGAN (right column) with the MNIST (top), CIFAR-10 (middle), and COIL-100 (bottom) data sets after 100 epochs of training.}
	\label{figure:A_4}
\end{figure}

\begin{table}[tb]
	\renewcommand{\arraystretch}{1.1}
	\centering

	\scalebox{0.9}{\begin{tabular}{|M{1.5cm}|M{1.75cm}|M{2cm}|M{2cm}|}
		\hline
		Data set & Test run & SWWAE & ACGAN \\
		\hline
		\noalign{\vskip 2pt}

		\hline
		\multirow{2}{*}{CIFAR-10}  & square             & $37.3 \pm 2.4$          & $23.7 \pm 4.5$          \\
		\cline{2-4}
		                           & \textbf{hexagonal} & $\mathbf{39.1 \pm 2.5}$ & $\mathbf{24.8 \pm 4.1}$ \\
		\hline
		\multirow{2}{*}{COIL-100}  & square             & $39.4 \pm 2.7$          & $25.3 \pm 3.9$          \\
		\cline{2-4}
		                           & \textbf{hexagonal} & $\mathbf{39.7 \pm 2.6}$ & $\mathbf{26.3 \pm 3.9}$ \\
		\hline
		\multirow{2}{*}{MNIST}     & square             & $44.2 \pm 1.9$          & $32.3 \pm 3.7$          \\
		\cline{2-4}
		                           & \textbf{hexagonal} & $\mathbf{45.1 \pm 1.7}$ & $\mathbf{33.6 \pm 4.2}$ \\
		\hline
	\end{tabular}}

	\caption{PSNR-based transformation efficiency test results after 100 epochs of training for one batch of images.}
	\label{table:T_3}
\end{table}


\section{Results, evaluation, and discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Current research that tries to combine principles from the domain of hexagonal image processing with machine learning often only provides application-specific test results with data which were either synthesized or captured using a hexagonal sensor. However, this contribution aims to provide a more general evaluation by assessing the quality of different S- and H-DNN models with square and hexagonal images.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Square and hexagonal deep neural networks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To enable a comparison of different S- and H-DNN models, we deployed two different kernel sizes for all convolutional layers and tested them against various data sets with an initial image resolution of $32 \times 32$ square and $34 \times 30$ hexagonal pixels. The realized architecture configurations for 10 classes are shown in Tables~\ref{table:T_1} and \ref{table:T_2}, resulting in $251\,931$ and $195\,946$ (S- and H-SWWAE) as well as $1\,149\,603$ and $1\,123\,706$ (S- and H-ACGAN) trainable parameters respectively. In comparison to the layer configurations as given by the original authors for both S-DNNs, the deployed kernel sizes were therefore substituted as shown. Our proposed training setup includes: the Glorot initializer for weight initialization, the Adam optimizer with a learning rate of $0.0002$ and exponential decay rates of $0.5$ and $0.999$, as well as a batch size of $100$. The SWWAE models were then trained by deploying the MSE-based transformation efficiency in~\eqref{F_1} as loss function.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Quality assessment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To assess the quality of the generated images, we evaluated the CIFAR-10, COIL-100, and MNIST data sets using their given data set split ratios. Our S- and H-SWWAE as well as S- and H-ACGAN test results after 100 epochs of training are shown in Fig.~\ref{figure:A_4}. For the differences in image quality of square and hexagonally generated images, the transformation efficiencies for the respective data sets are shown in Table~\ref{table:T_3} using the peak signal-to-noise ratio (PSNR). These results were obtained by comparing the original square lattice format based images with the generated hexagonal ones. Whereas a comparison of all test results for the given models and their proposed hexagonal equivalents reveals increased efficiencies for both hexagonal models, they also result in a reduction of trainable parameters. The hexagonal models enable therefore the reduction of the models' complexity, while hexagonal layers can furthermore improve image quality.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Conclusion and outlook}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Our test results highlight that H-DNNs for hexagonal image generation can outperform conventional DNNs, whereas an increase in test rates and the reduction of trainable parameters demonstrates their benefits. Future deep learning models can form the hexagonal equivalents to conventional DNNs, as further novel and application-specific models and data sets have to be developed and investigated.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \section*{Acknowledgment}
\section{Acknowledgment}

The European Union and the European Social Fund for Germany partially funded this research.
% This research is partially funded by the European Social Fund for Germany as well as the German Federal Ministry of Education and Research within the project group localizeIT.
% This research is partially funded by the European Social Fund for Germany as well as the German Federal Ministry of Education and Research within the project group localizeIT (funding code 03IPT608X).


\bibliographystyle{IEEEbib}
\bibliography{library}


\end{document}

