
\subsection{Threat Models for Private Federated Learning}
A typical federated learning (FL) system  \cite{pmlr-v54-mcmahan17a, AdvancesFLNOW2021} comprises users/workers, a server/curator, and an analyst, where users are connected to the server, and the server is subsequently connected to the analyst. Users wish to jointly train a machine learning model using their local datasets with the help of the server. The training is typically done using iterative algorithms such as gradient descent and its variants, where users receive the global learning model that needs to be trained from the server and compute gradients using their local datasets, and subsequently send the gradients or the updated local models back to the server for aggregation. 
The analyst may request for the model at any given time. Depending on who the malicious party is, its capability and intent, we can have several different threat models. For example, the analyst can be assumed to be honest but curious who does not actively attack the trustworthy server or users but tries to learn as much information about users as possible through the output released to it. Similarly, the server can also be assumed to be honest but curious. However, different from the analyst, the server can also be an active attacker, who alters the training process and/or baits users into revealing their information. Another possible threat model is when a subset of users is malicious, who try to tamper with the training process by sending altered gradients or model updates. We refer the reader to a recent excellent comprehensive survey on the subject of FL \cite{AdvancesFLNOW2021}, which gives an in-depth account of recent progress on various FL modalities, as well as challenges in achieving efficiency, privacy, fairness, and system level implementation. 

In this survey, we focus on the models where (a) the server is trustworthy and the analyst is honest but curious; and (b) both the server and the analyst are honest but curious. One may think that no information can be learned by the curious party due to the fact that the local data never leave the users, therefore, the local data is private. However, it has been shown that even gradients or updated models can be used to recover the data used during training for feed-forward neural networks \cite{Phong2017gradientLeak, Phong2018gradientLeak, Larochelle2020Inverting} and convolutional neural network \cite{Zhu2019DeepLeak, Wang2019Beyond}. This type of attack is known as gradient/model inversion attack. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/Fig-4-1-Private-FL-model.pdf}
    \caption{Conventional federated learning system, where $\mathbf{w}_t$ denotes the model parameters at iteration $t$, and $\mathbf{g}_k(\mathbf{w}_t)$ denotes the gradient computed using $\mathbf{w}_t$ by user $k$. After training, the model is released to an analyst.
    }
    \label{fig:FL_system_model}
\end{figure}



\subsection{Differential Private Federated Learning}

In private distributed computing, where the entire data is available at a central location (user), as discussed in the previous section, it is indeed possible to achieve perfect privacy (in an information-theoretic sense) when performing computations over distributed cluster of nodes. The federated learning paradigm, however, has several key distinctions as we briefly highlight next: since the data is already locally spread at the users (and is required to be kept private), perfect privacy against a single server can only be achieved by completely sacrificing utility (in terms of the model learned by perfectly private interactions with the user). Thus, in conventional single-server FL, one seeks to relax the privacy requirements from perfect privacy to allowing some leakage in a graceful manner. Indeed, as is shown in \cite{Jia_Jafar_XSTPFSL}, perfect privacy can be feasible with multiple servers, and when one may be interested in training multiple sub-models at the servers, or when some collaboration between the users is allowed (also see the discussion in Section~\ref{sec:securemodelaggregation}). For the remainder of this section, we will exclusively focus on the single-server FL setting when the users cannot collaborate. 


Differential privacy (DP) \cite{DworkRothDPBook} is one of the most widely used privacy notions and has been shown to be effective to mitigate not only inversion attacks, but also differential attacks. The goal is to protect the private data by perturbing the output before it is released to untrustworthy parties. Depending on who performs the perturbation or who we wish to protect against, differential privacy can be further categorized into local DP and central DP. For a FL system with $K$ users, the local and central DP are formally defined as follows.

\begin{definition} 
($(\epsilon_{\ell}^{(k)}, \delta_{\ell})$-LDP) Let $\mathcal{X}_k$ be a set of all possible data points at user $k$. For user $k$, a randomized mechanism $\mathcal{M}_k: \mathcal{X}_{k} \rightarrow \mathds{R}^{d}$ is $(\epsilon_{\ell}^{(k)}, \delta_{\ell})$-LDP if for any $x,~x' \in \mathcal{X}_k$, and any measurable subset $\mathcal{O}_k \subseteq \text{Range}(\mathcal{M}_k)$, we have
\begin{align}
    \operatorname{Pr}(\mathcal{M}_k(x) \in \mathcal{O}_k) \leq \exp{(\epsilon_{\ell}^{(k)})}  \operatorname{Pr}(\mathcal{M}_k(x') \in \mathcal{O}_k) + \delta_{\ell}.
\end{align}
The setting when $\delta_{\ell} = 0$ is referred as pure $\epsilon_{\ell}^{(k)}$-LDP.
\end{definition}

\begin{definition} 
($(\epsilon_{c}, \delta_{c})$-DP) Let $\mathcal{D}\triangleq \mathcal{X}_1 \times\mathcal{X}_2\times\dots\times\mathcal{X}_K$ be the collection of all possible datasets of all $K$ users. A randomized mechanism $\mathcal{M}: \mathcal{D} \rightarrow \mathds{R}^{d}$ is $(\epsilon_{c}, \delta_{c})$-DP if for any two neighboring datasets $D, D'$ and any measurable subset $\mathcal{O} \subseteq \text{Range}(\mathcal{M})$, we have
\begin{align}
    \operatorname{Pr}(\mathcal{M}(D) \in \mathcal{O}) &\leq \exp{(\epsilon_{c})}  \operatorname{Pr}(\mathcal{M}(D')  \in \mathcal{O}) + \delta_{c}. 
\end{align}
The setting when $\delta_{c} = 0$ is referred as pure $\epsilon_{c}$-DP. 
\end{definition}
% Depending on the definition of neighboring datasets, central DP can be further categorized into example-level DP and user-level DP.

% with the worst privacy parameters, i.e., $\max_k \epsilon_{\ell}^{(k)}$. 
We refer $\epsilon_c$ ($\epsilon_{\ell}^{(k)}$) and $\delta_c$ ($\delta_{\ell}$) as privacy parameters. These parameters are closely associated with a quantity called sensitivity, which is defined as the largest difference of a function over all available inputs. It is known that central DP is a weaker guarantee than local DP. Therefore, local DP guarantee implies central DP guarantee. Both central and local DP are first computed on a per-iteration basis. Then, the total leakage is computed by summing up the leakages over all iterations. However, simply summing up the leakages over all iterations provides bound on the actual total leakage due to the fact that data is often reused during training. It is known that the more a data point is used, the more information it leaks. Therefore, to capture this phenomenon, various of composition theorem/leakage accountant methods are used to tighten the bound on the total leakage, such as advanced composition theorem, and moment accountant \cite{Abadi2016}. 

\begin{table}[t]
 \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
     &  Noise Injection & Sampling & Shuffling & Others\\
     \hline
Central DP  & \cite{Agarwal2018cpSGD, Song2013SGD} & \cite{McMahan2018Recurrent, Asoodeh2020InfoTheory, Abadi2016, Bassily2014PrivateERM} & \cite{Erlingsson2019Shuffling, Bittau2017PROCHLO, Cheu2018Distributed} & \cite{Thakkar2019AdaptiveClipping} \\
Local DP  & \cite{Ono2020LDPReinforcementLearning, Li2020SecureFLwithDP} & \cite{Balle2018AmpSubsampling, Heikkila2020DPcrossSilo} & \cite{Erlingsson2019Shuffling, Balle2019Blanket, Ghazi2019Scalable, Balle2020RandomCheckIn, Girgis2021ShuffledModel} & \cite{Smith2017Interaction, Duchi2013Local} \\

         \hline
         
 \end{tabular}
%  \vspace{5pt}
 \caption{A quick reference of some of the key privacy preserving techniques for providing central and local DP guarantees in Federated Learning.
 \label{tbl:summary}}
\end{table}

\begin{enumerate}
    \item \textit{Basic privacy preserving mechanisms}: Let us first look at the case where the server is trustworthy, and the goal is to satisfy a desired central DP level against the curious analyst. The outputs, e.g., learning model iterates or gradients, have been shown to leak information about the local datasets. Therefore, the goal is to perturb the outputs so that it becomes difficult for the analyst to learn information about the local datasets. Typically, for works that focus on central DP, the perturbation is done at the server. The outputs can be perturbed by using random response, adding noise, or using approximations. For example, in \cite{Abadi2016}, Gaussian noise is added to the gradient before the model update. However, gradients and model parameters often are represented with finite precision, which make Gaussian noise injecting mechanism impractical. Thus, other types of noise injecting mechanisms are also considered, such as Laplace mechanism, and for discrete values, binomial mechanism can be used \cite{Agarwal2018cpSGD}. Noise with custom density can also be used \cite{Song2013SGD}. However, as mentioned earlier, the privacy guarantee degrades when the same data is used for training repeatedly. 
    
    \item \textit{Privacy amplification via sampling}: 
To remedy this issue, another line of work focuses on reducing the exposure of the data through user \cite{McMahan2018Recurrent, Asoodeh2020InfoTheory} or data point sampling \cite{Abadi2016, Bassily2014PrivateERM, Balle2018AmpSubsampling, Heikkila2020DPcrossSilo}. In \cite{McMahan2018Recurrent}, users are sampled i.i.d. according to some probability, who will then compute and send the gradients to the server for perturbation and model updates. In \cite{Bassily2014PrivateERM}, exponential mechanism is studied. In \cite{Balle2018AmpSubsampling}, various data sampling schemes, such as Poisson sampling, sampling with/without replacement, are studied and analyzed. As a result of sampling, the privacy level is \textit{amplified} \cite{Balle2018AmpSubsampling}, i.e., less noise is needed to achieve the same privacy level that is achieved by schemes without sampling. Amplification can also be obtained through shuffling \cite{Erlingsson2019Shuffling}, where a trusted shuffler shuffles the outputs from users before sending it to the server. Works that consider shuffling as part of the pipeline include \cite{Bittau2017PROCHLO, Cheu2018Distributed}. Another way to control leakage is to ensure that the sensitivity is small by carefully choosing the clipping norm \cite{Thakkar2019AdaptiveClipping}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\columnwidth]{figures/Fig-4-2-Private-FL-shuffle-model.pdf}
    \caption{Federated learning system with a shuffler, who shuffles the gradients before sending them to the PS. The trained model is subsequently released to the analyst.
    }
    \label{fig:FL_system_model_shuffle}
\end{figure}

\item \textit{Private FL over new communication models}:
The above works rely on assumption that the server (and the shuffler) is trustworthy. This assumption may not be practical in certain scenarios. To remove this assumption, local DP was proposed and studied, where each user is responsible for protecting their own data. Similar to central DP, one can ask users to directly perturb the information they want to send, e.g., in \cite{Ono2020LDPReinforcementLearning}. In addition to adding noise, one can also perturb the information by using approximation. In \cite{Smith2017Interaction}, the approximation is obtained by flipping a random bit in the input string of each user. In \cite{Duchi2013Local}, a random vector that is roughly in the same direction as the original gradient is sampled and used as an approximation by each user. However, it has been shown that techniques that let users perturb information directly to achieve LDP may suffer greatly in terms of utility, i.e., accuracy. In order to satisfy LDP and provide reasonable utility, we can again use the idea of privacy amplification using sampling and/or shuffling \cite{Erlingsson2019Shuffling, Balle2019Blanket, Ghazi2019Scalable, Balle2020RandomCheckIn, Girgis2021ShuffledModel}. Since information is perturbed at the user, honest but curious shuffler would not compromise the local DP. Intuitively, both sampling and shuffling are able to further confuse the curious party without injecting more noise. Therefore, one is able to inject less noise to maintain utility, and still achieve the desired privacy level via sampling and/or shuffling. Another line of work focuses on private FL over wireless channels \cite{Seif2020Wireless, Sonee2020quantizedWirelessDP, Koda2020WirelessDP, Liu2021privacyForFree, Hasircioglu2021WirelessDP, Seif2021privacyAmp}. With superposition property of wireless channel, the gradients can be naturally aggregated while being transmitted. It has been shown in \cite{Koda2020WirelessDP} and \cite{Liu2021privacyForFree} that, by carefully designing the power control factors, the channel noise can be used as perturbation and provides DP guarantee. Amplification results are shown in \cite{Seif2020Wireless}, where perturbation added by users that is aggregated over wireless channel enhances privacy guarantee, and in \cite{Seif2021privacyAmp}, the privacy is amplified by aggregated perturbation and the addition of user sampling in the FL pipeline. However, channel state information (CSI) is obtained with the help of the server and is crucial in these works. When the server is untrustworthy, CSI obtained from the server can be tampered to lurk users to leak information. Therefore, \cite{Hasircioglu2021WirelessDP} and \cite{Seif2021privacyAmp} study the case when CSI is not available.

\item \textit{Other privacy notions and connections to DP}: There are also works that use different privacy notions, such as Concentrated DP \cite{Dwork2016CDP, Bun2016zCDP}, Renyi DP \cite{RDP}, Bayesian DP \cite{Triastcyn2019Bayesian}, communication-constrained DP \cite{Girgis2021ShuffledModel} and information-theoretic privacy \cite{so2021codedprivateml}. Concentrated DP is a relaxed version of DP, where it ensures that leakage is centered around the expected privacy level $\epsilon_c$, and is subgaussian. The probability that leakage exceeds $\epsilon_c$ by a small amount is bounded. Unlike the standard DP, where the expected leakage is not bounded and could potentially go to infinity with probability $\delta_c$, leakage of concentrated DP does not go to infinity. Renyi DP is another relaxation of the standard DP that is based on the concept of Renyi divergence. Renyi DP can be translated to standard DP and it was shown to have better composition result in \cite{RDP}. Bayesian DP is essentially standard DP, however, the data distribution is taken into account when quantifying privacy parameters. While one can show connection between DP and information-theoretic privacy, the approaches that are used to secure data are completely different. In \cite{so2021codedprivateml}, data is kept private by using error control codes and the idea of secret sharing. The data is considered private when the mutual information of the original data and encoded data is zero. Other works such as \cite{Guo2020DPDecentralizedLearning} studies how to allocate the amount of noise added to the data by each user in a decentralized setting (without the presence of a server) so that the collective noise does not reduce the utility.
\end{enumerate}

\subsection{Secure Model Aggregation in Federated Learning}\label{sec:securemodelaggregation}

While data is kept at the user-side in FL, a user's model still carries a significant amount of information about the local dataset of this user. Specifically, as shown recently, the private training data can be reconstructed from the local models through inference or inversion attacks (see e.g.,~\cite{fredrikson2015model,nasr2019comprehensive,zhu2020deep,geiping2020inverting}). To prevent such information leakage, \textit{secure aggregation} protocols are proposed (e.g., \cite{bonawitz2017practical,so2021turbo,kadhe2020fastsecagg,zhao2021information,bell2020secure}) to protect the privacy of individual local models, both from the server and other users, while still allowing the server to learn the aggregate model of the users. 
More specifically, secure aggregation protocols ensure that, at any given round, the server can only learn the aggregate model of the users, and beyond that no further information is revealed about the individual local model of a particular user. The key idea of the secure aggregation protocols is that the users mask their models before sending them to the server.  These masks then cancel out when the server aggregates the masked models, which allows the server to learn the aggregate of the local models without revealing the individual models.  

In the secure aggregation protocol of  \cite{bonawitz2017practical}, known as SecAgg, pairwise secret keys are generated between each pair of users. For handling user dropouts, the pairwise keys in \cite{bonawitz2017practical} are secret shared among all users, and can be reconstructed by the server in case of dropouts. This protocol tolerates any $D$ dropped users and ensures privacy guarantee against up to $T$ colluding users, provided that $T+D<N$, where $N$ is the number of users. The communication cost of constructing these masks, however, scales as $O(N^2)$, which limits the scalability of this approach. Several works have considered designing communication-efficient secure aggregation protocol \cite{so2021turbo,kadhe2020fastsecagg,zhao2021information,elkordy2020secure,bell2020secure}. SecAgg+ \cite{bell2020secure} improves upon SecAgg \cite{bonawitz2017practical} by limiting the secret sharing according to a sparse random graph instead of the complete graph considered in SecAgg  \cite{bonawitz2017practical}. TurboAgg \cite{so2021turbo} overcomes the quadratic aggregation overhead of \cite{bonawitz2017practical}, achieving a secure aggregation overhead of $O(N\log N)$, while simultaneously tolerating up to a user dropout rate of $50\%$ and providing privacy against up to $N/2$ colluding users with high probability. The key idea of Turbo-Aggregate that enables communication-efficient aggregation is that it employs a multi-group circular strategy in which the users are partitioned into groups. The dropout and the privacy guarantees of TurboAgg, however, are not worst-case guarantees and it requires $\log N$ rounds.  FastSecAgg \cite{kadhe2020fastsecagg} is a $3$-round secure aggregation interactive communication-efficient protocol that is based on the Fast Fourier Transform multi-secret sharing, but it provides lower dropout and privacy guarantees compared to SecAgg \cite{bonawitz2017practical}. While all of aforementioned works in secure aggregation provide cryptographic security, the secure aggregation protocol \cite{zhao2021information} provides information-theoretic security. In addition, unlike all previous protocols that depend on the pairwise random-seed reconstruction of the dropped users, this protocol departs from the previous protocols by employing instead one-shot aggregate-mask reconstruction of the surviving users. This feature can reduce the aggregation complexity significantly. However, this protocol relies on a trusted-third party to distribute the masks over the users.  While all of the aforementioned works do not consider the bandwidth heterogeneity among the different users in secure aggregation, an adaptive secure aggregation protocol has been proposed in \cite{elkordy2020secure} which quantizes the model of each user according to the available bandwidth to improve the training accuracy.
 
 While secure aggregation seeks to resolve the issue of preserving user data privacy by masking the individual model updates, the learning protocol can be adversarially affected by Byzantine users that may aim to break or perturb the learning to their benefit \cite{blanchard2017machine,so2020byzantine,he2020secure,prakash2020mitigating,khazbak2020mlguard}.  As the local models are protected by random masks, the server cannot observe the individual user updates in the clear, which prevents the server from utilizing outlier detection protocols to protect the model against Byzantine manipulations. This problem has been recently addressed in \cite{so2020byzantine}, for the I.I.D.~setting, where the first single-server Byzantine-resilient secure aggregation protocol for secure federated learning known as BREA has been developed. BREA is based on distance based adversarial detection and leverages quantization and verifiable secret sharing to provide robustness against malicious users, while preserving the privacy of the individual user models.  
 
 
 All works on secure aggregation only guarantee the privacy of the individual users over a single aggregation round \cite{bonawitz2017practical,so2021turbo,kadhe2020fastsecagg,zhao2021information,bell2020secure}. While the privacy of the users is protected in each single round, the server can reconstruct an individual model from the aggregated models over multiple rounds of aggregation. Specifically, as a result of the client sampling strategy and the users dropouts, the server may be able to recover an individual model by exploiting the history of the aggregate models \cite{pejo2020quality, so2021securing}. This problem was studied for the first time in \cite{so2021securing} which  developed a client selection strategy known as Multi-RoundSecAgg that ensures the privacy of the individual users over all aggregation rounds while taking into account other important factors such as the aggregation fairness among the users and average number of users participating at each round (average aggregation cardinality) which control the convergence rate. 
 
 







