\subsection{Formulation}



We consider a general distributed computing framework, where the goal is to compute a function $g$ using $N$ distributed workers, while keeping the input dataset $\boldsymbol{X}$ secure (illustrated in Fig.~\ref{fig:overview}). 
The $N$ workers are assigned encrypted versions of the input using $N$ encoding functions $\boldsymbol{c}\triangleq (c_1,...,c_N)$, 
then each worker computes a function $f$ over the assigned share, which can be viewed as building blocks of computing $g$.

\begin{figure}[t]
\centering
\includegraphics[scale=0.4]{figures/fig-privateComputing.pdf}
\caption{An illustration of private computation. %A collection of workers aim to compute a function $g$ given an input dataset, where each worker can return an evaluation of a function $f$ with possibly coded data assignments. By carefully designing the coding functions ($c_i$'s), . 
}
\label{fig:overview}
\end{figure}

This framework captures many commonly used operations.  One example is \emph{block matrix multiplication}, where the goal is to compute the product $A^\intercal B$ given two large matrices $A\in \bF^{s\times t}$ and $B\in \bF^{s\times r}$. 
Here the input dataset is $X=(A,B)$, and the computation task is $g(A,B)=A^\intercal B$. Given some partitioning parameters $p$, $m$, and $n$, the input matrices are partitioned block-wise into $p$-by-$m$ and $p$-by-$n$ sub-blocks of equal sizes, respectively. Then each worker is assigned a pair of sub-blocks and computes their product, i.e., the function $f$ is the multiplication of two matrices of sizes $\bF^{\frac{t}{m}\times \frac{s}{p}}$ and $\bF^{\frac{s}{p}\times \frac{r}{n}}$. If there are no security requirements, the final result can be recovered using $N=pmn$ workers, by having each worker compute a  product of certain uncoded submatrices. 


Another example is to compute \emph{multivariate polynomials} on a dataset $X$. Particularly, given a general polynomial $f$, the input dataset is partitioned into $K$ subsets $X_1,\ldots,X_K$, and the goal is to compute $g(\boldsymbol{X})=(f(X_1),\ldots,f(X_K))$. If each worker can compute a single evaluation of $f$, then a computing design using $N=K$ workers can be obtained by assigning each worker a disjoint uncoded subset of the input.

However, in secure computing, we aim to carry out the computation with an additional requirement that the entire input dataset is information-theoretically secure from the workers, even if up to a certain number of them can collude. In particular, a set of encoding functions $\boldsymbol{c}\triangleq (c_1,\ldots,c_N)$ is $T$-secure, if 
    $I(\{c_i(X)\}_{i\in\mathcal{T}};X)=0$
for any subset $\mathcal{T}$ with a size of at most $T$, where $X$ is generated uniformly at random.

{\it Tradeoffs in secure distributed computing:} The goal is to design the encoding functions to achieve a tradeoff between the resources/constraints while ensuring the reliable recovery of the desired computation. Specifically, the resources could correspond to the number of available workers, storage and computation performed per worker. In addition to $T$-security, one may also be interested in communication efficient designs to account for bandwidth constraints (between master node and the workers). In the past few years, there have been significant progress in using ideas from coding/information theory to devise new schemes, and ultimately towards understanding these fundamental tradeoffs. As an example, a large body of work \cite{lee2015speeding, tandon2016gradient, yu2017optimally, dutta2016short, 8949560} have focused on the problem of distributed computing in the presence of stragglers. Here, since the overall latency of computation can be limited by the slowest workers, the goal is to design schemes which minimize the number of workers required to carry out the computation,  while satisfying the security requirement. 
More rigorously, let $\mathcal{C}_T$ denotes the set of allowable\footnote{The set $\mathcal{C}_T$ also captures practical constraints such as encoding complexities.} encoding function designs that are $T$-secure. Then, in a secure coded computing problem, given fixed parameters  $f,g,$ and $\mathcal{C}_T$, one aim could be to find computing schemes $\boldsymbol{c}\in \mathcal{C}_T$ that use as small number of workers $N$ as possible. Alternatively, when the total number of workers $N$ is fixed, one may be interested in the design of $T$-secure schemes with minimum download communication overhead. The capacity of secure distributed computation, analogous to that of PIR, can then be defined as the supremum of the ratio of the number of bits of desired information (the desired function, $g(\mathbf{X})$), to the total number of bits downloaded from the $N$ servers. 



%In the presence of stragglers, the decoder waits for a subset of fastest workers until $g(\boldsymbol{X})$ can be recovered given the returned results from the workers using some decoding functions. 

%We say a coded computing scheme achieves a \emph{recovery threshold} of $R$, if the master can correctly decode the final output given the computing results from \emph{any} subset of at least $R$ workers. 
%This is an equivalent measure of the number of stragglers \new{(as well as the number of Byzantine adversaries)} that can be tolerated. 


%-
\subsection{Schemes for Private Distributed Computing}
Information-theoretically secure distributed computing has its origins in the celebrated work of Ben-Or Goldwasser Micali (BGW protocol) on tasks involving linear/bilinear computations. Specifically, the master node creates $N$ coded shares with $T$-secure guarantees (using Shamir's secret sharing scheme) which are subsequently sent to the workers. The workers subsequently compute the function on the coded shares. In a recent work \cite{Bitar-Secure-staircase}, \textit{staircase codes}, presented originally for a PIR problem \cite{bitar2018staircase}, were combined with the idea of secret sharing to minimize the overall latency for secure distributed matrix multiplication. 

Lagrange coded computing (LCC) \cite{pmlr-v89-yu19b} has been proposed to provide a unified solution for computing general multivariate polynomials. In comparison to the classical BGW (or similar Shamir's secret sharing based protocols), LCC reduces the amount of storage, communication and randomness overhead.   Given any fixed parameter $T$, LCC encodes the input variables using the following \textit{Lagrange interpolation polynomial}
\begin{align*}
    c(x)\triangleq \sum_{j\in[K]}X_j\cdot \prod_{k\in [K+T]\setminus\{j\}}\frac{x-x_k}{x_j-x_k}+
    \sum_{j=K+1}^{K+T} Z_j\cdot \prod_{k\in [K+T]\setminus\{j\}}\frac{x-x_k}{x_j-x_k},
\end{align*}
where $x_1,\ldots,x_{K+T}$ are some arbitrary distinct elements from the base field $\bF$, and $Z_i$'s are some random cryptographic keys generated uniformly\footnote{We assume that $\bF$ is finite so that the uniform distribution is well defined. } at random on the domain of $X_i$'s. Each worker $i$ selects a distinct variable $y_i$ from the base field that is not from $\{x_{1},\ldots,x_{K}\}$, and obtains $\tilde{X}_i\triangleq c(y_i)$ as the coded variable. LCC is $T$-secure, because the coded variables sent to any subset of $T$ workers are padded by an invertible linear transformation of $T$ random keys, which are jointly uniformly random. 
    
After each worker $i$ applies function $f$ over the coded inputs, they essentially evaluate the composed polynomial $f(c)$ at point $y_i$. On the other hand, the evaluations of the same polynomial at $x_1,\ldots,x_{K}$ are exactly the $K$ needed final results. Hence, by polynomial interpolation, the decoder can recover all final results by recovering $f(c)$, by receiving results from any subset of workers with a size greater than the degree of  $f(c)$. More precisely, let $\textup{deg} f$ denote the total degree of polynomial $f$, the degree of the composed polynomial equals $(K-1)\textup{deg} f$. Thus, LCC computes any multivariate polynomial with at most $N=(K-1)\textup{deg} f+1 $ workers. 

Secure coded computation has also been studied for different computation tasks and settings. A majority of works are on matrix multiplication \cite{chang2018capacity, 8382305, DBLP:journals/corr/abs-1810-13006, DBLP:journals/corr/abs-1812-09962, 8613446, DBLP:journals/corr/abs-1901-07705, kim2019private, 8675905 ,chang2019upload, 8761275, nodehi2019secure, jia2019capacity, kakar2019uplinkdownlink, aliasgari2019private, d2019degree, 9174167}, and it has been shown in \cite{8437563, 9174167} that for block-partition-based designs, the optimum number of workers to enable secure computation can be within a constant factor of a fundamental quantity called the bilinear complexity \cite{gs005}. Private gradient computation was studied in \cite{8849245} and it was shown that the optimal coding design is encoding the input variables using harmonic sequences. \cite{9333639,8849547} considered a setting where the workers send compressed versions of their computing results to tradeoff the download communication cost and the required number of workers. 

LCC has also been widely leveraged to enable privacy-preserving machine learning~\cite{so2021codedprivateml,copML}. In particular, authors in~\cite{so2021codedprivateml} have considered a scenario in which a data-owner (e.g., a hospital) wishes to train a logistic regression model by offloading the large volume of data (e.g., healthcare records) and computationally-intensive training tasks (e.g., gradient computations) to $N$ machines over a cloud platform, while ensuring that any collusions between $T$ out of $N$ workers do not leak information about the dataset. In this setting, CodedPrivateML~\cite{so2021codedprivateml} has been proposed, which leverages LCC, to provide three salient features:
\begin{enumerate}
\item it provides strong information-theoretic privacy guarantees for both the training dataset and model parameters. 
\item it enables fast training by distributing the training computation load effectively across several workers.
\item it secret shares the dataset and model parameters using coding and information theory principles, which significantly reduces the training time. 
\end{enumerate}
LCC has also been leveraged to break a fundamental ``quadratic barrier'' for secure model aggregation in federated learning~\cite{so2021turbo}. We defer the discussion on this topic to the next section. 

Within the scope of this article, the connection between PIR, secure distributed computing, and private federated learning is  exemplified by the idea of cross-subspace alignment (CSA) which extends to all three domains.  CSA codes originated in \cite{Jia_Sun_Jafar_XSTPIR} as a solution to XS-TPIR, i.e., the problem of $T$-private information retrieval from $N$ servers that store $K$ messages in an $X$-secure fashion. CSA codes then found applications in private secure coded computation \cite{Kim_Lee_PSCC, Chang_Tandon_PSDMM, Jia_Jafar_MDSXSTPIR}, and in particular secure distributed matrix multiplication (SDMM) \cite{Chang_Tandon_SDMMOS}. CSA codes were first applied to SDMM by Kakar et al. in \cite{Kakar_Ebadifar_Sezgin_CSA}, and subsequently applied to  secure distributed \emph{batch} matrix multiplication  (SDBMM) by Jia et al. in \cite{Jia_Jafar_SDBMM}. These works produced sharp capacity\footnote{Analogous to PIR, the capacity of SDBMM is defined as the supremum of the ratio of the number of bits of desired information (the desired matrix products), to the total number of bits downloaded  from the $N$ servers.} characterizations for various cases. For example, in \cite{Jia_Jafar_SDBMM} the capacity for $X$-secure distributed computation by $N$ servers of a batch of outer products of two vectors is shown to be $(1-X/N)^+$,  the capacity for computing the inner product of two  length-$K$ vectors is  $\frac{1}{K}(1-\frac{X}{N})^+$ when $N\leq 2X$, and for long vectors $(K\rightarrow\infty)$ the capacity of computing inner products is shown to be $(1-2X/N)^+$.  While LCC \cite{pmlr-v89-yu19b} codes and CSA codes originated in seemingly unrelated contexts of distributed secure computing and secure PIR, there are interesting connections between them. For example, in a special case of secure multiparty/distributed batch matrix multiplications, CSA codes yield LCC codes as a special case\cite{Jia_Jafar_CSA_MM, Chen_Jia_Wang_Jafar}. The generalization inherent in CSA codes is beneficial primarily in download-limited settings, where CSA codes are able to strictly outperform LCC codes. However, it is worth mentioning that to achieve order-optimal performances, entangled polynomial codes \cite{8437563, 9174167} should be applied, which enables coding over bilinear-complexity-based algebraic structures, and achieving order-wise improvements.  Finally, in the domain of federated learning, CSA codes were applied in \cite{Jia_Jafar_XSTPFSL} to find a solution to the problem of $X$-secure $T$-private federated submodel learning. Fundamentally, this is a problem of privately reading from and privately writing to a database comprising $K$ files (messages/submodels) that are stored across $N$ distributed servers in an $X$-secure fashion. The CSA read-write scheme of \cite{Jia_Jafar_XSTPFSL}  is able to fully update the storage at all $N$ servers after each write operation even if some of the servers (up to a specified threshold value) are inaccessible, and  achieves a synergistic gain from the joint design of private-read and private-write operations. Intuitively, the connection between these problems arises because the operation required at each server for many (but not all) PIR (and private write) schemes can be interpreted as a \emph{matrix multiplication} between a threshold-$T$ secret-shared query vector/matrix (polynomial encoded for $T$-privacy) and a threshold-$X$ secret-shared data vector/matrix (polynomial encoded for $X$-security), which produces various desired and undesired products. CSA codes are characterized by a Cauchy-Vandermonde structure that facilitates interference alignment of undesired products along the Vandermonde terms, while the desired products remain separable along the Cauchy terms. This alignment structure allows efficient downloads by reducing interference dimensions. Therefore, to the extent that a multiplication of polynomial encoded matrices is involved, and download efficiency is of concern, the same Cauchy-Vandermonde alignment structure facilitated by CSA codes turns out to be useful across these problems. It is also noteworthy that applications of CSA codes generalize naturally beyond matrix products, to tensor products, as seen in Double Blind Private Information Retrieval ($M$-way blind PIR in general) \cite{Lu_Jia_Jafar_DBTPIR}. 