%\documentclass[12pt,final,twocolumn]{IEEEtran}
\documentclass[12pt,draftcls,onecolumn]{IEEEtran}
%\documentclass[final,conference]{IEEEtran}
%\documentclass[journal,draftcls,onecolumn,12pt,twoside]{IEEEtran}
% [draftcls, draftclsnofoot, compsoc]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{multirow}
\usepackage[utf8]{inputenc}
\usepackage{dblfloatfix}
\usepackage{bbm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{color}
\usepackage{cite}
\usepackage{graphicx}
\graphicspath{{./}{Figs/}}
\usepackage{subcaption}
\usepackage{mathrsfs}
\usepackage{verbatim}
\usepackage{indentfirst}
%\usepackage[titletoc,toc,title]{appendix}
%\usepackage{appendix}
\usepackage{url}
\usepackage{epsfig,endnotes,amsthm,changepage}
\usepackage{authblk}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{epstopdf,lipsum,etoolbox}

\usepackage{graphicx}
\usepackage{amsfonts}
%\usepackage[cmex10]{amsmath}
%\usepackage{graphicx}
\usepackage{setspace}
\usepackage{cite}
\usepackage{array}
%\usepackage{algorithmic}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{mdwtab, stmaryrd}
%%\usepackage[caption=false]{caption}
%\usepackage{subfigure}
%\usepackage[center]{caption}
%\usepackage[font=footnotesize]{subfig}

%\usepackage{fixltx2e}
%\usepackage{url}
\usepackage{dsfont}
\usepackage{times}
\usepackage{epsfig}
\usepackage{latexsym}
\usepackage{epstopdf}
\usepackage{verbatim}
\usepackage{units}
\usepackage{amsthm}
\usepackage{mdwlist}
\usepackage{dblfloatfix}
\usepackage{blindtext}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{tabu}
%\usepackage{algorithmic}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
%\providecommand{\keywords}[1]{\textbf{\textit{Index terms---}} #1}

\newcommand\Tstrut{\rule{0pt}{2.7ex}}  

\newcommand{\red}[1]{\textcolor[rgb]{1,0,0}{#1}}
\newcommand{\gre}[1]{\textcolor[rgb]{0,1,0}{#1}}

%\usepackage{epstopdf}
%\epstopdfDeclareGraphicsRule{.pdf}{png}{.png}{convert #1 \OutputFile}
\AppendGraphicsExtensions{.pdf}
\usepackage{epstopdf,lipsum,etoolbox}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem*{proposition*}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\newcommand{\new}[1]{#1}

 \newcommand{\bF}{\mathbb{F}}
 \newcommand{\bV}{\mathbb{V}}
 \newcommand{\bU}{\mathbb{U}}
 \newcommand{\bW}{\mathbb{W}}
 \newcommand{\bR}{\mathbb{R}}
 \newcommand{\cC}{\mathcal{C}}
 \newcommand{\cS}{\mathcal{S}}
 \newcommand{\cT}{\mathcal{T}}

\newcommand{\Expt}{\mbox{${\mathbb E}$} }
%\newcommand{\Expth}{\mbox{$\hat{{\mathbb E}}$} }
%\renewcommand{\vec}[1]{\mbox{\boldmath$#1$}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup}


\allowdisplaybreaks

\begin{document}

\title{Private Retrieval, Computing and Learning: Recent Progress and Future Challenges \footnote{Sennur Ulukus is with University of Maryland (email: ulukus@umd.edu). Salman Avestimehr is with University of Southern California (email: avestimehr@ee.usc.edu). Michael Gastpar is with EPFL (email: michael.gastpar@epfl.ch). Syed Jafar is with University of California Irvine (email: syed@ece.uci.edu). Ravi Tandon is with University of Arizona (email: tandonr@email.arizona.edu). Chao Tian is with Texas A\&M University (email: chao.tian@tamu.edu).}}
\author{Sennur Ulukus, Salman Avestimehr, Michael Gastpar, Syed Jafar, \\ \vspace*{-0.4cm} Ravi Tandon, Chao Tian}

\maketitle

\vspace*{-1cm}

\begin{abstract}
Most of our lives are conducted in the cyberspace. The human notion of privacy translates into a cyber notion of privacy on many functions that take place in the cyberspace. This article focuses on three such functions: how to privately retrieve information from cyberspace (privacy in information retrieval), how to privately leverage large-scale distributed/parallel processing (privacy in distributed computing), and how to learn/train machine learning models from private data spread across multiple users (privacy in distributed (federated) learning). The article motivates each privacy setting, describes the problem formulation, summarizes breakthrough results in the history of each problem, and gives recent results and discusses some of the major ideas that emerged in each field. In addition, the cross-cutting techniques and interconnections between the three topics are discussed along with a set of open problems and challenges. 
\end{abstract}

\section{Introduction} \label{sect:intro}

Privacy is an important part of human life. This article considers privacy in the context of three distinct but related engineering applications, namely, privacy in retrieving information, privacy in computing functions, and privacy in learning. In the first sub-topic of private information retrieval, a user wishes to download a content from publicly accessible databases in such a way that the databases do not learn which particular content the user has downloaded. Towards that goal, the user creates ambiguity by its actions during the download. This strategy prevents databases from guessing which content the user has downloaded. This in turn preserves the user's privacy because what is downloaded leaks information about interest and intent on the part of the user. In the second sub-topic of private computing, a user wishes to compute a function but does not have resources to perform the computation on its own. Thus, the user outsources the computation to many distributed servers. This necessitates the user send its data, which is private, to the distributed servers. The goal of the user is to utilize the servers for computation while preserving the privacy of its own data. To achieve that goal, the user introduces randomness in its data so that the servers cannot decipher the data while they are able to perform the computation successfully. In the third sub-topic of privacy in learning, a centralized unit (parameter server) wishes to train a learning model by utilizing distributed users (clients). The parameter server needs labeled training data to train the model. The data resides at the users, and the users prefer to keep their data at their site, i.e., not send it to the parameter server, to preserve the privacy of their data. Thus, such distributed (federated) learning has built-in privacy advantages. However, even then, the computations (e.g., gradients calculated on the data) may leak some information about the raw data. To prevent that, the users may want to add randomness to the calculation they send to the parameter server in order to further preserve their privacy.      

The underlying threat model common to all three settings is the undesired leak of information that is considered private by the respective entities. In the case of private information retrieval, the leak is about the identity (index) of the content being downloaded/accessed. In the case of private computation, the leak is about the user data on which computation needs to performed by distributed servers. In the case of private learning, the leak is user (client) data that is used to train the learning model. A common aspect of the solution approach to these problems is to randomize the information/actions in such a way to hide the private information. In private information retrieval, this corresponds to randomizing the downloads such that a certain download may happen equally likely for all possible user content requirements. In private computation, randomization is achieved by adding appropriate noise to the data whose effect can be nullified during the computation. In private learning, privacy of clients is achieved by keeping the data at the client side, and also my randomizing the transmitted calculations so that leaks are prevented.  

We present private information retrieval in Section~\ref{sect:pir}, private distributed computation in Section~\ref{sect:pdc} and private distributed machine learning in Section~\ref{sect:pml}. We conclude this article in Section~Section~\ref{sect:conc} by listing a few challenges and open problems.

\section{Private Information Retrieval} \label{sect:pir}
\input{PIR_part1}

\subsection{Connections to Other Security Primitives}
\input{PIR_part3}

\section{Private Distributed Computing} \label{sect:pdc}
\input{3-PrivateComputing}

\section{Private Federated Learning} \label{sect:pml}
\input{Private-FL}

\section{Discussion: Challenges and Open Problems} \label{sect:conc}

In this article, we have surveyed the privacy issues in information retrieval, distributed computation, and distributed (federated) learning. We conclude this article with the following incomplete list of remaining challenges and open problems in these areas.

\emph{Challenges and open problems in private information retrieval:}

\begin{itemize}
\item Coded colluding databases: The PIR problem is completely solved when the database content is coded for the case when the databases do not collude, and is also completely solved when the databases collude for the case when the database content is replicated (uncoded). However, the problem is open when database content is coded, potentially secured and the databases may collude. Remarkably, even the asymptotic capacity (for large number of messages) remains open. While the lower bound for U-B-XS-MDS-TPIR in Table \ref{table:capacity} is conjectured to be aymptotically optimal, the asymptotic capacity $C_\infty$ remains unknown in almost all cases.
\item Non-replicated databases: The basic form of PIR assumes that the databases contain exactly the same set of files. In reality, the databases will have some overlap in content and will also have distinct items. When the databases have arbitrary contents, the PIR capacity problem is open, with a few notable exceptions\cite{raviv2019GPIR, Karim_nonreplicated, Jia_Jafar_GXSTPIR,Sadeh_Gu_Tamo}. The challenge here is to be able exploit the replication to reduce the download cost, while at the same time deal with non-replication as efficiently as possible. 
\item Upload cost and message size: In the capacity formulation, the upload cost is largely ignored. However, when the message sizes are not very large, the consideration on the upload cost becomes important. The problem then becomes how to construct codes for small message sizes to achieve a smaller upload cost. In general, PIR schemes should be designed to minimize a combined measure of upload and download costs. 
\item Weakly private (leaky) information retrieval: In some practical applications of PIR, it may not be absolutely necessary to require perfect privacy, and a small leakage may be tolerable. In this setting, two questions stand out: What are good metric(s) to measure the leakage, and how to characterize the capacity as a function of these metrics. 
\item More complex message structure: The messages are usually required to be independent (and of the same length). What is the optimal coding strategy when the messages are dependent, either as overlapping parts, or are dependent following a general probability law?
\item Privacy, stragglers and timeliness of retrieval: While PIR focuses mainly only on the privacy of downloaded information, it assumes that the servers are ideal, which respond to queries immediately and with no delays. Robust PIR problem considers the case of servers being completely unresponsive \cite{Sun_Jafar_TPIR, bitar2018staircase}. However, most servers respond eventually, albeit slowly in many cases. Therefore, there is a need to design systems where information may be downloaded privately but also in a timely manner. An initial consideration of this issue is presented in a recent paper in \cite{TimelyPIR}, but the problem remains largely open. 
\end{itemize}

\emph{Challenges and open problems in private distributed computing:}
\begin{itemize}
\item Optimal coding for block matrix multiplication: In a standard coded computing setup, we aim to find coding designs to minimize the recovery threshold \cite{NIPS2017_7027} (or the number of workers when no straggler is present) given fixed constraints on computation, security, and privacy. While the optimal recovery thresholds have been characterized within a factor of $2$ for block matrix multiplication  \cite{8437563, 9174167}, its exact characterization is not known except for some boundary cases. In particular, a remaining interesting open problem is to show whether the factor-of-$2$ penalty in the state-of-the-art upper bound is necessary when all partition parameters are large.
\item Analog coded computing: Most works in coded computing such as \cite{NIPS2017_7027,pmlr-v89-yu19b,9174167,soleymani2021list,8437563} rely on quantizing the data into finite fields and then leveraging coding-theoretic techniques to mitigate stragglers and Byzantine workers, and to provide data privacy. This approach, however, degrades the accuracy of the computations \cite{soleymani2021analog,soleymani2020privacy}. In fact, this is a limitation of many other problems such as verifiable computing and machine learning \cite{ghodsi2017safetynets,ali2020polynomial}. Recently, several works have extended LCC to the analog domain to address these challenges \cite{subramaniam2019collaborative,jahani2020berrut,soleymani2021analog,soleymani2020privacy}, but they either focus only on straggler-mitigation \cite{jahani2020berrut}, privacy \cite{soleymani2021analog,soleymani2020privacy} or Byzantine-robustness \cite{subramaniam2019collaborative}. An interesting open problem is to design a framework that jointly tackles these three challenges in the analog domain. 
\end{itemize}

\emph{Challenges and open problems in private federated learning:}
\begin{itemize}
\item Secure and Byzantine-robust aggregation in federated learning: While BREA \cite{so2020byzantine} has considered secure aggregation and mitigating Byzantine users jointly, it has only focused on the i.i.d.~setting. Extending BREA to the non-i.i.d.~setting is an interesting future direction. The main challenge in this direction is to determine whether the updates that may seem deviating are due to the users having non-i.i.d.~data or because of Byzantine users sending erroneous updates. 
\item Secure aggregation and multi-round secure aggregation: 
There are many open problems related to the multi-round secure aggregation problem introduced in \cite{so2021securing}. While the secure aggregation protocols are believed to protect the privacy the of the individual users, it is not clear whether such protocols ensure privacy in the information-theoretic sense. Specifically, the secure aggregation protocols ensure that the server only learns the aggregate model of the users. However, the aggregate model of the users may still reveal information about the individual users and characterizing such a leakage is an important problem. While Multi-RoundSecAgg provides a trade-off between between the multi-round privacy, the average aggregation cardinality, and the aggregation fairness, investigating the optimality of Multi-RoundSecAgg remains an open problem.  
\end{itemize}

\bibliographystyle{IEEEtran}
%\bibliographystyle{unsrt}
\bibliography{referencesPrivateComputing,CSPIR,PIRref,PIRj,FL-DP-Ref,PIR-papers-su}

\end{document}
