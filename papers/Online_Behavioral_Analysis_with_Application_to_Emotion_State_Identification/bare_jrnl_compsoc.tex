%% bare_jrnl_compsoc.tex
%% V1.4a
%% 2014/09/17
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% Computer Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE!
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_conf_compsoc.tex,
%%                    bare_jrnl_compsoc.tex, bare_jrnl_transmag.tex
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices and paper sizes can       ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


\documentclass[10pt,journal,compsoc]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran

%\usepackage{spconf,amsmath,graphicx}

\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{epstopdf}
\usepackage{float}




% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later. Note also the use of a CLASSOPTION conditional provided by
% IEEEtran.cls V1.7 and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/tex-archive/info/epslatex/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex






% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Do not use the stfloats baselinefloat ability as IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/dblfloatfix/


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and
% Axel Sommerfeldt. This package may be useful when used in conjunction with
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a
% page by themselves.

% *** PDF, URL AND HYPERLINK PACKAGES ***
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/url/
% Basically, \url{my_url_here}.

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Online Behavioral Analysis with Application to Emotion State Identification}

% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%
%
%\IEEEcompsocitemizethanks is a special \thanks that produces the bulleted
% lists the Computer Society journals use for "first footnote" author
% affiliations. Use \IEEEcompsocthanksitem which works much like \item
% for each affiliation group. When not in compsoc mode,
% \IEEEcompsocitemizethanks becomes like \thanks and
% \IEEEcompsocthanksitem becomes a line break with idention. This
% facilitates dual compilation, although admittedly the differences in the
% desired content of \author between the different types of papers makes a
% one-size-fits-all approach a daunting prospect. For instance, compsoc
% journal papers have the author affiliations above the "Manuscript
% received ..."  text while in non-compsoc journals this is reversed. Sigh.

\author{Lei~Gao,~\IEEEmembership{student Member,~IEEE,}
        Lin~Qi,
        and~Ling~Guan,~\IEEEmembership{Fellow,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem L. Gao and L. Guan is with the Department of Electrical and Computer Engineering, Ryerson University, Canada.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: iegaolei@gmail.com; lguan@ee.ryerson.ca.
\IEEEcompsocthanksitem L. Qi is with the School of Information Engineering, Zhengzhou University, China.\protect\\
Email: ielqi@zzu.edu.cn}}% <-this % stops an unwanted space
%\thanks{Manuscript received April 19, 2005; revised September 17, 2014.}}

% note the % following the last \IEEEmembership and also \thanks -
% these prevent an unwanted space from occurring between the last author name
% and the end of the author line. i.e., if you had this:
%
% \author{....lastname \thanks{...} \thanks{...} }
%                     ^------------^------------^----Do not want these spaces!
%
% a space would be appended to the last name and could cause every name on that
% line to be shifted left slightly. This is one of those "LaTeX things". For
% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
% "AB" then you have to do: "\textbf{A}\textbf{B}"
% \thanks is no different in this regard, so shield the last } of each \thanks
% that ends a line with a % and do not let a space in before the next \thanks.
% Spaces after \IEEEmembership other than the last one are OK (and needed) as
% you are supposed to have spaces between the names. For what it is worth,
% this is a minor point as most people would not even notice if the said evil
% space somehow managed to creep in.



% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~13, No.~9, September~2014}%
%{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.



% The publisher's ID mark at the bottom of the page is less important with
% Computer Society journal papers as those publications place the marks
% outside of the main text columns and, therefore, unlike regular IEEE
% journals, the available text space is not reduced by their presence.
% If you want to put a publisher's ID mark on the page you can do it like
% this:
%\IEEEpubid{0000--0000/00\$00.00~\copyright~2014 IEEE}
% or like this to get the Computer Society new two part style.
%\IEEEpubid{\makebox[\columnwidth]{\hfill 0000--0000/00/\$00.00~\copyright~2014 IEEE}%
%\hspace{\columnsep}\makebox[\columnwidth]{Published by the IEEE Computer Society\hfill}}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark (Computer Society jorunal
% papers don't need this extra clearance.)



% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}



% for Computer Society papers, we must declare the abstract and index terms
% PRIOR to the title within the \IEEEtitleabstractindextext IEEEtran
% command as these need to go into the title area created by \maketitle.
% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\IEEEtitleabstractindextext{%
\begin{abstract}
In this paper, we propose a novel discriminative model for online behavioral analysis with application to emotion state identification. The proposed model is able to extract more discriminative characteristics from behavioral data effectively and find the direction of optimal projection efficiently to satisfy requirements of online data analysis, leading to better utilization of the behavioral information to produce more accurate recognition results.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Discriminative model, Online behavioral analysis, Emotion state identification.
\end{IEEEkeywords}}
% make the title area
\maketitle


% To allow for easy dual compilation without having to reenter the
% abstract/keywords data, the \IEEEtitleabstractindextext text will
% not be used in maketitle, but will appear (i.e., to be "transported")
% here as \IEEEdisplaynontitleabstractindextext when the compsoc
% or transmag modes are not selected <OR> if conference mode is selected
% - because all conference papers position the abstract like regular
% papers do.
\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}
% Computer Society journal (but not conference!) papers do something unusual
% with the very first section heading (almost always called "Introduction").
% They place it ABOVE the main text! IEEEtran.cls does not automatically do
% this for you, but you can achieve this effect with the provided
% \IEEEraisesectionheading{} command. Note the need to keep any \label that
% is to refer to the section immediately after \section in the above as
% \IEEEraisesectionheading puts \section within a raised box.
% The very first letter is a 2 line initial drop letter followed
% by the rest of the first word in caps (small caps for compsoc).
% form to use if the first word consists of a single letter:
% \IEEEPARstart{A}{demo} file is ....
% form to use if you need the single drop letter followed by
% normal text (unknown if ever used by IEEE):
% \IEEEPARstart{A}{}demo file is ....
% Some journals put the first two words in caps:
% \IEEEPARstart{T}{his demo} file is ....
% Here we have the typical use of a "T" for an initial drop letter
% and "HIS" in caps to complete the first word.


\IEEEPARstart{W}{ith} proliferation of web applications, such as search engine, e-education, e-commerce, social networking service and online gaming, more and more behavioral information are available online. Therefore, the need has arisen for a more natural communication interface between humans and web through online behavioral data analytics. To make the online behavioral data analytics more natural and friendly, it would be beneficial to give web the ability to recognize situations similar to how humans do.\\\indent
Thanks to the recent advancement in science and technology, behavioral data analytics has been advanced at a rapid speed. For example, users can use hand gestures for expression of their feelings and notifications of their thoughts, providing an attractive and natural interface to the web. Benefitting from the depth images, action analysis has been applied to autonomous video surveillance, video retrieval and human computer interaction. As another dimension of human behavior analysis, emotion state identification contributes to applications such as learning environment, entertainment, educational software and others.\\\indent Generally, online behavioral analytics is studied for two major purposes, \textit{understanding} and \textit{prediction}. In terms of understanding, machine learning algorithms such as principal component analysis (PCA), linear discriminant analysis (LDA), support vector machines (SVM) are adopted. On the other hand, Bayesian, Neutral network and hidden Markov model are normally considered to address the prediction problem. Since a predictive model (e.g. probabilistic or neural network based) does not necessarily need to be understood by a human, the focus of this paper is developing a discriminative model to support understanding of human behaviors and mobility patterns.\\\indent
Although researches in online behavioral analysis have advanced rapidly in recent years, realistically emulating the behavioral analysis capacity of the human brain is still far from mature. The main reason is that human brain is a natural behavioral analysis system, performing the task by studying multi-modal behavioral information from the different sensory modalities, such as sight, sound, touch, smell, self-motion and taste to have meaningful perceptual experiences. During the last few years, behavioral analysis was improved (reaction time and accuracy) when the objects were presented with multi-modal features compared to single modal features alone [1], suggesting that studying multi-modal information is a promising direction to explore in behavioral analysis. When one modality fails or is not good enough to determine a certain behavior, the other modalities may help to improve the performance.\\\indent Although the study of  multi-modal data for online behavioral analysis has been drawing attentions of the research community [2-3], it faces major challenges in the identification of the inherent relationship between different modalities, and the design of a fusion strategy that can effectively utilize the discriminatory information presented in different channels. \\\indent In this paper, we present a discriminative model for online behavioral analytics with application to emotion state identification. At first, it finds projected directions to maximize the correlation among multiple behavioral data in order to identify the inherent relationship between different modalities. Second, based on the proposed model, we verify that the best performance by discriminative representation achieves when only a small fraction of the data needs to be analyzed in numerous popular applications such as emotion recognition, digit and English character recognition. The effectiveness of the proposed model is demonstrated using comparison with serial fusion [4] and methods based on similar principles such as CCA [5], DCCA [6] and MCCA [7].
\section{The Discriminative Model}
In this section, we introduce a discriminative model to identify the inherent relationship and extract discriminatory representations between different modalities. The advantages of the proposed model for multi-modal behavioral data fusion rest on the following facts: 1) the correlation among the variables in multiple channels is taken as the metric of the similarity between the variables; 2) the within-class similarity and the between-class dissimilarity are considered jointly to extract discriminatory information.\\\indent Given \emph{$ P $} sets of zero-mean random behavioral features ${x_1} \in {R^{{m_1} \times n}},{x_2} \in {R^{{m_2} \times n}}, \cdots {x_P} \in {R^{{m_P} \times n}}$ for \textit{c} classes and $Q = {m_1} + {m_2} +  \cdots {m_P}$. Concretely, the discriminative model aims to seek the projected vectors $\omega  = {[{\omega _1}^T,{\omega _2}^T \cdots {\omega _P}^T]^T}$ $({\omega_1} \in {R^{{m_1} \times Q}},{\omega_2} \in {R^{{m_2} \times Q}}, \cdots {\omega_P} \in {R^{{m_P} \times Q}})$ for fused features extraction so that the within-class correlation is maximized and the between-class correlation is minimized. Specifically, it is formulated as the following optimization problem:
\begin{equation} \mathop {\arg \max \rho }\limits_{{\omega _1},{\omega _2} \cdots {\omega _P}}  = \frac{1}{{P(P - 1)}}\sum\limits_{\scriptstyle k,m = 1\hfill\atop \scriptstyle{\rm{ }}k \ne m\hfill}^P {{\omega _k}^T\mathop {{C_{{x_k}{x_m}}}}\limits^ \sim  {\omega _m}} {\rm{  (}}k \ne m{\rm{)}} \end{equation}
Subject to
\begin{equation} \sum\limits_{k = 1}^P {{\omega _k}^T{C_{{x_k}{x_k}}}{\omega _k}}  = P \end{equation}
where $\mathop {{C_{{x_k}{x_m}}}} \limits^ \sim  = {C_w} -  {C_b},{\rm{ }}{C_{{x_k}{x_k}}} = {x_k}{x_k}^T$. ${C_w}$ and ${C_b}$ denote the within-class and between-class correlation matrixes of \emph{$ P $} sets, respectively. The definition of $\mathop {{C_{{x_k}{x_m}}}}\limits^ \sim$ indicates that the optimal solution to (1) achieves by simultaneously minimizing the between-class correlation and maximizing the within-class correlation.\\\indent Based on the mathematical analysis in Appendix A, $C_w$ and $C_b$ can be explicitly expressed in equation (3) and (4).
\begin{equation}
{{C_w} = {x_k}A{x_m}^T} \end{equation}
\begin{equation} {C_b} =  - {x_k}A{x_m}^T \end{equation}
\begin{equation} \ A = \left[ {\left( {\begin{array}{*{20}{c}}{{H_{{n_{i1}} \times {n_{i1}}}}}& \ldots &0\\
 \vdots &{{H_{{n_{il}} \times {n_{il}}}}}& \vdots \\
0& \ldots &{{H_{_{{n_{ic}} \times {n_{ic}}}}}}
\end{array}} \right)} \right] \in {R^{n \times n}} \end{equation}
where ${n_{il}}$ is the number of samples in the \textit{l}th class of the set $x_i$ and ${H_{{n_{i{1}}} \times {n_{i1}}}}$ is in the form of ${n_{i1}} \times {n_{i1}}$ with unit values for all the elements. \\ Substituting equation (3) and (4) into (1) yields:
\begin{equation} \frac{{1}}{{P - 1}}(C - D)\omega  = \rho D\omega \end{equation}
where
\begin{equation} \ C = \left[ {\left( {\begin{array}{*{20}{c}}
{{x_1}{x_1}^T}& \ldots &{{x_1}A{x_P}^T}\\
 \vdots & \ddots & \vdots \\
{{x_P}A{x_1}^T}& \cdots &{{x_P}{x_P}^T}
\end{array}} \right)} \right] \end{equation}

\begin{equation} \ D = \left[ {\left( {\begin{array}{*{20}{c}}
{{x_1}{x_1}^T}& \ldots &0\\
 \vdots & \ddots & \vdots \\
0& \cdots &{{x_P}{x_P}^T}
\end{array}} \right)} \right] \end{equation}
\begin{equation}
\rho  = \left( {\begin{array}{*{20}{c}}
{{\rho _1}}& \ldots &0\\
 \vdots & \ddots & \vdots \\
0& \cdots &{{\rho _Q}}
\end{array}} \right)
 \end{equation}
\begin{equation} \omega  = [{\omega ^T}_1,{\omega ^T}_2, \cdots {\omega ^T}_P]^T \end{equation}
Based on the definition of $\mathop {{C_{{x_k}{x_m}}}}\limits^ \sim  $ in equation (1) , the value of ${\rho _i}$ $(i = 1,2,...,Q)$ in equation (9) plays a critical role in evaluating the relationship between within-class and between-class correlation matrixes. When the value of ${\rho _i}$ is greater than zero, the corresponding projected vector contributes positively to the discriminative power in classification while the projected vector corresponding to the non-positive values of ${\rho _i}$ would result in reducing the discriminative power in classification. Clearly, the solution obtained is the eigenvector associated to the largest eigenvalues in equation (6). \\\indent It is known that the time taken greatly depends on the computational process of eigenvalue-decomposition. When the rank of eigen-matrix is very high, the computation of eigenvalues and eigenvectors will be time-consuming. It is a big challenge to satisfy the requirement of online behavioral analysis. From studying the properties of within-class correlation matrix $C_w$ and between-class correlation matrix $C_b$, an important characteristic of the proposed model is discovered: the number of projected dimension \emph{d} corresponding to the optimal recognition accuracy is smaller than or equal to the number of classes \textit{c}:
\begin{equation} \ d \le c \end{equation}
The derivation of (11) is given in Appendix B. Therefore, we only need to calculate the first \emph{c} projected dimensions of the discriminative model to obtain the discriminatory representations, eliminating the need of computing the complete transformation process. Specifically, if the dimension of features space in fusion equals to \emph{M}, the computational complexity of the proposed method is in the order of \emph{O}(\emph{M}*\emph{c}), instead of \emph{O}(\emph{M}*\emph{M}) as the other transformation based methods would require, to find the optimal recognition accuracy. Thus, inequality (11) is particularly significant when \emph{c} is small compared with the dimension of feature space such as emotion recognition, digit recognition, English character recognition and many others, where \emph{c} ranges from a handful to a couple of dozens, but the dimension of feature space could be of hundreds or even thousands. \\\indent  Now, we will graphically verify the effectiveness of (11) for selecting optimal projection in information fusion. In general, the solutions to a large number of multi-modal information fusion methods are obtained by utilizing the algorithm of matrix transformation. Some examples are PCA, CCA, Cross-Modal Factor Analysis (CFA), Entropy Component analysis (ECA) and their kernel versions. The solutions to matrix transformation are usually the eigenvector associated with the eigenvalue in a form similar to that of equation (6):
\begin{equation}
\frac{1}{{P - 1}}inv(D)*(C - D)\omega  = \rho \omega
\end{equation}
where \textit{inv()} refers to the inverse transform of a matrix. However, unless the covariance matrices \textit{D} have full rank, the block matrix in equation (12) is singular. An approach [14] to dealing with singular covariance matrices and to controlling complexity is to add a multiple of the identity matrix $\lambda {\rm I}(\lambda  > 0)$ to \textit{D}. Thus, the generalized form of equation (12) can be written as:
\begin{equation}
\frac{1}{{P - 1}}inv({D^ + })*(C - D)\omega  = \eta \omega
\end{equation}
where
\begin{equation}
\begin{array}{l}
 {D^ + } = \left\{ {\begin{array}{*{20}{c}}
   {D\qquad \quad             {\rm{when}}\quad D\quad {\rm{is}}\quad {\rm{a}}\quad {\rm{full}}\quad {\rm{rank}}\quad {\rm{matrix}}}  \\
   {D + \lambda {\rm{I}}\quad {\rm{when}}\quad D\quad {\rm{is}}\quad {\rm{a}}\quad {\rm{singular}}\quad {\rm{matrix}}}  \\
\end{array}} \right. \\
 \eta  = \left( {\begin{array}{*{20}{c}}
   {{\eta _1}} &  \ldots  & 0  \\
    \vdots  &  \ddots  &  \vdots   \\
   0 &  \cdots  & {{\eta _Q}}  \\
\end{array}} \right) \\
 \end{array}
\end{equation}
In equation (13), $\eta_i$  is the criterion to seek the projected vectors for feature extraction. Hence, the value of $\eta_i$ is the key parameter to the effect of selecting features. A larger $\eta_i$ corresponds to the more discriminative feature, while a smaller $\eta_i$ corresponds to the less discriminative feature. \\\indent Thus, the optimal dimension of multi-modal information fusion results can be obtained by graphically plotting $J(\eta_q)$ vs $i$ with $J(\eta_q)$ defined by
\begin{equation}
J(\eta_q) = \sum\limits_{i = 1}^q {{\eta _i}}
\end{equation}
where $ q = 1,2,...,Q$ and ${{\eta _i}}$ is the \textit{i}th eigenvalue of equation (13).
\section{Emotion State Identification}
As a key behavior of humans, emotion plays a central role in our daily social interactions and activities. It reflects an individual's state of mind in response to the internal and external stimuli. Web recognition of human emotion has become an increasingly important research topic for accomplishing the goal of building a more natural and friendly communication interface between humans and the web. \\\indent Since visual and audio information are considered two major indicators of human affective state, and thus play a leading role in emotion recognition, substantial studies have been conducted in human emotion state identification in the past few decades. Facial Action Coding System (FACS) [15], Aligned Cluster Analysis (ACA) [16] and dimensional emotion system [17] are the three major models to address this problem. Although bimodal analysis has gained momentum in recent years, the majority of the works focus on speech alone, or facial expression only. However, as shown in [10], some of the emotion recognition tasks are audio dominant, while the others are visual dominant. The complementary relationship of these two modalities should be further explored to further improve the performance. A wide investigation on the dimensions of emotions reveals that at least six emotions are universal. The set of six principal emotions proposed by Ekman [15] is: happiness(HA), sadness(SA), anger(AN), fear(FE), surprise(SU), and disgust(DI), which are the focus of study in this paper. \\\indent State-of-the-art in multi-modal emotion state identification can be found in [8] and [9] to identify the intrinsic relationship among different modalities respectively. However, selecting the discriminatory representation in the fused space for effective recognition remains a challenging open problem. In what follows, we examine the performance of the proposed model in emotion state identification from audiovisual signals.
\subsection{Audio Feature}
For emotional audio, a good reference model is the human hearing system. Currently, Prosodic, MFCC and Formant Frequency (FF) are widely used in audio emotion recognition [18-19]. As our goal is to simulate human perception of emotion, and identify quality features that can convey the underlying emotions in speech regardless of the language, identity, and context, we investigate the use of all these three types of features which are summarized as follows:\\
1)  25-dimensional Prosodic features used in [10].\\
2)  65-dimensional MFCC features: the mean, median, standard deviation, max, and range of the first 13 MFCC coefficients.\\
3)  15-dimensional Formant frequency features: the mean, median, standard deviation, max and min of the first three Formant Frequencies.\\
\subsection{Visual Feature}
Since Gabor wavelet features have been shown to be effective to represent human facial space[12], in this paper, the algorithm proposed in [13] is used to construct the Gabor filter bank which consists of
filters in 4 scales and 6 orientations. To reduce computational complexity, we calculate the mean, standard deviation and median of the magnitude of the transform coefficients of each filter as the features, including\\
1) 24-dimensional Gabor transformation feature with the mean of the transform coefficients of each filter used in [13].\\
2) 24-dimensional Gabor transformation feature with the standard deviation of the transform coefficients of each filter used in [13].\\
3) 24-dimensional Gabor transformation feature with the median of the transform coefficients of each filter used in [13].\\\indent
Note, in all experiments, the features are first transformed using the proposed model. Subsequently, the newly generated features, which represent the multi-modal information among different patterns, are concatenated into projected vectors for classification with the algorithm of nearest neighbour. In order to demonstrate the effectiveness of the proposed method, we also implemented the serial fusion, CCA, MCCA, DCCA for comparison. A general block diagram of the proposed system is illustrated in Fig. 1, in which the + with circle means fusing different features together. Note, since Ekman's six basic emotional states are used in the work, \emph{c} equals to 6 and the dimension of features (\emph{M}=177) is equal to dimension of audio features (105) plus dimension of visual features (72). The ratio of \emph{O}(\emph{M}*\emph{c}) to \emph{O}(\emph{M}*\emph{M}) is about 1:30, and application of the proposed method indeed significantly reduces the computational complexity compared with the other transformation based methods for the problem on hand. To further show the efficiency of the proposed method, we also investigate the actual running time of the proposed method and that of the MCCA. All experiments are performed on a PC with windows 7 operation system, Intel i7-3.07GHz CPU \& 10 G RAM and are always coded in MATLAB language. The running time of the proposed method is 129.43s while that of MCCA is 11043s on RML Database. The ratio of computational times is 129.43:11043 = 1: 85.3. For eNT Database, the running time of the proposed method is 224.3s while that of MCCA is 15048s. The ratio of computational time is 224.3:15048 = 1: 67 . Therefore, the actual time saving by the proposed method on the two datasets clearly demonstrated the method¡¯s efficiency again.
\begin{figure*}[t]
\centering
\includegraphics[height=3.4in,width=6.0in]{ssystem-eps-converted-to.pdf}\\ Fig.1 The block diagram of the proposed system\\
\end{figure*}
\section{Experimental results and analysis}
To evaluate the effectiveness of the proposed model and criterion $J(\eta_q)$, we conduct experiments on Ryerson Multimedia Lab(RML) and eNTERFACE(eNT) audiovisual databases[11], respectively.\\\indent For audiovisual fusion based emotion state identification, visual features are extracted from the key frame image of videos, where the highest speech amplitude is found. The planar envelope approximation method in the HSV color space is used for face detection. To reduce the high memory requirement of the proposed model, 288 video clips of six basic emotions are selected for capturing the change of audio and visual information with respect to time simultaneously from RML database. Among them, 192 clips are chosen for training set and 96 for evaluation. For eNTERFACE database, 360 clips are chosen for training set and 96 for evaluation. As a benchmark, the performances of using prosodic, MFCC, formant frequency, mean, standard deviation and median features in emotion recognition are first evaluated, which are shown as TABLE 1.
\begin{table}[h]
\footnotesize
\renewcommand{\arraystretch}{1.5}
\caption{\normalsize{Results of Emotion Recognition with Single Feature}}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{10pt}
\centering
\tabcolsep 0.073in
\begin{tabular}{cc}
\hline
Single Feature & Recognition Accuracy\\
\hline
Prosodic(RML) &53.13\%\\
MFCC(RML) &47.92\%\\
Formant Frequency(RML) &29.17\%\\
Prosodic(eNT) &55.21\%\\
MFCC(eNT) &39.58\%\\
Formant Frequency(eNT) &31.25\%\\
Mean(RML) &60.42\%\\
Standard Deviation(RML) &67.71\%\\
Median(RML) &57.29\%\\
Mean(eNT) &75.00\%\\
Standard Deviation(eNT) &80.21\%\\
Median(eNT) &72.92\%\\
\hline
\end{tabular}
\end{table}\\
From TABLE 1, it shows that visual-based features achieve better recognition accuracy than audio-based. Among them, the prosodic features in audio and standard deviation of Gabor Transform coefficients in visual images could result in better performances in emotion recognition compared with other features. Therefore, in the following experiments, we will use prosodic and standard deviation for the methods of CCA and DCCA in audiovisual-based fusion. In addition, the results of serial fusion on all the six audiovisual features are investigated, and the overall recognition accuracy is 30.28\% for RML database and 35.42\% for eNTERFACE database. The performances by the methods of CCA(yellow line), MCCA(red line), DCCA(blue line), audio-based multi-feature discriminative model(magenta line), visual-based multi-feature discriminative model(cyan line) and audiovisual-based discriminative model(green line) are shown in Fig. 2. The calculation of $J(\eta_q)$ with the proposed model for audiovisual-based emotion state identification is shown as Fig.3.
\begin{figure*}[t]
\centering
\includegraphics[height=3.5in,width=6.5in]{av1-eps-converted-to.pdf}\\ Fig.2(a) Discriminative Model for Audiovisual emotion state identification experimental results with different methods on RML Database\\
\end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[height=3.5in,width=6.5in]{av2-eps-converted-to.pdf}\\ Fig.2(b) Discriminative Model for Audiovisual emotion state identification experimental results with different methods on eNTERFACE Database\\
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[height=3.5in,width=6.5in]{RML-final-eps-converted-to.pdf}\\ Fig.3(a) The calculation of $J(\eta_q)$ with the discriminative model for audiovisual-based emotion state identification on RML Database\\
\end{figure*}
\begin{figure*}[t]
\centering
\includegraphics[height=3.5in,width=6.5in]{ENT-final-eps-converted-to.pdf}\\ Fig.3(b) The calculation of $J(\eta_q)$ with the discriminative model for audiovisual-based emotion state identification on eNTERFACE Database\\
\end{figure*}
From Fig.2, clearly, the discrimination power of the proposed model provides a more effective modeling of the relationship between multi-modal audiovisual information fusion. The fusion of multiple audio and visual information indeed enhances the performance of emotion state identification, achieving much better results than the methods compared in all cases. Since visual-based features achieves better results than audio-based features as shown in TABLE 1, visual-based discriminative model(cyan line) arrives better recognition accuracy than the methods of audio-based discriminative model(magenta line) and DCCA(blue line), which only fuses prosodic and standard deviation features. Moreover, an important finding of the researches is that, the exact location of optimal recognition performance occurs when the number of projected dimension is smaller than or equals to the number of classes \textit{c} as shown in TABLE 2. Note that, when the number of projected dimension is more than number of classes \textit{c}, the non-positive values of ${\rho _i}$ would result in reducing the discriminative power and recognition accuracy in classification at the same time. Therefore, the significance here is that, we only need to calculate the first \textit{c} projected dimensions of the discriminative model to obtain the discriminatory representations, eliminating the need of computing the complete transformation processes associated with most of the other methods. This discovery substantially reduces the computational complexity to satisfy the requirement of online processing.
\begin{table}[h]
\scriptsize
\renewcommand{\arraystretch}{1.5}
\caption{\normalsize{The relation between optimal recognition accuracy and projected dimension by different methods}}
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{10pt}
\centering
\tabcolsep 0.01in
\begin{tabular}{ccc}
\hline
Method & Optimal Accuracy & Dimension(Number)\\
\hline
Discriminative Model(RML) &85.42\% & 6\\
Visual-based Discriminative Model(RML) &71.88\% & 6\\
Audio-based Discriminative Model(RML) &69.79\% & 5\\
DCCA(RML) &69.79\% & 6\\
MCCA(RML) &78.13\% & 14\\
CCA(RML) &64.58\% & 34\\
Discriminative Model(eNT) &88.54\% & 5\\
Visual-based Discriminative Model(eNT) &86.45\% & 6\\
Audio-based Discriminative Model(eNT) &75.00\% & 4\\
DCCA(eNT) &77.08\% & 6\\
MCCA(eNT) &78.13\% & 23\\
CCA(eNT) &67.17\% & 26\\
\hline
\end{tabular}
\end{table}\\
 Fig. 3 graphically illustrates the relationship between optimal projected dimensions and the recognition performance using the proposed criterion. In the figure, criterion $J(\eta_q)$ reaches the maximum when the projected dimension is 6 for RML database which is equal to the number of classes (\textit{c}=6). Similarly, the dimension of 5 is observed for the eNTERFACE database. The graphical presentation again confirms nicely with the mathematical analysis presented in Section 3.


% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex,
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.
% However, the Computer Society has been known to put floats at the bottom.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a
% line do not exceed the text width or a line break will occur.
%
%\begin{figure*}[!t]
%\centering
%\subfloat[Case I]{\includegraphics[width=2.5in]{box}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{box}%
%\label{fig_second_case}}
%\caption{Simulation results for the network.}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

\section{Conclusions}
In this paper, we proposed a discriminative model for online behavioral analysis and applied to emotion state identification. For the proposed model, not only the correlation from different channels is taken as the metric of the similarity between the variables, but also the within-class similarity and the between-class dissimilarity are taken into consideration for extracting the discriminatory representation. Experiments show that it outperforms serial fusion and methods based on similar principles such as CCA, MCCA and DCCA. Although we focus on the emotion state identification in this paper, the generic nature of the method enable it to be applied to other behavioral such as actions and gestures, or the combination of several behaviors. The fact that the best performance by the proposed discriminative representation can be accurately predicted offers an intuitive way of finding the optimal or near-optimal dimension of the features in the projected space in transformation based information fusion.

% if have a single appendix:
\appendix[ A. Proof of Equation (3) and (4)]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed
Let
\begin{small}
\begin{equation}
{x_i} = [{x_{i1}}^{(1)},{x_{i2}}^{(1)} \cdots {x_{i{n_1}}}^{(1)}, \cdots {x_{i1}}^{(c)},{x_{i2}}^{(c)} \cdots {x_{i{n_c}}}^{(c)}] \in {R^{{m_i} \times n}}
\end{equation}
\end{small}
\begin{equation}
{e_{{n_{il}}}} = [\underbrace {0,0, \cdots 0,}_{\sum\limits_{u = 1}^{l - 1} {{n_{iu}}} }\underbrace {1,1, \cdots 1}_{{n_{il}}}\underbrace {0,0, \cdots 0}_{n - \sum\limits_{u = 1}^l {{n_{iu}}} }]
\end{equation}
\begin{equation}
{\bf{1}} = {[1,1, \cdots 1]^T} \in {R^n}
\end{equation}
where \textit{i} is the number sequence of the random behavioral features, ${x_{ij}}^{(m)}$ denotes the \textit{j}th sample in the \textit{m}th class, and ${n_{il}}$ is the number of samples in the \textit{l}th class of the set $x_i$.
\begin{equation}
\sum\limits_{l = 1}^c {{n_{il}}}  = n
\end{equation}
where \textit{c} is the total number of classes, and \textit{n} is the total number of samples. \\ Note that, as the random features satisfy the property of zero-mean, it can be shown that:
\begin{equation}
{x_i}  {\bf{1}} = 0
\end{equation}
The within-class correlation matrix $C_w$ and between-class matrix $C_b$  can be written as:
\begin{equation} \begin{array}{l}
{C_w} = \sum\limits_{l = 1}^c {\sum\limits_{h = 1}^{{n_{kl}}} {\sum\limits_{g = 1}^{{n_{ml}}} {{x_{kh}}^{(l)}{x_{mg}}^{(l)T}} } } \\
{\rm{    }} = \sum\limits_{l = 1}^c {({x_k}{e_{{n_{kl}}}}){{({x_m}{e_{{n_{ml}}}})}^T}} \\
{\rm{    }} = {x_k}A{x_m}^T
\end{array} \end{equation}
\begin{equation} \begin{array}{l}
{C_b} = \sum\limits_{l = 1}^c {\sum\limits_{\scriptstyle q = 1\hfill\atop
\scriptstyle l \ne q\hfill}^c {\sum\limits_{h = 1}^{{n_{kl}}} {\sum\limits_{g = 1}^{{n_{mq}}} {{x_{kh}}^{(l)}{x_{mg}}^{(q)T}} } } } \\
{\rm{    }} = \sum\limits_{l = 1}^c {\sum\limits_{q = 1}^c {\sum\limits_{h = 1}^{{n_{kl}}} {\sum\limits_{g = 1}^{{n_{mq}}} {{x_{kh}}^{(l)}{x_{mg}}^{(q)T} - } } } } \sum\limits_{l = 1}^c {\sum\limits_{h = 1}^{{n_{kl}}} {\sum\limits_{g = 1}^{{n_{ml}}} {{x_{kh}}^{(l)}{x_{mg}}^{(l)T}} } } \\
{\rm{    }} = ({x_k}{\bf{1}}){({x_m}{\bf{1}})^T} - {x_k}A{x_m}^T\\
{\rm{    }} =  - {x_k}A{x_m}^T
\end{array} \end{equation}


\appendix[ B. Proof of Equation (11)]
From equation (5), the rank of matrix \emph{A} satisfies
\begin{equation} \ rank(A) \le c \end{equation}
Then, equation (23) leads to:
\begin{equation} \ rank({x_i}A{x_j}^T) \le \min ({r_i},{r_A},{r_j}) \end{equation}
where ${r_i},{\rm{  }}{r_A},{\rm{  }}{r_j}$ are the ranks of matrixes ${x_i},A,{x_j}$ (
$ i,j \in [1,2,3,...,P] $), respectively.\\
Due to the fact that $rank(A) \le c$, equation (24) satisfies
\begin{equation} \ rank({x_i}A{x_j}^T) \le \min ({r_i},{c},{r_j}) \end{equation}
when $c$ is less than ${r_i}$ and ${r_j}$, equation (25) is written as
\begin{equation} \ rank({x_i}A{x_j}^T) \le {c} \end{equation}
Otherwise, equation (25) satisfies
\begin{equation} \ rank({x_i}A{x_j}^T) \le min({r_i,r_j}) <\ {c} \end{equation}

It can be shown that the solution to equation (6) is in the form of:
{\small
\begin{equation}
\begin{array}{l}
{x_1}A{x_2}^T{\omega _2} + {x_1}A{x_3}^T{\omega _3} +  \cdots + {x_1}A{x_P}^T{\omega _P} = {x_1}{x_1}^T{\omega _1}\\
{x_2}A{x_1}^T{\omega _1} + {x_2}A{x_3}^T{\omega _3} +  \cdots + {x_2}A{x_P}^T{\omega _P} = {x_2}{x_2}^T{\omega _2}\\
                \centerline \vdots \\
{x_P}A{x_1}^T{\omega _1} + {x_P}A{x_2}^T{\omega _2} +  \cdots + {x_P}A{x_{P - 1}}^T{\omega _{P - 1}} = {x_P}{x_P}^T{\omega _P}
\end{array}
\end{equation}}
Since $A$ is a diagonal matrix and the diagonal element is ${H_{{n_{i{1}}} \times {n_{i1}}}}$, equation (28) is rewritten as following form:
\begin{equation}
\begin{array}{l}
{x_1}A({x_2}^T{\omega _2} + {x_3}^T{\omega _3} +  \cdots  + {x_P}^T{\omega _P}) = {x_1}{x_1}^T{\omega _1}\\
{x_2}A({x_1}^T{\omega _1} + {x_3}^T{\omega _3} +  \cdots  + {x_P}^T{\omega _P}) = {x_2}{x_2}^T{\omega _2}\\
                \centerline \vdots \\
{x_P}A({x_1}^T{\omega _1} + {x_2}^T{\omega _2} +  \cdots  + {x_{P - 1}}^T{\omega _{P - 1}}) = {x_P}{x_P}^T{\omega _P}
\end{array}
\end{equation}
The rank of $\omega$ satisfies the following equation:
\begin{equation} \ rank(\omega) \le c \end{equation}
Since when the value of ${\rho _i}$ greater than zero, the corresponding projected vector contributes positively to the discriminative power in classification. Therefore, when $\omega ^T$ is in the form of equation (31), it will achieve the optimal recognition accuracy.
\begin{equation}
{\omega ^T} = {R^{d \times Q}}(d \le c)
\end{equation}
\begin{equation}
{X} = {R^{Q \times n}}
\end{equation}
Thus, the projected vector satisfies the following relation:
\begin{equation}
Y = {\omega ^T}X = {\omega ^T}\left[ \begin{array}{l}
{x_1}\\
{x_2}\\
 \vdots \\
{x_P}
\end{array} \right] = {R^{d \times n}}  (d \le c)
\end{equation}

Thus, expressions in (33) lead to the proof of (11).\\\indent

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here.


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi

This work is supported by the National Natural Science
Foundation of China (NSFC, No.61071211), the State Key
Program of NSFC (No. 61331201), the Key International
Collaboration Program of NSFC (No. 61210005) and the
Discovery Grant of Natural Science and Engineering Council
of Canada (No. 238813/2010).


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
M. H. Giard and F. Peronnet. "Auditory-visual integration during multimodal object recognition in humans: a behavioral and electrophysiological study." Journal of cognitive neuroscience, vol. 11, no. 5, pp. 473-490, 1999.
\bibitem{IEEEhowto:kopka}
J. Klemen and C. D. Chambers. "Current perspectives and methods in studying neural mechanisms of multisensory interactions." Neuroscience and Biobehavioral Reviews, vol. 36, no. 1, pp. 111-133, 2012.
\bibitem{IEEEhowto:kopka}
M. R. Mercier, J. J. Foxe, I. C. Fiebelkorn, J. S. Butler, T. H. Schwartz and S. Molholm. "Auditory-driven phase reset in visual cortex: human electrocorticography reveals mechanisms of early multisensory integration." Neuroimage, vol. 79, pp. 19-29, 2013.
\bibitem{IEEEhowto:kopka}
J. Yang, J.Y. Yang, D. Zhang, J.F. Lu, "Feature fusion: parallel strategy
vs. serial strategy," Pattern Recognition, vol. 36, no. 6, pp. 1369-1381, 2003.
\bibitem{IEEEhowto:kopka}
Q. Sun, S. Zeng, Y. Liu, P. Heng, and D. Xia, "A new method of feature
fusion and its application in image recognition," Pattern Recognition., vol.
36, no. 12, pp. 2437-2448, Dec. 2005.
\bibitem{IEEEhowto:kopka}
T. K. Kim, J. Kittler and R. Cipolla, "Discriminative learning and
recognition of image set classes using canonical correlations," IEEE Trans
on Pattern Analysis and Machine Intelligence, Vol. 29, No. 6, pp.1005-
1018, 2007.
\bibitem{IEEEhowto:kopka}
A. A. Nielsen, "Multiset Canonical Correlations Analysis and Multispectral,
Truly Multitemporal Remote Sensing Data," IEEE Trans. Image
Processing, vol. 11, no. 3, pp. 293-305, Mar. 2002.
\bibitem{IEEEhowto:kopka}
P. K. Atrey, M. A. Hossain, A. El Saddik, and M. S. Kankanhalli,
"Multimodal fusion for multimedia analysis: A survey," Multimedia Syst.,
vol. 16, no. 6, pp. 345-379, 2010.
\bibitem{IEEEhowto:kopka}
L. Guan, Y. Wang, R. Zhang, Y. Tie, A. Bulzacki, and M. T. Ibrahim,
"Multimodal information fusion for selected multimedia applications,"
Int. J. Multimedia Intell. Sec., vol. 1, no. 1, pp. 5-32, 2010.
\bibitem{IEEEhowto:kopka}
Y. Wang and L. Guan, "Recognizing human emotional state from audiovisual
signals," IEEE Trans. Multimedia, vol. 10, no. 5, pp. 936-946, Oct.
2008.
\bibitem{IEEEhowto:kopka}
Y. Wang, L. Guan and A.N. Venetsanopoulos, "Kernel based fusion
with application to audiovisual emotion recognition," IEEE Trans. on
Multimedia, vol. 14, no. 3, pp. 597-607, Jun 2012.
\bibitem{IEEEhowto:kopka}
M. J. Lyons, J. Budynek, A. Plante, and S. Akamatsu, "Classifying facial
attributes using a 2-D Gabor wavelet representation and discriminant analysis",
in Proc. 4th Int. Conf. Automatic Face and Gesture Recognition,
France, Mar. 2000, pp. 202-207.
\bibitem{IEEEhowto:kopka}
B. S. Manjunath and W. Y. Ma, "Texture features for browsing and
retrieval of image data," IEEE Trans. Pattern Anal. Machine Intell., vol.
18, pp. 837-842, Aug. 1996.
\bibitem{IEEEhowto:kopka}
T. Melzer, M. Reiter, H. Bischof. "Appearance models based on kernel canonical
correlation analysis." Pattern Recognition, vol.36, pp. 1961-1971, 2003.
\bibitem{IEEEhowto:kopka}
P. Ekman, W.V. Friesen, and J.C. Hager. "The Facial Action Coding System." Research
Nexus eBook, Salt Lake City, second edition, 2002.
\bibitem{IEEEhowto:kopka}
F. Zhou, F. D. Torre and J. K. Hodgins. "Hierarchical aligned
cluster analysis for temporal clustering of human motion." IEEE Transactions Pattern
Analysis and Machine Intelligence (PAMI), vol. 35, no. 3, pp.582-596, 2013.
\bibitem{IEEEhowto:kopka}
L. Zelnik-Manor and M. Irani. "Temporal factorization vs. spatial factorization." Computer Vision - ECCV 2004, volume 3022
of Lecture Notes in Computer Science, pages 434-445. Springer Berlin Heidelberg, 2004.
\bibitem{IEEEhowto:kopka}
C. S. Ooi, K. P. Seng, L. M. Ang and L. W. Chew. "A new approach of audio emotion recognition." Expert Systems with Applications, vol. 41, no. 13 pp. 5858-5869, 2014.
\bibitem{IEEEhowto:kopka}
M. Shah, C. Chakrabarti and A. Spanias. "Within and cross-corpus speech emotion recognition using latent topic model-based features." EURASIP Journal on Audio, Speech, and Music Processing, no. 1, pp. 1-17, 2015.


\end{thebibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%author%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{IEEEbiographynophoto}{Lei Gao}
%received the M.S. degree in Communication and Information
%System from Zhengzhou University, China, in 2011. He was currently
%working toward the Ph. D degree. He is currently the participant of National Natural Science Foundation
%of China and Canada Research Chair Program. His research
%interests are in the areas of multimedia signal processing, pattern
%recognition and image processing. He is a student member of IEEE.
%\end{IEEEbiographynophoto}
%\begin{IEEEbiographynophoto}{Lin Qi}
%received the Ph.D. degree in Communication and Information
%System from Beijing Institute of Technology, China, in 2004.
%Since 1990 he has been with Zhengzhou University on various
%appointments including Lecturer at the Department of Electronic Engineering
%(1990-1996), Associate Professor at the Department of Electronic
%Engineering (1996-2001), Associate Professor at the School of
%Information Engineering, (2001-2004), and Full Professor at the
%School of Information Engineering (2004-now). He also serves as
%the Vice Dean of School of Information Engineering, Zhengzhou University, China, and the Director of Henan Key Laboratory of Laser
%and Optical Information Technology, China. He has received 4 grants
%from Chinese governmental agencies, including 2 National Nature
%Science Funding and 2 Natural Science Foundation of Henan province.
%His research interests are in the areas of digital signal processing,
%communication system and multimedia signal processing. He has
%published 70 refereed research papers and 2 books, including 24
%refereed papers in English and other 46 are published on top Chinese
%journals.
%\end{IEEEbiographynophoto}
%\begin{IEEEbiographynophoto}{Ling Guan}
%received the Ph.D. degree in electrical engineering from
%the University of British Columbia, Canada in 1989. He is currently a
%professor and a Tier I Canada Research Chair in the Department of
%Electrical and Computer Engineering at Ryerson University, Toronto,
%Canada. He held visiting positions at British Telecom (1994), Tokyo
%Institute of Technology (1999), Princeton University (2000), National
%ICT Australia (2007), Hong Kong Polytechnic University (2008-2009)
%and Microsoft Research Asia (2002, 2009). He has published extensively
%in multimedia processing and communications, human-centered
%computing, machine learning, and adaptive image and signal processing.
%Dr. Guan is a Fellow the Engineering Institute of Canada and an
%Elected Member of the Canadian Academy of Engineering. He is an
%IEEE Circuits and System Society Distinguished Lecturer (2010-2011)
%and a recipient of 2005 IEEE TRANSACTIONS ON CIRCUITS AND
%SYSTEMS Best Paper Award. He also is a Fellow of the IEEE.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


