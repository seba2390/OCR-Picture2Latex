\section{Conclusions}
\label{sec:conclusions}

The great success of classical machine learning algorithms in tasks such as classification, together with the expectation that quantum computers will allow us to explore algorithms in a larger complexity class than their classical counterparts, makes the exploration of the connections between these ideas a fertile ground for the discovery of novel approaches to automated inference.

In this work we have presented SQUID as a computational framework which allows to explore efficiently the possible advantages of quantum computing for machine learning purposes. This is achieved by embedding the quantum algorithm part in a more general multi-layer architecture that allows to interface classical and quantum networks while enjoying efficient optimization by using the automatic differentiation engine provided by PyTorch.
While there are similar packages (notably Xanadu's PennyLane~\cite{Bergholm2020Pennylane}, IBM's Qiskit~\cite{Qiskit}), they do not offer as much flexibility as SQUID.
For example, PennyLane offers much more complete experience, with many examples and models, as well as ability to run code on quantum computer, and not a simulator.
However, for the same reason, creating a custom model is much easier in SQUID.
Similar argument can be made about Qiskit.
This generalized frameworks provides several advantages over an either purely classical or purely quantum approach: it allows for a seamless dimensionality reduction of the inference problem, a step that would be necessary to explore high dimensional datasets on small quantum devices, while at the same time allowing for automatic tuning of the measurement settings needed to extract information from the quantum state produced by the algorithm.
This latter feature, implemented in SQUID by using a classical network as decoder after the central model as shown in Fig.~\ref{fig:model-arch}, is extremely important in order to reach high precisions.
The use of an explicit decoder at the output of the quantum model allows for a more careful optimization of the trade-off between measuring only vanishingly small fraction of the possible output probabilities on one hand, with the drawback that entanglement can start to be detrimental due to information scrambling (see eg.~\cite{Shen2020,marrero2021entanglement}), and a full measurement of the probability distribution on the computational basis states which will require an exponential number of repetitions.
In this work we used a simple classification problem from the MNIST database to show the effect of this tradeoff for a concrete high-dimensional problem.

Thanks to the generality of the architecture developed in this work, future explorations of algorithms with entanglement but with a classically efficient representation (such as tensor network states with polynomially large bond dimension, see eg.~\cite{Liu_2019,roberts2019tensornetwork}) could be carried out within SQUID with only minimal modifications to the code.
We expect the added flexibility in interchanging classical and quantum components in a global classifier to prove valuable in identifying promising datasets and inference problems where the presence of entanglement and quantum correlations can provide important accuracy gains.
Once these problems have been identified with a simplified approach as the one currently implemented in SQUID, a successive study of the sample complexity along the lines of the derivation presented in Sec.~\ref{sec:complexity} will be needed to assess the practical viability of the algorithm.
Extensions to implement the effect of finite statistics, together with more advanced effects such as models of decoherence for a specific target device, can be added efficiently within SQUID and we plan to explore their impact on classification problems in future work.
