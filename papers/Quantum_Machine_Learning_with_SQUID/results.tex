\section{Example applications}
\label{sec:results}
As an initial application of our framework, we present here results for binary classification on the MNIST database~\cite{MNIST} using digits 3 and 7. This is a standard benchmark for classification algorithms and analysis with a quantum model is made possible by the ability of SQUID to compress the input features into data with the appropriate dimensions.

\begin{figure}[]
	\centering
	 \includegraphics[width=0.45\textwidth]{plots/squid_circ_last.png}
	\caption{ Pictorial representation of the hybrid classifier model used for MNIST classification. Panel $(A)$ shows the complete network including an encoder with $M_0$ units and a decoder with $M_1$ units. Panel $(B)$ shows the implemented classical classifier composed by two units and panel $(C)$ shows a schematic of the quantum models: input parameters coming from the encoder determine the unitary $W$ while the output is obtained upon measurement of the qubits. }
	\label{fig:mnist_model}
\end{figure}

The input feature vectors for this dataset are real vectors of dimension 784 representing a grayscale $28\times 28$ pixel picture.
In this section we will compare results obtained with the architecture displayed in Fig.~\ref{fig:mnist_model} composed by: a single layer encoder with $M_0$ units, a black-box classifier to be specified below and a single layer decoder with $M_1$ units.
The classical black-box classifier used in this section consists of a simple single layer network with $2$ units (panel $(B)$ of Fig.~\ref{fig:mnist_model}) and we take $M_1=2$ for it's output.
In the following subsections we will also consider different implementations of quantum classifiers with the general structure displayed in panel $(C)$ of the same figure.

All of the calculations (classical and quantum) presented in this section were obtained using an Adam optimizer as implemented in PyTorch and using the hyperparameters reported in Tab~\ref{tab:hyppar}. In all case we use $48$ independent optimization runs that were performed in order to estimate the variance in the attained accuracy. In the following we will refer to this ensemble as "Bootstrap runs".

\begin{table}[]
\footnotesize
\centering
\begin{tabular}{l|l|l|l|l}
Batch size & Epochs & Learning rate & Train size & Val. size\\ \hline
16 & 100 & 0.0001 & 9916 & 2480
\end{tabular}
\caption{Hyperparameters used for the results on MNIST. \label{tab:hyppar}}
\end{table}

\begin{figure}[]
	\centering
	 \includegraphics[width=0.45\textwidth]{plots/classical_scaling}
	\caption{ Results for the accuracy achieved on MNIST with the classical model described in the text. Panel (a) shows the accuracy as a function of the number of parameters in the model (green solid circles), the other two sets of points show the location of  the $90\%$ accuracy percentile for both the validation set (red squares) and the training set (blue diamonds) . Panels (b) and (c) show the histogram of achieved accuracies for the smallest ($M_0=3$) and largest ($M_0=60$) models respectively.  }
	\label{fig:cl_accuracy}
\end{figure}

We present the results obtained with the classical network in Fig.~\ref{fig:cl_accuracy} (the full data is available in Tab.~\ref{tab:cl_res} of Appendix~\ref{app:mnist}). We can see from the evolution of the median accuracy (green circles) that the classical network is able to achieve classification accuracies above $99\%$ but the increase in the number of hidden units at the level of the input model doesn't seem to provide a statistically significant improvement on the final accuracy. The displayed error bars are $68\%$ confidence intervals extracted from our finite population sample.

In the main panel, we also show the location of the $90\%$ accuracy percentile, ie. the boundary value for which $10\%$ of bootstrap runs provide a higher accuracy, for both the validation set (red squares) and the training set(blue diamonds).
These results are consistent with the expectation that, as the number of parameters $K_{tot}$  in the model increases, the training set can be described almost exactly by the network while at the same time we see that the distribution of accuracies for the  validation set does not evolve significantly with $K_{tot}$. In order to understand this point better we show in the left panels the estimated histograms for accuracy reached in our set of $48$ bootstrap runs for the smallest classical model (panel (b)) and the largest model (panel (c)). As expected from the results in the main panel, most of the density is in the same location but for the larger models the tails are more important.

\begin{figure}[]
	\centering
	 \includegraphics[width=0.45\textwidth]{plots/classical_training_band}
	\caption{ Example of training of the classical model described in the main text. The left panel shows the increase in classification accuracy for the training set as a function of the number of epochs. The right panel shows the same for the validation set.  All bands are $90\%$ confidence intervals with averages indicated by the solid lines.}
	\label{fig:cl_train}
\end{figure}

Note that the dispersion in the accuracy around the median reported in the main panel of Fig.~\ref{fig:cl_accuracy} is relatively small, of the order of $\approx0.002$. This is caused in large part by the simplicity of the classification problem, as we can see in Fig.~\ref{fig:cl_train} (obtained for a medium sized model with $M_0=40$) the accuracy quickly converges to a narrow interval around the mean for both the training set and the validation set. With more than $~80$ epochs the accuracy for the training data set is able to reach $100\%$.

When the inner model is replaced by a quantum subroutine, as depicted in panel $(C)$ of Fig.~\ref{fig:mnist_model}, the output dimension for a quantum circuit over $N$ qubits is bounded by $M_1\leq2^{N}$. In the following sections we will consider two limiting situation: the maximum possible dimension $M_1=2^{N}$ (indicated as ``full'' below) and the minimum one $M_1=2$ (indicated as ``min'' below) and corresponding to the probability of measuring a single basis state (here we choose $\ket{0}^{\otimes N}$).

\subsection{Separable Quantum Models}

The first class of quantum models we consider here are separable models with a single layer of $U^1$ unitaries and are therefore fundamentally classical in that entanglement plays no role in shaping the output probabilities of the quantum model. The results are shown in Fig.~\ref{fig:qm_sep_accuracy} and the full data is available in Tab.~\ref{tab:qm_res_A} of Appendix~\ref{app:mnist}.

\begin{figure}[]
	\centering
	 \includegraphics[width=0.45\textwidth]{plots/quantum_sep_scaling}
	\caption{ Results for the accuracy achieved on MNIST with the classical model (green points) and the separable quantum models described in the text (red and blue points). Panel (a) shows the accuracy as a function of the number of parameters for the classical model (green solid circles), the full quantum separable models (blue solid circles) and the separable quantum models with a single output variable $M_1=1$ (red solid circles) indicated by ``min''. The inset panel (b) shows the location of the $90\%$ accuracy percentile for the classical (green squares), full separable quantum model (blue squares) and minimal separable quantum model (red squares). Panels (c) and (d) show the histogram of achieved accuracies for the 6 qubit models with either the full number of possible output variables ($M_1=64$, top panel) and the single output (bottom panel).  }
	\label{fig:qm_sep_accuracy}
\end{figure}

In this case, at least for the models with $M_1=2^{N}$, we can see a mild increase of the final accuracy as a function of the number of parameters/qubits in the model, the largest model is however outperformed by classical networks with a smaller size (see the blue circles in Fig.~\ref{fig:qm_sep_accuracy}(a)). The models with a latent space corresponding to the restricted output for the quantum layer (shown as red circles in Fig.~\ref{fig:qm_sep_accuracy}(a)) show instead a deterioration as we increase the number of qubits/parameters. This effect is especially clear looking at the histograms of achieved accuracy in the 48 bootstrap runs displayed in the right panels of Fig.~\ref{fig:qm_sep_accuracy} for the largest model with $N=6$ qubits: employing a large output vector from the quantum layer produces results with better than $99\%$ for more than half of the runs while restricting the output to a single probability prevents most runs from reaching this threshold. Strikingly, this is true even for the training set (not shown) where only a single bootstrap run achieved an accuracy above $99\%$. This is a first clear sign of the importance to supplement the quantum classifier with a rich decoder at the possible expense of a larger sample complexity.

\subsection{Quantum models with entanglement}
\label{sec:qm_res_withent}

We now turn to consider more general quantum models that are capable of creating entanglement in the quantum register through the use of the two-qubit unitary $U^2$ defined in Sec.~\ref{sec:qmodels}. The resulting median accuracy shown in panel (a) of Fig.~\ref{fig:qm_accuracy_bounds} show a similar trend to the simpler separable models above: the accuracy of the quantum model never exceeds the classical accuracy and there doesn't seem to be any measurable advantage in increasing the number of parameters. Interestingly, for the models with restricted output size (red circles denoted $QM - min$ in panel (a) of Fig.~\ref{fig:qm_accuracy_bounds}), we see that the optimization procedure is struggling to find a good set of parameters for the larger models and the accuracy decreases almost monotonically with size. It is possible that better results could be obtained using directly the accuracy as cost function instead of the cross-entropy. In order to clarify that the effect we are seeing is not coming from over-fitting of the training set, but really from difficulties in exploring efficiently the energy surface, we show on the left panels the evolution of the $90\%$ accuracy percentile as a function of the number of parameters for both the validation and training set (panels (b) and (c) respectively) for the 3 networks considered here: the classical feed-forward network considered before (green squares) and the quantum models with entanglement either with the full output (red squares) or the restricted output model (blue squares). As can be clearly seen in panel (c) the optimizer is not able to find a good parameter set for large models and the accuracy in training decreases.

These results highlight the importance of supplementing the quantum classifier with a non trivial output decoder in order to achieve a good efficiency, a possibility that is available only if we choose to measure more than a single qubit from the quantum device.

\begin{figure}
	\centering
	 \includegraphics[width=0.45\textwidth]{plots/quantum_scaling_bounds}
	\caption{ Results for the accuracy achieved on MNIST with the classical model (green points) and the quantum models with entanglement described in the text (red and blue points). Panel (a) shows the accuracy as a function of the number of parameters for the classical model (green solid circles), the full quantum models (blue solid circles) and the quantum models with a single output variable $M_1=1$ (red solid circles) indicated by ``min''. The left panels show the location of the $90\%$ accuracy percentile for the classical (green squares), full quantum (blue squares) and minimal quantum model (red squares) in either the validation set (panel (b)) or the training set (panel (c)).	\label{fig:qm_accuracy_bounds}}
\end{figure}
