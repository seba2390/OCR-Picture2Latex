\section{Dataflow Optimization}\label{section-relation-2}

Relations $\sqsubseteq$ and $\preceq$ are defined over substitutions that do not necessarily need to be \textit{injective}. Indeed, a single term occurring multiple times in one of the goals can potentially be generalized by two (or more) different variables. Therefore, some most specific generalizations may contain more different variables than others depending on the underlying variabilization process. 
Among two common generalizations of the same pair of goals, the common generalization that has more variables than the other can be considered \textit{less specific} as some information -- namely the fact that two or more values, possibly in different atoms, are equal -- has been abstracted by introducing different variables. In what follows, we will call the search of a common generalization with as few different variables as possible \textit{dataflow optimization}. The following example illustrates the concept over the finite domain from~\cite{clpbfd}. 

\begin{example}\label{ex:dataflow-bools}
	Consider the domain of Booleans $\mathbb{B} = \{true, false\}$ as well as the following goals: $G_1 = \{=\!\!(X, or(Y, Z)), =\!\!(V, and(Y, Z))\}$ and $G_2 = \{=\!\!(B, or(C, D)), =\!\!(A, and(C, D)), =\!\!(E, and(F, G))\}$. 
	Note that in $G_1$ the \textit{or} and \textit{and} operations are evaluated on the same values, represented by the multiple occurrences of the variables $Y$ and $Z$. In $G_2$ the \textit{or} and the \textit{and} operation from the second atom exhibit this very same behavior (represented by the variables $C$ and $D$), whereas the third atom represent an \textit{and} operation on different values. 
	Computing a $\preceq$-msg (and in this example, a $\sqsubseteq$-msg) for $G_1$ and $G_2$ can lead to two different generalizations, namely
	\[
	\begin{array}{lll}
	G & = & \{=\!\!(\Phi(X,B), or(\Phi(Y,C), \Phi(Z,D))), =\!\!(\Phi(V, E), and(\Phi(Y, F), \Phi(Z,G)))\}\\ %\\ = \{=\!\!(V_1, or(V_2, V_3)), =\!\!(V_5, and(V_6, V_7))\}
	G' & = & \{=\!\!(\Phi(X,B), or(\Phi(Y,C), \Phi(Z,D))), =\!\!(\Phi(V, A), and(\Phi(Y, C), \Phi(Z,D)))\}%= \{=\!\!(V_1, or(V_2, V_3)), =\!\!(V_4, and(V_2, V_3))\}\\
	\end{array}
	\] 
	Clearly, both generalizations are correct msg's, but the fact that all the variables in $G$ only occur once merely denotes that there exist six variables that together can make $G$ true. The repetition of $Y$ and $Z$ in $G_1$ as well as their connection with $C$ and $D$ is a lost information, abstracted by the anti-unification process. On the other hand, $G'$ by harboring less different variables introduces less variable abstraction, effectively depicting some dataflow logic that is common to $G_1$ and $G_2$, through the occurrence of $\Phi(Y,C)$ and $\Phi(Z,D)$ in both its atoms. On that level, $G'$ can be considered less general than $G$. 
\end{example}

Dataflow optimization thus formally boils down to finding, among a group of common generalizations for two goals $G_1$ and $G_2$, a goal $G$ such that $|\vars(G)|$ is minimal. In Example~\ref{ex:dataflow-bools}, we were interested in finding, among all possible msg's of $G_1$ and $G_2$, one that harbors a minimal number of variables; it makes sense, since abstracting one Boolean value with two different variables can be too liberal, depending on the applications. In that case of dataflow optimization, where the target goal must be a msg (i.e. when both structure and dataflow must be optimized), the dataflow problem is NP-complete. The same is true for lcg's. In order to show this formally, we consider a formulation in terms of decision problems.

%is a straightforward concern given that capturing as much common dataflow structure amongst $G_1$ and $G_2$ can further improve a msg's specificity and thus quality. 
\begin{theorem}\label{thm-dataflow-np-complete}
	Let MSG-MIN (resp. LCG-MIN) denote the following decision problem: "Given goals $G_1$, $G_2$ and a constant $p\in \mathbb{N}_0$, does there exist a $\leqslant$-msg (resp. $\leqslant$-lcg) of $G_1$ and $G_2$ that has less than $p$ different variables?". MSG-MIN and LCG-MIN are NP-complete.
\end{theorem}
%\begin{proof}
%	First, let us consider MSG-MIN. It clearly belongs to NP. Indeed, given an arbitrary generalization $G$, we can verify in polynomial time whether it is a most specific generalization. The procedure is as follows. We can compute at least one $\leqslant$-msg, say $G'$, in polynomial time (see Theorem~\ref{thm-preceq-lcg} for relation $\preceq$ and Theorem~\ref{thm-sqsubseteq-msg} for relation $\sqsubseteq$). It suffices then to compare the $\tau$-value of $G'$ with that of $G$ in order to decide whether $G$ is a msg. Next, verifying whether the number of variables in $G$ is bounded by a constant is obviously achieved in polynomial time as well.
%	
%	In order to prove NP-hardness, we will construct a reduction from the well-known set cover problem (known to be NP-complete~\cite{karp}) to MSG-MIN. The set cover problem in its decision-problem version (denoted SCP), can be formulated as follows. Given a constant $p \in \mathbb{N}_0$, a universe $U$ of values and a collection $S$ composed of $n$ sets $\{S_1, \dots, S_n\}$ that cover $U$, i.e. $U = \underset{i=1}{\overset{n}{\cup}}S_i$, the problem is to decide whether there exists $p$ subsets from $S$ that still cover $U$.
%	
%	We can transform an arbitrary instance of SCP into MSG-MIN as follows. Let us consider without loss of generality a universe $U$ where the elements are lowercase strings and $p \in \mathbb{N}_0$ a constant. Given a collection of sets $S=\{S_1, \dots, S_n\}$ we construct an instance of MSG-MIN as follows. In our construction we use $n+1$ different variables, namely $V$ and $(W_i)_{i\in1..n}$. We use $x_j$ to denote some element of $U$; these elements being strings, we can easily use them as predicate names. The construction of goals $G_1$ and $G_2$ proceeds then as follows:
%	
%	\begin{algorithmic}
%		\State $G_1 = \{\}$ 
%		\State $G_2 = \{\}$ 
%		\For {each ($S_i \in S$)}
%		\For {each ($x_j \in S_i$)}
%		\State $G_1 \gets G_1\cup \{x_j(V)\}$				
%		\State $G_2 \gets G_2\cup \{x_j(W_i)\}$
%		\EndFor
%		\EndFor
%	\end{algorithmic}
%	Note that all the atoms in $G_1$ have the same argument (namely the variable $V$) and there are as many atoms in $G_1$ as there are distinct elements in $S$. In $G_2$, however, there is an atom of the form $x_j(W_i)$ for each element $x_j$ occurring in $S_i$.
%	
%	The construction is such that any $\leqslant$-msg of $G_1$ and $G_2$ will be a version of $G_1$ where each occurrence of a variable $V$ is replaced by $\Phi(V, W_k)$ for some $W_k\in\vars(G_2)$ (where $\Phi$ is a variabilization function). Now, introducing such a variable $\Phi(V, W_k)$ in the generalization will allow to reuse the same variable for all the atoms $x_j(V)$ in $G_1$ that have a corresponding $x_j(W_k)$ in $G_2$. In other words, choosing to have variable $\Phi(V,W_k)$ in the $\leqslant$-msg is the same as selecting the subset $S_k$ to be part of the solution of the set cover problem. Consequently, using this transformation MSG-MIN can be used to decide SCP. Since the transformation can clearly be done in polynomial time, and since SCP is known to be NP-complete, we conclude that MSG-MIN is NP-complete as well.
%	
%	Now let us prove the result for LCG-MIN. We know that a $\leqslant$-lcg can be computed in polynomial time, so that a positive instance of LCG-MIN can be verified just like it can be for MSG-MIN. Moreover, the absence of non-variable terms in the transformation from SCP to MSG-MIN above allows us to reuse said transformation as-is to prove that LCG-MIN is NP-hard. Indeed, since the obtained anti-unification problem doesn't harbor terms other than variables, it is both an instance of MSG-MIN and LCG-MIN. LCG-MIN is therefore also NP-complete.
%\end{proof}

%Note that Theorem~\ref{thm-dataflow-np-complete} effectively concerns the NP-completeness of the statement for three common generalization patterns: $\sqsubseteq$-msg, $\preceq$-msg and $\preceq$-lcg -- the two latter being one and the same concept. The proof can in fact be adapted straightforwardly to the last of our subcases, namely the largest common generalization with relation $\sqsubseteq$. This observation is formalized in the following corollary.
%\begin{corollary} 
%	Let $G_1$ and $G_2$ be goals, and $p\in\mathbb{N}_0$ some constant. The decision problem ``does there exist a $\leqslant$-lcg of $G_1$ and $G_2$ that has less than $p$ different variables?'' is NP-complete. 
%\end{corollary} 
%\begin{proof}
%	The proof is analogous to that of Theorem~\ref{thm-dataflow-np-complete}. 
%	A generalization $G$ of two goals $G_1$ and $G_2$ can be verified to be a $\leqslant$-lcg by computing such a lcg (which can be done in a polynomial time), comparing their lengths and counting the number of variables in $G$. 
%	
%	Next, the reduction from SCP exposed above can be used as-is, since the obtained anti-unification instance harbors no term at all, so that in this case any generalization is a $\leqslant$-lcg iff it is a $\leqslant$-msg.
%\end{proof}
Now instead of looking to \textit{minimize} the number of different variables in the computed generalization $G$, one could be interested in \textit{forcing} to preserve all the dataflow implied in the generalized goals, not allowing to abstract away the links that appear in the goals' terms. Intuitively, this can be done by forbidding any term from one of the input goals to have more than one "corresponding term" in the other input goal. In other words, the dataflow is considered entirely preserved if the underlying variabilization function $\Phi$ doesn't associate any term with two or more different terms at the same time. Formally, this amounts to using an \textit{injective version} of our generalization relations. We say that a generalization relation is injective if its definition only holds for injective substitutions. For a common generalization $G$ of goals $G_1$ and $G_2$ and for some function $\Phi$ associating fresh variable names to couples of variables, this implies when using an anti-unification algorithm (e.g. Algorithm~\ref{algo-rel-1-lcg}) that for any two different variables $\Phi(T_1, T_2)$ and $\Phi(T_3, T_4)$ appearing in $G$, it holds that $T_1 \neq T_3 \neq T_2 \neq T_4\neq T_1$. We will denote by $\sqsubseteq^\iota$ (resp. $\preceq^\iota$) the versions of $\sqsubseteq$ (resp. $\preceq$) that exhibit this property.

\begin{example}
    Consider the injective relation $\preceq^\iota$ as well as the goals 
    	$G_1 = \{and(A,B), or(B,C), xor(C,A)\}$ and
    	$G_2 = \{and(X,Z), or(Y,X), xor(Z,Y)\}$.
    The only common generalizations are $\emptyset$, $\{and(\Phi(A,X),\Phi(B,Z))\}, \{or(\Phi(B,Y), \Phi(C,X))\}$ and $\{xor(\Phi(C,Z), \Phi(A,Y))\}$. No common generalization of size larger than 1 exists, since (at least) one of the matching substitutions is not injective. For example, the goal $G = \{and(\Phi(A,X), \Phi(B,Z)), or(\Phi(B,Y), \Phi(C,X))\}$ is not a common generalization of $G_1$ and $G_2$, since (at least) one of the substitutions mapping this goal to $G_1$ or $G_2$ is not injective. Indeed, the substitution $[\Phi(A,X) \mapsto A, \Phi(B,Z) \mapsto B, \Phi(B,Y) \mapsto B, \Phi(C,X) \mapsto C]$ maps both $\Phi(B,Z)$ and $\Phi(B,Y)$ to $B$; this is sufficient to reach the conclusion that $G$ is not an injective generalization of $G_1$ and $G_2$. Note that in this case, the other potential substitution, i.e. the one mapping $G$ on $G_2$, is not injective either. 
\end{example}

The two following observations immediately result from the injective relations being more constrained versions of their non-injective counterparts. 

\begin{proposition}
	Relations $\sqsubseteq^\iota$ and $\preceq^\iota$ are quasi-orders. 
\end{proposition}

\begin{proposition}
	Let $G_1$ and $G_2$ be goals. If $G_1\sqsubseteq_\theta^\iota G_2$, then $G_1\sqsubseteq_\theta G_2$. If $G_1\preceq^\iota_\theta G_2$, then $G_1\preceq_\theta G_2$ and $G_1\sqsubseteq^\iota_\theta G_2$.
\end{proposition}

With an injective generalization relation, the computing of a msg is fundamentally dissociated from that of an lcg, as an msg is not necessarily a lcg due to the injectivity constraint. However, both situations are intractable. In order to show this formally, we define the following decision problem variant.

\begin{theorem}\label{thm-inj-np-complete}
	Let INJ denote the following decision problem: "Given an injective generalization relation $\leqslant^\iota$ along with goals $G_1$ and $G_2$ such that $|G_1|\le|G_2|$, verify whether there exists an ad hoc injective substitution $\theta$ such that $G_1\theta\subseteq G_2$". INJ is NP-complete.
\end{theorem}
%\begin{proof}
%	INJ is in NP: given a relation $\leqslant^\iota$, goals $G_1$ and $G_2$ and a substitution (or renaming) $\theta$, it is possible to verify in polynomial time whether the application of $\theta$ on $G_1$ results on a subset of $G_2$ or not.
%	As for the proof of NP-hardness, we refer to~\cite{gen} in which the problem ``is $G_1$ a $\preceq^\iota$-lcg of $G_1$ and $G_2$?'' has been proved to be NP-complete using a polynomial reduction from the Induced Subgraph Isomorphism Problem~\cite{SYSLO198291}. The same reduction can be used for the other cases, leading to the conclusion that INJ is NP-complete.
%\end{proof}

INJ is basically the verification of whether a goal $G_1$ can be adequately mapped onto (a subset of) another goal $G_2$. If there exists a substitution $\theta$ (resp. a renaming $\rho$) making this possible, then $G_1$ is a $\sqsubseteq^\iota$- (resp. $\preceq^\iota$-)largest \textit{and} most specific generalization of $G_1$ and $G_2$, since no larger nor structurally more specific goal than $G_1$ can exist for this specific situation.

Due to the inherent intractability of injective relations, it is sometimes preferable to make use of tractable abstractions rather than exact brute-force algorithms, especially if a quick and approximate (though entirely dataflow-preserving) anti-unification result suffices for the application at hand. In the next section, we give such an efficient -- yet highly accurate~-- abstraction for the computation of $\preceq^\iota$-lcg's. 
