\section{Introduction}
We consider solving the following optimization:
\eql{\label{eq:p1}
	\min_{x\in \Cc} f(x).
	}
We assume:
\begin{description}
\item [A1] Problem \eqref{eq:p1} has solution(s) $x^*$.	
\item [A2]The constraint set $\Cc\in \RR^n$ is closed\footnote{Closeness of $\Cc$ and a lower bound on $f$ implies the existence of a minimum $x^*$ for \eqref{eq:p1}. However we one can drop the closeness and use approximation, see \cite{Blumen}.} and has a well-defined Euclidean projection $\pp_\Cc:\RR^n\rightarrow \Cc$. %that is well defined $\forall x\in \RR^n$ w.r.t. the Euclidean metric. 
\item [A3] The loss function $f(x):\RR^n\rightarrow \RR$ is bounded below, and has a well-defined gradient $\nabla f(x), \forall x\in \Cc$.
\item [A4] $f$ satisfies the Restricted Lipschitz Smoothness (RLS): $\forall x,x'\in \Cc$ it holds 
\eq{ f(x')-f(x) \leq \langle x'-x,\nabla f(x)\rangle + \frac{\MM}{2} \norm{x'-x}^2,}
where $M>0$ is the smallest constant satisfies the inequality above.
%\item [A4]  $f$ satisfies the Restricted Strong Convexity (RSC) for an (global) optimal point $x^*$ of \eqref{eq:p1} i.e., $\forall x\in \Cc$ it holds:
%\eq{ f(x^*)-f(x) \geq \langle x^*-x,f'(x)\rangle + \frac{\mm_{x^*}}{2} \norm{x-x^*}^2.}
\item [A5]  $f$ satisfies the Restricted Strong Convexity (RSC) for a non empty set $\Omega\in \Cc$: for each $x^\rsc\in\Omega$ it holds
\begin{align*} 
f(x^\rsc)-f(x) &\geq \langle x^\rsc -x,\nabla f(x)\rangle + \frac{\mm_{x^\rsc}}{2} \norm{x^\rsc-x}^2,\\
f(x)- f(x^\rsc) &\geq \langle x-x^\rsc,\nabla f(x^\rsc)\rangle + \frac{\mm_{x^\rsc}}{2} \norm{x^\rsc-x}^2.
\end{align*}
\end{description}





\section{Exact Iterative Projected Gradient}
First order methods in form of iterative projected gradient (IPG) are very popular to solve problem \eqref{eq:p1}. For positive integers $k$ and a well-chosen step size $\mu$, IPG iterates as follows:
\eql{\label{eq:IP}x^{k+1} = \pp_\Cc(x^k-\mu \nabla f(x^k)).}
We do not make a particular assumption on the initialization of \eqref{eq:IP} and we set $x^0=0$ throughout.
%they can handle complex sets $\Cc$ (as long as A1 holds). They are suitable for high dimensional inference tasks: their convergence is almost independent of dimension, cost per iteration is scalable with data size, they can be redesigned as stochastic descend with cheap cost per iteration.  

{\thm{\label{th:exactf} Assume (\textbf{A1-A5}) and $\MM< 2\mm$. Set the step size $(2\mm)^{-1}<\mu\leq\MM^{-1}$. The sequence generated by Algorithm \eqref{eq:IP} obeys the following bounds:
%\eq{
%	\norm{x^{k+1}-x'}\leq \left(\sqrt{\frac{1}{\mu \mm} -1}\right)^k \norm{x^0-x'} + \frac{2\norm{\nabla f(x')}}{m(1-(\sqrt{\frac{1}{\mu \mm} -1})}
%	}
\begin{itemize}
	\item For any $x^\rsc\in \Omega$ and positive integer $k$
	\eq{
	\norm{x^{k}-x^\rsc}\leq \rho^k \norm{x^\rsc} + \frac{2}{m(1-\rho)}\norm{\nabla f(x^\rsc)},
} 
where $\rho=\sqrt{\frac{1}{\mu \mm} -1}$.

\item  After a finite number $k\geq K$ of iterations 
\eq{
	\norm{x^{k}-x^*}\leq %\rho^k \norm{x^0-x'} +
 \frac{6-2\rho}{m(1-\rho)} \norm{\nabla f(x^\rsc_{\min})}.
 }
Where,
\begin{align*}
&x^\rsc_{\min}\in \argmin_{x^\rsc\in \Omega}\norm{\nabla f(x^\rsc)},\\
&K=\left\lceil\log_\rho\left( \frac{2\norm{\nabla f(x^\rsc_{\min})}}{m(1-\rho) \norm{x^\rsc_{\min}}}\right)\right\rceil,
\end{align*} 
and $x^*$ is a global solution of \eqref{eq:p1}.

\end{itemize}

		}}








\section{Inexact Iterative Projected Gradient}
In this section we show that IPG is robust against deterministic (worst case) errors. Moreover, we show that for certain decaying approximation errors, IPG solution maintain  the same accuracy as for the approximation-free (exact IPG) case.
\subsection{Absolute Approximation Errors}
inIPG iterates as follows:
\eql{\label{eq:inIP} x^{k+1} = \pp_\Cc\left(x^k-\mu( \nabla f(x^k) + \eg^k)\right) + \ep^k,}
where $\ep^k,\eg^k\in \RR^n$ account for (approximation) errors in calculating the projection and gradient steps respectively, and they both assumed to have bounded norms 
\eq{\norm{\ep^k}\leq \nup^k \qandq \norm{\eg^k}\leq \nug^k.}
Note that in this part we assume neither errors $\ep^k,\eg^k$ depend to the previous solutions and thus we refer to them as the \emph{absolute} approximation errors.  


{\thm{\label{th:inexact1}Assume (\textbf{A1-A5}) and $\MM< 2\mm$. Set the step size $(2\mm)^{-1}<\mu\leq\MM^{-1}$. The sequence generated by Algorithm \eqref{eq:inIP} obeys the following bounds:
\begin{itemize}
	\item For any $x^\rsc\in \Omega$ and positive integer $k$
	\eq{
		\norm{x^{k}-x^\rsc}\leq  \rho^k \left(\norm{x^\rsc}+\sum_{i=1}^k \rho^{-i} \nut^i \right)+ \frac{2}{m(1-\rho)}\norm{\nabla f(x^\rsc)}
	} 
	%where, \eq{\nut^i=%\sum_{i=1}^k \rho^{-i}
	% \left(\frac{2\nug^i}{m} + \sqrt{\frac{\nup^i}{\mu m}}\right).}
	
	\item The distance to $x^*$ the global solution of \eqref{eq:p1}
	\eq{
		\norm{x^{k}-x^*}\leq  \rho^k \left(\norm{x^\rsc}+\sum_{i=1}^k \rho^{-i} \nut^i \right)+ \frac{2(2-\rho)}{m(1-\rho)}\norm{\nabla f(x^\rsc)}
	} 
	where, \eq{\nut^i=%\sum_{i=1}^k \rho^{-i}
		\frac{2\nug^i}{m} + \sqrt{\frac{\nup^i}{\mu m}},}
\end{itemize}		
and $\rho$ is the same as in Theorem \ref{th:exactf}.		
	}
}

We achieve a linear convergence 
%(up to a tolerance level determined by $\norm{\nabla f(x^\rsc_{\min})}$)
 as long as $\nut^k$ geometrically decays to zero. 	
The following corollary makes this statement explicit:

{\cor{Assume $\nut^k= O(r^k)$ for some $0<r<1$. We have 
		%After a finite number $k\geq K$ of iterations 
		\eq{
			\norm{x^{k}-x^*}\leq %\rho^k \norm{x^0-x'} +
			\frac{4-2\rho}{m(1-\rho)} \norm{\nabla f(x^\rsc_{\min})}+ O({\bar \rho}^k), 
		}
		where $\rho$ and $x^\rsc_{\min}$ are the same as in Theorem \ref{th:exactf},
		\begin{align*}
		%&x^\rsc_{\min}\in \argmin_{x^\rsc\in \Omega}\norm{\nabla f(x^\rsc)},\\
		%&K=\left\lceil\log\left( \frac{2\norm{\nabla f(x^\rsc_{\min})}}{m(1-\rho) \norm{x^\rsc_{\min}}}\right)/\log(\bar \rho)\right\rceil,\\
		\bar \rho = \choice{\max(\rho,r) \qforq r\neq\rho \\
		                	r+\delta     \qforq r=\rho}
		\end{align*}
and any small $\delta>0$.	
		}
	}



\subsection{$(1+\epsilon)$-Approximate Projection}
$\pp\epsilon$


\section{Constrained Least squares} 
Here $f(x):= \frac{1}{2}\norm{y-Ax}^2$. Assumptions \textbf{A4-A5} can be equivalently summarized into the following property:
{\defn{[A6] $A$ is bi-Lipschitz w.r.t. $\Cc$ and $x_0\in \Cc$ if the two following inequalities hold:
		\begin{align*}
		&\norm{A(x-x')}^2\leq M \norm{x-x'}^2 \quad \forall x,x'\in \Cc\\
		&\norm{A(x-x_0)}^2 \geq m_{x_0} \norm{x-x_0}^2 \quad \forall x\in \Cc.
		\end{align*}				
		}}
\subsection{Absolute Approximation Errors}		
{\thm{\label{th:inexactLS1}Assume (\textbf{A1-A2, A6}) and $\MM< 2\mm$. Set the step size $(2\mm)^{-1}<\mu\leq\MM^{-1}$. The sequence generated by Algorithm \eqref{eq:inIP} obeys the following bounds:
\eq{
\norm{x^{k}-x_0}\leq  \rho^k \left(\norm{x^\rsc}+\sum_{i=1}^k \rho^{-i} \nut^i \right)+ \frac{2\sqrt{M}}{m(1-\rho)}w
	}
where $w=\norm{y-Ax_0}$, $\rho$ and $\nut^i$ are the same as in Theorem \ref{th:inexact1}.
}} 


\subsection{$(1+\epsilon)$-Approximate Projection}

{\thm{\label{th:inexactLS2}Assume (\textbf{A1-A2, A6}). Let
	\eq{\frac{\norm{A}}{\sqrt m}h(\epsilon)\leq \theta<1 \qandq \MM< (2+\theta^2-2\theta) \mm.} Set the step size $\left((2+\theta^2-2\theta) \mm\right)^{-1}<\mu\leq\MM^{-1}$. The sequence generated by Algorithm \eqref{eq:inIP} obeys the following bounds:
		\eq{
			\norm{x^{k}-x_0}\leq  \rho^k \left(\norm{x^\rsc}+\sum_{i=1}^k \rho^{-i} \nut^i \right)+ \left(\frac{ 2\frac{\sqrt{M}}{m}+\sqrt{\frac{\mu}{m}}h(\epsilon)}{1-\rho}\right)w
		}
		where 
		\begin{align*}
		&\rho=\sqrt{\frac{1}{\mu \mm} -1}+\frac{\norm{A}}{\sqrt{m}} h(\epsilon)\\
		&\nut^i= \left(\frac{1}{m}+\sqrt{\frac{\mu}{m}}h(\epsilon)\right)\nug^i
		\end{align*}
		
		and $w=\norm{y-Ax_0}$.
	}} 