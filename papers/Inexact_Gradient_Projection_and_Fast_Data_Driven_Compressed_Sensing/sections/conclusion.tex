\section{Conclusion and discussions}\label{sec:conclusion}
We studied the robustness of the iterative projected gradient  algorithm against inexact gradient and projection oracles and for solving inverse problems in compressed sensing. We considered fixed precision, progressive fixed precision and $(1+\epsilon)$-approximate oracles. A notion of model information preserving under a hybrid local-uniform embedding assumption is at the heart of our analysis. We showed that under the same assumptions, the algorithm with PFP approximate oracles achieves 
%(or even empirically improves) 
 the accuracy of the exact IPG.
For a certain rate of decay of the  approximation errors this scheme  can also maintain the  rate of linear convergence as for the exact algorithm. We  also conclude that choosing too fast decays does not help the convergence rate beyond the exact algorithm and therefore  can result in computational inefficiency. 
The $(1+\epsilon)$-approximate IPG can also achieves  
%(or empirically improves) 
the accuracy of the exact algorithm, however
under a stronger embedding condition, slower rate of convergence and possibly more noise amplification  compared to the exact algorithm. We show that this approximation is sensitive to the CS subsampling regime i.e. for high compression ratios one can not afford too large approximation. 
We applied our results to a class of data driven compressed sensing problems, where we replaced exhaustive NN searches over large datasets with fast and approximate alternatives introduced by the cover tree structure. Our experiments indicate that the inexact IPG with  $(1+\epsilon)$-ANN searches (and also comparably the PFP type search) can significantly accelerate the CS reconstruction.  
% compared to other exact/inexact forms of the algorithm. This observation is aligned with the fact that the $(1+\epsilon)$ approximations are often more efficiently implemented in practice (i.e. for most nonconvex settings) meanwhile achieving a fixed precision approximation error could be computationally expensive.

Our results require a lower bound on the chosen step size which is a critical assumption for globally solving a nonconvex CS problem, see e.g. \cite{IHTCS,Blumen,AIHT}. With no lower bound on the step size only convergence to a local critical point is guaranteed (for the exact algorithm) see e.g.~\cite{Nesterov:nonconvex,Attouch}. Such assumption is restrictive for solving convex problems, where an upper bound on the step size is generally sufficient for the convergence. However recent studies~\cite{negahban:fast,Gabriel:FBlocal,Oymak:tradeoff} established fast \emph{linear} convergence for solving non strongly convex problems such as CS with the exact IPG, and by assuming a notion of \emph{local} (model restricted) strong convexity as well as choosing large enough step sizes. In our future work  
we would like to make more explicit connection between these results and our embedding assumptions and extend our approximation robustness study to the convex CS recovery settings with sharper bounds than e.g. \cite{BachinexactIPG}.

%Another 
A limitation of our current method is the dependency on $\epsilon$ for the CS recovery using $(1+ \epsilon)$-ANN searches. In future we plan to investigate whether the ideas from~\cite{Hegde15} can be generalized to include our data-driven signal models.

Finally we did not provide much discussion or experiments on the applications of the approximate gradient updates within the IPG. We think such approximations might be related to the sketching techniques for solving large size inverse problems, see~\cite{Pilanci:IHS, GPIS}.
In our future work we would like to make explicit connections  in this regard as well.





