\section{Related works}\label{sec:SOA}

Inexact proximal-gradient methods (in particular IPG)  and their Nesterov accelerated variants have been the subject of a substantial amount of work in convex optimization~\cite{Nesterov-inexact,dasper-inexact,villa:inexactFISTA,BachinexactIPG,charles:inexactFISTA}. Here we review some highlights and refer the reader for a comprehensive literature review to \cite{BachinexactIPG}.
Fixed precision approximates have been studied 
%to save memory and computation in calculation of 
for the gradient step e.g. for using the smoothing techniques where the gradient is not explicit and requires solving an auxiliary optimization, see for more details \cite{Nesterov-inexact} and \cite{dasper-inexact} for a semi-definite programming example in solving sparse PCA. A similar approach has been extensively applied for carrying out the projection (or the proximity operator) approximately in cases where it does not have an analytical expression and requires solving another optimization within each iteration  e.g. in total variation constrained inverse problems \cite{TVprojGabirel,Chambolle2011} or the overlapping group Lasso problem \cite{FoGLasso}. Fortunately in convex settings %and by taking into account the duality gap of the inner minimization 
one can stop the inner optimization when its duality gap falls below a certain threshold and achieve a fixed precision approximate projection. In this case the solution accuracy of the main problem is proportional to the approximation level introduced within each iteration. Recently \cite{BachinexactIPG} studied the progressive fixed precision (PFP) type approximations for solving convex problems, e.g. a sparse CUR type factorization, via gradient-proximal (and its accelerated variant) methods. The authors show that IPG can afford  larger approximation errors (in both gradient and projection/proximal steps) in the earlier stages of the algorithm and by increasing the approximation accuracy at an appropriate rate one can attain a similar convergence rate and accuracy as for the exact IPG however with significantly less computation.

A part of our results draws a similar conclusion for solving nonconvex constrained least squares that appears in CS problems by using inexact IPG. Note that an FP type approximation has been previously considered for the nonconvex IPG e.g. for the UoS \cite{Blumen} or the manifold \cite{MIP} CS models, however these works only assume inexact projections whereas our result covers  an approximate gradient step as well. Of more importance and to the best of our  knowledge, our results on incorporating the PFP type approximation in nonconvex IPGs and analysing the associated rate of (global) convergence is the first result of its kind. We also note that the results in \cite{BachinexactIPG} are mainly about minimizing convex cost functions (i.e. not necessarily recovery guarantees)
and that the corresponding   
linear convergence results 
%in \cite{BachinexactIPG}in case of a geometric decay in the PFP approximation levels, 
only hold for uniformly strong convex objectives. We instead  
%by introducing a notion of local embedding (see Section~\ref{sec:prelim}) 
cover cases with local (and not uniform) strong convexity and establish the linear convergence of IPG for solving underdetermined inverse problems such as CS.  


%convergence of IPG for the non-convex CS problem using the PFP type approximation is the first result of its kind.  


%more recent works on CS and optimization literature.


Using relative $(1+\epsilon)$-approximate projections (described in Section \ref{sec:relative}) for  CS recovery has been subject of more recent research activities (and mainly for nonconvex models). %more recently appearing 
%has been recently proposed for low-rank matrix completion \nref[Volkan] and tree-sparse CS reconstruction \nref[Hedge]. Both results are considering nonconvex models. Our analysis encompasses both cases.
In \cite{GiryIPGaprox} the authors studied this type of approximation for an  IPG sparse recovery algorithm under the UoS model in redundant dictionaries. Our result encompasses this as a particular choice of model and additionally allows for inexactness in the gradient step. The work of \cite{StoIHT} studied similar inexact oracles for a stochastic gradient-projection type algorithm customized for sparse UoS and low-rank models (see also \cite{MatrixAlpsapprox} for low-rank CS using an accelerated variant of IPG). We share a similar conclusion with those works; for a large $\epsilon$ more measurements are required for CS recovery and the convergence becomes slow. 
% (the projection onto k-flats is simple as hardthresholding and thus as they mentioned their result concerns more robust calculation in distributed systems).
Hegde et al. \cite{HegdeISIT} proposed such projection oracle for tree-sparse signals and use it for the related model-based CS problem using a CoSamp type algorithm (see also~\cite{daven:analysiscosamp,Giri:analysiscosamp} for related works on inexact CoSamp type algorithms).  
In a later work~\cite{Hegde15} the authors consider a modified variant of IPG with $(1+\epsilon)$-approximate projections for application to structured sparse reconstruction problems (specifically tree-sparse and earthmover distance CS models). For this scenario they are able to introduce a modified gradient estimate (called the Head approximation oracle) to strengthen the recovery guarantees by removing the dependency on $\epsilon$, albeit with a more conservative Restricted Isometry Property. Unfortunately, this technique does not immediately generalize to arbitrary signal models and we therefore do not pursue this line of research here. 



% \red{In a later work \cite{Hegde15} the authors consider to modify IPG (for the tree-sparse and the earthmover distance CS models) and use an additional signal-preserving projection oracle to strengthen the reconstruction result.   
%%and overcome the convergence limitations introduced by large approximation 
%However here we do not assume modifications to IPG nor availability of an extra oracle other than the gradient and model projection.} 




