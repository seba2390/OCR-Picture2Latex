\section{Main results}
\label{sec:main}
\subsection{Uniform linear embeddings}
The success of CS paradigm relies heavily on 
%the low (intrinsic) dimensionality of most natural signals which can be captured by an efficient model, as well as 
the embedding property of certain random sampling matrices which preserves signal information for low dimensional but often complicated/combinatorial models. It has been shown that the exact IPG can stably predict the true signal $x^\gt$ from noisy CS measurements provided that $A$ satisfies the so called Restricted Isometry Property (RIP): 
\eql{\label{eq:RIP}(1-\theta)\norm{x-x'}^2 \leq \norm{A(x-x')}^2\leq (1+\theta)\norm{x-x'}^2, \quad \forall x,x' \in \Cc'
}
for a small constant  $0<\theta<1$.
This has been shown for models such as sparse, low-rank and low-dimensional smooth manifold signals and by using 
IPG type reconstruction 
algorithms which in the nonconvex settings are also known as Iterative Hard Thresholding \cite{IHTCS, Ma2011, AIHT,MIP, modelbasedCS}. Interestingly these results indicate that under the RIP condition (and without any assumption on the initialization) the first order IPG algorithms with cheap local oracles can globally solve
nonconvex optimization problems. 

For instance random orthoprojectors and i.i.d. subgaussian matrices $A$ satisfy RIP when the number of measurements $m$ is proportional to the intrinsic dimension of the model (i.e. signal sparsity level, rank of a data matrix or the dimension of a smooth signal manifold, see e.g. \cite{RichCSreview} for a review on comparing different CS models and their measurement complexities) and sublinearly scales with the ambient dimension $n$. 

A more recent work generalizes the theory of IPG to arbitrary \emph{bi-Lipschitz embeddable} models \cite{Blumen}, that is for given $\Cc'$ and $A$ it holds
\eq{\mm \norm{x-x'}^2\leq \norm{A(x-x')}^2\leq \MM \norm{x-x'}^2 \quad \forall x,x'\in \Cc'.}
for some constants $\mm, \MM >0$. Similar as for the RIP these constants are defined \emph{uniformly} over the constraint set i.e. $\forall x,x'\in \Cc'$. There Blumensath shows that if \eq{\MM<1.5\mm,} then IPG robustly solves the corresponding noisy CS reconstruction problem \emph{for all} $x^\gt\in \Cc'$. This result also relaxes the RIP requirement to a nonsymmetric and unnormalised notion of linear embedding whose implication in deriving sharper recovery bounds is previously studied by \cite{JaredJeff}. 

\subsection{Hybrid (local-uniform) linear embeddings}
Similarly the notion of restricted embedding plays a key role in our analysis. However we adopt a more local form of embedding and show that it is still able to guaranty stable CS reconstruction.  \marginpar{\todo{x0 or x*? C' deleted?}}

{\ass{ \label{def:Lip}
 Given $(x_0\in \Cc'\subseteq \Cc, A)$ there exists constants  $\MM,\mm_{x_0}>0$ for which the following inequalities hold:%\todo{$x_0$ or $x*$}
		%$A$ satisfies the following inequalities w.r.t. $\Cc$ and a point $x_0\in \Cc$:
		\begin{itemize}
			\item Uniform Upper Lipschitz Embedding (ULE)
			\begin{align*}
			\norm{A(x-x')}^2\leq \MM \norm{x-x'}^2 \quad \forall x,x'\in \Cc
			\end{align*}
			\item Local Lower Lipschitz Embedding (LLE)
			\begin{align*}
			\norm{A(x-x_0)}^2 \geq \mm_{x_0} \norm{x-x_0}^2 \quad \forall x\in \Cc
			\end{align*}
		\end{itemize}	
		Upon existence, $\MM$ and $\mm_{x_0}$ denote  respectively the smallest and largest constants for which the inequalities above hold.		
	}}
\newline
	
 Note that with respect to our distinction between the projection set $C$ and the original signal model $C'\subseteq C$ defined in \eqref{eq:proj1} and \eqref{eq:eproj}, our embedding assumption is on the projection set.
 
To compare with the previous results we shall set $\Cc=\Cc'$; which implies a weaker assumption compared to RIP or the uniform bi-Lipschitz embedding. Note that for any $x_0\in\Cc'$ and $\Cc=\Cc'$ we have: \eq{
\mm\leq\mm_{x_0}\leq\MM\leq \vertiii{A}^2
} 
(where $\vertiii{.}$ denotes the matrix spectral norm i.e. the largest singular value). 
In this case one has to sacrifice the \emph{universality} of the RIP dependant results for a signal $x^\gt$ dependent analysis. Depending on the study, local analysis could be very useful as for instance allows for avoiding worst-case scenarios that might unnecessarily restrict the recovery analysis~\cite{me:modelselecion}. In the convex settings it has been shown that local embeddings can improve the measurement bound and the speed of convergence up to sharp constants~\cite{recht:GW, Oymak:tradeoff}. 

Unfortunately we are currently unable to make the analysis fully local as we require the uniform RLS constraint. Nonetheless, one can always plug the stronger bi-Liptchitz assumption into our results throughout (i.e. replacing $\mmx$ with $\mm$) and regain the universality.  


%Our results on the exact and inexact FP approximate IPG improve the Blumensath's recovery condition in two folds: first by relaxing the uniform lower Lipschitz constant $\mm$ to a local form $\mmx$ which avoids worst-case scenarios that might unnecessarily restrict the recovery analysis (see Definition X), and second, by improving the factor in the Lipschitz embedding condition i.e. $\MM < 2\mmx$ for CS recovery (see e.g. Theorem 1). 






%In addition, if there exist a uniform constant $\mm>0$ such that 
%\eq{\mm\leq \mmx, \quad \forall x_0\in \Cc}
%then $A$ is a bi-Lipschitz embedding with constants $\mm,\MM$ i.e.
%\eq{\mm \norm{x-x'}^2\leq \norm{A(x-x')}^2\leq \MM \norm{x-x'}^2 \quad \forall x,x'\in \Cc.}
%Note that for any $x_0\in \Cc$ it holds \eq{\mm\leq\mm_{x_0}\leq\MM\leq \norm{A}^2.} 

%this assumption is weaker than  the so called Restricted Isometry Property (RIP) which guarantees performance of many CS reconstruction algorithms:
%{\defn{$A$ satisfies RIP w.r.t. a set $\Cc$ and a constant $0<\delta<1$, if $\forall x,x'\in \Cc$ it holds :
%		\eq{
%			(1-\delta)\norm{x-x'}^2\leq\norm{A(x-x')}^2\leq (1+\delta)\norm{x-x'}^2.
%			}
%}}
	
	
	
	

\subsection{Linear convergence of (P)FP inexact IPG for CS recovery}
In this section we show that IPG is robust against deterministic (worst case) errors. Moreover, we show that for certain decaying approximation errors, IPG solution maintain  the same accuracy as for the approximation-free (exact IPG) case.


In this part we assume neither $\ep^k$ or $\eg^k$ depend on $x^{k-1}$ or the previous updates.

{\thm{\label{th:inexactLS1} Assume $(x^\gt\in \Cc, \Cc,A)$ satisfy the main Lipschitz assumption  with constants $\MM< 2\mmx$. Set the step size $(2 \mmx )^{-1}<\mu\leq\MM^{-1}$. The sequence generated by Algorithm \eqref{eq:inIP} obeys the following bound:
		\eql{\label{eq:errbound}
			\norm{x^{k}-x^\gt}\leq  \rho^k \left(\norm{x^\gt}+\sum_{i=1}^k \rho^{-i} \nut^i \right)+ \frac{2\sqrt{\MM}}{\mmx(1-\rho)}w
		}
		where 
		\begin{align*}
		\rho=\sqrt{\frac{1}{\mu \mmx} -1} \qandq
		\nut^i=%\sum_{i=1}^k \rho^{-i}
		\frac{2\nug^i}{\mmx} + \sqrt{\frac{\nup^i}{\mu \mmx}},			
		\end{align*} 
		and $w=\norm{y-Ax^\gt}$.
	}} 

{\rem{Theorem \ref{th:inexactLS1} implications for the exact IPG (i.e. $\nup^k=\nug^k=0$) and inexact FP approximate IPG (i.e. $\nup^k=\nup,\nug^k=\nug, \forall k$)  improve  \cite[Theorem~2]{Blumen} in three folds: first by relaxing the uniform lower Lipschitz constant $\mm$ to a local form $\mmx\geq \mm$ and conduct a local recovery/convergence analysis. Second, by improving the embedding condition for CS stable recovery to 
\eql{\label{eq:cond}\MM < 2\mmx,
	} 
or $\MM < 2\mm$ for a uniform recovery $\forall x^\gt\in\Cc$. And third, by improving  twice faster the rate $\rho$ of convergence.}}

The following corollary is an immediate consequence of linear convergence established in Theorem \ref{th:inexactLS1} for which we do not provide proof:
{\cor{\label{cor:FP}With assumptions in Theorem \ref{th:inexactLS1} the IPG algorithm with FP approximate oracles achieves the solution accuracy
\eq{\norm{x^K-x^\gt}\leq 
		 	\frac{1}{1-\rho}\left(\frac{2\nug}{\mmx} + \sqrt{\frac{\nup}{\mu \mmx}},+ \frac{2\sqrt{\MM}}{\mmx}w\right) +\tau}
for a $\tau>0$ and in a finite number of iterations
\eq{
	K=\left\lceil\frac{1}{\log(\rho^{-1})} \log\left( \frac{ \norm{x^\gt}}{\tau}\right)\right\rceil
	}			
}}
As it turns out in our experiments and aligned with the result of Corollary~\ref{cor:FP}, the solution accuracy of IPG can not exceed the precision levels introduced by PF oracles. In this sense Corollary~\ref{cor:FP} is tight as a trivial converse example would be that IPG starts from the optimal solution $x^\gt$ but an adversarial FP scheme projects it to another point within a fixed distance. 

Interestingly one can make another implication of Theorem~\ref{th:inexactLS1} and overcome such limitation using the PFP type oracles.
%Here comes 
%an interesting part of Theorem \ref{th:inexactLS1} analysing the behaviour of the PFP type oracles. 
Remarkably 
one achieves a linear convergence to a solution with the same accuracy as for the exact IPG, 
as long as $\nut^k$ geometrically decays. 	
The following corollary makes this statement explicit:
	
%{\cor{\label{cor:decay}Assume $\nut^k= O(r^k)$ for some error decay rate $0<r<1$ and a constant $C$. We have 
%			%After a finite number $k\geq K$ of iterations 
%			\eq{
%				\norm{x^{k}-x^*}\leq %\rho^k \norm{x^0-x'} +
%				\frac{2\sqrt{\MM}}{\mm_{x^\gt}(1-\rho)} w+ O({\bar \rho}^k), 
%			}
%			where $\rho$ is the same as in Theorem \ref{th:inexactLS1}, and
%			\begin{align*}
%			%&x^\rsc_{\min}\in \argmin_{x^\rsc\in \Omega}\norm{\nabla f(x^\rsc)},\\
%			%&K=\left\lceil\log\left( \frac{2\norm{\nabla f(x^\rsc_{\min})}}{m(1-\rho) \norm{x^\rsc_{\min}}}\right)/\log(\bar \rho)\right\rceil,\\
%			\bar \rho = \choice{\max(\rho,r)\quad r\neq\rho \\
%				r+\xi\qquad\quad r=\rho}
%			\end{align*}
%			for an arbitrary small $\xi>0$.	
%}
%}
	
{\cor{\label{cor:decay}Assume $\nut^k\leq Cr^k$ for some error decay rate $0<r<1$ and a constant $C$. Under the assumptions of Theorem~\ref{th:inexactLS1} the solution updates $\norm{x^{k}-x^*}$ of the IPG algorithm with PFP approximate oracles is bounded above by:
		%After a finite number $k\geq K$ of iterations 
		\begin{align*}
			%\norm{x^{k}-x^*}\leq 
			&\max(\rho,r)^k \left(\norm{x^\gt}+\frac{C}{1-\frac{\min(\rho,r)}{\max(\rho,r)}}\right) +
			\frac{2\sqrt{\MM}}{\mm_{x^\gt}(1-\rho)} w,  &r\neq \rho \\
			&\rho^k \Big(\norm{x^\gt}+Ck\Big) +
			\frac{2\sqrt{\MM}}{\mm_{x^\gt}(1-\rho)} w, &r=\rho 
		\end{align*}
		Which implies a linear convergence at rate 
%		$\bar \rho$:
%		\eq{
%		\norm{x^{k}-x^*}\leq %\rho^k \norm{x^0-x'} +
%		\frac{2\sqrt{\MM}}{\mm_{x^\gt}(1-\rho)} w+ O({\bar \rho}^k), 
%		}
%		where,
		\begin{align*}
		%&x^\rsc_{\min}\in \argmin_{x^\rsc\in \Omega}\norm{\nabla f(x^\rsc)},\\
		%&K=\left\lceil\log\left( \frac{2\norm{\nabla f(x^\rsc_{\min})}}{m(1-\rho) \norm{x^\rsc_{\min}}}\right)/\log(\bar \rho)\right\rceil,\\
		\bar \rho = \choice{\max(\rho,r)\quad r\neq\rho \\
			\rho+\xi\qquad\quad r=\rho}
		\end{align*}
		for an arbitrary small $\xi>0$.	
	}
}
{\rem{Similar to Corollary \ref{cor:FP} 
%and due to the linear convergence 
one can increase the final solution precision of the FPF type  IPG with logarithmically more iterations i.e. in a finite number $K=O(\log(\tau^{-1}))$ of iterations one achieves $\norm{x^{k}-x^*}\leq O(w)+\tau$. As we can see (and in contrast with the FP oracles) one could achieve an accuracy within the noise level $O(w)$ that is the precision of an approximate-free IPG.
}}

{\rem{Using the PFP type oracles can also maintain the rate of linear convergence identical as for the exact IPG. For this the approximation errors suffice to follow a geometric decaying rate of  $r<\rho$. \todo{discuss suitable oracle complexity within PFP.}
}}

\subsection{Linear convergence of inexact IPG with $(1+\epsilon)$-approximate projection for CS recovery}
\label{sec:relative}
In this part we focus on analysing the inexact IPG algorithm with a $(1+\epsilon)$-approximate projection. As it turns out in the following theorem we require a stronger condition on the embedding to guaranty the CS stability as compared to that \eqref{eq:cond} for the previous cases.
%the exact or the FP/PFP type inexact IPG. 


{\thm{\label{th:inexactLS2} Assume $(x^\gt\in \Cc, \Cc,A)$ satisfy the main Lipschitz assumption and that
		\eq{\sqrt{\epsilon+\epsilon^2}\leq \delta\frac{\sqrt{\mmx}}{\vertiii{A}} \qandq \MM < (2-2\delta+\delta^2) \mmx} 	
		for $\epsilon\geq 0$ and some constant $\delta \in [0,1)$.
		Set the step size $\left((2-2\delta+\delta^2) \mmx\right)^{-1}<\mu\leq\MM^{-1}$. The sequence generated by Algorithm \eqref{eq:inIP2} obeys the following bound:
		\eq{
			\norm{x^{k}-x^\gt}\leq  \rho^k \left(\norm{x^\gt}+\kappa_g \sum_{i=1}^k \rho^{-i} \nug^i \right)+ 
			%\frac{ \left( 2\frac{\sqrt{M}}{m}+\frac{\sqrt\mu}{\norm{A}}\delta \right)}
			\frac{\kappa_w}{1-\rho}w
		}
		where 
		\begin{align*}
		&\rho=\sqrt{\frac{1}{\mu \mmx} -1}+ \delta, \quad
		\kappa_g = \frac{2}{\mmx}+\frac{\sqrt\mu}{\vertiii{A}}\delta, \\		
		&\kappa_w= 2\frac{\sqrt{\MM}}{\mmx}+\sqrt\mu\delta, \qandq w=\norm{y-Ax^\gt}.
		\end{align*}		
		
	}} 	
{\rem{Similar conclusions can be made as in Cor1,2 on the linear convergence, logarithmic number of iterations vs final level of accuracy (depending whether the gradient oracle is exact or FP/PFP) however with a stronger requirement than \eqref{eq:cond} on the embedding: increasing $\epsilon$ limits the recovery guaranty and slows down the convergence (by comparing $\rho$ in Theorems 1 and 2) as $\epsilon$ increases.}}

{\rem{Assumptions of Theorem \ref{th:inexactLS2} impose a stringent requirement on the scaling of the approximation parameter i.e. $\epsilon=O\left(\sqrt{\frac{\mmx}{\vertiii{A}}}\right)$ which is not purely dependant to the model restricted embedding conditioning but also the spectral norm of $\vertiii{A}$. In this sense since  $\vertiii{A}$ ignores the structure $\Cc$ of the problem it might scale very differently as compared to the corresponding embedding constants $\mmx,\mm$ and $\MM$. For instance an $m\times n$  i.i.d. Gaussian matrix have with high probability $\vertiii{A}=\Theta(n)$ (when $m\ll n$) whereas, e.g. for $s$-sparse signals, its embedding constants $\mm,\MM$ w.h.p. scale as $O(m)$. A similar gap exists for other low dimensional signal models and for other compressed sensing matrices e.g. random orthoprojectors, 
%for$(s\ll m)$ sufficiently large $m$ hav This is the case for e.g. orthoprojectors and subgaussians matrices often 
which is unfavourable for applying large approximation of this type to the extreme compressed sensing settings where $\sqrt{\frac{\mmx}{\vertiii{A}}}\approx \sqrt{\frac{m}{n}}\rightarrow 0$. 
}}

In the following we show by a deterministic example that this requirement is indeed tight. We also observe empirically in Section \ref{sec:expe} that such limitation indeed holds in randomized settings (e.g. i.i.d. Gaussian $A$) and on average. 

\subsubsection*{A converse example}
Consider a noiseless CS recovery problem where $n =2$, $m=1$ and the sampling matrix (i.e. here a row vector) is
\eq{A=[\cos(\gamma)\quad -\sin(\gamma)]}
for some parameter $0\leq\gamma\leq\pi/2$. Consider the following signal model which is a line segment across the first dimension:
\eq{
\Cc = \{x\in \RR^2: 0\leq x(1)\leq 1,\, x(2)=0\}.
}
We have indeed $\vertiii{A}=1$. It is easy to verify that both of the embedding constants w.r.t. to $\Cc$ are
\eq{ \mmx=\MM = \cos(\gamma)^2.}
Therefore one can tune $\gamma\rightarrow \pi/2$ to obtain arbitrary small ratios for $\sqrt{\frac{\mmx}{\vertiii{A}}}=\cos(\gamma)$. 
%$\vertiii{A}$ and $\mmx$ the 

Assume the true signal and the initialization point are at the either ends of $\Cc$ i.e.
\eq{
x^\gt=[1 \quad 0]^T,\quad y=Ax^\gt=\cos(\beta) \qandq x^0=[0 \quad 0]^T.
}
%and the initialization point $x^0=(0,0)^T$. 
Consider an adversarial $(1+\epsilon)$-approximate projection oracle which performs the following step for any given $x\in \RR^2$: \marginpar{\todo{verify + this part modified $\epsilon$}}
\eq{
		\pp_\Cc^{\epsilon}(x) := [x(1)+\epsilon x(2) \quad 0]^T.
	}
For simplicity we assume no errors on the gradient step. 	
By setting $\mu=1/\cos(\gamma)^2$, the corresponding inexact IPG updates as 
\eq{	
	x^k(1)= \epsilon\tan(\gamma) \left(x^{k-1}(1)-1\right) + 1 
}
and only along the first dimension (we note that due to the choice of oracle $x^k(2)=0, \forall k$). Therefore
\eq{	
	x^k(1)= \left(\epsilon\tan(\gamma)\right)^k+1.
}
which requires $\epsilon<\tan^{-1}(\gamma)=O(\cos(\gamma))$ for convergence, and it diverges otherwise. As we can see for $\gamma\rightarrow \pi/2$  (i.e. where $A$ becomes extremely unstable w.r.t. sampling the first dimension) the range of admissible $\epsilon$ shrinks, irrespective of the fact that the restricted embedding $\mmx=\MM$ exhibits an perfect isometry; which is an ideal situation for solving a noiseless CS with the exact IPG (i.e. in this case IPG takes only one iteration to converge).


