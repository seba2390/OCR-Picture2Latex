\section{Preliminaries}\label{sec:prelim}
Iterative projected gradient iterates between calculating the gradient and projection onto the model 
i.e. for positive integers $k$ the \emph{exact} form of IPG follows:
\eql{\label{eq:IP}
	x^{k} = \pp_\Cc\left(x^{k-1}-\mu \nabla f(x^{k-1})\right) }
where, $\mu$ is the step size, $\nabla f(x)=A^T(Ax-y)$ and $\pp_\Cc$ denote the exact gradient and the Euclidean projection oracles, respectively.  
The exact IPG requires   
%problem \eqref{eq:p1} has solution(s) and	
the constraint set $\Cc$ 
to have a well defined, not necessarily unique but  computationally tractable Euclidean projection $\pp_{\Cc}:\RR^n\rightarrow \Cc$ 
%We do not make a particular assumption on the initialization of \eqref{eq:IP} and we set $x^0=\mathbf{0}$ throughout. We  assume  
%%problem \eqref{eq:p1} has solution(s) and	
%the constraint set $\Cc$ 
% has a well defined Euclidean projection $\pp_\Cc:\RR^n\rightarrow \Cc$ 
% \footnote{Closeness of $\Cc$ %and a lower bound on $f$ 
% 	implies the existence of such minimum i.e. the exact projection. %$x^*$ for \eqref{eq:p1}. 
% 	%One might drop the closeness condition but rather use a FP type approximate projection as discussed in \cite{Blumen}.
% 	}:
\eq{
	\pp_{\Cc}(x)\in \argmin_{u\in\Cc}\norm{u-x}.}
Throughout we use $\norm{.}$ as a shorthand for  the Euclidean norm $\norm{.}_{\ell_2}$. 
%Closeness of $\Cc$ %and a lower bound on $f$ 
%implies the existence of a minimum, however we do not assume uniqueness for the exact projection. 

In the following we define three types of approximate oracles which frequently appear in the literature and could be incorporated within the IPG iterations. We also briefly discuss their applications. Each of these approximations are applicable to the data driven problem we will consider in Section~\ref{sec:datadrivenCS}.
\subsection{Fixed Precision (FP) approximate oracles}
\label{sec:FP}
We first consider 
%fixed precision (FP) 
approximate oracles with \emph{additive} bounded-norm 
%varying 
errors, namely the fixed precision gradient oracle $\nablaa^{\nug} f(x):\RR^n\rightarrow \RR^n$ where:
\begin{align}
\norm{\nablaa^{\nug} f(x) -\nabla f(x)}\leq \nug, \label{eq:grad} 
\end{align}
and the fixed precision projection oracle $\pp_\Cc^{\nup}:\RR^n\rightarrow\Cc$ where:
\begin{align}
\pp_\Cc^{\nup}(x) \in \Big\{ u\in \Cc :\,	\norm{u-x}^2 \leq \inf_{u'\in \Cc}\norm{u'-x}^2 +\nup^2  \Big\}.\label{eq:proj1}
\end{align} 
The values of $\nug,\nup$ denote the levels of inaccuracy in calculating the gradient and projection steps respectively.\footnote{Note that our fixed precision projection \eqref{eq:proj1} is defined on the squared norms in a same way as in e.g. \cite{Blumen,MIP}. %\red{In fact we found a typo in \cite{Blumen} eq. (7) for this definition, although the proofs there assumed a form similar to our definition.}
	} 
The corresponding \emph{inexact} IPG
%for the gradient IPG with a fixed-precision accuracy 
iterates as follows:
%\eql{\label{eq:inIP} x^k = \pp^{\nup^k}_{\Cc}\left(x^{k-1}-\mu( \nabla f(x^{k-1}) + \eg^k)\right) + \ep^k.}
\eql{\label{eq:inIP} x^k = \pp^{\nup^k}_{\Cc}\left(x^{k-1}
	%-\mu\left( \nabla f(x^{k-1}) + \eg^k\right)\right), }	
	-\mu \nablaa^{\nug^k} f(x^{k-1})\right).}
Note that, unlike \cite{Blumen,MIP} in this formulation we allow for variations in the inexactness levels at different stages of IPG. % \todo{however we assume neither $\ep^k$ or $\eg^k$ depend on $x^{k-1}$ or the previous solution updates.}  
The case where the accuracy levels are bounded by a constant threshold $\nup^k=\nup$ and $\nug^k=\nug$, $\forall k$, refers to an inexact IPG algorithm with \emph{fixed precision} (FP) approximate oracles.

\subsubsection*{Examples}
Such errors may occur for instance in distributed network
optimizations where the gradient calculations could be noisy during the communication on the network, or in CS under certain UoS models with infinite subspaces~\cite{Blumen}  where an exact projection might not exist by definition (e.g. when $\Cc$ is an open set) however a FP type approximation could be achievable. It also has application in finite (discrete) super resolution~\cite{recht:discretize}, source localization and separation~\cite{TASL14,SCOOP} and data driven CS problems e.g., in Magnetic Resonance Fingerprinting~\cite{MRF,BLIPsiam},  where typically a continuous manifold is discretized and approximated  by a large dictionary for e.g. sparse recovery tasks.

%into a finite precision and large dictionary which is eventually used for e.g. sparse recovery.
\subsection{Progressive Fixed Precision (PFP) approximate oracles}\label{sec:PFP}
One obtains a \emph{Progressive Fixed Precision} (PFP) approximate IPG by refining the FP type precisions thorough the course of iterations. Therefore any FP gradient or projection oracle which has control on tuning the accuracy parameter could be used in this setting and follows \eqref{eq:inIP} with decaying sequences $\nup^k,\nug^k$.
\subsubsection*{Examples}
For instance this case includes projection schemes that require iteratively solving an auxiliary optimization (e.g. the total variation ball \cite{TVprojGabirel}, sparse CUR factorization \cite{BachinexactIPG} or the multi-constraint inclusions~\cite{Dykstra,Dykstra2}, etc) and can progress (at an appropriate rate) on the accuracy of their solutions by adding more subiterations. We also discuss in Section~\ref{sec:covertree} another example 
%of the PFP type projection oracles 
of this form which is customized for fast approximate nearest neighbour searches with  
application to the data driven CS framework.

\subsection{$(1+\epsilon)$-approximate projection}\label{sec:epsproj}
Obtaining a fixed precision (and thus PFP) accuracy in  projections onto certain constraints might be still computationally exhaustive, whereas a notion of relative optimality 
%oracles with multiplicative errors are 
could be more efficient to implement. The $(1+\epsilon)$-approximate projection is defined as follows: for a given $\epsilon\geq0$,
\eql{\label{eq:eproj}
	\pp_\Cc^{\epsilon}(x) \in \Big\{ u\in \Cc :\,	\norm{u-x} \leq (1+\epsilon)\inf_{u'\in \Cc}\norm{u'-x}  \Big\}. 
}
We note that $\pp_\Cc^{\epsilon}(x)$ might not be unique. In this regard, the inexact IPG algorithm with a $(1+\epsilon)$-approximate projection takes the following form:
\eql{\label{eq:inIP2}
	x^k = \pp_\Cc^{\epsilon}\left(x^{k-1}
	%-\mu( \nabla f(x^{k-1}) + \eg^k)\right).
	-\mu \nablaa^{\nug^k} f(x^{k-1})\right).}
Note that we still assume a fixed precision gradient oracle with flexible accuracies $\nug^k$.  
%$\norm{\eg^k}\leq \nug^k$.  
One could also consider a $(1+\epsilon)$-approximate gradient oracle and in combination with those aforementioned inexact projections however for brevity and since the analysis would be quite  similar to the relative approximate projection we decide to skip more details on this case. % (one might be able to link this with d'aspremond works on subselecting row of $A$ for implementing cheaper statistical gradients).  
\subsubsection*{Examples}
The tree $s$-sparse projection in $\RR^n$ and in the exact form requires solving a dynamical programming with $O(ns)$ running time \cite{DRthompson} whereas solving this problem approximately with $1+\epsilon$ accuracy requires the time complexity $O(n\log(n))$ \cite{HegdeISIT} which better suits  imaging problems in practice with a typical Wavelet sparsity level $s=\Omega(\log(n))$. Also \cite{StoIHT,MatrixAlpsapprox} show that one can reduce the cost of low-rank matrix completion problem by using randomized linear algebra methods, e.g. see \cite{Deshpande2006,HalkoTropp}, and carry out fast low-rank factorizations with a $1+\epsilon$ type approximation. In a recent work on low-rank tensor CS~\cite{Holger:tensor} authors consider using a $1+\epsilon$ approximation (within an IPG algorithm) for low-rank tensor factorization since the exact problem is generally NP hard~\cite{NPtensor}. %\todo{can we say something here about submodular optimization [Krause] with greedy algos and their 1+e guaranty?}
Also in Section~\ref{sec:covertree} we discuss a data driven CS recovery algorithm which uses $1+\epsilon$ approximate nearest neighbour searches in order to break down the complexity of the projections from $O(d)$ (using an exhaustive search) to 
%$O(\log (d)+(1/\epsilon)^{O(1)})$, 
$O(\log (d))$, 
for  $d$ denoting a large number of data points with low intrinsic dimensionality.  

% into a log-linear order in term of $d$
%
%enables IPG to reduce the IPG cost log-linear in $d$, as 
%other example 
%%of the PFP type projection oracles 
%in this form that is customized for fast $1+\epsilon$ approximate nearest neighbour searches and its 
%application to the data driven CS framework.\todo{change this not identical to above}


%In a similar fashion the fixed precision (FP) approximate projection oracle is defined:

 

 
%It has been shown that under certain conditions and for a well chosen step size $\mu$ IPG converges to a solution of \eqref{eq:p1}.

