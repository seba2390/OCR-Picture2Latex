\section{Main results}
\label{sec:main}
\subsection{Uniform linear embeddings}
The success of CS paradigm heavily relies  on 
%the low (intrinsic) dimensionality of most natural signals which can be captured by an efficient model, as well as 
the embedding property of certain random sampling matrices which preserves signal information for low dimensional but often complicated/combinatorial models. It has been shown that IPG can stably predict the true signal $x^\gt$ from noisy CS measurements provided that $A$ satisfies the so called Restricted Isometry Property (RIP): 
\eql{\label{eq:RIP}(1-\theta)\norm{x-x'}^2 \leq \norm{A(x-x')}^2\leq (1+\theta)\norm{x-x'}^2, \quad \forall x,x' \in \Cc
}
for a small constant  $0<\theta<1$.
This has been shown for models such as sparse, low-rank and low-dimensional smooth manifold signals and by using 
IPG type reconstruction 
algorithms which in the nonconvex settings are also known as Iterative Hard Thresholding \cite{IHTCS, Ma2011, AIHT,MIP, modelbasedCS}. Interestingly these results indicate that under the RIP condition (and without any assumption on the initialization) the first order IPG algorithms with cheap local oracles can globally solve
nonconvex optimization problems. 

For instance random orthoprojectors and i.i.d. subgaussian matrices $A$ satisfy RIP when the number of measurements $m$ is proportional to the intrinsic dimension of the model (i.e. signal sparsity level, rank of a data matrix or the dimension of a smooth signal manifold, see e.g. \cite{RichCSreview} for a review on comparing different CS models and their measurement complexities) and sublinearly scales with the ambient dimension $n$. 

A more recent work generalizes the theory of IPG to arbitrary \emph{bi-Lipschitz embeddable} models \cite{Blumen}, that is for given $\Cc$ and $A$ it holds
\eq{\mm \norm{x-x'}^2\leq \norm{A(x-x')}^2\leq \MM \norm{x-x'}^2 \quad \forall x,x'\in \Cc.}
for some constants $\mm, \MM >0$. Similar to the RIP these constants are defined \emph{uniformly} over the constraint set i.e. $\forall x,x'\in \Cc$. There Blumensath shows that if \eq{\MM<1.5\mm,} then IPG robustly solves the corresponding noisy CS reconstruction problem \emph{for all} $x^\gt\in \Cc$. This result also relaxes the RIP requirement to a nonsymmetric and unnormalised notion of linear embedding whose implication in deriving sharper recovery bounds is previously studied by \cite{JaredJeff}. 

\subsection{Hybrid (local-uniform) linear embeddings}
Similarly the notion of restricted embedding plays a key role in our analysis. However we adopt a more local form of embedding and show that it is still able to guarantee stable CS reconstruction. 
{\ass{ \label{def:Lip}
		Given $(x_0\in \Cc, \Cc, A)$ there exists constants  $\MM,\mm_{x_0}>0$ for which the following inequalities hold:%\todo{$x_0$ or $x*$}
		%$A$ satisfies the following inequalities w.r.t. $\Cc$ and a point $x_0\in \Cc$:
		\begin{itemize}
			\item Uniform Upper Lipschitz Embedding (ULE)
			\begin{align*}
			\norm{A(x-x')}^2\leq \MM \norm{x-x'}^2 \quad \forall x,x'\in \Cc
			\end{align*}
			\item Local Lower Lipschitz Embedding (LLE)
			\begin{align*}
			\norm{A(x-x_0)}^2 \geq \mm_{x_0} \norm{x-x_0}^2 \quad \forall x\in \Cc
			\end{align*}
		\end{itemize}	
		Upon existence, $\MM$ and $\mm_{x_0}$ denote  respectively the smallest and largest constants for which the inequalities above hold.		
	}}
	\newline

This is a weaker assumption compared to RIP or the uniform bi-Lipschitz embedding. Note that for any $x_0\in \Cc$ we have: \eq{
\mm\leq\mm_{x_0}\leq\MM\leq \vertiii{A}^2
} 
(where $\vertiii{.}$ denotes the matrix spectral norm i.e. the largest singular value). 
However with such an assumption one has to sacrifice the \emph{universality} of the RIP-dependent results for a signal $x^\gt$ dependent analysis. Depending on the study, local analysis could be very useful to avoid e.g. worst-case scenarios that might unnecessarily restrict the recovery analysis~\cite{me:modelselecion}. Similar local assumptions in the convex settings are shown   to improve the measurement bound and the speed of convergence up to very sharp constants~\cite{recht:GW, Oymak:tradeoff}. 

Unfortunately we are currently unable to make the analysis fully local as we require the uniform ULE constraint. Nonetheless, one can always plug the stronger bi-Liptchitz assumption into our results throughout (i.e. replacing $\mmx$ with $\mm$) and regain the universality.  


%Our results on the exact and inexact FP approximate IPG improve the Blumensath's recovery condition in two folds: first by relaxing the uniform lower Lipschitz constant $\mm$ to a local form $\mmx$ which avoids worst-case scenarios that might unnecessarily restrict the recovery analysis (see Definition X), and second, by improving the factor in the Lipschitz embedding condition i.e. $\MM < 2\mmx$ for CS recovery (see e.g. Theorem 1). 






%In addition, if there exist a uniform constant $\mm>0$ such that 
%\eq{\mm\leq \mmx, \quad \forall x_0\in \Cc}
%then $A$ is a bi-Lipschitz embedding with constants $\mm,\MM$ i.e.
%\eq{\mm \norm{x-x'}^2\leq \norm{A(x-x')}^2\leq \MM \norm{x-x'}^2 \quad \forall x,x'\in \Cc.}
%Note that for any $x_0\in \Cc$ it holds \eq{\mm\leq\mm_{x_0}\leq\MM\leq \norm{A}^2.} 

%this assumption is weaker than  the so called Restricted Isometry Property (RIP) which guarantees performance of many CS reconstruction algorithms:
%{\defn{$A$ satisfies RIP w.r.t. a set $\Cc$ and a constant $0<\delta<1$, if $\forall x,x'\in \Cc$ it holds :
%		\eq{
%			(1-\delta)\norm{x-x'}^2\leq\norm{A(x-x')}^2\leq (1+\delta)\norm{x-x'}^2.
%			}
%}}
	
	
	
	

\subsection{Linear convergence of (P)FP inexact IPG for CS recovery}
In this section we show that IPG is robust against deterministic (worst case) errors. Moreover, we show that for certain decaying approximation errors, the IPG solution maintain  the same accuracy as for the approximation-free  algorithm. %In this part we assume neither $\ep^k$ or $\eg^k$ depend on $x^{k-1}$ or the previous updates.

{\thm{\label{th:inexactLS1} Assume $(x^\gt\in \Cc, \Cc,A)$ satisfy the main Lipschitz assumption  with constants $\MM< 2\mmx$. Set the step size $(2 \mmx )^{-1}<\mu\leq\MM^{-1}$. The sequence generated by Algorithm \eqref{eq:inIP} obeys the following bound:
		\eql{\label{eq:errbound}
			\norm{x^{k}-x^\gt}\leq  \rho^k \left(\norm{x^\gt}+\sum_{i=1}^k \rho^{-i} \nut^i \right)+ \frac{2\sqrt{\MM}}{\mmx(1-\rho)}w
		}
		where 
		\begin{align*}
		\rho=\sqrt{\frac{1}{\mu \mmx} -1} \qandq
		\nut^i=%\sum_{i=1}^k \rho^{-i}
		\frac{2\nug^i}{\mmx} + \frac{\nup^i}{\sqrt{\mu \mmx}},			
		\end{align*} 
		and $w=\norm{y-Ax^\gt}$.
	}} 

{\rem{Theorem \ref{th:inexactLS1} implications for the exact IPG (i.e. $\nup^k=\nug^k=0$) and inexact FP approximate IPG (i.e. $\nup^k=\nup,\nug^k=\nug, \forall k$)  improve  \cite[Theorem~2]{Blumen} in three ways: first by relaxing the uniform lower Lipschitz constant $\mm$ to a local form $\mmx\geq \mm$ with the possibility of conducting  a local recovery/convergence analysis. Second, by improving the embedding condition for CS stable recovery to 
\eql{\label{eq:cond}\MM < 2\mmx,
	} 
or $\MM < 2\mm$ for a uniform recovery $\forall x^\gt\in\Cc$. And third, by improving 
%(i.e. twice faster for the uniform case) 
the rate $\rho$ of convergence.}}

The following corollary is an immediate consequence of the linear convergence result established in Theorem~\ref{th:inexactLS1} for which we do not provide a proof:
{\cor{\label{cor:FP}With assumptions in Theorem \ref{th:inexactLS1} the IPG algorithm with FP approximate oracles achieves the solution accuracy
\eq{\norm{x^K-x^\gt}\leq 
		 	\frac{1}{1-\rho}\left(\frac{2\nug}{\mmx} + \frac{\nup}{\sqrt{\mu \mmx}}+ \frac{2\sqrt{\MM}}{\mmx}w\right) +\tau}
for any $\tau>0$ and in a finite number of iterations
\eq{
	K=\left\lceil\frac{1}{\log(\rho^{-1})} \log\left( \frac{ \norm{x^\gt}}{\tau}\right)\right\rceil
	}			
}}
As it turns out in our experiments and aligned with the result of Corollary~\ref{cor:FP}, the solution accuracy of IPG can not exceed the precision level introduced by a PF oracle. In this sense Corollary~\ref{cor:FP} is tight as a trivial converse example would be that IPG starts from the optimal solution $x^\gt$ but an adversarial FP scheme projects it to another point within a fixed distance. 

Interestingly one can deduce another implication from  Theorem~\ref{th:inexactLS1} and overcome such limitation by  using a PFP type oracle.
%Here comes 
%an interesting part of Theorem \ref{th:inexactLS1} analysing the behaviour of the PFP type oracles. 
Remarkably 
one achieves a linear convergence to a solution with the same accuracy as for the exact IPG, 
as long as $\nut^k$ geometrically decays. 	
The following corollary makes this statement explicit:
	
%{\cor{\label{cor:decay}Assume $\nut^k= O(r^k)$ for some error decay rate $0<r<1$ and a constant $C$. We have 
%			%After a finite number $k\geq K$ of iterations 
%			\eq{
%				\norm{x^{k}-x^*}\leq %\rho^k \norm{x^0-x'} +
%				\frac{2\sqrt{\MM}}{\mm_{x^\gt}(1-\rho)} w+ O({\bar \rho}^k), 
%			}
%			where $\rho$ is the same as in Theorem \ref{th:inexactLS1}, and
%			\begin{align*}
%			%&x^\rsc_{\min}\in \argmin_{x^\rsc\in \Omega}\norm{\nabla f(x^\rsc)},\\
%			%&K=\left\lceil\log\left( \frac{2\norm{\nabla f(x^\rsc_{\min})}}{m(1-\rho) \norm{x^\rsc_{\min}}}\right)/\log(\bar \rho)\right\rceil,\\
%			\bar \rho = \choice{\max(\rho,r)\quad r\neq\rho \\
%				r+\xi\qquad\quad r=\rho}
%			\end{align*}
%			for an arbitrary small $\xi>0$.	
%}
%}
	
{\cor{\label{cor:decay}Assume $\nut^k\leq Cr^k$ for some error decay rate $0<r<1$ and a constant $C$. Under the assumptions of Theorem~\ref{th:inexactLS1} the solution updates $\norm{x^{k}-x^*}$ of the IPG algorithm with PFP approximate oracles is bounded above by:
		%After a finite number $k\geq K$ of iterations 
		\begin{align*}
			%\norm{x^{k}-x^*}\leq 
			&\max(\rho,r)^k \left(\norm{x^\gt}+\frac{C}{1-\frac{\min(\rho,r)}{\max(\rho,r)}}\right) +
			\frac{2\sqrt{\MM}}{\mm_{x^\gt}(1-\rho)} w,  &r\neq \rho \\
			&\rho^k \Big(\norm{x^\gt}+Ck\Big) +
			\frac{2\sqrt{\MM}}{\mm_{x^\gt}(1-\rho)} w, &r=\rho 
		\end{align*}
		Which implies a linear convergence at rate 
%		$\bar \rho$:
%		\eq{
%		\norm{x^{k}-x^*}\leq %\rho^k \norm{x^0-x'} +
%		\frac{2\sqrt{\MM}}{\mm_{x^\gt}(1-\rho)} w+ O({\bar \rho}^k), 
%		}
%		where,
		\begin{align*}
		%&x^\rsc_{\min}\in \argmin_{x^\rsc\in \Omega}\norm{\nabla f(x^\rsc)},\\
		%&K=\left\lceil\log\left( \frac{2\norm{\nabla f(x^\rsc_{\min})}}{m(1-\rho) \norm{x^\rsc_{\min}}}\right)/\log(\bar \rho)\right\rceil,\\
		\bar \rho = \choice{\max(\rho,r)\quad r\neq\rho \\
			\rho+\xi\qquad\quad r=\rho}
		\end{align*}
		for an arbitrary small $\xi>0$.	
	}
}
{\rem{Similar to Corollary \ref{cor:FP} 
%and due to the linear convergence 
one can increase the final solution precision of the FPF type  IPG with logarithmically more iterations i.e. in a finite number $K=O(\log(\tau^{-1}))$ of iterations one achieves $\norm{x^{K}-x^*}\leq O(w)+\tau$. Therefore in contrast with the FP oracles one achieves an accuracy within the noise level $O(w)$ that is the precision of an approximation-free IPG.
}}

{\rem{Using the PFP type oracles can also maintain the rate of linear convergence identical as for the exact IPG. For this the approximation errors suffice to follow a geometric decaying rate of  $r<\rho$. % \todo{discuss suitable oracle complexity within PFP.}
}}

{\rem{The embedding condition~\eqref{eq:cond} sufficient to guarantee our stability results is invariant to the  precisions of the FP/PFP oracles and it is the same as for an exact IPG.}}

\subsection{Linear convergence of inexact IPG with $(1+\epsilon)$-approximate projection for CS recovery}
\label{sec:relative}
In this part we focus on the inexact  algorithm~\eqref{eq:inIP2} with a $(1+\epsilon)$-approximate projection. As it turns out by the following theorem we require a stronger embedding condition to guarantee the CS stability compared to the previous algorithms.
%the exact or the FP/PFP type inexact IPG. 


{\thm{\label{th:inexactLS2}  Assume $(x^\gt\in \Cc, \Cc,A)$ satisfy the main Lipschitz assumption and that
		\eq{\sqrt{2\epsilon+\epsilon^2}\leq \delta\frac{\sqrt{\mmx}}{\vertiii{A}} \qandq \MM < (2-2\delta+\delta^2) \mmx} 	
		for $\epsilon\geq 0$ and some constant $\delta \in [0,1)$.
		Set the step size $\left((2-2\delta+\delta^2) \mmx\right)^{-1}<\mu\leq\MM^{-1}$. The sequence generated by Algorithm \eqref{eq:inIP2} obeys the following bound:
		\eq{
			\norm{x^{k}-x^\gt}\leq  \rho^k \left(\norm{x^\gt}+\kappa_g \sum_{i=1}^k \rho^{-i} \nug^i \right)+ 
			%\frac{ \left( 2\frac{\sqrt{M}}{m}+\frac{\sqrt\mu}{\norm{A}}\delta \right)}
			\frac{\kappa_w}{1-\rho}w
		}
		where 
		\begin{align*}
		&\rho=\sqrt{\frac{1}{\mu \mmx} -1}+ \delta, \quad
		\kappa_g = \frac{2}{\mmx}+\frac{\sqrt\mu}{\vertiii{A}}\delta, \\		
		&\kappa_w= 2\frac{\sqrt{\MM}}{\mmx}+\sqrt\mu\delta, \qandq w=\norm{y-Ax^\gt}.
		\end{align*}		
		
	}} 	
{\rem{Similar conclusions follow as in Corollaries~\ref{cor:FP} and \ref{cor:decay} on the linear convergence, logarithmic number of iterations vs. final level of accuracy (depending whether the gradient oracle is exact or FP/PFP) however with a stronger requirement than \eqref{eq:cond} on the embedding; increasing $\epsilon$ i.e. consequently $\delta$, limits the recovery guarantee and slows down the convergence (compare $\rho$ in Theorems~\ref{th:inexactLS1} and \ref{th:inexactLS2}). Also approximations of this type result in amplifying distortions (i.e. constants $\kappa_w, \kappa_g$) due to the measurement noise and gradient errors. For example for an exact or a geometrically decaying PFP gradient updates and for a fixed $\epsilon$ chosen according to the Theorem~\ref{th:inexactLS2}  assumptions, Algorithm~\eqref{eq:inIP2} achieves a full precision accuracy $\norm{x^{K}-x^*}\leq O(w)+\tau$ (similar to the exact IPG) in a finite number $K=O(\log(\tau^{-1}))$ of iterations.
		}}



%(i.e. the LLE condition is not sufficient and we need to replace it with a uniform lower Lipschitz constant $\mm>0$ which holds $\forall x_0\in\Cc$). With such formalism one can update the noise term  in both (theorems) bounds i.e. $w:=\norm{y-Ax^{opt}}\leq \norm{y-Ax^\gt}+\norm{A(x^\gt-x^{opt})} $.


%One can also 
%incorporate throughout the flexibility of considering an approximate projection onto a possibly larger set $\Cc$ including the original signal model $\Cc'$ i.e. $x^\gt \in \Cc'\subseteq \Cc$ , which for instance finds practical implications in tree-sparse signal or lowrank matrix CS recovery, see \cite{HegdeISIT,MatrixAlpsapprox}. Such a distinction  modifies our earlier definitions \eqref{eq:proj1} and \eqref{eq:eproj}; an approximate FP projection reads, 
%\begin{align}
%\pp_\Cc^{\nup}(x) \in \Big\{ u\in \Cc :\,	\norm{u-x}^2 \leq \inf_{u'\in \Cc'}\norm{u'-x}^2 +\nup  \Big\},\label{eq:proj1-1}
%\end{align} 
%and a $(1+\epsilon)$-approximate projection is defined as
%\eql{\label{eq:eproj-1}
%	\pp_\Cc^{\epsilon}(x) \in \Big\{ u\in \Cc :\,	\norm{u-x} \leq (1+\epsilon)\inf_{u'\in \Cc'}\norm{u'-x}  \Big\}. 
%}
% Note that with respect to our distinction Theorems~\ref{th:inexactLS1} and \ref{th:inexactLS2} still hold.
% between the projection set $C$ and the original signal model $C'\subseteq C$ defined in \eqref{eq:proj1} and \eqref{eq:eproj}, our embedding assumption is on the projection set.

{\rem{\label{rem:stringe}The assumptions of Theorem~\ref{th:inexactLS2} impose a stringent requirement on the scaling of the approximation parameter i.e. $\epsilon=O\left(\sqrt{\frac{\mmx}{\vertiii{A}}}\right)$ which is not purely dependent on the model-restricted embedding condition but also on the spectral norm  $\vertiii{A}$. In this sense since  $\vertiii{A}$ ignores the structure $\Cc$ of the problem it might scale very differently than  the corresponding embedding constants $\mmx,\mm$ and $\MM$. For instance a $m\times n$  i.i.d. Gaussian matrix has w.h.p. $\vertiii{A}=\Theta(n)$ (when $m\ll n$) whereas, e.g. for  sparse signals, the embedding constants $\mm,\MM$ w.h.p. scale as $O(m)$. A similar gap exists for other low dimensional signal models and for other compressed sensing matrices e.g. random orthoprojectors. 
This indicates that the $1+\epsilon$ oracles may be
sensitive to the CS sampling ratio i.e. for $m\ll n $
we may be limited to use very small approximations  $\epsilon = O(\sqrt{ \frac{m}{n}} )$. 

%which indicates that the $1+\epsilon$ oracles are also sensitive to the CS sampling ratio i.e. for $m\ll n$ where $\sqrt{\frac{\mmx}{\vertiii{A}}}\approx \sqrt{\frac{m}{n}}$ becomes very small one should use proportionally small  approximations $\epsilon$. 

In the following we show by a deterministic example that this requirement is indeed tight. We also empirically observe in Section \ref{sec:expe} that such a limitation indeed holds in randomized settings (e.g. i.i.d. Gaussian $A$) and on average. 
Although it would be desirable to modify the IPG algorithm to avoid such restriction, as was done in~\cite{Hegde15} for specific structured sparse models, we note that this is the same term that appears due to 'noise folding' when the signal model is not exact or when there is noise in the signal domain (see the discussion in Section~\ref{sec:inexactmodel}). As such most practical CS systems will inevitably have to avoid regimes of extreme undersampling.
}}

%which is unfavourable for applying large approximation of this type to the extreme compressed sensing settings where $\sqrt{\frac{\mmx}{\vertiii{A}}}\approx \sqrt{\frac{m}{n}}\rightarrow 0$. \marginpar{\todo{rewrite it positively + relation to noise folding}}
%}}



\input{sections/aconverse.tex}
\subsection{When the projection is not onto the signal model}\label{sec:inexactmodel}

 One can also make a distinction between  the projection set $\Cc$ and the signal model here denoted as $\Cc'$ (i.e. $x^\gt\in\Cc'$) by modifying our earlier definitions \eqref{eq:proj1} and \eqref{eq:eproj} in the following ways:
an approximate FP projection reads, 
\begin{align}
\pp_\Cc^{\nup}(x) \in \Big\{ u\in \Cc :\,	\norm{u-x}^2 \leq \inf_{u'\in \Cc'}\norm{u'-x}^2 +\nup^2  \Big\},\label{eq:proj1-1}
\end{align} 
and a $(1+\epsilon)$-approximate projection reads
\eql{\label{eq:eproj-1}
	\pp_\Cc^{\epsilon}(x) \in \Big\{ u\in \Cc :\,	\norm{u-x} \leq (1+\epsilon)\inf_{u'\in \Cc'}\norm{u'-x}  \Big\}. 
}
With respect to such a distinction,  Theorems~\ref{th:inexactLS1} and \ref{th:inexactLS2} still hold (with the same embedding assumption/constants on the projection set $\Cc$), conditioned that $x^\gt\in \Cc$.
Indeed this can be verified by following identical steps as in the proof of both theorems.
This allows throughout the flexibility of considering an approximate projection onto a possibly larger set $\Cc$ including the original signal model $\Cc'$ i.e. $x^\gt \in \Cc'\subseteq \Cc$, which for instance finds application in fast tree-sparse signal or low-rank matrix CS recovery, see \cite{HegdeISIT,MatrixAlpsapprox}. Such an inclusion is also important to derive a uniform recovery result \emph{for all} $x^*\in\Cc'$. 

The case where $x^\gt \notin \Cc$ can also be bounded in a similar fashion as in \cite{Blumen}. We first consider a proximity point in $\Cc$ i.e. $x^{o}:=\argmin_{u\in\Cc} \norm{x^\gt-u}$  and update the noise term  to $w:=\norm{y-Ax^{o}}\leq \norm{y-Ax^\gt}+\norm{A(x^\gt-x^{o})} $. We then use Theorems~\ref{th:inexactLS1} and \ref{th:inexactLS2} to derive an error bound, here on $\norm{x^k-x^{o}}$. For this we assume the embedding condition \emph{uniformly} holds over the projection set $\Cc$ (which includes $x^{o}$). As a result we get a bound on the error $\norm{x^k-x^\gt} \leq \norm{x^\gt-x^{o}}+\norm{x^k-x^{o}}$ which includes a bias term with respect to the distance of $x^\gt$ to $\Cc$. Note that since here $w$ also includes a signal (and not only measurement) noise term introduced by  $\norm{A(x^\gt-x^{o})}$, the results are subjected to \emph{noise folding} i.e. a noise amplification 
%$\sim O(\sqrt{\frac{n}{m}})$ 
with a similar unfavourable scaling (when $m \ll n$) to our discussion in Remark~\ref{rem:stringe} (for more details on CS noise folding see e.g.~\cite{eldar:noisefolding,daven:noisefolding}). 


