\section{Application in Data driven compressed sensing}
\label{sec:datadrivenCS}
Many CS reconstruction programs resort to signal models promoted by certain (semi) algebraic functions $h(x):\RR^n\rightarrow \RR_+\cup\{ 0\}$. For example we can have
\eq{\Cc := \big\{x\in \RR^n : h(x)\leq \zeta\big\},
	}
where $h(x)$ may be chosen as the $\ell_{0}$ or $\ell_{0-2}$ semi-norms or as the $\rank(x)$  which promotes sparse, group-sparse or low-rank (for matrix spaces) solutions, respectively. One might also replace those penalties with their corresponding  %algebraic and 
convex relaxations namely, the $\ell_1$, $\ell_{1-2}$ norms or the nuclear norm. 
%For $R_i(x),\tau_i$ and $i=1,...,b$, one can define a $b$-dimensional linear model. Also the constraint $R(x)\leq \tau$ may correspond to a parametric manifold. In this case the projection is often algebraic.  
%\todo{ask mike for this part}

\emph{Data driven} compressed sensing however corresponds to cases where in the absence of an algebraic physical model one resorts to collecting a large number of data samples in a dictionary and use it as a \emph{point cloud} model for CS reconstruction~\cite{RichCSreview}.  
%In this case the model is $\Cc = \bigcup_{i=1}^{d}\{\psi_i\}$ where $\psi_i\in\RR^n$s are atoms of a $n\times d$ dictionary $\Psi$. %\{\psi_i\}_{i=1}^{d}$
% of $d$ points (atoms) in $\RR^n$ the model becomes.  
Data driven CS finds numerous applications e.g. in Hyperspectral imagery \cite{TIPHSI}, Mass spectroscopy (MALDI imaging) \cite{Kobarg2014}, Raman Imaging~\cite{ramanCS} and Magnetic Resonance Fingerprinting (MRF)~\cite{MRF,BLIPsiam} just to name a few. For instance the USGS Hyperspectral library \footnote{\url{http://speclab.cr.usgs.gov}} contains the spectral signatures (reflectance) of thousands of substances measured across a few hundred frequency bands. This side information is shown to be useful for CS reconstruction and classification in both convex and nonconvex settings (see e.g. \cite{TIPHSI} for more details and relations to sparse approximation in redundant dictionaries).  
%Recently MR Fingerprinting~\cite{MRF} proposed a fast CS acquisition scheme for quantifying the NMR properties (such as the $T1,T2$ relaxation times, off-resonance frequencies, proton density) of tissues. Since random excitation sequences generate magnetization responses in non-analytic forms, \cite{MRF} proposed build a very large dictionary of fingerprints driven by the Bloch dynamic equations for all combination of those parameters. 
Data driven CS may also apply to algebraic models with non trivial projections. For example in the MRF reconstruction problem one first constructs a huge  dictionary of fingerprints i.e. the magnetization responses (across the readout times) for many $T1,T2$ relaxation values (i.e. spin-spin and spin-echo) presented in normal tissues~\cite{MRF}. This corresponds to sampling a two-dimensional manifold associated with the solutions of the \emph{Bloch dynamic  equations}~\cite{BLIPsiam}, which in the MRF settings neither the response nor the projection has an analytic closed-form solution.
%the set $\Cc$ may also be a dense collection of samples from a continuous algebraic manifold with a non trivial projection e.g. Magnetic resonance fingerprinting \nref. 
% As the projection step might not be trivial or computationally easy for many complex manifolds we consider a data driven approach which consists of collecting discrete samples of the manifold in a dictionary and approximate 


 

%Recently MR Fingerprinting \cite{MRF} proposed a fast CS acquisition scheme for quantifying the NMR properties (namely the $T1,T2$ relaxation times) of tissues. Small number of  excitations in form of rotating the magnetic field applies to the tissue, and between each two excitations, the \emph{partial} k-space information is measured. An iterative exact projection algorithm (BLIP) is proposed for the MRF problem which achieves a great parameter estimation accuracy \cite{BLIPsiam}. % versus compression trade-off $\nref$.
%The projection consists of NN searches on a densely sampled manifold $\Mb$ of fingerprints driven by the Bloch dynamic equations.
\subsection{A data driven CS in product space}
To explore how our theoretical results can be used to accelerate CS reconstruction we consider a stylized data driven application for which we explain how one can obtain each of the aforementioned approximate projections. Consider a multi-dimensional image such as HSI, MALDI or MRF that can be represented by a $\n\times J$ matrix $X$, where $n=\n J$ is the total number of spatio-spectral pixels, $J$ is the spatial resolution and $\n$ is the number of spectral bands 
e.g. $\n=3$ for an RGB image, $\n\approx 400$ for an HSI acquired by NASA's AVIRIS spectrometer, $\n\approx 5000$ for a MALDI image~\cite{Kobarg2014}.
 In the simplest form we assume that 
%e.g., when the acquisition resolution is high enough so that the spectral contents do not merge spatially, one has pixel purity and thus 
each spatial pixel corresponds to a certain material with a specific signature, i.e. 
\eq{
	X_{j}\in \widetilde \Cc, \quad \forall j=1,\ldots,J,
} 
where $X_j$ denotes the $j$th column of $X$ and 
\eq{
	\widetilde \Cc:=\bigcup_{i=1}^{d}\{\psi_i\} \in \RR^J}
is the point cloud of a large number $d$ of signatures $\psi_i$ e.g. in a customized spectral library for HSI or MALDI data.

The CS sampling model follows \eqref{eq:CSsampling} by setting $x^*:=X_\text{vec}$, where by $X_\text{vec}\in \RR^n$ we denote the vector-rearranged form of the matrix $X$.
%\eq{y\approx AX^*(:),}  
%where in this set up the number of A's columns is $n=dN$. 
The CS reconstruction reads
\eql{\label{eq:datadrivenCS}
	\min_{\substack{X_j\in \widetilde\Cc,\\ \forall j=1,...,J}} \big\{f(x):= \frac{1}{2}\norm{y-AX_\text{vec}}^2\big\}.
}
or equivalently and similar to problem \eqref{eq:p1} 
\eq{
	\min_{x\in \prod_{j=1}^J \widetilde\Cc} \big\{f(x):= \frac{1}{2}\norm{y-Ax}^2\big\}.
}
The only update w.r.t. problem \eqref{eq:p1} 
%are the dimensionality of $A,x$ and 
is the fact that now the solution(s) $x$ lives in a product space of the same model i.e. 
\eql{\label{eq:prod}\Cc:=\prod_{j=1}^J \widetilde\Cc
} 
(see also~\cite{kronCS} on product/Kronecker space CS however using a sparsity inducing semi-algebraic model). We note that solving directly this problem for a general $A$ (e.g. sampling models which non trivially combine columns/spatial pixels of $X$)  is \emph{exponentially hard} $O(d^J)$ because of the combinatorial nature of the product space constraints. In this regard, a tractable scheme which has been frequently considered for this problem e.g. in \cite{BLIPsiam} would be the application of an IPG type algorithm 
%(or its proximal-gradient equivalences e.g. studied in \cite{TIPHSI,BLIPsiam}) 
in order to break down the cost into the gradient and projection computations (here the projection requires $O(Jd)$ computations to search the closest signatures  to the current solution) to locally solve the problem at each iteration. 
%where at each iteration one requires to perform $O(Jd)$ computations to locally solve/update the problem. 


%However this linear complexity in $d$ can still be a serious bottleneck for solving problems with very large size datasets. We address this issue by replacing the exhaustive search with fast approximate searches.


\subsection{Measurement bound}
The classic Johnson-Lindenstrauss lemma says that one can use random linear transforms to stably embed point clouds into a lower dimension of size $O(\log(\#\widetilde \Cc))$ where $\#$ stands for the set cardinality~\cite{JL}.
{\thm{\label{th:JL} Let $\widetilde \Cc$ be a finite set of points in $\RR^{\n}$. For $A$ drawn at random from the i.i.d. normal distribution and a positive constant $\widetilde \theta$, with very high probability one has
		\eq{(1-\widetilde \theta)\norm{(x-x')} \leq \norm{A(x-x')}\leq (1+\widetilde \theta)\norm{(x-x')}, \quad \forall x,x' \in \widetilde \Cc
			}
			provided $m=O(\log(\#\widetilde \Cc)/\widetilde \theta^2)$.
		}}
		
Note that this definition implies an RIP embedding for the point cloud model according to \eqref{eq:RIP} with a constant $\theta<3\widetilde \theta$ which in turn (and for small enough $\widetilde \theta$) implies the sufficient embedding condition for a stable CS recovery using the exact or approximate IPG. % (see for more details Section~\ref{sec:main}). 
This bound considers an arbitrary point cloud and could be improved when data points $\widetilde \Cc\subseteq \Mm$ are derived from a low-dimensional structure, e.g. a smooth manifold $\Mm$ such as the MR Fingerprints, for which one can alternatively use $m=\dim_{E}(\Mm)$ for the corresponding RIP type \emph{embedding dimension} listed e.g. in \cite{RichCSreview}.

%$m \propto \dim_{E}(\widetilde \Cc)\leq \n$ where $\dim_{E}(.)$ denotes the embedding dimension to satisfy RIP type embedding \eqref{eq:RIP}.

% e.g. points that are living on a smooth manifold 
%A more recent result \cite{Indyk:NNembedding} also considers the low dimensional structures within data points and weakened the measurement requirement to $m \propto \dim(\widetilde \Cc)\leq \n$ under certain conditions and notions of data intrinsic dimensionality. \todo{add embedding dims papers eg richs overview}

%This $\theta$-stable embedding has similarities with the bi-Liptchitz embedding that we discussed in section x that 
%
%defined above, also known as the Restricted Isometry Property (RIP), is the key to guaranty the success of many CS recovery programs. For certain infinite models such as sparse (or in general the union of linear models), low-rank or smooth manifold signals the measurement bound of JL lemma can be further milden to $m \propto \dim(\Cc)$, where $\dim$ denotes the intrinsic dimensionality of the corresponding model e.g. sparsity level, rank of a matrix or manifold (intrinsic) dimension \nref [classiCS]. 
%In relation to Definition \eqref{def:Lip}, we note that RIP is a uniform (stronger) guaranty whereas ours is local on RCS i.e.  we have $1-3\delta\leq \mmx$ and w.r.t. RLS it holds $L=1+3\delta$.
		
We note that such a measurement bound for a product space model  \eqref{eq:prod}  without considering any structure between spaces turns into 
\eq{
	m=O\left(J\min\left\{\log(d), \dim_E(\Mm)\right\}\right).
	}



