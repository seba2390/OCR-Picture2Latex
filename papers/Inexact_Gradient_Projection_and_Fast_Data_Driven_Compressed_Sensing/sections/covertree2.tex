\subsection{Cover tree for fast nearest neighbour search}
\label{sec:covertree}
With the data driven CS formalism and discretization of the model  %$\Cc = \prod_{i=1}^N \Mb$, 
the projection step of IPG reduces to searching for the nearest signature in each of the product spaces, however in a potentially very large $d$ size dictionary. %Therefore the linear complexity of an exact projection in $d$ can still be a bottneck  an extract IPG %which brings a serious bottleneck to searches with a linear complexity in term of $d$.  
And thus search strategies with linear complexity in $d$ e.g. an exhaustive search, can be a serious bottleneck for solving such problems. %We address this issue by replacing the exhaustive search with fast approximate searches. 
A very well-established approach to overcome the complexity of an exhaustive nearest neighbour (NN) search on a large dataset consists of hierarchically partitioning the solution space and forming a \emph{tree} whose nodes represents those partitions, and then using branch-and-bound methods on the resulting tree for a fast Approximate NN (ANN) search with $o(d)$ complexity e.g. see~\cite{Navigating,beygelzimer2006cover}.

In this regard, we address the computational shortcoming of the projection step in the exact IPG by preprocessing $\widetilde \Cc$ and form a \emph{cover tree} structure suitable for fast ANN searches \cite{beygelzimer2006cover}. 
A cover tree is a levelled tree whose nodes at different scales form covering nets for data points at multiple resolutions; 
%(i.e., coarse-to-fine dyadic coverage levels). 
if $\sigma:=\max_{\psi\in\widetilde \Cc}\norm{\psi_\text{root}-\psi}$ corresponds to the maximal coverage by the root, then  
nodes appearing at any finer scale $l>0$ form a $(\sigma 2^{-l})$-covering net for their descendants i.e. as we descend down the tree the covering resolution refines in a dyadic coarse-to-fine fashion.

 %Two approximate search strategies can be adopted with respect to the cover tree structure:
 We consider three possible search strategies 
 %(one exact and two approximate) 
 using such a tree structure:
 \begin{itemize}
 	\item \textbf{Exact NN:} which is based on the branch-and-bound algorithm proposed in \cite[Section 3.2]{beygelzimer2006cover}.	Note that we should distinguish between this strategy and performing a brute force search. Although they both perform an exact NN search, the complexity of the proposed algorithm in~\cite{beygelzimer2006cover} is shown to be way less in practical datasets.% (thanks to pre-processing a cover tree structure).
 	\item \textbf{$(1+\epsilon)$-ANN:} this search is also based on the branch-and-bound algorithm proposed in \cite[Section 3.2]{beygelzimer2006cover} which has includes an early search termination criteria (depending on the accuracy level $\epsilon$) 
 	%	that if for a parameter $\gamma=\epsilon$ the search stops as soon as having
 	%	\eq{(1+\frac{1}{\epsilon-1})\sigma2^{-i+1}\leq \argmin_{q\in Q_i} \norm{p-q}
 	%	}
 	%	then 
 	for which one obtains an approximate  oracle of type defined by \eqref{eq:eproj}. %that could be used for solving data driven CS problems \eqref{eq:datadrivenCS} with an inexact IPG.  
 	Note that the case $\epsilon=0$ refers to the exact tree NN search described above. %and performing a brute-force search. Although they both perform an exact NN search, the complexity of the proposed algorithm in~\cite{beygelzimer2006cover} is shown to be way less in practical datasets.
 	
 	\item \textbf{FP-ANN:} that is traversing down the tree up to a scale $l=\lceil \log(\frac{\nup}{\sigma})\rceil$ for which the covering resolution falls below a threshold $\nup$ on the search accuracy. This search results in a fixed precision type approximate oracle as described in Section~\ref{sec:FP} and in a sense it is similar to performing the former search with $\epsilon=0$, however on a truncated (low-resolution) cover tree. 
 	%	 Given the knowledge of $maxdist$ (or the bound in Lemma \ref{lem:maxdist}) we stop the search when
 	%	%the stoppage can be refined to 
 	%	\eq{ \max_{q\in Q_i} \{maxdist(q)\} < \gamma. }
 	%	This is similar as performing exact NN searches on a truncated (low-resolution) cover tree. 
 	%	\item \textbf{PFP ANN decaying at rate $r$:} as defined in Corollary \ref{cor:decay}. For this case we perform the stoppage procedure above for an updating accuracy parameter 
 	%	\eq{ 
 	%		\gamma := \nup^k =r^k
 	%	}
 	%	that geometrically decays at a certain rate $r<1$ through the iterations.
 	
 \end{itemize}
 All strategies could be applied to accelerate the projection step of an exact or inexact IPG (with variations discussed in Sections~\ref{sec:epsproj} and \ref{sec:FP}) to tackle the data driven CS problem~\eqref{eq:datadrivenCS}.  
 %with the inexact IPG variations discussed in Sections~\ref{sec:epsproj} and \ref{sec:FP}.  
 In addition one can iteratively refine the accuracy of the FP-ANN search (e.g. $\nup^k =r^k$ for a certain decay rate $r<1$ and IPG iteration number $k$) and obtain a PFP type approximate IPG discussed in Section~\ref{sec:PFP}.  
 
Note that while the cover tree construction is blind to the explicit structure of the data, several key growth properties such as the tree's explicit depth, the number of children per node, and importantly the overall search complexity are characterized by the intrinsic dimension of the model, called the \emph{doubling dimension},
% i.e.  a notion which is referred to as the \emph{doubling constant} or \emph{doubling dimension}
and defined as follows~\cite{assouad,heinonen}:

{\defn{\label{def:doub} Let $B(q,r)$ denotes a ball of radius $r$ centred at a point $q$ in some metric space. The doubling dimension $\dim_D(\Mm)$ of a set $\Mm$ is the smallest integer such that every ball of $\Mm$ (i.e. $\forall r>0$, $\forall q\in\Mm$, $B(q,2r)\cap \Mm$) can be covered by $2^{\dim_D(\Mm)}$  balls of half radius i.e. $B(q',r)\cap \Mm$, $q'\in \Mm$. %Where $B(q,r)$ denotes a ball of radius $r$ centred at $q$ in some metric space. 
		}}
		
The doubling dimension has several 
appealing properties e.g. $\dim_D(\RR^n)=\Theta(n)$, $\dim_D(\Mm_1)\leq \dim_D(\Mm_2)$ when $\Mm_1$ is a subspace of $\Mm_2$, and $\dim(\cup_{i=1}^I \Mm_i)\leq \max_i \dim_D(\Mm_i)+\log(I)$~\cite{heinonen,Navigating}. 
Practical datasets are often assumed to have small doubling dimensions e.g. when $\widetilde \Cc \subseteq \Mm$  %uniformly
 samples a low $K$-dimensional manifold $\Mm$ with certain smoothness and regularity one has $\dim_D(\widetilde \Cc)\leq \dim_D(\Mm)=O(K)$ \cite{dasgupta2008}.\footnote{Although the two notions of embedding $\dim_E$ and doubling $\dim_D$ dimensions scale similarly for certain sets e.g. linear subspaces, UoS, smooth manifolds..., this does not generally hold in $\RR^n$ and one needs to distinguish between them,  
%these two notions are different 
see for more discussions~\cite{Indyk:NNembedding,dasgupta2012}.}

Equipped with such a notion of dimensionality, the following theorem bounds the complexity of a  $(1+\epsilon)$-ANN cover tree search~\cite{Navigating,beygelzimer2006cover}: 

{\thm{\label{thm:NNcomp2}Given a query which might not belong to $\widetilde \Cc$, the approximate $(1+\epsilon)$-ANN search on a cover tree takes at most 
		\eql{
			2^{O(\dim_D(\widetilde \Cc))}\log \Delta+(1/\epsilon)^{O(\dim_D(\widetilde \Cc))} 
		}
		computations in time with $O(\#\widetilde \Cc)$ memory requirement, where  $\Delta$ is the aspect ratio of $\widetilde \Cc$.}	
}

For most applications $\log (\Delta) = O(\log(d))$~\cite{Navigating} and thus for datasets with low dimensional structures i.e. $\dim_D=O(1)$ and by using  moderate approximations one achieves a logarithmic search complexity in $d$, as opposed to the linear complexity of a brute force search.

Note that the complexity of an \emph{exact} cover tree search could be arbitrarily high 
%i.e. linear in $d$, at least in theory (unless the query belongs to the dataset~\cite{beygelzimer2006cover} which does not generally apply e.g. for the intermediate steps of the IPG). 
and thus the same applies to the FP and PFP type ANN searches since they are also based on performing exact NN (on a truncated tree). However in the next section we empirically observe  that the complexity of an exact cover tree NN (and also the FP and PFP type ANN) is much lower than performing an exhaustive search.

%(i.e. by a notion which is referred to as the \emph{doubling constant}~\cite{beygelzimer2006cover,Navigating,Indyk:NNembedding})  
%which in practical datasets is often a small number e.g. when $\widetilde \Cc$ uniformly samples of low dimensional manifolds.


%This structure hierarchically partitions the metric space and enables using branch-and-bound methods for fast NN search. 
%We denote by $\widetilde \Cc_i$ the set of nodes appear at scale $i=0,\dots,L_{\max}$.







%
%
%Let $\Tt_\Mb$ denotes a cover tree defined in order to arrange dataset $\Mb$.  %Cover tree is a leveled tree with multiple scales $i=0,\dots,L_{\max}$ where its nodes are associated with data points in  $\Mb$. 
%We denote by $\widetilde \Cc_i$ the set of nodes appear at scale $i=0,\dots,L_{\max}$.
%The lowest scale $i=0$ correspond to the root $\widetilde \Cc_0$ which is a point covering all data. We denote by $\sigma:=\max_{q\in\Mb}\dist(\widetilde \Cc_0,q)$ the maximum coverage i.e. the maximum distance between the root and any other node on the tree. As we descend down the tree (i.e. incrementing the scale) the nodes present at higher scales cover their descendants at finer resolutions. More precisely, a cover tree structure must have the following  three properties:
%% node $i$ denoted by $\Nn_{ji}$ represents a section of the space denoted by$\widetilde \Cc_{ji}$. Nodes and represented partitions must satisfy the following three properties.
%\begin{enumerate}
%	\item Nesting: $\widetilde \Cc_i \subseteq \widetilde \Cc_{i+1}$, 
%	that is once a point $p$ appears as a node in $\widetilde \Cc_i$, then every lower level in the tree has a node associated with $p$.
%	\item Covering: every node $q\in \widetilde \Cc_{i+1}$ has a parent node  $p\in \widetilde \Cc_{i}$, where $\dist(p,q)\leq \sigma2^{-i} $. As a result, covering becomes finer at higher scales in a dyadic fashion. 
%	%and For every $p\in \widetilde \Cc_{i−1}$, there exists
%	%a q 2 Ci such that d(p, q) < 2i and the node in
%	%level i associated with q is a parent of the node
%	%in level i − 1 associated with p.
%	
%	\item Separation: nodes belonging to the same scale are separated by a minimal distance which dyadically shrinks at higher scales i.e. $\forall q,q'\in\widetilde \Cc_i$ we have $\dist(q,q')>\sigma2^{-i}$.   
%\end{enumerate}  
%Each node $p$ also keeps the maximum distance to its descendants denoted by
%\eq{maxdist(q):= \max_{q'\in \text{descendant}(q)} \dist(q,q'),
%}
%which will be useful for the fast NN search.  Note that one might decide to avoid saving $maxdist$ values and use the following upper bound instead.
%
%
%{\lem{\label{lem:maxdist}For any $q\in \widetilde \Cc_i$ and due to the covering property we have:
%\begin{align}\label{eq:maxdist}
%maxdist(q) \leq \sigma \left( 2^{-i}+2^{-i-1}	+ 2^{-i-2}+\dots \right)
%< \sigma 2^{-i+1}.
%\end{align}
%}}
%
%Depth of the \emph{implicit} cover tree (constructed w.r.t. the three constraints above) might grow very large for arbitrary datasets. Indeed we can easily verify that $L_{\max}\leq \log(\Delta(\Mb))$, where 
%\eq{
%	\Delta(\Mb):= \frac{\max_{p,q \in \Mb}\dist(p,q)}{\min_{p\neq q \in \Mb}\dist(p,q)}.
%}
%is the aspect ratio of $\Mb$. In practice however we only keep one copy of the nodes which do not have either parent or a child other than themselves. This \emph{explicit} representation efficiently reduces the required storage space to $O(n)$. 
%
%\begin{algorithm}[t]
%	\label{alg:findNN}
%	\SetAlgoLined
%	$Q_0 = \{\widetilde \Cc_0\}$, where $\widetilde \Cc_0$ is the root of $\Tt_{\Mb}$\\
%	$d_{\min}=\norm{p-q_c}$\\
%	$i=0$\\
%	\While{$i<L_{\max}$ \& $! \textbf{stoppage}(\gamma)$}
%	{
%		$Q=\left\{\text{children}(q):\, q\in Q_i \right\} $\\
%		$q^* = \argmin_{q\in Q} \norm{p-q}$, \quad $d = \norm{p-q^*}$ \\
%		\If{$d<d_{\min}$}{$d_{\min}=d, \quad q_c=q^*$}
%		$Q_{i+1} = \left\{q\in Q: \norm{p-q}\leq d_{\min} + maxdist(q) \right\}$\\
%		$i  = i+1$\\
%	}	
%	\Return $q_c$\\%=\argmin_{q\in Q_{L_{\max}}} \norm{p-q}$\;
%	\caption{\label{alg:NN} \textbf{ANN}(cover tree $\Tt_\Mb$, query point $p$, current estimate $q_c \in \Mb$, accuracy parameter $\gamma$)}
%\end{algorithm}
%
%Algorithm \ref{alg:NN} details the procedure for approximate nearest neighbour (ANN) search on a given cover tree. 
%In short, we iteratively traverse down the cover tree and at each scale we populate the set of candidates $Q_i$ with nodes that can be ancestors of the closest NN point and discard others (this refinement uses the triangular inequality and the lower bound on the distance of the grandchildren of $Q$ to $p$, based on  $maxdist(q)$). In the next iteration, children of these candidates are similarly refined and added to the updated candidate set $Q_{i+1}$. At the finest scale (before stoppage) we search the whole set of final candidates and report an ANN point. Not that at each scale we only compute distances for non self-parent nodes (we pass, without computation, distance information of the self-parent children to finer scales).


