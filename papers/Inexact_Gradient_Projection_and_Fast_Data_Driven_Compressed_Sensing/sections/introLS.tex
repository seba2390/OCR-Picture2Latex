\section{Introduction}  
Signal inference under limited and noisy observations is a major line of research in signal processing, machine learning and statistics and it has a wide application ranging from biomedical imaging, astrophysics, remote sensing  to data mining. %\todo{maybe better apps to list?}.
Incorporating the structure of signals is proven to significantly help with an accurate inference since natural datasets often have limited degrees of freedom as compared to their original ambient dimensionality. This fact has been invoked in Compressed sensing (CS)  literature by adopting efficient signal models to achieve accurate signal reconstruction given near-minimal number of measurements i.e. much smaller than the signal ambient dimension (see \cite{DonohoCS, CRT:CS,Tropp:SOMP1,Candes:EMC,BW:manifold,modelbasedCS} and e.g. \cite{RichCSreview} for an overview on different CS models). 
%\todo{This for instance in sensory scenarios such as in MRI, video, high resHSI saves a lot of energy cost time...} 
%Compressed sensing (CS) algorithms adopt efficient signal models to achieve accurate reconstruction given small number of measurements. 
CS consists of a linear sampling protocol:
\eql{
	\label{eq:CSsampling}
	y \approx Ax^\gt,
}
where a linear mapping $A$ samples a $m$-dimensional vector $y$ of noisy measurements from a ground truth signal $x^\gt$ which typically lives in a high ambient dimension $n\gg m$. Natural signals often have efficient compact representations using nonlinear models such as low dimensional smooth manifolds, low-rank matrices or the Union of Subspaces (UoS) that itself includes sparse (unstructured) or structured sparse (e.g. group, tree or analysis sparsity) representations in properly chosen orthobases or redundant dictionaries~\cite{RichCSreview}.
%An efficient CS scheme incorporates such non-linearity into the reconstruction phase to achieve robustness and better compression rates. 
CS reconstruction algorithms for estimating $x^\gt$ from $y$ are in general more computationally complex (as opposed to the simple linear acquisition of CS) as they typically require solving a nonlinear optimization problem based around a prior signal model.  % signal models.  
 A proper model should be carefully chosen in order to efficiently promote the low-dimensional structure of signal meanwhile not bringing a huge computational burden to the reconstruction algorithm.
 
%\subsection{Iterative projected gradient}
Consider the following constrained least square problem for CS reconstruction:
\eql{\label{eq:p1}
	\min_{x\in \Cc} \{f(x):= \frac{1}{2}\norm{y-Ax}^2\},
}
where, the constraint set $\Cc\in \RR^n$ represents the signal model. First order algorithms in the form of projected Landweber iteration a.k.a. iterative projected gradient (IPG) descent or Forward-Backward are very popular for solving \eqref{eq:p1}. 
% in both convex and nonconvex settings. 
%In this case IPG iterates between calculating the gradient and projection onto the model 
% i.e. for positive integers $k$ and an initialization $x^0$ we have:
%\eql{\label{eq:IP}x^{k} = \pp_\Cc\left(x^{k-1}-\mu \nabla f(x^{k-1})\right) }
%where, $\nabla f(x)=A^T(Ax-y)$. It has been shown that under certain conditions and for a well chosen step size $\mu$ IPG converges to a solution of \eqref{eq:p1}.
%We do not make a particular assumption on the initialization of \eqref{eq:IP} and we set $x^0=0$ throughout. 
Interesting features of IPG include flexibility of handling various and often complicated signal models, e.g. $\Cc$ might be convex, nonconvex or/and semi-algebraic such as sparsity or rank constraints (these last models result in challenging combinatorial optimization problems but with tractable projection operators).  
%, as long as local oracles gradient (the forward model) and Euclidean projection onto $\Cc $ are available. 
%As long as the Euclidean projection\footnote{\todo{Could be generalize by proximal operators to solve sum of the LS with a non-smooth function}} operator is well-defined.
Also IPG (and more generally the proximal-gradient methods) has been considered to be particularly useful for big data applications \cite{Volkan:bigdata}. It is memory efficient due to using only first order local oracles e.g., the gradient and the projection onto $\Cc$, it can be implemented in a distributed/parallel fashion, and it is also robust to using cheap statistical estimates e.g. in stochastic descend methods~\cite{Bottou:SGD} to shortcut heavy gradient computations.% and as its rate of convergence is nearly independent of data size.


In this regard a major challenge that IPG may encounter is the computational burden of performing an \emph{exact} projection step onto certain complex models (or equivalently, performing an exact but complicated gradient step). In many interesting inverse problems the model projection amounts to solving another optimization within each iteration of IPG. This includes important cases in practice such as the total variation penalized least squares \cite{Chambolle2011,TVprojGabirel}, low-rank matrix completion \cite{Ma2011} or tree sparse model-based CS \cite{modelbasedCS}. Another example is the convex inclusion constraints $\Cc=\bigcap_i \Cc_i$, appearing in multi constrained problems e.g.~\cite{SPCA,LRJS}, where one might be required to perform  a Djkstra type feasibility algorithm at each iteration~\cite{Dykstra,Dykstra2}. Also, for data driven signal models the projection will typically involve some form of search through potentially large datasets. 
In all these cases accessing an exact oracle could be either computationally inefficient or even not possible (e.g. in analysis sparse recovery~\cite{GiryIPGaprox} or  tensor rank minimization~\cite{Holger:tensor} where the exact projection is NP hard), and therefore a natural line of thought is to carry those steps with cheaper \emph{approximate} oracles.

\subsection{Contributions}
In this paper we feature an important property of the IPG algorithm; that \emph{it is robust against deterministic  errors in calculation of the projection and gradient steps.} 
We cover different types of oracles: i) A \emph{fixed precision} (FP) oracle which compared to the exact one has an additive bounded approximation error.  ii) A \emph{progressive fixed precision} (PFP) oracle which allows for larger (additive) approximations in the earlier iterations and refines the precision as the algorithm  progresses. iii) A $(1+\epsilon)$-approximate oracle which introduces a notion of relatively optimal approximation with a multiplicative error (as compared to the exact oracle). 

Our analysis uses a notion of model restricted bi-Lipschitz \emph{embedding} similar to e.g. \cite{Blumen}, however in a more local form and with an improved conditioning (we discuss this in more details in Section~\ref{sec:main}). 
%with improved  
%recovery condition in two ways: first by relaxing the purely uniform restrictions to a more local form which can avoid worst-case scenarios unnecessarily restricting the analysis, and second, by improving the requirement on the embedding conditioning by a factor (we discuss these in more details in Section~\ref{sec:main}). 
With that respect, our analysis differs from the previous related works in the convex settings as the embedding  enables us for instance to prove a globally optimal recovery result for nonconvex models, as well as establishing linear rate of convergences for the inexact IPG applied for solving CS problems (e.g. results of \cite{BachinexactIPG} on linear convergence of the inexact IPG assumes strong convexity which does not hold in solving underdetermined least squares such as CS). 

In summary, we show that the FP type oracles restrict the final accuracy of the main reconstruction problem. 
% to be proportional to the approximation level introduced within each iteration. 
This limitation can be overcome by increasing the precision at an appropriate rate using the PFP type oracles where one could achieve the same solution accuracy as for the exact IPG algorithm under the same embedding assumptions (and even with the convergence rate). We show that the $(1+\epsilon)$-approximate projection can also achieve the accuracy of exact IPG however under a stronger embedding assumption, moderate compression ratios and using possibly more iterations (since using this type of oracle typically decreases the rate of convergence). In all the cases above we study conditions that provide us with linear convergence results. 

Finally we apply this theory to a stylized data driven compressed sensing application that requires a nearest neighbour search order to calculate the model projection. We shortcut the computations involved,  (iteratively) performing exhaustive searches over large datasets, by using approximate nearest neighbour search strategies corresponding to the aforementioned oracles and motivated by the cover tree structure introduced in \cite{beygelzimer2006cover}. Our proposed algorithm achieves a complexity logarithmic  in terms of the dataset population (as opposed to the linear complexity of a brute force search). 
By running several numerical experiments on different  datasets we conclude similar observations as predicted by our theoretical results.
\subsection{Paper organization}
The rest of this paper is organized as follows: In Section~\ref{sec:SOA} we review and compare our results to the previous related works on inexact IPG. In Section~\ref{sec:prelim} we define the inexact IPG algorithm for three types of approximate oracles. Section~\ref{sec:main} includes our main theoretical results on robustness and linear convergence of the inexact IPG for solving CS reconstruction problem. %In this section we also define a hybrid (local-uniform) notion of bi-Lipschitz embeddingg.
 In  Section~\ref{sec:datadrivenCS} we discuss an application of the proposed inexact algorithms to accelerate solving data driven CS problems. We also briefly discuss the cover tree data structure and the associated exact/approximate search strategies.  Section~\ref{sec:expe} is dedicated to the numerical experiments on using inexact IPG for data driven CS. 
And finally we discuss and conclude our results in Section~\ref{sec:conclusion}.

%This problem has been partly studied in both convex and nonconvex setting for which we provide more details in the next section. 

% This problem a.k.a. the \emph{inexact} IPG, has been studied in convex settings (and even for more general objectives than the CS problem under consideration here). A \emph{fixed precision} (FP) approximate oracles could be often obtained thanks to invoking the duality-gap information and stopping earlier the auxiliary optimization w.r.t. solving the projection. In this case the solution accuracy of the main  problem can not get better than the introduced approximation level. More recently \cite{BachinexactIPG} proposed and studied a \emph{progressive fixed precision} (FPF) type approximation scheme which refines the inexactness levels at an appropriate rate and achieves the same accuracy as for the exact IPG algorithm. However this result is only customized for convex problems and does not prove a linear convergence for CS related problems with non strong convexity.
%
%In nonconvex settings this has been tried for certain cases e.g. lowrank, tree-sparse, earth mover distance CS and by using $1+\epsilon$ type approximations.  This type of approximation introduces a notion of relatively optimal oracles with  a multiplicative error (as compared to the exact oracle).




%In this paper we show another important feature of IPG; that it is robust against deterministic (worst case) errors in calculation of the projection and gradient steps. This has straightforward implications \todo{e.g. for robust computation in distributed systems with errors}. However and of particular interest, this feature implies that one can  approximately conduct both steps to save in computations. For instance, this opens the possibility of using more complex models (which offer efficient representations for better CS reconstruction) with computationally exhaustive or even NP-hard projections, however approximable!







%. Moreover, we show that for certain decaying approximation errors, IPG solution maintain  the same accuracy as for the approximation-free (exact IPG) case.


%A general convergence result holds for both convex and non-convex constraints $\Cc$, that is by choosing the step size small enough $\gamma\leq 1/\norm{A}$, IPGD converges to a critical point of the problem \eqref{eq:p1} which for convex $\Cc$ is also a global convergence i.e., $\lim{x^{t}}\rightarrow x^{opt}$.  \nref ATOUCH. 
%
%Another class of results, guarantee near-optimal linear convergence of IPGD for non-convex $\Cc$, when $A$ drawn at random from certain distributions (e.g. element-wise i.i.d. subgaussian, or random orthoprojector,...) and when number of the measurements $m > \dim(\Cc)$. \marginpar{can we be more regorous about dim here?} 


  
