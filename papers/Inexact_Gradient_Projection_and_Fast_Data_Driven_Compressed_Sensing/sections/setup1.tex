\section{Preliminaries}\label{sec:prelim}
Iterative projected gradient iterates between calculating the gradient and projection onto the model 
i.e. for positive integers $k$ the \emph{exact} form of IPG follows:
\eql{\label{eq:IP}
	x^{k} = \pp_{\Cc'}\left(x^{k-1}-\mu \nabla f(x^{k-1})\right) }
where, $\nabla f(x)=A^T(Ax-y)$ and $\pp_\Cc'$ denote the exact gradient and the Euclidean projection oracles, respectively.  We do not make a particular assumption on  the initialization and we set $x^0=\mathbf{0}$ throughout (also for the inexact form of IPG which we shall define next). The exact IPG requires   
%problem \eqref{eq:p1} has solution(s) and	
the constraint set $\Cc'$ 
 to have a well defined, not necessarily unique but  computationally tractable Euclidean projection $\pp_{\Cc'}:\RR^n\rightarrow \Cc'$ 
% \footnote{Closeness of $\Cc$ %and a lower bound on $f$ 
% 	implies the existence of such minimum i.e. the exact projection. %$x^*$ for \eqref{eq:p1}. 
% 	%One might drop the closeness condition but rather use a FP type approximate projection as discussed in \cite{Blumen}.
% 	}:
\eq{
	\pp_{\Cc'}(x)\in \argmin_{u\in\Cc'}\norm{u-x}.}
Throughout we use $\norm{.}$ as a shorthand for  the Euclidean norm $\norm{.}_{\ell_2}$. 
%Closeness of $\Cc'$ 
%implies the existence of a minimum, however we do not assume uniqueness for the exact projection. 

In the following we define three types of approximate oracles which frequently appear in the literature and could be incorporated within the IPG iterations. We also briefly discuss their applications. 
\newline
\textbf{Important note:} We incorporate throughout the flexibility of considering an approximate projection onto a (possibly) larger set $C$ including the original signal model $x^\gt \in C'\subseteq C$ , which for instance finds practical implications in tree-sparse signal or lowrank matrix CS recovery, see \cite{HegdeISIT,MatrixAlpsapprox}.
\subsection{Fixed Precision (FP) approximate oracles}
\label{sec:FP}
We first consider 
%fixed precision (FP) 
approximate oracles with \emph{additive} bounded-norm 
%varying 
errors, namely the fixed precision gradient oracle $\nablaa^{\nug} f(x):\RR^n\rightarrow \RR^n$ where:
\begin{align}
\norm{\nablaa^{\nug} f(x) -\nabla f(x)}\leq \nug, \label{eq:grad} 
\end{align}
and the fixed precision projection oracle $\pp_\Cc^{\nup}:\RR^n\rightarrow\Cc$ where:
\begin{align}
\pp_\Cc^{\nup}(x) \in \Big\{ u\in \Cc :\,	\norm{u-x}^2 \leq \inf_{u'\in \Cc'}\norm{u'-x}^2 +\nup  \Big\}.\label{eq:proj1}
\end{align} 
The values of $\nug,\nup$ denote the levels of inaccuracy in calculating the gradient and projection steps respectively. 
The corresponding \emph{inexact} IPG
%for the gradient IPG with a fixed-precision accuracy 
iterates as follows:
%\eql{\label{eq:inIP} x^k = \pp^{\nup^k}_{\Cc}\left(x^{k-1}-\mu( \nabla f(x^{k-1}) + \eg^k)\right) + \ep^k.}
\eql{\label{eq:inIP} x^k = \pp^{\nup^k}_{\Cc}\left(x^{k-1}
	%-\mu\left( \nabla f(x^{k-1}) + \eg^k\right)\right), }	
-\mu \nablaa^{\nug^k} f(x^{k-1})\right).}
Note that in this formulation we allow for variations in the inexactness levels at different stages of IPG. % \todo{however we assume neither $\ep^k$ or $\eg^k$ depend on $x^{k-1}$ or the previous solution updates.}  
The case where the accuracy levels are bounded by a constant threshold $\nup^k=\nup$ and $\nug^k=\nug$, $\forall k$, refers to an inexact IPG algorithm with \emph{fixed precision} (FP) approximate oracles.

% which could apply for instance in  distributed network

%In this regard a \emph{Fixed precision} (FP) inexact IPG refers to having approximate oracles with accuracy levels bounded by a constant threshold i.e.  $\nup^k=\nup$ and $\nug^k=\nug$, $\forall k$. 
\subsubsection*{Examples}
Such errors can occur for instance in distributed network
optimizations where the gradient calculations could be noisy during the communication on the network\nref[.], or in CS under UoS models with infinite subspaces \cite{Blumen} where an exact projection might not exist by definition when e.g. $\Cc=\Cc'$ is an open set, however a FP type approximation could be achievable. It also has application in finite super resolution\nref, source localization and separation \cite{TASL14,SCOOP}, and data driven CS problems e.g., in Magnetic Resonance Fingerprinting \cite{MRF,BLIPsiam},  where typically a continuous manifold is first discretized into a finite precision and large dictionary which is eventually used for e.g. sparse recovery.\todo{but this is $C\subseteq C'$?}
\subsection{Progressive Fixed Precision (PFP) approximate oracles}\label{sec:PFP}
One obtains a \emph{Progressive Fixed Precision} (PFP) approximate IPG algorithm by refining the FP type precisions thorough the course of iterations. Therefore any FP gradient or projection oracle which has control on tuning its accuracy could be used in this setting and follows \eqref{eq:inIP} with decaying sequences $\nup^k,\nug^k$.
\subsubsection*{Examples}
For instance this case includes projection schemes which require iteratively solving an auxiliary optimization (e.g. the total variation ball \cite{TVprojGabirel}, sparse CUR factorization \cite{BachinexactIPG} or the multi-constraint inclusions\nref, etc) and can progress (at an appropriate rate) on the accuracy of their solutions by adding more subiterations. We also discuss in Section~\ref{sec:covertree} another example 
%of the PFP type projection oracles 
in this form that is customized for fast approximate nearest neighbour searches and its 
application to the data driven CS framework.

\subsection{$(1+\epsilon)$-approximate projection}\label{sec:epsproj}
For some constraint sets obtaining a fixed precision (and thus PFP) accuracy in projections might be still computationally exhaustive, whereas a notion of relative optimality 
%oracles with multiplicative errors are 
could be more efficient to implement. The $(1+\epsilon)$-approximate projection is defined as follows and for a given $\epsilon\geq0$:
\eql{\label{eq:eproj}
	\pp_\Cc^{\epsilon}(x) \in \Big\{ u\in \Cc :\,	\norm{u-x} \leq (1+\epsilon)\inf_{u'\in \Cc'}\norm{u'-x}  \Big\}. 
}
We note that $\pp_\Cc^{\epsilon}(x)$ might not be unique. In this regard, the inexact IPG algorithm with a $(1+\epsilon)$-approximate projection takes the following form:
\eql{\label{eq:inIP2}
	x^k = \pp_\Cc^{\epsilon}\left(x^{k-1}
	%-\mu( \nabla f(x^{k-1}) + \eg^k)\right).
	-\mu \nablaa^{\nug^k} f(x^{k-1})\right).}
Note that we still assume a fixed precision gradient oracle with flexible accuracies $\nug^k$.  
%$\norm{\eg^k}\leq \nug^k$. 
One could also consider a $(1+\epsilon)$-approximate gradient oracle and in combination with those aforementioned inexact projections however for brevity and since the analysis is quite  similar to the relative approximate projection case we skip more details on this case (one might be able to link this with d'aspremond works on subselecting row of $A$ for implementing cheaper statistical gradients).  
\subsubsection*{Examples}
The tree $s$-sparse projection in $\RR^n$ and in the exact form requires solving a dynamical programming with $O(ns)$ running time \cite{DRthompson} whereas solving this problem approximately with $1+\epsilon$ accuracy requires the time complexity of $O(n\log(n))$ \cite{HegdeISIT} which suits better most imaging problems in practice with a Wavelet sparsity level of typically $s=\Omega(\log(n))$. Also \cite{StoIHT,MatrixAlpsapprox} show one can reduce the cost of lowrank matrix completion problem by using randomized linear algebra methods, e.g. see \cite{Deshpande2006,HalkoTropp}, and carry out fast lowrank factorizations with a $1+\epsilon$ type approximation. \todo{can we say something here about submodular optimization [Krause] with greedy algos and their 1+e guaranty?}
We also discuss in Section~\ref{sec:covertree} another example 
%of the PFP type projection oracles 
in this form that is customized for fast approximate nearest neighbour searches and its 
application to the data driven CS framework.


%In a similar fashion the fixed precision (FP) approximate projection oracle is defined:

 

 
%It has been shown that under certain conditions and for a well chosen step size $\mu$ IPG converges to a solution of \eqref{eq:p1}.

