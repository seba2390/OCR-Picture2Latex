

\section {Methodology}
\label{sec:methodology}
Our work aims to enhance the ability of robots to navigate through large-scale environments by introducing a new multiscale extension to continuous attractor models. In Section~\ref{subsec:attractdyna}, we describe the dynamics of the 2D attractor network, which serves as the foundation of our approach. We then present the multiscale network extension in \mbox{Section~\ref{subsec:multiscalenetwork}}, which enables the network to handle large-scale velocity inputs and accurately track trajectories over extended periods. Additionally, we introduce the head direction network in Section~\ref{subsec:hdnetwork}, which processes the robot's heading direction information and feeds it into the multiscale network. To further improve the usability and performance of our approach, we employ a genetic algorithm-based tuning approach, which is detailed in Section~\ref{subsec:tuning}. Our methodology is evaluated in a new city-scale navigation simulator that we introduce in Section~\ref{subsec:simulator}, providing a comprehensive evaluation of our approach's ability to track motion through a range of large, varied street networks.


\subsection{Attractor Dynamics}
\label{subsec:attractdyna}

Attractor networks are widely used in neural modelling \cite{yu2019neuroslam} \cite{milford2010persistent} \cite{khona2022attractor} due to their ability to represent and maintain stable patterns of activity, even in the presence of noise and input variability. In this subsection, we describe the attractor dynamics used in our model, which is based on a 2D grid of neurons with recurrent excitatory connections. Specifically, a 2D attractor network $\mathbf{X}$ is defined as an $N_x\times N_y$ grid of neurons.

\subsubsection{\textbf{Initilization}}
%
At the beginning of the simulation, the neurons in the attractor network are initialized using a 2D Gaussian function with standard deviation $(\sigma_x, \sigma_y)$, which determines their initial state of activation. We initialize the activity in an $A\times A$ region centred around neuron $(x_0, y_0)$, resulting in the following activation profile:

\begin{equation}
\mathbf{G}(i,j) = \exp\left(-\frac{(i-x_0)^2}{2\sigma_x^2} - \frac{(j-y_0)^2}{2\sigma_y^2}\right)
\end{equation}

\begin{equation}
    \mathbf{X}(0,i,j) =
    \begin{cases}
        \mathbf{G}(i,j), & \text{if } i\in[x_0-A,x_0+A], \\
          & \hspace{-0.4cm}\qquad j\in[y_0-A,y_0+A] \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

\subsubsection{\textbf{Network update}}
At the end of each time step $\delta t$, the activity of each neuron is divided by the L2 norm of the activity across the entire network. This ensures that the maximum magnitude of the activity vector is always one and prevents explosive activity growth:
\begin{equation}
\mathbf{X}(t+\delta t,i,j) = \frac{\mathbf{X}(t,i,j)+\mathbf{C_f}(i,j)+\boldsymbol\epsilon(i,j)-\mu}{\left\| \mathbf{X}(t,i,j)+\mathbf{C_f}(i,j)+\boldsymbol\epsilon(i,j)-\mu \right\|}
\end{equation}
In this equation, the numerator represents the total input to each neuron, which is the sum of its current activity $\mathbf{X}(t,i,j)$, the activity of neurons shifted into its place field based on the input velocity $\mathbf{C_f}(i,j)$, the excitation caused by nearby active neurons $\boldsymbol\epsilon(i,j)$, and the global inhibition term $\mu$.

\subsubsection{\textbf{Copying and shifting the activity packet}}

After initialization, the attractor network is updated by injecting the activity of active neurons back into the network with an integer offset $\alpha_x$ and $\alpha_y$ corresponding to the input velocity, which shifts the activity packet across the grid. To achieve this, we copy the weights $\mathbf{C}$ of active neurons (i.e., those with an activity greater than zero) into the network, with wraparound connections at the edges. Specifically, we use the following equation:
\begin{equation}
\mathbf{C}_{(i',j')} = \begin{cases}
\mathbf{X}_{(t,i,j)}, & \text{if } \mathbf{C}_{(i',j')} >0, \\
0 & \mathrm{otherwise},
\end{cases}
\end{equation}
where $(i',j')$ is the shifted position of the active neuron $(i,j)$, computed as $(i',j') = \bigl((i+\alpha_x) \bmod N_x,  (j+\alpha_y)\bmod N_y\Bigl)$. 

\subsubsection{\textbf{Fractional shifts}}
In addition to integer shifts, the attractor network can also handle fractional shifts, which allow the activity packet to move more smoothly across the grid. To achieve this, we use linear interpolation to modify the copied weights $\mathbf{C}(i,j)$ with a fractional offset amount $\alpha_f$ and a confidence parameter $\gamma$, resulting in the following fractional copy function:
\begin{align}
\begin{split}
      \mathbf{C_f}(i,j) &= \gamma\big((1 - \alpha_f)\cdot \mathbf{C}(i,j) \\&+ \alpha_f \cdot \mathbf{C}((i+1) \bmod N_x,(j+1)\bmod N_y)\big).
\end{split}
\end{align}



\subsubsection{\textbf{Excitation}}
After the activity packet has been shifted across the grid, the active neurons excite a region of size $E\times E$ (such that $E$ represents the excitation radius), with the strength of the excitation being scaled by the weight, $W_n$, of each neuron. Specifically, we use a 2D Gaussian function to compute the excitation $\epsilon(i,j)$ of neuron $(i,j)$, resulting in the following expression:
\begin{equation}
\boldsymbol\epsilon(i,j)=\sum_{n=1}^{N_x \times N_y}  W_n\cdot\exp\left(-\frac{(i-x_n)^2}{2\sigma_x^2} - \frac{(j-y_n)^2}{2\sigma_y^2}\right).
\end{equation}
As for the initialization, if the neuron index is outside the range $x_n-E < i < x_n+E$ and $y_n-E < j < y_n+E$, then the activity is set to 0.





\subsubsection{\textbf{Inhibition}}
To prevent the network from explosive activity growth, a global inhibition term is applied. The global inhibition $\mu$ that is applied to each neuron is computed by summing the activities of all neurons and scaling the result by an inhibition factor $\phi$:

\begin{equation}
\mu=\sum_{i=1}^{N_x} \sum_{j=1}^{N_y} \mathbf{X}_{i,j}\times \phi.
\end{equation}
This inhibition term acts as a normalizing factor, reducing all neurons' activity when the total network activity becomes too high.


\subsection{Multiscale Attractor Network}
\label{subsec:multiscalenetwork}

The multiscale attractor network builds upon the capabilities of the single-scale attractor network by incorporating multiple 2D networks, each with its own unique scale resolution. This allows the network to capture information at various scales, which is especially relevant in spatial navigation where the agent's velocity can span a large range. By selecting the input network with the closest spatial resolution to the agent's speed, the network can effectively integrate motion information using attractor dynamics and enable accurate position tracking.



\subsubsection{\textbf{Storing network wraparound}}
 Continuous attractor networks in 2D have toroidal manifolds, i.e.~the surface wraps around and reconnects with itself in the $x$ and $y$ dimension, forming a seamless and continuous space that has a torus or donut shape. This enables more efficient computation as neurons on the edge can receive input from the opposite edge. However, the toroidal manifold of the network poses a challenge in terms of preventing decoded position reset when activity wraps around an edge. By storing the distance travelled in wraparound buffers, our algorithm ensures that the agent's position is accurately updated when the activity wraps around an edge, preventing position reset and enabling smooth navigation in the environment. 
 
 For example, if the decoded x position from the network decreases while the agent is facing right (270 < $\theta$ < 90), the wraparound buffer is incremented by the network size to store the magnitude of the position reset. Similarly, if the decoded x position from the network increases while the agent is facing left (90 < $\theta$ < 270), the wraparound buffer is decremented by the network size. In the 2D network, two wraparound buffers are used to store the distance travelled in $x$ and $y$ dimensions. 
 

%
%
%
%
%
%
%
%
%
%
%
%

\subsubsection{\textbf{Position decoding}} 
The final step in the process is to decode the estimated position of the agent using the combined activity of all networks and the wraparound buffer. This is accomplished by taking the circular mean of the most active neuron's row and column, with each network's activity weighed by its corresponding scale resolution. The resulting position estimation is therefore a combination of the information captured at different scales, resulting in a more accurate representation of the agent's true position. Specifically, the decoded position of $M$ networks, each of size $N$ and  scale $s$ is:

\begin{equation}
\small
D= \sum_{j=1}^{M} s(j)\cdot \frac{N}{2\pi} \atantwo\big(\sum_{i=1}^{N}  W_i \sin(i \frac{N}{2\pi}),\sum_{i=1}^{N}  W_i \cos(i\frac{N}{2\pi})\big),
\end{equation}
\normalsize
where $N$ is $N_x$ for decoding the $x$ position and $ N_y$ for decoding the $y$ position, as defined previously. 


\subsection{Head Direction Network}
\label{subsec:hdnetwork}
The head direction network plays a crucial role in the navigation system and is also modeled by a continuous attractor network, which is a well-established computational model in the field of neuroscience~\cite{taube1998head,skaggs1994model}. It has a 1D ring structure, and similar to the 2D system, it uses the same tuning parameters ($A, E, \gamma, \phi$) and attractor dynamics. At the beginning of the simulation, the 1D attractor network is initialized with a 1D Gaussian of radius $A$. The activity in the network is updated by shifting activity packets scaled by $\gamma$. Excitation and inhibition are applied with radius $E$ and inhibition factor $\phi$, respectively. The activity in the network is normalized after each update to maintain a stable firing rate.

The network estimates the current heading angle of the agent using the circular mean method. The circular mean is calculated by taking the weighted average of the cosine and sine of the firing angles of the neurons in the network. This method is used to ensure that the direction estimate is robust to circular data, where the values wrap around after reaching the maximum or minimum value. Overall, the head direction network provides an estimate of the agent's heading angle, which is used in conjunction with the multiscale attractor network to update the agent's position.

%

\subsection{Network Tuning with Genetic Algorithm}
\label{subsec:tuning}
To ensure stable dynamics and accurate velocity integration, the continuous attractor network parameters activation radius $A$, excitation radius $E$, motion confidence $\gamma$, and inhibition factor $\phi$ must be carefully tuned.  
%
We use a genetic algorithm \cite{katoch2021review} to automate this tuning process. Genetic algorithms are a type of optimization algorithm that is commonly used to explore the fitness landscape of a set of parameters and find the optimal solution. The goal of the tuning process is to ensure that the attractor network exhibits stable dynamics and accurate velocity integration. Each parameter has an operating range that is dependent on the network size and the number of dimensions.

To explore the fitness landscape, the genetic algorithm generates an initial population of potential solutions with varying parameter values. The fitness function is evaluated for each member of the population, and the fittest individuals are selected to be the parents of the next generation. The algorithm then creates three children from the fittest parents by mutating their genes with a mutation probability. This is further detailed in Algorithm~\ref{alg: genetic algorithm}. Fitness is evaluated in parallel at the start of each generation to reduce computing time.

The fitness function used in this case evaluates the performance of the attractor network in integrating velocity information and providing accurate estimates of position and heading. The velocity is sampled from a uniform distribution within a desired operating range to avoid overfitting to a specific input trajectory.

%

\begin{algorithm}[t]
\vspace{2mm}
\KwData{population $P$, population size $N$, fitness function $f$, mutation rate $r_m$, gene ranges $\rho$, mutation amount $\mu$}
\KwResult {Fittest Child Genome [$A, E, \gamma, \phi$]}
Generate initial population;\\
\While{generation $<$ max generation}{
    $fitnesses \gets$ $f(P_i)$ $\forall i \in P$;\\
    $parents \gets $ top 25\% of $P$ ordered by $fitnesses$;\\
    Clone each parent into three $children$;\\
    %
    \For {g $\in$ $children$} {
        \If{ $U(0,1) < r_m$ }{
        g $\gets$ [g(i)+$N(0,\mu)$,  $ \forall i \in [1,c.size]$];\\
        repeat until c is within $\rho$;\\}}
}
\caption{Tuning Attractor Networks parameters activation radius $A$, excitation radius $E$, motion confidence $\gamma$, and inhibition factor $\phi$ with a Genetic Algorithm}
\label{alg: genetic algorithm}
\end{algorithm}

Overall, the genetic algorithm provides an automated way to fine-tune the parameters of the continuous attractor network, which is essential for the system to work accurately in a variety of environments.



\subsection{City Scale Navigation Simulator}
\label{subsec:simulator}
Our City Scale Navigation (CSN) simulator is a tool used to evaluate the performance of our proposed system in realistic scenarios. The simulator generates trajectories based on real-world road networks within a 10km $\times$ 10km region from major cities. This enables the evaluation of the system's ability to navigate through complex and dynamic urban environments. The simulator extracts road network data from Open Street Maps to generate an occupancy map consisting of traversable, realistic roads. The occupied cells in the map represent the drivable areas of the road network. A path-finding distance transform algorithm \cite{corke2021not}, is then used to find the optimal route between two randomly generated points on the road map.

Once the sample paths are generated, they can be traversed using the kinematics of a bicycle motion model \cite{BicycleModel}, which is a common model used in the navigation of ground vehicles. During the traversal of the paths, motion information such as linear and angular velocities are recorded. This motion data is then used to evaluate the performance of the system, such as the accuracy of the estimated position and heading, the stability of the continuous attractor network, and the effectiveness of the buffer to prevent position resetting.
