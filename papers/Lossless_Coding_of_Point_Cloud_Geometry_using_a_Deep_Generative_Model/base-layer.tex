
%\par Point cloud data structure, voxel domain, octree domain
\subsubsection{VoxelDNN}\label{ssec:voxelDNN}

\begin{figure}
\captionsetup{singlelinecheck = false, format= hang, justification=raggedright, font=small, labelsep=space}
\begin{minipage}[b]{.45\linewidth}
  \centering
  \centerline{\includegraphics[width=0.65\linewidth]{figures_pcc/3D context.png}}
%  \vspace{1.5cm}
  \centerline{(a) 3D pixel context}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\linewidth}
\label{sfig:typeA}
  \centering
  \centerline{\includegraphics[width=0.85\linewidth]{figures_pcc/masktypeA1.png}}
%  \vspace{1.5cm}
  \centerline{(b) 3D type A mask }\medskip
\end{minipage}
\caption{(a): Example 3D context in a $5 \times 5 \times 5$ block. Previously scanned elements are in blue. (b): $3 \times 3 \times 3$ 3D type A mask. Type B mask is obtained by changing center position (marked red) to 1. }
\label{fig:context}
\end{figure}
%
\begin{figure}[tb]
\captionsetup{singlelinecheck = false, format= hang, justification=raggedright, font=small, labelsep=space}
\centering
\includegraphics[width=0.9\linewidth]{figures_pcc/network_architecture.png}
\caption{VoxelDNN architecture for block 64. A type A mask is applied in the first layer (dashed borders) and type B masks afterwards. `f64,k7,s1' stands for 64 filters, kernel size 7 and stride 1. }
\label{fig:Networkarchitecture}
\end{figure}
\setlength{\textfloatsep}{20pt}% Remove \textfloatsep
\input{base-layer-encoder-alg}

% \subsubsection{Voxel context model}
\par Our method encodes the voxelized point cloud losslessly using context-adaptive binary arithmetic coding. Specifically, we focus on estimating accurately a probability model $p(v)$  for the occupancy of a block $v$ composed by $d \times d \times d$ voxels. We factorize the joint distribution $p(v)$ as a product of conditional distributions $p(v_i|v_{i-1}, \ldots, v_1)$ over the voxel volume: 
\begin{equation}
    p(v)= \underset{i=1 }{\overset{d^3}{\Pi}}p(v_i|v_{i-1},v_{i-2},\ldots,v_{1}).
    \label{eq:p(v)}
\end{equation}
Each term $p(v_i|v_{i-1}, \ldots, v_1)$ above is the probability of the voxel $v_{i}$ being occupied given all previous voxels,  referred to as a context. Figure \ref{fig:context}(a) illustrates an example 3D context. We estimate $p(v_i|v_{i-1}, \ldots, v_1)$ using a neural network which we dub \textbf{VoxelDNN}.  

\par The conditional distributions in~\eqref{eq:p(v)} depend on previously decoded voxels. This requires a \textit{causality} constraint on the VoxelDNN network. To enforce causality, we extend to 3D the idea of masked convolutional filters, initially proposed in PixelCNN~\cite{oord2016pixel}. Specifically, two kinds of masks (A or B) can be employed. Type A mask is filled by zeros from the center position to the last position in raster scan order as shown in Figure \ref{fig:context}(b). Type B mask differs from type A in that the value in the center location is 1. We apply type A mask to the first convolutional layer to restrict both the connections from all future voxels and the voxel currently being predicted.  In contrast, from the second convolutional layer, type B masks are applied which relaxes the restrictions of mask A by allowing the connection from the current spatial location to itself. 
%filled by zeros from the center position to the last position in raster scan order, 
%The examples of 3D type A and B masks are shown in Figure \ref{fig:maskab}.
%Similarly to PixelCNN~\cite{oord2016pixel}, we apply a type A mask to the input convolution and type B mask to purely convolutional layers to enforce causality.

%The value in centre position in mask B is `1'.  %The masked filters in \cite{mentzer2018conditional} , \cite{salimans2017pixelcnn++} have been shown as an effective solution to solve the problem. 
%For each input block of size $d \times d \times d$, VoxelDNN predicts the distribution of each voxel $v_i$ given the previously scanned voxels. During training,

%    L&= \E_{v \sim p(v)} \left [  \right ]

\par In order to learn good estimates $\hat{p}(v_i|v_{i-1}, \ldots, v_1)$ of the underlying voxel occupancy distribution $p(v_i|v_{i-1}, \ldots, v_1)$, and thus minimize the coding bitrate, we train VoxelDNN using cross-entropy loss. That is, for a block $v$ of resolution $d$, we minimize :
\begin{equation}\label{eq:CEloss}
    H(p,\hat{p}) = \mathbb{E}_{v\sim p(v)}\left[\sum_{i=1}^{d^3} -\log \hat{p}(v_i)\right].
\end{equation}
It is well known that cross entropy represents the extra bitrate cost to pay when the approximate distribution $\hat{p}$ is used instead of the true $p$. More precisely, $H(p,\hat{p}) = H(p) + D_{KL}(p\| \hat{p})$, where $D_{KL}$ denotes the Kullback-Leibler divergence and $H(p)$ is Shannon entropy. Hence, by minimizing~\eqref{eq:CEloss}, we indirectly minimize the distance between the estimated conditional distributions and the real data distribution, yielding accurate contexts for arithmetic coding. Note that this is different from what is typically done in learning-based \textit{lossy} PC geometry compression, where the focal loss is used~\cite{quach2019learning, quach2020improved}. The motivation behind using focal loss is to cope with the high spatial unbalance between occupied and non-occupied voxels. The reconstructed PC is then obtained by hard thresholding $\hat{p}(v)$, and the target is thus the final classification accuracy. Conversely, here we aim at estimating accurate soft probabilities to be fed into an arithmetic coder.




\par Figure \ref{fig:Networkarchitecture} shows our VoxelDNN network architecture for block 64. Given the $64 \times 64 \times 64$ input block, VoxelDNN outputs the predicted occupancy probability of all input voxels. Our first 3D convolutional layer uses $7 \times 7 \times 7$ kernels with a type A mask. Type B masks are used in the subsequent layers. To avoid vanishing gradients and speed up the convergence, we implement two residual connections with $5 \times 5 \times 5$ kernels.  Throughout VoxelDNN, the ReLu activation function is applied after each convolutional layer, except in the last layer where we use softmax activation. In total, our model contains 290,754 parameters and requires less then 4MB of disk storage space.
%\input{encoder_algrth}
%\vspace{-1.2cm}

\subsubsection{Multi-resolution encoder and adaptive partitioning}\label{ssec:multires}

We use an arithmetic coder to encode the voxels sequentially from the first voxel to the last voxel of each block in a generative manner. Specifically, every time a voxel is encoded, it is fed back into VoxelDNN to predict the probability of the next voxel. Then, we pass the probability to the arithmetic coder to encode the next symbol. 

However, applying this coding process at a fixed resolution $d$ (in particular, on blocks 64), can be inefficient when blocks are sparse, i.e.,  they contain only few occupied voxels. 
% encoding the whole block as a single block is not always the best solution, especially when blocks are sparse. 
This is due to the fact that in this case, there is little or no information available in the receptive fields of the convolutional filters. To overcome this problem, we propose a rate-optimized multi-resolution splitting algorithm as follows.
We partition a block into 8 sub-blocks recursively and signal the occupancy of sub-blocks as well as the partitioning decision (0: empty, 1: encode as single block, 2: further partition). The partitioning decision depends on the output bits after arithmetic coding. If the total bitstream of partitioning flags and occupied sub-blocks is larger than encoding parent block as a single block, we do not perform partitioning. The details of this  process are shown in Algorithm \ref{algo:proposed_method}. The maximum partitioning level is controlled by $maxLv$ and  partitioning is performed up to $maxLv=5$ corresponding to a smallest block size of 4. Depending on the output bits of each partitioning solution, a block of size 64 can contain a combination of blocks with different sizes. Figure \ref{fig:4levelpartiitoning} shows 4 partitioning examples for an encoder with $maxLv=4$. We train 5 separate models for 5 input block sizes (64, 32,16, 8 and 4). All models have similar architecture as in Figure \ref{fig:Networkarchitecture} except the input size.
%Note that VoxelDNN learns to predict the distribution of the current voxel from previous encoded voxels. As a result, we only need to train a \textit{single} model to predict the probabilities for different input block sizes. 
%In particular, VoxelDNN can perform prediction on blocks of size $d \times d \times d$ with $d$ ranging from 2 to 64. 
% This enables the encoder to decide whether to encode the block as a single block or to partition it into smaller blocks, flag out empty child blocks and only perform prediction on occupied blocks.



\begin{figure}
%
\captionsetup{singlelinecheck = false, format= hang, justification=raggedright, font=small, labelsep=space}
\begin{minipage}[b]{.24\linewidth}
  \centering
  \centerline{\includegraphics[width=0.90\linewidth]{figures_pcc/1level partition.png}}
%  \vspace{1.5cm}
  \centerline{(a)}\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=0.90\linewidth]{figures_pcc/2level partition.png}}
%  \vspace{1.5cm}
  \centerline{(b) }\medskip
\end{minipage}
%
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=0.90\linewidth]{figures_pcc/3level partition.png}}
%  \vspace{1.5cm}
  \centerline{(c) }\medskip
\end{minipage}
%
\hfill
\begin{minipage}[b]{0.24\linewidth}
  \centering
  \centerline{\includegraphics[width=0.90\linewidth]{figures_pcc/4level partition.png}}
%  \vspace{1.5cm}
  \centerline{(d)}\medskip
\end{minipage}
%
\caption{Partitioning a block of size 64 into: (a) a single block of size 64, (b): blocks of size 32, (c): 32 and 16, (d): 32, 16 and 8. Non-empty blocks are indicated by blue cubes.}
\label{fig:4levelpartiitoning}
%
\end{figure}
\setlength{\textfloatsep}{20}