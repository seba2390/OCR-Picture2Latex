\subsection{Experimental Setup}
\input{train_test_pcs}


\subsubsection{Training dataset} We consider point clouds from different and varied datasets, including ModelNet40 \cite{wu20153d} which contains 12,311 models from 40 categories and three smaller datasets: MVUB \cite{loop2016microsoft}, MPEG CAT1 \cite{noauthor_common_nodate} and 8i \cite{d20178i,8i}. We uniformly sample points from the mesh models from ModelNet40 and then scale them to voxelized point clouds with 9 bit precision. To enforce the fairness between the smaller datasets in which we select point clouds for testing, point clouds from MPEG CAT1 are sampled to 10 bit precision as in MVUB and 8i. In addition, we measure the \textit{local density} $\rho$ of a point cloud, computed as the average portion of occupied voxels in the blocks of size 64, that is: 
\begin{equation}
    \rho = \frac{1}{N_{\mathcal{B}}}\times\sum_{\mathcal{B}_i \in \mathcal{B}} \frac{100 \times \text{number of points in }\mathcal{B}_i}{64^3}\ \ (\%)
    \label{rhodensity}
\end{equation}
%(x_{max}-x_{min})(y_{max}-y_{min})(z_{max}-z_{min})
where $\mathcal{B}$ is the set of occupied blocks of size $64$, and $N_{\mathcal{B}}$ is the cardinality of $\mathcal{B}$. 
% We found that $d=64$ provides a representative $\rho$ for the point cloud density.  
The higher the value of $\rho$ is, the denser the point cloud. The selected point clouds, number of frames as well as $\rho$ of the training data are shown in Table \ref{table:traintestpcs}. 
\par To train a VoxelDNN model of size $d$ we divide all selected PCs into occupied blocks of size $d\times d\times d$. Table \ref{table:noblocks} reports the number of blocks from each dataset for training, with the majority coming from the ModelNet40 dataset. For the models trained with data augmentation, we apply rotation with $\theta=45^\circ$ on $x,y,z$ axis and then sampling from all blocks with sampling rate $f_s=[0.7;0.4]$. In total, we augment from each block to 12 variations in terms of density and rotation which significantly increase the volume and diversity of our training set.
%Smallest bounding box removes most of the empty space of each occupied blocks in $B$. And we aim to measure local density of point clouds, therefore, in Equation \ref{rhodensity}, number of occupied voxels is normalized by Smallest bounding box instead of the original block.
\input{training-blocks-table}



\subsubsection{Test data} We evaluate the performance of our approach on a diverse set of point clouds in terms of spatial density and content type. All selected point clouds are either used in MPEG Common Test Condition or JPEG Pleno Common Test Condition to evaluate point cloud compression methods. As shown in Table \ref{table:traintestpcs} the test PCs can be categorized into four sets:
\begin{itemize}
    \item \textbf{MVUB}: Microsoft Voxelized Upper Bodies \cite{loop2016microsoft} - a dynamic voxelized point cloud dataset containing five subjects. For testing, we randomly select 2 frames from \textit{Phil} (frame number 10) and \textit{Ricardo} (76) sequences which are both very dense (high $\rho$) with smooth surfaces.
    \item \textbf{8i}: Dense point clouds from 8i Labs. They are also dynamic voxelized point clouds but each sequence contains the full body of a human subject. In the test set, \textit{loot} (1000) and \textit{redandblack} (1510) are from 8i Voxelized Full Bodies (8iVFB v2) \cite{d20178i} while \textit{boxer} and \textit{thaidancer} are selected and downsampled to 10 bits from 8i Voxelized Surface Light Field (8iVSLF) dataset \cite{8i}.
    \item \textbf{CAT1}: static point clouds for cultural heritage and other related 3D photography applications \cite{noauthor_common_nodate}. We select \textit{Arco\_Valentino\_Dense\_vox12, Frog\_00067\_vox12, and Shiva\_00035\_vox12} from this dataset and downsample to 10 bits to validate the performance of our method. PCs from this dataset are less dense compared to the previous two datasets. \textit{Frog\_00067} has smoother surfaces compared to the other two PCs which contain rough surfaces.
    \item \textbf{USP}: an inanimate dataset from the University of S\~ao Paulo, Brazil, related to cultural heritage with 10 bits geometry precision \cite{usp}. \textit{BumbaMeuBoi} and \textit{RomanOilLight} are two selected point clouds from this dataset. PCs from USP dataset have simple shape with smooth surfaces. \textit{BumbaMeuBoi} is the sparsest PC in our test set with the smallest $\rho$. 
\end{itemize}
\par Figure \ref{fig:test pcs} illustrates the test point clouds.



\subsubsection{Training procedure} 
We train 5 models for 5 input block sizes, i.e., 128, 64, 32, 16, 8. The mini-batch sizes are 1, 8, 64, 128, 128, respectively. Our models are implemented in TensorFlow and trained with Adam \cite{adam} optimizer, a learning rate of 0.001 for 80 epochs on a GeForce RTX 2080 GPU.\footnote{The source code, as well as the trained models, will be made publicly available upon acceptance of the paper.}
% \subsubsection{Evaluation procedure}



\subsection{Performance evaluation and ablation studies}
In the following, we evaluate the performance of the proposed approach as well as the impact of its various components. We start with models without data augmentation nor context extension in order to study the optimal maximal partitioning depth for our method and establish a baseline for the evaluation. 
Next, on top of the best encoder in this experiment (\textbf{Baseline}), we separately add data augmentation (\textbf{Baseline~+~DA}) and context extension (\textbf{Baseline~+~CE}). Finally, \textbf{Baseline~+~DA~+~CE} incorporates both data augmentation and context extension. We compare our method against the state-of-the-art point cloud compression method G-PCC from MPEG \cite{graziosi2020overview} which has a dedicated lossless geometry mode for static point clouds. We report the number of bits per occupied voxel (bpov) for each test point cloud and the average per dataset.



\begin{figure}[tb]
%\captionsetup{singlelinecheck = false, format= hang, justification=raggedright, font=small, labelsep=space}
\captionsetup{justification=raggedright}
\centering
\includegraphics[width=0.99\linewidth]{figures_pcc/allpc2.png}
\caption{Point clouds in the test set. (a) Phil, (b) Ricardo (c) BumbaMeuBoi (d) RomanOilLight, (e) Redandblack, (f) Loot, (g) Thaidancer (h) Boxer, (i) Frog, (j) Arco Valentino, (k) Shiva.}
\label{fig:test pcs}
\end{figure}
\setlength{\textfloatsep}{20pt}


In all experiments, the high-level octree plus partitioning signal are directly converted to bytes without any compression. For the encoders with context extension, we signal the selected size using two bits (maximum 4 options on block 8). This information is also directly converted to bytes in the bitstream. On average, signaling bits account for $2.44\%$ of the bitstream.

\input{partitioning-result-per_pc}





\subsubsection{Optimal maximum partition depth} 
To evaluate the effectiveness of the partitioning scheme, we increase the maximum partitioning level from 1 to 5, corresponding to a minimum block size of  64, 32, 16, 8, and 4. 
% As explained in Subsection \ref{ssec:multires}, we could use a bigger model size to predict the distribution of smaller block. 
As 3D convolution is not able to efficiently exploit voxel relations on a very small receptive field, we do not train a  separate model for block 4.  Instead, we use the model trained on blocks of size 8 to predict its probabilities. 

\begin{figure*}[tb]
\captionsetup{justification=raggedright}
\centering
\includegraphics[width=0.87\linewidth]{figures_pcc/ocv_in_block.png}
\vspace{0.1cm}
\caption{Percentage of occupied voxels encoded in each partition size. From top to bottom: block 8, 16, 32, 64. Most of occupied voxel are encoded in block 64 and block 32. }
\label{fig:ocv_per_block}
\end{figure*}
%\vspace{15mm}
\setlength{\textfloatsep}{20pt}

Table \ref{table:increase partitioning level} shows the average bpov of our encoder on the 4 test datasets at 4 partitioning levels. The results with 5 partitioning levels are identical to 4 partitioning levels and are not shown in the table. We observe that, as partitioning levels increases, the corresponding gain over single-level also increases. However, adding the $3^{rd}$ and $4^{th}$ level yields only a slight improvement compared to adding the $2^{nd}$ level. This can be explained with Figure \ref{fig:ocv_per_block} showing the percentages of occupied voxels in each partition size. We  observe that most  voxels are encoded using blocks 64 and 32, while very few voxels are encoded using blocks of smaller size. Adding more partitioning levels enables to better adapt to point cloud geometry, however, this is not often compensated by a bitrate reduction of the sub-blocks, since in the smaller partitions the encoder has access to limited contexts, resulting in less accurate probability estimation. However, there is an increase in the portion of block 32 and 16 on CAT1 and USP compared to MVUB and 8i. This reflects the density characteristics of each dataset: on sparser datasets (CAT1 and USP), the algorithm tends to partition point cloud into smaller blocks to eliminate as much empty space as possible. Based on these observations, we use a maximum of 4 partitioning levels for our baseline codec in later experiments.
%\input{partitioning-result-per_pc}
%It can be seen from $\rho$ value for each point cloud from Table \ref{table:traintestpcs} that PCs from MVUB and 8i datasets are denser compare to PCs from CAT1 and USP.


\input{results_table}

\subsubsection{Comparison with G-PCC} 
In table \ref{table:result table}, we report the output bitrate of our methods to compare with MPEG G-PCC. Both our method and G-PCC perform better on dense PCs while having higher rates on sparser PCs. 
%Our \textit{Baseline} is the encoder with 4 partitioning level.
Compared to G-PCC, the \textbf{Baseline} encoder obtains a significant gain -- over $29\%$ bitrate reduction on dense point clouds from MVUB and 8i dataset. On CAT1 and USP datasets, our method achieves a comparable rate with G-PCC. In particular, for Arco Valentino and BumbaMeuBoi, the two point clouds having the lowest $\rho$, our baseline codec yields a rate higher than G-PCC ($+8.17\%$ and $+5.10\%$, respectively). For point clouds with high local density levels, our VoxelDNN could efficiently leverage the relations between voxels and predict more accurate probability. In contrast, probability prediction is less accurate on sparser point clouds. 

This can be partially solved by adding data augmentation during training.
Indeed, by random subsampling the point clouds in the training set, VoxelDNN learns to predict more accurate probabilities when the point cloud is less dense. \textbf{Baseline + DA} yields higher gains over G-PCC on CAT1 and USP compared to \textbf{Baseline}, with average bitrate reductions of about $1.54\%$ and $4.09\%$, respectively. On the other hand, we observe a small degradation of the performance compared to \textbf{Baseline} for denser datasets, such as MVUB and 8i dataset. This is somehow expected, as data augmentation increases the generalization capability of VoxelDNN, which instead is more adapted to denser PCs in the baseline mode.
\par The encoder with context extension, \textbf{Baseline + CE}, obtains a better rate on all test point clouds compared to the  \textbf{Baseline} encoder, regardless of the density, with an average further bitrate reduction of $4.8\%$ over G-PCC. The cost to be paid for this performance improvement is a higher computational complexity in the encoding process.
%Adaptively expanding the context to neighboring blocks has clearly shown the effectiveness, however, in terms of complexity, this method increases the complexity when trying all possible expanding options for each block.


\par The last two columns of Table \ref{table:result table} show the experiment results for the encoder incorporating both data augmentation and context extension, \textbf{Baseline + DA + CE}. On average, we have a higher gain than \textbf{Baseline} and \textbf{Baseline + DA} because of the Context Extension. As expected, comparing with \textbf{Baseline + CE}, \textbf{Baseline + DA + CE} has increasing gains on CAT1 and USP datasets while obtaining a lower gain on MVUB and 8i datasets. Despite the different performance trends for different densities of the input point clouds, we obtain, on average, a bitrate reduction of $20.17\%$ compared to G-PCC. Note that, in practice, if the characteristics of point cloud to be coded are known in advance, our approach is flexible, in that we could deploy different models targeting a specific application (cultural heritage, tele-immersive conferencing, etc.) and content type to obtain the best compression rate.

\begin{figure*}[tb]
%\captionsetup{singlelinecheck = false, format= hang, justification=raggedright, font=small, labelsep=space}
\captionsetup{justification=raggedright}
\centering
\includegraphics[width=0.75\linewidth]{figures_pcc/bpov_performance_4pcs_ylog_32.png}
\caption{Performance on block 64 on four test point clouds. Each point corresponds to a block 64 with percentage of occupied voxels ($\rho$) and bpov ($\log$ scale) performance of that block. The size of each point indicates the partitioning level and each partitioning level was sized according to its frequency. Higher points indicate that VoxelDNN is performing worse. The marginal distributions of occupied voxels for each point cloud are on the top of the scatter plot. }
\label{fig:bpovperformace}
\end{figure*}
%Steeper slopes indicate that VoxelDNN is performing worse.
%
\begin{figure}[tb]
%\captionsetup{singlelinecheck = false, format= hang, justification=raggedright, font=small, labelsep=space}
\captionsetup{justification=raggedright}
\centering
\includegraphics[width=0.99\linewidth]{figures_pcc/extension_selection.png}
\caption{Number of extending block size for each block. (a) Phil, (b) Loot, (c) Arco Valentino, (d) BumbaMeuBoi. Most of the time, the encoder extend the context to neighboring voxels instead of independently encoding a block. }
\label{fig:extendingblocksize}
\end{figure}
\setlength{\textfloatsep}{20pt}





\subsubsection{Effect of PC content and density on coding performance} 

%\textcolor{red}{    The text of this section should be modified and shortened to make the message more clear. From the current version and the figure, my understanding is that the subsection is about the effect of density on coding.  It is not clear from the figure why different contents have different slopes. If the coding performance depends on the local density, then shouldn't all the contents lie on the same line? in other words, the fact that different contents have different slopes cannot be explained by the local density only. For ex, you can take a block in Look with 4000 occupied voxels, and a block in Arco Valentino with 4000 occupied voxels, and they require a totally different number of bits... can you explain this better?}

In order to better understand the performance of our codec for different types of content, we plot in Figure~\ref{fig:bpovperformace} the average bpov as a function of the percentage of occupied voxels for each block 64 of \textit{Phil, Loot, Arco Valentino} and \textit{BumbaMeuBoi} with the \textbf{Baseline + DA + CE} encoder. Notice that each block 64 can be split up to different partition levels, indicated by the size of the dots in the figure. The distribution of the density of blocks 64 is shown in the top panel. 

From this figure, we can draw some observations.
First, most of blocks are partitioned into 3 levels (smallest dots) and the majority of the remaining blocks are partitioned into 2 or 4 levels. Second, in each point cloud, denser blocks are easier to compress, as mentioned before, due to the better capabilities of convolution to capture spatial relations. On the other hand, our approach becomes inefficient when the blocks are less dense, and the bitrate associated to the very sparse blocks rapidly grows by an order of magnitude compared to the rest. This phenomenon is true for all kinds of contents, although it has a stronger effect when the block density distribution is skewed to the left, such as for \textit{Arco Valentino} or \textit{BumbaMeuBoi}, which have the highest bitrates in our experiments. 
%This can be explained by the fact that dense blocks provide better context to VoxelDNN. Moreover, the sparse blocks must pay a fixed overhead-bits for partitioning signal while contain just few occupied voxels.
\par We can also observe a content-dependence trend in the figure, which appears like a vertical offset for different PCs. \textit{Arco Valentino} and \textit{RomanOilLight} overall have higher bpov compared to \textit{Phil} and \textit{Loot} with the same number of occupied voxels. This suggests that local density alone is not the only factor affecting the performance of our approach, but that somehow higher-order statistics enter into play. We will speculate more about this behaviour when discussing the bitrate allocation in Figure~\ref{fig:bitallocation}. Further analysis of this trend, as well as how to take better into account the PC characteristics to improve coding performance, are left to future work.
% Given the same number of occupied voxels, those voxels can be spread over the whole block or landed on rough/smooth surfaces in which defenitely influence the performance. Therefore, the local density characteristic is not the only factor that affect the performance but also the content itself. We will further investigate this with Figure \ref{fig:bitallocation}. 


\begin{figure*}[tb]
%\captionsetup{singlelinecheck = false, format= hang, justification=raggedright, font=small, labelsep=space}
\captionsetup{justification=raggedright}
\centering
\includegraphics[width=0.95\linewidth]{figures_pcc/bitallocation3.png}
\caption{Output geometry bitrate in bpov per block. (a) Phil, (b) Loot, (c) Arco Valentino, (d) BumbaMeuBoi. The heatmap bar below each subfigure shows the minimum and maximum bpov and the corresponding color. }
\label{fig:bitallocation}
\end{figure*}
\vspace{15mm}
%\setlength{\textfloatsep}{15pt}% Remove

%\par First, we observe that most of blocks are partitioned into 3 levels (smallest points) and the majority of the remaning blocks are partitioned into 2 or 4 levels. However, in each point cloud, most of blocks are lying on the same slope and thus, there is a linear relation between the number of occupied voxels and encoded bits of block 64. We found similar characteristics on other point clouds. Those evidences show that the partitioning efficiently remove the redundancies within blocks and the output bits on each each block is significantly influenced by the occupied voxels even when most of voxels are empty. 
%\par However, different point clouds give different slopes and a higher slope is, a better VoxelDNN performance. Therefore, the local density characteristic is not the only factor that affect the performance but also the content itself. We will further investigate this with Figure \ref{fig:bitallocation}.
%\\

\subsubsection{Selection of context extension and impact on the partitioning}
Figure \ref{fig:extendingblocksize} shows how many times an extended block size is selected in the \textbf{Baseline + DA + CE} experiments. First, it can be seen that in most  cases our encoder choose to extend the context to encode the current block, and mostly the immediate larger size is selected. By extending context to exploit geometry information from the neighboring voxels, VoxelDNN can leverage a larger amount of information and predict a better probability. In most  cases where the encoder does not extend the context, the blocks are on the border of the volume, corresponding to a mostly empty  extending area.

By summing the quantities in each column, we obtain the number of blocks which are encoded using each block size and we observe that large parts of the point cloud are partitioned into block 32 or 16. This is in contrast with the previous observation on baseline experiments where the most frequent partitions are 64 and 32 (Figure~\ref{fig:ocv_per_block}). This has an intuitive explanation: without context extension, small block sizes of 32 or 16 were insufficient to provide a representative enough context for VoxelDNN in most of the cases, even if they would better adapt to areas with low point density. Conversely, the context extension allows to compensate for the small block dimension and renders these modes competitive.
% as the partitioning algorithm aims to reduce as much as possible empty blocks while maximizing the performance of VoxelDNN, with context extension, the encoder will not prefer to encode block 64 as a single block but partition it to lower resolution to remove the sparsity while still having the context from block 64.
As a result, context extension significantly affects the optimal partitioning and enables VoxelDNN to adapt better to local sparsity while still providing enough contextual information to predict accurate probabilities.

\subsubsection{Using multiple models for the context} For the multi-resolution encoder, instead of using a separate model for each block size, VoxelDNN can use only a single neural network to predict the distribution. Specifically, we place small blocks (8, 16, 32) into a block of size 64 and then use the network for block 64 to predict and extract the corresponding distributions. This method of computing the occupancy distribution is different from Context Extension in that the surrounding voxels are always set to $0$. In Table \ref{table:singlevsmulti}, we compare the performance of using a single model with \textbf{Baseline}, which is a multi-models encoder. In this experiment, both encoders have 4 maximum partitioning levels and use the same model 64. On average, by having a separate model for each block size, a multi-model encoder obtains about $1\%$ additional gain over G-PCC compared to the single model encoder. This amount of gain indicates that the bigger VoxelDNN model can predict the conditional distribution on smaller blocks as efficiently as using a separate model for each block size. However, model 64 is trained on blocks of size 64 only, and learns features at that scale. In general, a model trained on small blocks could better capture the context from small input blocks and thus provides a higher gain in some circumstances.
\input{single_vs_multi_model_table}


\subsubsection{Visualization of the bitrate allocation on coded PCs} The bpov heatmaps of 4 point cloud are shown in Figure \ref{fig:bitallocation}. The blocks in the figures reflect the optimal partitioning obtained by the algorithm. 
%All voxels within the same block have the same bpov value as we compute bpov for each output block of the partition.
First, we visually confirm what found in Figure~\ref{fig:bpovperformace}, i.e., VoxelDNN performs better, i.e., achieves a small bitrate, in the smooth and dense areas of the point cloud. Conversely, in the noisy areas  (\textit{Phil}'s hand, \textit{Loot}'s hand), sudden holes (\textit{Arco Valentino}) or very sparse regions (edges in \textit{Arco Valentino}, the bottom part of \textit{BumbaMeuBoi}), which are indicated in red, the performance is worse. 
% Overall, our approach is efficient on  dense and smooth content (e.g., Phil and Loot). BumbaMeuBoi is smooth and does not have much noise but it is a sparse point cloud, especially bottom part therefore, the performance is worser compare to Phil and Loot. On the contrary, noises, sparsity and rough surfaces exist almost on Arco Valentino and thus the bpov is worst. 
We can argue that the density of a point cloud, together with the smoothness and noise characteristics of the content, are among the main factors that influence the performance of VoxelDNN. On the other hand, we can argue that noisy and very sparse areas are intrinsically difficult to code in general, and indeed also the MPEG G-PCC codec requires a large number of bits to encode point clouds such as \textit{BumbaMeuBoi} and \textit{Arco Valentino}.





\subsection{Computational complexity analysis} A well-known drawback of auto-regressive generative models such as PixelCNN and VoxelDNN is the sequential generation of the symbol probabilities. This requires to run the network for each voxel, which has a complexity that increases linearly with the number of voxels. Therefore, VoxelDNN has a computational complexity which is 3 orders of magnitures bigger than G-PCC.


\begin{table}[t]
\caption{Average runtime (in seconds) of different encoders comparing with G-PCC.}
\centering
\resizebox{0.97\linewidth}{!}{ \begin{tabular}{M{0.7cm}M{1.2cm}M{1.5cm}M{1.8cm}}
\hline
\begin{bf}  \end{bf}
&\begin{bf}G-PCC\end{bf}
%&\begin{bf}1 level\end{bf}
%& \begin{bf}2 levels\end{bf} 
%&\begin{bf}3 levels\end{bf}
& \begin{bf}Baseline\end{bf} 
& \begin{bf}Baseline + CE\end{bf} \\
\hline
(Enc) &2.91&  3282&9271\\
(Dec) &2.85 &6783 &5765\\
\hline
\end{tabular}}
\label{table:complexity}
\end{table}

Table \ref{table:complexity} reports the encoding and decoding time of our \textbf{Baseline} and \textbf{Baseline + CE}. 
Tests are benchmarked on an Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz machine with an Nvidia GeForce GTX 2080 GPU and 16 GB of RAM, running Ubuntu 16.04. Our encoding time is highly dependent on the number of blocks and the number of voxels within each block. Besides, the number of modes in the partitioning algorithm and context extension also influence the complexity. 
% Since our implementations are mainly for evaluation purposes, we only serve complexity analysis as the reference for the complexity of our method.
The \textbf{Baseline + CE} encoder tries all the extending modes and selects the best one, thus its average encoding time is higher than \textbf{Baseline} -- an increase of about 182\%. The reason why the encoding time for the \textbf{Baseline} codec is lower than the decoding time is purely implementative: at the encoder it is possible to predict the whole block probabilities in a single batch on a GPU, while in a realistic scenario, at the decoder side the voxels need to be individually decoded. 
When context extension is enabled, point clouds are partitioned into even smaller blocks, corresponding to a smaller complexity at the decoder, as a smaller number of voxels need to be decoded. On the other hand, the total parameters of each VoxelDNN model corresponds only to about 3.5 MB which is a small-size network in practice. Notice that the bottleneck in our system comes from the adoption of an auto-regressive model, which has the advantage of providing, in principle, an exact likelihood estimation of the data, though at a high computational cost. We are currently investigating the use of alternative generative approaches that avoid sequential probability estimation.

