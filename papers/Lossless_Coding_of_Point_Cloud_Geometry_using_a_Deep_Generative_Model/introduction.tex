

\IEEEPARstart{P}{oint} clouds (PC) are becoming the most popular data structure for many 3D applications such as augmented, mixed or virtual reality, as they enable six degrees of freedom (6DoF) interaction. 
%including three translation and three rotation movements.
Typical PCs contain millions of points, each point being represented by $x,y,z$ coordinates, and attributes (e.g. color, normal, etc.). This entails a high transmission and storage cost. As a result, there is a massive demand for efficient Point Cloud Compression (PCC) methods to enable the practical use of this content. 
%
\par %The geometry and attributes of PCs are usually independently encoded. 
The Moving Picture Expert Group (MPEG) has studied coding solution for various categories of point clouds, including static point clouds (category 1), dynamic point clouds (category 2), and LiDAR sequences (category 3 -- dynamically acquired point clouds). As a result, two PCC standards have been developed \cite{8571288,jang2019video,graziosi2020overview}: Video-based PCC (V-PCC) and Geometry-based PCC (G-PCC). V-PCC focuses on dynamic point clouds, and projects the volumetric video onto 2D planes before encoding. The generated 2D videos are then compressed using 2D video coding standards. This approach benefits from efficient 2D video coding solutions which have been optimized over several decades.  On the other hand, G-PCC targets static content, and the geometry and attribute information are independently encoded. Color attributes can be encoded using methods based on the Region Adaptive Hierarchical Transform (RAHT) \cite{de2016compression}, Predicting Transform or Lifting Transform \cite{graziosi2020overview}. 
Coding the PC geometry is particularly important to convey the 3D structure of the PC, but is also challenging, as the non-regular sampling of point clouds makes it difficult to use conventional signal processing and compression tools.
% However, the geometry must be available before filling point clouds with attributes. 
In this paper, we focus on \textit{lossless coding} of point cloud geometry. 
\par In particular, we consider the case of \textit{voxelized} point clouds. Voxelization consists in pre-quantizing the geometric coordinates of the point cloud prior to coding in order to represent the geometry with integer precision. This operation is common in many coding scenarios, e.g., when dealing with dense point clouds such as those produced by camera arrays.
After voxelization, the point cloud geometry can be represented either directly in the voxel domain or using an octree spatial decomposition. PCs are divided into a fixed number of cubes, which defines the resolution (e.g., 10 bit = 1024 cubes per dimension).  Each cube is called a voxel. If a voxel contains at least one point, it is called an occupied voxel. Usually, very few voxels are occupied and a large part of the volume is empty. 
% In this paper, we also adopt this approach, which is particularly suited for dense point clouds. 
An octree representation can be obtained by %analyzing the geometry of the voxelized point cloud. Assuming that the point cloud is mapped into a volume of $D \times D \times D$ voxels, the volume is 
recursively splitting the volume into eight sub-cubes until the desired precision is achieved. Then, occupied blocks are marked by bit 1 and empty blocks are marked by bit 0. Consequently, at each level, the generated 8 bits represent the occupancy state of an octree node (octant). Our method operates in both the voxel and octree domain. On the one hand, the octree representation can naturally adapt to the  sparsity of the point cloud, as empty octants do not need to be further split; on the other hand, in the voxel domain convolutions can be naturally expressed, and geometric information (i.e., planes, surfaces, etc.) can be explicitly processed by a neural network. 

% \par MPEG identified three categories of point clouds: static point clouds (category 1), dynamic point clouds (category 2), and LiDAR sequences (category 3 - dynamically acquired point clouds). 
In this work, we propose a deep-learning-based method (named VoxelDNN) for lossless compression of static voxelized point cloud geometry. Our main contributions are:
\begin{itemize}
    \item We employ for the first time a deep generative model in the voxel domain to estimate the occupancy probabilities sequentially using a masked 3D convolutional network. The conditional distribution is then used to model the context of a context-based arithmetic coder.
    \item We propose an optimal rate-driven partitioning and context selection algorithm. The partitioning algorithm adapts to the point cloud sparsity by employing a hybrid octree/voxel representation while the context to encode each block is expanded to the neighboring blocks and the expansion size is optimally selected.
    \item  We propose specific data augmentation techniques for 3D point clouds coding, to increase its generalization capability.
\end{itemize}
     
% The encoded PC bitstream consists of the partitioning signal in the form of octree, which is sent to the decoder as side information, as well as the occupied voxel blocks entropy-encoded using the distributions estimated by our context model.
%    \item We expand the context to the encoded voxels in the neighboring blocks to obtain a better probability estimation. The expansion size is adaptively selected during the block partitioning process.
\begin{figure*}
\centering
\captionsetup{justification=raggedright}
% \captionsetup{singlelinecheck = false, format= hang, justification=raggedright, font=small, labelsep=space}
\includegraphics[width=.95\linewidth]{figures_pcc/systemoverview.png}
\vspace{0.1cm}
\caption{Overview of the proposed method. (a): a $n$ bit depth point cloud is partitioned down to the $n-6$ octree level, yielding occupied blocks of size $64\times 64 \times 64$. (b): We encode each block of $64^3$ voxels as a single block (b1), or divide it into 8 children blocks (b2), depending on the total number of bits of each solution (partitioning level = 2). This procedure is repeated recursively for increasing partitioning levels up to 5. (c): For each occupied block of size $d $, the context model estimates the distribution of each voxel given the previously encoded voxels.  }
\label{fig:system overview}
\end{figure*}

\par We demonstrate experimentally that the proposed solution
outperforms the state-of-the-art MPEG G-PCC lossless codec in terms of bits per occupied voxel over a set of point clouds with varying density and content type. The rest of the paper is structured as follows: Section \ref{sec:stateoftheart} reviews the related work; the proposed method is described in Section \ref{proposedmethod}; Section \ref{performanceeval} presents the experimental results; and finally Section \ref{conclusion} concludes the paper.
%  

