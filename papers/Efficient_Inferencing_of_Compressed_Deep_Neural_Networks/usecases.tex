Today, a large number of Artificial Intelligence (AI) applications rely on using deep learning models for various tasks, such as, image classification, speech recognition, natural language understanding, natural language generation and so on.
Due to the significant improvement in performance achieved by the deep learning models, there is a natural trend to use these models on the applications running on mobile phone and other edge devices in the context of IOT (Internet of Things). 
For example, more and more people now want to take pictures using their mobile phones and get information on the building and surroundings around them in a foreign place. Usage of voice based assistants on mobile phones and other home devices is another increasing trend. 
Applications in the area of augmented reality involves continuous image recognition with results being reported on a VR display to provide more information regarding the environment to the individual. 
For example, in security, this can be used for identity detection.
Similarly, in self-driven cars, deep learning models are used
to inference in real-time using data collected from a combination of sensing technologies including visual sensors, such as cameras, and range-to-object detecting sensors, such as lasers and radar. 
Increased instrumentation in various industries such as agriculture, manufacturing, renewable energy and retail generates lot structured and unstructured data which preferably needs to be analyzed at the edge device and so that real-time action can be taken.

For the scenarios described above, inferencing can be done either on the cloud (or server) or on the edge device itself. However, offloading  inferencing to  the cloud can be impractical in lot of situations due to  wireless  energy  overheads, turn-around latencies and data security reasons. On the other hand, given the sheer size of the deep learning models, inferencing on mobile/edge devices poses other kind of challenges on resources, such as memory, compute and energy which need to be utilized efficiently while continuing to provide high accuracy and similar latency.

%phone applications

Even when inferencing is done on the cloud, resources have to be efficiently utilized to keep the cost of inferencing minimum for the cloud vendor as the cost of inferencing is directly dependent on resource utilization. Just as an example, a vendor providing "Inferencing as a service" for image classification may want to keep hundreds of deep learning models customized for various domains and users in memory in order to provide the low response time. This calls for storing compressed models in-memory and directly inferencing using the compressed model when the requests come in. All of this has to be done without compromising on the latency and accuracy of the inferencing.

%1) inferencing on mobile phones without compromising latency and accuracy 
%2)  inferencing over a large cluster of customized models on cloud "inferencing as a service" 
%3) inferencing over edge devices in the context of IOT 
%4) inferencing on multiple models (speech, vision, ..) on driver less cars in limited memory where latency is critical
