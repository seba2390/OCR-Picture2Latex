The large number of weights in powerful and complex neural networks makes them difficult to be deployed on embedded systems 
with limited hardware resources/ mobile systems. 
Large networks do not fit in on-chip storage  and are stored in external DRAM: thus they 
need to be  fetched every time for inference of each image, word, or speech sample, leading to 
large consumption of energy. For instance,  Han et al. ~\cite{HanMD15}
states that the energy cost per  fetch ranges from 5pJ for 32bit coefficients in on-chip SRAM to 640pJ for 32b coefficients in off-chip
LPDDR2 DRAM:  thus running a 1 billion connection neural network,  at 20Hz would require  12.8W just for DRAM access - 
this prohibits the inferencing on a  typical mobile device.
A similar issue arises if the inferencing is done on cloud: the complex and large DNNs need to be loaded before the inferencing can be done, thus requiring more
memory and cost.

To address this issue, there has been significant work to reduce the size of networks.  
The objective in the ideal scenario is to get a model of smaller size, with limited loss of accuracy in the prediction, and no sacrifice in the inference time.
Model compression can be  effected  through a combination of pruning, weight sharing and encoding of the connection weights.
In the pruning step,  the network is pruned  by removing the redundant connections of the network.
Next, the weights are quantized so that multiple connections share the same weight, thus only the codebook (effective weights) and the indices need to be stored. 
The codebook is generally of small size, and hence the indices can be represented with fewer bits than that required for the original weights.
Finally, some encoding (like Huffman coding) is done to take advantage of the biased distribution of effective weights to further reduce the model size.


Neural network pruning was pioneered even in the early development of neural networks (~\cite{Reed93}).
Anwar et  al.~\cite{AnwarHS17} and Molchanov et  al ~\cite{MolchanovTKAK16} employ structured pruning  at the level of feature maps and kernels. The advantage of this 
scheme of pruning is that the resultant connection matrix can be considered dense. However this is more suited for 
convolution layers. Song Han et  al.~\cite{HanMD15} have considered the weight based pruning where they remove all connections whose weight is lower than a threshold. 
Their pruning strategy (along with quantisation and Huffman encoding) was able to get the model size of AlexNet reduced  from 240MB to 6.9MB, 
and that of VGG-16 from 552MB to 11.3MB, without loss of accuracy on Imagenet dataset.
A lot of work in literature is available for weight sharing and quantisation as well. 
Half-precision networks (Amodei et al., ~\cite{AmodeiABCCCCCCD15}) cut sizes of neural networks in half. XNOR-Net (Rastegari et al., ~\cite{RastegariORF16}), 
DoReFa-Net (Zhou et al., ~\cite{ZhouNZWWZ16}) and network binarization (Courbariaux et al.~\cite{CourbariauxB16}; Lin et al.~\cite{LinCMB15}) use aggressively quantized weights, activations and gradients to further reduce computation during training, however, the extreme compression rate comes with a loss of accuracy. Hubara et al.~\cite{HubaraCSEB16} and Li  Liu~\cite{LiL16} propose ternary weight networks to trade off between model size and accuracy.
Zhu et  al.~\cite{ZhuHMD16}  propose trained ternary quantization which uses two full-precision scaling coefficients for each layer, where these coefficients are trainable parameters.
Gong. et  al.~\cite{GongLYB14} consider vector quantization methods for compressing the parameters of CNNs.
HashedNets~\cite{ChenWTWC15} uses hash function to randomly group connection weights into hash buckets, so that all connections within the same hash bucket share a single parameter value. 


In this work,  we consider the compression strategy as suggested by  Han et  al.~\cite{HanMD15, HanPTD15}. Since 
all connections with weights below a threshold are removed from the network, the pruned network is a 
sparse structure that is stored using compressed sparse row (CSR) or compressed sparse column (CSC) format.
The model is further compressed by storing the  index difference instead of the absolute position, and encoding this difference in 
$k$ bits for each layer: for an index difference larger than the bound,  zero padding is employed. In our work, we fix $k$ to 4.
Finally Huffman encoding is employed on both the weight clusters and the index differences to ensure that most common cluster centres and  index differences
are represented with fewer bits. It is observed that Huffman encoding adds to 25\% additional
compression to the  model compressed without the encoding.  

The real challenge with compressed models lies in processing them for inferencing.
As stated before, with pruning the matrix becomes sparse and the indexes become relative. 
With weight sharing,  a short (8-bit) index for each weight is stored. More indirection is added with Huffman encoding.
This causes complexity and inefficiency to process the model on CPUs and GPUs.
The trivial method of exploding the matrix back to dense and doing the computation is not a good choice because of the 
excessive memory usage and the running time. 
Previous work has considered hardware and software accelerators to facilitate computation on compressed models.
Han et  al.~\cite{HanLMPPHD16} has proposed EIE, an efficient inference engine, that performs
customized sparse matrix vector multiplication and handles weight sharing with no loss of efficiency. 
However this requires  specialized hardware to be implemented to act as the accelerator. On the software side,
Intel Math Kernel Library (MKL~\cite{mkl}) provides  sparse solver for matrix-matrix and matrix-vector multiplications,
however this does not incorporate relative indexing and Huffman encoding, which are necessary for the compressed models.

 
This paper makes the following contributions: 
1) To the best of our knowledge, we are the first to develop fast kernels for sparse matrix-vector and matrix-matrix multiplications, where the 
sparse matrix is indexed using relative indices, and encoded with Huffman encoding
2) We have performed a comprehensive evaluation of different schemes,  which has shown that the best choice of the scheme depends on the
sparsity and the encoding scheme.
3) We have performed experiments on popular deep learning networks like AlexNet, VGG-16, to show that our method achieves x factor boost in 
the inference time, while maintaining the memory constraints.





 
 














