\subsection{Inferencing as matrix computations}

A fully-connected (FC) layer of a deep neural network (DNN) performs the computation as
\begin{equation}
b = Wa+v.
\label{eqn:fc1}
\end{equation}
where $a$ and $b$ are respectively the input activation vector and the output activation vector, $v$ is the bias, $W$ is the weight matrix.
%The output $b$ of the FC layer is generally fed next to the Rectified Linear Unit(ReLU), which applies a non-linear function to $b$.
The output activations of Equation ~\ref{eqn:fc1} is computed element-wise as:
\begin{equation}
b_i = \sum_{j=0}^{n-1}W_{ij}a_j+v_i.
\label{eqn:fc2}
\end{equation}

For a typical FC layer like FC7 of VGG-16 or AlexNet, the activation vectors are 4K long, and the weight matrix is 4K x 4K (16M weights). 
Weights are represented as single-precision floating-point numbers so such a layer requires 64MB of storage. Similarly for FC6 layer of 
AlexNet the weight matrix is of dimension 4096 x 9216, for  FC6 layer of 
VGG-16 the weight matrix is of dimension 4096 x 25088.


The computation of a convolution (CONV) layer of a CNN can also be expressed as a matrix-matrix multiplication operation.
The input activation for the CONV layer is a 3-dimensional tensor.
The convolution layer's parameters consist of a set of learnable filters (or kernels), which have a local connectivity 
along width and height  in the input,
but extend through the full depth of the input volume. Each filter is convolved across the width and height of the input volume, computing the dot product between the entries of the filter with the input and producing a 2-dimensional activation map of that filter. 
Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. 
The dot products between the filters and local regions of the input, can be formulated as a matrix-matrix multiplication, by 
flattening out the local regions of the input to individual columns
% by {\em im2col},
and the layer weights to rows: the result of a convolution is now equivalent to performing one large matrix which evaluates the dot product between every filter and every receptive field location.
See~\cite{deep_learn}
for details.

%The 3-dimensional input is converted to a 2-dimensional matrix by taking each 
%three-dimensional cube within the input where each kernel is to be applied, and flattening it out 
%as a single row of the matrix. The same is done for each kernel’s weights, serializing the 3D cubes into columns as the second matrix 
%for the multiplication. 

\subsection{Representation of compressed model}
As stated before,  this paper considers the compression technique, as suggested by  Han et  al.~\cite{HanMD15, HanPTD15}
to reduce the  size of the DNNs without loss of accuracy, obtained through a combination of pruning, weight sharing and Huffman encoding.
Pruning makes weight matrix $W$ sparse:
%, and let $D$ be the sparsity of $W$.
the pruned matrix $W$ is stored  in a variant of the standard compressed row storage (CSR) format.
The standard  CSR representation works as follows:
instead of storing the entire matrix $W$ of dimension $r$ x $c$,  
vectors, one of  floating-point numbers ({\em val}), and the other two of integers ({$col\_ind$, $row\_ptr$) are used. The $val$ vector stores the values of the nonzero elements of $W$, as they are traversed in a row-wise fashion.  The $row\_ptr$ vector stores the locations in the $val$ vector that start a row, that is, if $val(m)=W_{ij}$ then $row\_ptr(i) \leq m < row\_ptr(i+1)$. By convention, we define $row\_ptr(r+1) =nnz+1$, where $nnz$ is the number of nonzeros in $W$. The $col\_ind$ vector is used to store the column indexes of the elements in the val vector.   
 Figure~\ref{fig:repr_f2} shows the CSR representation for the matrix given in 
Figure~\ref{fig:repr_f1}.

\begin{figure*}[!tbp]
  \centering
  \subfloat[Sparse Matrix.]{\includegraphics[width=1.7in]{figures/matrix.pdf}\label{fig:repr_f1}}
  \hfill
  \subfloat[CSR Format.]{\includegraphics[width=1.7in]{figures/csr.pdf}\label{fig:repr_f2}}
  \hfill
  \subfloat[Relative Column Index.]{\includegraphics[width=1.7in]{figures/csr_rel.pdf}\label{fig:repr_f3}}
  \hfill
  \subfloat[Quantized Weight.]{\includegraphics[width=1.7in]{figures/csr_quant.pdf}\label{fig:repr_f4}}
  \hfill
  \subfloat[Huffman Encoded Model.]{\includegraphics[width=2.8in]{figures/huff.pdf}\label{fig:repr_f5}}
  \caption{Representation of a compressed model.}
\end{figure*}

The $col\_ind$ vector can be further compressed by making each of its entries exactly $k$ bits. This is achieved by modifying   $col\_ind$ as follows:
if $val(i)$ is the first non-zero entry of any particular row, then $col\_ind(i)$ is set to the corresponding column number; else 
$col\_ind(i)$ is set to the  number of columns between the current non-zero and the last non-zero entry of the row.
If more than $2^k$ zeros appear before a non-zero entry, we add a zero in both the $val$ and the $col\_ind$ vectors.
This representation format is the CSR with relative column index.
Figure~\ref{fig:repr_f3} shows the relative indexed CSR of  Figure~\ref{fig:repr_f2} with $k=2$. Since the first non-zero column of second row exceeds 4, 
we pad a zero at the fourth location
of both $val$ and $col\_ind$.  
Further compression is effected using quantization where similar valued non-zero entries of $val$ are clustered together to share the same 
value. If $r$ bits are used for quantization, we use at most ($2^r-1$) distinct  non-zero values along with 0 in quantized values, and  each entry of the $val$ vector is  a $r$ bit index
to the corresponding cluster centre. The cluster centre values are stored in the codebook. 
Figure~\ref{fig:repr_f4} shows the quantized model representation (quantization here is done using $2$ bits),  entries 
denoted by the same colour in the matrix of  Figure~\ref{fig:repr_f1} are mapped to a single cluster,  and each entry of the $val$ vector is  a $2$ bit index
to the corresponding cluster centre. The  codebook is also shown. 
Finally Figure~\ref{fig:repr_f5} shows the Huffman encoded bit representations of $val$ and $col\_ind$ vectors. 
Clearly,  entry $i$ of the $ row\_ptr$ array will a 2-tuple, the first field storing the starting address for row $i$ in $val$, while the 
second stores  the starting address for row $i$ in 
$col\_ind$.





%bi =ReLU  ∑ S[Iij]aj  (3) j?Xi?Y
%Where Xi is the set of columns j for which Wij ?= 0, Y is the set of indices j for which aj ?= 0, Iij is the index to the shared weight that replaces Wi j , and S is the table of shared weights. %Here Xi represents the static sparsity of W and Y represents the dynamic sparsity of a. The set Xi is fixed for a given model. The set Y varies from input to input.



