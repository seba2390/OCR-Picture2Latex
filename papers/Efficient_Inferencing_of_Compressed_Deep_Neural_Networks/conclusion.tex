In this paper, we study efficient inferencing using compressed models under memory constraints. 
We propose parallel algorithms  that can use tuned math libraries available on the platform to perform inferencing efficiently
with compressed models that rely on pruning, quantization, relative indexing and encoding techniques for compression.
We study different blocking  schemes and study the effect of block sizes on  the layer of the network, its sparsity and the batch size used for AlexNet and VGG-16.
We observe that in a typical neural network inference, different layers have different sized activation memory required; thus it is beneficial
to use variable sized batching across different layers. We propose a novel dynamic programming based algorithm that figures 
out the optimal batch size for throughput maximization for the case where the batch size used for inferencing in individual layers
is a monotonically increasing sequence, 
i.e., where larger batch sizes  can be used for layers closer to the output. We show that our dynamic programming solution 
achieves 15-25\%   performance improvement  in the inference throughput over the solution employing fixed batch size  across layers.
A future work in this direction is to relax the assumption of monotonically increasing batch sizes. Our results are applicable in training of neural network models as well.
There has been recent effort for employing compressed  models in reducing training time: our 
techniques, e.g dynamic batching,  will be useful here for designing a faster forward phase.
