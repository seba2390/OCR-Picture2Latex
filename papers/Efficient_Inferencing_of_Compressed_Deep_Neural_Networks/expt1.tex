In this section, we present the experimental results for our block inferencing procedure. We begin by specifying the system configurations and the dataset.

\subsection{System and Dataset}

For running our experiments (also the ones in Section~\ref{sec:expt2}), we have used Intel Xeon CPU E5-2697 system. It has two NUMA nodes 
with 12 cores, each with frequency of 2.70GHZ. The system has 32KB, 256KB and 30MB of L1, L2 and L3 cache respectively.
We consider compressed models for two popular deep neural networks, AlexNet and VGG-16. 
For each of these models we consider the compressed configurations corresponding to four different pruning percentages.
The first configuration corresponds to the procedure applied in  \cite{HanMD15}.  
Table~\ref{tab:alex_pr} and Table~\ref{tab:vgg_pr}
 present the pruning percentages of all the layers in this configuration. We refer to this configuration as 
{\em conventional} in subsequent discussion.
The compressed model sizes of AlexNet and VGG-16 for this configuration are respectively 6.81 MB and 10.64 MB.
The other three configurations correspond respectively to 70\%, 80\% and 90\% pruning of {\em all} the layers of the network.  
We consider these configurations to study how our scheme performs for a wide range of sparsity spectrum of the compressed models.
8 bit (5 bit) quantization for CONV (FC) layers
and 4 bit (5 bit) relative indexing for AlexNet (VGG-16) is employed for all the configurations.

\begin{table}[!tbp]
  \centering
  \subfloat[AlexNet]{
\begin{tabular}{|c|c|}
			\hline 
			Layer & Pruning \% \\ \hline 
			conv1 & 16 \\ \hline 
			conv2 & 62 \\ \hline 
			conv3 & 65 \\ \hline 
			conv4 & 63 \\ \hline 
			conv5 & 37 \\ \hline 
			fc6 & 91  \\ \hline 
			fc7 & 91  \\ \hline 
			fc8 & 75  \\ \hline
        \end{tabular}
        \label{tab:alex_pr}
  }
  \hspace{1mm}
    \subfloat[VGG-16]{
\begin{tabular}{|c|c|m{2em}|m{2em}|}
			\hline 
			Layer & Pruning \% \\ \hline 
			conv1\_1 & 42  \\ \hline
			conv1\_2 & 78 \\ \hline
			conv2\_1 & 66 \\ \hline
			conv2\_2 & 64 \\ \hline
			conv3\_1 & 47 \\ \hline
			conv3\_2 & 76 \\ \hline
			conv3\_3 & 58 \\ \hline
			conv4\_1 & 68 \\ \hline
			conv4\_2 & 73 \\ \hline
			conv4\_3 & 66 \\ \hline
			conv5\_1 & 65 \\ \hline
			conv5\_2 & 71 \\ \hline
			conv5\_3 & 64 \\ \hline
			fc6 & 96  \\ \hline
			fc7 & 96  \\ \hline
			fc8 & 77  \\ \hline
        \end{tabular}
        \label{tab:vgg_pr}
  }
  \caption{Compressed AlexNet and VGG-16 models.}
\end{table}


%
%\begin{table}[!htb]
%    \caption{Properties of compressed AlexNet and VGGNet Models}
%    \begin{minipage}{.5\linewidth}
%      \caption{AlexNet}
%      \centering
%        \begin{tabular}{|c|c|}
%			\hline 
%			Layer & Pruning \% \\ \hline 
%			conv1 & 16 \\ \hline 
%			conv2 & 62 \\ \hline 
%			conv3 & 65 \\ \hline 
%			conv4 & 63 \\ \hline 
%			conv5 & 37 \\ \hline 
%			fc6 & 91  \\ \hline 
%			fc7 & 91  \\ \hline 
%			fc8 & 75  \\ \hline
%        \end{tabular}
%    \end{minipage}%
%    \begin{minipage}{.5\linewidth}
%      \centering
%        \caption{VGGNet}
%        \begin{tabular}{|c|c|m{2em}|m{2em}|}
%			\hline 
%			Layer & Pruning \% \\ \hline 
%			conv1\_1 & 42  \\ \hline
%			conv1\_2 & 78 \\ \hline
%			conv2\_1 & 66 \\ \hline
%			conv2\_2 & 64 \\ \hline
%			conv3\_1 & 47 \\ \hline
%			conv3\_2 & 76 \\ \hline
%			conv3\_3 & 58 \\ \hline
%			conv4\_1 & 68 \\ \hline
%			conv4\_2 & 73 \\ \hline
%			conv4\_3 & 66 \\ \hline
%			conv5\_1 & 65 \\ \hline
%			conv5\_2 & 71 \\ \hline
%			conv5\_3 & 64 \\ \hline
%			fc6 & 96  \\ \hline
%			fc7 & 96  \\ \hline
%			fc8 & 77  \\ \hline
%        \end{tabular}
%    \end{minipage} 
%    \label{tab:compression-params}
%\end{table}
%
\subsection{Blocking results}

Our first set of experiments is aimed to study the effect of variation of  block sizes on the inference time (both the decoding time and  
the computation time) for individual layers corresponding to the different configurations of the  compressed models. 
 Figure~\ref{fig:blck_f1} and ~\ref{fig:blck_f2}
show the decoding time, computation time and total time, with different block sizes for FC6 layer of AlexNet and VGGnet,
using batch size of 16. The models used for these runs  correspond to the conventional configuration. All these experiments employ MKL with 4 threads
for computation.



We observe that for very small block sizes,  the decoding and the computation time are pretty high due to overhead of the too many function calls.
For very large block sizes, the level of parallelism gets limited, leading to increase in the inference time.
Figure~\ref{fig:blck_f3} and ~\ref{fig:blck_f4} show the same charts with batch size of 256. We note that for smaller batch size, the total time is dominated by the decoding time, 
whereas the computation time takes over at larger batch sizes. However the nature of variation of  inference time with the block size is consistent across batch sizes.
We observe similar nature of plots for other configurations and batch sizes as well.



\begin{figure*}[!tbp]
  \centering
  \subfloat[AlexNet Batch size16]{\includegraphics[width=1.7in]{figures/alex16.pdf}\label{fig:blck_f1}}
  \hspace{1mm}
  \subfloat[VGG-16 Batch size 16]{\includegraphics[width=1.7in]{figures/vgg16.pdf}\label{fig:blck_f2}}
  \hspace{1mm}
  \subfloat[AlexNet Batch size 256]{\includegraphics[width=1.7in]{figures/alex256.pdf}\label{fig:blck_f3}}
  \hspace{1mm}
  \subfloat[VGG-16 Batch size 256]{\includegraphics[width=1.7in]{figures/vgg256.pdf}\label{fig:blck_f4}}
  \caption{Inference Time Variation with Block Size.}
\end{figure*}




We also note that the working memory increases with increase in block size. 
Table~\ref{tab:workmem} presents the working memory required for matrix matrix multiplication for FC6 layer of AlexNet and VGG-16.
Since there is not significant difference in the inference timings between block sizes in range 128 x 128 to 1024 x 1024, we fix 128 x 128 as our block size
for the subsequent experiments.



\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
           \hline
Blocksize & AlexNet  & VGG-16 \\ \hline
  16  x   16 &    1.26KB &  0.92KB \\ \hline
  32  x   32 &    4.57KB &   3.42KB \\ \hline
  64  x   64 &   17.33KB &  12.97KB \\ \hline
 128  x  128 &   67.40KB &  50.22KB \\ \hline
 256  x  256 &  265.78KB & 197.26KB \\ \hline
 512  x  512 &    1.03MB & 781.52KB \\ \hline
1024  x 1024 &    4.11MB &   2.98MB \\ \hline
2048  x 2048 &   14.76MB &  11.42MB \\ \hline
4096  x 4096 &   36.88MB &  42.38MB \\ \hline

\end{tabular}
\caption{Working Memory Requirement for FC6 layer}
\label{tab:workmem}

\end{table}




We next observe  the variation of activation memory requirement and the inference time with batch sizes. Table~\ref{tab:batch}   presents
the results for batch sizes of 16 and 256. Clearly, for a fixed batch size,  the activation memory required by the convolution layers is  more than  that of the 
fully-connected layers.  Inferencing applications on a low resource system generally come with a cap on the available memory. 
Suppose we consider a fictitious scenario where the maximum available memory is 20MB. 
From Table~\ref{tab:batch}, it makes sense to run the fully connected layers with batch size 256, since the memory required is well below the
permissible threshold, and there is significant increase in throughput if we process in batch of 256. 
For the convolution layers, however, processing in batch of 256 is not a desirable option because of the large memory overhead.
This motivates us to use different batch sizes for different layers during the inferencing. We present this in more detail in the next section.

 


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
      & \multicolumn{2}{c|}{Memory (MB)} & \multicolumn{2}{c|}{Time (ms)} \\ \hline
Layer  & batch-size & batchsize & batchsize& batchsize \\
 &16 & 256 & 16 & 256 \\ \hline
conv1  &  17.72 & 283.59 &  349.93 &  5408.93 \\ \hline
norm1  &  17.72 & 283.59 &  98.87 & 1597.83 \\ \hline
pool1  &  4.27 &  68.34 & 11.68 & 176.42 \\ \hline
conv2  &  11.39 & 182.25 &  341.72 &  5745.49 \\ \hline
norm2  &  11.39 & 182.25 &  68.06 & 1081.80 \\ \hline
pool2  &  2.64 &  42.25 & 7.12 &  116.49 \\ \hline
conv3  &  3.96 &  63.38 & 153.11 &  2573.47 \\ \hline
conv4  &  3.96 &  63.38 & 204.01 &  3135.62 \\ \hline
conv5  &  2.64 &  42.25 & 135.66 &  2242.94 \\ \hline
pool5  &  0.56 &  9.00 &  1.92 &  25.72 \\ \hline
fc6  &  0.25 &  4.00 &  51.77 & 112.62 \\ \hline
fc7  &  0.25 &  4.00 &  21.06 & 46.61 \\ \hline
fc8  &  0.06 &  0.98 &  9.66 &  22.61 \\ \hline

\end{tabular}
\caption{Memory Requirement and Inference time for AlexNet individual layers}
\label{tab:batch}
\end{table}


