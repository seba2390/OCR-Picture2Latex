\newcommand{\OUT} {{\rm OUT}}
\newcommand{\IN} {{\rm IN}}
\newcommand{\WS} {{\rm WS}}
\newcommand{\Time} {{\rm Time}}
\newcommand{\OPT} {{\rm OPT}}
\newcommand{\TOT} {{\rm TOT}}

It is clear from the results shown in the previous section that  using a larger batch for inferencing increases the 
throughput as computing
resources are utilized more efficiently.
However, an issue with inferencing larger batches is the increase
in inferencing latency 
(due to wait time while assembling a batch, and because
larger batches take longer to process). 
Moreover the memory requirement for the input and the output
activations and buffer memory also increases for larger batch size.
Thus applications work with large batch sizes while keeping
the latency and memory utilization within certain thresholds.
The problem becomes more challenging since the 
available memory varies dynamically depending on the system load;
hence the  batch size for achieving the maximum throughput can be figured out only at the time of inferencing.
Moreover, the memory requirement and the computation time for 
inferencing varies with the layers even for a fixed batch size. 
Thus it might be advantageous to do the inferencing using different 
batch sizes for different layers.
We address this issue by proposing a dynamic programming based algorithm
for determining variable batch sizes for different layers for efficient
inferencing.
We describe our dynamic program below.

\subsection{Dynamic Programming}
Let $L_1$, $L_2$, $\cdots$, $L_f$ denote the layers of the DNN. 
For $i$ = 1,  2, $\cdots$, $f$, let \Time$(i, B)$ denote the time required to perform the inferencing computations for layer $L_i$
of the DNN using a batch size of $B$.  
Next, we let \IN$(i,B)$ and \OUT$(i,B)$ respectively denote 
the input activation and output activation memory required
to perform inferencing of layer $L_i$ with a batch size of $B$. 
Further, let \WS$(i)$ denote the size of the temporary workspace  required for layer $L_i$ computations (for instance this includes
the buffer memory required to decode blocks of the connection matrix for $L_i$). 
All the values \IN$(i,B)$,  \OUT$(i,B)$, \WS$(i)$ and \Time$(i, B)$
are obtained once for a given compressed model.
Note that the total memory required to perform inferencing computations for layer $L_i$ with a batch size of $B$ is captured by
$$ \IN(i,B) + \WS(i) +  \OUT(i,B). $$
Let {\TOT} denote the total memory available for performing the inference
computations for the entire model.

We now describe the dynamic program to determine the optimal batch size
to be used at all the individual layers in order to maximize the overall
throughput of the inferencing.
For this we define a configuration:
a configuration is a tuple $\langle i, B, A \rangle$,
where $i$ denotes the layer $L_i$, $B$ denotes a batch size and 
$A$ denotes amount of memory.
We maintain a dynamic program table {\OPT}. 
An entry \OPT$(i, B, A)$ of the dynamic program denotes the minimum time to perform the inferencing computations for layers $L_1$-$L_i$, when a batch size of 
$B$ is used for layer $L_i$, and $A$ units of memory (out of {\TOT}) are not available for performing the inferencing computations for layers $L_1$-$L_i$ (this memory is reserved for performing inferencing computations from layer $L_{i+1}$ to layer $L_f$).
Thus, we only have available ({\TOT} - $A$) units of memory
for inference computations of layers $L_1$ to $L_i$.

We say that configuration $\langle i,B,A \rangle$ is {\em feasible} if
the total memory required for performing inferencing computations at
layer $L_i$ with a batch size of $B$ is within the available memory bound,
i.e.,
$$A + \IN(i,B) + \WS(i) +  \OUT(i,B) \leq \TOT.$$

We now describe the recurrence relation for computing the 
entries of the dynamic programming table \OPT$(\cdot, \cdot, \cdot)$.
For simplicity, we assume that for every $i$, the batch size used for inferencing computations at layer $L_{i-1}$ is no more than
the batch size used for the inferencing computations at layer $L_i$.
Clearly,  \OPT$(i, B, A)$ can be finite only if  $\langle i,B,A \rangle$ is feasible.
Suppose that layer $L_i$ is computed with batch size $B$ and layer $L_{i-1}$ with a batch size $b$. 
For simplicity, we consider all $b \leq B$ such that $b$ divides $B$.
For a given $b$, 
the inferencing computations for layer $L_{i-1}$
will be performed in $ (B/b)$ phases, wherein in each phase
a batch of size $b$ will be processed up to layer $L_{i-1}$.
After the end of these phases, the $B$ output activations of 
layer $L_{i-1}$ will be fed as input activations to layer $L_i$.
Note that before the processing of the last of these phases,
\IN$(i,B-b)$ amount of output activation need to be buffered. 
Thus the total memory available for processing up to
layer $L_{i-1}$ gets reduced by \IN$(i, B-b)$ as this is required
for storing the activations before processing layer $L_i$.

We are now ready to present the recurrence relation.
For any $i > 1$, 
\begin{equation*}
\begin{split}
\OPT(i, &B, A)   =  \Time(i,B) \quad \quad  +   \\
 &\quad  \min \limits_{b \leq B} \left \{  (B/b) * \OPT(i-1,b,A+\IN(i,B-b)) \right \}\\
 &\quad \mbox{subject to} \quad \quad  \text{ $\langle i,B,A \rangle$ is feasible}
\end{split}
\end{equation*}



For the base case i.e., for $i$ = 1.
\begin{equation*}
  \OPT(1, B, A) =\begin{cases}
     \Time(1, B), & \text{if $\langle 1,B,A \rangle$ is feasible}.\\
    \infty, & \text{otherwise}.
  \end{cases}
\end{equation*} 





 
The maximum throughput for the inferencing is obtained 
by considering the configuration that yields the 
minimum inference time per input which is 
$$\min \limits_{ B} \frac{ \OPT(f, B, 0)}{B}. $$ 



The above dynamic program can be easily extended to ensure that the latency of inferencing is always less than some specified threshold. In the recurrence relation,
if \OPT$(i, B, A)$ exceeds the threshold value for some $i$, $B$, and $A$, we make   \OPT$(i, B, A)$ $\leftarrow$ $\infty$. This makes sure that our optimal solution 
never has larger latency.

\subsection{Additional Storage and Computation Overhead}
The table \OPT$(\cdot, \cdot, \cdot)$ needs to be evaluated for each entry in order to figure out the individual layer batch sizes that maximise the overall throughput.
We  now figure out the additional storage and computation that is needed for the dynamic programming space complexity for standard networks like AlexNet. 
The total number of layers in AlexNet  is 14. For requested input count of 64, we consider batch sizes 
in range 1 to 64 for the second dimension. For the case where the additional available memory is twice of the model size, the third dimension is considered from 0 to 14MB
in steps of {100KB}. Thus the total size of the table is around 500KB.
Each entry computation of the table computes the minimum over a set of possible batch sizes. Thus the computation complexity is at most $\cal B$ times the size of the table,
where $\cal B$ is the maximum number of distinct batch sizes considered.

We begin our  inferencing  with a pre-processing step, which computes the  individual layer batch sizes that maximise the overall throughput using the
above dynamic program.  The actual inferencing uses the batch sizes outputted from the dynamic program.










