In this section, we validate the results of our dynamic programming algorithm on practical test cases with AlexNet model.
Suppose the user requests for inference of a set of $K$ images, and is interested to get the maximum throughput for the inference.
We consider the scenarios where the total  memory in the system (in addition to the model) is 1.5x, 2x and 2.5x  times the model size.
Our baseline is selecting a fixed batch size such that (i) running any layer of inferencing using that batch size does not violate the
memory constraints (ii) out of all possible batch sizes which satisfy	 (i),  the baseline returns the batch size with maximum throughput.
We compare this baseline from our dynamic programming output, which uses variable batch sizes for different layers.
We perform our experiments $K =  32, 64, 128$ and with all the four configurations of AlexNet model (conventional pruning and
70\%, 80\%, 90\% pruning).
Figure~\ref{fig:real_dp1} - ~\ref{fig:real_dp3} compares the results of our dynamic program algorithm with the baseline 
(fixed batch size) output for AlexNet with conventional pruning. The x-axis shows the additional memory available (w.r.t to the model size)
over the model, and the y-axis plots the total time to infer K images. Our results show that the dynamic programming approach
improves the throughput  by 15-25\% over the fixed batch size approach.

\begin{figure*}[!tbp]
  \centering
  \subfloat[K=32]{\includegraphics[height = 1.3in, width=2in]{figures/DP_real_32.pdf}\label{fig:real_dp1}}
  \hspace{3mm}
  \subfloat[K=64]{\includegraphics[height = 1.3in, width=2in]{figures/DP_real_64.pdf}\label{fig:real_dp2}}
  \hspace{3mm}
  \subfloat[K=128]{\includegraphics[height = 1.3in, width=2in]{figures/DP_real_128.pdf}\label{fig:real_dp3}}
  \caption{Fixed batch size (baseline) time vs Time outputted from Dynamic Programming for AlexNet with conventional pruning.}
\end{figure*}

 
   

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
 Layer & 1.5x & 2x & 2.5x \\ \hline
 conv1 & 2 & 4 & 6    \\ \hline
 norm1 & 4 & 4 & 6    \\ \hline
 pool1 & 4 & 4 & 6    \\ \hline
 conv2 & 4 & 4 & 6    \\ \hline
 norm2 & 4 & 4 & 6    \\ \hline
 pool2 & 4 & 4& 6    \\ \hline
 conv3 & 4 & 4 & 6    \\ \hline
 conv4 & 4 & 4 & 6    \\ \hline
 conv5 & 4 & 4 & 6    \\ \hline
 pool5 & 4 & 4 & 32    \\ \hline
   fc6 & 64 & 64 & 60    \\ \hline
   fc7 & 64& 64 & 60    \\ \hline
   fc8 & 64& 64 & 60    \\ \hline
\end{tabular}
\caption{Variable batching for AlexNet.}
\label{tab:dp_path}
\end{table}


\begin{figure*}[!ht]
  \centering
  \subfloat[Pruning = 70\%]{\includegraphics[height = 1.3in, width=2in]{figures/DP_70_64.pdf}\label{fig:70_dp1}}
  \hspace{3mm}
  \subfloat[Pruning = 80\%]{\includegraphics[height = 1.3in, width=2in]{figures/DP_80_64.pdf}\label{fig:80_dp2}}
  \hspace{3mm}
  \subfloat[Pruning = 90\%]{\includegraphics[height = 1.3in, width=2in]{figures/DP_90_64.pdf}\label{fig:90_dp3}}
  \caption{Fixed batch size (baseline) time vs Time outputted from Dynamic Programming for AlexNet with 70\%, 80\% and 90\% pruning.}
\end{figure*}

Table~\ref{tab:dp_path} shows the dynamic programming output corresponding to the above run for K = 64.
 It is observed that the optimal inferencing scheme uses smaller batch sizes for the convolution layers (because of the larger memory overhead),
 and combines intermediate outputs to perform fully connected layer operations with larger batch sizes. This matches our intuition which motivated us to 
 develop the dynamic programming based solution. The dynamic programming solution  corresponding to column 2.5x picks 60 as the batchsize for final layers: thus for this case, we again compute the solution for requested input of 4, and report the total time for inferencing.  The baseline corresponding to these runs use fixed batch size of 3, 5 and 7 for  additional memory
 of  1.5x, 2x and 2.5x respectively.
 
 Figure~\ref{fig:70_dp1} - ~\ref{fig:90_dp3} extends our   results to the other configurations of the AlexNet model, namely, the
70\%, 80\% and 90\% pruned models. We show these results for fixed K of 64.  Our results show that the dynamic programming approach
performs well  over the fixed batch size approach for these scenarios as well.

