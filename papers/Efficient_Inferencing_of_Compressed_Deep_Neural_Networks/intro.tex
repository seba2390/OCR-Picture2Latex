Deep neural networks have been used extensively over the last decade in applications ranging from computer vision~\cite{KrizhevskySH12} to speech recognition~\cite{GravesS05} and natural language processing~\cite{Collobert2011}.
In this paper, we focus particularly on convolutional neural networks (CNNs) which have %gained signification progress over the last decade and have
become ubiquitous  in object recognition, image classification, and retrieval.  Almost all of the recent successful recognition systems  are built on top of this architecture (see \cite{JiaSDKLGGD14, DonahueJVHZTD14, GongWGL14, ZeilerF14}).
A simple convolution neural network consists of a sequence of layers, with every layer of the network transforming one volume of activations to another through a differentiable function. 
Thus for an image classification problem, the  input image is transformed layer by layer from the original pixel values to the final class scores. 

Convolution networks comprise of different types of layers including convolution (CONV), fully connected layer (FC), pooling layer (POOL),  Rectified Linear Unit (ReLU)  etc. 
Of these, the CONV and the FC layers contain weights and biases, which are   parameters trainable over some data set.
Thus the  CONV/FC layers perform transformations that are  functions of not only the activations in the input volume, but also of the parameters of the respective layers (the weights and biases of the neurons). On the other hand, the RELU/POOL layers  implement  fixed functions. 


As datasets increase in size, so do the  number of layers in the CNNs and the number of parameters, in order to absorb the enormous amount of supervision. In 1998 Lecun et al. designed a CNN model LeNet-5 with less than 1M parameters to classify handwritten digits~\cite{Lecun98}, while in 2012, Krizhevsky et al.~\cite{KrizhevskySH12} won the ImageNet competition with 60M parameters and 8 layers (this correspond to the popular AlexNet network). Deepface classified human faces with 120M parameters~\cite{TaigmanYRW14}, and Coates et al.~\cite{CoatesHWWCN13} scaled up a network to 10B parameters. Karen Simonyan and Andrew Zisserman ~\cite{SimonyanZ14a} developed VGG-16 network with 16 layers
and 138M parameters.


The large number of weights in powerful and complex neural networks makes the models difficult to be deployed 
in low memory environments such as, mobile phones, IOT edge devices etc.
Large networks do not fit in on-chip storage  and are stored in external DRAM: thus they 
need to be  fetched every time for the inferencing of each test sample. This leads to multiple issues. Firstly, the inference time shoots up due to the overhead of external memory accesses. Secondly,
fetching the model from the external DRAM consumes large amount of energy. For instance,  Han et al.~\cite{HanMD15}
states that the energy cost per  fetch ranges from 5pJ for 32bit coefficients in on-chip SRAM to 640pJ for 32bit coefficients in off-chip
LPDDR2 DRAM:  thus running a 1 billion connection neural network,  at 20Hz would require  12.8W just for DRAM access - 
this prohibits inferencing on a typical mobile device.
A similar issue arises for ``inferencing as a service'' environment on the cloud: 
in this case the networks need to be loaded before the inferencing, thus increasing 
memory requirement and cost. It is therefore advisable to have the model reside permanently in the memory. Moreover many applications like visual recognition 
require multiple models for inferencing: thus it is feasible to have all the models loaded
apriori in memory only if the models are pretty small in size.

To address the above issues, significant work has been done to reduce the size of the networks.  
The objective in the ideal scenario is to generate a model of smaller size, with limited loss of accuracy in the prediction, and no sacrifice in the inference time.
Model compression can be  effected  through a combination of pruning, weight sharing and encoding of the connection weights.
In the pruning step,  the network is pruned  by removing the redundant connections of the network.
Next, the weights are quantized so that multiple connections share the same weight, thus only the codebook (effective weights) and the indices need to be stored. 
The codebook is generally of small size, and hence the indices can be represented with fewer bits than that required for the original weights.
Finally, some encoding (like Huffman coding) is performed to take advantage of the biased distribution of effective weights in further reducing the model size.

Neural network pruning has been pioneered even in the early development of neural networks (see ~\cite{Reed93}), and has been implemented through various strategies over the years.
Anwar et  al.~\cite{AnwarHS17} and Molchanov et  al.  ~\cite{MolchanovTKAK16} employ structured pruning  at the level of feature maps and kernels. The advantage of this 
scheme of pruning is that the resultant connection matrix can be considered dense. However their strategy is more suited for 
convolution layers. Song Han et  al.~\cite{HanMD15} have considered the weight based pruning where they remove all connections whose weights are lower than fixed threshold. 
Their pruning strategy (along with quantization and Huffman encoding) was able to get the model size of AlexNet reduced  from 240MB to 6.9MB, 
and that of VGG-16 from 552MB to 11.3MB, without loss of accuracy on Imagenet dataset.
A lot of literature is available for weight sharing and quantisation as well. 
Half-precision networks (Amodei et al., ~\cite{AmodeiABCCCCCCD15}) cut sizes of neural networks in half. XNOR-Net (Rastegari et al., ~\cite{RastegariORF16}), 
DoReFa-Net (Zhou et al., ~\cite{ZhouNZWWZ16}) and network binarization (Courbariaux et al.~\cite{CourbariauxB16}; Lin et al.~\cite{LinCMB15}) use aggressively quantized weights, activations and gradients to further reduce computation during training, however, the extreme compression rate comes with a loss of accuracy. Hubara et al.~\cite{HubaraCSEB16} and Li  Liu~\cite{LiL16} propose ternary weight networks to trade off between model size and accuracy.
Zhu et  al.~\cite{ZhuHMD16}  propose trained ternary quantization which uses two full-precision scaling coefficients for each layer, where these coefficients are trainable parameters.
Gong. et  al.~\cite{GongLYB14} consider vector quantization methods for compressing the parameters of CNNs.
HashedNets~\cite{ChenWTWC15} uses hash function to randomly group connection weights into hash buckets, so that all connections within the same hash bucket share a single parameter value. 


In this work,  we consider the compression strategy as suggested by  Han et  al. (see ~\cite{HanMD15, HanPTD15}). As stated before, their compression technique has gained significant popularity due to the very little loss in accuracy for a number of networks and datasets. Since 
all connections with weights below a threshold are removed from the network, the pruned network becomes a 
sparse structure that is stored using compressed sparse row (CSR) or compressed sparse column (CSC) format.
The model is further compressed by storing the  index difference instead of the absolute position, and encoding this difference in 
$k$ bits for each layer: for an index difference larger than $2^k$,  zero padding is employed. 
Finally Huffman encoding is employed on both the weight clusters and the index differences to ensure that more common cluster centres and  index differences
are represented with fewer bits. 


The real challenge with compressed models lies in processing them for inferencing.
Efficient inferencing using the compressed models has received little attention.
As stated before, with pruning the matrix becomes sparse and the indices are stored in the form of relative differences. 
With weight sharing,  a short (8-bit) index for each weight is stored. More indirection is added with Huffman encoding.
All of these increase the complexity of the inferencing process, making it inefficient on CPUs and GPUs.
The trivial method of uncompressing the matrix back to dense form and performing the inferencing in a standard framework
(like Caffe, Tensorflow etc) is not a good choice because of the 
excessive memory usage and the running time. 
Previous work has considered hardware and software accelerators to facilitate computation on compressed models.
Han et  al.~\cite{HanLMPPHD16} has proposed EIE, an efficient inference engine, that performs
customized sparse matrix vector multiplication and handles weight sharing with no loss of efficiency. 
However this requires  specialized hardware to be designed  to act as the accelerator. 
On the software side,
Intel Math Kernel Library (MKL~\cite{mkl}) provides  optimized sparse solvers for matrix-matrix and matrix-vector multiplications.
However it does not incorporate relative indexing and Huffman encoding, which are necessary to gain the desired compression levels.
Tensorflow  has recently incorporated Gemmlowp library (see ~\cite{gemmlowp}) for fast inferencing using 
eight-bit arithmetic rather than floating-point - however, this does not handle pruned and encoded models.

Another important aspect is the transition of  mobile devices  to multi-core CPUs. 
The current generation of mobile processors are being designed to deal with the increased number of high performance use cases. 
To satisfy the rapidly growing demand for performance and form factor sleekness, the industry has begun to adopt Symmetrical
Multiprocessing even on mobile systems. This calls for leveraging
multiple cores to facilitate faster inferencing even in low memory systems. 
Nvidia has studied and developed GPU-Based Inferencing  (see \cite{2015GPUBasedDL});
in a recent work Huynh et al.~\cite{Huynh2017} has proposed 
DeepMon, a mobile GPU based deep learning inference system for mobile devices. However all of these
work on uncompressed models. Thus
very little has been studied on parallel domain for compressed (in particular encoded) models.  

%https://www.nvidia.com/content/tegra/embedded-systems/pdf/jetson_tx1_whitepaper.pdf}

Another key factor is the batch size that should be used for inferencing on these limited resource systems.
It is well-known that larger batch size for inferencing increase both the throughput (since computing resources can be utilized more efficiently) and the latency. Thus inferencing applications strive to 
maximize a usable batch size while keeping latency under a given threshold.
The  maximum batch size is also determined by the amount of the available memory in the system. However, this varies dynamically depending on the system load.
Hence the  batch size for achieving the maximum throughput can only be figured out at the time of inferencing.

The focus of this paper is to study and propose optimizations for efficient inferencing compressed models under various resource limitations. 
Our main contributions are as follows:
\begin{itemize}
\item We propose a framework for inferencing of images with compressed models that rely on pruning, quantization, relative indexing and encoding techniques for compression. To the best of our knowledge, this is the first effort towards efficient inferencing using compressed models under memory constraints.
\item We propose parallel algorithms under this framework that can use tuned math libraries available on the platform to perform  efficient inferencing. Our framework uses different blocking schemes to optimize the inferencing time, wherein the best choice of the block size
depends on the layer of the network, its sparsity and the batch size used. 
%{\color{red} Remove this: Our algorithm can be extended for sparse matrix-vector and sparse matrix-matrix multiplications in general, where the 
%sparse matrix is indexed using relative indices, and encoded with Huffman encoding. This can have varied applications other than model inferencing.}
\item We show that variable size batching that performs inferencing on
a different number of activations in each layer can lead to better inferencing performance. To this effect, we develop a novel dynamic programming based algorithm to figure out the optimal batch size to be used in the inferencing for each individual layer under memory and latency constraints.
\item Our experimental results show that our approach of using variable batch size for inferencing 
achieves 15-25\%   performance improvement  in the inference throughput for AlexNet, while maintaining  memory and latency constraints.
\end{itemize}

%Moreover, as stated before, during inferencing
%every layer of the CNN transform one volume of activations to another through a differentiable function. 
%The memory requirement and the computation time for this transformation operation varies with the
%layers for even a fixed batch size. Thus it might be advantageous to do the inferencing using different batch sizes for different layers.
%This makes the problem of figuring out the optimal batch sizes even more challenging. 
%We address this issue by proposing a novel dynamic programming based algorithm.
%To summarise, this paper makes the following contributions: 


The rest of the paper is organized as follows.
In Section~\ref{motivation}, we motivate our problem by defining the challenges and the use cases. 
Section~\ref{sec:prelims} establishes  necessary preliminaries and concepts before we present our 
inferencing schemes in Section~\ref{sec:inference}. Our results for different blocking schemes are presented in Section~\ref{sec:expt1}.  We next study 
variable size batching for  inferencing in Section~\ref{sec:dp}, the results of which are
presented in Section~\ref{sec:expt2}. Finally, we conclude in Section~\ref{sec:conc}.

\eat{

1) To the best of our knowledge, we are the first to develop fast kernels for sparse matrix-vector and matrix-matrix multiplications, where the 
sparse matrix is indexed using relative indices, and encoded with Huffman encoding
2) We have performed a comprehensive evaluation of different schemes,  which has shown that the best choice of the scheme depends on the
sparsity and the encoding scheme.
3) We have performed experiments on popular deep learning networks like AlexNet, VGG-16, to show that our method achieves x factor boost in 
the inference time, while maintaining the memory constraints.


Deep neural networks have been used extensively in applications ranging from computer vision [1] to speech recognition [2] and natural language processing [3].
In this paper, we focus on convolutional neural networks (CNNs) which has gained signification progress over the last decade and have become ubiquitous  in object recognition, image classification, and retrieval.  Almost all of the recent successful recognition systems (Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler \& Fergus, 2013; Gong et al., 2014) are built on top of this architecture. 


Neural networks have become ubiquitous in applications ranging from computer vision [1] to speech recognition [2] and natural language processing [3]. We consider convolutional neural networks used for computer vision tasks which have grown over time. In 1998 Lecun et al. designed a CNN model LeNet-5 with less than 1M parameters to classify handwritten digits [4], while in 2012, Krizhevsky et al. [1] won the ImageNet competition with 60M parameters. Deepface classified human faces with 120M parameters [5], and Coates et al. [6] scaled up a network to 10B parameters.


Convolutional neural networks (CNN) are used extensively in computer vision applications, including object classification and localization, pedestrian and car detection, and video classification. Many problems like these focus on specialized domains for which there are only small amounts of care- fully curated training data. In these cases, accuracy may be improved by fine-tuning an existing deep network previously trained on a much larger labeled vision dataset, such as images from Ima- geNet (Russakovsky et al., 2015) or videos from Sports-1M (Karpathy et al., 2014). While transfer learning of this form supports state of the art accuracy, inference is expensive due to the time, power, and memory demanded by the heavyweight architecture of the fine-tuned network.
While modern deep CNNs are composed of a variety of layer types, runtime during prediction is dominated by the evaluation of convolutional layers. With the goal of speeding up inference, we prune entire feature maps so the resulting networks may be run efficiently even on embedded devices. We interleave greedy criteria-based pruning with fine-tuning by backpropagation, a computationally efficient procedure that maintains good generalization in the pruned network.


Deep convolutional neural networks (Krizhevsky et al., 2012; LeCun et al., 1990; Szegedy et al., 2014; Simonyan \& Zisserman, 2014) has recently achieved significant progress and have become the gold standard for object recognition, image classification, and retrieval. Almost all of the recent successful recognition systems (Jia, 2013; Donahue et al., 2013; Simonyan et al., 2013; Sermanet et al., 2013; Zeiler \& Fergus, 2013; Gong et al., 2014) are built on top of this architecture. Importing CNN onto embedded platforms (Gokhale et al., 2013; Culurciello et al., 2013), the recent trend toward mobile computing, has a wide range of application impacts. It is especially useful when the bandwidth is limited or photos are not allowed to be sent to servers. However, the size of the CNN models are typically very large (e.g. more than 200M bytes), which limits the applicability of such models on the embedded platform. For example, it will be almost impossible for users to download an iPhone application with more than 20M. Thus, in order to apply neural network methods to embedded platforms, one important research problem is how to compress parameters to reduce storage requirements.

In the past decade deep neural networks have set new performance standards in many high-impact applications. These include object classification (Krizhevsky et al., 2012; Sermanet et al., 2013), speech recognition (Hinton et al., 2012), image caption generation (Vinyals et al., 2014; Karpathy \& Fei-Fei, 2014) and domain adaptation (Glo- rot et al., 2011b). As data sets increase in size, so do the number of parameters in these neural networks in or- der to absorb the enormous amount of supervision (Coates et al., 2013). Increasingly, these networks are trained on industrial-sized clusters (Le, 2013) or high-performance graphics processing units (GPUs) (Coates et al., 2013).
}

