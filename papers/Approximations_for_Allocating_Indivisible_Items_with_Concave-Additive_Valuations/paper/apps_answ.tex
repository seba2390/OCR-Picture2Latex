

% https://arxiv.org/pdf/1912.12541.pdf
% https://arxiv.org/pdf/2112.10199.pdf
% https://arxiv.org/pdf/2201.01419.pdf
% https://www.ijcai.org/proceedings/2019/0042.pdf


\subsection{Constant Approximation for Asymmetric Nash Welfare with Smooth Valuations}
\label{subsec:smooth-anw}
We now apply our techniques to the problem of Nash Welfare Maximization for asymmetric agents with smooth additive additive valuations.
In this problem, each agent $i$ has a weight $\eta_i > 0$, and the goal is to find an allocation that maximizes $ \left(\prod_i (u_i + \omega)^{\eta_i}\right)^{1/\eta}$
where $\eta = \sum_i \eta_i$ is the sum of the agent weights and $\omega \in (0,1]$ denotes the smoothing parameter of the instance.

As discussed in the introduction, observe that we can scale the objective of each 
agent $i$ by $(\max_j u_{i,j})^{-\eta_i}$ without changing the approximation ratio of the algorithm. 
Therefore, wlog for the rest of the section we will assume that $\max_{j}u_{i,j} = 1$ for every agent $i$.
After this scaling, we can think of the smoothing parameter as giving each agent $i$ an initial utility of $\omega \max_{j}u_{i,j}$
at the beginning of the instance. Also for simplicity, throughout the section we assume weights are normalized by dividing them by $\eta$, so $\eta = 1$ (i.e., we bring the $1/\eta$ exponent into each term in the product objective).  





\subsubsection{Algorithm Definition}

Our algorithm has a natural combinatorial interpretation  which we call the {\em Weighted Bang-Per-Buck} (WBB) algorithm. To define the algorithm, we first explicitly define the additive curvature parameter $\alpha_i$ in the case where $v_i(u_i) = \eta_i \ln(u_i + \omega)$. Let $\sigma_i(z)$ denote the slope of the lower-bounding secant line that intersects the points $(z, \eta_i\ln(z + \omega))$ and $(z+1, \eta_i\ln(z+\omega + 1))$, given as:
\begin{equation}
\label{eq:sn-slope}
\sigma_{i}(z,1) := \eta_i\ln(z+ \omega + 1) - \eta_i\ln(z+\omega) =\eta_i\ln\left(1 + (z+\omega)^{-1}\right).
\end{equation} 
We then define the local additive curvature bound $\alpha_i$ at $z$ for agent $i$:
\begin{equation}
\label{eq:sn-curv}
\alpha_i(z) := \max_{z^* \in (0, 1)} \left[\eta_i\ln(z + z^* + \omega) - (\eta_i\ln(z + \omega) + z^*\sigma_{i}(z,1))\right]
\end{equation}
The  WBB Algorithm is given below in Algorithm 2.  
Throughout its execution, we adjust a uniform bid $b_i$ each agent $i$ makes for on every item in the instance. The algorithm starts with bids that are underestimates of the optimal dual bids, and thus proceeds by increasing the uniform bid of each agent one at a time, ensuring throughout every item is assigned to a maximum weighted bang-per-buck ratio agent, i.e., an agent that maximizes $(\eta_i u_{i,j})/b_i$. 
The algorithm stops increasing the bid of an agent according an exponential potential function proportional to agent's average {\em unweighted} MBB ratio (which we derive from the while-loop condition from the additive version of the $\ICA$ algorithm given in Section \ref{subsec:additive}).


\begin{algorithm}

\caption{Maximum Weighted Bang-per-buck Algorithm ({\sc WBB})}
Initialize fixed bid $b_i \leftarrow \omega$ for each agent $i$  \\
 \label{alg:WMBB}
Allocate each item $j$ to maximum WBB agent $\arg\max_{i} \left( \frac{ \eta_i u_{i,j}}{b_i} \right)$ \Comment{weighted greedy assignment} \\
\While{there exists an agent $i$ such that $\frac{u_i + \omega}{b_i}< \exp\left(\frac{u_i + \omega}{b_i} - 1 - \alpha_i\right)$}{
    \While{$\frac{u_i + \omega}{b_i}< \exp\left(\frac{u_i + \omega}{b_i} - 1 - \alpha_i\right)$}{
    \eIf{there is an item $j$ assigned to agent $i$ such that $i$ is not $j$'s maximum WBB agent}
    {
     Reassign $j$ to agent $\arg\max_{k}\left(\frac{\eta_{k} u_{k,j}}{b_{k}}\right)$
    }
    {
    Increase agent $i$'s bid to be $b_i \leftarrow \frac{\eta_imb_i}{\eta_i m - \epsilon b_i}$ \\
    }
}  
}
Output resulting allocation $u_i$ for all agents
\end{algorithm}






\subsubsection{Analysis}
To analyze the algorithm, we first argue that the WBB algorithm is equivalent to executing the ICA algorithm for an additive guarantee (as outlined in Section \ref{subsec:additive}). We then derive a closed-form for the local additive curvature $\alpha_i$ in terms of the smoothing parameter $\omega$.

\begin{lemma} 
The WBB algorithm (Algorithm 2) is equivalent to executing the ICA algorithm for an additive guarantee, where in the ICA instance $v_i(u_i) = \eta_i \ln(u_i + \omega)$.
\end{lemma} 

\begin{proof} 
By Lemma \ref{lem:duality}, the  primal and dual program for an ICA instance with $v_i(u_i) = \eta_i \ln(u_i + \omega)$ is given by the following (denoted {\sc ASN-CP} for ``Asymmetric Smooth Nash''):  

\begin{center}
\begin{tabular}{c  c  c | c  c} 
\hspace{5mm} & 
$
\begin{gathered}
    \textnormal{({\sc ASN-CP}):}  \max \sum_i \eta_i\ln(u_i+\omega) \\
    \forall i: u_i = \sum_j u_{i,j}x_{i,j} \\
    \forall j: \sum_i x_{i,j} \leq 1 \\
    \forall i,j: x_{i,j} \geq 0 \\
\end{gathered}
$
& \hspace{1mm} & \hspace{1mm} & 
\vspace{-4mm}
$
\begin{gathered}
    \textnormal{({\sc ASN-D}):} \min \sum_i \eta_i \left(\ln(t_i+ \omega) - \frac{t_i}{t_i+\omega}\right) + \sum_j \beta_j \\
    \forall i,j: \beta_j \geq \frac{\eta_i u_{i,j}}{t_i + \omega} \\
    \forall i,j: t_i, \beta_j \geq 0 \\
\end{gathered}
$
\end{tabular} \\
\end{center} 
\vspace{5mm}


% The earlier section can be directly applied to maximizing $\NSW'$, since $\log$ is a concave function. \todo{I couldn't do the calculus all the way, but the plots look convincing:} We can show that for $f(x) = \log(x+1)$, we have $R^* = \log 2 \approx 0.693$. But this is only a multiplicative guarantee for $\NSW'$, when the usual goal is to obtain a guarantee for $\NSW$.

%Now we show how we can obtain a multiplicative guarantee for $\NSW$. Our approach is to modify our general algorithm from Section~\ref{sec:c-ell} to obtain an \emph{additive} guarantee for $\NSW'$, and then we show that this translates into a multiplicative guarantee for $\NSW$.

Note that for this application, we denote the dual variable $p_j$ as $\beta_j$, since it is interpreted as the weighted MBB ratio, not the price. In particular, in the WBB algorithm, we substitute the $t_i + \omega$ terms in the {\sc ASN-D} program to be the uniform bid $b_i$ made by agent $i$ for all items. Thus the function $D(u_i)$ becomes: 

\begin{equation} 
\label{eq:Dlogdef}
D(u_i) = \eta_i\left[\frac{u_i + \omega}{b_i} + \ln(b_i)  -1 \right].
\end{equation}

Rearranging \eqref{eq:Dlogdef} the while-loop condition in Algorithm \ref{alg:pd} (which is $v_i(u_i) - D(u_i) > \alpha_i$ for the general additive $\ICA$ algorithm) and  exponentiating, we obtain the while-loop condition in Algorithm~\ref{alg:WMBB} after canceling $\eta_i$ terms. Furthermore, since WBB algorithm maintains an assignment where each item is assigned to the agent with maximum weighted MBB ratio, the variables $t_i = b_i - \omega$ and $\beta_j = \arg\max_{i} \left( \frac{ \eta_i u_{i,j}}{b_i} \right)$ form a feasible dual solution in a manner identical to the ICA algorithm. 
Finally, it is easily seen that the update to bid $b_i$ decreases $v'_i(t_i)$ by $\epsilon/m$:


\begin{equation*}
v_i'^{(2)}(t_i) = \frac{\eta_i}{\frac{\eta_imb_i}{\eta_i m - \epsilon b_i}} = \frac{\eta_i}{b_i} - \frac{\epsilon}{m} = v_i'^{(1)}(t_i) - \frac{\epsilon}{m},
\end{equation*}
where $v_i'^{(2)}(t_i)$ and $v_i'^{(1)}(t_i)$ denote the $v_i'(t_i)$ before and after the update to $b_i$ (respectively). 
\end{proof}


\begin{lemma}
\label{lem:nsw-diff-gap}
The local additive curvature $\alpha_i$ for agent $i$ is given by:

\begin{equation*} 
\alpha_i = \eta_i  \left[ \ln\left(\frac{1}{\omega \ln(1+1/\omega)}\right) + \omega \ln(1+1/\omega) - 1 \right]= O\left(\ln\left(\frac{\eta_i}{\omega\ln(1+\omega)}\right)\right),
\end{equation*}
when valuation function of agent $i$ is $v_i(u_i) = \eta_i \ln(u_i + \omega)$.
\end{lemma}

\begin{proof} 

For a fixed value $z$, let $\gamma = z + \omega$ and let $\Delta(z^*)$ be a function of $z^*$ defined by the expression inside the $\max$ in Equation \eqref{eq:sn-curv} for $\alpha(z)$, given as:


\begin{equation*}
\Delta(z^*) = \eta_i\left(\ln(z^* + \gamma) - \ln \gamma - z^*\ln(1+1/\gamma)\right).
\end{equation*}
Observe $\Delta(z^*)$ is a concave function in $z^*$, since the term $\ln(z^* + \gamma)$ is a concave function of $z^*$
and the remainder of the expression is a linear function in $z^*$. 
Therefore its maximum is obtained when $\frac{d}{dz^*} \Delta(z^*) = 0.$ 
By basic calculus, it follows the maximizer $z_{\max}$ for Equation \eqref{eq:sn-curv}
is given by:

\begin{equation} 
\label{eq:zmax}
z_{\max} = \arg\max_{z^* \in (0, 1)} \Delta(z^*)  =  \frac{1}{\ln\left(1 + 1/\gamma \right)} - \gamma.
\end{equation}
Thus $\alpha_i(z)$ can be then be expressed in a
closed form by plugging in $z_{\max}$ from Equation \eqref{eq:zmax}
in for $z^*$, which can be simplified as:

\begin{equation*} 
\alpha_i(z) = \eta_i\left[\ln\left(\frac{1}{\gamma \ln(1+1/\gamma)}\right) + \gamma \ln\left(1+\frac{1}{\gamma}\right) - 1\right].
\end{equation*}

It can be verified that the derivative of $\alpha_i(z)$ with respect to $\gamma$ is negative for all $\gamma > 0$. Therefore since $\gamma = \omega + z$, 
the derivative of $\alpha_i(z)$ is also negative for all $z \geq 0$, so $\alpha_i(z)$ is maximized at $z = 0$. Since $\gamma = \omega$ when $z= 0$, the lemma follows.  
\end{proof}






We can now prove Theorem \ref{thm:snsw}.
\begin{proof}[Proof of Theorem~\ref{thm:snsw}]





By Theorem \ref{thm:add-alg-bound}, the algorithm achieves the desired run-time bound, since $v_i'(0) = \eta_i/\omega \leq 1/\omega$ (recall we normalized agent weights to be $\eta_i/\eta$) and the update on Line 8 in WBB takes $O(1)$ time.
Thus, we are left with bounding the approximation ratio of the algorithm for the product objective.  

Let $\POPT$ and $\LOPT$ denote the objective value of the optimal solution for the product and log objective, respectively.  
Consider the dual variables $(\beta_j, t_i)$ corresponding to the allocation returned by the algorithm.
Also by Theorem \ref{thm:add-alg-bound}, $\beta_j + \epsilon/m$ is a feasible dual solution to the dual program {\sc ASN-D}. 
Thus by Lemma \ref{lem:duality} we have:

\begin{align}
    \text{{\sc ASN-D}}(t,\beta) & = \sum_i \eta_i \left(\ln(t_i+ \omega) - \frac{t_i}{t_i+\omega}\right)+ \sum_j \left(\beta_j + \frac{\epsilon}{m}\right) \notag  \\
    &= \sum_i D(u_i) + \epsilon \geq \LOPT \label{eq:sn-dual-bound}. 
\end{align}

When the algorithm terminates we have $\eta_i\ln(u_i + \omega) \geq D(u_i)  - \alpha_i$ for every agent $i$. Along with Inequality \eqref{eq:sn-dual-bound}, 
this implies the total objective of the algorithm is bounded by: 

\begin{equation*}
\sum_i \eta_i \ln(u_i+\omega) \geq \sum_i (D(u_i) - \alpha_i) \geq \LOPT  - \sum_i \alpha_i - \epsilon.
\end{equation*}
From this inequality, and the  fact that $\LOPT = \ln\left(\POPT\right)$ (when weights are scaled such $\eta = 1$), it follows the algorithm's objective on the product objective is bounded by: 
\begin{alignat}{2}
\prod_i (u_i + \omega)^{\eta_i} = 
\exp\left(\sum_i \ln(u_i + \omega)\right) & \geq \exp\left(\LOPT - \sum_i \alpha_i -\epsilon  \right) \notag \\
& = \exp\left(- \sum_i \alpha_i -\epsilon  \right)\POPT. \label{eq:nsw-final}
\end{alignat}
From Lemma \ref{lem:nsw-diff-gap}, we have that $\sum_i \alpha_i  = O\left(\ln\left(\frac{1}{\omega\ln(1+\omega)}\right)\right)$, it follows $\exp\left(\sum_i \alpha_i +\epsilon\right) = O(e^{\epsilon}/(\omega\ln(1+1/\omega)))$. Therefore by rearranging the above inequality \eqref{eq:nsw-final}, the theorem is established. 
\end{proof}


\subsubsection{Extension to Smooth Piecewise-Linear Valuation}
\label{subsub:anw-pl}
The results in this section can be also extended to  piecewise-linear valuation, i.e., maximizing $\left(\prod_i (f_i(u_i + \omega))^{\eta_i}\right)^{1/\eta}$ where $f_i(\cdot)$ is a piecewise-linear function.  In the next section (Section \ref{subsec:app-pl-welfare}) we argue the multiplicative curvature $\mu_i$ is at most $4/3$ for all such functions. Combining arguments in this section  with that of Lemma \ref{lem:nsw-diff-gap} we can obtain the following result.

\begin{theorem} 
\label{thm:anw-pl}
Consider an ICA instance with $v_i(u_i) = \eta_i \ln(f_i(u_i + \omega))$ where $f_i(u_i)$ is a piecewise-linear function concave-additive function with $\min_{k}\left(x_{i,k+1} - x_{i,k}\right) \geq \max_{j} u_{ij}$.
If $\omega = \Omega(1)$, then there exists a polynomial-time algorithm for the problem with an $O(1)$ approximation factor.
\end{theorem}

The extension works since Lemma \ref{lem:nsw-diff-gap} can be adapted to derive an additive curvature $\alpha_i$ that is a constant if $\omega = \Omega(1)$.
For example, when $f_i(u_i) = \min(u_i, c_i)$ is a budget-additive function, we obtain an approximation of $\approx  1.154$ as $\epsilon \rightarrow 0$ and $\omega = 1$.
We defer the proof of this result to the full version of the paper.





%%%% UNUSED SEPARABLE PIECE WISE LINEAR ALGORITHM %%%%%%
