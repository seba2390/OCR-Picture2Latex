\section{Approximation Algorithms for $\ICA$} \label{sec:ica-algo}
In this section we define our $(1+ \epsilon)\max_i \mu_i$-approximation algorithm for $\ICA$.
%Recall that in the problem definition of $\ICA$ we assume $v_i'(0)$ is finite.
For simplicity, throughout the section we assume the algorithm has knowledge of the value of each $\mu_i$ for any given set of valuation functions. 
At the end of the section, we will briefly discuss how the algorithm can be redefined so that algorithm does not need knowledge of $\mu_i$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% ALGO DEF %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algorithm Definition}
\label{sec:pd-alg-def}

As was done for the algorithm in \cite{chakrabarty2010approximability} for budget-additive functions, it will be useful to partition the cost of the dual solution according to algorithm's current assignments. Let $A_i$ denote the current set of items assigned to agent $i$ by the algorithm, and let $t_i$ be the current dual variable maintained by the algorithm for agent $i$. Define
\begin{equation}
\label{eq:dual-agent}
D(u_i) = y_i(t_i) + u_i v_i'(t_i)
\end{equation}
to be the utility for agent $i$ but instead evaluated according 
to the tangent line in the dual objective taken at point $t_i$. The algorithm will maintain that at any point, each item $j$ will be assigned to the bidder $i$ that maximizes 
$u_{i,j}v_i'(t_i)$ (and will reassign an item if this doesn't hold). We call such an assignment a {\em proper assignment.} 

%\todo{Should we stick to ``allocated'' or ``assigned''? They're being used interchangeably.}

\begin{definition} \label{def:proper-allocation}
Given a dual solution $v$, an item $j$ is said to be properly assigned if $j$ is assigned to agent $\arg\max_{i}(u_{i,j}v_i'(t_i))$. Otherwise, item $j$ is said to be improperly assigned. 
\end{definition}

In an allocation where all item's are properly
assigned, we can obtain the following characterization of the dual objective.


\begin{lemma}
\label{lem:alg-dual-prop}
Fix a point in the algorithm with primal and dual variables $u_i$ and $t_i$ for each agent $i$. If all items are properly allocated, then (i) setting $p_j = \max_{i} v'_i(t_i)u_{i,j} $ forms a feasible solution (with the $t_i$) to the dual $\ICA$-D, and (ii)
the objective of the dual can be expressed as $\sum_{i} D(u_i)$. 
\end{lemma}

\begin{proof}
Note that (i) follows immediately from the dual constraints. 
To see (ii), observe that
\begin{equation}
\label{eq:alg-dual-prop2}
\sum_j p_j = \sum_i \sum_{j \in A_i} (u_{i,j} v_i'(t_i)) = \sum_i ( u_i v_i'(t_i)).
\end{equation}
Adding $\sum_i y_i(t_i)$ to both sides of  \eqref{eq:alg-dual-prop2}, on the LHS we obtain the dual objective,
and on the RHS we obtain $\sum_{i} D(u_i)$, as desired.
\end{proof}

We can now define our algorithm (given formally in  Algorithm \ref{alg:pd}). At any point the algorithm maintains a 
setting of dual variables $t_i$ for all agents $i$ and variables $p_j$ for all items $j$. 
Each $t_i$ variable is initialized to be 0, and then items are properly assigned accordingly.
The algorithm proceeds by continuously increasing $t_i$ values (thus decreasing $v_i'(t_i)$), allowing items to defect if they are no longer properly assigned. 
The goal of the algorithm is to eventually is obtain an allocation such that $D(u_i)/ v_i(u_i)  \leq \mu_i$ for all agents. 
Under this condition, as long items remain (close to) properly assigned upon the algorithm's termination, Lemmas~\ref{lem:duality} and~\ref{lem:alg-dual-prop} imply that the approximation ratio of the algorithm is at most $\max_i \mu_i$.





%Therefore, given $t_i$ and $\Gamma$, this dual solution has value $\sum_i D_i$ where $D_i = y(t_i) + u_i \cdot f'(t_i)$.



\begin{algorithm}
\caption{Multiplicative $(\max_i \mu_i$)-Approximation Algorithm for $\ICA$}
\label{alg:pd}
Initialize all $t_i = 0$ \\
Allocate each item $j$ to agent $\arg\max_{i} \left(u_{i,j}v_i'(0)\right)$ \\
\While{there exists a agent $i$ such that $D(u_i)/ v_i(u_i) > \mu_i$}{
    \While{$D(u_i)/ v_i(u_i) > \mu_i$}{
    \eIf{there is an item $j$ such that $j$ is assigned to $i$ and $i \neq \arg\max_{k} \left( u_{k,j}  v_k'(u_k)\right)$}{
    Reassign $j$ to agent $\arg\max_{k} \left( u_{k,j}  v_k'(u_k)\right)$
    }
    {Increase tangent point $t_i$ until $v_i'(t_i)$ decreases by a factor of $1/(1+\epsilon)$}
    }
}
Output resulting allocation $u_i$ for all agents
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% APPROXIMATION %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis}
\label{subsec:pd-analysis}
Our algorithm seeks to find an allocation such that, for every agent $i$, the inequality $v_i(u_i) / D(u_i) \geq \mu_i$ is satisfied. To this end, we establish the following terminology.

\begin{definition}
There are two types of agents such that $D(u_i)/v_i(u_i) > \mu_i$: either $u_i < t_i$ or $u_i < t_i$. 
Call such agents under allocated and over allocated, respectively. 
\end{definition}

The main technical hurdles for our more general setting of concave-additive functions is showing that no agent becomes under allocated and establishing the claimed run time bound. We first state and prove a useful a property of monotone-concave functions. 

\begin{lemma}
\label{fact:concave-fact}
Let $f:\mathbf{R}_+\rightarrow \mathbf{R}_+$ be a monotone concave function. Suppose $\ell_1(\cdot)$ and $\ell_2(\cdot)$ are the equations of two lines tangent to $f$ at points $(t_1, f(t_1))$ and $(t_2, f(t_2))$, respectively. If there exists $x \geq \max(t_1, t_2)$ such that $\ell_2(x) \leq \ell_1(x)$, then (i) $t_1 \leq t_2$ and (ii) $\ell_1(\tilde x) \geq \ell_2(\tilde x)$ for all $\tilde x \leq t_1$.
\end{lemma}

\begin{proof}
We first show (i). For sake of contradiction suppose $t_1 > t_2$. Since $f$ is monotone and concave, it follows that:

\begin{equation}
\label{eq:concav-fact-1}
f(t_1) < \ell_2(t_1) = f(t_2) + (t_1 - t_2)f'(t_2).
\end{equation}
Therefore, we have the following:
\begin{alignat*}{2}
\ell_1(x)  = f(t_1) + (x- t_1)f'(t_1)  &< f(t_2) + (t_1 - t_2)f'(t_2) + (x - t_1)f'(t_2) \\
& = f(t_2) + (x - t_2)f'(t_2) \\
& = \ell_2(x),
\end{alignat*}
where the inequality follows from \eqref{eq:concav-fact-1} and the fact that $f'(t_1) < f'(t_2)$. This directly contradicts the assumption in the lemma, thus showing (i). Given $t_1 \leq t_2$, (ii) then follows by a similar argument. 
\end{proof} 
\begin{lemma} \label{lem:under-bad}
Throughout the algorithm, an agent never becomes under allocated. In particular, if $u_i < t_i$ then  $D(u_i)/ v_i(u_i) \leq \mu_i$.
\end{lemma}

\begin{proof}
At the start of the algorithm $t_i = 0$ for all agents, and so no agent can be under allocated at the outset of the algorithm.
Therefore, the only point at which 
an agent $i$ with total utility $u_i$ could potentially become under allocated is when some item $j$ is reassigned to another agent on Line 6 of the algorithm such that after the reassignment $u_i - u_{i,j} < t_i$. Fix 
such a point in the algorithm. 

%reassigneonly way $u_i$ decreases is when a bidder defects, that is, it loses some item $j$ so that $u_i$ decreases by $u_{i,j}$. If $u_i - u_{i,j} \geq t_i$, then still, the lemma does not apply.
%Otherwise, in the worst case, the load $u_i$ decreases by the maximum bid value $b$. 

%Let $t_i$ denote the line tangent to $v_i$ at $t_i$, so when we enter the loop, $D(u_i) = t(u_i)$. 

Let $s(x)$ denote the equation for the secant line that passes through $v_i$ at points $(u_i- u_{i,j}, v_i(u_i- u_{i,j}))$ and $(u_i, v_i(u_i))$. 
More formally, let $\sigma_i' := \sigma_i(u_i - u_{i,j}, u_{i,j})$, 
where the definition of $\sigma_i(\cdot)$ is given by Equation \eqref{eq:lcb-slope}.
Then the equation for $s(x)$ is

\begin{equation}
s(x) = \sigma_i' x + v_i(u_i - u_{i,j}) - \sigma_i'  (u_i - u_{i,j}).
\end{equation}

Let $\tilde\mu_i := \tilde\mu_i(u_i-u_{i,j})$ denote the local multiplicative curvature of $v_i(\cdot)$ at $u_i-u_{i,j}$  , and let $z^*$ be value (given in Equation \eqref{eq:mult-lcb}) that determines $\tilde\mu_i$.
%Recall that from the definition of $\mu$, we have that $\mu \leq \ct$. 
Based on the definition of $\tilde\mu_i$, observe that if we scale $s(x)$  by a factor of $\tilde\mu_i$, we obtain an equation for a line that is tangent to 
$v_i$ at the point ($u_i - u_{i,j} + z^*, v_i(u_i - u_{i,j}+ z^*))$. 
Denote the equation for this line as $\tilde{s}(x) = \tilde\mu_is(x)$. 
Since by definition $s(u_i) = v(u_i)$, it follows that

\begin{equation}
\label{eq:secant-bound}
\tilde s(u_i) = \tilde\mu_i s(u_i) = \tilde\mu_iv_i(u_i)  \leq \mu_i v_i(u_i) < D(u_i),
\end{equation} 
where the first inequality holds by definition of $\mu_i$ and the last inequality holds because $i$ entered the loop on Line 3.  Given Inequality \eqref{eq:secant-bound}, we can  apply  Lemma \ref{fact:concave-fact} to show that $D(u_i - u_{i,j}) \leq  \tilde s(u_i - u_{i,j})$. In particular, by setting $\ell_1(\cdot) = s(\cdot)$, $\ell_2 = D(\cdot)$ and $x = u_i$, the lemma implies both that $t_i \leq z^*$ and $D(u_i - u_{i,j}) \leq   \tilde s(u_i - u_{i,j})$. Thus it follows:
\begin{equation}
\label{eq:ratio-est}
\frac{D(u_i-u_{i,j})}{v_i(u_i-u_{i,j})} =\frac{D(u_i-u_{i,j})}{s(u_i-u_{i,j})} \leq \frac{\tilde s(u_i-u_{i,j})}{s(u_i-u_{i,j})} = \tilde\mu_i \leq \mu_i,
\end{equation} 
 which establishes the lemma.  Note the first equality follows from the definition of $s(x)$, and the first inequality follows from $D_{t_i}(u_i - u_{i,j}) \leq   \tilde s(u_i - u_{i,j})$.
\end{proof}
% \todo{In the algorithm, when increasing $t_i$, we increase it until $f'(t_i)$ decreases by a $(1-\epsilon)$ factor. This only works if we assume $f'$ is strictly decreasing. But this is pretty reasonable, because without it, $f$ could be linear, and in that case you simply assign each item to its highest bidder. It's also convenient to assume $f'(0) = 1$ and $f'(bm) = 0$. This can probably be done up to scaling/smoothing.}

%Given the $t_i$'s and an allocation, we define $p_j$ as $u_{i,j}f'(t_i)$ where $i$ is the bidder to which $j$ is allocated.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% RUN TIME ANALYSIS %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



Next,  we establish the run time of the algorithm. For simplicity, we will assume that $\epsilon$ is selected such that for all agents $\mu_i \geq 1+\epsilon$. This is possible as long as $\mu > 1$. (If $\mu_i = 1$, then $v_i(u_i) = D(u_i)$, and thus the agent is never under or over allocated.)


\begin{lemma}
\label{lem:alg-pd-term}
Let $\umaxi = \sum_{j} u_{i,j}$ denote the maximum possible spend for fixed agent $i$, and let 
\begin{equation*}
\rmax = \max_i \left(\frac{v_i'(0)\umaxi}{v_i(\umaxi)}\right)
\end{equation*} 
denote the maximum ratio (over all agents $i$) between the total additive utility evaluated along the tangent line at $v_i'(0)$, versus the total spend evaluated with $v_i(\cdot)$. If for all agents $\mu_i \geq 1+\epsilon$, then the algorithm terminates in time $O(mnT\ln(\rmax/\epsilon)/\epsilon)$, 
where $T$ is the time needed to perform the update of $v_i'(t_i)$ on Line 8.
\end{lemma} 

\begin{proof}

We begin by showing that once the update on Line 8 occurs  $O(\ln(\rmax/\epsilon)/\epsilon)$ times for a fixed agent $i$, 
then agent $i$ cannot be under or over allocated for the remainder of algorithm's execution 
(i.e., the algorithm does not again enter the while loop on Line 3 for agent $i$). Define $H_i$ as the following:

\begin{equation*}
H_i = \frac{1}{\ln(1+\epsilon)}\cdot \ln\left(\frac{v'_i(0)\umaxi}{\epsilon v_i(\umaxi)}\right) \leq \frac{\ln(\rmax/\epsilon)}{\ln(1+\epsilon)} = O\left(\frac{\ln(\rmax/\epsilon)}{\epsilon}\right).
\end{equation*} 
After $H_i$ Line 8 updates to agent $i$, 
we can bound $v'_i(t_i)$ as follows:
%since $i$ cannot be over allocated  after $T_2$ updates on Line 8 for agent $i$, the value of $v_i(t_i)$ is can be bounded by

\begin{equation} 
\label{eq:runtime-1}
 v_i'(t_i) = \frac{v_i'(0)}{(1+\epsilon)^{H_i}}
= \frac{\epsilon  v_i(\umaxi)}{\umaxi} 
\leq \frac{(\mu_i-1)v_i(\umaxi)}{\umaxi}
\end{equation}
where the inequality follows from our assumption 
$\mu_i \geq 1+\epsilon$. Rearranging \eqref{eq:runtime-1}, we obtain
\begin{equation}
\label{eq:runtime-2}
v_i(\umaxi) + \umaxi v_i'(t_i) \leq \mu_i v_i(\umaxi).
\end{equation}
 Recall that by Lemma~\ref{lem:under-bad}, an agent can be never be under allocated. Therefore we may assume that $t_i < u_i$. We can then upper bound $D(u_i)$ as follows:
\begin{align*} % D_v(u) = y(v) + f'(v)?
D(u_i) &= v_i(t_i) + (u_i - t_i)\cdot v_i'(t_i) \\
& \leq v_i(\umaxi) + \umaxi v_i'(t_i) \leq \mu_iv_i(\umaxi),
\end{align*}
where the first inequality follows from the fact 
$t_i \leq u_i \leq \umaxi$, and the last inequality follows from \eqref{eq:runtime-2}. This means agent $i$ cannot be over allocated for the rest of the algorithm, which means agent $i$ will remain paid for.

To complete the argument, notice that once an item $j$ is reassigned on Line 6, the algorithm cannot reassign $j$ again until it increases $t_i$ for some agent $i$ on Line 8. 
Thus, the algorithm can perform at most $m$ reassignments on Line 6 before an update on Line 8 must occur for some agent.
Summing over all agents establishes the lemma.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{thm:pd-alg}]


Notice that after termination, by increasing each dual variable $p_j = \max_{i} v'_i(t_i)u_{i,j}$ by a factor of $(1+\epsilon)$ we obtain a feasible dual solution, since the only items that are improperly assigned are ones allocated to an agent that exited the while loop on Line 3 before the item should have been reallocated on Line 6 in the inner while loop. Let $\OPT$ denote the optimal solution for the instance. By the above argument and Lemma~\ref{lem:duality} we have
\[
\OPT \leq (1+\epsilon)\sum_i D(u_i).
\]
Lemma \ref{lem:alg-pd-term} ensures the algorithm will eventually terminate, and since the algorithm terminates, no agent can be over allocated.  Furthermore, by Lemma \ref{lem:under-bad}, no agent can be under allocated. Therefore for all agents $i$ we have $D(u_i) \leq \mu_i v_i(u_i)$ for every $i$. Combining these inequalities yields
\[
\OPT \leq (1+\epsilon)\sum_i \mu_i v_i(u_i) \leq (1+\epsilon) (\max_i \mu_i)\cdot \sum_i v_i(u_i),
\]
which proves the theorem.
\end{proof}
We conclude the section by briefly discussing how
Algorithm \ref{alg:pd} can still execute without knowledge of $\mu$ and can be adapted to the additive setting. 


\subsubsection{No Knowledge of $\mu_i$} The algorithm can be adapted to operate without knowledge of each $\mu_i$ by  (i) repeatedly guess values of $\mu = \max_i\mu_i$, then (ii) check Lines 3 and 4 against $\mu$ instead.
In particular, we can start with an overestimate of $\mu$ 
(i.e., start with $\mu = 1+\epsilon$ and repeatedly raise the guess in increments of $\epsilon$), and then check after all reassignments whether 
or not an agent is under allocated. If no agent ever becomes under allocated, then it follows the algorithm achieves an approximation at its current guess; furthermore, by the Lemma \ref{lem:under-bad}, the algorithm is guaranteed to have no agents become under allocated once the guess reaches the true value of $\mu$. This modification comes at an $O(1/\epsilon)$ factor in the run time of the algorithm and an additional additive $O(\epsilon)$ error in the approximation factor. 

\subsubsection{Adaptation to Additive Guarantee}
\label{subsec:additive} 

As discussed in the introduction, an appealing feature of our geometric-based arguments is that they easily extend to obtain additive guarantees as well. This result is stated formally as follows.

\begin{theorem}
\label{thm:add-alg-bound}
There exists an  algorithm for $ICA$ that achieves an additive bound of $\sum_i \alpha_i + \epsilon$ and runs in time $O(m^2nTv_i'(0)/\epsilon)$, 
where $T$ is the time needed to perform the update of $v_i'(t_i)$ on Line 8 of Algorithm~\ref{alg:pd}. Furthermore, the integrality gap of the corresponding assignment convex program is $\sum_i \alpha = n\alpha$ for instances whose valuations have additive curvature $\alpha$.
\end{theorem}

Unfortunately in a general instance $\ICA$ in the additive setting, our guarantee becomes $\sum_i \alpha_i$ (instead of $\max_i \alpha_i$) since our analysis bounds the objective on a per-agent basis.
We note (i) it can be verified that the multiplicative integrality gap example in the next section (Section \ref{sec:int-gap}) can be adapted to show that this bound is tight, and (ii) as we will see in Section \ref{subsec:smooth-anw}, the $\sum_i \alpha_i$ additive bound will translate nicely to a constant approximation for the smooth asymmetric Nash-welfare product objective, since the additive curvature of the weighted-log objective for each agent $i$ will be $O(\eta_i$) with  smoothing parameter $\omega = \Omega(1)$. (The contribution of $\sum_i \eta_i$ from the additive bound is canceled by the $\frac{1}{\sum_i \eta_i}$ term taken in the overall exponent of the product objective). 

The adaption of the algorithm and analysis from Sections \ref{sec:pd-alg-def} and \ref{subsec:pd-analysis} to the additive setting (to prove Theorem \ref{thm:add-alg-bound}) uses many of the same definitions and arguments, so we will only highlight the key differences here:

\begin{itemize}

\item In the algorithm, the while-loop conditions on Lines 3 and 4 are changed to instead be $D(u_i) -  v_i(u_i) > \alpha_i$. The update on Line 8 is changed so that $v_i'(t_i)$ instead decreases by (an absolute amount) $\epsilon/m$. 

\item In our analysis, the arguments in Lemmas \ref{lem:under-bad} work in the additive setting
since the relevant quantities are completely relative to each other, and therefore ratios can be replaced by differences while preserving the logic. The feasibility (adjusting $p_j$ by $\epsilon/m$) and run-time arguments can also be easily adapted to obtain the bounds stated in the theorem.\footnote{In the case of general $\ICA$ instance, the algorithm runs in pseudo-polynomial time based on the value of $v'(0)$. We note that in the our Nash-welfare application, this term will be constant, and thus runs in strongly-polynomial time for a fixed $\epsilon$ and smoothing parameter $\omega$.}
\end{itemize} 







