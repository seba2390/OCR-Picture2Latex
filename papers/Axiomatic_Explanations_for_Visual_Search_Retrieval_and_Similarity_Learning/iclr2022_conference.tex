
\documentclass{article} % For LaTeX2e
\usepackage{iclr2022_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsfonts}       % blackboard math symbols


%\usepackage{xcolor}         % colors
%\usepackage[table,xcdraw]{xcolor}
\usepackage{xcolor,colortbl}

\usepackage{multirow}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{lscape}
\usepackage[export]{adjustbox}% http://ctan.org/pkg/adjustbox
\usepackage{array,booktabs}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{soulutf8}
\usepackage{color}
\soulregister\cite7
\soulregister\citep7
\soulregister\ref7
\DeclareRobustCommand{\hly}[1]{{\sethlcolor{yellow}\hl{#1}}}


\newcolumntype{C}[0]{>{\centering\arraybackslash}p{6mm}}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]


%\DeclareMathOperator*{\argmax}{arg\,max}
%\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator*{\Range}{Range}

\DeclareMathOperator{\norm}{norm}
\newcommand{\defeq}{\vcentcolon=}

\definecolor{lgray}{rgb}{.9,.9,.9}
\definecolor{pink}{rgb}{1.0,.81,.78}
\sethlcolor{pink}


\title{Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Mark Hamilton$^{1,2}$, Scott Lundberg$^{2}$, Stephanie Fu$^{1}$, Lei Zhang$^{2}$, William T. Freeman$^{1,3}$ \\
$^{1}$MIT, $^{2}$Microsoft, $^{3}$Google \\
\texttt{markth@mit.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle
\vspace{-.35in}


\begin{abstract}
Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine's behavior. We show that the theory of fair credit assignment provides a \textit{unique} axiomatic solution that generalizes several existing recommendation- and metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate ``fairness'' and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained \textit{opaque-box} models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures. 
\end{abstract}

\vspace{-.1in}
\section{Introduction}
\vspace{-.15in}
Search, recommendation, retrieval, and contrastive similarity learning powers many of today's machine learning systems. These systems help us organize information at scales that no human could match. The recent surge in million and billion parameter contrastive learning architectures for vision and language underscore the growing need to understand these classes of systems \citep{nayak_2019,mocov2,simclrv1,CLIP,SWAV}. Like classifiers and regressors, contrastive systems face a key challenge: richer models can improve performance but hinder interpretability. In high-risk domains like medicine, incorrect search results can have serious consequences. In other domains, search engine bias can disproportionately ans systematically hide certain voices \citep{mowshowitz2002assessing,diaz2008through,goldman2005search}. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/interp_search_architectures.jpg}
    \vspace{-.3in}
    \caption{Architectures for search engine interpretability. Like classifier explanations, First-order search explanations yield heatmaps of important pixels for similarity (bottom row third column). Second order search interpretation methods yield a dense correspondence between image locations (last two columns). CAM (second column) is a particular case of Shapley value approximation, and we generalize it to yield dense correspondences (last column).}
    \label{fig:shap_architecture}
    \vspace{-.15in}
\end{figure}


 Currently, there are several competing techniques to understand a similarity model's predictions \citep{vedml,vesm,sbsm,gradcam,transformers}. However, there is no agreed ``best'' method and no a formal theory describing an ``optimal'' search explanation method. We show that the theory of fair credit assignment provides a uniquely determined and axiomatically grounded approach for ``explaining'' a trained model's similarity judgements. In many cases, existing approaches are special cases of this formalism. This observation allows us to design variants of these methods that better satisfy the axioms of fair credit assignment and can handle counterfactual or relative explanations. Though we explore this topic through the lens of visual search, we note that these techniques could also apply to text, tabular, or audio search systems.
 
 %We apply this formalism to yield both black and grey-box model interpretability approaches that give model developers insight into their model matches \textit{without} knowledge of an algorithm's internal construction.
 
 This work identifies two distinct classes of search engine explainability methods. ``First order'' approaches highlight the most important pixels that contribute to the similarity of objects and ``Second order'' explanations provide a full correspondence between the parts of query and retrieved image. We relate first order interpretations to existing theory on classifier explainability through a generic function transformation, as shown in the third column of Figure \ref{fig:shap_architecture}. We find that second order explanations correspond to a uniquely specified generalization of the Shapley values \citep{sundararajan2020shapley} and is equivalent to projecting Harsanyi Dividends onto low-order subsets \citep{harsanyi1963simplified}. We use this formalism to create new second-order generalizations of Class Activation Maps \citep{zhou2015cnnlocalization}, GradCAM \citep{gradcam}, LIME \citep{lime}, and SHAP \citep{shap}. Our contributions generalize several existing methods, illustrate a rich mathematical structure connecting model explainability and cooperative game theory, and allow practitioners to understand search engines with greater nuance and detail.  We include a short video detailing the work at {\color{blue}\url{https://aka.ms/axiomatic-video}}.
  % This not only extends a correspondence between model explainability and cooperative game theory, but also yields better empirical results on a search engine evaluated on the PASCAL VOC dataset \citep{Everingham10}. 
 In summary we:
  \vspace{-.07in}
\begin{itemize}
    \itemsep-0em 
    \item Present the first uniquely specified axiomatic framework for \textit{model-agnostic} search, retrieval, and metric learning interpretability using the theory of Harsanyi dividends.
    \item Show that our framework generalizes several existing model explanation methods \citep{zhou2015cnnlocalization,gradcam,vedml,lime} to yield dense pairwise correspondences between images and handle counterfactual information.
    \item Introduce a new kernel-based approximator for Shapley-Taylor indices that requires about $10\times$ fewer function evaluations.
    \item Show that our axiomatic approaches provide more faithful explanations of image similarity on the PascalVOC and MSCoCo datasets.
\end{itemize}

\vspace{-.1in}
\section{Background}
\vspace{-.1in}
This work focuses on search, retrieval, metric learning, and recommendation architectures. Often, these systems use similarity between objects or learned features \citep{bengio2013representation} to rank, retrieve, or suggest content \citep{bing_2017,su2009survey,chopra2005learning,CLIP}. More formally, we refer to systems that use a distance, relevance, or similarity function of the form: $d: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$ to quantify the relationship between items from sets $\mathcal{X}$ and $\mathcal{Y}$. In search and retrieval, $\mathcal{X}$ represents the space of search queries and $\mathcal{Y}$ represents the space of results, the function $d$ assigns a relevance to each query result pair. Without loss of generality, we consider $d$ as a ``distance-like'' function where smaller values indicate more relevance. The expression $\argmin_{y \in \mathcal{Y}} d(x,y)$ yields the most relevant result for a query $x \in \mathcal{X}$.

Specializing this notion yields a variety of different kinds of ML systems. If $\mathcal{X} = \mathcal{Y} = \Range(\mathcal{N}(\cdot))$ where $\mathcal{N}$ is an image featurization network such as ResNet50 \citep{he2016deep}, the formalism yields a visual search engine or ``reverse image search''. Though this work focuses on visual search, we note that if $\mathcal{X}$ is the space of character sequences and $\mathcal{Y}$ is the space of webpages, this represents web search. In recommendation problems, $\mathcal{X}$ are users and $\mathcal{Y}$ are items, such as songs or news articles. In this work we aim to extract meaningful ``interpretations'' or ``explanations'' of the function $d$.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/search_comparison.jpg}
    \vspace{-.3in}
    \caption{Comparison of first-order search interpretation methods which highlight pixels that contribute to similarity in red. Integrated Gradients (on pixels) struggles because well trained classifiers are invariant to minor pixel changes and have uninformative gradients.}
    \label{fig:comparison}
    \vspace{-.1in}
\end{figure*}

%p3 - Interpretation methods
\vspace{-0.1in}
\subsection{Model Interpretability}
\vspace{-.1in}
The Bias-Variance trade-off \citep{kohavi1996bias} affects all machine learning systems and governs the relationship between a model's expressiveness and generalization ability. In data-rich scenarios, a model's bias dominates generalization error and increasing the size of the model class can improve performance. However, increasing model complexity can degrade model interpretability because added parameters can lose their connection to physically meaningful quantities. This affects not only classification and regression systems, but search and recommendation architectures as well. 
%For example, the best performing competitors on the Netflix prize dataset \citep{bennett2007netflix} involve significantly more complex models than earlier baselines \citep{rendle2019difficulty}. 
For example, the Netflix-prize-winning ``BellKor'' algorithm \citep{koren2009bellkor}, boosts and ensembles several different methods making it difficult to interpret through model parameter inspection alone.

To tackle these challenges, some works introduce model classes that are naturally interpretable \citep{nori2019interpretml,hastie1990generalized}. Alternatively, other works propose \textit{model-agnostic} methods to explain the predictions of classifiers and regressors. Many of these approaches explain the local structure around a specific prediction. \cite{shap} show that the Shapley value \citep{shapley1951notes}, a measure of fair credit assignment, provides a unique and axiomatically characterized solution to classifier interpretability (SHAP). Furthermore, they show that Shapley values generalize LIME, DeepLIFT \citep{shrikumar2017learning}, Layer-Wise Relevance Propagation \citep{bach2015pixel}, and several other methods \citep{vstrumbelj2014explaining,datta2016algorithmic,lipovetsky2001analysis,saabas_2014}. Many works in computer vision use an alternative approach called Class Activation Maps (CAMs). CAM projects the predicted class of a deep global average pooled (GAP) convolutional network onto the feature space to create a low resolution heatmap of class-specific network attention. GradCAM \citep{gradcam} generalizes CAM to architectures other than GAP and can explain a prediction using only a single network evaluation. In Section \ref{sec:cam} we show that CAM, GradCAM, and their analogue for search engine interpretability, \cite{vedml}, are also unified by the Shapley value and its second order generalization, the Shapley-Taylor index.

\vspace{-0.1in}
\subsection{Fair Credit Assignment and the Shapley Value}
\label{sec:shap-value}
\vspace{-.1in}
%This work looks to extend this framework to the richer functional forms needed for search and recommendation systems.
Shapley values provide a principled and axiomatic framework for classifier interpretation. We briefly overview Shapley values and point readers to \cite{molnar2020interpretable} for more detail. Shapley values originated in cooperative game theory as the \textbf{only} fair way to allocate the profit of a company to its employees based on their contributions. To formalize this notion we define a ``coalition game'' as a set $N$ of $|N|$ players and a ``value'' function $v: 2^N \to \mathbb{R}$. In cooperative game theory, this function $v$ represents the expected payout earned by cooperating coalition of players. \cite{shapley1951notes} show that the unique, fair credit assignment to each player, $\phi_v(i \in N)$, can be calculated as:
%\vspace{-.1in}
 \begin{equation}
    \label{eqn:shap}
     \phi_v(i) \defeq \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! (|N|-|S|-1)!}{|N|!}(v(S \cup \{i\}) - v(S))
 \end{equation}
Informally, this equation measures the average increase in value that a player $i$ brings to a coalition $S$ by weighting each increase, $v(S \cup \{i\}) - v(S)$, by the number of ways this event could have happened during the formation of the ``grand coalition'' $N$. We note that this assignment, $\phi_v$, is the \textit{unique} assignment that satisfies four reasonable properties: \textbf{symmetry} under player re-labeling, no credit assignment to \textbf{dummy} players, \textbf{linearity} (or it's alternative \textbf{monotonicity}), and \textbf{efficiency} which states that Shapley values should sum to $v(N)-v(\emptyset)$ \citep{young1985monotonic}. Intuitively, these axioms require that a fair explanation should treat every feature equally (Symmetry), should not assign importance to features that are not used (Dummy), should behave linearly when the value function is transformed (Linear), and should sum to the function's value (Efficiency). 

Shapley Values provide a principled way to explain the predictions of a machine learning model. To connect this work to model interpretability, we can identify the ``features'' used in a model as the ``players'' and interpret the value function, $v(S)$, as the expected prediction of the model when features $N \setminus S$ are replaced by values from a ``background'' distribution. This background distribution allows for ``counterfactual'' or relative explanations \citep{goyal2019counterfactual}. 

\begin{figure}
\centering
\begin{minipage}[t]{.49\textwidth}
  \centering
     \includegraphics[height=88px,valign=t]{figures/explaining_relative_importance.jpg}
    %\includegraphics[height=88px,valign=t]{figures/counterfactual_search_interp.jpg}
    \vspace{-.1in}
    \captionof{figure}{Explanations relative to a background distribution show why a result is better than an alternative. When asked why the best result (lower left) was better than the second best result (top right) our method correctly selects the player.}
    \label{fig:counterfactual}
\end{minipage}%
\hspace{.01\textwidth}
\begin{minipage}[t]{.49\textwidth}
  \centering
     \includegraphics[height=90px,valign=t]{figures/second_order_interpretation.jpg}
     \vspace{-.1in}
    \captionof{figure}{Visualization of how regions of two similar images ``correspond'' according to the second-order search interpretability method SAM. We can use this correspondence to transfer labels or attention between similar images.}
    \label{fig:attention_projection}
\end{minipage}
\vspace{-.2in}
\end{figure}

\vspace{-.1in}
\section{Related Work}
\vspace{-.1in}
There is a considerable body of literature on model interpretability and we mention just a handful of the works that are particularly related. One of our baseline methods, \cite{sbsm}, was one of the first to present a generic visual search engine explanation reminiscent of a Parzen-Window based estimator. \cite{fong2017interpretable} introduce a method for explaining classifiers based on meaningful perturbation and \cite{chefer2021transformer} introduce a method for improving interpretation for transformer-based classifiers. \cite{vedml} lifted CAM to search engines and we find that our Shapley-Taylor based method aligns with their approach for GAP architectures. \cite{singh2019exs} and \cite{fernando2019study} use LIME and DeepSHAP to provide first-order interpretations of text but do not apply their methods to images. \cite{ancona2019explaining} introduce a distribution propagation approach for improving the estimation of Shapley Values for deep models and can be combined with our approach. Many works implicitly use components that align with Shapley-Taylor indices for particular functions. Works such as \cite{fischer2015flownet,sun2020mining,wang2020learning,MaCoSNet,hou2019cross} use feature correlation layers to estimate and utilize correspondences between images. We show these layers are equivalent to Shapley-Taylor indices on the GAP architecture, and this allows create a correlation layer that handles counterfactual backgrounds. Other recent works have used learned co-attention within transformer architectures to help pool and share information across multiple domain types \citep{wei2020multi}. \cite{fu2020axiom} attempt to learn a variant of GradCAM that better aligns with axioms similar to Shapley Values by adding efficiency regularizers. The method is not guaranteed to satisfy the axioms but is more ``efficient''. 

We rely on several works to extend Shapley values to more complex interactions. \cite{harsanyi1963simplified} generalized the Shapely value by introducing a ``dividend'' that, when split and distributed among players, yields the Shapley values. \cite{owen1972multilinear} introduces an equivalent way to extend Shapley values using a multi-linear extension of the game's characteristic function. \cite{sundararajan2020shapley} introduce the Shapley-Taylor index and show is equivalent to the Lagrangian remainder of Owen's multi-linear extension. Integrated Hessians \citep{integrated-hessians} enable estimation of a second-order variant of the Aumann-Shapley values and we use this approach to create a more principled second-order interpretation method for differentiable search engines.
\vspace{-.1in}

\section{Unifying First-Order Search Interpretation Techniques}
\vspace{-.1in}
\label{sec:cam}
Though there is a considerable body of work on opaque-box classifier interpretability, opaque-box search engine interpretability has only recently been investigated \citep{singh2019exs,vedml,vesm}. We introduce an approach to transform opaque and grey-box classification explainers into search engine explainers, allowing us to build on the rich body of existing work for classifiers. More formally, given a similarity function $d: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$ and elements $x \in \mathcal{X}$ and $y \in \mathcal{Y}$ we can find the ``parts'' of $y$ that most contribute to the similarity by computing the Shapley values for the following value function:
\begin{equation}
    \label{eq:first-order}
    v_{1}(S): 2^N \to \mathbb{R} \defeq d(x, mask(y, S))
\end{equation}
Where the function $mask(\cdot, \cdot): \mathcal{Y} \times 2^N \to  \mathcal{Y}$, replaces ``parts'' of $y$ indexed by $S$ with components from a background distribution. Depending on the method, ``parts'' could refer to image superpixels, small crops, or locations in a deep feature map. This formula allows us to lift many existing approaches to search engine interpretability. For example, let $\mathcal{X}$, and $\mathcal{Y}$ represent the space of pixel representations of images. Let the grand coalition, $N$, index a collection of superpixels from the retrieved image $y$. Let $mask(y, S)$ act on an image $y$ by replacing the $S$ superpixels with background signal. With these choices, the formalism provides a search-engine specific version of ImageLIME and KernelSHAP. Here, Shapley values for each $i \in S$ measure the impact of the corresponding superpixel on the similarity function. If we replace superpixels with hierarchical squares of pixels we arrive at Partition SHAP \citep{partition-shap}. We can also switch the order of the arguments to get an approach for explaining the query image's impact on the similarity. In Figure \ref{fig:comparison} we qualitatively compare how methods derived from our approach compare to two existing approaches: SBSM \citep{sbsm} and VESM \citep{vesm}, on a pair of images and a MocoV2 based image similarity model. In addition to generalizing LIME and SHAP we note that this approach generalizes VEDML \citep{vedml}, a metric-learning adaptation of CAM:
\vspace{-.05in}
\begin{proposition}
\label{prop:cam}
 Let  $\mathcal{X} = \mathcal{Y} = \mathbb{R}^{CHW}$ and represent the space of deep network features where $C,H,W$ represent a channel, height, and width of the feature maps respectively. Let the function $d \defeq \sum_c GAP(x)_c GAP(y)_c$. Let the grand coalition, $N = [0,H] \times [0,W]$, index the spatial coordinates of the image feature map $y$. Let the function $mask(y, S)$ act on a feature map $y$ by replacing the features at locations $S$ with a background signal $b$. Then:
 \vspace{-.05in}
\begin{equation}
    \label{eq:sam}
    \phi_{v_1}((h,w) \in N) = \frac{1}{HW}\sum_c GAP(x)_c (y_{chw} - b_{chw})
\end{equation}
\end{proposition}
\vspace{-.05in}
Where GAP refers to global average pooling. We defer proof of this and other propositions to the Supplement. The results of this proposition mirrors the form of VEDML but with an added term to handle background distributions. These extra terms broaden the applicability of VEDML and we demonstrate their effect on explanations in Figure \ref{fig:counterfactual}. In particular, we explain why two guitar players are similar in general (no background distribution), and relative to the second-best result of a guitar. Without a background, the explanation focuses on the guitar. However, when the explanation is relative to an image of a guitar the explanation focuses instead on the ``tie-breaking'' similarities, like the matching player. With counterfactual queries one can better understand a model's rationale behind \textit{relative} similarity judgements and this can help in domains such as search engine optimization and automated medical diagnosis. We refer to Equation \ref{eq:sam} as the Search Activation Map (SAM) in analogy with the Class Activation Map. We note that in non-GAP architectures, VEDML requires Taylor approximating nonlinear components. This heuristic corresponds estimating the Shapley values for a linear approximation of the true value function. For nonlinear architectures such as those that use cosine similarity, \textul{SAM diverges from Shapley value theory} and hence violates its axioms. We can remedy this by using a Kernel-based Shapley value approximator \citep{shap} and refer to this approach as Kernel SAM.   

Though the Shapley value framework unifies several methods for search engine interpretability, we note that the popular technique GradCAM does not align with Shapley value theory when applied to our feature-based value function (though it does align with Shapley values for GAP \textit{classifiers}). To connect this approach to the theory of fair credit assignment, we show that GradCAM closely resembles Integrated Gradients (IG) \citep{sundararajan2017axiomatic}, an approximator to the Aumann-Shapley values \citep{aumann2015values}:
\vspace{-.05in}
\begin{proposition}
\label{prop:ig}
 Let $v(S): [0,1]^N \to \mathbb{R} \defeq f(mask(x, S))$ represent soft masking of the spatial locations of a deep feature map $x$ with the vector of zeros and applying a differentiable function $f$. GradCAM is equivalent to Integrated Gradients approximated with a single sample at $\alpha=1$ only if the function $f$ has spatially invariant derivatives: 
 \vspace{-.05in}
 $$\forall (h,w),(i,j) \in N: \frac{\partial f(x)}{\partial x_{chw}} = \frac{\partial f(x)}{\partial x_{cij}}$$
 In typical case where $f$ does not have spatially invariant derivatives \textul{GradCAM violates the dummy axiom} (see Section \ref{sec:shap-value}) and does not represent an approximation of Integrated Gradients.
 \end{proposition}
 \vspace{-.05in}
Where $\alpha$ refers to the parameter of IG that blends background and foreground samples. We note that the Aumann-Shapley values generalize the Shapley value to games where infinite numbers of players can join finitely many ``coalitions''. These values align with Shapley values for linear functions but diverge in the nonlinear case. Proposition \ref{prop:ig} also shows that in general GradCAM is sub-optimal and can be improved by considering Integrated Gradients on the feature space. We refer to this modification to GradCAM as Integrated Gradient Search Activation Maps or ``IG SAM''. We also note that this modification can be applied to classifier-based GradCAM to yield a more principled classifier interpretation approach. We explore this and show an example of GradCAM violating the dummy axiom in the Supplement.


\vspace{-0.1in}
\section{Second-Order Search Interpretations}
\vspace{-.1in}

Visualizing the pixels that explain a similarity judgement provides a simple way to inspect where a retrieval system is attending to. However, this visualization is only part of the story. Images can be similar for many different reasons, and a good explanation should clearly delineate these independent reasons. For example, consider the pair of images in the left column of Figure \ref{fig:joint-eval}. These images show two similar scenes of people playing with dogs, but in different arrangements. We seek not just a heatmap highlighting similar aspects, but a data-structure capturing how parts of the query image \textit{correspond} to parts of a retrieved image. To this end we seek to measure the interaction strength between areas of query and retrieved images as opposed to the effect of single features. We refer to this class of search and retrieval explanation methods as ``second-order'' methods due to their relation with second-order terms in the Shapley-Taylor expansion in Section \ref{sec:hd}. 

\vspace{-.1in}
\subsection{Harsanyi Dividends}
\label{sec:hd}
\vspace{-.1in}

 To capture the notion of interactions between query and retrieved images, we must consider credit assignments to \textit{coalitions} of features. \citep{harsanyi1963simplified} formalize this notion with a unique and axiomatically specified way to assign credit or ``Harsanyi Dividends'' to every possible coalition, $S$, of $N$ players in a cooperative game using the formula:
\begin{equation}
      d_v(S) \defeq
  \begin{cases}
    v(S) & \text{if $|S|=1$} \\
    v(S) - \sum_{T \subsetneq S} d_v(T) & \text{if $|S|>1$} \\
    \end{cases}
\end{equation}
%also known as Mob\"ius coefficients \citep{grabisch2016set}, 
%More specifically, \cite{owen1972multilinear} show they arise naturally through multi-linear extensions of games.
These dividends provide a detailed view of the function's behavior at every coalition. In particular, \cite{harsanyi1963simplified} show that Shapley values arise from distributing these dividends evenly across members of the coalitions, a process we refer to a ``projecting'' the dividends down. In this work we seek a second-order analog of the Shapley values, so we generalize the notion of sharing these dividends between individuals to sharing these dividends between sub-coalitions. This computation re-derives the recently proposed Shapley-Taylor Indices \citep{sundararajan2020shapley}, which generalize the Shapley values to coalitions of a size $k$ using the discrete derivative operator. More specifically, by sharing dividends, we can alternatively express Shapley-Taylor values for coalitions $|S| = k$ as:
\vspace{-.05in}
 \begin{equation}
    \phi^k_v(S) = \sum_{T:S \subset T} \frac{d_v(T)}{\binom{|T|}{|S|}}
 \end{equation}
Which states that the Shapley-Taylor indices arise from projecting Harsanyi dividends onto the $k^{th}$ order terms. We note that this interpretation of the Shapley-Taylor indices is slightly more flexible than that of \cite{sundararajan2020shapley} as it allows one to define ``jagged'' fair credit assignments over just the coalitions of interest. %In particular, we project dividends onto just the pairs of image locations that span query and retrieved images as well as individual features. This results in different individual feature terms.
 Equipped with the Shapley-Taylor indices, $\phi^k_v$, we can now formulate a value function for ``second-order'' search interpretations. As in the first order case, consider two spaces $\mathcal{X}$, $\mathcal{Y}$ equipped with a similarity function $d$. We introduce the second-order value function:
\vspace{-.05in}
\begin{equation}
    \label{eq:second-order}
    v_{2}(S): 2^N \to \mathbb{R} \defeq d(mask(x, S), mask(y, S))
\end{equation} 
 Where the grand coalition, $N = L_q \cup L_r$, are ``locations'' in both the query and retrieved images. These ``locations'' can represent either superpixels or coordinates in a deep feature map. Our challenge now reduces to computing Shapley-Taylor indices for this function.
 \vspace{-.1in}
 \subsection{A Fast Shapley-Taylor Approximation Kernel} 
 \label{sec:so-kernel}
 \vspace{-.1in}

\begin{figure}
\centering
\begin{minipage}[t]{.48\textwidth}
    \centering
    \includegraphics[width=.9\linewidth]{figures/kernel_convergence.jpg}
    \vspace{-.05in}
    \caption{Convergence of Shapley-Taylor estimation schemes with respect to the Mean Squared Error (MSE) on randomly initialized deep networks with 15 dimensional input. Our strategies (Kernel) converge with significantly fewer function evaluations.}
    \label{fig:kernel-speedup}
\end{minipage}%
\hspace{.01\textwidth}
\begin{minipage}[t]{.48\textwidth}
  \centering
    \includegraphics[width=\linewidth]{figures/joint_eval.jpg}
    \vspace{-.2in}
    \captionof{figure}{Our Second-order explanation evaluation strategy. A good method should project query objects (top left and middle) to corresponding objects in the retrieved image (bottom left and middle). When censoring all but these shared objects (right column) the search engine should view these images as similar.}
    \label{fig:joint-eval}
\end{minipage}
\vspace{-.1in}
\end{figure}

 Though the Harsanyi Dividends and Shapley-Taylor indices provide a robust way to allocate credit, they are difficult to compute. The authors of the Shapley-Taylor indices provide a sampling-based approximation, but this requires estimating each interaction term separately and scales poorly as dimensionality increases. To make this approach more tractable for high dimensional functions we draw a parallel to the unification of LIME with Shapley values through a linear regression weighting kernel. In particular, one can efficiently approximate Shapley values by randomly sampling coalitions, evaluating the value function, and fitting a weighted linear map from coalition vectors to function values. We find that this connection between Shapley values and weighted linear models naturally lifts to a weighted quadratic estimation problem in the ``second-order'' case. In particular, we introduce a weighting kernel for second order Shapley-Taylor indices:
\vspace{-.05in}
\begin{equation}
    \Lambda(S) = \frac{|N| -1}{\binom{|N|}{|S|} \binom{|S|}{2} (|N|-|S|)}
\end{equation}
Using this kernel, one can instead sample random coalitions, evaluate $v$, and aggregate the information into weighted quadratic model with a term for each distinct coalition $|S| \leq 2$. This allows one to approximate \textit{all} Shapley-Taylor indices of $k=2$ with a single sampling procedure, and often \textul{requires $10\times$ fewer function evaluations to achieve the same estimation accuracy}. We show this speedup in Figure \ref{fig:kernel-speedup} on randomly initialized 15-dimensional deep networks. A detailed description of this and other experiments in this work are in the supplement. We find that one can further speed up the method by directly sampling from the induced distribution (Kernel-Direct) as opposed to randomly sampling coalitions and calculating weights (Kernel-Weighting). This direct sampling can be achieved by first sampling the size of the coalition from $p(s) \propto (|N| -1)/(\binom{s}{2} (|N|-s))$ and then randomly sampling a coalition of that size. When our masking function operates on super-pixels, we refer to this as the second-order generalization of Kernel SHAP. This also gives insight into the proper form for a second-order generalization of LIME. In particular we add L1 regularization \citep{tibshirani1996regression} and replace our kernel with a local similarity, $\Lambda(S) = \exp(-\lambda|mask(x,S);mask(y,S) -x;y|_2^2)$ where ``$;$'' represents concatenation, to create a higher-order analogue of LIME. Finally we note that certain terms of the kernel are undefined due to the presence of $\binom{s}{2}$ and $|N| - |S|$ in the denominator. These ``infinite'' weight terms encode hard constraints in the linear system and correspond to the efficiency axiom. In practice we enumerate these terms and give them a very large weight ($10^8$) in our regression. We reiterate that our kernel approximator converges to the same, uniquely-defined, values as prior sampling approaches but requires significantly fewer function evaluations. 

\vspace{-.1in}
\subsection{Second-Order Search Activation Maps}
\label{sec:attn-proj}
\vspace{-.1in}

In the first-order case, CAM and its search engine generalization, Search Activation Maps, arise naturally from the Shapley values of our first-order value function, Equation \ref{eq:first-order}. To derive a second order generalization of SAM we now look to the Shapley-Taylor indices of our second order value function, Equation \ref{eq:second-order}, applied to the same GAP architecture described in Proposition \ref{prop:cam}.
\vspace{-.05in}
\begin{proposition}
\label{prop:so-cam}
 Let the spaces $\mathcal{X}$, $\mathcal{Y}$ and function $d$ be as in Proposition \ref{prop:cam}. Let the grand coalition, $N$, index into the spatial coordinates of both the query image features $x$ and retrieved image features $y$. Let the function $mask(y, S)$ act on a feature map $y$ by replacing the corresponding features with a background feature map $a$ for query features and $b$ for retrieved features. Then:
\vspace{-.05in}
\begin{equation}
    \phi_{v_2}(\{(h,w) \in \mathcal{L}_q, (i,j)\in \mathcal{L}_r\}) = \frac{1}{H^2W^2}\sum_c x_{chw}y_{cij} - a_{chw}y_{cij} - x_{chw}b_{cij} + a_{chw}b_{cij}
\end{equation}
\vspace{-.05in}
\end{proposition}
\vspace{-.1in}
We note that the first term of the summation corresponds to the frequently used correlation layer \citep{flownet,sun2020mining,wang2020learning,MaCoSNet} and generalizes the ``point-to-point'' signal in \cite{vedml}. In particular, \textul{our axiomatically derived version has the extra terms allow counterfactual explanations against different background signals}. Like in the first-order case, this closed form only holds in the GAP architecture. To extend the method in a principled way we use our second-order kernel approximator and refer to this as second-order KSAM. We also introduce a generalization using a higher order analogue of Integrated Gradients, Integrated Hessians \citep{integrated-hessians}, applied to our feature maps. We refer to this as second-order IGSAM. In Section \ref{sec:proof-prop-ig} of the Supplement we prove that this approach is proportional to the Shapley-Taylor indices for the GAP architecture. We can visualize these second-order explanations by aggregating these Shapley-Taylor indices into a matrix with query image locations as rows and retrieved locations as columns. Using this matrix, we can ``project'' signals from a query to retrieved image. We show a few examples of attention projection using our second-order SAM in Figure \ref{fig:attention_projection}.

\vspace{-.1in}
\section{Experimental Evaluation}
\vspace{-.1in}

\label{sec:eval}
\newcommand*\rot{\rotatebox{45}}
\newcommand*\rotrow{\rotatebox[origin=c]{90}}

\begin{table}[t]
\caption{Comparison of performance of first- and second-order search explanation methods. Methods introduced in this work are highlighted in \hl{pink}. *Though SAM generalizes \citep{vedml} we refer to it as a baseline. For additional details see Section \ref{sec:eval}}
\label{tab:results}
\vspace{-.18in}
\centering
\begin{tabular}{CCCCCCCCCCCC}
                                                               &                                                        &                                  & \rot{SBSM} & \textbf{\rot{PSHAP}} & \textbf{\rot{LIME}}     & \textbf{\rot{KSHAP}}                                       & \rot{VESM} & \rot{GCAM} & \rot{SAM*} & \textbf{\rot{IG SAM}}                 & \textbf{\rot{KSAM}}                                        \\ \cline{4-12} 
\rot{Metric}                                                  & \rot{Order}                                            & \multicolumn{1}{c|}{\rot{Model}} & \multicolumn{4}{c|}{Model Agnostic}                                                                                                                                          & \multicolumn{5}{c|}{Architecture Dependent}                                                                                                                                                                                            \\ \hline
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{DN121}       & 0.18                     & \cellcolor[HTML]{FFCCC9}\textbf{0.26}        & \cellcolor[HTML]{FFCCC9}0.23          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.24}          & 0.08                                                                 & 0.12                        & 0.12                      & \cellcolor[HTML]{FFCCC9}\textbf{0.20} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.20}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{MoCoV2}      & 0.22                     & \cellcolor[HTML]{FFCCC9}\textbf{0.30}        & \cellcolor[HTML]{FFCCC9}0.28          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.30}} & 0.13                                                                 & 0.19                        & 0.21                      & \cellcolor[HTML]{FFCCC9}0.25          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.25}}          \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{RN50}        & 0.11                     & \cellcolor[HTML]{FFCCC9}\textbf{0.16}        & \cellcolor[HTML]{FFCCC9}0.14          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.14}          & 0.04                                                                 & 0.08                        & 0.07                      & \cellcolor[HTML]{FFCCC9}\textbf{0.11} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.11}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{\multirow{-4}{*}{\rotrow{First}}}  & \multicolumn{1}{c|}{VGG11}       & 0.14                     & \cellcolor[HTML]{FFCCC9}\textbf{0.16}        & \cellcolor[HTML]{FFCCC9}0.15          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.15}          & 0.05                                                                 & 0.09                        & 0.11                      & \cellcolor[HTML]{FFCCC9}\textbf{0.14} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.14}} \\ \cline{2-12} 
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{DN121}       & 0.48                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.54} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.54}} & -                                                                    & -                           & 0.48                      & \cellcolor[HTML]{FFCCC9}0.48          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.49}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{MoCoV2}      & 0.69                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.74} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.74}} & -                                                                    & -                           & \textbf{0.72}             & \cellcolor[HTML]{FFCCC9}0.70           & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.71}          \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{RN50}        & 0.74                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.77} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.77}} & -                                                                    & -                           & \textbf{0.74}             & \cellcolor[HTML]{FFCCC9}\textbf{0.74} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.74}} \\
\multicolumn{1}{|c|}{\multirow{-8}{*}{\rotrow{Faithfulness}}} & \multicolumn{1}{c|}{\multirow{-4}{*}{\rotrow{Second}}} & \multicolumn{1}{c|}{VGG11}       & 0.68                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.71} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.71}} & -                                                                    & -                           & 0.69                      & \cellcolor[HTML]{FFCCC9}0.69          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.70}}  \\ \hline
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{DN121}       & -                        & \cellcolor[HTML]{FFCCC9}\textbf{0.00}        & \cellcolor[HTML]{FFCCC9}0.20          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -                                                                    & 12.8                        & 0.56                      & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{MoCoV2}      & -                        & \cellcolor[HTML]{FFCCC9}\textbf{0.00}        & \cellcolor[HTML]{FFCCC9}0.10          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -                                                                    & 0.46                        & 0.53                      & \cellcolor[HTML]{FFCCC9}0.03          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{RN50}        & -                        & \cellcolor[HTML]{FFCCC9}\textbf{0.00}        & \cellcolor[HTML]{FFCCC9}0.22          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -                                                                    & 14.9                        & 0.47                      & \cellcolor[HTML]{FFCCC9}0.03          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{\multirow{-4}{*}{\rotrow{First}}}  & \multicolumn{1}{c|}{VGG11}       & -                        & \cellcolor[HTML]{FFCCC9}\textbf{0.00}        & \cellcolor[HTML]{FFCCC9}0.27          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -                                                                    & 4.20                        & 0.54                      & \cellcolor[HTML]{FFCCC9}0.05          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\ \cline{2-12} 
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{DN121}       & -                        & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}0.14          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} & -                                                                    & -                           & 0.21                      & \cellcolor[HTML]{FFCCC9}0.03          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{MoCoV2}      & -                        & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}0.13          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} & -                                                                    & -                           & 0.20                      & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{RN50}        & -                        & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}0.06          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} & -                                                                    & -                           & 0.06                      & \cellcolor[HTML]{FFCCC9}\textbf{0.01} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} \\
\multicolumn{1}{|c|}{\multirow{-8}{*}{\rotrow{Inefficiency}}}  & \multicolumn{1}{c|}{\multirow{-4}{*}{\rotrow{Second}}} & \multicolumn{1}{c|}{VGG11}       & -                        & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}0.11          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} & -                                                                   & -                           & 0.22                      & \cellcolor[HTML]{FFCCC9}0.03          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} \\ \hline
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{DN121}       & 0.55                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.68} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.67}          & -                                                                    & -                           & \textbf{0.68}             & \cellcolor[HTML]{FFCCC9}\textbf{0.68} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.67}          \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{MoCoV2}      & 0.57                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.70} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.69}          & -                                                                    & -                           & \textbf{0.70}             & \cellcolor[HTML]{FFCCC9}\textbf{0.70} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.69}          \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{RN50}        & 0.55                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.67} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.66}          & -                                                                    & -                           & \textbf{0.69}             & \cellcolor[HTML]{FFCCC9}0.66          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.65}          \\
\multicolumn{1}{|c|}{\multirow{-4}{*}{\rotrow{mIoU}}}         & \multicolumn{1}{c|}{\multirow{-4}{*}{\rotrow{Second}}} & \multicolumn{1}{c|}{VGG11}       & 0.54                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.68} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.67}          & -                                                                    & -                           & 0.72                      & \cellcolor[HTML]{FFCCC9}\textbf{0.73} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.70}          \\ \hline
\end{tabular}

\vspace{-0.1in}
\end{table}

\paragraph{First Order Evaluation} Evaluating the quality of an interpretability method requires careful experimental design and is independent from what ``looks good'' to our human eye. If a model explanation method produces ``semantic'' connections between images it should be because to the underlying model is sensitive to these semantics. As a result, we adopt the evaluation strategy of \cite{shap}, which measures how well the model explanation approximates the expected influence of individual features. In particular, these works calculate each features importance, replace the top $n\%$ of features with background signal, and measure the effect on the function. A good model interpretability method should cause the replacement of the most important features, and hence cause the largest expected change in the function. We refer to this metric as the ``Faithfulness'' of an interpretation measure as it directly measures how well an interpretation method captures the behavior of an underlying model. Figure \ref{fig:marginal-eval} in the Supplement diagrams this process for clarity. In our experiments we blur the top $30\%$ of image pixels to compute faithfulness. For those methods that permit it, we also measure how much the explanation violates the efficiency axiom. In particular we compare the sum of explanation coefficients with the value of $v(N) - v(\emptyset)$ and refer to this as the ``Inefficiency'' of the method. For additional details and evaluation code please see Section \ref{sec:eval-details} in the Supplement. 
\vspace{-0.15in}
\paragraph{Second Order Evaluation} In the second-order case we adopt the evaluation strategy of \cite{integrated-hessians} which introduce a analogous second-order faithfulness measure. In particular, we measure how well model explanations approximate the expected \textit{interaction} between two features. To achieve this, we select an object from the query image, use the second order explanation to find the corresponding object in the retrieved image, censor all but these two objects. We measure the new similarity as a measure of Faithfulness and illustrate this process in In Figure \ref{fig:joint-eval}.  We additionally quantify the inefficiency of several second-order methods as well as their effectiveness for semantic segmentation label propagation. In particular, we measure how well the explanation method can project a source object onto a target object. We treat this as a binary segmentation problem and measure the mean intersection over union (mIoU) of the projected object with respect to the true object mask. We note that mIoU is not a direct measurement of interpretation quality, but it can be useful for those intending to use model-interpretation methods for label propagation \citep{ahn2019weakly,wang2020self}. These results demonstrate that axiomatically grounded model explanation methods such as IG SAM could offer improvement on downstream tasks. Because human evaluations introduce biases such as preference for compact or smoothness explanations, we consider Mechanical Turk \citep{paolacci2010running} studies outside the scope of this work. 
\vspace{-0.15in}
\paragraph{Datasets} We evaluate our methods on the Pascal VOC \citep{Everingham10} and MSCoCo \cite{cocostuff} semantic segmentation datasets. To compute first and second order faithfulness we mine pairs of related images with shared object classes. We use the MoCo V2 \citep{mocov2} unsupervised image representation method to featurize the training and validation sets. For each image in the validation set we choose a random object from the image and find the training image that contains an object of the same class \citep{hamilton2020conditional}.
\vspace{-0.15in}
\paragraph{Results} In Table \ref{tab:results} and Table \ref{tab:more-results} of the Supplement we report experimental results for PascalVOC and MSCoCo respectively. We evaluate across visual search engines created from four different backbone networks: DenseNet121 \citep{huang2017densely}, MoCo v2 \citep{mocov2}, ResNet50 \citep{he2016deep}, and VGG11 \citep{simonyan2014very} using cosine similarity on GAP features. As baselines we include VESM, SBSM, and SAM which generalizes \citep{vedml}. We note that SBSM was not originally presented as a second-order method, and we describe how it can be lifted to this higher order setting in Section \ref{sec:joint-sbsm} of the Supplement. We also evaluate several existing classifier explanation approaches applied to our search explanation value functions such as Integrated Gradients \citep{integrated-gradients} on image pixels, Partition SHAP \citep{partition-shap}, LIME, Kernel SHAP (KSHAP), and GradCAM (GCAM) on deep feature maps \citep{gradcam}. For second-order variants of LIME and SHAP we used the local weighting kernel and our Shapley-Taylor approximation kernel from Section \ref{sec:so-kernel}. Overall, several key trends appear. First, Shapley and Aumann-Shapley based approaches tend to be the most faithful and efficient methods, but at the price of longer computation time. One method that strikes a balance between speed and quality is our Integrated Gradient generalization of CAM which has both high faithfulness, low inefficiency, and only requires a handful of network evaluations ($\sim10^2$). Furthermore, grey-box feature interpretation methods like SAM and IG SAM tend to perform better for label propagation. Finally, our methods beat existing baselines in several different categories and help to complete the space of higher order interpretation approaches. We point readers to the Section \ref{sec:eval-details} for additional details, compute information, and code. 
\vspace{-.1in}
\section{Conclusion}
\vspace{-.1in}
In this work we have presented a uniquely specified and axiomatic framework for \textit{model-agnostic} search, retrieval, and metric learning interpretability using the theory of Harsanyi dividends. We characterize search engine interpretability methods as either ``first'' or ``second'' order methods depending on whether they extract the most important areas or pairwise correspondences, respectively. We show that Shapley values of a particular class of value functions generalize many first-order methods, and this allows us to fix issues present in existing approaches and extend these approaches to counterfactual explanations. For second order methods we show that Shapley-Taylor indices generalize the work of \cite{vedml} and use our framework to introduce generalizations of LIME, SHAP, and GradCAM. We apply these methods to extract image correspondences from opaque-box similarity models, a feat not yet presented in the literature. To accelerate estimation higher order Shapley-Taylor indices, we contribute a new weighting kernel that requires $10\times$ fewer function evaluations. Finally, we show this game-theoretic formalism yields methods that are more ``faithful'' to the underlying model and better satisfy efficiency axioms across several visual similarity methods. 


\subsubsection*{Acknowledgments}

We would like to thank Siddhartha Sen for sponsoring access to the Microsoft Research compute infrastructure. We would also like to thank Zhoutong Zhang, Jason Wang, and Markus Weimer for their helpful commentary on the work. We thank the Systems that Learn program at MIT CSAIL for their funding and support. 

This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. 2021323067. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the National Science Foundation.This work is supported by the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions, http://iaifi.org/)


% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{iclr2022_conference}
\bibliographystyle{iclr2022_conference}

\newpage

\appendix
\section{Appendix}


\subsection{Video and Code}

We include a short video description of our work at {\color{blue} \url{https://aka.ms/axiomatic-vdieo}}.

We also provide training and evaluation code at {\color{blue} \url{https://aka.ms/axiomatic-code}}

\subsection{Evaluation and Implementation Details}
\label{sec:eval-details}

\begin{figure}[h]
\centering
\includegraphics[height=170px,valign=t]{figures/marginal_eval.jpg}
\captionof{figure}{First-order interpretation evaluation strategy. A good method should highlight pixels in the query image (top left and middle) that, when censored (top right), have the largest possible impact on the cosine distance.}
\label{fig:marginal-eval}
\end{figure}

\paragraph{Models:} Our evaluation experiments use visual similarity systems built from ``backbone'' networks that featurize images and compare their similarity using cosine distance. We consider supervised backbones and contrastive unsupervised backbones. In particular, ResNet50 \citep{he2016deep}, VGG11 \citep{simonyan2014very}, and DenseNet121 \citep{huang2017densely} are trained with human classification annotations from the ImageNet dataset \citep{imagenet} and MoCo V2 is trained using unsupervised contrastive learning on ImageNet. We use torchvision \citep{torchvision} based model implementations and pre-trained weights except for MocoV2 which we download from \cite{mocogithub} (800 epoch model). For kernel convergence experiments in Figure \ref{fig:kernel-speedup} we use randomly initialized three layer deep networks with Glorot \citep{glorot2011deep} initialization, rectified linear unit activations, and a 20 dimensional hidden layer. We note that the functional form is not of much importance for these experiments so long as the function is nonlinear and non-quadratic. We provide an additional example in Figure \ref{fig:kernel-convergence-2} on random 15 dimensional Boolean functions formed by enumerating and summing all possible variable products and weighting each by a uniform coefficient between 0 and 10.

\paragraph{Data:} For evaluations within Table \ref{tab:results} we use the Pascal VOC \citep{Everingham10} dataset. In particular we form a paired image dataset by using MoCo V2 to featurize the training and validation sets. All experiments use images that have been bi-linearly resized to $224 \times 224$ pixels. For each image in the PascalVOC validation set we choose a random segmentation class that contains over $5\%$ of image pixels. We then find each validation image's closest ``Conditional Nearest Neighbor'' \citep{hamilton2020conditional} from the images of the training set of the chosen segmentation class. We use cosine similarity between MoCoV2 deep features to find nearest neighbors. With this dataset of pairs, we can then compute our first and second order evaluation metrics. We provide instructions for downloading the pairs of images in the attached code. We note that our approach for selecting pairs of images with matching segmentation labels allow for measuring Faithfulness and success in label propagation as measured by mIoU.  


\paragraph{Metrics:} Our attached code contains implementations all metrics for preciseness but we include descriptions of metrics here for clarity. To measure first order faithfulness, we take a given validation image and training image from our dataset of paired images and compute the first order heat-map over the validation image. We then blur the top $30\%$ of pixels by blurring the image with a $25\times25$ pixel blur kernel and replacing the top $30\%$ of original image pixels with those from the blurred image. The drop in cosine similarity between the unblurred images and the unblurred training and blurred validation image is the first order faithfulness. We illustrate our first-order evaluation strategy in Figure \ref{fig:marginal-eval}.

For our second-order evaluation, we use the ground truth semantic segmentation mask of the training image as a ``query'' attention signal. We then use the second-order interpretation methods to ``project'' this attention to the ``retrieved'' validation image. We censor all but the most-attended pixels in the retrieved image. The size of the remaining pixels matches the size of the validation image's selected semantic segmentation mask. In the second-order case we additionally measure the mean intersection over union (mIoU) of the resulting mask compared to the ground-truth retrieved image segmentation. A good approach should attend to jut the pixels of the segmentation class and thus yield a mIoU of 1 (maximum value) as a binary segmentation problem. We illustrate our second-order evaluation strategy in Figure \ref{fig:joint-eval}.

Finally, for those methods that permit it, we measure how much they violate the efficiency axiom by summing the interpretation coefficients and comparing with $v(N) - v(\emptyset)$. In the first order setting $v(N)$ is the similarity between query and retrieved image, and $v(\emptyset)$ is the similarity between query and a blurred retrieved image (with 25 pixel blur). In the second order setting $v(\emptyset)$ represents the similarity when both images are blurred. For SAM-based methods we replace features with those from blurred images. To compute the sum of interpretation coefficients for kernel methods we sum over Shapley values in the first order case and over Shapley-Taylor indices of order $k \leq 2$ in the second-order case. For Partition SHAP \cite{partition-shap} we sum coefficients over all pixels. For Integrated Hessian's we sum over all first and second order coefficients as described in \cite{integrated-hessians}. 

In tables we report mean values of Inefficiency, and Faithfulness metrics and note that for these experiemtns the Standard Error of the Mean (SEM) is far below the three significant figure precision of the table.


\paragraph{First Order Methods:} For first order explanations we use the official implementation of ImageLIME \citep{lime-github} and use the SHAP package for Integrated Gradients, Partition SHAP, and Kernel SHAP \citep{partition-shap}. We re-implement SBSM and VESM in PyTorch from the instructions provided in their papers. For sampling procedures such as LIME, Kernel SHAP, and Partition SHAP we use 5000 function evaluations. For first and second-order super-pixel based methods (LIME, Kernel-SHAP) we use the SLIC superpixel method \citep{achanta2010slic} provided in the Scipy library \citep{scipy} with $50$ segments, $compactness=10$, and $\sigma=3$. For SBSM we use a window size of $20$ pixels and a stride of $3$ pixels. We batch function evaluations with minibatch size $64$ for backbone networks and $64 \times 20$ for SAM based methods. For all background distributions we blur the images with a 25-pixel blur kernel with the exception of LIME and SBSM which use mean color backgrounds. 

\paragraph{Second Order Methods:} For second order methods we use the same background and superpixel algorithms, but implement all methods within PyTorch for uniform comparison. For SBSM, Kernel SHAP, and LIME we use $20000$ samples and for KSAM and IGSAM we use $40000$ samples. For IGSAM we use the expected Hessians method referenced in the supplement of \cite{integrated-hessians}. We use the PyTorch ``lstsq''function for solving linear systems. For more details on our generalization of SBSM see Section \ref{sec:joint-sbsm}.

\paragraph{Compute and Environment:} Experiments use PyTorch \citep{pytorch} v1.7 pre-trained models, on an Ubuntu 16.04 Azure NV24 Virtual Machine with Python 3.6. For all methods that require many network evaluations we use PyTorch DataLoaders with 18 background processes to eliminate IO bottlenecks. We standardize experiments using Azure Machine Learning and run each experiment on a separate virtual machine to avoid slowdowns due to scarce CPU or GPU resources.

% \paragraph{Marginal Search Interpretation}

% We first generalize the interpret-and-censor evaluation to marginal search interpretation methods. Intuitively speaking, a good marginal search interpretation should highlight the ``most important'' pixels that contribute to the similarity of the two images. Censoring these important pixels by filling them with a mean-color background should dramatically reduce the similarity between these two images. The magnitude of the change in the similarity, $\Delta d$, between uncensored and censored image pairs is a direct measurement of the interpretability method's quality and is a direct analogue of the interpret-and-censor strategy of \cite{shap}. Figure \ref{fig:marginal-eval} shows a diagram and example of our proposed strategy. We use each marginal interpretation strategy to extract an attention map over the query image pixels. Finally, we censor the top 20\% of important pixels in the query and re-evaluate the engine's similarity computation and record the change in distance, $\Delta d$. The marginal columns of Table \ref{tab:results} show the results of this experiment, and compares computation times across marginal methods, we find that SAM is the k, fastest method, but that PartitionSHAP provides the best quality interpretations and significantly outperforms the current best visual search approach SBSM \cite{sbsm}. The dominance of Partition SHAP suggests that the formal basis for SHAP also yields higher-fidelity interpretation methods. We describe our models, data, and implementation details in Section \ref{sec:eval-details}

% \paragraph{Joint Search Interpretation}

% We also extend the interpret-and-censor evaluation strategy to joint interpretations by evaluating an interpretation's ability to project attention maps. We generate different query attention maps using Pascal VOC's ground truth semantic segmentation labels. Intuitively speaking, a good joint interpretation should find the region of the retrieved image that most closely matches a selected region of the query image. Censoring all but this region, as shown in Figure \ref{fig:joint-eval}, should yield similar images according to the search engine. More specifically, first we censor the query image to a randomly chosen semantic segmentation class. Second, we use the chosen class' segmentation mask as a query attention map attention and project this attention to the retrieved image using the joint interpretation method. Third we then censor all but the top $p$ pixels of the retrieved image where, $p$ is the size of corresponding semantic segmentation mask in the retrieved image. Finally, we measure the similarity between the censored images. A faithful interpretation approach should yield a similar (small $d$) image pair after censoring.

% In addition to measuring faithfulness of the representation method we also measure how well the approach performs semantic segmentation label propagation. To measure this, we treat the projected attention as a binary semantic segmentation prediction and compare with the known ground truth segmentation labels. The mean intersection over union (mIoU) of this segmentation prediction provides a second measurement of interpretation quality more focused on a particular downstream task like label propagation. However, we caution that this is not a direct measurement of interpretation faithfulness, but we feel this provides a complementary view that can help creation of better label propagation systems. The joint columns of Table \ref{tab:results} show the results of these experiments over several interpretation methods that we have generalized to the joint domain. We find that SAM is by far the fastest, but Kernel SHAP significantly outperforms all methods in terms of faithfulness to the underlying search engine. We describe our models, data, and implementation details in Section \ref{sec:eval-details}

% We note that because this is the first work to identify these joint interpretations, there are no existing approaches in the literature to compare against in the joint domain. To compare against a reasonable literature-derived benchmark, we have additionally generalized the SBSM method to the joint setting and detail this construction in the Supplement. 

 
\subsection{Proof of Proposition \ref{prop:ig}}
\label{sec:proof-prop-ig}
 Let $v(S): [0,1]^N \to \mathbb{R} \defeq f(mask(x, S))$ represent soft masking of the spatial locations of a deep feature map $x$ with the vector of zeros and applying a differentiable function $f$. We begin with the formulation of Integrated Gradients:
 \begin{align*}
     \text{IG}_{hw}(S) &= (S_{hw} - S_{hw}') \int_{\alpha=0}^1 \frac{\partial v(\alpha S + (1-\alpha)S')}{\partial T_{hw}}
 \end{align*}
 In our case the foreground, $S \defeq \mathbbm{1}^{HW}$, is a mask of all $1$s and the background, $S'$, is the zero mask of the same shape. We note that the $\frac{\partial}{\partial T_{hw}}$ refers to taking the partial of the full input $\alpha S$, not just the mask $S$. We include this to stress the subtle difference which can be missed in a quick reading of the equations of \cite{integrated-gradients}. In this case our formula is simplified to:
  \begin{align*}
     \text{IG}_{hw}(S) &= \int_{\alpha=0}^1 \frac{\partial v(\alpha S)}{\partial T_{hw}}
 \end{align*}
 Approximating this integral with a single sample at $\alpha=1$ yields:
   \begin{align*}
     \text{IG}_{hw}(S) &\approx \frac{\partial v(S)}{\partial S_{hw}} \\
     &= \frac{\partial f(mask(x,S))}{\partial T_{hw}} \\
     &= \frac{\partial}{\partial T_{hw}} f(x \odot S) \\
     &= \sum_c x_{chw} \frac{\partial f(x)}{\partial{x_{chw}}} \\
     &= \sum_c x_{chw} GAP(\nabla_x f(x)) && \text{(Spatially Invariant Derivatives)} 
 \end{align*}
 
 Which is precisely the formulation of GradCAM. This also makes it clear that the global average pooling of GradCAM causes the method to deviate from integrated gradients in the general case. To construct a function where GradCAM violates the dummy axiom we simply have to violate the spatial invariance of gradients. We provide a specific example of this violation in \ref{sec:dummy-violation}.


\subsection{GradCAM Violates the Dummy Axiom}
\label{sec:dummy-violation}

\begin{figure}[h]
\centering
\includegraphics[width=.7\linewidth]{figures/dummy_violation.png}
\captionof{figure}{Interpretations of a function that purposely ignores the left half of the image. KSAM and IGSAM properly assign zero weight to these features. GradCAM does not and hence violates the dummy axiom of fair credit assignment.}
\label{fig:dummy-violation}
\end{figure}


It is straightforward to construct examples where GradCAM violates the dummy axiom. For example, consider the function:
$$d(x,y) = sim_{cosine}(GAP(x), GAP(y \odot M))$$
Where $sim_{cosine}$ represents cosine similarity, $\odot$ represents elementwise multiplcation, and $M\in [0,1]^{CHW}$ is a mask where $M_{chw} = 0$ if $w \leq \frac{W}{2}$ and $M_{chw}=1$ otherwise. Intuitively, $M$ removes the influence of any feature on the left of the image making these features ``dummy'' features for the model. Because GradCAM spatially averages the gradients prior to taking the inner product with the feature map all features are treated equally regardless of how they are used. In this example, depicted in Figure \ref{fig:dummy-violation}, positive contributions from the right side of the image are extended to the left side of the image despite the fact that the mask, $M$ stops these features from impacting the prediction. Using a Shapley or Aumann-Shapley approximator on the feature space does not suffer from this effect as shown in the two right columns of Figure \ref{fig:dummy-violation}.


\subsection{Integrated Gradient CAM}

Sections \ref{sec:dummy-violation} and \ref{sec:proof-prop-ig} demonstrate that GradCAM can violate the dummy axiom when the function has spatially varying gradients which is a common occurrence especially if one is trying to interpret deeper layers of a network. We remedy this by instead considering Integrated Gradients on a function which masks the spatial locations of a deep feature map. More specifically our Integrated Gradient generalization of CAM takes the following form:
\begin{equation}
    IGCAM(h,w) \defeq \int_{\alpha=0}^1 \frac{\partial f(b + \alpha M \odot (x - b))}{\partial T_{hw}}
\end{equation}
Where $f$ is the classification ``head'', $x \in \mathbb{R}^{CHW}$ is a tensor of deep image features, $M \defeq \mathbbm{1}^{HW}$ is a mask of $1$s over the spatial location of the features, $b \in \mathbb{R}^{CHW}$ is a background signal commonly taken to be zero in GradCAM. We note that the $\frac{\partial}{\partial T_{hw}}$ refers to taking the partial of the full input $b + \alpha M \odot (x - b)$, not just the mask. We include this to stress the subtle difference which can be missed in a quick reading of the equations of \cite{integrated-gradients}. This variant of GradCAM does not violate the dummy axiom and satisfies the axioms of the Aumann-Shapley fair credit assignment.




\newpage
\subsection{Additional Similarity Visualizations}

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/additional-search-interp.jpg}
\captionof{figure}{Additional first-order search interpretations on random image pairs from the Pascal VOC dataset}
\label{fig:additional-results}
\end{figure}


\newpage

\subsection{Additional Results for Stanford Online Products}

\begin{table}[h]
\centering
\caption{Comparison of performance of first-order search interpretation methods across different visual search systems on the Stanford Online Product dataset. Methods introduced in this work are highlighted in \hl{pink}. *Though SAM generalizes \cite{vedml} we refer to it as a baseline. For additional details see Section \ref{sec:eval}}
\label{tab:sop-results}

\begin{tabular}{CCCCCCCCCCC}
                                                             &                             & \rot{SBSM} & \textbf{\rot{PSHAP}}                  & \textbf{\rot{LIME}}          & \textbf{\rot{KSHAP}}                                       & \rot{VESM} & \rot{GCAM} & \rot{SAM*} & \textbf{\rot{IG SAM}}                 & \textbf{\rot{KSAM}}                                        \\ \cline{3-11} 
Metric                                                       & \multicolumn{1}{c|}{Model}  & \multicolumn{4}{c|}{Model Agnostic}                                                                                                            & \multicolumn{5}{c|}{Architecture Dependent}                                                                                               \\ \hline
\multicolumn{1}{|c}{}                                        & \multicolumn{1}{c|}{DN121}  & 0.18       & \cellcolor[HTML]{FFCCC9}\textbf{0.23} & \cellcolor[HTML]{FFCCC9}0.20  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.22}          & 0.09       & 0.13       & 0.12       & \cellcolor[HTML]{FFCCC9}\textbf{0.18} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.18}} \\
\multicolumn{1}{|c}{}                                        & \multicolumn{1}{c|}{MoCoV2} & 0.24       & \cellcolor[HTML]{FFCCC9}\textbf{0.30}  & \cellcolor[HTML]{FFCCC9}0.27 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.18}          & 0.14       & 0.2        & 0.21       & \cellcolor[HTML]{FFCCC9}\textbf{0.24} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.24}} \\
\multicolumn{1}{|c}{}                                        & \multicolumn{1}{c|}{RN50}   & 0.11       & \cellcolor[HTML]{FFCCC9}\textbf{0.14} & \cellcolor[HTML]{FFCCC9}0.12 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.13}          & 0.03       & 0.07       & 0.07       & \cellcolor[HTML]{FFCCC9}\textbf{0.10}  & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.10}} \\
\multicolumn{1}{|c}{\multirow{-4}{*}{\rotrow{Faith.}}} & \multicolumn{1}{c|}{VGG11}  & 0.15       & \cellcolor[HTML]{FFCCC9}\textbf{0.16} & \cellcolor[HTML]{FFCCC9}0.14 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.15}          & 0.04       & 0.08       & 0.09       & \cellcolor[HTML]{FFCCC9}\textbf{0.12} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.12}} \\ \hline
\multicolumn{1}{|c}{}                                        & \multicolumn{1}{c|}{DN121}  & -          & \cellcolor[HTML]{FFCCC9}\textbf{0.00} & \cellcolor[HTML]{FFCCC9}0.24 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -          & 11.2       & 0.54       & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c}{}                                        & \multicolumn{1}{c|}{MoCoV2} & -          & \cellcolor[HTML]{FFCCC9}\textbf{0.00} & \cellcolor[HTML]{FFCCC9}0.17 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -          & 0.34       & 0.57       & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c}{}                                        & \multicolumn{1}{c|}{RN50}   & -          & \cellcolor[HTML]{FFCCC9}\textbf{0.00} & \cellcolor[HTML]{FFCCC9}0.21 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -          & 13.6       & 0.39       & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c}{\multirow{-4}{*}{\rotrow{Ineff.}}}  & \multicolumn{1}{c|}{VGG11}  & -          & \cellcolor[HTML]{FFCCC9}\textbf{0.00} & \cellcolor[HTML]{FFCCC9}0.24 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -          & 4.13       & 0.47       & \cellcolor[HTML]{FFCCC9}0.04          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\ \hline
\end{tabular}
\end{table}


\subsection{Additional Results for Caltech-UCSD Birds 200 (CUB) Dataset}

\begin{table}[h]
\centering
\caption{Comparison of performance of first-order search interpretation methods across different visual search systems on the CUB dataset. Methods introduced in this work are highlighted in \hl{pink}. *Though SAM generalizes \cite{vedml} we refer to it as a baseline. RN50-ML refers to a ResNet50 architecture trained for metric learning on the CUB dataset with the margin loss \cite{roth2020revisiting}. For additional details see Section \ref{sec:eval}}
\label{tab:cub-results}
\begin{tabular}{CCCCCCCCCCC}
\textbf{}                                              & \textbf{}                    & \rot{SBSM} & \textbf{\rot{PSHAP}}                  & \textbf{\rot{LIME}}          & \textbf{\rot{KSHAP}}                                       & \rot{VESM} & \rot{GCAM} & \rot{SAM*} & \textbf{\rot{IG SAM}}                 & \textbf{\rot{KSAM}}                                        \\ \cline{3-11} 
Metric                                                 & \multicolumn{1}{c|}{Model}   & \multicolumn{4}{c|}{Model Agnostic}                                                                                                            & \multicolumn{5}{c|}{Architecture Dependant}                                                                                               \\ \hline
\multicolumn{1}{|c}{}                                  & \multicolumn{1}{c|}{DN121}   & 0.25       & \cellcolor[HTML]{FFCCC9}\textbf{0.38} & \cellcolor[HTML]{FFCCC9}0.31 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.34}          & 0.15       & 0.10       & 0.12       & \cellcolor[HTML]{FFCCC9}\textbf{0.30} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.30}} \\
\multicolumn{1}{|c}{}                                  & \multicolumn{1}{c|}{RN50-ML} & 0.39       & \cellcolor[HTML]{FFCCC9}\textbf{0.49} & \cellcolor[HTML]{FFCCC9}0.47 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.49}} & 0.04       & 0.14       & 0.17       & \cellcolor[HTML]{FFCCC9}\textbf{0.41} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.41}} \\
\multicolumn{1}{|c}{}                                  & \multicolumn{1}{c|}{MoCoV2}  & 0.32       & \cellcolor[HTML]{FFCCC9}\textbf{0.47} & \cellcolor[HTML]{FFCCC9}0.39 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.41}          & 0.26       & 0.26       & 0.26       & \cellcolor[HTML]{FFCCC9}\textbf{0.34} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.34}} \\
\multicolumn{1}{|c}{}                                  & \multicolumn{1}{c|}{RN50}    & 0.14       & \cellcolor[HTML]{FFCCC9}\textbf{0.21} & \cellcolor[HTML]{FFCCC9}0.18 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.18}          & 0.05       & 0.07       & 0.07       & \cellcolor[HTML]{FFCCC9}\textbf{0.14} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.14}} \\
\multicolumn{1}{|c}{\multirow{-5}{*}{\rotrow{Faith.}}} & \multicolumn{1}{c|}{VGG11}   & 0.23       & \cellcolor[HTML]{FFCCC9}\textbf{0.31} & \cellcolor[HTML]{FFCCC9}0.26 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.27}          & 0.11       & 0.15       & 0.16       & \cellcolor[HTML]{FFCCC9}\textbf{0.23} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.22}          \\ \hline
\multicolumn{1}{|c}{}                                  & \multicolumn{1}{c|}{DN121}   & -          & \cellcolor[HTML]{FFCCC9}\textbf{0.00} & \cellcolor[HTML]{FFCCC9}0.17 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -          & 16.0       & 0.58       & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c}{}                                  & \multicolumn{1}{c|}{RN50-ML} & -          & \cellcolor[HTML]{FFCCC9}\textbf{0.00} & \cellcolor[HTML]{FFCCC9}0.13 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -          & 5.23       & 0.48       & \cellcolor[HTML]{FFCCC9}0.03          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c}{}                                  & \multicolumn{1}{c|}{MoCoV2}  & -          & \cellcolor[HTML]{FFCCC9}\textbf{0.00} & \cellcolor[HTML]{FFCCC9}0.19 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -          & 0.44       & 0.60       & \cellcolor[HTML]{FFCCC9}0.03          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c}{}                                  & \multicolumn{1}{c|}{RN50}    & -          & \cellcolor[HTML]{FFCCC9}\textbf{0.00} & \cellcolor[HTML]{FFCCC9}0.15 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -          & 15.5       & 0.43       & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c}{\multirow{-5}{*}{\rotrow{Ineff.}}} & \multicolumn{1}{c|}{VGG11}   & -          & \cellcolor[HTML]{FFCCC9}\textbf{0.00} & \cellcolor[HTML]{FFCCC9}0.17 & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -          & 4.25       & 0.54       & \cellcolor[HTML]{FFCCC9}0.05          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\ \hline
\end{tabular}
\end{table}


\newpage

\subsection{Additional Results for MS COCO}


\begin{table}[h]
\caption{Comparison of performance of first and second-order search interpretation methods across different visual search systems on the MSCOCO dataset. Methods introduced in this work are highlighted in \hl{pink}. *Though SAM generalizes \cite{vedml} we refer to it as a baseline. For additional details see Section \ref{sec:eval}}
\label{tab:more-results}
\vspace{-.04in}
\centering
\begin{tabular}{CCCCCCCCCCCC}
                                                              &                                                        &                                  & \rot{SBSM} & \textbf{\rot{PSHAP}} & \textbf{\rot{LIME}}     & \textbf{\rot{KSHAP}}                                       & \rot{VESM}  & \rot{GCAM} & \rot{SAM*} & \textbf{\rot{IG SAM}}                 & \textbf{\rot{KSAM}}                                        \\ \cline{4-12} 
\rot{Metric}                                                  & \rot{Order}                                            & \multicolumn{1}{c|}{\rot{Model}} & \multicolumn{4}{c|}{Model Agnostic}                                                                                                                                          & \multicolumn{5}{c|}{Architecture Dependent}                                                                                                                                                                                            \\ \hline
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{DN121}       & 0.18                     & \cellcolor[HTML]{FFCCC9}\textbf{0.24}        & \cellcolor[HTML]{FFCCC9}0.22          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.22}          & 0.10                                                                 & 0.12                        & 0.11                      & \cellcolor[HTML]{FFCCC9}0.14          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.17}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{MoCoV2}      & 0.25                     & \cellcolor[HTML]{FFCCC9}\textbf{0.37}        & \cellcolor[HTML]{FFCCC9}0.33          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.35}          & 0.15                                                                 & 0.23                        & 0.24                      & \cellcolor[HTML]{FFCCC9}0.24          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.28}}          \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{RN50}        & 0.10                     & \cellcolor[HTML]{FFCCC9}\textbf{0.14}        & \cellcolor[HTML]{FFCCC9}0.12          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.12}          & 0.04                                                                 & 0.07                        & 0.07                      & \cellcolor[HTML]{FFCCC9}0.07          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.09}}          \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{\multirow{-4}{*}{\rotrow{First}}}  & \multicolumn{1}{c|}{VGG11}       & 0.14                     & \cellcolor[HTML]{FFCCC9}\textbf{0.15}        & \cellcolor[HTML]{FFCCC9}0.14          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.14}          & 0.05                                                                 & 0.09                        & 0.1                       & \cellcolor[HTML]{FFCCC9}0.10          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.12}} \\ \cline{2-12} 
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{DN121}       & 0.49                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.57} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.57}} & -                                                                    & -                           & 0.5                       & \cellcolor[HTML]{FFCCC9}\textbf{0.51} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.49}          \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{MoCoV2}      & 0.73                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.79} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.79}} & -                                                                    & -                           & 0.77                      & \cellcolor[HTML]{FFCCC9}0.77          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.78}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{RN50}        & 0.73                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.78} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.78}} & -                                                                    & -                           & \textbf{0.75}             & \cellcolor[HTML]{FFCCC9}\textbf{0.75} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.73}          \\
\multicolumn{1}{|c|}{\multirow{-8}{*}{\rotrow{Faithfulness}}} & \multicolumn{1}{c|}{\multirow{-4}{*}{\rotrow{Second}}} & \multicolumn{1}{c|}{VGG11}       & 0.67                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.73} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.73}} & -                                                                    & -                           & 0.71                      & \cellcolor[HTML]{FFCCC9}0.71          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.72}} \\ \hline
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{DN121}       & -                        & \cellcolor[HTML]{FFCCC9}\textbf{0.00}        & \cellcolor[HTML]{FFCCC9}0.22          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -                                                                    & 12.3                        & 0.6                       & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{MoCoV2}      & -                        & \cellcolor[HTML]{FFCCC9}\textbf{0.00}        & \cellcolor[HTML]{FFCCC9}0.11          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -                                                                    & 0.46                        & 0.66                      & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{RN50}        & -                        & \cellcolor[HTML]{FFCCC9}\textbf{0.00}        & \cellcolor[HTML]{FFCCC9}0.22          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -                                                                    & 15.8                        & 0.47                      & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{\multirow{-4}{*}{\rotrow{First}}}  & \multicolumn{1}{c|}{VGG11}       & -                        & \cellcolor[HTML]{FFCCC9}\textbf{0.00}        & \cellcolor[HTML]{FFCCC9}0.31          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} & -                                                                    & 3.47                        & 0.59                      & \cellcolor[HTML]{FFCCC9}0.04          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\ \cline{2-12} 
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{DN121}       & -                        & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}0.15          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} & -                                                                    & -                           & 0.20                      & \cellcolor[HTML]{FFCCC9}0.01          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{MoCoV2}      & -                        & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}0.10          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} & -                                                                    & -                           & 0.09                      & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{RN50}        & -                        & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}0.07          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} & -                                                                    & -                           & 0.07                      & \cellcolor[HTML]{FFCCC9}0.02          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\
\multicolumn{1}{|c|}{\multirow{-8}{*}{\rotrow{Inefficiency}}}  & \multicolumn{1}{c|}{\multirow{-4}{*}{\rotrow{Second}}} & \multicolumn{1}{c|}{VGG11}       & -                        & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}0.11          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.01}} & -                                                                   & -                           & 0.19                      & \cellcolor[HTML]{FFCCC9}0.04          & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}\textbf{0.00}} \\ \hline
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{DN121}       & 0.50                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.62} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.61}          & -                                                                    & -                           & 0.62                      & \cellcolor[HTML]{FFCCC9}\textbf{0.63} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.52}          \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{MoCoV2}      & 0.52                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.62} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.61}          & -                                                                    & -                           & 0.64                      & \cellcolor[HTML]{FFCCC9}\textbf{0.66} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.60}          \\
\multicolumn{1}{|c|}{}                                        & \multicolumn{1}{c|}{}                                  & \multicolumn{1}{c|}{RN50}        & 0.50                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.62} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.61}          & -                                                                    & -                           & 0.63                      & \cellcolor[HTML]{FFCCC9}\textbf{0.65} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.48}          \\
\multicolumn{1}{|c|}{\multirow{-4}{*}{\rotrow{mIoU}}}         & \multicolumn{1}{c|}{\multirow{-4}{*}{\rotrow{Second}}} & \multicolumn{1}{c|}{VGG11}       & 0.50                     & \cellcolor[HTML]{FFCCC9}-                    & \cellcolor[HTML]{FFCCC9}\textbf{0.62} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.61}          & -                                                                    & -                           & 0.66                      & \cellcolor[HTML]{FFCCC9}\textbf{0.67} & \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}0.60}          \\ \hline
\end{tabular}

\vskip -0.2in
\end{table}

\newpage
\subsection{Additional Kernel Convergence Results}
\begin{figure}[h]
\centering
\includegraphics[width=.7\linewidth]{figures/kernel_convergence_2.jpg}
\captionof{figure}{Kernel convergence for random functions generated by randomly choosing coefficients. Results generally mirror those for randomly initialized deep networks}
\label{fig:kernel-convergence-2}
\end{figure}

\subsection{Generalizing SBSM to Joint Search Engine Interpretability}
\label{sec:joint-sbsm}

Before generalizing SBSM \citep{sbsm} to joint interpretability we will review the original implementation for marginal interpretability. SBSM uses a sliding square mask and multiple evaluations of the search engine to determine which regions of the image are important for similarity. More formally, let $q$, and $r$ represent the pixels of the query image and retrieved image. Let $M^s_{ij}(q)$ represent the result of replacing a square of pixels of size $s \times s$ centered at pixel $(i,j)$ with a ``background value'' which in our case is black. SBSM ``slides'' this mask across the query image and compares the similarity between the masked query and retrieved image. These masked similarity values are compares to the baseline similarity value and stored in a weight matrix, $w$:
\begin{equation}
    w_{ij} = \min \left[ d\left( M^s_{ij}(q), r\right) - d\left(q, r\right), 0\right]
\end{equation}
Intuitively speaking, the weights $w_{ij}$ represent the impact of masking a square centered at $(i, j)$. For areas that are critical to the similarity, this will result in $w_{ij}>0$. Finally, an attention mask on the query image is formed by a weighted average of the masks used to censor the images. For square masks, this can be achieved efficiently using a deconvolution with a kernel of ones of size $s \times s$ on the weight matrix $w$. We also note that instead of evaluating the (expensive) distance computation $d$ for every pixel $(i,j)$, one can also sample pixels to censor. We use this approach in our joint generalization. 

To generalize SBSM we use a pair of masks, one for the query image and one for the retrieved image respectively. We sample mask locations and calculate weights designed to capture the intuition that censoring corresponding areas cause similarity to increase as opposed to decrease. More specifically we use the following weighting scheme:
\begin{equation}
    \label{eqn:joint-sbsm}
    w_{ij}^{hw} = \min \left[ d\left(q, r\right) - d\left( M^s_{ij}\left(q\right), M^s_{hq}\left(r\right)\right), 0\right]
\end{equation}
Because evaluating the similarity function for every $(i,j,h,w)$ combination is prohibitively expensive, we instead sample masked images for our computation. To project attention from a query pixel, we query for all masks that overlap with the selected query pixel, and then average their corresponding retrieved masks according to the weights calculated in Equation \ref{eqn:joint-sbsm}.

\subsection{Proof of Proposition \ref{prop:cam}}


 Let  $\mathcal{X} = \mathcal{Y} = \mathbb{R}^{CHW}$ and represent the space of deep network features where $C,H,W$ represent a channel, height, and width of the feature maps respectively. Let the function $d \defeq \sum_c GAP(x)_c GAP(y)_c$. Let the grand coalition, $N = [0,H] \times [0,W]$, index into the spatial coordinates of the image feature map $y$. Let the function $mask(y, S)$ act on a feature map $y$ by replacing the features at locations $S$ with a background signal $b$. For notational convenience let $\psi_i (v) \defeq \phi_v(i)$ represent the Shapley value the $i^{th}$ player under the value function $v$. We begin by expressing the left-hand side of the proposition:
 \begin{align*}
\psi_{hw}(v) &=  \psi_{hw} \left( \sum_c GAP(x)_c GAP(mask(y, S))_c \right)\\
&= \psi_{hw} \left( \frac{1}{HW} \sum_c GAP(x)_c \sum_{h'w'} mask(y, S)_{ch'w'} \right)\\
&= \frac{1}{HW} \sum_{h'w'} \psi_{hw} \left(  \sum_c GAP(x)_c  mask(y, S)_{ch'w'} \right) && \text{(Linearity)} \\
&= \frac{1}{HW} \psi_{hw} \left(  \sum_c GAP(x)_c  mask(y, S)_{chw} \right) && \text{(Dummy)} \\
&= \frac{1}{HW} \sum_c GAP(x)_c  (y_{chw} - b_{chw}) && \text{(Efficiency)}\\
\end{align*}




\subsection{Proof of Proposition \ref{prop:so-cam}}

 Let the spaces $\mathcal{X}$, $\mathcal{Y}$ and function $d$ be as in Proposition \ref{prop:cam}. As a reminder the function $d$ represents the un-normalized GAP similarity function. Let the grand coalition, $N$, index into the spatial coordinates of both the query image features $x \in R^{CHW}$ and retrieved image features $y\in R^{CHW}$. Let the function $mask(y, S)$ act on a feature map $y$ by replacing the corresponding features with a background feature map $a$ for query features and $b$ for retrieved features. We can represent the set of players, $N$, as a set of ordered pairs of coordinates with additional information about which tensor, the query (0) or retrieved (1) features, they represent:
\begin{equation}
    N = ([1,H] \times [1,W] \times \{0\}) \cup ([1,H] \times [1,W]  \times \{1\})
\end{equation}  
In the subsequent proof we omit these $0$, $1$ tags as it is clear from our notation which side, query or retrieved, the index refers to based on the index $h,w$ for the query and $i,j$ for the retrieved image. We first consider the zero background value function, $v(S \subset N)$, defined by censoring the spatially varying features prior to global average pooling and comparing their inner product:
$$ v(S) = \left( \frac{1}{HW}\sum_{h,w} \tilde{x}_{chw} \right) \cdot \left( \frac{1}{HW}\sum_{i,j} \tilde{y}_{cij} \right) $$
where 
\[ \tilde{x}_{chw} = \begin{cases} 
      x_{chw}  & (h,w) \in S \\
      0 & o.w. 
   \end{cases}
\]
and likewise, for $y_{cij}$. When $S$ contains all $i,j,h,w$ this represents the similarity judgement from the GAP network architecture. We seek the Shapley-Taylor index for a pair of image locations $S =\{ (h,w), (i, j) \}$. For notational convenience let $\psi^k_S (v) \defeq \phi^k_v(S)$ represent the $k-$order interaction effects for the subset $S$ and the value function $v$.
\begin{align*}
\psi_S^k(v) &= \psi_S^k\left( \left(  \frac{1}{HW}\sum_{h',w'} \tilde{x}_{ch'w'} \right) \cdot \left(  \frac{1}{HW}\sum_{i',j'} \tilde{y}_{ci'j'} \right) \right) \\
&= \psi_S^k\left( \frac{1}{H^2W^2} \sum_c \sum_{h',w'} \sum_{i',j'} \tilde{x}_{ch'w'} \tilde{y}_{ci'j'} \right) \\
&= \psi_S^k\left( \frac{1}{H^2W^2}\sum_{h',w'} \sum_{i',j'} \sum_c \tilde{x}_{ch'w'} \tilde{y}_{ci'j'} \right) \\
&=  \sum_{h',w'} \sum_{i',j'} \psi_S^k\left( \sum_c \frac{1}{H^2W^2} \tilde{x}_{ch'w'} \tilde{y}_{ci'j'} \right)  && \text{(Linearity)}\\
&= \psi_S^k\left( \sum_c \frac{1}{H^2W^2} \tilde{x}_{chw} \tilde{y}_{cij} \right)  && \text{(Dummy)}\\
&= \psi_{S'}^k\left( v_{hwij} \right)  && \text{(Renaming)}\\
\end{align*}
Where the renaming of the last step was because we can now consider a simplified value function with just the non-dummy players as $v_{hwij}(S') := \sum_c \tilde{x}_{chw} \tilde{y}_{cij}$. Where $S'$ represents a subset of the non-dummy players: $N' = \{(h,w), (i,j)\}$. We can now explicitly calculate the index:
\begin{align*}
\psi_{S'}^2(v) &= \frac{2}{n} \sum_{T \subseteq N' \setminus S'} \delta_{S'} v_{hwij}(T) \frac{1}{\binom{n-1}{t}}\\
&= \delta_{S'} v_{hwij}(\emptyset)\\
&=  \frac{1}{H^2W^2}\sum_c x_{chw} y_{cij} \\
\end{align*}
By following the same set of reasoning, we can introduce nonzero background values $a_{chw}$ and $b_{cij}$ to yield the following:
\begin{equation}
\psi_{hw,ij}^2(v) = \frac{1}{H^2W^2}\sum_c x_{chw} y_{cij} -  x_{chw} b_{cij} -  a_{chw} y_{cij} +  a_{chw} b_{cij}
\end{equation}

\subsection{Proof that Shapley Taylor is Proportional to Integrated Hessians for GAP architecture}

As in Proposition \ref{prop:ig} we consider the soft masking or ``multilinear extension'' of our second-order value function $v_2$:
\begin{equation}
    v_2(S) : [0,1]^N \to \mathbb{R} \defeq d(mask(x, S), mask(y, S))
\end{equation}

let $hw$, and $ij$ be members of the grand coalition $N$ such that $hw \neq ij$. We begin our proof with the expression for the off-diagonal terms of the Integrated Hessian. 
\begin{equation}
    \Gamma_{hw,ij}(S) \defeq  \int_{\alpha = 0}^1 \int_{\beta = 0}^1 \alpha \beta \frac{\partial^2 v_2(\alpha\beta S)}{\partial T_{hw} \partial T_{ij}}
\end{equation}
Where $\frac{\partial}{\partial T_hw}$ represents the $hw$ component of the partial derivative with respect to $\alpha\beta S$, not to be confused with the partial derivative of just S. Like in our proof of Proposition \ref{prop:ig}, because our function is defined on the interval $[0,1]^N$ many of the terms mentioned in \cite{integrated-hessians} drop out and instead are captured in the Hessian of the function with repspect to the soft mask. We now expand the definition of $v_2(\alpha\beta S)$:
\begin{align*}
    v_2(\alpha\beta S) &= d(mask(x, \alpha\beta S), mask(y, \alpha\beta S)) \\
    &= \frac{1}{H^2W^2} \sum_c \left( \sum_{h,w} a_{chw}  + \alpha\beta S_{hw} (x_{chw}-a_{chw})\right) \left( \sum_{i,j} b_{cij}  + \alpha\beta S_{ij}(y_{cij}-b_{cij}) \right)
\end{align*}
From this function we can read off the appropriate term of the hessian with respect to the mask at location $(h,w)$ and location $(i,j)$
\begin{align*}
    \frac{\partial^2 v_2(\alpha\beta S)}{\partial T_{hw} \partial T_{ij}} &= \frac{1}{H^2W^2} \sum_c x_{chw}y_{cij} - x_{chw}b_{cij} - a_{chw}y_{cij} + a_{chw}b_{cij} \\
    &= \psi_{hw,ij}^2(v) 
\end{align*}
We can now pull this outside the integral to yeild:
\begin{align*}
    \Gamma_{hw,ij}(v_2) &= \int_{\alpha = 0}^1 \int_{\beta = 0}^1 \alpha \beta \frac{\partial^2 v_2(\alpha\beta S)}{\partial T_{hw} \partial T_{ij}} \\
         &=\psi_{hw,ij}^2(v)  \int_{\alpha = 0}^1 \int_{\beta = 0}^1 \alpha \beta \\
          &=\frac{1}{4} \psi_{hw,ij}^2(v)  
\end{align*}
Which proves that the Shapley-Taylor index and second order Aumann-Shapley values are proportional for the GAP architecture. 



\subsection{Explaining Dissimilarity}

In addition to explaining the similarity between two images, our methods naturally explain image dissimilarity. In particular, regions with a negative Shapely values (Blue regions in Figure \ref{fig:explaining-dissimilarity}) contribute negatively to the similarity between the two images. These coefficients can be helpful when trying to understand why an algorithm does not group two images together. 

\begin{figure}[h]
\centering
\includegraphics[width=.5\linewidth]{figures/explaining_dissimilarity.png}
\captionof{figure}{Explanation of why two images are similar (Red) and dissimilar (Blue). Blue regions highlight major differences between the images such as the dog playing the guitar, and the chain-link fence in the retrieved image.}
\label{fig:explaining-dissimilarity}
\end{figure}

\subsection{On the ''Axiomatic'' terminology}


The term ``axiomatic'' can mean different things to different readers. When this work refers to ``axiomatic methods we refer to methods that approximate the uniquely specified explanation values dictated by the axioms of fair-credit assignment. In the first-order case, these explanations are the Shapey Values and satisfy the axioms of linearity, efficiency, dummy, and symmetry. In the higher-order case these fair credit assignments are the Shapley-Taylor Indices and satisfy analogous axioms \cite{sundararajan2020shapley}.  We note that our methods \textit{converge} to the true Shapley and Shapley-Taylor indices and thus the deviations that arise as part of convergence induce corresponding deviations from the axioms of fair credit assignment. Nevertheless, we find that these deviations become negligible as our methods converge to the true Shapley and Shapley-Taylor values. This starkly contrasts the behavior of methods that do not converge to values that satisfy the axioms of fair credit assignment such as GradCAM as shown in Figure \ref{fig:dummy-violation}.



\end{document}
