\subsection{Learning as inference over group mappings}
\label{sec:group_learners}
Groups provide basic algebraic structures. At this point we will look into several properties that apply to learning of mappings between abelian groups. Since the concept of an abelian group is more general than that of a vector space, all our results readily apply to learning of mappings between vector spaces.


Next, we will consider the following framework for learning mappings $f:\inspace \to \outspace$ between such groups $(\inspace,+), (\outspace,+)$: 

At \emph{stage} or \emph{time} $n \in \nat$, we assume we have access to a \textit{sample} or \textit{data set} $\data_n:= \{\bigl( s_i, \tilde f_i\bigr) \vert i=1,\ldots, N_n \} $ containing $N_n \in \nat$ (possibly erroneous) sample images $\tilde f_i \in \outspace$ of function $f$ at sample input $s_i \in \inspace$. Here the term \emph{erroneous} refers to the situation of bounded additive corruption of the sample images. That is,  $f_i = \tilde f(s_i) = f(s_i) + \phi(s_i)$ where $\phi: \inspace \to \outspace$ is a stochastic or deterministic perturbation we refer to as \emph{(observational) noise} and $\tilde f$ defines the noise-corrupted version of $f$ we can observe. For the purpose of worst-case analysis, we assume the noise to be bounded, i.e. $\metric_\outspace(\phi(x),0) \leq \obserr(x) \leq \obserrbnd <\infty$ for all inputs $x$ of interest.
We refer to $\obserr$ as the \emph{ observational error} and to $\obserrbnd \in \Real_{\geq 0}$ as the \emph{observational error bound}.
To aide our discussion, we define $G_n =\{s_i | i =1,\ldots,N_n\}$ to be the \textit{grid} of sample inputs contained in $\data_n$. Subscript $n$ aides our exposition when we consider sequences $\seq{\data_n}{n \in \nat}$ of data sets indexed by $n$.

Let $I \subseteq \inspace$ be some domain of interest.
The task of learning the \emph{target} $f: I  \to \outspace$ on the basis of data $\data_n$ consists in the generation of a computable \emph{predictor} $\predfn: I \to \outspace$ that somehow approximates the target. Since the predictor is constructed on the basis of the data, we can construe computation of predictions $\predfn(x)$ as \emph{inferences} over function values. Therefore, any algorithm for computing $\predfn(x)$ will be referred to as an \emph{inference rule}. In this lingo, \emph{learning} is the construction of inference rules over function images and machine learning is the discipline of conceiving computatable inference rules.  
That is, a \emph{learner} $\ell$ constitutes a computable mapping $\ell: \data_n \mapsto \predfn$.

For the sake of analysis, the quality of the inference has to be quantifiable. To this end, we will assume the groups $\inspace, \outspace$ are endowed with (pseudo-) metrics $\metric_\inspace,\metric_{\outspace}$, respectively. The output pseudo-metric $\metric_\outspace$ allows us to quantify the error of a prediction as per $\metric_{\outspace} \bigl(\predfn(x),f(x) \bigr) $.
Furthermore, the input pseudo-metric $\metric_\inspace$ allows us to quantify the similarity between a test input $x \in I\subseteq \inspace$ and the learning experience given by the data. Intuitively, a sensible learner might perform well on test inputs $x$ that are similar to the learning experience. That is, we might expect the prediction error  $\metric_{\outspace} \bigl(\predfn(x),f(x) \bigr) $ to decrease with decreasing dissimilarity to the data measured by $\dist(x,\data_n) := \inf_{s \in G_n}  \metric_\inspace(x,s)$. It might not be surprising that we will be able to take advantage of H\"older continuity properties in order to establish such a desirable behaviour of an inference rule. 
In particular, they will prove key in the examination of uniform convergence of the \emph{worst-case prediction error} 
% 
$\sup_{x\in I} \metric_{\outspace} \bigl( \predfn(x), f(x) )$ as $n \to \infty$ in situations where 
the data becomes increasingly dense, i.e. where $\dist(G_n,I):= \sup_{x \in I} \inf_{s \in G_n} \metric_\inspace(x,s)  \stackrel{n \to \infty}{\rightarrow} 0$. In particular, we will construct a class of H\"older continuous inference rules that are universal approximators in the sense that they can guarantee a vanishing worst-case error in the limit of dense grids as long as the target $f$ is continuous (but not necessarily H\"older).
As a preliminary step, we will next provide convergence guarantees for certain H\"older continuous inference rules in situations where the target is indeed H\"older.

\subsubsection{Convergence properties of sample-consistent learners of H\"older continuous group mappings}

It will be convenient to restrict our attention to groups $\inspace$ that are \emph{translation-invariant} with respect to their metric $\metric$. That is, we assume $ \metric(g+g',g''+g') = \metric(g,g''),\forall g,g',g'' \in \inspace$.

In such groups the following holds true:

\begin{lem} Let $\metric : \inspace^2 \to \Real$ be a translation-invariant pseudo-metric on an abelian group $(\inspace,+)$.
We have 
\begin{itemize}
\item \textbf{(I)} $\forall g,g',g'' \in \inspace: \metric(g+g',g'') \leq \metric(g,g'')+\metric(g',0)$.
\item \textbf{(II)} $\forall g \in \inspace: \metric(g,0) = \metric(-g,0)$.
\item \textbf{(III)} If $g'' = g+g'$ and $\metric(g'',g) \leq b$ then $\metric(0,g') \leq b$.
\end{itemize}
\begin{proof}
(I): $\forall g,g',g'' \in \inspace: \metric(g+g',g'') \leq \metric(g+g',g)+\metric(g,g'') = \metric(g',0)+\metric(g,g'').$
(II): $\forall g \in \inspace: \metric(g,0) = \metric(g+(-g),-g) = \metric(0,-g) =\metric(-g,0).$
(III): Follows trivially from translation-invariance.
\end{proof}
\label{lem:bilinaddtransinvgroup}
\end{lem}
 
\begin{defn} [Sample-consistency] A predictor $\predfn : \inspace \to \outspace$ is consistent with sample $\data_n$ (up to error $E: \inspace \to \Real$) iff $\forall (s_i,\tilde f_i,\obserr(s_i) ) \in \data_n: \metric_{\outspace}\bigl(\predfn(s_i),\tilde f(s_i)\bigr) \leq E(s_i) $. An inference is called \emph{sample-consistent} if for every possible sample $\data_n$ the generated predictor $\predfn$ is consistent with $\data_n$. We denote the set of all functions consistent with sample $\data_n$ by $\mathcal K (\data_n)$.
\label{def:sampleconsistency}
\end{defn}

\begin{lem}\label{lem:groups_sampleconsandobserr} As always, let $\obserr(\cdot) $ denote the bound on the observational error.
If $\predf_n$ is sample-consistent up to error $E$ with respect to set data $\data_n$ then, for any sample input $s \in G_n$, we have $\metric_\outspace\bigl(f(s), \predfn(s) \bigr) \leq E(s) + \metric_\outspace(0,\obserr(s))$.  Given an upper bound  $\obserrbnd \in \Real$ on the observational error  such that $\obserrbnd \geq \obserr(x),\forall x$ then sample-consistency up to error $E$ entails that true function is contained in the $E(s) + \obserrbnd$-balls around the predictions of observed inputs. That is, for all $s \in G_n$ we have:
\begin{equation}
 f(s) \in \ball{ E(s) + \obserrbnd}{\predfn(s)} = \{y \in \outspace | \metric_\outspace(\predfn(s),y ) \leq E(s) + \obserrbnd \}.
\end{equation}
\end{lem}
\begin{proof}
Sample consistency up to $E$ implies that for each sample input $s \in G_n$ $\Metrico{\predfn}{\tilde f(s)} \leq E(s)$.
On the other hand, bounded observational errors mean that for $s$ 
there is $\nu_s \in \outspace, \Metrico 0 {\nu_s} \leq \obserrbnd$ such that we have $\Metrico{f(s)}{ \tilde f(s) } = \Metrico{f(s)}{ f(s)+\nu_s } \leq \Metrico{0}{\nu_s } \leq \obserrbnd$.
Since the output pseudo-metric $\Metrico \cdot \cdot$ adheres to the triangle inequality, we have 
$\Metrico{f(s)}{\predfn(s)} \leq \Metrico{f(s)}{ \tilde f(s) } + \Metrico{\predfn(s)}{ \tilde f(s) } \leq E(s) + \obserrbnd $.
\end{proof}

Next, we will establish that sample-consistent and continuous H\"older inference rules can learn any H\"older continuous target.

\begin{defn}
Relative to a pseudo-metric $\metric$, a sequence of sets $(\mathcal S_n)_{n \in \nat}$ converges to a set $\mathcal S$ iff  for all $s \in S$ there is a vanishing function $r_s:\nat \to \Real$, $r(n) \stackrel{n \to \infty} {\to} 0$ such that we have:  $$ \forall n \in \nat: \inf_{s_n \in \mathcal S_n} \metric(s_n,s)\leq r(n).$$
If the same function $r(\cdot)$ can be given for all $s \in \mathcal S$ then the convergence is uniform and we say that $(\mathcal S_n)$ converges to $\mathcal S$ uniformly with rate $r$. In the case of uniform convergence with rate of at most $r$ we have: $$\sup_{s \in \mathcal S} \inf_{s_n \in \mathcal S_n} \metric(s_n,s) \leq r(n) \stackrel{n \to \infty}{\to} 0$$.
\end{defn}


\begin{thm} \label{thm:convergenceifboundedconstandsamplecons}
Let $\seq{\data_n}{n \in \nat}$ be a data set sequence with pertaining grid sequence $\seq{G_n}{n \in \nat}$, \\ with $G_n \subset G_{n+1} \subset \inspace (n \in \nat)$ converging to a dense subset of input domain $\inspace$. Assume all of the following: 
\begin{enumerate} 
\item The data sets $\data_n$ ($n \in \nat$) all have bounded observational error with $\obserrbnd := \sup_x \Metrico{0}{\obserr(x)} \in \Real_{\geq 0}$.
\item The target $f: I \subseteq \inspace \to \outspace$ is H\"older continuous with $f \in \hoelset {L^*}{}  p$.
\item The predictors $\predfn$ $(n \in \nat)$ are sample-consistent up to error $E:\inspace \to \Real$ with $\bar E := \sup_x E(x) \leq \infty$.
\item The predictors are  H\"older continuous with H\"older constants bounded below $\bar L \in \Real_{\geq0}$. That is, 
$\exists \bar L \geq 0 \forall n \in \nat: \predfn \in \hoelset {L(n)}{} p$  $ \wedge$ $ L(n) \leq \bar L$.
\end{enumerate}


Then, we have: 
\begin{itemize}
\item \textbf{(I)}The sequence of predictors $\seq \predfn {n \in \nat}$ converges to the ground truth $f$ pointwise up to error $\bar E$. That is, $\forall \epsilon \geq 0,  x \in \inspace \exists n_0 \in \nat \forall n \geq n_0: \metric_{\outspace}\bigl(\predfn(x), f(x) \bigr) \leq \epsilon + \bar E + \obserrbnd$.
%\item 

\item \textbf{(II)} If the grid sequence $\seq{G_n}{n \in \nat }$ converges to the domain $\inspace$ uniformly then $\seq \predfn {n \in \nat}$ converges to the target $f$ uniformly up to error bound $\bar E + \obserrbnd$. 

That is, $\forall \epsilon \geq 0 \exists n_0 \in \nat \forall n \geq n_0, x \in \inspace: \metric_{\outspace}\bigl(\predfn(x), f(x) \bigr) \leq \epsilon + \bar E+\obserrbnd$.

\item \textbf{(III)} If the uniform convergence of the grid sequence as per (II) occurs with a rate of at most $r:\nat \to \Real$ then we have:  $$\forall n \in \nat \forall x \in I \subseteq\inspace: \metric_\outspace\bigl(\predfn(x) , f(x)\bigr)\in [0,(\bar L+ L^*) r(n)^p+ \bar E + \obserrbnd].$$ That is, as $n \to \infty$,  convergence (up to error $\bar E +\obserrbnd)$ of the predictors to the target occurs uniformly at a rate of at most $(\bar L+ L^*) r(n)^p$.
\end{itemize}

\begin{proof}
For $x \in I\subseteq \inspace$ let $\xi_n^x$ denote the nearest neighbour of $x$ in grid $G_n$. That is, $\xi_n^x = \arg\inf_{s \in G_n} \metric_\inspace(x,s)$.

\textbf{(I)} 
Since $G_n$ is assumed to converge to a dense subset of the domain $I$, we have$ \metric(x,\xi_n^x )^p
\stackrel{n \to \infty}{\longrightarrow} 0$.
Furthermore, since the the predictors are sample-consistent on grid $G_n$ up to error $E$ and since $\xi_n^x \in G_n$ we can appeal to Lem. \ref{lem:groups_sampleconsandobserr} (as well as to the triangle inequality) to reason as follows:
%there always is some $\phi(\xi_n^x) \in  \ball{\bar E}{0} = \{ y \in \outspace \,| \, \metric_\outspace(y,0 ) \leq \bar E \}$ such that (i) 
$\metric_\outspace\bigl(\predfn(x) , f(\xi_n^x)  \bigr) 
\leq 
\metric_\outspace\bigl(\predfn(x) , \predfn(\xi_n^x)  \bigr) + \metric_\outspace\bigl(\predfn(\xi_n^x) , f(\xi_n^x)  \bigr)
\stackrel{Lem. \ref{lem:groups_sampleconsandobserr}}{\leq} 
\metric_\outspace\bigl(\predfn(x) , \predfn(\xi_n^x)  \bigr) + E(\xi_n^x) + \obserr(\xi_n^x)$.
Hence,
\begin{equation}
\forall x: \metric_\outspace\bigl(\predfn(x) , f(\xi_n^x)  \bigr) \leq
\metric_\outspace\bigl(\predfn(x) , \predfn(\xi_n^x)  \bigr) + \bar E + \obserrbnd.
\label{ineq:ewlfjhflsape}
\end{equation}
%
Thus, for $x \in \inspace$:
%
$\metric_\outspace\bigl(\predfn(x) , f(x)\bigr) \leq \metric_\outspace\bigl(\predfn(x) ,  f(\xi_n^x)  \bigr) + \metric_\outspace\bigl(f(\xi_n^x) , f(x)\bigr)$
$\leq 
\metric_\outspace\bigl(\predfn(x) , \predfn(\xi_n^x)  \bigr) + \bar E + \obserrbnd+ \metric_\outspace\bigl(f(\xi_n^x) , f(x)\bigr)$
%$\norm{\predfn(x) - f(x)} \leq \norm{\predfn(x) - f(\xi_n)  } + \norm{f(\xi_n) - f(x)} \stackrel{(i)}{=}\norm{\predfn(x) - \predfn(\xi_n)  } + \norm{f(\xi_n) - f(x)}$
$\stackrel{(\dagger)}{\leq} (\bar L+ L^*) \metric_\inspace(x,\xi_n^x )^p + \bar E +\obserrbnd$
$\stackrel{n \to \infty}{\longrightarrow} \bar E +\obserrbnd$.
Here ($\dagger$) leverages the H\"older continuity assumptions. \\

\textbf{(II)} The proof is a trivial extension of (I). Let $\epsilon \geq 0.$ We show $\exists n_0 \in \nat \forall n \geq n_0, x \in \inspace: \metric_{\outspace}\bigl(\predfn(x), f(x) \bigr) \leq \epsilon + \bar E +\obserrbnd$.  
Since $G_n$ converges to $I$ uniformly, $\exists n_0 \forall n\geq n_0\forall x \in I: \metric_\inspace(x,\xi_n^x)^p \leq \frac{\epsilon}{2 \max\{\bar L, L^* \}}$. Hold such $n_0$ fixed. Then for all $n \geq n_0, x \in I$ we have 
$\metric_\outspace\bigl(\predfn(x) , f(x)\bigr) \newline \stackrel{(*)}{\leq} (\bar L+ L^*) \metric_\inspace(x,\xi_n^x )^p + \bar E +\obserrbnd \leq \epsilon + \bar E +\obserrbnd$. Here (*) follows from (\ref{ineq:ewlfjhflsape}) and ($\dagger$) in complete analogy to our derivations for case (I) above. 

\textbf{(III)} By assumption of uniform convergence of $\seq{G_n}{n \in \nat}$ with rate $r(n)$, $\sup_{x \in I} \inf_{s_n \in G_n} \metric_{\inspace} (x,s_n) =\sup_{x \in I} \metric_{\inspace} (x,\xi_n^x) \leq r(n)$. The rest follows from (*).
\end{proof}
\end{thm}


%% ============= ONLINE LEARNING CASE IMPORTANT 4 CONTROL


Above theorem considers the case where the data becomes dense in the domain and we are interested in the worst-case prediction error. Relevant to considerations in a setting where online-learning is utilised to control a trajectory $\seq{x_n}{n \in \nat}$ of states is the situation where we are only interested in the error on this specific trajectory in the limit of large time steps $n$. For trajectories on which the prediction error vanishes we will be able to establish convergence of the trajectory to the desired goal state.
In preparation of these considerations, we will establish the following facts:


\begin{lem}
Assume we are given a trajectory $\seq{x_n}{n \in \nat}$ of inputs with $x_n \in \inspace$ where input space $\inspace$ can be endowed with a shift-invariant measure. Furthermore, assume the sequence  is bounded, i.e.  
$\metric_\inspace(x_n,0) \leq \beta$ for some $\beta \in \Real_+$ and all $n \in \nat$.
Finally assume the inputs of the available data coincide with the complete history of past inputs, i.e. $G_n = \{ x_i | i \in \nat, i < n\}$.
Then we have: \[ \dist(G_n,x_n) = \min\{\metric_\inspace(g,x_n) | \, g \in G_n\} \stackrel{n \to \infty}{\longrightarrow} 0.\]
\begin{proof}
The intuition behind the following proof is that if the distances were not to converge, there was an infinite number of disjoint balls around the input points that summed up to infinite volume. This however, would be a contradiction to the presupposed boundedness of the sequence.
We formalise this intuition as follows:
We can rephrase the desired convergence statement as 
\begin{equation}
\forall \epsilon > 0 \exists n \in \nat \forall m \geq n : \dist(x_{m}, G_{m}) \leq \epsilon.
\end{equation} 
For contradiction, assume that
\begin{equation}
\exists \epsilon > 0 \forall n \in \nat \exists m(n) \geq n : \dist(x_{m(n)}, G_{m(n)}) > \epsilon.
\end{equation} 
Hold such an $\epsilon >0$ fixed and choose any $n \in \nat$. 
By definition of $G_{m(n)} =\{ x_i | i < m(n)\} $ we have:
\eqn{eq:i34kjjk3}{\forall i < m(n) : \metric_\inspace(x_{m(n)},x_i) > \epsilon.}

Let $C_n := \bigcup_{i < n} \ball{\frac \epsilon 2}{x_i} $ be the union of all $\frac \epsilon 2$-balls around each point in $G_n$ and define $\bar I = \bigcup_{n \in \nat} C_n$.
By definition, each $x_n$ is contained in $\bar I$.
Since sequence $(x_n)_{n \in \nat}$ is bounded, $\bar I $ has a finite volume relative to some positive, shift-invariant measure $\mu$. I.e. $\mu(\bar I) < \infty$ (e.g. choose the Lebesgue measure for $\mu$). Furthermore, $\mu(C_n) \leq \sum_{i <n} \mu(B_i) \leq \mu(\bar I)< \infty$ where $B_i := \ball{\frac \epsilon 2}{x_i}$. Owing to the assumed shift-invariance, we can assign the same measure $M$ each ball, i.e. $M:=\mu(B_1) = \mu(B_n)\forall n \in \nat$. Thus, $\mu(C_n) \leq n M$.
Define $q:= \ceil{\frac{\mu(\bar I)}{M}} \in \nat$. This is an upper bound on the number of disjoint balls of measure $M$  that can be contained in $\bar I$. Intuitively, since this number is finite, there cannot be an infinite number of non-intersecting balls around the elements of the sequence $(x_n)_{n \in \nat}$. More formally our argument proceeds as follows:
Choose $n > q+1$. Statement (\ref{eq:i34kjjk3}) yields:
\eqn{eq:i34kjjk33}{\forall i \in \{1,...,n\} \exists p(i) \geq i \forall j \leq p(i): \metric_\inspace(x_{p(i)},x_j) > \epsilon.}  Define a permutation $\pi$ such that $\pi(p(1)) \leq \ldots \leq \pi(p(n))$. 
With Statement (\ref{eq:i34kjjk33}) it follows that \\
${\metric_\inspace(x_{\pi(p(i))},x_{\pi(p(j))}) > \epsilon}$ , $\forall i,j =1,...,n, i < j$. Thus, we conclude the disjointness conditions $B_{\pi(p(i))} \cap B_{\pi(p(j))} = \emptyset , \forall i,j =1,...,n, i \neq j$. 
Hence,  $\mu(\bar I) \geq \mu(C_{\pi(p(n))}) \geq \mu(C_{\pi(p(1))}) + \sum_{i=1}^n \mu(B_{\pi(p(i))}) \newline = \mu(C_{\pi(p(1))}) +  n M > \mu(C_{\pi(p(1))}) + (q+1) M \geq\mu(C_{\pi(p(1))}) + \mu(\bar I)$, where the last inequality follows from the fact that  $M q= M \ceil{\frac{\mu(\bar I)}{M}} \geq \mu(\bar I)$. Since $\mu(C_{\pi(p(1))}) \geq 0$, we have concluded the false statement $\mu(\bar I) > \mu(\bar I)$.

%Hence, $\mu(\bar I) \geq \mu(C_{\pi(N_n)+1}) = \mu(C_{\pi(N_1)}) + \sum_{i=1}^n \mu(B_{\pi(N_i)}) = \mu(C_{\pi(N_1)}) +  n M > \mu(C_{\pi(N_1)}) + (m+1) M >\mu(C_{\pi(N_1)}) + \mu(\bar I)$ by definition of $m= \ceil{\frac{\mu(\bar I)}{M}}$. But since $\mu(C_{\pi(N_1)}) \geq 0$, we conclude the false statement $\mu(\bar I) > \mu(\bar I)$.
\end{proof}
\label{lem:bndseq_entails_distgridvanish}
\end{lem} 


\begin{lem}
As before, let the observational noise level be bounded by $\obserr$ and assume the output- space pseudo-metric to be translation-invariant. Assume the predictors $\predfn(\cdot)$ all are H\"older with H\"older constants bounded from above by some number $\bar L \in \Real_+$. 
Furthermore, we assume sample-consistency up to error $E$ (cf. Def. \ref{def:sampleconsistency}) where we assume bounded prediction errors $E_s = E + \obserrbnd$ (cf. Lem. \ref{lem:groups_sampleconsandobserr}) at sample inputs with $\sup_x E_s(x) \leq \bar E_s \in \Real$.
For some $L^* \in \Real_+, p \in [0,1]$, let target $f$ be $L^*-p$-H\"older up to error $E_h$. That is, there is an $L^*-p$-H\"older function $\phi \in \hoelset {L^*} { } p$ and a function $\psi$ such that $\forall x: f(x) = \phi(x)+\psi(x), \, \sup_x \metric_\outspace\bigl(0,\psi(x)\bigr) \leq \bar E_h \in \Real$.

Assume we are given a trajectory $\seq{x_n}{n \in \nat}$ of inputs that is bounded, i.e. where 
$\metric_\inspace(x_n,0) \leq \beta$ for some $\beta \in \Real_+$ and all $n \in \nat$.
Furthermore, assume $\data_{n+1} = \data_n \cup \{ \bigl(x_n, \tilde f(x_n)\bigr) \}$ and thus, $G_n = \{ x_i | i \in \nat, i < n\}$.
Then the prediction error on the sequence vanishes up to the level of sample-consistency and H\"older continuity in the following sense:
 \[\metric_\outspace\bigl(\predfn(x_n),f(x_n) \bigr) \stackrel{n \to \infty}{\longrightarrow} [0,\bar  E_s + 2 \bar E_h].\]
In particular, in case the observations are error-free ($\tilde f = f$) and assuming the target is H\"older continuous then the prediction error vanishes. That is,
\[\metric_\outspace\bigl(\predfn(x_n),f(x_n) \bigr) \stackrel{n \to \infty}{\longrightarrow}0.\]
\begin{proof}

Let $\xi_n := \argmin_{g \in G_n} \metric_\inspace(x_n,g)$ denote the nearest neighbour of $x_n$ in $G_n = \{x_1,...,x_{n-1}\}$.
We have assumed that $\predfn$ is $L(n)-p$- H\"older with $L(n) \leq \bar L$ for some $\bar L \in \Real_{\geq 0}$.
Since sequence $(x_n)$ is bounded, Lem. \ref{lem:bndseq_entails_distgridvanish} is applicable. 
Hence, $\forall n \in \nat: \metric_\inspace (x_n,\xi_n) \to 0$ as $n \to \infty$.

Let $y \in \outspace$ such that $\predfn(\xi_n) + y = f(\xi_n)$. The sample-consistency assumption renders Lem. \ref{lem:groups_sampleconsandobserr} applicable, allowing us to conclude that 
$\metric_\outspace\bigl(\predfn(\xi_n) ,  f(\xi_n)  \bigr) \leq \obserrbnd + E(\xi_n)$. Hence, by Lem. \ref{lem:bilinaddtransinvgroup}.(III), $\metric_\outspace(0,y) \leq \obserrbnd + E(\xi_n)$. Appealing to Lem. \ref{lem:bilinaddtransinvgroup}.(I), we see that 
$\metric_\outspace\bigl(\predfn(x_n) ,  f(\xi_n)  \bigr) \leq \metric_\outspace\bigl(\predfn(x_n) ,  \predfn(\xi_n)+y  \bigr) \leq 
\metric_\outspace\bigl(\predfn(x_n) ,  \predfn(\xi_n) \bigr) + \metric_\outspace(0,y)$. Thus, 

$(i) \,\,\,\, \metric_\outspace\bigl(\predfn(x_n) ,  f(\xi_n)  \bigr) \leq \metric_\outspace\bigl(\predfn(x_n) ,  \predfn(\xi_n) \bigr) + \obserrbnd + E(\xi_n)$.



Moreover: $(ii)$ There is a maximal constant $\bar L \in \Real$, such that for any $n \in \nat $, the predictor $\predfn$ is $L(n)-p$-H\"older with $L(n) \leq \bar L$. 
  
In conclusion,
$0\leq\metric_\outspace\bigl(\predfn(x_n) , f(x_n)\bigr) \leq \metric_\outspace\bigl(\predfn(x_n) ,  f(\xi_n)  \bigr) + \metric_\outspace\bigl(f(\xi_n) , f(x_n)\bigr) \stackrel{(i)}{\leq}\metric_\outspace\bigl(\predfn(x_n) , \predfn(\xi_n) \bigr) + \obserrbnd + E(\xi_n) + \metric_\outspace\bigl(f(\xi_n) , f(x_n)\bigr) \leq \metric_\outspace\bigl(\predfn(x_n) , \predfn(\xi_n) \bigr) +\obserrbnd + E(\xi_n) + \metric_\outspace\bigl(\phi(\xi_n) , \phi(x_n)\bigr) + 2 \bar E_h 
\newline
\stackrel{(ii)}{\leq} (\bar L+ L^* ) \metric_\inspace(x_n,\xi_n )^p + \obserrbnd + E(\xi_n) + 2 \bar E_h \stackrel{(ii)}{\leq} (\bar L+ L^* ) \metric_\inspace(x_n,\xi_n )^p + \bar E_s + 2 \bar E_h \stackrel{n \to \infty}{\longrightarrow} \bar E_s + 2 \bar E_h $.
\end{proof}
\label{lem:vanisishingseqprederr_groups}
\end{lem} 

