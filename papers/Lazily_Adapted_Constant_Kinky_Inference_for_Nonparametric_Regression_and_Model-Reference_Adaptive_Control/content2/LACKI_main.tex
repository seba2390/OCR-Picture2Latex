

\subsection{Lazily Adapted Constant Kinky Inference (LACKI)}
\label{sec:lacki}
Above we explained the benefits of choosing parameter $L(n)$ to coincide with a H\"older constant of the target. As the uncertainty bounds grow with this constant, choosing a small H\"older constant would be ideal. 
However, if such a constant is unavailable a priori, we desire to set $L(n)$ to an estimate of the H\"older constant. 

For notiational convenience, for two sets $S,S' \subset \inspace$ of inputs we define  $$U(S,S') := \{(s,s') \in S \times S' | \metric_\inspace(s,s') >0\} \text{ and  let } U_n := U(G_n,G_n) $$ be the set of all pairs grid inputs deemed disparate under the pseudo-metric $\metric_\inspace$.

The \emph{best} H\"older constant of a function $f$ is the smallest nonnegative number $L^*$ such that $f$ is contained in the set 
$\hoelset L \inspace p = \{\phi: \inspace \to \outspace \, \vert \, \forall x,x' \in \inspace : \metric_\outspace \bigl(\phi(x),\phi(x')\bigr) \leq L \, \bigl( \metric_\inspace (x,x') \bigr)^p\} $ of $L-p$- H\"older continuous functions. So, this best H\"older constant is given by  
%
$$L^* = \sup_{(x,x') \in U(\inspace,\inspace)} \frac{\metric_\outspace \bigl(f(x) - f(x')\bigr)}{\metric_{\inspace}^p(x,x') }.$$




Given the noisy data $\data_n = \{(s_i,\tilde f_i) | i=1,\ldots,N_n\}$ a natural estimate of the best H\"older constant might be to compute $\hat \ell_n^* := \max_{(s,s') \in U_n} \frac{\metric_\outspace(\tilde f(s),\tilde f(s')) }{\metric_{\inspace}^p(s,s')}$ \cite{Strongin1973}. In the absence of \emph{noise} (may it be stochastic or deterministic), that is, if $\tilde f_i = f(s_i),\forall i$, $\hat \ell_n^*$ never overestimates the true best H\"older constant. That is, $\hat \ell_n^* \leq L^*$. However, in the presence of noise $\nu: \inspace \to \outspace$ (such that $\tilde f = f+ \nu$) this boundedness assumption of the estimates no longer holds true. In particular if the noise is not H\"older continuous, we expect $\hat \ell_n^*$ to grow unbounded with increasingly dense data.
For practical reasons and for the sake of our theoretical arguments presented below, we desire the parameters $L(n)$ to remain bounded both. Thus, without further modifications $\hat \ell_n^*$ is not a suitable candidate for $L(n)$.

Therefore, we will modify the estimates to guarantee they are bounded even in the presence of noise. 
To compensate for the effect of the noise, we instead propose the parameterised estimates 
%
\begin{equation}\label{eq:lazyconstupdaterule_batch_main}
\ell(\data_n;\hestthresh,\underline L)  := 
 \max \Bigl\{ \underline L, \max_{(s,s') \in U_n} \frac{\metric_\outspace(\tilde f(s),\tilde f(s')) - \hestthresh}{\metric_{\inspace}^p(s,s')} \Bigr\}.
\end{equation}
%
Here $\underline L$ is a parameter that can be used to specify a priori knowledge of a lower bound on the best Lipschitz constant. In the absence of particular domain-specific knowledge, one can of course always set $\underline L =0$.
\begin{rem}
As shown in Sec. \ref{sec:lazyconstants}, by setting parameter $\hestthresh$ at least as large as twice the maximum level of observational noise ($\hestthresh \geq 2 \metric_{\outspace}(0,\obserr)$) we can guarantee that the $\ell(\data_n;\hestthresh,0)$ are bounded from above by the best H\"older constant. 
\end{rem}
Next, consider an online learning situation, where the available data increases over time. 
That is, $\data_n \subseteq \data_{n+1}$ for all time steps $n \in \nat$. 
For time step $n \in \nat$, let $S_{n+1} := G_{n+1} \backslash G_n$ be the set of new sample inputs.
Similarly to above, we can define an incremental update rule recursively as follows: 
\begin{align} \label{eq:Hoelconstlazyupdateincr_main}
\ell_{n+1} := \max\Bigl\{ & \ell_n, \max_{(s,s') \in U(G_n, S_{n+1})} \frac{\metric_\outspace\bigl(\tilde f(s),\tilde f(s')\bigr) - \hestthresh}{\metric_{\inspace}^p(s,s')},\\
&\max_{(s,s') \in U(S_{n+1}, S_{n+1})} \frac{\metric_\outspace\bigl(\tilde f(s),\tilde f(s')\bigr) - \hestthresh}{\metric_{\inspace}^p(s,s')} \Bigr\},
\end{align} for $n \in \nat$ 
and where 
$\ell_0 := \underline L$. 
The effort for computing $L(n+1)$ is in the order of $\mathcal O\bigl(M (\abs{S_{n+1}} N_n+ \abs{S_{n+1}}^2)\bigr)$ where $M$ denotes the effort for evaluating the pseudo-metrics.
By construction, we have \[\ell_n =\ell(\mathcal \data_n;\hestthresh,\underline L), \forall n \in \nat.\] 
We can show that the sequence $\seq{\ell_n}{n\in \nat}$ of estimates is not only bounded from above by the true H\"older constant $L^*$ but is also growing monotonically and convergent (cf. Lem. \ref{lem:constadaptation_boundedness}).

So far, we have defined a rule of how to update noise-robust and convergent estimates $\ell_n$ of the H\"older constant. Using these data-dependent estimates in place of $L(n)$ in our kinky inference framework as per Def. \ref{def:KIL} yields an inference rule that shall henceforth be referred to as \textit{Lazily Adapted Constant Kinky Inference (LACKI)}.

\begin{defn}[LACKI rule] \label{def:LACKI} For each output component $j \in \{1,\ldots,\text{dim} \, \outspace \}$ 
define $\predfn(\cdot)_{j}$  as per Def. \ref{def:KIL} but assume we choose the parameters $L_j(n) = \ell(\data_n;\hestthresh,\underline L)$ (according to Eq. \ref{eq:lazyconstupdaterule_batch_main}). We refer to the resulting predictor $\predfn\bigl(\cdot \bigr)$ as a \emph{Lazily Adapted Constant Kinky Inference (LACKI)} rule. Here free parameters are $p \in (0,1], \hestthresh \in \Real_{\geq 0}$ and $\underline L \in \Real_{\geq 0}$.  
\end{defn}

To develop a first feel for our inference rule, refer to Fig. \ref{fig:LACKInoise}. Here, we depicted the predictors for an underlying ground-truth function on the basis of a sample but with different parameter choices $\hestthresh$. When setting this parameter to two times the observational noise level, the predictor accurately smoothes out the noise. In contrast, when the parameter is set to zero, the resulting predictor will perfectly interpolate through the noisy observations, thereby limiting the approximation quality to the level of observational noise. 

Furthermore, we notice that the predictors are H\"older continuous but non-differentiable. Informally speaking, the inference exhibits ``kinks'', motivating the term ``kinky inference''.

%Below, we will provide some essential theoretical underpinnings for our method's properties.
\begin{figure}
        \centering
    \includegraphics[width = 8.8cm,height = 4cm]
								{content/figs/LACKIhestthreshvar}
   \caption{Two LACKI inferences over function values of the target $f: x \mapsto \abs{\cos(2\pi x)}+x$ (dashed line) on the basis of a noisy sample (plotted as dots). The predictors are plotted in grey, the noisy observational function $\tilde f (\cdot) = f(x) + \nu_x$ is plotted in cyan. Here the $\nu_x$ were drawn i.i.d. at random from a uniform distribution on the interval $[-0.5,0.5]$. In both cases we chose the parameters $\ubf \equiv \infty,\lbf \equiv -\infty,\underline L = 0$ and $\hexp=1$.
   The left plot shows the LACKI predictor $\predfn$ (grey line) for parameter choice $\hestthresh =0$, falsely assuming absence of observational noise. As a result, the prediction overfitted to the noise. The right plot depicts the prediction $\predfn$ (grey curve) for correct parameter choice $\hestthresh = 2 \obserrbnd =1$, causing the noise to be smoothed out and resulting in more accurate prediction of the underlying ground truth $f$.  }
			\label{fig:LACKInoise}
\end{figure}

\begin{figure}
        \centering
    \includegraphics[width = 8.8cm,height = 4cm]
								{content/figs/LACKIhestthreshvar_with_bounds}
   \caption{Repetition of the experiment but with $p=0.5$. This time, we also plotted floor and ceiling functions (grey dotted and dashed-dotted curves) delimiting the uncertainty bounds. Note, when setting $\lambda =0$ the estimate $\ell(\mathcal \data_n;\hestthresh,\underline L)$ was found to be 128 resulting in extremely conservative uncertainty estimates (left figure). In contrast, choosing $\lambda =2 \obserrbnd $ gave a parameter estimate $\ell(\mathcal \data_n;\hestthresh,\underline L) = 2.4$ yielding sensible uncertainty bounds (right figure). }
			\label{fig:LACKInoise2}
\end{figure}
%
%
%\begin{figure}
%        \centering
%				  \subfigure[ ]{
%    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    \includegraphics[width = 4.8cm]
%								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								{content/figs/LACKInoise_e_alpha_trueLinit_7tex.pdf}
%    \label{fig:LACKInoise1}
%  } 	
%	 \subfigure[ ]{
%    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    \includegraphics[width = 4.8cm]
%								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								{content/figs/LACKInoise_noe_noalpha_smallLinit_7tex.pdf}
%    \label{fig:LACKInoise2}
%  } 	
%  %
%  %\hspace{2cm}
%	  \subfigure[ ]{
%    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    \includegraphics[width = 4.8cm]
%								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								{content/figs/LACKInoise_e_alpha_smallLinit_7tex.pdf}
%    \label{fig:LACKInoise3}
%  } 
%  				  \subfigure[ ]{
%    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    \includegraphics[width = 4.8cm]
%								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								{content/figs/LACKInoise_e_alpha_trueLinit_77tex.pdf}
%    \label{fig:LACKInoise4}
%  } 	
%	 \subfigure[ ]{
%    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    \includegraphics[width = 4.8cm]
%								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								{content/figs/LACKInoise_noe_noalpha_smallLinit_77tex.pdf}
%    \label{fig:LACKInoise5}
%  } 	
%  %
%  %\hspace{2cm}
%	  \subfigure[ ]{
%    %\includegraphics[width = 3.7cm, height = 3cm]{content/figures/graph1_klein.eps}
%    \includegraphics[width = 4.8cm]
%								%{content/Ch_kinkyinf/figs/resultswingrock_555trials}
%								{content/figs/LACKInoise_e_alpha_smallLinit_77tex.pdf}
%    \label{fig:LACKInoise6}
%  } 	
%   \caption{LACKI inference on the basis of two noisy data sets 
%   $\data_1,\data_2$ with varying parameter settings. Target (blue curve) was $f:x \mapsto 5 \sqrt{\abs x}$ and observational model (magenta curve) $\tilde f: x \mapsto f(x) + \phi_x$ where $\phi_x$ is a uniformly distributed disturbance with $\phi_x \stackrel{i.i.d.}{\sim} \mathcal U(-0.5,0.5),\forall x$. 
%  %
%   \textbf{1st column:} Initial constant L(0) = 5; assumed obs. error bound $\obserrpar= 0.5$; $\hestthresh = 2 \obserr=1$. 
%   %
%   \textbf{2nd column:} Initial constant L(0) = 0.01; assumed obs. error bound $\obserrpar = 0$; $\hestthresh = 2 \obserr =0$.
%   %
%   \textbf{3rd column:} Initial constant L(0) = 0.01; assumed obs. error bound $\obserrpar = 0.5$; $\hestthresh = 2 \obserr =1$.
%   %
%   \textbf{Top row:} Data set $\data_1$ of size 7. 
%   \textbf{Bottom row:} Data set $\data_2$ of size 77.
%   In all cases the exponent parameter was set to $p = 0.5$.}
%			\label{fig:LACKInoise}
%\end{figure}
%
%The figure depicts the inference $\predfn$ (black curve)	 over function values of the target $f$ (blue curve) based on noisy data sets $\data_1$ and $\data_2$. The true observational noise was bounded by level $\obserrbnd = 0.5$ yielding a data-generating function $\tilde f(x) = f(x) +\nu(x)$ (magenta curve) where $\nu(x) \in [-0.5,0.5],\forall x$.
%The first row shows the predictions of our LACKI rule applied to a small sample $\data_1$ with varying parameter settings.
%The second row depicts the pertaining predictions resulting applying the corresponding LACKI rules to an enlarged data set $\data_2 \supset \data_1$.
%
%In plots in the first column and third columns depict situations where LACKI was applied with parameter $\hestthresh=2 \obserrbnd$.  By contrast, the second column depicts a situation where LACKI was parameterised with a parameter $\hestthresh = 0$ and a small initial guess of the H\"older constant of $\underline L = L(0) = 0.01$. As a result, in this case, the LACKI predictor perfectly follows the noisy data. Furthermore, we observe a large uncertainty bound $\prederrn$ with increasing sample size. In the third column the LACKI rule was also initialised with $\underline L =0.01$. However, the larger $\hestthresh = 2 \obserrbnd$ prevents 
%the H\"older constant estimate $L(2)$ to exceed the true constant $L^*=5$. As a result, the prediction ``smoothes out the noise''. The degree of smoothing and approximation error in the limit of infinite data will be theoretically investigated below. 
%At this point we will already mention that all behaviours depicted in the plots seem consistent with our theoretical guarantees we will give in what is to follow.


\subsubsection{Properties}
We will now establish several properties of the LACKI rules including sample-consistency and H\"older continuity. Most importantly however, we will show that the LACKI rules are universal approximators in the sense that they can be set to learn any continuous function with arbitrary worst-case error.

The core idea behind this can be sketched as follows: 
First, we establish H\"older continuity and sample-consistency (cf. Def. \ref{def:sampleconsistency}). This allows us to appeal to Thm. \ref{thm:convergenceifboundedconstandsamplecons} and prove that LACKI can learn any H\"older function.
Note, some universal approximators, such as RBFNs with Gaussian kernels, are provably Lipschitz. Therefore, learning any continuous function can be interpreted as learning some Gaussian RBFN with an observational error level that absorbs the discrepancy between the RBFN and the ground truth. Since a finite RBFN with smooth, bounded-derivative kernel is provably H\"older and since we can learn any H\"older function with LACKI up to the level of observational error, we can learn the continuous ground-truth up to the approximation error of the RBFN. 

Following this outline, we will now proceed to establish the desired properties formally.
 
\begin{lem}[Sample-consistency of LACKI] \label{lem:LACKIsampleconsistency} If for each output dimension $j \in \{1,...,d\}$ and some $\hestthresh \geq 0$ we have $L_j(n)  \geq  \max_{(s,s') \in U_n} \frac{\abs{\tilde f_j(s)- \tilde f_j(s')} -\hestthresh }{\metric_\inspace^p(s,s')}
$ then the LACKI rule is sample-consistent (up to $\frac \hestthresh 2$). 
That is,  \[\forall q \in \{1,\ldots,N_n \}:  \predfn(s_q) \in \ball{\frac \hestthresh 2}{\tilde f_q} \] where $\ball{\frac \hestthresh 2}{\tilde f_q} =\{ x \in \outspace |  \norm{x-\tilde f_q}_\infty \leq \frac \hestthresh 2\}$ denotes the $\frac \hestthresh 2$-ball around the observation $\tilde f_q$.\\
Thus, we also have $\norm{ f(s_q) - \predfn(s_q)}_\infty \leq \frac \hestthresh 2 + \norm{\obserr(s_q)}_\infty$.
\begin{proof}
For ease of notation, we will confine our proof to the case of one-dimensional outputs ($d=1$). The multi-dimensional case follows trivially from the one-dimensional result by applying it to each output component function. 
Let $n \in \nat  $ be fixed and, for ease of notation, write $L := L(n)$. Let $j,k \in   \{1,\ldots,N_n \} $ such that $j \in \argmin_i \tilde f_i + L \metric_\inspace^p(s_i,s_q) $ and 
$k \in \argmax_i \tilde f_i - L \metric_\inspace^p(s_i,s_q) $. 
By definition of $\predfn$ we have:
\begin{align}
\predfn(s_q) 
%&= \frac 1 2 \bigl( \tilde f_j + L \metric_\inspace^p(s_j,s_q)  +\obserrpar(s_q) \bigr) + \frac 1 2 \bigl( \tilde  f_k - L \metric_\inspace^p(s_k,s_q) - \obserrpar(s_q) \bigr)\\
&= \frac 1 2 \bigl(\underbrace{ \tilde f_j + L \metric_\inspace^p(s_j,s_q)}_{:=B}   \bigr) + \frac 1 2 \bigl( \underbrace{\tilde  f_k - L \metric_\inspace^p(s_k,s_q) }_{=:A}\bigr). \label{eq:hru582jsokbbbn}
\end{align}
%
%
(i) Firstly, we show  \underline{$ A \in [\tilde f_q, \tilde f_q +\hestthresh]$}:
If $k = q$, this holds trivially true since then $A= \tilde f_q$. 
So, assume $k \neq q$. 
We have $\tilde f_k \geq \tilde f_k - L \metric_\inspace^p(s_k,s_q)  \geq \tilde f_q - L \metric_\inspace^p(s_q,s_q)  = \tilde f_q$ where the second inequality holds due to $k \in \argmax_i \tilde f_i - L \metric_\inspace^p(s_i,s_q) $. That is,
%\begin{equation} 
%{\tilde f_k - \tilde f_q}  \geq L \metric(s_k,s_q). \label{eq:9p4t37ru8ewihk3hjtu}
%\end{equation} 
\begin{equation} 
A=\tilde f_k - L \metric_\inspace^p(s_k,s_q) \geq \tilde f_q. \label{eq:9p4t37ru8ewihk3hjtu}
\end{equation} 
On the other hand, since $L  \geq  \max_{(s,s') \in U_n} \frac{\abs{\tilde f(s)-\tilde f(s')} - \hestthresh}{\metric_\inspace^p(s,s')}$  we have in particular: $L  \geq  \frac{ \abs{\tilde f_k- \tilde f_q}-\hestthresh }{\metric_\inspace^p(s_k,s_q)}$. Thus,  
 $L \metric_\inspace^p(s_k,s_q) + \hestthresh \geq \abs{\tilde f_k-\tilde f_q} = \tilde f_k - \tilde f_q$. Hence, $\tilde f_q + \hestthresh \geq \tilde f_k - L \metric_\inspace^p(s_k,s_q) = A$. 
 Together with (\ref{eq:9p4t37ru8ewihk3hjtu}) we have shown $A \in [\tilde f_q , \tilde f_q +\hestthresh].$
%  In conjunction with (\ref{eq:9p4t37ru8ewihk3hjtu}) this means that $L \metric(s_k,s_q)  = \abs{\tilde f_k-\tilde f_q} $. Substituting this into the left hand side of Eq. \ref{eq:irndhfkkdnsww} yields the expression 
% $ \tilde f_k - \abs{\tilde  f_k -\tilde  f_q } \stackrel{!}{=}   \tilde f_q $. But since $f_k \geq f_q$, we also have $f_k - \abs{ f_k - f_q }  = f_q$. Therefore, Eq. \ref{eq:irndhfkkdnsww} is equivalent to $f_q = f_q$ which obviously holds. 
% 
%Analogously to the proof of  $f_k - L \metric(s_k,s_q) = f_q$ we can also show $f_j + L \metric(s_j,s_q) = f_q$. Leveraging both equations in Eq. \ref{eq:hru582jsokbbbn} concludes the proof. 

(ii) The proof of \underline{$B \in [ \tilde f_q - \hestthresh, \tilde f_q]$} is completely analogous to that of (i) and hence, is omitted.
%and secondly, that 
%\begin{equation}B \in [ \tilde f_q - \hestthresh, \tilde f_q]. \label{eq:irndhfkkdnswwB}
%\end{equation}

(iii) Together, the statements in (i) and (ii)  entail  $\predfn(s_q) = \frac 1 2 A + \frac 1 2 B  \in [\tilde f_q - \frac \hestthresh 2, \tilde f_q + \frac \hestthresh 2]$.\\
The final statement of the Lemma is a direct consequence of Lem. \ref{lem:groups_sampleconsandobserr}. 
\end{proof}
\end{lem}

Next, we move on to establish that the predictors of the LACKI inference rule are H\"older continuous. More specifically we have:

\begin{lem}[H\"older regularity of LACKI]
With definitions as before, let $(\outspace,\metric_\outspace) = (\Real^m,(x,y) \mapsto \norm{x-y}_\infty)$.
Provided that the bounding functions $\lbf,\ubf$ are H\"older continuous (or set to $-\infty,\infty$, respectively),
the predictors $\predfn$ are H\"older continuous $(n \in \nat)$ with constant $L(n)$ and exponent $p$. That is, $\forall n \in \nat: \predfn \in \Hoelset \bigl(L(n),p\bigr)$.

\begin{proof}
By combining Thm. \ref{thm:d2pmapishoelder} with Lem. \ref{lem:Hoeldarithmetic} it follows that each output component function $\predf_{n,j}$ $(j=1,...,m)$ is $L(n)$-$p$- H\"older. This implies the desired statement. 

\end{proof} 
\label{lem:LACKIpredHoelder}
\end{lem}

In the remainder of this section, we will establish convergence guarantees of our LACKI inference rules. In particular, we shall study the behaviour of the sequence of predictors $\seq{\predfn}{n \in \nat}$ computed on the basis of a sequence of data sets $\seq{\data_n}{n \in \nat}$ that becomes dense on the domain in the limit of increasing sample size. To clarify the latter statement, consider the sequence of grids $\seq{G_n}{n \in \nat}$. We say this sequence \emph{becomes dense in $I \subset \inspace$ in the limit of large $n$ }if it \emph{convergences} to $I$ as $n \to \infty$. That is, if $\forall \epsilon >0,x \in I \exists n_0 \forall n \geq n_0 \exists g \in G_n: \metric_\inspace(x,g) < \epsilon$. If the rate of convergence is independent of $x$ then we say that the grid sequence converges to $I$ \emph{uniformly}. This is the case iff $\forall \epsilon >0 \exists n_0 \forall n \geq n_0, x \in I \exists g \in G_n: \metric_\inspace(x,g) < \epsilon$.

\begin{defn}
Relative to a pseudo-metric $\metric$, a sequence of sets $(\mathcal S_n)_{n \in \nat}$ converges to a set $\mathcal S$ with \emph{rate} $r: \nat \to \Real_+$ iff for all $n \in \nat$ we have $\inf_{s_n \in \mathcal S_n}\inf_{s\in \mathcal S} \metric(s_n,s)\leq r(n)$ and $r(n) \to 0$.
\end{defn}


\begin{thm}[LACKI can learn any H\"older function] \label{thm:convergenceifboundedconstandsamplecons_LACKI}
Let $\seq{\data_n}{n \in \nat}$ be a data set sequence with pertaining grid sequence $\seq{G_n}{n \in \nat}, G_n \subset G_{n+1} \subset \inspace (n \in \nat)$ converging to a dense subset of input domain $\inspace$. 
We assume the observational errors given by $\obserr$ are bounded from above by $\obserrbnd= \sup_x \obserr(x)$.
Furthermore, let the target $f: \inspace \to \outspace$ be H\"older with $f \in \hoelset {L^*}{}  p$ and assume $\lambda$ is set to a value of at least $2 \bar e$.
%
%and let the predictors be sample-consistent up to error $\bar E$ and H\"older continuous, 
%$\predfn \in \hoelset {L(n)}{} p$ with $L(n) \leq \bar L \in \Real_+\, (n \in \nat)$ for some $\bar L \geq 0$.

Then, we have 

%\begin{itemize} \item 
(I) The sequence of predictors $\seq \predfn {n \in \nat}$ converges to the ground truth $f$ pointwise up to error $\frac \hestthresh 2 +\obserrbnd$. That is, $\forall \epsilon \geq 0,  x \in \inspace \exists n_0 \in \nat \forall n \geq n_0: \metric_{\outspace}\bigl(\predfn(x), f(x) \bigr) \leq \epsilon + \frac \hestthresh 2 + \obserrbnd$.

%\item 

(II) If the grid sequence $\seq{G_n}{n \in \nat }$ converges to the domain $\inspace$ uniformly then $\seq \predfn {n \in \nat}$ converges to the target $f$ uniformly up to error $ \frac \hestthresh 2 +\obserrbnd$. 

That is, $\forall \epsilon \geq 0 \exists n_0 \in \nat \forall n \geq n_0, x \in \inspace: \metric_{\outspace}\bigl(\predfn(x), f(x) \bigr) \leq \epsilon +   \frac \hestthresh 2  +\obserrbnd$.
%\end{itemize}
Convergence occurs with a rate of at most $n \mapsto 2 \max\{\bar L_\lambda, L^* \} r(n)^p$ where $r$ is the rate of convergence of the input data set sequence $(G_n)$ and $\bar L_\lambda =\max_{x,x'} \frac{\metric_\outspace(f(x), f(x')) + 2 \obserrbnd -\hestthresh }{\metric_\inspace^p(x,x')}$ is an upper bound on the maximally attainable empirical estimate of the H\"older constant.

\begin{proof}
We have established that the predictors $\predfn(\cdot)$ of the LACKI rule are $L(n)-p$- H\"older (Lem. \ref{lem:LACKIpredHoelder}) and sample-consistent up to level $\frac \hestthresh 2$ (Lem. \ref{lem:LACKIsampleconsistency}). Furthermore, we can show that the sequence $\seq{L(n)}{n \in \nat}$ is convergent and bounded (Lem. \ref{lem:constadaptation_boundedness}). Furthermore our input and output spaces are additive abelian groups. By assumption, pseudo-metrics $\metric_\inspace,\metric_\outspace$ are translation invariant with respect to addition. Therefore, the predictors meet all assumptions of Thm. \ref{thm:convergenceifboundedconstandsamplecons} which entails the desired convergence statements.
\end{proof}
\end{thm}



Having established that our LACKI rule can learn any H\"older function with any H\"older constant, we will now attend to extend the results to non-H\"older functions. In preparation of the necessary derivations we will first rehearse universality and H\"older properties of radial basis function networks. 

Park and Sandberg derived universal approximation guarantees for radial-basis function networks \cite{Park1991}. In particular, on page 252 in their article the authors make an assertion that translates to our notation as follows: 

\begin{lem}[Expressiveness of RBFNs] \label{lem:RBFNunifapproxcompact} Assume $\inspace \subseteq \Real^d$ is compact. Given parameter  vector $\theta := (w_1,\ldots,w_m,\sigma_1,...,\sigma_m,c_1,\ldots, c_m)$ and kernel function $K: \inspace \to \outspace $ let $\beta(\cdot;\theta ) = \sum_{i=1}^m w_i \, K(\frac{\cdot - c_i}{\sigma_i} )  $ denote a radial basis function network (RBFN). Assume $K: \Real^d \to \Real$ is continuous and has non-vanishing integral, i.e. $\int_{\Real^d} K(x) \d x \neq 0$.
Then, the set $S_K:= \{ \beta(\cdot; \theta) \vert  m \in \nat, \theta \in \Real^{3m}  \}$ of all RBFNs is uniformly dense in the set $C(\inspace)$ of continuous functions on compact domain $\inspace$. That is,  $\forall f \in C(\inspace) \forall \epsilon >0  \exists m, \theta \in \Real^{3m} : \sup_{x \in \inspace}{ \abs{f -\beta(\cdot;\theta)  } } <\epsilon $. 
\end{lem}

\begin{remark} \label{rem:LipconstofRBFN}
We note that, for any finite-dimensional parameter $\theta$, any RBFN $\beta(\cdot;\theta)$ is Lipschitz continuous as long as the kernel $K$ is. This can be seen by applying Lem. \ref{lem:Hoeldarithmetic} which allows us to conclude that the Lipschitz constant of RBFN $\beta(\cdot;\theta ) = \sum_{i=1}^m w_i \, K(\frac{\cdot - c_i}{\sigma_i} ) $ is given by $L_\beta = \sum_{i=1}^m \abs{\frac{w_i}{\sigma_i}} L_{K}$ where $L_K \in \Real_{\geq 0}$ denotes a Lipschitz constant of $K$. By the same Lemma it is easy to see that choosing the Gaussian kernel for $K$ satisfies both the Lipschitz requirement as well as the integrability requirements of Lem. \ref{lem:RBFNunifapproxcompact}. As a by-product this means that on a compact support, any continuous function can be approximated by some Lipschitz function with arbitrarily small, positive worst-case error $\epsilon >0$. Note, it may well be the case that the Lipschitz constant of the approximator grows with decreasing approximation error bound $\epsilon$. We expect this to be inevitable when the approximated function is not Lipschitz.
\end{remark}

Harnessed with these preparatory statements we can move on to give a guarantee of uniform convergence about our LACKI rule: 

\begin{thm}[Universality of LACKI]
\label{thm:LACKIuniversality}
Assume we are given a sequence $\seq{\data_n}{n \in \nat}$ of samples with observational errors bounded by $\obserrbnd \in \Real$. We set the parameters of the LACKI rule to $\lbf =-\infty,\ubf =\infty, \underline L =0$. In this theorem, we assume input space $\inspace$ to be compact.

\textbf{Then, we have:}
The LACKI rule as per Def. \ref{def:LACKI} is a universal approximator in the following sense:
If the sequence of input grids $\seq{G_n}{n \in \nat}$ converges to compact input space $\inspace$ (uniformly) then for any parameter choice $\hestthresh >0, p \in (0,1]$, the sequence of predictors $\seq{\predfn}{n \in \nat}$ computed by the LACKI rule (uniformly) converges to any continuous target $f : \inspace \to \Real$ up to error $\hestthresh+\obserrbnd$.
That is, 
\begin{itemize}
\item (II) $\forall \epsilon >0,x \in \inspace \exists n_0 \geq 0\forall n\geq n_0: \metric_\outspace\bigl(\predfn(x),f(x)\bigr) \leq \epsilon + \hestthresh+\obserrbnd$ if $G_n$ converges to $\inspace$ as $n \to \infty$.
\item (I) If $G_n$ converges to $\inspace$ \textit{uniformly} (as $n \to \infty$) we even have: $\forall \epsilon >0 \exists n_0 \geq 0\forall n\geq n_0, x \in \inspace: \metric_\outspace\bigl(\predfn(x),f(x)\bigr) \leq \epsilon + \hestthresh +\obserrbnd$. 
\end{itemize}
Convergence occurs with a rate of at most $n \mapsto 2 \max\{\bar L_\lambda, \tilde L \} \,r(n)^p$ where $r$ is the rate of convergence of the input data set sequence $(G_n)$, $\bar L_\lambda =\max_{x,x' \in \inspace} \frac{\metric_\outspace(f(x), f(x')) + 2 \obserrbnd -\hestthresh }{\metric_\inspace^p(x,x')}$ is the an upper bound on the maximally attainable empirical estimate of the H\"older constant and $\tilde L \in \Real$ is a constant such that the target $f$ is $\tilde L-p-$ H\"older up to error up to $\frac \lambda 2$.
\end{thm}

\begin{proof}
We choose any parameter $\hestthresh >0$. As observed in Rem. \ref{rem:LipconstofRBFN}, Lem. \ref{lem:RBFNunifapproxcompact} allows us to infer that there exists a Lipschitz function $\beta(\cdot;\theta)$ that approximates the target with worst-case error of at most $\frac \hestthresh 2$. That is, $\sup_{x \in \inspace} \metric_\outspace\bigl(\beta(x,\theta), f(x) \bigr) \stackrel{Lem. \ref{lem:bilinaddtransinvgroup}}{\leq} \sup_{x \in \inspace} \metric_\outspace\bigl(0, \psi(x) \bigr) \leq \frac \hestthresh 2$ where $\psi:\inspace \to \outspace$ is a function such that $f= \beta(\cdot;\theta) + \psi$ pointwise.
Instead of viewing the given sample as being generated by target $f$ (with some observational error) we can view the sample as being generated by the RBFN $\beta(\cdot;\theta)$ (with some larger observational error): 
We can interpret the erroneous sample $\data_n:= \{\bigl( s_i, \tilde f_i\bigr) \vert i=1,\ldots, N_n\} $ with observational error bound $\obserrbnd$ of $f$ as a ``noisy'' sample $\tilde \data_n:= \{\bigl( s_i, \tilde \beta_i \bigr) \vert i=1,\ldots, N_n\}$ of $\beta(\cdot;\theta)$ where $\tilde \beta_i = \tilde f(s_i) = \beta(s_i;\theta)+\nu(s_i) $. Here the new, enlarged observational error $\nu$ assumes the role of the  bounded ``observational error'' with upper bound $\bar \nu := \obserrbnd + \frac \hestthresh 2$. $\nu(\cdot)$ subsumes both the intrinsic observational error in the data $\data_n$, as well as the discrepancy between RBFN $\beta(\cdot;\theta)$ and the target $f$. Using LACKI to learn $f$ on the basis of $\data_n$ therefore is the same as learning $\beta$ with larger observational error bounded by $\bar \nu := \obserrbnd + \frac \hestthresh 2$.


Being a finite sum of smooth functions with bounded Lipschitz constants, $\beta(\cdot;\theta)$ also is Lipschitz with some optimal Lipschitz constant $L_\beta \in \Real_{\geq 0}$ (cf. Rem. \ref{rem:LipconstofRBFN}).
Since Lipschitz continuity implies H\"older continuity (see Lem. \ref{lem:hoeldexpprop2} ), we have $\beta(\cdot,\theta) \in \hoelset {\tilde L}{ }{p}$ for some $(\tilde L \in \Real_{\geq 0}$ and any $p \in (0,1])$.

Since $\beta(\cdot,\theta)$ is H\"older continuous, the desired convergence statements follow immediately by appealing to Thm. \ref{thm:convergenceifboundedconstandsamplecons_LACKI}.
%%OLD PROOF:
%
%By Lem. \ref{lem:constadaptation_boundedness}, the lazily adapted H\"older constant estimates $L(n)$ are bounded from above by $\max\{\underline L, \tilde L\} = \tilde L  < \infty$. In conjunction with Lem. 
%\ref{lem:LACKIsampleconsistency} (which establishes sample consistency up to $\frac \hestthresh 2$) we have established that all preconditions of Thm. \ref{thm:convergenceifboundedconstandsamplecons} are met by the predictors $\predfn$ $ ({n \in \nat})$. Hence, the theorem allows us to conclude that the sequence of predictors $\seq{\predfn}{n \in \nat}$ converges to $\beta(\cdot;\theta)$ up to an error less than or equal to $\sup_x \metric_\outspace(0,\psi(x)) \leq \frac \hestthresh 2$:
%
%
%(I) In the general case this convergence is pointwise. 
%Let $x \in  \inspace$ and $\epsilon > 0$.  Thm. \ref{thm:convergenceifboundedconstandsamplecons}.(I) implies $\exists n_0 \forall n\geq n_0: \metric_\outspace\bigl (\predfn(x), \beta(x,;\theta) \bigr) \leq \epsilon+\frac \hestthresh 2$. Hence, $\metric_\outspace\bigl (\predfn(x), f(x) \bigr) = \metric_\outspace\bigl (\predfn(x), \beta(x,;\theta) + \psi(x) \bigr) \stackrel{Lem. \ref{lem:bilinaddtransinvgroup}}{\leq} \metric_\outspace\bigl(\predfn(x), \beta(x,;\theta)\bigr) +\metric_\outspace\bigl(0, \psi(x) \bigr) \leq \metric_\outspace\bigl(\predfn(x), \beta(x,;\theta)\bigr) +\frac \hestthresh 2\leq \epsilon +\hestthresh$.
%
%(II)  If convergence of the grid to domain $\inspace$ is uniform convergence of the predictors to the RBFN is uniform (by Thm. \ref{thm:convergenceifboundedconstandsamplecons}.II ). The remainder of the proof is completely anaologous to (I), except that $n_0$ can be chosen independently of $x$.
%
%

\end{proof}

\begin{remark}
Note, our universality guarantee of Thm. \ref{thm:LACKIuniversality} is much stronger than the guarantee for RBFNs.
 The universality statement for RBFNs is purely representational. That is, it states that in principle it is possible to find some RBFN with some choice of parameter that approximates any continuous function with arbitrarily small worst-case approximation error. However, it makes no mention of how to find such an RBFN. By contrast, our universality statement is about a concrete learning rule. And, given a desired error level it tells how to set the parameters of the learning rule in order to achieve the desired approximation error in the dense sample limit.
\end{remark}

%\begin{remark}
%Note, the bound on the speed of convergence generally decreases with increasing best Lipschitz constant of the RBFN. Generally, the Lipschitz constant will increase with decreasing $\hestthresh$. Therefore, so far, we have not shown convergence in the case of $\hestthresh=0$. 
%Intuitively, we should still have convergence since in the limit $L^* \to \infty$ the KI rule seems to coincide with a nearest neighbour regression rule. By construction of integrable functions, it should be clear that this rule converges to any function in the set $\mathcal L_q$ of power-$q$ integrable function  with respect to the $\mathcal L_q$ metric and $q \in \nat$.
%Investigating this observation in greater rigour could be subject to future work. Of course, from a practical point of view our current results are sufficient. If we set $\hestthresh >0$ below the machine-epsilon of the computer the algorithm is intended to run on, our results ensure that the algorithm will not be able to distinguish $\predfn(x)$ and the target $f$ for any input value and sufficiently large training data size $N_n \in \nat$.
%\end{remark}


%\subsection{Convergence in probability with uniformly distributed inputs}
%\jcom{Might omit this subsection}
%Above we have given guarantees relative to the deterministic convergence rates of the input sample to the domain.
%In this subsection, we shall study probabilistic convergence rates as a function of the sample size in situations where the sample is obtained by drawing inputs independently from the domain according to the uniform probability distribution.
% 
%We assume the input data sets $G_n=\{s_1,\ldots,s_{n}\}$ are obtained by drawing the sample inputs $s_i$ i.i.d. uniformly at random from the bounded input domain $\inspace$. For ease of notation, we assume $\inspace := [0,1]^d$ is a normalised d-dimensional hypercube and that the input metric is induced by the maximum norm $\norm{\cdot}_\infty$.
%Under these conditions, we can prove that input sequence $\seq{G_n}{n \in \nat}$ converges to a dense subset in probability:
%\begin{lem}
%For all $x \in \inspace = [0,1]^d$ and $\varepsilon \in (0,1)$ we have
%\[ \Pr[\min\{ \norm{x - g_n}_\infty \,| \, g_n \in G_n \} \geq \varepsilon] \leq r(n) \]
%where 
%\[r(n) = \Bigl(1- \varepsilon^d \Bigr)^n \stackrel{n \to \infty}{\longrightarrow} 0.\]
%\begin{proof}
%Let $x \in \inspace$ and $ \varepsilon \in (0,1)$. 
%Let  $\ball{\varepsilon}{x} =\{\xi \in \inspace | \norm{x-\xi}_\infty \leq \varepsilon \}$ denote a $\varepsilon$-ball around $x$. It is easy to see that we have $\Pr[ \ball{\varepsilon}{x} ] = \int_{\xi \in \inspace} \indicator{\ball{\varepsilon}{x}} (\xi) \d \xi \geq \varepsilon^d$. 
%%
%Hence, 
%\begin{align}
%&\Pr[\min\{ \norm{x - g_n}_\infty \,| \, g_n \in G_n \} \geq \varepsilon ]  \\
%&=\Pr[\forall s \in G_n: \norm{x - s}_\infty \geq \varepsilon] \\
%&\stackrel{i.i.d.}{=} \prod_{i=1}^n \Pr[s_i \notin \ball{\varepsilon}{x}] \\
%&= \prod_{i=1}^n 1-\Pr[s_i \in \ball{\varepsilon}{x}] \\
%&\leq \prod_{i=1}^n 1-\varepsilon^d = (1-\varepsilon^d)^n.
%\end{align}
%\end{proof}
%\end{lem}
%\jcom{Next,will tie this to the main theorems..., or porbably leave out}