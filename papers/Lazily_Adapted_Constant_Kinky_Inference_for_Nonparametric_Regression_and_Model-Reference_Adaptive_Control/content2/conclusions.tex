\section{Conclusions}
In this paper, we have introduced \emph{Lazily Adapted Constant Kinky Inference (LACKI)} as method for nonparametric machine learning. Our method was built on the framework of \emph{Kinky Inference} (which in turn is a generalisation of well-known approaches such as \emph{Lipschitz Interpolation} \cite{Zabinsky2003,Beliakov2006} and \emph{Nonlinear Set Membership (NSM)}  methods \cite{Milanese2004}). 
 Our approach inherits the numerical simplicity of these methods. On top of this, it can deal with bounded additive noise and does not require a priori knowledge about a H\"older constant of the underlying target function but instead estimates the constant online from the data. This of course is of great practical interest since this endows LACKI with superior black-box learning capabilities.
 
To avoid the need to specify the H\"older constant, LACKI adapts the parameter $L(n)$ to reflect a modification of the empirical estimate of the H\"older constant of the underlying target function. 
The adapted parameters were carefully defined to be bounded even if the target function is not H\"older continuous and the data is subject to bounded observational uncertainty.
This allowed us to establish several theoretical guarantees. For instance, we proved that our LACKI rule can learn any H\"older function (up to the level of noise in the data). Moreover, we showed that LACKI can also learn continuous but non-H\"older target functions up to a worst-case approximation error that converges to the level of intrinsic observational  error in the data-generating process, plus a value coinciding with parameter $\hestthresh \geq 0$. Since 
$\hestthresh$ is a free parameter that can be set to arbitrarily small values, convergence can be reduced down to the observational noise-level as much as desired. However, while we have not investigated this theoretically, we would expect that reducing the magnitude of $\hestthresh$ will not only cause the uncertainty bounds to grow (or even grow unbounded if the target is not H\"older or there is noise in the data) but also to slow down the worst-case convergence rate of the LACKI predictors. 
Investigating such convergence rates is a topic we would find interesting to investigate in the context of future work. 

Our theoretical considerations were supplemented by an application of LACKI to 
online learning-based model-reference adaptive control. In a simulated aircraft control problem with nonlinear model uncertainty, we compared our LACKI-based controller against other learning-based methods that were recently proposed in the control literature. Across a range of performance metrics and randomised problem instances, LACKI-MRAC demonstrated consistent and robust performance and outperformed its competitors.
For discrete-time systems with additive nonlinear uncertainty, we provided theoretical guarantees on the tracking success of our LACKI-MRAC controller.

While previous work has utilised NSM methods in control, their methods relied on knowing the true Lipschitz constant a priori - something that is an assumption we would argue is not practically plausible.
Our work addresses this deficiency and provides learning and stability guarantees even in the absence of a priori knowledge of this constant.


In future work, we would like to apply our LACKI learning method to more challenging control tasks that require planning.
To this end, it might be beneficial to link our results to recent work on NSM-based model-predictive control \cite{Canale2014}.

In this work, we have assumed that the observational noise was bounded; we have addressed the issue of unbounded noise in a companion paper \cite{POKIdraft2016} where the H\"older constant parameter $L(n)$ is found as the minimiser of a prediction loss estimator.  We also believe that these estimators could be used to estimate the noise bound if these are unknown a priori.

In this work, our theoretical considerations relied on worst-case analysis. Consequently, our learning guarantees were unable to guarantee the reduction of the approximation error of the predictor to the target below the level of observational noise. However, in Fig. \ref{fig:LACKInoise} and Fig. \ref{fig:LACKInoise2} we observed that our predictors were capable of smoothing out stochastic noise resulting in predictors that exhibited close alignment with the true target. Investigating the extent to which distributional assumptions about the noise can be translated to probabilistic convergence results to the target will also be a subject of future investigation. 



%At this point, we believe that we have found a way to utilise the convergence guarantees derived in this paper to prove that the tracking-error of LACKI-MRAC is guaranteed to vanish as learning is allowed to proceed.

%\section{Conclusions}
%Our paper started by introducing \textit{Kinky Inference (KI)}, a nonparametric machine learning method that can take advantage of knowledge about H\"older regularity and optionally boundedness in order to facilitate inference over function values at unseen (possibly uncertain) inputs on the basis of a (possibly uncertain) sample. When the parameters $L(n)$ are chosen to coincide with a H\"older constant of the target function, the method also provides deterministic prediction uncertainty bounds which we have shown to be tight and (uniformly) convergent to zero in the limit of dense data.
%In contrast to much work in nonparametric regression our theoretical analysis has not made distributional assumptions. This is important in control settings where the collected data might be highly correlated or lie on manifolds of state space. 
%
%To accommodate cases where the target might not be H\"older continuous or the H\"older constant is unknown, we also have presented \textit{Lazily Adapted Constant Kinky Inference (LACKI)}. Here the parameters $L(n)$ are incrementally adapted to the sample $\data_n$ in a manner that allowed us to establish (uniform) universal approximation guarantees. That is, on a compact domain, we can set LACKI up such that its predictors can learn any continuous target up to any arbitrarily small error.
%
%In comparison with other learning methods our approach is not only highly flexible to learn continuous functions but also fast. We have illustrated both properties in the context of model-reference adaptive control where our LACKI method served as the adaptive element of a controller tasked with executing roll commands of a simple aircraft model under wingrock dynamics. In comparison to other recent online-learning methods our LACKI-MRAC controller outperformed its competitors across the board of randomized problem instances. 