\section{Grid sequence definitions}
For both the 1-dimensional case and the $d$- dimensional case we have derived convergence guarantees that were based on the assumption, that the grid sequence $(G_N)$ was an eventually dense partition of domain $I$. In this section, we will define grid sequences that satisfy this assumption.

\subsection{Multi-dimensional domains}

As before, for $N \in \nat$, let $H_N$ denote the number of grid hyperrectangles partitioning $I$. 
$G_N =: \{\bigl(c_1^N,f(c_1^N), J_1^N\bigr),\ldots,\bigl(c_{H_N}^N,f(c_{H_N}^{N}), J_{H_N}^{N}\bigr)\}$ being a set of triples consisting of centre points $c_i^N$, function values $f(c_i^N)$ and hyperrectangle $J_i^N$ (centred around $c_i^N$). 
Let $\mathcal J^N = \{ J_i^N | i=1,\ldots,H_N \} $ be the set of all hyperrectangles of grid $G_N$ that partition domain $I$.

We defined $\bar r_i^N = \max\{ r_{i,1}^N,\ldots, r_{i,d}^N\}$ denotes the maximum radius of hyperrectangle $J_i^N = c_i + \bigl([-r_{i,1}^N ,r_{i,1}^N ]\times \ldots \times [-r_{i,d}^N ,r_{i,d}^N ] \bigr)$. Furthermore, $\bar r^N := \max_{i \in \{1,...,N\}} \bar r_i^N$ is the maximum radius of any hyperrectangle in the $N$th partition of $I$.


\subsubsection{Method 1-- Doubling} 

Let $i \in \nat$, $c_{i,j}^N$ be the $j$th component of centre point $c_{i}^N$. We define hyperrectangle 
$J_i^N = c_i + [-r_i^N,r_i^N]^d =  [-r_i^N+c_{i,1}^N,r_i^N+c_{i,1}^N],\ldots,[-r_i^N+c_{i,d}^N,r_i^N+c_{i,d}^N] $ as follows:

Assume $I= [a_1,b_1] \times,...,\times [a_d,b_d]$.


$N=1$: we set $J^1_1 = I$ and $\mathcal J = \{J^1_1 \}$. Hence, the centre point is 
$c_1^1 = (c_{1,j}^1)_{j=1,...,d}$ where $c_{1,j}^1 = \frac{b_j-a_j}{2}$.

$N\geq 1$:  For $j=1,...,d:$ We partition the projection $[a_j,b_j]$ of the domain onto the $j$th axis into $2^N-1$ intervals $J_{1,j}^N = [a_j,a_j+ \frac{b_j-a_j}{2^N-1}],\ldots, J_{2^N-1,j}^N = [a_j+(2^N-2) \frac{b_j-a_j}{2^N-1},a_j+ (2^N-1) \frac{b_j-a_j}{2^N-1}] $ of length $l_{j}^N=\frac{b_j-a_j}{2^N-1}$.
That is, $J_{k,j}^N =[a_j + (k-1)\, l_j^N ,a_j+ k \, l_j^N]$ $(k=1,...,2^N-1, j=1,...,d)$.   

We set \[ \mathcal J^N := \bigl\{ J_{k_1,1}^N\times \ldots \times J_{k_d,d}^N \, \vert \, k_1,...,k_d \in \{1,...,2^N-1 \}\,   \bigr\}.\]

Note, we have $H_N = |J^N| = (2^N-1)^d$.

By definition, the intervals on the $j$th axis were of uniform length $l_{j}^N=\frac{b_j-a_j}{2^N-1}$.
With $r_{i,j}^N = \frac{l_j^N}{2} \, (i=1,...,H_N)$ we have   \[\bar r^N = \max_{i,j} r_{i,j}^N =  \max_{j \in \{1,...,d\}} \frac{b_j-a_j}{2^N-1} \stackrel{N \to \infty}{\longrightarrow} 0.\]

Therefore, the resulting grid sequence is an eventually dense partition of $I$ (cf. Def. \ref{defn:eventuallydensepartition}) as desired.  

\subsubsection{Method 2-- Incremental refinement}
Next, we will define a gridding method that incrementally refines the grid. 
This can be be of interest if our computational budget is not allowing us to evaluate a power of two function values, or, 
if a regular grid is not ideal. The latter could be the case, for instance, when the Lipschitz constant is not uniform over $I$. In this case it may be beneficial to refine the grids with a higher resolution in regions of the domain where the Lipschitz constant is large. Or, if we want to use our conservative function estimates to answer the question whether the target function ever drops below a certain threshold (as we will want to do in the context of collision detection), we may not choose to refine the estimate in regions where the Lipschitz floor is positive. 

In order to refine grids we proceed as follows:



%\IncMargin{-1em}
%\begin{algorithm}
%\begin{small}
%\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up} \SetKwData{Constraints}{Constraints} 
 %\SetKwData{Collisions}{Collisions} \SetKwData{flag}{flag} \SetKwData{C}{$\mathfrak C$} \SetKwData{winner}{winner} \SetKwData{tcoll}{$t_{\text{coll}}$}
%\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress} \SetKwFunction{Resolve}{Resolve}
%\SetKwFunction{Broadcast}{Broadcast} \SetKwFunction{Planner}{Planner} \SetKwFunction{Auction}{Auction}
%\SetKwFunction{Receive}{Receive} \SetKwFunction{Avoid}{Avoid}
%\SetKwFunction{DetectCollisions}{CollDetect}
%\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
%%
%\Input{Agents $\agi \in \agset$, cost functions $c^\agi$, dynamics, initial start and goal states, initial plans $p^1,...,p^{|\agset|}$ .}
%\Output{collision-free plans $p^1,...,p^{|\agset|}$.}
%\BlankLine
%%\emph{special treatment of the first line}\;
%\Repeat{$\forall \agi \in \agset: \flag^\agi =0 $}
%{
%\For{$\agi \in \agset$}{[
%\flag$^\agi,\C^\agi,\tcoll ]\leftarrow$ \DetectCollisions$^\agi (\agi, \agset-\{\agi\})$\\
%
%\If{$\flag^\agi =1$}
%{
%$\winner \leftarrow \Auction(\C^\agi \cup \{\agi\},\tcoll )$\\
%\ForEach{$\agii \in (\C^\agi \cup \{\agi\})-\{\winner\} $}{
%$p^\agii \leftarrow \Avoid^\agii((\C^\agi \cup \{\agi\})-\{\agii\},\tcoll )$\\
%$\Broadcast^\agii$ ($p^\agii$)
%}
%}
%
%}
%
%}
%\caption{Lazy auction coordination method (AUC) (written in a sequentialized form). Collisions are resolved by choosing new setpoints to enforce collision avoidance. 
%$\mathfrak C^\agi$: set of agents detected to be in conflict with $\agi$. flag$^\agi$: collision detection flag (=0, iff no collision detected). 
%$t_{\text{coll}}$: earliest time where a collision was detected. Avoid: collision resolution method updating the plan by a single new setpoint according to WAIT or FREE ( ref. (II), Sec. \ref{sec:avoidance}).}
%
%\label{alg:lazyauctions}
%\end{small}
%\end{algorithm}

%\IncMargin{-1em}
\begin{algorithm}
\begin{small}
%\SetKwData{flag}{flag}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwFunction{SelectHyperrectangle}{SelectHyperrectangle}
\SetKwFunction{RefineGrid}{RefineGrid}
\SetKwFunction{SplitRectAlongDim}{SplitRectAlongDim}

%
\Input{Set of hyperrectangles $S=\{J_1,...,J_{H_{N-1}} \}$, $J_i =  c_i +\prod_{j =1}^d \, [-r_{i,j},r_{i,j}] $}
\Output{Refined set of hyperrectangles $S'= J_1,...,J_{H_{N}}$}
\BlankLine
%\emph{special treatment of the first line}\;
%$\flag \leftarrow 0$;\\
$\mathfrak J = c+ \prod_{j =1}^d [-r_j,r_j] \leftarrow \SelectHyperrectangle(S)$;\\ %choose a hyperrectangle to refine
$ m \leftarrow \arg \max_{j \in \{1,....,d\}} r_j$;\\
%$Q := \{\mathfrak J_1, \mathfrak J_2, \mathfrak J_3\} \leftarrow \SplitRectAlongDim(\mathfrak J,m)$;
$\{J',J'',J'''\} \leftarrow \SplitRectAlongDim (\mathfrak J,m);$\\
$S' \leftarrow (S \backslash \{\mathfrak J\}) \cup \{J', J'', J'''\}$;\\
return $S'$;

\caption{Function \texttt{RefineGrid}. Incremental grid refinement.}

\label{alg:incgridrefinement1}
\end{small}
\end{algorithm}


%-------------------------------------------------------------------------
\begin{algorithm}
\begin{small}
%\SetKwData{flag}{flag}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\SetKwFunction{SelectHyperrectangle}{SelectHyperrectangle}
\SetKwFunction{RefineGrid}{RefineGrid}
\SetKwFunction{SplitRectAlongDim}{SplitRectAlongDim}

%
\Input{Hyperrectangle $J =  \vc c +\prod_{j =1}^d \, [-r_{j},r_{j}] $, dimension $m \in \{1,...,d\}$.}
\Output{Three hyperrectangles $J',J'',J'''$ partitioning $J$.}
\BlankLine
%\emph{special treatment of the first line}\;
%$\flag \leftarrow 0$;\\
$\vc c' \leftarrow \vc c - \frac{2 r_m}{3} \vc{e_m}$; $\vc c'' \leftarrow c$;
$\vc c''' \leftarrow \vc c + \frac{2 r_m}{3} \vc{e_m}$;\\
$\mathfrak I \leftarrow   \prod_{j =1}^{m-1} \, [-r_{j},r_{j}] \times [\frac{-1}{3} r_m,\frac{1}{3}r_m]\times \prod_{j =m+1}^d \, [-r_{j},r_{j}] $;\\
$J' \leftarrow \tilde c' + \mathfrak I$; $J'' \leftarrow \tilde c'' + \mathfrak I$; $J''' \leftarrow \tilde c''' + \mathfrak I$;\\

\caption{Function \texttt{SplitRectAlongDim}. Compart an input hyperrectangle into three sub-hyperretangles along the $m$th dimension. Each output hyperrectangle is one third of the volume of the original one. }

\label{alg:incgridrefinement1}
\end{small}
\end{algorithm}


Our approach to incremental grid refinment is similar to DIRECT... blahblah-cite.

The remaining issue is how to choose which hyperrectangle to refine. That is, we need to flesh out function 
\texttt{SelectHyperrectangle}. The answer as to how to make this choice depends on our objective. 

For instance, in the \textit{ estimation problem} we may be interested in making refinements that maximally reduce the maximal error on the domain.
That is we would choose to refine sub-hyperrectangle $J_q$ such that  
\begin{align}
q &= \argmax_{i=1}^{H_N} \sup_{x \in J_i} \decke_N^i (x) - \boden_N^i(x) \\
& \stackrel{Lem. \ref{lem:supinfonJimultidim}}{=} \argmax_{i=1}^{H_N} 2 \ell_i \max_{j=1}^d r_{i,j}^N.
\end{align}

Here, the choice of refinement only depends on the Lipschitz constant and the maximal expansion of the hyperrectangle in any dimension.

By contrast, in the \emph{threshold problem}, where we desire to test the hypothesis that the function drops below a certain threshold $\theta$, our choice of expansion should also depend on the function value in question. For instance, so far, all of our function evaluations $f(c_1),...,f(c_{H_N})$ have not dropped below $\theta$. Faced with the choice which hyperrectangle to split next, we may pick the hyperrectangle $J_i$ where the reduction of integral below $\theta$ after the split is maximal. Of course, here the actual function values may come into play. Unfortunately, this means that if our actual goal is to choose a regions such that the splitting results in a maximal decrease of the area of our conservative estimate under the threshold $\theta$, we would have to sample the target function at all candidate hyperrectangles before making our decision. In cases where obtaining such samples is expensive this may be infeasible. In such cases, practicality might suggest a heuristic approach. That is, we might simply pick the hyperrectangle whose current area under $\theta$minimum value is largest. That is, choose $J_q$ such that
\begin{align}
q &= \argmax_{i=1}^{H_N} \int_{J_i} \abs{\min\{ \theta, \boden_N^i(x)\}} \, dx. \\
\end{align}
Alternatively, we may choose to refine $J_q$ to be the hyperrectangle with the lowest value. That is to say,

\begin{align}
q &= \argmin_{i=1}^{H_N} \inf_{x \in J_i} \boden_N^i(x) \\
 &= \argmin_{i=1}^{H_N}  f(c_i) - \ell_i  \max_{j \in \{1,...,d\}} r_{i,j}^N \\
\end{align}

where the last equality follows from Lem. \ref{lem:supinfonJimultidim}.


....
....

....

Finally, in the conservative integration problem, we may choose to elect our next hyperrect $J_q$ for refinement such that 

\begin{align}
q &= \argmax_{i=1}^{H_N} \int_{J_i} \decke_N^i (x) - \boden_N^i(x) \, \d x \\
& = \argmax_{i=1}^{H_N} 2 \ell_i \int_{J_i}  \norm{x-c_i}_\infty \, \d x.
\end{align}

Fortunately, the definite integrals can be computed in closed-form utilizing Lem. \ref{lem:defint_of_maxnorm}. Furthermore, determination of $q$ is independent of the additional function values to be sampled in $J_i$.


\subsubsection{Method 0 -- Batch grid}
For now, we assume we are given a finite function sample of ${H_N}$ evaluations $\{\bigl(c_1^N,f(c_1^N)\bigr),\ldots,\bigl(c_{H_N}^N,f(c_{H_N}^{N})\bigr)\}$ without access to additional function values. How should we construct a grid $G_N$ for conservative estimation and quadrature?

That is, we need to determine $J_N^1,...,J_N^{H_N}$ with centre points $c_1^N,...,c_{H_N}^N$, respectively, that partition domain hyperrectangle $I$.

To this end, we consider the centre-point projections onto the $j$th axis and define a permutation $\mathfrak p_j \in \text{Perm}(H_N)$ such that $c_{\mathfrak p (1),j}^N \leq \ldots \leq c_{\mathfrak p (H_N),j}^N$.

...Ok...should not use centre points... but instead the framework with flexible samples.
Can solve this with Alg. \ref{alg:recursivequad1}. However, this one scales poorly in the dimensionality of the integrand's domain.