\section{A attempt to formalise inference}
An important part of this work pertains to the problem of inference 

To develop a clearer understanding of our design objectives, we need to formalize our understanding of inference in general. On this basis, the specific configurations of our inference problems and our proposed solutions can be discussed in a general, formalized context that links structurally related problems that could be addressed within the same framework as an inference problem. 

Let $(\mathcal D, \mathcal T_{\mathcal D})$, $(\mathcal F, \mathcal T_{\mathcal F})$ and $(\mathcal F', \mathcal T_{\mathcal F'})$ be three topological spaces with topologies of open sets $\mathcal T_{\mathcal D},\mathcal T_{\mathcal F},\mathcal T_{\mathcal F'}$.
We understand the topologies to encode our notions of uncertainty via inclusion. For instance, we understand $A \in \mathcal T_{\mathcal F} $ to be less uncertain than $B \in \mathcal T_{\mathcal F} $ if $A \subset B$.


In our view, an inference problem can, in general terms, be formalized as a tuple $(\mathcal D, \mathcal F, \mathcal F', E,T, )$ where $E: \mathcal T_{\mathcal D} \to \mathcal T_{\mathcal F}$, $T: \mathcal T_{\mathcal F} \to \mathcal T_{\mathcal F'}$ are maps and $\mathcal D$ is a data space representing some notion of environment. As an intuition, you may think of $\mathcal T_{\mathcal F}$ as a codification of what can be known about objects in $\mathcal F$ and $\mathcal T_{\mathcal F'}$ as encoding what can be known about objects in $\mathcal F'$. $T$ states how these things relate. $\mathcal D$ can be understood as a data space.
$E$ could describe how the data gives information about objects in $\mathcal F$. In a well-behaved setting, $E$ would be expected to be monotonous in the sense that $E(D_1) \subset E(D_2)$ if $D_2 \subset D_1$. That is, informally, the more we observe the more we know. 

\textit{A priori knowledge} can be formalized as a subset $\mathcal K_{\text{prior}} $ and \textit{a posteriori} knowledge as a subset $\mathcal K_{\text{post}}\in \mathcal T_{\mathcal F}$. The difference between the two types of knowledge is that the latter has to be extracted (at a ``data extraction cost'' $\gamma_{\text{dat}}$) from the environment via a data extraction mechanism \[M_{\text{dat}}: (\mathcal D,\gamma_{dat}) \mapsto \mathcal K_{\text{post}}.\] 

We define a \textit{topological inference mechanism} as a mapping \[ M_{\text{inf}}: (\mathcal K_{\text{prior}}, \mathcal K_{\text{post}}, \gamma_{\text{inf}}) \mapsto  \mathcal C \in \mathcal T_{\mathcal F'}\] taking an investment of magnitude $\gamma_{\text{inf}}$ (e.g. a computational/energy cost) to transform the given knowledge into a ``conclusion'' $\mathcal C \in \mathcal T_{\mathcal F'}$.

A \textit{data inference mechanism} \[M_{\text{dinf}} : (\mathcal K_{\text{prior}}, D,\gamma_{\text{dinf}}) \mapsto M_{\text{inf}} \bigl(\mathcal K_{\text{prior}},M_{\text{dat}}(D,\gamma_{\text{dat}}),\gamma_{\text{inf}} \bigr)\] couples a data extraction mechanism with an inductive inference mechanism incurring a cost $\gamma_{\text{dinf}} \geq \gamma_{\text{dat}} + \gamma_{\text{inf}}$.

When a distinction between data inference and inference is not required, we will use both notions interchangeably.

\begin{rem}
Of course, normally, we would expect the costs incurred to increase with the size of the data and the cardinality of the underlying topologies. The topologies define the complexity of the search space (search bias). So, an interesting question will be to investigate the dependence of the cost on the chosen spaces. What properties of the topologies are good measures of the degree of challenge a given inference problem poses in terms of cost?
\end{rem}

The inference is called \textit{deductive} if $M_{\text{inf}} (\mathcal K_{\text{prior}},\cdot,\gamma_{\text{inf}})$ is a constant function. Otherwise, it is called \textit{inductive}.

We call a conclusion $\mathcal C$  \textit{conservative} if $T(\mathcal K_{\text{prior}} \cap \mathcal K_{\text{post}}) \subset  \mathcal C$.
By extension, inference mechanism $M_{\text{inf}}$ is conservative if its outputs are conservative conclusions for all inputs.
The inference is \textit{optimal}, if $T(\mathcal K_{\text{prior}} \cap \mathcal K_{\text{post}}) =  \mathcal C$.
Similarily, the data inference mechanism is optimal if $M_{\text{dinf}} (\mathcal K_{\text{prior}}, E(D),\gamma_{\text{dinf}})$ for all data sets $D \in \mathcal T_{\mathcal D}$ and some sufficient investment $\gamma_{\text{dinf}}$.


It may be desirable that an inference mechanisms becomes increasingly certain when an increasing amount of knowledge becomes available. Furthermore, we may not wish that our inference become worse with increasing investment $\gamma$. We can state such a mechanism property as the following \textit{monotonicity} condition:
For $A,B \in \mathcal T_{\mathcal F}$ with $A \subset \mathcal K_{\text{prior}},B \subset \mathcal K_{\text{post}}, \gamma' \geq \gamma$ we have $M_{\text{inf}} (A,B,\gamma') \subset M_{\text{inf}} (\mathcal K_{\text{prior}},\mathcal K_{\text{post}},\gamma)$.

Given prior knowledge $\mathcal K_{\text{prior}}$ and a sequence of data sets $(D_N)_{N \in \nat}$, $D_N\in \mathcal T_{\mathcal D}$, we say the data inference \textit{converges} to \textit{target} $\mathcal C \in \mathcal T_{\mathcal F'}$ if there is a sequence of investments $\gamma_N$ such that the sequence of conclusions $(\mathcal C_N)$, $\mathcal C_N =  M_{\text{dinf}} : (\mathcal K_{\text{prior}}, D_N,\gamma_N) $, converges to $C$ in the topological sense.

After these abstract definitions we will consider a number of concrete examples.
\subsubsection{Examples}
 \begin{ex} [Function estimation] Function estimation is a core component of machine learning. We see that this is a special case of topological inference by choosing $\mathcal F= \mathcal F'$ to be a space of candidate functions endowed with the topology of all enclosures (compact subintervals) $\mathcal T_{\mathcal F'}=\mathcal T_{\mathcal F}= \{ \mathcal E \subset \mathcal F | \exists u,l \in \mathcal F\forall \phi \in \mathcal E\forall x \in I: u(x) \geq \phi(x) \geq l(x) \} $. We can choose $T$ to be the identity map.
\end{ex}

\begin{ex} [Quadrature] \label{ex:topinf_quad} For instance, we choose $\mathcal F = \mathcal L_1(I)$ to be a space of absolutely integrable functions on domain $I$ and endow the space with the discrete topology ($\mathcal T_{\mathcal F}= \textbf{2}^{\mathcal F} $) or the standard topology induced by the $\mathcal L_1(I)$ -norm metric. 
We can choose $\mathcal F' = \Real$ endowed with the standard topology induced by the distance metric $(x,y) \mapsto \abs{x-y}$ and define $T$ element-wise by the linear functional \cite{wernerfunkana} $T: H=\{\phi_i | i\in Q \} \mapsto \{ \int_I \phi_i(x) \, \d x | i \in Q \}$ where $Q$ is an index set arbitrarily chosen to index the elements of $H \in \mathcal T_{\mathcal F}$. 
The data inference method then is a numerical quadrature algorithm that estimates a target function's integral based on a finite sample. 

By contrast, a large class of quadrature methods provide an error interval of the their estimate. Here, $\mathcal F' = \Real, \mathcal T_{\mathcal F'} := \{ [\underline S, \overline S] | \underline S, \overline S \in \Real, \underline S\leq \overline S \}$. The estimate is found on the basis of a finite data set $D_N= \{ \bigl(s_i,f(s_i)\bigr) | s_i < s_{i+1}, s_{i+1}-s_{i} =: h, i=1,...,N \}$ of an equi-spaced samples of integrand $f \in \mathcal F$ and some additional prior knowledge.
For instance, knowing the target function is twice continuously-differentiable and has a maximum second derivative of $m$ translates to prior knowledge set 
$\mathcal K_{prior}(I) = \{f \in C^2(I) |\max_{t \in I} \abs{  f'' (t)} =m  \}$. Based on this knowledge, and with sufficient computational budget $\gamma$, a classic trapezoid rule 
 could output the error interval $M_{\text{dinf}}(\mathcal K_{prior},\mathcal K_{post}, \gamma) = [\hat S_N- \mathfrak E_N, \hat S_N + \mathfrak E_N]$ where integral estimate $\hat S_N = \frac{1}{2} \sum_{i=1}^N \bigl(\,(s_{i+1} - s_i) \,(f(s_{i+1}) - f(s_i)\, \bigr) $ and error bound $\mathfrak E_N = \frac{h^3}{12 N^2} m$.
 It is standard knowledge that under these conditions the trapezoid rule is conservative. That is, the integrals of all functions in the intersection $\mathcal K_{prior}\cap \mathcal K_{post}$ are guaranteed to fall into $[\hat S_N- \mathfrak E_N, \hat S_N + \mathfrak E_N]$ From the definitions, we see that it suffices to invest (computational) inference cost $ \gamma \in \mathcal O(N)$.
\end{ex}