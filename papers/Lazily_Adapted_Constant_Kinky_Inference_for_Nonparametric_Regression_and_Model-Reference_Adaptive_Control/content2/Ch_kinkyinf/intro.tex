\section{Introduction} Quadrature is key in a multitude of contexts including
probabilistic inference, numerical solutions to differential equations and their applications to optimal control. In many practically relevant
cases, integrals cannot be calculated in closed-form rendering it
necessary to resort to numerical approximation. 
A large number of numerical quadrature methods exist that allow a quantification of the error of the integrals estimate. This becomes 

While existing
methods guarantee convergence in the limit of infinite integrand
evaluations, the estimate of the integral may be either larger or
smaller than the true value. This renders those methods unsuitable
in situations where we desire a conservative estimate of the
integral in questions; that is, where we desire to guarantee that
our estimate is either always at least as large or at most as large
as the true integral.

In this paper, such a method for conservative integration is
developed. Our method is applicable in all cases where the integrand
is Lipschitz continuous. As an example application, we utilize our
quadrature method to compute conservative estimates of the collision
probability of stochastic trajectories of multiple objects.

\subsection{Problem 1 -- Erroneous samples, uncertain Lipschitz constant}
\textbf{Problem 2:} Let $I=[a_1,b_1] \times \ldots \times [a_d,b_d] \subset \Real^d$ be a hyperrectangle. For an unknown function $f: I \to \Real$ we are given access to an oracle $O_f:  I \subset \Real^d \times \Real_+ \to \Real^2$ that, incurring  an oracle query cost $\Gamma_O(x,\epsilon_x)$, we can query about target function value $f(x)$ up to error $\epsilon_x > 0$. That is, $O_f(x,\epsilon_x) = V_x := [\underline f(x), \overline f(x)] \subset \Real$ such that it is guaranteed that $f(x) \in V(x) $ and $|V(x)| \leq 2 \epsilon_x$. The oracle could be a numerical approximation procedure to compute target $f$ at input $x$ up to error $\epsilon(x)$ and oracle $\Gamma_O(x,\epsilon(x) )$ could be the algorithm's run-time. In quadrature, it becomes useful to have additional knowledge $K_f$ about the underlying target function. In classical quadrature, this often is knowledge about the maximum value of a $k$th - order derivative.  
In contrast, and allowing wider applicability, we assume this knowledge consists of an uncertain Lipschitz constant $L >0$ drawn from a distribution $P_L$.

Let $D_N = \Bigl( (x_1, V_{x_1}),\ldots,(x_N, V_{x_N}) \Bigr)$ be a set of $N$ samples
and define a function of permissible error $\epsilon : I \to \Real_+, x\mapsto \epsilon_x$.

\begin{defn}[Solution method]
A solution method $M_P = (E,Q)$ for problem $P = (I,O_f,\Gamma_O,K_f)$ consists of 
\begin{itemize}
 \item A sequence of incremental query generation functions $Q_N: D_N \mapsto x_{N+1}$; $\tilde Q_N: D_N \mapsto D_N \cup \{x_{N+1}\}$. 
With initial query set $D_0 = \emptyset$, we can the query set generation function $Q: \nat \to 2^{\nat}$, $Q(N; I, O_f) :=  \tilde Q_N \circ \ldots  \tilde Q_0 \circ D_0 $;
 \item An error estimator, i.e. $E: D_N \mapsto  [\underline S (D_N), \overline S (D_N) ] \subset\Real^2$  based on sample $D_N$;
\item An integral estimator $S: D_N \mapsto  \hat S_N$ where $\hat S_N$ is an estimate of target function integral $\mathcal I = \int_I f(x) \d x$ -- e.g. one could set $S(D_N) := \frac{\underline S (D_N) + \overline S(D_N)}{2}$.
 \end{itemize}
\end{defn}

\begin{defn}[Total cost of a method]

We define  \emph{ total horizon cost} $\Gamma_N(M)$ up to horizon $N$ of method $M = (E,P)$)  as   
\[\Gamma_N(E,Q) := \abs{E(D_N)} + \beta\Bigl( \sum_{i=1}^N \Gamma_O \bigl(x_i,\epsilon(x_i)\bigl), \sum_{i=1}^N \Gamma_{Q_i}(D_i), \Gamma_a(E(D_N) \Bigr).\] Here, 
\begin{itemize}
\item $\Gamma_{Q_i}$ is a query generation cost. For instance, it could be the time required to compute $Q_i(D_i)$;
\item $\Gamma_O$ is the oracle cost as defined before. For instance,  
\item $\Gamma_a (E(D_N))$ is a cost for evaluating and returning the algorithm's output.
\item $\beta: \Real \to \Real$ is a term penalizing processing cost. For example, we could set $\beta (\omega,\sigma,\alpha) = \lambda \max(0, \omega + \psi + \alpha - b), \lambda \approx \infty$ penalizing a solution that exceeds spending more than permitted by budget level $b$.
\end{itemize}
\end{defn}

Given problem $P$ we aim to design a method $M$ fulfilling the following \textbf{desiderata}:
\begin{enumerate}
 \item $\lim_{ N \to \infty} \abs{\hat S_N  - \int_I f(x) \d x} = 2 \int_I \epsilon(x) \d x$ (where convergence is uniform) with probability at least $\delta \in (0,1)$ 
 \item The error vanishes uniformly with increasing sample size: $\abs{E (D_N)} = \abs{\overline S(D_N) - \underline S(D_N)} \stackrel{N \to \infty}{\longrightarrow} 2 \int_I \epsilon(x) \d x$ with probability at least $\delta \in (0,1)$. 
\item The error estimator is \textit{anytime conservative.} That is, for all $N \in \nat, D_N$ it is guaranteed that $\mathcal I \in E (D_N)$ with probability at least $\delta \in (0,1)$;
\item The total horizon costs $\Gamma_N (M)$  are as small as possible $(N \in \nat)$. 
 \end{enumerate}

In this paper we will develop solution methods for our given quadrature problem. The total horizon costs give as a criterion with which to compare the performance of different methods subject to our convergence desiderata. ...


\subsection{Problem 2 -- Erroneous samples, known Lipschitz constant}
We now consider a special case of Problem 1:
\textbf{Problem 2:} Let $I=[a_1,b_1] \times \ldots \times [a_d,b_d] \subset \Real^d$ be a hyperrectangle. For an unknown function $f: I \to \Real$ we are given access to an oracle $O_f:  I \subset \Real^d \times \Real_+ \to \Real^2$ that, incurring  an oracle query cost $\Gamma_O(x,\epsilon_x)$, we can query about target function value $f(x)$ up to error $\epsilon_x > 0$. That is, $O_f(x,\epsilon_x) = V_x := [\underline f(x), \overline f(x)] \subset \Real$ such that it is guaranteed that $f(x) \in V(x) $ and $|V(x)| \leq 2 \epsilon_x$. The oracle could be a numerical approximation procedure to compute target $f$ at input $x$ up to error $\epsilon(x)$ and oracle $\Gamma_O(x,\epsilon(x) )$ could be the algorithm's run-time.
In quadrature, it becomes useful to have additional knowledge $K_f$ about the underlying target function. In classical quadrature, this often is knowledge about the maximum value of a $k$th - order derivative.  
In contrast, we base our consideration on the more general assumption that our knowledge consists of a Lipschitz constant $L >0$. Since all differentiable functions are Lipschitz (but not the other way around), our method applies to a wider scope of target functions.

Let $D_N = \Bigl( (x_1, V_{x_1}),\ldots,(x_N, V_{x_N}) \Bigr)$ be a set of $N$ samples
and define a function of permissible error $\epsilon : I \to \Real_+, x\mapsto \epsilon_x$.

\begin{defn}[Solution method]
A solution method $M_P = (E,Q)$ for problem $P = (I,O_f,\Gamma_O,K_f)$ consists of 
\begin{itemize}
 \item A sequence of incremental query generation functions $Q_N: D_N \mapsto x_{N+1}$; $\tilde Q_N: D_N \mapsto D_N \cup \{x_{N+1}\}$. 
With initial query set $D_0 = \emptyset$, we can the query set generation function $Q: \nat \to 2^{\nat}$, $Q(N; I, O_f) :=  \tilde Q_N \circ \ldots  \tilde Q_0 \circ D_0 $;
 \item An error estimator, i.e. $E: D_N \mapsto  [\underline S (D_N), \overline S (D_N) ] \subset\Real^2$  based on sample $D_N$;
\item An integral estimator $S: D_N \mapsto  \hat S_N$ where $\hat S_N$ is an estimate of target function integral $\mathcal I = \int_I f(x) \d x$ -- e.g. one could set $S(D_N) := \frac{\underline S (D_N) + \overline S(D_N)}{2}$.
 \end{itemize}
\end{defn}

\begin{defn}[Total cost of a method]

We define  \emph{ total horizon cost} $\Gamma_N(M)$ up to horizon $N$ of method $M = (E,P)$)  as   
\[\Gamma_N(E,Q) := \abs{E(D_N)} + \beta\Bigl( \sum_{i=1}^N \Gamma_O \bigl(x_i,\epsilon(x_i)\bigl), \sum_{i=1}^N \Gamma_{Q_i}(D_i), \Gamma_a(E(D_N) \Bigr).\] Here, 
\begin{itemize}
\item $\Gamma_{Q_i}$ is a query generation cost. For instance, it could be the time required to compute $Q_i(D_i)$;
\item $\Gamma_O$ is the oracle cost as defined before. For instance,  
\item $\Gamma_a (E(D_N))$ is a cost for evaluating and returning the algorithm's output.
\item $\beta: \Real \to \Real$ is a term penalizing processing cost. For example, we could set $\beta (\omega,\sigma,\alpha) = \lambda \max(0, \omega + \psi + \alpha - b), \lambda \approx \infty$ penalizing a solution that exceeds spending more than permitted by budget level $b$.
\end{itemize}
\end{defn}

Given problem $P$ we aim to design a method $M$ fulfilling the following \textbf{desiderata}:
\begin{enumerate}
 \item $\lim_{ N \to \infty} \abs{\hat S_N  - \int_I f(x) \d x} = 2 \int_I \epsilon(x) \d x$ where convergence is uniform. 
 \item The error vanishes uniformly with increasing sample size: $\abs{E (D_N)} = \abs{\overline S(D_N) - \underline S(D_N)} \stackrel{N \to \infty}{\longrightarrow} 2 \int_I \epsilon(x) \d x$. 
\item The error estimator is \textit{anytime conservative.} That is, for all $N \in \nat, D_N$ it is guaranteed that $\mathcal I \in E (D_N)$;
\item The total horizon costs $\Gamma_N (M)$  are as small as possible $(N \in \nat)$. 

 \end{enumerate}

In this paper we will develop solution methods for our given quadrature problem. The total horizon costs give as a criterion with which to compare the performance of different methods subject to our convergence desiderata. ...




\subsubsection{Idea on the side: Automated Algorithm Generation by solving a Variational Extremization Problem}
Can we just find a solution method as a solution to the variational problem ?:

\[ \min_{E \in \mathcal E,Q \in \mathcal Q} \sum_{N=1}^\infty \alpha_N \Gamma_N(E,Q) \] where the $\alpha_N$ are discount factors and $E,Q$ need to be appropriately constrained...

Some thoughts:
\begin{itemize}
	\item $\mathcal Q$, the space of all possible $Q$ functions, should essentially be isomorph (I hope) to the set of all countably infinite sequences contained in target domain $I$. I believe we could use diminishing returns properties to limit ourselves to $\ell_2(I)$ sequences (which should contain dense sequences in $I$). That means we should be able to recast the optimization problem as a search for $Q \in l_2(I) $. This now is a variational problem as $l_2(I) $ is isomorph to $L_2(I)$ as far as I recall (or some other well-behaved class of functions). Of course I am not sure how to recast the objective. 
	\item If we can do it for $Q$, the analogous statement should be derivable for $E$. 
	\item Assume, $\epsilon(x) = 0$. Perhaps we can set $\alpha_N : = \abs{E(D_N)}$. This makes sure that the series is not convergent unless $E(D_N) \to 0$ as desired. Not sure how to go about anytime conservativeness
	but that could be lifted and still the approach would be awesome.
	
	\item In fact, if we restrict ourselves to horizons up to a certain order, things should become even a bit easier. This should not be a problem, if our computational budget is restricted anyway as in the example above. Then, the problem degenerates to effectively a finite-dimensional optimization problem (I think).
	\item Additional knowledge (such as the Lipschitz constant) could go into the definition of the function spaces (or into the objective function).
	\item Of course, the whole method is not restricted to quadrature!
	 
\end{itemize}

\subsubsection{Other hunch: Dynamical system}
$Q$ describes the dynamics of a symbolic dynamical system...

\subsection{Coming up with examples:}
Solving for a mean in an SDE whose drift parameters were determined by a probabilistic system identification method -- > gives density over Lipschitz constant... work that out!



